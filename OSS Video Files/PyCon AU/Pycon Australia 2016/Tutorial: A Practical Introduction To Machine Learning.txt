Title: Tutorial: A Practical Introduction To Machine Learning
Publication date: 2016-08-16
Playlist: Pycon Australia 2016
Description: 
	Sam Hames
https://2016.pycon-au.org/schedule/133/view_talk
Machine learning is a hot topic, with lots of hype about what it can and might do. Given the broad landscape of machine learning, and the continuing proliferation of new tools and techniques it can be difficult to get a pragmatic view of how machine learning can be used, or even where to start.

This tutorial will provide a high level introduction to machine learning: what it is, what kind of problems we can solve with it, and how can we apply it. In doing so this tutorial will also introduce the scikit-learn library and show you why this library is a central part of the machine learning ecosystem in Python.

We will start from scratch with a small example dataset, and walk through the process of building and carefully validating a classifier with scikit-learn. The practical focus during the tutorial will be on hands on implementation and experimentation. The technical focus will be on machine learning algorithms as black boxes for making decisions.

Assumed background: This tutorial only assumes that you are comfortable with Python the language. We will *not* assume that you have any maths background, or that you are familiar with numerical computing: discussion of maths and algorithms will be strictly limited to hand waving.


About Jack

Jack Simpson is a PhD candidate at the Australian National University working on image processing and behavioural analysis. He received his Software Carpentry Instructor training over a year ago and since then has organised and taught multiple workshops university.

About Alistair

Alistair Walsh is a cognitive neuroscientist currently working at The University of Melbourne, Research Platforms department as a Community Manager. He teaches Python, machine learning and text and image processing to researchers who aren't from a computer science background but need to use programming tools in their research. Alistair is also an instructor trainer for Software Carpentry and has run Software Carpentry programming workshops in Melbourne, Sydney and Adelaide.
Captions: 
	00:00:00,000 --> 00:00:05,339
welcome everyone again to the last

00:00:01,680 --> 00:00:09,210
session of the day we have here Sam

00:00:05,339 --> 00:00:10,980
Haynes and Alistair Walsh one from the

00:00:09,210 --> 00:00:13,710
Union of Melbourne and one from the

00:00:10,980 --> 00:00:15,890
Union of Queensland and they're going to

00:00:13,710 --> 00:00:18,410
talk about my favorite topic which is

00:00:15,890 --> 00:00:22,769
machine learning so I'll hand that over

00:00:18,410 --> 00:00:24,240
Thanks okay so I'm Sam I think that's

00:00:22,769 --> 00:00:27,750
all the introduction you need for me

00:00:24,240 --> 00:00:29,880
Alastair will say some more later we

00:00:27,750 --> 00:00:32,940
also have another collaborator from Anu

00:00:29,880 --> 00:00:35,010
Jack Simpson who this is his dataset

00:00:32,940 --> 00:00:40,430
from his PhD project so if you don't

00:00:35,010 --> 00:00:43,530
like it we can blame him first challenge

00:00:40,430 --> 00:00:47,850
for the day is set up so you'll need

00:00:43,530 --> 00:00:50,100
numpy Syfy and scikit-learn installed if

00:00:47,850 --> 00:00:53,219
you have a problem put up a pink sticky

00:00:50,100 --> 00:00:56,010
note and we will magically fly over

00:00:53,219 --> 00:00:57,480
there you also need the workshop data

00:00:56,010 --> 00:00:59,579
set there's a link to it on the

00:00:57,480 --> 00:01:02,280
conference wiki under the tutorial setup

00:00:59,579 --> 00:01:05,489
page so just get that and stick it

00:01:02,280 --> 00:01:09,780
somewhere in your directory now show

00:01:05,489 --> 00:01:13,409
fans who's used scikit-learn before okay

00:01:09,780 --> 00:01:16,259
who's used numpy before okay

00:01:13,409 --> 00:01:19,619
so mu no apologize to some of you we may

00:01:16,259 --> 00:01:21,689
cover things a little more basically

00:01:19,619 --> 00:01:24,060
than you're used to we wanted to do a

00:01:21,689 --> 00:01:26,040
more accessible tutorial so if you're

00:01:24,060 --> 00:01:29,390
finding it too easy help out the people

00:01:26,040 --> 00:01:32,700
around you talk to the people around you

00:01:29,390 --> 00:01:36,900
also don't be afraid to ask for help

00:01:32,700 --> 00:01:38,700
with your sticky notes when when you're

00:01:36,900 --> 00:01:41,939
stuck on anything so I'll just give you

00:01:38,700 --> 00:01:43,680
all some time to get set up let us know

00:01:41,939 --> 00:01:45,780
if there's any problems and we'll

00:01:43,680 --> 00:01:47,970
hopefully get sticky notes to all of you

00:01:45,780 --> 00:01:51,829
I don't know if we have enough because I

00:01:47,970 --> 00:01:51,829
wasn't expecting to fill the room so

00:01:52,520 --> 00:01:57,680
so if you already put up a green sticky

00:01:55,130 --> 00:02:00,369
note so we can at least track some kind

00:01:57,680 --> 00:02:00,369
of progress

00:03:14,569 --> 00:03:22,890
so just just for your just so you know

00:03:21,840 --> 00:03:25,440
what we're doing we'll be demonstrating

00:03:22,890 --> 00:03:26,640
in Jupiter and we'll have some stuff in

00:03:25,440 --> 00:03:28,799
that plot lib they're bigger

00:03:26,640 --> 00:03:30,269
dependencies so if you don't normally

00:03:28,799 --> 00:03:31,769
work in Drupad or you don't want to work

00:03:30,269 --> 00:03:33,360
in Jupiter don't worry about installing

00:03:31,769 --> 00:03:34,650
it if you want to follow along in the

00:03:33,360 --> 00:03:37,760
exact environment we're doing it that's

00:03:34,650 --> 00:03:37,760
what we will be doing

00:07:43,040 --> 00:07:46,290
all right can I just have a quick show

00:07:45,450 --> 00:07:48,900
of hands

00:07:46,290 --> 00:07:51,300
so everybody's done that can you put up

00:07:48,900 --> 00:07:52,919
your hand if you're not ready sorry if

00:07:51,300 --> 00:07:55,860
you're not ready can you put up your

00:07:52,919 --> 00:08:02,660
hand did everybody get the data set from

00:07:55,860 --> 00:08:09,600
the link on the wiki yes you've got it

00:08:02,660 --> 00:08:13,110
okay all right I'm just gonna we'll do

00:08:09,600 --> 00:08:15,390
the final check so if you can run the

00:08:13,110 --> 00:08:18,540
testing your installation lines in

00:08:15,390 --> 00:08:19,680
whatever environment you're using so you

00:08:18,540 --> 00:08:20,880
should be able to import numpy you

00:08:19,680 --> 00:08:22,590
should be able to import scikit-learn

00:08:20,880 --> 00:08:24,540
and you should be able to load the data

00:08:22,590 --> 00:08:57,450
set if you're you're all good with that

00:08:24,540 --> 00:08:59,370
we're ready to go all right I'm just

00:08:57,450 --> 00:09:03,300
gonna give you another minute or two to

00:08:59,370 --> 00:09:04,740
make sure everybody's ready to go cause

00:09:03,300 --> 00:09:07,310
it's more fun when we're not installing

00:09:04,740 --> 00:09:07,310
software

00:10:32,890 --> 00:10:38,330
all right so I'm just going to explain

00:10:34,970 --> 00:10:40,850
how everything's going to work this is

00:10:38,330 --> 00:10:41,990
an interactive tutorial which means it's

00:10:40,850 --> 00:10:43,760
going to be lots of fun and we're all

00:10:41,990 --> 00:10:49,040
going to be really lost and hopefully

00:10:43,760 --> 00:10:50,300
stuff won't break too badly Alistair is

00:10:49,040 --> 00:10:52,190
going to start shortly he's going to

00:10:50,300 --> 00:10:55,160
introduce the data set and the problem

00:10:52,190 --> 00:10:57,380
we're working on and start diving into

00:10:55,160 --> 00:10:58,970
the actual content we're gonna do a

00:10:57,380 --> 00:11:00,380
quick overview of numpy just for those

00:10:58,970 --> 00:11:03,470
who aren't comfortable with it because

00:11:00,380 --> 00:11:05,210
in all honesty most of your adventures

00:11:03,470 --> 00:11:07,760
in machine learning are going to be

00:11:05,210 --> 00:11:10,580
adventures in manipulating your data not

00:11:07,760 --> 00:11:13,040
actually doing anything machine learning

00:11:10,580 --> 00:11:14,570
related it's just going to be fiddling

00:11:13,040 --> 00:11:16,100
with shapes and sizes and formats and

00:11:14,570 --> 00:11:19,550
all those other really exciting fun

00:11:16,100 --> 00:11:22,480
things that we all want to do so I hand

00:11:19,550 --> 00:11:22,480
over to Alastair now

00:11:36,460 --> 00:11:40,530
oh yes

00:11:41,439 --> 00:11:46,449
hey you going my name's Alastair Walsh

00:11:44,019 --> 00:11:50,459
my background is in cognitive

00:11:46,449 --> 00:11:53,259
neuroscience about to start a PhD at the

00:11:50,459 --> 00:11:56,410
Florian Heidelberg doing machine

00:11:53,259 --> 00:12:02,679
learning on brain scans to work on

00:11:56,410 --> 00:12:07,809
stroke treatment I reckon I can identify

00:12:02,679 --> 00:12:10,149
an iris at 50 paces and I reckon I know

00:12:07,809 --> 00:12:15,610
the survivors a Titanic by their first

00:12:10,149 --> 00:12:17,920
name so we thought we'd show you a data

00:12:15,610 --> 00:12:20,529
set from jack simpson's research Jack's

00:12:17,920 --> 00:12:24,100
up in Canberra doing research on

00:12:20,529 --> 00:12:28,480
honeybees because of the the problem

00:12:24,100 --> 00:12:30,899
with hive collapse it's actually a um a

00:12:28,480 --> 00:12:34,959
very important area of research because

00:12:30,899 --> 00:12:45,819
we don't really know why why the bees

00:12:34,959 --> 00:12:48,389
are a suffering so he's one if we have

00:12:45,819 --> 00:12:48,389
some pictures

00:12:53,390 --> 00:13:01,170
aha so here's some bees we have a

00:12:58,290 --> 00:13:03,090
control group an experimental group Jack

00:13:01,170 --> 00:13:05,160
does mean things like feed the

00:13:03,090 --> 00:13:07,740
experimental group caffeine just for

00:13:05,160 --> 00:13:09,210
kicks no it's actually really serious

00:13:07,740 --> 00:13:13,500
research he's not being mean to the bees

00:13:09,210 --> 00:13:15,450
and in each hive there's a queen so we

00:13:13,500 --> 00:13:17,880
have different tags for the control

00:13:15,450 --> 00:13:22,170
group the experimental group and the

00:13:17,880 --> 00:13:22,740
Queen and because it's dark inside the

00:13:22,170 --> 00:13:27,480
hive

00:13:22,740 --> 00:13:31,050
it's videotaped in infrared infrared

00:13:27,480 --> 00:13:34,200
light so the tags are reflective so what

00:13:31,050 --> 00:13:37,380
Jack ends up with is hundreds of hours

00:13:34,200 --> 00:13:40,050
of video of the movement inside the hive

00:13:37,380 --> 00:13:42,270
and that's what he's analyzing to try

00:13:40,050 --> 00:13:48,620
and find insights into what's going on

00:13:42,270 --> 00:13:51,120
with the bees he's used OpenCV to

00:13:48,620 --> 00:13:53,610
identify the tags they glow brightly

00:13:51,120 --> 00:13:56,220
under the infrared lights put a little

00:13:53,610 --> 00:13:59,010
box around them and we end up with a

00:13:56,220 --> 00:14:02,310
whole lot of little images that are 24

00:13:59,010 --> 00:14:03,060
pixels by 24 pixels it's in black and

00:14:02,310 --> 00:14:04,770
white

00:14:03,060 --> 00:14:10,020
there's no color information because

00:14:04,770 --> 00:14:12,089
it's infrared so that means we have a

00:14:10,020 --> 00:14:21,240
whole lot of little images that are

00:14:12,089 --> 00:14:26,250
suitable to reading to numpy there's the

00:14:21,240 --> 00:14:35,400
Queen and this is a shot from inside the

00:14:26,250 --> 00:14:39,180
hive so one frame of the video and here

00:14:35,400 --> 00:14:44,010
is a variety of the of the tags so we

00:14:39,180 --> 00:14:49,050
have a tag with a square Center for the

00:14:44,010 --> 00:14:54,920
control group and a circular Center for

00:14:49,050 --> 00:14:58,980
the treatment group and the queen tag is

00:14:54,920 --> 00:15:02,459
no no shape in the middle at all so we

00:14:58,980 --> 00:15:03,990
obviously have a lot more images of the

00:15:02,459 --> 00:15:05,490
control group and the treatment group

00:15:03,990 --> 00:15:06,819
than the Queen because there's only one

00:15:05,490 --> 00:15:08,079
queen

00:15:06,819 --> 00:15:15,339
we have picked out a number of those

00:15:08,079 --> 00:15:18,009
images for the Queen so that's an idea

00:15:15,339 --> 00:15:21,009
of the the type of data we have the

00:15:18,009 --> 00:15:22,649
problem is identifying the tags we need

00:15:21,009 --> 00:15:25,779
to know who belongs to which group

00:15:22,649 --> 00:15:30,999
there's too much footage too many images

00:15:25,779 --> 00:15:33,910
to do that by hand by far so to identify

00:15:30,999 --> 00:15:36,989
the bees we need some type of machine

00:15:33,910 --> 00:15:36,989
learning process

00:15:46,779 --> 00:16:07,070
back that way don't have a problem with

00:16:05,690 --> 00:16:25,430
machine learning I have a problem with

00:16:07,070 --> 00:16:28,040
using your trackpad what is it okay so

00:16:25,430 --> 00:16:30,410
in the setup we've we've set you up with

00:16:28,040 --> 00:16:34,279
the the libraries and you've got the

00:16:30,410 --> 00:16:37,760
data set this is the first chunk of code

00:16:34,279 --> 00:16:41,360
just importing numpy and matplotlib if

00:16:37,760 --> 00:16:43,310
using the jupiter notebook the magic for

00:16:41,360 --> 00:16:46,970
matplotlib inline to show the images

00:16:43,310 --> 00:16:50,660
within the jupiter notebook and we've

00:16:46,970 --> 00:16:58,190
saved the data in a number format so it

00:16:50,660 --> 00:17:00,050
loads in nicely we're going to save the

00:16:58,190 --> 00:17:03,920
images data under the and images

00:17:00,050 --> 00:17:06,589
variable and the identifying labels for

00:17:03,920 --> 00:17:10,429
the ones we have labeled under labels

00:17:06,589 --> 00:17:13,760
variable and if i run that to show you

00:17:10,429 --> 00:17:18,130
the shape of it the images are 24 pixels

00:17:13,760 --> 00:17:20,929
by 24 pixels and we have 730 of them and

00:17:18,130 --> 00:17:24,620
they're saved in a format which is

00:17:20,929 --> 00:17:26,000
unsigned 8-bit integers and that becomes

00:17:24,620 --> 00:17:29,000
important later on when we're

00:17:26,000 --> 00:17:34,270
manipulating them so unsigned 8-bit

00:17:29,000 --> 00:17:34,270
integers have a range of values of 255

00:17:42,830 --> 00:17:59,340
should I be able to go down there help

00:17:48,330 --> 00:18:00,870
Jack help Sam we love remote

00:17:59,340 --> 00:18:03,860
collaboration also actually have to

00:18:00,870 --> 00:18:11,730
click on that I can actually remember

00:18:03,860 --> 00:18:14,270
okay so we now have three dimensional

00:18:11,730 --> 00:18:18,990
matrix with all the image our data

00:18:14,270 --> 00:18:20,730
loaded in 24 by 24 pixels imagine a deck

00:18:18,990 --> 00:18:23,669
of cards which each with each card

00:18:20,730 --> 00:18:26,520
having an image on it and we have 730

00:18:23,669 --> 00:18:35,370
cards that's the an idea of what it

00:18:26,520 --> 00:18:37,530
looks like and I'm going to load one of

00:18:35,370 --> 00:18:41,460
those images just so we can have a look

00:18:37,530 --> 00:18:44,669
at it in a variable called first image

00:18:41,460 --> 00:18:48,059
and three of them into a variable called

00:18:44,669 --> 00:18:51,049
several images and just to show the

00:18:48,059 --> 00:18:54,090
shape the the single image is 24 24 and

00:18:51,049 --> 00:18:56,220
if we loaded 3 in we'd have a

00:18:54,090 --> 00:18:57,860
three-dimensional matrix 24 24 but

00:18:56,220 --> 00:19:07,260
there's three layers each layer

00:18:57,860 --> 00:19:08,580
representing an individual image so

00:19:07,260 --> 00:19:20,370
again just to remind you what they look

00:19:08,580 --> 00:19:24,720
like so I might go back to that slide

00:19:20,370 --> 00:19:26,280
just to so you notice we have the image

00:19:24,720 --> 00:19:28,140
in the middle which is what we're

00:19:26,280 --> 00:19:32,400
interested in that's what identifies the

00:19:28,140 --> 00:19:36,659
be around the outside around the border

00:19:32,400 --> 00:19:42,990
of the image there's some black areas

00:19:36,659 --> 00:19:45,299
we're going to crop the image to the

00:19:42,990 --> 00:19:49,950
central square so we just have that

00:19:45,299 --> 00:19:54,360
image in the middle anybody want to have

00:19:49,950 --> 00:19:56,700
a go at why we wouldn't leave the board

00:19:54,360 --> 00:20:01,770
around the outside why we're cropping it

00:19:56,700 --> 00:20:03,090
down just so we have the yeah yeah that

00:20:01,770 --> 00:20:08,100
that's going to confuse the machine

00:20:03,090 --> 00:20:10,649
learning algorithm if we leave the board

00:20:08,100 --> 00:20:12,600
around the outside the machine learning

00:20:10,649 --> 00:20:14,429
algorithm is going to look at that as

00:20:12,600 --> 00:20:18,200
meaningful data and look at the

00:20:14,429 --> 00:20:21,470
variability in the border and try to

00:20:18,200 --> 00:20:24,870
associate that with one of the groups so

00:20:21,470 --> 00:20:26,850
having that there is well let's call it

00:20:24,870 --> 00:20:28,140
noise you know it's information the

00:20:26,850 --> 00:20:29,880
machine learning algorithm is going to

00:20:28,140 --> 00:20:32,100
try and fit but it's not actually

00:20:29,880 --> 00:20:40,770
meaningful it's just the outer edge of

00:20:32,100 --> 00:20:43,950
the of the label of the tag so here's

00:20:40,770 --> 00:20:48,510
our code for taking the center of the

00:20:43,950 --> 00:20:52,260
image we're going to move in 4 pixels

00:20:48,510 --> 00:20:54,659
and only take 2 4 pixels in the rows and

00:20:52,260 --> 00:20:57,539
same with the columns we're just going

00:20:54,659 --> 00:21:05,460
to take 4 pixels in and 4 pixels from

00:20:57,539 --> 00:21:09,630
the end so we've gone from an image that

00:21:05,460 --> 00:21:11,730
was 24 by 24 down to 16 by 16 which is

00:21:09,630 --> 00:21:15,140
that central image with the shape in the

00:21:11,730 --> 00:21:15,140
middle that we're really interested in

00:21:19,980 --> 00:21:23,790
is everybody keeping up like people that

00:21:22,300 --> 00:21:25,840
are following along with the code I'm

00:21:23,790 --> 00:21:27,430
moving too fast there's too many of you

00:21:25,840 --> 00:21:28,350
you make me nervous and going too

00:21:27,430 --> 00:21:30,730
quickly

00:21:28,350 --> 00:21:32,650
can somebody suggest where you'd like to

00:21:30,730 --> 00:21:33,940
go back to just so we can keep up with

00:21:32,650 --> 00:21:35,440
the code it would be good if you could

00:21:33,940 --> 00:21:47,500
if you want to follow along with the

00:21:35,440 --> 00:22:19,120
code they are let me go back to loading

00:21:47,500 --> 00:22:21,130
in the data so I think everybody managed

00:22:19,120 --> 00:22:22,660
to download the data was that right we

00:22:21,130 --> 00:22:28,680
check that before if you wanted to

00:22:22,660 --> 00:22:37,840
follow along you've got the data and

00:22:28,680 --> 00:22:39,700
this is the code to load the data in so

00:22:37,840 --> 00:22:41,700
the data is saved in a numpy format so

00:22:39,700 --> 00:22:44,940
you can just use number to load that

00:22:41,700 --> 00:22:44,940
data in

00:23:01,910 --> 00:23:06,600
this step is really just looking at the

00:23:04,440 --> 00:23:07,710
images so if you didn't want to type

00:23:06,600 --> 00:23:09,060
this out that's not really going to

00:23:07,710 --> 00:23:19,340
affect anything I just wanted to show

00:23:09,060 --> 00:23:21,840
you what the image just looked like yep

00:23:19,340 --> 00:23:23,100
there's some demonstration steps just to

00:23:21,840 --> 00:23:24,840
see what it's like but when we actually

00:23:23,100 --> 00:23:28,200
go to do the machine learning bid it's

00:23:24,840 --> 00:23:37,320
using those original that data that we

00:23:28,200 --> 00:23:43,310
set up so this is just cropping one

00:23:37,320 --> 00:23:43,310
image to show that we're cropping it

00:23:46,940 --> 00:23:54,390
okay this next nice next step is

00:23:49,800 --> 00:23:59,820
important so we've got we've got a

00:23:54,390 --> 00:24:03,920
square image that's what we need to do

00:23:59,820 --> 00:24:07,650
is have a single row for every

00:24:03,920 --> 00:24:10,830
observation so what we're going to do is

00:24:07,650 --> 00:24:15,150
take a numpy array which is really just

00:24:10,830 --> 00:24:18,210
a list of lists so each row is an

00:24:15,150 --> 00:24:19,830
individual list they're they're

00:24:18,210 --> 00:24:22,020
displayed stacked on top of each other

00:24:19,830 --> 00:24:24,870
but numpy really understands it as a

00:24:22,020 --> 00:24:26,910
list of lists with each row being a list

00:24:24,870 --> 00:24:29,240
one after the other we're just going to

00:24:26,910 --> 00:24:34,770
join all those lists together so we have

00:24:29,240 --> 00:24:40,800
as a one-dimensional piece of data so

00:24:34,770 --> 00:24:42,870
we're going to reshape giving it the

00:24:40,800 --> 00:24:45,000
number rows the the minus one is just

00:24:42,870 --> 00:24:46,380
saying numpy you're good at maths you

00:24:45,000 --> 00:24:48,060
work that out I'm not going to tell you

00:24:46,380 --> 00:24:52,640
how long that's going to be when we lay

00:24:48,060 --> 00:24:52,640
everything out make the data that shape

00:24:56,280 --> 00:25:08,310
and so now we have 730 rows by 576

00:25:02,670 --> 00:25:12,450
columns or 576 features with each pixel

00:25:08,310 --> 00:25:14,820
value being a feature so it doesn't need

00:25:12,450 --> 00:25:16,560
to be 3 dimensional data because we're

00:25:14,820 --> 00:25:19,740
not actually taking any information

00:25:16,560 --> 00:25:21,570
about the relationship between one pixel

00:25:19,740 --> 00:25:23,760
to the pixels nearer we're just taking

00:25:21,570 --> 00:25:26,070
the value of that pixel the brightness

00:25:23,760 --> 00:25:29,190
of that pixel so we can just lay it out

00:25:26,070 --> 00:25:30,330
like that we're not what not doing

00:25:29,190 --> 00:25:38,400
anything more fancier than that

00:25:30,330 --> 00:25:41,400
basically so just pause for a second

00:25:38,400 --> 00:25:47,370
until I notice people that are trying to

00:25:41,400 --> 00:25:54,740
type finish typing although there may be

00:25:47,370 --> 00:25:54,740
running a mouse okay

00:26:02,299 --> 00:26:08,009
so when we load images in as numpy

00:26:05,399 --> 00:26:10,469
arrays it's great because we can do

00:26:08,009 --> 00:26:15,389
simple manipulations of the image quite

00:26:10,469 --> 00:26:19,679
easily what you're seeing here is we can

00:26:15,389 --> 00:26:22,289
we can add a value of 30 to the entire

00:26:19,679 --> 00:26:26,489
image just to increase the value of

00:26:22,289 --> 00:26:30,109
every pixels every pixel by 30 or we can

00:26:26,489 --> 00:26:34,559
reduce the brightness by 30 just by

00:26:30,109 --> 00:26:37,199
taking 30 away from every value we can

00:26:34,559 --> 00:26:40,440
increase the contrast by multiplying by

00:26:37,199 --> 00:26:42,629
a number greater than 1 so 1.5 here or

00:26:40,440 --> 00:26:46,289
we can reduce the current contrast by

00:26:42,629 --> 00:26:52,379
multiplying by a number less than 1 so

00:26:46,289 --> 00:26:57,029
very very simple changes this

00:26:52,379 --> 00:26:59,039
pre-processing you would probably do an

00:26:57,029 --> 00:27:00,599
iterative process to see whether

00:26:59,039 --> 00:27:02,519
increasing the brightness or the

00:27:00,599 --> 00:27:04,859
contrast or reducing the brightness or

00:27:02,519 --> 00:27:09,149
the contrast was making a difference to

00:27:04,859 --> 00:27:12,839
how identifiable the tags were another

00:27:09,149 --> 00:27:14,849
thing that's worth doing is blurring the

00:27:12,839 --> 00:27:17,699
images which might be a little bit

00:27:14,849 --> 00:27:20,519
counterintuitive anybody want to have a

00:27:17,699 --> 00:27:28,339
guess at why blurring the image actually

00:27:20,519 --> 00:27:28,339
improves the accuracy yep yep so

00:27:28,429 --> 00:27:32,699
smoothing the data reduces the things

00:27:31,409 --> 00:27:35,069
that the machine learning algorithm

00:27:32,699 --> 00:27:38,459
might take notice of that aren't really

00:27:35,069 --> 00:27:40,199
meaningful if that's a hand wavy sort of

00:27:38,459 --> 00:27:42,569
explanation of how that process works so

00:27:40,199 --> 00:27:44,729
blurring the image actually increases

00:27:42,569 --> 00:27:47,029
the the accuracy we're not doing that

00:27:44,729 --> 00:27:47,029
here

00:27:59,509 --> 00:28:16,229
let me see if I can find that image some

00:28:03,989 --> 00:28:17,070
way that we can see hmm okay I should

00:28:16,229 --> 00:28:20,909
have kept my mouth shut

00:28:17,070 --> 00:28:29,869
rather than promising what about if I

00:28:20,909 --> 00:28:29,869
reduce the size they're gonna wear okay

00:28:32,179 --> 00:28:38,099
no it's not look you can see a small

00:28:35,070 --> 00:28:39,889
slice on the top here one thing we

00:28:38,099 --> 00:28:47,669
wanted to point out when we're just

00:28:39,889 --> 00:28:50,940
adding values to the image can you see

00:28:47,669 --> 00:28:53,129
it's gone from this to this here where

00:28:50,940 --> 00:28:58,200
the bits that were bright are now

00:28:53,129 --> 00:29:00,570
actually black that's related to the

00:28:58,200 --> 00:29:03,929
fact that we have unsigned integers that

00:29:00,570 --> 00:29:07,889
have values from 0 to 25 if we just add

00:29:03,929 --> 00:29:10,200
values and we go over to 55 it'll scroll

00:29:07,889 --> 00:29:13,019
back around and be a very low value so

00:29:10,200 --> 00:29:15,179
even though it's very easy to be able to

00:29:13,019 --> 00:29:16,769
add values and do simple manipulations

00:29:15,179 --> 00:29:21,109
you have to be a little bit careful or

00:29:16,769 --> 00:29:21,109
else you can get an unexpected result

00:29:28,159 --> 00:29:33,950
which is that question there so does

00:29:30,929 --> 00:29:33,950
anybody know the answer to that question

00:29:35,090 --> 00:29:42,620
does that make sense to anybody who just

00:29:40,159 --> 00:29:45,000
the reason we put it there is because

00:29:42,620 --> 00:29:46,350
like I said most of the time you deal

00:29:45,000 --> 00:29:48,269
with your data not with the machine

00:29:46,350 --> 00:29:50,190
learning model and when you forget to do

00:29:48,269 --> 00:29:52,110
stuff like convert your integers into

00:29:50,190 --> 00:29:53,460
plates that will flow right through your

00:29:52,110 --> 00:29:54,440
problem and you'll wonder what is going

00:29:53,460 --> 00:29:56,279
on

00:29:54,440 --> 00:29:58,049
normally the place to look for problems

00:29:56,279 --> 00:29:59,490
is in your data and your formatting and

00:29:58,049 --> 00:30:01,639
all of the data manipulation that goes

00:29:59,490 --> 00:30:04,529
before that this is just a reminder that

00:30:01,639 --> 00:30:06,480
types and what you expect to be flowing

00:30:04,529 --> 00:30:07,950
through you need to verify that before

00:30:06,480 --> 00:30:09,269
you even worry about the modelling worry

00:30:07,950 --> 00:30:11,220
about what your data is and what it's

00:30:09,269 --> 00:30:24,120
doing because the modeling is honestly

00:30:11,220 --> 00:30:28,620
the easy part yep okay so he's just to

00:30:24,120 --> 00:30:30,210
remind you that we have a rectangle

00:30:28,620 --> 00:30:34,649
pattern a circle pattern and a blank

00:30:30,210 --> 00:30:36,330
pattern and we can't feed words into

00:30:34,649 --> 00:30:40,590
scikit-learn we need to feed everything

00:30:36,330 --> 00:30:45,000
in as numeric values so the labels are

00:30:40,590 --> 00:30:47,309
identified by 0 1 & 2 we have 100 bees

00:30:45,000 --> 00:30:49,380
in the control group 100 bees that are

00:30:47,309 --> 00:31:02,279
treated with caffeine and the single

00:30:49,380 --> 00:31:10,080
queen so here I'm loading in the labels

00:31:02,279 --> 00:31:13,110
the labels for each image and printing

00:31:10,080 --> 00:31:15,840
out the shape of the labels data so

00:31:13,110 --> 00:31:20,730
there's 730 labels to go along with our

00:31:15,840 --> 00:31:22,649
730 images and just stepping through the

00:31:20,730 --> 00:31:26,669
labels a hundred at a time just to show

00:31:22,649 --> 00:31:29,970
you that there's zeros ones and twos in

00:31:26,669 --> 00:31:31,860
a sequence so all the 0 images come

00:31:29,970 --> 00:31:34,639
first then the ones and then the Queen

00:31:31,860 --> 00:31:34,639
ones which are two

00:31:48,210 --> 00:31:53,860
so that's that's a very quick

00:31:51,880 --> 00:31:56,170
run-through the pre-processing a lot of

00:31:53,860 --> 00:31:58,270
the work is in getting the images to a

00:31:56,170 --> 00:32:02,860
point where you can do the machine

00:31:58,270 --> 00:32:05,170
learning on them the the bulk of the

00:32:02,860 --> 00:32:08,170
time is spent on preparing your images

00:32:05,170 --> 00:32:10,480
like that really is where the a lot of

00:32:08,170 --> 00:32:12,040
the hours come in trying to work out

00:32:10,480 --> 00:32:15,270
what's going to affect the success of

00:32:12,040 --> 00:32:17,370
your model training a model and

00:32:15,270 --> 00:32:20,350
predicting with it

00:32:17,370 --> 00:32:24,070
scikit-learn has made really easy so

00:32:20,350 --> 00:32:26,140
that um those preparation steps are

00:32:24,070 --> 00:32:31,150
really where a lot of trial and error

00:32:26,140 --> 00:32:37,920
comes in after we've pre-processed the

00:32:31,150 --> 00:32:44,500
images we then need a testing set a

00:32:37,920 --> 00:32:47,440
training set so we need to split the

00:32:44,500 --> 00:32:51,690
data up because we're going to train a

00:32:47,440 --> 00:32:55,750
machine learning model on some data and

00:32:51,690 --> 00:32:57,730
of course we can we can get that really

00:32:55,750 --> 00:33:00,850
accurate if we know the answer to

00:32:57,730 --> 00:33:04,210
everything but the the problem of

00:33:00,850 --> 00:33:07,110
overfitting the model to the data that

00:33:04,210 --> 00:33:12,730
we've actually got and then making it

00:33:07,110 --> 00:33:15,730
less able to detect new tags so our

00:33:12,730 --> 00:33:18,720
purpose here is to take take a set of

00:33:15,730 --> 00:33:23,020
data and identify them by hand so a

00:33:18,720 --> 00:33:23,980
supervised machine learning should I

00:33:23,020 --> 00:33:26,320
explain the difference between

00:33:23,980 --> 00:33:28,990
supervised machine learning unsupervised

00:33:26,320 --> 00:33:30,100
machine learning and neural networks I'm

00:33:28,990 --> 00:33:32,920
not going to explain your networks

00:33:30,100 --> 00:33:37,930
that's a black art but supervised and

00:33:32,920 --> 00:33:42,120
unsupervised yes okay basically the idea

00:33:37,930 --> 00:33:44,350
is if I've gone through some data and

00:33:42,120 --> 00:33:46,240
identified them so if I've looked at

00:33:44,350 --> 00:33:48,310
these tags and give it a number

00:33:46,240 --> 00:33:49,570
according to what it looks like I'm I'm

00:33:48,310 --> 00:33:51,640
well able

00:33:49,570 --> 00:33:53,340
to look at the tags individually and say

00:33:51,640 --> 00:33:55,270
yes that's that one and that's that one

00:33:53,340 --> 00:33:58,330
that would be supervised machine

00:33:55,270 --> 00:34:01,060
learning unsupervised machine learning

00:33:58,330 --> 00:34:02,530
is like clustering you're not giving the

00:34:01,060 --> 00:34:05,410
machine learning algorithm any

00:34:02,530 --> 00:34:07,270
information about which is which it's

00:34:05,410 --> 00:34:12,150
looking at the variability in the data

00:34:07,270 --> 00:34:17,530
and what features move together to group

00:34:12,150 --> 00:34:21,430
and we'll show you some some figures of

00:34:17,530 --> 00:34:23,410
grouping of data but basically our

00:34:21,430 --> 00:34:25,270
problem today is supervised machine

00:34:23,410 --> 00:34:27,670
learning we do have a set of answers but

00:34:25,270 --> 00:34:30,250
we're interested in all the rest of the

00:34:27,670 --> 00:34:31,900
data that we haven't labeled by hand we

00:34:30,250 --> 00:34:34,680
want the machine learning algorithm to

00:34:31,900 --> 00:34:44,050
predict which group they belong into

00:34:34,680 --> 00:34:47,290
based on that information so we need

00:34:44,050 --> 00:34:49,990
some data to train on and then we need

00:34:47,290 --> 00:34:51,700
some data that we haven't trained on to

00:34:49,990 --> 00:34:57,400
test the accuracy of our machine

00:34:51,700 --> 00:35:00,880
learning algorithm scikit-learn makes

00:34:57,400 --> 00:35:07,420
this process really easy with the Train

00:35:00,880 --> 00:35:10,870
test split function if I was just to

00:35:07,420 --> 00:35:14,590
take 80% of the data for my training set

00:35:10,870 --> 00:35:17,620
leave 20% of the data as my testing set

00:35:14,590 --> 00:35:20,710
later on you saw that the labels are in

00:35:17,620 --> 00:35:23,200
sequence if I just take the first 80%

00:35:20,710 --> 00:35:27,390
I'm going to end up with a whole lot of

00:35:23,200 --> 00:35:29,980
the control group maybe a couple of the

00:35:27,390 --> 00:35:32,500
the testing the whole load of the

00:35:29,980 --> 00:35:35,200
control group some of the experimental

00:35:32,500 --> 00:35:38,350
group and no Queen so I'm going to be

00:35:35,200 --> 00:35:43,300
training on data that is all one type so

00:35:38,350 --> 00:35:45,130
the train test split function make sure

00:35:43,300 --> 00:35:47,560
it's a selection of all the different

00:35:45,130 --> 00:35:49,930
types of images that are in the data set

00:35:47,560 --> 00:35:55,810
and it does it very easily we don't have

00:35:49,930 --> 00:35:58,390
to write a lot of code to to get a good

00:35:55,810 --> 00:36:02,310
mix of each group in the training and

00:35:58,390 --> 00:36:04,170
testing sets so that once we've trained

00:36:02,310 --> 00:36:06,030
we'll get it we'll get a good

00:36:04,170 --> 00:36:28,040
representation of how accurate that

00:36:06,030 --> 00:36:28,040
model is when we test it just trying to

00:36:30,770 --> 00:36:38,460
that that last bit is random underscore

00:36:33,930 --> 00:36:41,040
state equals four yes random underscore

00:36:38,460 --> 00:36:42,600
so you could for that's just so we can

00:36:41,040 --> 00:36:44,790
repeat this process and get the same

00:36:42,600 --> 00:36:46,530
answers it's a random process if we

00:36:44,790 --> 00:36:52,020
didn't set a random state we wouldn't be

00:36:46,530 --> 00:36:54,570
able to repeat it and we're saying we

00:36:52,020 --> 00:36:56,730
want 80% of the data in our training

00:36:54,570 --> 00:37:03,510
group we want to leave 20% of the data

00:36:56,730 --> 00:37:09,960
over to test on so you can see there X

00:37:03,510 --> 00:37:11,970
train shape 584 observations 576

00:37:09,960 --> 00:37:14,910
features when we laid the image out and

00:37:11,970 --> 00:37:19,890
end we ended up with five hundred and

00:37:14,910 --> 00:37:24,840
seventy six columns 584 labels to go

00:37:19,890 --> 00:37:27,210
with those 584 training examples in the

00:37:24,840 --> 00:37:30,090
testing set that's 20% hundred and forty

00:37:27,210 --> 00:37:31,560
six rows same amount of features has to

00:37:30,090 --> 00:37:33,750
be the same amount of features in the

00:37:31,560 --> 00:37:39,000
same order so for the training and the

00:37:33,750 --> 00:37:41,580
testing set does that make sense it has

00:37:39,000 --> 00:37:43,710
to be the same type of data in the same

00:37:41,580 --> 00:37:45,060
order or else the machine learning

00:37:43,710 --> 00:37:49,710
algorithm isn't going to be able to make

00:37:45,060 --> 00:37:53,630
sense of it and then finally a hundred

00:37:49,710 --> 00:37:53,630
and forty six labels for the testing set

00:37:58,340 --> 00:38:10,680
just listening for ketose yeah there's

00:38:06,420 --> 00:38:13,980
there's no rule it does depend on the

00:38:10,680 --> 00:38:17,580
type of problem you've got we've got a

00:38:13,980 --> 00:38:21,390
problem where the groups are relatively

00:38:17,580 --> 00:38:25,860
even you might have another problem

00:38:21,390 --> 00:38:28,980
where you're you're looking at intrusion

00:38:25,860 --> 00:38:33,900
detection or cancer detection where the

00:38:28,980 --> 00:38:36,650
positive is very very rare so this is

00:38:33,900 --> 00:38:39,060
very common an 80/20 split you know

00:38:36,650 --> 00:38:45,900
given a normal problem I don't know if

00:38:39,060 --> 00:38:47,790
there's any normal problems but it

00:38:45,900 --> 00:38:50,130
depends on how much data you've got if

00:38:47,790 --> 00:38:52,850
you've got all the data in the world you

00:38:50,130 --> 00:38:56,610
know it's like a bigger testing set

00:38:52,850 --> 00:38:58,740
would improve things but generally you

00:38:56,610 --> 00:39:00,830
haven't haven't got as much data as you

00:38:58,740 --> 00:39:04,650
want you've got a limited set of data

00:39:00,830 --> 00:39:06,330
and you want a decent amount of data to

00:39:04,650 --> 00:39:09,390
train the model on so that you get an

00:39:06,330 --> 00:39:11,340
idea of the variability so yeah it's a

00:39:09,390 --> 00:39:13,650
bit of a compromise between having

00:39:11,340 --> 00:39:16,080
enough data to train well but still

00:39:13,650 --> 00:39:19,070
having enough testing data to check how

00:39:16,080 --> 00:39:19,070
accurate your model is

00:39:32,460 --> 00:39:38,369
okay principal components analysis this

00:39:35,830 --> 00:39:41,800
is a type of clustering so a type of

00:39:38,369 --> 00:39:44,080
unsupervised machine learning we do have

00:39:41,800 --> 00:39:46,540
the labels this is a supervised machine

00:39:44,080 --> 00:39:49,270
learning problem but in this step we're

00:39:46,540 --> 00:39:51,480
not using the labels we're just going to

00:39:49,270 --> 00:39:55,330
get the principal components analysis

00:39:51,480 --> 00:39:59,260
algorithm to cluster the data and graph

00:39:55,330 --> 00:40:07,480
it for you to show you how it splits up

00:39:59,260 --> 00:40:10,750
how different these groups are so we

00:40:07,480 --> 00:40:18,130
important algorithm we instantiate the

00:40:10,750 --> 00:40:19,660
algorithm we fit the algorithm on the

00:40:18,130 --> 00:40:21,430
training data and you'll notice that

00:40:19,660 --> 00:40:23,680
there's a big X representing the

00:40:21,430 --> 00:40:24,430
training data so this is a convention in

00:40:23,680 --> 00:40:29,800
scikit-learn

00:40:24,430 --> 00:40:32,260
a big X for the training data and then

00:40:29,800 --> 00:40:34,510
we transform X you can do the fit

00:40:32,260 --> 00:40:35,680
transform in one step but we're showing

00:40:34,510 --> 00:40:43,210
you there that they're actually two

00:40:35,680 --> 00:40:45,930
steps and then we're just going to plot

00:40:43,210 --> 00:40:45,930
that clustering

00:40:57,800 --> 00:41:08,099
so you can see half a figure there which

00:41:01,140 --> 00:41:11,250
is great so what we've done is plot the

00:41:08,099 --> 00:41:13,530
data on the X and y axis and color it

00:41:11,250 --> 00:41:15,150
according to the label the little Y is

00:41:13,530 --> 00:41:17,339
the label so this is part of this

00:41:15,150 --> 00:41:21,960
convention of big X is the training data

00:41:17,339 --> 00:41:25,890
little Y as the labels and a fixed size

00:41:21,960 --> 00:41:27,839
and you can see there are three groups

00:41:25,890 --> 00:41:31,589
well we've colored them according to

00:41:27,839 --> 00:41:35,280
their group the red ones here seem to be

00:41:31,589 --> 00:41:37,589
sitting out nicely on their own the

00:41:35,280 --> 00:41:40,200
green in the blue ones there's a bit of

00:41:37,589 --> 00:41:43,770
overlap this is actually a three

00:41:40,200 --> 00:41:46,980
dimensional space and if we rotated it

00:41:43,770 --> 00:41:49,530
you might see that the groups are

00:41:46,980 --> 00:41:51,990
separated on a particular axis but not

00:41:49,530 --> 00:41:54,810
this one we're looking at so if I was to

00:41:51,990 --> 00:41:57,119
rotate that to the right you might see

00:41:54,810 --> 00:42:02,940
that there is a bit of space between the

00:41:57,119 --> 00:42:07,530
green and blue groups but that's using

00:42:02,940 --> 00:42:09,780
an unsupervised algorithm where

00:42:07,530 --> 00:42:12,480
scikit-learn is just looking at the

00:42:09,780 --> 00:42:15,630
variability in the features and seeing

00:42:12,480 --> 00:42:16,800
which one's moved together I'm not

00:42:15,630 --> 00:42:18,660
really going to have a go at explaining

00:42:16,800 --> 00:42:21,869
how principal components analysis works

00:42:18,660 --> 00:42:23,400
I have done an undergraduate psych and

00:42:21,869 --> 00:42:24,200
my statistics teacher would be ashamed

00:42:23,400 --> 00:42:28,490
of me

00:42:24,200 --> 00:42:31,260
but we can just treat it as a black box

00:42:28,490 --> 00:42:35,670
principal components analysis is the

00:42:31,260 --> 00:42:37,680
process look that up on Wikipedia who

00:42:35,670 --> 00:42:40,190
would give you the explanation that I'm

00:42:37,680 --> 00:42:40,190
going to give you

00:42:50,380 --> 00:42:57,110
so now we're going to try and visualize

00:42:53,870 --> 00:42:59,600
with another type of algorithm linear

00:42:57,110 --> 00:43:02,240
discriminant analysis this is using the

00:42:59,600 --> 00:43:07,640
label so it's a type of supervised

00:43:02,240 --> 00:43:14,270
machine learning and again it's the same

00:43:07,640 --> 00:43:19,630
process so the scikit-learn process yep

00:43:14,270 --> 00:43:22,550
I kind of expected that this process of

00:43:19,630 --> 00:43:24,980
import the algorithm Stan she ate the

00:43:22,550 --> 00:43:28,550
algorithm fit the algorithm to the data

00:43:24,980 --> 00:43:30,830
and then make predictions or we're going

00:43:28,550 --> 00:43:33,680
to display it this sum this process is

00:43:30,830 --> 00:43:36,380
what makes SK learn really easy to use

00:43:33,680 --> 00:43:38,450
it's the same procedure no matter what

00:43:36,380 --> 00:43:40,610
type of algorithm you're using so your

00:43:38,450 --> 00:43:44,870
workflow is these four steps all the

00:43:40,610 --> 00:43:46,760
time of course if you knew more about

00:43:44,870 --> 00:43:47,930
the algorithms and what they're suitable

00:43:46,760 --> 00:43:50,210
for and what they're not suitable for

00:43:47,930 --> 00:43:52,100
that's going to make your performance

00:43:50,210 --> 00:43:53,720
better but you can't really there's a

00:43:52,100 --> 00:43:58,000
bit of a black box and just plug your

00:43:53,720 --> 00:43:58,000
algorithm in to that spot there

00:44:07,210 --> 00:44:27,230
down here really nicely separated these

00:44:11,480 --> 00:44:29,150
are the red and green groups ah let me

00:44:27,230 --> 00:44:30,650
show you all the images right so that

00:44:29,150 --> 00:44:37,040
was the the contrast that went over the

00:44:30,650 --> 00:44:40,750
limits here is the one previous the

00:44:37,040 --> 00:44:45,190
principal components analysis and

00:44:40,750 --> 00:44:45,190
there's the linear discriminant analysis

00:44:45,700 --> 00:45:01,670
so a good cat swinging space between the

00:44:48,890 --> 00:45:03,710
groups there so here's a diagram of what

00:45:01,670 --> 00:45:19,580
linear discriminant analysis is trying

00:45:03,710 --> 00:45:21,440
to do it's sorry that's a really bad

00:45:19,580 --> 00:45:24,619
diagram of what linear discriminant

00:45:21,440 --> 00:45:33,320
analysis is that's really a support

00:45:24,619 --> 00:45:35,690
vector machine diagram linear

00:45:33,320 --> 00:45:38,540
discriminant analysis is trying to

00:45:35,690 --> 00:45:44,570
rotate the data so it can put a line

00:45:38,540 --> 00:45:48,020
between the different groups and best

00:45:44,570 --> 00:45:50,720
differentiate the groups that was really

00:45:48,020 --> 00:45:57,380
really hand wavy that was like really

00:45:50,720 --> 00:46:03,589
hand wavy it's magic yes it's a magic

00:45:57,380 --> 00:46:11,089
black box okay so I think we can go back

00:46:03,589 --> 00:46:13,349
to that was a bad idea so

00:46:11,089 --> 00:46:17,670
now that we've had a look at what these

00:46:13,349 --> 00:46:20,549
algorithms are trying to do let's train

00:46:17,670 --> 00:46:22,769
a model and make some predictions

00:46:20,549 --> 00:46:25,769
on the data that we've left aside that

00:46:22,769 --> 00:46:29,219
we haven't given the algorithm access to

00:46:25,769 --> 00:46:31,229
so it can learn from that we've we've

00:46:29,219 --> 00:46:33,869
given it the training data set that's

00:46:31,229 --> 00:46:36,089
the information that it's got and then

00:46:33,869 --> 00:46:39,449
we're going to try and make predictions

00:46:36,089 --> 00:46:42,239
on the data that we left over to see how

00:46:39,449 --> 00:46:44,729
well it goes to see if if there's enough

00:46:42,239 --> 00:46:46,979
information in the training data set for

00:46:44,729 --> 00:46:48,839
it to make predictions on data that it

00:46:46,979 --> 00:46:51,749
hasn't seen before because this will be

00:46:48,839 --> 00:46:53,940
the problem we'll hand label a certain

00:46:51,749 --> 00:46:56,160
number of images but then we want the

00:46:53,940 --> 00:46:58,650
machine learning algorithm to learn the

00:46:56,160 --> 00:47:01,589
relationship between the features and be

00:46:58,650 --> 00:47:05,449
accurate at predicting the thousands and

00:47:01,589 --> 00:47:05,449
thousands of images that it hasn't seen

00:47:11,509 --> 00:47:17,819
so we're going to import a support

00:47:13,799 --> 00:47:22,769
vector machine we're going to

00:47:17,819 --> 00:47:26,039
instantiate that classifier CLF we're

00:47:22,769 --> 00:47:29,160
going to fit the classifier to the

00:47:26,039 --> 00:47:33,029
training data big X train and the labels

00:47:29,160 --> 00:47:35,969
that go with that little why train so we

00:47:33,029 --> 00:47:39,329
have the image data in X underscore

00:47:35,969 --> 00:47:42,140
train and we have the labels in Y

00:47:39,329 --> 00:47:42,140
underscore train

00:47:49,850 --> 00:47:56,500
and then to get an idea of how accurate

00:47:52,250 --> 00:47:56,500
that model is going to be on unseen data

00:48:01,900 --> 00:48:07,300
we're going to transform the X test data

00:48:13,300 --> 00:48:21,650
did we re loading the oh no there's the

00:48:19,010 --> 00:48:24,140
line for the metrics there so we're

00:48:21,650 --> 00:48:28,100
going to transform the X test data the

00:48:24,140 --> 00:48:30,680
unseen data and then we're going to run

00:48:28,100 --> 00:48:33,050
a prediction on that data so we've

00:48:30,680 --> 00:48:35,870
trained the model and now it's going to

00:48:33,050 --> 00:48:41,120
have a go at predicting the label of

00:48:35,870 --> 00:48:44,200
each of the testing rows and SK learn

00:48:41,120 --> 00:48:47,780
also has built into it

00:48:44,200 --> 00:48:50,180
methods for testing the accuracy of the

00:48:47,780 --> 00:48:53,000
prediction we're going to use a very

00:48:50,180 --> 00:48:56,180
simple measure of accuracy here the

00:48:53,000 --> 00:49:00,470
accuracy score it's just the percent

00:48:56,180 --> 00:49:02,570
that it got right so the the most basic

00:49:00,470 --> 00:49:04,940
accuracy measure you can get which is

00:49:02,570 --> 00:49:08,870
suitable for our problem because we have

00:49:04,940 --> 00:49:12,700
examples from each class not suitable

00:49:08,870 --> 00:49:15,980
for all problems accuracy testing is a

00:49:12,700 --> 00:49:17,810
huge area in its own right different

00:49:15,980 --> 00:49:19,640
problems require different accuracy

00:49:17,810 --> 00:49:27,280
measures but this one's suitable for

00:49:19,640 --> 00:49:32,770
what we're doing and 0.69% correct

00:49:27,280 --> 00:49:38,300
better than a coin toss but not great so

00:49:32,770 --> 00:49:41,180
we're comparing this score so 0.69

00:49:38,300 --> 00:49:44,180
correct with the accuracy we could have

00:49:41,180 --> 00:49:47,240
got if we just used the most common

00:49:44,180 --> 00:49:48,770
label so if I just looked what's the

00:49:47,240 --> 00:49:52,610
most common label and I just said well

00:49:48,770 --> 00:49:56,150
they're all one that would be a baseline

00:49:52,610 --> 00:49:59,590
accuracy measure and this is this is a

00:49:56,150 --> 00:49:59,590
bit better than that but not much

00:50:02,090 --> 00:50:08,760
so we've got some time now for you to if

00:50:07,590 --> 00:50:11,490
you've been following along with the

00:50:08,760 --> 00:50:15,600
code because there's this standard

00:50:11,490 --> 00:50:20,010
process of no because this is standard

00:50:15,600 --> 00:50:23,250
process of select a model train the

00:50:20,010 --> 00:50:25,620
model on the the training data and then

00:50:23,250 --> 00:50:27,630
test it on the testing data you can

00:50:25,620 --> 00:50:31,410
replace that algorithm with whatever

00:50:27,630 --> 00:50:33,390
else you like in the Esk learn library

00:50:31,410 --> 00:50:35,520
it takes standard inputs and give

00:50:33,390 --> 00:50:55,890
standard outputs so if you're following

00:50:35,520 --> 00:50:57,780
along with the code has anybody got any

00:50:55,890 --> 00:51:00,660
questions that I might be able to answer

00:50:57,780 --> 00:51:02,160
while Sams Sams going to show you some

00:51:00,660 --> 00:51:06,590
fancy stuff that was that was a very

00:51:02,160 --> 00:51:08,460
single yeah before we get to that though

00:51:06,590 --> 00:51:11,040
fizzled around have a look at the

00:51:08,460 --> 00:51:12,720
circuit learn documentation find a

00:51:11,040 --> 00:51:14,520
different classifier and plug it in

00:51:12,720 --> 00:51:17,220
instead of SVM and see what happens

00:51:14,520 --> 00:51:18,690
so the the take-home message about this

00:51:17,220 --> 00:51:20,100
part is that you only need to know three

00:51:18,690 --> 00:51:21,450
things to you you only need to know a

00:51:20,100 --> 00:51:23,850
handful of things to use scikit-learn

00:51:21,450 --> 00:51:26,100
classifiers you instantiate an object

00:51:23,850 --> 00:51:27,570
that instantiation doesn't actually do

00:51:26,100 --> 00:51:29,550
anything excel for except set some

00:51:27,570 --> 00:51:31,860
parameters you call fit to feed your

00:51:29,550 --> 00:51:34,020
data in your call predict or transform

00:51:31,860 --> 00:51:35,940
to predict some values or transform some

00:51:34,020 --> 00:51:37,350
values they all have the same methods

00:51:35,940 --> 00:51:39,510
and you interact with them all the same

00:51:37,350 --> 00:51:43,800
way regardless of what the actual object

00:51:39,510 --> 00:51:47,940
is so just play around drop in a random

00:51:43,800 --> 00:51:50,460
forest classifier or linear regression

00:51:47,940 --> 00:51:52,890
sorry logistic regression or something

00:51:50,460 --> 00:51:58,890
naive Bayes yes if you want to devise in

00:51:52,890 --> 00:52:03,000
statistics yet one line here you go play

00:51:58,890 --> 00:52:04,110
around answer those questions and then

00:52:03,000 --> 00:52:06,780
I'll move on to the next step where

00:52:04,110 --> 00:52:08,220
actually amazingly we are doing Lofa

00:52:06,780 --> 00:52:09,619
time which has never happened to me

00:52:08,220 --> 00:52:12,549
before so

00:52:09,619 --> 00:52:15,319
I'm sure I'll ruin it later don't worry

00:52:12,549 --> 00:52:29,480
any questions like I'm sure there's a

00:52:15,319 --> 00:52:30,529
whole range of different yes yeah it's

00:52:29,480 --> 00:52:34,059
not a good number

00:52:30,529 --> 00:52:38,029
machine learning can regularly get up

00:52:34,059 --> 00:52:41,329
above 85 in the 90s you know that that's

00:52:38,029 --> 00:52:43,039
we'd be smiling if we saw that I would

00:52:41,329 --> 00:52:47,900
modify that though it really depends on

00:52:43,039 --> 00:52:51,499
your problem it does if if you get 69%

00:52:47,900 --> 00:52:53,480
but a human does 40% that's amazing it's

00:52:51,499 --> 00:53:11,839
always data set specific and problems

00:52:53,480 --> 00:53:13,670
specific yes and this is one way of

00:53:11,839 --> 00:53:15,589
looking at accuracy like this is just a

00:53:13,670 --> 00:53:18,589
how many to get right compared to how

00:53:15,589 --> 00:53:22,450
many it got wrong there other ways to

00:53:18,589 --> 00:53:25,369
look at it like when it misclassified

00:53:22,450 --> 00:53:31,900
did it miss classify in a particular way

00:53:25,369 --> 00:53:34,880
like a a confusion matrix would tell you

00:53:31,900 --> 00:53:37,339
is it able to identify the Queens really

00:53:34,880 --> 00:53:40,900
well but gets mixed up between the

00:53:37,339 --> 00:53:40,900
experimental group and the control group

00:53:46,240 --> 00:53:57,820
once once you've he's when I prepared

00:53:55,580 --> 00:53:57,820
earlier

00:54:04,960 --> 00:54:12,080
so this is another way to look look at

00:54:07,640 --> 00:54:14,300
this particular problem so this the

00:54:12,080 --> 00:54:16,190
accuracy is all of the diagonals so if

00:54:14,300 --> 00:54:19,010
it's on a diagonal its correctly

00:54:16,190 --> 00:54:21,050
classified 24 of the Queen's 49 of the

00:54:19,010 --> 00:54:23,090
treatment 29 of the control group all

00:54:21,050 --> 00:54:26,000
the off-axis ones are errors and there's

00:54:23,090 --> 00:54:27,920
a few systematic errors where the

00:54:26,000 --> 00:54:31,280
control group is often confused for a

00:54:27,920 --> 00:54:32,780
queen so this is another way to look at

00:54:31,280 --> 00:54:35,510
it we probably shouldn't get too bogged

00:54:32,780 --> 00:54:37,280
down in it because there are many

00:54:35,510 --> 00:54:38,930
interesting and wonderful ways to

00:54:37,280 --> 00:54:40,280
measure it and it all depends on what

00:54:38,930 --> 00:54:43,970
your actual problem you're trying to

00:54:40,280 --> 00:54:45,560
solve is so for this particular case 69

00:54:43,970 --> 00:54:47,900
percent accuracy probably isn't good

00:54:45,560 --> 00:54:49,730
enough I'm pretty sure Jack's PhD is

00:54:47,900 --> 00:54:51,680
using something that's significantly

00:54:49,730 --> 00:54:52,850
more accurate and we'll show you in a

00:54:51,680 --> 00:54:54,320
bit that I think you can do some

00:54:52,850 --> 00:54:55,820
fiddling or you might find yourself

00:54:54,320 --> 00:54:57,140
playing around with some other

00:54:55,820 --> 00:55:00,740
classifiers that you'll get better

00:54:57,140 --> 00:55:03,470
performance straight away so just maybe

00:55:00,740 --> 00:55:05,150
take three or four minutes just fiddle

00:55:03,470 --> 00:55:06,710
around with the classifiers and we'll

00:55:05,150 --> 00:55:08,060
come around and answer any individual

00:55:06,710 --> 00:55:10,190
questions you'll have and we'll and then

00:55:08,060 --> 00:55:11,600
we'll regroup and move on to some of the

00:55:10,190 --> 00:55:14,300
more interesting parts of scikit-learn

00:55:11,600 --> 00:55:17,780
API in terms of making this easier and

00:55:14,300 --> 00:55:34,100
manageable so three or four minutes

00:55:17,780 --> 00:55:35,660
fiddle around we'll be back so the

00:55:34,100 --> 00:55:39,160
question is what are we actually trying

00:55:35,660 --> 00:55:39,160
to do why we're using machine learning

00:55:40,240 --> 00:55:45,350
we're just trying to label the B's

00:55:42,500 --> 00:55:47,270
there's there's too many thousands of

00:55:45,350 --> 00:55:52,580
well not thousand to many hundreds of

00:55:47,270 --> 00:55:54,830
hours of video yep and the length of

00:55:52,580 --> 00:55:58,660
time that the experiment runs over so it

00:55:54,830 --> 00:55:58,660
runs over weeks continuously videoed

00:55:59,500 --> 00:56:06,619
labeling each tag by hand it's not

00:56:03,500 --> 00:56:09,080
possible so we need machine learning

00:56:06,619 --> 00:56:11,630
here but we need a reasonable level of

00:56:09,080 --> 00:56:16,160
accuracy if it's not accurate of no use

00:56:11,630 --> 00:56:18,200
yeah well that's the idea

00:56:16,160 --> 00:56:20,359
yeah can we actually show that but we'd

00:56:18,200 --> 00:56:23,119
have to we have to know which B was

00:56:20,359 --> 00:56:25,340
which B and then get a measure of that

00:56:23,119 --> 00:56:27,680
you know the next step would be did they

00:56:25,340 --> 00:56:30,160
move more yeah did it have an effect on

00:56:27,680 --> 00:56:30,160
the quaint

01:02:46,040 --> 01:02:51,569
all right so I know a few people have

01:02:48,960 --> 01:03:04,380
tried a few things out let's just throw

01:02:51,569 --> 01:03:08,000
one in here so you can see an example so

01:03:04,380 --> 01:03:08,000
somebody did a random forest classifier

01:03:08,450 --> 01:03:17,160
so for armed scikit-learn which is in

01:03:15,390 --> 01:03:20,040
ensembl you don't need to worry about

01:03:17,160 --> 01:03:24,750
what ensembles are I do need to remember

01:03:20,040 --> 01:03:29,940
what the syntax is though so we can say

01:03:24,750 --> 01:03:32,130
COF equals and first classifier all of

01:03:29,940 --> 01:03:34,200
the scikit-learn models they have

01:03:32,130 --> 01:03:35,819
default parameter set they may not be

01:03:34,200 --> 01:03:37,470
good defaults but they have them set so

01:03:35,819 --> 01:03:42,470
you don't need to feed in and anything

01:03:37,470 --> 01:03:42,470
to start with now I'm just gonna fit in

01:03:44,059 --> 01:04:04,049
our OD a transformed X data oh and our

01:03:52,260 --> 01:04:05,579
training labels and I'm just gonna steal

01:04:04,049 --> 01:04:14,280
this code and run it again and we can

01:04:05,579 --> 01:04:16,950
see what happens so just by switching

01:04:14,280 --> 01:04:18,869
from a support vector machine to a

01:04:16,950 --> 01:04:21,089
render burroughs classifier we got eight

01:04:18,869 --> 01:04:23,010
percent performance improvement we're

01:04:21,089 --> 01:04:24,569
also no longer misclassifying all the

01:04:23,010 --> 01:04:26,099
Queen images which might be important

01:04:24,569 --> 01:04:28,079
for our particular problem because maybe

01:04:26,099 --> 01:04:31,470
we really care about the Queen I bet

01:04:28,079 --> 01:04:32,819
it's in many problems one class is

01:04:31,470 --> 01:04:34,710
really other interesting all the other

01:04:32,819 --> 01:04:39,119
stuff isn't so interesting so if we were

01:04:34,710 --> 01:04:40,410
doing a project on Queen B behaviour how

01:04:39,119 --> 01:04:41,609
well we do on the queens is probably the

01:04:40,410 --> 01:04:44,780
most important thing and we build a

01:04:41,609 --> 01:04:48,089
scoring method around that so for this

01:04:44,780 --> 01:04:51,180
last section we're going to look at a

01:04:48,089 --> 01:04:52,829
little bit more of scikit-learn API and

01:04:51,180 --> 01:04:55,410
how scikit-learn makes it actually

01:04:52,829 --> 01:04:56,880
easier to deal with all of this stuff so

01:04:55,410 --> 01:04:58,589
up to this point we've been doing a lot

01:04:56,880 --> 01:04:59,790
of manual data manipulation so we've had

01:04:58,589 --> 01:05:01,820
some data we've Rishi

01:04:59,790 --> 01:05:04,500
we've had a bunch of temporary variables

01:05:01,820 --> 01:05:06,870
we've had X transform we've had X we've

01:05:04,500 --> 01:05:09,120
had X test we've had Xtreme now we're

01:05:06,870 --> 01:05:10,380
gonna we're going to use circuit learns

01:05:09,120 --> 01:05:12,500
API and we're going to make it all work

01:05:10,380 --> 01:05:15,450
nicely together and in a way that is

01:05:12,500 --> 01:05:17,700
more easily maintainable easier to think

01:05:15,450 --> 01:05:19,740
about easier to reason about easier to

01:05:17,700 --> 01:05:21,920
collect all of the logic of your program

01:05:19,740 --> 01:05:21,920
together

01:05:22,370 --> 01:05:43,530
so we're going to build a pipeline we

01:05:34,320 --> 01:05:47,870
are look too far sorry so a pipeline

01:05:43,530 --> 01:05:50,340
lets us take a bunch of scikit-learn or

01:05:47,870 --> 01:05:53,550
comparable objects and we'll get to the

01:05:50,340 --> 01:05:55,770
compatibility part next so we

01:05:53,550 --> 01:05:59,100
instantiate so we can replace all of our

01:05:55,770 --> 01:06:01,140
previous process with a pipeline so we

01:05:59,100 --> 01:06:02,460
have one part which is the LDA component

01:06:01,140 --> 01:06:04,650
so we're just going to construct the LDA

01:06:02,460 --> 01:06:07,760
transformer object we're going to

01:06:04,650 --> 01:06:10,680
construct our support vector classifier

01:06:07,760 --> 01:06:12,000
object we're not doing it note that

01:06:10,680 --> 01:06:15,120
we're not fitting or transforming or

01:06:12,000 --> 01:06:16,650
predicting or anything at this point now

01:06:15,120 --> 01:06:18,200
we're going to build from those two

01:06:16,650 --> 01:06:21,630
objects we're going to build a pipeline

01:06:18,200 --> 01:06:23,850
so a scikit-learn pipeline object is

01:06:21,630 --> 01:06:26,220
itself a classifier so we're going to

01:06:23,850 --> 01:06:28,020
take all of the components of our models

01:06:26,220 --> 01:06:30,870
and stick them together into one

01:06:28,020 --> 01:06:32,520
convenient object so instead of

01:06:30,870 --> 01:06:35,010
reasoning about all of the components of

01:06:32,520 --> 01:06:39,660
our process we just need to worry about

01:06:35,010 --> 01:06:43,140
the pipeline object and to fit predict

01:06:39,660 --> 01:06:44,640
and so on we don't need to worry about

01:06:43,140 --> 01:06:46,500
the individual components we just call

01:06:44,640 --> 01:06:48,690
pipeline got fit and that will start at

01:06:46,500 --> 01:06:50,310
the beginning of our pipeline call fit

01:06:48,690 --> 01:06:51,900
and transform on that part and move it

01:06:50,310 --> 01:06:53,700
onto the next part call fit and

01:06:51,900 --> 01:06:58,950
transform all the way to the end so we

01:06:53,700 --> 01:07:03,420
wrap our complex many stage process into

01:06:58,950 --> 01:07:05,820
a single convenient object and what

01:07:03,420 --> 01:07:08,250
we're doing here only has three steps

01:07:05,820 --> 01:07:11,970
we've got pre-process the data into the

01:07:08,250 --> 01:07:13,380
right shape run OD a on it run a

01:07:11,970 --> 01:07:15,570
classifier on it we

01:07:13,380 --> 01:07:17,400
a real system you might have five or ten

01:07:15,570 --> 01:07:19,260
like if you're doing text analytics you

01:07:17,400 --> 01:07:21,390
have to do a bunch of extra steps you

01:07:19,260 --> 01:07:23,880
can't just take the raw pixel values so

01:07:21,390 --> 01:07:26,250
pipeline object lets you collect all the

01:07:23,880 --> 01:07:30,150
pieces stick them into one convenient

01:07:26,250 --> 01:07:37,140
object and reason on them like that so

01:07:30,150 --> 01:07:40,400
what we do here so the syntax for the

01:07:37,140 --> 01:07:43,650
pipeline constructor it's a list of

01:07:40,400 --> 01:07:44,970
tuples or lists or whatever the

01:07:43,650 --> 01:07:47,460
important part is that you give that

01:07:44,970 --> 01:07:49,080
stage of the pipeline a name and then

01:07:47,460 --> 01:07:50,940
you give it the object at that stage in

01:07:49,080 --> 01:07:52,380
the pipeline so the first thing is the

01:07:50,940 --> 01:07:54,180
first step in the pipeline which is the

01:07:52,380 --> 01:07:57,210
ODA the second thing is the classifier

01:07:54,180 --> 01:07:58,680
and then a the names are important for a

01:07:57,210 --> 01:08:00,270
reason we'll come back to later because

01:07:58,680 --> 01:08:01,500
if you want to inspect the components of

01:08:00,270 --> 01:08:10,370
the pipeline you need to know what the

01:08:01,500 --> 01:08:13,500
name is so if we run this section our

01:08:10,370 --> 01:08:16,290
pipeline dot fit method behaves exactly

01:08:13,500 --> 01:08:20,430
the same as if we run LV a built a new

01:08:16,290 --> 01:08:25,260
matrix run the support vector classifier

01:08:20,430 --> 01:08:28,770
dot fit and so on I've also used this

01:08:25,260 --> 01:08:31,250
convenient shortcut score method which

01:08:28,770 --> 01:08:33,150
for this classifier gives us accuracy

01:08:31,250 --> 01:08:34,740
because I'm lazy and I don't want to

01:08:33,150 --> 01:08:36,390
type those extra lines and it's nearly

01:08:34,740 --> 01:08:38,280
time for the conference so let's not

01:08:36,390 --> 01:08:40,440
just remember that score is a quick

01:08:38,280 --> 01:08:43,770
shortcut method you may not want to use

01:08:40,440 --> 01:08:47,510
it for all problems but it's there where

01:08:43,770 --> 01:08:47,510
scikit-learn becomes really good though

01:09:04,500 --> 01:09:17,679
it's not a workshop if something doesn't

01:09:06,969 --> 01:09:19,150
break so remember at the start where we

01:09:17,679 --> 01:09:22,299
said we were going to crop it and then

01:09:19,150 --> 01:09:25,719
we kind of forgot about that let's bring

01:09:22,299 --> 01:09:28,270
it back so the very first stage in our

01:09:25,719 --> 01:09:30,850
process was to take the images unwrapped

01:09:28,270 --> 01:09:31,770
them and we forgot to crop them but

01:09:30,850 --> 01:09:34,210
we'll do it now

01:09:31,770 --> 01:09:36,699
none of that process fits within

01:09:34,210 --> 01:09:38,589
scikit-learn x' default objects so by

01:09:36,699 --> 01:09:41,259
default scikit-learn every object takes

01:09:38,589 --> 01:09:44,529
an X which is a 2d array and Y which is

01:09:41,259 --> 01:09:46,509
a 1d array of labels if you want to

01:09:44,529 --> 01:09:48,609
extend your own stuff and adapt your

01:09:46,509 --> 01:09:50,140
data sources or do your pre-processing

01:09:48,609 --> 01:09:54,429
in a way that doesn't make sense in

01:09:50,140 --> 01:09:55,870
scikit-learn you can implement an object

01:09:54,429 --> 01:09:58,810
that fits in with scikit-learn x'

01:09:55,870 --> 01:10:01,030
api like this so you only need to do a

01:09:58,810 --> 01:10:04,540
handful of things you need to import

01:10:01,030 --> 01:10:06,850
those two classes they don't actually do

01:10:04,540 --> 01:10:08,920
much but they're important for all the

01:10:06,850 --> 01:10:10,449
other stuff to work like I think the

01:10:08,920 --> 01:10:12,130
base estimator just has a way to copy

01:10:10,449 --> 01:10:14,080
parameters from one classifier to

01:10:12,130 --> 01:10:17,530
another and a few other bits and the

01:10:14,080 --> 01:10:19,660
transformer mixin just has a quick

01:10:17,530 --> 01:10:23,350
method for fit transform define that's

01:10:19,660 --> 01:10:26,350
all that's all those doing all you need

01:10:23,350 --> 01:10:27,969
to do to write an object is sorry it to

01:10:26,350 --> 01:10:30,550
be compatible with the scikit-learn api

01:10:27,969 --> 01:10:34,270
so write an object that does that sorry

01:10:30,550 --> 01:10:37,810
that create a class inherits those two

01:10:34,270 --> 01:10:40,030
things your init method is limited you

01:10:37,810 --> 01:10:41,530
can only do variable assignment although

01:10:40,030 --> 01:10:42,820
you can't do any processing in that

01:10:41,530 --> 01:10:44,290
method otherwise it will break other

01:10:42,820 --> 01:10:46,300
stuff when you try and use it further on

01:10:44,290 --> 01:10:50,739
in your pipeline then all you need to do

01:10:46,300 --> 01:10:51,940
is implement fit transform or predict so

01:10:50,739 --> 01:10:53,410
you always need to fit and then

01:10:51,940 --> 01:10:54,699
transform or predict depending on

01:10:53,410 --> 01:10:57,969
whether you're doing a classifier or

01:10:54,699 --> 01:11:00,550
something else so this method sorry this

01:10:57,969 --> 01:11:02,890
class is a psychic learning compatible

01:11:00,550 --> 01:11:04,960
class we can use it at the start of our

01:11:02,890 --> 01:11:08,410
pipeline to go from our raw images in a

01:11:04,960 --> 01:11:10,030
3d array to the format scikit-learn

01:11:08,410 --> 01:11:12,190
expects and then it's just going to work

01:11:10,030 --> 01:11:15,150
transparently through it all so if we

01:11:12,190 --> 01:11:17,320
define this class

01:11:15,150 --> 01:11:22,540
we can plug it straight into our

01:11:17,320 --> 01:11:25,630
pipeline like this so now the very first

01:11:22,540 --> 01:11:29,860
method component of our pipeline is the

01:11:25,630 --> 01:11:32,770
unwrap class we just defined and that's

01:11:29,860 --> 01:11:35,260
it we now have a scikit-learn compatible

01:11:32,770 --> 01:11:38,050
class that works with all the psychic

01:11:35,260 --> 01:11:39,940
learns classifiers and transformers that

01:11:38,050 --> 01:11:41,830
we want to use and all you need to do is

01:11:39,940 --> 01:11:43,750
implement a couple of methods in inherit

01:11:41,830 --> 01:11:47,230
from some stuff that's pretty much all

01:11:43,750 --> 01:11:49,120
you need so now we have a pipeline that

01:11:47,230 --> 01:11:51,100
goes from our raw image data in the

01:11:49,120 --> 01:11:52,690
format we had it to begin with it

01:11:51,100 --> 01:11:54,460
implements all the logic of our

01:11:52,690 --> 01:11:58,120
transforming and pre-processing and so

01:11:54,460 --> 01:12:00,940
on and it also does the cropping that we

01:11:58,120 --> 01:12:03,699
forgot about and the crop pixels is a

01:12:00,940 --> 01:12:05,260
parameter to this method so if we want

01:12:03,699 --> 01:12:07,239
to experiment with different amounts of

01:12:05,260 --> 01:12:10,179
cropping we can do it at this layer in

01:12:07,239 --> 01:12:13,989
our pipeline we don't need to worry

01:12:10,179 --> 01:12:15,850
about throwing around some individual we

01:12:13,989 --> 01:12:17,170
don't need to have a bunch of temporary

01:12:15,850 --> 01:12:20,410
arrays floating around while we do our

01:12:17,170 --> 01:12:21,880
transformations manually does anybody

01:12:20,410 --> 01:12:23,290
have any questions up till there because

01:12:21,880 --> 01:12:26,820
it gets really good at the next part and

01:12:23,290 --> 01:12:26,820
I don't want any wedding to be lost or

01:12:29,130 --> 01:12:38,650
everybody's ready for dinner

01:12:30,730 --> 01:12:40,570
well either way is good so I just run

01:12:38,650 --> 01:12:47,320
this and convince you that it doesn't

01:12:40,570 --> 01:12:48,790
fail so now instead of X transform X

01:12:47,320 --> 01:12:50,770
train all that other stuff we started

01:12:48,790 --> 01:12:52,900
with our raw images which are a 3d array

01:12:50,770 --> 01:12:54,160
we can fit the pipeline all the way from

01:12:52,900 --> 01:12:55,390
beginning to end and we don't have to

01:12:54,160 --> 01:12:57,670
worry about all the intermediate steps

01:12:55,390 --> 01:12:59,080
because we've got rekted them away we

01:12:57,670 --> 01:13:02,410
can just think about what is this

01:12:59,080 --> 01:13:03,610
pipeline object doing and not worry

01:13:02,410 --> 01:13:06,780
about the details when we're actually

01:13:03,610 --> 01:13:06,780
using all of this stuff

01:13:09,780 --> 01:13:13,199
let's make it even better so you would

01:13:12,179 --> 01:13:15,329
have noticed that all of our numbers

01:13:13,199 --> 01:13:17,849
were magic numbers so we set parameters

01:13:15,329 --> 01:13:21,659
for the support vector machine C and

01:13:17,849 --> 01:13:24,119
gamma I think we set a magic number of

01:13:21,659 --> 01:13:27,329
pixels to crop out and we set an a magic

01:13:24,119 --> 01:13:29,219
number of components to our Lda using

01:13:27,329 --> 01:13:31,380
the randomized search cross-validation

01:13:29,219 --> 01:13:33,690
which you should use don't use grid

01:13:31,380 --> 01:13:40,800
search use that one you can ask me why

01:13:33,690 --> 01:13:42,270
later the parameters you choose the

01:13:40,800 --> 01:13:44,190
optimal ones they're always always

01:13:42,270 --> 01:13:45,300
always problem dependent you can't just

01:13:44,190 --> 01:13:47,369
pick them and say that's good enough

01:13:45,300 --> 01:13:49,860
well you can if it solves your problem

01:13:47,369 --> 01:13:51,210
you may not have the best parameters and

01:13:49,860 --> 01:13:52,409
most of the time you go in you'll use

01:13:51,210 --> 01:13:55,050
the default parameters and you'll get

01:13:52,409 --> 01:13:56,539
20% performance and you go oh really I'm

01:13:55,050 --> 01:13:59,369
gonna have to deal with these things

01:13:56,539 --> 01:14:06,300
this randomized search cross validation

01:13:59,369 --> 01:14:09,750
object lets you do it so see here where

01:14:06,300 --> 01:14:12,630
I've used LD a double underscore end

01:14:09,750 --> 01:14:14,670
components for this randomized search

01:14:12,630 --> 01:14:16,829
I'm telling it for the LDA component on

01:14:14,670 --> 01:14:20,550
my pipeline vary the number of

01:14:16,829 --> 01:14:23,309
components pick from those options for

01:14:20,550 --> 01:14:25,230
the unwrap component of my pipeline vary

01:14:23,309 --> 01:14:26,639
the crop pixel was variable along these

01:14:25,230 --> 01:14:28,320
particular things so we're going to try

01:14:26,639 --> 01:14:32,690
cropping between 0 and 8 pixels and

01:14:28,320 --> 01:14:32,690
let's vary C and see what happens to our

01:14:33,619 --> 01:14:42,510
performance so when we do a randomized

01:14:36,510 --> 01:14:44,429
search the cross-validation part is it

01:14:42,510 --> 01:14:49,079
splits up the data we give it into a

01:14:44,429 --> 01:14:50,940
bunch of subsets it trains on in mine if

01:14:49,079 --> 01:14:53,010
we split it up into K subsets it'll

01:14:50,940 --> 01:14:55,020
train on K minus 1 so 1 less than the

01:14:53,010 --> 01:14:56,760
amount you break it up into and test on

01:14:55,020 --> 01:14:59,099
the other one and it'll repeat for all

01:14:56,760 --> 01:15:03,300
of those combinations when you do this

01:14:59,099 --> 01:15:05,579
process it'll give you the it will

01:15:03,300 --> 01:15:07,829
choose the best combo best scoring

01:15:05,579 --> 01:15:10,949
combination of parameters from running

01:15:07,829 --> 01:15:14,610
all of that stuff as the optimal values

01:15:10,949 --> 01:15:17,400
for your particular problem so if we if

01:15:14,610 --> 01:15:19,019
we run this object it's going to give us

01:15:17,400 --> 01:15:21,420
a principled way to choose amongst all

01:15:19,019 --> 01:15:24,680
of our different bits and pieces

01:15:21,420 --> 01:15:24,680
Oh yep

01:15:33,510 --> 01:15:37,200
the default is to use stratification but

01:15:35,340 --> 01:15:41,160
we'll come back to that because there's

01:15:37,200 --> 01:15:42,900
a more important question there so if we

01:15:41,160 --> 01:15:44,850
run that it's gonna give us a whole

01:15:42,900 --> 01:15:48,050
bunch of really exciting numerical

01:15:44,850 --> 01:15:50,640
errors which I think are to do with the

01:15:48,050 --> 01:15:51,630
ova component so if you see something

01:15:50,640 --> 01:15:53,070
like this it means you need to

01:15:51,630 --> 01:15:55,470
investigate what's going on in your

01:15:53,070 --> 01:15:57,780
things because it may be numerically ill

01:15:55,470 --> 01:16:01,260
conditioned and there'll be a lot of

01:15:57,780 --> 01:16:06,870
them because I said use 20 iterations of

01:16:01,260 --> 01:16:08,190
random search to find something so every

01:16:06,870 --> 01:16:10,050
single time we tried to run this model

01:16:08,190 --> 01:16:13,830
it gave us a numerical warning so that's

01:16:10,050 --> 01:16:17,850
something to be aware of and at the end

01:16:13,830 --> 01:16:20,310
of it we have a randomized search CV

01:16:17,850 --> 01:16:22,170
object which itself has a predictor a

01:16:20,310 --> 01:16:24,840
transform method so that randomized

01:16:22,170 --> 01:16:26,940
search method CV object took our

01:16:24,840 --> 01:16:29,730
pipeline and gave us something even

01:16:26,940 --> 01:16:31,710
better so we have our whole beginning to

01:16:29,730 --> 01:16:33,270
end of our problem encapsulated in a

01:16:31,710 --> 01:16:36,000
single object all of the logic is in

01:16:33,270 --> 01:16:37,920
there and you only need to call fit once

01:16:36,000 --> 01:16:48,900
and predict whenever you need to predict

01:16:37,920 --> 01:16:51,000
later on so when we having fit that we

01:16:48,900 --> 01:16:52,800
can inspector per the attributes of the

01:16:51,000 --> 01:16:54,330
searcher object and they'll tell us what

01:16:52,800 --> 01:16:56,100
it thinks the best score is out of all

01:16:54,330 --> 01:16:58,050
of those popery option all of those

01:16:56,100 --> 01:17:00,570
options and what the best parameters

01:16:58,050 --> 01:17:04,340
were and it thinks that cropping out

01:17:00,570 --> 01:17:06,630
eight pixels was the best thing to do

01:17:04,340 --> 01:17:13,670
it's a randomized search though so you

01:17:06,630 --> 01:17:13,670
may get different answers every time now

01:17:13,730 --> 01:17:18,450
this is we're gonna start kind of

01:17:16,200 --> 01:17:23,220
wrapping up for this last section but I

01:17:18,450 --> 01:17:27,300
do have some exercises for you first one

01:17:23,220 --> 01:17:29,850
is why is what I did there completely

01:17:27,300 --> 01:17:32,570
wrong and why should you never ever do

01:17:29,850 --> 01:17:32,570
that in practice

01:17:33,480 --> 01:17:38,290
yes don't hear the test on your training

01:17:36,460 --> 01:17:40,060
set if you do that you get wonderful

01:17:38,290 --> 01:17:42,730
numbers that look like they're working

01:17:40,060 --> 01:17:48,010
and you go look it's almost perfectly

01:17:42,730 --> 01:17:49,390
accurate let's roll it out no it's

01:17:48,010 --> 01:17:52,500
really not it's really easy to trick

01:17:49,390 --> 01:17:54,940
yourself and accidentally mix stuff up I

01:17:52,500 --> 01:17:57,130
am speaking from experience I'm not just

01:17:54,940 --> 01:17:59,770
telling you it's a bad idea it's really

01:17:57,130 --> 01:18:01,660
really really easy particular if you

01:17:59,770 --> 01:18:03,340
have time series as well it's even

01:18:01,660 --> 01:18:05,170
harder because you can't mix past and

01:18:03,340 --> 01:18:09,310
future values together because that'll

01:18:05,170 --> 01:18:13,239
make it even worse so if we actually

01:18:09,310 --> 01:18:16,000
repeat this process using a proper

01:18:13,239 --> 01:18:19,360
training test split we only get 78%

01:18:16,000 --> 01:18:24,190
performance so that 96% you saw before

01:18:19,360 --> 01:18:25,989
that was a lie I mean I know I know it

01:18:24,190 --> 01:18:27,520
seems really obvious when it's pointed

01:18:25,989 --> 01:18:28,930
out and in hindsight but it's really

01:18:27,520 --> 01:18:30,960
easy to get caught up in your particular

01:18:28,930 --> 01:18:33,810
problem and you start hunting before

01:18:30,960 --> 01:18:35,950
performance and you think you found it

01:18:33,810 --> 01:18:37,090
but if you find it by doing something

01:18:35,950 --> 01:18:43,780
like this you haven't found anything

01:18:37,090 --> 01:18:45,220
you've just tricked yourself yes because

01:18:43,780 --> 01:18:46,420
green search is computationally

01:18:45,220 --> 01:18:50,680
inefficient and there's a really nice

01:18:46,420 --> 01:18:53,260
paper which is in the notes short answer

01:18:50,680 --> 01:19:00,489
is use randomized search as your

01:18:53,260 --> 01:19:02,890
baseline any more questions I'll wrap up

01:19:00,489 --> 01:19:06,430
after if there's no more questions if

01:19:02,890 --> 01:19:09,720
you remember nothing else remember don't

01:19:06,430 --> 01:19:09,720
test on your trainings

01:19:13,460 --> 01:19:17,220
not for performance because we use the

01:19:16,020 --> 01:19:19,290
cross validated if she picked the

01:19:17,220 --> 01:19:21,780
parameters we can't use the performance

01:19:19,290 --> 01:19:24,270
numbers you have to nest them if you

01:19:21,780 --> 01:19:25,530
want to do it that way but the rest if

01:19:24,270 --> 01:19:28,200
that doesn't make sense to you don't

01:19:25,530 --> 01:19:40,710
worry we're getting ahead of ourselves

01:19:28,200 --> 01:19:49,770
more questions more questions okay close

01:19:40,710 --> 01:20:01,520
it oh yes yeah yeah absolutely

01:19:49,770 --> 01:20:01,520
I like Google's API is and stuff yep I

01:20:13,700 --> 01:20:18,030
it depends on what it is if it's

01:20:16,200 --> 01:20:20,160
something like I want you to find what's

01:20:18,030 --> 01:20:22,230
in this natural image go use Google's

01:20:20,160 --> 01:20:24,930
thingy you'll never beat Google at that

01:20:22,230 --> 01:20:27,150
if it's I have a specific business

01:20:24,930 --> 01:20:29,490
problem based on my specific business

01:20:27,150 --> 01:20:30,870
data and my specific business goals I

01:20:29,490 --> 01:20:33,060
don't think that will be automated

01:20:30,870 --> 01:20:34,980
there were that parts of it are always

01:20:33,060 --> 01:20:36,390
going to get easier and better like all

01:20:34,980 --> 01:20:37,740
I was talking about with the randomized

01:20:36,390 --> 01:20:38,610
search and picking different models and

01:20:37,740 --> 01:20:41,640
all that other stuff there's already

01:20:38,610 --> 01:20:42,780
tools to automate parts of that but you

01:20:41,640 --> 01:20:43,890
can never get I don't think we'll get

01:20:42,780 --> 01:20:45,660
ever get to the point where they would

01:20:43,890 --> 01:20:46,980
just be an API you send it some data and

01:20:45,660 --> 01:20:51,180
you get back meaningful predictions

01:20:46,980 --> 01:20:53,220
except for very specific problems but

01:20:51,180 --> 01:20:56,190
that's just my opinion but maybe I'd

01:20:53,220 --> 01:20:59,580
like a job going forward so I could be

01:20:56,190 --> 01:21:03,060
biased I've got four pieces of advice

01:20:59,580 --> 01:21:05,460
for you oh there's also the workshop

01:21:03,060 --> 01:21:07,470
reach for there if you have any

01:21:05,460 --> 01:21:09,540
questions or comments or feedback you

01:21:07,470 --> 01:21:10,950
can raise an issue or things in the

01:21:09,540 --> 01:21:12,290
notes that you're looking back on and

01:21:10,950 --> 01:21:18,690
don't make sense please feel free to

01:21:12,290 --> 01:21:20,130
leave notes there in general for machine

01:21:18,690 --> 01:21:22,470
learning just because you can doesn't

01:21:20,130 --> 01:21:25,870
mean you should

01:21:22,470 --> 01:21:27,820
which I know is a not a not necessarily

01:21:25,870 --> 01:21:30,040
a well-received thing to a technical

01:21:27,820 --> 01:21:34,360
community but it's not always the right

01:21:30,040 --> 01:21:37,120
tool for your approach I'll just leave

01:21:34,360 --> 01:21:40,480
that um accuracy is not necessarily the

01:21:37,120 --> 01:21:41,890
right metric um I can say none of you

01:21:40,480 --> 01:21:44,650
have cancer and that is a really

01:21:41,890 --> 01:21:46,420
accurate statement if you did happen to

01:21:44,650 --> 01:21:52,290
have cancer that's another very useful

01:21:46,420 --> 01:21:55,390
statement I don't I rephrase that I

01:21:52,290 --> 01:21:56,350
could state that none nobody in this

01:21:55,390 --> 01:22:00,730
room has cancer

01:21:56,350 --> 01:22:02,350
I could be 99% accurate to the person

01:22:00,730 --> 01:22:05,200
who does have cancer that is not very

01:22:02,350 --> 01:22:07,300
useful because you're wrong and if

01:22:05,200 --> 01:22:09,370
getting saying someone doesn't have

01:22:07,300 --> 01:22:17,740
cancer when they do is a really serious

01:22:09,370 --> 01:22:20,650
consequence we're number three is you

01:22:17,740 --> 01:22:21,940
and your data are biased and you will

01:22:20,650 --> 01:22:25,930
never know all of the sources of the

01:22:21,940 --> 01:22:28,270
bias so don't go and build predictive

01:22:25,930 --> 01:22:32,320
models for crime based on 200 years of

01:22:28,270 --> 01:22:34,050
probably biased crime data I wish that

01:22:32,320 --> 01:22:37,660
wasn't a real thing that there you go

01:22:34,050 --> 01:22:39,550
don't predict terrorists based on 15

01:22:37,660 --> 01:22:42,910
examples across a population of 15

01:22:39,550 --> 01:22:47,940
million because well that was CIA hey

01:22:42,910 --> 01:22:50,290
that's not really a good way to do it I

01:22:47,940 --> 01:22:51,790
put that one in because neural networks

01:22:50,290 --> 01:22:53,290
on deep learning are a really good way

01:22:51,790 --> 01:22:57,060
to waste time they're a really good way

01:22:53,290 --> 01:23:01,150
to overcomplicate a problem seriously

01:22:57,060 --> 01:23:02,500
they're a rather learn you if you may

01:23:01,150 --> 01:23:06,720
find that they're useful for your

01:23:02,500 --> 01:23:08,590
problem sorry I would never start there

01:23:06,720 --> 01:23:10,630
unless you're doing something very

01:23:08,590 --> 01:23:12,790
specific about image and speech where

01:23:10,630 --> 01:23:14,920
that's the obvious way to go don't start

01:23:12,790 --> 01:23:16,810
that don't feed your business problem

01:23:14,920 --> 01:23:18,370
into a deep learning thing and expect to

01:23:16,810 --> 01:23:22,360
get magically good results that are

01:23:18,370 --> 01:23:25,320
repeatable and reliable that's it

01:23:22,360 --> 01:23:25,320

YouTube URL: https://www.youtube.com/watch?v=3cWUqd6rGaM


