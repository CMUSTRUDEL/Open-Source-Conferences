Title: Text mining online data with scikit-learn by Robert Layton
Publication date: 2014-08-18
Playlist: PyCon Australia 2014
Description: 
	Text mining has a large variety of applications and is becoming used in more businesses for gathering intelligence and providing insight. People are sending text constantly online via social media, chat rooms and blogs. Tapping into this information can help businesses gain an advantage and is increasingly a necessary skill for data analytics. Text mining is a unique data mining problem, dealing with real world data that is often heavy on artefacts, difficult to model and challenging to properly manage. Text mining can be seen as a bit of a dark art that is difficult to learn and gain traction. However some basic strategies can often be applied to get good results quite quickly, and the same basic models appear in many text mining challenges.

The scikit-learn project is a library of machine learning algorithms for the scientific python stack (numpy & scipy). It is known for having detailed documentation, a high quality of coding and a growing list of users worldwide. The documentation includes tutorials for learning machine learning as well as the library and is a great place to start for beginners wanting to learn data analytics. There is a strong focus on reusable components and useful algorithms, and the text mining sections of scikit-learn follow the “standard model” of text mining quite well.

In this presentation, we will go through the scikit-learn project for machine learning and show how to use it for text mining applications. Real world data and applications will be used, including spam detection on Twitter, predicting the author of a program and determining a user's political bent based on their social media account.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2014, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

August 1-5, Brisbane, Queensland, Australia
Captions: 
	00:00:05,779 --> 00:00:12,290
I'll start yeah hi my name is Robert

00:00:09,299 --> 00:00:16,220
Layton I'm having technical problems I

00:00:12,290 --> 00:00:18,689
got a new laptop and so it was not

00:00:16,220 --> 00:00:22,439
recognizing that there was a another

00:00:18,689 --> 00:00:24,689
screen attached at all so I'm here today

00:00:22,439 --> 00:00:30,420
to talk about text my online data with

00:00:24,689 --> 00:00:32,489
scikit-learn thank you I did turn this

00:00:30,420 --> 00:00:36,809
into a slide show but will not do that

00:00:32,489 --> 00:00:40,950
for now so that was my abstract and so

00:00:36,809 --> 00:00:42,329
who am i thanks for the intro this was I

00:00:40,950 --> 00:00:44,760
was going to talk a bit more about this

00:00:42,329 --> 00:00:46,649
but um yeah I'm a research fellow at the

00:00:44,760 --> 00:00:48,809
Federation University Australia which

00:00:46,649 --> 00:00:51,989
was the University of Ballarat and the

00:00:48,809 --> 00:00:55,980
Gibson campus campus of Monash until the

00:00:51,989 --> 00:00:57,449
first of this year there I work at the

00:00:55,980 --> 00:00:59,879
internet commerce security laboratory

00:00:57,449 --> 00:01:01,559
and basically I look at cyber cyber

00:00:59,879 --> 00:01:04,289
attacks and try and work out whether the

00:01:01,559 --> 00:01:05,489
same person did the same whether two

00:01:04,289 --> 00:01:08,520
different attacks was done by the same

00:01:05,489 --> 00:01:11,700
person a lot of what I do is working on

00:01:08,520 --> 00:01:14,220
text so my PhD for instance was on

00:01:11,700 --> 00:01:15,840
analyzing phishing websites trying to

00:01:14,220 --> 00:01:18,420
find artifacts within the source code

00:01:15,840 --> 00:01:20,069
and determining whether those artifacts

00:01:18,420 --> 00:01:23,550
mean that the same person probably wrote

00:01:20,069 --> 00:01:26,209
both of these phishing attacks so I do a

00:01:23,550 --> 00:01:29,250
lot of research with text I'm

00:01:26,209 --> 00:01:31,500
scikit-learn contributor if you've used

00:01:29,250 --> 00:01:33,479
the DV scan I wrote the first bit of

00:01:31,500 --> 00:01:37,679
that someone improved it that I wrote

00:01:33,479 --> 00:01:39,300
that first and and this year I'm

00:01:37,679 --> 00:01:42,259
mentoring a student for the google

00:01:39,300 --> 00:01:44,910
Summer of Code for scikit-learn on

00:01:42,259 --> 00:01:47,849
locality sensitive hashing so basically

00:01:44,910 --> 00:01:52,140
making a more scalable way to find

00:01:47,849 --> 00:01:57,569
nearest neighbors now this presentation

00:01:52,140 --> 00:02:00,090
I put these up here because two reasons

00:01:57,569 --> 00:02:02,009
first of all to show that everything

00:02:00,090 --> 00:02:03,929
here can be done with Python 3 so I

00:02:02,009 --> 00:02:05,429
think the days of the incubator

00:02:03,929 --> 00:02:07,860
incompatibility at least in the

00:02:05,429 --> 00:02:09,920
scientific Python stack

00:02:07,860 --> 00:02:09,920
you

00:02:11,209 --> 00:02:17,900
without documents into their words and

00:02:14,599 --> 00:02:22,549
then for each document we have a feature

00:02:17,900 --> 00:02:25,370
so such as they X would say the first

00:02:22,549 --> 00:02:29,239
column and that feature corresponds to

00:02:25,370 --> 00:02:30,769
the same word so for X I it will be the

00:02:29,239 --> 00:02:33,109
frequency of that word in the eyes

00:02:30,769 --> 00:02:35,629
document for extra will be the frequency

00:02:33,109 --> 00:02:39,889
of that same word in the Jay document

00:02:35,629 --> 00:02:42,739
and so on and the bag of words model is

00:02:39,889 --> 00:02:44,870
pretty much just that counts of how many

00:02:42,739 --> 00:02:49,430
times each word appears in each document

00:02:44,870 --> 00:02:51,709
and this is called the bag of words

00:02:49,430 --> 00:02:55,359
model because basically we get rid of

00:02:51,709 --> 00:02:58,340
any sort of grammar any sort of

00:02:55,359 --> 00:03:00,950
assumptions about how the sentences are

00:02:58,340 --> 00:03:02,510
formed or how the words relate to each

00:03:00,950 --> 00:03:04,549
other we just throw all the words into a

00:03:02,510 --> 00:03:06,290
bag and we just count them so we don't

00:03:04,549 --> 00:03:08,629
care about what order they appeared in

00:03:06,290 --> 00:03:12,769
now this is a very crude model but what

00:03:08,629 --> 00:03:16,780
we find is it works and and it works

00:03:12,769 --> 00:03:21,620
quite well for a large number of cases

00:03:16,780 --> 00:03:24,019
and psychic learned has a count

00:03:21,620 --> 00:03:26,169
vectorizer class which will do this for

00:03:24,019 --> 00:03:26,169
you

00:03:26,379 --> 00:03:36,709
the count vectorizer class by default

00:03:31,040 --> 00:03:40,189
will use word word frequencies and and

00:03:36,709 --> 00:03:45,019
give you a bag of words model so in this

00:03:40,189 --> 00:03:47,000
line here in this line here we define a

00:03:45,019 --> 00:03:49,329
new account vectorizer and in this line

00:03:47,000 --> 00:03:49,329
here

00:03:50,680 --> 00:04:00,939
I believe that's a again a Christianity

00:03:56,950 --> 00:04:04,060
and atheism turn so what you'll

00:04:00,939 --> 00:04:05,709
typically find is that for a large

00:04:04,060 --> 00:04:08,469
portion of words in Christianity in the

00:04:05,709 --> 00:04:09,909
atheism frequency to be approximately

00:04:08,469 --> 00:04:11,170
similar they're talking about the same

00:04:09,909 --> 00:04:16,680
thing but from different angles

00:04:11,170 --> 00:04:21,609
obviously so we've trained our model and

00:04:16,680 --> 00:04:25,449
and I just realize these that should be

00:04:21,609 --> 00:04:27,460
up there but because we've already fit

00:04:25,449 --> 00:04:29,050
our model on our training data set we

00:04:27,460 --> 00:04:30,669
can then use that to transform our

00:04:29,050 --> 00:04:34,599
testing data set now this is the data

00:04:30,669 --> 00:04:37,300
set that we left till the end so we want

00:04:34,599 --> 00:04:39,009
those column features those those

00:04:37,300 --> 00:04:41,199
columns to match exactly the same as you

00:04:39,009 --> 00:04:43,120
know I'd artist training data set so we

00:04:41,199 --> 00:04:45,460
just do a transform here and that will

00:04:43,120 --> 00:04:47,520
use our previously went dictionary to

00:04:45,460 --> 00:04:53,919
make sure all the columns match up and

00:04:47,520 --> 00:04:56,729
and from here we can take out our matrix

00:04:53,919 --> 00:05:00,010
representing the training documents and

00:04:56,729 --> 00:05:02,530
white rain which is the categories that

00:05:00,010 --> 00:05:05,710
they're in and put them into a support

00:05:02,530 --> 00:05:10,630
vector machine with no parameters just

00:05:05,710 --> 00:05:13,659
the defaults and then and then that we

00:05:10,630 --> 00:05:15,250
fit that model and this basically it

00:05:13,659 --> 00:05:18,099
tells the support vector machines do its

00:05:15,250 --> 00:05:19,840
magic find out the certain boundaries

00:05:18,099 --> 00:05:21,789
that support vector machines have I

00:05:19,840 --> 00:05:23,530
won't go into too many details about how

00:05:21,789 --> 00:05:26,710
that works and that's a whole nother

00:05:23,530 --> 00:05:28,599
presentation but basically we just fit a

00:05:26,710 --> 00:05:31,150
learning model around that around just

00:05:28,599 --> 00:05:35,530
the training data they will use that

00:05:31,150 --> 00:05:39,970
fitted model here to predict what was

00:05:35,530 --> 00:05:43,509
you know test it so you can see here it

00:05:39,970 --> 00:05:45,340
is only given the bag of words model for

00:05:43,509 --> 00:05:47,380
the documents it's not given what's

00:05:45,340 --> 00:05:50,830
expected what they expected classes are

00:05:47,380 --> 00:05:54,820
it's designed to predict that and this

00:05:50,830 --> 00:05:58,150
is the f pred or f prediction and we can

00:05:54,820 --> 00:06:01,760
see here that the f1 score we calculate

00:05:58,150 --> 00:06:04,430
that and it's 0.5 to 7

00:06:01,760 --> 00:06:06,380
as I said before the f scores related to

00:06:04,430 --> 00:06:09,080
accuracy and generally wherever you see

00:06:06,380 --> 00:06:11,560
in F school you can think in your head

00:06:09,080 --> 00:06:14,030
either yeah that's roughly the accuracy

00:06:11,560 --> 00:06:15,710
it's not that's not a very correct thing

00:06:14,030 --> 00:06:19,250
to say but it's approximately right most

00:06:15,710 --> 00:06:23,080
of time but I just put the accuracy

00:06:19,250 --> 00:06:26,480
there because that comes in handy later

00:06:23,080 --> 00:06:28,970
so yeah so we get four three classes we

00:06:26,480 --> 00:06:31,730
would expect a random chance here to be

00:06:28,970 --> 00:06:36,290
about 0.33 so our bag of my words model

00:06:31,730 --> 00:06:38,270
does much better than chance there's a

00:06:36,290 --> 00:06:42,260
whole line of research that goes into

00:06:38,270 --> 00:06:43,730
getting this that figure up that you

00:06:42,260 --> 00:06:45,790
know works on different parameters and

00:06:43,730 --> 00:06:48,920
stuff like that to try and improve this

00:06:45,790 --> 00:06:54,110
performance but we can see just with the

00:06:48,920 --> 00:06:56,750
defaults we do a reasonable job and this

00:06:54,110 --> 00:06:59,150
here is just basically everything we've

00:06:56,750 --> 00:07:07,580
talked about but just in one handy

00:06:59,150 --> 00:07:10,370
script now the next step is I had a very

00:07:07,580 --> 00:07:14,260
cool Batman were cloud here so if anyone

00:07:10,370 --> 00:07:14,260
wants to see that later just let me know

00:07:14,290 --> 00:07:21,800
yeah so so the next example is spam

00:07:19,340 --> 00:07:25,640
detection using Twitter or spam

00:07:21,800 --> 00:07:29,570
detection on Twitter I said before I do

00:07:25,640 --> 00:07:33,010
a lot of cybercrime stuff and in spam is

00:07:29,570 --> 00:07:36,170
a big problem first of all it's annoying

00:07:33,010 --> 00:07:40,160
but also it's a vector for crime whether

00:07:36,170 --> 00:07:43,490
that's spreading malware sending you to

00:07:40,160 --> 00:07:45,230
illegal pharmacy shops where who knows

00:07:43,490 --> 00:07:47,120
what's going to happen to your credit

00:07:45,230 --> 00:07:52,690
card or your body after using one of

00:07:47,120 --> 00:07:56,210
those and as well as that it basically

00:07:52,690 --> 00:07:58,700
it can actually cause significant damage

00:07:56,210 --> 00:08:00,440
in Australia we see spam has been a

00:07:58,700 --> 00:08:03,410
nuisance but if you can imagine yourself

00:08:00,440 --> 00:08:05,480
in a island in the middle Pacific where

00:08:03,410 --> 00:08:08,660
the whole country's bandwidth is very

00:08:05,480 --> 00:08:11,060
limited and eighty percent of the emails

00:08:08,660 --> 00:08:12,530
going into that country as spam you're

00:08:11,060 --> 00:08:15,050
actually causing significant economic

00:08:12,530 --> 00:08:15,449
damage to the whole country by using up

00:08:15,050 --> 00:08:19,710
a whole

00:08:15,449 --> 00:08:22,789
country's pipes and your craft so it's

00:08:19,710 --> 00:08:22,789
it's a big problem

00:08:23,279 --> 00:08:29,939
so for twitter twitter has a big spam

00:08:26,999 --> 00:08:31,770
problem to probably argue that most

00:08:29,939 --> 00:08:37,039
stuff on Twitter is spam but there is

00:08:31,770 --> 00:08:37,039
actually actual spam in there as well

00:08:37,849 --> 00:08:46,350
and I sent up I said here that here that

00:08:44,219 --> 00:08:49,380
spam detection is generally a pretty

00:08:46,350 --> 00:08:50,940
easy job for a human but in Twitter it's

00:08:49,380 --> 00:08:53,670
a bit more difficult because without

00:08:50,940 --> 00:08:55,649
context a lot of a lot of tweets are

00:08:53,670 --> 00:08:59,010
actually very short and mean absolutely

00:08:55,649 --> 00:09:01,920
nothing without the context you know

00:08:59,010 --> 00:09:04,829
they might be replying to a tweet or be

00:09:01,920 --> 00:09:06,329
part of a contest and as well as that

00:09:04,829 --> 00:09:09,600
when I was going through and labeling

00:09:06,329 --> 00:09:11,790
the data set for this I said there was a

00:09:09,600 --> 00:09:12,959
whole bunch of tweets that were contests

00:09:11,790 --> 00:09:16,709
related you know like who's your

00:09:12,959 --> 00:09:18,540
favorite MTV star and if they come up my

00:09:16,709 --> 00:09:20,490
Twitter feed I would consider those spam

00:09:18,540 --> 00:09:22,050
because I'm not participating in that

00:09:20,490 --> 00:09:23,760
contest at all

00:09:22,050 --> 00:09:26,490
but for someone who is participating

00:09:23,760 --> 00:09:27,660
that con in that contest it's actually

00:09:26,490 --> 00:09:33,420
what they want to see they want to see

00:09:27,660 --> 00:09:36,600
how their artist is doing so it's it's

00:09:33,420 --> 00:09:39,540
an easy job generally for for me to duel

00:09:36,600 --> 00:09:41,610
Thomas for myself but it's actually

00:09:39,540 --> 00:09:48,089
determined globally what spams not

00:09:41,610 --> 00:09:51,420
always that easy thank you so here we

00:09:48,089 --> 00:09:56,339
just slide the data up in our data set a

00:09:51,420 --> 00:10:00,959
hundred spoilers we have the hundred

00:09:56,339 --> 00:10:02,850
that was spam and 398 that or not so we

00:10:00,959 --> 00:10:06,149
have that unbalanced data set that will

00:10:02,850 --> 00:10:08,760
talking about before and this is exactly

00:10:06,149 --> 00:10:12,630
the problem that accuracy is not good at

00:10:08,760 --> 00:10:16,470
evaluating so if we use exactly our

00:10:12,630 --> 00:10:18,899
script as before we we fit a bag of

00:10:16,470 --> 00:10:21,269
words model we use that to fit a support

00:10:18,899 --> 00:10:26,730
vector machine use that support vector

00:10:21,269 --> 00:10:29,190
machine to predict our test set we get a

00:10:26,730 --> 00:10:31,680
F score of zero

00:10:29,190 --> 00:10:35,310
and an accuracy of seventy eight point

00:10:31,680 --> 00:10:38,579
six so if we only measured accuracy we

00:10:35,310 --> 00:10:41,879
think we've done quite well but this F

00:10:38,579 --> 00:10:43,800
score by default only looks at the

00:10:41,879 --> 00:10:47,040
positive classes only looks at what

00:10:43,800 --> 00:10:48,870
we've actually labeled as spam and only

00:10:47,040 --> 00:10:51,870
evaluates based on that you can change

00:10:48,870 --> 00:10:57,209
that but but by default it only looks at

00:10:51,870 --> 00:11:01,079
the positive and we can see why by using

00:10:57,209 --> 00:11:04,259
this classification report for things

00:11:01,079 --> 00:11:08,550
that are not spam whenever we will

00:11:04,259 --> 00:11:12,540
basically what we did here is we always

00:11:08,550 --> 00:11:15,420
got our not spam and we never tried with

00:11:12,540 --> 00:11:17,069
the spam so all that classified here was

00:11:15,420 --> 00:11:27,569
doing which is saying everything is not

00:11:17,069 --> 00:11:32,269
spam and it did quite well so we can fix

00:11:27,569 --> 00:11:35,160
this by doing some parameterization and

00:11:32,269 --> 00:11:37,819
basically with this cell here I've done

00:11:35,160 --> 00:11:42,449
two things the first is introduce some

00:11:37,819 --> 00:11:45,660
parameters but first I've set up a

00:11:42,449 --> 00:11:46,889
pipeline now a pipeline is a standard

00:11:45,660 --> 00:11:49,230
thing in machine learning because you

00:11:46,889 --> 00:11:51,209
often have input going into a one

00:11:49,230 --> 00:11:52,769
algorithm the output of that input to

00:11:51,209 --> 00:11:54,149
the next algorithm the output of that

00:11:52,769 --> 00:11:56,449
into the next algorithm all these

00:11:54,149 --> 00:12:00,750
transformers transformers ations

00:11:56,449 --> 00:12:03,480
including normalization you know our bag

00:12:00,750 --> 00:12:06,949
of words which takes text documents as

00:12:03,480 --> 00:12:09,389
input and gives your matrix as output

00:12:06,949 --> 00:12:11,310
even support vector machines can be seen

00:12:09,389 --> 00:12:14,189
as a transformer where it takes a matrix

00:12:11,310 --> 00:12:16,589
as input and returns a vector as of

00:12:14,189 --> 00:12:19,220
predictions as output but but that's

00:12:16,589 --> 00:12:22,319
that's handled separately in a pipeline

00:12:19,220 --> 00:12:26,370
so basically a pipeline is just a list

00:12:22,319 --> 00:12:28,709
of things that that fit the transformal

00:12:26,370 --> 00:12:31,110
model of taking one thing is important

00:12:28,709 --> 00:12:33,240
giving a different thing as output it'll

00:12:31,110 --> 00:12:35,730
and it will do what I said before input

00:12:33,240 --> 00:12:37,920
input into one output as inputs into the

00:12:35,730 --> 00:12:39,509
second output as input into third and

00:12:37,920 --> 00:12:42,839
keeps going until it runs out of models

00:12:39,509 --> 00:12:43,150
here and we also name them so that we

00:12:42,839 --> 00:12:48,339
can

00:12:43,150 --> 00:12:51,310
say later that we want to vectorizer to

00:12:48,339 --> 00:12:54,940
have values for this specific parameter

00:12:51,310 --> 00:12:57,190
and here is a list of things for that

00:12:54,940 --> 00:12:59,890
parameter that's probably not a good

00:12:57,190 --> 00:13:03,190
example but here we have a vectorizer

00:12:59,890 --> 00:13:04,839
the min DF and we want to try those of

00:13:03,190 --> 00:13:07,779
two and three for that now this

00:13:04,839 --> 00:13:10,570
parameter here is the minimum document

00:13:07,779 --> 00:13:13,300
frequency so we want to say here we will

00:13:10,570 --> 00:13:14,890
only include features that occur in at

00:13:13,300 --> 00:13:17,470
least two documents and this gets rid of

00:13:14,890 --> 00:13:18,850
a whole lot of things that only appear

00:13:17,470 --> 00:13:22,810
once in the document are never going to

00:13:18,850 --> 00:13:25,710
be useful for classification but what

00:13:22,810 --> 00:13:28,839
what the most important thing in this

00:13:25,710 --> 00:13:33,970
model is is the Engram range so an

00:13:28,839 --> 00:13:36,160
Engram actually one step back with spam

00:13:33,970 --> 00:13:39,160
on Twitter just from a bag of words

00:13:36,160 --> 00:13:42,060
model often looks like nods non-spam the

00:13:39,160 --> 00:13:45,490
words themselves are used everywhere

00:13:42,060 --> 00:13:50,290
spammers are very good at looking like

00:13:45,490 --> 00:13:51,490
normal messages that's their job so when

00:13:50,290 --> 00:13:54,839
you actually look at the individual

00:13:51,490 --> 00:13:57,310
words and their frequencies they

00:13:54,839 --> 00:14:01,029
actually have this similar frequency

00:13:57,310 --> 00:14:05,040
distribution but engrams make it harder

00:14:01,029 --> 00:14:08,290
to to fake that so an Engram is a

00:14:05,040 --> 00:14:11,050
subsequence of n consecutive tokens in a

00:14:08,290 --> 00:14:14,560
row so when we're looking at words we

00:14:11,050 --> 00:14:16,990
and n is say 2 we have two words in row

00:14:14,560 --> 00:14:19,120
and we use that as our feature not just

00:14:16,990 --> 00:14:21,610
the individual words and we still use

00:14:19,120 --> 00:14:23,980
this bag of words model so we would grab

00:14:21,610 --> 00:14:25,450
our engrams and that's a feature but

00:14:23,980 --> 00:14:26,740
then we don't give any other context

00:14:25,450 --> 00:14:30,220
into where it occurred we just see how

00:14:26,740 --> 00:14:33,779
many times this feature occurred and we

00:14:30,220 --> 00:14:35,980
just do that count across the whole lot

00:14:33,779 --> 00:14:38,170
and we'll also introduce the analyzer

00:14:35,980 --> 00:14:41,020
here because there'll be a different

00:14:38,170 --> 00:14:45,430
later thank you difference later but

00:14:41,020 --> 00:14:47,890
it's word so we use an Engram range of 1

00:14:45,430 --> 00:14:51,300
which is just one word what we did

00:14:47,890 --> 00:14:54,490
before up to 3 and this will do

00:14:51,300 --> 00:14:58,100
combinations of one and two and

00:14:54,490 --> 00:14:59,990
all-in-one model and these are

00:14:58,100 --> 00:15:02,110
parameters for the classifier but I'm

00:14:59,990 --> 00:15:05,630
not going to go into those in this talk

00:15:02,110 --> 00:15:08,840
now with our pipeline and our parameters

00:15:05,630 --> 00:15:10,640
that we've set up we can we can use a

00:15:08,840 --> 00:15:14,690
thing called cross-validation which

00:15:10,640 --> 00:15:16,730
basically does what we said before about

00:15:14,690 --> 00:15:18,680
taking a bit of our data away for

00:15:16,730 --> 00:15:23,120
evaluation purposes and training just on

00:15:18,680 --> 00:15:25,190
a bit I'm gonna very quickly skip over

00:15:23,120 --> 00:15:26,780
the detail but basically we do this a

00:15:25,190 --> 00:15:29,000
whole bunch of times with the whole with

00:15:26,780 --> 00:15:31,490
a different testing data set so we're

00:15:29,000 --> 00:15:33,140
still within our whole training data set

00:15:31,490 --> 00:15:35,210
we're not haven't used this bit of data

00:15:33,140 --> 00:15:36,500
that we took away at the start we're

00:15:35,210 --> 00:15:38,450
still within our training data set but

00:15:36,500 --> 00:15:40,640
we take a little portion of that just to

00:15:38,450 --> 00:15:42,680
evaluate a little one attempt at a model

00:15:40,640 --> 00:15:47,110
we do this a whole bunch of times and

00:15:42,680 --> 00:15:50,090
basically what grid search CV does is it

00:15:47,110 --> 00:15:53,000
uses all possible combinations of all of

00:15:50,090 --> 00:15:55,340
these parameters test them all out under

00:15:53,000 --> 00:15:57,170
a cross-validation framework which if

00:15:55,340 --> 00:16:02,150
you're doing machine learning you really

00:15:57,170 --> 00:16:04,220
need to be using and and it will tell

00:16:02,150 --> 00:16:11,750
you which the best which is the best

00:16:04,220 --> 00:16:15,200
model so we this itself is a classifier

00:16:11,750 --> 00:16:17,840
which means all we need to do is fit on

00:16:15,200 --> 00:16:19,550
our training set and it will go and do

00:16:17,840 --> 00:16:22,610
all this cross validation testing all

00:16:19,550 --> 00:16:25,810
possible combinations by itself it will

00:16:22,610 --> 00:16:28,430
come back maybe a little while later and

00:16:25,810 --> 00:16:31,370
give you some predictions now these are

00:16:28,430 --> 00:16:33,740
the predictions on the best possible set

00:16:31,370 --> 00:16:35,270
of parameters that grid has found so

00:16:33,740 --> 00:16:37,580
it's going off tested all these models

00:16:35,270 --> 00:16:44,690
chosen the best one and then use that

00:16:37,580 --> 00:16:46,070
for prediction and so based on the small

00:16:44,690 --> 00:16:48,590
number of parameters that we define up

00:16:46,070 --> 00:16:53,870
here which for a machine loan is quite

00:16:48,590 --> 00:16:58,820
small it does quite well if we have a

00:16:53,870 --> 00:17:03,189
look here at our just our span

00:16:58,820 --> 00:17:06,199
predictions what the precision means is

00:17:03,189 --> 00:17:09,650
when we said something was spam will

00:17:06,199 --> 00:17:12,350
write 80% of the time and when something

00:17:09,650 --> 00:17:15,079
was actually spam we got that correct

00:17:12,350 --> 00:17:20,360
like we actually found 59% of the spam

00:17:15,079 --> 00:17:21,829
in their whole data set and these are

00:17:20,360 --> 00:17:23,600
basically just calculating your score

00:17:21,829 --> 00:17:29,630
based on that so the F score is a

00:17:23,600 --> 00:17:36,140
combination of the precision recall so

00:17:29,630 --> 00:17:38,179
that's as detecting spam in Twitter the

00:17:36,140 --> 00:17:40,790
next application I'm going to quickly go

00:17:38,179 --> 00:17:43,940
over is related to my research which is

00:17:40,790 --> 00:17:47,750
on determining the author of say a

00:17:43,940 --> 00:17:52,460
program based on attributes only within

00:17:47,750 --> 00:17:53,809
that program so so how what we do here

00:17:52,460 --> 00:17:57,650
is we get the source code of a whole

00:17:53,809 --> 00:18:00,169
bunch of different software we train a

00:17:57,650 --> 00:18:02,179
model very similar to this one but

00:18:00,169 --> 00:18:05,840
instead of using words we use characters

00:18:02,179 --> 00:18:07,580
as what we find both in natural

00:18:05,840 --> 00:18:10,640
languages and in programming languages

00:18:07,580 --> 00:18:13,100
is that words are not very good for

00:18:10,640 --> 00:18:15,410
picking up authorship but characters

00:18:13,100 --> 00:18:19,490
which just you know one say you money

00:18:15,410 --> 00:18:22,130
character engrams are very good at doing

00:18:19,490 --> 00:18:24,110
this and the reason they work for

00:18:22,130 --> 00:18:27,950
natural languages is that people often

00:18:24,110 --> 00:18:30,110
use words that they can say and and

00:18:27,950 --> 00:18:33,440
while you know a lot of people here

00:18:30,110 --> 00:18:36,140
might have learned English approximately

00:18:33,440 --> 00:18:38,900
similar times in school in approximately

00:18:36,140 --> 00:18:41,210
the same areas the variances in how you

00:18:38,900 --> 00:18:43,130
learn language dictate what you can say

00:18:41,210 --> 00:18:45,049
and dictate how you write and those

00:18:43,130 --> 00:18:46,730
little differences can be mapped in a

00:18:45,049 --> 00:18:52,309
frequency and it's similar bag of words

00:18:46,730 --> 00:18:54,799
model and and we can build a model using

00:18:52,309 --> 00:18:55,880
this sort of bag of words model using

00:18:54,799 --> 00:18:57,590
the support vector machines or even

00:18:55,880 --> 00:19:00,350
simple models and we can predict the

00:18:57,590 --> 00:19:04,190
author so I don't have a working example

00:19:00,350 --> 00:19:06,650
of that but I just thought I will go

00:19:04,190 --> 00:19:10,280
over that that is a thing that can be

00:19:06,650 --> 00:19:14,540
done another example which I think come

00:19:10,280 --> 00:19:16,160
up properly but is trying to predict

00:19:14,540 --> 00:19:16,700
someone's political stance based on

00:19:16,160 --> 00:19:21,080
their social

00:19:16,700 --> 00:19:24,170
Media profile and basically this

00:19:21,080 --> 00:19:26,480
research uses a much more complex model

00:19:24,170 --> 00:19:28,910
based on the Li WC corpus which

00:19:26,480 --> 00:19:30,590
basically takes these words and gives a

00:19:28,910 --> 00:19:31,940
whole bunch of extra information so

00:19:30,590 --> 00:19:33,710
where before I said that the bag of

00:19:31,940 --> 00:19:36,110
words model discards all this extra

00:19:33,710 --> 00:19:38,270
information doesn't care about it this

00:19:36,110 --> 00:19:40,310
uses that bag of words model with a

00:19:38,270 --> 00:19:42,530
whole bunch of extra information such as

00:19:40,310 --> 00:19:48,740
when this particular word is used in

00:19:42,530 --> 00:19:50,480
this context it means this so so it go

00:19:48,740 --> 00:19:52,190
it goes a bit beyond semantics so

00:19:50,480 --> 00:19:53,990
semantics basically means when someone

00:19:52,190 --> 00:19:55,910
uses a word bank that could mean a

00:19:53,990 --> 00:19:58,130
financial institution or a comedian side

00:19:55,910 --> 00:20:00,740
of a river we use semantics to work out

00:19:58,130 --> 00:20:02,000
which one they meant but with Li WC it

00:20:00,740 --> 00:20:07,580
adds that with a whole bunch of extra

00:20:02,000 --> 00:20:07,910
context information and that was perfect

00:20:07,580 --> 00:20:11,060
timing

00:20:07,910 --> 00:20:14,240
so so yet so those are my two other

00:20:11,060 --> 00:20:16,280
examples so thank you for listening and

00:20:14,240 --> 00:20:20,890
sorry about the technical information we

00:20:16,280 --> 00:20:20,890
got there so we've got about

00:20:24,590 --> 00:20:31,080
one minute of questions and then we'll

00:20:27,600 --> 00:20:35,840
have read window you can quickly set up

00:20:31,080 --> 00:20:35,840
now this is thank you

00:20:35,970 --> 00:20:49,649
questions anyone one or two questions

00:20:38,879 --> 00:20:53,759
right good robot if you can explain can

00:20:49,649 --> 00:20:56,820
it do more aquifer like in the in the

00:20:53,759 --> 00:20:59,070
context of Symantec rod and then only

00:20:56,820 --> 00:21:05,370
individual words sorry could you repeat

00:20:59,070 --> 00:21:09,029
the question when you categorize can it

00:21:05,370 --> 00:21:12,600
be more clever that it can comprise it a

00:21:09,029 --> 00:21:14,970
pack arise it based on semantics rather

00:21:12,600 --> 00:21:16,799
than than just words yes so you're

00:21:14,970 --> 00:21:18,690
asking me if we can use semantics in the

00:21:16,799 --> 00:21:20,879
bag of words 100 for instance or just

00:21:18,690 --> 00:21:24,690
generally yeah you can definitely use it

00:21:20,879 --> 00:21:26,039
generally so we're like we're before I

00:21:24,690 --> 00:21:27,299
said you would use a particular word and

00:21:26,039 --> 00:21:29,039
that would be the feature for a

00:21:27,299 --> 00:21:31,440
particular column you could use a

00:21:29,039 --> 00:21:32,730
particular meaning of a word instead and

00:21:31,440 --> 00:21:36,600
everything else would still work the

00:21:32,730 --> 00:21:39,659
same way but you would have a your word

00:21:36,600 --> 00:21:42,269
net which handles semantics for instance

00:21:39,659 --> 00:21:44,669
would say this is the version of Bank

00:21:42,269 --> 00:21:46,139
which means financial institution and

00:21:44,669 --> 00:21:48,360
this is the version of Bank which means

00:21:46,139 --> 00:21:50,730
side of a river and it would create

00:21:48,360 --> 00:21:53,039
different features for each of those so

00:21:50,730 --> 00:21:54,600
when it is a high computational cost

00:21:53,039 --> 00:21:56,730
because it needs to look at the sentence

00:21:54,600 --> 00:21:58,500
and the other words but after it does

00:21:56,730 --> 00:22:00,330
that you can still use that as a single

00:21:58,500 --> 00:22:03,240
token and everything else here still

00:22:00,330 --> 00:22:07,169
works that's not built into scikit-learn

00:22:03,240 --> 00:22:09,659
but it is in an LT k so you would use n

00:22:07,169 --> 00:22:13,220
LT k to do that processing and then send

00:22:09,659 --> 00:22:13,220
it into scikit-learn for classification

00:22:13,610 --> 00:22:17,639
all right to question it all yeah that's

00:22:16,049 --> 00:22:19,940
that there was pretty much time ok so

00:22:17,639 --> 00:22:19,940
cool

00:22:26,419 --> 00:22:28,480

YouTube URL: https://www.youtube.com/watch?v=2AEM89ltcgY


