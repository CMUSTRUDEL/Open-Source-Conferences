Title: OpenStack MiniConf: Large Scale Identification of Race Conditions  by Joseph Gordon (HP)
Publication date: 2014-08-11
Playlist: PyCon Australia 2014 Openstack
Description: 
	PyCon Australia is the national conference for users of the Python Programming Language. In August 2014, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

August 1-5, Brisbane, Queensland, Australia
Captions: 
	00:00:06,170 --> 00:00:11,820
okay for our last session we have Joe

00:00:10,170 --> 00:00:14,130
Gordon who's going to speak about

00:00:11,820 --> 00:00:18,029
large-scale identification of race

00:00:14,130 --> 00:00:20,460
conditions in openstack CI Joe is a Nova

00:00:18,029 --> 00:00:22,980
Corps probably where's our hats that I'm

00:00:20,460 --> 00:00:30,630
unaware of I'm unaware of them to say

00:00:22,980 --> 00:00:32,519
okay and yeah please welcome go hi so

00:00:30,630 --> 00:00:35,970
thanks thank you to need up Jesus is

00:00:32,519 --> 00:00:37,590
loud sorry thanks to Neeta for

00:00:35,970 --> 00:00:40,739
introducing the log stuff a bit that

00:00:37,590 --> 00:00:42,000
sort of ties into that a big part of

00:00:40,739 --> 00:00:45,660
this actually turn understand the logs

00:00:42,000 --> 00:00:47,219
and use them in for useful things so

00:00:45,660 --> 00:00:48,300
open sac you guys are probably seen this

00:00:47,219 --> 00:00:50,160
image by now so I'm just going to sort

00:00:48,300 --> 00:00:52,170
of skip it open sac is a big thing

00:00:50,160 --> 00:00:53,399
provides lots of stuff yada yada yada

00:00:52,170 --> 00:00:58,199
hopefully by now we all know what this

00:00:53,399 --> 00:01:00,120
is right yes great okay so dope intact

00:00:58,199 --> 00:01:03,030
is really really big in development

00:01:00,120 --> 00:01:04,739
scale we're simply massive I think we're

00:01:03,030 --> 00:01:06,869
the biggest Python project that's open

00:01:04,739 --> 00:01:08,759
tourists that I know of if anybody here

00:01:06,869 --> 00:01:10,349
thinks that's incorrect please let me

00:01:08,759 --> 00:01:12,750
know because I haven't found any better

00:01:10,349 --> 00:01:15,209
numbers and this past 30 days we've had

00:01:12,750 --> 00:01:17,340
almost five incher contributors and past

00:01:15,209 --> 00:01:19,709
12 months we've had eighteen hundred and

00:01:17,340 --> 00:01:22,950
sixty one thousand commits so we're

00:01:19,709 --> 00:01:25,440
really big where a lot of commits a lot

00:01:22,950 --> 00:01:27,030
of code to over two million lines of

00:01:25,440 --> 00:01:29,069
code this is bio low so this is a bit of

00:01:27,030 --> 00:01:30,450
a third party that's hopefully less bias

00:01:29,069 --> 00:01:33,360
and some of the OpenStack folks

00:01:30,450 --> 00:01:39,030
themselves so with the scale there's a

00:01:33,360 --> 00:01:40,590
lot of complexity we have to develop

00:01:39,030 --> 00:01:41,550
principles which is not working on

00:01:40,590 --> 00:01:44,030
OpenStack they would sort of try to

00:01:41,550 --> 00:01:46,170
follow that dictate a lot of our

00:01:44,030 --> 00:01:47,789
processes and they're never break trunk

00:01:46,170 --> 00:01:50,099
so that's why I have all this testing we

00:01:47,789 --> 00:01:51,539
have generate tons and tons of tests

00:01:50,099 --> 00:01:54,869
every day we saw that we generated over

00:01:51,539 --> 00:01:57,690
a million test slaves the past few

00:01:54,869 --> 00:01:59,610
months or whatever that was so never

00:01:57,690 --> 00:02:00,780
break trunk we want to block each other

00:01:59,610 --> 00:02:02,399
if you're a developer you're working on

00:02:00,780 --> 00:02:03,989
a piece of code you downloaded from get

00:02:02,399 --> 00:02:05,819
you download the trunk and it's broken

00:02:03,989 --> 00:02:07,709
you get really upset nothing's working

00:02:05,819 --> 00:02:10,259
now you can't do your work and that

00:02:07,709 --> 00:02:11,099
caused a lot of problems this also helps

00:02:10,259 --> 00:02:13,110
support the continuous deployment

00:02:11,099 --> 00:02:14,820
process so unlike most projects for

00:02:13,110 --> 00:02:16,260
supporting stable releases and

00:02:14,820 --> 00:02:17,680
continuous deployment so she bled to

00:02:16,260 --> 00:02:19,930
play from trunk on

00:02:17,680 --> 00:02:21,730
any given day of the week anytime this

00:02:19,930 --> 00:02:23,950
is actually being done by Rackspace and

00:02:21,730 --> 00:02:27,340
HP public clouds both of them are I

00:02:23,950 --> 00:02:29,829
think within about six weeks of trunk so

00:02:27,340 --> 00:02:31,829
the code we write today gets pushed out

00:02:29,829 --> 00:02:33,849
to public clouds in a matter of weeks

00:02:31,829 --> 00:02:35,319
and this actually is all kinds of

00:02:33,849 --> 00:02:36,930
complexity we can't do it you know a

00:02:35,319 --> 00:02:39,219
stabilization period late in the cycle

00:02:36,930 --> 00:02:40,450
if we know things are broken and trunk

00:02:39,219 --> 00:02:42,969
people are going to be using it broken

00:02:40,450 --> 00:02:44,109
so we can't let that happen transparency

00:02:42,969 --> 00:02:46,239
we try to throw everything in the public

00:02:44,109 --> 00:02:47,530
or a big project and if we do everything

00:02:46,239 --> 00:02:50,169
secretly it makes it really hard to

00:02:47,530 --> 00:02:52,659
scale automate everything the same

00:02:50,169 --> 00:02:54,159
reason that this scale we can't run

00:02:52,659 --> 00:03:01,859
things manually we don't wanna run tests

00:02:54,159 --> 00:03:01,859
manually and just not really you yeah

00:03:05,370 --> 00:03:08,980
running things manually is not really

00:03:07,209 --> 00:03:11,560
scalable when people forget about it you

00:03:08,980 --> 00:03:12,669
forget to run you know Pepe even and you

00:03:11,560 --> 00:03:15,010
know imagine if we had to run all the

00:03:12,669 --> 00:03:16,030
tests manually magallon tarian this is

00:03:15,010 --> 00:03:17,169
for the same reason it's a big project

00:03:16,030 --> 00:03:19,299
and do everything ever the open

00:03:17,169 --> 00:03:22,239
everybody has a vhost voice in

00:03:19,299 --> 00:03:23,919
everything if they choose and the last

00:03:22,239 --> 00:03:25,239
ones a bit more controversial but be

00:03:23,919 --> 00:03:27,190
strict I want to reduce the burden on

00:03:25,239 --> 00:03:29,530
reviewers I'm so big part of this is a

00:03:27,190 --> 00:03:30,669
reviewer the most constrained resource

00:03:29,530 --> 00:03:32,169
right now an OpenStack is the human

00:03:30,669 --> 00:03:33,760
reviewer so we have a lot of developers

00:03:32,169 --> 00:03:35,439
we have a lot of reviewers but we don't

00:03:33,760 --> 00:03:36,879
have enough reviewers so we try to

00:03:35,439 --> 00:03:38,049
remove the burden off the reviewer so

00:03:36,879 --> 00:03:39,970
we're a viewer doesn't have to check for

00:03:38,049 --> 00:03:42,129
style correctly so we have a pet paid

00:03:39,970 --> 00:03:44,169
job for that we don't check for the unit

00:03:42,129 --> 00:03:45,340
test passing or integration tests

00:03:44,169 --> 00:03:46,870
passing so we try to do as much as

00:03:45,340 --> 00:03:48,669
possible to move the burden off the

00:03:46,870 --> 00:03:50,290
human reviewer and push that into an

00:03:48,669 --> 00:03:54,280
automated system that have been more

00:03:50,290 --> 00:03:55,599
scalable than not the humans are so what

00:03:54,280 --> 00:03:57,810
happens when you submit code who here is

00:03:55,599 --> 00:04:00,040
actually submitted a patch to OpenStack

00:03:57,810 --> 00:04:03,489
okay a good amount I don't think josh

00:04:00,040 --> 00:04:04,900
has so a lot of things happen

00:04:03,489 --> 00:04:06,669
automatically so we do a whole bunch of

00:04:04,900 --> 00:04:08,650
things right away so we have a pep eight

00:04:06,669 --> 00:04:10,509
job you push rate your patch push it

00:04:08,650 --> 00:04:12,280
into get type and get review it goes up

00:04:10,509 --> 00:04:13,299
to our Garrett server and then behind

00:04:12,280 --> 00:04:14,919
the scenes a whole bunch of things

00:04:13,299 --> 00:04:17,229
happen thanks to our great infra team

00:04:14,919 --> 00:04:18,840
the pet paid job that's for style

00:04:17,229 --> 00:04:20,949
correctness to make sure that's correct

00:04:18,840 --> 00:04:22,960
we don't want the humans to really care

00:04:20,949 --> 00:04:24,430
about that too much although as a team

00:04:22,960 --> 00:04:26,979
you like the code to look the same and

00:04:24,430 --> 00:04:29,620
so it's important for us to enforce some

00:04:26,979 --> 00:04:30,550
sort of style formatting unit tests want

00:04:29,620 --> 00:04:31,150
to make sure unit tests are always

00:04:30,550 --> 00:04:33,970
passing

00:04:31,150 --> 00:04:37,030
we generally test against Python 2627

00:04:33,970 --> 00:04:41,860
and we're starting to migrate to 33 and

00:04:37,030 --> 00:04:43,540
off 33 and now on two three four so 26 i

00:04:41,860 --> 00:04:45,880
think is we're getting the next we're

00:04:43,540 --> 00:04:47,020
going to drop 26 in the next release 27

00:04:45,880 --> 00:04:50,440
to be the big one and we're trying to

00:04:47,020 --> 00:04:52,480
move to 34 compatibility as soon as

00:04:50,440 --> 00:04:53,680
possible which may take a long time

00:04:52,480 --> 00:04:56,140
because you a lot of dependency issues

00:04:53,680 --> 00:04:58,150
I'm devstack tempest this is our big

00:04:56,140 --> 00:04:59,260
integration environment and devstack

00:04:58,150 --> 00:05:00,340
foreign aid this is our upgrade

00:04:59,260 --> 00:05:02,470
environment this spins up an old

00:05:00,340 --> 00:05:04,060
devstack and up and upgrades everything

00:05:02,470 --> 00:05:05,800
it up putting in a new devstack so this

00:05:04,060 --> 00:05:07,810
goes for example Icehouse to junos what

00:05:05,800 --> 00:05:08,800
we're doing right now and this is

00:05:07,810 --> 00:05:09,910
because we believe upgrades are

00:05:08,800 --> 00:05:12,790
important and we want to make sure they

00:05:09,910 --> 00:05:14,170
work at any given time so in a single

00:05:12,790 --> 00:05:15,550
devstack environment we have several

00:05:14,170 --> 00:05:19,240
these on any given test will run

00:05:15,550 --> 00:05:21,340
postgres my sequel we run against

00:05:19,240 --> 00:05:24,880
Neutron Nova networking a whole bunch of

00:05:21,340 --> 00:05:25,990
other parameters right about 180 vm is

00:05:24,880 --> 00:05:27,820
inside of every single deficit

00:05:25,990 --> 00:05:29,770
environment so this is a really big

00:05:27,820 --> 00:05:34,780
system and we're generating tons of data

00:05:29,770 --> 00:05:36,550
out of every single job so this is a

00:05:34,780 --> 00:05:38,050
example of a real patch this would

00:05:36,550 --> 00:05:41,110
happen to submit a piece of coach

00:05:38,050 --> 00:05:43,390
OpenStack so here's a little you know as

00:05:41,110 --> 00:05:46,020
Tiny doc change this is not even a code

00:05:43,390 --> 00:05:51,100
change it goes up the test get run and

00:05:46,020 --> 00:05:52,750
they fail it this this doesn't work

00:05:51,100 --> 00:05:56,140
right the something's broken here what

00:05:52,750 --> 00:05:58,300
happened why does making a doc string

00:05:56,140 --> 00:05:59,950
change break all the integration

00:05:58,300 --> 00:06:04,330
environments that's not supposed to

00:05:59,950 --> 00:06:05,890
happen so it's a lot of data inherent

00:06:04,330 --> 00:06:08,080
it's hard to try to interpret all this

00:06:05,890 --> 00:06:10,150
data and as of you know what happened in

00:06:08,080 --> 00:06:12,240
this whole system is you generated five

00:06:10,150 --> 00:06:15,790
to ten depth sacks in any single patch

00:06:12,240 --> 00:06:17,470
over 10,000 degraded tests thousands of

00:06:15,790 --> 00:06:21,310
second level guests so guests inside of

00:06:17,470 --> 00:06:23,290
our devstack VMs and about a gigabyte of

00:06:21,310 --> 00:06:24,670
data per job so we're generating tons of

00:06:23,290 --> 00:06:26,800
data in the job that's what Anita was

00:06:24,670 --> 00:06:29,200
talking about is reading the data is

00:06:26,800 --> 00:06:31,150
really hard actually this adds up to a

00:06:29,200 --> 00:06:33,430
lot of a lot of stuff every week on

00:06:31,150 --> 00:06:35,230
average given week we bet 250 to 500

00:06:33,430 --> 00:06:36,910
changes our merge that's not considering

00:06:35,230 --> 00:06:42,370
patches up for review that's actually

00:06:36,910 --> 00:06:44,180
code landing into trunk 15,000 to 30,000

00:06:42,370 --> 00:06:47,750
change revisions per week

00:06:44,180 --> 00:06:50,360
and just to give a nice point in time a

00:06:47,750 --> 00:06:52,430
reference it was about 10,000 new first

00:06:50,360 --> 00:07:00,020
time changes proposed in 42 every 42

00:06:52,430 --> 00:07:03,050
days yes so the garrett ID number we

00:07:00,020 --> 00:07:10,190
went from 70 k to ATK in 42 days an ATK

00:07:03,050 --> 00:07:13,509
290k in 42 days roughly every new every

00:07:10,190 --> 00:07:13,509
new patch not a revision of a patch

00:07:14,320 --> 00:07:19,099
sorry that's not clear yeah it's it for

00:07:17,479 --> 00:07:21,860
a new patch so not a revision of a patch

00:07:19,099 --> 00:07:24,740
a new patch so we're generating tons and

00:07:21,860 --> 00:07:30,050
tons of jobs any given day the week and

00:07:24,740 --> 00:07:31,970
any you know every day etc so we have

00:07:30,050 --> 00:07:34,220
large numbers here so some basic

00:07:31,970 --> 00:07:36,440
probability of large numbers we have a

00:07:34,220 --> 00:07:39,229
few basic factors here the chance of the

00:07:36,440 --> 00:07:41,360
event the number of events per run and

00:07:39,229 --> 00:07:43,729
the number of runs so here's a real

00:07:41,360 --> 00:07:44,840
example actually some of these numbers

00:07:43,729 --> 00:07:47,360
are a bit made up but this is a basic

00:07:44,840 --> 00:07:49,580
they're very close so let's just say

00:07:47,360 --> 00:07:51,139
github is down point zero five percent

00:07:49,580 --> 00:07:53,120
of the time that's actually pretty good

00:07:51,139 --> 00:07:56,780
uptime and this is probably close to

00:07:53,120 --> 00:07:59,000
true with that failure rate you have

00:07:56,780 --> 00:08:02,330
about 20 clones per run and we have

00:07:59,000 --> 00:08:04,550
1,500 jobs per week so that means github

00:08:02,330 --> 00:08:06,080
is failing and 15 jobs a week so that

00:08:04,550 --> 00:08:07,820
means 15 people write a patch that

00:08:06,080 --> 00:08:11,510
changes a doc string and it fails

00:08:07,820 --> 00:08:13,070
because github is down as a conclusion

00:08:11,510 --> 00:08:14,330
we actually no longer use github for

00:08:13,070 --> 00:08:16,430
cloning things we have our own get

00:08:14,330 --> 00:08:18,550
server because of this get how great

00:08:16,430 --> 00:08:21,050
limits you they're not designed to be

00:08:18,550 --> 00:08:24,919
you know you know supporting a CI system

00:08:21,050 --> 00:08:28,009
there for a different purpose so we've

00:08:24,919 --> 00:08:29,690
seen this in numerous times that even if

00:08:28,009 --> 00:08:34,579
something fails point zero five percent

00:08:29,690 --> 00:08:35,839
of the time that is way too much so this

00:08:34,579 --> 00:08:37,250
comes from another picture you guys

00:08:35,839 --> 00:08:38,839
probably all seen too many times today

00:08:37,250 --> 00:08:39,919
which is this is actually a really

00:08:38,839 --> 00:08:41,539
little picture of this point but

00:08:39,919 --> 00:08:44,260
OpenStack has a lot of services we

00:08:41,539 --> 00:08:46,160
actually hit a point we had a bug in our

00:08:44,260 --> 00:08:48,020
integration environment where postgres

00:08:46,160 --> 00:08:49,670
was falling over because we had too many

00:08:48,020 --> 00:08:52,220
connections with the default config I

00:08:49,670 --> 00:08:54,380
think that was about 100 150 connections

00:08:52,220 --> 00:08:56,529
so on the standard job or actually have

00:08:54,380 --> 00:08:57,920
about 100 hundred fifty processes

00:08:56,529 --> 00:09:00,139
talking to

00:08:57,920 --> 00:09:02,180
stress so this is a really big system of

00:09:00,139 --> 00:09:05,750
this is all on a tiny machine there's a

00:09:02,180 --> 00:09:08,450
lot of moving parts here so how I used

00:09:05,750 --> 00:09:10,730
to do this back in grizzly something

00:09:08,450 --> 00:09:12,680
would fail we had a tool because that

00:09:10,730 --> 00:09:13,940
happened we had a tool to run recheck so

00:09:12,680 --> 00:09:15,139
you just type in a recheck and you look

00:09:13,940 --> 00:09:17,290
up a bug number you try to figure out

00:09:15,139 --> 00:09:20,690
what it is on your own and then

00:09:17,290 --> 00:09:21,769
hopefully would pass the second time and

00:09:20,690 --> 00:09:25,130
nobody actually really knew what the

00:09:21,769 --> 00:09:26,240
bugs were and this was a big mess this

00:09:25,130 --> 00:09:28,160
is actually real example of something

00:09:26,240 --> 00:09:29,570
that happened somebody had a big patch

00:09:28,160 --> 00:09:32,240
series we like to break our patches up

00:09:29,570 --> 00:09:35,510
into small reviewable chunks somebody

00:09:32,240 --> 00:09:37,279
had a big patch at those 15 deep so they

00:09:35,510 --> 00:09:39,709
put it up the patch that fails you know

00:09:37,279 --> 00:09:41,329
a third of the patches fail they run

00:09:39,709 --> 00:09:43,220
recheck a different through the patches

00:09:41,329 --> 00:09:44,420
fail and then the guy goes hey have you

00:09:43,220 --> 00:09:46,579
seen this failure and they point to a

00:09:44,420 --> 00:09:50,180
URL and it's you know a giant stack

00:09:46,579 --> 00:09:52,220
trace in a random file and you know it

00:09:50,180 --> 00:09:53,570
turns out the brain is not a great big

00:09:52,220 --> 00:09:54,980
data solution you look at he go I don't

00:09:53,570 --> 00:09:57,949
know you know maybe I talk about so we

00:09:54,980 --> 00:09:59,120
could go maybe not then we turned on

00:09:57,949 --> 00:10:01,459
parallel testing at the time we're

00:09:59,120 --> 00:10:03,529
running one run tests worker on an

00:10:01,459 --> 00:10:04,790
entire integration environment and then

00:10:03,529 --> 00:10:06,949
we did in parallel and that broke

00:10:04,790 --> 00:10:09,740
everything and that's thanks to Matt

00:10:06,949 --> 00:10:13,640
over there he didn't break things he

00:10:09,740 --> 00:10:15,290
just facilitated the breaking and that

00:10:13,640 --> 00:10:17,630
really put us really far back it turns

00:10:15,290 --> 00:10:19,790
out we were testing a really simple test

00:10:17,630 --> 00:10:21,260
scenario where one person was talking to

00:10:19,790 --> 00:10:24,709
your cloud that's not really what

00:10:21,260 --> 00:10:26,660
happens in reality I think we're about

00:10:24,709 --> 00:10:28,160
at four workers now and we when he first

00:10:26,660 --> 00:10:30,709
switched over we broke everything all

00:10:28,160 --> 00:10:33,350
the time so you really needed a better

00:10:30,709 --> 00:10:35,060
solution than just looking at you know

00:10:33,350 --> 00:10:36,320
one gigabyte of logs and trying to

00:10:35,060 --> 00:10:37,490
figure out what happened and then

00:10:36,320 --> 00:10:39,260
somebody else hits it and they have to

00:10:37,490 --> 00:10:40,699
figure it all over again and they were

00:10:39,260 --> 00:10:42,800
wasting a lot of reviewer time a lot of

00:10:40,699 --> 00:10:43,970
developer time and there's a lot of

00:10:42,800 --> 00:10:45,769
people are freaking out they don't know

00:10:43,970 --> 00:10:47,630
whether patches failing they have to sit

00:10:45,769 --> 00:10:49,790
down and spend half an hour an hour a

00:10:47,630 --> 00:10:52,010
day a week figure out why the patch

00:10:49,790 --> 00:10:53,360
failed or they just don't care and they

00:10:52,010 --> 00:10:56,990
let everything fail and we have too many

00:10:53,360 --> 00:10:58,850
failures in the system so this is the

00:10:56,990 --> 00:11:01,069
key question or trying to answer so have

00:10:58,850 --> 00:11:03,079
you seen this recently we had an old

00:11:01,069 --> 00:11:04,910
tool was recheck and we started to use a

00:11:03,079 --> 00:11:08,350
we have a new tool now called elastic

00:11:04,910 --> 00:11:10,940
recheck so instead of the stack trace

00:11:08,350 --> 00:11:11,690
have you seen this recently we're using

00:11:10,940 --> 00:11:14,780
cabana

00:11:11,690 --> 00:11:17,060
the front end to actually look at things

00:11:14,780 --> 00:11:18,890
so now we can actually have you type in

00:11:17,060 --> 00:11:22,340
a query so a continent the stack trace

00:11:18,890 --> 00:11:24,680
for example something the console log

00:11:22,340 --> 00:11:26,420
something in an OpenStack log you could

00:11:24,680 --> 00:11:29,210
type it in here and you can actually see

00:11:26,420 --> 00:11:31,100
a plotted over time so we can see if

00:11:29,210 --> 00:11:32,570
it's coming up we could dig into it more

00:11:31,100 --> 00:11:34,820
we have some metadata associated with it

00:11:32,570 --> 00:11:36,680
what jobs it's running on is it a

00:11:34,820 --> 00:11:38,930
postgres job only is it of everything

00:11:36,680 --> 00:11:42,080
job is it a you know a unit test job

00:11:38,930 --> 00:11:43,850
things like that have you seen this

00:11:42,080 --> 00:11:45,260
recently so we take this data are

00:11:43,850 --> 00:11:47,090
actually pushing it back to the reviewer

00:11:45,260 --> 00:11:48,740
so now what happens you push a patch up

00:11:47,090 --> 00:11:50,780
to OpenStack instead of getting these

00:11:48,740 --> 00:11:52,850
failures we actually go and tell you hey

00:11:50,780 --> 00:11:53,840
we think you fail for these reasons so

00:11:52,850 --> 00:11:56,210
now you can actually go through and

00:11:53,840 --> 00:11:59,390
validate that yes you hit some known

00:11:56,210 --> 00:12:00,620
bugs and you can type and recheck

00:11:59,390 --> 00:12:03,410
whatever the bug was and you don't have

00:12:00,620 --> 00:12:05,270
to waste you know half a day trying to

00:12:03,410 --> 00:12:10,310
debug what happened with your patch and

00:12:05,270 --> 00:12:12,710
it wasn't your fault let me see this

00:12:10,310 --> 00:12:14,180
recently so we actually have want to

00:12:12,710 --> 00:12:16,250
actually track the bugs going through

00:12:14,180 --> 00:12:18,710
the system so we actually have a page to

00:12:16,250 --> 00:12:20,780
track the most recent bugs and the

00:12:18,710 --> 00:12:23,510
trends of them so we sort this graph

00:12:20,780 --> 00:12:25,940
this page by 24 hour increments to the

00:12:23,510 --> 00:12:28,760
most recent the top bug in the past 24

00:12:25,940 --> 00:12:30,020
hours and now if you're interested in

00:12:28,760 --> 00:12:31,820
seeing what's failing the system in the

00:12:30,020 --> 00:12:33,020
most common the biggest bug you just go

00:12:31,820 --> 00:12:35,360
to a single page and you list all the

00:12:33,020 --> 00:12:39,590
bugs out and they sorted by the big ones

00:12:35,360 --> 00:12:41,210
of the day so going back to what happens

00:12:39,590 --> 00:12:45,410
when you submit your code with this

00:12:41,210 --> 00:12:47,270
elastic recheck system so this is a bit

00:12:45,410 --> 00:12:50,000
of a crazy looking graph because it's a

00:12:47,270 --> 00:12:51,290
bit of a big crazy system and we have a

00:12:50,000 --> 00:12:53,360
lot of big moving parts in here and it's

00:12:51,290 --> 00:12:54,589
a big asynchronous system as well so the

00:12:53,360 --> 00:12:56,630
test completely go through your job

00:12:54,589 --> 00:12:58,070
runner we take all the artifacts we

00:12:56,630 --> 00:12:59,630
store them in logs that OpenStack 10

00:12:58,070 --> 00:13:00,980
organs and neda mentioned but we also

00:12:59,630 --> 00:13:02,990
take a bunch of the log so you put them

00:13:00,980 --> 00:13:05,780
in log stash at open org which is a big

00:13:02,990 --> 00:13:08,440
elasticsearch cluster I believe it's you

00:13:05,780 --> 00:13:08,440
knowing machine tutors

00:13:12,230 --> 00:13:20,200
yeah so there gets the log stash workers

00:13:18,260 --> 00:13:22,310
are taking the data to processing them

00:13:20,200 --> 00:13:24,230
adding metadata things like that or

00:13:22,310 --> 00:13:26,750
formatting them and then they put them

00:13:24,230 --> 00:13:28,910
into the actual elastic search search

00:13:26,750 --> 00:13:31,160
engine so as you can see from those

00:13:28,910 --> 00:13:32,690
numbers this is a huge system we're only

00:13:31,160 --> 00:13:34,880
able to store about 10 days of logs

00:13:32,690 --> 00:13:37,490
right now and we commonly get backed up

00:13:34,880 --> 00:13:39,230
we don't have enough logstash workers so

00:13:37,490 --> 00:13:44,120
even with this big system it's actually

00:13:39,230 --> 00:13:45,740
quite slow or what yes everything is VMs

00:13:44,120 --> 00:13:48,410
I don't think anyone has any real

00:13:45,740 --> 00:13:49,820
machines and I know of yeah so this is

00:13:48,410 --> 00:13:51,950
all on virtual machines that's correct

00:13:49,820 --> 00:13:53,420
so take all these these logs we don't

00:13:51,950 --> 00:13:55,130
actually store debug logs in here

00:13:53,420 --> 00:13:56,710
because that would be too much data we

00:13:55,130 --> 00:13:59,150
want people to store anything in there

00:13:56,710 --> 00:14:01,160
so this is only ten days of data we went

00:13:59,150 --> 00:14:04,250
down from 14 days because we didn't have

00:14:01,160 --> 00:14:05,990
enough space from there we actually do

00:14:04,250 --> 00:14:08,540
two things with the data one is we build

00:14:05,990 --> 00:14:11,090
the static web pages and we have a job

00:14:08,540 --> 00:14:13,280
that runs every 30 minutes or so and

00:14:11,090 --> 00:14:16,100
updates the elastic recheck page with

00:14:13,280 --> 00:14:18,950
the the unknown bugs I'll get to and the

00:14:16,100 --> 00:14:20,480
list of the most frequent bugs we also

00:14:18,950 --> 00:14:22,910
take that data we hook into the Garrett

00:14:20,480 --> 00:14:24,980
stream the results of your test

00:14:22,910 --> 00:14:26,540
completed and we actually have a bot

00:14:24,980 --> 00:14:28,400
that will go through and run an elastic

00:14:26,540 --> 00:14:30,620
search queries against elasticsearch

00:14:28,400 --> 00:14:32,570
with no fingerprints of the bugs and if

00:14:30,620 --> 00:14:34,640
it finds a bug in one of your jobs it

00:14:32,570 --> 00:14:36,770
will report back on IRC if it's unknown

00:14:34,640 --> 00:14:38,270
bug so if you found a bug in the gate

00:14:36,770 --> 00:14:39,950
job which the gate job she never fail

00:14:38,270 --> 00:14:42,350
because it already has been checking in

00:14:39,950 --> 00:14:43,970
many many many many times and we were

00:14:42,350 --> 00:14:47,180
assuming it's clean will report that

00:14:43,970 --> 00:14:48,920
into an IRC channel for tracking and we

00:14:47,180 --> 00:14:52,610
also if we will report the data back

00:14:48,920 --> 00:14:53,990
into the Garrett system so if you put

00:14:52,610 --> 00:14:56,270
your patch up and it fails because of a

00:14:53,990 --> 00:14:57,590
you know github went down that's not a

00:14:56,270 --> 00:14:59,270
you don't have that anymore thankfully

00:14:57,590 --> 00:15:00,920
but that used to happen I'll report back

00:14:59,270 --> 00:15:03,140
saying hey you failed because of this

00:15:00,920 --> 00:15:05,900
bug we know about it you could run this

00:15:03,140 --> 00:15:07,540
command and you'll retry we don't

00:15:05,900 --> 00:15:09,920
actually automate the retry loop because

00:15:07,540 --> 00:15:11,500
we're human and we're afraid we'll make

00:15:09,920 --> 00:15:14,540
a big mistake and have infinite loop

00:15:11,500 --> 00:15:17,420
furthermore the system fails open so we

00:15:14,540 --> 00:15:19,610
only look for if this failure if this

00:15:17,420 --> 00:15:20,870
known issue hit your job but that

00:15:19,610 --> 00:15:23,570
doesn't mean no other failures hit your

00:15:20,870 --> 00:15:25,320
job so this is to help people but this

00:15:23,570 --> 00:15:28,350
isn't you can't assume nothing else fail

00:15:25,320 --> 00:15:29,760
it and further than where you have to be

00:15:28,350 --> 00:15:32,060
careful about the bug and you know

00:15:29,760 --> 00:15:35,550
always double-check the fingerprints are

00:15:32,060 --> 00:15:36,870
never perfect maybe we have false

00:15:35,550 --> 00:15:40,530
positives all the time and where you try

00:15:36,870 --> 00:15:42,630
to fix those so when we started this

00:15:40,530 --> 00:15:45,320
this is actually started by me and a few

00:15:42,630 --> 00:15:47,970
other people Matt transition Sean degg

00:15:45,320 --> 00:15:49,980
we thought they need to be at six or ten

00:15:47,970 --> 00:15:51,150
bugs in the system we were having a lot

00:15:49,980 --> 00:15:53,100
of problems in the system we weren't

00:15:51,150 --> 00:15:54,900
able to land code everybody is running a

00:15:53,100 --> 00:15:55,980
recheck nothing was happening weeds

00:15:54,900 --> 00:15:57,510
figured that are the must be about six

00:15:55,980 --> 00:15:59,490
or ten big bugs and the system will fix

00:15:57,510 --> 00:16:03,660
those six or ten bugs and that'll be it

00:15:59,490 --> 00:16:05,760
everything will work it'll be great it

00:16:03,660 --> 00:16:07,320
turns out we were completely wrong there

00:16:05,760 --> 00:16:10,350
was I think right now we're tracking

00:16:07,320 --> 00:16:12,770
about a hundred and thirty bugs today I

00:16:10,350 --> 00:16:15,150
think it's 136 why check this morning

00:16:12,770 --> 00:16:16,620
it's not six or ten bucks six or ten

00:16:15,150 --> 00:16:18,240
bugs is what a human could sort of

00:16:16,620 --> 00:16:20,580
remember and that goes back to the have

00:16:18,240 --> 00:16:22,350
you seen this bug I can't remember you

00:16:20,580 --> 00:16:24,840
know 130 bugs but the system can I

00:16:22,350 --> 00:16:27,030
commemorate six or ten bugs at best so

00:16:24,840 --> 00:16:29,040
that's why we sort of saw that it turns

00:16:27,030 --> 00:16:30,510
out we have all kinds of crazy bugs we

00:16:29,040 --> 00:16:32,340
have a lot of the bugs are well under

00:16:30,510 --> 00:16:34,320
one percent failure rates we have bugs

00:16:32,340 --> 00:16:37,800
will hit once a week and those are

00:16:34,320 --> 00:16:39,810
consistent we have all kinds of bugs in

00:16:37,800 --> 00:16:41,970
the system we have some upstream service

00:16:39,810 --> 00:16:47,250
problems pipe I will go down

00:16:41,970 --> 00:16:50,400
occasionally github dns issues will have

00:16:47,250 --> 00:16:53,760
upstream vendor problems upstream at

00:16:50,400 --> 00:16:54,840
marys we use that marries or not always

00:16:53,760 --> 00:16:55,950
perfect sometimes a little break you'll

00:16:54,840 --> 00:16:57,450
get in a strange state when they're

00:16:55,950 --> 00:17:00,140
getting updated on the back end and

00:16:57,450 --> 00:17:02,310
that'll break all kinds of things for us

00:17:00,140 --> 00:17:04,530
as a result of this we catch a lot of

00:17:02,310 --> 00:17:05,640
things and we assume touching the

00:17:04,530 --> 00:17:07,350
network to pull things down it's

00:17:05,640 --> 00:17:09,150
dangerous and we try to measure

00:17:07,350 --> 00:17:11,520
everything we can ourselves we have our

00:17:09,150 --> 00:17:15,120
own pipe I'm error I don't think we have

00:17:11,520 --> 00:17:17,520
our mayor yet but that's in the works we

00:17:15,120 --> 00:17:19,589
don't pull things from github we pull

00:17:17,520 --> 00:17:21,209
things we cache things from our or get

00:17:19,589 --> 00:17:23,600
servers when possible and all kinds of

00:17:21,209 --> 00:17:30,770
things like that

00:17:23,600 --> 00:17:32,510
I think we pull down installing pip from

00:17:30,770 --> 00:17:35,630
github and it makes me cringe every time

00:17:32,510 --> 00:17:41,720
I see it I is that I think there's only

00:17:35,630 --> 00:17:44,809
one and I don't think it's 0 anyway so

00:17:41,720 --> 00:17:46,130
those are still there I don't think

00:17:44,809 --> 00:17:48,140
they're cached I'm not sure though

00:17:46,130 --> 00:17:50,630
that's a good question we have some

00:17:48,140 --> 00:17:52,070
infrared get bad node pool images we've

00:17:50,630 --> 00:17:53,240
had this I think we actually have this

00:17:52,070 --> 00:17:55,460
all the time this is actually pretty bad

00:17:53,240 --> 00:17:58,309
one it'd be nice if we had better way of

00:17:55,460 --> 00:17:59,690
getting around that service outages

00:17:58,309 --> 00:18:01,100
we'll get that all the time something

00:17:59,690 --> 00:18:02,059
happens with the upstream you know

00:18:01,100 --> 00:18:04,880
provide or something you know

00:18:02,059 --> 00:18:06,679
unavoidable and marriage break it

00:18:04,880 --> 00:18:08,659
happens we also have a lot of bugs in

00:18:06,679 --> 00:18:09,860
OpenStack it's a big system it's written

00:18:08,659 --> 00:18:13,159
in Python it's written by a lot of

00:18:09,860 --> 00:18:14,059
people and it's it's a big asynchronous

00:18:13,159 --> 00:18:17,090
system so it's going to have a lot of

00:18:14,059 --> 00:18:18,830
issues in it state corruption races with

00:18:17,090 --> 00:18:20,330
asynchronous messaging it's a really

00:18:18,830 --> 00:18:21,500
really big a sinker in a system with

00:18:20,330 --> 00:18:23,330
lots and lots of eight synchronous

00:18:21,500 --> 00:18:25,520
workers so you get a lot of these

00:18:23,330 --> 00:18:26,840
problems I'm database 10 blocks we got a

00:18:25,520 --> 00:18:28,820
new big one today or the other day I

00:18:26,840 --> 00:18:30,860
think actually we always have a few of

00:18:28,820 --> 00:18:33,350
these in the system once again a lot of

00:18:30,860 --> 00:18:36,289
workers it's a big system this happens a

00:18:33,350 --> 00:18:38,570
lot unfortunately races with multiple

00:18:36,289 --> 00:18:41,750
workers all kinds of things we had

00:18:38,570 --> 00:18:45,320
issues with we had a keystone middleware

00:18:41,750 --> 00:18:47,659
problem where it was it wasn't updating

00:18:45,320 --> 00:18:49,460
files atomically we get you name it

00:18:47,659 --> 00:18:51,200
we've seen it before I mean these things

00:18:49,460 --> 00:18:52,820
sometimes hit often sometimes they'll

00:18:51,200 --> 00:18:54,409
hit you know thirty percent to the jobs

00:18:52,820 --> 00:18:56,360
sometimes we'll hit one percent of the

00:18:54,409 --> 00:18:59,539
jobs one pretend the job is still

00:18:56,360 --> 00:19:02,570
massive we have bugs and tests will have

00:18:59,539 --> 00:19:04,730
untape unsafe global state so once we

00:19:02,570 --> 00:19:06,110
hit multiple multiple workers all of a

00:19:04,730 --> 00:19:13,130
sudden these unsafe state problems were

00:19:06,110 --> 00:19:14,210
really apparent to us what yes this

00:19:13,130 --> 00:19:15,440
still single no that's a great point

00:19:14,210 --> 00:19:18,020
we're actually running everything in one

00:19:15,440 --> 00:19:21,230
node so OpenStack is designed to run on

00:19:18,020 --> 00:19:22,760
many many machines but for restricted

00:19:21,230 --> 00:19:24,409
you know resource problems running this

00:19:22,760 --> 00:19:29,150
integration environment on one node for

00:19:24,409 --> 00:19:30,770
now and we have all these bugs right

00:19:29,150 --> 00:19:32,690
that's changing soon and we'll see how

00:19:30,770 --> 00:19:34,970
that goes that should be interesting bug

00:19:32,690 --> 00:19:37,130
wise comparing type stance when you fix

00:19:34,970 --> 00:19:39,530
things to run in parallel things you

00:19:37,130 --> 00:19:41,420
every soft and somebody puts a you know

00:19:39,530 --> 00:19:43,010
a bad test and that checks a timestamp

00:19:41,420 --> 00:19:45,320
it runs too fast or too slow and it

00:19:43,010 --> 00:19:46,250
breaks and we fixed a lot of these bugs

00:19:45,320 --> 00:19:48,530
and I don't think we're seeing too many

00:19:46,250 --> 00:19:50,990
of these now we've seen bugs and

00:19:48,530 --> 00:19:54,440
dependencies we've seen libert bugs I

00:19:50,990 --> 00:19:55,940
think constantly unfortunately we

00:19:54,440 --> 00:19:58,460
actually are really aggressive on Lebert

00:19:55,940 --> 00:20:00,110
where we're hitting it all our tests run

00:19:58,460 --> 00:20:01,460
alavert right now on the gate so we

00:20:00,110 --> 00:20:04,400
running you know thousands of these

00:20:01,460 --> 00:20:06,020
libvirt jobs a day on each jobs about an

00:20:04,400 --> 00:20:09,410
hour long we're spinning up you know

00:20:06,020 --> 00:20:11,300
over a hundred qmu machines inside of it

00:20:09,410 --> 00:20:12,830
so actually really abusive alavert today

00:20:11,300 --> 00:20:15,140
and that's one of the reasons why we see

00:20:12,830 --> 00:20:16,640
so many problems in it we've seen some

00:20:15,140 --> 00:20:18,530
really bizarre bugs we've seen a bug

00:20:16,640 --> 00:20:20,390
with the colonel network block device if

00:20:18,530 --> 00:20:22,790
you're doing file injection and you had

00:20:20,390 --> 00:20:25,310
a neutron set up on the same machine it

00:20:22,790 --> 00:20:26,600
would break things and why this happens

00:20:25,310 --> 00:20:29,840
i'm actually really not it's not clear

00:20:26,600 --> 00:20:31,430
to me still but we've like you want to

00:20:29,840 --> 00:20:33,530
think oh while injection would break you

00:20:31,430 --> 00:20:35,750
know neutron that's it's not supposed to

00:20:33,530 --> 00:20:38,200
work that way but we've seen things like

00:20:35,750 --> 00:20:42,410
that we've seen a problem and we had a

00:20:38,200 --> 00:20:44,480
file descriptor leak in HTTP lib to that

00:20:42,410 --> 00:20:46,880
it was causing crosstalk between our PC

00:20:44,480 --> 00:20:49,070
and rest api's who are getting rest

00:20:46,880 --> 00:20:51,860
calls and RPC buses and vice versa and

00:20:49,070 --> 00:20:53,510
that took I think that a month of you

00:20:51,860 --> 00:20:57,170
know three or four people solid to fix

00:20:53,510 --> 00:21:00,380
it we see the craziest bugs it's really

00:20:57,170 --> 00:21:01,610
bizarre and we usually get through them

00:21:00,380 --> 00:21:03,320
but sometimes they take months and

00:21:01,610 --> 00:21:04,490
months and months and that's where it's

00:21:03,320 --> 00:21:06,410
really nice people attract these bugs

00:21:04,490 --> 00:21:08,960
and see how you know be able to triage

00:21:06,410 --> 00:21:12,140
done really well we've actually had to

00:21:08,960 --> 00:21:14,090
get Liberty versions backported from

00:21:12,140 --> 00:21:16,460
canonical for us and all kinds of things

00:21:14,090 --> 00:21:19,070
in fact canonical has a pretty same

00:21:16,460 --> 00:21:20,960
policy of asking for test two you know

00:21:19,070 --> 00:21:21,980
detect the bug for aggressions and we

00:21:20,960 --> 00:21:23,660
told them we don't have one and they

00:21:21,980 --> 00:21:26,030
they took a while for us to convince

00:21:23,660 --> 00:21:27,530
them that but you know we could barely

00:21:26,030 --> 00:21:29,960
you know reproducing it means running

00:21:27,530 --> 00:21:31,660
you know a thousand jobs and we do that

00:21:29,960 --> 00:21:35,330
every day so that's easy for us but for

00:21:31,660 --> 00:21:37,610
economical that's a little harder so a

00:21:35,330 --> 00:21:39,770
contributing pattern so a pattern for a

00:21:37,610 --> 00:21:43,010
bug but what happens you know your patch

00:21:39,770 --> 00:21:44,720
fails you find the bug let's say and you

00:21:43,010 --> 00:21:45,920
want to write a query for it said you go

00:21:44,720 --> 00:21:47,240
through the logs you find a query front

00:21:45,920 --> 00:21:50,150
of stack trace that's usually a good

00:21:47,240 --> 00:21:51,680
telltale sign of a bug

00:21:50,150 --> 00:21:54,200
we use elastic search for this so it's

00:21:51,680 --> 00:21:55,310
the lucene queries language and

00:21:54,200 --> 00:21:58,430
everything's in get so you can actually

00:21:55,310 --> 00:22:00,080
go to get that open text at Oregon go to

00:21:58,430 --> 00:22:01,910
open second for an elastic recheck and

00:22:00,080 --> 00:22:04,790
put a patch into the queries and the

00:22:01,910 --> 00:22:06,710
format here is just a bug number and

00:22:04,790 --> 00:22:08,150
then dot yambol and then it's a you know

00:22:06,710 --> 00:22:09,980
query and then it's the actual query

00:22:08,150 --> 00:22:11,450
itself you put that up and you're right

00:22:09,980 --> 00:22:13,880
a little you know commit message and you

00:22:11,450 --> 00:22:16,070
push it in and now we can actually track

00:22:13,880 --> 00:22:18,620
this bug and report back on individuals

00:22:16,070 --> 00:22:19,970
commits an individual patches if they

00:22:18,620 --> 00:22:22,990
see that bug and we can actually track

00:22:19,970 --> 00:22:24,890
into the priority of in the trend of it

00:22:22,990 --> 00:22:26,240
we have another thing we could do with

00:22:24,890 --> 00:22:29,180
this data now that we actually are able

00:22:26,240 --> 00:22:31,520
to classify failures instead of just

00:22:29,180 --> 00:22:32,930
going through and classifying failure as

00:22:31,520 --> 00:22:35,090
a failure comes in will actually go

00:22:32,930 --> 00:22:37,370
through over the previous failures in

00:22:35,090 --> 00:22:38,960
the system and will actually try to you

00:22:37,370 --> 00:22:40,790
have a list here of unclassified failed

00:22:38,960 --> 00:22:42,650
jobs so this is really nice for us if

00:22:40,790 --> 00:22:44,090
you want to help us out here this is a

00:22:42,650 --> 00:22:45,860
great place to start you look at a job

00:22:44,090 --> 00:22:47,330
we have the list of the jobs on the

00:22:45,860 --> 00:22:49,640
right and we break this down to either

00:22:47,330 --> 00:22:52,430
click it one day to day seven day or ten

00:22:49,640 --> 00:22:54,530
day list and this is a list of all the

00:22:52,430 --> 00:22:56,660
jobs that have failed for unknown

00:22:54,530 --> 00:22:59,090
reasons so we try to keep the

00:22:56,660 --> 00:23:01,790
classification rate categorization rate

00:22:59,090 --> 00:23:03,050
about eighty ninety percent anything

00:23:01,790 --> 00:23:05,570
lower than that means we have a really

00:23:03,050 --> 00:23:06,620
serious bug in fact we caught a bug

00:23:05,570 --> 00:23:08,000
today and that was one of the reasons

00:23:06,620 --> 00:23:09,530
why I think our classification airy

00:23:08,000 --> 00:23:11,150
today is about eighty percent maybe a

00:23:09,530 --> 00:23:14,410
little lower and that's because we had a

00:23:11,150 --> 00:23:18,050
big bug in there that we just caught

00:23:14,410 --> 00:23:19,880
some next steps one of the big things

00:23:18,050 --> 00:23:21,170
are trying to do is one of the problems

00:23:19,880 --> 00:23:23,060
we have this we have with the system

00:23:21,170 --> 00:23:24,500
today is that when things get really bad

00:23:23,060 --> 00:23:26,750
many of you who've worked in OpenStack

00:23:24,500 --> 00:23:28,190
have probably seen this where the queue

00:23:26,750 --> 00:23:30,230
builds up everything is getting thrashed

00:23:28,190 --> 00:23:31,580
you put a patch up it doesn't work

00:23:30,230 --> 00:23:33,200
you're right you call recheck and

00:23:31,580 --> 00:23:34,790
everybody else is doing the same thing

00:23:33,200 --> 00:23:36,890
so now we have a we have a finite number

00:23:34,790 --> 00:23:39,380
of workers and now we've exhausted our

00:23:36,890 --> 00:23:40,940
quota for resources and everything is

00:23:39,380 --> 00:23:42,800
taking hours and hours to run and

00:23:40,940 --> 00:23:44,090
everything just really really slow all

00:23:42,800 --> 00:23:45,950
of a sudden it's really painful for

00:23:44,090 --> 00:23:47,090
everybody and this generally happens

00:23:45,950 --> 00:23:48,380
when you have too many bugs in the

00:23:47,090 --> 00:23:50,840
system so we're always going to have a

00:23:48,380 --> 00:23:52,190
few letting bugs in the system that sort

00:23:50,840 --> 00:23:54,080
of expected but they build up to certain

00:23:52,190 --> 00:23:56,120
threshold before you notice them and by

00:23:54,080 --> 00:23:57,080
that point it's really too late so one

00:23:56,120 --> 00:23:59,560
of the things we're trying to do is

00:23:57,080 --> 00:24:02,330
actually we have a manual process to

00:23:59,560 --> 00:24:04,070
promote bug fixes to the top of the gate

00:24:02,330 --> 00:24:06,230
Q so they can skip ahead

00:24:04,070 --> 00:24:07,490
and we're actually when the plans we

00:24:06,230 --> 00:24:09,380
discuss actually this mid cycle in

00:24:07,490 --> 00:24:10,640
Germany is to automatically be able to

00:24:09,380 --> 00:24:13,490
promote bug fixes if they have a

00:24:10,640 --> 00:24:14,960
fingerprint the elastic rechecked so

00:24:13,490 --> 00:24:17,120
this should help the process of actually

00:24:14,960 --> 00:24:18,920
getting these bug fix so if everything's

00:24:17,120 --> 00:24:20,090
really really broken one day and we have

00:24:18,920 --> 00:24:21,170
a patch of well think we'll fix a lot of

00:24:20,090 --> 00:24:23,930
things we could put this in and

00:24:21,170 --> 00:24:27,350
automatically promote it to the top and

00:24:23,930 --> 00:24:28,310
hopefully get things fixed quicker what

00:24:27,350 --> 00:24:30,530
a better better job of actually

00:24:28,310 --> 00:24:32,060
presenting this data to people there's a

00:24:30,530 --> 00:24:34,010
bit of sort of hidden knowledge of this

00:24:32,060 --> 00:24:36,470
you have to you know stare it along

00:24:34,010 --> 00:24:37,880
enough to figure it out you want to

00:24:36,470 --> 00:24:39,800
build up you know present patterns and

00:24:37,880 --> 00:24:40,970
patterns better so we have a failure

00:24:39,800 --> 00:24:44,240
have a fingerprint we could show the

00:24:40,970 --> 00:24:45,800
trend of the fingerprint but you know is

00:24:44,240 --> 00:24:48,260
this a postgres the only failures is how

00:24:45,800 --> 00:24:50,990
one providers it's a rackspace or HP you

00:24:48,260 --> 00:24:52,340
no problem there's other ways we just

00:24:50,990 --> 00:24:53,750
Eliza did we have a lot of metadata and

00:24:52,340 --> 00:24:56,420
being able to present that metadata much

00:24:53,750 --> 00:24:58,850
easier you could go to the to actually

00:24:56,420 --> 00:25:00,350
go to logs logs that OpenStack that are

00:24:58,850 --> 00:25:01,490
going to see that but it's not really

00:25:00,350 --> 00:25:03,920
that easy if you don't know what you're

00:25:01,490 --> 00:25:05,990
looking for well the problem you have

00:25:03,920 --> 00:25:08,720
now is we can tell you this Pat this bug

00:25:05,990 --> 00:25:09,740
was seen 80 times the past 24 hours but

00:25:08,720 --> 00:25:11,390
that doesn't mean anything like that

00:25:09,740 --> 00:25:13,400
just means that's a pisser wrong number

00:25:11,390 --> 00:25:14,750
but trying to put that in percentages

00:25:13,400 --> 00:25:17,960
actually haven't done that yet so we

00:25:14,750 --> 00:25:19,430
can't say this bug has caused ten

00:25:17,960 --> 00:25:22,100
percent of the failures and past 24

00:25:19,430 --> 00:25:23,960
hours or even ten percent of the you

00:25:22,100 --> 00:25:25,040
know the jobs to fail so you don't

00:25:23,960 --> 00:25:27,500
actually have any nice ways of doing

00:25:25,040 --> 00:25:29,090
things like that yet another thing we

00:25:27,500 --> 00:25:31,010
want to do and we talked about this

00:25:29,090 --> 00:25:33,320
actually recently spoke some people

00:25:31,010 --> 00:25:35,060
about this is packaging except for

00:25:33,320 --> 00:25:38,420
downstream consumers it turns out

00:25:35,060 --> 00:25:39,980
running a large they're running tons and

00:25:38,420 --> 00:25:41,900
tons of small OpenStack clouds is a very

00:25:39,980 --> 00:25:44,240
similar turning 11 really really big

00:25:41,900 --> 00:25:47,090
open sack cloud for a very very long

00:25:44,240 --> 00:25:48,320
time and in fact it actually is even

00:25:47,090 --> 00:25:49,610
better than that because by the time we

00:25:48,320 --> 00:25:51,410
ship code we actually are shipping with

00:25:49,610 --> 00:25:53,570
known bugs and we have a list of the

00:25:51,410 --> 00:25:55,250
known bugs in here now and so if you're

00:25:53,570 --> 00:25:57,290
a downstream consumer of OpenStack and

00:25:55,250 --> 00:25:58,580
you want to you know monitor your system

00:25:57,290 --> 00:26:00,590
and debugging and things like that make

00:25:58,580 --> 00:26:02,030
sure it's working we could actually we

00:26:00,590 --> 00:26:05,030
would like to make elastic recheck work

00:26:02,030 --> 00:26:06,770
for them as well and we can actually

00:26:05,030 --> 00:26:10,370
report you could actually see when a

00:26:06,770 --> 00:26:12,170
known failure is hit you and another

00:26:10,370 --> 00:26:13,670
thing we have a problem with is is where

00:26:12,170 --> 00:26:15,110
have a huge elasticsearch cluster

00:26:13,670 --> 00:26:17,280
they're trying to figure out a better

00:26:15,110 --> 00:26:19,200
way to move forward we

00:26:17,280 --> 00:26:20,730
some ideas been you know been floating

00:26:19,200 --> 00:26:24,180
around for a little bit what idea is to

00:26:20,730 --> 00:26:25,770
move away put a Jason lager in and the

00:26:24,180 --> 00:26:27,570
problem we have now is where you know we

00:26:25,770 --> 00:26:29,330
take all this formatted data from Python

00:26:27,570 --> 00:26:33,180
we're putting it in you know random

00:26:29,330 --> 00:26:36,920
junkin in the log formats then we r you

00:26:33,180 --> 00:26:39,180
know parsing it again on the log stash

00:26:36,920 --> 00:26:41,360
and that's a lot of extra work that need

00:26:39,180 --> 00:26:43,260
to happen where you know serializing and

00:26:41,360 --> 00:26:45,240
putting in a string and taking it out of

00:26:43,260 --> 00:26:47,880
a string for no reason we probably cut

00:26:45,240 --> 00:26:49,650
that whole step out and save a lot of

00:26:47,880 --> 00:26:51,750
work for ourselves and so we're trying

00:26:49,650 --> 00:26:53,370
to every so often will actually see due

00:26:51,750 --> 00:26:55,110
to some assorted problems will actually

00:26:53,370 --> 00:26:56,130
see the elastic source cluster get

00:26:55,110 --> 00:26:57,900
backed up and trying to figure out how

00:26:56,130 --> 00:27:01,980
to do a better job of making that not

00:26:57,900 --> 00:27:03,690
happen I'd like to thank everybody who's

00:27:01,980 --> 00:27:04,530
contributed to Las agree check a lot of

00:27:03,690 --> 00:27:07,830
these even people who have put

00:27:04,530 --> 00:27:09,180
fingerprints in and if you haven't yet

00:27:07,830 --> 00:27:11,340
it be great if you go and give us a hand

00:27:09,180 --> 00:27:12,690
on this one another actually really big

00:27:11,340 --> 00:27:14,060
part of this is actually fixing the bugs

00:27:12,690 --> 00:27:17,130
this is really the first step in

00:27:14,060 --> 00:27:19,200
wrangling some of the failures we have

00:27:17,130 --> 00:27:21,120
an open stack the first step is knowing

00:27:19,200 --> 00:27:22,680
why it's failing so now we actually are

00:27:21,120 --> 00:27:24,630
give a pretty good insight into why it's

00:27:22,680 --> 00:27:26,660
failing and the hard part is really

00:27:24,630 --> 00:27:28,590
fixing those failures and so we have

00:27:26,660 --> 00:27:30,720
I'll show you this in a second we have a

00:27:28,590 --> 00:27:31,830
huge page of bugs that are open that are

00:27:30,720 --> 00:27:35,580
causing all kinds of painting for

00:27:31,830 --> 00:27:40,980
everybody so thank you i like to show

00:27:35,580 --> 00:27:43,220
you one more thing so this is not

00:27:40,980 --> 00:27:43,220
working

00:27:48,490 --> 00:27:55,710
see if there we go so this is the page

00:27:51,520 --> 00:27:58,059
today so you see here this is the

00:27:55,710 --> 00:27:59,830
biggest bug we have now I think this was

00:27:58,059 --> 00:28:01,960
caused by a patch recently proposed in

00:27:59,830 --> 00:28:04,000
noble as a database migration and we

00:28:01,960 --> 00:28:05,110
need to sort out the proper way to fix

00:28:04,000 --> 00:28:07,870
that because it can actually revert

00:28:05,110 --> 00:28:08,770
database migrations because we have to

00:28:07,870 --> 00:28:10,570
assume somebody's been using the

00:28:08,770 --> 00:28:12,160
database migration and we can't break

00:28:10,570 --> 00:28:14,860
the migration path otherwise we could

00:28:12,160 --> 00:28:17,760
get in weird States for upgrading we

00:28:14,860 --> 00:28:20,230
have a deadlock here in the scheduler

00:28:17,760 --> 00:28:21,550
all kinds of crazy bugs we actually so

00:28:20,230 --> 00:28:26,440
here you can see this is a nova bug in

00:28:21,550 --> 00:28:28,570
progress this one is a CI bug note

00:28:26,440 --> 00:28:32,170
allocated node polar very slow I just

00:28:28,570 --> 00:28:33,910
love that title we have all kinds of

00:28:32,170 --> 00:28:35,620
things here this is you can see the new

00:28:33,910 --> 00:28:37,179
one so this is a nice example of why

00:28:35,620 --> 00:28:40,030
this is really useful we can actually

00:28:37,179 --> 00:28:41,650
see this bug came up on July thirtieth

00:28:40,030 --> 00:28:43,630
so you can actually go through the logs

00:28:41,650 --> 00:28:45,340
and OpenStack and we could actually see

00:28:43,630 --> 00:28:47,830
look for anything that you know sticks

00:28:45,340 --> 00:28:48,850
out so we found the stack trace and you

00:28:47,830 --> 00:28:50,080
can actually go with you it took about

00:28:48,850 --> 00:28:52,120
five minutes to go through the open

00:28:50,080 --> 00:28:53,530
stack logs and see oh this was you know

00:28:52,120 --> 00:28:57,940
committed on this day in this window

00:28:53,530 --> 00:29:02,050
it's probably the cause we've had this

00:28:57,940 --> 00:29:03,670
grenade bug for quite a long time we

00:29:02,050 --> 00:29:05,679
have all kinds of crazy bugs you know

00:29:03,670 --> 00:29:07,630
new releases of things break things cql

00:29:05,679 --> 00:29:09,760
kameez 095 broke the world and it

00:29:07,630 --> 00:29:10,990
apparently came back this may be a fault

00:29:09,760 --> 00:29:14,170
of red herring actually this may be a

00:29:10,990 --> 00:29:16,330
different bug that it caused the same

00:29:14,170 --> 00:29:17,350
fingerprint to to trigger this is

00:29:16,330 --> 00:29:18,700
actually when the problems we have right

00:29:17,350 --> 00:29:21,280
now and it's a hard one to fix is

00:29:18,700 --> 00:29:23,309
getting really good fingerprints the

00:29:21,280 --> 00:29:25,630
logs are really not great all the time

00:29:23,309 --> 00:29:28,030
and we try to do our best job of finding

00:29:25,630 --> 00:29:29,980
a single line that will make do you know

00:29:28,030 --> 00:29:31,210
show a bug happen but we can't it's not

00:29:29,980 --> 00:29:34,690
all it doesn't always work sometimes

00:29:31,210 --> 00:29:35,920
multiple bugs will hit the same thing as

00:29:34,690 --> 00:29:40,080
you can see this list goes on and on and

00:29:35,920 --> 00:29:42,429
on and we have bugs this happen you know

00:29:40,080 --> 00:29:44,320
one time of the past 24 hours and 10

00:29:42,429 --> 00:29:46,420
times in 10 days so this is a really

00:29:44,320 --> 00:29:52,860
serious bug for us but in the grand

00:29:46,420 --> 00:29:56,200
scheme of things it's not here's one

00:29:52,860 --> 00:29:58,690
this happens very rarely and it keeps

00:29:56,200 --> 00:30:00,370
going down the list so we don't have

00:29:58,690 --> 00:30:01,850
just a few big bugs we have tons and

00:30:00,370 --> 00:30:05,600
tons of small ones

00:30:01,850 --> 00:30:12,260
or Apache failing to start happens very

00:30:05,600 --> 00:30:15,130
rarely but it still happens then we have

00:30:12,260 --> 00:30:17,660
another page which is our unclassified

00:30:15,130 --> 00:30:20,630
failure rate as you see here we actually

00:30:17,660 --> 00:30:22,010
break it down so we could look at the

00:30:20,630 --> 00:30:24,289
past 24 hours you can see this one has a

00:30:22,010 --> 00:30:26,059
huge number of failures the past 24

00:30:24,289 --> 00:30:29,320
hours there's actually a patch up to fix

00:30:26,059 --> 00:30:32,299
this one or to detect this fingerprint I

00:30:29,320 --> 00:30:35,780
think actually look on the right here

00:30:32,299 --> 00:30:37,070
and you can see you know if you're

00:30:35,780 --> 00:30:38,780
working a project you can pick your job

00:30:37,070 --> 00:30:40,190
off this list and subtract it and take a

00:30:38,780 --> 00:30:42,620
look and make sure you don't have too

00:30:40,190 --> 00:30:44,000
many failures in it we get really

00:30:42,620 --> 00:30:50,630
strange going to test failures every so

00:30:44,000 --> 00:30:51,919
often see you this will load so matter

00:30:50,630 --> 00:30:53,270
what the job we were probably seeing

00:30:51,919 --> 00:30:55,010
these failures and these are all

00:30:53,270 --> 00:30:56,780
failures in the gate Q only because we

00:30:55,010 --> 00:30:58,490
assume the check he was full of too much

00:30:56,780 --> 00:31:00,799
dirty data but the gate QR assuming

00:30:58,490 --> 00:31:05,809
everything has had a pass several rounds

00:31:00,799 --> 00:31:07,070
of testing before makes it in and my

00:31:05,809 --> 00:31:11,900
connection is apparently really slow

00:31:07,070 --> 00:31:14,260
right now so no logs for you any

00:31:11,900 --> 00:31:14,260
questions

00:31:19,760 --> 00:31:21,820

YouTube URL: https://www.youtube.com/watch?v=4YTz3qUBxSk


