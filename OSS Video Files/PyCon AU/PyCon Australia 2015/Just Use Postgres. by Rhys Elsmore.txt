Title: Just Use Postgres. by Rhys Elsmore
Publication date: 2015-08-04
Playlist: PyCon Australia 2015
Description: 
	Making use of the latest database for your data is considered trendy and edgy, with vendors promising unrealistic expectations when it comes to capabilities of these systems. Furthermore, communities such as Hacker News enforce the idea that you must use the latest and coolest technology in order to be a ‘rockstar’ engineer.

Marketing buzzwords like ‘real-time’, ‘distributed’, ‘high-availability’, and ‘schemaless’ impact our design decisions, and the expectation for applications to be ‘scalable’ often forces developers to prematurely introduce complexity and anti-patterns into their applications by making use of these untested and immature data stores.

Forget about adding the latest trendy data store into your stack; just use Postgres.

This talk will explore the capabilities of Postgres 9.4, and various use-cases where you can involve it as your primary datastore. I will cover some of the projects at Heroku where we have used Postgres, such as:

- Quickly querying over 300M records to get a list of all users on a particular server at a particular point in time.
- Providing almost real-time aggregations of all Heroku customer’s usage.
- Joining 3 different tables on 3 different database servers without any external code.
- Creating a ‘social graph’ of all of Heroku’s users based on commonly used IP addresses.
- Ensuring that coupon codes are used only once, preventing attackers from exploiting ‘race condition’ bugs.

 In particular, I will focus on topics such as:

- Storing, indexing, and querying schemaless data.
- Materialized Views.
- NOTIFY/LISTEN.
- Storing and querying time-series data.
- Updatable views.
- Foreign tables.
- Recursive views.
- Stored functions.
- Range Types
- Partitioning and Sharding data.

Each topic will include links to sample schemas and iPython notebooks so you can take these examples home with you.

PyCon Australia is the national conference for users of the Python Programming Language. In 2015, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

July 31-August 4, Brisbane, Queensland, Australia
Captions: 
	00:00:09,590 --> 00:00:15,450
all right well I present to you rice

00:00:12,870 --> 00:00:19,020
else more rice is an Internet more cup

00:00:15,450 --> 00:00:22,800
at Hiroko based on the platform security

00:00:19,020 --> 00:00:25,170
team the rest of most some of his day

00:00:22,800 --> 00:00:27,989
usually consists of mushy strong coffee

00:00:25,170 --> 00:00:32,910
and making up fake radio jingles about

00:00:27,989 --> 00:00:41,850
items in his kitchen but let's give it

00:00:32,910 --> 00:00:43,800
up for rise are we doing yeah yeah oh

00:00:41,850 --> 00:00:45,810
yeah so anyway thank you

00:00:43,800 --> 00:00:47,640
ah so yeah I'm Reese I'm on the security

00:00:45,810 --> 00:00:51,480
team at Heroku and I make sure that our

00:00:47,640 --> 00:00:53,850
platforms not abused or broken into so

00:00:51,480 --> 00:00:55,829
this is my talk basically the concept is

00:00:53,850 --> 00:00:58,170
to just use post press and use something

00:00:55,829 --> 00:00:59,610
else when it stops working hint I

00:00:58,170 --> 00:01:03,600
haven't found it to stop working for

00:00:59,610 --> 00:01:04,739
many of the use cases so for this talk I

00:01:03,600 --> 00:01:07,409
want you to bear with me normal heart

00:01:04,739 --> 00:01:09,720
rate 60 70 if it bit says 130 at the

00:01:07,409 --> 00:01:11,070
moment your guinea pigs for different

00:01:09,720 --> 00:01:13,350
sort of presenter notes to what I

00:01:11,070 --> 00:01:15,630
usually do so if I have to go back some

00:01:13,350 --> 00:01:19,350
slides or I stumble or I stop please

00:01:15,630 --> 00:01:21,210
bear with me yeah make me a little bit

00:01:19,350 --> 00:01:23,220
nervous but we'll get through it so

00:01:21,210 --> 00:01:25,049
there's a repo if you go there at the

00:01:23,220 --> 00:01:26,610
moment it is empty I've got about ten

00:01:25,049 --> 00:01:27,630
ipython notebooks I'm gonna be putting

00:01:26,610 --> 00:01:29,640
in there and I want to turn it into a

00:01:27,630 --> 00:01:31,020
sort of once a week I take an issue I

00:01:29,640 --> 00:01:35,820
turn it into an I Python notebook to

00:01:31,020 --> 00:01:36,659
solve an issue with Postgres so this is

00:01:35,820 --> 00:01:38,820
what I'm going to be stepping through

00:01:36,659 --> 00:01:40,890
basically the pain of many services

00:01:38,820 --> 00:01:43,350
 services or services that

00:01:40,890 --> 00:01:45,540
talk to each other going through some

00:01:43,350 --> 00:01:48,540
basically all in one part just some

00:01:45,540 --> 00:01:51,860
real-world use cases of Postgres beyond

00:01:48,540 --> 00:01:54,030
your normal selects group buys ordering

00:01:51,860 --> 00:01:55,170
well so gonna be talking about some of

00:01:54,030 --> 00:01:57,570
the things you didn't know the Postgres

00:01:55,170 --> 00:02:00,020
could do you can actually do lists and

00:01:57,570 --> 00:02:02,880
notify and pub/sub in your database and

00:02:00,020 --> 00:02:05,430
I'm gonna also cover things that you

00:02:02,880 --> 00:02:07,049
shouldn't make Postgres do so I'm gonna

00:02:05,430 --> 00:02:08,250
be talking about security Van ronk won't

00:02:07,049 --> 00:02:10,049
sorry

00:02:08,250 --> 00:02:11,730
security vulnerabilities by the way of

00:02:10,049 --> 00:02:15,420
race conditions so that should be kind

00:02:11,730 --> 00:02:17,659
of interesting if you do application

00:02:15,420 --> 00:02:19,950
developer development and you've been

00:02:17,659 --> 00:02:21,659
sort of lured by the case of micro

00:02:19,950 --> 00:02:23,459
services then you're going to understand

00:02:21,659 --> 00:02:27,329
this I like to call it software

00:02:23,459 --> 00:02:30,780
oriented architecture basically it's

00:02:27,329 --> 00:02:31,170
just pain so you've got a layout like

00:02:30,780 --> 00:02:33,019
this

00:02:31,170 --> 00:02:35,069
and you get a request from flask

00:02:33,019 --> 00:02:36,329
basically you're storing an order within

00:02:35,069 --> 00:02:38,280
a Postgres database and then you're

00:02:36,329 --> 00:02:40,109
calling celery to shove that to take

00:02:38,280 --> 00:02:42,750
that order out of Postgres and shove it

00:02:40,109 --> 00:02:45,269
into register case then you might shove

00:02:42,750 --> 00:02:46,409
it into neo4j and then query neo4j for a

00:02:45,269 --> 00:02:47,790
network of other people who have placed

00:02:46,409 --> 00:02:49,049
that orders and put that into a lastic

00:02:47,790 --> 00:02:50,909
search so that you might be able to

00:02:49,049 --> 00:02:52,500
search for similar people then you get a

00:02:50,909 --> 00:02:53,879
request for another order and it looks

00:02:52,500 --> 00:02:55,200
in Redis it doesn't find it and it goes

00:02:53,879 --> 00:02:58,099
over to Postgres and says well it's not

00:02:55,200 --> 00:03:01,079
here it must be in the MongoDB warehouse

00:02:58,099 --> 00:03:03,120
celery actually calls Hadoop which puts

00:03:01,079 --> 00:03:05,849
data into redshift and then shoves into

00:03:03,120 --> 00:03:07,470
the MongoDB warehouse and celery might

00:03:05,849 --> 00:03:09,780
query that occasionally and grew into

00:03:07,470 --> 00:03:11,519
Cassandra and then an attacker over here

00:03:09,780 --> 00:03:13,109
accesses Cassandra because you've got an

00:03:11,519 --> 00:03:15,569
engineer who didn't consider you know

00:03:13,109 --> 00:03:17,310
securing Cassandra then the CEO gets

00:03:15,569 --> 00:03:20,040
kind of mad because the data is taken

00:03:17,310 --> 00:03:22,879
out of Cassandra he's big data and it's

00:03:20,040 --> 00:03:25,799
put on to pace B and everyone to see

00:03:22,879 --> 00:03:27,269
basically it's a ruby Goldberg effect

00:03:25,799 --> 00:03:28,590
it's poor planning you're not thinking

00:03:27,269 --> 00:03:31,109
the problem through this premature

00:03:28,590 --> 00:03:34,949
optimization and this need to scale I

00:03:31,109 --> 00:03:36,510
don't like that word you know you might

00:03:34,949 --> 00:03:38,669
see something on hacker news or Y

00:03:36,510 --> 00:03:41,370
Combinator about using the hippest piece

00:03:38,669 --> 00:03:42,870
of technology and honestly for each

00:03:41,370 --> 00:03:44,579
piece of technology introduced into your

00:03:42,870 --> 00:03:47,579
stack you're going to be putting burdens

00:03:44,579 --> 00:03:48,989
like this on your team you know you've

00:03:47,579 --> 00:03:50,879
got you've got more threat surface to

00:03:48,989 --> 00:03:52,650
secure you've got problems when it comes

00:03:50,879 --> 00:03:54,299
to tracing bugs across mortal service

00:03:52,650 --> 00:03:58,709
and you know performance profiling

00:03:54,299 --> 00:04:00,449
that's absolutely terrible you know with

00:03:58,709 --> 00:04:02,400
this cost if the shiny new tool you add

00:04:00,449 --> 00:04:05,129
drops your query time in half it causes

00:04:02,400 --> 00:04:09,239
everything to basically go to crap then

00:04:05,129 --> 00:04:11,069
what use is it I went I went through

00:04:09,239 --> 00:04:12,629
this pain for two years trying to you

00:04:11,069 --> 00:04:15,479
know use shiny add-ons in the Heroku

00:04:12,629 --> 00:04:17,849
Store so this is my motto this is where

00:04:15,479 --> 00:04:18,870
I am and you know basically as the lead

00:04:17,849 --> 00:04:20,459
engineer on a bunch of different

00:04:18,870 --> 00:04:22,229
security focused projects over the last

00:04:20,459 --> 00:04:24,360
two years the more I follow this advice

00:04:22,229 --> 00:04:27,180
the better off the projects seem to run

00:04:24,360 --> 00:04:29,010
in the better off we are afterwards so

00:04:27,180 --> 00:04:31,529
you know this is anecdotal evidence from

00:04:29,010 --> 00:04:33,029
me but I've been personally impressed by

00:04:31,529 --> 00:04:36,750
the results of just sticking to one data

00:04:33,029 --> 00:04:37,100
store the key project that I wanted to

00:04:36,750 --> 00:04:38,840
cover

00:04:37,100 --> 00:04:40,480
was just this idea of unbounded datasets

00:04:38,840 --> 00:04:43,640
so I'll go through the problem space

00:04:40,480 --> 00:04:45,140
basically a crapload of data it's coming

00:04:43,640 --> 00:04:46,880
in fast it's requiring storage and a

00:04:45,140 --> 00:04:48,290
little bit of analysis so old me would

00:04:46,880 --> 00:04:51,860
have jumped and looked at solutions such

00:04:48,290 --> 00:04:53,630
as Hadoop or spark or Cassandra but we

00:04:51,860 --> 00:04:55,070
took a bit of time we decided to go down

00:04:53,630 --> 00:04:57,530
the path to sing if Postgres could

00:04:55,070 --> 00:05:00,830
actually handle this so we started

00:04:57,530 --> 00:05:02,270
documenting our facts you know Heroku

00:05:00,830 --> 00:05:04,760
apps restart a fair bit you can issue a

00:05:02,270 --> 00:05:06,620
command to restart them and after 24

00:05:04,760 --> 00:05:08,270
hours we generally restart them and each

00:05:06,620 --> 00:05:09,920
hat has more than one process in each

00:05:08,270 --> 00:05:11,930
process lives on a different server and

00:05:09,920 --> 00:05:15,080
we've got a lot of apps running at once

00:05:11,930 --> 00:05:17,630
like hundreds of thousands and each

00:05:15,080 --> 00:05:18,890
start and restock that occurs we get

00:05:17,630 --> 00:05:22,790
this we get this event from our

00:05:18,890 --> 00:05:24,050
orchestration team so with these facts

00:05:22,790 --> 00:05:25,670
in mind and you know there's this event

00:05:24,050 --> 00:05:29,240
that we're receiving looks a little bit

00:05:25,670 --> 00:05:30,710
like this we decided to look at some

00:05:29,240 --> 00:05:33,710
more facts and that's you know Heroku

00:05:30,710 --> 00:05:35,480
apps can live on any server so we've got

00:05:33,710 --> 00:05:37,610
a lot of different variations of this

00:05:35,480 --> 00:05:39,110
event coming through rebalancing takes

00:05:37,610 --> 00:05:41,450
place often you know these apps moved

00:05:39,110 --> 00:05:43,280
multiple times a day if anyone who

00:05:41,450 --> 00:05:45,260
hasn't hasn't used AWS there are

00:05:43,280 --> 00:05:50,810
gremlins everywhere incidences disappear

00:05:45,260 --> 00:05:53,240
instances restart the reason for storing

00:05:50,810 --> 00:05:54,590
this data on the security team you know

00:05:53,240 --> 00:05:56,380
we look at abuse reports we look at

00:05:54,590 --> 00:05:59,300
intrusion detection abuse detection

00:05:56,380 --> 00:06:00,650
people's billing rates and this old

00:05:59,300 --> 00:06:03,470
method that we had that was actually a

00:06:00,650 --> 00:06:05,630
database table maintained by the by the

00:06:03,470 --> 00:06:07,160
billing team it was just causing 30

00:06:05,630 --> 00:06:09,380
minute queries there were timeouts they

00:06:07,160 --> 00:06:10,790
were frustration it was a single table

00:06:09,380 --> 00:06:12,320
with so many records that issuing a

00:06:10,790 --> 00:06:17,090
delete from wouldn't actually work

00:06:12,320 --> 00:06:18,830
because there were so many records so

00:06:17,090 --> 00:06:21,830
with these assumptions in mind we

00:06:18,830 --> 00:06:23,360
started looking at looking at what we

00:06:21,830 --> 00:06:25,190
actually needed and we don't actually

00:06:23,360 --> 00:06:26,420
care about the start events we just

00:06:25,190 --> 00:06:27,470
really want to store when the diner was

00:06:26,420 --> 00:06:29,690
stopped in a history of when it was

00:06:27,470 --> 00:06:31,280
wrong and we actually only need 30 days

00:06:29,690 --> 00:06:34,700
of this hot storage afterwards we can

00:06:31,280 --> 00:06:36,230
just move to s3 with our calculation so

00:06:34,700 --> 00:06:38,600
it looked like we were getting about 300

00:06:36,230 --> 00:06:39,620
million records a month and we wanted to

00:06:38,600 --> 00:06:40,820
be able to query them as little as

00:06:39,620 --> 00:06:42,170
possible and have this system that we

00:06:40,820 --> 00:06:43,670
didn't have to maintain a lot of it you

00:06:42,170 --> 00:06:44,710
know just need to sit there and run and

00:06:43,670 --> 00:06:47,030
do just that

00:06:44,710 --> 00:06:49,729
so we did we just create a table like

00:06:47,030 --> 00:06:50,569
this but with a little bit of difference

00:06:49,729 --> 00:06:51,889
we

00:06:50,569 --> 00:06:53,719
Postgres has this feature we can

00:06:51,889 --> 00:06:55,909
actually inherit from a parent table and

00:06:53,719 --> 00:06:58,279
inherit the schema there's a few gotchas

00:06:55,909 --> 00:07:01,429
where it won't inherit constraints or it

00:06:58,279 --> 00:07:02,749
only inherits indexes a create time that

00:07:01,429 --> 00:07:04,249
causes prom when you try and propagate

00:07:02,749 --> 00:07:06,110
an index and you index across many

00:07:04,249 --> 00:07:07,849
tables but basically this takes your

00:07:06,110 --> 00:07:10,279
data and it splits it up into small

00:07:07,849 --> 00:07:13,309
logical tables it doesn't do this all by

00:07:10,279 --> 00:07:15,020
itself though I mean selecting you can

00:07:13,309 --> 00:07:16,699
you can select from dinos and that will

00:07:15,020 --> 00:07:20,330
query every table for what you're

00:07:16,699 --> 00:07:23,869
looking for you can delete from dinos

00:07:20,330 --> 00:07:25,669
sorry you can't delete from dinos you

00:07:23,869 --> 00:07:28,159
can drop a table so with our

00:07:25,669 --> 00:07:28,999
time-stamped table like this removing

00:07:28,159 --> 00:07:32,839
here through was just a matter of

00:07:28,999 --> 00:07:35,180
deleting a table what I touched on

00:07:32,839 --> 00:07:38,479
though was insert into dinos was going

00:07:35,180 --> 00:07:40,219
to be hard so what we did we have looked

00:07:38,479 --> 00:07:41,599
at the problem and we we realized that

00:07:40,219 --> 00:07:43,719
we could just run a cron every 10

00:07:41,599 --> 00:07:46,459
minutes to create some new partitions

00:07:43,719 --> 00:07:50,990
and remove any table that was you know

00:07:46,459 --> 00:07:52,909
30 or 60 days old and we also created a

00:07:50,990 --> 00:07:55,249
way to rewrite a trigger for any insert

00:07:52,909 --> 00:07:57,349
that was occurring to the dinos table it

00:07:55,249 --> 00:07:59,119
would pass the create a table pass the

00:07:57,349 --> 00:08:00,649
created up value and it would rewrite

00:07:59,119 --> 00:08:03,949
the query to insert it into the actual

00:08:00,649 --> 00:08:06,829
proper child table this script will be

00:08:03,949 --> 00:08:08,329
in the repo at the end of the talk so

00:08:06,829 --> 00:08:09,800
every 10 minutes we just ended up

00:08:08,329 --> 00:08:12,649
running this select message message

00:08:09,800 --> 00:08:14,509
partitions query and that would allow us

00:08:12,649 --> 00:08:17,749
to insert into a master table it would

00:08:14,509 --> 00:08:19,550
get sent to a child table and once we

00:08:17,749 --> 00:08:21,169
started looking at our indexes we use so

00:08:19,550 --> 00:08:22,899
we had a few hard lessons along the way

00:08:21,169 --> 00:08:27,979
but this is where we decide as optimal

00:08:22,899 --> 00:08:29,509
we make use of the be tree gist index so

00:08:27,979 --> 00:08:31,369
what we can do we can take a start time

00:08:29,509 --> 00:08:34,399
in an end time and add an attribute to

00:08:31,369 --> 00:08:35,659
it and query that really quickly I'll go

00:08:34,399 --> 00:08:38,599
into some of the queries that were used

00:08:35,659 --> 00:08:40,339
but you know for data of this size we

00:08:38,599 --> 00:08:43,909
ended up using seven indexes so that's

00:08:40,339 --> 00:08:46,880
pretty good I'll give an example of some

00:08:43,909 --> 00:08:48,500
of the queries so in the event that we

00:08:46,880 --> 00:08:50,959
receive an abuse report for an instance

00:08:48,500 --> 00:08:52,730
so an IP address in a time we could

00:08:50,959 --> 00:08:55,339
actually pull the running apps on that

00:08:52,730 --> 00:08:58,160
instance at that time now if you look

00:08:55,339 --> 00:09:01,100
over here the median runtime for 300

00:08:58,160 --> 00:09:03,570
million rows is 4 milliseconds that's

00:09:01,100 --> 00:09:06,870
pretty damn good

00:09:03,570 --> 00:09:08,070
now you might pull a hundred apps that

00:09:06,870 --> 00:09:09,510
were running on this instance at one

00:09:08,070 --> 00:09:11,970
time we realized that if we got to abuse

00:09:09,510 --> 00:09:13,620
reports for the same matter we could do

00:09:11,970 --> 00:09:15,300
two queries and intersect the rows and

00:09:13,620 --> 00:09:17,400
find the common app ID between both of

00:09:15,300 --> 00:09:22,650
them and that would take about eight

00:09:17,400 --> 00:09:24,600
milliseconds so that's pretty good with

00:09:22,650 --> 00:09:25,800
this sort of query we could sort of you

00:09:24,600 --> 00:09:27,900
know if we discovered that there was an

00:09:25,800 --> 00:09:30,120
app that was being malicious or trying

00:09:27,900 --> 00:09:31,590
to you know pop our infrastructure we

00:09:30,120 --> 00:09:33,510
could actually pull a historic reference

00:09:31,590 --> 00:09:35,220
of where you know where this app was at

00:09:33,510 --> 00:09:37,560
this certain time what servers it was

00:09:35,220 --> 00:09:41,460
running on that was actually two

00:09:37,560 --> 00:09:43,170
milliseconds so that's pretty good now

00:09:41,460 --> 00:09:45,000
1,800 milliseconds it might not be good

00:09:43,170 --> 00:09:46,200
but when when we consider that what

00:09:45,000 --> 00:09:48,120
we're doing is picking every app that

00:09:46,200 --> 00:09:49,740
runs on Heroku and getting us some of

00:09:48,120 --> 00:09:51,990
the powers that it's used on our

00:09:49,740 --> 00:09:54,420
platform over the past month it takes

00:09:51,990 --> 00:09:55,560
eighteen hundred milliseconds I can live

00:09:54,420 --> 00:09:58,290
with that that's not much out of my day

00:09:55,560 --> 00:10:00,120
and compared to running you know elastic

00:09:58,290 --> 00:10:02,880
Hadoop on Amazon and you know trying to

00:10:00,120 --> 00:10:04,380
maintain that I'd much rather be in this

00:10:02,880 --> 00:10:05,970
place and you know the takeaways were

00:10:04,380 --> 00:10:08,550
that we we've got quicker queries most

00:10:05,970 --> 00:10:10,530
things run in a matter of seconds

00:10:08,550 --> 00:10:11,760
it's maintainable we've got monitoring

00:10:10,530 --> 00:10:16,140
set up it hasn't gone off in three

00:10:11,760 --> 00:10:17,280
months I haven't touched it and you know

00:10:16,140 --> 00:10:19,050
we came into this with a lot of

00:10:17,280 --> 00:10:20,520
hesitations about this this you know

00:10:19,050 --> 00:10:22,700
wanting to store all the data and having

00:10:20,520 --> 00:10:24,540
data as far back as we could go I

00:10:22,700 --> 00:10:27,240
personally and most of my teammates

00:10:24,540 --> 00:10:29,070
haven't touched the the cold storage

00:10:27,240 --> 00:10:31,320
you know if serán Amazon if we need it

00:10:29,070 --> 00:10:32,310
but haven't touched it and the cost is

00:10:31,320 --> 00:10:34,140
raised well this is all done on a

00:10:32,310 --> 00:10:36,500
Postgres instance with 15 gig of ram one

00:10:34,140 --> 00:10:38,910
server and a few processes talking to it

00:10:36,500 --> 00:10:40,770
this this was the project for me that

00:10:38,910 --> 00:10:42,030
really really showed that hey Postgres

00:10:40,770 --> 00:10:43,590
does a pretty good job at the heavy

00:10:42,030 --> 00:10:46,140
lifting and there's a lot of cool

00:10:43,590 --> 00:10:47,400
features and from here I actually

00:10:46,140 --> 00:10:48,840
started looking to other ways that we

00:10:47,400 --> 00:10:53,880
could use Postgres and supplement other

00:10:48,840 --> 00:10:54,960
services with this tool so who uses code

00:10:53,880 --> 00:10:56,100
level locks in your DB you've got

00:10:54,960 --> 00:10:57,660
multiple processors across multiple

00:10:56,100 --> 00:11:01,200
servers and then the exclusive access to

00:10:57,660 --> 00:11:06,060
one thing anybody a few people so do use

00:11:01,200 --> 00:11:07,200
Redis or memcache for that files yeah

00:11:06,060 --> 00:11:09,120
okay

00:11:07,200 --> 00:11:11,250
so you know that this model ways to do

00:11:09,120 --> 00:11:12,750
and we we used Redis for a long time and

00:11:11,250 --> 00:11:14,100
you know with this example we've got a

00:11:12,750 --> 00:11:15,720
clock process and what this does is it

00:11:14,100 --> 00:11:18,320
queries Braintree and it pulls in a list

00:11:15,720 --> 00:11:20,519
of all card verifications that happened

00:11:18,320 --> 00:11:21,959
due to the way we query Braintree we've

00:11:20,519 --> 00:11:23,220
got to give it a start time and an end

00:11:21,959 --> 00:11:24,930
time so what we do we just give the

00:11:23,220 --> 00:11:26,790
start time of the last record that we

00:11:24,930 --> 00:11:29,730
pulled in and it pulls in every record

00:11:26,790 --> 00:11:31,230
since then but to avoid any sort of race

00:11:29,730 --> 00:11:34,620
conditions we needed to ensure that this

00:11:31,230 --> 00:11:35,760
ran only once so we didn't want to put

00:11:34,620 --> 00:11:39,990
miss out on any of the records being

00:11:35,760 --> 00:11:42,029
pulled in so so we've got to set up like

00:11:39,990 --> 00:11:43,920
this and someone issues the clock equals

00:11:42,029 --> 00:11:45,510
two so they scale to two processes

00:11:43,920 --> 00:11:47,730
command we want to prevent this and

00:11:45,510 --> 00:11:49,560
brace conditions so it did we put the

00:11:47,730 --> 00:11:51,329
Redis lock in if cork two starts up it

00:11:49,560 --> 00:11:56,970
tries to acquire the lock but it can't

00:11:51,329 --> 00:12:01,019
get it no what we decided was to

00:11:56,970 --> 00:12:02,970
actually scrap Redis in this case using

00:12:01,019 --> 00:12:04,829
Postgres you can query for any sort of

00:12:02,970 --> 00:12:06,600
lock that you want you can give it a

00:12:04,829 --> 00:12:09,630
64-bit integer so we just take a string

00:12:06,600 --> 00:12:11,880
transform it into a transform in into a

00:12:09,630 --> 00:12:13,199
number and whoever it in our own tool

00:12:11,880 --> 00:12:14,459
around this to obtain locks but there's

00:12:13,199 --> 00:12:15,660
many out there for django and SQL

00:12:14,459 --> 00:12:18,529
alchemy where you can just use Postgres

00:12:15,660 --> 00:12:21,180
instead of adding a moving part of Redis

00:12:18,529 --> 00:12:23,490
yeah I mean that's that's a pretty crude

00:12:21,180 --> 00:12:25,230
example what really impressed me was

00:12:23,490 --> 00:12:27,779
social graphs so if anyone's used

00:12:25,230 --> 00:12:29,640
something like neo4j we can get directed

00:12:27,779 --> 00:12:31,620
graphs and you know transverse graphs

00:12:29,640 --> 00:12:33,630
and such I never knew that Postgres

00:12:31,620 --> 00:12:36,959
could actually do this so what we have

00:12:33,630 --> 00:12:38,640
here is basically a user ID the credit

00:12:36,959 --> 00:12:40,410
card they've used and the label where

00:12:38,640 --> 00:12:45,839
they about use have we labeled them as

00:12:40,410 --> 00:12:47,430
good or are they unlabeled now we

00:12:45,839 --> 00:12:49,410
started out creating a graph like this

00:12:47,430 --> 00:12:52,500
basically we're taking a union of card

00:12:49,410 --> 00:12:55,740
to user and user to card basically an

00:12:52,500 --> 00:12:57,329
undirected graph now this is a small

00:12:55,740 --> 00:12:58,440
query you know that's small text to fit

00:12:57,329 --> 00:13:00,540
it all in but what I'm going to do is

00:12:58,440 --> 00:13:02,880
just step through it so recursively

00:13:00,540 --> 00:13:06,149
Postgres says this has a common table

00:13:02,880 --> 00:13:08,160
expression declaration called with

00:13:06,149 --> 00:13:10,319
recursive so you can recursively select

00:13:08,160 --> 00:13:12,240
your initial rows and then recursively

00:13:10,319 --> 00:13:13,740
go through those so what we're doing

00:13:12,240 --> 00:13:17,040
we're recursively getting all the bad

00:13:13,740 --> 00:13:18,569
users we're linking them with other

00:13:17,040 --> 00:13:20,240
cards and users who have not been

00:13:18,569 --> 00:13:23,940
labeled bad

00:13:20,240 --> 00:13:25,620
except where we form a loop and from

00:13:23,940 --> 00:13:28,680
this set we're finding all end nodes who

00:13:25,620 --> 00:13:30,270
are users and what that gives us now

00:13:28,680 --> 00:13:32,640
this text is very small as well I've got

00:13:30,270 --> 00:13:34,620
a gun example here but basically it

00:13:32,640 --> 00:13:35,970
steps through how a users of the related

00:13:34,620 --> 00:13:38,490
to a card to a user it just gives us a

00:13:35,970 --> 00:13:40,560
graph of how transverses and what this

00:13:38,490 --> 00:13:42,480
what this actually able enables us to do

00:13:40,560 --> 00:13:44,790
is to look at user a so someone we've

00:13:42,480 --> 00:13:46,830
found who used a stolen credit card they

00:13:44,790 --> 00:13:48,660
might have used card a but then use a B

00:13:46,830 --> 00:13:50,850
who is not bad they've also used their

00:13:48,660 --> 00:13:52,650
card and their fuse card B and use the C

00:13:50,850 --> 00:13:54,590
who is not bad you know they might be

00:13:52,650 --> 00:13:56,790
unlabeled they've also used that card B

00:13:54,590 --> 00:13:59,100
that's pretty cool in Postgres in about

00:13:56,790 --> 00:13:59,910
67 lines of code you know my initial

00:13:59,100 --> 00:14:01,710
thoughts would have been to put this

00:13:59,910 --> 00:14:05,580
data in a neo4j and set up a syncing

00:14:01,710 --> 00:14:06,810
mechanism I'm lazy I'm lazy at work I'm

00:14:05,580 --> 00:14:08,250
trying to be less lazy outside of work

00:14:06,810 --> 00:14:10,920
but I want to spend my 8 hours a day

00:14:08,250 --> 00:14:12,180
being productive and that's with the

00:14:10,920 --> 00:14:13,050
amount of data it just seemed like an

00:14:12,180 --> 00:14:14,670
absolute nightmare

00:14:13,050 --> 00:14:16,890
so keeping an impost rest has been

00:14:14,670 --> 00:14:20,370
really effective basically we're able to

00:14:16,890 --> 00:14:22,140
transfer scraps like this and you know

00:14:20,370 --> 00:14:24,390
using this with recursive method you can

00:14:22,140 --> 00:14:26,250
actually implement so some well-known

00:14:24,390 --> 00:14:28,710
lono tools or other other pieces of

00:14:26,250 --> 00:14:30,540
other companies have used so you know

00:14:28,710 --> 00:14:33,360
user and use of user a and user B share

00:14:30,540 --> 00:14:35,460
common connections user a is connected

00:14:33,360 --> 00:14:37,440
to use their a through these users or

00:14:35,460 --> 00:14:38,490
the you might know feature I won't

00:14:37,440 --> 00:14:41,220
explain that because I'm just going to

00:14:38,490 --> 00:14:42,210
get tongue-tied so how many people

00:14:41,220 --> 00:14:45,240
didn't know that you could do social

00:14:42,210 --> 00:14:50,100
graphs like that in Postgres pretty cool

00:14:45,240 --> 00:14:52,350
feature I I won't cover those too much

00:14:50,100 --> 00:14:53,490
because I didn't have really many really

00:14:52,350 --> 00:14:55,410
good examples that I could share that

00:14:53,490 --> 00:14:56,760
actually gave us anything interesting so

00:14:55,410 --> 00:14:58,950
what I did I took the same sort of data

00:14:56,760 --> 00:15:01,470
where we have a user ID a card ID and I

00:14:58,950 --> 00:15:03,450
verified that and I basically ran it

00:15:01,470 --> 00:15:05,370
through this this window partition

00:15:03,450 --> 00:15:09,990
function basically some context within

00:15:05,370 --> 00:15:11,540
each row of your query so this is

00:15:09,990 --> 00:15:14,790
actually in one of the ipython notebooks

00:15:11,540 --> 00:15:16,470
if you see here basically I've got all

00:15:14,790 --> 00:15:18,060
these keywords like partition and lag

00:15:16,470 --> 00:15:20,370
and you know unbound preceding and

00:15:18,060 --> 00:15:22,170
current row basically you're able to

00:15:20,370 --> 00:15:23,700
look at each row and get some statistics

00:15:22,170 --> 00:15:25,410
about the row before it in the row after

00:15:23,700 --> 00:15:27,720
it based on some common groupings so in

00:15:25,410 --> 00:15:30,510
this case I've taken the user ID and I'm

00:15:27,720 --> 00:15:32,730
able to see that this user first

00:15:30,510 --> 00:15:35,040
verified 194 days ago

00:15:32,730 --> 00:15:37,260
their last verification before this one

00:15:35,040 --> 00:15:39,480
was two days and one hour and 54 minutes

00:15:37,260 --> 00:15:42,360
ago and this is number 57 of their

00:15:39,480 --> 00:15:44,190
verification I'll explain that that's

00:15:42,360 --> 00:15:45,810
user ID that's their first verification

00:15:44,190 --> 00:15:47,490
that said most previous and that's how

00:15:45,810 --> 00:15:50,040
many previous verifications user at this

00:15:47,490 --> 00:15:51,360
point before investigating these

00:15:50,040 --> 00:15:52,980
functions I always thought though I'd

00:15:51,360 --> 00:15:54,720
have to do sub queries and pull this

00:15:52,980 --> 00:15:57,870
data out and munjin Python put it back

00:15:54,720 --> 00:16:01,139
together it's not the case I can do that

00:15:57,870 --> 00:16:02,880
within the actual database that was a

00:16:01,139 --> 00:16:03,990
pretty cool Epiphany and you know I've

00:16:02,880 --> 00:16:05,370
been exploring those more and we use

00:16:03,990 --> 00:16:08,010
those in data collection to drive some

00:16:05,370 --> 00:16:09,149
of our models so like I said I wouldn't

00:16:08,010 --> 00:16:11,120
touch on that too much but I will touch

00:16:09,149 --> 00:16:13,490
on common table expressions a bit more

00:16:11,120 --> 00:16:16,529
these blew my mind

00:16:13,490 --> 00:16:18,779
basically these statements are referred

00:16:16,529 --> 00:16:20,519
to as common table expressions or CTS if

00:16:18,779 --> 00:16:22,740
you see them on you know people talking

00:16:20,519 --> 00:16:24,120
about them and think about them is just

00:16:22,740 --> 00:16:27,690
defining these temporary tables that

00:16:24,120 --> 00:16:30,320
exist for just one query so here I'm

00:16:27,690 --> 00:16:33,660
declaring with stats as and I'm

00:16:30,320 --> 00:16:35,459
basically running the same query as last

00:16:33,660 --> 00:16:37,709
time and then I'm able to select from

00:16:35,459 --> 00:16:40,170
that stat stable while seat here that I

00:16:37,709 --> 00:16:42,000
just declared within the same query I

00:16:40,170 --> 00:16:43,740
don't have to create temporary tables or

00:16:42,000 --> 00:16:45,810
sub selects or anything like that I'm

00:16:43,740 --> 00:16:50,250
just able to query from another

00:16:45,810 --> 00:16:54,149
temporary table I've created one cool

00:16:50,250 --> 00:16:56,399
feature we do use CTAs for is basically

00:16:54,149 --> 00:16:58,079
taking all of our users who are been

00:16:56,399 --> 00:17:00,300
deleted and moving him into an archive

00:16:58,079 --> 00:17:01,949
to use this table that begin an

00:17:00,300 --> 00:17:03,209
isolation level serializable I'll talk

00:17:01,949 --> 00:17:06,360
about that with the racing editions at

00:17:03,209 --> 00:17:07,860
the end but basically within a few lines

00:17:06,360 --> 00:17:11,160
of code we're able to move all of our

00:17:07,860 --> 00:17:14,850
deleted users into an archive table yeah

00:17:11,160 --> 00:17:15,480
it's really easy we can also query them

00:17:14,850 --> 00:17:17,549
with a CTA

00:17:15,480 --> 00:17:19,650
so we say all users is basically the

00:17:17,549 --> 00:17:22,110
union of users and our archive users and

00:17:19,650 --> 00:17:23,400
we can query them the same way I always

00:17:22,110 --> 00:17:24,959
thought temporary tables were the thing

00:17:23,400 --> 00:17:27,020
but yeah this this is really blowing my

00:17:24,959 --> 00:17:29,790
mind

00:17:27,020 --> 00:17:31,230
notify listen I I know a few talks have

00:17:29,790 --> 00:17:34,470
touched on this so I didn't add too much

00:17:31,230 --> 00:17:36,120
about it basically within a post rest

00:17:34,470 --> 00:17:37,799
shell you can say listen on a queue now

00:17:36,120 --> 00:17:41,090
if you issue this is within the psql

00:17:37,799 --> 00:17:43,500
terminal it'll go into asynchronous mode

00:17:41,090 --> 00:17:45,539
another process or from within the same

00:17:43,500 --> 00:17:47,429
terminal you can say notify queue of

00:17:45,539 --> 00:17:49,789
you that's just what I've used and

00:17:47,429 --> 00:17:51,899
basically that message is received now

00:17:49,789 --> 00:17:54,299
on its own like that that's really

00:17:51,899 --> 00:17:56,190
boring but for example you can whenever

00:17:54,299 --> 00:17:57,479
you update a row you can notify a

00:17:56,190 --> 00:17:59,340
certain queue that that row has been

00:17:57,479 --> 00:18:01,739
updated and have a Python process

00:17:59,340 --> 00:18:04,320
listening to that queue in order to you

00:18:01,739 --> 00:18:05,669
know maybe clear a Redis cache or update

00:18:04,320 --> 00:18:07,379
an external data store and I talked

00:18:05,669 --> 00:18:09,629
about service pain but that's something

00:18:07,379 --> 00:18:12,539
you might want to do it's also an

00:18:09,629 --> 00:18:15,659
alternative to signals and keeping them

00:18:12,539 --> 00:18:17,100
within the within the database one one

00:18:15,659 --> 00:18:19,739
dragon though is the messages are not

00:18:17,100 --> 00:18:22,349
persistent so what I have seen some

00:18:19,739 --> 00:18:24,509
people do is use a trigger to put a

00:18:22,349 --> 00:18:27,149
notification into an updates table and

00:18:24,509 --> 00:18:29,399
then notify when an insert occurs online

00:18:27,149 --> 00:18:31,349
updates table and also the service

00:18:29,399 --> 00:18:33,090
consuming those notifications is also

00:18:31,349 --> 00:18:36,869
able to query that updates table in case

00:18:33,090 --> 00:18:38,399
they miss something now like I said I'm

00:18:36,869 --> 00:18:41,099
lazy and when it comes to database

00:18:38,399 --> 00:18:42,330
schemas I'm really vain I always get

00:18:41,099 --> 00:18:45,570
hesitant about putting things in the

00:18:42,330 --> 00:18:48,720
wrong order or making things look ugly I

00:18:45,570 --> 00:18:51,659
mean I I want the ID row to be the first

00:18:48,720 --> 00:18:53,669
row in there so bear with me why I

00:18:51,659 --> 00:18:55,259
explained this one today we've got a

00:18:53,669 --> 00:18:58,109
table which is base users and we've got

00:18:55,259 --> 00:19:00,029
a name and is awesome and we can save it

00:18:58,109 --> 00:19:01,289
or is awesome default to true and we've

00:19:00,029 --> 00:19:05,669
inserted these people that are obviously

00:19:01,289 --> 00:19:07,139
awesome if we alter that table Postgres

00:19:05,669 --> 00:19:09,090
doesn't actually allow us to specify

00:19:07,139 --> 00:19:11,460
where we want the row so I've added a

00:19:09,090 --> 00:19:13,169
primary key here and if I look at the

00:19:11,460 --> 00:19:16,679
skimmer again that row but that new

00:19:13,169 --> 00:19:22,499
column is put on the end that doesn't

00:19:16,679 --> 00:19:23,729
look cool you know if if I go and do and

00:19:22,499 --> 00:19:25,379
select the skimmer and I see that I I

00:19:23,729 --> 00:19:27,479
don't know if something clicks in my

00:19:25,379 --> 00:19:28,919
head but updatable views actually solved

00:19:27,479 --> 00:19:31,019
this problem which you know most people

00:19:28,919 --> 00:19:32,489
would say it's kind of silly what you

00:19:31,019 --> 00:19:34,710
can do you can create a view so I create

00:19:32,489 --> 00:19:35,970
a view called public users and I saw you

00:19:34,710 --> 00:19:38,039
know I put it in the order that I

00:19:35,970 --> 00:19:40,499
actually want and now I can select from

00:19:38,039 --> 00:19:42,509
it okay that's pretty cool for view you

00:19:40,499 --> 00:19:45,690
know these things are in most database

00:19:42,509 --> 00:19:48,179
systems if I have a look at it it's in

00:19:45,690 --> 00:19:50,700
the order I want but with Postgres if

00:19:48,179 --> 00:19:52,379
you issue an update to that view it will

00:19:50,700 --> 00:19:55,109
actually a proxy that to the table which

00:19:52,379 --> 00:19:56,549
the view is based on so that was

00:19:55,109 --> 00:19:59,099
something that really blew my mind and

00:19:56,549 --> 00:19:59,440
yeah it's got a few gotchas when you're

00:19:59,099 --> 00:20:00,789
using

00:19:59,440 --> 00:20:03,370
tables and you're doing you know

00:20:00,789 --> 00:20:04,509
aggregates and such but this gave us a

00:20:03,370 --> 00:20:07,029
way to not worry about the schema

00:20:04,509 --> 00:20:11,019
because ideally we do want the schema to

00:20:07,029 --> 00:20:12,370
be in some sort of order foreign data

00:20:11,019 --> 00:20:13,840
Rapids is something I've been using a

00:20:12,370 --> 00:20:15,970
fair bit with our micro services

00:20:13,840 --> 00:20:17,350
exploration basically this is database

00:20:15,970 --> 00:20:19,570
one so I'll create a table called users

00:20:17,350 --> 00:20:24,220
and I insert some values into that table

00:20:19,570 --> 00:20:26,110
over on database to I enable my Postgres

00:20:24,220 --> 00:20:28,690
foreign data wrapper I create a server

00:20:26,110 --> 00:20:30,549
called the remote DB and there's a user

00:20:28,690 --> 00:20:34,149
mapping in there and I create a foreign

00:20:30,549 --> 00:20:36,250
table called remote users what you can

00:20:34,149 --> 00:20:37,840
do on the db2 side is then select from

00:20:36,250 --> 00:20:39,039
those remote users and across the

00:20:37,840 --> 00:20:43,179
network it will pull them in using

00:20:39,039 --> 00:20:44,320
Postgres queries this just doesn't stop

00:20:43,179 --> 00:20:47,309
it selecting though you can also do

00:20:44,320 --> 00:20:49,450
updates deletes work you can do inserts

00:20:47,309 --> 00:20:50,590
so that was that was something that was

00:20:49,450 --> 00:20:53,159
completely new to me before I bought

00:20:50,590 --> 00:20:53,159
embarked on this adventure

00:20:54,000 --> 00:20:58,179
so I touched on this within my

00:20:56,679 --> 00:21:01,360
JavaScript talk yesterday but uh

00:20:58,179 --> 00:21:03,039
post-arrest has a fair few rappers I I

00:21:01,360 --> 00:21:04,299
look to arsenite at the latest list and

00:21:03,039 --> 00:21:06,340
there's things like Heroku dart eclipse

00:21:04,299 --> 00:21:09,309
this seems like RSS you can do HTTP

00:21:06,340 --> 00:21:11,500
queries most of these things are enabled

00:21:09,309 --> 00:21:13,899
by two or three commands or you need to

00:21:11,500 --> 00:21:15,340
download something and build it but when

00:21:13,899 --> 00:21:16,870
you've got ways to talk to docker and os

00:21:15,340 --> 00:21:19,149
query and OpenStack from within your

00:21:16,870 --> 00:21:20,860
database makes things pretty interesting

00:21:19,149 --> 00:21:24,090
you know turn to datastore into

00:21:20,860 --> 00:21:24,090
something that's a bit more extensible

00:21:28,870 --> 00:21:36,880
has anyone used materialized views okay

00:21:33,250 --> 00:21:39,130
few people say I have a table called

00:21:36,880 --> 00:21:42,130
users and I insert some people internet

00:21:39,130 --> 00:21:44,200
user tables and I I do a one-to-many

00:21:42,130 --> 00:21:46,720
Association of the user and their cats

00:21:44,200 --> 00:21:49,090
so as you can see over here

00:21:46,720 --> 00:21:50,830
Jane has mittens and Bridget has rag nor

00:21:49,090 --> 00:21:55,090
the great nor mighty that's her cat

00:21:50,830 --> 00:21:56,440
I can I can actually say create

00:21:55,090 --> 00:21:58,510
materialized view which is user cat

00:21:56,440 --> 00:22:00,550
counts and I can create an index on it

00:21:58,510 --> 00:22:02,980
and that becomes a persistent view of

00:22:00,550 --> 00:22:06,460
that data I can select from it I can't

00:22:02,980 --> 00:22:08,800
actually insert into it but say I you

00:22:06,460 --> 00:22:15,340
know crazy cat race puts his cats in

00:22:08,800 --> 00:22:18,160
there if I try and select from it you'll

00:22:15,340 --> 00:22:20,080
see that it's not there what I actually

00:22:18,160 --> 00:22:22,300
have to do is say refresh materialized

00:22:20,080 --> 00:22:25,720
view user cat counts I try and select it

00:22:22,300 --> 00:22:26,830
from there again and I've actually got

00:22:25,720 --> 00:22:28,360
the wrong result that's not a function

00:22:26,830 --> 00:22:29,440
of post grades that's my poor slide so

00:22:28,360 --> 00:22:33,910
excuse me though there should be seven

00:22:29,440 --> 00:22:35,320
cats but materialized views what what

00:22:33,910 --> 00:22:36,640
are you personally using for I use them

00:22:35,320 --> 00:22:38,230
for importing from other post gross

00:22:36,640 --> 00:22:39,760
tables so we might have a team over here

00:22:38,230 --> 00:22:40,390
who's got a list of users in some sort

00:22:39,760 --> 00:22:41,920
of attribute

00:22:40,390 --> 00:22:45,429
I'll periodically pull them in with a

00:22:41,920 --> 00:22:47,380
cron we also do roll-ups of data that

00:22:45,429 --> 00:22:50,010
doesn't need to be that fresh and also

00:22:47,380 --> 00:22:53,860
add some caching into the database so

00:22:50,010 --> 00:22:55,840
it's basically caching a query as the

00:22:53,860 --> 00:22:57,550
Postgres 9.4 you can't actually refresh

00:22:55,840 --> 00:22:58,510
them concurrently without block you know

00:22:57,550 --> 00:23:01,000
you can refresh them concurrently

00:22:58,510 --> 00:23:02,620
without blocking other queries but if

00:23:01,000 --> 00:23:05,290
you do issue a refresh it's got to go

00:23:02,620 --> 00:23:07,240
through everything again and do it from

00:23:05,290 --> 00:23:08,740
scratch we might see partial refreshes

00:23:07,240 --> 00:23:13,480
in the future and that would be really

00:23:08,740 --> 00:23:15,010
exciting one question I put to is you

00:23:13,480 --> 00:23:16,300
know can you use the trigger to refresh

00:23:15,010 --> 00:23:17,530
your materialized view and that's

00:23:16,300 --> 00:23:18,970
actually the case for our payment

00:23:17,530 --> 00:23:21,610
verifications we use multiple triggers

00:23:18,970 --> 00:23:24,160
to refresh materialized views of roll up

00:23:21,610 --> 00:23:29,710
roll up stats of you know how users a

00:23:24,160 --> 00:23:31,480
very verified so one thing that was also

00:23:29,710 --> 00:23:35,200
kind of exciting is the Postgres has

00:23:31,480 --> 00:23:36,700
range types so say we've got a table

00:23:35,200 --> 00:23:38,080
called cat reservations and you can

00:23:36,700 --> 00:23:41,440
reserve a cat for a period of time

00:23:38,080 --> 00:23:42,640
probably for padding or such yeah you

00:23:41,440 --> 00:23:49,450
can see here I've reserved mister

00:23:42,640 --> 00:23:51,010
mittens for from 2 p.m. to 15 1,500 if I

00:23:49,450 --> 00:23:52,870
save that and then I try and reserve

00:23:51,010 --> 00:23:54,460
again I get a conflicting key value

00:23:52,870 --> 00:23:56,440
because there's an exclusion good

00:23:54,460 --> 00:23:58,840
straight basically it's not letting me

00:23:56,440 --> 00:24:01,750
go within that same same period as the

00:23:58,840 --> 00:24:05,080
first cap was reserved you can see here

00:24:01,750 --> 00:24:07,270
I've used a square bracket and a normal

00:24:05,080 --> 00:24:08,730
bracket on different ends basic bare

00:24:07,270 --> 00:24:12,490
there upper bound and lower bound

00:24:08,730 --> 00:24:14,440
inclusion and exclusion constraints so I

00:24:12,490 --> 00:24:18,180
can say you know it's basically less

00:24:14,440 --> 00:24:22,660
than or greater - oh sorry greater then

00:24:18,180 --> 00:24:24,010
you know what I mean now this is where I

00:24:22,660 --> 00:24:25,480
didn't take notes so it starts to get

00:24:24,010 --> 00:24:27,700
kind of curly here and this is the new

00:24:25,480 --> 00:24:29,620
part I was talking about Jason in

00:24:27,700 --> 00:24:31,000
Postgres this was exciting because we

00:24:29,620 --> 00:24:34,900
were able to do away with our skinless

00:24:31,000 --> 00:24:36,970
mongodb set up once again with the cats

00:24:34,900 --> 00:24:38,620
sorry I had a common pattern here

00:24:36,970 --> 00:24:40,060
I can created table called cats and I've

00:24:38,620 --> 00:24:41,620
got a column called metadata and let's

00:24:40,060 --> 00:24:44,200
go find us the Jason B data type

00:24:41,620 --> 00:24:46,050
Postgres 29.3 had Jason this is Jason

00:24:44,200 --> 00:24:48,850
binary so it doesn't just store a

00:24:46,050 --> 00:24:51,370
validated textual representation of the

00:24:48,850 --> 00:24:52,600
eurasian blob it actually passes it and

00:24:51,370 --> 00:24:54,610
doesn't intelligent things with it

00:24:52,600 --> 00:24:56,320
so you can see here that i put in three

00:24:54,610 --> 00:24:58,600
cats and you know their likes their

00:24:56,320 --> 00:25:02,080
hates because we all know cats hate most

00:24:58,600 --> 00:25:03,310
things now if i do a select from cats

00:25:02,080 --> 00:25:04,750
you can see that it returns to the

00:25:03,310 --> 00:25:05,950
textual representation which was the

00:25:04,750 --> 00:25:09,640
first column and their returns the meta

00:25:05,950 --> 00:25:12,130
data and it's let's new JSON blob what I

00:25:09,640 --> 00:25:13,390
can do is query that so I can say well

00:25:12,130 --> 00:25:14,890
where does a meta data contain the

00:25:13,390 --> 00:25:17,680
object where I've got a key of like and

00:25:14,890 --> 00:25:20,980
it's an array with nihilism in there and

00:25:17,680 --> 00:25:24,190
I get a result like this so that's

00:25:20,980 --> 00:25:27,850
pretty cool you can query Jason what if

00:25:24,190 --> 00:25:29,740
I was to use one of the JSONP operators

00:25:27,850 --> 00:25:32,770
where it takes every array element and

00:25:29,740 --> 00:25:34,120
turns it into an actual row you can see

00:25:32,770 --> 00:25:37,300
here I've got the cat I've got its like

00:25:34,120 --> 00:25:38,860
and it's hate this is a really really

00:25:37,300 --> 00:25:41,560
simple query but you know it's pretty

00:25:38,860 --> 00:25:43,990
powerful we use it too one of the things

00:25:41,560 --> 00:25:45,730
we do is store a file list of just file

00:25:43,990 --> 00:25:46,570
names and md5 hashes of some of the code

00:25:45,730 --> 00:25:48,640
that's pushed to us

00:25:46,570 --> 00:25:50,950
we were storing that as Jason we still

00:25:48,640 --> 00:25:53,350
do but we run a materialized view every

00:25:50,950 --> 00:25:55,720
30 minutes - unless it and turn it into

00:25:53,350 --> 00:25:58,060
a nice indexed materialized view

00:25:55,720 --> 00:26:01,780
in this sort of structure it makes you

00:25:58,060 --> 00:26:04,990
know much easier querying so Jason B

00:26:01,780 --> 00:26:06,670
supports indexing as well if I use the

00:26:04,990 --> 00:26:09,610
gene index that's usually pretty fast

00:26:06,670 --> 00:26:11,110
there are there are ways with it within

00:26:09,610 --> 00:26:13,240
Jason B there are some new operators to

00:26:11,110 --> 00:26:14,830
get some faster indexes but you know

00:26:13,240 --> 00:26:18,550
we've found for our cases it works

00:26:14,830 --> 00:26:21,130
pretty well but say this is the

00:26:18,550 --> 00:26:22,480
containment operator so over on the Left

00:26:21,130 --> 00:26:24,700
I've got a JSON blob and I'm saying well

00:26:22,480 --> 00:26:29,050
does this JSON blob contain this JSON

00:26:24,700 --> 00:26:31,270
object I can say if they're an instance

00:26:29,050 --> 00:26:32,680
of a key within this JSON object just

00:26:31,270 --> 00:26:35,590
the jason blob on the Left contained the

00:26:32,680 --> 00:26:37,360
key text our test I can ask if it has

00:26:35,590 --> 00:26:39,250
all of the keys that are declared that's

00:26:37,360 --> 00:26:41,560
a Postgres text array on the right and a

00:26:39,250 --> 00:26:43,140
JSON blob on the left I can also say

00:26:41,560 --> 00:26:47,590
does it have at least one of these keys

00:26:43,140 --> 00:26:50,920
I can also issue a query like this I'm

00:26:47,590 --> 00:26:53,740
saying hey navigate down the JSON path

00:26:50,920 --> 00:26:56,710
and look at the test key and pull me out

00:26:53,740 --> 00:26:57,970
what that is equal to in this case it's

00:26:56,710 --> 00:27:01,270
pulled out the object where something is

00:26:57,970 --> 00:27:03,310
true there's a lot more I recommend

00:27:01,270 --> 00:27:05,350
reading the JSON documentation because I

00:27:03,310 --> 00:27:07,420
mean a lot of people have done

00:27:05,350 --> 00:27:11,410
benchmarks on it and it's really

00:27:07,420 --> 00:27:12,430
performant this quote basically EDB did

00:27:11,410 --> 00:27:15,370
some testing on it and they found that

00:27:12,430 --> 00:27:18,070
it was you know 2.1 times more faster

00:27:15,370 --> 00:27:20,500
than MongoDB for actually putting data

00:27:18,070 --> 00:27:22,510
in data inserts took three times longer

00:27:20,500 --> 00:27:25,480
in MongoDB I'm not trying to rag on a

00:27:22,510 --> 00:27:26,920
bit you know between having your primary

00:27:25,480 --> 00:27:28,480
data store which is usually Postgres on

00:27:26,920 --> 00:27:29,860
twice ql a MongoDB over here it

00:27:28,480 --> 00:27:35,890
absolutely scraps the need to have

00:27:29,860 --> 00:27:39,700
MongoDB we also make heavily use our

00:27:35,890 --> 00:27:41,320
heavy use of partial indexes so using

00:27:39,700 --> 00:27:43,150
the Katz table again if i've insert a

00:27:41,320 --> 00:27:47,770
lot of cats some with pun names like a

00:27:43,150 --> 00:27:49,210
ricky purveyors and we're running a

00:27:47,770 --> 00:27:52,510
society that connects people with cats

00:27:49,210 --> 00:27:54,220
that aren't jerks now we might query

00:27:52,510 --> 00:27:55,830
this by select star from cats where is a

00:27:54,220 --> 00:27:58,120
jerk equals false

00:27:55,830 --> 00:28:00,010
so hey we get a lot of a lot of cats on

00:27:58,120 --> 00:28:03,250
the site a few million will create an

00:28:00,010 --> 00:28:04,630
index where says is a jerk what we've

00:28:03,250 --> 00:28:08,800
observed though is two percent of cats

00:28:04,630 --> 00:28:09,430
and not jerks so by that we can say well

00:28:08,800 --> 00:28:10,990
00:28:09,430 --> 00:28:13,330
none of our cats is a jerk index is

00:28:10,990 --> 00:28:17,410
wasted space we we rarely query for is a

00:28:13,330 --> 00:28:18,700
jerk it's true so essentially we're

00:28:17,410 --> 00:28:21,940
wasting space what you can actually do

00:28:18,700 --> 00:28:23,530
with Postgres is implement a conditional

00:28:21,940 --> 00:28:24,970
index so what I've done is created the

00:28:23,530 --> 00:28:28,840
same index but I put the predicate on

00:28:24,970 --> 00:28:31,810
the end of where a jerk is false what

00:28:28,840 --> 00:28:33,460
that actually means is you know when we

00:28:31,810 --> 00:28:36,190
include this predicate we're actually

00:28:33,460 --> 00:28:37,690
creating a very small index in this in

00:28:36,190 --> 00:28:39,550
this case we drop our index size by

00:28:37,690 --> 00:28:40,870
about 98% because there's data in there

00:28:39,550 --> 00:28:44,650
we don't really care about we only want

00:28:40,870 --> 00:28:46,840
to index certain fields we we commonly

00:28:44,650 --> 00:28:49,210
use this where deleted out is no on a

00:28:46,840 --> 00:28:51,160
lot of our lot of our fields in our core

00:28:49,210 --> 00:28:53,710
you know core database for our API set

00:28:51,160 --> 00:28:55,120
up so we don't care about querying the

00:28:53,710 --> 00:28:59,050
email addresses of users who have been

00:28:55,120 --> 00:29:02,140
deleted you know in some cases we've

00:28:59,050 --> 00:29:10,780
dropped index sizes by 70 80 % it's

00:29:02,140 --> 00:29:12,280
pretty damn cool so concurrent web

00:29:10,780 --> 00:29:13,480
servers and concurrent application

00:29:12,280 --> 00:29:14,620
access it's led to this case where

00:29:13,480 --> 00:29:18,220
there's a lot of race conditions out

00:29:14,620 --> 00:29:20,050
there I do I do security research on the

00:29:18,220 --> 00:29:21,430
side and I found about five big game

00:29:20,050 --> 00:29:22,930
sites who have race conditions just

00:29:21,430 --> 00:29:26,590
because they're using I RMS with no

00:29:22,930 --> 00:29:29,950
locks as a really common example so you

00:29:26,590 --> 00:29:33,910
know let's look at taking 100 whatever

00:29:29,950 --> 00:29:35,680
our user who has a balance of 300 we

00:29:33,910 --> 00:29:36,820
could do this in Python we could you

00:29:35,680 --> 00:29:38,200
know select their balance from the

00:29:36,820 --> 00:29:39,940
accounts we could check that they have

00:29:38,200 --> 00:29:43,450
an adequate balance we could deduct it

00:29:39,940 --> 00:29:45,250
within the code and then update this

00:29:43,450 --> 00:29:46,840
this creates massive issues when we've

00:29:45,250 --> 00:29:49,060
got concurrent concurrent sessions

00:29:46,840 --> 00:29:51,160
accessing it so say on the Left we

00:29:49,060 --> 00:29:53,020
select the balance on the right we

00:29:51,160 --> 00:29:55,210
select the balance on the left we update

00:29:53,020 --> 00:29:57,490
it gets set to 200 on the right way up

00:29:55,210 --> 00:29:58,660
update it it's 200 that's a really

00:29:57,490 --> 00:30:00,220
common problem

00:29:58,660 --> 00:30:01,420
Postgres has some pretty cool ways of

00:30:00,220 --> 00:30:03,070
dealing with this

00:30:01,420 --> 00:30:04,630
you know most most people on Stack

00:30:03,070 --> 00:30:06,490
Overflow say well just put in a begin

00:30:04,630 --> 00:30:08,020
commit statement that doesn't actually

00:30:06,490 --> 00:30:10,690
solve anything

00:30:08,020 --> 00:30:14,800
we can either use row level locking or

00:30:10,690 --> 00:30:15,970
we can use serializable locking so let's

00:30:14,800 --> 00:30:17,080
look at the first way of you know the

00:30:15,970 --> 00:30:19,420
suggestion of doing this all in one

00:30:17,080 --> 00:30:20,710
statement we can say well updated

00:30:19,420 --> 00:30:22,480
accounts set the balance equals a

00:30:20,710 --> 00:30:23,200
balance minus 100 where the user ID

00:30:22,480 --> 00:30:26,380
equals

00:30:23,200 --> 00:30:27,789
you know X and balances over 100 if we

00:30:26,380 --> 00:30:29,860
want to do any sort of inspection within

00:30:27,789 --> 00:30:31,559
our code this is useless because you

00:30:29,860 --> 00:30:36,340
know we can't actually pull data out

00:30:31,559 --> 00:30:38,289
so what I personally do is row level

00:30:36,340 --> 00:30:41,260
locking so got our session on the Left

00:30:38,289 --> 00:30:43,299
we issue a begin over on our right

00:30:41,260 --> 00:30:47,230
this could be a session that's a setting

00:30:43,299 --> 00:30:48,970
at the same time it issues it again my

00:30:47,230 --> 00:30:50,500
first session was select from the

00:30:48,970 --> 00:30:52,299
accounts where the user ID equals one

00:30:50,500 --> 00:30:55,960
and it will add the full update

00:30:52,299 --> 00:30:58,510
statement on the end what that does that

00:30:55,960 --> 00:30:59,669
locks row if session two tries to access

00:30:58,510 --> 00:31:01,899
that row

00:30:59,669 --> 00:31:03,610
it'll just wait indefinitely the other

00:31:01,899 --> 00:31:06,279
transaction is still in process it has

00:31:03,610 --> 00:31:08,289
to wait over on the left will update the

00:31:06,279 --> 00:31:11,500
balance set it to 200 and will issue a

00:31:08,289 --> 00:31:13,690
commit that row is now unlocked over on

00:31:11,500 --> 00:31:16,169
the right it's cleared it can update the

00:31:13,690 --> 00:31:18,250
balance and set it to 200 oh sorry

00:31:16,169 --> 00:31:20,049
it's select statement is now allowed to

00:31:18,250 --> 00:31:22,630
go through it can update the balance and

00:31:20,049 --> 00:31:24,039
set it to whatever was that we declared

00:31:22,630 --> 00:31:26,200
within the application logic my slides

00:31:24,039 --> 00:31:30,090
are a bit wrong now can issue a commit

00:31:26,200 --> 00:31:32,019
that allows for basically atomic access

00:31:30,090 --> 00:31:34,240
Postgres also has this feature of

00:31:32,019 --> 00:31:35,919
isolated you know just this isolation

00:31:34,240 --> 00:31:37,450
level serializable what that does it

00:31:35,919 --> 00:31:39,850
uses some fancy math that I don't even

00:31:37,450 --> 00:31:42,580
know about to expect which rows might be

00:31:39,850 --> 00:31:45,220
touched so if we are two sessions issue

00:31:42,580 --> 00:31:46,659
this isolation level serializable over

00:31:45,220 --> 00:31:48,490
on the Left we select for update oh

00:31:46,659 --> 00:31:50,350
sorry we just do issue a normal select

00:31:48,490 --> 00:31:52,000
my slides are once again wrong the

00:31:50,350 --> 00:31:53,620
update won't do anything in this case so

00:31:52,000 --> 00:31:56,200
you can leave it in there over on the

00:31:53,620 --> 00:31:58,299
right we select it over on the left we

00:31:56,200 --> 00:32:00,399
update if we try and issue an update

00:31:58,299 --> 00:32:01,840
from the the right-hand session we're

00:32:00,399 --> 00:32:03,429
gonna get an error saying could not

00:32:01,840 --> 00:32:06,970
serialize access due to this concurrent

00:32:03,429 --> 00:32:09,909
update over on the lateral commitment it

00:32:06,970 --> 00:32:13,659
will all go through fine what a lot of

00:32:09,909 --> 00:32:15,610
the libraries for um for this type of

00:32:13,659 --> 00:32:16,960
method so with in Ruby you've got sequel

00:32:15,610 --> 00:32:19,330
and I'm not too sure if Django does it

00:32:16,960 --> 00:32:21,159
they'll basically allow you to specify a

00:32:19,330 --> 00:32:22,630
number of attempts to rerun the

00:32:21,159 --> 00:32:25,870
transaction on the right which did fail

00:32:22,630 --> 00:32:28,269
due to this concurrent update between

00:32:25,870 --> 00:32:29,649
row level locking and this ones once

00:32:28,269 --> 00:32:31,480
basically just this explicit lock and

00:32:29,649 --> 00:32:32,799
the others this optimistic lock you know

00:32:31,480 --> 00:32:34,830
it's hoping nothing else touches it in

00:32:32,799 --> 00:32:37,539
the program in the process

00:32:34,830 --> 00:32:39,179
now over on the right normally commit

00:32:37,539 --> 00:32:42,130
would be issued and you get that error

00:32:39,179 --> 00:32:46,179
in this case a rollback emitters turned

00:32:42,130 --> 00:32:47,470
into a rollback so I'll try and speed

00:32:46,179 --> 00:32:50,049
this up I've got a few more slides left

00:32:47,470 --> 00:32:52,720
there's some other extensions all of

00:32:50,049 --> 00:32:54,159
them have their own dragons as you saw

00:32:52,720 --> 00:32:55,860
yesterday you can do JavaScript within

00:32:54,159 --> 00:32:59,260
your Postgres using pilla v8

00:32:55,860 --> 00:33:02,350
there's trigram tools via sort of the

00:32:59,260 --> 00:33:03,760
trigram extension so you can do looking

00:33:02,350 --> 00:33:05,710
at sequences of letters and they've got

00:33:03,760 --> 00:33:08,169
a few fancy operators for analyzing them

00:33:05,710 --> 00:33:10,270
you've got partition management so my

00:33:08,169 --> 00:33:11,890
first example where we had tables based

00:33:10,270 --> 00:33:14,169
on hour of the day there's now an

00:33:11,890 --> 00:33:15,340
extension to do that for us we did want

00:33:14,169 --> 00:33:16,539
to use that initially but it didn't have

00:33:15,340 --> 00:33:18,370
all the features it's had some really

00:33:16,539 --> 00:33:19,780
good improvement you got fuzzy string

00:33:18,370 --> 00:33:21,700
matching you've got geo functions

00:33:19,780 --> 00:33:23,350
international numbering standards you

00:33:21,700 --> 00:33:26,140
can store trees it's crypto there's

00:33:23,350 --> 00:33:27,340
query statistics there's just this whole

00:33:26,140 --> 00:33:29,530
mixed bag of things that you might wanna

00:33:27,340 --> 00:33:30,730
do in your database if someone else has

00:33:29,530 --> 00:33:32,260
already done with legwork and you get to

00:33:30,730 --> 00:33:38,950
take this code out of Python that's a

00:33:32,260 --> 00:33:40,270
win in my book on on that topic I I was

00:33:38,950 --> 00:33:42,130
I was chatting to a few different people

00:33:40,270 --> 00:33:45,460
and the question came up about doing web

00:33:42,130 --> 00:33:47,559
sessions in your database I know there's

00:33:45,460 --> 00:33:50,760
been a bit of a trend to use Redis for

00:33:47,559 --> 00:33:56,710
this I I still stick with the database

00:33:50,760 --> 00:33:58,480
you know it might impact performance but

00:33:56,710 --> 00:34:00,220
you've got this you've got the ability

00:33:58,480 --> 00:34:02,049
to revoke these sessions so if a user

00:34:00,220 --> 00:34:04,210
goes in they can click well revoke this

00:34:02,049 --> 00:34:05,679
session because my machines been being

00:34:04,210 --> 00:34:09,220
compromised you can get some analytics

00:34:05,679 --> 00:34:10,690
in primary data store as a security

00:34:09,220 --> 00:34:13,119
person if I'm able to get my users a big

00:34:10,690 --> 00:34:14,320
button to hit with a list of the current

00:34:13,119 --> 00:34:15,940
sessions that are active on their

00:34:14,320 --> 00:34:17,560
account that's you know that that's a

00:34:15,940 --> 00:34:19,419
pretty cool feature to give with the

00:34:17,560 --> 00:34:21,520
bredis you can't get that so I say put

00:34:19,419 --> 00:34:23,230
web sessions in your database still if

00:34:21,520 --> 00:34:25,330
it does impact performance give it at

00:34:23,230 --> 00:34:27,760
sown database bump up you know bump up

00:34:25,330 --> 00:34:29,500
the RAM it should be an afterthought in

00:34:27,760 --> 00:34:30,369
terms of performance because you will be

00:34:29,500 --> 00:34:36,940
out of squeeze a fair bit out of

00:34:30,369 --> 00:34:38,740
Postgres so I do have a wish list of

00:34:36,940 --> 00:34:41,599
shiny things that I want in Postgres

00:34:38,740 --> 00:34:43,619
because you know thank you

00:34:41,599 --> 00:34:47,310
because it doesn't have everything I

00:34:43,619 --> 00:34:48,540
absolutely adore elasticsearch as you

00:34:47,310 --> 00:34:49,980
know their search functionality I can

00:34:48,540 --> 00:34:52,379
issue a search string which is this

00:34:49,980 --> 00:34:53,790
really nice syntax to use I can even

00:34:52,379 --> 00:34:55,980
submit that on behalf of your users and

00:34:53,790 --> 00:34:57,599
it will show results some of the

00:34:55,980 --> 00:34:59,190
examples in the documentation that you

00:34:57,599 --> 00:35:01,710
know they take to concentration to look

00:34:59,190 --> 00:35:05,040
at and there's just some really varied

00:35:01,710 --> 00:35:06,480
opinions around scaling you know where

00:35:05,040 --> 00:35:08,099
to from one Postgres server what are we

00:35:06,480 --> 00:35:10,500
going to do if we improve ten times the

00:35:08,099 --> 00:35:12,710
size and we're still trying to store

00:35:10,500 --> 00:35:14,550
records of dinos launched on servers

00:35:12,710 --> 00:35:15,920
we're not even thinking about that at

00:35:14,550 --> 00:35:17,940
the moment because it's kind of yeah

00:35:15,920 --> 00:35:20,970
it's not worth thinking about it were

00:35:17,940 --> 00:35:22,290
just cause undue pain so there's one

00:35:20,970 --> 00:35:24,180
place where I don't use Postgres and

00:35:22,290 --> 00:35:25,530
that's Redis I use Redis for distributed

00:35:24,180 --> 00:35:27,570
job queues and that's basically because

00:35:25,530 --> 00:35:30,390
the libraries like celery and sidekick

00:35:27,570 --> 00:35:32,280
for Ruby they support it it's it's more

00:35:30,390 --> 00:35:33,750
of a convenience really because you know

00:35:32,280 --> 00:35:35,010
they've written it for Redis that just

00:35:33,750 --> 00:35:37,430
means we have to do the rightest

00:35:35,010 --> 00:35:42,599
instance that's all there is to it

00:35:37,430 --> 00:35:43,859
so I guess my takeaways from this would

00:35:42,599 --> 00:35:45,900
be that there's a cost with adding

00:35:43,859 --> 00:35:48,810
services and adding data pipes and

00:35:45,900 --> 00:35:52,170
moving data around you run into dragons

00:35:48,810 --> 00:35:54,050
it causes problems post-race does a

00:35:52,170 --> 00:35:56,520
pretty good job at things I throw at it

00:35:54,050 --> 00:35:57,570
you know I said I've fallen into the

00:35:56,520 --> 00:36:00,240
trap before we're premature optimization

00:35:57,570 --> 00:36:02,250
causes really poor design decisions and

00:36:00,240 --> 00:36:03,570
you know the features complement each

00:36:02,250 --> 00:36:05,010
other I said that you can use a trigger

00:36:03,570 --> 00:36:07,080
to refresh your materialized view of

00:36:05,010 --> 00:36:09,270
some partitioned data just to shove in a

00:36:07,080 --> 00:36:11,550
cache and who cares if it's two minutes

00:36:09,270 --> 00:36:15,300
out of day date you know if we don't

00:36:11,550 --> 00:36:17,730
need that I'm also a big fan of the

00:36:15,300 --> 00:36:19,410
consistency consistency they're just

00:36:17,730 --> 00:36:21,000
having everything in Postgres provides I

00:36:19,410 --> 00:36:22,650
know you can talk to read us from within

00:36:21,000 --> 00:36:25,859
Postgres but Redis doesn't have a way to

00:36:22,650 --> 00:36:27,119
rollback transactions so over time there

00:36:25,859 --> 00:36:28,920
is going to be a near consistent State

00:36:27,119 --> 00:36:31,290
my data and I'm not ready to accept that

00:36:28,920 --> 00:36:33,060
all right with the trade-off being a

00:36:31,290 --> 00:36:35,580
small performance Rob I'm happy with

00:36:33,060 --> 00:36:36,930
that and most of all I have to give a

00:36:35,580 --> 00:36:39,119
shout out to the Postgres community

00:36:36,930 --> 00:36:41,310
because they have got an awesome roadmap

00:36:39,119 --> 00:36:44,119
their code is you know fantastic and

00:36:41,310 --> 00:36:46,380
there's always improvements

00:36:44,119 --> 00:36:48,720
two years ago where I was using all

00:36:46,380 --> 00:36:51,359
these brand new services to now yeah I

00:36:48,720 --> 00:36:52,740
you know I've I've had developed a

00:36:51,359 --> 00:36:54,869
really fond appreciation for postcards

00:36:52,740 --> 00:36:55,470
so I'd recommend it using it in place of

00:36:54,869 --> 00:36:59,609
a lot of tools

00:36:55,470 --> 00:37:02,340
you use so yeah thanks for listening to

00:36:59,609 --> 00:37:03,450
the opinionated really talk and we're

00:37:02,340 --> 00:37:06,660
gonna have a bit of time for questions

00:37:03,450 --> 00:37:08,550
if anyone has any did anyone learn

00:37:06,660 --> 00:37:10,619
anything about Postgres that they didn't

00:37:08,550 --> 00:37:33,210
know existed okay thank you my job is

00:37:10,619 --> 00:37:35,490
done Oh God okay just use MongoDB so you

00:37:33,210 --> 00:37:39,359
win over CTS and also the recursive

00:37:35,490 --> 00:37:43,609
expression yep in other less open-source

00:37:39,359 --> 00:37:46,590
databases you use the CTA's to do

00:37:43,609 --> 00:37:49,830
recursive queries yep can you do that in

00:37:46,590 --> 00:37:52,590
Postgres yes yes I didn't have enough

00:37:49,830 --> 00:37:53,730
slider in to do it but I would chat you

00:37:52,590 --> 00:37:54,990
about this repo afterwards because

00:37:53,730 --> 00:37:56,790
that'd be a really fun challenge and I

00:37:54,990 --> 00:37:58,260
would love to put that in my repo right

00:37:56,790 --> 00:38:00,300
yeah absolutely

00:37:58,260 --> 00:38:02,250
plus we don't like paying for things

00:38:00,300 --> 00:38:07,320
open-source is nice it's free it's

00:38:02,250 --> 00:38:10,650
really good yeah thank you that was a

00:38:07,320 --> 00:38:12,930
brilliant talk thing is there any way

00:38:10,650 --> 00:38:14,670
for Postgres to do essentially a live

00:38:12,930 --> 00:38:17,970
query where I do a select and then I

00:38:14,670 --> 00:38:20,010
start getting some mechanism where I can

00:38:17,970 --> 00:38:22,470
say what's changed since the last time

00:38:20,010 --> 00:38:24,839
this happened or even like the listen

00:38:22,470 --> 00:38:26,670
notify get notifications that hey

00:38:24,839 --> 00:38:29,220
there's some data that affects your

00:38:26,670 --> 00:38:33,000
query that might be spending multiple

00:38:29,220 --> 00:38:35,910
tables through joins so like a streaming

00:38:33,000 --> 00:38:38,099
view yeah yeah I know there are some

00:38:35,910 --> 00:38:40,410
extensions out there and I know that a

00:38:38,099 --> 00:38:42,060
few companies are using developing

00:38:40,410 --> 00:38:44,790
openings open source tools on Postgres

00:38:42,060 --> 00:38:46,710
we can do streaming views

00:38:44,790 --> 00:38:48,390
so basically subscribe to a view and it

00:38:46,710 --> 00:38:51,089
it'll do your query and presenting with

00:38:48,390 --> 00:38:52,740
results so I know people are interested

00:38:51,089 --> 00:38:53,849
but it's a really hard topic there are

00:38:52,740 --> 00:38:54,990
levels computer science I don't even

00:38:53,849 --> 00:38:57,330
want to touch where people are

00:38:54,990 --> 00:38:58,560
investigating this problem if you don't

00:38:57,330 --> 00:39:02,359
mind stale data there are definitely

00:38:58,560 --> 00:39:02,359
ways to automate it though so

00:39:02,890 --> 00:39:09,529
yeah great talk thanks thanks another

00:39:05,569 --> 00:39:11,690
question about CTAs do you know if the

00:39:09,529 --> 00:39:14,869
query optimizer is able to like get in

00:39:11,690 --> 00:39:16,789
and optimize you know inside the CTE

00:39:14,869 --> 00:39:18,890
like is that a optimization boundary or

00:39:16,789 --> 00:39:20,960
is it oh I believe there are you know

00:39:18,890 --> 00:39:22,400
it's not as optimized as say that the

00:39:20,960 --> 00:39:24,410
table indexes or doing a materialized

00:39:22,400 --> 00:39:25,579
view with indexes yeah but I know there

00:39:24,410 --> 00:39:26,989
is some forethought that's put into the

00:39:25,579 --> 00:39:29,420
planner to actually make these things

00:39:26,989 --> 00:39:31,190
pretty fast with that verifications

00:39:29,420 --> 00:39:33,680
example I had there's there's probably

00:39:31,190 --> 00:39:35,210
150,000 verifications in that table it's

00:39:33,680 --> 00:39:37,940
multiple millisecond so it does

00:39:35,210 --> 00:39:40,549
something on the back end if you want to

00:39:37,940 --> 00:39:41,900
put that into the repo I'd gladly do do

00:39:40,549 --> 00:39:43,009
an analysis of how the planner is

00:39:41,900 --> 00:39:43,730
actually stepping through these things

00:39:43,009 --> 00:39:44,900
so I think that'd be a really

00:39:43,730 --> 00:39:48,140
interesting one to investigate

00:39:44,900 --> 00:39:49,640
cool thanks Sarris I've got good for

00:39:48,140 --> 00:39:51,349
this in the past but didn't financing

00:39:49,640 --> 00:39:53,059
very good and would you know where I can

00:39:51,349 --> 00:39:58,009
go to get the best info to migrate from

00:39:53,059 --> 00:39:59,809
maestro to Bosporus every project within

00:39:58,009 --> 00:40:03,940
the Postgres community to help with that

00:39:59,809 --> 00:40:07,009
to help with it I I know that there is I

00:40:03,940 --> 00:40:08,900
think the name might be taps but it was

00:40:07,009 --> 00:40:10,249
basically a generic generic wrapper for

00:40:08,900 --> 00:40:13,160
databases to move data from one to the

00:40:10,249 --> 00:40:15,049
other can I talk to you after this

00:40:13,160 --> 00:40:16,640
because I know that I know that people

00:40:15,049 --> 00:40:17,930
at Heroku have worked on these things so

00:40:16,640 --> 00:40:23,569
I'd love to get you details and work

00:40:17,930 --> 00:40:25,309
this one out okay thanks yeah thanks

00:40:23,569 --> 00:40:27,589
Reese um I just have a question about

00:40:25,309 --> 00:40:29,749
the new JSON B data type you talked

00:40:27,589 --> 00:40:32,390
about yeah so if I got some tables that

00:40:29,749 --> 00:40:34,609
have JSON data type say I did them on 9

00:40:32,390 --> 00:40:36,829
- yep and we want to just alter the

00:40:34,609 --> 00:40:38,809
columns to change it to JSON B most

00:40:36,829 --> 00:40:40,489
careers do all the kind of optimizations

00:40:38,809 --> 00:40:42,890
that you would you know you'd expect to

00:40:40,489 --> 00:40:44,569
get out of Jason B if we just want to if

00:40:42,890 --> 00:40:45,829
you want to change the type yes we would

00:40:44,569 --> 00:40:47,450
just wait like we just want to change

00:40:45,829 --> 00:40:49,190
the type from JSON to the new JSON B

00:40:47,450 --> 00:40:51,829
basically I mean if you if you do a step

00:40:49,190 --> 00:40:53,329
migration you could add a another column

00:40:51,829 --> 00:40:54,769
to the table with the Jason B data type

00:40:53,329 --> 00:40:57,799
and just do a coercion from the string

00:40:54,769 --> 00:41:00,769
to the Jason B you might have two or

00:40:57,799 --> 00:41:01,789
three minutes of I guess issues while

00:41:00,769 --> 00:41:03,380
you do the swing but if you do it in

00:41:01,789 --> 00:41:04,880
small steps it will reoptimize it into

00:41:03,380 --> 00:41:07,880
the actual binary format so yeah

00:41:04,880 --> 00:41:09,319
definitely doable I could put that in

00:41:07,880 --> 00:41:11,180
exact as an example in the repo as well

00:41:09,319 --> 00:41:13,039
these are all great questions so yeah

00:41:11,180 --> 00:41:15,489
cool thanks Cheers

00:41:13,039 --> 00:41:18,529
you mentioned there might be some

00:41:15,489 --> 00:41:20,929
extensions to help with live query type

00:41:18,529 --> 00:41:24,589
scenario and you also mentioned a great

00:41:20,929 --> 00:41:26,239
list of extensions in your talk when I

00:41:24,589 --> 00:41:28,630
looked at the Postgres documentation

00:41:26,239 --> 00:41:32,539
there was essentially a list of cryptic

00:41:28,630 --> 00:41:34,400
extension names and no description on it

00:41:32,539 --> 00:41:36,589
until you actually click on each link is

00:41:34,400 --> 00:41:39,949
there a good source of information on

00:41:36,589 --> 00:41:41,989
third-party Postgres contributed code

00:41:39,949 --> 00:41:43,369
yeah I know within the documentation I

00:41:41,989 --> 00:41:44,749
mean one easy way to do it is just to

00:41:43,369 --> 00:41:45,979
click the next button and see the name

00:41:44,749 --> 00:41:49,039
and then do the quick description of

00:41:45,979 --> 00:41:49,999
what it reads but there's no real good I

00:41:49,039 --> 00:41:51,439
guess

00:41:49,999 --> 00:41:52,789
source for it I know there is now a

00:41:51,439 --> 00:41:56,029
package manager for post-crisis

00:41:52,789 --> 00:41:58,249
extensions so I can talk about that

00:41:56,029 --> 00:41:59,869
afterwards as well I can't we took

00:41:58,249 --> 00:42:01,699
mainly because I can't remember the name

00:41:59,869 --> 00:42:03,739
of it it's four letters in combination I

00:42:01,699 --> 00:42:05,599
wouldn't want to get the wrong bit maybe

00:42:03,739 --> 00:42:16,729
that's the one thank you

00:42:05,599 --> 00:42:18,739
sorry what was that thank you PGX n okay

00:42:16,729 --> 00:42:20,179
so there we go being able to install

00:42:18,739 --> 00:42:21,559
dissensions so they might have some sort

00:42:20,179 --> 00:42:23,449
of directory or they might be putting in

00:42:21,559 --> 00:42:25,069
a directory where you've got a nice list

00:42:23,449 --> 00:42:27,459
where translates a name to what it

00:42:25,069 --> 00:42:27,459
actually does

00:42:37,430 --> 00:42:39,490
you

00:42:44,869 --> 00:42:46,930

YouTube URL: https://www.youtube.com/watch?v=UgcC_bY4rPg


