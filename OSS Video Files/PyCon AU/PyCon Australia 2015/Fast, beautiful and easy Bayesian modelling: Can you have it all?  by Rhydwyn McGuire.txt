Title: Fast, beautiful and easy Bayesian modelling: Can you have it all?  by Rhydwyn McGuire
Publication date: 2015-08-04
Playlist: PyCon Australia 2015
Description: 
	Bayesian models are often more useful than classical statistical models when dealing with data concerning rare events in small populations. For example, if you are looking at the number of cancer diagnoses in a local government area, Bayesian models allow you to combine the individual and population level data to produce more reliable estimates. 

Fitting these models can be both complex and computationally expensive, so we need it to be fast. Some of the datasets we deal with are extremely big, and the need for both tweaking and regular update cycles means we need to re-run the models frequently.

But beauty is also crucial. We need the package to be easy to code and describe, so that the statisticians and scientists we work with can build, maintain and understand their own models without needing to refer to programmers. We don’t want a statistical black box. 

Finally, we need it to be easy. Easy to fit into our systems; technology-agnostic so we can change systems; and automatable so that the regular work of updating models with new data can happen ‘hands off’.  

There are a number of Bayesian modelling packages available, but how do you know which one to use? This talk will take you through the positives and negatives of the major packages, focusing on the specifics of my work in health statistics, as well as providing a general overview of what these packages can do. 


PyCon Australia is the national conference for users of the Python Programming Language. In 2015, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

July 31-August 4, Brisbane, Queensland, Australia
Captions: 
	00:00:09,660 --> 00:00:13,349
making science easier and getting

00:00:11,670 --> 00:00:16,289
meaningful meaningful results in two

00:00:13,349 --> 00:00:26,039
scientists and policymakers hands rid

00:00:16,289 --> 00:00:28,829
please until me thank you as I said fast

00:00:26,039 --> 00:00:38,820
beautiful and easy bayesian modeling can

00:00:28,829 --> 00:00:40,800
we have it all Who am I I'm Redwyne I'm

00:00:38,820 --> 00:00:44,940
a medical statistician working in public

00:00:40,800 --> 00:00:47,579
health this means that while I have some

00:00:44,940 --> 00:00:50,479
computer science in my background almost

00:00:47,579 --> 00:00:54,000
everyone that I work with doesn't we

00:00:50,479 --> 00:00:56,519
trained scripting in data languages on

00:00:54,000 --> 00:00:59,009
the job and we don't do it that well and

00:00:56,519 --> 00:01:01,549
so that's going to come into my talk I'm

00:00:59,009 --> 00:01:05,790
not working with people who do

00:01:01,549 --> 00:01:08,189
end-to-end well tested systems unit

00:01:05,790 --> 00:01:14,640
testing is a great idea and I look

00:01:08,189 --> 00:01:18,899
forward to it we have where I work we do

00:01:14,640 --> 00:01:21,840
open data where um we're providing data

00:01:18,899 --> 00:01:26,070
on the new south wales health system to

00:01:21,840 --> 00:01:29,159
a whole range of customers and often

00:01:26,070 --> 00:01:32,490
they want data which is about rare

00:01:29,159 --> 00:01:35,479
diseases or rare events and about small

00:01:32,490 --> 00:01:39,780
populations the kind of people will get

00:01:35,479 --> 00:01:42,810
will get a someone who's working in a

00:01:39,780 --> 00:01:44,520
community group or a charity who wants

00:01:42,810 --> 00:01:47,460
to know about the kind of cancer that

00:01:44,520 --> 00:01:49,890
they work with but that it might not be

00:01:47,460 --> 00:01:52,080
a very common cancer and if they want to

00:01:49,890 --> 00:01:53,940
break down over local government areas

00:01:52,080 --> 00:01:57,330
you might be talking about some local

00:01:53,940 --> 00:02:00,180
government get areas having a handful of

00:01:57,330 --> 00:02:02,400
cases a year a lot of people get cancer

00:02:00,180 --> 00:02:05,670
but once you're starting to talk about

00:02:02,400 --> 00:02:08,039
small cell lung cancer which is the way

00:02:05,670 --> 00:02:10,799
that researchers that policymakers that

00:02:08,039 --> 00:02:13,019
journalists want to think about that can

00:02:10,799 --> 00:02:16,040
be a really small number so you get a

00:02:13,019 --> 00:02:16,040
lot of variation

00:02:18,620 --> 00:02:25,909
we make our data publicly available

00:02:22,580 --> 00:02:29,690
through maps like this so this is the

00:02:25,909 --> 00:02:33,670
end point nice easy to understand at a

00:02:29,690 --> 00:02:33,670
glance stuff

00:02:34,520 --> 00:02:41,540
what do I mean by fast we do a lot of

00:02:38,480 --> 00:02:46,870
those maps we have a two weekly release

00:02:41,540 --> 00:02:49,640
cycle and in that we might push 30

00:02:46,870 --> 00:02:52,070
diseases or hospitalizations or whatever

00:02:49,640 --> 00:02:54,980
we're thinking about will do that broken

00:02:52,070 --> 00:02:57,080
down by sex by age groups by other

00:02:54,980 --> 00:03:00,350
demographics like Aboriginality or

00:02:57,080 --> 00:03:04,190
country of birth and we're building a

00:03:00,350 --> 00:03:07,700
map for every one of those if we're

00:03:04,190 --> 00:03:11,060
looking at a 25-minute build time for

00:03:07,700 --> 00:03:13,700
one of these models that can mean that

00:03:11,060 --> 00:03:17,090
we're looking at a couple of days just

00:03:13,700 --> 00:03:21,650
to get the data prepared which runs

00:03:17,090 --> 00:03:24,410
everything else for down to a halt there

00:03:21,650 --> 00:03:26,420
are obviously other options but again

00:03:24,410 --> 00:03:29,090
i'm working with statistical analysts

00:03:26,420 --> 00:03:32,870
I'm not working with programmers things

00:03:29,090 --> 00:03:35,750
like dupe are great but it's an extra

00:03:32,870 --> 00:03:37,580
level of understanding it's a lot of

00:03:35,750 --> 00:03:39,500
extra training to get people who are not

00:03:37,580 --> 00:03:43,130
used to thinking about things like

00:03:39,500 --> 00:03:44,690
networks into a position where you need

00:03:43,130 --> 00:03:46,820
to think about I might not have access

00:03:44,690 --> 00:03:49,670
to my hard drive the network might go

00:03:46,820 --> 00:03:51,560
down this is so it's good to do it all

00:03:49,670 --> 00:03:54,170
in-house we've got a local data server

00:03:51,560 --> 00:03:57,050
which is quite powerful but it can be

00:03:54,170 --> 00:03:59,300
not really powerful enough we want it to

00:03:57,050 --> 00:04:01,730
be easy then not programmers I don't

00:03:59,300 --> 00:04:03,709
want to be teaching advanced programming

00:04:01,730 --> 00:04:05,240
topics it's a waste of my time it's a

00:04:03,709 --> 00:04:07,760
waste of their time we want this to be

00:04:05,240 --> 00:04:09,170
accessible we want them to be able to do

00:04:07,760 --> 00:04:16,400
what they can do which is be good

00:04:09,170 --> 00:04:18,260
statisticians not deep coders but I like

00:04:16,400 --> 00:04:19,790
code that's beautiful I don't want to

00:04:18,260 --> 00:04:22,580
look at something that's ugly all day I

00:04:19,790 --> 00:04:24,140
don't want the syntax to fight me I

00:04:22,580 --> 00:04:26,860
don't want to have to struggle to

00:04:24,140 --> 00:04:29,450
understand what I'm looking at

00:04:26,860 --> 00:04:31,520
here's a classic example you'll find

00:04:29,450 --> 00:04:34,610
this in a lot of text books about the

00:04:31,520 --> 00:04:36,380
problems we have so kidney cancer death

00:04:34,610 --> 00:04:37,820
rates can tell us some really important

00:04:36,380 --> 00:04:41,810
things it might be about how late you're

00:04:37,820 --> 00:04:43,730
being diagnosed it might be about what

00:04:41,810 --> 00:04:45,560
your access to care is like but if we

00:04:43,730 --> 00:04:49,190
look at the highest and lowest cancer

00:04:45,560 --> 00:04:50,960
rates they look fairly similar if you

00:04:49,190 --> 00:04:55,190
know anything about the geography of the

00:04:50,960 --> 00:05:00,260
u.s. you might see what's the similarity

00:04:55,190 --> 00:05:03,380
is population density looks like this

00:05:00,260 --> 00:05:05,900
all those high and low rates were small

00:05:03,380 --> 00:05:07,760
number issues they were low and high

00:05:05,900 --> 00:05:10,850
because it might be that only five

00:05:07,760 --> 00:05:13,130
people in that County had kidney cancer

00:05:10,850 --> 00:05:16,480
so the difference between two of them

00:05:13,130 --> 00:05:19,550
dying and three of them dying is huge

00:05:16,480 --> 00:05:20,960
this isn't great this again we're

00:05:19,550 --> 00:05:22,220
working with journalists we're working

00:05:20,960 --> 00:05:26,450
with politicians were working with

00:05:22,220 --> 00:05:28,640
policymakers they get headlines they get

00:05:26,450 --> 00:05:30,170
raked over the coals over a twenty

00:05:28,640 --> 00:05:31,850
percent increase in the cancer eight

00:05:30,170 --> 00:05:34,930
deaths we need to be providing them

00:05:31,850 --> 00:05:37,610
better options than that so that they

00:05:34,930 --> 00:05:39,590
can come back and say yeah no look it's

00:05:37,610 --> 00:05:43,870
a 20-percent jump but there's not really

00:05:39,590 --> 00:05:43,870
a 20 cent job it's a one-person job

00:05:46,770 --> 00:05:51,750
here's an example in baseball this graph

00:05:49,259 --> 00:05:55,020
is great you've got a really good sign

00:05:51,750 --> 00:05:59,039
of variance and I hope that no one would

00:05:55,020 --> 00:06:01,650
look at this and say hey that guy by the

00:05:59,039 --> 00:06:04,080
first red line right up there that's the

00:06:01,650 --> 00:06:07,860
batter I want that's the best performer

00:06:04,080 --> 00:06:10,440
of this year like hire that guy it's

00:06:07,860 --> 00:06:18,180
beginner's luck we know this but we want

00:06:10,440 --> 00:06:20,190
to wrap this up in a model this is this

00:06:18,180 --> 00:06:22,319
is pike on a you I don't want to talk to

00:06:20,190 --> 00:06:27,690
you about baseball let's talk about

00:06:22,319 --> 00:06:30,990
cricket right so when I played cricket

00:06:27,690 --> 00:06:33,240
as a kid I loved hitting over the fence

00:06:30,990 --> 00:06:37,710
whole lot of reasons got to annoy the

00:06:33,240 --> 00:06:40,800
neighbors pretty sweet definite bragging

00:06:37,710 --> 00:06:43,050
rights so let's measure this that way

00:06:40,800 --> 00:06:44,849
the rate of hitting sextants I'm going

00:06:43,050 --> 00:06:47,460
to call the best australian cricketer

00:06:44,849 --> 00:06:53,729
the guy who hit the most sixes i'm going

00:06:47,460 --> 00:06:57,919
to go for tests days and twenty20 first

00:06:53,729 --> 00:06:57,919
class oh sorry international

00:07:01,539 --> 00:07:07,030
so here's what that data looks like

00:07:03,590 --> 00:07:11,930
again now the interesting thing is that

00:07:07,030 --> 00:07:13,130
the baseball's was much larger than this

00:07:11,930 --> 00:07:15,860
so what we've got we've got a whole

00:07:13,130 --> 00:07:17,150
bunch of people who never hits exits now

00:07:15,860 --> 00:07:18,560
a lot of those are going to be primarily

00:07:17,150 --> 00:07:21,259
bowlers or they're going to be people

00:07:18,560 --> 00:07:23,780
who don't have that aggressive playing

00:07:21,259 --> 00:07:25,430
style some of them might be very good i

00:07:23,780 --> 00:07:31,759
was told at dinner last night that don

00:07:25,430 --> 00:07:33,979
bradman got only six sexes in his entire

00:07:31,759 --> 00:07:35,930
career I'm not going to stand here and

00:07:33,979 --> 00:07:44,990
say he was a crap player he wasn't very

00:07:35,930 --> 00:07:47,360
good at getting sexes so as we said in

00:07:44,990 --> 00:07:49,360
the last one who's our best player who's

00:07:47,360 --> 00:07:54,340
that guy that's right aren't there

00:07:49,360 --> 00:07:54,340
 brilliant at hitting sixes sorry

00:07:54,759 --> 00:08:01,789
chris linn show of hands who's heard of

00:07:58,069 --> 00:08:04,789
chris linn more than none of you you're

00:08:01,789 --> 00:08:07,460
doing better than me now this guy is

00:08:04,789 --> 00:08:10,190
pretty good at hitting sexes he's fairly

00:08:07,460 --> 00:08:12,770
young he only plays 20 20 and 20 20

00:08:10,190 --> 00:08:15,919
years again where that's useful so maybe

00:08:12,770 --> 00:08:18,380
he is but I'm still nervous I'm still

00:08:15,919 --> 00:08:21,740
interested in this guy I think he might

00:08:18,380 --> 00:08:25,930
be our guy um he's got a lot more

00:08:21,740 --> 00:08:29,409
experience we can be lot more shot

00:08:25,930 --> 00:08:29,409
that's chris linn

00:08:30,470 --> 00:08:36,080
how are we going to do this we're going

00:08:34,789 --> 00:08:39,229
to use what's called a shrinkage

00:08:36,080 --> 00:08:42,919
estimator we're going to use a bit of

00:08:39,229 --> 00:08:44,720
information from the players individual

00:08:42,919 --> 00:08:47,210
score that's the rate that we saw in the

00:08:44,720 --> 00:08:51,980
first graph and we're going to use a bit

00:08:47,210 --> 00:08:54,530
of information from the team score that

00:08:51,980 --> 00:08:56,300
is the average information and we're

00:08:54,530 --> 00:08:57,620
going to trust because basically we've

00:08:56,300 --> 00:09:00,650
got two things going on here we've got

00:08:57,620 --> 00:09:03,860
actual talent your ability to hit sixes

00:09:00,650 --> 00:09:05,180
in a kind of long-term way and we've got

00:09:03,860 --> 00:09:06,740
some luck and we've got a lot of

00:09:05,180 --> 00:09:08,840
beginners luck in those people that we

00:09:06,740 --> 00:09:11,780
don't have a whole lot of data for so

00:09:08,840 --> 00:09:15,590
we're going to trust people are really

00:09:11,780 --> 00:09:17,830
talented more if we've got more data for

00:09:15,590 --> 00:09:17,830
them

00:09:21,200 --> 00:09:30,460
so now I'm going to talk about defining

00:09:26,090 --> 00:09:33,500
the problem space in a pythonic way so

00:09:30,460 --> 00:09:35,630
people who know me know that I do most

00:09:33,500 --> 00:09:39,890
of my work in our I'm a big fan of are

00:09:35,630 --> 00:09:42,530
we can have those fights later notable

00:09:39,890 --> 00:09:45,920
mentions our PI exists there are lots of

00:09:42,530 --> 00:09:50,000
great libraries in our that do bayesian

00:09:45,920 --> 00:09:52,810
modeling they're out of scope there's

00:09:50,000 --> 00:09:55,370
lots of good things about them but

00:09:52,810 --> 00:10:00,260
that's cheating we could do a whole day

00:09:55,370 --> 00:10:04,430
on bayesian modeling in our we are going

00:10:00,260 --> 00:10:08,230
to look at hi Stan excuse me which is an

00:10:04,430 --> 00:10:10,700
independent library written in C++

00:10:08,230 --> 00:10:12,890
bindings into lots of things and we're

00:10:10,700 --> 00:10:15,130
going to talk about AMC which is a pure

00:10:12,890 --> 00:10:15,130
python

00:10:21,990 --> 00:10:28,570
wipe I Stan

00:10:24,930 --> 00:10:32,050
hamiltonian monte carlo with a no u-turn

00:10:28,570 --> 00:10:34,630
sampler is really cool and really fast

00:10:32,050 --> 00:10:36,640
and it's got a really low memory

00:10:34,630 --> 00:10:39,670
footprint compared to a lot of other

00:10:36,640 --> 00:10:41,769
options this means you can fit bigger

00:10:39,670 --> 00:10:46,570
models than you would otherwise have

00:10:41,769 --> 00:10:49,570
been able to do heavily templated c++ as

00:10:46,570 --> 00:10:52,380
i said good performance lots of

00:10:49,570 --> 00:10:58,420
interfaces your models are written in

00:10:52,380 --> 00:11:00,310
specific stand syntax and then you run

00:10:58,420 --> 00:11:03,010
them using in it using an execute in

00:11:00,310 --> 00:11:05,940
Python that exposes all the data to

00:11:03,010 --> 00:11:09,910
python and then you can work with the

00:11:05,940 --> 00:11:11,620
the data in Python same for your other

00:11:09,910 --> 00:11:13,540
things so you can actually move these

00:11:11,620 --> 00:11:16,209
models around which might be useful if

00:11:13,540 --> 00:11:19,769
you're not sure where you're going to

00:11:16,209 --> 00:11:19,769
end up from a modeling perspective

00:11:23,820 --> 00:11:28,740
let's start with a pulled model so

00:11:27,030 --> 00:11:30,350
basically what we're going to do for our

00:11:28,740 --> 00:11:34,320
first attempt is we're going to say

00:11:30,350 --> 00:11:36,150
there's no such thing as talent every

00:11:34,320 --> 00:11:39,750
cricketer is as good as every other

00:11:36,150 --> 00:11:41,760
cricketer at hitting sexes everything

00:11:39,750 --> 00:11:43,260
about those differences the ones who

00:11:41,760 --> 00:11:45,870
have a lot of sexes the ones who have no

00:11:43,260 --> 00:11:51,300
sexes that's all luck don't care we're

00:11:45,870 --> 00:11:54,060
just looking at the center value so when

00:11:51,300 --> 00:11:56,370
we're matching sixes as a binomial

00:11:54,060 --> 00:12:00,150
distribution on the numbers of balls

00:11:56,370 --> 00:12:03,420
you've faced and theta which is the

00:12:00,150 --> 00:12:05,310
group mean the group talent and that's

00:12:03,420 --> 00:12:07,340
always going to be zero and one it's got

00:12:05,310 --> 00:12:15,110
a really simple understanding it's just

00:12:07,340 --> 00:12:15,110
the rate of hitting sixes on balls faced

00:12:16,430 --> 00:12:24,089
for a reminder on binomial data it's a

00:12:20,010 --> 00:12:26,459
series of independent events it's it's

00:12:24,089 --> 00:12:29,580
the model that you'd use for flipping

00:12:26,459 --> 00:12:32,790
coins or rolling dices anything like

00:12:29,580 --> 00:12:34,890
that really nice and simple the mean is

00:12:32,790 --> 00:12:36,480
just your rate of hitting sixes by the

00:12:34,890 --> 00:12:38,820
numbers of balls you're faced you've got

00:12:36,480 --> 00:12:40,950
a rate of point one and you face 100

00:12:38,820 --> 00:12:46,230
balls then we'd expect you to get about

00:12:40,950 --> 00:12:48,870
10 sexes the variance is n theta by 1

00:12:46,230 --> 00:12:50,940
minus theta so the closest closer you

00:12:48,870 --> 00:12:54,360
are to fifty percent the closer your

00:12:50,940 --> 00:12:57,120
rate is to the middle the larger the

00:12:54,360 --> 00:12:58,529
variance so the more if someone's

00:12:57,120 --> 00:13:01,680
hitting around fifty percent and we'd

00:12:58,529 --> 00:13:04,589
expect them to to get more different

00:13:01,680 --> 00:13:08,660
numbers over a bunch of trials then

00:13:04,589 --> 00:13:08,660
someone who's got a one-percent chance

00:13:11,290 --> 00:13:17,079
this is the stand syntax whether or not

00:13:15,519 --> 00:13:19,839
you like it as a matter of taste I

00:13:17,079 --> 00:13:21,550
really like it I think for

00:13:19,839 --> 00:13:23,160
mathematicians for statisticians if

00:13:21,550 --> 00:13:26,709
you're dealing with people like that

00:13:23,160 --> 00:13:29,800
it's really quite intuitive it looks a

00:13:26,709 --> 00:13:33,490
lot like the kind of maths that you're

00:13:29,800 --> 00:13:36,459
used to doing and that's on purpose we

00:13:33,490 --> 00:13:41,949
have data where we're defining the

00:13:36,459 --> 00:13:43,959
number of players we have the number of

00:13:41,949 --> 00:13:45,579
balls they face that's a vector because

00:13:43,959 --> 00:13:47,740
every players faced a different number

00:13:45,579 --> 00:13:50,889
of balls and the number of sixes that

00:13:47,740 --> 00:13:55,209
that play a hit we've got a parameter

00:13:50,889 --> 00:13:59,160
theta that's our ability to hit balls

00:13:55,209 --> 00:14:01,420
are there our ability to hit sixes sorry

00:13:59,160 --> 00:14:05,680
we're giving theta a fairly

00:14:01,420 --> 00:14:07,389
uninformative prior arm that biases us

00:14:05,680 --> 00:14:09,399
to towards the middle but not very

00:14:07,389 --> 00:14:11,199
strongly basically all we're doing is

00:14:09,399 --> 00:14:14,470
cutting down the search space and just

00:14:11,199 --> 00:14:18,310
saying look it's going to be around

00:14:14,470 --> 00:14:22,139
there and we're setting up the

00:14:18,310 --> 00:14:22,139
likelihood as we saw on the last slide

00:14:24,649 --> 00:14:35,180
in Python it looks like this we just set

00:14:28,699 --> 00:14:37,160
up some data run fit bin the model code

00:14:35,180 --> 00:14:39,879
here I've just put in as a string you

00:14:37,160 --> 00:14:43,189
can also just call it as a file on disk

00:14:39,879 --> 00:14:45,230
10,000 iterations for four chains so

00:14:43,189 --> 00:14:47,059
that's just how long I want it to run

00:14:45,230 --> 00:14:50,689
and I want it to do that on four

00:14:47,059 --> 00:14:55,249
processes stan is multi-threaded so

00:14:50,689 --> 00:14:57,619
upping your number of chains is a good

00:14:55,249 --> 00:15:03,740
way to increase your sampling it's

00:14:57,619 --> 00:15:05,929
really trivially parallelizable we

00:15:03,740 --> 00:15:07,939
extract the data fairly easily and if

00:15:05,929 --> 00:15:10,189
we've got Matt plot lib installed we can

00:15:07,939 --> 00:15:12,999
get really nice plots just through the

00:15:10,189 --> 00:15:12,999
plot functions

00:15:14,550 --> 00:15:20,760
you may be wondering why I'm saying plot

00:15:17,130 --> 00:15:23,250
for eater and mu that is a mistake that

00:15:20,760 --> 00:15:26,010
is our next model will have those we

00:15:23,250 --> 00:15:27,630
should have a plot theta there this is

00:15:26,010 --> 00:15:33,810
what happens when you write slides at

00:15:27,630 --> 00:15:36,180
the last minute one thing that I didn't

00:15:33,810 --> 00:15:38,190
want to do would spend all my time

00:15:36,180 --> 00:15:40,529
telling you about a model that you could

00:15:38,190 --> 00:15:43,170
fit in stats models in five minutes in

00:15:40,529 --> 00:15:45,329
an entirely frequentist framework so

00:15:43,170 --> 00:15:47,430
let's actually talk about that shrinkage

00:15:45,329 --> 00:15:51,060
model let's have a look at how we do

00:15:47,430 --> 00:15:53,339
when we want to have a more complex

00:15:51,060 --> 00:15:59,399
model and a model which is a reason that

00:15:53,339 --> 00:16:02,190
we'd go to Bayesian analysis so now

00:15:59,399 --> 00:16:08,940
first line looks very similar but we've

00:16:02,190 --> 00:16:12,980
picked up some underscores because every

00:16:08,940 --> 00:16:17,010
player now has their own sexes their own

00:16:12,980 --> 00:16:19,890
balls as they did before but also their

00:16:17,010 --> 00:16:22,410
own cita they have their own parameter

00:16:19,890 --> 00:16:25,380
they have a level of talent but these

00:16:22,410 --> 00:16:27,950
talents aren't completely independent

00:16:25,380 --> 00:16:32,959
the talents are forced to center around

00:16:27,950 --> 00:16:32,959
and come in towards the

00:16:36,080 --> 00:16:46,480
the general populations theta which is

00:16:42,980 --> 00:16:46,480
mu in this one

00:16:49,140 --> 00:16:55,030
now

00:16:52,030 --> 00:16:58,780
you may there's an inverse logit out the

00:16:55,030 --> 00:17:00,490
front of our theta this is if you've

00:16:58,780 --> 00:17:04,090
ever done logistic regression or we're

00:17:00,490 --> 00:17:05,530
doing here is setting up a logistic

00:17:04,090 --> 00:17:07,570
regression which means that we're taking

00:17:05,530 --> 00:17:10,780
numbers that can occur anywhere on the

00:17:07,570 --> 00:17:17,320
real line and wheres mapping them in 201

00:17:10,780 --> 00:17:24,100
so that we've got a nice so that we've

00:17:17,320 --> 00:17:26,940
always got a valid value here's the

00:17:24,100 --> 00:17:26,940
logic function

00:17:27,699 --> 00:17:34,809
so in Stan we set up very similarly

00:17:31,809 --> 00:17:36,639
we've got data which is exactly the same

00:17:34,809 --> 00:17:39,220
we've got now two parameters we've got

00:17:36,639 --> 00:17:42,820
mu which is our groups mean and we've

00:17:39,220 --> 00:17:45,730
got eater which is a vector of length n

00:17:42,820 --> 00:17:49,799
that's going to be the offset which is

00:17:45,730 --> 00:17:49,799
the players personal talent

00:17:51,800 --> 00:18:00,080
we've got a new thing transformed

00:17:54,710 --> 00:18:04,420
parameters theta is now defined this is

00:18:00,080 --> 00:18:07,430
just the logic function of peoples of

00:18:04,420 --> 00:18:09,500
the group mean and the individual mean

00:18:07,430 --> 00:18:13,490
so that's how we're doing the shrinkage

00:18:09,500 --> 00:18:15,800
estimator then in the model we set up

00:18:13,490 --> 00:18:18,940
priors again minerally informal

00:18:15,800 --> 00:18:24,350
informative priors we're only saying

00:18:18,940 --> 00:18:26,240
these numbers are reasonable this

00:18:24,350 --> 00:18:33,320
outside this search space were probably

00:18:26,240 --> 00:18:38,110
not going to do much and for each for

00:18:33,320 --> 00:18:38,110
each player we have their own model

00:18:41,480 --> 00:18:48,770
we run that we get a really nice

00:18:43,960 --> 00:18:51,770
posterior on mu and we get a posterior

00:18:48,770 --> 00:18:55,610
for each of the eaters and if we look at

00:18:51,770 --> 00:18:57,230
this we have kind of two groups one of

00:18:55,610 --> 00:19:00,160
them doesn't appear to converge very

00:18:57,230 --> 00:19:04,220
well you've got a big variance there

00:19:00,160 --> 00:19:06,770
that one those are the people with 0

00:19:04,220 --> 00:19:10,220
sixes it's really hard to say something

00:19:06,770 --> 00:19:14,270
about the quality of a player if you

00:19:10,220 --> 00:19:17,090
have no sexes players faced 100 balls

00:19:14,270 --> 00:19:19,010
got none are they an unlucky one in a

00:19:17,090 --> 00:19:21,559
hundred are they are one in 10,000 or

00:19:19,010 --> 00:19:26,440
they are one in never we don't know so

00:19:21,559 --> 00:19:26,440
that's why we've got that separation

00:19:27,700 --> 00:19:32,540
Stan's got a lot of good press a lot of

00:19:30,590 --> 00:19:37,400
statisticians are interested in in the

00:19:32,540 --> 00:19:39,610
moment why wouldn't you use it you're

00:19:37,400 --> 00:19:42,020
calling out something outside of Python

00:19:39,610 --> 00:19:44,030
you get a lot of info on the terminal

00:19:42,020 --> 00:19:46,000
but it's it's a bit of a black box

00:19:44,030 --> 00:19:48,309
you've got less control than you would

00:19:46,000 --> 00:19:54,350
in something that's pure of Python

00:19:48,309 --> 00:19:57,919
there's also testing and they do in

00:19:54,350 --> 00:19:59,510
their manuals there's a bit of a look we

00:19:57,919 --> 00:20:05,150
know we don't have a full testing

00:19:59,510 --> 00:20:08,799
framework but probably you want a bit

00:20:05,150 --> 00:20:08,799
more than that and they don't

00:20:12,519 --> 00:20:25,419
I am c-3po mc3 is a complete rewrite of

00:20:18,239 --> 00:20:27,849
Pi mc2 it's got a new new syntax which I

00:20:25,419 --> 00:20:32,700
really like I think his again quite

00:20:27,849 --> 00:20:35,499
beautiful I wasn't a fan of the pnc to

00:20:32,700 --> 00:20:38,049
syntax it's currently in beta but

00:20:35,499 --> 00:20:40,269
there's a lot of work going on on it it

00:20:38,049 --> 00:20:43,839
uses a no you term sampler and

00:20:40,269 --> 00:20:45,609
Hamiltonian Monte Carlo they developers

00:20:43,839 --> 00:20:52,450
have specifically said we were inspired

00:20:45,609 --> 00:20:54,609
by by the stand project and wanted to be

00:20:52,450 --> 00:20:58,809
able to use some of these newer fitters

00:20:54,609 --> 00:21:00,459
to in the pie MC and we found that we

00:20:58,809 --> 00:21:02,229
needed to do a full rewrite because it

00:21:00,459 --> 00:21:05,549
makes a lot of use of gradient methods

00:21:02,229 --> 00:21:09,339
that aren't really possible to retrofit

00:21:05,549 --> 00:21:11,499
it also does optimal starting positions

00:21:09,339 --> 00:21:13,329
so you get a really good you can ask it

00:21:11,499 --> 00:21:15,369
to give you a good guess of your

00:21:13,329 --> 00:21:21,659
starting position and that can cut down

00:21:15,369 --> 00:21:21,659
your time to convergence by a lot

00:21:24,070 --> 00:21:27,330
this is what it looks like

00:21:27,770 --> 00:21:36,190
you do everything in the context manager

00:21:32,290 --> 00:21:38,780
again we're defining theta with a prior

00:21:36,190 --> 00:21:41,030
we're using a better as we did in out

00:21:38,780 --> 00:21:42,740
this is just the pulled model we're not

00:21:41,030 --> 00:21:49,220
going to go into the hierarchical model

00:21:42,740 --> 00:21:53,180
for pi MC you define your likelihood is

00:21:49,220 --> 00:21:57,650
why this start find map that's their

00:21:53,180 --> 00:22:01,700
optimizer it performs quite well i'm

00:21:57,650 --> 00:22:05,240
using a metropolis optimizer here but

00:22:01,700 --> 00:22:07,280
you could also use ur no u-turns or they

00:22:05,240 --> 00:22:10,040
have a whole range of optimizers which

00:22:07,280 --> 00:22:11,810
is a good thing in Stan Stan uses really

00:22:10,040 --> 00:22:13,640
good optimizers but you've only got one

00:22:11,810 --> 00:22:16,760
of them so if it turns out that your

00:22:13,640 --> 00:22:20,930
problem doesn't perform well you're

00:22:16,760 --> 00:22:22,460
stuffed this one you can pull out a

00:22:20,930 --> 00:22:23,960
whole bunch and so there's probably

00:22:22,460 --> 00:22:29,750
something in there that will work well

00:22:23,960 --> 00:22:32,270
for your problem fit the model

00:22:29,750 --> 00:22:34,540
get the trace plots get all your data

00:22:32,270 --> 00:22:34,540
out

00:22:38,630 --> 00:22:44,620
excellent we've got good convergence

00:22:40,790 --> 00:22:44,620
we've got very similar results

00:22:46,720 --> 00:22:49,710
final thoughts

00:22:50,540 --> 00:22:56,910
this modeling is hard there's a lot of

00:22:52,950 --> 00:22:59,370
pitfalls do build your model with dummy

00:22:56,910 --> 00:23:02,160
data make sure that it gives you the

00:22:59,370 --> 00:23:05,070
right answers there then move on to

00:23:02,160 --> 00:23:08,910
starting to look at your problem what

00:23:05,070 --> 00:23:12,780
would I use personally at the moment I'd

00:23:08,910 --> 00:23:16,040
still go with Stan pie mc3 looks really

00:23:12,780 --> 00:23:20,340
good but it is in active development

00:23:16,040 --> 00:23:21,960
it's still in beta Stan's been around

00:23:20,340 --> 00:23:23,940
for a couple of years it's what I'd use

00:23:21,960 --> 00:23:26,250
for production in a year or two I'd

00:23:23,940 --> 00:23:30,140
probably be making a different call I'm

00:23:26,250 --> 00:23:41,750
really excited about where pmc is going

00:23:30,140 --> 00:23:41,750
thanks any questions

00:23:48,830 --> 00:23:54,200
thank you for this can you remind us

00:23:51,440 --> 00:23:57,140
again why you didn't do it in our is it

00:23:54,200 --> 00:24:01,429
because it spiked on basically yeah that

00:23:57,140 --> 00:24:04,610
seems um the when I started this project

00:24:01,429 --> 00:24:08,090
we these were things that we were

00:24:04,610 --> 00:24:10,159
looking at at work we ended up going in

00:24:08,090 --> 00:24:13,399
a different direction arm and we're

00:24:10,159 --> 00:24:16,370
using a spatial smoother on a markov

00:24:13,399 --> 00:24:19,360
random field and we're doing that in our

00:24:16,370 --> 00:24:19,360
so

00:24:27,490 --> 00:24:32,539
aa-are is a statistical programming

00:24:30,020 --> 00:24:34,760
language that it basically sits between

00:24:32,539 --> 00:24:38,000
a sort of standard programming language

00:24:34,760 --> 00:24:39,740
in a stats package so you can do a lot

00:24:38,000 --> 00:24:44,210
of the things that you can do with a

00:24:39,740 --> 00:24:48,320
statistical package but you can also do

00:24:44,210 --> 00:24:50,360
things like access network servers do

00:24:48,320 --> 00:24:52,610
web scraping so it's a much more

00:24:50,360 --> 00:24:54,289
full-featured than a sort of specific

00:24:52,610 --> 00:24:59,750
statistical package like you might have

00:24:54,289 --> 00:25:02,059
with Seder so working with some other

00:24:59,750 --> 00:25:04,280
things like random forests or neural

00:25:02,059 --> 00:25:06,260
networks a critical stage is essentially

00:25:04,280 --> 00:25:08,419
data pre-processing things like like

00:25:06,260 --> 00:25:12,710
normalization or handling missing data

00:25:08,419 --> 00:25:14,570
etc Bayesian models it potentially more

00:25:12,710 --> 00:25:17,870
robust to some of those issues or is

00:25:14,570 --> 00:25:20,510
that still a big deal it depends on the

00:25:17,870 --> 00:25:25,760
specific model but many Bayesian models

00:25:20,510 --> 00:25:28,640
are like so specific reason that you

00:25:25,760 --> 00:25:31,490
would use Bayesian modeling sometimes is

00:25:28,640 --> 00:25:35,150
because its robust to missing data now

00:25:31,490 --> 00:25:40,450
you need to get your models right to do

00:25:35,150 --> 00:25:42,370
that for scaling their normalization

00:25:40,450 --> 00:25:47,059
generally you won't need to normalize

00:25:42,370 --> 00:25:49,460
for model convergence but you might

00:25:47,059 --> 00:25:51,500
normalize for efficiency there are some

00:25:49,460 --> 00:25:54,049
things which will converge faster with

00:25:51,500 --> 00:25:57,700
normalized models but will still work

00:25:54,049 --> 00:25:57,700
with non-normalized

00:25:58,520 --> 00:26:02,960
hi just got to comment since that

00:26:01,220 --> 00:26:04,970
there's no questions at the moment just

00:26:02,960 --> 00:26:08,180
as sort of two books sort of talking

00:26:04,970 --> 00:26:10,160
about Bayesian stuff and Python that are

00:26:08,180 --> 00:26:12,110
quite good ones called think base and

00:26:10,160 --> 00:26:14,990
it's creative commons license as well as

00:26:12,110 --> 00:26:18,050
been viable by a guy called Allan Downey

00:26:14,990 --> 00:26:19,730
which is sort of much more kind of tacky

00:26:18,050 --> 00:26:21,680
through it from sort of first principles

00:26:19,730 --> 00:26:23,060
kind of a really basic one and then

00:26:21,680 --> 00:26:25,210
there's an ipass the notebook book

00:26:23,060 --> 00:26:27,710
called

00:26:25,210 --> 00:26:29,359
probabilistic programming invasion

00:26:27,710 --> 00:26:31,639
methods for hackers that's the ipython

00:26:29,359 --> 00:26:33,979
notebook and he's using pmc i'm not sure

00:26:31,639 --> 00:26:35,830
if it's version two or three verbs put

00:26:33,979 --> 00:26:39,429
that out there

00:26:35,830 --> 00:26:39,429
great thanks

00:26:41,070 --> 00:26:43,789
anymore

00:26:49,309 --> 00:26:53,580
curiosity how big are the data sets that

00:26:51,990 --> 00:26:57,179
you're running through this and do you

00:26:53,580 --> 00:26:58,830
find that do you find the limit it to

00:26:57,179 --> 00:27:00,899
you find limitations and differences

00:26:58,830 --> 00:27:04,559
within Python and are when you start

00:27:00,899 --> 00:27:06,179
hitting large numbers so the data sets

00:27:04,559 --> 00:27:11,789
I'm working with here are reasonably

00:27:06,179 --> 00:27:13,260
small I could get information on 424

00:27:11,789 --> 00:27:20,190
cricketers so that's what I've been

00:27:13,260 --> 00:27:22,590
working with ya there yet I'm sure you

00:27:20,190 --> 00:27:25,350
can find information on that I have

00:27:22,590 --> 00:27:29,880
friends who do very large bayesian data

00:27:25,350 --> 00:27:31,440
sets in both our and and Python so I

00:27:29,880 --> 00:27:33,779
know it's possible but I can't comment

00:27:31,440 --> 00:27:35,130
on efficiency have you ever because it

00:27:33,779 --> 00:27:36,870
was something that we hit in Mozilla is

00:27:35,130 --> 00:27:38,789
when we started to get to the really

00:27:36,870 --> 00:27:40,649
large numbers like hundreds of thousands

00:27:38,789 --> 00:27:44,190
of comments that we were trying to work

00:27:40,649 --> 00:27:46,049
through and somebody like one of our

00:27:44,190 --> 00:27:49,340
team really started looking into Java

00:27:46,049 --> 00:27:51,170
for that because we just started heating

00:27:49,340 --> 00:27:52,610
other issues maybe it was also our

00:27:51,170 --> 00:27:58,760
limitation of understanding the

00:27:52,610 --> 00:28:01,720
languages as well but yeah yeah I

00:27:58,760 --> 00:28:06,740
haven't looked at bayesian modeling in

00:28:01,720 --> 00:28:11,740
java although again stan is in c++ i

00:28:06,740 --> 00:28:15,980
would be tempted to to look at that as a

00:28:11,740 --> 00:28:18,310
as an option we probably have time for

00:28:15,980 --> 00:28:18,310
one more

00:28:28,860 --> 00:28:34,170
hi this I guess more of a general

00:28:31,920 --> 00:28:37,520
question like that one of the earlier

00:28:34,170 --> 00:28:39,620
comments about unit testing

00:28:37,520 --> 00:28:41,180
also have the same difficulty trying to

00:28:39,620 --> 00:28:45,110
convince people that it's something you

00:28:41,180 --> 00:28:48,500
really should do I'm just wondering what

00:28:45,110 --> 00:28:50,900
stuff you may be tried to convince

00:28:48,500 --> 00:28:52,519
people on your team or people you work

00:28:50,900 --> 00:28:54,700
with

00:28:52,519 --> 00:28:57,340
just stuff out or

00:28:54,700 --> 00:28:59,680
some ideas you've had because the thing

00:28:57,340 --> 00:29:02,340
that comes to my mind is particularly

00:28:59,680 --> 00:29:04,980
with the top early this morning it's

00:29:02,340 --> 00:29:06,870
often high-stakes data and it's just not

00:29:04,980 --> 00:29:09,409
really acceptable that there's you know

00:29:06,870 --> 00:29:11,789
looks like it works and then

00:29:09,409 --> 00:29:13,860
particularly in organizations like you

00:29:11,789 --> 00:29:16,970
work for it that that is going to a

00:29:13,860 --> 00:29:16,970
policy maker and

00:29:17,080 --> 00:29:22,930
so one of the things we've been doing is

00:29:20,400 --> 00:29:24,640
building some internal we've got we've

00:29:22,930 --> 00:29:25,990
got a mix of people who are more

00:29:24,640 --> 00:29:28,780
programmers and people who are more

00:29:25,990 --> 00:29:32,170
analysts and having the people who are

00:29:28,780 --> 00:29:34,210
more on the programmers side build some

00:29:32,170 --> 00:29:36,370
general test frameworks we have a fairly

00:29:34,210 --> 00:29:38,880
consistent things that our data looks

00:29:36,370 --> 00:29:41,170
like things about other base looks like

00:29:38,880 --> 00:29:45,240
basically to take some weight off so

00:29:41,170 --> 00:29:47,200
that we say look you don't need to

00:29:45,240 --> 00:29:48,670
understand learn about testing but if

00:29:47,200 --> 00:29:50,710
you don't want to you don't understand

00:29:48,670 --> 00:29:52,990
it we can just put this last line here

00:29:50,710 --> 00:29:55,810
you just need to put in these values and

00:29:52,990 --> 00:29:57,190
it will do its thing and come find us if

00:29:55,810 --> 00:29:59,500
it doesn't work so we're trying to take

00:29:57,190 --> 00:30:01,810
some of the load off the analysts to

00:29:59,500 --> 00:30:05,500
things they're not really interested in

00:30:01,810 --> 00:30:08,500
doing I guess is an alternative to

00:30:05,500 --> 00:30:14,010
trying to scale them up but it would be

00:30:08,500 --> 00:30:14,010
good to to have more ability to scale up

00:30:17,860 --> 00:30:20,880
I'm just wondering

00:30:21,220 --> 00:30:32,159
about a bias in this and how the lack of

00:30:26,559 --> 00:30:35,700
data influences bias of the results in

00:30:32,159 --> 00:30:40,289
comparison with random forest

00:30:35,700 --> 00:30:44,220
so Bayesian models can be biased they

00:30:40,289 --> 00:30:50,130
can be biased by their priors and but

00:30:44,220 --> 00:30:55,919
that goes down as as the data gets

00:30:50,130 --> 00:30:57,929
larger and 44 it's not big data but it's

00:30:55,919 --> 00:31:00,210
big enough to kind of be walking away

00:30:57,929 --> 00:31:01,769
from that to be walking back from that

00:31:00,210 --> 00:31:05,190
bias particularly when you're not

00:31:01,769 --> 00:31:08,460
putting strong priors on because you're

00:31:05,190 --> 00:31:09,899
explicitly defining a model you while

00:31:08,460 --> 00:31:12,539
you can still run into the issue of

00:31:09,899 --> 00:31:14,100
overfitting you've got a lot less risk

00:31:12,539 --> 00:31:17,399
of overfitting because you're not just

00:31:14,100 --> 00:31:19,049
saying do the best you can which will

00:31:17,399 --> 00:31:21,630
tend to overfit you're actually saying

00:31:19,049 --> 00:31:23,789
this is what it looks like make the

00:31:21,630 --> 00:31:26,570
model make the data fit that as best you

00:31:23,789 --> 00:31:29,669
can so you can control bias from that

00:31:26,570 --> 00:31:31,500
site as well but I think you'd need to

00:31:29,669 --> 00:31:33,750
be much more domain-specific if you

00:31:31,500 --> 00:31:38,279
actually wanted a which will give me

00:31:33,750 --> 00:31:40,970
less bias okay yeah the question

00:31:38,279 --> 00:31:43,220
probably

00:31:40,970 --> 00:31:45,110
for me probably stems from the fact that

00:31:43,220 --> 00:31:47,690
a lot of the stuff that you're doing or

00:31:45,110 --> 00:31:49,039
I mean just surmising a lot of stuff

00:31:47,690 --> 00:31:53,080
that you you're doing is for

00:31:49,039 --> 00:31:57,250
policymakers so therefore was it was it

00:31:53,080 --> 00:32:00,250
your team or was it policymakers who set

00:31:57,250 --> 00:32:00,250
the

00:32:01,789 --> 00:32:05,380
the

00:32:03,320 --> 00:32:09,360
sort the

00:32:05,380 --> 00:32:13,570
the sort functions that you actually use

00:32:09,360 --> 00:32:16,480
we tend to set them but but my team has

00:32:13,570 --> 00:32:17,800
a lot of static it had I'm a medical

00:32:16,480 --> 00:32:22,390
statistician we have other medical

00:32:17,800 --> 00:32:24,040
statisticians we are aware of the area

00:32:22,390 --> 00:32:27,730
and even if we don't know much about

00:32:24,040 --> 00:32:29,740
that particular cancer we are deeply

00:32:27,730 --> 00:32:32,260
involved in the ways and I mean we are

00:32:29,740 --> 00:32:34,390
we are intentionally biasing it if you

00:32:32,260 --> 00:32:36,400
think about a players the best measure

00:32:34,390 --> 00:32:39,310
the unbiased measure of a player's

00:32:36,400 --> 00:32:41,710
performance is there straight

00:32:39,310 --> 00:32:44,950
performance we are intentionally biasing

00:32:41,710 --> 00:32:47,530
but towards the mean because we believe

00:32:44,950 --> 00:32:49,420
that is a better number and again in the

00:32:47,530 --> 00:32:53,860
kind of state data we are intentionally

00:32:49,420 --> 00:32:56,170
biasing small groups or small arm areas

00:32:53,860 --> 00:33:01,660
towards the mean because we believe that

00:32:56,170 --> 00:33:06,420
is a arm is a better estimate than the

00:33:01,660 --> 00:33:06,420
raw estimate thank you

00:33:07,309 --> 00:33:14,779
right Thank You ribbon and as a memento

00:33:11,360 --> 00:33:24,679
of appreciation we have this thank you

00:33:14,779 --> 00:33:28,990
mark from pike on guys our lunch should

00:33:24,679 --> 00:33:28,990
be served so help yourselves

00:33:43,900 --> 00:33:45,960

YouTube URL: https://www.youtube.com/watch?v=N6kVJTKT6H8


