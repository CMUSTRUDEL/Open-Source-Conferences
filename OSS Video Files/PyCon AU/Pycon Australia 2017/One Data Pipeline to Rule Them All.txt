Title: One Data Pipeline to Rule Them All
Publication date: 2017-08-05
Playlist: Pycon Australia 2017
Description: 
	Sam Kitajima-Kimbrel

http://2017.pycon-au.org/schedule/presentation/35/

#pyconau

This talk was given at PyCon Australia 2017 which was held from 3-8 August, 2017 in Melbourne, Victoria.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2017, we're returning to Melbourne, bringing together students, enthusiasts, and professionals with a love of Python from around Australia, and from all over the World. 

August 3-8 2017, Melbourne, Victoria

Python, PyCon, PyConAU
Captions: 
	00:00:00,000 --> 00:00:04,770
to the third session of today it's one

00:00:02,939 --> 00:00:06,810
data pipeline to rule them all is it

00:00:04,770 --> 00:00:09,179
seeing on the screen the speaker is Sam

00:00:06,810 --> 00:00:10,980
katachi mocking Braille he's a software

00:00:09,179 --> 00:00:13,110
engineer with many opinions about

00:00:10,980 --> 00:00:15,900
distributed systems data routing in

00:00:13,110 --> 00:00:18,660
storage currently leading Tullius Data

00:00:15,900 --> 00:00:20,310
Platform team building scalable and

00:00:18,660 --> 00:00:23,939
reusable data infrastructure to support

00:00:20,310 --> 00:00:27,029
a 400 person R&D organization and

00:00:23,939 --> 00:00:28,470
obviously it's many users Sam has a

00:00:27,029 --> 00:00:30,240
different hair color every month

00:00:28,470 --> 00:00:33,660
definitely a different hair color every

00:00:30,240 --> 00:00:35,250
conference and he lives in the San

00:00:33,660 --> 00:00:37,290
Francisco Bay Area with his husband

00:00:35,250 --> 00:00:40,530
Cameron and the dogs basil and Maki

00:00:37,290 --> 00:00:50,789
please give a hand to keep that you Mac

00:00:40,530 --> 00:00:52,440
in Braille thank you great so yes this

00:00:50,789 --> 00:00:55,110
is a one data pipeline to rule them all

00:00:52,440 --> 00:00:56,430
thank you for coming we're going to

00:00:55,110 --> 00:00:59,460
start with a show of hands please put

00:00:56,430 --> 00:01:00,890
your hand up if you have data great for

00:00:59,460 --> 00:01:03,510
the video that's basically Intel room

00:01:00,890 --> 00:01:05,189
please keep your hand up if you have

00:01:03,510 --> 00:01:08,070
your data you know all the data sets you

00:01:05,189 --> 00:01:09,810
have how you access those data sets how

00:01:08,070 --> 00:01:11,100
you arrive how those data sets arrived

00:01:09,810 --> 00:01:12,900
at that place it that they're stored and

00:01:11,100 --> 00:01:16,770
how you combine them to drive new data

00:01:12,900 --> 00:01:17,790
or analysis and so on okay a few hands

00:01:16,770 --> 00:01:20,490
so some people have thought about this a

00:01:17,790 --> 00:01:24,060
little bit but definitely less than

00:01:20,490 --> 00:01:26,400
everybody who has data so hi I'm Sam I

00:01:24,060 --> 00:01:28,439
have data too I lead the Data Platform

00:01:26,400 --> 00:01:30,600
team at Toyo where we build and maintain

00:01:28,439 --> 00:01:32,880
a unified system for storing retrieving

00:01:30,600 --> 00:01:36,090
and doing computation and all the data

00:01:32,880 --> 00:01:37,590
generated by all of Toledo's products so

00:01:36,090 --> 00:01:40,079
I'm going to open this with it slightly

00:01:37,590 --> 00:01:43,170
abstracted but hopefully familiar to

00:01:40,079 --> 00:01:45,210
some of you story this is a relational

00:01:43,170 --> 00:01:47,070
database let's say it belongs to team a

00:01:45,210 --> 00:01:49,350
and they set it up to store some data

00:01:47,070 --> 00:01:52,259
generated by their application and it

00:01:49,350 --> 00:01:54,509
works really well until the end of the

00:01:52,259 --> 00:01:56,219
quarter comes and the PM wants to run

00:01:54,509 --> 00:01:57,799
some queries to see how much their

00:01:56,219 --> 00:02:00,840
products usage grew over last quarter

00:01:57,799 --> 00:02:02,640
since this was built as an OLTP database

00:02:00,840 --> 00:02:04,530
for single row transaction processing

00:02:02,640 --> 00:02:05,909
with low latency these giant queries

00:02:04,530 --> 00:02:08,640
that are summing up you know millions

00:02:05,909 --> 00:02:10,229
and billions of rows are slow and the PM

00:02:08,640 --> 00:02:11,400
says hey could we put this data

00:02:10,229 --> 00:02:12,930
somewhere that's going to make this

00:02:11,400 --> 00:02:13,930
analysis faster for me so I can actually

00:02:12,930 --> 00:02:15,790
get my reports done

00:02:13,930 --> 00:02:17,530
and maybe not knock over before actually

00:02:15,790 --> 00:02:20,950
production infrastructure at the same

00:02:17,530 --> 00:02:22,750
time so now we have a data warehouse and

00:02:20,950 --> 00:02:25,209
it's copying data out of the application

00:02:22,750 --> 00:02:27,310
database it works pretty well right

00:02:25,209 --> 00:02:29,349
we picked the redshift or something

00:02:27,310 --> 00:02:31,060
similar it's a column store it's really

00:02:29,349 --> 00:02:33,040
good at running those giant roll-ups and

00:02:31,060 --> 00:02:36,189
it's easy enough to copy the data in

00:02:33,040 --> 00:02:38,140
with a cron job and a Python ETL was

00:02:36,189 --> 00:02:39,970
just extract transform load the pattern

00:02:38,140 --> 00:02:42,900
for how you do this a script that

00:02:39,970 --> 00:02:45,579
somebody knocked out over an afternoon

00:02:42,900 --> 00:02:47,440
so then team B comes along and says oh

00:02:45,579 --> 00:02:48,819
hey you have a native warehouse this

00:02:47,440 --> 00:02:52,150
looks really cool can we put some stuff

00:02:48,819 --> 00:02:53,409
in here and so they or are you the newly

00:02:52,150 --> 00:02:56,950
appointed custodian of the data

00:02:53,409 --> 00:02:58,569
warehouse welcome to your new job write

00:02:56,950 --> 00:03:00,099
another cron job to run a slightly

00:02:58,569 --> 00:03:02,280
different Python scripts point at a

00:03:00,099 --> 00:03:03,819
different database and copy the data

00:03:02,280 --> 00:03:06,700
okay cool

00:03:03,819 --> 00:03:07,989
so then somebody on your team decides to

00:03:06,700 --> 00:03:09,459
change a piece of the schema because

00:03:07,989 --> 00:03:11,019
they need to support a new feature but

00:03:09,459 --> 00:03:12,730
everybody forgets about the data

00:03:11,019 --> 00:03:14,379
warehouse until a week later when

00:03:12,730 --> 00:03:16,319
somebody on the accounting team and by

00:03:14,379 --> 00:03:18,699
the way when did you start using this

00:03:16,319 --> 00:03:20,560
says hey why isn't the reporting

00:03:18,699 --> 00:03:24,120
warehouse updating well crap

00:03:20,560 --> 00:03:26,440
so somebody updates it and life goes on

00:03:24,120 --> 00:03:27,940
then team C comes along and says hey we

00:03:26,440 --> 00:03:29,019
put this Cassandra cluster that we want

00:03:27,940 --> 00:03:32,590
to dump into the warehouse could you

00:03:29,019 --> 00:03:34,120
help us and somebody else says we want

00:03:32,590 --> 00:03:35,470
to do real-time spam detection on our

00:03:34,120 --> 00:03:37,030
data but we want to run Knightly

00:03:35,470 --> 00:03:39,150
training jobs for the model over all of

00:03:37,030 --> 00:03:39,150
history

00:03:39,180 --> 00:03:44,500
give me a full-text search and so just

00:03:42,489 --> 00:03:46,239
like this our data environment is not so

00:03:44,500 --> 00:03:48,639
simple anymore right we had this simple

00:03:46,239 --> 00:03:51,400
ETL photo that worked now we have a lot

00:03:48,639 --> 00:03:52,569
more pieces we put a bunch of cron jobs

00:03:51,400 --> 00:03:53,650
and all right how is something that

00:03:52,569 --> 00:03:55,989
somebody rigged up to do stream

00:03:53,650 --> 00:03:57,459
processing and a running flat out just

00:03:55,989 --> 00:04:00,909
to keep up with schema evolutions and

00:03:57,459 --> 00:04:02,620
changes to the data model and then

00:04:00,909 --> 00:04:05,620
finally the accounting team comes back

00:04:02,620 --> 00:04:08,229
and they say why don't these two systems

00:04:05,620 --> 00:04:09,819
agree on how many API calls or page news

00:04:08,229 --> 00:04:11,859
or whatever it is that we sell any way

00:04:09,819 --> 00:04:14,139
that we did last month because this is

00:04:11,859 --> 00:04:19,169
breaking our quarter clothes for the

00:04:14,139 --> 00:04:21,579
reports we can't state our earnings Wow

00:04:19,169 --> 00:04:23,650
so yeah we're the screen we've crashed

00:04:21,579 --> 00:04:26,050
out the side of the building we're not

00:04:23,650 --> 00:04:27,880
in a very good State so this obvious a

00:04:26,050 --> 00:04:29,020
very abstracted story but

00:04:27,880 --> 00:04:30,580
pretty much all of these situations

00:04:29,020 --> 00:04:33,190
would happen - over the last seven years

00:04:30,580 --> 00:04:34,990
at Tulio and they're probably familiar

00:04:33,190 --> 00:04:38,310
to a lot of other people other companies

00:04:34,990 --> 00:04:41,440
as well so what happened what went wrong

00:04:38,310 --> 00:04:43,420
we had everything working but it costs

00:04:41,440 --> 00:04:46,750
us a lot in developer time and machine

00:04:43,420 --> 00:04:48,340
time so at tokyo we took a look back at

00:04:46,750 --> 00:04:50,470
all of our legacy data infrastructure

00:04:48,340 --> 00:04:51,670
and we found a couple of high-level

00:04:50,470 --> 00:04:54,730
problems with it and so when I go

00:04:51,670 --> 00:04:56,530
through those first so first we had

00:04:54,730 --> 00:04:57,970
architectural problems including there

00:04:56,530 --> 00:05:00,970
are multiple sources of data and

00:04:57,970 --> 00:05:03,010
multiple destinations for it right there

00:05:00,970 --> 00:05:05,560
was some code reuse and each new type of

00:05:03,010 --> 00:05:07,870
system but each new type of system

00:05:05,560 --> 00:05:09,370
required major work to add and even new

00:05:07,870 --> 00:05:11,860
things from systems we knew about had to

00:05:09,370 --> 00:05:13,030
be hand configured this wasn't quite the

00:05:11,860 --> 00:05:15,130
worst case right there was some reuse

00:05:13,030 --> 00:05:16,390
but uh the Grossman is clear if you have

00:05:15,130 --> 00:05:18,130
a graph within nodes in it the

00:05:16,390 --> 00:05:21,460
connections between them grows as N

00:05:18,130 --> 00:05:22,990
squared this is compounded by the fact

00:05:21,460 --> 00:05:26,140
that there is no single source of truth

00:05:22,990 --> 00:05:28,930
for schemas so changing data types or

00:05:26,140 --> 00:05:30,640
adding columns was risky and requires

00:05:28,930 --> 00:05:36,430
manual work to verify the things are

00:05:30,640 --> 00:05:37,720
working correctly and finally there was

00:05:36,430 --> 00:05:39,610
no way to guarantee that all the data

00:05:37,720 --> 00:05:41,680
was correct in all the places that was

00:05:39,610 --> 00:05:43,750
supposed to appear right so network

00:05:41,680 --> 00:05:45,580
failures host failures even programming

00:05:43,750 --> 00:05:48,760
errors can manifest as data being wrong

00:05:45,580 --> 00:05:50,260
in one place or another so if that's

00:05:48,760 --> 00:05:53,200
what was wrong with our Train and why we

00:05:50,260 --> 00:05:54,490
crashed at the side of the station what

00:05:53,200 --> 00:05:56,140
does her replacement architecture want

00:05:54,490 --> 00:05:59,680
to look like great already want to get

00:05:56,140 --> 00:06:01,870
to so we took a look at what the Giants

00:05:59,680 --> 00:06:03,250
of the industry so LinkedIn Netflix what

00:06:01,870 --> 00:06:05,620
they were doing we looked at their

00:06:03,250 --> 00:06:07,420
capabilities as a development team and

00:06:05,620 --> 00:06:09,460
as a company and came up with some

00:06:07,420 --> 00:06:11,080
guidelines to optimize our data

00:06:09,460 --> 00:06:16,420
architecture for developer productivity

00:06:11,080 --> 00:06:18,010
for scalability and for correctness so

00:06:16,420 --> 00:06:20,380
first we decided that there should be

00:06:18,010 --> 00:06:22,690
one and only one way to publish data

00:06:20,380 --> 00:06:25,390
into the new platform from applications

00:06:22,690 --> 00:06:27,010
a lot of our early headaches were

00:06:25,390 --> 00:06:29,260
brought on by inconsistencies in how I

00:06:27,010 --> 00:06:31,390
move data between systems and having to

00:06:29,260 --> 00:06:32,860
duplicate effort across these systems

00:06:31,390 --> 00:06:34,660
when making changes so we wanted to get

00:06:32,860 --> 00:06:36,220
rid of that we wanted to have a single

00:06:34,660 --> 00:06:38,229
system a single pathway to be the

00:06:36,220 --> 00:06:39,970
canonical pipeline for records to flow

00:06:38,229 --> 00:06:42,359
through regardless of how many places

00:06:39,970 --> 00:06:44,979
the idea stored in the end

00:06:42,359 --> 00:06:46,329
on the flip side given that we need to

00:06:44,979 --> 00:06:48,039
put the same data in multiple storage

00:06:46,329 --> 00:06:50,559
systems to support all the ways we want

00:06:48,039 --> 00:06:51,699
to use it we can't dictate a single

00:06:50,559 --> 00:06:53,979
method of getting data into those

00:06:51,699 --> 00:06:55,779
systems from our pipeline but we did

00:06:53,979 --> 00:06:57,399
feel it was important to provide common

00:06:55,779 --> 00:07:01,209
libraries and tooling to share as much

00:06:57,399 --> 00:07:03,429
of that logic as possible and we wanted

00:07:01,209 --> 00:07:05,019
schemas I'm going to go into detail on

00:07:03,429 --> 00:07:06,429
this later but for now I'll just say

00:07:05,019 --> 00:07:08,379
that's a lot easier to ensure that all

00:07:06,429 --> 00:07:10,359
your systems work together nicely when

00:07:08,379 --> 00:07:12,309
there's one single authority on what

00:07:10,359 --> 00:07:16,689
constitutes a valid record for a given

00:07:12,309 --> 00:07:18,279
type of data and finally we wanted a way

00:07:16,689 --> 00:07:20,139
to make sure that every system that

00:07:18,279 --> 00:07:22,749
purported can to contain the given set

00:07:20,139 --> 00:07:24,129
of data had all the data right if a

00:07:22,749 --> 00:07:25,269
record is present and correct in one

00:07:24,129 --> 00:07:29,349
place it should be the same everywhere

00:07:25,269 --> 00:07:31,359
else and so since the title of the talk

00:07:29,349 --> 00:07:32,619
includes the words data pipeline you

00:07:31,359 --> 00:07:35,349
might have guessed that we picked an

00:07:32,619 --> 00:07:36,519
event pipeline architecture so from a

00:07:35,349 --> 00:07:37,119
thousand meters up it looks something

00:07:36,519 --> 00:07:39,129
like this

00:07:37,119 --> 00:07:41,559
this isn't it the classic event sourcing

00:07:39,129 --> 00:07:44,199
architecture you model all your data as

00:07:41,559 --> 00:07:45,699
an ordered series of events in time so

00:07:44,199 --> 00:07:47,769
if we're log structured data things like

00:07:45,699 --> 00:07:50,229
web requests add clicks and so on this

00:07:47,769 --> 00:07:51,879
is very natural for data where records

00:07:50,229 --> 00:07:54,459
change over time so configurations

00:07:51,879 --> 00:07:55,629
anything that changes states you have to

00:07:54,459 --> 00:07:56,979
kind of think about a little differently

00:07:55,629 --> 00:07:58,929
you need to view your records as a

00:07:56,979 --> 00:08:01,269
sequence of deltas or changes to the

00:07:58,929 --> 00:08:02,799
data in order from which if you play

00:08:01,269 --> 00:08:04,569
them back into the same place then

00:08:02,799 --> 00:08:05,969
reconstruct that you can reconstruct the

00:08:04,569 --> 00:08:09,309
state of the record at any time

00:08:05,969 --> 00:08:11,019
but they both are suited to this kind of

00:08:09,309 --> 00:08:12,489
architecture so we have some sorts of

00:08:11,019 --> 00:08:14,679
events right let's call it a web server

00:08:12,489 --> 00:08:16,359
and say we're handling requests and we

00:08:14,679 --> 00:08:19,599
generate a log of into that every time

00:08:16,359 --> 00:08:21,609
somebody views a page that server emits

00:08:19,599 --> 00:08:22,989
a record onto a system data bus a

00:08:21,609 --> 00:08:25,449
pipeline what if you want to call this

00:08:22,989 --> 00:08:27,189
that stores all the events in the order

00:08:25,449 --> 00:08:29,549
they were received as a first in first

00:08:27,189 --> 00:08:32,019
out queue this is important here and

00:08:29,549 --> 00:08:34,000
every system that's interested in that

00:08:32,019 --> 00:08:35,529
type of event subscribes that stream

00:08:34,000 --> 00:08:37,419
that single authoritative stream of data

00:08:35,529 --> 00:08:39,490
and consumes the events in the same

00:08:37,419 --> 00:08:40,599
order they were produced so important to

00:08:39,490 --> 00:08:43,659
note that these don't have to be the

00:08:40,599 --> 00:08:45,519
same type of data or a system sorry this

00:08:43,659 --> 00:08:47,019
could be a relational database suited

00:08:45,519 --> 00:08:49,689
for transaction processing and on the

00:08:47,019 --> 00:08:51,100
top we could have a column store for

00:08:49,689 --> 00:08:52,629
doing analytics on the bottom we could

00:08:51,100 --> 00:08:55,450
have you know some archive all to s3 or

00:08:52,629 --> 00:08:56,890
something in the middle tier right

00:08:55,450 --> 00:08:58,030
but they all consume they did it the

00:08:56,890 --> 00:09:01,750
same way even if they do different

00:08:58,030 --> 00:09:03,250
things at the end and just just as

00:09:01,750 --> 00:09:05,380
similarly we can have multiple systems

00:09:03,250 --> 00:09:07,120
at the front of the queue sitting

00:09:05,380 --> 00:09:10,090
multiple deceit types of events on

00:09:07,120 --> 00:09:11,800
different topics or streams and set up

00:09:10,090 --> 00:09:12,730
each consuming system to only consume

00:09:11,800 --> 00:09:20,620
the event streams that they're

00:09:12,730 --> 00:09:24,010
interested in so Twilio Apache Kafka is

00:09:20,620 --> 00:09:25,810
the backbone and circuit texture Kafka

00:09:24,010 --> 00:09:27,700
is a horizontally scalable fault

00:09:25,810 --> 00:09:29,800
tolerant very high throughput and many

00:09:27,700 --> 00:09:32,230
other buzzwords as well platform for

00:09:29,800 --> 00:09:33,850
streaming message delivery now if you're

00:09:32,230 --> 00:09:36,130
not familiar with it it originated at

00:09:33,850 --> 00:09:37,720
LinkedIn and LinkedIn publishes

00:09:36,130 --> 00:09:40,600
published a blog post saying that they

00:09:37,720 --> 00:09:43,240
currently use Kafka to move over 1

00:09:40,600 --> 00:09:45,370
trillion with a T events per day and

00:09:43,240 --> 00:09:47,440
it's become wildly popular since they

00:09:45,370 --> 00:09:48,730
went open-source it's very actively used

00:09:47,440 --> 00:09:51,340
very actively developed and maintained

00:09:48,730 --> 00:09:53,020
and people ranging from Netflix Goldman

00:09:51,340 --> 00:09:57,420
Sachs and since I'm up here talking

00:09:53,020 --> 00:09:59,470
about it Twilio use it so Kafka

00:09:57,420 --> 00:10:01,510
fundamentally Kafka deployment consists

00:09:59,470 --> 00:10:03,190
of a set of broker nodes and brokers

00:10:01,510 --> 00:10:04,600
host topics which are those streams of

00:10:03,190 --> 00:10:07,180
ordered events that I was talking about

00:10:04,600 --> 00:10:09,910
they take rights the topics from

00:10:07,180 --> 00:10:12,250
producers and persist this produced

00:10:09,910 --> 00:10:15,550
events to disk and replicate amongst

00:10:12,250 --> 00:10:17,230
themselves to make a durable another

00:10:15,550 --> 00:10:20,080
sighting of consumer processes that

00:10:17,230 --> 00:10:22,450
connect the broker nodes and the brokers

00:10:20,080 --> 00:10:23,860
deliver to each consumer to a topic all

00:10:22,450 --> 00:10:26,440
of the events in the same order they are

00:10:23,860 --> 00:10:28,600
produced and as they process events

00:10:26,440 --> 00:10:30,430
consumers send back acknowledgments of

00:10:28,600 --> 00:10:32,950
the latest event they received back to

00:10:30,430 --> 00:10:35,230
the broker so that if they were a start

00:10:32,950 --> 00:10:36,610
or a fail or it replaced the consumer

00:10:35,230 --> 00:10:38,470
can start from the latest offsets that

00:10:36,610 --> 00:10:40,120
it acknowledged so in this way Kafka

00:10:38,470 --> 00:10:41,980
guarantees that at least once delivery

00:10:40,120 --> 00:10:43,930
happens for each message that was sent

00:10:41,980 --> 00:10:45,070
to a topic to all consumers right so

00:10:43,930 --> 00:10:47,740
anything that comes in the front is

00:10:45,070 --> 00:10:49,750
guaranteed to land at least once on each

00:10:47,740 --> 00:10:52,450
individual consumer of a topic assuming

00:10:49,750 --> 00:10:53,980
you write everything correctly confident

00:10:52,450 --> 00:10:55,780
is fairly straightforward to operate

00:10:53,980 --> 00:10:57,730
make sure to read the manual but if

00:10:55,780 --> 00:10:59,140
you're into managed services Heroku has

00:10:57,730 --> 00:11:01,930
a Kafka add-on you can add your

00:10:59,140 --> 00:11:04,180
deployments and Amazon's Kinesis service

00:11:01,930 --> 00:11:06,660
exposes similar api's and contracts to a

00:11:04,180 --> 00:11:06,660
Kafka does

00:11:07,700 --> 00:11:11,450
so to recap what we've done so far right

00:11:09,350 --> 00:11:14,840
if we use Kafka as the core of this

00:11:11,450 --> 00:11:16,580
durable guaranteed delivery eventbus we

00:11:14,840 --> 00:11:18,590
can treat every data set as a separate

00:11:16,580 --> 00:11:20,750
topic of events that are produced by the

00:11:18,590 --> 00:11:22,940
application systems to generate them and

00:11:20,750 --> 00:11:24,890
then we can connect consumers as we need

00:11:22,940 --> 00:11:30,080
for each data storage system or

00:11:24,890 --> 00:11:31,730
processing application so so far this is

00:11:30,080 --> 00:11:33,230
pretty standard stuff you can find a

00:11:31,730 --> 00:11:35,300
couple dozen blog posts on this from

00:11:33,230 --> 00:11:38,210
linkedin netflix confluent and anybody

00:11:35,300 --> 00:11:39,530
else uses Kafka and from now on I want

00:11:38,210 --> 00:11:41,330
to cover kind of thrust this time we

00:11:39,530 --> 00:11:42,950
have things we learned and systems we

00:11:41,330 --> 00:11:48,080
built that are less obvious from the

00:11:42,950 --> 00:11:49,160
blog posts so first up schemas are

00:11:48,080 --> 00:11:50,540
important this is a really strong

00:11:49,160 --> 00:11:51,620
opinion so here goes you should

00:11:50,540 --> 00:11:53,480
absolutely

00:11:51,620 --> 00:11:55,460
by all means definitely without question

00:11:53,480 --> 00:11:57,890
use a strongly typed serialization

00:11:55,460 --> 00:11:59,840
format with predefined schemas and

00:11:57,890 --> 00:12:02,240
validation libraries on both sides of

00:11:59,840 --> 00:12:03,200
your rent bus so there are a lot of

00:12:02,240 --> 00:12:05,630
systems out there

00:12:03,200 --> 00:12:07,220
MongoDB to name one that just let you

00:12:05,630 --> 00:12:09,500
chuck arbitrary documents at them right

00:12:07,220 --> 00:12:10,850
um here's a blob of JSON please store it

00:12:09,500 --> 00:12:13,370
and then let me get it back some at some

00:12:10,850 --> 00:12:14,420
time later this is tempting the

00:12:13,370 --> 00:12:15,020
slightest might compare price I'm just

00:12:14,420 --> 00:12:17,270
gonna talk for a while

00:12:15,020 --> 00:12:18,410
this is tempting you can get started

00:12:17,270 --> 00:12:21,140
really quickly with things like or

00:12:18,410 --> 00:12:23,150
elastic but there's a cost to this your

00:12:21,140 --> 00:12:25,400
schemas and the enforcement of your

00:12:23,150 --> 00:12:27,650
schemas in those systems move from a

00:12:25,400 --> 00:12:30,020
single place the database to many

00:12:27,650 --> 00:12:31,250
scattered places in your app code it

00:12:30,020 --> 00:12:32,900
opens the possibility that you're going

00:12:31,250 --> 00:12:34,370
to send or store data that you can't

00:12:32,900 --> 00:12:36,710
process later when you change your code

00:12:34,370 --> 00:12:37,880
or in other systems that have to know

00:12:36,710 --> 00:12:39,350
about that thing if there's a different

00:12:37,880 --> 00:12:41,750
implementation that it shares a common

00:12:39,350 --> 00:12:43,580
schema if that schema isn't defined

00:12:41,750 --> 00:12:46,880
anywhere except in the code you can get

00:12:43,580 --> 00:12:48,380
out of sync so to kind of have an

00:12:46,880 --> 00:12:50,420
example right if we have an application

00:12:48,380 --> 00:12:51,860
that's writing data somewhere and a

00:12:50,420 --> 00:12:53,150
batch job we're in a different language

00:12:51,860 --> 00:12:54,770
that picks it up and does something with

00:12:53,150 --> 00:12:57,200
it let's say it generates a report for

00:12:54,770 --> 00:12:59,270
the sales team if somebody on the team

00:12:57,200 --> 00:13:01,070
that generates the data drops a field

00:12:59,270 --> 00:13:03,110
from the data in the application without

00:13:01,070 --> 00:13:04,670
telling the batch jobs maintainer rate

00:13:03,110 --> 00:13:06,980
and the batch depends on that field well

00:13:04,670 --> 00:13:08,540
explodes somebody has to go pick up the

00:13:06,980 --> 00:13:11,180
pieces fix the code and rerun the batch

00:13:08,540 --> 00:13:13,430
and that's if you're lucky if you're not

00:13:11,180 --> 00:13:15,290
lucky it exposed silently and just

00:13:13,430 --> 00:13:17,990
leaves you with incorrect missing stale

00:13:15,290 --> 00:13:19,340
or corrupt data and like I said earlier

00:13:17,990 --> 00:13:20,540
let's really hope that this data isn't

00:13:19,340 --> 00:13:24,590
less driving your

00:13:20,540 --> 00:13:26,450
reports so yeah please use schemas there

00:13:24,590 --> 00:13:27,890
are a lot of open-source libraries for

00:13:26,450 --> 00:13:30,020
schema validation and serialization

00:13:27,890 --> 00:13:32,750
Evaro protocol buffers thrift message

00:13:30,020 --> 00:13:36,110
pack and many others as well please pick

00:13:32,750 --> 00:13:39,160
one and only one and use it everywhere

00:13:36,110 --> 00:13:39,160
the two trains my data between systems

00:13:39,700 --> 00:13:43,430
so at obeah we ended up choosing Avro

00:13:41,990 --> 00:13:45,260
for a number of reasons

00:13:43,430 --> 00:13:47,480
it has official cross-platform support

00:13:45,260 --> 00:13:49,180
for Java and Python plus community

00:13:47,480 --> 00:13:51,470
libraries for just about everything else

00:13:49,180 --> 00:13:52,970
the Java library gives you both dynamic

00:13:51,470 --> 00:13:54,920
and Static types so you can either just

00:13:52,970 --> 00:13:57,920
get back a Java map or a code generated

00:13:54,920 --> 00:13:59,540
POJO which makes it nice for both

00:13:57,920 --> 00:14:01,160
whether you're going to deploy sins they

00:13:59,540 --> 00:14:02,480
only deal with one type of data or make

00:14:01,160 --> 00:14:03,920
a nice dynamic system they can be

00:14:02,480 --> 00:14:07,220
reconfigured and I'll get back to this

00:14:03,920 --> 00:14:09,800
later and additionally when you

00:14:07,220 --> 00:14:11,630
deserialize things with a fro as long as

00:14:09,800 --> 00:14:13,400
the versions of the same schema are

00:14:11,630 --> 00:14:15,020
compatible so like you're adding a new

00:14:13,400 --> 00:14:16,730
field that has a default value or drop

00:14:15,020 --> 00:14:17,960
something that has a default value ever

00:14:16,730 --> 00:14:20,420
will automatically convert between

00:14:17,960 --> 00:14:22,430
versions meaning that you can produce

00:14:20,420 --> 00:14:23,960
new versions of data into a system and

00:14:22,430 --> 00:14:25,790
only upgrade your consumer when you need

00:14:23,960 --> 00:14:27,590
to and finally it has this nice

00:14:25,790 --> 00:14:29,780
compacted by a binary serialization

00:14:27,590 --> 00:14:33,500
format that doesn't tag the schema on to

00:14:29,780 --> 00:14:35,860
every records you save some bytes more

00:14:33,500 --> 00:14:38,840
on schemas I'm still not done here

00:14:35,860 --> 00:14:41,150
enforce your schemas at produced time so

00:14:38,840 --> 00:14:43,130
don't send anything on to your Kafka

00:14:41,150 --> 00:14:45,980
topics or into your event bus it is not

00:14:43,130 --> 00:14:46,880
valid without exception so if you do

00:14:45,980 --> 00:14:49,430
this you could has a really strong

00:14:46,880 --> 00:14:50,960
guarantee every record that comes down a

00:14:49,430 --> 00:14:53,450
topic is going to be valid for the

00:14:50,960 --> 00:14:55,270
consumers to deserialize it keeps the

00:14:53,450 --> 00:14:57,500
onus of the data validation on producers

00:14:55,270 --> 00:14:59,480
where it's easier to test an update

00:14:57,500 --> 00:15:01,580
because most data types are only going

00:14:59,480 --> 00:15:03,080
to be produced by one or a small number

00:15:01,580 --> 00:15:06,290
of code bases but might ultimately be

00:15:03,080 --> 00:15:08,210
consumed by many more within a single

00:15:06,290 --> 00:15:09,740
topic only make backwards-compatible

00:15:08,210 --> 00:15:12,560
changes to the schema so you get that

00:15:09,740 --> 00:15:14,570
nice transparent across grading if you

00:15:12,560 --> 00:15:16,460
have to rate compatibility start a new

00:15:14,570 --> 00:15:18,400
topic on your bus and just migrate your

00:15:16,460 --> 00:15:20,660
consumers after they finish the olan off

00:15:18,400 --> 00:15:22,220
so if you use these rules and follow

00:15:20,660 --> 00:15:24,380
them and you get this guarantee that the

00:15:22,220 --> 00:15:26,270
worst that can happen when a change gets

00:15:24,380 --> 00:15:27,650
missed by a consuming application is a

00:15:26,270 --> 00:15:32,090
delay and availability right you can

00:15:27,650 --> 00:15:34,400
always go back and reprocess so

00:15:32,090 --> 00:15:35,720
we're gonna use schemas now we have to

00:15:34,400 --> 00:15:39,320
know about them right so we need to know

00:15:35,720 --> 00:15:40,970
what schema is in a given topic so given

00:15:39,320 --> 00:15:42,530
the Kafka topic name we want to find out

00:15:40,970 --> 00:15:43,850
what schema applies to the records in it

00:15:42,530 --> 00:15:47,690
and what versions of that schema are

00:15:43,850 --> 00:15:49,790
available and because the supergate

00:15:47,690 --> 00:15:50,870
systems are hard as andrew Godwin's

00:15:49,790 --> 00:15:53,180
talked pointed out yesterday if you were

00:15:50,870 --> 00:15:54,770
here for that in the face of network

00:15:53,180 --> 00:15:56,240
partitions and host failures we can't be

00:15:54,770 --> 00:15:59,150
guarantee they were going to produce or

00:15:56,240 --> 00:16:00,410
consumer record exactly once and what's

00:15:59,150 --> 00:16:03,140
more even putting them in order is

00:16:00,410 --> 00:16:04,400
tricky because clocks are hard Kafka

00:16:03,140 --> 00:16:06,020
only knows about the order in which

00:16:04,400 --> 00:16:07,880
records arrive at the broker so if you

00:16:06,020 --> 00:16:09,200
have multiple hosts producing to the

00:16:07,880 --> 00:16:10,790
same topic you're going to have a sort

00:16:09,200 --> 00:16:13,550
they're ordering out a consume time on

00:16:10,790 --> 00:16:15,620
the other side so to do that we need to

00:16:13,550 --> 00:16:17,180
know a couple things we need to know

00:16:15,620 --> 00:16:19,550
which fields in the record constitute a

00:16:17,180 --> 00:16:21,530
primary or unique key and we need to

00:16:19,550 --> 00:16:22,970
know what logic we want to use to choose

00:16:21,530 --> 00:16:25,690
a winner or merge conflicting versions

00:16:22,970 --> 00:16:28,490
and probably how to order things two

00:16:25,690 --> 00:16:30,230
quick tip here on the side wall clocks

00:16:28,490 --> 00:16:32,390
are not to be trusted across multiple

00:16:30,230 --> 00:16:33,620
hosts because clock Direct happens so

00:16:32,390 --> 00:16:35,480
don't just use the latest of all time

00:16:33,620 --> 00:16:37,160
wins there are things called vector

00:16:35,480 --> 00:16:39,650
clocks which actually track the time

00:16:37,160 --> 00:16:41,000
across all the hosts that produce but

00:16:39,650 --> 00:16:42,590
you do have to have some source of

00:16:41,000 --> 00:16:45,250
serialization I'm walking to do

00:16:42,590 --> 00:16:48,200
concurrent operations in the same object

00:16:45,250 --> 00:16:49,910
and finally we might want to know how to

00:16:48,200 --> 00:16:50,390
verify that data sets are complete and

00:16:49,910 --> 00:16:52,970
correct

00:16:50,390 --> 00:16:54,890
so since he already said that in order

00:16:52,970 --> 00:16:56,990
to do the application and understand and

00:16:54,890 --> 00:16:58,640
correctly handle data coming through we

00:16:56,990 --> 00:17:00,290
need a unique key we can reuse that

00:16:58,640 --> 00:17:02,630
right so first pass written

00:17:00,290 --> 00:17:04,370
reconciliation between different systems

00:17:02,630 --> 00:17:06,800
let's just do a count distinct write

00:17:04,370 --> 00:17:07,700
count all the keys we've seen if there

00:17:06,800 --> 00:17:09,110
are additional checks you want to

00:17:07,700 --> 00:17:10,760
perform right like like summing up the

00:17:09,110 --> 00:17:12,500
amount of all your transactions or

00:17:10,760 --> 00:17:14,270
something you could you want to know

00:17:12,500 --> 00:17:15,470
that aggregate operations to apply and

00:17:14,270 --> 00:17:23,450
which fields you're going to run them

00:17:15,470 --> 00:17:25,160
over so so far we've gone through what

00:17:23,450 --> 00:17:27,050
we started off where that Tullio what we

00:17:25,160 --> 00:17:28,310
disliked about the original systems and

00:17:27,050 --> 00:17:30,290
what we wanted out of our new

00:17:28,310 --> 00:17:31,730
architecture and then finally the core

00:17:30,290 --> 00:17:34,280
abstractions that are involved in this

00:17:31,730 --> 00:17:35,960
Kafka based event pipeline I want to

00:17:34,280 --> 00:17:37,400
tell you now about the systems and

00:17:35,960 --> 00:17:39,110
libraries he built to augment our Kafka

00:17:37,400 --> 00:17:41,390
cluster and make it actually usable and

00:17:39,110 --> 00:17:44,169
useful to us to connect it to our data

00:17:41,390 --> 00:17:46,059
sources and sinks so

00:17:44,169 --> 00:17:49,029
hinted at we have our made a data

00:17:46,059 --> 00:17:50,559
register API storing the record schemas

00:17:49,029 --> 00:17:52,659
and all of the additional metadata about

00:17:50,559 --> 00:17:54,940
uniqueness ordering and correctness in

00:17:52,659 --> 00:17:57,519
this registry lets us access that

00:17:54,940 --> 00:17:58,690
information programmatically and use

00:17:57,519 --> 00:18:00,700
that from any system that's going to

00:17:58,690 --> 00:18:02,080
produce or consume records so important

00:18:00,700 --> 00:18:03,279
to note that this schema registry

00:18:02,080 --> 00:18:04,720
service doesn't actually handle any

00:18:03,279 --> 00:18:06,850
records itself it just stores the

00:18:04,720 --> 00:18:09,159
schemas for the records and the extra

00:18:06,850 --> 00:18:10,539
metadata for topics validates that new

00:18:09,159 --> 00:18:11,799
versions of schemas that you register

00:18:10,539 --> 00:18:13,210
are compatible with the previous ones

00:18:11,799 --> 00:18:15,460
because you want version compatibility

00:18:13,210 --> 00:18:18,580
and then surf that metadata back out to

00:18:15,460 --> 00:18:20,559
any system that needs it ok so obviously

00:18:18,580 --> 00:18:22,659
use it in two places

00:18:20,559 --> 00:18:23,889
before you produce a record to a topic

00:18:22,659 --> 00:18:25,419
you want to make sure that it's the

00:18:23,889 --> 00:18:27,940
correct schema for that topic so you go

00:18:25,419 --> 00:18:30,070
look up look it up kava comes with

00:18:27,940 --> 00:18:31,779
producer and consumers implementations

00:18:30,070 --> 00:18:33,850
for a whole bunch of languages but like

00:18:31,779 --> 00:18:35,499
Kafka itself it just knows about those

00:18:33,850 --> 00:18:37,419
just know about byte streams right Kafka

00:18:35,499 --> 00:18:38,830
brokers don't view records in a topic as

00:18:37,419 --> 00:18:40,539
anything other than binary blobs and

00:18:38,830 --> 00:18:42,820
this is part of why it's so fast it's

00:18:40,539 --> 00:18:44,950
just shifting bytes around so we built a

00:18:42,820 --> 00:18:47,049
wrapper around these libraries to handle

00:18:44,950 --> 00:18:48,789
the serialization and validation so our

00:18:47,049 --> 00:18:50,970
producer library when you go to send

00:18:48,789 --> 00:18:53,350
something accepts a never record object

00:18:50,970 --> 00:18:54,639
validates that it conforms to the known

00:18:53,350 --> 00:18:56,230
schema for the top of it you want to

00:18:54,639 --> 00:18:58,299
send it to and then serialize this

00:18:56,230 --> 00:19:00,009
correctly and adds an extra byte on the

00:18:58,299 --> 00:19:02,230
front I signal X version the version of

00:19:00,009 --> 00:19:04,269
this scheme that we're using here on the

00:19:02,230 --> 00:19:05,950
far side and the consumer our wrapper

00:19:04,269 --> 00:19:08,379
does the opposite right says I'm

00:19:05,950 --> 00:19:10,809
consuming the widgets topic goes looks

00:19:08,379 --> 00:19:12,879
it up finds the schema document for that

00:19:10,809 --> 00:19:14,619
version on the serialize record that it

00:19:12,879 --> 00:19:15,879
just pulled off the topic and then

00:19:14,619 --> 00:19:17,230
deserialize is back to whatever

00:19:15,879 --> 00:19:22,539
application code and that side wants to

00:19:17,230 --> 00:19:25,029
work with so 12e o is a very polyglot

00:19:22,539 --> 00:19:27,159
development environment we run a lot of

00:19:25,029 --> 00:19:28,720
Python a lot of Java and Scala some PHP

00:19:27,159 --> 00:19:30,850
and there's even a little bit of Ruby

00:19:28,720 --> 00:19:32,109
node and go scattered around our

00:19:30,850 --> 00:19:33,580
pipeline library is the official

00:19:32,109 --> 00:19:35,259
libraries who support for people want to

00:19:33,580 --> 00:19:37,330
do produce read SATA Kafka support

00:19:35,259 --> 00:19:39,159
Python and Java but we decided against

00:19:37,330 --> 00:19:40,389
porting it to those for more languages

00:19:39,159 --> 00:19:41,759
because there have been a lot of effort

00:19:40,389 --> 00:19:44,259
for diminishing returns

00:19:41,759 --> 00:19:45,850
so we also already have very well

00:19:44,259 --> 00:19:48,009
defined and supported standards inside

00:19:45,850 --> 00:19:50,200
of Twilio for internal rest api is with

00:19:48,009 --> 00:19:52,090
json serialization so we designed a

00:19:50,200 --> 00:19:54,340
simple service and it does exactly what

00:19:52,090 --> 00:19:56,109
it looks like here to produce two Kafka

00:19:54,340 --> 00:19:57,940
topics in response to HTTP requests

00:19:56,109 --> 00:19:59,740
right so you just put at this system

00:19:57,940 --> 00:20:01,090
with a JSON blob with your data and

00:19:59,740 --> 00:20:02,320
invalidate does the exact same work

00:20:01,090 --> 00:20:03,730
right validates that this is the correct

00:20:02,320 --> 00:20:05,770
schema for this and then puts it on

00:20:03,730 --> 00:20:08,470
topic for you so if you're not in Java

00:20:05,770 --> 00:20:09,850
or in Python you can just chuck records

00:20:08,470 --> 00:20:15,550
at this API over HTTP you not don't

00:20:09,850 --> 00:20:18,190
worry about talking to Kafka okay so

00:20:15,550 --> 00:20:19,270
that covers it for producing records I'm

00:20:18,190 --> 00:20:21,730
gonna talk about what we do with them on

00:20:19,270 --> 00:20:23,680
the far side now and this breaks down

00:20:21,730 --> 00:20:26,080
into roughly six types of systems so

00:20:23,680 --> 00:20:29,470
first we can archive things to

00:20:26,080 --> 00:20:30,760
inexpensive and scalable cold storage we

00:20:29,470 --> 00:20:32,470
use this archive data to power three

00:20:30,760 --> 00:20:34,150
more use cases right so we can put it in

00:20:32,470 --> 00:20:35,410
data warehouses to do analysis on the

00:20:34,150 --> 00:20:36,970
structure in subsets that we know about

00:20:35,410 --> 00:20:39,430
ahead of time we can run batch

00:20:36,970 --> 00:20:41,440
processing jobs over arbitrary large

00:20:39,430 --> 00:20:44,050
amounts of data with on-demand compute

00:20:41,440 --> 00:20:45,280
capacity and we can do ad hoc analysis

00:20:44,050 --> 00:20:47,680
of archive data without having to load

00:20:45,280 --> 00:20:48,970
it into a warehouse then we have stream

00:20:47,680 --> 00:20:50,800
processing since Kafka is high

00:20:48,970 --> 00:20:52,150
throughput and low latency you can

00:20:50,800 --> 00:20:53,830
connect streaming applications directly

00:20:52,150 --> 00:20:56,380
as coffin consumers to process records

00:20:53,830 --> 00:20:58,000
very close to real-time and finally you

00:20:56,380 --> 00:21:00,280
can connect transaction databases like

00:20:58,000 --> 00:21:01,720
my sequel or elasticsearch to provide

00:21:00,280 --> 00:21:03,700
access to small amounts of recent

00:21:01,720 --> 00:21:06,640
history with very low end in latency

00:21:03,700 --> 00:21:09,000
from produced to query time so go freeze

00:21:06,640 --> 00:21:12,490
in order and we'll start with archival

00:21:09,000 --> 00:21:14,140
so we call this Twilio FS which just a

00:21:12,490 --> 00:21:15,430
data Lake which is a term that some data

00:21:14,140 --> 00:21:16,660
architects made up to describe what

00:21:15,430 --> 00:21:18,370
happens when you dump everything you

00:21:16,660 --> 00:21:19,600
collect into one place so you can

00:21:18,370 --> 00:21:22,090
extract chunks of its for further

00:21:19,600 --> 00:21:24,940
processing or analysis this looks like

00:21:22,090 --> 00:21:27,280
this we use Amazon s3 and we have a

00:21:24,940 --> 00:21:29,200
system we call copycat cough cough cough

00:21:27,280 --> 00:21:30,820
cough workers most importantly don't

00:21:29,200 --> 00:21:32,860
have infinite storage space because they

00:21:30,820 --> 00:21:34,630
use the local disk and therefore they

00:21:32,860 --> 00:21:36,250
only retain a fixed amount of time for

00:21:34,630 --> 00:21:38,650
each topic before they just delete data

00:21:36,250 --> 00:21:41,140
to make room for new data so copycat has

00:21:38,650 --> 00:21:44,940
one job to copy byte for byte entire

00:21:41,140 --> 00:21:47,860
kafka topics into Amazon s3

00:21:44,940 --> 00:21:49,300
it's a cough connect application connect

00:21:47,860 --> 00:21:50,770
is a framework that's distributed with

00:21:49,300 --> 00:21:52,930
Kafka and it's designed for exactly this

00:21:50,770 --> 00:21:54,850
of consuming from Kafka and writing it

00:21:52,930 --> 00:21:57,250
somewhere else or reading from a source

00:21:54,850 --> 00:21:59,500
system and producing a Kafka it has this

00:21:57,250 --> 00:22:01,330
framework basically it just handles the

00:21:59,500 --> 00:22:02,470
operation of connecting to Kafka and

00:22:01,330 --> 00:22:03,550
consuming records and just hands you

00:22:02,470 --> 00:22:05,590
records and you decide what you want to

00:22:03,550 --> 00:22:07,540
do with them we use this for most of the

00:22:05,590 --> 00:22:09,850
sink systems we write Kafka records to

00:22:07,540 --> 00:22:11,440
its output is organized by the topic and

00:22:09,850 --> 00:22:11,860
partition then the date in the hour and

00:22:11,440 --> 00:22:13,360
in the

00:22:11,860 --> 00:22:16,559
highest offset and the Kafka batch that

00:22:13,360 --> 00:22:18,549
was flushed into that this output file

00:22:16,559 --> 00:22:20,260
before we've done anything else to this

00:22:18,549 --> 00:22:22,929
data right so copycat has already given

00:22:20,260 --> 00:22:25,210
this really neat capability the output

00:22:22,929 --> 00:22:27,340
is a perfect copy of the original Kafka

00:22:25,210 --> 00:22:29,260
topic in order and so we can replay any

00:22:27,340 --> 00:22:31,000
subset of it back into Kafka and it's

00:22:29,260 --> 00:22:32,440
like we've gone back in time so it's

00:22:31,000 --> 00:22:33,910
really useful if you find three months

00:22:32,440 --> 00:22:35,950
down the line that you had a bug in your

00:22:33,910 --> 00:22:37,210
logic and you want to go and reprocess

00:22:35,950 --> 00:22:38,620
those three months right in use go pick

00:22:37,210 --> 00:22:42,760
it back up at s3 and replay it into

00:22:38,620 --> 00:22:45,160
Kafka but it's just the first step in

00:22:42,760 --> 00:22:47,049
our data archive because copycats output

00:22:45,160 --> 00:22:48,400
is just the Kafka topic it's not

00:22:47,049 --> 00:22:51,100
guaranteed to be unique or in order

00:22:48,400 --> 00:22:53,350
because distributed systems so we're

00:22:51,100 --> 00:22:54,700
running system called a copy dog I don't

00:22:53,350 --> 00:22:56,640
really know why we named it this I think

00:22:54,700 --> 00:22:58,270
it's just that the dog chases the cat

00:22:56,640 --> 00:23:00,610
and cleans it up

00:22:58,270 --> 00:23:02,200
so copy dog is a spark job and what it

00:23:00,610 --> 00:23:04,179
does is it takes the raw topic data that

00:23:02,200 --> 00:23:05,290
could get from copy cat and s3 and it

00:23:04,179 --> 00:23:06,880
organizes it for us

00:23:05,290 --> 00:23:08,350
this is the first place with that topic

00:23:06,880 --> 00:23:10,690
metadata I was talking about it comes

00:23:08,350 --> 00:23:12,940
into play the deduplication and ordering

00:23:10,690 --> 00:23:14,980
configuration is configuration that

00:23:12,940 --> 00:23:16,780
drives copy dog so it uses its knowledge

00:23:14,980 --> 00:23:18,700
of how to sort and merge records to give

00:23:16,780 --> 00:23:22,410
us a verified sorted indeed duplicated

00:23:18,700 --> 00:23:25,120
archive of all of our topic data in s3

00:23:22,410 --> 00:23:27,309
it actually has two inputs the latest

00:23:25,120 --> 00:23:28,840
input file from the last time we ran so

00:23:27,309 --> 00:23:30,820
the raw records that are coming in from

00:23:28,840 --> 00:23:33,610
copy cat from Kafka and then the

00:23:30,820 --> 00:23:35,530
existing entire set of data and the neat

00:23:33,610 --> 00:23:36,669
trick here is that you take the first in

00:23:35,530 --> 00:23:38,590
the new set of Records that are coming

00:23:36,669 --> 00:23:41,020
in sort them using the sort key you know

00:23:38,590 --> 00:23:43,270
about so usually a time field and then

00:23:41,020 --> 00:23:44,650
identify the prefixes of the time that

00:23:43,270 --> 00:23:46,720
we found new records for in this

00:23:44,650 --> 00:23:48,730
incoming batch reload all of the

00:23:46,720 --> 00:23:50,169
existing records from their archive for

00:23:48,730 --> 00:23:52,750
that and then merge everything together

00:23:50,169 --> 00:23:54,220
and then D duplicate and then write out

00:23:52,750 --> 00:23:55,330
all the changed and only the change

00:23:54,220 --> 00:23:57,040
fields so we're not touching all of

00:23:55,330 --> 00:23:58,240
history every time just the pieces that

00:23:57,040 --> 00:24:00,160
are changing because we've seen new

00:23:58,240 --> 00:24:02,799
records and where you write that back

00:24:00,160 --> 00:24:04,840
out to the s3 in the archive and since

00:24:02,799 --> 00:24:06,490
s3 doesn't have an atomic move operation

00:24:04,840 --> 00:24:07,960
we actually write out new version file

00:24:06,490 --> 00:24:09,400
names and maintain a single metadata

00:24:07,960 --> 00:24:11,710
file at the end that points to the

00:24:09,400 --> 00:24:13,419
current version for each chunk this lets

00:24:11,710 --> 00:24:14,919
other tasks other spark jobs see

00:24:13,419 --> 00:24:16,390
consistent view of the data and only

00:24:14,919 --> 00:24:20,250
pick up new records once they're finally

00:24:16,390 --> 00:24:22,929
done so this is our true data archive

00:24:20,250 --> 00:24:25,750
every record produced to a topic on

00:24:22,929 --> 00:24:27,970
Kafka is in this three bucket

00:24:25,750 --> 00:24:30,280
once in the latest tape we've seen for

00:24:27,970 --> 00:24:31,720
it and sorted according to the ordering

00:24:30,280 --> 00:24:33,280
for the data set in the pivotal element

00:24:31,720 --> 00:24:35,080
of both these things to kind of come

00:24:33,280 --> 00:24:36,820
back to how much time we were spending

00:24:35,080 --> 00:24:39,130
on new integrations coming into the data

00:24:36,820 --> 00:24:41,050
platform is it there 100% configuration

00:24:39,130 --> 00:24:43,240
driven all we need to turn this on is

00:24:41,050 --> 00:24:45,940
everything that lives in that schema API

00:24:43,240 --> 00:24:47,560
so we just push a new job that says go

00:24:45,940 --> 00:24:49,420
find this topic and just set everything

00:24:47,560 --> 00:24:51,100
up and we can provision new tasks with

00:24:49,420 --> 00:24:53,290
an API call instead of writing code and

00:24:51,100 --> 00:24:56,980
we just push new configuration to task

00:24:53,290 --> 00:24:58,660
special and off we go so as I was saying

00:24:56,980 --> 00:25:00,430
earlier this archive is now immediately

00:24:58,660 --> 00:25:01,840
usable for three things so we can put it

00:25:00,430 --> 00:25:03,990
in data warehouses for interactive

00:25:01,840 --> 00:25:06,790
querying we use Amazon redshift for this

00:25:03,990 --> 00:25:07,180
red ship is Amazon's managed columnar

00:25:06,790 --> 00:25:10,480
database

00:25:07,180 --> 00:25:11,980
it speaks Postgres so RBI analysts love

00:25:10,480 --> 00:25:13,720
it and you can just put liquor in front

00:25:11,980 --> 00:25:15,490
of it and have visual dashboards because

00:25:13,720 --> 00:25:18,070
it's big sequel and it's optimized for

00:25:15,490 --> 00:25:19,690
doing bulk loads from s3 we built a lot

00:25:18,070 --> 00:25:21,280
of custom tooling around it around the

00:25:19,690 --> 00:25:23,020
redshift management API so let us

00:25:21,280 --> 00:25:24,580
provision multiple redshift clusters to

00:25:23,020 --> 00:25:26,200
basically slice up different pieces of

00:25:24,580 --> 00:25:28,870
that data warehouse or sorry of the data

00:25:26,200 --> 00:25:30,370
Lake individual data Mart's and generate

00:25:28,870 --> 00:25:32,500
schedules based on our metadata to

00:25:30,370 --> 00:25:34,540
reload data from s3 into one or more

00:25:32,500 --> 00:25:35,710
places let's let's let's keep the

00:25:34,540 --> 00:25:37,330
clusters small and performing optimally

00:25:35,710 --> 00:25:38,590
and keep the sensitive financial

00:25:37,330 --> 00:25:39,670
information away from people who don't

00:25:38,590 --> 00:25:41,140
need to see it by having separate

00:25:39,670 --> 00:25:45,130
warehouses and only handing out keys the

00:25:41,140 --> 00:25:46,360
appropriate ones we can also use this

00:25:45,130 --> 00:25:48,160
primary archives to drive batch

00:25:46,360 --> 00:25:50,010
processing for drive data and ad-hoc

00:25:48,160 --> 00:25:52,300
analysis so this is just spark again

00:25:50,010 --> 00:25:54,130
spark has a very well supported s3

00:25:52,300 --> 00:25:55,960
driver and we've extended it to support

00:25:54,130 --> 00:25:59,020
the organizational and schema metadata

00:25:55,960 --> 00:26:00,220
that involved in toyou FS other

00:25:59,020 --> 00:26:02,650
engineering teams that need to do

00:26:00,220 --> 00:26:04,390
derived data analysis just right

00:26:02,650 --> 00:26:06,010
dedicated spark jobs and use our

00:26:04,390 --> 00:26:07,330
infrastructure to schedule them so if we

00:26:06,010 --> 00:26:09,670
have something raw coming in some raw

00:26:07,330 --> 00:26:11,230
sort of events and we want to produce a

00:26:09,670 --> 00:26:13,120
new drive set you just write a spark job

00:26:11,230 --> 00:26:14,800
pick them up and then run it over all

00:26:13,120 --> 00:26:17,590
the input continuously and just spit it

00:26:14,800 --> 00:26:19,030
back out we also use spark for ad-hoc

00:26:17,590 --> 00:26:20,860
processing so stuff that's not in the

00:26:19,030 --> 00:26:22,540
warehouse but every once in a while

00:26:20,860 --> 00:26:23,980
somebody wants to go and query a huge

00:26:22,540 --> 00:26:26,530
amount of data like three years worth of

00:26:23,980 --> 00:26:27,700
API logs and it's expensive and slow to

00:26:26,530 --> 00:26:29,620
load that into a redshift cluster just

00:26:27,700 --> 00:26:31,030
to run a few queries so we use Jupiter

00:26:29,620 --> 00:26:33,400
notebook in front of SPARC to just do

00:26:31,030 --> 00:26:35,140
this on ad hoc basis this tends up to

00:26:33,400 --> 00:26:36,340
cop a lot during outage response or if

00:26:35,140 --> 00:26:37,870
you're prototyping a new scheduled back

00:26:36,340 --> 00:26:39,590
jobs right just type it out as a sequel

00:26:37,870 --> 00:26:43,820
query see what happens and then

00:26:39,590 --> 00:26:44,990
to a full-on spark job and I'm gonna

00:26:43,820 --> 00:26:46,340
sound like a broken record but we also

00:26:44,990 --> 00:26:49,340
use spark and streaming mode to do

00:26:46,340 --> 00:26:50,840
stream processing and this is what it

00:26:49,340 --> 00:26:52,610
looks like you just connect a spark job

00:26:50,840 --> 00:26:54,409
in stream mode to the kafka topic

00:26:52,610 --> 00:26:56,299
process data in real time and they mate

00:26:54,409 --> 00:27:00,320
results back into a different coffin

00:26:56,299 --> 00:27:02,120
topic to do whatever you want with and

00:27:00,320 --> 00:27:04,039
finally we can consume directly from

00:27:02,120 --> 00:27:06,110
coffee topics and write the data to

00:27:04,039 --> 00:27:07,610
online storage systems and this is

00:27:06,110 --> 00:27:08,960
pretty simple you just run a coffee

00:27:07,610 --> 00:27:10,520
connect or something else that takes

00:27:08,960 --> 00:27:14,230
records from Kafka and puts them into a

00:27:10,520 --> 00:27:14,230
database and then you can just query it

00:27:14,470 --> 00:27:19,279
so we have data in our pipeline we've

00:27:18,049 --> 00:27:20,929
consumed it and put it in a bunch of

00:27:19,279 --> 00:27:23,149
places to store it and do compute over

00:27:20,929 --> 00:27:24,950
it how do we know that you have all the

00:27:23,149 --> 00:27:26,179
data in the right places that it needs

00:27:24,950 --> 00:27:29,450
to be and that it's correct everywhere

00:27:26,179 --> 00:27:32,020
as I was saying earlier so first

00:27:29,450 --> 00:27:34,190
Kafka is consumer offset tracking

00:27:32,020 --> 00:27:35,960
assures that we only acknowledge records

00:27:34,190 --> 00:27:38,899
that's delivered to a system once we

00:27:35,960 --> 00:27:40,370
actually store the processed it so those

00:27:38,899 --> 00:27:42,649
offsets are available in Kafka

00:27:40,370 --> 00:27:44,799
themselves as streams so you can track

00:27:42,649 --> 00:27:47,270
and monitor consumer progress over time

00:27:44,799 --> 00:27:49,279
so if we see that a particular system

00:27:47,270 --> 00:27:50,750
gets stuck or slows down you can set off

00:27:49,279 --> 00:27:52,100
on alert because it's not going to be

00:27:50,750 --> 00:27:54,409
committing new offsets and not making

00:27:52,100 --> 00:27:56,779
forward progress and make sure that you

00:27:54,409 --> 00:27:58,880
set up alert you go fix it for data

00:27:56,779 --> 00:28:00,440
correctness we reuse our topic metadata

00:27:58,880 --> 00:28:01,880
to do lightweight SMO testing it's

00:28:00,440 --> 00:28:03,140
pretty much like I was hinting at

00:28:01,880 --> 00:28:05,630
earlier you just say oh well we know

00:28:03,140 --> 00:28:07,399
buddy Yuki is for this data and just run

00:28:05,630 --> 00:28:08,990
a job that connects to each data store

00:28:07,399 --> 00:28:11,029
and then it says count unique and it

00:28:08,990 --> 00:28:14,890
says cool everything lines up or maybe

00:28:11,029 --> 00:28:18,500
this is wrong we should go look at this

00:28:14,890 --> 00:28:19,880
so to recap what we have is we went

00:28:18,500 --> 00:28:22,789
through an event oriented data pipeline

00:28:19,880 --> 00:28:24,409
architecture and used Kafka as a

00:28:22,789 --> 00:28:27,020
reliable and high-performance eight of

00:28:24,409 --> 00:28:28,460
us it gives us a strong foundation for

00:28:27,020 --> 00:28:30,409
reasoning about in planning data flows

00:28:28,460 --> 00:28:31,880
between systems systems that produce

00:28:30,409 --> 00:28:34,039
data the systems that process and

00:28:31,880 --> 00:28:36,200
consume that data and the systems a

00:28:34,039 --> 00:28:37,820
story I spend a lot of time talking

00:28:36,200 --> 00:28:39,200
about why you strongly type data with a

00:28:37,820 --> 00:28:40,970
single source was true for a schemas is

00:28:39,200 --> 00:28:42,200
important and why you'll benefit from

00:28:40,970 --> 00:28:44,630
doing this in your dated processing

00:28:42,200 --> 00:28:46,640
systems and toast briefly on the

00:28:44,630 --> 00:28:47,899
properties of these systems how the

00:28:46,640 --> 00:28:49,490
properties are this doing this

00:28:47,899 --> 00:28:51,710
guarantees that data is delivered

00:28:49,490 --> 00:28:53,360
everywhere it needs to be and how we can

00:28:51,710 --> 00:28:54,679
build monitoring systems to verified

00:28:53,360 --> 00:28:56,809
is the case and talked through all the

00:28:54,679 --> 00:28:58,340
systems we have to actually do useful

00:28:56,809 --> 00:29:01,850
things with the data once we have it in

00:28:58,340 --> 00:29:03,440
our main pipeline at folio so to revisit

00:29:01,850 --> 00:29:05,690
our motivating story from earlier

00:29:03,440 --> 00:29:07,790
instead of this N squared size web of

00:29:05,690 --> 00:29:10,280
connections between sources and sinks

00:29:07,790 --> 00:29:11,750
made directly we have a single path via

00:29:10,280 --> 00:29:14,090
flows as a single source of events

00:29:11,750 --> 00:29:15,440
through the Kafka bus and any system

00:29:14,090 --> 00:29:19,160
that needs that particular act of data

00:29:15,440 --> 00:29:21,230
can just connect and listen to it if we

00:29:19,160 --> 00:29:22,700
come on and change the schema right the

00:29:21,230 --> 00:29:24,350
systems that are connected either

00:29:22,700 --> 00:29:26,419
seamlessly convert data to the version

00:29:24,350 --> 00:29:27,710
they expect thanks to a fro or they

00:29:26,419 --> 00:29:29,090
gracefully process to the end of the

00:29:27,710 --> 00:29:31,520
topic with the old version and we just

00:29:29,090 --> 00:29:32,840
update them to roll onto the new one the

00:29:31,520 --> 00:29:34,429
end of the year comes around and the

00:29:32,840 --> 00:29:36,350
p.m. just requested temporary redshift

00:29:34,429 --> 00:29:39,530
cluster with historical data to build

00:29:36,350 --> 00:29:41,390
numbers for a slide deck we try out new

00:29:39,530 --> 00:29:42,559
data stores all the time we find out

00:29:41,390 --> 00:29:43,820
that they totally suit their needs or

00:29:42,559 --> 00:29:46,460
they don't and we just drop them in as

00:29:43,820 --> 00:29:47,840
new flavors of caca consumers we run

00:29:46,460 --> 00:29:50,120
spam detection code or fraud detection

00:29:47,840 --> 00:29:52,100
code that runs nightly spark jobs over

00:29:50,120 --> 00:29:53,690
all of history updates and new models

00:29:52,100 --> 00:29:56,120
and then reloads those models with the

00:29:53,690 --> 00:29:58,490
new parameters into streaming jobs to

00:29:56,120 --> 00:30:00,799
watch for spam in real time we hook up

00:29:58,490 --> 00:30:02,240
elasticsearch or whatever a new online

00:30:00,799 --> 00:30:03,770
store we want and the you full-text

00:30:02,240 --> 00:30:06,530
search feature works flawlessly and so

00:30:03,770 --> 00:30:07,940
on so yeah that's all I have thank you

00:30:06,530 --> 00:30:09,140
for listening I've hopefully I'm sure

00:30:07,940 --> 00:30:09,950
today can help you with your own data

00:30:09,140 --> 00:30:16,209
challenges

00:30:09,950 --> 00:30:16,209
[Applause]

00:30:17,320 --> 00:30:23,690
thank you Sam on behalf of the

00:30:20,420 --> 00:30:27,620
organization is your mug and for

00:30:23,690 --> 00:30:30,790
everyone now it's lunch time so please

00:30:27,620 --> 00:30:30,790

YouTube URL: https://www.youtube.com/watch?v=-BGrV5mB9G0


