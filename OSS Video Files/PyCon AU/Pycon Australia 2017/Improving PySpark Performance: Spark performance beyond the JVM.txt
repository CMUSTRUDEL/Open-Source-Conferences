Title: Improving PySpark Performance: Spark performance beyond the JVM
Publication date: 2017-08-05
Playlist: Pycon Australia 2017
Description: 
	Holden Karau

http://2017.pycon-au.org/schedule/presentation/62/

#pyconau

This talk was given at PyCon Australia 2017 which was held from 3-8 August, 2017 in Melbourne, Victoria.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2017, we're returning to Melbourne, bringing together students, enthusiasts, and professionals with a love of Python from around Australia, and from all over the World. 

August 3-8 2017, Melbourne, Victoria

Python, PyCon, PyConAU
Captions: 
	00:00:00,030 --> 00:00:03,560
our last speaker for the day is Holden

00:00:03,030 --> 00:00:06,390
Carrell

00:00:03,560 --> 00:00:09,030
Holden is an active open-source

00:00:06,390 --> 00:00:10,769
contributor particularly to spark and

00:00:09,030 --> 00:00:13,920
has co-authored numerous books on the

00:00:10,769 --> 00:00:15,480
subject she's based in San Francisco

00:00:13,920 --> 00:00:17,250
working as a software development

00:00:15,480 --> 00:00:18,810
engineer at IBM a spark Technology

00:00:17,250 --> 00:00:20,699
Center and today we're going to hear

00:00:18,810 --> 00:00:23,939
about improving PI spark performance

00:00:20,699 --> 00:00:28,980
awesome thank you yeah thank you all for

00:00:23,939 --> 00:00:30,949
staying thank you yeah thanks for

00:00:28,980 --> 00:00:34,700
staying for the last slot I know it's

00:00:30,949 --> 00:00:37,200
it's a long day and I'm probably gonna

00:00:34,700 --> 00:00:39,210
pass out right after this talk so so

00:00:37,200 --> 00:00:41,190
thank you for hanging in with us

00:00:39,210 --> 00:00:43,920
so yeah I'm holding my preferred

00:00:41,190 --> 00:00:45,660
pronouns are hear her and I I do work at

00:00:43,920 --> 00:00:50,129
IBM so our Technology Center I am a

00:00:45,660 --> 00:00:52,020
smart committer which is nice except now

00:00:50,129 --> 00:00:54,660
it means I can't just blame other people

00:00:52,020 --> 00:00:55,920
for my problems in life now I'm

00:00:54,660 --> 00:00:58,289
partially responsible for my own

00:00:55,920 --> 00:00:59,850
problems in life but if you come to me

00:00:58,289 --> 00:01:02,640
with smart problems I will still blame

00:00:59,850 --> 00:01:05,760
other people you can follow me on

00:01:02,640 --> 00:01:08,119
Twitter if you're interested in feeling

00:01:05,760 --> 00:01:12,570
really good about not living in America

00:01:08,119 --> 00:01:15,090
and I've got a SlideShare where the

00:01:12,570 --> 00:01:17,939
slides from today's talk will go and

00:01:15,090 --> 00:01:19,320
I've got our LinkedIn and github as well

00:01:17,939 --> 00:01:21,509
they could hardly know what to do with

00:01:19,320 --> 00:01:23,880
the LinkedIn I really hope someone

00:01:21,509 --> 00:01:27,420
figures out what that's for but my boss

00:01:23,880 --> 00:01:30,570
seems to like it and if this is your

00:01:27,420 --> 00:01:32,909
first spark related talk I'm sorry this

00:01:30,570 --> 00:01:34,140
is not a great intro to spark but I do

00:01:32,909 --> 00:01:35,700
have some other videos that you can

00:01:34,140 --> 00:01:40,950
check out and they're there on YouTube

00:01:35,700 --> 00:01:43,890
and hopefully they're fun too and so

00:01:40,950 --> 00:01:46,110
just outside of software like I'm trans

00:01:43,890 --> 00:01:48,689
and queer and Canadian that's maybe less

00:01:46,110 --> 00:01:50,100
surprising here than in America and I

00:01:48,689 --> 00:01:52,409
consider myself a part of the broader

00:01:50,100 --> 00:01:54,030
leather community and this is just like

00:01:52,409 --> 00:01:55,729
a reminder like there's a lot of

00:01:54,030 --> 00:01:58,829
different types of people in the world

00:01:55,729 --> 00:02:00,240
we all write soft well not all of us

00:01:58,829 --> 00:02:02,520
write software but many of us write

00:02:00,240 --> 00:02:07,469
software and we should be nice to

00:02:02,520 --> 00:02:09,599
everyone you know the slide matters more

00:02:07,469 --> 00:02:11,700
in America personally but but I hope you

00:02:09,599 --> 00:02:13,400
all are nice people and and please

00:02:11,700 --> 00:02:16,790
continue to be nice folks

00:02:13,400 --> 00:02:20,060
if you're not just fake it it's not that

00:02:16,790 --> 00:02:24,500
hard this is the mandatory slide from my

00:02:20,060 --> 00:02:26,120
employer we have this office with we've

00:02:24,500 --> 00:02:28,159
got this Lobby it's got a lot of green

00:02:26,120 --> 00:02:30,709
in it which is the color of a passing

00:02:28,159 --> 00:02:32,540
Travis build and therefore clearly

00:02:30,709 --> 00:02:34,730
indicates that we write high quality

00:02:32,540 --> 00:02:38,000
software otherwise these plants would

00:02:34,730 --> 00:02:39,709
all be red or sort of yellow so if

00:02:38,000 --> 00:02:42,049
you're ever considering buying a support

00:02:39,709 --> 00:02:46,190
contract from IBM or whatever it is we

00:02:42,049 --> 00:02:49,280
sell please buy an IBM support contract

00:02:46,190 --> 00:02:51,650
I don't get involved with selling them

00:02:49,280 --> 00:02:53,540
but I'm told if we don't sell anything I

00:02:51,650 --> 00:02:58,010
probably don't have a job anymore so so

00:02:53,540 --> 00:02:59,540
buy things from IBM yeah there's a bunch

00:02:58,010 --> 00:03:02,239
of other people who I work with who work

00:02:59,540 --> 00:03:04,609
on SPARC as well so it's not just myself

00:03:02,239 --> 00:03:06,739
I just happen to like drink a lot more

00:03:04,609 --> 00:03:09,069
coffee than they do and like talking to

00:03:06,739 --> 00:03:12,019
humans so so I tend to be around more

00:03:09,069 --> 00:03:14,750
and we do a lot of work specifically in

00:03:12,019 --> 00:03:16,609
the Python area the the marketing people

00:03:14,750 --> 00:03:20,060
like this slide because it implies we do

00:03:16,609 --> 00:03:21,769
more work than we actually do but you

00:03:20,060 --> 00:03:23,690
can see here we have a big number and

00:03:21,769 --> 00:03:24,859
our logo and then we have some smaller

00:03:23,690 --> 00:03:25,389
numbers and some names of other

00:03:24,859 --> 00:03:31,129
companies

00:03:25,389 --> 00:03:33,199
so definitely iBM is good ok um so back

00:03:31,129 --> 00:03:34,730
to happy like computery things so what

00:03:33,199 --> 00:03:36,139
what are we gonna talk about I'm gonna

00:03:34,730 --> 00:03:39,019
talk about my assumptions about who you

00:03:36,139 --> 00:03:40,970
all are we're gonna talk about how to

00:03:39,019 --> 00:03:43,639
reuse data inside of SPARC because this

00:03:40,970 --> 00:03:45,799
is like a thing which we really should

00:03:43,639 --> 00:03:47,690
talk about more commonly we're going to

00:03:45,799 --> 00:03:50,299
talk about the challenges of working

00:03:47,690 --> 00:03:52,010
with key value data and SPARC I'm going

00:03:50,299 --> 00:03:54,379
to include the word count example again

00:03:52,010 --> 00:03:56,060
because everyone really loves word count

00:03:54,379 --> 00:03:58,129
which is why it's an every big data talk

00:03:56,060 --> 00:04:01,790
ever it's definitely a real-world

00:03:58,129 --> 00:04:04,310
example we're gonna talk about sort of

00:04:01,790 --> 00:04:05,900
the the Python specific challenges of

00:04:04,310 --> 00:04:08,180
working in SPARC and we're gonna talk

00:04:05,900 --> 00:04:11,919
about this is the part which is kind of

00:04:08,180 --> 00:04:14,389
sad is how to call JVM code from Python

00:04:11,919 --> 00:04:16,880
because sometimes we still need to do it

00:04:14,389 --> 00:04:19,609
and then I'll also promise you a magical

00:04:16,880 --> 00:04:21,409
shiny future where everything works and

00:04:19,609 --> 00:04:23,300
the magical shiny future may or may not

00:04:21,409 --> 00:04:26,200
come but it comes with like fake engine

00:04:23,300 --> 00:04:29,460
noises vroom vroom ok

00:04:26,200 --> 00:04:32,380
that's funny so I'm hoping you're nice

00:04:29,460 --> 00:04:36,040
if you really don't like pictures of

00:04:32,380 --> 00:04:39,420
cats this is not the talk for you how

00:04:36,040 --> 00:04:42,430
many people know some Apache spark

00:04:39,420 --> 00:04:47,530
huzzah okay how many people have no idea

00:04:42,430 --> 00:04:49,960
what Apache spark is huzzah okay lovely

00:04:47,530 --> 00:04:51,430
um so if you don't know some Apache

00:04:49,960 --> 00:04:53,470
spark hopefully this doesn't scare you

00:04:51,430 --> 00:04:57,220
away will do we'll do a very quick tour

00:04:53,470 --> 00:04:58,840
of it um hopefully you know some good

00:04:57,220 --> 00:05:02,590
places to get coffee I've been to like

00:04:58,840 --> 00:05:04,530
six or seven so far in two days but I

00:05:02,590 --> 00:05:07,090
still want to go to more places as well

00:05:04,530 --> 00:05:08,440
so yeah let's talk about what spark is

00:05:07,090 --> 00:05:10,450
since it appears what a lot of people

00:05:08,440 --> 00:05:12,730
don't know what it is um so it's a

00:05:10,450 --> 00:05:14,110
general-purpose distributed system this

00:05:12,730 --> 00:05:16,060
means that it's not just like it's not

00:05:14,110 --> 00:05:18,610
just like a sequel system or not just

00:05:16,060 --> 00:05:20,380
like an ETL system we can do pretty much

00:05:18,610 --> 00:05:23,170
arbitrary code inside of it and it's

00:05:20,380 --> 00:05:24,130
pretty awesome and happy spark does ask

00:05:23,170 --> 00:05:26,200
us to do this

00:05:24,130 --> 00:05:28,690
like essentially functional programming

00:05:26,200 --> 00:05:30,160
type approach to data manipulation so

00:05:28,690 --> 00:05:32,110
rather than doing things with like

00:05:30,160 --> 00:05:34,120
message passing it really wants us to

00:05:32,110 --> 00:05:37,030
write things as map statements or flat

00:05:34,120 --> 00:05:38,830
maps and reduces and essentially in

00:05:37,030 --> 00:05:42,340
exchange for restricting ourselves to

00:05:38,830 --> 00:05:44,080
this sort of paradigm it promises to

00:05:42,340 --> 00:05:47,080
scale our stuff and make it super fast

00:05:44,080 --> 00:05:48,880
and fun it's an Apache project so it's

00:05:47,080 --> 00:05:50,770
not an IBM project despite the earlier

00:05:48,880 --> 00:05:54,940
slides and what the IBM salespeople may

00:05:50,770 --> 00:05:57,340
imply and it's faster than Hadoop

00:05:54,940 --> 00:05:59,260
MapReduce and I really love Hadoop

00:05:57,340 --> 00:06:03,550
MapReduce it's at this nice low bar for

00:05:59,260 --> 00:06:05,320
us just like hop over and it's really

00:06:03,550 --> 00:06:07,570
good for when problems become too big

00:06:05,320 --> 00:06:10,240
for a single machine which is which is

00:06:07,570 --> 00:06:12,370
probably why you care about it I mean

00:06:10,240 --> 00:06:14,260
it's got two core abstractions and we're

00:06:12,370 --> 00:06:15,970
not going to spend a lot of time on the

00:06:14,260 --> 00:06:20,200
differences between them but that's

00:06:15,970 --> 00:06:22,660
that's okay so the the the terms that

00:06:20,200 --> 00:06:25,390
I'm gonna use in this talk are RDD which

00:06:22,660 --> 00:06:26,730
is a resilient distributed data set we

00:06:25,390 --> 00:06:29,260
can think of it as an immutable

00:06:26,730 --> 00:06:32,460
magically distributed Python collection

00:06:29,260 --> 00:06:35,230
and it totally works

00:06:32,460 --> 00:06:37,900
you can put anything you want inside of

00:06:35,230 --> 00:06:39,520
it except occasionally things will go

00:06:37,900 --> 00:06:39,700
boom if you try and put something larger

00:06:39,520 --> 00:06:42,010
than

00:06:39,700 --> 00:06:43,750
two gigs in a single record inside of it

00:06:42,010 --> 00:06:45,850
but provided that your individual

00:06:43,750 --> 00:06:47,820
records are like reasonably sized you

00:06:45,850 --> 00:06:50,170
can put whatever you want inside of them

00:06:47,820 --> 00:06:52,300
SPARC also has things called data frames

00:06:50,170 --> 00:06:54,040
this tends to make people that work in

00:06:52,300 --> 00:06:56,080
Python excited because they think about

00:06:54,040 --> 00:06:57,250
pandas and data frames and they think

00:06:56,080 --> 00:06:58,960
about a panda's data frame that's

00:06:57,250 --> 00:07:03,070
distributed and that's that's really

00:06:58,960 --> 00:07:05,470
awesome it's not that but it sounds like

00:07:03,070 --> 00:07:07,810
that could be it but that's not what we

00:07:05,470 --> 00:07:10,000
get there's also these data set things

00:07:07,810 --> 00:07:11,080
they're compile time typed we don't

00:07:10,000 --> 00:07:14,980
really care about them because we're

00:07:11,080 --> 00:07:17,770
working in Python but it's okay so why

00:07:14,980 --> 00:07:19,720
why do people come to spark so for

00:07:17,770 --> 00:07:21,400
people that are stuck in the javelin

00:07:19,720 --> 00:07:23,860
they often come to spark because they've

00:07:21,400 --> 00:07:25,960
got a MapReduce job it's taking about 16

00:07:23,860 --> 00:07:27,970
hours to complete they search on Google

00:07:25,960 --> 00:07:29,920
how do I make my MapReduce job faster

00:07:27,970 --> 00:07:31,540
and they get a search result which says

00:07:29,920 --> 00:07:33,940
spark is a hundred times faster than

00:07:31,540 --> 00:07:36,100
MapReduce and they're like whoa they

00:07:33,940 --> 00:07:38,110
probably lied right like we all know

00:07:36,100 --> 00:07:39,580
vendors lie about benchmarks but they

00:07:38,110 --> 00:07:41,770
wouldn't lie by two orders of magnitude

00:07:39,580 --> 00:07:44,500
I'll get at least one order of magnitude

00:07:41,770 --> 00:07:46,060
improvement and one hour is okay with me

00:07:44,500 --> 00:07:48,310
and so then we trick them and they come

00:07:46,060 --> 00:07:52,230
into spark and then they're stuck coming

00:07:48,310 --> 00:07:54,370
through these talks the other one is

00:07:52,230 --> 00:07:56,500
their data just doesn't fit in memory

00:07:54,370 --> 00:07:58,120
anymore so when you're when your data

00:07:56,500 --> 00:08:00,490
frame isn't fitting in memory you come

00:07:58,120 --> 00:08:02,740
to spark because you're like I really I

00:08:00,490 --> 00:08:04,630
want my stuff to work and it's just

00:08:02,740 --> 00:08:06,220
crashing sometimes people come to spark

00:08:04,630 --> 00:08:10,840
because their data doesn't fit in Excel

00:08:06,220 --> 00:08:16,030
anymore and I love those people but yeah

00:08:10,840 --> 00:08:17,320
it's slightly slightly larger class

00:08:16,030 --> 00:08:18,640
problems that it tends to work that

00:08:17,320 --> 00:08:23,640
you're on and the other reason why

00:08:18,640 --> 00:08:23,640
people come is magic and cat pictures

00:08:24,480 --> 00:08:29,010
what more realistically though the other

00:08:26,650 --> 00:08:31,150
reason why people come to spark is that

00:08:29,010 --> 00:08:34,420
unlike a lot of traditional sort of

00:08:31,150 --> 00:08:37,060
distributed systems SPARC ships with a

00:08:34,420 --> 00:08:38,290
large photo variety of tools so in

00:08:37,060 --> 00:08:40,210
addition to having this like sort of

00:08:38,290 --> 00:08:42,910
core raw level where we can write Python

00:08:40,210 --> 00:08:45,280
lambda expressions over our data and do

00:08:42,910 --> 00:08:46,840
really sort of arbitrary things we've

00:08:45,280 --> 00:08:49,480
got a sequel engine so we can write

00:08:46,840 --> 00:08:50,890
sequel expressions on top of our data if

00:08:49,480 --> 00:08:52,240
you want to do streaming data we've got

00:08:50,890 --> 00:08:53,350
some streaming stuff and we have a bunch

00:08:52,240 --> 00:08:55,750
of machine learning tool

00:08:53,350 --> 00:08:57,120
and these all shipped together and sort

00:08:55,750 --> 00:08:59,410
of in the traditional Hadoop ecosystem

00:08:57,120 --> 00:09:01,360
what happens is you have six different

00:08:59,410 --> 00:09:03,610
packages released on different schedules

00:09:01,360 --> 00:09:04,840
that use incompatible versions of

00:09:03,610 --> 00:09:07,090
protobufs in between them

00:09:04,840 --> 00:09:09,970
and then your system administrator cries

00:09:07,090 --> 00:09:15,040
and then you can just come to the

00:09:09,970 --> 00:09:18,130
wonderful world of swag yeah oh yeah so

00:09:15,040 --> 00:09:20,950
there is a catch and this is that spark

00:09:18,130 --> 00:09:23,560
is written in Scala Scala is notably not

00:09:20,950 --> 00:09:27,310
Python as much as we may wish that it

00:09:23,560 --> 00:09:29,040
was and it also has the slight problem

00:09:27,310 --> 00:09:35,560
of that it runs on the JVM

00:09:29,040 --> 00:09:38,440
and so we have to get the JVM and Python

00:09:35,560 --> 00:09:40,420
to be friends and we want to still be

00:09:38,440 --> 00:09:43,240
able to use libraries that are useful so

00:09:40,420 --> 00:09:44,830
we're not gonna do it in JSON right we

00:09:43,240 --> 00:09:46,090
want to be able to use pandas we want to

00:09:44,830 --> 00:09:49,720
be able to use I could learn we want to

00:09:46,090 --> 00:09:52,690
have fun with our lives so PI spark

00:09:49,720 --> 00:09:54,100
looks at this problem and says I know

00:09:52,690 --> 00:09:56,250
the solution to this problem

00:09:54,100 --> 00:10:00,760
it's serializing everything as strings

00:09:56,250 --> 00:10:02,500
and technically that is a solution and

00:10:00,760 --> 00:10:04,210
it turns out that if you have a lot of

00:10:02,500 --> 00:10:06,580
money you can just buy a lot of machines

00:10:04,210 --> 00:10:08,560
and and that works out and and this is

00:10:06,580 --> 00:10:10,560
essentially proof that you can do really

00:10:08,560 --> 00:10:12,850
inefficient things when you have money

00:10:10,560 --> 00:10:14,440
but we're gonna talk about how to avoid

00:10:12,850 --> 00:10:15,550
some of these problems just in case we

00:10:14,440 --> 00:10:20,200
don't have an infinite number of

00:10:15,550 --> 00:10:22,150
machines and sparks equal sort of solves

00:10:20,200 --> 00:10:23,650
this problem and doesn't just serialize

00:10:22,150 --> 00:10:26,260
everything as strings back and forth

00:10:23,650 --> 00:10:29,860
across the wire but but you can actually

00:10:26,260 --> 00:10:31,540
get really far with that so this is this

00:10:29,860 --> 00:10:33,580
is what the PI SPARC architecture looks

00:10:31,540 --> 00:10:35,320
like this architecture diagram

00:10:33,580 --> 00:10:37,120
illustrates that I have yet to convince

00:10:35,320 --> 00:10:39,820
the designers to work with me on any of

00:10:37,120 --> 00:10:42,040
my slides but we can see here we have a

00:10:39,820 --> 00:10:45,370
driver process or we've got Python

00:10:42,040 --> 00:10:48,460
talking to Java and we feel a little sad

00:10:45,370 --> 00:10:50,230
about that but it does the job and so

00:10:48,460 --> 00:10:53,170
we'll ask spark to do something in

00:10:50,230 --> 00:10:54,700
Python and it'll then go ahead ask Java

00:10:53,170 --> 00:10:56,770
like this is the thing I want you to do

00:10:54,700 --> 00:10:58,450
it'll take my lambda expression it'll

00:10:56,770 --> 00:10:59,410
send it over the wire and then we'll

00:10:58,450 --> 00:11:00,970
have all of these different worker

00:10:59,410 --> 00:11:03,430
machines to do our actual computation

00:11:00,970 --> 00:11:06,010
and they'll take the lambda expression

00:11:03,430 --> 00:11:12,910
and deserialize it get our data and

00:11:06,010 --> 00:11:15,460
actual work has right so this impact

00:11:12,910 --> 00:11:17,050
spice park in a few different ways the

00:11:15,460 --> 00:11:18,820
Python workers startup like we all know

00:11:17,050 --> 00:11:20,230
Python games take a bit of time to start

00:11:18,820 --> 00:11:23,830
but at the end of the day we can

00:11:20,230 --> 00:11:25,810
amortize that cost also known as pretend

00:11:23,830 --> 00:11:29,230
it doesn't exist when I have enough

00:11:25,810 --> 00:11:30,700
records really the part which I find

00:11:29,230 --> 00:11:32,740
hurts me the most is that the error

00:11:30,700 --> 00:11:35,080
messages don't really make any sense at

00:11:32,740 --> 00:11:37,120
all you get a Python exception wrapped

00:11:35,080 --> 00:11:38,860
inside of a Java exception wrapped

00:11:37,120 --> 00:11:40,390
inside of another Java exception wrapped

00:11:38,860 --> 00:11:42,700
inside of a Python exception when

00:11:40,390 --> 00:11:44,380
something goes wrong so if you're using

00:11:42,700 --> 00:11:48,370
spark and you're like I don't know what

00:11:44,380 --> 00:11:51,550
the hell just happened that's normal do

00:11:48,370 --> 00:11:52,990
not feel bad the double serialization

00:11:51,550 --> 00:11:54,160
cost is the thing we're gonna focus on

00:11:52,990 --> 00:11:57,370
today though because we're going to talk

00:11:54,160 --> 00:11:59,260
about how to make it go fast so

00:11:57,370 --> 00:12:01,240
hopefully I didn't scare away the people

00:11:59,260 --> 00:12:06,010
that don't know spark yet if I did

00:12:01,240 --> 00:12:08,320
please do not tell my boss and yeah okay

00:12:06,010 --> 00:12:10,660
cool yeah we're gonna get everyone's

00:12:08,320 --> 00:12:12,250
favorite word count example now as we

00:12:10,660 --> 00:12:13,540
can tell this cat is about to get

00:12:12,250 --> 00:12:16,030
covered in water and scratch our

00:12:13,540 --> 00:12:18,190
eyeballs out so there is some mistakes

00:12:16,030 --> 00:12:20,110
with our word count example here but

00:12:18,190 --> 00:12:22,450
this will technically produce the right

00:12:20,110 --> 00:12:25,780
answer it'll just do it inefficiently

00:12:22,450 --> 00:12:27,310
right so here we can see we've got this

00:12:25,780 --> 00:12:29,740
thing called the spark context we can

00:12:27,310 --> 00:12:32,200
use it to load data in this case we're

00:12:29,740 --> 00:12:34,090
loading text files these text files

00:12:32,200 --> 00:12:36,280
could be terabytes of text file stored

00:12:34,090 --> 00:12:39,220
in HDFS or NFS or whatever thing we've

00:12:36,280 --> 00:12:43,750
got we can tokenize our data with the

00:12:39,220 --> 00:12:45,310
very fancy space tokenizer living in

00:12:43,750 --> 00:12:47,890
America I get to mostly pretend that

00:12:45,310 --> 00:12:50,260
other languages don't exist here I think

00:12:47,890 --> 00:12:52,330
you might not have that luxury but so

00:12:50,260 --> 00:12:54,820
you put your fancy tokenization logic

00:12:52,330 --> 00:12:57,250
inside of that then we go ahead and we

00:12:54,820 --> 00:12:59,050
make our word and count pairs we group

00:12:57,250 --> 00:13:01,390
them together for all the same words and

00:12:59,050 --> 00:13:03,280
we compute the sum and this looks pretty

00:13:01,390 --> 00:13:05,110
reasonable and so we save the result out

00:13:03,280 --> 00:13:06,670
and then I go ahead and I'm like well

00:13:05,110 --> 00:13:09,460
I'm actually it turns out that the data

00:13:06,670 --> 00:13:10,750
I'm processing is log files and I'm

00:13:09,460 --> 00:13:12,640
really interested in knowing the number

00:13:10,750 --> 00:13:14,230
of times I've got a warning so I'm also

00:13:12,640 --> 00:13:18,040
going to compute that data too on my

00:13:14,230 --> 00:13:19,930
same inputs right but so what is what is

00:13:18,040 --> 00:13:22,330
wrong with this what what is

00:13:19,930 --> 00:13:24,700
to make spark not be super efficient in

00:13:22,330 --> 00:13:26,020
this case right we've we complied to its

00:13:24,700 --> 00:13:28,390
contract right we haven't tried to

00:13:26,020 --> 00:13:30,790
mutate the rdd's we've done essentially

00:13:28,390 --> 00:13:32,050
functional transformations but things

00:13:30,790 --> 00:13:33,399
are still going to be sad and we're

00:13:32,050 --> 00:13:36,490
still gonna lose our eyeballs because

00:13:33,399 --> 00:13:39,820
this cat is going to be pissed off so

00:13:36,490 --> 00:13:42,910
the problem is that spark is really cool

00:13:39,820 --> 00:13:46,029
but it's not a compiler it cannot see

00:13:42,910 --> 00:13:48,279
into the future oh I had a Marty McFly

00:13:46,029 --> 00:13:51,010
image that I wanted to put here but we

00:13:48,279 --> 00:13:53,170
we can't see into the future and so what

00:13:51,010 --> 00:13:56,110
this means is that when we when we go

00:13:53,170 --> 00:13:58,270
back here spark can you can see here

00:13:56,110 --> 00:14:00,459
right we we ask it to save this result

00:13:58,270 --> 00:14:02,350
out and sparks optimizer can see all of

00:14:00,459 --> 00:14:04,450
the things we've asked it to do so far

00:14:02,350 --> 00:14:06,670
but it doesn't know that we're going to

00:14:04,450 --> 00:14:08,500
immediately try and do something else

00:14:06,670 --> 00:14:10,420
with the same data so when we were gonna

00:14:08,500 --> 00:14:13,570
use data multiple times we have to help

00:14:10,420 --> 00:14:16,360
spark out by explicitly caching this

00:14:13,570 --> 00:14:18,490
stuff it's pretty easy we just we just

00:14:16,360 --> 00:14:20,560
persist and we tell spark ok I want to

00:14:18,490 --> 00:14:25,150
use this data twice my own spark it's

00:14:20,560 --> 00:14:27,670
like oh awesome thanks okay the other

00:14:25,150 --> 00:14:30,370
problem is we might have key skew with

00:14:27,670 --> 00:14:33,010
our data and if we were doing this with

00:14:30,370 --> 00:14:34,270
like log files we probably I mean if

00:14:33,010 --> 00:14:35,830
your code is anything like mine you

00:14:34,270 --> 00:14:38,709
would have a key skew for the word error

00:14:35,830 --> 00:14:40,180
or the word warning if you're lucky you

00:14:38,709 --> 00:14:42,390
might have key skew for the word pass

00:14:40,180 --> 00:14:45,040
like that that would be really exciting

00:14:42,390 --> 00:14:47,220
but if you had humans you might have key

00:14:45,040 --> 00:14:49,779
skew for ZIP codes or postal codes

00:14:47,220 --> 00:14:52,570
because it turns out humans we cluster

00:14:49,779 --> 00:14:55,330
in cities and we do these inconvenient

00:14:52,570 --> 00:14:58,000
things if you have like data about

00:14:55,330 --> 00:15:00,459
computers it's probably clustered around

00:14:58,000 --> 00:15:03,310
something called null I don't know why

00:15:00,459 --> 00:15:05,829
but null is very popular this season it

00:15:03,310 --> 00:15:08,920
is definitely the new black and so the

00:15:05,829 --> 00:15:11,620
problem is that key skew can make spark

00:15:08,920 --> 00:15:13,839
behave in in not-so-great ways right

00:15:11,620 --> 00:15:15,670
and at the end of the day this is

00:15:13,839 --> 00:15:18,399
because the magic magic isn't perfect

00:15:15,670 --> 00:15:19,720
and so essentially spark tries to split

00:15:18,399 --> 00:15:21,700
up our data amongst these different

00:15:19,720 --> 00:15:23,260
workers that we were showing earlier but

00:15:21,700 --> 00:15:25,660
part of how this happens is it tries to

00:15:23,260 --> 00:15:28,060
do this by key so if we remember when we

00:15:25,660 --> 00:15:29,170
were creating these word count pairs it

00:15:28,060 --> 00:15:30,940
was going to be using that word

00:15:29,170 --> 00:15:32,050
information to try and split up the data

00:15:30,940 --> 00:15:33,340
on two different machines

00:15:32,050 --> 00:15:38,050
so that it can do this

00:15:33,340 --> 00:15:40,870
processing efficiently so group by ki is

00:15:38,050 --> 00:15:43,960
part of our problem here it is pretty

00:15:40,870 --> 00:15:46,420
evil and the the part which makes me

00:15:43,960 --> 00:15:48,550
really sad about group by ki is that it

00:15:46,420 --> 00:15:50,140
sounds really safe especially if you

00:15:48,550 --> 00:15:51,730
come from a database background or

00:15:50,140 --> 00:15:53,410
something like that right like you group

00:15:51,730 --> 00:15:54,850
your tape either together by key and

00:15:53,410 --> 00:15:56,710
then you compute some aggregate

00:15:54,850 --> 00:15:59,230
statistics over it and that's normal

00:15:56,710 --> 00:16:02,920
right or at least I think that's normal

00:15:59,230 --> 00:16:06,760
but in SPARC it's actually evil as evil

00:16:02,920 --> 00:16:10,270
as this cat and so what happens is that

00:16:06,760 --> 00:16:11,740
once again SPARC can't see in a place

00:16:10,270 --> 00:16:14,080
where you might expect it to be able to

00:16:11,740 --> 00:16:16,600
see and optimize this problem away for

00:16:14,080 --> 00:16:20,860
us so when we call that group by key on

00:16:16,600 --> 00:16:23,590
that RTD SPARC creates this giant list

00:16:20,860 --> 00:16:25,570
of all of the records right or sorry for

00:16:23,590 --> 00:16:28,870
each key it creates a giant list of all

00:16:25,570 --> 00:16:31,480
of the values and then it applies the

00:16:28,870 --> 00:16:33,490
sum function afterwards right and the

00:16:31,480 --> 00:16:35,290
problem is it doesn't see inside of the

00:16:33,490 --> 00:16:37,600
lambda expressions that we give it it

00:16:35,290 --> 00:16:39,640
can optimize sort of on the flow of the

00:16:37,600 --> 00:16:41,620
different transformations but it can't

00:16:39,640 --> 00:16:43,330
see inside of our lambdas to optimize

00:16:41,620 --> 00:16:47,500
them so we have to help it out a little

00:16:43,330 --> 00:16:49,900
bit more oh yeah here's here's what it

00:16:47,500 --> 00:16:52,210
looks like this is if we pretend we're

00:16:49,900 --> 00:16:53,680
in San Francisco I've decided I want to

00:16:52,210 --> 00:16:55,300
get out of technology and I want to

00:16:53,680 --> 00:16:58,120
start an artisanal moustache wax shop

00:16:55,300 --> 00:17:00,310
and so nine four one one zero is the

00:16:58,120 --> 00:17:01,990
Mission District in San Francisco there

00:17:00,310 --> 00:17:03,970
are a lot of hipsters it would be a good

00:17:01,990 --> 00:17:06,280
place to open a moustache wax shop in

00:17:03,970 --> 00:17:08,740
fact there are so many hipsters that the

00:17:06,280 --> 00:17:10,480
number of records of these hipsters will

00:17:08,740 --> 00:17:12,580
crash my computer when I try and make my

00:17:10,480 --> 00:17:15,880
marketing merge and then I'll be very

00:17:12,580 --> 00:17:17,950
sad but that's that's ok with hipsters

00:17:15,880 --> 00:17:20,500
and their moustache wax there's actually

00:17:17,950 --> 00:17:25,110
surprisingly few moustaches for how good

00:17:20,500 --> 00:17:27,790
your coffee is I'm very impressed right

00:17:25,110 --> 00:17:30,100
so so what we actually do in spark

00:17:27,790 --> 00:17:31,750
instead of doing group by key is we use

00:17:30,100 --> 00:17:33,760
this other primitive that spark provides

00:17:31,750 --> 00:17:36,130
us called reduce by key and the

00:17:33,760 --> 00:17:38,230
difference is essentially instead of

00:17:36,130 --> 00:17:41,230
telling spark make this giant list and

00:17:38,230 --> 00:17:43,060
then compute the summation we tell spark

00:17:41,230 --> 00:17:45,120
as you find things which have the same

00:17:43,060 --> 00:17:46,600
key this is the rule that you can apply

00:17:45,120 --> 00:17:48,460
to compute

00:17:46,600 --> 00:17:50,620
the summation or whatever aggregate

00:17:48,460 --> 00:17:53,590
statistic I'm interested in as we're

00:17:50,620 --> 00:17:55,179
going and so we can use reduce by key

00:17:53,590 --> 00:17:57,100
and then our word count is safe and you

00:17:55,179 --> 00:17:59,259
know we've got this nice five line

00:17:57,100 --> 00:18:02,230
example that we can claim as better than

00:17:59,259 --> 00:18:05,830
a hundred lines of Java code which once

00:18:02,230 --> 00:18:07,509
again low bar but you know it's not we

00:18:05,830 --> 00:18:12,880
have this really safe reduce by key

00:18:07,509 --> 00:18:15,549
example here's an example that you can't

00:18:12,880 --> 00:18:17,230
read but I assure you this is on

00:18:15,549 --> 00:18:19,330
kilobytes of data because I wanted this

00:18:17,230 --> 00:18:21,730
to actually succeed but we've got some

00:18:19,330 --> 00:18:24,070
input data and my shuffled write is

00:18:21,730 --> 00:18:26,409
larger than my input and when we do this

00:18:24,070 --> 00:18:28,179
with reduced by key what happens is we

00:18:26,409 --> 00:18:30,039
have some input and then our shuffled

00:18:28,179 --> 00:18:32,259
write is actually smaller because what

00:18:30,039 --> 00:18:35,350
happens is as spark is processing the

00:18:32,259 --> 00:18:37,750
data locally on each machine it sort of

00:18:35,350 --> 00:18:39,419
applies this reduction before it even

00:18:37,750 --> 00:18:41,740
has to send the data across the network

00:18:39,419 --> 00:18:44,710
so each machine is responsible for a

00:18:41,740 --> 00:18:46,539
piece of the input data and then for

00:18:44,710 --> 00:18:48,909
each of the words we reduce that count

00:18:46,539 --> 00:18:54,570
down in advance and then we we shuffle

00:18:48,909 --> 00:18:56,799
it around afterwards to do right so

00:18:54,570 --> 00:18:58,809
whenever you find yourself using group

00:18:56,799 --> 00:19:01,000
by key inside of spark just use reduce

00:18:58,809 --> 00:19:03,340
by key or aggregate by key your life

00:19:01,000 --> 00:19:09,250
will be a lot simpler and your jobs will

00:19:03,340 --> 00:19:11,139
succeed more frequently but so even even

00:19:09,250 --> 00:19:13,090
though like even if we do all of those

00:19:11,139 --> 00:19:15,610
things right like our spark jobs will

00:19:13,090 --> 00:19:16,990
start to work right like we've we've got

00:19:15,610 --> 00:19:19,690
sparks and it's going to be in a good

00:19:16,990 --> 00:19:22,929
place but we're still not gonna be as

00:19:19,690 --> 00:19:24,429
fast as the Scala code that some other

00:19:22,929 --> 00:19:26,289
people are gonna be writing on spark

00:19:24,429 --> 00:19:28,090
right like we're gonna succeed on large

00:19:26,289 --> 00:19:31,210
data sets but we're not gonna be like

00:19:28,090 --> 00:19:32,950
amazing like it would be a meets

00:19:31,210 --> 00:19:37,330
expectations instead of exceeds

00:19:32,950 --> 00:19:39,279
expectations so the challenge is even

00:19:37,330 --> 00:19:41,620
when we do our reduced by key we have to

00:19:39,279 --> 00:19:46,360
copy data from the JVM to Python and

00:19:41,620 --> 00:19:48,370
back really frequently instead we can

00:19:46,360 --> 00:19:50,250
use Sparx data frames which while not

00:19:48,370 --> 00:19:53,080
pandas dataframes

00:19:50,250 --> 00:19:55,059
do really awesome things for us in that

00:19:53,080 --> 00:19:58,149
they take all of the operations that we

00:19:55,059 --> 00:19:59,890
ask spark to do that it can understand

00:19:58,149 --> 00:20:01,930
really well and it

00:19:59,890 --> 00:20:03,970
just compiles them down to sort of JVM

00:20:01,930 --> 00:20:05,650
execution plans and this is really

00:20:03,970 --> 00:20:07,990
convenient because it means our data

00:20:05,650 --> 00:20:10,030
gets to live inside of the JVM without

00:20:07,990 --> 00:20:11,980
having their copied to Python and now

00:20:10,030 --> 00:20:13,270
this is this might sound sad to you this

00:20:11,980 --> 00:20:16,150
might be like making you cry on the

00:20:13,270 --> 00:20:18,310
inside but it makes your job run a lot

00:20:16,150 --> 00:20:20,350
faster so even though like our data

00:20:18,310 --> 00:20:25,030
isn't living a side of Python it's it's

00:20:20,350 --> 00:20:28,330
possibly worth it yeah right so so

00:20:25,030 --> 00:20:30,190
essentially what happens is instead of

00:20:28,330 --> 00:20:32,020
having a lambda expression where we put

00:20:30,190 --> 00:20:34,690
whatever we want we have to write things

00:20:32,020 --> 00:20:36,520
inside of this little DSL and when we

00:20:34,690 --> 00:20:39,190
write it in that DSL the operation can

00:20:36,520 --> 00:20:42,190
happen purely inside of the JVM now now

00:20:39,190 --> 00:20:45,130
the nice thing about this is that we can

00:20:42,190 --> 00:20:46,690
do these parts first and then we can

00:20:45,130 --> 00:20:48,610
start using our lambda expressions when

00:20:46,690 --> 00:20:50,380
we want to get down into actually doing

00:20:48,610 --> 00:20:53,650
some really awesome scikit-learn stuff

00:20:50,380 --> 00:20:55,750
right so essentially if you've got a

00:20:53,650 --> 00:20:57,430
large data set right like you've got

00:20:55,750 --> 00:20:59,050
terabytes of data but then you have some

00:20:57,430 --> 00:21:01,420
filtering conditions or some other

00:20:59,050 --> 00:21:03,520
really simple things that spark can

00:21:01,420 --> 00:21:05,500
understand nicely for you you can have

00:21:03,520 --> 00:21:08,020
those pieces just evaluated purely

00:21:05,500 --> 00:21:09,400
inside of the JVM first and then once

00:21:08,020 --> 00:21:11,410
you've got it down to a more reasonable

00:21:09,400 --> 00:21:13,120
size you can start doing your operations

00:21:11,410 --> 00:21:15,850
in Python and you'll pay the

00:21:13,120 --> 00:21:20,080
serialization cost but it'll be it'll be

00:21:15,850 --> 00:21:21,190
comparatively fast there's a bunch of

00:21:20,080 --> 00:21:23,470
reasons for why they're good for

00:21:21,190 --> 00:21:26,290
performance since most people here are

00:21:23,470 --> 00:21:27,730
new to SPARC we'll just just trust me on

00:21:26,290 --> 00:21:29,140
this and if you don't you can go read

00:21:27,730 --> 00:21:30,820
this slide later but it's it's

00:21:29,140 --> 00:21:34,680
definitely fast and I have this

00:21:30,820 --> 00:21:37,150
benchmark which bigger numbers are bad

00:21:34,680 --> 00:21:39,520
unlike the previous slide where IBM had

00:21:37,150 --> 00:21:41,410
the big numbers those those big numbers

00:21:39,520 --> 00:21:43,660
were good these big numbers are bad

00:21:41,410 --> 00:21:46,510
these are execution time unless you're

00:21:43,660 --> 00:21:49,150
running on IBM SoftLayer in which case

00:21:46,510 --> 00:21:52,510
please use group by key I think we

00:21:49,150 --> 00:21:54,700
charge you by the hour right so we can

00:21:52,510 --> 00:21:58,720
see here you know reduced by key and

00:21:54,700 --> 00:22:01,930
Python performs better than group by key

00:21:58,720 --> 00:22:03,700
right like we actually succeed but if we

00:22:01,930 --> 00:22:06,010
do the same operation on data frames it

00:22:03,700 --> 00:22:07,330
runs a lot faster right because what's

00:22:06,010 --> 00:22:10,390
happening is these are actually being

00:22:07,330 --> 00:22:13,140
evaluated inside of the JVM and and we

00:22:10,390 --> 00:22:17,050
don't have to copy our data Python

00:22:13,140 --> 00:22:20,320
um yeah here's word count yet again I'm

00:22:17,050 --> 00:22:22,600
sorry if I don't put word count in every

00:22:20,320 --> 00:22:25,030
one of my examples they they take away

00:22:22,600 --> 00:22:27,940
my Big Data license and as a licensed

00:22:25,030 --> 00:22:29,590
Big Data professional I live in San

00:22:27,940 --> 00:22:32,500
Francisco without doing big data I

00:22:29,590 --> 00:22:34,840
really just can't afford the coffee it's

00:22:32,500 --> 00:22:36,820
it's a rough life it's a rough life

00:22:34,840 --> 00:22:39,220
thank you for laughing at my really

00:22:36,820 --> 00:22:42,550
corny jokes in the last slot I really

00:22:39,220 --> 00:22:44,440
appreciate it um so okay there's there's

00:22:42,550 --> 00:22:45,970
another option but we're running a bit

00:22:44,440 --> 00:22:50,760
low on time does anyone really like

00:22:45,970 --> 00:22:55,240
writing Java code to people okay cool um

00:22:50,760 --> 00:22:57,040
so find those two people and you can get

00:22:55,240 --> 00:22:58,930
them to write the the part of your

00:22:57,040 --> 00:23:01,480
python code which is taking way too long

00:22:58,930 --> 00:23:04,480
and then you can you can call it from

00:23:01,480 --> 00:23:06,100
Python inside of spark and and for those

00:23:04,480 --> 00:23:08,140
two people we'll talk later about how to

00:23:06,100 --> 00:23:11,980
make this happen but we're gonna we're

00:23:08,140 --> 00:23:14,560
gonna skip forward essentially the TLDR

00:23:11,980 --> 00:23:16,390
is you can peel back the covers and do a

00:23:14,560 --> 00:23:19,090
lot of terrible things with spark and

00:23:16,390 --> 00:23:20,440
Java if you really really want to but I

00:23:19,090 --> 00:23:23,800
don't think this is the audience that

00:23:20,440 --> 00:23:26,800
wants to do that so instead we're gonna

00:23:23,800 --> 00:23:30,010
talk about the future and the future is

00:23:26,800 --> 00:23:32,200
awesome in part because it doesn't have

00:23:30,010 --> 00:23:33,940
to exist today so it's a lot easier to

00:23:32,200 --> 00:23:36,970
tell you that things are gonna work well

00:23:33,940 --> 00:23:41,080
and believe it instead of things that

00:23:36,970 --> 00:23:44,080
happen today which I know are bad so the

00:23:41,080 --> 00:23:46,270
future for spark and python is is

00:23:44,080 --> 00:23:48,940
actually really exciting

00:23:46,270 --> 00:23:52,300
we recently integrated apache arrow has

00:23:48,940 --> 00:23:54,370
anyone heard about apache arrow come on

00:23:52,300 --> 00:23:56,200
hands to people there's got to be more

00:23:54,370 --> 00:23:57,790
people than that okay apache arrow is

00:23:56,200 --> 00:24:00,970
really awesome it's what's mckinney and

00:23:57,790 --> 00:24:03,430
a lot of really cool people it turns out

00:24:00,970 --> 00:24:06,070
finance has a lot of money and they make

00:24:03,430 --> 00:24:07,590
kind of cool software sometimes but so

00:24:06,070 --> 00:24:10,510
we're starting to use Apache arrow

00:24:07,590 --> 00:24:13,600
inside of spark to accelerate the

00:24:10,510 --> 00:24:16,000
interconnect between Java and Python so

00:24:13,600 --> 00:24:18,550
that this serialization cost will go

00:24:16,000 --> 00:24:20,200
down and that whenever we had need to

00:24:18,550 --> 00:24:22,510
write our arbitrary lambda expressions

00:24:20,200 --> 00:24:24,320
they can go really fast even when we're

00:24:22,510 --> 00:24:27,309
working in Python

00:24:24,320 --> 00:24:30,020
and so it's gonna get better I promise

00:24:27,309 --> 00:24:31,400
you still have to use reduce by key you

00:24:30,020 --> 00:24:34,640
can't use group by key that's still

00:24:31,400 --> 00:24:34,970
always gonna suck but it's gonna get

00:24:34,640 --> 00:24:38,809
better

00:24:34,970 --> 00:24:41,840
um if anyone is willing to share their

00:24:38,809 --> 00:24:43,480
UDF's in Python with me for benchmarking

00:24:41,840 --> 00:24:46,429
purposes I would really appreciate it

00:24:43,480 --> 00:24:48,380
people really just don't believe me when

00:24:46,429 --> 00:24:50,510
I keep giving them word count benchmarks

00:24:48,380 --> 00:24:51,860
and telling them that it's definitely

00:24:50,510 --> 00:24:53,600
worth it and we should integrate these

00:24:51,860 --> 00:24:56,720
changes that I thought up with over the

00:24:53,600 --> 00:24:58,429
weekend it helps when I can get

00:24:56,720 --> 00:25:01,250
real-world use cases so I would really

00:24:58,429 --> 00:25:02,630
if anyone has who sorry real-world spark

00:25:01,250 --> 00:25:05,210
use cases you're willing to share with

00:25:02,630 --> 00:25:08,929
me it can be from a hotmail account I

00:25:05,210 --> 00:25:11,540
won't ask a lot of questions just please

00:25:08,929 --> 00:25:13,340
don't send me something from the NSA

00:25:11,540 --> 00:25:15,470
from a hotmail account I don't want to

00:25:13,340 --> 00:25:18,200
spend the rest of my life in prison

00:25:15,470 --> 00:25:19,640
but yeah so yeah send me here UDS for

00:25:18,200 --> 00:25:21,320
benchmarking unless you work for a

00:25:19,640 --> 00:25:23,390
three-letter agency or whatever the

00:25:21,320 --> 00:25:25,990
Australian version of a three-layer

00:25:23,390 --> 00:25:29,480
agency is maybe a fourth or whatever um

00:25:25,990 --> 00:25:31,580
okay there's some spark resources ooh

00:25:29,480 --> 00:25:32,840
actually this is the most important part

00:25:31,580 --> 00:25:34,850
this is the part where I tell you to buy

00:25:32,840 --> 00:25:36,830
books um

00:25:34,850 --> 00:25:38,900
as mentioned I'm a co-author of many

00:25:36,830 --> 00:25:40,970
spark books and I have a new one out

00:25:38,900 --> 00:25:43,160
where I negotiated the royalties because

00:25:40,970 --> 00:25:45,590
I realized you could do that after I did

00:25:43,160 --> 00:25:49,460
that three times I'm not very smart um

00:25:45,590 --> 00:25:50,750
if you think spark is really cool I have

00:25:49,460 --> 00:25:54,830
this book it's called high-performance

00:25:50,750 --> 00:25:56,780
spark you can buy it I get money if

00:25:54,830 --> 00:26:00,140
you're completely new to spark Denny has

00:25:56,780 --> 00:26:01,640
this really awesome book and Tomas as

00:26:00,140 --> 00:26:03,049
well I don't get any money from this

00:26:01,640 --> 00:26:05,540
book but it's really good it's called

00:26:03,049 --> 00:26:07,340
learning PI spark if you buy that one

00:26:05,540 --> 00:26:09,799
though please buy my other book too so I

00:26:07,340 --> 00:26:16,610
can get some money Coffee is expensive

00:26:09,799 --> 00:26:19,100
in San Francisco if anyone really wants

00:26:16,610 --> 00:26:20,600
to chat about spark you can just DM me

00:26:19,100 --> 00:26:22,160
on Twitter I don't think there's gonna

00:26:20,600 --> 00:26:24,530
be a lot of people but we can find some

00:26:22,160 --> 00:26:26,510
time to chat otherwise I'm just gonna go

00:26:24,530 --> 00:26:29,480
buy some cute stuffed animals for my

00:26:26,510 --> 00:26:32,179
wife after this talk is finished but

00:26:29,480 --> 00:26:36,150
yeah so does anyone have questions or

00:26:32,179 --> 00:26:41,550
like can I just go sleep in the bathtub

00:26:36,150 --> 00:26:42,090
it's been a long day thanks for a great

00:26:41,550 --> 00:26:51,480
talk

00:26:42,090 --> 00:26:55,770
it's your Python mug does anybody have

00:26:51,480 --> 00:26:59,280
questions ok I have a question which is

00:26:55,770 --> 00:27:00,930
about arrow because this is because I

00:26:59,280 --> 00:27:03,060
don't know I'm surprised that very few

00:27:00,930 --> 00:27:06,150
people raise their hands about arrow

00:27:03,060 --> 00:27:07,950
because I mean maybe people who use

00:27:06,150 --> 00:27:11,340
pandas know it better as feather and

00:27:07,950 --> 00:27:12,960
that is kind of like the holy grail for

00:27:11,340 --> 00:27:14,340
people and bioinformatics and

00:27:12,960 --> 00:27:17,250
computational biology because we have to

00:27:14,340 --> 00:27:20,190
interact with our users and our code and

00:27:17,250 --> 00:27:22,860
getting stuff back and forth so is that

00:27:20,190 --> 00:27:24,450
gonna make this stuff really easy like

00:27:22,860 --> 00:27:26,340
can we will we be able to just like dump

00:27:24,450 --> 00:27:28,290
stuff in feather and work with it as if

00:27:26,340 --> 00:27:31,440
it were feather and then do this stuff

00:27:28,290 --> 00:27:35,010
with it yeah so the the goal of this

00:27:31,440 --> 00:27:37,800
integration is that it's going to break

00:27:35,010 --> 00:27:40,230
down the barrier between Java Python and

00:27:37,800 --> 00:27:41,850
R and we can all just live together in

00:27:40,230 --> 00:27:44,460
this happy space where we pretend that

00:27:41,850 --> 00:27:46,710
JVM doesn't exist and write really

00:27:44,460 --> 00:27:48,120
awesome distributed systems code and we

00:27:46,710 --> 00:27:50,130
can just like switch between these

00:27:48,120 --> 00:27:52,110
things and if you if you really like the

00:27:50,130 --> 00:27:54,570
JVM say this is being recorded

00:27:52,110 --> 00:27:56,760
I like the JVM - it's fine you can

00:27:54,570 --> 00:28:01,980
pretend are doesn't exist I don't really

00:27:56,760 --> 00:28:02,970
care but yes the goal is that all of

00:28:01,980 --> 00:28:05,340
these things are gonna actually start

00:28:02,970 --> 00:28:08,040
fitting together nicely and the

00:28:05,340 --> 00:28:11,210
integration is finally like it's not

00:28:08,040 --> 00:28:14,130
finished but it's like I mean right now

00:28:11,210 --> 00:28:16,770
if you have any stuff with date times

00:28:14,130 --> 00:28:18,870
inside of it it's it's terrible but like

00:28:16,770 --> 00:28:24,200
if you pretend that you don't have any

00:28:18,870 --> 00:28:27,590
time series data it totally almost works

00:28:24,200 --> 00:28:27,590
other questions

00:28:31,200 --> 00:28:35,270
um I think you had a bullet point in

00:28:33,870 --> 00:28:37,920
there that referenced something about

00:28:35,270 --> 00:28:39,840
spark running on yarn and some kind of a

00:28:37,920 --> 00:28:42,450
limitation I was wondering if you could

00:28:39,840 --> 00:28:44,690
go into that or maybe I misread it oh um

00:28:42,450 --> 00:28:47,370
there are a bunch of interesting

00:28:44,690 --> 00:28:48,600
features which happen when I don't

00:28:47,370 --> 00:28:50,790
actually know where that bullet point is

00:28:48,600 --> 00:28:52,620
but yeah we can talk about it anyways

00:28:50,790 --> 00:28:54,950
I'm gonna put this slide up because it's

00:28:52,620 --> 00:28:57,810
got a picture of one button

00:28:54,950 --> 00:29:01,980
so yes bark on yarn has some challenges

00:28:57,810 --> 00:29:04,980
with Python and this is that yarn

00:29:01,980 --> 00:29:06,120
containers you have to split your memory

00:29:04,980 --> 00:29:08,670
you have to manage your memory

00:29:06,120 --> 00:29:10,980
essentially intelligently and spurt

00:29:08,670 --> 00:29:13,410
doesn't do that when it comes to Python

00:29:10,980 --> 00:29:15,750
we reserved 90% of the memory for the

00:29:13,410 --> 00:29:18,780
JVM and 10% of the memory for everything

00:29:15,750 --> 00:29:20,640
else and so if you're using Python all

00:29:18,780 --> 00:29:22,770
of your data has to fit in 10% of your

00:29:20,640 --> 00:29:25,500
container memory and that's kind of

00:29:22,770 --> 00:29:27,330
rough and but the nice thing is like

00:29:25,500 --> 00:29:30,750
there's this magic knob and you can just

00:29:27,330 --> 00:29:33,300
tune it and most people end up tuning it

00:29:30,750 --> 00:29:34,980
by experimentation also known as just

00:29:33,300 --> 00:29:41,550
binary search until your code starts

00:29:34,980 --> 00:29:43,200
working any other questions right oh

00:29:41,550 --> 00:29:45,350
let's thank Holden again for a great

00:29:43,200 --> 00:29:45,350
talk

00:29:48,109 --> 00:29:52,609

YouTube URL: https://www.youtube.com/watch?v=jGhju2bw3RQ


