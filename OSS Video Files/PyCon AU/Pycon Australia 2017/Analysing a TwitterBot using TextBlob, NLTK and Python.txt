Title: Analysing a TwitterBot using TextBlob, NLTK and Python
Publication date: 2017-08-05
Playlist: Pycon Australia 2017
Description: 
	Lachlan Blackhall

http://2017.pycon-au.org/schedule/presentation/5/

#pyconau

This talk was given at PyCon Australia 2017 which was held from 3-8 August, 2017 in Melbourne, Victoria.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2017, we're returning to Melbourne, bringing together students, enthusiasts, and professionals with a love of Python from around Australia, and from all over the World. 

August 3-8 2017, Melbourne, Victoria

Python, PyCon, PyConAU
Captions: 
	00:00:01,879 --> 00:00:09,769
hello thank you for coming to this the

00:00:05,960 --> 00:00:14,960
after lunch session or pikin on a

00:00:09,769 --> 00:00:17,369
Saturday speaker today we'll talk about

00:00:14,960 --> 00:00:21,859
analyzing a Twitter bot using text block

00:00:17,369 --> 00:00:24,960
text blob NL TK and Python and he's

00:00:21,859 --> 00:00:28,439
Lachlan black cow he's the co-founder

00:00:24,960 --> 00:00:30,990
CTO of repositories and Australian

00:00:28,439 --> 00:00:34,710
technology company that does things with

00:00:30,990 --> 00:00:38,790
meters and electricity and he's also a

00:00:34,710 --> 00:00:40,739
Python user of all evolved and one of

00:00:38,790 --> 00:00:42,570
the organizers of this conference so

00:00:40,739 --> 00:00:44,010
please give a hand and a welcome to

00:00:42,570 --> 00:00:50,340
Lachlan black hole

00:00:44,010 --> 00:00:53,610
oh good afternoon everyone I hope you

00:00:50,340 --> 00:00:55,469
had a nice break over lunch it's a real

00:00:53,610 --> 00:00:59,280
pleasure to be here to give a talk as

00:00:55,469 --> 00:01:01,320
always and I'm gonna give you a talk in

00:00:59,280 --> 00:01:04,409
an area in which I have absolutely no

00:01:01,320 --> 00:01:06,810
expertise so what I'd like to do is I

00:01:04,409 --> 00:01:09,930
sort of take you through the the topic

00:01:06,810 --> 00:01:13,590
it's really sort of tell you a bit of a

00:01:09,930 --> 00:01:16,560
story about how I came to to be using

00:01:13,590 --> 00:01:17,850
text blob and how I came to be sort of

00:01:16,560 --> 00:01:19,470
investigating natural language

00:01:17,850 --> 00:01:22,409
processing which if you know my

00:01:19,470 --> 00:01:25,140
background it's quite quite theoretical

00:01:22,409 --> 00:01:27,000
and technical you'd realize that sort of

00:01:25,140 --> 00:01:32,280
language is not my primary area of

00:01:27,000 --> 00:01:35,280
strength so what what I'd like to talk

00:01:32,280 --> 00:01:37,049
about today is something called natural

00:01:35,280 --> 00:01:39,950
language processing most of you have

00:01:37,049 --> 00:01:42,689
probably heard about it if you haven't

00:01:39,950 --> 00:01:45,170
natural language processing is basically

00:01:42,689 --> 00:01:48,720
computational techniques for analyzing

00:01:45,170 --> 00:01:50,640
language and increasingly it's becoming

00:01:48,720 --> 00:01:52,950
more and more important that we

00:01:50,640 --> 00:01:54,899
understand this particularly as we start

00:01:52,950 --> 00:01:56,939
to talk about artificial intelligence

00:01:54,899 --> 00:01:58,469
and machines taking over the world and

00:01:56,939 --> 00:02:00,540
whatnot it's probably going to be a good

00:01:58,469 --> 00:02:01,710
idea that we have some way of

00:02:00,540 --> 00:02:03,990
understanding the computers and that

00:02:01,710 --> 00:02:10,080
hopefully the computers have some way of

00:02:03,990 --> 00:02:11,670
understanding us but where you tend to

00:02:10,080 --> 00:02:13,740
see a lot of the work being done at the

00:02:11,670 --> 00:02:15,780
moment particularly the research area is

00:02:13,740 --> 00:02:18,420
things around sort of conversational

00:02:15,780 --> 00:02:20,730
interfaces and so there's been a lot of

00:02:18,420 --> 00:02:24,330
talk about this in the last two to three

00:02:20,730 --> 00:02:26,070
years which is really focused on

00:02:24,330 --> 00:02:28,050
actually being able to have sort of

00:02:26,070 --> 00:02:32,090
text-based interactions with computers

00:02:28,050 --> 00:02:35,700
where computers can actually understand

00:02:32,090 --> 00:02:38,010
understand what it is we're trying to

00:02:35,700 --> 00:02:39,870
say to them and so my interest in this

00:02:38,010 --> 00:02:43,170
actually has come from sort of a

00:02:39,870 --> 00:02:44,720
background of I saw some you know some

00:02:43,170 --> 00:02:46,740
interesting articles about

00:02:44,720 --> 00:02:48,510
conversational user interfaces a few

00:02:46,740 --> 00:02:49,740
years ago and I thought that sounds

00:02:48,510 --> 00:02:51,540
really awesome like it'd be great to

00:02:49,740 --> 00:02:53,850
actually for computers to be able to

00:02:51,540 --> 00:02:55,710
actually understand users and

00:02:53,850 --> 00:02:56,940
particularly I'm a controls engineer so

00:02:55,710 --> 00:02:59,250
I work in sort of very very deep

00:02:56,940 --> 00:03:00,840
embedded systems it's like it would be

00:02:59,250 --> 00:03:02,310
really really interesting to be able to

00:03:00,840 --> 00:03:06,230
marry kind of those deep technical

00:03:02,310 --> 00:03:09,990
concepts that I work in and I love with

00:03:06,230 --> 00:03:12,620
consumers users being able to actually

00:03:09,990 --> 00:03:15,660
ask questions that we could then as a

00:03:12,620 --> 00:03:18,510
programmer sort of semantically pass and

00:03:15,660 --> 00:03:21,030
use to actually change or modify or even

00:03:18,510 --> 00:03:22,650
to explain to to the user what we're

00:03:21,030 --> 00:03:24,990
doing so that deep down in the guts of

00:03:22,650 --> 00:03:26,820
the system so that's really where my

00:03:24,990 --> 00:03:30,720
interest in natural language processing

00:03:26,820 --> 00:03:32,550
is kind of come from and what I'm what

00:03:30,720 --> 00:03:34,620
I'd like to do for you today is to sort

00:03:32,550 --> 00:03:37,350
of to tell you a little bit about a

00:03:34,620 --> 00:03:41,790
really interesting platform library that

00:03:37,350 --> 00:03:43,980
makes doing NLP stuff relatively simple

00:03:41,790 --> 00:03:52,200
and then take you through a bit of a

00:03:43,980 --> 00:03:54,660
side project I've been working on so in

00:03:52,200 --> 00:03:56,880
the in the title we said we're going to

00:03:54,660 --> 00:03:58,620
talk about Twitter bots so probably is

00:03:56,880 --> 00:04:03,540
helpful to start with what is a Twitter

00:03:58,620 --> 00:04:05,700
bot and I've chosen one of my my one of

00:04:03,540 --> 00:04:07,290
my favorites one of my wife's favorites

00:04:05,700 --> 00:04:09,720
which would become clear why that's

00:04:07,290 --> 00:04:13,170
relevant in a minute and it's called the

00:04:09,720 --> 00:04:14,220
magical realism bot and Twitter bots are

00:04:13,170 --> 00:04:17,520
essentially just you know they're

00:04:14,220 --> 00:04:19,650
automated pieces of code or automated

00:04:17,520 --> 00:04:22,440
systems that are that sit there and

00:04:19,650 --> 00:04:24,750
basically post off to Twitter and in

00:04:22,440 --> 00:04:27,690
this instance the magical realism bought

00:04:24,750 --> 00:04:30,230
it's pretty awesome it posts

00:04:27,690 --> 00:04:34,410
Queens which are the synopses of

00:04:30,230 --> 00:04:37,980
far-fetched stories and so these are

00:04:34,410 --> 00:04:40,500
only sort of a handful of the like three

00:04:37,980 --> 00:04:42,360
and a half or four thousand tweets that

00:04:40,500 --> 00:04:45,690
this Twitter boy has made over the last

00:04:42,360 --> 00:04:48,600
couple of years and they're absolutely

00:04:45,690 --> 00:04:50,550
wonderful and when you read through some

00:04:48,600 --> 00:04:53,010
of the tweets that have come out of this

00:04:50,550 --> 00:04:55,220
thing like it's there they're just

00:04:53,010 --> 00:04:57,810
flawless in the way that they have

00:04:55,220 --> 00:05:00,090
created these synopses for the stories

00:04:57,810 --> 00:05:01,620
and this this is really interesting to

00:05:00,090 --> 00:05:03,000
me because you've got this Twitter bot

00:05:01,620 --> 00:05:05,870
that every couple of hours spits out a

00:05:03,000 --> 00:05:08,280
new story that you know automatically

00:05:05,870 --> 00:05:10,230
makes sense like we can read this and

00:05:08,280 --> 00:05:11,610
you can understand it and you know you

00:05:10,230 --> 00:05:14,970
can see that it's sort of far-fetched

00:05:11,610 --> 00:05:17,970
and and fantastic but it it actually

00:05:14,970 --> 00:05:20,100
does make sense to us in English which

00:05:17,970 --> 00:05:24,570
is you know pretty amazing I thought

00:05:20,100 --> 00:05:27,060
from an automated process so the reason

00:05:24,570 --> 00:05:30,030
why I became a lot more interested in

00:05:27,060 --> 00:05:31,620
this was that my wife and I have

00:05:30,030 --> 00:05:34,380
discussed many many times how I should

00:05:31,620 --> 00:05:37,110
actually use my Python expertise to help

00:05:34,380 --> 00:05:38,760
her out with her side projects and so I

00:05:37,110 --> 00:05:41,610
sort of finally relented on this phone

00:05:38,760 --> 00:05:44,100
and this talk is also a lot about the

00:05:41,610 --> 00:05:45,510
hubris of a Python programmer she said

00:05:44,100 --> 00:05:46,950
to me one afternoon look I really I

00:05:45,510 --> 00:05:48,450
really love this this Twitter boy

00:05:46,950 --> 00:05:50,810
wouldn't it be great if I could also

00:05:48,450 --> 00:05:53,370
have a Twitter byte in you know the

00:05:50,810 --> 00:05:55,080
created stories in another area I said

00:05:53,370 --> 00:05:57,060
yeah I mean that'd be wonderful and we

00:05:55,080 --> 00:05:59,190
could use Python just totally reverse

00:05:57,060 --> 00:06:01,169
engineer the whole thing really simple

00:05:59,190 --> 00:06:02,760
I'll knock it off next week you'll be

00:06:01,169 --> 00:06:05,340
doing it we'll both be famous wouldn't

00:06:02,760 --> 00:06:07,280
that be great so this site you'll see

00:06:05,340 --> 00:06:09,990
through this talk that it's a it's been

00:06:07,280 --> 00:06:11,940
it's turned into much more of an epic

00:06:09,990 --> 00:06:14,580
side project than the sort of afternoon

00:06:11,940 --> 00:06:17,520
I thought it would take but ultimately

00:06:14,580 --> 00:06:19,800
this is all down to my wife l who sort

00:06:17,520 --> 00:06:22,650
of put me on the path to actually try to

00:06:19,800 --> 00:06:30,090
understand this Twitter bite in an

00:06:22,650 --> 00:06:34,320
automated way so and the reason that

00:06:30,090 --> 00:06:36,720
that I got really interested in sort of

00:06:34,320 --> 00:06:38,580
how you might approach understanding how

00:06:36,720 --> 00:06:40,960
these tweets are generated comes from

00:06:38,580 --> 00:06:44,039
the fact that the

00:06:40,960 --> 00:06:48,039
the author of the software that actually

00:06:44,039 --> 00:06:49,960
generates this at Yale Dora she's

00:06:48,039 --> 00:06:53,020
actually an Australian schoolteacher I

00:06:49,960 --> 00:06:58,000
believe based in Sydney and so she's

00:06:53,020 --> 00:07:00,550
been and her brother is the is the guy

00:06:58,000 --> 00:07:02,889
basically behind the magical realism boy

00:07:00,550 --> 00:07:05,320
and so both of them have been quite open

00:07:02,889 --> 00:07:07,240
about how they've gone about

00:07:05,320 --> 00:07:09,550
implementing it and it's also

00:07:07,240 --> 00:07:12,160
implemented in Python and from their

00:07:09,550 --> 00:07:14,320
tweet you can see and then from a sort

00:07:12,160 --> 00:07:16,060
of a snapshot of an article I've pasted

00:07:14,320 --> 00:07:17,380
it's you can understand that they're

00:07:16,060 --> 00:07:18,490
using nested templates and that makes

00:07:17,380 --> 00:07:20,830
that makes perfect sense

00:07:18,490 --> 00:07:23,110
all right so they've they're creating

00:07:20,830 --> 00:07:25,780
these sort of you know tweet size

00:07:23,110 --> 00:07:28,150
templates they're creating some form of

00:07:25,780 --> 00:07:30,310
word list and then they're populating

00:07:28,150 --> 00:07:32,860
their templates with the word list and

00:07:30,310 --> 00:07:34,000
my understanding from what I've sort of

00:07:32,860 --> 00:07:37,960
read and what I've seen when they've

00:07:34,000 --> 00:07:41,080
done QA is that essentially they don't

00:07:37,960 --> 00:07:42,430
really do any cleanup of what's produced

00:07:41,080 --> 00:07:44,830
so it's not like they're producing

00:07:42,430 --> 00:07:47,050
rubbish and then manually cleaning it up

00:07:44,830 --> 00:07:49,800
by hand my understanding is that they've

00:07:47,050 --> 00:07:53,199
actually their system is able to

00:07:49,800 --> 00:07:55,599
systematically fill in these templates

00:07:53,199 --> 00:07:58,840
that they have and ensure that you know

00:07:55,599 --> 00:08:01,750
tenses and singulars and verses plural

00:07:58,840 --> 00:08:03,220
is all is all is all handled so I

00:08:01,750 --> 00:08:04,389
thought that was pretty impressive in

00:08:03,220 --> 00:08:07,690
terms of the sort of the forward

00:08:04,389 --> 00:08:10,120
direction and we're trying to go now

00:08:07,690 --> 00:08:13,169
somewhat in the reverse direction to

00:08:10,120 --> 00:08:16,599
actually see if we can try and unearth

00:08:13,169 --> 00:08:18,820
the templates that they were using and

00:08:16,599 --> 00:08:20,199
the reason that the reason that that I

00:08:18,820 --> 00:08:22,449
thought it would be you know would be

00:08:20,199 --> 00:08:25,270
something that we could do yeah easily

00:08:22,449 --> 00:08:26,919
in paint pie thing is that you know

00:08:25,270 --> 00:08:28,270
where you have someone's lying structure

00:08:26,919 --> 00:08:30,039
I mean that's what computers are really

00:08:28,270 --> 00:08:32,380
good and you know it's programmers we

00:08:30,039 --> 00:08:33,640
often try to discover the underlying

00:08:32,380 --> 00:08:35,829
structure so I thought that would be

00:08:33,640 --> 00:08:38,110
that's why I thought it'd be a nice sort

00:08:35,829 --> 00:08:39,579
of Sunday afternoon project and if you

00:08:38,110 --> 00:08:46,360
brownie points then obviously get famous

00:08:39,579 --> 00:08:49,540
with our own Twitter bot so the first

00:08:46,360 --> 00:08:51,880
sort of the first step that we took to

00:08:49,540 --> 00:08:53,829
actually to start this was actually

00:08:51,880 --> 00:08:54,790
pretty mundanes actually just grab the

00:08:53,829 --> 00:08:56,980
tweets

00:08:54,790 --> 00:09:01,120
and so it's really easy to retrieve

00:08:56,980 --> 00:09:04,720
tweets from Twitter using Tweety or type

00:09:01,120 --> 00:09:06,820
I and there's this really nice gist that

00:09:04,720 --> 00:09:08,560
actually basically sets it out for you

00:09:06,820 --> 00:09:10,269
so collecting the tweets actually

00:09:08,560 --> 00:09:11,560
relatively simple you can't collect all

00:09:10,269 --> 00:09:14,920
of them in the manner that they describe

00:09:11,560 --> 00:09:18,459
here but you can collect about 2600 ad

00:09:14,920 --> 00:09:20,949
ago which was plenty for sort of what I

00:09:18,459 --> 00:09:23,670
was looking to do with it so grabbing

00:09:20,949 --> 00:09:29,820
the tweets putting them into a CSV file

00:09:23,670 --> 00:09:29,820
that was all that was all pretty simple

00:09:30,480 --> 00:09:35,170
so the next step was you know I always

00:09:33,820 --> 00:09:38,350
thought of this as a natural language

00:09:35,170 --> 00:09:41,139
processing type problem and so I've been

00:09:38,350 --> 00:09:44,230
really looking for an excuse to use a

00:09:41,139 --> 00:09:46,449
Python package called text blob for some

00:09:44,230 --> 00:09:48,160
time so like this is a perfect perfect

00:09:46,449 --> 00:09:52,120
example of where we could start to use

00:09:48,160 --> 00:09:56,139
text blog now text blog is essentially a

00:09:52,120 --> 00:09:59,380
Python wrapper around an LT K so ml TK

00:09:56,139 --> 00:10:01,630
is the natural language toolkit it was

00:09:59,380 --> 00:10:03,459
developed by a couple of professors at

00:10:01,630 --> 00:10:06,639
the University of Pennsylvania and it's

00:10:03,459 --> 00:10:09,130
the first release was in 2001 and so

00:10:06,639 --> 00:10:10,269
it's you know it's it's very

00:10:09,130 --> 00:10:13,420
well-regarded

00:10:10,269 --> 00:10:15,699
in terms of the natural language

00:10:13,420 --> 00:10:16,990
processing that it can do and has

00:10:15,699 --> 00:10:19,269
obviously been developed over many many

00:10:16,990 --> 00:10:22,360
years and so text blob is essentially a

00:10:19,269 --> 00:10:24,639
wrapper around ltk it just makes it a

00:10:22,360 --> 00:10:26,440
little bit easier to use some of the

00:10:24,639 --> 00:10:28,769
underlying methods in it it sort of

00:10:26,440 --> 00:10:31,449
shortcuts the process of setting up

00:10:28,769 --> 00:10:34,300
certain things within natural language

00:10:31,449 --> 00:10:36,279
processing so it's really really good

00:10:34,300 --> 00:10:38,260
actually text blobs are like a really

00:10:36,279 --> 00:10:39,190
it's a well-maintained library and it's

00:10:38,260 --> 00:10:42,149
very easy to wrap your head around

00:10:39,190 --> 00:10:44,740
what's actually going on there so

00:10:42,149 --> 00:10:46,930
installations easy these ones just worth

00:10:44,740 --> 00:10:48,279
touching on actually you do the PP

00:10:46,930 --> 00:10:51,550
install of text blob and that's fine

00:10:48,279 --> 00:10:54,670
that also grabs you some of the NL that

00:10:51,550 --> 00:10:57,399
grabs you VN ltk package as well which

00:10:54,670 --> 00:10:59,860
is also a Python package the way that

00:10:57,399 --> 00:11:07,250
natural language processing works is it

00:10:59,860 --> 00:11:09,670
typically uses a corpus of text or core

00:11:07,250 --> 00:11:13,189
the what's the plural something in Latin

00:11:09,670 --> 00:11:15,410
corpora there we go and so what that

00:11:13,189 --> 00:11:19,459
means is you end up with these large

00:11:15,410 --> 00:11:21,110
tagged text-based datasets and that's

00:11:19,459 --> 00:11:22,970
how when you start doing the analysis

00:11:21,110 --> 00:11:26,360
they're basically going back and forth

00:11:22,970 --> 00:11:29,029
into these into these tagged datasets to

00:11:26,360 --> 00:11:32,360
actually do the analysis so you can grab

00:11:29,029 --> 00:11:34,009
the software but you can't actually do

00:11:32,360 --> 00:11:36,709
much useful stuff until you actually

00:11:34,009 --> 00:11:38,839
download the corpora which is actually

00:11:36,709 --> 00:11:40,220
really easy to do so you type the

00:11:38,839 --> 00:11:45,019
command in and you get a whole bunch of

00:11:40,220 --> 00:11:46,550
stuff you can use either method there so

00:11:45,019 --> 00:11:49,220
textbook text blog has a wrapper around

00:11:46,550 --> 00:11:52,129
that the actual underlying else okay

00:11:49,220 --> 00:11:53,959
corporate downloader and if spaces are

00:11:52,129 --> 00:11:56,509
concerned you don't have to use all you

00:11:53,959 --> 00:11:58,519
can actually pull out different you know

00:11:56,509 --> 00:12:00,139
different corpora that that you actually

00:11:58,519 --> 00:12:04,389
need for the particular problem that

00:12:00,139 --> 00:12:11,720
you're working on so easy to get started

00:12:04,389 --> 00:12:14,180
and very very quick to download so just

00:12:11,720 --> 00:12:16,550
to just to give you an idea of some of

00:12:14,180 --> 00:12:19,189
the capabilities of what text blog can

00:12:16,550 --> 00:12:21,559
actually do you know ultimately we're

00:12:19,189 --> 00:12:23,959
talking about doing analysis on pieces

00:12:21,559 --> 00:12:25,399
of text so for everything we're going to

00:12:23,959 --> 00:12:28,250
talk about here I'm basically going to

00:12:25,399 --> 00:12:30,139
talk about the analysis on individual

00:12:28,250 --> 00:12:31,670
tweets and so we're going to redo all

00:12:30,139 --> 00:12:34,899
this analysis for each of the individual

00:12:31,670 --> 00:12:38,930
tweets in the 2600 that we downloaded

00:12:34,899 --> 00:12:42,319
you input text blob you get your your

00:12:38,930 --> 00:12:45,319
string you load it up into this text

00:12:42,319 --> 00:12:48,970
blob object and then you can sort of

00:12:45,319 --> 00:12:52,220
unleash its analytic capabilities so you

00:12:48,970 --> 00:12:54,559
can get the sentences that are there

00:12:52,220 --> 00:12:56,870
which probably doesn't sound super super

00:12:54,559 --> 00:12:59,269
useful but in lots of cases it is it can

00:12:56,870 --> 00:13:00,800
do language detection which I believe it

00:12:59,269 --> 00:13:02,240
actually does through interface into

00:13:00,800 --> 00:13:04,850
Google so that's probably only got you

00:13:02,240 --> 00:13:07,550
here most of the most of text blob runs

00:13:04,850 --> 00:13:08,870
offline but I found out the other day

00:13:07,550 --> 00:13:10,550
that if you're not online it does blow

00:13:08,870 --> 00:13:13,490
up because it actually does try to go to

00:13:10,550 --> 00:13:15,230
google for some things it can do word

00:13:13,490 --> 00:13:16,519
counts that can get noun phrases and

00:13:15,230 --> 00:13:19,639
we'll talk a bit about them in a minute

00:13:16,519 --> 00:13:20,810
and it can do this thing we call part of

00:13:19,639 --> 00:13:22,040
speech tagging

00:13:20,810 --> 00:13:23,680
and we're going to spend most of our

00:13:22,040 --> 00:13:26,780
time on part of speech tagging today

00:13:23,680 --> 00:13:30,620
because I think it's pretty incredible

00:13:26,780 --> 00:13:32,750
what you can do with it so just to break

00:13:30,620 --> 00:13:34,730
out what your what you'd expect for each

00:13:32,750 --> 00:13:36,650
of these so if you get the sentences it

00:13:34,730 --> 00:13:38,480
returns conveniently enough the

00:13:36,650 --> 00:13:42,050
sentences probably nothing unexpected

00:13:38,480 --> 00:13:43,010
there the nice thing is you know most of

00:13:42,050 --> 00:13:44,660
these tweets are only one or two

00:13:43,010 --> 00:13:46,640
sentences so you're probably not going

00:13:44,660 --> 00:13:49,850
to be using this kind of capability a

00:13:46,640 --> 00:13:52,580
lot there but when you end up with large

00:13:49,850 --> 00:13:55,580
numbers of sentences or a very very

00:13:52,580 --> 00:13:57,550
large corpus of text you might find it

00:13:55,580 --> 00:14:00,280
very very convenient to break it down

00:13:57,550 --> 00:14:02,990
into sentences to do your analysis so

00:14:00,280 --> 00:14:05,750
for example we're not going to talk

00:14:02,990 --> 00:14:08,150
about neural networks at all today but

00:14:05,750 --> 00:14:10,730
when you do your network analysis on

00:14:08,150 --> 00:14:13,760
text you typically want to feed it fixed

00:14:10,730 --> 00:14:16,730
length pieces of content and so one way

00:14:13,760 --> 00:14:19,010
you could do that is you might break it

00:14:16,730 --> 00:14:20,930
down into these sentences pad all the

00:14:19,010 --> 00:14:22,730
sentences to constant length and then

00:14:20,930 --> 00:14:24,500
you'd be able to feed it all straight

00:14:22,730 --> 00:14:27,860
into into when you're on there to do

00:14:24,500 --> 00:14:29,900
analysis that way so there are reasons

00:14:27,860 --> 00:14:31,880
why you would use a lot of these methods

00:14:29,900 --> 00:14:34,340
and you can do language detection so I

00:14:31,880 --> 00:14:38,510
worked out that was that we have English

00:14:34,340 --> 00:14:41,840
which is useful perhaps you can get the

00:14:38,510 --> 00:14:43,490
word counts from it and so you know

00:14:41,840 --> 00:14:46,610
given that every words unique here we

00:14:43,490 --> 00:14:49,340
get a dictionary of of unique of unique

00:14:46,610 --> 00:14:51,290
words but again over a very very large

00:14:49,340 --> 00:14:53,660
corpus of text it might be really

00:14:51,290 --> 00:14:55,130
interesting to you to understand what

00:14:53,660 --> 00:14:57,980
the common words are and I'll actually

00:14:55,130 --> 00:15:00,560
talk a little bit later about how I was

00:14:57,980 --> 00:15:01,970
able to analyze all of the the texts in

00:15:00,560 --> 00:15:04,130
this all of the tweets are in a single

00:15:01,970 --> 00:15:09,980
corpus and actually extracts and some

00:15:04,130 --> 00:15:11,900
some useful stuff from that so you can

00:15:09,980 --> 00:15:13,310
get into noun phrases so noun phrases

00:15:11,900 --> 00:15:15,170
are things like the subject and the

00:15:13,310 --> 00:15:16,730
object of a sentence and so text blob

00:15:15,170 --> 00:15:18,470
and NLT can actually pull them out

00:15:16,730 --> 00:15:19,730
relatively automatically for you so

00:15:18,470 --> 00:15:21,050
that's kind of interesting if you're

00:15:19,730 --> 00:15:22,880
looking at doing sort of semantic

00:15:21,050 --> 00:15:24,590
analysis and understanding what the

00:15:22,880 --> 00:15:26,030
person's actually what you know what's

00:15:24,590 --> 00:15:27,280
actually being asked in the sentence or

00:15:26,030 --> 00:15:32,360
what's being referenced in the sentence

00:15:27,280 --> 00:15:33,590
and then you can do tagging and so as I

00:15:32,360 --> 00:15:34,279
said this is what I'm going to focus on

00:15:33,590 --> 00:15:37,279
for the

00:15:34,279 --> 00:15:39,829
the talk so essentially what the tagging

00:15:37,279 --> 00:15:43,100
does is it tags for every word in a

00:15:39,829 --> 00:15:48,560
sentence or in a corpus it actually it

00:15:43,100 --> 00:15:54,249
tags it with the type of word it is so a

00:15:48,560 --> 00:15:58,129
is a determiner you know made is a verb

00:15:54,249 --> 00:16:00,740
Romans a proper noun and the interesting

00:15:58,129 --> 00:16:02,779
thing about why you'd want to use this

00:16:00,740 --> 00:16:04,759
kind of analysis is there's you know

00:16:02,779 --> 00:16:07,430
tens of thousands of words in the

00:16:04,759 --> 00:16:11,389
English language but the tagging turns

00:16:07,430 --> 00:16:14,329
out there's only about 30 unique tags

00:16:11,389 --> 00:16:18,170
that you can use to Tagil these pieces

00:16:14,329 --> 00:16:20,240
of speech and so this I thought was the

00:16:18,170 --> 00:16:22,160
correct attack vector to actually try

00:16:20,240 --> 00:16:24,709
and solve this problem of understanding

00:16:22,160 --> 00:16:27,439
what the Twitter bot was doing so you

00:16:24,709 --> 00:16:28,279
can see to here there's you know we

00:16:27,439 --> 00:16:31,279
would have you would have come across

00:16:28,279 --> 00:16:33,529
most of these parts of speech through

00:16:31,279 --> 00:16:35,779
your education but perhaps not quite as

00:16:33,529 --> 00:16:37,490
formally when you think of it being

00:16:35,779 --> 00:16:38,990
structured like this so this is

00:16:37,490 --> 00:16:42,019
something that's called the penn

00:16:38,990 --> 00:16:43,730
treebank part of speech tag set and this

00:16:42,019 --> 00:16:46,699
is what NLT Kay uses as its default

00:16:43,730 --> 00:16:52,120
tagger to actually go through and tag

00:16:46,699 --> 00:16:54,740
parts of speech so the question that I

00:16:52,120 --> 00:16:56,509
thought I thought would be interesting

00:16:54,740 --> 00:16:58,399
to answer and might actually allow us to

00:16:56,509 --> 00:17:02,000
really understand what was going on in

00:16:58,399 --> 00:17:03,439
this in this Twitter bot was to see if

00:17:02,000 --> 00:17:07,970
you could actually generate meaningful

00:17:03,439 --> 00:17:10,819
sentences using only structured parts of

00:17:07,970 --> 00:17:12,199
speech tagging so rather than like you

00:17:10,819 --> 00:17:13,579
know you'd go to a neural net and try

00:17:12,199 --> 00:17:15,199
and learn all these tweets and then

00:17:13,579 --> 00:17:17,390
hopefully your neuron that would be able

00:17:15,199 --> 00:17:18,799
to spit you out a tweet that's sort of

00:17:17,390 --> 00:17:20,600
similar to one of the tweets that seen

00:17:18,799 --> 00:17:23,059
before I thought no no let's let's not

00:17:20,600 --> 00:17:27,919
use a neural net let's see if we can

00:17:23,059 --> 00:17:31,669
actually go from a string or a sentence

00:17:27,919 --> 00:17:34,789
of parts of speech tagging back into a

00:17:31,669 --> 00:17:36,860
meaningful tweet in English so it's

00:17:34,789 --> 00:17:38,539
obviously easy to go the first direction

00:17:36,860 --> 00:17:40,940
from the sentence to the parts of speech

00:17:38,539 --> 00:17:42,830
tag but can you go in the reverse

00:17:40,940 --> 00:17:46,070
direction so that was my that was my

00:17:42,830 --> 00:17:47,510
hypothesis or the question that I set

00:17:46,070 --> 00:17:51,290
out to answer in

00:17:47,510 --> 00:17:55,700
in all of this so the way I approached

00:17:51,290 --> 00:17:58,630
it for every tweet went through pulled

00:17:55,700 --> 00:18:01,549
out the parts of speech tagging I

00:17:58,630 --> 00:18:03,320
created these parts of speech tagging

00:18:01,549 --> 00:18:04,790
sentences so this is obviously not a

00:18:03,320 --> 00:18:08,510
sentence it's meaningless but it's the

00:18:04,790 --> 00:18:11,750
it's the collection of parts of speech

00:18:08,510 --> 00:18:13,880
that constituted the original tweet that

00:18:11,750 --> 00:18:16,880
we started with and the second thing I

00:18:13,880 --> 00:18:20,720
did was I also created a corpus of words

00:18:16,880 --> 00:18:24,770
for each type of part of speech so as we

00:18:20,720 --> 00:18:27,380
go through this tweet we say well the

00:18:24,770 --> 00:18:28,760
first the first type of part of speech

00:18:27,380 --> 00:18:31,010
was a determiner so that goes in the

00:18:28,760 --> 00:18:33,140
first part of sentence and we also

00:18:31,010 --> 00:18:37,790
record that in this determiner

00:18:33,140 --> 00:18:39,890
dictionary we've seen the word ah so

00:18:37,790 --> 00:18:41,030
hopefully that that all makes sense

00:18:39,890 --> 00:18:42,980
that's what we're doing there so really

00:18:41,030 --> 00:18:44,510
all we're trying to do is rather than

00:18:42,980 --> 00:18:46,070
trying to analyze this Twitter bot from

00:18:44,510 --> 00:18:47,780
the perspective of analyzing the tweets

00:18:46,070 --> 00:18:50,030
we're trying to analyze it from the

00:18:47,780 --> 00:18:51,860
perspective of using the parts of speech

00:18:50,030 --> 00:18:53,030
that we're there and the reason I

00:18:51,860 --> 00:18:55,220
thought that that would be an

00:18:53,030 --> 00:18:56,660
interesting way to do it is that I

00:18:55,220 --> 00:18:59,750
thought the structure if you actually

00:18:56,660 --> 00:19:01,520
have a nested template that the template

00:18:59,750 --> 00:19:03,860
actually represents in lots of cases

00:19:01,520 --> 00:19:05,450
things like you know verbs and

00:19:03,860 --> 00:19:07,040
adjectives that you're drawing from a

00:19:05,450 --> 00:19:09,740
list and so rather than trying to deal

00:19:07,040 --> 00:19:10,850
with all of the words that are there you

00:19:09,740 --> 00:19:13,250
try and just deal with the parts of

00:19:10,850 --> 00:19:16,549
speech from which these templates were

00:19:13,250 --> 00:19:18,230
actually composed so that was my that

00:19:16,549 --> 00:19:23,080
was my my thought on how it would work

00:19:18,230 --> 00:19:25,549
so the second thing I did was I took

00:19:23,080 --> 00:19:27,500
these parts of speech sentences and I

00:19:25,549 --> 00:19:31,549
encoded them into something called a

00:19:27,500 --> 00:19:33,440
directed acyclic word graph using this

00:19:31,549 --> 00:19:35,179
and died this is not actually crucial to

00:19:33,440 --> 00:19:38,470
what's going but it is what what I did

00:19:35,179 --> 00:19:43,010
first so a directed acyclic word graph

00:19:38,470 --> 00:19:45,230
is simply what you see here on the right

00:19:43,010 --> 00:19:47,090
hand side it's that you for a given set

00:19:45,230 --> 00:19:51,230
of characters or a given set of symbols

00:19:47,090 --> 00:19:52,429
you you just encode it in a way that

00:19:51,230 --> 00:19:54,799
allows you to represent the possible

00:19:52,429 --> 00:19:56,540
choices here because remember I my goal

00:19:54,799 --> 00:19:58,070
here it was to reverse-engineer this

00:19:56,540 --> 00:19:59,390
Twitter bot for my wife so I actually

00:19:58,070 --> 00:20:01,220
needed to have this kind of like a

00:19:59,390 --> 00:20:03,710
minimal representation of all the straw

00:20:01,220 --> 00:20:05,120
and so I'm not doing it with the words

00:20:03,710 --> 00:20:08,030
I'm now doing it with the parts of

00:20:05,120 --> 00:20:12,470
speech and so this is actually a tree

00:20:08,030 --> 00:20:17,810
and this is the the dog I think which is

00:20:12,470 --> 00:20:19,370
an interesting pronunciation so I was

00:20:17,810 --> 00:20:20,960
like that's fine and then I was like

00:20:19,370 --> 00:20:23,570
once it's in a directed acyclic Word

00:20:20,960 --> 00:20:24,770
graph then like then it's really simple

00:20:23,570 --> 00:20:26,270
because once you've got that you can

00:20:24,770 --> 00:20:30,170
guarantee that you can produce a minimal

00:20:26,270 --> 00:20:32,750
regex of these parts of speech you can

00:20:30,170 --> 00:20:35,810
generate a random sentence of parts of

00:20:32,750 --> 00:20:37,250
speech that come out of this and I was

00:20:35,810 --> 00:20:38,810
like once you've got that then you can

00:20:37,250 --> 00:20:42,410
go back and for each part of speech you

00:20:38,810 --> 00:20:43,700
can pop in a word from your word list

00:20:42,410 --> 00:20:45,020
and I was like and it's going to be

00:20:43,700 --> 00:20:46,580
amazing like once we going to do this

00:20:45,020 --> 00:20:48,680
we're going to be able to totally

00:20:46,580 --> 00:20:49,850
reverse engineer this and we are going

00:20:48,680 --> 00:20:52,220
to be able to produce tweets that are

00:20:49,850 --> 00:20:54,500
totally independent of the original

00:20:52,220 --> 00:20:58,280
Twitter boy and so we got these amazing

00:20:54,500 --> 00:21:00,710
story synopsis from my from from the

00:20:58,280 --> 00:21:02,920
reverse-engineered approach and as you

00:21:00,710 --> 00:21:04,880
can tell they don't make a lot of sense

00:21:02,920 --> 00:21:07,130
except the last one here which i think

00:21:04,880 --> 00:21:11,750
is actually quite a deep a deep comment

00:21:07,130 --> 00:21:13,100
on society but with appropriate grammar

00:21:11,750 --> 00:21:18,020
I think he can make the last sentence

00:21:13,100 --> 00:21:20,600
actually make sense so the the thing

00:21:18,020 --> 00:21:23,510
this is I this this was the hubris part

00:21:20,600 --> 00:21:24,950
of it because I I suddenly realized of

00:21:23,510 --> 00:21:27,760
course which something we all know but

00:21:24,950 --> 00:21:30,050
of course English is is extraordinarily

00:21:27,760 --> 00:21:33,440
structured and it's very complex and it

00:21:30,050 --> 00:21:34,730
takes us ages to learn and the reason it

00:21:33,440 --> 00:21:37,340
takes so long to learn is there's so

00:21:34,730 --> 00:21:39,290
many different rules and exceptions to

00:21:37,340 --> 00:21:43,730
how you actually construct meaningful

00:21:39,290 --> 00:21:45,860
meaningful content and so you know the I

00:21:43,730 --> 00:21:48,110
suppose long story short to think that

00:21:45,860 --> 00:21:50,450
you could actually from 30 different

00:21:48,110 --> 00:21:54,800
parts of speech reverse-engineer and get

00:21:50,450 --> 00:21:57,520
some get sentences that made sense is

00:21:54,800 --> 00:21:59,510
kind of perhaps a little bit bold

00:21:57,520 --> 00:22:01,310
but nonetheless I wasn't deterred

00:21:59,510 --> 00:22:03,050
because particularly after I realized

00:22:01,310 --> 00:22:04,490
the laughs in the last sentence made

00:22:03,050 --> 00:22:06,500
sense I thought well look there's

00:22:04,490 --> 00:22:09,820
probably hope that we can actually and

00:22:06,500 --> 00:22:13,460
start to analyze this in a different way

00:22:09,820 --> 00:22:14,620
and and see what we can see what we can

00:22:13,460 --> 00:22:17,660
get out of it

00:22:14,620 --> 00:22:19,790
so it's pretty clear that's what's

00:22:17,660 --> 00:22:23,570
missing is structure here so we're

00:22:19,790 --> 00:22:26,780
choosing all these random words from a

00:22:23,570 --> 00:22:30,590
word list and so we you know we're we're

00:22:26,780 --> 00:22:32,960
we're choosing nouns against verbs and

00:22:30,590 --> 00:22:34,700
it might not necessarily they might not

00:22:32,960 --> 00:22:36,080
make sense like they're you know as

00:22:34,700 --> 00:22:37,220
individual words they're fine but once

00:22:36,080 --> 00:22:39,860
you start constructing them into

00:22:37,220 --> 00:22:42,620
sentences they make absolutely no sense

00:22:39,860 --> 00:22:44,120
and why hypothesis was that well maybe

00:22:42,620 --> 00:22:45,770
you actually need just a bit more

00:22:44,120 --> 00:22:49,850
structure here and then you can still

00:22:45,770 --> 00:22:52,010
you can still do this and so I started

00:22:49,850 --> 00:22:53,900
with you know this is an as an example

00:22:52,010 --> 00:22:59,540
tweet and then you know converted into

00:22:53,900 --> 00:23:02,390
this part of speech string but rather

00:22:59,540 --> 00:23:04,670
than just trying to generate a tweet

00:23:02,390 --> 00:23:06,500
directly from that I was able to use the

00:23:04,670 --> 00:23:08,030
word count feature I talked earlier to

00:23:06,500 --> 00:23:10,370
try and find common words and phrases

00:23:08,030 --> 00:23:13,550
that existed within this within all the

00:23:10,370 --> 00:23:14,870
tweets and so based on that I found that

00:23:13,550 --> 00:23:16,940
there were a lot of tweets that had this

00:23:14,870 --> 00:23:19,280
kind of structure here where there was

00:23:16,940 --> 00:23:22,610
like parts of speech and then made of

00:23:19,280 --> 00:23:24,230
and then another part of speech and so I

00:23:22,610 --> 00:23:25,850
thought well maybe this is the right way

00:23:24,230 --> 00:23:27,110
to do it this is the way of introducing

00:23:25,850 --> 00:23:31,130
some structure so it's there's not so

00:23:27,110 --> 00:23:33,320
much free form free form that we're

00:23:31,130 --> 00:23:35,980
trying to like free form words that

00:23:33,320 --> 00:23:38,900
we're trying to write to generate and so

00:23:35,980 --> 00:23:40,700
these were some of the some of the some

00:23:38,900 --> 00:23:43,730
of the tweets that we were able that

00:23:40,700 --> 00:23:45,320
started to emerge and so and you know

00:23:43,730 --> 00:23:48,020
all of these I mean they might seem

00:23:45,320 --> 00:23:49,100
ludicrous but then the corpus of words

00:23:48,020 --> 00:23:52,460
that they were drawn from was ludicrous

00:23:49,100 --> 00:23:54,680
to begin with but they do make sense in

00:23:52,460 --> 00:23:56,090
English and particularly the first one

00:23:54,680 --> 00:24:01,130
struck me because as an entrepreneur I

00:23:56,090 --> 00:24:05,390
often feel like I'm made of panic and so

00:24:01,130 --> 00:24:07,040
I suppose the the the what you start to

00:24:05,390 --> 00:24:09,680
come to realize is that these parts of

00:24:07,040 --> 00:24:12,620
speech some of them actually form really

00:24:09,680 --> 00:24:14,870
simple structures that you can that you

00:24:12,620 --> 00:24:17,960
can actually generate with relative ease

00:24:14,870 --> 00:24:20,630
and so theses actually are really common

00:24:17,960 --> 00:24:22,190
structuring particularly in these tweets

00:24:20,630 --> 00:24:25,130
but but in language as well where you

00:24:22,190 --> 00:24:26,900
have determinar adjective adjective or

00:24:25,130 --> 00:24:30,050
multiple adjectives and then a noun

00:24:26,900 --> 00:24:31,850
and so you can generate these at will

00:24:30,050 --> 00:24:33,500
almost a hundred percent of the ones you

00:24:31,850 --> 00:24:34,940
generate make sense

00:24:33,500 --> 00:24:36,950
and these were some of the ones that

00:24:34,940 --> 00:24:40,550
sort of started to emerge and I thought

00:24:36,950 --> 00:24:43,270
they were quite quite entertaining but I

00:24:40,550 --> 00:24:45,620
think the the overarching thing that I

00:24:43,270 --> 00:24:49,040
know that I unfortunately discovered is

00:24:45,620 --> 00:24:52,010
that there's not quite so simple a

00:24:49,040 --> 00:24:54,470
translation between these parts of

00:24:52,010 --> 00:24:57,500
speech and the ability to generate sort

00:24:54,470 --> 00:24:59,059
of arbitrary phrases and sentences that

00:24:57,500 --> 00:25:01,370
you'd want to actually be able to sort

00:24:59,059 --> 00:25:05,570
of perfectly reverse-engineer a Twitter

00:25:01,370 --> 00:25:07,280
bot like this so once I sort of accepted

00:25:05,570 --> 00:25:08,990
that I sort of got into this mindset of

00:25:07,280 --> 00:25:10,670
well how would you actually go about

00:25:08,990 --> 00:25:12,410
doing this practically in terms of

00:25:10,670 --> 00:25:14,210
reverse engineering it and it turns out

00:25:12,410 --> 00:25:16,309
that you know the reality is there's

00:25:14,210 --> 00:25:19,790
lots of chunks that look like this that

00:25:16,309 --> 00:25:21,920
in short you know four three four or

00:25:19,790 --> 00:25:23,720
five parts of speech it's really really

00:25:21,920 --> 00:25:27,260
easy to generate like actually

00:25:23,720 --> 00:25:30,170
meaningful content from them but you do

00:25:27,260 --> 00:25:32,410
have to find these structural pivots and

00:25:30,170 --> 00:25:35,630
those pivots would have been the things

00:25:32,410 --> 00:25:38,150
in the underlying templates like made of

00:25:35,630 --> 00:25:40,040
we're made of is actually always in the

00:25:38,150 --> 00:25:44,240
template and it's the other parts that

00:25:40,040 --> 00:25:49,520
are generated automatically by this by

00:25:44,240 --> 00:25:50,540
this Twitter bot so unfortunately the

00:25:49,520 --> 00:25:51,980
work on this is going to have to

00:25:50,540 --> 00:25:54,470
continue

00:25:51,980 --> 00:25:56,030
I haven't actually told my wife that I'm

00:25:54,470 --> 00:25:58,330
sort of becoming a little disheartened

00:25:56,030 --> 00:26:01,130
with the whole the whole affair just yet

00:25:58,330 --> 00:26:02,990
but but the really interesting part is

00:26:01,130 --> 00:26:04,309
you know it just it really did remind me

00:26:02,990 --> 00:26:07,550
of sort of that you know the really

00:26:04,309 --> 00:26:09,020
critical structure that is that easy and

00:26:07,550 --> 00:26:11,450
language and how complex it is to

00:26:09,020 --> 00:26:13,010
understand it it was a really great

00:26:11,450 --> 00:26:17,780
experience in learning and understanding

00:26:13,010 --> 00:26:19,730
what text blog could do and you know

00:26:17,780 --> 00:26:21,980
hopefully a good reminder that reverse

00:26:19,730 --> 00:26:23,570
engineering humans is is still a pretty

00:26:21,980 --> 00:26:26,360
challenging task yet so we're probably

00:26:23,570 --> 00:26:28,520
safe we're probably safe from robots for

00:26:26,360 --> 00:26:32,059
for a while yet particularly if I'm

00:26:28,520 --> 00:26:35,660
going to be programming them so I will

00:26:32,059 --> 00:26:37,100
the I probably will continue with these

00:26:35,660 --> 00:26:39,920
because I am now actually came to so to

00:26:37,100 --> 00:26:40,700
say it through and say how much of the

00:26:39,920 --> 00:26:42,500
original content

00:26:40,700 --> 00:26:46,280
you actually can you can reverse

00:26:42,500 --> 00:26:49,780
engineer automatically so just in

00:26:46,280 --> 00:26:51,920
winding up I've only touched on a really

00:26:49,780 --> 00:26:54,530
simple couple of things that text blob

00:26:51,920 --> 00:26:57,050
can do text blob actually is quite

00:26:54,530 --> 00:26:59,600
amusing men sleep our 'fl it can do

00:26:57,050 --> 00:27:01,310
sentiment analysis so given given a

00:26:59,600 --> 00:27:03,500
piece of text it can actually tell you

00:27:01,310 --> 00:27:05,300
whether that the text is overly is

00:27:03,500 --> 00:27:07,490
generally happy generally sad or

00:27:05,300 --> 00:27:10,940
generally neutral it can do tokenization

00:27:07,490 --> 00:27:12,470
which is stripping all your content into

00:27:10,940 --> 00:27:13,670
chunks so into word chunks or in

00:27:12,470 --> 00:27:17,450
sentence chunks which we talked about

00:27:13,670 --> 00:27:18,800
before it can do limit ization which is

00:27:17,450 --> 00:27:21,860
actually really cool it basically it'll

00:27:18,800 --> 00:27:22,970
convert any word into the equivalent

00:27:21,860 --> 00:27:25,850
word that's likely to be in the

00:27:22,970 --> 00:27:29,210
dictionary so for example if you have

00:27:25,850 --> 00:27:31,010
the word octopi when you do limit

00:27:29,210 --> 00:27:34,970
ization on it alternate octopus and

00:27:31,010 --> 00:27:36,530
that's might sound irrelevant but when

00:27:34,970 --> 00:27:39,950
you're trying to actually automatically

00:27:36,530 --> 00:27:41,720
understand what's going on in a piece of

00:27:39,950 --> 00:27:43,490
text you actually want to make sure that

00:27:41,720 --> 00:27:45,680
you can try and work with the reduced

00:27:43,490 --> 00:27:48,710
form of the word always and so this

00:27:45,680 --> 00:27:53,150
gives you a way to come back to one form

00:27:48,710 --> 00:27:54,410
from any pluralization and things like

00:27:53,150 --> 00:27:55,400
that so that that's actually really

00:27:54,410 --> 00:27:57,350
quite useful as well

00:27:55,400 --> 00:27:59,990
it does spelling correction I believe

00:27:57,350 --> 00:28:03,140
that's also through Google it can do

00:27:59,990 --> 00:28:05,270
translation and the list goes on and on

00:28:03,140 --> 00:28:07,940
so as I said the documentation for it is

00:28:05,270 --> 00:28:12,890
is really wonderful as is the

00:28:07,940 --> 00:28:14,270
documentation for free ltk so that's all

00:28:12,890 --> 00:28:18,490
for me and happy to take any questions

00:28:14,270 --> 00:28:18,490
you might have so

00:28:23,260 --> 00:28:31,070
we are good for one question so I hope I

00:28:28,880 --> 00:28:33,110
make it a good one mate yeah love you

00:28:31,070 --> 00:28:34,910
spy talk just quick one so do you think

00:28:33,110 --> 00:28:38,840
there's any use for our context-free

00:28:34,910 --> 00:28:41,450
grammars to help analyze tweets look

00:28:38,840 --> 00:28:44,870
this is by no means my area of expertise

00:28:41,450 --> 00:28:46,130
so I'm sure there is the reason I wanted

00:28:44,870 --> 00:28:48,170
to do it this way is I'm quiet I'm

00:28:46,130 --> 00:28:50,090
really quite interested in algorithms

00:28:48,170 --> 00:28:52,130
that are like that are really easy to

00:28:50,090 --> 00:28:53,960
explain and so this one to me was sort

00:28:52,130 --> 00:28:55,970
of nice and simple I could it was just a

00:28:53,960 --> 00:28:58,190
just a mapping from one to the other so

00:28:55,970 --> 00:28:59,450
yeah yeah I mean there's plenty of

00:28:58,190 --> 00:29:01,010
experts who are gonna solve this better

00:28:59,450 --> 00:29:03,590
than me hacking around with parts of

00:29:01,010 --> 00:29:05,690
speech I suspect that was a very brief

00:29:03,590 --> 00:29:12,140
if someone else has one very short

00:29:05,690 --> 00:29:14,390
question - I can get to you yes I was

00:29:12,140 --> 00:29:17,030
just wondering whether enhancing that 30

00:29:14,390 --> 00:29:23,300
tag list of unique tags would be a

00:29:17,030 --> 00:29:25,010
pathway - yeah that's really interesting

00:29:23,300 --> 00:29:27,560
actually I went the other way I went the

00:29:25,010 --> 00:29:31,520
other way initially I went if you look

00:29:27,560 --> 00:29:34,070
at if you look at the these I actually

00:29:31,520 --> 00:29:37,790
initially started by reducing all of

00:29:34,070 --> 00:29:38,750
them so all of the nouns - to a common

00:29:37,790 --> 00:29:42,110
one because I thought that might be

00:29:38,750 --> 00:29:43,910
young that might be easier and it wasn't

00:29:42,110 --> 00:29:46,070
so I suspect that yeah the correct path

00:29:43,910 --> 00:29:46,580
is potentially to go for like to go

00:29:46,070 --> 00:29:48,260
further

00:29:46,580 --> 00:29:49,730
the other challenge - might have been

00:29:48,260 --> 00:29:53,210
that because this was using sort of some

00:29:49,730 --> 00:29:55,370
you know it was a fantastical corpus

00:29:53,210 --> 00:29:57,920
that was input into it potentially the

00:29:55,370 --> 00:30:00,530
challenge was that it was already a set

00:29:57,920 --> 00:30:02,540
of words that might not be a perfect

00:30:00,530 --> 00:30:05,090
choice for actually reconstructing in

00:30:02,540 --> 00:30:09,710
the opposite direction so yeah food for

00:30:05,090 --> 00:30:12,050
future work Thank You Lauren and on

00:30:09,710 --> 00:30:14,060
behalf of PyCon Australia I'd like to

00:30:12,050 --> 00:30:17,200
give to you with a mug for speakers

00:30:14,060 --> 00:30:17,200
thank you very much

00:30:17,929 --> 00:30:19,990

YouTube URL: https://www.youtube.com/watch?v=AkcPLo_5NOI


