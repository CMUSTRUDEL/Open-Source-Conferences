Title: The Messaging of Things
Publication date: 2017-08-04
Playlist: Pycon Australia 2017
Description: 
	Mike Leonard

http://2017.pycon-au.org/schedule/presentation/70/

#pyconau

This talk was given at PyCon Australia 2017 which was held from 3-8 August, 2017 in Melbourne, Victoria.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2017, we're returning to Melbourne, bringing together students, enthusiasts, and professionals with a love of Python from around Australia, and from all over the World. 

August 3-8 2017, Melbourne, Victoria

Python, PyCon, PyConAU
Captions: 
	00:00:01,159 --> 00:00:08,880
good afternoon everyone welcome back to

00:00:03,990 --> 00:00:12,120
the IOT a mini cough my name's Ben I'm a

00:00:08,880 --> 00:00:13,730
developer at repository player to

00:00:12,120 --> 00:00:17,279
introduce my colleague and repose upon

00:00:13,730 --> 00:00:19,500
man of mystery Mike Leonard hailing from

00:00:17,279 --> 00:00:21,109
the boroughs of London or Southampton or

00:00:19,500 --> 00:00:24,090
somewhere like that

00:00:21,109 --> 00:00:26,640
so weekdays you'll find Mike working for

00:00:24,090 --> 00:00:28,890
apposite doing a lot of programming a

00:00:26,640 --> 00:00:30,599
lot of programming in Python weekends

00:00:28,890 --> 00:00:33,149
you'll find him writing the mean streets

00:00:30,599 --> 00:00:35,640
of Canberra on his Harley Davidson and

00:00:33,149 --> 00:00:38,460
he's been given the coveted directly

00:00:35,640 --> 00:00:40,739
after lunch time slot to talk to us and

00:00:38,460 --> 00:00:42,329
entertain us and inform us about the

00:00:40,739 --> 00:00:50,520
messaging of things so I'll hand over to

00:00:42,329 --> 00:00:51,719
Mike thank very much man

00:00:50,520 --> 00:00:54,899
I thought I was actually gonna be be

00:00:51,719 --> 00:00:56,340
worse than that Oh God lightly so I'm

00:00:54,899 --> 00:00:59,850
here today to talk about machining of

00:00:56,340 --> 00:01:02,850
things to start off as a picture of me

00:00:59,850 --> 00:01:04,589
so you know I'm not an imposter and I'm

00:01:02,850 --> 00:01:06,840
channeling the power of Python there I

00:01:04,589 --> 00:01:09,600
work for a company called posit power

00:01:06,840 --> 00:01:11,189
we're in Internet of Things sort of

00:01:09,600 --> 00:01:12,990
company within Internet of Things device

00:01:11,189 --> 00:01:15,150
and we collect a bunch of data and that

00:01:12,990 --> 00:01:16,770
kind of forms a base of this talk and

00:01:15,150 --> 00:01:20,520
there's my Twitter so you can tell me

00:01:16,770 --> 00:01:22,229
all the things I got wrong I'm going to

00:01:20,520 --> 00:01:25,610
quickly talk about sort of three kind of

00:01:22,229 --> 00:01:27,630
areas one is about a sort of note about

00:01:25,610 --> 00:01:30,390
messaging of things so sending data

00:01:27,630 --> 00:01:32,310
backwards and forwards in the context of

00:01:30,390 --> 00:01:35,520
collecting data from a like an edge

00:01:32,310 --> 00:01:37,140
device so about how we how we transfer

00:01:35,520 --> 00:01:39,780
that data so some sort of protocols to

00:01:37,140 --> 00:01:42,570
use ways we can serialize that data to

00:01:39,780 --> 00:01:45,840
try and save bytes on the wire and kind

00:01:42,570 --> 00:01:48,570
of performance the performance is is

00:01:45,840 --> 00:01:49,649
kind of a tricky one we're at comparing

00:01:48,570 --> 00:01:51,030
different standards rather than

00:01:49,649 --> 00:01:53,460
different implementations or libraries

00:01:51,030 --> 00:01:55,500
as different devices you can run it on

00:01:53,460 --> 00:01:57,000
trying to compare sort of actual speed

00:01:55,500 --> 00:01:59,189
of things are going to be read difficult

00:01:57,000 --> 00:02:02,520
so gonna focus mainly on on size of

00:01:59,189 --> 00:02:06,030
packets being sent and how robust these

00:02:02,520 --> 00:02:07,649
different things are as well so there's

00:02:06,030 --> 00:02:10,020
a whole bunch of different serialization

00:02:07,649 --> 00:02:13,020
formats and protocols you can use to

00:02:10,020 --> 00:02:13,720
send data the choice is almost endless

00:02:13,020 --> 00:02:15,400
especially when you

00:02:13,720 --> 00:02:16,930
I think the combinations you can use or

00:02:15,400 --> 00:02:19,420
protocols and civilization formats

00:02:16,930 --> 00:02:20,860
together and we're pretty hard to figure

00:02:19,420 --> 00:02:22,600
out what you should use where you should

00:02:20,860 --> 00:02:26,260
use it and what's best for your

00:02:22,600 --> 00:02:29,650
application so throughout this sort of

00:02:26,260 --> 00:02:31,840
this sort of presentation we're gonna do

00:02:29,650 --> 00:02:33,640
a simple example where we kind of kind

00:02:31,840 --> 00:02:35,920
of the hello world almost of like a data

00:02:33,640 --> 00:02:38,530
collection device we've got sort of a

00:02:35,920 --> 00:02:40,900
sensor we can collect temperature and

00:02:38,530 --> 00:02:42,340
humidity and we want to know when this

00:02:40,900 --> 00:02:45,190
reading was taken and sort of report it

00:02:42,340 --> 00:02:46,480
back to her to a server somewhere and

00:02:45,190 --> 00:02:49,600
we'll compare how we can send this data

00:02:46,480 --> 00:02:51,280
back with the Fords so the first thing

00:02:49,600 --> 00:02:52,420
one that talked about is is how size

00:02:51,280 --> 00:02:55,000
doesn't matter when we're talking about

00:02:52,420 --> 00:02:56,680
lots of devices on the Internet

00:02:55,000 --> 00:02:58,630
size doesn't matter especially when

00:02:56,680 --> 00:03:00,550
they're connected to consumer home Wi-Fi

00:02:58,630 --> 00:03:03,280
which is typically quite quite flaky

00:03:00,550 --> 00:03:04,780
might be online offline you could be

00:03:03,280 --> 00:03:06,490
busy streaming your favorite show on

00:03:04,780 --> 00:03:11,230
Netflix there's no bandwidth to upload

00:03:06,490 --> 00:03:12,610
you your precious humidity data so if we

00:03:11,230 --> 00:03:14,709
start with something pretty much

00:03:12,610 --> 00:03:17,160
everyone is familiar with to start with

00:03:14,709 --> 00:03:19,269
Jason so here we've got a pretty basic

00:03:17,160 --> 00:03:22,120
example of how we might send this data

00:03:19,269 --> 00:03:23,950
using Jason there's a timestamp in there

00:03:22,120 --> 00:03:25,150
and sort of you next time there's a

00:03:23,950 --> 00:03:27,580
temperature reading is and what the

00:03:25,150 --> 00:03:30,540
humidity reading is we start off there

00:03:27,580 --> 00:03:33,580
is like a basic benchmark with 65 bytes

00:03:30,540 --> 00:03:35,980
and 65 byte seems seems pretty small for

00:03:33,580 --> 00:03:37,180
sending this data it seems great when

00:03:35,980 --> 00:03:39,310
you start putting in a context of the

00:03:37,180 --> 00:03:41,050
Internet of Things let's say wanted to

00:03:39,310 --> 00:03:42,489
get kind of real-time data collection on

00:03:41,050 --> 00:03:43,720
this stuff so we're gonna read once a

00:03:42,489 --> 00:03:45,790
second that's sort of you know eighty

00:03:43,720 --> 00:03:48,190
six thousand times a day that gives you

00:03:45,790 --> 00:03:49,360
a lot five and a half mega a day you

00:03:48,190 --> 00:03:50,320
scale up to a month you're looking at

00:03:49,360 --> 00:03:53,220
sort of a hundred and seventy bag a

00:03:50,320 --> 00:03:55,450
month but we don't want just one device

00:03:53,220 --> 00:03:56,890
as look maybe an organization or a

00:03:55,450 --> 00:03:58,630
company you want to collect from tens of

00:03:56,890 --> 00:04:00,220
thousands of devices so suddenly your

00:03:58,630 --> 00:04:02,140
data center even though you would type

00:04:00,220 --> 00:04:03,850
send in these tiny packets needs to

00:04:02,140 --> 00:04:06,400
start taking in a sort of like tens of

00:04:03,850 --> 00:04:08,110
gigabytes of data or perhaps you have

00:04:06,400 --> 00:04:09,910
multiple sensors in your home you start

00:04:08,110 --> 00:04:12,250
getting to two significant size as well

00:04:09,910 --> 00:04:14,920
and again half half a gig might not seem

00:04:12,250 --> 00:04:16,450
that much but if you're thinking about

00:04:14,920 --> 00:04:18,910
these Internet of Things devices perhaps

00:04:16,450 --> 00:04:20,350
using a 3G or 4G connection it start

00:04:18,910 --> 00:04:22,690
getting expensive send this sort of data

00:04:20,350 --> 00:04:24,070
this is a really trivial example with

00:04:22,690 --> 00:04:27,550
sort of the smallest amount of data we

00:04:24,070 --> 00:04:28,990
concerned and if the tiny device

00:04:27,550 --> 00:04:31,000
if you scale this up to something more

00:04:28,990 --> 00:04:34,440
complicated more devices these numbers

00:04:31,000 --> 00:04:34,440
really start mattering quite quickly

00:04:35,379 --> 00:04:39,909
so can we do to this Jason to make it

00:04:37,810 --> 00:04:41,139
better so just focus on the basics we

00:04:39,909 --> 00:04:42,430
could take the white space out it's a

00:04:41,139 --> 00:04:44,919
pretty it's a pretty simple starting

00:04:42,430 --> 00:04:48,310
point and we take some bytes off so

00:04:44,919 --> 00:04:50,680
there's a good start we could think of

00:04:48,310 --> 00:04:52,360
well about abbreviating on the stuff to

00:04:50,680 --> 00:04:54,759
take out full words we're sending every

00:04:52,360 --> 00:04:56,139
time for maybe put into your age but

00:04:54,759 --> 00:04:57,699
then we've got the problem of what if we

00:04:56,139 --> 00:04:59,830
have another reading which also starts

00:04:57,699 --> 00:05:01,780
with the T do we go t to something else

00:04:59,830 --> 00:05:04,150
do you start giving cryptic messages and

00:05:01,780 --> 00:05:07,690
I Jason starts perhaps being up there

00:05:04,150 --> 00:05:09,490
not the easiest format to follow but we

00:05:07,690 --> 00:05:11,229
can go down to 35 bytes and so what

00:05:09,490 --> 00:05:13,030
we'll do is we'll take this as our kind

00:05:11,229 --> 00:05:14,919
of minimal Jason structure that we can

00:05:13,030 --> 00:05:17,979
we can send it's kind of the best we can

00:05:14,919 --> 00:05:19,389
do and so here's just 35 bytes as our

00:05:17,979 --> 00:05:24,069
benchmark going forward for this

00:05:19,389 --> 00:05:26,530
presentation so next up we can look at

00:05:24,069 --> 00:05:28,210
other serialization formats so these

00:05:26,530 --> 00:05:29,919
tend to be slightly more complicated to

00:05:28,210 --> 00:05:31,180
use but they have additional benefits

00:05:29,919 --> 00:05:34,300
and the fact we have binary

00:05:31,180 --> 00:05:35,680
serialization we don't need to have some

00:05:34,300 --> 00:05:36,940
one kind of boilerplate in the

00:05:35,680 --> 00:05:39,969
serialization itself such as the

00:05:36,940 --> 00:05:43,449
brackets and things so one such format

00:05:39,969 --> 00:05:46,240
is is a verb by Apache this is a binary

00:05:43,449 --> 00:05:48,310
serialization format and to use this you

00:05:46,240 --> 00:05:50,080
define a schema so this is what an

00:05:48,310 --> 00:05:51,520
average schema looks like we've kind of

00:05:50,080 --> 00:05:53,830
given it some some name information at

00:05:51,520 --> 00:05:55,779
the top and then inside that we define

00:05:53,830 --> 00:05:58,060
what the fields are inside this

00:05:55,779 --> 00:05:59,349
serialize format it's with a timestamp

00:05:58,060 --> 00:06:01,509
we've got the temperature and the

00:05:59,349 --> 00:06:03,009
humidity we can define the types there

00:06:01,509 --> 00:06:04,629
as well so that's kind of almost that's

00:06:03,009 --> 00:06:05,830
the another benefit over Jason

00:06:04,629 --> 00:06:07,810
straightaway we can have more complex

00:06:05,830 --> 00:06:09,190
types in there you can do unions of

00:06:07,810 --> 00:06:12,069
types as well to say give me like a

00:06:09,190 --> 00:06:15,069
float or an end whether or it's not able

00:06:12,069 --> 00:06:16,569
that type of thing and all these field

00:06:15,069 --> 00:06:18,219
names are stuffer defined in the schema

00:06:16,569 --> 00:06:22,000
itself rather than the actual serialize

00:06:18,219 --> 00:06:23,199
data was anything forward so when you

00:06:22,000 --> 00:06:24,819
kind of serialize that you end up with

00:06:23,199 --> 00:06:26,319
something that looks horrendous like

00:06:24,819 --> 00:06:28,750
this with some some binary stuff in

00:06:26,319 --> 00:06:30,520
there but the interesting thing to know

00:06:28,750 --> 00:06:33,699
here is that the prefix of this message

00:06:30,520 --> 00:06:35,379
includes the schema itself which will

00:06:33,699 --> 00:06:37,779
come back to you later

00:06:35,379 --> 00:06:39,339
there so with a schema itself and

00:06:37,779 --> 00:06:40,930
encoding that same bit of day so we're

00:06:39,339 --> 00:06:42,759
281 bytes

00:06:40,930 --> 00:06:45,720
which is significantly worse than the

00:06:42,759 --> 00:06:47,919
night Jason we had earlier at 35 bytes

00:06:45,720 --> 00:06:50,860
but the nice thing about this is we can

00:06:47,919 --> 00:06:53,259
we can we can enhance us further so we

00:06:50,860 --> 00:06:55,090
can batch our readings so if we if we

00:06:53,259 --> 00:06:56,620
understand each reading at a time we're

00:06:55,090 --> 00:07:00,070
going to be looking at sort of 35 bytes

00:06:56,620 --> 00:07:02,020
with the adjacent format or with this

00:07:00,070 --> 00:07:05,919
format we're looking at 280 whatever it

00:07:02,020 --> 00:07:07,090
was but if we batches into into a single

00:07:05,919 --> 00:07:09,100
reader we can save some that boilerplate

00:07:07,090 --> 00:07:10,780
especially with Avro we can include the

00:07:09,100 --> 00:07:12,370
scheme of once the beginning and then

00:07:10,780 --> 00:07:14,380
include ten hundred whatever type

00:07:12,370 --> 00:07:16,090
readings in there so we take sort of our

00:07:14,380 --> 00:07:18,400
ten readings and if we take an example

00:07:16,090 --> 00:07:20,080
of ten readings inside the schema and

00:07:18,400 --> 00:07:21,820
batch them up we get sort of foreign of

00:07:20,080 --> 00:07:22,780
bytes payload we take off the initial

00:07:21,820 --> 00:07:24,550
part of the scheme when the first

00:07:22,780 --> 00:07:26,710
reading we get down to about thirteen

00:07:24,550 --> 00:07:28,990
bytes per reading after that so once you

00:07:26,710 --> 00:07:30,639
get past that initial overhead and this

00:07:28,990 --> 00:07:32,380
gets quite small and if you're sending a

00:07:30,639 --> 00:07:36,699
better than R then our 35 bytes I'm

00:07:32,380 --> 00:07:41,080
Jason but I think we can improve on that

00:07:36,699 --> 00:07:43,509
still so what if we do so with with Avro

00:07:41,080 --> 00:07:46,090
the way ever encodes integers they're in

00:07:43,509 --> 00:07:48,580
a variable number of bytes so the times

00:07:46,090 --> 00:07:50,139
that service is quite large so rather

00:07:48,580 --> 00:07:52,900
than including each timestamp in a batch

00:07:50,139 --> 00:07:56,020
in a batch message we conclude a

00:07:52,900 --> 00:07:57,490
starting timestamp and then an offset

00:07:56,020 --> 00:07:59,199
from that which take one bite each and

00:07:57,490 --> 00:08:00,820
if we do that and put that in there and

00:07:59,199 --> 00:08:02,470
we sort of add several messages to the

00:08:00,820 --> 00:08:04,690
to the payload we start getting down to

00:08:02,470 --> 00:08:05,979
about nine bytes per message so now we

00:08:04,690 --> 00:08:07,780
start getting a lot smaller and we can

00:08:05,979 --> 00:08:09,310
we can really we haven't done too much

00:08:07,780 --> 00:08:12,490
here to really reduce the payload size

00:08:09,310 --> 00:08:17,680
and we understand far more data as we're

00:08:12,490 --> 00:08:20,020
collecting things so so average house

00:08:17,680 --> 00:08:21,580
has Python libraries it's pretty

00:08:20,020 --> 00:08:24,900
straightforward to use you define your

00:08:21,580 --> 00:08:27,789
schema in a in a averaged schema file

00:08:24,900 --> 00:08:30,070
and then there's different options so

00:08:27,789 --> 00:08:32,529
here's an option for reading that schema

00:08:30,070 --> 00:08:35,289
in and reading some data to a file using

00:08:32,529 --> 00:08:38,070
that schema it's pretty straightforward

00:08:35,289 --> 00:08:40,209
just a few few lines of Python there

00:08:38,070 --> 00:08:42,399
some other languages do have

00:08:40,209 --> 00:08:44,500
cogeneration tools for forever as well

00:08:42,399 --> 00:08:45,700
to sort of generate objects from your

00:08:44,500 --> 00:08:50,760
schema to use but unfortunately the

00:08:45,700 --> 00:08:52,600
Python library doesn't so next thing

00:08:50,760 --> 00:08:54,370
which is also becoming quite popular

00:08:52,600 --> 00:08:56,080
it's protocol buffers

00:08:54,370 --> 00:08:57,580
it's a very similar concept to to

00:08:56,080 --> 00:09:00,220
advertise a serialization format and

00:08:57,580 --> 00:09:01,420
binary serialization format but it has

00:09:00,220 --> 00:09:04,330
kind of a different approach to things

00:09:01,420 --> 00:09:05,850
so similarly to Avro it has a schema

00:09:04,330 --> 00:09:09,700
format which is what we can see here

00:09:05,850 --> 00:09:11,290
some basic introduction stuff and then

00:09:09,700 --> 00:09:13,960
kind of the message format beneath that

00:09:11,290 --> 00:09:16,540
again we've got types in there and the

00:09:13,960 --> 00:09:18,880
field names difference with protobuf

00:09:16,540 --> 00:09:20,680
which will come to a bit later is that

00:09:18,880 --> 00:09:23,410
the the fields in protobuf a tagged for

00:09:20,680 --> 00:09:25,089
these these numbers here identify need

00:09:23,410 --> 00:09:31,360
to be static and then defy the bit of

00:09:25,089 --> 00:09:33,250
data if you're encoding so protocol

00:09:31,360 --> 00:09:35,350
buffers have really good code generation

00:09:33,250 --> 00:09:36,940
tools so with this you basically take

00:09:35,350 --> 00:09:39,130
your your input proto file that we just

00:09:36,940 --> 00:09:40,960
saw we run a command line command then

00:09:39,130 --> 00:09:43,839
it spits out essentially a Python

00:09:40,960 --> 00:09:44,860
library specific to that that schema and

00:09:43,839 --> 00:09:46,060
then from there you can treat it like a

00:09:44,860 --> 00:09:47,950
sort of plain

00:09:46,060 --> 00:09:50,680
Python object you can import it so this

00:09:47,950 --> 00:09:53,230
proto environment pb2 is the generated

00:09:50,680 --> 00:09:54,430
code and then within that we can create

00:09:53,230 --> 00:09:55,570
an environment object we can set the

00:09:54,430 --> 00:09:57,520
timestamp for the temperature humidity

00:09:55,570 --> 00:09:59,650
and then we can just serialize it to a

00:09:57,520 --> 00:10:01,390
string and with that we get something

00:09:59,650 --> 00:10:03,370
that's about 9 9 bytes spread off the

00:10:01,390 --> 00:10:06,820
bat so slightly better than the average

00:10:03,370 --> 00:10:08,200
schema to start with and with the

00:10:06,820 --> 00:10:15,250
protocol buffers you don't have to send

00:10:08,200 --> 00:10:19,089
the the schema so trying to go on what

00:10:15,250 --> 00:10:21,850
we did before we can try and do multiple

00:10:19,089 --> 00:10:24,970
messages in a single batch so to try and

00:10:21,850 --> 00:10:27,459
do this I created a message types this

00:10:24,970 --> 00:10:28,510
is the sort of record this top bit and

00:10:27,459 --> 00:10:30,480
then you can basically say this this

00:10:28,510 --> 00:10:32,560
reading is repeated several times

00:10:30,480 --> 00:10:34,300
interestingly this actually quite

00:10:32,560 --> 00:10:37,660
significantly increase the payload size

00:10:34,300 --> 00:10:39,490
per per record I think it's probably due

00:10:37,660 --> 00:10:42,880
to the complexity of defining the type

00:10:39,490 --> 00:10:46,270
like that and having to ever map back so

00:10:42,880 --> 00:10:47,650
I don't know quite why that is but

00:10:46,270 --> 00:10:49,060
that's significantly increased it but

00:10:47,650 --> 00:10:54,000
there's no reason why I can't just send

00:10:49,060 --> 00:10:54,000
lots of lots of messages for this format

00:10:54,510 --> 00:10:59,200
similarly I did the same thing at the

00:10:56,860 --> 00:11:00,910
Avro where I had a starting time stamp

00:10:59,200 --> 00:11:02,589
and an offset and that did reduce it

00:11:00,910 --> 00:11:04,329
down a bit to fourteen bytes but still

00:11:02,589 --> 00:11:07,630
not quite as good as the initial nine

00:11:04,329 --> 00:11:08,290
bytes we had for for just straight-up

00:11:07,630 --> 00:11:13,690
protocol

00:11:08,290 --> 00:11:15,670
with one record in it so now moving on

00:11:13,690 --> 00:11:18,040
to protocols now we've seen how we can

00:11:15,670 --> 00:11:20,279
serialize the data we've seen how we can

00:11:18,040 --> 00:11:23,110
have savings in there either just by

00:11:20,279 --> 00:11:25,029
perhaps taken from shortcuts in the

00:11:23,110 --> 00:11:28,060
Jason so trying to reduce the payload

00:11:25,029 --> 00:11:29,350
size by being in abbreviations or using

00:11:28,060 --> 00:11:31,209
something like a door which is binary

00:11:29,350 --> 00:11:33,160
serialization and putting multiple

00:11:31,209 --> 00:11:34,779
things in there now once we've serialize

00:11:33,160 --> 00:11:35,860
that data as small as we can how can we

00:11:34,779 --> 00:11:40,089
send it in the most efficient way

00:11:35,860 --> 00:11:41,620
possible so probably go and go back to

00:11:40,089 --> 00:11:42,850
basics again so probably the most the

00:11:41,620 --> 00:11:44,800
most common way to think of just been

00:11:42,850 --> 00:11:46,750
making an HTTP POST request we've got

00:11:44,800 --> 00:11:49,449
some data we've got some Jason we can

00:11:46,750 --> 00:11:51,880
send that so back to that same original

00:11:49,449 --> 00:11:54,250
example this was this was 35 by to start

00:11:51,880 --> 00:11:57,069
with but um we add the the overhead of

00:11:54,250 --> 00:11:58,360
the request headers in this example here

00:11:57,069 --> 00:12:02,470
which is pretty common you see we've got

00:11:58,360 --> 00:12:04,269
the host the content type in our hundred

00:12:02,470 --> 00:12:07,480
and twelve bytes so I sort of tripled

00:12:04,269 --> 00:12:08,649
that message size just with the h-2b

00:12:07,480 --> 00:12:11,259
headers which have to be there and kind

00:12:08,649 --> 00:12:13,269
of fixed so this got me thinking how can

00:12:11,259 --> 00:12:14,889
we how can we reduce the header size in

00:12:13,269 --> 00:12:19,779
a regular HTTP what's the bare minimum

00:12:14,889 --> 00:12:22,690
we can have turns out with HTTP 1 you

00:12:19,779 --> 00:12:24,880
can have just the post part the HTTP 1.1

00:12:22,690 --> 00:12:28,810
is pretty commonly used it's been around

00:12:24,880 --> 00:12:30,250
since 1997 you must have a manaphy host

00:12:28,810 --> 00:12:32,139
header

00:12:30,250 --> 00:12:34,440
I was wondering how mandatory that

00:12:32,139 --> 00:12:37,269
actually was I tried

00:12:34,440 --> 00:12:40,720
removing it to see what happened and

00:12:37,269 --> 00:12:42,880
with nginx by default I don't know if

00:12:40,720 --> 00:12:45,699
you can change this but nginx by default

00:12:42,880 --> 00:12:47,079
if you don't include the host header it

00:12:45,699 --> 00:12:48,490
will give you a bad request and that

00:12:47,079 --> 00:12:50,410
curl at bottom there I'm stripping the

00:12:48,490 --> 00:12:52,689
host header out and it comes back with a

00:12:50,410 --> 00:12:53,949
bad request and I'd imagine that

00:12:52,689 --> 00:12:55,240
probably quite a lot of people in this

00:12:53,949 --> 00:12:58,600
room if they're doing web servers are

00:12:55,240 --> 00:12:59,709
sitting it behind nginx so really you

00:12:58,600 --> 00:13:01,630
know you're gonna be stuck with those

00:12:59,709 --> 00:13:03,100
two headers at least which is a large

00:13:01,630 --> 00:13:13,240
overhead for the amount of data you're

00:13:03,100 --> 00:13:15,910
sending so that brings us to http 2 so H

00:13:13,240 --> 00:13:17,949
2 B 2 is kind of the history is from

00:13:15,910 --> 00:13:21,069
Google originally created a protocol

00:13:17,949 --> 00:13:21,910
called called speedy SPDY which is kind

00:13:21,069 --> 00:13:23,680
of experiment

00:13:21,910 --> 00:13:25,570
was enabled in the Chrome browser only

00:13:23,680 --> 00:13:28,900
and that kind of went on to form the

00:13:25,570 --> 00:13:32,620
basis of the HTTP to spec and speed is

00:13:28,900 --> 00:13:34,780
now now deprecated and gone from Chrome

00:13:32,620 --> 00:13:36,520
the idea of HTTP 2 is it basically works

00:13:34,780 --> 00:13:37,900
the same but it changed the way the data

00:13:36,520 --> 00:13:38,440
sent on the wire and what the browser's

00:13:37,900 --> 00:13:41,800
do with it

00:13:38,440 --> 00:13:43,060
to offer improvements and there some

00:13:41,800 --> 00:13:45,490
additional features in there as well

00:13:43,060 --> 00:13:49,210
does multiplexing and there's a feature

00:13:45,490 --> 00:13:51,070
called server push server push is

00:13:49,210 --> 00:13:52,780
basically you can you can request make

00:13:51,070 --> 00:13:54,940
one request and the server can decide to

00:13:52,780 --> 00:13:56,380
give you multiple things back so for

00:13:54,940 --> 00:13:58,600
example if your question sort of a home

00:13:56,380 --> 00:14:00,580
page and that home page you need to the

00:13:58,600 --> 00:14:02,980
CSS and JavaScript and stuff it can you

00:14:00,580 --> 00:14:04,540
can push those things to you as well to

00:14:02,980 --> 00:14:06,040
save the overhead of making additional

00:14:04,540 --> 00:14:07,960
connections I'm waiting for that

00:14:06,040 --> 00:14:11,410
round-trip to make requests the next

00:14:07,960 --> 00:14:13,750
resources probably the most interesting

00:14:11,410 --> 00:14:16,480
thing in context of messaging of things

00:14:13,750 --> 00:14:19,570
is header compression which is a new

00:14:16,480 --> 00:14:20,950
thing in HTTP 2 which compresses the the

00:14:19,570 --> 00:14:23,650
headers to try and reduce their their

00:14:20,950 --> 00:14:27,970
size and it uses an algorithm called

00:14:23,650 --> 00:14:31,480
h-back it was a kind of fun example sore

00:14:27,970 --> 00:14:32,350
of HP 1 versus 2 where HTTP 1 you have

00:14:31,480 --> 00:14:33,130
to parse all the ingredients

00:14:32,350 --> 00:14:34,870
individually

00:14:33,130 --> 00:14:39,760
whereas nature be 2 if you want a

00:14:34,870 --> 00:14:42,430
million let's have a little one go so

00:14:39,760 --> 00:14:44,020
with age park how does it work so kind

00:14:42,430 --> 00:14:47,170
of the most fundamental part of age pack

00:14:44,020 --> 00:14:50,290
is it uses a aesthetic dictionary to

00:14:47,170 --> 00:14:51,910
look up commonly used headers and in

00:14:50,290 --> 00:14:54,280
some cases that commonly use values as

00:14:51,910 --> 00:14:56,980
well so this sort of things maybe a

00:14:54,280 --> 00:14:58,060
hundred ish different values in the in

00:14:56,980 --> 00:15:01,090
the static dictionary as part of the

00:14:58,060 --> 00:15:02,590
spec and some of those include the

00:15:01,090 --> 00:15:04,270
values they can see status codes for

00:15:02,590 --> 00:15:05,830
example just very very common status

00:15:04,270 --> 00:15:08,380
codes we use they live in this

00:15:05,830 --> 00:15:11,140
dictionary so if a header lives in

00:15:08,380 --> 00:15:13,360
dictionary either the just ahead a name

00:15:11,140 --> 00:15:15,340
or the header and the value that can be

00:15:13,360 --> 00:15:17,710
encoded in the header of the HTTP

00:15:15,340 --> 00:15:19,270
request as just 1 or 2 bytes rather than

00:15:17,710 --> 00:15:21,430
sort of a byte per character as it

00:15:19,270 --> 00:15:24,070
usually would be so we can save matters

00:15:21,430 --> 00:15:25,540
of space is there also the servers and

00:15:24,070 --> 00:15:27,040
clients can build up dynamic dictionary

00:15:25,540 --> 00:15:28,420
so sort of as a quest go backwards and

00:15:27,040 --> 00:15:30,040
forwards they can come up with read

00:15:28,420 --> 00:15:33,250
dictionary specific to those requests to

00:15:30,040 --> 00:15:35,770
try and compress further and so these

00:15:33,250 --> 00:15:37,970
can really save a mass amount of space

00:15:35,770 --> 00:15:39,920
and I kind of got an example here don't

00:15:37,970 --> 00:15:41,930
know how where you can see it but it's

00:15:39,920 --> 00:15:44,870
two equivalent requests one against an

00:15:41,930 --> 00:15:47,030
HTTP one server and one against an HTTP

00:15:44,870 --> 00:15:49,310
2 server you can kind of see they've got

00:15:47,030 --> 00:15:51,410
all the same data being stained in the

00:15:49,310 --> 00:15:55,040
response 36 bytes got the same sort of

00:15:51,410 --> 00:15:57,170
headers but if we take a look at the the

00:15:55,040 --> 00:16:00,170
size on the Y here so this is just a

00:15:57,170 --> 00:16:02,750
screenshot from chrome dev tools the the

00:16:00,170 --> 00:16:05,240
top one is HTTP one bottom HTTP 2 and

00:16:02,750 --> 00:16:06,680
the size figures on the right the black

00:16:05,240 --> 00:16:08,900
number is kind of sizing the wire so

00:16:06,680 --> 00:16:10,280
what was actually transferred so it

00:16:08,900 --> 00:16:12,410
takes into account that the headers and

00:16:10,280 --> 00:16:15,830
things takes into account any like gzip

00:16:12,410 --> 00:16:17,750
compression you have but specifically

00:16:15,830 --> 00:16:18,260
here you can see for the HTTP one

00:16:17,750 --> 00:16:22,430
request

00:16:18,260 --> 00:16:24,920
it's 134 bytes for the 36 by payload and

00:16:22,430 --> 00:16:26,810
52 B - it was just 58 bytes for the 36

00:16:24,920 --> 00:16:28,370
bit by payload so they're completely

00:16:26,810 --> 00:16:30,410
equivalent but significantly smaller

00:16:28,370 --> 00:16:32,180
fake could be - and that's mostly to do

00:16:30,410 --> 00:16:36,770
with the the the edge pack had a

00:16:32,180 --> 00:16:38,780
compression see example here here's

00:16:36,770 --> 00:16:41,000
basically most of the request headers

00:16:38,780 --> 00:16:42,470
that were sent if we take a look at

00:16:41,000 --> 00:16:44,030
those we can actually see that all the

00:16:42,470 --> 00:16:46,250
ones there and white they can all be

00:16:44,030 --> 00:16:47,900
compressed to just one pretty much just

00:16:46,250 --> 00:16:49,880
one bite each which is pretty

00:16:47,900 --> 00:16:51,110
significant saving you don't really have

00:16:49,880 --> 00:16:55,970
to do much to achieve that if you're

00:16:51,110 --> 00:16:59,450
using HTTP - so the next thing we talk

00:16:55,970 --> 00:17:03,440
about is mqtt so this has been touched

00:16:59,450 --> 00:17:06,800
on this morning so MQTT is reasonably

00:17:03,440 --> 00:17:09,589
old the standard messaging developed by

00:17:06,800 --> 00:17:11,780
IBM it stands for message queue limitary

00:17:09,589 --> 00:17:14,630
transport but interestingly it's not

00:17:11,780 --> 00:17:16,100
actually a queue I think came out of a

00:17:14,630 --> 00:17:18,620
suite of a suite of tools that were

00:17:16,100 --> 00:17:21,620
branded like that but it's not actually

00:17:18,620 --> 00:17:24,290
queue it's designed to be the super

00:17:21,620 --> 00:17:28,250
lightweight for devices with either blow

00:17:24,290 --> 00:17:29,330
low-power CPU or in restricted

00:17:28,250 --> 00:17:31,910
environments where they've got low

00:17:29,330 --> 00:17:35,240
bandwidth connections it's a pub/sub

00:17:31,910 --> 00:17:38,210
architecture which I'll show you a

00:17:35,240 --> 00:17:39,590
diagram of in a second it's really quite

00:17:38,210 --> 00:17:41,060
a simple protocol we can sort of refer

00:17:39,590 --> 00:17:44,570
this back and isn't there's not much to

00:17:41,060 --> 00:17:45,890
it and you kind of use these methods in

00:17:44,570 --> 00:17:47,500
there to say you're going to connect to

00:17:45,890 --> 00:17:49,540
a to a message broker

00:17:47,500 --> 00:17:52,570
to publish a message that sort of thing

00:17:49,540 --> 00:17:53,980
and they also have some some settings in

00:17:52,570 --> 00:17:58,060
there for defining quality of service to

00:17:53,980 --> 00:17:59,320
ensure message delivery so for those of

00:17:58,060 --> 00:18:01,360
you that don't know about the the

00:17:59,320 --> 00:18:03,040
pub/sub sort of model the general idea

00:18:01,360 --> 00:18:07,600
is this temperature sensor on the on the

00:18:03,040 --> 00:18:10,060
left here could be a edge node device hi

00:18:07,600 --> 00:18:12,070
them Q in the middle is a mqtt broker

00:18:10,060 --> 00:18:14,110
you can get basically matches get sent

00:18:12,070 --> 00:18:15,610
to that broker and then things I want to

00:18:14,110 --> 00:18:17,650
do something that message II but display

00:18:15,610 --> 00:18:20,650
it process it store in database whatever

00:18:17,650 --> 00:18:24,460
they subscribe to the broker and we use

00:18:20,650 --> 00:18:26,950
topics so you might say a topic like

00:18:24,460 --> 00:18:28,360
this so you might enter Phi web data is

00:18:26,950 --> 00:18:30,520
coming from so like an address for

00:18:28,360 --> 00:18:32,350
example if so it's a bedroom and we're

00:18:30,520 --> 00:18:35,560
getting the environment data so each

00:18:32,350 --> 00:18:37,480
time I devised publishes some data it

00:18:35,560 --> 00:18:40,570
publishes it on this topic that goes to

00:18:37,480 --> 00:18:42,730
the broker and then servers or other

00:18:40,570 --> 00:18:44,410
devices or you know Web Apps whatever

00:18:42,730 --> 00:18:46,030
that want to read that data they

00:18:44,410 --> 00:18:47,530
subscribe to the same topic and each

00:18:46,030 --> 00:18:50,230
time message comes in they get they get

00:18:47,530 --> 00:18:51,910
notified you can do some my stuff in

00:18:50,230 --> 00:18:55,000
there as well with sort of wildcard II

00:18:51,910 --> 00:18:57,430
things so the plus means match one level

00:18:55,000 --> 00:18:58,990
here no matter what it is and the hash

00:18:57,430 --> 00:19:03,250
means match any number of levels and

00:18:58,990 --> 00:19:03,850
subscribe to all those things now the

00:19:03,250 --> 00:19:05,770
nice thing about

00:19:03,850 --> 00:19:08,920
mqtt when we're talking about message

00:19:05,770 --> 00:19:12,130
size is that the headers compared to

00:19:08,920 --> 00:19:14,880
http are tiny the fixed headers of an

00:19:12,130 --> 00:19:17,380
MQTT packet are just two bytes in size

00:19:14,880 --> 00:19:19,330
these are fine the type of method we're

00:19:17,380 --> 00:19:22,270
sending so if it's connect/disconnect

00:19:19,330 --> 00:19:23,590
publish and they also define some some

00:19:22,270 --> 00:19:25,540
flag specific to sort of quality of

00:19:23,590 --> 00:19:28,180
service whether this is a repeat message

00:19:25,540 --> 00:19:30,460
some basic stuff like that the second

00:19:28,180 --> 00:19:33,130
byte just tells it how big the rest of

00:19:30,460 --> 00:19:35,920
the messages so mqtt doesn't necessarily

00:19:33,130 --> 00:19:37,660
define what you send kind of like HTTP

00:19:35,920 --> 00:19:40,750
you can send binary you can serialize

00:19:37,660 --> 00:19:41,830
some format jason whatever you want so

00:19:40,750 --> 00:19:44,170
you tell it how big the rest of the

00:19:41,830 --> 00:19:46,030
payload is ascending and then depending

00:19:44,170 --> 00:19:47,530
on the specific message there's might be

00:19:46,030 --> 00:19:50,950
a few other little head as you need such

00:19:47,530 --> 00:19:52,690
as the topic name that sort of thing but

00:19:50,950 --> 00:19:54,370
given the the fixed header with every

00:19:52,690 --> 00:19:56,080
messages of two bytes that adds very

00:19:54,370 --> 00:19:58,830
very little overhead to whatever it is

00:19:56,080 --> 00:19:58,830
you're actually sending

00:20:00,039 --> 00:20:06,100
so you can do you can do MQTT on in in

00:20:03,610 --> 00:20:07,659
micro Python as well as Python and kind

00:20:06,100 --> 00:20:08,889
of gist of it is you create a client you

00:20:07,659 --> 00:20:11,559
connect and you publish a message just

00:20:08,889 --> 00:20:14,490
really quite simple to use as head as a

00:20:11,559 --> 00:20:16,389
Dom for you

00:20:14,490 --> 00:20:19,169
so another couple of things I'm gonna

00:20:16,389 --> 00:20:19,169
talk about quickly

00:20:19,269 --> 00:20:22,269
WebSockets for one so WebSockets are

00:20:20,950 --> 00:20:23,980
great because there's the basic no

00:20:22,269 --> 00:20:25,960
overhead once you've got the connection

00:20:23,980 --> 00:20:27,190
there there's a TCP stack but beyond

00:20:25,960 --> 00:20:28,690
that there's no additional headers that

00:20:27,190 --> 00:20:31,029
sort of you typically send backwards and

00:20:28,690 --> 00:20:32,559
forwards which is nice the bad thing is

00:20:31,029 --> 00:20:34,090
it means there's no topic type of

00:20:32,559 --> 00:20:35,380
information to indicate what you're

00:20:34,090 --> 00:20:38,200
sending a message for or anything like

00:20:35,380 --> 00:20:39,639
that so you're gonna have to to detect

00:20:38,200 --> 00:20:42,179
in such a way or send something with

00:20:39,639 --> 00:20:44,139
each message

00:20:42,179 --> 00:20:45,519
one interesting thing you can do is you

00:20:44,139 --> 00:20:47,620
can actually use sort of from a browser

00:20:45,519 --> 00:20:50,289
or something you can do mqtt over

00:20:47,620 --> 00:20:51,880
WebSockets we have all the power of MQTT

00:20:50,289 --> 00:20:53,049
like in a browser or something so if

00:20:51,880 --> 00:20:55,840
you're building a solution that uses

00:20:53,049 --> 00:20:57,880
MQTT you can use that same technology to

00:20:55,840 --> 00:21:01,210
power the data being delivered to your

00:20:57,880 --> 00:21:02,559
browser and then whether it's good for

00:21:01,210 --> 00:21:05,889
the Internet of Things playing

00:21:02,559 --> 00:21:07,210
WebSockets I'm not super sure but you

00:21:05,889 --> 00:21:08,230
cannot go to have the full if you've got

00:21:07,210 --> 00:21:10,539
a small device you've got to have the

00:21:08,230 --> 00:21:12,519
full TCP stack and kind of a WebSocket

00:21:10,539 --> 00:21:15,190
stack running there I think things like

00:21:12,519 --> 00:21:17,169
MQTT or even just using straight-up HTTP

00:21:15,190 --> 00:21:18,460
if you can you can have that overhead of

00:21:17,169 --> 00:21:22,960
the bytes and the header is probably

00:21:18,460 --> 00:21:24,639
better solution another thing I think

00:21:22,960 --> 00:21:26,380
that's interesting but perhaps not a

00:21:24,639 --> 00:21:29,769
great application for the Internet of

00:21:26,380 --> 00:21:32,289
Things is is G RPC this is getting quite

00:21:29,769 --> 00:21:34,720
popular I think it's more for a sort of

00:21:32,289 --> 00:21:38,799
server to server communication so I

00:21:34,720 --> 00:21:40,929
won't talk too much about that so just

00:21:38,799 --> 00:21:42,580
want to talk quickly about robustness as

00:21:40,929 --> 00:21:44,620
well so it's all very good being I have

00:21:42,580 --> 00:21:46,899
super tiny packets using super light

00:21:44,620 --> 00:21:48,880
protocols send messages back back with

00:21:46,899 --> 00:21:50,769
them forwards but it's no good if you

00:21:48,880 --> 00:21:52,720
can if you can't actually get the

00:21:50,769 --> 00:21:57,370
message you expect to get or parts of

00:21:52,720 --> 00:21:58,600
them corrupt that sort of thing so so

00:21:57,370 --> 00:22:00,429
one of these things about being robust

00:21:58,600 --> 00:22:02,559
is to try and use a serialization format

00:22:00,429 --> 00:22:04,600
rather than just sending Jason if we can

00:22:02,559 --> 00:22:06,250
use something like like Avro or proto

00:22:04,600 --> 00:22:08,169
broth or some of the ones out there like

00:22:06,250 --> 00:22:09,639
like the Rift then you can kind of have

00:22:08,169 --> 00:22:12,250
better guarantees about what data is

00:22:09,639 --> 00:22:15,080
going into that packet both ends have

00:22:12,250 --> 00:22:17,540
kind of agreed upon a format

00:22:15,080 --> 00:22:19,010
so you know what to expect but also you

00:22:17,540 --> 00:22:22,790
can you can have Richard datatypes in

00:22:19,010 --> 00:22:24,320
there so you can you can define better

00:22:22,790 --> 00:22:25,580
dodeth types a union of datatypes

00:22:24,320 --> 00:22:29,450
whether or not something was not able

00:22:25,580 --> 00:22:31,820
that sort of stuff but with this there

00:22:29,450 --> 00:22:33,290
comes a complication in that because

00:22:31,820 --> 00:22:35,120
both these parties have agreed on the

00:22:33,290 --> 00:22:36,410
schema format what happens if you want

00:22:35,120 --> 00:22:37,790
to change it and especially in the

00:22:36,410 --> 00:22:39,560
internet of things if we've got

00:22:37,790 --> 00:22:41,120
thousands of devices out there if we

00:22:39,560 --> 00:22:42,460
make a change to it how would we push

00:22:41,120 --> 00:22:44,420
that update out to all the devices

00:22:42,460 --> 00:22:45,980
perhaps some of them have received an

00:22:44,420 --> 00:22:48,080
update and other ones haven't and they

00:22:45,980 --> 00:22:52,400
could be quite far behind I need to deal

00:22:48,080 --> 00:22:55,400
with that so it's a product buff gives

00:22:52,400 --> 00:22:56,660
us reasonably well the just a protobuf

00:22:55,400 --> 00:22:58,520
inherits e realizes it basically just

00:22:56,660 --> 00:23:00,050
concatenates all the fields together in

00:22:58,520 --> 00:23:02,000
a big long list and there's a bike

00:23:00,050 --> 00:23:04,280
before each field indicating which tag

00:23:02,000 --> 00:23:08,270
it is notorious or there's one two three

00:23:04,280 --> 00:23:10,070
sort of tags beforehand if a field is

00:23:08,270 --> 00:23:13,070
marked is optional is simply emitted

00:23:10,070 --> 00:23:16,820
from the message entirely so doesn't

00:23:13,070 --> 00:23:18,200
matter in improving things and because

00:23:16,820 --> 00:23:20,330
fields are tagged we can we can rename

00:23:18,200 --> 00:23:21,980
and quite happily so what this means is

00:23:20,330 --> 00:23:24,200
we can we can change the the schema

00:23:21,980 --> 00:23:27,020
format quite easily so long as we always

00:23:24,200 --> 00:23:29,390
tag things so the optional one required

00:23:27,020 --> 00:23:31,100
type fields required meant they always

00:23:29,390 --> 00:23:32,330
had to be present and in proto three

00:23:31,100 --> 00:23:34,700
that's actually been been dropped

00:23:32,330 --> 00:23:36,080
because it was seen as being almost

00:23:34,700 --> 00:23:37,520
impossible to maintain back some path

00:23:36,080 --> 00:23:38,000
ability if something was once mark is

00:23:37,520 --> 00:23:39,560
required

00:23:38,000 --> 00:23:41,660
it must always be marked as required so

00:23:39,560 --> 00:23:43,550
that's gone now in favour of doing sort

00:23:41,660 --> 00:23:47,240
of software validations in your own your

00:23:43,550 --> 00:23:48,290
own logic so you can and because

00:23:47,240 --> 00:23:50,150
everything's tagged we can quite happily

00:23:48,290 --> 00:23:51,410
add or remove fields and if it gets to a

00:23:50,150 --> 00:23:54,590
target as understand it would actually

00:23:51,410 --> 00:23:56,360
just skip it and we can mark new fields

00:23:54,590 --> 00:23:57,770
we add in as optional so the old of

00:23:56,360 --> 00:24:01,540
things that are see realized with that

00:23:57,770 --> 00:24:04,430
item in there they just ignore it

00:24:01,540 --> 00:24:06,350
however it does things quite differently

00:24:04,430 --> 00:24:08,000
so the way the way every kind of work

00:24:06,350 --> 00:24:10,010
does everybody can give a read and a

00:24:08,000 --> 00:24:11,420
write schema so you can tell it what

00:24:10,010 --> 00:24:13,010
scheme of this data was written with and

00:24:11,420 --> 00:24:15,620
what scheme you should you should read

00:24:13,010 --> 00:24:17,660
this data with and that's partly why an

00:24:15,620 --> 00:24:20,350
example I showed earlier I included the

00:24:17,660 --> 00:24:22,220
schemer as part of that initial message

00:24:20,350 --> 00:24:23,420
was so that when the when the reader

00:24:22,220 --> 00:24:25,310
picks it up it knows exactly what

00:24:23,420 --> 00:24:26,660
version the scheme was written with now

00:24:25,310 --> 00:24:28,220
that's not the only way of doing it you

00:24:26,660 --> 00:24:31,700
could tag with that are in some way does

00:24:28,220 --> 00:24:32,840
this is written with a schemer or you

00:24:31,700 --> 00:24:34,580
could perhaps when you like if you're

00:24:32,840 --> 00:24:36,500
using mqtt you could perhaps connect

00:24:34,580 --> 00:24:38,750
initially and then kind of agree on the

00:24:36,500 --> 00:24:42,409
scheme you're using and do things rather

00:24:38,750 --> 00:24:43,580
than sending it with every message but

00:24:42,409 --> 00:24:44,990
I've read of some really smart stuff if

00:24:43,580 --> 00:24:46,879
you do provide a read and write schemer

00:24:44,990 --> 00:24:48,919
Avro can do some really smart stuff to

00:24:46,879 --> 00:24:50,059
to figure out kind of precedents in

00:24:48,919 --> 00:24:52,250
there and figure out what field should

00:24:50,059 --> 00:24:54,350
be and because it's got both schemers in

00:24:52,250 --> 00:24:55,399
there as well you can do you can like

00:24:54,350 --> 00:24:57,200
realtor fields and stuff and you can

00:24:55,399 --> 00:24:59,240
rename them to some extent you know kind

00:24:57,200 --> 00:25:01,700
of figure them out for you you can also

00:24:59,240 --> 00:25:04,940
rename fields and supply an alias to

00:25:01,700 --> 00:25:06,769
them which means that you can say it you

00:25:04,940 --> 00:25:07,460
have to be called called this name so

00:25:06,769 --> 00:25:09,409
it's gives you quite a lot of

00:25:07,460 --> 00:25:11,389
flexibility over be able to modify that

00:25:09,409 --> 00:25:13,309
schemer and still have devices out there

00:25:11,389 --> 00:25:14,990
using a slightly different version an

00:25:13,309 --> 00:25:20,629
outdated version and still read the data

00:25:14,990 --> 00:25:23,019
you've got and sort of finally the last

00:25:20,629 --> 00:25:26,389
thing we'll talk about in robustness is

00:25:23,019 --> 00:25:27,889
MQTT quality of service so something can

00:25:26,389 --> 00:25:29,840
specify and build as part of the spec

00:25:27,889 --> 00:25:32,629
and most the libraries implement is this

00:25:29,840 --> 00:25:34,250
idea of quality of service and when you

00:25:32,629 --> 00:25:37,039
send a message you can basically say if

00:25:34,250 --> 00:25:39,080
you won't be delivered at most once at

00:25:37,039 --> 00:25:41,539
least once or exactly once

00:25:39,080 --> 00:25:44,000
each one of these kind of has additional

00:25:41,539 --> 00:25:45,289
overhead but the the specification

00:25:44,000 --> 00:25:46,549
around how this works is pretty

00:25:45,289 --> 00:25:48,230
straightforward and they just some

00:25:46,549 --> 00:25:51,200
messages backwards and forwards are kind

00:25:48,230 --> 00:25:52,970
of ensure these things so if you can

00:25:51,200 --> 00:25:55,370
couple a serialization format with

00:25:52,970 --> 00:25:58,129
something like mqtt we can do guaranteed

00:25:55,370 --> 00:26:01,220
message delivery you know you can really

00:25:58,129 --> 00:26:02,690
start reducing that payload size you can

00:26:01,220 --> 00:26:03,799
ensure the quality of the data you're

00:26:02,690 --> 00:26:05,809
sending you can be back with a

00:26:03,799 --> 00:26:09,799
paddleball and it all works really well

00:26:05,809 --> 00:26:11,120
so just kind of to sum up what I wanted

00:26:09,799 --> 00:26:13,549
to kind of point I want to get across

00:26:11,120 --> 00:26:15,080
was when we're sending messages on the

00:26:13,549 --> 00:26:17,210
the Internet of Things and we're sending

00:26:15,080 --> 00:26:20,330
a lot of volume it's really easy to just

00:26:17,210 --> 00:26:21,889
see lies adjacent endows HTTP requests

00:26:20,330 --> 00:26:23,360
we just find it works everyone's

00:26:21,889 --> 00:26:25,789
familiar with it pretty much every

00:26:23,360 --> 00:26:27,350
device can do it you can very very

00:26:25,789 --> 00:26:29,690
quickly get to massive amounts of

00:26:27,350 --> 00:26:31,429
bandwidth without even realizing be

00:26:29,690 --> 00:26:34,100
there either on the device itself and

00:26:31,429 --> 00:26:36,289
some consumers home Wi-Fi or the fact

00:26:34,100 --> 00:26:38,029
you as a company to support tens of

00:26:36,289 --> 00:26:39,590
thousands of devices and all those

00:26:38,029 --> 00:26:40,179
little bits of data added together can

00:26:39,590 --> 00:26:41,919
form

00:26:40,179 --> 00:26:44,409
masses of masses and masses of gigabytes

00:26:41,919 --> 00:26:46,210
of data so really think about the

00:26:44,409 --> 00:26:47,799
message format in terms of how you

00:26:46,210 --> 00:26:48,999
representing it can you get some wins by

00:26:47,799 --> 00:26:51,279
patching things up and send them

00:26:48,999 --> 00:26:53,200
occasionally to minimize overhead can

00:26:51,279 --> 00:26:55,869
you use a serialization format to reduce

00:26:53,200 --> 00:26:58,330
it down even further and then think

00:26:55,869 --> 00:27:00,580
about the protocol we're using HTTP

00:26:58,330 --> 00:27:03,580
comes with a reason why overhead can you

00:27:00,580 --> 00:27:05,169
use something else do you need to to

00:27:03,580 --> 00:27:07,240
have that additional which sort of

00:27:05,169 --> 00:27:10,419
quality of service guarantees like MQTT

00:27:07,240 --> 00:27:12,039
offers and then the final thing I

00:27:10,419 --> 00:27:15,940
haven't here was planned for scale or

00:27:12,039 --> 00:27:17,169
plan to fail just simply the idea of the

00:27:15,940 --> 00:27:19,480
Internet of Things it's not one device

00:27:17,169 --> 00:27:21,100
there's many millions of them you will

00:27:19,480 --> 00:27:23,919
need to plan for that kind of scale or

00:27:21,100 --> 00:27:42,669
things aren't gonna work now is it thank

00:27:23,919 --> 00:27:47,230
very much do you have any questions just

00:27:42,669 --> 00:27:50,289
one thing about security so AWS for

00:27:47,230 --> 00:27:51,460
example you can connect to their IOT

00:27:50,289 --> 00:27:53,860
platform but you have to have

00:27:51,460 --> 00:27:56,169
certificates and stuff so what's what's

00:27:53,860 --> 00:27:57,669
been your experience with getting that

00:27:56,169 --> 00:28:01,330
up and running or do you think it's

00:27:57,669 --> 00:28:03,129
important it's definitely important I

00:28:01,330 --> 00:28:05,440
don't have much personal experience of

00:28:03,129 --> 00:28:07,090
doing it some of these things most of

00:28:05,440 --> 00:28:09,759
the stuff is based off research rather

00:28:07,090 --> 00:28:11,619
than a production environment there's a

00:28:09,759 --> 00:28:13,570
variety of ways of doing things that

00:28:11,619 --> 00:28:15,220
will again encode our own overhead so I

00:28:13,570 --> 00:28:17,830
think I think the important thing here

00:28:15,220 --> 00:28:19,600
is just can consider what you want to do

00:28:17,830 --> 00:28:21,009
like do these benchmarks kind of figure

00:28:19,600 --> 00:28:23,860
out how long things take what the packet

00:28:21,009 --> 00:28:25,929
size is do you have massive overhead and

00:28:23,860 --> 00:28:28,210
how can you reduce those things if you

00:28:25,929 --> 00:28:29,889
do have you know exchanges of keys or in

00:28:28,210 --> 00:28:32,139
your connect in some way can you do that

00:28:29,889 --> 00:28:33,369
once upfront and then send lots of

00:28:32,139 --> 00:28:37,230
messages rather than having to do this

00:28:33,369 --> 00:28:37,230
connection and kind of do this each time

00:28:40,619 --> 00:28:46,299
essentially the same question some of

00:28:42,730 --> 00:28:48,789
them some of them are protocols use HTTP

00:28:46,299 --> 00:28:50,619
headers to have the keys in there I

00:28:48,789 --> 00:28:53,260
don't don't know if you can will be able

00:28:50,619 --> 00:28:56,890
to HTTP to will be able to

00:28:53,260 --> 00:28:58,690
and core dictionary can code that the

00:28:56,890 --> 00:29:01,720
dictionary probably encode it but the

00:28:58,690 --> 00:29:03,010
HTTP to for the essentially works

00:29:01,720 --> 00:29:04,150
exactly the same way as far as you're

00:29:03,010 --> 00:29:07,419
concerned if you can put something into

00:29:04,150 --> 00:29:08,830
a header in HTTP one you can do that and

00:29:07,419 --> 00:29:10,720
I believe the dynamic dictionary so if

00:29:08,830 --> 00:29:12,429
you're sending the same sort of

00:29:10,720 --> 00:29:14,950
authentication so the authorization

00:29:12,429 --> 00:29:16,809
header the key part of that is is an in

00:29:14,950 --> 00:29:19,240
dictionary so can you hide that but I

00:29:16,809 --> 00:29:20,559
believe the value part contentiously

00:29:19,240 --> 00:29:27,460
form part of this dynamic dictionary

00:29:20,559 --> 00:29:29,950
they can build up alright so you've

00:29:27,460 --> 00:29:32,410
mentioned serialization formats a bit in

00:29:29,950 --> 00:29:34,540
binary serialization formats but I know

00:29:32,410 --> 00:29:36,370
in my personal experience sometimes it's

00:29:34,540 --> 00:29:38,080
really handy to just be able to say open

00:29:36,370 --> 00:29:40,600
up Wireshark and start logging real

00:29:38,080 --> 00:29:42,400
packets if your bit suspicious if the

00:29:40,600 --> 00:29:45,190
data coming in said you have sort of an

00:29:42,400 --> 00:29:48,250
opinion on that these to debug versus

00:29:45,190 --> 00:29:49,660
small data yeah it is it is harder to

00:29:48,250 --> 00:29:51,669
debug there are some tools out there

00:29:49,660 --> 00:29:53,890
where you can get some I think I don't

00:29:51,669 --> 00:29:55,780
remember areas but there's some plugins

00:29:53,890 --> 00:29:57,070
you can get to life like a belief which

00:29:55,780 --> 00:29:59,230
was some of these protocols you can

00:29:57,070 --> 00:30:00,850
actually decode them in Wireshark so for

00:29:59,230 --> 00:30:02,590
the for the most part you can just treat

00:30:00,850 --> 00:30:07,630
it as if you would any other thing over

00:30:02,590 --> 00:30:09,910
HTTP thanks Mike for the talk here's

00:30:07,630 --> 00:30:11,530
your obligatory mug if you finally go

00:30:09,910 --> 00:30:14,180
missing from the breaker room it

00:30:11,530 --> 00:30:18,140
probably wasn't me

00:30:14,180 --> 00:30:18,140

YouTube URL: https://www.youtube.com/watch?v=UfPhXhCjMpw


