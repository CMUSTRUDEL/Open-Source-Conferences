Title: Learn You a PyTorch!
Publication date: 2017-08-05
Playlist: Pycon Australia 2017
Description: 
	Kendrick Tan

http://2017.pycon-au.org/schedule/presentation/59/

#pyconau

This talk was given at PyCon Australia 2017 which was held from 3-8 August, 2017 in Melbourne, Victoria.

PyCon Australia is the national conference for users of the Python Programming Language. In August 2017, we're returning to Melbourne, bringing together students, enthusiasts, and professionals with a love of Python from around Australia, and from all over the World. 

August 3-8 2017, Melbourne, Victoria

Python, PyCon, PyConAU
Captions: 
	00:00:00,420 --> 00:00:05,759
welcome back to dollar science mini-cons

00:00:02,879 --> 00:00:07,770
so so next up we have Kendrick Kendricks

00:00:05,759 --> 00:00:09,929
an undergraduate student and also

00:00:07,770 --> 00:00:10,920
machine learning engineer so so well

00:00:09,929 --> 00:00:13,559
done

00:00:10,920 --> 00:00:16,470
has to be quite busy so um so he's

00:00:13,559 --> 00:00:18,720
trying to write music but he wants a

00:00:16,470 --> 00:00:21,660
computer to do it instead so so he'll

00:00:18,720 --> 00:00:31,410
work us through how that works so so

00:00:21,660 --> 00:00:34,380
welcome Kendrick hello oh this is

00:00:31,410 --> 00:00:36,750
working so I would like to first start

00:00:34,380 --> 00:00:39,059
off this by saying that this isn't a

00:00:36,750 --> 00:00:41,579
sponsor talk by PI touch of Facebook but

00:00:39,059 --> 00:00:48,860
if you guys are out there I wouldn't

00:00:41,579 --> 00:00:48,860
mind some sponsorships so Who am I I'm

00:00:49,340 --> 00:00:54,360
I'm Kendrick and my languages of choice

00:00:52,050 --> 00:00:57,120
are Python in Haskell and I'm currently

00:00:54,360 --> 00:00:58,859
pursuing my bachelor's of IT at the

00:00:57,120 --> 00:01:00,750
Queensland University of Technology it's

00:00:58,859 --> 00:01:04,049
a kind of work in progress at the moment

00:01:00,750 --> 00:01:06,090
might not finish it I don't know so my

00:01:04,049 --> 00:01:08,100
day job I work as a machine learning

00:01:06,090 --> 00:01:10,140
engineer at pop cottoned on AI and we

00:01:08,100 --> 00:01:14,040
try and kind of teach computers how to

00:01:10,140 --> 00:01:16,740
interpret music and responds interpret

00:01:14,040 --> 00:01:18,210
and responds music so even though my day

00:01:16,740 --> 00:01:20,520
job sounds fancy

00:01:18,210 --> 00:01:23,009
this is this xkcd comic kind of reflects

00:01:20,520 --> 00:01:25,380
how I feel I'm just kind of stirring a

00:01:23,009 --> 00:01:32,130
big pile of matrices to try and fit my

00:01:25,380 --> 00:01:34,740
data in so what this talk covers is why

00:01:32,130 --> 00:01:36,659
you should use PI torch and not say the

00:01:34,740 --> 00:01:39,780
sixteen different other frameworks out

00:01:36,659 --> 00:01:43,590
there like tenza flow MX net Caffey t

00:01:39,780 --> 00:01:46,770
know and so on what this talk isn't

00:01:43,590 --> 00:01:48,960
covering however is me explaining back

00:01:46,770 --> 00:01:53,130
propagation regression matrix

00:01:48,960 --> 00:01:55,560
multiplication and so on I don't claim

00:01:53,130 --> 00:01:57,180
to know everything so if you feel like I

00:01:55,560 --> 00:01:59,549
got something wrong or you want to

00:01:57,180 --> 00:02:01,469
clarify in anything feel free to hackle

00:01:59,549 --> 00:02:06,149
me but I would appreciate if you don't

00:02:01,469 --> 00:02:08,940
harass me so these are a couple of

00:02:06,149 --> 00:02:11,640
thoughts that I a granade it from the

00:02:08,940 --> 00:02:14,220
machine learning subreddit and thoughts

00:02:11,640 --> 00:02:16,770
on using pipe arch and the bottle 1x

00:02:14,220 --> 00:02:19,140
resonates with me so much when of our

00:02:16,770 --> 00:02:22,460
right tensorflow code it's great when

00:02:19,140 --> 00:02:26,160
you're not trying to reinvent the wheel

00:02:22,460 --> 00:02:28,380
so that many things today that I could

00:02:26,160 --> 00:02:30,980
talk about pi touch but today I just

00:02:28,380 --> 00:02:35,040
want to focus on these five topics a

00:02:30,980 --> 00:02:39,090
standardized minimal abstraction API

00:02:35,040 --> 00:02:43,410
this results in a non leaky abstraction

00:02:39,090 --> 00:02:46,290
allows you to reason to easily reason

00:02:43,410 --> 00:02:48,870
with your code so all you research is

00:02:46,290 --> 00:02:51,270
out there I love your work but the code

00:02:48,870 --> 00:02:51,810
you write is really really hard to

00:02:51,270 --> 00:02:54,420
follow

00:02:51,810 --> 00:02:56,700
you have you have a class get that get

00:02:54,420 --> 00:02:58,440
get that gets passed into a function and

00:02:56,700 --> 00:03:00,660
the function implicitly mutates the

00:02:58,440 --> 00:03:02,610
class's state and then you call us

00:03:00,660 --> 00:03:05,490
several subclass functions to mutate the

00:03:02,610 --> 00:03:07,680
state again it's just so hot to follow I

00:03:05,490 --> 00:03:10,520
suspect even worse when it's written in

00:03:07,680 --> 00:03:12,810
tensor flow it's just so hard to follow

00:03:10,520 --> 00:03:15,150
but with PI torch you're able to

00:03:12,810 --> 00:03:17,820
minimize that the kind of layers of

00:03:15,150 --> 00:03:21,600
abstraction and I have clear and concise

00:03:17,820 --> 00:03:24,959
code multi-gpu support and PI torch is

00:03:21,600 --> 00:03:26,910
easy as it should be and for those of

00:03:24,959 --> 00:03:30,480
you coming from the numpy or sigh pile

00:03:26,910 --> 00:03:33,180
and PI clutch has deep integrations with

00:03:30,480 --> 00:03:36,180
PI or with Python so using PI torch

00:03:33,180 --> 00:03:38,040
should feel like second nature to you a

00:03:36,180 --> 00:03:40,230
big part in machine learning is actually

00:03:38,040 --> 00:03:42,930
the data cleaning data processing data

00:03:40,230 --> 00:03:46,200
augmentation stage and PI coach actually

00:03:42,930 --> 00:03:48,090
has some pre it actually has some

00:03:46,200 --> 00:03:49,739
pre-built tools that allows you to

00:03:48,090 --> 00:03:52,410
leverage it so you can use your custom

00:03:49,739 --> 00:03:54,739
data set with ease including the

00:03:52,410 --> 00:03:57,120
documentation and pre-processing stage

00:03:54,739 --> 00:03:59,519
Python texture actually runs in

00:03:57,120 --> 00:04:02,730
something called a dynamic computation

00:03:59,519 --> 00:04:04,380
graph which is different from static

00:04:02,730 --> 00:04:09,060
computation graphs and I'll talk a bit

00:04:04,380 --> 00:04:10,860
more on that later on so this picture

00:04:09,060 --> 00:04:14,100
this network picture is just to give you

00:04:10,860 --> 00:04:15,840
guys a visualization on the neural net

00:04:14,100 --> 00:04:18,479
near a network that will be building

00:04:15,840 --> 00:04:21,450
later on in PI torch to familiarize

00:04:18,479 --> 00:04:24,960
yourself of you guys with PI torches API

00:04:21,450 --> 00:04:27,960
we have 28 by 20 as the input 32 has a

00:04:24,960 --> 00:04:29,520
hidden and tendancy output the

00:04:27,960 --> 00:04:31,530
numbers for the hidden layer which shows

00:04:29,520 --> 00:04:36,000
an arbitrarily so pay no attention to

00:04:31,530 --> 00:04:39,479
that and this is this is how you define

00:04:36,000 --> 00:04:42,060
a fully connected Network in pi coach

00:04:39,479 --> 00:04:44,880
which is what we did which is what is

00:04:42,060 --> 00:04:46,620
shown in this picture so on the left

00:04:44,880 --> 00:04:48,900
hand side you can see the network

00:04:46,620 --> 00:04:50,430
architecture being declared and on the

00:04:48,900 --> 00:04:53,610
right hand side you can see being

00:04:50,430 --> 00:04:56,039
constructed and called so to define your

00:04:53,610 --> 00:04:58,020
network you're a near enough work your

00:04:56,039 --> 00:05:03,599
class needs to inherit inherit from

00:04:58,020 --> 00:05:06,960
porch NN module and if you can see here

00:05:03,599 --> 00:05:11,250
I'm defining two class members in the

00:05:06,960 --> 00:05:13,710
constructor FC 1 and FC 2 now FC 1 is a

00:05:11,250 --> 00:05:16,740
linear linear unit a linear layer which

00:05:13,710 --> 00:05:19,590
takes in 28 by 28 and output study 2

00:05:16,740 --> 00:05:23,669
which is the first half of this this

00:05:19,590 --> 00:05:25,860
network architecture and FC 2 is a

00:05:23,669 --> 00:05:28,919
linear unit which takes in 32 and

00:05:25,860 --> 00:05:35,970
outputs 10 which is the second half of

00:05:28,919 --> 00:05:37,800
this of this network and you can see so

00:05:35,970 --> 00:05:41,070
when the class is being called it

00:05:37,800 --> 00:05:43,740
actually invokes the Follet function and

00:05:41,070 --> 00:05:47,430
you pass in the variable X as your input

00:05:43,740 --> 00:05:51,360
as you can see here input is X and so

00:05:47,430 --> 00:05:55,500
you pass the input through FC 1 and you

00:05:51,360 --> 00:05:58,620
returned a 30 to length tensile and X is

00:05:55,500 --> 00:06:01,139
passed again into FC 2 and your return X

00:05:58,620 --> 00:06:06,360
which is essentially what this picture

00:06:01,139 --> 00:06:08,219
does as a whole and so the issue with

00:06:06,360 --> 00:06:09,780
this code is that it only runs in CPU

00:06:08,219 --> 00:06:14,159
it's not crueller support it doesn't

00:06:09,780 --> 00:06:20,280
isn't supported on GPU just to make a

00:06:14,159 --> 00:06:24,780
GPU compatible what you do is call CUDA

00:06:20,280 --> 00:06:28,560
and that's it that is all you do to turn

00:06:24,780 --> 00:06:30,060
your CPU network into a GPU compatible

00:06:28,560 --> 00:06:32,699
network how easy is that

00:06:30,060 --> 00:06:34,979
that that that would have took me at

00:06:32,699 --> 00:06:37,279
least two hours intensive law but in

00:06:34,979 --> 00:06:40,020
Pike watch all I do is called dot cruda

00:06:37,279 --> 00:06:42,690
now this one more issue with this is

00:06:40,020 --> 00:06:44,550
this is only supported on one GPU but

00:06:42,690 --> 00:06:48,900
say you have you want to paralyze it

00:06:44,550 --> 00:06:51,570
what would you do and then you kind of

00:06:48,900 --> 00:06:54,120
encapsulate in any other parallel and

00:06:51,570 --> 00:06:57,120
that is it that is how you have multi

00:06:54,120 --> 00:07:00,900
GPU support for PI torch on networks

00:06:57,120 --> 00:07:04,099
look how easy it is multi-gpu GPU now

00:07:00,900 --> 00:07:07,339
CPU and I'm back to GP multi chief you

00:07:04,099 --> 00:07:07,339
so easy

00:07:08,909 --> 00:07:16,260
so again PI coach has deep integrations

00:07:12,719 --> 00:07:19,589
with Python and so say you have the

00:07:16,260 --> 00:07:21,810
output so now you have your output you

00:07:19,589 --> 00:07:24,599
want to say turning into numpy array for

00:07:21,810 --> 00:07:27,599
whatever reason and chuck it into the

00:07:24,599 --> 00:07:28,440
csv or something what you do is called

00:07:27,599 --> 00:07:30,449
dejara

00:07:28,440 --> 00:07:34,010
but numpy and you return a numpy array

00:07:30,449 --> 00:07:37,349
but if you're on CUDA you call dot CPU

00:07:34,010 --> 00:07:39,960
numpy that's it oh there's actually a

00:07:37,349 --> 00:07:51,029
typo here sorry if it's if CUDA you call

00:07:39,960 --> 00:07:54,080
dot CPU numpy sorry and that's how you

00:07:51,029 --> 00:07:56,729
turn your tensor into a numpy array and

00:07:54,080 --> 00:07:58,889
to turn your numpy array into tensors

00:07:56,729 --> 00:08:00,870
all you do is call torch dot from numpy

00:07:58,889 --> 00:08:03,570
and your numpy and your returned a

00:08:00,870 --> 00:08:06,449
tensor again these features are baked

00:08:03,570 --> 00:08:08,969
into the framework for your convenience

00:08:06,449 --> 00:08:11,849
you don't need to go to like a third

00:08:08,969 --> 00:08:14,099
party contract library to to streamline

00:08:11,849 --> 00:08:19,199
this process it's built into a framework

00:08:14,099 --> 00:08:21,180
so a big part of machine learning again

00:08:19,199 --> 00:08:24,509
is the data pre-processing and data

00:08:21,180 --> 00:08:26,610
augmentation stage in pi touch land this

00:08:24,509 --> 00:08:29,190
is how it works you have a data set

00:08:26,610 --> 00:08:32,490
which you say you have a list of x and

00:08:29,190 --> 00:08:34,560
y's which sits inside a data loader the

00:08:32,490 --> 00:08:36,839
dada loaders job is to augment and

00:08:34,560 --> 00:08:39,569
pre-process the data from either house

00:08:36,839 --> 00:08:42,149
and batch them up and put and kind of

00:08:39,569 --> 00:08:44,810
feed it into your model so normally if

00:08:42,149 --> 00:08:46,949
you want to ride a multi multi-threaded

00:08:44,810 --> 00:08:49,319
pre-processing utility it could take you

00:08:46,949 --> 00:08:52,470
a couple I don't know for me it took me

00:08:49,319 --> 00:08:57,029
a couple of days but in Python it's very

00:08:52,470 --> 00:08:58,139
very easy to do so you define to define

00:08:57,029 --> 00:08:59,790
your custom data set

00:08:58,139 --> 00:09:03,079
you just need three special method

00:08:59,790 --> 00:09:07,139
functions the constructor get item and

00:09:03,079 --> 00:09:09,120
length so the constructor and and and

00:09:07,139 --> 00:09:12,920
your class needs to inherit from taught

00:09:09,120 --> 00:09:15,329
you to yuto's the data the data set and

00:09:12,920 --> 00:09:17,999
there you have it you have your custom

00:09:15,329 --> 00:09:20,309
data set ready to be put into your data

00:09:17,999 --> 00:09:24,660
loader but say you want to populate your

00:09:20,309 --> 00:09:29,519
data set all you do is this in your init

00:09:24,660 --> 00:09:32,490
function you supply say say X and

00:09:29,519 --> 00:09:35,339
transforms and transforms is a

00:09:32,490 --> 00:09:36,930
pre-processing utility which is built by

00:09:35,339 --> 00:09:38,879
the PI thoughts team which I'll talk

00:09:36,930 --> 00:09:40,889
about more in the latest slide so as you

00:09:38,879 --> 00:09:42,750
can see I'm just defining my class

00:09:40,889 --> 00:09:45,720
variable X as X and transforms as

00:09:42,750 --> 00:09:48,839
transforms and when you index it it gets

00:09:45,720 --> 00:09:52,379
X at that index and if you defined a

00:09:48,839 --> 00:09:55,079
pre-processing or data augmentation step

00:09:52,379 --> 00:09:58,050
it up mends and all pre process your

00:09:55,079 --> 00:09:58,410
data and returns X the length of your

00:09:58,050 --> 00:10:00,269
data

00:09:58,410 --> 00:10:04,980
well length just is just the length of

00:10:00,269 --> 00:10:07,589
your data so yeah so going on to touch

00:10:04,980 --> 00:10:10,559
vision so it transforms transforms is

00:10:07,589 --> 00:10:13,139
actually a is actually a pain from torch

00:10:10,559 --> 00:10:15,959
vision which is a computer vision

00:10:13,139 --> 00:10:18,899
specific pre-processing library there's

00:10:15,959 --> 00:10:23,009
also text and audio coming along and all

00:10:18,899 --> 00:10:25,050
done by the amazing PI coach team so you

00:10:23,009 --> 00:10:27,230
can compose your transformations so for

00:10:25,050 --> 00:10:30,629
example Oh again sorry transforms

00:10:27,230 --> 00:10:32,100
contains the commonly used pre

00:10:30,629 --> 00:10:34,829
processing and data augmentation

00:10:32,100 --> 00:10:37,769
functions like scaling converting your

00:10:34,829 --> 00:10:41,009
pillow array to a tensor flipping your

00:10:37,769 --> 00:10:44,009
image or randomly cropping it randomly

00:10:41,009 --> 00:10:46,290
flipping it and so on so you can compose

00:10:44,009 --> 00:10:48,870
multiple transforms so say I want to

00:10:46,290 --> 00:10:50,280
scale an image and I want to transform

00:10:48,870 --> 00:10:53,670
it to a tensor

00:10:50,280 --> 00:10:58,410
so all I can compose it into t2 and I

00:10:53,670 --> 00:11:01,290
pass t2 into my custom data set so now

00:10:58,410 --> 00:11:04,530
when you call D set - it's actually it

00:11:01,290 --> 00:11:07,050
actually returns you a 28 by 28 tensor

00:11:04,530 --> 00:11:10,740
but if you call D set one it returns you

00:11:07,050 --> 00:11:15,120
an n-by-n tensor so now we have our

00:11:10,740 --> 00:11:18,630
custom data set what do we do how do we

00:11:15,120 --> 00:11:22,170
put it into data loader we just put it

00:11:18,630 --> 00:11:27,540
in as a special keyword argument s data

00:11:22,170 --> 00:11:30,530
data set and your and your custom data

00:11:27,540 --> 00:11:34,110
set is now multi-threaded by default and

00:11:30,530 --> 00:11:37,470
whatever transformations you define will

00:11:34,110 --> 00:11:41,130
be performed during runtime in multiple

00:11:37,470 --> 00:11:44,280
threads and that is that your custom

00:11:41,130 --> 00:11:46,230
data set can now be can now be utilized

00:11:44,280 --> 00:11:48,650
and put into your model so all you do

00:11:46,230 --> 00:11:50,940
use your data loaded all you do now is

00:11:48,650 --> 00:11:54,480
enumerate over your data loader

00:11:50,940 --> 00:11:58,400
and you can sort of just pass X or Y's

00:11:54,480 --> 00:11:58,400
into your data into your model so

00:12:01,040 --> 00:12:08,160
dynamic graph computation or DGC is what

00:12:05,820 --> 00:12:09,810
pipe which runs on as opposed to static

00:12:08,160 --> 00:12:12,540
graph computation which is what its

00:12:09,810 --> 00:12:15,630
cousins run on such as again tense flow

00:12:12,540 --> 00:12:19,080
caffeine or torch so a really good way

00:12:15,630 --> 00:12:23,580
to describe PI torch is that you define

00:12:19,080 --> 00:12:26,670
your network by running it but static

00:12:23,580 --> 00:12:30,420
static graph computation you define your

00:12:26,670 --> 00:12:33,750
network and then run it so it's really

00:12:30,420 --> 00:12:38,040
hot to kind of visualize this process so

00:12:33,750 --> 00:12:40,589
I included a jiff so this is stolen from

00:12:38,040 --> 00:12:43,170
the PI torch website and so what you do

00:12:40,589 --> 00:12:45,030
is you kind of you kind of define your

00:12:43,170 --> 00:12:47,430
variables and describe how to interact

00:12:45,030 --> 00:12:50,430
with each other and the graph is created

00:12:47,430 --> 00:12:52,980
during interpretation rather than being

00:12:50,430 --> 00:12:55,440
interpreted then created and you might

00:12:52,980 --> 00:13:00,420
ask me why what's the benefit of using

00:12:55,440 --> 00:13:03,900
the GC over SGC and here's a couple

00:13:00,420 --> 00:13:06,180
answers well for starters dynamic graph

00:13:03,900 --> 00:13:08,550
is actually really easier to debug than

00:13:06,180 --> 00:13:10,530
static rough computation if your little

00:13:08,550 --> 00:13:12,330
black box near a network suddenly during

00:13:10,530 --> 00:13:14,730
our intense flow you'll be greeted with

00:13:12,330 --> 00:13:16,380
like a 10 lying stack trace which is

00:13:14,730 --> 00:13:20,720
quite an ideal when you're trying to

00:13:16,380 --> 00:13:24,240
debug it and you have limited time

00:13:20,720 --> 00:13:27,780
unpackage oh oh in any other framework

00:13:24,240 --> 00:13:31,050
that utilizes the GC you actually you

00:13:27,780 --> 00:13:32,820
know which file and on which line your

00:13:31,050 --> 00:13:34,800
network failed on which is really handy

00:13:32,820 --> 00:13:36,810
and saves a lot of man-hours

00:13:34,800 --> 00:13:41,970
you're able to process inputs of

00:13:36,810 --> 00:13:48,150
variable sizes in DGC which is really

00:13:41,970 --> 00:13:50,390
which is which is kind of like key when

00:13:48,150 --> 00:13:53,790
you're doing the machine learning for

00:13:50,390 --> 00:13:57,390
text and audio there is no tensorflow

00:13:53,790 --> 00:13:58,710
sessions absolutely none and you can do

00:13:57,390 --> 00:14:00,270
some interesting stuff such as

00:13:58,710 --> 00:14:05,700
manipulate your grade in this during a

00:14:00,270 --> 00:14:08,390
runtime so this piece of code snippet if

00:14:05,700 --> 00:14:11,280
you include it into your training logic

00:14:08,390 --> 00:14:13,470
you can clip away the gradients to

00:14:11,280 --> 00:14:15,150
prevent great and explosion which is

00:14:13,470 --> 00:14:17,880
quite handy when you're trying to train

00:14:15,150 --> 00:14:23,730
something that is really unstable during

00:14:17,880 --> 00:14:25,530
the initial stages like again so one of

00:14:23,730 --> 00:14:28,530
the most common questions that I get

00:14:25,530 --> 00:14:30,570
asked from people want to use pocket

00:14:28,530 --> 00:14:33,060
watches how can I visualize my training

00:14:30,570 --> 00:14:34,140
output or progress because there's ten

00:14:33,060 --> 00:14:36,660
support to tensorflow

00:14:34,140 --> 00:14:39,360
so what's there to vist what's that's a

00:14:36,660 --> 00:14:42,330
tie coach well the nice folks at

00:14:39,360 --> 00:14:44,490
Facebook actually created a library

00:14:42,330 --> 00:14:46,980
cursory a tool a very powerful

00:14:44,490 --> 00:14:49,200
visualization toolkit called vishton

00:14:46,980 --> 00:14:51,870
what's great about it is it's platform

00:14:49,200 --> 00:14:56,670
agnostic and you can communicate via

00:14:51,870 --> 00:14:59,700
HTTP rest which is really handy so this

00:14:56,670 --> 00:15:01,880
is a nice fish nice drif and how it

00:14:59,700 --> 00:15:01,880
works

00:15:14,360 --> 00:15:22,250
so yeah you have vishton 4pi torch how

00:15:19,800 --> 00:15:22,250
do I go back

00:15:25,100 --> 00:15:31,380
so in summary why you should use PI

00:15:28,140 --> 00:15:33,390
torch it has a non leaky API and allows

00:15:31,380 --> 00:15:35,330
either easier reason about your code

00:15:33,390 --> 00:15:37,920
especially when it's a black box code

00:15:35,330 --> 00:15:39,750
commonly used features are baked into

00:15:37,920 --> 00:15:42,150
the framework for your convenience like

00:15:39,750 --> 00:15:44,730
converting from a tensor to numpy enum

00:15:42,150 --> 00:15:47,460
p2 tensor documentation data

00:15:44,730 --> 00:15:49,740
pre-processing multi-threading and so on

00:15:47,460 --> 00:15:51,270
I understand a community is a really big

00:15:49,740 --> 00:15:52,590
part of whichever framework we choose

00:15:51,270 --> 00:15:54,630
and it has a really vibrant and

00:15:52,590 --> 00:15:57,120
supporting community on the both on the

00:15:54,630 --> 00:16:01,140
forums and on the slack Channel plus it

00:15:57,120 --> 00:16:04,910
has really good Doc's and yeah that's it

00:16:01,140 --> 00:16:04,910
any questions

00:16:15,160 --> 00:16:25,040
sorry what this one yeah something you

00:16:24,110 --> 00:16:26,750
mentioned here that you actually

00:16:25,040 --> 00:16:29,090
automatically get the multi-threaded

00:16:26,750 --> 00:16:31,340
yeah that I mean not native threads you

00:16:29,090 --> 00:16:32,090
mean Python threads right I'm not too

00:16:31,340 --> 00:16:37,670
sure about that

00:16:32,090 --> 00:16:41,240
but it's nothing threaded yeah I haven't

00:16:37,670 --> 00:16:43,810
even looked into it but yeah all right

00:16:41,240 --> 00:16:54,200
any other questions

00:16:43,810 --> 00:16:56,480
yep thanks for good talk

00:16:54,200 --> 00:16:59,180
so you talked about why it's good to use

00:16:56,480 --> 00:17:00,290
part touch what what why would you what

00:16:59,180 --> 00:17:05,390
are some of its shortcomings or why

00:17:00,290 --> 00:17:07,970
might you not use it well first it's not

00:17:05,390 --> 00:17:10,520
a lot of the document it's kind of still

00:17:07,970 --> 00:17:13,209
in beta mode so the the API actually

00:17:10,520 --> 00:17:15,470
changes quite a bit and you have

00:17:13,209 --> 00:17:18,230
existing modules which gets put into

00:17:15,470 --> 00:17:19,850
legacy modules so if you if you want to

00:17:18,230 --> 00:17:21,520
run on the bleeding edge it shouldn't be

00:17:19,850 --> 00:17:24,589
an issue but if you want a stable

00:17:21,520 --> 00:17:25,910
production level sort of system I

00:17:24,589 --> 00:17:27,110
wouldn't recommend using it but if

00:17:25,910 --> 00:17:29,650
you're doing research I would highly

00:17:27,110 --> 00:17:32,060
recommend it as for the shortcomings

00:17:29,650 --> 00:17:36,230
other than that other than the changing

00:17:32,060 --> 00:17:38,690
API I haven't personally oh there's not

00:17:36,230 --> 00:17:41,030
a lot of that's not a lot of what guides

00:17:38,690 --> 00:17:43,220
on how to use it online when you compare

00:17:41,030 --> 00:17:45,320
it to 10th floor that's pretty much it

00:17:43,220 --> 00:17:49,400
it's more at the community rather than

00:17:45,320 --> 00:17:50,740
the framework itself cool any other

00:17:49,400 --> 00:17:58,160
questions

00:17:50,740 --> 00:18:00,230
yep sorry so you just said that it was

00:17:58,160 --> 00:18:02,780
kind of beta mode is there a date that

00:18:00,230 --> 00:18:04,910
you guys are working towards before when

00:18:02,780 --> 00:18:08,630
you'll like PI on releasing a stable

00:18:04,910 --> 00:18:11,900
version or is it do you know if there is

00:18:08,630 --> 00:18:13,970
a date so there's monthly releases on

00:18:11,900 --> 00:18:16,910
the websites but it's still in beta so

00:18:13,970 --> 00:18:19,430
it's very rocky very very unstable as in

00:18:16,910 --> 00:18:21,170
the API but you can fix that by kind of

00:18:19,430 --> 00:18:25,100
specifying a bush and what version PI

00:18:21,170 --> 00:18:27,560
torch you want to use ok so do you know

00:18:25,100 --> 00:18:28,490
if there's a plan for a 1.0 or something

00:18:27,560 --> 00:18:35,059
like that where it starts

00:18:28,490 --> 00:18:37,820
no no okay any other questions I've got

00:18:35,059 --> 00:18:40,490
one so it was probably just because you

00:18:37,820 --> 00:18:42,679
showed us the EZ in your network but it

00:18:40,490 --> 00:18:45,860
seems to me that if you're defining the

00:18:42,679 --> 00:18:49,010
network in you know pash partial PI

00:18:45,860 --> 00:18:50,420
torch and partial Python code so for the

00:18:49,010 --> 00:18:52,580
forward function here you can imagine if

00:18:50,420 --> 00:18:54,320
you've got tens of layers you'll write a

00:18:52,580 --> 00:18:56,480
loop and then everything goes through a

00:18:54,320 --> 00:18:58,100
Python loop does that cause issues or do

00:18:56,480 --> 00:19:01,040
you have to use other PI torch classes

00:18:58,100 --> 00:19:03,200
to it so there's actually a module

00:19:01,040 --> 00:19:05,600
called nn-not sequential and so it's

00:19:03,200 --> 00:19:08,480
like you can compose multiple multiple

00:19:05,600 --> 00:19:09,950
layers together which if you look at the

00:19:08,480 --> 00:19:11,210
source code is actually just a for loop

00:19:09,950 --> 00:19:13,670
but yeah

00:19:11,210 --> 00:19:15,440
yeah because that's one of the one of

00:19:13,670 --> 00:19:16,760
the key bits about ends flow and numpy

00:19:15,440 --> 00:19:18,080
and stuff like that is try to push

00:19:16,760 --> 00:19:19,940
everything down to those lower layers

00:19:18,080 --> 00:19:22,130
and try to come up to python as as

00:19:19,940 --> 00:19:24,050
infrequently as possible yeah so this

00:19:22,130 --> 00:19:26,420
kind of abstracts away that concept so

00:19:24,050 --> 00:19:30,500
which is which is why I love it so you

00:19:26,420 --> 00:19:34,790
rarely see that rarely awesome all right

00:19:30,500 --> 00:19:37,240
any final questions No okay well thank

00:19:34,790 --> 00:19:37,240
you very much

00:19:42,119 --> 00:19:44,179

YouTube URL: https://www.youtube.com/watch?v=ICMsWq7c5Z8


