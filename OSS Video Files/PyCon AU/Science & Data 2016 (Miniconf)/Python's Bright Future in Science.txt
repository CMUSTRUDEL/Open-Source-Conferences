Title: Python's Bright Future in Science
Publication date: 2016-08-16
Playlist: Science & Data 2016 (Miniconf)
Description: 
	Juan Nunez-Iglesias
https://2016.pycon-au.org/schedule/200/view_talk
Over the past five years, Python has skyrocketed in popularity in the scientific world, pushing out proprietary languages such as IDL and Matlab. This rise was powered by simple syntax and efficient numerical libraries. But many operations in Python are still slow, and upstart languages, such as Julia and Go, promise simplicity *and* speed. Can Python cement its place in scientific computing?
Captions: 
	00:00:00,000 --> 00:00:06,839
all right hi everyone thanks for coming

00:00:03,560 --> 00:00:08,700
so I guess all of you are either a

00:00:06,839 --> 00:00:11,940
scientist or interested in science since

00:00:08,700 --> 00:00:13,469
while you would be here and as a

00:00:11,940 --> 00:00:16,109
scientist at various times in your

00:00:13,469 --> 00:00:17,430
career you might think I want to learn

00:00:16,109 --> 00:00:21,539
programming what programming language

00:00:17,430 --> 00:00:23,400
should I learn is that Python or I'm

00:00:21,539 --> 00:00:24,840
getting pretty good Python but some

00:00:23,400 --> 00:00:27,570
things are hard some things are slow

00:00:24,840 --> 00:00:30,660
should I go and learn this other new

00:00:27,570 --> 00:00:32,820
language or if you're like me you might

00:00:30,660 --> 00:00:34,860
say I'm massively invested in Python by

00:00:32,820 --> 00:00:38,790
now how do I convince as many people as

00:00:34,860 --> 00:00:42,480
possible to buy into Python and then

00:00:38,790 --> 00:00:44,280
this came along so no but in reality we

00:00:42,480 --> 00:00:47,160
we all face the question of do I keep

00:00:44,280 --> 00:00:50,399
using what I'm using or do I pivot to

00:00:47,160 --> 00:00:52,110
this new and shiny thing and so

00:00:50,399 --> 00:00:53,820
periodically you might read some news

00:00:52,110 --> 00:00:56,070
you know hacker news it reddit or

00:00:53,820 --> 00:00:58,920
whatever and I'm trying to figure out

00:00:56,070 --> 00:01:04,280
the answer to that question so let's

00:00:58,920 --> 00:01:04,280
start with a Google search so

00:01:04,489 --> 00:01:10,590
there are the two questions that I'm

00:01:06,689 --> 00:01:14,060
going to try to talk about today so why

00:01:10,590 --> 00:01:17,549
is it so popular and why is it so slow

00:01:14,060 --> 00:01:20,939
so for the first question let's see how

00:01:17,549 --> 00:01:25,259
popular it actually is in science so a

00:01:20,939 --> 00:01:28,170
couple years ago said my Robitaille and

00:01:25,259 --> 00:01:31,409
Chris Bowman did an analysis of Python

00:01:28,170 --> 00:01:33,360
mentions in astronomy papers and so I

00:01:31,409 --> 00:01:37,380
took that analysis they published it as

00:01:33,360 --> 00:01:39,630
a Jupiter nut book and I took that and

00:01:37,380 --> 00:01:42,569
updated it and this is what it looks

00:01:39,630 --> 00:01:44,369
like so that's a proportion of papers

00:01:42,569 --> 00:01:46,049
mentioning each of these programming

00:01:44,369 --> 00:01:49,020
languages and you can see if I thumb is

00:01:46,049 --> 00:01:50,819
just really I mean this actually be my

00:01:49,020 --> 00:01:55,229
expected to see just how popular it was

00:01:50,819 --> 00:02:03,119
becoming and at this rate in ten years

00:01:55,229 --> 00:02:04,289
two hundred percent of papers will so

00:02:03,119 --> 00:02:08,910
the next question is why is it so

00:02:04,289 --> 00:02:11,730
popular and now I'm just going to be

00:02:08,910 --> 00:02:13,530
arguing I don't have data but I think

00:02:11,730 --> 00:02:15,650
the first reason is the community of

00:02:13,530 --> 00:02:21,599
scientific Python and it really is

00:02:15,650 --> 00:02:24,360
excellent so to illustrate this is my

00:02:21,599 --> 00:02:28,290
first ever pull request that's my second

00:02:24,360 --> 00:02:31,950
one both of those happened at the SyFy

00:02:28,290 --> 00:02:35,220
2012 conference where I was you know

00:02:31,950 --> 00:02:37,260
just a user of Python I'd never made any

00:02:35,220 --> 00:02:39,900
open source contributions whatsoever

00:02:37,260 --> 00:02:44,840
just you know written very scripts for

00:02:39,900 --> 00:02:46,980
my research and when I gave a talk there

00:02:44,840 --> 00:02:51,090
Stephan wonderful to create a second

00:02:46,980 --> 00:02:52,680
image said hey why don't you join us and

00:02:51,090 --> 00:02:54,989
they talked me through the whole open

00:02:52,680 --> 00:02:59,489
source contribution project process and

00:02:54,989 --> 00:03:02,220
so on so one of the take homes of this

00:02:59,489 --> 00:03:03,299
keynote is come to this princey don't

00:03:02,220 --> 00:03:04,500
think you're not good enough because I

00:03:03,299 --> 00:03:06,930
definitely thought I was not good enough

00:03:04,500 --> 00:03:12,700
at the time and you just learned a lot

00:03:06,930 --> 00:03:15,430
and and you can contribute so

00:03:12,700 --> 00:03:18,280
the next reason for Python success that

00:03:15,430 --> 00:03:19,720
I want to talk about is elegance and you

00:03:18,280 --> 00:03:22,660
can see I have a thing about it based on

00:03:19,720 --> 00:03:24,730
my book but I think Python is popular

00:03:22,660 --> 00:03:27,819
because it's easy to write and it's easy

00:03:24,730 --> 00:03:30,610
to read and it was born as a teaching

00:03:27,819 --> 00:03:33,760
language and I think it really shows and

00:03:30,610 --> 00:03:36,790
it has a style guide pepe which is

00:03:33,760 --> 00:03:38,440
universally accepted and so python code

00:03:36,790 --> 00:03:41,050
is remarkably consistent you can go and

00:03:38,440 --> 00:03:43,870
read some code online and everything

00:03:41,050 --> 00:03:47,500
will be instantly familiar to you so

00:03:43,870 --> 00:03:51,269
this is you know how do you find a

00:03:47,500 --> 00:03:55,360
pattern in a file for line and file if

00:03:51,269 --> 00:03:57,310
pattern in line yield line you can it

00:03:55,360 --> 00:04:00,040
almost reads like an English language

00:03:57,310 --> 00:04:03,550
description of what you want to do and

00:04:00,040 --> 00:04:06,010
you could really not remove anything

00:04:03,550 --> 00:04:08,050
there and make it clearer right so it's

00:04:06,010 --> 00:04:11,470
it's really as compact as possible while

00:04:08,050 --> 00:04:15,489
still being readable and of course it is

00:04:11,470 --> 00:04:17,049
possible to write obtuse Python code but

00:04:15,489 --> 00:04:19,329
I think Python makes it easy to write

00:04:17,049 --> 00:04:22,660
very clear code there's very little

00:04:19,329 --> 00:04:24,970
extraneous syntax that gets in the way

00:04:22,660 --> 00:04:31,840
of what you want to ride so curly braces

00:04:24,970 --> 00:04:34,840
for example here's a example using numpy

00:04:31,840 --> 00:04:36,820
inside pi so quantile normalization is a

00:04:34,840 --> 00:04:39,870
statistical technique to make all of

00:04:36,820 --> 00:04:43,770
your data fit a particular distribution

00:04:39,870 --> 00:04:48,490
and you can do it in three lines of

00:04:43,770 --> 00:04:51,540
Sipan numpy so you sort your do we have

00:04:48,490 --> 00:04:55,630
a laser pointer of some description

00:04:51,540 --> 00:05:01,450
maybe not all right let me see if I can

00:04:55,630 --> 00:05:04,680
good all right anyway first line we're

00:05:01,450 --> 00:05:07,960
taking we're sorting our input data

00:05:04,680 --> 00:05:12,070
column wise and then we take the mean

00:05:07,960 --> 00:05:14,200
across the rows awesome thank you great

00:05:12,070 --> 00:05:15,940
and then we take the mean across the

00:05:14,200 --> 00:05:19,840
rows of that so that gives us the

00:05:15,940 --> 00:05:22,330
average quantiles of our data and then

00:05:19,840 --> 00:05:27,030
we can use from the M stats package

00:05:22,330 --> 00:05:29,800
inside by rank data so we

00:05:27,030 --> 00:05:37,870
find the rank of each observation in

00:05:29,800 --> 00:05:40,810
every call and we're done so being able

00:05:37,870 --> 00:05:44,830
to produce code like this that takes up

00:05:40,810 --> 00:05:46,690
very little space and reads clearly is a

00:05:44,830 --> 00:05:47,320
huge advantage in debugging and

00:05:46,690 --> 00:05:49,900
correctness

00:05:47,320 --> 00:05:53,620
so NASA which produces the most bug-free

00:05:49,900 --> 00:05:55,750
code on earth has a style guide for free

00:05:53,620 --> 00:05:57,130
code and it's C code but a lot of the

00:05:55,750 --> 00:06:00,430
things apply to Python and one of them

00:05:57,130 --> 00:06:01,810
is keep your functions within sixty

00:06:00,430 --> 00:06:03,310
lines of code something that can fit on

00:06:01,810 --> 00:06:05,260
a screen because if you start to scroll

00:06:03,310 --> 00:06:06,880
up and down it's very hard for us to

00:06:05,260 --> 00:06:11,530
just keep track of of everything that's

00:06:06,880 --> 00:06:13,510
going on so it's really good that Python

00:06:11,530 --> 00:06:17,200
lets you write concise things like this

00:06:13,510 --> 00:06:21,100
so Python by itself is really good one

00:06:17,200 --> 00:06:22,660
of the design goals is easy things

00:06:21,100 --> 00:06:26,680
should be sorry common things should be

00:06:22,660 --> 00:06:31,120
easy and numpy and scifi are even better

00:06:26,680 --> 00:06:33,580
so we'll revisit that as well but python

00:06:31,120 --> 00:06:36,700
is not the only language that can write

00:06:33,580 --> 00:06:40,830
you can write elegant code in so here's

00:06:36,700 --> 00:06:43,240
an example from our in large part to

00:06:40,830 --> 00:06:45,400
Headley we complete player you can you

00:06:43,240 --> 00:06:47,950
get this pipe operator and that makes

00:06:45,400 --> 00:06:50,440
writing complex analysis and very few

00:06:47,950 --> 00:06:52,560
lines of code very easy so you can take

00:06:50,440 --> 00:06:57,510
the flight's data group by the date

00:06:52,560 --> 00:07:01,150
we're gonna select the arrival delays

00:06:57,510 --> 00:07:04,210
and then for each of those groups we're

00:07:01,150 --> 00:07:06,280
gonna summarize them by the median then

00:07:04,210 --> 00:07:09,580
we're gonna sort it by date and we're

00:07:06,280 --> 00:07:11,830
going to add a date and what day of the

00:07:09,580 --> 00:07:15,190
week and then we pipe that directly into

00:07:11,830 --> 00:07:21,490
our plot okay so in just that amount of

00:07:15,190 --> 00:07:23,830
code now I've got date versus the median

00:07:21,490 --> 00:07:27,060
delay and you can get a tremendous

00:07:23,830 --> 00:07:29,350
amount of information from your data so

00:07:27,060 --> 00:07:32,830
one of the things for example Christmas

00:07:29,350 --> 00:07:35,320
is this Tuesday here it's actually

00:07:32,830 --> 00:07:37,780
flying on Christmas is not a not a bad

00:07:35,320 --> 00:07:39,770
thing flying on the weekends so Saturday

00:07:37,780 --> 00:07:43,520
and Sunday mostly stay below

00:07:39,770 --> 00:07:46,280
you're aligned so that surprised me you

00:07:43,520 --> 00:07:49,370
can if you want to get on time you can

00:07:46,280 --> 00:07:52,250
fly in the weekend and similarly these

00:07:49,370 --> 00:07:56,360
are the weekdays before the fourth of

00:07:52,250 --> 00:07:58,250
July and after so as I said it's it's a

00:07:56,360 --> 00:08:03,830
lot of insight from from very little

00:07:58,250 --> 00:08:05,840
code and there's also Haskell so this is

00:08:03,830 --> 00:08:08,509
how you define the Fibonacci sequence in

00:08:05,840 --> 00:08:11,030
Haskell and this is just the definition

00:08:08,509 --> 00:08:14,270
of the Fibonacci sequence and yet it's

00:08:11,030 --> 00:08:18,830
also valid Haskell code so that's pretty

00:08:14,270 --> 00:08:21,860
amazing and this is a little bit of a

00:08:18,830 --> 00:08:24,650
slow implementation that's the fasten

00:08:21,860 --> 00:08:26,330
limitation so I think it's not just by

00:08:24,650 --> 00:08:31,520
Python is not the only game in town when

00:08:26,330 --> 00:08:33,560
it comes to elegance so if you want

00:08:31,520 --> 00:08:39,169
Python to go the distance you have to

00:08:33,560 --> 00:08:40,700
offer more than that right and so we

00:08:39,169 --> 00:08:44,089
also have to contend with the second

00:08:40,700 --> 00:08:47,540
part of the search which is why is it so

00:08:44,089 --> 00:08:49,940
slow and this is the fact that Google

00:08:47,540 --> 00:08:52,940
autocompletes with this is actually a

00:08:49,940 --> 00:08:55,370
huge statement although I have to say a

00:08:52,940 --> 00:08:59,170
year ago when I tried this so slow was

00:08:55,370 --> 00:09:01,160
was actually at the top so that's good

00:08:59,170 --> 00:09:03,140
but it's still it's still a big problem

00:09:01,160 --> 00:09:04,190
right as people are going on the

00:09:03,140 --> 00:09:06,500
internet and I'm wondering why their

00:09:04,190 --> 00:09:09,860
Python code is the best there's a

00:09:06,500 --> 00:09:11,329
website that's cut off sorry there's

00:09:09,860 --> 00:09:14,209
lights I'll put them up on the web so

00:09:11,329 --> 00:09:15,920
you can get all the references but yeah

00:09:14,209 --> 00:09:17,180
there's a benchmarks website where

00:09:15,920 --> 00:09:19,540
there's a whole bunch of benchmarks and

00:09:17,180 --> 00:09:21,649
you can compare languages

00:09:19,540 --> 00:09:23,500
implementations of certain problems in

00:09:21,649 --> 00:09:25,910
different languages and you can see here

00:09:23,500 --> 00:09:30,589
python is a hundred times slower than

00:09:25,910 --> 00:09:33,170
the C code and actually when I ran this

00:09:30,589 --> 00:09:35,899
on my own laptop it was 900 times slower

00:09:33,170 --> 00:09:37,190
than C code so maybe depending on the

00:09:35,899 --> 00:09:39,350
virtual machine that they're running or

00:09:37,190 --> 00:09:42,529
whatever the C can't take full advantage

00:09:39,350 --> 00:09:47,690
of their processors but yeah the

00:09:42,529 --> 00:09:51,410
difference is at least this and so why

00:09:47,690 --> 00:09:53,240
is that so let's look at a statement

00:09:51,410 --> 00:09:56,480
like a equals five

00:09:53,240 --> 00:09:56,839
so if you did it and see that's what it

00:09:56,480 --> 00:09:58,009
would be

00:09:56,839 --> 00:10:00,290
you're just making an integer there's

00:09:58,009 --> 00:10:05,179
gonna be a tiny bit of memory that this

00:10:00,290 --> 00:10:08,629
integer occupies in Python is what an

00:10:05,179 --> 00:10:10,759
integer object looks like and you've got

00:10:08,629 --> 00:10:12,619
a ref count and you've got a type and

00:10:10,759 --> 00:10:14,209
you've got the size of it and then

00:10:12,619 --> 00:10:17,540
finally you've got the actual data in

00:10:14,209 --> 00:10:19,459
there so every time that you create an

00:10:17,540 --> 00:10:21,829
integer you have to initialize this

00:10:19,459 --> 00:10:23,360
whole big thing and although all of

00:10:21,829 --> 00:10:24,769
these pointers need to go somewhere and

00:10:23,360 --> 00:10:26,029
then every time you're working with that

00:10:24,769 --> 00:10:31,999
integer you have to check all of this

00:10:26,029 --> 00:10:34,819
and you have to modify so this is all

00:10:31,999 --> 00:10:36,220
from a post by Jack wonder Plus that he

00:10:34,819 --> 00:10:39,740
did in 2014

00:10:36,220 --> 00:10:41,540
why Python is slow and yeah I highly

00:10:39,740 --> 00:10:48,499
recommend that you read it it's it's got

00:10:41,540 --> 00:10:53,749
a lot of insight so if you look at code

00:10:48,499 --> 00:10:55,910
like this which is very simple in C

00:10:53,749 --> 00:10:59,029
right these would be limited to numbers

00:10:55,910 --> 00:11:00,679
and you would just have to add them but

00:10:59,029 --> 00:11:02,720
in Python a and B could really be

00:11:00,679 --> 00:11:06,529
anything right it can be numbers but it

00:11:02,720 --> 00:11:08,149
can also be graphs or it can be bananas

00:11:06,529 --> 00:11:09,920
and then the function returns a bunch of

00:11:08,149 --> 00:11:12,290
bananas or it can be a banana and an

00:11:09,920 --> 00:11:14,929
apple and then you've got a fruit salad

00:11:12,290 --> 00:11:16,699
so it like Python you know every line of

00:11:14,929 --> 00:11:18,439
code that Python has to execute it has

00:11:16,699 --> 00:11:22,459
to check for all these types and check

00:11:18,439 --> 00:11:24,410
that all of this can work so one

00:11:22,459 --> 00:11:26,660
solution to this for numeric computation

00:11:24,410 --> 00:11:32,420
as probably most of you know is an

00:11:26,660 --> 00:11:35,959
unplanned sci-fi so in Python again this

00:11:32,420 --> 00:11:38,540
could be fruit and you get a fruit salad

00:11:35,959 --> 00:11:42,920
or this could be soldiers and you get a

00:11:38,540 --> 00:11:45,769
battalion but numpy can make use of the

00:11:42,920 --> 00:11:50,470
data type in my array to just some

00:11:45,769 --> 00:11:53,480
numbers in C and and be very fast so

00:11:50,470 --> 00:11:55,549
numpy can switch to c functions that are

00:11:53,480 --> 00:12:01,689
what people say close to the metal which

00:11:55,549 --> 00:12:01,689
means it's really processor instructions

00:12:02,260 --> 00:12:07,090
so again a quantile normalization that

00:12:04,840 --> 00:12:12,580
we saw earlier this is actually very

00:12:07,090 --> 00:12:16,150
fast way to compute this another example

00:12:12,580 --> 00:12:18,400
this is using scifi sparse matrices so

00:12:16,150 --> 00:12:24,160
each of these capitalized well variables

00:12:18,400 --> 00:12:26,950
is a matrix and you can make different

00:12:24,160 --> 00:12:30,930
matrices based on the row sums of things

00:12:26,950 --> 00:12:35,820
here's a degree inverse degree matrix

00:12:30,930 --> 00:12:39,010
using this sparse construct from Sify

00:12:35,820 --> 00:12:40,660
and then you can do matrix arithmetic in

00:12:39,010 --> 00:12:42,910
Python 3.5 you've got matrix

00:12:40,660 --> 00:12:45,160
multiplication operator that's one big

00:12:42,910 --> 00:12:48,430
reason to switch to Python 35 if you're

00:12:45,160 --> 00:12:51,490
working science and there yeah then you

00:12:48,430 --> 00:12:54,040
call get the eigen vectors and then you

00:12:51,490 --> 00:12:57,490
do this matrix multiplication and now

00:12:54,040 --> 00:12:59,980
you've got spectral coordinates to plot

00:12:57,490 --> 00:13:01,780
your your graph nodes in such a way as

00:12:59,980 --> 00:13:04,810
to minimize the total distance of your

00:13:01,780 --> 00:13:06,850
edges so that's that's a pretty neat

00:13:04,810 --> 00:13:08,950
little piece of linear algebra that you

00:13:06,850 --> 00:13:10,750
can do with with numbers I by and it's

00:13:08,950 --> 00:13:14,800
it's very fast I think it's close to as

00:13:10,750 --> 00:13:17,620
fast as you can do it but of course the

00:13:14,800 --> 00:13:20,470
problem with this is that it's very hard

00:13:17,620 --> 00:13:22,800
to write your own numerical routines so

00:13:20,470 --> 00:13:26,560
Python for science can be fast like a

00:13:22,800 --> 00:13:28,750
code that I just showed and that's why

00:13:26,560 --> 00:13:30,640
we have this massive growth because for

00:13:28,750 --> 00:13:35,370
for standard things you've got libraries

00:13:30,640 --> 00:13:38,130
that you see and can make things fast

00:13:35,370 --> 00:13:41,410
but it's very hard to write your own

00:13:38,130 --> 00:13:44,950
numerical routines Python is fast by

00:13:41,410 --> 00:13:49,930
hiding the messy C code that we all rely

00:13:44,950 --> 00:13:53,020
on and having only Python code do some

00:13:49,930 --> 00:13:55,480
kind of glue of these routines and it's

00:13:53,020 --> 00:13:58,650
very hard to derive I'm sorry to

00:13:55,480 --> 00:14:01,690
implement your own algorithms from this

00:13:58,650 --> 00:14:04,450
so things like graphs are always gonna

00:14:01,690 --> 00:14:07,060
be slow unless you have some graph C

00:14:04,450 --> 00:14:09,340
graph library that you pipe into and

00:14:07,060 --> 00:14:11,530
that that was I was pessimistic you can

00:14:09,340 --> 00:14:14,110
if you look at my blog a year ago I was

00:14:11,530 --> 00:14:15,660
kind of down on on a lot of stuff in

00:14:14,110 --> 00:14:18,550
Python

00:14:15,660 --> 00:14:21,490
but but there's there's definitely new

00:14:18,550 --> 00:14:25,210
developments that mean I'm really

00:14:21,490 --> 00:14:27,220
optimistic now again so the first one is

00:14:25,210 --> 00:14:29,560
siphon which just keeps getting better

00:14:27,220 --> 00:14:32,530
and better so it's actually very very

00:14:29,560 --> 00:14:37,180
easy to a turn your your Python code

00:14:32,530 --> 00:14:38,650
into C and be wrapped C code that is

00:14:37,180 --> 00:14:40,060
pre-existing if someone's got a C

00:14:38,650 --> 00:14:42,250
library that does what you want it's

00:14:40,060 --> 00:14:46,420
very easy to use iPhone to import that C

00:14:42,250 --> 00:14:49,240
library into Python so here's the

00:14:46,420 --> 00:14:50,800
numerical routine in Python so you're

00:14:49,240 --> 00:14:54,160
just taking a function and then you're

00:14:50,800 --> 00:14:58,060
integrating that using a production of

00:14:54,160 --> 00:15:00,100
the name of this algorithm and so here

00:14:58,060 --> 00:15:02,530
is what the equivalent psyphon code

00:15:00,100 --> 00:15:07,150
looks like so all you're doing I've

00:15:02,530 --> 00:15:08,830
highlighted in red is adding a bit of

00:15:07,150 --> 00:15:12,520
type information to your to your

00:15:08,830 --> 00:15:14,080
functions so this function you you don't

00:15:12,520 --> 00:15:16,630
want to use from Python itself you just

00:15:14,080 --> 00:15:18,910
want to use in C and so you just see

00:15:16,630 --> 00:15:20,320
death instead of death and then you say

00:15:18,910 --> 00:15:22,750
I'm gonna return the double and I'm

00:15:20,320 --> 00:15:25,720
gonna take a double and then some

00:15:22,750 --> 00:15:31,330
variables inside I also declare and

00:15:25,720 --> 00:15:34,480
similarly in here you used F which means

00:15:31,330 --> 00:15:36,250
you can call this from Python but inside

00:15:34,480 --> 00:15:39,760
of the function it's all going to be C

00:15:36,250 --> 00:15:41,410
and it'll be fast by the way if I'm

00:15:39,760 --> 00:15:46,270
going too fast or anything feel free to

00:15:41,410 --> 00:15:48,880
interrupt me okay so this is really

00:15:46,270 --> 00:15:51,880
great and you can also call in you can

00:15:48,880 --> 00:15:54,370
import instead of include you use import

00:15:51,880 --> 00:15:56,380
like you wouldn't sorry you would use C

00:15:54,370 --> 00:16:00,370
import and then you import the C library

00:15:56,380 --> 00:16:02,140
and and psyphon does all of the type

00:16:00,370 --> 00:16:03,820
conversion for you so it'll take

00:16:02,140 --> 00:16:06,550
collections and turn them into lists

00:16:03,820 --> 00:16:10,600
it'll take arrays and turn them into

00:16:06,550 --> 00:16:13,390
lists and so on the only problem with

00:16:10,600 --> 00:16:15,490
psyphon is that it is a bit harder to

00:16:13,390 --> 00:16:17,830
develop in because now you've got two

00:16:15,490 --> 00:16:19,420
compilation steps there's this gets

00:16:17,830 --> 00:16:21,640
turned into C and then the C has to be

00:16:19,420 --> 00:16:26,959
compiled and you lose quite a bit of the

00:16:21,640 --> 00:16:30,180
niceness of Python so is it hopeless

00:16:26,959 --> 00:16:32,760
while it's not so I said this is

00:16:30,180 --> 00:16:36,269
actually what an integer looks like in

00:16:32,760 --> 00:16:38,700
in C Python code but actually there is

00:16:36,269 --> 00:16:40,890
literally nothing in the Python language

00:16:38,700 --> 00:16:42,570
specification that says that this is how

00:16:40,890 --> 00:16:45,329
it has to be implemented right this is

00:16:42,570 --> 00:16:50,060
just an implementation detail of how

00:16:45,329 --> 00:16:52,470
Python was historically developed and

00:16:50,060 --> 00:16:55,529
you many of you might have heard of

00:16:52,470 --> 00:16:58,350
Julia which is a language that looks a

00:16:55,529 --> 00:17:03,199
lot like Python so this is a Julia

00:16:58,350 --> 00:17:05,850
program to solve quadratic equations and

00:17:03,199 --> 00:17:09,900
except for these ugly ends that I

00:17:05,850 --> 00:17:11,670
despise it's it's it looks very much

00:17:09,900 --> 00:17:16,020
like Python and yet julia is very

00:17:11,670 --> 00:17:18,630
performant it's very fast and how does

00:17:16,020 --> 00:17:21,000
it achieve that it uses something called

00:17:18,630 --> 00:17:22,620
just-in-time compilation so I'm gonna

00:17:21,000 --> 00:17:24,329
use a function to illustrate what that

00:17:22,620 --> 00:17:27,900
is and by the way I'm like a total

00:17:24,329 --> 00:17:29,550
amateur abyss and I I don't understand

00:17:27,900 --> 00:17:31,919
any details of it but but on a high

00:17:29,550 --> 00:17:35,190
level what's happening is you've got

00:17:31,919 --> 00:17:37,140
this Python function and as I said

00:17:35,190 --> 00:17:38,970
Python doesn't know what the types of a

00:17:37,140 --> 00:17:40,440
and B are so it doesn't know what to do

00:17:38,970 --> 00:17:41,700
every time but it calls a function you

00:17:40,440 --> 00:17:43,110
have to check the types of the arguments

00:17:41,700 --> 00:17:45,990
and so on

00:17:43,110 --> 00:17:47,580
do some some hard work basically to to

00:17:45,990 --> 00:17:51,150
make sure that you're doing the same

00:17:47,580 --> 00:17:53,160
things here but with just-in-time

00:17:51,150 --> 00:17:56,400
compilation you're actually looking at

00:17:53,160 --> 00:17:57,720
the code as it executes and you know

00:17:56,400 --> 00:18:02,130
normally you're just gonna call this

00:17:57,720 --> 00:18:04,050
with to float arrays of floats and so

00:18:02,130 --> 00:18:04,320
the compiler just needs to know all

00:18:04,050 --> 00:18:05,610
right

00:18:04,320 --> 00:18:08,010
I've called it with two arrays of floats

00:18:05,610 --> 00:18:09,419
okay when when these are two arrays of

00:18:08,010 --> 00:18:11,460
floods this is going to be an integer

00:18:09,419 --> 00:18:13,340
this is gonna be a float because it's

00:18:11,460 --> 00:18:15,480
gonna get set it to a float down here

00:18:13,340 --> 00:18:17,700
and this is just gonna be an iteration

00:18:15,480 --> 00:18:20,850
and finally we're gonna take the square

00:18:17,700 --> 00:18:23,520
root of that and return another float so

00:18:20,850 --> 00:18:27,179
if you introspect the code as it's

00:18:23,520 --> 00:18:28,770
executing you can then produce a

00:18:27,179 --> 00:18:30,360
function that looks like this I found

00:18:28,770 --> 00:18:36,240
function that I talked told you about

00:18:30,360 --> 00:18:38,010
before and yeah has information about

00:18:36,240 --> 00:18:39,990
the types and runs very quickly and so

00:18:38,010 --> 00:18:41,520
you put that aside

00:18:39,990 --> 00:18:42,960
and you make a note of it and the next

00:18:41,520 --> 00:18:44,400
time that you get call this function on

00:18:42,960 --> 00:18:46,230
two arrays of floats you're like ah I've

00:18:44,400 --> 00:18:47,700
got this function that I've compiled

00:18:46,230 --> 00:18:50,250
that's super fast I'm gonna use that

00:18:47,700 --> 00:18:53,160
instead of using the Python code except

00:18:50,250 --> 00:18:55,740
when you call it with two bananas then

00:18:53,160 --> 00:19:00,540
it just drops back into the Python code

00:18:55,740 --> 00:19:03,060
so that's that's a really high-level

00:19:00,540 --> 00:19:04,380
view of what Jude's do and there's quite

00:19:03,060 --> 00:19:06,720
a few examples so you might have heard

00:19:04,380 --> 00:19:12,600
of pi PI there's also a number and

00:19:06,720 --> 00:19:16,050
piston and yeah so that's what you're

00:19:12,600 --> 00:19:21,810
doing so if we go back to this function

00:19:16,050 --> 00:19:25,800
that I did before I tried this yesterday

00:19:21,810 --> 00:19:30,450
and all I do is import number at number

00:19:25,800 --> 00:19:35,760
JIT and la magia this is actually 600

00:19:30,450 --> 00:19:37,710
times faster than this so very little

00:19:35,760 --> 00:19:41,820
effort all you need to do is make sure

00:19:37,710 --> 00:19:45,000
that all of your values are numerical or

00:19:41,820 --> 00:19:50,010
numerical arrays and number just yeah

00:19:45,000 --> 00:19:54,150
can be a miracle worker so the question

00:19:50,010 --> 00:19:55,950
is are we done and the answer is no so

00:19:54,150 --> 00:19:57,540
so two years ago I tried number and I

00:19:55,950 --> 00:19:59,760
couldn't get it to do what I did what I

00:19:57,540 --> 00:20:04,230
wanted and even now it's still a bit

00:19:59,760 --> 00:20:06,450
fiddly because sometimes no python means

00:20:04,230 --> 00:20:08,880
okay if you can't get this then give me

00:20:06,450 --> 00:20:10,310
an exception and it'll fail and you

00:20:08,880 --> 00:20:13,740
won't really understand why it's failing

00:20:10,310 --> 00:20:17,840
so it's still hard to a bit hard to work

00:20:13,740 --> 00:20:22,860
with and there's also a little bit of a

00:20:17,840 --> 00:20:27,390
I don't want to say dirty secret but it

00:20:22,860 --> 00:20:29,880
is it's not well known so number can can

00:20:27,390 --> 00:20:32,610
take this NP dot square root colon and

00:20:29,880 --> 00:20:34,130
work with it but in general C functions

00:20:32,610 --> 00:20:36,060
things that are developed in seed

00:20:34,130 --> 00:20:37,800
just-in-time compilers can't touch

00:20:36,060 --> 00:20:39,510
because it's just this black box binary

00:20:37,800 --> 00:20:41,910
that you know it takes an input and

00:20:39,510 --> 00:20:43,910
produces an output and so all of the

00:20:41,910 --> 00:20:47,850
Jets don't actually work with the numpy

00:20:43,910 --> 00:20:49,560
compile C functions and even though a

00:20:47,850 --> 00:20:51,180
number does whenever you have an empty

00:20:49,560 --> 00:20:53,130
dot something called number will just

00:20:51,180 --> 00:20:55,520
work with it

00:20:53,130 --> 00:20:58,950
the secret is that number number just

00:20:55,520 --> 00:21:02,100
re-implemented a lot of this the numpy

00:20:58,950 --> 00:21:04,679
functions themselves and and work with

00:21:02,100 --> 00:21:08,660
that and so that's and this is true also

00:21:04,679 --> 00:21:10,950
of pi PI and a bunch of others and

00:21:08,660 --> 00:21:18,690
that's because as I said it's hard to

00:21:10,950 --> 00:21:20,760
work with C code and yeah and and so

00:21:18,690 --> 00:21:23,130
they every implemented it's a bad thing

00:21:20,760 --> 00:21:26,070
because now if numpy evolves and they're

00:21:23,130 --> 00:21:28,620
out of step so that there's a for

00:21:26,070 --> 00:21:31,919
example if you have none hideouts um you

00:21:28,620 --> 00:21:34,280
can pass it an axis argument if you do

00:21:31,919 --> 00:21:36,570
that inside number a number will fail so

00:21:34,280 --> 00:21:38,159
B and that's because they have their own

00:21:36,570 --> 00:21:40,200
their own implementation things number

00:21:38,159 --> 00:21:47,130
also doesn't have a non peanut art sort

00:21:40,200 --> 00:21:51,750
for example so this is really not a good

00:21:47,130 --> 00:21:55,140
state of affairs and but what's the

00:21:51,750 --> 00:21:57,240
solution and so you can go back to this

00:21:55,140 --> 00:22:00,260
2012 blog post which is not gonna appear

00:21:57,240 --> 00:22:03,690
but again my output my notes on Twitter

00:22:00,260 --> 00:22:08,450
it's by Jake van Laplace and it's in

00:22:03,690 --> 00:22:10,470
2012 which is remarkably prescient but

00:22:08,450 --> 00:22:12,240
he talks about the future

00:22:10,470 --> 00:22:14,130
he talks kind of the way I'm talking

00:22:12,240 --> 00:22:15,600
about it and you see it's called why

00:22:14,130 --> 00:22:18,450
Python is the last language you'll ever

00:22:15,600 --> 00:22:19,830
need to learn so one of the things that

00:22:18,450 --> 00:22:22,980
you might not know about numpy is that

00:22:19,830 --> 00:22:26,700
it didn't just happen there were two

00:22:22,980 --> 00:22:30,840
competing array libraries in the early

00:22:26,700 --> 00:22:31,980
2000s Nam array and numeric and and they

00:22:30,840 --> 00:22:34,679
were kind of stepping on each other's

00:22:31,980 --> 00:22:36,750
toes and and the community was split and

00:22:34,679 --> 00:22:39,419
Travis elephant came and basically

00:22:36,750 --> 00:22:40,830
produced numpy which took all of the

00:22:39,419 --> 00:22:43,140
best code from both of those libraries

00:22:40,830 --> 00:22:46,320
and coordinated the community effort to

00:22:43,140 --> 00:22:48,419
have everyone migrate to numpy and now

00:22:46,320 --> 00:22:50,850
numpy of course underpins all of

00:22:48,419 --> 00:22:55,260
scientific computing in python so so

00:22:50,850 --> 00:22:57,840
that was just a fantastic effort and so

00:22:55,260 --> 00:22:59,510
jake argues that this is going to happen

00:22:57,840 --> 00:23:03,630
with just-in-time compilation as well

00:22:59,510 --> 00:23:07,080
and now four years later I think we're

00:23:03,630 --> 00:23:08,460
really starting to see this happen so

00:23:07,080 --> 00:23:10,799
he said there will be another Travis

00:23:08,460 --> 00:23:14,390
elephant who is the person who unified

00:23:10,799 --> 00:23:17,130
numpy and so maybe that person could be

00:23:14,390 --> 00:23:21,809
Nathaniel Smith who is a core developer

00:23:17,130 --> 00:23:25,740
of numpy and inside by 2016 which I went

00:23:21,809 --> 00:23:27,809
to have a month ago sir organised the

00:23:25,740 --> 00:23:29,360
Python compilers a workshop where

00:23:27,809 --> 00:23:32,340
they're coming up with solutions to

00:23:29,360 --> 00:23:33,899
provide numpy have numpy provide an

00:23:32,340 --> 00:23:36,029
intermediate representation of its

00:23:33,899 --> 00:23:38,610
algorithms so that just-in-time

00:23:36,029 --> 00:23:41,639
compilers instead of having to look into

00:23:38,610 --> 00:23:42,990
a see black box binary I can look at

00:23:41,639 --> 00:23:46,679
this intermediate representation and

00:23:42,990 --> 00:23:50,700
just do that so so and basically all of

00:23:46,679 --> 00:23:55,320
the just um come are on board with this

00:23:50,700 --> 00:23:56,880
plan another possible solution very

00:23:55,320 --> 00:23:59,010
exciting I recommend you see the stock

00:23:56,880 --> 00:24:01,679
it's bread cannon who is a seeker

00:23:59,010 --> 00:24:04,139
developer and Dino veal and they're both

00:24:01,679 --> 00:24:05,639
at Microsoft and they've developed this

00:24:04,139 --> 00:24:08,010
thing called pigeon which is or not yet

00:24:05,639 --> 00:24:11,370
another just-in-time compiler but it's

00:24:08,010 --> 00:24:13,380
actually very different in spirit to the

00:24:11,370 --> 00:24:15,120
others and the reason is very different

00:24:13,380 --> 00:24:18,419
is that in addition to the just-in-time

00:24:15,120 --> 00:24:22,950
compiler that they've produced they've

00:24:18,419 --> 00:24:24,500
produced a plugging plug-in system for C

00:24:22,950 --> 00:24:27,590
Python so the main Python implementation

00:24:24,500 --> 00:24:31,830
that would allow people to very easily

00:24:27,590 --> 00:24:35,010
provide just-in-time compilers and this

00:24:31,830 --> 00:24:37,769
might make it into the C Python spec so

00:24:35,010 --> 00:24:39,480
then what that could trigger is lots and

00:24:37,769 --> 00:24:40,470
lots of just-in-time compilers competing

00:24:39,480 --> 00:24:43,230
with each other

00:24:40,470 --> 00:24:45,720
for speed kind of an arms race and

00:24:43,230 --> 00:24:49,830
you're gonna end up with a system that

00:24:45,720 --> 00:24:53,389
can use any of these buried and be all

00:24:49,830 --> 00:24:55,080
of these improving very quickly and

00:24:53,389 --> 00:24:58,620
demonstrating the flexibility of their

00:24:55,080 --> 00:25:00,990
system they actually were able to just

00:24:58,620 --> 00:25:05,760
the white pigeon works it's quite

00:25:00,990 --> 00:25:09,029
different from number and they got 100%

00:25:05,760 --> 00:25:10,580
compatibility with numpy straightaway so

00:25:09,029 --> 00:25:12,570
that's that's pretty amazing

00:25:10,580 --> 00:25:14,850
unfortunately currently you can only run

00:25:12,570 --> 00:25:16,620
pigeon on on Windows but that's not by

00:25:14,850 --> 00:25:17,760
design but just because they work at

00:25:16,620 --> 00:25:19,139
Microsoft and that's how they've

00:25:17,760 --> 00:25:20,330
developed it but there's nothing about

00:25:19,139 --> 00:25:22,940
this

00:25:20,330 --> 00:25:25,759
that is Windows only and if they address

00:25:22,940 --> 00:25:28,669
that in between the talk so yeah this is

00:25:25,759 --> 00:25:33,859
this is definitely cause for for

00:25:28,669 --> 00:25:36,499
optimism I think so I've got these three

00:25:33,859 --> 00:25:37,940
tech homes about Python Python in

00:25:36,499 --> 00:25:40,969
science the first one is the community

00:25:37,940 --> 00:25:43,669
so definitely one of the things that I

00:25:40,969 --> 00:25:45,559
didn't say is with number some of the

00:25:43,669 --> 00:25:48,379
work that I was doing and preparing for

00:25:45,559 --> 00:25:51,769
this talk I couldn't really quite get it

00:25:48,379 --> 00:25:54,049
to speed things up I read to the mailing

00:25:51,769 --> 00:25:56,299
list less than a day

00:25:54,049 --> 00:25:58,129
I had a response back but basically

00:25:56,299 --> 00:25:59,509
solved all the problems so the community

00:25:58,129 --> 00:26:01,789
in the side by is really fantastic

00:25:59,509 --> 00:26:04,809
if you're having trouble go to the

00:26:01,789 --> 00:26:07,549
mailing lists people will help you out

00:26:04,809 --> 00:26:10,789
definitely come to the sprints I'm gonna

00:26:07,549 --> 00:26:14,320
be around on Monday not sure what I'm

00:26:10,789 --> 00:26:19,940
probably working on some number porting

00:26:14,320 --> 00:26:21,769
and yet happy to help out then yeah

00:26:19,940 --> 00:26:24,679
elegance as I said you can you can

00:26:21,769 --> 00:26:27,829
really write good code small plug for my

00:26:24,679 --> 00:26:31,129
book but yeah there's there's just a lot

00:26:27,829 --> 00:26:33,799
of amazing code and amazing analysis you

00:26:31,129 --> 00:26:36,070
can do with just side by and combining

00:26:33,799 --> 00:26:39,409
all of the different functions inside by

00:26:36,070 --> 00:26:41,869
and performance as I said it's only

00:26:39,409 --> 00:26:43,879
getting better and there's lots and lots

00:26:41,869 --> 00:26:47,239
of really smart people working to make

00:26:43,879 --> 00:26:49,099
your Python code faster so with very

00:26:47,239 --> 00:26:51,379
little work from you suddenly like Paula

00:26:49,099 --> 00:26:55,669
Python is gonna become much faster and I

00:26:51,379 --> 00:26:59,959
think that's amazing and as a bonus I'm

00:26:55,669 --> 00:27:02,869
just gonna plug Python 3.5 just it's

00:26:59,959 --> 00:27:06,589
ready switch to it it's really nice

00:27:02,869 --> 00:27:09,829
and especially for scientific computing

00:27:06,589 --> 00:27:11,509
now with the app multiplication

00:27:09,829 --> 00:27:15,469
operator it just makes things so much

00:27:11,509 --> 00:27:17,529
nicer and with that I'll open for

00:27:15,469 --> 00:27:17,529
questions

00:27:35,460 --> 00:27:42,220
thank you for the talk I wasn't familiar

00:27:39,820 --> 00:27:45,369
with our having that piping operator but

00:27:42,220 --> 00:27:47,919
it seems like a very cool idea in Python

00:27:45,369 --> 00:27:49,600
this is just a syntactic question almost

00:27:47,919 --> 00:27:51,970
it seems you either have to go with

00:27:49,600 --> 00:27:54,159
massive chains of intermediate variables

00:27:51,970 --> 00:27:56,740
or monster nested expressions is there

00:27:54,159 --> 00:28:00,879
any advice you have for like big data

00:27:56,740 --> 00:28:03,100
flows like that I have a talk at

00:28:00,879 --> 00:28:06,039
Lester's here at Syfy about tools tools

00:28:03,100 --> 00:28:13,269
with his head it is an awesome library

00:28:06,039 --> 00:28:15,929
that provides just that do we have any

00:28:13,269 --> 00:28:15,929
other questions

00:28:29,899 --> 00:28:34,769
Thanks so thanks for one that was a

00:28:33,389 --> 00:28:36,419
great talk

00:28:34,769 --> 00:28:40,110
I just have a question about your book

00:28:36,419 --> 00:28:42,419
and you talked about optimization I

00:28:40,110 --> 00:28:45,539
noticed in the early release you haven't

00:28:42,419 --> 00:28:48,179
finished that part in fact is that

00:28:45,539 --> 00:28:54,029
related to some of the advances in

00:28:48,179 --> 00:28:58,470
optimization just a question about your

00:28:54,029 --> 00:29:02,009
book the early release has the

00:28:58,470 --> 00:29:11,070
optimization section not quite complete

00:29:02,009 --> 00:29:12,330
is that something it just that's

00:29:11,070 --> 00:29:15,149
definitely something that we want to

00:29:12,330 --> 00:29:20,999
cover the SyFy library has an optimized

00:29:15,149 --> 00:29:22,830
package with awesome so ultimately it's

00:29:20,999 --> 00:29:24,600
not about taking your code and speeding

00:29:22,830 --> 00:29:28,440
it up if that's your question it's about

00:29:24,600 --> 00:29:33,720
using the functional optimization sub

00:29:28,440 --> 00:29:38,100
module inside by if you want which one

00:29:33,720 --> 00:29:40,919
were you talking about more about

00:29:38,100 --> 00:29:44,309
splitting up okay okay so there's a

00:29:40,919 --> 00:29:46,710
bunch of other Python books that cover

00:29:44,309 --> 00:29:48,480
this I think effective computation in

00:29:46,710 --> 00:29:50,879
physics might have a chapter on this

00:29:48,480 --> 00:29:53,309
have a look at the the TSE there's

00:29:50,879 --> 00:29:55,649
there's a bunch of yeah scientific

00:29:53,309 --> 00:29:58,649
Python books that have come out in the

00:29:55,649 --> 00:30:02,779
past couple of years that that cover

00:29:58,649 --> 00:30:04,559
exactly this in great detail also

00:30:02,779 --> 00:30:10,399
high-performance Python I think it's

00:30:04,559 --> 00:30:10,399
called by in Oswald yep

00:30:15,950 --> 00:30:20,610
so I've become delightfully and happily

00:30:18,780 --> 00:30:23,220
spoiled by the the friendliness of

00:30:20,610 --> 00:30:26,040
Python I love the ipython triple and PDB

00:30:23,220 --> 00:30:28,260
and largely readable stack traces with

00:30:26,040 --> 00:30:29,550
exceptions and all that sort of stuff if

00:30:28,260 --> 00:30:31,770
I was to head off into the wonderful

00:30:29,550 --> 00:30:33,480
world of this sort of embedded C code

00:30:31,770 --> 00:30:35,040
and even the seed f'ing functions that

00:30:33,480 --> 00:30:36,450
you that she demonstrated do I lose out

00:30:35,040 --> 00:30:39,180
on much of that do I get to keep the

00:30:36,450 --> 00:30:40,620
beautiful ipython triple PDB or do I

00:30:39,180 --> 00:30:46,140
start looking at segmentation faults

00:30:40,620 --> 00:30:47,940
instead so I found by default will not

00:30:46,140 --> 00:30:49,350
give you a segmentation fault it has

00:30:47,940 --> 00:30:51,720
bounced checks and all these nice things

00:30:49,350 --> 00:30:53,160
and exception throwing but you can turn

00:30:51,720 --> 00:30:58,100
those off if you know that this is a

00:30:53,160 --> 00:31:03,170
super solid piece of code and yes I

00:30:58,100 --> 00:31:03,170
prefer the jits I have no idea sorry

00:31:18,680 --> 00:31:23,670
okay so you mentioned that seitan had

00:31:21,210 --> 00:31:25,680
two purposes one of which was speeding

00:31:23,670 --> 00:31:29,160
up existing parking code and the other

00:31:25,680 --> 00:31:31,380
which was like wrapping C so I can see

00:31:29,160 --> 00:31:34,170
the speeding up the existing Python code

00:31:31,380 --> 00:31:38,730
could go away if the jits work as well

00:31:34,170 --> 00:31:41,610
as they should but the wrapping part is

00:31:38,730 --> 00:31:44,190
still something that I think is valuable

00:31:41,610 --> 00:31:47,810
I mean and there was something called

00:31:44,190 --> 00:31:52,770
sweet as well and also scythe and wraps

00:31:47,810 --> 00:31:54,690
C++ as well as just C so you know not

00:31:52,770 --> 00:31:57,480
sure what do you think that's going at

00:31:54,690 --> 00:32:00,150
the moment I thinks I found I think

00:31:57,480 --> 00:32:03,990
sighs um definitely supersedes things

00:32:00,150 --> 00:32:07,890
like Zeus pythons it's in my experience

00:32:03,990 --> 00:32:10,020
a lot easier to work with and it'll keep

00:32:07,890 --> 00:32:12,750
up to date with the Python to Python 3

00:32:10,020 --> 00:32:14,340
thing unlike swegen boost Python you

00:32:12,750 --> 00:32:18,330
don't have to write different things for

00:32:14,340 --> 00:32:21,360
the two different versions of Python so

00:32:18,330 --> 00:32:22,410
yeah Cezanne's not going away and and

00:32:21,360 --> 00:32:24,630
it's awesome

00:32:22,410 --> 00:32:26,840
the the link that I had there that was

00:32:24,630 --> 00:32:26,840
cut off

00:32:26,880 --> 00:32:31,840
and again I'm a tempest has a Saipan

00:32:30,160 --> 00:32:33,070
tutorial where you you do all of the

00:32:31,840 --> 00:32:35,920
speeding up and the wrapping and the

00:32:33,070 --> 00:32:38,170
profiling cyclone has a very nice way to

00:32:35,920 --> 00:32:40,690
tell you where your code is not being

00:32:38,170 --> 00:32:42,760
sped up by saikhan just by showing how

00:32:40,690 --> 00:32:46,360
many lines of secret are generated by

00:32:42,760 --> 00:32:48,490
every line of sight on code so it's very

00:32:46,360 --> 00:32:51,000
fast to iterate and improve your your

00:32:48,490 --> 00:32:51,000
second code

00:32:52,800 --> 00:32:57,460
so often when I complain that my code is

00:32:55,870 --> 00:32:58,660
really slow and my academic environment

00:32:57,460 --> 00:33:00,880
people say oh we'll just run on the

00:32:58,660 --> 00:33:03,850
cluster and that's really hard if I

00:33:00,880 --> 00:33:06,220
thought and so I was wondering if the

00:33:03,850 --> 00:33:07,660
the jits aren't gonna make that kind of

00:33:06,220 --> 00:33:09,160
not be an issue

00:33:07,660 --> 00:33:14,380
I mean maybe probably not for most

00:33:09,160 --> 00:33:16,660
things but can you implement that sort

00:33:14,380 --> 00:33:20,740
of multi-threaded cluster environment

00:33:16,660 --> 00:33:22,870
stuff within scythe on and have that

00:33:20,740 --> 00:33:26,350
work or is that have you have any

00:33:22,870 --> 00:33:29,320
experience with that that is also

00:33:26,350 --> 00:33:30,640
getting just amazingly better there's

00:33:29,320 --> 00:33:34,300
another talk that I could have given

00:33:30,640 --> 00:33:37,300
which is on desk which is developed

00:33:34,300 --> 00:33:40,240
tools that I mentioned before and it is

00:33:37,300 --> 00:33:41,440
an amazing library and again it's only

00:33:40,240 --> 00:33:44,890
getting better they just got this

00:33:41,440 --> 00:33:46,060
enormous grant to make it sort of work

00:33:44,890 --> 00:33:48,390
in all different kind of clusters

00:33:46,060 --> 00:33:53,140
environments it already works with slurm

00:33:48,390 --> 00:33:59,820
yeah and it's tasks like a test but with

00:33:53,140 --> 00:34:02,830
addy yeah and it does does really great

00:33:59,820 --> 00:34:04,360
number itself has primitives for

00:34:02,830 --> 00:34:06,280
multi-threading and so on

00:34:04,360 --> 00:34:08,260
psiphon you can turn off a gill in your

00:34:06,280 --> 00:34:10,020
site home code there's definitely lots

00:34:08,260 --> 00:34:13,320
of options I actually I used to be a

00:34:10,020 --> 00:34:16,360
just running on the cluster persuasion

00:34:13,320 --> 00:34:19,380
now I'm kind of annoyed when I realized

00:34:16,360 --> 00:34:22,180
that my processor is just spending time

00:34:19,380 --> 00:34:27,340
checking things that it doesn't need to

00:34:22,180 --> 00:34:29,230
check so so I'm very much of the idea

00:34:27,340 --> 00:34:30,880
that you should make sure that your code

00:34:29,230 --> 00:34:32,380
is as fast as it can be and then you've

00:34:30,880 --> 00:34:35,610
run another cluster but otherwise you're

00:34:32,380 --> 00:34:35,610
just wasting people that you see

00:34:35,639 --> 00:34:41,490
but yet the cluster thing is getting

00:34:38,230 --> 00:34:46,599
better and better as well one yeah

00:34:41,490 --> 00:34:48,220
excellent talk often you know people go

00:34:46,599 --> 00:34:49,840
ahead and say oh let's use some

00:34:48,220 --> 00:34:53,409
just-in-time compiler or something like

00:34:49,840 --> 00:34:56,200
that at the best of times I think I'm a

00:34:53,409 --> 00:34:58,810
terrible programmer would you have any

00:34:56,200 --> 00:35:00,849
advice that premature optimization you

00:34:58,810 --> 00:35:02,590
know like instead of trying to optimize

00:35:00,849 --> 00:35:04,300
something with JIT or something like

00:35:02,590 --> 00:35:05,950
that just go ahead and read us on your

00:35:04,300 --> 00:35:09,010
algorithm use a different data structure

00:35:05,950 --> 00:35:11,020
stuff like that so I mean right now with

00:35:09,010 --> 00:35:13,570
with digits you have to go and use a

00:35:11,020 --> 00:35:16,119
different data structure anyway because

00:35:13,570 --> 00:35:17,530
they all work on numpy arrays and that

00:35:16,119 --> 00:35:20,260
sort of stuff

00:35:17,530 --> 00:35:22,359
in for example pi PI which is more

00:35:20,260 --> 00:35:24,550
flexible and it really targets pure

00:35:22,359 --> 00:35:25,810
Python will let you work with

00:35:24,550 --> 00:35:29,020
dictionaries but your dictionary should

00:35:25,810 --> 00:35:31,150
have a homogeneous type so really you do

00:35:29,020 --> 00:35:38,200
need to clean up your code to get get

00:35:31,150 --> 00:35:40,290
the most out of idiots I sorry what was

00:35:38,200 --> 00:35:44,500
the second part of your question

00:35:40,290 --> 00:35:46,690
yeah you should I got started premature

00:35:44,500 --> 00:35:48,580
optimization so apparently that's a huge

00:35:46,690 --> 00:35:50,500
there's a great article online stop

00:35:48,580 --> 00:35:52,890
misreading Donald Knuth um premature

00:35:50,500 --> 00:35:55,030
optimization and if you look at the

00:35:52,890 --> 00:35:57,670
article where he wrote that he then goes

00:35:55,030 --> 00:36:02,640
on to say but you should definitely

00:35:57,670 --> 00:36:06,609
optimize your code and and so on and I

00:36:02,640 --> 00:36:09,190
think if you if you don't worry about it

00:36:06,609 --> 00:36:11,650
from relatively early on you can end up

00:36:09,190 --> 00:36:13,480
with design choices that will haunt you

00:36:11,650 --> 00:36:14,920
for a very long time and and I've

00:36:13,480 --> 00:36:17,440
definitely experienced this I have this

00:36:14,920 --> 00:36:18,730
curve that is I keep speeding up and

00:36:17,440 --> 00:36:21,880
speeding up but because I started with

00:36:18,730 --> 00:36:23,530
such bad data structures it's it's

00:36:21,880 --> 00:36:25,839
becoming really really hard to refactor

00:36:23,530 --> 00:36:29,740
so so I definitely think about the speed

00:36:25,839 --> 00:36:31,780
of your code from the beginning and

00:36:29,740 --> 00:36:34,450
wherever you can fit stuff into arrays

00:36:31,780 --> 00:36:37,000
and do it because it makes things a lot

00:36:34,450 --> 00:36:39,810
a lot easier one one aspect of

00:36:37,000 --> 00:36:43,230
optimization optimization that I hadn't

00:36:39,810 --> 00:36:46,150
quite perhaps how important it is is

00:36:43,230 --> 00:36:47,380
it's a lot easier to do operations on

00:36:46,150 --> 00:36:48,030
things that are next to each other in

00:36:47,380 --> 00:36:50,100
memory

00:36:48,030 --> 00:36:54,290
and it is it just grab this integer grab

00:36:50,100 --> 00:36:57,480
position representated times faster so

00:36:54,290 --> 00:36:59,840
thinking about stuff in a race is a huge

00:36:57,480 --> 00:36:59,840
advantage

00:37:00,230 --> 00:37:19,650
do we have more questions thanks for a

00:37:15,840 --> 00:37:21,180
guy talk just well I think of it I've

00:37:19,650 --> 00:37:22,740
got a feeling there might be a talk on

00:37:21,180 --> 00:37:24,150
tasks given by someone else at the

00:37:22,740 --> 00:37:31,200
conference just for memory I hope I'm

00:37:24,150 --> 00:37:33,630
not wrong there okay so we've got you

00:37:31,200 --> 00:37:35,850
know no numpy and the various I PI

00:37:33,630 --> 00:37:38,760
things which is really someone smarts

00:37:35,850 --> 00:37:41,250
gone and written C code Fortran code and

00:37:38,760 --> 00:37:43,950
then I can use it with this nice Python

00:37:41,250 --> 00:37:46,920
interface if a couple years down the the

00:37:43,950 --> 00:37:52,080
road these and other things are much

00:37:46,920 --> 00:37:55,230
more mature and someone maybe has won

00:37:52,080 --> 00:37:59,670
the race is that going to mean that the

00:37:55,230 --> 00:38:01,860
people in psych it image the numpy them

00:37:59,670 --> 00:38:04,410
do most of their stuff in Python with a

00:38:01,860 --> 00:38:06,120
little bit of core stuff in the other

00:38:04,410 --> 00:38:10,770
languages or we're always going to live

00:38:06,120 --> 00:38:14,430
in this polyglot language world I think

00:38:10,770 --> 00:38:17,910
abandoning psiphon and C it's gonna take

00:38:14,430 --> 00:38:19,380
a very long time because backwards

00:38:17,910 --> 00:38:22,530
compatibility as well is very very

00:38:19,380 --> 00:38:24,360
important for library developers you

00:38:22,530 --> 00:38:28,650
want to keep supporting things like

00:38:24,360 --> 00:38:31,140
Python 2 7 and that sort of stuff so I

00:38:28,650 --> 00:38:33,060
think that Oh that'll last a long time

00:38:31,140 --> 00:38:35,910
I don't think it's a problem we're

00:38:33,060 --> 00:38:37,410
definitely at psychedelic at least you

00:38:35,910 --> 00:38:38,730
know we're always exploring all of these

00:38:37,410 --> 00:38:43,460
JIT options and whatever but for example

00:38:38,730 --> 00:38:47,550
we number unless you're doing anaconda

00:38:43,460 --> 00:38:51,660
it's quite tricky to install so in cycle

00:38:47,550 --> 00:38:53,430
image definitely cycle and we won't take

00:38:51,660 --> 00:38:56,550
off on dependencies that will require

00:38:53,430 --> 00:38:58,620
compilation by the user so number now

00:38:56,550 --> 00:39:00,910
only recently got the ability to pre

00:38:58,620 --> 00:39:02,950
compile your fun

00:39:00,910 --> 00:39:06,670
and then you ship kind of like a sea

00:39:02,950 --> 00:39:08,190
binary with your package so that's the

00:39:06,670 --> 00:39:10,779
sort of solutions that we would look at

00:39:08,190 --> 00:39:13,059
in in psyche attention in the in the

00:39:10,779 --> 00:39:16,000
near term in the in the long term maybe

00:39:13,059 --> 00:39:17,710
let's say Python for 10 years it could

00:39:16,000 --> 00:39:25,779
be that everything will be written in

00:39:17,710 --> 00:39:31,509
Python and super quick don't you do we

00:39:25,779 --> 00:39:34,690
have more questions I've got one um I've

00:39:31,509 --> 00:39:36,220
got the microphone so why not when we

00:39:34,690 --> 00:39:38,140
look at the Jets and a number of the

00:39:36,220 --> 00:39:40,029
other Python implementations one of the

00:39:38,140 --> 00:39:43,539
ways that they achieve their speed-up is

00:39:40,029 --> 00:39:45,849
by actually defining types if we have a

00:39:43,539 --> 00:39:47,289
look at what the Python or the C Python

00:39:45,849 --> 00:39:49,180
community is doing is they're moving

00:39:47,289 --> 00:39:51,819
towards type annotations and there was a

00:39:49,180 --> 00:39:54,099
pet this week co-authored by Guido of

00:39:51,819 --> 00:39:57,880
actually more explicitly structuring

00:39:54,099 --> 00:39:59,259
those type annotations in C Pisan there

00:39:57,880 --> 00:40:01,059
are quite different ways of doing that

00:39:59,259 --> 00:40:03,849
do you think that the community will end

00:40:01,059 --> 00:40:05,980
up being forced to adopt the C Python

00:40:03,849 --> 00:40:07,930
approach for type annotations or will

00:40:05,980 --> 00:40:09,519
see this ongoing fragmentation with

00:40:07,930 --> 00:40:11,650
different tools doing it different ways

00:40:09,519 --> 00:40:13,450
and people just having to wear the

00:40:11,650 --> 00:40:17,680
cognitive overload of different tools

00:40:13,450 --> 00:40:21,460
different ways my hope is that the

00:40:17,680 --> 00:40:23,710
community will make use of the C Python

00:40:21,460 --> 00:40:26,890
type annotations I think it makes a lot

00:40:23,710 --> 00:40:30,630
of sense they're optional but say

00:40:26,890 --> 00:40:33,460
they're also optional projects and so

00:40:30,630 --> 00:40:35,920
yeah why not why not use them I think

00:40:33,460 --> 00:40:39,700
they're it's a nice syntax and it's very

00:40:35,920 --> 00:40:45,009
flexible framework so I would I would

00:40:39,700 --> 00:40:47,680
say if it were up to me we would we

00:40:45,009 --> 00:40:49,420
would eventually coalesce some two onto

00:40:47,680 --> 00:40:52,029
one thing I don't I don't know if sicom

00:40:49,420 --> 00:40:53,460
can do that but definitely for the jits

00:40:52,029 --> 00:40:59,519
I think it's it's a strong possibility

00:40:53,460 --> 00:40:59,519
okay any other questions

00:41:08,840 --> 00:41:16,650
I'm glad talk I just have a question

00:41:12,420 --> 00:41:20,010
about using GPUs for our performance

00:41:16,650 --> 00:41:22,650
computing it seems like it's it's a

00:41:20,010 --> 00:41:25,230
helluva undertaking to install CUDA and

00:41:22,650 --> 00:41:28,200
get the right drivers so is there any

00:41:25,230 --> 00:41:35,160
sort of plan to neaten that up make it a

00:41:28,200 --> 00:41:38,630
little easier but I do know that number

00:41:35,160 --> 00:41:41,240
does offer us some GPU compilation

00:41:38,630 --> 00:41:45,810
options for certain kinds of functions

00:41:41,240 --> 00:41:49,530
so there's definitely a direction of

00:41:45,810 --> 00:41:53,880
some dates to allow kind of transparent

00:41:49,530 --> 00:41:55,350
GPU computations and and I think that's

00:41:53,880 --> 00:41:57,480
a strong possibility yeah there's

00:41:55,350 --> 00:41:58,460
definitely a big community to try to

00:41:57,480 --> 00:42:01,890
make that easier

00:41:58,460 --> 00:42:03,930
Nvidia being you know the prime sort of

00:42:01,890 --> 00:42:07,880
proponents of this thing but they're

00:42:03,930 --> 00:42:07,880
sponsoring a lot of efforts to do that

00:42:10,670 --> 00:42:17,520
and this is any penny we have any

00:42:14,040 --> 00:42:20,280
question Thanks yeah thanks to the talk

00:42:17,520 --> 00:42:28,410
one um slightly off topic but what's on

00:42:20,280 --> 00:42:30,840
the list full cycle image okay I think

00:42:28,410 --> 00:42:34,020
we're kind of nearing a 1.0 which would

00:42:30,840 --> 00:42:35,730
mean cleaning up a lot of the

00:42:34,020 --> 00:42:39,270
inconsistencies you know that creep up

00:42:35,730 --> 00:42:42,960
through historical you know for

00:42:39,270 --> 00:42:45,080
historical reasons and the next thing

00:42:42,960 --> 00:42:48,869
actually does include things like

00:42:45,080 --> 00:42:50,990
just-in-time and GPU and and basically

00:42:48,869 --> 00:42:53,280
offering a lot of higher performance

00:42:50,990 --> 00:42:54,930
everyone in the psychedelic seemed I

00:42:53,280 --> 00:42:56,490
don't know about everyone but a lot of

00:42:54,930 --> 00:42:58,380
people in the second image team used to

00:42:56,490 --> 00:43:01,859
feel like whatever is it see it's fast

00:42:58,380 --> 00:43:02,820
enough but now it's actually become

00:43:01,859 --> 00:43:06,270
apparent that there's a lot of

00:43:02,820 --> 00:43:09,750
inefficiencies there were so I just I

00:43:06,270 --> 00:43:11,160
just cleaned up fashion filters a month

00:43:09,750 --> 00:43:13,020
or two ago

00:43:11,160 --> 00:43:16,560
and it was a very naive implementation

00:43:13,020 --> 00:43:19,530
and with very little effort we got it

00:43:16,560 --> 00:43:20,970
was 17 fold speed up so I think there's

00:43:19,530 --> 00:43:22,560
a lot of low-hanging fruit like that

00:43:20,970 --> 00:43:26,630
where we want to mix I could image

00:43:22,560 --> 00:43:26,630
really the go-to for for image analysis

00:43:28,280 --> 00:43:35,960
we have a big show of thanks to one for

00:43:32,280 --> 00:43:35,960

YouTube URL: https://www.youtube.com/watch?v=rr_YwtBIG7U


