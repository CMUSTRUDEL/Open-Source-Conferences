Title: Learning TensorFlow
Publication date: 2016-08-16
Playlist: Science & Data 2016 (Miniconf)
Description: 
	Robert Layton
https://2016.pycon-au.org/schedule/126/view_talk
The recently released TensorFlow library has caused great waves in machine learning circles, with its powerful syntax that allows for distributed computation, improved efficiency, and modularisation. The framework allows you to build graph-based models, such as those used in machine learning and artificial intelligence, and have those models run on a distributed computing systems, including GPUs.

This talk will cover what TensorFlow is, why/when you should use it, and cover the basics surrounding Variables, Placeholders, and Custom Functions. Importantly, there are several use cases *not* focused on data analytics - TensorFlow is more than just a machine learning library!
Captions: 
	00:00:00,350 --> 00:00:05,879
good afternoon everyone thanks again

00:00:03,720 --> 00:00:08,189
welcome once more this is the last stack

00:00:05,879 --> 00:00:11,460
of the signs last talk of the science

00:00:08,189 --> 00:00:14,160
and data minik oh and next speaker is

00:00:11,460 --> 00:00:16,470
dr. Robert Leighton who has a PhD

00:00:14,160 --> 00:00:19,050
investigating cybercrime analytics and

00:00:16,470 --> 00:00:21,960
he said data analysts working on text

00:00:19,050 --> 00:00:25,019
problems for business please welcome

00:00:21,960 --> 00:00:34,800
Robert and the talk learning tensorflow

00:00:25,019 --> 00:00:36,570
oh thanks for coming and so probably one

00:00:34,800 --> 00:00:40,860
thing to know my shameless self-promoter

00:00:36,570 --> 00:00:45,030
so website so yeah today I'm going to

00:00:40,860 --> 00:00:46,379
overview the tensorflow library many of

00:00:45,030 --> 00:00:49,350
you have heard of it and that's probably

00:00:46,379 --> 00:00:51,600
why you're here I'm going to make a

00:00:49,350 --> 00:00:54,120
prediction now and it's on camera that

00:00:51,600 --> 00:00:57,020
next year's mini-com on this topic will

00:00:54,120 --> 00:01:01,980
have quite a number of tensorflow talks

00:00:57,020 --> 00:01:04,049
going to guess half just them just

00:01:01,980 --> 00:01:05,790
there's a bit of a bit of background

00:01:04,049 --> 00:01:09,750
when tensorflow is released late last

00:01:05,790 --> 00:01:11,640
year on github within a day it had more

00:01:09,750 --> 00:01:14,490
stars than what the scikit-learn project

00:01:11,640 --> 00:01:16,770
has and scikit-learn has very long

00:01:14,490 --> 00:01:20,270
history in very well-regarded in data

00:01:16,770 --> 00:01:25,770
mining space so so tend to flow is big

00:01:20,270 --> 00:01:27,000
it's very popular it's very new but what

00:01:25,770 --> 00:01:33,180
is it how do we use it

00:01:27,000 --> 00:01:35,159
and why would we bother so yeah so my

00:01:33,180 --> 00:01:39,630
name's Robert and I wear quite a number

00:01:35,159 --> 00:01:40,530
of hats but um but for this talk at

00:01:39,630 --> 00:01:41,970
least

00:01:40,530 --> 00:01:45,540
yeah we're just going to have a look am

00:01:41,970 --> 00:01:48,390
at the tenth blow n slow library this is

00:01:45,540 --> 00:01:51,360
a very introductory I have to apologize

00:01:48,390 --> 00:01:52,740
I have tried and tried and tried to get

00:01:51,360 --> 00:01:56,280
it to stop doing that but it's not

00:01:52,740 --> 00:01:59,610
listening so I've just kept moving the

00:01:56,280 --> 00:02:01,649
mouse randomly yeah so it's an

00:01:59,610 --> 00:02:03,990
introductory talking to tend to flow so

00:02:01,649 --> 00:02:06,509
this is not a deep learning talk I'm

00:02:03,990 --> 00:02:09,599
happy to take questions and then just

00:02:06,509 --> 00:02:11,009
delegate them to someone else later I

00:02:09,599 --> 00:02:13,480
know a little bit about deep learning

00:02:11,009 --> 00:02:16,690
I'm not an expert

00:02:13,480 --> 00:02:18,459
so so I might do a poll later and see if

00:02:16,690 --> 00:02:21,930
anyone actually does isn't deeply an

00:02:18,459 --> 00:02:24,370
expert do that just for questions so

00:02:21,930 --> 00:02:27,340
I've also got a book it doesn't have

00:02:24,370 --> 00:02:32,830
tens flow in it but it has some related

00:02:27,340 --> 00:02:36,790
material so by that please so so what is

00:02:32,830 --> 00:02:40,420
tenth flow so when we when we're in

00:02:36,790 --> 00:02:44,080
Python as we all know is it optimizes

00:02:40,420 --> 00:02:46,569
programmer time so we can do something

00:02:44,080 --> 00:02:48,010
in a day of Python that might take us a

00:02:46,569 --> 00:02:52,299
week in Java

00:02:48,010 --> 00:02:54,280
might take us a very long time in C so

00:02:52,299 --> 00:02:56,680
pythons built from the ground up to

00:02:54,280 --> 00:03:00,790
really optimize programmer time and I

00:02:56,680 --> 00:03:02,069
say that over you know skipping over a

00:03:00,790 --> 00:03:04,780
whole bunch of life in history but

00:03:02,069 --> 00:03:06,610
that's the idea anyway so when we did

00:03:04,780 --> 00:03:08,500
want to do something fly take a long

00:03:06,610 --> 00:03:11,140
list of numbers and multiply them all by

00:03:08,500 --> 00:03:15,010
two pythons not actually good at doing

00:03:11,140 --> 00:03:18,190
that so the reason for that is that that

00:03:15,010 --> 00:03:20,440
a list in Python is just a ordered

00:03:18,190 --> 00:03:22,090
collection of things and as far as

00:03:20,440 --> 00:03:24,700
python is concerned it doesn't care what

00:03:22,090 --> 00:03:26,560
those things are so when we want to

00:03:24,700 --> 00:03:29,709
multiply them each by 2 it asks each of

00:03:26,560 --> 00:03:31,329
the things how do I multiply you by 2 so

00:03:29,709 --> 00:03:33,819
that's the reason why Python is slow

00:03:31,329 --> 00:03:36,100
phonemic or processing so when we want

00:03:33,819 --> 00:03:38,400
to do that in Python instead of staining

00:03:36,100 --> 00:03:41,200
Python then we move to numpy lands and

00:03:38,400 --> 00:03:45,519
we create an umpire array object which

00:03:41,200 --> 00:03:48,549
is represented the low-level with quite

00:03:45,519 --> 00:03:50,980
a see like interface with a number is a

00:03:48,549 --> 00:03:54,579
number it's or specifically an integer

00:03:50,980 --> 00:03:58,450
specifically has this many many many bit

00:03:54,579 --> 00:04:00,370
bits to represent it and so on so when

00:03:58,450 --> 00:04:02,099
we now ask the question I want to

00:04:00,370 --> 00:04:04,510
multiply each number in that list by two

00:04:02,099 --> 00:04:06,459
we can actually use more optimized

00:04:04,510 --> 00:04:08,349
algorithms to go through and say I get

00:04:06,459 --> 00:04:09,489
each of these as a number I know how to

00:04:08,349 --> 00:04:11,049
multiply numbers by two

00:04:09,489 --> 00:04:15,639
let's just keep raising that operation

00:04:11,049 --> 00:04:19,299
the entire time so so we use numpy to do

00:04:15,639 --> 00:04:22,120
numerical processing in Python and that

00:04:19,299 --> 00:04:26,570
allows us to do this processing much

00:04:22,120 --> 00:04:30,200
faster the

00:04:26,570 --> 00:04:33,140
problem with using numpy is that when we

00:04:30,200 --> 00:04:35,060
do an operation we get back a Python

00:04:33,140 --> 00:04:37,820
object so when we do our next part next

00:04:35,060 --> 00:04:39,950
operation we then have to transfer our

00:04:37,820 --> 00:04:41,750
operation back into numpy land and most

00:04:39,950 --> 00:04:42,770
of the time this doesn't matter but but

00:04:41,750 --> 00:04:47,030
if we want to do a python-based

00:04:42,770 --> 00:04:49,820
operation operation on that on that list

00:04:47,030 --> 00:04:51,920
we have to do we have to convert all

00:04:49,820 --> 00:04:54,320
those numbers into Python things do a

00:04:51,920 --> 00:04:56,030
Python operation on that and then do our

00:04:54,320 --> 00:05:00,110
next step so that that all that

00:04:56,030 --> 00:05:02,750
to-and-fro becomes quite expensive so as

00:05:00,110 --> 00:05:05,030
a broad rule whenever we have numpy code

00:05:02,750 --> 00:05:07,160
we want to make it go faster we push all

00:05:05,030 --> 00:05:08,840
the operation down into num Palin and we

00:05:07,160 --> 00:05:13,490
stop coming back up the pipe and land

00:05:08,840 --> 00:05:15,740
all the time so why don't I just bother

00:05:13,490 --> 00:05:19,040
saying that the tensor flow basically

00:05:15,740 --> 00:05:22,760
lets us identify all the operations that

00:05:19,040 --> 00:05:24,710
we want to do and then just say ok now

00:05:22,760 --> 00:05:28,490
go do all of those in tensor flow land

00:05:24,710 --> 00:05:31,190
and just optimize all of that so so

00:05:28,490 --> 00:05:36,410
tensor flow is a liar for creating the

00:05:31,190 --> 00:05:38,260
computational graph so if the there's

00:05:36,410 --> 00:05:40,760
really two phases to attend to flow

00:05:38,260 --> 00:05:42,350
application the first phase we define

00:05:40,760 --> 00:05:43,880
the graph we say you're going to get

00:05:42,350 --> 00:05:45,470
data of this format you're going to do

00:05:43,880 --> 00:05:47,780
this operation to it you're then going

00:05:45,470 --> 00:05:49,280
to move over and do this other operation

00:05:47,780 --> 00:05:52,130
to it you want to pull this other data

00:05:49,280 --> 00:05:54,050
in and combine it and so on but we're

00:05:52,130 --> 00:05:55,820
not actually doing any work there and

00:05:54,050 --> 00:05:57,500
this is lazy computation there are quite

00:05:55,820 --> 00:06:00,350
a few ways of doing this in Python in

00:05:57,500 --> 00:06:01,880
other languages but tensorflow is one of

00:06:00,350 --> 00:06:05,720
those options but we're creating this

00:06:01,880 --> 00:06:08,300
computational graph and then at a later

00:06:05,720 --> 00:06:10,670
stage we say okay here's a graph

00:06:08,300 --> 00:06:14,140
now here's a data and I want you to go

00:06:10,670 --> 00:06:17,630
and actually work out what the result is

00:06:14,140 --> 00:06:20,750
so by first creating that computational

00:06:17,630 --> 00:06:21,980
graph we get to a point where all of

00:06:20,750 --> 00:06:24,350
those operations can be done in

00:06:21,980 --> 00:06:27,310
tensorflow land and it no longer needs

00:06:24,350 --> 00:06:33,400
to ask python what to do next

00:06:27,310 --> 00:06:35,570
so tend to flow itself is C++ but it's a

00:06:33,400 --> 00:06:36,890
I'm going to say official I'm not from

00:06:35,570 --> 00:06:38,900
Google and I really don't know if this

00:06:36,890 --> 00:06:40,400
is true it's official languages Python

00:06:38,900 --> 00:06:41,540
to the point where the Python

00:06:40,400 --> 00:06:47,300
api's actually have more functionality

00:06:41,540 --> 00:06:52,370
than the C++ API so you can do just C++

00:06:47,300 --> 00:06:54,610
but why so so yeah so pythons the main

00:06:52,370 --> 00:06:56,780
way of using tensorflow

00:06:54,610 --> 00:07:01,220
but all the heavy lifting is done with

00:06:56,780 --> 00:07:03,169
that much more efficient code so so the

00:07:01,220 --> 00:07:05,240
basic idea of attenti flows in that

00:07:03,169 --> 00:07:06,979
second bullet or the first of the second

00:07:05,240 --> 00:07:09,979
block their computational graphs build a

00:07:06,979 --> 00:07:11,449
graft and run it later this gives us the

00:07:09,979 --> 00:07:13,639
advantage of doing lazy computing which

00:07:11,449 --> 00:07:17,449
is almost always a good thing and mercy

00:07:13,639 --> 00:07:22,240
at the bug in and the other thing that

00:07:17,449 --> 00:07:24,440
this allows us to do is it allows us to

00:07:22,240 --> 00:07:26,840
maybe not this stage intend to flows

00:07:24,440 --> 00:07:28,850
life that at least in the next year or

00:07:26,840 --> 00:07:31,400
so be able to do things like transparent

00:07:28,850 --> 00:07:34,130
distributed process in moving different

00:07:31,400 --> 00:07:37,910
operations to different devices such as

00:07:34,130 --> 00:07:40,280
GPUs and CPUs and because it's a

00:07:37,910 --> 00:07:44,030
computational graph and not code we can

00:07:40,280 --> 00:07:47,030
we can we can reference the graph and

00:07:44,030 --> 00:07:48,949
say these things are better done on a C

00:07:47,030 --> 00:07:52,220
on a CPU and these things are better

00:07:48,949 --> 00:07:55,220
done on a GPU and I'll get to that a

00:07:52,220 --> 00:07:57,159
little bit later it allows us to focus

00:07:55,220 --> 00:08:00,860
on the algorithm not implementation and

00:07:57,159 --> 00:08:02,900
if you haven't thought if you haven't

00:08:00,860 --> 00:08:05,570
written a program with that mind frame

00:08:02,900 --> 00:08:07,970
before one thing you may have done it's

00:08:05,570 --> 00:08:10,460
similar is write an SQL statement so

00:08:07,970 --> 00:08:11,900
with SQL you're not actually telling the

00:08:10,460 --> 00:08:13,970
database engine how to go off and

00:08:11,900 --> 00:08:15,889
collect the data all you're saying is I

00:08:13,970 --> 00:08:19,039
want these columns from these tables

00:08:15,889 --> 00:08:20,750
where this condition is true MySQL or

00:08:19,039 --> 00:08:23,120
PostgreSQL

00:08:20,750 --> 00:08:25,909
then turn it into operations and if

00:08:23,120 --> 00:08:28,340
you've got a distributed database server

00:08:25,909 --> 00:08:29,870
like multiple databases data collected

00:08:28,340 --> 00:08:32,000
over different spots it will work out

00:08:29,870 --> 00:08:34,159
all those details for you you basically

00:08:32,000 --> 00:08:35,450
just be given a query and say I want to

00:08:34,159 --> 00:08:36,950
do I want you to give me this

00:08:35,450 --> 00:08:38,659
information and then it works out the

00:08:36,950 --> 00:08:43,039
details and tensorflow does a lot of

00:08:38,659 --> 00:08:48,500
that for us 10:00 to 5:00 still quite

00:08:43,039 --> 00:08:50,709
new it has a very large and very very

00:08:48,500 --> 00:08:53,120
quickly growing number of libraries I

00:08:50,709 --> 00:08:54,500
think if when I first did this talk

00:08:53,120 --> 00:08:56,550
there are only a few

00:08:54,500 --> 00:08:58,680
libraries on github to actually use

00:08:56,550 --> 00:09:01,530
tensorflow for anything specific

00:08:58,680 --> 00:09:07,260
now there's dozens coming through every

00:09:01,530 --> 00:09:09,090
week on some malware so what the other

00:09:07,260 --> 00:09:10,950
thing I should point out is that tend to

00:09:09,090 --> 00:09:13,380
flow is not a new thing

00:09:10,950 --> 00:09:15,420
like the concepts behind it these this

00:09:13,380 --> 00:09:16,950
isn't like drug product drastic

00:09:15,420 --> 00:09:18,900
groundbreaking stuff in terms of

00:09:16,950 --> 00:09:20,970
computer science

00:09:18,900 --> 00:09:22,740
it's not even groundbreaking from that

00:09:20,970 --> 00:09:25,410
respect in terms of Python because we

00:09:22,740 --> 00:09:28,400
have the piano and torch libraries which

00:09:25,410 --> 00:09:31,310
does which do pretty much the same thing

00:09:28,400 --> 00:09:34,290
so if you've used the uh north or torch

00:09:31,310 --> 00:09:39,570
you know everything what I'm telling you

00:09:34,290 --> 00:09:41,850
about and the the probably the main

00:09:39,570 --> 00:09:44,250
reason why tend to flow as a thing is

00:09:41,850 --> 00:09:46,440
that Google uses it Google basically

00:09:44,250 --> 00:09:50,040
runs on it or is transitioning to a

00:09:46,440 --> 00:09:52,650
point where Google run on it and it also

00:09:50,040 --> 00:09:53,910
that also means it's got backing by one

00:09:52,650 --> 00:09:57,410
of most powerful companies in the world

00:09:53,910 --> 00:09:59,490
so be highly unlikely that's going any

00:09:57,410 --> 00:10:01,680
highly unlikely that it goes away

00:09:59,490 --> 00:10:03,150
anytime soon and it's going to have a

00:10:01,680 --> 00:10:05,550
lot of resources thrown out both

00:10:03,150 --> 00:10:09,750
officially and unofficially because of

00:10:05,550 --> 00:10:12,420
its brand name recognition so that's why

00:10:09,750 --> 00:10:15,290
I made that prediction next year this

00:10:12,420 --> 00:10:18,960
will basically be tensorflow mini comp

00:10:15,290 --> 00:10:23,460
but we'll see what happens I'm on record

00:10:18,960 --> 00:10:27,210
now so this is an example graph stolen

00:10:23,460 --> 00:10:29,250
or borrowed from the documentation we

00:10:27,210 --> 00:10:31,950
basically see down the bottom there

00:10:29,250 --> 00:10:34,320
there's an input and then the lines

00:10:31,950 --> 00:10:36,780
basically say you know move this input

00:10:34,320 --> 00:10:41,310
up through this operation combine it

00:10:36,780 --> 00:10:43,830
with this other data and so on and we

00:10:41,310 --> 00:10:45,480
get to a point where all of the

00:10:43,830 --> 00:10:47,250
operations including calculating the

00:10:45,480 --> 00:10:49,350
accuracy for instance in the training

00:10:47,250 --> 00:10:51,810
the training accuracy they're all

00:10:49,350 --> 00:10:53,640
operations on this computational graph

00:10:51,810 --> 00:10:56,580
so we don't need to move away from this

00:10:53,640 --> 00:10:58,650
graph to train it we don't need to move

00:10:56,580 --> 00:10:59,820
away from this graph to evaluate it and

00:10:58,650 --> 00:11:04,380
we don't even need to move away from

00:10:59,820 --> 00:11:07,140
this graph to do the final prediction so

00:11:04,380 --> 00:11:07,770
I think here's stays intensify land

00:11:07,140 --> 00:11:11,820
which is

00:11:07,770 --> 00:11:15,030
good thing so here we've got a very very

00:11:11,820 --> 00:11:17,430
basic example so we import tensorflow

00:11:15,030 --> 00:11:21,900
and it seems to be that ten to flow is

00:11:17,430 --> 00:11:23,760
taken the two letter T F you'll say in

00:11:21,900 --> 00:11:27,630
the same way that numpy is NP everywhere

00:11:23,760 --> 00:11:31,590
to enter flows base is TF so in the

00:11:27,630 --> 00:11:33,990
first line we create a but let's just

00:11:31,590 --> 00:11:36,840
call it a variable X well I'll get back

00:11:33,990 --> 00:11:39,600
to the distinction in a bit later which

00:11:36,840 --> 00:11:42,030
is going to be the value 35 implicitly

00:11:39,600 --> 00:11:44,310
an integer and we give it a name X that

00:11:42,030 --> 00:11:46,080
name there is optional but really useful

00:11:44,310 --> 00:11:47,430
for debugging we're not going to use it

00:11:46,080 --> 00:11:49,260
anywhere else in this talk but it's good

00:11:47,430 --> 00:11:51,480
habit to get into

00:11:49,260 --> 00:11:54,480
we're then going to say y equals x plus

00:11:51,480 --> 00:11:58,640
5 but then if we print it out we don't

00:11:54,480 --> 00:12:01,560
get 40 we just get this operation object

00:11:58,640 --> 00:12:03,480
to actually run that we create a

00:12:01,560 --> 00:12:05,940
tensorflow session which connects to the

00:12:03,480 --> 00:12:07,140
back end we pass in at least the part of

00:12:05,940 --> 00:12:09,530
the computational graph that we're

00:12:07,140 --> 00:12:11,760
interested in so we're saying we want y

00:12:09,530 --> 00:12:14,340
do whatever you need to do to get me

00:12:11,760 --> 00:12:19,350
that value and this is the other

00:12:14,340 --> 00:12:22,010
advantage from say this graph so um this

00:12:19,350 --> 00:12:24,300
whole F here we can run the whole thing

00:12:22,010 --> 00:12:27,000
but if we're only interested in a small

00:12:24,300 --> 00:12:29,940
part of it so for instance we just

00:12:27,000 --> 00:12:31,650
wanted to calculate WX plus B we could

00:12:29,940 --> 00:12:33,390
ask for just that value and it will only

00:12:31,650 --> 00:12:40,020
do the computation necessary to get to

00:12:33,390 --> 00:12:42,540
that so so yeah so that's a very very

00:12:40,020 --> 00:12:44,790
basic example and then once we have once

00:12:42,540 --> 00:12:46,470
we do that run that prints gives us our

00:12:44,790 --> 00:12:50,430
result so you'll see this pattern a few

00:12:46,470 --> 00:12:52,170
times I'm going to use this image images

00:12:50,430 --> 00:12:53,970
are quite useful in this regard because

00:12:52,170 --> 00:12:57,660
you can read them in and they convert to

00:12:53,970 --> 00:13:00,450
useful large number arrays so if we can

00:12:57,660 --> 00:13:02,760
get a data set to work on they actually

00:13:00,450 --> 00:13:09,140
mean something I'm going to use this

00:13:02,760 --> 00:13:12,120
image so we read it in with matplotlib

00:13:09,140 --> 00:13:13,800
that gives us raw image data which is

00:13:12,120 --> 00:13:19,140
out which is basically a numpy array

00:13:13,800 --> 00:13:21,600
it has about 5000 rows three and a half

00:13:19,140 --> 00:13:24,529
thousand columns and three color depth

00:13:21,600 --> 00:13:28,019
that's three-dimensional numpy array

00:13:24,529 --> 00:13:30,889
we've created variable this is different

00:13:28,019 --> 00:13:34,889
to TF constant but I'm getting to that

00:13:30,889 --> 00:13:36,990
passing that raw image data we then

00:13:34,889 --> 00:13:38,670
initialize all the variables and I'm

00:13:36,990 --> 00:13:41,009
going to be honest here I don't know why

00:13:38,670 --> 00:13:42,750
I don't know why we have to do that step

00:13:41,009 --> 00:13:45,060
all the time and why can't be this part

00:13:42,750 --> 00:13:47,550
of running the graph but it's necessary

00:13:45,060 --> 00:13:49,680
so we create our variable X which is

00:13:47,550 --> 00:13:51,750
going to be a raw image data so we go

00:13:49,680 --> 00:13:54,569
from numpy object into a tensorflow

00:13:51,750 --> 00:13:56,819
object we then create a transpose

00:13:54,569 --> 00:14:00,509
operation which is the first tons of the

00:13:56,819 --> 00:14:03,300
context deck context manager

00:14:00,509 --> 00:14:07,019
so tell transpose operation we're going

00:14:03,300 --> 00:14:09,329
to take that X and permutate the axes so

00:14:07,019 --> 00:14:15,120
it basically just swap in the first and

00:14:09,329 --> 00:14:16,740
second axes turn that sideways so so

00:14:15,120 --> 00:14:19,440
that's our transpose operation we pass

00:14:16,740 --> 00:14:20,819
that into session run get the result out

00:14:19,440 --> 00:14:22,139
and then I just plot that here so you

00:14:20,819 --> 00:14:29,160
can see that with turn their image

00:14:22,139 --> 00:14:32,819
sideways okay so so going back to that

00:14:29,160 --> 00:14:35,880
variable will that constant what are

00:14:32,819 --> 00:14:39,329
they so there's actually a third one

00:14:35,880 --> 00:14:41,220
which is called placeholder so constants

00:14:39,329 --> 00:14:42,959
variables I really should have reordered

00:14:41,220 --> 00:14:45,029
these constants and variables are for

00:14:42,959 --> 00:14:48,329
when you have values placeholders for

00:14:45,029 --> 00:14:52,110
when you don't so with regards to fill

00:14:48,329 --> 00:14:53,910
in the computational graph you want to

00:14:52,110 --> 00:14:56,970
be able to build that graph and then

00:14:53,910 --> 00:14:58,769
push data into it later and then that

00:14:56,970 --> 00:15:02,279
gives us all the advantages tensorflow

00:14:58,769 --> 00:15:04,949
has so we use a placeholder for that so

00:15:02,279 --> 00:15:08,310
in the first example in cell 4

00:15:04,949 --> 00:15:10,050
we create a placeholder called X it's

00:15:08,310 --> 00:15:11,939
going to be of type float so we don't

00:15:10,050 --> 00:15:14,579
have any data yet so accounting firt ID

00:15:11,939 --> 00:15:16,019
but that's actually quite important and

00:15:14,579 --> 00:15:19,670
we say we're going to pass in three of

00:15:16,019 --> 00:15:22,589
those the size is optional but does help

00:15:19,670 --> 00:15:26,490
with in creative 8 seek operation wise X

00:15:22,589 --> 00:15:28,439
times 2 and then we pass in Y

00:15:26,490 --> 00:15:30,240
intercession that run print the result

00:15:28,439 --> 00:15:32,880
the difference between this and previous

00:15:30,240 --> 00:15:34,680
is this feed dictionary and what we're

00:15:32,880 --> 00:15:35,670
saying with feed dictionary is the

00:15:34,680 --> 00:15:37,830
values for x

00:15:35,670 --> 00:15:40,980
are going to be this this list here one

00:15:37,830 --> 00:15:42,810
two and three so we've created a

00:15:40,980 --> 00:15:46,050
computational graph which is all the

00:15:42,810 --> 00:15:47,490
steps above the width that was creating

00:15:46,050 --> 00:15:49,500
the computational graph and then we run

00:15:47,490 --> 00:15:51,710
it later we part we give in our data

00:15:49,500 --> 00:15:54,600
which we didn't even have at the start

00:15:51,710 --> 00:15:57,050
later and then that gives us our result

00:15:54,600 --> 00:15:58,740
each of those numbers multiplied by two

00:15:57,050 --> 00:16:00,750
where it starts to get a bit more

00:15:58,740 --> 00:16:02,640
interesting is placeholders with

00:16:00,750 --> 00:16:06,300
multi-dimensional so in this case we

00:16:02,640 --> 00:16:08,940
have a two dimensional placeholder the

00:16:06,300 --> 00:16:10,680
first is none the first the size of the

00:16:08,940 --> 00:16:12,270
first axis is none and the size at

00:16:10,680 --> 00:16:14,520
second axis is three so it's something

00:16:12,270 --> 00:16:17,010
by 3 matrix and by passing none it's

00:16:14,520 --> 00:16:18,660
arbitrary size and you'll probably see

00:16:17,010 --> 00:16:20,760
that quite a lot with machine learning

00:16:18,660 --> 00:16:22,710
because we're going to pass an arbitrary

00:16:20,760 --> 00:16:25,160
number of samples in with three features

00:16:22,710 --> 00:16:28,530
or something like that

00:16:25,160 --> 00:16:30,510
so that's sort of the common well that I

00:16:28,530 --> 00:16:33,300
think it will be a line that you might

00:16:30,510 --> 00:16:34,710
see quite a lot then from that on that

00:16:33,300 --> 00:16:37,500
point on it's all the same except for

00:16:34,710 --> 00:16:39,030
our data is now a two by three matrix so

00:16:37,500 --> 00:16:40,830
this could be a 3 by 3 or a 4 by 3

00:16:39,030 --> 00:16:46,560
matrix or so on and the code will work

00:16:40,830 --> 00:16:48,180
just fine getting back to our image here

00:16:46,560 --> 00:16:50,970
we create a placeholder which is a

00:16:48,180 --> 00:16:52,890
number I number three I could have

00:16:50,970 --> 00:16:53,910
worked out exactly how many pixels there

00:16:52,890 --> 00:16:57,330
where I probably should have done that

00:16:53,910 --> 00:17:00,540
but this will take an arbitrary image

00:16:57,330 --> 00:17:04,500
with three color depth or three levels

00:17:00,540 --> 00:17:06,180
of color depth and then reverse them the

00:17:04,500 --> 00:17:08,850
reverse operation here has a weird

00:17:06,180 --> 00:17:10,530
syntax basically saying reverse the

00:17:08,850 --> 00:17:14,490
first axes and keep the other two axes

00:17:10,530 --> 00:17:16,460
the same I'll get back to that because

00:17:14,490 --> 00:17:19,770
it annoys me

00:17:16,460 --> 00:17:22,500
so we create a placeholder and then in

00:17:19,770 --> 00:17:25,410
the first line under the width we then

00:17:22,500 --> 00:17:26,970
stay the image placeholder give it this

00:17:25,410 --> 00:17:28,760
raw image data and that's the data we

00:17:26,970 --> 00:17:33,990
loaded in from that plant lab earlier

00:17:28,760 --> 00:17:37,020
and then I printed the shape but then I

00:17:33,990 --> 00:17:39,210
show the image so in this case we we

00:17:37,020 --> 00:17:43,620
reverse the first axis which is rows

00:17:39,210 --> 00:17:46,470
flip the image over going back to the

00:17:43,620 --> 00:17:48,620
true true false true false false a lot

00:17:46,470 --> 00:17:48,620
of

00:17:49,180 --> 00:17:54,880
something I run into a lot and maybe

00:17:51,550 --> 00:17:56,050
it's just me is I sort of automatically

00:17:54,880 --> 00:17:57,970
convert an umpire

00:17:56,050 --> 00:17:59,680
operation into a tensorflow operation

00:17:57,970 --> 00:18:02,680
that won't work

00:17:59,680 --> 00:18:04,930
tensorflow have a lot of operations that

00:18:02,680 --> 00:18:07,390
are do similar things to numpy

00:18:04,930 --> 00:18:09,220
operations and they have somewhat

00:18:07,390 --> 00:18:10,750
similar signatures but at the end of the

00:18:09,220 --> 00:18:12,970
day you really need to check the

00:18:10,750 --> 00:18:15,790
tensorflow documentation for what those

00:18:12,970 --> 00:18:17,620
variables are because for instance I

00:18:15,790 --> 00:18:21,000
would have expected just to pass in a

00:18:17,620 --> 00:18:23,980
zero there to say reverse the first axis

00:18:21,000 --> 00:18:26,290
that would that's numpy thinking tend to

00:18:23,980 --> 00:18:28,870
flow thinking it actually does this

00:18:26,290 --> 00:18:30,940
quite a bit where you pass in values for

00:18:28,870 --> 00:18:33,030
each axis and we'll see that in a minute

00:18:30,940 --> 00:18:35,470
with them reverse sequence I think it is

00:18:33,030 --> 00:18:37,840
you end up giving another value across

00:18:35,470 --> 00:18:39,660
each axis and does an operation or at

00:18:37,840 --> 00:18:43,660
once

00:18:39,660 --> 00:18:45,640
so yeah quite a placeholder do an

00:18:43,660 --> 00:18:47,380
operation on that placeholder so for the

00:18:45,640 --> 00:18:48,790
first two lines only rarely

00:18:47,380 --> 00:18:50,800
computational graph and we don't even

00:18:48,790 --> 00:18:54,510
know what the image is yet and then run

00:18:50,800 --> 00:18:54,510
it later by passing out imaging

00:18:55,890 --> 00:19:02,950
constants you use these for values that

00:18:58,929 --> 00:19:05,020
don't change and days into opera so in

00:19:02,950 --> 00:19:06,820
this case we did know the image upfront

00:19:05,020 --> 00:19:08,620
so we can create a constant we don't

00:19:06,820 --> 00:19:11,200
need to pass any information like the

00:19:08,620 --> 00:19:13,360
size of it or the type of type of that

00:19:11,200 --> 00:19:16,690
that's all in third you can pass it if

00:19:13,360 --> 00:19:19,750
you want to and then we do a slice

00:19:16,690 --> 00:19:22,480
operation this was the other case with

00:19:19,750 --> 00:19:26,350
the axes so in this case we do a slice

00:19:22,480 --> 00:19:28,450
operation on the image the first list

00:19:26,350 --> 00:19:30,880
there is the starting position across

00:19:28,450 --> 00:19:33,940
each axes and the second list there is

00:19:30,880 --> 00:19:39,809
the size with negative one being go to

00:19:33,940 --> 00:19:42,970
the end which is not like that at all

00:19:39,809 --> 00:19:44,530
that's probably okay being picky here

00:19:42,970 --> 00:19:46,780
it's probably something like there's a

00:19:44,530 --> 00:19:49,030
nun nun or something like that but um

00:19:46,780 --> 00:19:50,530
but yeah so this is a common pattern you

00:19:49,030 --> 00:19:53,440
give it the data and you give it for

00:19:50,530 --> 00:19:55,740
multi-dimensional operations you give it

00:19:53,440 --> 00:19:58,060
what to do across each of those axes

00:19:55,740 --> 00:20:00,630
which is probably a bit more efficient

00:19:58,060 --> 00:20:02,500
but it's a little bit annoying as well

00:20:00,630 --> 00:20:03,820
so

00:20:02,500 --> 00:20:05,350
but apart from that everything else here

00:20:03,820 --> 00:20:08,110
is fine the difference the only

00:20:05,350 --> 00:20:09,970
difference really apart from the exact

00:20:08,110 --> 00:20:12,370
operation between this and the previous

00:20:09,970 --> 00:20:14,409
is I use a placeholder here and give it

00:20:12,370 --> 00:20:15,909
the information leader and I use the

00:20:14,409 --> 00:20:17,620
constant hearing over the information

00:20:15,909 --> 00:20:19,240
straight away so the information was

00:20:17,620 --> 00:20:22,900
included as part of the computational

00:20:19,240 --> 00:20:24,789
graph in this point which made which at

00:20:22,900 --> 00:20:28,440
least normally in this case is not what

00:20:24,789 --> 00:20:32,500
you want but serves as a useful idea

00:20:28,440 --> 00:20:35,909
variables are placeholders for things

00:20:32,500 --> 00:20:38,830
that will change later so in this case

00:20:35,909 --> 00:20:43,809
this is a standard linear regression

00:20:38,830 --> 00:20:46,840
model ax plus B we I'll get back to that

00:20:43,809 --> 00:20:52,750
I'll get to the training in a minute but

00:20:46,840 --> 00:20:56,740
B so there we go that's the key key line

00:20:52,750 --> 00:21:01,240
there the W so in that case those are

00:20:56,740 --> 00:21:02,860
our weights into our algorithm so that's

00:21:01,240 --> 00:21:07,090
a variable so we're going to learn those

00:21:02,860 --> 00:21:09,909
weights so the idea here is we're going

00:21:07,090 --> 00:21:11,200
to we are we know what the model is we

00:21:09,909 --> 00:21:13,150
don't know what the particular variables

00:21:11,200 --> 00:21:15,490
the values of a and B are so we're going

00:21:13,150 --> 00:21:16,840
to learn those by giving it some data

00:21:15,490 --> 00:21:18,429
from that model and then it's going to

00:21:16,840 --> 00:21:23,140
go off and try and predict what those

00:21:18,429 --> 00:21:25,059
days are so W is a variable it has two

00:21:23,140 --> 00:21:28,210
two values in it but those will be

00:21:25,059 --> 00:21:33,070
updated as part of the learning why-why

00:21:28,210 --> 00:21:36,190
model is an operation on W so so at this

00:21:33,070 --> 00:21:38,350
point not that no work actually gets

00:21:36,190 --> 00:21:40,200
done but it just identifies it that's

00:21:38,350 --> 00:21:43,900
the relationship between those values

00:21:40,200 --> 00:21:45,640
and then and then later on we're going

00:21:43,900 --> 00:21:47,049
to pass in x and y values which are

00:21:45,640 --> 00:21:50,100
represented there as placeholders so

00:21:47,049 --> 00:21:52,659
that later on would give those days

00:21:50,100 --> 00:21:55,179
they'll come with variables is training

00:21:52,659 --> 00:21:59,919
them so we've got a great interest and

00:21:55,179 --> 00:22:01,480
optimizer this is basically a sub graph

00:21:59,919 --> 00:22:04,750
that you can include in your graph to do

00:22:01,480 --> 00:22:06,610
the training how particular variables so

00:22:04,750 --> 00:22:09,100
someone else is written up written that

00:22:06,610 --> 00:22:11,850
for you you could reimplemented yourself

00:22:09,100 --> 00:22:16,120
it's just just part of it's just a graph

00:22:11,850 --> 00:22:18,520
and then it's job is to minimize error

00:22:16,120 --> 00:22:20,620
an error if you go up two lines is the

00:22:18,520 --> 00:22:23,620
square of the difference between the

00:22:20,620 --> 00:22:27,640
model and their given base so that's a

00:22:23,620 --> 00:22:29,559
tensor flow a tensor and then it's up to

00:22:27,640 --> 00:22:33,309
grating to send optimizer to take that

00:22:29,559 --> 00:22:36,970
work out how to minimize that and how it

00:22:33,309 --> 00:22:40,770
minimizes that is is done by changing Y

00:22:36,970 --> 00:22:44,049
model which is done by change in W the

00:22:40,770 --> 00:22:46,450
advantage of this over say something a

00:22:44,049 --> 00:22:48,700
bit more direct or say Python land is

00:22:46,450 --> 00:22:50,409
that because error has information about

00:22:48,700 --> 00:22:52,149
how it's created because it's a

00:22:50,409 --> 00:22:56,070
relationship between different variables

00:22:52,149 --> 00:22:58,510
things like gradient and derivative

00:22:56,070 --> 00:23:00,730
derivatives can be calculated where

00:22:58,510 --> 00:23:02,830
possible so you don't need to parse in

00:23:00,730 --> 00:23:04,570
derivatives and stuff like that because

00:23:02,830 --> 00:23:10,299
it knows those operations it can do

00:23:04,570 --> 00:23:12,789
smart things in the background yeah so

00:23:10,299 --> 00:23:18,309
those are the secret variables so you

00:23:12,789 --> 00:23:23,230
know ax plus B those are values so we

00:23:18,309 --> 00:23:24,549
open up a session then we do 1000

00:23:23,230 --> 00:23:26,260
training loops and there's other ways to

00:23:24,549 --> 00:23:28,059
do this with convergence and stuff like

00:23:26,260 --> 00:23:30,190
that but in this case we just do 1000

00:23:28,059 --> 00:23:36,760
training loops where we give it random

00:23:30,190 --> 00:23:39,399
data so random X values we pass in we

00:23:36,760 --> 00:23:41,860
calculate the actual Y values from those

00:23:39,399 --> 00:23:44,220
X values pass those en is our feed

00:23:41,860 --> 00:23:47,080
dictionary to the x and y placeholders

00:23:44,220 --> 00:23:48,159
and then do that training operation

00:23:47,080 --> 00:23:50,470
which was that gradient descent

00:23:48,159 --> 00:23:52,720
optimizer so you so what it's saying is

00:23:50,470 --> 00:23:56,710
here is some data try and find the a in

00:23:52,720 --> 00:23:58,029
a in B values the best minimize this and

00:23:56,710 --> 00:24:00,940
then we just keep doing that the more

00:23:58,029 --> 00:24:02,620
this this one this runs the better or

00:24:00,940 --> 00:24:08,200
the closer it gets the actual values so

00:24:02,620 --> 00:24:12,520
we had 206 so there and it's estimated

00:24:08,200 --> 00:24:14,049
2.2 and 5.8 so pretty close I would have

00:24:12,520 --> 00:24:15,850
expected that to be a big close for such

00:24:14,049 --> 00:24:24,460
a simple example and at thousand

00:24:15,850 --> 00:24:25,960
iterations but no no now one one sort of

00:24:24,460 --> 00:24:27,460
hidden feature of tensorflow that I

00:24:25,960 --> 00:24:30,040
found really really useful when I was

00:24:27,460 --> 00:24:33,730
learning tensorflow

00:24:30,040 --> 00:24:36,460
is hi funk and hi funk which is the in

00:24:33,730 --> 00:24:38,920
the last line of cell 12

00:24:36,460 --> 00:24:39,910
basically takes a Python function and

00:24:38,920 --> 00:24:43,000
turns it into a node of your

00:24:39,910 --> 00:24:44,620
computational graph so that if you know

00:24:43,000 --> 00:24:45,820
how to do it in numpy or you know how to

00:24:44,620 --> 00:24:47,650
do it in Python but you don't know how

00:24:45,820 --> 00:24:49,930
to do an intent to flow or there's not

00:24:47,650 --> 00:24:52,300
the functions of doing intensive flow

00:24:49,930 --> 00:24:53,710
this can help you cross that hurdle so

00:24:52,300 --> 00:24:55,600
you don't spend now looking through the

00:24:53,710 --> 00:24:58,030
documentation to work out that what that

00:24:55,600 --> 00:25:00,310
step is so I found this really useful to

00:24:58,030 --> 00:25:03,420
sort of build computational graphs along

00:25:00,310 --> 00:25:06,720
these lines and then swap those out with

00:25:03,420 --> 00:25:09,070
with actual tensorflow operations later

00:25:06,720 --> 00:25:11,590
so in this case we have a game of life

00:25:09,070 --> 00:25:15,400
implementation so an update board will

00:25:11,590 --> 00:25:17,260
take a game of life board and then

00:25:15,400 --> 00:25:18,400
update it according to the rules of game

00:25:17,260 --> 00:25:22,690
of life

00:25:18,400 --> 00:25:24,490
Jake's website there has a really small

00:25:22,690 --> 00:25:27,310
one liner which uses Conville

00:25:24,490 --> 00:25:31,390
convolution to to work out how many

00:25:27,310 --> 00:25:33,160
neighbors cell has so feel free to ask a

00:25:31,390 --> 00:25:35,800
question if you're not sure about a game

00:25:33,160 --> 00:25:38,650
of life but I'll skip over those details

00:25:35,800 --> 00:25:40,150
for now but basically we take a next we

00:25:38,650 --> 00:25:42,580
update according to some rules and we

00:25:40,150 --> 00:25:45,880
return another X but that's all done in

00:25:42,580 --> 00:25:48,040
um PI or Sipe I convolve to these aside

00:25:45,880 --> 00:25:50,680
PI operation so you can just pretend

00:25:48,040 --> 00:25:52,860
that I didn't actually realize there was

00:25:50,680 --> 00:25:56,440
it convolve operation intensive flow

00:25:52,860 --> 00:26:00,820
when I wrote this and that's actually

00:25:56,440 --> 00:26:02,170
what happened so so yeah so we can

00:26:00,820 --> 00:26:04,180
replace all this with the tensor flow

00:26:02,170 --> 00:26:06,850
operation but this is a very good

00:26:04,180 --> 00:26:09,190
example I think for Python so we create

00:26:06,850 --> 00:26:10,960
that board update which is a Python so

00:26:09,190 --> 00:26:12,400
create a Python function turns it into a

00:26:10,960 --> 00:26:15,850
10-2 flowed node and puts it on a

00:26:12,400 --> 00:26:17,500
computational graph and then from that

00:26:15,850 --> 00:26:19,660
point on everything else is pretty

00:26:17,500 --> 00:26:21,840
straightforward we create initial board

00:26:19,660 --> 00:26:24,040
and tend to flow have some random

00:26:21,840 --> 00:26:25,690
generators so you don't need to create

00:26:24,040 --> 00:26:30,580
an umpire random and then put it into

00:26:25,690 --> 00:26:32,110
flow and then all the rest here is just

00:26:30,580 --> 00:26:34,630
running that cell and then doing a

00:26:32,110 --> 00:26:39,960
matplotlib animation so I'll show you

00:26:34,630 --> 00:26:39,960
that see both

00:26:41,519 --> 00:26:53,679
made enough sacrifices to the demo gods

00:26:46,529 --> 00:26:55,600
so Python so so in this case is a very

00:26:53,679 --> 00:26:57,340
large game of life operations all been

00:26:55,600 --> 00:26:58,659
done on the GPU and I'll get to that in

00:26:57,340 --> 00:27:01,809
a second

00:26:58,659 --> 00:27:04,240
so this takes an X which represents one

00:27:01,809 --> 00:27:06,610
of these States updates according to the

00:27:04,240 --> 00:27:11,619
game of life and then shows the next

00:27:06,610 --> 00:27:17,289
step so that's very large that's 2000 by

00:27:11,619 --> 00:27:20,139
2000 which would hurt other systems but

00:27:17,289 --> 00:27:23,769
in this case it does quite well and that

00:27:20,139 --> 00:27:28,149
that was done using a Python so to

00:27:23,769 --> 00:27:30,340
represent that part of the graph the

00:27:28,149 --> 00:27:32,710
other thing you might have noticed is

00:27:30,340 --> 00:27:35,919
all this output here which base which is

00:27:32,710 --> 00:27:42,809
that the tail they are in basically it

00:27:35,919 --> 00:27:44,889
was running on the GPU cool all right so

00:27:42,809 --> 00:27:48,820
so let's get into a little bit of

00:27:44,889 --> 00:27:50,200
machine learning now so you might have

00:27:48,820 --> 00:27:52,330
heard of a package or you might have

00:27:50,200 --> 00:27:53,860
heard rumors of a package called SK flow

00:27:52,330 --> 00:27:57,129
that doesn't exist anymore so they'll

00:27:53,860 --> 00:27:58,929
tend to float learn and someone has a

00:27:57,129 --> 00:28:01,240
website called learning tensorflow and

00:27:58,929 --> 00:28:05,139
that's actually give me a good boost I

00:28:01,240 --> 00:28:09,190
swear I didn't do that on purpose but

00:28:05,139 --> 00:28:10,450
ten to fly learn is basically wrap wraps

00:28:09,190 --> 00:28:12,129
a whole bunch of learning algorithms

00:28:10,450 --> 00:28:13,330
that are implemented in tensor flow and

00:28:12,129 --> 00:28:15,460
wrap some according to scikit-learn

00:28:13,330 --> 00:28:18,940
interface so that if you notice I get

00:28:15,460 --> 00:28:20,139
learn you can just basically copy copied

00:28:18,940 --> 00:28:22,779
your psychic learning code and just

00:28:20,139 --> 00:28:25,210
start doing stuff intensive flow so so

00:28:22,779 --> 00:28:26,200
I've got that numpy is the scikit-learn

00:28:25,210 --> 00:28:31,059
what tend to flows

00:28:26,200 --> 00:28:34,720
tend to fail earn and and if you've done

00:28:31,059 --> 00:28:37,539
any scikit-learn before then then this

00:28:34,720 --> 00:28:39,730
will all look familiar I had a bug which

00:28:37,539 --> 00:28:41,320
made this didn't work up until about

00:28:39,730 --> 00:28:46,119
three hours ago so I was able to fix it

00:28:41,320 --> 00:28:49,240
pretty happy so but basically the idea

00:28:46,119 --> 00:28:50,769
here is that we we created classifier

00:28:49,240 --> 00:28:53,020
which is our second last line which is

00:28:50,769 --> 00:28:56,040
just a shell of a classifier and

00:28:53,020 --> 00:29:00,040
we fit it with some training data and

00:28:56,040 --> 00:29:02,830
then and then we use that model to do

00:29:00,040 --> 00:29:05,920
prediction and then we can see there the

00:29:02,830 --> 00:29:07,810
output there so in this case if you're

00:29:05,920 --> 00:29:09,520
interested what we do is we load the

00:29:07,810 --> 00:29:12,250
digits data set which is handwritten

00:29:09,520 --> 00:29:14,140
digits for and the job of the machine

00:29:12,250 --> 00:29:16,030
learning is to work out what the digit

00:29:14,140 --> 00:29:20,200
is that's been handwritten whether it's

00:29:16,030 --> 00:29:21,940
a 1 or a 2 or so on so in this case it's

00:29:20,200 --> 00:29:24,330
able to predict when the number is a

00:29:21,940 --> 00:29:28,060
zero and so will predict that with 98%

00:29:24,330 --> 00:29:30,880
precision and perfect recall so it's a

00:29:28,060 --> 00:29:37,090
pretty good model but that's well known

00:29:30,880 --> 00:29:39,340
data set so so let's move on to you so

00:29:37,090 --> 00:29:45,790
the example I gave before was running on

00:29:39,340 --> 00:29:48,460
on my GPU and the reason the reason this

00:29:45,790 --> 00:29:50,230
is a good thing is CPUs are quite smart

00:29:48,460 --> 00:29:52,140
they have a lot of operations for doing

00:29:50,230 --> 00:29:55,900
things efficiently but they're generally

00:29:52,140 --> 00:29:57,460
single or single ish thread operations

00:29:55,900 --> 00:29:59,800
so they can do one thing very well at a

00:29:57,460 --> 00:30:02,290
time GPUs have a whole bunch of really

00:29:59,800 --> 00:30:06,910
dumb CPUs if I want to gloss over a lot

00:30:02,290 --> 00:30:09,370
of technical details so-so GPUs have a

00:30:06,910 --> 00:30:12,250
modern GPU will have thousands of little

00:30:09,370 --> 00:30:17,140
processes running but they all do one

00:30:12,250 --> 00:30:19,240
thing and but they can all take and do

00:30:17,140 --> 00:30:21,490
it across a whole data set all at one

00:30:19,240 --> 00:30:23,140
time the good thing for us as machine

00:30:21,490 --> 00:30:26,770
learners is that that one thing usually

00:30:23,140 --> 00:30:29,770
involves operations on matrices which is

00:30:26,770 --> 00:30:32,380
like 90% of that data mining so it's

00:30:29,770 --> 00:30:34,030
really good for us so all the people

00:30:32,380 --> 00:30:35,380
playing video games have really helps

00:30:34,030 --> 00:30:41,530
the field of machine learning quite a

00:30:35,380 --> 00:30:43,860
lot alright so all of this is going to

00:30:41,530 --> 00:30:47,260
look quite familiar to you except for

00:30:43,860 --> 00:30:49,200
the width at the top so in this case we

00:30:47,260 --> 00:30:51,760
set a device name which is the first CPU

00:30:49,200 --> 00:30:54,070
we then create a contact manager and we

00:30:51,760 --> 00:30:56,500
say for these operations do it on that

00:30:54,070 --> 00:30:59,590
device yep

00:30:56,500 --> 00:31:01,900
and so do those operations on that

00:30:59,590 --> 00:31:03,160
device and then the rest of it is pretty

00:31:01,900 --> 00:31:04,960
much just running that there's a session

00:31:03,160 --> 00:31:06,940
confit in there if you want to have a

00:31:04,960 --> 00:31:09,820
look at the code later

00:31:06,940 --> 00:31:12,429
so in this case we run this dot product

00:31:09,820 --> 00:31:17,409
and sum in on a cpu and the bottom line

00:31:12,429 --> 00:31:19,299
there took nearly 12 seconds and in this

00:31:17,409 --> 00:31:22,870
one here we just changed it over to the

00:31:19,299 --> 00:31:25,830
GPU and it took 1.3 seconds GPS are

00:31:22,870 --> 00:31:28,899
really good at those types of operations

00:31:25,830 --> 00:31:30,580
but unfortunately not all machine

00:31:28,899 --> 00:31:32,529
learning operations are those type of

00:31:30,580 --> 00:31:33,940
operations so sometimes you have to

00:31:32,529 --> 00:31:36,429
switch intensify gives us that ability

00:31:33,940 --> 00:31:37,870
run some nodes on the CPU run some nodes

00:31:36,429 --> 00:31:42,090
on the GPU combine them later

00:31:37,870 --> 00:31:42,090
keep in mind that switching is expensive

00:31:43,139 --> 00:31:53,139
there's also Kerris which used to work

00:31:48,370 --> 00:31:56,710
on piano still does but you see and but

00:31:53,139 --> 00:31:58,929
now it's also tensorflow enabled so so

00:31:56,710 --> 00:32:00,639
carers in that case I'm just doing one

00:31:58,929 --> 00:32:04,360
hot transformation carers can be pip

00:32:00,639 --> 00:32:06,610
installed and the only real thing that

00:32:04,360 --> 00:32:08,679
you need to do is create a JSON file

00:32:06,610 --> 00:32:11,710
that has the back end there's tensorflow

00:32:08,679 --> 00:32:13,509
so that's all I'm doing there so that

00:32:11,710 --> 00:32:16,990
just tells carers to use 10 to flow

00:32:13,509 --> 00:32:18,960
instead of Yano by default and then at

00:32:16,990 --> 00:32:21,730
this point we were actually doing

00:32:18,960 --> 00:32:24,220
creating a neural network by using a

00:32:21,730 --> 00:32:26,070
sequential model and a whole bunch of

00:32:24,220 --> 00:32:28,029
layers to the neural network I'm

00:32:26,070 --> 00:32:29,769
deliberately glossing over a whole bunch

00:32:28,029 --> 00:32:32,769
of details I was given extra time and

00:32:29,769 --> 00:32:35,559
I'm still running over time and then

00:32:32,769 --> 00:32:38,559
just fitting and training that so that

00:32:35,559 --> 00:32:39,460
secures so the good news there is if you

00:32:38,559 --> 00:32:40,929
want to do machine learning with

00:32:39,460 --> 00:32:43,210
tensorflow you don't need to know all

00:32:40,929 --> 00:32:44,710
the plumbing details there's really good

00:32:43,210 --> 00:32:47,409
libraries popping up and over the next

00:32:44,710 --> 00:32:50,860
year or so we're going to get a huge

00:32:47,409 --> 00:32:52,779
proliferation of and flow across in the

00:32:50,860 --> 00:32:55,629
backend of all pretty much all machine

00:32:52,779 --> 00:33:00,610
learning in python over the next few

00:32:55,629 --> 00:33:03,309
years so that's my talk I'm Robert

00:33:00,610 --> 00:33:04,870
Leighton at those things and feel free

00:33:03,309 --> 00:33:06,820
to drop me a line I'm not a very active

00:33:04,870 --> 00:33:08,230
on Twitter but Twitter pings my email

00:33:06,820 --> 00:33:09,399
which brings my phone so you'll still

00:33:08,230 --> 00:33:13,110
get to me

00:33:09,399 --> 00:33:13,110
so thanks

00:33:19,630 --> 00:33:23,810
thank you Robert for this talk we have

00:33:21,950 --> 00:33:26,900
some time for questions I'd like to see

00:33:23,810 --> 00:33:28,940
raised hands and how one first question

00:33:26,900 --> 00:33:31,790
is how much do you does the programmer

00:33:28,940 --> 00:33:33,830
have to think about where they want to

00:33:31,790 --> 00:33:35,900
perform their computation how much do

00:33:33,830 --> 00:33:38,270
you have to say well I have a bigger GPU

00:33:35,900 --> 00:33:41,240
a smaller GPU I have more memory when

00:33:38,270 --> 00:33:43,040
you do this kind of oh absolutely yeah

00:33:41,240 --> 00:33:47,000
tensile I will tell you when it can't do

00:33:43,040 --> 00:33:49,100
an operation on GPU so if you want to be

00:33:47,000 --> 00:33:50,450
empirical about it you the workflow is

00:33:49,100 --> 00:33:54,590
basically try it on GP and if that

00:33:50,450 --> 00:33:57,290
doesn't work put on a CPU but but I

00:33:54,590 --> 00:33:59,150
don't think at least for most of our

00:33:57,290 --> 00:34:00,140
most of the stuff most of the people

00:33:59,150 --> 00:34:01,550
here are going to do I don't think

00:34:00,140 --> 00:34:03,950
there's a huge amount of thought that

00:34:01,550 --> 00:34:07,280
needs to go behind it but then there's

00:34:03,950 --> 00:34:09,530
those one percenters then really need to

00:34:07,280 --> 00:34:11,419
care about it but I think you'll know if

00:34:09,530 --> 00:34:12,650
you need to care about how much memory

00:34:11,419 --> 00:34:18,409
you're moving around and stuff like that

00:34:12,650 --> 00:34:20,810
yeah will had question so two questions

00:34:18,409 --> 00:34:25,100
on this side and I'm going to let you go

00:34:20,810 --> 00:34:26,720
first since you're building like a large

00:34:25,100 --> 00:34:28,460
computation graph then running it is

00:34:26,720 --> 00:34:30,620
like a single step what happens for

00:34:28,460 --> 00:34:35,240
debugging that when it doesn't do what

00:34:30,620 --> 00:34:37,429
you expected it to yes so so debugging

00:34:35,240 --> 00:34:39,800
machine learning is a hard problem and

00:34:37,429 --> 00:34:40,159
I'm going to pretend that's a different

00:34:39,800 --> 00:34:42,980
question

00:34:40,159 --> 00:34:45,409
debugging that the tensorflow part of

00:34:42,980 --> 00:34:48,710
the machine learning process if you give

00:34:45,409 --> 00:34:52,550
your variables names and by default

00:34:48,710 --> 00:34:54,470
they'll get a name corresponding to the

00:34:52,550 --> 00:34:58,220
name of the variable you assigned to it

00:34:54,470 --> 00:35:01,730
so so in this case the board here is a

00:34:58,220 --> 00:35:03,200
little bit redundant it'll tell you that

00:35:01,730 --> 00:35:05,930
there's some problem with the board

00:35:03,200 --> 00:35:08,990
variable but if you start giving names

00:35:05,930 --> 00:35:13,820
the things that the stack that trace

00:35:08,990 --> 00:35:15,080
will say variable X 0 was expecting data

00:35:13,820 --> 00:35:17,930
of this format and it's not of that

00:35:15,080 --> 00:35:19,460
format or something like that so if

00:35:17,930 --> 00:35:25,310
you're get into habit of having names

00:35:19,460 --> 00:35:26,700
and I've lost the name of it but there's

00:35:25,310 --> 00:35:28,320
also

00:35:26,700 --> 00:35:30,839
going to call it context so you can open

00:35:28,320 --> 00:35:32,880
up a context and all variables defines

00:35:30,839 --> 00:35:35,070
within that context get a get that

00:35:32,880 --> 00:35:38,339
prepended so you can say things like

00:35:35,070 --> 00:35:41,220
this is the lay one context and this is

00:35:38,339 --> 00:35:42,780
layer ones way so then when that prints

00:35:41,220 --> 00:35:45,690
out in a stack trace it'll say layer one

00:35:42,780 --> 00:35:47,339
don't wait so if you get used to naming

00:35:45,690 --> 00:35:51,589
them that don't come out in stack trace

00:35:47,339 --> 00:35:54,359
and make upon in those much easier yeah

00:35:51,589 --> 00:35:57,329
I've sort of got two questions the first

00:35:54,359 --> 00:36:00,720
question is if each node or tensor in

00:35:57,329 --> 00:36:04,380
the graph is this fun operation is that

00:36:00,720 --> 00:36:08,160
now just to sort of like a psychic loan

00:36:04,380 --> 00:36:11,760
pipeline like is it I work for yeah yeah

00:36:08,160 --> 00:36:13,829
it's um it's very similar to that yeah

00:36:11,760 --> 00:36:16,050
as I said before like this is not

00:36:13,829 --> 00:36:17,970
groundbreaking from a theoretical point

00:36:16,050 --> 00:36:19,380
of view it's probably more

00:36:17,970 --> 00:36:21,869
groundbreaking from a practical point of

00:36:19,380 --> 00:36:24,060
view but because of the resources behind

00:36:21,869 --> 00:36:25,910
it and just the general efficiency of

00:36:24,060 --> 00:36:29,010
these things hence flows probably

00:36:25,910 --> 00:36:31,410
probably in the lead in my browser and

00:36:29,010 --> 00:36:33,890
how does tensor flow compared to fair

00:36:31,410 --> 00:36:36,599
know in terms of ease of use and

00:36:33,890 --> 00:36:39,660
performance if you're using carers it's

00:36:36,599 --> 00:36:40,560
the same so you just switch the backend

00:36:39,660 --> 00:36:42,930
actually it's a little more difficult

00:36:40,560 --> 00:36:45,839
because you need to change this bulb but

00:36:42,930 --> 00:36:47,940
but in terms of the actual plumbing like

00:36:45,839 --> 00:36:49,410
if you want to do it yourself

00:36:47,940 --> 00:36:52,349
I think tents files a little bit a

00:36:49,410 --> 00:36:53,700
little bit easier but it's more recent

00:36:52,349 --> 00:36:57,869
in my memory and I've had a bit more

00:36:53,700 --> 00:36:59,339
experience with that so yeah so I'm

00:36:57,869 --> 00:37:02,940
gonna like ask you the question that I

00:36:59,339 --> 00:37:04,200
got asked um which is so this is all

00:37:02,940 --> 00:37:06,720
machine learning in PI the new be done

00:37:04,200 --> 00:37:14,280
in tensorflow next year or not and if

00:37:06,720 --> 00:37:18,390
not why okay so I think I think the

00:37:14,280 --> 00:37:20,010
large majority of at least we're going

00:37:18,390 --> 00:37:21,630
to hit a tipping point where new

00:37:20,010 --> 00:37:23,150
academic research is going to come out

00:37:21,630 --> 00:37:26,160
with tensorflow

00:37:23,150 --> 00:37:27,660
but if you think about how much academic

00:37:26,160 --> 00:37:31,099
work still comes out with very old

00:37:27,660 --> 00:37:33,780
languages then it's a very slow process

00:37:31,099 --> 00:37:37,260
but I think this is I think tensorflow

00:37:33,780 --> 00:37:39,390
generally is the direction to take so if

00:37:37,260 --> 00:37:40,530
I was like my old role as a machine

00:37:39,390 --> 00:37:42,890
learning researcher

00:37:40,530 --> 00:37:46,080
we start using this for a research and

00:37:42,890 --> 00:37:51,540
delivering code with tensorflow yeah

00:37:46,080 --> 00:37:55,200
oh okay so so limitations of tensorflow

00:37:51,540 --> 00:37:58,170
yeah I suppose the main limitations tend

00:37:55,200 --> 00:38:01,320
to flow is a dozen scaled-down well so

00:37:58,170 --> 00:38:03,570
this is for you you can do your standard

00:38:01,320 --> 00:38:04,890
iris data mining problem with 150

00:38:03,570 --> 00:38:07,470
samples you can definitely do that with

00:38:04,890 --> 00:38:09,390
pencil flow but you shouldn't need to

00:38:07,470 --> 00:38:10,950
follow because it's in memory you're

00:38:09,390 --> 00:38:12,780
just adding a whole bunch of overhead

00:38:10,950 --> 00:38:15,180
that you don't really need

00:38:12,780 --> 00:38:17,670
but that said when you get down to that

00:38:15,180 --> 00:38:18,990
size a little bit of extra writing time

00:38:17,670 --> 00:38:21,030
doesn't really matter so you can

00:38:18,990 --> 00:38:25,200
probably pour that overhead if you think

00:38:21,030 --> 00:38:27,240
your problem is going to grow yeah I'm

00:38:25,200 --> 00:38:28,680
just wondering if you've got a data set

00:38:27,240 --> 00:38:30,120
that you want to work on the doesn't fit

00:38:28,680 --> 00:38:32,670
in my memory is there a way to

00:38:30,120 --> 00:38:36,110
iteratively feed it into tense flow yes

00:38:32,670 --> 00:38:36,110
so if you think about

00:38:36,770 --> 00:38:46,860
so this learned of linear model if we

00:38:41,760 --> 00:38:50,280
change the size of this from the current

00:38:46,860 --> 00:38:52,770
one to just enough to fit in memory then

00:38:50,280 --> 00:38:55,080
this will run and then the next time X

00:38:52,770 --> 00:38:56,870
variable we over in written with new

00:38:55,080 --> 00:38:59,160
data that's just enough to fit in memory

00:38:56,870 --> 00:39:02,010
so effectively we have a training data

00:38:59,160 --> 00:39:02,580
set that's too big for memory but that's

00:39:02,010 --> 00:39:06,360
the basic way

00:39:02,580 --> 00:39:09,180
chunk it and just pass those chunks in

00:39:06,360 --> 00:39:14,370
one at a time yeah we have time for one

00:39:09,180 --> 00:39:16,170
last question and I that that's pretty

00:39:14,370 --> 00:39:18,480
exciting that I can just select which

00:39:16,170 --> 00:39:20,460
device a GPU rather than having to learn

00:39:18,480 --> 00:39:22,650
CUDA or open C or myself for certain

00:39:20,460 --> 00:39:27,750
parts of the problems that I face so

00:39:22,650 --> 00:39:29,760
it's pretty exciting and which which GPS

00:39:27,750 --> 00:39:31,110
does it have better support for is it

00:39:29,760 --> 00:39:33,840
going to run better on ati or nvidia

00:39:31,110 --> 00:39:35,670
does it use a mix of OpenCL or CUDA

00:39:33,840 --> 00:39:37,890
depending on which device it detects so

00:39:35,670 --> 00:39:41,640
yeah so my experience is only being with

00:39:37,890 --> 00:39:44,760
CUDA and I'm pretty sure that is what's

00:39:41,640 --> 00:39:47,400
supported I haven't seen anything really

00:39:44,760 --> 00:39:51,650
that talked about graphics cards are the

00:39:47,400 --> 00:39:53,990
varying Nvidia and I think

00:39:51,650 --> 00:39:58,340
and then deliberately baekje I think you

00:39:53,990 --> 00:39:59,990
need to compute power of 3/2 or I think

00:39:58,340 --> 00:40:01,250
the expectation is your graphics card

00:39:59,990 --> 00:40:04,610
will have you compute power of three or

00:40:01,250 --> 00:40:06,530
higher to to run as a GPU intensive flow

00:40:04,610 --> 00:40:08,180
I think old ones will work but may not

00:40:06,530 --> 00:40:10,970
be well supported

00:40:08,180 --> 00:40:12,380
but yeah you say you don't need to learn

00:40:10,970 --> 00:40:14,500
CUDA but you need to learn to at least

00:40:12,380 --> 00:40:19,940
install it which is a little bit payable

00:40:14,500 --> 00:40:22,580
yeah cool thank you Robert and we have

00:40:19,940 --> 00:40:27,590
in I'm gonna give you on behalf of Pike

00:40:22,580 --> 00:40:29,990
on a mug antastic it's it has the flying

00:40:27,590 --> 00:40:32,600
tram the symbol of Melbourne and all I

00:40:29,990 --> 00:40:36,340
see is it's on the t-shirts also and

00:40:32,600 --> 00:40:36,340

YouTube URL: https://www.youtube.com/watch?v=bvHgESVuS6Q


