Title: Keynote: Consequences of an Insightful Algorithm by Carina C. Zona
Publication date: 2015-08-02
Playlist: PyCon Australia 2015 Keynotes
Description: 
	We have ethical responsibilities when coding. We’re able to extract remarkably precise intuitions about an individual. But do we have a right to know what they didn’t consent to share, even when they willingly shared the data that leads us there? A major retailer’s data-driven marketing accidentally revealed to a teen’s family that she was pregnant. Eek.

What are our obligations to people who did not expect themselves to be so intimately known without sharing directly? How do we mitigate against unintended outcomes? For instance, an activity tracker carelessly revealed users’ sexual activity data to search engines. A social network’s algorithm accidentally triggered painful memories for grieving families who’d recently experienced death of their child and other loved ones.

We design software for humans. Balancing human needs and business specs can be tough. It’s crucial that we learn how to build in systematic empathy.

In this talk, we’ll delve into specific examples of uncritical programming, and painful results from using insightful data in ways that were benignly intended. You’ll learn ways we can integrate practices for examining how our code might harm individuals. We’ll look at how to flip the paradigm, netting consequences that can be better for everyone.

PyCon Australia is the national conference for users of the Python Programming Language. In 2015, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

July 31-August 4, Brisbane, Queensland, Australia
Captions: 
	00:00:10,190 --> 00:00:13,849
our keynote presenter today is the

00:00:12,139 --> 00:00:15,530
founder of call back women an initiative

00:00:13,849 --> 00:00:17,420
that aims to reduce the world docking

00:00:15,530 --> 00:00:19,610
an agenda disparity in speaker lineups

00:00:17,420 --> 00:00:21,980
at programming conferences she is a

00:00:19,610 --> 00:00:24,740
developer community manager advocate

00:00:21,980 --> 00:00:27,380
certified sex educator and musical

00:00:24,740 --> 00:00:29,300
gluten-free Baker she has been an

00:00:27,380 --> 00:00:31,460
organizer instructor for many tech

00:00:29,300 --> 00:00:33,380
women's organizations and is given talks

00:00:31,460 --> 00:00:36,170
at more than 20 technical conferences

00:00:33,380 --> 00:00:39,200
among the Python Ruby javascript and PHP

00:00:36,170 --> 00:00:51,800
communities please welcome Karina see

00:00:39,200 --> 00:00:54,739
Xena all right yeah all right good

00:00:51,800 --> 00:00:55,730
morning thank you so much for that you

00:00:54,739 --> 00:00:57,409
met your callback woman so I'm

00:00:55,730 --> 00:00:59,690
immediately gonna say please put in your

00:00:57,409 --> 00:01:02,809
proposals to LCA really want to see you

00:00:59,690 --> 00:01:06,110
there this is my second time at PyCon I

00:01:02,809 --> 00:01:08,200
you I was a Hobart in 2013 so it's

00:01:06,110 --> 00:01:11,360
really great to be coming back here and

00:01:08,200 --> 00:01:13,760
the talk I'm giving is something that

00:01:11,360 --> 00:01:15,560
really comes from those two places that

00:01:13,760 --> 00:01:17,479
i live in as both the developer and as a

00:01:15,560 --> 00:01:19,390
certified sex educator I see the world

00:01:17,479 --> 00:01:21,170
from a certain perspective that is

00:01:19,390 --> 00:01:23,329
relatively unique and when I say

00:01:21,170 --> 00:01:25,250
relatively I'd say you'd be surprised

00:01:23,329 --> 00:01:29,149
how many developers are also sex

00:01:25,250 --> 00:01:30,979
educators there's quite a few of us but

00:01:29,149 --> 00:01:32,270
we have a special lens on the world and

00:01:30,979 --> 00:01:33,799
that's what this talk is really going to

00:01:32,270 --> 00:01:36,049
be dealing with a little bit it's a

00:01:33,799 --> 00:01:37,340
toolkit for empathetic coding we're

00:01:36,049 --> 00:01:39,409
going to be delving into specific

00:01:37,340 --> 00:01:41,689
examples of uncritical programming and

00:01:39,409 --> 00:01:43,520
painful results from doing things in

00:01:41,689 --> 00:01:45,530
ways that were benignly intended and

00:01:43,520 --> 00:01:47,350
doing that includes stepping back a

00:01:45,530 --> 00:01:49,880
little to look at the ways that we can

00:01:47,350 --> 00:01:51,880
carelessly push buttons or fail to make

00:01:49,880 --> 00:01:54,409
it easy for people to opt out of things

00:01:51,880 --> 00:01:55,880
which is why this particular talk is

00:01:54,409 --> 00:01:58,100
going to start with a Content morning

00:01:55,880 --> 00:02:00,140
this talk addresses problems and how

00:01:58,100 --> 00:02:02,030
developers deal with sensitive topics

00:02:00,140 --> 00:02:03,859
that section is going to begin in about

00:02:02,030 --> 00:02:05,719
five or six minutes in and it's going to

00:02:03,859 --> 00:02:09,440
some particular issues including

00:02:05,719 --> 00:02:11,810
grieving PTSD depression miscarriage

00:02:09,440 --> 00:02:14,480
infertility surveillance racial

00:02:11,810 --> 00:02:17,060
profiling sexual history consent and

00:02:14,480 --> 00:02:18,770
assault so like I said there's five ten

00:02:17,060 --> 00:02:19,820
minutes until we get to that so if it's

00:02:18,770 --> 00:02:23,989
something you want to think about you've

00:02:19,820 --> 00:02:24,990
got a little bit of time algorithms are

00:02:23,989 --> 00:02:28,020
imposing hunts

00:02:24,990 --> 00:02:30,810
it's is on people all the time we're

00:02:28,020 --> 00:02:34,020
able to extract these remarkably precise

00:02:30,810 --> 00:02:36,090
insights about an individual but do we

00:02:34,020 --> 00:02:39,240
really have a right to know what they

00:02:36,090 --> 00:02:41,760
didn't consent to share with us even

00:02:39,240 --> 00:02:44,490
when they willingly consent to share

00:02:41,760 --> 00:02:46,140
something else that leads us there how

00:02:44,490 --> 00:02:50,790
are we going to mitigate against

00:02:46,140 --> 00:02:52,020
unintended consequences of this we start

00:02:50,790 --> 00:02:54,570
by looking at what is an algorithm

00:02:52,020 --> 00:02:57,060
itself we have some preconceived ideas

00:02:54,570 --> 00:03:00,180
but in its most basic form it's a

00:02:57,060 --> 00:03:03,180
step-by-step set of instructions for

00:03:00,180 --> 00:03:06,480
operations for predictably arriving at a

00:03:03,180 --> 00:03:11,550
certain outcome and predictably is key

00:03:06,480 --> 00:03:13,680
here so we think about you know

00:03:11,550 --> 00:03:15,780
algorithms usually in terms of computer

00:03:13,680 --> 00:03:17,580
science and mathematics patterns of

00:03:15,780 --> 00:03:20,340
instructions that are articulated in

00:03:17,580 --> 00:03:22,200
code or in formulas but there are other

00:03:20,340 --> 00:03:25,110
kinds of algorithms in the world all the

00:03:22,200 --> 00:03:27,000
time around us and those are just

00:03:25,110 --> 00:03:28,830
patterns of instructions articulated in

00:03:27,000 --> 00:03:34,020
all sorts of interesting ways they could

00:03:28,830 --> 00:03:36,330
be map instructions recipes crochet

00:03:34,020 --> 00:03:41,250
patterns if you think your code is

00:03:36,330 --> 00:03:44,870
obscure try reading that for a while all

00:03:41,250 --> 00:03:47,730
right so algorithms we have certain new

00:03:44,870 --> 00:03:49,290
emergent technology lately at called

00:03:47,730 --> 00:03:50,880
deep learning it's really the new Hot

00:03:49,290 --> 00:03:53,670
List hotness right now in machine

00:03:50,880 --> 00:03:54,960
learning it's technically not new at all

00:03:53,670 --> 00:03:56,910
it's actually been around for decades

00:03:54,960 --> 00:03:59,970
but it's really been locked in academia

00:03:56,910 --> 00:04:01,890
due to problems of scaling but we've

00:03:59,970 --> 00:04:04,950
really now been able to take it from

00:04:01,890 --> 00:04:07,680
theory into real production practice

00:04:04,950 --> 00:04:09,150
with serious data sets so let's talk

00:04:07,680 --> 00:04:10,710
about what that is oversimplified you

00:04:09,150 --> 00:04:14,670
can think of deep learning simply as a

00:04:10,710 --> 00:04:16,950
technology that ooh sorry learning for

00:04:14,670 --> 00:04:19,290
algorithms that are fast trainable

00:04:16,950 --> 00:04:22,380
artificial neural networks so they're

00:04:19,290 --> 00:04:26,490
using a and ends to make fast trainable

00:04:22,380 --> 00:04:28,410
algorithms it's a particular approach to

00:04:26,490 --> 00:04:30,000
building and training artificial neural

00:04:28,410 --> 00:04:33,990
networks you can think of these as

00:04:30,000 --> 00:04:37,160
decision making black boxes so you have

00:04:33,990 --> 00:04:38,990
input and assist an array of numbers

00:04:37,160 --> 00:04:40,490
ening words concepts or objects we're

00:04:38,990 --> 00:04:42,350
not talking about binary here we're

00:04:40,490 --> 00:04:45,050
talking about just assigning some sort

00:04:42,350 --> 00:04:47,480
of numerical values to give some sort of

00:04:45,050 --> 00:04:49,040
meaning and in execution you're just

00:04:47,480 --> 00:04:51,290
running a series of functions against

00:04:49,040 --> 00:04:52,880
the array and outputs our predictors of

00:04:51,290 --> 00:04:55,550
properties that will be useful for

00:04:52,880 --> 00:04:58,100
drawing future intuitions about similar

00:04:55,550 --> 00:05:00,560
data and similar is really important

00:04:58,100 --> 00:05:02,840
here those intuitions only work if the

00:05:00,560 --> 00:05:04,760
data that you train it with truly is

00:05:02,840 --> 00:05:13,400
similar enough to the drain data that

00:05:04,760 --> 00:05:16,990
you then later try to intuit from all

00:05:13,400 --> 00:05:19,430
right so deep learning relies on that

00:05:16,990 --> 00:05:21,320
automated discovery of patterns within

00:05:19,430 --> 00:05:23,390
the training data set and then it

00:05:21,320 --> 00:05:25,400
applies those discoveries to draw

00:05:23,390 --> 00:05:27,220
intuitions so let's look at a very

00:05:25,400 --> 00:05:31,040
practical example of what I mean by that

00:05:27,220 --> 00:05:33,680
Mario it's an example of an artificial

00:05:31,040 --> 00:05:36,050
neural network it and I'm just going to

00:05:33,680 --> 00:05:38,990
quote the author kicks butt at Super

00:05:36,050 --> 00:05:42,140
Mario World here's what's important

00:05:38,990 --> 00:05:45,620
about Mariah it teaches itself how to

00:05:42,140 --> 00:05:48,020
play super mario world it starts with no

00:05:45,620 --> 00:05:50,480
clue at all it's just manipulating

00:05:48,020 --> 00:05:53,030
numbers and taking note of what happens

00:05:50,480 --> 00:05:55,160
so it's learning basic patterns of

00:05:53,030 --> 00:05:57,290
movement and play via a training session

00:05:55,160 --> 00:05:59,660
in which it engages in 24 hours of

00:05:57,290 --> 00:06:01,700
experimentation iterating over and over

00:05:59,660 --> 00:06:04,250
it that leads it to identify patterns

00:06:01,700 --> 00:06:07,070
that give it insights into how to use

00:06:04,250 --> 00:06:09,890
its world deep learning depends on

00:06:07,070 --> 00:06:11,480
artificial neural nets and those in turn

00:06:09,890 --> 00:06:14,330
depend on the data that they've been

00:06:11,480 --> 00:06:16,520
trained on every flaw every assumption

00:06:14,330 --> 00:06:18,380
in the training data set is going to

00:06:16,520 --> 00:06:20,510
have unacknowledged influence on

00:06:18,380 --> 00:06:22,970
algorithms and the outcomes that they

00:06:20,510 --> 00:06:24,230
generate that part is the important

00:06:22,970 --> 00:06:26,950
thing to remember is we look at some

00:06:24,230 --> 00:06:26,950
examples up ahead

00:06:31,739 --> 00:06:39,610
okay ask a question would you like to

00:06:34,209 --> 00:06:43,389
play a game of data mining fail oh good

00:06:39,610 --> 00:06:48,279
Australians thank God it's Sunday

00:06:43,389 --> 00:06:50,139
morning you could have said no all right

00:06:48,279 --> 00:06:52,209
so we're gonna try to do this as a game

00:06:50,139 --> 00:06:55,679
of bingo I warn you right now it's the

00:06:52,209 --> 00:06:58,300
strangest game of bingo you ever played

00:06:55,679 --> 00:07:02,139
there are just too many problems to deal

00:06:58,300 --> 00:07:05,619
with it's really a beyond bingo and

00:07:02,139 --> 00:07:07,419
trying not to fill the entire board with

00:07:05,619 --> 00:07:09,309
all the possible failures you can make

00:07:07,419 --> 00:07:11,259
there's so many pitfalls but we can look

00:07:09,309 --> 00:07:13,419
at some examples and appreciate how many

00:07:11,259 --> 00:07:15,279
of the pitfalls exist and why they

00:07:13,419 --> 00:07:18,009
present new challenges for us to

00:07:15,279 --> 00:07:19,419
consider for us to deal with so here's

00:07:18,009 --> 00:07:23,199
that bingo board and let's play a couple

00:07:19,419 --> 00:07:25,659
of rounds using some case studies target

00:07:23,199 --> 00:07:27,219
target stories exists here and they're

00:07:25,659 --> 00:07:28,509
relatively similar to the ones that I'm

00:07:27,219 --> 00:07:30,039
talking about but they're actually two

00:07:28,509 --> 00:07:32,139
separate chains with different owners

00:07:30,039 --> 00:07:34,419
but in principle it's the same kind of

00:07:32,139 --> 00:07:35,860
idea of a massive chain of stores that

00:07:34,419 --> 00:07:38,559
sells a lot of different things

00:07:35,860 --> 00:07:44,979
everything from you know clothing to

00:07:38,559 --> 00:07:46,689
groceries to housewares and tools so

00:07:44,979 --> 00:07:49,179
they've got this wide range of products

00:07:46,689 --> 00:07:51,279
and the thing about a store like this is

00:07:49,179 --> 00:07:54,759
that they need you to buy all of it

00:07:51,279 --> 00:07:57,429
right so retailers are always looking

00:07:54,759 --> 00:08:00,669
for the loyal customer and target in

00:07:57,429 --> 00:08:02,559
particular was interested in a certain

00:08:00,669 --> 00:08:04,989
period because there are only a few

00:08:02,559 --> 00:08:07,539
moments in life as it turns out when

00:08:04,989 --> 00:08:10,029
people change radically their ways of

00:08:07,539 --> 00:08:12,279
buying things their habits for where

00:08:10,029 --> 00:08:14,409
they go what they buy what brands they

00:08:12,279 --> 00:08:16,779
choose and one of those is when they

00:08:14,409 --> 00:08:19,059
have a child and it's not even just I'm

00:08:16,779 --> 00:08:21,789
having a child it's at a particular time

00:08:19,059 --> 00:08:23,469
around the second trimester that people

00:08:21,789 --> 00:08:25,269
start radically changing those habits

00:08:23,469 --> 00:08:27,099
and everything that they've spent a

00:08:25,269 --> 00:08:29,349
lifetime building up a spending habits

00:08:27,099 --> 00:08:31,629
is suddenly up for grabs and so for

00:08:29,349 --> 00:08:34,269
merchandiser can get you at that moment

00:08:31,629 --> 00:08:36,579
to start buying from them and start

00:08:34,269 --> 00:08:39,240
seeing them as the source of all of your

00:08:36,579 --> 00:08:41,219
needs then they've got

00:08:39,240 --> 00:08:43,260
you locked in for life you and that

00:08:41,219 --> 00:08:47,399
child that's coming in your whole family

00:08:43,260 --> 00:08:49,980
so for target trying to identify when is

00:08:47,399 --> 00:08:52,529
someone in their second trimester was a

00:08:49,980 --> 00:08:53,700
really interesting question and they

00:08:52,529 --> 00:08:55,860
looked at their data and they actually

00:08:53,700 --> 00:08:57,930
were able to pin that down pretty darn

00:08:55,860 --> 00:09:01,860
well without everyone anyone telling

00:08:57,930 --> 00:09:04,260
them anything about you know any direct

00:09:01,860 --> 00:09:06,420
information that would lead to that just

00:09:04,260 --> 00:09:09,149
by using all the cloud of information

00:09:06,420 --> 00:09:10,920
that they had they were able to pin this

00:09:09,149 --> 00:09:12,720
down all right so they start sending out

00:09:10,920 --> 00:09:15,510
mailers they sent out mailers with

00:09:12,720 --> 00:09:17,100
coupons for various things that one

00:09:15,510 --> 00:09:18,750
would get as you are approaching your

00:09:17,100 --> 00:09:20,640
second trimester and they sent it out to

00:09:18,750 --> 00:09:22,709
the people that they had targeted very

00:09:20,640 --> 00:09:24,870
successfully and that was working great

00:09:22,709 --> 00:09:26,790
until guy came into a store and he

00:09:24,870 --> 00:09:29,040
started yelling at the store manager how

00:09:26,790 --> 00:09:31,680
dare you send these coupons to my

00:09:29,040 --> 00:09:35,029
teenage daughter are you telling her to

00:09:31,680 --> 00:09:38,190
go get pregnant what's wrong with you

00:09:35,029 --> 00:09:39,390
manager apologizes a couple of days

00:09:38,190 --> 00:09:42,360
later he feels like he should apologize

00:09:39,390 --> 00:09:45,240
again he calls up the guy guy says I'm

00:09:42,360 --> 00:09:47,130
so sorry I talked to my daughter and

00:09:45,240 --> 00:09:48,720
there have been some things going on in

00:09:47,130 --> 00:09:55,829
the house I didn't know about my

00:09:48,720 --> 00:09:58,680
daughter is pregnant okay so target took

00:09:55,829 --> 00:10:01,050
a lesson from this their lesson was

00:09:58,680 --> 00:10:03,329
don't invade privacy though their lesson

00:10:01,050 --> 00:10:05,579
wasn't let's tune the algorithm the

00:10:03,329 --> 00:10:08,160
lesson they took is let's change the

00:10:05,579 --> 00:10:10,820
coupons so it's less obvious that we're

00:10:08,160 --> 00:10:14,399
targeting you specifically for pregnancy

00:10:10,820 --> 00:10:16,890
so now you get the coupon but there's a

00:10:14,399 --> 00:10:18,720
lawnmower coupon right next to it so you

00:10:16,890 --> 00:10:20,250
won't know I mean hey everyone's

00:10:18,720 --> 00:10:25,020
probably getting coupons for you know

00:10:20,250 --> 00:10:28,079
diapers and lawn mowers that's a nearly

00:10:25,020 --> 00:10:31,320
direct quote so this is this is their

00:10:28,079 --> 00:10:33,660
very strategy right I want to call it a

00:10:31,320 --> 00:10:35,730
pitfall let's not do that let's deal

00:10:33,660 --> 00:10:38,579
with the actual source of the problem

00:10:35,730 --> 00:10:40,890
one of those problems is disclosure and

00:10:38,579 --> 00:10:44,880
consent all right let's look at another

00:10:40,890 --> 00:10:48,060
one shutterfly all sorts of fun problems

00:10:44,880 --> 00:10:50,670
in this case study so they too were

00:10:48,060 --> 00:10:52,560
interested in you know that period when

00:10:50,670 --> 00:10:56,550
people are starting a family

00:10:52,560 --> 00:10:59,220
in this case they sent out emails or

00:10:56,550 --> 00:11:01,529
coupons I forget which was saying

00:10:59,220 --> 00:11:04,320
basically congratulations on your bundle

00:11:01,529 --> 00:11:06,390
of joy I know now that you're a parent

00:11:04,320 --> 00:11:10,350
you need to send out thank you letters

00:11:06,390 --> 00:11:15,450
for the shower you had a couple of

00:11:10,350 --> 00:11:21,450
problems one how do you know I had a

00:11:15,450 --> 00:11:23,250
shower okay too thanks shutterfly for

00:11:21,450 --> 00:11:26,970
the congratulations on my new bundle of

00:11:23,250 --> 00:11:32,760
joy I'm horribly infertile but hey I'm

00:11:26,970 --> 00:11:35,279
adopting a cat so I lost a baby in

00:11:32,760 --> 00:11:39,770
November who had been due this week it

00:11:35,279 --> 00:11:39,770
was like hitting a wall over again

00:11:40,070 --> 00:11:45,029
shutterfly responded to that essentially

00:11:42,630 --> 00:11:47,400
saying oops we had some false positives

00:11:45,029 --> 00:11:51,870
our intent was merely to target people

00:11:47,400 --> 00:11:53,580
who had recently had a baby false

00:11:51,870 --> 00:11:55,920
positives when we're starting to deal

00:11:53,580 --> 00:11:58,110
with human data we have to start caring

00:11:55,920 --> 00:12:00,839
about them way more they're no longer

00:11:58,110 --> 00:12:02,870
just an edge case a false negative a

00:12:00,839 --> 00:12:06,870
false positive can be deeply meaningful

00:12:02,870 --> 00:12:09,720
for the person that we're talking to we

00:12:06,870 --> 00:12:12,870
have to be much more sure or much more

00:12:09,720 --> 00:12:14,910
careful and what we communicate Mark

00:12:12,870 --> 00:12:18,030
Zuckerberg announced this week that he's

00:12:14,910 --> 00:12:19,980
going to become a father and he wrote in

00:12:18,030 --> 00:12:22,680
his blog post about this or Facebook

00:12:19,980 --> 00:12:25,080
post that they had already had three

00:12:22,680 --> 00:12:27,360
miscarriages before this pregnancy and

00:12:25,080 --> 00:12:28,589
he described that you feel so hopeful

00:12:27,360 --> 00:12:30,390
when you learn you're going to have a

00:12:28,589 --> 00:12:32,520
child you start imagining who they'll

00:12:30,390 --> 00:12:34,710
become dreaming of hopes for their

00:12:32,520 --> 00:12:37,370
future you start making plans and then

00:12:34,710 --> 00:12:40,440
they're gone it's a lonely experience

00:12:37,370 --> 00:12:42,930
most people won't discuss miscarriages

00:12:40,440 --> 00:12:45,930
because you worry your problems will

00:12:42,930 --> 00:12:48,060
distance you or reflect upon you as if

00:12:45,930 --> 00:12:52,339
you're defective or did something to

00:12:48,060 --> 00:12:56,730
cause this so you struggle on your own

00:12:52,339 --> 00:13:00,330
this mark zuckerberg is not the one that

00:12:56,730 --> 00:13:04,530
VCS are eagerly recruiting the 110 years

00:13:00,330 --> 00:13:05,640
ago who didn't know that half of all

00:13:04,530 --> 00:13:09,240
pregnancies will

00:13:05,640 --> 00:13:12,480
miscarriage that ten to twenty-five

00:13:09,240 --> 00:13:15,240
percent of known pregnancies will result

00:13:12,480 --> 00:13:17,130
in miscarriage the narco Zuckerberg

00:13:15,240 --> 00:13:19,770
who's experienced the pain of that

00:13:17,130 --> 00:13:24,030
that's the guy who isn't interesting

00:13:19,770 --> 00:13:26,430
right we need people like this one to

00:13:24,030 --> 00:13:28,710
have the understanding of the world and

00:13:26,430 --> 00:13:31,410
some sort of compassion for how

00:13:28,710 --> 00:13:36,240
difficult life becomes a little while

00:13:31,410 --> 00:13:37,860
after you get out of college so let's

00:13:36,240 --> 00:13:39,780
deal with Facebook they did a study

00:13:37,860 --> 00:13:42,960
several years ago they called it the

00:13:39,780 --> 00:13:46,920
emotional contagion study the idea here

00:13:42,960 --> 00:13:49,260
was to see if by manipulating your

00:13:46,920 --> 00:13:51,930
timeline you could make someone a little

00:13:49,260 --> 00:13:54,060
bit happier or a little bit sadder the

00:13:51,930 --> 00:13:56,540
idea here was to select posts from your

00:13:54,060 --> 00:14:00,330
friends that had words that were

00:13:56,540 --> 00:14:02,690
positive or negative and see if we just

00:14:00,330 --> 00:14:07,500
start filling your timeline with those

00:14:02,690 --> 00:14:10,490
does your mood change what they did not

00:14:07,500 --> 00:14:12,660
do was consider the difference between

00:14:10,490 --> 00:14:15,600
manipulating someone and feeling a

00:14:12,660 --> 00:14:17,610
little bit happier and for instance

00:14:15,600 --> 00:14:20,490
manipulating a depressed person into

00:14:17,610 --> 00:14:21,960
being a little bit sadder these are the

00:14:20,490 --> 00:14:24,420
kinds of studies if they're done in

00:14:21,960 --> 00:14:27,330
academia require a considerable amount

00:14:24,420 --> 00:14:29,970
of review they require informed active

00:14:27,330 --> 00:14:32,340
consent from participants Facebook

00:14:29,970 --> 00:14:36,420
didn't do those things what they did do

00:14:32,340 --> 00:14:39,990
is use researchers from academia to give

00:14:36,420 --> 00:14:42,750
them kind of the imprimatur of ethical

00:14:39,990 --> 00:14:46,350
research and got it published in

00:14:42,750 --> 00:14:49,140
academic journal but academics looked

00:14:46,350 --> 00:14:51,540
upon this research as incredibly not

00:14:49,140 --> 00:14:53,850
okay that this isn't the way we do

00:14:51,540 --> 00:14:55,590
things this is the way corporations do

00:14:53,850 --> 00:14:58,440
things and we don't want to participate

00:14:55,590 --> 00:15:00,810
in research like this so the standards

00:14:58,440 --> 00:15:03,570
for how to conduct ethical research on

00:15:00,810 --> 00:15:06,840
people are really only being applied in

00:15:03,570 --> 00:15:09,900
this one side of the industry the side

00:15:06,840 --> 00:15:11,730
that's not professional practice we need

00:15:09,900 --> 00:15:15,960
to be taking some of their knowledge

00:15:11,730 --> 00:15:18,150
about how to oversee a study of people

00:15:15,960 --> 00:15:19,660
to make sure that we're doing that in

00:15:18,150 --> 00:15:21,880
ways that are not abuse

00:15:19,660 --> 00:15:22,900
of them in this case those people had

00:15:21,880 --> 00:15:24,640
absolutely no idea they were

00:15:22,900 --> 00:15:27,280
participating a study let alone were

00:15:24,640 --> 00:15:28,810
okay with it and the end result of this

00:15:27,280 --> 00:15:31,360
is that even though it was the largest

00:15:28,810 --> 00:15:34,110
study ever conducted like this it also

00:15:31,360 --> 00:15:37,750
had what is believed to be the tiniest

00:15:34,110 --> 00:15:40,720
detectable change ever so they did all

00:15:37,750 --> 00:15:43,120
of that and what was the result was it

00:15:40,720 --> 00:15:47,160
worth the trade-off did human knowledge

00:15:43,120 --> 00:15:47,160
advanced in some significant way

00:15:48,060 --> 00:15:54,100
facebook in Revere in review has been

00:15:50,380 --> 00:15:55,510
around for a while in previous years it

00:15:54,100 --> 00:15:58,120
was something more or less that you

00:15:55,510 --> 00:16:00,940
could curate last year they decided to

00:15:58,120 --> 00:16:02,200
get more automated about it being able

00:16:00,940 --> 00:16:04,420
to look at your timeline and

00:16:02,200 --> 00:16:06,760
algorithmically predict what posts of

00:16:04,420 --> 00:16:08,500
the past year were your most important

00:16:06,760 --> 00:16:12,580
moments in life and to be able to

00:16:08,500 --> 00:16:14,890
present you with those an advert and

00:16:12,580 --> 00:16:16,900
algorithmic cruelty is the result of

00:16:14,890 --> 00:16:19,420
code that works in the overwhelming

00:16:16,900 --> 00:16:22,240
majority of use cases but doesn't take

00:16:19,420 --> 00:16:24,670
other use cases into account that's Eric

00:16:22,240 --> 00:16:26,560
Myers definition and the reason that he

00:16:24,670 --> 00:16:31,420
gets to coin it and name it is because

00:16:26,560 --> 00:16:33,940
he's the one that had happened to his

00:16:31,420 --> 00:16:36,310
daughter died and he had to be

00:16:33,940 --> 00:16:40,870
confronted with a facebook post

00:16:36,310 --> 00:16:42,880
reminding her reminding him of her this

00:16:40,870 --> 00:16:44,410
might have been the most important

00:16:42,880 --> 00:16:45,940
moment of his year but it certainly

00:16:44,410 --> 00:16:48,070
wasn't the one that he wanted to

00:16:45,940 --> 00:16:52,000
commemorate it certainly wasn't the one

00:16:48,070 --> 00:16:55,180
who want to be confronted with all year

00:16:52,000 --> 00:16:58,500
long we have things change we have

00:16:55,180 --> 00:17:01,720
losses things that were even back then

00:16:58,500 --> 00:17:04,000
great might not be the ones anymore that

00:17:01,720 --> 00:17:06,040
we're excited about algorithmically

00:17:04,000 --> 00:17:08,740
trying to predict based on things like

00:17:06,040 --> 00:17:11,710
how many page views and how many likes

00:17:08,740 --> 00:17:15,210
are not ever going to tell us how

00:17:11,710 --> 00:17:15,210
someone feels about that today

00:17:17,290 --> 00:17:21,340
Eric's recommendation is that we need to

00:17:19,300 --> 00:17:24,070
increase awareness and consideration of

00:17:21,340 --> 00:17:26,110
the failure modes the edge cases the

00:17:24,070 --> 00:17:28,090
worst-case scenarios and so that's what

00:17:26,110 --> 00:17:30,700
I'm hoping that we can do here today and

00:17:28,090 --> 00:17:31,480
that you can carry with you and one of

00:17:30,700 --> 00:17:33,130
the things that I think is really

00:17:31,480 --> 00:17:36,310
important in this is that we have to be

00:17:33,130 --> 00:17:39,220
humble we have to acknowledge that we

00:17:36,310 --> 00:17:42,000
cannot into it in a tour interstates we

00:17:39,220 --> 00:17:45,010
cannot into it private emotions

00:17:42,000 --> 00:17:47,980
subjectivity all the stuff that's going

00:17:45,010 --> 00:17:53,290
on in heads and hearts we are not that

00:17:47,980 --> 00:17:56,290
smart not yet anyway that does bring us

00:17:53,290 --> 00:18:00,400
to issues of consent and do naam Indian

00:17:56,290 --> 00:18:04,930
autumn is a shin Fitbit you're probably

00:18:00,400 --> 00:18:07,900
familiar with it tracker mostly used to

00:18:04,930 --> 00:18:11,020
track activities walking running things

00:18:07,900 --> 00:18:14,320
like that when it was first released it

00:18:11,020 --> 00:18:16,480
had a sex tracker giving you the

00:18:14,320 --> 00:18:20,380
opportunity to keep track of yet another

00:18:16,480 --> 00:18:22,060
physical activity right this is really

00:18:20,380 --> 00:18:24,190
great this is really helpful as a sex

00:18:22,060 --> 00:18:25,950
educator I can only applaud paying close

00:18:24,190 --> 00:18:30,240
attention to a healthy happy sex life

00:18:25,950 --> 00:18:33,190
however what Fitbit did not do is

00:18:30,240 --> 00:18:36,490
consider that some kinds of data we

00:18:33,190 --> 00:18:46,540
track are more private than others which

00:18:36,490 --> 00:18:48,490
led to it being default public ooh we're

00:18:46,540 --> 00:18:50,620
missing that slide oh my gosh okay well

00:18:48,490 --> 00:18:55,270
to just tell you it was actually a

00:18:50,620 --> 00:18:59,130
control panel showing you know how many

00:18:55,270 --> 00:19:01,740
hours of sex you've had this week I

00:18:59,130 --> 00:19:03,800
realized that some people gamify

00:19:01,740 --> 00:19:06,980
everything

00:19:03,800 --> 00:19:09,320
and from fitbit's product perspective

00:19:06,980 --> 00:19:11,330
what a great way to market it right get

00:19:09,320 --> 00:19:15,950
people excited about ma you know

00:19:11,330 --> 00:19:19,130
gamifying their whole life but it was

00:19:15,950 --> 00:19:22,490
the lack of thought about the user

00:19:19,130 --> 00:19:24,770
experience of sharing everything from

00:19:22,490 --> 00:19:28,700
the user perspective that was really the

00:19:24,770 --> 00:19:31,730
problem here from a product perspective

00:19:28,700 --> 00:19:33,890
the more visible the product was the

00:19:31,730 --> 00:19:37,130
more visible that data was the better

00:19:33,890 --> 00:19:39,590
for a new product to get buzz get

00:19:37,130 --> 00:19:44,180
launched yet visible from a user

00:19:39,590 --> 00:19:45,890
perspective this was really abusive so

00:19:44,180 --> 00:19:52,180
here's how they solve this problem when

00:19:45,890 --> 00:19:56,810
it was finally exposed robots.txt done

00:19:52,180 --> 00:19:59,810
uh-huh this kills me they no longer as i

00:19:56,810 --> 00:20:01,400
understand it offer sex tracking i guess

00:19:59,810 --> 00:20:03,710
they decided that that was a little too

00:20:01,400 --> 00:20:05,000
difficult to figure out which i think is

00:20:03,710 --> 00:20:07,310
interesting because really the only

00:20:05,000 --> 00:20:09,830
identified problem here was how to

00:20:07,310 --> 00:20:11,300
handle data as a concept it was

00:20:09,830 --> 00:20:13,610
perfectly sound and in fact there are a

00:20:11,300 --> 00:20:16,270
lot of other products that have tried to

00:20:13,610 --> 00:20:20,170
fill that space as well in various ways

00:20:16,270 --> 00:20:22,730
so you know there's this there is a need

00:20:20,170 --> 00:20:24,380
but they didn't deal with it in the

00:20:22,730 --> 00:20:26,360
beginning they didn't think about it and

00:20:24,380 --> 00:20:28,070
care about it enough and they still

00:20:26,360 --> 00:20:30,710
haven't decided that they want to

00:20:28,070 --> 00:20:32,150
address it how do you tackle this so

00:20:30,710 --> 00:20:36,860
they've passed over the opportunity all

00:20:32,150 --> 00:20:38,180
together and that's sad uber alright

00:20:36,860 --> 00:20:41,890
there's so many things that you could

00:20:38,180 --> 00:20:45,830
say about uber so I only get 45 minutes

00:20:41,890 --> 00:20:49,940
I'm going to focus on God view alright

00:20:45,830 --> 00:20:51,710
so God view is a control panel you know

00:20:49,940 --> 00:20:54,950
we all have things that we need to

00:20:51,710 --> 00:20:58,700
monitor you know performance stats

00:20:54,950 --> 00:21:00,830
activity up time you know whatever it is

00:20:58,700 --> 00:21:03,920
we've all got stuff that needs to be

00:21:00,830 --> 00:21:07,250
continuously monitored God view is

00:21:03,920 --> 00:21:11,830
simply they're monitoring control panel

00:21:07,250 --> 00:21:15,370
it's high level and it tracks the cars

00:21:11,830 --> 00:21:18,700
it tracks individual

00:21:15,370 --> 00:21:20,860
verse in the cars it tracks individual

00:21:18,700 --> 00:21:22,870
customers locations when they're not in

00:21:20,860 --> 00:21:25,510
the cars in fact it was revealed last

00:21:22,870 --> 00:21:28,840
week that attract individual customers

00:21:25,510 --> 00:21:33,960
always if you have over on your phone

00:21:28,840 --> 00:21:36,130
right now odd view is aware of that so

00:21:33,960 --> 00:21:39,760
what was the problem i mean you know

00:21:36,130 --> 00:21:42,280
like that's just ordinary stuff that we

00:21:39,760 --> 00:21:44,530
do the problem is that they used it for

00:21:42,280 --> 00:21:46,450
entertainment value it wasn't being used

00:21:44,530 --> 00:21:49,780
for business purposes it was being used

00:21:46,450 --> 00:21:51,940
in whining and dying investors sorry

00:21:49,780 --> 00:21:53,620
wining and dining investors in order to

00:21:51,940 --> 00:21:57,160
impress them with how well they could

00:21:53,620 --> 00:21:59,860
stalk people got a friend who's coming

00:21:57,160 --> 00:22:05,290
someone's late to the party hey this

00:21:59,860 --> 00:22:06,550
will be fun let's see where they are and

00:22:05,290 --> 00:22:09,400
then they discovered that they can also

00:22:06,550 --> 00:22:11,350
do it really excellent things like a

00:22:09,400 --> 00:22:14,110
reporter one day was running late to a

00:22:11,350 --> 00:22:18,070
meeting so an executive use god view to

00:22:14,110 --> 00:22:19,750
track her down for some reason knowing

00:22:18,070 --> 00:22:22,780
that she had a phone that he was

00:22:19,750 --> 00:22:27,820
tracking her on he did not call or text

00:22:22,780 --> 00:22:30,190
her he used this great God power instead

00:22:27,820 --> 00:22:33,460
these are the kinds of abuses of the

00:22:30,190 --> 00:22:36,309
extent of our vast access to data that

00:22:33,460 --> 00:22:38,860
are our problem it's not the algorithm

00:22:36,309 --> 00:22:41,140
it's the assumptions of the users of the

00:22:38,860 --> 00:22:47,380
algorithm we need to be really careful

00:22:41,140 --> 00:22:50,980
in how we use these precious data they

00:22:47,380 --> 00:22:53,140
also used it they had to release some

00:22:50,980 --> 00:22:54,640
data publicly there's Freedom of

00:22:53,140 --> 00:22:57,429
Information Act which I understand you

00:22:54,640 --> 00:23:01,660
have similar things here in the US it is

00:22:57,429 --> 00:23:04,570
simply a law that allows for government

00:23:01,660 --> 00:23:07,090
generated data to be accessible to the

00:23:04,570 --> 00:23:09,610
public by request so in this case ubers

00:23:07,090 --> 00:23:12,070
data was released they had supposedly

00:23:09,610 --> 00:23:14,890
anonymized it the anonymous ation was

00:23:12,070 --> 00:23:17,020
insufficient resulting in the ability to

00:23:14,890 --> 00:23:19,090
actually trace back those anonymize

00:23:17,020 --> 00:23:20,710
records to real people in this case

00:23:19,090 --> 00:23:23,400
including celebrities and the ability to

00:23:20,710 --> 00:23:26,410
find out exactly where they were staying

00:23:23,400 --> 00:23:27,640
this two presents a little bit of a

00:23:26,410 --> 00:23:29,080
problem especially when you start

00:23:27,640 --> 00:23:30,970
thinking about other people

00:23:29,080 --> 00:23:33,400
and other reasons why someone might be

00:23:30,970 --> 00:23:37,270
stopped other reasons why someone who

00:23:33,400 --> 00:23:39,910
has need to use a private vehicle to get

00:23:37,270 --> 00:23:42,360
around might not want the ability to

00:23:39,910 --> 00:23:45,250
have in real time they're supposedly

00:23:42,360 --> 00:23:47,140
identification number 12 77 be

00:23:45,250 --> 00:23:55,270
identified directly back to them by

00:23:47,140 --> 00:23:59,170
anyone and then there was this one so

00:23:55,270 --> 00:24:01,510
the walk of shame is the idea that you

00:23:59,170 --> 00:24:03,640
have casual sex and come home late at

00:24:01,510 --> 00:24:06,220
night in the same clothes you had on

00:24:03,640 --> 00:24:08,410
yesterday uber decided that they can

00:24:06,220 --> 00:24:12,700
algorithmically determine which of their

00:24:08,410 --> 00:24:14,620
riders is doing the walk of shame okay

00:24:12,700 --> 00:24:16,570
so you know that sounds kind of funny

00:24:14,620 --> 00:24:17,980
right like we can really learn

00:24:16,570 --> 00:24:20,620
interesting you know we can mine our

00:24:17,980 --> 00:24:23,830
data and find interesting fun things to

00:24:20,620 --> 00:24:26,020
learn about our our customers difficulty

00:24:23,830 --> 00:24:28,810
here is that this is incredibly

00:24:26,020 --> 00:24:31,030
unrelated to the primary mission of the

00:24:28,810 --> 00:24:33,220
company when okcupid does a blog post

00:24:31,030 --> 00:24:36,270
about funny things they learn about

00:24:33,220 --> 00:24:39,400
dating from their data that's one thing

00:24:36,270 --> 00:24:42,460
no one expects uber to be using their

00:24:39,400 --> 00:24:44,320
data to judge their personal life it's a

00:24:42,460 --> 00:24:47,700
really different scenario and they treat

00:24:44,320 --> 00:24:50,650
it as if they're kind of interchangeable

00:24:47,700 --> 00:24:52,540
this is a problem of a lot of the

00:24:50,650 --> 00:24:54,670
companies that are doing this is they

00:24:52,540 --> 00:24:57,310
see consent as something that's simply

00:24:54,670 --> 00:24:58,660
permission granted and they treat it as

00:24:57,310 --> 00:25:00,310
if there's a whole bunch of fine print

00:24:58,660 --> 00:25:03,010
that really can just be tossed into

00:25:00,310 --> 00:25:06,120
Terms of Service and that you know all

00:25:03,010 --> 00:25:09,280
done with it the reality of consent is

00:25:06,120 --> 00:25:11,200
used by professional researchers and sex

00:25:09,280 --> 00:25:13,720
educators a different standard all

00:25:11,200 --> 00:25:16,320
together called informed consent and in

00:25:13,720 --> 00:25:20,050
this informed consent is something

00:25:16,320 --> 00:25:23,590
freely granted where know is consequence

00:25:20,050 --> 00:25:26,920
free alternative and no is the default

00:25:23,590 --> 00:25:29,500
value and it's given with informed

00:25:26,920 --> 00:25:33,430
appreciation and understanding ahead of

00:25:29,500 --> 00:25:35,170
time of the facts involved and when you

00:25:33,430 --> 00:25:39,040
look at that definition not just

00:25:35,170 --> 00:25:41,040
permission granted the apps that we make

00:25:39,040 --> 00:25:44,490
the apps that we use

00:25:41,040 --> 00:25:48,120
have we ever gotten or given informed

00:25:44,490 --> 00:25:51,440
consent to do the kinds of things that

00:25:48,120 --> 00:25:53,370
we're looking at here we're really not

00:25:51,440 --> 00:25:55,980
informed consent doesn't have to

00:25:53,370 --> 00:25:57,750
necessarily be hard it can be something

00:25:55,980 --> 00:26:00,000
simply when asking a question of saying

00:25:57,750 --> 00:26:01,290
in Facebook's case would you be

00:26:00,000 --> 00:26:06,930
interested in participating in a

00:26:01,290 --> 00:26:09,980
research study check you know we would

00:26:06,930 --> 00:26:14,250
like to learn more about depression and

00:26:09,980 --> 00:26:16,080
and Facebook posts check you know these

00:26:14,250 --> 00:26:18,270
don't necessarily have to be so deeply

00:26:16,080 --> 00:26:19,980
involved that people stop reading and as

00:26:18,270 --> 00:26:21,930
soon as we create it in such a way that

00:26:19,980 --> 00:26:27,300
people are going to stop reading we're

00:26:21,930 --> 00:26:31,280
really evading the whole point Google

00:26:27,300 --> 00:26:35,190
AdWords Harvard research study looked at

00:26:31,280 --> 00:26:37,290
names that are highly correlated with

00:26:35,190 --> 00:26:39,900
black people and highly correlated with

00:26:37,290 --> 00:26:42,030
white people and did some google adword

00:26:39,900 --> 00:26:44,280
searches with them just looking for the

00:26:42,030 --> 00:26:46,950
person's name and what they did was they

00:26:44,280 --> 00:26:49,530
they determine what names fit these

00:26:46,950 --> 00:26:51,870
categories and then match them with real

00:26:49,530 --> 00:26:54,000
people who are academics so people who

00:26:51,870 --> 00:26:57,240
had real professional qualifications

00:26:54,000 --> 00:26:58,620
working in their field and by searching

00:26:57,240 --> 00:27:04,610
on their names they found something

00:26:58,620 --> 00:27:07,980
really interesting women who were black

00:27:04,610 --> 00:27:09,990
were significantly more likely to return

00:27:07,980 --> 00:27:17,040
and add suggesting that they had an

00:27:09,990 --> 00:27:18,660
arrest record why were the advertisers

00:27:17,040 --> 00:27:23,430
doing this were there records that were

00:27:18,660 --> 00:27:28,440
different no the problem is the whole

00:27:23,430 --> 00:27:31,170
notion of how Google calculates PageRank

00:27:28,440 --> 00:27:33,200
think about what it is it considers

00:27:31,170 --> 00:27:38,040
relevance on the basis of things like

00:27:33,200 --> 00:27:42,420
clicks how often people click here the

00:27:38,040 --> 00:27:45,570
racial biases of the vast community of

00:27:42,420 --> 00:27:48,630
users of Google subtly biases this

00:27:45,570 --> 00:27:50,850
algorithm so the fact that people are

00:27:48,630 --> 00:27:53,160
more likely to click when they see a

00:27:50,850 --> 00:27:53,880
black person's name associated with the

00:27:53,160 --> 00:27:56,100
rest

00:27:53,880 --> 00:27:57,990
means that over time that ad starts to

00:27:56,100 --> 00:28:00,510
be more favorably chosen among all the

00:27:57,990 --> 00:28:03,150
possibilities it had a bunch of

00:28:00,510 --> 00:28:05,820
possibilities all the same as confirmed

00:28:03,150 --> 00:28:08,490
by the advertisers themselves identical

00:28:05,820 --> 00:28:09,990
ads we Google slowly learning that what

00:28:08,490 --> 00:28:13,080
you really should do is put at the top

00:28:09,990 --> 00:28:17,040
for black women the ones that suggest

00:28:13,080 --> 00:28:19,260
that they have been arrested black

00:28:17,040 --> 00:28:23,610
identifying name was twenty percent more

00:28:19,260 --> 00:28:30,120
likely to result in that arrest record

00:28:23,610 --> 00:28:32,690
add image analysis is another really

00:28:30,120 --> 00:28:35,730
popular application for deep learning

00:28:32,690 --> 00:28:38,610
there's various things you can do you

00:28:35,730 --> 00:28:40,620
can do image confirmation essentially

00:28:38,610 --> 00:28:43,170
confirming that this image is that image

00:28:40,620 --> 00:28:44,460
or that the subject of this image is the

00:28:43,170 --> 00:28:45,920
subject of that image even if the

00:28:44,460 --> 00:28:49,740
pictures themselves are not identical

00:28:45,920 --> 00:28:51,990
you can do image analysis you can do

00:28:49,740 --> 00:28:53,540
image recognition for instance looking

00:28:51,990 --> 00:28:56,190
at a picture and saying oh that's a cat

00:28:53,540 --> 00:28:59,700
all these things are ways that machine

00:28:56,190 --> 00:29:02,550
machine learning can be applied one

00:28:59,700 --> 00:29:07,670
example for instance is iPhoto face

00:29:02,550 --> 00:29:07,670
detection it's really good

00:29:08,450 --> 00:29:11,910
this is a couple of years ago so cut

00:29:10,680 --> 00:29:16,530
them some slack they've gotten better

00:29:11,910 --> 00:29:18,300
but we have been seeing facial

00:29:16,530 --> 00:29:19,950
recognition technology cropping up in

00:29:18,300 --> 00:29:21,420
consumer technology for a few years now

00:29:19,950 --> 00:29:22,860
and this is you know just one of those

00:29:21,420 --> 00:29:25,920
examples you probably have it in your

00:29:22,860 --> 00:29:29,850
camera as well and of course we do see

00:29:25,920 --> 00:29:32,940
the funny mistakes you know detecting a

00:29:29,850 --> 00:29:35,460
face in your cookie and that's a pretty

00:29:32,940 --> 00:29:36,960
harmless mistake and so it's easy to

00:29:35,460 --> 00:29:39,990
chuckle at because it's so harmless

00:29:36,960 --> 00:29:42,090
facial recognition technology has been

00:29:39,990 --> 00:29:44,520
merging though with pattern matching and

00:29:42,090 --> 00:29:48,630
that's where we're seeing things that

00:29:44,520 --> 00:29:50,880
matter much more this is flicker they

00:29:48,630 --> 00:29:55,700
introduce auto-tagging recently and this

00:29:50,880 --> 00:29:58,080
is just two months ago this is ash wits

00:29:55,700 --> 00:30:10,590
concentration camp that it automatically

00:29:58,080 --> 00:30:13,440
tagged as sports and jungle gym in some

00:30:10,590 --> 00:30:16,800
sort of way it got pattern matching

00:30:13,440 --> 00:30:18,390
right I mean there's something here you

00:30:16,800 --> 00:30:21,080
can see where it might have reached

00:30:18,390 --> 00:30:23,910
those conclusions the problem is that

00:30:21,080 --> 00:30:26,580
pattern matching and image recognition

00:30:23,910 --> 00:30:29,490
are not the same thing as comprehension

00:30:26,580 --> 00:30:37,620
and in this case comprehension matters

00:30:29,490 --> 00:30:39,210
far more also auto tagged the one that's

00:30:37,620 --> 00:30:46,290
missing is where he was also auto tagged

00:30:39,210 --> 00:30:49,050
as ape why would this happen this was

00:30:46,290 --> 00:30:51,330
two months ago here's one month ago

00:30:49,050 --> 00:30:55,970
google photos doing almost the same

00:30:51,330 --> 00:31:01,710
thing why are black people being Auto

00:30:55,970 --> 00:31:04,040
tagged as animals for this you actually

00:31:01,710 --> 00:31:08,520
have to go back in history quite a while

00:31:04,040 --> 00:31:11,790
in the 1950s when film stocks were

00:31:08,520 --> 00:31:14,730
really being refined this was George

00:31:11,790 --> 00:31:16,500
Eastman United States 1950s a time when

00:31:14,730 --> 00:31:19,230
the US was still incredibly racially

00:31:16,500 --> 00:31:20,700
segregated and black people were their

00:31:19,230 --> 00:31:23,910
own economical market of

00:31:20,700 --> 00:31:25,920
particular importance and so film stocks

00:31:23,910 --> 00:31:30,210
were optimizing for getting maximum

00:31:25,920 --> 00:31:35,630
details out of white values at a fair

00:31:30,210 --> 00:31:39,090
skin out of white cloth all of this so

00:31:35,630 --> 00:31:42,030
historically we have a huge corpus of

00:31:39,090 --> 00:31:44,700
images of black people with very low

00:31:42,030 --> 00:31:46,830
detail and so even though these

00:31:44,700 --> 00:31:50,430
algorithms are being trained by vast

00:31:46,830 --> 00:31:54,480
data sets that are considered very high

00:31:50,430 --> 00:31:57,030
quality the vast data set is always

00:31:54,480 --> 00:31:59,520
biased in terms of quality of pictures

00:31:57,030 --> 00:32:01,980
of white people versus black people the

00:31:59,520 --> 00:32:04,980
amount of useful data that can be drawn

00:32:01,980 --> 00:32:07,230
from those images is very different and

00:32:04,980 --> 00:32:10,890
the ability to make mistakes will thus

00:32:07,230 --> 00:32:14,130
always be very different this isn't easy

00:32:10,890 --> 00:32:16,440
to change digital has started to shift

00:32:14,130 --> 00:32:18,210
in the video realm they start to make

00:32:16,440 --> 00:32:21,260
new advancements that make it much more

00:32:18,210 --> 00:32:25,200
possible to capture a full range of

00:32:21,260 --> 00:32:27,330
detail across the spectrum still images

00:32:25,200 --> 00:32:29,880
even in digital today are really not

00:32:27,330 --> 00:32:31,470
showing that same kind of innovation so

00:32:29,880 --> 00:32:33,000
these problems are reproduced over and

00:32:31,470 --> 00:32:35,310
over again and as we have more more

00:32:33,000 --> 00:32:37,560
mobile phones think about how poor your

00:32:35,310 --> 00:32:41,460
mobile phones image sensor is compared

00:32:37,560 --> 00:32:42,330
to fill these problems will continue so

00:32:41,460 --> 00:32:45,890
we're going to have to ask ourselves

00:32:42,330 --> 00:32:50,580
questions about what to do about it and

00:32:45,890 --> 00:32:55,200
finally here is a firm it is a credit

00:32:50,580 --> 00:32:59,400
lending service the idea here is to

00:32:55,200 --> 00:33:03,120
evaluate creditworthiness the way they

00:32:59,400 --> 00:33:06,090
do this is with machine learning and

00:33:03,120 --> 00:33:08,790
their innovation is to make it super

00:33:06,090 --> 00:33:11,790
duper easy so when you apply you only

00:33:08,790 --> 00:33:14,580
have to give I think four things mobile

00:33:11,790 --> 00:33:16,890
phone number name let's see we get the

00:33:14,580 --> 00:33:20,190
birthday and last four digits of your

00:33:16,890 --> 00:33:21,900
national ID number okay and based on

00:33:20,190 --> 00:33:23,490
that they go and they come the web and

00:33:21,900 --> 00:33:25,680
they find all sorts of things they say

00:33:23,490 --> 00:33:30,210
that they have 70,000 attributes that

00:33:25,680 --> 00:33:31,860
the algorithm looks at and that's that

00:33:30,210 --> 00:33:33,840
sounds like a really complete way to

00:33:31,860 --> 00:33:34,530
look at people right not just a few

00:33:33,840 --> 00:33:36,270
simple thing

00:33:34,530 --> 00:33:38,460
about their spending patterns but really

00:33:36,270 --> 00:33:40,500
looking at them as a whole person the

00:33:38,460 --> 00:33:44,790
problem is that the things that you

00:33:40,500 --> 00:33:46,680
choose come with biases 2% of open

00:33:44,790 --> 00:33:49,650
source contributors are women so when

00:33:46,680 --> 00:33:51,570
you choose to use github profile as a

00:33:49,650 --> 00:33:54,990
way of evaluating credit worthiness you

00:33:51,570 --> 00:33:58,380
immediately are changing who is able to

00:33:54,990 --> 00:34:00,660
get evaluated by this and how fair that

00:33:58,380 --> 00:34:04,830
evaluation will be look at some of the

00:34:00,660 --> 00:34:07,800
other ways that they're evaluating which

00:34:04,830 --> 00:34:11,159
of your social networks is useful

00:34:07,800 --> 00:34:13,500
information i'm told that facebook and

00:34:11,159 --> 00:34:15,900
twitter do not indicate good credit

00:34:13,500 --> 00:34:20,150
worthiness but LinkedIn does okay why

00:34:15,900 --> 00:34:22,500
would that be why if you think about it

00:34:20,150 --> 00:34:25,169
white-collar professionals are much more

00:34:22,500 --> 00:34:27,900
likely to use LinkedIn than blue collar

00:34:25,169 --> 00:34:30,270
workers right it's it's built for that

00:34:27,900 --> 00:34:31,950
so people who are much more likely to

00:34:30,270 --> 00:34:34,409
have a stable income in the first place

00:34:31,950 --> 00:34:36,390
are you evaluating how good their

00:34:34,409 --> 00:34:38,700
character is at paying debt or are you

00:34:36,390 --> 00:34:41,190
simply evaluating that they are middle

00:34:38,700 --> 00:34:44,190
class or more what are you actually

00:34:41,190 --> 00:34:46,470
getting okay so you're getting a lot of

00:34:44,190 --> 00:34:48,480
false negatives people who are rejected

00:34:46,470 --> 00:34:50,490
for reasons that have nothing to do with

00:34:48,480 --> 00:34:54,679
their ability to pay and have everything

00:34:50,490 --> 00:34:54,679
to do with replicating privileged

00:34:55,340 --> 00:35:01,590
replication remember that's an algorithm

00:34:58,580 --> 00:35:04,190
alright so the founders idea here is

00:35:01,590 --> 00:35:06,690
that this is great because we do have

00:35:04,190 --> 00:35:08,460
70,000 qualities I mean sure what Archie

00:35:06,690 --> 00:35:10,700
why you bias but who's gonna know who's

00:35:08,460 --> 00:35:14,220
gonna care and good thing here is that

00:35:10,700 --> 00:35:15,810
humans aren't biasing it woo we sure

00:35:14,220 --> 00:35:17,820
wouldn't want any humans messing with it

00:35:15,810 --> 00:35:19,950
this is a crazy idea data is not

00:35:17,820 --> 00:35:22,770
objective it's never objective it always

00:35:19,950 --> 00:35:25,320
has bias inherent at minimum with how it

00:35:22,770 --> 00:35:27,870
was collected and interpreted how many

00:35:25,320 --> 00:35:30,240
of those 70,000 factors are accidentally

00:35:27,870 --> 00:35:31,830
leading to discriminatory outcomes how

00:35:30,240 --> 00:35:34,140
would any of us know how would the

00:35:31,830 --> 00:35:36,030
person affected possibly know and what

00:35:34,140 --> 00:35:40,380
recourse would they possibly have even

00:35:36,030 --> 00:35:42,090
if they could pin that down rationales

00:35:40,380 --> 00:35:45,840
for the algorithm can only be seen from

00:35:42,090 --> 00:35:48,000
inside of a black box so I took a

00:35:45,840 --> 00:35:53,040
picture for you of the inside of a

00:35:48,000 --> 00:35:55,140
really really black box think about

00:35:53,040 --> 00:35:57,990
lending decisions inside of a black box

00:35:55,140 --> 00:36:00,840
is it's not disrupting the lending

00:35:57,990 --> 00:36:03,150
industry it's disrupting the regulation

00:36:00,840 --> 00:36:04,920
of the lending industry and this worries

00:36:03,150 --> 00:36:06,390
me because right now US government

00:36:04,920 --> 00:36:07,770
regulators are in fact looking

00:36:06,390 --> 00:36:09,300
specifically at a firm and other

00:36:07,770 --> 00:36:11,580
companies that are using these models

00:36:09,300 --> 00:36:14,220
and they're excited they want to know

00:36:11,580 --> 00:36:16,080
how they can use this to other large

00:36:14,220 --> 00:36:19,200
financial institutions are watching

00:36:16,080 --> 00:36:21,210
closely they're excited the thing here

00:36:19,200 --> 00:36:23,730
is that we're not innovating on

00:36:21,210 --> 00:36:25,770
technology we're innovating on getting

00:36:23,730 --> 00:36:27,810
rid of regulation that protects people

00:36:25,770 --> 00:36:29,970
from predatory lending we need to be

00:36:27,810 --> 00:36:32,280
very cautious in what we do with this is

00:36:29,970 --> 00:36:35,940
black box really how we should be

00:36:32,280 --> 00:36:38,190
shunting off those decisions so we can

00:36:35,940 --> 00:36:40,020
do some things about this by taking

00:36:38,190 --> 00:36:42,390
lessons from the professional ethicists

00:36:40,020 --> 00:36:44,580
this is a specialized field in itself

00:36:42,390 --> 00:36:46,520
I've adapted a few ideas from the

00:36:44,580 --> 00:36:49,590
Association for Computing Machinery ins

00:36:46,520 --> 00:36:53,360
machineries code of ethics as well a few

00:36:49,590 --> 00:36:56,640
other sources first avoid harm to others

00:36:53,360 --> 00:36:59,100
second consider the decisions impacts

00:36:56,640 --> 00:37:01,170
and potential impact on others I say

00:36:59,100 --> 00:37:03,630
others specifically because this isn't

00:37:01,170 --> 00:37:06,390
about us we're always going to make you

00:37:03,630 --> 00:37:07,650
know evaluating the company part of the

00:37:06,390 --> 00:37:09,540
decision making process we need to be

00:37:07,650 --> 00:37:11,630
conscious about making sure others are

00:37:09,540 --> 00:37:14,280
also in the decision-making process

00:37:11,630 --> 00:37:17,190
project the likelihood of consequences

00:37:14,280 --> 00:37:21,630
to others we need to be thinking about

00:37:17,190 --> 00:37:23,520
how to contribute to human well-being by

00:37:21,630 --> 00:37:25,460
minimizing negative consequences to

00:37:23,520 --> 00:37:27,390
others all right we've anticipated

00:37:25,460 --> 00:37:30,810
possibilities now what can we do to

00:37:27,390 --> 00:37:33,030
minimize we need to be incredibly honest

00:37:30,810 --> 00:37:34,590
and trustworthy because guaranteed we

00:37:33,030 --> 00:37:36,480
will continue to make mistakes like

00:37:34,590 --> 00:37:38,370
these and others we have not anticipated

00:37:36,480 --> 00:37:41,700
at all none of these were things that

00:37:38,370 --> 00:37:43,050
were easily predictable so count on the

00:37:41,700 --> 00:37:45,060
fact that we're going to screw up and

00:37:43,050 --> 00:37:47,040
the important thing is we need to know

00:37:45,060 --> 00:37:50,250
that we've built enough trust to survive

00:37:47,040 --> 00:37:52,500
it we need to provide others with full

00:37:50,250 --> 00:37:54,960
disclosure of limitations we need to

00:37:52,500 --> 00:37:57,360
call attention to signs of risk of harm

00:37:54,960 --> 00:38:00,690
to others we need to actively counter

00:37:57,360 --> 00:38:02,010
bias and inequality doing that levels of

00:38:00,690 --> 00:38:04,560
equality tall

00:38:02,010 --> 00:38:07,320
respect justice anti-discrimination

00:38:04,560 --> 00:38:09,510
these are things that are systemic their

00:38:07,320 --> 00:38:12,120
cultural there in our assumptions are

00:38:09,510 --> 00:38:16,020
ignorance and sometimes even in malice

00:38:12,120 --> 00:38:18,390
but they're all over there's unequal

00:38:16,020 --> 00:38:20,280
access to resources and power unfair

00:38:18,390 --> 00:38:23,760
outcomes our algorithms are

00:38:20,280 --> 00:38:26,190
participating in that so what can we do

00:38:23,760 --> 00:38:27,810
we can audit outcomes even though we

00:38:26,190 --> 00:38:29,730
can't see what's going on inside of the

00:38:27,810 --> 00:38:32,580
black box what we can do is look in from

00:38:29,730 --> 00:38:34,610
the reverse these are essentially

00:38:32,580 --> 00:38:37,080
designed patterns that are well used in

00:38:34,610 --> 00:38:39,780
auditing for things like discrimination

00:38:37,080 --> 00:38:41,670
and housing and in employment so for

00:38:39,780 --> 00:38:44,070
instance you would have two people who

00:38:41,670 --> 00:38:46,050
should be exactly equally qualified put

00:38:44,070 --> 00:38:50,040
in an identical resumes simply with

00:38:46,050 --> 00:38:51,240
different names for instance the black

00:38:50,040 --> 00:38:53,850
sounding name and the white sounding

00:38:51,240 --> 00:38:56,160
name and see if they do get the same

00:38:53,850 --> 00:38:58,770
result if the algorithm treats them as

00:38:56,160 --> 00:39:01,170
identical great then we can say on that

00:38:58,770 --> 00:39:03,390
particular metric the system does seem

00:39:01,170 --> 00:39:05,250
to be working correctly if we do those

00:39:03,390 --> 00:39:08,070
audits and we find that some of those

00:39:05,250 --> 00:39:10,080
are being treated unequally that's where

00:39:08,070 --> 00:39:12,060
things get harder because then what do

00:39:10,080 --> 00:39:16,050
you do about it how do you tune for that

00:39:12,060 --> 00:39:18,510
that's a whole problem in itself right

00:39:16,050 --> 00:39:21,720
now we're in a race we're in an arms

00:39:18,510 --> 00:39:23,730
race in the past year there have been a

00:39:21,720 --> 00:39:26,100
tremendous number of acquisitions of

00:39:23,730 --> 00:39:29,340
deep learning companies by Apple

00:39:26,100 --> 00:39:32,310
Facebook Google Microsoft they're all

00:39:29,340 --> 00:39:34,140
jumping into this right now and so now

00:39:32,310 --> 00:39:36,420
is the time when we need to be actively

00:39:34,140 --> 00:39:39,780
thinking about what is this future going

00:39:36,420 --> 00:39:42,300
to be we're writing this code how are we

00:39:39,780 --> 00:39:43,830
writing it for human users and how are

00:39:42,300 --> 00:39:47,700
we writing it for the consequences that

00:39:43,830 --> 00:39:50,160
are built in for them for the moment the

00:39:47,700 --> 00:39:52,560
quality of these things does vary but

00:39:50,160 --> 00:39:55,140
the point is that their training their

00:39:52,560 --> 00:39:57,990
learning their refining things will move

00:39:55,140 --> 00:40:00,060
very quickly and that's a dilemma that

00:39:57,990 --> 00:40:02,190
we have to take seriously as developers

00:40:00,060 --> 00:40:05,250
our algorithms are suddenly growing

00:40:02,190 --> 00:40:07,920
massively more intuitive that's exciting

00:40:05,250 --> 00:40:09,870
but it does have a lot of potential for

00:40:07,920 --> 00:40:11,850
problems and that's why we have to

00:40:09,870 --> 00:40:13,110
identify potential harms we have to

00:40:11,850 --> 00:40:15,310
insist on getting the stuff right

00:40:13,110 --> 00:40:18,760
because we want to be empathetic

00:40:15,310 --> 00:40:21,910
we see the potential but often what we

00:40:18,760 --> 00:40:23,350
see is too abstract to guide us in doing

00:40:21,910 --> 00:40:26,350
something about it we need to be better

00:40:23,350 --> 00:40:28,060
at identifying problematic scenarios and

00:40:26,350 --> 00:40:32,500
for that we have to be able to

00:40:28,060 --> 00:40:34,600
anticipate diverse ways to screw up we

00:40:32,500 --> 00:40:38,320
cannot do that without highly diverse

00:40:34,600 --> 00:40:41,170
teams and I mean the real kind genuinely

00:40:38,320 --> 00:40:43,570
deeply diverse in as many facets as

00:40:41,170 --> 00:40:46,000
possible whenever the team's charged

00:40:43,570 --> 00:40:48,550
with defining data collection data use

00:40:46,000 --> 00:40:50,290
analysis are less diverse than the user

00:40:48,550 --> 00:40:54,520
base itself we're going to keep on

00:40:50,290 --> 00:40:56,710
failing think about that team at Flickr

00:40:54,520 --> 00:40:59,980
and at Google photos how many black

00:40:56,710 --> 00:41:01,600
engineers that they have those problems

00:40:59,980 --> 00:41:03,910
are incredibly well known to black

00:41:01,600 --> 00:41:07,390
people they're used to their photos

00:41:03,910 --> 00:41:08,860
always sucking okay this is something

00:41:07,390 --> 00:41:11,110
that a diverse team would have been more

00:41:08,860 --> 00:41:13,210
resilient on we have to have

00:41:11,110 --> 00:41:16,510
decision-making authority in the hands

00:41:13,210 --> 00:41:19,870
of genuinely diverse theme teams they

00:41:16,510 --> 00:41:22,210
have to be diverse in kind form and

00:41:19,870 --> 00:41:28,360
character that is the definition of

00:41:22,210 --> 00:41:31,210
diversity diversity is not tokenism it's

00:41:28,360 --> 00:41:34,180
not culture fit the whole point of

00:41:31,210 --> 00:41:39,970
culture fit is too devoid disruption of

00:41:34,180 --> 00:41:42,250
groupthink yoona dimensional variety is

00:41:39,970 --> 00:41:44,080
not diversity either it's superficial

00:41:42,250 --> 00:41:46,510
the point is we need to have people of

00:41:44,080 --> 00:41:48,760
very different experiences really

00:41:46,510 --> 00:41:51,250
different perspectives people who have

00:41:48,760 --> 00:41:54,880
completely different ideas about how the

00:41:51,250 --> 00:41:57,130
world works this is diversity it's

00:41:54,880 --> 00:41:59,680
wildly buried on as many dimensions as

00:41:57,130 --> 00:42:02,140
possible differing origins differing

00:41:59,680 --> 00:42:04,540
ages differing assumptions different

00:42:02,140 --> 00:42:09,220
experiences differing ideas on how to

00:42:04,540 --> 00:42:11,650
solve the same basic problem we need to

00:42:09,220 --> 00:42:14,320
cultivate informed consent and by that I

00:42:11,650 --> 00:42:17,920
mean enthusiastically informed consent

00:42:14,320 --> 00:42:21,610
not just saying a fine whatever I agree

00:42:17,920 --> 00:42:24,190
but saying yeah I want that feature give

00:42:21,610 --> 00:42:27,610
me that where do I sign up for that that

00:42:24,190 --> 00:42:28,170
is informed excited consent that's the

00:42:27,610 --> 00:42:29,940
kind of kin

00:42:28,170 --> 00:42:32,160
that we should be cultivating excited

00:42:29,940 --> 00:42:34,410
about we have to be doing at auditing

00:42:32,160 --> 00:42:36,869
constantly there will always be things

00:42:34,410 --> 00:42:39,390
that we can't see on our own we need to

00:42:36,869 --> 00:42:41,490
recognize that the bias is inherent it's

00:42:39,390 --> 00:42:43,440
always inheriting the bias conditions

00:42:41,490 --> 00:42:45,059
that are arose from and so we're gonna

00:42:43,440 --> 00:42:48,210
have to be visionary about countering

00:42:45,059 --> 00:42:52,440
bias in the data and the analyses in the

00:42:48,210 --> 00:42:55,380
impact and finally we have to aim mining

00:42:52,440 --> 00:42:58,049
tools themselves at public benefit

00:42:55,380 --> 00:43:00,960
consequences all these things we have

00:42:58,049 --> 00:43:02,880
our aims right now so much at the

00:43:00,960 --> 00:43:05,339
private sector what are the

00:43:02,880 --> 00:43:08,280
possibilities we take that much data and

00:43:05,339 --> 00:43:13,140
be able to ask questions about say

00:43:08,280 --> 00:43:14,910
public health and finally we have to

00:43:13,140 --> 00:43:17,720
commit to transparency that means that

00:43:14,910 --> 00:43:19,650
both data transparency and algorithmic

00:43:17,720 --> 00:43:22,770
transparency you need to be part of that

00:43:19,650 --> 00:43:25,410
conversation and that is truly the hard

00:43:22,770 --> 00:43:27,420
one right that's the one where companies

00:43:25,410 --> 00:43:29,940
are always pulling back and saying you

00:43:27,420 --> 00:43:32,010
know the data is reprise Terry the

00:43:29,940 --> 00:43:33,930
algorithm that's the core IP what are

00:43:32,010 --> 00:43:36,030
you talking about we can't possibly make

00:43:33,930 --> 00:43:39,030
that public you know they consider these

00:43:36,030 --> 00:43:41,309
things trade secrets and all I could say

00:43:39,030 --> 00:43:43,740
to that is I remember time not very long

00:43:41,309 --> 00:43:46,440
ago at all when we had to fight for open

00:43:43,740 --> 00:43:48,329
source in our tool bag that they made a

00:43:46,440 --> 00:43:49,890
lot of those same arguments if we can't

00:43:48,329 --> 00:43:54,030
this will ruin the company you don't

00:43:49,890 --> 00:43:56,880
understand we pushed back and we were

00:43:54,030 --> 00:43:59,069
right to push back and we have those

00:43:56,880 --> 00:44:01,970
tools available to dusted for us a day

00:43:59,069 --> 00:44:05,250
and our profession is better off for it

00:44:01,970 --> 00:44:07,740
we are the professionals we know that

00:44:05,250 --> 00:44:10,710
transparency is crucial for drawing

00:44:07,740 --> 00:44:12,180
insights that are real so we need to be

00:44:10,710 --> 00:44:14,460
the ones to argue for increasing

00:44:12,180 --> 00:44:16,170
transparency because it's for the sake

00:44:14,460 --> 00:44:18,059
of a better product because it's for

00:44:16,170 --> 00:44:20,670
cleaner features because there's going

00:44:18,059 --> 00:44:23,280
to be fewer bugs because our tests will

00:44:20,670 --> 00:44:25,200
be more resilient because the users are

00:44:23,280 --> 00:44:27,420
going to be happier and more trusting

00:44:25,200 --> 00:44:32,430
and we'll have the public stress as well

00:44:27,420 --> 00:44:34,410
these are reasons to insist we are hired

00:44:32,430 --> 00:44:37,049
for more than just writing code were

00:44:34,410 --> 00:44:39,809
hired as professionals who use our

00:44:37,049 --> 00:44:41,530
expertise and our judgment about how to

00:44:39,809 --> 00:44:44,520
solve problems

00:44:41,530 --> 00:44:47,080
problem solving is our core feature our

00:44:44,520 --> 00:44:51,010
role is to be opinionated about how to

00:44:47,080 --> 00:44:53,350
make code serve a problem space well so

00:44:51,010 --> 00:44:55,180
do that here when we're asked to write

00:44:53,350 --> 00:44:57,790
code that presumes to into its people's

00:44:55,180 --> 00:45:00,100
intro internal life and to act on

00:44:57,790 --> 00:45:01,510
assumptions about that we have to be

00:45:00,100 --> 00:45:05,530
their proxies we have to be there

00:45:01,510 --> 00:45:07,540
advocates saying no on their behalf to

00:45:05,530 --> 00:45:09,490
using their data in ways that they have

00:45:07,540 --> 00:45:12,820
not enthusiastically and knowingly

00:45:09,490 --> 00:45:15,280
consented to say no uncritically

00:45:12,820 --> 00:45:19,030
reproducing systems that were biased to

00:45:15,280 --> 00:45:21,730
begin with saying no to writing code

00:45:19,030 --> 00:45:27,100
that imposes unauthorized consequences

00:45:21,730 --> 00:45:32,820
on their lives and short we need to

00:45:27,100 --> 00:45:32,820
refuse to play along thank

00:45:40,200 --> 00:45:42,230
I

00:45:48,890 --> 00:45:50,950

YouTube URL: https://www.youtube.com/watch?v=NheE6udjfGI


