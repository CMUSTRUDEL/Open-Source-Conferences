Title: Privacy Preserving Record Linkage
Publication date: 2018-08-24
Playlist: PyCon Australia 2018
Description: 
	Brian Thorne

https://2018.pycon-au.org/talks/44892-privacy-preserving-record-linkage/

Record linkage is essential for organizations to collaborate and carry out joint analysis. Instead of trusting someone with lots of personally identifiable information like name/address we can learn the entity matching in a privacy preserving way. Let's talk about a Python implementation of that!

Python, PyCon, PyConAU, australia, programming, sydney

This video is licensed under CC BY 3.0 AU ‹https://creativecommons.org/licenses/by/3.0/au/›.

PyCon Australia (“PyCon AU”) is the national conference for the Python Programming Community, bringing together professional, student and enthusiast developers with a love for developing with Python.

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 24 - 28 2018.

Python, PyCon, PyConAU
Captions: 
	00:00:00,469 --> 00:00:06,180
morning everyone can we all squish to

00:00:04,110 --> 00:00:08,700
the middle and all be friends because we

00:00:06,180 --> 00:00:10,170
have a giant room and we don't want to

00:00:08,700 --> 00:00:11,730
have to do huge amounts of running

00:00:10,170 --> 00:00:17,910
around like we metalizer to earlier

00:00:11,730 --> 00:00:20,699
please Brian leads a small engineering

00:00:17,910 --> 00:00:22,920
team at data 61 called confidential

00:00:20,699 --> 00:00:24,840
computing he and his team work on

00:00:22,920 --> 00:00:27,480
building privacy-preserving solutions

00:00:24,840 --> 00:00:29,910
and Brian's gonna give us the lowdown on

00:00:27,480 --> 00:00:37,649
linking data in a privacy-preserving way

00:00:29,910 --> 00:00:39,000
so please enjoy thank you now terrified

00:00:37,649 --> 00:00:40,410
I'm gonna clear my throat and you're

00:00:39,000 --> 00:00:43,590
gonna all hear it very real very loud

00:00:40,410 --> 00:00:45,629
but good morning everybody and I'm Tony

00:00:43,590 --> 00:00:48,329
and Zed on Twitter if you want to ask

00:00:45,629 --> 00:00:49,980
questions or heckle their confidential

00:00:48,329 --> 00:00:52,230
computing team does a whole lot in the

00:00:49,980 --> 00:00:53,789
privacy-preserving space primarily

00:00:52,230 --> 00:00:55,829
looking at can we do privacy preserving

00:00:53,789 --> 00:00:58,680
variants of machine learning and also

00:00:55,829 --> 00:01:00,750
record linkage data 61 works in a wider

00:00:58,680 --> 00:01:02,730
area than this and there's quite a few

00:01:00,750 --> 00:01:04,379
people from data 61 here I'm not going

00:01:02,730 --> 00:01:06,240
to go into any of the projects that they

00:01:04,379 --> 00:01:08,640
do but I would encourage you to come up

00:01:06,240 --> 00:01:09,060
say hello we're a relatively a friendly

00:01:08,640 --> 00:01:10,530
Bunch

00:01:09,060 --> 00:01:13,860
this morning what I'm going to talk

00:01:10,530 --> 00:01:15,210
about is how record linkage works in

00:01:13,860 --> 00:01:17,250
general and then how a

00:01:15,210 --> 00:01:18,720
privacy-preserving variant of that can

00:01:17,250 --> 00:01:20,900
work so we're good we'll get straight

00:01:18,720 --> 00:01:23,520
into it there's quite a bit to cover

00:01:20,900 --> 00:01:25,350
record linkage as a principle as a

00:01:23,520 --> 00:01:27,900
concept that's really simple we want to

00:01:25,350 --> 00:01:29,610
link records rows and one data set to

00:01:27,900 --> 00:01:32,189
rows and another data set and this

00:01:29,610 --> 00:01:33,509
seemingly simple task is carried out all

00:01:32,189 --> 00:01:37,229
around the world and government

00:01:33,509 --> 00:01:39,750
education health businesses every single

00:01:37,229 --> 00:01:43,890
day it's often done in spreadsheets and

00:01:39,750 --> 00:01:45,479
databases by hand or by programs and in

00:01:43,890 --> 00:01:49,079
general the process looks something like

00:01:45,479 --> 00:01:50,579
this we have a two different data sets

00:01:49,079 --> 00:01:53,460
and we compute the similarity between

00:01:50,579 --> 00:01:56,340
those rows that comparison table gets

00:01:53,460 --> 00:01:59,250
passed into a decision function which

00:01:56,340 --> 00:02:03,149
has to determine is this pair of Mis of

00:01:59,250 --> 00:02:04,829
rows an actual match or not so this

00:02:03,149 --> 00:02:06,540
morning I'm going to use an example

00:02:04,829 --> 00:02:09,720
right through and to warm up we'll just

00:02:06,540 --> 00:02:12,120
do a very simple record linkage

00:02:09,720 --> 00:02:13,560
operation using SQL so the synthetic

00:02:12,120 --> 00:02:17,760
data set that I'm going to

00:02:13,560 --> 00:02:20,310
use comes from the febrile tool out of

00:02:17,760 --> 00:02:22,290
Anu and it's exposed through this this

00:02:20,310 --> 00:02:24,630
Python library for record linkage called

00:02:22,290 --> 00:02:29,010
record linkage and which is very very

00:02:24,630 --> 00:02:31,530
good and we have two tables and they're

00:02:29,010 --> 00:02:32,849
both five thousand rows and there's an

00:02:31,530 --> 00:02:35,250
exact correspondence and we're just

00:02:32,849 --> 00:02:37,440
trying to learn that correspondence and

00:02:35,250 --> 00:02:39,269
the data set has the type of columns

00:02:37,440 --> 00:02:41,220
that you might expect so you've got

00:02:39,269 --> 00:02:42,540
things that identify a person personally

00:02:41,220 --> 00:02:45,840
identifiable information like name

00:02:42,540 --> 00:02:47,700
address information date of birth things

00:02:45,840 --> 00:02:49,769
like that now to keep the magic to a

00:02:47,700 --> 00:02:52,980
minimum although the record linkage

00:02:49,769 --> 00:02:55,430
toolkit uses pandas data frames which

00:02:52,980 --> 00:02:58,470
are excellent and instead we're going to

00:02:55,430 --> 00:03:01,590
export to SQL and just keep keep

00:02:58,470 --> 00:03:04,590
everything on the table in plain sight

00:03:01,590 --> 00:03:07,680
so to start with if we have a shared key

00:03:04,590 --> 00:03:10,049
in this case the social security number

00:03:07,680 --> 00:03:13,290
we have in front of us a very easy job

00:03:10,049 --> 00:03:14,880
we do a simple join which in SQL looks

00:03:13,290 --> 00:03:17,660
something like this and what we'd get

00:03:14,880 --> 00:03:20,850
out would be a table of record peers

00:03:17,660 --> 00:03:23,820
it's on this case we're mapping exactly

00:03:20,850 --> 00:03:25,769
where a shared key Scizor but the much

00:03:23,820 --> 00:03:28,709
more interesting case is where we don't

00:03:25,769 --> 00:03:30,120
have a shared key and then we have a bit

00:03:28,709 --> 00:03:32,220
more of a tricky task and can be a bit

00:03:30,120 --> 00:03:35,010
bit daunting certainly time consuming

00:03:32,220 --> 00:03:37,079
and more prone to error because what

00:03:35,010 --> 00:03:38,970
we'll have to do is use a whole bunch of

00:03:37,079 --> 00:03:41,569
weak identifiers and we'll use the

00:03:38,970 --> 00:03:44,430
personally identifiable information and

00:03:41,569 --> 00:03:47,310
some rules to work out whether or not we

00:03:44,430 --> 00:03:50,190
believe a candidate Pierre is actually a

00:03:47,310 --> 00:03:53,310
match and we'll separate those so here

00:03:50,190 --> 00:03:55,680
in SQL we can create binary features

00:03:53,310 --> 00:03:58,079
which is just exact matching of

00:03:55,680 --> 00:03:59,700
particular features very straightforward

00:03:58,079 --> 00:04:02,310
we're comparing surname with surname and

00:03:59,700 --> 00:04:04,500
if if one of them was null we're trying

00:04:02,310 --> 00:04:06,260
to handle the missing value by assuming

00:04:04,500 --> 00:04:09,569
some value that's not zero but not one

00:04:06,260 --> 00:04:11,010
and we're doing the same for for all of

00:04:09,569 --> 00:04:14,850
the features that we have a list and

00:04:11,010 --> 00:04:16,859
data set now there's many many ways to

00:04:14,850 --> 00:04:19,049
compare features that aren't just exact

00:04:16,859 --> 00:04:21,930
comparisons if you can if you can

00:04:19,049 --> 00:04:23,130
measure a distance between them then you

00:04:21,930 --> 00:04:25,410
know

00:04:23,130 --> 00:04:27,780
then you can use that distance metric to

00:04:25,410 --> 00:04:30,120
know how close the rows actually are to

00:04:27,780 --> 00:04:32,040
each other and close can be a whole

00:04:30,120 --> 00:04:34,770
bunch of different definitions it could

00:04:32,040 --> 00:04:37,020
be it at distance between words it could

00:04:34,770 --> 00:04:40,320
be how similar words sound to each other

00:04:37,020 --> 00:04:42,960
it could be temporal how close rows are

00:04:40,320 --> 00:04:44,820
in time could be in space geo coded

00:04:42,960 --> 00:04:46,740
addresses you can measure the physical

00:04:44,820 --> 00:04:48,510
distance between them things like that

00:04:46,740 --> 00:04:52,200
in this case we're going to stay very

00:04:48,510 --> 00:04:55,290
simple and so we're going to just use

00:04:52,200 --> 00:04:57,180
these binary almost binary given when

00:04:55,290 --> 00:04:59,610
there's missing data it's not going to

00:04:57,180 --> 00:05:04,110
be quite 1 or 0 and that will give us a

00:04:59,610 --> 00:05:06,090
table of mostly binary features for all

00:05:04,110 --> 00:05:09,030
of our candidate peers now looking at

00:05:06,090 --> 00:05:11,070
these samples what we've got here is one

00:05:09,030 --> 00:05:15,330
row for every possible candidate link

00:05:11,070 --> 00:05:16,530
and the the features are showing for the

00:05:15,330 --> 00:05:18,960
first couple of rows that there's not

00:05:16,530 --> 00:05:21,750
good matches the street number address

00:05:18,960 --> 00:05:24,090
postcode state are all not exactly

00:05:21,750 --> 00:05:26,010
matching so most likely those proposed

00:05:24,090 --> 00:05:27,780
links those candidate pairs of records

00:05:26,010 --> 00:05:29,520
are terrible they're not a good match

00:05:27,780 --> 00:05:30,590
whereas the last ones most of the

00:05:29,520 --> 00:05:33,270
features are matching

00:05:30,590 --> 00:05:34,800
it looks like the postcode isn't

00:05:33,270 --> 00:05:36,870
matching on the last two but all the

00:05:34,800 --> 00:05:39,720
other features are so they're quite

00:05:36,870 --> 00:05:42,000
quite likely going to be a good record

00:05:39,720 --> 00:05:43,980
match I know I forget my postcode and

00:05:42,000 --> 00:05:47,370
put put down right the last place I

00:05:43,980 --> 00:05:49,320
lived all the time as a quick aside how

00:05:47,370 --> 00:05:51,930
many candidate piers do we possibly have

00:05:49,320 --> 00:05:54,240
here this is a toy data set right 5,000

00:05:51,930 --> 00:05:58,080
records on it either side that gives us

00:05:54,240 --> 00:05:59,580
25 million possible links and you might

00:05:58,080 --> 00:06:01,440
imagine that as we go into more

00:05:59,580 --> 00:06:05,130
real-world data sets this problem just

00:06:01,440 --> 00:06:07,770
gets worse so we'll make our job a

00:06:05,130 --> 00:06:10,350
little bit easier by using a concept of

00:06:07,770 --> 00:06:12,180
blocking or indexing and that's a really

00:06:10,350 --> 00:06:13,800
important practical aspect of record

00:06:12,180 --> 00:06:15,840
linkage which simply limits what we

00:06:13,800 --> 00:06:18,810
consider at all based on some other

00:06:15,840 --> 00:06:20,760
rules now here we'll take our candidate

00:06:18,810 --> 00:06:23,640
peers from all records where this so

00:06:20,760 --> 00:06:25,860
name the postcode or the suburb have to

00:06:23,640 --> 00:06:27,750
match already and this takes the number

00:06:25,860 --> 00:06:29,640
of candidate peers down from 25 million

00:06:27,750 --> 00:06:32,590
to one hundred and thirty thousand

00:06:29,640 --> 00:06:34,990
considerably easier for us to work with

00:06:32,590 --> 00:06:37,510
Adam epic on a you a couple of years ago

00:06:34,990 --> 00:06:42,310
in Brisbane ridwan gave a an excellent

00:06:37,510 --> 00:06:46,360
talk on record linkage we're and a lot

00:06:42,310 --> 00:06:48,669
more detail was spent going into Cuba

00:06:46,360 --> 00:06:50,860
and blocking sorted neighborhoods and

00:06:48,669 --> 00:06:53,350
and how to deal with missing data and a

00:06:50,860 --> 00:06:55,870
lot of the implications that you have in

00:06:53,350 --> 00:06:57,370
blocking in record linkage in general

00:06:55,870 --> 00:07:00,490
and I'd highly recommend checking that

00:06:57,370 --> 00:07:02,680
out um I'm gonna I'm gonna carry on

00:07:00,490 --> 00:07:05,700
before before we got distracted by

00:07:02,680 --> 00:07:08,590
blocking we had this big binary table of

00:07:05,700 --> 00:07:10,930
features and we need to decide which

00:07:08,590 --> 00:07:15,460
ones are a real link or not so we need

00:07:10,930 --> 00:07:17,800
to classify that and that's simply given

00:07:15,460 --> 00:07:19,750
these features can we can rewrite a rule

00:07:17,800 --> 00:07:22,200
can we determine some some rule that

00:07:19,750 --> 00:07:24,310
will say I count these as matches and I

00:07:22,200 --> 00:07:26,680
don't count these right these are not

00:07:24,310 --> 00:07:27,910
matches so we might ensure that we just

00:07:26,680 --> 00:07:29,500
come up with something might say well if

00:07:27,910 --> 00:07:31,270
the social security number matches and

00:07:29,500 --> 00:07:33,040
the state matches and the name matches

00:07:31,270 --> 00:07:34,810
then we'll consider that good enough

00:07:33,040 --> 00:07:37,930
or we could say well if five of the

00:07:34,810 --> 00:07:39,340
features match that's good enough now to

00:07:37,930 --> 00:07:42,010
get a bit of a feel for the data I

00:07:39,340 --> 00:07:46,150
wanted to plot the distribution of how

00:07:42,010 --> 00:07:47,289
many features are matching so this is

00:07:46,150 --> 00:07:49,630
after blocking so we don't we're not

00:07:47,289 --> 00:07:52,120
looking at all 25 million but of those

00:07:49,630 --> 00:07:54,190
130 thousand left I've counted the

00:07:52,120 --> 00:07:56,710
number of features that matched exactly

00:07:54,190 --> 00:07:58,990
and it's dwarfed by when only one or two

00:07:56,710 --> 00:08:00,520
features match as you might expect a lot

00:07:58,990 --> 00:08:04,960
more people live in New South Wales that

00:08:00,520 --> 00:08:06,550
aren't actually the same person so all

00:08:04,960 --> 00:08:08,320
of the data is kind of dwarfed by those

00:08:06,550 --> 00:08:10,600
first two columns so let's enhance and

00:08:08,320 --> 00:08:12,300
and look look it at the right and we get

00:08:10,600 --> 00:08:15,100
a bit of a bit of picture a lot of

00:08:12,300 --> 00:08:19,030
possible candidates seem to agree on six

00:08:15,100 --> 00:08:20,620
seven eight nine features and you might

00:08:19,030 --> 00:08:23,200
imagine that some features are more

00:08:20,620 --> 00:08:25,000
important than others some might be more

00:08:23,200 --> 00:08:26,080
reliable than others and we could we

00:08:25,000 --> 00:08:27,550
could think about how to weigh those

00:08:26,080 --> 00:08:29,169
differently and this could be from

00:08:27,550 --> 00:08:30,700
arrows and the data I mean I'm certainly

00:08:29,169 --> 00:08:32,860
not going to point fingers at the

00:08:30,700 --> 00:08:34,479
emergency departments triage nurse who

00:08:32,860 --> 00:08:35,800
has far better things to do than the

00:08:34,479 --> 00:08:36,909
triple check that she spelled the name

00:08:35,800 --> 00:08:39,310
right well did they spelled the name

00:08:36,909 --> 00:08:42,400
right sorry and it could also be changes

00:08:39,310 --> 00:08:43,839
in the data so features including but

00:08:42,400 --> 00:08:44,980
not limited to what we have here a

00:08:43,839 --> 00:08:47,649
drifts phone

00:08:44,980 --> 00:08:49,959
names date of birth gender or subject to

00:08:47,649 --> 00:08:51,910
change as an aside I'd highly recommend

00:08:49,959 --> 00:08:52,540
people to check out two blog posts

00:08:51,910 --> 00:08:55,149
falsehoods

00:08:52,540 --> 00:08:56,589
programmers believe about names and

00:08:55,149 --> 00:08:58,149
falsehoods programmers believe about

00:08:56,589 --> 00:09:01,680
addresses it's got some very interesting

00:08:58,149 --> 00:09:04,149
counter examples to a lot of things um

00:09:01,680 --> 00:09:07,860
one more note on classification is if we

00:09:04,149 --> 00:09:10,899
had training data say if we had know in

00:09:07,860 --> 00:09:12,399
evidence of known good matches and bad

00:09:10,899 --> 00:09:13,769
matches then we could perhaps train a

00:09:12,399 --> 00:09:15,790
machine learning classifier to do this

00:09:13,769 --> 00:09:19,779
but we're going to keep it simple again

00:09:15,790 --> 00:09:21,790
we're going to simply decide that a

00:09:19,779 --> 00:09:24,579
candidate will be a match of five or

00:09:21,790 --> 00:09:27,490
more features happen to happen to

00:09:24,579 --> 00:09:29,589
exactly match and that takes what was a

00:09:27,490 --> 00:09:31,690
hundred thirty thousand candidates down

00:09:29,589 --> 00:09:33,220
to four thousand eight hundred now we

00:09:31,690 --> 00:09:34,690
happen to know that there's an exact

00:09:33,220 --> 00:09:37,420
correspondence here so we're looking for

00:09:34,690 --> 00:09:40,029
a brand or five thousand would be the

00:09:37,420 --> 00:09:42,310
perfect answer and we've put quite

00:09:40,029 --> 00:09:43,990
confident about those like we know that

00:09:42,310 --> 00:09:48,190
that seems to have separated most of the

00:09:43,990 --> 00:09:49,600
non matches but that's just kind of

00:09:48,190 --> 00:09:52,959
intuition we'd really like to evaluate

00:09:49,600 --> 00:09:54,639
how well how accurate this actually is

00:09:52,959 --> 00:09:56,440
so there are two metrics that have

00:09:54,639 --> 00:09:58,750
interests here one is precision which

00:09:56,440 --> 00:10:01,930
asks how many of our proposed matches

00:09:58,750 --> 00:10:03,940
were wrong so of those 4,800 were any of

00:10:01,930 --> 00:10:06,670
them a link between people that weren't

00:10:03,940 --> 00:10:08,410
the same entity and in this case no we

00:10:06,670 --> 00:10:12,459
got that perfectly right we had perfect

00:10:08,410 --> 00:10:15,190
precision recall asks did we miss any so

00:10:12,459 --> 00:10:17,199
were any of the valid matches not picked

00:10:15,190 --> 00:10:19,329
up by our simple rule of thumb that five

00:10:17,199 --> 00:10:21,220
of the columns had to match exactly and

00:10:19,329 --> 00:10:22,569
sure enough that simple rule didn't

00:10:21,220 --> 00:10:23,889
quite pick everything up doesn't have

00:10:22,569 --> 00:10:24,720
good recall or doesn't have perfect

00:10:23,889 --> 00:10:26,829
recall

00:10:24,720 --> 00:10:29,350
depending on the use case this might be

00:10:26,829 --> 00:10:32,740
perfectly adequate recall and we'd have

00:10:29,350 --> 00:10:34,449
to investigate what was missed and was

00:10:32,740 --> 00:10:37,959
it particularly bad for some

00:10:34,449 --> 00:10:39,880
demographics rather than others now I'd

00:10:37,959 --> 00:10:42,310
like to step back a little bit and think

00:10:39,880 --> 00:10:44,050
okay so all of this has happened the

00:10:42,310 --> 00:10:45,220
analyst has done this work probably

00:10:44,050 --> 00:10:46,839
they've done it a little bit more

00:10:45,220 --> 00:10:48,459
sophisticated than what I just walked

00:10:46,839 --> 00:10:49,899
through now but they've done it all in

00:10:48,459 --> 00:10:51,760
one place and they had access to the

00:10:49,899 --> 00:10:54,189
data locally and that's the traditional

00:10:51,760 --> 00:10:56,949
method of recording each We Trust have

00:10:54,189 --> 00:10:58,810
these semi trusted linkage authorities

00:10:56,949 --> 00:11:01,450
or third-party businesses that

00:10:58,810 --> 00:11:03,340
this as part of their business now here

00:11:01,450 --> 00:11:04,810
in New South Wales that might mean there

00:11:03,340 --> 00:11:07,029
are the center for health record linkage

00:11:04,810 --> 00:11:10,210
and this essay and T's data link curtain

00:11:07,029 --> 00:11:11,470
Centre for data linkage a lot of these

00:11:10,210 --> 00:11:13,660
places they all have the same

00:11:11,470 --> 00:11:15,430
requirement that you trust them with the

00:11:13,660 --> 00:11:16,960
personally identifiable information on

00:11:15,430 --> 00:11:19,180
which you're going to carry out linkage

00:11:16,960 --> 00:11:22,210
so the rest of this presentation is

00:11:19,180 --> 00:11:24,400
going to consider not doing that not

00:11:22,210 --> 00:11:28,240
having to trust anyone with this

00:11:24,400 --> 00:11:29,770
personally identifiable information now

00:11:28,240 --> 00:11:31,120
there are essentially three

00:11:29,770 --> 00:11:32,290
considerations three things I'd like you

00:11:31,120 --> 00:11:35,020
to keep in mind while I go through the

00:11:32,290 --> 00:11:37,570
rest which is is is what we're doing or

00:11:35,020 --> 00:11:41,470
what I'm proposing accurate is it

00:11:37,570 --> 00:11:42,700
actually private and does it scale so if

00:11:41,470 --> 00:11:44,350
we're going to be doing some sort of

00:11:42,700 --> 00:11:45,700
blind computation we have to be

00:11:44,350 --> 00:11:48,220
confident about the fact that we're

00:11:45,700 --> 00:11:50,170
still doing well performance wise in

00:11:48,220 --> 00:11:52,510
both accuracy and scale like if it

00:11:50,170 --> 00:11:57,070
doesn't work on in real-world scale it's

00:11:52,510 --> 00:11:59,830
not very helpful and we have to deal

00:11:57,070 --> 00:12:02,170
with the real world which we know has

00:11:59,830 --> 00:12:07,930
Missy and missing and just generally

00:12:02,170 --> 00:12:11,589
rubbish data so I guess two to summarize

00:12:07,930 --> 00:12:12,910
there we want to accurately do record

00:12:11,589 --> 00:12:14,530
linkage knowing the dad is full of

00:12:12,910 --> 00:12:16,480
rubbish we want it to scale to

00:12:14,530 --> 00:12:19,810
population so say millions of records

00:12:16,480 --> 00:12:21,460
and we'd like it to do all that without

00:12:19,810 --> 00:12:24,790
sharing the information which is most

00:12:21,460 --> 00:12:26,920
useful for record linkage so hope you've

00:12:24,790 --> 00:12:28,630
had your coffee now

00:12:26,920 --> 00:12:30,280
our team has open sourced to libraries

00:12:28,630 --> 00:12:32,800
that can help in this regard and I'm

00:12:30,280 --> 00:12:36,100
going to go through the same example now

00:12:32,800 --> 00:12:37,930
about using this instead that called

00:12:36,100 --> 00:12:39,310
click hash and an on link they're

00:12:37,930 --> 00:12:42,370
available on github you can install them

00:12:39,310 --> 00:12:44,830
from pip click hash will run on your

00:12:42,370 --> 00:12:47,589
normal computer whether it's Windows Mac

00:12:44,830 --> 00:12:49,060
Linux Python 2 or python 3 and non link

00:12:47,589 --> 00:12:51,070
is the part that would be installed in

00:12:49,060 --> 00:12:55,060
the data center with a similar and it's

00:12:51,070 --> 00:12:56,320
got compiled code to go fast and that's

00:12:55,060 --> 00:13:02,860
a little bit more restrictive I think

00:12:56,320 --> 00:13:04,240
it's Python 3 7 only so now now we're

00:13:02,860 --> 00:13:05,410
going to have a little bit more

00:13:04,240 --> 00:13:09,040
separation and there are three players

00:13:05,410 --> 00:13:10,270
and that means the data analyst can't

00:13:09,040 --> 00:13:11,380
just kind of look at the data and go

00:13:10,270 --> 00:13:12,460
well I'm going to use this column in

00:13:11,380 --> 00:13:14,290
this column and I'm going to norm

00:13:12,460 --> 00:13:15,820
allows them in this way it means a lot

00:13:14,290 --> 00:13:18,640
of that has to happen in a bit more of a

00:13:15,820 --> 00:13:20,680
distributed environment and so we

00:13:18,640 --> 00:13:23,260
imagine that there's a schema where our

00:13:20,680 --> 00:13:24,940
tools take a schema that is the contract

00:13:23,260 --> 00:13:27,400
between the two parties that will

00:13:24,940 --> 00:13:30,460
instruct our tool click ash how to treat

00:13:27,400 --> 00:13:33,040
each column to encode the data and I've

00:13:30,460 --> 00:13:34,900
just got a very simple example here of

00:13:33,040 --> 00:13:37,060
this schema which is describing how we

00:13:34,900 --> 00:13:40,060
might encode a surname and how we might

00:13:37,060 --> 00:13:41,680
encode a street number the details here

00:13:40,060 --> 00:13:43,180
don't matter too much but essentially

00:13:41,680 --> 00:13:46,330
we're going to be encoding the press the

00:13:43,180 --> 00:13:48,460
identifiable information on the data

00:13:46,330 --> 00:13:49,900
provider like at the data provider at

00:13:48,460 --> 00:13:54,130
the source of the data and then the

00:13:49,900 --> 00:13:56,680
encoding is going to be shed now if

00:13:54,130 --> 00:13:58,720
you'll forgive my Jupiter templated

00:13:56,680 --> 00:14:00,220
shell command briefly I wanted to show

00:13:58,720 --> 00:14:04,030
that we also had a command-line tool not

00:14:00,220 --> 00:14:07,480
just a Python library and essentially

00:14:04,030 --> 00:14:10,810
this would be the command that a data

00:14:07,480 --> 00:14:13,120
analyst is using to hash or encode their

00:14:10,810 --> 00:14:15,640
CSV file of personally identifiable

00:14:13,120 --> 00:14:17,410
information they're passing in a schema

00:14:15,640 --> 00:14:20,710
that we just saw and they're passing in

00:14:17,410 --> 00:14:22,390
a secret horse staple I think and in

00:14:20,710 --> 00:14:24,460
this case an output file name now that

00:14:22,390 --> 00:14:27,400
would happen on two location two data

00:14:24,460 --> 00:14:29,410
providers would do this and then they

00:14:27,400 --> 00:14:32,050
would upload it to a semi trusted third

00:14:29,410 --> 00:14:33,970
party so this is I'll go into this in a

00:14:32,050 --> 00:14:35,950
bit it's still sensitive information but

00:14:33,970 --> 00:14:37,450
it's not your personally identifiable

00:14:35,950 --> 00:14:37,920
information and it's a long way from

00:14:37,450 --> 00:14:41,080
that

00:14:37,920 --> 00:14:43,080
so now assume that we're sitting at the

00:14:41,080 --> 00:14:48,490
linkage authority we've loaded those

00:14:43,080 --> 00:14:51,400
encoded PII which I call clicks into

00:14:48,490 --> 00:14:53,380
Python and we can compute in the same

00:14:51,400 --> 00:14:54,850
way the same process I've introduced at

00:14:53,380 --> 00:14:58,480
the beginning we can compute their

00:14:54,850 --> 00:15:00,250
similarity so with an on link we can

00:14:58,480 --> 00:15:02,040
calculate the similarity and that will

00:15:00,250 --> 00:15:05,140
simply go through I'm passing in

00:15:02,040 --> 00:15:06,910
threshold here saying don't discard

00:15:05,140 --> 00:15:08,470
anything I want you to give me all of

00:15:06,910 --> 00:15:12,070
the results back so that will go through

00:15:08,470 --> 00:15:14,650
and compute all 25 million similarity

00:15:12,070 --> 00:15:16,870
scores like every road every row in data

00:15:14,650 --> 00:15:18,520
set a to be and that's of course quite a

00:15:16,870 --> 00:15:21,160
lot and that gets worse as we have

00:15:18,520 --> 00:15:23,200
larger data sets 25 million comparisons

00:15:21,160 --> 00:15:24,850
of these cryptographic long term keys

00:15:23,200 --> 00:15:27,639
these encodings of PII

00:15:24,850 --> 00:15:30,519
that can take some time and usually you

00:15:27,639 --> 00:15:32,649
would use a default threshold you might

00:15:30,519 --> 00:15:36,009
use blocking to improve the performance

00:15:32,649 --> 00:15:37,720
of this so now that we've computed the

00:15:36,009 --> 00:15:40,060
similarity scores which are all just a

00:15:37,720 --> 00:15:42,399
whole bunch of values between 0 & 1 how

00:15:40,060 --> 00:15:45,880
similar was this row to this row we've

00:15:42,399 --> 00:15:48,550
got 25 million of them we could have we

00:15:45,880 --> 00:15:51,699
could have a look at those and I want to

00:15:48,550 --> 00:15:54,190
touch though on this is an expensive

00:15:51,699 --> 00:15:56,199
problem in in the performance sense but

00:15:54,190 --> 00:15:59,380
it is also very easily parallelizable

00:15:56,199 --> 00:16:02,050
so we have wrapped an on link inside a

00:15:59,380 --> 00:16:04,750
REST API doc arose that containerized it

00:16:02,050 --> 00:16:07,089
put it on kubernetes and scaled it to I

00:16:04,750 --> 00:16:11,620
think we tried last week with about 128

00:16:07,089 --> 00:16:13,290
cause 128 CPU cores and we measure a

00:16:11,620 --> 00:16:16,660
throughput of around about 4 billion

00:16:13,290 --> 00:16:18,220
comparisons per second and like what

00:16:16,660 --> 00:16:20,709
that translates to is essentially we can

00:16:18,220 --> 00:16:23,470
do a match of a million records on

00:16:20,709 --> 00:16:25,509
either side in under 5 minutes which

00:16:23,470 --> 00:16:26,949
we're we're we're quite happy with it

00:16:25,509 --> 00:16:30,490
seems to be approaching useful for

00:16:26,949 --> 00:16:32,290
real-world size data problems so now

00:16:30,490 --> 00:16:34,360
looking at these similarity scores this

00:16:32,290 --> 00:16:36,880
is essentially the same graph that we

00:16:34,360 --> 00:16:39,699
saw before with the eight or nine binary

00:16:36,880 --> 00:16:43,630
features in SQL we've now got a whole

00:16:39,699 --> 00:16:45,399
bunch of no that's not quite true I

00:16:43,630 --> 00:16:46,810
won't say that we're looking at the

00:16:45,399 --> 00:16:49,029
distribution of similarity scores

00:16:46,810 --> 00:16:50,680
between these rows and the analysts can

00:16:49,029 --> 00:16:54,430
use this to make an educated guess about

00:16:50,680 --> 00:16:56,759
which scores are probably from these nan

00:16:54,430 --> 00:16:59,949
matches like we had in SQL and which

00:16:56,759 --> 00:17:01,509
scores are probably indicating a good

00:16:59,949 --> 00:17:04,449
match and now can you see the two

00:17:01,509 --> 00:17:06,130
distributions there there's there's one

00:17:04,449 --> 00:17:08,470
very big obvious one which is the normal

00:17:06,130 --> 00:17:10,360
distribution in the middle and if we

00:17:08,470 --> 00:17:12,069
zoom in on the right there's a very

00:17:10,360 --> 00:17:14,500
small distribution which hides all our

00:17:12,069 --> 00:17:16,630
actual matches and so probably what we

00:17:14,500 --> 00:17:20,620
want to do is discard all of that huge

00:17:16,630 --> 00:17:24,429
normal distribution of non matches and

00:17:20,620 --> 00:17:27,100
look in and work out can we use can we

00:17:24,429 --> 00:17:32,409
trust all of these similarity scores

00:17:27,100 --> 00:17:36,100
above 0.9 here and are they very good

00:17:32,409 --> 00:17:37,540
matches so if we knew the answers and in

00:17:36,100 --> 00:17:38,570
this case this is a synthetic data set

00:17:37,540 --> 00:17:41,060
so we can peek under the hood

00:17:38,570 --> 00:17:44,330
say well this is what the actual data is

00:17:41,060 --> 00:17:47,600
and the matches here are shown in red

00:17:44,330 --> 00:17:49,820
what I'm I might I might slow down to

00:17:47,600 --> 00:17:52,520
describe this graph on the bottom I have

00:17:49,820 --> 00:17:56,120
a few samples of records from data set

00:17:52,520 --> 00:17:59,690
one and all of the scores all of them

00:17:56,120 --> 00:18:02,450
links to data set B that are above a

00:17:59,690 --> 00:18:03,980
similarity of 0.8 and that colored by

00:18:02,450 --> 00:18:07,250
whether or not there are non match or an

00:18:03,980 --> 00:18:08,690
actual match so very helpfully the

00:18:07,250 --> 00:18:11,930
actual matches are all the

00:18:08,690 --> 00:18:13,810
highest-scoring matches and they are all

00:18:11,930 --> 00:18:16,040
looking like that between point 9 and 1

00:18:13,810 --> 00:18:18,650
but you see out on the right there that

00:18:16,040 --> 00:18:20,300
a few actual matches are hidden down and

00:18:18,650 --> 00:18:21,920
so if we just apply a really simple

00:18:20,300 --> 00:18:23,720
blanket rule and just say discard

00:18:21,920 --> 00:18:26,810
everything below point 9 we're probably

00:18:23,720 --> 00:18:29,300
going to have low recall as in some of

00:18:26,810 --> 00:18:30,850
the matches won't be identified and that

00:18:29,300 --> 00:18:33,350
might be okay

00:18:30,850 --> 00:18:35,090
so if we apply that threshold and we'll

00:18:33,350 --> 00:18:36,650
just use that blanket rule of everything

00:18:35,090 --> 00:18:38,990
below zero point nine we'll discard and

00:18:36,650 --> 00:18:41,900
we can just go through our similarity

00:18:38,990 --> 00:18:45,020
scores and and and killed those we now

00:18:41,900 --> 00:18:48,320
have a much smaller data set and we can

00:18:45,020 --> 00:18:49,970
apply a classifier now we've got a we've

00:18:48,320 --> 00:18:51,680
found that a greedy approach works

00:18:49,970 --> 00:18:54,890
really well here so an on link

00:18:51,680 --> 00:18:56,870
implements a greedy classifier you pass

00:18:54,890 --> 00:18:58,730
at this similarity matrix after we've

00:18:56,870 --> 00:19:00,410
done the thresholding and it essentially

00:18:58,730 --> 00:19:03,050
picks those strongest links like it

00:19:00,410 --> 00:19:05,150
looks from the top for each entity and

00:19:03,050 --> 00:19:09,680
chooses the best one and what we get out

00:19:05,150 --> 00:19:15,200
is a mapping it's the decision model has

00:19:09,680 --> 00:19:19,190
output a map insane row 2751 in data set

00:19:15,200 --> 00:19:21,680
a is this is the same record as 1016 in

00:19:19,190 --> 00:19:22,550
deficit B I got those the wrong way

00:19:21,680 --> 00:19:25,760
around because I was reading the wrong

00:19:22,550 --> 00:19:29,000
column but you get the idea and in this

00:19:25,760 --> 00:19:30,650
case we get about 4700 links and as

00:19:29,000 --> 00:19:33,530
before if we have the ground truth

00:19:30,650 --> 00:19:36,290
information we can evaluate that so the

00:19:33,530 --> 00:19:38,480
same metrics can be applied we find

00:19:36,290 --> 00:19:41,540
again we have really good precision so

00:19:38,480 --> 00:19:46,270
we're finding good quality matches but

00:19:41,540 --> 00:19:48,950
we've not got perfect recall and

00:19:46,270 --> 00:19:50,450
depending on the use case again that

00:19:48,950 --> 00:19:51,540
could be considered good read call or

00:19:50,450 --> 00:19:54,030
maybe not

00:19:51,540 --> 00:19:56,520
and that means that in a

00:19:54,030 --> 00:19:58,980
privacy-preserving way a record linkage

00:19:56,520 --> 00:20:01,110
service and has been used the analysts

00:19:58,980 --> 00:20:02,730
been able to link records without them

00:20:01,110 --> 00:20:07,010
being exposed to raw personally

00:20:02,730 --> 00:20:09,900
identifiable information themselves no I

00:20:07,010 --> 00:20:11,940
should definitely put in a side note

00:20:09,900 --> 00:20:13,920
here and say that when we're discussing

00:20:11,940 --> 00:20:16,470
probably sztyc record linkage there is

00:20:13,920 --> 00:20:17,910
limitations there are limitations you

00:20:16,470 --> 00:20:21,210
have to be aware that there's always the

00:20:17,910 --> 00:20:23,580
possibility of mismatches and depending

00:20:21,210 --> 00:20:25,230
on the use case this might not be a big

00:20:23,580 --> 00:20:28,440
deal or it could be really unethical

00:20:25,230 --> 00:20:30,090
deeply flawed slightly idiotic and you

00:20:28,440 --> 00:20:33,480
might consider the effect of a bad link

00:20:30,090 --> 00:20:36,420
when you're aggregating queries across a

00:20:33,480 --> 00:20:38,210
linked data set for a report a single

00:20:36,420 --> 00:20:41,180
link being wrong might not mean much

00:20:38,210 --> 00:20:43,320
however if you're doing something like

00:20:41,180 --> 00:20:44,670
predicting welfare debt and deciding

00:20:43,320 --> 00:20:47,670
Winterson letters at Christmas

00:20:44,670 --> 00:20:50,730
using mismatched data that might not be

00:20:47,670 --> 00:20:52,890
a good idea so technology doesn't exist

00:20:50,730 --> 00:20:55,010
in a vacuum here there are really

00:20:52,890 --> 00:20:57,900
important steps in terms of governance

00:20:55,010 --> 00:21:00,180
legal ethical implications that I'm not

00:20:57,900 --> 00:21:02,040
going to go into today and at Cheryl

00:21:00,180 --> 00:21:04,590
they have they have a line that all

00:21:02,040 --> 00:21:06,210
studies using linked data must have an

00:21:04,590 --> 00:21:07,980
appropriate legal basis and in most

00:21:06,210 --> 00:21:10,200
cases require ethical approval and

00:21:07,980 --> 00:21:12,570
research institutes like like us at

00:21:10,200 --> 00:21:14,970
CSIRO any projects that use personally

00:21:12,570 --> 00:21:16,830
identifiable information have oversight

00:21:14,970 --> 00:21:18,330
from an independent body an independent

00:21:16,830 --> 00:21:22,260
committee for reap for human research

00:21:18,330 --> 00:21:25,200
ethics in particular ok back on with the

00:21:22,260 --> 00:21:26,640
tick so linkage today is essentially

00:21:25,200 --> 00:21:28,290
done in this way that personally

00:21:26,640 --> 00:21:31,170
identifiable information is shared with

00:21:28,290 --> 00:21:33,210
someone who uses that to personally

00:21:31,170 --> 00:21:36,120
identify them and we want to make sure

00:21:33,210 --> 00:21:39,420
we want to get that same insight without

00:21:36,120 --> 00:21:42,060
having to share the PII and we're saying

00:21:39,420 --> 00:21:44,040
that that's that's possible and now I'm

00:21:42,060 --> 00:21:47,940
going to go into what we did how that

00:21:44,040 --> 00:21:50,520
works so with we're still going to send

00:21:47,940 --> 00:21:52,650
something into a trusted semi trusted

00:21:50,520 --> 00:21:55,590
third party but we're also going to do a

00:21:52,650 --> 00:21:57,600
little bit of computation at the data

00:21:55,590 --> 00:22:02,880
let's let's say we're doing edge

00:21:57,600 --> 00:22:04,260
computing on the edge maybe the first

00:22:02,880 --> 00:22:05,070
thing that comes to mind is why don't we

00:22:04,260 --> 00:22:07,020
just hash it

00:22:05,070 --> 00:22:08,250
and I mean maybe that could work we

00:22:07,020 --> 00:22:09,480
could hash each feature or it could

00:22:08,250 --> 00:22:12,120
hatch the personally identifiable

00:22:09,480 --> 00:22:14,580
information altogether but we end up

00:22:12,120 --> 00:22:16,320
with exact comparisons and so there are

00:22:14,580 --> 00:22:18,330
really two issues firstly hashing

00:22:16,320 --> 00:22:19,710
doesn't necessarily protect much if you

00:22:18,330 --> 00:22:21,870
know information about the underlying

00:22:19,710 --> 00:22:23,640
distribution the underlying distribution

00:22:21,870 --> 00:22:25,320
of dates of birth and Australia or so

00:22:23,640 --> 00:22:27,300
names in Australia then you can attack

00:22:25,320 --> 00:22:28,980
that and for better accuracy we really

00:22:27,300 --> 00:22:31,260
want to do a probably stick linkage so

00:22:28,980 --> 00:22:32,520
we deal with the types of errors in the

00:22:31,260 --> 00:22:35,010
data we know a prison-like

00:22:32,520 --> 00:22:38,190
misspellings so we need a way to encode

00:22:35,010 --> 00:22:53,160
the PII now perhaps xkcd has some advice

00:22:38,190 --> 00:22:54,780
well I take some water okay I'm here I'm

00:22:53,160 --> 00:22:58,620
hearing a few members and we've got five

00:22:54,780 --> 00:23:00,270
minutes left so there we've established

00:22:58,620 --> 00:23:03,210
really that hashing is not going to work

00:23:00,270 --> 00:23:04,800
and perhaps the other tool in the

00:23:03,210 --> 00:23:08,010
security toolbox would be we could

00:23:04,800 --> 00:23:10,110
encrypt the data maybe but comparing

00:23:08,010 --> 00:23:11,700
encrypted data well there's really two

00:23:10,110 --> 00:23:14,130
ways you do that either you're

00:23:11,700 --> 00:23:17,580
decrypting it which just brings us back

00:23:14,130 --> 00:23:18,840
to step one or we're using some some new

00:23:17,580 --> 00:23:21,540
methodology like secure multi-party

00:23:18,840 --> 00:23:23,160
compute or homomorphic encryption and

00:23:21,540 --> 00:23:25,620
that looks really promising and I

00:23:23,160 --> 00:23:27,030
believe that will be the eventual place

00:23:25,620 --> 00:23:30,510
that privacy preserving record linkage

00:23:27,030 --> 00:23:33,030
goes but it's not necessary practical

00:23:30,510 --> 00:23:34,860
for real world scale today and as an

00:23:33,030 --> 00:23:36,810
aside cryptography is not all sit on

00:23:34,860 --> 00:23:38,970
this and cryptography tries really hard

00:23:36,810 --> 00:23:41,490
not to be in the move fast and break

00:23:38,970 --> 00:23:42,990
things way of doing things so we're

00:23:41,490 --> 00:23:45,480
going to look at three concepts for the

00:23:42,990 --> 00:23:47,250
last couple of minutes which is what we

00:23:45,480 --> 00:23:49,280
used in the cliquish and on LinkedIn I

00:23:47,250 --> 00:23:51,030
just gave they are bloom filters

00:23:49,280 --> 00:23:53,010
changing bloom filters into

00:23:51,030 --> 00:23:56,190
cryptographic long term keys and then

00:23:53,010 --> 00:23:58,140
comparing those so the bloom filter is

00:23:56,190 --> 00:24:01,110
really cool it's a very simple data

00:23:58,140 --> 00:24:03,840
structure and it really has to two

00:24:01,110 --> 00:24:05,460
things you can do you can add keys into

00:24:03,840 --> 00:24:07,740
it and you can query was a key part of

00:24:05,460 --> 00:24:09,900
it so here we're adding the key banana

00:24:07,740 --> 00:24:11,820
into a bloom filter and we simply cache

00:24:09,900 --> 00:24:13,800
the string banana a couple of times and

00:24:11,820 --> 00:24:16,300
when you hash that you get and when you

00:24:13,800 --> 00:24:17,770
when your hash banana you might get and

00:24:16,300 --> 00:24:20,050
which we're going to treat as an address

00:24:17,770 --> 00:24:22,000
in the bloom filter and all we're going

00:24:20,050 --> 00:24:25,390
to do is look at that address and set

00:24:22,000 --> 00:24:26,980
those bits to one easy you might do that

00:24:25,390 --> 00:24:28,570
for multiple keys into one bloom filter

00:24:26,980 --> 00:24:30,220
and then when you want to ask is

00:24:28,570 --> 00:24:32,320
something in a bloom filter you just do

00:24:30,220 --> 00:24:34,900
that again and you see what the results

00:24:32,320 --> 00:24:36,010
were at that address so if we hash grape

00:24:34,900 --> 00:24:38,080
three times we get three different

00:24:36,010 --> 00:24:40,720
addresses and it's a probabilistic data

00:24:38,080 --> 00:24:43,570
structure as well so if we got all ones

00:24:40,720 --> 00:24:45,520
back at the addresses then it tells us

00:24:43,570 --> 00:24:47,410
that the filter probably contains the

00:24:45,520 --> 00:24:49,630
key not necessarily there are false

00:24:47,410 --> 00:24:51,730
positives and if any zeros were found we

00:24:49,630 --> 00:24:56,260
know for sure that that exact key wasn't

00:24:51,730 --> 00:24:57,990
hashed into that bloom filter so you're

00:24:56,260 --> 00:25:00,880
probably very aware that a hash function

00:24:57,990 --> 00:25:03,010
works in a way that if you change the

00:25:00,880 --> 00:25:04,930
input slightly the output changes

00:25:03,010 --> 00:25:06,190
drastically and that's not really what

00:25:04,930 --> 00:25:07,780
we want if we're trying to get probably

00:25:06,190 --> 00:25:10,600
stick data linkage and so if we just

00:25:07,780 --> 00:25:12,340
hash the name using a cryptographic hash

00:25:10,600 --> 00:25:14,320
function that's not going to be useful

00:25:12,340 --> 00:25:15,520
Brian spelt with the Y and with an eye

00:25:14,320 --> 00:25:18,040
it's going to hash two completely

00:25:15,520 --> 00:25:21,700
different values for example so instead

00:25:18,040 --> 00:25:24,430
we're going to use in grams really quite

00:25:21,700 --> 00:25:26,020
simple tokens of a string here I've got

00:25:24,430 --> 00:25:32,200
unis grams and by grams of the word

00:25:26,020 --> 00:25:35,020
Smith and combining combining in grams

00:25:32,200 --> 00:25:37,630
into a bloom filter really gives us the

00:25:35,020 --> 00:25:40,810
cryptographic long term key the only

00:25:37,630 --> 00:25:43,420
other piece to the puzzle is an H Mac

00:25:40,810 --> 00:25:45,850
which is a keyed hash function

00:25:43,420 --> 00:25:50,050
essentially and that's that gives us a

00:25:45,850 --> 00:25:54,580
bit more of the security I'm going to

00:25:50,050 --> 00:25:56,650
show the overall view of the system now

00:25:54,580 --> 00:25:59,710
just to kind of walk through and put us

00:25:56,650 --> 00:26:01,600
into perspective so on the data provider

00:25:59,710 --> 00:26:03,250
side we had the personally identifiable

00:26:01,600 --> 00:26:06,040
information and that would be encoded

00:26:03,250 --> 00:26:07,510
now the H Mac comes in here because

00:26:06,040 --> 00:26:10,300
that's that where the shared secret is

00:26:07,510 --> 00:26:12,520
useful to Toyota providers require that

00:26:10,300 --> 00:26:15,030
plus a schema and they encode their

00:26:12,520 --> 00:26:18,970
information upload that to the top box

00:26:15,030 --> 00:26:21,310
some linkage authority a non link can

00:26:18,970 --> 00:26:23,260
take that and compute the similarity

00:26:21,310 --> 00:26:23,860
between all of the cryptographic long

00:26:23,260 --> 00:26:26,110
term keys

00:26:23,860 --> 00:26:28,450
I'll just encoding and then pass it to a

00:26:26,110 --> 00:26:29,800
decision model and we saw an example of

00:26:28,450 --> 00:26:31,910
the greedy sober

00:26:29,800 --> 00:26:34,670
now doing the actual comparison is

00:26:31,910 --> 00:26:36,230
really easy the bloom filter it really

00:26:34,670 --> 00:26:38,120
gives us this if we can count the number

00:26:36,230 --> 00:26:39,470
of bits which are in common then we can

00:26:38,120 --> 00:26:42,410
compare them and we can come up with

00:26:39,470 --> 00:26:43,970
some number between 0 and 1 now I won't

00:26:42,410 --> 00:26:46,580
go into the math of how that works but

00:26:43,970 --> 00:26:50,320
all it requires is an ability to do a

00:26:46,580 --> 00:26:53,720
pop count and an and operator between

00:26:50,320 --> 00:26:56,900
their clicks so we can do that really

00:26:53,720 --> 00:26:58,700
fast and I've been through the decision

00:26:56,900 --> 00:27:01,340
model it's really simple we just take

00:26:58,700 --> 00:27:03,770
this strongest link for any entity and

00:27:01,340 --> 00:27:06,040
in a greedy fashion and that really

00:27:03,770 --> 00:27:09,170
gives us an error error tolerant

00:27:06,040 --> 00:27:11,510
privacy-preserving record linkage thing

00:27:09,170 --> 00:27:13,220
and so we can accurately carry out

00:27:11,510 --> 00:27:15,470
record linkage knowing that the data is

00:27:13,220 --> 00:27:17,780
full of rubbish scale it to millions of

00:27:15,470 --> 00:27:19,030
records and do it without sharing any of

00:27:17,780 --> 00:27:27,650
that personally identifiable information

00:27:19,030 --> 00:27:31,540
that's thanks thank you and oh thank you

00:27:27,650 --> 00:27:31,540
for putting up I get a cup yes come on

00:27:32,260 --> 00:27:35,800

YouTube URL: https://www.youtube.com/watch?v=riskUqGPfcY


