Title: Guide to your own artificial intelligence application in 3 easy steps
Publication date: 2018-08-26
Playlist: PyCon Australia 2018
Description: 
	Norah Klintberg Sakal

https://2018.pycon-au.org/talks/45386-guide-to-your-own-artificial-intelligence-application-in-3-easy-steps/

What do you think of when you hear “artificial intelligence”? Perhaps self-driving cars, autonomous robots and Siri, Alexa or Google Home? But it doesn’t have to be that complex. You can build a powerful image classification model within a topic that inspires and interests you - with 3 easy steps.

Python, PyCon, PyConAU, australia, programming, sydney

This video is licensed under CC BY 3.0 AU - https://creativecommons.org/licenses/by/3.0/au/

PyCon Australia (“PyCon AU”) is the national conference for the Python Programming Community, bringing together professional, student and enthusiast developers with a love for developing with Python.

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 24 - 28 2018.

Python, PyCon, PyConAU
Captions: 
	00:00:00,000 --> 00:00:09,000
morning everyone this is the first track

00:00:04,920 --> 00:00:11,090
session of the morning and we're going

00:00:09,000 --> 00:00:14,460
to kick off with an introduction to AI

00:00:11,090 --> 00:00:17,430
now nor has come over from San Francisco

00:00:14,460 --> 00:00:19,949
to present this and it's her first Pike

00:00:17,430 --> 00:00:31,560
on a you so please make Norah feel

00:00:19,949 --> 00:00:33,840
welcome hi I'm so excited to be here Wow

00:00:31,560 --> 00:00:38,280
let's talk some deep learning is awesome

00:00:33,840 --> 00:00:40,290
ai okay just a few words about me I'm

00:00:38,280 --> 00:00:41,969
originally from Sweden I've been living

00:00:40,290 --> 00:00:44,070
in San Francisco for a year and a half

00:00:41,969 --> 00:00:46,110
and currently I'm doing my master's

00:00:44,070 --> 00:00:49,010
thesis at a company called Shang

00:00:46,110 --> 00:00:52,289
Zuckerberg bio hub that's basically a

00:00:49,010 --> 00:00:54,660
non-profit cam Medical Center where

00:00:52,289 --> 00:00:56,100
Research Center where they want to cure

00:00:54,660 --> 00:00:58,050
all the diseases and it's Mark

00:00:56,100 --> 00:01:01,949
Zuckerberg and his wife Priscilla or

00:00:58,050 --> 00:01:03,750
finding that nonprofit so just a quick

00:01:01,949 --> 00:01:07,680
overview today gonna talk a bit about

00:01:03,750 --> 00:01:09,680
how to find an idea and a bit about

00:01:07,680 --> 00:01:12,210
collecting data which is a very

00:01:09,680 --> 00:01:14,189
important resource especially when you

00:01:12,210 --> 00:01:16,560
want to train your neural network and

00:01:14,189 --> 00:01:20,430
the last part is of course it's raining

00:01:16,560 --> 00:01:23,280
which is the most exciting part and just

00:01:20,430 --> 00:01:26,700
before we begin usually when you think

00:01:23,280 --> 00:01:29,700
about state of the art deep learning or

00:01:26,700 --> 00:01:32,400
AI you think about complex solutions

00:01:29,700 --> 00:01:35,610
like self-driving cars or intelligent

00:01:32,400 --> 00:01:36,900
assistants like Google home or Alexa or

00:01:35,610 --> 00:01:39,180
what I'm doing at the White House so

00:01:36,900 --> 00:01:40,829
we're trying to predict how cancer cells

00:01:39,180 --> 00:01:43,049
are communicating by using neural

00:01:40,829 --> 00:01:45,049
networks but I want to emphasize with

00:01:43,049 --> 00:01:47,280
this talk that doesn't have to be a

00:01:45,049 --> 00:01:49,439
complex solution could be something is

00:01:47,280 --> 00:01:52,799
simple it can be something that you're

00:01:49,439 --> 00:01:56,399
passionate about or a simple day-to-day

00:01:52,799 --> 00:02:02,520
problem or perhaps like a megatrends and

00:01:56,399 --> 00:02:04,619
globalization or demographical shapes so

00:02:02,520 --> 00:02:07,799
what do you I what's my topic or what

00:02:04,619 --> 00:02:09,450
it's what's my idea it's basically a

00:02:07,799 --> 00:02:11,489
day-to-day problem I'm spending a lot of

00:02:09,450 --> 00:02:12,709
time in front of the mirror putting on

00:02:11,489 --> 00:02:15,950
makeup

00:02:12,709 --> 00:02:20,269
and for inspiration I'm using I'm

00:02:15,950 --> 00:02:22,849
looking at some tutorials on YouTube the

00:02:20,269 --> 00:02:24,709
only problem is that if you type search

00:02:22,849 --> 00:02:27,230
for makeup tutorials you get over 30

00:02:24,709 --> 00:02:30,379
million results I mean it I'm seriously

00:02:27,230 --> 00:02:32,689
it's like a lot of results and this

00:02:30,379 --> 00:02:35,030
combined with beauty being a four

00:02:32,689 --> 00:02:38,780
hundred billion dollar market it shows

00:02:35,030 --> 00:02:42,670
that beauty work and makeup is probably

00:02:38,780 --> 00:02:46,459
a significant part of people's lives so

00:02:42,670 --> 00:02:49,510
my idea is a combination of putting on

00:02:46,459 --> 00:02:53,629
makeup and trying to find the right

00:02:49,510 --> 00:02:56,239
inspirational tutorial for it and how do

00:02:53,629 --> 00:02:59,090
you know which one of these 30 million

00:02:56,239 --> 00:03:04,310
videos that will be suitable for your

00:02:59,090 --> 00:03:08,469
facial features so I thought why not use

00:03:04,310 --> 00:03:10,969
a deep Learning Network for finding out

00:03:08,469 --> 00:03:12,950
so that's ID and the key take away with

00:03:10,969 --> 00:03:14,840
this it's just it doesn't have to be a

00:03:12,950 --> 00:03:19,340
complex problem it can be as simple as

00:03:14,840 --> 00:03:23,510
makeup the second part tell you down the

00:03:19,340 --> 00:03:31,459
most valuable resource creating a data

00:03:23,510 --> 00:03:33,919
set so deep learning deep learning

00:03:31,459 --> 00:03:36,349
methods are are excelling in taking high

00:03:33,919 --> 00:03:43,579
dimensional data like videos and images

00:03:36,349 --> 00:03:45,439
and just oh yeah taking high dimensional

00:03:43,579 --> 00:03:48,340
images and classifying them into

00:03:45,439 --> 00:03:50,299
discrete categories but for this talk

00:03:48,340 --> 00:03:52,669
focusing on a classification problem

00:03:50,299 --> 00:03:55,430
where the neural network is exposed to

00:03:52,669 --> 00:03:56,900
images and how its dividing them into

00:03:55,430 --> 00:03:58,759
different categories it's a

00:03:56,900 --> 00:04:01,400
classification problem I'm focusing on

00:03:58,759 --> 00:04:03,979
right here back to your problem how do i

00:04:01,400 --> 00:04:06,979
how do i build a network that helps me

00:04:03,979 --> 00:04:09,859
find the right video so i looked at a

00:04:06,979 --> 00:04:11,989
lot of videos and started to research

00:04:09,859 --> 00:04:15,109
the facial features available and

00:04:11,989 --> 00:04:18,019
there's a lot of them but the most

00:04:15,109 --> 00:04:20,719
striking one were like the eye the

00:04:18,019 --> 00:04:22,639
region around the eyes it's the most

00:04:20,719 --> 00:04:24,919
time-consuming part and

00:04:22,639 --> 00:04:27,560
there's a lot of videos like focusing on

00:04:24,919 --> 00:04:30,620
that so my ideas could you use some data

00:04:27,560 --> 00:04:34,610
to learn a neural network to understand

00:04:30,620 --> 00:04:37,629
eyes shapes they looked into eyes shapes

00:04:34,610 --> 00:04:41,210
and apparently there's four distinct

00:04:37,629 --> 00:04:44,210
different eye shapes round monolid

00:04:41,210 --> 00:04:46,400
hooded and almond shape there's a lot of

00:04:44,210 --> 00:04:49,330
like very priorities of them but these

00:04:46,400 --> 00:04:53,780
are the four like distinct ones

00:04:49,330 --> 00:04:54,229
how do you create a data set with eyes

00:04:53,780 --> 00:04:58,039
shapes

00:04:54,229 --> 00:05:01,099
I used open dataset with hundreds of

00:04:58,039 --> 00:05:04,129
images of celebrities I manually cropped

00:05:01,099 --> 00:05:06,139
out the eye region and divided it into

00:05:04,129 --> 00:05:07,310
four different categories in this order

00:05:06,139 --> 00:05:11,210
four different eye shapes though it's

00:05:07,310 --> 00:05:13,189
Alma who did monolith and round eyes and

00:05:11,210 --> 00:05:17,029
this is how the data is the data used

00:05:13,189 --> 00:05:19,430
it's a fairly small data set I use 200 a

00:05:17,029 --> 00:05:21,889
cropped 200 images for training for each

00:05:19,430 --> 00:05:25,509
of the categories and then 100 more for

00:05:21,889 --> 00:05:31,240
validation and this is the structure

00:05:25,509 --> 00:05:31,240
it's a fairly small data set start with

00:05:31,539 --> 00:05:38,029
over to the training part which is the

00:05:34,639 --> 00:05:39,770
most exciting the same garnet before

00:05:38,029 --> 00:05:42,500
that just read a show of hands how many

00:05:39,770 --> 00:05:46,669
here works with neural networks or

00:05:42,500 --> 00:05:48,580
convolutional neural networks okay how

00:05:46,669 --> 00:05:55,099
many knows how a convolution operation

00:05:48,580 --> 00:05:56,889
work okay so basically you can use when

00:05:55,099 --> 00:05:59,659
you're when I train a model you can go

00:05:56,889 --> 00:06:01,969
train it from scratch and initialize it

00:05:59,659 --> 00:06:03,919
with random weights or you can go with

00:06:01,969 --> 00:06:05,509
transfer learning which is exactly what

00:06:03,919 --> 00:06:08,560
it sounds like you're transferring

00:06:05,509 --> 00:06:10,789
learning from waste are pre-trained and

00:06:08,560 --> 00:06:13,250
that is what I didn't mind that because

00:06:10,789 --> 00:06:16,129
it was a small data set just 800 images

00:06:13,250 --> 00:06:18,830
I used pre train weights that see in

00:06:16,129 --> 00:06:20,839
over a million other images divided into

00:06:18,830 --> 00:06:28,849
thousand different categories called the

00:06:20,839 --> 00:06:31,580
image net choosing neural architecture

00:06:28,849 --> 00:06:34,339
there's a lot of different models out

00:06:31,580 --> 00:06:35,810
there if you have smaller ones with very

00:06:34,339 --> 00:06:38,419
few operations

00:06:35,810 --> 00:06:43,389
or larger ones but a lot of operations

00:06:38,419 --> 00:06:45,740
the one I use is called vgg 16 and why

00:06:43,389 --> 00:06:47,900
because it's a fairly simple and

00:06:45,740 --> 00:06:49,730
straightforward architectures easy to

00:06:47,900 --> 00:06:51,410
understand easy to play around with it

00:06:49,730 --> 00:06:59,000
and also because I had a pretty small

00:06:51,410 --> 00:07:03,050
data set and this is how the

00:06:59,000 --> 00:07:05,600
architecture looks like for vgg 16 it's

00:07:03,050 --> 00:07:08,210
five blocks each block with a

00:07:05,600 --> 00:07:11,600
convolutional layer and a pooling layer

00:07:08,210 --> 00:07:14,120
and on the top there is a fully

00:07:11,600 --> 00:07:16,460
connected layer which is the one that

00:07:14,120 --> 00:07:20,990
predicts which image which class the

00:07:16,460 --> 00:07:26,210
image belongs to looking through one of

00:07:20,990 --> 00:07:30,560
those so there's the convolutional part

00:07:26,210 --> 00:07:33,880
the block and pooling part so

00:07:30,560 --> 00:07:36,139
essentially every image can be

00:07:33,880 --> 00:07:40,160
represented as a matrix with pixel

00:07:36,139 --> 00:07:42,470
values so consider this image it's four

00:07:40,160 --> 00:07:44,060
it's simply I simplified it a bit

00:07:42,470 --> 00:07:47,150
because it's a color image it's supposed

00:07:44,060 --> 00:07:51,440
to be between 0 and 355 but this it's a

00:07:47,150 --> 00:07:53,450
very simplified example I also consider

00:07:51,440 --> 00:07:55,850
this 2x2 matrix

00:07:53,450 --> 00:07:59,000
it's a filter or our transformational

00:07:55,850 --> 00:08:02,660
matrix and the convolution of the image

00:07:59,000 --> 00:08:05,000
is basically this little matrix sliding

00:08:02,660 --> 00:08:10,570
over the original image and for each

00:08:05,000 --> 00:08:16,220
step and may it does element elemental

00:08:10,570 --> 00:08:17,720
it works yeah element wise

00:08:16,220 --> 00:08:20,330
multiplication between the original

00:08:17,720 --> 00:08:24,620
image and the small little slide sliding

00:08:20,330 --> 00:08:27,139
matrix and for each step you get you get

00:08:24,620 --> 00:08:30,410
an integer and that's the future feature

00:08:27,139 --> 00:08:33,080
map for each image it's a bit abstract

00:08:30,410 --> 00:08:35,900
but I will show you simulator how they

00:08:33,080 --> 00:08:38,930
are all connected and for the pooling

00:08:35,900 --> 00:08:42,650
step you take this feature map and you

00:08:38,930 --> 00:08:45,380
reduce the dimensionality onto until the

00:08:42,650 --> 00:08:47,800
way the most important features it gets

00:08:45,380 --> 00:08:47,800
even smaller

00:08:49,350 --> 00:08:59,170
that's the part that's how the then your

00:08:52,510 --> 00:09:02,140
network died right and since its

00:08:59,170 --> 00:09:08,730
transfer learning I I locked all of the

00:09:02,140 --> 00:09:11,980
layers to really use the pre train waits

00:09:08,730 --> 00:09:15,130
for the best so instead of having this

00:09:11,980 --> 00:09:21,130
random matrix trying to understand the

00:09:15,130 --> 00:09:23,230
images I locked all the layers and I so

00:09:21,130 --> 00:09:26,070
the ones that swept over the original

00:09:23,230 --> 00:09:29,760
image already knew exactly how to

00:09:26,070 --> 00:09:34,029
understand shapes and contrast and

00:09:29,760 --> 00:09:42,970
basically how images looks like and I'll

00:09:34,029 --> 00:09:47,490
show you the layer in the years so

00:09:42,970 --> 00:09:47,490
that's a feature feature map right there

00:09:49,850 --> 00:09:55,459
you see all the deeper layers are with

00:09:53,240 --> 00:09:58,009
higher dimension and then the dimension

00:09:55,459 --> 00:10:00,740
is getting lower than the word for the

00:09:58,009 --> 00:10:02,959
last words basically just some pick up

00:10:00,740 --> 00:10:09,529
pixels you see some of them are pretty

00:10:02,959 --> 00:10:12,110
scary like this one over here yeah and

00:10:09,529 --> 00:10:19,970
this is basically what's the last fully

00:10:12,110 --> 00:10:23,899
connected layer C's and this is the

00:10:19,970 --> 00:10:26,209
funnel and you see the higher dimension

00:10:23,899 --> 00:10:29,180
and how it gets reduced for each step

00:10:26,209 --> 00:10:36,130
until it it classifies the image into

00:10:29,180 --> 00:10:39,889
four different categories okay

00:10:36,130 --> 00:10:43,910
used among linear activation for the

00:10:39,889 --> 00:10:46,519
layers that are locked and la linear

00:10:43,910 --> 00:10:51,829
activation for the fully connected layer

00:10:46,519 --> 00:10:53,930
and just to recap I use pre trained

00:10:51,829 --> 00:10:57,380
weights I use the fairly simple

00:10:53,930 --> 00:10:59,209
architecture Calvi GG 16 I locked all

00:10:57,380 --> 00:11:03,199
the convolutional and the pooling layers

00:10:59,209 --> 00:11:07,459
I use a dropper rate to get the model to

00:11:03,199 --> 00:11:09,589
generalize better and the softmax for it

00:11:07,459 --> 00:11:11,959
it's a fully connected layer and then I

00:11:09,589 --> 00:11:14,180
run it for 50 bucks sixteen images at a

00:11:11,959 --> 00:11:20,060
time until the model had seen all of the

00:11:14,180 --> 00:11:22,189
images and it's this print it maybe it

00:11:20,060 --> 00:11:26,000
sounds a bit complex but really this is

00:11:22,189 --> 00:11:28,550
all code that's needed to really do

00:11:26,000 --> 00:11:31,550
these operations so if you write this

00:11:28,550 --> 00:11:37,399
like 15 lines of code you initialize the

00:11:31,550 --> 00:11:39,410
model you build this little top lock the

00:11:37,399 --> 00:11:42,290
layer that you want to use that you

00:11:39,410 --> 00:11:43,790
already pre-trained compile it and save

00:11:42,290 --> 00:11:47,180
the model that's everything that's

00:11:43,790 --> 00:11:49,009
needed so back to this little

00:11:47,180 --> 00:11:51,470
classification problem

00:11:49,009 --> 00:11:54,970
what kind of accuracy do you think that

00:11:51,470 --> 00:12:00,160
my net received for the first time

00:11:54,970 --> 00:12:00,160
between zero and 100% what do you think

00:12:01,350 --> 00:12:07,740
and if I you think 50 all right no else

00:12:06,440 --> 00:12:11,279
okay

00:12:07,740 --> 00:12:12,839
80% okay it's a fairly small dataset but

00:12:11,279 --> 00:12:16,079
it actually got between nineteen ninety

00:12:12,839 --> 00:12:18,060
three percent for the first run so it

00:12:16,079 --> 00:12:19,829
was pretty exciting because I just

00:12:18,060 --> 00:12:21,569
cropped this to 108 my dress and put her

00:12:19,829 --> 00:12:27,420
mo stitch them all together and and I

00:12:21,569 --> 00:12:29,940
got this result so yeah that's the

00:12:27,420 --> 00:12:32,370
training part and because of their

00:12:29,940 --> 00:12:34,290
courtesy because it was fairly high I

00:12:32,370 --> 00:12:35,880
got really excited and I really wanted

00:12:34,290 --> 00:12:39,360
to try to take it from the idea face

00:12:35,880 --> 00:12:44,009
into a real product so I built a small

00:12:39,360 --> 00:12:47,850
app and I'll show it super simple front

00:12:44,009 --> 00:12:50,430
end using angular and bootstrap and the

00:12:47,850 --> 00:12:52,949
backend is basically just the model so

00:12:50,430 --> 00:12:56,639
this is all the Dakota it's needed like

00:12:52,949 --> 00:12:58,949
50 lines of code the base model the top

00:12:56,639 --> 00:13:02,339
model and then just the pre-trained

00:12:58,949 --> 00:13:04,319
weights so everything all of this into

00:13:02,339 --> 00:13:08,430
this few lines of code

00:13:04,319 --> 00:13:10,649
and for the backend I use the title web

00:13:08,430 --> 00:13:12,209
server framework flask and it's

00:13:10,649 --> 00:13:14,790
basically just taking an image from the

00:13:12,209 --> 00:13:17,010
front end and predict which one of the

00:13:14,790 --> 00:13:21,690
four classes it is and then returns the

00:13:17,010 --> 00:13:24,510
class so from time that takes a picture

00:13:21,690 --> 00:13:26,970
of throwing it to the back ends back end

00:13:24,510 --> 00:13:34,319
that predicts what kind of class it is

00:13:26,970 --> 00:13:38,069
and it shows you the eye shape so in

00:13:34,319 --> 00:13:39,750
conclusion and I love AI and I throw

00:13:38,069 --> 00:13:42,690
deep learning on everything I could

00:13:39,750 --> 00:13:44,819
possibly find I mean makeup but the

00:13:42,690 --> 00:13:46,800
thing I love even more is converting

00:13:44,819 --> 00:13:48,569
ideas into products and I think that's a

00:13:46,800 --> 00:13:50,760
very important part of learning by

00:13:48,569 --> 00:13:53,250
creating something end to end so I

00:13:50,760 --> 00:13:55,889
really encourage you to take even simple

00:13:53,250 --> 00:13:58,199
problem like this like finding what can

00:13:55,889 --> 00:14:03,209
make suits you and trying to make a

00:13:58,199 --> 00:14:06,439
product out of it I'll show you real

00:14:03,209 --> 00:14:06,439
quick them all

00:14:07,500 --> 00:14:14,660
so this is how the app looks like go for

00:14:12,090 --> 00:14:14,660
a shave

00:14:33,850 --> 00:14:47,590
and the model predicted my thanks I'll

00:14:45,820 --> 00:14:49,300
stick around here is the repository if

00:14:47,590 --> 00:14:51,550
you want to train your own model just

00:14:49,300 --> 00:14:52,360
add all the images and all the code is

00:14:51,550 --> 00:14:53,800
all there is there

00:14:52,360 --> 00:14:56,050
it's pre training everything you just

00:14:53,800 --> 00:15:07,810
put a path to your images and it will

00:14:56,050 --> 00:15:09,310
tell you which classes thank you just

00:15:07,810 --> 00:15:11,350
making the microphone work Thank You

00:15:09,310 --> 00:15:17,380
Norah and a little token of our

00:15:11,350 --> 00:15:18,940
appreciation traditional nor is asked to

00:15:17,380 --> 00:15:21,870
just have questions at the front

00:15:18,940 --> 00:15:29,649
afterwards so thank you again Norah

00:15:21,870 --> 00:15:29,649

YouTube URL: https://www.youtube.com/watch?v=ymRr0AkMb5E


