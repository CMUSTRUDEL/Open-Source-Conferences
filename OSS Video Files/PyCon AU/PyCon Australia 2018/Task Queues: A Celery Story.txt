Title: Task Queues: A Celery Story
Publication date: 2018-08-26
Playlist: PyCon Australia 2018
Description: 
	Tom Manderson

https://2018.pycon-au.org/talks/45392-task-queues-a-celery-story/

Python has a surprisingly large number of task queue libraries, but Celery reigns supreme. Unfortunately, there are a few use cases where it's remarkably bad. Learn about why you might want a task queue (and when you definitely don't), when Celery is appropriate, and what you can do when it's not.

Python, PyCon, PyConAU, australia, programming, sydney

This video is licensed under CC BY 3.0 AU - https://creativecommons.org/licenses/by/3.0/au/

PyCon Australia (“PyCon AU”) is the national conference for the Python Programming Community, bringing together professional, student and enthusiast developers with a love for developing with Python.

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 24 - 28 2018.

Python, PyCon, PyConAU
Captions: 
	00:00:00,000 --> 00:00:02,939
first week of this afternoon is Tom

00:00:01,650 --> 00:00:04,740
medicine he'll be speaking to us about

00:00:02,939 --> 00:00:06,330
task queues particularly salary and

00:00:04,740 --> 00:00:13,620
where and when you may need to use them

00:00:06,330 --> 00:00:14,160
please make him feel welcome hey

00:00:13,620 --> 00:00:16,680
everybody

00:00:14,160 --> 00:00:19,500
as Mitch said this talk is titled toss

00:00:16,680 --> 00:00:21,080
you so celery story we're gonna start

00:00:19,500 --> 00:00:24,119
with just a little bit about Who I am

00:00:21,080 --> 00:00:25,890
I'm a professional Python developer at a

00:00:24,119 --> 00:00:28,410
company called polymath you actually

00:00:25,890 --> 00:00:31,019
work with Mitch I sit opposite him so

00:00:28,410 --> 00:00:32,700
thanks for the interim edge so polymath

00:00:31,019 --> 00:00:35,570
you in we call ourselves a bespoke

00:00:32,700 --> 00:00:38,010
hosted mathematical optimization company

00:00:35,570 --> 00:00:40,050
basically what that means is we build

00:00:38,010 --> 00:00:43,410
tools that help big industry usually

00:00:40,050 --> 00:00:45,870
mining make better decisions and save

00:00:43,410 --> 00:00:47,520
them or make them money I've been there

00:00:45,870 --> 00:00:50,129
for two and three-quarters eight years

00:00:47,520 --> 00:00:51,449
as of now I'm also a fifty-year software

00:00:50,129 --> 00:00:54,079
engineering student at the University of

00:00:51,449 --> 00:00:56,219
Queensland and I love Haskell but

00:00:54,079 --> 00:00:58,890
unfortunately no Haskell today

00:00:56,219 --> 00:00:59,850
Python is pretty great too although all

00:00:58,890 --> 00:01:02,219
the details are on there there's the

00:00:59,850 --> 00:01:05,040
plugin math um logo alright let's get

00:01:02,219 --> 00:01:06,150
into it so this talk is going to be

00:01:05,040 --> 00:01:08,670
mainly in two parts

00:01:06,150 --> 00:01:10,799
first of all an overview of task queues

00:01:08,670 --> 00:01:12,600
themselves and second of all some

00:01:10,799 --> 00:01:15,000
specific tasks you libraries for Python

00:01:12,600 --> 00:01:17,549
as well as how we've used celery and

00:01:15,000 --> 00:01:20,700
other libraries apply matthean and a few

00:01:17,549 --> 00:01:22,140
terrible jokes so yeah let's just dive

00:01:20,700 --> 00:01:25,890
right on in starting with tasks use more

00:01:22,140 --> 00:01:30,810
generally what is a task queue task

00:01:25,890 --> 00:01:32,250
queues are queues of tasks crazy what

00:01:30,810 --> 00:01:33,869
that actually means is there's a queue

00:01:32,250 --> 00:01:36,210
of messages where each message

00:01:33,869 --> 00:01:38,909
represents a unit of work or a task and

00:01:36,210 --> 00:01:40,729
then these messages are distributed by a

00:01:38,909 --> 00:01:42,600
message queue which we call the burka

00:01:40,729 --> 00:01:45,119
each of these are distributed to

00:01:42,600 --> 00:01:47,759
different processes and those processes

00:01:45,119 --> 00:01:50,369
are called workers that actually do the

00:01:47,759 --> 00:01:52,619
work that the task describes these are

00:01:50,369 --> 00:01:55,049
processed asynchronously although you

00:01:52,619 --> 00:01:56,880
can when you start your task block

00:01:55,049 --> 00:01:59,250
waiting for it to finish that little

00:01:56,880 --> 00:02:03,210
diagram there is just stealing AWS

00:01:59,250 --> 00:02:05,430
infrastructure diagrams so we've got web

00:02:03,210 --> 00:02:07,619
servers which are ec2 instances then

00:02:05,430 --> 00:02:10,020
we've got a message queue that's Amazon

00:02:07,619 --> 00:02:12,150
sqs in this context and that sends

00:02:10,020 --> 00:02:13,349
messages onto our workers that do the

00:02:12,150 --> 00:02:14,609
processing

00:02:13,349 --> 00:02:19,319
it's generally pretty simple stuff

00:02:14,609 --> 00:02:21,269
relatively easy to understand so when

00:02:19,319 --> 00:02:23,129
might I want to use a task queue so

00:02:21,269 --> 00:02:24,689
there are two fundamental requirements

00:02:23,129 --> 00:02:26,519
when you're using task queues first of

00:02:24,689 --> 00:02:29,129
all is the work you're doing can be run

00:02:26,519 --> 00:02:30,810
asynchronously and second of all you're

00:02:29,129 --> 00:02:32,849
in a context where distributed systems

00:02:30,810 --> 00:02:34,530
make sense because you've got to go out

00:02:32,849 --> 00:02:37,049
to your broker and back in so they're

00:02:34,530 --> 00:02:38,939
all separate interacting systems and

00:02:37,049 --> 00:02:40,709
what you get from this situation when

00:02:38,939 --> 00:02:43,290
you use a task queue you get a degree of

00:02:40,709 --> 00:02:44,969
architectural flexibility and the main

00:02:43,290 --> 00:02:46,379
benefit that I find with that is it

00:02:44,969 --> 00:02:48,659
allows you to make your architecture

00:02:46,379 --> 00:02:51,389
asymmetric what I mean by that is your

00:02:48,659 --> 00:02:53,790
workers can be totally different to

00:02:51,389 --> 00:02:56,819
whatever is sending the request for TAS

00:02:53,790 --> 00:02:59,400
to be run because we're in Python we get

00:02:56,819 --> 00:03:01,079
extra stuff which is actually an

00:02:59,400 --> 00:03:03,659
artifact of us having less stuff in the

00:03:01,079 --> 00:03:05,310
beginning task queues are really useful

00:03:03,659 --> 00:03:07,769
in Python for a thing called process but

00:03:05,310 --> 00:03:10,859
process based parallelism which I'll

00:03:07,769 --> 00:03:13,709
talk a bit on further so basically when

00:03:10,859 --> 00:03:16,169
you have a sink and a distributed system

00:03:13,709 --> 00:03:18,209
you get some architectural flexibility

00:03:16,169 --> 00:03:19,829
and you can see that that particular

00:03:18,209 --> 00:03:22,069
architectural flexibility is also

00:03:19,829 --> 00:03:22,069
asymmetric

00:03:22,759 --> 00:03:27,479
oh let's skip the slide yeah so a

00:03:25,709 --> 00:03:28,889
process based parallelism

00:03:27,479 --> 00:03:30,449
we're in Python land and we've got a

00:03:28,889 --> 00:03:32,639
thing called the global interpreter lock

00:03:30,449 --> 00:03:37,139
I call it the Gil some people call it

00:03:32,639 --> 00:03:38,849
the Jill it's like gif and jiff in C

00:03:37,139 --> 00:03:40,859
Python which is the normal Python

00:03:38,849 --> 00:03:42,870
interpreter memory management isn't

00:03:40,859 --> 00:03:45,120
thread safe so that means that when you

00:03:42,870 --> 00:03:47,009
change stuff sorry when when memory is

00:03:45,120 --> 00:03:50,430
released that if another threads using

00:03:47,009 --> 00:03:53,040
it there's no real guarantees but we get

00:03:50,430 --> 00:03:54,329
those guarantees by adding locking with

00:03:53,040 --> 00:03:56,340
the global interpreter lock and that

00:03:54,329 --> 00:03:59,729
says that only one thread can execute

00:03:56,340 --> 00:04:01,620
passing code at once and that means that

00:03:59,729 --> 00:04:03,810
if you have high intensity high

00:04:01,620 --> 00:04:06,269
processing stuff written in your Python

00:04:03,810 --> 00:04:08,790
because only one part of that can run at

00:04:06,269 --> 00:04:10,909
a time using threading to do things that

00:04:08,790 --> 00:04:13,769
have high computation in Python is

00:04:10,909 --> 00:04:17,190
pretty rubbish and it's all run at once

00:04:13,769 --> 00:04:18,959
anyway so a lot less useful when you use

00:04:17,190 --> 00:04:20,969
task queues you have your workers in

00:04:18,959 --> 00:04:23,310
separate processes which means each

00:04:20,969 --> 00:04:25,440
separate process has its own tilt which

00:04:23,310 --> 00:04:27,090
means that each process has one thread

00:04:25,440 --> 00:04:28,530
of execution in parallel

00:04:27,090 --> 00:04:30,270
and so if you're able to distribute the

00:04:28,530 --> 00:04:31,919
work you're doing across these workers

00:04:30,270 --> 00:04:33,419
then you still get all the parallelism

00:04:31,919 --> 00:04:35,190
benefits it's just running in separate

00:04:33,419 --> 00:04:38,820
processes tasks use happen to be a

00:04:35,190 --> 00:04:40,440
pretty nice way to coordinate this so

00:04:38,820 --> 00:04:42,060
just from some linger at ask you to

00:04:40,440 --> 00:04:44,010
provide message passing concurrency

00:04:42,060 --> 00:04:45,180
because you're passing messages which is

00:04:44,010 --> 00:04:46,889
an alternative to shared memory

00:04:45,180 --> 00:04:48,750
concurrency which you get when you're

00:04:46,889 --> 00:04:52,979
using threads in other languages like C

00:04:48,750 --> 00:04:54,990
and C++ so yeah just some common use

00:04:52,979 --> 00:04:56,550
cases for tasks use user signs up to

00:04:54,990 --> 00:04:59,639
your system you want to send them a

00:04:56,550 --> 00:05:00,960
registration or a validation email it

00:04:59,639 --> 00:05:02,520
doesn't really make sense for them to

00:05:00,960 --> 00:05:04,620
wait on the web request to complete

00:05:02,520 --> 00:05:06,570
while you send them an email so instead

00:05:04,620 --> 00:05:08,610
you could spit up a task and send that

00:05:06,570 --> 00:05:10,229
to a worker because you don't have to

00:05:08,610 --> 00:05:12,600
wait for that to complete it just has to

00:05:10,229 --> 00:05:14,160
be done eventually other ones generating

00:05:12,600 --> 00:05:15,270
image thumbnails you don't have to

00:05:14,160 --> 00:05:16,530
immediately generate the thumbnail

00:05:15,270 --> 00:05:17,760
people could download the full image

00:05:16,530 --> 00:05:20,460
until you've got a good something

00:05:17,760 --> 00:05:22,320
already if you're updating some complex

00:05:20,460 --> 00:05:24,630
and difficult to compute cached value

00:05:22,320 --> 00:05:26,280
you can just update the cache over on

00:05:24,630 --> 00:05:29,100
the side and keep using the old one in

00:05:26,280 --> 00:05:30,930
most contexts in most contexts one use

00:05:29,100 --> 00:05:32,850
case we use is we generate reports so

00:05:30,930 --> 00:05:35,850
Excel spreadsheets for stuff to download

00:05:32,850 --> 00:05:37,289
and then data parallel workloads so

00:05:35,850 --> 00:05:39,660
workloads where you do the same bit of

00:05:37,289 --> 00:05:41,070
work on a huge amount of data you just

00:05:39,660 --> 00:05:42,300
chunk up the work and send it to

00:05:41,070 --> 00:05:43,620
different workers that's certain to

00:05:42,300 --> 00:05:47,940
process based parallelism I was

00:05:43,620 --> 00:05:50,160
mentioning there are some drawbacks so

00:05:47,940 --> 00:05:51,780
starting a task is a lot harder than a

00:05:50,160 --> 00:05:53,940
normal function call because you've got

00:05:51,780 --> 00:05:55,200
to write your message into a stream and

00:05:53,940 --> 00:05:56,849
then you're going to send that over the

00:05:55,200 --> 00:05:58,320
network and then you've got to pick that

00:05:56,849 --> 00:05:59,940
up on the worker and then you've got to

00:05:58,320 --> 00:06:01,320
deserialize it and then you've got to do

00:05:59,940 --> 00:06:03,210
the work and then you've got to say hey

00:06:01,320 --> 00:06:04,950
I'm done basically there's a whole lot

00:06:03,210 --> 00:06:06,539
of reporting that adds some overhead so

00:06:04,950 --> 00:06:08,130
if you've got these tiny tasks that you

00:06:06,539 --> 00:06:12,410
could just do a little in a web request

00:06:08,130 --> 00:06:14,789
please don't do them with a task queue

00:06:12,410 --> 00:06:16,680
second of all is state tracking if

00:06:14,789 --> 00:06:19,590
you've got a lot of incremental progress

00:06:16,680 --> 00:06:22,950
going on task queues aren't always the

00:06:19,590 --> 00:06:26,130
best approach usually you can tell when

00:06:22,950 --> 00:06:27,660
a task is done or started and that's

00:06:26,130 --> 00:06:29,160
about it and you'd have to implement

00:06:27,660 --> 00:06:30,660
your own state tracking layer of the top

00:06:29,160 --> 00:06:33,090
realistically what you want in this

00:06:30,660 --> 00:06:34,770
situation is to write your own worker

00:06:33,090 --> 00:06:37,680
system and just use a database to track

00:06:34,770 --> 00:06:39,700
the state task user for sending a quick

00:06:37,680 --> 00:06:42,580
message picking up the message and then

00:06:39,700 --> 00:06:43,900
a message back if you use the database

00:06:42,580 --> 00:06:45,460
you get to write your state and you

00:06:43,900 --> 00:06:48,490
write any extra information because you

00:06:45,460 --> 00:06:50,290
want it to stay there yeah and then in

00:06:48,490 --> 00:06:52,240
passing it's hard to serialize and

00:06:50,290 --> 00:06:55,030
deserialize some objects for example

00:06:52,240 --> 00:06:56,440
functions so there are some use cases

00:06:55,030 --> 00:07:00,790
where you'd want to use a tie to you

00:06:56,440 --> 00:07:02,860
where you can't so basically in short

00:07:00,790 --> 00:07:05,320
task queues are great but they're not

00:07:02,860 --> 00:07:07,330
magical secret scale source they provide

00:07:05,320 --> 00:07:09,730
you architectural flexibility but you

00:07:07,330 --> 00:07:11,620
still have to remember that there are

00:07:09,730 --> 00:07:15,070
drawbacks and you need to know what

00:07:11,620 --> 00:07:18,520
you're doing all right

00:07:15,070 --> 00:07:22,660
enter celery celery like I say on the

00:07:18,520 --> 00:07:24,790
slide is the standard in toss cues yeah

00:07:22,660 --> 00:07:27,160
it's really simple for a simple use case

00:07:24,790 --> 00:07:29,020
you literally just write celery workout

00:07:27,160 --> 00:07:32,590
and then suddenly have a worker and then

00:07:29,020 --> 00:07:34,840
on the other end you go task delay and

00:07:32,590 --> 00:07:38,680
done I'll show you the code sample in a

00:07:34,840 --> 00:07:40,090
second it has a lot of features and as

00:07:38,680 --> 00:07:43,060
soon as you start using those features

00:07:40,090 --> 00:07:45,610
it really really blows up in complexity

00:07:43,060 --> 00:07:47,350
it's insanely powerful there's so much

00:07:45,610 --> 00:07:51,790
you can do there's like complex message

00:07:47,350 --> 00:07:52,960
routing features toss chaining basically

00:07:51,790 --> 00:07:54,250
everything you could ever imagine and

00:07:52,960 --> 00:07:56,080
support for a bunch of different message

00:07:54,250 --> 00:07:57,700
brokers support for storing results on

00:07:56,080 --> 00:07:59,890
different backends there's a lot of

00:07:57,700 --> 00:08:02,020
features it's really cool and really

00:07:59,890 --> 00:08:03,730
powerful which is part of why it is D

00:08:02,020 --> 00:08:06,250
standard and all of these features are

00:08:03,730 --> 00:08:09,580
really robustly tested which is pretty

00:08:06,250 --> 00:08:11,410
great so here's a code example I'll

00:08:09,580 --> 00:08:13,690
actually be doing a few code examples

00:08:11,410 --> 00:08:15,040
throughout this presentation this is

00:08:13,690 --> 00:08:17,620
what all of them are going to look like

00:08:15,040 --> 00:08:19,660
so on the left hand side we have the

00:08:17,620 --> 00:08:21,580
shared code so this is how you define

00:08:19,660 --> 00:08:23,920
this is a task that I want to be able to

00:08:21,580 --> 00:08:24,850
run with my task queue and then on the

00:08:23,920 --> 00:08:27,730
right hand side we've got a client

00:08:24,850 --> 00:08:30,160
function and a worker function so in the

00:08:27,730 --> 00:08:32,680
client you do a delay one and two which

00:08:30,160 --> 00:08:35,200
says I want my task to task queue to

00:08:32,680 --> 00:08:38,410
compute one plus two and get the result

00:08:35,200 --> 00:08:40,540
back in celery result don't get is

00:08:38,410 --> 00:08:41,800
blocking so it sits there and waits for

00:08:40,540 --> 00:08:45,040
your task to complete and then it gives

00:08:41,800 --> 00:08:47,560
you the result adding two numbers is a

00:08:45,040 --> 00:08:48,010
horrible way to use a task you don't do

00:08:47,560 --> 00:08:49,480
this

00:08:48,010 --> 00:08:51,880
this is a trivial example for

00:08:49,480 --> 00:08:53,020
demonstration purposes but yeah so all

00:08:51,880 --> 00:08:55,140
of my code examples are going to look

00:08:53,020 --> 00:08:55,140
like

00:08:55,750 --> 00:09:00,100
so cutting in you've seen tiny bit of

00:08:58,329 --> 00:09:03,699
celery now I'm gonna talk about polymath

00:09:00,100 --> 00:09:04,870
Ian and how we used celery so like I

00:09:03,699 --> 00:09:06,130
said we call ourselves an industrial

00:09:04,870 --> 00:09:07,630
mathematics company we build

00:09:06,130 --> 00:09:11,290
optimization tools and the way we use

00:09:07,630 --> 00:09:13,209
these is we build web apps these web

00:09:11,290 --> 00:09:16,029
apps basically all have a very common

00:09:13,209 --> 00:09:17,680
workflow of input your data configure

00:09:16,029 --> 00:09:19,300
some settings click the Run button and

00:09:17,680 --> 00:09:22,360
it solves a really complex optimization

00:09:19,300 --> 00:09:24,069
and then you get data visualization and

00:09:22,360 --> 00:09:25,660
you get some reports to export and you

00:09:24,069 --> 00:09:27,310
can do stuff with those reports

00:09:25,660 --> 00:09:30,610
this isn't how all that stuff works but

00:09:27,310 --> 00:09:32,529
pretty much 90% of it does this all of

00:09:30,610 --> 00:09:33,970
our software is deployed on AWS and we

00:09:32,529 --> 00:09:35,769
kind of have this implicit goal of

00:09:33,970 --> 00:09:37,660
making use of AWS software wherever

00:09:35,769 --> 00:09:39,670
possible although we do have some

00:09:37,660 --> 00:09:43,240
on-premise deployments so it still has

00:09:39,670 --> 00:09:45,220
to be flexible and then what actually

00:09:43,240 --> 00:09:46,839
makes us the big bucks is we have these

00:09:45,220 --> 00:09:48,459
really powerful optimization engines

00:09:46,839 --> 00:09:50,430
that solve hard problems for people and

00:09:48,459 --> 00:09:53,139
because they're hard problems to solve

00:09:50,430 --> 00:09:57,639
they take a lot of memory and a lot of

00:09:53,139 --> 00:10:01,300
CPU and so that basically means that

00:09:57,639 --> 00:10:03,459
task queues are awesome whoops bad

00:10:01,300 --> 00:10:05,769
animations yeah so we've got these high

00:10:03,459 --> 00:10:07,240
memory and high CPU requirements which

00:10:05,769 --> 00:10:08,620
means that we've got different resource

00:10:07,240 --> 00:10:10,269
constraints between the web server and

00:10:08,620 --> 00:10:13,240
the workers perfect for asymmetric

00:10:10,269 --> 00:10:15,069
architecture these optimizations take a

00:10:13,240 --> 00:10:16,779
while to run and so they definitely are

00:10:15,069 --> 00:10:18,069
too long for a web request and we can

00:10:16,779 --> 00:10:21,880
definitely wait for them to complete a

00:10:18,069 --> 00:10:24,490
synchronously perfect for tasks use this

00:10:21,880 --> 00:10:26,470
workflow with import optimized export

00:10:24,490 --> 00:10:28,810
it's inherently asynchronous again

00:10:26,470 --> 00:10:30,430
perfect for task queues and we actually

00:10:28,810 --> 00:10:32,050
just add nifty little extra is we have a

00:10:30,430 --> 00:10:33,790
fixed cost support model for all of our

00:10:32,050 --> 00:10:35,139
clients which means that we can just

00:10:33,790 --> 00:10:36,850
have the service sitting there and if

00:10:35,139 --> 00:10:38,380
they try and do too much stuff it's

00:10:36,850 --> 00:10:39,839
perfectly fine for tasks to sit in the

00:10:38,380 --> 00:10:42,810
queue while they wait to be processed

00:10:39,839 --> 00:10:45,220
which is awesome for us save this money

00:10:42,810 --> 00:10:50,500
so yeah basically task queues are

00:10:45,220 --> 00:10:52,720
awesome for us yeah so as I said earlier

00:10:50,500 --> 00:10:54,490
celery is D standard which is why we

00:10:52,720 --> 00:10:57,430
initially chose it for our task queuing

00:10:54,490 --> 00:10:58,870
implementation it really well supports

00:10:57,430 --> 00:11:02,230
the use case of a fixed pool of worker

00:10:58,870 --> 00:11:03,819
machines but yet what we would do is we

00:11:02,230 --> 00:11:06,040
would have a bunch of machines we would

00:11:03,819 --> 00:11:06,580
run a fixed pool of workers on our fixed

00:11:06,040 --> 00:11:09,610
pool of

00:11:06,580 --> 00:11:10,899
and then we would report back logging

00:11:09,610 --> 00:11:13,360
messages as we were going and we would

00:11:10,899 --> 00:11:14,860
save those in the database so in terms

00:11:13,360 --> 00:11:16,600
of actually coordinating the tasks

00:11:14,860 --> 00:11:18,279
salary gave us enough and we didn't need

00:11:16,600 --> 00:11:20,350
any particularly complex their tracking

00:11:18,279 --> 00:11:23,380
on top of that but we did want to report

00:11:20,350 --> 00:11:25,470
back progress messages so we used us we

00:11:23,380 --> 00:11:29,019
use the database to write log messages

00:11:25,470 --> 00:11:32,980
but unfortunately celery broke our

00:11:29,019 --> 00:11:35,170
hearts like I said celery has a lot of

00:11:32,980 --> 00:11:37,959
really complex and rich options that are

00:11:35,170 --> 00:11:39,810
incredibly powerful but unfortunately

00:11:37,959 --> 00:11:43,149
that means that there's an incredibly

00:11:39,810 --> 00:11:45,610
large set of configuration options and

00:11:43,149 --> 00:11:47,560
to make it worse there are four

00:11:45,610 --> 00:11:50,950
different ways to configure your celery

00:11:47,560 --> 00:11:53,140
setup and it's impossible to diagnose

00:11:50,950 --> 00:11:55,360
miss configuration because those four

00:11:53,140 --> 00:11:57,190
different ways all have a lot of the

00:11:55,360 --> 00:11:59,860
same settings and if you've got settings

00:11:57,190 --> 00:12:01,420
interact with each other there's no way

00:11:59,860 --> 00:12:02,920
to tell how they interact and so you get

00:12:01,420 --> 00:12:06,339
these weird dependencies between

00:12:02,920 --> 00:12:08,079
different celery settings basically if

00:12:06,339 --> 00:12:09,790
you google celery horror stories on the

00:12:08,079 --> 00:12:12,370
internet everyone will talk about oh

00:12:09,790 --> 00:12:13,870
wait we miss configured celery

00:12:12,370 --> 00:12:15,850
unfortunately I miss Brianna's talk

00:12:13,870 --> 00:12:18,329
yesterday but apparently they even had

00:12:15,850 --> 00:12:21,670
issues with celery configuration so

00:12:18,329 --> 00:12:23,350
pretty common problem what made it worse

00:12:21,670 --> 00:12:25,560
was that the defaults and celery were

00:12:23,350 --> 00:12:28,180
not at all configured for our use case

00:12:25,560 --> 00:12:29,529
salaries designed for processing short

00:12:28,180 --> 00:12:31,480
tasks that yeah they're a bit longer

00:12:29,529 --> 00:12:33,490
than a web request but they're really

00:12:31,480 --> 00:12:37,810
quick and because of that it's got a lot

00:12:33,490 --> 00:12:39,640
of prefetching defaults set up so it'll

00:12:37,810 --> 00:12:40,839
grab a bunch of tasks process them and

00:12:39,640 --> 00:12:42,610
then once it's done all those it'll grab

00:12:40,839 --> 00:12:44,079
some more tasks and this is to reduce

00:12:42,610 --> 00:12:45,550
Network communication and makes great

00:12:44,079 --> 00:12:50,050
sense for the use case where you've got

00:12:45,550 --> 00:12:51,880
a lot of short running tasks but given

00:12:50,050 --> 00:12:55,420
we've got solves that run between one

00:12:51,880 --> 00:12:57,130
minute and one day it kind of didn't

00:12:55,420 --> 00:12:59,020
really work in our favor because I we

00:12:57,130 --> 00:13:00,579
prefetch tasks on the same worker and

00:12:59,020 --> 00:13:02,140
then they would never get run because we

00:13:00,579 --> 00:13:04,390
were running a day long solve with

00:13:02,140 --> 00:13:07,329
another thing pre fetched and all this

00:13:04,390 --> 00:13:08,890
spare capacity and it actually took a

00:13:07,329 --> 00:13:10,390
surprisingly long amount of time for all

00:13:08,890 --> 00:13:12,399
of these issues to become clear as we

00:13:10,390 --> 00:13:14,860
got bigger people were using the workers

00:13:12,399 --> 00:13:16,149
more frequently and so if you had two

00:13:14,860 --> 00:13:18,040
things queue at the same time your

00:13:16,149 --> 00:13:19,900
worker would pick up someone else's

00:13:18,040 --> 00:13:22,690
tasks and it would sit there and

00:13:19,900 --> 00:13:25,870
get executed but once we reach this

00:13:22,690 --> 00:13:27,820
point we kind of were screwed we had to

00:13:25,870 --> 00:13:30,820
do something and celery wasn't going to

00:13:27,820 --> 00:13:32,830
cut it so we ended up oh yeah so just

00:13:30,820 --> 00:13:34,570
for your benefit in order for our setup

00:13:32,830 --> 00:13:35,800
to work we had to do three separate

00:13:34,570 --> 00:13:38,410
configuration options in two places

00:13:35,800 --> 00:13:39,760
these are the settings this is what's

00:13:38,410 --> 00:13:41,560
enabled this is what you have to do to

00:13:39,760 --> 00:13:44,920
disable all prefetching and salary' I

00:13:41,560 --> 00:13:46,780
will post my slides on slack but

00:13:44,920 --> 00:13:48,550
basically the top two can be configured

00:13:46,780 --> 00:13:49,960
with three of the four configuration

00:13:48,550 --> 00:13:51,370
options and the bottom one has to be

00:13:49,960 --> 00:13:54,820
configured with the fourth and not the

00:13:51,370 --> 00:13:56,620
first three yeah weird weird

00:13:54,820 --> 00:13:58,180
dependencies and also if you have the

00:13:56,620 --> 00:14:01,000
first one enabled it changes how the

00:13:58,180 --> 00:14:03,010
second one behaves weird dependency it's

00:14:01,000 --> 00:14:05,860
like I said anyway yeah we looked at a

00:14:03,010 --> 00:14:06,460
bunch of other Tasca installations crazy

00:14:05,860 --> 00:14:08,200
enough

00:14:06,460 --> 00:14:10,300
rolling your own is actually pretty easy

00:14:08,200 --> 00:14:11,920
if you've got a good message coming

00:14:10,300 --> 00:14:14,130
library you can write a simple enough

00:14:11,920 --> 00:14:17,200
task you and about a hundred lines

00:14:14,130 --> 00:14:20,340
there's a flask snippet that does it in

00:14:17,200 --> 00:14:22,180
60 so task queues aren't that complex

00:14:20,340 --> 00:14:24,220
until you have to add a bunch of extra

00:14:22,180 --> 00:14:25,780
features which everyone does so the ones

00:14:24,220 --> 00:14:28,120
we looked at I listed there I'll be

00:14:25,780 --> 00:14:31,720
going through them one by one first up

00:14:28,120 --> 00:14:33,700
ah Q stands for read askew it's really

00:14:31,720 --> 00:14:35,260
really simple like dead simple super

00:14:33,700 --> 00:14:36,970
easy to understand the code is really

00:14:35,260 --> 00:14:39,340
easy to read it's really well documented

00:14:36,970 --> 00:14:41,680
and the execution models super simple

00:14:39,340 --> 00:14:43,690
you can actually look at Redis and

00:14:41,680 --> 00:14:46,170
understand what's going on internally

00:14:43,690 --> 00:14:48,550
because the structure is that simple

00:14:46,170 --> 00:14:53,680
which makes it really easy to get high

00:14:48,550 --> 00:14:56,020
quality monitoring so over on the slides

00:14:53,680 --> 00:14:57,520
you can see task definition we don't

00:14:56,020 --> 00:15:00,550
need the task decorator thing like

00:14:57,520 --> 00:15:03,040
celery had it had at celery dot task and

00:15:00,550 --> 00:15:07,930
you come up with an rqq and you add your

00:15:03,040 --> 00:15:11,250
task to the queue I think that codes

00:15:07,930 --> 00:15:13,920
wrong that's bad

00:15:11,250 --> 00:15:16,560
yeah so what the code actually should

00:15:13,920 --> 00:15:19,259
read is that first line should be qnq at

00:15:16,560 --> 00:15:20,910
brackets add one two and then everything

00:15:19,259 --> 00:15:23,850
else is exactly the same

00:15:20,910 --> 00:15:26,189
I've used this OS system shorthand here

00:15:23,850 --> 00:15:27,990
to demonstrate you just run this stuff

00:15:26,189 --> 00:15:30,540
on the command line so you all you do is

00:15:27,990 --> 00:15:31,920
write IQ worker and then whenever it

00:15:30,540 --> 00:15:33,540
receives a message with a task

00:15:31,920 --> 00:15:35,279
it'll import the task from the correct

00:15:33,540 --> 00:15:37,019
location which is why you don't need the

00:15:35,279 --> 00:15:43,230
decorator it just goes and uses pythons

00:15:37,019 --> 00:15:44,790
import system yeah Huey Huey is it

00:15:43,230 --> 00:15:46,860
builds itself as a little task queue

00:15:44,790 --> 00:15:48,930
it's got a lot less code than salary and

00:15:46,860 --> 00:15:51,600
it's a lot simpler but it actually has

00:15:48,930 --> 00:15:53,610
pretty much all of the same features the

00:15:51,600 --> 00:15:56,100
one you do lose is it only supports

00:15:53,610 --> 00:16:01,290
Redis and SQLite unlike slurry which

00:15:56,100 --> 00:16:04,500
supports Amazon sqs Redis aimed 8 HQ MP

00:16:01,290 --> 00:16:07,470
AMQP the the rabbitmq Queuing protocol

00:16:04,500 --> 00:16:09,360
and some other stuff and it has a kind

00:16:07,470 --> 00:16:11,160
of nicer toss chaining api them celery

00:16:09,360 --> 00:16:13,220
if we have enough time I will show you

00:16:11,160 --> 00:16:15,600
the toss training stuff it's really cool

00:16:13,220 --> 00:16:16,889
but yeah otherwise pretty much all of

00:16:15,600 --> 00:16:20,790
these are very similar

00:16:16,889 --> 00:16:23,579
you have decorator and then you in Huey

00:16:20,790 --> 00:16:24,930
by default they're always the default

00:16:23,579 --> 00:16:26,160
call will be asynchronous whereas in

00:16:24,930 --> 00:16:28,170
other libraries you have to specify that

00:16:26,160 --> 00:16:33,300
you want the asynchronous call and it's

00:16:28,170 --> 00:16:35,790
got the same get API dramatic again

00:16:33,300 --> 00:16:38,879
basically the same except you've got

00:16:35,790 --> 00:16:41,329
send instead of delay and you don't have

00:16:38,879 --> 00:16:45,149
to wait this will this call the block

00:16:41,329 --> 00:16:47,220
which is less than ideal again you just

00:16:45,149 --> 00:16:48,689
run it command-line except this one you

00:16:47,220 --> 00:16:50,790
point to a module path and that module

00:16:48,689 --> 00:16:54,569
path is used to configure your dramatic

00:16:50,790 --> 00:16:58,199
actors this one has rabbitmq and Redis

00:16:54,569 --> 00:16:59,910
support but terrible Docs basically if

00:16:58,199 --> 00:17:05,730
you want to do anything beyond just

00:16:59,910 --> 00:17:08,699
sending a task it's not fun tasks dagger

00:17:05,730 --> 00:17:09,929
task tag is actually really cool it was

00:17:08,699 --> 00:17:11,400
the first library I found that did

00:17:09,929 --> 00:17:14,250
distributed locking I found this before

00:17:11,400 --> 00:17:16,199
I found here we distributed locking is

00:17:14,250 --> 00:17:18,199
where over all of your different workers

00:17:16,199 --> 00:17:20,250
only one can enter a section at a time

00:17:18,199 --> 00:17:21,750
and it actually provides a bunch of

00:17:20,250 --> 00:17:23,459
extra queueing criminals which are

00:17:21,750 --> 00:17:24,839
really good and it's architectural II

00:17:23,459 --> 00:17:27,419
really sold

00:17:24,839 --> 00:17:30,890
it just supports a lot of stuff that

00:17:27,419 --> 00:17:33,240
makes failure less painful unfortunately

00:17:30,890 --> 00:17:35,190
all you can do is find out your task is

00:17:33,240 --> 00:17:36,990
done you can't find out any information

00:17:35,190 --> 00:17:39,539
about that you can't find out if failed

00:17:36,990 --> 00:17:41,850
you can't find out what the result was

00:17:39,539 --> 00:17:43,380
but again API is exactly the same as

00:17:41,850 --> 00:17:45,299
everything else they're all really

00:17:43,380 --> 00:17:46,740
simple which shows that celery did a

00:17:45,299 --> 00:17:48,600
pretty good job with the simple stuff

00:17:46,740 --> 00:17:53,100
because they pretty much will copy the

00:17:48,600 --> 00:17:56,070
celery API for defining a task and then

00:17:53,100 --> 00:17:59,490
tasks tasks it's a bit different so dusk

00:17:56,070 --> 00:18:02,480
is a project by the PI data foundation

00:17:59,490 --> 00:18:05,429
the same people behind Syfy and numpy

00:18:02,480 --> 00:18:07,080
it's not a task queue but it does really

00:18:05,429 --> 00:18:10,529
well solve the data parallelism in

00:18:07,080 --> 00:18:12,570
Python problem so instead of a queue

00:18:10,529 --> 00:18:14,850
it's got a scheduler that you run over

00:18:12,570 --> 00:18:18,149
your entire cluster and it's designed to

00:18:14,850 --> 00:18:20,789
distribute pandas dataframes and numpy

00:18:18,149 --> 00:18:22,529
arrays over your entire cluster to let

00:18:20,789 --> 00:18:24,600
you do your processing and it actually

00:18:22,529 --> 00:18:26,789
implements all of the scikit-learn api's

00:18:24,600 --> 00:18:30,500
in a distributed fashion which is

00:18:26,789 --> 00:18:33,870
absolutely awesome it does also have a

00:18:30,500 --> 00:18:36,210
task queuing like API but it uses the

00:18:33,870 --> 00:18:37,799
scheduler to distribute those tasks so

00:18:36,210 --> 00:18:38,970
if all you want to do is run your work

00:18:37,799 --> 00:18:41,039
somewhere else and you happen to be

00:18:38,970 --> 00:18:42,899
using pandas and it's super big datasets

00:18:41,039 --> 00:18:47,039
that you use in clustering check this

00:18:42,899 --> 00:18:48,960
out because it's really cool yeah so

00:18:47,039 --> 00:18:51,600
that's that's the list of tasking api's

00:18:48,960 --> 00:18:53,730
this is where we are now we're running

00:18:51,600 --> 00:18:56,100
our queue in places of celery for most

00:18:53,730 --> 00:18:57,240
applications the ones that are still

00:18:56,100 --> 00:18:58,529
running celery are the ones that I

00:18:57,240 --> 00:19:00,480
haven't gotten around to porting over

00:18:58,529 --> 00:19:03,809
yet and it's actually very easy for us

00:19:00,480 --> 00:19:05,940
to pull our queue is so simple that we

00:19:03,809 --> 00:19:07,890
were able to actually implement Windows

00:19:05,940 --> 00:19:09,149
support for a limited use case it

00:19:07,890 --> 00:19:12,179
doesn't have Windows support by default

00:19:09,149 --> 00:19:15,330
but now half of our developers who run

00:19:12,179 --> 00:19:16,919
Windows are running our queue and it

00:19:15,330 --> 00:19:19,860
took less than 60 lines to add Windows

00:19:16,919 --> 00:19:21,539
support which is pretty awesome and our

00:19:19,860 --> 00:19:23,220
tasks never get stuck in the queue while

00:19:21,539 --> 00:19:26,100
we've still got capacity available which

00:19:23,220 --> 00:19:30,110
is a big improvement of the celery so we

00:19:26,100 --> 00:19:30,110
love lucky right now it's great I

00:19:30,350 --> 00:19:34,049
already covered all that oh yeah we were

00:19:32,370 --> 00:19:35,250
already using ratification so it made a

00:19:34,049 --> 00:19:37,580
lot of sense to use something they used

00:19:35,250 --> 00:19:39,080
Redis is a queue as well and

00:19:37,580 --> 00:19:43,879
so I thought we haven't really had any

00:19:39,080 --> 00:19:45,799
issues did I miss anything else nope

00:19:43,879 --> 00:19:49,399
it's simple it's great it's lovely easy

00:19:45,799 --> 00:19:52,489
to understand yeah so their conclusions

00:19:49,399 --> 00:19:54,139
their tasks use are really great they

00:19:52,489 --> 00:19:55,549
give you architectural flexibility but

00:19:54,139 --> 00:19:57,200
the only cost is stuff you should be

00:19:55,549 --> 00:20:00,139
doing anyway if you're doing scale web

00:19:57,200 --> 00:20:02,710
systems and they're super relevant at

00:20:00,139 --> 00:20:06,080
the moment like everyone uses toss keys

00:20:02,710 --> 00:20:09,049
I would take away celery is really

00:20:06,080 --> 00:20:10,759
powerful but complex if you don't using

00:20:09,049 --> 00:20:11,989
any of the complexity you'll find you

00:20:10,759 --> 00:20:13,460
won't have to worry that much but as

00:20:11,989 --> 00:20:14,720
soon as you do anything beyond their

00:20:13,460 --> 00:20:17,359
default use case you're going to have to

00:20:14,720 --> 00:20:18,769
do some research and it's important to

00:20:17,359 --> 00:20:22,070
know what you need from your task queue

00:20:18,769 --> 00:20:23,749
because if you do you can realize that

00:20:22,070 --> 00:20:25,340
celery is too much in too complex a

00:20:23,749 --> 00:20:28,940
burden and switch to something tiny and

00:20:25,340 --> 00:20:30,799
simple like we did for our queue it's

00:20:28,940 --> 00:20:33,019
probably a generally good takeaway to

00:20:30,799 --> 00:20:35,749
know what you need for any piece of

00:20:33,019 --> 00:20:37,909
infrastructure anyway but there's a lot

00:20:35,749 --> 00:20:39,769
of complexity in the standard solution

00:20:37,909 --> 00:20:42,859
in this particular case in Python land

00:20:39,769 --> 00:20:46,929
so especially relevant here

00:20:42,859 --> 00:20:46,929
I guess questions

00:20:53,380 --> 00:20:56,960
people thank you Tom we have some time

00:20:55,700 --> 00:21:03,560
for questions does anyone have any

00:20:56,960 --> 00:21:05,270
questions you were going to show us that

00:21:03,560 --> 00:21:07,990
task chaining oh yeah learners content

00:21:05,270 --> 00:21:10,130
also yeah lucky you

00:21:07,990 --> 00:21:11,740
yeah so let's toss chaining and celery

00:21:10,130 --> 00:21:15,020
it gives you two different api's for it

00:21:11,740 --> 00:21:17,510
and when you do a des that represents a

00:21:15,020 --> 00:21:19,610
signature of AD partially applied with

00:21:17,510 --> 00:21:21,500
those arguments in this case ad takes

00:21:19,610 --> 00:21:24,200
two arguments so you don't provide any

00:21:21,500 --> 00:21:25,400
extras it generates the result and

00:21:24,200 --> 00:21:27,140
passes it on to the next one and

00:21:25,400 --> 00:21:29,390
generates the resultant passes on to the

00:21:27,140 --> 00:21:31,340
next one and to actually trigger that

00:21:29,390 --> 00:21:33,650
you just call and then you get a normal

00:21:31,340 --> 00:21:35,150
task result but internally it does all

00:21:33,650 --> 00:21:37,450
the chaining for you in the system which

00:21:35,150 --> 00:21:41,240
is pretty cool and then it's also got

00:21:37,450 --> 00:21:44,150
the binary or pipe operator you can use

00:21:41,240 --> 00:21:47,180
as well so that's that's celery land

00:21:44,150 --> 00:21:48,620
Hughie's on here as well I like the dog

00:21:47,180 --> 00:21:49,790
pen API is because I'm a high school guy

00:21:48,620 --> 00:21:53,570
and it makes me feel like I'm using

00:21:49,790 --> 00:21:56,000
monads even though I'm not so yeah again

00:21:53,570 --> 00:21:59,840
pretty simple you do the signature API

00:21:56,000 --> 00:22:03,560
dot then don't then dot then yeah and

00:21:59,840 --> 00:22:04,970
then distributed logging so this is

00:22:03,560 --> 00:22:07,370
carried directly stolen from the huy

00:22:04,970 --> 00:22:09,320
documentation on the bottom two lines

00:22:07,370 --> 00:22:11,900
you see with Huey don't lock task DB

00:22:09,320 --> 00:22:15,470
backup means only one task can enter

00:22:11,900 --> 00:22:17,690
there at a time yeah distributed locking

00:22:15,470 --> 00:22:19,190
is actually surprisingly easy to

00:22:17,690 --> 00:22:20,810
implement if you have something like

00:22:19,190 --> 00:22:23,150
gratis because you can just have a key

00:22:20,810 --> 00:22:25,730
there and say you can use Redis

00:22:23,150 --> 00:22:30,490
primitives to effectively do locking on

00:22:25,730 --> 00:22:30,490
that key yeah that's the bonus content

00:22:37,250 --> 00:22:42,750
thank you very talk quick question did

00:22:40,140 --> 00:22:48,530
you guys try to engage with readies so

00:22:42,750 --> 00:22:48,530
it's every dev team like know about your

00:22:51,050 --> 00:22:54,960
they probably have better things to do

00:22:53,160 --> 00:22:56,490
and they've it's very clear that they've

00:22:54,960 --> 00:22:58,440
designed it for the most common use case

00:22:56,490 --> 00:23:01,170
which is not Alice and changing the

00:22:58,440 --> 00:23:03,690
defaults does not make sense for them so

00:23:01,170 --> 00:23:05,820
just finding those configuration options

00:23:03,690 --> 00:23:07,140
was great for us but what actually

00:23:05,820 --> 00:23:10,020
killed us was it turned out there was a

00:23:07,140 --> 00:23:11,640
bug in the sqs implementation that meant

00:23:10,020 --> 00:23:12,780
that one of those was ignored and it

00:23:11,640 --> 00:23:14,100
meant that our stuff was still getting

00:23:12,780 --> 00:23:16,230
stuck

00:23:14,100 --> 00:23:17,850
so that bug has since been fixed in a

00:23:16,230 --> 00:23:19,320
more recent celery version but by the

00:23:17,850 --> 00:23:25,980
time it was fixed we'd already moved to

00:23:19,320 --> 00:23:28,050
okey great talk Thanks

00:23:25,980 --> 00:23:30,920
one quick question I had is in the past

00:23:28,050 --> 00:23:33,330
I've used celery and when celery is

00:23:30,920 --> 00:23:35,640
distributing the tasks it has to I think

00:23:33,330 --> 00:23:37,740
pickle them yes are there any

00:23:35,640 --> 00:23:44,370
restrictions on what can be sent as a

00:23:37,740 --> 00:23:45,750
task with our queue so we didn't have to

00:23:44,370 --> 00:23:47,790
deal with that so because of the

00:23:45,750 --> 00:23:49,530
restrictions that celery provides you

00:23:47,790 --> 00:23:51,210
we'd run into a lot of issues and I

00:23:49,530 --> 00:23:53,070
actually ended up implementing our own

00:23:51,210 --> 00:23:56,610
serialization layer on top of that

00:23:53,070 --> 00:23:58,740
because there's no easy way to flexibly

00:23:56,610 --> 00:24:00,660
extend the existing serializers without

00:23:58,740 --> 00:24:01,950
defining your own and we just wanted to

00:24:00,660 --> 00:24:04,050
use bog-standard Jason to serialize

00:24:01,950 --> 00:24:05,490
stuff and so what we ended up doing was

00:24:04,050 --> 00:24:07,080
converting stuff into stuff that was

00:24:05,490 --> 00:24:08,850
Jason serializable and passing that in

00:24:07,080 --> 00:24:11,460
and we just reused that exact same logic

00:24:08,850 --> 00:24:15,360
for what I'm pretty sure actually does

00:24:11,460 --> 00:24:17,870
is it also just pickles but we didn't

00:24:15,360 --> 00:24:17,870
need to deal with that

00:24:23,180 --> 00:24:29,610
we're looking at using celery at the

00:24:27,090 --> 00:24:31,920
moment and one of the some of the

00:24:29,610 --> 00:24:35,400
features that we like about it are that

00:24:31,920 --> 00:24:38,970
it's got a scheduler add-on celery beet

00:24:35,400 --> 00:24:41,700
and flower looks like a really nice

00:24:38,970 --> 00:24:44,670
monitoring tool for celery is there any

00:24:41,700 --> 00:24:47,250
similar scheduling and monitoring things

00:24:44,670 --> 00:24:48,960
for our queue pretty much all of the

00:24:47,250 --> 00:24:51,030
tasks systems I listed have some sort of

00:24:48,960 --> 00:24:53,400
schedule II type thing so that

00:24:51,030 --> 00:24:56,030
distributed locking thing from Huey it

00:24:53,400 --> 00:24:58,380
has the chute or a wrong direction

00:24:56,030 --> 00:25:00,090
yeah so that actually has Huey using

00:24:58,380 --> 00:25:02,670
periodic toss straight up so effectively

00:25:00,090 --> 00:25:05,460
it's scheduling off the bat ah q has one

00:25:02,670 --> 00:25:07,500
toss Tyga has one das has one everything

00:25:05,460 --> 00:25:09,510
has that remonda drink celery has a

00:25:07,500 --> 00:25:10,770
pretty nice monitoring dashboard and

00:25:09,510 --> 00:25:12,300
it's a lot easy to build your own

00:25:10,770 --> 00:25:13,860
monitoring tools because you can

00:25:12,300 --> 00:25:18,270
actually understand what's going on in

00:25:13,860 --> 00:25:20,250
the internal structure we actually had

00:25:18,270 --> 00:25:22,620
some issues with the celery flow a thing

00:25:20,250 --> 00:25:24,690
at some stage that was before my time

00:25:22,620 --> 00:25:29,270
and Mitch did that what were the issues

00:25:24,690 --> 00:25:31,290
we ran into didn't work didn't work okay

00:25:29,270 --> 00:25:33,300
okay this was two years ago that we

00:25:31,290 --> 00:25:35,010
tried that out and we basically gave up

00:25:33,300 --> 00:25:37,590
on it and did a tiny bit of state

00:25:35,010 --> 00:25:39,480
tracking ourselves and it was enough

00:25:37,590 --> 00:25:43,410
that we didn't hate our lives but it

00:25:39,480 --> 00:25:47,760
wasn't great but IQs just nicer so far

00:25:43,410 --> 00:25:49,910
because it's simple I think that answers

00:25:47,760 --> 00:25:49,910
your question

00:25:55,070 --> 00:26:03,970
I think volt okay and they are used in

00:25:59,930 --> 00:26:07,670
the excuse as a back-end of theory and

00:26:03,970 --> 00:26:11,090
do something like we try is a nightmare

00:26:07,670 --> 00:26:15,140
so I'd like to know a new switch from

00:26:11,090 --> 00:26:16,700
sorry to argue they'll happen to have

00:26:15,140 --> 00:26:22,460
some experience

00:26:16,700 --> 00:26:25,700
I'm really trying and I feel mostly

00:26:22,460 --> 00:26:29,390
transept yeah so we actually mostly

00:26:25,700 --> 00:26:31,970
avoided retry it's not too hard to write

00:26:29,390 --> 00:26:33,110
your own retry for any of these task

00:26:31,970 --> 00:26:35,060
queuing systems because they do

00:26:33,110 --> 00:26:37,130
generally provide enough that you can

00:26:35,060 --> 00:26:38,870
just catch your exceptions and manually

00:26:37,130 --> 00:26:41,530
write retry but who wants to write

00:26:38,870 --> 00:26:44,510
manual retry with exponential back-off

00:26:41,530 --> 00:26:47,150
basically none of our tasks suited retry

00:26:44,510 --> 00:26:48,710
so we never actually had to use it we

00:26:47,150 --> 00:26:50,000
just manually reported the failure and

00:26:48,710 --> 00:26:52,130
relied on the user determining whether

00:26:50,000 --> 00:26:54,500
or not to retry because if in all of our

00:26:52,130 --> 00:26:59,740
situations if it failed there was an

00:26:54,500 --> 00:26:59,740
issue with your data so sorry can tumble

00:27:03,310 --> 00:27:08,900
what kind of tools are you using to

00:27:05,930 --> 00:27:11,300
monitor tasks to be able to tell how

00:27:08,900 --> 00:27:14,930
many failures how many successes and so

00:27:11,300 --> 00:27:17,240
on so in our queue they provide a

00:27:14,930 --> 00:27:18,590
dashboard that tells you like you

00:27:17,240 --> 00:27:20,330
actually when your task fails it weaves

00:27:18,590 --> 00:27:22,370
ill into a failed queue and then

00:27:20,330 --> 00:27:23,780
provides a one-liner script that moves

00:27:22,370 --> 00:27:26,480
all failed tasks back into their

00:27:23,780 --> 00:27:29,180
original queues which is cool but it

00:27:26,480 --> 00:27:31,490
also has retry straight up in terms of

00:27:29,180 --> 00:27:32,930
monitoring we actually directly report

00:27:31,490 --> 00:27:36,470
these values to the user because like I

00:27:32,930 --> 00:27:38,720
said if something goes wrong in one of

00:27:36,470 --> 00:27:41,390
these it's almost certainly your data is

00:27:38,720 --> 00:27:43,580
wrong so for our use case at least but

00:27:41,390 --> 00:27:45,230
yet the dashboards really nice you can

00:27:43,580 --> 00:27:46,880
get reporting straight up because it

00:27:45,230 --> 00:27:49,520
sends messages when it puts stuff in the

00:27:46,880 --> 00:27:51,620
queue so it's something similar to be

00:27:49,520 --> 00:27:53,930
deflowered and so yeah

00:27:51,620 --> 00:27:56,480
like I said we tried it before my time

00:27:53,930 --> 00:27:58,400
and it died and it didn't work but

00:27:56,480 --> 00:27:59,720
celery does have an event system and

00:27:58,400 --> 00:28:01,850
that event system will give you messages

00:27:59,720 --> 00:28:03,830
every time the state of your task

00:28:01,850 --> 00:28:06,440
changes which is actually really useful

00:28:03,830 --> 00:28:08,960
it I use a bit different in that it

00:28:06,440 --> 00:28:10,549
provides in Redis you've got the

00:28:08,960 --> 00:28:14,059
up-to-date state of the system rather

00:28:10,549 --> 00:28:15,140
than an event streaming system so yeah

00:28:14,059 --> 00:28:17,200
but the event system is salary is

00:28:15,140 --> 00:28:19,399
actually quite cool

00:28:17,200 --> 00:28:21,830
Huey provides the exact same event

00:28:19,399 --> 00:28:26,270
system basically Huey is celery but with

00:28:21,830 --> 00:28:27,770
less complexity and simpler code we

00:28:26,270 --> 00:28:32,230
probably have time for one more question

00:28:27,770 --> 00:28:32,230
if there are any further questions

00:28:38,419 --> 00:28:43,830
the question is really with the message

00:28:41,400 --> 00:28:47,510
queue systems are using which of those

00:28:43,830 --> 00:28:49,080
scales to millions and millions of tasks

00:28:47,510 --> 00:28:51,270
all of them

00:28:49,080 --> 00:28:53,820
so Redis has a lot of complexity in

00:28:51,270 --> 00:28:55,620
scaling to huge numbers because it's

00:28:53,820 --> 00:28:59,580
designed as a key value stole primarily

00:28:55,620 --> 00:29:01,860
but RabbitMQ and Amazon sqs both

00:28:59,580 --> 00:29:04,890
designed for massive scale message

00:29:01,860 --> 00:29:07,200
passing which is kind of the use case

00:29:04,890 --> 00:29:07,679
celery absolutely excels at if you're

00:29:07,200 --> 00:29:09,600
doing

00:29:07,679 --> 00:29:12,809
millions of tasks in flight constantly

00:29:09,600 --> 00:29:14,640
you probably want salary and it's worth

00:29:12,809 --> 00:29:16,049
the overhead of spending that those

00:29:14,640 --> 00:29:22,980
hours working out what configuration

00:29:16,049 --> 00:29:24,299
options you need people thank you very

00:29:22,980 --> 00:29:25,620
much for your time some stuff name can

00:29:24,299 --> 00:29:27,830
everyone give me another round of

00:29:25,620 --> 00:29:27,830
applause

00:29:30,720 --> 00:29:35,300
and as a token of appreciation enjoys

00:29:33,030 --> 00:29:35,300

YouTube URL: https://www.youtube.com/watch?v=ceJ-vy7fvus


