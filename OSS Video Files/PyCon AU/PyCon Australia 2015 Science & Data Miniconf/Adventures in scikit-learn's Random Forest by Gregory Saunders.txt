Title: Adventures in scikit-learn's Random Forest by Gregory Saunders
Publication date: 2015-08-06
Playlist: PyCon Australia 2015 Science & Data Miniconf
Description: 
	Scikit-learn's Random Forests are a great first choice for tackling a machine-learning problem. They are easy to use with only a handful of tuning parameters but nevertheless produce good results. Additionally, a separate cross-validation step can be avoided using the out-of-bag sample predictions generated during the construction of the forest, and finally they make it relatively easy to identify and extract the most important features of the sample data.

In this talk we’ll go through the process of using scikit-learn’s random forests using a financial data-set (of ASX equities) as an example. We’ll begin with a basic overview of the random forest algorithm and of the tuning parameters available and their impact on the effectiveness of the forest. Secondly we’ll go over the basic usage of scikit-learn’s random forests and in the process trouble-shoot some common problems such as dealing with missing sample data. Next we’ll discuss the use of out-of-bag sample predictions as a method for quickly performing cross-validation and optimising the tuning parameters. Finally we’ll look at how to extract information from the model that scikit-learn has generated, most notably the relative importances of the features in the sample data.

PyCon Australia is the national conference for users of the Python Programming Language. In 2015, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

July 31-August 4, Brisbane, Queensland, Australia
Captions: 
	00:00:10,090 --> 00:00:16,130
okay we might get started with our next

00:00:12,320 --> 00:00:18,920
talk so next up

00:00:16,130 --> 00:00:20,330
we have Gregory Saunders he has a PhD in

00:00:18,920 --> 00:00:22,670
computer science and it's been working

00:00:20,330 --> 00:00:26,210
the financial industry for more than 12

00:00:22,670 --> 00:00:29,470
years he found that he had data and he

00:00:26,210 --> 00:00:34,510
had time and he loves random forests so

00:00:29,470 --> 00:00:36,530
welcome thank you yes well as Rob said I

00:00:34,510 --> 00:00:39,290
work in the financial services industry

00:00:36,530 --> 00:00:40,730
and I had a pile of data about ASX

00:00:39,290 --> 00:00:43,220
equities and I had some competing

00:00:40,730 --> 00:00:45,379
horsepower and some free time so I

00:00:43,220 --> 00:00:48,110
decided to to see what I could do with

00:00:45,379 --> 00:00:50,000
with random forests and of course being

00:00:48,110 --> 00:00:52,250
in financial services industry with data

00:00:50,000 --> 00:00:53,510
about equities what do we want to do we

00:00:52,250 --> 00:00:56,479
want to predict how they're going to

00:00:53,510 --> 00:00:59,780
perform in the future so we'll see if we

00:00:56,479 --> 00:01:04,220
can get anywhere along that front later

00:00:59,780 --> 00:01:07,549
on so a quick introduction to supervised

00:01:04,220 --> 00:01:10,010
learning so random forests are used for

00:01:07,549 --> 00:01:11,320
obviously supervised learning there are

00:01:10,010 --> 00:01:13,760
two types of supervised learning

00:01:11,320 --> 00:01:15,620
classification and regression in

00:01:13,760 --> 00:01:18,770
classification you're given some data

00:01:15,620 --> 00:01:20,630
and it's already been classified right

00:01:18,770 --> 00:01:22,130
so you're given several data points

00:01:20,630 --> 00:01:24,409
usually thousands of data points

00:01:22,130 --> 00:01:26,450
tens of thousands millions and you've

00:01:24,409 --> 00:01:29,180
got the classifications of those data

00:01:26,450 --> 00:01:31,100
points already ok and what you want to

00:01:29,180 --> 00:01:35,659
do is you want to predict predicted the

00:01:31,100 --> 00:01:37,820
class of unseen data in a regression

00:01:35,659 --> 00:01:41,479
problem the desired result is not a

00:01:37,820 --> 00:01:43,909
class but a number usually a real number

00:01:41,479 --> 00:01:46,100
right so it's like predicting the result

00:01:43,909 --> 00:01:48,439
of a function you'll give you're given a

00:01:46,100 --> 00:01:49,909
set of samples again with known results

00:01:48,439 --> 00:01:52,159
and you want to predict the results on

00:01:49,909 --> 00:01:54,200
previously unseen data and I'll be

00:01:52,159 --> 00:01:56,659
focusing on regression in my

00:01:54,200 --> 00:01:59,179
presentation so what does the data look

00:01:56,659 --> 00:02:03,049
like this is what the data looks like

00:01:59,179 --> 00:02:06,289
very simply you have a matrix of

00:02:03,049 --> 00:02:09,410
training data over here with rows for

00:02:06,289 --> 00:02:12,920
each sample of data and columns for each

00:02:09,410 --> 00:02:14,360
feature and given that this is the

00:02:12,920 --> 00:02:18,140
training data we know the results of

00:02:14,360 --> 00:02:23,150
this data so we have the vector results

00:02:18,140 --> 00:02:25,010
here now notice that the the matrix of

00:02:23,150 --> 00:02:27,500
training data and the results are real

00:02:25,010 --> 00:02:29,540
numbers ok so if you have data that

00:02:27,500 --> 00:02:30,470
isn't a real number for example it's a

00:02:29,540 --> 00:02:33,560
string you're going

00:02:30,470 --> 00:02:34,640
need to convert it to a real number you

00:02:33,560 --> 00:02:38,540
can convert it to an integer of course

00:02:34,640 --> 00:02:40,070
because that's a real number now that's

00:02:38,540 --> 00:02:43,360
the training data that's how we're going

00:02:40,070 --> 00:02:45,320
to train our supervised learning model

00:02:43,360 --> 00:02:47,000
but we're going to test it with some

00:02:45,320 --> 00:02:49,220
data where we don't already have the

00:02:47,000 --> 00:02:51,860
results and again it just looks the same

00:02:49,220 --> 00:02:54,140
as the training data it's a matrix with

00:02:51,860 --> 00:02:58,700
one row per sample and one column per

00:02:54,140 --> 00:03:01,390
feature now in my sample data the

00:02:58,700 --> 00:03:03,770
features are just measurements about

00:03:01,390 --> 00:03:05,840
stocks on the Australian Stock Exchange

00:03:03,770 --> 00:03:07,130
so it could be analyst estimates of

00:03:05,840 --> 00:03:10,520
future earnings

00:03:07,130 --> 00:03:11,590
it could be price momentum all the kinds

00:03:10,520 --> 00:03:14,060
of things you can look up on Wikipedia

00:03:11,590 --> 00:03:15,950
it's all pretty pretty standard stuff

00:03:14,060 --> 00:03:17,420
but I can't actually tell you how it's

00:03:15,950 --> 00:03:22,280
derived because that would be

00:03:17,420 --> 00:03:24,020
proprietary so before we go and talk

00:03:22,280 --> 00:03:26,680
about random for us let's just have a

00:03:24,020 --> 00:03:26,680
look at my data

00:03:42,350 --> 00:03:48,650
never type for everyone so here's my

00:03:46,010 --> 00:03:52,820
here's my training data as you can see

00:03:48,650 --> 00:03:58,070
just a matrix of real numbers and if I

00:03:52,820 --> 00:03:59,660
print my training results that's just a

00:03:58,070 --> 00:04:01,250
vector of numbers and well note a little

00:03:59,660 --> 00:04:03,530
little problem here we've got some Nan's

00:04:01,250 --> 00:04:06,050
in our data that means those those

00:04:03,530 --> 00:04:08,930
results are missing we'll get to that in

00:04:06,050 --> 00:04:11,980
a second and if I just have a look at my

00:04:08,930 --> 00:04:11,980
test data

00:04:14,650 --> 00:04:19,930
there's my test data also a matrix can

00:04:17,480 --> 00:04:21,710
everyone read that by the way yep good

00:04:19,930 --> 00:04:26,870
okay so let's go back to the

00:04:21,710 --> 00:04:29,390
presentation so why use random forests

00:04:26,870 --> 00:04:33,380
there are a number of supervised

00:04:29,390 --> 00:04:35,810
learning algorithms available many of

00:04:33,380 --> 00:04:37,640
them in the scikit-learn package so why

00:04:35,810 --> 00:04:40,040
pick random forests why not use

00:04:37,640 --> 00:04:42,190
something like neural nets they're sexy

00:04:40,040 --> 00:04:45,500
they're hot everyone seems to love them

00:04:42,190 --> 00:04:48,110
but I've gone with random forests and

00:04:45,500 --> 00:04:49,390
the reason I went with random forests is

00:04:48,110 --> 00:04:52,930
because they have some advantages

00:04:49,390 --> 00:04:55,940
they're bit bit easier to use than other

00:04:52,930 --> 00:05:00,260
learning algorithms because they have

00:04:55,940 --> 00:05:02,300
few tuning parameters right so you with

00:05:00,260 --> 00:05:05,210
any supervised learning method you have

00:05:02,300 --> 00:05:07,220
to give it some parameters about how

00:05:05,210 --> 00:05:09,560
it's going to perform its work and

00:05:07,220 --> 00:05:12,200
different parameters will produce

00:05:09,560 --> 00:05:14,420
different results so with fewer tuning

00:05:12,200 --> 00:05:16,580
parameters there's fewer different

00:05:14,420 --> 00:05:19,820
values to choose and that makes it

00:05:16,580 --> 00:05:21,590
simpler to to implement nevertheless

00:05:19,820 --> 00:05:24,170
despite the fact that there are few

00:05:21,590 --> 00:05:28,670
tuning parameters it still produces good

00:05:24,170 --> 00:05:30,470
performance also you don't need to

00:05:28,670 --> 00:05:32,000
standardize your training data with

00:05:30,470 --> 00:05:34,520
certain other supervised learning

00:05:32,000 --> 00:05:36,650
algorithms that matrix of training data

00:05:34,520 --> 00:05:38,960
real numbers you have to standardize it

00:05:36,650 --> 00:05:40,220
which means you have to convert it so it

00:05:38,960 --> 00:05:41,090
has a mean of 0 and a standard deviation

00:05:40,220 --> 00:05:43,580
of 1

00:05:41,090 --> 00:05:47,060
all right you don't have to do that with

00:05:43,580 --> 00:05:49,430
random forests so that's good it has

00:05:47,060 --> 00:05:51,200
built-in cross-validation which will

00:05:49,430 --> 00:05:56,060
I'll talk a little bit more about later

00:05:51,200 --> 00:05:57,560
and it it automatically quantifies the

00:05:56,060 --> 00:06:03,260
importance of each of the features for

00:05:57,560 --> 00:06:05,690
you so it has some nice advantages so

00:06:03,260 --> 00:06:07,760
what I random first hmm

00:06:05,690 --> 00:06:10,070
well rainforest is an ensemble learning

00:06:07,760 --> 00:06:12,590
method what the hell does that mean in

00:06:10,070 --> 00:06:15,200
in computer science we like to use you

00:06:12,590 --> 00:06:17,660
know normal words like ensemble with

00:06:15,200 --> 00:06:21,440
bizarre meanings so that we look smart

00:06:17,660 --> 00:06:23,990
right in fact all that it means is that

00:06:21,440 --> 00:06:25,820
we're taking two or more other learning

00:06:23,990 --> 00:06:27,710
methods and aggregating their results

00:06:25,820 --> 00:06:30,889
that's all that in samba learning means

00:06:27,710 --> 00:06:33,229
right and in this case the the two or

00:06:30,889 --> 00:06:35,900
more other learning methods are decision

00:06:33,229 --> 00:06:40,910
trees right decision trees take two or

00:06:35,900 --> 00:06:43,220
more of them you've got a forest okay so

00:06:40,910 --> 00:06:46,400
what are decision trees here's one mmm

00:06:43,220 --> 00:06:48,979
this one is actually built from from my

00:06:46,400 --> 00:06:51,350
data excuse me

00:06:48,979 --> 00:06:55,010
so what it's basically saying up here is

00:06:51,350 --> 00:06:59,780
that if feature number 75 is less than

00:06:55,010 --> 00:07:03,289
0.02 92 then take this branch otherwise

00:06:59,780 --> 00:07:04,669
take this branch right and you keep

00:07:03,289 --> 00:07:07,400
doing that all the way until you get to

00:07:04,669 --> 00:07:10,039
a leaf here's a leaf right now and once

00:07:07,400 --> 00:07:11,750
you get to that leaf you've got a value

00:07:10,039 --> 00:07:14,539
and that's going to be the value that it

00:07:11,750 --> 00:07:16,760
outputs okay now the real tree is

00:07:14,539 --> 00:07:21,470
something on the order of 40 levels deep

00:07:16,760 --> 00:07:24,440
at its maximum but I've only obviously

00:07:21,470 --> 00:07:26,780
printed out three levels here how

00:07:24,440 --> 00:07:28,220
decision tree is constructed well you

00:07:26,780 --> 00:07:31,010
you give them the training data and

00:07:28,220 --> 00:07:35,990
basically what they want to do is try to

00:07:31,010 --> 00:07:38,419
choose the feature where you can split

00:07:35,990 --> 00:07:41,150
on values of that feature that and

00:07:38,419 --> 00:07:43,760
derive the most information or split the

00:07:41,150 --> 00:07:46,389
data into the inter categories that give

00:07:43,760 --> 00:07:49,100
you the most information on each side so

00:07:46,389 --> 00:07:52,490
it just builds it up iteratively like

00:07:49,100 --> 00:07:56,150
that until it gets to a leaf which is a

00:07:52,490 --> 00:07:59,330
leaf will have one or more samples each

00:07:56,150 --> 00:08:00,860
of which have the same value in them now

00:07:59,330 --> 00:08:02,840
in a regression problem of course the

00:08:00,860 --> 00:08:04,310
values of real numbers so you most

00:08:02,840 --> 00:08:06,080
leaves are likely to have just one

00:08:04,310 --> 00:08:08,840
sample in them in a classification

00:08:06,080 --> 00:08:09,729
problem a leaf would have multiple

00:08:08,840 --> 00:08:16,029
samples all

00:08:09,729 --> 00:08:18,189
have the same class mmm so what's good

00:08:16,029 --> 00:08:21,339
about decision trees and what's bad well

00:08:18,189 --> 00:08:22,839
they have low bias high variance and

00:08:21,339 --> 00:08:27,339
they're prone to overfitting what on

00:08:22,839 --> 00:08:30,669
earth does that mean well this little

00:08:27,339 --> 00:08:33,039
diagram I'm indebted to UVs for this

00:08:30,669 --> 00:08:34,779
this diagram it basically illustrates

00:08:33,039 --> 00:08:37,570
the difference between bias and variance

00:08:34,779 --> 00:08:39,880
so over here we have bias you can see

00:08:37,570 --> 00:08:41,800
that the if we were shooting at this

00:08:39,880 --> 00:08:45,040
target we've obviously missed but we're

00:08:41,800 --> 00:08:46,329
missing the same all the time ok so

00:08:45,040 --> 00:08:48,850
we've got a little bit of bias up in

00:08:46,329 --> 00:08:53,680
this direction but it's clustered around

00:08:48,850 --> 00:08:56,350
this point with variance we don't have

00:08:53,680 --> 00:08:58,870
so much bias here but we do have a large

00:08:56,350 --> 00:09:00,820
amount of variance there's a great deal

00:08:58,870 --> 00:09:03,160
more difference between the data points

00:09:00,820 --> 00:09:07,660
then there is over here so that's the

00:09:03,160 --> 00:09:10,480
difference between bias and variance so

00:09:07,660 --> 00:09:13,060
the idea with random forests is that

00:09:10,480 --> 00:09:16,600
we're going to build a bunch of decision

00:09:13,060 --> 00:09:20,069
trees and we're going to try to reduce

00:09:16,600 --> 00:09:25,300
this variance whilst hopefully not

00:09:20,069 --> 00:09:30,670
affecting a bias too much ok so how do

00:09:25,300 --> 00:09:32,260
we build a random forest well you you

00:09:30,670 --> 00:09:34,899
would imagine that it has the word

00:09:32,260 --> 00:09:36,610
random in it somewhere and this is how

00:09:34,899 --> 00:09:39,310
it's done it's called random sampling

00:09:36,610 --> 00:09:42,220
with replacement if this is our training

00:09:39,310 --> 00:09:46,360
data here we've got sample 1 2 3 4 and 5

00:09:42,220 --> 00:09:47,769
right what we do is we take a random

00:09:46,360 --> 00:09:50,260
sample with replacement now with

00:09:47,769 --> 00:09:53,740
replacement just means that we can take

00:09:50,260 --> 00:09:55,540
the same sample more than once ok so

00:09:53,740 --> 00:09:58,060
here we've got sample 3 more than one

00:09:55,540 --> 00:10:00,699
sample 5 more than once sample 1 only

00:09:58,060 --> 00:10:02,470
once ok and then what we're going to do

00:10:00,699 --> 00:10:06,279
with this random sample is we're going

00:10:02,470 --> 00:10:08,019
to train a decision tree on that ok and

00:10:06,279 --> 00:10:10,750
then the next step is to take another

00:10:08,019 --> 00:10:12,040
random sample which presumably unless

00:10:10,750 --> 00:10:13,209
we're very unlucky will be different

00:10:12,040 --> 00:10:15,010
from the first one and we're going to

00:10:13,209 --> 00:10:16,149
train another tree on that I'm going to

00:10:15,010 --> 00:10:19,870
keep doing that until we've trained the

00:10:16,149 --> 00:10:22,730
desired number of trees now the idea is

00:10:19,870 --> 00:10:25,310
well sorry first let me say that the met

00:10:22,730 --> 00:10:28,220
this uses is called bagging that will

00:10:25,310 --> 00:10:29,540
that will come back later and and as I

00:10:28,220 --> 00:10:34,579
said it reduces the variance without

00:10:29,540 --> 00:10:36,199
increasing the bias okay mmm sar we're

00:10:34,579 --> 00:10:38,480
going to build a forest of decision

00:10:36,199 --> 00:10:40,430
trees each of which is built with a

00:10:38,480 --> 00:10:43,190
random sample of the data so it's not

00:10:40,430 --> 00:10:44,510
going to use all of the data it's going

00:10:43,190 --> 00:10:48,519
to use some of the data but it's going

00:10:44,510 --> 00:10:48,519
to get some of that data more than once

00:10:49,029 --> 00:10:54,829
okay so I mentioned earlier the tuning

00:10:52,790 --> 00:10:56,149
parameters what are the tuning

00:10:54,829 --> 00:10:59,600
parameters if we're building a random

00:10:56,149 --> 00:11:02,810
forest well it's really there's three

00:10:59,600 --> 00:11:07,190
but really it's only the first two that

00:11:02,810 --> 00:11:09,920
get a lot of action basically the number

00:11:07,190 --> 00:11:11,870
of trees number of features to consider

00:11:09,920 --> 00:11:14,449
at each split and the depth of the tree

00:11:11,870 --> 00:11:16,730
so let's look at those in detail the

00:11:14,449 --> 00:11:19,490
number of trees right so more trees is

00:11:16,730 --> 00:11:22,100
better okay but you suffer from

00:11:19,490 --> 00:11:23,690
diminishing returns so once you get

00:11:22,100 --> 00:11:26,510
above a critical number of trees that

00:11:23,690 --> 00:11:29,870
the the extra predictive power of the

00:11:26,510 --> 00:11:32,329
model doesn't improve very much more

00:11:29,870 --> 00:11:34,430
okay and typically you're looking at a

00:11:32,329 --> 00:11:36,170
few hundred to a few thousand trees

00:11:34,430 --> 00:11:38,420
sometimes tens of thousands depending on

00:11:36,170 --> 00:11:42,199
the the size of the data set that you're

00:11:38,420 --> 00:11:44,260
looking at of course the more trees the

00:11:42,199 --> 00:11:46,839
slower it is to construct the model okay

00:11:44,260 --> 00:11:49,760
so you want to strike a balance between

00:11:46,839 --> 00:11:54,860
the number of trees and how long it

00:11:49,760 --> 00:11:56,540
takes to construct the model the second

00:11:54,860 --> 00:11:59,149
tuning parameter is the number of

00:11:56,540 --> 00:12:01,550
features so remember that the columns of

00:11:59,149 --> 00:12:04,699
the training data represent the features

00:12:01,550 --> 00:12:07,279
of the training data so in my data set

00:12:04,699 --> 00:12:10,910
that was things like and analyst

00:12:07,279 --> 00:12:13,760
estimates of forward earnings price

00:12:10,910 --> 00:12:16,389
momentum and those sorts of things so we

00:12:13,760 --> 00:12:19,130
don't just choose a random sample of

00:12:16,389 --> 00:12:22,699
sorry yeah a random sample of the

00:12:19,130 --> 00:12:24,470
samples we also choose a random set of

00:12:22,699 --> 00:12:26,540
features that we're going to allow the

00:12:24,470 --> 00:12:28,399
model to split on at each point when

00:12:26,540 --> 00:12:29,899
it's building the trees and because

00:12:28,399 --> 00:12:33,199
we're using different data and different

00:12:29,899 --> 00:12:34,720
features to build each tree you end up

00:12:33,199 --> 00:12:36,440
with a whole bunch of trees that

00:12:34,720 --> 00:12:38,320
hopefully

00:12:36,440 --> 00:12:40,490
not highly correlated with each other

00:12:38,320 --> 00:12:44,330
okay

00:12:40,490 --> 00:12:46,280
now if you use more features of course

00:12:44,330 --> 00:12:48,950
it's going to choose the same feature

00:12:46,280 --> 00:12:51,140
every time because it's choosing the

00:12:48,950 --> 00:12:53,360
feature that gives it the best

00:12:51,140 --> 00:12:56,060
information split at each point so if

00:12:53,360 --> 00:12:58,070
all the features are available at every

00:12:56,060 --> 00:13:00,500
node on every decision tree it's always

00:12:58,070 --> 00:13:04,400
going to choose the same one so if you

00:13:00,500 --> 00:13:06,590
cut that down you increase the bias but

00:13:04,400 --> 00:13:07,520
you reduce the correlation of the trees

00:13:06,590 --> 00:13:10,280
okay

00:13:07,520 --> 00:13:11,660
and we want trees that aren't too highly

00:13:10,280 --> 00:13:13,130
correlated because if they were highly

00:13:11,660 --> 00:13:15,640
correlated then really we've just got a

00:13:13,130 --> 00:13:15,640
decision tree

00:13:15,880 --> 00:13:21,050
the third tuning parameter is the depth

00:13:18,410 --> 00:13:24,860
of the trees how how deep do you make

00:13:21,050 --> 00:13:27,410
them if you go too deep you risk

00:13:24,860 --> 00:13:30,230
overfitting the data okay remember that

00:13:27,410 --> 00:13:32,870
the the leaves of the tree in a

00:13:30,230 --> 00:13:35,510
regression problem contain typically

00:13:32,870 --> 00:13:40,280
only one sample and therefore one value

00:13:35,510 --> 00:13:41,390
okay so your or if you get to that leaf

00:13:40,280 --> 00:13:43,790
in the tree you're always going to get

00:13:41,390 --> 00:13:46,640
the same value if you cut the depth of

00:13:43,790 --> 00:13:49,010
the tree somehow you're going to get

00:13:46,640 --> 00:13:51,140
multiple samples and the value that

00:13:49,010 --> 00:13:53,000
you're going to get as a result is the

00:13:51,140 --> 00:13:57,140
average of the values for those samples

00:13:53,000 --> 00:13:58,520
okay so you're generalizing whereas if

00:13:57,140 --> 00:14:03,610
you don't cut the depth of the trees

00:13:58,520 --> 00:14:06,200
you're potentially overfitting now just

00:14:03,610 --> 00:14:08,090
just by creating a forest of decision

00:14:06,200 --> 00:14:09,890
trees we mitigate this risk of

00:14:08,090 --> 00:14:10,460
overfitting but we don't eliminate it

00:14:09,890 --> 00:14:14,990
entirely

00:14:10,460 --> 00:14:16,790
so forests are better than decision

00:14:14,990 --> 00:14:19,450
trees in that aspect but they're not

00:14:16,790 --> 00:14:22,970
they don't fix this problem completely

00:14:19,450 --> 00:14:24,680
one of the problems with limiting the

00:14:22,970 --> 00:14:26,570
depth of the trees is that we have no

00:14:24,680 --> 00:14:28,760
way of knowing ahead of time how deep

00:14:26,570 --> 00:14:31,400
the trees are going to be I told you

00:14:28,760 --> 00:14:33,380
earlier that the the tree I showed you

00:14:31,400 --> 00:14:35,210
was approximately 40 levels deep but the

00:14:33,380 --> 00:14:37,550
only way I could know that was to

00:14:35,210 --> 00:14:41,210
actually build the model get the tree

00:14:37,550 --> 00:14:42,589
and how deep is it right so we don't

00:14:41,210 --> 00:14:45,200
know ahead of time how deep the trees

00:14:42,589 --> 00:14:47,089
are going to be so limiting the depth of

00:14:45,200 --> 00:14:49,100
the trees can be a little bit of a

00:14:47,089 --> 00:14:51,460
problem into determining how deep they

00:14:49,100 --> 00:14:51,460
should go

00:14:51,940 --> 00:14:58,940
okay so let's have a look at so I kid

00:14:54,829 --> 00:15:00,290
learns random forests how do we how do

00:14:58,940 --> 00:15:02,000
we install circuit learn just in case

00:15:00,290 --> 00:15:04,069
you haven't done this yet it's pretty

00:15:02,000 --> 00:15:06,589
simple you're going to need numpy inside

00:15:04,069 --> 00:15:09,199
pi and then you just peep install so I

00:15:06,589 --> 00:15:11,209
could learn if you if you're not a fan

00:15:09,199 --> 00:15:11,690
of PIP then you can download it from

00:15:11,209 --> 00:15:17,149
scikit-learn

00:15:11,690 --> 00:15:19,730
or org that's enough of that so mmm

00:15:17,149 --> 00:15:22,850
before we can use the random forest

00:15:19,730 --> 00:15:24,139
model we need to deal with that missing

00:15:22,850 --> 00:15:26,110
sample data remember I showed you a

00:15:24,139 --> 00:15:28,670
minute ago we had some Nan's in our data

00:15:26,110 --> 00:15:33,829
well we need to deal with that so let me

00:15:28,670 --> 00:15:36,199
show you how we can deal with that so I

00:15:33,829 --> 00:15:38,480
could learn has init tools for doing

00:15:36,199 --> 00:15:41,660
what's called imputation and in random

00:15:38,480 --> 00:15:42,980
forests you typically use the median for

00:15:41,660 --> 00:15:45,019
missing values so if you imagine a

00:15:42,980 --> 00:15:47,089
feature you've got some missing values

00:15:45,019 --> 00:15:49,279
for that feature what you do is you

00:15:47,089 --> 00:15:51,230
calculate the median value for the

00:15:49,279 --> 00:15:52,759
feature and then every missing value you

00:15:51,230 --> 00:15:55,160
just replace that missing value with the

00:15:52,759 --> 00:15:56,389
median and the reason why you want to do

00:15:55,160 --> 00:15:59,209
it that way I remember is because the

00:15:56,389 --> 00:16:01,519
decision tree splits on a certain value

00:15:59,209 --> 00:16:03,110
and if it's less than the value it's

00:16:01,519 --> 00:16:04,819
going to go left if it's greater than

00:16:03,110 --> 00:16:09,470
the value it's going to go right so if

00:16:04,819 --> 00:16:11,569
you pick the median for the missing

00:16:09,470 --> 00:16:14,779
values you're just picking the middle

00:16:11,569 --> 00:16:16,130
value right so it's not going to bias it

00:16:14,779 --> 00:16:23,740
one way or the other with respect to

00:16:16,130 --> 00:16:26,360
that feature okay and in the imputation

00:16:23,740 --> 00:16:28,730
code the missing values are by default

00:16:26,360 --> 00:16:30,769
represented by nan but you can you can

00:16:28,730 --> 00:16:33,290
configure that so if we just have a look

00:16:30,769 --> 00:16:38,889
at the code that you could use to do

00:16:33,290 --> 00:16:43,490
this that's it just import the in pewter

00:16:38,889 --> 00:16:46,970
then create an impudence strategy fit it

00:16:43,490 --> 00:16:49,279
to your sample data and then transform

00:16:46,970 --> 00:16:51,800
your sample data with the fitted model

00:16:49,279 --> 00:16:54,560
and that will replace any missing values

00:16:51,800 --> 00:16:57,889
with their medians now you have to do

00:16:54,560 --> 00:16:59,120
the same thing to the test data right if

00:16:57,889 --> 00:17:00,500
you don't do it at the testator of

00:16:59,120 --> 00:17:03,220
course you'll have problems when you try

00:17:00,500 --> 00:17:05,890
to predict the results so you do this

00:17:03,220 --> 00:17:07,600
in using the same in pewter for the test

00:17:05,890 --> 00:17:10,209
data because remember the model is built

00:17:07,600 --> 00:17:14,049
using the meeting of the training data

00:17:10,209 --> 00:17:18,299
not the meeting of the test data so if

00:17:14,049 --> 00:17:18,299
we if I'll just cut to a demo of this

00:17:20,909 --> 00:17:31,750
okay so if I just have a look for any

00:17:28,209 --> 00:17:34,750
missing data here this is just some

00:17:31,750 --> 00:17:40,210
numpy magic to tell me if there's any

00:17:34,750 --> 00:17:44,470
missing data okay so what this is

00:17:40,210 --> 00:17:46,480
telling me is that basically for each

00:17:44,470 --> 00:17:49,360
column of data so for each feature

00:17:46,480 --> 00:17:51,789
are there any Nan's in that data and

00:17:49,360 --> 00:17:57,039
where it's true yes there are Nan's

00:17:51,789 --> 00:18:00,460
okay nan is not a number if you not

00:17:57,039 --> 00:18:04,799
familiar with that okay so how do we

00:18:00,460 --> 00:18:04,799
deal with it here's our code

00:18:10,500 --> 00:18:18,190
but that run okay so there's the code

00:18:13,900 --> 00:18:21,250
that I used earlier and here is the

00:18:18,190 --> 00:18:23,110
result it's it's replaced all of the

00:18:21,250 --> 00:18:25,630
names with their medians and now there

00:18:23,110 --> 00:18:26,520
is no column which has any names in it

00:18:25,630 --> 00:18:29,680
okay

00:18:26,520 --> 00:18:33,400
and of course we're going to have to do

00:18:29,680 --> 00:18:35,490
that for our test data as well where are

00:18:33,400 --> 00:18:35,490
you

00:18:46,640 --> 00:18:51,370
yeah it's really hard working like this

00:18:52,380 --> 00:18:59,580
okay now if you recall we also had the

00:18:58,230 --> 00:19:03,150
problem that there was some missing data

00:18:59,580 --> 00:19:07,380
in our results okay so what I'll do is

00:19:03,150 --> 00:19:10,530
I'll just use this which is pretty much

00:19:07,380 --> 00:19:12,630
doing what the imputed it'd to to

00:19:10,530 --> 00:19:19,800
replace the nouns in the results with

00:19:12,630 --> 00:19:24,150
their medians as well okay so now we've

00:19:19,800 --> 00:19:25,290
cleaned up our data let's go back to

00:19:24,150 --> 00:19:28,410
presentation

00:19:25,290 --> 00:19:31,560
okay so that was using the imputed to

00:19:28,410 --> 00:19:34,380
clean up our data but what happens if a

00:19:31,560 --> 00:19:37,170
feature is missing entirely you've got

00:19:34,380 --> 00:19:41,370
Nan's in every row for a certain column

00:19:37,170 --> 00:19:42,660
okay now one way to deal with that of

00:19:41,370 --> 00:19:45,150
course would be to just delete that

00:19:42,660 --> 00:19:49,890
column I didn't want to do it that way

00:19:45,150 --> 00:19:52,040
because I was repeatedly constructing

00:19:49,890 --> 00:19:54,150
training data over a period of time and

00:19:52,040 --> 00:19:57,270
it would happen that in certain periods

00:19:54,150 --> 00:19:59,070
of time that that certain features were

00:19:57,270 --> 00:20:02,460
missing in other periods of time they

00:19:59,070 --> 00:20:05,150
were present so what I decided to do was

00:20:02,460 --> 00:20:07,040
just replace that feature with all zeros

00:20:05,150 --> 00:20:10,980
okay

00:20:07,040 --> 00:20:12,690
zero if it's all zeros then there's no

00:20:10,980 --> 00:20:14,100
differentiation between any sample and

00:20:12,690 --> 00:20:17,040
that feature and it will never be chosen

00:20:14,100 --> 00:20:19,710
by a decision tree as a splitting

00:20:17,040 --> 00:20:22,620
variable so we can just replace them all

00:20:19,710 --> 00:20:26,340
with zero using code like that okay we

00:20:22,620 --> 00:20:30,300
just create a mask where if all of a

00:20:26,340 --> 00:20:35,790
certain column is NaN then just put zero

00:20:30,300 --> 00:20:38,960
in there okay okay and don't forget to

00:20:35,790 --> 00:20:41,700
check the the training results - okay

00:20:38,960 --> 00:20:45,330
all right so how do we actually

00:20:41,700 --> 00:20:50,490
construct our our learning model here's

00:20:45,330 --> 00:20:52,470
the code not terribly hard to do so we

00:20:50,490 --> 00:20:54,170
import the Rainforest regressor we

00:20:52,470 --> 00:20:57,240
create a random forest regressor in

00:20:54,170 --> 00:20:58,920
instance and I'll tell you about the

00:20:57,240 --> 00:21:00,480
perimeters in a second but once we've

00:20:58,920 --> 00:21:04,650
created it we fit it to our training

00:21:00,480 --> 00:21:05,890
data and then we predict the result of

00:21:04,650 --> 00:21:08,590
our test data

00:21:05,890 --> 00:21:10,000
that's that's all it takes so what are

00:21:08,590 --> 00:21:13,000
these parameters to the random forest

00:21:10,000 --> 00:21:16,660
regressor n estimators is the number of

00:21:13,000 --> 00:21:18,100
trees that max features that's the

00:21:16,660 --> 00:21:20,950
number of features to consider at each

00:21:18,100 --> 00:21:22,480
split the auto these these are the

00:21:20,950 --> 00:21:25,660
default values for the parameters by the

00:21:22,480 --> 00:21:28,840
way auto just in a regression case means

00:21:25,660 --> 00:21:31,540
all features in a classification a

00:21:28,840 --> 00:21:33,420
supervised learning case it's um the

00:21:31,540 --> 00:21:37,570
square root of the number of features

00:21:33,420 --> 00:21:39,880
the maximum depth is none so we could

00:21:37,570 --> 00:21:41,350
set this to do some level as the maximum

00:21:39,880 --> 00:21:43,809
depth if we had an idea about what makes

00:21:41,350 --> 00:21:45,850
them and deaths to choose and then these

00:21:43,809 --> 00:21:48,549
four parameters here are actually just a

00:21:45,850 --> 00:21:50,080
different way of constraining the depth

00:21:48,549 --> 00:21:53,620
of the tree remember I said earlier that

00:21:50,080 --> 00:21:56,200
you can't know ahead of time how deep

00:21:53,620 --> 00:21:58,360
the tree is going to be so instead of

00:21:56,200 --> 00:22:01,630
specifying a maximum depth what we could

00:21:58,360 --> 00:22:04,750
do is we could say there must be a

00:22:01,630 --> 00:22:07,030
certain number of samples left in order

00:22:04,750 --> 00:22:10,000
to split on a certain node in a decision

00:22:07,030 --> 00:22:12,130
tree right so the default value of two

00:22:10,000 --> 00:22:15,490
means that if there is even only two

00:22:12,130 --> 00:22:19,480
samples we can do a split and create two

00:22:15,490 --> 00:22:22,480
two leaves whereas if you set the

00:22:19,480 --> 00:22:23,830
minimum samples to say three then if you

00:22:22,480 --> 00:22:27,250
had only two left

00:22:23,830 --> 00:22:30,700
it would just leave that node as a leaf

00:22:27,250 --> 00:22:32,350
and average its results okay so that's a

00:22:30,700 --> 00:22:34,660
way of constraining the depth by

00:22:32,350 --> 00:22:37,480
limiting how many samples there has to

00:22:34,660 --> 00:22:39,610
be when it does a split another way of

00:22:37,480 --> 00:22:42,040
doing it is to say well any leaf must

00:22:39,610 --> 00:22:43,360
have at least this many samples that

00:22:42,040 --> 00:22:47,380
would be another way of constraining the

00:22:43,360 --> 00:22:49,090
depth of the decision trees another way

00:22:47,380 --> 00:22:54,010
would be to say well the certain has to

00:22:49,090 --> 00:22:55,660
be a certain weight of samples if there

00:22:54,010 --> 00:22:56,950
are you know a thousand samples and you

00:22:55,660 --> 00:22:59,559
want ten percent of them in each leaf

00:22:56,950 --> 00:23:02,740
then obviously you point one for the

00:22:59,559 --> 00:23:04,510
fret for the minimum weight and there

00:23:02,740 --> 00:23:07,030
would have to be at least a hundred

00:23:04,510 --> 00:23:09,910
samples in each leaf so again

00:23:07,030 --> 00:23:11,919
constraining the depth and then another

00:23:09,910 --> 00:23:14,080
way to do it is to set the the maximum

00:23:11,919 --> 00:23:15,880
number of leaf nodes so basically you're

00:23:14,080 --> 00:23:19,260
constraining the depth by saying there

00:23:15,880 --> 00:23:19,260
cannot be more than this many leaves

00:23:19,419 --> 00:23:24,279
and then the last parameter here end

00:23:21,549 --> 00:23:27,509
jobs that basically is just an

00:23:24,279 --> 00:23:30,729
optimization if by default it will use

00:23:27,509 --> 00:23:31,959
one CPU core if you said this to

00:23:30,729 --> 00:23:35,409
negative one it will use all available

00:23:31,959 --> 00:23:38,499
cores on your machine or you could set

00:23:35,409 --> 00:23:39,820
it to an integer which is some number of

00:23:38,499 --> 00:23:41,619
cores that you want it to use and

00:23:39,820 --> 00:23:47,609
basically it just means that a little do

00:23:41,619 --> 00:23:47,609
it faster so let's cut to a quick demo

00:23:58,109 --> 00:24:03,399
so I'm only going to set the number of

00:24:01,239 --> 00:24:05,889
estimators here we're going to leave the

00:24:03,399 --> 00:24:09,570
maximum features at Auto and I'm going

00:24:05,889 --> 00:24:09,570
to use all available CPU cores

00:24:17,840 --> 00:24:24,270
okay now that's actually it's training

00:24:21,330 --> 00:24:28,529
at the moment so unfortunately my laptop

00:24:24,270 --> 00:24:33,149
is only a dual core machine so we have

00:24:28,529 --> 00:24:35,870
to wait a little bit obviously if you've

00:24:33,149 --> 00:24:38,610
got a machine with a lot of cores and

00:24:35,870 --> 00:24:43,289
you know really fast clock speed that

00:24:38,610 --> 00:24:49,590
would be better waiting very keen still

00:24:43,289 --> 00:25:00,390
waiting there we go okay what happened

00:24:49,590 --> 00:25:03,870
ooooh our predicted result oh yes okay

00:25:00,390 --> 00:25:06,480
so that's a pretty good result okay so

00:25:03,870 --> 00:25:08,490
we have some results we've predicted the

00:25:06,480 --> 00:25:11,210
performance of some stocks so the

00:25:08,490 --> 00:25:15,860
question is is our prediction any good

00:25:11,210 --> 00:25:15,860
that's the the subject of my next slide

00:25:18,080 --> 00:25:22,529
here's the prediction any good okay how

00:25:20,460 --> 00:25:24,779
can we evaluate the results well mmm

00:25:22,529 --> 00:25:26,039
the default way of evaluating the

00:25:24,779 --> 00:25:28,409
results when you're doing a regression

00:25:26,039 --> 00:25:30,960
is the coefficient of determination the

00:25:28,409 --> 00:25:33,779
or the the r-squared value now

00:25:30,960 --> 00:25:38,820
interestingly enough this value can be

00:25:33,779 --> 00:25:41,190
negative and that's kind of weird

00:25:38,820 --> 00:25:44,940
because if it's r-squared right how can

00:25:41,190 --> 00:25:47,309
r-squared be negative and the answer is

00:25:44,940 --> 00:25:49,169
that it's not calculated as the square

00:25:47,309 --> 00:25:51,750
of some value the way that it's

00:25:49,169 --> 00:25:54,390
calculated is basically using the ratio

00:25:51,750 --> 00:25:57,179
of the squares of the differences

00:25:54,390 --> 00:26:01,679
between the correct result and the

00:25:57,179 --> 00:26:03,870
predicted result over the race over the

00:26:01,679 --> 00:26:06,720
squares of the differences between the

00:26:03,870 --> 00:26:10,260
correct result and just the average of

00:26:06,720 --> 00:26:13,710
all the correct results okay now in my

00:26:10,260 --> 00:26:16,320
dataset the results are standardized

00:26:13,710 --> 00:26:19,020
which means they have a mean of 0 and a

00:26:16,320 --> 00:26:22,700
standard deviation of 1 which means that

00:26:19,020 --> 00:26:26,549
if the if the result is negative for me

00:26:22,700 --> 00:26:29,820
just predicting zero for everything

00:26:26,549 --> 00:26:31,080
would be a better predictor of the of

00:26:29,820 --> 00:26:34,169
the results than using my

00:26:31,080 --> 00:26:38,659
model so let's sir let's take a look and

00:26:34,169 --> 00:26:38,659
see how it goes

00:26:40,159 --> 00:26:45,360
so the way we can see the score is we

00:26:42,990 --> 00:26:54,779
can use the the score method on the on

00:26:45,360 --> 00:26:58,740
the model to have a look oh dear how

00:26:54,779 --> 00:27:02,909
embarrassment my my score my a squared

00:26:58,740 --> 00:27:07,409
score is negative 0.02 hmm doesn't look

00:27:02,909 --> 00:27:09,360
like my model has done very well it

00:27:07,409 --> 00:27:10,769
would have been better actually to just

00:27:09,360 --> 00:27:12,840
predict zero as the performance for all

00:27:10,769 --> 00:27:15,870
stocks well what can we what can we do

00:27:12,840 --> 00:27:18,090
about this well actually it turns out in

00:27:15,870 --> 00:27:20,309
predicting stock performance that there

00:27:18,090 --> 00:27:23,820
are Squared's below zero hunt all that

00:27:20,309 --> 00:27:25,830
uncommon so those of you who have

00:27:23,820 --> 00:27:30,539
superannuation in the stock market

00:27:25,830 --> 00:27:32,669
take note so what can we do that gives

00:27:30,539 --> 00:27:34,799
us a better result let's go back to my

00:27:32,669 --> 00:27:36,600
slides here well that doesn't give us a

00:27:34,799 --> 00:27:38,130
better result but that helps us to

00:27:36,600 --> 00:27:41,250
evaluate our results a bit differently

00:27:38,130 --> 00:27:44,070
well in finance we often use what's

00:27:41,250 --> 00:27:48,840
called the rank correlation as a way of

00:27:44,070 --> 00:27:54,179
evaluating the results so how can we use

00:27:48,840 --> 00:27:55,700
do rank correlation in in Sify or in so

00:27:54,179 --> 00:27:58,500
I could learn while we're using so fi

00:27:55,700 --> 00:28:01,080
the Kendall tower and the Spearman row

00:27:58,500 --> 00:28:05,010
are two different measures of rank

00:28:01,080 --> 00:28:07,679
correlation so we can use this code to

00:28:05,010 --> 00:28:11,480
measure the rank correlation of our

00:28:07,679 --> 00:28:11,480
results so let's take a look at that

00:28:27,240 --> 00:28:35,260
okay so here we have our results the

00:28:32,289 --> 00:28:40,260
Kendall tower result was basically 11%

00:28:35,260 --> 00:28:42,909
and the Spearman row was 15% now

00:28:40,260 --> 00:28:45,399
actually in finance the hoster numbers

00:28:42,909 --> 00:28:48,639
aren't too bad I mean they're not great

00:28:45,399 --> 00:28:50,799
but they're not too bad so we could

00:28:48,639 --> 00:28:54,039
actually try out this model and see if

00:28:50,799 --> 00:28:57,970
it actually works in a in a simulated

00:28:54,039 --> 00:29:01,809
portfolio and the results that I got

00:28:57,970 --> 00:29:04,600
basically in 2014 it outperformed the

00:29:01,809 --> 00:29:07,809
the model we were really using to to run

00:29:04,600 --> 00:29:09,850
our customers money but in 2013 it

00:29:07,809 --> 00:29:10,990
significantly underperformed so it's

00:29:09,850 --> 00:29:17,110
still a bit of work to do here

00:29:10,990 --> 00:29:18,360
but it can work so there you go that's a

00:29:17,110 --> 00:29:24,179
different way of evaluating the results

00:29:18,360 --> 00:29:26,980
of our regressor and okay

00:29:24,179 --> 00:29:28,029
okay now I mentioned earlier

00:29:26,980 --> 00:29:31,179
cross-validation

00:29:28,029 --> 00:29:33,340
what's cross-validation so the idea of

00:29:31,179 --> 00:29:35,110
cross-validation is that you instead of

00:29:33,340 --> 00:29:37,059
training the data or training the model

00:29:35,110 --> 00:29:40,029
sorry on your entire training set of

00:29:37,059 --> 00:29:41,350
data you train it on part of the

00:29:40,029 --> 00:29:43,899
training set of data and then you use

00:29:41,350 --> 00:29:46,240
the rest of the training set to test how

00:29:43,899 --> 00:29:49,840
effective it is okay to test how good it

00:29:46,240 --> 00:29:51,309
is and one use for doing that sort of

00:29:49,840 --> 00:29:52,659
thing is to optimize the tuning

00:29:51,309 --> 00:29:54,490
permanence what you can do is you can

00:29:52,659 --> 00:29:57,549
try a certain set of tuning parameters

00:29:54,490 --> 00:30:02,320
say a hundred trees and max features of

00:29:57,549 --> 00:30:03,700
0.5 and see how good it is and then you

00:30:02,320 --> 00:30:06,850
could try another set of training

00:30:03,700 --> 00:30:09,990
parameters 150 trees and max features of

00:30:06,850 --> 00:30:12,250
point four and see how good it was and

00:30:09,990 --> 00:30:15,460
and then you can use that to optimize

00:30:12,250 --> 00:30:17,260
your tuning parameters and scikit-learn

00:30:15,460 --> 00:30:20,049
has tools to help you to do this the

00:30:17,260 --> 00:30:21,639
problem is it kind of takes a while it's

00:30:20,049 --> 00:30:23,380
slow I have to write some extra code

00:30:21,639 --> 00:30:27,039
don't really want to do that because I'm

00:30:23,380 --> 00:30:28,389
lazy so how can we do that how can we

00:30:27,039 --> 00:30:30,100
get sort of something like the same

00:30:28,389 --> 00:30:33,490
result without doing all that work

00:30:30,100 --> 00:30:35,889
AHA out-of-bag estimates so remember I

00:30:33,490 --> 00:30:37,720
told you earlier that the process of

00:30:35,889 --> 00:30:39,730
taking those random samples with

00:30:37,720 --> 00:30:40,540
replacement that are used to train the

00:30:39,730 --> 00:30:45,160
decision trees

00:30:40,540 --> 00:30:46,900
bagging okay well if you've got a random

00:30:45,160 --> 00:30:49,120
sample with replacement that's the same

00:30:46,900 --> 00:30:51,130
size or smaller than the original set of

00:30:49,120 --> 00:30:53,560
samples then you're going to in all

00:30:51,130 --> 00:30:55,480
likelihood have some samples that miss

00:30:53,560 --> 00:30:57,610
out that aren't in the training data

00:30:55,480 --> 00:31:00,460
that are used for any given tree and if

00:30:57,610 --> 00:31:03,520
you're training a hundred trees chances

00:31:00,460 --> 00:31:06,490
are well the more trees that you train

00:31:03,520 --> 00:31:08,200
the chances are higher that the for

00:31:06,490 --> 00:31:10,870
every single sample in your training

00:31:08,200 --> 00:31:13,240
data there will be at least one tree

00:31:10,870 --> 00:31:14,500
where that sample was not in the

00:31:13,240 --> 00:31:16,930
training data for that tree does

00:31:14,500 --> 00:31:19,590
everyone follow me there for every

00:31:16,930 --> 00:31:22,030
single tree so if every single sample

00:31:19,590 --> 00:31:24,310
there'll be at least some trees that

00:31:22,030 --> 00:31:28,210
that sample wasn't used in the training

00:31:24,310 --> 00:31:33,430
data for that tree so what do we do we

00:31:28,210 --> 00:31:36,520
take those trees that this sample was

00:31:33,430 --> 00:31:41,140
not used in those trees and we used them

00:31:36,520 --> 00:31:43,060
to predict a result for that sample and

00:31:41,140 --> 00:31:46,690
then we do that for every sample and as

00:31:43,060 --> 00:31:47,800
long as you've got enough trees then the

00:31:46,690 --> 00:31:50,110
chances are you'll be able to get a

00:31:47,800 --> 00:31:51,610
result for every single sample and then

00:31:50,110 --> 00:31:54,430
you'll be able to measure the result for

00:31:51,610 --> 00:31:56,920
that sample and that gives you an

00:31:54,430 --> 00:31:58,900
estimate of how well your random forest

00:31:56,920 --> 00:32:02,140
is performing and then you can use that

00:31:58,900 --> 00:32:04,000
to optimize your tuning parameters now

00:32:02,140 --> 00:32:07,210
is it as good it's doing proper

00:32:04,000 --> 00:32:10,150
cross-validation no is it faster and

00:32:07,210 --> 00:32:12,040
easier yes so let's let's go with that

00:32:10,150 --> 00:32:16,390
until we until we determine that we need

00:32:12,040 --> 00:32:19,030
to do the the whole hog okay so that's

00:32:16,390 --> 00:32:22,330
out of bag estimates and how could we

00:32:19,030 --> 00:32:24,670
use that to to optimize our parameters

00:32:22,330 --> 00:32:28,930
well here's some code to do it I've got

00:32:24,670 --> 00:32:31,450
just a list here of the different values

00:32:28,930 --> 00:32:33,520
for our parameters so we'll try one

00:32:31,450 --> 00:32:35,170
hundred and two hundred trees we'll try

00:32:33,520 --> 00:32:38,020
point three point four and point five

00:32:35,170 --> 00:32:40,270
value percentages of the number of

00:32:38,020 --> 00:32:43,660
features to use at each split and then

00:32:40,270 --> 00:32:46,600
what we basically do is we just take the

00:32:43,660 --> 00:32:50,440
product of those different parameters

00:32:46,600 --> 00:32:52,270
and then train a regresar with the with

00:32:50,440 --> 00:32:53,789
the each of the values lead to the

00:32:52,270 --> 00:32:56,940
combination to the values

00:32:53,789 --> 00:33:00,269
and we can use this herb score out of

00:32:56,940 --> 00:33:02,399
bag score to estimate the performance of

00:33:00,269 --> 00:33:04,889
the model and we just remember the one

00:33:02,399 --> 00:33:07,019
with the best score okay and that gives

00:33:04,889 --> 00:33:11,970
us a quick and dirty way of optimizing

00:33:07,019 --> 00:33:14,580
our tuning parameters all right it's

00:33:11,970 --> 00:33:17,100
nearly done so I mentioned earlier as

00:33:14,580 --> 00:33:19,830
well that we can get a quantification of

00:33:17,100 --> 00:33:23,190
the importance of each feature out of

00:33:19,830 --> 00:33:24,629
the the model how do we do that well the

00:33:23,190 --> 00:33:26,639
way that you determine how important the

00:33:24,629 --> 00:33:28,529
feature is is basically where it appears

00:33:26,639 --> 00:33:30,539
in the tree if it's up near the top it's

00:33:28,529 --> 00:33:33,239
more important because more samples will

00:33:30,539 --> 00:33:34,229
go through that tree that node but it's

00:33:33,239 --> 00:33:35,879
down near a leaf

00:33:34,229 --> 00:33:38,220
it's less important because fewer

00:33:35,879 --> 00:33:42,179
samples will go through that node you

00:33:38,220 --> 00:33:44,429
can basically quantify the importance of

00:33:42,179 --> 00:33:46,590
each feature in each tree and then

00:33:44,429 --> 00:33:50,009
average them across all trees to get the

00:33:46,590 --> 00:33:53,940
importance of the features in your model

00:33:50,009 --> 00:34:01,159
so just take a quick look at that if I

00:33:53,940 --> 00:34:03,899
just well I need to do is go S dot and

00:34:01,159 --> 00:34:07,169
there's the importances of all of my

00:34:03,899 --> 00:34:09,599
features so these are incidentally this

00:34:07,169 --> 00:34:14,450
these numbers all sum to one right so

00:34:09,599 --> 00:34:17,490
there it's the relative importance is

00:34:14,450 --> 00:34:19,290
measured in any way so that basically

00:34:17,490 --> 00:34:21,530
tells you that this feature here is sort

00:34:19,290 --> 00:34:24,210
of two percent of samples on average

00:34:21,530 --> 00:34:26,849
this one here's one point three

00:34:24,210 --> 00:34:29,639
that's one here's very few so gives you

00:34:26,849 --> 00:34:31,409
an exemplar an understanding of how

00:34:29,639 --> 00:34:36,089
important each particular feature is to

00:34:31,409 --> 00:34:37,829
your tiel model okay and then really

00:34:36,089 --> 00:34:40,889
quickly because I'm probably going over

00:34:37,829 --> 00:34:43,829
time how do we get to the individual

00:34:40,889 --> 00:34:46,319
trees the estimator has an attribute

00:34:43,829 --> 00:34:48,210
called estimators which you can use to

00:34:46,319 --> 00:34:50,250
get at the individual trees and then

00:34:48,210 --> 00:34:52,829
scikit-learn has an export graphviz

00:34:50,250 --> 00:34:55,290
method which allow you to dump a tree

00:34:52,829 --> 00:34:56,790
out into graph these file format and

00:34:55,290 --> 00:35:00,329
then you can use graphically as tools to

00:34:56,790 --> 00:35:02,099
produce something like that that you can

00:35:00,329 --> 00:35:03,180
use to visualize each of your trees now

00:35:02,099 --> 00:35:05,369
you're probably going to have hundreds

00:35:03,180 --> 00:35:06,510
maybe thousands of trees so how useful

00:35:05,369 --> 00:35:10,530
this is I don't know

00:35:06,510 --> 00:35:12,480
but there you go you can do it alright

00:35:10,530 --> 00:35:14,220
I'm nearly done so if you like

00:35:12,480 --> 00:35:16,800
rainforests and you like scikit-learn

00:35:14,220 --> 00:35:18,900
and you didn't like my my talk now

00:35:16,800 --> 00:35:21,330
there's some other talks about them that

00:35:18,900 --> 00:35:24,869
that you can go to later today and on

00:35:21,330 --> 00:35:26,820
Sunday so at 1:10 p.m. Chris Hoffa is

00:35:24,869 --> 00:35:29,340
doing a talk on an end-to-end machine

00:35:26,820 --> 00:35:33,119
learning ecosystem in in a quarter and

00:35:29,340 --> 00:35:35,820
our esteemed host Robert will be doing a

00:35:33,119 --> 00:35:38,190
talk later this afternoon on predicting

00:35:35,820 --> 00:35:40,740
sports winners using data analytics with

00:35:38,190 --> 00:35:42,030
Patterson scikit-learn I believe random

00:35:40,740 --> 00:35:44,160
forests will be making appearance in

00:35:42,030 --> 00:35:46,859
that talk as well and then on Sunday

00:35:44,160 --> 00:35:49,170
we've got a bit of a tutorial on on

00:35:46,859 --> 00:35:51,930
doing data science in Python which

00:35:49,170 --> 00:35:54,380
should be pretty awesome ok that's it

00:35:51,930 --> 00:35:54,380
I'm done

00:36:04,620 --> 00:36:06,680

YouTube URL: https://www.youtube.com/watch?v=YkVscKsV_qk


