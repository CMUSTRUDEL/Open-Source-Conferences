Title: An End-to-End Machine Learning Ecosystem in a Quarter by Chris Hausler
Publication date: 2015-08-06
Playlist: PyCon Australia 2015 Science & Data Miniconf
Description: 
	Building a machine learning model is hard. Building and serving individual machine learning models for thousands of customers every single day is harder. Model training time must be kept to a minimum, memory constraints are exacerbated and the love and insight that one would usually apply to refining a single model must be automated and scaled. At Zendesk we successfully use pandas, scikit-learn, flask and hadoop to run a machine learning ecosystem that builds, tunes and serves account specific models via an API for many of our customers. This talk is about what we learnt building it and how Python's data stack let us get it to production really really fast.

PyCon Australia is the national conference for users of the Python Programming Language. In 2015, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

July 31-August 4, Brisbane, Queensland, Australia
Captions: 
	00:00:13,610 --> 00:00:20,820
hi I'm Chris I suggested I I work for a

00:00:18,090 --> 00:00:24,810
company called Zendesk in the data team

00:00:20,820 --> 00:00:26,220
which is aptly named vanya and I'm gonna

00:00:24,810 --> 00:00:27,960
talk today about an end-to-end machine

00:00:26,220 --> 00:00:31,110
learning ecosystem and how we built it

00:00:27,960 --> 00:00:34,140
in a quarter almost we're pretty close

00:00:31,110 --> 00:00:36,150
so like just a sort of a brief

00:00:34,140 --> 00:00:37,170
background so I guess the data team

00:00:36,150 --> 00:00:38,520
would send us where we're pretty

00:00:37,170 --> 00:00:40,110
interested in building you know like

00:00:38,520 --> 00:00:41,760
everything data related that's like data

00:00:40,110 --> 00:00:44,220
infrastructure we deal with sort of the

00:00:41,760 --> 00:00:46,200
big data sort of stuff we're interested

00:00:44,220 --> 00:00:48,180
in advanced analytics machine learning

00:00:46,200 --> 00:00:50,220
and generally trying to like turn the

00:00:48,180 --> 00:00:53,040
data that we have into data product that

00:00:50,220 --> 00:00:58,190
we can use to sort of build into into

00:00:53,040 --> 00:00:58,190
Zendesk so you might be asking yourself

00:00:58,970 --> 00:01:03,830
why am i next slide is not coming up and

00:01:01,260 --> 00:01:06,780
I am not sure either

00:01:03,830 --> 00:01:10,290
okay

00:01:06,780 --> 00:01:11,670
so as I mentioned I work Resendez I'll

00:01:10,290 --> 00:01:13,140
give you a brief intro to Zendesk

00:01:11,670 --> 00:01:14,070
because it kind of gives the context for

00:01:13,140 --> 00:01:16,110
why we're building this thing in the

00:01:14,070 --> 00:01:18,150
first place so we build software that

00:01:16,110 --> 00:01:19,140
helps sort of manage customer support so

00:01:18,150 --> 00:01:21,690
they're kind of like the conversation

00:01:19,140 --> 00:01:23,970
between a customer and a business so

00:01:21,690 --> 00:01:25,530
when you have like you know like a

00:01:23,970 --> 00:01:26,640
support request is like you left your

00:01:25,530 --> 00:01:27,960
hat in an uber or something like that

00:01:26,640 --> 00:01:30,330
and you need to write to them and say

00:01:27,960 --> 00:01:31,350
hello I left my hat there it kind of

00:01:30,330 --> 00:01:32,850
goes through our software and helps

00:01:31,350 --> 00:01:35,850
manage that communication backwards and

00:01:32,850 --> 00:01:38,280
forwards support software kind of used

00:01:35,850 --> 00:01:39,780
to look like this it still does at some

00:01:38,280 --> 00:01:41,760
places this might be what you get when

00:01:39,780 --> 00:01:45,000
you contact your mobile telephone

00:01:41,760 --> 00:01:47,520
provider and ask for support it it's

00:01:45,000 --> 00:01:49,950
it's kind of it was common case that

00:01:47,520 --> 00:01:51,810
people use something really clunky like

00:01:49,950 --> 00:01:53,310
this that was it made sort of like the I

00:01:51,810 --> 00:01:55,950
guess like this sort of communication

00:01:53,310 --> 00:01:58,710
backwards and forwards quite I guess it

00:01:55,950 --> 00:02:01,890
didn't really flow right it kind of

00:01:58,710 --> 00:02:04,320
stifle the communication and so where we

00:02:01,890 --> 00:02:06,630
work we have a much nicer platform for

00:02:04,320 --> 00:02:08,819
sort of managing this this communication

00:02:06,630 --> 00:02:11,159
and integrates with a lot of sort of

00:02:08,819 --> 00:02:13,980
different channels like email and voice

00:02:11,159 --> 00:02:15,390
and Twitter and so forth and it's quite

00:02:13,980 --> 00:02:16,860
lightweight and it sort of it's pretty

00:02:15,390 --> 00:02:19,290
easy to use for both for people on both

00:02:16,860 --> 00:02:20,640
sides and and and the idea is that this

00:02:19,290 --> 00:02:22,350
leads to a better support experience

00:02:20,640 --> 00:02:26,220
both for the come

00:02:22,350 --> 00:02:28,590
the end for the customers so we have

00:02:26,220 --> 00:02:31,380
we're a software as a service company so

00:02:28,590 --> 00:02:33,120
we're on the Internet's and we have lots

00:02:31,380 --> 00:02:34,740
of customers about 50,000 they all have

00:02:33,120 --> 00:02:37,110
their own customers so we deal with a

00:02:34,740 --> 00:02:38,670
lot of tickets on a daily basis which I

00:02:37,110 --> 00:02:40,590
guess is why we come up with the need

00:02:38,670 --> 00:02:42,660
for a data team in the first place it's

00:02:40,590 --> 00:02:43,800
that we generate a lot of data so any

00:02:42,660 --> 00:02:45,390
you know people who are interested in

00:02:43,800 --> 00:02:46,830
taking like this this magnet this

00:02:45,390 --> 00:02:49,020
magnitude of records and trying to work

00:02:46,830 --> 00:02:50,220
out you know what we can do with it and

00:02:49,020 --> 00:02:51,660
how we can turn this into something

00:02:50,220 --> 00:02:55,200
useful what's effectively sort of the

00:02:51,660 --> 00:02:58,200
exhaust of our of our actual product and

00:02:55,200 --> 00:02:59,910
how we can make it do cool things and

00:02:58,200 --> 00:03:03,240
that's what our team is interested in

00:02:59,910 --> 00:03:05,580
doing so for this talk the motivation

00:03:03,240 --> 00:03:07,200
you can consider this use case you have

00:03:05,580 --> 00:03:10,110
a customer who's Marg and a support

00:03:07,200 --> 00:03:12,270
agent who's Homer you know Marge writes

00:03:10,110 --> 00:03:13,830
in and says hello am i thing broke and

00:03:12,270 --> 00:03:15,300
Homer writes back very quickly my

00:03:13,830 --> 00:03:16,590
apologies we'll fix the thing and marge

00:03:15,300 --> 00:03:18,150
is very happy at the end right and this

00:03:16,590 --> 00:03:19,790
is this is how you want your support

00:03:18,150 --> 00:03:22,170
experience to be right you want to

00:03:19,790 --> 00:03:23,670
contact the company that's involved and

00:03:22,170 --> 00:03:24,600
you want to tell them your problem you

00:03:23,670 --> 00:03:27,630
want to get an answer and you want to

00:03:24,600 --> 00:03:29,550
get it resolved very quickly the problem

00:03:27,630 --> 00:03:33,630
is that often it's a little bit more

00:03:29,550 --> 00:03:35,160
like this where you contact them and you

00:03:33,630 --> 00:03:36,630
don't hear anything back or you hear

00:03:35,160 --> 00:03:39,500
like that can automate a response it

00:03:36,630 --> 00:03:41,820
doesn't really answer your question and

00:03:39,500 --> 00:03:43,320
you end up sort of feeling a little bit

00:03:41,820 --> 00:03:48,330
disheartened or angry about the whole

00:03:43,320 --> 00:03:50,010
sort of process right and so as part of

00:03:48,330 --> 00:03:51,660
as part of having a Zendesk what you can

00:03:50,010 --> 00:03:53,010
do actually as you can send out a survey

00:03:51,660 --> 00:03:54,630
are at the end of the support experience

00:03:53,010 --> 00:03:55,920
and ask your customers how they felt

00:03:54,630 --> 00:03:57,420
about the process right whether they

00:03:55,920 --> 00:03:58,950
were happy or whether they were not

00:03:57,420 --> 00:03:59,850
right it's like a it's a yes/no sort of

00:03:58,950 --> 00:04:01,110
thing

00:03:59,850 --> 00:04:05,940
and we call this a customer satisfaction

00:04:01,110 --> 00:04:07,620
survey and you know as a result you can

00:04:05,940 --> 00:04:09,930
say I was happier I was not and you can

00:04:07,620 --> 00:04:11,580
give a reason but this kind of sets you

00:04:09,930 --> 00:04:13,020
up for like a pretty classic supervised

00:04:11,580 --> 00:04:14,880
learning sort of problem right so we've

00:04:13,020 --> 00:04:16,530
got a whole bunch of tickets and we have

00:04:14,880 --> 00:04:18,000
a really nice labeled data set where we

00:04:16,530 --> 00:04:20,160
know whether the people were happy or

00:04:18,000 --> 00:04:22,770
not and so you can come to this business

00:04:20,160 --> 00:04:24,900
question can we predict the customer

00:04:22,770 --> 00:04:26,610
satisfaction for a ticket given the

00:04:24,900 --> 00:04:28,130
lifecycle of the ticket so given the

00:04:26,610 --> 00:04:31,080
things we know about the ticket the text

00:04:28,130 --> 00:04:32,550
how you know how long an agent

00:04:31,080 --> 00:04:34,320
interacted with it how many people

00:04:32,550 --> 00:04:35,490
different people you had to speak to you

00:04:34,320 --> 00:04:36,360
know when they first responded to you

00:04:35,490 --> 00:04:38,610
that sort of

00:04:36,360 --> 00:04:40,379
and there's a bunch a bunch of sort of

00:04:38,610 --> 00:04:42,659
interesting reasons why you might want

00:04:40,379 --> 00:04:44,459
to know this one is that you could infer

00:04:42,659 --> 00:04:46,529
the Q health right so if you're a

00:04:44,459 --> 00:04:47,699
company and you have a thousand open

00:04:46,529 --> 00:04:49,799
tickets where you're trying to support

00:04:47,699 --> 00:04:51,629
your customers you can get if we could

00:04:49,799 --> 00:04:53,159
infer how likely each of those tickets

00:04:51,629 --> 00:04:54,539
are to get good or bad satisfaction you

00:04:53,159 --> 00:04:56,069
could sort of get an idea and how well

00:04:54,539 --> 00:04:58,469
your support organization is doing at

00:04:56,069 --> 00:04:59,699
this point in time right you could use

00:04:58,469 --> 00:05:01,229
it for early intervention so you could

00:04:59,699 --> 00:05:02,939
take that and you could say here are my

00:05:01,229 --> 00:05:04,589
10 worst tickets I need to manage to go

00:05:02,939 --> 00:05:07,019
and look at this right and - and sort of

00:05:04,589 --> 00:05:09,629
like and to intervene and you can also

00:05:07,019 --> 00:05:10,739
use it for automation so automations

00:05:09,629 --> 00:05:12,539
would be instead of getting a manager to

00:05:10,739 --> 00:05:14,069
look at it you could get you know a

00:05:12,539 --> 00:05:15,659
machine to look at it and say if the

00:05:14,069 --> 00:05:17,729
likelihood that they're gonna have a bad

00:05:15,659 --> 00:05:19,349
experience is greater than blah then we

00:05:17,729 --> 00:05:21,749
should go into a thing right and that's

00:05:19,349 --> 00:05:25,499
sort of our motivation and that's what

00:05:21,749 --> 00:05:26,519
the business question is but the the

00:05:25,499 --> 00:05:28,439
engineering question is slightly

00:05:26,519 --> 00:05:29,459
different right so the engineering

00:05:28,439 --> 00:05:31,999
question starts with a business question

00:05:29,459 --> 00:05:34,379
can we predict the customer satisfaction

00:05:31,999 --> 00:05:36,360
but given that we can can we do this for

00:05:34,379 --> 00:05:37,409
multiple customers right because each of

00:05:36,360 --> 00:05:39,719
our customers has sort of like a

00:05:37,409 --> 00:05:41,129
different sort of user base they have

00:05:39,719 --> 00:05:46,019
like different product ideas and so

00:05:41,129 --> 00:05:48,029
forth and it's you know building a model

00:05:46,019 --> 00:05:50,579
for these things is going to be quite

00:05:48,029 --> 00:05:51,869
quite different I guess and then the

00:05:50,579 --> 00:05:53,099
question is if we you know if we know we

00:05:51,869 --> 00:05:54,959
can build a model that works

00:05:53,099 --> 00:05:56,489
can we actually scale the building of

00:05:54,959 --> 00:05:58,279
these models and can we get that into

00:05:56,489 --> 00:06:04,289
production so can we integrate it back

00:05:58,279 --> 00:06:05,579
with with our existing product and I

00:06:04,289 --> 00:06:06,779
guess the talk today is about that

00:06:05,579 --> 00:06:10,169
process and what we went through to get

00:06:06,779 --> 00:06:15,659
there and it's all in Python which is

00:06:10,169 --> 00:06:17,999
why we're talking Python so can we

00:06:15,659 --> 00:06:19,289
predict the customer satisfaction it

00:06:17,999 --> 00:06:20,119
turns out we can we can actually do it

00:06:19,289 --> 00:06:23,369
quite well

00:06:20,119 --> 00:06:24,629
we get quite high accuracy for being

00:06:23,369 --> 00:06:26,789
able to do this for quite a number of

00:06:24,629 --> 00:06:27,869
our customers which is a good thing so

00:06:26,789 --> 00:06:31,069
it's a good starting point because it

00:06:27,869 --> 00:06:31,069
means we're sort of on the right path

00:06:33,930 --> 00:06:39,330
so as it's probably been mentioned you

00:06:36,570 --> 00:06:40,889
know earlier like the and I guess if

00:06:39,330 --> 00:06:43,380
you're not if you're not sort of clear

00:06:40,889 --> 00:06:44,580
with what you know what I mean by

00:06:43,380 --> 00:06:45,870
predict customer satisfaction is

00:06:44,580 --> 00:06:48,690
basically we want to take the stuff we

00:06:45,870 --> 00:06:50,070
know about the ticket we want to learn

00:06:48,690 --> 00:06:51,870
some sort of model we want to project it

00:06:50,070 --> 00:06:53,910
into some numerical space and learn a

00:06:51,870 --> 00:06:54,930
model that says if I draw a line all the

00:06:53,910 --> 00:06:56,070
tickets on this side of the line are

00:06:54,930 --> 00:06:57,479
good and all the tickets on that side of

00:06:56,070 --> 00:07:00,750
bad and that's like that's the result

00:06:57,479 --> 00:07:01,919
we're after but actually the process of

00:07:00,750 --> 00:07:03,360
doing that is quite a bit more difficult

00:07:01,919 --> 00:07:04,889
right so the first step is you've got to

00:07:03,360 --> 00:07:08,220
get the data so we need to get this out

00:07:04,889 --> 00:07:09,600
of our databases you know like having a

00:07:08,220 --> 00:07:11,340
data team is kind of an afterthought

00:07:09,600 --> 00:07:13,020
it's not our main product so our

00:07:11,340 --> 00:07:15,570
databases are designed like if most

00:07:13,020 --> 00:07:17,639
companies for you know for they're

00:07:15,570 --> 00:07:19,380
optimized for transactions so it's a

00:07:17,639 --> 00:07:20,669
normalized database so you need to go

00:07:19,380 --> 00:07:22,050
and get we don't just have all about

00:07:20,669 --> 00:07:23,039
ticket data in one spot we have to go

00:07:22,050 --> 00:07:25,130
and get it from a whole bunch of

00:07:23,039 --> 00:07:26,910
different places to stick it together

00:07:25,130 --> 00:07:29,729
the next thing you want to do is build

00:07:26,910 --> 00:07:30,990
the features and you know this was

00:07:29,729 --> 00:07:32,190
spoken about earlier this is like the

00:07:30,990 --> 00:07:33,180
idea of taking all the things you know

00:07:32,190 --> 00:07:35,130
about the tickets and turning it into

00:07:33,180 --> 00:07:36,330
something you want to give to what is

00:07:35,130 --> 00:07:38,460
effectively just a mathematical equation

00:07:36,330 --> 00:07:39,870
to say it's good or it's bad right and

00:07:38,460 --> 00:07:41,580
so if it's some things it's easy like

00:07:39,870 --> 00:07:42,960
how long a ticket was open this is a

00:07:41,580 --> 00:07:45,090
number you can put it straight in other

00:07:42,960 --> 00:07:47,340
things are a bit more difficult like you

00:07:45,090 --> 00:07:48,630
know the the conversation backwards and

00:07:47,340 --> 00:07:49,740
forwards like the actual text right you

00:07:48,630 --> 00:07:51,330
need to turn that into a representation

00:07:49,740 --> 00:07:53,400
that you know machine learning

00:07:51,330 --> 00:07:54,419
algorithms care about and once you've

00:07:53,400 --> 00:07:56,070
done that then you might want to choose

00:07:54,419 --> 00:07:57,270
a model like we heard about random

00:07:56,070 --> 00:07:58,229
forests before but there's a whole bunch

00:07:57,270 --> 00:08:01,139
of different models that you might want

00:07:58,229 --> 00:08:03,270
to use you want to train it you need

00:08:01,139 --> 00:08:05,550
some sort of success metric that says

00:08:03,270 --> 00:08:07,590
I'm doing kind of ok or I'm doing really

00:08:05,550 --> 00:08:09,570
well and then you iterate on that right

00:08:07,590 --> 00:08:13,620
and that's kind of like the the data

00:08:09,570 --> 00:08:15,510
science part of this process and to do

00:08:13,620 --> 00:08:16,949
that we use pandas which we've heard

00:08:15,510 --> 00:08:18,510
about before and I think we'll hear more

00:08:16,949 --> 00:08:20,400
about throughout the conference when you

00:08:18,510 --> 00:08:24,409
psych it low which is a great machine

00:08:20,400 --> 00:08:26,789
learning library and we use Jupiter hub

00:08:24,409 --> 00:08:28,560
so Jupiter for those who don't know is

00:08:26,789 --> 00:08:30,270
like the new incarnation of what was I

00:08:28,560 --> 00:08:32,250
Python notebooks and it's kind of this

00:08:30,270 --> 00:08:33,539
like this cool way of I guess

00:08:32,250 --> 00:08:35,490
documenting a workflow where you can

00:08:33,539 --> 00:08:36,990
kind of embed you know documentation and

00:08:35,490 --> 00:08:39,419
your code together you can execute that

00:08:36,990 --> 00:08:41,279
code within the document it's great for

00:08:39,419 --> 00:08:42,690
collaboration because people can really

00:08:41,279 --> 00:08:44,010
read a story of the analytics you did

00:08:42,690 --> 00:08:45,660
from from top to bottom right you know

00:08:44,010 --> 00:08:46,960
you can say I'm doing this because of

00:08:45,660 --> 00:08:48,730
blah and

00:08:46,960 --> 00:08:50,950
you can insert some images maybe you've

00:08:48,730 --> 00:08:52,990
got some code and so forth so it's quite

00:08:50,950 --> 00:08:57,100
nice and we use it pretty extensively at

00:08:52,990 --> 00:08:58,450
Zendesk okay so let's get to how we

00:08:57,100 --> 00:08:59,470
might actually like what this might

00:08:58,450 --> 00:09:02,710
actually look like as a machine learning

00:08:59,470 --> 00:09:03,760
problem so this might be the things you

00:09:02,710 --> 00:09:05,620
know about a ticket right you've got a

00:09:03,760 --> 00:09:07,870
subject a description a couple of

00:09:05,620 --> 00:09:09,220
comments and then some you know some

00:09:07,870 --> 00:09:10,540
numeric features that we know about it

00:09:09,220 --> 00:09:12,670
like the number of comments first reply

00:09:10,540 --> 00:09:14,830
time and maybe a categorical feature

00:09:12,670 --> 00:09:16,780
like the requesters user ID right so

00:09:14,830 --> 00:09:19,810
it's a number but it's not a number in

00:09:16,780 --> 00:09:21,280
the sense that like a meter 60 tall is

00:09:19,810 --> 00:09:22,270
bigger than a meter 50 tall but it's a

00:09:21,280 --> 00:09:23,680
number in the Senate it's a

00:09:22,270 --> 00:09:25,330
representation of a category right the

00:09:23,680 --> 00:09:26,710
number represents one user and it

00:09:25,330 --> 00:09:29,530
doesn't relate to the user has the ID

00:09:26,710 --> 00:09:30,490
one higher or lower so we need to be

00:09:29,530 --> 00:09:32,710
able to take this and we need to be able

00:09:30,490 --> 00:09:34,870
to node into features so our approach

00:09:32,710 --> 00:09:36,390
was to break was to break this up into

00:09:34,870 --> 00:09:38,380
sort of like three different workflows

00:09:36,390 --> 00:09:39,760
one dealing with the initial ticket

00:09:38,380 --> 00:09:42,490
details so at the beginning we just get

00:09:39,760 --> 00:09:43,660
in you know a subject in a description

00:09:42,490 --> 00:09:45,940
that's basically all we know about a

00:09:43,660 --> 00:09:47,200
ticket then there's like the textual

00:09:45,940 --> 00:09:48,400
component where which is like the

00:09:47,200 --> 00:09:49,780
conversation backwards and forwards

00:09:48,400 --> 00:09:51,220
these are the comments on the ticket and

00:09:49,780 --> 00:09:52,870
then we have like the metric sets which

00:09:51,220 --> 00:09:57,750
are like the the metrics the things that

00:09:52,870 --> 00:10:01,510
we can measure about a ticket okay so to

00:09:57,750 --> 00:10:03,220
to show you what one of these pipelines

00:10:01,510 --> 00:10:04,600
look like we can look a little bit

00:10:03,220 --> 00:10:05,560
deeper into the comments pipeline

00:10:04,600 --> 00:10:07,210
because it's kind of interesting because

00:10:05,560 --> 00:10:10,600
it deals with both numeric and textual

00:10:07,210 --> 00:10:12,220
features so we would take our comments

00:10:10,600 --> 00:10:13,540
pipeline which you can see the details

00:10:12,220 --> 00:10:14,920
at the top so we got two comments and

00:10:13,540 --> 00:10:16,240
any numeric thing which tells you how

00:10:14,920 --> 00:10:17,950
many comments there were on the ticket

00:10:16,240 --> 00:10:20,460
and we probably split that up into sort

00:10:17,950 --> 00:10:23,470
of two different flows I guess right so

00:10:20,460 --> 00:10:25,180
building features for from numeric

00:10:23,470 --> 00:10:26,260
features is pretty straightforward the

00:10:25,180 --> 00:10:27,250
simplest thing is you just take the

00:10:26,260 --> 00:10:29,020
number you're already having your done

00:10:27,250 --> 00:10:30,430
right you can do other sort of fancy

00:10:29,020 --> 00:10:32,230
things you could do maybe a polynomial

00:10:30,430 --> 00:10:35,440
expansion like take the square of the

00:10:32,230 --> 00:10:37,360
number or you know the cube you could

00:10:35,440 --> 00:10:39,520
pin it so you could maybe like you know

00:10:37,360 --> 00:10:41,200
create a bunch of bins and say is this

00:10:39,520 --> 00:10:42,730
did this ticket have more than five

00:10:41,200 --> 00:10:44,650
comments or less than five comments or

00:10:42,730 --> 00:10:46,270
did it have more than one comment or did

00:10:44,650 --> 00:10:47,590
it have exactly zero comments and you

00:10:46,270 --> 00:10:49,120
can you know generate binary features

00:10:47,590 --> 00:10:49,510
that sort of answer those questions for

00:10:49,120 --> 00:10:50,890
you

00:10:49,510 --> 00:10:53,080
all of those things are kind of

00:10:50,890 --> 00:10:53,860
interesting representations of a number

00:10:53,080 --> 00:10:55,180
that you might want to give to a

00:10:53,860 --> 00:10:57,010
classifier that can help you sort of

00:10:55,180 --> 00:10:59,910
learn better decision boundaries better

00:10:57,010 --> 00:11:03,759
ways of like working out whether tickets

00:10:59,910 --> 00:11:06,639
with text text this kind of it's not

00:11:03,759 --> 00:11:07,720
really trickier it's different in the

00:11:06,639 --> 00:11:08,529
respect that obviously you can't just

00:11:07,720 --> 00:11:10,029
take the text and give it to the

00:11:08,529 --> 00:11:13,060
classifier because classifiers want

00:11:10,029 --> 00:11:14,500
numbers not words so there's you know

00:11:13,060 --> 00:11:16,149
there's quite a few sort of general

00:11:14,500 --> 00:11:18,819
approaches to how you might convert text

00:11:16,149 --> 00:11:20,319
into into numbers like the you know the

00:11:18,819 --> 00:11:22,449
the simplest most classic is called bag

00:11:20,319 --> 00:11:24,310
of words where you effectively just make

00:11:22,449 --> 00:11:25,839
like a giant dictionary and you say like

00:11:24,310 --> 00:11:28,180
each column represents a word and we

00:11:25,839 --> 00:11:29,610
just put a 1 wherever we find a word in

00:11:28,180 --> 00:11:32,139
the document that we're interested in

00:11:29,610 --> 00:11:34,959
the other is tf-idf and hashing a kind

00:11:32,139 --> 00:11:37,329
of variant on that the thing that you

00:11:34,959 --> 00:11:38,350
find with texts data that's different to

00:11:37,329 --> 00:11:40,120
sort of the numeric data is the

00:11:38,350 --> 00:11:43,000
dimensionality tends to be quite high so

00:11:40,120 --> 00:11:45,069
when we run sort of our like our bag of

00:11:43,000 --> 00:11:46,420
words algorithm we generate I could jump

00:11:45,069 --> 00:11:48,610
a sparse matrix that has like about a

00:11:46,420 --> 00:11:49,810
million columns right so for each ticket

00:11:48,610 --> 00:11:51,009
you suddenly have like a million

00:11:49,810 --> 00:11:54,189
features where you could have ones or

00:11:51,009 --> 00:11:55,779
zeros whereas from the numeric from the

00:11:54,189 --> 00:11:56,980
numeric ones we don't we don't quite see

00:11:55,779 --> 00:11:58,870
that sort of expansion we can take the

00:11:56,980 --> 00:12:00,449
numbers themselves as they were so the

00:11:58,870 --> 00:12:02,620
feature space is quite a bit smaller

00:12:00,449 --> 00:12:03,670
okay so for each of those right we just

00:12:02,620 --> 00:12:05,199
put up the pipeline's and we would

00:12:03,670 --> 00:12:07,509
rebuild features right so for each

00:12:05,199 --> 00:12:09,310
ticket where each ticket was a row then

00:12:07,509 --> 00:12:10,930
each column is like a feature and we

00:12:09,310 --> 00:12:12,310
have you know features related to the

00:12:10,930 --> 00:12:13,990
number of comments and features related

00:12:12,310 --> 00:12:15,370
to the text and we can stick those back

00:12:13,990 --> 00:12:16,600
together to make like you know like a

00:12:15,370 --> 00:12:18,819
super feature for the ticket that we'll

00:12:16,600 --> 00:12:20,170
give to our classifier and that would

00:12:18,819 --> 00:12:22,209
look something a little like this right

00:12:20,170 --> 00:12:23,230
so we've got on the very left hand side

00:12:22,209 --> 00:12:27,040
we've got the comments data that comes

00:12:23,230 --> 00:12:28,569
in then we've got you know in this case

00:12:27,040 --> 00:12:32,019
we choose three different pipelines one

00:12:28,569 --> 00:12:33,699
where we're extracting tf-idf features

00:12:32,019 --> 00:12:35,199
when we were expecting we're doing

00:12:33,699 --> 00:12:36,459
hashing on the on the text as well and

00:12:35,199 --> 00:12:38,319
the bottom one is just numeric features

00:12:36,459 --> 00:12:39,519
and we take those we do like all the

00:12:38,319 --> 00:12:41,410
feature extraction that we combine it

00:12:39,519 --> 00:12:42,730
back together into sort of one big it's

00:12:41,410 --> 00:12:44,589
one big matrix and we can pass that on

00:12:42,730 --> 00:12:45,970
to our classifier so that we can you

00:12:44,589 --> 00:12:48,160
know train it and try and work out

00:12:45,970 --> 00:12:51,490
whether we can predict a ticket

00:12:48,160 --> 00:12:54,040
satisfaction or not if you look at that

00:12:51,490 --> 00:12:55,180
sort of in encode scikit-learn gives you

00:12:54,040 --> 00:12:58,480
a really nice ability to build these

00:12:55,180 --> 00:13:00,279
sort of pipelines as well right so this

00:12:58,480 --> 00:13:01,600
is like the code equivalent of of the

00:13:00,279 --> 00:13:03,490
diagram we saw on the last page where

00:13:01,600 --> 00:13:05,079
you've got the top line is like select

00:13:03,490 --> 00:13:07,630
features out of the out of the data that

00:13:05,079 --> 00:13:09,370
we have then do a feature Union so like

00:13:07,630 --> 00:13:10,750
a union on like a bunch of different

00:13:09,370 --> 00:13:11,949
feature extraction things we want where

00:13:10,750 --> 00:13:12,420
we can say which column we want to

00:13:11,949 --> 00:13:14,130
extract

00:13:12,420 --> 00:13:16,110
from and what we want to do with it so

00:13:14,130 --> 00:13:18,269
we've got like a tf-idf transformer a

00:13:16,110 --> 00:13:19,410
hash transformer bidding transformers we

00:13:18,269 --> 00:13:21,149
stick all of that together and we throw

00:13:19,410 --> 00:13:23,100
it on a model in this case logistic

00:13:21,149 --> 00:13:23,610
regression because it works and it's

00:13:23,100 --> 00:13:28,019
super simple

00:13:23,610 --> 00:13:29,250
basically so some of this stuff is

00:13:28,019 --> 00:13:31,620
straight out of scikit-learn some of the

00:13:29,250 --> 00:13:33,000
stuff we've sort of written on top so

00:13:31,620 --> 00:13:35,220
we've sort of extended a little bit

00:13:33,000 --> 00:13:36,750
written some of our own transformers our

00:13:35,220 --> 00:13:38,250
own way of building pipelines a little

00:13:36,750 --> 00:13:43,760
bit to make this process a little bit

00:13:38,250 --> 00:13:45,810
easier for us to iterate on okay so

00:13:43,760 --> 00:13:47,250
that's kind of what the comment pipeline

00:13:45,810 --> 00:13:49,019
looks like but we've got the initial

00:13:47,250 --> 00:13:50,010
ticket and we've got the ticket metrics

00:13:49,019 --> 00:13:51,839
so we've got these other two pipelines

00:13:50,010 --> 00:13:53,130
and we've got to kind of do the same

00:13:51,839 --> 00:13:54,899
thing with all of those and we want to

00:13:53,130 --> 00:13:57,029
bring all of that back together and that

00:13:54,899 --> 00:13:58,320
would look a little bit like this very

00:13:57,029 --> 00:14:00,060
similar to the last diagram right we

00:13:58,320 --> 00:14:01,050
have all of our ticket data they get

00:14:00,060 --> 00:14:02,790
split up into each of the three

00:14:01,050 --> 00:14:04,680
different pipelines where we sort of

00:14:02,790 --> 00:14:06,360
where we do feature extraction we train

00:14:04,680 --> 00:14:08,040
a classifier and their classifier learns

00:14:06,360 --> 00:14:10,889
how to predict the ticket satisfaction

00:14:08,040 --> 00:14:12,600
based on those features and we do that

00:14:10,889 --> 00:14:14,279
individually for the initial information

00:14:12,600 --> 00:14:16,380
the comments and the metrics and then we

00:14:14,279 --> 00:14:17,850
combine them back together using just in

00:14:16,380 --> 00:14:19,709
in our case actually just taking the

00:14:17,850 --> 00:14:21,680
mean and often that works quite well

00:14:19,709 --> 00:14:23,610
this is another form of ensemble

00:14:21,680 --> 00:14:24,839
classification it's similar to like

00:14:23,610 --> 00:14:25,709
random forests what we heard before so

00:14:24,839 --> 00:14:27,269
we've got a bunch of different

00:14:25,709 --> 00:14:28,589
classifiers we asked them all what they

00:14:27,269 --> 00:14:29,519
think we stick it together and we sort

00:14:28,589 --> 00:14:29,970
of go we'll take the average and that's

00:14:29,519 --> 00:14:33,990
pretty good

00:14:29,970 --> 00:14:35,010
and it turns out that's quite okay the

00:14:33,990 --> 00:14:36,149
interesting thing is there's a lot of

00:14:35,010 --> 00:14:37,920
parameters to tune here and I'll come

00:14:36,149 --> 00:14:39,390
back to that later but each of the

00:14:37,920 --> 00:14:40,920
feature extractors each of the

00:14:39,390 --> 00:14:42,600
classifiers themselves all have their

00:14:40,920 --> 00:14:43,680
own parameters on top of that there's

00:14:42,600 --> 00:14:45,180
like the process of choosing which

00:14:43,680 --> 00:14:47,399
features you might actually want to use

00:14:45,180 --> 00:14:51,570
right so there's kind of a lot of moving

00:14:47,399 --> 00:14:52,529
parts a lot of things to chew okay which

00:14:51,570 --> 00:14:56,240
brings me to the next thing

00:14:52,529 --> 00:14:59,459
can we automatically tune your models so

00:14:56,240 --> 00:15:01,170
using what I've described so far it's we

00:14:59,459 --> 00:15:03,180
can build a model that will predict

00:15:01,170 --> 00:15:04,620
quite well for a single customer we

00:15:03,180 --> 00:15:05,819
started with ourselves like whether we

00:15:04,620 --> 00:15:07,199
can predict the satisfaction of our

00:15:05,819 --> 00:15:08,670
customers when they communicate with us

00:15:07,199 --> 00:15:11,399
and that worked quite well so then the

00:15:08,670 --> 00:15:13,529
next question is okay so so far it's

00:15:11,399 --> 00:15:14,430
been me sitting at a computer or me and

00:15:13,529 --> 00:15:16,139
some colleagues sitting at a computer

00:15:14,430 --> 00:15:17,160
and like tuning these algorithms and

00:15:16,139 --> 00:15:18,240
saying well what if we do this instead

00:15:17,160 --> 00:15:20,730
of that and what if we change this

00:15:18,240 --> 00:15:21,899
parameter this and and so forth but if

00:15:20,730 --> 00:15:23,519
we want to scale this to do it for

00:15:21,899 --> 00:15:25,209
50,000 of our customers obviously we

00:15:23,519 --> 00:15:26,679
can't hire that many people to sit

00:15:25,209 --> 00:15:27,819
of computers right and shooing these

00:15:26,679 --> 00:15:35,429
models so we need to automate the

00:15:27,819 --> 00:15:37,929
process and the scale of the problem is

00:15:35,429 --> 00:15:39,040
maybe not a parent if you sort of look

00:15:37,929 --> 00:15:40,839
at it at the beginning but if you think

00:15:39,040 --> 00:15:42,089
for example that we have 50 different

00:15:40,839 --> 00:15:44,529
parameters that we might want to tune

00:15:42,089 --> 00:15:45,610
and maybe we have five options for each

00:15:44,529 --> 00:15:46,540
of these parameters like five values

00:15:45,610 --> 00:15:48,040
that we can choose some of them have

00:15:46,540 --> 00:15:49,329
more or some of them have less but if we

00:15:48,040 --> 00:15:50,889
go with the number five that gives us

00:15:49,329 --> 00:15:52,209
this number at the bottom possible

00:15:50,889 --> 00:15:53,529
combinations of how we could surf these

00:15:52,209 --> 00:15:57,429
models right so this is a really big

00:15:53,529 --> 00:15:58,749
number you can't you can't we saw an

00:15:57,429 --> 00:15:59,860
approach of like tuning parameters

00:15:58,749 --> 00:16:01,240
earlier which is called grid search

00:15:59,860 --> 00:16:03,220
where you go through each possible

00:16:01,240 --> 00:16:04,929
combination this is not an option for us

00:16:03,220 --> 00:16:07,509
right because the combination is massive

00:16:04,929 --> 00:16:10,300
and actually this combination is so big

00:16:07,509 --> 00:16:14,110
that it caused it broke scikit-learn

00:16:10,300 --> 00:16:16,029
when we started doing it because because

00:16:14,110 --> 00:16:18,910
basically if you if you try to use

00:16:16,029 --> 00:16:21,009
scikit-learn has like a thing called a a

00:16:18,910 --> 00:16:22,209
grid search right which will do this for

00:16:21,009 --> 00:16:23,410
you but the problem is that it tries to

00:16:22,209 --> 00:16:25,269
build all of these parameter

00:16:23,410 --> 00:16:28,179
combinations in advance before actually

00:16:25,269 --> 00:16:29,309
starting the search if you don't tweak

00:16:28,179 --> 00:16:32,199
it properly there's a bug in the code

00:16:29,309 --> 00:16:33,279
and basically we were finding we'd kick

00:16:32,199 --> 00:16:34,269
off the thing we like all right it's

00:16:33,279 --> 00:16:37,809
gonna run and then we just run out of

00:16:34,269 --> 00:16:39,100
memory so you can kind of get around

00:16:37,809 --> 00:16:40,389
that and in a weird way hopefully

00:16:39,100 --> 00:16:44,230
that'll be fixed soon I think there's a

00:16:40,389 --> 00:16:45,910
fixing in master so basically the way

00:16:44,230 --> 00:16:48,369
the way we go and we build these models

00:16:45,910 --> 00:16:49,389
then is we instead of using a grid

00:16:48,369 --> 00:16:51,309
search we want use what's called

00:16:49,389 --> 00:16:53,499
randomized cross-validation so basically

00:16:51,309 --> 00:16:54,879
instead of trying to search all of the

00:16:53,499 --> 00:16:57,369
possible parameter combinations we just

00:16:54,879 --> 00:16:59,529
draw a random sample of them n random

00:16:57,369 --> 00:17:01,660
samples in our case maybe 10 20 30

00:16:59,529 --> 00:17:03,009
whatever it might be right so we go for

00:17:01,660 --> 00:17:04,419
each of the parameters we'll just draw a

00:17:03,009 --> 00:17:05,919
random value of it we'll stick it

00:17:04,419 --> 00:17:07,510
together we'll put that into the

00:17:05,919 --> 00:17:08,110
classifier we'll train it we'll see how

00:17:07,510 --> 00:17:09,880
it goes

00:17:08,110 --> 00:17:11,319
we'll do it again we'll do that ten

00:17:09,880 --> 00:17:12,669
times with random draws and then we'll

00:17:11,319 --> 00:17:14,230
just take whichever ones the best and we

00:17:12,669 --> 00:17:18,100
say that's a pretty good estimate of how

00:17:14,230 --> 00:17:20,020
well we can do so in that that's kind of

00:17:18,100 --> 00:17:21,490
what you see here so we have the data it

00:17:20,020 --> 00:17:23,589
goes in we've got our three pipelines

00:17:21,490 --> 00:17:25,179
where we sort of tune the parameters we

00:17:23,589 --> 00:17:26,500
you know do that ten times twenty times

00:17:25,179 --> 00:17:28,360
and we say okay this is the best model

00:17:26,500 --> 00:17:31,480
we found we take that one we combine

00:17:28,360 --> 00:17:32,890
them together and we save the model we

00:17:31,480 --> 00:17:34,630
say that's the best and and that's

00:17:32,890 --> 00:17:36,730
almost good except that if you do that

00:17:34,630 --> 00:17:37,080
today and you do it tomorrow and you're

00:17:36,730 --> 00:17:39,840
only

00:17:37,080 --> 00:17:42,809
say 10 times out of you know this space

00:17:39,840 --> 00:17:43,919
of five to the 50 possible parameter

00:17:42,809 --> 00:17:45,149
combinations there's a chance that you

00:17:43,919 --> 00:17:46,409
actually what you come up with tomorrow

00:17:45,149 --> 00:17:48,090
is worse than what you had yesterday so

00:17:46,409 --> 00:17:50,129
we have to add a step that basically

00:17:48,090 --> 00:17:51,419
says compare the new model that we found

00:17:50,129 --> 00:17:53,039
to the last model that we thought was

00:17:51,419 --> 00:17:54,570
the best and see and we just get the

00:17:53,039 --> 00:17:56,009
best one out of that so it allows us to

00:17:54,570 --> 00:17:57,600
sort of iteratively search this

00:17:56,009 --> 00:18:01,950
parameter space really really slowly

00:17:57,600 --> 00:18:04,889
over time and and this process basically

00:18:01,950 --> 00:18:06,600
allows us then to you know to just scale

00:18:04,889 --> 00:18:08,429
our our model search though so that we

00:18:06,600 --> 00:18:10,710
can do this for all of our customers

00:18:08,429 --> 00:18:13,200
given a reasonable amount of computing

00:18:10,710 --> 00:18:15,359
resources right so this whole process

00:18:13,200 --> 00:18:18,359
that we've done so far gave birth to a

00:18:15,359 --> 00:18:20,609
project called selfie for self learning

00:18:18,359 --> 00:18:21,899
machine learning and and basically this

00:18:20,609 --> 00:18:23,299
is kind of like a framework or a wrapper

00:18:21,899 --> 00:18:25,950
that we've built on top of scikit-learn

00:18:23,299 --> 00:18:27,480
it allows you to sort of specify like

00:18:25,950 --> 00:18:28,529
these combinations of pipelines that

00:18:27,480 --> 00:18:30,269
I've been talking about like the way

00:18:28,529 --> 00:18:31,739
that you have your original data you

00:18:30,269 --> 00:18:33,539
might split it up into subsections you

00:18:31,739 --> 00:18:35,159
might split those up further and have a

00:18:33,539 --> 00:18:37,499
whole bunch of feature transformers a

00:18:35,159 --> 00:18:38,639
whole you know a quite large parameter

00:18:37,499 --> 00:18:40,919
space that you might want to search and

00:18:38,639 --> 00:18:42,570
you can just sort of say here's a here's

00:18:40,919 --> 00:18:44,909
a relatively chunky computer just go and

00:18:42,570 --> 00:18:46,169
find me a pretty good version of this

00:18:44,909 --> 00:18:49,889
model without actually having to go into

00:18:46,169 --> 00:18:51,749
it yourself yeah so that was quite fun

00:18:49,889 --> 00:18:56,279
to build I don't know it's it's trying

00:18:51,749 --> 00:18:57,570
to be pretty useful so far and so okay

00:18:56,279 --> 00:18:59,009
so we've gotten to the part where we are

00:18:57,570 --> 00:19:01,049
like okay can we do the problem we can

00:18:59,009 --> 00:19:03,330
predict a customer satisfaction given

00:19:01,049 --> 00:19:05,129
you know the information on ticket we

00:19:03,330 --> 00:19:06,269
know that we can we can do this from

00:19:05,129 --> 00:19:08,369
multiple accounts without having to

00:19:06,269 --> 00:19:09,779
scale manpower very much but the

00:19:08,369 --> 00:19:11,340
question is like how do we build all of

00:19:09,779 --> 00:19:13,739
these models right it's still you know

00:19:11,340 --> 00:19:16,049
building 50,000 models it's relatively

00:19:13,739 --> 00:19:17,730
computationally expensive you know how

00:19:16,049 --> 00:19:19,169
where do we get the computational

00:19:17,730 --> 00:19:21,989
resources and how do we make sure this

00:19:19,169 --> 00:19:25,440
works and basically we abuse Hadoop to

00:19:21,989 --> 00:19:26,669
do this so we have an existing Hadoop

00:19:25,440 --> 00:19:30,059
infrastructure that we use for a lot of

00:19:26,669 --> 00:19:31,499
our sort of ETL and tasks and various

00:19:30,059 --> 00:19:33,480
other sort of like aggregations and

00:19:31,499 --> 00:19:35,669
statistics and other things that we sort

00:19:33,480 --> 00:19:37,289
of integrate back into our product this

00:19:35,669 --> 00:19:39,779
is built on using Casca log which is

00:19:37,289 --> 00:19:41,190
sort of like it's a closure framework

00:19:39,779 --> 00:19:42,419
that sits on top of cascading which is

00:19:41,190 --> 00:19:43,679
basically a way of being able to use

00:19:42,419 --> 00:19:46,350
MapReduce without ever having to write

00:19:43,679 --> 00:19:48,160
Java which is a good thing and we

00:19:46,350 --> 00:19:51,040
basically we abuse

00:19:48,160 --> 00:19:56,260
the resilience I guess in job tracking

00:19:51,040 --> 00:19:57,670
of her due to run our Python processes

00:19:56,260 --> 00:19:59,560
individually on each of our data nodes

00:19:57,670 --> 00:20:00,490
so what that means is basically we used

00:19:59,560 --> 00:20:02,200
to dupe to get all the data together

00:20:00,490 --> 00:20:03,850
send it to a bunch of data nodes and

00:20:02,200 --> 00:20:05,890
then start up our Python processes

00:20:03,850 --> 00:20:07,530
outside of the MapReduce framework they

00:20:05,890 --> 00:20:09,640
find a good model they save it to disk

00:20:07,530 --> 00:20:11,320
using pickle I don't know if you've

00:20:09,640 --> 00:20:13,060
heard of pickle before probably maybe

00:20:11,320 --> 00:20:15,610
it's a really nice way of being able to

00:20:13,060 --> 00:20:17,890
save things to disk in Python it works

00:20:15,610 --> 00:20:19,300
with scikit-learn models so you can

00:20:17,890 --> 00:20:20,440
train your model you can save it to disk

00:20:19,300 --> 00:20:22,450
and you can open it up somewhere else

00:20:20,440 --> 00:20:23,800
and use it which is a very useful thing

00:20:22,450 --> 00:20:27,540
if you want to train a model here and

00:20:23,800 --> 00:20:30,370
maybe use it in a product over there so

00:20:27,540 --> 00:20:31,840
so basically MapReduce just starts up

00:20:30,370 --> 00:20:33,310
our process our model gets trained it

00:20:31,840 --> 00:20:35,320
gets sucked back into HDFS and then we

00:20:33,310 --> 00:20:36,670
have you know we have like you in our

00:20:35,320 --> 00:20:38,860
Hadoop cluster we have like all of the

00:20:36,670 --> 00:20:40,270
information about the the various models

00:20:38,860 --> 00:20:41,740
that we've trained we've got logging and

00:20:40,270 --> 00:20:44,050
so forth and the thing that Hadoop gives

00:20:41,740 --> 00:20:46,210
us is scheduling failure handling and

00:20:44,050 --> 00:20:47,380
job retries right so if our we're trying

00:20:46,210 --> 00:20:49,000
to build you know these thousands of

00:20:47,380 --> 00:20:50,140
models if one of them dies we don't want

00:20:49,000 --> 00:20:52,000
to have to care about that personally

00:20:50,140 --> 00:20:53,980
and and we don't because who dude

00:20:52,000 --> 00:20:55,810
manages that for us it knows to restart

00:20:53,980 --> 00:20:57,160
the training maybe somewhere else if the

00:20:55,810 --> 00:20:59,080
dabber node has gone down or so forth

00:20:57,160 --> 00:21:01,360
right and and this allows us basically

00:20:59,080 --> 00:21:02,650
to just ship just save the models and

00:21:01,360 --> 00:21:04,510
then ship them around somewhere else so

00:21:02,650 --> 00:21:07,660
it sort of gives us the scaling that we

00:21:04,510 --> 00:21:09,370
need another cool Python thing that we

00:21:07,660 --> 00:21:11,140
actually use to schedule this is called

00:21:09,370 --> 00:21:13,270
airflow it's built by Airbnb and it's

00:21:11,140 --> 00:21:17,700
like a task scheduler I guess and we use

00:21:13,270 --> 00:21:21,360
that to like to run all of our all about

00:21:17,700 --> 00:21:29,050
all of our jobs on our Hadoop framework

00:21:21,360 --> 00:21:30,370
ok production so we know we can build a

00:21:29,050 --> 00:21:32,230
model that works when you know we can do

00:21:30,370 --> 00:21:33,490
that for lots of customers we even think

00:21:32,230 --> 00:21:34,570
we know where we might actually build

00:21:33,490 --> 00:21:36,250
the models but then the question is how

00:21:34,570 --> 00:21:38,320
the hell do you take a machine learning

00:21:36,250 --> 00:21:41,020
model that's written in Python and make

00:21:38,320 --> 00:21:43,690
that accessible to our product which is

00:21:41,020 --> 00:21:44,640
mostly built in Ruby there's a good

00:21:43,690 --> 00:21:46,630
question

00:21:44,640 --> 00:21:49,870
mostly we want it to be kind of magic

00:21:46,630 --> 00:21:50,980
right we want we don't want the rest of

00:21:49,870 --> 00:21:53,140
our product have to know about it we

00:21:50,980 --> 00:21:55,510
effectively want an API that you can hit

00:21:53,140 --> 00:21:56,860
and say dear API here is a bunch of

00:21:55,510 --> 00:21:58,240
ticket information you tell me how

00:21:56,860 --> 00:21:59,380
likely it is that this ticket is going

00:21:58,240 --> 00:22:00,830
to have good satisfaction or bad

00:21:59,380 --> 00:22:02,990
satisfaction

00:22:00,830 --> 00:22:05,180
and our API in the background is going

00:22:02,990 --> 00:22:06,290
to load our pickled model you know it's

00:22:05,180 --> 00:22:07,730
gonna run the data through the model

00:22:06,290 --> 00:22:09,050
it's going to make a prediction it's

00:22:07,730 --> 00:22:10,670
going to send the information back and

00:22:09,050 --> 00:22:12,620
the rest of the Ruby world will sort of

00:22:10,670 --> 00:22:16,190
be none the wiser of what's going on in

00:22:12,620 --> 00:22:17,480
the background so we build a thing

00:22:16,190 --> 00:22:18,590
called crystal ball because you can ask

00:22:17,480 --> 00:22:21,770
your questions and it gives you answers

00:22:18,590 --> 00:22:23,570
about the future and it is basically

00:22:21,770 --> 00:22:24,830
this cool interface that hides all of

00:22:23,570 --> 00:22:26,300
the hard work that we've done in the

00:22:24,830 --> 00:22:27,500
past and it's actually there's not even

00:22:26,300 --> 00:22:29,060
really much to talk about here because

00:22:27,500 --> 00:22:31,270
it's super simple it's like it is the

00:22:29,060 --> 00:22:34,130
most simple flask API you've ever seen

00:22:31,270 --> 00:22:36,320
except for a couple of caveats it's got

00:22:34,130 --> 00:22:37,250
some some some interesting caching on

00:22:36,320 --> 00:22:39,770
there because these models are pretty

00:22:37,250 --> 00:22:42,110
big on disk in the hundreds of megabytes

00:22:39,770 --> 00:22:43,370
so we don't want to have to each time we

00:22:42,110 --> 00:22:44,180
get a request for a different account we

00:22:43,370 --> 00:22:45,740
don't wanna have to go and load that

00:22:44,180 --> 00:22:50,750
accounts model off disk because it's

00:22:45,740 --> 00:22:51,980
gonna be slow so and and the sort of

00:22:50,750 --> 00:22:53,000
servers we run it on are probably a

00:22:51,980 --> 00:22:54,140
little bit different to our normal web

00:22:53,000 --> 00:22:56,150
servers just because of the memory

00:22:54,140 --> 00:22:57,530
requirements so but outside of using a

00:22:56,150 --> 00:22:58,880
bit of caching running it on a web

00:22:57,530 --> 00:23:00,290
server that's slightly bigger than what

00:22:58,880 --> 00:23:01,670
our normal web servers look like this is

00:23:00,290 --> 00:23:03,350
basically you know a really

00:23:01,670 --> 00:23:07,220
straightforward API and then the reason

00:23:03,350 --> 00:23:08,450
that it's so easy to integrate super to

00:23:07,220 --> 00:23:09,890
build such a straightforward API is

00:23:08,450 --> 00:23:10,790
because you know the Python framework

00:23:09,890 --> 00:23:12,080
from the start to the end you know

00:23:10,790 --> 00:23:15,470
everything is compatible right we just

00:23:12,080 --> 00:23:16,910
write out a pickle Python running with

00:23:15,470 --> 00:23:18,530
flask also knows how to read this thing

00:23:16,910 --> 00:23:19,520
it doesn't have to know how we built the

00:23:18,530 --> 00:23:22,310
model it just sort of imports

00:23:19,520 --> 00:23:26,150
scikit-learn it does its thing which is

00:23:22,310 --> 00:23:27,230
pretty nice so I posted a response looks

00:23:26,150 --> 00:23:29,120
something like this a whole bunch of

00:23:27,230 --> 00:23:31,010
information you know in a JSON document

00:23:29,120 --> 00:23:32,870
we get back a thing saying probability

00:23:31,010 --> 00:23:35,480
of good customer of good customer

00:23:32,870 --> 00:23:37,070
satisfaction is 0.99 so that's like you

00:23:35,480 --> 00:23:41,480
know that's that's a happy customer I

00:23:37,070 --> 00:23:44,060
guess ok so that's kind of what we've

00:23:41,480 --> 00:23:46,940
done the future is we would like to

00:23:44,060 --> 00:23:48,860
open-source selfie get Pete are the

00:23:46,940 --> 00:23:50,120
people to start using it get them to try

00:23:48,860 --> 00:23:52,100
and benchmark it we'd like to play with

00:23:50,120 --> 00:23:53,480
Kegel competitions using it we haven't

00:23:52,100 --> 00:23:54,650
really had the chance to do that yet but

00:23:53,480 --> 00:23:56,360
it's on the list of things to do and

00:23:54,650 --> 00:23:57,560
hopefully once other people start using

00:23:56,360 --> 00:24:01,310
it they can also give us feedback on how

00:23:57,560 --> 00:24:02,510
we can make it better right and another

00:24:01,310 --> 00:24:04,190
interesting thing I think to do in the

00:24:02,510 --> 00:24:05,390
future is also doing model

00:24:04,190 --> 00:24:06,650
interpretation so the moment we can

00:24:05,390 --> 00:24:09,470
build a model that says you know given

00:24:06,650 --> 00:24:10,610
this ticket we think the probability of

00:24:09,470 --> 00:24:12,620
it you know having good customer

00:24:10,610 --> 00:24:14,240
satisfaction is blah but we can't really

00:24:12,620 --> 00:24:16,610
tell you why we

00:24:14,240 --> 00:24:17,660
so you know that and there's certain

00:24:16,610 --> 00:24:19,670
ways that you can interrogate these

00:24:17,660 --> 00:24:20,840
models basically to get a good feeling

00:24:19,670 --> 00:24:25,130
of sort of how they respond to different

00:24:20,840 --> 00:24:26,480
parameters changing and so forth so I'm

00:24:25,130 --> 00:24:29,180
almost done that says thanks but I have

00:24:26,480 --> 00:24:30,200
a couple more points so the question is

00:24:29,180 --> 00:24:32,600
how do we manage to build this in the

00:24:30,200 --> 00:24:34,010
quarter I think that the it's kind of a

00:24:32,600 --> 00:24:35,059
lot of work to get done in a quarter and

00:24:34,010 --> 00:24:36,320
the main reason that we were even able

00:24:35,059 --> 00:24:38,090
to come close because we didn't quite

00:24:36,320 --> 00:24:38,900
make it but we were so close and I had

00:24:38,090 --> 00:24:40,190
to give him the title of this talk

00:24:38,900 --> 00:24:42,920
before I actually knew whether we were

00:24:40,190 --> 00:24:44,360
going to finish him up is because

00:24:42,920 --> 00:24:46,760
everything was in Python and it really

00:24:44,360 --> 00:24:49,100
like I think just the you know the

00:24:46,760 --> 00:24:50,750
ability to you know to build your models

00:24:49,100 --> 00:24:51,890
over here and ship all of that stuff

00:24:50,750 --> 00:24:53,330
around the place to get it to run on

00:24:51,890 --> 00:24:54,620
Hadoop all and within one framework

00:24:53,330 --> 00:24:56,900
allowed us to sort of iterate very

00:24:54,620 --> 00:24:58,550
quickly on building these models and and

00:24:56,900 --> 00:25:00,080
the great data data stack that's

00:24:58,550 --> 00:25:02,150
available in Python script pandas and

00:25:00,080 --> 00:25:03,170
scikit-learn and numpy and Syfy gives

00:25:02,150 --> 00:25:07,100
you all the tools you need to build

00:25:03,170 --> 00:25:08,630
these sorts of things and the idea is

00:25:07,100 --> 00:25:10,400
that you know this is our first pass but

00:25:08,630 --> 00:25:11,720
in the future it gives us a platform

00:25:10,400 --> 00:25:13,280
that we're going to be able to build on

00:25:11,720 --> 00:25:14,360
well next time we go to build a new

00:25:13,280 --> 00:25:16,550
machine learning model to predict

00:25:14,360 --> 00:25:17,840
something else all of the infrastructure

00:25:16,550 --> 00:25:20,059
is there we just need to build the thing

00:25:17,840 --> 00:25:21,200
that builds the model and it will just

00:25:20,059 --> 00:25:23,450
flow through and come out through our

00:25:21,200 --> 00:25:26,330
API and life will be good

00:25:23,450 --> 00:25:27,860
aside from that we're hiring so if you

00:25:26,330 --> 00:25:29,540
want a data job come come and see me for

00:25:27,860 --> 00:25:32,350
emailing or look at the jobs thing

00:25:29,540 --> 00:25:32,350
that's it

00:25:48,630 --> 00:25:58,090
thanks a talk how much do you find the

00:25:52,269 --> 00:25:59,769
models change between customers they

00:25:58,090 --> 00:26:00,880
change a fair bit between customers but

00:25:59,769 --> 00:26:03,580
to be fair they change a fair bit

00:26:00,880 --> 00:26:04,779
between any given run on training on the

00:26:03,580 --> 00:26:10,090
same customer because we're sampling

00:26:04,779 --> 00:26:11,799
from such a huge parameter space so it's

00:26:10,090 --> 00:26:13,630
something that we haven't investigated a

00:26:11,799 --> 00:26:14,649
huge amount but there there's different

00:26:13,630 --> 00:26:16,990
there's definitely differences between

00:26:14,649 --> 00:26:19,000
the parameters for sure how much of that

00:26:16,990 --> 00:26:20,289
is meaningful and how much is not so not

00:26:19,000 --> 00:26:21,490
so important to the end result of the

00:26:20,289 --> 00:26:27,870
model I think we haven't dug into that

00:26:21,490 --> 00:26:27,870
far enough yet okay any other questions

00:26:31,679 --> 00:26:35,919
on a related question it sounds like

00:26:34,690 --> 00:26:37,269
you're keeping the the data from

00:26:35,919 --> 00:26:39,070
different companies very separate as

00:26:37,269 --> 00:26:42,460
that for practical reasons or for

00:26:39,070 --> 00:26:45,340
commercial sensitivity probably well

00:26:42,460 --> 00:26:46,870
yeah both I think obviously commercial

00:26:45,340 --> 00:26:48,100
sensitivity you know it's not our data

00:26:46,870 --> 00:26:49,779
to share with other people obviously

00:26:48,100 --> 00:26:50,950
even sharing in a model that no one else

00:26:49,779 --> 00:26:53,409
is going to see so we don't want to do

00:26:50,950 --> 00:26:54,909
that on top of that I think the you know

00:26:53,409 --> 00:26:56,200
the sort of questions like the tick the

00:26:54,909 --> 00:26:57,460
support tickets that you would get if

00:26:56,200 --> 00:26:58,990
you're a university are very different

00:26:57,460 --> 00:27:00,970
to the support to kiss you again if

00:26:58,990 --> 00:27:02,409
you're Airbnb right so and probably the

00:27:00,970 --> 00:27:03,700
expectations of the people submitting

00:27:02,409 --> 00:27:04,809
those things and therefore the drug the

00:27:03,700 --> 00:27:05,950
things that will drive whether it's good

00:27:04,809 --> 00:27:07,299
or bad satisfaction will also be

00:27:05,950 --> 00:27:08,500
different so I think like the

00:27:07,299 --> 00:27:11,740
differences in domain between our

00:27:08,500 --> 00:27:13,179
customers probably mean that that having

00:27:11,740 --> 00:27:16,350
like a generalized model is probably not

00:27:13,179 --> 00:27:16,350
going to work that great

00:27:18,430 --> 00:27:23,750
yeah with respect to the parameter

00:27:22,010 --> 00:27:27,680
search have you considered some kind of

00:27:23,750 --> 00:27:28,790
a gradient descent method or yes yes we

00:27:27,680 --> 00:27:30,590
haven't done it yet though I met most

00:27:28,790 --> 00:27:33,260
mostly because this is out of the box in

00:27:30,590 --> 00:27:34,430
scikit-learn and it works there's I mean

00:27:33,260 --> 00:27:36,290
there's there's a whole bunch of

00:27:34,430 --> 00:27:38,090
literature around like better ways than

00:27:36,290 --> 00:27:40,760
this to look for parameters in a huge

00:27:38,090 --> 00:27:41,900
parameter space I think when we get to

00:27:40,760 --> 00:27:44,890
the point that we need to do that it's

00:27:41,900 --> 00:27:44,890
certainly on the list of things to do

00:27:58,900 --> 00:28:03,100
and thanks for the talk just a quick

00:28:01,450 --> 00:28:04,809
question about your use of pickle we've

00:28:03,100 --> 00:28:07,420
come across the same problem with

00:28:04,809 --> 00:28:09,580
shifting scikit-learn models around yeah

00:28:07,420 --> 00:28:10,990
it always terrifies me a little bit that

00:28:09,580 --> 00:28:12,340
we're forced to use pickle rather than

00:28:10,990 --> 00:28:14,170
just being a to pop out the parameters

00:28:12,340 --> 00:28:15,809
if the parameters somewhere and do it

00:28:14,170 --> 00:28:19,240
again so have you run into any problems

00:28:15,809 --> 00:28:20,800
using it like version mismatches not so

00:28:19,240 --> 00:28:22,420
far but wait so our versions are all

00:28:20,800 --> 00:28:24,580
managed by this like we everything's

00:28:22,420 --> 00:28:26,200
managed through a chef so theoretically

00:28:24,580 --> 00:28:27,670
all of our environments that have to

00:28:26,200 --> 00:28:32,470
deal with this have to set the exact

00:28:27,670 --> 00:28:34,360
same Python setup yeah I hear you right

00:28:32,470 --> 00:28:35,860
like the ship ship shipping the

00:28:34,360 --> 00:28:38,410
parameters is probably a nicer way to do

00:28:35,860 --> 00:28:39,910
a nicer thing to do and it's something

00:28:38,410 --> 00:28:41,260
that we could write into this because of

00:28:39,910 --> 00:28:42,790
like this structure of the model the way

00:28:41,260 --> 00:28:44,590
it is at the moment it's not super easy

00:28:42,790 --> 00:28:46,809
to just go drop all the parameters and

00:28:44,590 --> 00:28:47,980
rebuild from all the parameters but it

00:28:46,809 --> 00:28:51,760
would be nice thing we started using the

00:28:47,980 --> 00:28:53,590
job live dump which is like what you

00:28:51,760 --> 00:28:56,530
also get through psychic learn problem

00:28:53,590 --> 00:28:59,710
is it it creates one pickle file or one

00:28:56,530 --> 00:29:00,730
numpy file per array that exists within

00:28:59,710 --> 00:29:02,170
your whole model so you end up with like

00:29:00,730 --> 00:29:04,330
a thousand files for your model which is

00:29:02,170 --> 00:29:07,510
also painful to ship so pickle kind of

00:29:04,330 --> 00:29:08,860
does the job for the moment hi I'm

00:29:07,510 --> 00:29:11,410
thanks for your talk it was very

00:29:08,860 --> 00:29:14,620
interesting I just want to ask does your

00:29:11,410 --> 00:29:18,280
tool take into account that the model

00:29:14,620 --> 00:29:20,830
may evolve temporarily that is exactly

00:29:18,280 --> 00:29:23,050
the idea so that's that's we we will

00:29:20,830 --> 00:29:24,280
rebuild this on a regular basis so maybe

00:29:23,050 --> 00:29:26,620
like once a week for each account as

00:29:24,280 --> 00:29:28,559
like new you know we're getting more and

00:29:26,620 --> 00:29:30,700
more information all the time right so

00:29:28,559 --> 00:29:32,320
we want to rebuild it all the time and

00:29:30,700 --> 00:29:34,510
that's kind of part of the reason why we

00:29:32,320 --> 00:29:37,240
we do this comparison to the previous

00:29:34,510 --> 00:29:38,800
model as well because we might the new

00:29:37,240 --> 00:29:40,210
model we build might still not be better

00:29:38,800 --> 00:29:41,559
than the last one that we had and it

00:29:40,210 --> 00:29:43,780
depends on how the data might change

00:29:41,559 --> 00:29:47,470
over time but is it also possible that

00:29:43,780 --> 00:29:49,030
the way people behave itself changes

00:29:47,470 --> 00:29:50,650
totally so if you I mean if you're a

00:29:49,030 --> 00:29:53,380
company for example and you release a

00:29:50,650 --> 00:29:54,730
new product that didn't exist last week

00:29:53,380 --> 00:29:55,929
you'll end up with a whole bunch of

00:29:54,730 --> 00:29:57,280
support tickets talking about something

00:29:55,929 --> 00:29:58,660
you never knew about before right so

00:29:57,280 --> 00:30:00,640
there's there's going to be sort of

00:29:58,660 --> 00:30:01,990
they'll be big changes to the dynamics

00:30:00,640 --> 00:30:04,410
of how the model will predict what's

00:30:01,990 --> 00:30:04,410
good what's bad

00:30:05,610 --> 00:30:10,080
hi so it sounds like you set up quite a

00:30:08,160 --> 00:30:10,500
lot of machinery there to get the job

00:30:10,080 --> 00:30:12,540
done

00:30:10,500 --> 00:30:14,490
do you have some any kind of simplified

00:30:12,540 --> 00:30:16,110
model that you use to evaluate your

00:30:14,490 --> 00:30:19,200
performance against a baseline to kind

00:30:16,110 --> 00:30:21,059
of prove the need for that much work we

00:30:19,200 --> 00:30:22,620
did we did build a pretty simple model

00:30:21,059 --> 00:30:24,270
the beginning it didn't work very well I

00:30:22,620 --> 00:30:25,920
don't think we don't rebuild that on a

00:30:24,270 --> 00:30:29,580
regular case because the the difference

00:30:25,920 --> 00:30:32,250
was was significant but we're certainly

00:30:29,580 --> 00:30:33,450
open to like evolving you know like the

00:30:32,250 --> 00:30:35,130
number of pipelines we need you know

00:30:33,450 --> 00:30:36,990
what sort of feature extraction we use

00:30:35,130 --> 00:30:38,669
and I think at the moment we've been

00:30:36,990 --> 00:30:40,169
like skips and it works as well as we

00:30:38,669 --> 00:30:41,640
can think of and then the next stage is

00:30:40,169 --> 00:30:43,440
also pair that back and actually make it

00:30:41,640 --> 00:30:46,950
like you know Minimum Viable Product I

00:30:43,440 --> 00:30:49,230
guess in that respect I don't know how I

00:30:46,950 --> 00:30:51,870
learned this question is better what is

00:30:49,230 --> 00:30:54,750
the sample size before you actually said

00:30:51,870 --> 00:30:57,540
the customer responses is right or wrong

00:30:54,750 --> 00:31:00,840
for certain product product for the

00:30:57,540 --> 00:31:04,440
company how big is your sample size what

00:31:00,840 --> 00:31:06,960
is the power of these stats so we I mean

00:31:04,440 --> 00:31:08,730
training examples no I mean how many

00:31:06,960 --> 00:31:14,460
number of people have to respond so that

00:31:08,730 --> 00:31:16,380
you get something relevant out of it in

00:31:14,460 --> 00:31:18,150
terms of yes and I mean this is not a

00:31:16,380 --> 00:31:21,510
good service is not a good product how

00:31:18,150 --> 00:31:22,770
many number of customers or kind of not

00:31:21,510 --> 00:31:24,360
so much to do with a particular product

00:31:22,770 --> 00:31:26,250
but to an individual support experience

00:31:24,360 --> 00:31:27,840
so each one is like a sample of N equals

00:31:26,250 --> 00:31:29,820
one I guess because it's it's sort of

00:31:27,840 --> 00:31:31,380
you know I the way I write to this

00:31:29,820 --> 00:31:32,700
company and talk about my particular

00:31:31,380 --> 00:31:35,400
problem is gonna be the different to the

00:31:32,700 --> 00:31:37,290
way anyone else did as far as like how

00:31:35,400 --> 00:31:39,330
much data we use to build the models it

00:31:37,290 --> 00:31:40,559
depends on how many how much how many

00:31:39,330 --> 00:31:42,960
tickets accounts get this can be

00:31:40,559 --> 00:31:48,270
anywhere between a couple of thousand to

00:31:42,960 --> 00:31:49,830
hundreds of thousands or millions hi I'm

00:31:48,270 --> 00:31:52,440
not really an expert on this stuff but

00:31:49,830 --> 00:31:54,350
did you say that it was a purely random

00:31:52,440 --> 00:31:57,720
where you assign your parameter

00:31:54,350 --> 00:31:59,429
optimization do you have you did you

00:31:57,720 --> 00:32:01,169
consider going down the road out of

00:31:59,429 --> 00:32:03,150
using the previous one and weighting

00:32:01,169 --> 00:32:06,600
what you thought was important and

00:32:03,150 --> 00:32:08,070
reseed those means yeah so we could we

00:32:06,600 --> 00:32:09,330
could totally do that basically we just

00:32:08,070 --> 00:32:10,950
did that at the moment cuz it's out of

00:32:09,330 --> 00:32:12,960
the box and it seems to work pretty well

00:32:10,950 --> 00:32:14,820
there's certainly ways we could improve

00:32:12,960 --> 00:32:17,480
on it yeah I mean at the moment it's a

00:32:14,820 --> 00:32:19,490
random walk like

00:32:17,480 --> 00:32:27,770
50 dimensions it's not super efficient

00:32:19,490 --> 00:32:30,830
yeah okay I just wandered out of

00:32:27,770 --> 00:32:33,320
curiosity what sort of accuracy you're

00:32:30,830 --> 00:32:37,010
receiving from your predictions so we

00:32:33,320 --> 00:32:39,640
use receiver off ROC AUC so like the

00:32:37,010 --> 00:32:42,770
area under the curve for the receiver

00:32:39,640 --> 00:32:44,330
operator characteristic this is

00:32:42,770 --> 00:32:45,799
basically a way of measuring like where

00:32:44,330 --> 00:32:46,850
the right like your your trade-off

00:32:45,799 --> 00:32:49,490
between true positives and false

00:32:46,850 --> 00:32:51,919
positives and so forth for some accounts

00:32:49,490 --> 00:32:54,440
we get up to like 0.99 so basically

00:32:51,919 --> 00:32:56,120
perfect other accounts it's it's a bit

00:32:54,440 --> 00:32:57,890
less than that like in the 0.8 sort of

00:32:56,120 --> 00:32:58,309
range and then in between so it works

00:32:57,890 --> 00:33:01,190
pretty well

00:32:58,309 --> 00:33:02,150
there's definitely well above chance we

00:33:01,190 --> 00:33:07,040
can give it a bit of a push for

00:33:02,150 --> 00:33:09,410
something else so the classifier output

00:33:07,040 --> 00:33:12,919
is like the probability of them being in

00:33:09,410 --> 00:33:15,110
the like happy class um given that I'm

00:33:12,919 --> 00:33:17,480
an end-user of your software not not the

00:33:15,110 --> 00:33:21,799
person with the issue but the like that

00:33:17,480 --> 00:33:24,049
yeah how do they actually use that

00:33:21,799 --> 00:33:25,820
information do they if it predicts a

00:33:24,049 --> 00:33:27,230
negative experience they go and try and

00:33:25,820 --> 00:33:29,450
intervene and make it a positive

00:33:27,230 --> 00:33:31,160
experience is up to the individual

00:33:29,450 --> 00:33:32,299
accounts we make the information

00:33:31,160 --> 00:33:34,070
available that's certainly one of the

00:33:32,299 --> 00:33:36,049
options they could do you can use it for

00:33:34,070 --> 00:33:37,880
monitoring you can use it for for

00:33:36,049 --> 00:33:40,400
intervention for automatic routing of

00:33:37,880 --> 00:33:41,690
tickets you know basically you can build

00:33:40,400 --> 00:33:47,720
it into the workflow like anything else

00:33:41,690 --> 00:33:50,090
that's and how do you handle customers

00:33:47,720 --> 00:33:52,610
who have different types of tickets for

00:33:50,090 --> 00:33:55,130
instance maybe ones that are maybe some

00:33:52,610 --> 00:33:57,380
of them are super long and will involve

00:33:55,130 --> 00:33:58,970
a number of different replies whereas

00:33:57,380 --> 00:34:00,980
others are really easy ones that you can

00:33:58,970 --> 00:34:02,030
just solve in a single reply at the

00:34:00,980 --> 00:34:04,790
moment we throw them all on the same

00:34:02,030 --> 00:34:06,080
bucket and hope for the best

00:34:04,790 --> 00:34:07,070
so there's definitely something to be

00:34:06,080 --> 00:34:09,740
said for trying to split those things

00:34:07,070 --> 00:34:10,909
out again this is kind of first pass it

00:34:09,740 --> 00:34:12,919
seems to work pretty well at the moment

00:34:10,909 --> 00:34:15,740
I think as we dig into the customers

00:34:12,919 --> 00:34:17,090
that don't work that or the ones that

00:34:15,740 --> 00:34:18,440
don't work as well as some others I

00:34:17,090 --> 00:34:19,820
think these are the sort of things we're

00:34:18,440 --> 00:34:21,320
going to find you know it's it's quite

00:34:19,820 --> 00:34:23,119
common that you'll have more than one

00:34:21,320 --> 00:34:25,010
complaint in a given they give it a

00:34:23,119 --> 00:34:26,240
ticket that said like I think a lot of

00:34:25,010 --> 00:34:28,129
the driving factors around whether

00:34:26,240 --> 00:34:28,659
someone is happy or not is to do with

00:34:28,129 --> 00:34:29,919
like

00:34:28,659 --> 00:34:31,329
I guess the sentiment of the

00:34:29,919 --> 00:34:33,669
conversation backwards and forwards and

00:34:31,329 --> 00:34:37,139
I think that probably doesn't scale too

00:34:33,669 --> 00:34:37,139
badly with people the size of the ticket

00:34:43,090 --> 00:34:45,150

YouTube URL: https://www.youtube.com/watch?v=of6pHdSbtOM


