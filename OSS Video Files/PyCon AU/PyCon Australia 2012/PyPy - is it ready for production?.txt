Title: PyPy - is it ready for production?
Publication date: 2012-08-23
Playlist: PyCon Australia 2012
Description: 
	Mark Rees
I have followed the development of PyPy since 2004 and played with various releases to see what the PyPy team had achieved. It wasn't until the release of PyPy 1.18 that I actually ran some existing production python code under it. The perc
Captions: 
	00:00:01,310 --> 00:00:07,710
thank you very much and welcome to the

00:00:04,500 --> 00:00:10,410
derwent rooms for the second talk of

00:00:07,710 --> 00:00:12,090
this bracket for this talk we have mark

00:00:10,410 --> 00:00:14,759
Reeves who is a programmer of more than

00:00:12,090 --> 00:00:17,550
25 years experience and has been coding

00:00:14,759 --> 00:00:19,080
in Python since 1997 during that time

00:00:17,550 --> 00:00:20,970
he's been a contributor to many Python

00:00:19,080 --> 00:00:22,500
open source projects he's a regular app

00:00:20,970 --> 00:00:24,300
icon Australia is presented at both of

00:00:22,500 --> 00:00:25,800
our sydney conferences he's also passed

00:00:24,300 --> 00:00:28,260
convenor of the sydney python user group

00:00:25,800 --> 00:00:29,160
today mark will be investigating where

00:00:28,260 --> 00:00:39,420
the pie pie is ready for production

00:00:29,160 --> 00:00:42,239
please make him welcome thank you okay

00:00:39,420 --> 00:00:45,300
firstly these slides have been passed

00:00:42,239 --> 00:00:46,649
passed the pipe I team so they asked me

00:00:45,300 --> 00:00:49,289
to say I'm not affiliated with the PO

00:00:46,649 --> 00:00:54,270
fighting and the opinions expressed here

00:00:49,289 --> 00:00:56,250
are not their opinions me and pie pie so

00:00:54,270 --> 00:00:58,379
I'll start there was a Python programmer

00:00:56,250 --> 00:01:04,519
a Java program and a pill programmer and

00:00:58,379 --> 00:01:08,250
a guy with a gaelic accent in a pub 2004

00:01:04,519 --> 00:01:09,900
second night of ovis DC and we're

00:01:08,250 --> 00:01:11,280
talking about projects that we didn't he

00:01:09,900 --> 00:01:13,380
said I'm working on this project where

00:01:11,280 --> 00:01:15,509
we going to write a Python interpreter

00:01:13,380 --> 00:01:18,240
in Python and it's going to be faster

00:01:15,509 --> 00:01:20,360
and we said and I can't repeat it

00:01:18,240 --> 00:01:24,680
because it would be a code violation and

00:01:20,360 --> 00:01:28,770
that was my first experience with pipe i

00:01:24,680 --> 00:01:31,799
I'm a little bit of a alternative Python

00:01:28,770 --> 00:01:34,650
implementation junkie I've spoken about

00:01:31,799 --> 00:01:38,130
ironpython I've worked on the community

00:01:34,650 --> 00:01:43,439
version of ironpython we use jython at

00:01:38,130 --> 00:01:45,420
work a lot with all our products Python

00:01:43,439 --> 00:01:47,040
C Python the ship that we also ship

00:01:45,420 --> 00:01:48,470
jathan for various reasons because

00:01:47,040 --> 00:01:52,380
there's some libraries we need to use

00:01:48,470 --> 00:01:56,520
and so I I have been keeping an eye on

00:01:52,380 --> 00:01:58,560
the pipe i project and sort of randomly

00:01:56,520 --> 00:02:02,850
when they do a release i might download

00:01:58,560 --> 00:02:08,869
it or on my not download and when 1.8

00:02:02,850 --> 00:02:11,870
came out i downloaded it ran it it felt

00:02:08,869 --> 00:02:15,440
faster than other felt before

00:02:11,870 --> 00:02:17,690
and I thought is this the time to work

00:02:15,440 --> 00:02:19,940
out if PI PI's an option for me to

00:02:17,690 --> 00:02:22,610
improve some things we're having a

00:02:19,940 --> 00:02:25,129
little bit of a performance issue with

00:02:22,610 --> 00:02:27,890
our C Python code and I'll talk in more

00:02:25,129 --> 00:02:32,090
detail about that so that's my question

00:02:27,890 --> 00:02:35,540
this is what this talk is based on sadly

00:02:32,090 --> 00:02:37,430
I'm a manager now and I still want to be

00:02:35,540 --> 00:02:39,590
a programmer so this is my little

00:02:37,430 --> 00:02:41,000
project and being the boss I'm allowed

00:02:39,590 --> 00:02:45,650
to go you're not going to do this one

00:02:41,000 --> 00:02:49,280
I'm going to do this one okay so how

00:02:45,650 --> 00:02:50,900
many people know what pay pay is good so

00:02:49,280 --> 00:02:53,900
it's not the package index it's the

00:02:50,900 --> 00:02:55,489
whole interpret okay now there is a bit

00:02:53,900 --> 00:02:58,640
of a problem because PI pi is actually

00:02:55,489 --> 00:03:02,000
two things and people tend to get a mix

00:02:58,640 --> 00:03:05,180
it up so the first one is it's a trance

00:03:02,000 --> 00:03:08,599
a framework for building dynamic virtual

00:03:05,180 --> 00:03:10,819
machines for interpreters and that's

00:03:08,599 --> 00:03:17,660
done in a subset of Python called our

00:03:10,819 --> 00:03:19,609
Python and pi pi is the ultimate example

00:03:17,660 --> 00:03:22,940
of what you can build with our Python

00:03:19,609 --> 00:03:24,859
and if you want to hunt around on the

00:03:22,940 --> 00:03:27,609
web you'll find various attempts at

00:03:24,859 --> 00:03:31,519
taking languages like JavaScript and

00:03:27,609 --> 00:03:34,400
subsets of lists for scheme there's even

00:03:31,519 --> 00:03:37,069
a PHP interpreter which was done with a

00:03:34,400 --> 00:03:39,379
proof of concept but some of the nice

00:03:37,069 --> 00:03:42,680
features that the translation framework

00:03:39,379 --> 00:03:45,650
gives you and not specific to python so

00:03:42,680 --> 00:03:47,150
you can use it for other things but you

00:03:45,650 --> 00:03:49,280
know this talk is about the Python

00:03:47,150 --> 00:03:51,440
version so as I said my first

00:03:49,280 --> 00:03:56,840
introduction was 2004 but it's been

00:03:51,440 --> 00:03:59,060
around since 2003 it was a EU project so

00:03:56,840 --> 00:04:01,910
got a lot of funding and so on the web

00:03:59,060 --> 00:04:06,500
you will find academic papers about it

00:04:01,910 --> 00:04:09,470
it's a very reflected project there have

00:04:06,500 --> 00:04:12,699
been some we're getting there didn't

00:04:09,470 --> 00:04:14,810
quite make her let's rethink about it i

00:04:12,699 --> 00:04:16,970
would suggest this is the third or

00:04:14,810 --> 00:04:20,409
fourth iteration don't quote me on that

00:04:16,970 --> 00:04:23,180
but the big thing was the adding of a

00:04:20,409 --> 00:04:24,710
just-in-time compiler which all of a

00:04:23,180 --> 00:04:26,750
sudden took the benchmarks

00:04:24,710 --> 00:04:29,479
so they used to go to Pike on each year

00:04:26,750 --> 00:04:31,810
and they go we are 16 times slower than

00:04:29,479 --> 00:04:35,120
C Python we are eight times slower than

00:04:31,810 --> 00:04:38,860
C Python we are two times slower than C

00:04:35,120 --> 00:04:41,180
Python to wear now fasten see buzzin

00:04:38,860 --> 00:04:45,139
they considered the first production

00:04:41,180 --> 00:04:50,930
ready release was 1.4 I never downloaded

00:04:45,139 --> 00:04:52,340
that one ok the other thing since this

00:04:50,930 --> 00:04:54,319
talk is about the benchmarks and stuff

00:04:52,340 --> 00:04:55,580
so I'm on the slides which there's a

00:04:54,319 --> 00:04:57,770
link at the end of where it will be put

00:04:55,580 --> 00:04:59,630
up these are things that I found really

00:04:57,770 --> 00:05:02,030
useful the documentation on the pipe

00:04:59,630 --> 00:05:03,320
eyesight has got a lot better there's a

00:05:02,030 --> 00:05:06,800
link to a blog which is really

00:05:03,320 --> 00:05:10,400
worthwhile reading and these other three

00:05:06,800 --> 00:05:14,180
lengths are actually YouTube videos all

00:05:10,400 --> 00:05:18,710
talks from Pike on and the states this

00:05:14,180 --> 00:05:20,780
year really good solid introductions if

00:05:18,710 --> 00:05:22,099
you want to understand the underlying

00:05:20,780 --> 00:05:26,539
hair pie pie works well you want to see

00:05:22,099 --> 00:05:29,770
some cool things that are running okay

00:05:26,539 --> 00:05:32,840
so production-ready it's sort of a very

00:05:29,770 --> 00:05:35,060
vague term depending on who you are and

00:05:32,840 --> 00:05:38,139
you can take the the business definition

00:05:35,060 --> 00:05:40,250
which is it runs and will it make money

00:05:38,139 --> 00:05:43,070
but this is more the programmers

00:05:40,250 --> 00:05:44,900
definition so this is how I am going to

00:05:43,070 --> 00:05:47,659
measure whether what I believe is

00:05:44,900 --> 00:05:49,699
production ready and I throw a few other

00:05:47,659 --> 00:05:54,530
things in as well so what's important to

00:05:49,699 --> 00:05:56,690
me is do I have to rework my code and is

00:05:54,530 --> 00:05:58,940
it as fast as II Python or hopefully

00:05:56,690 --> 00:06:00,650
faster because as I said my original

00:05:58,940 --> 00:06:03,349
question was am I going to get a

00:06:00,650 --> 00:06:06,080
performance improvement out of running

00:06:03,349 --> 00:06:10,729
pi PI so we'll sort of go through the

00:06:06,080 --> 00:06:14,259
list does it run yes it runs so it's

00:06:10,729 --> 00:06:17,210
exactly like Python when you start it up

00:06:14,259 --> 00:06:19,759
the good thing is you can get just

00:06:17,210 --> 00:06:26,509
download the binary the big thing to

00:06:19,759 --> 00:06:30,139
remember is the stable version runs only

00:06:26,509 --> 00:06:32,659
on x86 32 bit or 64 bit implementation

00:06:30,139 --> 00:06:36,560
all right there are other back-end

00:06:32,659 --> 00:06:39,070
implementations forearm PowerPC jar

00:06:36,560 --> 00:06:41,360
dotnet viens but they're all in

00:06:39,070 --> 00:06:46,220
different states and some of them needs

00:06:41,360 --> 00:06:48,560
a lot of love but looking at the

00:06:46,220 --> 00:06:50,570
chickens of late the arm and the PowerPC

00:06:48,560 --> 00:06:54,919
one seems to be getting a fair bit of

00:06:50,570 --> 00:06:56,270
work you will also see backends for

00:06:54,919 --> 00:07:01,430
JavaScript and stuff which also you've

00:06:56,270 --> 00:07:03,860
gone gone away and died this is a little

00:07:01,430 --> 00:07:05,060
bit of a problem for me because we have

00:07:03,860 --> 00:07:08,810
to support things on other platforms

00:07:05,060 --> 00:07:12,530
other than x86 but it's not the end of

00:07:08,810 --> 00:07:16,340
the world play play implements Python

00:07:12,530 --> 00:07:23,120
2.7 point2 with some point three

00:07:16,340 --> 00:07:27,080
adjustments okay the others does it

00:07:23,120 --> 00:07:28,729
satisfy the project requirements for pi

00:07:27,080 --> 00:07:30,110
PI I guess it did I mean I can't can't

00:07:28,729 --> 00:07:31,550
really comment on that they've got money

00:07:30,110 --> 00:07:33,620
from the ground so they must have

00:07:31,550 --> 00:07:35,210
satisfied the budget requirement my

00:07:33,620 --> 00:07:38,389
project requirement comes more about

00:07:35,210 --> 00:07:40,729
when we talk about benchmarking I'm not

00:07:38,389 --> 00:07:42,500
in a language interpreter designer so I

00:07:40,729 --> 00:07:44,180
can only assume the design was well

00:07:42,500 --> 00:07:46,570
thought out I would suggest with all the

00:07:44,180 --> 00:07:50,570
refactoring it is well thought out now

00:07:46,570 --> 00:07:52,370
is it stable all the playing I've done

00:07:50,570 --> 00:07:54,110
with it and using in the benchmarks and

00:07:52,370 --> 00:07:57,050
some production runs and things I

00:07:54,110 --> 00:07:59,870
haven't had any Python equivalents of

00:07:57,050 --> 00:08:02,229
blue screens of death it doesn't seem to

00:07:59,870 --> 00:08:06,139
be memory leaks or anything like that

00:08:02,229 --> 00:08:07,700
it's it maintainable I know I gave this

00:08:06,139 --> 00:08:10,160
a 7 out of 10 and the reason why I gave

00:08:07,700 --> 00:08:12,530
it a seven out of 10 is exactly the same

00:08:10,160 --> 00:08:15,919
reason in my eyes while I probably give

00:08:12,530 --> 00:08:17,960
C Python a 7 out of 10 I'm looking at it

00:08:15,919 --> 00:08:21,500
as a Python programmer how maintainable

00:08:17,960 --> 00:08:24,770
is it for me so pipe is written in our

00:08:21,500 --> 00:08:30,320
Python a lot of them's are our Python it

00:08:24,770 --> 00:08:33,970
is a subset how you use it isn't very

00:08:30,320 --> 00:08:33,970
well-documented it's getting better

00:08:34,360 --> 00:08:42,260
David Beasley said in his talk if you

00:08:37,219 --> 00:08:44,690
love Python you will hate our Python so

00:08:42,260 --> 00:08:47,540
the architecture is quite complex so if

00:08:44,690 --> 00:08:48,950
I had to for a reason because any of the

00:08:47,540 --> 00:08:49,700
reason we use our open source software

00:08:48,950 --> 00:08:51,620
and the come

00:08:49,700 --> 00:08:54,230
is worse comes to worse we can maintain

00:08:51,620 --> 00:08:57,380
it that is a little bit of a concern for

00:08:54,230 --> 00:08:59,720
me mind you the Pope aight guys do now

00:08:57,380 --> 00:09:03,110
consult so that is a fallback option for

00:08:59,720 --> 00:09:05,270
me is it scalable it's a scalable as

00:09:03,110 --> 00:09:07,520
Python is scalable so depending what

00:09:05,270 --> 00:09:09,500
side of the feature on you can say it is

00:09:07,520 --> 00:09:13,820
or it isn't but it does actually have

00:09:09,500 --> 00:09:18,140
stakus building so you've got options

00:09:13,820 --> 00:09:22,040
for massive concurrency that way is it

00:09:18,140 --> 00:09:24,950
documented one parts very easy it

00:09:22,040 --> 00:09:27,460
implements exactly the language of 2.7

00:09:24,950 --> 00:09:32,240
so all the C Python docks are exactly

00:09:27,460 --> 00:09:34,280
the functionality of how it runs where

00:09:32,240 --> 00:09:38,180
it's less as I said it's the our Python

00:09:34,280 --> 00:09:44,050
and the toolchain is documented but not

00:09:38,180 --> 00:09:46,340
as well as C Python us so you know it's

00:09:44,050 --> 00:09:48,020
depending on how much you want to dive

00:09:46,340 --> 00:09:50,630
over the fencin and looking behind the

00:09:48,020 --> 00:09:54,350
scenes you maybe give it a little bit

00:09:50,630 --> 00:09:58,700
higher and it work with the models we

00:09:54,350 --> 00:10:00,980
use I know it's a very crowded slide but

00:09:58,700 --> 00:10:05,510
I should have really done one of what's

00:10:00,980 --> 00:10:06,620
missing but for for us and we'll get

00:10:05,510 --> 00:10:09,560
into the only thing that we had a

00:10:06,620 --> 00:10:11,390
problem with we try to stick with the

00:10:09,560 --> 00:10:12,920
standard if we go to using external

00:10:11,390 --> 00:10:16,820
libraries they tend to be the ones that

00:10:12,920 --> 00:10:22,460
are more commonly used so the big

00:10:16,820 --> 00:10:26,390
difference with pi PI is the C Python

00:10:22,460 --> 00:10:28,940
modules that have C extensions to power

00:10:26,390 --> 00:10:31,870
them they seem to be written in pure

00:10:28,940 --> 00:10:34,850
python and PI point okay they live in a

00:10:31,870 --> 00:10:36,080
direction called lib underscore pi PI so

00:10:34,850 --> 00:10:38,540
they still sort of work in the con

00:10:36,080 --> 00:10:41,120
concept of there's this extension module

00:10:38,540 --> 00:10:42,620
but it's in pure python and relies on

00:10:41,120 --> 00:10:46,490
the speed ups that pipe i give you the

00:10:42,620 --> 00:10:49,430
other thing is a lot of third-party

00:10:46,490 --> 00:10:51,230
libraries work so tipsy types work so

00:10:49,430 --> 00:10:55,100
because of that things like piglet will

00:10:51,230 --> 00:10:58,070
work fine and then SQ alchemy I've

00:10:55,100 --> 00:11:02,140
repeated or twice no why Python imaging

00:10:58,070 --> 00:11:03,370
library works perfectly there's a nice

00:11:02,140 --> 00:11:06,070
compatibility

00:11:03,370 --> 00:11:06,970
it can go to just be careful a lot of

00:11:06,070 --> 00:11:08,710
the things that are listed there

00:11:06,970 --> 00:11:11,470
actually have a red mark beside them

00:11:08,710 --> 00:11:19,570
which means they don't work so first of

00:11:11,470 --> 00:11:23,110
all yes no okay so that's this slide

00:11:19,570 --> 00:11:26,560
here sort of shows how play play can be

00:11:23,110 --> 00:11:29,560
hard all right so we use report lab a

00:11:26,560 --> 00:11:31,630
lot and when we installed it because in

00:11:29,560 --> 00:11:33,850
popo you just stall as install as you

00:11:31,630 --> 00:11:36,940
naturally would perp install installs

00:11:33,850 --> 00:11:41,529
compiler fires off because pi pi has a

00:11:36,940 --> 00:11:44,050
capi compatibility layer and report lab

00:11:41,529 --> 00:11:48,370
has some Speedos accelerators rittman

00:11:44,050 --> 00:11:51,540
see they compiled and I went to run one

00:11:48,370 --> 00:11:53,730
of our scripts and I got this error and

00:11:51,540 --> 00:11:57,540
it's really hard to work out where the

00:11:53,730 --> 00:12:00,010
problem is but that was the only

00:11:57,540 --> 00:12:02,110
compatibility issue we got with every

00:12:00,010 --> 00:12:05,410
single piece of Python code we run in

00:12:02,110 --> 00:12:08,020
house all right and was not a major

00:12:05,410 --> 00:12:11,110
problem because report layered does have

00:12:08,020 --> 00:12:13,570
the fallback of exactly what these the

00:12:11,110 --> 00:12:14,890
accelerators are for they've got Python

00:12:13,570 --> 00:12:17,290
equivalents just in case you didn't have

00:12:14,890 --> 00:12:20,380
the c compiler all the libraries so it

00:12:17,290 --> 00:12:24,070
did work your mileage will vary

00:12:20,380 --> 00:12:29,730
depending on what's the extensions and

00:12:24,070 --> 00:12:29,730
stuff you use okay you can only buy try

00:12:30,450 --> 00:12:35,020
does it run run as fast asleep ivan well

00:12:33,339 --> 00:12:38,500
according to their benchmarks it runs

00:12:35,020 --> 00:12:40,900
much faster and so there is a website

00:12:38,500 --> 00:12:45,250
where you can go to speedy IP org and

00:12:40,900 --> 00:12:48,700
every night they run against the nightly

00:12:45,250 --> 00:12:51,160
build a full set of benchmarks and they

00:12:48,700 --> 00:12:53,500
push it up onto the site and there's

00:12:51,160 --> 00:12:55,060
some other good graphs that compare what

00:12:53,500 --> 00:12:58,720
their performance has been like over

00:12:55,060 --> 00:13:04,950
time and how they improve so they you

00:12:58,720 --> 00:13:04,950
know it looks looks pretty good but okay

00:13:04,980 --> 00:13:11,160
and this is where I have a slight

00:13:07,810 --> 00:13:11,160
disagreement with the pipe fighting

00:13:11,399 --> 00:13:20,639
their benchmarks and I would say it's

00:13:17,439 --> 00:13:26,049
one way of doing it I have a very small

00:13:20,639 --> 00:13:28,359
targeted piece of functionality okay so

00:13:26,049 --> 00:13:33,579
if we take as an example to django

00:13:28,359 --> 00:13:37,779
benchmark which is the third one from

00:13:33,579 --> 00:13:44,259
the left okay massive performance

00:13:37,779 --> 00:13:48,099
improvement so basically that is just

00:13:44,259 --> 00:13:52,929
testing django template rendering okay

00:13:48,099 --> 00:13:54,699
so when you see things like that you

00:13:52,929 --> 00:13:56,229
just need to be careful because people

00:13:54,699 --> 00:13:59,079
might believe Oh Django is going to be

00:13:56,229 --> 00:14:02,049
that much faster now if your django site

00:13:59,079 --> 00:14:03,759
is heavily into templates you are going

00:14:02,049 --> 00:14:05,349
to see a really good performance

00:14:03,759 --> 00:14:06,729
improvement and i think that's one of

00:14:05,349 --> 00:14:08,919
the sort of cool things about what the

00:14:06,729 --> 00:14:10,899
pipe I guys have done and what they've

00:14:08,919 --> 00:14:15,549
achieved you can run the whole Jengo

00:14:10,899 --> 00:14:17,379
stack on pipeline but as I said is the

00:14:15,549 --> 00:14:19,629
benchmark would you keep I keep getting

00:14:17,379 --> 00:14:22,029
pointed to is the Django benchmark now

00:14:19,629 --> 00:14:28,569
this was the original benchmark from

00:14:22,029 --> 00:14:29,949
unladen swallow and as I said it's fine

00:14:28,569 --> 00:14:31,209
when you're when you're trying to work

00:14:29,949 --> 00:14:33,720
at our things of working you have small

00:14:31,209 --> 00:14:38,619
standard ones but when you're in

00:14:33,720 --> 00:14:40,389
enterprise it's you you know that you

00:14:38,619 --> 00:14:42,129
don't get all the wounds from those

00:14:40,389 --> 00:14:44,559
little things that's the old 8020 rule

00:14:42,129 --> 00:14:45,879
and sadly eighty percent of my code

00:14:44,559 --> 00:14:49,509
isn't going to be Django template

00:14:45,879 --> 00:14:52,720
rendering it'll be something else so

00:14:49,509 --> 00:14:56,379
what I tried to do was create work loads

00:14:52,720 --> 00:14:59,169
which are closer to what we use at work

00:14:56,379 --> 00:15:01,809
and so I'm not very complex but a little

00:14:59,169 --> 00:15:03,970
bit more mixed where I 0 is involved

00:15:01,809 --> 00:15:08,679
okay so I'm running the C Python the

00:15:03,970 --> 00:15:12,039
pipe I on that on the same platform and

00:15:08,679 --> 00:15:15,069
we do a lot of extract transform load

00:15:12,039 --> 00:15:18,230
stuff with Python okay so minimally

00:15:15,069 --> 00:15:23,129
hundred megabyte files coming

00:15:18,230 --> 00:15:28,529
but they can get half a gig so they run

00:15:23,129 --> 00:15:31,410
for quite a long time so my benchmark so

00:15:28,529 --> 00:15:32,970
a little bit more of a mix and so what I

00:15:31,410 --> 00:15:34,470
wanted to do from the benchmarking point

00:15:32,970 --> 00:15:38,369
of viewers I wanted to use the same

00:15:34,470 --> 00:15:42,329
benchmarking sweet that the pipe I team

00:15:38,369 --> 00:15:46,189
uses so I did you they have it sitting

00:15:42,329 --> 00:15:48,809
up a bit bucket so I downloaded it and I

00:15:46,189 --> 00:15:50,339
sorry cloned it and so sitting on

00:15:48,809 --> 00:15:53,519
bitbucket is a link so you can all see

00:15:50,339 --> 00:15:56,399
it add in my benchmarks in and so in the

00:15:53,519 --> 00:15:58,850
same environment I ran their benchmarks

00:15:56,399 --> 00:16:01,559
just to make sure I was getting the same

00:15:58,850 --> 00:16:05,040
level of performance improvement which

00:16:01,559 --> 00:16:06,839
which I am it's only a little vm so I

00:16:05,040 --> 00:16:08,699
also ran it on some physical hardware

00:16:06,839 --> 00:16:10,499
just to make sure again so I got similar

00:16:08,699 --> 00:16:14,040
results so the first one which is

00:16:10,499 --> 00:16:17,129
basically this one seeing here so this

00:16:14,040 --> 00:16:22,679
is 100 megabytes CSV file which is US

00:16:17,129 --> 00:16:24,209
census data it gets pars the little bit

00:16:22,679 --> 00:16:28,799
of work done on it and then we output

00:16:24,209 --> 00:16:32,879
some XML and so excitement time it's

00:16:28,799 --> 00:16:33,959
faster excellent the only issue was I'm

00:16:32,879 --> 00:16:39,179
thinking but I thought it would be

00:16:33,959 --> 00:16:43,999
faster so I thought well I'll pull it

00:16:39,179 --> 00:16:47,610
apart and so I did the CSV first and

00:16:43,999 --> 00:16:53,220
it's slower so all my game was actually

00:16:47,610 --> 00:16:55,019
in my xml generation and I'm like this

00:16:53,220 --> 00:16:57,480
is looking really bad for the rest of my

00:16:55,019 --> 00:16:58,889
 max I'll go in a little bit a

00:16:57,480 --> 00:17:02,790
little bit later on or go into what

00:16:58,889 --> 00:17:04,169
actually the reason for this was so I

00:17:02,790 --> 00:17:07,020
kept on going because I had to get my

00:17:04,169 --> 00:17:12,809
slides done so this is 1 this is a pure

00:17:07,020 --> 00:17:16,559
python excel manipulation package we use

00:17:12,809 --> 00:17:18,120
it pulling in quite a large Excel

00:17:16,559 --> 00:17:20,909
spreadsheet because we get a lot of a

00:17:18,120 --> 00:17:23,240
data in that and again a little bit

00:17:20,909 --> 00:17:23,240
slower

00:17:24,310 --> 00:17:28,450
something we do a lot of we as I said we

00:17:26,890 --> 00:17:30,430
use report later a lot we've got a

00:17:28,450 --> 00:17:34,000
really big project up in Malaysia where

00:17:30,430 --> 00:17:36,640
we generate lots of reports out of a web

00:17:34,000 --> 00:17:38,410
site which is not in Python and it all

00:17:36,640 --> 00:17:41,980
has to be in PDF and so we actually use

00:17:38,410 --> 00:17:48,430
X HTML to PDF would some modifications

00:17:41,980 --> 00:17:52,780
again not not too bad but again not what

00:17:48,430 --> 00:17:55,000
I was expecting and then I thought just

00:17:52,780 --> 00:17:58,630
for completeness because I tell me the

00:17:55,000 --> 00:18:00,910
memory usage is less I will run the

00:17:58,630 --> 00:18:02,410
memory thing so hidden and the bowels of

00:18:00,910 --> 00:18:07,110
the benchmark suite is you can actually

00:18:02,410 --> 00:18:10,210
track memory so I reenabled all that and

00:18:07,110 --> 00:18:12,010
so the first one is basically just the

00:18:10,210 --> 00:18:13,600
interpreter because one of the things to

00:18:12,010 --> 00:18:16,060
pipe I guys said to me they accept that

00:18:13,600 --> 00:18:17,860
the interpreter does use a lot more

00:18:16,060 --> 00:18:20,110
memory than what the C Python

00:18:17,860 --> 00:18:24,340
interpreter that's so that's basically

00:18:20,110 --> 00:18:26,950
to give you a baseline and also just in

00:18:24,340 --> 00:18:29,200
case with both its benchmarks I was

00:18:26,950 --> 00:18:31,030
using the nightly build so the last time

00:18:29,200 --> 00:18:36,970
these benchmarks were run was Thursday

00:18:31,030 --> 00:18:38,380
night this week and so again so if you

00:18:36,970 --> 00:18:42,640
have a look at them the memory

00:18:38,380 --> 00:18:45,340
footprints bigger in all cases for the

00:18:42,640 --> 00:18:47,290
pie pie for me that's not an issue but

00:18:45,340 --> 00:18:51,250
for some people it might be it's

00:18:47,290 --> 00:18:53,080
something that they're working on that

00:18:51,250 --> 00:18:54,280
as I said is that the CSV thing but

00:18:53,080 --> 00:18:58,060
something that really interested me so

00:18:54,280 --> 00:19:00,820
and what I did discover is what where

00:18:58,060 --> 00:19:03,850
the slowness was was when it makes the

00:19:00,820 --> 00:19:08,740
call to the extension which in C Python

00:19:03,850 --> 00:19:11,560
isn't see and in pi PI is in pure python

00:19:08,740 --> 00:19:14,680
so that mean i could go look at it they

00:19:11,560 --> 00:19:16,600
had some nice little tools so they have

00:19:14,680 --> 00:19:18,730
a jet viewer so you can basically see

00:19:16,600 --> 00:19:22,120
where it's optimizing the code or not

00:19:18,730 --> 00:19:25,330
optimizing the code and you can get

00:19:22,120 --> 00:19:26,650
right down so those the the instructions

00:19:25,330 --> 00:19:28,030
that are happening in the interpreter

00:19:26,650 --> 00:19:30,160
and then you can actually get down to

00:19:28,030 --> 00:19:31,420
assembler and so you can sort of see

00:19:30,160 --> 00:19:34,820
things it's really interesting you

00:19:31,420 --> 00:19:37,080
discover how many times you're looping

00:19:34,820 --> 00:19:41,490
and with a bit of hunting and also a

00:19:37,080 --> 00:19:45,330
hint from an issue that was in the pipe

00:19:41,490 --> 00:19:49,770
I tracker I started poking around in the

00:19:45,330 --> 00:19:53,730
underscore CSV python module and managed

00:19:49,770 --> 00:19:56,970
to get this so I really don't think

00:19:53,730 --> 00:19:59,580
point eight slower is I would say it's

00:19:56,970 --> 00:20:03,210
on a par there's almost the jitter of my

00:19:59,580 --> 00:20:05,550
benchmark there so that's I thought that

00:20:03,210 --> 00:20:06,720
was cool I mean I follow the C Python I

00:20:05,550 --> 00:20:09,980
would have been stuffed if I was in

00:20:06,720 --> 00:20:12,540
there looking at under school CSV dot C

00:20:09,980 --> 00:20:17,309
but I was able to sit there play around

00:20:12,540 --> 00:20:19,110
with it because it's a python module not

00:20:17,309 --> 00:20:20,520
in our Python or was this case I can

00:20:19,110 --> 00:20:22,080
make my change i could rewrite my

00:20:20,520 --> 00:20:24,420
benchmark I could relook at the jet

00:20:22,080 --> 00:20:25,860
viewer I didn't have to rebuild pi PI a

00:20:24,420 --> 00:20:27,750
lot of people seem to believe that you

00:20:25,860 --> 00:20:31,740
have to constantly keep rebuilding Piper

00:20:27,750 --> 00:20:33,570
you don't and so like it out i actually

00:20:31,740 --> 00:20:36,330
got a good improvement for that

00:20:33,570 --> 00:20:37,530
particular one and I'm I intend when I

00:20:36,330 --> 00:20:39,929
get some free time to look at those

00:20:37,530 --> 00:20:46,920
other ones and see where else i can get

00:20:39,929 --> 00:20:52,730
things from so we need to go back

00:20:46,920 --> 00:20:56,059
through the list so if we take items 127

00:20:52,730 --> 00:20:59,730
yes pie pie is ready for production

00:20:56,059 --> 00:21:03,660
exactly the same as Jeff owners or iron

00:20:59,730 --> 00:21:05,730
playfulness okay it meets all the

00:21:03,660 --> 00:21:08,220
criteria and if I if I compared pi PI to

00:21:05,730 --> 00:21:10,050
Joy on and I'm Python where I probably

00:21:08,220 --> 00:21:11,490
marked down pipe I in some places i

00:21:10,050 --> 00:21:14,130
would probably mark down those other

00:21:11,490 --> 00:21:16,470
alternative implementations so in that

00:21:14,130 --> 00:21:19,440
respect that it's pretty good and you

00:21:16,470 --> 00:21:21,120
can say its production ready I think

00:21:19,440 --> 00:21:24,330
number eights one of the most important

00:21:21,120 --> 00:21:27,470
ones and as I said we were lucky we only

00:21:24,330 --> 00:21:31,040
had one module we had a problem and the

00:21:27,470 --> 00:21:37,890
the savior for that was relatively easy

00:21:31,040 --> 00:21:39,059
but your mileage will vary number nine

00:21:37,890 --> 00:21:42,570
is the one where it gets a little bit

00:21:39,059 --> 00:21:45,150
more interesting if you're looking at pi

00:21:42,570 --> 00:21:46,480
pi just for performance it's all going

00:21:45,150 --> 00:21:47,980
to be about benchmarking and

00:21:46,480 --> 00:21:51,090
deciding whether that's the criteria of

00:21:47,980 --> 00:21:55,030
reproduction all right I'm being unfair

00:21:51,090 --> 00:21:57,520
and some respects to pi PI but for me to

00:21:55,030 --> 00:22:00,090
put the effort in to test all this and

00:21:57,520 --> 00:22:02,770
to deploy deploy it at client sites

00:22:00,090 --> 00:22:05,049
there has to be some reason for doing

00:22:02,770 --> 00:22:08,470
something that C Python is already doing

00:22:05,049 --> 00:22:10,000
okay you know do i do we focus on

00:22:08,470 --> 00:22:12,220
modifying our Python code to work better

00:22:10,000 --> 00:22:16,240
under C Python or M are we going to get

00:22:12,220 --> 00:22:23,799
a big one by taking some risks by

00:22:16,240 --> 00:22:25,780
putting Python out into production I had

00:22:23,799 --> 00:22:28,600
hoped that my big one was going to be in

00:22:25,780 --> 00:22:32,559
the PDF generation because that's where

00:22:28,600 --> 00:22:33,850
I've got a problem at the moment so at

00:22:32,559 --> 00:22:35,559
the moment I would say no I'm not going

00:22:33,850 --> 00:22:37,090
to be deploying for that I mean

00:22:35,559 --> 00:22:39,730
hopefully sometime over the next six

00:22:37,090 --> 00:22:41,290
months we may find an issue or some

00:22:39,730 --> 00:22:45,700
change in pipe i may mean all of a

00:22:41,290 --> 00:22:47,500
sudden I get the leap that I want but

00:22:45,700 --> 00:22:52,419
looking at the benchmark results I got

00:22:47,500 --> 00:22:54,010
for the CSV XML conversion I think it's

00:22:52,419 --> 00:22:56,830
likely sometime in the next six months

00:22:54,010 --> 00:23:00,010
we will probably deploy some etl

00:22:56,830 --> 00:23:01,630
workloads using pi x there's a couple of

00:23:00,010 --> 00:23:05,440
projects we're working on which I have

00:23:01,630 --> 00:23:11,410
some concerns and my stomach that we may

00:23:05,440 --> 00:23:13,330
have to actually look at that so that's

00:23:11,410 --> 00:23:14,620
not the finishing my talk because it's

00:23:13,330 --> 00:23:17,679
other things that you need to consider

00:23:14,620 --> 00:23:21,610
motives it's about production so last

00:23:17,679 --> 00:23:24,790
week they released I don't know if

00:23:21,610 --> 00:23:26,890
everybody at the sea foreign function

00:23:24,790 --> 00:23:29,470
interface all right so something's I've

00:23:26,890 --> 00:23:31,120
been developing so that it's they get

00:23:29,470 --> 00:23:33,630
better performance when they call see

00:23:31,120 --> 00:23:37,530
libraries from and put with them pi PI

00:23:33,630 --> 00:23:40,840
but it works with C Python as well and

00:23:37,530 --> 00:23:45,910
it's really cool you can copy a manual

00:23:40,840 --> 00:23:48,130
page of a c function and basically in

00:23:45,910 --> 00:23:50,080
and to make a call and then you

00:23:48,130 --> 00:23:53,049
basically it compiles in the background

00:23:50,080 --> 00:23:56,140
and then you can talk to the c library

00:23:53,049 --> 00:23:57,250
directly so for some of the things where

00:23:56,140 --> 00:23:59,280
they've been having some problems with

00:23:57,250 --> 00:24:01,410
like lxml and things

00:23:59,280 --> 00:24:05,580
that that is an option for them doing a

00:24:01,410 --> 00:24:08,640
non capi type interface for bringing

00:24:05,580 --> 00:24:13,710
some decoder but working on a number

00:24:08,640 --> 00:24:18,090
I've compatible module that's totally

00:24:13,710 --> 00:24:20,480
written in our Python and they're

00:24:18,090 --> 00:24:24,660
getting really close with that and the

00:24:20,480 --> 00:24:26,130
Python 3 version that there's a branch

00:24:24,660 --> 00:24:29,370
and that's getting really close to

00:24:26,130 --> 00:24:34,200
passing all the tests as well so as I

00:24:29,370 --> 00:24:35,550
said as its I didn't want to do pai pai

00:24:34,200 --> 00:24:37,110
an injustice because i was totally

00:24:35,550 --> 00:24:39,570
focused on performance it's a really

00:24:37,110 --> 00:24:42,810
really cool project and there's little

00:24:39,570 --> 00:24:45,300
things that come out of it that actually

00:24:42,810 --> 00:24:46,650
help the community overall so the other

00:24:45,300 --> 00:24:49,470
thing i will do will do a plug for them

00:24:46,650 --> 00:24:51,660
for they do have a notation page so

00:24:49,470 --> 00:24:53,490
that's they survived by grants so the

00:24:51,660 --> 00:24:55,200
python software foundation is given some

00:24:53,490 --> 00:24:57,360
grants for them to do work but they have

00:24:55,200 --> 00:25:02,600
specific donation pages for the numpy

00:24:57,360 --> 00:25:08,310
work there's Armand is working on a

00:25:02,600 --> 00:25:10,590
software transaction management thing to

00:25:08,310 --> 00:25:13,050
try and improve concurrency and things

00:25:10,590 --> 00:25:15,000
like that again if he can get it working

00:25:13,050 --> 00:25:16,380
because he wants it to be as something

00:25:15,000 --> 00:25:20,310
could possibly role in C pies and that's

00:25:16,380 --> 00:25:22,920
really good for the community as i said

00:25:20,310 --> 00:25:24,270
the slides are up it's a slightly

00:25:22,920 --> 00:25:27,030
earlier version of these i will update

00:25:24,270 --> 00:25:30,720
them with the later ones and if anybody

00:25:27,030 --> 00:25:33,480
wants to talk to me i'm here until mid

00:25:30,720 --> 00:25:35,850
tuesday afternoon i was asked if he was

00:25:33,480 --> 00:25:37,170
going to be a pipe i sprint i might be

00:25:35,850 --> 00:25:39,180
working on trying to make some of my

00:25:37,170 --> 00:25:41,760
stuff go pass faster and pi phi if you

00:25:39,180 --> 00:25:44,750
interested but if anybody's got

00:25:41,760 --> 00:25:44,750
questions i'm heavy dancer

00:25:49,960 --> 00:25:53,360
how does it go with the global

00:25:52,010 --> 00:25:56,810
interpreter lock and all that sort of

00:25:53,360 --> 00:25:58,400
stuff exactly the same as C Python thank

00:25:56,810 --> 00:25:59,840
you that's what the software

00:25:58,400 --> 00:26:02,120
transactional memory stuff yeah he

00:25:59,840 --> 00:26:09,200
honors out them and to address any more

00:26:02,120 --> 00:26:10,490
questions there's a bit of naive

00:26:09,200 --> 00:26:14,330
question because I'm totally out of the

00:26:10,490 --> 00:26:17,600
loop with pi PI you said Django runs on

00:26:14,330 --> 00:26:20,000
Wi-Fi are there any but i'm very lazy do

00:26:17,600 --> 00:26:22,520
i need to do anything modify django do

00:26:20,000 --> 00:26:23,930
it just runs just runs really well you'd

00:26:22,520 --> 00:26:25,910
have to watch out for is like a fuse

00:26:23,930 --> 00:26:27,110
report lame as if you had the Django

00:26:25,910 --> 00:26:29,600
plugin for there then you're going to

00:26:27,110 --> 00:26:32,260
get your second same prob loaded go and

00:26:29,600 --> 00:26:43,130
delete the dot heroes and you'll be fine

00:26:32,260 --> 00:26:45,440
yep looks back I was just looking at

00:26:43,130 --> 00:26:47,840
your benchmarks before and one thing

00:26:45,440 --> 00:26:50,420
that struck me a little bit was that all

00:26:47,840 --> 00:26:52,190
of the short running benchmarks that

00:26:50,420 --> 00:26:54,620
were the ones that were all slow and the

00:26:52,190 --> 00:26:57,290
long running wand was the one that had

00:26:54,620 --> 00:26:59,330
sped up three times and i was wondering

00:26:57,290 --> 00:27:01,280
is it perhaps like a start-up

00:26:59,330 --> 00:27:04,580
initialization cost well look when

00:27:01,280 --> 00:27:07,130
you're just to run pi PI yep I mean yeah

00:27:04,580 --> 00:27:09,440
I did look at that as an issue the

00:27:07,130 --> 00:27:11,120
interesting thing is I'm I was I failed

00:27:09,440 --> 00:27:13,820
to me so I'm doing 50 iterations of all

00:27:11,120 --> 00:27:18,230
of those so you just seeing the speed of

00:27:13,820 --> 00:27:20,750
one iteration it doesn't prove if the

00:27:18,230 --> 00:27:25,310
the way the jit works is around about

00:27:20,750 --> 00:27:27,050
2,000 loops it starts to trace and it's

00:27:25,310 --> 00:27:28,720
a there's a fudge factor involved so

00:27:27,050 --> 00:27:31,790
that not everything gets compiler once

00:27:28,720 --> 00:27:33,770
and it will depending on how functions

00:27:31,790 --> 00:27:35,900
get called and things a little still

00:27:33,770 --> 00:27:37,400
keep trying to optimize through the

00:27:35,900 --> 00:27:38,870
other thing which is quite neat is the

00:27:37,400 --> 00:27:40,670
regular expression engine is actually

00:27:38,870 --> 00:27:42,770
jittered so if you've got regular

00:27:40,670 --> 00:27:44,270
expressions you get some addresses that

00:27:42,770 --> 00:27:45,890
guy didn't get a chance to get that

00:27:44,270 --> 00:27:49,040
benchmark good good good enough for that

00:27:45,890 --> 00:27:51,770
was really good demo but you know I

00:27:49,040 --> 00:27:54,859
looked at that it's just the XML one was

00:27:51,770 --> 00:27:58,019
long because of a lot of data

00:27:54,859 --> 00:28:01,109
and you as you would have seen on the

00:27:58,019 --> 00:28:04,229
jetta fewer we were 400,000 loops in

00:28:01,109 --> 00:28:07,499
just that one benchmark but in the xl1

00:28:04,229 --> 00:28:10,049
which was reasonably short the jetta

00:28:07,499 --> 00:28:11,940
fewer i think there was about 16,000

00:28:10,049 --> 00:28:17,759
loops so the the jet should have kicked

00:28:11,940 --> 00:28:20,639
in you've mentioned that you don't need

00:28:17,759 --> 00:28:22,919
to rebuild by pi after modifying the

00:28:20,639 --> 00:28:25,619
Python 5s that go in it could you expand

00:28:22,919 --> 00:28:27,869
a bit more on that yep okay so I mean pi

00:28:25,619 --> 00:28:29,970
pi is written and two types of Python so

00:28:27,869 --> 00:28:32,159
there's the our Python alright so that's

00:28:29,970 --> 00:28:34,590
the stuff that you do have to regenerate

00:28:32,159 --> 00:28:39,539
for so if you are down into the core of

00:28:34,590 --> 00:28:41,399
the interpreter with the IR pythoness if

00:28:39,539 --> 00:28:45,029
you made a change there yes you have to

00:28:41,399 --> 00:28:46,259
regenerate that they do have some things

00:28:45,029 --> 00:28:48,269
with what they have is they have a very

00:28:46,259 --> 00:28:49,679
slow mechanism where you actually run it

00:28:48,269 --> 00:28:50,940
with the normal Python interpreter you

00:28:49,679 --> 00:28:53,099
can run unit tests and that's this

00:28:50,940 --> 00:28:56,249
suggestion for if you're making little

00:28:53,099 --> 00:28:58,229
tweaks in the back end part of it but in

00:28:56,249 --> 00:28:59,669
a lot of cases because where I was

00:28:58,229 --> 00:29:02,999
looking at it from a production point of

00:28:59,669 --> 00:29:06,149
view it's where this DC extensions would

00:29:02,999 --> 00:29:08,580
be for C Python that's in pure poison

00:29:06,149 --> 00:29:10,559
them because they load at each time that

00:29:08,580 --> 00:29:13,109
you run run it of course the optimizer

00:29:10,559 --> 00:29:14,190
hits that so that's for the type of

00:29:13,109 --> 00:29:16,859
thing I was doing that's where I didn't

00:29:14,190 --> 00:29:19,159
have to rebuild it but they've you know

00:29:16,859 --> 00:29:21,570
one thing that has probably never been

00:29:19,159 --> 00:29:23,909
put across well enough until what ice

00:29:21,570 --> 00:29:26,159
when I saw those videos from bike on

00:29:23,909 --> 00:29:28,619
this year is they sort of we never said

00:29:26,159 --> 00:29:31,019
is you know you don't need to rebuild it

00:29:28,619 --> 00:29:33,210
for every single thing a lot of it is

00:29:31,019 --> 00:29:35,009
all about just making sure did I get a

00:29:33,210 --> 00:29:37,289
small improvement and you run it through

00:29:35,009 --> 00:29:38,940
a little thing called pi PI with us to

00:29:37,289 --> 00:29:41,820
see interpret runs two thousand times

00:29:38,940 --> 00:29:46,109
slower but allows you to test out your

00:29:41,820 --> 00:29:47,970
changes so that because they say 45

00:29:46,109 --> 00:29:50,450
minutes to build best i can do is about

00:29:47,970 --> 00:29:53,309
an hour and a half and I've got an i7 so

00:29:50,450 --> 00:29:54,119
I want to know what hardware they've

00:29:53,309 --> 00:29:56,699
caused

00:29:54,119 --> 00:29:58,829
but no it's it's it's an interesting

00:29:56,699 --> 00:29:59,939
project and maybe mind my talk next to

00:29:58,829 --> 00:30:03,389
you it might be about something written

00:29:59,939 --> 00:30:14,759
an hour python so it's got me hooked one

00:30:03,389 --> 00:30:16,229
more question okay so first quick

00:30:14,759 --> 00:30:18,719
comment just by way of defending the

00:30:16,229 --> 00:30:20,519
Django benchmark the reason that is the

00:30:18,719 --> 00:30:22,559
benchmark as far as I'm aware is that

00:30:20,519 --> 00:30:24,809
Django boobs most frequent thing getting

00:30:22,559 --> 00:30:26,969
tagged as Django's slow because the

00:30:24,809 --> 00:30:28,199
template language is slow I don't

00:30:26,969 --> 00:30:29,759
believe it is true that it is slow or

00:30:28,199 --> 00:30:32,119
that it is slow enough that about is but

00:30:29,759 --> 00:30:36,509
that is the reason why the template the

00:30:32,119 --> 00:30:37,739
benchmark is that it's also not intended

00:30:36,509 --> 00:30:39,479
to be a comprehensive benchmark it's

00:30:37,739 --> 00:30:40,829
just a benchmark more Django because it

00:30:39,479 --> 00:30:43,109
happens to do something that is Jago

00:30:40,829 --> 00:30:44,849
related what I want to follow on from

00:30:43,109 --> 00:30:46,139
that is that I've got as someone's

00:30:44,849 --> 00:30:47,969
essentially the same sort of question of

00:30:46,139 --> 00:30:49,319
back at you about your benchmarks we

00:30:47,969 --> 00:30:52,139
didn't see the code that you were

00:30:49,319 --> 00:30:54,809
running yep exactly so can you summarize

00:30:52,139 --> 00:30:56,899
it all out of the benchmarking that you

00:30:54,809 --> 00:30:59,849
have done what your perception of what

00:30:56,899 --> 00:31:01,559
what pipe I'd does fast and what it does

00:30:59,849 --> 00:31:03,029
Sloan is it is an i/o operations that

00:31:01,559 --> 00:31:04,709
are going to be exactly the same but if

00:31:03,029 --> 00:31:05,909
you do fast looping or lots of lots of

00:31:04,709 --> 00:31:08,489
small loops it's going to be faster or

00:31:05,909 --> 00:31:10,529
it where we did your benchmarking take

00:31:08,489 --> 00:31:12,569
you in terms of what's faster and what

00:31:10,529 --> 00:31:14,189
is it well I mean I oh seems to be

00:31:12,569 --> 00:31:16,319
relatively seems to be very fast and

00:31:14,189 --> 00:31:17,669
certainly with the XML writing out

00:31:16,319 --> 00:31:19,399
because I was writing to a file there

00:31:17,669 --> 00:31:24,089
does seem to be a little bit of

00:31:19,399 --> 00:31:28,259
improvement there I i mean i was using

00:31:24,089 --> 00:31:31,099
just the standard sacks that comes with

00:31:28,259 --> 00:31:33,449
xml lib and you know normally that's

00:31:31,099 --> 00:31:34,709
sometimes i've had a few performance

00:31:33,449 --> 00:31:36,779
problems with that even in the writing

00:31:34,709 --> 00:31:38,129
thing it really picked up and then when

00:31:36,779 --> 00:31:41,429
you analyze it it's because it is

00:31:38,129 --> 00:31:44,369
looping really heavily so when you say

00:31:41,429 --> 00:31:46,799
in tag a whole lot of loops and stuff go

00:31:44,369 --> 00:31:48,779
off and i'm going to be a little keV the

00:31:46,799 --> 00:31:50,279
jitter doesn't just do loops I mean

00:31:48,779 --> 00:31:56,279
Luke's where it's going to get its big

00:31:50,279 --> 00:31:58,769
one and the second link on the the

00:31:56,279 --> 00:32:03,169
resource I gave the Benjamin who did the

00:31:58,769 --> 00:32:05,519
jet for pi PI explains it really well I

00:32:03,169 --> 00:32:06,240
mean I was surprised that so much

00:32:05,519 --> 00:32:08,820
improvement was

00:32:06,240 --> 00:32:12,150
and the xml stuff i really thought that

00:32:08,820 --> 00:32:14,190
the parsing and the CSV thing was going

00:32:12,150 --> 00:32:17,100
to really be improved by the by the jet

00:32:14,190 --> 00:32:18,330
in the end what I found thanks to

00:32:17,100 --> 00:32:20,790
somebody else had done a bit of tracking

00:32:18,330 --> 00:32:22,230
as the passing of floating-point wasn't

00:32:20,790 --> 00:32:24,150
quite right and the way they were

00:32:22,230 --> 00:32:25,590
passing one other part was that was sort

00:32:24,150 --> 00:32:29,280
of being doing a little bit repeatable

00:32:25,590 --> 00:32:33,000
so by changing those things it made it

00:32:29,280 --> 00:32:34,500
easier for the the jet to kickin and and

00:32:33,000 --> 00:32:36,150
in some respects that's what even the

00:32:34,500 --> 00:32:37,290
popo guys are saving you when you watch

00:32:36,150 --> 00:32:40,080
them talking about where they have

00:32:37,290 --> 00:32:41,550
problems is you look at why you know

00:32:40,080 --> 00:32:44,130
what was the reason why it wasn't

00:32:41,550 --> 00:32:46,230
optimized because the jet has this think

00:32:44,130 --> 00:32:48,030
we like what it calls guards so when

00:32:46,230 --> 00:32:49,800
it's done is tracing and it decides to

00:32:48,030 --> 00:32:51,030
break it down it sets this guard and

00:32:49,800 --> 00:32:52,620
it's basically got these little chicks

00:32:51,030 --> 00:32:54,330
saying you know have these variables

00:32:52,620 --> 00:32:57,630
changed if these variables change it has

00:32:54,330 --> 00:33:01,110
to jump out and go back and start back

00:32:57,630 --> 00:33:02,610
in Python again and so I I'm sort of

00:33:01,110 --> 00:33:03,900
guessing where I'm having problems in

00:33:02,610 --> 00:33:06,179
some of the other ones where i think the

00:33:03,900 --> 00:33:07,650
looping should be a win for me we're

00:33:06,179 --> 00:33:10,370
getting in a situation we're basically

00:33:07,650 --> 00:33:12,780
those guards are getting triggered and

00:33:10,370 --> 00:33:15,330
it may be as simple as changing a

00:33:12,780 --> 00:33:20,010
variable and things like that i'm not

00:33:15,330 --> 00:33:23,309
i'm not an expert and this is six weeks

00:33:20,010 --> 00:33:25,080
of fiddling and not full six weeks

00:33:23,309 --> 00:33:27,630
that's you know we know the day job was

00:33:25,080 --> 00:33:29,160
free or Norman normally when I'm on a

00:33:27,630 --> 00:33:31,980
plane is when I'm doing this type of

00:33:29,160 --> 00:33:33,420
stuff so but better you know but its

00:33:31,980 --> 00:33:37,740
thing and you're right about the Django

00:33:33,420 --> 00:33:39,510
thing I mean that that you know there's

00:33:37,740 --> 00:33:42,090
one site that's that supposedly ran

00:33:39,510 --> 00:33:43,950
Jenga for a long time running out the

00:33:42,090 --> 00:33:46,500
pipe I am sadly now that site is

00:33:43,950 --> 00:33:48,000
converted to the scholar and part of the

00:33:46,500 --> 00:33:50,220
reason was it was for page rendering and

00:33:48,000 --> 00:33:53,220
being fast and as you said it so it's a

00:33:50,220 --> 00:33:54,420
pure perception thing we all know with

00:33:53,220 --> 00:33:56,730
Django there are ways you can make the

00:33:54,420 --> 00:34:00,630
rendering problem not be the impact that

00:33:56,730 --> 00:34:01,920
everybody imagines it is thank you Mark

00:34:00,630 --> 00:34:03,980
thank you

00:34:01,920 --> 00:34:03,980

YouTube URL: https://www.youtube.com/watch?v=KN9eHa2cXNQ


