Title: "Understanding GPUs" - Varun Nayyar (PyCon AU 2019)
Publication date: 2019-08-03
Playlist: PyCon Australia 2019
Description: 
	Varun Nayyar

With torch and tensorflow we have begun to rely on GPUs to speed up
computations. Deep Learning or not, GPUs can provide massive computation speed
ups but it's not a panacea as NVIDIA would have you believe. Understanding how
GPUs work can tell us where we should and shouldn't use them.

https://2019.pycon-au.org/talks/understanding-gpus

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Fri Aug  2 14:50:00 2019 at Cockle Bay
Captions: 
	00:00:00,030 --> 00:00:05,640
welcome to the last talk for this

00:00:02,550 --> 00:00:08,639
session the science the data stream so

00:00:05,640 --> 00:00:10,349
following on from our previous GPU talk

00:00:08,639 --> 00:00:13,110
we've got another GPU talk on

00:00:10,349 --> 00:00:16,430
understanding GPS this time so it's

00:00:13,110 --> 00:00:18,840
given by Brunel Varun is a mathematician

00:00:16,430 --> 00:00:20,580
and former script Kitty that has worked

00:00:18,840 --> 00:00:24,960
with various research roles for startups

00:00:20,580 --> 00:00:28,349
tech hft and even Parliament relates to

00:00:24,960 --> 00:00:30,750
our earlier earlier talk he learned to

00:00:28,349 --> 00:00:32,040
program GPS when he was when his program

00:00:30,750 --> 00:00:34,020
was taking a week to finish and

00:00:32,040 --> 00:00:36,000
discovered python cuda and open source

00:00:34,020 --> 00:00:36,930
on the way bringing down compute times

00:00:36,000 --> 00:00:39,149
to four hours

00:00:36,930 --> 00:00:41,489
Python is the language he dreams in and

00:00:39,149 --> 00:00:44,670
has followed him from the cult of oo to

00:00:41,489 --> 00:00:46,440
church of functional programming thank

00:00:44,670 --> 00:00:53,219
you thank you thanks everyone for coming

00:00:46,440 --> 00:00:55,920
out today as I said so I think the title

00:00:53,219 --> 00:00:57,180
my talk is understanding GPUs and I hope

00:00:55,920 --> 00:00:59,520
to provide a very different perspective

00:00:57,180 --> 00:01:01,170
what Mark did I don't want us want to

00:00:59,520 --> 00:01:02,399
say that I learned on my GP program from

00:01:01,170 --> 00:01:05,820
Mark's tutorials so if you have any

00:01:02,399 --> 00:01:06,689
questions go to him not to me all right

00:01:05,820 --> 00:01:07,740
there's a few things that we're trying

00:01:06,689 --> 00:01:09,570
to get through today I want to kind of

00:01:07,740 --> 00:01:12,180
get it high-level view of how GPS work

00:01:09,570 --> 00:01:13,409
what the CUDA MA model works like and I

00:01:12,180 --> 00:01:14,700
think it's a bit of an irony here that

00:01:13,409 --> 00:01:16,740
I'm basic talk with extreme

00:01:14,700 --> 00:01:19,650
multi-processing at a Python conference

00:01:16,740 --> 00:01:21,689
but you know that's the reality of you

00:01:19,650 --> 00:01:23,610
know programming and machine learning in

00:01:21,689 --> 00:01:25,680
general nowadays so a few things want to

00:01:23,610 --> 00:01:27,659
talk about so firstly why what a GP is

00:01:25,680 --> 00:01:29,850
good for and of course we have the three

00:01:27,659 --> 00:01:32,880
big ones graphics what you see lara

00:01:29,850 --> 00:01:35,400
croft and evolution from 1996 to like

00:01:32,880 --> 00:01:37,140
now cryptocurrency which is something

00:01:35,400 --> 00:01:39,119
Nvidia never talks about it's the use of

00:01:37,140 --> 00:01:41,220
one of their GPUs and of course machine

00:01:39,119 --> 00:01:43,890
learning so I suppose what is something

00:01:41,220 --> 00:01:45,960
that unites all of these AI things that

00:01:43,890 --> 00:01:47,700
go together and a really really

00:01:45,960 --> 00:01:49,710
simplified level computer graphics

00:01:47,700 --> 00:01:51,720
basically made up of 3d polygons

00:01:49,710 --> 00:01:53,280
movements are done via rotations and

00:01:51,720 --> 00:01:55,590
skews which are just matrix operations

00:01:53,280 --> 00:01:57,170
so graphics are fundamentally can be

00:01:55,590 --> 00:01:59,280
pretty close to matrix operations

00:01:57,170 --> 00:02:01,049
machine learning is very very similar

00:01:59,280 --> 00:02:02,640
that way deep learning is just matrix

00:02:01,049 --> 00:02:04,649
multiplication we'll go in a bit more

00:02:02,640 --> 00:02:06,869
detail but a lot of machine learning is

00:02:04,649 --> 00:02:07,979
actually just linear algebra you think

00:02:06,869 --> 00:02:09,750
about you saw singular value

00:02:07,979 --> 00:02:11,910
decomposition that give you like your

00:02:09,750 --> 00:02:12,730
PCAs there's all these kind of like

00:02:11,910 --> 00:02:14,650
linear

00:02:12,730 --> 00:02:17,140
they're really really local and

00:02:14,650 --> 00:02:18,069
optimizable in CUDA and that it really

00:02:17,140 --> 00:02:20,110
underpin a lot of our machine learning

00:02:18,069 --> 00:02:21,459
today and cryptocurrency which is of

00:02:20,110 --> 00:02:23,530
course the redheaded stepchild of the

00:02:21,459 --> 00:02:25,360
three is really like about solving

00:02:23,530 --> 00:02:27,159
Matt's problems basically using random

00:02:25,360 --> 00:02:28,900
search and that's against something

00:02:27,159 --> 00:02:30,459
that's fundamentally distributable and

00:02:28,900 --> 00:02:32,530
something it has to be done in press

00:02:30,459 --> 00:02:35,500
many nodes and works again really well

00:02:32,530 --> 00:02:37,239
the CUDA compute model so let's start

00:02:35,500 --> 00:02:39,910
off by like how do you think with CUDA

00:02:37,239 --> 00:02:42,879
what are the things that you need to

00:02:39,910 --> 00:02:45,129
know when you when you want to program

00:02:42,879 --> 00:02:46,480
in CUDA so first of all let's get some

00:02:45,129 --> 00:02:46,810
quick couple of definitions out of the

00:02:46,480 --> 00:02:48,910
way

00:02:46,810 --> 00:02:50,950
what is cuter stands for compute unified

00:02:48,910 --> 00:02:52,540
device architecture however I've never

00:02:50,950 --> 00:02:54,280
seen it's actually written anywhere um

00:02:52,540 --> 00:02:55,540
so I think this is good as basically

00:02:54,280 --> 00:02:57,340
it's our name at this point it's not

00:02:55,540 --> 00:03:00,970
like it stands for something it just is

00:02:57,340 --> 00:03:02,530
CUDA and so CUDA GPUs are made up of

00:03:00,970 --> 00:03:03,609
CUDA cores which are grouped together in

00:03:02,530 --> 00:03:05,680
something called streaming multi

00:03:03,609 --> 00:03:07,750
processes and through multi processors

00:03:05,680 --> 00:03:09,340
can sort of analogous to your core but

00:03:07,750 --> 00:03:12,280
they're a bit more a computer core CPU

00:03:09,340 --> 00:03:14,799
core but a bit more constrained and to

00:03:12,280 --> 00:03:16,989
give you two examples like a GTX 970 has

00:03:14,799 --> 00:03:19,299
this kind of configuration and r-tx has

00:03:16,989 --> 00:03:20,799
that configuration this gives you code

00:03:19,299 --> 00:03:24,130
of an idea from its compute capability

00:03:20,799 --> 00:03:25,510
but it's only part of picture one thing

00:03:24,130 --> 00:03:27,220
that could it really requires to work

00:03:25,510 --> 00:03:29,349
really well is a lot of threads doing

00:03:27,220 --> 00:03:30,819
similar work in the similar areas of

00:03:29,349 --> 00:03:33,639
memory otherwise it doesn't really work

00:03:30,819 --> 00:03:34,900
very well and I think one of the things

00:03:33,639 --> 00:03:36,730
that really changes the difference from

00:03:34,900 --> 00:03:38,650
exciting programming openmp of some kind

00:03:36,730 --> 00:03:40,329
of like normal multi-core programming is

00:03:38,650 --> 00:03:42,130
there a way more threads than you would

00:03:40,329 --> 00:03:43,359
ever have like it was like you know

00:03:42,130 --> 00:03:46,060
magnitudes of orders difference like

00:03:43,359 --> 00:03:48,609
thousands two ten thousands versus just

00:03:46,060 --> 00:03:50,620
like tens and CPW programming so it's

00:03:48,609 --> 00:03:52,750
safety is still important but other

00:03:50,620 --> 00:03:54,880
those memory access is also a lot less

00:03:52,750 --> 00:03:56,500
flexible than CPU but you have more

00:03:54,880 --> 00:03:57,879
control of it caches so this kind of

00:03:56,500 --> 00:04:00,459
changes fundamentally the way you

00:03:57,879 --> 00:04:02,109
program in many ways but to start up a

00:04:00,459 --> 00:04:03,519
theoretical CUDA let's sort of let's

00:04:02,109 --> 00:04:05,230
dive into a couple of civil problems

00:04:03,519 --> 00:04:06,760
with a couple of assumptions first

00:04:05,230 --> 00:04:08,139
assumption is that we have an infinite

00:04:06,760 --> 00:04:10,030
number of threads all running at the

00:04:08,139 --> 00:04:11,349
same time and the second assumption

00:04:10,030 --> 00:04:12,819
we're gonna have is that our memory

00:04:11,349 --> 00:04:14,530
access is all the same as all

00:04:12,819 --> 00:04:15,819
simultaneous these are both actually

00:04:14,530 --> 00:04:18,430
lies but you know there's a good

00:04:15,819 --> 00:04:20,919
starting point I will come in I will fix

00:04:18,430 --> 00:04:23,469
our assumptions in the in the next few

00:04:20,919 --> 00:04:24,969
slides so first example is adding two

00:04:23,469 --> 00:04:26,139
arrays this is your classic tensor

00:04:24,969 --> 00:04:27,909
operation this is

00:04:26,139 --> 00:04:29,830
gonna go into deep learning I think

00:04:27,909 --> 00:04:32,680
we're just basically doing like a you

00:04:29,830 --> 00:04:34,300
know the bias so how would a CPU

00:04:32,680 --> 00:04:36,219
implement this and a CPU we just have to

00:04:34,300 --> 00:04:37,749
go loop through every element add them

00:04:36,219 --> 00:04:39,909
up together it's Owen

00:04:37,749 --> 00:04:42,219
in CUDA land however we can actually

00:04:39,909 --> 00:04:43,960
spawn n threads and each of these

00:04:42,219 --> 00:04:45,669
threads takes control of like adding

00:04:43,960 --> 00:04:48,189
each index together so like you know

00:04:45,669 --> 00:04:52,060
thread I takes care of adding AI and bi

00:04:48,189 --> 00:04:53,740
into CI this makes it a 1 so already

00:04:52,060 --> 00:04:55,810
were kind of being to see like how you

00:04:53,740 --> 00:04:58,389
know matrix how linear algebra maps

00:04:55,810 --> 00:05:01,419
really well to recruit a device summing

00:04:58,389 --> 00:05:02,439
the Ray to CPU again just gonna have to

00:05:01,419 --> 00:05:04,330
go through everything and add it to an

00:05:02,439 --> 00:05:06,699
accumulator this obviously applies to

00:05:04,330 --> 00:05:08,020
any kind of reduction approach and in

00:05:06,699 --> 00:05:09,939
the CUDA world we do something slightly

00:05:08,020 --> 00:05:12,639
different and so this is a nice picture

00:05:09,939 --> 00:05:14,469
courtesy remark actually but how we kind

00:05:12,639 --> 00:05:16,659
of reduce this what we do is we take n

00:05:14,469 --> 00:05:18,460
on two threads we take the second half

00:05:16,659 --> 00:05:19,870
of the array add it to the first half

00:05:18,460 --> 00:05:21,789
then we take the second quarter of the

00:05:19,870 --> 00:05:23,139
array added the first quarter and so on

00:05:21,789 --> 00:05:25,449
and so forth until we get just one

00:05:23,139 --> 00:05:27,189
element at the end this of course is

00:05:25,449 --> 00:05:29,710
this is just a very very simplified

00:05:27,189 --> 00:05:31,270
version of how you would do this but

00:05:29,710 --> 00:05:34,449
there's obviously far more depth you can

00:05:31,270 --> 00:05:35,400
go into an optimization finally I want

00:05:34,449 --> 00:05:37,870
to talk about matrix multiplication

00:05:35,400 --> 00:05:39,430
matrix multiplication as I stated is the

00:05:37,870 --> 00:05:41,560
under what underpins deep learning and

00:05:39,430 --> 00:05:44,139
this is something that GPUs are really

00:05:41,560 --> 00:05:45,909
really really good at so similar same

00:05:44,139 --> 00:05:48,789
idea we have N squared elements on their

00:05:45,909 --> 00:05:50,229
output so we have C 1 take C the C

00:05:48,789 --> 00:05:52,479
matrix takes the a matrix and the B

00:05:50,229 --> 00:05:55,270
matrix and we take like the first row of

00:05:52,479 --> 00:05:57,550
this left-hand matrix and take the first

00:05:55,270 --> 00:06:00,099
column of the right-hand matrix in CUDA

00:05:57,550 --> 00:06:02,409
land in CPU land we have N squared

00:06:00,099 --> 00:06:04,300
elements each element has a dot product

00:06:02,409 --> 00:06:07,000
that goes to it which makes it n cubed

00:06:04,300 --> 00:06:10,000
or if you have the magic trick then 2.81

00:06:07,000 --> 00:06:12,279
but enough to deal with that in coudl

00:06:10,000 --> 00:06:14,199
and we would spawn N squared threads

00:06:12,279 --> 00:06:15,729
each N squared thread would have an

00:06:14,199 --> 00:06:17,139
output it has a thread that calculates

00:06:15,729 --> 00:06:19,389
the dot product across the did the row

00:06:17,139 --> 00:06:21,969
in the matrix around the column and

00:06:19,389 --> 00:06:25,899
therefore kind of giving us a you know a

00:06:21,969 --> 00:06:27,370
version of Oh N and so I do wanna point

00:06:25,899 --> 00:06:29,229
out here that this is where model starts

00:06:27,370 --> 00:06:31,689
to fail a little bit you know N squared

00:06:29,229 --> 00:06:33,520
threads may not be reasonable a and B

00:06:31,689 --> 00:06:35,469
also like we have a lot of memory access

00:06:33,520 --> 00:06:37,750
here like for each time we calculate

00:06:35,469 --> 00:06:38,810
across c11 so you want to see one end

00:06:37,750 --> 00:06:40,340
all that what

00:06:38,810 --> 00:06:41,780
constantly rereading the first row of

00:06:40,340 --> 00:06:44,660
the matrix and that's actually

00:06:41,780 --> 00:06:46,430
surprisingly slow so let's start to talk

00:06:44,660 --> 00:06:51,050
about what happens when reality sets in

00:06:46,430 --> 00:06:52,250
in CUDA land so we have so first things

00:06:51,050 --> 00:06:53,990
we need to talk about that stick a bit

00:06:52,250 --> 00:06:56,180
terminology because I will be dropping

00:06:53,990 --> 00:06:58,639
this on you here in there first when I

00:06:56,180 --> 00:07:02,540
say host I mean the system and the CPU

00:06:58,639 --> 00:07:04,460
device usually represent is the

00:07:02,540 --> 00:07:07,130
streaming multiprocessor kernel is the

00:07:04,460 --> 00:07:08,930
code that runs on the GPU and the shared

00:07:07,130 --> 00:07:12,080
memory is the memory that's available in

00:07:08,930 --> 00:07:15,500
each GPU Jason

00:07:12,080 --> 00:07:17,840
so threads in a GPU are kind of sort of

00:07:15,500 --> 00:07:18,830
spread again into blocks and each of

00:07:17,840 --> 00:07:21,290
these blocks are kind of arranged

00:07:18,830 --> 00:07:26,000
together in a grid this is a simple way

00:07:21,290 --> 00:07:28,850
this is how the sir this is how the

00:07:26,000 --> 00:07:30,440
threads are sort of schedule because

00:07:28,850 --> 00:07:31,880
obviously we can't have like a million

00:07:30,440 --> 00:07:33,740
threads running simultaneously it has to

00:07:31,880 --> 00:07:35,780
be some fundamental limit we all know

00:07:33,740 --> 00:07:37,820
this answer as I said threads were

00:07:35,780 --> 00:07:39,560
divided into blocks and the blocks run

00:07:37,820 --> 00:07:41,150
these threads 32 at a time so if you had

00:07:39,560 --> 00:07:43,070
like 64 threads you'd have the block

00:07:41,150 --> 00:07:45,950
running them like you know a warp at

00:07:43,070 --> 00:07:48,950
each time and each of these threads in a

00:07:45,950 --> 00:07:50,060
warp run Simon sink so that's pretty

00:07:48,950 --> 00:07:52,340
that's one of the guarantees you have

00:07:50,060 --> 00:07:54,710
and when you're running the actual code

00:07:52,340 --> 00:07:57,050
your blocks a kind of a assigned to each

00:07:54,710 --> 00:08:00,080
of your sm's when your machine so if you

00:07:57,050 --> 00:08:02,240
have again like 68 sm's like in a 2828

00:08:00,080 --> 00:08:04,340
ETI then of course again you want to

00:08:02,240 --> 00:08:07,789
have that many blocks to actually use

00:08:04,340 --> 00:08:09,229
that your GP to the best in fact you

00:08:07,789 --> 00:08:11,030
find that Nvidia should provide you

00:08:09,229 --> 00:08:12,979
spreadsheets where you can put in your

00:08:11,030 --> 00:08:14,930
device and will tell you exactly like

00:08:12,979 --> 00:08:16,610
what kind of block thread combinations

00:08:14,930 --> 00:08:19,130
you should be using to maximize

00:08:16,610 --> 00:08:21,169
throughput one thing to notice note here

00:08:19,130 --> 00:08:22,310
is that um and I'm gonna bring this up

00:08:21,169 --> 00:08:24,139
and when I talk about memory that warps

00:08:22,310 --> 00:08:25,850
aren't completely like synchronous is

00:08:24,139 --> 00:08:27,410
sort of at a high level view that kind

00:08:25,850 --> 00:08:29,180
of asynchronous and what I mean by that

00:08:27,410 --> 00:08:30,260
is that whenever they have something I

00:08:29,180 --> 00:08:32,209
have to do something which fries them to

00:08:30,260 --> 00:08:34,070
pause like a memory access for example

00:08:32,209 --> 00:08:35,810
they kind of just give up control and

00:08:34,070 --> 00:08:38,690
then the block will didn't run another

00:08:35,810 --> 00:08:39,979
batch of warps that might that might

00:08:38,690 --> 00:08:41,810
already have might have work to do so

00:08:39,979 --> 00:08:44,060
that way you can mask your memory

00:08:41,810 --> 00:08:45,440
latency quite significantly with CUDA

00:08:44,060 --> 00:08:47,480
because you don't always have to like be

00:08:45,440 --> 00:08:50,120
waiting for data

00:08:47,480 --> 00:08:51,370
so just in quickly throw you a little

00:08:50,120 --> 00:08:53,110
bit of C++ code

00:08:51,370 --> 00:08:55,540
again there's a disco this talk is

00:08:53,110 --> 00:08:57,340
almost no Python code in it although

00:08:55,540 --> 00:08:59,140
we've all the later the second half will

00:08:57,340 --> 00:09:00,400
have a lot of albums are familiar with

00:08:59,140 --> 00:09:02,800
but this is talking about the underlying

00:09:00,400 --> 00:09:07,570
stuff of what's going on so here we talk

00:09:02,800 --> 00:09:09,760
about you know so we have the kind of

00:09:07,570 --> 00:09:11,650
thing so here this int index tells us

00:09:09,760 --> 00:09:13,360
about what it's this kind of idiomatic

00:09:11,650 --> 00:09:15,580
Kudo to tell us where we want to operate

00:09:13,360 --> 00:09:17,800
in the matrix and we just kind of like

00:09:15,580 --> 00:09:20,260
we just simply write like a function

00:09:17,800 --> 00:09:21,640
that just as the Y the summation and

00:09:20,260 --> 00:09:23,380
that's all we do we don't really have to

00:09:21,640 --> 00:09:24,490
do much more than that and then the

00:09:23,380 --> 00:09:26,650
kernel will just spread it across the

00:09:24,490 --> 00:09:29,200
device to do that addition for us it's

00:09:26,650 --> 00:09:30,790
very very straightforward I want to talk

00:09:29,200 --> 00:09:32,650
about the memory because the memory is

00:09:30,790 --> 00:09:34,870
by far the biggest issue whenever you

00:09:32,650 --> 00:09:36,580
program in CUDA and this kind of where

00:09:34,870 --> 00:09:37,900
the hard limits of what a GPU can and

00:09:36,580 --> 00:09:40,510
cannot do really come through very

00:09:37,900 --> 00:09:43,210
rarely is a GPU compute bound it can be

00:09:40,510 --> 00:09:44,770
but that's rarely the case and this is

00:09:43,210 --> 00:09:47,710
really comes back to the situation where

00:09:44,770 --> 00:09:50,380
oh one is not exactly a one I'll tell

00:09:47,710 --> 00:09:53,680
you why so eight the first two examples

00:09:50,380 --> 00:09:55,330
provided there a I plus equals bi and in

00:09:53,680 --> 00:09:56,860
C equals three plus five in the standard

00:09:55,330 --> 00:09:58,240
Comp Sci syntax we're like hey that's

00:09:56,860 --> 00:10:00,610
just a one each of these is just one

00:09:58,240 --> 00:10:03,130
operation but the first operation has

00:10:00,610 --> 00:10:06,190
memory latency involved when that AAI

00:10:03,130 --> 00:10:07,810
has to be added to BI they have to

00:10:06,190 --> 00:10:08,920
receive both of those data points and

00:10:07,810 --> 00:10:11,170
that's just not something that's

00:10:08,920 --> 00:10:13,570
instantaneous we know this for even in a

00:10:11,170 --> 00:10:16,150
CPU this is an issue but in a GPU it's

00:10:13,570 --> 00:10:18,670
much more explicit and so as a result

00:10:16,150 --> 00:10:20,080
you can't really heat you got you have

00:10:18,670 --> 00:10:22,360
to kind of really design around the

00:10:20,080 --> 00:10:23,470
memory access quite a lot and this kind

00:10:22,360 --> 00:10:25,420
of limits a lot of what we're talking

00:10:23,470 --> 00:10:27,910
about and for example when I talk about

00:10:25,420 --> 00:10:29,410
when you say you know doing a you know

00:10:27,910 --> 00:10:31,450
trying to access a random element in an

00:10:29,410 --> 00:10:33,370
array that's not something that's super

00:10:31,450 --> 00:10:34,780
cool fast with the CUDA because let's

00:10:33,370 --> 00:10:36,700
say each thread is accessing a different

00:10:34,780 --> 00:10:39,280
point in the array that's not continuous

00:10:36,700 --> 00:10:41,020
memory access so we have a few latency

00:10:39,280 --> 00:10:43,030
sources in addition to all this the

00:10:41,020 --> 00:10:44,650
first one and this is something that I

00:10:43,030 --> 00:10:46,780
think we've seen is that we have to copy

00:10:44,650 --> 00:10:48,730
the data from the CPU to the GPU and

00:10:46,780 --> 00:10:50,560
this is high bandwidth but high overhead

00:10:48,730 --> 00:10:52,750
so you know sending small amounts of

00:10:50,560 --> 00:10:55,060
data versus large amounts of data is not

00:10:52,750 --> 00:10:56,530
quite linear that way basically you

00:10:55,060 --> 00:10:59,170
almost always want to batch as much data

00:10:56,530 --> 00:11:01,150
together as possible data from the GPU

00:10:59,170 --> 00:11:02,440
to the RAM is probably when you do any

00:11:01,150 --> 00:11:03,950
computations that's the biggest

00:11:02,440 --> 00:11:05,330
limitation the

00:11:03,950 --> 00:11:07,820
computer GPU is just kind of like your

00:11:05,330 --> 00:11:09,740
overhead at the start and yet some cases

00:11:07,820 --> 00:11:11,620
are really really fast and they're the

00:11:09,740 --> 00:11:13,730
ones you want to use as much as possible

00:11:11,620 --> 00:11:15,020
anyway so let's now get to the part

00:11:13,730 --> 00:11:17,990
which we really wanted to get to which

00:11:15,020 --> 00:11:18,530
is talking about GPUs in machine

00:11:17,990 --> 00:11:22,010
learning

00:11:18,530 --> 00:11:23,360
okay so firstly deep learning what

00:11:22,010 --> 00:11:24,590
exactly is deep learning we've all

00:11:23,360 --> 00:11:25,130
probably read use a bit of pike torture

00:11:24,590 --> 00:11:28,490
tensorflow

00:11:25,130 --> 00:11:30,920
but each layer each forward pass is just

00:11:28,490 --> 00:11:33,080
a matrix multiply and if anyone is not

00:11:30,920 --> 00:11:34,100
actually interested anyone who does deep

00:11:33,080 --> 00:11:36,380
learning has are implemented around

00:11:34,100 --> 00:11:37,820
deepening it's a really simple thing to

00:11:36,380 --> 00:11:39,440
do in fact I think it's one of the

00:11:37,820 --> 00:11:42,050
simplest algorithms that we use in ml

00:11:39,440 --> 00:11:45,230
from a pure mathematical or programming

00:11:42,050 --> 00:11:46,760
perspective and one of the fold passes

00:11:45,230 --> 00:11:48,710
is a matrix multiply and we've kind of

00:11:46,760 --> 00:11:51,470
discussed briefly beforehand that this

00:11:48,710 --> 00:11:53,870
matrix multiply is really really fast on

00:11:51,470 --> 00:11:56,360
a GPU so that's something that we get

00:11:53,870 --> 00:11:58,730
speed up now non-linearity is not going

00:11:56,360 --> 00:12:01,040
to be vastly different to how it runs on

00:11:58,730 --> 00:12:03,230
a CPU versus a GPU but additionally the

00:12:01,040 --> 00:12:05,480
back prop that we have is going to use a

00:12:03,230 --> 00:12:06,530
lot of matrix multiplications I've just

00:12:05,480 --> 00:12:08,540
provided them there for anyone

00:12:06,530 --> 00:12:09,890
interested but you have another couple

00:12:08,540 --> 00:12:11,930
of matrix multiplications coming through

00:12:09,890 --> 00:12:14,510
one of the tricky parts here is usually

00:12:11,930 --> 00:12:15,680
how we cache each for the back pro but

00:12:14,510 --> 00:12:17,630
that's something that can be dealt with

00:12:15,680 --> 00:12:18,770
a different time I want to point out

00:12:17,630 --> 00:12:20,690
here than other thing that we actually

00:12:18,770 --> 00:12:22,880
haven't fundamentally changed the way we

00:12:20,690 --> 00:12:24,740
train your own that's greater descent is

00:12:22,880 --> 00:12:28,580
a sequential algorithm we need to see

00:12:24,740 --> 00:12:30,170
our error back propagate the error back

00:12:28,580 --> 00:12:31,220
prop these this is still a sequential

00:12:30,170 --> 00:12:33,050
algorithm we have not actually touched

00:12:31,220 --> 00:12:35,450
how the algorithm works what we've done

00:12:33,050 --> 00:12:36,860
is improved how those passes were and

00:12:35,450 --> 00:12:38,540
speed up the forward pass and the

00:12:36,860 --> 00:12:41,450
backward pass are now significantly

00:12:38,540 --> 00:12:43,580
faster because of the GPU and this is

00:12:41,450 --> 00:12:45,920
particularly noticeable because in a GPU

00:12:43,580 --> 00:12:47,900
training under in a neural net training

00:12:45,920 --> 00:12:49,520
scenario we could have a lot of like

00:12:47,900 --> 00:12:51,740
epochs and batches and I've just taken

00:12:49,520 --> 00:12:53,420
20,000 as an example but that's probably

00:12:51,740 --> 00:12:55,400
under selling how many forward and

00:12:53,420 --> 00:12:57,080
backward passes really go through in any

00:12:55,400 --> 00:12:59,240
kind of neural net training and when you

00:12:57,080 --> 00:13:00,830
have those 20x to 30x speed ups that we

00:12:59,240 --> 00:13:02,960
usually get from like some standard

00:13:00,830 --> 00:13:04,040
classic GP trainings this really adds up

00:13:02,960 --> 00:13:06,860
and you're going from like you know a

00:13:04,040 --> 00:13:08,660
week a few hours and that's something

00:13:06,860 --> 00:13:10,550
that is obviously really really

00:13:08,660 --> 00:13:11,990
noticeable when you're backward and

00:13:10,550 --> 00:13:14,990
forward and backward passing just so

00:13:11,990 --> 00:13:15,920
that many times I want to do sort of

00:13:14,990 --> 00:13:17,030
like spread back and just think about

00:13:15,920 --> 00:13:18,140
other forms of deep learning

00:13:17,030 --> 00:13:20,030
obviously that

00:13:18,140 --> 00:13:21,680
this here just talks at the multi-layer

00:13:20,030 --> 00:13:23,180
perceptron which is your standard fully

00:13:21,680 --> 00:13:26,080
connected layer or linear layer

00:13:23,180 --> 00:13:28,250
depending on which package you're using

00:13:26,080 --> 00:13:30,050
so these have about a 20 to 30 X

00:13:28,250 --> 00:13:32,750
speed-up and that's a pretty well set

00:13:30,050 --> 00:13:35,900
benchmark convolutions are interesting

00:13:32,750 --> 00:13:38,510
in this case why because convolutions

00:13:35,900 --> 00:13:40,430
are compute bound on a GPU which is not

00:13:38,510 --> 00:13:41,330
quite which is very uncommon because if

00:13:40,430 --> 00:13:43,190
you can find a minute about how the

00:13:41,330 --> 00:13:46,520
spatial convolution works we're kind of

00:13:43,190 --> 00:13:48,680
like running a filter across a matrix

00:13:46,520 --> 00:13:50,990
and we don't have as many reads and this

00:13:48,680 --> 00:13:53,510
is the lack of requirement for reading

00:13:50,990 --> 00:13:55,460
is act help speed up the convolution

00:13:53,510 --> 00:13:56,540
significantly I do actually point out

00:13:55,460 --> 00:13:58,430
that the way convolutions are

00:13:56,540 --> 00:13:59,720
implemented on a GPU fundamentally very

00:13:58,430 --> 00:14:01,790
different to how we would implement them

00:13:59,720 --> 00:14:03,770
on a CPU and this is again something

00:14:01,790 --> 00:14:06,410
that NVIDIA has really optimized quite a

00:14:03,770 --> 00:14:07,910
lot for us and this is something also

00:14:06,410 --> 00:14:09,350
that actually shares across quite nicely

00:14:07,910 --> 00:14:10,640
so actually for anyone who remember

00:14:09,350 --> 00:14:14,330
who's done the neural net research might

00:14:10,640 --> 00:14:15,740
recognize the Aleks net topology there

00:14:14,330 --> 00:14:16,760
and one of the things that Alex and I

00:14:15,740 --> 00:14:19,700
did was to actually share the

00:14:16,760 --> 00:14:20,960
convolution layers across two GPUs and

00:14:19,700 --> 00:14:23,330
this is something that paralyzes really

00:14:20,960 --> 00:14:25,340
well in fact if you're using a multi GPU

00:14:23,330 --> 00:14:27,920
set up convolution is paranoiac

00:14:25,340 --> 00:14:30,530
convolutions parallel eyes very very

00:14:27,920 --> 00:14:31,790
nicely now RN ends are the interesting

00:14:30,530 --> 00:14:34,820
one here so our ananza

00:14:31,790 --> 00:14:36,230
from my experience a lot slower I don't

00:14:34,820 --> 00:14:38,270
know how people been training for a

00:14:36,230 --> 00:14:41,720
while but I think for the longest time

00:14:38,270 --> 00:14:43,340
that we went before ku DNN came out RN

00:14:41,720 --> 00:14:46,670
ends are actually not much faster on the

00:14:43,340 --> 00:14:49,820
GPU than were on a CPU for plus yes but

00:14:46,670 --> 00:14:51,020
backward prop not so great and so this

00:14:49,820 --> 00:14:52,850
kind of says that and this is another

00:14:51,020 --> 00:14:55,760
thing we're just memory bound here

00:14:52,850 --> 00:14:57,350
LST msg i use have a lot of small units

00:14:55,760 --> 00:15:00,110
in them as opposed to these giant

00:14:57,350 --> 00:15:02,360
matrices that we have for multi-layer

00:15:00,110 --> 00:15:03,740
perceptrons and the small tiling

00:15:02,360 --> 00:15:06,170
matrices that we have for convolutions

00:15:03,740 --> 00:15:08,090
and this kind of results in Dhahran ends

00:15:06,170 --> 00:15:09,440
being mostly main memory bound and much

00:15:08,090 --> 00:15:12,260
harder to deal with because it kind of

00:15:09,440 --> 00:15:13,760
sequential I also wanted to point out

00:15:12,260 --> 00:15:15,860
that besides deep learning

00:15:13,760 --> 00:15:18,770
you can't find CUDA unexpected places

00:15:15,860 --> 00:15:19,580
and this is a I think Rapids has already

00:15:18,770 --> 00:15:22,370
worked this one out

00:15:19,580 --> 00:15:25,970
but this is the GMM equation for those

00:15:22,370 --> 00:15:27,470
of you who haven't recognized the ml g

00:15:25,970 --> 00:15:28,670
Gaussian mixture models have that

00:15:27,470 --> 00:15:30,470
likelihood function needs to be

00:15:28,670 --> 00:15:31,290
evaluated for each iteration of

00:15:30,470 --> 00:15:34,470
expectation that

00:15:31,290 --> 00:15:37,259
zatia and this summation can be can be

00:15:34,470 --> 00:15:40,199
paired up across the data points quite

00:15:37,259 --> 00:15:43,350
significantly I've I've got speed up to

00:15:40,199 --> 00:15:45,750
25 X from my research and I think

00:15:43,350 --> 00:15:47,399
another thing this is e m is again

00:15:45,750 --> 00:15:49,290
fundamentally unchanged in this point

00:15:47,399 --> 00:15:51,540
what we're changing is the really one of

00:15:49,290 --> 00:15:53,100
the expensive computations we sped that

00:15:51,540 --> 00:15:54,959
up significantly and that speeds up the

00:15:53,100 --> 00:15:56,040
overall algorithm e/m has not been

00:15:54,959 --> 00:15:57,959
suddenly paralyzed

00:15:56,040 --> 00:16:00,240
eeehm is still sequential an a.m. he and

00:15:57,959 --> 00:16:02,940
his expectation maximization so again

00:16:00,240 --> 00:16:05,220
buddy M is generally but in any James

00:16:02,940 --> 00:16:05,940
tend to be much less expensive to Train

00:16:05,220 --> 00:16:07,769
so the speed-up

00:16:05,940 --> 00:16:10,019
goes from like you know a few hours to

00:16:07,769 --> 00:16:12,209
minutes which is still the same X but

00:16:10,019 --> 00:16:14,480
you know weeks today's feels a lot lot

00:16:12,209 --> 00:16:16,440
greater in terms of the improvements I

00:16:14,480 --> 00:16:18,149
wanna go with classification trees

00:16:16,440 --> 00:16:20,970
because this obviously extra boost rules

00:16:18,149 --> 00:16:22,470
kaggle and extra boost I have seen get

00:16:20,970 --> 00:16:24,360
ridiculously good results without any

00:16:22,470 --> 00:16:26,699
work and I'm frankly jealous of the

00:16:24,360 --> 00:16:28,410
algorithm being so good and but I think

00:16:26,699 --> 00:16:31,350
I want to talk about extra boost on the

00:16:28,410 --> 00:16:32,519
GPU and I want to talk about these are

00:16:31,350 --> 00:16:35,160
three very different approaches to

00:16:32,519 --> 00:16:37,560
looking at how GPUs can speed up your ml

00:16:35,160 --> 00:16:38,850
so for free from mind for those who

00:16:37,560 --> 00:16:40,740
forgot what classification trees are

00:16:38,850 --> 00:16:42,560
they're greedy algorithms they look for

00:16:40,740 --> 00:16:44,790
the optimal split across each parameter

00:16:42,560 --> 00:16:48,000
and choose the best one each at each

00:16:44,790 --> 00:16:49,050
point so you have to kind of adapt

00:16:48,000 --> 00:16:51,740
ations that have found a lot of

00:16:49,050 --> 00:16:53,639
popularity in ml one is boosting

00:16:51,740 --> 00:16:55,889
boosting have we have an ensemble of

00:16:53,639 --> 00:16:57,410
shallow trees and in this case it's a

00:16:55,889 --> 00:16:59,760
sequential algorithm in that that

00:16:57,410 --> 00:17:01,230
predictions that we we didn't make so

00:16:59,760 --> 00:17:03,480
which we got wrong in the previous

00:17:01,230 --> 00:17:06,209
iteration get re-weighted to be more

00:17:03,480 --> 00:17:08,189
heavily penalized so a more incentivized

00:17:06,209 --> 00:17:09,319
to correctly predict them and this kind

00:17:08,189 --> 00:17:11,130
of happens in a sequential pattern

00:17:09,319 --> 00:17:13,559
random forests are a completely

00:17:11,130 --> 00:17:15,390
different take on that random forests we

00:17:13,559 --> 00:17:17,189
just build many deep trees on a subset

00:17:15,390 --> 00:17:20,610
of the data and predictors why do we do

00:17:17,189 --> 00:17:22,409
that we get a bit more a little variance

00:17:20,610 --> 00:17:24,120
and trying to like find with the best

00:17:22,409 --> 00:17:27,030
predictors are and it has a lot more

00:17:24,120 --> 00:17:28,980
robustness than say acoustic does and

00:17:27,030 --> 00:17:29,909
why this is good because this is not

00:17:28,980 --> 00:17:32,100
linear algebra this is not about

00:17:29,909 --> 00:17:34,140
contiguous this is a very fundamentally

00:17:32,100 --> 00:17:37,440
different algorithm to what we've seen

00:17:34,140 --> 00:17:39,150
before so how do you build a tree in

00:17:37,440 --> 00:17:40,980
CUDA and we actually have two different

00:17:39,150 --> 00:17:44,520
approaches here that you could take the

00:17:40,980 --> 00:17:45,059
first approach is I think the right what

00:17:44,520 --> 00:17:47,279
I've seen

00:17:45,059 --> 00:17:48,629
is called depth-first and what this kind

00:17:47,279 --> 00:17:51,360
of says that if you think about like

00:17:48,629 --> 00:17:51,809
this we have ten parameters in our data

00:17:51,360 --> 00:17:53,929
set

00:17:51,809 --> 00:17:56,490
we can give each of our like cause or

00:17:53,929 --> 00:17:57,990
stringing multi-process is a single

00:17:56,490 --> 00:18:01,049
parameter to decide where the best split

00:17:57,990 --> 00:18:02,789
is and this is really really good when

00:18:01,049 --> 00:18:05,580
at the early stages when we have a large

00:18:02,789 --> 00:18:06,720
amount of data but we have and we need

00:18:05,580 --> 00:18:09,749
to sort of work out where the best split

00:18:06,720 --> 00:18:11,399
is across the large n so boosting fits

00:18:09,749 --> 00:18:13,110
really well with this kind of strategy

00:18:11,399 --> 00:18:14,789
because boosting tends to make very

00:18:13,110 --> 00:18:16,649
shallow trees we don't have a lot of

00:18:14,789 --> 00:18:18,299
depth we don't really have we have large

00:18:16,649 --> 00:18:20,879
amounts from day data and we still have

00:18:18,299 --> 00:18:22,710
large feature space so really what

00:18:20,879 --> 00:18:24,350
matches quite well the other alternate

00:18:22,710 --> 00:18:26,669
stretch strategy is called breadth-first

00:18:24,350 --> 00:18:29,009
where the full block instead of trying

00:18:26,669 --> 00:18:31,049
to look at just one for one feature one

00:18:29,009 --> 00:18:32,309
dimension okay is going to look at all

00:18:31,049 --> 00:18:34,830
the dimensions to find the best split

00:18:32,309 --> 00:18:36,799
now conceptually this is much better in

00:18:34,830 --> 00:18:38,820
later stages where the overhead of

00:18:36,799 --> 00:18:41,700
launching kernels to do that kind of

00:18:38,820 --> 00:18:43,139
search is much much greater and there's

00:18:41,700 --> 00:18:44,249
but much less data to search through so

00:18:43,139 --> 00:18:45,629
this actually works quite well for

00:18:44,249 --> 00:18:47,610
boosting for bagging sorry

00:18:45,629 --> 00:18:49,860
because bagging makes much deeper trees

00:18:47,610 --> 00:18:52,799
and needs to find like small app needs

00:18:49,860 --> 00:18:53,940
to basically group smaller subsets again

00:18:52,799 --> 00:18:55,769
as I want to point out this is something

00:18:53,940 --> 00:18:58,639
that I another key thing key thing

00:18:55,769 --> 00:18:59,999
that's worth know it noting is that

00:18:58,639 --> 00:19:02,369
fundamentally there's very different

00:18:59,999 --> 00:19:04,679
implementations and on the CPU we use a

00:19:02,369 --> 00:19:05,759
completely different algorithm and for

00:19:04,679 --> 00:19:07,799
anyone who's done an extra boost on the

00:19:05,759 --> 00:19:09,299
GPU so far a lot of notice that you have

00:19:07,799 --> 00:19:10,799
some sort of randomization on the answer

00:19:09,299 --> 00:19:13,350
which is not there on the CPU I think

00:19:10,799 --> 00:19:14,970
and yes that's something that's because

00:19:13,350 --> 00:19:16,110
of the way implementation it has to be

00:19:14,970 --> 00:19:19,590
fundamentally different to actually be

00:19:16,110 --> 00:19:21,929
viable on the GPU this is sort of like

00:19:19,590 --> 00:19:23,009
another interesting thing the and they

00:19:21,929 --> 00:19:25,049
improve in this because of the

00:19:23,009 --> 00:19:26,519
fundamental change here our performance

00:19:25,049 --> 00:19:28,049
improvements are significantly smaller

00:19:26,519 --> 00:19:31,830
this is from the official extra boost

00:19:28,049 --> 00:19:32,789
website and the three then one of the

00:19:31,830 --> 00:19:34,110
things they don't actually talk about

00:19:32,789 --> 00:19:36,509
they say hey this goes the scales really

00:19:34,110 --> 00:19:38,070
well with n where n is our data set it

00:19:36,509 --> 00:19:40,259
doesn't scale so well for how big a

00:19:38,070 --> 00:19:42,809
parameter size so we have like a hundred

00:19:40,259 --> 00:19:44,730
two thousand parameters this is the

00:19:42,809 --> 00:19:46,919
extra boost is a lot less impressive on

00:19:44,730 --> 00:19:49,110
its performance and on a GPU still

00:19:46,919 --> 00:19:51,480
better but not quite like the digit deep

00:19:49,110 --> 00:19:53,909
learning levels random forests are quite

00:19:51,480 --> 00:19:55,169
I'm not I got a very lot of variance on

00:19:53,909 --> 00:19:58,320
the benchmarks there so don't actually

00:19:55,169 --> 00:19:59,850
commit to a number but this also kind of

00:19:58,320 --> 00:20:02,580
has had very limited improvements and

00:19:59,850 --> 00:20:04,019
this I'm excited to see what Rapids is

00:20:02,580 --> 00:20:05,580
gonna take this because until now this

00:20:04,019 --> 00:20:07,769
is mostly being a community-driven a

00:20:05,580 --> 00:20:10,019
thing and for example there's a package

00:20:07,769 --> 00:20:12,059
called CUDA tree which was only about

00:20:10,019 --> 00:20:14,279
five to six times faster in scoring then

00:20:12,059 --> 00:20:15,509
sklearn kind of scikit-learn basically

00:20:14,279 --> 00:20:17,669
went and improved their implementation

00:20:15,509 --> 00:20:19,529
then the improvement we got in the CUDA

00:20:17,669 --> 00:20:21,240
version was much reduced so there are

00:20:19,529 --> 00:20:22,500
all kind of things this is this is

00:20:21,240 --> 00:20:23,789
probably due to the memory access

00:20:22,500 --> 00:20:27,169
patterns that we have here our memory

00:20:23,789 --> 00:20:30,450
access is very different with a with a

00:20:27,169 --> 00:20:31,980
would you go with tree building and that

00:20:30,450 --> 00:20:35,549
doesn't fit so well with the whole kind

00:20:31,980 --> 00:20:37,860
of local data access the GPS are really

00:20:35,549 --> 00:20:39,269
really good at okay so what are some

00:20:37,860 --> 00:20:40,799
trouble with the GPUs and this is sort

00:20:39,269 --> 00:20:41,940
of a bit of expert experience in an and

00:20:40,799 --> 00:20:43,200
other things have heard from people

00:20:41,940 --> 00:20:45,990
working them

00:20:43,200 --> 00:20:48,000
the first thing by flying far large is

00:20:45,990 --> 00:20:49,830
the GPU Ram obviously you can spend

00:20:48,000 --> 00:20:52,860
money on like V hundreds and get 32 gigs

00:20:49,830 --> 00:20:54,570
but they're very expensive even even a

00:20:52,860 --> 00:20:56,490
work budgets gonna bulk at that

00:20:54,570 --> 00:20:59,250
so you commonly buy let's say at 20 atti

00:20:56,490 --> 00:21:01,590
it's got 12 gigs of ram versus the cpu

00:20:59,250 --> 00:21:04,620
which is 128 gigs without even trying

00:21:01,590 --> 00:21:05,850
like you can't um this is a big issue

00:21:04,620 --> 00:21:07,470
when you're trying to like train blodge

00:21:05,850 --> 00:21:08,909
models so now we have this interesting

00:21:07,470 --> 00:21:11,220
thing because deep learning loves its

00:21:08,909 --> 00:21:12,779
batching and as a result you can

00:21:11,220 --> 00:21:14,789
actually work around it quite nicely you

00:21:12,779 --> 00:21:16,080
can load data in and out and run your

00:21:14,789 --> 00:21:18,029
forward and backward passes quite

00:21:16,080 --> 00:21:19,649
happily however that's not something

00:21:18,029 --> 00:21:21,210
unnecessary work so well across all the

00:21:19,649 --> 00:21:22,679
others and this again requires

00:21:21,210 --> 00:21:25,590
fundamental changes the way we think

00:21:22,679 --> 00:21:27,269
about solving problems we can't just you

00:21:25,590 --> 00:21:29,490
know the way we learn programming is

00:21:27,269 --> 00:21:31,139
like memory is cheap computers expensive

00:21:29,490 --> 00:21:32,850
and now we kind of completely faces

00:21:31,139 --> 00:21:33,990
other different model and we need a lot

00:21:32,850 --> 00:21:34,500
of expertise to be able to solve these

00:21:33,990 --> 00:21:36,389
problems

00:21:34,500 --> 00:21:37,919
another thing or data point I just had

00:21:36,389 --> 00:21:39,899
deep learning models have huge number of

00:21:37,919 --> 00:21:41,399
parameters and can take quite a

00:21:39,899 --> 00:21:42,870
significant percentage of your memory up

00:21:41,399 --> 00:21:45,240
and that's not something people really

00:21:42,870 --> 00:21:47,190
think about Alex net for example would

00:21:45,240 --> 00:21:48,659
take about one gig at flat-6 and mix

00:21:47,190 --> 00:21:52,230
Precision's that's float16

00:21:48,659 --> 00:21:54,809
feed each parameter multi-gpu setups and

00:21:52,230 --> 00:21:56,460
I think the multi-gpu setups are rarely

00:21:54,809 --> 00:21:58,919
a reason because of a computer issue but

00:21:56,460 --> 00:22:00,629
rather more a ram issue by forcing them

00:21:58,919 --> 00:22:02,730
due to RAM problems more so than

00:22:00,629 --> 00:22:04,320
anything other reason and again talking

00:22:02,730 --> 00:22:07,620
about things like asynchronous algorithm

00:22:04,320 --> 00:22:09,240
changes they're all I think research and

00:22:07,620 --> 00:22:11,490
asynchronous algorithms are heavily

00:22:09,240 --> 00:22:12,260
influenced by GPU limitations in many

00:22:11,490 --> 00:22:14,510
ways

00:22:12,260 --> 00:22:16,280
um identifiability in machine learning

00:22:14,510 --> 00:22:18,710
which by that I mean like if you had a

00:22:16,280 --> 00:22:20,420
hidden layer in a neural net there about

00:22:18,710 --> 00:22:22,100
their thousands of configurations that

00:22:20,420 --> 00:22:23,990
would provide provide identical results

00:22:22,100 --> 00:22:25,280
and that's the issue we can't really

00:22:23,990 --> 00:22:27,050
train things independently then merge

00:22:25,280 --> 00:22:28,100
them that's there's no determinants

00:22:27,050 --> 00:22:30,530
there's no identify ability of the

00:22:28,100 --> 00:22:32,420
parameters they're synchronous GPUs are

00:22:30,530 --> 00:22:33,800
slow this is obviously makes sense

00:22:32,420 --> 00:22:35,570
because synchronization is always slow

00:22:33,800 --> 00:22:37,700
and I think performance scaling is

00:22:35,570 --> 00:22:40,000
variable we have great benchmarks for

00:22:37,700 --> 00:22:42,170
certain algorithms like XG boost and

00:22:40,000 --> 00:22:44,090
CNN's that scale really well but not

00:22:42,170 --> 00:22:45,230
everything scales well especially not

00:22:44,090 --> 00:22:47,120
off-the-shelf you may have to do a lot

00:22:45,230 --> 00:22:48,290
of customization and tweaking and when

00:22:47,120 --> 00:22:50,600
you go into that stage you've got

00:22:48,290 --> 00:22:53,270
basically started you've signed up for a

00:22:50,600 --> 00:22:54,440
lot more work sequential algorithms so

00:22:53,270 --> 00:22:57,530
sequential algorithms are another thing

00:22:54,440 --> 00:22:58,640
that are poorly when not really done

00:22:57,530 --> 00:23:00,530
really get benefit from the whole

00:22:58,640 --> 00:23:02,060
computer computer model CUDA computer

00:23:00,530 --> 00:23:03,230
models have basically built on the side

00:23:02,060 --> 00:23:04,430
of this tense operation you have like

00:23:03,230 --> 00:23:06,380
this matrix you have this vector that

00:23:04,430 --> 00:23:08,510
needs to flow through but a lot of time

00:23:06,380 --> 00:23:10,880
series models do use like arrow errors

00:23:08,510 --> 00:23:12,080
from the previous step to sort of to

00:23:10,880 --> 00:23:13,730
predict in the next step and that's

00:23:12,080 --> 00:23:15,980
something that doesn't work well we need

00:23:13,730 --> 00:23:17,660
to sort of do like a full approach

00:23:15,980 --> 00:23:19,760
convolution approaches work really well

00:23:17,660 --> 00:23:21,020
but time series tend to be weak now this

00:23:19,760 --> 00:23:22,790
is one of the reasons why are an ends

00:23:21,020 --> 00:23:24,650
were quite slow and as I said before

00:23:22,790 --> 00:23:27,350
that you know before ku DNN came out

00:23:24,650 --> 00:23:30,560
some we didn't have fast GPU

00:23:27,350 --> 00:23:31,490
implementations of RN n training mics G

00:23:30,560 --> 00:23:33,440
boost which is a fundamentally

00:23:31,490 --> 00:23:35,600
sequential algorithm has a very variable

00:23:33,440 --> 00:23:37,070
improvement in speed this is of course

00:23:35,600 --> 00:23:38,870
can be changed with the number of GPUs

00:23:37,070 --> 00:23:40,520
you're throwing at it but upon her to

00:23:38,870 --> 00:23:42,740
make is it fundamentally a GPU has way

00:23:40,520 --> 00:23:44,330
more flops than its standard CPU so you

00:23:42,740 --> 00:23:46,430
the sudden improvement is something

00:23:44,330 --> 00:23:48,500
that's you should expect more off and

00:23:46,430 --> 00:23:49,820
you don't really get it great design in

00:23:48,500 --> 00:23:51,950
um again unchanged

00:23:49,820 --> 00:23:54,200
we are under we're fixing what's really

00:23:51,950 --> 00:23:55,610
good what's slow about it but the change

00:23:54,200 --> 00:23:57,530
of the album is not there and this could

00:23:55,610 --> 00:23:59,630
have doesn't help us scale well across

00:23:57,530 --> 00:24:00,740
many different algorithms and from talk

00:23:59,630 --> 00:24:02,120
about Bayesian inference before which

00:24:00,740 --> 00:24:04,610
over a Markov chain Monte Carlo a

00:24:02,120 --> 00:24:05,960
Hamiltonian Monte Carlo again these are

00:24:04,610 --> 00:24:08,470
sequential algorithms that see very

00:24:05,960 --> 00:24:10,220
little benefit from a CUDA compute model

00:24:08,470 --> 00:24:11,960
algorithm changes and this is something

00:24:10,220 --> 00:24:13,580
I think is very very big algorithm

00:24:11,960 --> 00:24:16,550
changes mean that we don't we don't

00:24:13,580 --> 00:24:19,160
necessarily have an instant one-to-one

00:24:16,550 --> 00:24:21,710
correlation if I know how a CPU how I

00:24:19,160 --> 00:24:24,710
find out a program a certain algorithm

00:24:21,710 --> 00:24:26,119
in ml I can't automatically put it

00:24:24,710 --> 00:24:27,139
straight into a GPU there's

00:24:26,119 --> 00:24:29,419
very big changes that went through

00:24:27,139 --> 00:24:30,739
memories not as cheap compute is so this

00:24:29,419 --> 00:24:33,529
changes the way we do things

00:24:30,739 --> 00:24:36,199
DB scan cannon and trees have a very

00:24:33,529 --> 00:24:37,969
different implementation on a CPU versus

00:24:36,199 --> 00:24:39,859
a GPU and this is something that's not

00:24:37,969 --> 00:24:41,959
obvious I say this mostly as a

00:24:39,859 --> 00:24:44,329
researcher but something like that is

00:24:41,959 --> 00:24:45,529
not it's quite a big thing like you're

00:24:44,329 --> 00:24:47,359
not trying to do you know if using

00:24:45,529 --> 00:24:48,979
off-the-shelf yeah you find you have

00:24:47,359 --> 00:24:49,309
someone else who's an expert in the

00:24:48,979 --> 00:24:50,689
field

00:24:49,309 --> 00:24:52,129
try not to myself but if you're trying

00:24:50,689 --> 00:24:53,899
to find something new you're gonna find

00:24:52,129 --> 00:24:56,179
that this kind of has a huge limitation

00:24:53,899 --> 00:24:57,529
because you have this inability to like

00:24:56,179 --> 00:24:59,839
you to just instantly say hey this is a

00:24:57,529 --> 00:25:01,819
great idea and how do I put this onto my

00:24:59,839 --> 00:25:03,739
GPU and if suddenly like you don't

00:25:01,819 --> 00:25:05,359
really have a good answer there and this

00:25:03,739 --> 00:25:09,289
is something that is work is worth

00:25:05,359 --> 00:25:11,119
considering and this is the diagram I

00:25:09,289 --> 00:25:12,769
put up here is actually the disk as a

00:25:11,119 --> 00:25:16,069
diagram of how convolutions are

00:25:12,769 --> 00:25:18,109
implemented so on a GPU convolutions are

00:25:16,069 --> 00:25:20,119
reduced to matrix multiplications which

00:25:18,109 --> 00:25:21,589
have already heavily optimized and that

00:25:20,119 --> 00:25:25,159
allows these things to go really really

00:25:21,589 --> 00:25:26,569
fast without too much extra work and

00:25:25,159 --> 00:25:27,649
finally this is kind of like leads all

00:25:26,569 --> 00:25:29,239
these things kind of lead into another

00:25:27,649 --> 00:25:31,549
and a technical debt is something that I

00:25:29,239 --> 00:25:33,049
think is something that maybe not the

00:25:31,549 --> 00:25:35,299
right term here but we are very heavily

00:25:33,049 --> 00:25:37,099
dependent on Nvidia on providing all

00:25:35,299 --> 00:25:39,109
these libraries as Mark pointed out

00:25:37,099 --> 00:25:41,059
there's a lot of disparate work going on

00:25:39,109 --> 00:25:43,129
in the CUDA field before this and we

00:25:41,059 --> 00:25:45,739
don't have good implementations across

00:25:43,129 --> 00:25:46,879
many things and this kind of this is an

00:25:45,739 --> 00:25:47,839
interesting problem because now we have

00:25:46,879 --> 00:25:50,509
increased compute power and one

00:25:47,839 --> 00:25:52,159
algorithm are we able to sort of do work

00:25:50,509 --> 00:25:55,369
other algorithms as effectively are we

00:25:52,159 --> 00:25:56,299
limited to just GPU compatible programs

00:25:55,369 --> 00:25:58,609
and let's say you want to do something

00:25:56,299 --> 00:25:59,629
that is GPU compatible what about let's

00:25:58,609 --> 00:26:00,799
say you want to do it yourself there's a

00:25:59,629 --> 00:26:03,229
lot of work that goes into developing

00:26:00,799 --> 00:26:04,549
these CUDA kernels a lot of specialized

00:26:03,229 --> 00:26:07,069
knowledge that doesn't transfer cross

00:26:04,549 --> 00:26:08,479
quite well so this kind of anything from

00:26:07,069 --> 00:26:09,889
I'm of the view that every line of code

00:26:08,479 --> 00:26:10,519
you write is a good line of code you

00:26:09,889 --> 00:26:12,799
have to support

00:26:10,519 --> 00:26:14,419
so right here CUDA kernel is a lot of

00:26:12,799 --> 00:26:16,669
work that you have to support in the

00:26:14,419 --> 00:26:17,989
future another thing this is probably a

00:26:16,669 --> 00:26:21,709
bit nitpicky for me is a lack of

00:26:17,989 --> 00:26:23,719
streaming this is the GPU CPU over have

00:26:21,709 --> 00:26:25,189
high overheads so sending small bits of

00:26:23,719 --> 00:26:27,019
information back and forth is not a

00:26:25,189 --> 00:26:29,779
really good use of your GPU

00:26:27,019 --> 00:26:31,459
I think this is the this is obviously

00:26:29,779 --> 00:26:32,989
unnoticeable ii mean when i do any kind

00:26:31,459 --> 00:26:35,359
of like signal processing kind of work

00:26:32,989 --> 00:26:37,009
but RL reinforcement learning is a

00:26:35,359 --> 00:26:39,870
really good example of something that is

00:26:37,009 --> 00:26:41,910
extremely influenced by this

00:26:39,870 --> 00:26:44,250
you see dqn which tries to like batch

00:26:41,910 --> 00:26:46,200
past samples and replays and send all

00:26:44,250 --> 00:26:48,270
that to run forward instead of just

00:26:46,200 --> 00:26:49,590
sending single states forward trying to

00:26:48,270 --> 00:26:51,480
get a sense of what action should take

00:26:49,590 --> 00:26:53,940
that's basically one of the things

00:26:51,480 --> 00:26:55,680
because I GPUs literally their overhead

00:26:53,940 --> 00:26:57,510
of sending one versus 500 isn't not a

00:26:55,680 --> 00:27:00,150
huge difference and you might as well

00:26:57,510 --> 00:27:03,030
get like a bit of an historical like

00:27:00,150 --> 00:27:06,600
overview of what happens anyway so I'm

00:27:03,030 --> 00:27:07,830
gonna quickly summarise fundamentally

00:27:06,600 --> 00:27:09,300
CPUs have high single-threaded

00:27:07,830 --> 00:27:11,130
performance with large amounts of memory

00:27:09,300 --> 00:27:11,790
they've one of the things I really want

00:27:11,130 --> 00:27:13,470
to talk about is that they're very

00:27:11,790 --> 00:27:14,700
flexible and easy to program you don't

00:27:13,470 --> 00:27:17,929
have to worry too much about how they

00:27:14,700 --> 00:27:20,400
work GPUs have multiple slower cause

00:27:17,929 --> 00:27:22,800
memory limits is much more limited and

00:27:20,400 --> 00:27:25,200
so optimized for tensile ops you can get

00:27:22,800 --> 00:27:27,240
incredible improvements but they are

00:27:25,200 --> 00:27:30,000
also much more inflexible so it all

00:27:27,240 --> 00:27:31,470
depends on what's good for you now I

00:27:30,000 --> 00:27:32,790
have some other hot takes but I'm gonna

00:27:31,470 --> 00:27:34,800
skip over them because I'm running out

00:27:32,790 --> 00:27:36,970
running over time and I wanna say thank

00:27:34,800 --> 00:27:43,759
you everyone for who came today

00:27:36,970 --> 00:27:43,759
[Applause]

00:27:44,390 --> 00:27:52,650
my people I think may have time for one

00:27:49,919 --> 00:28:01,860
or two questions do we have any

00:27:52,650 --> 00:28:06,750
questions given the lack of questions

00:28:01,860 --> 00:28:08,220
can we please see your dates okay so

00:28:06,750 --> 00:28:10,770
yeah just doesn't I may have them in

00:28:08,220 --> 00:28:12,570
different opinions so I am for people

00:28:10,770 --> 00:28:14,100
who know me I I think deep learnings

00:28:12,570 --> 00:28:16,710
right but I don't think it's the be-all

00:28:14,100 --> 00:28:18,419
and end-all so first of all if you think

00:28:16,710 --> 00:28:20,100
of investing in GPUs first check what

00:28:18,419 --> 00:28:21,929
you're doing and before going in

00:28:20,100 --> 00:28:23,669
investing in GPUs think about the

00:28:21,929 --> 00:28:25,260
algorithm first let that lead you to

00:28:23,669 --> 00:28:27,480
where you need to go don't fit like X

00:28:25,260 --> 00:28:28,590
John just or XE Bruce in there mmm and

00:28:27,480 --> 00:28:29,070
deep learning just because it's the

00:28:28,590 --> 00:28:32,010
coolest thing

00:28:29,070 --> 00:28:34,500
think about your problem wrap it well if

00:28:32,010 --> 00:28:36,330
you can use off-the-shelf code use it do

00:28:34,500 --> 00:28:37,260
not try and develop your own code one

00:28:36,330 --> 00:28:38,340
thing I wanted to point out this is

00:28:37,260 --> 00:28:39,870
something that I dealt with in the past

00:28:38,340 --> 00:28:41,520
is that device

00:28:39,870 --> 00:28:43,710
you know money device Ram is the biggest

00:28:41,520 --> 00:28:45,179
limitation so don't don't skimp there

00:28:43,710 --> 00:28:47,790
just buy the most expensive card that

00:28:45,179 --> 00:28:48,690
you can really afford another thing I

00:28:47,790 --> 00:28:50,309
wanted to point out is that local

00:28:48,690 --> 00:28:52,049
hardware tends to pay itself off quite

00:28:50,309 --> 00:28:53,520
quickly one of the reasons that Nvidia

00:28:52,049 --> 00:28:55,890
is pushing the djx is it actually does

00:28:53,520 --> 00:28:57,390
kind of make sense it's really

00:28:55,890 --> 00:28:58,799
convenient to have things locally

00:28:57,390 --> 00:29:00,660
especially in Sydney where the Internet

00:28:58,799 --> 00:29:03,240
can be a fee trying to deal with cloud

00:29:00,660 --> 00:29:05,220
data transfers can be annoying midscale

00:29:03,240 --> 00:29:09,480
does get a bit of awkward but that's one

00:29:05,220 --> 00:29:11,580
thing opinions on multi-gpu I would say

00:29:09,480 --> 00:29:12,660
the best thing if you have four GPUs run

00:29:11,580 --> 00:29:14,130
four independent experiments

00:29:12,660 --> 00:29:16,530
simultaneously don't try and use

00:29:14,130 --> 00:29:18,299
multi-gpu setups to start with I'm an

00:29:16,530 --> 00:29:20,220
opinion they're unlikely to be worth it

00:29:18,299 --> 00:29:22,410
for most unless of course you have an

00:29:20,220 --> 00:29:24,440
off-the-shelf thing like cat boost or

00:29:22,410 --> 00:29:26,790
actually Bruce it automatically scales

00:29:24,440 --> 00:29:28,500
and I would also recommend if you're

00:29:26,790 --> 00:29:29,760
doing multigp training you should be

00:29:28,500 --> 00:29:30,960
looking you need a strong engineering

00:29:29,760 --> 00:29:33,120
team and you need to be like thinking

00:29:30,960 --> 00:29:34,950
not just for GPUs in a node we need many

00:29:33,120 --> 00:29:36,600
many GPUs that's the scale you should be

00:29:34,950 --> 00:29:39,270
running at I think until then just stick

00:29:36,600 --> 00:29:41,490
with one GPU training and you will yeah

00:29:39,270 --> 00:29:44,299
you'll probably be happier for it all

00:29:41,490 --> 00:29:44,299
right those are my heart aches

00:29:47,830 --> 00:29:55,310
thank you her on those hot takes perfect

00:29:51,080 --> 00:29:59,560
amount of time thank you so much and

00:29:55,310 --> 00:29:59,560

YouTube URL: https://www.youtube.com/watch?v=gitE4bVDoyA


