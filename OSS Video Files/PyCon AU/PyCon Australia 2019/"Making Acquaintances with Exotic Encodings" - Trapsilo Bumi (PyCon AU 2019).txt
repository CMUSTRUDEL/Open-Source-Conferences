Title: "Making Acquaintances with Exotic Encodings" - Trapsilo Bumi (PyCon AU 2019)
Publication date: 2019-08-03
Playlist: PyCon Australia 2019
Description: 
	Trapsilo Bumi

Text encoding is a relatively straightforward feature of Python at first glance, but in the wild there are a lot of strange cases we can encounter. Together we will explore some occurences of out-of-ordinary encodings and how to handle them properly in Python.

https://2019.pycon-au.org/talks/making-acquaintances-with-exotic-encodings

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Sat Aug  3 11:10:00 2019 at C3.4 & C3.5
Captions: 
	00:00:00,000 --> 00:00:04,859
everybody excellent two thumbs up

00:00:03,210 --> 00:00:09,330
can I get a couple of thumbs amazing

00:00:04,859 --> 00:00:11,309
well welcome to the session the awesome

00:00:09,330 --> 00:00:13,500
session titled making acquaintances with

00:00:11,309 --> 00:00:15,210
exotic encoding so we're making lots of

00:00:13,500 --> 00:00:17,100
acquaintances right now and now we'll

00:00:15,210 --> 00:00:20,430
make acquaintances with exotic encodings

00:00:17,100 --> 00:00:22,289
and my job is just to do a quick

00:00:20,430 --> 00:00:27,269
introduction of our speaker our amazing

00:00:22,289 --> 00:00:29,130
speaker today so our speaker his name is

00:00:27,269 --> 00:00:30,660
Tripp Zillow Bumi he's come all the way

00:00:29,130 --> 00:00:33,420
over from Tokyo today to give this talk

00:00:30,660 --> 00:00:39,420
which is awesome and Bumi is a software

00:00:33,420 --> 00:00:43,020
developer for Haiti and nje so that's on

00:00:39,420 --> 00:00:44,399
me sorry guys hangar based in Tokyo and

00:00:43,020 --> 00:00:46,260
he's passionate for all kinds of tech

00:00:44,399 --> 00:00:48,000
and always exciting challenges whether

00:00:46,260 --> 00:00:49,079
it be building software solutions are

00:00:48,000 --> 00:00:51,210
sharing it with others through talks

00:00:49,079 --> 00:00:52,770
like he's doing right here and when boom

00:00:51,210 --> 00:00:55,770
is not coding he can be seen traveling

00:00:52,770 --> 00:00:59,820
ball gaming or plane and conducting the

00:00:55,770 --> 00:01:02,430
the Anglin are the angklung angklung oh

00:00:59,820 --> 00:01:04,619
my goodness sorry guys so can we all get

00:01:02,430 --> 00:01:04,950
like a really warm round of applause for

00:01:04,619 --> 00:01:12,249
Billy

00:01:04,950 --> 00:01:12,249
[Applause]

00:01:29,649 --> 00:01:36,740
just okay thank you for the introduction

00:01:32,799 --> 00:01:40,159
my name is Bumi and I'm from hen gate

00:01:36,740 --> 00:01:43,820
which is based in Tokyo Japan so first

00:01:40,159 --> 00:01:47,299
of all slides okay a little bit about me

00:01:43,820 --> 00:01:48,830
I'm from Indonesia and here I'm

00:01:47,299 --> 00:01:52,580
currently a software developer for

00:01:48,830 --> 00:01:54,789
engage which is based in Shibuya it's a

00:01:52,580 --> 00:01:58,909
b2b cloud security solutions company and

00:01:54,789 --> 00:02:01,549
it it also has a cool internship program

00:01:58,909 --> 00:02:02,780
so if you're looking for internships or

00:02:01,549 --> 00:02:05,720
if you are just looking for a new job

00:02:02,780 --> 00:02:09,140
just go and visit this URL and we have a

00:02:05,720 --> 00:02:11,690
challenge for you waiting ok so let's

00:02:09,140 --> 00:02:15,290
get right to it first of all before

00:02:11,690 --> 00:02:17,799
talking about encoding let's revisit a

00:02:15,290 --> 00:02:20,630
little bit about what encoding really is

00:02:17,799 --> 00:02:25,280
has anyone ever seen this picture before

00:02:20,630 --> 00:02:28,940
or this table Wow cool one over there so

00:02:25,280 --> 00:02:31,280
this is probably the world's arguably

00:02:28,940 --> 00:02:37,030
the world's first encoding ever it's

00:02:31,280 --> 00:02:40,310
basically the telegraph table that that

00:02:37,030 --> 00:02:42,620
defines how to encode the letters of the

00:02:40,310 --> 00:02:44,420
English language into numbers so that's

00:02:42,620 --> 00:02:47,090
what encoding basically is it's how to

00:02:44,420 --> 00:02:50,780
transform letters which are not numbers

00:02:47,090 --> 00:02:53,510
into numbers so here we can see how the

00:02:50,780 --> 00:02:55,250
letter A is you there's there's some

00:02:53,510 --> 00:02:57,340
symbols over there that that tells you

00:02:55,250 --> 00:03:01,130
which keys to press in the telegraph

00:02:57,340 --> 00:03:06,260
this is also another example of encoding

00:03:01,130 --> 00:03:10,100
basically so it's the Braille code as

00:03:06,260 --> 00:03:13,280
you probably know it it also transforms

00:03:10,100 --> 00:03:21,440
letters into symbols in this case which

00:03:13,280 --> 00:03:23,389
are dots on a 6 6 2 x 3 grid and there

00:03:21,440 --> 00:03:27,069
are they are divided into what they are

00:03:23,389 --> 00:03:30,109
called tick what are called decades and

00:03:27,069 --> 00:03:34,639
yeah so this this is also another

00:03:30,109 --> 00:03:37,300
another example of encoding when I when

00:03:34,639 --> 00:03:41,350
I was browsing Reddit I saw a great

00:03:37,300 --> 00:03:45,010
a great Eli five which someone was was

00:03:41,350 --> 00:03:48,060
asking like why am I seeing these

00:03:45,010 --> 00:03:50,830
strange symbols in my word processor and

00:03:48,060 --> 00:03:53,650
this guy explained it very well in

00:03:50,830 --> 00:03:55,810
several just several sentences computer

00:03:53,650 --> 00:03:58,930
store everything as numbers right and

00:03:55,810 --> 00:04:01,450
letters are not numbers so to fix this

00:03:58,930 --> 00:04:03,190
we agreed upon ways to encode letters as

00:04:01,450 --> 00:04:09,100
numbers so encoding is basically how to

00:04:03,190 --> 00:04:10,780
transform letters into numbers so USS Q

00:04:09,100 --> 00:04:14,980
is one of the first successful

00:04:10,780 --> 00:04:19,030
standardized mappings so we needed a way

00:04:14,980 --> 00:04:21,850
to how to we need to agree on a way of

00:04:19,030 --> 00:04:24,910
how to transform these letters into

00:04:21,850 --> 00:04:28,030
numbers that can be read by different

00:04:24,910 --> 00:04:30,730
machines so the the us ASCII was

00:04:28,030 --> 00:04:33,040
invented then and then after that

00:04:30,730 --> 00:04:35,740
different standards was created by

00:04:33,040 --> 00:04:37,840
different people around the world those

00:04:35,740 --> 00:04:40,570
those people those countries were

00:04:37,840 --> 00:04:43,030
extending this this code that was

00:04:40,570 --> 00:04:44,860
already established in two ways that can

00:04:43,030 --> 00:04:48,340
accommodate their own language so for

00:04:44,860 --> 00:04:51,730
example the Spanish people in the made

00:04:48,340 --> 00:04:56,230
up one code page that that extended esky

00:04:51,730 --> 00:04:57,640
with their own critic letters and the

00:04:56,230 --> 00:05:00,790
for example the Greek people also

00:04:57,640 --> 00:05:03,550
created some another encoding which

00:05:00,790 --> 00:05:06,760
which accommodated their letters and so

00:05:03,550 --> 00:05:09,730
on and so forth but this created what I

00:05:06,760 --> 00:05:11,740
call bilingual encoding so these in

00:05:09,730 --> 00:05:14,050
codings only accommodate their language

00:05:11,740 --> 00:05:15,520
and English they didn't need to support

00:05:14,050 --> 00:05:17,410
other languages other than their own

00:05:15,520 --> 00:05:18,430
language and because it was based on

00:05:17,410 --> 00:05:19,870
ascii then automatically it

00:05:18,430 --> 00:05:23,530
automatically supports English

00:05:19,870 --> 00:05:28,080
so these encoding z' were not financial

00:05:23,530 --> 00:05:31,210
multilingual so then some people thought

00:05:28,080 --> 00:05:32,950
well the internet is bound to bound to

00:05:31,210 --> 00:05:35,560
be used by everyone around the world so

00:05:32,950 --> 00:05:38,440
we need one way we need to agree on one

00:05:35,560 --> 00:05:40,810
way to encode all of these letters of

00:05:38,440 --> 00:05:44,500
the world all of these languages that

00:05:40,810 --> 00:05:47,050
have different kinds of glyphs into into

00:05:44,500 --> 00:05:51,030
one standard that we can we can all use

00:05:47,050 --> 00:05:54,330
so then then the Unipart the

00:05:51,030 --> 00:06:01,740
Unicode standard was created to unify

00:05:54,330 --> 00:06:04,770
those encoding so here there's a there's

00:06:01,740 --> 00:06:08,940
a unique concept called code points so

00:06:04,770 --> 00:06:10,920
Unicode doesn't actually transform the

00:06:08,940 --> 00:06:14,190
letters themselves into numbers but they

00:06:10,920 --> 00:06:17,010
first they assign the letters into

00:06:14,190 --> 00:06:19,170
specific code points which are which are

00:06:17,010 --> 00:06:21,660
numbers those code code points are

00:06:19,170 --> 00:06:26,580
numbers and then those numbers are

00:06:21,660 --> 00:06:28,830
transformed into bits so there there's a

00:06:26,580 --> 00:06:31,370
little intermediate there between the

00:06:28,830 --> 00:06:36,260
the concept of the letters themselves

00:06:31,370 --> 00:06:40,170
into what what we send which are bits so

00:06:36,260 --> 00:06:43,050
and then the Unicode standard leaves the

00:06:40,170 --> 00:06:46,050
visual rendering to a each software so

00:06:43,050 --> 00:06:48,390
it doesn't it doesn't dictate that this

00:06:46,050 --> 00:06:49,890
letter has to appear like this or this

00:06:48,390 --> 00:06:53,970
rather has to look like this

00:06:49,890 --> 00:06:56,220
it just it just defines the concept of a

00:06:53,970 --> 00:06:59,040
letter like for example the letter A in

00:06:56,220 --> 00:07:00,750
the Latin alphabet into an defines it

00:06:59,040 --> 00:07:04,170
into a specific code point like for

00:07:00,750 --> 00:07:07,920
example the code point u46 for example

00:07:04,170 --> 00:07:11,850
and then that 46 is converted into bits

00:07:07,920 --> 00:07:15,240
depending on which with you you want to

00:07:11,850 --> 00:07:18,420
use so the width of the bit sequence is

00:07:15,240 --> 00:07:20,580
called the code unit and so in this in

00:07:18,420 --> 00:07:24,630
this table for example you can see that

00:07:20,580 --> 00:07:27,330
the that the letter A can be represented

00:07:24,630 --> 00:07:29,850
by at least two different types right

00:07:27,330 --> 00:07:32,040
there's the the printed a and there's

00:07:29,850 --> 00:07:33,720
the handwritten a and Unicode doesn't

00:07:32,040 --> 00:07:35,940
care about those two different types it

00:07:33,720 --> 00:07:40,520
just cares that what you see there is

00:07:35,940 --> 00:07:42,870
the letter a and then it encodes it into

00:07:40,520 --> 00:07:45,120
into a specific code point it assigns

00:07:42,870 --> 00:07:53,490
the letter A to you plus zero zero six

00:07:45,120 --> 00:07:55,920
one and then okay and and then the sixth

00:07:53,490 --> 00:07:59,610
one can be encoded differently depending

00:07:55,920 --> 00:08:01,950
on which bit byte width you want to

00:07:59,610 --> 00:08:03,520
choose for example in utf-8 it can be

00:08:01,950 --> 00:08:08,110
encoded as 0 is

00:08:03,520 --> 00:08:10,569
as the bite 61 but in utf-32

00:08:08,110 --> 00:08:15,220
it can be it coded differently and then

00:08:10,569 --> 00:08:18,659
for other letters like for example the

00:08:15,220 --> 00:08:22,629
sharp s or the the kanji for east or

00:08:18,659 --> 00:08:24,550
this exotic letter it can be encoded it

00:08:22,629 --> 00:08:25,900
can be just assigned into a code point

00:08:24,550 --> 00:08:31,900
and then that code point can be

00:08:25,900 --> 00:08:34,659
transformed into bits so how do we do it

00:08:31,900 --> 00:08:37,919
in Python so in Python is you already

00:08:34,659 --> 00:08:41,020
know there are two types of strings for

00:08:37,919 --> 00:08:43,770
there are text strings or Unicode

00:08:41,020 --> 00:08:46,660
strings and there are byte strings or

00:08:43,770 --> 00:08:48,520
the in the documentation is officially

00:08:46,660 --> 00:08:49,390
pronounced as text sequence and binary

00:08:48,520 --> 00:08:53,740
sequence

00:08:49,390 --> 00:08:55,390
what are these actually so the text

00:08:53,740 --> 00:08:58,170
sequence is actually a sequence of

00:08:55,390 --> 00:09:00,490
Unicode points as I explained before

00:08:58,170 --> 00:09:02,829
they're not actually letters but there

00:09:00,490 --> 00:09:05,740
are actually just code points that that

00:09:02,829 --> 00:09:09,790
are assigned to a concept of a letter

00:09:05,740 --> 00:09:14,709
and then the byte streams themselves are

00:09:09,790 --> 00:09:18,459
actually just a sequence of bytes so you

00:09:14,709 --> 00:09:21,130
can know that the these sequences don't

00:09:18,459 --> 00:09:23,700
necessarily have the same length even

00:09:21,130 --> 00:09:28,149
though they encode the same information

00:09:23,700 --> 00:09:31,720
for example here you can see that in a

00:09:28,149 --> 00:09:33,670
in a text string or a Unicode string you

00:09:31,720 --> 00:09:37,050
can you can put all sorts of symbols in

00:09:33,670 --> 00:09:39,730
there and then it will work fine because

00:09:37,050 --> 00:09:43,570
because those those letters there are

00:09:39,730 --> 00:09:46,630
just code points in the Unicode whereas

00:09:43,570 --> 00:09:49,420
in a byte string those letters aren't

00:09:46,630 --> 00:09:52,240
actually aren't actually letters but

00:09:49,420 --> 00:09:54,670
there are just bytes that we represent

00:09:52,240 --> 00:09:56,680
as letters so they're the the first

00:09:54,670 --> 00:10:05,380
string there is actually a sequence of

00:09:56,680 --> 00:10:09,010
of bytes so to convert between one

00:10:05,380 --> 00:10:11,170
another we use the the functions decode

00:10:09,010 --> 00:10:14,800
and encode which we can call directly on

00:10:11,170 --> 00:10:17,080
those sequences so for if you want to

00:10:14,800 --> 00:10:21,370
transform bytes into strings

00:10:17,080 --> 00:10:23,440
then you can use the decode function and

00:10:21,370 --> 00:10:25,240
then you supply the two parameters and

00:10:23,440 --> 00:10:27,519
if you want to transform the string back

00:10:25,240 --> 00:10:32,019
into bytes you use the encode function

00:10:27,519 --> 00:10:34,149
which is vital parameters and then it

00:10:32,019 --> 00:10:37,720
transforms those code points into bytes

00:10:34,149 --> 00:10:39,910
so the first parameter is encoding which

00:10:37,720 --> 00:10:43,750
dictates how you want to transform those

00:10:39,910 --> 00:10:46,209
code points into bytes and then the

00:10:43,750 --> 00:10:49,660
second parameter is how to handle the

00:10:46,209 --> 00:10:52,120
how to handle a case where those point

00:10:49,660 --> 00:10:54,820
code points are aren't recognized or the

00:10:52,120 --> 00:10:58,480
this particular sequence of bytes aren't

00:10:54,820 --> 00:11:01,029
recognized as a valid code point so it's

00:10:58,480 --> 00:11:06,190
pretty simple if in at a glance if you

00:11:01,029 --> 00:11:09,820
want to handle encoding right so let's

00:11:06,190 --> 00:11:15,339
take a little look at how unicode is

00:11:09,820 --> 00:11:17,170
used in the wild so well we probably

00:11:15,339 --> 00:11:19,269
asked ourselves right like who in the

00:11:17,170 --> 00:11:24,220
world doesn't use Unicode in this survey

00:11:19,269 --> 00:11:26,529
by w3 text com utf-8 is used by 93% of

00:11:24,220 --> 00:11:28,959
the websites but then there are the 7%

00:11:26,529 --> 00:11:31,810
of websites that don't use Unicode like

00:11:28,959 --> 00:11:34,870
these encoding that you see here are

00:11:31,810 --> 00:11:38,110
still used by a small percentage of the

00:11:34,870 --> 00:11:41,050
internet and so even if we like to think

00:11:38,110 --> 00:11:43,990
that Unicode is wickedest and used

00:11:41,050 --> 00:11:46,510
everywhere like if you scrape around a

00:11:43,990 --> 00:11:50,260
website and you'll expect to for that

00:11:46,510 --> 00:11:52,839
website to use Unicode then probably you

00:11:50,260 --> 00:11:55,029
will be sometimes wrong and those

00:11:52,839 --> 00:11:59,170
websites will use this different

00:11:55,029 --> 00:12:02,140
encoding this is an interesting graph

00:11:59,170 --> 00:12:06,850
from Google about how Unicode has

00:12:02,140 --> 00:12:08,730
changed over the years so from the early

00:12:06,850 --> 00:12:12,550
days of the internet we can see that

00:12:08,730 --> 00:12:16,660
unicode wasn't recognized in in the in

00:12:12,550 --> 00:12:19,180
the first place but as the internet

00:12:16,660 --> 00:12:24,820
grows the usage of Unicode also grows

00:12:19,180 --> 00:12:28,120
and at 2008 it's it surpassed the the

00:12:24,820 --> 00:12:31,220
usage of old encodings like a ski and

00:12:28,120 --> 00:12:35,570
windows Latin and the other Chinese and

00:12:31,220 --> 00:12:38,630
Japanese individual encoding but yeah

00:12:35,570 --> 00:12:40,900
this graph shows us that Unicode wasn't

00:12:38,630 --> 00:12:44,500
the standard from the beginning and

00:12:40,900 --> 00:12:49,280
Unicode is still a journey to take for

00:12:44,500 --> 00:12:50,780
some countries to change their initial

00:12:49,280 --> 00:12:52,820
encoding from something that they

00:12:50,780 --> 00:12:58,760
invented themselves into the standard

00:12:52,820 --> 00:13:03,550
that everyone accepts so who hasn't seen

00:12:58,760 --> 00:13:06,800
this error like this errors is pretty

00:13:03,550 --> 00:13:10,010
for poor people handled Texas you see it

00:13:06,800 --> 00:13:13,040
quite a lot so bla bla codec cant encode

00:13:10,010 --> 00:13:15,890
character blah blah blah imposition blah

00:13:13,040 --> 00:13:19,400
blah so it this error is basically like

00:13:15,890 --> 00:13:21,860
saying you telling you that I can't

00:13:19,400 --> 00:13:24,380
recognize this this character this code

00:13:21,860 --> 00:13:26,750
point I can't I can't I don't know how

00:13:24,380 --> 00:13:29,060
to transform this code point into the

00:13:26,750 --> 00:13:31,220
sequence of bytes or if you reverse it

00:13:29,060 --> 00:13:33,380
like Unicode decoder then it's telling

00:13:31,220 --> 00:13:35,180
you that I I don't recognize the

00:13:33,380 --> 00:13:37,700
sequence of bytes and I don't know how

00:13:35,180 --> 00:13:43,910
to transform it into a valid code point

00:13:37,700 --> 00:13:47,800
a valid unicode code point so here I

00:13:43,910 --> 00:13:51,980
want to show you the example of some

00:13:47,800 --> 00:13:54,860
real-world examples of how unicode is

00:13:51,980 --> 00:13:57,380
not used as the standard which is emails

00:13:54,860 --> 00:14:00,380
because in the experience of my company

00:13:57,380 --> 00:14:02,270
we handle emails a lot and we often run

00:14:00,380 --> 00:14:04,190
into this problem because of where a

00:14:02,270 --> 00:14:06,260
Japanese company and sometimes Japanese

00:14:04,190 --> 00:14:10,250
companies don't recognize or follow the

00:14:06,260 --> 00:14:16,300
standard and we're on we run into these

00:14:10,250 --> 00:14:21,740
problems where they still use the like

00:14:16,300 --> 00:14:24,830
the Japanese old encoding so why is this

00:14:21,740 --> 00:14:28,610
happening it's because RFC eight-to-one

00:14:24,830 --> 00:14:31,730
the the RFC that formalizes the SMTP

00:14:28,610 --> 00:14:37,210
protocol it dictates the use of ascii as

00:14:31,730 --> 00:14:40,100
its transfer as its transfer format and

00:14:37,210 --> 00:14:41,200
using non ASCII characters in an email

00:14:40,100 --> 00:14:44,080
it

00:14:41,200 --> 00:14:48,000
requires SMTP extensions so SMTP

00:14:44,080 --> 00:14:50,950
initially didn't support using those

00:14:48,000 --> 00:14:55,690
characters outside of ASCII and then we

00:14:50,950 --> 00:14:59,950
needed some way some workaround to still

00:14:55,690 --> 00:15:03,520
use SMTP but transfer but transmit non

00:14:59,950 --> 00:15:07,660
ASCII characters or letters so then our

00:15:03,520 --> 00:15:11,140
se5 three to one was was created but

00:15:07,660 --> 00:15:12,940
still many email clients they don't

00:15:11,140 --> 00:15:15,570
recognize and they still don't use

00:15:12,940 --> 00:15:19,060
Unicode as the default which is why

00:15:15,570 --> 00:15:22,570
which is why some some clients some

00:15:19,060 --> 00:15:25,960
companies still don't use Unicode maybe

00:15:22,570 --> 00:15:29,650
not maybe not on purpose but unknowingly

00:15:25,960 --> 00:15:32,290
they're the employees of their of this

00:15:29,650 --> 00:15:34,330
business still use old email clients

00:15:32,290 --> 00:15:37,660
that don't provide Unicode as the

00:15:34,330 --> 00:15:41,050
default so this is one kind of a problem

00:15:37,660 --> 00:15:46,210
that we see often that sometimes makes

00:15:41,050 --> 00:15:48,250
it difficult to handle emails in a

00:15:46,210 --> 00:15:51,670
straightforward way so we needed some

00:15:48,250 --> 00:15:56,160
tricks to work around those encoding so

00:15:51,670 --> 00:16:00,220
what kind of encoding are not unicode

00:15:56,160 --> 00:16:02,740
exactly one example is this i so far

00:16:00,220 --> 00:16:04,840
eight eight five nine one or line one or

00:16:02,740 --> 00:16:07,390
CT 8 1 9 has a lot of names but

00:16:04,840 --> 00:16:10,270
basically it's the basis for the most

00:16:07,390 --> 00:16:13,570
popular 8-bit character set so if you

00:16:10,270 --> 00:16:15,520
need an 8-bit character set and you

00:16:13,570 --> 00:16:18,790
didn't you created this before the

00:16:15,520 --> 00:16:22,660
unicode standard was was implemented

00:16:18,790 --> 00:16:25,420
worldwide then you'd use latin one which

00:16:22,660 --> 00:16:27,610
is also the basis for several popular

00:16:25,420 --> 00:16:30,820
languages one of several of the most

00:16:27,610 --> 00:16:33,960
popular languages in the world can this

00:16:30,820 --> 00:16:38,740
is accommodated by this encoding so

00:16:33,960 --> 00:16:40,710
that's why this is so popular and the

00:16:38,740 --> 00:16:42,850
default encoding for these documents

00:16:40,710 --> 00:16:45,640
this is the default encoding for the

00:16:42,850 --> 00:16:48,310
documents that are delivered by HTTP and

00:16:45,640 --> 00:16:52,300
have the mime type text slash something

00:16:48,310 --> 00:16:54,190
so it's the default of many things which

00:16:52,300 --> 00:16:55,110
is why it's so popular and then there's

00:16:54,190 --> 00:16:57,150
windows

00:16:55,110 --> 00:17:00,240
fifty-two which is used by the default

00:16:57,150 --> 00:17:02,730
legacy windows legacy components for

00:17:00,240 --> 00:17:05,580
Windows why this is popular is because

00:17:02,730 --> 00:17:08,550
well one of the parts of Windows Windows

00:17:05,580 --> 00:17:11,010
or Microsoft is Microsoft Office people

00:17:08,550 --> 00:17:13,290
use Windows obviously use Microsoft's so

00:17:11,010 --> 00:17:15,990
this is why it's so popular and it's

00:17:13,290 --> 00:17:18,030
used in instill a lot of emails like if

00:17:15,990 --> 00:17:21,270
you use Outlook then probably outlook

00:17:18,030 --> 00:17:23,959
still uses this encoding and this is one

00:17:21,270 --> 00:17:28,380
of the this is the official standard for

00:17:23,959 --> 00:17:31,320
the People's Republic of China so they

00:17:28,380 --> 00:17:33,030
they don't actually recognize Unicode

00:17:31,320 --> 00:17:35,460
but they made their own character set

00:17:33,030 --> 00:17:37,560
which is Unicode compatible and it's

00:17:35,460 --> 00:17:40,590
called the Unicode transformation format

00:17:37,560 --> 00:17:42,180
and it supersedes another standard

00:17:40,590 --> 00:17:46,350
created by them before Unicode which is

00:17:42,180 --> 00:17:48,270
GP two three one two three one two this

00:17:46,350 --> 00:17:50,160
is another encoding which is popular in

00:17:48,270 --> 00:17:53,490
countries other than China which is

00:17:50,160 --> 00:17:55,080
Taiwan Hong Kong and Macau it was

00:17:53,490 --> 00:17:57,330
created in the 1980s

00:17:55,080 --> 00:18:00,870
so before unicode was popular was

00:17:57,330 --> 00:18:02,370
implemented everywhere and trying to

00:18:00,870 --> 00:18:04,290
also use this before they created their

00:18:02,370 --> 00:18:07,230
own standard which is the gb something

00:18:04,290 --> 00:18:10,110
something standard it is this big five

00:18:07,230 --> 00:18:13,140
encoding is also used by a lot of a lot

00:18:10,110 --> 00:18:16,260
of organizations extend extended like

00:18:13,140 --> 00:18:19,520
for by Microsoft by IBM by other

00:18:16,260 --> 00:18:22,740
companies so it is quite popular too and

00:18:19,520 --> 00:18:25,410
then this is the popular encoding used

00:18:22,740 --> 00:18:27,810
in Japan called shift Geist is developed

00:18:25,410 --> 00:18:30,420
by the ASCII Corporation which is quite

00:18:27,810 --> 00:18:32,640
a funny name and it is also one of the

00:18:30,420 --> 00:18:35,880
prominent Japanese character encodings

00:18:32,640 --> 00:18:37,800
that they use before unicode and then

00:18:35,880 --> 00:18:40,370
there's also other other encoding so I

00:18:37,800 --> 00:18:42,600
could go on about this forever but I

00:18:40,370 --> 00:18:44,640
just wanted to show you that there are a

00:18:42,600 --> 00:18:47,190
lot of encoding that people still use

00:18:44,640 --> 00:18:50,220
other than Unicode so how do you handle

00:18:47,190 --> 00:18:51,720
this in Python first of all of you

00:18:50,220 --> 00:18:55,410
obviously the first thing we do is use

00:18:51,720 --> 00:18:58,250
the codecs package right like so we we

00:18:55,410 --> 00:19:00,660
just call encode or decode function on

00:18:58,250 --> 00:19:02,100
the the string that we want to convert

00:19:00,660 --> 00:19:05,460
to bytes or the price we want to convert

00:19:02,100 --> 00:19:08,490
to string we can choose between three

00:19:05,460 --> 00:19:11,580
error handling parameters which

00:19:08,490 --> 00:19:13,320
is do we want to skip this or do we want

00:19:11,580 --> 00:19:15,120
to raise an exception if we can't care

00:19:13,320 --> 00:19:17,670
if we can't recognize the code point or

00:19:15,120 --> 00:19:20,070
do we want to replace the unknown code

00:19:17,670 --> 00:19:24,290
point with something else like that

00:19:20,070 --> 00:19:30,000
symbol there which you sometimes see if

00:19:24,290 --> 00:19:31,500
there's a mistake in Indian coding but

00:19:30,000 --> 00:19:34,710
what if we don't know the encoding for

00:19:31,500 --> 00:19:36,600
this text or it there is a specified

00:19:34,710 --> 00:19:38,700
encoding in like for example in the HTML

00:19:36,600 --> 00:19:40,380
document there is they specify the

00:19:38,700 --> 00:19:42,210
encoding but it doesn't match or it

00:19:40,380 --> 00:19:44,970
spits out characters that we can't see

00:19:42,210 --> 00:19:48,720
or jumbled characters or what Japanese

00:19:44,970 --> 00:19:50,429
Japanese call Mooji bucket there's or or

00:19:48,720 --> 00:19:52,350
maybe there's just no way to specify the

00:19:50,429 --> 00:19:53,760
encoding in that document or file or

00:19:52,350 --> 00:19:56,429
whatever so it's not an HTML document

00:19:53,760 --> 00:20:00,150
it's not an email it's something in the

00:19:56,429 --> 00:20:02,750
file system created by an application

00:20:00,150 --> 00:20:05,970
that just doesn't specify the encoding

00:20:02,750 --> 00:20:07,740
it happens a lot in for example in

00:20:05,970 --> 00:20:09,240
straight from websites in reading or

00:20:07,740 --> 00:20:10,620
parsing email when you open the

00:20:09,240 --> 00:20:13,620
attachments the attachments don't

00:20:10,620 --> 00:20:16,500
specify what encoding they use and well

00:20:13,620 --> 00:20:17,940
we have to guess so

00:20:16,500 --> 00:20:20,670
there's a package in Python called

00:20:17,940 --> 00:20:23,010
carded it's a character encoding or

00:20:20,670 --> 00:20:26,040
order detection library so it's based on

00:20:23,010 --> 00:20:29,070
the universal Carter project my mozilla

00:20:26,040 --> 00:20:31,400
it basically uses heuristics to to map

00:20:29,070 --> 00:20:34,320
common patterns into a specific language

00:20:31,400 --> 00:20:36,540
not be encoding but the language but

00:20:34,320 --> 00:20:38,970
then that language usually uses a

00:20:36,540 --> 00:20:41,760
specific encoding so we can guess the

00:20:38,970 --> 00:20:44,580
encoding with a specific width with

00:20:41,760 --> 00:20:46,950
certain confidence but we don't know for

00:20:44,580 --> 00:20:49,730
sure 100% if that's the encoding used

00:20:46,950 --> 00:20:53,059
but it's better than nothing right

00:20:49,730 --> 00:20:56,580
so is carded the answers to everything

00:20:53,059 --> 00:20:59,910
well I would say no we only use it as a

00:20:56,580 --> 00:21:03,330
last resort we we don't need to use card

00:20:59,910 --> 00:21:07,400
in in the first place but we have to try

00:21:03,330 --> 00:21:10,140
first for example if it's an HTTP HTTP

00:21:07,400 --> 00:21:11,850
transfer document and there is no car

00:21:10,140 --> 00:21:13,550
set parameter in the content type error

00:21:11,850 --> 00:21:15,929
if there is then we should use that

00:21:13,550 --> 00:21:17,580
because they are telling us that that's

00:21:15,929 --> 00:21:20,640
the encoding used so we don't need to

00:21:17,580 --> 00:21:21,410
guess right or if it's HTML and there is

00:21:20,640 --> 00:21:23,870
a May

00:21:21,410 --> 00:21:25,880
attack in the HTML document then we

00:21:23,870 --> 00:21:27,950
should use that encoding specified

00:21:25,880 --> 00:21:30,290
because that's what they what they tell

00:21:27,950 --> 00:21:33,260
us the encoding that is the encoding

00:21:30,290 --> 00:21:35,330
that they're using or it's XML and there

00:21:33,260 --> 00:21:39,980
is no encoding attribute in the prologue

00:21:35,330 --> 00:21:41,630
of the XML part so carnot is not the

00:21:39,980 --> 00:21:43,130
answer to everything we shouldn't use it

00:21:41,630 --> 00:21:45,920
in the first two days but only use it as

00:21:43,130 --> 00:21:48,890
a last resort but then coordinate itself

00:21:45,920 --> 00:21:50,810
is not perfect it can spit out some

00:21:48,890 --> 00:21:52,640
encoding that aren't even in the Python

00:21:50,810 --> 00:21:56,900
standard library for example you see

00:21:52,640 --> 00:21:58,580
Taiwan so and also it has sometimes as a

00:21:56,900 --> 00:22:00,290
confidence rating that is very low it's

00:21:58,580 --> 00:22:02,360
based on heuristics so it's not perfect

00:22:00,290 --> 00:22:05,780
it sometimes fails sometimes fails to

00:22:02,360 --> 00:22:07,880
get the encoding so the best the best

00:22:05,780 --> 00:22:09,380
thing to do is to set a threshold if

00:22:07,880 --> 00:22:12,800
it's below the threshold of the

00:22:09,380 --> 00:22:16,430
confidence then you should you should

00:22:12,800 --> 00:22:18,560
try other methods or just don't try to

00:22:16,430 --> 00:22:22,310
do transform it because it will spit out

00:22:18,560 --> 00:22:25,400
jumbled characters anyway then there's

00:22:22,310 --> 00:22:27,800
the last last resort which is using a

00:22:25,400 --> 00:22:30,980
static table which is based on the

00:22:27,800 --> 00:22:35,360
experience of of your organization in

00:22:30,980 --> 00:22:38,630
this case my my company collected a lot

00:22:35,360 --> 00:22:42,980
of weird encoding syn the emails that

00:22:38,630 --> 00:22:46,790
they specified and like for example some

00:22:42,980 --> 00:22:49,940
emails mean CP 932 but they write these

00:22:46,790 --> 00:22:55,370
lot of different things which mean just

00:22:49,940 --> 00:22:58,070
one thing also like for example utf-8 in

00:22:55,370 --> 00:23:01,850
the email can be specified as utf-8 Mac

00:22:58,070 --> 00:23:03,860
utf-8 DoCoMo utf-8 KDDI is a software

00:23:01,850 --> 00:23:05,930
like each operator basically has their

00:23:03,860 --> 00:23:09,770
own encoding which is basically utf-8

00:23:05,930 --> 00:23:12,280
but maybe they use the private space for

00:23:09,770 --> 00:23:18,260
their own stuff like emoji or something

00:23:12,280 --> 00:23:19,790
so in summary encoding is important for

00:23:18,260 --> 00:23:24,260
us to understand if we want to process

00:23:19,790 --> 00:23:26,390
text in a lot a lot so we should keep

00:23:24,260 --> 00:23:28,130
some tricks up to sleeve for handling

00:23:26,390 --> 00:23:32,050
exotic character mappings for example

00:23:28,130 --> 00:23:35,220
the carded package if if we can't find

00:23:32,050 --> 00:23:36,780
first of all we can't find the

00:23:35,220 --> 00:23:38,790
encoding that is specified in the

00:23:36,780 --> 00:23:41,010
document itself then we should use card

00:23:38,790 --> 00:23:42,780
it but then if card it fails then we

00:23:41,010 --> 00:23:45,450
fall back to the last resort which is a

00:23:42,780 --> 00:23:48,150
static list that we keep based on our

00:23:45,450 --> 00:23:50,430
experience on collecting documents or

00:23:48,150 --> 00:23:53,750
emails and then which one works we just

00:23:50,430 --> 00:23:58,050
try it and then if it works then we just

00:23:53,750 --> 00:24:02,040
keep that static list for for processing

00:23:58,050 --> 00:24:03,540
the next documents that we find okay

00:24:02,040 --> 00:24:05,340
that's all from me thank you and if you

00:24:03,540 --> 00:24:29,520
want to access the slides they are

00:24:05,340 --> 00:24:33,540
available here questions yeah awesome

00:24:29,520 --> 00:24:33,930
anybody have any questions I got a

00:24:33,540 --> 00:24:35,070
question

00:24:33,930 --> 00:24:38,460
and then hopefully somebody else will

00:24:35,070 --> 00:24:40,140
have a question so I'm just curious as

00:24:38,460 --> 00:24:42,000
to what business's success rate is like

00:24:40,140 --> 00:24:44,340
I loved how it was like last resort last

00:24:42,000 --> 00:24:45,690
last resort right but that's definitely

00:24:44,340 --> 00:24:47,310
that's definitely like practice right

00:24:45,690 --> 00:24:48,480
I'm just wondering what the success rate

00:24:47,310 --> 00:24:49,860
is typically it would like you know a

00:24:48,480 --> 00:24:53,640
corpus of emails whatever else that

00:24:49,860 --> 00:24:55,290
you'd find they like decoding success

00:24:53,640 --> 00:25:00,480
rate like just using all these packages

00:24:55,290 --> 00:25:02,670
okay so um well I would say what when

00:25:00,480 --> 00:25:07,020
you look at the encoding that is in the

00:25:02,670 --> 00:25:09,690
document and is probably about 70% I

00:25:07,020 --> 00:25:12,300
would say 75% of the documents just

00:25:09,690 --> 00:25:14,160
encode correctly but 25% of those

00:25:12,300 --> 00:25:16,320
documents don't so we need to use card

00:25:14,160 --> 00:25:19,530
it and then the succession credit card

00:25:16,320 --> 00:25:21,300
it is pretty high maybe if you feed like

00:25:19,530 --> 00:25:24,620
hundred documents maybe ninety eight

00:25:21,300 --> 00:25:27,720
would come out as correct I would say

00:25:24,620 --> 00:25:29,730
but there are a small percentage of

00:25:27,720 --> 00:25:31,280
documents that even card it can't guess

00:25:29,730 --> 00:25:35,010
so

00:25:31,280 --> 00:25:38,340
for example those weird extended

00:25:35,010 --> 00:25:39,810
encoding that you need to keep static

00:25:38,340 --> 00:25:42,080
list of that's why it's the last nice

00:25:39,810 --> 00:25:42,080
resort

00:25:42,340 --> 00:25:51,400
all right epic amazing so from the

00:25:49,390 --> 00:25:52,990
statistics it showed it looks like utf-8

00:25:51,400 --> 00:25:56,860
is taking over the world or at least the

00:25:52,990 --> 00:25:59,200
internet and is there ever a good reason

00:25:56,860 --> 00:26:02,250
not to use utf-8 other than making your

00:25:59,200 --> 00:26:02,250
job harder harder

00:26:02,860 --> 00:26:11,860
you mean utf-8 specifically or Unicode

00:26:06,250 --> 00:26:16,180
in general either okay so there are

00:26:11,860 --> 00:26:18,910
reasons you don't want to use utf-8 but

00:26:16,180 --> 00:26:22,470
what I mean by that is you probably want

00:26:18,910 --> 00:26:25,030
to use you f-16 or utf-32 depending on

00:26:22,470 --> 00:26:28,630
whether you use those in exotic

00:26:25,030 --> 00:26:32,710
characters a lot because if you just use

00:26:28,630 --> 00:26:35,470
the English language then utf-8 is fine

00:26:32,710 --> 00:26:37,450
but if you want to use other languages

00:26:35,470 --> 00:26:39,400
that are far away in the unicode mapping

00:26:37,450 --> 00:26:43,150
then probably it's better to consider

00:26:39,400 --> 00:26:48,670
using a 16 bit width or or or a 32 with

00:26:43,150 --> 00:26:51,250
bits bit width but for long Unicode I

00:26:48,670 --> 00:26:55,090
can't imagine why you would create new

00:26:51,250 --> 00:26:56,980
documents with non Unicode encoding the

00:26:55,090 --> 00:26:59,290
reason they exist is because they were

00:26:56,980 --> 00:27:00,760
created before Unicode existed like the

00:26:59,290 --> 00:27:03,040
programs themselves that generate the

00:27:00,760 --> 00:27:06,280
documents didn't recognize Unicode back

00:27:03,040 --> 00:27:09,280
then so that's why they exist but in now

00:27:06,280 --> 00:27:15,070
like the carnage you should use Unicode

00:27:09,280 --> 00:27:17,430
everywhere okay thank you any more

00:27:15,070 --> 00:27:17,430
questions

00:27:20,940 --> 00:27:27,330
all right well maybe just one more round

00:27:25,560 --> 00:27:29,790
of applause then for Bernie for our next

00:27:27,330 --> 00:27:30,850
presentation thank you here you go a

00:27:29,790 --> 00:27:34,200
huge thank you

00:27:30,850 --> 00:27:34,200

YouTube URL: https://www.youtube.com/watch?v=RiP6F3CL47s


