Title: "Tracing, Profiling & Debugging in Production (eBPF)" - Trent Lloyd (PyCon AU 2019)
Publication date: 2019-08-03
Playlist: PyCon Australia 2019
Description: 
	Trent Lloyd

Trace, Profile & Debug applications in production without restarting or loading a framework.We will look at a number of modern techniques including eBPF and ptrace to attach to running production applications with no preparation and minimal performance impact.

https://2019.pycon-au.org/talks/tracing-profiling-debugging-in-production-ebpf

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Sat Aug  3 14:50:00 2019 at C3.6
Captions: 
	00:00:03,589 --> 00:00:13,380
good afternoon everyone it's now my

00:00:10,080 --> 00:00:15,599
privilege to introduce Trent Loyd Trent

00:00:13,380 --> 00:00:17,880
works in Canonical's sustaining

00:00:15,599 --> 00:00:20,730
engineering team providing Ubuntu and

00:00:17,880 --> 00:00:23,519
OpenStack support where Python is the

00:00:20,730 --> 00:00:25,740
primary language previously having spent

00:00:23,519 --> 00:00:28,050
nearly a decade on the MySQL support

00:00:25,740 --> 00:00:30,359
team he's no stranger to being thrust

00:00:28,050 --> 00:00:37,890
into other teams production problems

00:00:30,359 --> 00:00:40,739
please give Trent a warm welcome all

00:00:37,890 --> 00:00:43,050
right good afternoon everyone my hope

00:00:40,739 --> 00:00:44,670
today is to give you some insight into a

00:00:43,050 --> 00:00:47,789
few little different tools or different

00:00:44,670 --> 00:00:50,480
approaches to looking at some production

00:00:47,789 --> 00:00:52,289
problems that you might have seen and

00:00:50,480 --> 00:00:55,670
hopefully the be something in here that

00:00:52,289 --> 00:00:57,870
it's all this interesting to everyone as

00:00:55,670 --> 00:00:59,910
was just mentioned

00:00:57,870 --> 00:01:02,100
I've spent pretty much my whole career

00:00:59,910 --> 00:01:03,840
working in support which means I get to

00:01:02,100 --> 00:01:05,369
enjoy other people's code bases and

00:01:03,840 --> 00:01:08,280
other people's problems without any real

00:01:05,369 --> 00:01:10,650
ability to prepare that environment for

00:01:08,280 --> 00:01:12,500
whatever debugging or whatever you know

00:01:10,650 --> 00:01:14,970
I need to do to solve that problem

00:01:12,500 --> 00:01:16,320
this might sound familiar usually most

00:01:14,970 --> 00:01:19,799
people think of it as a legacy code base

00:01:16,320 --> 00:01:22,049
but they had no input in but even on

00:01:19,799 --> 00:01:23,430
your currently non legacy code bases

00:01:22,049 --> 00:01:25,380
some of these approaches can actually

00:01:23,430 --> 00:01:30,270
still be interesting and useful in some

00:01:25,380 --> 00:01:32,970
cases the first thing I'd like to do I

00:01:30,270 --> 00:01:35,369
put up a quick poll on my Twitter just

00:01:32,970 --> 00:01:36,689
to get I'm just curious what kind of

00:01:35,369 --> 00:01:38,759
approaches people use in their

00:01:36,689 --> 00:01:40,229
production environments so at some point

00:01:38,759 --> 00:01:41,880
during the talk if you'd like to pull up

00:01:40,229 --> 00:01:43,350
my Twitter and just answer that poll

00:01:41,880 --> 00:01:45,479
about what sort of tooling or what

00:01:43,350 --> 00:01:46,979
methods you tend to use when you're

00:01:45,479 --> 00:01:48,360
looking at production problems I'd just

00:01:46,979 --> 00:01:50,549
be curious to look at that at the end

00:01:48,360 --> 00:01:52,259
and see whether that's kind of in line

00:01:50,549 --> 00:01:54,210
with what I was expecting or maybe we've

00:01:52,259 --> 00:01:57,450
got a whole bunch of like really awesome

00:01:54,210 --> 00:01:59,130
developers here who knows so Lafayette

00:01:57,450 --> 00:02:02,570
is my twitter handle and you can find

00:01:59,130 --> 00:02:02,570
that there we'll look at that at the end

00:02:05,080 --> 00:02:08,170
so right in the title of the talk I'd

00:02:06,820 --> 00:02:10,240
given you three things that we can look

00:02:08,170 --> 00:02:11,500
at they're all kind of related and I

00:02:10,240 --> 00:02:13,600
just want to kind of set the ground for

00:02:11,500 --> 00:02:15,340
what those things are so we'll talk

00:02:13,600 --> 00:02:17,560
about tracing I'm talking about

00:02:15,340 --> 00:02:19,450
precisely logging every invocation of

00:02:17,560 --> 00:02:22,810
either a function or a line within a

00:02:19,450 --> 00:02:24,010
program precisely so we want every

00:02:22,810 --> 00:02:25,600
indication we're not going to miss any

00:02:24,010 --> 00:02:28,810
we know exactly what's happened in that

00:02:25,600 --> 00:02:30,490
program by contrast often when were

00:02:28,810 --> 00:02:32,260
profiling we're looking at sampling your

00:02:30,490 --> 00:02:34,780
programs currently executing lyonel

00:02:32,260 --> 00:02:37,090
function at a set interval so instead of

00:02:34,780 --> 00:02:38,740
checking every single execution we might

00:02:37,090 --> 00:02:39,940
simply ask paths in a hundred times a

00:02:38,740 --> 00:02:43,150
second what are you doing now what are

00:02:39,940 --> 00:02:44,920
you doing now what are you doing now you

00:02:43,150 --> 00:02:46,630
vanished that is that over time we can

00:02:44,920 --> 00:02:49,500
aggregate that data in some useful way

00:02:46,630 --> 00:02:51,520
and kind of get a good feel for what

00:02:49,500 --> 00:02:53,110
Python is doing most the time without

00:02:51,520 --> 00:02:56,410
actually counting every single little

00:02:53,110 --> 00:02:58,780
item and lastly when I talk about

00:02:56,410 --> 00:03:00,850
debugging we heard earlier today for me

00:02:58,780 --> 00:03:02,350
know about interactively inspecting a

00:03:00,850 --> 00:03:04,120
program to execution state using a

00:03:02,350 --> 00:03:07,060
debugger we can maybe set a breakpoint

00:03:04,120 --> 00:03:09,040
and inspect things but I find that

00:03:07,060 --> 00:03:10,930
method of debugging is not often that

00:03:09,040 --> 00:03:12,100
great in production scenarios

00:03:10,930 --> 00:03:14,290
particularly for like online

00:03:12,100 --> 00:03:15,970
applications because once we break in

00:03:14,290 --> 00:03:17,260
our debugger at the very least we're

00:03:15,970 --> 00:03:19,270
going to pause that request that's

00:03:17,260 --> 00:03:20,470
currently being handled and most likely

00:03:19,270 --> 00:03:21,520
will actually gonna pause all the

00:03:20,470 --> 00:03:24,510
requests coming in to that application

00:03:21,520 --> 00:03:26,800
server well we're trying to debug it and

00:03:24,510 --> 00:03:28,390
so using the interactive debugger is a

00:03:26,800 --> 00:03:30,190
fantastic tool in the development phase

00:03:28,390 --> 00:03:34,180
but if you're trying to diagnose a more

00:03:30,190 --> 00:03:35,650
production scenario problem I find it's

00:03:34,180 --> 00:03:37,900
often better to look at other avenues

00:03:35,650 --> 00:03:40,540
and so when I say debugging I'm gonna be

00:03:37,900 --> 00:03:42,780
looking at more at diagnosing and fixing

00:03:40,540 --> 00:03:42,780
a bug

00:03:47,530 --> 00:03:52,720
so when we're looking at profiling well

00:03:50,200 --> 00:03:54,190
what does a profiler do when we're

00:03:52,720 --> 00:03:56,470
looking exactly what code is being

00:03:54,190 --> 00:03:58,270
executed by the program over time on

00:03:56,470 --> 00:04:00,280
average it's kind of similar at the top

00:03:58,270 --> 00:04:01,780
or task manager but instead of looking

00:04:00,280 --> 00:04:03,550
at your whole system and what processes

00:04:01,780 --> 00:04:04,810
was running you're looking just inside

00:04:03,550 --> 00:04:06,010
of your Python process and what

00:04:04,810 --> 00:04:09,760
functions are what lines are being

00:04:06,010 --> 00:04:11,560
executed now despite being four slides

00:04:09,760 --> 00:04:13,060
in I've already lied to you all because

00:04:11,560 --> 00:04:15,370
there are in fact two kinds of profilers

00:04:13,060 --> 00:04:17,410
there are tracing profilers which will

00:04:15,370 --> 00:04:19,450
actually exactly instrument every single

00:04:17,410 --> 00:04:21,190
execution and we'll know whether with

00:04:19,450 --> 00:04:24,040
the exact certain C you know how often

00:04:21,190 --> 00:04:26,260
each line was run but more commonly we

00:04:24,040 --> 00:04:27,940
use a sampling profiler which is the one

00:04:26,260 --> 00:04:29,440
that I talked about where we just look

00:04:27,940 --> 00:04:31,390
at what we're doing maybe 100 times a

00:04:29,440 --> 00:04:34,270
second or even a thousand times a second

00:04:31,390 --> 00:04:35,740
and on average you'll capture the

00:04:34,270 --> 00:04:37,840
majority of the code paths that your

00:04:35,740 --> 00:04:40,680
program is going through that aren't you

00:04:37,840 --> 00:04:40,680
know super super fast

00:04:41,370 --> 00:04:45,130
now when I say profiling most people

00:04:43,720 --> 00:04:47,320
think about that in terms of improving

00:04:45,130 --> 00:04:48,820
performance so maybe their applications

00:04:47,320 --> 00:04:51,580
not doing things as quickly as they want

00:04:48,820 --> 00:04:53,680
to or most people's dreaded my test

00:04:51,580 --> 00:04:55,810
suite takes 77 hours and so I can only

00:04:53,680 --> 00:04:57,010
get my commits done per week and so we

00:04:55,810 --> 00:04:58,540
look at a profile to kind of help us

00:04:57,010 --> 00:05:01,390
with those kinds of problems which tests

00:04:58,540 --> 00:05:02,620
are slow what parts of it are slow but

00:05:01,390 --> 00:05:04,419
we can also actually use this in

00:05:02,620 --> 00:05:06,550
production scenarios that not directly

00:05:04,419 --> 00:05:08,320
performance related such as if a program

00:05:06,550 --> 00:05:10,060
is stuck so if your application sort of

00:05:08,320 --> 00:05:12,250
stuck it's not doing anything it's not

00:05:10,060 --> 00:05:13,450
outputting any log we have no idea what

00:05:12,250 --> 00:05:15,430
it's doing but we can see it sitting

00:05:13,450 --> 00:05:18,010
there are they using all the CPU or

00:05:15,430 --> 00:05:19,479
maybe doing nothing at all we can use a

00:05:18,010 --> 00:05:21,039
profile to look at it and find what

00:05:19,479 --> 00:05:22,419
actually is it doing it's not outputting

00:05:21,039 --> 00:05:26,500
anything but it's probably executing

00:05:22,419 --> 00:05:28,600
something or conversely we might see

00:05:26,500 --> 00:05:29,950
this doing a bunch of stuff but we want

00:05:28,600 --> 00:05:32,260
to kind of understand maybe it's running

00:05:29,950 --> 00:05:33,610
super super slow so it's not a typical

00:05:32,260 --> 00:05:36,039
performance problem where we want to

00:05:33,610 --> 00:05:37,360
make it maybe 50% better but for some

00:05:36,039 --> 00:05:39,490
reason the whole thing's just running a

00:05:37,360 --> 00:05:41,169
thousand times slower than normal will

00:05:39,490 --> 00:05:42,850
by profiling it we might get a look at

00:05:41,169 --> 00:05:45,610
what kinds of requests are consuming

00:05:42,850 --> 00:05:46,930
resources you know proportionally which

00:05:45,610 --> 00:05:48,970
what are the cause of requests are

00:05:46,930 --> 00:05:50,830
taking too long we might find out

00:05:48,970 --> 00:05:52,979
exactly what is kind of getting us hung

00:05:50,830 --> 00:05:52,979
up

00:05:55,279 --> 00:06:00,569
and so really what we're working with in

00:05:57,870 --> 00:06:01,800
profilers are stacked races so what we

00:06:00,569 --> 00:06:04,379
want to know what functions are we

00:06:01,800 --> 00:06:05,999
executing in and ideally we'd like to

00:06:04,379 --> 00:06:07,650
have a hierarchy of the functions that

00:06:05,999 --> 00:06:09,059
caught in to that so the full call stack

00:06:07,650 --> 00:06:11,370
down into where they're currently

00:06:09,059 --> 00:06:13,499
executing and we see this all the time

00:06:11,370 --> 00:06:15,960
in arrow error reports so if you throw

00:06:13,499 --> 00:06:17,909
an exception or some other error in

00:06:15,960 --> 00:06:20,249
Python we'll get that nice stack trace

00:06:17,909 --> 00:06:21,599
printout but in profiling we want to

00:06:20,249 --> 00:06:22,919
really aggregate these from those

00:06:21,599 --> 00:06:24,509
hundreds of times a second into

00:06:22,919 --> 00:06:27,240
something that we can consume usefully

00:06:24,509 --> 00:06:30,889
rather than looking at a single a single

00:06:27,240 --> 00:06:33,360
output so how do we get a stack trace

00:06:30,889 --> 00:06:33,659
well if you're in Python that's super

00:06:33,360 --> 00:06:35,789
easy

00:06:33,659 --> 00:06:37,499
we can just call import the trace back

00:06:35,789 --> 00:06:39,389
module and called trace back print stack

00:06:37,499 --> 00:06:41,009
and there's a couple other modules like

00:06:39,389 --> 00:06:42,479
inspect where we can problematically

00:06:41,009 --> 00:06:46,229
kind of step through that instead of

00:06:42,479 --> 00:06:47,639
looking at a string and we have quite a

00:06:46,229 --> 00:06:49,439
few tools that can make use of this

00:06:47,639 --> 00:06:52,529
which I like to call an in process

00:06:49,439 --> 00:06:54,599
profiler - has one built in called C

00:06:52,529 --> 00:06:56,550
profile and there are a few others that

00:06:54,599 --> 00:07:00,719
kind of tweak this PI instrument at the

00:06:56,550 --> 00:07:02,580
improv but all of these profilers they

00:07:00,719 --> 00:07:04,050
run inside of our Python interpreter and

00:07:02,580 --> 00:07:06,270
they generally require some kind of

00:07:04,050 --> 00:07:08,550
setup in advance so you might need to

00:07:06,270 --> 00:07:11,189
edit the source import the profiling

00:07:08,550 --> 00:07:12,629
module maybe make some calls to say when

00:07:11,189 --> 00:07:16,229
to start profiling where to store the

00:07:12,629 --> 00:07:17,999
data or at the very least such as in the

00:07:16,229 --> 00:07:19,740
case of C profile we at least need to

00:07:17,999 --> 00:07:23,009
edit the launch parameters of Python so

00:07:19,740 --> 00:07:24,360
we need to pass - MC profile we need to

00:07:23,009 --> 00:07:25,740
edit that in whatever is launching our

00:07:24,360 --> 00:07:29,219
application then we have to restart the

00:07:25,740 --> 00:07:30,599
program and that requires pre-planning

00:07:29,219 --> 00:07:32,159
which is not so great in these

00:07:30,599 --> 00:07:33,719
production scenarios I'm talking about

00:07:32,159 --> 00:07:36,899
particularly if you haven't been able to

00:07:33,719 --> 00:07:39,479
set up in advance and then if your

00:07:36,899 --> 00:07:41,490
errors a little less common it might

00:07:39,479 --> 00:07:42,629
clear that error condition so after

00:07:41,490 --> 00:07:44,699
you've restarted the program you've

00:07:42,629 --> 00:07:47,779
started profiling but now the situation

00:07:44,699 --> 00:07:50,339
you were trying to profile has gone away

00:07:47,779 --> 00:07:51,839
Plus now our whole programs running but

00:07:50,339 --> 00:07:54,689
the profiler enabled which has some

00:07:51,839 --> 00:07:56,249
overhead and produces a lot of data and

00:07:54,689 --> 00:07:58,709
we might better add some kind of trigger

00:07:56,249 --> 00:08:01,080
mechanism to say turn it on and off but

00:07:58,709 --> 00:08:03,599
again that requires pre-planning and it

00:08:01,080 --> 00:08:05,039
also may not work depending on how your

00:08:03,599 --> 00:08:06,360
programs gotten stuck the actual

00:08:05,039 --> 00:08:06,750
mechanism you're using to turn the

00:08:06,360 --> 00:08:10,500
profile

00:08:06,750 --> 00:08:12,690
on and off may not actually function and

00:08:10,500 --> 00:08:14,520
again because this is an in process

00:08:12,690 --> 00:08:15,960
profiler it's competing with your

00:08:14,520 --> 00:08:18,150
program within the Python interpreter

00:08:15,960 --> 00:08:19,860
for runtime and so when it's doing the

00:08:18,150 --> 00:08:24,870
profiling work it's actually taking away

00:08:19,860 --> 00:08:26,070
time from your program itself so while

00:08:24,870 --> 00:08:27,600
there's a real great development in

00:08:26,070 --> 00:08:29,520
production this is kind of just like too

00:08:27,600 --> 00:08:30,870
hard people tend to shy away from it

00:08:29,520 --> 00:08:32,849
because it's just not friendly to new

00:08:30,870 --> 00:08:35,870
users it's not great in production

00:08:32,849 --> 00:08:37,950
scenarios you have to do all this setup

00:08:35,870 --> 00:08:39,539
so what other methods could be look at

00:08:37,950 --> 00:08:41,580
well what if there's a way we can

00:08:39,539 --> 00:08:43,680
profile externally and so instead of

00:08:41,580 --> 00:08:45,240
being inside a Python we could reach in

00:08:43,680 --> 00:08:49,320
from outside from another process and

00:08:45,240 --> 00:08:51,300
look at what part that is doing how

00:08:49,320 --> 00:08:52,530
might we do that well we know that

00:08:51,300 --> 00:08:54,510
pythons already stored all this

00:08:52,530 --> 00:08:55,920
information in memory somewhere it has

00:08:54,510 --> 00:08:58,170
to have it in order to give us that

00:08:55,920 --> 00:09:01,020
error error print anytime an exception

00:08:58,170 --> 00:09:04,290
happens which because pythor's dynamic

00:09:01,020 --> 00:09:05,700
could happen at any time so what if we

00:09:04,290 --> 00:09:06,870
could breach in from another process and

00:09:05,700 --> 00:09:09,930
read the memory of the Python

00:09:06,870 --> 00:09:12,510
interpreter well we can do this and in

00:09:09,930 --> 00:09:16,200
fact this is how most c and c++ or other

00:09:12,510 --> 00:09:18,839
machine language debuggers work so they

00:09:16,200 --> 00:09:20,700
use an API like p trace and the p trace

00:09:18,839 --> 00:09:22,530
api at least on linux allows you to

00:09:20,700 --> 00:09:24,480
control another process so you can

00:09:22,530 --> 00:09:26,640
attach that process you know the stop

00:09:24,480 --> 00:09:28,080
you can tell it to start you can read

00:09:26,640 --> 00:09:29,220
memory you can even write memory you can

00:09:28,080 --> 00:09:31,080
actually change data or change

00:09:29,220 --> 00:09:33,470
executable code within the process with

00:09:31,080 --> 00:09:33,470
that API

00:09:36,360 --> 00:09:41,279
and so we know as an external process we

00:09:39,779 --> 00:09:43,290
have this superpower we can reach in we

00:09:41,279 --> 00:09:46,579
can read the memory of Python we just

00:09:43,290 --> 00:09:46,579
have to figure out how to decode it

00:09:46,820 --> 00:09:50,040
unfortunately

00:09:48,120 --> 00:09:51,450
platon doesn't just store that back

00:09:50,040 --> 00:09:53,790
trace in the nice pretty string we used

00:09:51,450 --> 00:09:55,230
to sing summer in memory and even if it

00:09:53,790 --> 00:09:56,640
did store that string summer in memory

00:09:55,230 --> 00:09:58,440
we'd have to know where in memory to

00:09:56,640 --> 00:10:00,779
look because you have like a 64-bit

00:09:58,440 --> 00:10:02,279
address space and it could be anywhere

00:10:00,779 --> 00:10:07,380
and so you need to figure those things

00:10:02,279 --> 00:10:12,360
out so it turns out that's super easy

00:10:07,380 --> 00:10:14,820
just refer to this simple diagram this

00:10:12,360 --> 00:10:16,740
is the layout of the structures in the C

00:10:14,820 --> 00:10:18,360
Python interpreter and so the path

00:10:16,740 --> 00:10:22,529
interpreters a C program it has C

00:10:18,360 --> 00:10:24,570
structures and a C structure given the

00:10:22,529 --> 00:10:26,610
same structure the same text format of a

00:10:24,570 --> 00:10:28,680
structure in C the layout of that

00:10:26,610 --> 00:10:30,240
structure and you know what byte offset

00:10:28,680 --> 00:10:31,709
all of the things are stored is

00:10:30,240 --> 00:10:33,720
predictable it doesn't matter what

00:10:31,709 --> 00:10:36,120
compiler you use that's a standard and

00:10:33,720 --> 00:10:38,160
so if we know the source code that our

00:10:36,120 --> 00:10:39,510
version of Python is compiled with we we

00:10:38,160 --> 00:10:41,550
know it structures and we can figure out

00:10:39,510 --> 00:10:43,050
at what offsets we can find all this

00:10:41,550 --> 00:10:49,620
information about the Python interpreter

00:10:43,050 --> 00:10:50,880
state we also have a global variable my

00:10:49,620 --> 00:10:53,100
here we go

00:10:50,880 --> 00:10:56,250
called PI thread state current and this

00:10:53,100 --> 00:10:58,680
global variable points to the current

00:10:56,250 --> 00:11:00,149
Python thread so in Python we can have

00:10:58,680 --> 00:11:02,399
multiple threads we may only have one

00:11:00,149 --> 00:11:04,680
but this global variable points that

00:11:02,399 --> 00:11:06,390
current thread and then the thread keeps

00:11:04,680 --> 00:11:08,579
track of the top of its stack the

00:11:06,390 --> 00:11:10,050
current calling function and that

00:11:08,579 --> 00:11:14,310
calling function keeps track of the one

00:11:10,050 --> 00:11:15,720
that called it and so on and so forth so

00:11:14,310 --> 00:11:19,050
we just need to know how to find this PI

00:11:15,720 --> 00:11:21,510
thread state that might sound easy by

00:11:19,050 --> 00:11:25,110
name it's not quite so easy it's a

00:11:21,510 --> 00:11:26,640
little complicated first of all it's in

00:11:25,110 --> 00:11:29,130
a Python shared library which gets

00:11:26,640 --> 00:11:31,079
loaded summer in memory and because

00:11:29,130 --> 00:11:31,769
security is a thing we have this thing

00:11:31,079 --> 00:11:34,050
called address space layout

00:11:31,769 --> 00:11:35,940
randomization which means the library

00:11:34,050 --> 00:11:37,339
gets loaded summary memory at random and

00:11:35,940 --> 00:11:40,709
so we need to find out where that is

00:11:37,339 --> 00:11:41,970
because scanning 32 gig of ram or 48

00:11:40,709 --> 00:11:44,819
bits of address space would take quite a

00:11:41,970 --> 00:11:47,040
while so at least on Linux there's a

00:11:44,819 --> 00:11:48,930
file and slash called maps and because

00:11:47,040 --> 00:11:49,870
we're root and we need to be root to be

00:11:48,930 --> 00:11:53,200
able to do this kind

00:11:49,870 --> 00:11:55,050
access we can read that maps file to

00:11:53,200 --> 00:11:57,180
find out where the library's loaded and

00:11:55,050 --> 00:11:59,830
then if we look at the Python lock

00:11:57,180 --> 00:12:02,260
binaries itself we have debug symbols

00:11:59,830 --> 00:12:04,150
that will tell us now that we know where

00:12:02,260 --> 00:12:05,560
in memory Python is we now know the

00:12:04,150 --> 00:12:07,990
offset in term memory of that global

00:12:05,560 --> 00:12:09,400
variable exists that and then from there

00:12:07,990 --> 00:12:11,200
we can just follow all of these C

00:12:09,400 --> 00:12:13,540
structures and it's really just a bunch

00:12:11,200 --> 00:12:20,140
of pointers to each other to find the

00:12:13,540 --> 00:12:22,810
actual code that's running so the thread

00:12:20,140 --> 00:12:24,430
state has a pointer to the frame object

00:12:22,810 --> 00:12:28,200
which is the current executing function

00:12:24,430 --> 00:12:31,780
it's called a frame and the frame object

00:12:28,200 --> 00:12:34,240
then will link to all of the other frame

00:12:31,780 --> 00:12:36,130
objects like in the stack and then from

00:12:34,240 --> 00:12:39,700
that frame object it has a better link

00:12:36,130 --> 00:12:41,770
to a an object called the PI code object

00:12:39,700 --> 00:12:43,060
and that actually has the file name and

00:12:41,770 --> 00:12:45,040
the name and the line number they're

00:12:43,060 --> 00:12:47,080
executing at and so it's a hundred times

00:12:45,040 --> 00:12:49,360
a second we reach into Python we follow

00:12:47,080 --> 00:12:53,070
all these pointers we can get the file

00:12:49,360 --> 00:12:53,070
name the function we're executing it and

00:12:55,140 --> 00:12:58,990
so you don't really need to know about

00:12:57,520 --> 00:13:00,310
all these but I just kind of find these

00:12:58,990 --> 00:13:02,770
little details like kind of interesting

00:13:00,310 --> 00:13:03,820
sometimes how do these things work so

00:13:02,770 --> 00:13:05,230
the good news is if someone's done the

00:13:03,820 --> 00:13:06,940
work for us and you don't need to do

00:13:05,230 --> 00:13:11,470
this manually and it's a great tool

00:13:06,940 --> 00:13:13,990
called pi splay and so pi spy lets you

00:13:11,470 --> 00:13:15,670
just watch the process and you can like

00:13:13,990 --> 00:13:18,490
I did in this example which is straight

00:13:15,670 --> 00:13:20,470
from their github just tell it to launch

00:13:18,490 --> 00:13:22,360
a new Python process but much more

00:13:20,470 --> 00:13:24,670
powerfully we can just pass it a process

00:13:22,360 --> 00:13:26,050
ID of the existing Python process and we

00:13:24,670 --> 00:13:28,180
don't need to do any preset up on that

00:13:26,050 --> 00:13:30,160
Python process whatsoever out of the box

00:13:28,180 --> 00:13:31,900
on a bun to you didn't you start a

00:13:30,160 --> 00:13:34,330
Python before you have installed Pi spy

00:13:31,900 --> 00:13:35,920
you install PI spire you attach it and

00:13:34,330 --> 00:13:40,110
then now we get profiling information

00:13:35,920 --> 00:13:40,110
from that Python process just by magic

00:13:40,230 --> 00:13:46,720
what we see in this output is basically

00:13:43,150 --> 00:13:48,310
a live updating view this is like a

00:13:46,720 --> 00:13:50,020
really cool mode it's again it's just

00:13:48,310 --> 00:13:51,370
like top of which functions are

00:13:50,020 --> 00:13:53,670
executing and how much time they're

00:13:51,370 --> 00:13:53,670
using

00:13:55,460 --> 00:14:01,029
by default it samples a hundred times a

00:13:58,160 --> 00:14:03,500
second but you can configure that and

00:14:01,029 --> 00:14:04,880
already this is kind of like a good way

00:14:03,500 --> 00:14:06,830
to understand what's happening on its

00:14:04,880 --> 00:14:08,330
own so if your process is just a bit

00:14:06,830 --> 00:14:09,649
stuck it's not really doing something

00:14:08,330 --> 00:14:11,420
you want to know what it's doing you

00:14:09,649 --> 00:14:14,450
might be able to get that just from this

00:14:11,420 --> 00:14:16,220
output but it does have a couple of

00:14:14,450 --> 00:14:18,170
other interesting modes one of them is a

00:14:16,220 --> 00:14:19,580
simple dump mode so that's just a simple

00:14:18,170 --> 00:14:22,760
command line that prints out the

00:14:19,580 --> 00:14:24,050
currently executing call stack that

00:14:22,760 --> 00:14:26,450
might be interesting in some limited

00:14:24,050 --> 00:14:28,040
cases but what's even more powerful is

00:14:26,450 --> 00:14:31,279
we could visualize that data with flame

00:14:28,040 --> 00:14:32,350
graphs and so flame graphs if you've

00:14:31,279 --> 00:14:35,209
never seen one before

00:14:32,350 --> 00:14:37,910
represent the data in your program along

00:14:35,209 --> 00:14:39,589
the x-axis is how much time so we

00:14:37,910 --> 00:14:42,170
sampled the process a hundred times in

00:14:39,589 --> 00:14:43,820
one second and for each of those we now

00:14:42,170 --> 00:14:45,560
divide the whole width of the page up

00:14:43,820 --> 00:14:46,850
into 100 and for each time we saw a

00:14:45,560 --> 00:14:49,010
process we'd give it a little bit of

00:14:46,850 --> 00:14:56,990
that space and so in this particular

00:14:49,010 --> 00:14:58,430
example my little heart this particular

00:14:56,990 --> 00:14:59,570
example we can see along the top I've

00:14:58,430 --> 00:15:01,310
got a big red bar that's the whole

00:14:59,570 --> 00:15:03,830
program obviously the whole program was

00:15:01,310 --> 00:15:05,630
executing 100% of the time under that

00:15:03,830 --> 00:15:06,950
we've got an orange bar that was taking

00:15:05,630 --> 00:15:08,630
up a lot of the time but under that you

00:15:06,950 --> 00:15:11,180
can see a yellow bar in an orange bar a

00:15:08,630 --> 00:15:12,620
kind of split maybe 30 60 and so we know

00:15:11,180 --> 00:15:14,900
that that particular function or in fact

00:15:12,620 --> 00:15:17,870
those two lines we're using about 30%

00:15:14,900 --> 00:15:19,670
60% of the time and then within each of

00:15:17,870 --> 00:15:21,050
those functions further down as we go

00:15:19,670 --> 00:15:22,550
down we see what percentage of that

00:15:21,050 --> 00:15:24,470
functions time was in another function

00:15:22,550 --> 00:15:27,740
and so on and so on down the course

00:15:24,470 --> 00:15:29,510
tackle and what's even cooler about this

00:15:27,740 --> 00:15:32,209
is this is not just a static image this

00:15:29,510 --> 00:15:33,890
is actually interactive SVG and we can

00:15:32,209 --> 00:15:36,410
actually quick and zoom in on various

00:15:33,890 --> 00:15:39,589
parts of that image to find the bit that

00:15:36,410 --> 00:15:40,930
we're interested in so if I just bring

00:15:39,589 --> 00:15:43,339
it up in a web browser here

00:15:40,930 --> 00:15:45,079
if for example I wanted to look at this

00:15:43,339 --> 00:15:47,209
particular function I can click on a

00:15:45,079 --> 00:15:49,220
zoom right in and so now those spits are

00:15:47,209 --> 00:15:50,450
a bit smaller I couldn't see before we

00:15:49,220 --> 00:15:51,649
can see a little better and if I wanted

00:15:50,450 --> 00:15:53,870
to see what's happening here I can click

00:15:51,649 --> 00:15:55,370
that and it expands it out and so this

00:15:53,870 --> 00:15:57,230
is a really powerful tool to actually

00:15:55,370 --> 00:15:59,510
find out what my program is doing in

00:15:57,230 --> 00:16:03,399
much more detail and and quite a

00:15:59,510 --> 00:16:03,399
consumable way

00:16:06,089 --> 00:16:10,930
and the other nice thing about pie Spy

00:16:08,440 --> 00:16:13,350
is this flame graph is anyone here use

00:16:10,930 --> 00:16:15,399
flame graphs before in any language and

00:16:13,350 --> 00:16:16,540
you know that when you used them it's

00:16:15,399 --> 00:16:18,640
like a multi like four step process

00:16:16,540 --> 00:16:20,620
right you like run one command you like

00:16:18,640 --> 00:16:22,750
process it with another command and then

00:16:20,620 --> 00:16:25,720
then you finally get your SVG output

00:16:22,750 --> 00:16:27,160
well impress by it just directly outputs

00:16:25,720 --> 00:16:29,170
the SVG file for you so there's no

00:16:27,160 --> 00:16:37,180
intermediate data sort of wrangling that

00:16:29,170 --> 00:16:38,230
you need to do and so possibly a chance

00:16:37,180 --> 00:16:41,529
a little bit of history it's not the

00:16:38,230 --> 00:16:42,910
first tool like this and at least one of

00:16:41,529 --> 00:16:45,160
the more modern versions of this that

00:16:42,910 --> 00:16:47,080
first came out was called pi flame it

00:16:45,160 --> 00:16:49,810
was written by Evan at uber released in

00:16:47,080 --> 00:16:51,730
about September 2016 and it did the same

00:16:49,810 --> 00:16:54,070
sort of thing it reached into the

00:16:51,730 --> 00:16:56,200
process and read its memory but it

00:16:54,070 --> 00:16:58,750
wasn't quite as nice so it used P trace

00:16:56,200 --> 00:17:00,760
it was written in C++ didn't have that

00:16:58,750 --> 00:17:02,620
interactive view it just outputted a

00:17:00,760 --> 00:17:04,120
flame graph but it didn't give you the

00:17:02,620 --> 00:17:05,500
graph straight away it gave me just like

00:17:04,120 --> 00:17:08,350
text file which we didn't had to process

00:17:05,500 --> 00:17:10,120
extra and so if you integrated this into

00:17:08,350 --> 00:17:11,860
your own automation it was great but it

00:17:10,120 --> 00:17:13,720
wasn't super user-friendly you couldn't

00:17:11,860 --> 00:17:15,790
just sit down with no prior experiences

00:17:13,720 --> 00:17:20,040
run it and then bang immediately get

00:17:15,790 --> 00:17:22,209
good output and so in April 20 18 joules

00:17:20,040 --> 00:17:23,770
she was so passionate about this project

00:17:22,209 --> 00:17:25,959
she wants to do something similar for

00:17:23,770 --> 00:17:28,830
Ruby called RB spy she took a three

00:17:25,959 --> 00:17:31,150
month sabbatical to write this tool and

00:17:28,830 --> 00:17:32,500
she did a few interesting things that

00:17:31,150 --> 00:17:35,679
were different to how high flame worked

00:17:32,500 --> 00:17:37,240
even though it was for Ruby while it was

00:17:35,679 --> 00:17:39,070
written in rust which is kind of

00:17:37,240 --> 00:17:41,020
hilarious because we're using rust which

00:17:39,070 --> 00:17:43,210
is a very memory safe language it goes

00:17:41,020 --> 00:17:45,250
to a lot of effort to make sure we're

00:17:43,210 --> 00:17:47,410
not reading memory you know out of water

00:17:45,250 --> 00:17:48,670
without locks we basically throw all

00:17:47,410 --> 00:17:50,559
that away and we just really memory

00:17:48,670 --> 00:17:52,620
straight out of another process while it

00:17:50,559 --> 00:17:55,059
might be changing it great use of rust

00:17:52,620 --> 00:17:56,710
but a great feature of rust that works

00:17:55,059 --> 00:17:58,780
really well for this is the talk of

00:17:56,710 --> 00:18:01,059
bondage in and so remember all those see

00:17:58,780 --> 00:18:02,470
structures I showed you imply flame

00:18:01,059 --> 00:18:04,150
they're done a bit of work to kind of

00:18:02,470 --> 00:18:06,820
you know for this version of Python this

00:18:04,150 --> 00:18:09,340
is the structure and the offset in in

00:18:06,820 --> 00:18:11,230
our beast buyer we used rust bond gentle

00:18:09,340 --> 00:18:13,750
in matically process the SI headers for

00:18:11,230 --> 00:18:15,640
every different version of Ruby and just

00:18:13,750 --> 00:18:17,500
wrote those all into the binary and so

00:18:15,640 --> 00:18:19,210
at runtime it figured out what version

00:18:17,500 --> 00:18:19,600
of Ruby was running and then use the

00:18:19,210 --> 00:18:21,179
appropriate

00:18:19,600 --> 00:18:23,320
versions the headers without any manual

00:18:21,179 --> 00:18:26,230
sort of data wrangling so that was all

00:18:23,320 --> 00:18:27,910
automatic and then she added that

00:18:26,230 --> 00:18:29,350
interactive top like for you which is

00:18:27,910 --> 00:18:30,940
really great because you can just jump

00:18:29,350 --> 00:18:33,700
right in and get some data straight away

00:18:30,940 --> 00:18:35,230
without any kind of wrangling or even

00:18:33,700 --> 00:18:38,890
have to you know copy the file to a

00:18:35,230 --> 00:18:40,750
bread browser and she embedded that

00:18:38,890 --> 00:18:42,130
automatic flame graph generation so

00:18:40,750 --> 00:18:42,760
instead of this multi-step manual

00:18:42,130 --> 00:18:44,679
process

00:18:42,760 --> 00:18:46,900
she literally copied and pasted the pill

00:18:44,679 --> 00:18:48,400
program into the rust binary and it like

00:18:46,900 --> 00:18:48,960
outputs and runs it automatically for

00:18:48,400 --> 00:18:51,010
you

00:18:48,960 --> 00:18:53,740
sounds a little cloudy but it works well

00:18:51,010 --> 00:18:54,960
and makes it super easy to use and she's

00:18:53,740 --> 00:18:57,190
actually got a really good presentation

00:18:54,960 --> 00:18:58,990
about her process of developing this

00:18:57,190 --> 00:19:01,150
tool it's a great presentation I'd

00:18:58,990 --> 00:19:03,250
highly recommend you search for it and

00:19:01,150 --> 00:19:05,740
watch it if you kind of interested in

00:19:03,250 --> 00:19:08,039
the nitty-gritty a longer version of

00:19:05,740 --> 00:19:11,260
that no see structures I was showing you

00:19:08,039 --> 00:19:13,539
and so they're from a beast by that was

00:19:11,260 --> 00:19:16,240
April 2018 so on only a year ago this is

00:19:13,539 --> 00:19:18,190
relatively new tooling been Frederickson

00:19:16,240 --> 00:19:20,320
wrote PI spire which is heavily derived

00:19:18,190 --> 00:19:22,000
from our beast by it's also rust it

00:19:20,320 --> 00:19:24,370
shares a bunch of Cobras I'd be a spire

00:19:22,000 --> 00:19:26,260
but just implemented all the special

00:19:24,370 --> 00:19:27,880
bits for Python so it knows specifically

00:19:26,260 --> 00:19:30,419
for Python how to understand all that

00:19:27,880 --> 00:19:30,419
information

00:19:40,330 --> 00:19:45,200
so so far I've talked about profiling

00:19:42,820 --> 00:19:46,460
those are my favorite tools some of the

00:19:45,200 --> 00:19:49,310
things I'm most excited about at the

00:19:46,460 --> 00:19:50,750
moment because for me I can apply such a

00:19:49,310 --> 00:19:53,450
tool to a customer's Production system

00:19:50,750 --> 00:19:56,420
with no preset up that's really powerful

00:19:53,450 --> 00:19:58,430
for me particularly cuz a lot of the

00:19:56,420 --> 00:20:00,770
problems I work on in OpenStack which is

00:19:58,430 --> 00:20:02,960
a very large piping codebase it's it was

00:20:00,770 --> 00:20:06,290
a million lines in 2017 who knows what

00:20:02,960 --> 00:20:10,000
it is now it always likes to get itself

00:20:06,290 --> 00:20:10,000
hung up and so it helps me quite a bit

00:20:10,600 --> 00:20:14,480
but really what I'm talking about here

00:20:12,680 --> 00:20:15,920
is debugging production problems and so

00:20:14,480 --> 00:20:20,090
sometimes we can pre prepare for those

00:20:15,920 --> 00:20:21,620
production problems and as I said using

00:20:20,090 --> 00:20:23,570
an interactive debugger it's not super

00:20:21,620 --> 00:20:25,310
helpful and everyone's guilty of print

00:20:23,570 --> 00:20:26,570
debugging I'm sure we've all written a

00:20:25,310 --> 00:20:29,030
million print statements into our

00:20:26,570 --> 00:20:30,230
production programs a debug there are a

00:20:29,030 --> 00:20:32,180
couple tools they can make them a little

00:20:30,230 --> 00:20:35,600
bit nicer and so one of those is PI

00:20:32,180 --> 00:20:37,580
Snooper and what PI Snooper does is it

00:20:35,600 --> 00:20:40,040
allows us to trace a functions execution

00:20:37,580 --> 00:20:41,330
not including just the function name but

00:20:40,040 --> 00:20:42,800
actually also the values of all the

00:20:41,330 --> 00:20:46,070
variables in the exact code part that

00:20:42,800 --> 00:20:48,320
was followed and lets us do that just by

00:20:46,070 --> 00:20:52,190
using a function decorator or using a

00:20:48,320 --> 00:20:53,810
with block and so instead of having to

00:20:52,190 --> 00:20:55,460
sort of guess where to put my print

00:20:53,810 --> 00:20:57,110
statements and all that sort of thing we

00:20:55,460 --> 00:20:59,290
can just decorate a function like this

00:20:57,110 --> 00:21:01,700
we've passed a new product snoop and

00:20:59,290 --> 00:21:03,530
then we get an output that looks a bit

00:21:01,700 --> 00:21:05,510
like this and so what we can see on this

00:21:03,530 --> 00:21:08,060
is the exact code that's executing gets

00:21:05,510 --> 00:21:09,800
printed as it executes and so if a loop

00:21:08,060 --> 00:21:12,260
runs more than once it outputs the whole

00:21:09,800 --> 00:21:14,330
loop each time it runs and each time a

00:21:12,260 --> 00:21:16,970
variable is set or the value changes it

00:21:14,330 --> 00:21:20,600
also outputs that and so now by using

00:21:16,970 --> 00:21:22,460
this instead of manually putting prints

00:21:20,600 --> 00:21:24,410
in and then realizing you miss something

00:21:22,460 --> 00:21:26,360
and iterative ly doing that process I

00:21:24,410 --> 00:21:29,300
can just go well I know my problems in

00:21:26,360 --> 00:21:30,890
this process I can just call the snoop

00:21:29,300 --> 00:21:32,350
I'll get a full trace out put and I can

00:21:30,890 --> 00:21:34,610
analyze that later it's kind of like a

00:21:32,350 --> 00:21:37,640
past tense debugging instead of an

00:21:34,610 --> 00:21:39,140
interactive debugging and so this is

00:21:37,640 --> 00:21:41,510
really good again for production where I

00:21:39,140 --> 00:21:44,630
can do some pre setup but there's low

00:21:41,510 --> 00:21:46,610
overhead I don't have to trace the whole

00:21:44,630 --> 00:21:48,500
process I can trace sub bits and you can

00:21:46,610 --> 00:21:50,060
even put conditionals on that so if you

00:21:48,500 --> 00:21:52,370
know it was a certain user or something

00:21:50,060 --> 00:21:54,680
you could you could add a conditional to

00:21:52,370 --> 00:21:58,130
the start PI Snooper if you know a

00:21:54,680 --> 00:21:59,809
certain condition was met and so this I

00:21:58,130 --> 00:22:02,660
would suggest is a much nicer way of

00:21:59,809 --> 00:22:12,320
doing print style debugging particularly

00:22:02,660 --> 00:22:13,700
for a production scenario so the last

00:22:12,320 --> 00:22:14,840
kind of tool I want to talk about now

00:22:13,700 --> 00:22:16,040
this is probably something you're not

00:22:14,840 --> 00:22:17,480
going to use straight away it's a bit of

00:22:16,040 --> 00:22:19,460
an emerging technology but I wanted to

00:22:17,480 --> 00:22:22,880
talk a bit about it because I have some

00:22:19,460 --> 00:22:24,500
interesting properties and with Python

00:22:22,880 --> 00:22:25,400
right now this only works at least on a

00:22:24,500 --> 00:22:26,809
bloom to in the latest development

00:22:25,400 --> 00:22:28,730
release it's not in the production

00:22:26,809 --> 00:22:32,420
versions so again it's a little bit of a

00:22:28,730 --> 00:22:35,480
preview but we have this concept of

00:22:32,420 --> 00:22:37,040
tracing in the kernel code EBP F which

00:22:35,480 --> 00:22:38,870
lets us do a bunch of tracing work

00:22:37,040 --> 00:22:40,460
inside of the kernel and so instead of

00:22:38,870 --> 00:22:43,070
needing to actually process stuff in

00:22:40,460 --> 00:22:44,630
user space we can have the kernel sort

00:22:43,070 --> 00:22:46,309
of trap the user program and then do a

00:22:44,630 --> 00:22:48,470
bunch of work inside the kernel in

00:22:46,309 --> 00:22:49,820
kernel space a lot faster than we might

00:22:48,470 --> 00:22:52,820
if we had to get all that information

00:22:49,820 --> 00:22:55,880
back out to whatever external process

00:22:52,820 --> 00:22:58,520
was trying to do that tracing and so we

00:22:55,880 --> 00:23:00,710
have a concept called us DT or user

00:22:58,520 --> 00:23:02,090
space to find trace probes and Python

00:23:00,710 --> 00:23:04,580
defines a few of these probes and

00:23:02,090 --> 00:23:06,980
they're on screen now so we can actually

00:23:04,580 --> 00:23:09,559
attach to when Python starts and stops

00:23:06,980 --> 00:23:12,440
garbage collection we can attach to when

00:23:09,559 --> 00:23:14,090
it tries to load a library and we can

00:23:12,440 --> 00:23:15,320
attach to when it calls functions that's

00:23:14,090 --> 00:23:18,050
what the function entry and function

00:23:15,320 --> 00:23:19,429
return trace points are then we can also

00:23:18,050 --> 00:23:25,340
attach to every single time it executes

00:23:19,429 --> 00:23:26,630
a line and so the ones that might be

00:23:25,340 --> 00:23:29,230
easily here a function entry and

00:23:26,630 --> 00:23:31,790
function returns so with this user space

00:23:29,230 --> 00:23:36,290
trazan technology i can trace every time

00:23:31,790 --> 00:23:38,300
the Python functions executed the nifty

00:23:36,290 --> 00:23:40,520
way apart the way user space probes work

00:23:38,300 --> 00:23:43,429
it's kind of crazy but at compile time

00:23:40,520 --> 00:23:45,500
Python inserts or the compiler inserts a

00:23:43,429 --> 00:23:47,990
single knop instruction don't execute

00:23:45,500 --> 00:23:50,410
anything instruction on x86 and it's

00:23:47,990 --> 00:23:53,600
just there to kind of free up some space

00:23:50,410 --> 00:23:55,220
at runtime if I ask the kernel to trace

00:23:53,600 --> 00:23:57,530
this Python process it will actually go

00:23:55,220 --> 00:23:59,600
in and replace in memory that that null

00:23:57,530 --> 00:24:01,880
instruction with an interrupt to trap

00:23:59,600 --> 00:24:03,200
into the kernel and so when we're not

00:24:01,880 --> 00:24:04,460
tracing the process the code does

00:24:03,200 --> 00:24:05,700
absolutely nothing and it has no

00:24:04,460 --> 00:24:07,350
performance impact

00:24:05,700 --> 00:24:09,210
and only when we want to trace it

00:24:07,350 --> 00:24:14,909
doesn't actually start trapping into the

00:24:09,210 --> 00:24:16,950
kernel the overhead is so low that you

00:24:14,909 --> 00:24:18,750
may not realize that purana all of your

00:24:16,950 --> 00:24:20,669
linux systems have these trace points on

00:24:18,750 --> 00:24:22,320
every time see does a memory allocation

00:24:20,669 --> 00:24:25,200
every time the kernel function is

00:24:22,320 --> 00:24:27,929
executed and you can even probe every

00:24:25,200 --> 00:24:30,029
time any function in any program is

00:24:27,929 --> 00:24:31,169
executed using the same technology so

00:24:30,029 --> 00:24:31,919
there's trace points are already

00:24:31,169 --> 00:24:33,389
everywhere

00:24:31,919 --> 00:24:35,340
that's how we can know that when they're

00:24:33,389 --> 00:24:39,330
not activated the overhead is basically

00:24:35,340 --> 00:24:41,279
zero and so I can write a little script

00:24:39,330 --> 00:24:43,710
like this and this is a tool called BPF

00:24:41,279 --> 00:24:48,840
trace this is quite new but I can tell

00:24:43,710 --> 00:24:51,779
it that I want to attach to a USD ta

00:24:48,840 --> 00:24:54,149
user space to find trace point for

00:24:51,779 --> 00:24:56,639
Python 3.8 and I want to trace every

00:24:54,149 --> 00:24:58,139
time of functions executed and then I'm

00:24:56,639 --> 00:25:00,419
just doing a simple printf in this case

00:24:58,139 --> 00:25:03,750
is a very simple example to print the

00:25:00,419 --> 00:25:05,490
process ID and the the function the line

00:25:03,750 --> 00:25:08,100
number and the file name and so you can

00:25:05,490 --> 00:25:11,159
see the output of an example here so I

00:25:08,100 --> 00:25:13,110
ran this probe and I didn't I didn't

00:25:11,159 --> 00:25:14,340
have to modify Python except it needs to

00:25:13,110 --> 00:25:16,559
be this new enough version with this

00:25:14,340 --> 00:25:18,720
function enabled so as again right now

00:25:16,559 --> 00:25:20,730
that's only in a bun to 19:10

00:25:18,720 --> 00:25:25,019
development but then I get this trace

00:25:20,730 --> 00:25:26,490
output now in this simple example we're

00:25:25,019 --> 00:25:28,710
just outputting text that is still a

00:25:26,490 --> 00:25:31,440
little heavyweight but the power of EBP

00:25:28,710 --> 00:25:33,630
F and what EBP F itself is is it's a an

00:25:31,440 --> 00:25:35,340
interpreter inside the kernel and so we

00:25:33,630 --> 00:25:38,100
can program the curl every time this

00:25:35,340 --> 00:25:41,190
trace point is hit to actually do

00:25:38,100 --> 00:25:43,639
something for us so this simple example

00:25:41,190 --> 00:25:46,260
here what we do is we tell the colonel

00:25:43,639 --> 00:25:47,820
trace function entry but I only want to

00:25:46,260 --> 00:25:49,559
trace the function if it's called test1

00:25:47,820 --> 00:25:51,960
so it's not called test one it won't

00:25:49,559 --> 00:25:55,230
output anything so you can see in this

00:25:51,960 --> 00:25:56,519
output we only have the five code of

00:25:55,230 --> 00:25:58,320
func one that's a typo but nevertheless

00:25:56,519 --> 00:26:00,720
they're the only ones that output no

00:25:58,320 --> 00:26:02,039
other function output and if you're more

00:26:00,720 --> 00:26:03,899
powerfully I can get the curl to do a

00:26:02,039 --> 00:26:06,570
bunch of statistics for me so in this

00:26:03,899 --> 00:26:08,429
example instead of printing what I do is

00:26:06,570 --> 00:26:11,340
I ask the kernel to count how many times

00:26:08,429 --> 00:26:13,169
each function is executed and so in

00:26:11,340 --> 00:26:15,029
kernel in very fast machine code it

00:26:13,169 --> 00:26:16,860
keeps track of each function how many

00:26:15,029 --> 00:26:18,179
times executed and it never actually

00:26:16,860 --> 00:26:19,560
sends that information back to use the

00:26:18,179 --> 00:26:21,390
space until

00:26:19,560 --> 00:26:22,830
like maybe every 30 seconds or something

00:26:21,390 --> 00:26:26,340
like that but so it's a very low

00:26:22,830 --> 00:26:28,890
overhead and this is such an example of

00:26:26,340 --> 00:26:31,740
a tracing profiler so unlike our PI

00:26:28,890 --> 00:26:33,300
spire example this trace point will

00:26:31,740 --> 00:26:35,700
catch every single invocation of that

00:26:33,300 --> 00:26:36,990
function if you're maybe trying to look

00:26:35,700 --> 00:26:43,110
at something that's happening very

00:26:36,990 --> 00:26:44,520
infrequently and again we can do it with

00:26:43,110 --> 00:26:46,710
no preparation so we didn't need to pre

00:26:44,520 --> 00:26:48,000
prepare our system other than to maybe

00:26:46,710 --> 00:26:51,600
wait a couple years until it's on all

00:26:48,000 --> 00:26:53,760
our production machines and this final

00:26:51,600 --> 00:26:55,320
example it's a different tool the BPF

00:26:53,760 --> 00:26:57,330
trace tool I just showed has its own

00:26:55,320 --> 00:26:59,280
little language which means you can

00:26:57,330 --> 00:27:00,720
write little scripts on the fly but

00:26:59,280 --> 00:27:03,210
there are also some pre-written tools

00:27:00,720 --> 00:27:05,010
that use the same technology but in a

00:27:03,210 --> 00:27:06,510
different way but this particular tool

00:27:05,010 --> 00:27:08,070
is a bit nicer and so when it shows the

00:27:06,510 --> 00:27:10,200
call stack and actually indents it so it

00:27:08,070 --> 00:27:11,840
shows you you know this functions

00:27:10,200 --> 00:27:15,630
running under that function and so on

00:27:11,840 --> 00:27:19,170
this isn't a set of tools called BPF CC

00:27:15,630 --> 00:27:24,000
tools that's the name there this slides

00:27:19,170 --> 00:27:25,470
will be online so you can check them and

00:27:24,000 --> 00:27:26,730
so that's basically it the BPF so it's

00:27:25,470 --> 00:27:28,190
just a quick little preview you're

00:27:26,730 --> 00:27:32,580
probably not going to use that one yet

00:27:28,190 --> 00:27:33,810
but as a bit of revision profilers

00:27:32,580 --> 00:27:36,210
really can help with what is my program

00:27:33,810 --> 00:27:37,950
doing now and we now we know about pi

00:27:36,210 --> 00:27:41,160
spire which we can use to do that with

00:27:37,950 --> 00:27:42,450
with no pre setup and no overhead we

00:27:41,160 --> 00:27:44,130
also learnt a little bit about PI

00:27:42,450 --> 00:27:45,930
Snooper which allows us to annotate

00:27:44,130 --> 00:27:47,580
specific functions exactly what it's

00:27:45,930 --> 00:27:50,040
doing exactly what codes been running

00:27:47,580 --> 00:27:51,600
and what the values are of of the

00:27:50,040 --> 00:27:53,310
variables which is something we don't

00:27:51,600 --> 00:27:54,420
see in most of the profiling tools they

00:27:53,310 --> 00:27:56,280
don't show us the values or the

00:27:54,420 --> 00:28:00,000
arguments to functions we can do that

00:27:56,280 --> 00:28:01,740
with tracing and this BPF and BPF trace

00:28:00,000 --> 00:28:03,660
is a really cool upcoming technology

00:28:01,740 --> 00:28:06,120
that applies not just to python but to

00:28:03,660 --> 00:28:06,990
actually all languages perl PHP node C

00:28:06,120 --> 00:28:09,330
C++

00:28:06,990 --> 00:28:10,380
they've all got similar hooks in fact

00:28:09,330 --> 00:28:14,280
most of them have better hooks than

00:28:10,380 --> 00:28:16,020
Python at the moment sadly that will let

00:28:14,280 --> 00:28:17,370
us do very fast analysis in kernel the

00:28:16,020 --> 00:28:18,990
kernel can do the statistics and

00:28:17,370 --> 00:28:22,540
Counting and histograms and all sorts of

00:28:18,990 --> 00:28:24,190
cool stuff so

00:28:22,540 --> 00:28:26,830
have a quick look at our poll result

00:28:24,190 --> 00:28:35,260
because I'm just curious what people

00:28:26,830 --> 00:28:40,420
have said about what sort of tools you

00:28:35,260 --> 00:28:45,580
might use in production problems 22

00:28:40,420 --> 00:28:47,730
votes so that's kind of interesting even

00:28:45,580 --> 00:28:49,930
even even now the 22 votes not bad

00:28:47,730 --> 00:28:51,250
everyone's guilty of print debugging if

00:28:49,930 --> 00:28:53,200
it was multiple choice I'm sure that

00:28:51,250 --> 00:28:54,580
would be at the end but I mean pretty

00:28:53,200 --> 00:28:57,040
buckin is actually really powerful and

00:28:54,580 --> 00:28:58,270
helpful but in production where it's

00:28:57,040 --> 00:28:59,920
maybe harder to change the codes

00:28:58,270 --> 00:29:02,860
sometimes that Tracy method it might

00:28:59,920 --> 00:29:03,880
just save us some time I'm also curious

00:29:02,860 --> 00:29:05,920
that no one's really using this

00:29:03,880 --> 00:29:07,480
commercial and maybe I made this clear

00:29:05,920 --> 00:29:09,640
but there are a bunch of like commercial

00:29:07,480 --> 00:29:11,950
products like honeycomb and others that

00:29:09,640 --> 00:29:13,750
kind of dynamically like instrument in

00:29:11,950 --> 00:29:15,490
profile and clock that or not dashboard

00:29:13,750 --> 00:29:19,690
for them as you know here using any

00:29:15,490 --> 00:29:22,810
tools like that wow that's fascinating I

00:29:19,690 --> 00:29:27,010
kind of expect those maybe more used so

00:29:22,810 --> 00:29:28,630
real interesting well I hope everyone

00:29:27,010 --> 00:29:30,790
learn something and I think we have a

00:29:28,630 --> 00:29:37,780
couple minutes for any questions if

00:29:30,790 --> 00:29:39,730
anyone has any well maybe one minute one

00:29:37,780 --> 00:29:48,160
minute or one question and we have time

00:29:39,730 --> 00:29:50,410
for one quick question regarding ebps

00:29:48,160 --> 00:29:53,520
specifically how complicated is it to

00:29:50,410 --> 00:29:57,400
write something complex for example

00:29:53,520 --> 00:30:01,180
getting a back trace of an async program

00:29:57,400 --> 00:30:02,800
I actually haven't looked into a sink so

00:30:01,180 --> 00:30:04,920
the question basically about getting

00:30:02,800 --> 00:30:08,590
back trace to make complex like a sink

00:30:04,920 --> 00:30:12,100
there are some gotchas with the G event

00:30:08,590 --> 00:30:13,480
style stuff some profiles work with it

00:30:12,100 --> 00:30:15,340
some don't most the ones that talk to

00:30:13,480 --> 00:30:17,440
that do but I've not looked specifically

00:30:15,340 --> 00:30:18,490
at async so I would have to have a look

00:30:17,440 --> 00:30:19,690
at that and maybe you can let you know

00:30:18,490 --> 00:30:20,830
if you want to like Twitter me or

00:30:19,690 --> 00:30:22,360
something I could try and look at that

00:30:20,830 --> 00:30:25,360
and see whether that works in any of

00:30:22,360 --> 00:30:26,560
those technologies all right well thank

00:30:25,360 --> 00:30:27,910
you all for coming I appreciate it I

00:30:26,560 --> 00:30:30,690
hope you found something interesting and

00:30:27,910 --> 00:30:33,250
the slides will be online on my website

00:30:30,690 --> 00:30:36,420
or on my Twitter in case you want to

00:30:33,250 --> 00:30:36,420

YouTube URL: https://www.youtube.com/watch?v=jXzEzmz-oag


