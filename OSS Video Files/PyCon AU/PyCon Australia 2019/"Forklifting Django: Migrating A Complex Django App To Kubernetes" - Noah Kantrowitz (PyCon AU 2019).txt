Title: "Forklifting Django: Migrating A Complex Django App To Kubernetes" - Noah Kantrowitz (PyCon AU 2019)
Publication date: 2019-08-02
Playlist: PyCon Australia 2019
Description: 
	Noah Kantrowitz

Everyone is talking about Kubernetes, but migrating existing applications is often easier said than done. This talk will cover the tale of migrating our main Django application to Kubernetes, and all the problems and solutions we ran into along the way.

https://2019.pycon-au.org/talks/forklifting-django-migrating-a-complex-django-app-to-kubernetes

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Fri Aug  2 11:50:00 2019 at C3.3
Captions: 
	00:00:00,030 --> 00:00:04,740
so everybody is talking about kubernetes

00:00:02,520 --> 00:00:05,370
but migrating to it is often easier said

00:00:04,740 --> 00:00:08,099
than done

00:00:05,370 --> 00:00:09,719
no encounter wits runs an infrastructure

00:00:08,099 --> 00:00:11,969
team a tried cell and he's going to show

00:00:09,719 --> 00:00:13,380
us what they tried what they learned and

00:00:11,969 --> 00:00:21,900
how to do it please make Noah feel

00:00:13,380 --> 00:00:24,090
welcome awesome thanks so much Lee like

00:00:21,900 --> 00:00:25,590
I said no carrots that's me and all of

00:00:24,090 --> 00:00:27,289
the places you can't actually see it it

00:00:25,590 --> 00:00:29,250
is in the bottom of the slides but well

00:00:27,289 --> 00:00:32,790
we're gonna start up called right cell

00:00:29,250 --> 00:00:35,190
over in America we do car stuff but not

00:00:32,790 --> 00:00:37,200
really relevant to this talk deploying

00:00:35,190 --> 00:00:38,520
web applications is really hard there's

00:00:37,200 --> 00:00:39,660
a lot of individual bits they have to

00:00:38,520 --> 00:00:41,430
happen in the right order

00:00:39,660 --> 00:00:42,570
over and over has to be fast has to be

00:00:41,430 --> 00:00:45,510
easy to use and has to be error-free

00:00:42,570 --> 00:00:46,950
every single time in the beginning back

00:00:45,510 --> 00:00:48,120
at the dawn of operations we all

00:00:46,950 --> 00:00:49,500
basically just did this by hand maybe

00:00:48,120 --> 00:00:51,930
some people had some bash scripts maybe

00:00:49,500 --> 00:00:54,149
some are sync if you were real fancy but

00:00:51,930 --> 00:00:56,640
it was extremely slow is very tedious

00:00:54,149 --> 00:00:59,129
there were lots of errors and it took

00:00:56,640 --> 00:01:00,870
you a lot of human hours so in time we

00:00:59,129 --> 00:01:02,730
had new tools things like fabric chef

00:01:00,870 --> 00:01:04,229
and ansible those helped automate all of

00:01:02,730 --> 00:01:05,250
our processes and then we got new things

00:01:04,229 --> 00:01:07,200
like docker and the rest of the

00:01:05,250 --> 00:01:09,689
container ecosystem that helped us move

00:01:07,200 --> 00:01:11,670
stuff from deploy time to build time and

00:01:09,689 --> 00:01:13,320
generally improve our repeatability but

00:01:11,670 --> 00:01:15,479
with all these new tools came a lot of

00:01:13,320 --> 00:01:16,680
new challenges so let's set the stage

00:01:15,479 --> 00:01:17,610
for the application we're gonna be

00:01:16,680 --> 00:01:18,840
talking about here this is not a

00:01:17,610 --> 00:01:20,729
hypothetical this is the actual

00:01:18,840 --> 00:01:23,250
application that my team works on at my

00:01:20,729 --> 00:01:24,960
company it is a fairly standard django

00:01:23,250 --> 00:01:25,710
monolith we're still in Python - feel

00:01:24,960 --> 00:01:27,840
free to booth later

00:01:25,710 --> 00:01:29,369
so that means django 1 it's not really

00:01:27,840 --> 00:01:31,020
relevant python 2 versus python 3 or

00:01:29,369 --> 00:01:32,909
django 1 versus 2 for deployment so it

00:01:31,020 --> 00:01:34,439
doesn't actually change very much we've

00:01:32,909 --> 00:01:37,380
got celery for background tasks and

00:01:34,439 --> 00:01:39,329
channels for WebSockets Postgres as our

00:01:37,380 --> 00:01:41,729
sequel database Redis as a cash rabbit

00:01:39,329 --> 00:01:44,820
for celery fairly standard big django

00:01:41,729 --> 00:01:46,860
web application we do have one slightly

00:01:44,820 --> 00:01:48,930
special thing which is that we deploy a

00:01:46,860 --> 00:01:51,329
single copy of a single tenant

00:01:48,930 --> 00:01:52,530
application per customer the main reason

00:01:51,329 --> 00:01:53,970
we do this is so that we can have

00:01:52,530 --> 00:01:55,409
customers on different versions although

00:01:53,970 --> 00:01:57,090
it does is a sort of side benefit mean

00:01:55,409 --> 00:01:58,469
that there's a much lower chance of a

00:01:57,090 --> 00:01:59,909
code bug meaning customers can see each

00:01:58,469 --> 00:02:01,350
other's data the only thing that's

00:01:59,909 --> 00:02:03,119
really relevant for this is that it

00:02:01,350 --> 00:02:04,770
means we deploy a lot more copies of our

00:02:03,119 --> 00:02:06,390
application than most other people but

00:02:04,770 --> 00:02:09,270
it's all the same problems as everyone

00:02:06,390 --> 00:02:11,670
has we just have more of them

00:02:09,270 --> 00:02:14,640
so what the old deployment system looks

00:02:11,670 --> 00:02:16,500
like the whole application is AWS based

00:02:14,640 --> 00:02:17,880
our company does use Google cloud for

00:02:16,500 --> 00:02:20,460
some other things but this application

00:02:17,880 --> 00:02:21,990
was all in AWS main two tools were

00:02:20,460 --> 00:02:23,250
terraformed for the initial provisioning

00:02:21,990 --> 00:02:24,690
although wrapped up in some big Python

00:02:23,250 --> 00:02:26,190
scripts so that we could deal with all

00:02:24,690 --> 00:02:28,470
the things that didn't support terraform

00:02:26,190 --> 00:02:30,450
and then ansible for configuring the

00:02:28,470 --> 00:02:33,090
system and also ansible for deploying

00:02:30,450 --> 00:02:34,860
the application code itself secrets

00:02:33,090 --> 00:02:36,300
lived in ansible vault and access

00:02:34,860 --> 00:02:38,100
control to the systems themselves was

00:02:36,300 --> 00:02:39,450
controlled by who had an SSH key so if

00:02:38,100 --> 00:02:41,340
your SSH keys on the machine you can

00:02:39,450 --> 00:02:42,660
deploy to that machine it's a solid

00:02:41,340 --> 00:02:44,070
setup it was very automated it was

00:02:42,660 --> 00:02:46,500
fairly easy to work with we could push

00:02:44,070 --> 00:02:48,270
deploys left right and center but it did

00:02:46,500 --> 00:02:49,950
have some issues launching a new

00:02:48,270 --> 00:02:51,390
instance of the application was fairly

00:02:49,950 --> 00:02:53,250
time-consuming was a couple hours from

00:02:51,390 --> 00:02:54,690
someone on my team it's all automated

00:02:53,250 --> 00:02:56,340
but you gotta run the scripts in the

00:02:54,690 --> 00:02:57,900
right order and check every step to make

00:02:56,340 --> 00:02:59,910
sure that things happen properly and

00:02:57,900 --> 00:03:01,320
that like the cloud didn't oops one of

00:02:59,910 --> 00:03:02,580
your machines

00:03:01,320 --> 00:03:04,740
similarly this doesn't really support

00:03:02,580 --> 00:03:06,360
auto scaling and even manual scaling was

00:03:04,740 --> 00:03:07,950
fairly time-consuming of setting up a

00:03:06,360 --> 00:03:09,300
new machine again making sure to join

00:03:07,950 --> 00:03:12,480
the cluster properly all that kind of

00:03:09,300 --> 00:03:14,070
stuff wasn't great we do also have bunch

00:03:12,480 --> 00:03:15,480
of Jango base micro services I'm just

00:03:14,070 --> 00:03:17,310
kind of ignoring them for the moment

00:03:15,480 --> 00:03:20,010
we're gonna focus on just this one big

00:03:17,310 --> 00:03:22,170
monolithic application so we did have

00:03:20,010 --> 00:03:23,520
some constraints on our change we knew

00:03:22,170 --> 00:03:25,080
we wanted to stick with kubernetes for

00:03:23,520 --> 00:03:26,459
container management we're already using

00:03:25,080 --> 00:03:28,230
it for micro services and it's my

00:03:26,459 --> 00:03:30,390
personal weapon of choice for container

00:03:28,230 --> 00:03:32,430
wrangling additionally we knew we wanted

00:03:30,390 --> 00:03:36,120
to keep using RDS for sequel and

00:03:32,430 --> 00:03:37,440
Claddagh MPP for rabbitmq I'm overall

00:03:36,120 --> 00:03:38,970
actually fairly in favor of running

00:03:37,440 --> 00:03:40,680
databases in kubernetes compared to most

00:03:38,970 --> 00:03:42,600
people but I have a small operations

00:03:40,680 --> 00:03:44,010
team if I can trade money for not having

00:03:42,600 --> 00:03:44,850
to run a database I'm pretty much always

00:03:44,010 --> 00:03:46,320
going to do that

00:03:44,850 --> 00:03:47,850
additionally made the migration easier

00:03:46,320 --> 00:03:50,580
didn't have to move production databases

00:03:47,850 --> 00:03:52,920
around so what do I mean by a forklift

00:03:50,580 --> 00:03:54,870
migration this term for me at least

00:03:52,920 --> 00:03:56,550
means I can make some changes to the

00:03:54,870 --> 00:03:57,900
backend application I'm you know I would

00:03:56,550 --> 00:03:59,640
a good working relations with all the

00:03:57,900 --> 00:04:01,500
backend teams but the overall goal was

00:03:59,640 --> 00:04:03,450
pick up the current code base and just

00:04:01,500 --> 00:04:06,239
slot it into kubernetes with as few

00:04:03,450 --> 00:04:08,220
changes as possible before start talking

00:04:06,239 --> 00:04:09,480
specifics just define some jargon that

00:04:08,220 --> 00:04:10,650
I'm gonna be using here and there for

00:04:09,480 --> 00:04:13,080
people that haven't worked cuber netis

00:04:10,650 --> 00:04:14,250
before a container is a cool way to run

00:04:13,080 --> 00:04:16,109
a process inside a whole bunch of

00:04:14,250 --> 00:04:17,790
security flags and an image is the

00:04:16,109 --> 00:04:19,380
larval form of a container contains all

00:04:17,790 --> 00:04:20,570
the stuff that you need to create a new

00:04:19,380 --> 00:04:22,260
container

00:04:20,570 --> 00:04:22,949
kubernetes has many things to many

00:04:22,260 --> 00:04:24,270
people

00:04:22,949 --> 00:04:26,310
the most important thing for this talk

00:04:24,270 --> 00:04:28,949
is it's an API so there's an actual like

00:04:26,310 --> 00:04:30,900
REST API that you can talk to to make

00:04:28,949 --> 00:04:33,240
your containers do stuff launch them

00:04:30,900 --> 00:04:34,199
create more shut them down make them

00:04:33,240 --> 00:04:35,639
jump through hoops so that your

00:04:34,199 --> 00:04:36,840
containers are actually useful it's very

00:04:35,639 --> 00:04:38,159
rare that a single container and

00:04:36,840 --> 00:04:39,779
isolation is really gonna do very much

00:04:38,159 --> 00:04:40,949
work for you in general you're gonna

00:04:39,779 --> 00:04:42,360
need a whole bunch of different

00:04:40,949 --> 00:04:44,060
containers doing different things and

00:04:42,360 --> 00:04:46,830
kubernetes puts that behind a rest api

00:04:44,060 --> 00:04:48,180
pods are a kubernetes specific term it

00:04:46,830 --> 00:04:49,499
mostly means the same thing as container

00:04:48,180 --> 00:04:50,909
but with a couple extras so when you

00:04:49,499 --> 00:04:52,259
hear me say pod if that's confusing to

00:04:50,909 --> 00:04:54,870
you just mentally replace that with the

00:04:52,259 --> 00:04:56,340
word container all right all that laid

00:04:54,870 --> 00:04:58,020
out I was ready to roll up my sleeves

00:04:56,340 --> 00:05:00,360
and jump in moving my application to

00:04:58,020 --> 00:05:01,889
kubernetes first step in container icing

00:05:00,360 --> 00:05:04,110
any application is to make a container

00:05:01,889 --> 00:05:05,250
image multi-stage builds in docker

00:05:04,110 --> 00:05:06,960
they're not even that new anymore

00:05:05,250 --> 00:05:08,610
they're added a couple years ago and

00:05:06,960 --> 00:05:09,719
they're especially useful for dealing

00:05:08,610 --> 00:05:11,669
with things like this big Python

00:05:09,719 --> 00:05:13,710
applications the idea of a multi-stage

00:05:11,669 --> 00:05:15,270
build is you have two separate images

00:05:13,710 --> 00:05:17,639
one for the build and one for the

00:05:15,270 --> 00:05:19,020
runtime image usually the building

00:05:17,639 --> 00:05:21,629
miju's going to be a lot bigger you need

00:05:19,020 --> 00:05:23,610
a Python compiler and the headers or

00:05:21,629 --> 00:05:25,259
sorry C compiler all the headers all

00:05:23,610 --> 00:05:27,870
that stuff you want to build all of your

00:05:25,259 --> 00:05:30,089
packages and then you dump just the

00:05:27,870 --> 00:05:32,099
necessary files into your runtime image

00:05:30,089 --> 00:05:33,360
along with like making a user and all

00:05:32,099 --> 00:05:34,979
the little sort of minor things

00:05:33,360 --> 00:05:37,830
so this runtime image is gonna be a lot

00:05:34,979 --> 00:05:39,120
smaller and leaner my first attempt to

00:05:37,830 --> 00:05:40,439
get things set up in kubernetes I

00:05:39,120 --> 00:05:42,899
decided we're not gonna deal with

00:05:40,439 --> 00:05:44,909
automation just can I get a manual proof

00:05:42,899 --> 00:05:47,490
of concept working writing a whole lot

00:05:44,909 --> 00:05:49,199
of Yambol by hand fitting a whole

00:05:47,490 --> 00:05:50,610
kubernetes TMO file on screen is

00:05:49,199 --> 00:05:52,919
basically impossible you would never be

00:05:50,610 --> 00:05:54,779
able to read it so this is just a small

00:05:52,919 --> 00:05:57,360
subsection of the container definition

00:05:54,779 --> 00:05:59,669
but here's a general idea we're going to

00:05:57,360 --> 00:06:01,949
launch a container it's going to run our

00:05:59,669 --> 00:06:04,710
migrations and then it's going to X X

00:06:01,949 --> 00:06:07,469
into G unicorn fairly simple it's going

00:06:04,710 --> 00:06:09,750
to mount up the configuration data from

00:06:07,469 --> 00:06:13,469
a you can't actually see it but from a

00:06:09,750 --> 00:06:17,310
config map so this gives us a fairly

00:06:13,469 --> 00:06:18,779
simple django environment but there are

00:06:17,310 --> 00:06:20,339
some problems with this it means every

00:06:18,779 --> 00:06:23,069
time the container restarts it's going

00:06:20,339 --> 00:06:24,210
to attempt to run the migrations so we

00:06:23,069 --> 00:06:25,740
could move that to a thing called an

00:06:24,210 --> 00:06:27,990
init container which is a kubernetes

00:06:25,740 --> 00:06:31,649
abstraction for before you actually run

00:06:27,990 --> 00:06:34,589
your application do some stuff good

00:06:31,649 --> 00:06:35,969
start but if we tried to run two copies

00:06:34,589 --> 00:06:36,780
of our pod if we wanted to save

00:06:35,969 --> 00:06:38,790
redundancy

00:06:36,780 --> 00:06:39,990
some additional load capacity they're

00:06:38,790 --> 00:06:41,940
both gonna try to run the migrations

00:06:39,990 --> 00:06:46,170
which is not usually a fun thing with

00:06:41,940 --> 00:06:48,030
Django so additionally we have to figure

00:06:46,170 --> 00:06:50,580
out how we were going to fit channels

00:06:48,030 --> 00:06:52,500
and celery into this but overall this

00:06:50,580 --> 00:06:55,320
gives you sort of a very basic idea of

00:06:52,500 --> 00:06:57,450
the manual version of this use right

00:06:55,320 --> 00:06:59,130
versions of this over and over until you

00:06:57,450 --> 00:07:01,680
have all the different types of things

00:06:59,130 --> 00:07:03,510
you need jewnicorn you need celery D you

00:07:01,680 --> 00:07:04,800
need celery beet and you need the

00:07:03,510 --> 00:07:06,660
various WebSockets things were actually

00:07:04,800 --> 00:07:08,070
on an old version WebSockets so it's

00:07:06,660 --> 00:07:09,600
still using the channel worker system

00:07:08,070 --> 00:07:11,210
and you can feel free to make fun of me

00:07:09,600 --> 00:07:15,060
later

00:07:11,210 --> 00:07:18,000
but yeah so so keeping track so we had

00:07:15,060 --> 00:07:20,280
other pods for dealing with celery

00:07:18,000 --> 00:07:22,590
Daphne and channel workers celery beets

00:07:20,280 --> 00:07:24,150
I would have needed to deal with but I

00:07:22,590 --> 00:07:26,850
just actually ignored for this initial

00:07:24,150 --> 00:07:28,230
prototype static file handling for the

00:07:26,850 --> 00:07:30,450
old system we were dealing with them

00:07:28,230 --> 00:07:33,960
through Jango storages and uploading

00:07:30,450 --> 00:07:36,450
them to s3 because we're an API driven

00:07:33,960 --> 00:07:38,010
environment we do have some web UI's but

00:07:36,450 --> 00:07:39,780
they're mostly for like admin consoles

00:07:38,010 --> 00:07:42,990
and stuff so we decided screw it let's

00:07:39,780 --> 00:07:44,280
just put all of that locally it helps to

00:07:42,990 --> 00:07:46,560
simplify the deployment process we

00:07:44,280 --> 00:07:48,450
didn't have to like deploy and then sync

00:07:46,560 --> 00:07:50,790
something up to s3 or deal with tracking

00:07:48,450 --> 00:07:52,320
what things in s3 are in use so they're

00:07:50,790 --> 00:07:54,540
just all serving from a little local

00:07:52,320 --> 00:07:56,640
webserver it's called caddy it's faster

00:07:54,540 --> 00:08:00,150
than like Jango run server but it's

00:07:56,640 --> 00:08:01,290
still handling it all locally I haven't

00:08:00,150 --> 00:08:02,550
really mentioned in grasses and I'm not

00:08:01,290 --> 00:08:04,950
going to go into them too deeply because

00:08:02,550 --> 00:08:07,200
we'd be here all day but I was using the

00:08:04,950 --> 00:08:09,810
traffic ingress controller to deal with

00:08:07,200 --> 00:08:10,950
routing between the jewnicorn the

00:08:09,810 --> 00:08:12,150
Daphne's server and the static file

00:08:10,950 --> 00:08:15,510
server because those are all split up

00:08:12,150 --> 00:08:17,400
very easily based on URL prefixes and

00:08:15,510 --> 00:08:18,750
finally I said that I wanted to use RDS

00:08:17,400 --> 00:08:21,840
but for some of the initial testing

00:08:18,750 --> 00:08:23,729
phases booting an RDS database takes

00:08:21,840 --> 00:08:25,350
about 20 minutes and I didn't want to

00:08:23,729 --> 00:08:27,300
have to wait that long every time so we

00:08:25,350 --> 00:08:30,229
started out using local Postgres through

00:08:27,300 --> 00:08:32,280
a thing called Postgres operator instead

00:08:30,229 --> 00:08:33,870
but then sort of slowly switched over

00:08:32,280 --> 00:08:36,450
RDS as things got a little bit more

00:08:33,870 --> 00:08:38,400
stable but okay I had a working proof of

00:08:36,450 --> 00:08:40,710
concept this did work but it was

00:08:38,400 --> 00:08:42,990
extremely robust didn't really handle

00:08:40,710 --> 00:08:45,390
migrations very well as we mentioned and

00:08:42,990 --> 00:08:47,370
it was not exactly easy to replicate

00:08:45,390 --> 00:08:49,060
this from one environment the next we

00:08:47,370 --> 00:08:50,950
would have to copy the whole man

00:08:49,060 --> 00:08:51,880
fessed edit out the different config

00:08:50,950 --> 00:08:53,950
cases that are different between

00:08:51,880 --> 00:08:56,380
customers and stuff not a great solution

00:08:53,950 --> 00:08:58,240
so the next tool in the standard

00:08:56,380 --> 00:08:59,740
kubernetes quiver is a thing called helm

00:08:58,240 --> 00:09:01,630
calls itself the kubernetes package

00:08:59,740 --> 00:09:03,190
manager and the overall idea is very

00:09:01,630 --> 00:09:04,750
similar to what we had before I'm not

00:09:03,190 --> 00:09:06,370
going to dive in too much into the chart

00:09:04,750 --> 00:09:07,660
itself because it's gonna look basically

00:09:06,370 --> 00:09:10,120
same as we had before just with more

00:09:07,660 --> 00:09:11,020
curly braces for variable replacements

00:09:10,120 --> 00:09:14,320
but it looks kind of like Django

00:09:11,020 --> 00:09:16,240
templates but they do is we take all of

00:09:14,320 --> 00:09:18,130
that yeah Mel that we saw before and we

00:09:16,240 --> 00:09:19,900
slot it into a big ol helm sharts and

00:09:18,130 --> 00:09:22,360
then we rubber stamp out our helm chart

00:09:19,900 --> 00:09:24,100
for each environment that we need this

00:09:22,360 --> 00:09:25,600
did improve some things

00:09:24,100 --> 00:09:27,100
Ament that we could store just the

00:09:25,600 --> 00:09:28,480
values that differ between environments

00:09:27,100 --> 00:09:30,520
as opposed to having giant piles of

00:09:28,480 --> 00:09:33,100
repeated yeah Mille the templating

00:09:30,520 --> 00:09:34,570
system is very easy to use it's great

00:09:33,100 --> 00:09:36,160
for community support and most of the

00:09:34,570 --> 00:09:38,460
communities community uses helm we even

00:09:36,160 --> 00:09:42,520
used it for some other things already

00:09:38,460 --> 00:09:44,200
but it did bring some problems it didn't

00:09:42,520 --> 00:09:45,940
really do much to fix the ordering and

00:09:44,200 --> 00:09:47,800
sequencing problems there is a system in

00:09:45,940 --> 00:09:50,470
helm called hooks that lets you say like

00:09:47,800 --> 00:09:53,140
please run this command before or after

00:09:50,470 --> 00:09:54,400
you you do your helm deploy we're hoping

00:09:53,140 --> 00:09:56,140
that we could use that for managing

00:09:54,400 --> 00:09:57,730
migrations but it turned out to be

00:09:56,140 --> 00:10:00,040
fairly error-prone and more importantly

00:09:57,730 --> 00:10:02,620
the debugging when hooks fail when the

00:10:00,040 --> 00:10:05,020
migrations failed was almost impossible

00:10:02,620 --> 00:10:07,150
so it didn't really actually fix that

00:10:05,020 --> 00:10:08,920
problem very much also there's basically

00:10:07,150 --> 00:10:10,450
no testing tools for helm it does have a

00:10:08,920 --> 00:10:12,580
thing called helm tests that you would

00:10:10,450 --> 00:10:14,560
think is useful for testing but actually

00:10:12,580 --> 00:10:16,180
that doesn't do very much so if you want

00:10:14,560 --> 00:10:19,089
to write unit integration tests which I

00:10:16,180 --> 00:10:20,440
really really did you're kind of going

00:10:19,089 --> 00:10:21,930
to have to build your own massive

00:10:20,440 --> 00:10:23,950
complicated scripting harness

00:10:21,930 --> 00:10:25,360
additionally secrets management they

00:10:23,950 --> 00:10:26,920
basically just say not our problem

00:10:25,360 --> 00:10:28,360
there's a lot of plugins for managing it

00:10:26,920 --> 00:10:30,430
all of which work completely differently

00:10:28,360 --> 00:10:32,140
but overall helm doesn't really give you

00:10:30,430 --> 00:10:35,470
much there other than sorry

00:10:32,140 --> 00:10:36,459
figure it out with your own scripts and

00:10:35,470 --> 00:10:39,370
then the final problem

00:10:36,459 --> 00:10:41,980
tiller this is a server-side component

00:10:39,370 --> 00:10:44,680
of helm 2 it has already been removed in

00:10:41,980 --> 00:10:46,750
helm 3 which is not currently available

00:10:44,680 --> 00:10:48,610
there is an early alpha build that

00:10:46,750 --> 00:10:50,500
doesn't actually work yet they're

00:10:48,610 --> 00:10:53,560
working on removing tiller like it's

00:10:50,500 --> 00:10:55,000
already been removed in master but if

00:10:53,560 --> 00:10:56,260
you want to actually use helm today that

00:10:55,000 --> 00:10:58,870
means helm 2 and that means you have

00:10:56,260 --> 00:11:00,940
tiller tiller is a security nightmare

00:10:58,870 --> 00:11:02,770
there are workarounds for dealing with

00:11:00,940 --> 00:11:03,070
it you can run multiple copies of it all

00:11:02,770 --> 00:11:04,770
over

00:11:03,070 --> 00:11:07,090
place with different security contexts

00:11:04,770 --> 00:11:08,350
but usually the way that you're going to

00:11:07,090 --> 00:11:09,310
use tiller is you are going to give it

00:11:08,350 --> 00:11:10,630
root permissions in your entire

00:11:09,310 --> 00:11:12,250
kubernetes cluster which means that

00:11:10,630 --> 00:11:13,510
anyone can top that can talk to it also

00:11:12,250 --> 00:11:16,240
has root permissions on your entire

00:11:13,510 --> 00:11:18,900
kubernetes cluster this did not make me

00:11:16,240 --> 00:11:22,960
happy as an infrastructure engineer

00:11:18,900 --> 00:11:24,490
so overall he'll definitely help with

00:11:22,960 --> 00:11:26,230
some stuff we could rubber stamp out our

00:11:24,490 --> 00:11:27,430
environments very very quickly and very

00:11:26,230 --> 00:11:30,130
era error-free

00:11:27,430 --> 00:11:33,940
but we didn't really have the the right

00:11:30,130 --> 00:11:38,710
state machine modeling so we moved on to

00:11:33,940 --> 00:11:41,020
version 3 this is our hopefully final

00:11:38,710 --> 00:11:43,450
approach we'll see how long this lasts

00:11:41,020 --> 00:11:45,730
as saying final but the idea was to

00:11:43,450 --> 00:11:47,260
write a kubernetes operator kubernetes

00:11:45,730 --> 00:11:50,230
operator is a general concept tied

00:11:47,260 --> 00:11:52,510
together three main things CR DS custom

00:11:50,230 --> 00:11:53,830
resource definitions watches and

00:11:52,510 --> 00:11:55,960
controllers and we'll go through all

00:11:53,830 --> 00:11:57,580
three of those now a custom resource

00:11:55,960 --> 00:11:59,560
definition or CR dekes I'm not going to

00:11:57,580 --> 00:12:01,810
say all of those words forever adds a

00:11:59,560 --> 00:12:03,400
new object type into kubernetes so for

00:12:01,810 --> 00:12:05,020
any let's work with kubernetes or seen

00:12:03,400 --> 00:12:07,750
my jargon definition before things like

00:12:05,020 --> 00:12:10,060
pods or services or in grasses those are

00:12:07,750 --> 00:12:11,320
types of objects in kubernetes they're

00:12:10,060 --> 00:12:13,420
things you can put them in the amol they

00:12:11,320 --> 00:12:15,340
have properties all that stuff CRT is

00:12:13,420 --> 00:12:17,050
let you define new object types and add

00:12:15,340 --> 00:12:19,030
them to kubernetes so that it will allow

00:12:17,050 --> 00:12:21,850
those objects in the API just like

00:12:19,030 --> 00:12:23,740
everything else so here we are our

00:12:21,850 --> 00:12:25,150
applications called summon or summon

00:12:23,740 --> 00:12:27,100
platform depending on how formal you

00:12:25,150 --> 00:12:29,530
want to be so we defined a new object

00:12:27,100 --> 00:12:31,540
type if you can basically imagine this

00:12:29,530 --> 00:12:34,830
just says django app we are giving that

00:12:31,540 --> 00:12:37,690
to kubernetes as a top-level type and

00:12:34,830 --> 00:12:39,400
then you can use that and just have a

00:12:37,690 --> 00:12:41,980
sort of reduced definition it's a little

00:12:39,400 --> 00:12:43,840
bit more boilerplate than just recording

00:12:41,980 --> 00:12:45,460
the the differences between environments

00:12:43,840 --> 00:12:47,980
like we did with helm but it's fairly

00:12:45,460 --> 00:12:50,080
clean you can see what version we are

00:12:47,980 --> 00:12:51,250
deploying we turn the debug flag on to

00:12:50,080 --> 00:12:55,330
true it's got a name it's got a name

00:12:51,250 --> 00:12:57,280
space pretty simple all around this is a

00:12:55,330 --> 00:12:59,110
valid kubernetes object if you have the

00:12:57,280 --> 00:13:01,870
CRT installed this is a thing that the

00:12:59,110 --> 00:13:03,580
kubernetes api will accept but they

00:13:01,870 --> 00:13:06,220
don't do anything on their own CR DS

00:13:03,580 --> 00:13:07,840
just add API as they add data storage

00:13:06,220 --> 00:13:09,400
but we have to actually set up an

00:13:07,840 --> 00:13:10,870
implementation of those objects to power

00:13:09,400 --> 00:13:12,130
them the way that we do that is through

00:13:10,870 --> 00:13:13,360
custom controllers and the way that

00:13:12,130 --> 00:13:14,709
controllers do that is through a thing

00:13:13,360 --> 00:13:18,129
called watches

00:13:14,709 --> 00:13:20,589
a watch is a piece of the kubernetes API

00:13:18,129 --> 00:13:22,720
where you can say for every object of

00:13:20,589 --> 00:13:25,089
type X please tell me every time an

00:13:22,720 --> 00:13:27,309
object of those changes so you can

00:13:25,089 --> 00:13:30,220
basically subscribe to changes or watch

00:13:27,309 --> 00:13:33,100
for changes in the kubernetes api is if

00:13:30,220 --> 00:13:35,470
it were a database so the general heart

00:13:33,100 --> 00:13:37,869
of every controller is going to be these

00:13:35,470 --> 00:13:39,790
three things set up a watch for some

00:13:37,869 --> 00:13:42,069
type of object every time one of those

00:13:39,790 --> 00:13:43,389
objects changes run it's generally

00:13:42,069 --> 00:13:45,129
called a reconcile though you could also

00:13:43,389 --> 00:13:47,259
say a converge or whatever the same

00:13:45,129 --> 00:13:49,480
thing as like chef or ansible uses take

00:13:47,259 --> 00:13:52,269
that new object that just changed and

00:13:49,480 --> 00:13:56,379
use that to reconfigure whatever it was

00:13:52,269 --> 00:13:58,059
managing repeat forever before we talk

00:13:56,379 --> 00:14:00,100
in dive into meant to talk about this

00:13:58,059 --> 00:14:01,240
business viewer django controller let's

00:14:00,100 --> 00:14:05,170
talk a little bit generally about

00:14:01,240 --> 00:14:06,670
convergence systems so before I even say

00:14:05,170 --> 00:14:08,920
convergence what is the opposite a

00:14:06,670 --> 00:14:11,529
procedural system this is when you

00:14:08,920 --> 00:14:13,389
design a system by giving it every step

00:14:11,529 --> 00:14:16,360
that you want to take in what order you

00:14:13,389 --> 00:14:18,939
want it to take them so every step of

00:14:16,360 --> 00:14:20,649
the owl as opposed to a convergent

00:14:18,939 --> 00:14:22,809
system where we don't tell it what steps

00:14:20,649 --> 00:14:24,399
we want to take instead we just say this

00:14:22,809 --> 00:14:27,129
is what we want the system to look like

00:14:24,399 --> 00:14:29,019
when you're done figure out what to do

00:14:27,129 --> 00:14:31,089
and the system will work out all the

00:14:29,019 --> 00:14:34,120
details of which pieces to change so on

00:14:31,089 --> 00:14:35,679
and so forth and two other sort of

00:14:34,120 --> 00:14:37,929
important concepts in convergence system

00:14:35,679 --> 00:14:39,189
design idempotence very fancy word all

00:14:37,929 --> 00:14:41,170
it means is that the system doesn't take

00:14:39,189 --> 00:14:42,790
action unless it needs to again this is

00:14:41,170 --> 00:14:44,679
also used in things like chef and an

00:14:42,790 --> 00:14:46,389
saman puppet if a package is already

00:14:44,679 --> 00:14:47,860
installed do not reinstall it if a pod

00:14:46,389 --> 00:14:50,649
is already running do not start another

00:14:47,860 --> 00:14:51,970
copy of it and then promise theory this

00:14:50,649 --> 00:14:53,439
one's a little bit more complicated so I

00:14:51,970 --> 00:14:55,089
can't go into all the details but it's a

00:14:53,439 --> 00:14:57,939
mathematical description and framework

00:14:55,089 --> 00:14:59,769
for breaking a convergent system down

00:14:57,939 --> 00:15:02,049
into smaller convergent systems these

00:14:59,769 --> 00:15:04,720
are called actors and each actor

00:15:02,049 --> 00:15:07,149
provides a promise or a contract to the

00:15:04,720 --> 00:15:09,009
rest of the world of if you give me this

00:15:07,149 --> 00:15:11,019
input I will do this thing and I will

00:15:09,009 --> 00:15:13,179
keep trying to do it over and over even

00:15:11,019 --> 00:15:17,220
if I fail until I manage to do the thing

00:15:13,179 --> 00:15:19,449
that I promised so why does this matter

00:15:17,220 --> 00:15:21,160
because being successful the kubernetes

00:15:19,449 --> 00:15:23,230
basically requires that you rethink your

00:15:21,160 --> 00:15:25,299
deployment generally from a procedural

00:15:23,230 --> 00:15:26,259
system to a convergent one most

00:15:25,299 --> 00:15:27,500
deployment systems are at least

00:15:26,259 --> 00:15:29,720
originally written

00:15:27,500 --> 00:15:31,940
some kind of bash script possibly via

00:15:29,720 --> 00:15:33,590
ansible but like they're generally do

00:15:31,940 --> 00:15:36,650
step one then step two then step three

00:15:33,590 --> 00:15:38,150
then step four and you'll see we do have

00:15:36,650 --> 00:15:40,250
some sequencing in here especially

00:15:38,150 --> 00:15:43,040
dealing with migrations but overall the

00:15:40,250 --> 00:15:44,810
idea is to instead think of this as what

00:15:43,040 --> 00:15:48,200
state do I want the system to be in when

00:15:44,810 --> 00:15:49,550
I am done with each step and trying to

00:15:48,200 --> 00:15:51,380
sort of limit the number of steps to

00:15:49,550 --> 00:15:54,200
only two places that we actually need a

00:15:51,380 --> 00:15:56,270
formal boundary so migrating versus

00:15:54,200 --> 00:15:57,860
migrations are done that's a formal

00:15:56,270 --> 00:15:59,690
boundary but after migrations are done

00:15:57,860 --> 00:16:01,070
we can just sort of tell it and this is

00:15:59,690 --> 00:16:03,950
what the system should look like I want

00:16:01,070 --> 00:16:09,820
15 copies of my web server and I want 16

00:16:03,950 --> 00:16:11,930
copies of seller ID make it happen so

00:16:09,820 --> 00:16:13,310
why does kubernetes use these patterns

00:16:11,930 --> 00:16:16,490
in the first place

00:16:13,310 --> 00:16:18,770
again complicated issue but the overall

00:16:16,490 --> 00:16:20,660
problem with procedural management of

00:16:18,770 --> 00:16:23,140
these big distributed and massive

00:16:20,660 --> 00:16:26,270
systems is the stuff drifts over time

00:16:23,140 --> 00:16:28,070
you end up with a failure here or

00:16:26,270 --> 00:16:29,390
failure there this machine reboots and

00:16:28,070 --> 00:16:31,670
now it's in a slightly different state

00:16:29,390 --> 00:16:33,740
unless you write your procedural code

00:16:31,670 --> 00:16:35,870
very carefully it has to account for

00:16:33,740 --> 00:16:37,580
every possible starting state to reach

00:16:35,870 --> 00:16:39,470
the desired end state as opposed to

00:16:37,580 --> 00:16:41,270
designing convergently where all you

00:16:39,470 --> 00:16:43,280
give it is the end state and a whole

00:16:41,270 --> 00:16:45,710
bunch of ways to reach that desired end

00:16:43,280 --> 00:16:47,270
state so you don't have to separately

00:16:45,710 --> 00:16:50,810
account for every possible starting

00:16:47,270 --> 00:16:52,550
condition all right so that's the

00:16:50,810 --> 00:16:54,710
general theory back to talking about our

00:16:52,550 --> 00:16:55,610
actual controller so controllers can be

00:16:54,710 --> 00:16:57,080
used for all kinds of things in

00:16:55,610 --> 00:16:58,970
kubernetes there's lots of weird ones

00:16:57,080 --> 00:17:00,350
already built in but for custom

00:16:58,970 --> 00:17:01,940
controllers they usually follow this

00:17:00,350 --> 00:17:03,710
pattern you're going to have a root

00:17:01,940 --> 00:17:06,860
object in our case it is called summon

00:17:03,710 --> 00:17:08,390
platform use that to generate a whole

00:17:06,860 --> 00:17:10,640
bunch of other kubernetes objects or

00:17:08,390 --> 00:17:13,459
sometimes to do like non kubernetes e

00:17:10,640 --> 00:17:15,319
things but for the simple ones our

00:17:13,459 --> 00:17:16,760
sudden platform object all it's really

00:17:15,319 --> 00:17:18,470
doing all this controller is doing is

00:17:16,760 --> 00:17:19,640
dealing with the ordering and sequencing

00:17:18,470 --> 00:17:21,500
of creating a whole bunch of other

00:17:19,640 --> 00:17:24,740
kubernetes objects and talking back into

00:17:21,500 --> 00:17:26,600
the API for those so this loop will run

00:17:24,740 --> 00:17:28,490
every single time so every time the

00:17:26,600 --> 00:17:29,780
Summum platform changes it goes through

00:17:28,490 --> 00:17:30,800
all of these so create a bunch of

00:17:29,780 --> 00:17:32,570
deployments pretty much the services

00:17:30,800 --> 00:17:33,860
create a bunch of in grasses make sure

00:17:32,570 --> 00:17:37,070
that they match we have a bunch of

00:17:33,860 --> 00:17:39,110
templates in our code make sure they

00:17:37,070 --> 00:17:41,120
match what we want for the requested

00:17:39,110 --> 00:17:44,720
input configuration on that some of

00:17:41,120 --> 00:17:46,220
we saw a bunch of slides ago this helps

00:17:44,720 --> 00:17:48,410
to ensure that we don't get drift as we

00:17:46,220 --> 00:17:50,300
mentioned before if anything is ever

00:17:48,410 --> 00:17:52,190
changing the controller will get a

00:17:50,300 --> 00:17:54,380
notification it'll see that hey look

00:17:52,190 --> 00:17:59,030
this unplanned it'll just fix whatever

00:17:54,380 --> 00:18:01,370
is running in kubernetes so enough about

00:17:59,030 --> 00:18:03,170
the ideas where's the code unfortunately

00:18:01,370 --> 00:18:04,610
writing complex operators in Python is

00:18:03,170 --> 00:18:06,200
doable but it's currently a bit tricky

00:18:04,610 --> 00:18:07,940
there are a couple of projects that are

00:18:06,200 --> 00:18:09,800
trying copy and meta controller are the

00:18:07,940 --> 00:18:11,240
two major ones but they're still very

00:18:09,800 --> 00:18:12,680
early in development and they're

00:18:11,240 --> 00:18:15,980
probably not really suitable for

00:18:12,680 --> 00:18:19,160
large-scale use so we wrote this in go

00:18:15,980 --> 00:18:22,670
COO builder is the most official of the

00:18:19,160 --> 00:18:24,890
the tools for doing this so even though

00:18:22,670 --> 00:18:26,720
we are a Python company and I personally

00:18:24,890 --> 00:18:28,640
like Python obviously I wouldn't be here

00:18:26,720 --> 00:18:30,920
we did write this whole thing and go

00:18:28,640 --> 00:18:35,390
instead hopefully won't hold that

00:18:30,920 --> 00:18:36,920
against me but okay so we sort of talked

00:18:35,390 --> 00:18:38,660
about this before of we need the state

00:18:36,920 --> 00:18:40,550
machine to figure out how we sort of

00:18:38,660 --> 00:18:41,840
sequence and manage things but now that

00:18:40,550 --> 00:18:43,750
we're into talking about controllers

00:18:41,840 --> 00:18:45,950
here's the actual state machine so

00:18:43,750 --> 00:18:46,540
initializing migrating deploying ready

00:18:45,950 --> 00:18:48,679
and error

00:18:46,540 --> 00:18:50,450
initializing is all of the one-time

00:18:48,679 --> 00:18:52,929
setup stuff booting the database setting

00:18:50,450 --> 00:18:56,030
up Redis setting up the RabbitMQ stuff

00:18:52,929 --> 00:18:57,890
migrating means I have detected that the

00:18:56,030 --> 00:18:59,660
version that I am deploying is not the

00:18:57,890 --> 00:19:01,850
last version I ran migrations for please

00:18:59,660 --> 00:19:03,050
run django migrations and deploying

00:19:01,850 --> 00:19:05,630
means the migrations have happened

00:19:03,050 --> 00:19:07,220
successfully please launch all of my new

00:19:05,630 --> 00:19:08,420
containers with the new version ready

00:19:07,220 --> 00:19:09,980
means all of those containers have

00:19:08,420 --> 00:19:13,190
launched successfully and there means

00:19:09,980 --> 00:19:14,960
something went wrong so we generally

00:19:13,190 --> 00:19:17,330
follow these just in a linear order it's

00:19:14,960 --> 00:19:19,400
a really simple state machine we wanted

00:19:17,330 --> 00:19:22,100
to keep this as thin as possible but

00:19:19,400 --> 00:19:23,929
this gives us fairly reasonable control

00:19:22,100 --> 00:19:26,870
over having all of our django stuff

00:19:23,929 --> 00:19:28,040
happen in the right order since the

00:19:26,870 --> 00:19:29,720
original is ago as I mentioned here's

00:19:28,040 --> 00:19:32,030
sort of a Python II example of how some

00:19:29,720 --> 00:19:35,270
of this stuff works so here's how we

00:19:32,030 --> 00:19:37,640
handle migrations for example if the

00:19:35,270 --> 00:19:38,929
input spec the spec version the input

00:19:37,640 --> 00:19:40,520
version that we want to deploy matches

00:19:38,929 --> 00:19:42,050
the last version we ran migrations for

00:19:40,520 --> 00:19:44,660
we're good we're done

00:19:42,050 --> 00:19:46,400
exit otherwise try to start the

00:19:44,660 --> 00:19:49,280
migrations if the migrations succeeded

00:19:46,400 --> 00:19:52,100
set the migrated version otherwise we're

00:19:49,280 --> 00:19:54,470
currently running the migrations so this

00:19:52,100 --> 00:19:55,970
kind of logic the advantage of control

00:19:54,470 --> 00:19:56,960
or something like helm or one of the

00:19:55,970 --> 00:19:59,299
advantages is that you can actually

00:19:56,960 --> 00:20:00,980
write this in code with helm you're

00:19:59,299 --> 00:20:02,509
limited to just the features that helm

00:20:00,980 --> 00:20:04,429
exposes you have to use their hook

00:20:02,509 --> 00:20:06,799
system because that's all you have with

00:20:04,429 --> 00:20:08,269
a controller you can mix and match so

00:20:06,799 --> 00:20:09,860
most of our code is relatively

00:20:08,269 --> 00:20:11,360
declarative it's basically the same as

00:20:09,860 --> 00:20:13,850
it would be in helm or ansible or

00:20:11,360 --> 00:20:15,590
whatever but when we need the power of

00:20:13,850 --> 00:20:21,080
an actual programming language we have

00:20:15,590 --> 00:20:23,480
it available now just sort of some

00:20:21,080 --> 00:20:25,370
specific stuff as advice for anyone that

00:20:23,480 --> 00:20:29,200
may have to deal with this in the future

00:20:25,370 --> 00:20:31,850
celery beets is a royal pain to run

00:20:29,200 --> 00:20:34,879
because it is a stateful service like

00:20:31,850 --> 00:20:36,529
web services channels all that stuff

00:20:34,879 --> 00:20:37,789
they're all stateless you can shut them

00:20:36,529 --> 00:20:38,509
down and start them back up and they

00:20:37,789 --> 00:20:40,429
don't care

00:20:38,509 --> 00:20:42,950
celery beet is special because it has to

00:20:40,429 --> 00:20:45,129
maintain state of which of this the the

00:20:42,950 --> 00:20:47,690
scheduled tasks have run and when

00:20:45,129 --> 00:20:49,759
because of this you have two basic

00:20:47,690 --> 00:20:51,200
options for dealing with us the first

00:20:49,759 --> 00:20:53,029
one is to run it as what's called a

00:20:51,200 --> 00:20:54,620
stateful set that is a tool in

00:20:53,029 --> 00:20:56,960
kubernetes the designed to help running

00:20:54,620 --> 00:20:58,549
stateful applications you can totally do

00:20:56,960 --> 00:21:00,529
that but it does require persistent

00:20:58,549 --> 00:21:04,220
storage for storing the statefulness of

00:21:00,529 --> 00:21:06,019
it and that can be tricky we have

00:21:04,220 --> 00:21:07,789
currently a slight recurring failure in

00:21:06,019 --> 00:21:10,129
Amazon that persistent storage doesn't

00:21:07,789 --> 00:21:14,870
always work right or there's a tool

00:21:10,129 --> 00:21:18,769
called celery bdx BDX lets you move the

00:21:14,870 --> 00:21:20,539
stateful storage into Redis database so

00:21:18,769 --> 00:21:22,279
as opposed to having to deal with local

00:21:20,539 --> 00:21:23,990
disk storage you can put it into your

00:21:22,279 --> 00:21:26,659
cache where it's a little bit easier to

00:21:23,990 --> 00:21:28,940
deal with B decks also solves another

00:21:26,659 --> 00:21:30,679
problem normally if you run two copies

00:21:28,940 --> 00:21:33,440
of celery beets you will end up with two

00:21:30,679 --> 00:21:35,179
copies of most of your scheduled tasks

00:21:33,440 --> 00:21:37,909
which is not what you want B decks has a

00:21:35,179 --> 00:21:39,110
fairly simple leader election process so

00:21:37,909 --> 00:21:40,519
you can run multiple copies for

00:21:39,110 --> 00:21:43,190
redundancy without having to deal with

00:21:40,519 --> 00:21:46,159
all of the madness of your own leader

00:21:43,190 --> 00:21:48,049
election system there is also Jango

00:21:46,159 --> 00:21:50,029
celery beets which lets you store the

00:21:48,049 --> 00:21:51,440
state in the Jango database but it also

00:21:50,029 --> 00:21:53,240
does a whole lot more than that like it

00:21:51,440 --> 00:21:54,919
sets up a web UI for manipulating your

00:21:53,240 --> 00:21:58,309
scheduled tasks which I didn't want

00:21:54,919 --> 00:22:00,649
anyone doing also we have tasks that run

00:21:58,309 --> 00:22:02,299
every second so this generates a lot of

00:22:00,649 --> 00:22:03,740
load on our sequel database and we

00:22:02,299 --> 00:22:04,759
didn't really want to do that Redis is

00:22:03,740 --> 00:22:06,980
generally a better fit

00:22:04,759 --> 00:22:08,090
there's a but or I wouldn't be giving

00:22:06,980 --> 00:22:09,950
you two options

00:22:08,090 --> 00:22:11,720
Naxos Python 3 only and as mentioned I'm

00:22:09,950 --> 00:22:13,309
not currently running Python 3 hopefully

00:22:11,720 --> 00:22:14,390
you all are and you do not have to deal

00:22:13,309 --> 00:22:17,450
with this and you can just use the nice

00:22:14,390 --> 00:22:19,130
thing but I can't have the nice thing so

00:22:17,450 --> 00:22:22,760
I'm doing it the staples that way

00:22:19,130 --> 00:22:26,390
please don't if you can't alright

00:22:22,760 --> 00:22:28,880
database management so we dealt with

00:22:26,390 --> 00:22:31,250
this by making more custom types and

00:22:28,880 --> 00:22:34,130
custom controllers we wanted to have a

00:22:31,250 --> 00:22:36,529
slight abstraction for local versus RDS

00:22:34,130 --> 00:22:38,600
databases so we wrote a new type called

00:22:36,529 --> 00:22:41,299
Postgres database of just give me a

00:22:38,600 --> 00:22:43,190
database if I'm running in dev make it

00:22:41,299 --> 00:22:45,830
locally if I'm running in prod give me

00:22:43,190 --> 00:22:47,149
RDS all that kind of stuff one of the

00:22:45,830 --> 00:22:49,490
nice things about controllers is that

00:22:47,149 --> 00:22:51,289
you can custom controllers and custom

00:22:49,490 --> 00:22:52,909
types can use other custom controllers

00:22:51,289 --> 00:22:54,350
and other custom types because as far as

00:22:52,909 --> 00:22:56,240
kubernetes is concerned they're all the

00:22:54,350 --> 00:22:59,690
same thing kubernetes just sees API

00:22:56,240 --> 00:23:01,520
calls and API types so you can nest

00:22:59,690 --> 00:23:03,590
these as deeply as you want some of them

00:23:01,520 --> 00:23:05,720
get a little bit squirrely but you can

00:23:03,590 --> 00:23:10,429
create tools and then use them in your

00:23:05,720 --> 00:23:12,230
own code app configuration is a little

00:23:10,429 --> 00:23:14,380
bit of a complex thing because your

00:23:12,230 --> 00:23:16,700
environment is now extremely dynamic

00:23:14,380 --> 00:23:18,590
dealing with things like what is my

00:23:16,700 --> 00:23:20,299
database password we wanted to automate

00:23:18,590 --> 00:23:22,909
that so we didn't have to like record it

00:23:20,299 --> 00:23:24,919
somewhere randomly generate passwords

00:23:22,909 --> 00:23:26,120
store them in a cooper net a secret pull

00:23:24,919 --> 00:23:27,649
them back out and put them into the

00:23:26,120 --> 00:23:30,289
Django config again this is pseudocode

00:23:27,649 --> 00:23:32,720
because the real one is in go but this

00:23:30,289 --> 00:23:34,789
lives in our summon controller and it

00:23:32,720 --> 00:23:38,179
will dynamically generate a Hamel config

00:23:34,789 --> 00:23:39,590
file that we then feed into Django and I

00:23:38,179 --> 00:23:41,149
am running low on time so I'm gonna try

00:23:39,590 --> 00:23:43,669
and speed through this a little bit that

00:23:41,149 --> 00:23:45,260
covers the sort of tech behind our

00:23:43,669 --> 00:23:48,110
deployment system but what about the

00:23:45,260 --> 00:23:50,059
workflow as I mentioned way back when we

00:23:48,110 --> 00:23:51,830
sort of started with a deployed SH

00:23:50,059 --> 00:23:53,179
script it it's a little more complicated

00:23:51,830 --> 00:23:54,559
in this because it's got some like input

00:23:53,179 --> 00:23:57,080
verification but it basically boils down

00:23:54,559 --> 00:24:00,580
to this run ansible playbook with a

00:23:57,080 --> 00:24:03,860
limit and some tags and an extra VARs

00:24:00,580 --> 00:24:06,710
the idea of defining sources of truth in

00:24:03,860 --> 00:24:08,450
a system means you try to map out what

00:24:06,710 --> 00:24:10,279
is authoritative for each piece of

00:24:08,450 --> 00:24:12,169
information in your system so for

00:24:10,279 --> 00:24:13,730
example with our ansible code the play

00:24:12,169 --> 00:24:15,350
books and roles they live and get and

00:24:13,730 --> 00:24:18,230
they are authoritative for how do you

00:24:15,350 --> 00:24:21,110
configure a thing but a problem with our

00:24:18,230 --> 00:24:21,680
old system was the the source code for

00:24:21,110 --> 00:24:23,720
the config

00:24:21,680 --> 00:24:26,000
management's lived and get but what

00:24:23,720 --> 00:24:27,260
version was deployed where that didn't

00:24:26,000 --> 00:24:29,210
really live anywhere definitely didn't

00:24:27,260 --> 00:24:31,280
live and get it kind of lived on each

00:24:29,210 --> 00:24:33,650
system based on the state of a git

00:24:31,280 --> 00:24:35,120
repository but that's not great this

00:24:33,650 --> 00:24:36,410
also comes up a lot with people that use

00:24:35,120 --> 00:24:38,780
Jenkins for deployment that you'll make

00:24:36,410 --> 00:24:40,460
a nice parameterize job and you can sort

00:24:38,780 --> 00:24:42,260
of fill in a version to deploy that's

00:24:40,460 --> 00:24:44,990
great but that means that the record of

00:24:42,260 --> 00:24:51,020
what version lives where is in a Jenkins

00:24:44,990 --> 00:24:56,210
build log not great so a related issue

00:24:51,020 --> 00:24:58,610
to this is drift so over time your

00:24:56,210 --> 00:25:00,530
system will sort of slowly as we said

00:24:58,610 --> 00:25:03,740
they'll be failures things will move out

00:25:00,530 --> 00:25:05,810
of config so we came up with this idea

00:25:03,740 --> 00:25:08,270
of we're not we the internet came up

00:25:05,810 --> 00:25:10,520
with the idea of get ops where git is

00:25:08,270 --> 00:25:12,950
your only source of truth all

00:25:10,520 --> 00:25:14,420
configuration goes in a file in a git

00:25:12,950 --> 00:25:15,440
repository usually they're giant piles

00:25:14,420 --> 00:25:17,900
of the animal because that's just how

00:25:15,440 --> 00:25:19,820
office rolls but you have something that

00:25:17,900 --> 00:25:22,070
watches those git repositories someone

00:25:19,820 --> 00:25:24,590
makes a pull request or whatever source

00:25:22,070 --> 00:25:26,840
control abstraction you use it gets

00:25:24,590 --> 00:25:29,030
merged the automation sees that there

00:25:26,840 --> 00:25:30,950
has been a commit to the correct branch

00:25:29,030 --> 00:25:33,380
it wakes up and it applies that config

00:25:30,950 --> 00:25:35,090
so the only way to effect change on the

00:25:33,380 --> 00:25:38,210
entire system is to make a change and

00:25:35,090 --> 00:25:40,190
get this is a lot of benefits I am a

00:25:38,210 --> 00:25:42,830
real big fan of this approach you get

00:25:40,190 --> 00:25:44,690
continuous drift resolution if anyone

00:25:42,830 --> 00:25:46,310
say like they go into prod and they just

00:25:44,690 --> 00:25:47,750
manually change something because that

00:25:46,310 --> 00:25:50,210
happens from time to time whether or not

00:25:47,750 --> 00:25:51,740
it should the next time there is a

00:25:50,210 --> 00:25:53,480
change because the entire state of the

00:25:51,740 --> 00:25:55,220
system lives and get the next time

00:25:53,480 --> 00:25:56,810
anything happens the system will check

00:25:55,220 --> 00:25:58,730
it'll see oh look this is out of date

00:25:56,810 --> 00:26:01,520
I'm fixing it for you

00:25:58,730 --> 00:26:03,410
so over time drift will be continuously

00:26:01,520 --> 00:26:05,150
fixed for you it also helps with

00:26:03,410 --> 00:26:07,280
disaster recovery again the whole state

00:26:05,150 --> 00:26:09,200
of the system is in one or more git

00:26:07,280 --> 00:26:11,360
repositories so you do a deploy through

00:26:09,200 --> 00:26:14,450
your normal deploy stuff and all of your

00:26:11,360 --> 00:26:16,070
stuff is back and you get to have a sort

00:26:14,450 --> 00:26:17,960
of unified workflow between code and

00:26:16,070 --> 00:26:20,090
infrastructure changes you can use code

00:26:17,960 --> 00:26:22,090
reviews you get logs it's useful as a

00:26:20,090 --> 00:26:24,470
sort of discount audit logging system

00:26:22,090 --> 00:26:25,730
you get you get to see who made changes

00:26:24,470 --> 00:26:27,920
and when hopefully they put a good

00:26:25,730 --> 00:26:29,270
commit message on it or there are review

00:26:27,920 --> 00:26:33,260
comments that you can look at to figure

00:26:29,270 --> 00:26:34,750
out why people made changes so we put

00:26:33,260 --> 00:26:37,390
all of this together our customs

00:26:34,750 --> 00:26:38,440
our custom operators get ups and this is

00:26:37,390 --> 00:26:41,680
what our new deployment workflow looks

00:26:38,440 --> 00:26:43,360
like so there is a big github repository

00:26:41,680 --> 00:26:45,160
for all of our deployments somebody

00:26:43,360 --> 00:26:46,240
finds the correct the mo file for the

00:26:45,160 --> 00:26:47,380
thing that they want to change the

00:26:46,240 --> 00:26:48,490
version of or could they create a new

00:26:47,380 --> 00:26:51,820
yeah mo file if they're making a new

00:26:48,490 --> 00:26:53,800
deployment they make a PR changing a

00:26:51,820 --> 00:26:56,260
version field or whatever else someone

00:26:53,800 --> 00:26:57,820
approves that it gets merged we use

00:26:56,260 --> 00:26:59,530
Argos edy is the thing that watches the

00:26:57,820 --> 00:27:01,270
github repository so it sees that

00:26:59,530 --> 00:27:03,340
there's been a change and get it applies

00:27:01,270 --> 00:27:05,800
the new yamo file out into kubernetes

00:27:03,340 --> 00:27:07,480
the operator wakes up it sees a look

00:27:05,800 --> 00:27:09,400
somebody changed some and platform

00:27:07,480 --> 00:27:13,840
object in the kubernetes api i'm going

00:27:09,400 --> 00:27:15,340
to go and start a deploy and really

00:27:13,840 --> 00:27:16,930
running at a time so going through this

00:27:15,340 --> 00:27:18,400
real quick some other pieces that we

00:27:16,930 --> 00:27:19,840
wrote that can be very helpful we wrote

00:27:18,400 --> 00:27:21,310
a command line tool to interface the

00:27:19,840 --> 00:27:24,190
system we called it ride cuddle because

00:27:21,310 --> 00:27:25,630
ride cell but shell pie shell and DB

00:27:24,190 --> 00:27:27,940
shell are just sort of helper commands

00:27:25,630 --> 00:27:31,120
for given an instance name and it'll go

00:27:27,940 --> 00:27:33,730
and find give you an xx shell or the the

00:27:31,120 --> 00:27:36,160
Jango shell or a Postgres shell and we

00:27:33,730 --> 00:27:38,230
also took a cue from the the homebrew

00:27:36,160 --> 00:27:40,480
book and we wrote a command called dr.

00:27:38,230 --> 00:27:41,890
that checks your environment and tries

00:27:40,480 --> 00:27:43,450
to figure out like what is wrong with

00:27:41,890 --> 00:27:44,680
your environment like maybe you don't

00:27:43,450 --> 00:27:46,960
have a kubernetes config that's why

00:27:44,680 --> 00:27:48,940
nothing is going to work it can also

00:27:46,960 --> 00:27:50,170
automatically fix things in cases that

00:27:48,940 --> 00:27:54,930
they're simple enough to automatically

00:27:50,170 --> 00:27:57,340
fix and quick bonus round we I

00:27:54,930 --> 00:27:58,750
personally like command lines I'm super

00:27:57,340 --> 00:28:00,160
happy with cube cuddle describe is my

00:27:58,750 --> 00:28:01,360
way to get information about the system

00:28:00,160 --> 00:28:03,370
but we do have a lot of people at my

00:28:01,360 --> 00:28:06,370
company that are less console focused

00:28:03,370 --> 00:28:07,300
and that's great so this is very new we

00:28:06,370 --> 00:28:09,130
haven't actually been deployed this to

00:28:07,300 --> 00:28:10,300
prod yet so a little bit of a preview if

00:28:09,130 --> 00:28:15,490
anyone at my company is watching this

00:28:10,300 --> 00:28:17,260
later but we wrote a very simple

00:28:15,490 --> 00:28:18,700
read-only web interface just to give

00:28:17,260 --> 00:28:20,980
people some way to access state

00:28:18,700 --> 00:28:22,510
information we hope to improve this over

00:28:20,980 --> 00:28:23,950
time but hopefully this will give people

00:28:22,510 --> 00:28:26,230
a little bit more transparency into the

00:28:23,950 --> 00:28:32,080
system if they're not kubernetes console

00:28:26,230 --> 00:28:34,780
jockeys and like I said this is all in

00:28:32,080 --> 00:28:36,370
go but it is also all open source it's

00:28:34,780 --> 00:28:38,020
not particularly well documented but if

00:28:36,370 --> 00:28:40,900
you would like to go look at this it is

00:28:38,020 --> 00:28:42,400
all there github right cell right so

00:28:40,900 --> 00:28:44,860
operators the operator ride cuddle is

00:28:42,400 --> 00:28:46,030
the command-line tool it's all super

00:28:44,860 --> 00:28:47,860
tailored for us but if you want to like

00:28:46,030 --> 00:28:48,850
take some code and remix it or use it

00:28:47,860 --> 00:28:51,490
anywhere else

00:28:48,850 --> 00:28:52,210
we welcome you to do so thank you very

00:28:51,490 --> 00:28:54,100
much

00:28:52,210 --> 00:28:57,660
we've got like 60 seconds for questions

00:28:54,100 --> 00:28:57,660
so probably mostly just come up the talk

00:29:00,120 --> 00:29:14,940
I can take like one or two questions but

00:29:05,520 --> 00:29:17,419
otherwise I'll be up here test do you

00:29:14,940 --> 00:29:20,880
know how interested the community these

00:29:17,419 --> 00:29:23,159
ecosystem is in Python in general or do

00:29:20,880 --> 00:29:26,490
you think it's worthwhile investing and

00:29:23,159 --> 00:29:28,289
go to interact with it at the moment the

00:29:26,490 --> 00:29:32,029
question was how how invested is the

00:29:28,289 --> 00:29:35,880
kubernetes ecosystem in Python medium

00:29:32,029 --> 00:29:37,770
there is a fairly decent sort of low

00:29:35,880 --> 00:29:41,429
level client library that's mostly

00:29:37,770 --> 00:29:45,090
auto-generated from the giant proto

00:29:41,429 --> 00:29:47,610
buffer code so that works ok but the

00:29:45,090 --> 00:29:49,380
sort of higher level tools not so much

00:29:47,610 --> 00:29:50,789
there's interest but there's not a lot

00:29:49,380 --> 00:29:54,440
of like money behind it so it's gonna

00:29:50,789 --> 00:29:56,490
move slowly as compared to the go stuff

00:29:54,440 --> 00:29:57,960
all right I think that's all the time we

00:29:56,490 --> 00:29:59,820
have if anyone else any questions please

00:29:57,960 --> 00:30:01,330
come up but other than that thank you so

00:29:59,820 --> 00:30:04,490
much for having me thank you

00:30:01,330 --> 00:30:04,490

YouTube URL: https://www.youtube.com/watch?v=7HQUInkb7Fo


