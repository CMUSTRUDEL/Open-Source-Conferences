Title: "cuDF: RAPIDS GPU-Accelerated Dataframe Library" - Mark Harris (PyCon AU 2019)
Publication date: 2019-08-02
Playlist: PyCon Australia 2019
Description: 
	Mark Harris

RAPIDS open-source software enables end-to-end data science and analytics pipelines to run entirely on GPUs. Key to RAPIDS is cuDF, a pandas-like Python data frame library with a high-performance CUDA C++ implementation. cuDF and RAPIDS enable large speedups for end-to-end data science using GPUs.

https://2019.pycon-au.org/talks/cudf-rapids-gpu-accelerated-dataframe-library

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Fri Aug  2 14:10:00 2019 at Cockle Bay
Captions: 
	00:00:00,000 --> 00:00:07,470
I'm on now great hi everyone welcome

00:00:03,330 --> 00:00:12,630
back to science and data I'm so up now

00:00:07,470 --> 00:00:14,429
we got Mark Harris to bio mark is a

00:00:12,630 --> 00:00:15,480
principal system software engineer

00:00:14,429 --> 00:00:17,010
working on Rapids

00:00:15,480 --> 00:00:19,439
mark has had 20 years of experience

00:00:17,010 --> 00:00:21,810
developing software for GPUs ranging

00:00:19,439 --> 00:00:23,640
from graphics and games to physically

00:00:21,810 --> 00:00:26,189
based simulations to parallel algorithms

00:00:23,640 --> 00:00:27,810
and high-performance computing

00:00:26,189 --> 00:00:30,990
and that's what he's going to be talking

00:00:27,810 --> 00:00:34,559
to us about today it's on qdf rapid GPU

00:00:30,990 --> 00:00:42,600
accelerated data frame library thanks

00:00:34,559 --> 00:00:44,190
for each oh hi everybody just a little

00:00:42,600 --> 00:00:47,579
caveat I'm a little out of my element

00:00:44,190 --> 00:00:49,980
here I'm not a data scientist and I also

00:00:47,579 --> 00:00:53,430
would call my Python programming at the

00:00:49,980 --> 00:00:56,930
level of dabbler so let me talk to you

00:00:53,430 --> 00:00:56,930
about Python data science

00:00:57,710 --> 00:01:04,830
so actually what I'm going to talk about

00:01:00,000 --> 00:01:08,010
is Rapids which is an open source suite

00:01:04,830 --> 00:01:11,130
of packages for accelerating data

00:01:08,010 --> 00:01:14,549
science on GPUs which are graphics

00:01:11,130 --> 00:01:16,290
processing units I'm not going to go

00:01:14,549 --> 00:01:19,470
into a lot of detail about what GPUs are

00:01:16,290 --> 00:01:21,869
or about CUDA which is the parallel

00:01:19,470 --> 00:01:25,710
computing platform for NVIDIA GPUs that

00:01:21,869 --> 00:01:28,619
we run on but there's a talk after mine

00:01:25,710 --> 00:01:32,009
by Varun who will cover a lot of that

00:01:28,619 --> 00:01:35,180
hopefully so stick around for that so

00:01:32,009 --> 00:01:37,680
the part I'm going to focus on mostly is

00:01:35,180 --> 00:01:43,860
GPU accelerated data frame library

00:01:37,680 --> 00:01:45,960
called ku DF so the goal of Rapids is to

00:01:43,860 --> 00:01:48,299
accelerate data science on GPUs

00:01:45,960 --> 00:01:53,600
end-to-end and so when I say end-to-end

00:01:48,299 --> 00:01:57,479
I mean from loading the data sets into

00:01:53,600 --> 00:01:59,969
memory preparing data training machine

00:01:57,479 --> 00:02:04,200
learning models all the way through to

00:01:59,969 --> 00:02:04,890
visualizing the results so the rapid

00:02:04,200 --> 00:02:06,810
suite

00:02:04,890 --> 00:02:10,760
comprises a number of components

00:02:06,810 --> 00:02:13,000
including the data frame library ku DF

00:02:10,760 --> 00:02:16,480
which also includes a mod

00:02:13,000 --> 00:02:19,420
for IO called qio a machine larb

00:02:16,480 --> 00:02:22,240
learning library called qml a graph

00:02:19,420 --> 00:02:24,940
analytics library cout graph and there's

00:02:22,240 --> 00:02:28,180
also a visualization component called

00:02:24,940 --> 00:02:30,370
cou cross filter and then there's

00:02:28,180 --> 00:02:32,580
interoperability with popular

00:02:30,370 --> 00:02:40,740
python-based deep learning frameworks

00:02:32,580 --> 00:02:45,790
like pi pi torch so if you if we rewind

00:02:40,740 --> 00:02:48,600
say a decade or so to the early days of

00:02:45,790 --> 00:02:48,600
Big Data

00:02:49,480 --> 00:02:55,510
Hadoop came along in it and it allowed

00:02:52,050 --> 00:02:58,360
people to scale up their data

00:02:55,510 --> 00:03:00,670
applications or scale out to their data

00:02:58,360 --> 00:03:03,750
applications to to a large number of

00:03:00,670 --> 00:03:06,790
nodes achieving much higher throughput

00:03:03,750 --> 00:03:09,340
but it had a number of inefficiencies in

00:03:06,790 --> 00:03:11,890
particular that if you have multiple

00:03:09,340 --> 00:03:15,040
phases in your workflow in this case we

00:03:11,890 --> 00:03:16,150
have like a database query some ETL if

00:03:15,040 --> 00:03:18,070
you're not familiar with that phrase

00:03:16,150 --> 00:03:20,680
it's it pops up a lot in these slides

00:03:18,070 --> 00:03:23,350
it's extract transform load but it

00:03:20,680 --> 00:03:25,780
basically is an old database term that

00:03:23,350 --> 00:03:28,209
today gets applied to data preparation

00:03:25,780 --> 00:03:31,950
and feature engineering and things like

00:03:28,209 --> 00:03:35,220
that and then machine learning training

00:03:31,950 --> 00:03:37,209
in between each of these phases Hadoop

00:03:35,220 --> 00:03:41,170
applications would have to write data

00:03:37,209 --> 00:03:43,390
out to disk in HDFS and read it back

00:03:41,170 --> 00:03:46,890
into the next phase etcetera and that

00:03:43,390 --> 00:03:51,130
was the source of a lot of inefficiency

00:03:46,890 --> 00:03:53,620
a few years ago

00:03:51,130 --> 00:03:57,310
Along Came Apache spark which is a big

00:03:53,620 --> 00:03:59,830
step forward which takes the point of

00:03:57,310 --> 00:04:03,700
view that if we keep all the data in

00:03:59,830 --> 00:04:06,670
memory and distributed and parcel it out

00:04:03,700 --> 00:04:08,620
across the nodes then we can get much

00:04:06,670 --> 00:04:10,360
better efficiency running keeping the

00:04:08,620 --> 00:04:12,690
data in memory and just passing it

00:04:10,360 --> 00:04:16,120
directly between the different phases

00:04:12,690 --> 00:04:20,260
and that led to much higher efficiency

00:04:16,120 --> 00:04:23,530
and better scaling now when we started

00:04:20,260 --> 00:04:26,470
on down the road towards accelerating

00:04:23,530 --> 00:04:28,240
data science on GPUs we looked

00:04:26,470 --> 00:04:31,390
and there was actually a fairly rich

00:04:28,240 --> 00:04:33,730
ecosystem of different components for

00:04:31,390 --> 00:04:37,870
accelerating different parts of data

00:04:33,730 --> 00:04:41,280
science on GPUs but it was very

00:04:37,870 --> 00:04:44,200
disparate and so while you can get

00:04:41,280 --> 00:04:47,440
acceleration by doing the computation on

00:04:44,200 --> 00:04:50,770
GPUs each of these components would

00:04:47,440 --> 00:04:55,260
require reading data into the GPU and

00:04:50,770 --> 00:04:57,460
then rewriting it out of the GPU which

00:04:55,260 --> 00:05:00,370
meant that it wasn't as efficient as it

00:04:57,460 --> 00:05:02,200
could be so just a here's a crude

00:05:00,370 --> 00:05:05,020
cartoon of that where on the Left we

00:05:02,200 --> 00:05:06,220
have the CPU and on the right the GPU if

00:05:05,020 --> 00:05:08,320
you imagine you have a couple of

00:05:06,220 --> 00:05:12,130
different components in your machine or

00:05:08,320 --> 00:05:15,130
in your data science workflow call them

00:05:12,130 --> 00:05:17,260
app a and at B then a payloads data onto

00:05:15,130 --> 00:05:20,620
the GPU and then processes it really

00:05:17,260 --> 00:05:23,050
fast but then has to copy the data back

00:05:20,620 --> 00:05:24,760
to the CPU memory converted into a

00:05:23,050 --> 00:05:27,360
format for application B which then

00:05:24,760 --> 00:05:30,070
loads it into back into GPU memory

00:05:27,360 --> 00:05:32,169
processes it really fast and then reads

00:05:30,070 --> 00:05:35,380
the data back and so all the gains that

00:05:32,169 --> 00:05:38,320
we get from optimizing the computation

00:05:35,380 --> 00:05:41,980
on the GPU in each of these components

00:05:38,320 --> 00:05:48,340
is largely lost by all this copy and

00:05:41,980 --> 00:05:51,760
convert so ideally we'd get rid of all

00:05:48,340 --> 00:05:55,930
those conversions and so the question is

00:05:51,760 --> 00:06:01,770
how do we do that well around the same

00:05:55,930 --> 00:06:05,020
time this same copy and convert problem

00:06:01,770 --> 00:06:06,550
was happening not just on GPUs but in in

00:06:05,020 --> 00:06:08,320
data science in general because there

00:06:06,550 --> 00:06:11,980
were a lot of different tools that

00:06:08,320 --> 00:06:13,930
people are using and they didn't share

00:06:11,980 --> 00:06:16,810
necessarily the same data formats and so

00:06:13,930 --> 00:06:19,930
Apache Aero is a project which aims to

00:06:16,810 --> 00:06:23,770
provide a common in memory columnar data

00:06:19,930 --> 00:06:26,200
format and a library of functionality

00:06:23,770 --> 00:06:28,480
such as Parque loaders and things like

00:06:26,200 --> 00:06:30,190
that which enables different

00:06:28,480 --> 00:06:33,310
technologies and different systems to

00:06:30,190 --> 00:06:36,150
share data in memory and eliminate all

00:06:33,310 --> 00:06:40,090
this copying and converting and so

00:06:36,150 --> 00:06:43,449
Rapids builds on

00:06:40,090 --> 00:06:46,750
g'bye using this format as its

00:06:43,449 --> 00:06:52,120
representation for columnar data and so

00:06:46,750 --> 00:06:55,030
by by keeping the data in memory we can

00:06:52,120 --> 00:06:56,979
eliminate all the GPU memory reads and

00:06:55,030 --> 00:06:59,110
writes or a lot of the copies and

00:06:56,979 --> 00:07:02,830
conversions and get much higher

00:06:59,110 --> 00:07:07,570
performance and this translates to

00:07:02,830 --> 00:07:09,340
real-world performance so this there's a

00:07:07,570 --> 00:07:14,950
lot on this slide but let me summarize

00:07:09,340 --> 00:07:18,040
it the left two graphs are showing two

00:07:14,950 --> 00:07:19,540
different components there's the data

00:07:18,040 --> 00:07:21,160
loading and preparation

00:07:19,540 --> 00:07:26,979
this is using ku DF which you can think

00:07:21,160 --> 00:07:29,560
of as similar to pandas and the middle

00:07:26,979 --> 00:07:32,169
one is XG boost training so this is

00:07:29,560 --> 00:07:35,919
gradient boosted trees we're training a

00:07:32,169 --> 00:07:40,300
model and on the right is end-to-end so

00:07:35,919 --> 00:07:42,490
this is the left two plus the conversion

00:07:40,300 --> 00:07:46,530
needed to get the data into extra boost

00:07:42,490 --> 00:07:50,020
format which is called a D matrix and

00:07:46,530 --> 00:07:53,740
the take-home is that if you look at a

00:07:50,020 --> 00:07:58,780
similar cost point so the DG x2 is an

00:07:53,740 --> 00:08:04,030
nvidia server that has 16 high-end Tesla

00:07:58,780 --> 00:08:06,220
V 100 GPUs and dual Xeon CPUs the

00:08:04,030 --> 00:08:08,380
similar cost point in terms of cloud

00:08:06,220 --> 00:08:13,810
data processing would be about 40 CPU

00:08:08,380 --> 00:08:16,960
nodes with you know 8 V CPUs each and so

00:08:13,810 --> 00:08:18,849
at that point the total cost cost of

00:08:16,960 --> 00:08:21,940
ownership or the speed-up is you know

00:08:18,849 --> 00:08:25,900
over 12 X here so the total cost of

00:08:21,940 --> 00:08:30,820
ownership is reduced by a factor of 10

00:08:25,900 --> 00:08:33,310
about the example here it's a 200

00:08:30,820 --> 00:08:35,219
gigabyte CSV data set this data

00:08:33,310 --> 00:08:40,089
preparation includes things like joins

00:08:35,219 --> 00:08:43,120
filtering group by operations and then

00:08:40,089 --> 00:08:48,520
the XG boost training is also

00:08:43,120 --> 00:08:50,740
accelerated on the GPUs but none of this

00:08:48,520 --> 00:08:54,230
matters if you have to change your code

00:08:50,740 --> 00:08:57,860
and so it the other main focus of rap

00:08:54,230 --> 00:09:01,970
is to accelerate familiar pythonic api's

00:08:57,860 --> 00:09:04,520
so we build on the existing PI data open

00:09:01,970 --> 00:09:07,730
source ecosystem so libraries are

00:09:04,520 --> 00:09:09,430
familiar with pandas for analytic

00:09:07,730 --> 00:09:14,330
scikit-learn for machine learning

00:09:09,430 --> 00:09:19,100
network X for graph analytics etc and we

00:09:14,330 --> 00:09:22,760
provide similar api's or compatible

00:09:19,100 --> 00:09:26,240
api's wherever possible that are GPU

00:09:22,760 --> 00:09:28,040
accelerated so ku DF for analytics QML

00:09:26,240 --> 00:09:32,030
for machine learning to graph for graph

00:09:28,040 --> 00:09:35,030
analytics now for the rest of the talk

00:09:32,030 --> 00:09:37,960
I'm going to focus mostly on qu TF since

00:09:35,030 --> 00:09:41,270
that's the component that I work on and

00:09:37,960 --> 00:09:47,150
then I'll give a really brief summary of

00:09:41,270 --> 00:09:50,570
Kuh ml at the end okay so why GPU

00:09:47,150 --> 00:09:53,300
accelerate ETL that's effectively what

00:09:50,570 --> 00:09:57,050
we're accelerating when we when we build

00:09:53,300 --> 00:09:58,400
a data from a frame library we talked to

00:09:57,050 --> 00:10:01,340
a lot of data scientists and we found

00:09:58,400 --> 00:10:04,340
that they don't spend most of their time

00:10:01,340 --> 00:10:06,410
training machine learning models they

00:10:04,340 --> 00:10:11,270
spend a lot of their time most of their

00:10:06,410 --> 00:10:14,060
time preparing data and one of our

00:10:11,270 --> 00:10:17,330
engineers drew this cartoon it's kind of

00:10:14,060 --> 00:10:19,430
tongue-in-cheek but on the left you have

00:10:17,330 --> 00:10:21,950
a traditional CPU powered workflow say

00:10:19,430 --> 00:10:24,680
using pandas or something like that and

00:10:21,950 --> 00:10:28,160
the data scientists starts in the

00:10:24,680 --> 00:10:30,710
morning by writing some code to load

00:10:28,160 --> 00:10:32,930
their data engineer some features clean

00:10:30,710 --> 00:10:35,020
it up etc they start it running it's a

00:10:32,930 --> 00:10:37,640
big data set so they go and get a coffee

00:10:35,020 --> 00:10:40,910
maybe come back it's not done they get

00:10:37,640 --> 00:10:45,080
another coffee then they realize crap I

00:10:40,910 --> 00:10:46,280
forgot one of the features so I've

00:10:45,080 --> 00:10:50,830
changed my code a little bit

00:10:46,280 --> 00:10:53,030
and restart it and wait and then oh

00:10:50,830 --> 00:10:54,680
there was none expected there were

00:10:53,030 --> 00:10:57,650
unexpected null values or missing data

00:10:54,680 --> 00:10:59,890
somewhere I need to catch that so I have

00:10:57,650 --> 00:11:02,630
to change the code again restart it etc

00:10:59,890 --> 00:11:05,690
so it's not just a fire-and-forget

00:11:02,630 --> 00:11:07,759
process it's an iterative process and if

00:11:05,690 --> 00:11:10,879
we can accelerate

00:11:07,759 --> 00:11:14,299
that wait when processing large data

00:11:10,879 --> 00:11:19,279
sets then data scientists can work at

00:11:14,299 --> 00:11:22,129
their speed of thought rather than you

00:11:19,279 --> 00:11:27,470
know waiting on the waiting on their

00:11:22,129 --> 00:11:29,629
computer so Lib soku DF is two

00:11:27,470 --> 00:11:33,139
components there's underneath as lib Kuh

00:11:29,629 --> 00:11:38,989
TF that's what I work on which is a CUDA

00:11:33,139 --> 00:11:42,919
C++ library which provides a low-level

00:11:38,989 --> 00:11:45,100
API to accelerate various data frame

00:11:42,919 --> 00:11:52,459
operations such as elementwise

00:11:45,100 --> 00:11:58,040
arithmetic filtering aggregations group

00:11:52,459 --> 00:12:01,699
bys joins etc sorting and it's built in

00:11:58,040 --> 00:12:03,019
C++ as I said and uses Nvidia CUDA for

00:12:01,699 --> 00:12:05,720
the parallel functions that run on the

00:12:03,019 --> 00:12:08,679
GPU as well as the library called thrust

00:12:05,720 --> 00:12:13,369
which is a parallel algorithms library

00:12:08,679 --> 00:12:17,089
and on top of that is KU DF which is a

00:12:13,369 --> 00:12:19,999
Python package which provides an API

00:12:17,089 --> 00:12:21,739
that is pandas like it's not to the

00:12:19,999 --> 00:12:23,539
letter compatible with pandas because

00:12:21,739 --> 00:12:26,209
there are some decisions that the pandas

00:12:23,539 --> 00:12:32,989
API makes that would cost performance on

00:12:26,209 --> 00:12:34,549
the GPU and it basically provides an

00:12:32,989 --> 00:12:37,160
interface to that lower level data frame

00:12:34,549 --> 00:12:39,289
library it allows you to create GPU data

00:12:37,160 --> 00:12:42,379
frames from the numpy arrays or from

00:12:39,289 --> 00:12:46,039
pandas dataframes or from PI Aero tables

00:12:42,379 --> 00:12:49,720
and it even provides user-defined

00:12:46,039 --> 00:12:52,339
functions by just-in-time compiling

00:12:49,720 --> 00:12:55,910
Python functions using a package called

00:12:52,339 --> 00:12:58,069
numba which is a just-in-time compiler

00:12:55,910 --> 00:13:07,459
for Python which can compile for CPUs or

00:12:58,069 --> 00:13:09,529
GPUs so because this is the part of that

00:13:07,459 --> 00:13:10,939
I work on it I'll just spend a little

00:13:09,529 --> 00:13:13,639
bit talking about some of the challenges

00:13:10,939 --> 00:13:16,279
that we have to deal with when building

00:13:13,639 --> 00:13:18,319
a library like this so the aim is to do

00:13:16,279 --> 00:13:20,179
accelerated GPU performance but with the

00:13:18,319 --> 00:13:21,710
flexibility and the breadth of

00:13:20,179 --> 00:13:26,800
functionality of

00:13:21,710 --> 00:13:32,420
library like pandas so we have to bridge

00:13:26,800 --> 00:13:39,620
dynamic language - static languages C++

00:13:32,420 --> 00:13:41,090
and CUDA C++ and we need to this is a

00:13:39,620 --> 00:13:43,640
challenge because we have to support

00:13:41,090 --> 00:13:46,250
operations on tables with a broad range

00:13:43,640 --> 00:13:49,040
of datatypes arbitrary number of columns

00:13:46,250 --> 00:13:52,310
and so that means it's important that we

00:13:49,040 --> 00:13:56,690
have flexible runtime dispatch on

00:13:52,310 --> 00:13:58,580
different types which is something

00:13:56,690 --> 00:14:02,630
that's natural in a dynamic language

00:13:58,580 --> 00:14:06,110
like Python but in a statically compiled

00:14:02,630 --> 00:14:08,870
language like C++ you need to work hard

00:14:06,110 --> 00:14:11,900
to make sure that this doesn't cause you

00:14:08,870 --> 00:14:14,810
to suffer extreme compile times and

00:14:11,900 --> 00:14:16,280
extreme binary sizes because you know if

00:14:14,810 --> 00:14:21,080
it takes eight hours to compile the

00:14:16,280 --> 00:14:23,450
library or the library as a gigabyte or

00:14:21,080 --> 00:14:25,550
ten gigabytes then just because of the

00:14:23,450 --> 00:14:27,230
different combinations of types that we

00:14:25,550 --> 00:14:30,350
have to support then that's not as

00:14:27,230 --> 00:14:33,050
that's a non-starter we have to support

00:14:30,350 --> 00:14:35,540
have to support missing an invalid data

00:14:33,050 --> 00:14:41,420
so there's this thing called a null bit

00:14:35,540 --> 00:14:43,490
mask which is just caused us all kinds

00:14:41,420 --> 00:14:47,630
of headaches but it basically enables us

00:14:43,490 --> 00:14:51,850
to support datasets that come in with

00:14:47,630 --> 00:14:55,570
missing data or Nan's or etc and

00:14:51,850 --> 00:14:59,140
enabling Python user-defined functions

00:14:55,570 --> 00:15:01,250
to operate on data frames on the GPU

00:14:59,140 --> 00:15:03,290
we're developing some interesting

00:15:01,250 --> 00:15:07,370
solutions on that by combining

00:15:03,290 --> 00:15:09,140
just-in-time compilation using numba and

00:15:07,370 --> 00:15:12,530
injecting the assembly that we get from

00:15:09,140 --> 00:15:16,700
that into c++ which we can then just in

00:15:12,530 --> 00:15:18,440
time compiled for the GPU and then

00:15:16,700 --> 00:15:19,880
another major challenge is high

00:15:18,440 --> 00:15:24,260
performance memory management we're

00:15:19,880 --> 00:15:26,930
dealing with very large datasets on

00:15:24,260 --> 00:15:30,050
across multiple GPUs and so we need to

00:15:26,930 --> 00:15:32,390
well and also there's a high frequency

00:15:30,050 --> 00:15:33,770
of allocation and de-allocation because

00:15:32,390 --> 00:15:35,690
often the

00:15:33,770 --> 00:15:37,880
the data frames are immutable so when

00:15:35,690 --> 00:15:40,310
you perform an operation on it you get a

00:15:37,880 --> 00:15:41,600
new data frame out and so we need to be

00:15:40,310 --> 00:15:46,160
able to do that efficiently so we've

00:15:41,600 --> 00:15:52,550
built a high-performance sub alligator

00:15:46,160 --> 00:15:54,670
on top of CUDA to enable that just some

00:15:52,550 --> 00:15:58,570
micro benchmarks for cootie F this is

00:15:54,670 --> 00:16:02,270
single GPU speed up compared to pandas

00:15:58,570 --> 00:16:06,650
all the bars are speed ups so GPU versus

00:16:02,270 --> 00:16:08,240
CPU and the blue is a larger data frame

00:16:06,650 --> 00:16:10,760
so two columns of a hundred million rows

00:16:08,240 --> 00:16:12,170
versus two columns at 10 million rows I

00:16:10,760 --> 00:16:15,140
think they're all double precision

00:16:12,170 --> 00:16:19,070
floats or 64-bit floats and you so you

00:16:15,140 --> 00:16:21,340
can see speed ups from 30 to 60 X on

00:16:19,070 --> 00:16:27,490
large data frames this is for group by

00:16:21,340 --> 00:16:30,830
merge which is like a join and sort o

00:16:27,490 --> 00:16:33,140
and it's running on NVIDIA DG x1 which

00:16:30,830 --> 00:16:35,870
is a server it's a 2-u server that has

00:16:33,140 --> 00:16:38,210
eight Tesla GPUs we're just using one of

00:16:35,870 --> 00:16:46,430
them for this these benchmarks and it

00:16:38,210 --> 00:16:48,200
has Intel Xeon CPUs ku DF also provides

00:16:46,430 --> 00:16:50,090
string support stringing support is

00:16:48,200 --> 00:16:53,570
really important for a lot of data sets

00:16:50,090 --> 00:16:57,350
and a lot of analysis we're currently at

00:16:53,570 --> 00:16:59,510
Rapids version 0.8 and so we currently

00:16:57,350 --> 00:17:01,190
have regular expressions support we have

00:16:59,510 --> 00:17:05,209
elementwise string operations like

00:17:01,190 --> 00:17:08,270
splitting and finding concatenation etc

00:17:05,209 --> 00:17:12,830
we can also do group bys and joins and

00:17:08,270 --> 00:17:15,380
sorting on on string columns in data

00:17:12,830 --> 00:17:17,570
frames and in the future we'll be adding

00:17:15,380 --> 00:17:19,700
a lot more string support in it

00:17:17,570 --> 00:17:22,220
improving it we're going to be combining

00:17:19,700 --> 00:17:24,740
the current ku strings library which is

00:17:22,220 --> 00:17:26,270
the way we implement strings as part of

00:17:24,740 --> 00:17:27,980
Lib qu DF so that we get better

00:17:26,270 --> 00:17:31,040
integration and reduced code duplicate

00:17:27,980 --> 00:17:34,390
duplication we're going to be optimizing

00:17:31,040 --> 00:17:36,590
performance improving compatibility and

00:17:34,390 --> 00:17:43,630
improving our support for categorical

00:17:36,590 --> 00:17:43,630
string columns and another big part of

00:17:43,690 --> 00:17:47,180
accelerating data frames is accelerating

00:17:46,610 --> 00:17:48,980
the i/o

00:17:47,180 --> 00:17:52,010
so being able to load different file

00:17:48,980 --> 00:17:55,610
formats into GPU memory and do it fast

00:17:52,010 --> 00:17:59,140
so the example here is showing comparing

00:17:55,610 --> 00:18:00,680
loading a two gigabyte a 1.9 gigabyte

00:17:59,140 --> 00:18:05,480
CSV file

00:18:00,680 --> 00:18:10,910
it's the taxi trip data set using pandas

00:18:05,480 --> 00:18:14,060
versus using ku DF and it's about a well

00:18:10,910 --> 00:18:19,190
29 seconds on the pandas and two seconds

00:18:14,060 --> 00:18:20,720
in QT f we also have a CSV brighter we

00:18:19,190 --> 00:18:23,600
have a park a reader and work reader

00:18:20,720 --> 00:18:26,510
Jace JSON and then in the next released

00:18:23,600 --> 00:18:30,140
and this month will be an Avro reader I

00:18:26,510 --> 00:18:33,080
think it's already merged and an HD f5

00:18:30,140 --> 00:18:35,300
reader in version zero about ten and

00:18:33,080 --> 00:18:39,290
October so the key to getting this

00:18:35,300 --> 00:18:40,850
performance is to accelerate both the

00:18:39,290 --> 00:18:42,380
compression if it's there I think in

00:18:40,850 --> 00:18:45,140
this CSV example there's no compression

00:18:42,380 --> 00:18:54,400
and also parsing and doing that all on

00:18:45,140 --> 00:18:58,190
the GPU so Rapids enables you to scale

00:18:54,400 --> 00:19:03,410
up to from CPUs to GPUs

00:18:58,190 --> 00:19:05,750
the PI data ecosystem effectively pandas

00:19:03,410 --> 00:19:08,390
scikit-learn you can also accelerate

00:19:05,750 --> 00:19:10,460
numpy using a package called KU pi which

00:19:08,390 --> 00:19:14,330
is from part of by the folks who do

00:19:10,460 --> 00:19:16,850
chainer and numba is another big part of

00:19:14,330 --> 00:19:19,940
that but we also want to be able to

00:19:16,850 --> 00:19:23,480
scale out so that we can handle real big

00:19:19,940 --> 00:19:26,840
data problems and so to do that we're

00:19:23,480 --> 00:19:28,460
leveraging desk which is a package a

00:19:26,840 --> 00:19:32,450
Python package for distributed computing

00:19:28,460 --> 00:19:34,610
which can scale from from two all the

00:19:32,450 --> 00:19:39,050
cores of your laptop or two all the

00:19:34,610 --> 00:19:41,600
nodes in your cluster and with Rapids we

00:19:39,050 --> 00:19:44,060
can use it to scale to all the GPU

00:19:41,600 --> 00:19:46,700
accelerated nodes in your cluster and

00:19:44,060 --> 00:19:49,610
combined with another package called

00:19:46,700 --> 00:19:53,830
open ucx which is an interface to

00:19:49,610 --> 00:19:58,700
hardware networking transports such as

00:19:53,830 --> 00:20:00,800
TCP well in five

00:19:58,700 --> 00:20:03,170
and envy link which is in videos

00:20:00,800 --> 00:20:04,640
interconnect between GPUs and things

00:20:03,170 --> 00:20:07,400
like that so that way we can scale up

00:20:04,640 --> 00:20:11,960
efficiently and not be bottlenecked by

00:20:07,400 --> 00:20:16,880
communication okay I just want to get a

00:20:11,960 --> 00:20:19,250
quick overview of qml so cou ml is

00:20:16,880 --> 00:20:21,260
another library like KU TF it has a

00:20:19,250 --> 00:20:24,080
Python interface in this case it's

00:20:21,260 --> 00:20:27,380
intended to be largely compatible with

00:20:24,080 --> 00:20:34,760
scikit-learn so familiar to PI data

00:20:27,380 --> 00:20:38,780
users and it has it's built on a CUDA

00:20:34,760 --> 00:20:42,490
substrate so a suite of CUDA custom

00:20:38,780 --> 00:20:46,900
algorithms as well as calls into the

00:20:42,490 --> 00:20:49,760
rich set of existing CUDA libraries like

00:20:46,900 --> 00:20:54,440
linear solvers and linear algebra

00:20:49,760 --> 00:20:56,780
libraries so we have a lot of algorithms

00:20:54,440 --> 00:20:58,310
listed here currently most of these are

00:20:56,780 --> 00:21:01,490
single GPU and a few of them are

00:20:58,310 --> 00:21:03,740
multi-gpu that's as of rapid 0.8

00:21:01,490 --> 00:21:06,560
we're basically gradually adding more

00:21:03,740 --> 00:21:10,220
algorithms adding more parallelism to

00:21:06,560 --> 00:21:12,590
multiple GPUs and then also scaling

00:21:10,220 --> 00:21:17,960
those to multiple nodes with multiple

00:21:12,590 --> 00:21:20,870
GPUs just as an example of what I mean

00:21:17,960 --> 00:21:23,590
when we say that the the api's are

00:21:20,870 --> 00:21:28,730
familiar this is a really simple example

00:21:23,590 --> 00:21:31,970
using SK learns make moons data set and

00:21:28,730 --> 00:21:34,220
pandas so we basically call make moons

00:21:31,970 --> 00:21:37,760
and then create a panda's data frame and

00:21:34,220 --> 00:21:42,050
then use that as input to SK learns DB

00:21:37,760 --> 00:21:43,850
scan and so we call DB scan not fit and

00:21:42,050 --> 00:21:45,430
DB Skanda predict and you can see the

00:21:43,850 --> 00:21:50,510
output here with the different color

00:21:45,430 --> 00:21:52,930
points that are clustered so to run this

00:21:50,510 --> 00:21:56,750
on the GPU is a simple in this case as

00:21:52,930 --> 00:22:03,920
switching from pandas to qu DF and

00:21:56,750 --> 00:22:06,830
switching from SK learn to q ml and the

00:22:03,920 --> 00:22:08,690
obligatory benchmarks these are again

00:22:06,830 --> 00:22:11,550
single GPU speed ups as I said it's

00:22:08,690 --> 00:22:13,590
still early days for ku ml so

00:22:11,550 --> 00:22:15,810
most of the things that we have today

00:22:13,590 --> 00:22:19,050
are single GPU but we're rapidly adding

00:22:15,810 --> 00:22:21,930
multi GPU acceleration for even for

00:22:19,050 --> 00:22:23,880
better speed ups and more scaling and so

00:22:21,930 --> 00:22:28,950
here you can see depending on the

00:22:23,880 --> 00:22:31,910
algorithm between say five X up to over

00:22:28,950 --> 00:22:34,320
a hundred x speed-up versus a SKU learn

00:22:31,910 --> 00:22:38,270
the one on the right I think they're

00:22:34,320 --> 00:22:40,980
separated because for DB scan and KN

00:22:38,270 --> 00:22:43,950
running the data sets are smaller

00:22:40,980 --> 00:22:45,930
because running data sets of millions of

00:22:43,950 --> 00:22:52,130
elements on these would on the CPU would

00:22:45,930 --> 00:22:54,950
be would take a very long time so okay

00:22:52,130 --> 00:22:58,410
how do you get started with Rapids or

00:22:54,950 --> 00:23:03,090
happens is entirely open source so all

00:22:58,410 --> 00:23:05,940
the code is on github there's a website

00:23:03,090 --> 00:23:11,130
home for Rapids Rapids a I or you can go

00:23:05,940 --> 00:23:12,990
to find information and links out to to

00:23:11,130 --> 00:23:14,930
whatever you need the source is on

00:23:12,990 --> 00:23:18,420
github as I said under the rapids AI

00:23:14,930 --> 00:23:20,940
organization so you'd go to Rapids AI

00:23:18,420 --> 00:23:23,330
/qu DF if you were interested in looking

00:23:20,940 --> 00:23:26,040
at the source for qu DF for example

00:23:23,330 --> 00:23:27,360
there are also condo packages so you

00:23:26,040 --> 00:23:30,890
don't have to build from source there

00:23:27,360 --> 00:23:33,200
are condo packages under the rapids AI I

00:23:30,890 --> 00:23:37,740
figure what they call it channel or

00:23:33,200 --> 00:23:39,060
whatever and also containers so if

00:23:37,740 --> 00:23:42,870
you're using containers you can get

00:23:39,060 --> 00:23:45,120
pre-built containers from either docker

00:23:42,870 --> 00:23:51,570
hub or from the NVIDIA GPU cloud

00:23:45,120 --> 00:23:54,260
container registry documentation or at

00:23:51,570 --> 00:23:57,630
Docs is that Doc's taught Rapids data I

00:23:54,260 --> 00:24:00,240
there's even tutorials like 10 minutes -

00:23:57,630 --> 00:24:01,620
coup d F which is analogous to 10

00:24:00,240 --> 00:24:05,400
minutes to pandas which you might have

00:24:01,620 --> 00:24:10,710
seen before and again it it's an open

00:24:05,400 --> 00:24:12,390
source endeavor and so if you're

00:24:10,710 --> 00:24:14,670
interested you can get involved either

00:24:12,390 --> 00:24:16,140
with the Rapids projects or with any of

00:24:14,670 --> 00:24:16,640
the these other projects like Apache

00:24:16,140 --> 00:24:20,370
Aero

00:24:16,640 --> 00:24:23,990
desk etc

00:24:20,370 --> 00:24:32,140
and thanks a lot thanks for listening

00:24:23,990 --> 00:24:32,140
[Applause]

00:24:32,779 --> 00:24:38,490
okay okay cool thanks mark yep right

00:24:36,450 --> 00:24:40,100
over there yep cool questions we got

00:24:38,490 --> 00:24:52,220
plenty of time for questions

00:24:40,100 --> 00:24:55,669
okay thanks really informative your

00:24:52,220 --> 00:24:58,470
increase in speeds were relative toward

00:24:55,669 --> 00:25:03,510
particular device that had a bunch of

00:24:58,470 --> 00:25:04,710
tensor or sorry um well they Tesla yeah

00:25:03,510 --> 00:25:07,230
just what if I just happened to have a

00:25:04,710 --> 00:25:11,669
1080 on my machine am I likely to get a

00:25:07,230 --> 00:25:13,110
benefit from using CUDA if sure well it

00:25:11,669 --> 00:25:15,500
also depends on how big your problem is

00:25:13,110 --> 00:25:18,419
of course so if your problems big enough

00:25:15,500 --> 00:25:21,539
then yeah if you've got a 1080 it's

00:25:18,419 --> 00:25:23,399
probably gonna accelerate it quite a bit

00:25:21,539 --> 00:25:27,539
anyway it you don't have to have the

00:25:23,399 --> 00:25:30,240
top-of-the-line Tesla because CUDA which

00:25:27,539 --> 00:25:33,809
is what we built on runs on all NVIDIA

00:25:30,240 --> 00:25:36,380
GPUs so great question at the very end

00:25:33,809 --> 00:25:39,899
there was a slide that mentioned Bose

00:25:36,380 --> 00:25:41,940
views and docker at the same time I and

00:25:39,899 --> 00:25:46,740
I'm curious just how difficult is that

00:25:41,940 --> 00:25:50,970
to set up there is a there's an Nvidia

00:25:46,740 --> 00:25:54,659
docker runtime so it's basically a

00:25:50,970 --> 00:25:55,950
plugin I don't know how I'm not a huge

00:25:54,659 --> 00:25:58,740
docker user so I don't know what the

00:25:55,950 --> 00:26:01,169
terminology is but basically you you

00:25:58,740 --> 00:26:05,490
install the Nvidia docker runtime and

00:26:01,169 --> 00:26:10,289
then there's an option when you do

00:26:05,490 --> 00:26:12,770
docker run to use that runtime and or

00:26:10,289 --> 00:26:15,169
maybe it's in the container but it

00:26:12,770 --> 00:26:18,539
effectively means that it loads the

00:26:15,169 --> 00:26:20,779
underlying device driver for the GPU

00:26:18,539 --> 00:26:22,559
when you run the container so that the

00:26:20,779 --> 00:26:26,580
applications within container have

00:26:22,559 --> 00:26:30,840
access to things like CUDA so yeah it's

00:26:26,580 --> 00:26:34,669
not it's not difficult do we have any

00:26:30,840 --> 00:26:34,669
more questions yeah

00:26:45,450 --> 00:26:52,080
given that the plan is to effectively

00:26:48,510 --> 00:26:55,110
move ETL to GPS which I don't think any

00:26:52,080 --> 00:26:57,950
people do today a lot are we also gonna

00:26:55,110 --> 00:27:03,690
see just more memory on the video GPUs

00:26:57,950 --> 00:27:06,929
so there's a couple of things in GPU

00:27:03,690 --> 00:27:08,910
memory is growing I mean in the dgx 2

00:27:06,929 --> 00:27:11,400
for example or if you if you were to buy

00:27:08,910 --> 00:27:13,230
a single Tesla via 100 they have 32

00:27:11,400 --> 00:27:15,780
gigabytes obviously if your data is

00:27:13,230 --> 00:27:17,760
really big it's going to have to be

00:27:15,780 --> 00:27:22,820
staged in and out of that so that

00:27:17,760 --> 00:27:25,320
there's a cost to that but even with

00:27:22,820 --> 00:27:28,640
with today's high-performance

00:27:25,320 --> 00:27:31,770
interconnects even with those copies I

00:27:28,640 --> 00:27:37,440
think you can still get big speed ups

00:27:31,770 --> 00:27:39,600
compared to something like pandas and a

00:27:37,440 --> 00:27:42,330
big part of that is the fact that you

00:27:39,600 --> 00:27:46,130
can if you're if you are chunking it up

00:27:42,330 --> 00:27:48,510
then the software underneath can overlap

00:27:46,130 --> 00:27:50,039
transfers with computation on other

00:27:48,510 --> 00:27:56,309
chunks and so it can be completely

00:27:50,039 --> 00:27:59,190
hidden the other thing is that in the

00:27:56,309 --> 00:28:00,929
high end GPU servers we have envy link

00:27:59,190 --> 00:28:04,350
which is a very high bandwidth

00:28:00,929 --> 00:28:07,470
interconnect between GPUs and so we're

00:28:04,350 --> 00:28:10,140
able able to peer-to-peer access data on

00:28:07,470 --> 00:28:13,409
other GPUs in a node so it basically

00:28:10,140 --> 00:28:18,630
amplifies the amount of memory that you

00:28:13,409 --> 00:28:20,039
have available and another is that where

00:28:18,630 --> 00:28:22,559
we have a technology called unified

00:28:20,039 --> 00:28:25,409
memory which is virtual memory for GPUs

00:28:22,559 --> 00:28:29,820
so we can effectively page data in and

00:28:25,409 --> 00:28:31,799
out of the GPU automatically to to to

00:28:29,820 --> 00:28:34,740
host memory that's something that is

00:28:31,799 --> 00:28:38,909
available in CUDA today that we're not

00:28:34,740 --> 00:28:41,220
using heavily in rapids yet but we will

00:28:38,909 --> 00:28:42,809
be moving in that direction you

00:28:41,220 --> 00:28:44,429
mentioned that you're sort of slowly

00:28:42,809 --> 00:28:46,770
migrating some of the functions and

00:28:44,429 --> 00:28:50,640
getting more complete parity with

00:28:46,770 --> 00:28:52,919
vendors and the ML stuff like it you

00:28:50,640 --> 00:28:57,480
have a matrix of this or timeline or

00:28:52,919 --> 00:29:00,269
prioritization of what we do

00:28:57,480 --> 00:29:01,710
if not extremely well publicized we're

00:29:00,269 --> 00:29:02,850
actually working out we had a meeting

00:29:01,710 --> 00:29:05,789
recently where we were talking about

00:29:02,850 --> 00:29:10,769
getting roadmaps on the rapids website

00:29:05,789 --> 00:29:12,840
at least rough ones but the github

00:29:10,769 --> 00:29:15,360
issues are a good place if you have a

00:29:12,840 --> 00:29:20,429
request you know if we don't hear about

00:29:15,360 --> 00:29:22,620
it then we can't prioritize it so I won

00:29:20,429 --> 00:29:24,419
I apologize for the estupid vagueness of

00:29:22,620 --> 00:29:26,309
this question I haven't been following

00:29:24,419 --> 00:29:28,289
PC specs for many years and I don't I

00:29:26,309 --> 00:29:30,149
works assured me a laptop can't remember

00:29:28,289 --> 00:29:31,980
what I've got is it the case that every

00:29:30,149 --> 00:29:33,990
modern laptop is going to have a GPU

00:29:31,980 --> 00:29:38,220
that'll be kind of compatible or would I

00:29:33,990 --> 00:29:44,549
know if I have this so you to run it on

00:29:38,220 --> 00:29:47,399
your laptop you would need a NVIDIA GPU

00:29:44,549 --> 00:29:50,130
in the case of Rapids because it's built

00:29:47,399 --> 00:29:51,980
on CUDA the other thing is currently

00:29:50,130 --> 00:29:56,820
we're Linux only so you need a Linux

00:29:51,980 --> 00:29:59,340
laptop but you know I am a cook and I've

00:29:56,820 --> 00:30:01,740
developed completely remotely you can

00:29:59,340 --> 00:30:04,590
run Rapids in the cloud

00:30:01,740 --> 00:30:08,490
you can even try it out for free on

00:30:04,590 --> 00:30:11,090
google colab collaboratory which has

00:30:08,490 --> 00:30:13,230
GPUs that can run Rapids geez its frito

00:30:11,090 --> 00:30:14,370
I've never played games on my laptop so

00:30:13,230 --> 00:30:18,769
I never actually looked at the sticker

00:30:14,370 --> 00:30:18,769
before so yeah just figured I'd oh

00:30:20,690 --> 00:30:26,110
that's all we have time for so thank you

00:30:23,730 --> 00:30:30,869
again Mark for having such a great cool

00:30:26,110 --> 00:30:30,869

YouTube URL: https://www.youtube.com/watch?v=lV7rtDW94do


