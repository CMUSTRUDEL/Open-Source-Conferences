Title: Tunnel Snakes Rule! Bringing the many worlds of Python together to monitor Melbourne's biggest infra
Publication date: 2019-08-03
Playlist: PyCon Australia 2019
Description: 
	Evan Brumley

Python is being used to provide real-time environmental monitoring on the Melbourne Metro Tunnel project. Come along to see how open source Python tools from the web, IoT, cloud infrastructure and scientific domains are being used together to monitor environmental telemetry on a city-wide scale.

https://2019.pycon-au.org/talks/tunnel-snakes-rule-bringing-the-many-worlds-of-python-together-to-monitor-melbournes-biggest-infrastructure-project

PyCon AU, the national Python Language conference, is on again this August in Sydney, at the International Convention Centre, Sydney, August 2 - 6 2019.

Video licence: CC BY-NC-SA 4.0 - https://creativecommons.org/licenses/by-nc-sa/4.0/

Python, PyCon, PyConAU

Sat Aug  3 11:10:00 2019 at C3.3
Captions: 
	00:00:00,060 --> 00:00:03,950
hello everybody we're in the second

00:00:02,159 --> 00:00:06,810
session at this room

00:00:03,950 --> 00:00:09,120
can you welcome everybody to stage and

00:00:06,810 --> 00:00:18,930
give him a big round of applause to talk

00:00:09,120 --> 00:00:20,880
to us about trance hi everyone I'm Evan

00:00:18,930 --> 00:00:23,070
Bromley and Evan Bromley on Twitter I'm

00:00:20,880 --> 00:00:24,630
a senior solutions architects with WSB

00:00:23,070 --> 00:00:26,730
digital in Melbourne

00:00:24,630 --> 00:00:29,279
WSP has been heavily involved in the

00:00:26,730 --> 00:00:31,230
Melbourne metro tunnel project Melbourne

00:00:29,279 --> 00:00:32,910
metro tunnel project is a new non

00:00:31,230 --> 00:00:34,590
kilometer twin tunnel underneath the

00:00:32,910 --> 00:00:36,360
Melbourne central business district and

00:00:34,590 --> 00:00:38,550
encompasses five news stations it's an

00:00:36,360 --> 00:00:40,160
11 billion dollar project started in

00:00:38,550 --> 00:00:43,140
2018 and the scheduled for completion

00:00:40,160 --> 00:00:44,969
2025 this is the route of the new tunnel

00:00:43,140 --> 00:00:47,550
so you can see we've got our five

00:00:44,969 --> 00:00:50,039
stations north melbourne parkville state

00:00:47,550 --> 00:00:51,680
library town hall and anzac those are

00:00:50,039 --> 00:00:54,030
currently five giant construction pits

00:00:51,680 --> 00:00:55,739
and there's also another two

00:00:54,030 --> 00:00:57,960
construction pits at the entrances to

00:00:55,739 --> 00:00:59,850
each the tunnels so seven large

00:00:57,960 --> 00:01:01,649
construction sites in total I'm

00:00:59,850 --> 00:01:05,600
contractually obliged to show you three

00:01:01,649 --> 00:01:08,729
of our shiny station renders this is the

00:01:05,600 --> 00:01:10,740
this is the new North Melbourne station

00:01:08,729 --> 00:01:12,659
this is the new parkville station and

00:01:10,740 --> 00:01:14,880
this is the new state library station

00:01:12,659 --> 00:01:16,439
I'm also contracted by to show three

00:01:14,880 --> 00:01:23,970
photos of people in high hats looking at

00:01:16,439 --> 00:01:25,380
things so here are them and one final

00:01:23,970 --> 00:01:27,000
one this is Meg the tunnel boring

00:01:25,380 --> 00:01:29,869
machine named after the current captain

00:01:27,000 --> 00:01:33,360
of the Australian women's cricket team

00:01:29,869 --> 00:01:35,909
so melbourne metro project had a problem

00:01:33,360 --> 00:01:38,280
and here is the problem or one of our

00:01:35,909 --> 00:01:39,540
problems this is the parkville precinct

00:01:38,280 --> 00:01:41,640
it can be a little hard to read the

00:01:39,540 --> 00:01:43,710
white text but we have the University of

00:01:41,640 --> 00:01:45,090
Melbourne the great Victorian

00:01:43,710 --> 00:01:46,320
Comprehensive Cancer Center Peter

00:01:45,090 --> 00:01:49,460
Doherty Institute the Royal Melbourne

00:01:46,320 --> 00:01:51,060
Hospital and in these buildings we have

00:01:49,460 --> 00:01:52,320
radiotherapy machines in the

00:01:51,060 --> 00:01:55,710
Comprehensive Cancer Center these are

00:01:52,320 --> 00:01:57,240
the big hulking multi-ton radiotherapy

00:01:55,710 --> 00:01:58,920
treatment machines linear accelerators

00:01:57,240 --> 00:02:01,290
that are treating patients with

00:01:58,920 --> 00:02:02,790
millimeter precision electron

00:02:01,290 --> 00:02:03,869
microscopes in the Peter Doherty viral

00:02:02,790 --> 00:02:06,090
research institute some of the most

00:02:03,869 --> 00:02:09,060
sensitive expensive electron microscopes

00:02:06,090 --> 00:02:10,410
in Australia animal laboratories in the

00:02:09,060 --> 00:02:13,410
Melbourne University of Melbourne

00:02:10,410 --> 00:02:15,210
medical building rats mice

00:02:13,410 --> 00:02:16,890
fish exotic fish they've been doing a

00:02:15,210 --> 00:02:19,230
longitudinal study of fish for 20 years

00:02:16,890 --> 00:02:23,460
and those fish apparently don't like

00:02:19,230 --> 00:02:24,690
being shaken or allowed noises there are

00:02:23,460 --> 00:02:26,010
medical classrooms that building was

00:02:24,690 --> 00:02:27,690
built in the 70s their idea of

00:02:26,010 --> 00:02:29,850
ventilation was to have an air gap at

00:02:27,690 --> 00:02:31,020
the bottom of every window so when you

00:02:29,850 --> 00:02:32,460
have a construction site next door that

00:02:31,020 --> 00:02:33,750
can be unpleasant

00:02:32,460 --> 00:02:36,270
there are operating theaters and you

00:02:33,750 --> 00:02:38,610
know patient bedrooms on in the robot

00:02:36,270 --> 00:02:41,970
hospital we have engineering workshops

00:02:38,610 --> 00:02:42,960
with wave pools and flow chambers and

00:02:41,970 --> 00:02:47,850
that sort of thing also so sense into

00:02:42,960 --> 00:02:50,160
equipment and finally we have a thermal

00:02:47,850 --> 00:02:53,760
University Law School which which is

00:02:50,160 --> 00:02:55,410
completely filled with lawyers so what's

00:02:53,760 --> 00:02:59,580
our problem the problem is that this is

00:02:55,410 --> 00:03:03,380
the construction site the construction

00:02:59,580 --> 00:03:05,250
site as of last month looks like this

00:03:03,380 --> 00:03:07,560
this is actually looking a bit cleaner

00:03:05,250 --> 00:03:09,330
than it was I've just finished piling so

00:03:07,560 --> 00:03:13,050
it's it's looking a little bit neater

00:03:09,330 --> 00:03:15,180
but yeah it's it's a mess so we have to

00:03:13,050 --> 00:03:16,620
handle some pretty sensitive locations

00:03:15,180 --> 00:03:18,360
and the environmental reporting

00:03:16,620 --> 00:03:20,370
requirements are really really strict so

00:03:18,360 --> 00:03:21,810
we have this document this is the

00:03:20,370 --> 00:03:23,130
environmental management framework you

00:03:21,810 --> 00:03:24,450
can access it at the link which you

00:03:23,130 --> 00:03:27,120
can't see because my slides are cut off

00:03:24,450 --> 00:03:28,110
but if you google search for metro

00:03:27,120 --> 00:03:31,950
tunnel environmental management

00:03:28,110 --> 00:03:33,510
framework you'll you will find it it's a

00:03:31,950 --> 00:03:34,920
fairly lengthy document I think there's

00:03:33,510 --> 00:03:37,110
about 89 pages

00:03:34,920 --> 00:03:38,670
it contains really useful information

00:03:37,110 --> 00:03:40,410
like what we're supposed to do for

00:03:38,670 --> 00:03:42,810
monitoring reporting how we can get

00:03:40,410 --> 00:03:43,950
audited and most importantly it has this

00:03:42,810 --> 00:03:46,380
our environmental performance

00:03:43,950 --> 00:03:48,180
requirements if you look at the page

00:03:46,380 --> 00:03:54,570
numbers carefully you'll see they go 27

00:03:48,180 --> 00:03:57,420
27 27 28 89 and that's because of table

00:03:54,570 --> 00:04:00,150
7 table 7 is the list of Environment

00:03:57,420 --> 00:04:02,730
over 25 minutes 59 pages long 132

00:04:00,150 --> 00:04:04,620
categorized labelled requirements many

00:04:02,730 --> 00:04:06,570
of these requirements are fairly

00:04:04,620 --> 00:04:08,310
procedural things so they'll be things

00:04:06,570 --> 00:04:09,810
like you'll must have this particular

00:04:08,310 --> 00:04:11,250
paperwork you should have this plan in

00:04:09,810 --> 00:04:12,630
place you should consult before you look

00:04:11,250 --> 00:04:14,790
at the before you talk about damaging

00:04:12,630 --> 00:04:17,270
these trees but a really good number of

00:04:14,790 --> 00:04:19,980
them are actually quite quantitative

00:04:17,270 --> 00:04:21,750
they rely on sensor data like that you

00:04:19,980 --> 00:04:22,950
should have senses in these places you

00:04:21,750 --> 00:04:25,740
should be monitoring these requirements

00:04:22,950 --> 00:04:27,210
and they're not as simple as just saying

00:04:25,740 --> 00:04:28,710
well you know you

00:04:27,210 --> 00:04:31,650
on a decibel level from this sensor

00:04:28,710 --> 00:04:33,060
can't go above this line it's we want

00:04:31,650 --> 00:04:36,509
you either put a sensor in in this

00:04:33,060 --> 00:04:37,919
animal laboratory and then take the

00:04:36,509 --> 00:04:42,270
readings from that and calculate what a

00:04:37,919 --> 00:04:45,150
fish would hear and and then do a few

00:04:42,270 --> 00:04:47,099
more things and and then you know you

00:04:45,150 --> 00:04:49,310
need to report those and levels back to

00:04:47,099 --> 00:04:51,539
the University of Melbourne in real time

00:04:49,310 --> 00:04:54,090
so that if something happens and they

00:04:51,539 --> 00:04:57,030
can go on through their fish more

00:04:54,090 --> 00:04:58,530
importanly they also have a rat a rat

00:04:57,030 --> 00:04:59,789
experiments going on and apparently rats

00:04:58,530 --> 00:05:01,860
eat each other

00:04:59,789 --> 00:05:04,160
if you shake them or or have experienced

00:05:01,860 --> 00:05:06,419
all that noises so it's pretty important

00:05:04,160 --> 00:05:07,949
and there's also you know the linear

00:05:06,419 --> 00:05:09,449
accelerators and so on actually treating

00:05:07,949 --> 00:05:11,970
patients and if we put them out of

00:05:09,449 --> 00:05:15,030
alignment then people die but yeah the

00:05:11,970 --> 00:05:16,410
fish are important as well so there's a

00:05:15,030 --> 00:05:17,940
question how do we keep track of all the

00:05:16,410 --> 00:05:19,229
requirements when the project is the

00:05:17,940 --> 00:05:21,449
size of a city that's just one of the

00:05:19,229 --> 00:05:23,340
sites there are seven sites and they're

00:05:21,449 --> 00:05:26,849
all they all have problems in their own

00:05:23,340 --> 00:05:28,259
unique ways the classical approach which

00:05:26,849 --> 00:05:30,330
is engineering projects have been doing

00:05:28,259 --> 00:05:32,550
for a long time you go and buy or rent

00:05:30,330 --> 00:05:34,440
some sensors usually about twenty

00:05:32,550 --> 00:05:35,370
thousand dollars per sensor if you're

00:05:34,440 --> 00:05:37,800
getting the ones that you can be audited

00:05:35,370 --> 00:05:39,449
on you send graduate engineers to

00:05:37,800 --> 00:05:41,159
install them on-site they come in a

00:05:39,449 --> 00:05:42,570
briefcase they send the gratis out with

00:05:41,159 --> 00:05:43,919
a briefcase and they set them up and

00:05:42,570 --> 00:05:45,900
then usually sit with them for a night

00:05:43,919 --> 00:05:48,000
so don't get stolen take some readings

00:05:45,900 --> 00:05:50,130
and then then bring them back to the

00:05:48,000 --> 00:05:51,479
office you collect your CSV files

00:05:50,130 --> 00:05:52,949
directly from the sensors if you're

00:05:51,479 --> 00:05:55,199
really lucky they might have an FTP

00:05:52,949 --> 00:05:58,380
server built-in usually you'll just have

00:05:55,199 --> 00:05:59,789
an rs-232 cable you talk to you take

00:05:58,380 --> 00:06:01,229
those back to the office you analyze in

00:05:59,789 --> 00:06:03,060
Excel like a ledger you do your

00:06:01,229 --> 00:06:05,909
calculations usually have a big

00:06:03,060 --> 00:06:07,590
acoustics team and then you file report

00:06:05,909 --> 00:06:09,870
to the end of month to say yeah we we

00:06:07,590 --> 00:06:13,500
shook the fish ten times this month

00:06:09,870 --> 00:06:16,620
maybe you go check on them if it's not

00:06:13,500 --> 00:06:18,659
too late the modern approach which which

00:06:16,620 --> 00:06:20,759
more companies use these days is you go

00:06:18,659 --> 00:06:22,949
and acquire sensors from a vendor

00:06:20,759 --> 00:06:26,190
capital V vendor they have a SAS

00:06:22,949 --> 00:06:27,960
platform and they say yes you buy us

00:06:26,190 --> 00:06:31,139
biosensors from us they will have SIM

00:06:27,960 --> 00:06:34,820
cards built in and they will send data

00:06:31,139 --> 00:06:37,710
back directly to our SAS platform and

00:06:34,820 --> 00:06:40,740
they say ok login to our SAS platform

00:06:37,710 --> 00:06:43,650
set up your alerts we can do a bunch of

00:06:40,740 --> 00:06:45,300
of analysis and then you go to them and

00:06:43,650 --> 00:06:48,990
say yeah we need to find out what a fish

00:06:45,300 --> 00:06:50,220
could hear and thus ask guys and the

00:06:48,990 --> 00:06:52,770
vendor says well that's not really in

00:06:50,220 --> 00:06:54,150
our business plan it's going to take

00:06:52,770 --> 00:06:56,729
quite a bit of development time to make

00:06:54,150 --> 00:06:58,349
that work and then you go to the vendor

00:06:56,729 --> 00:06:59,669
and say we've also got sensors from like

00:06:58,349 --> 00:07:01,590
these other three companies that we

00:06:59,669 --> 00:07:06,410
inherited from another project can you

00:07:01,590 --> 00:07:06,410
integrate those and they just say no so

00:07:06,650 --> 00:07:11,000
these process these weren't going to

00:07:08,880 --> 00:07:14,720
work so we had to come up with a plan

00:07:11,000 --> 00:07:17,759
and the plan was to build a new platform

00:07:14,720 --> 00:07:21,360
this is the the project badge this is on

00:07:17,759 --> 00:07:23,940
my laptop they've got our noise cat

00:07:21,360 --> 00:07:25,800
vibration cat and air quality cat the

00:07:23,940 --> 00:07:27,270
MLM stands for Melbourne livability

00:07:25,800 --> 00:07:28,470
monitor which was the original name for

00:07:27,270 --> 00:07:29,759
the project it says changed the

00:07:28,470 --> 00:07:31,259
Melbourne tunnel environmental monitor

00:07:29,759 --> 00:07:34,349
because we had to have Metro tunnel in

00:07:31,259 --> 00:07:36,270
the name the requirements of this were

00:07:34,349 --> 00:07:38,759
we need to accept data from any device

00:07:36,270 --> 00:07:41,190
at all focus on noise vibration and air

00:07:38,759 --> 00:07:43,080
quality some of those devices were going

00:07:41,190 --> 00:07:44,460
to talk to us directly many others

00:07:43,080 --> 00:07:45,870
become going to come back with via the

00:07:44,460 --> 00:07:47,190
vend or SAS platforms we were still

00:07:45,870 --> 00:07:48,360
going to use those Venice's platforms

00:07:47,190 --> 00:07:49,800
because they do a lot of really good

00:07:48,360 --> 00:07:52,050
work in terms of actually talking

00:07:49,800 --> 00:07:54,449
directly to the devices if you are

00:07:52,050 --> 00:07:55,979
working on an IT project try not to work

00:07:54,449 --> 00:07:57,419
with devices directly if you possibly

00:07:55,979 --> 00:08:01,320
can

00:07:57,419 --> 00:08:02,789
so they were doing all the horrible you

00:08:01,320 --> 00:08:03,990
know connectivity stuff and going out

00:08:02,789 --> 00:08:06,630
and fixing devices if they weren't if

00:08:03,990 --> 00:08:10,380
they broke and mostly we talking vibe

00:08:06,630 --> 00:08:11,909
into platforms so we had to accept that

00:08:10,380 --> 00:08:14,280
telemetry we had to validate and store

00:08:11,909 --> 00:08:16,349
it we have Nessa made a peak load of

00:08:14,280 --> 00:08:17,490
about a billion records per month we

00:08:16,349 --> 00:08:18,900
don't actually have all that many

00:08:17,490 --> 00:08:21,349
sensors like in the scheme of things

00:08:18,900 --> 00:08:23,460
we're talking about 150 to 200 sensors

00:08:21,349 --> 00:08:26,819
but these sensors are often sending up

00:08:23,460 --> 00:08:30,030
records at a half-second frequency so it

00:08:26,819 --> 00:08:31,349
all adds up losing data is a really big

00:08:30,030 --> 00:08:36,240
deal because as I mentioned before we

00:08:31,349 --> 00:08:39,089
can get audited so if the if you know if

00:08:36,240 --> 00:08:41,070
we shake the fish or if something goes

00:08:39,089 --> 00:08:43,649
wrong like say we've got in other sites

00:08:41,070 --> 00:08:45,360
we have apartment blocks and there are

00:08:43,649 --> 00:08:47,940
contractual arrangements there so that

00:08:45,360 --> 00:08:50,339
if we have noise events on a certain

00:08:47,940 --> 00:08:52,199
number of days within a six month period

00:08:50,339 --> 00:08:53,940
then we have to relocate everyone in

00:08:52,199 --> 00:08:54,510
that apartment block so there is real

00:08:53,940 --> 00:08:58,160
money

00:08:54,510 --> 00:09:00,270
at stake and we have to have that record

00:08:58,160 --> 00:09:01,620
of every piece of Camaro that come

00:09:00,270 --> 00:09:04,080
through and if we miss stuff it's a big

00:09:01,620 --> 00:09:05,670
deal we have to analyze and process it

00:09:04,080 --> 00:09:07,050
that's a limitary as I said before the

00:09:05,670 --> 00:09:09,180
environmental requirements don't map

00:09:07,050 --> 00:09:11,040
directly onto sensor data points so we

00:09:09,180 --> 00:09:13,620
have to have some sort of calculation

00:09:11,040 --> 00:09:15,060
engine to make that work the

00:09:13,620 --> 00:09:18,210
calculations are also complex and

00:09:15,060 --> 00:09:19,560
they're liable to change they often come

00:09:18,210 --> 00:09:20,760
up with baseline readings at the start

00:09:19,560 --> 00:09:24,030
of the project and those baselines are

00:09:20,760 --> 00:09:25,410
incorrect the the calculations might not

00:09:24,030 --> 00:09:28,490
be quite right

00:09:25,410 --> 00:09:30,630
that environmental performance thing

00:09:28,490 --> 00:09:32,370
registered environmental guidelines

00:09:30,630 --> 00:09:34,500
document has been updated several times

00:09:32,370 --> 00:09:35,760
with new calculations so we have to have

00:09:34,500 --> 00:09:39,080
an engine where we can handle that

00:09:35,760 --> 00:09:41,160
easily and also retrospectively

00:09:39,080 --> 00:09:43,830
they'll say also this happened in real

00:09:41,160 --> 00:09:45,900
time because we want to be able to send

00:09:43,830 --> 00:09:48,150
near real-time we're not working at

00:09:45,900 --> 00:09:49,650
sub-second accuracy we're talking sort

00:09:48,150 --> 00:09:52,260
of minutes like you know we want to know

00:09:49,650 --> 00:09:54,300
roughly when something happens we have

00:09:52,260 --> 00:09:55,890
to provide access to that data so that

00:09:54,300 --> 00:09:57,570
should be available firstly to our

00:09:55,890 --> 00:09:58,800
environmental teams so they can see if

00:09:57,570 --> 00:10:02,280
anything's going wrong and report to the

00:09:58,800 --> 00:10:05,040
site supervisors but our external data

00:10:02,280 --> 00:10:06,630
stakeholders also need access like the

00:10:05,040 --> 00:10:08,370
University of Melbourne and the

00:10:06,630 --> 00:10:09,690
Comprehensive Cancer Center they

00:10:08,370 --> 00:10:12,960
wouldn't actually let us start work

00:10:09,690 --> 00:10:15,390
until they had access to these sensors

00:10:12,960 --> 00:10:18,120
in near real-time so they could verify

00:10:15,390 --> 00:10:20,340
for themselves that we weren't causing

00:10:18,120 --> 00:10:22,020
any problems lastly we need to send

00:10:20,340 --> 00:10:24,180
alerts so at the very least we need to

00:10:22,020 --> 00:10:25,530
send someone an email whenever an EPR or

00:10:24,180 --> 00:10:28,350
environmental performance requirement is

00:10:25,530 --> 00:10:29,340
breached and those reports that were

00:10:28,350 --> 00:10:30,930
sending at the end of the month they

00:10:29,340 --> 00:10:33,510
still need to go out so we need to be

00:10:30,930 --> 00:10:37,080
generating PDFs with all of that

00:10:33,510 --> 00:10:38,640
telemetry included every alert that

00:10:37,080 --> 00:10:40,200
happens in our system so every time that

00:10:38,640 --> 00:10:42,540
we've gone over the one of the magic

00:10:40,200 --> 00:10:43,920
lines we need to document that and a

00:10:42,540 --> 00:10:47,490
team member from the environmental team

00:10:43,920 --> 00:10:50,490
needs to actually go in and say yeah

00:10:47,490 --> 00:10:52,260
okay we figured out what this was a bird

00:10:50,490 --> 00:10:54,060
landed on the microphone or no we

00:10:52,260 --> 00:10:55,890
weren't working at this time or actually

00:10:54,060 --> 00:10:57,150
yeah this was a planned exceedance we

00:10:55,890 --> 00:11:00,090
had prior arrangements with the

00:10:57,150 --> 00:11:01,590
stakeholders etc the time frame for

00:11:00,090 --> 00:11:05,250
building this was four months to the

00:11:01,590 --> 00:11:07,110
full first release so this started we

00:11:05,250 --> 00:11:08,490
had our first kickoff meeting at the

00:11:07,110 --> 00:11:12,269
very end of 2017

00:11:08,490 --> 00:11:15,779
and our first release was in sort of

00:11:12,269 --> 00:11:17,639
April May of 2018 that was a hard

00:11:15,779 --> 00:11:18,629
deadline as we said the various

00:11:17,639 --> 00:11:20,579
stakeholders wouldn't actually let us

00:11:18,629 --> 00:11:22,529
begin certain works until we had access

00:11:20,579 --> 00:11:24,569
to sensor data after that four-month

00:11:22,529 --> 00:11:26,670
build we had six more months to reach

00:11:24,569 --> 00:11:27,899
feature complete which is sort of having

00:11:26,670 --> 00:11:30,569
all the reporting requirements every

00:11:27,899 --> 00:11:32,040
sensor on line that sort of thing the

00:11:30,569 --> 00:11:34,680
team this was actually a really unique

00:11:32,040 --> 00:11:38,420
team so this was a joint development

00:11:34,680 --> 00:11:41,519
between WSP and Eric who are two pretty

00:11:38,420 --> 00:11:45,660
pretty strong competitors we both threw

00:11:41,519 --> 00:11:46,920
very similar work but in the way in the

00:11:45,660 --> 00:11:48,269
engineering world is actually pretty

00:11:46,920 --> 00:11:49,920
common for competitors to be working

00:11:48,269 --> 00:11:52,139
together projects like the Melbourne

00:11:49,920 --> 00:11:53,939
metro tunnel a way bigger than any one

00:11:52,139 --> 00:11:55,589
organization so you end up with

00:11:53,939 --> 00:11:57,899
contractual arrangements where you have

00:11:55,589 --> 00:11:59,100
one two three four in this case I think

00:11:57,899 --> 00:12:00,779
there's something six or seven different

00:11:59,100 --> 00:12:03,360
engineering companies all working on the

00:12:00,779 --> 00:12:04,860
one project together and the contracts

00:12:03,360 --> 00:12:08,430
are actually quite strict in terms of

00:12:04,860 --> 00:12:10,589
how much manpower you can each team can

00:12:08,430 --> 00:12:12,839
allocate to a project and this was no

00:12:10,589 --> 00:12:14,939
different so this was mandated that

00:12:12,839 --> 00:12:16,860
there will be a 50% split in development

00:12:14,939 --> 00:12:18,389
hours between two companies so we

00:12:16,860 --> 00:12:20,639
actually had to merge two development

00:12:18,389 --> 00:12:22,139
teams to make this work overall they

00:12:20,639 --> 00:12:23,819
were probably two to three developers in

00:12:22,139 --> 00:12:25,319
one project manager at any one time but

00:12:23,819 --> 00:12:27,290
the product the team was fairly fluid

00:12:25,319 --> 00:12:29,279
and we had people swapping in and out

00:12:27,290 --> 00:12:31,610
the other interesting thing about this

00:12:29,279 --> 00:12:33,689
was that it was fully self-contained so

00:12:31,610 --> 00:12:35,100
because it was a joint development and

00:12:33,689 --> 00:12:37,559
the code we were writing actually

00:12:35,100 --> 00:12:39,839
wouldn't belong in the end to either of

00:12:37,559 --> 00:12:41,759
our organisations all of our

00:12:39,839 --> 00:12:44,279
infrastructure all of our code had to be

00:12:41,759 --> 00:12:46,589
basically built from scratch setting up

00:12:44,279 --> 00:12:50,879
our own bitbucket accounts setting up

00:12:46,589 --> 00:12:53,670
our own R&D accounts our own AWS

00:12:50,879 --> 00:12:55,529
accounts everything there are references

00:12:53,670 --> 00:12:56,819
to cyp digital all the way through

00:12:55,529 --> 00:12:58,819
because cyp was the name of the

00:12:56,819 --> 00:13:01,649
consortium that we were building it for

00:12:58,819 --> 00:13:03,029
so it was almost felt like deciding a

00:13:01,649 --> 00:13:04,170
start-up at the restart you know we were

00:13:03,029 --> 00:13:06,899
sitting there with the corporate credit

00:13:04,170 --> 00:13:09,029
card making accounts on every services

00:13:06,899 --> 00:13:13,980
year from one password through to

00:13:09,029 --> 00:13:15,749
Atlassian so that gets me to the build

00:13:13,980 --> 00:13:17,699
so what did we actually end up building

00:13:15,749 --> 00:13:18,750
and how did we structure it so what I'm

00:13:17,699 --> 00:13:20,730
gonna go through is a bit of the

00:13:18,750 --> 00:13:21,390
architecture that we came up with I

00:13:20,730 --> 00:13:22,650
don't have

00:13:21,390 --> 00:13:23,490
enough time to go into a really deep

00:13:22,650 --> 00:13:24,930
dive because there are lots of

00:13:23,490 --> 00:13:28,170
components but I just want to pick out a

00:13:24,930 --> 00:13:29,790
few of the interesting parts and the

00:13:28,170 --> 00:13:32,850
parts where we were using different

00:13:29,790 --> 00:13:34,680
parts of the Python ecosystem to make a

00:13:32,850 --> 00:13:38,670
fairly ambitious project actually

00:13:34,680 --> 00:13:42,540
possible so starting off devices in the

00:13:38,670 --> 00:13:46,380
field just a few like a couple of couple

00:13:42,540 --> 00:13:48,210
few sensors picked at random we had to

00:13:46,380 --> 00:13:50,820
write basically a device agent so we're

00:13:48,210 --> 00:13:52,770
talking sort of micro services here the

00:13:50,820 --> 00:13:55,110
contract for a device agent is it must

00:13:52,770 --> 00:13:56,970
grab data from a device either directly

00:13:55,110 --> 00:13:59,520
or indirectly or receive data from a

00:13:56,970 --> 00:14:01,350
device and then forwarded across into up

00:13:59,520 --> 00:14:02,610
into our platform so these are fairly

00:14:01,350 --> 00:14:04,860
self-contained ones I'll talk about

00:14:02,610 --> 00:14:07,020
these in detail a little bit later but

00:14:04,860 --> 00:14:08,820
we have a few different types so you can

00:14:07,020 --> 00:14:11,000
see here I've got API polling ones and

00:14:08,820 --> 00:14:14,130
FTP service I'll describe those in a bit

00:14:11,000 --> 00:14:17,070
those send data into a data pipeline so

00:14:14,130 --> 00:14:19,950
we're using an AWS Kinesis what this

00:14:17,070 --> 00:14:22,620
gives us is a 48-hour buffer so for

00:14:19,950 --> 00:14:24,420
those who don't know AWS Kinesis it's

00:14:22,620 --> 00:14:26,040
essentially a pipeline it has multiple

00:14:24,420 --> 00:14:28,050
people multiple things that push data in

00:14:26,040 --> 00:14:30,540
multiple subscribers can pull data out

00:14:28,050 --> 00:14:33,930
and it keeps a record from the last 48

00:14:30,540 --> 00:14:35,340
hours and it's hugely scalable so if you

00:14:33,930 --> 00:14:37,860
want to have one you can have shards

00:14:35,340 --> 00:14:40,950
that there were hundreds of shards and

00:14:37,860 --> 00:14:42,420
it's completely managed so like managing

00:14:40,950 --> 00:14:44,190
infrastructure was kind of a big deal

00:14:42,420 --> 00:14:46,260
for us because we didn't have a lot of

00:14:44,190 --> 00:14:48,900
time and we didn't have much trust in

00:14:46,260 --> 00:14:51,840
our own code so for things where the

00:14:48,900 --> 00:14:54,150
audit process was like anything in this

00:14:51,840 --> 00:14:55,980
but a pipeline where we could be audited

00:14:54,150 --> 00:14:58,470
later on were like yes please let's use

00:14:55,980 --> 00:14:59,880
managed services as much as we can if we

00:14:58,470 --> 00:15:01,860
were going the full open-source route

00:14:59,880 --> 00:15:05,850
then we could use something like it like

00:15:01,860 --> 00:15:09,120
Kafka would do the same job first stop

00:15:05,850 --> 00:15:12,600
after that data pipeline is auditing so

00:15:09,120 --> 00:15:14,310
we have our audit markup which is a tiny

00:15:12,600 --> 00:15:15,540
tiny script again we don't want to

00:15:14,310 --> 00:15:18,660
inject much of our code into this

00:15:15,540 --> 00:15:22,500
process because code is scary and full

00:15:18,660 --> 00:15:24,060
of bugs so what this autumn markup thing

00:15:22,500 --> 00:15:26,070
does is basically take this data that

00:15:24,060 --> 00:15:27,690
the agents are sent so all that device

00:15:26,070 --> 00:15:28,740
started coming through and just adds a

00:15:27,690 --> 00:15:31,680
little bit of information basically

00:15:28,740 --> 00:15:33,240
wraps it up in a JSON blob gives a

00:15:31,680 --> 00:15:33,810
timestamp of when it saw that piece of

00:15:33,240 --> 00:15:35,160
data

00:15:33,810 --> 00:15:37,170
maybe extracts a couple of

00:15:35,160 --> 00:15:39,920
fields so that later on we can do a

00:15:37,170 --> 00:15:43,829
little bit of analysis on our on our

00:15:39,920 --> 00:15:45,779
auditing pool because after that it

00:15:43,829 --> 00:15:50,579
basically Ford's it straight into AWS s3

00:15:45,779 --> 00:15:52,889
it uses some Kinesis fire hose to do

00:15:50,579 --> 00:15:54,149
that so we have this sort of now you've

00:15:52,889 --> 00:15:55,829
now got this pipeline where I've got

00:15:54,149 --> 00:15:57,540
telemetry coming in from our agents

00:15:55,829 --> 00:15:59,069
going through our pipeline through a

00:15:57,540 --> 00:16:01,019
tiny little audit markups grid which is

00:15:59,069 --> 00:16:03,660
a deployed Anita West lamda and they're

00:16:01,019 --> 00:16:06,240
straight into s3 so that is our safety

00:16:03,660 --> 00:16:08,129
net basically the next step is we have

00:16:06,240 --> 00:16:09,779
streaming analytics are going to listen

00:16:08,129 --> 00:16:10,949
a little bit more detail through my

00:16:09,779 --> 00:16:12,750
analytics is probably the one area where

00:16:10,949 --> 00:16:15,389
we couldn't use Python or we didn't use

00:16:12,750 --> 00:16:17,160
Python we probably could have I'll talk

00:16:15,389 --> 00:16:19,290
about that in a sec that's during

00:16:17,160 --> 00:16:21,170
analytics cluster then dumps all that

00:16:19,290 --> 00:16:24,990
information into a time series database

00:16:21,170 --> 00:16:27,540
we're using influx TV an influx CB has

00:16:24,990 --> 00:16:28,769
been wonderful we like for the for the

00:16:27,540 --> 00:16:30,509
workloads we're experiencing which is

00:16:28,769 --> 00:16:33,870
about 10 percent of that 1 billion so

00:16:30,509 --> 00:16:37,649
far we're running it on a t2 micro

00:16:33,870 --> 00:16:39,240
instance and because we have our data

00:16:37,649 --> 00:16:40,860
pipeline buffer we've got our auditing

00:16:39,240 --> 00:16:43,290
system we have our streaming analytics

00:16:40,860 --> 00:16:45,209
cluster which is capable of having hold

00:16:43,290 --> 00:16:46,139
on data sending we don't even have to

00:16:45,209 --> 00:16:49,019
have that on particularly high

00:16:46,139 --> 00:16:53,130
availability so that that's really it's

00:16:49,019 --> 00:16:57,029
been a really good choice and finally we

00:16:53,130 --> 00:16:58,500
have the analysis and basically the core

00:16:57,029 --> 00:17:01,589
core of the system which is a big

00:16:58,500 --> 00:17:04,199
monolithic Jango app we've got web

00:17:01,589 --> 00:17:05,909
application API workers etc user

00:17:04,199 --> 00:17:10,439
interfaces a user interface is done in

00:17:05,909 --> 00:17:12,659
react and get that that's really where

00:17:10,439 --> 00:17:14,730
most of the action happens one thing

00:17:12,659 --> 00:17:17,370
that's really really important is that

00:17:14,730 --> 00:17:20,549
from steps 1 2 3 & 4 here there is

00:17:17,370 --> 00:17:22,260
actually a hard gap between steps 1 &

00:17:20,549 --> 00:17:24,630
through 4 and then 5 so our core

00:17:22,260 --> 00:17:27,059
platform has a read-only access to that

00:17:24,630 --> 00:17:28,980
time series database and we want to keep

00:17:27,059 --> 00:17:31,620
it that way because then we can work

00:17:28,980 --> 00:17:34,260
freely on this Jango app without ever

00:17:31,620 --> 00:17:36,270
having to worry about losing important

00:17:34,260 --> 00:17:40,110
valuable data that we can beep what I

00:17:36,270 --> 00:17:41,490
think what up in court later on so a

00:17:40,110 --> 00:17:45,059
little bit information about each of our

00:17:41,490 --> 00:17:46,380
components back on that device that on

00:17:45,059 --> 00:17:48,550
that call at that first column with our

00:17:46,380 --> 00:17:50,200
device agents

00:17:48,550 --> 00:17:52,150
have what we call API pullers so those

00:17:50,200 --> 00:17:55,300
vendor SAS platforms usually provide

00:17:52,150 --> 00:17:56,770
some sort of REST API and so we're

00:17:55,300 --> 00:17:59,710
writing some straightforward Python

00:17:56,770 --> 00:18:02,590
scripts to pull that vendor API pull

00:17:59,710 --> 00:18:04,510
data in and send into our system those

00:18:02,590 --> 00:18:06,850
vendor api's aren't always what you'd

00:18:04,510 --> 00:18:08,800
hoped for so for instance we have

00:18:06,850 --> 00:18:11,260
vendors who have given us say we have

00:18:08,800 --> 00:18:12,580
100 devices on their platform each of

00:18:11,260 --> 00:18:14,830
those devices puts up three telemetry

00:18:12,580 --> 00:18:19,120
streams but they're like here's 300

00:18:14,830 --> 00:18:20,200
endpoints for you to pull so that can be

00:18:19,120 --> 00:18:22,000
a bit of a challenge writing Python

00:18:20,200 --> 00:18:25,690
scripts we've done it the simple way to

00:18:22,000 --> 00:18:27,820
start with and we actually deploy the

00:18:25,690 --> 00:18:29,320
script into elastic Beanstalk which is

00:18:27,820 --> 00:18:30,490
usually used for web applications but

00:18:29,320 --> 00:18:33,580
actually does a really good job if

00:18:30,490 --> 00:18:35,470
you're in a hurry you can use using

00:18:33,580 --> 00:18:37,540
their docker environments then it

00:18:35,470 --> 00:18:40,360
provides a really easy well-trodden path

00:18:37,540 --> 00:18:40,750
for deploying scripts and having them at

00:18:40,360 --> 00:18:42,940
least

00:18:40,750 --> 00:18:45,460
this is a basic amount of monitoring and

00:18:42,940 --> 00:18:50,260
be able to strike your environment bring

00:18:45,460 --> 00:18:52,810
it up again really quickly the simple

00:18:50,260 --> 00:18:54,220
state we have most of these api apollo's

00:18:52,810 --> 00:18:57,070
need a bit of state so they need to be

00:18:54,220 --> 00:18:59,080
able to say ok this device this

00:18:57,070 --> 00:19:00,730
telemetry stream the last time stamp we

00:18:59,080 --> 00:19:03,850
saw was this so then if they crashed or

00:19:00,730 --> 00:19:05,950
they go down they can start again from

00:19:03,850 --> 00:19:07,420
when they left off we use AWS dynamodb

00:19:05,950 --> 00:19:10,230
for that just a simple key value store

00:19:07,420 --> 00:19:13,030
saying this device this timestamp

00:19:10,230 --> 00:19:14,620
as I mentioned before we had to pull a

00:19:13,030 --> 00:19:17,200
lot of endpoints so they're heavily

00:19:14,620 --> 00:19:19,540
multi-threaded this would be a really

00:19:17,200 --> 00:19:21,430
good candidate for async IO and IO HTTP

00:19:19,540 --> 00:19:23,200
at some point so that we don't have to

00:19:21,430 --> 00:19:24,460
have ridiculous numbers of threads if we

00:19:23,200 --> 00:19:27,970
end up with four thousand threads

00:19:24,460 --> 00:19:29,950
pulling then you know besides the fact

00:19:27,970 --> 00:19:32,800
that we're kind of ddossing us a splice

00:19:29,950 --> 00:19:37,660
offender we're also using out way too

00:19:32,800 --> 00:19:39,220
much ram so yeah we've also got FTP so

00:19:37,660 --> 00:19:41,800
this is actually one area which I think

00:19:39,220 --> 00:19:45,820
that web Python really helped us out is

00:19:41,800 --> 00:19:47,140
PI F TBD Lib don't know how many people

00:19:45,820 --> 00:19:49,180
have had that your chance to look at

00:19:47,140 --> 00:19:50,890
this but it's a really really nice FTP

00:19:49,180 --> 00:19:54,100
so the traditional way of doing a device

00:19:50,890 --> 00:19:57,100
agent like this is you bring up a lining

00:19:54,100 --> 00:20:00,640
server install you do your app get

00:19:57,100 --> 00:20:02,320
install FTP whatever and then you have

00:20:00,640 --> 00:20:04,960
FTP server dumping files into

00:20:02,320 --> 00:20:06,190
location and the polling script that's

00:20:04,960 --> 00:20:07,570
looking at those files and pulling them

00:20:06,190 --> 00:20:11,440
out but we want to do something a little

00:20:07,570 --> 00:20:13,809
bit more modern so we actually wrote a

00:20:11,440 --> 00:20:17,799
we use PI F TBD Lib to write our own

00:20:13,809 --> 00:20:19,509
server and that server has instant

00:20:17,799 --> 00:20:21,190
processing of incoming files where our

00:20:19,509 --> 00:20:24,250
thing called event callbacks they look a

00:20:21,190 --> 00:20:25,720
bit like this so you can write your FTP

00:20:24,250 --> 00:20:28,539
handler register that with your server

00:20:25,720 --> 00:20:30,159
and then all you need to do is write a

00:20:28,539 --> 00:20:32,409
little function called on file received

00:20:30,159 --> 00:20:36,159
and whenever a filer is received you can

00:20:32,409 --> 00:20:38,139
do something with it so in this case we

00:20:36,159 --> 00:20:40,539
have a Python agent that just sits there

00:20:38,139 --> 00:20:42,730
and runs and accepts FTP connections and

00:20:40,539 --> 00:20:44,379
as soon as a file arrives so basically

00:20:42,730 --> 00:20:47,860
in real time as soon as we get that data

00:20:44,379 --> 00:20:50,139
from the server we can pass that file

00:20:47,860 --> 00:20:51,549
extract the telemetry out of it

00:20:50,139 --> 00:20:55,320
and then upload that and push it into

00:20:51,549 --> 00:20:57,250
the pipeline we're also able to do some

00:20:55,320 --> 00:20:58,750
slightly creative things we really

00:20:57,250 --> 00:21:00,759
wanted these to be load balanced because

00:20:58,750 --> 00:21:02,470
some of these devices will just send you

00:21:00,759 --> 00:21:04,960
a file and then assume that you've got

00:21:02,470 --> 00:21:06,669
it even if it failed so again losing

00:21:04,960 --> 00:21:09,669
data as a big issue so we wanted to have

00:21:06,669 --> 00:21:11,799
some sort of TCP load balancing and this

00:21:09,669 --> 00:21:14,769
is actually theoretically possible and

00:21:11,799 --> 00:21:16,570
we got some version of it working if you

00:21:14,769 --> 00:21:18,730
know about FTP protocol you'll know that

00:21:16,570 --> 00:21:22,179
it does not react kindly to load

00:21:18,730 --> 00:21:23,649
balancing but it is theoretically

00:21:22,179 --> 00:21:27,820
possible and we got it working with

00:21:23,649 --> 00:21:28,809
elastic Beanstalk and TCP proxying asked

00:21:27,820 --> 00:21:31,360
me about that letter and the whole way

00:21:28,809 --> 00:21:36,370
track if you're interested in adapting

00:21:31,360 --> 00:21:38,409
technologies from the 70s so we had a

00:21:36,370 --> 00:21:40,120
streaming analyst cluster the goal the

00:21:38,409 --> 00:21:43,450
idea of the streaming analytics cluster

00:21:40,120 --> 00:21:45,190
was that it was going to be this all

00:21:43,450 --> 00:21:47,289
like we knew we wanted some sort of like

00:21:45,190 --> 00:21:49,090
real-time stream processing things so he

00:21:47,289 --> 00:21:51,580
could do really complex aggregates of

00:21:49,090 --> 00:21:53,409
telemetry coming in and we did a heap of

00:21:51,580 --> 00:21:55,360
research weeks have researched on and

00:21:53,409 --> 00:21:56,799
off to to figure out what the best tool

00:21:55,360 --> 00:21:59,740
for the job was and we settled on Apache

00:21:56,799 --> 00:22:01,480
flink and apache flink is an amazing

00:21:59,740 --> 00:22:03,399
piece of technology for real-time stream

00:22:01,480 --> 00:22:06,210
processing that's what Alibaba uses to

00:22:03,399 --> 00:22:08,950
process their real-time sales workflows

00:22:06,210 --> 00:22:11,500
it's also a huge industrial piece of

00:22:08,950 --> 00:22:15,250
machinery where really what we needed

00:22:11,500 --> 00:22:16,780
was a desktop printer and we had to

00:22:15,250 --> 00:22:19,870
write in Scala

00:22:16,780 --> 00:22:21,070
which was disappointing Apache flag

00:22:19,870 --> 00:22:24,880
actually used to have Python bindings

00:22:21,070 --> 00:22:26,860
but no longer does but that is currently

00:22:24,880 --> 00:22:29,830
performing some reasonably complex

00:22:26,860 --> 00:22:31,210
validation of telemetry so we had a

00:22:29,830 --> 00:22:33,190
vision of using I'll describe later

00:22:31,210 --> 00:22:36,520
while we haven't done our analysis in

00:22:33,190 --> 00:22:38,440
flink but currently is doing a bit of

00:22:36,520 --> 00:22:39,880
validation it basically make sure that

00:22:38,440 --> 00:22:43,480
everything coming through our stream is

00:22:39,880 --> 00:22:46,780
correct and then forwards out into our

00:22:43,480 --> 00:22:48,430
time series database what it really does

00:22:46,780 --> 00:22:49,930
do a good job at and what would have

00:22:48,430 --> 00:22:52,510
been a pain to write in Python if we

00:22:49,930 --> 00:22:54,730
hadn't used it is essentially buffering

00:22:52,510 --> 00:22:58,690
checkpointing error handling like let's

00:22:54,730 --> 00:23:00,490
say our influx DB goes down and we then

00:22:58,690 --> 00:23:02,650
have to basically spin our wheels while

00:23:00,490 --> 00:23:03,970
we wait for it to come back up or for

00:23:02,650 --> 00:23:07,030
instance we want to upload so new

00:23:03,970 --> 00:23:09,250
telemetry validation rules and we have

00:23:07,030 --> 00:23:11,770
to somehow take that that stream

00:23:09,250 --> 00:23:15,070
processing down take a save point where

00:23:11,770 --> 00:23:16,810
we left off and then deploy the new

00:23:15,070 --> 00:23:21,070
version and then start from the that

00:23:16,810 --> 00:23:23,560
same save point link does an amazing job

00:23:21,070 --> 00:23:25,750
of that so as I said this is overkill

00:23:23,560 --> 00:23:28,300
but the OU vehicle was actually mostly

00:23:25,750 --> 00:23:32,710
around the DevOps side as it was about

00:23:28,300 --> 00:23:35,140
deploying that cluster we would actually

00:23:32,710 --> 00:23:36,790
probably still use it because AWS has

00:23:35,140 --> 00:23:39,250
just announced a new product called

00:23:36,790 --> 00:23:42,280
Connexus streaming analytics which is a

00:23:39,250 --> 00:23:44,830
fully managed Apache flink service which

00:23:42,280 --> 00:23:46,270
is quite cheap and easy to use so we

00:23:44,830 --> 00:23:48,460
were actually beacon strongly considered

00:23:46,270 --> 00:23:51,820
using that again even though we sort of

00:23:48,460 --> 00:23:53,800
decided not to in the end so finally we

00:23:51,820 --> 00:23:57,040
have our web application being

00:23:53,800 --> 00:23:58,540
monolithic Django celery react

00:23:57,040 --> 00:24:00,160
this has celery workers in the

00:23:58,540 --> 00:24:02,170
background passing through all the data

00:24:00,160 --> 00:24:04,360
in the time series database looking for

00:24:02,170 --> 00:24:07,180
alerts the alerting rules are set up via

00:24:04,360 --> 00:24:09,580
Django admin it's the absolute core of

00:24:07,180 --> 00:24:10,960
the system and as I mentioned before

00:24:09,580 --> 00:24:12,520
it's purposely disconnected from the

00:24:10,960 --> 00:24:14,530
telemetry collection because we want to

00:24:12,520 --> 00:24:15,790
be able to basically to our write our

00:24:14,530 --> 00:24:19,510
own buggy code this is where all of our

00:24:15,790 --> 00:24:21,010
buggy code lives and we don't want that

00:24:19,510 --> 00:24:24,610
to have anything to do with the stuff we

00:24:21,010 --> 00:24:25,900
can get ordered on so yeah we disconnect

00:24:24,610 --> 00:24:28,390
that from that telemetry collection

00:24:25,900 --> 00:24:29,710
process the most interesting thing about

00:24:28,390 --> 00:24:29,980
this I'm just going to focus on this one

00:24:29,710 --> 00:24:30,970
thing

00:24:29,980 --> 00:24:34,690
about this cliff only got five minutes

00:24:30,970 --> 00:24:36,490
left is that this is really strongly

00:24:34,690 --> 00:24:37,780
powered by pandas and the path and

00:24:36,490 --> 00:24:40,780
scientific toolkit

00:24:37,780 --> 00:24:42,010
so why pandas if you remember that our

00:24:40,780 --> 00:24:44,410
reporting requirements don't map

00:24:42,010 --> 00:24:47,080
directly to raw sensor data pandas lets

00:24:44,410 --> 00:24:50,410
us transform raw telemetry live on

00:24:47,080 --> 00:24:53,020
request so when we combine we combine

00:24:50,410 --> 00:24:55,690
pandas with a good time series database

00:24:53,020 --> 00:24:57,490
like in flux DB we can easily load huge

00:24:55,690 --> 00:24:59,410
amounts of data up to about we've tested

00:24:57,490 --> 00:25:02,350
up to about ten thousand records in one

00:24:59,410 --> 00:25:03,820
data frame we can obviously much more

00:25:02,350 --> 00:25:06,940
than that but maybe not within a web

00:25:03,820 --> 00:25:08,380
request we can perform like a series of

00:25:06,940 --> 00:25:10,030
complex calculations we actually have a

00:25:08,380 --> 00:25:12,760
full taxonomy of different telemetry

00:25:10,030 --> 00:25:14,950
types that go from your raw planetary at

00:25:12,760 --> 00:25:16,540
the bottom all the way up to to the

00:25:14,950 --> 00:25:19,750
calculated telemetry several layers on

00:25:16,540 --> 00:25:22,720
top of that and we do that by cycling

00:25:19,750 --> 00:25:25,450
through data frames all the way up and

00:25:22,720 --> 00:25:27,220
we can do incredibly fast with pandas it

00:25:25,450 --> 00:25:28,660
doesn't fully solve every problem life

00:25:27,220 --> 00:25:30,250
for instance that half second telemetry

00:25:28,660 --> 00:25:31,990
we're getting we only get actually a few

00:25:30,250 --> 00:25:35,260
days of data if we ran a year 10,000

00:25:31,990 --> 00:25:36,520
points but we can do additional roll-ups

00:25:35,260 --> 00:25:38,410
elsewhere in our platform we can

00:25:36,520 --> 00:25:40,750
basically pick and choose where we do

00:25:38,410 --> 00:25:42,130
our complex roll-ups in the case of that

00:25:40,750 --> 00:25:43,270
half second telemetry actually get the

00:25:42,130 --> 00:25:44,740
device agents to do it themselves

00:25:43,270 --> 00:25:47,020
because they get their data in one

00:25:44,740 --> 00:25:48,910
minute chunks of half second values so

00:25:47,020 --> 00:25:51,840
they just do that telemetry roll up

00:25:48,910 --> 00:25:55,630
themselves and then we main platform can

00:25:51,840 --> 00:25:57,100
continue on so this approach was

00:25:55,630 --> 00:25:58,390
probably the biggest single time server

00:25:57,100 --> 00:25:59,799
on the project and it's probably the

00:25:58,390 --> 00:26:01,900
biggest takeaway is that you can

00:25:59,799 --> 00:26:04,240
actually do a lot of this calculation

00:26:01,900 --> 00:26:07,390
live when you go and look at io T

00:26:04,240 --> 00:26:09,250
platforms online and try and find your

00:26:07,390 --> 00:26:10,900
best practices you'll get a lot of

00:26:09,250 --> 00:26:14,530
information about how to perform data

00:26:10,900 --> 00:26:16,360
based roll-ups and and how to set up

00:26:14,530 --> 00:26:19,660
streaming analytics classes like flink

00:26:16,360 --> 00:26:21,820
but if you don't ever have to load more

00:26:19,660 --> 00:26:26,530
than say a few days or a few weeks worth

00:26:21,820 --> 00:26:28,660
of data for a graph then you can get

00:26:26,530 --> 00:26:30,130
away with just loading that raw stuff in

00:26:28,660 --> 00:26:32,890
passing it through some data frames

00:26:30,130 --> 00:26:36,429
returning it and you will get almost all

00:26:32,890 --> 00:26:39,070
the way there so I'll quickly cycle

00:26:36,429 --> 00:26:40,600
through the results so we've have

00:26:39,070 --> 00:26:42,820
currently have sensors deployed at all

00:26:40,600 --> 00:26:43,820
of these locations this is the part fuel

00:26:42,820 --> 00:26:47,059
section

00:26:43,820 --> 00:26:48,919
Town Hall station and annexation

00:26:47,059 --> 00:26:50,239
annexation is the one of the residential

00:26:48,919 --> 00:26:56,269
areas so I've got lots of apartment

00:26:50,239 --> 00:26:58,879
buildings this is our login screen we

00:26:56,269 --> 00:27:02,539
have a kind of an overview of the tunnel

00:26:58,879 --> 00:27:03,830
site so you can see all of our sensitive

00:27:02,539 --> 00:27:05,149
locations through that and that's a bit

00:27:03,830 --> 00:27:08,809
of an older image so we've got a few

00:27:05,149 --> 00:27:10,190
very few more sensors there now a little

00:27:08,809 --> 00:27:13,399
preview of the telemetry from the device

00:27:10,190 --> 00:27:16,190
we can then dig into a device we see

00:27:13,399 --> 00:27:17,869
here got a noise monitor near one of the

00:27:16,190 --> 00:27:19,340
apartment buildings we can see that

00:27:17,869 --> 00:27:21,349
device there there are little red lines

00:27:19,340 --> 00:27:24,379
we can see that that are alerting traces

00:27:21,349 --> 00:27:25,909
so the alerts in the apartment case of

00:27:24,379 --> 00:27:29,119
apartment blocks only work within

00:27:25,909 --> 00:27:32,929
nighttime hours so see those alerts are

00:27:29,119 --> 00:27:34,700
set up over night and they change

00:27:32,929 --> 00:27:39,470
depending on 200 on the day a week as

00:27:34,700 --> 00:27:41,599
well this is an example of a sensor

00:27:39,470 --> 00:27:43,039
where we're actually doing a bit of

00:27:41,599 --> 00:27:45,200
calculation

00:27:43,039 --> 00:27:47,929
Vesey curve classifications what that

00:27:45,200 --> 00:27:50,629
says on the left-hand axis those letters

00:27:47,929 --> 00:27:52,210
e d c b a NOP each correspond to

00:27:50,629 --> 00:27:55,039
different activities within a hospital

00:27:52,210 --> 00:27:56,299
that can occur safely at that level of

00:27:55,039 --> 00:27:59,029
vibration so there's actually a

00:27:56,299 --> 00:28:01,849
calculation based on blonde vibration

00:27:59,029 --> 00:28:03,529
velocity curves and he means you can use

00:28:01,849 --> 00:28:05,779
an electron microscopy means that you

00:28:03,529 --> 00:28:09,019
can conduct operating of things in an

00:28:05,779 --> 00:28:11,239
operating theatre we have our alerts so

00:28:09,019 --> 00:28:13,519
that's an alerting tray succeed that's

00:28:11,239 --> 00:28:17,479
been exceeded at 22 past midnight and

00:28:13,519 --> 00:28:19,970
then came back at 26 past about

00:28:17,479 --> 00:28:21,470
dashboards so you can see we've got our

00:28:19,970 --> 00:28:23,179
device health monitoring we know roughly

00:28:21,470 --> 00:28:25,999
how much data reckon a device can

00:28:23,179 --> 00:28:28,309
collect our alerts changes of alerts

00:28:25,999 --> 00:28:29,720
different categories etc and the

00:28:28,309 --> 00:28:31,879
resolutions there on the right so that's

00:28:29,720 --> 00:28:33,229
our engineers going and saying okay we

00:28:31,879 --> 00:28:35,139
plan to exceed here or no they will

00:28:33,229 --> 00:28:39,739
notice he epic CIP works at this time

00:28:35,139 --> 00:28:41,690
our PDF reports Django admin backends

00:28:39,739 --> 00:28:43,729
look familiar

00:28:41,690 --> 00:28:45,049
status pages we have a heartbeat thing

00:28:43,729 --> 00:28:49,220
so we can actually always see when I

00:28:45,049 --> 00:28:50,509
symmetry is is going it is flowing

00:28:49,220 --> 00:28:53,929
through it's because that heartbeat will

00:28:50,509 --> 00:28:55,549
say it will stop we even have some help

00:28:53,929 --> 00:28:56,950
pages so this is our alerting real page

00:28:55,549 --> 00:28:58,179
we have 100

00:28:56,950 --> 00:28:59,080
of different learning rules set up

00:28:58,179 --> 00:29:01,200
across for all those different

00:28:59,080 --> 00:29:06,630
environmental performance requirements

00:29:01,200 --> 00:29:06,630
and I think that's my talk thank you

00:29:12,360 --> 00:29:17,230
Thank You Epping a year and here's your

00:29:15,640 --> 00:29:19,100
lovely milk thank you very much can I

00:29:17,230 --> 00:29:22,880
get everybody to thank Kevin again

00:29:19,100 --> 00:29:22,880

YouTube URL: https://www.youtube.com/watch?v=aeZOVaULoNI


