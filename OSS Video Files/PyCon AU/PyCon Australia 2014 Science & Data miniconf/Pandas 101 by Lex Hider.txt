Title: Pandas 101 by Lex Hider
Publication date: 2014-08-11
Playlist: PyCon Australia 2014 Science & Data miniconf
Description: 
	PyCon Australia is the national conference for users of the Python Programming Language. In August 2014, we're heading to Brisbane to bring together students, enthusiasts, and professionals with a love of Python from around Australia, and all around the World. 

August 1-5, Brisbane, Queensland, Australia
Captions: 
	00:00:06,129 --> 00:00:10,789
okay let's get started so this is more

00:00:09,410 --> 00:00:12,500
of a sort of a cheat royal style talked

00:00:10,789 --> 00:00:14,299
and sort of a lot of slides so it's just

00:00:12,500 --> 00:00:17,510
going to step you through some code

00:00:14,299 --> 00:00:19,670
doing some things with pandas now the

00:00:17,510 --> 00:00:20,810
slides are available online so if

00:00:19,670 --> 00:00:23,990
everyone's got their laptop I'd

00:00:20,810 --> 00:00:25,700
recommend following along and you can

00:00:23,990 --> 00:00:29,230
download it later and sort of play with

00:00:25,700 --> 00:00:29,230
the data sets that I'm playing with here

00:00:30,279 --> 00:00:35,660
so yeah I'm going to load up two data

00:00:33,559 --> 00:00:37,340
sets and go through them so the first

00:00:35,660 --> 00:00:40,280
one is from weather data the next is

00:00:37,340 --> 00:00:43,070
some Brisbane beer data that I've hacked

00:00:40,280 --> 00:00:48,079
together yeah i'm just going to take you

00:00:43,070 --> 00:00:52,309
through those and so just to gauge the

00:00:48,079 --> 00:00:55,270
level of the audience here who has used

00:00:52,309 --> 00:01:00,469
penis before today I could presence okay

00:00:55,270 --> 00:01:01,879
and I plus on notebook okay and and who

00:01:00,469 --> 00:01:06,890
is it the first talked this morning with

00:01:01,879 --> 00:01:09,619
Arthur okay so this is focus people for

00:01:06,890 --> 00:01:11,390
the people who aren't experts with

00:01:09,619 --> 00:01:12,740
panners but I think you'll still

00:01:11,390 --> 00:01:19,100
probably go a few things out of it if

00:01:12,740 --> 00:01:22,040
you are a bit more experienced okay so

00:01:19,100 --> 00:01:23,229
first thing we need to do is to install

00:01:22,040 --> 00:01:28,130
pandas if we're going to play with it

00:01:23,229 --> 00:01:29,780
and my first tip is that if you haven't

00:01:28,130 --> 00:01:31,100
got it installed and you must play with

00:01:29,780 --> 00:01:34,250
it the first time you don't need to

00:01:31,100 --> 00:01:37,939
install it there's this web app called

00:01:34,250 --> 00:01:41,240
wakari it's I applied the notebook

00:01:37,939 --> 00:01:43,000
closely in the cloud for free and you

00:01:41,240 --> 00:01:45,020
know if you if you want to play with a

00:01:43,000 --> 00:01:47,140
panda's for the first time that would be

00:01:45,020 --> 00:01:50,450
my recommendation of how to play with it

00:01:47,140 --> 00:01:55,630
this is what it looks like so this is I

00:01:50,450 --> 00:01:55,630
pasta in the in the cloud I can you know

00:01:55,689 --> 00:02:04,189
make changes and from connected to the

00:01:59,299 --> 00:02:08,200
network here but anyway you know we can

00:02:04,189 --> 00:02:08,200
do our analysis without installation

00:02:10,050 --> 00:02:17,530
once you're familiar the best way to

00:02:13,120 --> 00:02:19,270
install it I find is anaconda this is a

00:02:17,530 --> 00:02:22,030
distribution that gives you a Python a

00:02:19,270 --> 00:02:23,890
full supply stack including pandas it

00:02:22,030 --> 00:02:26,650
works on linux that works on mac it

00:02:23,890 --> 00:02:32,350
works on windows it's a single installer

00:02:26,650 --> 00:02:34,239
it's really easy I've just put a patch

00:02:32,350 --> 00:02:38,140
through on the docks for pandas this

00:02:34,239 --> 00:02:40,570
week actually which is recommending

00:02:38,140 --> 00:02:41,950
that's the way to go so it's not part of

00:02:40,570 --> 00:02:44,590
the official docs yet but if you follow

00:02:41,950 --> 00:02:50,860
that link you'll that'll tell you how to

00:02:44,590 --> 00:02:55,480
go about it so briefly way why is it

00:02:50,860 --> 00:02:57,040
should we be using pandas pandas is

00:02:55,480 --> 00:03:02,290
really pythons answer to are the very

00:02:57,040 --> 00:03:03,640
similar very high performance and really

00:03:02,290 --> 00:03:09,850
the way to think of it is it to any

00:03:03,640 --> 00:03:12,579
memory sequel or excel on steroids and

00:03:09,850 --> 00:03:14,290
it's got stacks of useful features it's

00:03:12,579 --> 00:03:17,079
really easy to be plotting really

00:03:14,290 --> 00:03:20,920
flexible group buys and it's built on

00:03:17,079 --> 00:03:22,510
top of none player so it's very fast its

00:03:20,920 --> 00:03:25,180
memory fishing a lot of the time you're

00:03:22,510 --> 00:03:30,070
calling out to see your sight line code

00:03:25,180 --> 00:03:35,739
and performance has really been a really

00:03:30,070 --> 00:03:36,730
key part of the development of pandas so

00:03:35,739 --> 00:03:39,160
I'm going to start looking at this

00:03:36,730 --> 00:03:41,670
weather data so the step one is to load

00:03:39,160 --> 00:03:41,670
data

00:03:43,190 --> 00:03:50,600
and so I just went to the bottom website

00:03:47,450 --> 00:03:52,010
this is Mel burns recent weather

00:03:50,600 --> 00:03:55,220
observations it gives it to you every

00:03:52,010 --> 00:03:57,800
half an hour I flew up from Melbourne

00:03:55,220 --> 00:04:02,180
last night and got hit by a massive

00:03:57,800 --> 00:04:04,100
storm and was an hour and a half driving

00:04:02,180 --> 00:04:06,230
to the airport and delayed flight which

00:04:04,100 --> 00:04:13,010
was very fun so you might get some

00:04:06,230 --> 00:04:15,830
insights out of that so Candace is

00:04:13,010 --> 00:04:17,570
really flexible some convenient ways of

00:04:15,830 --> 00:04:20,359
reading in your data if it's either see

00:04:17,570 --> 00:04:22,340
spell CSP that's really easy you can

00:04:20,359 --> 00:04:25,190
give it sequel queries or sequel tables

00:04:22,340 --> 00:04:30,890
it can read in JSON for you you can read

00:04:25,190 --> 00:04:31,910
in your excel google nexus hdf5 there's

00:04:30,890 --> 00:04:39,410
a stack more that I probably have

00:04:31,910 --> 00:04:42,440
mustard but it makes it really easy so

00:04:39,410 --> 00:04:46,430
this is your alpha our CSV data that

00:04:42,440 --> 00:04:49,100
we're going to try and load up and we

00:04:46,430 --> 00:04:50,530
call the red CSV function and we can see

00:04:49,100 --> 00:04:55,490
that that you can actually work

00:04:50,530 --> 00:04:58,370
something went wrong so let's have a

00:04:55,490 --> 00:05:00,500
look at the raw data we have a look here

00:04:58,370 --> 00:05:01,700
we can see that there's actually 20 rows

00:05:00,500 --> 00:05:03,050
of junk at the start that we're not

00:05:01,700 --> 00:05:06,110
really interested in and it's not till

00:05:03,050 --> 00:05:11,480
road 19 that our data starts looking

00:05:06,110 --> 00:05:14,900
like a CSV file so we call read CSP we

00:05:11,480 --> 00:05:18,530
tilt to skip the first 19 rows and now

00:05:14,900 --> 00:05:20,419
we have a data frame so data frame is

00:05:18,530 --> 00:05:22,490
really the key data structure that the

00:05:20,419 --> 00:05:27,950
penance gives you and it's a table with

00:05:22,490 --> 00:05:30,800
you know columns and indexes each each

00:05:27,950 --> 00:05:35,830
column is a is a series and we have the

00:05:30,800 --> 00:05:39,040
index here which is allows us to access

00:05:35,830 --> 00:05:39,040
individual rows

00:05:41,160 --> 00:05:46,120
so one way to think of a data frame is

00:05:43,450 --> 00:05:50,710
like a dictionary a dictionary of

00:05:46,120 --> 00:05:52,120
columns and so we can treat it like a

00:05:50,710 --> 00:05:56,140
dictionary we can delete columns that

00:05:52,120 --> 00:05:58,360
we're not interested in and if we try

00:05:56,140 --> 00:06:01,990
and access a column we get back a column

00:05:58,360 --> 00:06:06,240
so this is the air temperature and again

00:06:01,990 --> 00:06:06,240
we have an index and we have the values

00:06:08,700 --> 00:06:14,530
so because it's built on top of numpy

00:06:10,930 --> 00:06:17,160
doing arithmetic and math is really fast

00:06:14,530 --> 00:06:20,350
and it's really descriptive descriptive

00:06:17,160 --> 00:06:22,300
and you should never really find

00:06:20,350 --> 00:06:23,860
yourself looping through through some

00:06:22,300 --> 00:06:24,910
columns and series that's you're

00:06:23,860 --> 00:06:28,090
probably doing it wrong if you're doing

00:06:24,910 --> 00:06:29,290
it that way if we have any Americans in

00:06:28,090 --> 00:06:31,900
the audience but I thought a simple

00:06:29,290 --> 00:06:36,330
example would be to move it from Celsius

00:06:31,900 --> 00:06:42,100
to Fahrenheit so one line of code and

00:06:36,330 --> 00:06:43,720
it's that easy and so as an example if

00:06:42,100 --> 00:06:46,570
our task was simply they grab up the

00:06:43,720 --> 00:06:49,570
data and vert it to Fahrenheit write it

00:06:46,570 --> 00:06:53,979
back to disk as a CSV three lines of

00:06:49,570 --> 00:06:58,360
code we've got the to CSV one which is

00:06:53,979 --> 00:06:59,320
you know the opposite of Reese esv and

00:06:58,360 --> 00:07:01,810
so if you ever find yourself using

00:06:59,320 --> 00:07:04,630
import CSV you're probably much it

00:07:01,810 --> 00:07:10,690
better to be using pandas I think in

00:07:04,630 --> 00:07:12,820
almost any case so it's loaded the data

00:07:10,690 --> 00:07:16,240
into memory now we need to clean our

00:07:12,820 --> 00:07:17,740
data and the unwritten rule of data

00:07:16,240 --> 00:07:20,410
analysis really this is where you gonna

00:07:17,740 --> 00:07:25,360
spend most of your time getting it

00:07:20,410 --> 00:07:26,590
looking nice and how you want it so if

00:07:25,360 --> 00:07:28,030
we look at the data the first team all

00:07:26,590 --> 00:07:32,080
notices we've got these weird column

00:07:28,030 --> 00:07:33,610
names with the 80 at the end don't know

00:07:32,080 --> 00:07:36,060
what the deal is with that but let's fix

00:07:33,610 --> 00:07:36,060
that first

00:07:36,160 --> 00:07:40,300
so what the rename method so if I pass

00:07:38,840 --> 00:07:46,100
it a dictionary it's going to change

00:07:40,300 --> 00:07:47,780
name to name 82 name so that was pretty

00:07:46,100 --> 00:07:51,290
easy but we've still got all these other

00:07:47,780 --> 00:07:54,080
columns that are no good so I can also

00:07:51,290 --> 00:07:55,970
pass it a function so fixed column names

00:07:54,080 --> 00:08:00,170
is pretty simple I'm just going to

00:07:55,970 --> 00:08:03,010
replace where I see 80 with nothing I'm

00:08:00,170 --> 00:08:11,000
going to pass in the function and now

00:08:03,010 --> 00:08:12,530
we've got some better call names so

00:08:11,000 --> 00:08:14,990
seeing that you can treat the data frame

00:08:12,530 --> 00:08:18,680
like a dictionary can also treat a data

00:08:14,990 --> 00:08:27,320
frame like a list to do that we use this

00:08:18,680 --> 00:08:29,300
i loke attribute and so if we want to

00:08:27,320 --> 00:08:32,600
get the first row we just pass it the

00:08:29,300 --> 00:08:35,660
first element and it returns us a series

00:08:32,600 --> 00:08:38,360
with the data safe here the index is the

00:08:35,660 --> 00:08:43,610
different column names and then we've

00:08:38,360 --> 00:08:45,650
got the different values does anyone

00:08:43,610 --> 00:08:50,000
notice anything weird about the last Roy

00:08:45,650 --> 00:08:53,300
the dulcet so this brings me to the next

00:08:50,000 --> 00:08:57,110
thing which is ni n which is not a

00:08:53,300 --> 00:08:58,760
number which is so if you have a look at

00:08:57,110 --> 00:09:01,490
the last four rows we can do slicing as

00:08:58,760 --> 00:09:03,980
well with with I my work will notice

00:09:01,490 --> 00:09:05,690
that the last two rows a pretty dodgy

00:09:03,980 --> 00:09:09,350
and that was from the CSV that we had

00:09:05,690 --> 00:09:11,720
that it's got these empty final one and

00:09:09,350 --> 00:09:17,660
then the one with the dollar in it so

00:09:11,720 --> 00:09:18,950
let's get rid of that so yes and then if

00:09:17,660 --> 00:09:21,410
you used to sequel think of it like a

00:09:18,950 --> 00:09:24,170
null that's just where you say there was

00:09:21,410 --> 00:09:26,300
a missing data here and we've got these

00:09:24,170 --> 00:09:31,190
methods that make it easy to work with

00:09:26,300 --> 00:09:34,430
nolles so if I call is no it's going to

00:09:31,190 --> 00:09:38,180
return true and false februari rowing my

00:09:34,430 --> 00:09:39,949
data saying whether or not it was no not

00:09:38,180 --> 00:09:43,319
no Liz the opposite

00:09:39,949 --> 00:09:46,110
and I can also feel missing data with

00:09:43,319 --> 00:09:49,069
with zeros or means or depending on the

00:09:46,110 --> 00:09:51,660
context what I want to do with it so

00:09:49,069 --> 00:09:53,970
there's three ways we could get rid of

00:09:51,660 --> 00:09:55,170
this data we could just do a slice and

00:09:53,970 --> 00:09:59,100
get rid of the last two that's pretty

00:09:55,170 --> 00:10:00,930
easy we can call drop in a drop a day is

00:09:59,100 --> 00:10:08,009
going to look at each row there's any

00:10:00,930 --> 00:10:09,360
nulls it's going to discard it I'm going

00:10:08,009 --> 00:10:12,509
to do a third way which is a bit more

00:10:09,360 --> 00:10:14,970
flexible so this is if we call not now

00:10:12,509 --> 00:10:17,100
on our series it's going to go through

00:10:14,970 --> 00:10:19,649
every row you know in our data and say

00:10:17,100 --> 00:10:21,959
is it not now not now so we see we've

00:10:19,649 --> 00:10:33,569
got true true true true true and without

00:10:21,959 --> 00:10:35,250
two forces at the bottom which brings me

00:10:33,569 --> 00:10:38,879
to another way of accessing our data so

00:10:35,250 --> 00:10:40,470
if we pass one of these bullying indexes

00:10:38,879 --> 00:10:44,699
so it's really a list of true false true

00:10:40,470 --> 00:10:47,250
false false true to our data frame it's

00:10:44,699 --> 00:10:50,750
going to filter out where it's false and

00:10:47,250 --> 00:10:53,339
just return you where you've got true so

00:10:50,750 --> 00:10:54,389
this is going to give me everywhere

00:10:53,339 --> 00:10:56,329
where it's true and it's going to get

00:10:54,389 --> 00:10:59,610
rid of those last two lines of data I

00:10:56,329 --> 00:11:03,079
could have done that in one line and now

00:10:59,610 --> 00:11:06,439
if we look at the end of our data set

00:11:03,079 --> 00:11:06,439
looks a lot better

00:11:11,140 --> 00:11:18,890
does that look like good date time data

00:11:15,350 --> 00:11:22,070
to anyone no so you can see that's

00:11:18,890 --> 00:11:24,050
passed it in as a float because if the

00:11:22,070 --> 00:11:27,380
data format that the bong is decided to

00:11:24,050 --> 00:11:30,050
use for their CSV which brings me to my

00:11:27,380 --> 00:11:32,030
next point so if you have a look at the

00:11:30,050 --> 00:11:36,160
bottom instead that the there's a d-type

00:11:32,030 --> 00:11:39,650
for our data so each of our columns is a

00:11:36,160 --> 00:11:44,650
homogeneous type so most of these are

00:11:39,650 --> 00:11:44,650
floats where it's a number you can see

00:11:45,130 --> 00:11:52,180
the amount temperature is a float that

00:11:47,480 --> 00:11:55,130
seems to make sense letting long flow

00:11:52,180 --> 00:11:56,750
could also be an object in this example

00:11:55,130 --> 00:11:58,820
here those objects will just be streams

00:11:56,750 --> 00:12:01,040
where it's string data but it can be

00:11:58,820 --> 00:12:05,330
arbitrary objects seen in your data if

00:12:01,040 --> 00:12:08,150
you like and if we find our local

00:12:05,330 --> 00:12:14,300
datetime fall that's a float and then

00:12:08,150 --> 00:12:15,350
that's not right so let's fix that so

00:12:14,300 --> 00:12:18,470
the first thing I do is I'm going to

00:12:15,350 --> 00:12:21,380
move it from a float to a string now

00:12:18,470 --> 00:12:26,510
it's looking a bit more like datetime

00:12:21,380 --> 00:12:28,430
data but we can do one better so here

00:12:26,510 --> 00:12:30,500
I'm calling map which is you know

00:12:28,430 --> 00:12:32,270
similar to the standard library map it's

00:12:30,500 --> 00:12:41,060
going to call this function on every row

00:12:32,270 --> 00:12:44,120
of my data so can this has this really

00:12:41,060 --> 00:12:45,380
convenient to daytime function if your

00:12:44,120 --> 00:12:48,500
data has anything that looks like a

00:12:45,380 --> 00:12:51,140
string and you pass it to to 2 pi day

00:12:48,500 --> 00:12:52,880
time it will figure out and passes a

00:12:51,140 --> 00:12:54,950
date time you don't need to tell it the

00:12:52,880 --> 00:12:56,420
format normally it's pretty smart

00:12:54,950 --> 00:12:58,910
figuring out here's some examples that

00:12:56,420 --> 00:13:00,970
it would make sense of I flying pretty

00:12:58,910 --> 00:13:06,470
much almost anything I'll throw at it

00:13:00,970 --> 00:13:07,610
it'll figure it out so here you know

00:13:06,470 --> 00:13:08,540
we've changed it to a string and then

00:13:07,610 --> 00:13:14,510
whether

00:13:08,540 --> 00:13:17,750
call to date time and now we have a date

00:13:14,510 --> 00:13:20,060
time 64 d type which is stored as an

00:13:17,750 --> 00:13:21,770
import it's much more efficient and it

00:13:20,060 --> 00:13:35,690
gives you nanosecond resolution of your

00:13:21,770 --> 00:13:37,790
your time better yeah so just to talk

00:13:35,690 --> 00:13:39,320
about the index a little bit so if we

00:13:37,790 --> 00:13:41,380
don't specify an index it's going to

00:13:39,320 --> 00:13:43,340
it's like kind of like a seagull auto

00:13:41,380 --> 00:13:46,390
incrementing ID is kind of what it does

00:13:43,340 --> 00:13:49,010
it the called ranged length of my data

00:13:46,390 --> 00:13:53,510
and so that's why it's listed here 0 0 1

00:13:49,010 --> 00:13:56,390
2 3 4 5 now for our data that doesn't

00:13:53,510 --> 00:14:00,590
make as much sense so I'm going to set

00:13:56,390 --> 00:14:02,510
the index to be that date time I'm going

00:14:00,590 --> 00:14:06,680
to sort that index because it was in

00:14:02,510 --> 00:14:09,680
reverse order before and now our index

00:14:06,680 --> 00:14:11,470
is this data that we've got every half

00:14:09,680 --> 00:14:14,980
an hour with these observations of

00:14:11,470 --> 00:14:14,980
temperature and things like that

00:14:17,059 --> 00:14:21,709
so now that I've got a date time as an

00:14:19,069 --> 00:14:23,659
index I have a time series this gives me

00:14:21,709 --> 00:14:26,799
lots of flexible features one of which

00:14:23,659 --> 00:14:29,989
is smart indexing so when I get just

00:14:26,799 --> 00:14:33,079
2014 starter that will return it to me

00:14:29,989 --> 00:14:36,019
if I want to get just a month this will

00:14:33,079 --> 00:14:37,999
return it to me I want you today this

00:14:36,019 --> 00:14:40,309
will return it to me I can also do

00:14:37,999 --> 00:14:41,539
slicing now slicing like this will give

00:14:40,309 --> 00:14:44,029
you both ends of the data not like a

00:14:41,539 --> 00:14:46,569
normal list slice and so this has given

00:14:44,029 --> 00:14:50,319
me between three-thirty and five o'clock

00:14:46,569 --> 00:14:50,319
data that I'm interested in

00:14:53,320 --> 00:14:58,090
so a quick recap of how to get at your

00:14:55,780 --> 00:14:59,740
data was with pandas you can treat it

00:14:58,090 --> 00:15:02,950
like a dictionary and give it its labels

00:14:59,740 --> 00:15:05,110
if the column or the index we can treat

00:15:02,950 --> 00:15:08,890
it like a list with the my local we can

00:15:05,110 --> 00:15:10,180
do slicing and things like that and we

00:15:08,890 --> 00:15:14,290
can do between indexing where if we

00:15:10,180 --> 00:15:18,160
passing a true/false list it will give

00:15:14,290 --> 00:15:19,660
us where we pass in a truth and so I've

00:15:18,160 --> 00:15:24,400
got our data frame which is you know a

00:15:19,660 --> 00:15:26,380
table of data series or each columns we

00:15:24,400 --> 00:15:29,910
have an index and if for the indexes

00:15:26,380 --> 00:15:29,910
date times then you've got a time series

00:15:31,290 --> 00:15:35,530
so we've got a clean data set nellore a

00:15:33,760 --> 00:15:44,740
much cleaner data set let's do some

00:15:35,530 --> 00:15:47,770
analysis so it's really easy to do some

00:15:44,740 --> 00:15:50,620
graphics one line of code here I've

00:15:47,770 --> 00:15:53,830
plotted the temperature in the apparent

00:15:50,620 --> 00:15:58,930
temperature because it's a time serious

00:15:53,830 --> 00:16:03,460
and it's it's figured out my x-axis like

00:15:58,930 --> 00:16:06,160
that for me have a look around about

00:16:03,460 --> 00:16:09,700
three o'clock yesterday afternoon storm

00:16:06,160 --> 00:16:15,810
hit the temperature dropped 10 degrees

00:16:09,700 --> 00:16:19,510
in under an hour as also a lot of rain

00:16:15,810 --> 00:16:24,880
and yeah it sort of got very cold very

00:16:19,510 --> 00:16:27,430
quickly so because we have this time

00:16:24,880 --> 00:16:30,220
series data we can Reese ample our data

00:16:27,430 --> 00:16:33,430
so we were given half the hourly data if

00:16:30,220 --> 00:16:37,240
we want to be every four hours we call

00:16:33,430 --> 00:16:40,930
resample we say how frequently we want

00:16:37,240 --> 00:16:44,950
the data and we want to we tell it how

00:16:40,930 --> 00:16:47,500
we want to figure out the value so here

00:16:44,950 --> 00:16:50,850
we've gone for the average every four

00:16:47,500 --> 00:16:50,850
hours so now we've got

00:16:51,150 --> 00:16:58,530
our data every four hours if that always

00:16:53,850 --> 00:17:02,280
wanted we can also make a war granular

00:16:58,530 --> 00:17:04,080
if I passing 15 minutes I then need to

00:17:02,280 --> 00:17:06,810
tell I can fill in the missing gaps and

00:17:04,080 --> 00:17:08,220
here i call it to feel forward I could

00:17:06,810 --> 00:17:12,000
have told it to feel backwards I can

00:17:08,220 --> 00:17:13,770
also give it a limit of how many gaps to

00:17:12,000 --> 00:17:21,510
feel if there's a big gap I only fuel in

00:17:13,770 --> 00:17:24,480
say three I don't use this a lot um I

00:17:21,510 --> 00:17:27,450
don't think it does I would imagine this

00:17:24,480 --> 00:17:29,970
would be a way of hacking something like

00:17:27,450 --> 00:17:32,100
that in other methods but I don't think

00:17:29,970 --> 00:17:33,810
I think there's only forward and back

00:17:32,100 --> 00:17:41,790
from tool method but it'd be worth while

00:17:33,810 --> 00:17:44,310
checking my dogs um okay so we've got

00:17:41,790 --> 00:17:46,620
out you know standard statistic method

00:17:44,310 --> 00:17:49,470
methods that it gives us so i can call

00:17:46,620 --> 00:17:53,670
the mean on my series a really useful

00:17:49,470 --> 00:17:56,510
one is described it'll give me the core

00:17:53,670 --> 00:17:59,790
tiles of my data will give me my

00:17:56,510 --> 00:18:06,660
standard deviation my main number of

00:17:59,790 --> 00:18:07,980
values and if i call it on the data

00:18:06,660 --> 00:18:10,590
frame itself is going to do that for

00:18:07,980 --> 00:18:17,460
every column of my day column in my data

00:18:10,590 --> 00:18:19,740
set so this is a larger example don't

00:18:17,460 --> 00:18:21,270
don't try and follow along with all the

00:18:19,740 --> 00:18:23,970
code here but basically what I'm doing

00:18:21,270 --> 00:18:27,860
I've downloaded for the last couple of

00:18:23,970 --> 00:18:27,860
days the data for Brisbane and Melbourne

00:18:28,580 --> 00:18:33,210
so here I'm just creating a list I'm

00:18:31,680 --> 00:18:38,250
reading in each one as a data frame and

00:18:33,210 --> 00:18:39,690
adding that to a list concat allows me

00:18:38,250 --> 00:18:42,800
to stitch my data frames together so

00:18:39,690 --> 00:18:45,210
this is going to just end to end by Rho

00:18:42,800 --> 00:18:49,560
stitched together all that data into one

00:18:45,210 --> 00:18:51,690
data set because that's going to have

00:18:49,560 --> 00:18:55,580
overlapping date ranges I'm going to

00:18:51,690 --> 00:18:55,580
drop whether there's any duplicates

00:18:56,440 --> 00:19:02,570
and in this example instead of just the

00:18:59,179 --> 00:19:08,210
data to exit we like both the date and

00:19:02,570 --> 00:19:11,720
the city so now I doubt it looks like

00:19:08,210 --> 00:19:16,400
this we've got our two levels your next

00:19:11,720 --> 00:19:18,950
one we've got both cds data so the next

00:19:16,400 --> 00:19:21,770
feature i'm going to talk about is a

00:19:18,950 --> 00:19:26,260
stack and unstack it's another useful

00:19:21,770 --> 00:19:31,490
way of playing around with your data and

00:19:26,260 --> 00:19:32,929
it's pretty simple so I'm stack if I

00:19:31,490 --> 00:19:35,720
call unstack on my data it's going to

00:19:32,929 --> 00:19:38,450
take this this index here this is this

00:19:35,720 --> 00:19:41,030
colon one of my index of the name and

00:19:38,450 --> 00:19:42,230
it's going to make that a kata in two

00:19:41,030 --> 00:19:45,230
columns so then I'm gonna have a column

00:19:42,230 --> 00:19:48,610
for both Melbourne and Brisbane their

00:19:45,230 --> 00:19:52,280
temperature every calling in my data set

00:19:48,610 --> 00:19:54,470
the only thing i can do is i can call so

00:19:52,280 --> 00:19:59,210
that's unstacked if i call stack it's

00:19:54,470 --> 00:20:01,250
going to grab all my columns and make my

00:19:59,210 --> 00:20:02,539
data into a record for our so i probably

00:20:01,250 --> 00:20:06,770
does it make a lot of sense with that an

00:20:02,539 --> 00:20:10,130
example so let's do that so if I unstack

00:20:06,770 --> 00:20:14,809
my data by name now I've got a column

00:20:10,130 --> 00:20:20,950
for each one of my CDs and we can see

00:20:14,809 --> 00:20:20,950
that it's definitely warmer in Brisbane

00:20:23,860 --> 00:20:32,390
if I call stack on my data it's going to

00:20:30,160 --> 00:20:35,000
grab all those columns and put them into

00:20:32,390 --> 00:20:38,740
one column and then also grab all those

00:20:35,000 --> 00:20:38,740
valleys and make data column so now for

00:20:39,220 --> 00:20:43,160
example if i was going to store my data

00:20:41,570 --> 00:20:46,340
in a database that might be a good way

00:20:43,160 --> 00:20:53,000
of shaking it and let's do nain just a

00:20:46,340 --> 00:20:56,840
couple of lines of code and so that

00:20:53,000 --> 00:20:59,480
allows us to do some quite another grab

00:20:56,840 --> 00:21:02,390
so here I'm getting the apparent

00:20:59,480 --> 00:21:04,850
temperature I'm stacking it and then I'm

00:21:02,390 --> 00:21:09,920
plotting it and we can see that since I

00:21:04,850 --> 00:21:14,540
left Melbourne it's kind of been as cold

00:21:09,920 --> 00:21:17,000
as your fridge and we're not really

00:21:14,540 --> 00:21:18,440
seeing the hockey stick effect in

00:21:17,000 --> 00:21:20,890
Melbourne that we've seen in Brisbane

00:21:18,440 --> 00:21:20,890
this morning

00:21:26,430 --> 00:21:34,990
so that's my first data set it's getting

00:21:30,430 --> 00:21:38,080
close to lunch I'm getting a little bit

00:21:34,990 --> 00:21:44,740
thirsty which brings me to my second

00:21:38,080 --> 00:21:47,260
data set last week I found out about a

00:21:44,740 --> 00:21:51,430
website called now chaps com anyone

00:21:47,260 --> 00:21:54,660
heard of this it's a website it's a

00:21:51,430 --> 00:22:00,670
social media app basically go to a pub

00:21:54,660 --> 00:22:03,760
you order a beer sort of login checking

00:22:00,670 --> 00:22:07,600
for square style you say I was at this

00:22:03,760 --> 00:22:10,420
pub I had this beer tasted great I can

00:22:07,600 --> 00:22:15,010
then log in and see which pubs have what

00:22:10,420 --> 00:22:17,920
beers on tap where so now to look at

00:22:15,010 --> 00:22:19,840
that I kind of hack their API to get a

00:22:17,920 --> 00:22:28,300
couple of data sets for Brisbane going

00:22:19,840 --> 00:22:31,500
to have a look at so yeah this is the

00:22:28,300 --> 00:22:31,500
website here so I can see

00:22:34,370 --> 00:22:40,440
hopefully the network's going to help me

00:22:36,270 --> 00:22:44,010
out here maybe not but yeah I can search

00:22:40,440 --> 00:22:45,150
by the map and it would list me all the

00:22:44,010 --> 00:22:50,390
beers that they've got on tap and what

00:22:45,150 --> 00:22:50,390
people thought of them well my data set

00:22:54,230 --> 00:22:59,610
ok so i got about 20 venues in the

00:22:58,200 --> 00:23:01,710
brisbane area close to the conference

00:22:59,610 --> 00:23:04,890
center so i've downloaded these into a

00:23:01,710 --> 00:23:07,980
data set called venues dot CSV so if we

00:23:04,890 --> 00:23:10,620
load that up we can see we've got each

00:23:07,980 --> 00:23:14,160
of the the bars we've got the latitude

00:23:10,620 --> 00:23:15,870
and longitude we've got this venue ID

00:23:14,160 --> 00:23:23,790
which is we're going to come in useful

00:23:15,870 --> 00:23:25,650
later we've got the address etc so where

00:23:23,790 --> 00:23:32,840
is the closest pub to where we are right

00:23:25,650 --> 00:23:35,970
now again we can use the fast math an

00:23:32,840 --> 00:23:38,929
umpire style code here just as quite

00:23:35,970 --> 00:23:43,679
agoura system square root of two squares

00:23:38,929 --> 00:23:47,550
of the distances so I'm setting that is

00:23:43,679 --> 00:23:49,410
a nudist column I'm going to sort by

00:23:47,550 --> 00:23:53,370
distance so that the first one is the

00:23:49,410 --> 00:23:57,630
closest and we can see that the closest

00:23:53,370 --> 00:24:00,770
pub is the archive be boutique 100

00:23:57,630 --> 00:24:00,770
boundary street west end

00:24:03,099 --> 00:24:19,429
is that no I'll do of the pub said I've

00:24:09,200 --> 00:24:22,099
got of my list oh okay well I'm sure I

00:24:19,429 --> 00:24:23,749
am wrong I'm sure that the archive beer

00:24:22,099 --> 00:24:26,599
though was closer to them like the

00:24:23,749 --> 00:24:30,739
scratch or the cameras cocktail which

00:24:26,599 --> 00:24:32,149
one is closest in the West it could be

00:24:30,739 --> 00:24:38,979
in where my have set my letter long that

00:24:32,149 --> 00:24:38,979
was off gmaps somewhere in this area but

00:24:39,970 --> 00:24:51,859
okay well yeah I didn't want to put like

00:24:46,460 --> 00:24:53,149
a 5-line formula in my slides this is

00:24:51,859 --> 00:24:54,499
one that got into my data set that's in

00:24:53,149 --> 00:24:57,409
Sydney so at least we know that that is

00:24:54,499 --> 00:25:00,169
definitely the furthest away pub on the

00:24:57,409 --> 00:25:03,289
list so that was that was my main

00:25:00,169 --> 00:25:08,509
validation say this is also available on

00:25:03,289 --> 00:25:10,429
github so pull requests or accepted all

00:25:08,509 --> 00:25:14,259
my slides here over by Python notebook

00:25:10,429 --> 00:25:17,479
so you can yeah please fix my code

00:25:14,259 --> 00:25:20,979
anyway we can say we've got some some

00:25:17,479 --> 00:25:20,979
pubs closer than others

00:25:22,950 --> 00:25:34,840
which makes me thirsty wait for it um so

00:25:33,790 --> 00:25:36,670
there's two parts of this data set we've

00:25:34,840 --> 00:25:38,470
seen the venue's we've also got the

00:25:36,670 --> 00:25:40,570
check in so this is you know each

00:25:38,470 --> 00:25:44,260
individual person is gone had a beer

00:25:40,570 --> 00:25:48,010
checked in and possibly left a rating in

00:25:44,260 --> 00:25:49,960
a comment so we've got different beers

00:25:48,010 --> 00:25:54,940
that they're drunk where they drunk them

00:25:49,960 --> 00:25:58,810
who brewed them where they're from got a

00:25:54,940 --> 00:26:01,690
rating gallery important thing is we put

00:25:58,810 --> 00:26:06,280
a venue ID so that's going to part of

00:26:01,690 --> 00:26:09,610
our next step which is that we can do

00:26:06,280 --> 00:26:11,620
fast cycle stole joins it's really

00:26:09,610 --> 00:26:15,900
simple we call the merge function

00:26:11,620 --> 00:26:17,740
commanders we passing out two datasets

00:26:15,900 --> 00:26:19,420
actually by default if there's any

00:26:17,740 --> 00:26:22,990
common columns between the datasets

00:26:19,420 --> 00:26:24,640
it'll join on those here I've been

00:26:22,990 --> 00:26:27,550
explicit and said drawn on the venue ID

00:26:24,640 --> 00:26:29,230
and I told us to do an adjoint I could

00:26:27,550 --> 00:26:33,490
have done a left to right now to join I

00:26:29,230 --> 00:26:34,900
can handle all of those really smartly

00:26:33,490 --> 00:26:37,570
so now we've got one data set with all

00:26:34,900 --> 00:26:40,450
of our data so we can see we've got now

00:26:37,570 --> 00:26:44,200
got our beer data but we've also got you

00:26:40,450 --> 00:26:47,160
know our lat long you know venue and the

00:26:44,200 --> 00:26:47,160
address

00:26:48,820 --> 00:26:54,409
so what can we do with this so what's

00:26:52,340 --> 00:26:59,720
what are the positive is in the area

00:26:54,409 --> 00:27:02,330
according to now tap drinkers we've got

00:26:59,720 --> 00:27:04,549
the value cancer methods of a series

00:27:02,330 --> 00:27:09,559
this will do frequency counting for you

00:27:04,549 --> 00:27:12,740
of your data that you passed out so here

00:27:09,559 --> 00:27:15,350
we're getting the top ten beers as a

00:27:12,740 --> 00:27:21,289
period in that data set so apparently

00:27:15,350 --> 00:27:24,860
people like pale L if I give it our

00:27:21,289 --> 00:27:29,809
normalize equals true parameter it will

00:27:24,860 --> 00:27:32,389
give me relative frequencies so here x x

00:27:29,809 --> 00:27:34,039
100 to get a percentage so it's about

00:27:32,389 --> 00:27:39,830
three percent of people who've checked

00:27:34,039 --> 00:27:41,269
in for this pale out so it brings me to

00:27:39,830 --> 00:27:44,990
group by which is sort of one of the

00:27:41,269 --> 00:27:46,730
killer features of pandas really it's a

00:27:44,990 --> 00:27:51,350
bit analogous to sequel group by but

00:27:46,730 --> 00:27:54,590
much more powerful and flexible and the

00:27:51,350 --> 00:27:56,350
concepts pretty pretty simple it's we

00:27:54,590 --> 00:27:59,840
split our data according to some

00:27:56,350 --> 00:28:02,779
parameters we played data to each of

00:27:59,840 --> 00:28:11,000
those data sets and then we combine it

00:28:02,779 --> 00:28:13,730
again and it's very flexible so who has

00:28:11,000 --> 00:28:20,960
the most variety of beers of those pubs

00:28:13,730 --> 00:28:23,570
listed so this is a function that will

00:28:20,960 --> 00:28:25,760
call in unique and unique is another

00:28:23,570 --> 00:28:28,279
feature of a series it'll tell you how

00:28:25,760 --> 00:28:32,720
many unique values you have in your data

00:28:28,279 --> 00:28:33,740
set as an aside we're working on a patch

00:28:32,720 --> 00:28:36,110
at the moment it's going to speed that

00:28:33,740 --> 00:28:38,120
up of a lot hopefully that gets merged

00:28:36,110 --> 00:28:42,649
soon as I've got some time not running a

00:28:38,120 --> 00:28:45,139
talk so we call group I on our data set

00:28:42,649 --> 00:28:46,580
we're going to group by name just that's

00:28:45,139 --> 00:28:49,140
the venue

00:28:46,580 --> 00:28:52,980
we're going to look at the beer column

00:28:49,140 --> 00:28:55,920
and we're going to run an aggregate and

00:28:52,980 --> 00:28:58,590
so we're going to have a result of

00:28:55,920 --> 00:29:02,160
numbers which is going to call for each

00:28:58,590 --> 00:29:08,130
of those groups the unique action up

00:29:02,160 --> 00:29:11,040
here so here's a result we can see that

00:29:08,130 --> 00:29:14,910
the MC seems to have the biggest choice

00:29:11,040 --> 00:29:17,510
of beers this is the one that I said was

00:29:14,910 --> 00:29:21,090
closest well all is the real closest one

00:29:17,510 --> 00:29:29,430
tomahawk yes you haven't got this much

00:29:21,090 --> 00:29:30,780
range um so you know we can see that

00:29:29,430 --> 00:29:33,450
this is the sequel that we kind of do a

00:29:30,780 --> 00:29:38,670
similar thing for people who are more

00:29:33,450 --> 00:29:44,590
familiar with sequel so another straw

00:29:38,670 --> 00:29:47,950
poll ends up too likeah the bad

00:29:44,590 --> 00:29:50,919
hands up if you like chocolate it's more

00:29:47,950 --> 00:30:00,490
people hands up if you like chocolate

00:29:50,919 --> 00:30:03,640
beer this is a Victorian Brewer holgate

00:30:00,490 --> 00:30:08,039
they brew a chocolate porter called the

00:30:03,640 --> 00:30:10,450
temptress it has chocolate and vanilla

00:30:08,039 --> 00:30:15,070
beans as part of the ingredients it's

00:30:10,450 --> 00:30:19,929
very good where can I get one in

00:30:15,070 --> 00:30:21,130
Brisbane so here we I'm setting the is

00:30:19,929 --> 00:30:26,970
delicious that's going to be one of

00:30:21,130 --> 00:30:29,230
these boolean true false values I have

00:30:26,970 --> 00:30:31,659
quite an has these vectorized string

00:30:29,230 --> 00:30:35,380
operations that calls out to these

00:30:31,659 --> 00:30:36,760
scythe on implementations of the various

00:30:35,380 --> 00:30:38,950
string things that you normally have

00:30:36,760 --> 00:30:40,450
string operations so there's going to be

00:30:38,950 --> 00:30:41,860
for me any beers with the name temptress

00:30:40,450 --> 00:30:44,049
image because I wasn't sure if it would

00:30:41,860 --> 00:30:47,409
have a couple of words in the title or

00:30:44,049 --> 00:30:50,230
something I'm going to get just the

00:30:47,409 --> 00:30:54,640
subset of the data that is the beers fit

00:30:50,230 --> 00:30:56,169
of the temperatures I'm going to sort

00:30:54,640 --> 00:30:58,450
that by credit out no credits when

00:30:56,169 --> 00:31:01,210
someone's checked in and I want that

00:30:58,450 --> 00:31:04,960
going most recent because if someone

00:31:01,210 --> 00:31:08,620
drunk one a month ago that doesn't mean

00:31:04,960 --> 00:31:15,429
it still on tap so we have a look at our

00:31:08,620 --> 00:31:19,630
data their ears 16 people in recently

00:31:15,429 --> 00:31:21,870
who have had a temptress they've had

00:31:19,630 --> 00:31:27,059
them at one two three four different

00:31:21,870 --> 00:31:30,669
venues and actually the archive via

00:31:27,059 --> 00:31:33,210
boutique someone had one last night at

00:31:30,669 --> 00:31:33,210
seven thirty

00:31:35,280 --> 00:31:51,550
apparently it's on the hand pump and

00:31:39,790 --> 00:31:53,500
it's chocolate en vanilla I was aa 730 I

00:31:51,550 --> 00:31:57,040
was in the air I should have probably

00:31:53,500 --> 00:32:00,630
already arrived no it wasn't me i might

00:31:57,040 --> 00:32:04,000
check in tonight laughing Saudi app or

00:32:00,630 --> 00:32:10,150
after this talk so this is my last thing

00:32:04,000 --> 00:32:12,130
to cover Excel has pivot tables seagull

00:32:10,150 --> 00:32:16,440
doesn't have pivot tables panis has

00:32:12,130 --> 00:32:19,150
pivoted alls it's pretty simple a cello

00:32:16,440 --> 00:32:21,370
what I want the rose to be here it's the

00:32:19,150 --> 00:32:23,440
different beers until it what I want the

00:32:21,370 --> 00:32:27,880
cons to be this is the different venues

00:32:23,440 --> 00:32:32,440
I taught what values to look at people

00:32:27,880 --> 00:32:35,110
think it's a five star rating and then I

00:32:32,440 --> 00:32:43,930
tell it how to aggregate that so we want

00:32:35,110 --> 00:32:47,320
to average rating so we can see I don't

00:32:43,930 --> 00:32:49,690
know the Indian Pale Ale averages a four

00:32:47,320 --> 00:32:57,090
star rating at the collar I whatever

00:32:49,690 --> 00:32:57,090
that is that's in Sydney so to wrap up

00:32:58,830 --> 00:33:06,520
panis is invaluable tool from claim with

00:33:04,420 --> 00:33:09,670
any kind of data it's just first thing I

00:33:06,520 --> 00:33:11,680
turn to so start playing with it the

00:33:09,670 --> 00:33:14,020
documentation is really good actually

00:33:11,680 --> 00:33:16,270
doing this talk I've read through most

00:33:14,020 --> 00:33:20,170
of it and and also made a lot of patches

00:33:16,270 --> 00:33:23,590
to improve it along the way there's an a

00:33:20,170 --> 00:33:28,000
Riley book so West McKinney is the guy

00:33:23,590 --> 00:33:30,280
who wrote pandas really smart guy so

00:33:28,000 --> 00:33:32,080
he's the guy who's written this book it

00:33:30,280 --> 00:33:33,970
says Python for data analysis is really

00:33:32,080 --> 00:33:39,460
just mainly covering

00:33:33,970 --> 00:33:46,330
and I Python i would highly recommend

00:33:39,460 --> 00:33:47,650
that it's about finishing me up we're

00:33:46,330 --> 00:33:50,830
also hiring at the moment comes speak to

00:33:47,650 --> 00:33:54,030
me during the talk otherwise questions

00:33:50,830 --> 00:33:54,030
in beach one

00:33:58,960 --> 00:34:01,020

YouTube URL: https://www.youtube.com/watch?v=1QOMk2k9aI8


