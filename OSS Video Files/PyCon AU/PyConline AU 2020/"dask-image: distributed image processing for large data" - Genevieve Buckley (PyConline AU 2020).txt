Title: "dask-image: distributed image processing for large data" - Genevieve Buckley (PyConline AU 2020)
Publication date: 2020-09-06
Playlist: PyConline AU 2020
Description: 
	Genevieve Buckley

https://2020.pycon.org.au/program/KHULAN

Image datasets are large, and becoming larger. The widely used benchmark dataset COCO (Common Objects in Context) contains 330,000 individual images. The average size of a single entry on the image database EMPIAR is over 1TB, and can easily reach several terabytes.  Even where individual images are small enough to fit in-memory, many existing parallelization methods are difficult to scale seamlessly between a laptop and a supercomputing cluster. For instance, the python multiprocessing module is restricted to a single mode and can't take advantage of multiple compute nodes on a distributed supercomputing cluster.

We need easy ways to work with large image data. This talk introduces dask-image, a python library for distributed image processing. The target audience are python programmers currently using numpy and scipy with large array data, where the whole dataset cannot fit in memory or is close to that limit. It's for people who want to get started with parallel processing, either because they have large single-image data, or because they want to do batch processing applying the same analysis to many smaller images (sometimes known an embarrassingly parallel problem). The specific image analysis functions provided by dask-image are of broad interest to a diverse range of analysis applications including (but not limited to) video/streaming data, computer vision, and scientific fields including astronomy, microscopy and geosciences.

Specifically, this talk will cover:
* An overview of the dask-image library
    * Lazy image loading
    * Image pre-processing functionality (convolutions, filters, etc.)
    * Analysis of segmented images (distributed labeling, and measurements of those label regions)
* Mixing in your own custom analysis functions (using dask delayed, map_blocks, and map_overlap)
* A practical case study of a Python image processing pipeline

dask-image is open source, released under a BSD 3-Clause license, and can be installed using conda or pip. You can find the source code at https://github.com/dask/dask-image and the quickstart guide at https://github.com/dask/dask-examples/blob/master/applications/image-processing.ipynb

Produced by NDV: https://youtube.com/channel/UCQ7dFBzZGlBvtU2hCecsBBg?sub_confirmation=1

Python, PyCon, PyConAU, PyConline

Fri Sep  4 13:55:00 2020 at Curlyboi
Captions: 
	00:00:00,960 --> 00:00:05,040
hello everyone welcome back

00:00:03,040 --> 00:00:06,319
welcome back to the science data and

00:00:05,040 --> 00:00:09,679
analytics track of

00:00:06,319 --> 00:00:10,880
pike online eu for our next talk we have

00:00:09,679 --> 00:00:13,679
genevieve buckley

00:00:10,880 --> 00:00:14,880
talking about dark image distributed

00:00:13,679 --> 00:00:16,640
image processing for

00:00:14,880 --> 00:00:18,160
large data which i'm pretty excited by

00:00:16,640 --> 00:00:19,520
i've been hearing a lot about dusk and

00:00:18,160 --> 00:00:21,119
i'm super keen to see what sort of

00:00:19,520 --> 00:00:23,279
things that we can do with it

00:00:21,119 --> 00:00:24,800
quickly a little bit about genevieve

00:00:23,279 --> 00:00:26,240
she's a scientist and programmer based

00:00:24,800 --> 00:00:28,320
in melbourne australia she builds

00:00:26,240 --> 00:00:29,840
software tools for scientific discovery

00:00:28,320 --> 00:00:31,519
her interests include deep learning

00:00:29,840 --> 00:00:32,640
automated analysis and contributing to

00:00:31,519 --> 00:00:33,840
open source projects

00:00:32,640 --> 00:00:35,280
she's a wealth of professional

00:00:33,840 --> 00:00:36,160
experience with image processing and

00:00:35,280 --> 00:00:37,840
analogous

00:00:36,160 --> 00:00:39,280
analysis spanning x-ray imaging

00:00:37,840 --> 00:00:41,680
fluorescence microscopy

00:00:39,280 --> 00:00:43,680
and electron beam microscopy and she is

00:00:41,680 --> 00:00:44,000
a maintainer for the dusk image project

00:00:43,680 --> 00:00:47,039
so

00:00:44,000 --> 00:00:48,960
take it away genevieve thank you

00:00:47,039 --> 00:00:51,199
welcome everybody it's fantastic to be

00:00:48,960 --> 00:00:53,199
here uh i'd like to acknowledge

00:00:51,199 --> 00:00:55,039
the people of the kulin nations whose

00:00:53,199 --> 00:00:55,760
land that i am speaking to you from

00:00:55,039 --> 00:00:58,160
today

00:00:55,760 --> 00:01:00,000
and pay my respects to their elders past

00:00:58,160 --> 00:01:01,440
present and emerging

00:01:00,000 --> 00:01:03,199
as that said the project i'm going to

00:01:01,440 --> 00:01:05,280
talk to you today about is called dark

00:01:03,199 --> 00:01:08,640
image it's a project for distributed

00:01:05,280 --> 00:01:11,040
processing of large array data

00:01:08,640 --> 00:01:13,280
my name is genevieve i'm yeah as ned

00:01:11,040 --> 00:01:14,400
said a programmer and a scientist based

00:01:13,280 --> 00:01:18,080
in melbourne australia

00:01:14,400 --> 00:01:18,080
and maintainer for this project

00:01:18,159 --> 00:01:23,439
so who needs a project like dark image

00:01:21,200 --> 00:01:24,479
well if you're somebody who is working

00:01:23,439 --> 00:01:27,439
with array data

00:01:24,479 --> 00:01:28,400
and you're using numpy a lot and or like

00:01:27,439 --> 00:01:30,320
the scipy

00:01:28,400 --> 00:01:32,560
n dimensional image functions and you

00:01:30,320 --> 00:01:34,320
find that you're running out of ram or

00:01:32,560 --> 00:01:36,479
hitting that kind of a ceiling

00:01:34,320 --> 00:01:37,920
then dusk image is a project that's

00:01:36,479 --> 00:01:40,560
built for you

00:01:37,920 --> 00:01:42,479
there are two main cases with uh

00:01:40,560 --> 00:01:44,560
distributed image processing

00:01:42,479 --> 00:01:45,600
uh there's a case of batch processing

00:01:44,560 --> 00:01:49,439
where we have

00:01:45,600 --> 00:01:51,200
um maybe a lot of uh different images

00:01:49,439 --> 00:01:54,159
where you want to apply the same kind of

00:01:51,200 --> 00:01:56,799
analysis or processing steps to them

00:01:54,159 --> 00:01:58,640
this might be something like video data

00:01:56,799 --> 00:01:59,439
where you have a lot of individual video

00:01:58,640 --> 00:02:02,560
frames

00:01:59,439 --> 00:02:05,200
or something similar like to that

00:02:02,560 --> 00:02:06,399
where the processing that we do on the

00:02:05,200 --> 00:02:08,239
first image frame

00:02:06,399 --> 00:02:09,840
really is totally independent and has

00:02:08,239 --> 00:02:13,200
nothing to do with what we do

00:02:09,840 --> 00:02:16,560
to like the 10th image frame

00:02:13,200 --> 00:02:19,440
the second and slightly more complicated

00:02:16,560 --> 00:02:19,920
option here as a use case is where we

00:02:19,440 --> 00:02:22,160
have

00:02:19,920 --> 00:02:23,599
data that has a really large field of

00:02:22,160 --> 00:02:25,920
view

00:02:23,599 --> 00:02:26,879
and this is a little bit trickier to

00:02:25,920 --> 00:02:29,360
handle because

00:02:26,879 --> 00:02:31,360
while we might be able to do things like

00:02:29,360 --> 00:02:33,200
chopping our data up into many small

00:02:31,360 --> 00:02:34,400
tiles and processing each of them

00:02:33,200 --> 00:02:36,800
separately

00:02:34,400 --> 00:02:37,599
becomes a bit tricky uh in the joins and

00:02:36,800 --> 00:02:40,160
the gaps

00:02:37,599 --> 00:02:41,360
we want to be able to have a bit of

00:02:40,160 --> 00:02:43,280
overlapping

00:02:41,360 --> 00:02:46,160
happening at those edges so that we

00:02:43,280 --> 00:02:48,080
don't get strange edge effects

00:02:46,160 --> 00:02:50,800
so these are the two main use cases

00:02:48,080 --> 00:02:53,120
where it's really helpful

00:02:50,800 --> 00:02:54,560
we certainly might find in the second

00:02:53,120 --> 00:02:56,800
case the large field of view that

00:02:54,560 --> 00:02:58,560
ram is certainly an issue in the first

00:02:56,800 --> 00:03:00,959
case maybe you're sort of squeaking by

00:02:58,560 --> 00:03:02,319
because each of the single images aren't

00:03:00,959 --> 00:03:03,920
particularly large

00:03:02,319 --> 00:03:05,920
but if you're putting that in something

00:03:03,920 --> 00:03:07,040
like a for loop it might start being a

00:03:05,920 --> 00:03:09,280
bit time consuming

00:03:07,040 --> 00:03:11,599
and just kind of dragging dragging you

00:03:09,280 --> 00:03:11,599
down

00:03:11,760 --> 00:03:15,680
for a couple of different scientific

00:03:13,680 --> 00:03:16,800
motivating examples i just want to show

00:03:15,680 --> 00:03:20,400
you here

00:03:16,800 --> 00:03:21,440
that the kinds of data array data we can

00:03:20,400 --> 00:03:23,440
use with dusk

00:03:21,440 --> 00:03:24,879
are really really broad we have from the

00:03:23,440 --> 00:03:28,159
very very large

00:03:24,879 --> 00:03:30,879
on the left i have an example of some

00:03:28,159 --> 00:03:32,000
sentinel satellite imagery being shown

00:03:30,879 --> 00:03:34,239
in the nepari image

00:03:32,000 --> 00:03:35,680
viewer and this covers like hundreds and

00:03:34,239 --> 00:03:38,560
thousands of kilometers

00:03:35,680 --> 00:03:40,000
on a very regular basis uh right down to

00:03:38,560 --> 00:03:40,879
something at the very other end of the

00:03:40,000 --> 00:03:43,519
scale

00:03:40,879 --> 00:03:44,400
so they're very very small over here we

00:03:43,519 --> 00:03:47,760
have um

00:03:44,400 --> 00:03:49,440
some fluorescent image data microscopy

00:03:47,760 --> 00:03:50,879
of individual neurons within brain

00:03:49,440 --> 00:03:53,120
tissue

00:03:50,879 --> 00:03:56,560
so there's a really wide variety of kind

00:03:53,120 --> 00:03:58,879
of applications for these techniques

00:03:56,560 --> 00:04:00,400
so if we want to get started we have

00:03:58,879 --> 00:04:03,760
links to our documentation

00:04:00,400 --> 00:04:05,680
page from a github desk image project

00:04:03,760 --> 00:04:07,760
and you can install dusk image from

00:04:05,680 --> 00:04:09,760
condaforge or from pip

00:04:07,760 --> 00:04:12,000
whichever your preferred package manager

00:04:09,760 --> 00:04:14,239
is

00:04:12,000 --> 00:04:15,840
we have five different sub packages

00:04:14,239 --> 00:04:19,040
within dark image

00:04:15,840 --> 00:04:22,000
the first is imread for

00:04:19,040 --> 00:04:24,320
reading in image data this pack this

00:04:22,000 --> 00:04:26,240
part of dark image is built on top of a

00:04:24,320 --> 00:04:28,880
python library called pims

00:04:26,240 --> 00:04:30,960
pims is excellent for lazy loading of

00:04:28,880 --> 00:04:32,479
images and it's really quite flexible

00:04:30,960 --> 00:04:35,040
and allows you

00:04:32,479 --> 00:04:36,479
to read in lots and lots of different

00:04:35,040 --> 00:04:40,160
types of file formats

00:04:36,479 --> 00:04:42,240
um all the way from like video files

00:04:40,160 --> 00:04:43,759
all sorts of interesting things right

00:04:42,240 --> 00:04:44,800
the way through to

00:04:43,759 --> 00:04:47,120
things that might be like more

00:04:44,800 --> 00:04:49,600
proprietary uh

00:04:47,120 --> 00:04:51,280
microscopy and data file formats that

00:04:49,600 --> 00:04:54,080
you might typically only be able to

00:04:51,280 --> 00:04:56,160
open with something like bio formats so

00:04:54,080 --> 00:04:58,720
there's a lot of choice there

00:04:56,160 --> 00:05:00,080
the remaining four sub-packages are

00:04:58,720 --> 00:05:02,080
really focused around

00:05:00,080 --> 00:05:04,000
processing that data or what we do with

00:05:02,080 --> 00:05:05,280
it after we've turned it into a desk

00:05:04,000 --> 00:05:07,520
array

00:05:05,280 --> 00:05:09,520
so we have an nd filters package that

00:05:07,520 --> 00:05:11,120
contains things like gaussian filters

00:05:09,520 --> 00:05:13,680
median filters

00:05:11,120 --> 00:05:15,280
convolves correlations a fourier

00:05:13,680 --> 00:05:17,520
transform package with

00:05:15,280 --> 00:05:19,600
a bunch of fourier transforms a

00:05:17,520 --> 00:05:21,840
morphological like endymorph is our

00:05:19,600 --> 00:05:23,360
morphological operations package

00:05:21,840 --> 00:05:25,360
and this has a lot of binary

00:05:23,360 --> 00:05:27,039
morphological operations that we might

00:05:25,360 --> 00:05:30,160
use to tidy up mask

00:05:27,039 --> 00:05:31,919
images and an nd measure package

00:05:30,160 --> 00:05:33,680
which really has a lot of measurement

00:05:31,919 --> 00:05:36,560
functions

00:05:33,680 --> 00:05:36,560
that we can use

00:05:37,440 --> 00:05:41,440
so we have on our documentation page

00:05:40,320 --> 00:05:43,039
this nice table

00:05:41,440 --> 00:05:44,479
that really kind of gives a clear

00:05:43,039 --> 00:05:47,039
overview of

00:05:44,479 --> 00:05:48,160
the function coverage we have relative

00:05:47,039 --> 00:05:51,520
to scipy's

00:05:48,160 --> 00:05:53,280
nd image functionality what we're trying

00:05:51,520 --> 00:05:56,400
to do here is really provide

00:05:53,280 --> 00:05:58,800
a very familiar api for people so in

00:05:56,400 --> 00:06:00,160
many cases we're kind of mimicking

00:05:58,800 --> 00:06:03,039
in almost all cases really we're

00:06:00,160 --> 00:06:05,600
mimicking the api of sci-fi nd image

00:06:03,039 --> 00:06:08,160
and just allowing that kind of scaling

00:06:05,600 --> 00:06:08,160
to happen

00:06:08,479 --> 00:06:12,720
i'm really excited to announce to you

00:06:11,199 --> 00:06:14,960
all today

00:06:12,720 --> 00:06:17,199
that in their latest release of dark

00:06:14,960 --> 00:06:19,360
image we actually have included

00:06:17,199 --> 00:06:21,199
support for arrays and computation on

00:06:19,360 --> 00:06:24,000
the gpu

00:06:21,199 --> 00:06:25,039
so this is really fantastic we have that

00:06:24,000 --> 00:06:27,280
support

00:06:25,039 --> 00:06:28,479
available in two of the sub packages at

00:06:27,280 --> 00:06:30,960
the moment

00:06:28,479 --> 00:06:31,919
the nd filters package that i spoke

00:06:30,960 --> 00:06:35,120
about earlier

00:06:31,919 --> 00:06:35,919
and our in read package we're able to do

00:06:35,120 --> 00:06:39,440
this by

00:06:35,919 --> 00:06:42,560
using a python library called coupey

00:06:39,440 --> 00:06:44,639
which is has a numpy like api

00:06:42,560 --> 00:06:47,039
but works with cuda in order to do

00:06:44,639 --> 00:06:50,400
computation of array data on

00:06:47,039 --> 00:06:52,240
uh on the gpu so instead of having dark

00:06:50,400 --> 00:06:55,199
arrays where each individual chunks are

00:06:52,240 --> 00:06:56,319
numpy based we can have dark arrays

00:06:55,199 --> 00:06:59,840
where those chunks

00:06:56,319 --> 00:07:01,599
occupy instead we're going to roll out

00:06:59,840 --> 00:07:04,000
increased support across the rest of the

00:07:01,599 --> 00:07:06,400
sub packages in dusk image

00:07:04,000 --> 00:07:07,680
and we're quite a way towards a little

00:07:06,400 --> 00:07:10,240
bit of that now particularly

00:07:07,680 --> 00:07:11,120
uh in the morphological operations sub

00:07:10,240 --> 00:07:12,400
package

00:07:11,120 --> 00:07:14,800
but there's still a bit of work that

00:07:12,400 --> 00:07:17,599
needs to be done both in dusk itself and

00:07:14,800 --> 00:07:19,360
in coupe in order to sort of support

00:07:17,599 --> 00:07:22,160
that roll out so that's what we're

00:07:19,360 --> 00:07:22,160
working on now

00:07:22,560 --> 00:07:27,280
but how much does it help really

00:07:25,599 --> 00:07:28,720
we want a bit of a speed up by using the

00:07:27,280 --> 00:07:30,800
gpu but

00:07:28,720 --> 00:07:32,000
how much is it worth it well matthew

00:07:30,800 --> 00:07:34,160
rocklin

00:07:32,000 --> 00:07:36,960
wrote a blog post a while back where he

00:07:34,160 --> 00:07:39,520
randomly generated two terabytes of data

00:07:36,960 --> 00:07:41,120
and ran some computation and compared it

00:07:39,520 --> 00:07:45,360
on a single cpu

00:07:41,120 --> 00:07:48,319
multiple cpus a gpu and multiple gpus

00:07:45,360 --> 00:07:49,120
and found that the kind of computation

00:07:48,319 --> 00:07:52,080
that was taking

00:07:49,120 --> 00:07:53,120
over two and a half hours on a single

00:07:52,080 --> 00:07:55,199
cpu

00:07:53,120 --> 00:07:56,400
got a really significant speed up just

00:07:55,199 --> 00:08:00,639
by moving that onto

00:07:56,400 --> 00:08:03,680
one gpu and when extended extended

00:08:00,639 --> 00:08:05,120
to 8 gpus like a small cluster

00:08:03,680 --> 00:08:06,879
this thing that was taking two and a

00:08:05,120 --> 00:08:09,120
half hours suddenly down to

00:08:06,879 --> 00:08:11,919
under 20 minutes so there's some really

00:08:09,120 --> 00:08:13,599
exciting performance gains we can get by

00:08:11,919 --> 00:08:15,440
adding this kind of gpu support

00:08:13,599 --> 00:08:18,479
which is why i'm excited to announce

00:08:15,440 --> 00:08:20,080
like the very beginnings of that

00:08:18,479 --> 00:08:22,800
so you've heard all about it you're

00:08:20,080 --> 00:08:26,479
probably excited now you want to try it

00:08:22,800 --> 00:08:28,639
so great let's build a pipeline um

00:08:26,479 --> 00:08:30,639
we'll do that now let's take a bunch of

00:08:28,639 --> 00:08:31,599
images we'll read in some data we'll do

00:08:30,639 --> 00:08:33,919
some filtering

00:08:31,599 --> 00:08:35,599
segment some objects from those images

00:08:33,919 --> 00:08:38,839
do some morphological operations and

00:08:35,599 --> 00:08:42,159
then we'll measure something about those

00:08:38,839 --> 00:08:45,440
objects um

00:08:42,159 --> 00:08:47,519
i'm going to use the publicly available

00:08:45,440 --> 00:08:49,120
image data set from the broad bioimage

00:08:47,519 --> 00:08:52,480
benchmark collection

00:08:49,120 --> 00:08:54,080
and this data set um is a lot of single

00:08:52,480 --> 00:08:56,160
like 2d image frames

00:08:54,080 --> 00:08:58,160
of fluorescence microscopy data where

00:08:56,160 --> 00:09:00,320
the nuclei have been stained so those

00:08:58,160 --> 00:09:04,080
are the bright objects that we see here

00:09:00,320 --> 00:09:04,080
the nuclear individual cells

00:09:06,080 --> 00:09:10,000
so to read in data we're going to use

00:09:08,160 --> 00:09:13,519
our inread module

00:09:10,000 --> 00:09:16,720
um and i can pass it

00:09:13,519 --> 00:09:18,959
a file path to like a particular folder

00:09:16,720 --> 00:09:21,200
and this wildcard.tiff so i have a

00:09:18,959 --> 00:09:23,279
folder it's full of images i want

00:09:21,200 --> 00:09:24,320
all of the tiff images to be put in this

00:09:23,279 --> 00:09:27,519
task array

00:09:24,320 --> 00:09:29,519
and by default uh now every image

00:09:27,519 --> 00:09:31,760
file was an individual file on disk is

00:09:29,519 --> 00:09:34,080
now a separate chunk in the dark array

00:09:31,760 --> 00:09:35,040
if i wanted my array to be sitting on

00:09:34,080 --> 00:09:37,040
the gpu

00:09:35,040 --> 00:09:40,640
with coupe i could specify this with

00:09:37,040 --> 00:09:40,640
this array type argument

00:09:41,600 --> 00:09:46,959
so when we want to do segmentations a

00:09:44,720 --> 00:09:49,279
really common pre-processing operation

00:09:46,959 --> 00:09:50,080
is to filter images and blur them very

00:09:49,279 --> 00:09:53,200
slightly

00:09:50,080 --> 00:09:55,120
before we do that segmentation

00:09:53,200 --> 00:09:57,360
this just means that we usually end up

00:09:55,120 --> 00:10:00,560
with like nicer smoother

00:09:57,360 --> 00:10:01,839
edges to our masks and it's just a

00:10:00,560 --> 00:10:04,480
really kind of useful

00:10:01,839 --> 00:10:06,240
first pre-processing step i'll use a

00:10:04,480 --> 00:10:07,680
small gaussian filter from the undo

00:10:06,240 --> 00:10:12,240
filters package

00:10:07,680 --> 00:10:12,240
and we can do that here

00:10:13,519 --> 00:10:17,600
now in order to segment objects from the

00:10:16,480 --> 00:10:20,399
background we could

00:10:17,600 --> 00:10:21,920
do something that's just like an

00:10:20,399 --> 00:10:24,320
absolute threshold

00:10:21,920 --> 00:10:25,519
and just say that say everything below

00:10:24,320 --> 00:10:27,360
this threshold value

00:10:25,519 --> 00:10:28,640
it's part of the background everything

00:10:27,360 --> 00:10:30,880
above it is

00:10:28,640 --> 00:10:33,519
something that we think is part of the

00:10:30,880 --> 00:10:35,360
objects that we're interested in

00:10:33,519 --> 00:10:37,519
it's maybe easier to talk about this if

00:10:35,360 --> 00:10:40,720
i show you this so i'm going to open

00:10:37,519 --> 00:10:44,000
the nepari image viewer and

00:10:40,720 --> 00:10:46,640
show you what that looks like

00:10:44,000 --> 00:10:47,279
with our data set so here is the data

00:10:46,640 --> 00:10:49,360
set

00:10:47,279 --> 00:10:51,279
that we're looking at with that

00:10:49,360 --> 00:10:54,640
fluorescent nuclei

00:10:51,279 --> 00:10:56,640
data a lot of different image frames

00:10:54,640 --> 00:10:59,040
looking very interesting and if i turn

00:10:56,640 --> 00:11:01,760
that off and just look at this threshold

00:10:59,040 --> 00:11:02,320
image that we've produced we can see

00:11:01,760 --> 00:11:05,760
here

00:11:02,320 --> 00:11:08,160
it's looking pretty nice but actually

00:11:05,760 --> 00:11:09,440
one single threshold is not fitting all

00:11:08,160 --> 00:11:10,240
of these images some of them look

00:11:09,440 --> 00:11:12,079
alright

00:11:10,240 --> 00:11:13,360
some of them less alright some of them

00:11:12,079 --> 00:11:15,200
really terrible so

00:11:13,360 --> 00:11:17,440
you know what i think we can improve

00:11:15,200 --> 00:11:17,440
this

00:11:17,680 --> 00:11:21,519
so maybe something that's going to work

00:11:19,839 --> 00:11:23,680
a lot better than a single number

00:11:21,519 --> 00:11:25,920
that we expect to work for everything

00:11:23,680 --> 00:11:27,279
could be using a local thresholding

00:11:25,920 --> 00:11:29,519
method

00:11:27,279 --> 00:11:31,200
and so we have one of those available

00:11:29,519 --> 00:11:34,320
which is great

00:11:31,200 --> 00:11:35,760
and we might want to try it so with

00:11:34,320 --> 00:11:37,760
local thresholding

00:11:35,760 --> 00:11:40,560
what you do is essentially choose a

00:11:37,760 --> 00:11:42,560
method and you choose a region

00:11:40,560 --> 00:11:44,720
or a spatial area that you want to kind

00:11:42,560 --> 00:11:48,000
of apply this to

00:11:44,720 --> 00:11:49,360
because each of um the image frames are

00:11:48,000 --> 00:11:50,320
separate and there's quite a lot of

00:11:49,360 --> 00:11:53,440
variability

00:11:50,320 --> 00:11:55,440
in the intensity for each uh

00:11:53,440 --> 00:11:56,959
we're going to make our local threshold

00:11:55,440 --> 00:11:59,200
local purely to like

00:11:56,959 --> 00:12:01,040
each image frame so we'll calculate

00:11:59,200 --> 00:12:01,920
thresholds separately for each of those

00:12:01,040 --> 00:12:04,800
images

00:12:01,920 --> 00:12:04,800
and take a look

00:12:05,600 --> 00:12:09,440
so we'll do that let's let's load it and

00:12:08,639 --> 00:12:13,680
see

00:12:09,440 --> 00:12:13,680
um i think that could be good

00:12:14,399 --> 00:12:20,639
this is interesting

00:12:17,440 --> 00:12:22,160
um yeah and so i'll turn that off for a

00:12:20,639 --> 00:12:23,519
second and just make it invisible so

00:12:22,160 --> 00:12:24,399
we'll compare it to what we had

00:12:23,519 --> 00:12:27,120
originally

00:12:24,399 --> 00:12:28,880
which is our like absolute threshold

00:12:27,120 --> 00:12:30,399
working well for some not working well

00:12:28,880 --> 00:12:32,639
for others

00:12:30,399 --> 00:12:33,600
and compare that to the thresholded

00:12:32,639 --> 00:12:36,000
image

00:12:33,600 --> 00:12:37,279
using this local threshold method and

00:12:36,000 --> 00:12:41,440
you know that's looking

00:12:37,279 --> 00:12:41,440
a lot better at least to my eyes

00:12:41,839 --> 00:12:45,200
so i'd say we're happy with that and

00:12:43,920 --> 00:12:49,680
what we want to do next

00:12:45,200 --> 00:12:52,000
is to tidy up some of this mask

00:12:49,680 --> 00:12:53,600
that we've made using some morphological

00:12:52,000 --> 00:12:55,440
operations

00:12:53,600 --> 00:12:56,639
morphological operations are maybe a

00:12:55,440 --> 00:12:57,200
little bit weird to wrap your head

00:12:56,639 --> 00:12:59,680
around

00:12:57,200 --> 00:13:01,360
if you're not using them a lot so i'm

00:12:59,680 --> 00:13:04,079
going to just take a moment

00:13:01,360 --> 00:13:04,800
and a few diagrams from the opencv

00:13:04,079 --> 00:13:07,120
webpage

00:13:04,800 --> 00:13:08,480
in order to just talk through some of

00:13:07,120 --> 00:13:10,800
the concepts here

00:13:08,480 --> 00:13:11,680
so we'll talk about uh what erosion

00:13:10,800 --> 00:13:13,600
dilation

00:13:11,680 --> 00:13:16,320
and then opening up and then we'll try

00:13:13,600 --> 00:13:18,639
that on some of our data

00:13:16,320 --> 00:13:19,519
so an erosion is a type of morphological

00:13:18,639 --> 00:13:22,720
operation

00:13:19,519 --> 00:13:23,920
where we have a binary image so pixels

00:13:22,720 --> 00:13:25,600
are either black or they're white

00:13:23,920 --> 00:13:28,240
they're true or they're false

00:13:25,600 --> 00:13:28,880
and what it does when we apply it is eat

00:13:28,240 --> 00:13:32,839
away

00:13:28,880 --> 00:13:34,160
at the edges of of those objects in the

00:13:32,839 --> 00:13:35,519
image

00:13:34,160 --> 00:13:37,120
so we can see that it looks like

00:13:35,519 --> 00:13:39,040
everything's kind of shrinking around

00:13:37,120 --> 00:13:42,240
this letter j

00:13:39,040 --> 00:13:45,279
a dilation is the opposite operation

00:13:42,240 --> 00:13:47,279
essentially so instead of shrinking away

00:13:45,279 --> 00:13:48,000
the edges of an object we're expanding

00:13:47,279 --> 00:13:49,600
that object

00:13:48,000 --> 00:13:51,920
out it's kind of blowing up a little bit

00:13:49,600 --> 00:13:53,519
like a balloon

00:13:51,920 --> 00:13:55,440
things start to get interesting when we

00:13:53,519 --> 00:13:58,880
combine these operations in different

00:13:55,440 --> 00:14:00,320
ways and so what we call a morphological

00:13:58,880 --> 00:14:02,560
opening operation

00:14:00,320 --> 00:14:03,519
is a combination of these two things uh

00:14:02,560 --> 00:14:05,680
so what that is is

00:14:03,519 --> 00:14:06,959
a one erosion and then we follow it by a

00:14:05,680 --> 00:14:09,360
dilation

00:14:06,959 --> 00:14:11,519
and so what that means here we can see

00:14:09,360 --> 00:14:12,720
on the left with this j we have a lot of

00:14:11,519 --> 00:14:14,399
like bright pixels

00:14:12,720 --> 00:14:17,279
single sort of spots hanging out in the

00:14:14,399 --> 00:14:20,959
background there and if we do an erosion

00:14:17,279 --> 00:14:24,000
on this image what happens is that

00:14:20,959 --> 00:14:25,360
if the bright spot in this background if

00:14:24,000 --> 00:14:26,639
it's smaller than the structuring

00:14:25,360 --> 00:14:27,600
element that we're using for that

00:14:26,639 --> 00:14:29,600
erosion

00:14:27,600 --> 00:14:32,320
it'll disappear and become completely

00:14:29,600 --> 00:14:35,360
black so by the time we get around to

00:14:32,320 --> 00:14:36,880
doing our dilation which effectively

00:14:35,360 --> 00:14:39,279
restores

00:14:36,880 --> 00:14:41,120
the main object in this image to kind of

00:14:39,279 --> 00:14:43,040
what it originally was

00:14:41,120 --> 00:14:45,120
those tiny little background blobs

00:14:43,040 --> 00:14:46,880
aren't there anymore to be expanded

00:14:45,120 --> 00:14:49,199
so what we've really done is like

00:14:46,880 --> 00:14:52,720
cleaned up a lot of the noise

00:14:49,199 --> 00:14:55,600
that we might have had around our signal

00:14:52,720 --> 00:14:56,800
so i think we should try one of these

00:14:55,600 --> 00:14:59,600
morphological

00:14:56,800 --> 00:15:01,199
operations we'll do a binary opening on

00:14:59,600 --> 00:15:04,959
our mask image

00:15:01,199 --> 00:15:07,600
and because all uh dusk image

00:15:04,959 --> 00:15:08,800
operations are n-dimensional by default

00:15:07,600 --> 00:15:10,399
what that means is they're

00:15:08,800 --> 00:15:12,079
going to assume that whatever dash array

00:15:10,399 --> 00:15:14,959
you hand them is all like one

00:15:12,079 --> 00:15:16,079
thing and that's great but it's actually

00:15:14,959 --> 00:15:18,240
not strictly true

00:15:16,079 --> 00:15:19,519
in the case we have here in the case

00:15:18,240 --> 00:15:23,120
i've shown you

00:15:19,519 --> 00:15:24,720
uh every every image frame

00:15:23,120 --> 00:15:26,399
is kind of separate but we're storing

00:15:24,720 --> 00:15:29,600
them all

00:15:26,399 --> 00:15:30,880
in a single dark array so what i've done

00:15:29,600 --> 00:15:32,480
to get around that is just be kind of

00:15:30,880 --> 00:15:34,160
careful about the structuring element

00:15:32,480 --> 00:15:36,880
that i'm using in this case

00:15:34,160 --> 00:15:38,160
where i have what the default 2d

00:15:36,880 --> 00:15:41,040
structuring element

00:15:38,160 --> 00:15:42,800
is and i've just bookended that by like

00:15:41,040 --> 00:15:45,600
zeros on either side

00:15:42,800 --> 00:15:46,560
so that means when we do this processing

00:15:45,600 --> 00:15:48,639
um

00:15:46,560 --> 00:15:51,360
each image frame is kind of counted

00:15:48,639 --> 00:15:51,360
individually

00:15:52,240 --> 00:15:57,759
so fantastic we have almost

00:15:55,440 --> 00:15:58,800
our pipeline complete what we're going

00:15:57,759 --> 00:16:00,320
to do now is

00:15:58,800 --> 00:16:02,399
measure something interesting about

00:16:00,320 --> 00:16:05,040
those objects that we've segmented

00:16:02,399 --> 00:16:06,320
now that we're happy with it so just in

00:16:05,040 --> 00:16:07,680
the interest of keeping this

00:16:06,320 --> 00:16:11,600
demonstration

00:16:07,680 --> 00:16:14,240
nice and quick um i'm probably going to

00:16:11,600 --> 00:16:16,320
just measure a very small subset of the

00:16:14,240 --> 00:16:18,399
entire data set here

00:16:16,320 --> 00:16:20,720
we have hundreds of images in this like

00:16:18,399 --> 00:16:24,000
fairly small example data set but

00:16:20,720 --> 00:16:26,800
each of those images really does contain

00:16:24,000 --> 00:16:27,759
uh like maybe around 100 nuclei each on

00:16:26,800 --> 00:16:31,199
average

00:16:27,759 --> 00:16:33,360
um so now that we're moving from

00:16:31,199 --> 00:16:34,399
what was operations happening on the

00:16:33,360 --> 00:16:36,560
whole images to

00:16:34,399 --> 00:16:38,399
operations that need to happen on each

00:16:36,560 --> 00:16:40,240
individual object

00:16:38,399 --> 00:16:41,759
we're just going to like cut down the

00:16:40,240 --> 00:16:43,120
size of our data that we're actually

00:16:41,759 --> 00:16:44,880
doing this on

00:16:43,120 --> 00:16:47,920
so that we can see this in a reasonable

00:16:44,880 --> 00:16:50,560
amount of time

00:16:47,920 --> 00:16:51,279
so in order to measure these objects

00:16:50,560 --> 00:16:54,560
we're first

00:16:51,279 --> 00:16:57,279
going to have to create a label image

00:16:54,560 --> 00:16:58,959
from the mask that we have and what a

00:16:57,279 --> 00:17:02,639
label image is

00:16:58,959 --> 00:17:04,959
is uh if we take a mask and we look

00:17:02,639 --> 00:17:06,400
it has a lot of bright pixels and then a

00:17:04,959 --> 00:17:07,280
black background and some other bright

00:17:06,400 --> 00:17:10,319
pixels

00:17:07,280 --> 00:17:12,640
each of those individual isolated blobs

00:17:10,319 --> 00:17:13,600
are labeled with a different integer

00:17:12,640 --> 00:17:18,799
value

00:17:13,600 --> 00:17:21,120
so this means we can separate what um

00:17:18,799 --> 00:17:22,720
which pixels are part of which objects

00:17:21,120 --> 00:17:25,120
uh in a really kind of easy and

00:17:22,720 --> 00:17:27,280
identifiable way

00:17:25,120 --> 00:17:28,799
so we're going to do this just for the

00:17:27,280 --> 00:17:31,919
first couple of frames we can see

00:17:28,799 --> 00:17:36,000
even there we have like almost 300

00:17:31,919 --> 00:17:38,320
nuclear that are being picked out and

00:17:36,000 --> 00:17:39,039
what we can do now that we have a label

00:17:38,320 --> 00:17:42,240
image

00:17:39,039 --> 00:17:44,840
is actually make some measurements on

00:17:42,240 --> 00:17:46,400
each of each of the things that we've

00:17:44,840 --> 00:17:48,400
labeled

00:17:46,400 --> 00:17:50,240
i'll just show you first before we do

00:17:48,400 --> 00:17:52,160
that what those labels look like

00:17:50,240 --> 00:17:53,440
just so we have a bit of a sense of kind

00:17:52,160 --> 00:17:56,640
of what we

00:17:53,440 --> 00:18:00,480
are doing but maybe not

00:17:56,640 --> 00:18:02,320
maybe napari has had a little hissy yet

00:18:00,480 --> 00:18:04,400
um all right so so this is our label

00:18:02,320 --> 00:18:07,520
image um

00:18:04,400 --> 00:18:11,840
and kind of this is what we are

00:18:07,520 --> 00:18:11,840
um going to going to use

00:18:12,000 --> 00:18:16,880
alrighty so the very last part of our

00:18:15,520 --> 00:18:19,039
processing pipeline

00:18:16,880 --> 00:18:21,120
is to use those labels and use the

00:18:19,039 --> 00:18:21,679
original images and we want to measure

00:18:21,120 --> 00:18:24,400
stuff

00:18:21,679 --> 00:18:26,880
about each of the labels sets of pixels

00:18:24,400 --> 00:18:28,640
that we have

00:18:26,880 --> 00:18:30,080
like i said earlier we have a lot of

00:18:28,640 --> 00:18:32,080
different uh

00:18:30,080 --> 00:18:33,919
uh measurement functions available in

00:18:32,080 --> 00:18:35,679
our nd measure package

00:18:33,919 --> 00:18:37,440
um and we're just going to pick a

00:18:35,679 --> 00:18:41,120
handful of them today

00:18:37,440 --> 00:18:43,520
we will measure the area in pixels

00:18:41,120 --> 00:18:44,320
of each of each of these segmented

00:18:43,520 --> 00:18:46,240
objects

00:18:44,320 --> 00:18:47,440
and we'll also measure uh like the

00:18:46,240 --> 00:18:51,039
average intensity

00:18:47,440 --> 00:18:52,640
for each of those as well

00:18:51,039 --> 00:18:55,440
and that would be something that's like

00:18:52,640 --> 00:19:00,559
a fairly straightforward fairly common

00:18:55,440 --> 00:19:00,559
to do in an image processing pipeline

00:19:01,600 --> 00:19:05,039
so i'm going to go ahead and just like

00:19:04,640 --> 00:19:08,000
plot

00:19:05,039 --> 00:19:09,520
some of this data remember that up until

00:19:08,000 --> 00:19:11,600
now we haven't actually run

00:19:09,520 --> 00:19:14,160
our computation what we've been doing is

00:19:11,600 --> 00:19:16,960
constructing the task task graph

00:19:14,160 --> 00:19:18,880
which is essentially a map for okay this

00:19:16,960 --> 00:19:19,679
is what computation i want you to run

00:19:18,880 --> 00:19:22,160
when

00:19:19,679 --> 00:19:24,240
um but hold off i'm actually doing that

00:19:22,160 --> 00:19:26,640
until i tell you to um

00:19:24,240 --> 00:19:27,520
which is a really fantastic way to be

00:19:26,640 --> 00:19:29,600
able to then

00:19:27,520 --> 00:19:30,640
um kind of seamlessly kind of move

00:19:29,600 --> 00:19:32,880
between like a

00:19:30,640 --> 00:19:33,919
an analysis you might do on your single

00:19:32,880 --> 00:19:36,480
laptop

00:19:33,919 --> 00:19:37,919
up to uh being able to just scale that

00:19:36,480 --> 00:19:39,679
out over something like a super

00:19:37,919 --> 00:19:43,600
computing cluster

00:19:39,679 --> 00:19:44,160
or onto like a very sort of wide range

00:19:43,600 --> 00:19:47,280
of

00:19:44,160 --> 00:19:49,120
compute resources but now when we

00:19:47,280 --> 00:19:52,240
actually want to see what the data is

00:19:49,120 --> 00:19:54,240
um and i want to plot some of this

00:19:52,240 --> 00:19:56,400
on a graph it's going to do two things

00:19:54,240 --> 00:19:59,440
it's going to run the actual computation

00:19:56,400 --> 00:20:02,000
and then plot the results so

00:19:59,440 --> 00:20:02,799
i'm just going to make a graph here um

00:20:02,000 --> 00:20:05,840
and

00:20:02,799 --> 00:20:09,200
we might just show you something that's

00:20:05,840 --> 00:20:10,480
say a graph of area in pixels along the

00:20:09,200 --> 00:20:13,360
x-axis

00:20:10,480 --> 00:20:14,559
compared with on the y-axis the average

00:20:13,360 --> 00:20:17,760
intensity

00:20:14,559 --> 00:20:18,400
in each of these nuclei so i can see

00:20:17,760 --> 00:20:20,159
here

00:20:18,400 --> 00:20:22,000
that maybe this is something that i

00:20:20,159 --> 00:20:24,640
would more or less expect

00:20:22,000 --> 00:20:25,760
there's a lot of clustering uh in this

00:20:24,640 --> 00:20:28,400
region here

00:20:25,760 --> 00:20:29,360
um so a lot of a lot of cells looking

00:20:28,400 --> 00:20:32,320
very similar

00:20:29,360 --> 00:20:33,039
with similar brightnesses similar sizes

00:20:32,320 --> 00:20:36,240
a few different

00:20:33,039 --> 00:20:36,799
outliers um in different different

00:20:36,240 --> 00:20:38,559
places

00:20:36,799 --> 00:20:40,880
uh that may or may not be unexpected

00:20:38,559 --> 00:20:42,720
that we could sort of dive into

00:20:40,880 --> 00:20:44,480
and it gives us a lot more kind of

00:20:42,720 --> 00:20:45,840
information

00:20:44,480 --> 00:20:47,840
if we were running this on a really

00:20:45,840 --> 00:20:49,760
really big data set uh maybe we wouldn't

00:20:47,840 --> 00:20:51,840
want to plot every individual point

00:20:49,760 --> 00:20:52,880
but at this this stage uh now that our

00:20:51,840 --> 00:20:55,919
pipeline is finished

00:20:52,880 --> 00:20:58,720
we might do things like uh put that

00:20:55,919 --> 00:20:59,679
data into a data frame or run aggregate

00:20:58,720 --> 00:21:04,400
statistics

00:20:59,679 --> 00:21:04,400
or do something else for our analysis

00:21:05,600 --> 00:21:09,840
so the full pipeline is maybe just a

00:21:08,080 --> 00:21:13,280
little bit too long to show

00:21:09,840 --> 00:21:16,720
on this single uh image slide but

00:21:13,280 --> 00:21:20,480
it is honestly quite succinct

00:21:16,720 --> 00:21:23,280
and the real benefit here is that

00:21:20,480 --> 00:21:25,120
unlike say an analysis that has a bunch

00:21:23,280 --> 00:21:27,840
of nested for loops

00:21:25,120 --> 00:21:30,080
it really allows you to very easily

00:21:27,840 --> 00:21:32,320
inspect intermediate stages

00:21:30,080 --> 00:21:33,120
and just intermediate portions of your

00:21:32,320 --> 00:21:35,280
data

00:21:33,120 --> 00:21:36,960
without having to run the whole thing or

00:21:35,280 --> 00:21:38,240
run a lot and then stop at a particular

00:21:36,960 --> 00:21:41,520
point

00:21:38,240 --> 00:21:42,480
so this is this is really huge so that's

00:21:41,520 --> 00:21:44,080
great

00:21:42,480 --> 00:21:45,919
but what happens if you want to do

00:21:44,080 --> 00:21:46,880
something that isn't included in dark

00:21:45,919 --> 00:21:49,440
image

00:21:46,880 --> 00:21:50,960
well if you're using psychic image which

00:21:49,440 --> 00:21:52,080
is very typical for a lot of image

00:21:50,960 --> 00:21:54,080
processing people

00:21:52,080 --> 00:21:55,520
you can use their apply parallel

00:21:54,080 --> 00:21:57,679
function

00:21:55,520 --> 00:21:58,960
and what this does is essentially give

00:21:57,679 --> 00:22:02,799
you a nice way to

00:21:58,960 --> 00:22:04,000
put your data into a desk array and use

00:22:02,799 --> 00:22:06,320
map overlap

00:22:04,000 --> 00:22:08,720
you can use map overlap from dusk or map

00:22:06,320 --> 00:22:11,520
blocks directly yourself

00:22:08,720 --> 00:22:13,679
which is what we do in dusk image that's

00:22:11,520 --> 00:22:15,679
what our functions are based on

00:22:13,679 --> 00:22:17,520
or if you want like a lot of flexibility

00:22:15,679 --> 00:22:19,120
around what it is that you're doing and

00:22:17,520 --> 00:22:20,799
a lot of fine grained control

00:22:19,120 --> 00:22:23,520
then you can use the dash delayed

00:22:20,799 --> 00:22:24,840
decorator as well

00:22:23,520 --> 00:22:27,440
when you want to scale up your

00:22:24,840 --> 00:22:29,120
computation from a laptop to say a

00:22:27,440 --> 00:22:32,000
supercomputing cloud

00:22:29,120 --> 00:22:33,200
the das distributed library is what lets

00:22:32,000 --> 00:22:35,840
you do that

00:22:33,200 --> 00:22:36,799
and you can import like a client set up

00:22:35,840 --> 00:22:38,400
a cluster

00:22:36,799 --> 00:22:41,280
it's all very easy happens at the start

00:22:38,400 --> 00:22:43,840
of your file

00:22:41,280 --> 00:22:46,000
so fantastic so if you want to get

00:22:43,840 --> 00:22:47,039
started with dusk image or try some of

00:22:46,000 --> 00:22:49,120
this out

00:22:47,039 --> 00:22:51,039
you can install it with condor or pip we

00:22:49,120 --> 00:22:51,919
have our documentation available at read

00:22:51,039 --> 00:22:54,159
the docs

00:22:51,919 --> 00:22:55,520
and you can swing by our github page to

00:22:54,159 --> 00:22:57,760
check that out

00:22:55,520 --> 00:22:58,960
i'm happy to answer some questions here

00:22:57,760 --> 00:23:00,799
maybe uh

00:22:58,960 --> 00:23:02,799
maybe even in the text in the chat later

00:23:00,799 --> 00:23:05,679
on i hope this was interesting and

00:23:02,799 --> 00:23:05,679
informative for you

00:23:06,000 --> 00:23:09,440
cool thanks so much for that genevieve

00:23:07,440 --> 00:23:09,919
that was great um yeah we do have time

00:23:09,440 --> 00:23:11,919
for

00:23:09,919 --> 00:23:14,400
one or maybe two questions um one from

00:23:11,919 --> 00:23:17,520
jeff in the cpu to gpu performance

00:23:14,400 --> 00:23:19,520
shootout is the gpu using 16 or 32-bit

00:23:17,520 --> 00:23:21,840
floating point precision

00:23:19,520 --> 00:23:23,039
if you happen to know i don't happen to

00:23:21,840 --> 00:23:25,919
know off the top of my head

00:23:23,039 --> 00:23:26,880
um i can shoot you a link to that blog

00:23:25,919 --> 00:23:28,559
post by

00:23:26,880 --> 00:23:29,919
matt rocklin which may or may not have

00:23:28,559 --> 00:23:32,640
the details in it

00:23:29,919 --> 00:23:36,080
um and if it doesn't i think maybe matt

00:23:32,640 --> 00:23:40,080
is the person to ask

00:23:36,080 --> 00:23:42,960
cool um and does a question from claire

00:23:40,080 --> 00:23:45,919
does opening change the main object like

00:23:42,960 --> 00:23:48,000
for example smoothing the edges

00:23:45,919 --> 00:23:49,039
yeah so it definitely can um

00:23:48,000 --> 00:23:51,840
particularly

00:23:49,039 --> 00:23:52,240
uh so this j that we had drawn has a lot

00:23:51,840 --> 00:23:54,880
of like

00:23:52,240 --> 00:23:56,480
loops and things that are closed so if

00:23:54,880 --> 00:23:59,440
your object is like very

00:23:56,480 --> 00:24:00,240
um convex or like a sphere or something

00:23:59,440 --> 00:24:02,000
like that you

00:24:00,240 --> 00:24:04,240
don't often see a lot of difference if

00:24:02,000 --> 00:24:07,919
you run an opening or you run a closing

00:24:04,240 --> 00:24:11,120
um but if you have um like bits where

00:24:07,919 --> 00:24:12,960
edges are maybe a bit like uh

00:24:11,120 --> 00:24:15,520
like a very involved coastline or

00:24:12,960 --> 00:24:17,200
they're curling back on one another

00:24:15,520 --> 00:24:20,880
then you can actually start to see

00:24:17,200 --> 00:24:23,520
differences happening there

00:24:20,880 --> 00:24:24,640
cool cool um there's one more question

00:24:23,520 --> 00:24:27,840
i'm going to um

00:24:24,640 --> 00:24:29,840
uh i'm going to read so it is from adam

00:24:27,840 --> 00:24:31,760
we're operating on satellite imagery and

00:24:29,840 --> 00:24:33,120
i have features that straddle chunks and

00:24:31,760 --> 00:24:34,960
are detected in one chunk

00:24:33,120 --> 00:24:36,400
and not the other leaving a half

00:24:34,960 --> 00:24:38,240
detected feature with a straight

00:24:36,400 --> 00:24:39,919
vertical line cutting it off

00:24:38,240 --> 00:24:41,360
if these operations are performed on a

00:24:39,919 --> 00:24:42,880
per chunk basis

00:24:41,360 --> 00:24:45,120
are there approaches that can be taken

00:24:42,880 --> 00:24:47,760
to prevent strange artifacts at the

00:24:45,120 --> 00:24:50,799
chunk boundaries

00:24:47,760 --> 00:24:52,960
um yes there are approaches no i don't

00:24:50,799 --> 00:24:55,120
think this is a solved problem

00:24:52,960 --> 00:24:57,120
at all it's certainly something very

00:24:55,120 --> 00:25:00,080
challenging um

00:24:57,120 --> 00:25:01,760
i'd be interested in talking to you more

00:25:00,080 --> 00:25:04,159
about it i think i'm more interested

00:25:01,760 --> 00:25:05,919
um than like the one or two minutes we

00:25:04,159 --> 00:25:09,360
maybe have left here

00:25:05,919 --> 00:25:11,360
um it's

00:25:09,360 --> 00:25:12,799
yeah like i said it's there are

00:25:11,360 --> 00:25:14,320
approaches you can take it's not a self

00:25:12,799 --> 00:25:16,080
problem entirely um

00:25:14,320 --> 00:25:17,279
but it's fascinating and i'd really like

00:25:16,080 --> 00:25:19,279
to talk to you more about it

00:25:17,279 --> 00:25:21,360
particularly uh so that we can maybe see

00:25:19,279 --> 00:25:22,640
if there's easy ways we can support this

00:25:21,360 --> 00:25:26,000
kind of use case better

00:25:22,640 --> 00:25:27,440
uh in dark image yeah cool so it sounds

00:25:26,000 --> 00:25:30,400
like there's a discussion to

00:25:27,440 --> 00:25:32,080
to continue on in venues just a reminder

00:25:30,400 --> 00:25:35,919
that we have both the um

00:25:32,080 --> 00:25:37,840
uh the the the um uh

00:25:35,919 --> 00:25:39,840
yes the stage conversation and then we

00:25:37,840 --> 00:25:41,760
also have our our science data

00:25:39,840 --> 00:25:43,120
uh separate thread so if it sort of goes

00:25:41,760 --> 00:25:45,440
beyond the length of

00:25:43,120 --> 00:25:46,320
of the proceedings um you can jump over

00:25:45,440 --> 00:25:47,919
there as well

00:25:46,320 --> 00:25:50,400
uh so thanks again so much for that

00:25:47,919 --> 00:25:53,200
genevieve that was great um

00:25:50,400 --> 00:25:54,000
and uh stick around uh in the next uh in

00:25:53,200 --> 00:25:56,000
temple

00:25:54,000 --> 00:25:58,320
short break and then back in 10 minutes

00:25:56,000 --> 00:26:00,240
and we'll be talking uh have a talk from

00:25:58,320 --> 00:26:02,080
patrick or botham on uh

00:26:00,240 --> 00:26:03,919
model selection with python an

00:26:02,080 --> 00:26:05,279
introduction to hyperparameter tuning so

00:26:03,919 --> 00:26:10,720
looking forward to that and see you all

00:26:05,279 --> 00:26:12,799
in a bit

00:26:10,720 --> 00:26:12,799

YouTube URL: https://www.youtube.com/watch?v=MpjgzNeISeI


