Title: "What The GIL Means For Containers" - Noah Kantrowitz (PyConline AU 2020)
Publication date: 2020-09-08
Playlist: PyConline AU 2020
Description: 
	Noah Kantrowitz

https://2020.pycon.org.au/program/SL3C7M

This talk covers a very basic introduction to the Python Global Interpreter Lock from the point of view of container operations and deployment, along with the knowledge and tools to architect high quality Python application infrastructure. This includes how various application servers interact with the GIL, what problems is causes from an ops-centric point of view, and best practices for running Python applications efficiently and at scale despite the limitations.

Produced by NDV: https://youtube.com/channel/UCQ7dFBzZGlBvtU2hCecsBBg?sub_confirmation=1

Python, PyCon, PyConAU, PyConline

Sat Sep  5 10:25:00 2020 at Python 2
Captions: 
	00:00:05,680 --> 00:00:08,080
hi there i'm noah kantrowitz i'm an

00:00:07,520 --> 00:00:09,920
operations

00:00:08,080 --> 00:00:11,280
engineer at geomagical labs and today

00:00:09,920 --> 00:00:12,639
we're going to talk about the gill

00:00:11,280 --> 00:00:14,960
containers and how to make awesome

00:00:12,639 --> 00:00:16,400
infrastructure before we jump into the

00:00:14,960 --> 00:00:18,560
operations let's lay out what the global

00:00:16,400 --> 00:00:20,240
interpreter lock actually is

00:00:18,560 --> 00:00:22,080
when we talk about locks in this context

00:00:20,240 --> 00:00:24,080
we mean a mutex it's a value in memory

00:00:22,080 --> 00:00:25,760
that tracks which thread owns the lock

00:00:24,080 --> 00:00:27,439
when two threads call locked out acquire

00:00:25,760 --> 00:00:29,039
at the same nanosecond only one of them

00:00:27,439 --> 00:00:30,480
is going to get it

00:00:29,039 --> 00:00:32,399
the gill behaves like a traditional

00:00:30,480 --> 00:00:33,920
blocking inquire if you try to get the

00:00:32,399 --> 00:00:35,440
lock and someone else has it you wait

00:00:33,920 --> 00:00:37,040
until it's available

00:00:35,440 --> 00:00:38,480
one slight complications the blocking

00:00:37,040 --> 00:00:39,920
weight has a timeout so we can do

00:00:38,480 --> 00:00:41,920
something special if the gill takes too

00:00:39,920 --> 00:00:43,440
long to acquire

00:00:41,920 --> 00:00:45,280
so here's a representation of a running

00:00:43,440 --> 00:00:47,280
python program boring simple

00:00:45,280 --> 00:00:48,960
single threaded the gill is running here

00:00:47,280 --> 00:00:50,320
in the background but because no other

00:00:48,960 --> 00:00:52,399
thread is trying to acquire it we can

00:00:50,320 --> 00:00:53,760
just pretend it doesn't exist

00:00:52,399 --> 00:00:55,520
so let's add a second thread spice

00:00:53,760 --> 00:00:56,960
things up in a perfect world

00:00:55,520 --> 00:00:58,640
these would run concurrently and the

00:00:56,960 --> 00:01:00,559
output would be a garbled mess

00:00:58,640 --> 00:01:02,000
but the gill prevents two python threads

00:01:00,559 --> 00:01:03,520
from running at the same time

00:01:02,000 --> 00:01:05,119
so while both threads are running in

00:01:03,520 --> 00:01:06,560
parallel only one of them can be making

00:01:05,119 --> 00:01:07,920
progress in a given moment

00:01:06,560 --> 00:01:09,600
this means that we see it swapping back

00:01:07,920 --> 00:01:10,840
and forth seemingly at random but a

00:01:09,600 --> 00:01:12,240
single call to print can't be

00:01:10,840 --> 00:01:13,760
interrupted

00:01:12,240 --> 00:01:15,360
you need the gill in order to run python

00:01:13,760 --> 00:01:17,119
code where does it come from the

00:01:15,360 --> 00:01:18,720
simplest case is the main thread starts

00:01:17,119 --> 00:01:19,920
with the gill locked when python loads

00:01:18,720 --> 00:01:20,640
otherwise you wouldn't be able to run

00:01:19,920 --> 00:01:22,000
anything

00:01:20,640 --> 00:01:23,840
but about a different thread that wants

00:01:22,000 --> 00:01:25,520
to run it'll need to acquire the kill

00:01:23,840 --> 00:01:27,439
this means blocking until the gill is

00:01:25,520 --> 00:01:28,720
available but there's a special case

00:01:27,439 --> 00:01:30,479
after it's been waiting for more than

00:01:28,720 --> 00:01:32,560
five milliseconds the requesting thread

00:01:30,479 --> 00:01:33,600
sets a global flag asking nicely for the

00:01:32,560 --> 00:01:35,920
guild to be released

00:01:33,600 --> 00:01:37,680
and then goes back to waiting once you

00:01:35,920 --> 00:01:39,040
have the gill you're running python code

00:01:37,680 --> 00:01:40,320
you eventually have to give it up so

00:01:39,040 --> 00:01:40,960
that some other threads get a chance to

00:01:40,320 --> 00:01:42,479
run

00:01:40,960 --> 00:01:44,320
anytime you make a potentially blocking

00:01:42,479 --> 00:01:46,000
syscall python automatically releases

00:01:44,320 --> 00:01:46,880
the gill for you and then reacquires it

00:01:46,000 --> 00:01:48,159
afterwards

00:01:46,880 --> 00:01:49,520
there's also that five millisecond

00:01:48,159 --> 00:01:51,119
interval check from before which allows

00:01:49,520 --> 00:01:53,119
for a graceful handoff to a waiting

00:01:51,119 --> 00:01:54,640
thread that wants to run

00:01:53,119 --> 00:01:56,479
a frequent example of that syscall

00:01:54,640 --> 00:01:57,920
behavior is making an http request

00:01:56,479 --> 00:01:59,399
somewhere way deep down inside the

00:01:57,920 --> 00:02:00,560
library code something is calling

00:01:59,399 --> 00:02:01,840
socket.receive

00:02:00,560 --> 00:02:03,600
which is going to block until the

00:02:01,840 --> 00:02:05,200
response arrives that could be quite a

00:02:03,600 --> 00:02:06,799
while in computer terms so we

00:02:05,200 --> 00:02:07,280
automatically release the gill while we

00:02:06,799 --> 00:02:09,039
wait

00:02:07,280 --> 00:02:10,720
in this example it's going to allow some

00:02:09,039 --> 00:02:13,760
prints to happen concurrently while

00:02:10,720 --> 00:02:15,200
waiting for the http response

00:02:13,760 --> 00:02:16,959
and there's one last complication i keep

00:02:15,200 --> 00:02:17,440
saying you need the guild to run python

00:02:16,959 --> 00:02:19,200
code

00:02:17,440 --> 00:02:20,560
because c extensions can do whatever

00:02:19,200 --> 00:02:22,239
they want there's

00:02:20,560 --> 00:02:23,760
a bunch of macros and functions in lib

00:02:22,239 --> 00:02:25,040
python to release and acquire the guild

00:02:23,760 --> 00:02:26,560
just like python does

00:02:25,040 --> 00:02:29,360
for the syscalls but it's up to the

00:02:26,560 --> 00:02:30,800
extension author to use them correctly

00:02:29,360 --> 00:02:32,640
all of this together gives us the four

00:02:30,800 --> 00:02:34,480
rules of the gill only one thread

00:02:32,640 --> 00:02:36,160
running python code at a given time

00:02:34,480 --> 00:02:37,920
calling a potentially blocking syscall

00:02:36,160 --> 00:02:39,040
will release the gill do the call and

00:02:37,920 --> 00:02:40,560
then acquire it again

00:02:39,040 --> 00:02:42,319
after five milliseconds with the gill

00:02:40,560 --> 00:02:43,840
check if someone else wants it and see

00:02:42,319 --> 00:02:45,680
extensions are a wild card they can do

00:02:43,840 --> 00:02:47,200
whatever they want

00:02:45,680 --> 00:02:48,319
so that's the basics of the guild plane

00:02:47,200 --> 00:02:49,680
scripts are nice when we're talking

00:02:48,319 --> 00:02:51,040
about production environments we usually

00:02:49,680 --> 00:02:52,720
mean web applications

00:02:51,040 --> 00:02:53,840
we'll talk about q workers like celery

00:02:52,720 --> 00:02:55,200
later but let's look at the basic

00:02:53,840 --> 00:02:56,480
anatomy of a web server

00:02:55,200 --> 00:02:59,040
and figure out where it interacts with

00:02:56,480 --> 00:03:00,959
the gill a web server has two major

00:02:59,040 --> 00:03:02,879
components the listener and the worker

00:03:00,959 --> 00:03:04,640
the listener opens the socket spawn some

00:03:02,879 --> 00:03:05,920
workers if needed and then waits and

00:03:04,640 --> 00:03:07,840
monitors the whole thing

00:03:05,920 --> 00:03:09,760
the worker accepts a new connection on

00:03:07,840 --> 00:03:11,280
that socket reads in the request

00:03:09,760 --> 00:03:13,280
runs some application code and sends

00:03:11,280 --> 00:03:15,120
back a response doing that for one

00:03:13,280 --> 00:03:16,720
request at a time is relatively easy

00:03:15,120 --> 00:03:19,120
where things get fun is in how we handle

00:03:16,720 --> 00:03:20,560
multiple parallel requests

00:03:19,120 --> 00:03:22,000
the most common worker model used in

00:03:20,560 --> 00:03:23,120
python applications are multiple

00:03:22,000 --> 00:03:25,440
sub-processes

00:03:23,120 --> 00:03:27,040
this avoids all guild complications each

00:03:25,440 --> 00:03:27,519
request runs the only thread in its

00:03:27,040 --> 00:03:29,040
process

00:03:27,519 --> 00:03:30,560
when it needs to make a blocking call it

00:03:29,040 --> 00:03:31,920
just blocks and lets the operating

00:03:30,560 --> 00:03:33,280
system worry about scheduling another

00:03:31,920 --> 00:03:34,640
process

00:03:33,280 --> 00:03:36,400
the next simplest after that would be

00:03:34,640 --> 00:03:37,120
operating system threads p threads on

00:03:36,400 --> 00:03:38,959
linux

00:03:37,120 --> 00:03:40,720
have one process with multiple threads

00:03:38,959 --> 00:03:42,239
each handling one request

00:03:40,720 --> 00:03:44,000
other than that it's pretty similar to

00:03:42,239 --> 00:03:44,319
process workers when a thread wants to

00:03:44,000 --> 00:03:45,680
block

00:03:44,319 --> 00:03:47,120
it just blocks and it lets the operating

00:03:45,680 --> 00:03:48,159
system schedule another thread that's

00:03:47,120 --> 00:03:49,360
ready to run

00:03:48,159 --> 00:03:50,959
since the gill is released in any

00:03:49,360 --> 00:03:52,319
blocking syscall the main problem here

00:03:50,959 --> 00:03:54,159
is compute heavy code

00:03:52,319 --> 00:03:56,319
two parallel requests can't execute

00:03:54,159 --> 00:03:57,680
python code concurrently

00:03:56,319 --> 00:03:59,280
green thread moves the complexity from

00:03:57,680 --> 00:04:01,200
the operating system and gil into user

00:03:59,280 --> 00:04:02,400
space the goal is to have code that

00:04:01,200 --> 00:04:04,000
looks like it would with operating

00:04:02,400 --> 00:04:05,519
system threads but under the hood it's

00:04:04,000 --> 00:04:06,799
actually all sharing a single thread in

00:04:05,519 --> 00:04:08,560
a cooperative manner

00:04:06,799 --> 00:04:10,080
rather than blocking an on an operation

00:04:08,560 --> 00:04:11,920
that will take some time the running

00:04:10,080 --> 00:04:12,400
application has to explicitly go to

00:04:11,920 --> 00:04:14,319
sleep

00:04:12,400 --> 00:04:15,840
and let another request run the way this

00:04:14,319 --> 00:04:16,880
is implemented is that any syscall that

00:04:15,840 --> 00:04:18,560
would release the gill

00:04:16,880 --> 00:04:20,079
now has to be monkey patched to instead

00:04:18,560 --> 00:04:20,799
suspend the green thread task and run

00:04:20,079 --> 00:04:22,400
another

00:04:20,799 --> 00:04:23,919
both g event and eventlet automate this

00:04:22,400 --> 00:04:25,520
patching for common libraries but

00:04:23,919 --> 00:04:27,680
anything that's not patched will block

00:04:25,520 --> 00:04:29,440
all tasks

00:04:27,680 --> 00:04:31,040
async code is similar to green threads

00:04:29,440 --> 00:04:32,400
cooperatively sharing execution on a

00:04:31,040 --> 00:04:33,680
single thread but instead of trying to

00:04:32,400 --> 00:04:35,680
make things transparent

00:04:33,680 --> 00:04:37,440
it requires explicit code to handle the

00:04:35,680 --> 00:04:39,919
exchange of control like async functions

00:04:37,440 --> 00:04:41,520
and awaitables

00:04:39,919 --> 00:04:43,120
these can also be mixed and layered like

00:04:41,520 --> 00:04:44,240
running multiple processes each running

00:04:43,120 --> 00:04:46,320
multiple threads

00:04:44,240 --> 00:04:48,320
this lets each model focus on the pieces

00:04:46,320 --> 00:04:49,919
that it's best at

00:04:48,320 --> 00:04:52,000
these four options sit on a spectrum

00:04:49,919 --> 00:04:53,520
trading resource usage for complexity

00:04:52,000 --> 00:04:55,120
spawning a new process is relatively

00:04:53,520 --> 00:04:56,080
slow and uses a lot of memory but they

00:04:55,120 --> 00:04:57,919
provide a very simple

00:04:56,080 --> 00:04:59,600
operational model threads add some

00:04:57,919 --> 00:05:00,720
complexity but they're much faster and

00:04:59,600 --> 00:05:02,160
smaller to create

00:05:00,720 --> 00:05:03,759
green threads or async tasks are

00:05:02,160 --> 00:05:05,039
entirely in user space and are so

00:05:03,759 --> 00:05:06,639
lightweight that they're usually created

00:05:05,039 --> 00:05:08,160
on demand rather than ahead of time in a

00:05:06,639 --> 00:05:09,520
pool

00:05:08,160 --> 00:05:10,880
worker queue or stream processing

00:05:09,520 --> 00:05:12,479
systems generally follow the same

00:05:10,880 --> 00:05:14,639
overall models rather than listening for

00:05:12,479 --> 00:05:16,720
an http request on a socket they instead

00:05:14,639 --> 00:05:19,039
listen on a queue or a database but the

00:05:16,720 --> 00:05:20,639
request handoff and execution is similar

00:05:19,039 --> 00:05:22,840
seller in particular offers process

00:05:20,639 --> 00:05:24,000
thread and green thread concurrency

00:05:22,840 --> 00:05:25,199
implementations

00:05:24,000 --> 00:05:26,960
each of these four models has its

00:05:25,199 --> 00:05:28,720
strengths and weaknesses from this point

00:05:26,960 --> 00:05:30,479
on i'm going to mostly set aside async

00:05:28,720 --> 00:05:32,080
because most python async frameworks are

00:05:30,479 --> 00:05:33,440
tied to a single web server so there's

00:05:32,080 --> 00:05:35,120
not much to select there

00:05:33,440 --> 00:05:36,720
the ascii standard is trying to improve

00:05:35,120 --> 00:05:37,280
that but things move slowly in standards

00:05:36,720 --> 00:05:38,960
world

00:05:37,280 --> 00:05:40,880
this leaves us three main building

00:05:38,960 --> 00:05:42,400
blocks to play with

00:05:40,880 --> 00:05:43,600
when running any production application

00:05:42,400 --> 00:05:44,639
we need to think about redundancy and

00:05:43,600 --> 00:05:46,080
load balancing

00:05:44,639 --> 00:05:47,520
in the container world this means

00:05:46,080 --> 00:05:48,880
running multiple replicas across

00:05:47,520 --> 00:05:50,320
different physical hosts

00:05:48,880 --> 00:05:52,000
this shares some characteristics with a

00:05:50,320 --> 00:05:53,759
process worker model rather than being

00:05:52,000 --> 00:05:55,600
forked off from the listener instead

00:05:53,759 --> 00:05:57,039
they'll be started from scratch

00:05:55,600 --> 00:05:58,400
unfortunately this means they use the

00:05:57,039 --> 00:06:00,319
full amount of memory and are even

00:05:58,400 --> 00:06:01,360
slower to create but on the plus side it

00:06:00,319 --> 00:06:02,960
means that we get some baseline

00:06:01,360 --> 00:06:06,000
concurrency before we even start

00:06:02,960 --> 00:06:07,680
thinking about which web servers to use

00:06:06,000 --> 00:06:09,039
the next task which of these models

00:06:07,680 --> 00:06:09,360
should we use and how should we tune

00:06:09,039 --> 00:06:10,319
them

00:06:09,360 --> 00:06:11,840
let's talk about some common

00:06:10,319 --> 00:06:13,840
complications so we know which models

00:06:11,840 --> 00:06:15,520
could cause problems

00:06:13,840 --> 00:06:17,199
the first stumbling block health checks

00:06:15,520 --> 00:06:19,039
and socket back pressure in any worker

00:06:17,199 --> 00:06:19,680
model using a fixed or capped number of

00:06:19,039 --> 00:06:21,280
workers

00:06:19,680 --> 00:06:23,120
that also becomes the maximum number of

00:06:21,280 --> 00:06:25,039
parallel requests that can be handled

00:06:23,120 --> 00:06:26,720
most container systems support http

00:06:25,039 --> 00:06:28,000
probes to know the server is healthy

00:06:26,720 --> 00:06:29,759
automatically taking the server out of

00:06:28,000 --> 00:06:30,400
the load balancer rotation if the probes

00:06:29,759 --> 00:06:32,240
fail

00:06:30,400 --> 00:06:33,759
this allows for a degree of self-healing

00:06:32,240 --> 00:06:35,280
if a server is overloaded with slow

00:06:33,759 --> 00:06:36,720
requests it'll eventually be

00:06:35,280 --> 00:06:38,400
uh taken out of the load balancer

00:06:36,720 --> 00:06:39,680
rotation it'll give it some time to

00:06:38,400 --> 00:06:41,520
finish its running requests

00:06:39,680 --> 00:06:42,720
and then it'll go back into service but

00:06:41,520 --> 00:06:44,639
with a worker model that allows an

00:06:42,720 --> 00:06:46,479
unlimited number of parallel tasks like

00:06:44,639 --> 00:06:47,759
many green thread or async systems

00:06:46,479 --> 00:06:49,680
this can allow slow requests to

00:06:47,759 --> 00:06:51,120
continually stack up eventually the

00:06:49,680 --> 00:06:51,919
container will run out of resources and

00:06:51,120 --> 00:06:53,120
get killed

00:06:51,919 --> 00:06:55,440
dropping every request it had in

00:06:53,120 --> 00:06:57,039
progress infinite scaling can sound good

00:06:55,440 --> 00:06:59,199
but even low overhead systems will

00:06:57,039 --> 00:07:00,880
eventually hit hard limits

00:06:59,199 --> 00:07:02,319
and speaking of memory limits out of

00:07:00,880 --> 00:07:03,360
memory errors are a lot more common in

00:07:02,319 --> 00:07:05,120
container environments

00:07:03,360 --> 00:07:06,720
this is not unexpected since increased

00:07:05,120 --> 00:07:08,160
density and resource utilization is

00:07:06,720 --> 00:07:09,840
often an explicit goal of moving to

00:07:08,160 --> 00:07:10,800
containers but it does create some new

00:07:09,840 --> 00:07:11,919
challenges

00:07:10,800 --> 00:07:13,440
these limits are implemented at the

00:07:11,919 --> 00:07:15,199
c-group level so it adds up all the

00:07:13,440 --> 00:07:16,720
sub-processes inside the container

00:07:15,199 --> 00:07:18,479
when the limit is hit the process of the

00:07:16,720 --> 00:07:20,080
highest usage will be stopped

00:07:18,479 --> 00:07:21,680
most container systems include some form

00:07:20,080 --> 00:07:23,360
of last restart reason to help track

00:07:21,680 --> 00:07:24,880
down the source of om events

00:07:23,360 --> 00:07:26,960
but because of how process signaling

00:07:24,880 --> 00:07:29,199
works in linux this can only record when

00:07:26,960 --> 00:07:30,720
pid 1 inside the container is terminated

00:07:29,199 --> 00:07:32,240
if a sub process gets zapped that's

00:07:30,720 --> 00:07:33,680
invisible to the container system

00:07:32,240 --> 00:07:36,319
and your web server would have to handle

00:07:33,680 --> 00:07:37,919
it and most do not as a specific example

00:07:36,319 --> 00:07:39,919
unicorn doesn't even log a warning when

00:07:37,919 --> 00:07:41,680
this happens if we can keep everything

00:07:39,919 --> 00:07:43,120
inside a single process in our container

00:07:41,680 --> 00:07:45,039
it makes monitoring administration a

00:07:43,120 --> 00:07:46,160
whole lot easier

00:07:45,039 --> 00:07:47,440
one thing that can bloat the memory

00:07:46,160 --> 00:07:48,879
usage of process-based servers in

00:07:47,440 --> 00:07:50,639
particular is the interaction between

00:07:48,879 --> 00:07:51,759
linux copy and write memory and python's

00:07:50,639 --> 00:07:53,120
reference counting

00:07:51,759 --> 00:07:54,479
all python objects have a reference

00:07:53,120 --> 00:07:55,680
count in their object header used to

00:07:54,479 --> 00:07:57,120
track when the garbage collector can get

00:07:55,680 --> 00:07:58,639
rid of an unused object

00:07:57,120 --> 00:08:00,080
when a process forked everything in

00:07:58,639 --> 00:08:01,360
memory has to be copied for the new

00:08:00,080 --> 00:08:03,199
process being created

00:08:01,360 --> 00:08:04,400
but linux is sneaky rather than copy

00:08:03,199 --> 00:08:05,039
them all up front which would take a

00:08:04,400 --> 00:08:06,319
long time

00:08:05,039 --> 00:08:08,319
it leaves them as a reference to the

00:08:06,319 --> 00:08:10,080
original if either the original or

00:08:08,319 --> 00:08:11,680
new process right to a chunk of memory

00:08:10,080 --> 00:08:13,120
that particular chunk will be duplicated

00:08:11,680 --> 00:08:14,560
so that each copy gets its own

00:08:13,120 --> 00:08:16,160
this is great for efficiency since it

00:08:14,560 --> 00:08:17,280
means that a forked copy uses a fraction

00:08:16,160 --> 00:08:18,720
of the memory of the original it can be

00:08:17,280 --> 00:08:20,319
started a lot faster

00:08:18,720 --> 00:08:21,919
but then ref counts kind of mess

00:08:20,319 --> 00:08:23,599
everything up updating the ref counts on

00:08:21,919 --> 00:08:24,319
all kinds of global objects like classes

00:08:23,599 --> 00:08:25,840
and modules

00:08:24,319 --> 00:08:28,240
mean that those pages get written to and

00:08:25,840 --> 00:08:30,800
they have to be duplicated python 3.7

00:08:28,240 --> 00:08:32,159
did add a gc.freeze function to lock ref

00:08:30,800 --> 00:08:33,839
counts which can be called

00:08:32,159 --> 00:08:35,599
right before forking and might help with

00:08:33,839 --> 00:08:37,919
efficiency but it's relatively new and

00:08:35,599 --> 00:08:39,200
hasn't been thoroughly battle tested

00:08:37,919 --> 00:08:40,719
while it's not specifically tied to

00:08:39,200 --> 00:08:41,919
containers prometheus is especially

00:08:40,719 --> 00:08:43,360
common in the container world for

00:08:41,919 --> 00:08:44,240
gathering both system and application

00:08:43,360 --> 00:08:45,600
metrics

00:08:44,240 --> 00:08:47,279
for some complicated architectural

00:08:45,600 --> 00:08:49,040
reasons the prometheus client library

00:08:47,279 --> 00:08:50,160
exposes each metric being collected as a

00:08:49,040 --> 00:08:52,080
global variable

00:08:50,160 --> 00:08:53,600
all these globals get exposed via an

00:08:52,080 --> 00:08:54,000
embedded http server hidden in a

00:08:53,600 --> 00:08:55,680
background

00:08:54,000 --> 00:08:57,120
thread and prometheus scrapes that

00:08:55,680 --> 00:08:58,399
server to get all of the data into the

00:08:57,120 --> 00:09:00,240
time series database

00:08:58,399 --> 00:09:02,320
this works as expected with threads and

00:09:00,240 --> 00:09:04,320
with green threads but a variable can't

00:09:02,320 --> 00:09:05,760
be shared between sub-processes

00:09:04,320 --> 00:09:07,920
because multi-process workers are so

00:09:05,760 --> 00:09:09,519
common in python the client library does

00:09:07,920 --> 00:09:11,440
have some handling for them using

00:09:09,519 --> 00:09:12,880
mapped files under the hood but it's

00:09:11,440 --> 00:09:14,560
relatively fragile and it's nice to

00:09:12,880 --> 00:09:15,760
avoid needing

00:09:14,560 --> 00:09:17,279
i mentioned a while back that green

00:09:15,760 --> 00:09:18,399
threads and async code have this failed

00:09:17,279 --> 00:09:20,000
dangerous risk

00:09:18,399 --> 00:09:21,600
this happens when your application code

00:09:20,000 --> 00:09:23,279
makes a call outside the scope of the

00:09:21,600 --> 00:09:24,720
green thread or async library

00:09:23,279 --> 00:09:26,160
for g thread or event lit this would

00:09:24,720 --> 00:09:28,000
mean calling a blocking function that

00:09:26,160 --> 00:09:29,600
isn't monkey patched or for async io it

00:09:28,000 --> 00:09:30,800
would mean calling any non-async

00:09:29,600 --> 00:09:32,800
blocking function

00:09:30,800 --> 00:09:34,080
in either case the main thread is stuck

00:09:32,800 --> 00:09:36,160
waiting for that call to finish

00:09:34,080 --> 00:09:37,680
so that it can resume running tasks both

00:09:36,160 --> 00:09:38,399
of these can be avoided with careful

00:09:37,680 --> 00:09:40,080
code review

00:09:38,399 --> 00:09:41,839
but i say it fails dangerous as opposed

00:09:40,080 --> 00:09:42,640
to fail safe because it only takes one

00:09:41,839 --> 00:09:44,320
errant commit

00:09:42,640 --> 00:09:45,680
to bring your throughput crashing down

00:09:44,320 --> 00:09:47,440
and these bugs can be really hard to

00:09:45,680 --> 00:09:49,200
catch in tests where concurrency isn't a

00:09:47,440 --> 00:09:51,360
factor

00:09:49,200 --> 00:09:53,279
now i get to put on my opinionated hat

00:09:51,360 --> 00:09:54,959
those five issues combined make a strong

00:09:53,279 --> 00:09:55,760
case for deploying python applications

00:09:54,959 --> 00:09:57,920
containers

00:09:55,760 --> 00:09:58,880
using a single process with thread-based

00:09:57,920 --> 00:10:00,640
workers

00:09:58,880 --> 00:10:02,320
we have multiple replicas redundancy and

00:10:00,640 --> 00:10:04,240
concurrency but this gives us

00:10:02,320 --> 00:10:05,279
good back pressure behavior dodges

00:10:04,240 --> 00:10:06,560
problems with memory limits and

00:10:05,279 --> 00:10:08,000
efficiency integrates well with

00:10:06,560 --> 00:10:09,440
container ecosystem tooling

00:10:08,000 --> 00:10:11,839
and limits the effect of concurrency

00:10:09,440 --> 00:10:12,959
bugs what's not to love

00:10:11,839 --> 00:10:14,160
you might have guessed this talk was

00:10:12,959 --> 00:10:15,279
going to be about threading or the gill

00:10:14,160 --> 00:10:16,640
wouldn't have really been an issue but

00:10:15,279 --> 00:10:17,519
now that we're full steam ahead for a

00:10:16,640 --> 00:10:18,880
threaded server

00:10:17,519 --> 00:10:20,800
let's think about the kind of code that

00:10:18,880 --> 00:10:22,240
runs in a modern web app

00:10:20,800 --> 00:10:24,160
on the website we don't usually have

00:10:22,240 --> 00:10:25,600
much cpu intensive code almost all of

00:10:24,160 --> 00:10:26,959
the time in web handlers is going to be

00:10:25,600 --> 00:10:28,800
spent blocking on a socket either

00:10:26,959 --> 00:10:30,959
waiting for a database query response

00:10:28,800 --> 00:10:33,200
or for some json from an http api to

00:10:30,959 --> 00:10:35,040
come back the json parsing itself can

00:10:33,200 --> 00:10:36,720
actually be a cpu hotspot and notably

00:10:35,040 --> 00:10:38,160
the standard library json model does not

00:10:36,720 --> 00:10:39,040
release the gill even when you're using

00:10:38,160 --> 00:10:41,120
the c extension

00:10:39,040 --> 00:10:42,640
so keep an eye on that but overall the

00:10:41,120 --> 00:10:44,240
guild generally has a pretty low impact

00:10:42,640 --> 00:10:46,000
on web server code

00:10:44,240 --> 00:10:47,360
unfortunately cpu bound code is much

00:10:46,000 --> 00:10:49,279
more common in q workers

00:10:47,360 --> 00:10:50,720
and with good reason the whole idea of

00:10:49,279 --> 00:10:52,480
tools like celery and spark streaming is

00:10:50,720 --> 00:10:53,600
to keep our web responses really fast by

00:10:52,480 --> 00:10:54,160
moving the slow parts into the

00:10:53,600 --> 00:10:56,480
background

00:10:54,160 --> 00:10:58,000
but we do have to handle them eventually

00:10:56,480 --> 00:10:59,040
the main things to be on the lookout for

00:10:58,000 --> 00:11:00,560
are tight loops

00:10:59,040 --> 00:11:02,640
in python code doing some kind of

00:11:00,560 --> 00:11:04,399
computation just like in web code

00:11:02,640 --> 00:11:06,160
database queries and hp requests will

00:11:04,399 --> 00:11:07,760
drop the gill and some common libraries

00:11:06,160 --> 00:11:09,440
like pillow are careful to release the

00:11:07,760 --> 00:11:10,800
guild during compute heavy tasks even if

00:11:09,440 --> 00:11:12,800
not doing any io

00:11:10,800 --> 00:11:14,959
but python code aggregating database

00:11:12,800 --> 00:11:16,320
results or reformatting some json is

00:11:14,959 --> 00:11:17,519
going to hold onto the gill and block

00:11:16,320 --> 00:11:19,360
other tasks

00:11:17,519 --> 00:11:21,920
especially for sql queries do as much of

00:11:19,360 --> 00:11:23,519
the work in the database as you can

00:11:21,920 --> 00:11:25,600
the convoy effect is a behavior of

00:11:23,519 --> 00:11:27,760
schedulers where one big slow job

00:11:25,600 --> 00:11:29,680
blocks a bunch of faster ones just like

00:11:27,760 --> 00:11:31,200
a convoy traffic jam on a highway

00:11:29,680 --> 00:11:32,560
this can happen in queue workers where

00:11:31,200 --> 00:11:34,320
the same server is handling many

00:11:32,560 --> 00:11:35,040
different kinds of tasks that vary in

00:11:34,320 --> 00:11:36,640
speed

00:11:35,040 --> 00:11:38,079
this results in uneven throughput

00:11:36,640 --> 00:11:39,120
overall the three the cpu is still

00:11:38,079 --> 00:11:40,959
always being used

00:11:39,120 --> 00:11:42,480
but you'll see bursts of smaller tasks

00:11:40,959 --> 00:11:44,240
completing every few milliseconds as the

00:11:42,480 --> 00:11:45,920
larger tasks get preempted

00:11:44,240 --> 00:11:47,360
if you have any c extensions though and

00:11:45,920 --> 00:11:48,959
they hang onto the gill this can be

00:11:47,360 --> 00:11:50,959
magnified many times over

00:11:48,959 --> 00:11:52,000
as those c calls can't be interrupted by

00:11:50,959 --> 00:11:53,839
python

00:11:52,000 --> 00:11:55,680
monitor your task latency and consider

00:11:53,839 --> 00:11:56,800
sharding your workers by task priority

00:11:55,680 --> 00:11:58,480
or task speed

00:11:56,800 --> 00:11:59,680
if you want to smooth things out this is

00:11:58,480 --> 00:12:00,720
especially important in real time

00:11:59,680 --> 00:12:02,480
streaming systems

00:12:00,720 --> 00:12:04,000
where a slow task can also block the

00:12:02,480 --> 00:12:06,160
listener thread in addition to the other

00:12:04,000 --> 00:12:07,680
jobs

00:12:06,160 --> 00:12:09,839
so that's a bunch of potential problems

00:12:07,680 --> 00:12:11,839
how do we rephrase them into solutions

00:12:09,839 --> 00:12:13,440
our optimal web server will run a single

00:12:11,839 --> 00:12:14,399
process and a configurable number of

00:12:13,440 --> 00:12:15,839
worker threads

00:12:14,399 --> 00:12:17,680
finding a web server that fits the bill

00:12:15,839 --> 00:12:19,760
is surprisingly difficult while unicorn

00:12:17,680 --> 00:12:21,360
and usb both support thread workers they

00:12:19,760 --> 00:12:23,279
use a process supervision system that

00:12:21,360 --> 00:12:25,120
can't be disabled so neither provides a

00:12:23,279 --> 00:12:27,040
true single process server

00:12:25,120 --> 00:12:28,480
some async servers like hyperkorn offer

00:12:27,040 --> 00:12:30,079
thread-based backwards compatibility

00:12:28,480 --> 00:12:31,440
with whiskey but these are complicated

00:12:30,079 --> 00:12:34,160
to set up if you aren't already using

00:12:31,440 --> 00:12:36,560
asgi for other things

00:12:34,160 --> 00:12:38,079
the best server i found is twisted web

00:12:36,560 --> 00:12:39,680
it provides a simple threaded whiskey

00:12:38,079 --> 00:12:41,040
runtime on top of a high performance

00:12:39,680 --> 00:12:42,560
async socket layer

00:12:41,040 --> 00:12:44,079
to get started you can use it directly

00:12:42,560 --> 00:12:45,360
from the command line or you can call it

00:12:44,079 --> 00:12:46,399
from a server script for deeper

00:12:45,360 --> 00:12:47,920
customization

00:12:46,399 --> 00:12:49,680
this also means you build on the entire

00:12:47,920 --> 00:12:51,040
twisted networking ecosystem including

00:12:49,680 --> 00:12:51,839
things like automatic let's encrypt

00:12:51,040 --> 00:12:53,600
support

00:12:51,839 --> 00:12:55,360
deep metrics and instrumentation and of

00:12:53,600 --> 00:12:56,880
course native async handling if you need

00:12:55,360 --> 00:12:58,480
it later on

00:12:56,880 --> 00:12:59,839
i've collected my tweaks and additions

00:12:58,480 --> 00:13:01,600
for things like prometheus metrics and

00:12:59,839 --> 00:13:02,959
log handling into a small library

00:13:01,600 --> 00:13:04,399
you can also use that directly as a

00:13:02,959 --> 00:13:06,000
command line tool to start a web server

00:13:04,399 --> 00:13:08,160
or you could look at it for ideas on

00:13:06,000 --> 00:13:09,760
your own customizations

00:13:08,160 --> 00:13:11,519
worker queue servers aren't standardized

00:13:09,760 --> 00:13:12,959
like web servers so i can't be quite as

00:13:11,519 --> 00:13:14,480
general there but celery

00:13:12,959 --> 00:13:16,480
in particular does support a thread

00:13:14,480 --> 00:13:17,519
worker as a version 4.4

00:13:16,480 --> 00:13:18,800
for other tools check their

00:13:17,519 --> 00:13:21,360
documentation and look for things like

00:13:18,800 --> 00:13:22,720
concurrency and runtime settings

00:13:21,360 --> 00:13:24,560
picking the right number of replicas and

00:13:22,720 --> 00:13:26,639
threads is more of an art than a science

00:13:24,560 --> 00:13:28,720
replicas cost more in terms of ram but

00:13:26,639 --> 00:13:30,959
they provide redundancy and concurrency

00:13:28,720 --> 00:13:32,079
and threads are cheap but using more

00:13:30,959 --> 00:13:33,360
than a few dozen of them is usually

00:13:32,079 --> 00:13:34,000
going to hit diminishing returns on

00:13:33,360 --> 00:13:36,399
performance because

00:13:34,000 --> 00:13:37,680
of the gill to really dial in your

00:13:36,399 --> 00:13:39,040
scaling factors you're going to need to

00:13:37,680 --> 00:13:40,000
be monitoring your container cpu and

00:13:39,040 --> 00:13:41,440
memory usage

00:13:40,000 --> 00:13:43,440
in kubernetes you can get these from the

00:13:41,440 --> 00:13:45,279
kubelet metrics and in other systems you

00:13:43,440 --> 00:13:47,120
can run c advisor yourself

00:13:45,279 --> 00:13:48,720
if you see requests start timing out at

00:13:47,120 --> 00:13:49,680
your load balancer but your cpu is

00:13:48,720 --> 00:13:51,519
mostly idle

00:13:49,680 --> 00:13:53,120
adding more threads may help if requests

00:13:51,519 --> 00:13:54,880
are timing out after reaching your web

00:13:53,120 --> 00:13:57,360
server or if your cpu usage

00:13:54,880 --> 00:13:58,480
is nearing one core per container then

00:13:57,360 --> 00:14:00,560
you probably want to either add more

00:13:58,480 --> 00:14:02,320
replicas or optimize your code

00:14:00,560 --> 00:14:04,240
if possible directly track the number of

00:14:02,320 --> 00:14:06,560
active worker threads on each server for

00:14:04,240 --> 00:14:08,160
more accurate analysis

00:14:06,560 --> 00:14:09,920
i opened this with a vague question what

00:14:08,160 --> 00:14:11,440
does the gill mean for containers

00:14:09,920 --> 00:14:13,199
over the last 20 minutes hopefully i've

00:14:11,440 --> 00:14:15,680
convinced you the answer is not much

00:14:13,199 --> 00:14:17,440
you just need to be careful so to review

00:14:15,680 --> 00:14:19,120
today we learned how the gill works

00:14:17,440 --> 00:14:21,600
the four main types of worker models in

00:14:19,120 --> 00:14:22,480
python that threaded servers have a lot

00:14:21,600 --> 00:14:23,839
of benefits

00:14:22,480 --> 00:14:25,120
and some patterns for operating

00:14:23,839 --> 00:14:26,320
applications using threads and

00:14:25,120 --> 00:14:28,000
containers

00:14:26,320 --> 00:14:29,680
and most of all hopefully we've learned

00:14:28,000 --> 00:14:42,720
that we don't need to fear the gill

00:14:29,680 --> 00:14:42,720

YouTube URL: https://www.youtube.com/watch?v=vLilbA-ilFM


