Title: Building data flows with Celery and SQLAlchemy
Publication date: 2013-07-10
Playlist: Pycon Australia 2013
Description: 
	Roger Barnes
http://2013.pycon-au.org/schedule/30051/view_talk
Reporting and analysis systems rely on coherent and reliable data, often from disparate sources. To that end, a series of well established data warehousing practices have emerged to extract data and produce a consistent data store.

This talk will look at some options for composing workflows using Python. In particular, we'll explore beyond Celery's asynchronous task processing functionality into its workflow (aka Canvas) system an
Captions: 
	00:00:00,000 --> 00:00:04,920
that I actually I'm here to talk about

00:00:01,260 --> 00:00:07,049
data processing today thank you all for

00:00:04,920 --> 00:00:09,330
braving data processing is the last talk

00:00:07,049 --> 00:00:11,700
of plaque on before the lightning talks

00:00:09,330 --> 00:00:15,150
I'm gonna try and be engaging enough to

00:00:11,700 --> 00:00:17,100
not put you all to sleep and if you do

00:00:15,150 --> 00:00:18,990
know a little bit about SQL alchemy your

00:00:17,100 --> 00:00:21,060
celery already I'll try and shout that

00:00:18,990 --> 00:00:24,000
the big junctures to wake you up for the

00:00:21,060 --> 00:00:26,580
next section of things so I'm Roger

00:00:24,000 --> 00:00:28,260
bands you can find me on Twitter Email

00:00:26,580 --> 00:00:31,560
and I will put my slides on slideshow

00:00:28,260 --> 00:00:33,120
after today so I'm going to talk a bit

00:00:31,560 --> 00:00:35,730
about data warehousing and data

00:00:33,120 --> 00:00:37,140
integration and I realize looking at the

00:00:35,730 --> 00:00:39,180
program today in particular there's a

00:00:37,140 --> 00:00:40,590
lot of people who do data a lot more

00:00:39,180 --> 00:00:45,620
than I do

00:00:40,590 --> 00:00:48,090
so in terms of big data and analytics

00:00:45,620 --> 00:00:49,469
I'm not going to try and cover a lot of

00:00:48,090 --> 00:00:51,059
that sort of stuff what I'm really

00:00:49,469 --> 00:00:55,140
looking at is a framework for doing data

00:00:51,059 --> 00:00:56,730
processing within a smaller environment

00:00:55,140 --> 00:00:58,440
it's not to say you couldn't scale this

00:00:56,730 --> 00:01:01,620
stuff up but this is really talking

00:00:58,440 --> 00:01:03,809
about an experience I had with taking

00:01:01,620 --> 00:01:05,400
sort of disparate systems and managing

00:01:03,809 --> 00:01:08,040
the data together using a pure Python

00:01:05,400 --> 00:01:09,619
sort of framework so to do that we're

00:01:08,040 --> 00:01:11,880
gonna look at a bit a bit of SQL alchemy

00:01:09,619 --> 00:01:13,320
and also celery which is sort of an

00:01:11,880 --> 00:01:16,860
unlikely contender here and how I've

00:01:13,320 --> 00:01:19,110
used that so I've been doing a software

00:01:16,860 --> 00:01:21,030
development and other things for quite a

00:01:19,110 --> 00:01:22,770
while now and I spent 11 years at a

00:01:21,030 --> 00:01:24,119
business intelligence vendor while I

00:01:22,770 --> 00:01:25,770
wasn't directly involved with the

00:01:24,119 --> 00:01:28,470
software they produced I certainly cut

00:01:25,770 --> 00:01:30,420
my teeth on a lot of this kind of these

00:01:28,470 --> 00:01:32,009
kind of concepts at the time

00:01:30,420 --> 00:01:33,869
I'm currently contracting and this

00:01:32,009 --> 00:01:36,119
particular talk is actually about a

00:01:33,869 --> 00:01:39,360
specific reporting system that I've

00:01:36,119 --> 00:01:40,829
built for one of those contracts so

00:01:39,360 --> 00:01:42,030
first of all for those who aren't

00:01:40,829 --> 00:01:43,380
familiar a bit of an introduction to the

00:01:42,030 --> 00:01:46,409
idea of data warehousing or data

00:01:43,380 --> 00:01:48,750
integration because there are lots of

00:01:46,409 --> 00:01:50,369
different systems in an organization you

00:01:48,750 --> 00:01:53,520
might have different data quality levels

00:01:50,369 --> 00:01:54,899
different data life cycles different

00:01:53,520 --> 00:01:55,920
departments collecting and doing

00:01:54,899 --> 00:01:59,369
different things on all sorts of

00:01:55,920 --> 00:02:00,479
different different levels what that

00:01:59,369 --> 00:02:03,960
means is you end up with a very

00:02:00,479 --> 00:02:06,119
disparate set of non heterogeneous data

00:02:03,960 --> 00:02:09,060
and it's also potentially used in

00:02:06,119 --> 00:02:10,200
different ways which is ambiguous and

00:02:09,060 --> 00:02:11,370
not actually consistent across an

00:02:10,200 --> 00:02:14,340
organization

00:02:11,370 --> 00:02:15,930
what we really want is a some sort of

00:02:14,340 --> 00:02:19,140
centralized reporting that is timely

00:02:15,930 --> 00:02:21,299
that is accurate unambiguous complete

00:02:19,140 --> 00:02:23,220
and ideally isn't impacting production

00:02:21,299 --> 00:02:25,290
systems a lot of this is about making

00:02:23,220 --> 00:02:27,659
sure that you don't have the Excel

00:02:25,290 --> 00:02:29,700
Empire up there where Bob in marketing

00:02:27,659 --> 00:02:30,840
and Jane the IT director aren't getting

00:02:29,700 --> 00:02:32,879
their little bit of data from their

00:02:30,840 --> 00:02:34,349
little spot and they might not be they

00:02:32,879 --> 00:02:35,879
might be hitting that the systems too

00:02:34,349 --> 00:02:37,440
hard or they might be getting in

00:02:35,879 --> 00:02:39,299
completely wrong and actually reporting

00:02:37,440 --> 00:02:40,829
on data that they thought was accurate

00:02:39,299 --> 00:02:44,940
or complete when they're not getting the

00:02:40,829 --> 00:02:49,200
right information at all so looking at a

00:02:44,940 --> 00:02:50,430
data warehouse I'm sort of at a top

00:02:49,200 --> 00:02:51,930
level it really is just a central

00:02:50,430 --> 00:02:53,180
repository of data that integrates all

00:02:51,930 --> 00:02:55,290
of these disparate systems together

00:02:53,180 --> 00:02:57,000
beneath all that there's a huge amount

00:02:55,290 --> 00:02:58,590
of theory and really good practices in

00:02:57,000 --> 00:03:00,989
data warehousing that I'm not going to

00:02:58,590 --> 00:03:02,280
go into today but that concept of

00:03:00,989 --> 00:03:05,329
basically integrating everything

00:03:02,280 --> 00:03:07,709
together is what I'm going to focus on

00:03:05,329 --> 00:03:09,900
this particular slide isn't designed to

00:03:07,709 --> 00:03:12,180
be readable so I'm not going to

00:03:09,900 --> 00:03:13,769
apologize for it yet this is a slide

00:03:12,180 --> 00:03:15,450
from the school of data it basically

00:03:13,769 --> 00:03:19,019
shows the different skill sets involved

00:03:15,450 --> 00:03:21,269
in what's it called data processing

00:03:19,019 --> 00:03:23,609
pipeline and basically there's a very

00:03:21,269 --> 00:03:25,590
large range of things from understanding

00:03:23,609 --> 00:03:27,930
data analyzing it even things like

00:03:25,590 --> 00:03:29,250
governance and compliance and so on what

00:03:27,930 --> 00:03:31,169
I'm actually going to focus on is really

00:03:29,250 --> 00:03:32,819
that middle bit of this diagram which is

00:03:31,169 --> 00:03:35,700
about the extraction and transformation

00:03:32,819 --> 00:03:37,739
of data or in other words this is a bit

00:03:35,700 --> 00:03:40,949
of a nicer diagram I think extracting

00:03:37,739 --> 00:03:42,690
from multiple systems transforming that

00:03:40,949 --> 00:03:43,889
through a staging layer and then putting

00:03:42,690 --> 00:03:48,239
it into a central database for

00:03:43,889 --> 00:03:51,359
consumption so we're all here for Python

00:03:48,239 --> 00:03:53,250
Python can help do this it's really good

00:03:51,359 --> 00:03:54,870
I think I'm probably preaching to the

00:03:53,250 --> 00:03:57,120
converted here really good for rapid

00:03:54,870 --> 00:03:59,730
prototyping lots of potential for code

00:03:57,120 --> 00:04:01,859
reuse there are a ton of libraries out

00:03:59,730 --> 00:04:05,400
there so whether you're parsing XML or

00:04:01,859 --> 00:04:08,250
scraping webpages reading CSV files or

00:04:05,400 --> 00:04:09,750
reading out of databases there's likely

00:04:08,250 --> 00:04:12,629
a library out there or somewhere to get

00:04:09,750 --> 00:04:15,959
to that data finally pythons quite good

00:04:12,629 --> 00:04:18,180
for decoupling so actually keeping your

00:04:15,959 --> 00:04:19,919
flow the data flow that your are you're

00:04:18,180 --> 00:04:22,289
dealing with under management doesn't

00:04:19,919 --> 00:04:24,210
have to be tied to the actual processing

00:04:22,289 --> 00:04:26,430
and the business logic doesn't

00:04:24,210 --> 00:04:27,780
to be in the same place as well so good

00:04:26,430 --> 00:04:31,289
design allows you to separate those

00:04:27,780 --> 00:04:32,699
three concerns quite well in terms of

00:04:31,289 --> 00:04:34,410
existing solutions I sort of did a

00:04:32,699 --> 00:04:36,690
survey of this before I embarked on this

00:04:34,410 --> 00:04:38,639
project and there's not a whole lot in

00:04:36,690 --> 00:04:40,650
the Python space around any business

00:04:38,639 --> 00:04:41,970
intelligence there are there are some

00:04:40,650 --> 00:04:43,729
things out there and I'm happy to hear

00:04:41,970 --> 00:04:45,870
about more of them if I've missed any

00:04:43,729 --> 00:04:48,650
people do tend to roll their own and

00:04:45,870 --> 00:04:50,960
quite often that's because the business

00:04:48,650 --> 00:04:53,400
problem is often quite different and

00:04:50,960 --> 00:04:56,009
often the domain or the scale or the

00:04:53,400 --> 00:04:58,099
shape of the data really informs what

00:04:56,009 --> 00:05:02,070
decision you might make about a solution

00:04:58,099 --> 00:05:05,039
there is recently released a framework

00:05:02,070 --> 00:05:07,500
called bubbles or brewery - data brewery

00:05:05,039 --> 00:05:09,380
org is where you can find that there is

00:05:07,500 --> 00:05:12,240
a link in a resource slide at the end

00:05:09,380 --> 00:05:15,720
it's basically a framework that lets you

00:05:12,240 --> 00:05:17,370
build up operations and patterns that's

00:05:15,720 --> 00:05:19,650
that you can string together in a

00:05:17,370 --> 00:05:21,060
pipeline so it's not unlike what I'm

00:05:19,650 --> 00:05:22,590
going to talk about

00:05:21,060 --> 00:05:23,940
I haven't dug very deeply into it though

00:05:22,590 --> 00:05:28,380
so I can't really give you a comparison

00:05:23,940 --> 00:05:31,080
at this point I'm gonna call quickly

00:05:28,380 --> 00:05:32,580
about ways to move data around often the

00:05:31,080 --> 00:05:34,770
source data you have you have no choice

00:05:32,580 --> 00:05:35,940
about but once you have your data you

00:05:34,770 --> 00:05:38,940
could be looking at things like flat

00:05:35,940 --> 00:05:40,620
files being CSV XML or what-have-you

00:05:38,940 --> 00:05:42,599
there's a whole lot of no SQL data

00:05:40,620 --> 00:05:44,969
stores around and depending on your data

00:05:42,599 --> 00:05:46,139
they could be quite useful and of course

00:05:44,969 --> 00:05:47,909
you've got your good old relational

00:05:46,139 --> 00:05:49,560
database systems and what most

00:05:47,909 --> 00:05:51,330
traditional data warehouses and this is

00:05:49,560 --> 00:05:53,400
certainly an area that's changing tend

00:05:51,330 --> 00:05:56,849
to lean on these systems I'm going to

00:05:53,400 --> 00:05:58,229
talk about SQL alchemy today but really

00:05:56,849 --> 00:06:00,110
going back to that whole concept of

00:05:58,229 --> 00:06:04,289
decoupling you could use some of the

00:06:00,110 --> 00:06:07,610
concepts from this framework and build

00:06:04,289 --> 00:06:09,930
something based on a no SQL solution

00:06:07,610 --> 00:06:12,780
it's really about sort of gluing

00:06:09,930 --> 00:06:13,860
together some concepts here today so

00:06:12,780 --> 00:06:16,349
very quickly I'm going to talk about

00:06:13,860 --> 00:06:19,860
what SQL alchemy is just quickly who is

00:06:16,349 --> 00:06:21,930
familiar with SQL alchemy at all who

00:06:19,860 --> 00:06:25,680
thinks they're they've used it enough to

00:06:21,930 --> 00:06:26,969
be regularly familiar okay that's good

00:06:25,680 --> 00:06:29,060
there's a few people maybe I can

00:06:26,969 --> 00:06:33,000
convince you that it's worth looking at

00:06:29,060 --> 00:06:34,800
basically SQL alchemy is a Python SQL

00:06:33,000 --> 00:06:36,340
toolkit in one layer and an object

00:06:34,800 --> 00:06:39,760
relational mapper

00:06:36,340 --> 00:06:43,630
top of that what that basically means is

00:06:39,760 --> 00:06:46,960
that you can generate SQL for various

00:06:43,630 --> 00:06:50,290
various databases in a declarative sense

00:06:46,960 --> 00:06:51,460
using Python we're actually not going to

00:06:50,290 --> 00:06:53,110
look at the object relational mapper

00:06:51,460 --> 00:06:55,570
today because with this sort of

00:06:53,110 --> 00:06:57,699
low-level data-processing it's it's not

00:06:55,570 --> 00:07:00,040
something where objects and relational

00:06:57,699 --> 00:07:02,050
mapping really will help you out

00:07:00,040 --> 00:07:04,060
you can perhaps fall back to that and

00:07:02,050 --> 00:07:05,380
SQL alchemy does let you sort of jump

00:07:04,060 --> 00:07:08,650
back and forth between them if you want

00:07:05,380 --> 00:07:10,630
to so if you do need that for perhaps

00:07:08,650 --> 00:07:12,669
being strict about your your data

00:07:10,630 --> 00:07:16,180
definitions and so on then it's

00:07:12,669 --> 00:07:17,560
something you can also consider so SQL

00:07:16,180 --> 00:07:18,970
alchemy is fairly well established I

00:07:17,560 --> 00:07:21,940
think everyone's heard about it so I

00:07:18,970 --> 00:07:24,250
won't go on too much about that and it's

00:07:21,940 --> 00:07:26,070
also well used throughout the industry

00:07:24,250 --> 00:07:28,030
so you shouldn't have any concerns about

00:07:26,070 --> 00:07:31,810
using it if you think that it's sort of

00:07:28,030 --> 00:07:35,050
an under supported kind of system it has

00:07:31,810 --> 00:07:36,639
really good database support there's

00:07:35,050 --> 00:07:38,560
probably obscure databases out there

00:07:36,639 --> 00:07:40,240
that people are using that are supported

00:07:38,560 --> 00:07:42,210
so it's certainly worth checking but

00:07:40,240 --> 00:07:45,039
it's got the basics pretty well covered

00:07:42,210 --> 00:07:45,970
and it's also got good Python support so

00:07:45,039 --> 00:07:50,490
again depending on your environment

00:07:45,970 --> 00:07:50,490
there's there's lots of options there

00:07:51,060 --> 00:07:56,770
SQL alchemy is something like an onion

00:07:54,160 --> 00:07:59,710
that has layers basically at the bottom

00:07:56,770 --> 00:08:01,479
we have the python db-api and SQL can we

00:07:59,710 --> 00:08:02,740
just build on top of that it has

00:08:01,479 --> 00:08:05,200
dialects for dealing with different

00:08:02,740 --> 00:08:07,660
databases connection pooling transaction

00:08:05,200 --> 00:08:09,310
management and an SQL expression

00:08:07,660 --> 00:08:11,260
language and schemas and types which is

00:08:09,310 --> 00:08:13,389
really the where the heavy lifting

00:08:11,260 --> 00:08:16,150
happens in terms of translating your

00:08:13,389 --> 00:08:18,039
you're talking about data in Python into

00:08:16,150 --> 00:08:21,310
SQL that actually pushes and pulls

00:08:18,039 --> 00:08:25,210
information out of databases which is

00:08:21,310 --> 00:08:28,060
what this slide says so like I said the

00:08:25,210 --> 00:08:29,800
ORM level is somewhat unnecessary when

00:08:28,060 --> 00:08:32,409
you're doing data flow processing so

00:08:29,800 --> 00:08:34,330
we're looking just at the core it does

00:08:32,409 --> 00:08:36,010
those bulk operations you can fine tune

00:08:34,330 --> 00:08:37,990
it ultimately this stuff is just

00:08:36,010 --> 00:08:40,570
generating SQL and so if you need to you

00:08:37,990 --> 00:08:43,450
can dive into lower layers of the the

00:08:40,570 --> 00:08:45,130
SQL alchemy infrastructure and and

00:08:43,450 --> 00:08:45,950
fiddle with it to make things work

00:08:45,130 --> 00:08:49,310
better

00:08:45,950 --> 00:08:52,210
a really quick example of how SQL

00:08:49,310 --> 00:08:54,650
alchemy looks this is creating a table

00:08:52,210 --> 00:08:57,500
basically you create an engine you have

00:08:54,650 --> 00:09:00,050
some metadata you then generate that

00:08:57,500 --> 00:09:02,180
table against that metadata using the

00:09:00,050 --> 00:09:03,860
engine that you've created so this is

00:09:02,180 --> 00:09:05,780
just an in-memory SQLite you can run

00:09:03,860 --> 00:09:10,460
that code and you would have a database

00:09:05,780 --> 00:09:12,050
table inserting data is simply a matter

00:09:10,460 --> 00:09:15,320
of having in this case a list of

00:09:12,050 --> 00:09:17,480
dictionaries key value pairs and putting

00:09:15,320 --> 00:09:19,760
them into the table using an insert

00:09:17,480 --> 00:09:22,010
operation what you can even do is say

00:09:19,760 --> 00:09:23,300
vehicle table dot insert down here and

00:09:22,010 --> 00:09:24,800
actually print that out and it will give

00:09:23,300 --> 00:09:28,640
you the string representation that is

00:09:24,800 --> 00:09:30,230
the SQL that it would run finally you

00:09:28,640 --> 00:09:32,030
can pull data back out of course and

00:09:30,230 --> 00:09:33,830
again it's very much a declarative kind

00:09:32,030 --> 00:09:34,370
of thing where you say I want a select

00:09:33,830 --> 00:09:37,220
statement

00:09:34,370 --> 00:09:39,050
I have filtration you can order and

00:09:37,220 --> 00:09:41,210
group and all the things that SQL can do

00:09:39,050 --> 00:09:44,960
basically SQL can be tries to do that

00:09:41,210 --> 00:09:46,670
across a common set of backends for any

00:09:44,960 --> 00:09:47,840
imaginable set of functions and for

00:09:46,670 --> 00:09:49,760
those that it doesn't cover you can

00:09:47,840 --> 00:09:51,890
always generate little bits of manual

00:09:49,760 --> 00:09:53,480
literal columns and SQL and functions

00:09:51,890 --> 00:09:57,170
that that might be specific to your

00:09:53,480 --> 00:09:58,750
database so now we've got SQL alchemy

00:09:57,170 --> 00:10:00,950
sort of figured out a little bit

00:09:58,750 --> 00:10:03,470
thinking back to our data processing

00:10:00,950 --> 00:10:05,660
there's this idea of going from that

00:10:03,470 --> 00:10:08,080
extraction stage and through a series of

00:10:05,660 --> 00:10:10,490
transformations and every one of those

00:10:08,080 --> 00:10:13,550
should be encapsulated into a unit of

00:10:10,490 --> 00:10:15,260
work so extracting from this CSV file

00:10:13,550 --> 00:10:17,660
into this table or extracting from this

00:10:15,260 --> 00:10:20,450
database to this table or transforming

00:10:17,660 --> 00:10:22,040
that will add a derive a new column in

00:10:20,450 --> 00:10:24,470
an existing table could all be

00:10:22,040 --> 00:10:25,760
considered single operations and so to

00:10:24,470 --> 00:10:29,060
that end we have this idea of a

00:10:25,760 --> 00:10:32,240
processor so here's some example

00:10:29,060 --> 00:10:35,120
possibilities of generic processor types

00:10:32,240 --> 00:10:38,020
you might even have report types that

00:10:35,120 --> 00:10:38,020
are processors as well

00:10:39,620 --> 00:10:43,390
and so now we're going to get a bit

00:10:41,510 --> 00:10:45,920
dirty and we're gonna look at some code

00:10:43,390 --> 00:10:48,830
this is a base processor and it's really

00:10:45,920 --> 00:10:50,750
just a shell at this point if you call

00:10:48,830 --> 00:10:52,760
dispatch on this processor class which

00:10:50,750 --> 00:10:54,470
is an abstract one it will ultimately

00:10:52,760 --> 00:10:57,620
call the run method which will need to

00:10:54,470 --> 00:11:00,200
implement in some sort of subclass the

00:10:57,620 --> 00:11:01,670
next logical extension of a processor is

00:11:00,200 --> 00:11:04,640
to have one that actually processes on a

00:11:01,670 --> 00:11:07,220
database so here we're subclass in the

00:11:04,640 --> 00:11:10,250
base processor and where we now need to

00:11:07,220 --> 00:11:13,010
provide a DB class and now when the run

00:11:10,250 --> 00:11:14,840
method gets called it will set up a

00:11:13,010 --> 00:11:17,930
session for us that's available for our

00:11:14,840 --> 00:11:20,030
run method to use going slightly

00:11:17,930 --> 00:11:21,470
sideways for a moment we have things

00:11:20,030 --> 00:11:25,160
like mix-ins where you can actually have

00:11:21,470 --> 00:11:29,270
say a CSV extract mixing which will

00:11:25,160 --> 00:11:31,610
encapsulate giving you a reader given an

00:11:29,270 --> 00:11:33,650
input file and so we can combine these

00:11:31,610 --> 00:11:35,360
these blocks at a class level into

00:11:33,650 --> 00:11:38,660
things that will extract from a file and

00:11:35,360 --> 00:11:41,210
put it into a database I should point

00:11:38,660 --> 00:11:42,800
out here that these code examples are

00:11:41,210 --> 00:11:44,270
functional but they are somewhat

00:11:42,800 --> 00:11:46,420
simplified from what I've actually ended

00:11:44,270 --> 00:11:48,710
up with in the project that I work on

00:11:46,420 --> 00:11:53,420
there are certainly some additional

00:11:48,710 --> 00:11:55,520
possibilities here so here's a concrete

00:11:53,420 --> 00:11:57,680
example of an extract it is a CSV

00:11:55,520 --> 00:12:00,650
extract mixing and a database processor

00:11:57,680 --> 00:12:03,320
and basically what it does is it uses

00:12:00,650 --> 00:12:06,500
SQL alchemy to create a table instance

00:12:03,320 --> 00:12:08,510
it then creates columns based on the

00:12:06,500 --> 00:12:10,640
first row of the data from your CSV file

00:12:08,510 --> 00:12:12,350
and then for every row beneath that it

00:12:10,640 --> 00:12:14,960
inserts them into the table it's a very

00:12:12,350 --> 00:12:17,750
naive but complete example of extracting

00:12:14,960 --> 00:12:19,400
from a CSV file into a table and so now

00:12:17,750 --> 00:12:22,700
what we have is a single processor that

00:12:19,400 --> 00:12:24,020
can do that one job repeatedly in this

00:12:22,700 --> 00:12:25,670
case it actually tries to create the

00:12:24,020 --> 00:12:27,500
table as well so you might need

00:12:25,670 --> 00:12:29,780
something that either resets it or is

00:12:27,500 --> 00:12:33,070
conscious of that possibility and needs

00:12:29,780 --> 00:12:36,280
to decide to do something from there

00:12:33,070 --> 00:12:39,200
thinking ahead a little bit further

00:12:36,280 --> 00:12:41,360
there's the concept of transforming data

00:12:39,200 --> 00:12:43,520
so you already have a table and you

00:12:41,360 --> 00:12:45,680
might want to add an extra column that

00:12:43,520 --> 00:12:48,350
is computed based on other information

00:12:45,680 --> 00:12:49,950
already in that table so here we have

00:12:48,350 --> 00:12:51,960
the concept of an abstract

00:12:49,950 --> 00:12:54,260
I've transformed that given an input

00:12:51,960 --> 00:12:56,460
table which is also at the target table

00:12:54,260 --> 00:12:58,950
some sort of key column that lets you

00:12:56,460 --> 00:13:01,590
keep track which columns you want to

00:12:58,950 --> 00:13:03,300
select to do your computation and which

00:13:01,590 --> 00:13:06,750
target column you're actually promising

00:13:03,300 --> 00:13:09,560
to provide we have then this abstract

00:13:06,750 --> 00:13:12,750
process flow where having selected

00:13:09,560 --> 00:13:14,760
certain rows out of your your table you

00:13:12,750 --> 00:13:17,490
will apply a process and they will get

00:13:14,760 --> 00:13:19,170
written back out to the database and I'm

00:13:17,490 --> 00:13:21,330
having to do slide driven development

00:13:19,170 --> 00:13:23,490
here where there's a lot of additional

00:13:21,330 --> 00:13:25,410
code potentially involved in doing that

00:13:23,490 --> 00:13:27,510
if anyone does want more code snippets

00:13:25,410 --> 00:13:28,890
I'm happy to provide them it's not

00:13:27,510 --> 00:13:30,420
something I've actually extracted into

00:13:28,890 --> 00:13:32,490
an open source form though at this point

00:13:30,420 --> 00:13:36,390
so I can't point you straight at it at

00:13:32,490 --> 00:13:38,940
the moment so a concrete implementation

00:13:36,390 --> 00:13:41,670
of a transform might be the derived food

00:13:38,940 --> 00:13:44,130
transform so we have a sales table it's

00:13:41,670 --> 00:13:45,870
keyed by transaction ID we know we need

00:13:44,130 --> 00:13:48,150
the location and the user name to derive

00:13:45,870 --> 00:13:51,030
foo and we're going to write out a

00:13:48,150 --> 00:13:53,070
target column called foo and all we have

00:13:51,030 --> 00:13:55,080
to implement is the process row which

00:13:53,070 --> 00:13:57,480
calls upon some external business logic

00:13:55,080 --> 00:13:59,790
to derive foo based on the location and

00:13:57,480 --> 00:14:03,420
the user name provided again this is

00:13:59,790 --> 00:14:04,920
quite a simplified example and it might

00:14:03,420 --> 00:14:06,300
be that your your abstract processor

00:14:04,920 --> 00:14:08,430
will only update values that aren't

00:14:06,300 --> 00:14:10,290
already derived so if you've previously

00:14:08,430 --> 00:14:13,410
already extracted some information and

00:14:10,290 --> 00:14:16,080
derived it this would let you excuse me

00:14:13,410 --> 00:14:19,110
just update those that need derivation

00:14:16,080 --> 00:14:20,100
that haven't had it before there any

00:14:19,110 --> 00:14:21,630
questions at this point

00:14:20,100 --> 00:14:25,230
maybe we actually should save it to the

00:14:21,630 --> 00:14:26,400
end be easier so that's kind of SQL

00:14:25,230 --> 00:14:30,630
alchemy and we've defined these

00:14:26,400 --> 00:14:32,730
processor units of work that will do

00:14:30,630 --> 00:14:34,500
jobs that will extract and transform and

00:14:32,730 --> 00:14:36,870
at this point you could run them in an

00:14:34,500 --> 00:14:38,490
order that you know and you can get some

00:14:36,870 --> 00:14:43,710
information out of the other end from a

00:14:38,490 --> 00:14:47,230
consistent and coherent set of tables so

00:14:43,710 --> 00:14:49,190
now let's have a look at celery wake up

00:14:47,230 --> 00:14:51,140
we're now looking at celery which

00:14:49,190 --> 00:14:53,930
traditionally is used in web

00:14:51,140 --> 00:14:55,940
applications and it's used for back-end

00:14:53,930 --> 00:14:57,500
processing of things that you know a

00:14:55,940 --> 00:14:58,940
user logs in and you might need to go

00:14:57,500 --> 00:15:00,800
off and do something like download their

00:14:58,940 --> 00:15:02,120
Facebook friends and you don't need to

00:15:00,800 --> 00:15:03,950
interrupt the user with that so you do

00:15:02,120 --> 00:15:06,230
it in back-end so celery is a

00:15:03,950 --> 00:15:08,570
distributed task queue you can push

00:15:06,230 --> 00:15:09,770
tasks into the broker and a bunch of

00:15:08,570 --> 00:15:14,620
workers will pick them up and do that

00:15:09,770 --> 00:15:17,720
work that's pretty much all celery is

00:15:14,620 --> 00:15:20,270
basically at its core celery has the

00:15:17,720 --> 00:15:22,850
concept of a task and so what we have

00:15:20,270 --> 00:15:25,970
here is a very simple wrapper that takes

00:15:22,850 --> 00:15:27,320
a celery task and extends it so that

00:15:25,970 --> 00:15:31,250
given a processor which we've just

00:15:27,320 --> 00:15:32,780
looked at it will actually run it so at

00:15:31,250 --> 00:15:34,910
the top we have the abstract version of

00:15:32,780 --> 00:15:37,190
the class the two lines near the bottom

00:15:34,910 --> 00:15:39,680
are saying the derive food task is

00:15:37,190 --> 00:15:42,290
simply a process a task that runs the

00:15:39,680 --> 00:15:44,480
derive food transform and then the way

00:15:42,290 --> 00:15:47,360
that you run a task with celery you say

00:15:44,480 --> 00:15:48,860
derive food task apply async and so that

00:15:47,360 --> 00:15:53,900
would get sent off to the celery broker

00:15:48,860 --> 00:15:56,720
in the backend to be run so now we've

00:15:53,900 --> 00:15:59,180
gotten from the point of a transform

00:15:56,720 --> 00:16:02,210
that can be run to a task that can be

00:15:59,180 --> 00:16:07,370
written run asynchronously how can this

00:16:02,210 --> 00:16:10,310
help us from here on top of celery there

00:16:07,370 --> 00:16:11,720
is a thing called a canvas and canvas

00:16:10,310 --> 00:16:14,990
basically gives you a way to combine

00:16:11,720 --> 00:16:16,880
these tasks together you can create

00:16:14,990 --> 00:16:18,590
groups so you can say I want this task

00:16:16,880 --> 00:16:20,240
and this task and this task to run and

00:16:18,590 --> 00:16:22,790
they don't depend on each other they can

00:16:20,240 --> 00:16:25,370
all happen in parallel but then you

00:16:22,790 --> 00:16:26,930
might need to say tasks do depend on

00:16:25,370 --> 00:16:29,450
each other and so you have this concept

00:16:26,930 --> 00:16:31,550
of chains that run in series and so you

00:16:29,450 --> 00:16:33,410
can combine series of groups and chains

00:16:31,550 --> 00:16:35,420
and there's also things called chords

00:16:33,410 --> 00:16:38,200
and maps and various things that will

00:16:35,420 --> 00:16:41,150
build kind of a graph kind of shaped

00:16:38,200 --> 00:16:43,130
series of dependencies and this isn't

00:16:41,150 --> 00:16:45,470
necessarily about getting performance

00:16:43,130 --> 00:16:47,630
out through parallelization although

00:16:45,470 --> 00:16:48,980
that is something you can get it's more

00:16:47,630 --> 00:16:51,650
about putting things in an order that

00:16:48,980 --> 00:16:53,480
you know that this task is going to

00:16:51,650 --> 00:16:57,620
depend on the output of this task so run

00:16:53,480 --> 00:16:58,399
it beforehand this example here is from

00:16:57,620 --> 00:17:00,230
the

00:16:58,399 --> 00:17:02,689
the celery docks and basically

00:17:00,230 --> 00:17:05,510
demonstrates that after creating a new

00:17:02,689 --> 00:17:07,490
user well the new user workflow will

00:17:05,510 --> 00:17:10,250
create a new user and the pipe here is

00:17:07,490 --> 00:17:11,900
actually going to create a chain and it

00:17:10,250 --> 00:17:14,059
pipes that into a group which is in

00:17:11,900 --> 00:17:16,610
parallel importing the contacts and

00:17:14,059 --> 00:17:18,679
sending a welcome email and then again

00:17:16,610 --> 00:17:21,909
we're going to then run that workflow

00:17:18,679 --> 00:17:21,909
given a set of input

00:17:27,120 --> 00:17:32,160
so now we're looking at a data

00:17:29,910 --> 00:17:34,110
processing flow we've talked a lot about

00:17:32,160 --> 00:17:36,630
the concept of how this all works now so

00:17:34,110 --> 00:17:37,920
this is just one fairly contrived

00:17:36,630 --> 00:17:40,710
example where you might have different

00:17:37,920 --> 00:17:42,900
data from different sources and we sort

00:17:40,710 --> 00:17:46,260
of grouped together the extract time

00:17:42,900 --> 00:17:48,180
concerns we then have a series of

00:17:46,260 --> 00:17:49,620
transforms that copy that to a transform

00:17:48,180 --> 00:17:51,809
layer so we leave our extract layer

00:17:49,620 --> 00:17:53,280
alone and we then start transforming in

00:17:51,809 --> 00:17:54,600
a separate layer this gives us some

00:17:53,280 --> 00:17:56,460
options where if we want to actually

00:17:54,600 --> 00:17:57,720
blow away that middle layer we don't

00:17:56,460 --> 00:18:00,420
have to reiax tracked everything all

00:17:57,720 --> 00:18:02,610
over again sometimes having multiple

00:18:00,420 --> 00:18:05,100
copies through the the pipeline can give

00:18:02,610 --> 00:18:06,840
you that flexibility and we have just

00:18:05,100 --> 00:18:08,640
one simple transform here that will

00:18:06,840 --> 00:18:11,250
normalize the currency of your sales

00:18:08,640 --> 00:18:13,020
Trent sales data before joining them all

00:18:11,250 --> 00:18:15,120
together into some reporting structure

00:18:13,020 --> 00:18:17,730
and then actually running some

00:18:15,120 --> 00:18:19,410
aggregations and an exception report

00:18:17,730 --> 00:18:22,050
that will perhaps look at data quality

00:18:19,410 --> 00:18:24,210
so that's the visual representation of

00:18:22,050 --> 00:18:27,960
that and these boxes and arrows are

00:18:24,210 --> 00:18:29,190
effectively groups and chains the way

00:18:27,960 --> 00:18:32,700
that looks in code I hope that's

00:18:29,190 --> 00:18:34,890
readable is actually we defined free

00:18:32,700 --> 00:18:37,740
flows an extract flow which is one group

00:18:34,890 --> 00:18:39,780
a transform flow which is another group

00:18:37,740 --> 00:18:43,020
that also happens to contain a couple of

00:18:39,780 --> 00:18:45,270
chains and then a load flow which is the

00:18:43,020 --> 00:18:48,000
quality task and our aggregation tasks

00:18:45,270 --> 00:18:49,970
and then our all flow is the chaining of

00:18:48,000 --> 00:18:52,470
all three of those sub flows together

00:18:49,970 --> 00:18:54,510
the met the process of running that we

00:18:52,470 --> 00:18:57,660
then simply be a matter of all flow dot

00:18:54,510 --> 00:18:59,280
apply async so once you've done that

00:18:57,660 --> 00:19:00,660
you've sent off all these tasks and

00:18:59,280 --> 00:19:03,030
they're they're sitting in the broker

00:19:00,660 --> 00:19:04,380
and sort of ticking over and the workers

00:19:03,030 --> 00:19:06,150
are picking them up and things can go

00:19:04,380 --> 00:19:07,860
wrong and so on we need to think about

00:19:06,150 --> 00:19:09,860
ways that we can actually monitor this

00:19:07,860 --> 00:19:13,320
and see how it's going

00:19:09,860 --> 00:19:14,880
so celery has within it a an events view

00:19:13,320 --> 00:19:18,450
which is a simple cursors based

00:19:14,880 --> 00:19:20,100
interface and it's actually I've had to

00:19:18,450 --> 00:19:21,840
blank some things out in there so that's

00:19:20,100 --> 00:19:23,340
what the gaps mean you can actually see

00:19:21,840 --> 00:19:24,480
how this flow is running and you can see

00:19:23,340 --> 00:19:26,730
where there are cords that are waiting

00:19:24,480 --> 00:19:29,250
to unlock because there are processes in

00:19:26,730 --> 00:19:30,809
action you can see that one of the

00:19:29,250 --> 00:19:32,610
transforms are started and some others

00:19:30,809 --> 00:19:34,170
have already finished so as this process

00:19:32,610 --> 00:19:35,910
is flowing as soon as you've kicked it

00:19:34,170 --> 00:19:37,800
off you can look at this view and see

00:19:35,910 --> 00:19:38,770
how things are ticking over you can also

00:19:37,800 --> 00:19:41,260
navigate in and

00:19:38,770 --> 00:19:42,910
the results of each of these I didn't

00:19:41,260 --> 00:19:45,970
point out specifically before that each

00:19:42,910 --> 00:19:47,440
of your tasks can return a value in the

00:19:45,970 --> 00:19:50,710
example I just returned the row count

00:19:47,440 --> 00:19:53,260
but you can also return for example a

00:19:50,710 --> 00:19:55,390
dictionary containing multiple bits of

00:19:53,260 --> 00:19:56,920
information maybe some messages some

00:19:55,390 --> 00:19:59,860
kind of metadata about how things are

00:19:56,920 --> 00:20:02,260
going there's another view into things

00:19:59,860 --> 00:20:04,390
which is more modern but this is sort of

00:20:02,260 --> 00:20:05,650
processing I found was less real time

00:20:04,390 --> 00:20:07,960
because it doesn't refresh as often

00:20:05,650 --> 00:20:11,220
called flower and this gives you a

00:20:07,960 --> 00:20:13,420
web-based view have much the same thing

00:20:11,220 --> 00:20:15,670
so as I said before this is really a

00:20:13,420 --> 00:20:17,590
simple example turning it up to 11 you

00:20:15,670 --> 00:20:20,500
can add all sorts of things to this

00:20:17,590 --> 00:20:22,330
infrastructure I've added a requires and

00:20:20,500 --> 00:20:25,060
a depend structure to this where each

00:20:22,330 --> 00:20:27,370
task says what it provides and says what

00:20:25,060 --> 00:20:28,900
it needs to do that and so there's a

00:20:27,370 --> 00:20:31,480
unit test that once you've defined a

00:20:28,900 --> 00:20:34,660
flow asserts that every task in the

00:20:31,480 --> 00:20:36,490
pipeline actually gets provided what it

00:20:34,660 --> 00:20:38,490
needs upstream and provides to the

00:20:36,490 --> 00:20:41,910
things downstream that it's supposed to

00:20:38,490 --> 00:20:47,050
doing things like incremental data loads

00:20:41,910 --> 00:20:48,820
lets you rather than doing on mass

00:20:47,050 --> 00:20:50,200
replacements through your data flow you

00:20:48,820 --> 00:20:52,390
could look for say just the last 24

00:20:50,200 --> 00:20:54,310
hours of data and not reload everything

00:20:52,390 --> 00:20:56,260
over and over again this has a lot of

00:20:54,310 --> 00:20:57,940
potential to speed up processing time

00:20:56,260 --> 00:20:59,410
where you're just loading the new data

00:20:57,940 --> 00:21:01,630
you're just processing the new data

00:20:59,410 --> 00:21:03,010
you're just reporting or perhaps you

00:21:01,630 --> 00:21:05,260
have to rebuild the reports at the end

00:21:03,010 --> 00:21:06,790
because of aggregations but it certainly

00:21:05,260 --> 00:21:10,240
can speed things up this particular

00:21:06,790 --> 00:21:11,770
reporting system took a process that for

00:21:10,240 --> 00:21:14,320
whatever reason it took four hours to

00:21:11,770 --> 00:21:16,300
run and now after an initial run of

00:21:14,320 --> 00:21:20,620
several minutes can potentially run in

00:21:16,300 --> 00:21:22,870
several seconds you can also potentially

00:21:20,620 --> 00:21:24,370
parameterize these flows and say well I

00:21:22,870 --> 00:21:26,290
have a nightly run that actually does

00:21:24,370 --> 00:21:28,450
everything but maybe every four hours I

00:21:26,290 --> 00:21:30,940
just want to refresh the extracts and

00:21:28,450 --> 00:21:33,490
rerun the data quality report which only

00:21:30,940 --> 00:21:35,410
needs the extracts to have run you can

00:21:33,490 --> 00:21:38,410
customize and choose which bits of this

00:21:35,410 --> 00:21:40,090
you want to do tracking flow history

00:21:38,410 --> 00:21:41,830
actually storing the results each time

00:21:40,090 --> 00:21:43,780
that you run your flow means that you

00:21:41,830 --> 00:21:46,150
can keep track of just as you would with

00:21:43,780 --> 00:21:48,640
any other batch system knowing historic

00:21:46,150 --> 00:21:49,840
how things have gone and of course with

00:21:48,640 --> 00:21:51,580
the power of Python you can hook into

00:21:49,840 --> 00:21:52,900
just about anything else if you want to

00:21:51,580 --> 00:21:55,540
do some fancy natural language

00:21:52,900 --> 00:21:57,780
processing or scientific computing in

00:21:55,540 --> 00:22:00,010
your transforms you certainly could

00:21:57,780 --> 00:22:03,700
really though this this particular

00:22:00,010 --> 00:22:07,270
system is about getting data in a well

00:22:03,700 --> 00:22:09,490
enough mangled form or unmingled form to

00:22:07,270 --> 00:22:14,230
do those those analytics in a separate

00:22:09,490 --> 00:22:16,240
environment so I'm running quite quickly

00:22:14,230 --> 00:22:18,490
through this to summarize basically we

00:22:16,240 --> 00:22:20,350
talked about an intro to the concept of

00:22:18,490 --> 00:22:24,940
data warehousing or data integration and

00:22:20,350 --> 00:22:26,320
I've shown a simple example of how one

00:22:24,940 --> 00:22:28,420
would process started with SQL alchemy

00:22:26,320 --> 00:22:33,040
and then string it all together using

00:22:28,420 --> 00:22:34,450
celery's canvas i've got a list of

00:22:33,040 --> 00:22:36,700
resources here which will go up on

00:22:34,450 --> 00:22:39,360
SlideShare and that's actually me done

00:22:36,700 --> 00:22:39,360
any questions

00:22:45,230 --> 00:22:51,810
hi thanks to the top girls great um I

00:22:48,050 --> 00:22:57,120
actually do very similar things to what

00:22:51,810 --> 00:22:59,310
you're doing here at my work although my

00:22:57,120 --> 00:23:00,660
RM is kind of purchased use the Jango

00:22:59,310 --> 00:23:02,820
one because that's what I what I knew

00:23:00,660 --> 00:23:06,600
and I've thought about whether or not I

00:23:02,820 --> 00:23:07,710
should go to seek Wow alchemy a good

00:23:06,600 --> 00:23:10,050
thing with the Jango one is I've got

00:23:07,710 --> 00:23:11,670
south for my migrations what's the

00:23:10,050 --> 00:23:14,670
quality of seeker welcome means kind of

00:23:11,670 --> 00:23:16,590
migration handling for me does anything

00:23:14,670 --> 00:23:19,800
could exist so it was a question the

00:23:16,590 --> 00:23:25,050
quality of tools that can do schema

00:23:19,800 --> 00:23:27,090
migration for me yes yes there is there

00:23:25,050 --> 00:23:30,090
are two migration tools in SQL are

00:23:27,090 --> 00:23:31,890
coming ones call SQL alchemy migrate and

00:23:30,090 --> 00:23:35,580
I believe it's it's quite functional but

00:23:31,890 --> 00:23:38,640
is now sort of unofficially deprecated

00:23:35,580 --> 00:23:41,340
in favor of one called Alembic which

00:23:38,640 --> 00:23:43,080
will do migrations also it's something I

00:23:41,340 --> 00:23:45,210
didn't really touch on and it's a very

00:23:43,080 --> 00:23:47,250
good question because often your data

00:23:45,210 --> 00:23:48,990
changes and then your your flows need to

00:23:47,250 --> 00:23:51,470
update accordingly some of the things

00:23:48,990 --> 00:23:53,700
I've done is actually put poor men's

00:23:51,470 --> 00:23:55,730
migration into these flows where if a

00:23:53,700 --> 00:23:58,460
new column appears it will automatically

00:23:55,730 --> 00:24:01,320
put it through your your middle stage

00:23:58,460 --> 00:24:03,060
layers and you can expect it to show up

00:24:01,320 --> 00:24:04,650
on the report at the other end now you

00:24:03,060 --> 00:24:07,740
might not be comfortable with that and

00:24:04,650 --> 00:24:10,500
so then you could look at some some more

00:24:07,740 --> 00:24:11,910
specific migration tools at the moment

00:24:10,500 --> 00:24:14,580
it's it seems to be working well that

00:24:11,910 --> 00:24:16,050
this thing is kind of a more first in it

00:24:14,580 --> 00:24:18,570
it adds columns when it knows it needs

00:24:16,050 --> 00:24:20,850
to and then by the time you get to the

00:24:18,570 --> 00:24:23,130
end there's a certain amount of also

00:24:20,850 --> 00:24:25,320
putting testing in place and validation

00:24:23,130 --> 00:24:26,580
reports at the end to say this number

00:24:25,320 --> 00:24:29,180
should have come out or this column

00:24:26,580 --> 00:24:31,560
should have at least come out as well

00:24:29,180 --> 00:24:33,510
all right it's nice to see someone

00:24:31,560 --> 00:24:35,760
talking about sick love me yeah it's

00:24:33,510 --> 00:24:37,170
probably way more powerful than Django's

00:24:35,760 --> 00:24:39,390
it's not as expressive I think I've used

00:24:37,170 --> 00:24:41,930
both and yeah because pretty fantastic

00:24:39,390 --> 00:24:44,820
we use it in production my question is

00:24:41,930 --> 00:24:46,230
that you had a very structured way about

00:24:44,820 --> 00:24:47,790
it you know you had your classes and

00:24:46,230 --> 00:24:49,759
your subclasses and I was getting a bit

00:24:47,790 --> 00:24:52,169
of a Java feel it's

00:24:49,759 --> 00:24:55,620
I'm not trying to insult you anything

00:24:52,169 --> 00:24:57,600
but um but is this because you you often

00:24:55,620 --> 00:24:59,309
do these kind of things you've got a

00:24:57,600 --> 00:25:01,700
very structured way of doing it or

00:24:59,309 --> 00:25:04,500
you're just a very structured thinker

00:25:01,700 --> 00:25:06,269
exactly it kind of evolved that way

00:25:04,500 --> 00:25:08,730
first of all you've uncovered my dirty

00:25:06,269 --> 00:25:10,230
little secret that the 11 years I was at

00:25:08,730 --> 00:25:14,429
the business intelligence vendor was bad

00:25:10,230 --> 00:25:16,049
Java but it actually I started out

00:25:14,429 --> 00:25:19,169
writing this quite functionally with

00:25:16,049 --> 00:25:20,940
functions to do a lot of things and as

00:25:19,169 --> 00:25:24,059
soon as I wanted to refactor the common

00:25:20,940 --> 00:25:26,519
code I found myself in classes so I have

00:25:24,059 --> 00:25:28,740
a mix in for this and there's there's

00:25:26,519 --> 00:25:32,029
mix-ins for updating blank columns or

00:25:28,740 --> 00:25:34,230
for selecting rows out of a database and

00:25:32,029 --> 00:25:35,580
in some ways it's probably got a couple

00:25:34,230 --> 00:25:39,210
more layers and I'd like and it's time

00:25:35,580 --> 00:25:42,419
to refactor back down and perhaps this

00:25:39,210 --> 00:25:44,730
example shows about us as you know it

00:25:42,419 --> 00:25:46,200
shouldn't get any more sub class than

00:25:44,730 --> 00:25:49,259
that really once you've got those layers

00:25:46,200 --> 00:25:52,769
in the top you then just use them and

00:25:49,259 --> 00:25:54,570
then you have a flat structure so it's

00:25:52,769 --> 00:25:56,879
it's not necessarily strictly the case

00:25:54,570 --> 00:25:58,710
but I what happened in my cases that

00:25:56,879 --> 00:26:01,019
that worked out the best and the person

00:25:58,710 --> 00:26:05,340
who I was working with didn't disagree

00:26:01,019 --> 00:26:07,230
and he knows some Python too so yeah it

00:26:05,340 --> 00:26:09,299
seemed to work I'd be I'd be open to

00:26:07,230 --> 00:26:15,450
hearing alternatives to that I remember

00:26:09,299 --> 00:26:19,019
saying it's perfect by any means yep how

00:26:15,450 --> 00:26:20,970
do you pass data between each stage in

00:26:19,019 --> 00:26:23,519
the pipeline can you take this with the

00:26:20,970 --> 00:26:25,769
output of one and pipe it into the next

00:26:23,519 --> 00:26:29,009
stage how have you found the best way to

00:26:25,769 --> 00:26:30,779
do that very good question you can

00:26:29,009 --> 00:26:33,750
actually have the tasks receive input

00:26:30,779 --> 00:26:36,029
and provide output but for the purposes

00:26:33,750 --> 00:26:38,009
of these flows they actually don't take

00:26:36,029 --> 00:26:41,100
any parameters or produce any result

00:26:38,009 --> 00:26:43,679
that's consumed by other steps it's

00:26:41,100 --> 00:26:46,019
certainly possible but what we decided

00:26:43,679 --> 00:26:47,549
in this was actually we only have one

00:26:46,019 --> 00:26:49,259
database that actually is the core of

00:26:47,549 --> 00:26:51,120
all of this this warehousing once

00:26:49,259 --> 00:26:53,820
everything's extracted the rest of the

00:26:51,120 --> 00:26:55,379
process is the same database so at that

00:26:53,820 --> 00:26:57,450
point we know what tables are in there

00:26:55,379 --> 00:27:00,090
and we let the data speak for itself

00:26:57,450 --> 00:27:02,190
so in more complicated cases we actually

00:27:00,090 --> 00:27:06,600
put timestamps on things and we say

00:27:02,190 --> 00:27:08,070
the last modified record in or the last

00:27:06,600 --> 00:27:11,720
modified column even we actually have

00:27:08,070 --> 00:27:14,730
per column time stamps was this time

00:27:11,720 --> 00:27:16,440
let's look back at the the columns that

00:27:14,730 --> 00:27:19,980
feed that and get any data that's newer

00:27:16,440 --> 00:27:22,110
or refreshed and rerun those but it's

00:27:19,980 --> 00:27:24,240
purely the data lives in the database

00:27:22,110 --> 00:27:25,650
and the transforms by virtue of the fact

00:27:24,240 --> 00:27:26,730
that they know that they have to come

00:27:25,650 --> 00:27:29,070
one after one another

00:27:26,730 --> 00:27:31,920
trust that the last step is put stuff in

00:27:29,070 --> 00:27:33,570
there but as I said it is possible for

00:27:31,920 --> 00:27:35,760
the maybe the upstream process to

00:27:33,570 --> 00:27:38,550
communicate something to another task to

00:27:35,760 --> 00:27:39,900
say how you need to know about this but

00:27:38,550 --> 00:27:42,060
it's not so much of the streaming data

00:27:39,900 --> 00:27:44,970
level that's really pick it up put it

00:27:42,060 --> 00:27:47,820
down pick it up put it down in terms of

00:27:44,970 --> 00:27:50,750
its structure are there any other

00:27:47,820 --> 00:27:50,750
questions for Roger

00:27:54,419 --> 00:28:00,690
I know that there are a fair few options

00:27:58,289 --> 00:28:02,850
with salary for how you actually pass

00:28:00,690 --> 00:28:05,700
messages around and I was wondering

00:28:02,850 --> 00:28:07,649
pubbing RabbitMQ or Redis or and I

00:28:05,700 --> 00:28:09,629
presume you can use the database itself

00:28:07,649 --> 00:28:11,039
did you actually need to care about that

00:28:09,629 --> 00:28:12,809
that side of things or was there any

00:28:11,039 --> 00:28:16,230
particular reason you chose one solution

00:28:12,809 --> 00:28:17,489
or another not particularly the because

00:28:16,230 --> 00:28:20,429
it's really using salary in such a

00:28:17,489 --> 00:28:23,220
different way things like you know

00:28:20,429 --> 00:28:24,600
concurrency and how it handles messages

00:28:23,220 --> 00:28:27,570
and whether you can have different kinds

00:28:24,600 --> 00:28:29,759
of queues weren't a factor I've used

00:28:27,570 --> 00:28:32,909
both rabbit and Redis and at one point I

00:28:29,759 --> 00:28:36,299
was using SQL alchemy even itself as as

00:28:32,909 --> 00:28:38,820
a back-end Broker and SQL alchemy wasn't

00:28:36,299 --> 00:28:41,159
full-featured so I dropped that simply

00:28:38,820 --> 00:28:43,109
because I wasn't getting events and sort

00:28:41,159 --> 00:28:45,600
of broker level information in the in

00:28:43,109 --> 00:28:46,799
the monitoring but reticent rabbit have

00:28:45,600 --> 00:28:48,419
both been fine

00:28:46,799 --> 00:28:52,019
a lot of the decision-making came down

00:28:48,419 --> 00:28:54,090
to whether some of these ran better on

00:28:52,019 --> 00:28:56,639
Windows or not I don't use Windows but

00:28:54,090 --> 00:28:58,379
my colleague does and so sometimes that

00:28:56,639 --> 00:29:04,399
was a factor but both actually works

00:28:58,379 --> 00:29:04,399
just fine anybody else

00:29:11,309 --> 00:29:18,759
well Rajat here is a a packet of the

00:29:15,610 --> 00:29:19,960
Norwegian coast of the coffee that was

00:29:18,759 --> 00:29:21,399
being sent through the conference and

00:29:19,960 --> 00:29:23,529
here is the conference Python au

00:29:21,399 --> 00:29:25,119
conference mug and please join me in

00:29:23,529 --> 00:29:27,419
thanking Roger bands thank your time

00:29:25,119 --> 00:29:27,419

YouTube URL: https://www.youtube.com/watch?v=AhIoAMltzVw


