Title: scikit-learn, machine learning and cybercrime attribution
Publication date: 2013-07-11
Playlist: Pycon Australia 2013
Description: 
	Robert Layton
http://2013.pycon-au.org/schedule/30019/view_talk
The scikit-learn library is a rapidly growing open source toolkit for machine learning in python. It allows for practitioners and researchers to apply machine learning in a variety of applications and is used by companies worldwide. Developed by programmers from around the world, the project has a large (and increasing) number of machine learning algorithms, a very useful set of utility functions and has also spawned a set of detail
Captions: 
	00:00:00,140 --> 00:00:07,460
so the next speaker is Robert Leighton

00:00:03,540 --> 00:00:09,809
and like to introduce him he is

00:00:07,460 --> 00:00:11,849
researcher at the Internet commerce

00:00:09,809 --> 00:00:15,089
security lab at the university of

00:00:11,849 --> 00:00:17,430
ballarat and he's worked on projects and

00:00:15,089 --> 00:00:20,970
supervising students across all three of

00:00:17,430 --> 00:00:24,390
these areas so that is internet commerce

00:00:20,970 --> 00:00:26,250
and security and so he's been

00:00:24,390 --> 00:00:28,380
investigating cybercrime attribution

00:00:26,250 --> 00:00:30,390
using features within attacks to

00:00:28,380 --> 00:00:32,809
determine patterns and build machine

00:00:30,390 --> 00:00:35,010
learning models around those patterns so

00:00:32,809 --> 00:00:37,500
if we talking today about using

00:00:35,010 --> 00:00:38,790
scikit-learn to help with cyber cyber

00:00:37,500 --> 00:00:47,610
crime attribution so thanks very much

00:00:38,790 --> 00:00:49,649
Robert thank you yes so thanks for

00:00:47,610 --> 00:00:51,329
coming to my talk I wear a number of

00:00:49,649 --> 00:00:53,340
different hats after this presentation

00:00:51,329 --> 00:00:56,010
I'm a research fellow the university of

00:00:53,340 --> 00:00:58,199
ballarat in the internet commerce key

00:00:56,010 --> 00:00:59,520
laboratory and we have industry partners

00:00:58,199 --> 00:01:03,270
Westpac in the Australian Federal Police

00:00:59,520 --> 00:01:06,840
and IBM and basically our goal is to

00:01:03,270 --> 00:01:09,119
disrupt cybercrime the word disrupt is

00:01:06,840 --> 00:01:10,970
deliberately broad and in many ways our

00:01:09,119 --> 00:01:14,810
research in the areas deliberately broad

00:01:10,970 --> 00:01:16,619
I'm also a contributor to scikit-learn

00:01:14,810 --> 00:01:19,439
library I wouldn't say I'm a core

00:01:16,619 --> 00:01:23,119
contributor but I try and you know do as

00:01:19,439 --> 00:01:25,439
much as I can and the scikit-learn

00:01:23,119 --> 00:01:27,590
library is pretty much a base dependency

00:01:25,439 --> 00:01:29,970
forever all the research that I do

00:01:27,590 --> 00:01:32,450
there's a number of positives to using

00:01:29,970 --> 00:01:35,100
this from machine learning point of view

00:01:32,450 --> 00:01:37,290
they give us a sort of framework that

00:01:35,100 --> 00:01:40,979
easily allows us to develop new models

00:01:37,290 --> 00:01:43,979
test them out don't have to write cross

00:01:40,979 --> 00:01:45,930
validation code and it gives us access

00:01:43,979 --> 00:01:49,619
to a lot of frameworks that are really

00:01:45,930 --> 00:01:51,060
useful so there's three main chunks in

00:01:49,619 --> 00:01:53,790
my talk today are we talking about a

00:01:51,060 --> 00:01:55,560
psychic learning library for those that

00:01:53,790 --> 00:01:58,020
haven't heard of it or want to know a

00:01:55,560 --> 00:02:01,350
little bit more information be talking

00:01:58,020 --> 00:02:03,000
about how to contribute and you know

00:02:01,350 --> 00:02:06,990
some future directions we're hoping to

00:02:03,000 --> 00:02:09,720
go in also going to be talking a very

00:02:06,990 --> 00:02:11,200
very small introduction to machine

00:02:09,720 --> 00:02:14,349
learning itself

00:02:11,200 --> 00:02:16,750
you know basically for those that are

00:02:14,349 --> 00:02:18,610
familiar with that and finally give a

00:02:16,750 --> 00:02:21,040
summary of some of my research in

00:02:18,610 --> 00:02:22,629
cybercrime attribution so it's an

00:02:21,040 --> 00:02:26,019
introductory talk in each of these three

00:02:22,629 --> 00:02:29,260
areas so I hope I hope at most people

00:02:26,019 --> 00:02:31,630
here are knowledgeable at two of the

00:02:29,260 --> 00:02:33,069
three areas and not all three but um so

00:02:31,630 --> 00:02:37,450
I hope they're fun I can get something

00:02:33,069 --> 00:02:39,310
gum out of it so scikit-learn library

00:02:37,450 --> 00:02:44,380
was originally a google Summer of Code

00:02:39,310 --> 00:02:49,690
project in 2007 it was revived in 2010

00:02:44,380 --> 00:02:53,590
and then they turned into a

00:02:49,690 --> 00:02:55,630
community-driven project we on github

00:02:53,590 --> 00:02:57,220
where most of our development happens

00:02:55,630 --> 00:03:04,239
there there in the mailing list of

00:02:57,220 --> 00:03:06,459
course and for but basically what

00:03:04,239 --> 00:03:07,989
separates our scikit-learn library from

00:03:06,459 --> 00:03:10,150
other machine learning libraries in

00:03:07,989 --> 00:03:14,230
Python is that we we try to be very

00:03:10,150 --> 00:03:16,090
accessible there's languages that do Big

00:03:14,230 --> 00:03:18,430
Data better as languages that do

00:03:16,090 --> 00:03:19,359
particular Owens better but from a

00:03:18,430 --> 00:03:23,680
personal point of view I think

00:03:19,359 --> 00:03:25,359
scikit-learn has that good focus where

00:03:23,680 --> 00:03:27,549
we've got a large number of algorithms

00:03:25,359 --> 00:03:31,480
they're all fairly well written they're

00:03:27,549 --> 00:03:35,920
all really well documented and if you

00:03:31,480 --> 00:03:38,079
want to do sort of research in in this

00:03:35,920 --> 00:03:41,829
area I find this library is really good

00:03:38,079 --> 00:03:43,660
because in many regards what's a machine

00:03:41,829 --> 00:03:44,889
learning research comes down to is we

00:03:43,660 --> 00:03:46,510
want to test that a big bunch of

00:03:44,889 --> 00:03:49,170
algorithms and if we can do all that in

00:03:46,510 --> 00:03:54,310
the same framework that really benefits

00:03:49,170 --> 00:03:56,739
moving the research long quickly so

00:03:54,310 --> 00:03:59,980
library itself has supervised

00:03:56,739 --> 00:04:02,799
unsupervised algorithms which I'll get

00:03:59,980 --> 00:04:05,349
to the distinction bit later has large

00:04:02,799 --> 00:04:07,450
number of metrics metrics and both

00:04:05,349 --> 00:04:09,340
ascend to the word where evaluation

00:04:07,450 --> 00:04:12,910
metrics such as accuracy and f1 score

00:04:09,340 --> 00:04:16,000
implemented as well as distance metrics

00:04:12,910 --> 00:04:18,240
like cosine and Manhattan distances are

00:04:16,000 --> 00:04:22,240
also in there

00:04:18,240 --> 00:04:24,730
there's the the feature that I find most

00:04:22,240 --> 00:04:28,630
useful about it is the framework code

00:04:24,730 --> 00:04:31,750
and there's a there's a few mix-ins

00:04:28,630 --> 00:04:34,510
which give us access to you know sort of

00:04:31,750 --> 00:04:35,740
an abstract class and from that abstract

00:04:34,510 --> 00:04:38,230
class we have things like cross

00:04:35,740 --> 00:04:40,600
validation code that just you know

00:04:38,230 --> 00:04:43,150
performs duck typing on that on those

00:04:40,600 --> 00:04:45,640
models and this is where I use the

00:04:43,150 --> 00:04:53,500
library in my research most is I have my

00:04:45,640 --> 00:04:55,840
models come off those mix-ins and that

00:04:53,500 --> 00:04:58,920
allows me to use the the evaluation

00:04:55,840 --> 00:05:01,060
codes within scikit-learn very easily

00:04:58,920 --> 00:05:04,030
one of the advantages I find about

00:05:01,060 --> 00:05:05,950
python is because it because you can

00:05:04,030 --> 00:05:08,110
code something up so quickly the

00:05:05,950 --> 00:05:11,680
developer then has more time to document

00:05:08,110 --> 00:05:12,910
and I think most most libraries that are

00:05:11,680 --> 00:05:14,740
going to be talked about in this

00:05:12,910 --> 00:05:17,080
conference have very fantastic

00:05:14,740 --> 00:05:19,210
documentation and I think this is a

00:05:17,080 --> 00:05:20,590
direct effect of the fact that you know

00:05:19,210 --> 00:05:22,450
you can code sound very quickly in

00:05:20,590 --> 00:05:26,470
Python and it means we have time to

00:05:22,450 --> 00:05:28,690
document it and scikit-learn is no

00:05:26,470 --> 00:05:31,240
different there's really fantastic

00:05:28,690 --> 00:05:34,240
documentation for I'd probably say about

00:05:31,240 --> 00:05:35,920
ninety percent of what's in there it's a

00:05:34,240 --> 00:05:37,810
if you're adding feet if you're

00:05:35,920 --> 00:05:39,370
contributing features to the library you

00:05:37,810 --> 00:05:42,040
have to write documentation or it won't

00:05:39,370 --> 00:05:44,560
get in and as well as that we also have

00:05:42,040 --> 00:05:49,540
num tutorials both in the official docs

00:05:44,560 --> 00:05:51,700
and unofficial and a most of the main

00:05:49,540 --> 00:05:54,670
developers except for myself have blogs

00:05:51,700 --> 00:05:57,010
that are regularly post on you know

00:05:54,670 --> 00:06:01,750
features or applications of scikit-learn

00:05:57,010 --> 00:06:05,080
I'm sorry as I said we're on github we

00:06:01,750 --> 00:06:08,800
use Python siphon is for use for speed

00:06:05,080 --> 00:06:10,870
in many cases we have eighty eight

00:06:08,800 --> 00:06:12,640
percent test coverage but this doesn't

00:06:10,870 --> 00:06:14,470
this is an adjusted for platform

00:06:12,640 --> 00:06:17,320
specific code which we have a little bit

00:06:14,470 --> 00:06:20,710
of and if you're interested in like

00:06:17,320 --> 00:06:22,810
contributing just jump on our mailing

00:06:20,710 --> 00:06:24,340
list introduce yourself and this is

00:06:22,810 --> 00:06:26,700
especially important if you're proposing

00:06:24,340 --> 00:06:31,050
new features because we are pretty

00:06:26,700 --> 00:06:33,120
about coding styles so a keyword

00:06:31,050 --> 00:06:36,750
compliant from a machine learning point

00:06:33,120 --> 00:06:41,280
of view svm naive Bayes hit a Markov

00:06:36,750 --> 00:06:43,530
models we have a very large number of

00:06:41,280 --> 00:06:48,660
algorithms it's not comprehensive in any

00:06:43,530 --> 00:06:51,990
regard but we're getting closer so for

00:06:48,660 --> 00:06:54,240
those that know what these words are you

00:06:51,990 --> 00:06:58,050
know there's a large very large chunk of

00:06:54,240 --> 00:07:01,950
algorithms and as well as that we have

00:06:58,050 --> 00:07:04,740
plenty of examples of usage not only you

00:07:01,950 --> 00:07:07,830
know your sort of 101 introductory usage

00:07:04,740 --> 00:07:09,120
but also more in-depth applications that

00:07:07,830 --> 00:07:13,470
are pretty standard like topic

00:07:09,120 --> 00:07:18,270
clustering so just go over a couple of

00:07:13,470 --> 00:07:21,150
tutorials with we have the the one in

00:07:18,270 --> 00:07:23,040
the official docs was originally for one

00:07:21,150 --> 00:07:25,950
hour tutorial so there's a large amount

00:07:23,040 --> 00:07:29,130
of content in this tutorial and it's now

00:07:25,950 --> 00:07:31,560
part of the documentation are not shorts

00:07:29,130 --> 00:07:35,220
in the last release bro it's definitely

00:07:31,560 --> 00:07:37,590
in dev and this gives an introduction to

00:07:35,220 --> 00:07:40,710
machine learning which is very

00:07:37,590 --> 00:07:44,880
accessible for anyone that hasn't done

00:07:40,710 --> 00:07:47,850
it in the past and uses scikit-learn for

00:07:44,880 --> 00:07:49,710
all the code examples going through this

00:07:47,850 --> 00:07:51,000
tutorial even if you have some machine

00:07:49,710 --> 00:07:53,700
learning knowledge is a very useful

00:07:51,000 --> 00:07:57,150
exercise in covering the gaps and making

00:07:53,700 --> 00:08:00,990
sure you're aware of everything there's

00:07:57,150 --> 00:08:03,990
lots of applications so in here we have

00:08:00,990 --> 00:08:06,570
an eigen face extractor where available

00:08:03,990 --> 00:08:08,130
to you sort of extract features from

00:08:06,570 --> 00:08:12,990
faces and use that for classification

00:08:08,130 --> 00:08:16,140
later another tutorial is yesterday ml

00:08:12,990 --> 00:08:20,850
what about I think a couple of our core

00:08:16,140 --> 00:08:25,020
developers are sort of astro people and

00:08:20,850 --> 00:08:28,370
they use there's another library astro

00:08:25,020 --> 00:08:33,120
ml which has a link down the bottom and

00:08:28,370 --> 00:08:34,780
and they this library is based off

00:08:33,120 --> 00:08:37,310
scikit-learn

00:08:34,780 --> 00:08:39,350
so if you're in that field thence the

00:08:37,310 --> 00:08:41,090
good library to have a look at but

00:08:39,350 --> 00:08:46,880
they've developed a tutorial on using it

00:08:41,090 --> 00:08:48,710
for a stroller astronomy and a well

00:08:46,880 --> 00:08:50,750
again while you may not have a

00:08:48,710 --> 00:08:53,320
particular application in this area this

00:08:50,750 --> 00:08:55,580
is a good tutorial to go through as well

00:08:53,320 --> 00:08:58,790
both tutorials they are really well

00:08:55,580 --> 00:09:02,060
written as well as that we have a large

00:08:58,790 --> 00:09:03,740
number of tutorials at that page some of

00:09:02,060 --> 00:09:06,170
them are quite short and very specific

00:09:03,740 --> 00:09:09,190
and some of them are quite long from

00:09:06,170 --> 00:09:12,020
tutorial sessions at other conferences

00:09:09,190 --> 00:09:13,820
as well as that some some tutorials

00:09:12,020 --> 00:09:17,240
itself a hosted on github and allow

00:09:13,820 --> 00:09:19,430
contributors and and they're getting

00:09:17,240 --> 00:09:23,810
quite active in particularly one of the

00:09:19,430 --> 00:09:26,300
tutorials so what is machine learning

00:09:23,810 --> 00:09:29,180
and the disclaimer down the bottom is

00:09:26,300 --> 00:09:30,920
very important you know magic hand

00:09:29,180 --> 00:09:32,810
waving some of this stuff here is not

00:09:30,920 --> 00:09:38,360
technically accurate but it's good

00:09:32,810 --> 00:09:39,950
enough so machine we basically is trying

00:09:38,360 --> 00:09:43,150
to find information from data and

00:09:39,950 --> 00:09:45,470
there's a large number of applications

00:09:43,150 --> 00:09:47,180
classification is one most people

00:09:45,470 --> 00:09:49,760
familiar with you know is this

00:09:47,180 --> 00:09:53,120
particular spam is this particular email

00:09:49,760 --> 00:09:56,030
spam or not facial recognition is

00:09:53,120 --> 00:10:01,010
another one image segmentation is

00:09:56,030 --> 00:10:02,900
another one and but what does the actual

00:10:01,010 --> 00:10:04,960
data look like and there's many

00:10:02,900 --> 00:10:07,490
different ways to represent data the

00:10:04,960 --> 00:10:11,510
model that scikit-learn uses is

00:10:07,490 --> 00:10:15,470
basically an XY model so x is a 2d array

00:10:11,510 --> 00:10:19,490
of feature values and each row in this

00:10:15,470 --> 00:10:24,170
matrix is one thing and each column is

00:10:19,490 --> 00:10:25,970
one attribute of that thing so the

00:10:24,170 --> 00:10:30,250
example I've got up here is from a

00:10:25,970 --> 00:10:33,530
famous data set of a virus plants and

00:10:30,250 --> 00:10:35,090
the first row corresponds to one plant

00:10:33,530 --> 00:10:39,110
the second row corresponds to another

00:10:35,090 --> 00:10:41,210
plan the first column I think is the

00:10:39,110 --> 00:10:43,470
length of the petals on that plan and

00:10:41,210 --> 00:10:45,590
the second column is a width

00:10:43,470 --> 00:10:48,690
I might have those around the other way

00:10:45,590 --> 00:10:50,760
so we can say that for instance the

00:10:48,690 --> 00:10:54,840
width of the pedal of the first plan is

00:10:50,760 --> 00:10:56,670
3.5 centimeters or equivalent so this is

00:10:54,840 --> 00:11:01,290
our X matrix and this describes our

00:10:56,670 --> 00:11:04,200
things the way the Y array is basically

00:11:01,290 --> 00:11:06,570
what type of thing isn't and in the

00:11:04,200 --> 00:11:09,930
classification task this might be 0 if

00:11:06,570 --> 00:11:12,840
the email is not spam and one if it is

00:11:09,930 --> 00:11:15,120
spam and basically the length of this Y

00:11:12,840 --> 00:11:20,520
array has to be the same as the number

00:11:15,120 --> 00:11:22,320
of rows and X matrix and basically X I

00:11:20,520 --> 00:11:27,210
are the features for the object why I

00:11:22,320 --> 00:11:30,210
tells us what that object is so this is

00:11:27,210 --> 00:11:36,810
the data model that the numpy are that

00:11:30,210 --> 00:11:38,970
scikit-learn uses yeah so the

00:11:36,810 --> 00:11:41,850
supervisors unsupervised distinction if

00:11:38,970 --> 00:11:43,380
ahead of time we know what Y is like we

00:11:41,850 --> 00:11:44,940
know what sort of things we're looking

00:11:43,380 --> 00:11:48,090
for then it's called a supervised

00:11:44,940 --> 00:11:49,980
learning task there and we use

00:11:48,090 --> 00:11:52,440
algorithms like support vector machines

00:11:49,980 --> 00:11:54,660
or naive bayes we build a model around

00:11:52,440 --> 00:11:57,030
those things we do know this so that we

00:11:54,660 --> 00:11:59,880
can predict what the type is of things

00:11:57,030 --> 00:12:01,950
we don't know so we have a corpus of

00:11:59,880 --> 00:12:05,100
email we build a model based on whether

00:12:01,950 --> 00:12:07,050
it's spam or not we use that model to

00:12:05,100 --> 00:12:10,590
look at new emails and determine where

00:12:07,050 --> 00:12:12,150
those new emails are spam or not if

00:12:10,590 --> 00:12:14,330
ahead of time we don't know what Y is

00:12:12,150 --> 00:12:18,120
it's an unsupervised learning problem

00:12:14,330 --> 00:12:19,920
and one of the most common examples of

00:12:18,120 --> 00:12:21,450
this is topic clustering you know we

00:12:19,920 --> 00:12:24,990
have this large collection of documents

00:12:21,450 --> 00:12:28,710
and we want to know you know what sort

00:12:24,990 --> 00:12:32,580
of topics are in that in that corpus so

00:12:28,710 --> 00:12:35,100
what we do is we we perform a clustering

00:12:32,580 --> 00:12:37,200
and put documents that are very similar

00:12:35,100 --> 00:12:38,760
to each other together and then those

00:12:37,200 --> 00:12:42,390
groups that come out from that classroom

00:12:38,760 --> 00:12:43,740
process give us not not a topic but it

00:12:42,390 --> 00:12:45,450
gives us a group and then we can

00:12:43,740 --> 00:12:47,460
manually examine that group and see what

00:12:45,450 --> 00:12:48,780
sort of topic all those things are if we

00:12:47,460 --> 00:12:51,540
do this well this can be a very good

00:12:48,780 --> 00:12:52,980
exploratory analysis on data tells us

00:12:51,540 --> 00:12:55,610
what's in data without knowing

00:12:52,980 --> 00:12:55,610
beforehand

00:12:55,920 --> 00:13:02,230
so with classification as I said we have

00:12:59,140 --> 00:13:05,740
a large number of dots there the color

00:13:02,230 --> 00:13:08,200
is what we knew beforehand and these are

00:13:05,740 --> 00:13:10,780
you know plotted against two different

00:13:08,200 --> 00:13:13,120
attribute values so the dots of what we

00:13:10,780 --> 00:13:17,350
know beforehand and the background color

00:13:13,120 --> 00:13:21,760
is what we would predict a new object to

00:13:17,350 --> 00:13:24,520
be so you know if we had a new object

00:13:21,760 --> 00:13:26,200
with values eight and two it would

00:13:24,520 --> 00:13:32,110
appear in the blue area and we will

00:13:26,200 --> 00:13:38,020
predict that it's that object it's very

00:13:32,110 --> 00:13:40,300
easy to do you know line one the basic

00:13:38,020 --> 00:13:43,030
import line two and three is basically

00:13:40,300 --> 00:13:44,950
extracting the data you know this could

00:13:43,030 --> 00:13:47,680
be pre-computer beforehand or computed

00:13:44,950 --> 00:13:52,090
some other way line four we develop a

00:13:47,680 --> 00:13:55,090
model so pretty much we're just you know

00:13:52,090 --> 00:13:58,680
instantiate it line five is we fit that

00:13:55,090 --> 00:14:01,720
model to the data that we know of and

00:13:58,680 --> 00:14:05,310
this sets internal attributes and

00:14:01,720 --> 00:14:09,040
parameters and then line 6 is we have

00:14:05,310 --> 00:14:11,230
new data that we don't know we we call

00:14:09,040 --> 00:14:13,150
the predict function on our model and it

00:14:11,230 --> 00:14:16,660
tells us what at least the model thinks

00:14:13,150 --> 00:14:19,870
it is so in six lines of code we

00:14:16,660 --> 00:14:26,110
developed that dr. graf but the model

00:14:19,870 --> 00:14:28,300
for it so one of the big benefits is

00:14:26,110 --> 00:14:30,460
that of the psyche alone is that that

00:14:28,300 --> 00:14:32,650
line for can be swapped out and replaced

00:14:30,460 --> 00:14:36,970
with other algorithms and this whole

00:14:32,650 --> 00:14:40,000
process generally still works we have a

00:14:36,970 --> 00:14:43,240
large number of algorithms and as you

00:14:40,000 --> 00:14:45,250
can see with some sort of those

00:14:43,240 --> 00:14:51,250
background colors they all work in

00:14:45,250 --> 00:14:53,620
different ways and the meaning behind

00:14:51,250 --> 00:14:57,000
you know how they work why they've

00:14:53,620 --> 00:14:59,320
chosen those decision boundaries is

00:14:57,000 --> 00:15:01,990
positive machine learning literature and

00:14:59,320 --> 00:15:05,120
something if you're interested to talk

00:15:01,990 --> 00:15:07,020
to me talk to me about it later

00:15:05,120 --> 00:15:09,960
so we have a large number of algorithms

00:15:07,020 --> 00:15:18,690
they're all good in some ways that in

00:15:09,960 --> 00:15:21,800
other ways fast slow scalable not so

00:15:18,690 --> 00:15:25,290
this is the case where with that Y value

00:15:21,800 --> 00:15:27,420
we know what it is before hand and it's

00:15:25,290 --> 00:15:28,890
also distinct classes you know it's this

00:15:27,420 --> 00:15:31,440
type of plan or it's just type of

00:15:28,890 --> 00:15:35,250
planets spam or it's not if we don't

00:15:31,440 --> 00:15:37,550
have that sort of discrete knowledge we

00:15:35,250 --> 00:15:40,980
have a similar problem called regression

00:15:37,550 --> 00:15:43,050
now regression is you know it's not spam

00:15:40,980 --> 00:15:46,740
or not but it might be what is the

00:15:43,050 --> 00:15:48,660
height of someone given you know their

00:15:46,740 --> 00:15:51,630
weight and gender you know how can we

00:15:48,660 --> 00:15:53,430
predict their height from that and their

00:15:51,630 --> 00:15:55,740
height is a continuous valuable it could

00:15:53,430 --> 00:15:57,420
be anything from you know one value to

00:15:55,740 --> 00:16:00,000
another and could be you know at any

00:15:57,420 --> 00:16:04,250
point in between there and this this

00:16:00,000 --> 00:16:06,870
sort of problem is called regression and

00:16:04,250 --> 00:16:10,320
pretty much we can just have a look at

00:16:06,870 --> 00:16:11,910
how different models so in this figure

00:16:10,320 --> 00:16:13,410
we have different dots which are our

00:16:11,910 --> 00:16:15,900
ground troops so this is what we knew

00:16:13,410 --> 00:16:18,840
beforehand and our lines are trying to

00:16:15,900 --> 00:16:22,110
fit a model to the to that ground truth

00:16:18,840 --> 00:16:24,750
to try and predict the new values and

00:16:22,110 --> 00:16:30,930
you know this is a this is a tricky

00:16:24,750 --> 00:16:35,240
problem in many cases and to have a look

00:16:30,930 --> 00:16:39,480
at the regression toad nothing changed

00:16:35,240 --> 00:16:41,880
line four is a different model but

00:16:39,480 --> 00:16:44,280
that's it line ones a different input

00:16:41,880 --> 00:16:45,900
line for is a different model but as

00:16:44,280 --> 00:16:48,810
well you know but that framework is

00:16:45,900 --> 00:16:50,880
still the same and this is this is

00:16:48,810 --> 00:16:54,360
something we can rely on for new models

00:16:50,880 --> 00:16:59,310
in the library now for unsupervised

00:16:54,360 --> 00:17:01,050
learning we have a whole bunch of dots

00:16:59,310 --> 00:17:02,700
there and we don't know they're all

00:17:01,050 --> 00:17:06,060
black you know we don't know beforehand

00:17:02,700 --> 00:17:10,410
what any of them are and what this data

00:17:06,060 --> 00:17:12,689
set is is images of digits so drawn or

00:17:10,410 --> 00:17:13,439
someone's drawing a one or two or three

00:17:12,689 --> 00:17:17,329
it's been

00:17:13,439 --> 00:17:19,740
represented using its intensity values

00:17:17,329 --> 00:17:22,919
then we've plotted against these two

00:17:19,740 --> 00:17:24,569
dimensions and we've performed some

00:17:22,919 --> 00:17:26,939
class trains try and find different

00:17:24,569 --> 00:17:29,750
classes now the background color is what

00:17:26,939 --> 00:17:32,279
it's going to be predicted as and as

00:17:29,750 --> 00:17:35,190
I've said you know down the bottom

00:17:32,279 --> 00:17:37,139
ignoring some complex philosophical to

00:17:35,190 --> 00:17:38,940
start discussions about clusters and

00:17:37,139 --> 00:17:44,639
classes this is about sixty percent

00:17:38,940 --> 00:17:45,690
accurate the more hand waving the the

00:17:44,639 --> 00:17:46,980
points that are going to be clustered

00:17:45,690 --> 00:17:49,200
together are generally going to be the

00:17:46,980 --> 00:17:52,139
same number about sixty percent of the

00:17:49,200 --> 00:17:54,029
time and this is without you know any

00:17:52,139 --> 00:17:58,559
prior knowledge except for the number of

00:17:54,029 --> 00:18:00,629
clusters in this case the code for

00:17:58,559 --> 00:18:02,759
clustering is pretty much the same

00:18:00,629 --> 00:18:07,139
except we don't worry about why because

00:18:02,759 --> 00:18:09,240
we don't know it and line 5 o's we just

00:18:07,139 --> 00:18:10,830
extract the labels themselves from the

00:18:09,240 --> 00:18:12,840
model rather than try and predict new

00:18:10,830 --> 00:18:15,360
data generally with clustering its

00:18:12,840 --> 00:18:18,000
exploratory know we have our one data

00:18:15,360 --> 00:18:20,250
source and we try and put classes on to

00:18:18,000 --> 00:18:23,639
that data source rather than take new

00:18:20,250 --> 00:18:28,919
data and we have a whole bunch of

00:18:23,639 --> 00:18:33,360
different algorithms one of one of the

00:18:28,919 --> 00:18:36,809
key issues in the unsupervised field is

00:18:33,360 --> 00:18:39,539
that is that of parameters the last

00:18:36,809 --> 00:18:41,700
column here the DB scan algorithm sort

00:18:39,539 --> 00:18:44,610
of appears to clustering very very very

00:18:41,700 --> 00:18:45,899
well but this was bit ly with the

00:18:44,610 --> 00:18:48,419
parameters you know you had to choose

00:18:45,899 --> 00:18:51,000
parameters beforehand they gave us this

00:18:48,419 --> 00:18:55,860
rather than just some magic algorithm

00:18:51,000 --> 00:18:58,919
which does what we want all the time so

00:18:55,860 --> 00:19:02,070
why why you scikit-learn i clear easy to

00:18:58,919 --> 00:19:06,120
use api's at some very cut fast in most

00:19:02,070 --> 00:19:09,539
use cases we make good use of siphon you

00:19:06,120 --> 00:19:12,090
know as was introduced earlier you know

00:19:09,539 --> 00:19:16,710
which gives us python write code that

00:19:12,090 --> 00:19:18,490
runs really fast we have good

00:19:16,710 --> 00:19:21,550
paralyzation

00:19:18,490 --> 00:19:23,500
it's easy to use I would say it's the

00:19:21,550 --> 00:19:26,380
most scalable thing but it's incredibly

00:19:23,500 --> 00:19:30,460
easy to use and it's well integrated

00:19:26,380 --> 00:19:32,500
within scikit-learn lots of utility

00:19:30,460 --> 00:19:35,380
functions as I said you know I I don't

00:19:32,500 --> 00:19:37,660
for my research I don't build models

00:19:35,380 --> 00:19:40,120
that are already part of the psychic

00:19:37,660 --> 00:19:42,040
learning library I build new models but

00:19:40,120 --> 00:19:43,630
I can just use the loot utility

00:19:42,040 --> 00:19:45,700
functions within scikit-learn very

00:19:43,630 --> 00:19:48,670
easily these are some complex utility

00:19:45,700 --> 00:19:50,740
functions as well great documentation

00:19:48,670 --> 00:19:52,780
and getting better there's a great

00:19:50,740 --> 00:19:58,090
community as well and it's growing as

00:19:52,780 --> 00:20:01,059
well so where's the where's the psychic

00:19:58,090 --> 00:20:03,790
learn library going in in the future we

00:20:01,059 --> 00:20:05,380
recently performs a survey put in survey

00:20:03,790 --> 00:20:09,760
on our homepage and asked people to

00:20:05,380 --> 00:20:11,920
respond fifty-seven percent of psychic

00:20:09,760 --> 00:20:14,980
learn users that completed the survey

00:20:11,920 --> 00:20:17,530
our academic and thirty-six percent

00:20:14,980 --> 00:20:18,880
where industry and what people want to

00:20:17,530 --> 00:20:20,830
see in your networks and deep learning

00:20:18,880 --> 00:20:22,809
which is not really surprising this is

00:20:20,830 --> 00:20:24,580
sort of where the fields going people

00:20:22,809 --> 00:20:27,520
who are familiar with machine learning

00:20:24,580 --> 00:20:28,900
say the 80s would have seen your

00:20:27,520 --> 00:20:30,309
networks back down and they didn't do

00:20:28,900 --> 00:20:31,900
very well but there's been some really

00:20:30,309 --> 00:20:34,780
big breakthroughs in the field in the

00:20:31,900 --> 00:20:37,210
last few years and your networks are now

00:20:34,780 --> 00:20:39,420
we're all the you know latest

00:20:37,210 --> 00:20:42,970
cutting-edge results are coming from

00:20:39,420 --> 00:20:45,520
structure prediction python3 support now

00:20:42,970 --> 00:20:48,550
we have Python 3 support but it needs

00:20:45,520 --> 00:20:52,630
work so if you really good with Python 3

00:20:48,550 --> 00:20:55,090
let us know and sparse matrix coverage

00:20:52,630 --> 00:20:57,360
increased we have many algorithms that

00:20:55,090 --> 00:21:03,610
support sparse matrices from the sci-fi

00:20:57,360 --> 00:21:04,900
library but not all of them do now I'll

00:21:03,610 --> 00:21:07,150
skip through this but if you're

00:21:04,900 --> 00:21:10,170
interested in contributing jump on the

00:21:07,150 --> 00:21:13,390
mail in their store let me know or both

00:21:10,170 --> 00:21:16,140
there's a sprint and if you're

00:21:13,390 --> 00:21:18,370
interested in helping fund that just

00:21:16,140 --> 00:21:21,970
again let me know and I'll pointing in

00:21:18,370 --> 00:21:23,770
the direction of funding so now more

00:21:21,970 --> 00:21:25,600
hand wave inside but now get to the

00:21:23,770 --> 00:21:27,510
cybercrime attribution part of my talk

00:21:25,600 --> 00:21:30,900
and I don't have much time

00:21:27,510 --> 00:21:33,060
so basically my main issue that I'm

00:21:30,900 --> 00:21:34,620
trying to resolve in my research is we

00:21:33,060 --> 00:21:36,750
can't directly determine who is

00:21:34,620 --> 00:21:39,630
performing a cyberattack what people do

00:21:36,750 --> 00:21:42,540
is they set up behind proxies or use tor

00:21:39,630 --> 00:21:44,100
or or you know all a bunch of other

00:21:42,540 --> 00:21:46,350
technologies are very very good at

00:21:44,100 --> 00:21:47,730
preserving this so what we want to do is

00:21:46,350 --> 00:21:52,140
we want to find other ways to link

00:21:47,730 --> 00:21:53,490
attacks to the people and I apologize I

00:21:52,140 --> 00:21:56,070
don't have too much time to go into

00:21:53,490 --> 00:21:59,190
details what we basically do is we

00:21:56,070 --> 00:22:01,560
extract text features from documents we

00:21:59,190 --> 00:22:03,600
cluster them we do this with a bunch of

00:22:01,560 --> 00:22:05,850
different parameters and then we get

00:22:03,600 --> 00:22:08,430
classes at where we can say that all

00:22:05,850 --> 00:22:12,810
attacks within the same plaster from the

00:22:08,430 --> 00:22:15,630
same person this is a diagram of one of

00:22:12,810 --> 00:22:17,070
my latest algorithms and what this

00:22:15,630 --> 00:22:20,490
allows us to do when we apply to

00:22:17,070 --> 00:22:23,310
phishing websites this sort of attack

00:22:20,490 --> 00:22:25,740
pattern by itself is very random but

00:22:23,310 --> 00:22:27,270
once we extracted into groups were able

00:22:25,740 --> 00:22:29,400
to do a lot more prediction around what

00:22:27,270 --> 00:22:33,990
those groups are those that know their

00:22:29,400 --> 00:22:37,080
fishing history would know that in

00:22:33,990 --> 00:22:40,650
October 2009 a particular group called

00:22:37,080 --> 00:22:43,280
rockfish had a had a large chunk of

00:22:40,650 --> 00:22:45,840
their infrastructure taken down now

00:22:43,280 --> 00:22:47,640
independent correlation shows that the

00:22:45,840 --> 00:22:51,300
light blue group is rockfish which are

00:22:47,640 --> 00:22:54,330
very prolific in 2008-2009 and then

00:22:51,300 --> 00:22:56,700
oktober 2009 you can see that pretty

00:22:54,330 --> 00:22:59,460
much died you know and this correlates

00:22:56,700 --> 00:23:01,140
with what we know externally so we're

00:22:59,460 --> 00:23:02,880
able to hit here we're able to track

00:23:01,140 --> 00:23:05,280
different fishing groups without knowing

00:23:02,880 --> 00:23:08,700
beforehand what those groups are and

00:23:05,280 --> 00:23:13,530
this gives us a lot of intelligence so

00:23:08,700 --> 00:23:14,940
that's that's my talk so yeah I sort of

00:23:13,530 --> 00:23:16,860
went over a lot of material three

00:23:14,940 --> 00:23:19,910
different areas scikit-learn machine

00:23:16,860 --> 00:23:22,260
learning and cybercrime attribution so

00:23:19,910 --> 00:23:25,380
if you have any questions on any of

00:23:22,260 --> 00:23:27,390
those just let me know thank you thanks

00:23:25,380 --> 00:23:30,170
Robert for a very we're talking thanks

00:23:27,390 --> 00:23:30,170
for your work on taking

00:23:34,210 --> 00:23:45,770
questions okay um if you could very

00:23:43,549 --> 00:23:47,360
quickly with that attribution of

00:23:45,770 --> 00:23:50,000
phishing websites what are some of the

00:23:47,360 --> 00:23:54,320
features that that you use for that cut

00:23:50,000 --> 00:23:56,360
for that clustering okay so for a topic

00:23:54,320 --> 00:23:58,070
clustering well we want to know you what

00:23:56,360 --> 00:23:59,840
the topic of a document is we extract

00:23:58,070 --> 00:24:02,600
the words get rid of some of the words

00:23:59,840 --> 00:24:04,190
we know I useful and you know if it's a

00:24:02,600 --> 00:24:06,830
sports document is going to have

00:24:04,190 --> 00:24:08,750
football or you know ball in a lot more

00:24:06,830 --> 00:24:11,539
than common usage and we put those ones

00:24:08,750 --> 00:24:14,960
together for authorship we look at the

00:24:11,539 --> 00:24:17,240
characters instead the frequency of

00:24:14,960 --> 00:24:19,700
character in grams or sub sequences of

00:24:17,240 --> 00:24:22,730
characters is different for each

00:24:19,700 --> 00:24:25,159
particular author and what we can do is

00:24:22,730 --> 00:24:28,190
we can extract those character sub

00:24:25,159 --> 00:24:30,440
sequences out model the overall usage

00:24:28,190 --> 00:24:32,659
and then use that to model their

00:24:30,440 --> 00:24:34,760
authorship style and then we compare

00:24:32,659 --> 00:24:40,970
these profiles of authorship to each

00:24:34,760 --> 00:24:43,909
other the the top well this sort of the

00:24:40,970 --> 00:24:45,640
blue green yellow green models are

00:24:43,909 --> 00:24:48,020
different types of authorship models

00:24:45,640 --> 00:24:52,370
here and then the rest of it's pretty

00:24:48,020 --> 00:24:53,899
much an ensemble in approach yes there's

00:24:52,370 --> 00:24:55,399
a bunch of different ways to actually do

00:24:53,899 --> 00:24:57,039
the modeling but character those

00:24:55,399 --> 00:25:00,039
character in grams are very effective

00:24:57,039 --> 00:25:00,039
yeah

00:25:04,840 --> 00:25:12,909
my question is about if you know some

00:25:09,169 --> 00:25:18,169
way of providing the models as a service

00:25:12,909 --> 00:25:21,740
using psyche in a way of of where you

00:25:18,169 --> 00:25:25,010
can online provide feedback to keep

00:25:21,740 --> 00:25:27,049
training the model and control day over

00:25:25,010 --> 00:25:30,110
fitting that you can have when you

00:25:27,049 --> 00:25:35,450
provide new data I sorry I didn't quite

00:25:30,110 --> 00:25:40,429
get that so basically once you train

00:25:35,450 --> 00:25:45,470
your model you can then provide a

00:25:40,429 --> 00:25:48,169
service right so you can have all and

00:25:45,470 --> 00:25:52,630
data coming that you can retrain your

00:25:48,169 --> 00:25:55,010
model one another time okay and that can

00:25:52,630 --> 00:25:58,909
provide you some other fitting that you

00:25:55,010 --> 00:26:01,970
should avoid to to keep the accuracy of

00:25:58,909 --> 00:26:04,370
the model pretty high and the question

00:26:01,970 --> 00:26:08,690
is if you know if inciting scikit-learn

00:26:04,370 --> 00:26:13,580
there is something to provide that model

00:26:08,690 --> 00:26:16,820
haces service where you can give this

00:26:13,580 --> 00:26:18,230
feedback of data and keep it okay so the

00:26:16,820 --> 00:26:20,299
problem you talked about there is called

00:26:18,230 --> 00:26:23,090
online learning where we have a model me

00:26:20,299 --> 00:26:24,980
updated with new information I'm not

00:26:23,090 --> 00:26:26,539
actually sure whether we have any in

00:26:24,980 --> 00:26:29,330
there at the moment but i have heard

00:26:26,539 --> 00:26:31,370
talk about on a mailing list i'm more of

00:26:29,330 --> 00:26:34,460
an unsupervised clustering guys i'll

00:26:31,370 --> 00:26:37,299
focus on that but i believe it is

00:26:34,460 --> 00:26:42,289
something it's being looked into yeah

00:26:37,299 --> 00:26:45,590
question yeah it's not really a pipe in

00:26:42,289 --> 00:26:48,289
question but for the example you gave

00:26:45,590 --> 00:26:52,010
with fishing how do you how do you pick

00:26:48,289 --> 00:26:55,419
your number of clusters yep this

00:26:52,010 --> 00:26:58,100
algorithm here funny one that knows

00:26:55,419 --> 00:26:59,990
cluster analysis is going to sort of

00:26:58,100 --> 00:27:01,760
scoffer what I'm about to say but this

00:26:59,990 --> 00:27:04,909
algorithm here is an automated method

00:27:01,760 --> 00:27:07,110
for treating the number of clusters with

00:27:04,909 --> 00:27:09,570
any unsupervised approach

00:27:07,110 --> 00:27:11,850
there's always some interpretation in

00:27:09,570 --> 00:27:16,309
the results and it's very difficult to

00:27:11,850 --> 00:27:18,750
get rid of that what I've done here is

00:27:16,309 --> 00:27:21,240
from this top step we get a whole bunch

00:27:18,750 --> 00:27:23,790
of different classrooms and basically we

00:27:21,240 --> 00:27:27,240
use a framework called AAC that are

00:27:23,790 --> 00:27:29,490
remaps those and what this what the

00:27:27,240 --> 00:27:30,929
purpose of this is is we take classes

00:27:29,490 --> 00:27:35,460
which may not be that separated

00:27:30,929 --> 00:27:37,200
initially we we do EAC which makes those

00:27:35,460 --> 00:27:38,820
differences a lot bigger and makes it

00:27:37,200 --> 00:27:40,530
easier for us to automatically select

00:27:38,820 --> 00:27:44,790
the middle point to separate two things

00:27:40,530 --> 00:27:46,950
out it's not perfect I think this is

00:27:44,790 --> 00:27:52,140
about you know him waving seventy

00:27:46,950 --> 00:27:53,790
seventy percent accurate against our but

00:27:52,140 --> 00:27:57,210
I mean I found that this works quite

00:27:53,790 --> 00:28:00,419
well and we for the fishing data set we

00:27:57,210 --> 00:28:04,620
had about 99% purity with expert and

00:28:00,419 --> 00:28:06,120
Alison Alice's of the same data set we

00:28:04,620 --> 00:28:09,390
have time for I think one more question

00:28:06,120 --> 00:28:15,270
Nathan the good news is you just

00:28:09,390 --> 00:28:17,730
answered my question but quickly so

00:28:15,270 --> 00:28:20,160
using k-means right could you top yeah

00:28:17,730 --> 00:28:23,130
could you have used another clustering

00:28:20,160 --> 00:28:24,660
algorithm that had some magic about

00:28:23,130 --> 00:28:28,169
picking the clusters for you like I know

00:28:24,660 --> 00:28:30,059
there's some some clustering algorithms

00:28:28,169 --> 00:28:33,990
that you showed before that you could

00:28:30,059 --> 00:28:37,309
just use without specifying k yep so the

00:28:33,990 --> 00:28:41,880
k-means used here is done with random k

00:28:37,309 --> 00:28:44,100
values and it's that random k value is

00:28:41,880 --> 00:28:46,140
the first part into the ACA with and

00:28:44,100 --> 00:28:48,980
then AAC does the magic on those

00:28:46,140 --> 00:28:51,929
different clusterings and combines them

00:28:48,980 --> 00:28:54,360
so I'm happy to talk to you offline in

00:28:51,929 --> 00:28:56,910
more detail but but basically at the top

00:28:54,360 --> 00:29:02,429
it's random how many clusters and then

00:28:56,910 --> 00:29:04,500
at the bottom hopefully it's not yeah so

00:29:02,429 --> 00:29:06,919
thanks again Robert thank you we'd like

00:29:04,500 --> 00:29:06,919
to run

00:29:09,519 --> 00:29:19,389
you like to offer you a bag of coffee

00:29:13,700 --> 00:29:19,389
and a mug for scientists and Irma take a

00:29:20,559 --> 00:29:23,559

YouTube URL: https://www.youtube.com/watch?v=onYs20ykbBY


