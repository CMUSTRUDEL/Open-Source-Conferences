Title: Cassandra Serving Netflix @ Scale - Vinay Chella, Netflix
Publication date: 2017-05-17
Playlist: ApacheCon 2017 - Miami
Description: 
	Cassandra Serving Netflix @ Scale - Vinay Chella, Netflix

Cassandra is the main data storage engine supporting thousands of microservices and more than 90+ million subscribers at Netflix. It serves as a solid foundation for Netflixâ€™s global replication bringing internet TV network to customers around the world. With 250+ Clusters, 10,000+ Nodes and 3+ PB of data deployment, Cassandra is serving Netflix with several Millions of operations/sec with multiple nines of availability.

In this talk, Vinay Chella and Roopa Tangirala will shed light on Netflix's Cassandra deployment architecture, the services and tools that power Netflix's persistence layer in giving seamless movie experience to its customers. The talk will provide insights and proven path on how to run, certify, benchmark, monitor, and test large scale deployments of open source distributed backends.

About Vinay Chella
Vinay Chella is a Cloud Data Architect @ Netflix and Apache Cassandra MVP. He possesses a great understanding of Cassandra (C*), distributed systems and relational databases. As an Engineer and Architect, he has extensively worked on building distributed systems, highly efficient data access layers and performance tuning of C*. Vinay Chella has assisted several teams in successfully building next generation data access layers @ Netflix Roopa Tangirala is an experienced engineering leader and Apache Cassandra MVP with an extensive background in databases, be they distributed or relational. She manages the Cloud Database Engineering team at Netflix responsible for operating cloud persistent and semi-persistent runtime stores for Netflix, which includes Cassandra, Elasticsearch, Dynomite and MySQL by ensuring data availability, durability, and scalability.
Captions: 
	00:00:00,030 --> 00:00:02,659
in a queue

00:00:06,160 --> 00:00:16,290
okay there's about as you can okay

00:00:10,590 --> 00:00:16,290
welcome to a body con not

00:00:17,330 --> 00:00:23,560
and

00:00:19,190 --> 00:00:23,560
metrics google analytics here

00:00:24,230 --> 00:00:29,930
the search all that we create on one day

00:00:28,580 --> 00:00:37,540
other than the clicks of Sabre may be

00:00:29,930 --> 00:00:37,540
changing but you're not a customer

00:00:40,030 --> 00:00:47,140
getting to our topic

00:00:42,790 --> 00:00:51,980
syncope one of the oceans shows police

00:00:47,140 --> 00:00:55,309
last year I with what I've heard about

00:00:51,980 --> 00:00:58,430
it so on a public official with my

00:00:55,309 --> 00:01:03,470
colleague and then I started watching it

00:00:58,430 --> 00:01:07,160
on a late evening on a lay three days on

00:01:03,470 --> 00:01:09,200
a couch on my advance of course I think

00:01:07,160 --> 00:01:11,030
starts this soap and invention of

00:01:09,200 --> 00:01:14,690
printing it overnight or completing you

00:01:11,030 --> 00:01:18,680
can make say it is started casually but

00:01:14,690 --> 00:01:21,380
all the cleaning fans out there you are

00:01:18,680 --> 00:01:23,320
not up that would be to stop this

00:01:21,380 --> 00:01:27,270
in the middle

00:01:23,320 --> 00:01:27,270
you know without speaking it networking

00:01:31,189 --> 00:01:38,740
so

00:01:33,020 --> 00:01:44,030
I completed about the episode my iPad

00:01:38,740 --> 00:01:49,670
granted I then had to improvise to you

00:01:44,030 --> 00:01:51,920
and continue watching on TV interesting

00:01:49,670 --> 00:01:55,009
thing there then I stopped when my

00:01:51,920 --> 00:01:57,679
battery died animated in the mood to TV

00:01:55,009 --> 00:02:00,590
and when I opened the same thing I did

00:01:57,679 --> 00:02:03,200
spend any time you know providing going

00:02:00,590 --> 00:02:06,799
forward again okay let s talk just

00:02:03,200 --> 00:02:09,940
looking very most I think a couple of

00:02:06,799 --> 00:02:12,870
awesome desire

00:02:09,940 --> 00:02:12,870
there's nothing

00:02:13,060 --> 00:02:19,780
wanted to get some fun and then it is

00:02:17,200 --> 00:02:22,000
then the grateful

00:02:19,780 --> 00:02:23,860
well the interesting thing is I couldn't

00:02:22,000 --> 00:02:24,970
stop watching it though I started

00:02:23,860 --> 00:02:27,709
walking tonight

00:02:24,970 --> 00:02:37,159
on the traffic

00:02:27,709 --> 00:02:40,989
the same thing I didn't spend any time

00:02:37,159 --> 00:02:44,690
figuring out basically I just opened my

00:02:40,989 --> 00:02:49,540
metrics app on an iPhone and open those

00:02:44,690 --> 00:02:49,540
anything it just became real earnest all

00:02:49,680 --> 00:02:56,580
while what makes it possible what makes

00:02:52,799 --> 00:03:00,690
seamless movie opinion

00:02:56,580 --> 00:03:02,850
wasn't something that city advisor and

00:03:00,690 --> 00:03:04,710
that's gone and they work with our own

00:03:02,850 --> 00:03:07,860
fabric so this is nonsense great

00:03:04,710 --> 00:03:10,260
research that perfect has which

00:03:07,860 --> 00:03:12,750
basically got Moulton what we felt we

00:03:10,260 --> 00:03:14,879
are clear the background and between

00:03:12,750 --> 00:03:19,439
credit

00:03:14,879 --> 00:03:23,639
and what makes it possible to get like

00:03:19,439 --> 00:03:27,870
photography what do now is like for

00:03:23,639 --> 00:03:29,969
Nexus computation is basically cost all

00:03:27,870 --> 00:03:33,599
your industry and your dating history

00:03:29,969 --> 00:03:38,879
figures out which shows which movies you

00:03:33,599 --> 00:03:42,689
might like that and also three sort of

00:03:38,879 --> 00:03:45,510
customer information about what emboss

00:03:42,689 --> 00:03:49,200
what he rated waiting for the way follow

00:03:45,510 --> 00:03:54,510
me bound was and how many circulates

00:03:49,200 --> 00:03:57,109
everything and most of this comes all of

00:03:54,510 --> 00:04:01,379
the features that that message has

00:03:57,109 --> 00:04:04,379
directly are invited me depends on

00:04:01,379 --> 00:04:07,109
amazing precision story that is

00:04:04,379 --> 00:04:09,030
Cassandra because that's what our system

00:04:07,109 --> 00:04:11,370
today we'll talk about Cassandra that

00:04:09,030 --> 00:04:13,290
means that filters case because for the

00:04:11,370 --> 00:04:16,259
use that we could use my toes even each

00:04:13,290 --> 00:04:18,870
other Cassandra and a 12-acre architect

00:04:16,259 --> 00:04:23,849
watch out for cell database in reading

00:04:18,870 --> 00:04:25,740
phenotypic cd18 because internally we

00:04:23,849 --> 00:04:29,009
are marching is responsible for

00:04:25,740 --> 00:04:31,860
providing services position sold and

00:04:29,009 --> 00:04:36,120
service to the stuff it's an application

00:04:31,860 --> 00:04:39,330
a part of the Sunapee provides several

00:04:36,120 --> 00:04:43,080
other services as several other states

00:04:39,330 --> 00:04:46,229
so that services which includes that's

00:04:43,080 --> 00:04:50,270
huge sign might an audience Mexico

00:04:46,229 --> 00:04:55,030
zookeeper in all the planes Larry Tooley

00:04:50,270 --> 00:04:55,030
which suppose any of you

00:04:55,770 --> 00:05:01,860
we'll talk about the specifically Apache

00:04:58,350 --> 00:05:04,530
Cassandra head metrics and what all the

00:05:01,860 --> 00:05:08,090
challenges we face in Floyd in Savannah

00:05:04,530 --> 00:05:11,310
as a service to the application fee and

00:05:08,090 --> 00:05:15,180
hope how do we satisfy and vex market

00:05:11,310 --> 00:05:17,990
and so we make introductions any we have

00:05:15,180 --> 00:05:17,990
large trucks

00:05:18,560 --> 00:05:24,020
getting into the details of Cassandra

00:05:20,470 --> 00:05:26,900
and metrics almost kept chipping in so

00:05:24,020 --> 00:05:28,100
Dean Cassandra awesome we accept the

00:05:26,900 --> 00:05:31,060
moon is everything

00:05:28,100 --> 00:05:34,550
metadata about the customer information

00:05:31,060 --> 00:05:39,380
seamlessly trading billing and payment

00:05:34,550 --> 00:05:41,300
anything is being registered in the

00:05:39,380 --> 00:05:44,240
stuffer kingdom we have a hundreds of

00:05:41,300 --> 00:05:46,790
cluster they tense up those in some food

00:05:44,240 --> 00:05:50,210
making some of the specter bytes of data

00:05:46,790 --> 00:05:54,040
and setting the limits of possessions

00:05:50,210 --> 00:05:54,040
but they just sort of examples

00:05:54,180 --> 00:06:00,060
so on a high level concatenates or

00:05:57,240 --> 00:06:03,240
challenges that we face across some of

00:06:00,060 --> 00:06:08,660
the we're dividing Pasadena as a certain

00:06:03,240 --> 00:06:11,639
ethnicity monitoring maintenance

00:06:08,660 --> 00:06:14,129
benchmarking ambassador

00:06:11,639 --> 00:06:16,370
and basically products living to be the

00:06:14,129 --> 00:06:16,370
third

00:06:16,419 --> 00:06:22,210
so in today's talk I'll get into the

00:06:19,240 --> 00:06:26,409
specifics of these challenges and how we

00:06:22,210 --> 00:06:28,719
solve those how do you came in town how

00:06:26,409 --> 00:06:30,340
we build system and what do some

00:06:28,719 --> 00:06:32,530
treatment so get a doubt these

00:06:30,340 --> 00:06:35,650
challenges

00:06:32,530 --> 00:06:40,110
collect our first rattle quality or

00:06:35,650 --> 00:06:42,569
talented marketing gentleness

00:06:40,110 --> 00:06:46,050
generally talking about positions you're

00:06:42,569 --> 00:06:49,379
not recipients and that is getting new

00:06:46,050 --> 00:06:51,569
details of this manner here so before

00:06:49,379 --> 00:06:55,289
getting to that they're seeing or

00:06:51,569 --> 00:06:58,139
emotional we monitored latency we

00:06:55,289 --> 00:07:01,530
monitor latencies in terms of read

00:06:58,139 --> 00:07:04,860
Layton sees right Layton sees 99th and

00:07:01,530 --> 00:07:08,460
95th we don't monitor average Layton

00:07:04,860 --> 00:07:11,310
sees and when we monitor 99th and 95th

00:07:08,460 --> 00:07:13,620
coordinator Layton sees one of the

00:07:11,310 --> 00:07:17,699
critical thing is not every Cassandra

00:07:13,620 --> 00:07:20,250
cluster gives you the same amount of you

00:07:17,699 --> 00:07:22,770
know performance because in terms of

00:07:20,250 --> 00:07:25,440
Cassandra it majorly depends on what

00:07:22,770 --> 00:07:28,229
kind of data you are storing how you are

00:07:25,440 --> 00:07:30,569
accessing and your usage pattern and

00:07:28,229 --> 00:07:32,610
access pattern data model and several

00:07:30,569 --> 00:07:35,490
other things so key important things

00:07:32,610 --> 00:07:37,919
here is we do not blindly come up with a

00:07:35,490 --> 00:07:41,759
number and enforce that number on every

00:07:37,919 --> 00:07:45,060
cluster so we have slh defined based on

00:07:41,759 --> 00:07:47,580
cluster configurations and all all of

00:07:45,060 --> 00:07:51,270
our tooling and monitoring goes off of

00:07:47,580 --> 00:07:54,449
that and we monitor health of the

00:07:51,270 --> 00:07:57,719
Cassandra which includes gossip status

00:07:54,449 --> 00:08:01,080
notes per perspective about all of other

00:07:57,719 --> 00:08:03,990
nodes and client protocols that are the

00:08:01,080 --> 00:08:06,090
mean you know thrift and binary and also

00:08:03,990 --> 00:08:08,449
any networking issues on the machine on

00:08:06,090 --> 00:08:11,250
any hardware issues on the machine and

00:08:08,449 --> 00:08:14,969
since Cassandra is the Java based system

00:08:11,250 --> 00:08:18,870
we also monitor JVM and sheep of the

00:08:14,969 --> 00:08:21,870
Cassandra and apart from that we also

00:08:18,870 --> 00:08:25,020
look at what these and maintenances that

00:08:21,870 --> 00:08:27,840
have been running on Cassandra maybe

00:08:25,020 --> 00:08:30,409
it's a user initiated maintenance or the

00:08:27,840 --> 00:08:32,610
system initiated maintenances and

00:08:30,409 --> 00:08:35,700
another critical thing that we do

00:08:32,610 --> 00:08:39,839
monitor is the wide rows with mainly

00:08:35,700 --> 00:08:41,880
with the Cassandra how long how large

00:08:39,839 --> 00:08:45,480
your partition actually decides your

00:08:41,880 --> 00:08:48,500
cluster behavior so we do keep a look at

00:08:45,480 --> 00:08:52,440
wide row and wide partition matrix and

00:08:48,500 --> 00:08:53,130
we also look at a lot of system warnings

00:08:52,440 --> 00:08:54,750
and in

00:08:53,130 --> 00:08:57,000
information messages and errors are

00:08:54,750 --> 00:09:00,779
being logged in Cassandra system logs so

00:08:57,000 --> 00:09:03,480
we do monitor logs as well so the first

00:09:00,779 --> 00:09:06,240
question that comes to anyone minds

00:09:03,480 --> 00:09:09,300
anyone's mind when you have tens of

00:09:06,240 --> 00:09:10,920
thousands of instances is - how do we

00:09:09,300 --> 00:09:14,310
even monitor those thousands of

00:09:10,920 --> 00:09:17,250
instances and the common approach would

00:09:14,310 --> 00:09:20,190
be a cron based system or a job runner

00:09:17,250 --> 00:09:22,670
which wakes up on a schedule and reaches

00:09:20,190 --> 00:09:25,680
out to your thousands of instances and

00:09:22,670 --> 00:09:28,199
comes up with a something called state

00:09:25,680 --> 00:09:31,319
of the system or the health check health

00:09:28,199 --> 00:09:34,649
state at any given point in time let us

00:09:31,319 --> 00:09:36,389
call it a t0 snapshot the entire

00:09:34,649 --> 00:09:40,350
Cassandra cluster of thousand nodes

00:09:36,389 --> 00:09:42,839
looks healthy and then since it's a cron

00:09:40,350 --> 00:09:45,600
based system you would deploy something

00:09:42,839 --> 00:09:48,420
like this you have I took an example of

00:09:45,600 --> 00:09:51,300
Jenkins you have a Jenkins slave or

00:09:48,420 --> 00:09:54,089
Jenkins master that is being deployed in

00:09:51,300 --> 00:09:56,490
one data center and you have several

00:09:54,089 --> 00:10:00,209
data centers or several regions where

00:09:56,490 --> 00:10:02,639
your actual database or data systems are

00:10:00,209 --> 00:10:04,860
being deployed so your junking system

00:10:02,639 --> 00:10:07,079
reaches out to all of your Cassandra

00:10:04,860 --> 00:10:11,040
instances and figures out health of the

00:10:07,079 --> 00:10:14,120
Sandra but the typical problems with the

00:10:11,040 --> 00:10:17,970
common approach and common architecture

00:10:14,120 --> 00:10:20,189
would be since they are crown based

00:10:17,970 --> 00:10:24,750
systems or job Dennis they do not

00:10:20,189 --> 00:10:26,490
persist any state but thus the database

00:10:24,750 --> 00:10:28,199
or the system that we are dealing with

00:10:26,490 --> 00:10:31,889
is not a stateless system it's a

00:10:28,199 --> 00:10:34,290
stateful system which has an importance

00:10:31,889 --> 00:10:36,870
for the state and the transition that

00:10:34,290 --> 00:10:39,930
every cluster goes through and being in

00:10:36,870 --> 00:10:42,269
a cloud knitter system all of you many

00:10:39,930 --> 00:10:45,449
of your nodes experience network

00:10:42,269 --> 00:10:48,360
switches are you know bad Hardware bad

00:10:45,449 --> 00:10:50,880
network or maybe a transient issue or

00:10:48,360 --> 00:10:54,240
maybe a critical issue but you do need

00:10:50,880 --> 00:10:56,910
to understand the state of a system to

00:10:54,240 --> 00:11:00,870
assess the health of a system so let me

00:10:56,910 --> 00:11:04,260
take a common scenario and explain the

00:11:00,870 --> 00:11:05,890
problems that are involved when building

00:11:04,260 --> 00:11:08,050
something

00:11:05,890 --> 00:11:12,220
you know crown based health check system

00:11:08,050 --> 00:11:14,380
so let's say at the t0 your crown based

00:11:12,220 --> 00:11:16,470
system wakes up and reaches out to

00:11:14,380 --> 00:11:20,800
thousands of instances and figures out

00:11:16,470 --> 00:11:23,020
the cluster is healthy and then again it

00:11:20,800 --> 00:11:25,930
wakes up at t1 and tries to figure out

00:11:23,020 --> 00:11:28,420
if the cluster is healthy or not but at

00:11:25,930 --> 00:11:30,730
p1 when is it when it is reaching out to

00:11:28,420 --> 00:11:33,730
thousands of instances maybe couple of

00:11:30,730 --> 00:11:37,600
instances might be experiencing the

00:11:33,730 --> 00:11:39,640
transient network glitch or maybe some

00:11:37,600 --> 00:11:43,930
you know system is under pressure of

00:11:39,640 --> 00:11:46,840
whatever then instead of giving up you

00:11:43,930 --> 00:11:49,420
just sleep for maybe you know 10 seconds

00:11:46,840 --> 00:11:51,190
and retie again and when you are

00:11:49,420 --> 00:11:53,560
reaching this time maybe some other

00:11:51,190 --> 00:11:56,800
instances are experiencing the network

00:11:53,560 --> 00:11:57,820
pick up then you sleep again so that

00:11:56,800 --> 00:12:00,610
will that would be a never-ending

00:11:57,820 --> 00:12:03,190
problem then you come up with some you

00:12:00,610 --> 00:12:04,960
know hacky algorithm to figure out what

00:12:03,190 --> 00:12:07,650
nodes are actually having an issue and

00:12:04,960 --> 00:12:10,480
what nodes are experiencing variation

00:12:07,650 --> 00:12:13,690
but these are just workarounds on top of

00:12:10,480 --> 00:12:15,760
workarounds and another issue with the

00:12:13,690 --> 00:12:18,010
crown based systems is let's say your

00:12:15,760 --> 00:12:19,930
system is under pressure and you are

00:12:18,010 --> 00:12:23,650
trying to establish a connection to find

00:12:19,930 --> 00:12:25,600
out the health of a system but you

00:12:23,650 --> 00:12:27,730
cannot establish a connection to figure

00:12:25,600 --> 00:12:29,230
out health of a node because system is

00:12:27,730 --> 00:12:31,330
already in the pressure pressure and

00:12:29,230 --> 00:12:35,410
your health check system fails when you

00:12:31,330 --> 00:12:37,660
need it most and also since these cron

00:12:35,410 --> 00:12:40,660
based systems would not have any state

00:12:37,660 --> 00:12:42,310
of the cluster of what is what has

00:12:40,660 --> 00:12:45,330
happened and what is going to happen in

00:12:42,310 --> 00:12:48,400
the future what is going on right now

00:12:45,330 --> 00:12:51,700
they would be they would not be

00:12:48,400 --> 00:12:53,800
resilient for any temporary network

00:12:51,700 --> 00:12:58,510
breaches or any temporary jvm pressure

00:12:53,800 --> 00:13:01,330
and stuff like that and these systems

00:12:58,510 --> 00:13:03,430
would not scale when you are going from

00:13:01,330 --> 00:13:05,460
thousand nodes to 10,000 nodes to

00:13:03,430 --> 00:13:07,660
hundred thousand nodes and we we

00:13:05,460 --> 00:13:10,540
initially had a system something like

00:13:07,660 --> 00:13:13,420
this and which did not serve our needs

00:13:10,540 --> 00:13:16,300
as we were scaling out and that is when

00:13:13,420 --> 00:13:18,490
we took a step back and thought about

00:13:16,300 --> 00:13:21,340
the problem in a different way

00:13:18,490 --> 00:13:25,570
what if we had a fine-grained snapshot

00:13:21,340 --> 00:13:27,580
of health of every instance that is

00:13:25,570 --> 00:13:30,040
being pushed out on a persistent

00:13:27,580 --> 00:13:32,830
connection instead of establishing the

00:13:30,040 --> 00:13:36,880
connection and figuring out the state at

00:13:32,830 --> 00:13:39,340
any given snapshot so simply instead of

00:13:36,880 --> 00:13:41,400
a pull based approach something like a

00:13:39,340 --> 00:13:43,510
push based approach where all of your

00:13:41,400 --> 00:13:46,450
instances thousands and thousands of

00:13:43,510 --> 00:13:47,800
instances would send heartbeats to a

00:13:46,450 --> 00:13:50,950
centralized other distributed

00:13:47,800 --> 00:13:55,180
distributed system which figures out the

00:13:50,950 --> 00:13:56,500
health of your overall cluster so that

00:13:55,180 --> 00:13:58,770
is when we thought about something like

00:13:56,500 --> 00:14:01,650
a streaming system we looked around and

00:13:58,770 --> 00:14:05,320
we found out that there is already a

00:14:01,650 --> 00:14:08,050
system built in-house which serves

00:14:05,320 --> 00:14:12,430
exactly same needs that we were looking

00:14:08,050 --> 00:14:15,760
for which is high throughput low latency

00:14:12,430 --> 00:14:18,370
operational shaming system that is

00:14:15,760 --> 00:14:21,700
called mantis which is again a open

00:14:18,370 --> 00:14:23,440
source offering I think it is you have

00:14:21,700 --> 00:14:26,050
to be open sourced in next month in this

00:14:23,440 --> 00:14:28,630
quarter but mantis streaming system is

00:14:26,050 --> 00:14:31,360
basically built on top of apache

00:14:28,630 --> 00:14:35,590
missiles provides a flexible programming

00:14:31,360 --> 00:14:39,520
model energy ActiveX and also it models

00:14:35,590 --> 00:14:42,640
your computations as it bags and which

00:14:39,520 --> 00:14:48,100
is basically designed for operational

00:14:42,640 --> 00:14:50,230
insights so by rethinking the

00:14:48,100 --> 00:14:52,810
traditional ground-based health check

00:14:50,230 --> 00:14:55,840
system in a streaming based system this

00:14:52,810 --> 00:14:57,670
is how the topology the architecture of

00:14:55,840 --> 00:14:59,230
a health check system looks like you

00:14:57,670 --> 00:15:01,570
have thousands and thousands of

00:14:59,230 --> 00:15:04,180
instances sending heartbeats to

00:15:01,570 --> 00:15:06,760
something called a source job so job is

00:15:04,180 --> 00:15:10,030
a terminology that I took from mantis

00:15:06,760 --> 00:15:12,880
framework job in a mantis framework is

00:15:10,030 --> 00:15:13,630
not actually a you know a scheduled job

00:15:12,880 --> 00:15:16,990
or something like that

00:15:13,630 --> 00:15:19,330
it is a micro service which is built on

00:15:16,990 --> 00:15:22,720
top of the active programming model

00:15:19,330 --> 00:15:24,550
which which collects heartbeats from

00:15:22,720 --> 00:15:27,850
every instance that is out there and

00:15:24,550 --> 00:15:30,040
builds something called a message that

00:15:27,850 --> 00:15:32,410
is being sent to our own job that we

00:15:30,040 --> 00:15:35,080
wrote on top of mantis frameworks

00:15:32,410 --> 00:15:37,810
so we have called it a local being

00:15:35,080 --> 00:15:39,490
aggregated and the region and the reason

00:15:37,810 --> 00:15:40,390
we have three different local Xing

00:15:39,490 --> 00:15:42,550
aggregators here

00:15:40,390 --> 00:15:45,970
I took an example of three data centers

00:15:42,550 --> 00:15:49,330
and when you are sending you know hard

00:15:45,970 --> 00:15:51,760
beats every 20 seconds some thousands of

00:15:49,330 --> 00:15:55,240
instances and we were talking about

00:15:51,760 --> 00:15:57,160
almost 60 MB per second of data that is

00:15:55,240 --> 00:15:59,560
being transmitted just for the health

00:15:57,160 --> 00:16:02,410
check and transmitting that much amount

00:15:59,560 --> 00:16:05,110
of data cross region was expensive and

00:16:02,410 --> 00:16:08,530
was not reliable so that is when we

00:16:05,110 --> 00:16:11,320
built isolated a local thing aggregator

00:16:08,530 --> 00:16:14,020
for every region the main purpose of

00:16:11,320 --> 00:16:17,110
local Xing aggregator is to collect all

00:16:14,020 --> 00:16:18,970
the messages that are being sent by all

00:16:17,110 --> 00:16:21,370
the modes and come up with something

00:16:18,970 --> 00:16:25,780
called a score which is a mathematical

00:16:21,370 --> 00:16:28,660
formula derived from every nodes

00:16:25,780 --> 00:16:31,590
perspective let's say you have 360 node

00:16:28,660 --> 00:16:34,540
cluster in casandra's point of view

00:16:31,590 --> 00:16:36,280
cluster when you call a cluster is

00:16:34,540 --> 00:16:38,950
healthy not when every node is healthy

00:16:36,280 --> 00:16:41,470
but every node identifies and sees

00:16:38,950 --> 00:16:43,420
communicates with everything every other

00:16:41,470 --> 00:16:45,400
nodes in the cluster so if you are

00:16:43,420 --> 00:16:48,280
talking about a 300 node cluster you are

00:16:45,400 --> 00:16:50,950
talking about 300 times 300 perspectives

00:16:48,280 --> 00:16:53,050
that are being generated from every node

00:16:50,950 --> 00:16:54,930
so that is why there was a humongous

00:16:53,050 --> 00:16:59,860
amount of data that is being generated

00:16:54,930 --> 00:17:03,310
from every node from every region so we

00:16:59,860 --> 00:17:05,440
built a local ting aggregate width which

00:17:03,310 --> 00:17:07,210
has a perspective which has the gossip

00:17:05,440 --> 00:17:10,750
status which has the thrift traitors all

00:17:07,210 --> 00:17:14,380
the Cline protocols and heap you know

00:17:10,750 --> 00:17:18,430
network issues and any hardware issues

00:17:14,380 --> 00:17:21,130
all these being put in something called

00:17:18,430 --> 00:17:25,540
a score which is much more minified

00:17:21,130 --> 00:17:28,030
version of your Cassandra instances

00:17:25,540 --> 00:17:31,180
health in the region and these messages

00:17:28,030 --> 00:17:33,910
are being these scores are being sent to

00:17:31,180 --> 00:17:36,160
something called centralized global link

00:17:33,910 --> 00:17:40,240
aggregator which collects the scores

00:17:36,160 --> 00:17:43,450
from every individual regions and come

00:17:40,240 --> 00:17:45,480
up builds evaluates a cluster health

00:17:43,450 --> 00:17:48,059
based on certain business rules

00:17:45,480 --> 00:17:50,840
based on whatever you call a cluster is

00:17:48,059 --> 00:17:53,820
healthy your definition of healthy and

00:17:50,840 --> 00:17:55,919
which runs a finite state machine the

00:17:53,820 --> 00:17:58,559
reason we need a finite state machine

00:17:55,919 --> 00:18:01,740
there is because being in a cloud

00:17:58,559 --> 00:18:04,860
ecosystem nodes die all the time when

00:18:01,740 --> 00:18:06,929
when a node gets terminated unlike a

00:18:04,860 --> 00:18:08,580
straight list machine where when a node

00:18:06,929 --> 00:18:10,950
gets terminated a new node comes up and

00:18:08,580 --> 00:18:12,840
it installs all the binaries that you

00:18:10,950 --> 00:18:15,330
wanted and starts serving the traffic

00:18:12,840 --> 00:18:18,240
that doesn't happen in distributed

00:18:15,330 --> 00:18:20,400
databases like a sensor you need that

00:18:18,240 --> 00:18:23,010
new node apart from installing binaries

00:18:20,400 --> 00:18:24,990
it needs to participate in the gossip it

00:18:23,010 --> 00:18:27,330
needs to join the Ring it means to get

00:18:24,990 --> 00:18:29,730
petabytes a terabytes of data that it is

00:18:27,330 --> 00:18:32,010
responsible for and we can slice the

00:18:29,730 --> 00:18:34,200
token range and get the data and this

00:18:32,010 --> 00:18:35,970
all could happen anyway from several

00:18:34,200 --> 00:18:40,590
hours to several days so we have

00:18:35,970 --> 00:18:42,240
instances where each node is almost

00:18:40,590 --> 00:18:43,830
carrying four terabyte of data and

00:18:42,240 --> 00:18:46,080
transmitting that foreteller byte of

00:18:43,830 --> 00:18:48,960
data from its peers would take anyways

00:18:46,080 --> 00:18:51,179
between 24 to 48 hours so for the 24 to

00:18:48,960 --> 00:18:53,669
48 hours your health check system should

00:18:51,179 --> 00:18:55,290
not alert you because that node is down

00:18:53,669 --> 00:18:58,650
it should detect that there was the

00:18:55,290 --> 00:19:01,799
termination in the cloud and this node

00:18:58,650 --> 00:19:04,470
is trying to join the ring and avoid any

00:19:01,799 --> 00:19:06,870
false positives so that is all beings

00:19:04,470 --> 00:19:09,360
all will be handled only when you know

00:19:06,870 --> 00:19:11,520
the state of a system instead of you

00:19:09,360 --> 00:19:13,919
know point in time from a point in times

00:19:11,520 --> 00:19:16,770
now snapshot based health check system

00:19:13,919 --> 00:19:18,299
so that is why this global aggregator

00:19:16,770 --> 00:19:21,510
has to have something called a finite

00:19:18,299 --> 00:19:24,240
state machine which has an aware which

00:19:21,510 --> 00:19:28,169
is aware of all the issues that is that

00:19:24,240 --> 00:19:30,299
are going in your cluster so global

00:19:28,169 --> 00:19:34,559
thing aggregate values cluster health

00:19:30,299 --> 00:19:36,630
and produces a stream of health check

00:19:34,559 --> 00:19:39,390
heartbeats so whether this cluster is

00:19:36,630 --> 00:19:44,250
healthy or not healthy every 10 every 10

00:19:39,390 --> 00:19:46,520
seconds it produces heartbeats out well

00:19:44,250 --> 00:19:49,410
we have a good health care system and

00:19:46,520 --> 00:19:52,230
you know that's great we have the entire

00:19:49,410 --> 00:19:56,010
the fleet of cell check encapsulated in

00:19:52,230 --> 00:19:57,510
single data stream that is being sent

00:19:56,010 --> 00:19:58,870
out signals whether the cluster is

00:19:57,510 --> 00:20:02,260
healthy or not

00:19:58,870 --> 00:20:04,930
but how do we you know visualize it hard

00:20:02,260 --> 00:20:07,810
how do we make sense out of it so that

00:20:04,930 --> 00:20:10,750
is when we built a system UI system

00:20:07,810 --> 00:20:14,020
which subscribes to the data stream that

00:20:10,750 --> 00:20:16,390
is being emitted by Glo building

00:20:14,020 --> 00:20:18,760
aggregator and here I am showing is a

00:20:16,390 --> 00:20:21,520
macro view where you have thousands of

00:20:18,760 --> 00:20:24,160
clusters hundreds of clusters being

00:20:21,520 --> 00:20:26,620
shown in a hallway dashboards with green

00:20:24,160 --> 00:20:27,880
red or you know yellow indicators

00:20:26,620 --> 00:20:31,600
whether the cluster is healthy or not

00:20:27,880 --> 00:20:31,990
and clicking on any of that rectangle

00:20:31,600 --> 00:20:34,360
box

00:20:31,990 --> 00:20:36,460
they're basically rectangle bomb the

00:20:34,360 --> 00:20:39,100
size of the box clip size and the size

00:20:36,460 --> 00:20:40,960
of the cluster and clicking on that boss

00:20:39,100 --> 00:20:44,140
brings you down to the cluster view

00:20:40,960 --> 00:20:47,290
where I am showing an example of 36

00:20:44,140 --> 00:20:49,140
nodes in one region this cluster is span

00:20:47,290 --> 00:20:51,430
across three different regions and

00:20:49,140 --> 00:20:53,650
everything is green which means the

00:20:51,430 --> 00:20:55,660
cluster is healthy and clicking on any

00:20:53,650 --> 00:20:58,900
of the things gives you the perspective

00:20:55,660 --> 00:21:00,520
view of the cluster again this is not a

00:20:58,900 --> 00:21:03,280
stateless machine this is a stateful

00:21:00,520 --> 00:21:05,830
machine you need to have a knowledge of

00:21:03,280 --> 00:21:08,650
from the entire gossip and perspective

00:21:05,830 --> 00:21:10,780
of every node every nodes perspective

00:21:08,650 --> 00:21:14,200
about every other node in the cluster so

00:21:10,780 --> 00:21:16,810
this this rectangle gives you the

00:21:14,200 --> 00:21:19,630
representation of any gossip issues and

00:21:16,810 --> 00:21:21,910
any network glitches that we had so

00:21:19,630 --> 00:21:24,480
before we had this to figure out if

00:21:21,910 --> 00:21:27,010
there was any gossip issue in the system

00:21:24,480 --> 00:21:29,110
we used to take anywhere between fifteen

00:21:27,010 --> 00:21:30,520
to thirty minutes depending on the

00:21:29,110 --> 00:21:32,290
cluster size if it is a three node

00:21:30,520 --> 00:21:34,390
cluster you log in to three different

00:21:32,290 --> 00:21:37,330
instances and figure out is every node

00:21:34,390 --> 00:21:39,250
seeing everyone else school but we had

00:21:37,330 --> 00:21:42,130
clusters with three hundred nodes and

00:21:39,250 --> 00:21:44,740
figuring out which node is missing the

00:21:42,130 --> 00:21:48,040
view of which other node was a nightmare

00:21:44,740 --> 00:21:51,370
so logging in so we had we had instances

00:21:48,040 --> 00:21:53,740
where are the you know on-call used to

00:21:51,370 --> 00:21:55,650
log into hundred different instances

00:21:53,740 --> 00:21:59,470
using all of his shell script and

00:21:55,650 --> 00:22:01,840
figuring out which node is experiencing

00:21:59,470 --> 00:22:04,920
the network issue talking to a different

00:22:01,840 --> 00:22:08,980
node but all of that was rolled down to

00:22:04,920 --> 00:22:11,700
less than m2 detect any of the gossip

00:22:08,980 --> 00:22:13,260
issue we became

00:22:11,700 --> 00:22:15,510
all the way from 30 minutes to a less

00:22:13,260 --> 00:22:18,120
than minute clicking on opening this

00:22:15,510 --> 00:22:20,700
screen would give you exact

00:22:18,120 --> 00:22:23,460
representation or exact in the Turkish

00:22:20,700 --> 00:22:25,320
which is happening between you know your

00:22:23,460 --> 00:22:29,370
data center between your tracks are

00:22:25,320 --> 00:22:31,740
between your individual nodes so what

00:22:29,370 --> 00:22:35,159
did we gain first I detect faster

00:22:31,740 --> 00:22:38,279
detection of issues and greater accuracy

00:22:35,159 --> 00:22:41,490
because it's not a you know pointing

00:22:38,279 --> 00:22:43,620
science now based health check your

00:22:41,490 --> 00:22:47,730
health check as being sent as a

00:22:43,620 --> 00:22:50,760
heartbeat every 10 or 20 seconds so you

00:22:47,730 --> 00:22:54,120
detect the issues faster and they are

00:22:50,760 --> 00:22:56,370
much more accurate and massive detection

00:22:54,120 --> 00:23:02,130
of false positives just to give you the

00:22:56,370 --> 00:23:04,830
number we had instances of 300 pages per

00:23:02,130 --> 00:23:07,559
day all the way from going down from 300

00:23:04,830 --> 00:23:10,980
pages per day to now we have less than

00:23:07,559 --> 00:23:13,889
100 pages per week so that was just

00:23:10,980 --> 00:23:16,919
because of mantis based health check

00:23:13,889 --> 00:23:19,620
system that we told out and also

00:23:16,919 --> 00:23:21,840
separation of concerns so when we had

00:23:19,620 --> 00:23:24,630
our health check health check system in

00:23:21,840 --> 00:23:26,370
the past you know detecting the issue

00:23:24,630 --> 00:23:28,769
and remediating the issues everything

00:23:26,370 --> 00:23:32,220
was everything was put in the same

00:23:28,769 --> 00:23:35,760
system which was overcomplicated and

00:23:32,220 --> 00:23:38,190
then with this swimming based system we

00:23:35,760 --> 00:23:40,889
you know we made it to different systems

00:23:38,190 --> 00:23:42,990
where the swimming based system would

00:23:40,889 --> 00:23:45,179
only detect the issues and then you have

00:23:42,990 --> 00:23:47,700
your remediation system which autos

00:23:45,179 --> 00:23:53,340
immediates any other issues that are

00:23:47,700 --> 00:23:57,779
possible so the next challenge the type

00:23:53,340 --> 00:24:00,600
of tackle is challenge maintenance so we

00:23:57,779 --> 00:24:02,429
talked about monitoring and what are the

00:24:00,600 --> 00:24:05,850
known problems with the maintenance when

00:24:02,429 --> 00:24:09,230
you are running Cassandra at scale so

00:24:05,850 --> 00:24:11,460
the first would be it's not a stateless

00:24:09,230 --> 00:24:15,210
system it's the stateful system so it

00:24:11,460 --> 00:24:17,760
needs to persist the state in terms of

00:24:15,210 --> 00:24:21,179
you know monitoring or in terms of

00:24:17,760 --> 00:24:23,190
maintenance and if you are the if you

00:24:21,179 --> 00:24:24,840
are operating in a cloud ecosystem your

00:24:23,190 --> 00:24:27,750
nodes become

00:24:24,840 --> 00:24:29,610
unresponsive for no reason because your

00:24:27,750 --> 00:24:32,070
hardware is not in your control you are

00:24:29,610 --> 00:24:34,860
being given by your Brent your cloud

00:24:32,070 --> 00:24:38,040
vendor and you do not have any idea of

00:24:34,860 --> 00:24:41,040
virtualization or physical machines that

00:24:38,040 --> 00:24:44,460
you are using and cloud being in cloud

00:24:41,040 --> 00:24:47,450
ecosystem comes up with its own cost

00:24:44,460 --> 00:24:49,500
shift some problems so and also

00:24:47,450 --> 00:24:52,380
configuration setup and tuning any

00:24:49,500 --> 00:24:54,810
considerations on you know twenty

00:24:52,380 --> 00:24:57,960
thousand instances would be nightmare

00:24:54,810 --> 00:24:59,550
with any shell script or any system with

00:24:57,960 --> 00:25:03,720
any scripting tools that you come up

00:24:59,550 --> 00:25:05,850
with and in Cassandra token distribution

00:25:03,720 --> 00:25:08,420
is key if you are not using V nodes

00:25:05,850 --> 00:25:11,580
which in our case we are not

00:25:08,420 --> 00:25:13,710
distributing the token equally among all

00:25:11,580 --> 00:25:16,050
of your data center is important

00:25:13,710 --> 00:25:19,050
otherwise you would create hot spots in

00:25:16,050 --> 00:25:22,890
your system which which affects your

00:25:19,050 --> 00:25:25,410
performance and latencies and also

00:25:22,890 --> 00:25:27,780
resilient for any other issues so when

00:25:25,410 --> 00:25:32,220
we had a three outages when we had any

00:25:27,780 --> 00:25:35,100
region outages as I said almost every

00:25:32,220 --> 00:25:37,800
feature directly or indirectly depends

00:25:35,100 --> 00:25:40,920
on Cassandra providing resiliency at a

00:25:37,800 --> 00:25:42,540
data store layer is very critical to

00:25:40,920 --> 00:25:45,120
keep Netflix up and running all the time

00:25:42,540 --> 00:25:48,570
so that is another problem with it is

00:25:45,120 --> 00:25:50,970
resiliency so to solve all of these

00:25:48,570 --> 00:25:54,290
problems we built a system called cream

00:25:50,970 --> 00:25:56,370
with stress queue which helps us

00:25:54,290 --> 00:26:01,110
resolving all issues that we talked

00:25:56,370 --> 00:26:03,720
about so let's get into details of priam

00:26:01,110 --> 00:26:05,910
using you know building Cassandra in

00:26:03,720 --> 00:26:09,630
cloud with p.m. so cream is a sidecar

00:26:05,910 --> 00:26:11,520
which runs on us on the same instance

00:26:09,630 --> 00:26:14,880
along with your main data store

00:26:11,520 --> 00:26:16,770
Cassandra we use the same sidecar for

00:26:14,880 --> 00:26:18,720
providing elastic search as a service to

00:26:16,770 --> 00:26:21,240
provide dynamite as a service or to

00:26:18,720 --> 00:26:24,180
provide zookeeper as a service so all of

00:26:21,240 --> 00:26:27,570
our CD provided services comes up to the

00:26:24,180 --> 00:26:31,170
side cost and in in case of priam and

00:26:27,570 --> 00:26:33,720
Cassandra prim is mainly responsible for

00:26:31,170 --> 00:26:36,180
bootstrapping cluster bootstrapping a

00:26:33,720 --> 00:26:38,370
node and automated token assignments so

00:26:36,180 --> 00:26:41,179
that we do not create a hotspots

00:26:38,370 --> 00:26:43,890
and backing up the data all the time and

00:26:41,179 --> 00:26:47,720
restoring and recovering the data in

00:26:43,890 --> 00:26:50,730
case of an emergency and also

00:26:47,720 --> 00:26:53,070
configuration management across your

00:26:50,730 --> 00:26:56,789
10,000 nodes or 20,000 nodes is being

00:26:53,070 --> 00:26:59,279
done with the help of p.m. and we which

00:26:56,789 --> 00:27:02,059
has REST API for all of your node tool

00:26:59,279 --> 00:27:05,490
commands and all of the management of

00:27:02,059 --> 00:27:06,480
database and we collect metrics using

00:27:05,490 --> 00:27:10,350
Cassandra JMX

00:27:06,480 --> 00:27:12,210
and heartbeats that I talked about in

00:27:10,350 --> 00:27:18,480
the health check are being sent from the

00:27:12,210 --> 00:27:20,929
p.m. so this is a high-level

00:27:18,480 --> 00:27:23,100
architecture layout of how we build

00:27:20,929 --> 00:27:25,710
Cassandra ring in the cloud

00:27:23,100 --> 00:27:30,840
specifically in AWS because we are in a

00:27:25,710 --> 00:27:33,299
place so we have ABC let us call each of

00:27:30,840 --> 00:27:37,080
those nodes as at different regions so

00:27:33,299 --> 00:27:39,000
you have one node in one region and the

00:27:37,080 --> 00:27:41,640
next token would be in a different

00:27:39,000 --> 00:27:44,399
region and the next token would be in

00:27:41,640 --> 00:27:46,649
another different region so that we can

00:27:44,399 --> 00:27:49,020
have this really NC I will get into how

00:27:46,649 --> 00:27:51,679
we achieve that resiliency across zone

00:27:49,020 --> 00:27:56,250
across the region outages in next slide

00:27:51,679 --> 00:27:58,860
but each Cassandra instance in AWS would

00:27:56,250 --> 00:28:00,929
have three processes two processes

00:27:58,860 --> 00:28:03,659
coming in it one is the main process

00:28:00,929 --> 00:28:07,799
Cassandra and we and also the sidecar

00:28:03,659 --> 00:28:09,620
called Priam and cream is as I said

00:28:07,799 --> 00:28:12,360
Priam is responsible for managing the

00:28:09,620 --> 00:28:17,370
Cassandra so it's basically babysits

00:28:12,360 --> 00:28:20,460
Cassandra so this would be the thing

00:28:17,370 --> 00:28:23,279
output it's not an interior output but

00:28:20,460 --> 00:28:27,320
this is how your ring is deployed so you

00:28:23,279 --> 00:28:32,340
have another example you have you West

00:28:27,320 --> 00:28:34,740
brac 1a USC is to 1e you vest 1b this is

00:28:32,340 --> 00:28:36,899
a layout of tokens to achieve the

00:28:34,740 --> 00:28:39,539
resiliency when I share these slides you

00:28:36,899 --> 00:28:42,330
can follow the same token distribution

00:28:39,539 --> 00:28:44,909
or if you are using Priam these token

00:28:42,330 --> 00:28:47,730
distribution is comes to you for free

00:28:44,909 --> 00:28:49,649
which gives you the resiliency at every

00:28:47,730 --> 00:28:50,809
layer every level that we're going to

00:28:49,649 --> 00:28:54,570
talk about

00:28:50,809 --> 00:28:57,900
so with this layout with this

00:28:54,570 --> 00:29:00,210
distribution of tokens we get resiliency

00:28:57,900 --> 00:29:02,100
at instance level availability zone

00:29:00,210 --> 00:29:05,309
level multiple availability zones level

00:29:02,100 --> 00:29:08,790
and vision level so there would be there

00:29:05,309 --> 00:29:10,920
would be no incident because of a WSSC

00:29:08,790 --> 00:29:13,460
outage or a double screen outages or

00:29:10,920 --> 00:29:15,510
later bliss availability zone outages

00:29:13,460 --> 00:29:16,970
regardless of any of those outages

00:29:15,510 --> 00:29:20,400
Cassandra would be up and running

00:29:16,970 --> 00:29:25,850
because of how we deployed with the help

00:29:20,400 --> 00:29:30,290
of p.m. so how do we sustain instance

00:29:25,850 --> 00:29:33,260
outages so most of our data is

00:29:30,290 --> 00:29:36,419
replicated with replication factor 3 and

00:29:33,260 --> 00:29:38,760
we deployed Cassandra in three different

00:29:36,419 --> 00:29:42,480
availability zones within a region so

00:29:38,760 --> 00:29:44,600
and the way we distributed token across

00:29:42,480 --> 00:29:47,669
three availability zones gives us

00:29:44,600 --> 00:29:50,070
flexibility that when you insert a

00:29:47,669 --> 00:29:51,840
record it would be inserted in three

00:29:50,070 --> 00:29:54,240
available three different availability

00:29:51,840 --> 00:29:56,220
zones no matter what so even if one

00:29:54,240 --> 00:29:58,350
availability zone goes away or even if

00:29:56,220 --> 00:29:59,760
one node goes away you have two other

00:29:58,350 --> 00:30:02,610
availability zones and two other

00:29:59,760 --> 00:30:04,950
instances which holds the same record so

00:30:02,610 --> 00:30:09,660
that way you get instance level

00:30:04,950 --> 00:30:12,510
resiliency and we use Priam Priam

00:30:09,660 --> 00:30:15,000
bootstraps Cassandra in terms of

00:30:12,510 --> 00:30:18,630
instance instance being terminated or

00:30:15,000 --> 00:30:20,160
sanctions being replaced and that is all

00:30:18,630 --> 00:30:21,980
automated through Priam so when a new

00:30:20,160 --> 00:30:24,240
node comes up clean bootstraps and

00:30:21,980 --> 00:30:26,370
figures out whether that Cassandra

00:30:24,240 --> 00:30:30,830
instance needs to be replaced or boots

00:30:26,370 --> 00:30:30,830
AB and does that automatically and

00:30:31,010 --> 00:30:34,950
resilience how do we provide resiliency

00:30:33,240 --> 00:30:37,500
eight available one availability zone

00:30:34,950 --> 00:30:39,780
again is this because the application

00:30:37,500 --> 00:30:43,290
factor is 3 and we deploy it in three

00:30:39,780 --> 00:30:45,120
different availability zones so even if

00:30:43,290 --> 00:30:46,590
the entire availability zone goes away

00:30:45,120 --> 00:30:48,840
you have two different availability

00:30:46,590 --> 00:30:53,340
zones which is serving your traffic and

00:30:48,840 --> 00:30:55,500
we have chaos Kong exercises which takes

00:30:53,340 --> 00:30:57,480
away instances all the time

00:30:55,500 --> 00:30:59,640
which takes away availability zones all

00:30:57,480 --> 00:31:02,010
the time but Cassandra would not be

00:30:59,640 --> 00:31:03,250
impacted I mean not even 9090 latencies

00:31:02,010 --> 00:31:07,720
would be impacted

00:31:03,250 --> 00:31:11,170
when your kiosk exercise is going on so

00:31:07,720 --> 00:31:13,930
and also another key important thing

00:31:11,170 --> 00:31:17,620
here to sustain availabilities on

00:31:13,930 --> 00:31:18,790
outages is if you are cluster is 99th

00:31:17,620 --> 00:31:22,630
latency-sensitive

00:31:18,790 --> 00:31:24,790
and even if even in case of an emergency

00:31:22,630 --> 00:31:26,500
even in case of the Untied availability

00:31:24,790 --> 00:31:29,410
zone going away if you don't want your

00:31:26,500 --> 00:31:31,300
99 to latencies to be impacted you are

00:31:29,410 --> 00:31:34,840
you need to provision cluster at 2/3

00:31:31,300 --> 00:31:36,970
capacity because when the entire velvety

00:31:34,840 --> 00:31:38,800
zone goes away you have one less of an

00:31:36,970 --> 00:31:42,040
instance to serve the same token range

00:31:38,800 --> 00:31:44,860
and your your traffic could be

00:31:42,040 --> 00:31:47,560
distributed only among two instances

00:31:44,860 --> 00:31:49,960
that are out there so it's always good

00:31:47,560 --> 00:31:52,590
to provision with 2/3 capacity so that

00:31:49,960 --> 00:31:58,150
it can sustain any zone outages as well

00:31:52,590 --> 00:32:00,790
and because we provision a 2/3 capacity

00:31:58,150 --> 00:32:02,890
we do take zone maintenance this all the

00:32:00,790 --> 00:32:04,660
time let's say if there is a restart if

00:32:02,890 --> 00:32:06,880
there is an update we take down the

00:32:04,660 --> 00:32:10,740
entire zone in upgrade and bring it back

00:32:06,880 --> 00:32:13,570
up without impacting any life's traffic

00:32:10,740 --> 00:32:17,080
how do we sustain multiple availability

00:32:13,570 --> 00:32:20,170
zones outage it depends on the usage of

00:32:17,080 --> 00:32:21,910
the cluster if you are using local

00:32:20,170 --> 00:32:24,070
Cottam application if your application

00:32:21,910 --> 00:32:25,810
is using local code room we cannot

00:32:24,070 --> 00:32:28,180
sustain multiple availability zones so

00:32:25,810 --> 00:32:30,700
that is when we fail out the traffic to

00:32:28,180 --> 00:32:33,280
a different region because that position

00:32:30,700 --> 00:32:35,200
would have the same data and can take

00:32:33,280 --> 00:32:38,050
your traffic but if your application is

00:32:35,200 --> 00:32:39,760
using a local one then we do sustain

00:32:38,050 --> 00:32:41,650
multiple availability zones outages and

00:32:39,760 --> 00:32:47,070
that depends on application to

00:32:41,650 --> 00:32:50,410
application so how do we sustain from

00:32:47,070 --> 00:32:52,120
region failures so in case of any

00:32:50,410 --> 00:32:55,360
connectivity issues between the regions

00:32:52,120 --> 00:32:57,370
or you know a SS region going away our

00:32:55,360 --> 00:32:59,740
traffic team shifts the traffic to a

00:32:57,370 --> 00:33:01,900
different region and which has all the

00:32:59,740 --> 00:33:04,870
data all the time because we do runs a

00:33:01,900 --> 00:33:07,960
pair frequently and we do keep our data

00:33:04,870 --> 00:33:10,780
center you know consistent with the

00:33:07,960 --> 00:33:13,810
other data centers so that we can have

00:33:10,780 --> 00:33:16,290
the data across three different three

00:33:13,810 --> 00:33:16,290
different regions

00:33:18,080 --> 00:33:22,700
so we talked about maintenance and

00:33:20,179 --> 00:33:24,139
monitoring so to get around with the

00:33:22,700 --> 00:33:25,840
monitoring issues we built a system

00:33:24,139 --> 00:33:28,610
called mantis based health check system

00:33:25,840 --> 00:33:31,669
to make out maintenance life easier we

00:33:28,610 --> 00:33:33,649
built a side cost called cream and how

00:33:31,669 --> 00:33:35,539
do we solve open source product

00:33:33,649 --> 00:33:36,889
challenges when you are something in a

00:33:35,539 --> 00:33:40,070
production when you're done in at scale

00:33:36,889 --> 00:33:42,320
just to give you the snapshot this is I

00:33:40,070 --> 00:33:47,230
think yesterday's snapshot of Cassandra

00:33:42,320 --> 00:33:51,379
open issues across several releases and

00:33:47,230 --> 00:33:55,840
as you can see you have so many issues

00:33:51,379 --> 00:33:57,950
coming up every week and the best way to

00:33:55,840 --> 00:34:00,200
support your open source product

00:33:57,950 --> 00:34:02,929
introduction is to have Apache

00:34:00,200 --> 00:34:05,480
committers in your team and keep looking

00:34:02,929 --> 00:34:08,030
at your feet lick you know keep an eye

00:34:05,480 --> 00:34:10,549
on the JIRA keep an eye on the product

00:34:08,030 --> 00:34:13,129
all the time and when you are using open

00:34:10,549 --> 00:34:15,409
source product driving the product

00:34:13,129 --> 00:34:17,659
vision also comes to your responsibility

00:34:15,409 --> 00:34:20,809
because based on your needs based on

00:34:17,659 --> 00:34:23,119
your requirement you would direct you

00:34:20,809 --> 00:34:25,549
know you would talk in JIRA you talk in

00:34:23,119 --> 00:34:27,889
conversations and then come up with the

00:34:25,549 --> 00:34:32,599
new features and makes which make the

00:34:27,889 --> 00:34:35,990
product better so how do we certify and

00:34:32,599 --> 00:34:37,819
do the benchmarking for the open source

00:34:35,990 --> 00:34:40,190
product like Cassandra and make it

00:34:37,819 --> 00:34:42,740
production very so that is one rebuilt

00:34:40,190 --> 00:34:46,520
system called Indy bench Netflix data

00:34:42,740 --> 00:34:49,149
benchmarking so let's fix data by MD

00:34:46,520 --> 00:34:51,710
bench is a pluggable cloud enabled

00:34:49,149 --> 00:34:54,770
benchmarking tool that can be used to

00:34:51,710 --> 00:34:57,079
benchmark beat your Cassandra

00:34:54,770 --> 00:34:59,690
elasticsearch on any persistent store

00:34:57,079 --> 00:35:03,099
that is out there and the reason we

00:34:59,690 --> 00:35:05,780
built Indy bench because we looked at

00:35:03,099 --> 00:35:08,869
open source products out there we looked

00:35:05,780 --> 00:35:12,530
at you know Yahoo Cloud benchmarking and

00:35:08,869 --> 00:35:14,030
many other been testing tools but we

00:35:12,530 --> 00:35:16,819
were mainly looking at something which

00:35:14,030 --> 00:35:21,559
which can be very close to production

00:35:16,819 --> 00:35:23,750
which is which simulates and imitates

00:35:21,559 --> 00:35:26,270
your production traffic not by just

00:35:23,750 --> 00:35:28,050
replaying the traffic but actually

00:35:26,270 --> 00:35:29,940
generating the production traffic

00:35:28,050 --> 00:35:32,370
so one of the key things there would be

00:35:29,940 --> 00:35:33,770
dynamic benchmark configurations for

00:35:32,370 --> 00:35:36,600
example if you are running a

00:35:33,770 --> 00:35:39,660
benchmarking for six hours you might not

00:35:36,600 --> 00:35:41,190
be able to reproduce any memory leak

00:35:39,660 --> 00:35:43,770
issues that are happening in your system

00:35:41,190 --> 00:35:46,590
so let's say you're trying to reproduce

00:35:43,770 --> 00:35:48,480
a memory leak that is happening in your

00:35:46,590 --> 00:35:51,030
collection system so you tried it for

00:35:48,480 --> 00:35:53,160
six hours then your your the

00:35:51,030 --> 00:35:55,590
benchmarking stops it and you can't

00:35:53,160 --> 00:35:57,450
reproduce because that memory leak might

00:35:55,590 --> 00:35:59,040
happen only after something it first two

00:35:57,450 --> 00:36:02,610
days or three days that you never know

00:35:59,040 --> 00:36:04,260
unless you reproduce it so then to Rico

00:36:02,610 --> 00:36:06,330
the same problem you would take several

00:36:04,260 --> 00:36:07,920
days to figure out when that actually

00:36:06,330 --> 00:36:10,260
when that memory leak is actually

00:36:07,920 --> 00:36:12,480
happening so that is when when you have

00:36:10,260 --> 00:36:15,540
something like in the bench which allows

00:36:12,480 --> 00:36:17,730
you to dynamically you know tune your

00:36:15,540 --> 00:36:20,100
considerations which is similar to your

00:36:17,730 --> 00:36:22,320
production environment where you have

00:36:20,100 --> 00:36:24,840
traffic going up and down you know

00:36:22,320 --> 00:36:26,490
during the peak as it goes up and during

00:36:24,840 --> 00:36:29,730
non-peak as your traffic goes down and

00:36:26,490 --> 00:36:31,680
maybe due to some you know traffic

00:36:29,730 --> 00:36:33,690
behavior there is a memory leak which

00:36:31,680 --> 00:36:35,940
you cannot reproduce when you have a

00:36:33,690 --> 00:36:38,880
constant traffic coming in so that is

00:36:35,940 --> 00:36:41,130
when you come up with you know these you

00:36:38,880 --> 00:36:42,900
know you tune the configuration while

00:36:41,130 --> 00:36:45,360
the load is coming you let it transfer

00:36:42,900 --> 00:36:47,400
the days weeks months it does not matter

00:36:45,360 --> 00:36:49,380
it runs as if there is a production

00:36:47,400 --> 00:36:51,090
traffic coming into your system and then

00:36:49,380 --> 00:36:53,550
you consume the configuration as you do

00:36:51,090 --> 00:36:55,710
in the production and you use the

00:36:53,550 --> 00:36:58,830
pattern a different load pattern and

00:36:55,710 --> 00:37:02,880
then it would be easy for you to repro

00:36:58,830 --> 00:37:04,500
any any of such issues and also be able

00:37:02,880 --> 00:37:06,740
to integrate with the rest of the cloud

00:37:04,500 --> 00:37:08,970
systems so typical problems with the

00:37:06,740 --> 00:37:11,250
benchmarking tools would be running on

00:37:08,970 --> 00:37:13,740
the different machines they do not share

00:37:11,250 --> 00:37:16,020
the same ecosystem that your production

00:37:13,740 --> 00:37:17,730
system is shading they do not co-host

00:37:16,020 --> 00:37:19,770
other services that are running out

00:37:17,730 --> 00:37:22,350
there as a result you won't be able to

00:37:19,770 --> 00:37:24,990
deploy the problem all the time and we

00:37:22,350 --> 00:37:28,020
tend a bench we solve that and when the

00:37:24,990 --> 00:37:30,090
Indy bench is running on any ego in any

00:37:28,020 --> 00:37:33,120
cloud-based instance it would be as

00:37:30,090 --> 00:37:35,100
close I mean as close as it can to any

00:37:33,120 --> 00:37:37,070
production instance it would have seen

00:37:35,100 --> 00:37:40,320
sidecar standing it would have seen

00:37:37,070 --> 00:37:41,670
other services that are running in your

00:37:40,320 --> 00:37:43,140
production system so the

00:37:41,670 --> 00:37:46,380
it would be easy for you to reprove any

00:37:43,140 --> 00:37:48,210
issues and it provides a pluggable

00:37:46,380 --> 00:37:51,420
patterns in terms of you know random

00:37:48,210 --> 00:37:53,520
pattern or you know next traffic pattern

00:37:51,420 --> 00:37:56,490
or next week's offline batch processing

00:37:53,520 --> 00:37:59,130
pattern and generate the load the way

00:37:56,490 --> 00:38:01,020
you want and it supports different

00:37:59,130 --> 00:38:04,230
client API because we have different

00:38:01,020 --> 00:38:06,869
persistent stores to be benchmarked so

00:38:04,230 --> 00:38:09,950
we made it pluggable so that it can

00:38:06,869 --> 00:38:13,109
support any different client appear so

00:38:09,950 --> 00:38:15,960
out of box we have Cassandra dynamite

00:38:13,109 --> 00:38:17,819
elasticsearch and elisandra as a plugins

00:38:15,960 --> 00:38:20,280
that are available there but you can

00:38:17,819 --> 00:38:24,210
clip in much write a plug-in for any of

00:38:20,280 --> 00:38:26,880
client that you are trying and start

00:38:24,210 --> 00:38:29,520
using it so we use nd bench at Netflix

00:38:26,880 --> 00:38:31,829
as a benchmarking tool we use it in part

00:38:29,520 --> 00:38:33,680
of integration test we use it as part of

00:38:31,829 --> 00:38:36,750
a deployment wallet validation as well

00:38:33,680 --> 00:38:39,630
so for example when you use in the bench

00:38:36,750 --> 00:38:42,839
as part of you know benchmarking so here

00:38:39,630 --> 00:38:46,319
is a snapshot of Cassandra - dot was -

00:38:42,839 --> 00:38:50,700
dot over 62.1 seed on thrift as you can

00:38:46,319 --> 00:38:54,510
see you know the blue one is to dot o

00:38:50,700 --> 00:38:57,059
and I think the purple red one is 2.1 so

00:38:54,510 --> 00:38:59,400
this gives you side-by-side comparison

00:38:57,059 --> 00:39:01,799
of different versions that are out there

00:38:59,400 --> 00:39:04,079
or different different drivers that you

00:39:01,799 --> 00:39:06,329
are trying or different environments

00:39:04,079 --> 00:39:08,339
maybe it is on a linux or maybe Ubuntu

00:39:06,329 --> 00:39:09,960
or whatever you can have this

00:39:08,339 --> 00:39:13,230
side-by-side comparison with the help of

00:39:09,960 --> 00:39:15,180
nd bench and the right after you run

00:39:13,230 --> 00:39:16,710
this performance benchmarking tool it

00:39:15,180 --> 00:39:18,750
would be easy for you to take a decision

00:39:16,710 --> 00:39:21,329
whether to go forward with the new

00:39:18,750 --> 00:39:24,690
release of Cassandra or new release of

00:39:21,329 --> 00:39:27,180
Cassandra Java driver or whatever as

00:39:24,690 --> 00:39:29,160
part of certification so every a might

00:39:27,180 --> 00:39:31,260
that we baked since we are in a tablet

00:39:29,160 --> 00:39:34,410
and ecosystem every a might that we bake

00:39:31,260 --> 00:39:37,440
goes through my bench performance test

00:39:34,410 --> 00:39:39,270
suit and based on the result only we

00:39:37,440 --> 00:39:45,119
promote that a may to be production

00:39:39,270 --> 00:39:47,400
dairy and typical problems that people

00:39:45,119 --> 00:39:49,049
run into when they are building a

00:39:47,400 --> 00:39:51,450
benchmarking tools is that benchmarking

00:39:49,049 --> 00:39:53,220
tool itself puts a lot of overhead as a

00:39:51,450 --> 00:39:54,220
result you would not be measuring the

00:39:53,220 --> 00:39:58,060
numbers

00:39:54,220 --> 00:39:59,710
so but as you can see with you know with

00:39:58,060 --> 00:40:02,170
nd bench we were able to generate

00:39:59,710 --> 00:40:06,280
millions of operations per second which

00:40:02,170 --> 00:40:09,790
are under several several milliseconds

00:40:06,280 --> 00:40:12,780
like so in this example dynamite was

00:40:09,790 --> 00:40:15,400
generating with 100 microseconds so

00:40:12,780 --> 00:40:18,520
which clearly shows in the bench doesn't

00:40:15,400 --> 00:40:21,280
put any overhead as the tools itself to

00:40:18,520 --> 00:40:24,040
measure your performance I'm not talking

00:40:21,280 --> 00:40:26,320
more about Indy bench this is a Apache

00:40:24,040 --> 00:40:30,420
License open source project you can find

00:40:26,320 --> 00:40:32,710
it out on our github netflix github and

00:40:30,420 --> 00:40:35,050
stitching it all together to provide

00:40:32,710 --> 00:40:38,350
cassandra as a service at netflix this

00:40:35,050 --> 00:40:40,960
is how our architecture looks like every

00:40:38,350 --> 00:40:43,270
Cassandra instance is having three

00:40:40,960 --> 00:40:45,630
different processes that are running you

00:40:43,270 --> 00:40:48,400
have mantas based health check

00:40:45,630 --> 00:40:50,380
Winston is our auto de mediation system

00:40:48,400 --> 00:40:52,930
which is again open sourced you can look

00:40:50,380 --> 00:40:55,450
at our Netflix tech blog which talks

00:40:52,930 --> 00:40:59,680
about Winston we have you know Mia which

00:40:55,450 --> 00:41:01,780
is which is the you know advisory or the

00:40:59,680 --> 00:41:04,630
governador to monitor all of a

00:41:01,780 --> 00:41:06,760
production fleet and as you can see all

00:41:04,630 --> 00:41:09,550
these events are being sent to our alert

00:41:06,760 --> 00:41:12,030
system Atlas which is our metric system

00:41:09,550 --> 00:41:14,890
or being auto remediated by Winston

00:41:12,030 --> 00:41:17,200
automatically and we use Jenkins for

00:41:14,890 --> 00:41:19,450
some of our maintenance and we have

00:41:17,200 --> 00:41:22,150
capacity prediction to predict the

00:41:19,450 --> 00:41:24,160
traffic to predict the you know to

00:41:22,150 --> 00:41:27,130
predict the usage pattern of any

00:41:24,160 --> 00:41:29,260
Cassandra instance we have we have

00:41:27,130 --> 00:41:32,260
outlier detection we have fork lifter

00:41:29,260 --> 00:41:36,250
tools and we have log analysis which is

00:41:32,260 --> 00:41:38,080
LK based and we see Reaper some get

00:41:36,250 --> 00:41:40,030
spaced only after going through several

00:41:38,080 --> 00:41:41,800
these systems when none of these systems

00:41:40,030 --> 00:41:45,010
cannot solve the issue that is when we

00:41:41,800 --> 00:41:47,230
get paged that I'm opening up for

00:41:45,010 --> 00:41:50,410
questions and the thing we have five

00:41:47,230 --> 00:41:54,070
more minutes and yes we do Netflix is

00:41:50,410 --> 00:41:55,960
hiring job start Netflix calm and these

00:41:54,070 --> 00:41:58,000
are the two opening positions in our

00:41:55,960 --> 00:42:00,310
team if you want to work in you know

00:41:58,000 --> 00:42:03,420
this kind of architecture based systems

00:42:00,310 --> 00:42:03,420
you see certain

00:42:03,910 --> 00:42:21,619
[Applause]

00:42:09,850 --> 00:42:23,840
yes so that we used mantas are shaming

00:42:21,619 --> 00:42:26,150
based system and we came up with all the

00:42:23,840 --> 00:42:27,800
business logic that we gained over the

00:42:26,150 --> 00:42:31,100
several years of running percentile

00:42:27,800 --> 00:42:33,650
service and that is how we decided what

00:42:31,100 --> 00:42:36,260
makes sense to page

00:42:33,650 --> 00:42:41,770
CDR to page order the mediation to

00:42:36,260 --> 00:42:41,770
system to put a can action on

00:42:52,499 --> 00:42:57,819
okay so the question is in health check

00:42:55,930 --> 00:42:59,170
system where do local aggregate and

00:42:57,819 --> 00:43:01,989
global thing aggregate I store this

00:42:59,170 --> 00:43:05,019
state so that is given by mandates

00:43:01,989 --> 00:43:09,999
framework so mantis framework holds the

00:43:05,019 --> 00:43:11,589
data in there in the ecosystem and it it

00:43:09,999 --> 00:43:15,279
also has a check pointing system which

00:43:11,589 --> 00:43:17,670
checkpoints to HDFS s3 and several other

00:43:15,279 --> 00:43:19,869
data stores but that comes from

00:43:17,670 --> 00:43:22,900
streaming service offering that we have

00:43:19,869 --> 00:43:25,380
you can simulate this in spark streaming

00:43:22,900 --> 00:43:27,880
environment as well if you write a

00:43:25,380 --> 00:43:32,079
window based or the time window based

00:43:27,880 --> 00:43:33,249
job in spark streaming that I think you

00:43:32,079 --> 00:43:35,739
can come up with the different

00:43:33,249 --> 00:43:38,680
persistent stores so this quarter we are

00:43:35,739 --> 00:43:40,449
releasing we are open sourcing mantis in

00:43:38,680 --> 00:43:43,239
that open source blog we'll talk about

00:43:40,449 --> 00:43:50,799
more on how these jobs persist the state

00:43:43,239 --> 00:43:53,469
and all that stuff yeah yes so when we

00:43:50,799 --> 00:43:57,209
tried V nodes I'm talking about two and

00:43:53,469 --> 00:43:58,989
two years back when they released so the

00:43:57,209 --> 00:44:00,789
resiliency that we were talking about

00:43:58,989 --> 00:44:04,059
instance availability zones a zillion

00:44:00,789 --> 00:44:07,150
see the way we nodes distributed tokens

00:44:04,059 --> 00:44:10,019
was not aware of availability zones and

00:44:07,150 --> 00:44:13,539
regions so we had instances where same

00:44:10,019 --> 00:44:15,849
availability zones had replication

00:44:13,539 --> 00:44:18,009
replicated tokens as a result if you

00:44:15,849 --> 00:44:18,819
when you take the availability zone out

00:44:18,009 --> 00:44:20,410
of the picture

00:44:18,819 --> 00:44:22,180
you have a token ginger officer in

00:44:20,410 --> 00:44:24,009
exception as a result you have a

00:44:22,180 --> 00:44:25,869
downtime in Cassandra that is when we

00:44:24,009 --> 00:44:28,449
decided not to use V notes but at

00:44:25,869 --> 00:44:30,099
current state of V nodes I know lot of

00:44:28,449 --> 00:44:31,839
token distributed algorithms have been

00:44:30,099 --> 00:44:34,809
improved in V nodes I don't know the

00:44:31,839 --> 00:44:37,199
current state but we might revisit in

00:44:34,809 --> 00:44:37,199
future

00:44:37,800 --> 00:44:49,950
yes battle in your own so yeah so next

00:44:49,320 --> 00:44:52,560
is

00:44:49,950 --> 00:44:56,160
I think hundred percent is on AWS we

00:44:52,560 --> 00:44:58,380
don't have any wave metal or any of data

00:44:56,160 --> 00:45:01,980
center so we deploy it in I to based

00:44:58,380 --> 00:45:03,930
instances and mostly 95 percent on I to

00:45:01,980 --> 00:45:07,410
based instances IT to excel I too for

00:45:03,930 --> 00:45:15,630
Excel and I 2x8 Excel in terms of a SS

00:45:07,410 --> 00:45:20,180
terminology unopen - yeah we burn on

00:45:15,630 --> 00:45:20,180
Linux but we might get it to boot yes

00:45:20,510 --> 00:45:23,510
correct

00:45:26,390 --> 00:45:32,310
yeah so today we have automated BOTS

00:45:29,280 --> 00:45:35,520
which are keeping an eye on our usage

00:45:32,310 --> 00:45:38,480
and direction so we automated our I

00:45:35,520 --> 00:45:42,000
would say 20% of auto-scaling

00:45:38,480 --> 00:45:43,350
where it is so obvious that you need to

00:45:42,000 --> 00:45:45,780
you know exchange the cluster you need

00:45:43,350 --> 00:45:47,550
to add nodes to the cluster but as you

00:45:45,780 --> 00:45:49,980
know the problems with the Cassandra

00:45:47,550 --> 00:45:51,750
doubling and process it takes you know

00:45:49,980 --> 00:45:55,200
several months depends on our data says

00:45:51,750 --> 00:45:56,880
we haven't automated 800 percent but we

00:45:55,200 --> 00:46:01,970
are in the process of automating that as

00:45:56,880 --> 00:46:07,390
well thank you

00:46:01,970 --> 00:46:07,390

YouTube URL: https://www.youtube.com/watch?v=2l0_onmQsPI


