Title: Spotlight on Emerging Data Science Worskspaces and Spark as Development Levers
Publication date: 2017-05-17
Playlist: ApacheCon 2017 - Miami
Description: 
	Spotlight on Emerging Data Science Worskspaces and Spark as Development Levers - Carlo Appugliese, IBM

Developers are changing the way we live, play and work, and a focal point of their work is  using Data Science and Machine Learning on data applications. In this session, you will learn how Application Developers can measily build Machine Learning solutions using Apache Spark, Watson Machine Learning and a collaborative workspace called the 'Data Science Experience.' IBM's Data Science Experience is a new cloud-based, social workspace that helps the data professional collaborate and create digital solutions using application services and open source technologies like Python and Apache Spark.
Captions: 
	00:00:00,000 --> 00:00:05,850
I say hey everybody Carlo Puglisi here I

00:00:02,399 --> 00:00:09,090
like to keep it informal I've been with

00:00:05,850 --> 00:00:10,650
datas with IBM for a few years now prior

00:00:09,090 --> 00:00:11,759
I used to work for a company I was

00:00:10,650 --> 00:00:13,469
director of innovations I've always

00:00:11,759 --> 00:00:17,970
dealt with emerging technologies and

00:00:13,469 --> 00:00:20,850
trends my focus at IBM is I'm the global

00:00:17,970 --> 00:00:23,189
data science and big data evangelist for

00:00:20,850 --> 00:00:25,519
our platforms and our tools so I'm going

00:00:23,189 --> 00:00:27,599
to cover for you a little bit about

00:00:25,519 --> 00:00:29,279
things that are emerging in the space

00:00:27,599 --> 00:00:32,279
around data science the particularly how

00:00:29,279 --> 00:00:33,630
the team approach Apache spark I'll go

00:00:32,279 --> 00:00:35,160
in three deep why patch spark is

00:00:33,630 --> 00:00:38,129
important and the ecosystem open source

00:00:35,160 --> 00:00:40,530
tools versus tools out in the space for

00:00:38,129 --> 00:00:43,079
data science and then at the end I'll

00:00:40,530 --> 00:00:46,320
give you a quick demo so how many people

00:00:43,079 --> 00:00:49,579
here actually understand no Apache spark

00:00:46,320 --> 00:00:52,710
pretty well or deep with it at all

00:00:49,579 --> 00:00:55,949
hands-on coding Python developers at all

00:00:52,710 --> 00:00:57,899
a little bit okay cool and any are

00:00:55,949 --> 00:01:00,180
developers or data scientists in the

00:00:57,899 --> 00:01:01,770
room ok good this'll be good talk for

00:01:00,180 --> 00:01:04,530
you guys then I was worried about

00:01:01,770 --> 00:01:08,100
getting to high level and then go to

00:01:04,530 --> 00:01:09,630
deep so well first you know I talked to

00:01:08,100 --> 00:01:13,020
clients all the time I meet with large

00:01:09,630 --> 00:01:14,490
big clients and everybody's faced the

00:01:13,020 --> 00:01:16,950
same problem you know we live in a

00:01:14,490 --> 00:01:19,890
digital age and you know you guys know

00:01:16,950 --> 00:01:21,630
this but everything we do we live play

00:01:19,890 --> 00:01:23,759
learn and you know everything's

00:01:21,630 --> 00:01:26,729
digitally now and so the way customers

00:01:23,759 --> 00:01:28,890
are being handled and way companies are

00:01:26,729 --> 00:01:30,869
dealing with the digital evolution is

00:01:28,890 --> 00:01:33,780
different for each company I mean we I

00:01:30,869 --> 00:01:35,820
work with companies such as you know

00:01:33,780 --> 00:01:38,369
large banks you know Bank of America JP

00:01:35,820 --> 00:01:39,810
Morgan Chase and these guys are going

00:01:38,369 --> 00:01:41,880
deep in the data science and they're

00:01:39,810 --> 00:01:44,759
transforming their businesses but still

00:01:41,880 --> 00:01:47,450
their main money is based on banking but

00:01:44,759 --> 00:01:49,979
everybody is faced with how do they

00:01:47,450 --> 00:01:53,070
innovate and how they keep up in today's

00:01:49,979 --> 00:01:55,140
market and and so we did it recently a

00:01:53,070 --> 00:01:56,969
business Harvard a Harvard Business

00:01:55,140 --> 00:02:00,380
Review recently what came together and

00:01:56,969 --> 00:02:03,149
it said that 72% of companies are

00:02:00,380 --> 00:02:05,549
vulnerable to disruption from digital

00:02:03,149 --> 00:02:07,439
businesses so digital businesses are

00:02:05,549 --> 00:02:09,390
disrupting industries I mean you see it

00:02:07,439 --> 00:02:11,379
with the uber 's and so on but it's

00:02:09,390 --> 00:02:15,150
happening it's real and

00:02:11,379 --> 00:02:17,409
so to all be done with data science in

00:02:15,150 --> 00:02:19,560
application development so that's kind

00:02:17,409 --> 00:02:23,019
of my my thing I'm going to go into here

00:02:19,560 --> 00:02:25,840
however data-driven organizations are

00:02:23,019 --> 00:02:28,840
really built around a team data science

00:02:25,840 --> 00:02:32,620
is really team sport and we at IBM have

00:02:28,840 --> 00:02:35,109
focused in on trying to enable these

00:02:32,620 --> 00:02:37,659
these four personas as well as the chief

00:02:35,109 --> 00:02:39,250
data officer the data scientist is at

00:02:37,659 --> 00:02:41,739
the core this is the person that builds

00:02:39,250 --> 00:02:43,719
your models your algorithms does

00:02:41,739 --> 00:02:45,760
discovery work figure out new business

00:02:43,719 --> 00:02:47,139
models the business analyst is the

00:02:45,760 --> 00:02:49,359
person that's kind of got the business

00:02:47,139 --> 00:02:51,280
expertise and they're the ones that are

00:02:49,359 --> 00:02:54,609
kind of identifying opportunities that

00:02:51,280 --> 00:02:57,129
could be a come available based on your

00:02:54,609 --> 00:02:59,169
data the data engineer is really that

00:02:57,129 --> 00:03:01,269
guy that's you know you deal with a lot

00:02:59,169 --> 00:03:03,340
the DBA type who's got all the data

00:03:01,269 --> 00:03:05,680
don't wanna let go of it and I'm more of

00:03:03,340 --> 00:03:07,870
a developer type myself but the data

00:03:05,680 --> 00:03:09,699
engineer is still relevant you know and

00:03:07,870 --> 00:03:11,620
their job is really bringing in that

00:03:09,699 --> 00:03:12,939
data making sure it's secure all the

00:03:11,620 --> 00:03:14,409
things that the developers typically

00:03:12,939 --> 00:03:15,819
don't want to deal with and then the

00:03:14,409 --> 00:03:17,620
application developers really the true

00:03:15,819 --> 00:03:19,659
one that's kind of putting that in the

00:03:17,620 --> 00:03:24,340
hands of your customers and building the

00:03:19,659 --> 00:03:26,470
applications however in this time right

00:03:24,340 --> 00:03:28,780
now there's really a shortage of data

00:03:26,470 --> 00:03:31,389
scientists because they scientist seems

00:03:28,780 --> 00:03:32,979
to be the the go-to persona or person

00:03:31,389 --> 00:03:34,720
for all these problems we're trying to

00:03:32,979 --> 00:03:36,909
solve what the reality is is we're

00:03:34,720 --> 00:03:40,030
really just trying to solve putting data

00:03:36,909 --> 00:03:41,949
science into our applications and so so

00:03:40,030 --> 00:03:44,409
a lot of companies are having data

00:03:41,949 --> 00:03:45,939
scientists be assigned silos and they're

00:03:44,409 --> 00:03:48,609
doing discovery data model and

00:03:45,939 --> 00:03:51,099
development and then they work with the

00:03:48,609 --> 00:03:52,989
engineers developers and data analysts

00:03:51,099 --> 00:03:55,840
for them to be truly productive there's

00:03:52,989 --> 00:03:57,969
a lot of draw there and so what I want

00:03:55,840 --> 00:04:00,759
to talk to a little bit about is how

00:03:57,969 --> 00:04:03,969
that particular role is becoming a

00:04:00,759 --> 00:04:06,280
really a difficult job in the tools that

00:04:03,969 --> 00:04:08,169
are in the space and why Apache spark is

00:04:06,280 --> 00:04:09,879
kind of changing that a bit and why the

00:04:08,169 --> 00:04:14,109
application developer will become more

00:04:09,879 --> 00:04:16,989
relevant in this role so really like

00:04:14,109 --> 00:04:20,169
what is a data scientist and a data

00:04:16,989 --> 00:04:22,960
scientist I always talk about they they

00:04:20,169 --> 00:04:25,300
have three oh by the way I forgot to

00:04:22,960 --> 00:04:28,419
mention I got three hoodies here

00:04:25,300 --> 00:04:30,550
IBM Sparky so whoever ask questions

00:04:28,419 --> 00:04:34,840
first gets a hoodie and I only got a

00:04:30,550 --> 00:04:37,870
medium of large and extra-large just so

00:04:34,840 --> 00:04:41,830
you know one each so you have a question

00:04:37,870 --> 00:04:46,000
interrupt me but a data scientist really

00:04:41,830 --> 00:04:47,560
has three main skills and this comes

00:04:46,000 --> 00:04:49,630
together with you have the statistical

00:04:47,560 --> 00:04:51,300
side statistician understand our

00:04:49,630 --> 00:04:53,740
algorithms they understand statistics

00:04:51,300 --> 00:04:56,050
then there's the domain expertise this

00:04:53,740 --> 00:04:57,699
is really the understanding the domain

00:04:56,050 --> 00:04:58,990
you're in the business problem and the

00:04:57,699 --> 00:05:00,940
challenge you're trying to address and

00:04:58,990 --> 00:05:03,580
then the programming skills or the

00:05:00,940 --> 00:05:05,110
computer science skills there's no real

00:05:03,580 --> 00:05:07,060
one person that has all three of these

00:05:05,110 --> 00:05:08,259
you know that's why data science is kind

00:05:07,060 --> 00:05:10,150
of a team sport we see this in big

00:05:08,259 --> 00:05:12,520
companies we have some guys that are

00:05:10,150 --> 00:05:14,199
that they're pure mathematicians and

00:05:12,520 --> 00:05:16,300
they're very good at data scientists

00:05:14,199 --> 00:05:18,550
they also maybe can understand the

00:05:16,300 --> 00:05:20,470
domain but they may not be as deep in

00:05:18,550 --> 00:05:22,240
the coding side of things but then you

00:05:20,470 --> 00:05:24,789
have people that are more coders data

00:05:22,240 --> 00:05:26,440
you know developers who understand the

00:05:24,789 --> 00:05:28,690
other two or domain expertise but they

00:05:26,440 --> 00:05:31,210
don't you know so so data scientists

00:05:28,690 --> 00:05:33,159
could fall anywhere in this diagram and

00:05:31,210 --> 00:05:37,719
it's kind of what makes up a data

00:05:33,159 --> 00:05:39,310
scientist data science tools so Gardner

00:05:37,719 --> 00:05:41,830
recently came out with the Magic

00:05:39,310 --> 00:05:46,650
Quadrant and you know I know IBM DUP

00:05:41,830 --> 00:05:49,449
there's a shameless plug if you see if I

00:05:46,650 --> 00:05:51,610
got to make a profit there's no open

00:05:49,449 --> 00:05:54,940
source tools here so my point here is to

00:05:51,610 --> 00:05:56,860
truly talk about manage platforms are

00:05:54,940 --> 00:05:58,449
provided but there's a variety of tools

00:05:56,860 --> 00:06:01,479
and there's a variety approaches that

00:05:58,449 --> 00:06:03,159
were are being solved IBM obviously we

00:06:01,479 --> 00:06:05,319
have our SPSS product that's quite

00:06:03,159 --> 00:06:06,969
mature but we're also embracing open

00:06:05,319 --> 00:06:10,509
sources with really I want to cover in

00:06:06,969 --> 00:06:12,969
this talk but if you look on Google

00:06:10,509 --> 00:06:16,000
Trends this is not market share but

00:06:12,969 --> 00:06:18,400
you'll see that if you put in Gardner

00:06:16,000 --> 00:06:20,289
doesn't put open source against manage

00:06:18,400 --> 00:06:22,539
platforms they just don't do that but if

00:06:20,289 --> 00:06:24,310
you if you put Google Trends and you'll

00:06:22,539 --> 00:06:28,030
see the interest the open source

00:06:24,310 --> 00:06:32,349
community with Python and R is grown

00:06:28,030 --> 00:06:35,229
considerably now SPSS and SAS really has

00:06:32,349 --> 00:06:36,940
managed platforms that are out there and

00:06:35,229 --> 00:06:39,069
that have quite a bit of market share

00:06:36,940 --> 00:06:40,809
but you'll see there's a ton of people

00:06:39,069 --> 00:06:44,050
using the open-source technology and

00:06:40,809 --> 00:06:46,719
Scala as well as kind of role because

00:06:44,050 --> 00:06:49,029
the niche area but but a lot of people

00:06:46,719 --> 00:06:51,479
helping scholar do data science work any

00:06:49,029 --> 00:07:01,149
questions so far

00:06:51,479 --> 00:07:03,129
yes you know that's a great question

00:07:01,149 --> 00:07:05,889
there's really no data out that I know

00:07:03,129 --> 00:07:08,619
of on that I know there was recently a

00:07:05,889 --> 00:07:11,020
great blog out there with one of the

00:07:08,619 --> 00:07:12,759
analysts who compared to Gardner report

00:07:11,020 --> 00:07:14,800
that came out in February with all the

00:07:12,759 --> 00:07:16,749
open source and there's nobody does I

00:07:14,800 --> 00:07:20,680
think there's gonna be work done to that

00:07:16,749 --> 00:07:23,949
but but they really break it down based

00:07:20,680 --> 00:07:25,629
on manage tools by vendors and open

00:07:23,949 --> 00:07:26,649
source is kind of separate so I don't

00:07:25,629 --> 00:07:28,389
know that for sure

00:07:26,649 --> 00:07:32,800
I would think open source has a big

00:07:28,389 --> 00:07:34,659
market share too or maybe more but IBM

00:07:32,800 --> 00:07:38,069
tells me we have good marks here I'm

00:07:34,659 --> 00:07:38,069
sure SAS tells the other guys they got

00:07:39,029 --> 00:07:46,059
yeah well Gardner is independent so I

00:07:42,729 --> 00:07:51,159
can't really by those guys come where I

00:07:46,059 --> 00:07:53,709
hear hey that was the question you want

00:07:51,159 --> 00:08:00,809
to spark city with IBM logo on it what

00:07:53,709 --> 00:08:06,610
size up so let's large let me see here

00:08:00,809 --> 00:08:09,550
it's not for sale ok you can't sell this

00:08:06,610 --> 00:08:16,709
if you sell you a little unit there you

00:08:09,550 --> 00:08:21,149
go to my home I want you rhythm so

00:08:16,709 --> 00:08:24,279
here's the open source ecosystem that I

00:08:21,149 --> 00:08:26,490
talk about this all the time and you

00:08:24,279 --> 00:08:29,469
know for if you're looking for a job

00:08:26,490 --> 00:08:32,139
seriously making good money learn Big

00:08:29,469 --> 00:08:34,389
Data learn patch spark learn you know

00:08:32,139 --> 00:08:35,079
you can go either to our path or the

00:08:34,389 --> 00:08:36,909
Python path

00:08:35,079 --> 00:08:39,130
I recommend Python I particularly like

00:08:36,909 --> 00:08:41,800
my son better but I'm the programmer a

00:08:39,130 --> 00:08:44,260
lot of stats that God is like are but

00:08:41,800 --> 00:08:45,670
there's a I did a search out and I

00:08:44,260 --> 00:08:48,040
didn't put a chart up for this but I did

00:08:45,670 --> 00:08:49,959
search out on think it's indeed or

00:08:48,040 --> 00:08:51,910
somewhere in you know searching terms

00:08:49,959 --> 00:08:52,390
and these these data science rules

00:08:51,910 --> 00:08:53,680
there

00:08:52,390 --> 00:08:55,270
I earn like crazy everywhere I go

00:08:53,680 --> 00:08:56,530
there's a shortage of them and these are

00:08:55,270 --> 00:08:59,020
the skill sets that are really coming

00:08:56,530 --> 00:09:00,340
together well and even at IBM our tool

00:08:59,020 --> 00:09:05,230
sets are really good and built around

00:09:00,340 --> 00:09:07,540
these technologies so just kind of a

00:09:05,230 --> 00:09:10,060
heads up but if you look at Python

00:09:07,540 --> 00:09:14,170
you'll see they right here familiar

00:09:10,060 --> 00:09:15,580
Jupiter notebooks at all ok good to my

00:09:14,170 --> 00:09:18,160
demo is going to be really well put so

00:09:15,580 --> 00:09:20,290
so Jupiter notebooks it's an interface

00:09:18,160 --> 00:09:22,870
that allows you to do Python development

00:09:20,290 --> 00:09:24,130
which works well with spark PI spark

00:09:22,870 --> 00:09:26,140
which we're gonna get into more depth

00:09:24,130 --> 00:09:27,730
around what a pet spark is you know

00:09:26,140 --> 00:09:29,160
pythons obviously language and this guy

00:09:27,730 --> 00:09:30,940
can't learn which is a machine learning

00:09:29,160 --> 00:09:33,010
capability as well as the machine

00:09:30,940 --> 00:09:35,980
learning capability and spark really are

00:09:33,010 --> 00:09:38,590
a good combination of tools that give

00:09:35,980 --> 00:09:41,860
you the ability to create some

00:09:38,590 --> 00:09:43,840
predictive models and then are you got

00:09:41,860 --> 00:09:45,940
our studio and shiny shiny is a great

00:09:43,840 --> 00:09:47,650
visualization dashboard tool if you're

00:09:45,940 --> 00:09:49,270
familiar with it they all complement

00:09:47,650 --> 00:09:51,930
each other really well and I'll give you

00:09:49,270 --> 00:09:54,400
a demo of what that looks like

00:09:51,930 --> 00:09:55,780
so Apache spark I'm gonna be very heavy

00:09:54,400 --> 00:09:59,710
on petty spark here because this is a

00:09:55,780 --> 00:10:02,020
petty con but we folks that don't know

00:09:59,710 --> 00:10:05,590
if folks do know but pachi spark is an

00:10:02,020 --> 00:10:07,750
in-memory application framework for

00:10:05,590 --> 00:10:09,610
distributed computing you know and you

00:10:07,750 --> 00:10:11,260
know a lot of people for iterative

00:10:09,610 --> 00:10:13,810
analysis on massive data of gripper but

00:10:11,260 --> 00:10:16,330
a lot of people think of spark the same

00:10:13,810 --> 00:10:17,920
as I do but it's really not if you

00:10:16,330 --> 00:10:20,380
familiar and very familiar with Hadoop

00:10:17,920 --> 00:10:22,660
but what Hadoop is so Hadoop's really a

00:10:20,380 --> 00:10:24,280
two-headed coin it's got a computer

00:10:22,660 --> 00:10:26,950
engine and as well it's got a

00:10:24,280 --> 00:10:29,230
distributed file system and so they've

00:10:26,950 --> 00:10:31,240
kind of work together with spark it's

00:10:29,230 --> 00:10:31,420
really that compute engine that's all it

00:10:31,240 --> 00:10:34,780
is

00:10:31,420 --> 00:10:37,090
I mean it's a lot but it doesn't have

00:10:34,780 --> 00:10:39,190
the storage layer and there's a reason

00:10:37,090 --> 00:10:41,680
for that and we consider it we call it

00:10:39,190 --> 00:10:43,750
our analytic operating system at IBM we

00:10:41,680 --> 00:10:46,090
embrace spark quite a bit and we're

00:10:43,750 --> 00:10:49,150
invested in spark a lot but it's a

00:10:46,090 --> 00:10:53,080
analytic application framework that does

00:10:49,150 --> 00:10:55,030
computation on the fly and it scales out

00:10:53,080 --> 00:10:59,500
endlessly obviously it's distributed

00:10:55,030 --> 00:11:02,530
there's no limit if I rambled that one

00:10:59,500 --> 00:11:05,300
on pretty bad so sparks really hot I

00:11:02,530 --> 00:11:08,120
mean it's one of the top projects

00:11:05,300 --> 00:11:10,580
active open-source projects if you look

00:11:08,120 --> 00:11:13,399
at compared to Kafka the storm and flank

00:11:10,580 --> 00:11:16,820
and some of the other ones there's way

00:11:13,399 --> 00:11:19,519
more contributions and commits so

00:11:16,820 --> 00:11:23,870
there's a lot of activity going on with

00:11:19,519 --> 00:11:26,810
spark and there's a reason I don't know

00:11:23,870 --> 00:11:30,290
why this is there's a reason

00:11:26,810 --> 00:11:33,380
Apache spark if you look at MapReduce to

00:11:30,290 --> 00:11:35,660
write a simple word count with MapReduce

00:11:33,380 --> 00:11:38,089
took tons of lines of code you know so

00:11:35,660 --> 00:11:39,980
maybe 100 lines of code would spark it's

00:11:38,089 --> 00:11:43,220
really three lines of code and it's just

00:11:39,980 --> 00:11:45,260
it simplified the development against

00:11:43,220 --> 00:11:47,450
Big Data so much more and it's so much

00:11:45,260 --> 00:11:48,589
faster that's the reason there's so much

00:11:47,450 --> 00:11:53,329
interested in and I'm going to explain

00:11:48,589 --> 00:11:56,029
why so here just a high-level overview

00:11:53,329 --> 00:11:59,390
is anybody familiar with this slide at

00:11:56,029 --> 00:12:02,120
all so this is a patch of spark I'm

00:11:59,390 --> 00:12:05,740
supposed to stay by this microphone but

00:12:02,120 --> 00:12:08,510
I like to wander this particular

00:12:05,740 --> 00:12:10,790
architecture kind of talks about how

00:12:08,510 --> 00:12:13,640
what Apache spark is so that there's

00:12:10,790 --> 00:12:16,970
really it's got a set of API layers that

00:12:13,640 --> 00:12:20,029
that sit on top of a core compute engine

00:12:16,970 --> 00:12:21,529
the core compute engine is quite

00:12:20,029 --> 00:12:25,970
sophisticated it's much better than

00:12:21,529 --> 00:12:28,459
MapReduce it uses a called a cyclical or

00:12:25,970 --> 00:12:30,950
critical graph it's a graph that manages

00:12:28,459 --> 00:12:33,920
all the transformations so that when you

00:12:30,950 --> 00:12:35,720
run you have two types of actions that

00:12:33,920 --> 00:12:37,910
could happen in your compute engine you

00:12:35,720 --> 00:12:39,950
have a transformation or an action so

00:12:37,910 --> 00:12:42,860
every transformation it keeps tracking a

00:12:39,950 --> 00:12:44,899
graph and then at the end when you hit

00:12:42,860 --> 00:12:46,399
an action it'll do them all together as

00:12:44,899 --> 00:12:48,709
one computation so you're much faster

00:12:46,399 --> 00:12:52,130
MapReduce which spills back and forth to

00:12:48,709 --> 00:12:54,560
disk I always tell the story of if you

00:12:52,130 --> 00:12:56,770
have a file that has a million rows in

00:12:54,560 --> 00:12:59,600
it and then the next line of code says

00:12:56,770 --> 00:13:01,579
just sum up the numbers for the first

00:12:59,600 --> 00:13:03,260
hundred rows why does it have to load

00:13:01,579 --> 00:13:05,209
that whole thing in the memory and then

00:13:03,260 --> 00:13:07,699
wait for the next action so it kind of

00:13:05,209 --> 00:13:08,990
waits and does it all together so it

00:13:07,699 --> 00:13:10,550
knows that just need to read the first

00:13:08,990 --> 00:13:12,980
under rows and we'll give you back your

00:13:10,550 --> 00:13:15,529
results so it's a better way of doing

00:13:12,980 --> 00:13:17,630
distributed computing and it's very

00:13:15,529 --> 00:13:18,950
powerful the next thing that's great

00:13:17,630 --> 00:13:21,380
about SPARC is

00:13:18,950 --> 00:13:23,540
a variety of api's which are all part of

00:13:21,380 --> 00:13:25,970
the same project so if you look at

00:13:23,540 --> 00:13:28,310
Hadoop it's an ecosystem projects you

00:13:25,970 --> 00:13:29,930
got hive one project you got you know

00:13:28,310 --> 00:13:31,520
MapReduce another project you got

00:13:29,930 --> 00:13:33,920
another project for storm which is

00:13:31,520 --> 00:13:37,250
streaming with spark you have spark

00:13:33,920 --> 00:13:39,350
sequel built in you have streaming which

00:13:37,250 --> 00:13:41,720
is built in these streams is the API you

00:13:39,350 --> 00:13:42,830
have machine learning capabilities ml

00:13:41,720 --> 00:13:44,780
Lib which is what I'm going to be

00:13:42,830 --> 00:13:47,990
digging into in this session and then

00:13:44,780 --> 00:13:49,910
you got graph capabilities and then it

00:13:47,990 --> 00:13:52,310
doesn't matter so companies have a

00:13:49,910 --> 00:13:54,380
variety of data stores so you don't

00:13:52,310 --> 00:13:55,610
really need to care about what data

00:13:54,380 --> 00:13:58,040
store you use you don't have to just use

00:13:55,610 --> 00:14:00,170
HDFS you can use a variety of data

00:13:58,040 --> 00:14:02,540
stores whether it's public you know on

00:14:00,170 --> 00:14:04,250
the cloud or it's on Prem that works

00:14:02,540 --> 00:14:06,410
with all of them together so you can

00:14:04,250 --> 00:14:08,360
pull a lot of jobs that I do I pull data

00:14:06,410 --> 00:14:10,070
from you know a relational data

00:14:08,360 --> 00:14:11,840
warehouse and then I'll supplement with

00:14:10,070 --> 00:14:14,330
just a file on a distributed file system

00:14:11,840 --> 00:14:16,100
or or I can put it in s3 or something

00:14:14,330 --> 00:14:17,930
and then put them together and do some

00:14:16,100 --> 00:14:21,320
analysis on the fly so it's quite

00:14:17,930 --> 00:14:23,950
powerful any questions on the

00:14:21,320 --> 00:14:23,950
architecture

00:14:27,990 --> 00:14:35,050
it goes into so spark has a cluster just

00:14:32,709 --> 00:14:38,800
like to do so you install spark on a

00:14:35,050 --> 00:14:41,440
cluster and it has tasks and executors I

00:14:38,800 --> 00:14:43,690
went on a bunch of nodes and so the data

00:14:41,440 --> 00:14:45,100
will be pulled into memory and if it

00:14:43,690 --> 00:14:48,779
doesn't have enough memory it'll spill

00:14:45,100 --> 00:14:48,779
over into some disk so it does something

00:14:50,519 --> 00:14:56,589
just for the scope of the drive

00:14:53,709 --> 00:14:59,199
application so when you initiate when

00:14:56,589 --> 00:15:01,060
you create a spark context it will no it

00:14:59,199 --> 00:15:03,160
will keep it will be aware of the

00:15:01,060 --> 00:15:05,350
application it's telling us what to do

00:15:03,160 --> 00:15:06,910
and that spark context but it makes

00:15:05,350 --> 00:15:07,990
queries which pulls the data from

00:15:06,910 --> 00:15:10,089
different sources that will create

00:15:07,990 --> 00:15:12,579
connectors into the sources pull that

00:15:10,089 --> 00:15:14,440
data into memory run the computations or

00:15:12,579 --> 00:15:17,290
iterate through it and then get back the

00:15:14,440 --> 00:15:18,610
result to that application then that's a

00:15:17,290 --> 00:15:28,029
really good question what size well

00:15:18,610 --> 00:15:29,470
medium or extra-large thank you and that

00:15:28,029 --> 00:15:32,680
so that's a really good question and

00:15:29,470 --> 00:15:35,230
then sells spark is it big part of the

00:15:32,680 --> 00:15:37,630
big data ecosystem but it's really more

00:15:35,230 --> 00:15:43,079
for me I see more application

00:15:37,630 --> 00:15:45,930
development paradigm any other questions

00:15:43,079 --> 00:15:51,069
could you think like that traditional

00:15:45,930 --> 00:16:03,670
yep man you don't want it it's extra

00:15:51,069 --> 00:16:07,540
large for us fat guys damn just kidding

00:16:03,670 --> 00:16:12,940
so now nobody got to speaking like I'm

00:16:07,540 --> 00:16:18,160
out there I'll just keep it myself it's

00:16:12,940 --> 00:16:19,690
just too hot Miami but yeah you can use

00:16:18,160 --> 00:16:21,880
any data source I mean there's

00:16:19,690 --> 00:16:25,060
connectors so some connectors are

00:16:21,880 --> 00:16:27,399
optimized so if you have a distributed

00:16:25,060 --> 00:16:30,220
data source for example s3 is

00:16:27,399 --> 00:16:33,699
distributed and the connector from spark

00:16:30,220 --> 00:16:36,010
test 3 can co-locate based on you know

00:16:33,699 --> 00:16:38,140
compute and processing thought well HDFS

00:16:36,010 --> 00:16:40,720
is like that I know our object stores

00:16:38,140 --> 00:16:42,610
like that so so some connectors

00:16:40,720 --> 00:16:44,649
are smart enough to know to put some

00:16:42,610 --> 00:16:46,660
computation with the location of where

00:16:44,649 --> 00:16:49,149
the data is persist if that makes if you

00:16:46,660 --> 00:16:50,709
understand what I mean but so it just

00:16:49,149 --> 00:16:52,750
depends on the connectors but some

00:16:50,709 --> 00:16:55,540
connectors - like relational data

00:16:52,750 --> 00:16:57,310
rousers will go through one node which

00:16:55,540 --> 00:17:00,310
means may have a bottleneck depending on

00:16:57,310 --> 00:17:02,199
the size your data but but Sparx really

00:17:00,310 --> 00:17:03,970
good for machine learning application

00:17:02,199 --> 00:17:06,789
that doesn't really mean you have to

00:17:03,970 --> 00:17:08,439
have big data and I always talk to some

00:17:06,789 --> 00:17:10,780
of the SPSS people and stuffing like aw

00:17:08,439 --> 00:17:12,579
man I got a million rows to deal with

00:17:10,780 --> 00:17:14,350
and my well million rows is not a

00:17:12,579 --> 00:17:15,610
big deal at all even just through if

00:17:14,350 --> 00:17:19,030
it's pulling through connected through

00:17:15,610 --> 00:17:20,919
one node so it's quite quite interesting

00:17:19,030 --> 00:17:22,959
it's very different than you

00:17:20,919 --> 00:17:29,380
traditionally think in and it's quite

00:17:22,959 --> 00:17:30,970
powerful good question the other thing

00:17:29,380 --> 00:17:33,760
that tonight works well with apache

00:17:30,970 --> 00:17:37,299
spark is the open source notebook so

00:17:33,760 --> 00:17:38,620
notebooks you know years ago people used

00:17:37,299 --> 00:17:40,179
to use pen and paper they'd have a

00:17:38,620 --> 00:17:42,400
thought a formulated write it down

00:17:40,179 --> 00:17:44,350
they'd write numbers down put the

00:17:42,400 --> 00:17:46,210
results now it's all done in a

00:17:44,350 --> 00:17:49,000
browser-based application so it's a way

00:17:46,210 --> 00:17:51,159
of basically writing a line of code to

00:17:49,000 --> 00:17:53,020
request some information and pulls it

00:17:51,159 --> 00:17:54,700
back you see it you visualize it then

00:17:53,020 --> 00:17:56,620
you can compute you know do more

00:17:54,700 --> 00:17:58,360
computations on that and kind of iterate

00:17:56,620 --> 00:18:00,669
in a notebook style where you're just

00:17:58,360 --> 00:18:04,030
thinking thought to computer back and

00:18:00,669 --> 00:18:06,850
forth and it's a really interesting way

00:18:04,030 --> 00:18:09,250
of data scientists working and I'm also

00:18:06,850 --> 00:18:10,659
very good for developers as well and

00:18:09,250 --> 00:18:15,059
I'll show you a demo what that looks

00:18:10,659 --> 00:18:18,400
like the spark spark supports multiple

00:18:15,059 --> 00:18:21,120
programming languages so you can you can

00:18:18,400 --> 00:18:26,679
inter interact with spark whether it's

00:18:21,120 --> 00:18:27,340
somebody in the light though I think you

00:18:26,679 --> 00:18:30,580
okay

00:18:27,340 --> 00:18:31,780
whether it's Scala you know scholars

00:18:30,580 --> 00:18:34,120
sparks written in Scala

00:18:31,780 --> 00:18:36,370
it's the scholar the advantage of

00:18:34,120 --> 00:18:38,230
scholar is that when you fir for

00:18:36,370 --> 00:18:39,970
everything new develop the spark

00:18:38,230 --> 00:18:42,370
it will come in Scala available Scala

00:18:39,970 --> 00:18:43,840
first sequel I don't know why that's

00:18:42,370 --> 00:18:46,390
there you should take that up a Python

00:18:43,840 --> 00:18:48,220
is it another language which is growing

00:18:46,390 --> 00:18:50,830
in usage if you look over here over here

00:18:48,220 --> 00:18:52,600
scholars going down R is new but it's

00:18:50,830 --> 00:18:53,320
starting to grow and then Java is

00:18:52,600 --> 00:18:57,100
decreasing

00:18:53,320 --> 00:18:59,169
the harder one more lines of code so so

00:18:57,100 --> 00:19:01,210
what this is telling us is that if you

00:18:59,169 --> 00:19:03,580
look at the usage it's around the growth

00:19:01,210 --> 00:19:05,649
is around Python and R and this is where

00:19:03,580 --> 00:19:10,380
the data science workload or data

00:19:05,649 --> 00:19:14,110
science use cases is coming to spark and

00:19:10,380 --> 00:19:16,809
then the spark library usage so data

00:19:14,110 --> 00:19:19,539
frames is taken off like crazy and

00:19:16,809 --> 00:19:21,880
that's again ideal for that data science

00:19:19,539 --> 00:19:23,889
workload and then you got sequel

00:19:21,880 --> 00:19:25,419
streaming so they're all on the increase

00:19:23,889 --> 00:19:28,029
and then machine learning is also of

00:19:25,419 --> 00:19:31,720
increasing as well and this is the

00:19:28,029 --> 00:19:34,750
survey it was done to see what api's are

00:19:31,720 --> 00:19:37,000
being used in production I expect next

00:19:34,750 --> 00:19:42,399
year you'll see machine learning to grow

00:19:37,000 --> 00:19:44,679
more so IBM is betting big on that spark

00:19:42,399 --> 00:19:47,559
so we've invested in the spark

00:19:44,679 --> 00:19:50,320
Technology Center which we put I think

00:19:47,559 --> 00:19:52,659
about 100 engineers there to do nothing

00:19:50,320 --> 00:19:56,950
but contributing to a patchy spark we

00:19:52,659 --> 00:19:59,620
also use spark in our portfolio so not

00:19:56,950 --> 00:20:00,700
only are we contributing to spark you

00:19:59,620 --> 00:20:03,279
know we don't just do that because we're

00:20:00,700 --> 00:20:06,370
having you know weekly like spark we're

00:20:03,279 --> 00:20:07,509
doing it because we are setting a lot of

00:20:06,370 --> 00:20:09,549
support we're putting a lot of sparking

00:20:07,509 --> 00:20:12,100
out tools in our applications Watson

00:20:09,549 --> 00:20:14,830
uses spark are some are health

00:20:12,100 --> 00:20:16,509
applications uses spark so even our

00:20:14,830 --> 00:20:18,909
security applications you spark

00:20:16,509 --> 00:20:20,080
undercover so we're using spark a lot so

00:20:18,909 --> 00:20:21,669
we're contributing quite a bit

00:20:20,080 --> 00:20:24,730
everything we do we put out into the

00:20:21,669 --> 00:20:26,230
open source community and we also help

00:20:24,730 --> 00:20:27,730
our clients use spark if they want to

00:20:26,230 --> 00:20:29,889
just you know use spark themselves and

00:20:27,730 --> 00:20:32,320
when we correct it we created a spark

00:20:29,889 --> 00:20:35,110
Technology Center Ben Horowitz who's the

00:20:32,320 --> 00:20:36,580
big VC guy said it was like spark just

00:20:35,110 --> 00:20:39,639
got blessed by the enterprise rabbi is

00:20:36,580 --> 00:20:41,740
pretty funny but and so SDC continues to

00:20:39,639 --> 00:20:44,470
grow and it's just really show that

00:20:41,740 --> 00:20:46,539
we're committed if you look at our

00:20:44,470 --> 00:20:48,789
contributions to apache spark we're

00:20:46,539 --> 00:20:51,429
really contributing heavily around

00:20:48,789 --> 00:20:53,409
machine learning spark sequel I mean

00:20:51,429 --> 00:20:55,419
we've created spark sequel we have a lot

00:20:53,409 --> 00:20:57,519
of mindshare out there so we've been

00:20:55,419 --> 00:20:59,250
growing spark sequel machine learning in

00:20:57,519 --> 00:21:02,049
pi spark which is the Python

00:20:59,250 --> 00:21:05,220
incorporating Python capabilities within

00:21:02,049 --> 00:21:06,690
to spark at the Tribune distributed so

00:21:05,220 --> 00:21:10,440
those are the areas we focused in on in

00:21:06,690 --> 00:21:13,890
ours becoming another hot area that

00:21:10,440 --> 00:21:15,419
we're doing a lot of contributions and

00:21:13,890 --> 00:21:17,370
you know data bricks the founding

00:21:15,419 --> 00:21:20,220
company they have most of the committers

00:21:17,370 --> 00:21:22,230
there so they have you know Mattei who's

00:21:20,220 --> 00:21:23,970
kind of the guy who invented spark a

00:21:22,230 --> 00:21:25,799
tape labs which we support a plug so

00:21:23,970 --> 00:21:27,990
they do a lot of contributions you know

00:21:25,799 --> 00:21:30,750
you're not going to beat those guys but

00:21:27,990 --> 00:21:32,909
I actually got lucky ones I have a

00:21:30,750 --> 00:21:36,030
selfie with Matteo I was on I land on

00:21:32,909 --> 00:21:38,010
the flight and I was flying from Boston

00:21:36,030 --> 00:21:39,539
to Tampa I live in Tampa and then all of

00:21:38,010 --> 00:21:41,549
sudden this guy comes down and scheme

00:21:39,539 --> 00:21:43,679
guy sits right next to me and I'm like

00:21:41,549 --> 00:21:46,080
man he looks familiar and I'm thinking

00:21:43,679 --> 00:21:48,000
I'm like I started talking to him and I

00:21:46,080 --> 00:21:50,850
go he goes I am in distributed country

00:21:48,000 --> 00:21:52,470
on compute data science all stuff and he

00:21:50,850 --> 00:21:54,120
goes I'm like data breaks he goes yeah

00:21:52,470 --> 00:21:56,190
I'm it a it's like he's a founder

00:21:54,120 --> 00:21:58,289
creator spark is it felt like a groupie

00:21:56,190 --> 00:22:00,419
like I'm like I'm doing nothing but

00:21:58,289 --> 00:22:02,940
learning spark lately so it's kind of

00:22:00,419 --> 00:22:04,799
funny but he's a nice guy and data

00:22:02,940 --> 00:22:07,610
bricks is a partner with IBM so we work

00:22:04,799 --> 00:22:10,620
with them on a lot of projects

00:22:07,610 --> 00:22:12,890
particularly a GPU acceleration or R&D

00:22:10,620 --> 00:22:15,539
we've done quite a bit with those guys

00:22:12,890 --> 00:22:17,429
so we're number two in contributions

00:22:15,539 --> 00:22:19,140
behind data bricks but then if you look

00:22:17,429 --> 00:22:20,940
at all the rest I mean we have more

00:22:19,140 --> 00:22:23,340
contributions in the next five companies

00:22:20,940 --> 00:22:24,809
and this actually these five six

00:22:23,340 --> 00:22:27,059
companies here one two three four five

00:22:24,809 --> 00:22:30,179
six seven companies make up 70% of all

00:22:27,059 --> 00:22:34,950
the contributions to Apache part -

00:22:30,179 --> 00:22:37,260
righto so in the other thing I'll make

00:22:34,950 --> 00:22:38,730
note is you know we're heavily invested

00:22:37,260 --> 00:22:40,169
in machine learning we have been doing

00:22:38,730 --> 00:22:43,070
machine on quite a while and so we're

00:22:40,169 --> 00:22:47,630
contributing a lot in that space and

00:22:43,070 --> 00:22:50,760
sequel and our and as well as Python so

00:22:47,630 --> 00:22:52,260
spark has been except it's accelerating

00:22:50,760 --> 00:22:54,870
machine learning and here's some of the

00:22:52,260 --> 00:22:58,409
reasons why you know spark is easy you

00:22:54,870 --> 00:23:00,169
know you can code solutions much faster

00:22:58,409 --> 00:23:02,460
with spark just less lines of code

00:23:00,169 --> 00:23:04,049
there's multiple programming languages

00:23:02,460 --> 00:23:05,820
so you can have a Python person get to

00:23:04,049 --> 00:23:08,730
speak quite quickly you can also get

00:23:05,820 --> 00:23:11,130
Java people to speed quickly sparks also

00:23:08,730 --> 00:23:13,409
agile and I look at a typo in there

00:23:11,130 --> 00:23:16,919
quickly you can build pipelines quickly

00:23:13,409 --> 00:23:18,780
and it's got a unified API library that

00:23:16,919 --> 00:23:21,180
is accessible

00:23:18,780 --> 00:23:24,510
and easy-to-use and sports notebooks

00:23:21,180 --> 00:23:26,700
which make it even more usable and then

00:23:24,510 --> 00:23:29,280
it sparks really fast you know you can

00:23:26,700 --> 00:23:32,000
iterate and train models quicker

00:23:29,280 --> 00:23:35,160
I should have proofread this and then

00:23:32,000 --> 00:23:36,840
in-memory processing that scales you

00:23:35,160 --> 00:23:39,360
know it's just it's much faster that

00:23:36,840 --> 00:23:43,620
lazy evaluation I talked about optimize

00:23:39,360 --> 00:23:45,240
compute so there's been I'm telling you

00:23:43,620 --> 00:23:46,800
the market right now around machine

00:23:45,240 --> 00:23:49,290
learning if you're a developer and you

00:23:46,800 --> 00:23:51,090
want to get into data science space

00:23:49,290 --> 00:23:53,040
start to play with machine learning

00:23:51,090 --> 00:23:54,540
because it's just everybody's looking at

00:23:53,040 --> 00:23:57,270
machine learning right now so what is

00:23:54,540 --> 00:23:59,790
machine learning you know I was a

00:23:57,270 --> 00:24:02,520
programmer so for me I programmed

00:23:59,790 --> 00:24:05,010
everything explicitly so machine

00:24:02,520 --> 00:24:07,320
learning is really you know the use of

00:24:05,010 --> 00:24:09,240
data to where you have computers acting

00:24:07,320 --> 00:24:10,530
without things being explicitly

00:24:09,240 --> 00:24:13,680
programmed you're just using a formula

00:24:10,530 --> 00:24:16,110
so it's really just a you know you guys

00:24:13,680 --> 00:24:18,060
probably know this but sometimes it's

00:24:16,110 --> 00:24:20,370
good to talk about so people understand

00:24:18,060 --> 00:24:21,840
it and then the process to create a

00:24:20,370 --> 00:24:24,720
machine learning model this is kind of

00:24:21,840 --> 00:24:26,430
the standard basic way to do it but you

00:24:24,720 --> 00:24:28,740
know it's all about you bring your data

00:24:26,430 --> 00:24:31,530
in you cleanse and transform your data

00:24:28,740 --> 00:24:34,410
then you train a model you pilled you

00:24:31,530 --> 00:24:37,020
build out some of that data as a train

00:24:34,410 --> 00:24:38,670
you have some of that data for test and

00:24:37,020 --> 00:24:40,320
cross validation to get an error rate

00:24:38,670 --> 00:24:42,540
and then you run it against the

00:24:40,320 --> 00:24:43,980
algorithm you train that data but for

00:24:42,540 --> 00:24:46,140
predictor fields so you're really trying

00:24:43,980 --> 00:24:48,660
to default you can defy one field that

00:24:46,140 --> 00:24:50,400
says hey here's all my data here's one

00:24:48,660 --> 00:24:53,310
field I'm trying to see what algorithm

00:24:50,400 --> 00:24:55,650
how accurate it is to predict it so it

00:24:53,310 --> 00:24:57,660
trains to get that algorithm against 80%

00:24:55,650 --> 00:25:00,360
of your data and then 10% of your data

00:24:57,660 --> 00:25:02,700
you can pick any amount 70% 6 . but you

00:25:00,360 --> 00:25:04,610
train your data against there and you

00:25:02,700 --> 00:25:06,720
try to build that model and then you

00:25:04,610 --> 00:25:09,900
validate and test it and get an error

00:25:06,720 --> 00:25:11,160
rate so so it's quite basic model and

00:25:09,900 --> 00:25:12,770
among the reason going through this is

00:25:11,160 --> 00:25:15,810
because we've been putting together

00:25:12,770 --> 00:25:18,150
application development interfaces to

00:25:15,810 --> 00:25:20,640
let people do this simply so that they

00:25:18,150 --> 00:25:22,530
can quickly embrace machine learning and

00:25:20,640 --> 00:25:27,090
build an endpoint that they can quickly

00:25:22,530 --> 00:25:28,800
deploy which I'll show you guys so the

00:25:27,090 --> 00:25:30,810
data scientists while it's a sexiest job

00:25:28,800 --> 00:25:32,760
it's also one of the toughest jobs right

00:25:30,810 --> 00:25:34,140
now I've known a lot of guys get higher

00:25:32,760 --> 00:25:36,390
quickly quit and go to another place

00:25:34,140 --> 00:25:38,970
because it's just a lot of the problem

00:25:36,390 --> 00:25:41,010
is more than just the data scientists so

00:25:38,970 --> 00:25:42,299
the tool sets you know there's so many

00:25:41,010 --> 00:25:44,880
tool sets out there so it's really

00:25:42,299 --> 00:25:46,830
difficult for them to be successful and

00:25:44,880 --> 00:25:48,570
their approach is really limited by the

00:25:46,830 --> 00:25:51,900
tools that the companies kind of

00:25:48,570 --> 00:25:53,970
embraced and use it and which has been a

00:25:51,900 --> 00:25:57,929
challenge you have fragmented and

00:25:53,970 --> 00:25:59,610
time-consuming a disjointed environment

00:25:57,929 --> 00:26:01,679
so everything kind of takes forever they

00:25:59,610 --> 00:26:03,830
get your stuff together and then your

00:26:01,679 --> 00:26:05,490
analytic silos which you know you have

00:26:03,830 --> 00:26:07,110
different organizations they have

00:26:05,490 --> 00:26:09,000
different data sets and it's hard to

00:26:07,110 --> 00:26:12,330
bring that together so data scientist

00:26:09,000 --> 00:26:14,940
has a tough job so here comes the sales

00:26:12,330 --> 00:26:16,620
part so so our mission is really to make

00:26:14,940 --> 00:26:18,120
data and analytics simple and accessible

00:26:16,620 --> 00:26:20,610
to all so that we've built this thing

00:26:18,120 --> 00:26:22,290
called a Watson data platform and in

00:26:20,610 --> 00:26:24,570
there we built a data science experience

00:26:22,290 --> 00:26:27,210
which I'm going to show you a demo of

00:26:24,570 --> 00:26:30,600
and data science experience is really a

00:26:27,210 --> 00:26:32,760
collaborative space that lets you learn

00:26:30,600 --> 00:26:34,919
about data science particular algorithms

00:26:32,760 --> 00:26:38,330
python libraries you can create

00:26:34,919 --> 00:26:40,890
different analytical assets or different

00:26:38,330 --> 00:26:42,660
analysis and then you can collaborate

00:26:40,890 --> 00:26:46,559
with other data scientists so it's a

00:26:42,660 --> 00:26:48,750
pretty cool environment and what it does

00:26:46,559 --> 00:26:52,020
is it really brings together three

00:26:48,750 --> 00:26:54,299
things community which is the place for

00:26:52,020 --> 00:26:57,390
tutorials datasets and different things

00:26:54,299 --> 00:26:58,620
about for education open source we bring

00:26:57,390 --> 00:27:02,700
together you know spark

00:26:58,620 --> 00:27:06,720
jupiter notebooks python r we have our

00:27:02,700 --> 00:27:10,890
shiny in our studio and then we added

00:27:06,720 --> 00:27:12,240
some of our IBM products capabilities

00:27:10,890 --> 00:27:14,910
that we've done over the years and

00:27:12,240 --> 00:27:17,040
things like data shaping pipe and UI to

00:27:14,910 --> 00:27:20,880
simplify the process

00:27:17,040 --> 00:27:22,500
auto auto data preparation and advanced

00:27:20,880 --> 00:27:25,950
visualization so we kind of pull that

00:27:22,500 --> 00:27:28,530
all together into one platform and what

00:27:25,950 --> 00:27:31,260
this is allowed that what it allows us

00:27:28,530 --> 00:27:33,510
to do and kind of my pitch to you guys

00:27:31,260 --> 00:27:35,880
that if you're a developer you really

00:27:33,510 --> 00:27:37,410
can become more more embedded in that

00:27:35,880 --> 00:27:39,990
team with the data scientist where you

00:27:37,410 --> 00:27:42,720
can actually argue supplement them and

00:27:39,990 --> 00:27:44,490
do some capabilities in deploy the

00:27:42,720 --> 00:27:46,260
mountain's applications without the

00:27:44,490 --> 00:27:47,790
truly need for that data scientist

00:27:46,260 --> 00:27:51,059
you could be kind of become the data

00:27:47,790 --> 00:27:52,679
scientist but I'm not saying you know I

00:27:51,059 --> 00:27:54,120
have a lot of clients yell at me if I

00:27:52,679 --> 00:27:55,679
say you know we can automate the bids

00:27:54,120 --> 00:27:56,940
new application developers can automate

00:27:55,679 --> 00:27:59,870
data scientists and everything no you

00:27:56,940 --> 00:28:03,780
can so it depends on how you view it I

00:27:59,870 --> 00:28:05,549
think really once you begin to learn how

00:28:03,780 --> 00:28:08,580
to do some stuff your R&D scientist so

00:28:05,549 --> 00:28:10,020
it's kind of like back in the day when I

00:28:08,580 --> 00:28:11,220
don't know if any of your old enough but

00:28:10,020 --> 00:28:14,100
remember back in the day when everybody

00:28:11,220 --> 00:28:16,080
was webmaster remember everybody was a

00:28:14,100 --> 00:28:19,080
webmaster now everybody defenses so

00:28:16,080 --> 00:28:22,919
you're a decent any questions I'm going

00:28:19,080 --> 00:28:23,700
to do a quick demo or when you guys got

00:28:22,919 --> 00:28:33,690
thoughts

00:28:23,700 --> 00:28:36,510
I got a hoodie here you know our pricing

00:28:33,690 --> 00:28:38,490
for the tool no it's just so we have a

00:28:36,510 --> 00:28:40,230
couple ways that we price it so for

00:28:38,490 --> 00:28:42,809
companies we have Enterprise license or

00:28:40,230 --> 00:28:44,880
you have five seats and so much storage

00:28:42,809 --> 00:28:47,280
is on the cloud fully managed you have a

00:28:44,880 --> 00:28:49,140
30 node spark cluster and you just pay

00:28:47,280 --> 00:28:51,270
so much per month and then we also have

00:28:49,140 --> 00:28:52,470
individual plans right now it's free so

00:28:51,270 --> 00:28:54,750
data size experience go out there is

00:28:52,470 --> 00:28:56,790
free you get like a little bit of

00:28:54,750 --> 00:28:58,410
storage and you can try it out and we're

00:28:56,790 --> 00:29:00,510
working on a freemium model and then if

00:28:58,410 --> 00:29:03,030
you want to like add more computation

00:29:00,510 --> 00:29:04,020
and more storage you kind of just kind

00:29:03,030 --> 00:29:06,720
of put a credit card in there and it

00:29:04,020 --> 00:29:09,270
kind of like consumption so we got a few

00:29:06,720 --> 00:29:11,880
models if our big clients we kind of

00:29:09,270 --> 00:29:13,679
just give it to them and say here pound

00:29:11,880 --> 00:29:15,809
that it and tell us what you think and

00:29:13,679 --> 00:29:17,160
so we can grow our product cuz a big

00:29:15,809 --> 00:29:19,650
part of what we're trying to do is make

00:29:17,160 --> 00:29:21,270
sure that we get people to use you know

00:29:19,650 --> 00:29:22,799
this open-source technologies and start

00:29:21,270 --> 00:29:24,750
to embrace it and put it into your

00:29:22,799 --> 00:29:26,970
applications so we're big you know I'm

00:29:24,750 --> 00:29:28,950
the global sales leader for this all

00:29:26,970 --> 00:29:32,549
right I work with OEM and Devon and

00:29:28,950 --> 00:29:34,799
drive in the dark go to market and right

00:29:32,549 --> 00:29:37,710
now we have quite a few clients using it

00:29:34,799 --> 00:29:41,070
but we just went to market with it late

00:29:37,710 --> 00:29:42,510
last year November and now we're just

00:29:41,070 --> 00:29:44,309
end of this month we're releasing our

00:29:42,510 --> 00:29:46,020
Watson machine learning which is what

00:29:44,309 --> 00:29:48,210
I'm going to demo for you and some other

00:29:46,020 --> 00:29:50,640
capabilities so we're expecting that Q

00:29:48,210 --> 00:29:52,380
two and three we're going to hit quite a

00:29:50,640 --> 00:29:55,020
few deals that clients are going to

00:29:52,380 --> 00:29:57,360
right now I gotta I think I got my 40

00:29:55,020 --> 00:30:00,310
customers on it so it's grown pretty

00:29:57,360 --> 00:30:03,020
quick let me show you

00:30:00,310 --> 00:30:07,010
so each of you go to data science

00:30:03,020 --> 00:30:10,720
experience calm or I know it's just it's

00:30:07,010 --> 00:30:14,930
just a sorry

00:30:10,720 --> 00:30:16,430
data science Tybee em calm and you'll go

00:30:14,930 --> 00:30:18,170
right here to this landing page and you

00:30:16,430 --> 00:30:19,550
just do sign up and you get a 30 day

00:30:18,170 --> 00:30:22,160
trial right now and eventually that's

00:30:19,550 --> 00:30:25,130
going to stay free for education and

00:30:22,160 --> 00:30:27,860
just usage and then as you do more

00:30:25,130 --> 00:30:30,650
enterprise or larger volume projects it

00:30:27,860 --> 00:30:33,130
will cost you something but we just sign

00:30:30,650 --> 00:30:37,100
in so it's a fully managed environment

00:30:33,130 --> 00:30:39,560
and when you land into dfx you'll see

00:30:37,100 --> 00:30:41,420
you go to community page and here you

00:30:39,560 --> 00:30:44,330
can search on articles like I wrote a

00:30:41,420 --> 00:30:46,670
few labs on spark so if you want to

00:30:44,330 --> 00:30:48,680
learn about Apache spark you got a batch

00:30:46,670 --> 00:30:50,750
spark lab one in here you can quickly

00:30:48,680 --> 00:30:53,360
create with using PI spark create some

00:30:50,750 --> 00:30:55,910
rdd's some quick transformations and

00:30:53,360 --> 00:30:58,490
actions it'll teach you about spark but

00:30:55,910 --> 00:31:01,520
there's also lab two right here which is

00:30:58,490 --> 00:31:04,120
query so you spark sequel and lab 3 is

00:31:01,520 --> 00:31:06,290
all about machine learning so I

00:31:04,120 --> 00:31:08,570
recommend you go in there and in those

00:31:06,290 --> 00:31:10,670
labs if you look at them this list

00:31:08,570 --> 00:31:12,140
you'll see it has a little bit of a

00:31:10,670 --> 00:31:16,280
description but also has a notebook

00:31:12,140 --> 00:31:19,160
which has all of this description which

00:31:16,280 --> 00:31:21,830
as well as code in here to show you how

00:31:19,160 --> 00:31:24,500
to actually code with spark so it's a

00:31:21,830 --> 00:31:26,930
great tutorial and for some of our

00:31:24,500 --> 00:31:28,340
clients I'll do them on site too but you

00:31:26,930 --> 00:31:32,840
can even really need to do it on your

00:31:28,340 --> 00:31:36,440
own so let me go back and you can search

00:31:32,840 --> 00:31:42,100
up different data sets so do you have a

00:31:36,440 --> 00:31:42,100
question did you have a question

00:31:55,380 --> 00:32:38,350
it's very stable to do daily a lot of

00:31:58,179 --> 00:32:55,809
people support in production so yeah

00:32:38,350 --> 00:32:58,059
yeah yeah so we have yeah we have the

00:32:55,809 --> 00:33:00,549
capability to set so the question was

00:32:58,059 --> 00:33:02,980
how do you refresh your models so that

00:33:00,549 --> 00:33:04,960
changing data refreshes to it so we have

00:33:02,980 --> 00:33:07,960
intervals that we define where you can

00:33:04,960 --> 00:33:09,940
have it retrain every day or every

00:33:07,960 --> 00:33:12,010
couple hours and we're also working on

00:33:09,940 --> 00:33:14,140
capability and this is part of the value

00:33:12,010 --> 00:33:15,669
out of IBM you know today with SPARC

00:33:14,140 --> 00:33:17,620
you'd have to create a spark submit job

00:33:15,669 --> 00:33:19,659
and you have to rerun it and every day

00:33:17,620 --> 00:33:21,429
and schedule that as a cron or something

00:33:19,659 --> 00:33:25,240
we kind of built some applications

00:33:21,429 --> 00:33:27,520
around that but you also have the

00:33:25,240 --> 00:33:29,770
capable working on capability so as data

00:33:27,520 --> 00:33:31,960
goes through this in real time it could

00:33:29,770 --> 00:33:34,150
continue to score and improve and as

00:33:31,960 --> 00:33:35,890
well define some alerts that says if it

00:33:34,150 --> 00:33:39,370
goes out of a certain error rate once we

00:33:35,890 --> 00:33:41,080
have a feedback loop which says hey you

00:33:39,370 --> 00:33:43,090
know we predicted this but then it ended

00:33:41,080 --> 00:33:44,679
up being say this value when we get that

00:33:43,090 --> 00:33:46,450
feedback loop we're going to monitor the

00:33:44,679 --> 00:33:48,640
error rate and if it goes out of some

00:33:46,450 --> 00:33:50,830
type of KPI then we'll send an alert

00:33:48,640 --> 00:33:53,830
because this is all about our model

00:33:50,830 --> 00:33:55,390
management of strategy because companies

00:33:53,830 --> 00:33:56,919
are building models and they're

00:33:55,390 --> 00:33:59,679
deploying these endpoints on the

00:33:56,919 --> 00:34:02,110
platform and then applications are just

00:33:59,679 --> 00:34:04,210
calling it and so so we got to keep up

00:34:02,110 --> 00:34:05,350
with that in monitor them and so we're

00:34:04,210 --> 00:34:07,270
working on a whole strategy around that

00:34:05,350 --> 00:34:08,590
and that's all coming up this year

00:34:07,270 --> 00:34:13,179
and I'll show you some of that

00:34:08,590 --> 00:34:18,580
capability you want the city it's all

00:34:13,179 --> 00:34:21,750
extra-large though too big for you all

00:34:18,580 --> 00:34:23,260
right did you want it you got a question

00:34:21,750 --> 00:34:32,619
all right

00:34:23,260 --> 00:34:34,600
what was your question oh so spark we

00:34:32,619 --> 00:34:37,720
are working with companies like General

00:34:34,600 --> 00:34:39,159
Motors where there have 500 petabytes of

00:34:37,720 --> 00:34:41,710
data and we're able to do some spark

00:34:39,159 --> 00:34:43,540
jobs so there's no it's pretty mature

00:34:41,710 --> 00:34:53,440
when it comes to dealing with large

00:34:43,540 --> 00:34:56,290
volumes of data yeah you can for sure

00:34:53,440 --> 00:34:58,960
that's why you got to design your spark

00:34:56,290 --> 00:35:01,210
cluster for the size of the data that's

00:34:58,960 --> 00:35:04,210
kind of going to be dealing with there

00:35:01,210 --> 00:35:06,490
is some so it's Auto so your cluster

00:35:04,210 --> 00:35:08,650
manager can manage some of that and

00:35:06,490 --> 00:35:11,650
spill some of the disk but you kind of

00:35:08,650 --> 00:35:14,650
you can't you can get memory errors if

00:35:11,650 --> 00:35:16,390
you blow up a small spark cluster we

00:35:14,650 --> 00:35:19,060
have some things we've built in there

00:35:16,390 --> 00:35:20,800
for optimization based on the data

00:35:19,060 --> 00:35:24,280
science experience so that people don't

00:35:20,800 --> 00:35:27,670
hit that and but but you have that's

00:35:24,280 --> 00:35:28,990
still part of it it's not as mature I

00:35:27,670 --> 00:35:30,730
guess is what you're saying they're like

00:35:28,990 --> 00:35:33,130
a relational data warehouse which won't

00:35:30,730 --> 00:35:35,590
let you do anything if you're going to

00:35:33,130 --> 00:35:37,810
blow it up is that what you mean yeah I

00:35:35,590 --> 00:35:40,060
agree with you there it's getting there

00:35:37,810 --> 00:35:40,780
the catalyst which is an optimizer built

00:35:40,060 --> 00:35:43,420
inside

00:35:40,780 --> 00:35:47,200
huffs Park allows data frames to be

00:35:43,420 --> 00:35:49,210
optimized and when you do Sparkle code

00:35:47,200 --> 00:35:51,700
code and spark you stick with data

00:35:49,210 --> 00:35:53,530
frames and that catalyst will continue

00:35:51,700 --> 00:35:55,300
to grow I think and you'll see it but

00:35:53,530 --> 00:35:57,970
it's not a database it's an application

00:35:55,300 --> 00:35:59,830
so just like in C if you code an

00:35:57,970 --> 00:36:02,520
application you could blow it out I mean

00:35:59,830 --> 00:36:07,020
it's like it's really still more of a

00:36:02,520 --> 00:36:07,020
application than it is a database that

00:36:07,050 --> 00:36:12,410
makes sense

00:36:09,770 --> 00:36:15,170
all right so here let me show you

00:36:12,410 --> 00:36:18,290
everything that we do is some reason

00:36:15,170 --> 00:36:19,550
this because of my resolution my bars

00:36:18,290 --> 00:36:21,619
are different but everything we do is

00:36:19,550 --> 00:36:24,770
under projects so if I go to view

00:36:21,619 --> 00:36:27,610
projects we've created a space that

00:36:24,770 --> 00:36:30,890
allows you to create a project and add

00:36:27,610 --> 00:36:33,440
collaborators so like I have a project

00:36:30,890 --> 00:36:37,030
here I call it focus 5 machine learning

00:36:33,440 --> 00:36:40,010
in five minutes but I've put in here a

00:36:37,030 --> 00:36:42,260
bunch of my you'll see once you're in

00:36:40,010 --> 00:36:43,910
the project is organized by analytic

00:36:42,260 --> 00:36:45,290
assets and this is kind of your

00:36:43,910 --> 00:36:48,800
notebooks which I'll show you what they

00:36:45,290 --> 00:36:51,410
look like models in flows which you're

00:36:48,800 --> 00:36:53,990
really SPSS flows data assets which are

00:36:51,410 --> 00:36:55,280
really data files or data connections

00:36:53,990 --> 00:36:58,450
that we've kind of organized together

00:36:55,280 --> 00:37:00,950
here bookmarks are really articles

00:36:58,450 --> 00:37:03,590
deployments these are endpoints that are

00:37:00,950 --> 00:37:07,480
built to of like the model that I built

00:37:03,590 --> 00:37:10,490
that I want to call through an API and

00:37:07,480 --> 00:37:12,230
collaborators here folks that I've given

00:37:10,490 --> 00:37:14,930
them access rights to my project I

00:37:12,230 --> 00:37:17,480
created it and I gave editors and viewer

00:37:14,930 --> 00:37:20,359
access and then settings and if you look

00:37:17,480 --> 00:37:23,600
here it's just some details around the

00:37:20,359 --> 00:37:25,609
project storage that's used associated

00:37:23,600 --> 00:37:27,680
services like I have a spark service on

00:37:25,609 --> 00:37:29,990
the background on our bluemix platform I

00:37:27,680 --> 00:37:32,030
use as well as our IBM Watson machine

00:37:29,990 --> 00:37:33,619
learning service and you can even define

00:37:32,030 --> 00:37:36,980
the service here if you want to like an

00:37:33,619 --> 00:37:40,520
AWS EMR spark service so you have some

00:37:36,980 --> 00:37:42,859
flexibility and then you have access

00:37:40,520 --> 00:37:46,160
tokens it's for security which I don't

00:37:42,859 --> 00:37:47,990
really deal with a lot no I got we got a

00:37:46,160 --> 00:37:50,450
whole team of digital security then you

00:37:47,990 --> 00:37:52,609
got github integration and I don't know

00:37:50,450 --> 00:37:55,100
what this is to be honest project scope

00:37:52,609 --> 00:37:57,800
but let me go show you something or

00:37:55,100 --> 00:38:03,410
hopefully animal guys it so so here I'm

00:37:57,800 --> 00:38:04,300
just going to show you you guys familiar

00:38:03,410 --> 00:38:06,740
with tensorflow

00:38:04,300 --> 00:38:09,140
try to think what notebook show you so

00:38:06,740 --> 00:38:12,500
here is a example of deep learning

00:38:09,140 --> 00:38:14,030
tensor flow so this is what a notebook

00:38:12,500 --> 00:38:16,970
looks like so when you open up a

00:38:14,030 --> 00:38:20,330
notebook you'll see it's a space where

00:38:16,970 --> 00:38:23,150
you can put some documentation as well

00:38:20,330 --> 00:38:23,710
as do have code themselves so right here

00:38:23,150 --> 00:38:26,470
I can

00:38:23,710 --> 00:38:32,349
create a co2 cell and let me show you

00:38:26,470 --> 00:38:37,180
what this looks like from scratch so if

00:38:32,349 --> 00:38:42,160
I add a notebook I can create a notebook

00:38:37,180 --> 00:38:46,119
and then here I could pick the language

00:38:42,160 --> 00:38:48,369
I can do Python our Scala or Python 3.5

00:38:46,119 --> 00:38:50,680
I could pick a spark version I'll pick

00:38:48,369 --> 00:38:53,200
my spark service and I can just create a

00:38:50,680 --> 00:38:55,240
space and now I've created a Jupiter

00:38:53,200 --> 00:38:57,609
notebook and in the background I have

00:38:55,240 --> 00:39:01,020
automatically created for Mia's instance

00:38:57,609 --> 00:39:03,849
for a spark cluster as well as some

00:39:01,020 --> 00:39:06,160
Python libraries that are installed and

00:39:03,849 --> 00:39:08,589
ready to go and if you can see the

00:39:06,160 --> 00:39:12,400
navigation is along the top here so if I

00:39:08,589 --> 00:39:14,980
go here I can say oh look I have a good

00:39:12,400 --> 00:39:17,829
load data files in I can also define

00:39:14,980 --> 00:39:18,790
connections but here is a data file and

00:39:17,829 --> 00:39:21,099
then I'm going to say I'm going to take

00:39:18,790 --> 00:39:23,530
this data file and I'm just going to

00:39:21,099 --> 00:39:25,569
ahead and create insert a spark session

00:39:23,530 --> 00:39:28,240
data frame and it'll create the code for

00:39:25,569 --> 00:39:30,640
me and then if I run this I've now taken

00:39:28,240 --> 00:39:32,619
that data file it's now running against

00:39:30,640 --> 00:39:34,000
spark cluster you see - right it'll come

00:39:32,619 --> 00:39:36,910
back a minute and it'll have a data

00:39:34,000 --> 00:39:39,069
frame available and you can see the job

00:39:36,910 --> 00:39:41,589
right here is run in spark job so it's

00:39:39,069 --> 00:39:43,510
kind of a cool interface and it allows

00:39:41,589 --> 00:39:45,819
you to it you know kind of iterate back

00:39:43,510 --> 00:39:48,549
with the spark cluster as well as your

00:39:45,819 --> 00:39:50,890
data and do some data signs it's just a

00:39:48,549 --> 00:39:53,349
Jupiter notebook but for those that know

00:39:50,890 --> 00:39:55,240
it but it kind of adds some simple

00:39:53,349 --> 00:39:58,660
easy-to-use capabilities the other thing

00:39:55,240 --> 00:40:01,450
that's nice is if you look here you'll

00:39:58,660 --> 00:40:03,339
see this is my notebook but in the

00:40:01,450 --> 00:40:06,069
environment so I have a spark

00:40:03,339 --> 00:40:08,920
environment running that's accessible so

00:40:06,069 --> 00:40:10,270
you know SC tab you'll see the spark

00:40:08,920 --> 00:40:12,099
context is already available I don't

00:40:10,270 --> 00:40:13,630
have to create it on this connected and

00:40:12,099 --> 00:40:16,119
then here I have all these Python

00:40:13,630 --> 00:40:19,329
libraries so I have you know sky can't

00:40:16,119 --> 00:40:21,369
learn I got mat mat pot Lib pandas

00:40:19,329 --> 00:40:24,130
they're all available I can even got pip

00:40:21,369 --> 00:40:26,530
so I could do a pip install and I could

00:40:24,130 --> 00:40:28,359
show you some for example like Google's

00:40:26,530 --> 00:40:30,910
temps are flow on one of my other

00:40:28,359 --> 00:40:32,890
notebooks I did a install Tesla flow and

00:40:30,910 --> 00:40:33,730
then I did a style transfer which I

00:40:32,890 --> 00:40:36,700
could show you what that looks like

00:40:33,730 --> 00:40:39,309
any questions yet so far

00:40:36,700 --> 00:40:41,710
no let me show you what the style

00:40:39,309 --> 00:40:43,349
transfer looks like they may familiar

00:40:41,710 --> 00:40:49,720
with style transfer intensive low

00:40:43,349 --> 00:40:51,400
alright cool this one's fun so here's a

00:40:49,720 --> 00:40:58,030
deep learning example using the tensor

00:40:51,400 --> 00:40:59,200
flow library and but really my sessions

00:40:58,030 --> 00:41:03,089
around machine learning I got to get to

00:40:59,200 --> 00:41:03,089
next for a lot of time on it ten minutes

00:41:03,420 --> 00:41:08,260
they're good the other thing you could

00:41:06,490 --> 00:41:10,299
do - I forgot to mention is you have the

00:41:08,260 --> 00:41:14,650
capability to share so I could share

00:41:10,299 --> 00:41:16,660
some of these you could also there's a

00:41:14,650 --> 00:41:18,819
scheduler which I have to go back to the

00:41:16,660 --> 00:41:21,160
one and you can create chat like

00:41:18,819 --> 00:41:22,990
capabilities here I can add comments so

00:41:21,160 --> 00:41:24,760
that while I'm sharing so for example

00:41:22,990 --> 00:41:27,010
you create a project I go in there I

00:41:24,760 --> 00:41:28,270
create a analysis as the day scientists

00:41:27,010 --> 00:41:30,520
say I'm trying to figure this out

00:41:28,270 --> 00:41:32,109
somebody else can look at and say you

00:41:30,520 --> 00:41:33,670
could say I need more data sets and

00:41:32,109 --> 00:41:35,319
they're like okay and so we can

00:41:33,670 --> 00:41:37,180
collaborate and share and create check

00:41:35,319 --> 00:41:39,069
points with the notebooks so it really

00:41:37,180 --> 00:41:42,400
creates it like an IDE for data

00:41:39,069 --> 00:41:44,290
scientists and developers but here it

00:41:42,400 --> 00:41:45,520
tends to flow don't ignore all the codes

00:41:44,290 --> 00:41:49,420
I'm going to put that as imports

00:41:45,520 --> 00:41:53,619
identity but basically it's using a deep

00:41:49,420 --> 00:42:01,559
learning algorithm to take two pixel

00:41:53,619 --> 00:42:05,319
images and it's saying okay using this

00:42:01,559 --> 00:42:07,030
deep learning algorithm take these two

00:42:05,319 --> 00:42:09,130
together and let's merge the styles so

00:42:07,030 --> 00:42:11,380
as you iterate you see ten iterations

00:42:09,130 --> 00:42:14,079
you start to see the new image that's

00:42:11,380 --> 00:42:15,970
created just using an algorithm after 20

00:42:14,079 --> 00:42:20,020
iterations it gets better and after 30

00:42:15,970 --> 00:42:22,089
and 40 and then 50 and then 60

00:42:20,020 --> 00:42:24,940
iterations you'll see and I have the

00:42:22,089 --> 00:42:27,160
Incredible Hulk with this style so I

00:42:24,940 --> 00:42:29,020
took this image with this style and made

00:42:27,160 --> 00:42:30,339
that and I you know a lot of days times

00:42:29,020 --> 00:42:31,809
it's nowadays they're doing really

00:42:30,339 --> 00:42:33,700
important work like this like take a

00:42:31,809 --> 00:42:36,839
picture of cat and make it look like an

00:42:33,700 --> 00:42:40,930
art deco and stuff but it's just kind of

00:42:36,839 --> 00:42:44,190
kind of a neat to use case here but let

00:42:40,930 --> 00:42:44,190
me show you a little more practical

00:42:46,040 --> 00:42:49,770
the other thing nice about defense

00:42:48,000 --> 00:42:52,410
experience it really creates a space for

00:42:49,770 --> 00:42:54,119
you to load data in create connections

00:42:52,410 --> 00:42:56,790
so it's kind of organized your data for

00:42:54,119 --> 00:42:58,349
you so I encourage all you guys to play

00:42:56,790 --> 00:43:02,069
with it and start to learn and do the

00:42:58,349 --> 00:43:03,540
tutorials but here's here's what I want

00:43:02,069 --> 00:43:08,250
to show you so under your analytic

00:43:03,540 --> 00:43:11,460
assets so if you were to code a machine

00:43:08,250 --> 00:43:13,829
learning model and deploy it you would

00:43:11,460 --> 00:43:15,809
have to you have to create quite a bit

00:43:13,829 --> 00:43:17,490
of spark code it's not a lot but you

00:43:15,809 --> 00:43:21,180
have to create some spark code and

00:43:17,490 --> 00:43:23,730
here's a notebook that does a prediction

00:43:21,180 --> 00:43:25,950
of outdoor equipment that we collected

00:43:23,730 --> 00:43:29,790
through an API you know of purchases on

00:43:25,950 --> 00:43:32,940
a website so you'll see here it kind of

00:43:29,790 --> 00:43:34,829
you know import PI spark and then you'll

00:43:32,940 --> 00:43:37,380
and I'm going to go quickly through this

00:43:34,829 --> 00:43:39,420
but you Kray here you can see the schema

00:43:37,380 --> 00:43:42,450
of the data it shows you the records you

00:43:39,420 --> 00:43:44,700
have product line gender age marital

00:43:42,450 --> 00:43:46,950
status and profession and based on that

00:43:44,700 --> 00:43:49,349
we want to do predictions so then you

00:43:46,950 --> 00:43:50,760
would create a ml model so we create a

00:43:49,349 --> 00:43:53,849
model of few lines of code

00:43:50,760 --> 00:43:56,010
we'd have to split that data into a T 18

00:43:53,849 --> 00:43:58,440
in two percent on the way they did that

00:43:56,010 --> 00:44:00,780
but so and then you would train that

00:43:58,440 --> 00:44:02,369
data and test and validate and you kind

00:44:00,780 --> 00:44:05,030
of run that against your spark ml

00:44:02,369 --> 00:44:08,099
libraries two right here PI spark ml and

00:44:05,030 --> 00:44:11,400
and then you can come down and you could

00:44:08,099 --> 00:44:12,990
print your schema and see here and you

00:44:11,400 --> 00:44:15,000
can get your prediction your accuracy

00:44:12,990 --> 00:44:16,890
and error rate so so it's more accurate

00:44:15,000 --> 00:44:19,380
than flipping a coin it's like 58

00:44:16,890 --> 00:44:21,210
percent accurate but it's not that great

00:44:19,380 --> 00:44:23,220
but anyway it's just a good way to shown

00:44:21,210 --> 00:44:25,859
how you can do that and then you have a

00:44:23,220 --> 00:44:28,500
way to with Watson machine learning you

00:44:25,859 --> 00:44:30,450
can create a deployable endpoint through

00:44:28,500 --> 00:44:32,339
our services so that you don't have to

00:44:30,450 --> 00:44:34,950
kind of redeploy the whole application

00:44:32,339 --> 00:44:36,839
yourself it simplifies it for you but

00:44:34,950 --> 00:44:39,450
let me show you how to do this simply

00:44:36,839 --> 00:44:42,240
with the GUI so we've created a way to

00:44:39,450 --> 00:44:44,670
do this with a GUI so here I can take

00:44:42,240 --> 00:44:46,740
and this is a sales predict so here it's

00:44:44,670 --> 00:44:48,540
part of the pipeline this is your pipe

00:44:46,740 --> 00:44:50,400
on the left and you could just pick some

00:44:48,540 --> 00:44:55,020
sales data you can drop it in here if

00:44:50,400 --> 00:44:57,809
you wanted to within the data you just

00:44:55,020 --> 00:44:59,279
drop files in it becomes accessible so

00:44:57,809 --> 00:45:01,529
here created data

00:44:59,279 --> 00:45:03,689
up it you'll see the same data I can

00:45:01,529 --> 00:45:06,419
pick a auto data transformer or I could

00:45:03,689 --> 00:45:07,679
just pick some canned Transformers you

00:45:06,419 --> 00:45:10,439
have quite a bit of them that you can

00:45:07,679 --> 00:45:13,109
play with then I could do train and way

00:45:10,439 --> 00:45:15,659
this works is you pick a predictor field

00:45:13,109 --> 00:45:16,979
I'm kind of not in edit mode but I'll

00:45:15,659 --> 00:45:18,989
just talk to it and then once you pick a

00:45:16,979 --> 00:45:21,809
particular field it gives you it kind of

00:45:18,989 --> 00:45:23,999
it tells you an approach this one says

00:45:21,809 --> 00:45:25,589
multi-class calcification you pick some

00:45:23,999 --> 00:45:27,630
estimators which really are nothing more

00:45:25,589 --> 00:45:29,219
than just the algorithms available and

00:45:27,630 --> 00:45:30,929
we're continuing to grow this this is

00:45:29,219 --> 00:45:33,959
right now in beta and it's using

00:45:30,929 --> 00:45:35,729
Allspark ml and then here at the bottom

00:45:33,959 --> 00:45:39,269
you'll see you'll say I want to set a

00:45:35,729 --> 00:45:40,650
60% trained 20% test went and holdout so

00:45:39,269 --> 00:45:43,289
it's you don't have to kind of code that

00:45:40,650 --> 00:45:45,119
and then you can go to evaluate and it

00:45:43,289 --> 00:45:47,189
will bring you back a little train you

00:45:45,119 --> 00:45:49,829
run it to come in it it'll come back and

00:45:47,189 --> 00:45:51,659
give you the results of them and though

00:45:49,829 --> 00:45:53,640
it says poor but again you get the same

00:45:51,659 --> 00:45:56,339
error rate it was the same data you'll

00:45:53,640 --> 00:45:58,469
see that that's that's the best part

00:45:56,339 --> 00:46:01,130
once you get to where you're happy you

00:45:58,469 --> 00:46:03,449
click Save and I have one saved here

00:46:01,130 --> 00:46:07,439
trying to go fast so once it's saved

00:46:03,449 --> 00:46:09,689
you'll see it says oh look I have a IBM

00:46:07,439 --> 00:46:12,659
Watson she learned service product line

00:46:09,689 --> 00:46:15,809
I have a random forest with SPARC ml

00:46:12,659 --> 00:46:17,669
classification algorithm available and

00:46:15,809 --> 00:46:19,529
then I can do a one-click deployment

00:46:17,669 --> 00:46:21,869
kind of very simple just I want to do

00:46:19,529 --> 00:46:26,309
online deployment and what it does is it

00:46:21,869 --> 00:46:28,199
will create you a REST API that you can

00:46:26,309 --> 00:46:29,969
access with your applications so once

00:46:28,199 --> 00:46:31,979
you have that you have this scored end

00:46:29,969 --> 00:46:34,279
point that you can college applications

00:46:31,979 --> 00:46:37,559
and it can do predictions for you and

00:46:34,279 --> 00:46:39,689
it's quite it's quite powerful and easy

00:46:37,559 --> 00:46:41,789
to use and then if you look here you

00:46:39,689 --> 00:46:49,140
just do a simple test so I'll say let's

00:46:41,789 --> 00:46:52,559
say a female age 30 , import puts single

00:46:49,140 --> 00:46:56,579
as professional I run the against the

00:46:52,559 --> 00:46:58,169
predictor and it'll say 68% likely to

00:46:56,579 --> 00:47:00,630
buy personal accessories and like okay

00:46:58,169 --> 00:47:04,289
cool so let's say if she's married right

00:47:00,630 --> 00:47:06,089
when she's differently predict and it's

00:47:04,289 --> 00:47:08,459
less likely to do personal accessories

00:47:06,089 --> 00:47:11,519
and buy other stuff so so it really is a

00:47:08,459 --> 00:47:12,869
cool way for you to just take data take

00:47:11,519 --> 00:47:17,509
it in here now this is a beta

00:47:12,869 --> 00:47:17,509
coming out here in a few weeks yes oh

00:47:21,380 --> 00:47:28,950
right yeah we're right now we're using

00:47:25,710 --> 00:47:30,569
all the spark FML algorithms but we're

00:47:28,950 --> 00:47:32,130
working there's some projects out there

00:47:30,569 --> 00:47:34,140
that we can plug in here that we can

00:47:32,130 --> 00:47:36,720
pull your own algorithms in but most

00:47:34,140 --> 00:47:40,920
people use use the canned ones right now

00:47:36,720 --> 00:47:43,499
though yeah it would be great back at

00:47:40,920 --> 00:47:45,839
the AMIA for sure and I'll check because

00:47:43,499 --> 00:47:47,999
I know that we're we're adding

00:47:45,839 --> 00:47:50,609
algorithms to spark ml we're also

00:47:47,999 --> 00:47:52,650
including spss algorithms that we have

00:47:50,609 --> 00:47:55,410
available to us in here that are well

00:47:52,650 --> 00:47:57,950
known and usable so we're continuing to

00:47:55,410 --> 00:48:00,480
expand the algorithms that are available

00:47:57,950 --> 00:48:02,759
and then you can just see right here I

00:48:00,480 --> 00:48:05,910
actually have a GUI so this is just a

00:48:02,759 --> 00:48:08,670
dashboard shows that here's the service

00:48:05,910 --> 00:48:10,619
and you know he kind of just showed some

00:48:08,670 --> 00:48:16,980
customers and generate predictions and

00:48:10,619 --> 00:48:24,869
for Alexander oh I got a selective

00:48:16,980 --> 00:48:27,779
deployment Alexander who's a male 36

00:48:24,869 --> 00:48:30,150
single hit by camping equipment this is

00:48:27,779 --> 00:48:32,759
all kits really for an outdoor store is

00:48:30,150 --> 00:48:34,799
what the data was not a great use case

00:48:32,759 --> 00:48:38,279
but it's kind of cool but this is just a

00:48:34,799 --> 00:48:45,029
simple nodejs application so with my

00:48:38,279 --> 00:48:47,630
last minute any questions no more I'm

00:48:45,029 --> 00:48:49,739
out of hoodies I got three in my room

00:48:47,630 --> 00:48:53,940
I'll bring him down I should have

00:48:49,739 --> 00:48:55,559
brought him down I gotta get I got a lot

00:48:53,940 --> 00:48:57,420
to see Cisco Thursday and I'm doing a

00:48:55,559 --> 00:48:58,680
briefing in San Francisco then I gotta

00:48:57,420 --> 00:49:00,749
fly all the way back home to Florida

00:48:58,680 --> 00:49:03,380
temp on my wife's birthday Saturday just

00:49:00,749 --> 00:49:03,380
can't kill me

00:49:07,960 --> 00:49:15,030
yep absolutely

00:49:17,490 --> 00:49:27,790
yes well you can do there's a couple

00:49:20,110 --> 00:49:30,370
ways most people share notebooks like I

00:49:27,790 --> 00:49:34,570
have on github a ton of notebooks but

00:49:30,370 --> 00:49:37,770
you can one you can download them right

00:49:34,570 --> 00:49:40,600
here into I pipe Python notebooks and

00:49:37,770 --> 00:49:42,640
you just hit the download button and if

00:49:40,600 --> 00:49:46,150
you'll see it'll save it as a standard

00:49:42,640 --> 00:49:48,340
it's just I py and B file which then you

00:49:46,150 --> 00:49:50,290
can import into any Jupiter notebook so

00:49:48,340 --> 00:49:54,160
it's all the open source stuff and you

00:49:50,290 --> 00:50:01,510
can also share it as a just a URL right

00:49:54,160 --> 00:50:04,840
here that people can look at and now you

00:50:01,510 --> 00:50:07,660
have to take it from HTML to PDF but you

00:50:04,840 --> 00:50:09,640
get it you can send an HTML the other

00:50:07,660 --> 00:50:12,010
thing I forgot to show you there's a lot

00:50:09,640 --> 00:50:15,010
of cool visualizations I've got that I

00:50:12,010 --> 00:50:18,070
promised David he's got to talk tomorrow

00:50:15,010 --> 00:50:19,780
at 11:00 on pixie dust

00:50:18,070 --> 00:50:21,480
pixie dust is a visualization library

00:50:19,780 --> 00:50:25,120
he's going to go much deeper into

00:50:21,480 --> 00:50:27,580
visualization libraries on in notebooks

00:50:25,120 --> 00:50:29,680
but you can see here here's an example

00:50:27,580 --> 00:50:31,870
of Brunel these are these are quite

00:50:29,680 --> 00:50:34,690
extensive I mean you could just build

00:50:31,870 --> 00:50:37,660
this this lot of these visualizations in

00:50:34,690 --> 00:50:39,190
Jupiter notebooks we've actually open

00:50:37,660 --> 00:50:41,680
source to Brunel libraries which are

00:50:39,190 --> 00:50:43,240
came out of Cognos pixi does is another

00:50:41,680 --> 00:50:44,020
library that we've open-source which are

00:50:43,240 --> 00:50:45,700
visualizations

00:50:44,020 --> 00:50:47,770
on Jupiter notebooks so you can build

00:50:45,700 --> 00:50:50,470
these visualizations and then share them

00:50:47,770 --> 00:50:53,190
as HTML pages so it's quite interesting

00:50:50,470 --> 00:50:57,820
nice thing about it is you have the code

00:50:53,190 --> 00:50:59,920
and the code that was to generate it as

00:50:57,820 --> 00:51:02,590
well as together as a visualization of

00:50:59,920 --> 00:51:06,780
the results that's kind of what's the

00:51:02,590 --> 00:51:09,250
power of these notebooks within the

00:51:06,780 --> 00:51:11,440
state self experience and then all of

00:51:09,250 --> 00:51:14,350
the computation and analysis happens on

00:51:11,440 --> 00:51:16,860
the back end in the spark cluster did

00:51:14,350 --> 00:51:16,860
you have another question

00:51:17,460 --> 00:51:28,559
yeah that is I yeah thank you guys

00:51:25,420 --> 00:51:28,559

YouTube URL: https://www.youtube.com/watch?v=v_3uHKyQOsc


