Title: Analytics and Graph Traversal with Solr - Yonik Seeley, Cloudera
Publication date: 2017-05-19
Playlist: ApacheCon 2017 - Miami
Description: 
	Analytics and Graph Traversal with Solr - Yonik Seeley, Cloudera

Analytics has increasingly become a major focus for Apache Solr, the primary search engine in big data stacks. This talk will cover recent Solr developments in the areas related to faceted analytics, streaming expressions, and graph traversal. We'll also cover where multiple APIs overlap in functionality and which approaches should be used in different scenarios.

About Yonik Seeley
Yonik Seeley is the creator of Solr. He works at Cloudera integrating and leveraging "Big Search" technologies into the many components comprising the Cloudera enterprise data hub (EDH). Yonik was a co-founder of LucidWorks, and holds a master's degree in computer science from Stanford University.
Captions: 
	00:00:00,359 --> 00:00:06,600
alright let's get started so this is

00:00:04,049 --> 00:00:08,820
actually the first time that I've

00:00:06,600 --> 00:00:10,260
actually given the first talk of the day

00:00:08,820 --> 00:00:12,509
when it hasn't been like the first day

00:00:10,260 --> 00:00:14,460
or something so those of you who manage

00:00:12,509 --> 00:00:17,640
to you know get out of bed and make it

00:00:14,460 --> 00:00:19,920
here thank you I'm going to be talking

00:00:17,640 --> 00:00:23,160
about analytics and graph traversal with

00:00:19,920 --> 00:00:24,900
solar so mostly graph type stuff but I'm

00:00:23,160 --> 00:00:26,789
also going to go over some a little bit

00:00:24,900 --> 00:00:30,240
of background you know in case people

00:00:26,789 --> 00:00:33,270
who don't know about solar so first of

00:00:30,240 --> 00:00:36,390
all those who don't know me I'm beyond

00:00:33,270 --> 00:00:39,270
exceed I originally created solar way

00:00:36,390 --> 00:00:42,300
back when I was working at C net in 2004

00:00:39,270 --> 00:00:44,879
about and it was later contributed to

00:00:42,300 --> 00:00:47,700
the Apache Software Foundation I'm also

00:00:44,879 --> 00:00:49,950
the co-founder of lucid works and I

00:00:47,700 --> 00:00:55,800
currently work at Cloudera integrating

00:00:49,950 --> 00:00:57,570
solar into their big data stack so just

00:00:55,800 --> 00:01:00,420
in case anyone doesn't know what solar

00:00:57,570 --> 00:01:02,399
is I sort of describe it as two people

00:01:00,420 --> 00:01:05,220
who don't even know what it is at all is

00:01:02,399 --> 00:01:08,070
it it fits in the same part of your

00:01:05,220 --> 00:01:10,939
infrastructure as a database would it's

00:01:08,070 --> 00:01:13,200
a search server it is just based on

00:01:10,939 --> 00:01:15,030
different underlying indexing

00:01:13,200 --> 00:01:17,909
technologies based on Apache we've seen

00:01:15,030 --> 00:01:20,549
then other types of databases are and

00:01:17,909 --> 00:01:23,070
it's really optimized for interactive

00:01:20,549 --> 00:01:26,220
results and so you know interactive

00:01:23,070 --> 00:01:28,799
being a web web scale people you know

00:01:26,220 --> 00:01:31,229
doing a search or clicking a button on a

00:01:28,799 --> 00:01:34,049
web page and solar needs to return those

00:01:31,229 --> 00:01:35,430
responses in you know less than a second

00:01:34,049 --> 00:01:37,530
ideally right because you don't want to

00:01:35,430 --> 00:01:40,409
be sitting around waiting so it has

00:01:37,530 --> 00:01:43,110
columns what we call dock values for

00:01:40,409 --> 00:01:46,110
fast scans fast analytics we have

00:01:43,110 --> 00:01:47,850
highlighting spatial search faceting

00:01:46,110 --> 00:01:51,299
which I'll go over a little bit

00:01:47,850 --> 00:01:53,189
streaming expressions which really need

00:01:51,299 --> 00:01:55,439
some explanation I'll go over that what

00:01:53,189 --> 00:01:58,380
that means as well we have some graph

00:01:55,439 --> 00:02:04,079
search and also sequel which is layered

00:01:58,380 --> 00:02:06,890
over the streaming expressions so just a

00:02:04,079 --> 00:02:10,349
little refresher on faceted search

00:02:06,890 --> 00:02:12,239
faceted search is really about taking a

00:02:10,349 --> 00:02:13,930
set of documents that are the hope of

00:02:12,239 --> 00:02:16,780
like a user

00:02:13,930 --> 00:02:19,030
weary and breaking those up in and

00:02:16,780 --> 00:02:21,189
categorizing them and then giving

00:02:19,030 --> 00:02:23,739
feedback like counts based on those

00:02:21,189 --> 00:02:27,069
categorizations so this is like one of

00:02:23,739 --> 00:02:31,030
the oldest screenshots that I did you

00:02:27,069 --> 00:02:33,639
know like the first tutorial I did on a

00:02:31,030 --> 00:02:35,200
faceted search it's on digital cameras

00:02:33,639 --> 00:02:38,290
which of course are dying these days

00:02:35,200 --> 00:02:41,889
because of smartphones but so here we

00:02:38,290 --> 00:02:43,900
have a manufacturer facet and resolution

00:02:41,889 --> 00:02:47,139
facet and you can see that they've been

00:02:43,900 --> 00:02:50,859
broken up and this lets the user know

00:02:47,139 --> 00:02:53,409
that we have five Canon digital cameras

00:02:50,859 --> 00:02:55,359
and two Sony digital cameras and so that

00:02:53,409 --> 00:02:58,060
gives the information to the user

00:02:55,359 --> 00:03:00,370
upfront they know that if they refine

00:02:58,060 --> 00:03:02,139
their search results to Canon they're

00:03:00,370 --> 00:03:04,810
going to end up with like five entries

00:03:02,139 --> 00:03:06,280
and then below that at the bottom of the

00:03:04,810 --> 00:03:08,769
screen off the bottom of the screen is

00:03:06,280 --> 00:03:10,209
like the list of top results what you'd

00:03:08,769 --> 00:03:13,599
normally expect from you know rank

00:03:10,209 --> 00:03:16,659
search engine and so this allows the

00:03:13,599 --> 00:03:19,269
user to you know click on one of these

00:03:16,659 --> 00:03:21,639
categories like manufacturing resolution

00:03:19,269 --> 00:03:23,590
and you know refine their search results

00:03:21,639 --> 00:03:27,669
drill down into their searches also find

00:03:23,590 --> 00:03:30,669
what they're looking for so we have

00:03:27,669 --> 00:03:32,459
different ways of calculating things in

00:03:30,669 --> 00:03:37,239
solar that had been developed over time

00:03:32,459 --> 00:03:40,449
so like the first is basically faceted

00:03:37,239 --> 00:03:42,669
search version 1 and that uses flat

00:03:40,449 --> 00:03:45,849
query parameters you say you know facet

00:03:42,669 --> 00:03:49,060
field equals color facet limit equals 5

00:03:45,849 --> 00:03:51,699
and what that means is categorize you

00:03:49,060 --> 00:03:55,989
know this set of results by color and

00:03:51,699 --> 00:03:59,019
give me the top five so lately I've been

00:03:55,989 --> 00:04:00,489
working on the JSON facet API which I've

00:03:59,019 --> 00:04:04,449
sort of referred to as the you know

00:04:00,489 --> 00:04:08,590
version 2 and that expresses things in

00:04:04,449 --> 00:04:11,349
JSON one reason is that it allows much

00:04:08,590 --> 00:04:13,750
more nesting facets under facets and so

00:04:11,349 --> 00:04:16,630
it made sense to use something like JSON

00:04:13,750 --> 00:04:18,519
which is naturally nested and so this

00:04:16,630 --> 00:04:20,949
just does the same thing if the type

00:04:18,519 --> 00:04:25,120
terms on the color field give you the

00:04:20,949 --> 00:04:26,610
top 5 just a different syntax streaming

00:04:25,120 --> 00:04:29,490
expressions

00:04:26,610 --> 00:04:31,199
is a whole nother ball of wax I'm going

00:04:29,490 --> 00:04:33,210
to get into that in another slide but

00:04:31,199 --> 00:04:35,520
this just gives a little sample of doing

00:04:33,210 --> 00:04:37,139
the same thing it's not a quite an

00:04:35,520 --> 00:04:39,780
apples-to-apples comparison because this

00:04:37,139 --> 00:04:41,879
also includes the search so we're doing

00:04:39,780 --> 00:04:44,069
a search and then we're doing a roll up

00:04:41,879 --> 00:04:46,620
over the color field and you know

00:04:44,069 --> 00:04:48,449
collecting count and then of course

00:04:46,620 --> 00:04:52,020
parallel sequel which I'm not going to

00:04:48,449 --> 00:04:55,099
cover more today but that essentially

00:04:52,020 --> 00:05:00,389
translates into streaming expressions

00:04:55,099 --> 00:05:02,990
for execution all right

00:05:00,389 --> 00:05:07,259
so getting into a little bit about graph

00:05:02,990 --> 00:05:10,590
so graph databases are all about nodes

00:05:07,259 --> 00:05:13,740
and edges or relationships between the

00:05:10,590 --> 00:05:15,870
nodes until in this example you know I'm

00:05:13,740 --> 00:05:19,169
starting off with you know and is a node

00:05:15,870 --> 00:05:21,330
and Stanford is a node and an attended

00:05:19,169 --> 00:05:24,090
Stanford and attended is like an edge or

00:05:21,330 --> 00:05:26,849
a relationship and Ann also attended RPI

00:05:24,090 --> 00:05:27,539
she recommended Mike she works at

00:05:26,849 --> 00:05:32,129
Cloudera

00:05:27,539 --> 00:05:37,409
and she lives in New Jersey so what's

00:05:32,129 --> 00:05:39,479
missing here is properties because the

00:05:37,409 --> 00:05:42,089
fact that Anne attended Stanford we're

00:05:39,479 --> 00:05:43,919
not going to get too far with that

00:05:42,089 --> 00:05:46,440
because we don't know when she attended

00:05:43,919 --> 00:05:48,599
Stanford and we don't know what kind of

00:05:46,440 --> 00:05:52,349
degree that she got and so we really

00:05:48,599 --> 00:05:54,089
need some more information and so we

00:05:52,349 --> 00:05:56,279
normally have in graph databases things

00:05:54,089 --> 00:05:58,919
called properties that we can add on

00:05:56,279 --> 00:06:01,860
both nodes and edges most of the graph

00:05:58,919 --> 00:06:04,229
databases that I know about like neo4j

00:06:01,860 --> 00:06:06,719
allow properties to be on both nodes and

00:06:04,229 --> 00:06:09,810
edges so here's just an example of the

00:06:06,719 --> 00:06:12,210
properties we added we zoomed in on the

00:06:09,810 --> 00:06:13,979
Stanford relationship we said that Anne

00:06:12,210 --> 00:06:15,930
attended Stanford and the start and the

00:06:13,979 --> 00:06:17,819
end date what degree she got and what

00:06:15,930 --> 00:06:22,789
the subject was and we also added some

00:06:17,819 --> 00:06:22,789
properties on and in Stanford as well

00:06:22,849 --> 00:06:30,629
but what from from a search perspective

00:06:25,349 --> 00:06:33,659
though these things the nodes and the

00:06:30,629 --> 00:06:35,849
relationships start looking just very

00:06:33,659 --> 00:06:38,580
similar to they both start looking like

00:06:35,849 --> 00:06:40,470
documents to me because you can just

00:06:38,580 --> 00:06:42,960
put whatever properties you want on

00:06:40,470 --> 00:06:46,350
nodes or edges and so that really kind

00:06:42,960 --> 00:06:48,180
of maps towards documents in the search

00:06:46,350 --> 00:06:49,620
world because documents you can just

00:06:48,180 --> 00:06:52,590
have arbitrary properties

00:06:49,620 --> 00:06:54,450
it's like schema free so it's very

00:06:52,590 --> 00:06:57,120
there's more in common it feels like

00:06:54,450 --> 00:07:02,100
with a graph database and search engines

00:06:57,120 --> 00:07:05,060
than there are differences so this is

00:07:02,100 --> 00:07:07,760
how we map from the graph world to

00:07:05,060 --> 00:07:11,820
document space or a search engine world

00:07:07,760 --> 00:07:15,890
if we have a relationship without any

00:07:11,820 --> 00:07:18,780
properties then we can simply model that

00:07:15,890 --> 00:07:22,230
implicitly based on matches and field

00:07:18,780 --> 00:07:25,170
values so at the top is an example of

00:07:22,230 --> 00:07:27,150
that node one points to node two all we

00:07:25,170 --> 00:07:30,720
really have to do is have something on

00:07:27,150 --> 00:07:33,600
node one that contains the ID of node

00:07:30,720 --> 00:07:37,200
two or something unique that somehow

00:07:33,600 --> 00:07:40,260
points to node two and so this is not an

00:07:37,200 --> 00:07:42,240
explicit edge that's indexed or anything

00:07:40,260 --> 00:07:44,640
like that it's just sort of implicit and

00:07:42,240 --> 00:07:48,870
we can use that relationship the fact

00:07:44,640 --> 00:07:50,340
that those values match at query time so

00:07:48,870 --> 00:07:53,100
when we have relationships with

00:07:50,340 --> 00:07:55,470
properties that's when it just makes

00:07:53,100 --> 00:07:58,940
sense to treat that relationship also as

00:07:55,470 --> 00:08:01,410
a document so both nodes and edges or

00:07:58,940 --> 00:08:06,210
relationships everything's a document

00:08:01,410 --> 00:08:08,730
and the actual edges between those or

00:08:06,210 --> 00:08:11,190
how the relationship is encoded again

00:08:08,730 --> 00:08:13,680
it's just implicit based on mapping or

00:08:11,190 --> 00:08:15,360
matching of the field values and so you

00:08:13,680 --> 00:08:17,670
can do that one of two ways you can do

00:08:15,360 --> 00:08:21,140
it on the left-hand side we have our

00:08:17,670 --> 00:08:23,850
nodes pointing to the relationship

00:08:21,140 --> 00:08:27,240
document and on the right hand side we

00:08:23,850 --> 00:08:30,390
have the relationship pointing to both

00:08:27,240 --> 00:08:35,070
node objects so you can do it either way

00:08:30,390 --> 00:08:37,800
or a combination thereof so going back

00:08:35,070 --> 00:08:40,710
to an attending Stanford again this is

00:08:37,800 --> 00:08:43,860
how we model that in solar or in a

00:08:40,710 --> 00:08:47,010
search engine so on the right hand side

00:08:43,860 --> 00:08:49,350
we have a document that represents

00:08:47,010 --> 00:08:51,580
Stanford a document that represents and

00:08:49,350 --> 00:08:55,089
and then we have a

00:08:51,580 --> 00:08:57,970
attendance type document now we added

00:08:55,089 --> 00:09:01,930
this type that's not required it just

00:08:57,970 --> 00:09:03,100
helps disambiguate between like

00:09:01,930 --> 00:09:06,190
different types of records there

00:09:03,100 --> 00:09:08,290
probably make querying easier later on

00:09:06,190 --> 00:09:11,470
especially since we sort of reuse some

00:09:08,290 --> 00:09:14,140
of the fields right so we have a name

00:09:11,470 --> 00:09:16,000
field for Stanford we have a name field

00:09:14,140 --> 00:09:17,560
for Ann we could have like used

00:09:16,000 --> 00:09:19,930
different field names if you'd wanted

00:09:17,560 --> 00:09:20,079
like you know person name versus you

00:09:19,930 --> 00:09:23,620
know

00:09:20,079 --> 00:09:26,709
edu name it's really a matter of

00:09:23,620 --> 00:09:29,680
preference and so you can see our

00:09:26,709 --> 00:09:32,649
attendance document we have a whoo that

00:09:29,680 --> 00:09:39,820
points to an and aware that points to

00:09:32,649 --> 00:09:41,260
Stanford so now that we've like you now

00:09:39,820 --> 00:09:44,260
learned how to like index some of this

00:09:41,260 --> 00:09:47,260
graph stuff let's look at how some

00:09:44,260 --> 00:09:49,600
different ways to query so first of all

00:09:47,260 --> 00:09:52,180
I'm going to go over a graph filter also

00:09:49,600 --> 00:09:54,730
called graph query it's a breath it does

00:09:52,180 --> 00:09:56,740
a breadth-first traversal and it's

00:09:54,730 --> 00:09:59,709
modeled and implemented as a normal

00:09:56,740 --> 00:10:02,980
we've seen query and so what that means

00:09:59,709 --> 00:10:05,230
is that can be used anywhere a Lucene

00:10:02,980 --> 00:10:06,670
query can be used within solar and so

00:10:05,230 --> 00:10:08,079
that's actually a lot of places you know

00:10:06,670 --> 00:10:10,660
the normal places is it could be like a

00:10:08,079 --> 00:10:13,690
filter but it could also be things like

00:10:10,660 --> 00:10:15,790
a facet query another nice property if

00:10:13,690 --> 00:10:17,860
it's just a query is that it is

00:10:15,790 --> 00:10:20,470
automatically cached in the filter cache

00:10:17,860 --> 00:10:22,779
so you could use graph say for setting

00:10:20,470 --> 00:10:25,029
setting up like a permissions hierarchy

00:10:22,779 --> 00:10:27,220
like groups within groups custom groups

00:10:25,029 --> 00:10:29,620
you can do as a graph query to actually

00:10:27,220 --> 00:10:31,000
map from a user to the full set of

00:10:29,620 --> 00:10:33,700
documents that they're allowed to see

00:10:31,000 --> 00:10:35,260
and because it's automatically cached in

00:10:33,700 --> 00:10:38,050
the filter cache that graph traversal

00:10:35,260 --> 00:10:39,640
will only be done every time the index

00:10:38,050 --> 00:10:41,110
changes but then it will be like the

00:10:39,640 --> 00:10:43,149
results of it will be automatically

00:10:41,110 --> 00:10:47,470
reused as long as the index doesn't

00:10:43,149 --> 00:10:50,350
change oh and so the output I called it

00:10:47,470 --> 00:10:52,690
a graph filter because the output is

00:10:50,350 --> 00:10:55,120
just a set of documents there's no

00:10:52,690 --> 00:10:56,860
ranking or scoring between documents

00:10:55,120 --> 00:10:58,660
that says this document is better than

00:10:56,860 --> 00:11:03,190
this other document all you really get

00:10:58,660 --> 00:11:05,379
is a set of documents out and the how we

00:11:03,190 --> 00:11:07,389
follow the edges the edges are define

00:11:05,379 --> 00:11:09,669
signed by the from in the to field those

00:11:07,389 --> 00:11:12,279
the only two required parameters and so

00:11:09,669 --> 00:11:14,829
we look at the from field see what

00:11:12,279 --> 00:11:17,109
values it has and then match those up to

00:11:14,829 --> 00:11:20,169
the two field and that sort of defines

00:11:17,109 --> 00:11:24,970
the edges sort of like so it's a query

00:11:20,169 --> 00:11:26,979
time on the fly so we have a couple

00:11:24,970 --> 00:11:30,489
other optional arguments to the graph

00:11:26,979 --> 00:11:31,959
filter one is max depth by default it

00:11:30,489 --> 00:11:33,609
just keeps going until the set doesn't

00:11:31,959 --> 00:11:35,349
change anymore

00:11:33,609 --> 00:11:37,239
we have a traversal filter though which

00:11:35,349 --> 00:11:39,879
is really useful and that says as you're

00:11:37,239 --> 00:11:42,069
following these edges for nodes to be

00:11:39,879 --> 00:11:45,879
included in the set they must follow I

00:11:42,069 --> 00:11:47,470
mean they must match this filter and the

00:11:45,879 --> 00:11:49,419
filters just expressed as another

00:11:47,470 --> 00:11:53,829
arbitrary query it could be another

00:11:49,419 --> 00:11:55,809
graph query even and return root and

00:11:53,829 --> 00:11:59,199
leaf nodes only those are just bullying

00:11:55,809 --> 00:12:01,029
that tell what final set of documents

00:11:59,199 --> 00:12:05,289
you know whether the root is included or

00:12:01,029 --> 00:12:08,019
not so the big caveat though to this

00:12:05,289 --> 00:12:11,169
graph filter graph query is that it is

00:12:08,019 --> 00:12:13,689
not distributed and what I mean by that

00:12:11,169 --> 00:12:15,039
is that edges are not followed across

00:12:13,689 --> 00:12:17,799
shards if you've broken up your

00:12:15,039 --> 00:12:20,019
collection into multiple shards the

00:12:17,799 --> 00:12:23,079
graph query will only follow edges in

00:12:20,019 --> 00:12:27,249
each individual shard they won't follow

00:12:23,079 --> 00:12:29,649
edges that cross shards it is still

00:12:27,249 --> 00:12:33,009
compatible with distributed search in

00:12:29,649 --> 00:12:35,439
that as long as you can live with that

00:12:33,009 --> 00:12:37,449
then everything you know however you're

00:12:35,439 --> 00:12:40,449
using the graph query as a filter in

00:12:37,449 --> 00:12:42,850
faceting whatever distributed search

00:12:40,449 --> 00:12:44,949
will still combine results and and think

00:12:42,850 --> 00:12:46,419
everything else will still work and so

00:12:44,949 --> 00:12:48,069
this is still a really useful query

00:12:46,419 --> 00:12:50,619
because sometimes you can partition your

00:12:48,069 --> 00:12:54,789
data such that traversals you're doing

00:12:50,619 --> 00:12:57,189
don't have any cross shard edges or if

00:12:54,789 --> 00:12:58,809
they do exist you might just not care it

00:12:57,189 --> 00:13:04,959
depends on the type of computations

00:12:58,809 --> 00:13:07,089
you're doing so here's the Futurama

00:13:04,959 --> 00:13:09,909
example where we're trying to use a

00:13:07,089 --> 00:13:12,999
graph filter to find the full set of

00:13:09,909 --> 00:13:16,209
fries ancestors and so we've modeled

00:13:12,999 --> 00:13:18,549
people here we just have an ID field

00:13:16,209 --> 00:13:19,370
that's essentially their name and then

00:13:18,549 --> 00:13:23,140
we have a

00:13:19,370 --> 00:13:26,210
parents filled that just lists the the

00:13:23,140 --> 00:13:33,920
the names of their biological parents

00:13:26,210 --> 00:13:36,650
and so our graph filter we are how you

00:13:33,920 --> 00:13:41,420
read this right we're it's from parents

00:13:36,650 --> 00:13:44,240
to ID starting with Philip J fry and so

00:13:41,420 --> 00:13:46,820
we start with Philip J fry we look at

00:13:44,240 --> 00:13:49,790
the from field the parents values are

00:13:46,820 --> 00:13:52,880
Yancy fry senior and mrs. fry and we

00:13:49,790 --> 00:13:55,820
match those values up with the ID fields

00:13:52,880 --> 00:14:00,290
of all documents and that basically

00:13:55,820 --> 00:14:02,300
visits arm his parents and it just

00:14:00,290 --> 00:14:04,490
continues from there until if that

00:14:02,300 --> 00:14:08,240
doesn't change anymore now when we get

00:14:04,490 --> 00:14:12,640
to fries father Yancy fry senior we look

00:14:08,240 --> 00:14:16,190
at his parents field and we see

00:14:12,640 --> 00:14:20,840
something odd pretty quickly at Philip J

00:14:16,190 --> 00:14:25,100
fry is his father and so if you're a

00:14:20,840 --> 00:14:27,350
Futurama fan you might recognize is

00:14:25,100 --> 00:14:31,610
actually his own grandfather just due to

00:14:27,350 --> 00:14:32,990
some time-travel weirdness and so we hit

00:14:31,610 --> 00:14:35,120
a cycle right if we're actually

00:14:32,990 --> 00:14:37,640
following the rate of thing we'd like to

00:14:35,120 --> 00:14:39,980
get into a loop but that's okay because

00:14:37,640 --> 00:14:41,960
we have cycle detection and so it

00:14:39,980 --> 00:14:43,490
actually does not you know get into an

00:14:41,960 --> 00:14:45,830
infinite loop or anything it all just

00:14:43,490 --> 00:14:49,430
works and so you know at the at the

00:14:45,830 --> 00:14:51,590
after this whole thing keeps traversing

00:14:49,430 --> 00:14:54,170
the set doesn't change anymore and we

00:14:51,590 --> 00:14:56,240
have the full set of fries ancestors and

00:14:54,170 --> 00:14:58,010
then we can do use that with you know

00:14:56,240 --> 00:15:00,590
whatever we want we can calculate like

00:14:58,010 --> 00:15:02,870
the average IQ I guess of fries

00:15:00,590 --> 00:15:09,890
ancestors which I'm not sure how high

00:15:02,870 --> 00:15:12,640
that would be but okay on to streaming

00:15:09,890 --> 00:15:15,020
expressions so streaming expressions are

00:15:12,640 --> 00:15:16,310
relatively new to the solar world I

00:15:15,020 --> 00:15:19,010
think they've started really coming on

00:15:16,310 --> 00:15:20,560
strong in solar six even though they

00:15:19,010 --> 00:15:24,410
were introduced a little bit before that

00:15:20,560 --> 00:15:26,480
and it's really a new way for solar to

00:15:24,410 --> 00:15:29,690
compute things it's a generic platform

00:15:26,480 --> 00:15:32,300
for distributed computation it does form

00:15:29,690 --> 00:15:33,230
the basis for our sequel support we use

00:15:32,300 --> 00:15:36,590
like

00:15:33,230 --> 00:15:38,210
- calcite and translate things into

00:15:36,590 --> 00:15:40,910
streaming expressions to actually do the

00:15:38,210 --> 00:15:42,530
execution of the sequel

00:15:40,910 --> 00:15:44,960
so like the streaming expressions is

00:15:42,530 --> 00:15:48,410
really about relationships on streams

00:15:44,960 --> 00:15:50,780
and it is really optimized and to work

00:15:48,410 --> 00:15:52,250
across entire sets of data so this is a

00:15:50,780 --> 00:15:53,780
little bit different than how sollars

00:15:52,250 --> 00:15:56,450
worked in the past which is that's

00:15:53,780 --> 00:15:59,060
optimized for finding the top end of

00:15:56,450 --> 00:16:01,400
things distributing expressions normally

00:15:59,060 --> 00:16:05,770
look at all the data or all the data

00:16:01,400 --> 00:16:09,260
matching something so we have like a

00:16:05,770 --> 00:16:10,850
MapReduce type shuffle and so if you

00:16:09,260 --> 00:16:13,130
remember some of the MapReduce word

00:16:10,850 --> 00:16:15,110
count type things instead of querying a

00:16:13,130 --> 00:16:16,850
bunch of shards and having a single

00:16:15,110 --> 00:16:20,360
internal aggregator for increased

00:16:16,850 --> 00:16:23,000
parallelism we can like do partitioning

00:16:20,360 --> 00:16:24,830
and have multiple internal aggregators

00:16:23,000 --> 00:16:26,630
doing whatever computations the streams

00:16:24,830 --> 00:16:30,310
are doing and then have an additional

00:16:26,630 --> 00:16:34,100
level after that that does a final merge

00:16:30,310 --> 00:16:35,900
and this can also incorporate data from

00:16:34,100 --> 00:16:38,390
non-solar systems so it's a little bit

00:16:35,900 --> 00:16:41,420
less document oriented than a lot of the

00:16:38,390 --> 00:16:43,760
other previous work and solar so you

00:16:41,420 --> 00:16:46,310
like have a JDBC stream that points to a

00:16:43,760 --> 00:16:48,980
different you know external database and

00:16:46,310 --> 00:16:51,920
then like basically do joins or whatever

00:16:48,980 --> 00:16:57,170
type of computation with solar results

00:16:51,920 --> 00:16:58,910
as well so here's an example of a

00:16:57,170 --> 00:17:00,740
streaming expression it's you know one

00:16:58,910 --> 00:17:02,860
of the most basic fundamental streaming

00:17:00,740 --> 00:17:05,090
expressions in that it searches solar

00:17:02,860 --> 00:17:06,620
and so it's like you know we call it a

00:17:05,090 --> 00:17:08,480
stream source it's one way to like you

00:17:06,620 --> 00:17:11,570
know kick things off you know a source

00:17:08,480 --> 00:17:14,360
of data and so instead of hitting the

00:17:11,570 --> 00:17:18,140
Select or query URL we hit the stream

00:17:14,360 --> 00:17:21,140
URL so it's just the same exact URL it's

00:17:18,140 --> 00:17:24,290
just reflect the the end part is flash

00:17:21,140 --> 00:17:26,390
stream and so we pass that expression

00:17:24,290 --> 00:17:28,700
equals and then our search expression it

00:17:26,390 --> 00:17:30,350
has sort of a functional notation the

00:17:28,700 --> 00:17:31,730
first parameter is the collection name

00:17:30,350 --> 00:17:33,170
and then we have our normal search

00:17:31,730 --> 00:17:36,140
parameters if you're familiar with solar

00:17:33,170 --> 00:17:38,030
you double the query the field list that

00:17:36,140 --> 00:17:40,370
we want to return back and how to sort

00:17:38,030 --> 00:17:42,800
it and then we can have our tuple list

00:17:40,370 --> 00:17:46,450
as a response and that you know that

00:17:42,800 --> 00:17:47,919
looks very much like a normal solar

00:17:46,450 --> 00:17:51,269
research requests even though it works

00:17:47,919 --> 00:17:51,269
very differently under the covers

00:17:51,360 --> 00:17:55,750
so here's more like a logical diagram

00:17:53,799 --> 00:17:58,360
but like what the search expression is

00:17:55,750 --> 00:18:01,590
doing it's fully solar cloud aware so

00:17:58,360 --> 00:18:04,360
when you say search this collection

00:18:01,590 --> 00:18:06,789
Solar knows you know what the cluster

00:18:04,360 --> 00:18:08,919
layout is what it looks like

00:18:06,789 --> 00:18:11,380
how many shards there are in the

00:18:08,919 --> 00:18:13,419
collection how many replicas there are

00:18:11,380 --> 00:18:15,940
for each shard and the physical

00:18:13,419 --> 00:18:20,250
locations of those so it can just go and

00:18:15,940 --> 00:18:24,130
query one replicas of each shard and

00:18:20,250 --> 00:18:28,029
then stream the results back to a worker

00:18:24,130 --> 00:18:29,679
node who that's just really another in

00:18:28,029 --> 00:18:32,549
this case the worker node will be the

00:18:29,679 --> 00:18:36,760
endpoint that you hit with Slash stream

00:18:32,549 --> 00:18:42,909
and then it produces the final result so

00:18:36,760 --> 00:18:46,690
we call this streaming because we try to

00:18:42,909 --> 00:18:48,960
compute things as we're receiving data

00:18:46,690 --> 00:18:53,679
so the worker node in the middle it's

00:18:48,960 --> 00:18:57,370
receiving documents from all the

00:18:53,679 --> 00:18:59,350
replicas that it queried at once but it

00:18:57,370 --> 00:19:01,840
does not fully read all the data before

00:18:59,350 --> 00:19:05,559
it starts producing its output because

00:19:01,840 --> 00:19:08,320
we take care to like sort the sub tuple

00:19:05,559 --> 00:19:10,480
streams correctly we can just do like a

00:19:08,320 --> 00:19:12,760
merge sort and really we just have to

00:19:10,480 --> 00:19:15,789
see the first item we just have to have

00:19:12,760 --> 00:19:17,769
one item on each incoming queue before

00:19:15,789 --> 00:19:20,860
we can select the next item that should

00:19:17,769 --> 00:19:22,750
be on our output queue and so when you

00:19:20,860 --> 00:19:25,480
build up streams of streams of streams

00:19:22,750 --> 00:19:28,450
and wrap them as long as you take care

00:19:25,480 --> 00:19:30,490
to you know sort things correctly and

00:19:28,450 --> 00:19:33,039
have things sorted correctly we can do

00:19:30,490 --> 00:19:34,960
things in a streaming computation manner

00:19:33,039 --> 00:19:36,669
and that really helps with scalability

00:19:34,960 --> 00:19:38,740
so you know you can go from like a

00:19:36,669 --> 00:19:40,840
million tuples to a billion people to a

00:19:38,740 --> 00:19:45,309
trillion tuples and what's going to

00:19:40,840 --> 00:19:47,260
increase is you know your the execution

00:19:45,309 --> 00:19:49,120
time the amount of memory from that

00:19:47,260 --> 00:19:52,539
worker is not going to increase at all

00:19:49,120 --> 00:19:54,840
so you get scalability in one dimension

00:19:52,539 --> 00:19:54,840
at least

00:19:56,660 --> 00:20:01,440
so graph streaming expressions there are

00:19:59,910 --> 00:20:04,920
a lot of streaming expressions there

00:20:01,440 --> 00:20:06,570
join streaming expressions but I'm

00:20:04,920 --> 00:20:07,860
really going to cover just more graph

00:20:06,570 --> 00:20:10,260
oriented ones today

00:20:07,860 --> 00:20:14,160
so the graph streaming expression does a

00:20:10,260 --> 00:20:17,160
breadth-first search and since it is

00:20:14,160 --> 00:20:19,050
just another streaming expression it is

00:20:17,160 --> 00:20:22,980
fully distributed it follows edges

00:20:19,050 --> 00:20:25,350
across nodes across shards and even

00:20:22,980 --> 00:20:27,330
across collections so you can just go

00:20:25,350 --> 00:20:30,150
from and I think even across different

00:20:27,330 --> 00:20:33,810
solar cloud clusters if you really want

00:20:30,150 --> 00:20:36,570
and can keep you aggregations as it goes

00:20:33,810 --> 00:20:38,760
so here's an example of the most basic

00:20:36,570 --> 00:20:41,730
graph streaming expression it's called

00:20:38,760 --> 00:20:44,040
gather nodes so we hit the stream URL

00:20:41,730 --> 00:20:46,800
again say gather nodes the first

00:20:44,040 --> 00:20:49,020
parameters emails that is just going to

00:20:46,800 --> 00:20:52,680
be the collection name and then we say

00:20:49,020 --> 00:20:55,920
walk from a literal John Doe at Apache

00:20:52,680 --> 00:20:59,190
org to the from field so this is just

00:20:55,920 --> 00:21:01,440
our root set and then we're going to

00:20:59,190 --> 00:21:03,630
gather the two field so really what this

00:21:01,440 --> 00:21:11,760
outputs is just anybody that John Doe

00:21:03,630 --> 00:21:13,950
has emails so for an actual a real

00:21:11,760 --> 00:21:16,850
example with a small data set sometimes

00:21:13,950 --> 00:21:20,430
I like small data sets because when you

00:21:16,850 --> 00:21:22,680
try this stuff out if you try it on a

00:21:20,430 --> 00:21:24,000
very large data set you often don't know

00:21:22,680 --> 00:21:25,890
if you're getting the correct results

00:21:24,000 --> 00:21:28,050
right you get a result and you're like I

00:21:25,890 --> 00:21:30,570
don't is it doing what I think it's

00:21:28,050 --> 00:21:32,970
doing I can't tell so having a small

00:21:30,570 --> 00:21:35,100
data set first that you can actually do

00:21:32,970 --> 00:21:37,860
the computation yourself is really

00:21:35,100 --> 00:21:39,930
helpful so this is going to be about

00:21:37,860 --> 00:21:42,150
books and book reviews so we're just

00:21:39,930 --> 00:21:46,050
going to index a bunch of books into one

00:21:42,150 --> 00:21:50,310
collection on CSV format two random

00:21:46,050 --> 00:21:53,340
books that I've read over time and we're

00:21:50,310 --> 00:21:55,530
also going to index book reviews into

00:21:53,340 --> 00:21:57,600
another collection and so if we just

00:21:55,530 --> 00:22:01,350
look at it but you know the the schema

00:21:57,600 --> 00:22:04,470
we're using here for each book review we

00:22:01,350 --> 00:22:08,730
have a book field book underscore s that

00:22:04,470 --> 00:22:09,690
matches the ID of the actual book and so

00:22:08,730 --> 00:22:12,059
that's sort of like the

00:22:09,690 --> 00:22:17,100
clicks it pointers to what book is this

00:22:12,059 --> 00:22:18,960
review for and so now that we have book

00:22:17,100 --> 00:22:20,879
reviews we come up with this brilliant

00:22:18,960 --> 00:22:22,769
idea that's actually not so brilliant of

00:22:20,879 --> 00:22:24,210
how to do a recommender so this is

00:22:22,769 --> 00:22:26,100
actually not you should not do a

00:22:24,210 --> 00:22:28,860
recommender this way this is really just

00:22:26,100 --> 00:22:32,039
to illustrate how you use some graph

00:22:28,860 --> 00:22:35,009
queries so the steps that we want to

00:22:32,039 --> 00:22:36,509
take is like number one find books that

00:22:35,009 --> 00:22:38,570
I like and we're just going to define

00:22:36,509 --> 00:22:41,610
that by like having a rating of five

00:22:38,570 --> 00:22:44,700
step two is we're going to find other

00:22:41,610 --> 00:22:46,529
users who rated that who also liked

00:22:44,700 --> 00:22:48,029
those books and we're going to define

00:22:46,529 --> 00:22:50,940
that it's like having a rating of four

00:22:48,029 --> 00:22:54,360
or above and then step three is just

00:22:50,940 --> 00:22:58,110
find other books that those users have

00:22:54,360 --> 00:23:01,320
rated well so pretty simple so if you

00:22:58,110 --> 00:23:07,019
actually look at the data Haruka and

00:23:01,320 --> 00:23:09,269
Maria actually rated same books that I

00:23:07,019 --> 00:23:12,090
liked so they should be the output of

00:23:09,269 --> 00:23:16,080
step number two and then for step number

00:23:12,090 --> 00:23:18,480
three only Maria rated another book

00:23:16,080 --> 00:23:21,210
highly that I haven't read and so that

00:23:18,480 --> 00:23:24,830
would be book ten Gridlink so that

00:23:21,210 --> 00:23:28,289
should be the output of our algorithm

00:23:24,830 --> 00:23:30,809
so let's actually do it so step number

00:23:28,289 --> 00:23:32,820
one is the search expression to find my

00:23:30,809 --> 00:23:34,740
high ratings so this actually isn't

00:23:32,820 --> 00:23:36,809
graph related at all this is just a

00:23:34,740 --> 00:23:40,980
normal search expression but we need a

00:23:36,809 --> 00:23:42,870
root set to start with so yeah so we're

00:23:40,980 --> 00:23:45,629
searching reviews and the query is user

00:23:42,870 --> 00:23:47,399
yannick and rating five very simple and

00:23:45,629 --> 00:23:50,789
we get back to books that I've rated

00:23:47,399 --> 00:23:54,539
highly now for step two we're going to

00:23:50,789 --> 00:23:58,110
wrap that search expression into in a

00:23:54,539 --> 00:23:59,460
gather nodes expression so that the

00:23:58,110 --> 00:24:01,289
search expression there is just a copy

00:23:59,460 --> 00:24:03,990
and paste from the first one the

00:24:01,289 --> 00:24:06,840
previous slide and the next parameter is

00:24:03,990 --> 00:24:10,230
we're going to walk from book to book

00:24:06,840 --> 00:24:13,769
and so what that is going to do is that

00:24:10,230 --> 00:24:16,559
we're starting with my reviews my target

00:24:13,769 --> 00:24:19,289
reviews and we're expanding to all

00:24:16,559 --> 00:24:22,440
reviews by those same books so it's like

00:24:19,289 --> 00:24:23,460
a self join on the book field and we're

00:24:22,440 --> 00:24:25,800
going to gather

00:24:23,460 --> 00:24:29,340
the user field and so what that does

00:24:25,800 --> 00:24:32,550
that gather of the user field that makes

00:24:29,340 --> 00:24:36,030
the node value in the response below be

00:24:32,550 --> 00:24:38,760
equal to the user field and so it's sort

00:24:36,030 --> 00:24:41,930
of like the virtual we don't actually we

00:24:38,760 --> 00:24:44,460
didn't actually index any user objects

00:24:41,930 --> 00:24:46,650
so this these are like virtual nodes

00:24:44,460 --> 00:24:48,870
we're on like user nodes now that we've

00:24:46,650 --> 00:24:51,810
just sort of invented by gathering the

00:24:48,870 --> 00:24:53,400
user field but we're not going to gather

00:24:51,810 --> 00:24:55,890
all the user fields we're only going to

00:24:53,400 --> 00:24:57,780
gather the user fields on the reviews

00:24:55,890 --> 00:24:59,910
that passed the filter because we

00:24:57,780 --> 00:25:01,710
specified a filter and that filter is

00:24:59,910 --> 00:25:04,770
you know well it have some herb high

00:25:01,710 --> 00:25:08,520
enough rating for two for and above and

00:25:04,770 --> 00:25:10,440
it has to not be by me and we've also

00:25:08,520 --> 00:25:13,080
added an optional parameter track

00:25:10,440 --> 00:25:15,960
traversal equals true and that just adds

00:25:13,080 --> 00:25:18,990
in the response that ancestors filled

00:25:15,960 --> 00:25:26,660
the ancestors that says where we came

00:25:18,990 --> 00:25:30,120
from to get to our current tuple set and

00:25:26,660 --> 00:25:31,890
so now on to step three we take that

00:25:30,120 --> 00:25:34,890
whole streaming expression from the

00:25:31,890 --> 00:25:38,850
previous slide we wrap it in to another

00:25:34,890 --> 00:25:42,720
gather nodes on reviews yet again and

00:25:38,850 --> 00:25:45,270
this time we walk from node to user and

00:25:42,720 --> 00:25:50,670
so what this does if you were called the

00:25:45,270 --> 00:25:54,330
plasti back yeah the past slide the node

00:25:50,670 --> 00:25:57,660
field was user names our target user

00:25:54,330 --> 00:25:59,760
names and so this actually matches up

00:25:57,660 --> 00:26:03,300
the nodes to the user fields and what

00:25:59,760 --> 00:26:07,020
that does is it selects all reviews by

00:26:03,300 --> 00:26:09,210
those users and then we gather the book

00:26:07,020 --> 00:26:10,740
field because that's what we actually

00:26:09,210 --> 00:26:14,700
want out of this whole thing what book

00:26:10,740 --> 00:26:17,910
should I read but we only do this if the

00:26:14,700 --> 00:26:20,430
rating is for above and then we also ask

00:26:17,910 --> 00:26:23,400
for the average rating of all the

00:26:20,430 --> 00:26:26,070
incoming edges and you track traversal

00:26:23,400 --> 00:26:28,590
true again and at the end of all this we

00:26:26,070 --> 00:26:30,930
actually get out what we wanted book ten

00:26:28,590 --> 00:26:36,800
and we say that it came from the

00:26:30,930 --> 00:26:36,800
ancestor was Maria and it worked

00:26:39,029 --> 00:26:44,860
all right now if we want the complete

00:26:41,679 --> 00:26:48,070
traversal we can add this optional

00:26:44,860 --> 00:26:50,019
parameter scatter and say branches comma

00:26:48,070 --> 00:26:52,570
leaves and then that will give us the

00:26:50,019 --> 00:26:54,370
complete graph traversal and so you then

00:26:52,570 --> 00:26:57,249
we can see we started off with level 0

00:26:54,370 --> 00:26:59,889
with books level 1 with our target users

00:26:57,249 --> 00:27:08,049
and then finally level 2 back to the

00:26:59,889 --> 00:27:09,970
book that was recommended so here's how

00:27:08,049 --> 00:27:12,369
to do this quickly how to do the same

00:27:09,970 --> 00:27:15,519
thing with the graph filter if we didn't

00:27:12,369 --> 00:27:17,350
want to use streaming expressions now to

00:27:15,519 --> 00:27:19,029
use this all of the data would have to

00:27:17,350 --> 00:27:21,100
be in the same shard remember because

00:27:19,029 --> 00:27:24,309
we're not going to follow edges and

00:27:21,100 --> 00:27:27,759
cross shards and so this is actually a

00:27:24,309 --> 00:27:29,740
single query that uses parameter

00:27:27,759 --> 00:27:31,960
dereferencing to sort of break it up and

00:27:29,740 --> 00:27:34,779
meet it make it a bit more readable so

00:27:31,960 --> 00:27:39,669
our main query is a graph query or a

00:27:34,779 --> 00:27:43,690
graph filter and the input the root set

00:27:39,669 --> 00:27:45,700
at what its operating on is V and that

00:27:43,690 --> 00:27:47,860
is dollar sign g1 so referring to

00:27:45,700 --> 00:27:51,669
something else g1 is yet another graph

00:27:47,860 --> 00:27:54,580
filter its input is dollar sign Q 1 and

00:27:51,669 --> 00:27:59,769
Q 1 we finally get to a real root set

00:27:54,580 --> 00:28:03,610
and that is user yannick and rating 5 so

00:27:59,769 --> 00:28:07,690
that's the review that I the books that

00:28:03,610 --> 00:28:10,210
I liked and so then back to G 1 we walk

00:28:07,690 --> 00:28:13,720
from books to book and so that selects

00:28:10,210 --> 00:28:19,659
all the reviews but we select all the

00:28:13,720 --> 00:28:21,909
reviews of those books we specify a

00:28:19,659 --> 00:28:25,059
traversal filter though and so the

00:28:21,909 --> 00:28:28,899
rating has to be high enough and so then

00:28:25,059 --> 00:28:31,360
you know the output is all reviews of

00:28:28,899 --> 00:28:35,169
those same books and then we go finally

00:28:31,360 --> 00:28:37,210
to the first graph our original Q

00:28:35,169 --> 00:28:40,539
original graph query where we walk from

00:28:37,210 --> 00:28:45,009
user to user and that basically does you

00:28:40,539 --> 00:28:47,230
know finds all reviews by those same

00:28:45,009 --> 00:28:48,940
users from at the output of the previous

00:28:47,230 --> 00:28:51,340
graph query

00:28:48,940 --> 00:28:52,780
but again we passed the same traversal

00:28:51,340 --> 00:28:55,210
filter it has done a high enough rating

00:28:52,780 --> 00:28:57,700
anyway if you execute this whole thing

00:28:55,210 --> 00:28:59,980
you'll get the same output and why you

00:28:57,700 --> 00:29:01,330
might want to do the graph filter is

00:28:59,980 --> 00:29:04,060
that you know it does all its

00:29:01,330 --> 00:29:08,740
computation internally and so it's going

00:29:04,060 --> 00:29:11,730
to be faster so if you put annex

00:29:08,740 --> 00:29:14,140
creaming expression into the solar admin

00:29:11,730 --> 00:29:16,510
you can execute it from there you see

00:29:14,140 --> 00:29:18,220
the output it'll also give you like this

00:29:16,510 --> 00:29:21,280
little logical diagram this is not a

00:29:18,220 --> 00:29:27,760
diagram of the results it's a diagram of

00:29:21,280 --> 00:29:30,550
the query structure itself we have a few

00:29:27,760 --> 00:29:32,380
more graph expressions the first one is

00:29:30,550 --> 00:29:34,090
shortest paths or you can just give two

00:29:32,380 --> 00:29:36,040
nodes and we'll do a breadth-first

00:29:34,090 --> 00:29:38,950
search to find the shortest path between

00:29:36,040 --> 00:29:41,440
the two nodes score nodes is an

00:29:38,950 --> 00:29:44,710
interesting one and right from the basis

00:29:41,440 --> 00:29:49,060
for an actual better recommendation

00:29:44,710 --> 00:29:51,850
system and so if like say that you put

00:29:49,060 --> 00:29:54,250
an item in a shopping basket right and

00:29:51,850 --> 00:29:57,280
you want to know what other items should

00:29:54,250 --> 00:30:00,220
I suggest to the user and so the score

00:29:57,280 --> 00:30:03,370
nodes has a tf-idf inspired scoring

00:30:00,220 --> 00:30:05,050
system if you remember tf-idf TF is

00:30:03,370 --> 00:30:07,480
simply well you know how many times does

00:30:05,050 --> 00:30:13,030
this search term appear in the document

00:30:07,480 --> 00:30:14,920
but IDF adds this sense of importance

00:30:13,030 --> 00:30:16,660
for you know when you have multiple

00:30:14,920 --> 00:30:18,280
terms which one is more important than

00:30:16,660 --> 00:30:20,440
others and so if you're searching for

00:30:18,280 --> 00:30:23,040
something like blue whale in a large

00:30:20,440 --> 00:30:25,720
corpus IDF inverse document frequency

00:30:23,040 --> 00:30:28,570
will give more weighting to the rarer

00:30:25,720 --> 00:30:31,840
term and so if you see blue whale you're

00:30:28,570 --> 00:30:34,780
like well probably whale is the more

00:30:31,840 --> 00:30:36,520
important term than blue because blue is

00:30:34,780 --> 00:30:40,600
just going to be just more prevalent in

00:30:36,520 --> 00:30:44,080
you know a big corpus overall and so how

00:30:40,600 --> 00:30:47,620
the score nodes works is it wraps the

00:30:44,080 --> 00:30:51,300
gather nodes and that calculates a

00:30:47,620 --> 00:30:54,630
co-current count how many times every

00:30:51,300 --> 00:30:57,690
item appears with this other item right

00:30:54,630 --> 00:31:02,860
but it also adds in this element of

00:30:57,690 --> 00:31:04,990
rarity so if something appears a lot in

00:31:02,860 --> 00:31:06,790
lot of shopping baskets and something

00:31:04,990 --> 00:31:10,210
only piers a few times it's going to

00:31:06,790 --> 00:31:17,200
wait the rarer one more all else being

00:31:10,210 --> 00:31:19,720
considered equal so another thing we can

00:31:17,200 --> 00:31:23,350
do is when we have our whole streaming

00:31:19,720 --> 00:31:25,090
expression that we built up if instead

00:31:23,350 --> 00:31:28,210
of passing it to the stream for

00:31:25,090 --> 00:31:30,549
execution if we pass it to graph graph

00:31:28,210 --> 00:31:33,010
we'll do the whole execution but instead

00:31:30,549 --> 00:31:36,910
of outputting the tuple list it will

00:31:33,010 --> 00:31:39,490
output graph ml and which is just like

00:31:36,910 --> 00:31:41,230
an XML standard for representing graphs

00:31:39,490 --> 00:31:42,940
and then what you can do with that

00:31:41,230 --> 00:31:45,850
there's a number of tools that can read

00:31:42,940 --> 00:31:51,460
that you can import it into Getty for

00:31:45,850 --> 00:31:55,830
visualization or more analysis all right

00:31:51,460 --> 00:32:00,130
so now let's look at some of the same

00:31:55,830 --> 00:32:02,530
type of computations or things we can do

00:32:00,130 --> 00:32:08,220
with the JSON facet API over the same

00:32:02,530 --> 00:32:11,530
set of data so the JSON facet API is

00:32:08,220 --> 00:32:14,020
also its types of graph operators or

00:32:11,530 --> 00:32:16,929
joint operators are currently not going

00:32:14,020 --> 00:32:19,720
to cross any edges and so for this what

00:32:16,929 --> 00:32:21,940
we want to do is index books and reviews

00:32:19,720 --> 00:32:25,030
into the same collection that doesn't

00:32:21,940 --> 00:32:27,040
mean we have to throw away scalability

00:32:25,030 --> 00:32:30,090
entirely we can still have multiple

00:32:27,040 --> 00:32:33,120
shards we just need to make sure that

00:32:30,090 --> 00:32:36,010
the book and all of its reviews are

00:32:33,120 --> 00:32:39,250
indexed onto the same shard we can still

00:32:36,010 --> 00:32:41,020
have multiple shards and that will just

00:32:39,250 --> 00:32:43,510
eliminate if we're just joining between

00:32:41,020 --> 00:32:45,010
and traversing between a book and its

00:32:43,510 --> 00:32:47,309
reviews back and forth that'll just

00:32:45,010 --> 00:32:50,650
ensure that there's no cross shard edges

00:32:47,309 --> 00:32:54,130
so the composite ID router is really the

00:32:50,650 --> 00:32:56,110
easiest way to do this and it's actually

00:32:54,130 --> 00:32:57,820
the default in solar cloud so if you

00:32:56,110 --> 00:33:00,280
just create a new collection you'll get

00:32:57,820 --> 00:33:03,460
the composite ID router by default and

00:33:00,280 --> 00:33:05,440
so how this works to co-locate documents

00:33:03,460 --> 00:33:09,640
together is you just use a common prefix

00:33:05,440 --> 00:33:13,120
in the ID field and so if we just have

00:33:09,640 --> 00:33:15,280
ID book 1 for example we compute a hash

00:33:13,120 --> 00:33:17,090
and you know there's a hash ring that

00:33:15,280 --> 00:33:21,809
falls somewhere on the hatch

00:33:17,090 --> 00:33:25,850
but if the ID has a bang in it then it

00:33:21,809 --> 00:33:30,150
will take the first part of the ID and

00:33:25,850 --> 00:33:32,490
just use the top 16 bits by default and

00:33:30,150 --> 00:33:35,520
then take the second part and use that

00:33:32,490 --> 00:33:38,250
for the second set of 16 bits the lower

00:33:35,520 --> 00:33:41,670
set of 16 bits and so what that does is

00:33:38,250 --> 00:33:45,750
if the book ends up on the hash ring say

00:33:41,670 --> 00:33:49,380
here then all the reviews will end up

00:33:45,750 --> 00:33:53,790
surrounding it into the same 16 bit

00:33:49,380 --> 00:33:55,650
block so the same 65536 of the hash ring

00:33:53,790 --> 00:33:57,559
and so what that means if you have less

00:33:55,650 --> 00:34:00,330
than 65536

00:33:57,559 --> 00:34:02,850
shards which pretty much everybody does

00:34:00,330 --> 00:34:08,460
they think you're guaranteed that all

00:34:02,850 --> 00:34:10,169
those documents will be co-located so

00:34:08,460 --> 00:34:12,780
this is just a little refresher on the

00:34:10,169 --> 00:34:15,600
JSON facet API and how we some

00:34:12,780 --> 00:34:18,060
terminology how we think about it so we

00:34:15,600 --> 00:34:22,649
start off with a root domain the root

00:34:18,060 --> 00:34:24,780
domain is the set of documents normally

00:34:22,649 --> 00:34:28,770
that just match our base query and the

00:34:24,780 --> 00:34:32,340
filters and so then a facet command

00:34:28,770 --> 00:34:35,490
it takes a as input and it just produces

00:34:32,340 --> 00:34:38,429
more domains as output so a field facet

00:34:35,490 --> 00:34:40,980
is going to create new domains or we

00:34:38,429 --> 00:34:43,950
also call them facet buckets based on

00:34:40,980 --> 00:34:48,629
unique values in a field but we can also

00:34:43,950 --> 00:34:50,310
do it based on ranges or queries so we

00:34:48,629 --> 00:34:53,280
also have the you know the concept of

00:34:50,310 --> 00:34:55,889
sub facets so we've taken our root

00:34:53,280 --> 00:34:59,940
domain broken it up by you know facet a

00:34:55,889 --> 00:35:03,090
and facet B but facet C is a sub facet

00:34:59,940 --> 00:35:06,480
of facet B and so what that means is

00:35:03,090 --> 00:35:08,430
that for every domain produced by facet

00:35:06,480 --> 00:35:11,040
command B we're going to run that

00:35:08,430 --> 00:35:13,230
through facet command C to produce a new

00:35:11,040 --> 00:35:16,140
set of domains that normally you know

00:35:13,230 --> 00:35:19,290
further split things up so you know this

00:35:16,140 --> 00:35:21,540
could be you know we could first you

00:35:19,290 --> 00:35:23,850
know categorize by for doing cars like

00:35:21,540 --> 00:35:25,770
by make or model and then maybe by color

00:35:23,850 --> 00:35:28,130
you know however you want to like split

00:35:25,770 --> 00:35:28,130
things up

00:35:30,450 --> 00:35:40,540
so here's an example of a JSON facet API

00:35:34,590 --> 00:35:43,150
on our books data and so our root query

00:35:40,540 --> 00:35:45,010
is cat : star I'm using cat as the

00:35:43,150 --> 00:35:46,510
category field that's a genre field

00:35:45,010 --> 00:35:50,230
essentially so it's values are going to

00:35:46,510 --> 00:35:55,119
be like you know sci-fi fantasy romance

00:35:50,230 --> 00:35:56,859
mystery whatever but I've only read

00:35:55,119 --> 00:35:58,270
really sci-fi and fantasy books and so

00:35:56,859 --> 00:36:04,450
that's why there's only two buckets and

00:35:58,270 --> 00:36:07,030
all these examples so number one we want

00:36:04,450 --> 00:36:10,180
to know the number of unique authors and

00:36:07,030 --> 00:36:13,359
so then write in our root facet bucket

00:36:10,180 --> 00:36:17,619
we're asking for hll of the author field

00:36:13,359 --> 00:36:18,640
hll just stands for Hyper log-log and if

00:36:17,619 --> 00:36:20,710
you haven't heard that before it's

00:36:18,640 --> 00:36:24,190
really just a distributed cardinality

00:36:20,710 --> 00:36:28,480
algorithm and it is if you go cross

00:36:24,190 --> 00:36:32,020
shards it is an estimate and then we're

00:36:28,480 --> 00:36:35,619
asking for a genre facet the type is a

00:36:32,020 --> 00:36:37,780
terms facet and the field is cat and so

00:36:35,619 --> 00:36:41,290
that will basically make facet buckets

00:36:37,780 --> 00:36:43,869
or subdomains based on unique values in

00:36:41,290 --> 00:36:46,930
the cat field and then in our response

00:36:43,869 --> 00:36:49,750
we have a count of 13 that simply means

00:36:46,930 --> 00:36:52,450
that there were 13 books coming into the

00:36:49,750 --> 00:36:55,000
root domain we have our numbers that was

00:36:52,450 --> 00:36:57,390
calculated to be 5 and then we have our

00:36:55,000 --> 00:37:00,280
genre facet and we have a bucket list

00:36:57,390 --> 00:37:02,320
and the bucket list has well now we can

00:37:00,280 --> 00:37:08,859
see we have seven fantasy books and six

00:37:02,320 --> 00:37:13,000
scifi books so jumping ahead a little

00:37:08,859 --> 00:37:15,190
bit I'm going to go over domain changes

00:37:13,000 --> 00:37:19,480
and join so this is something that was

00:37:15,190 --> 00:37:21,900
just very recently committed and so

00:37:19,480 --> 00:37:25,119
let's say that we start off with our

00:37:21,900 --> 00:37:31,210
genres facet again it's a terms facet on

00:37:25,119 --> 00:37:35,740
the cat field and so now we stick in

00:37:31,210 --> 00:37:38,460
this new part a sub facet in blue and we

00:37:35,740 --> 00:37:42,550
say okay we're doing a reviews facet

00:37:38,460 --> 00:37:45,970
type is query now a query facet normally

00:37:42,550 --> 00:37:47,710
takes the input domain applies a filter

00:37:45,970 --> 00:37:49,990
and that's the output domain so it's a

00:37:47,710 --> 00:37:51,960
single single domain two single domain

00:37:49,990 --> 00:37:54,670
just like essentially applying a filter

00:37:51,960 --> 00:37:57,010
in this case we're not even specifying a

00:37:54,670 --> 00:37:59,080
filter so the input domain the output

00:37:57,010 --> 00:38:01,300
domain is going to be the same as the

00:37:59,080 --> 00:38:02,890
input domain but we're doing something

00:38:01,300 --> 00:38:06,430
different in this case that you can do

00:38:02,890 --> 00:38:08,710
with any facet now and that is we're

00:38:06,430 --> 00:38:11,740
specifying a domain change and the

00:38:08,710 --> 00:38:14,320
domain change happens before we facet

00:38:11,740 --> 00:38:20,160
that domain and the domain change is

00:38:14,320 --> 00:38:23,230
saying join from ID to book and so

00:38:20,160 --> 00:38:24,460
really this is sort of more like if

00:38:23,230 --> 00:38:27,820
you're familiar with it

00:38:24,460 --> 00:38:31,810
sollars pseudo join or a single step

00:38:27,820 --> 00:38:34,480
graph traversal and so we're going from

00:38:31,810 --> 00:38:38,500
the ID field remember so we're faceting

00:38:34,480 --> 00:38:40,900
by we're starting with books and so we

00:38:38,500 --> 00:38:45,070
have the ID field of the book and then

00:38:40,900 --> 00:38:47,860
we're moving to documents that have that

00:38:45,070 --> 00:38:49,240
value in the book field which means it's

00:38:47,860 --> 00:38:52,090
going to be the reviews if we remember

00:38:49,240 --> 00:38:55,570
our schema and so essentially what if

00:38:52,090 --> 00:38:57,160
what this is going to do is the

00:38:55,570 --> 00:39:00,010
top-level thing is going to produce a

00:38:57,160 --> 00:39:02,770
bucket for fantasy books and that domain

00:39:00,010 --> 00:39:06,760
change is going to switch to all reviews

00:39:02,770 --> 00:39:08,380
for those fantasy books and so then we

00:39:06,760 --> 00:39:10,540
get our additional data on the

00:39:08,380 --> 00:39:12,460
right-hand side under reviews it's

00:39:10,540 --> 00:39:15,070
colored in blue and then we can see for

00:39:12,460 --> 00:39:17,620
our center 7 fantasy books we have 7

00:39:15,070 --> 00:39:24,340
reviews and for our 6 scifi books we

00:39:17,620 --> 00:39:26,740
have 5 reviews and so now we can ask for

00:39:24,340 --> 00:39:30,130
some additional data once we've done

00:39:26,740 --> 00:39:33,520
that domain switch and so we add an

00:39:30,130 --> 00:39:35,980
additional sub facet to the reviews

00:39:33,520 --> 00:39:39,220
facet and we say he'll give me the

00:39:35,980 --> 00:39:40,540
average rating for that bucket and then

00:39:39,220 --> 00:39:42,430
you see the you know you get the average

00:39:40,540 --> 00:39:43,960
rating as well just stuck in there so

00:39:42,430 --> 00:39:47,260
that's one of the nice things about the

00:39:43,960 --> 00:39:49,540
JSON facet API is the the structure is

00:39:47,260 --> 00:39:51,670
more canonical than it is with the old

00:39:49,540 --> 00:39:53,770
faceting stuff in that you know if you

00:39:51,670 --> 00:39:55,730
ask for additional information the basic

00:39:53,770 --> 00:39:59,619
structure of the response does not

00:39:55,730 --> 00:39:59,619
changed just add some additional things

00:40:01,359 --> 00:40:05,960
so say that we want to define who gives

00:40:04,100 --> 00:40:10,369
the highest rating perch on rrah

00:40:05,960 --> 00:40:12,290
so to answer that what we change we

00:40:10,369 --> 00:40:15,530
change the reviews facet from a query

00:40:12,290 --> 00:40:19,220
facet to a terms facet on the user field

00:40:15,530 --> 00:40:21,710
and we're specifying that we want to

00:40:19,220 --> 00:40:24,290
sort by the ratings descending and get

00:40:21,710 --> 00:40:26,840
the top three but we still have our

00:40:24,290 --> 00:40:29,300
domain change in there so that happens

00:40:26,840 --> 00:40:33,320
first so for all of our fantasy you know

00:40:29,300 --> 00:40:36,980
our fantasy buckets say we switch to the

00:40:33,320 --> 00:40:40,400
set of fantasy reviews we and then we

00:40:36,980 --> 00:40:42,230
break those up by user until then on the

00:40:40,400 --> 00:40:44,660
right hand side you can see that our

00:40:42,230 --> 00:40:47,210
reviews facet now has a bucket list and

00:40:44,660 --> 00:40:48,740
the bucket list is you know the ratings

00:40:47,210 --> 00:40:51,350
the average ratings that we asked for

00:40:48,740 --> 00:40:57,020
and the buckets are sorted by that we've

00:40:51,350 --> 00:40:59,600
taken the top three and just another

00:40:57,020 --> 00:41:01,070
like random thing that you know that I

00:40:59,600 --> 00:41:04,340
thought of that you might want to do if

00:41:01,070 --> 00:41:05,240
you wanted average rating trends over

00:41:04,340 --> 00:41:08,060
time

00:41:05,240 --> 00:41:10,640
well then instead of doing breaking it

00:41:08,060 --> 00:41:13,520
down by user you could change the type

00:41:10,640 --> 00:41:16,730
of facet of a reviews facet to arrange

00:41:13,520 --> 00:41:18,020
facet on a review date field and you

00:41:16,730 --> 00:41:19,580
know you can give the start in the end

00:41:18,020 --> 00:41:21,800
and the gap and that's just going to

00:41:19,580 --> 00:41:24,170
create buckets based on time range and

00:41:21,800 --> 00:41:25,760
give the average rating for each bucket

00:41:24,170 --> 00:41:27,980
so you can sort of see whether you know

00:41:25,760 --> 00:41:32,420
fantasy or sci-fi or whatever genre is

00:41:27,980 --> 00:41:33,890
trending over time so this data is

00:41:32,420 --> 00:41:36,950
actually the first one that I made up

00:41:33,890 --> 00:41:39,700
because I didn't originally stick in any

00:41:36,950 --> 00:41:41,960
review date and so so for the response

00:41:39,700 --> 00:41:44,000
that's a real response but there were

00:41:41,960 --> 00:41:47,660
all zeroes and so I just like typed in

00:41:44,000 --> 00:41:49,220
some random answers maybe for some

00:41:47,660 --> 00:41:55,609
future version of the talk I'll go back

00:41:49,220 --> 00:41:59,660
and fill in a real review date yeah so

00:41:55,609 --> 00:42:02,210
and then one final slide wrapping it up

00:41:59,660 --> 00:42:04,250
on streaming expressions versus JSON

00:42:02,210 --> 00:42:06,230
facets since these are like two

00:42:04,250 --> 00:42:09,140
different ways that you can calculate

00:42:06,230 --> 00:42:12,440
things so uh

00:42:09,140 --> 00:42:15,350
on the JSON facet API it's it's more

00:42:12,440 --> 00:42:18,410
focused on web-scale interactive

00:42:15,350 --> 00:42:20,630
responses so remember cover streaming

00:42:18,410 --> 00:42:24,230
expressions how you can automatically

00:42:20,630 --> 00:42:26,840
paralyze things to really use the whole

00:42:24,230 --> 00:42:29,510
cluster resources to compute your answer

00:42:26,840 --> 00:42:32,360
faster but if you have thousands of

00:42:29,510 --> 00:42:35,110
users at once hitting your cluster

00:42:32,360 --> 00:42:39,860
that's going to be a very expensive

00:42:35,110 --> 00:42:41,900
solution right you really want to use

00:42:39,860 --> 00:42:43,550
also if there's if it's not just a

00:42:41,900 --> 00:42:45,320
couple analysts like banging away at a

00:42:43,550 --> 00:42:50,150
keyboard you really want to use your

00:42:45,320 --> 00:42:52,730
resources efficiently as well it does

00:42:50,150 --> 00:42:56,030
have the JSON facet API has like tighter

00:42:52,730 --> 00:42:57,830
integration with search components and

00:42:56,030 --> 00:42:59,900
solar it's just another search component

00:42:57,830 --> 00:43:01,700
so for example if you also want

00:42:59,900 --> 00:43:03,560
highlighting back at the same time or

00:43:01,700 --> 00:43:07,220
anything like that right now it's a

00:43:03,560 --> 00:43:09,770
better solution and like I said it is

00:43:07,220 --> 00:43:12,920
more oriented towards very quickly

00:43:09,770 --> 00:43:17,270
finding top ends of things top and you

00:43:12,920 --> 00:43:19,520
know facet buckets top end documents and

00:43:17,270 --> 00:43:20,960
it has currently has a couple

00:43:19,520 --> 00:43:23,000
capabilities that the extreme

00:43:20,960 --> 00:43:26,390
expressions don't have yet like the

00:43:23,000 --> 00:43:29,750
ability to do block join and nested

00:43:26,390 --> 00:43:32,390
document support so streaming

00:43:29,750 --> 00:43:35,630
expressions though I think hold a lot of

00:43:32,390 --> 00:43:39,410
promise they're more general-purpose

00:43:35,630 --> 00:43:41,450
more larger in scope because you can

00:43:39,410 --> 00:43:43,430
wrap streams within streams to do pretty

00:43:41,450 --> 00:43:46,900
much anything that really doesn't say

00:43:43,430 --> 00:43:49,640
you know how long it's going to take but

00:43:46,900 --> 00:43:51,260
but it is very powerful in the past

00:43:49,640 --> 00:43:53,330
we've like you know really only focused

00:43:51,260 --> 00:43:55,250
on things we could do very quickly and

00:43:53,330 --> 00:43:58,700
streaming expressions is much more

00:43:55,250 --> 00:44:00,290
general than that and so you start one I

00:43:58,700 --> 00:44:03,550
guess one way to think about it is like

00:44:00,290 --> 00:44:06,860
you know OLAP versus OLTP they obviously

00:44:03,550 --> 00:44:10,790
overlap a lot but it's in general that's

00:44:06,860 --> 00:44:12,140
sort of how they differ so it's also I

00:44:10,790 --> 00:44:14,120
think I mentioned before it's not

00:44:12,140 --> 00:44:20,360
streaming expressions are not really

00:44:14,120 --> 00:44:21,980
tied to documents so you can get data

00:44:20,360 --> 00:44:22,950
from an external data source and treat

00:44:21,980 --> 00:44:26,970
it like just any other

00:44:22,950 --> 00:44:28,980
dream do computations joins on it we

00:44:26,970 --> 00:44:31,440
also have things like update streams

00:44:28,980 --> 00:44:33,690
where you can stream get get your

00:44:31,440 --> 00:44:37,170
results and stream them into an index

00:44:33,690 --> 00:44:39,710
into another solar collection and you

00:44:37,170 --> 00:44:44,220
that as a data source in the future

00:44:39,710 --> 00:44:45,900
machine learning streams look like just

00:44:44,220 --> 00:44:49,589
we just started adding a whole bunch

00:44:45,900 --> 00:44:51,480
bunch of math operations so you can do

00:44:49,589 --> 00:44:54,900
things like convolution and cross

00:44:51,480 --> 00:44:58,530
correlation and you can do some things

00:44:54,900 --> 00:45:01,589
like exact a calculate exact cardinality

00:44:58,530 --> 00:45:03,690
like the hll is an estimate once you

00:45:01,589 --> 00:45:06,030
start crossing shards but if you're

00:45:03,690 --> 00:45:08,130
streaming all of the data past a single

00:45:06,030 --> 00:45:12,359
node you can actually do exact

00:45:08,130 --> 00:45:14,960
cardinality counts if you care we have

00:45:12,359 --> 00:45:17,400
distributed joins distributed graph and

00:45:14,960 --> 00:45:21,450
increasingly - it's working with the

00:45:17,400 --> 00:45:23,400
JSON facet API and so you know one

00:45:21,450 --> 00:45:25,740
reason why you'd want to use say

00:45:23,400 --> 00:45:28,290
streaming expressions over like some

00:45:25,740 --> 00:45:31,500
spark stuff in general is that we can

00:45:28,290 --> 00:45:34,079
push down more computation into the

00:45:31,500 --> 00:45:36,089
nodes and really use the power of the

00:45:34,079 --> 00:45:38,640
Lucene indexes that we have you know

00:45:36,089 --> 00:45:43,250
just like essentially smarter endpoints

00:45:38,640 --> 00:45:47,250
do more calculations first all in memory

00:45:43,250 --> 00:45:47,670
with the fast indexes and then it's you

00:45:47,250 --> 00:45:50,250
know

00:45:47,670 --> 00:45:52,589
delay as much as possible shipping out

00:45:50,250 --> 00:45:56,720
like the results to do more generic

00:45:52,589 --> 00:45:56,720
computations in streaming expressions

00:45:58,280 --> 00:46:06,150
yeah alright so that's the end of my

00:46:02,940 --> 00:46:08,160
presentation I think we have some time

00:46:06,150 --> 00:46:18,530
for ya we have a couple minutes for

00:46:08,160 --> 00:46:18,530
questions yeah

00:46:19,660 --> 00:46:28,060
screaming which I get right shortest

00:46:23,360 --> 00:46:28,060
path query with Auto external data

00:46:30,380 --> 00:46:34,660
like mixing the filler de Ternay I

00:46:35,050 --> 00:46:39,320
that's a good question and I had it

00:46:38,120 --> 00:46:41,330
myself it's something I haven't

00:46:39,320 --> 00:46:43,700
investigated yet a lot of the time it

00:46:41,330 --> 00:46:47,450
seems you know it's we're visiting nodes

00:46:43,700 --> 00:46:52,700
and then the gather I think it currently

00:46:47,450 --> 00:46:55,520
is based on picking out

00:46:52,700 --> 00:47:00,080
I think it's currently based on search

00:46:55,520 --> 00:47:02,000
within solar itself so currently I'm not

00:47:00,080 --> 00:47:05,240
sure if that will work

00:47:02,000 --> 00:47:07,670
for many parts of it but it there's no

00:47:05,240 --> 00:47:09,890
reason it can't though it should so I

00:47:07,670 --> 00:47:11,180
think that if it doesn't so the places

00:47:09,890 --> 00:47:16,180
where we don't currently support it I

00:47:11,180 --> 00:47:16,180
think we will in the future yeah

00:47:17,350 --> 00:47:20,860
any other questions

00:47:23,370 --> 00:47:53,460
yes so yeah so the question was you know

00:47:50,330 --> 00:47:59,390
streaming can access faceting in the

00:47:53,460 --> 00:47:59,390
future will faceting access streaming

00:47:59,720 --> 00:48:14,870
maybe yeah maybe um I think I think to

00:48:11,270 --> 00:48:18,420
not it's it's a we can't do it currently

00:48:14,870 --> 00:48:19,920
the JSON facet API can't like at some

00:48:18,420 --> 00:48:22,890
level all of a sudden kick off a

00:48:19,920 --> 00:48:24,720
streaming expression it's the JSON API

00:48:22,890 --> 00:48:27,360
is really like I said it's oriented

00:48:24,720 --> 00:48:29,850
towards like being really fast and it's

00:48:27,360 --> 00:48:32,250
like currently like two-phase so like it

00:48:29,850 --> 00:48:34,290
you know phase number one gathered the

00:48:32,250 --> 00:48:36,570
top buckets phase number twos do

00:48:34,290 --> 00:48:41,700
refinements so you get actually accurate

00:48:36,570 --> 00:48:45,600
counts in that second phase is even that

00:48:41,700 --> 00:48:47,790
second phase is optional but it's so

00:48:45,600 --> 00:48:49,350
going in in any of those places going

00:48:47,790 --> 00:48:51,720
out and kicking off a full streaming

00:48:49,350 --> 00:48:56,700
expression seems really heavyweight but

00:48:51,720 --> 00:48:58,890
just from just four I would say yes in

00:48:56,700 --> 00:49:00,330
the future I don't see any reason why we

00:48:58,890 --> 00:49:01,020
won't be able to it just hasn't been

00:49:00,330 --> 00:49:02,880
done yet

00:49:01,020 --> 00:49:05,010
and I'm wondering I'm not sure exactly

00:49:02,880 --> 00:49:06,990
what the interface will look like maybe

00:49:05,010 --> 00:49:12,120
we'll wrap up streaming expressions into

00:49:06,990 --> 00:49:14,190
a type of query no I'm sorry that

00:49:12,120 --> 00:49:16,680
wouldn't work so the problem Jay some

00:49:14,190 --> 00:49:19,860
facets today are really oriented towards

00:49:16,680 --> 00:49:23,250
documents every domain sub domain that

00:49:19,860 --> 00:49:26,100
you get out those represented as bit

00:49:23,250 --> 00:49:29,160
sets about you know and that they mapped

00:49:26,100 --> 00:49:31,770
to documents in the index and so we drew

00:49:29,160 --> 00:49:35,160
to actually really utilize streaming

00:49:31,770 --> 00:49:37,240
within that we'd have to like expand the

00:49:35,160 --> 00:49:39,790
internals of JSON faceting

00:49:37,240 --> 00:49:42,040
to work on values because at some point

00:49:39,790 --> 00:49:43,720
you do go to a streaming expression it's

00:49:42,040 --> 00:49:45,610
pulling in external data you're just

00:49:43,720 --> 00:49:47,619
getting tuples now these don't have any

00:49:45,610 --> 00:49:51,030
correlation to what's in the index

00:49:47,619 --> 00:49:56,110
anymore and so that's more like generic

00:49:51,030 --> 00:49:58,930
faceting and so while yeah so maybe it

00:49:56,110 --> 00:50:00,520
doesn't make so much sense JSON faceting

00:49:58,930 --> 00:50:08,310
would have to expand a lot to actually

00:50:00,520 --> 00:50:08,310
make that work great any other questions

00:50:09,480 --> 00:50:16,949
all right cool Thanks

00:50:13,010 --> 00:50:16,949

YouTube URL: https://www.youtube.com/watch?v=AG0eKcRyEeY


