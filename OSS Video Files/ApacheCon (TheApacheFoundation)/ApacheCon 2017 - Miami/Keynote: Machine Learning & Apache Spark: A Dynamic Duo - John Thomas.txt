Title: Keynote: Machine Learning & Apache Spark: A Dynamic Duo - John Thomas
Publication date: 2017-05-18
Playlist: ApacheCon 2017 - Miami
Description: 
	Keynote: Machine Learning & Apache Spark: A Dynamic Duo - John Thomas, Distinguished Engineer and Director, IBM Analytics

The Machine Learning revolution is underway and is changing industries and delivering outcomes that were unimaginable a few years ago. Learn how Apache Spark and other related projects are being used by innovative companies to remake products and services and enabling data-driven decision making.

About John Thomas
John J. Thomas is a Distinguished Engineer and Director in IBMâ€™s Analytics business. His experience ranges from enterprise architecture and competitive strategy to worldwide client facing engagements. He currently leads a technology acceleration team that focuses on topics like Machine Learning, Data Science and Blockchain Analytics.
Captions: 
	00:00:00,410 --> 00:00:07,770
hello folks how was everyone doing but

00:00:03,360 --> 00:00:09,450
gone from so far very good so I was

00:00:07,770 --> 00:00:12,179
listening to the bio format for a

00:00:09,450 --> 00:00:13,860
sous-vide they play the speaker and it

00:00:12,179 --> 00:00:16,500
looks like he went through like ten

00:00:13,860 --> 00:00:18,420
different companies in ten years and I'm

00:00:16,500 --> 00:00:20,910
thinking you know I spent the last 18

00:00:18,420 --> 00:00:24,119
years an idea is that a good thing or a

00:00:20,910 --> 00:00:27,779
bad thing but then he said one thing

00:00:24,119 --> 00:00:29,310
which really was interesting he said no

00:00:27,779 --> 00:00:32,489
offense but you guys have suddenly

00:00:29,310 --> 00:00:35,100
because started becoming cool now okay

00:00:32,489 --> 00:00:36,540
I'll take that because you know you guys

00:00:35,100 --> 00:00:38,550
are doing open sores you guys are

00:00:36,540 --> 00:00:43,410
catering to developers I'll take that

00:00:38,550 --> 00:00:45,690
thank you to deep so um in my role now I

00:00:43,410 --> 00:00:48,629
actually file a lot I meet with

00:00:45,690 --> 00:00:51,899
customers and work with them on real

00:00:48,629 --> 00:00:56,670
machine learning use cases so going

00:00:51,899 --> 00:00:59,250
beyond the theoretical stuff to actual

00:00:56,670 --> 00:01:01,739
implementations and I want to share some

00:00:59,250 --> 00:01:05,670
perspectives with you and also what I

00:01:01,739 --> 00:01:08,549
see in terms of a huge role that open

00:01:05,670 --> 00:01:14,159
source is playing in advancing the

00:01:08,549 --> 00:01:15,869
technologies now I'm sure you know

00:01:14,159 --> 00:01:18,450
pretty much everyone knows the

00:01:15,869 --> 00:01:21,750
fundamentals but I'll take maybe a

00:01:18,450 --> 00:01:24,150
minute what is machine learning there is

00:01:21,750 --> 00:01:25,770
the definition on Wikipedia that goes

00:01:24,150 --> 00:01:27,720
all the way back to 1959

00:01:25,770 --> 00:01:30,090
you know in a computer that learn

00:01:27,720 --> 00:01:33,720
without being explicitly programmed so

00:01:30,090 --> 00:01:36,930
the auto family built the the checker

00:01:33,720 --> 00:01:38,520
program right at IBM you know computers

00:01:36,930 --> 00:01:43,290
that learn automatically sounds kind of

00:01:38,520 --> 00:01:46,590
like Skynet so I tend to describe this

00:01:43,290 --> 00:01:48,450
in my terms I say you know it's

00:01:46,590 --> 00:01:51,270
basically about understanding patterns

00:01:48,450 --> 00:01:53,250
and data using algorithms now it goes a

00:01:51,270 --> 00:01:54,390
little bit beyond that you may have

00:01:53,250 --> 00:01:56,640
structured data you may have

00:01:54,390 --> 00:01:59,070
unstructured data and you're essentially

00:01:56,640 --> 00:02:02,040
trying to find patterns and inside the

00:01:59,070 --> 00:02:04,409
data and then maybe in certain cases use

00:02:02,040 --> 00:02:05,700
those and use that understanding to

00:02:04,409 --> 00:02:08,099
predict what's going to happen so you

00:02:05,700 --> 00:02:12,060
know about supervised and unsupervised

00:02:08,099 --> 00:02:13,940
supervised when you have labeled data

00:02:12,060 --> 00:02:16,940
with known outcomes

00:02:13,940 --> 00:02:18,320
as far already happened here does your

00:02:16,940 --> 00:02:20,570
historical data already have that

00:02:18,320 --> 00:02:22,100
information what across the law for

00:02:20,570 --> 00:02:24,680
successful so if you have that

00:02:22,100 --> 00:02:26,480
historical known outcome data you use

00:02:24,680 --> 00:02:28,040
that a create a model understand

00:02:26,480 --> 00:02:30,080
patterns and use that it predicts and

00:02:28,040 --> 00:02:32,900
then unsupervised where you know you

00:02:30,080 --> 00:02:34,850
don't even know what the outcomes where

00:02:32,900 --> 00:02:36,220
we are asking the algorithms to try and

00:02:34,850 --> 00:02:38,630
understand the relationship between

00:02:36,220 --> 00:02:43,970
different parts of the data understand

00:02:38,630 --> 00:02:47,170
plus okay so that is the basics now as I

00:02:43,970 --> 00:02:50,540
interacted customers what I see is very

00:02:47,170 --> 00:02:52,970
innovative and interesting use cases so

00:02:50,540 --> 00:02:55,010
some of these are obvious ones like in

00:02:52,970 --> 00:02:57,830
the finance industry fraud detection is

00:02:55,010 --> 00:03:00,110
is like the number one thing any bank

00:02:57,830 --> 00:03:02,840
you walk into so the number one thing

00:03:00,110 --> 00:03:07,460
that they want to focus on now going

00:03:02,840 --> 00:03:11,000
from batch scoring of after record

00:03:07,460 --> 00:03:15,050
transactions to in in transaction

00:03:11,000 --> 00:03:16,820
real-time scoring predicting fraud there

00:03:15,050 --> 00:03:20,739
are other very interesting use case of

00:03:16,820 --> 00:03:24,230
healthcare but so using the ABCs and

00:03:20,739 --> 00:03:26,840
predicting you know the chance of

00:03:24,230 --> 00:03:32,510
likelihood of you of a patient becoming

00:03:26,840 --> 00:03:34,520
diabetic preventive interventions trying

00:03:32,510 --> 00:03:38,930
to avoid emergency room visits by

00:03:34,520 --> 00:03:42,760
predicting certain lifestyle outcomes

00:03:38,930 --> 00:03:46,730
these are all happening as we speak

00:03:42,760 --> 00:03:49,610
across across industries I thought I'll

00:03:46,730 --> 00:03:52,489
share a couple of these and also then

00:03:49,610 --> 00:03:54,860
start talking about the role of spark

00:03:52,489 --> 00:03:56,900
that I see happening in here so this is

00:03:54,860 --> 00:03:59,870
a very interesting one this is not a

00:03:56,900 --> 00:04:02,420
traditional industry this is a company

00:03:59,870 --> 00:04:05,959
that works with inner city buildings

00:04:02,420 --> 00:04:08,150
inner city businesses in improving the

00:04:05,959 --> 00:04:09,980
energy efficiency of the building so

00:04:08,150 --> 00:04:12,200
they will work with nonprofit

00:04:09,980 --> 00:04:15,260
organizations churches and the like and

00:04:12,200 --> 00:04:17,209
help those those businesses understand

00:04:15,260 --> 00:04:19,910
the energy profile of their building and

00:04:17,209 --> 00:04:21,380
help optimize it their current model is

00:04:19,910 --> 00:04:23,570
that they will send an engineer or a

00:04:21,380 --> 00:04:26,270
couple of engineers they go out and they

00:04:23,570 --> 00:04:27,060
sit down and they measure and they talk

00:04:26,270 --> 00:04:28,920
and interview

00:04:27,060 --> 00:04:31,050
the superintendent of the building and

00:04:28,920 --> 00:04:32,670
collect a lot of data and then they've

00:04:31,050 --> 00:04:34,800
got a pretty sophisticated model which

00:04:32,670 --> 00:04:39,240
which tries to calculate what is called

00:04:34,800 --> 00:04:39,720
the energy unit the efficiency of the

00:04:39,240 --> 00:04:42,690
building

00:04:39,720 --> 00:04:44,700
EUI and and then it comes back and okay

00:04:42,690 --> 00:04:45,690
how can I improve this and so on there

00:04:44,700 --> 00:04:48,540
is a problem with this

00:04:45,690 --> 00:04:51,230
sending engineers to each and every

00:04:48,540 --> 00:04:54,840
nonprofit is not a very scalable model

00:04:51,230 --> 00:04:57,690
the nonprofits and churches usually

00:04:54,840 --> 00:05:00,480
cannot afford that initial assessment in

00:04:57,690 --> 00:05:01,830
in a few thousand dollars range and they

00:05:00,480 --> 00:05:04,800
want to scale this there's one scale

00:05:01,830 --> 00:05:08,190
this operation now the way they did this

00:05:04,800 --> 00:05:10,080
is and it's and this is an actual thing

00:05:08,190 --> 00:05:10,560
that they have rolled out it's very

00:05:10,080 --> 00:05:13,530
interesting

00:05:10,560 --> 00:05:16,500
instead of spending engineers they make

00:05:13,530 --> 00:05:18,450
a mobile app available to the building

00:05:16,500 --> 00:05:21,570
superintendent so the building

00:05:18,450 --> 00:05:23,940
superintendent has his app on their

00:05:21,570 --> 00:05:26,190
phone and instead of asking and

00:05:23,940 --> 00:05:29,310
answering a bunch of questions you know

00:05:26,190 --> 00:05:30,900
you start the app and the app tells them

00:05:29,310 --> 00:05:33,090
to go outside and take a picture of the

00:05:30,900 --> 00:05:34,200
dual-income outside then go down to the

00:05:33,090 --> 00:05:36,660
basement and take a picture of the

00:05:34,200 --> 00:05:38,370
boiler take a picture of the the heating

00:05:36,660 --> 00:05:40,919
system take a picture of the cooling

00:05:38,370 --> 00:05:44,090
system and all of this information is

00:05:40,919 --> 00:05:46,440
uploaded into into the environment and

00:05:44,090 --> 00:05:48,750
basically pieces of information are

00:05:46,440 --> 00:05:50,550
extracted from these images fed into a

00:05:48,750 --> 00:05:53,160
machine learning models and it comes

00:05:50,550 --> 00:05:55,740
back with a score of what our efficient

00:05:53,160 --> 00:05:57,479
this building is and then if you want

00:05:55,740 --> 00:05:59,340
further intervention if you want the

00:05:57,479 --> 00:06:02,400
company to go out and improve the energy

00:05:59,340 --> 00:06:05,039
efficiency that's when they engage so at

00:06:02,400 --> 00:06:07,200
the fringe at the a at the mobile app

00:06:05,039 --> 00:06:09,330
level now that is using deep learning to

00:06:07,200 --> 00:06:11,340
understand the visual images you know

00:06:09,330 --> 00:06:13,260
things like oh you know that buildings

00:06:11,340 --> 00:06:16,050
made of stone has got large windows and

00:06:13,260 --> 00:06:18,510
it's got a high ceiling to base ratio

00:06:16,050 --> 00:06:20,520
and it does not have a lot of shade on

00:06:18,510 --> 00:06:24,850
the sides these are all being extracted

00:06:20,520 --> 00:06:28,810
out through custom visual recognition

00:06:24,850 --> 00:06:30,910
models and then it builds a set of four

00:06:28,810 --> 00:06:33,850
files or attributes in machine language

00:06:30,910 --> 00:06:35,560
machine learning lingo it is actually

00:06:33,850 --> 00:06:37,330
building out the feature set that is

00:06:35,560 --> 00:06:38,770
going to be used to score against the

00:06:37,330 --> 00:06:40,930
model that has been built and deployed

00:06:38,770 --> 00:06:43,510
coming back with the score very

00:06:40,930 --> 00:06:46,060
interesting use cases next one is

00:06:43,510 --> 00:06:48,970
another interesting one what we call a

00:06:46,060 --> 00:06:51,910
celebrity experience so in here the idea

00:06:48,970 --> 00:06:52,900
is that you have code fans of by the way

00:06:51,910 --> 00:06:55,060
at the mainframe

00:06:52,900 --> 00:06:57,670
it's an IBM keynote right I got to put a

00:06:55,060 --> 00:06:59,410
mainframe up there somewhere but all

00:06:57,670 --> 00:07:01,690
kidding apart you know this is where

00:06:59,410 --> 00:07:04,180
most of the enterprises have your code

00:07:01,690 --> 00:07:06,100
transactional data the question is how

00:07:04,180 --> 00:07:09,400
will you use that data in new and

00:07:06,100 --> 00:07:10,900
interesting ways so what what this

00:07:09,400 --> 00:07:12,970
scenario is about is you know you've got

00:07:10,900 --> 00:07:15,850
a transactional history of God to record

00:07:12,970 --> 00:07:17,530
transaction data there volumes of it so

00:07:15,850 --> 00:07:19,540
you kind of have an understanding of how

00:07:17,530 --> 00:07:22,780
this how customers are using clickers

00:07:19,540 --> 00:07:26,980
but couple that with other beta social

00:07:22,780 --> 00:07:30,310
media data whether data etc and build in

00:07:26,980 --> 00:07:32,740
real time offers for the customer as

00:07:30,310 --> 00:07:34,600
they are transacting with the bank you

00:07:32,740 --> 00:07:36,490
know what we call a celebrity experience

00:07:34,600 --> 00:07:38,380
for us or uses that phrase celebrity

00:07:36,490 --> 00:07:40,540
experience where the end user feels that

00:07:38,380 --> 00:07:43,480
is a celebrity that the business knows

00:07:40,540 --> 00:07:45,340
him personally and is not reading as a

00:07:43,480 --> 00:07:47,620
segment but it's rating him as an

00:07:45,340 --> 00:07:50,860
individual with an understanding of what

00:07:47,620 --> 00:07:53,920
is likes and dislikes are so lots of

00:07:50,860 --> 00:07:56,860
lots of variations of this in the retail

00:07:53,920 --> 00:07:59,530
industry in banking and so on where your

00:07:56,860 --> 00:08:01,860
target offers or provide a customized

00:07:59,530 --> 00:08:03,730
experience for your end user through

00:08:01,860 --> 00:08:05,890
using machine learning

00:08:03,730 --> 00:08:07,510
now machine learning it so does not mean

00:08:05,890 --> 00:08:10,660
so what exactly is different now right

00:08:07,510 --> 00:08:12,610
so I think there are three major forces

00:08:10,660 --> 00:08:14,560
are coming together to make this happen

00:08:12,610 --> 00:08:16,960
if you go about this in reverse order

00:08:14,560 --> 00:08:18,370
from what is on the screen availability

00:08:16,960 --> 00:08:21,220
of different types of data you have

00:08:18,370 --> 00:08:23,350
transactional data all along but now

00:08:21,220 --> 00:08:25,390
when you when you supplement bed with

00:08:23,350 --> 00:08:27,790
other types of data for example fall

00:08:25,390 --> 00:08:30,460
Center records who would have thought

00:08:27,790 --> 00:08:32,800
calls on the record could be a valuable

00:08:30,460 --> 00:08:34,290
it's a pressure flow of information that

00:08:32,800 --> 00:08:37,630
can be used to couple with your

00:08:34,290 --> 00:08:38,680
transactional data social media in a

00:08:37,630 --> 00:08:43,899
Twitter or

00:08:38,680 --> 00:08:46,540
Facebook etc etc blog whatever the your

00:08:43,899 --> 00:08:51,190
customer has put out there in the social

00:08:46,540 --> 00:08:54,700
sphere these things become additional

00:08:51,190 --> 00:08:56,740
data that can be brought into enhancing

00:08:54,700 --> 00:08:59,620
your model now external data may not

00:08:56,740 --> 00:09:01,750
always enrich your model it is the next

00:08:59,620 --> 00:09:06,160
it is typically data scientists job to

00:09:01,750 --> 00:09:08,050
see if adding external data and rich of

00:09:06,160 --> 00:09:10,000
your data enriches your model and

00:09:08,050 --> 00:09:12,550
improve the accuracy or performance of

00:09:10,000 --> 00:09:14,050
the model advances in computer

00:09:12,550 --> 00:09:15,459
performance and the wave of open

00:09:14,050 --> 00:09:19,500
languages talk about a little bit about

00:09:15,459 --> 00:09:21,820
that if you look at about a decade ago

00:09:19,500 --> 00:09:24,010
they said machine learning itself is not

00:09:21,820 --> 00:09:25,990
me but what most enterprises spend their

00:09:24,010 --> 00:09:28,000
dollars we're in was in the

00:09:25,990 --> 00:09:30,850
infrastructure needed to run those

00:09:28,000 --> 00:09:33,160
algorithms we the amount of money spent

00:09:30,850 --> 00:09:34,930
on compute and storage was huge and

00:09:33,160 --> 00:09:36,730
there was not enough money left for

00:09:34,930 --> 00:09:39,339
investing in where the actual heart of

00:09:36,730 --> 00:09:41,440
the matter was the algorithm but now

00:09:39,339 --> 00:09:45,970
with advances in compute and storage

00:09:41,440 --> 00:09:49,950
technology that is not the challenge per

00:09:45,970 --> 00:09:52,660
se you have much more room much more

00:09:49,950 --> 00:09:54,760
Headroom to invest in in terms of your

00:09:52,660 --> 00:09:56,860
actual algorithm natural development

00:09:54,760 --> 00:09:59,440
especially with advances in GPU

00:09:56,860 --> 00:10:01,180
technology for deep learning especially

00:09:59,440 --> 00:10:03,550
coupling that with your standard

00:10:01,180 --> 00:10:07,420
processes you're also a little more with

00:10:03,550 --> 00:10:09,070
the with the GPUs from Nvidia and how

00:10:07,420 --> 00:10:11,770
that can be parts that can be paired

00:10:09,070 --> 00:10:13,870
with standard processor technology for

00:10:11,770 --> 00:10:16,990
example with the power processor coupled

00:10:13,870 --> 00:10:19,240
with Nvidia using the envy links an

00:10:16,990 --> 00:10:21,730
order of magnitude improvement in

00:10:19,240 --> 00:10:25,600
performance right so you've got much

00:10:21,730 --> 00:10:28,180
more ability to innovate in the actual

00:10:25,600 --> 00:10:30,579
algorithms and the and the neural nets

00:10:28,180 --> 00:10:32,980
and so on and the other part that's

00:10:30,579 --> 00:10:34,510
happening is that data science would

00:10:32,980 --> 00:10:36,880
become a team sport it used to be

00:10:34,510 --> 00:10:39,370
especially if you're doing SPSS or SAS

00:10:36,880 --> 00:10:41,440
or one of the existing technologies

00:10:39,370 --> 00:10:44,320
those are very conveying space you need

00:10:41,440 --> 00:10:45,370
a PhD to understand that space and why

00:10:44,320 --> 00:10:48,910
I'm just kidding you don't need a PhD

00:10:45,370 --> 00:10:51,960
but you know you need you needed to have

00:10:48,910 --> 00:10:53,760
extreme expertise in that specific area

00:10:51,960 --> 00:10:55,710
and it was very difficult for the

00:10:53,760 --> 00:10:57,930
business analyst the app developer the

00:10:55,710 --> 00:11:00,089
data engineer to collaborate on machine

00:10:57,930 --> 00:11:02,190
learning projects now where the industry

00:11:00,089 --> 00:11:04,680
is the headed is collaborative idea team

00:11:02,190 --> 00:11:06,899
sport where all of these people can

00:11:04,680 --> 00:11:10,709
collaborate so the concept of

00:11:06,899 --> 00:11:12,990
democratization is catching on and it be

00:11:10,709 --> 00:11:15,750
a developed star in this gallery of

00:11:12,990 --> 00:11:18,690
players is a battery spark so you've got

00:11:15,750 --> 00:11:20,880
many different open-source offerings and

00:11:18,690 --> 00:11:23,520
frameworks and languages out there you

00:11:20,880 --> 00:11:27,029
got tensorflow for deep learning

00:11:23,520 --> 00:11:29,459
you got scikit-learn spark is becoming

00:11:27,029 --> 00:11:31,980
sort of a star in this in this gallery

00:11:29,459 --> 00:11:35,700
of players for machine learning and and

00:11:31,980 --> 00:11:37,680
IBM is very consciously embraced as we

00:11:35,700 --> 00:11:40,680
are we've made a commitment to embrace

00:11:37,680 --> 00:11:41,940
spark and we are doing that commitment

00:11:40,680 --> 00:11:44,190
in one of two ways

00:11:41,940 --> 00:11:47,190
the first one is a commitment to

00:11:44,190 --> 00:11:50,070
contribute to this ecosystem the second

00:11:47,190 --> 00:11:51,570
is a car is a commitment adopt so if you

00:11:50,070 --> 00:11:54,020
look at the first one the commitment to

00:11:51,570 --> 00:11:56,339
contribute this is where we talk about

00:11:54,020 --> 00:11:58,470
this past technology Center is an

00:11:56,339 --> 00:12:00,870
investment where we've got a bunch of

00:11:58,470 --> 00:12:04,050
developers for constantly their only job

00:12:00,870 --> 00:12:07,260
is to produce code that is contributed

00:12:04,050 --> 00:12:09,150
to this park ecosystem if you look at

00:12:07,260 --> 00:12:12,540
some of the things that have happened

00:12:09,150 --> 00:12:15,390
now some of the metrics you know lines

00:12:12,540 --> 00:12:17,339
of code contributed to spark you know

00:12:15,390 --> 00:12:19,470
the number of commits in sports

00:12:17,339 --> 00:12:21,930
throughout our cetera and the increase

00:12:19,470 --> 00:12:24,540
in the level of contribution to the

00:12:21,930 --> 00:12:26,430
spark ecosystem it is incredible I

00:12:24,540 --> 00:12:28,620
believe we are the number one

00:12:26,430 --> 00:12:30,870
contributor to this ecosystem right now

00:12:28,620 --> 00:12:32,670
especially in the machine learning space

00:12:30,870 --> 00:12:35,220
if you look at the top three

00:12:32,670 --> 00:12:36,570
contribution areas you'll see that there

00:12:35,220 --> 00:12:39,120
are sparks equal itself

00:12:36,570 --> 00:12:40,890
then there are spice park so python is

00:12:39,120 --> 00:12:42,270
the most popular language for what one

00:12:40,890 --> 00:12:45,390
of the most popular languages for

00:12:42,270 --> 00:12:47,279
machine learning and bringing the Python

00:12:45,390 --> 00:12:48,870
and spark environments together with

00:12:47,279 --> 00:12:50,940
fight for is a large array of

00:12:48,870 --> 00:12:52,980
contribution and the ML libraries

00:12:50,940 --> 00:12:57,300
themselves these are the top three of

00:12:52,980 --> 00:12:58,740
our contributions amongst the the the

00:12:57,300 --> 00:13:02,130
overwhelming

00:12:58,740 --> 00:13:04,230
in high contributions to the overalls

00:13:02,130 --> 00:13:05,640
particle system so that is the

00:13:04,230 --> 00:13:08,070
commitment to contribute and that will

00:13:05,640 --> 00:13:09,839
continue but we also have a commitment

00:13:08,070 --> 00:13:12,600
to adopt it is not just at the right

00:13:09,839 --> 00:13:14,970
code and and and don't I did we actually

00:13:12,600 --> 00:13:17,430
are embracing this internally off

00:13:14,970 --> 00:13:19,290
furnaces on furnaces on the cloud on the

00:13:17,430 --> 00:13:21,630
mainframe of not joking about the

00:13:19,290 --> 00:13:23,850
mainframe by the way it was it is

00:13:21,630 --> 00:13:27,540
actually the first on-premises platform

00:13:23,850 --> 00:13:31,620
that we put spark on and put a broad

00:13:27,540 --> 00:13:34,050
machine learning young so on on

00:13:31,620 --> 00:13:38,149
mainframes on other hardware platforms

00:13:34,050 --> 00:13:42,089
in connectors to various data sources

00:13:38,149 --> 00:13:44,399
both connectors cloud services you have

00:13:42,089 --> 00:13:46,890
spark up a service in two clicks you can

00:13:44,399 --> 00:13:50,640
get a spark service provision for you

00:13:46,890 --> 00:13:52,560
you can you have beta science experience

00:13:50,640 --> 00:13:56,820
which is a platform for data scientists

00:13:52,560 --> 00:13:59,040
which makes machine learning easy and

00:13:56,820 --> 00:14:03,510
and brings the collaborative team sport

00:13:59,040 --> 00:14:06,060
effort to today design if you look at

00:14:03,510 --> 00:14:08,220
data science experience and what we are

00:14:06,060 --> 00:14:10,260
doing there if you have not wired that

00:14:08,220 --> 00:14:12,829
out I would highly recommend that go on

00:14:10,260 --> 00:14:17,220
rare science or ibm.com and try that out

00:14:12,829 --> 00:14:19,020
you get a variety of options to play

00:14:17,220 --> 00:14:21,270
around with data science and machine

00:14:19,020 --> 00:14:22,980
learning constructs both hard-core

00:14:21,270 --> 00:14:25,140
programmatic approaches in Jupiter

00:14:22,980 --> 00:14:28,110
notebooks as well as visual guided

00:14:25,140 --> 00:14:30,360
interfaces that let you play with these

00:14:28,110 --> 00:14:33,120
environments if you if you're building

00:14:30,360 --> 00:14:35,339
ML models you'll actually what guide you

00:14:33,120 --> 00:14:37,709
through the process of fading ml models

00:14:35,339 --> 00:14:40,800
you select and choose between different

00:14:37,709 --> 00:14:42,839
options in a drop-down you try out your

00:14:40,800 --> 00:14:45,240
data you look at the accuracy that is

00:14:42,839 --> 00:14:47,399
coming out of the auto B models and C

00:14:45,240 --> 00:14:49,380
can deploy one and the deployment is

00:14:47,399 --> 00:14:51,149
actually pretty cool now in one click

00:14:49,380 --> 00:14:53,760
the model gets deployed in the spark

00:14:51,149 --> 00:14:57,089
environment but beyond that we get to

00:14:53,760 --> 00:14:59,220
concepts of operationalizing what what I

00:14:57,089 --> 00:15:01,020
mean by that is if you if you have

00:14:59,220 --> 00:15:02,940
deployed your model as well and good it

00:15:01,020 --> 00:15:04,450
performs great area under the ROC curve

00:15:02,940 --> 00:15:07,240
is great or

00:15:04,450 --> 00:15:09,430
you know it's wonderful but a week from

00:15:07,240 --> 00:15:11,350
now the model starts degrading and you

00:15:09,430 --> 00:15:13,600
don't know what's going on so the

00:15:11,350 --> 00:15:16,630
concept of constantly monitoring the

00:15:13,600 --> 00:15:19,030
model looking at the performance of the

00:15:16,630 --> 00:15:22,270
model and providing that as a feedback

00:15:19,030 --> 00:15:24,640
loop into in a close feedback so that

00:15:22,270 --> 00:15:28,060
the model can constantly be retrained if

00:15:24,640 --> 00:15:31,180
you need to is a very interesting aspect

00:15:28,060 --> 00:15:32,830
very interesting concept so that the

00:15:31,180 --> 00:15:34,900
model that you build today and performs

00:15:32,830 --> 00:15:37,420
well today can be used to perform well

00:15:34,900 --> 00:15:39,610
six months from now as new data comes in

00:15:37,420 --> 00:15:44,560
and as new patterns revolve in your data

00:15:39,610 --> 00:15:46,870
the concept of deploy monitor constantly

00:15:44,560 --> 00:15:48,840
improve your model is something that I

00:15:46,870 --> 00:15:52,030
would suggest that you take a look at

00:15:48,840 --> 00:15:53,920
and spa basically allows this

00:15:52,030 --> 00:15:56,620
connectivity to different data sources

00:15:53,920 --> 00:15:59,460
provides a pipeline for your ml but it

00:15:56,620 --> 00:16:02,800
also enables very interesting use cases

00:15:59,460 --> 00:16:05,470
blockchain is another hot topic these

00:16:02,800 --> 00:16:07,270
days so here is an example where we are

00:16:05,470 --> 00:16:09,310
using spark connectors into the data

00:16:07,270 --> 00:16:11,080
that is held in a blockchain environment

00:16:09,310 --> 00:16:15,070
this happens to be a supply chain

00:16:11,080 --> 00:16:17,440
blockchain so we're looking at supply

00:16:15,070 --> 00:16:21,130
chain data and and find you understand

00:16:17,440 --> 00:16:23,020
or predict delays in shipments so the

00:16:21,130 --> 00:16:24,670
shipment delays could happen because of

00:16:23,020 --> 00:16:26,740
a lot of different things later the

00:16:24,670 --> 00:16:29,650
supply chain itself it could be related

00:16:26,740 --> 00:16:31,090
to the things that are being shipped it

00:16:29,650 --> 00:16:34,570
could be the carrier it could be a

00:16:31,090 --> 00:16:37,360
destination and so it could be time of

00:16:34,570 --> 00:16:39,310
shipment it could be external things

00:16:37,360 --> 00:16:42,190
like weather so using machine learning

00:16:39,310 --> 00:16:45,310
in the context of this data made

00:16:42,190 --> 00:16:46,540
possible through spark so spark allows

00:16:45,310 --> 00:16:48,850
you to connect to the underlying data

00:16:46,540 --> 00:16:51,220
sources bring it into the effects read

00:16:48,850 --> 00:16:53,950
events experience understand factors

00:16:51,220 --> 00:16:55,840
contributing to the delays and shipments

00:16:53,950 --> 00:16:57,610
and then build an ml model that can be

00:16:55,840 --> 00:17:00,220
used to predict whether there will be a

00:16:57,610 --> 00:17:02,550
delay as well as what to belay will be

00:17:00,220 --> 00:17:07,060
right interesting new space is evolving

00:17:02,550 --> 00:17:09,250
and finally one word about Watson so

00:17:07,060 --> 00:17:14,370
Watson it set up a cognitive functions

00:17:09,250 --> 00:17:16,829
in terms of speech recognition language

00:17:14,370 --> 00:17:18,420
standing natural language image

00:17:16,829 --> 00:17:21,270
recognition and so on so what do that

00:17:18,420 --> 00:17:23,250
you have in common we see you seen many

00:17:21,270 --> 00:17:26,480
customer use cases where the Fremen

00:17:23,250 --> 00:17:29,610
interaction uses one or more of Watson's

00:17:26,480 --> 00:17:31,440
cloud-based services so for example all

00:17:29,610 --> 00:17:34,440
Center Records so speech-to-text

00:17:31,440 --> 00:17:36,150
happening at the call center which are

00:17:34,440 --> 00:17:39,240
then used to extract pieces of

00:17:36,150 --> 00:17:40,740
information build a model builder build

00:17:39,240 --> 00:17:43,050
a set of features which are then fed

00:17:40,740 --> 00:17:44,940
into the model to do a scoring real-time

00:17:43,050 --> 00:17:45,570
and then guide the call center

00:17:44,940 --> 00:17:48,390
discussion

00:17:45,570 --> 00:17:51,000
okay so speech to text in the front end

00:17:48,390 --> 00:17:53,070
using Watson coupled with a spark ml

00:17:51,000 --> 00:17:56,309
model in the backend coming together so

00:17:53,070 --> 00:17:58,650
the concept of art for machine learning

00:17:56,309 --> 00:18:01,710
processing arm spark coupled with

00:17:58,650 --> 00:18:04,650
rebuild deep learning services on Watson

00:18:01,710 --> 00:18:06,929
make some very interesting cognitive

00:18:04,650 --> 00:18:07,380
applications I want to end there my time

00:18:06,929 --> 00:18:10,590
has run out

00:18:07,380 --> 00:18:12,510
I'm happy to discuss offline I would

00:18:10,590 --> 00:18:14,220
highly encourage you to take a look at

00:18:12,510 --> 00:18:17,940
some of these things machine learning

00:18:14,220 --> 00:18:20,160
and data signs you are also here we feel

00:18:17,940 --> 00:18:21,520
free to to play around right thank you

00:18:20,160 --> 00:18:25,779
for your time

00:18:21,520 --> 00:18:25,779

YouTube URL: https://www.youtube.com/watch?v=2YRzsEGt67Y


