Title: Container Pods with Docker Compose in Apache Mesos - Meghdoot Bhattacharya, Paypal
Publication date: 2017-05-18
Playlist: ApacheCon 2017 - Miami
Description: 
	Container Pods with Docker Compose in Apache Mesos - Meghdoot Bhattacharya, Paypal

Composite containers representing an application is a common requirement for modular architecture and to model micro-services . Composition requires co-location and treating a set of containers as a single unit (aka pod) for scheduling. Sidecar, ambassador, adapter patterns use container pod. K8 introduced the notion of a collection of docker containers that share namespaces and treat the collection as a single scaling unit. 

Docker Compose specification is very flexible and one can model a flexible pod collapsing different namespaces (net, IPC, pid). Apache Mesos on the other hand plays the critical role of a resource and cluster manager for large clusters. 

The presentation would showcase how docker compose can be integrated with different Apache Mesos frameworks like Apache Aurora, Marathon etc to launch true docker container pods in Mesos cluster and manage its lifecycle.

About MEGHDOOT BHATTACHARYA
Meghdoot Bhattacharya, is a director of Cloud Engineering team at PayPal, developing a large scale containerization platform using various technologies including Mesos and docker. He and his former PayPal team mate along with MapR and Mesosphere team were co-creators of Apache Myriad project running Yarn on Mesos. Meghdoot and his team had previously blogged in depth how eBay's CI farm was run on Mesos and docker.
Captions: 
	00:00:00,030 --> 00:00:06,120
hello everyone my name is Meg booth and

00:00:02,939 --> 00:00:09,450
I manage the container platform

00:00:06,120 --> 00:00:13,019
development at PayPal so welcome to the

00:00:09,450 --> 00:00:16,379
talk and just before we dive in to the

00:00:13,019 --> 00:00:18,240
talk itself a little quick show of hands

00:00:16,379 --> 00:00:22,890
on people familiar with

00:00:18,240 --> 00:00:29,550
Apache missiles okay and how about

00:00:22,890 --> 00:00:33,480
docker compose okay so the topic of

00:00:29,550 --> 00:00:36,110
today is how to run container pods with

00:00:33,480 --> 00:00:39,300
docker compose in Apache methods and

00:00:36,110 --> 00:00:42,300
with that this is pretty much the goal

00:00:39,300 --> 00:00:44,640
of the talk that with Apache missiles

00:00:42,300 --> 00:00:47,610
and docker both getting treated as

00:00:44,640 --> 00:00:51,719
first-class citizens how to run

00:00:47,610 --> 00:00:55,520
container pods in it and by first class

00:00:51,719 --> 00:00:57,989
citizens meaning that the developers and

00:00:55,520 --> 00:01:00,960
operations is using the docker runtime

00:00:57,989 --> 00:01:02,850
the docker to set the volumes the

00:01:00,960 --> 00:01:04,860
network plugins with network a Lib

00:01:02,850 --> 00:01:06,420
network and just not the docker image

00:01:04,860 --> 00:01:08,189
itself and having a different runtime

00:01:06,420 --> 00:01:09,750
like rocket or something else so you are

00:01:08,189 --> 00:01:11,909
treating docker as first class in your

00:01:09,750 --> 00:01:14,220
ecosystem and then you already have

00:01:11,909 --> 00:01:16,140
Apache methods running also so in case

00:01:14,220 --> 00:01:18,140
of PayPal we were running missiles

00:01:16,140 --> 00:01:21,270
before containers came into the picture

00:01:18,140 --> 00:01:23,790
so it was running POSIX processes and

00:01:21,270 --> 00:01:25,110
when containers came in you do not want

00:01:23,790 --> 00:01:27,810
to switch our cluster manager just

00:01:25,110 --> 00:01:30,420
because of other like non pod support

00:01:27,810 --> 00:01:33,450
there for some time and the other aspect

00:01:30,420 --> 00:01:35,549
was we wanted developers to run the

00:01:33,450 --> 00:01:37,799
container ports locally in the laptop or

00:01:35,549 --> 00:01:40,829
desktop without even getting worried

00:01:37,799 --> 00:01:42,360
about cluster managers get that going

00:01:40,829 --> 00:01:44,420
and when they want to take it into QA

00:01:42,360 --> 00:01:47,430
and production use the same

00:01:44,420 --> 00:01:49,170
specification to run it in those

00:01:47,430 --> 00:01:53,520
environments so that there is no drift

00:01:49,170 --> 00:01:55,500
and translation errors and with that the

00:01:53,520 --> 00:01:58,110
solution which we kind of built and we

00:01:55,500 --> 00:02:00,060
just released it last week it's out

00:01:58,110 --> 00:02:02,159
there in open now a docker compose

00:02:00,060 --> 00:02:06,540
message executors and we will go through

00:02:02,159 --> 00:02:10,950
all the details in the talk so what

00:02:06,540 --> 00:02:13,170
exactly are pods so a pods loosely our

00:02:10,950 --> 00:02:13,470
collection of docker containers which

00:02:13,170 --> 00:02:16,020
you are

00:02:13,470 --> 00:02:19,170
bundling them together and scheduling

00:02:16,020 --> 00:02:21,360
and and deploying them as a unit so you

00:02:19,170 --> 00:02:24,240
kind of treat them as a single scaling

00:02:21,360 --> 00:02:28,680
unit so every instance is a collection

00:02:24,240 --> 00:02:31,080
of these pods now two important things

00:02:28,680 --> 00:02:32,760
to keep in mind is all the containers

00:02:31,080 --> 00:02:36,600
which you are bundling together as this

00:02:32,760 --> 00:02:38,910
unit will most likely share namespaces

00:02:36,600 --> 00:02:40,980
so network namespaces is the most common

00:02:38,910 --> 00:02:43,860
thing so one of the containers in the

00:02:40,980 --> 00:02:47,130
pod will have the primary network

00:02:43,860 --> 00:02:49,110
interfaces and the IP associated with it

00:02:47,130 --> 00:02:50,880
and the rest of the containers in the

00:02:49,110 --> 00:02:53,700
pod will just collapse and use the same

00:02:50,880 --> 00:02:55,650
interface and an IP you can collapse pit

00:02:53,700 --> 00:02:58,380
or IPC namespace as well but network is

00:02:55,650 --> 00:03:01,200
the most common the second point for

00:02:58,380 --> 00:03:04,230
port definition is this group of

00:03:01,200 --> 00:03:09,120
containers treated as a unit will be

00:03:04,230 --> 00:03:11,310
capped by a C group label our top-level

00:03:09,120 --> 00:03:14,190
C group or a common C group so that the

00:03:11,310 --> 00:03:17,400
resources consumed by this port in total

00:03:14,190 --> 00:03:21,390
whether it's like memory or CPU or disk

00:03:17,400 --> 00:03:23,430
is kind of capped and that is to ensure

00:03:21,390 --> 00:03:25,830
that it does not steal resources from

00:03:23,430 --> 00:03:28,890
other pods running in the host

00:03:25,830 --> 00:03:30,540
now the containers individual containers

00:03:28,890 --> 00:03:32,610
in the pod

00:03:30,540 --> 00:03:34,320
you can constrain them you may not

00:03:32,610 --> 00:03:36,000
constrain them so they can fight within

00:03:34,320 --> 00:03:39,600
each other to steal resources it's up to

00:03:36,000 --> 00:03:42,049
the configuration but the whole unit as

00:03:39,600 --> 00:03:45,299
a whole needs to be kind of capped and

00:03:42,049 --> 00:03:47,400
the last one is very important so if you

00:03:45,299 --> 00:03:49,709
are using a cluster manager and you have

00:03:47,400 --> 00:03:52,350
like different containers and you are

00:03:49,709 --> 00:03:54,690
using just constraints to make sure all

00:03:52,350 --> 00:03:57,720
of them land into the same host and say

00:03:54,690 --> 00:03:59,340
oh that is my pod that isn't because you

00:03:57,720 --> 00:04:02,130
are most likely not collapsing Network

00:03:59,340 --> 00:04:04,049
namespaces you are most definitely not

00:04:02,130 --> 00:04:06,440
capping all these containers under a

00:04:04,049 --> 00:04:09,350
common C group to limit resources so

00:04:06,440 --> 00:04:13,049
colocation using constraints is not quad

00:04:09,350 --> 00:04:14,940
so why are a pod needed wire pods needed

00:04:13,049 --> 00:04:18,540
so the first use case or the first point

00:04:14,940 --> 00:04:21,000
is definitely true for PayPal as we are

00:04:18,540 --> 00:04:23,849
migrating our legacy workloads which

00:04:21,000 --> 00:04:27,270
were running in VMs into the container

00:04:23,849 --> 00:04:29,310
ecosystem pods help in a lift and shift

00:04:27,270 --> 00:04:31,500
kind of strategy so let's say you have

00:04:29,310 --> 00:04:33,750
three processes or applications running

00:04:31,500 --> 00:04:35,490
in that VM some of them might be using

00:04:33,750 --> 00:04:36,509
localhost communication you just don't

00:04:35,490 --> 00:04:39,750
know all the details

00:04:36,509 --> 00:04:41,849
so you model them in a pod treat them as

00:04:39,750 --> 00:04:45,360
a unit and bring it to the cluster

00:04:41,849 --> 00:04:47,910
manager and that actually gives time to

00:04:45,360 --> 00:04:50,699
start refactoring you can't go to micro

00:04:47,910 --> 00:04:53,550
services in day one and the other fact

00:04:50,699 --> 00:04:55,680
is that as you are bringing these parts

00:04:53,550 --> 00:04:57,810
together in the same host you say oh

00:04:55,680 --> 00:04:59,819
there is a common container in both the

00:04:57,810 --> 00:05:01,889
pods which you can refactor into like

00:04:59,819 --> 00:05:04,470
single engine system service container

00:05:01,889 --> 00:05:06,419
and those things come like organically

00:05:04,470 --> 00:05:10,139
and you don't have to like worry all

00:05:06,419 --> 00:05:12,650
about that before hand so pods is really

00:05:10,139 --> 00:05:16,319
helping lift and shift of the legacy my

00:05:12,650 --> 00:05:18,620
workloads the second point is definitely

00:05:16,319 --> 00:05:22,440
our micro services and composite

00:05:18,620 --> 00:05:24,330
containers or composite services through

00:05:22,440 --> 00:05:25,699
containers is well supported through

00:05:24,330 --> 00:05:29,520
pods so whether it's a five-car

00:05:25,699 --> 00:05:32,130
Ambassador adapter patterns you can

00:05:29,520 --> 00:05:35,270
definitely model it through pods and

00:05:32,130 --> 00:05:38,909
it's a very common pattern in that case

00:05:35,270 --> 00:05:41,280
the third point is oftentimes in in

00:05:38,909 --> 00:05:43,710
traditional deployment environments you

00:05:41,280 --> 00:05:45,360
will have like a pre deployment step in

00:05:43,710 --> 00:05:47,699
the node you are deploying and then you

00:05:45,360 --> 00:05:49,259
deploy your actual application and then

00:05:47,699 --> 00:05:53,219
then you might have like post deployment

00:05:49,259 --> 00:05:57,030
steps so if you are just running a pod

00:05:53,219 --> 00:06:00,030
some of those tasks can be like achieved

00:05:57,030 --> 00:06:01,349
through short-lived containers they

00:06:00,030 --> 00:06:03,180
could be data containers they could be

00:06:01,349 --> 00:06:05,460
just containers running some steps and

00:06:03,180 --> 00:06:08,729
then going away and that can be cleanly

00:06:05,460 --> 00:06:11,009
a model through port as well so those

00:06:08,729 --> 00:06:14,969
are the three use cases I can think of

00:06:11,009 --> 00:06:17,159
why pods are needed so with that I will

00:06:14,969 --> 00:06:19,110
just go through docker compose and mesos

00:06:17,159 --> 00:06:22,919
kind of recap and then we will dive into

00:06:19,110 --> 00:06:25,080
the solution so composed has been there

00:06:22,919 --> 00:06:27,449
in the community for a long time it used

00:06:25,080 --> 00:06:29,729
to be called fig and docker accured the

00:06:27,449 --> 00:06:32,039
the team and it was called composed

00:06:29,729 --> 00:06:34,050
after that it provides you a

00:06:32,039 --> 00:06:36,389
specification of running containers

00:06:34,050 --> 00:06:38,339
through a ml file so you don't have to

00:06:36,389 --> 00:06:40,320
remember all the docker on options and

00:06:38,339 --> 00:06:42,210
pass it like that and on top

00:06:40,320 --> 00:06:45,960
of that it has ordering primitives and

00:06:42,210 --> 00:06:48,150
certain other semantics there and you

00:06:45,960 --> 00:06:50,490
can both specify and and run just

00:06:48,150 --> 00:06:53,550
through command line in compose now two

00:06:50,490 --> 00:06:55,980
things has happened lately so there is a

00:06:53,550 --> 00:06:58,860
2 dot X composed version series I

00:06:55,980 --> 00:07:02,670
believe two dot one is the latest one so

00:06:58,860 --> 00:07:05,660
that preserves strictly all the local

00:07:02,670 --> 00:07:08,220
features by meaning that is composed

00:07:05,660 --> 00:07:10,260
always interacted with a single docker

00:07:08,220 --> 00:07:12,450
engine so you launch all the containers

00:07:10,260 --> 00:07:14,100
with a single doctor engine that could

00:07:12,450 --> 00:07:17,040
be most likely locally running in your

00:07:14,100 --> 00:07:19,530
laptop or it may be in a remote location

00:07:17,040 --> 00:07:22,710
but you are launching the containers

00:07:19,530 --> 00:07:26,460
against a single engine version 3 dot

00:07:22,710 --> 00:07:29,160
which is the the other series it is

00:07:26,460 --> 00:07:30,200
basically composed with docker swarm

00:07:29,160 --> 00:07:32,700
that's the cluster management

00:07:30,200 --> 00:07:34,980
integration and as they introduce

00:07:32,700 --> 00:07:37,140
certain options there like docker

00:07:34,980 --> 00:07:39,810
compose deploy and things like that they

00:07:37,140 --> 00:07:42,960
took away certain parameters which meant

00:07:39,810 --> 00:07:45,300
or which made sense in a in a local pod

00:07:42,960 --> 00:07:47,490
kind of environment so three dot X

00:07:45,300 --> 00:07:50,310
actually duplicates certain features now

00:07:47,490 --> 00:07:51,750
two dot x and three dot X are both alive

00:07:50,310 --> 00:07:54,510
and I think they are working on

00:07:51,750 --> 00:07:59,100
collapsing them in in some form but to

00:07:54,510 --> 00:08:02,490
call that out so if you wanted to model

00:07:59,100 --> 00:08:04,320
ports through our docker compose let's

00:08:02,490 --> 00:08:05,910
see what we can do with the compos

00:08:04,320 --> 00:08:09,570
tooling is already there so you can

00:08:05,910 --> 00:08:11,130
define containers all in that sing in

00:08:09,570 --> 00:08:15,330
EML files they are bundled together

00:08:11,130 --> 00:08:16,890
relies on the two dot x for sure you can

00:08:15,330 --> 00:08:20,160
if you have written your lip next

00:08:16,890 --> 00:08:23,130
network plugins your volume plugins you

00:08:20,160 --> 00:08:24,510
can specify them in the compose file it

00:08:23,130 --> 00:08:27,810
could be pre created or you can create

00:08:24,510 --> 00:08:30,420
right to the compose file themselves you

00:08:27,810 --> 00:08:32,070
can collapse network name spaces or

00:08:30,420 --> 00:08:34,020
other namespaces it doesn't happen by

00:08:32,070 --> 00:08:36,030
default so in the compose file you have

00:08:34,020 --> 00:08:39,060
to say use the name space of the other

00:08:36,030 --> 00:08:40,590
container but you can do any combination

00:08:39,060 --> 00:08:42,030
of the containers defined in the compose

00:08:40,590 --> 00:08:45,110
file and we will see the examples when

00:08:42,030 --> 00:08:48,690
we do the demo there you can define

00:08:45,110 --> 00:08:50,910
ordering of the containers in the pod so

00:08:48,690 --> 00:08:53,340
before there you get depends on

00:08:50,910 --> 00:08:54,140
primitive income in composed so you can

00:08:53,340 --> 00:08:56,959
say

00:08:54,140 --> 00:08:58,970
let's container B start after container

00:08:56,959 --> 00:09:01,790
a have started and sometimes you need

00:08:58,970 --> 00:09:04,660
that sequence but what used to happen is

00:09:01,790 --> 00:09:07,670
if the process inside the container B

00:09:04,660 --> 00:09:09,680
started faster than the process inside

00:09:07,670 --> 00:09:11,839
the container a you just had a reverse

00:09:09,680 --> 00:09:13,670
order now and a lot of the legacy

00:09:11,839 --> 00:09:16,579
applications broke so people had

00:09:13,670 --> 00:09:19,519
different kind of containers injected

00:09:16,579 --> 00:09:23,510
for weight there was no thing supported

00:09:19,519 --> 00:09:25,310
in 2 dot X support is now therefore held

00:09:23,510 --> 00:09:28,790
check of the containers so you can say

00:09:25,310 --> 00:09:31,339
let container B start when container is

00:09:28,790 --> 00:09:34,730
health check has passed so much more

00:09:31,339 --> 00:09:38,149
strict ordering the guarantees now you

00:09:34,730 --> 00:09:40,070
can do in 2 dot X as I said you can

00:09:38,149 --> 00:09:43,190
point to external created volumes the

00:09:40,070 --> 00:09:45,680
network's multiple files can be

00:09:43,190 --> 00:09:48,110
dedicated in in compose files that are

00:09:45,680 --> 00:09:49,820
merged so you can have a base file you

00:09:48,110 --> 00:09:51,620
can have a QA environment file and a

00:09:49,820 --> 00:09:54,200
production file and you can merge those

00:09:51,620 --> 00:09:56,690
definitions that so it's pretty powerful

00:09:54,200 --> 00:09:59,180
we definitely use that in paypal and

00:09:56,690 --> 00:10:02,000
last but not the least you can run a

00:09:59,180 --> 00:10:05,089
bunch of these are pods through-composed

00:10:02,000 --> 00:10:07,550
in the same host without getting

00:10:05,089 --> 00:10:10,430
conflicts they name them differently

00:10:07,550 --> 00:10:12,290
with certain configurations so one of

00:10:10,430 --> 00:10:14,990
the things definitely missing here is

00:10:12,290 --> 00:10:16,579
that a point we made in the pod

00:10:14,990 --> 00:10:18,949
definition is bringing all these

00:10:16,579 --> 00:10:20,930
containers under a common c group so

00:10:18,949 --> 00:10:23,209
even if you use compose each of these

00:10:20,930 --> 00:10:27,320
containers will have the top-level c

00:10:23,209 --> 00:10:30,320
group created so that part doesn't

00:10:27,320 --> 00:10:33,100
happen and the collapsing you have to

00:10:30,320 --> 00:10:36,680
explicitly say that so that doesn't

00:10:33,100 --> 00:10:38,779
plussing of the namespaces i mean so we

00:10:36,680 --> 00:10:42,190
will revisit things during the demo I

00:10:38,779 --> 00:10:46,339
wanted to do a recap of the mesos

00:10:42,190 --> 00:10:48,680
architecture so as you see in the

00:10:46,339 --> 00:10:50,529
methyls architecture it's the cluster

00:10:48,680 --> 00:10:53,720
manager it has the master and the worker

00:10:50,529 --> 00:10:55,850
models so methyls master there's a

00:10:53,720 --> 00:10:58,550
primary master the other two masters or

00:10:55,850 --> 00:11:00,890
more than that in the in the quorum it's

00:10:58,550 --> 00:11:02,870
not a quorum actual they are passive you

00:11:00,890 --> 00:11:04,310
have different frameworks it's a two

00:11:02,870 --> 00:11:07,370
level scheduler we'll dive a little

00:11:04,310 --> 00:11:09,110
deeper you can have marathon

00:11:07,370 --> 00:11:11,120
a framework for services similarly

00:11:09,110 --> 00:11:13,130
authorized there you can have Cassandra

00:11:11,120 --> 00:11:15,830
Phillips Kafka frameworks and the

00:11:13,130 --> 00:11:17,960
methyls agent is the agent running in

00:11:15,830 --> 00:11:21,560
all your hosts where your actual

00:11:17,960 --> 00:11:23,480
workload is running methods agents do

00:11:21,560 --> 00:11:26,810
have a concept of continued rise err

00:11:23,480 --> 00:11:29,480
which is methyls definition of a

00:11:26,810 --> 00:11:35,060
container and we will cover what is that

00:11:29,480 --> 00:11:36,680
now the executor is your task lifecycle

00:11:35,060 --> 00:11:40,370
management manager you can think of that

00:11:36,680 --> 00:11:42,800
way and the t1 t2 those are the actual

00:11:40,370 --> 00:11:44,630
workloads getting run it could be docker

00:11:42,800 --> 00:11:45,920
containers it could be k vm it could be

00:11:44,630 --> 00:11:48,890
unique kernel it could be POSIX

00:11:45,920 --> 00:11:51,950
processes so in the whole methyls

00:11:48,890 --> 00:11:54,830
ecosystem the primitives are such that

00:11:51,950 --> 00:11:57,410
the frameworks the masters the agent

00:11:54,830 --> 00:12:00,110
doesn't know what actual workload you're

00:11:57,410 --> 00:12:03,110
running so the executor is the one which

00:12:00,110 --> 00:12:05,240
implements the primitives and knows what

00:12:03,110 --> 00:12:06,920
actual workload is running and how to

00:12:05,240 --> 00:12:09,650
maintain the life cycle of those

00:12:06,920 --> 00:12:13,660
workloads so it's really agnostic in

00:12:09,650 --> 00:12:16,339
that sense ok some of the key

00:12:13,660 --> 00:12:19,310
abstractions in the methyls world the

00:12:16,339 --> 00:12:21,740
agents if you see at the bottom this is

00:12:19,310 --> 00:12:23,209
where your application workloads are

00:12:21,740 --> 00:12:25,880
running the agents typically will

00:12:23,209 --> 00:12:28,550
advertise resources to the master saying

00:12:25,880 --> 00:12:32,180
this is how much memory and CPU and disk

00:12:28,550 --> 00:12:33,980
resources you could have GPU you can

00:12:32,180 --> 00:12:35,779
figure that one out what you want and

00:12:33,980 --> 00:12:38,320
they advertise the resources of the

00:12:35,779 --> 00:12:40,700
master and the master in turn

00:12:38,320 --> 00:12:43,520
advertisers these resources to the

00:12:40,700 --> 00:12:45,800
frameworks so you can have multiple

00:12:43,520 --> 00:12:48,260
frameworks competing for these resources

00:12:45,800 --> 00:12:51,500
and for the different workloads they are

00:12:48,260 --> 00:12:55,610
supposed to run and the frameworks will

00:12:51,500 --> 00:12:58,310
then decide which offers best match for

00:12:55,610 --> 00:13:00,110
the application workloads so then they

00:12:58,310 --> 00:13:03,410
will basically say to the master I want

00:13:00,110 --> 00:13:05,839
to pick resources offered from host

00:13:03,410 --> 00:13:08,360
number one and host number five and by

00:13:05,839 --> 00:13:11,150
the way here is the executor to launch

00:13:08,360 --> 00:13:13,520
that task and then the master will get

00:13:11,150 --> 00:13:16,459
that details and then contacts those

00:13:13,520 --> 00:13:17,990
agents and launch the task and say by

00:13:16,459 --> 00:13:20,240
the way use that executor again the

00:13:17,990 --> 00:13:22,700
master doesn't know what kind of

00:13:20,240 --> 00:13:25,220
it's running it just knows that how much

00:13:22,700 --> 00:13:27,500
resources will be consumed and the agent

00:13:25,220 --> 00:13:30,610
as well so that is kind of the the

00:13:27,500 --> 00:13:34,310
abstractions in the metals model and

00:13:30,610 --> 00:13:37,250
different executors can coexist in the

00:13:34,310 --> 00:13:43,520
in the same post running different

00:13:37,250 --> 00:13:47,180
workloads so to double-click in how we

00:13:43,520 --> 00:13:49,399
obviously implemented executor dedicated

00:13:47,180 --> 00:13:51,529
for docker compose kind of workloads and

00:13:49,399 --> 00:13:54,860
we mentioned that in docker compose

00:13:51,529 --> 00:13:59,089
executors so if you look at the top box

00:13:54,860 --> 00:14:00,980
that's the mesos agent and it uses the

00:13:59,089 --> 00:14:05,839
default methods container Iser

00:14:00,980 --> 00:14:08,870
now there is a concept of Isolators in

00:14:05,839 --> 00:14:11,360
in container Iser so Isolators help

00:14:08,870 --> 00:14:13,820
define the top-level mesos container

00:14:11,360 --> 00:14:16,459
which i mentioned and so in this case we

00:14:13,820 --> 00:14:20,060
are using the c group's memory and cpu

00:14:16,459 --> 00:14:22,209
isolator to get this like bottom gray

00:14:20,060 --> 00:14:24,950
box if it's your parent container

00:14:22,209 --> 00:14:28,899
created with the parent c group in

00:14:24,950 --> 00:14:31,970
metals and then your executor task is

00:14:28,899 --> 00:14:34,310
actually the main task running under the

00:14:31,970 --> 00:14:37,250
sea group so mesos is pretty much

00:14:34,310 --> 00:14:40,279
monitoring this parent task in

00:14:37,250 --> 00:14:42,680
independent top-level c group what we do

00:14:40,279 --> 00:14:45,410
in compose executor is then launch this

00:14:42,680 --> 00:14:48,950
actual pod which is this collection of

00:14:45,410 --> 00:14:53,360
containers under the parent C group and

00:14:48,950 --> 00:14:56,980
maintain the hierarchy so this so if I

00:14:53,360 --> 00:15:01,370
go to the next point and say this helps

00:14:56,980 --> 00:15:05,060
achieve the port criteria of making sure

00:15:01,370 --> 00:15:12,500
that the containers coming up in the pod

00:15:05,060 --> 00:15:17,149
has a common C group and so you can

00:15:12,500 --> 00:15:20,839
define the C group CFS hot limits so a

00:15:17,149 --> 00:15:22,600
throttle the the maximum kind of CPU you

00:15:20,839 --> 00:15:25,130
can have memory limits so that

00:15:22,600 --> 00:15:26,779
containers can be um the entire port

00:15:25,130 --> 00:15:29,600
will be owned

00:15:26,779 --> 00:15:32,360
um killed in that case now the point is

00:15:29,600 --> 00:15:34,070
the individual containers in the pod

00:15:32,360 --> 00:15:37,220
docker gives you

00:15:34,070 --> 00:15:38,900
options today with - - CPUs as of the

00:15:37,220 --> 00:15:41,780
latest and memory

00:15:38,900 --> 00:15:44,120
- kind of cap the individual resources

00:15:41,780 --> 00:15:47,720
of the container but in this case even

00:15:44,120 --> 00:15:49,850
if you don't do that you have a parent C

00:15:47,720 --> 00:15:52,880
group who which has the resources for

00:15:49,850 --> 00:15:55,160
the entire pod so even if you even if

00:15:52,880 --> 00:15:57,950
one of the containers is acting bad and

00:15:55,160 --> 00:16:00,650
stealing more resources the minute it

00:15:57,950 --> 00:16:04,040
goes over the parent C group resources

00:16:00,650 --> 00:16:08,900
it will be taken care of by bio metals

00:16:04,040 --> 00:16:10,430
and the and the C group itself so that's

00:16:08,900 --> 00:16:12,620
what I said individual containers will

00:16:10,430 --> 00:16:15,170
not be limited unless specified but

00:16:12,620 --> 00:16:17,660
cannot go over the parent and the last

00:16:15,170 --> 00:16:21,650
point is when we have a C group

00:16:17,660 --> 00:16:23,300
hierarchy CPUs are throttled but for the

00:16:21,650 --> 00:16:26,420
memory you have to make sure the use

00:16:23,300 --> 00:16:31,640
hierarchy flag is enabled and that will

00:16:26,420 --> 00:16:34,100
ensure things are in order now we will

00:16:31,640 --> 00:16:37,130
now go into the details of the compose

00:16:34,100 --> 00:16:41,600
executors and how it works and what

00:16:37,130 --> 00:16:44,330
features it's bringing by default so as

00:16:41,600 --> 00:16:46,850
an executor going back to the

00:16:44,330 --> 00:16:49,310
programming model it implements the

00:16:46,850 --> 00:16:51,470
callbacks to maintain the life cycle of

00:16:49,310 --> 00:16:53,330
a pod so these are the primitives and

00:16:51,470 --> 00:16:56,210
the callbacks which is implement with

00:16:53,330 --> 00:16:58,640
the metals agent so that it can it can

00:16:56,210 --> 00:17:03,350
launch a pod it can kill a pod in terms

00:16:58,640 --> 00:17:06,620
of the task primitives and so by default

00:17:03,350 --> 00:17:08,660
you can give a series of compose files

00:17:06,620 --> 00:17:12,199
and we can see and they will see in the

00:17:08,660 --> 00:17:15,770
demo shortly but this edition of the C

00:17:12,199 --> 00:17:20,449
group so docker run takes a C group

00:17:15,770 --> 00:17:22,820
parent kind of option so in this case

00:17:20,449 --> 00:17:25,459
the executor will figure figure out what

00:17:22,820 --> 00:17:27,949
is your parent metal C group and add it

00:17:25,459 --> 00:17:30,410
to the containers in the pod it will add

00:17:27,949 --> 00:17:32,510
other important labels like executor ID

00:17:30,410 --> 00:17:35,240
and task ID so you can do docker PS and

00:17:32,510 --> 00:17:37,970
filter those if needed so it will do a

00:17:35,240 --> 00:17:39,680
bunch of things similar to that it will

00:17:37,970 --> 00:17:41,810
collapse the network namespace by

00:17:39,680 --> 00:17:44,660
default in all the containers in the pod

00:17:41,810 --> 00:17:46,880
so local host works for all the

00:17:44,660 --> 00:17:49,520
containers in communicating with

00:17:46,880 --> 00:17:54,470
each other inside the pod and they share

00:17:49,520 --> 00:17:57,980
the same IP there will be a pod health

00:17:54,470 --> 00:18:00,590
check monitor which gets launched this

00:17:57,980 --> 00:18:03,110
is not only kind of monitoring that

00:18:00,590 --> 00:18:05,870
whether one of the containers in the pod

00:18:03,110 --> 00:18:08,180
crashed out in that case the default

00:18:05,870 --> 00:18:10,490
behavior is completely kill the pod

00:18:08,180 --> 00:18:12,050
because we don't want to know like

00:18:10,490 --> 00:18:13,820
whether it's the important container or

00:18:12,050 --> 00:18:16,970
a sidecar or whatever it is we would

00:18:13,820 --> 00:18:19,340
kill it but with the addition of the

00:18:16,970 --> 00:18:21,800
docker health checks it is often times

00:18:19,340 --> 00:18:24,590
that your container may be up but your

00:18:21,800 --> 00:18:26,990
health check is failing it is in a in a

00:18:24,590 --> 00:18:29,240
batch state so we will detect that and

00:18:26,990 --> 00:18:30,740
also kind of kill the pod because when

00:18:29,240 --> 00:18:33,770
you're running in the cluster manager is

00:18:30,740 --> 00:18:37,550
better to kill it and have a new pod

00:18:33,770 --> 00:18:42,380
come up so it supports running multiple

00:18:37,550 --> 00:18:44,360
compost files it also implements and

00:18:42,380 --> 00:18:47,120
this is outside of the executor the

00:18:44,360 --> 00:18:50,150
project defines methyls module and we'll

00:18:47,120 --> 00:18:53,930
cover what missiles modules are to

00:18:50,150 --> 00:18:56,180
prevent container port leaks in case the

00:18:53,930 --> 00:18:58,220
executor crashes out because remember

00:18:56,180 --> 00:19:01,130
the executor is is managing the

00:18:58,220 --> 00:19:03,590
lifecycle of the pod and what if it

00:19:01,130 --> 00:19:07,490
crashes out your containers will keep

00:19:03,590 --> 00:19:08,750
running and so the module will help to

00:19:07,490 --> 00:19:13,700
clean those things out and we will see

00:19:08,750 --> 00:19:14,900
that in the demo as well the DCE goal I

00:19:13,700 --> 00:19:16,880
did not explain the name of that doctor

00:19:14,900 --> 00:19:18,590
compose executor we implemented this in

00:19:16,880 --> 00:19:21,320
goal there was an earlier version with

00:19:18,590 --> 00:19:23,210
Java and of course the JVM completely

00:19:21,320 --> 00:19:27,140
sucks out performance if you're running

00:19:23,210 --> 00:19:29,750
40 pods in a large mesh node so it's in

00:19:27,140 --> 00:19:32,240
go and a lot of features have been

00:19:29,750 --> 00:19:34,880
implemented in in this specific version

00:19:32,240 --> 00:19:38,540
so what exactly are plugins we'll cover

00:19:34,880 --> 00:19:41,150
that in the next slide and last but not

00:19:38,540 --> 00:19:43,280
the least any existing methods of

00:19:41,150 --> 00:19:46,520
frameworks whether it's marathon or Ora

00:19:43,280 --> 00:19:48,920
or singularity whatever you have for

00:19:46,520 --> 00:19:52,010
running services you can just plug and

00:19:48,920 --> 00:19:57,140
play without making any changes in the

00:19:52,010 --> 00:20:00,080
framework okay with that let me look at

00:19:57,140 --> 00:20:01,840
a time okay so what what exactly are

00:20:00,080 --> 00:20:05,270
plugins

00:20:01,840 --> 00:20:08,299
so plugins provide a way to easily

00:20:05,270 --> 00:20:13,220
extend the inner workings of docker

00:20:08,299 --> 00:20:14,929
compose executor so think of think as

00:20:13,220 --> 00:20:18,559
plugins I'll just get all the points

00:20:14,929 --> 00:20:21,620
here pink as plugins to customize the

00:20:18,559 --> 00:20:24,400
behavior as you are launching the pod so

00:20:21,620 --> 00:20:27,260
we have hooks before the launch and

00:20:24,400 --> 00:20:28,820
before the launch task primitive before

00:20:27,260 --> 00:20:31,010
and after the launch task and similarly

00:20:28,820 --> 00:20:35,360
before and after the kill task so

00:20:31,010 --> 00:20:38,510
whether at runtime you are deciding on

00:20:35,360 --> 00:20:40,370
what pre-existing network to configure

00:20:38,510 --> 00:20:43,010
the pod with remember the idea is the

00:20:40,370 --> 00:20:45,980
developer has run the port locally with

00:20:43,010 --> 00:20:48,409
their bridge or network mode host and

00:20:45,980 --> 00:20:50,450
it's specifying the same port files in

00:20:48,409 --> 00:20:53,450
in the production or QA environment and

00:20:50,450 --> 00:20:55,549
at runtime you decide that I'm using a

00:20:53,450 --> 00:20:58,730
container networking solution here are

00:20:55,549 --> 00:21:01,100
the network details here are maybe some

00:20:58,730 --> 00:21:03,140
volumes you are mounting certain runtime

00:21:01,100 --> 00:21:06,169
levels you're injecting so you can do

00:21:03,140 --> 00:21:08,600
all those massaging before the actual

00:21:06,169 --> 00:21:11,289
pot is launched and you can actually

00:21:08,600 --> 00:21:15,289
even get resources and free up resources

00:21:11,289 --> 00:21:18,440
so that helps you to customize behavior

00:21:15,289 --> 00:21:22,279
to plugins and you can have multiple

00:21:18,440 --> 00:21:24,020
plugins they can be chained in order so

00:21:22,279 --> 00:21:29,360
different things can contribute towards

00:21:24,020 --> 00:21:33,169
it so we do provide a default plugin and

00:21:29,360 --> 00:21:36,289
the default plug-in adds labels like

00:21:33,169 --> 00:21:38,510
methyls task ID and executor ID against

00:21:36,289 --> 00:21:40,070
every container in the pod so that if

00:21:38,510 --> 00:21:42,770
you're in the host and you're using

00:21:40,070 --> 00:21:44,720
docker PS - mark filter and you want to

00:21:42,770 --> 00:21:46,940
like quickly get two containers of a

00:21:44,720 --> 00:21:52,760
certain methods task you should be able

00:21:46,940 --> 00:21:56,330
to use that we by default add the the

00:21:52,760 --> 00:21:59,120
methyls parent a task c group to all the

00:21:56,330 --> 00:22:01,549
containers and then as we looked before

00:21:59,120 --> 00:22:03,620
that compose doesn't collapse the

00:22:01,549 --> 00:22:07,240
network names is by default so what

00:22:03,620 --> 00:22:09,950
happens in this case we create a secret

00:22:07,240 --> 00:22:12,740
kind of infrastructure container in the

00:22:09,950 --> 00:22:14,300
pod and you can define it in the plugin

00:22:12,740 --> 00:22:16,280
and that creates

00:22:14,300 --> 00:22:18,560
our main network interfaces gets the IP

00:22:16,280 --> 00:22:20,840
and all other containers in the port

00:22:18,560 --> 00:22:21,920
basically collapses against this

00:22:20,840 --> 00:22:24,170
infrastructure container

00:22:21,920 --> 00:22:25,730
it's similarly how kubernetes pots work

00:22:24,170 --> 00:22:28,430
is just that they create the

00:22:25,730 --> 00:22:33,890
infrastructure container as well it's

00:22:28,430 --> 00:22:37,130
just you don't see it okay with that we

00:22:33,890 --> 00:22:39,350
will cover what exactly are metals

00:22:37,130 --> 00:22:43,760
modules we specifically use a module

00:22:39,350 --> 00:22:46,550
called cook so just like plugins extend

00:22:43,760 --> 00:22:47,900
the docker compose executors default

00:22:46,550 --> 00:22:50,020
behavior

00:22:47,900 --> 00:22:52,850
one thing about plugins these are just

00:22:50,020 --> 00:22:56,120
go modules which you will compile in

00:22:52,850 --> 00:22:57,500
with the executor in case of methyls

00:22:56,120 --> 00:22:59,720
modules these are actually shared

00:22:57,500 --> 00:23:01,940
libraries which are dynamically loaded

00:22:59,720 --> 00:23:05,380
it could be in the master or the agent

00:23:01,940 --> 00:23:08,180
to extend the inner functionality

00:23:05,380 --> 00:23:11,300
methyls modules very very important is

00:23:08,180 --> 00:23:13,540
you have to build it against the version

00:23:11,300 --> 00:23:17,240
of metals you're running in the cluster

00:23:13,540 --> 00:23:19,940
and in the project which we have put out

00:23:17,240 --> 00:23:23,420
in the open you have full instructions

00:23:19,940 --> 00:23:26,060
how to build modules and we have a

00:23:23,420 --> 00:23:27,650
docker container which will help in

00:23:26,060 --> 00:23:29,540
building the methyls module against any

00:23:27,650 --> 00:23:31,700
Methos versions you want we have

00:23:29,540 --> 00:23:33,920
provided all the other good practices on

00:23:31,700 --> 00:23:35,870
building memorials because that one is

00:23:33,920 --> 00:23:37,490
not documented very well there are

00:23:35,870 --> 00:23:39,830
different classification of modules so

00:23:37,490 --> 00:23:43,190
there is a location module which can

00:23:39,830 --> 00:23:45,020
switch the default DRF allocation

00:23:43,190 --> 00:23:47,930
algorithm of methods on on scheduling

00:23:45,020 --> 00:23:50,780
side you have isolator modules which we

00:23:47,930 --> 00:23:53,240
saw that in that slide we had different

00:23:50,780 --> 00:23:56,810
Isolators inside of maintainer Iser and

00:23:53,240 --> 00:24:01,160
then there is a module called hooks now

00:23:56,810 --> 00:24:03,980
hoops are basically a way to tie in to

00:24:01,160 --> 00:24:06,290
the events that a task has been launched

00:24:03,980 --> 00:24:10,250
a task has been killed when these things

00:24:06,290 --> 00:24:13,220
are happening these hook informants are

00:24:10,250 --> 00:24:17,330
called by the methods agent or the

00:24:13,220 --> 00:24:19,400
master so we very specifically implement

00:24:17,330 --> 00:24:24,080
one of the events which is the executor

00:24:19,400 --> 00:24:28,100
removal event and this we will see in

00:24:24,080 --> 00:24:31,400
the demo when executor crashes the

00:24:28,100 --> 00:24:33,380
misil's agent will still detect that the

00:24:31,400 --> 00:24:35,270
executor exited and you have an

00:24:33,380 --> 00:24:37,669
additional way to intercept that and

00:24:35,270 --> 00:24:39,559
make sure whether the pods were really

00:24:37,669 --> 00:24:41,870
cleaned up or it was leaked or things

00:24:39,559 --> 00:24:48,919
like that and that hook is guaranteed to

00:24:41,870 --> 00:24:51,950
get calls ok so how is the current

00:24:48,919 --> 00:24:53,840
ecosystem around pods looking as of last

00:24:51,950 --> 00:24:55,490
week let's put it that way because every

00:24:53,840 --> 00:24:59,510
cluster manager is like moving very fast

00:24:55,490 --> 00:25:00,980
so docker swarm as of 1.2 dot 6 that's

00:24:59,510 --> 00:25:04,460
the latest version I think it will move

00:25:00,980 --> 00:25:10,309
to 17 versions as well does not support

00:25:04,460 --> 00:25:13,370
local pods so you you cannot have native

00:25:10,309 --> 00:25:16,010
support for local pods what they have is

00:25:13,370 --> 00:25:18,530
the docker compose 3 dot X version you

00:25:16,010 --> 00:25:20,780
can launch the set of containers in

00:25:18,530 --> 00:25:23,419
docker swarm so they will land in

00:25:20,780 --> 00:25:25,400
multiple hosts in the cluster and by

00:25:23,419 --> 00:25:27,549
default they will have a overlay network

00:25:25,400 --> 00:25:30,289
so that all these containers can locally

00:25:27,549 --> 00:25:33,169
talk to each other but they are not

00:25:30,289 --> 00:25:35,929
treated as a single unit to be

00:25:33,169 --> 00:25:38,929
co-located sharing see groups or

00:25:35,929 --> 00:25:41,510
namespaces so it is not a pod I think

00:25:38,929 --> 00:25:44,780
they are working something on it so in

00:25:41,510 --> 00:25:47,510
the future that should help kubernetes

00:25:44,780 --> 00:25:49,340
has excellent support for pods in fact

00:25:47,510 --> 00:25:53,419
they are the ones who coined the term

00:25:49,340 --> 00:25:57,169
pods but they don't treat a docker as

00:25:53,419 --> 00:26:00,110
first-class and by that I mean is they

00:25:57,169 --> 00:26:03,320
have different volume specs when you

00:26:00,110 --> 00:26:05,179
want to do storage integrations they

00:26:03,320 --> 00:26:07,190
have different they don't follow that

00:26:05,179 --> 00:26:11,150
network they have their own specs for

00:26:07,190 --> 00:26:14,360
network integration is CNI so if you are

00:26:11,150 --> 00:26:17,200
looking at the CRI runtime which till

00:26:14,360 --> 00:26:19,610
date used to call docker engine is

00:26:17,200 --> 00:26:22,010
actually going to switch to container D

00:26:19,610 --> 00:26:24,200
there is a project active because

00:26:22,010 --> 00:26:26,750
container D has been donated to CN CF

00:26:24,200 --> 00:26:29,409
and they will completely skip docker

00:26:26,750 --> 00:26:32,210
engine to launch the kubernetes pods

00:26:29,409 --> 00:26:34,820
they don't need runtime in the future

00:26:32,210 --> 00:26:39,169
and of course therefore specs is

00:26:34,820 --> 00:26:41,270
different than the compose spec and you

00:26:39,169 --> 00:26:43,520
and even the docker commands don't work

00:26:41,270 --> 00:26:45,370
in the kubernetes cluster as I said they

00:26:43,520 --> 00:26:49,340
set up the networking differently they

00:26:45,370 --> 00:26:52,010
provide equivalent commands but not all

00:26:49,340 --> 00:26:54,170
doctor commands will work so it's it's

00:26:52,010 --> 00:26:57,440
pretty much you will have the image

00:26:54,170 --> 00:26:59,810
which is the common minimum thing but it

00:26:57,440 --> 00:27:02,150
is it's a different environment nothing

00:26:59,810 --> 00:27:06,740
bad about it but that's how when it is

00:27:02,150 --> 00:27:10,010
is kind of going ahead and missiles

00:27:06,740 --> 00:27:11,690
for a long long time used to natively

00:27:10,010 --> 00:27:13,970
support running docker containers

00:27:11,690 --> 00:27:16,010
through a docker container Iser but what

00:27:13,970 --> 00:27:17,960
it did was it used to only spawn one

00:27:16,010 --> 00:27:20,330
container against the task so you could

00:27:17,960 --> 00:27:23,030
not ever run a collection of containers

00:27:20,330 --> 00:27:25,280
against a task so in 1.1 they added a

00:27:23,030 --> 00:27:27,560
pod support through experimental task

00:27:25,280 --> 00:27:30,050
groups and nested container but this is

00:27:27,560 --> 00:27:32,030
not for docker so they have again taken

00:27:30,050 --> 00:27:34,520
the primitive option of it's a

00:27:32,030 --> 00:27:35,210
collection of tasks which you can model

00:27:34,520 --> 00:27:37,340
as a pod

00:27:35,210 --> 00:27:39,380
so the tasks could be containers could

00:27:37,340 --> 00:27:41,960
be POSIX processes could be something

00:27:39,380 --> 00:27:44,030
else and obviously that that

00:27:41,960 --> 00:27:48,170
specification is different than

00:27:44,030 --> 00:27:51,620
something like a compa spec and they are

00:27:48,170 --> 00:27:55,340
also going towards a universal container

00:27:51,620 --> 00:27:57,770
Iser approach to kind of swap out docker

00:27:55,340 --> 00:27:59,510
runtime you say that we will implement

00:27:57,770 --> 00:28:00,860
all the different Isolators whether it

00:27:59,510 --> 00:28:05,420
is a volume isolator and network

00:28:00,860 --> 00:28:08,060
isolator to create a container runtime

00:28:05,420 --> 00:28:09,730
which can consume a docker image but

00:28:08,060 --> 00:28:12,500
it's not the docker runtime

00:28:09,730 --> 00:28:15,490
however as I mention in the last point

00:28:12,500 --> 00:28:18,320
Messer's still continues to be the most

00:28:15,490 --> 00:28:21,980
flexible cluster manager out there

00:28:18,320 --> 00:28:24,710
because literally it can run any sort of

00:28:21,980 --> 00:28:29,630
workloads yoona kernels containers kvn

00:28:24,710 --> 00:28:31,550
POSIX processes it's it's not tied to

00:28:29,630 --> 00:28:33,050
containers and we could do something

00:28:31,550 --> 00:28:36,290
like docker compose executors with

00:28:33,050 --> 00:28:40,550
methods as well with that I think I will

00:28:36,290 --> 00:28:44,410
switch to the demo a little bit to see

00:28:40,550 --> 00:28:47,480
all of this what we talked about in

00:28:44,410 --> 00:28:50,150
action so I have a vagrant set up

00:28:47,480 --> 00:28:52,070
running and everything is local because

00:28:50,150 --> 00:28:55,280
I did not want to trust on the Wi-Fi

00:28:52,070 --> 00:28:57,230
connection so I have

00:28:55,280 --> 00:29:00,620
even the docker registry running locally

00:28:57,230 --> 00:29:04,130
in vagrant so what I'm going to do now

00:29:00,620 --> 00:29:06,620
is launch so let's see what we have a

00:29:04,130 --> 00:29:09,590
little quick we have the the methyls

00:29:06,620 --> 00:29:10,880
master of view it's showing that there

00:29:09,590 --> 00:29:11,420
are no tasks running right now in the

00:29:10,880 --> 00:29:15,080
cluster

00:29:11,420 --> 00:29:17,690
we have Aurora here which is a framework

00:29:15,080 --> 00:29:20,180
in methods to run services or long

00:29:17,690 --> 00:29:22,250
running tasks we have marathon is also

00:29:20,180 --> 00:29:24,890
another popular competing framework to

00:29:22,250 --> 00:29:28,460
launch running tasks along running tasks

00:29:24,890 --> 00:29:31,460
and with that first we are going to

00:29:28,460 --> 00:29:35,120
launch a workload in in Aurora

00:29:31,460 --> 00:29:39,440
now Aurora comes with a CLI client

00:29:35,120 --> 00:29:42,710
we wrote a thrift go library against it

00:29:39,440 --> 00:29:45,800
it's open sourced as well and I'm going

00:29:42,710 --> 00:29:48,970
to just launch and I will go through the

00:29:45,800 --> 00:29:53,540
details of that we will also launch a

00:29:48,970 --> 00:29:55,250
task in marathon in parallel so we can

00:29:53,540 --> 00:29:59,750
have things up and running as we go

00:29:55,250 --> 00:30:07,760
through it so let's go here go to

00:29:59,750 --> 00:30:09,830
marathon create application JSON mode so

00:30:07,760 --> 00:30:11,720
what we are trying to do is launch a pod

00:30:09,830 --> 00:30:15,680
in this case we have given point five

00:30:11,720 --> 00:30:18,140
CPU 64 Meg's memory to the pod we said

00:30:15,680 --> 00:30:21,230
we need three dynamic ports executors

00:30:18,140 --> 00:30:23,660
and then the URI is a standard way in

00:30:21,230 --> 00:30:26,090
metals to say what to resource artifacts

00:30:23,660 --> 00:30:29,060
you want the metals agent to download

00:30:26,090 --> 00:30:32,450
before you start the workload so in this

00:30:29,060 --> 00:30:35,540
case the app third has the docker

00:30:32,450 --> 00:30:38,480
compose files inside it along with the

00:30:35,540 --> 00:30:42,320
application bundle so with that we'll

00:30:38,480 --> 00:30:45,410
just do a we will launch that now if you

00:30:42,320 --> 00:30:47,660
go to metals we see the workload which

00:30:45,410 --> 00:30:50,600
was started by Aurora is already

00:30:47,660 --> 00:30:54,620
launched so let's go to the sandbox to

00:30:50,600 --> 00:30:57,890
see what else is happening there so

00:30:54,620 --> 00:31:01,010
there is a folder here now you see

00:30:57,890 --> 00:31:02,840
docker compose the AML file let's say

00:31:01,010 --> 00:31:05,390
that was the file which the developer

00:31:02,840 --> 00:31:08,870
tested in its development environment so

00:31:05,390 --> 00:31:10,820
if you open it and bring it this is

00:31:08,870 --> 00:31:15,970
chillie pure compose you mentioned the

00:31:10,820 --> 00:31:15,970
version at the top and is it okay or

00:31:16,090 --> 00:31:27,559
okay so you can see there's a node.js

00:31:24,200 --> 00:31:30,650
service running we have a nginx service

00:31:27,559 --> 00:31:34,880
running and in this case nginx is doing

00:31:30,650 --> 00:31:36,679
the SSL termination for node and there's

00:31:34,880 --> 00:31:38,570
a reason for that node is not super

00:31:36,679 --> 00:31:40,549
efficient in that and they are using

00:31:38,570 --> 00:31:41,899
let's in a local environment they are

00:31:40,549 --> 00:31:44,210
using bridge networking they even are

00:31:41,899 --> 00:31:46,580
hot coding the ports what port they want

00:31:44,210 --> 00:31:50,029
in host as well as we see from this

00:31:46,580 --> 00:31:52,520
example and then they define a health

00:31:50,029 --> 00:31:54,409
check for the web or the node.js

00:31:52,520 --> 00:31:56,600
container and it can mean a different

00:31:54,409 --> 00:32:00,590
compose file compose merges those files

00:31:56,600 --> 00:32:02,600
now when compose executors launched it

00:32:00,590 --> 00:32:04,760
created first the infrastructure

00:32:02,600 --> 00:32:07,940
container so the infrastructure

00:32:04,760 --> 00:32:11,510
container is the secret container it is

00:32:07,940 --> 00:32:14,450
defined as a service of there and the

00:32:11,510 --> 00:32:17,510
first thing you see is it got all the

00:32:14,450 --> 00:32:21,830
ports defined in the different composed

00:32:17,510 --> 00:32:23,899
files together as part of the network

00:32:21,830 --> 00:32:25,309
container itself because for docker the

00:32:23,899 --> 00:32:27,799
primary container which is setting up

00:32:25,309 --> 00:32:31,279
the interface has to get the ports there

00:32:27,799 --> 00:32:34,640
as well so it does that in the C group

00:32:31,279 --> 00:32:37,039
parent if you see we specify what is the

00:32:34,640 --> 00:32:39,860
parent metal c group so it is there as

00:32:37,039 --> 00:32:42,529
well and then labels like executor ID

00:32:39,860 --> 00:32:44,899
and task ID these are metals label has

00:32:42,529 --> 00:32:48,919
been added and we are using bridge

00:32:44,899 --> 00:32:51,649
networking in this case now if i see the

00:32:48,919 --> 00:32:54,080
generated docker compose the base file

00:32:51,649 --> 00:32:56,510
and we generated this file out of it the

00:32:54,080 --> 00:32:57,799
port sections have vanished as you see

00:32:56,510 --> 00:33:00,830
because ports have now landed in a

00:32:57,799 --> 00:33:04,220
network proxy here is the network node

00:33:00,830 --> 00:33:08,390
which is now using the collapsed network

00:33:04,220 --> 00:33:12,890
namespace of the of the net for proxy

00:33:08,390 --> 00:33:18,350
there so that is how things are as we

00:33:12,890 --> 00:33:21,110
are collapsing namespaces and if we go

00:33:18,350 --> 00:33:22,670
back here we see actually Marathon has

00:33:21,110 --> 00:33:25,160
also started

00:33:22,670 --> 00:33:28,130
the task now one of the things you will

00:33:25,160 --> 00:33:30,260
see here different to see the plug-in

00:33:28,130 --> 00:33:34,190
now in action is the same example which

00:33:30,260 --> 00:33:36,710
we launched is if I open the docker

00:33:34,190 --> 00:33:39,980
compose generated file there has been an

00:33:36,710 --> 00:33:42,110
additional test label getting attached

00:33:39,980 --> 00:33:45,440
to the container and this is what the

00:33:42,110 --> 00:33:50,840
plug-in implemented so the plug-in

00:33:45,440 --> 00:33:53,660
definitions are here so let me go here

00:33:50,840 --> 00:33:57,070
the config so this is what is saying

00:33:53,660 --> 00:34:00,080
that the general plugin is by default a

00:33:57,070 --> 00:34:02,240
bundled with the executor but if you

00:34:00,080 --> 00:34:04,730
have other plugins you can define that

00:34:02,240 --> 00:34:07,430
the code is already compiling inside the

00:34:04,730 --> 00:34:10,690
executor but it activates your plugins

00:34:07,430 --> 00:34:15,350
as you go forward or as you configure

00:34:10,690 --> 00:34:18,980
now we will do quick two things so what

00:34:15,350 --> 00:34:22,399
we will do is we're going to see some

00:34:18,980 --> 00:34:24,140
things real quick so I do a docker PS we

00:34:22,399 --> 00:34:25,760
see a pod running in two minutes this is

00:34:24,140 --> 00:34:27,290
where marathon launched it and four

00:34:25,760 --> 00:34:30,320
minutes which is the set of containers

00:34:27,290 --> 00:34:34,580
which Aurora launched and what I'm going

00:34:30,320 --> 00:34:37,040
to do is I'm going to kill as if

00:34:34,580 --> 00:34:43,330
simulating a container crashing out the

00:34:37,040 --> 00:34:48,470
node.js container here in the pod run by

00:34:43,330 --> 00:34:54,320
Marathon and if you go here to message

00:34:48,470 --> 00:34:58,970
and open the sandbox and open the logs

00:34:54,320 --> 00:35:01,880
the logs will actually show you all what

00:34:58,970 --> 00:35:06,650
is happening with the executor so if you

00:35:01,880 --> 00:35:09,860
see it detected that the pod failed and

00:35:06,650 --> 00:35:11,510
then it is signaling two methyls agent

00:35:09,860 --> 00:35:14,450
that actually we are killing the entire

00:35:11,510 --> 00:35:17,570
pod and consider the task as failed and

00:35:14,450 --> 00:35:23,210
when message gets those things it will

00:35:17,570 --> 00:35:25,910
go and replace so if you just see it's

00:35:23,210 --> 00:35:27,950
running just now because methyls got the

00:35:25,910 --> 00:35:31,610
signal kill the pod and has restarted

00:35:27,950 --> 00:35:35,000
the pod so one last example I want say

00:35:31,610 --> 00:35:36,660
if I do a docker PS here it's just

00:35:35,000 --> 00:35:42,630
coming up 26 seconds

00:35:36,660 --> 00:35:46,500
if you see one thing I want to show the

00:35:42,630 --> 00:35:52,319
rare rare scenario of the modules coming

00:35:46,500 --> 00:35:55,079
into the picture so you have so the

00:35:52,319 --> 00:35:58,200
first executor is the one which marathon

00:35:55,079 --> 00:36:00,319
is running the second one is what aura

00:35:58,200 --> 00:36:03,240
is running so what I'm going to do is

00:36:00,319 --> 00:36:05,970
think you are upgrading your methyls

00:36:03,240 --> 00:36:08,700
agent and your method agent is not

00:36:05,970 --> 00:36:11,400
running either the tasks continue to run

00:36:08,700 --> 00:36:18,569
in this world so I'm going to just stop

00:36:11,400 --> 00:36:21,180
it and then I'm going to just kill the

00:36:18,569 --> 00:36:26,940
executor there let me see

00:36:21,180 --> 00:36:28,859
three one two zero so your executor

00:36:26,940 --> 00:36:31,109
basically crashed out if I'm doing

00:36:28,859 --> 00:36:33,089
docker PS everything is running your

00:36:31,109 --> 00:36:35,520
containers are supposed to run by

00:36:33,089 --> 00:36:36,839
default when agent doesn't run because

00:36:35,520 --> 00:36:38,520
there is checkpointing enable but when

00:36:36,839 --> 00:36:41,490
the executor crashed all your containers

00:36:38,520 --> 00:36:43,740
are still leaking here now what I do is

00:36:41,490 --> 00:36:47,880
the container is managed by the math on

00:36:43,740 --> 00:36:49,980
task so what we do is we start let's say

00:36:47,880 --> 00:36:53,910
maintenance has completed slaves are the

00:36:49,980 --> 00:36:57,270
agent is coming up and if we do a docker

00:36:53,910 --> 00:37:00,480
PS real quick you see basically see the

00:36:57,270 --> 00:37:02,940
containers have vanished because as the

00:37:00,480 --> 00:37:06,390
slaves started agents started up it

00:37:02,940 --> 00:37:08,309
actually uses the checkpointing feature

00:37:06,390 --> 00:37:10,200
in methods to figure it out one of the

00:37:08,309 --> 00:37:13,319
executors which are supposed to run what

00:37:10,200 --> 00:37:16,230
are not and for the existed executors it

00:37:13,319 --> 00:37:18,539
fired that hook which we intercepted and

00:37:16,230 --> 00:37:22,170
cleaned up the containers which the

00:37:18,539 --> 00:37:25,470
executor could not clean up and if I'm

00:37:22,170 --> 00:37:28,260
back to the metals dashboard you can see

00:37:25,470 --> 00:37:30,240
the containers are basically now again

00:37:28,260 --> 00:37:34,950
getting respond because they were killed

00:37:30,240 --> 00:37:36,960
in that sense and in the true sense if

00:37:34,950 --> 00:37:38,279
you want to kill the entire pot because

00:37:36,960 --> 00:37:40,319
you really want to kill it

00:37:38,279 --> 00:37:42,660
I don't want to do a create application

00:37:40,319 --> 00:37:44,250
anymore I can just destroy the

00:37:42,660 --> 00:37:47,789
applications which will call the

00:37:44,250 --> 00:37:50,289
lifecycle management methods to kill a

00:37:47,789 --> 00:37:53,259
pod in the executor and I can

00:37:50,289 --> 00:37:56,559
both of them out actually that is they

00:37:53,259 --> 00:38:03,549
pretty much the end of the demo you can

00:37:56,559 --> 00:38:05,410
do kill in hora and that should do it

00:38:03,549 --> 00:38:07,390
and it will take a while to message to

00:38:05,410 --> 00:38:09,609
get the signal so one of the tasks has

00:38:07,390 --> 00:38:15,339
already been killed the second one will

00:38:09,609 --> 00:38:20,669
get killed as it's happening so that was

00:38:15,339 --> 00:38:23,109
pretty much the demo and so we have the

00:38:20,669 --> 00:38:25,900
executor which is out there in the open

00:38:23,109 --> 00:38:28,119
and the first point the second one says

00:38:25,900 --> 00:38:30,459
which Java executors we deprecated it

00:38:28,119 --> 00:38:31,989
doesn't have a lot of features and I

00:38:30,459 --> 00:38:33,489
took the diagram some methods from the

00:38:31,989 --> 00:38:36,969
stock didn't want Orion in the wheel

00:38:33,489 --> 00:38:38,949
here so quick recap words you have

00:38:36,969 --> 00:38:39,519
methods as the cluster management of

00:38:38,949 --> 00:38:42,249
choice

00:38:39,519 --> 00:38:43,929
you--you'll of docker engine and all the

00:38:42,249 --> 00:38:46,539
doctor tooling with it you don't want to

00:38:43,929 --> 00:38:49,089
let go of that thing and you want to

00:38:46,539 --> 00:38:51,969
solve this problem at scale so that is

00:38:49,089 --> 00:38:54,219
the recap of the talk the engineers

00:38:51,969 --> 00:38:56,799
working in this project Tamar and Mia

00:38:54,219 --> 00:39:00,159
are here as well so we can take any

00:38:56,799 --> 00:39:18,299
questions if you go after this so I can

00:39:00,159 --> 00:39:18,299
take questions at this point yes but it

00:39:22,499 --> 00:39:33,400
right right yeah so one thing about the

00:39:31,749 --> 00:39:35,559
task groups and the nettles nested

00:39:33,400 --> 00:39:37,449
container feature is a you have to make

00:39:35,559 --> 00:39:39,429
changes in the framework so if you are

00:39:37,449 --> 00:39:41,319
using Aurora marathon has started adding

00:39:39,429 --> 00:39:43,539
adding added the features if you are

00:39:41,319 --> 00:39:44,859
using order or singularity first to even

00:39:43,539 --> 00:39:46,269
consume that feature you have to make

00:39:44,859 --> 00:39:47,619
changes in the framework whereas the

00:39:46,269 --> 00:39:49,599
approach we have taken is you can just

00:39:47,619 --> 00:39:53,979
plug and play with the frameworks the

00:39:49,599 --> 00:39:56,829
second thing is we wanted to really say

00:39:53,979 --> 00:39:58,299
that developers don't care about cluster

00:39:56,829 --> 00:40:00,519
managers as they are developing in the

00:39:58,299 --> 00:40:03,159
local environment and let them define

00:40:00,519 --> 00:40:03,700
the pod specs and things like that and

00:40:03,159 --> 00:40:06,130
we

00:40:03,700 --> 00:40:08,140
can we take the exact spec file and run

00:40:06,130 --> 00:40:09,880
it in a classroom manager you can't do

00:40:08,140 --> 00:40:12,040
that because now as we said even

00:40:09,880 --> 00:40:13,869
kubernetes or the task groups will have

00:40:12,040 --> 00:40:15,369
a different way to define the same

00:40:13,869 --> 00:40:17,349
containers of course we can run like

00:40:15,369 --> 00:40:19,240
write translators and things like that

00:40:17,349 --> 00:40:38,770
so we will just wait and watch how

00:40:19,240 --> 00:40:40,030
things are so yeah yeah yeah docker

00:40:38,770 --> 00:40:44,380
engine crashing yeah and things like

00:40:40,030 --> 00:40:47,079
that so one thing is if you are having a

00:40:44,380 --> 00:40:49,089
cluster manager behind the scenes some

00:40:47,079 --> 00:40:51,730
of those things can often get masked

00:40:49,089 --> 00:40:54,070
because if the engine crashes by default

00:40:51,730 --> 00:40:55,690
it does take the containers down with it

00:40:54,070 --> 00:40:58,450
unless you are using the library store

00:40:55,690 --> 00:41:00,640
flag and the cluster will just replace

00:40:58,450 --> 00:41:03,040
the running tasks you know another

00:41:00,640 --> 00:41:05,140
healthy node if you don't have a cluster

00:41:03,040 --> 00:41:07,359
manager behind the scenes then you are

00:41:05,140 --> 00:41:09,880
having issues that yes my engine is

00:41:07,359 --> 00:41:11,650
inner-ear said but we have we run it at

00:41:09,880 --> 00:41:14,680
large scale especially the latest

00:41:11,650 --> 00:41:17,470
versions of docker and we haven't seen

00:41:14,680 --> 00:41:19,780
the issues of the past which we used to

00:41:17,470 --> 00:41:21,849
see a lot of the indie instability which

00:41:19,780 --> 00:41:23,980
led to a lot of the people saying that

00:41:21,849 --> 00:41:26,710
we don't need the runtime we haven't

00:41:23,980 --> 00:41:29,589
faced it so we have around this executor

00:41:26,710 --> 00:41:32,950
at scale in in public clouds as well up

00:41:29,589 --> 00:41:37,440
to like 50,000 of these pods running and

00:41:32,950 --> 00:41:37,440
and haven't kind of faced any issues

00:41:41,730 --> 00:41:50,760
yeah sure yeah

00:41:59,299 --> 00:42:07,109
yeah yeah so that's the common pod

00:42:04,380 --> 00:42:08,640
versus application group kind of concept

00:42:07,109 --> 00:42:11,160
so if you are running let's say a web

00:42:08,640 --> 00:42:13,319
and a Redis or a my sequel it's a

00:42:11,160 --> 00:42:16,410
distributed that is not a pod so that

00:42:13,319 --> 00:42:18,839
you continue to do that so your pod is I

00:42:16,410 --> 00:42:21,599
have a main application container but I

00:42:18,839 --> 00:42:24,119
need to run nginx to do my SSL terminate

00:42:21,599 --> 00:42:27,000
or I have the fight card container to do

00:42:24,119 --> 00:42:29,730
certain work which is not the main

00:42:27,000 --> 00:42:32,460
application containers job but you still

00:42:29,730 --> 00:42:35,280
need that to make the application run

00:42:32,460 --> 00:42:36,839
and there is ambassador pattern there is

00:42:35,280 --> 00:42:39,839
adapter patterns where you of sending

00:42:36,839 --> 00:42:42,750
monitoring logs to a common interface so

00:42:39,839 --> 00:42:45,930
it is the application itself broken into

00:42:42,750 --> 00:43:00,029
components not the distributed nature of

00:42:45,930 --> 00:43:03,660
it yeah yep yeah no so a single service

00:43:00,029 --> 00:43:07,440
broken into smaller parts and that is

00:43:03,660 --> 00:43:10,559
what a pod is but if your system needs

00:43:07,440 --> 00:43:13,140
service a service B service C service D

00:43:10,559 --> 00:43:15,569
these would be actually different things

00:43:13,140 --> 00:43:17,430
not co-located together but the service

00:43:15,569 --> 00:43:23,970
a needs to be co-located together with

00:43:17,430 --> 00:43:25,859
its components right example so for

00:43:23,970 --> 00:43:28,799
example the example we showed in in

00:43:25,859 --> 00:43:30,690
PayPal when we run node.js we run nginx

00:43:28,799 --> 00:43:32,099
for let's say SSL termination we don't

00:43:30,690 --> 00:43:35,339
exactly like that

00:43:32,099 --> 00:43:37,589
a lot of people use promises as let's

00:43:35,339 --> 00:43:40,170
say the container like monitoring and

00:43:37,589 --> 00:43:42,960
stats you might have your application

00:43:40,170 --> 00:43:45,900
container doing legacy formatting of the

00:43:42,960 --> 00:43:49,170
log messages which the site card

00:43:45,900 --> 00:43:51,390
container can consume and then send it

00:43:49,170 --> 00:43:53,190
to Prometheus so it's like a adapter

00:43:51,390 --> 00:43:54,869
container running with your application

00:43:53,190 --> 00:43:57,089
container similarly you can have

00:43:54,869 --> 00:43:59,789
Ambassador containers where you don't

00:43:57,089 --> 00:44:01,349
want to say that your main endpoint is

00:43:59,789 --> 00:44:03,450
radius but you're running some sort of

00:44:01,349 --> 00:44:04,790
proxy so that from reddit you can change

00:44:03,450 --> 00:44:06,530
to may impact memcache

00:44:04,790 --> 00:44:08,420
thing like that so it's like you can run

00:44:06,530 --> 00:44:11,720
like service discovery site car

00:44:08,420 --> 00:44:14,330
containers so it is a very very common

00:44:11,720 --> 00:44:15,380
pattern when you go to enterprise ninety

00:44:14,330 --> 00:44:19,000
percent of the time you will have a

00:44:15,380 --> 00:44:19,000
situation you need sidecar containers

00:44:28,990 --> 00:44:35,390
things we have we have our own discovery

00:44:32,450 --> 00:44:36,890
system or old logging supposes so we

00:44:35,390 --> 00:44:38,390
would run a separate container that

00:44:36,890 --> 00:44:40,640
would be an adapter to say I'm going to

00:44:38,390 --> 00:44:42,770
register them taxing or a service

00:44:40,640 --> 00:44:44,570
discovery run that container there

00:44:42,770 --> 00:44:47,180
because it shares the same local network

00:44:44,570 --> 00:44:50,870
same IP address and it's a global volume

00:44:47,180 --> 00:44:52,430
and so it can represent memcache into

00:44:50,870 --> 00:44:54,410
our services studies assistant will

00:44:52,430 --> 00:44:56,570
represent been attached into the our

00:44:54,410 --> 00:44:58,220
logging system snap it in without having

00:44:56,570 --> 00:45:00,950
to change the event a stalker to tear

00:44:58,220 --> 00:45:02,960
itself so we can create a standard

00:45:00,950 --> 00:45:05,170
sidecars so that anyone could say well

00:45:02,960 --> 00:45:07,670
use our Tommy system review the our

00:45:05,170 --> 00:45:09,200
discovery systems add the container to

00:45:07,670 --> 00:45:10,600
your collection containers for your

00:45:09,200 --> 00:45:14,750
specific application

00:45:10,600 --> 00:45:17,630
yeah well and actually there was a great

00:45:14,750 --> 00:45:20,360
talk by Brendan burns in doc upon 15

00:45:17,630 --> 00:45:22,370
2015 about composition containers where

00:45:20,360 --> 00:45:31,010
he goes about all the patterns of our

00:45:22,370 --> 00:45:32,100
pod okay sounds good I think you get 5s

00:45:31,010 --> 00:45:35,650
back thank you

00:45:32,100 --> 00:45:35,650

YouTube URL: https://www.youtube.com/watch?v=P8TMO_wh41w


