Title: Evaluating Text Extraction: Apache Tika's™ New Tika-Eval Module - Tim Allison, The MITRE Corporation
Publication date: 2017-05-19
Playlist: ApacheCon 2017 - Miami
Description: 
	Evaluating Text Extraction: Apache Tika's™ New Tika-Eval Module - Tim Allison, The MITRE Corporation

Text extraction tools are essential for obtaining the textual content and metadata of computer files for use in a wide variety of applications, including search and natural language processing tools. Techniques and tools for evaluating text extraction tools are missing from academia and industry. 

Apache Tika™ detects file types and extracts metadata and text from many file types. Tika is a crucial component in a wide variety of tools, including Solr™, Nutch™, Alfresco, Elasticsearch and Sleuth Kit®/Autopsy®.

In this talk, we will give an overview of the new tika-eval module that allows developers to evaluate Tika and other content extraction systems. This talk will end with a brief discussion of the results of taking this evaluation methodology public and evaluating Tika on large batches of public domain documents on a public vm over the last two years.

About Tim Allison
Tim has been working in natural language processing since 2002. In recent years, his focus has shifted to advanced search and content/metadata extraction. Tim is committer and PMC member on Apache PDFBox (since September 2016), and on Apache POI and Apache Tika since (July, 2013). Tim holds a Ph.D. in Classical Studies from the University of Michigan, and in a former life, he was a professor of Latin and Greek.
Captions: 
	00:00:00,000 --> 00:00:06,750
all righty welcome to my talk my name is

00:00:04,529 --> 00:00:08,160
Tim Allison I've been asked to speak at

00:00:06,750 --> 00:00:09,300
the podium so I will stay at the podium

00:00:08,160 --> 00:00:11,429
I will not wander around and be a

00:00:09,300 --> 00:00:13,710
dynamic speaker like I've been trained

00:00:11,429 --> 00:00:15,630
to be as if I've had training

00:00:13,710 --> 00:00:18,630
anyways I work at The MITRE corporation

00:00:15,630 --> 00:00:22,949
on the committer on Apache poi TF box

00:00:18,630 --> 00:00:25,500
and tikka so off we go with evaluating

00:00:22,949 --> 00:00:29,310
text extraction with Pak Apache chicas

00:00:25,500 --> 00:00:35,730
new tika eval module and this is where

00:00:29,310 --> 00:00:37,680
the next slide happens naturally yay

00:00:35,730 --> 00:00:40,320
okay so I have a bunch of debts of

00:00:37,680 --> 00:00:42,809
gratitude my my debt of gratitude rather

00:00:40,320 --> 00:00:45,030
large first to David smiley commit her

00:00:42,809 --> 00:00:49,050
on Patchi Solar who first got me off of

00:00:45,030 --> 00:00:51,030
my laptop and into open source I without

00:00:49,050 --> 00:00:53,010
him I would I would definitely not be

00:00:51,030 --> 00:00:55,500
here Nick Burch of course brought me in

00:00:53,010 --> 00:00:59,399
even though my first patches were done

00:00:55,500 --> 00:01:02,329
in Notepad and groovy converted to Java

00:00:59,399 --> 00:01:06,479
so sorry and yet thank you so much Nick

00:01:02,329 --> 00:01:08,280
same for Chris madmen who fostered my

00:01:06,479 --> 00:01:12,720
early early stuff and has been a

00:01:08,280 --> 00:01:14,760
fantastic collaborator on tika Tillman

00:01:12,720 --> 00:01:16,259
house sure on Apache PDF box has been a

00:01:14,760 --> 00:01:18,030
great colleague and working with

00:01:16,259 --> 00:01:19,920
especially this eval stuff and helping

00:01:18,030 --> 00:01:24,479
us figure out what metrics we want to

00:01:19,920 --> 00:01:26,490
use in the eval Dominic Stadler from the

00:01:24,479 --> 00:01:30,030
Apache poi project has been really

00:01:26,490 --> 00:01:31,920
helpful in bird of a feather and

00:01:30,030 --> 00:01:33,210
gathering common crawl data and running

00:01:31,920 --> 00:01:35,159
large-scale regression testing and of

00:01:33,210 --> 00:01:37,710
course all of my other fellow devs and

00:01:35,159 --> 00:01:39,869
users on Apache Commons Poe API Paktika

00:01:37,710 --> 00:01:43,759
and the entire ASF community thank you

00:01:39,869 --> 00:01:47,880
all marvelous community to be a part of

00:01:43,759 --> 00:01:49,560
so those are the people also hugely

00:01:47,880 --> 00:01:52,020
indebted to common crawled common crawl

00:01:49,560 --> 00:01:54,090
project from which we've gathered a

00:01:52,020 --> 00:01:55,560
number of documents so that we can run

00:01:54,090 --> 00:01:57,899
our regression testing and also the Gov

00:01:55,560 --> 00:01:59,850
Docs one corpus which was gathered a

00:01:57,899 --> 00:02:01,979
number of years ago and then Rackspace

00:01:59,850 --> 00:02:06,270
has kindly hosted a VM for us and I'll

00:02:01,979 --> 00:02:07,469
talk about that public vm shortly so

00:02:06,270 --> 00:02:11,069
anyways I'd like to start by saying

00:02:07,469 --> 00:02:13,330
thank you to so many people so an

00:02:11,069 --> 00:02:14,740
overview of the talk today I'll talk

00:02:13,330 --> 00:02:16,510
a little bit about content and metadata

00:02:14,740 --> 00:02:18,490
extraction in case anybody somehow

00:02:16,510 --> 00:02:21,880
doesn't know anything about tika this is

00:02:18,490 --> 00:02:24,910
the tika a medium or tika 200 class not

00:02:21,880 --> 00:02:26,950
the tika 101 but I'll go over a little

00:02:24,910 --> 00:02:28,690
bit of what it does I'll talk a little

00:02:26,950 --> 00:02:30,460
bit about the motivation for Tiki valve

00:02:28,690 --> 00:02:32,620
I mean what could possibly go wrong with

00:02:30,460 --> 00:02:34,780
text extraction I will tell you we some

00:02:32,620 --> 00:02:36,400
of the things I've encountered and I'll

00:02:34,780 --> 00:02:38,560
talk about an overview of what's in the

00:02:36,400 --> 00:02:40,780
new package the workflow and using it

00:02:38,560 --> 00:02:42,250
and then I'll go back and forth a little

00:02:40,780 --> 00:02:44,830
bit and talk there a little bit about

00:02:42,250 --> 00:02:47,050
our terabyte public corpus that we use

00:02:44,830 --> 00:02:48,910
for regression testing before the next

00:02:47,050 --> 00:02:52,720
version of tika for the next version of

00:02:48,910 --> 00:02:54,970
POI and also PDF box I'll also talk

00:02:52,720 --> 00:02:57,100
about limitations I do not have an easy

00:02:54,970 --> 00:03:00,370
button I've seen the word magic and to

00:02:57,100 --> 00:03:01,780
talks earlier today I have no magic

00:03:00,370 --> 00:03:04,720
sorry

00:03:01,780 --> 00:03:07,480
I also yeah so if you came from magic I

00:03:04,720 --> 00:03:07,840
don't want to disappoint it's still

00:03:07,480 --> 00:03:09,730
early

00:03:07,840 --> 00:03:11,950
David North is giving a fantastic talk

00:03:09,730 --> 00:03:14,080
on Apache poi and the room over there so

00:03:11,950 --> 00:03:17,380
if you're looking for magic that might

00:03:14,080 --> 00:03:19,390
be a better option all right so you

00:03:17,380 --> 00:03:20,650
might ask if you are paying attention of

00:03:19,390 --> 00:03:23,050
these things and looking at my slides

00:03:20,650 --> 00:03:24,730
from two years ago well what's what's

00:03:23,050 --> 00:03:26,860
really different from their talk two

00:03:24,730 --> 00:03:29,050
years ago well now it works and now it's

00:03:26,860 --> 00:03:31,030
actually integrated into tika and it

00:03:29,050 --> 00:03:32,769
will come out with the next release

00:03:31,030 --> 00:03:33,489
which we should be kicking off tonight

00:03:32,769 --> 00:03:36,190
or tomorrow

00:03:33,489 --> 00:03:38,230
I think we're cleared to go now on tika

00:03:36,190 --> 00:03:41,080
1:15 so a lot of work has been done

00:03:38,230 --> 00:03:45,250
especially iterating with Tilghman house

00:03:41,080 --> 00:03:48,340
sure on on PDF box to improve evaluation

00:03:45,250 --> 00:03:50,050
a lot as a lot of things have have been

00:03:48,340 --> 00:03:53,590
improved and it out and it now basically

00:03:50,050 --> 00:03:56,680
works so first off content extraction

00:03:53,590 --> 00:03:58,300
and human language technology for folks

00:03:56,680 --> 00:04:00,100
who do more fun things like energy

00:03:58,300 --> 00:04:01,300
extraction and search and machine

00:04:00,100 --> 00:04:03,580
translation and whatnot

00:04:01,300 --> 00:04:05,590
the stuff that we do on Apache Tech is

00:04:03,580 --> 00:04:07,060
really boring we get stuff from all

00:04:05,590 --> 00:04:09,850
sorts of different file formats at the

00:04:07,060 --> 00:04:11,410
bottom over that red line so that they n

00:04:09,850 --> 00:04:14,650
have text so that then they can do the

00:04:11,410 --> 00:04:17,709
more interesting things like search or

00:04:14,650 --> 00:04:19,209
energy extraction and so on but it's a

00:04:17,709 --> 00:04:21,190
challenge to get all of those different

00:04:19,209 --> 00:04:22,990
file formats figure out which file which

00:04:21,190 --> 00:04:25,000
which type of file you're looking at and

00:04:22,990 --> 00:04:27,320
then apply the appropriate parser a nice

00:04:25,000 --> 00:04:29,600
thing about apache tika and the goal

00:04:27,320 --> 00:04:31,820
of it is to have the same interface no

00:04:29,600 --> 00:04:33,770
matter what type of file you aim at it

00:04:31,820 --> 00:04:36,680
so that we don't all have to go reinvent

00:04:33,770 --> 00:04:39,680
our own file ID and then figure out

00:04:36,680 --> 00:04:43,190
which parsers to apply so the whole goal

00:04:39,680 --> 00:04:44,720
of Apache Tiki is to get from bytes at

00:04:43,190 --> 00:04:47,900
the bottom to something that we can then

00:04:44,720 --> 00:04:49,160
process at a higher level speaking of

00:04:47,900 --> 00:04:52,030
which I learned that this week is

00:04:49,160 --> 00:04:54,950
national infrastructure week in the u.s.

00:04:52,030 --> 00:04:56,840
celebrating infrastructure projects that

00:04:54,950 --> 00:05:00,710
nobody pays attention to or nobody cares

00:04:56,840 --> 00:05:04,820
about so this is not deep learning IOT

00:05:00,710 --> 00:05:06,410
stuff but it is critical to to those

00:05:04,820 --> 00:05:08,480
kinds of things because if content

00:05:06,410 --> 00:05:09,800
extraction fails those other downstream

00:05:08,480 --> 00:05:13,640
more interesting and more exciting

00:05:09,800 --> 00:05:15,470
projects will also not fare well so

00:05:13,640 --> 00:05:17,270
typically high-level components of a

00:05:15,470 --> 00:05:20,450
media processing stack you start out

00:05:17,270 --> 00:05:22,730
with files or structured data and you

00:05:20,450 --> 00:05:24,110
throw those into your something and of

00:05:22,730 --> 00:05:25,310
course this is multi-step of collapse

00:05:24,110 --> 00:05:27,410
this because I'm interested in the

00:05:25,310 --> 00:05:28,760
bottom part and then you have a user

00:05:27,410 --> 00:05:31,790
interface on top of that and I'll come

00:05:28,760 --> 00:05:33,500
back to this shortly but let's not

00:05:31,790 --> 00:05:34,700
forget about metadata another useful

00:05:33,500 --> 00:05:36,470
thing that teeka does is it will pull

00:05:34,700 --> 00:05:37,790
out metadata that's embedded or stored

00:05:36,470 --> 00:05:41,390
within the documents so you can often

00:05:37,790 --> 00:05:45,320
get the who at least who said they were

00:05:41,390 --> 00:05:46,700
who self-identified as the author you

00:05:45,320 --> 00:05:49,190
can get digital signatures you can get

00:05:46,700 --> 00:05:51,380
company from emails the from twos and so

00:05:49,190 --> 00:05:53,900
on you can often get Hardware versions

00:05:51,380 --> 00:05:55,820
or names software versions or names you

00:05:53,900 --> 00:05:58,040
can sometimes get globally unique file

00:05:55,820 --> 00:06:00,200
IDs if your file happens have XMP in it

00:05:58,040 --> 00:06:02,320
which can be quite useful for some use

00:06:00,200 --> 00:06:04,790
cases you also get title and keywords

00:06:02,320 --> 00:06:08,450
sometimes you can even get a geolocation

00:06:04,790 --> 00:06:10,550
so for images that have latitude and

00:06:08,450 --> 00:06:11,750
longitude you can extract that sometimes

00:06:10,550 --> 00:06:13,580
even you can get the original file

00:06:11,750 --> 00:06:16,010
location for where it was last saved on

00:06:13,580 --> 00:06:17,480
on somebody's hard drive or sometimes

00:06:16,010 --> 00:06:20,270
you can get that for embedded images in

00:06:17,480 --> 00:06:21,380
a file I thought that Microsoft had been

00:06:20,270 --> 00:06:23,420
moving away from that but I just

00:06:21,380 --> 00:06:25,730
discovered recently that Microsoft has

00:06:23,420 --> 00:06:29,120
started putting that back into xlsx

00:06:25,730 --> 00:06:31,370
files in their future ish thing for OS

00:06:29,120 --> 00:06:33,830
ml so we can now extract where somebody

00:06:31,370 --> 00:06:35,870
last saved their xlsx file which for

00:06:33,830 --> 00:06:37,280
some applications may be useful then

00:06:35,870 --> 00:06:39,650
also of course when when it was created

00:06:37,280 --> 00:06:41,000
less modified and printed so beyond the

00:06:39,650 --> 00:06:42,230
standard types there's all sorts of fun

00:06:41,000 --> 00:06:43,760
custom metadata that you can pull out of

00:06:42,230 --> 00:06:46,880
files if that's the kind of thing you're

00:06:43,760 --> 00:06:47,390
interested in if you're into that kind

00:06:46,880 --> 00:06:49,820
of thing

00:06:47,390 --> 00:06:51,590
so this is what I call blood in the

00:06:49,820 --> 00:06:54,620
blood on the highway which is a

00:06:51,590 --> 00:06:56,690
reference to driving films from when I

00:06:54,620 --> 00:06:58,880
was younger for actually from before my

00:06:56,690 --> 00:07:00,890
time where people are being scared

00:06:58,880 --> 00:07:02,840
scared straight about you know not not

00:07:00,890 --> 00:07:05,780
drinking and driving or being careful

00:07:02,840 --> 00:07:06,950
while driving when I talk about these

00:07:05,780 --> 00:07:08,300
things these kinds of things don't

00:07:06,950 --> 00:07:10,400
happen all the time and teka teka

00:07:08,300 --> 00:07:12,140
actually works really really well but

00:07:10,400 --> 00:07:14,180
given that I sit and watch the giro a

00:07:12,140 --> 00:07:16,700
whole lot and given that I run tika

00:07:14,180 --> 00:07:18,169
against a whole lot of files I see a lot

00:07:16,700 --> 00:07:19,400
of things that can go wrong with tika so

00:07:18,169 --> 00:07:21,350
that's what you're hearing from me today

00:07:19,400 --> 00:07:22,970
what you should not take away from this

00:07:21,350 --> 00:07:25,340
is the teak is a total disaster and you

00:07:22,970 --> 00:07:29,390
should run from it it actually does work

00:07:25,340 --> 00:07:31,160
quite well but this is more of the kind

00:07:29,390 --> 00:07:33,590
of a taxonomy of when things go wrong

00:07:31,160 --> 00:07:35,240
with sex extraction coming up so in this

00:07:33,590 --> 00:07:37,220
example application I focus on search

00:07:35,240 --> 00:07:38,660
which is one of the more basic and

00:07:37,220 --> 00:07:40,340
primary things that people often do with

00:07:38,660 --> 00:07:43,820
content that's pulled out of slides or

00:07:40,340 --> 00:07:45,260
out of excuse me out of files what I've

00:07:43,820 --> 00:07:47,660
seen in a number of places where worked

00:07:45,260 --> 00:07:49,130
is the kind we have kind of content

00:07:47,660 --> 00:07:51,440
metadata extraction or data from a

00:07:49,130 --> 00:07:53,780
structured data store search system and

00:07:51,440 --> 00:07:55,160
then we have that user interface and the

00:07:53,780 --> 00:07:56,810
amount of attention that's applied to

00:07:55,160 --> 00:07:58,520
each layer at least in a couple of

00:07:56,810 --> 00:08:01,100
places where I've seen these kinds of

00:07:58,520 --> 00:08:03,410
systems is all at the user interface as

00:08:01,100 --> 00:08:05,840
long as somebody can enter in search

00:08:03,410 --> 00:08:07,910
term and some documents come back the

00:08:05,840 --> 00:08:10,910
users are happy the managers are happy

00:08:07,910 --> 00:08:12,979
the GUI developers are happy it works

00:08:10,910 --> 00:08:15,530
everything just works this is marvelous

00:08:12,979 --> 00:08:17,900
but where I don't see as much effort put

00:08:15,530 --> 00:08:20,270
in is all of the is any regression

00:08:17,900 --> 00:08:22,130
testing or any testing on the the lower

00:08:20,270 --> 00:08:24,530
parts of the stack which frankly aren't

00:08:22,130 --> 00:08:26,720
that interesting and we just it's often

00:08:24,530 --> 00:08:27,890
times that people hope that that the

00:08:26,720 --> 00:08:29,600
components that those levels are just

00:08:27,890 --> 00:08:32,479
working and they trust that those are

00:08:29,600 --> 00:08:33,469
working off and without checking so as

00:08:32,479 --> 00:08:37,539
you know when things go wrong with

00:08:33,469 --> 00:08:39,830
foundation you know stuff happens so

00:08:37,539 --> 00:08:41,570
what can go wrong and here's my taxonomy

00:08:39,830 --> 00:08:44,690
of things that can go wrong not to scare

00:08:41,570 --> 00:08:46,010
you away but one we have completely

00:08:44,690 --> 00:08:47,390
expected exceptions

00:08:46,010 --> 00:08:49,459
expected exceptions yes they are

00:08:47,390 --> 00:08:51,020
expected exceptions so if you have

00:08:49,459 --> 00:08:52,670
truncated files our parsers are not

00:08:51,020 --> 00:08:54,080
designed to handle truncated files some

00:08:52,670 --> 00:08:55,040
handle some types of truncated files

00:08:54,080 --> 00:08:56,839
better than other

00:08:55,040 --> 00:08:58,100
if it's password-protected file and you

00:08:56,839 --> 00:09:00,079
don't have a password we're not we're

00:08:58,100 --> 00:09:01,910
not handling it if the format version

00:09:00,079 --> 00:09:03,889
isn't handled or if the file type isn't

00:09:01,910 --> 00:09:06,079
handled we aren't going to be able to do

00:09:03,889 --> 00:09:07,939
anything with that and sometimes they're

00:09:06,079 --> 00:09:09,319
just plain corrupt files for whatever

00:09:07,939 --> 00:09:11,269
reason and we're not going to be able to

00:09:09,319 --> 00:09:13,459
do those and of course there's a

00:09:11,269 --> 00:09:15,230
spectrum of corruption so it could be

00:09:13,459 --> 00:09:16,910
the case that the main application

00:09:15,230 --> 00:09:19,100
that's associated with it Adobe Reader

00:09:16,910 --> 00:09:21,499
for example can get text out of a file

00:09:19,100 --> 00:09:23,329
but we can't the flip side also happens

00:09:21,499 --> 00:09:25,489
sometimes the PDF box is able to get

00:09:23,329 --> 00:09:28,850
content out of a file where a period o B

00:09:25,489 --> 00:09:32,089
Reader is not able to and sometimes you

00:09:28,850 --> 00:09:34,399
try to open it up and you just get

00:09:32,089 --> 00:09:36,619
nothing so there's a continuum a

00:09:34,399 --> 00:09:38,839
spectrum of what I mean by corrupt file

00:09:36,619 --> 00:09:40,939
and then you have a somewhat expected

00:09:38,839 --> 00:09:43,399
exception so a parser has a problem with

00:09:40,939 --> 00:09:46,269
a non corrupt file the code base for

00:09:43,399 --> 00:09:49,579
tico we're now at 50 Meg's ish of a jar

00:09:46,269 --> 00:09:51,949
there's a lot of code in there in a lot

00:09:49,579 --> 00:09:54,739
of moving parts we don't have eyes on

00:09:51,949 --> 00:09:56,439
all of the code all the time each each

00:09:54,739 --> 00:10:00,319
dependency is constantly being upgraded

00:09:56,439 --> 00:10:02,119
and improved so we don't have a chance

00:10:00,319 --> 00:10:04,999
to look into all of that code and follow

00:10:02,119 --> 00:10:07,100
all of that to rigorous you know code

00:10:04,999 --> 00:10:09,199
reviews of what's coming into teeka so

00:10:07,100 --> 00:10:11,119
it happens that parsers sometimes have

00:10:09,199 --> 00:10:12,679
some problems and hopefully they will

00:10:11,119 --> 00:10:13,790
throw an exception and you'll be done

00:10:12,679 --> 00:10:15,110
with it and everything's fine and you

00:10:13,790 --> 00:10:17,179
can log that exception and hopefully

00:10:15,110 --> 00:10:18,199
open a ticket on the JIRA get it fixed

00:10:17,179 --> 00:10:20,720
and then it will be fixed in the next

00:10:18,199 --> 00:10:22,249
version of teeka ideally and sometimes

00:10:20,720 --> 00:10:23,629
they're just corrupt files that nothing

00:10:22,249 --> 00:10:27,410
can handle and there's not a lot that we

00:10:23,629 --> 00:10:29,990
can do about that all right so note that

00:10:27,410 --> 00:10:31,939
when you do get exceptions especially of

00:10:29,990 --> 00:10:33,949
the somewhat expected exception category

00:10:31,939 --> 00:10:35,569
you may get some text you may get some

00:10:33,949 --> 00:10:38,449
meta data it all depends on the parser

00:10:35,569 --> 00:10:40,339
so if you do get an exception don't

00:10:38,449 --> 00:10:42,079
necessarily throw out whatever you got

00:10:40,339 --> 00:10:43,309
from the handler you may still that may

00:10:42,079 --> 00:10:47,470
still be of use for some use cases

00:10:43,309 --> 00:10:50,839
search for example or other other things

00:10:47,470 --> 00:10:53,419
so those are the basic problems we also

00:10:50,839 --> 00:10:56,629
have catastrophic problems now these

00:10:53,419 --> 00:10:57,949
happen really rarely fortunately and the

00:10:56,629 --> 00:11:01,160
good news is that when they happen you

00:10:57,949 --> 00:11:02,959
you often know about it not always but

00:11:01,160 --> 00:11:05,839
sometimes you do so out of memory errors

00:11:02,959 --> 00:11:07,850
and out of memory errors can be not so

00:11:05,839 --> 00:11:08,390
bad sometimes you have occur up your JVM

00:11:07,850 --> 00:11:10,700
and it could be a

00:11:08,390 --> 00:11:14,260
problem but what really gets interesting

00:11:10,700 --> 00:11:16,820
is when you're approaching in oom and

00:11:14,260 --> 00:11:18,440
Chavez the garbage collector is just

00:11:16,820 --> 00:11:21,200
trying so if you're even running a

00:11:18,440 --> 00:11:23,060
single-threaded parser I guess one file

00:11:21,200 --> 00:11:24,860
and you somehow trigger something they

00:11:23,060 --> 00:11:26,960
kicked off the garbage collector you can

00:11:24,860 --> 00:11:29,870
bring down an entire box and it's just

00:11:26,960 --> 00:11:31,250
snail's pace and its really it's

00:11:29,870 --> 00:11:32,510
dramatic to watch it's it's an

00:11:31,250 --> 00:11:34,910
entertaining thing to watch unless

00:11:32,510 --> 00:11:36,140
you're I don't know in production or you

00:11:34,910 --> 00:11:38,750
know doing doing something else that

00:11:36,140 --> 00:11:39,860
matters and people care about but from

00:11:38,750 --> 00:11:41,300
my perspective if I get one of these

00:11:39,860 --> 00:11:43,460
files off the JIRA it's it's an

00:11:41,300 --> 00:11:45,500
entertaining thing and exciting don't

00:11:43,460 --> 00:11:47,450
get me wrong the other is sometimes you

00:11:45,500 --> 00:11:49,910
can get an out of memory error from a

00:11:47,450 --> 00:11:52,310
four byte file just because something

00:11:49,910 --> 00:11:54,350
went wrong with the parser yeah there's

00:11:52,310 --> 00:11:58,010
a misunderstanding in the parser and you

00:11:54,350 --> 00:11:59,660
can get an out of memory error the other

00:11:58,010 --> 00:12:01,520
when you get those at least you know

00:11:59,660 --> 00:12:02,480
something's gone wrong slowly building

00:12:01,520 --> 00:12:03,890
memory leaks are a little more

00:12:02,480 --> 00:12:05,680
interesting unless you're doing

00:12:03,890 --> 00:12:08,380
profiling and really paying attention

00:12:05,680 --> 00:12:11,510
you often don't realize what's going on

00:12:08,380 --> 00:12:14,360
so again you can get garbage collection

00:12:11,510 --> 00:12:16,310
issues where you're just running five

00:12:14,360 --> 00:12:18,830
threads how can you possibly be taking

00:12:16,310 --> 00:12:20,750
your your quad-core machine well it may

00:12:18,830 --> 00:12:22,040
be a slow building memory leak and the

00:12:20,750 --> 00:12:24,140
garbage collector is working on all

00:12:22,040 --> 00:12:26,270
threads or multi-threaded to try to

00:12:24,140 --> 00:12:28,940
clean up some of that memory permanent

00:12:26,270 --> 00:12:31,640
hangs are a joy to behold that's also

00:12:28,940 --> 00:12:33,680
that that typically only results in a

00:12:31,640 --> 00:12:35,950
single thread hanging it won't often

00:12:33,680 --> 00:12:38,410
corrupt me or take up the entire machine

00:12:35,950 --> 00:12:40,940
but when those hit those can be quite

00:12:38,410 --> 00:12:43,340
exciting and teeka 11:32 has some nice

00:12:40,940 --> 00:12:45,440
descriptive narrative for you if you

00:12:43,340 --> 00:12:47,210
care to follow about what happens when a

00:12:45,440 --> 00:12:48,680
parser just goes into an infective lee

00:12:47,210 --> 00:12:50,540
an infinite loop

00:12:48,680 --> 00:12:51,920
that can also be a real problem we also

00:12:50,540 --> 00:12:54,170
have had a couple of security

00:12:51,920 --> 00:12:56,420
vulnerabilities which we fixed in one

00:12:54,170 --> 00:13:00,470
leased by the ones that we're aware of

00:12:56,420 --> 00:13:02,750
we fixed by 114 so we had a couple of xx

00:13:00,470 --> 00:13:04,360
ease which we fixed we also had this

00:13:02,750 --> 00:13:06,770
great arbitrary code execution

00:13:04,360 --> 00:13:08,990
vulnerability so that somebody who

00:13:06,770 --> 00:13:11,150
carefully crafted a MATLAB file would be

00:13:08,990 --> 00:13:13,610
able to run whatever software they felt

00:13:11,150 --> 00:13:17,830
like on your on your computer that was

00:13:13,610 --> 00:13:21,110
running teeka which is really exciting

00:13:17,830 --> 00:13:22,310
in the wrong way so again this is blood

00:13:21,110 --> 00:13:24,470
on the highway these are

00:13:22,310 --> 00:13:26,450
extremely rare issues but these can

00:13:24,470 --> 00:13:29,480
happen and if you are in production if

00:13:26,450 --> 00:13:31,279
you are handling as Nick says millions

00:13:29,480 --> 00:13:32,120
of files or billions of files from the

00:13:31,279 --> 00:13:34,930
internet that you don't trust

00:13:32,120 --> 00:13:39,620
necessarily these things will happen and

00:13:34,930 --> 00:13:41,270
if you are running tika in the same JVM

00:13:39,620 --> 00:13:43,730
that you're running you're indexer these

00:13:41,270 --> 00:13:47,300
things can cause real problems so please

00:13:43,730 --> 00:13:49,220
do not do that or just beware that these

00:13:47,300 --> 00:13:51,140
things can happen all right

00:13:49,220 --> 00:13:52,700
so those are the kind of expected

00:13:51,140 --> 00:13:54,380
exceptions the really really rare

00:13:52,700 --> 00:13:56,360
exceptions and then there are the hidden

00:13:54,380 --> 00:13:58,220
problems which you're not really aware

00:13:56,360 --> 00:14:00,020
happened and these can be you can get

00:13:58,220 --> 00:14:02,060
garbled text out of the out of content

00:14:00,020 --> 00:14:03,830
extraction from slightly garbled to

00:14:02,060 --> 00:14:06,890
totally hosed and I have some examples

00:14:03,830 --> 00:14:08,270
of that we also have missing text where

00:14:06,890 --> 00:14:09,500
you have just blocks of text are missing

00:14:08,270 --> 00:14:11,510
from the document you would expect to

00:14:09,500 --> 00:14:13,640
get much more text if you open it up in

00:14:11,510 --> 00:14:14,930
the native application that's supposed

00:14:13,640 --> 00:14:15,890
to handle that file you get a whole

00:14:14,930 --> 00:14:18,560
bunch of text but when you run it

00:14:15,890 --> 00:14:20,480
through TV you don't get much at all you

00:14:18,560 --> 00:14:22,850
can just you can be missing attachments

00:14:20,480 --> 00:14:26,150
thanks to a bug that I added and then

00:14:22,850 --> 00:14:29,900
fix just in time before 1:15 for the

00:14:26,150 --> 00:14:32,660
process for voting for 1:15 and also if

00:14:29,900 --> 00:14:34,430
you are using the classic tikka handlers

00:14:32,660 --> 00:14:37,400
which just pulls text out of documents

00:14:34,430 --> 00:14:39,800
or XHTML the default there was to

00:14:37,400 --> 00:14:41,360
swallow embedded file exceptions so if

00:14:39,800 --> 00:14:43,520
you've had a zip file that contained a

00:14:41,360 --> 00:14:45,890
whole bunch of Excel files and those

00:14:43,520 --> 00:14:48,320
Excel files all happened to be generated

00:14:45,890 --> 00:14:49,670
by the same source and that particular

00:14:48,320 --> 00:14:52,370
version of Excel was a little bit

00:14:49,670 --> 00:14:55,160
different from most you could wind up

00:14:52,370 --> 00:14:57,170
getting exceptions on all those files

00:14:55,160 --> 00:14:59,270
and you would never know it because a

00:14:57,170 --> 00:15:01,070
kikah doesn't let you in the default use

00:14:59,270 --> 00:15:02,270
of tika we do not let you know that

00:15:01,070 --> 00:15:05,300
there was an exception on an embedded

00:15:02,270 --> 00:15:07,940
file one batch of documents I recently

00:15:05,300 --> 00:15:09,470
reviewed show that they had 50% of 50%

00:15:07,940 --> 00:15:11,570
of their Excel files were getting

00:15:09,470 --> 00:15:13,100
exceptions and the folks who were

00:15:11,570 --> 00:15:16,100
running tika had no idea because they

00:15:13,100 --> 00:15:17,810
were running the classic the classic

00:15:16,100 --> 00:15:20,540
method which has no warnings about these

00:15:17,810 --> 00:15:22,040
things so those are things where you're

00:15:20,540 --> 00:15:23,900
not getting an exception but something

00:15:22,040 --> 00:15:26,330
could still kind of go not not as well

00:15:23,900 --> 00:15:30,320
as one might like so examples examples

00:15:26,330 --> 00:15:32,420
here we have the highway so this is what

00:15:30,320 --> 00:15:34,700
happened on one file when we upgraded

00:15:32,420 --> 00:15:36,020
from PDF box one point eight two one one

00:15:34,700 --> 00:15:38,450
point eight one point eight six

00:15:36,020 --> 00:15:40,370
27 we were getting the text on the top

00:15:38,450 --> 00:15:42,339
and then when we upgraded we got the

00:15:40,370 --> 00:15:44,750
text on the bottom it's a complete

00:15:42,339 --> 00:15:47,180
single substitution cipher

00:15:44,750 --> 00:15:50,570
so that capital T goes to capital B

00:15:47,180 --> 00:15:51,740
lowercase a goes capital G fine this

00:15:50,570 --> 00:15:54,260
would pass Tzipi and distribution

00:15:51,740 --> 00:15:55,790
beautifully it would pass word lengths

00:15:54,260 --> 00:15:58,010
statistics beautifully because all the

00:15:55,790 --> 00:15:59,330
words are the same length but now you're

00:15:58,010 --> 00:16:02,810
throwing all of these brilliant words

00:15:59,330 --> 00:16:04,540
into your solar index or your Lucene

00:16:02,810 --> 00:16:06,709
index and now you're blowing out your

00:16:04,540 --> 00:16:07,970
course solar strong leucine strong

00:16:06,709 --> 00:16:09,800
you're not actually going to blow out

00:16:07,970 --> 00:16:10,850
the index with this noise but you are

00:16:09,800 --> 00:16:12,589
adding a whole bunch of noise to your

00:16:10,850 --> 00:16:15,500
index and your users can't find the

00:16:12,589 --> 00:16:16,700
document so it's it's it's two great

00:16:15,500 --> 00:16:17,899
things that happen when this kind of

00:16:16,700 --> 00:16:20,180
things happen when this kind of thing

00:16:17,899 --> 00:16:22,790
happens is you or your but making your

00:16:20,180 --> 00:16:26,750
index suboptimal and also people can't

00:16:22,790 --> 00:16:28,370
find the documents they need this is an

00:16:26,750 --> 00:16:30,560
example of missing text and this is was

00:16:28,370 --> 00:16:33,680
one of the issues I think that initially

00:16:30,560 --> 00:16:37,399
got me out of my notepad developing

00:16:33,680 --> 00:16:41,000
laptop and into open source and Nick put

00:16:37,399 --> 00:16:44,420
up with so much on this little patch I

00:16:41,000 --> 00:16:48,079
had for this particular issue this is

00:16:44,420 --> 00:16:51,230
this poor person's CV with lots of great

00:16:48,079 --> 00:16:55,760
detail about what she does and that's

00:16:51,230 --> 00:16:57,649
all the tika pulled out so this is

00:16:55,760 --> 00:16:58,390
mildly amusing because it doesn't look

00:16:57,649 --> 00:16:59,899
to us like it has real-world

00:16:58,390 --> 00:17:01,970
consequences but if you think about

00:16:59,899 --> 00:17:04,130
recruiters or anybody in that company

00:17:01,970 --> 00:17:05,209
looking for particular keywords they're

00:17:04,130 --> 00:17:07,459
not going to be able to find that for

00:17:05,209 --> 00:17:09,679
this person so these can have real life

00:17:07,459 --> 00:17:11,689
consequences when text extraction does

00:17:09,679 --> 00:17:13,400
not work and again you can run all sorts

00:17:11,689 --> 00:17:15,170
of metrics on this and it would pass a

00:17:13,400 --> 00:17:17,750
number of them because it's basically

00:17:15,170 --> 00:17:19,429
you're getting English ish stuff out of

00:17:17,750 --> 00:17:20,689
there you're getting some text out of

00:17:19,429 --> 00:17:22,309
there but you're not getting the full

00:17:20,689 --> 00:17:25,400
document so this is an example of

00:17:22,309 --> 00:17:27,439
missing text some other things that can

00:17:25,400 --> 00:17:28,970
happen is you may get some noise if a

00:17:27,439 --> 00:17:31,730
PDF has been scanned and it has embedded

00:17:28,970 --> 00:17:33,260
OCR in there or texted some generated by

00:17:31,730 --> 00:17:35,150
OCR you could get some noise in there

00:17:33,260 --> 00:17:37,340
and that's you know we all know and

00:17:35,150 --> 00:17:41,080
expect that kind of thing sometimes in

00:17:37,340 --> 00:17:43,130
PDFs but this was a great example where

00:17:41,080 --> 00:17:44,570
yeah the image you have the text that

00:17:43,130 --> 00:17:45,950
was extracted and you can prove to

00:17:44,570 --> 00:17:48,080
people that you can find this exact

00:17:45,950 --> 00:17:49,790
document if you search for III on a--

00:17:48,080 --> 00:17:51,620
turing on google because

00:17:49,790 --> 00:17:52,910
Google at this point was relying on the

00:17:51,620 --> 00:17:56,330
text that was stored in the document in

00:17:52,910 --> 00:17:58,760
this case it happened to be OCR all

00:17:56,330 --> 00:18:00,980
right so basically my main point on this

00:17:58,760 --> 00:18:02,540
is that if you take whatever you get out

00:18:00,980 --> 00:18:04,280
of teak and throw it into solar and then

00:18:02,540 --> 00:18:05,330
throw a great user interface on top of

00:18:04,280 --> 00:18:07,640
that and hope for the best

00:18:05,330 --> 00:18:09,530
you don't know what you can't find so

00:18:07,640 --> 00:18:11,090
please take a little bit of time if

00:18:09,530 --> 00:18:13,030
possible to do some kind of evaluation

00:18:11,090 --> 00:18:15,020
on the content that you're getting out

00:18:13,030 --> 00:18:18,170
so that's that's the blood on the

00:18:15,020 --> 00:18:19,310
highway chunk of the talk for some

00:18:18,170 --> 00:18:23,000
examples of things that can go wrong

00:18:19,310 --> 00:18:26,150
again Kiko works quite well a lot of the

00:18:23,000 --> 00:18:28,250
time so so please don't go and build

00:18:26,150 --> 00:18:32,000
another Apache project that pulled

00:18:28,250 --> 00:18:35,600
content out of documents all right

00:18:32,000 --> 00:18:40,070
so the dream from all of this is tika

00:18:35,600 --> 00:18:41,960
1302 so my dream was I was motivated by

00:18:40,070 --> 00:18:43,850
all of this stuff of the above we have

00:18:41,960 --> 00:18:46,610
only about a thousand test files between

00:18:43,850 --> 00:18:48,290
po a PDF box and tika it sounds like a

00:18:46,610 --> 00:18:50,750
fair number but it really isn't given

00:18:48,290 --> 00:18:54,470
the amount of marvelous things that can

00:18:50,750 --> 00:18:56,390
happen in in PDF files and also in

00:18:54,470 --> 00:18:58,370
Microsoft Office files and I'm just

00:18:56,390 --> 00:19:01,100
talking here about PDF and Microsoft

00:18:58,370 --> 00:19:02,600
Office of course we handle a number of

00:19:01,100 --> 00:19:04,970
other file formats so a thousand test

00:19:02,600 --> 00:19:07,160
file sounds like a lot it's not the

00:19:04,970 --> 00:19:11,090
other motivation behind tika 1302

00:19:07,160 --> 00:19:12,620
is that some groups made the mistake of

00:19:11,090 --> 00:19:15,980
giving the right access to their

00:19:12,620 --> 00:19:17,330
projects so unit tests are nice but

00:19:15,980 --> 00:19:18,740
against me come on you're not going to

00:19:17,330 --> 00:19:20,960
cover all of the all of the stuff I'm

00:19:18,740 --> 00:19:22,490
going to ruin in your code bases so this

00:19:20,960 --> 00:19:25,820
was another motivation for why I had the

00:19:22,490 --> 00:19:28,550
trainee code 1302 so my dream was to run

00:19:25,820 --> 00:19:30,310
tika against a much larger corpus either

00:19:28,550 --> 00:19:32,510
nightly or weekly or something and then

00:19:30,310 --> 00:19:33,950
automatically recognize regressions and

00:19:32,510 --> 00:19:36,500
this seems like a great thing we have

00:19:33,950 --> 00:19:37,910
regression testing we have you know

00:19:36,500 --> 00:19:41,540
continuous integration we have all these

00:19:37,910 --> 00:19:42,980
systems set up that's great there is no

00:19:41,540 --> 00:19:45,590
magic and I'll talk about that shortly

00:19:42,980 --> 00:19:47,180
so that was the dream as part of this

00:19:45,590 --> 00:19:49,070
dream one of the components was some

00:19:47,180 --> 00:19:50,930
evaluation metric that could say how

00:19:49,070 --> 00:19:53,540
well did we get how well the techs come

00:19:50,930 --> 00:19:54,890
out of here or how well are we getting

00:19:53,540 --> 00:19:56,330
text versus how well where are we

00:19:54,890 --> 00:19:59,300
getting text with an earlier version of

00:19:56,330 --> 00:20:01,880
tika or a different tool so tiki evals

00:19:59,300 --> 00:20:03,530
focus on that now for now that's the

00:20:01,880 --> 00:20:05,600
component within the larger dream

00:20:03,530 --> 00:20:07,580
of okay so you've extracted some text

00:20:05,600 --> 00:20:10,730
how language he is it you have it some

00:20:07,580 --> 00:20:12,140
kind of sense of how well you did or can

00:20:10,730 --> 00:20:14,150
you compare two different groups so this

00:20:12,140 --> 00:20:17,030
as I mentioned is available in or will

00:20:14,150 --> 00:20:19,700
be available in TK 115 which should be

00:20:17,030 --> 00:20:21,470
coming out shortly okay so a high-level

00:20:19,700 --> 00:20:27,680
overview of what's in the tika eval

00:20:21,470 --> 00:20:28,310
module it does not run in SPARC I'm

00:20:27,680 --> 00:20:32,900
sorry

00:20:28,310 --> 00:20:35,630
um so it's I I don't have any any of the

00:20:32,900 --> 00:20:38,810
cool hip things that all of the kids are

00:20:35,630 --> 00:20:40,490
using nowadays but it runs from file

00:20:38,810 --> 00:20:43,340
share to file share

00:20:40,490 --> 00:20:46,160
there is no bat scripting involved yet

00:20:43,340 --> 00:20:47,690
there's no Perl here but it does run

00:20:46,160 --> 00:20:50,150
file share to file share and the notion

00:20:47,690 --> 00:20:52,130
is if you do have a modern document

00:20:50,150 --> 00:20:52,580
processing pipelines the spark aduke and

00:20:52,130 --> 00:20:54,530
so on

00:20:52,580 --> 00:20:55,940
then at least with tika eval you can do

00:20:54,530 --> 00:20:57,890
a random sample of what you have and run

00:20:55,940 --> 00:21:00,020
a file share to file share or as I

00:20:57,890 --> 00:21:02,060
mentioned our JIRA is open the commuters

00:21:00,020 --> 00:21:04,340
are standing by so if you do want to add

00:21:02,060 --> 00:21:05,840
an integration point for tika or even

00:21:04,340 --> 00:21:07,460
just share lessons learned let us know

00:21:05,840 --> 00:21:09,500
as Nick pointed out in his talk one of

00:21:07,460 --> 00:21:11,360
the great things about the do is it will

00:21:09,500 --> 00:21:13,790
try files again and again and unless you

00:21:11,360 --> 00:21:15,050
tell it to stop trying files it you

00:21:13,790 --> 00:21:18,350
could run into some problems so you have

00:21:15,050 --> 00:21:21,560
to be careful in the large scale

00:21:18,350 --> 00:21:23,480
processing frameworks so the scope of

00:21:21,560 --> 00:21:26,120
tika eval is really quite humble its

00:21:23,480 --> 00:21:29,270
file share to file share and there are

00:21:26,120 --> 00:21:31,370
basically two modes one is profile a

00:21:29,270 --> 00:21:33,950
single extraction run so if you run tika

00:21:31,370 --> 00:21:36,860
against a batch of documents and you

00:21:33,950 --> 00:21:38,660
have a parallel directory structure with

00:21:36,860 --> 00:21:40,820
the original binary documents and the

00:21:38,660 --> 00:21:41,990
text that was extracted you can run a

00:21:40,820 --> 00:21:43,580
profile on that and say how many

00:21:41,990 --> 00:21:47,960
exceptions that I have perm I'm type

00:21:43,580 --> 00:21:50,120
what languages were detected what what's

00:21:47,960 --> 00:21:51,260
the number of words per page ratio which

00:21:50,120 --> 00:21:54,830
can give you insight into whether your

00:21:51,260 --> 00:21:56,750
PDFs are image only perhaps or you can

00:21:54,830 --> 00:21:57,530
give you other insight so there are a

00:21:56,750 --> 00:21:59,810
number of things and I'll talk about

00:21:57,530 --> 00:22:03,080
that the other big mode is to extraction

00:21:59,810 --> 00:22:05,000
runs and the notion here is hey if you

00:22:03,080 --> 00:22:07,580
have ground truth great if you're

00:22:05,000 --> 00:22:08,840
running an OCR study and you have ground

00:22:07,580 --> 00:22:12,290
truth for what text should have been

00:22:08,840 --> 00:22:14,180
ocr'd you can use tika eval the compare

00:22:12,290 --> 00:22:15,830
mode to run ground truth against what

00:22:14,180 --> 00:22:16,450
you're currently getting out or you can

00:22:15,830 --> 00:22:17,830
experiment with

00:22:16,450 --> 00:22:19,120
different settings in your OCR to see

00:22:17,830 --> 00:22:20,880
which one overall gives you better

00:22:19,120 --> 00:22:23,980
performance on your batch of documents

00:22:20,880 --> 00:22:25,780
you could also be comparing different

00:22:23,980 --> 00:22:27,790
tools and I'm not saying that there are

00:22:25,780 --> 00:22:30,490
other tools besides the one Apache tikka

00:22:27,790 --> 00:22:31,540
to pull content out in a Python world

00:22:30,490 --> 00:22:36,190
for example there wouldn't be anything

00:22:31,540 --> 00:22:38,200
similar to that but you could if you

00:22:36,190 --> 00:22:40,240
wanted to I guess compare other tools to

00:22:38,200 --> 00:22:43,150
some of the things in Apache tikka or

00:22:40,240 --> 00:22:45,640
you could compare a tool a settings axe

00:22:43,150 --> 00:22:47,500
to lay with settings Y or even just two

00:22:45,640 --> 00:22:49,420
different versions of Apache tikka so

00:22:47,500 --> 00:22:51,280
when we say that we have a new great new

00:22:49,420 --> 00:22:53,140
version out you can run teak eval on

00:22:51,280 --> 00:22:54,370
your documents to see if there are major

00:22:53,140 --> 00:22:56,590
regressions that would prevent you from

00:22:54,370 --> 00:22:57,970
upgrading and when that happens please

00:22:56,590 --> 00:23:00,520
do open tickets and I'll talk a little

00:22:57,970 --> 00:23:02,320
bit about that so that's a high-level

00:23:00,520 --> 00:23:04,780
overview of the scope and basically the

00:23:02,320 --> 00:23:06,460
two modes before I go much further I do

00:23:04,780 --> 00:23:09,310
have a couple of definitions that I've

00:23:06,460 --> 00:23:11,680
come up with on it or excuse me that the

00:23:09,310 --> 00:23:14,200
Tiki eval community has come up with and

00:23:11,680 --> 00:23:18,460
the community would be a bus quotient of

00:23:14,200 --> 00:23:19,510
one largely so the original documents

00:23:18,460 --> 00:23:21,160
are container documents those are just

00:23:19,510 --> 00:23:23,260
the binary files that you aim tikka ad

00:23:21,160 --> 00:23:25,000
whether it's an actual text file or

00:23:23,260 --> 00:23:26,890
whether it's a word file that might have

00:23:25,000 --> 00:23:28,200
attachments I just call them original

00:23:26,890 --> 00:23:30,190
documents or container documents

00:23:28,200 --> 00:23:32,200
embedded documents are anything that

00:23:30,190 --> 00:23:34,300
show up inside that document which are

00:23:32,200 --> 00:23:36,670
basically viewed by some people or many

00:23:34,300 --> 00:23:38,350
people as an embedded document that

00:23:36,670 --> 00:23:40,240
could be for example an attachment on an

00:23:38,350 --> 00:23:42,640
email it could be an actually embedded

00:23:40,240 --> 00:23:45,130
file you can embed a doc file inside a

00:23:42,640 --> 00:23:48,490
PDF and then put a zip inside that docx

00:23:45,130 --> 00:23:51,010
file and do all sorts of choice code all

00:23:48,490 --> 00:23:52,570
things or it could be a file format that

00:23:51,010 --> 00:23:54,250
doesn't really ever exist outside of

00:23:52,570 --> 00:23:57,430
being an embedded document like EMF or

00:23:54,250 --> 00:23:59,620
wmf for XMP or SFA so some file formats

00:23:57,430 --> 00:24:01,210
really don't exist on their own so by

00:23:59,620 --> 00:24:03,460
embedded document I mean anything that

00:24:01,210 --> 00:24:05,440
shows up inside a document and is

00:24:03,460 --> 00:24:08,410
basically recognized as an embedded

00:24:05,440 --> 00:24:10,150
document another term is extract so an

00:24:08,410 --> 00:24:12,880
extract covers anything that you pulled

00:24:10,150 --> 00:24:16,300
out of a document there are two basic

00:24:12,880 --> 00:24:17,890
modes in teeka eval one is text so this

00:24:16,300 --> 00:24:20,320
is useful for if you're running another

00:24:17,890 --> 00:24:22,270
content extraction tool and it just

00:24:20,320 --> 00:24:23,710
pulls out text you can you have the text

00:24:22,270 --> 00:24:26,590
extract what text was extracted from

00:24:23,710 --> 00:24:28,420
that we also have this recursive parser

00:24:26,590 --> 00:24:28,970
JSON stuff and I'll talk about that in

00:24:28,420 --> 00:24:33,289
the next

00:24:28,970 --> 00:24:36,350
two slides eval was basically

00:24:33,289 --> 00:24:38,450
designed for the json format but based

00:24:36,350 --> 00:24:42,260
on apache con from - two years ago I

00:24:38,450 --> 00:24:44,299
added the dot text handling option yeah

00:24:42,260 --> 00:24:46,340
so and I'll talk there the details on

00:24:44,299 --> 00:24:52,070
handling dot text files are as extracts

00:24:46,340 --> 00:24:53,720
are all on our wiki all right so why the

00:24:52,070 --> 00:24:58,070
recursive parser wrapper so let's say

00:24:53,720 --> 00:25:02,179
that some person has taken a as it an

00:24:58,070 --> 00:25:03,950
embedded embed for text file thrown that

00:25:02,179 --> 00:25:05,390
in a zip file put that in another zip

00:25:03,950 --> 00:25:07,280
file put that in another zip file

00:25:05,390 --> 00:25:09,220
another zip file and then put that in a

00:25:07,280 --> 00:25:12,169
word file so now you have this massively

00:25:09,220 --> 00:25:16,030
hierarchical bunch of documents all in a

00:25:12,169 --> 00:25:20,020
nice Microsoft Word file in the classic

00:25:16,030 --> 00:25:23,510
extraction that is handled as one

00:25:20,020 --> 00:25:25,190
collection of sax events and the way

00:25:23,510 --> 00:25:27,140
that embedded documents are handled as

00:25:25,190 --> 00:25:29,600
you have a div class for that embedded

00:25:27,140 --> 00:25:31,039
file embed one embed one hey and you get

00:25:29,600 --> 00:25:32,809
the embedded path for where that

00:25:31,039 --> 00:25:34,220
embedded file is and then you get the

00:25:32,809 --> 00:25:36,799
text that's extracted from that and

00:25:34,220 --> 00:25:40,039
given the framework of streaming that's

00:25:36,799 --> 00:25:42,260
kind of what could happen that's what

00:25:40,039 --> 00:25:44,659
you can do with with embedded documents

00:25:42,260 --> 00:25:46,100
but one problem with this is that the

00:25:44,659 --> 00:25:47,780
embedded documented data from those

00:25:46,100 --> 00:25:49,400
embedded documents is lost so if you

00:25:47,780 --> 00:25:50,870
have a zip file of image files and those

00:25:49,400 --> 00:25:53,210
image files actually have latitudes and

00:25:50,870 --> 00:25:55,549
longitudes with the old version or the

00:25:53,210 --> 00:25:56,780
classic version of using at HTML all of

00:25:55,549 --> 00:25:59,419
those lat/longs could no longer be

00:25:56,780 --> 00:26:01,070
processed and you'd lose them the other

00:25:59,419 --> 00:26:02,510
thing as I mentioned we swallow embedded

00:26:01,070 --> 00:26:07,210
exceptions without warning you at this

00:26:02,510 --> 00:26:07,210
point so that's another area of concern

00:26:07,390 --> 00:26:13,250
yeah oh the other thing is that

00:26:09,679 --> 00:26:15,020
sometimes the classic XHTML we try the

00:26:13,250 --> 00:26:18,200
best we can to get the metadata out of a

00:26:15,020 --> 00:26:20,000
document before writing the XHTML but

00:26:18,200 --> 00:26:21,289
sometimes we can't get metadata until we

00:26:20,000 --> 00:26:23,059
get further down the parse of the

00:26:21,289 --> 00:26:24,470
original document so it could be the

00:26:23,059 --> 00:26:25,880
case that you get metadata extracted

00:26:24,470 --> 00:26:29,570
from a file but it doesn't show up in

00:26:25,880 --> 00:26:33,500
the XHTML so to solve these problems

00:26:29,570 --> 00:26:35,690
I stole code from Nick and yoga and put

00:26:33,500 --> 00:26:37,730
that into tika and caught we're calling

00:26:35,690 --> 00:26:39,740
another recursive parser wrapper and you

00:26:37,730 --> 00:26:41,820
can get to this through tika app to

00:26:39,740 --> 00:26:44,280
capital j option or our mad

00:26:41,820 --> 00:26:46,320
at the endpoint Antigua eval and this

00:26:44,280 --> 00:26:48,210
gives you a list of metadata objects for

00:26:46,320 --> 00:26:51,030
each embedded document and then each

00:26:48,210 --> 00:26:52,680
metadata object has the content type or

00:26:51,030 --> 00:26:55,500
it has all of the metadata with a

00:26:52,680 --> 00:26:56,790
special metadata key of xt via content

00:26:55,500 --> 00:26:58,800
and that's the actual text that was

00:26:56,790 --> 00:27:00,600
extracted from the document so this will

00:26:58,800 --> 00:27:02,670
maintain stack traces for embedded

00:27:00,600 --> 00:27:03,810
documents and it will maintain content

00:27:02,670 --> 00:27:05,450
for all of the embedded documents like

00:27:03,810 --> 00:27:08,130
we had in traditional but it will also

00:27:05,450 --> 00:27:09,690
mint maintain all of your embedded

00:27:08,130 --> 00:27:11,730
metadata so you can get all of your

00:27:09,690 --> 00:27:15,000
latitudes and longitudes if those images

00:27:11,730 --> 00:27:16,590
are put in a zip file so in TECO eval

00:27:15,000 --> 00:27:18,660
everything is built around this but we

00:27:16,590 --> 00:27:22,650
do have a way to handle a regular and

00:27:18,660 --> 00:27:26,160
flat text files for extracts so the

00:27:22,650 --> 00:27:30,300
workflow for profile you generate your

00:27:26,160 --> 00:27:34,440
extracts in my case it's use tika batch

00:27:30,300 --> 00:27:37,560
tika tika in batch mode so Java jar tika

00:27:34,440 --> 00:27:38,820
app - I for input directory o for output

00:27:37,560 --> 00:27:41,070
directory and then we have a parallel

00:27:38,820 --> 00:27:44,220
directory of input files and extract

00:27:41,070 --> 00:27:46,680
files then we run the profiler to

00:27:44,220 --> 00:27:48,000
populate in in process h2 database and

00:27:46,680 --> 00:27:51,240
there's the Java command for that you

00:27:48,000 --> 00:27:53,940
specify the extracts directory and what

00:27:51,240 --> 00:27:56,130
you want to name your your h2 dB and

00:27:53,940 --> 00:27:57,480
after you do that that calculates a

00:27:56,130 --> 00:27:58,560
number of statistics puts them in the

00:27:57,480 --> 00:28:00,660
database and then you can dump the

00:27:58,560 --> 00:28:02,130
reports and the reports are driven by a

00:28:00,660 --> 00:28:04,410
whole bunch of sequel that's stored in

00:28:02,130 --> 00:28:05,940
an XML file that you can modify so you

00:28:04,410 --> 00:28:07,920
can choose which reports you want to run

00:28:05,940 --> 00:28:09,690
on that batch of documents tika comes

00:28:07,920 --> 00:28:14,010
with reports that I found to be useful

00:28:09,690 --> 00:28:15,810
or sequel that I've run against the h2

00:28:14,010 --> 00:28:18,930
that have found to be useful and then

00:28:15,810 --> 00:28:21,920
those reports are xlsx files and then

00:28:18,930 --> 00:28:24,360
you can go rummage through those a

00:28:21,920 --> 00:28:27,690
directory full of a bunch of xlsx file

00:28:24,360 --> 00:28:29,400
is not a GUI it's a horrible interface

00:28:27,690 --> 00:28:33,090
it's abysmal but it's better than what

00:28:29,400 --> 00:28:36,000
we had so I'm sorry but if anybody knows

00:28:33,090 --> 00:28:38,310
JavaScript at all and wants to pitch in

00:28:36,000 --> 00:28:40,080
on TK 13 34 it would be really nice to

00:28:38,310 --> 00:28:42,030
have a user yeah it would be nice that

00:28:40,080 --> 00:28:43,290
somebody ought to be doing it it would

00:28:42,030 --> 00:28:45,930
be nice to have a user interface for

00:28:43,290 --> 00:28:47,340
that because navigating through the

00:28:45,930 --> 00:28:49,200
directories navigating through the

00:28:47,340 --> 00:28:52,680
reports can be a bit of a challenge at

00:28:49,200 --> 00:28:55,539
this point let nobody mistake I do not

00:28:52,680 --> 00:28:58,509
believe that a bunch of Excel files

00:28:55,539 --> 00:29:05,379
is sufficient as as as reporting on this

00:28:58,509 --> 00:29:06,399
but it's what we have and next slide all

00:29:05,379 --> 00:29:07,570
right the other one

00:29:06,399 --> 00:29:09,190
so that's profile that's when you have a

00:29:07,570 --> 00:29:10,539
single run the other one is compared and

00:29:09,190 --> 00:29:12,399
for that you run

00:29:10,539 --> 00:29:13,840
Kika on two different you're on two

00:29:12,399 --> 00:29:15,609
different versions of tika you have an

00:29:13,840 --> 00:29:17,229
extract Sega directory and an extra xB

00:29:15,609 --> 00:29:18,789
directory and then you just run the

00:29:17,229 --> 00:29:20,859
compare to land that will compare

00:29:18,789 --> 00:29:23,950
extracts a with extra xB pump all of

00:29:20,859 --> 00:29:25,840
that comparison information into the HQ

00:29:23,950 --> 00:29:27,580
database and then you dump the reports

00:29:25,840 --> 00:29:28,899
from that and again you have reports you

00:29:27,580 --> 00:29:30,220
have all of the individual reports you

00:29:28,899 --> 00:29:32,710
would get from the profiling mode for

00:29:30,220 --> 00:29:34,840
each of those a and B and then you also

00:29:32,710 --> 00:29:37,690
have some comparison statistics that

00:29:34,840 --> 00:29:39,639
compare a with B let me talk about some

00:29:37,690 --> 00:29:43,119
of the features that we can extract or

00:29:39,639 --> 00:29:44,349
that we can get out of a and B all right

00:29:43,119 --> 00:29:45,849
I'm sorry there's one other tool and

00:29:44,349 --> 00:29:47,200
that's start D B so this just starts the

00:29:45,849 --> 00:29:48,940
h2 database so you can navigate to

00:29:47,200 --> 00:29:50,200
localhost and actually interact with the

00:29:48,940 --> 00:29:51,759
database if you want and that can be

00:29:50,200 --> 00:29:53,200
really useful especially as you're

00:29:51,759 --> 00:29:56,849
developing the sequel for the reports

00:29:53,200 --> 00:29:59,950
that you want to develop all right

00:29:56,849 --> 00:30:01,539
so for pro for the reports were in the

00:29:59,950 --> 00:30:04,299
profile mode we get count of metadata

00:30:01,539 --> 00:30:06,460
values we get counts of attachments we

00:30:04,299 --> 00:30:07,720
get mine counts for containers and an

00:30:06,460 --> 00:30:09,669
embedded document so you get a sense of

00:30:07,720 --> 00:30:11,320
what types of file type you have what

00:30:09,669 --> 00:30:13,479
kind of embedded file types you have in

00:30:11,320 --> 00:30:16,119
your corpus we get fairly lengthy

00:30:13,479 --> 00:30:17,649
breakdown of exceptions so counts by

00:30:16,119 --> 00:30:19,409
type of exception whether it was a

00:30:17,649 --> 00:30:22,720
password exception or whether it was a

00:30:19,409 --> 00:30:24,940
runtime exception exceptions by mime

00:30:22,720 --> 00:30:26,320
type so you can see you know PDFs we're

00:30:24,940 --> 00:30:30,489
getting very small number of exceptions

00:30:26,320 --> 00:30:32,169
or jar files we're getting a high rate

00:30:30,489 --> 00:30:34,479
of exceptions we also counts by

00:30:32,169 --> 00:30:36,279
normalized stack traces sometimes the

00:30:34,479 --> 00:30:37,570
message in a stack trace we remove the

00:30:36,279 --> 00:30:39,249
messages of the stack trace so those can

00:30:37,570 --> 00:30:41,289
be collapsed so you can look for common

00:30:39,249 --> 00:30:42,879
patterns in your stack traces and then

00:30:41,289 --> 00:30:44,320
we also have one report that shows you

00:30:42,879 --> 00:30:45,789
all of your stack traces so you can only

00:30:44,320 --> 00:30:46,830
swing back the original file if you want

00:30:45,789 --> 00:30:50,009
to do some digging

00:30:46,830 --> 00:30:52,179
we also have in profile modes and

00:30:50,009 --> 00:30:53,349
utility some stuff that helps you

00:30:52,179 --> 00:30:54,460
understand a little bit about the

00:30:53,349 --> 00:30:56,080
content that was pulled out of this

00:30:54,460 --> 00:30:58,299
document so we have language ID we have

00:30:56,080 --> 00:30:59,169
token counts we have the common word

00:30:58,299 --> 00:31:01,269
count thing which I'll talk about

00:30:59,169 --> 00:31:03,190
shortly we have some statistics on word

00:31:01,269 --> 00:31:04,809
lengths which can be miles can be useful

00:31:03,190 --> 00:31:06,759
for some things and we also have page

00:31:04,809 --> 00:31:08,080
counts so if the file type has a notion

00:31:06,759 --> 00:31:10,270
of page in it

00:31:08,080 --> 00:31:13,300
we we record that so that for exactly

00:31:10,270 --> 00:31:15,130
the use case of which PDFs don't have

00:31:13,300 --> 00:31:18,640
that many words per page and should we

00:31:15,130 --> 00:31:20,740
be looking to OCR those in compare mode

00:31:18,640 --> 00:31:22,630
we export all of the metadata that we

00:31:20,740 --> 00:31:24,430
did for profile but we do it for both a

00:31:22,630 --> 00:31:26,920
and B but then we also have some

00:31:24,430 --> 00:31:28,840
comparison so compares compare the mine

00:31:26,920 --> 00:31:32,080
counts for containers versus embedded

00:31:28,840 --> 00:31:33,700
documents so we had a lot more docx in a

00:31:32,080 --> 00:31:37,630
than we did and B did something change

00:31:33,700 --> 00:31:39,580
in our in our file type ID system and we

00:31:37,630 --> 00:31:41,560
also have course counts of mine changes

00:31:39,580 --> 00:31:43,030
and typically that a number of these

00:31:41,560 --> 00:31:45,910
things require human intervention to in

00:31:43,030 --> 00:31:47,590
humans to interpret what's going on and

00:31:45,910 --> 00:31:49,450
I'll get to that we also have

00:31:47,590 --> 00:31:52,180
comparisons of counts by mine type and a

00:31:49,450 --> 00:31:53,440
number of other things for and then

00:31:52,180 --> 00:31:57,700
content and I'll talk a little bit about

00:31:53,440 --> 00:31:59,770
the content comparison so before I talk

00:31:57,700 --> 00:32:01,150
a little bit about the content

00:31:59,770 --> 00:32:02,860
comparison let me step back a little bit

00:32:01,150 --> 00:32:06,280
and talk about this common words metric

00:32:02,860 --> 00:32:09,820
so this was first proposed at least in

00:32:06,280 --> 00:32:12,250
us in our little group by Tillman Hal

00:32:09,820 --> 00:32:14,530
sure the notion is just take a corpus

00:32:12,250 --> 00:32:16,210
count the most common words for now

00:32:14,530 --> 00:32:18,190
where we started with with English only

00:32:16,210 --> 00:32:21,430
and we dropped words it had fewer than

00:32:18,190 --> 00:32:22,300
four letters I think and just count the

00:32:21,430 --> 00:32:23,920
number of common words that you're

00:32:22,300 --> 00:32:25,810
pulling out of a document that that

00:32:23,920 --> 00:32:27,160
divided by the number of alphabetic

00:32:25,810 --> 00:32:28,630
words gives you some insight so if

00:32:27,160 --> 00:32:29,860
you're only getting you know point zero

00:32:28,630 --> 00:32:31,120
one percent of your terms in your

00:32:29,860 --> 00:32:33,850
document are actually in the common

00:32:31,120 --> 00:32:35,530
words something may be going on and I'll

00:32:33,850 --> 00:32:37,960
talk about how that might not be a

00:32:35,530 --> 00:32:39,550
problem but overall over a large corpus

00:32:37,960 --> 00:32:41,470
that should have roughly language II

00:32:39,550 --> 00:32:44,530
kinds of things in it that can be a

00:32:41,470 --> 00:32:46,390
useful metric we also did some custom

00:32:44,530 --> 00:32:48,250
removal of HTML markup terms like body

00:32:46,390 --> 00:32:50,950
and table and some other things so that

00:32:48,250 --> 00:32:51,640
if an HTML file was misunderstood as a

00:32:50,950 --> 00:32:53,050
text file

00:32:51,640 --> 00:32:54,370
all of a sudden we would get this huge

00:32:53,050 --> 00:32:55,900
boost in common words but it's all

00:32:54,370 --> 00:32:59,460
markup it's just stuff we don't actually

00:32:55,900 --> 00:33:02,770
want so we removed a bunch of those so

00:32:59,460 --> 00:33:05,500
when you're doing when you run with the

00:33:02,770 --> 00:33:07,360
common words and with a number of pages

00:33:05,500 --> 00:33:08,830
you can do some useful statistics on

00:33:07,360 --> 00:33:10,360
number of words per page and you can

00:33:08,830 --> 00:33:11,740
also as I said do the number of common

00:33:10,360 --> 00:33:13,780
words divided by the number of alphabet

00:33:11,740 --> 00:33:15,310
occurs to get some indication of how

00:33:13,780 --> 00:33:17,320
well you're doing per file or at least

00:33:15,310 --> 00:33:19,490
get some metric of files that don't look

00:33:17,320 --> 00:33:22,190
like the others

00:33:19,490 --> 00:33:24,320
for content comparisons we have built-in

00:33:22,190 --> 00:33:26,870
similarity metrics openly this is

00:33:24,320 --> 00:33:28,100
basically overlap so that's of the of

00:33:26,870 --> 00:33:30,409
all of the words that were pulled out of

00:33:28,100 --> 00:33:32,500
document a how many of them are pulled

00:33:30,409 --> 00:33:34,399
show up in document B divided the total

00:33:32,500 --> 00:33:36,799
divided by the total number of unique

00:33:34,399 --> 00:33:37,970
words and both of those documents or you

00:33:36,799 --> 00:33:39,620
can measure how similar they are

00:33:37,970 --> 00:33:41,210
including the number of counts so if the

00:33:39,620 --> 00:33:42,649
word does shows up two hundred times in

00:33:41,210 --> 00:33:44,870
document a but only shows up once in

00:33:42,649 --> 00:33:47,029
document b the first metric would only

00:33:44,870 --> 00:33:49,159
look at the binary does the words does

00:33:47,029 --> 00:33:50,510
show up the second metric takes into

00:33:49,159 --> 00:33:52,520
account that shows up two hundred times

00:33:50,510 --> 00:33:55,730
in document a but only once in document

00:33:52,520 --> 00:33:56,690
B so some other content comparisons we

00:33:55,730 --> 00:33:59,179
can look at the improvement in the

00:33:56,690 --> 00:34:01,730
common words score and then you can do

00:33:59,179 --> 00:34:03,020
this / - a so for PDF documents are we

00:34:01,730 --> 00:34:04,460
doing better on the common word score

00:34:03,020 --> 00:34:07,100
now than we were before which can be

00:34:04,460 --> 00:34:09,169
quite useful to least get a sense of

00:34:07,100 --> 00:34:14,089
what's going on so this is an example of

00:34:09,169 --> 00:34:16,250
looking at 114 as we were moving into as

00:34:14,089 --> 00:34:17,869
I was doing some work to see if we had

00:34:16,250 --> 00:34:19,970
made improvements on 115 or at least

00:34:17,869 --> 00:34:22,520
major regressions so here here we have a

00:34:19,970 --> 00:34:25,310
single file we have unique we had 786

00:34:22,520 --> 00:34:27,290
unique tokens in TECO 114 sixteen

00:34:25,310 --> 00:34:30,909
hundred total tokens the language ID was

00:34:27,290 --> 00:34:33,649
Chinese number of common words zero

00:34:30,909 --> 00:34:34,820
interesting okay then we have the top

00:34:33,649 --> 00:34:36,200
end tokens from that I don't know

00:34:34,820 --> 00:34:37,460
Chinese so that frankly doesn't mean no

00:34:36,200 --> 00:34:38,540
good but I can at least plop them into

00:34:37,460 --> 00:34:40,580
Google Translate and see if I get

00:34:38,540 --> 00:34:42,770
anything useful the common words metric

00:34:40,580 --> 00:34:46,460
for that was 0% because we had zero

00:34:42,770 --> 00:34:50,000
common words over 1603 alphabetic tokens

00:34:46,460 --> 00:34:51,560
rose and 115 we're now getting 272

00:34:50,000 --> 00:34:52,909
tokens so the numbers gone down who

00:34:51,560 --> 00:34:54,919
knows what that's good or bad language

00:34:52,909 --> 00:34:57,080
ID has changed okay something's going on

00:34:54,919 --> 00:34:59,810
with this file the common words now goes

00:34:57,080 --> 00:35:01,130
up to 116 for ratio 46 percent of year

00:34:59,810 --> 00:35:04,670
of the words that were extracted and now

00:35:01,130 --> 00:35:06,859
part of that common words list and I

00:35:04,670 --> 00:35:08,390
know a bunch of German so those look

00:35:06,859 --> 00:35:10,339
like German ish words can be so this is

00:35:08,390 --> 00:35:12,530
this is an improvement and the goal is

00:35:10,339 --> 00:35:13,910
not necessarily to manually review all

00:35:12,530 --> 00:35:16,130
of your files a that looks like a

00:35:13,910 --> 00:35:17,810
language I don't know or not but really

00:35:16,130 --> 00:35:19,330
to rely on that Condors thing to get a

00:35:17,810 --> 00:35:22,339
general view of how well you're doing

00:35:19,330 --> 00:35:24,440
when you're comparing things this is an

00:35:22,339 --> 00:35:28,030
example of a small regression that we

00:35:24,440 --> 00:35:32,810
found in moving from 114 to 115 snapshot

00:35:28,030 --> 00:35:35,090
this is you can see that we had slightly

00:35:32,810 --> 00:35:37,640
more total tokens we had slightly fewer

00:35:35,090 --> 00:35:39,500
common words the overlap between these

00:35:37,640 --> 00:35:41,360
two strings is quite high it's ninety

00:35:39,500 --> 00:35:45,110
five point five percent of the words in

00:35:41,360 --> 00:35:46,790
a we're in B the top 10 unique tokens

00:35:45,110 --> 00:35:49,460
which is a measure of which words only

00:35:46,790 --> 00:35:50,690
show up in a and never show up and B we

00:35:49,460 --> 00:35:52,250
get some good English looking words

00:35:50,690 --> 00:35:54,260
whereas we if we look at the words that

00:35:52,250 --> 00:35:57,530
only show up and B and never show up in

00:35:54,260 --> 00:35:58,970
a we get some things that show that you

00:35:57,530 --> 00:36:00,200
know something's not quite going on and

00:35:58,970 --> 00:36:01,820
it's probably the single quotes are

00:36:00,200 --> 00:36:03,020
being converted to eyes so something's

00:36:01,820 --> 00:36:05,390
not quite working with the car set

00:36:03,020 --> 00:36:07,400
recognition in that file as a human we

00:36:05,390 --> 00:36:09,230
can look at that and reverse-engineer

00:36:07,400 --> 00:36:11,690
with what the issue probably is but you

00:36:09,230 --> 00:36:14,120
can see there is a small decrease in the

00:36:11,690 --> 00:36:17,360
in that ratio of common words for

00:36:14,120 --> 00:36:18,800
alphabetic tokens and there's a sorry

00:36:17,360 --> 00:36:20,630
there should be yes the increase in

00:36:18,800 --> 00:36:24,140
common words is negative 89 or the

00:36:20,630 --> 00:36:26,180
decrease in common words is 89 so you

00:36:24,140 --> 00:36:27,650
can see that we this this was able to

00:36:26,180 --> 00:36:29,180
point me to a specific regression or a

00:36:27,650 --> 00:36:33,110
file that had a regression when we move

00:36:29,180 --> 00:36:34,820
from 114 and 115 so we did actually we

00:36:33,110 --> 00:36:38,090
did take this evaluation metric public

00:36:34,820 --> 00:36:42,170
we now have thanks to Rackspace VM that

00:36:38,090 --> 00:36:44,150
is thanks to http D we're posting files

00:36:42,170 --> 00:36:45,770
and results on that server we have a

00:36:44,150 --> 00:36:47,150
terabyte of data in there it's roughly

00:36:45,770 --> 00:36:48,800
three million files from common crawl

00:36:47,150 --> 00:36:50,810
and gov Doc's one above all sort of

00:36:48,800 --> 00:36:53,120
different file formats we've highly over

00:36:50,810 --> 00:36:55,190
sampled for non HTML mount XT things

00:36:53,120 --> 00:36:57,110
we're collaborating with PDF box

00:36:55,190 --> 00:36:58,370
employee to run evals as part of the

00:36:57,110 --> 00:37:01,370
release projects for each of those

00:36:58,370 --> 00:37:02,870
projects and then also for tika this is

00:37:01,370 --> 00:37:05,060
really useful for new parsers and it's

00:37:02,870 --> 00:37:06,470
also really useful for that hey I'm not

00:37:05,060 --> 00:37:07,880
on getting this parse exception but I

00:37:06,470 --> 00:37:10,160
can't share the document with you

00:37:07,880 --> 00:37:11,360
problem that we get all the time because

00:37:10,160 --> 00:37:13,160
now somebody can give us a stack trace

00:37:11,360 --> 00:37:14,570
we can go to look in our database of

00:37:13,160 --> 00:37:16,400
stack traces and say that here's an

00:37:14,570 --> 00:37:17,690
example of what's tricking triggering

00:37:16,400 --> 00:37:20,300
your stack trace and then we can go work

00:37:17,690 --> 00:37:22,250
on it one of the common crawl tools we

00:37:20,300 --> 00:37:25,100
used initially was Dominic Stadler who

00:37:22,250 --> 00:37:26,600
works with us on Apache poi and that's

00:37:25,100 --> 00:37:29,890
really useful for picking out specific

00:37:26,600 --> 00:37:37,250
documents from the common crawl corpus

00:37:29,890 --> 00:37:38,480
so limits yeah limits so if you get more

00:37:37,250 --> 00:37:40,610
exceptions we have a problem

00:37:38,480 --> 00:37:41,720
well no not always because sometimes you

00:37:40,610 --> 00:37:42,800
have a new parser when you weren't

00:37:41,720 --> 00:37:44,870
getting exceptions before because you

00:37:42,800 --> 00:37:46,070
want parsing those files or before maybe

00:37:44,870 --> 00:37:46,520
the parser was yielding junk and now

00:37:46,070 --> 00:37:48,890
you're getting

00:37:46,520 --> 00:37:50,420
exception so that's actually good we

00:37:48,890 --> 00:37:52,970
have fewer exceptions that's great no

00:37:50,420 --> 00:37:54,800
you might not have been detecting that

00:37:52,970 --> 00:37:56,690
prior or might now be failing to detect

00:37:54,800 --> 00:37:58,340
that file properly so you're skipping it

00:37:56,690 --> 00:38:01,250
or now we're getting junk more common

00:37:58,340 --> 00:38:03,380
words great no actually and this was

00:38:01,250 --> 00:38:05,180
again my fault I'm a serious bug that's

00:38:03,380 --> 00:38:07,130
duplicating worksheets so weak getting

00:38:05,180 --> 00:38:09,110
more common words great nothing you're

00:38:07,130 --> 00:38:11,720
duplicating it your worksheets so that's

00:38:09,110 --> 00:38:13,730
a problem or you might get non each team

00:38:11,720 --> 00:38:15,350
Elish markup it's still markup and not

00:38:13,730 --> 00:38:16,520
the stuff that we've removed from a

00:38:15,350 --> 00:38:19,220
commons word list and that might be

00:38:16,520 --> 00:38:21,710
creeping through fewer common words

00:38:19,220 --> 00:38:23,800
there's a problem no it might actually

00:38:21,710 --> 00:38:26,330
be a good thing so for all of these

00:38:23,800 --> 00:38:27,860
things that one initially thinking about

00:38:26,330 --> 00:38:32,210
wow that's a sign of improving oh that's

00:38:27,860 --> 00:38:34,400
a sign of a regression no it can be it

00:38:32,210 --> 00:38:36,880
often is but not all the time and all of

00:38:34,400 --> 00:38:38,600
these things require human

00:38:36,880 --> 00:38:42,080
interpretation of the data

00:38:38,600 --> 00:38:43,790
so for Pink Floyd's and the Pink Floyd

00:38:42,080 --> 00:38:45,290
fans in the audience the ticket is

00:38:43,790 --> 00:38:47,840
thrown the dream is gone

00:38:45,290 --> 00:38:50,330
we you know we can run use regression

00:38:47,840 --> 00:38:52,010
without ground truth we will get some

00:38:50,330 --> 00:38:53,300
insight we will know when things have

00:38:52,010 --> 00:38:54,920
changed we will be able to drill down

00:38:53,300 --> 00:38:56,240
and figure out what's changed and try to

00:38:54,920 --> 00:38:58,700
make sense of that which is far better

00:38:56,240 --> 00:39:00,200
than what we were doing but given that

00:38:58,700 --> 00:39:02,120
humans needed we really do need a user

00:39:00,200 --> 00:39:04,070
interface so please chip into that if

00:39:02,120 --> 00:39:05,360
you can there's also the notion of

00:39:04,070 --> 00:39:07,070
collaborative tagging we've been working

00:39:05,360 --> 00:39:07,580
on the same corpus for about two years

00:39:07,070 --> 00:39:09,020
now

00:39:07,580 --> 00:39:10,280
you know each time we look at the file

00:39:09,020 --> 00:39:11,930
we can say hey that's really good or

00:39:10,280 --> 00:39:13,280
that's a bad version or this file is

00:39:11,930 --> 00:39:14,900
totally hosed and we can expect to get

00:39:13,280 --> 00:39:16,190
nothing out of it and we should be doing

00:39:14,900 --> 00:39:18,310
collaborative tagging we don't have a

00:39:16,190 --> 00:39:21,140
user interface to help with that yet so

00:39:18,310 --> 00:39:23,840
the dream of teeka 1302 happen to hit

00:39:21,140 --> 00:39:27,170
reality but we're far better than where

00:39:23,840 --> 00:39:28,790
we were so to conclude text extraction

00:39:27,170 --> 00:39:31,250
is critical to many of our projects

00:39:28,790 --> 00:39:32,540
please evaluate at least on a random

00:39:31,250 --> 00:39:34,490
sample of some of your documents please

00:39:32,540 --> 00:39:35,300
do not just throw stuff in the solar and

00:39:34,490 --> 00:39:37,370
hope for the best

00:39:35,300 --> 00:39:39,350
please use tika eval if it suits your

00:39:37,370 --> 00:39:41,870
needs please join our community and help

00:39:39,350 --> 00:39:43,790
us with the evaluation with tika with

00:39:41,870 --> 00:39:46,970
content extraction it's really quite

00:39:43,790 --> 00:39:48,710
critical I have some resources Nick

00:39:46,970 --> 00:39:50,540
written gave a great talk on what's new

00:39:48,710 --> 00:39:52,730
with Apache tika also a great overview

00:39:50,540 --> 00:39:54,380
of what's in tika we have the Tiki Val

00:39:52,730 --> 00:39:54,740
wiki with a bunch of other pages at that

00:39:54,380 --> 00:39:58,760
point

00:39:54,740 --> 00:39:59,850
- Ryan Baumann a fellow classicist who's

00:39:58,760 --> 00:40:01,650
worked on automatic

00:39:59,850 --> 00:40:03,900
evaluation of OCR OCR where you do not

00:40:01,650 --> 00:40:05,970
have ground truth has a really good post

00:40:03,900 --> 00:40:08,730
on what he's done he has some really

00:40:05,970 --> 00:40:10,980
good ideas so those are some resources

00:40:08,730 --> 00:40:23,490
and onward thank you so much any

00:40:10,980 --> 00:40:25,230
questions with that thank you so much

00:40:23,490 --> 00:40:33,350
for coming when to to this up please

00:40:25,230 --> 00:40:33,350
thank you yes please

00:40:41,920 --> 00:40:45,070
yes fortunately we have solar people in

00:40:44,020 --> 00:40:46,630
the room I don't know if you've worked a

00:40:45,070 --> 00:40:47,710
lot with the data import handler so

00:40:46,630 --> 00:40:48,910
there's a data import Handler and

00:40:47,710 --> 00:40:52,000
there's a way that you can map from

00:40:48,910 --> 00:40:53,830
teeka fields to solar fields and there's

00:40:52,000 --> 00:40:55,990
a way to configure that and I recommend

00:40:53,830 --> 00:40:57,160
doing that yourself because key care can

00:40:55,990 --> 00:40:58,870
come up with all sorts of crazy field

00:40:57,160 --> 00:41:01,060
and you do not want to use the schema

00:40:58,870 --> 00:41:02,260
list version of soul or the schema was

00:41:01,060 --> 00:41:04,420
set up and solar when you're importing

00:41:02,260 --> 00:41:06,640
from but if you look at data import

00:41:04,420 --> 00:41:08,440
handler or any other resources you'd

00:41:06,640 --> 00:41:17,710
recommend Christine or anybody else

00:41:08,440 --> 00:41:19,510
familiar with solar ok yeah yeah so not

00:41:17,710 --> 00:41:20,950
not that I'm aware of there I will say

00:41:19,510 --> 00:41:22,390
with the data import handler that's to

00:41:20,950 --> 00:41:23,710
get people off the ground quickly so

00:41:22,390 --> 00:41:25,540
that they feel like they can ingest

00:41:23,710 --> 00:41:27,430
these things easily but if you are

00:41:25,540 --> 00:41:28,810
handling a lot of documents that you

00:41:27,430 --> 00:41:30,490
don't trust it's a really dangerous

00:41:28,810 --> 00:41:33,100
thing to do so

00:41:30,490 --> 00:41:36,820
so do there's a fantastic post on using

00:41:33,100 --> 00:41:38,050
solar j2 to separate your JVM so that

00:41:36,820 --> 00:41:48,180
you have a different whole different

00:41:38,050 --> 00:41:48,180
cluster of solar versus teeka yeah

00:41:50,310 --> 00:41:55,690
yeah but Nick really hit home in his

00:41:53,290 --> 00:41:57,220
talk he could does a good job of trying

00:41:55,690 --> 00:42:00,550
to normalize those different metadata

00:41:57,220 --> 00:42:02,290
tags so that if PDF happens to call the

00:42:00,550 --> 00:42:04,030
person who created the thing the creator

00:42:02,290 --> 00:42:08,170
but Microsoft calls it the author we

00:42:04,030 --> 00:42:09,310
normalize those to Dublin core forget

00:42:08,170 --> 00:42:11,020
what does dublin core is a creator or

00:42:09,310 --> 00:42:12,910
author whichever it is whatever the

00:42:11,020 --> 00:42:14,830
dublin core is we try to normalize boat

00:42:12,910 --> 00:42:16,119
both of those two dublin core so we try

00:42:14,830 --> 00:42:19,210
to help normalize to the degree we can

00:42:16,119 --> 00:42:20,880
there are some file formats specific

00:42:19,210 --> 00:42:24,340
metadata things that we can normalize

00:42:20,880 --> 00:42:25,900
but you can once you do the extracts you

00:42:24,340 --> 00:42:26,830
can only see what keys you have I'm

00:42:25,900 --> 00:42:29,340
going to figure out which ones you want

00:42:26,830 --> 00:42:32,730
to extract but it is also a largely

00:42:29,340 --> 00:42:32,730
application dependent

00:42:56,490 --> 00:43:03,359
Yeah right so you can run yeah Java -

00:43:00,330 --> 00:43:04,920
Chartier app dr. r and then just put the

00:43:03,359 --> 00:43:06,480
name of the file you'll get you'll get

00:43:04,920 --> 00:43:08,280
stuff dumped it and standard out you

00:43:06,480 --> 00:43:09,599
also we have a user interface you can

00:43:08,280 --> 00:43:11,339
drop a file in there and you'll see what

00:43:09,599 --> 00:43:12,210
information you're getting out I would

00:43:11,339 --> 00:43:15,000
recommend if you have a bunch of

00:43:12,210 --> 00:43:17,640
documents though running teeka

00:43:15,000 --> 00:43:20,099
perhaps key Captiva batik a batch java

00:43:17,640 --> 00:43:23,790
Sharky cap - I input your actual output

00:43:20,099 --> 00:43:25,230
directory and then perhaps the J option

00:43:23,790 --> 00:43:26,550
so you get all of that metadata and then

00:43:25,230 --> 00:43:27,780
running some parsing on that to figure

00:43:26,550 --> 00:43:29,849
out exactly what you're getting across

00:43:27,780 --> 00:43:31,109
your corpus because just looking at one

00:43:29,849 --> 00:43:33,060
file at a time will not give you a sense

00:43:31,109 --> 00:43:35,820
of the diversity of tags available to

00:43:33,060 --> 00:43:36,930
you so you do have to do some mining of

00:43:35,820 --> 00:43:38,550
your documents to figure out what you

00:43:36,930 --> 00:43:39,869
have and with some with a huge

00:43:38,550 --> 00:43:47,369
consideration of what your information

00:43:39,869 --> 00:43:48,540
needs are at the search layer great well

00:43:47,369 --> 00:43:49,859
I'll be happy to stick around if there

00:43:48,540 --> 00:43:51,450
any other questions thank you all so

00:43:49,859 --> 00:43:52,680
much I know the feather left the

00:43:51,450 --> 00:43:56,280
building so I'm thrilled that anybody

00:43:52,680 --> 00:44:02,049
showed up and thank you so much

00:43:56,280 --> 00:44:02,049

YouTube URL: https://www.youtube.com/watch?v=vRPTPMwI53k


