Title: How Container Schedulers and Software-based Storage will Change the Cloud - David vonThenen
Publication date: 2017-05-18
Playlist: ApacheCon 2017 - Miami
Description: 
	How Container Schedulers and Software-based Storage will Change the Cloud - David vonThenen, {code} by Dell EMC

Persistent applications can be complex to manage and operate at scale but tend to be perfect for modern schedulers like Apache Mesos. The current trend in Containers is taking their ephemeral nature and turning it upside-down by running databases, key/value stores, WordPress, and etc within them. Internal direct attached storage and external storage are both options in running your long-running, persistent applications. The problem is how do you run these applications and containers in production environments?

This talk outlines how 2 Layer Scheduling, as known as the Offer-Accept model, found in Mesos and Software-based Storage enables deployment of managed frameworks and tasks while maintaining high availability, scale-out growth, and automation. This combination of technology will help build a ""Skynet"" like architecture for persistent applications and containers in the cloud.

About David von Thenen
David vonThenen is an Open Source Engineer at {code} by Dell EMC. The {code} team lives and breathes Open Source by making contributions to the community in a wide variety of projects ranging from Apache Mesos, Docker, Kubernetes, and storage orchestration platforms. Prior to joining {code} by Dell EMC, David was a technical architect and development lead for a Backup/Recovery solution with a heavy focus in the virtualization space, VMware in particular.
Captions: 
	00:00:00,030 --> 00:00:05,069
so yeah David bump tenon I'm from

00:00:02,220 --> 00:00:07,200
actually a team called code by Delhi and

00:00:05,069 --> 00:00:09,750
C so the code team what we do is we

00:00:07,200 --> 00:00:13,080
focus on everything open-source we work

00:00:09,750 --> 00:00:14,759
with maize O's kubernetes swarm and kind

00:00:13,080 --> 00:00:17,400
of what we're known for is taking

00:00:14,759 --> 00:00:19,949
different container schedulers and

00:00:17,400 --> 00:00:23,250
enabling them to consume different

00:00:19,949 --> 00:00:25,230
storage platforms on the backend so kind

00:00:23,250 --> 00:00:26,939
of focusing around the talk today right

00:00:25,230 --> 00:00:30,630
talk container schedule a software based

00:00:26,939 --> 00:00:32,820
storage and the cloud prior to working

00:00:30,630 --> 00:00:33,980
on the code team I actually worked in

00:00:32,820 --> 00:00:37,530
[Music]

00:00:33,980 --> 00:00:39,420
soft backup and recovery solutions in

00:00:37,530 --> 00:00:40,469
the virtualized space vmware in

00:00:39,420 --> 00:00:42,510
particular so backing up virtual

00:00:40,469 --> 00:00:47,070
machines be cloud director and be cloud

00:00:42,510 --> 00:00:48,149
air when it existed so yeah so today

00:00:47,070 --> 00:00:49,890
we're going I'm just going to do a

00:00:48,149 --> 00:00:51,600
really brief overview of what software

00:00:49,890 --> 00:00:53,789
based storage is and then we're going to

00:00:51,600 --> 00:00:55,590
talk container schedulers for those just

00:00:53,789 --> 00:00:57,629
do a little quick review and then kind

00:00:55,590 --> 00:00:59,430
of go into depth for those that don't

00:00:57,629 --> 00:01:00,840
know much about it and then we're going

00:00:59,430 --> 00:01:03,359
to talk about what happens when we

00:01:00,840 --> 00:01:05,909
combine a maze O's framework and

00:01:03,359 --> 00:01:07,470
software based storage and then what

00:01:05,909 --> 00:01:09,900
happens when we take that framework into

00:01:07,470 --> 00:01:12,619
the cloud and then hopefully we're going

00:01:09,900 --> 00:01:17,100
to do a demo maybe two if time permits

00:01:12,619 --> 00:01:18,210
so software based storage so there's

00:01:17,100 --> 00:01:19,380
what are they so there are a lot of

00:01:18,210 --> 00:01:20,759
definitions that are kind of floating

00:01:19,380 --> 00:01:23,490
out there if you look at the Wikipedia

00:01:20,759 --> 00:01:25,500
page there's just gobs of information I

00:01:23,490 --> 00:01:27,930
think most people kind of agreed on two

00:01:25,500 --> 00:01:31,049
things one software-defined storage

00:01:27,930 --> 00:01:34,950
serves as an abstraction layer on the

00:01:31,049 --> 00:01:36,450
underlying storage so you know well go

00:01:34,950 --> 00:01:39,030
into a couple examples in a few minutes

00:01:36,450 --> 00:01:40,500
and then they also provide

00:01:39,030 --> 00:01:42,930
software-defined storage usually

00:01:40,500 --> 00:01:47,369
provides an API so some storms some form

00:01:42,930 --> 00:01:50,070
of CLI or API REST API to make changes

00:01:47,369 --> 00:01:52,409
manipulate the underlying storage or the

00:01:50,070 --> 00:01:55,140
storage platform so there's a couple

00:01:52,409 --> 00:01:58,890
examples I have here are NFS and beasts

00:01:55,140 --> 00:02:01,310
and so if we look at NFS so actually

00:01:58,890 --> 00:02:04,219
before we get into that so what makes

00:02:01,310 --> 00:02:09,750
software-defined storage unique so

00:02:04,219 --> 00:02:12,000
operationally you provision the hardware

00:02:09,750 --> 00:02:13,740
or you provision your storage through

00:02:12,000 --> 00:02:16,200
those api's so

00:02:13,740 --> 00:02:18,240
you and it's independent of the data

00:02:16,200 --> 00:02:21,840
that's on the underlying hardware so as

00:02:18,240 --> 00:02:24,660
NFS as an example right that's a very

00:02:21,840 --> 00:02:26,190
primitive software-defined storage very

00:02:24,660 --> 00:02:27,900
very primitive it's kind of like one of

00:02:26,190 --> 00:02:30,240
the earlier ones but if you have a host

00:02:27,900 --> 00:02:33,450
system and then you have rotating discs

00:02:30,240 --> 00:02:35,970
underneath you have a just through CLI

00:02:33,450 --> 00:02:39,180
you can say create an NFS server to

00:02:35,970 --> 00:02:41,360
serve this up to some consumer and that

00:02:39,180 --> 00:02:44,880
consumer would be an NFS client right

00:02:41,360 --> 00:02:47,250
and then kind of the fit there's a

00:02:44,880 --> 00:02:49,950
physical abstraction layer that that

00:02:47,250 --> 00:02:51,480
also exists so you could have instead of

00:02:49,950 --> 00:02:53,640
having one rotating disc you could have

00:02:51,480 --> 00:02:55,710
two rotating discs that form an LVM

00:02:53,640 --> 00:02:59,910
group hence you can serve that combined

00:02:55,710 --> 00:03:02,460
storage up to to your NFS clients and

00:02:59,910 --> 00:03:05,070
there's also a form of notion of policy

00:03:02,460 --> 00:03:09,720
so you can usually on software-defined

00:03:05,070 --> 00:03:11,820
storage say I want to have tiered levels

00:03:09,720 --> 00:03:13,670
of service so I want to have like a gold

00:03:11,820 --> 00:03:16,200
tier that's the you know that's maybe

00:03:13,670 --> 00:03:18,540
SSDs and then you maybe have a bronze

00:03:16,200 --> 00:03:21,450
tier that's rotating disk and based on

00:03:18,540 --> 00:03:22,830
your level of service that you want for

00:03:21,450 --> 00:03:24,600
your particular application you say I

00:03:22,830 --> 00:03:25,860
need to have the gold tier or I need to

00:03:24,600 --> 00:03:28,380
have the bronze tier because I don't

00:03:25,860 --> 00:03:29,250
really care about performance and then

00:03:28,380 --> 00:03:30,420
the other thing that's kind of

00:03:29,250 --> 00:03:32,640
interesting about software-defined

00:03:30,420 --> 00:03:36,120
storage is that the de two operations

00:03:32,640 --> 00:03:38,040
are inherently unique so what I mean by

00:03:36,120 --> 00:03:39,660
day two is maintenance right so once you

00:03:38,040 --> 00:03:42,840
you know throw up your software-defined

00:03:39,660 --> 00:03:44,340
storage platform how you manage that

00:03:42,840 --> 00:03:46,860
platform is very different than how you

00:03:44,340 --> 00:03:48,840
manage your typical array that sits in

00:03:46,860 --> 00:03:51,690
some data center that takes up an entire

00:03:48,840 --> 00:03:55,710
rack right because it exists all mostly

00:03:51,690 --> 00:03:58,350
in software it you know how you handle

00:03:55,710 --> 00:04:00,150
like approaching full and and those

00:03:58,350 --> 00:04:03,480
types of events are very different than

00:04:00,150 --> 00:04:05,730
the traditional storage array so I kind

00:04:03,480 --> 00:04:07,590
of already gave a little brief example

00:04:05,730 --> 00:04:09,270
of what NFS looks like so this is kind

00:04:07,590 --> 00:04:11,550
of like just a visual representation of

00:04:09,270 --> 00:04:13,770
that so two physical drives maybe

00:04:11,550 --> 00:04:15,240
they're in an L VM group then you have

00:04:13,770 --> 00:04:17,670
that software-defined storage layer

00:04:15,240 --> 00:04:20,100
which is just your NFS server and then

00:04:17,670 --> 00:04:22,830
that gets exposed out to the end user

00:04:20,100 --> 00:04:25,800
and you consume that nfo storage or like

00:04:22,830 --> 00:04:27,300
we all know using an NFS client that's a

00:04:25,800 --> 00:04:29,639
very very simple

00:04:27,300 --> 00:04:32,520
definition of what like software-defined

00:04:29,639 --> 00:04:34,139
storage is we're going to skip over V

00:04:32,520 --> 00:04:36,300
sand because I don't want to put more

00:04:34,139 --> 00:04:39,449
time towards the demo but this is a very

00:04:36,300 --> 00:04:43,139
complex example of V sand whose use

00:04:39,449 --> 00:04:48,060
VMware VMware visa and it'll so yeah so

00:04:43,139 --> 00:04:49,979
it's it's rotating disk with a SSD front

00:04:48,060 --> 00:04:52,199
end that's aggregated to look like a

00:04:49,979 --> 00:04:54,150
global pool of storage that you can

00:04:52,199 --> 00:04:56,639
create logical volumes out so you can

00:04:54,150 --> 00:05:02,190
create VM decays for your virtual

00:04:56,639 --> 00:05:05,699
machines in a nutshell so I picked NFS

00:05:02,190 --> 00:05:07,319
and V sandy in particular and the reason

00:05:05,699 --> 00:05:10,139
why and what makes them special is that

00:05:07,319 --> 00:05:12,509
they all exist in software so there's no

00:05:10,139 --> 00:05:16,650
special purpose-built hardware that's

00:05:12,509 --> 00:05:19,740
required to run NFS and beasts and yes

00:05:16,650 --> 00:05:21,719
vfan does have an HCl and all that stuff

00:05:19,740 --> 00:05:24,449
you can roll the dice and bring your own

00:05:21,719 --> 00:05:26,009
SSDs and stuff like that but again

00:05:24,449 --> 00:05:28,319
there's no special purpose-built

00:05:26,009 --> 00:05:31,349
hardware that's required in order to

00:05:28,319 --> 00:05:32,909
have a software based sword so software

00:05:31,349 --> 00:05:35,610
based storage another way of looking at

00:05:32,909 --> 00:05:38,310
it is if you have a host system that's

00:05:35,610 --> 00:05:40,710
out there and you have a software based

00:05:38,310 --> 00:05:42,479
storage platform usually the way you

00:05:40,710 --> 00:05:44,460
install that is nothing through nothing

00:05:42,479 --> 00:05:46,469
more than installing rpms or Deb's on

00:05:44,460 --> 00:05:48,630
top of the host system and then exposing

00:05:46,469 --> 00:05:57,719
some sort of storage out from that end

00:05:48,630 --> 00:06:02,219
point questions so far yeah so that's

00:05:57,719 --> 00:06:06,840
another good example yep okay

00:06:02,219 --> 00:06:11,879
container schedulers so who here has

00:06:06,840 --> 00:06:13,110
used mais O's kubernetes swarm okay so

00:06:11,879 --> 00:06:14,550
we're just going to do a little quick

00:06:13,110 --> 00:06:16,919
overview and then we're going to kind of

00:06:14,550 --> 00:06:19,379
because we're here at Apache town we're

00:06:16,919 --> 00:06:21,240
going to talk maze O's and then go into

00:06:19,379 --> 00:06:23,250
a little more detail so what is a

00:06:21,240 --> 00:06:25,620
scheduler so a scheduler effectively

00:06:23,250 --> 00:06:28,860
does a fair and efficient placement of

00:06:25,620 --> 00:06:31,830
workloads and they usually adhere to a

00:06:28,860 --> 00:06:33,900
set of constraints so if you have you

00:06:31,830 --> 00:06:35,490
know a particular workload maybe you

00:06:33,900 --> 00:06:37,050
have a Postgres database or even

00:06:35,490 --> 00:06:40,200
something simple like a little Java app

00:06:37,050 --> 00:06:43,200
and you deploy that into your

00:06:40,200 --> 00:06:44,640
your container scheduler it makes sense

00:06:43,200 --> 00:06:46,110
that the container schedule is not going

00:06:44,640 --> 00:06:47,490
to take all your workload and put it on

00:06:46,110 --> 00:06:50,040
one piece of compute it's going to try

00:06:47,490 --> 00:06:52,710
to distribute that through through all

00:06:50,040 --> 00:06:53,820
your notes all your compute notes and it

00:06:52,710 --> 00:06:56,040
also does that quickly and

00:06:53,820 --> 00:06:57,510
deterministically so if you have a

00:06:56,040 --> 00:06:59,670
particular workload that needs to be

00:06:57,510 --> 00:07:02,130
rescheduled there's a there's a

00:06:59,670 --> 00:07:04,920
deterministic time that that's known

00:07:02,130 --> 00:07:06,690
where if a workload comes down and it

00:07:04,920 --> 00:07:09,600
comes back up on another node that's a

00:07:06,690 --> 00:07:12,660
set finite time and the reason why it's

00:07:09,600 --> 00:07:14,640
deterministic is if it wasn't then yeah

00:07:12,660 --> 00:07:16,680
it just becomes very difficult to

00:07:14,640 --> 00:07:18,600
predict the behavior of your application

00:07:16,680 --> 00:07:20,610
in your in your workload and then it's

00:07:18,600 --> 00:07:24,270
also robust and tolerate tolerates

00:07:20,610 --> 00:07:26,520
failure so if you have a particular

00:07:24,270 --> 00:07:28,140
compute node that fails that workload is

00:07:26,520 --> 00:07:30,300
going to get rescheduled on to a

00:07:28,140 --> 00:07:35,520
different node so that your applications

00:07:30,300 --> 00:07:37,170
available again so scheduling work so

00:07:35,520 --> 00:07:39,750
what kind of work am I talking about so

00:07:37,170 --> 00:07:41,070
I'm talking about containers so we're

00:07:39,750 --> 00:07:43,050
talking container schedulers so

00:07:41,070 --> 00:07:45,900
containers like docker you know made

00:07:43,050 --> 00:07:47,340
those has a unified container Iser and

00:07:45,900 --> 00:07:50,100
then for those that are familiar with

00:07:47,340 --> 00:07:52,260
rocket and core OS that you know those

00:07:50,100 --> 00:07:54,990
types of containers already talked about

00:07:52,260 --> 00:07:59,370
that container schedules are effectively

00:07:54,990 --> 00:08:01,560
cluster manager if you you know if you

00:07:59,370 --> 00:08:03,390
have a particular workload that comes

00:08:01,560 --> 00:08:06,090
down and it needs to be rescheduled it

00:08:03,390 --> 00:08:08,970
does that it's also a resource manager

00:08:06,090 --> 00:08:11,910
right so if you have n pieces of compute

00:08:08,970 --> 00:08:13,830
node you have X number of CPUs that are

00:08:11,910 --> 00:08:17,460
out there and wide Y amounts of memory

00:08:13,830 --> 00:08:19,350
and when you schedule a particular piece

00:08:17,460 --> 00:08:22,440
of work to be run on your compute it

00:08:19,350 --> 00:08:23,970
takes into account how many CPUs you

00:08:22,440 --> 00:08:26,130
need and how much memory you need and

00:08:23,970 --> 00:08:28,530
based on that it will decide where to

00:08:26,130 --> 00:08:30,510
place your particular tasks so as an

00:08:28,530 --> 00:08:33,479
example if you had a particular workload

00:08:30,510 --> 00:08:35,340
that required two CPUs and very little

00:08:33,479 --> 00:08:37,830
memory and you had another piece of work

00:08:35,340 --> 00:08:40,979
another workload that required like you

00:08:37,830 --> 00:08:44,130
know 0.1 CPUs and like 60 gigs of memory

00:08:40,979 --> 00:08:45,750
it might be advantageous to place both

00:08:44,130 --> 00:08:47,310
those workloads on the same piece of

00:08:45,750 --> 00:08:50,280
compute because those resources aren't

00:08:47,310 --> 00:08:52,020
competing with each other right and then

00:08:50,280 --> 00:08:53,910
there's also operational constraints so

00:08:52,020 --> 00:08:56,190
when you schedule work you can say

00:08:53,910 --> 00:08:58,710
I want to deploy this particular

00:08:56,190 --> 00:09:00,870
application I want it to be scaled to

00:08:58,710 --> 00:09:02,910
three nodes but maybe you want to

00:09:00,870 --> 00:09:05,130
constraint that says hey when you place

00:09:02,910 --> 00:09:07,170
these you know these three instances

00:09:05,130 --> 00:09:09,600
don't place them on the same node right

00:09:07,170 --> 00:09:11,460
spread them out on three unique nodes so

00:09:09,600 --> 00:09:18,750
that with one fails at the other two are

00:09:11,460 --> 00:09:20,310
up and running and the other cool thing

00:09:18,750 --> 00:09:24,150
about container schedulers is that you

00:09:20,310 --> 00:09:27,300
can do forms of custom scheduling so if

00:09:24,150 --> 00:09:30,090
you want to kind of extend the the

00:09:27,300 --> 00:09:31,470
analogy of you know the example of

00:09:30,090 --> 00:09:34,350
placing on different nodes

00:09:31,470 --> 00:09:36,690
you might also if your application is

00:09:34,350 --> 00:09:39,030
highly sensitive you like you know maybe

00:09:36,690 --> 00:09:42,000
using no sequel and Cassandra and yes a

00:09:39,030 --> 00:09:43,650
three node Cassandra instance you might

00:09:42,000 --> 00:09:45,330
want to even do something like hey

00:09:43,650 --> 00:09:47,010
instead of placing these on three

00:09:45,330 --> 00:09:48,870
separate nodes I want you actually place

00:09:47,010 --> 00:09:51,600
these three instances on separate racks

00:09:48,870 --> 00:09:53,790
right so if one rack goes down you know

00:09:51,600 --> 00:09:57,300
that I still have my application

00:09:53,790 --> 00:09:59,580
available and you might also have a

00:09:57,300 --> 00:10:02,310
particular workload that requires like I

00:09:59,580 --> 00:10:05,610
don't know like heavy amount of compute

00:10:02,310 --> 00:10:07,410
cycles and if you have nodes out there

00:10:05,610 --> 00:10:09,060
that are tagged that's saying hey I have

00:10:07,410 --> 00:10:11,190
a GPU that's available you know

00:10:09,060 --> 00:10:12,630
high-performing GPU you might want to

00:10:11,190 --> 00:10:14,040
schedule that workload on that

00:10:12,630 --> 00:10:17,430
particular node so that you could take

00:10:14,040 --> 00:10:18,450
advantage of the GPU that's there so

00:10:17,430 --> 00:10:22,550
that's like a form of hardware

00:10:18,450 --> 00:10:22,550
acceleration yeah

00:10:22,880 --> 00:10:28,020
so I kind of generically talked about

00:10:26,220 --> 00:10:29,880
container schedulers and now we're going

00:10:28,020 --> 00:10:32,370
to talk about one in particular so

00:10:29,880 --> 00:10:36,450
patchy mezzos since we're here to patch

00:10:32,370 --> 00:10:38,460
econ it's been around for some time but

00:10:36,450 --> 00:10:40,110
the cool thing about it being around for

00:10:38,460 --> 00:10:42,030
some time is that all the companies that

00:10:40,110 --> 00:10:44,220
you know here's the logo screen right

00:10:42,030 --> 00:10:46,950
all the companies that are on this logo

00:10:44,220 --> 00:10:50,010
screen here are all using in production

00:10:46,950 --> 00:10:52,050
today so it's not like some hypothetical

00:10:50,010 --> 00:10:53,520
if I'm thinking about deploying it or

00:10:52,050 --> 00:10:58,140
I'm kicking the tires these companies

00:10:53,520 --> 00:11:00,270
are doing it as an example so my VP Josh

00:10:58,140 --> 00:11:03,060
Bernstein he actually came from Apple

00:11:00,270 --> 00:11:05,310
and so the Apple Siri infrastructure is

00:11:03,060 --> 00:11:06,710
all built on Bezos so it's not like a

00:11:05,310 --> 00:11:08,870
hypothetical

00:11:06,710 --> 00:11:11,720
thing it's like real is out there and

00:11:08,870 --> 00:11:12,950
then if you know you probably don't even

00:11:11,720 --> 00:11:14,510
realize that you use some of these

00:11:12,950 --> 00:11:16,340
services so if you took an uber here

00:11:14,510 --> 00:11:20,320
from the airport to the hotel you you

00:11:16,340 --> 00:11:20,320
know you're a user of Apache mazes right

00:11:21,370 --> 00:11:25,520
so kind of gave you a little intro to

00:11:24,440 --> 00:11:29,750
mazes

00:11:25,520 --> 00:11:31,850
now one cool feature that mazes has is a

00:11:29,750 --> 00:11:34,220
thing called frameworks and what a

00:11:31,850 --> 00:11:36,700
framework does is it allows you to

00:11:34,220 --> 00:11:41,630
schedule a task based on your specific

00:11:36,700 --> 00:11:43,640
applications needs so how a framework is

00:11:41,630 --> 00:11:45,860
implemented it's really in two pieces so

00:11:43,640 --> 00:11:48,230
there's a scheduling piece and then

00:11:45,860 --> 00:11:50,180
there's the work piece so that

00:11:48,230 --> 00:11:53,150
scheduling piece is called the scheduler

00:11:50,180 --> 00:11:56,200
ironically enough right that scheduler

00:11:53,150 --> 00:11:59,570
can accept them to deny resources and

00:11:56,200 --> 00:12:00,860
based on what's offered the based on the

00:11:59,570 --> 00:12:02,390
resources that are offered up to the

00:12:00,860 --> 00:12:03,950
scheduler will they either decide to

00:12:02,390 --> 00:12:06,230
take some of that resource and deploy

00:12:03,950 --> 00:12:08,570
the application or decide to pass and do

00:12:06,230 --> 00:12:12,260
something else and when it decides to

00:12:08,570 --> 00:12:14,180
deploy that application that application

00:12:12,260 --> 00:12:15,800
or your application container gets

00:12:14,180 --> 00:12:17,960
deployed in the form of an executor

00:12:15,800 --> 00:12:20,210
which is that second component so two

00:12:17,960 --> 00:12:23,450
components scheduler and executor and

00:12:20,210 --> 00:12:24,680
what that really that kind of little

00:12:23,450 --> 00:12:27,050
those two components that make a

00:12:24,680 --> 00:12:29,150
framework are actually what it is is an

00:12:27,050 --> 00:12:31,250
implementation of the offer accept model

00:12:29,150 --> 00:12:34,030
and we're going to go to go into that a

00:12:31,250 --> 00:12:36,680
little you know the next slide here but

00:12:34,030 --> 00:12:39,950
what a framework does just to kind of

00:12:36,680 --> 00:12:42,980
really nail the point is a framework is

00:12:39,950 --> 00:12:44,540
a very specialized it's like tightly

00:12:42,980 --> 00:12:46,790
super tightly coupled to your

00:12:44,540 --> 00:12:49,100
applications so tightly coupled that if

00:12:46,790 --> 00:12:51,680
you your framework if you wanted to do

00:12:49,100 --> 00:12:53,210
something beyond deploy your application

00:12:51,680 --> 00:12:54,890
and configure your application you could

00:12:53,210 --> 00:12:56,360
do things like monitor your application

00:12:54,890 --> 00:12:57,860
right so if you wanted to do health

00:12:56,360 --> 00:13:00,500
checks and stuff like that they're very

00:12:57,860 --> 00:13:02,720
specific to your app so as an example

00:13:00,500 --> 00:13:04,550
maybe you have a Postgres database and

00:13:02,720 --> 00:13:06,530
you want to actually check to make sure

00:13:04,550 --> 00:13:08,090
that your database is available like one

00:13:06,530 --> 00:13:09,740
of your database is under Postgres you

00:13:08,090 --> 00:13:14,720
could implement a health check that does

00:13:09,740 --> 00:13:16,820
something like that so a framework gives

00:13:14,720 --> 00:13:18,320
you that you know that specificity to

00:13:16,820 --> 00:13:20,410
your app now if you didn't want to use a

00:13:18,320 --> 00:13:23,379
framework who here is no swimmer

00:13:20,410 --> 00:13:25,600
Shawna's for mezzos okay so marathon

00:13:23,379 --> 00:13:27,639
itself is a framework so most people

00:13:25,600 --> 00:13:29,319
don't realize that but what marathon

00:13:27,639 --> 00:13:32,079
provides you instead of having that very

00:13:29,319 --> 00:13:34,180
specific framework for your application

00:13:32,079 --> 00:13:37,149
what marathon does is it's a generic

00:13:34,180 --> 00:13:37,810
provider to launch containers and that's

00:13:37,149 --> 00:13:40,949
all it is

00:13:37,810 --> 00:13:43,839
but that provides you kind of like a

00:13:40,949 --> 00:13:45,040
kind of like a catch-all generic way of

00:13:43,839 --> 00:13:46,629
deploying a container whereas a

00:13:45,040 --> 00:13:48,189
framework is you know giving something

00:13:46,629 --> 00:13:50,379
very specific so that's kind of like the

00:13:48,189 --> 00:13:54,329
difference between what marathon does

00:13:50,379 --> 00:13:54,329
and what a like a custom framework does

00:13:54,870 --> 00:14:02,860
get done so

00:13:59,939 --> 00:14:05,740
so we talked frameworks and we talked

00:14:02,860 --> 00:14:07,899
the offer accept model so this is

00:14:05,740 --> 00:14:11,949
actually so this was taken straight off

00:14:07,899 --> 00:14:14,050
of the mazes patchy mazes website this

00:14:11,949 --> 00:14:15,310
actually is just a visual representation

00:14:14,050 --> 00:14:16,990
of what the offer accept model looks

00:14:15,310 --> 00:14:19,509
like and what the actual workflow for

00:14:16,990 --> 00:14:21,040
framework is I'm kind of calling it out

00:14:19,509 --> 00:14:22,720
here because towards the end we're

00:14:21,040 --> 00:14:25,660
actually going to deploy a framework so

00:14:22,720 --> 00:14:26,949
it's just going to you know just to have

00:14:25,660 --> 00:14:30,610
an understanding of what's actually

00:14:26,949 --> 00:14:32,230
happening behind the scenes so the way

00:14:30,610 --> 00:14:33,819
the offer accept model is is all the

00:14:32,230 --> 00:14:38,699
compute nodes which is the number one

00:14:33,819 --> 00:14:42,279
box down there and the lower-left yes is

00:14:38,699 --> 00:14:43,569
a all their compute nodes up there down

00:14:42,279 --> 00:14:45,279
at the bottom there what they do is they

00:14:43,569 --> 00:14:47,980
offer up all the resources so if like

00:14:45,279 --> 00:14:50,380
agent node one says I have four CPUs and

00:14:47,980 --> 00:14:52,600
four gigs of memory it offers it up to

00:14:50,380 --> 00:14:54,699
the mazes master which is effectively

00:14:52,600 --> 00:14:56,139
just a control plane that you know

00:14:54,699 --> 00:14:57,060
manages all the messages and stuff

00:14:56,139 --> 00:15:01,360
that's going on

00:14:57,060 --> 00:15:03,040
it made us master then in turn offers up

00:15:01,360 --> 00:15:04,240
those resources to all the frameworks

00:15:03,040 --> 00:15:06,939
that are out there which is that number

00:15:04,240 --> 00:15:08,170
two box and though the frameworks that

00:15:06,939 --> 00:15:11,880
are sitting out there so you could have

00:15:08,170 --> 00:15:14,350
multiple frameworks what they do is they

00:15:11,880 --> 00:15:15,939
see the resources that are available on

00:15:14,350 --> 00:15:19,480
all the various pieces of compute and

00:15:15,939 --> 00:15:23,459
they decide to either deny it or consume

00:15:19,480 --> 00:15:25,839
some of those resources so in if the

00:15:23,459 --> 00:15:27,850
framework in the form of that scheduler

00:15:25,839 --> 00:15:30,490
says I want to actually schedule work

00:15:27,850 --> 00:15:33,040
it'll say I want two CPUs and two gigs

00:15:30,490 --> 00:15:33,649
of memory out of that cool on like agent

00:15:33,040 --> 00:15:35,990
one

00:15:33,649 --> 00:15:37,430
and when it says I want to see PS and 2

00:15:35,990 --> 00:15:39,050
gigs of memory that's what number 3 box

00:15:37,430 --> 00:15:40,639
it's going to say this is what I want to

00:15:39,050 --> 00:15:42,920
run and this is how much resource I'm

00:15:40,639 --> 00:15:44,959
going to consume in order to run it so

00:15:42,920 --> 00:15:47,930
that's at number 3 and it sends it back

00:15:44,959 --> 00:15:49,699
to the Maysles master and it's just

00:15:47,930 --> 00:15:51,559
effectively just going to be a control

00:15:49,699 --> 00:15:53,480
plane thing where it's going to go to

00:15:51,559 --> 00:15:56,329
that agent node which is recited as

00:15:53,480 --> 00:15:58,339
agent 1 and said I want you to launch

00:15:56,329 --> 00:16:00,740
this container you're going to consume

00:15:58,339 --> 00:16:03,019
this much memory and you know have at it

00:16:00,740 --> 00:16:04,819
and that's that number 4 bucks so when

00:16:03,019 --> 00:16:06,740
it actually when it hit that number 4

00:16:04,819 --> 00:16:08,779
hits the agent node your container gets

00:16:06,740 --> 00:16:10,699
spun up it consumes resources and that's

00:16:08,779 --> 00:16:11,929
how it knows what resources are

00:16:10,699 --> 00:16:13,850
available at the end right because the

00:16:11,929 --> 00:16:15,889
next time that agent node offers up that

00:16:13,850 --> 00:16:17,149
resource it's going to be 2 CPUs and 2

00:16:15,889 --> 00:16:20,360
gigs of memory less than what it

00:16:17,149 --> 00:16:22,189
currently has right so this is basically

00:16:20,360 --> 00:16:24,829
just an encapsulation of what the

00:16:22,189 --> 00:16:27,019
offered set model looks like and kind of

00:16:24,829 --> 00:16:29,689
what and what the framework

00:16:27,019 --> 00:16:36,579
implementation actually is and for a

00:16:29,689 --> 00:16:36,579
Bezos framework questions

00:16:46,720 --> 00:16:53,920
you can actually do it either way oh

00:16:49,680 --> 00:16:55,990
yeah so when you launch the task in the

00:16:53,920 --> 00:16:59,079
form of that executor does the executor

00:16:55,990 --> 00:17:00,759
talk directly to the scheduler or does

00:16:59,079 --> 00:17:01,750
it talk through the master and you can

00:17:00,759 --> 00:17:03,160
actually do either way there's a

00:17:01,750 --> 00:17:04,720
mechanism for doing either way there's

00:17:03,160 --> 00:17:07,750
advantages and disadvantages of doing

00:17:04,720 --> 00:17:11,939
either if you want let's talk afterwards

00:17:07,750 --> 00:17:14,709
that's a kind of a complex question so

00:17:11,939 --> 00:17:17,740
so we talked you know gave you a brief

00:17:14,709 --> 00:17:23,409
background on software based storage we

00:17:17,740 --> 00:17:24,640
talked schedulers and maze O's so this

00:17:23,409 --> 00:17:26,890
was about a year ago I had this idea

00:17:24,640 --> 00:17:28,329
where what like what happens if we

00:17:26,890 --> 00:17:33,100
create a software based storage

00:17:28,329 --> 00:17:36,309
framework and I built built it I first

00:17:33,100 --> 00:17:38,350
released it in September 2016 it's now

00:17:36,309 --> 00:17:40,960
on version zero three one it's open

00:17:38,350 --> 00:17:47,110
source it's available on github the URLs

00:17:40,960 --> 00:17:48,669
right there yeah and kind of kind of the

00:17:47,110 --> 00:17:52,600
idea we'll go into the idea a little bit

00:17:48,669 --> 00:17:55,150
in the next couple slides down but this

00:17:52,600 --> 00:17:58,260
framework what it does it in a deploy

00:17:55,150 --> 00:18:02,080
scale IO and manages scale IO and it's a

00:17:58,260 --> 00:18:05,830
software based storage platform and talk

00:18:02,080 --> 00:18:07,900
about it a little bit so so scale IO is

00:18:05,830 --> 00:18:10,659
a software based storage platform it is

00:18:07,900 --> 00:18:13,630
scale out block storage so what's

00:18:10,659 --> 00:18:15,070
interesting about scale IO is that you

00:18:13,630 --> 00:18:17,110
it's all software based that's all you

00:18:15,070 --> 00:18:18,850
install rpms you install debs on however

00:18:17,110 --> 00:18:20,770
many nodes you can do it in the

00:18:18,850 --> 00:18:24,730
hyper-converged configuration or a

00:18:20,770 --> 00:18:27,520
two-tier can we get configuration as you

00:18:24,730 --> 00:18:30,460
add nodes because it's all software

00:18:27,520 --> 00:18:32,620
based scale IO and like them like the

00:18:30,460 --> 00:18:34,630
metadata manager automatically knows

00:18:32,620 --> 00:18:36,789
that it needs to rebalance the data and

00:18:34,630 --> 00:18:39,280
if you take nodes out for maintenance or

00:18:36,789 --> 00:18:40,929
you have a hardware failure any data

00:18:39,280 --> 00:18:43,510
that was on that note that's missing it

00:18:40,929 --> 00:18:45,039
will get automatically get rebalanced so

00:18:43,510 --> 00:18:46,570
why is that kind of cool it's because

00:18:45,039 --> 00:18:49,030
all the maintenance operations are

00:18:46,570 --> 00:18:51,039
completely taken care of for you there's

00:18:49,030 --> 00:18:54,640
no user intervention to make that happen

00:18:51,039 --> 00:18:56,020
and and yeah and if you need to take

00:18:54,640 --> 00:18:57,220
stuff out for maintenance or hardware

00:18:56,020 --> 00:18:58,340
failures you don't really have to think

00:18:57,220 --> 00:19:01,190
about it

00:18:58,340 --> 00:19:02,900
and it also has an elastic architecture

00:19:01,190 --> 00:19:06,500
so if you need more eye ops you can

00:19:02,900 --> 00:19:09,260
throw more nodes at it and it will you

00:19:06,500 --> 00:19:10,850
can instead of like going through a

00:19:09,260 --> 00:19:12,580
traditional storage array where you have

00:19:10,850 --> 00:19:15,590
one controller that you're going through

00:19:12,580 --> 00:19:17,660
scale i/o when you as a consumer of a

00:19:15,590 --> 00:19:19,640
volume from scale i/o what it does is

00:19:17,660 --> 00:19:21,440
actually stripe the data from end nodes

00:19:19,640 --> 00:19:22,880
all at once which is kind of cool so

00:19:21,440 --> 00:19:26,570
that's how you get the as you add more

00:19:22,880 --> 00:19:28,960
nodes your eye ops increase so yeah the

00:19:26,570 --> 00:19:33,830
link fork try it it's a free download

00:19:28,960 --> 00:19:35,810
yeah give it a try there so the

00:19:33,830 --> 00:19:39,170
interesting thing is all the stuff that

00:19:35,810 --> 00:19:40,940
I'm kind of describing right now it can

00:19:39,170 --> 00:19:42,620
be inherently done with any software

00:19:40,940 --> 00:19:45,020
based storage platform that's just one

00:19:42,620 --> 00:19:46,370
that I happen to implement with there

00:19:45,020 --> 00:19:49,670
are plenty of other ones that are out

00:19:46,370 --> 00:19:51,800
there so I believe rancher has a like a

00:19:49,670 --> 00:19:53,990
competing product too that they just

00:19:51,800 --> 00:19:56,210
released I think at docker con I believe

00:19:53,990 --> 00:19:57,920
was their announcement for it so yeah it

00:19:56,210 --> 00:20:01,100
in the end what I'm saying is any

00:19:57,920 --> 00:20:02,840
software based storage framework you

00:20:01,100 --> 00:20:07,840
could build something like that and have

00:20:02,840 --> 00:20:10,430
the advantages of something like this so

00:20:07,840 --> 00:20:12,110
so yeah so if you deploy this software

00:20:10,430 --> 00:20:14,830
based storage framework what ends up

00:20:12,110 --> 00:20:18,500
happening is if you had a pure mais O's

00:20:14,830 --> 00:20:20,810
you know like highly available three

00:20:18,500 --> 00:20:23,110
node H a cluster and you have various

00:20:20,810 --> 00:20:26,810
pieces of compute that are underneath

00:20:23,110 --> 00:20:29,750
when you deploy the framework it will

00:20:26,810 --> 00:20:33,830
imprint every single agent node with the

00:20:29,750 --> 00:20:36,700
ability to provision and consume storage

00:20:33,830 --> 00:20:39,980
your software based storage platform and

00:20:36,700 --> 00:20:42,320
why it's so why is that cool well first

00:20:39,980 --> 00:20:43,640
if it's imprinted on every single one of

00:20:42,320 --> 00:20:46,760
your agent nodes you can provision

00:20:43,640 --> 00:20:48,560
storage from one and then also have that

00:20:46,760 --> 00:20:50,750
storage in the case of like failover

00:20:48,560 --> 00:20:52,700
when you that your work gets rescheduled

00:20:50,750 --> 00:20:54,620
on it to another piece like another

00:20:52,700 --> 00:20:57,110
piece of compute you could reattach that

00:20:54,620 --> 00:20:58,550
volume to that other node so that all

00:20:57,110 --> 00:21:01,100
you have all your data available right

00:20:58,550 --> 00:21:03,290
so what I basically just described as

00:21:01,100 --> 00:21:06,830
high high availability for containers

00:21:03,290 --> 00:21:08,730
right and it's also if you think about

00:21:06,830 --> 00:21:11,040
it because you have this framework it

00:21:08,730 --> 00:21:14,070
when you bring new mais au s-- agent

00:21:11,040 --> 00:21:17,070
nodes up and that mate so the agent

00:21:14,070 --> 00:21:20,430
nodes get gets registered to your maze

00:21:17,070 --> 00:21:23,030
owes master cluster the instant that it

00:21:20,430 --> 00:21:26,190
registers its resources to that of

00:21:23,030 --> 00:21:28,230
tomatoes that agent will send up its

00:21:26,190 --> 00:21:29,730
resources up to that framework that

00:21:28,230 --> 00:21:33,390
software based storage framework and

00:21:29,730 --> 00:21:35,160
then immediately imprint that node with

00:21:33,390 --> 00:21:39,270
the ability to access that software

00:21:35,160 --> 00:21:41,130
based storage platform so because

00:21:39,270 --> 00:21:42,990
everything software based and then put

00:21:41,130 --> 00:21:44,610
in in this case we're talking scale i/o

00:21:42,990 --> 00:21:46,230
because all the maintenance operations

00:21:44,610 --> 00:21:49,290
are handled for you and it can scale out

00:21:46,230 --> 00:21:50,940
linearly like you don't have these like

00:21:49,290 --> 00:21:53,040
this weird operational complexity of

00:21:50,940 --> 00:21:54,720
having you know a storage array where

00:21:53,040 --> 00:21:56,520
you have to worry about failed disks and

00:21:54,720 --> 00:21:58,290
this that and the other right if you

00:21:56,520 --> 00:22:00,690
have a hardware failure bring the no

00:21:58,290 --> 00:22:02,910
doubt it'll automatically rebalance once

00:22:00,690 --> 00:22:04,710
you fix your your note of whether it's a

00:22:02,910 --> 00:22:08,150
hard drive failure or not then you bring

00:22:04,710 --> 00:22:10,710
it you reintroduce it back in and

00:22:08,150 --> 00:22:13,740
because it's all RPM and it's dead based

00:22:10,710 --> 00:22:15,770
like RPM dev or whatever platform it is

00:22:13,740 --> 00:22:18,330
it's you can deploy this anywhere it's

00:22:15,770 --> 00:22:19,890
completely platform agnostic if you want

00:22:18,330 --> 00:22:21,750
to run this on bare metal go for it if

00:22:19,890 --> 00:22:24,240
you want to run this on hypervisor like

00:22:21,750 --> 00:22:26,160
vmware if you want to run this in AWS

00:22:24,240 --> 00:22:28,200
which is what the demo is going to be or

00:22:26,160 --> 00:22:30,540
that you can deploy that native just as

00:22:28,200 --> 00:22:36,810
well as your or whatever it supports

00:22:30,540 --> 00:22:42,240
windows so yeah so like why do we care

00:22:36,810 --> 00:22:44,190
so if you look at so you know but you

00:22:42,240 --> 00:22:45,570
know bike containers nature if you look

00:22:44,190 --> 00:22:46,980
at what they are they are ephemeral

00:22:45,570 --> 00:22:48,900
right you bring a container up you bring

00:22:46,980 --> 00:22:51,780
it down any data that was written to it

00:22:48,900 --> 00:22:55,590
is gone but if you look at this was

00:22:51,780 --> 00:22:57,060
actually a snapshot of docker hub if you

00:22:55,590 --> 00:22:59,190
look at docker hub today and you just go

00:22:57,060 --> 00:23:01,050
browse it sorts it by popularity like

00:22:59,190 --> 00:23:02,220
most use containers on docker hub and if

00:23:01,050 --> 00:23:04,080
you actually look at the containers that

00:23:02,220 --> 00:23:05,970
are there 10 out of 20 of them are

00:23:04,080 --> 00:23:09,230
actually stateful so if you look at

00:23:05,970 --> 00:23:12,480
there's Postgres MongoDB elasticsearch

00:23:09,230 --> 00:23:15,060
Cassandra WordPress I even think is up

00:23:12,480 --> 00:23:18,060
there but they're all applications that

00:23:15,060 --> 00:23:19,950
have state and so what happens when you

00:23:18,060 --> 00:23:21,210
have a stateful application that's

00:23:19,950 --> 00:23:22,020
running in a container that is

00:23:21,210 --> 00:23:26,429
inherently state

00:23:22,020 --> 00:23:29,790
right so traditionally when that happens

00:23:26,429 --> 00:23:31,860
if you spun up Postgres you write your

00:23:29,790 --> 00:23:33,480
data to your contained within your

00:23:31,860 --> 00:23:35,850
container and you bring Postgres down

00:23:33,480 --> 00:23:39,870
you've lost your entire database it's

00:23:35,850 --> 00:23:41,670
just gone like what a lot of the

00:23:39,870 --> 00:23:44,900
container schedulers did they recognized

00:23:41,670 --> 00:23:48,780
so including so you know kubernetes

00:23:44,900 --> 00:23:50,160
mazes and swarm and docker what they

00:23:48,780 --> 00:23:52,290
realize is that you're obviously running

00:23:50,160 --> 00:23:54,900
this your container on a particular

00:23:52,290 --> 00:23:56,700
piece of compute there is usually a you

00:23:54,900 --> 00:23:59,790
know direct-attached disk to that piece

00:23:56,700 --> 00:24:02,490
of compute so let's just go ahead and

00:23:59,790 --> 00:24:03,840
rewrite our reroute the data so that

00:24:02,490 --> 00:24:06,030
when we're writing to our Postgres

00:24:03,840 --> 00:24:08,160
database we write to a local disk

00:24:06,030 --> 00:24:09,690
instead of within the container itself

00:24:08,160 --> 00:24:12,179
so that when you bring the container

00:24:09,690 --> 00:24:15,900
down you bring it back up it can

00:24:12,179 --> 00:24:18,630
reattach and remount a you know amount

00:24:15,900 --> 00:24:20,429
on the local disk to your container so

00:24:18,630 --> 00:24:22,860
that your Postgres database can reattach

00:24:20,429 --> 00:24:25,410
and then you have all your data and have

00:24:22,860 --> 00:24:26,309
your database there now the problem is

00:24:25,410 --> 00:24:28,380
is that data

00:24:26,309 --> 00:24:30,270
there's no data locality right you're

00:24:28,380 --> 00:24:31,860
writing all your data to local disk and

00:24:30,270 --> 00:24:33,559
what happens if you have a hard drive

00:24:31,860 --> 00:24:36,510
failure what happens if you your

00:24:33,559 --> 00:24:38,370
motherboard on that system goes out what

00:24:36,510 --> 00:24:40,530
happens to your data well because it's

00:24:38,370 --> 00:24:42,840
all local to that particular piece of

00:24:40,530 --> 00:24:46,410
compute you've lost all your data so you

00:24:42,840 --> 00:24:48,240
kind of gum you know you've kind of won

00:24:46,410 --> 00:24:50,610
certain advantages of doing it like that

00:24:48,240 --> 00:24:53,429
but then you've also unfortunately it's

00:24:50,610 --> 00:24:55,140
all tied to that node and so kind of

00:24:53,429 --> 00:24:58,050
like what we've realized you know going

00:24:55,140 --> 00:25:00,510
way back is that if you want to have

00:24:58,050 --> 00:25:02,670
like a an application that's highly

00:25:00,510 --> 00:25:04,860
available you need to have that data

00:25:02,670 --> 00:25:10,530
live on some piece of external storage

00:25:04,860 --> 00:25:12,150
and yeah let's so yeah just need to have

00:25:10,530 --> 00:25:17,580
it I live you have your data live

00:25:12,150 --> 00:25:21,809
external to your computers and I kind of

00:25:17,580 --> 00:25:24,570
glossed over really quickly boat so V

00:25:21,809 --> 00:25:27,390
Sam and scale i/o they inherently do the

00:25:24,570 --> 00:25:30,110
same thing so what scale i/o does is it

00:25:27,390 --> 00:25:32,400
takes your direct-attached disks

00:25:30,110 --> 00:25:35,340
contributes them to a global pool and

00:25:32,400 --> 00:25:36,360
that's how that pool is that you know

00:25:35,340 --> 00:25:38,220
it's basically

00:25:36,360 --> 00:25:41,370
and stuff like that across that global

00:25:38,220 --> 00:25:43,080
pool and when you provision storage of

00:25:41,370 --> 00:25:45,059
provision storage or volumes out of that

00:25:43,080 --> 00:25:46,890
pool and because it's a globally

00:25:45,059 --> 00:25:49,320
accessible pool if you need to provision

00:25:46,890 --> 00:25:49,980
storage or move volumes from one node to

00:25:49,320 --> 00:25:52,610
the next

00:25:49,980 --> 00:25:56,910
that's how scale IO works in the backend

00:25:52,610 --> 00:25:59,549
and because it can be even though you're

00:25:56,910 --> 00:26:01,320
using local aggregated disks because

00:25:59,549 --> 00:26:04,350
it's accessible from every node

00:26:01,320 --> 00:26:06,990
it looks like external storage right

00:26:04,350 --> 00:26:12,020
even though it's using the local disks

00:26:06,990 --> 00:26:12,020
as a back-end to your storage platform

00:26:12,650 --> 00:26:15,650
yep

00:26:19,960 --> 00:26:27,070
so yeah it's not and actually I

00:26:24,610 --> 00:26:29,080
intentionally built the scale i/o

00:26:27,070 --> 00:26:31,299
framework some of the framework to

00:26:29,080 --> 00:26:33,159
install on bare metal because if it

00:26:31,299 --> 00:26:36,659
installs on bare metal it can install it

00:26:33,159 --> 00:26:36,659
in a VM and it can install in the cloud

00:26:38,279 --> 00:26:44,799
what's that it's not I kind of just

00:26:42,809 --> 00:26:46,779
intentionally glossed over it you can

00:26:44,799 --> 00:26:48,999
use any software based storage platform

00:26:46,779 --> 00:26:51,039
that's the one I happen to implement

00:26:48,999 --> 00:26:52,570
against like I said there are competing

00:26:51,039 --> 00:26:54,580
products like rancher they have their

00:26:52,570 --> 00:26:59,950
own software based storage platform I

00:26:54,580 --> 00:27:02,139
also believe I can't remember the other

00:26:59,950 --> 00:27:04,149
one off the top of my head but yeah it's

00:27:02,139 --> 00:27:07,029
it is free you can download it and try

00:27:04,149 --> 00:27:09,850
it it's not like some stripped-down

00:27:07,029 --> 00:27:11,350
version that's you know that's like

00:27:09,850 --> 00:27:18,129
throttled or anything like that it is

00:27:11,350 --> 00:27:20,529
the full version so what's that yeah

00:27:18,129 --> 00:27:23,049
that's the in the backend that's that's

00:27:20,529 --> 00:27:24,759
how scale ioad works right it's if you

00:27:23,049 --> 00:27:26,830
lose any one piece with no and what in

00:27:24,759 --> 00:27:29,019
one piece of compute with your disk

00:27:26,830 --> 00:27:30,669
that's there it's obviously replicated

00:27:29,019 --> 00:27:32,529
so your data sits on multiple nodes in

00:27:30,669 --> 00:27:35,639
order for it to be highly to be

00:27:32,529 --> 00:27:35,639
available so your data is available

00:27:44,150 --> 00:27:48,549
what

00:27:45,940 --> 00:27:50,500
I let's take that offline that's a very

00:27:48,549 --> 00:27:55,870
complicated question and I kind of want

00:27:50,500 --> 00:27:57,460
to get to the demo but so yeah so this

00:27:55,870 --> 00:28:00,039
framework installed a software based

00:27:57,460 --> 00:28:01,570
storage platform it also installs the

00:28:00,039 --> 00:28:03,370
following components for x-ray which is

00:28:01,570 --> 00:28:05,529
open source it's a vendor agnostic

00:28:03,370 --> 00:28:07,090
storage orchestration platform so what

00:28:05,529 --> 00:28:08,679
it does is if you have something like

00:28:07,090 --> 00:28:10,870
docker Amazo sand you want a provision

00:28:08,679 --> 00:28:12,730
storage this provides you the mechanism

00:28:10,870 --> 00:28:15,600
to provision that storage it supports

00:28:12,730 --> 00:28:19,240
AWS in the form of EBS volume ZFS

00:28:15,600 --> 00:28:21,519
digitalocean GCE and scale i/o so that's

00:28:19,240 --> 00:28:24,009
why it installs this and then mazes

00:28:21,519 --> 00:28:27,370
module DVD I for if you're not using

00:28:24,009 --> 00:28:29,049
docker workloads rex ray is implements

00:28:27,370 --> 00:28:30,909
the docker volume driver interface and

00:28:29,049 --> 00:28:33,039
so when you're deploying a docker

00:28:30,909 --> 00:28:35,080
container that requires external storage

00:28:33,039 --> 00:28:37,659
it goes through x-ray now may this

00:28:35,080 --> 00:28:40,840
module DVD I is a maze O's file system

00:28:37,659 --> 00:28:41,230
isolator and that basically you hook

00:28:40,840 --> 00:28:42,669
that up

00:28:41,230 --> 00:28:44,230
well the framework will automatically

00:28:42,669 --> 00:28:46,330
deploy and configure all this but what

00:28:44,230 --> 00:28:48,610
you gets hooked up the maze Osen on the

00:28:46,330 --> 00:28:51,549
agent node and it effectively just is a

00:28:48,610 --> 00:28:54,220
file system hook to call R x-ray to go

00:28:51,549 --> 00:28:55,990
ahead and provision a volume for non

00:28:54,220 --> 00:28:57,610
docker workloads so using the mazes

00:28:55,990 --> 00:28:59,950
container Iser and actually for that

00:28:57,610 --> 00:29:02,590
project that both open source the second

00:28:59,950 --> 00:29:05,350
one the mazes module DVD I it actually

00:29:02,590 --> 00:29:06,899
happened to be because it was open

00:29:05,350 --> 00:29:10,210
source and is actually quite successful

00:29:06,899 --> 00:29:12,190
mais au s-- itself is what it was

00:29:10,210 --> 00:29:13,659
actually pushed upstream and as of one o

00:29:12,190 --> 00:29:15,879
it's actually available in their their

00:29:13,659 --> 00:29:17,830
source tree so you just need to flip the

00:29:15,879 --> 00:29:19,779
bid on there to enable external storage

00:29:17,830 --> 00:29:24,700
it's listed as experimental but it's

00:29:19,779 --> 00:29:27,340
there so yeah what does this mean for

00:29:24,700 --> 00:29:29,230
your applications so if you have an

00:29:27,340 --> 00:29:31,840
application because you can provision

00:29:29,230 --> 00:29:33,250
and excess access the storage from one

00:29:31,840 --> 00:29:35,049
node and then move that access to

00:29:33,250 --> 00:29:37,210
another node it means your applications

00:29:35,049 --> 00:29:39,129
are highly available right piece of

00:29:37,210 --> 00:29:42,429
compute dies where you can bring it up

00:29:39,129 --> 00:29:43,870
and yeah you have your application come

00:29:42,429 --> 00:29:46,480
up on the other piece of compute and

00:29:43,870 --> 00:29:49,090
you'd have your data all available all

00:29:46,480 --> 00:29:50,230
right so take everything we've said and

00:29:49,090 --> 00:29:55,389
wrap it up and we throw it into the

00:29:50,230 --> 00:29:57,369
cloud so yeah what enables all this

00:29:55,389 --> 00:29:59,500
stuff to happen is all it's all about

00:29:57,369 --> 00:29:59,980
the api's right if you're in the cloud

00:29:59,500 --> 00:30:02,200
you have

00:29:59,980 --> 00:30:04,990
api's that are there you know through

00:30:02,200 --> 00:30:09,220
the form of Amazon AWS so we're going to

00:30:04,990 --> 00:30:10,750
show in a few seconds and for software

00:30:09,220 --> 00:30:12,520
based storage platforms they have api's

00:30:10,750 --> 00:30:19,330
right to manage the actual storage

00:30:12,520 --> 00:30:21,880
platform themselves so kind of where

00:30:19,330 --> 00:30:23,410
this is going is if we have a framework

00:30:21,880 --> 00:30:28,000
that deploys and configures in

00:30:23,410 --> 00:30:31,090
applications and then we have api's to

00:30:28,000 --> 00:30:33,030
manage and monitor the application we

00:30:31,090 --> 00:30:37,120
should be able to through this framework

00:30:33,030 --> 00:30:40,120
do a health and remediation on the

00:30:37,120 --> 00:30:41,799
storage platform so if you can do health

00:30:40,120 --> 00:30:44,679
and remediation that means when you run

00:30:41,799 --> 00:30:46,540
into trouble you should be able to fix

00:30:44,679 --> 00:30:49,360
your storage platform your software

00:30:46,540 --> 00:30:51,250
based storage platform so yeah

00:30:49,360 --> 00:30:53,110
johnny-five is live and then we're

00:30:51,250 --> 00:30:55,390
getting into you know the whole Skynet

00:30:53,110 --> 00:30:59,140
thing right so I'm going to do a demo in

00:30:55,390 --> 00:31:01,179
a few seconds if we take a source the

00:30:59,140 --> 00:31:02,559
storage based soft software based

00:31:01,179 --> 00:31:04,960
storage platform and move it into the

00:31:02,559 --> 00:31:07,150
cloud like AWS and then using the AWS

00:31:04,960 --> 00:31:10,270
SDK which is available in 10 language

00:31:07,150 --> 00:31:13,840
bindings that application should be able

00:31:10,270 --> 00:31:15,730
to do things like Auto scale instances

00:31:13,840 --> 00:31:17,440
it should be able to do stuff like dial

00:31:15,730 --> 00:31:19,570
in the number of eye ops required on

00:31:17,440 --> 00:31:21,940
your disks and then it should also be

00:31:19,570 --> 00:31:25,179
able to provision new hard drives and

00:31:21,940 --> 00:31:28,809
stuff in order to expand capacity so

00:31:25,179 --> 00:31:30,370
what I'm saying is a framework it's kind

00:31:28,809 --> 00:31:32,260
of cool if you have one that's very

00:31:30,370 --> 00:31:34,299
specific to your application it almost

00:31:32,260 --> 00:31:36,160
gives you the ability to like make your

00:31:34,299 --> 00:31:40,690
application do things that were like

00:31:36,160 --> 00:31:41,500
otherwise not intended and how much time

00:31:40,690 --> 00:31:43,660
do I have

00:31:41,500 --> 00:31:45,690
I got 20 minutes all right we're going

00:31:43,660 --> 00:31:49,809
to go we're going to do both demos today

00:31:45,690 --> 00:31:52,419
so the second half of the demo that I'm

00:31:49,809 --> 00:31:54,660
going to have is a that kind of the idea

00:31:52,419 --> 00:31:58,570
is is so I have a software based storage

00:31:54,660 --> 00:32:01,870
framework what we're going to do is

00:31:58,570 --> 00:32:04,000
we're going to have you know roll out

00:32:01,870 --> 00:32:06,910
this software based storage on all the

00:32:04,000 --> 00:32:11,710
nodes that are out there we're going to

00:32:06,910 --> 00:32:13,690
inject a crapload of data to make that

00:32:11,710 --> 00:32:16,000
software storage platform look

00:32:13,690 --> 00:32:18,250
like it's 98% full and who storage

00:32:16,000 --> 00:32:22,659
admins out here anybody storage admins

00:32:18,250 --> 00:32:25,029
no ok storage admins the dreaded like if

00:32:22,659 --> 00:32:26,559
I have a traditional box and I have a

00:32:25,029 --> 00:32:29,049
whole bunch of disks that are in there

00:32:26,559 --> 00:32:30,250
and people's you know you know I'm not

00:32:29,049 --> 00:32:32,110
managing it really well and it's

00:32:30,250 --> 00:32:33,610
starting to approach full that's like a

00:32:32,110 --> 00:32:34,750
nightmare scenario because as soon as

00:32:33,610 --> 00:32:36,519
the next thing you need to do is quickly

00:32:34,750 --> 00:32:38,379
add another shelf quickly add a whole

00:32:36,519 --> 00:32:40,000
bunch of hard drives in there then go to

00:32:38,379 --> 00:32:42,009
the storage array and say hey go

00:32:40,000 --> 00:32:43,600
rebalance the things so that I have you

00:32:42,009 --> 00:32:45,399
know I start striping the data cross on

00:32:43,600 --> 00:32:50,139
it so it's a huge operational nightmare

00:32:45,399 --> 00:32:52,149
and in this case what because the

00:32:50,139 --> 00:32:55,840
software based storage framework does

00:32:52,149 --> 00:32:58,509
the auto balancing and the framework

00:32:55,840 --> 00:33:00,340
itself can add new disks it can

00:32:58,509 --> 00:33:03,730
provision more storage expand the

00:33:00,340 --> 00:33:05,590
capacity of your storage pool and then

00:33:03,730 --> 00:33:07,750
give you more capacity so you're not

00:33:05,590 --> 00:33:13,899
full anymore and so actually that's all

00:33:07,750 --> 00:33:15,669
I'm going to demo configuration info the

00:33:13,899 --> 00:33:16,779
slides will be available afterwards so

00:33:15,669 --> 00:33:19,450
really quickly this is what's going to

00:33:16,779 --> 00:33:24,370
happen deploy the scheduler the offers

00:33:19,450 --> 00:33:25,990
go up to the maize O's cluster the

00:33:24,370 --> 00:33:27,669
scheduler then says I want some of that

00:33:25,990 --> 00:33:31,029
resource it's going to imprint the

00:33:27,669 --> 00:33:33,610
software based storage platform on to

00:33:31,029 --> 00:33:37,120
every piece of compute I'm going to

00:33:33,610 --> 00:33:39,340
inject full into the into every into the

00:33:37,120 --> 00:33:42,970
storage platform it's going to make a

00:33:39,340 --> 00:33:44,679
call to AWS to create EBS volumes it's

00:33:42,970 --> 00:33:47,259
going to send the volumes reattach them

00:33:44,679 --> 00:33:48,850
attach the new volumes to the software

00:33:47,259 --> 00:33:53,370
space storage platform and everything

00:33:48,850 --> 00:33:53,370
will be ok let's go ahead and do that I

00:33:54,419 --> 00:33:57,960
hope this works

00:34:00,530 --> 00:34:07,490
all right so here I have everything in

00:34:03,360 --> 00:34:07,490
AWS three master nodes three agent nodes

00:34:12,919 --> 00:34:20,310
and actually let's take note of a few

00:34:18,510 --> 00:34:21,690
things so here's the three node AJ

00:34:20,310 --> 00:34:28,230
configuration typical mezzos

00:34:21,690 --> 00:34:29,250
configuration I have three 180 gig disks

00:34:28,230 --> 00:34:30,990
that are already hooked into there

00:34:29,250 --> 00:34:34,649
because we need existing storage to

00:34:30,990 --> 00:34:36,419
create the storage platform notice there

00:34:34,649 --> 00:34:37,530
aren't any one terabyte disks those are

00:34:36,419 --> 00:34:39,540
going to get create it in a little bit

00:34:37,530 --> 00:34:40,890
and then this is what marathon looks

00:34:39,540 --> 00:34:41,970
like for those who haven't seen it

00:34:40,890 --> 00:34:44,760
before what we're going to do is we're

00:34:41,970 --> 00:34:46,980
going to curl JSON which basically says

00:34:44,760 --> 00:34:49,140
deploy my framework and it's going to go

00:34:46,980 --> 00:34:52,320
ahead and print the all the Maysles

00:34:49,140 --> 00:34:54,360
agent nodes with the with the software

00:34:52,320 --> 00:34:56,870
based storage platform let's do that

00:34:54,360 --> 00:34:56,870
really quickly

00:34:59,680 --> 00:35:13,090
I hope this works okay so the schedule

00:35:10,780 --> 00:35:17,590
is getting deployed I've been bedded a

00:35:13,090 --> 00:35:18,910
little UI in the into the scheduler in

00:35:17,590 --> 00:35:26,290
the form of a rest endpoint with a

00:35:18,910 --> 00:35:28,060
little UI on there so I'm going to go

00:35:26,290 --> 00:35:30,820
that UI right now and it basically gives

00:35:28,060 --> 00:35:32,500
you the state of what's going on with in

00:35:30,820 --> 00:35:34,120
those three executor nodes right those

00:35:32,500 --> 00:35:35,740
represent the three eight Maysles agent

00:35:34,120 --> 00:35:37,420
nodes that are out there so it's

00:35:35,740 --> 00:35:41,710
installing the scale i/o packages right

00:35:37,420 --> 00:35:43,780
now right

00:35:41,710 --> 00:35:46,290
so this it's just installing in this

00:35:43,780 --> 00:35:52,990
case it's rpms because we're on RedHat

00:35:46,290 --> 00:35:54,460
it's creating the cluster and that

00:35:52,990 --> 00:35:55,810
actually that first note on the top is

00:35:54,460 --> 00:35:58,720
all and that's actually creating it and

00:35:55,810 --> 00:36:01,590
then we're initializing the the plot

00:35:58,720 --> 00:36:01,590
Sorge platform

00:36:08,360 --> 00:36:14,000
right and we saw that they had we had

00:36:10,280 --> 00:36:15,680
three 180 gig disc EBS volumes so we're

00:36:14,000 --> 00:36:20,480
going to add those to the scale i/o

00:36:15,680 --> 00:36:22,040
storage pool and then hopefully we

00:36:20,480 --> 00:36:23,570
should see the capacity right once that

00:36:22,040 --> 00:36:26,150
gets added and the capacity should be

00:36:23,570 --> 00:36:28,700
there so that's how much capacity is

00:36:26,150 --> 00:36:30,950
available in our storage platform we're

00:36:28,700 --> 00:36:32,210
installing R x-ray and mazes module DVD

00:36:30,950 --> 00:36:34,040
I so those are the two open-source

00:36:32,210 --> 00:36:35,810
projects right Rex rays the docker

00:36:34,040 --> 00:36:38,120
volume drivers so for docker workloads

00:36:35,810 --> 00:36:39,920
and it installs mezzos module DVD I for

00:36:38,120 --> 00:36:42,520
non docker workloads for using the mazes

00:36:39,920 --> 00:36:42,520
container Iser

00:36:44,020 --> 00:36:50,360
and it's not going to need to reboot so

00:36:47,750 --> 00:36:53,000
right there so now everything's in the

00:36:50,360 --> 00:36:56,510
running state so starting with the pure

00:36:53,000 --> 00:36:59,120
mazes installation without having any

00:36:56,510 --> 00:37:01,850
storage stuff on there deploy the

00:36:59,120 --> 00:37:03,620
framework and imprints all the nodes

00:37:01,850 --> 00:37:05,600
your agent nodes so that you have you

00:37:03,620 --> 00:37:08,180
basically have access to provision

00:37:05,600 --> 00:37:11,090
volumes from a storage platform software

00:37:08,180 --> 00:37:13,160
based storage platform and then it's got

00:37:11,090 --> 00:37:17,260
that much capacity and because we have

00:37:13,160 --> 00:37:21,080
some time hopefully let's go ahead and

00:37:17,260 --> 00:37:21,970
provision a persistent application so go

00:37:21,080 --> 00:37:24,080
ahead

00:37:21,970 --> 00:37:29,570
so going to do another curl I'm going to

00:37:24,080 --> 00:37:32,170
deploy a go based HTTP server that's

00:37:29,570 --> 00:37:36,920
going to provision an external volume

00:37:32,170 --> 00:37:41,990
curl that right there go back it's a

00:37:36,920 --> 00:37:44,390
little web server so now it's running

00:37:41,990 --> 00:37:46,900
and what I did was just like in the

00:37:44,390 --> 00:37:49,550
framework case I embedded a little UI

00:37:46,900 --> 00:37:51,110
and notice that it's got the lawson

00:37:49,550 --> 00:37:54,110
sound so that we know that it's external

00:37:51,110 --> 00:37:56,420
storage I have a little data file that

00:37:54,110 --> 00:37:59,330
I'm just basically scribbling stuff in

00:37:56,420 --> 00:38:02,270
to notice that it keeps going up and up

00:37:59,330 --> 00:38:03,920
so if I go ahead and I stop this if I

00:38:02,270 --> 00:38:10,010
kill the app kill the container write

00:38:03,920 --> 00:38:11,840
this web server and now we notice that

00:38:10,010 --> 00:38:16,280
it's not there now if I go ahead and I

00:38:11,840 --> 00:38:17,600
redeploy it I did my job right we should

00:38:16,280 --> 00:38:18,980
see that all the data is still there and

00:38:17,600 --> 00:38:21,280
we keep appending to the end of the file

00:38:18,980 --> 00:38:21,280
right

00:38:25,650 --> 00:38:34,360
yay so yeah so here's a good example if

00:38:32,380 --> 00:38:36,880
I could have killed one of the nodes in

00:38:34,360 --> 00:38:38,920
AWS but I didn't want to risk it but if

00:38:36,880 --> 00:38:41,470
you had a node failure and on that

00:38:38,920 --> 00:38:42,760
wherever this HTTP server was running if

00:38:41,470 --> 00:38:44,320
you had a node failure that work would

00:38:42,760 --> 00:38:46,840
get rescheduled it would reattach that

00:38:44,320 --> 00:38:48,220
volume to that other node and then so

00:38:46,840 --> 00:38:49,660
when you looked at this you know looked

00:38:48,220 --> 00:38:51,070
at your data file you'd have all your

00:38:49,660 --> 00:38:53,590
data available so if this were something

00:38:51,070 --> 00:38:55,000
like a Postgres database right that's

00:38:53,590 --> 00:38:55,990
like some serious stuff like this with a

00:38:55,000 --> 00:38:58,000
little text file but if you had a

00:38:55,990 --> 00:38:59,770
Postgres database and you need you had a

00:38:58,000 --> 00:39:01,240
hardware failure or needed to reschedule

00:38:59,770 --> 00:39:02,680
to work for because of maintenance to

00:39:01,240 --> 00:39:04,000
pull the system out it would get

00:39:02,680 --> 00:39:07,960
rescheduled somewhere else and you have

00:39:04,000 --> 00:39:11,620
all your data available for it so get

00:39:07,960 --> 00:39:13,960
rid of that guy now let's go back to the

00:39:11,620 --> 00:39:15,040
scenario so this is the capacity that I

00:39:13,960 --> 00:39:16,750
have and what I'm going to do right now

00:39:15,040 --> 00:39:20,710
is I'm going to inject full and this is

00:39:16,750 --> 00:39:22,600
kind of why how a framework in the cloud

00:39:20,710 --> 00:39:24,340
can be super advantageous because you

00:39:22,600 --> 00:39:27,880
can start modifying the underlying

00:39:24,340 --> 00:39:32,020
infrastructure so I'm gonna do oops need

00:39:27,880 --> 00:39:34,980
to take note of this guy here because

00:39:32,020 --> 00:39:34,980
the endpoint is changed

00:39:35,810 --> 00:39:41,600
so anybody is familiar with clothes man

00:39:38,510 --> 00:39:44,650
you it's basically a way to post the

00:39:41,600 --> 00:39:46,460
rest call so header information the body

00:39:44,650 --> 00:39:48,860
acknowledge and this is how much fake

00:39:46,460 --> 00:39:49,730
data I'm going to inject and that

00:39:48,860 --> 00:39:54,380
actually worked

00:39:49,730 --> 00:39:56,450
yay so if we go back to well this is not

00:39:54,380 --> 00:39:58,610
going to update oh there we go so yeah

00:39:56,450 --> 00:40:00,380
so now it's not 98 percent used out of

00:39:58,610 --> 00:40:03,680
ninety percent so the threshold was

00:40:00,380 --> 00:40:06,170
ninety percent and this is the amount of

00:40:03,680 --> 00:40:12,380
fake data being used and if we go back

00:40:06,170 --> 00:40:14,980
to the console here AWS focus work in

00:40:12,380 --> 00:40:14,980
five minutes

00:40:15,070 --> 00:40:19,520
keep refreshing so now the storage

00:40:17,540 --> 00:40:23,240
platform knows that it's 98 percent full

00:40:19,520 --> 00:40:25,310
and hopefully it's on a pole right now

00:40:23,240 --> 00:40:28,640
so I think it's like 30 seconds or a

00:40:25,310 --> 00:40:31,160
minute okay so there's the first one the

00:40:28,640 --> 00:40:33,800
first one one one terabyte and if we

00:40:31,160 --> 00:40:35,450
keep refreshing it should create the

00:40:33,800 --> 00:40:38,200
second one and this notice the first

00:40:35,450 --> 00:40:38,200
ones already attached

00:40:38,800 --> 00:40:42,620
now it's creating the third one and it's

00:40:41,030 --> 00:40:46,060
tatting it to the each of the three

00:40:42,620 --> 00:40:46,060
maids those agent nodes that are there

00:40:46,810 --> 00:40:52,310
should be done now what's happening in

00:40:50,690 --> 00:40:54,350
the backend because we kind of said that

00:40:52,310 --> 00:40:56,240
software based storage platforms they

00:40:54,350 --> 00:40:58,640
all provide ap is that you can do to

00:40:56,240 --> 00:40:59,990
manipulate the underlying platform so

00:40:58,640 --> 00:41:01,670
what's happening in the backend is it's

00:40:59,990 --> 00:41:03,980
making a the framework the scheduling

00:41:01,670 --> 00:41:06,740
component of the framework is making API

00:41:03,980 --> 00:41:09,590
calls to create the EBS volumes but it's

00:41:06,740 --> 00:41:11,780
also that's controlling the backend the

00:41:09,590 --> 00:41:14,090
AWS side but it's also making API calls

00:41:11,780 --> 00:41:15,650
on the storage platform and to say hey

00:41:14,090 --> 00:41:18,170
take these three disks on these three

00:41:15,650 --> 00:41:21,290
nodes and add them into the storage pool

00:41:18,170 --> 00:41:23,020
and then once that storage gets the EBS

00:41:21,290 --> 00:41:25,310
volumes gets added to that storage pool

00:41:23,020 --> 00:41:27,620
we should hopefully see the capacity

00:41:25,310 --> 00:41:32,120
expand and then the percent used drop

00:41:27,620 --> 00:41:35,020
and this is on the polti and it makes a

00:41:32,120 --> 00:41:39,160
few API calls here and there and

00:41:35,020 --> 00:41:39,160
hopefully any second now

00:41:40,130 --> 00:41:44,210
and you I can take some questions and

00:41:42,470 --> 00:41:46,820
I'm sure it'll kick over in a second

00:41:44,210 --> 00:41:50,480
cross your fingers so there it is 14%

00:41:46,820 --> 00:41:52,190
you capacities expanded so yeah the

00:41:50,480 --> 00:41:55,100
framework itself just modified the

00:41:52,190 --> 00:41:56,890
underlying AWS stuff took the storage

00:41:55,100 --> 00:41:59,120
introduced into storage platform and

00:41:56,890 --> 00:42:01,970
recognized the you know that is full

00:41:59,120 --> 00:42:04,670
event and basically fixed itself right

00:42:01,970 --> 00:42:06,380
so that's kind of the idea of the talk

00:42:04,670 --> 00:42:08,420
is I've wanted to float this idea that

00:42:06,380 --> 00:42:10,100
using frameworks and stuff like that you

00:42:08,420 --> 00:42:11,720
could literally have applications that

00:42:10,100 --> 00:42:13,040
kind of like manipulate the

00:42:11,720 --> 00:42:17,110
infrastructure right out from underneath

00:42:13,040 --> 00:42:17,110
it so I thought it's a pretty cool idea

00:42:22,450 --> 00:42:26,720
it depends on the storage platform right

00:42:25,010 --> 00:42:28,790
and how they do the data striping across

00:42:26,720 --> 00:42:31,310
all the nodes in this case you're only

00:42:28,790 --> 00:42:34,760
going to ever get like it's about 50% is

00:42:31,310 --> 00:42:36,050
what your usable is but yeah like I said

00:42:34,760 --> 00:42:38,450
it depends on the stored platform and

00:42:36,050 --> 00:42:40,430
how they strike the data yeah and yeah

00:42:38,450 --> 00:42:41,750
but kind of a point is that this can be

00:42:40,430 --> 00:42:44,600
done with any software based storage

00:42:41,750 --> 00:42:47,920
platform right so it's just a cool idea

00:42:44,600 --> 00:42:47,920
for any who yep

00:42:50,650 --> 00:42:58,160
yes well it actually there's a bunch of

00:42:55,940 --> 00:42:59,900
things that happens so I picked three

00:42:58,160 --> 00:43:02,150
nodes three agent nodes because it

00:42:59,900 --> 00:43:04,519
requires a three node minimum right for

00:43:02,150 --> 00:43:07,069
quorum but all those three nodes get

00:43:04,519 --> 00:43:08,420
installed with a metadata manager so

00:43:07,069 --> 00:43:10,069
it's a primary secondary and a

00:43:08,420 --> 00:43:11,509
tiebreaker that's how it knows like what

00:43:10,069 --> 00:43:14,450
data needs to go where to in order to

00:43:11,509 --> 00:43:16,880
stripe it then some other packages that

00:43:14,450 --> 00:43:20,390
get installed as a server component

00:43:16,880 --> 00:43:22,220
which basically takes the devices that

00:43:20,390 --> 00:43:24,339
were introduced or that's a direct

00:43:22,220 --> 00:43:26,779
attach to that system and provides it to

00:43:24,339 --> 00:43:28,160
be consumed and then there's a client

00:43:26,779 --> 00:43:30,529
component that gets installed that

00:43:28,160 --> 00:43:38,630
consumes and is able to consume in

00:43:30,529 --> 00:43:47,229
provision storage out of it the Red Hat

00:43:38,630 --> 00:43:50,690
so 6 6 X 7 X Ubuntu 14 1404 1604 core OS

00:43:47,229 --> 00:43:54,220
I'm there's a huge list I go to the

00:43:50,690 --> 00:43:54,220
website and take a look yeah

00:43:55,059 --> 00:44:05,680
any other questions the live demo worked

00:44:00,519 --> 00:44:05,680
happy about that yeah

00:44:22,960 --> 00:44:28,790
so what I deployed here is effectively a

00:44:26,570 --> 00:44:32,210
hyper-converged scenario right is

00:44:28,790 --> 00:44:33,950
because I have three nodes here I am

00:44:32,210 --> 00:44:35,750
also providing and consuming the same

00:44:33,950 --> 00:44:37,760
storage on my pieces of compute there's

00:44:35,750 --> 00:44:40,490
other ways to install this if you wanted

00:44:37,760 --> 00:44:41,930
to in most stores like software storage

00:44:40,490 --> 00:44:44,060
platforms do this but if you want to

00:44:41,930 --> 00:44:45,440
have like a two-tiered type deployment

00:44:44,060 --> 00:44:47,960
you could have your software storage

00:44:45,440 --> 00:44:49,520
that kind of live out external like

00:44:47,960 --> 00:44:52,430
completely external to your mezzos

00:44:49,520 --> 00:44:54,800
configuration and then just and then

00:44:52,430 --> 00:44:56,210
your this like a framework a storage

00:44:54,800 --> 00:44:57,830
software based storage frame of what it

00:44:56,210 --> 00:44:59,570
would do instead of providing storage

00:44:57,830 --> 00:45:01,550
all it would do is consume storage and

00:44:59,570 --> 00:45:03,020
you just pointed externally and this

00:45:01,550 --> 00:45:08,270
framework actually does support that

00:45:03,020 --> 00:45:09,980
configuration - you can still like right

00:45:08,270 --> 00:45:11,780
now if I've decided to kill in one of

00:45:09,980 --> 00:45:13,490
these nodes right now like all my data

00:45:11,780 --> 00:45:16,010
would be available and if I wanted to

00:45:13,490 --> 00:45:17,360
add in a new node the framework the

00:45:16,010 --> 00:45:20,960
scheduling component of the framework

00:45:17,360 --> 00:45:23,780
would automatically imprint the NuForce

00:45:20,960 --> 00:45:25,460
node with scale IO and because scale ID

00:45:23,780 --> 00:45:26,810
does all the maintenance for you and the

00:45:25,460 --> 00:45:28,700
rebalance is everything it would

00:45:26,810 --> 00:45:30,680
rebalance all the data and you wouldn't

00:45:28,700 --> 00:45:31,970
know you know it would do it just

00:45:30,680 --> 00:45:32,930
basically take care of that for you

00:45:31,970 --> 00:45:36,010
wouldn't know that the maintenance that

00:45:32,930 --> 00:45:36,010
stuff was happening in the backend

00:45:43,190 --> 00:45:48,859
there is I don't so that's some of the

00:45:46,190 --> 00:45:52,400
stuff that I want to add into like the

00:45:48,859 --> 00:45:55,569
next version scale IO does have its own

00:45:52,400 --> 00:45:57,770
UI that's obviously way you know way

00:45:55,569 --> 00:46:00,650
tracks everything in the sharding and

00:45:57,770 --> 00:46:02,990
all that other stuff you can go and view

00:46:00,650 --> 00:46:05,030
it there so I think just like any other

00:46:02,990 --> 00:46:06,980
like storage platform would have some

00:46:05,030 --> 00:46:10,069
sort of mechanism for that but yet in

00:46:06,980 --> 00:46:12,200
the UI itself in this particular one in

00:46:10,069 --> 00:46:14,540
the framework UI it's not there but

00:46:12,200 --> 00:46:17,560
there is a scale out UI that does

00:46:14,540 --> 00:46:17,560
provide stuff like that

00:46:18,940 --> 00:46:24,950
any other questions

00:46:22,329 --> 00:46:27,750
cool oh I hope you found it interesting

00:46:24,950 --> 00:46:32,690
and entertaining so thank you guys

00:46:27,750 --> 00:46:32,690

YouTube URL: https://www.youtube.com/watch?v=ph8KlAXX49I


