Title: Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Geospatial
Description: 
	Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics
Dalton Lunga, PhD

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

With volumes of acquired remotely sensed imagery growing at an exponential rate, there is an ever-increasing burden of research and development challenges associated with processing this data at scale. In particular, the application of object detection models across large geographic areas predominantly faces three obstacles: (1) a lack of workflows for gathering representative training data and mitigating data bias, (2) the inability of current machine learning algorithms to generalize across diverse sensor and geographic conditions, and (3) the deployment and reuse of hundreds of unique models at scale. By considering the above challenges in a joint manner, we formulate and present an efficient, geographically agnostic framework for remote sensing imagery analysis at a global scale. The framework addresses the problem of bias-free data selection by mapping observed satellite images to a novel metric space rooted in the manifold geometry of the data itself, forming natural partitions of similar data. Using these partitions to seed training, the framework enables simpler, localized models to be developed; alleviating the challenge of generalization seen by more complex models for larger geographic extents. In an agile manner the framework further exploits the inherent parallelism for dataflow, and harnesses Apache Spark to implement distributed inference and training strategies which are seen to favorably scale. We discuss the challenges and merits of using Spark with current deep learning frameworks, providing insight into solutions developed for overall workflow harmonization. As a test case study, with no training data gathered for any entire country, we deploy the framework to detect buildings and roads, over areas that spans thousands of square kilometers and covered by 26TB of satellite image data. Drawing understanding from the results of this study, we finally present future directions which this exciting research may take.

Dalton is currently a research scientist in machine learning driven geospatial image analytics at ORNL. In this role he deploys machine learning and computer vision techniques in high performance computing environments, focusing on creating imagery-based data layers of interest to various societal problems e.g. enable accurate population distribution estimates and damage mapping for disaster management needs. He currently conducts research and development in machine learning techniques and advanced workflows for handling large volumes of geospatial data. Prior to ORNL, Dalton worked as machine learning research scientist at the council for scientific and industrial research in South Africa on a variety of projects. He received his PhD in electrical and computer engineering from Purdue University, West Lafayette, IN, US.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,240 --> 00:00:27,119
share

00:00:25,359 --> 00:00:29,039
some of the work that i've been doing at

00:00:27,119 --> 00:00:30,880
the lab as part of a team

00:00:29,039 --> 00:00:33,440
that that is looking at enabling large

00:00:30,880 --> 00:00:35,520
scale automated feature extraction

00:00:33,440 --> 00:00:38,800
you know to leverage high-performance

00:00:35,520 --> 00:00:40,399
computing as well as satellite imagery

00:00:38,800 --> 00:00:42,079
you know to deliver data sets that i

00:00:40,399 --> 00:00:45,039
country scale in help

00:00:42,079 --> 00:00:47,039
and support for our mission partners so

00:00:45,039 --> 00:00:49,680
in the original title i had

00:00:47,039 --> 00:00:50,079
uh deep learning inference as part of

00:00:49,680 --> 00:00:53,440
what

00:00:50,079 --> 00:00:55,199
um i had you know

00:00:53,440 --> 00:00:56,960
intended to speak about but i then i

00:00:55,199 --> 00:00:58,320
realized that the work that we're

00:00:56,960 --> 00:01:00,079
actually doing here

00:00:58,320 --> 00:01:02,160
there's another component i'll talk

00:01:00,079 --> 00:01:05,040
about extensively through my slides

00:01:02,160 --> 00:01:06,560
which will be about uh training so

00:01:05,040 --> 00:01:07,600
training of the deep learning models is

00:01:06,560 --> 00:01:10,720
also part of

00:01:07,600 --> 00:01:12,400
um what apache spark has helped us to be

00:01:10,720 --> 00:01:15,439
able to extend

00:01:12,400 --> 00:01:18,720
deep learning methods to do the work uh

00:01:15,439 --> 00:01:19,520
that we're doing so in terms of the

00:01:18,720 --> 00:01:21,920
outline

00:01:19,520 --> 00:01:23,280
i've got a few things uh the first point

00:01:21,920 --> 00:01:25,600
was i was

00:01:23,280 --> 00:01:27,680
going to go through the oranl overview

00:01:25,600 --> 00:01:30,960
but if you were part of this morning's

00:01:27,680 --> 00:01:33,439
keynote my colleague edmund begoli

00:01:30,960 --> 00:01:34,320
he did a great job in terms of working

00:01:33,439 --> 00:01:37,119
through

00:01:34,320 --> 00:01:37,520
uh some of the introduction of what rnl

00:01:37,119 --> 00:01:39,840
is

00:01:37,520 --> 00:01:41,520
so i'll skip that part and then i'll

00:01:39,840 --> 00:01:43,600
talk about the extensibility the need

00:01:41,520 --> 00:01:46,320
for accessibility and deep learning

00:01:43,600 --> 00:01:47,840
and also the new uh workflow that we've

00:01:46,320 --> 00:01:51,520
been working on for the past

00:01:47,840 --> 00:01:53,680
uh nine months

00:01:51,520 --> 00:01:55,360
where we've sort of like put together

00:01:53,680 --> 00:01:55,920
high performance computing machine

00:01:55,360 --> 00:01:58,640
learning

00:01:55,920 --> 00:02:00,719
but then brought on apache spark to be

00:01:58,640 --> 00:02:03,600
the enabler for us to be able to

00:02:00,719 --> 00:02:04,799
work with tens and tens of trained

00:02:03,600 --> 00:02:07,119
models

00:02:04,799 --> 00:02:08,560
to be able to do inference at scale and

00:02:07,119 --> 00:02:09,840
then i'll talk about the deployment of

00:02:08,560 --> 00:02:12,160
that workflow

00:02:09,840 --> 00:02:14,239
on two use cases where i'll share some

00:02:12,160 --> 00:02:18,319
results on building detection

00:02:14,239 --> 00:02:18,319
as well as rod road mapping

00:02:18,959 --> 00:02:22,879
so edmond already worked through this

00:02:21,120 --> 00:02:24,160
very similar picture about the orange

00:02:22,879 --> 00:02:24,560
camper so i'm not going to talk about

00:02:24,160 --> 00:02:27,040
that

00:02:24,560 --> 00:02:27,599
the only thing to highlight on this

00:02:27,040 --> 00:02:31,200
slide

00:02:27,599 --> 00:02:34,319
is that what i know is at the

00:02:31,200 --> 00:02:35,840
leading position in terms of pushing

00:02:34,319 --> 00:02:37,920
geospatial science

00:02:35,840 --> 00:02:39,920
and technologies so there's a growing

00:02:37,920 --> 00:02:40,720
capability that we are building tapping

00:02:39,920 --> 00:02:42,560
into

00:02:40,720 --> 00:02:44,640
artificial intelligence and leveraging

00:02:42,560 --> 00:02:47,920
high performance computing facilities

00:02:44,640 --> 00:02:50,560
to be able to address

00:02:47,920 --> 00:02:53,760
some of the hard challenges that we are

00:02:50,560 --> 00:02:53,760
facing as a community

00:02:54,319 --> 00:02:58,239
so this is again about the core mission

00:02:56,400 --> 00:03:01,519
of oranell we had a lot

00:02:58,239 --> 00:03:02,159
from edmund de gaulle and this is an

00:03:01,519 --> 00:03:04,159
example

00:03:02,159 --> 00:03:06,480
as all of the facilities edmond already

00:03:04,159 --> 00:03:08,959
walked through our summit and the next

00:03:06,480 --> 00:03:11,599
supercomputing uh facility that's coming

00:03:08,959 --> 00:03:11,599
up frontier

00:03:12,000 --> 00:03:19,599
so as i mentioned that rhino

00:03:15,360 --> 00:03:21,120
is a non-leader in geospatial science

00:03:19,599 --> 00:03:23,159
and technology

00:03:21,120 --> 00:03:24,799
the enabler to this is a

00:03:23,159 --> 00:03:26,480
multi-disciplinary team

00:03:24,799 --> 00:03:28,879
that is working across you know from the

00:03:26,480 --> 00:03:30,560
domain science people who are

00:03:28,879 --> 00:03:32,319
informed about the human geographic

00:03:30,560 --> 00:03:35,760
computing and then keep up

00:03:32,319 --> 00:03:39,120
human geography studies and

00:03:35,760 --> 00:03:40,799
people that have informed from the

00:03:39,120 --> 00:03:42,560
high performance computing side and

00:03:40,799 --> 00:03:43,840
people like myself with a background in

00:03:42,560 --> 00:03:44,959
machine learning and artificial

00:03:43,840 --> 00:03:47,519
intelligence

00:03:44,959 --> 00:03:48,959
and we all find ourselves working within

00:03:47,519 --> 00:03:51,120
a division

00:03:48,959 --> 00:03:53,040
uh where the big problems that are

00:03:51,120 --> 00:03:56,640
presented by our customers

00:03:53,040 --> 00:03:59,280
does require that each of us play a role

00:03:56,640 --> 00:04:00,319
in enabling a single solution to deliver

00:03:59,280 --> 00:04:02,239
data sets

00:04:00,319 --> 00:04:04,000
that are helpful for downstream

00:04:02,239 --> 00:04:06,799
applications

00:04:04,000 --> 00:04:07,760
so i already mentioned that i mean you

00:04:06,799 --> 00:04:09,360
know scalable

00:04:07,760 --> 00:04:12,400
and high performance computing is

00:04:09,360 --> 00:04:15,439
forming part of the solutions and

00:04:12,400 --> 00:04:18,880
the new thing that has come out of our

00:04:15,439 --> 00:04:19,440
uh this past year is the growing need

00:04:18,880 --> 00:04:22,560
for

00:04:19,440 --> 00:04:25,360
you know geo ai as

00:04:22,560 --> 00:04:25,680
a user capability that we are enabling

00:04:25,360 --> 00:04:28,960
and

00:04:25,680 --> 00:04:32,240
we are now growing this by pulling in

00:04:28,960 --> 00:04:33,600
some of some of the foundational aspects

00:04:32,240 --> 00:04:36,560
from machine learning

00:04:33,600 --> 00:04:37,280
but also tapping into uh some of the

00:04:36,560 --> 00:04:39,199
social

00:04:37,280 --> 00:04:40,320
science non-problems to be able to

00:04:39,199 --> 00:04:43,360
formulate

00:04:40,320 --> 00:04:47,040
solutions that are helpful for a variety

00:04:43,360 --> 00:04:47,040
of impact areas

00:04:48,320 --> 00:04:51,360
so today the case study that i'm going

00:04:50,479 --> 00:04:53,199
to talk about

00:04:51,360 --> 00:04:55,520
is about why do we need to have

00:04:53,199 --> 00:04:57,360
extensibility in deep learning and what

00:04:55,520 --> 00:04:58,960
what is this extensibility in deep

00:04:57,360 --> 00:05:02,000
learning that we're talking about

00:04:58,960 --> 00:05:05,680
so the motivation comes from uh

00:05:02,000 --> 00:05:08,639
you know that being able to

00:05:05,680 --> 00:05:09,360
to to derive solutions from remote

00:05:08,639 --> 00:05:11,440
sensing

00:05:09,360 --> 00:05:12,720
images for example it's something that

00:05:11,440 --> 00:05:15,520
has been known

00:05:12,720 --> 00:05:17,039
by the community for quite um you know a

00:05:15,520 --> 00:05:19,600
long time

00:05:17,039 --> 00:05:21,199
but with the images of you know the

00:05:19,600 --> 00:05:24,240
recent advances in in

00:05:21,199 --> 00:05:27,840
in deep learning machine learning and

00:05:24,240 --> 00:05:30,320
bringing on top uh in in the mix

00:05:27,840 --> 00:05:32,479
the high performance computing you know

00:05:30,320 --> 00:05:34,720
accelerators

00:05:32,479 --> 00:05:36,240
one thing that's been very encouraging

00:05:34,720 --> 00:05:38,639
is to see that

00:05:36,240 --> 00:05:39,280
we are now able to derive you know data

00:05:38,639 --> 00:05:42,560
sets

00:05:39,280 --> 00:05:45,280
that can be used to help uh you know

00:05:42,560 --> 00:05:46,800
things from population distributions

00:05:45,280 --> 00:05:48,639
infrastructure mapping

00:05:46,800 --> 00:05:51,120
you know in the case of disasters

00:05:48,639 --> 00:05:53,600
there's often a request to be able to

00:05:51,120 --> 00:05:55,600
you know provide data that can provide

00:05:53,600 --> 00:05:59,199
that can enable

00:05:55,600 --> 00:06:01,520
you know the field workers to assess

00:05:59,199 --> 00:06:02,319
you know the impact of the damage and

00:06:01,520 --> 00:06:05,520
what's often

00:06:02,319 --> 00:06:08,319
easier to do than going into those uh

00:06:05,520 --> 00:06:09,840
damaged areas is to have this overhead

00:06:08,319 --> 00:06:11,680
look so we can take advantage of the

00:06:09,840 --> 00:06:14,800
remote sensing images to provide

00:06:11,680 --> 00:06:15,840
insights of what was before the disaster

00:06:14,800 --> 00:06:18,240
and then

00:06:15,840 --> 00:06:20,560
how the disaster affected an area so

00:06:18,240 --> 00:06:21,759
through object detection we are able to

00:06:20,560 --> 00:06:25,600
build solutions

00:06:21,759 --> 00:06:28,560
that provide these data sets in time

00:06:25,600 --> 00:06:29,600
so high resolution images um you know

00:06:28,560 --> 00:06:32,960
from remote sensing

00:06:29,600 --> 00:06:35,039
as much as they are easily available

00:06:32,960 --> 00:06:36,880
you know and they are cost effective to

00:06:35,039 --> 00:06:40,400
enable these kinds of

00:06:36,880 --> 00:06:42,479
solutions they also do you know present

00:06:40,400 --> 00:06:44,560
a challenge that ought to be addressed

00:06:42,479 --> 00:06:47,600
if you have to leverage machine learning

00:06:44,560 --> 00:06:48,000
in particular one known thing is that to

00:06:47,600 --> 00:06:49,520
be able

00:06:48,000 --> 00:06:51,039
to come up with supervised machine

00:06:49,520 --> 00:06:54,160
learning methods

00:06:51,039 --> 00:06:56,639
we need to create labels for the data

00:06:54,160 --> 00:06:57,680
the data is known to extend over large

00:06:56,639 --> 00:07:00,639
geographic

00:06:57,680 --> 00:07:01,520
areas so it's very laborious and

00:07:00,639 --> 00:07:05,440
expensive

00:07:01,520 --> 00:07:08,479
to you know label each pixel

00:07:05,440 --> 00:07:10,720
you know one by one and be able to train

00:07:08,479 --> 00:07:12,479
the models and the other thing apart

00:07:10,720 --> 00:07:13,199
from the volume of the data is the fact

00:07:12,479 --> 00:07:15,599
that

00:07:13,199 --> 00:07:16,960
there's a lot of variety of you know

00:07:15,599 --> 00:07:19,520
different characterization

00:07:16,960 --> 00:07:20,880
of the objects that we encounter when we

00:07:19,520 --> 00:07:23,199
look at the data

00:07:20,880 --> 00:07:25,199
and all these challenges are known to

00:07:23,199 --> 00:07:25,840
limit how one can exploit machine

00:07:25,199 --> 00:07:27,919
learning

00:07:25,840 --> 00:07:29,840
so what we ought to do in terms of

00:07:27,919 --> 00:07:30,800
coming up with these methods that can

00:07:29,840 --> 00:07:33,039
generalize

00:07:30,800 --> 00:07:34,400
both in the special and the temporal

00:07:33,039 --> 00:07:36,880
dimension

00:07:34,400 --> 00:07:37,520
we've been looking at this problem for

00:07:36,880 --> 00:07:40,560
the past

00:07:37,520 --> 00:07:43,199
three years to try to come up with

00:07:40,560 --> 00:07:46,400
workflows and frameworks that can allow

00:07:43,199 --> 00:07:49,280
for us to either reuse the models or to

00:07:46,400 --> 00:07:50,560
limit the need for recreating the

00:07:49,280 --> 00:07:52,560
training labels

00:07:50,560 --> 00:07:54,160
whenever we encounter a need that

00:07:52,560 --> 00:07:55,199
requires that we do a large scale

00:07:54,160 --> 00:07:58,960
computing

00:07:55,199 --> 00:07:59,759
effort so another way to understand the

00:07:58,960 --> 00:08:02,479
problem that

00:07:59,759 --> 00:08:04,960
prohibits the the use of these machine

00:08:02,479 --> 00:08:06,960
learning algorithms is

00:08:04,960 --> 00:08:08,639
this limitation that we call uh the need

00:08:06,960 --> 00:08:10,879
for special temporal

00:08:08,639 --> 00:08:12,319
uh generalization and the picture that

00:08:10,879 --> 00:08:15,520
you looking at here

00:08:12,319 --> 00:08:18,720
is imagine you have two of

00:08:15,520 --> 00:08:20,240
two domains in one domain we call it the

00:08:18,720 --> 00:08:22,400
source domain

00:08:20,240 --> 00:08:23,520
you have a large amount of training

00:08:22,400 --> 00:08:25,680
labels

00:08:23,520 --> 00:08:27,120
and in another domain you don't have

00:08:25,680 --> 00:08:29,840
however what you're looking at

00:08:27,120 --> 00:08:30,560
on the target domain here it is the same

00:08:29,840 --> 00:08:33,039
picture

00:08:30,560 --> 00:08:34,159
but something has changed in between you

00:08:33,039 --> 00:08:35,839
know over time

00:08:34,159 --> 00:08:37,519
so what has changed is that on the

00:08:35,839 --> 00:08:40,719
target domain we now

00:08:37,519 --> 00:08:44,159
have satellite imagery that contains

00:08:40,719 --> 00:08:46,720
uh you know haze or the clouds

00:08:44,159 --> 00:08:47,920
and the introduction of these new things

00:08:46,720 --> 00:08:49,440
on a given location

00:08:47,920 --> 00:08:52,000
even though you still have access

00:08:49,440 --> 00:08:53,920
training labels is a non-problem

00:08:52,000 --> 00:08:55,200
in machine learning that causes what is

00:08:53,920 --> 00:08:59,040
uh

00:08:55,200 --> 00:09:00,640
known as distribution shifts so

00:08:59,040 --> 00:09:03,600
just to explain what the distribution

00:09:00,640 --> 00:09:04,959
shifts you know mean here is that

00:09:03,600 --> 00:09:07,200
when you estimate a model in the

00:09:04,959 --> 00:09:08,160
training domain you have a distribution

00:09:07,200 --> 00:09:10,800
of the objects

00:09:08,160 --> 00:09:11,760
you know different classes that you have

00:09:10,800 --> 00:09:13,360
and you're able to

00:09:11,760 --> 00:09:15,360
train a model you can deploy it at

00:09:13,360 --> 00:09:18,480
influence on the same distribution your

00:09:15,360 --> 00:09:20,000
thermo your performance is could be high

00:09:18,480 --> 00:09:22,640
could be 90

00:09:20,000 --> 00:09:24,000
but now due to the presence of these

00:09:22,640 --> 00:09:26,480
acquisition conditions

00:09:24,000 --> 00:09:27,680
that are in the middle that distribution

00:09:26,480 --> 00:09:29,680
gets shifted

00:09:27,680 --> 00:09:31,360
so we should mean that in the target

00:09:29,680 --> 00:09:32,800
domain when you seek to deploy the

00:09:31,360 --> 00:09:33,760
original model that you trained in your

00:09:32,800 --> 00:09:37,040
source domain

00:09:33,760 --> 00:09:40,000
it will not account for the same objects

00:09:37,040 --> 00:09:41,200
that you had in your training uh data so

00:09:40,000 --> 00:09:43,200
the question is

00:09:41,200 --> 00:09:44,240
how do we go about addressing this

00:09:43,200 --> 00:09:48,320
challenge so that we

00:09:44,240 --> 00:09:51,519
able to account for these shifts

00:09:48,320 --> 00:09:53,920
but then enable the reuse of the models

00:09:51,519 --> 00:09:56,040
so this is where we see you know that

00:09:53,920 --> 00:09:58,080
reduction in the need of having to

00:09:56,040 --> 00:09:59,600
re-label the data

00:09:58,080 --> 00:10:01,120
but take advantage of the already

00:09:59,600 --> 00:10:03,839
trained models

00:10:01,120 --> 00:10:05,600
and use them at scale so that's what i

00:10:03,839 --> 00:10:06,240
would talk about in terms of this case

00:10:05,600 --> 00:10:08,079
study

00:10:06,240 --> 00:10:10,160
and how we've put different things

00:10:08,079 --> 00:10:12,399
together including apache spark

00:10:10,160 --> 00:10:13,440
and some of the basic research uh

00:10:12,399 --> 00:10:14,959
approaches

00:10:13,440 --> 00:10:17,760
uh that we've been working on for the

00:10:14,959 --> 00:10:19,680
past three years

00:10:17,760 --> 00:10:21,519
so just to give you an idea of the scale

00:10:19,680 --> 00:10:21,920
and data volume so what we're looking at

00:10:21,519 --> 00:10:25,040
here

00:10:21,920 --> 00:10:26,720
is a country scale datasets the diff

00:10:25,040 --> 00:10:28,959
different colors that that you see in

00:10:26,720 --> 00:10:30,399
the blocks here so

00:10:28,959 --> 00:10:32,000
the country itself here is actually

00:10:30,399 --> 00:10:36,640
zambia and the different

00:10:32,000 --> 00:10:39,680
colors are blocks indicating this

00:10:36,640 --> 00:10:42,959
a collection of an image satellite image

00:10:39,680 --> 00:10:45,279
on a given area and the differences

00:10:42,959 --> 00:10:46,640
are the different times of acquiring

00:10:45,279 --> 00:10:50,000
these images

00:10:46,640 --> 00:10:53,600
so on a typical country data we handle

00:10:50,000 --> 00:10:55,680
about 15 to 120 terabytes of data

00:10:53,600 --> 00:10:56,880
just to be able to you know provide

00:10:55,680 --> 00:10:58,880
either a mapping

00:10:56,880 --> 00:11:00,160
of objects so when i say objects it

00:10:58,880 --> 00:11:02,399
could be a mapping of

00:11:00,160 --> 00:11:04,240
all the buildings in a given country or

00:11:02,399 --> 00:11:05,360
it could be a mapping of all the roads

00:11:04,240 --> 00:11:07,839
in a given country

00:11:05,360 --> 00:11:10,160
so we would have to you know approach it

00:11:07,839 --> 00:11:12,880
from a perspective where by

00:11:10,160 --> 00:11:13,760
we deal with data that then spans two

00:11:12,880 --> 00:11:17,440
times

00:11:13,760 --> 00:11:19,680
the land area size of a given country

00:11:17,440 --> 00:11:21,360
this is as a result of the overlapping

00:11:19,680 --> 00:11:24,880
uh nature of the satellite

00:11:21,360 --> 00:11:27,040
of image images that we deal with

00:11:24,880 --> 00:11:28,320
so another challenge to to highlight is

00:11:27,040 --> 00:11:31,279
the fact that often

00:11:28,320 --> 00:11:33,440
the objects that we are looking for they

00:11:31,279 --> 00:11:35,440
are unevenly distributed

00:11:33,440 --> 00:11:36,959
to the things that we're not interested

00:11:35,440 --> 00:11:38,800
interested in so for example

00:11:36,959 --> 00:11:40,000
the buildings or areas that contain

00:11:38,800 --> 00:11:44,160
building pixels

00:11:40,000 --> 00:11:46,959
in a given country is about two percent

00:11:44,160 --> 00:11:49,120
of you know the 183 million square

00:11:46,959 --> 00:11:52,160
kilometers uh for example for

00:11:49,120 --> 00:11:53,920
for for zambia which would mean that 98

00:11:52,160 --> 00:11:56,160
of the time the data that we are

00:11:53,920 --> 00:11:57,440
processing or that we are seeing

00:11:56,160 --> 00:11:59,279
that we are handling to be able to

00:11:57,440 --> 00:12:02,959
derive models contain

00:11:59,279 --> 00:12:06,320
objects of no interest to to to us

00:12:02,959 --> 00:12:09,040
so an analogy i would be you know

00:12:06,320 --> 00:12:10,079
having to solve this needle in a hastic

00:12:09,040 --> 00:12:13,279
problem

00:12:10,079 --> 00:12:15,920
so it is part of this imbalance of

00:12:13,279 --> 00:12:16,880
nature of the data that we often see

00:12:15,920 --> 00:12:19,839
machine learning

00:12:16,880 --> 00:12:21,519
algorithms failing to be deployed in

00:12:19,839 --> 00:12:26,320
different areas even though

00:12:21,519 --> 00:12:26,320
trained for the same class of objects

00:12:27,600 --> 00:12:30,720
so what we've come up with in the past

00:12:29,600 --> 00:12:34,079
eight to nine months

00:12:30,720 --> 00:12:36,839
is a workflow that we seek to address

00:12:34,079 --> 00:12:38,160
these special temporal generalization

00:12:36,839 --> 00:12:41,440
limitations

00:12:38,160 --> 00:12:42,560
and the idea here is that we approaching

00:12:41,440 --> 00:12:44,800
these

00:12:42,560 --> 00:12:45,680
from taking advantage of what we know to

00:12:44,800 --> 00:12:47,760
be the uh

00:12:45,680 --> 00:12:49,920
you know the the capabilities of deep

00:12:47,760 --> 00:12:52,240
convolutional neural networks

00:12:49,920 --> 00:12:53,440
and enabled by by high performance

00:12:52,240 --> 00:12:56,000
computing

00:12:53,440 --> 00:12:58,720
what we then said to do before we create

00:12:56,000 --> 00:13:00,240
our models was to take a step back

00:12:58,720 --> 00:13:01,760
in terms of understanding the

00:13:00,240 --> 00:13:04,880
characteristics of the data

00:13:01,760 --> 00:13:06,639
and to be able to do so we looked at the

00:13:04,880 --> 00:13:08,320
data from a manifold

00:13:06,639 --> 00:13:10,240
learning perspective and i'll talk about

00:13:08,320 --> 00:13:11,040
what this manifold learning perspective

00:13:10,240 --> 00:13:14,800
is

00:13:11,040 --> 00:13:17,680
and that then allows us you know

00:13:14,800 --> 00:13:19,360
to have this data-driven approach that

00:13:17,680 --> 00:13:22,160
helps us to create

00:13:19,360 --> 00:13:24,320
what we will see in the next slide as

00:13:22,160 --> 00:13:26,720
homogeneous data partitions

00:13:24,320 --> 00:13:28,320
so by introducing these homogeneous or

00:13:26,720 --> 00:13:30,800
homogeneity conditions

00:13:28,320 --> 00:13:31,760
in our partitions we can then take

00:13:30,800 --> 00:13:34,800
advantage of

00:13:31,760 --> 00:13:36,639
similar distributions and we anticipate

00:13:34,800 --> 00:13:38,399
you actually see similar distributions

00:13:36,639 --> 00:13:41,360
when we deploy the models

00:13:38,399 --> 00:13:42,800
so as a result there shouldn't be much

00:13:41,360 --> 00:13:46,079
of a distribution shift

00:13:42,800 --> 00:13:49,199
once we train a model and another key

00:13:46,079 --> 00:13:51,519
thing to highlight is that

00:13:49,199 --> 00:13:54,560
creating these partitions enables us to

00:13:51,519 --> 00:13:57,440
also gather training data in a bias-free

00:13:54,560 --> 00:13:59,600
manner because our analysts will then

00:13:57,440 --> 00:14:02,160
just focus on a single partition

00:13:59,600 --> 00:14:03,120
to acquire training labels that are

00:14:02,160 --> 00:14:06,160
going to inform

00:14:03,120 --> 00:14:08,720
that partition and what we see from this

00:14:06,160 --> 00:14:10,320
workflow is a convergence of two things

00:14:08,720 --> 00:14:10,959
but the training as well as the

00:14:10,320 --> 00:14:13,040
inference

00:14:10,959 --> 00:14:15,920
so once we train the models we can then

00:14:13,040 --> 00:14:16,959
reuse it reuse them using the same

00:14:15,920 --> 00:14:19,920
underlying

00:14:16,959 --> 00:14:21,680
framework for inference i mentioned in

00:14:19,920 --> 00:14:22,959
the previous two slides the need for

00:14:21,680 --> 00:14:25,199
using the models

00:14:22,959 --> 00:14:26,720
we'll see again how this is enabled at

00:14:25,199 --> 00:14:30,240
an angle

00:14:26,720 --> 00:14:34,160
pace and country scale in different

00:14:30,240 --> 00:14:36,240
use cases so the manifold

00:14:34,160 --> 00:14:37,519
learning geometry what this means is

00:14:36,240 --> 00:14:40,240
that imagine you have

00:14:37,519 --> 00:14:41,279
uh on your left here a satellite image

00:14:40,240 --> 00:14:44,880
scene

00:14:41,279 --> 00:14:46,480
we tile the image into say 500 by 500

00:14:44,880 --> 00:14:49,680
pixels and then through some

00:14:46,480 --> 00:14:50,160
mapping we create a projection of that

00:14:49,680 --> 00:14:52,959
image

00:14:50,160 --> 00:14:53,600
so what you seeing in the middle picture

00:14:52,959 --> 00:14:56,639
here

00:14:53,600 --> 00:14:59,440
is a projection that comes out via some

00:14:56,639 --> 00:15:01,760
mapping function and what this enables

00:14:59,440 --> 00:15:03,519
us to do is to create you know the

00:15:01,760 --> 00:15:04,639
collocational thing of things that are

00:15:03,519 --> 00:15:06,560
very similar

00:15:04,639 --> 00:15:09,199
and then once we've created this core

00:15:06,560 --> 00:15:11,199
location we then bring in some effort to

00:15:09,199 --> 00:15:14,880
try to partition

00:15:11,199 --> 00:15:17,279
the projected space so that we can

00:15:14,880 --> 00:15:19,040
end up with a single model that is

00:15:17,279 --> 00:15:21,680
trained for each of the partitions

00:15:19,040 --> 00:15:22,480
and we do this both uh after training

00:15:21,680 --> 00:15:24,880
and then at

00:15:22,480 --> 00:15:26,560
inference to be able to then redeploy

00:15:24,880 --> 00:15:28,959
models and index

00:15:26,560 --> 00:15:30,079
index each of the partitions to a

00:15:28,959 --> 00:15:31,839
specific model

00:15:30,079 --> 00:15:33,680
so this is just showing a picture of a

00:15:31,839 --> 00:15:34,560
single image that is projected to this

00:15:33,680 --> 00:15:36,160
space and

00:15:34,560 --> 00:15:37,680
if you look at if you bring another

00:15:36,160 --> 00:15:40,720
image we kind of have

00:15:37,680 --> 00:15:43,040
like a similar picture as well whereby

00:15:40,720 --> 00:15:44,240
from the zoomed in picture you can see

00:15:43,040 --> 00:15:45,839
that you know

00:15:44,240 --> 00:15:47,600
a lot of structures things that are very

00:15:45,839 --> 00:15:49,759
similar it could be that i mean

00:15:47,600 --> 00:15:50,800
these are the buildings they tend to

00:15:49,759 --> 00:15:52,959
collocate

00:15:50,800 --> 00:15:54,720
and then as you move out to the tail

00:15:52,959 --> 00:15:56,480
tail at the top

00:15:54,720 --> 00:15:57,759
you start to see you know the core

00:15:56,480 --> 00:16:01,199
location

00:15:57,759 --> 00:16:04,079
of tiles that do not contain you know

00:16:01,199 --> 00:16:05,759
objects of interest so this is one way

00:16:04,079 --> 00:16:09,440
we can get to actually you know

00:16:05,759 --> 00:16:10,079
exclude uh tiles that would need not to

00:16:09,440 --> 00:16:13,519
process

00:16:10,079 --> 00:16:16,000
uh during inference and then only focus

00:16:13,519 --> 00:16:17,600
on tiles that contains the objects you

00:16:16,000 --> 00:16:18,560
know in trying to balance that two

00:16:17,600 --> 00:16:21,360
percent

00:16:18,560 --> 00:16:23,759
you know uh data that contain pixels of

00:16:21,360 --> 00:16:27,040
interest this is the 98

00:16:23,759 --> 00:16:28,880
uh that does that is in the background

00:16:27,040 --> 00:16:30,639
so here's another look again on the

00:16:28,880 --> 00:16:33,199
right you know where we

00:16:30,639 --> 00:16:33,839
projecting instead of the original image

00:16:33,199 --> 00:16:36,320
tiles

00:16:33,839 --> 00:16:37,279
but now looking at the actual you know

00:16:36,320 --> 00:16:38,959
labels

00:16:37,279 --> 00:16:40,560
that accompany that town and you can

00:16:38,959 --> 00:16:42,480
easily see where these structured

00:16:40,560 --> 00:16:44,320
environments are and then on details you

00:16:42,480 --> 00:16:45,040
see that it's pretty much you know bell

00:16:44,320 --> 00:16:47,600
and

00:16:45,040 --> 00:16:49,120
and if we are to train a model will

00:16:47,600 --> 00:16:50,000
target areas where we have the

00:16:49,120 --> 00:16:52,720
structured

00:16:50,000 --> 00:16:52,720
environments

00:16:53,360 --> 00:16:56,720
so the framework that we've put together

00:16:55,839 --> 00:17:00,720
that is uh

00:16:56,720 --> 00:17:04,240
you know up on the slide here we call it

00:17:00,720 --> 00:17:05,120
rest law so response stands for remote

00:17:04,240 --> 00:17:08,319
sensing

00:17:05,120 --> 00:17:09,199
data flow for analytics so the idea here

00:17:08,319 --> 00:17:11,760
is that

00:17:09,199 --> 00:17:13,199
once we've done pulled out this manifold

00:17:11,760 --> 00:17:15,280
geometry

00:17:13,199 --> 00:17:17,360
and done the partitioning so this would

00:17:15,280 --> 00:17:18,240
be the offline initialization that is at

00:17:17,360 --> 00:17:22,319
the top

00:17:18,240 --> 00:17:26,640
of the slide here

00:17:22,319 --> 00:17:29,679
we then create uh this bucketing

00:17:26,640 --> 00:17:32,160
bucket information so the idea is that

00:17:29,679 --> 00:17:34,559
from the partition of the manifold

00:17:32,160 --> 00:17:37,039
geometry

00:17:34,559 --> 00:17:38,240
we map each of the partitions and our

00:17:37,039 --> 00:17:40,240
intention in mapping

00:17:38,240 --> 00:17:42,160
each of the partitions is that we drop

00:17:40,240 --> 00:17:43,440
all the collocated images into your

00:17:42,160 --> 00:17:46,559
pocket

00:17:43,440 --> 00:17:49,760
and then the bucket will then inform

00:17:46,559 --> 00:17:51,919
all similar things for that uh

00:17:49,760 --> 00:17:53,200
for that core location and will end up

00:17:51,919 --> 00:17:55,440
deriving a model

00:17:53,200 --> 00:17:57,520
that is uh tailored for that and then at

00:17:55,440 --> 00:17:59,840
the bottom the online inference

00:17:57,520 --> 00:18:00,880
this is where we seek to reuse some of

00:17:59,840 --> 00:18:04,160
the components

00:18:00,880 --> 00:18:07,440
from the offline initializations up

00:18:04,160 --> 00:18:09,600
stage however in an inference

00:18:07,440 --> 00:18:12,160
let me step into each of these two

00:18:09,600 --> 00:18:16,000
components the offline initialization

00:18:12,160 --> 00:18:19,039
online influence more carefully

00:18:16,000 --> 00:18:20,960
so as i mentioned you start with a set

00:18:19,039 --> 00:18:23,200
of unlabeled images

00:18:20,960 --> 00:18:24,480
that is at the beginning uh and then we

00:18:23,200 --> 00:18:26,080
do have a component of

00:18:24,480 --> 00:18:28,320
what we call a feature extraction that

00:18:26,080 --> 00:18:30,400
is a deep learning uh module

00:18:28,320 --> 00:18:32,160
that extract features so it could be at

00:18:30,400 --> 00:18:32,960
the if you're familiar with deep

00:18:32,160 --> 00:18:34,640
learning more

00:18:32,960 --> 00:18:36,320
models deep convolutional neural

00:18:34,640 --> 00:18:37,360
networks this could be the fully

00:18:36,320 --> 00:18:38,880
connected layer

00:18:37,360 --> 00:18:41,200
and then when you pull this fully

00:18:38,880 --> 00:18:42,720
connected layer for each of the images

00:18:41,200 --> 00:18:44,640
that are in your archive

00:18:42,720 --> 00:18:45,760
you do then this partitioning using the

00:18:44,640 --> 00:18:49,440
manifold

00:18:45,760 --> 00:18:50,480
geometry and this metric space formation

00:18:49,440 --> 00:18:52,480
function

00:18:50,480 --> 00:18:53,919
it's all purpose is to do the mapping

00:18:52,480 --> 00:18:56,000
while you're presenting

00:18:53,919 --> 00:18:57,039
you know the partitions where do you

00:18:56,000 --> 00:18:59,520
drop

00:18:57,039 --> 00:19:00,799
each of these images in terms of these

00:18:59,520 --> 00:19:01,760
uh the set of buckets that you're

00:19:00,799 --> 00:19:04,080
looking at

00:19:01,760 --> 00:19:05,919
and after you've dropped the images into

00:19:04,080 --> 00:19:08,480
the buckets you then train

00:19:05,919 --> 00:19:09,280
a single model for each of the buckets

00:19:08,480 --> 00:19:11,440
so that is

00:19:09,280 --> 00:19:12,960
where we have the last part here as the

00:19:11,440 --> 00:19:16,240
parallel training

00:19:12,960 --> 00:19:16,960
and we do this at scale but the other

00:19:16,240 --> 00:19:19,200
thing that we

00:19:16,960 --> 00:19:20,160
do as well when we drop images into the

00:19:19,200 --> 00:19:23,039
buckets

00:19:20,160 --> 00:19:24,799
is to create an image gallery so that

00:19:23,039 --> 00:19:26,240
later on we able to index

00:19:24,799 --> 00:19:28,400
the images that we dropped into the

00:19:26,240 --> 00:19:30,720
buckets and tie these images

00:19:28,400 --> 00:19:32,559
to the trained models if we start to

00:19:30,720 --> 00:19:34,799
either retrain and inform different

00:19:32,559 --> 00:19:37,760
models we could easily do that

00:19:34,799 --> 00:19:39,440
and on the parallel training once we've

00:19:37,760 --> 00:19:41,120
trained the models we push all the

00:19:39,440 --> 00:19:43,520
models into a model gallery

00:19:41,120 --> 00:19:45,039
because our intention is to reuse these

00:19:43,520 --> 00:19:49,039
models at influence

00:19:45,039 --> 00:19:51,600
stage so the metric

00:19:49,039 --> 00:19:53,360
uh space formation is also a deep

00:19:51,600 --> 00:19:56,400
learning module

00:19:53,360 --> 00:19:57,760
that allows us to you know recreate the

00:19:56,400 --> 00:19:59,919
core location

00:19:57,760 --> 00:20:00,880
of similar images so that when we map

00:19:59,919 --> 00:20:03,360
into the buckets

00:20:00,880 --> 00:20:04,840
we preserve that collocation because

00:20:03,360 --> 00:20:06,400
it's important that we have that

00:20:04,840 --> 00:20:09,679
homogeneity

00:20:06,400 --> 00:20:11,120
in the bucket formation this is the big

00:20:09,679 --> 00:20:14,960
advantage that helps us to

00:20:11,120 --> 00:20:16,880
be able to reuse the models and also

00:20:14,960 --> 00:20:18,080
to reduce the need for us to have to

00:20:16,880 --> 00:20:21,280
recreate

00:20:18,080 --> 00:20:24,960
different data at different times

00:20:21,280 --> 00:20:27,280
so to move on to the on online influence

00:20:24,960 --> 00:20:29,039
the idea here is that we would like to

00:20:27,280 --> 00:20:32,720
reuse models coming from the

00:20:29,039 --> 00:20:35,840
uh training initialization so in a way

00:20:32,720 --> 00:20:38,159
we're seeking to pair trained models

00:20:35,840 --> 00:20:39,039
with new satellite images so that we are

00:20:38,159 --> 00:20:41,520
able to do

00:20:39,039 --> 00:20:42,240
the detection of the objects of interest

00:20:41,520 --> 00:20:45,120
and again

00:20:42,240 --> 00:20:46,480
here we make use of the future

00:20:45,120 --> 00:20:49,840
extraction

00:20:46,480 --> 00:20:52,880
and present extracted features to the

00:20:49,840 --> 00:20:56,080
mapping function the idea here

00:20:52,880 --> 00:20:59,200
now is to match

00:20:56,080 --> 00:21:01,600
each incoming test image with an

00:20:59,200 --> 00:21:02,960
existing trained model that is being

00:21:01,600 --> 00:21:05,919
pulled from the

00:21:02,960 --> 00:21:06,720
model gallery so once you train the

00:21:05,919 --> 00:21:09,919
model

00:21:06,720 --> 00:21:12,240
the idea is that the mapping

00:21:09,919 --> 00:21:13,440
when once once the mapping drops into

00:21:12,240 --> 00:21:16,960
your bucket

00:21:13,440 --> 00:21:20,240
the implication is that that tile

00:21:16,960 --> 00:21:22,320
is more similar or most relevant to be

00:21:20,240 --> 00:21:23,520
processed by this existing model so

00:21:22,320 --> 00:21:26,480
that's

00:21:23,520 --> 00:21:27,679
why this hashing mapping function gets

00:21:26,480 --> 00:21:31,039
to be very useful

00:21:27,679 --> 00:21:34,240
in this uh agile reuse of models to pair

00:21:31,039 --> 00:21:35,200
existing models to unlabeled satellite

00:21:34,240 --> 00:21:38,159
imagery

00:21:35,200 --> 00:21:39,280
but now to be able to do this without um

00:21:38,159 --> 00:21:42,240
you know

00:21:39,280 --> 00:21:44,080
bringing in platforms like apache spark

00:21:42,240 --> 00:21:46,960
what we are looking at here is

00:21:44,080 --> 00:21:47,840
three disjointed uh machine learning

00:21:46,960 --> 00:21:49,520
components

00:21:47,840 --> 00:21:51,679
the first being that the feature

00:21:49,520 --> 00:21:53,679
extraction is a deep planning module

00:21:51,679 --> 00:21:55,679
the metric space embedding is a deep

00:21:53,679 --> 00:21:56,960
learning module and then the object

00:21:55,679 --> 00:22:00,240
detection or the

00:21:56,960 --> 00:22:03,039
semantic segmentation or the labeling uh

00:22:00,240 --> 00:22:05,039
deep learning is a stand-alone deep

00:22:03,039 --> 00:22:07,679
learning model so the question is

00:22:05,039 --> 00:22:09,039
how can we combine these three and be

00:22:07,679 --> 00:22:11,840
able to do

00:22:09,039 --> 00:22:12,320
the work at scale and also be able to

00:22:11,840 --> 00:22:14,640
you know

00:22:12,320 --> 00:22:16,000
take advantage of the resources that we

00:22:14,640 --> 00:22:19,360
have

00:22:16,000 --> 00:22:21,919
resources being the reuse of a gpu to do

00:22:19,360 --> 00:22:21,919
three things

00:22:22,799 --> 00:22:27,120
so putting it together i mean when i

00:22:24,880 --> 00:22:28,559
mentioned the convergence of the two

00:22:27,120 --> 00:22:30,159
you know we take advantage of the

00:22:28,559 --> 00:22:32,080
underlying uh off

00:22:30,159 --> 00:22:35,360
line initialization to be able to

00:22:32,080 --> 00:22:35,360
facilitate the inference

00:22:35,760 --> 00:22:39,520
and just to give you an idea of once you

00:22:37,679 --> 00:22:40,400
drop things into the buckets what things

00:22:39,520 --> 00:22:43,520
look like

00:22:40,400 --> 00:22:44,880
um you know this is example an example

00:22:43,520 --> 00:22:48,400
on bucket one

00:22:44,880 --> 00:22:51,360
whereby we're looking at uh sparse tiles

00:22:48,400 --> 00:22:53,600
and these are this is a different bucket

00:22:51,360 --> 00:22:55,120
where you have dance tiles dense meaning

00:22:53,600 --> 00:22:57,600
that you know there's a lot of

00:22:55,120 --> 00:22:59,679
structures in these small tiles so what

00:22:57,600 --> 00:23:02,559
we've done here is just to project it to

00:22:59,679 --> 00:23:04,000
you know a two-dimensional visualization

00:23:02,559 --> 00:23:05,919
space to be able to tell

00:23:04,000 --> 00:23:07,600
you know the relationship you know

00:23:05,919 --> 00:23:10,559
between different buckets

00:23:07,600 --> 00:23:12,400
a closer look at these two you can start

00:23:10,559 --> 00:23:13,120
to see that yeah in bucket one pretty

00:23:12,400 --> 00:23:15,360
much uh

00:23:13,120 --> 00:23:16,640
looks like we don't have a lot of

00:23:15,360 --> 00:23:18,240
objects of interest

00:23:16,640 --> 00:23:20,400
and there's a lot of building structures

00:23:18,240 --> 00:23:21,039
or built structures or pixels built

00:23:20,400 --> 00:23:24,640
structures

00:23:21,039 --> 00:23:25,280
on the on bucket too and looking again

00:23:24,640 --> 00:23:27,919
at the same

00:23:25,280 --> 00:23:28,960
areas however looking at the label space

00:23:27,919 --> 00:23:31,600
where we do have

00:23:28,960 --> 00:23:32,799
in this case uh the buildings you can

00:23:31,600 --> 00:23:34,559
see the density

00:23:32,799 --> 00:23:36,720
of buildings in bucket two and the

00:23:34,559 --> 00:23:39,360
various parts presence of buildings in

00:23:36,720 --> 00:23:39,360
bucket one

00:23:40,240 --> 00:23:43,760
so again this is also another look at

00:23:42,960 --> 00:23:45,760
the buckets

00:23:43,760 --> 00:23:47,520
where we comparing two different hash

00:23:45,760 --> 00:23:49,200
tables and we can see

00:23:47,520 --> 00:23:51,039
again for example if you're looking at

00:23:49,200 --> 00:23:53,840
bucket one hash table one

00:23:51,039 --> 00:23:56,000
in bucket zero you know that's past

00:23:53,840 --> 00:23:59,120
distribution as compared to

00:23:56,000 --> 00:24:01,279
what we have in bucket one and two and

00:23:59,120 --> 00:24:03,360
we see these kinds of patterns are being

00:24:01,279 --> 00:24:05,600
helpful when we train the models

00:24:03,360 --> 00:24:07,760
in terms of getting a model to converge

00:24:05,600 --> 00:24:09,039
it's much much easier to get a model

00:24:07,760 --> 00:24:10,960
you know for object detection to

00:24:09,039 --> 00:24:12,240
converge when you have a dense bucket

00:24:10,960 --> 00:24:14,400
for example packet 2

00:24:12,240 --> 00:24:16,240
bucket one as compared to when you're

00:24:14,400 --> 00:24:18,000
training with bucket zero

00:24:16,240 --> 00:24:20,000
and once you have a model from bucket

00:24:18,000 --> 00:24:22,159
one two or four

00:24:20,000 --> 00:24:23,600
it's actually easier to reuse that model

00:24:22,159 --> 00:24:26,320
to do work on bucket

00:24:23,600 --> 00:24:26,320
bucket zero

00:24:27,039 --> 00:24:30,320
so putting these all together again i

00:24:29,760 --> 00:24:32,320
just to

00:24:30,320 --> 00:24:34,320
sum it before i move on to the apache

00:24:32,320 --> 00:24:36,400
spark uh component

00:24:34,320 --> 00:24:37,600
is that you start with i mean you know a

00:24:36,400 --> 00:24:39,919
large volume of

00:24:37,600 --> 00:24:41,200
satellite imagery and we do the

00:24:39,919 --> 00:24:43,760
partitioning

00:24:41,200 --> 00:24:45,440
into the image gallery the idea is that

00:24:43,760 --> 00:24:46,320
you know we do have existing trained

00:24:45,440 --> 00:24:49,520
models

00:24:46,320 --> 00:24:52,080
in a gallery that we would like to reuse

00:24:49,520 --> 00:24:53,520
and again this picture what is showing

00:24:52,080 --> 00:24:55,919
here is that

00:24:53,520 --> 00:24:57,279
you start with the partitioning but on

00:24:55,919 --> 00:24:58,720
the output what you're actually

00:24:57,279 --> 00:25:02,240
interested in

00:24:58,720 --> 00:25:04,799
is a single uh product

00:25:02,240 --> 00:25:06,159
that reassembles these partitions back

00:25:04,799 --> 00:25:09,120
together

00:25:06,159 --> 00:25:10,400
so these are things that are limiting if

00:25:09,120 --> 00:25:12,559
you do

00:25:10,400 --> 00:25:13,600
the computing of these modules

00:25:12,559 --> 00:25:16,400
independently

00:25:13,600 --> 00:25:18,400
and we'll see how apache spark helps us

00:25:16,400 --> 00:25:20,880
not only to deploy the models

00:25:18,400 --> 00:25:21,919
but also to manage back the results so

00:25:20,880 --> 00:25:24,880
that when we

00:25:21,919 --> 00:25:26,559
get the outputs we have all the tiles

00:25:24,880 --> 00:25:29,440
that are corresponding to a single

00:25:26,559 --> 00:25:30,080
image scene you know already already

00:25:29,440 --> 00:25:31,840
managed

00:25:30,080 --> 00:25:33,840
so that we can do post processing

00:25:31,840 --> 00:25:37,760
without uh having to

00:25:33,840 --> 00:25:42,240
you know have an off offline uh managing

00:25:37,760 --> 00:25:46,000
managing effort

00:25:42,240 --> 00:25:47,760
so under the hood uh the hardware stack

00:25:46,000 --> 00:25:51,679
that we're making use of

00:25:47,760 --> 00:25:54,000
is as shown on this up on this slide

00:25:51,679 --> 00:25:55,440
you know the framework consists of the

00:25:54,000 --> 00:25:56,480
parallel training and the parallel

00:25:55,440 --> 00:25:58,799
infants

00:25:56,480 --> 00:26:02,880
and we have apache spark being deployed

00:25:58,799 --> 00:26:05,279
in a container containerized environment

00:26:02,880 --> 00:26:06,960
and for the containers we making use of

00:26:05,279 --> 00:26:09,200
our singularity

00:26:06,960 --> 00:26:10,240
and the case that i'm going to show that

00:26:09,200 --> 00:26:13,679
i'm talking about here

00:26:10,240 --> 00:26:17,440
is we tested this on a set of dgx

00:26:13,679 --> 00:26:20,400
uh boxes where we had uh three

00:26:17,440 --> 00:26:22,640
three dgx ones connected via infiniband

00:26:20,400 --> 00:26:24,480
and then we also had two djx2s that are

00:26:22,640 --> 00:26:26,840
connected um

00:26:24,480 --> 00:26:28,640
with infiniband to be able to do

00:26:26,840 --> 00:26:31,760
multi-gpu processing

00:26:28,640 --> 00:26:33,760
at scale so what we have as well here is

00:26:31,760 --> 00:26:35,520
an example of how

00:26:33,760 --> 00:26:37,120
this uh hardware stack and the

00:26:35,520 --> 00:26:40,640
deployment of apache spark

00:26:37,120 --> 00:26:43,679
you know plays in the background uh

00:26:40,640 --> 00:26:46,320
when doing influence uh play this year

00:26:43,679 --> 00:26:47,120
so this is an area out of puerto rico

00:26:46,320 --> 00:26:50,240
and

00:26:47,120 --> 00:26:53,200
we you have nine different cnn models

00:26:50,240 --> 00:26:54,480
and different colors corresponds to

00:26:53,200 --> 00:26:56,960
different buckets

00:26:54,480 --> 00:26:58,320
so we can see here what we're seeing

00:26:56,960 --> 00:27:00,880
happening is that

00:26:58,320 --> 00:27:02,000
there is a reuse of different models in

00:27:00,880 --> 00:27:05,200
different areas

00:27:02,000 --> 00:27:08,080
for example you do have the bucket four

00:27:05,200 --> 00:27:10,320
that has that orange uh color being you

00:27:08,080 --> 00:27:14,080
know reused in different special

00:27:10,320 --> 00:27:16,080
disjoint special areas so what we

00:27:14,080 --> 00:27:20,000
envisioned envisioned at the at the

00:27:16,080 --> 00:27:21,760
beginning was that this disjoined uh

00:27:20,000 --> 00:27:24,720
attendance of the images where we have

00:27:21,760 --> 00:27:26,559
this special specialties joint

00:27:24,720 --> 00:27:28,080
what ends up happening is that you pull

00:27:26,559 --> 00:27:30,559
all things that they

00:27:28,080 --> 00:27:32,159
have might might have been you know

00:27:30,559 --> 00:27:33,440
especially disjointed however when you

00:27:32,159 --> 00:27:36,559
do the processing

00:27:33,440 --> 00:27:38,640
they all see a single model and then

00:27:36,559 --> 00:27:40,000
at the output when you do the imaging

00:27:38,640 --> 00:27:41,760
you get a result

00:27:40,000 --> 00:27:43,679
that looks like this with all the

00:27:41,760 --> 00:27:45,520
buildings detected

00:27:43,679 --> 00:27:48,720
with different models that you have in

00:27:45,520 --> 00:27:52,880
your hash table

00:27:48,720 --> 00:27:52,880
moving on to the next slide

00:27:53,200 --> 00:28:01,919
so the workflow itself underneath um

00:27:57,440 --> 00:28:03,679
this rest floor in terms of apache spark

00:28:01,919 --> 00:28:08,720
is up on this slide

00:28:03,679 --> 00:28:11,760
we we've built it in such a way that

00:28:08,720 --> 00:28:13,919
when we process uh the computing we're

00:28:11,760 --> 00:28:15,360
not actually carrying over the image

00:28:13,919 --> 00:28:18,559
pixel data

00:28:15,360 --> 00:28:21,039
we partition the data

00:28:18,559 --> 00:28:22,159
in the form of uh we create like a meta

00:28:21,039 --> 00:28:24,559
space

00:28:22,159 --> 00:28:26,399
where we partition the data and have

00:28:24,559 --> 00:28:28,720
this meta information for each of the

00:28:26,399 --> 00:28:30,880
image scenes that we are processing

00:28:28,720 --> 00:28:31,840
and throughout the spark modules and the

00:28:30,880 --> 00:28:35,039
rdds

00:28:31,840 --> 00:28:36,799
uh you know from the map flat map all

00:28:35,039 --> 00:28:38,960
that we are seeing

00:28:36,799 --> 00:28:40,399
in different stages from the tile batch

00:28:38,960 --> 00:28:42,960
and getting the extents

00:28:40,399 --> 00:28:44,320
is the meta information and then at the

00:28:42,960 --> 00:28:46,320
point where we

00:28:44,320 --> 00:28:48,720
need to invoke the three deep learning

00:28:46,320 --> 00:28:50,159
modules that's when we then read from

00:28:48,720 --> 00:28:52,399
the rdds for example

00:28:50,159 --> 00:28:53,279
in the middle here we have the embedding

00:28:52,399 --> 00:28:55,520
and the hash

00:28:53,279 --> 00:28:56,320
so this is the component that allows us

00:28:55,520 --> 00:28:59,520
to do the first

00:28:56,320 --> 00:29:01,840
stage to extract you know the

00:28:59,520 --> 00:29:03,679
semantic high level features with the

00:29:01,840 --> 00:29:05,760
deep learning first deep learning module

00:29:03,679 --> 00:29:07,200
and then the melting component then

00:29:05,760 --> 00:29:09,279
looks at this rdd

00:29:07,200 --> 00:29:11,679
with the meta information and does the

00:29:09,279 --> 00:29:12,320
hashing to be able to drop the image

00:29:11,679 --> 00:29:16,000
styles

00:29:12,320 --> 00:29:19,360
into a given bucket and if we needed to

00:29:16,000 --> 00:29:21,840
write these to a database we do this

00:29:19,360 --> 00:29:23,039
in another module where we you know drop

00:29:21,840 --> 00:29:26,640
things and write to

00:29:23,039 --> 00:29:29,520
the image gallery and the group by keys

00:29:26,640 --> 00:29:30,480
allows us to then you know have thing

00:29:29,520 --> 00:29:32,080
have tiles

00:29:30,480 --> 00:29:34,559
that are corresponding to each of the

00:29:32,080 --> 00:29:37,919
buckets group grouped together

00:29:34,559 --> 00:29:40,960
and during influence we index

00:29:37,919 --> 00:29:43,520
a given model for

00:29:40,960 --> 00:29:44,640
one bucket and then present the group

00:29:43,520 --> 00:29:47,679
bucket tiles

00:29:44,640 --> 00:29:48,559
to do inference of scale and the reduce

00:29:47,679 --> 00:29:52,000
by key

00:29:48,559 --> 00:29:55,360
allows us to then recollect

00:29:52,000 --> 00:29:58,720
the already you know mapped uh

00:29:55,360 --> 00:29:59,440
tiles so that we have this easier

00:29:58,720 --> 00:30:02,799
process to

00:29:59,440 --> 00:30:04,399
reconstruct the images

00:30:02,799 --> 00:30:06,559
that are corresponding to the size of

00:30:04,399 --> 00:30:09,679
the image style that we

00:30:06,559 --> 00:30:12,159
present it at the input

00:30:09,679 --> 00:30:15,120
column you've got about five minutes

00:30:12,159 --> 00:30:19,039
left in this session

00:30:15,120 --> 00:30:21,919
okay yeah so this is an example to

00:30:19,039 --> 00:30:23,600
instead of deploying you know the

00:30:21,919 --> 00:30:24,159
different set of models for example we

00:30:23,600 --> 00:30:26,559
deploy

00:30:24,159 --> 00:30:30,080
deploying here about 20 models within

00:30:26,559 --> 00:30:30,080
this singularity environment

00:30:30,240 --> 00:30:34,000
and here again another example you know

00:30:33,039 --> 00:30:37,039
that showcase

00:30:34,000 --> 00:30:39,039
how we so like orchestrate the inference

00:30:37,039 --> 00:30:41,279
so the key thing to take away here is

00:30:39,039 --> 00:30:42,240
that i do have this uh parameter same

00:30:41,279 --> 00:30:44,000
bed size

00:30:42,240 --> 00:30:46,000
12. so this is not like you know the

00:30:44,000 --> 00:30:47,600
size bit size

00:30:46,000 --> 00:30:49,679
that's common in computer vision where

00:30:47,600 --> 00:30:50,080
you're looking at like 256 size of an

00:30:49,679 --> 00:30:54,000
image

00:30:50,080 --> 00:30:56,159
of image file so the bag size

00:30:54,000 --> 00:30:57,840
here corresponds to 12 things of which

00:30:56,159 --> 00:30:58,399
the file size that you're processing at

00:30:57,840 --> 00:31:01,600
a given

00:30:58,399 --> 00:31:03,760
instance is about 200 gigabytes and this

00:31:01,600 --> 00:31:06,720
maps about 4200

00:31:03,760 --> 00:31:06,720
square kilometers

00:31:07,120 --> 00:31:13,679
for for processing uh the data

00:31:10,320 --> 00:31:14,799
you know for a given area so here's an

00:31:13,679 --> 00:31:16,960
example of the

00:31:14,799 --> 00:31:18,240
core location and reuse of models for

00:31:16,960 --> 00:31:20,559
building detection

00:31:18,240 --> 00:31:22,720
where we looked again at puerto rico new

00:31:20,559 --> 00:31:24,880
mexico and south sudan

00:31:22,720 --> 00:31:25,840
so the idea is that you collect images

00:31:24,880 --> 00:31:27,919
from all these

00:31:25,840 --> 00:31:29,760
different areas you dump them together

00:31:27,919 --> 00:31:31,519
and you run them through the workflow

00:31:29,760 --> 00:31:33,039
and what the workflow then does is that

00:31:31,519 --> 00:31:35,120
it indexes

00:31:33,039 --> 00:31:36,640
the existing models for example shown

00:31:35,120 --> 00:31:38,000
here in different colors

00:31:36,640 --> 00:31:39,600
so there will be a different bucket

00:31:38,000 --> 00:31:40,559
green different bucket red different

00:31:39,600 --> 00:31:42,640
bucket

00:31:40,559 --> 00:31:43,840
and what we see from this result is

00:31:42,640 --> 00:31:46,480
attendance

00:31:43,840 --> 00:31:47,039
of reusing you know the green and red

00:31:46,480 --> 00:31:49,360
models

00:31:47,039 --> 00:31:50,960
in new mexico and puerto rico partly

00:31:49,360 --> 00:31:52,000
because i mean there's a lot of

00:31:50,960 --> 00:31:54,720
similarities

00:31:52,000 --> 00:31:56,240
in structures in these areas however

00:31:54,720 --> 00:31:58,240
when you look at south sudan

00:31:56,240 --> 00:32:00,240
that's a different geography and

00:31:58,240 --> 00:32:00,799
different geometry in buildings we start

00:32:00,240 --> 00:32:04,720
to see

00:32:00,799 --> 00:32:05,600
a purple bucket model being reused much

00:32:04,720 --> 00:32:08,240
more often

00:32:05,600 --> 00:32:08,720
as compared to what you have on the red

00:32:08,240 --> 00:32:11,840
uh

00:32:08,720 --> 00:32:14,000
model for puerto rico in new mexico

00:32:11,840 --> 00:32:15,919
and here is another example as well in

00:32:14,000 --> 00:32:17,600
terms of doing road mapping where we're

00:32:15,919 --> 00:32:20,320
looking at two different countries and

00:32:17,600 --> 00:32:22,399
using models trained in iraq however

00:32:20,320 --> 00:32:24,399
being deployed in venezuela

00:32:22,399 --> 00:32:26,080
and keep in mind that when we deploy in

00:32:24,399 --> 00:32:26,880
these other areas for example in

00:32:26,080 --> 00:32:28,640
venezuela

00:32:26,880 --> 00:32:30,240
we're not actually retraining the models

00:32:28,640 --> 00:32:31,679
we're reusing the existing models that

00:32:30,240 --> 00:32:35,200
we trained

00:32:31,679 --> 00:32:38,559
in using data from other areas

00:32:35,200 --> 00:32:39,200
for example iraq here so quickly to sum

00:32:38,559 --> 00:32:41,600
up here

00:32:39,200 --> 00:32:43,279
uh here's some of the throughputs that

00:32:41,600 --> 00:32:44,399
we got uh you know from the building

00:32:43,279 --> 00:32:48,559
detection

00:32:44,399 --> 00:32:50,399
uh case study where again the main

00:32:48,559 --> 00:32:52,399
portion here is that we're combining

00:32:50,399 --> 00:32:53,679
three modules and being enabled by

00:32:52,399 --> 00:32:56,720
apache spark

00:32:53,679 --> 00:32:58,159
to run through all of these at once and

00:32:56,720 --> 00:33:01,840
get getting

00:32:58,159 --> 00:33:02,159
such uh throughputs so i was gonna sum

00:33:01,840 --> 00:33:04,240
up

00:33:02,159 --> 00:33:05,519
with uh you know some of the limitations

00:33:04,240 --> 00:33:07,440
and considerations that

00:33:05,519 --> 00:33:08,640
you know we've seen uh encountered

00:33:07,440 --> 00:33:10,240
during the process

00:33:08,640 --> 00:33:13,039
and these include how to you know

00:33:10,240 --> 00:33:15,840
repartition your data to be able to do

00:33:13,039 --> 00:33:17,039
uh to have an an even distribution

00:33:15,840 --> 00:33:19,600
during you know

00:33:17,039 --> 00:33:20,640
the rdds and one of the painful things

00:33:19,600 --> 00:33:22,480
that we encountered

00:33:20,640 --> 00:33:23,919
then and i'm hoping that with the

00:33:22,480 --> 00:33:25,919
patches part 3

00:33:23,919 --> 00:33:28,080
this has been resolved the fact that you

00:33:25,919 --> 00:33:29,600
know the gpu support was not

00:33:28,080 --> 00:33:31,760
friendly at the time that we did this

00:33:29,600 --> 00:33:34,480
with a purchase pack 2.4

00:33:31,760 --> 00:33:35,039
and also another limitation was the fact

00:33:34,480 --> 00:33:36,880
that

00:33:35,039 --> 00:33:38,559
you know handling two files from

00:33:36,880 --> 00:33:41,679
geospatial

00:33:38,559 --> 00:33:42,159
was not something that came with uh you

00:33:41,679 --> 00:33:45,120
know

00:33:42,159 --> 00:33:46,320
the apache spark and i'm hoping that as

00:33:45,120 --> 00:33:48,720
we move ahead

00:33:46,320 --> 00:33:50,480
you know we start to see more and more

00:33:48,720 --> 00:33:53,840
of these capabilities being built

00:33:50,480 --> 00:33:53,840
into this platform

00:33:54,000 --> 00:33:57,360
so just to acknowledge uh you know my

00:33:56,480 --> 00:33:59,600
teammates

00:33:57,360 --> 00:34:00,640
uh colleagues you know and orchid

00:33:59,600 --> 00:34:03,519
national laboratory

00:34:00,640 --> 00:34:04,480
uh you know this work has been due to a

00:34:03,519 --> 00:34:06,640
lot of help

00:34:04,480 --> 00:34:08,879
from different people and if there is

00:34:06,640 --> 00:34:11,040
any question and i've got my contacts

00:34:08,879 --> 00:34:12,480
at the bottom here i'll quickly just

00:34:11,040 --> 00:34:14,960
turn back my

00:34:12,480 --> 00:34:15,839
video maybe if there's a chance to do

00:34:14,960 --> 00:34:19,119
like one

00:34:15,839 --> 00:34:22,560
question i'll be happy to

00:34:19,119 --> 00:34:25,599
um i i think i'm gonna go ahead and uh

00:34:22,560 --> 00:34:26,960
ask uh a question or two uh when

00:34:25,599 --> 00:34:29,359
when you showed what you showed with

00:34:26,960 --> 00:34:32,240
puerto rico new mexico and south sudan

00:34:29,359 --> 00:34:34,159
and also uh the road comparison is this

00:34:32,240 --> 00:34:36,320
giving you a way to get a feedback

00:34:34,159 --> 00:34:38,240
uh cycle to know where you might need to

00:34:36,320 --> 00:34:40,560
go do some more model training and

00:34:38,240 --> 00:34:43,200
create new models

00:34:40,560 --> 00:34:44,000
yes great it is that that's a great

00:34:43,200 --> 00:34:47,040
point

00:34:44,000 --> 00:34:49,919
because we start to see where the

00:34:47,040 --> 00:34:50,399
originally trained models are sort of

00:34:49,919 --> 00:34:52,960
you know

00:34:50,399 --> 00:34:54,240
either starved in terms of being able to

00:34:52,960 --> 00:34:56,560
do object detection

00:34:54,240 --> 00:34:58,079
so now it becomes very clear to be able

00:34:56,560 --> 00:35:01,760
to know that oh okay

00:34:58,079 --> 00:35:04,240
we see that bucket four is struggling

00:35:01,760 --> 00:35:05,119
and we then go on to build a case to

00:35:04,240 --> 00:35:08,560
enhance

00:35:05,119 --> 00:35:10,000
the data for packet four uh nick's got a

00:35:08,560 --> 00:35:11,920
really quick question and then we should

00:35:10,000 --> 00:35:13,680
probably wrap up and go to the next talk

00:35:11,920 --> 00:35:15,839
that's starting presently in this track

00:35:13,680 --> 00:35:17,119
and he's asking uh what's the balance

00:35:15,839 --> 00:35:19,280
today between

00:35:17,119 --> 00:35:20,480
uh the public and private sectors as

00:35:19,280 --> 00:35:23,520
they're

00:35:20,480 --> 00:35:26,640
doing research for this sort of um

00:35:23,520 --> 00:35:28,320
thing so yeah i mean he's surprised to

00:35:26,640 --> 00:35:31,040
hear this coming out of a government lab

00:35:28,320 --> 00:35:32,000
uh i'm i'm happy to say this is u.s tax

00:35:31,040 --> 00:35:34,640
dollars at work

00:35:32,000 --> 00:35:35,040
uh but you know uh do you have thoughts

00:35:34,640 --> 00:35:37,520
on that

00:35:35,040 --> 00:35:37,520
briefly

00:35:38,240 --> 00:35:42,160
yeah i i would say yeah i've seen a lot

00:35:40,480 --> 00:35:43,200
of this capability also coming from

00:35:42,160 --> 00:35:45,359
industry

00:35:43,200 --> 00:35:46,560
and keep in mind that the scale that we

00:35:45,359 --> 00:35:48,880
are doing this

00:35:46,560 --> 00:35:50,079
uh for us because we're taking advantage

00:35:48,880 --> 00:35:51,599
of for example i mean the super

00:35:50,079 --> 00:35:53,760
computing facilities

00:35:51,599 --> 00:35:54,960
when we create these data sets we're not

00:35:53,760 --> 00:35:57,839
actually creating a

00:35:54,960 --> 00:35:58,640
like a competition uh space with the

00:35:57,839 --> 00:36:02,000
industry

00:35:58,640 --> 00:36:04,400
but however you know enabling

00:36:02,000 --> 00:36:06,720
some of these science products so that i

00:36:04,400 --> 00:36:08,640
mean some of the downstream applications

00:36:06,720 --> 00:36:10,240
can become a lot easier because i don't

00:36:08,640 --> 00:36:12,400
know of many people

00:36:10,240 --> 00:36:14,400
even in the industry that i am able to

00:36:12,400 --> 00:36:15,200
process you know the data problems that

00:36:14,400 --> 00:36:17,359
we handle

00:36:15,200 --> 00:36:18,240
and be able to actually do this at large

00:36:17,359 --> 00:36:20,000
scale

00:36:18,240 --> 00:36:21,599
well uh thank you very much for your

00:36:20,000 --> 00:36:24,720
talk thanks for sharing your

00:36:21,599 --> 00:36:25,839
uh email i'm uh sure people will reach

00:36:24,720 --> 00:36:28,000
out

00:36:25,839 --> 00:36:29,839
let's hop over to the next talk and

00:36:28,000 --> 00:36:41,839
thank you very much thank you

00:36:29,839 --> 00:36:41,839
thank you

00:37:06,640 --> 00:37:08,720

YouTube URL: https://www.youtube.com/watch?v=Nq0ia65CJ6c


