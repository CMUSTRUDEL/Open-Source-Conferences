Title: Taking the guesswork out of your Hadoop Infrastructure
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 2
Description: 
	Steve Watt
ApacheCon NA 2013
Big Data
Captions: 
	00:00:01,610 --> 00:00:08,099
alright our next speaker is Steve wat

00:00:04,710 --> 00:00:09,719
and he's going to talk about taking

00:00:08,099 --> 00:00:13,349
guesswork out of your group

00:00:09,719 --> 00:00:15,690
infrastructure okay hi folks Steve wat

00:00:13,349 --> 00:00:16,500
will be presenting on taking the

00:00:15,690 --> 00:00:19,289
guesswork out of your Hadoop

00:00:16,500 --> 00:00:21,990
infrastructure so just to sort of level

00:00:19,289 --> 00:00:23,760
set up till December last year I had

00:00:21,990 --> 00:00:24,990
spent the last two years at HP is the

00:00:23,760 --> 00:00:28,140
chief technologist for the Hadoop

00:00:24,990 --> 00:00:29,939
program so a lot of the findings I from

00:00:28,140 --> 00:00:32,430
that and HPS graciously allowed me to

00:00:29,939 --> 00:00:36,149
represent our work so but I do currently

00:00:32,430 --> 00:00:38,850
work at red hat at the moment so my

00:00:36,149 --> 00:00:41,700
agenda for this talk is to clear up some

00:00:38,850 --> 00:00:43,800
pretty persistent misconceptions about

00:00:41,700 --> 00:00:47,039
Hadoop infrastructure and part of this

00:00:43,800 --> 00:00:48,989
is around the fact that there's a lot of

00:00:47,039 --> 00:00:54,000
collateral on the web that sort of left

00:00:48,989 --> 00:00:56,129
over from 2006 around Hadoop guidelines

00:00:54,000 --> 00:00:57,930
and best practices and so you know I

00:00:56,129 --> 00:01:00,690
sort of highlighted it around here but

00:00:57,930 --> 00:01:03,359
you know the the recommendations that

00:01:00,690 --> 00:01:05,970
you sort of see on this blog are sort of

00:01:03,359 --> 00:01:08,400
2006 like you you know given the current

00:01:05,970 --> 00:01:10,080
processors and a balanced knowed you'd

00:01:08,400 --> 00:01:12,330
probably want something like four disks

00:01:10,080 --> 00:01:15,420
or something but in today's day with the

00:01:12,330 --> 00:01:17,040
economy so commodity architectures you'd

00:01:15,420 --> 00:01:18,780
probably want something that's more like

00:01:17,040 --> 00:01:21,210
12 disks right so this is actually

00:01:18,780 --> 00:01:22,920
completely incorrect right now and so

00:01:21,210 --> 00:01:25,259
part of the reason that we have that is

00:01:22,920 --> 00:01:28,049
the web scale origins for Hadoop are

00:01:25,259 --> 00:01:30,090
around single or dual socket 1 plus

00:01:28,049 --> 00:01:33,240
gigahertz or somewhere around 1.2

00:01:30,090 --> 00:01:37,100
gigahertz 21.8 four to eight gig of ram

00:01:33,240 --> 00:01:39,689
often 32-bit moving later to 64-bit and

00:01:37,100 --> 00:01:41,460
with two to four cores and a single one

00:01:39,689 --> 00:01:43,200
gigabit nic right which is a bad thing

00:01:41,460 --> 00:01:46,200
right if you lose your neck your whole

00:01:43,200 --> 00:01:49,280
node goes offline and then to 24 1

00:01:46,200 --> 00:01:51,869
terabyte SATA drives whereas today

00:01:49,280 --> 00:01:54,090
you're looking the majority of Hadoop

00:01:51,869 --> 00:01:57,860
deployments are dual socket 2 plus

00:01:54,090 --> 00:02:02,369
gigahertz so I commonly see between 2.4

00:01:57,860 --> 00:02:07,890
22.9 gigahertz 24 to 48 gig of ram four

00:02:02,369 --> 00:02:10,860
to six cores to 24 1 gig in X and you

00:02:07,890 --> 00:02:12,450
know just as a case in point like HP's

00:02:10,860 --> 00:02:13,530
class or service that they sell for

00:02:12,450 --> 00:02:15,600
Hadoop don't

00:02:13,530 --> 00:02:19,080
come with the two gigabit just a two gig

00:02:15,600 --> 00:02:21,600
neck option you know most enterprise

00:02:19,080 --> 00:02:23,819
cloth servers these days have a flexible

00:02:21,600 --> 00:02:25,500
land on motherboard type in a regardless

00:02:23,819 --> 00:02:28,740
of vandor where you either get for one

00:02:25,500 --> 00:02:30,900
gig e or to 10 gige just that right so

00:02:28,740 --> 00:02:34,350
you can't really get it to one gig e at

00:02:30,900 --> 00:02:35,910
the moment so and then typically you

00:02:34,350 --> 00:02:39,330
know if you're looking at a 1u server

00:02:35,910 --> 00:02:41,010
you might have four disks but you know

00:02:39,330 --> 00:02:42,660
you'd probably be going that right

00:02:41,010 --> 00:02:44,130
because you're having rack density

00:02:42,660 --> 00:02:47,370
issues or something to that effect

00:02:44,130 --> 00:02:51,030
really it's a to you sir with 14 large

00:02:47,370 --> 00:02:53,040
form factor SATA drives and so so it's

00:02:51,030 --> 00:02:54,720
sort of completely different right to

00:02:53,040 --> 00:02:56,880
the other one and there's there's other

00:02:54,720 --> 00:02:59,670
issues why the enterprise is quite

00:02:56,880 --> 00:03:01,590
different to the web web scale right so

00:02:59,670 --> 00:03:04,350
by that I mean like the yahoos the

00:03:01,590 --> 00:03:06,720
LinkedIn's the Facebook's the ebays in

00:03:04,350 --> 00:03:09,690
that you know they're running thousands

00:03:06,720 --> 00:03:11,519
of nodes right or hundreds of nodes and

00:03:09,690 --> 00:03:13,290
economies of scale are quite different

00:03:11,519 --> 00:03:15,239
so in the enterprise they're typically

00:03:13,290 --> 00:03:17,579
on average running one to two racks and

00:03:15,239 --> 00:03:18,989
if you're writing one to two racks in

00:03:17,579 --> 00:03:22,799
production with a service level

00:03:18,989 --> 00:03:24,510
agreement around up time the it's quite

00:03:22,799 --> 00:03:27,060
a big deal if you lose three notes right

00:03:24,510 --> 00:03:28,980
so typical rack you'll have 20 nodes per

00:03:27,060 --> 00:03:31,560
rack right so you got your total cluster

00:03:28,980 --> 00:03:33,329
is 40 40 notes right so if two or two to

00:03:31,560 --> 00:03:35,070
four nodes go offline it's kind of a big

00:03:33,329 --> 00:03:37,200
deal and so what you'll see different

00:03:35,070 --> 00:03:40,860
behavior in the enterprise right you'll

00:03:37,200 --> 00:03:42,930
often hear people say no raid in Hadoop

00:03:40,860 --> 00:03:45,450
worker nodes right those who deep slaves

00:03:42,930 --> 00:03:48,930
right don't don't huge rate at all well

00:03:45,450 --> 00:03:50,640
actually if you have 14 drives it kind

00:03:48,930 --> 00:03:52,739
of makes a lot of sense to put your OS

00:03:50,640 --> 00:03:55,350
and her d prime time on one of the

00:03:52,739 --> 00:03:57,329
drives and do a red merle of that be so

00:03:55,350 --> 00:03:59,370
to avoid the case of if you lose that

00:03:57,329 --> 00:04:01,650
one drive the whole node goes offline

00:03:59,370 --> 00:04:03,299
right and so you do a raid one mirror of

00:04:01,650 --> 00:04:05,160
those and then you still have 12 disks

00:04:03,299 --> 00:04:08,250
left over you know twenty four terabytes

00:04:05,160 --> 00:04:11,010
to do you need such as data discs right

00:04:08,250 --> 00:04:15,060
for your data node or your tie stracker

00:04:11,010 --> 00:04:18,239
temp info so there's a bit of a you you

00:04:15,060 --> 00:04:20,220
really have to factor scale in here so

00:04:18,239 --> 00:04:22,770
there are really three things to get

00:04:20,220 --> 00:04:25,470
right with with hoodie right you've got

00:04:22,770 --> 00:04:26,580
a balanced storage capacity specifically

00:04:25,470 --> 00:04:27,310
the requirements of the Hadoop

00:04:26,580 --> 00:04:28,840
distributed

00:04:27,310 --> 00:04:30,490
our system with the performance of the

00:04:28,840 --> 00:04:33,790
computational layers that sit on top of

00:04:30,490 --> 00:04:36,070
it so and then at the same point you

00:04:33,790 --> 00:04:39,040
want to get them at a price that you

00:04:36,070 --> 00:04:41,410
find palatable right so enterprises

00:04:39,040 --> 00:04:43,540
typically want to spend all my

00:04:41,410 --> 00:04:45,610
recommendation is not to really spend

00:04:43,540 --> 00:04:47,410
more than will get as close to ten

00:04:45,610 --> 00:04:50,470
thousand dollars per node as possible

00:04:47,410 --> 00:04:51,940
right if you want a more performance

00:04:50,470 --> 00:04:53,530
architecture that's going to start

00:04:51,940 --> 00:04:55,030
edging up towards twenty thousand

00:04:53,530 --> 00:04:59,080
dollars right and it adds up fairly

00:04:55,030 --> 00:05:00,729
quickly so there's you know it's tricky

00:04:59,080 --> 00:05:05,100
to balance and so what I'm going to do

00:05:00,729 --> 00:05:08,010
is show you a method to be able to

00:05:05,100 --> 00:05:10,210
instrument your workload and be able to

00:05:08,010 --> 00:05:12,040
completely optimize your your

00:05:10,210 --> 00:05:14,229
infrastructure and pick the exact things

00:05:12,040 --> 00:05:17,590
that you should buy to be able to get

00:05:14,229 --> 00:05:18,970
the best bang for your buck so the first

00:05:17,590 --> 00:05:21,160
thing obviously is to understand your

00:05:18,970 --> 00:05:24,040
workload right so what you know you

00:05:21,160 --> 00:05:26,320
doing MapReduce etl ingest or you doing

00:05:24,040 --> 00:05:28,380
k-means clustering or some sort of

00:05:26,320 --> 00:05:30,820
PageRank graph processing algorithm

00:05:28,380 --> 00:05:34,090
you've got to know whether your workload

00:05:30,820 --> 00:05:37,930
is i/o bound or CPU bound etc and keep

00:05:34,090 --> 00:05:40,380
in mind that's a a mixture of your

00:05:37,930 --> 00:05:42,520
application design with the

00:05:40,380 --> 00:05:44,280
infrastructure that it's running on if

00:05:42,520 --> 00:05:47,440
you remember the keynote the other day

00:05:44,280 --> 00:05:50,770
the guy said the guy gave an example of

00:05:47,440 --> 00:05:52,090
someone saying hey I'm IO bound and he

00:05:50,770 --> 00:05:53,350
said well tell me about your server

00:05:52,090 --> 00:05:55,539
right the whole point of understanding

00:05:53,350 --> 00:05:57,910
right up and down the sack stack and the

00:05:55,539 --> 00:06:01,270
guys like well for 9th I Oh intense of

00:05:57,910 --> 00:06:02,919
workload with you know dual socket lots

00:06:01,270 --> 00:06:05,740
of course but I have two discs right

00:06:02,919 --> 00:06:08,020
well duh you're you know buy more discs

00:06:05,740 --> 00:06:09,880
right you know spread the I hose up so

00:06:08,020 --> 00:06:12,400
you've got to know whether your i/o

00:06:09,880 --> 00:06:14,080
bound cpu-bound the problem is almost it

00:06:12,400 --> 00:06:16,660
when i was at HP almost all our

00:06:14,080 --> 00:06:18,669
customers i spoke to nobody's nobody's

00:06:16,660 --> 00:06:20,320
done this right it's very uncommon to

00:06:18,669 --> 00:06:22,960
find somebody that actually understands

00:06:20,320 --> 00:06:25,060
the characteristics of their workload on

00:06:22,960 --> 00:06:27,610
their hardware in fact the only person i

00:06:25,060 --> 00:06:31,200
know this is the linkedin ops alan will

00:06:27,610 --> 00:06:34,180
it with an hour so here's here's a

00:06:31,200 --> 00:06:35,650
method to do that right very simple it's

00:06:34,180 --> 00:06:37,659
actually common sense you instrument the

00:06:35,650 --> 00:06:39,760
cluster then you run your workload and

00:06:37,659 --> 00:06:41,699
then you analyze the numbers now it's a

00:06:39,760 --> 00:06:44,710
common

00:06:41,699 --> 00:06:46,749
miss tip to do a paper exercise for

00:06:44,710 --> 00:06:48,939
Hadoop which is you know theoretically

00:06:46,749 --> 00:06:51,249
you know with this disk to call ratio

00:06:48,939 --> 00:06:53,199
and you know what we know about our AO

00:06:51,249 --> 00:06:55,930
profile on a single know that it should

00:06:53,199 --> 00:06:59,379
be behave like this don't just don't

00:06:55,930 --> 00:07:01,330
it's every single time we posited an

00:06:59,379 --> 00:07:03,759
outcome on paper at HP we were wrong

00:07:01,330 --> 00:07:05,830
right who do proved us wrong every

00:07:03,759 --> 00:07:08,169
single time so so what you're seeing

00:07:05,830 --> 00:07:11,589
here in this picture is Andy learner who

00:07:08,169 --> 00:07:13,569
is a member of our team you seeing a map

00:07:11,589 --> 00:07:17,710
or cluster on the rack on the right and

00:07:13,569 --> 00:07:18,999
clawed error on the left so let me just

00:07:17,710 --> 00:07:23,020
sort of explain and I'm going to just

00:07:18,999 --> 00:07:25,150
point out so if the way racks are set up

00:07:23,020 --> 00:07:26,499
right so not all of us spend a lot of

00:07:25,150 --> 00:07:27,669
time around rack so I'm just going to

00:07:26,499 --> 00:07:29,919
for those that don't know I'm just going

00:07:27,669 --> 00:07:32,649
to give you a quick overview a typical

00:07:29,919 --> 00:07:34,569
enterprise-class rack is 42 you right

00:07:32,649 --> 00:07:37,960
and that's a it's a height measurement

00:07:34,569 --> 00:07:39,999
right and a you is is basically roughly

00:07:37,960 --> 00:07:42,939
this big right it's about this high it's

00:07:39,999 --> 00:07:45,129
probably about two inches and that is a

00:07:42,939 --> 00:07:48,249
unit of measurement and so there's 42 of

00:07:45,129 --> 00:07:50,349
those use that fill up Iraq and servers

00:07:48,249 --> 00:07:53,439
that slide into Iraq are usually one you

00:07:50,349 --> 00:07:55,479
or to you or for you right you can get

00:07:53,439 --> 00:07:57,969
more I guess bigger more dense ones but

00:07:55,479 --> 00:08:00,189
what you'll see typically for Hadoop is

00:07:57,969 --> 00:08:02,289
a one you're a to you which basically is

00:08:00,189 --> 00:08:04,330
something a server this thick we're a

00:08:02,289 --> 00:08:09,819
server that effect right they they slide

00:08:04,330 --> 00:08:13,689
in like a tray right so so so what we

00:08:09,819 --> 00:08:17,229
did here is any time we did a reference

00:08:13,689 --> 00:08:19,029
architecture for a partner on our on our

00:08:17,229 --> 00:08:21,819
platforms we had to figure out what was

00:08:19,029 --> 00:08:23,409
the right server guidelines or

00:08:21,819 --> 00:08:24,759
recommendations that we would make for

00:08:23,409 --> 00:08:27,249
their software and this is actually

00:08:24,759 --> 00:08:29,259
standard for this business unit so

00:08:27,249 --> 00:08:30,999
whether they're doing you know ASAP or

00:08:29,259 --> 00:08:34,269
SAS analytics they go through the same

00:08:30,999 --> 00:08:35,740
process and so they run a bunch of

00:08:34,269 --> 00:08:37,360
workloads and then they look and

00:08:35,740 --> 00:08:38,949
understand the constraints and then that

00:08:37,360 --> 00:08:41,529
you know add a controller or add a

00:08:38,949 --> 00:08:43,959
couple of disks or you know improve the

00:08:41,529 --> 00:08:46,000
processors more cores etc etc so they go

00:08:43,959 --> 00:08:49,269
through this process so the process that

00:08:46,000 --> 00:08:51,880
we used just to to get an understanding

00:08:49,269 --> 00:08:54,010
is that we did a 10 terabyte Terra sort

00:08:51,880 --> 00:08:54,410
this is sort of a funny segue to me

00:08:54,010 --> 00:08:56,750
because

00:08:54,410 --> 00:09:00,440
is this very convenient there's a flash

00:08:56,750 --> 00:09:03,589
drive right here you know the majority

00:09:00,440 --> 00:09:06,290
of customers are doing at least using a

00:09:03,589 --> 00:09:08,149
couple of terabytes for 400 there might

00:09:06,290 --> 00:09:10,459
not it's not necessarily everyone's in

00:09:08,149 --> 00:09:11,930
the petabyte age right but they're doing

00:09:10,459 --> 00:09:13,370
a couple of terabytes if you see

00:09:11,930 --> 00:09:17,060
somebody that tells you that they've

00:09:13,370 --> 00:09:18,649
done a 100 gigabyte benchmark for Hadoop

00:09:17,060 --> 00:09:21,079
you say you should say you see this

00:09:18,649 --> 00:09:23,509
that's 100 gigabytes it's on a flash

00:09:21,079 --> 00:09:26,300
drive it's nonsense right so you want to

00:09:23,509 --> 00:09:27,350
do you know at least 10 terabytes was

00:09:26,300 --> 00:09:31,370
something if you're going to properly

00:09:27,350 --> 00:09:33,139
exercised your rack so so what we've got

00:09:31,370 --> 00:09:36,589
here is we've got a dedicated job

00:09:33,139 --> 00:09:38,750
tracker in a dedicated name node that's

00:09:36,589 --> 00:09:41,480
pretty much standard practice for full

00:09:38,750 --> 00:09:45,529
deployment and then we've got 18 Hadoop

00:09:41,480 --> 00:09:47,689
slaves so 18 Hadoop nodes and just to

00:09:45,529 --> 00:09:49,939
make sure you understand that the 360

00:09:47,689 --> 00:09:52,100
which runs the job tracker in the name

00:09:49,939 --> 00:09:54,050
no that's a one year and it's it's

00:09:52,100 --> 00:09:56,540
exactly the same server as the other one

00:09:54,050 --> 00:09:57,680
the difference is it's the other ones

00:09:56,540 --> 00:10:00,110
are to you because it's got to have

00:09:57,680 --> 00:10:02,120
space for all the drives right so that

00:10:00,110 --> 00:10:05,329
the jobtracker name no don't really have

00:10:02,120 --> 00:10:08,000
they need lots of CPU and memory but not

00:10:05,329 --> 00:10:09,740
to space right so so that's really

00:10:08,000 --> 00:10:12,139
what's different between them and then

00:10:09,740 --> 00:10:14,329
for the Hadoop slaves we have we

00:10:12,139 --> 00:10:16,459
configured Tarasov with 18 maps and 12

00:10:14,329 --> 00:10:19,279
reduces we played around a little bit

00:10:16,459 --> 00:10:23,209
with that to improve in performance and

00:10:19,279 --> 00:10:27,230
just to give you an idea this we set the

00:10:23,209 --> 00:10:31,939
the fastest terabyte sorry Hadoop terror

00:10:27,230 --> 00:10:33,800
sort record with this this config okay

00:10:31,939 --> 00:10:37,819
and so basically what you got a 64 gig

00:10:33,800 --> 00:10:40,670
ram to point a dual six core Intel 2.9

00:10:37,819 --> 00:10:45,680
gigahertz processors and then to two

00:10:40,670 --> 00:10:47,149
controllers with 16 1 terabyte small

00:10:45,680 --> 00:10:49,519
form factor this so I'm going to stop

00:10:47,149 --> 00:10:51,559
here and sort of explain why we went

00:10:49,519 --> 00:10:53,480
with that route right 64 gig of ram is

00:10:51,559 --> 00:10:55,879
could be a little bit over-provisioning

00:10:53,480 --> 00:10:57,980
the memory for typical hoodie pieces but

00:10:55,879 --> 00:10:59,660
when you when you set up a system like

00:10:57,980 --> 00:11:01,459
this you want a sort of over provision

00:10:59,660 --> 00:11:03,860
and then tune down right so remove a

00:11:01,459 --> 00:11:05,899
controller or remove some does so we

00:11:03,860 --> 00:11:07,190
specifically over provisioned a little

00:11:05,899 --> 00:11:09,080
bit but

00:11:07,190 --> 00:11:12,110
when you want a screaming fast i/o

00:11:09,080 --> 00:11:15,410
subsystem you've got to look at how your

00:11:12,110 --> 00:11:17,390
controller connects to your drive cage

00:11:15,410 --> 00:11:20,240
in your drives so these particular

00:11:17,390 --> 00:11:22,280
controllers have four cables coming off

00:11:20,240 --> 00:11:25,340
them within each cable there's two sasse

00:11:22,280 --> 00:11:28,190
lanes and basically you that way you can

00:11:25,340 --> 00:11:30,620
have a the drive cage takes eight disks

00:11:28,190 --> 00:11:33,560
so with four cables and two lanes you

00:11:30,620 --> 00:11:36,110
get eight lanes / controller going to a

00:11:33,560 --> 00:11:37,910
drive cage right so we could fully drive

00:11:36,110 --> 00:11:40,190
eight drives without them having to

00:11:37,910 --> 00:11:41,600
share lanes and run into bandwidth

00:11:40,190 --> 00:11:44,780
issues that would later affect

00:11:41,600 --> 00:11:47,780
throughput issues right and so since we

00:11:44,780 --> 00:11:49,580
had to drive cages we had 16 disks and

00:11:47,780 --> 00:11:51,410
therefore we had two controllers so we

00:11:49,580 --> 00:11:54,020
could fully drive or 16 disks without

00:11:51,410 --> 00:11:56,630
running into and you'll see later we had

00:11:54,020 --> 00:11:58,760
great i/o performance and then we do

00:11:56,630 --> 00:12:03,590
fall bonded one gig in X so that you

00:11:58,760 --> 00:12:06,940
have roughly four gigabits of throughput

00:12:03,590 --> 00:12:10,970
on your necks and we do that at the

00:12:06,940 --> 00:12:12,710
configured out through linux ok so the

00:12:10,970 --> 00:12:15,040
keys to capture the data use whatever

00:12:12,710 --> 00:12:19,610
framework that you're comfortable with

00:12:15,040 --> 00:12:21,560
Paul our performance architect was most

00:12:19,610 --> 00:12:25,670
familiar with the Linux our tool ganglia

00:12:21,560 --> 00:12:27,260
is another popular option but I'll sort

00:12:25,670 --> 00:12:30,650
of walk through quickly what the way we

00:12:27,260 --> 00:12:32,810
captured this right so you have an outer

00:12:30,650 --> 00:12:34,580
script and it shells into every node in

00:12:32,810 --> 00:12:36,380
the Hadoop cluster and starts the Linux

00:12:34,580 --> 00:12:38,390
our tool and it starts capturing metrics

00:12:36,380 --> 00:12:40,100
the outer script once it's done that

00:12:38,390 --> 00:12:42,740
goes and then submits the terror sort

00:12:40,100 --> 00:12:45,800
job Terra sort then runs to completion

00:12:42,740 --> 00:12:47,990
and then the sort then it shells back

00:12:45,800 --> 00:12:50,810
into the nodes it stops are and then

00:12:47,990 --> 00:12:52,850
basically it takes the DAT files thats

00:12:50,810 --> 00:12:56,300
our generates and it converts them to

00:12:52,850 --> 00:12:58,400
CSV we load that into my sequel and then

00:12:56,300 --> 00:13:00,920
we do ad hoc analytics on this and then

00:12:58,400 --> 00:13:03,170
we use Excel to from my sequel to

00:13:00,920 --> 00:13:05,120
generate our charts right and that looks

00:13:03,170 --> 00:13:07,700
just like that right so that's in a sort

00:13:05,120 --> 00:13:10,730
of an example of us piping out you can

00:13:07,700 --> 00:13:13,040
sort of see the top right i have the cpu

00:13:10,730 --> 00:13:14,990
utilization information then the i/o

00:13:13,040 --> 00:13:17,839
rate and then the network devices right

00:13:14,990 --> 00:13:19,910
so once this is piped out into my sequel

00:13:17,839 --> 00:13:20,130
we can really do ad hoc analytics so if

00:13:19,910 --> 00:13:22,500
we

00:13:20,130 --> 00:13:24,030
to dive down into one particular node we

00:13:22,500 --> 00:13:25,350
can just do it through sequence sort of

00:13:24,030 --> 00:13:29,970
see what was going on if there are any

00:13:25,350 --> 00:13:33,120
anonymous anomalies so now let's look at

00:13:29,970 --> 00:13:35,130
the data so the first thing you know

00:13:33,120 --> 00:13:37,860
really what you want to achieve is you

00:13:35,130 --> 00:13:40,470
want to get the CPU as busy as possible

00:13:37,860 --> 00:13:42,240
without the CPU waiting on any sort of

00:13:40,470 --> 00:13:43,980
i/o subsystem whether it's the disk

00:13:42,240 --> 00:13:46,170
subsystem or the network device that's

00:13:43,980 --> 00:13:47,880
right and so you want to make sure that

00:13:46,170 --> 00:13:49,740
you're not I owe bound or network bound

00:13:47,880 --> 00:13:51,360
and that you can keep your CPU busy and

00:13:49,740 --> 00:13:53,760
doing as much work for you as possible

00:13:51,360 --> 00:13:56,490
right that'll improve your time to

00:13:53,760 --> 00:13:59,370
completion so the first thing is you

00:13:56,490 --> 00:14:02,280
want to get a sort of a baseline of how

00:13:59,370 --> 00:14:03,810
good your i/o subsystem performs outside

00:14:02,280 --> 00:14:06,720
of her deep right which is you run the

00:14:03,810 --> 00:14:08,670
DD test if you curious about this just

00:14:06,720 --> 00:14:09,960
google it do you know DeeDee test io

00:14:08,670 --> 00:14:11,490
throughput and there's lots of great

00:14:09,960 --> 00:14:14,850
blog posts that explain how to do this

00:14:11,490 --> 00:14:17,100
but this will basically give you a idea

00:14:14,850 --> 00:14:19,830
of your throughput of your i/o subsystem

00:14:17,100 --> 00:14:21,930
right and so usually what you can expect

00:14:19,830 --> 00:14:24,750
is it's 100 megabytes a second per disk

00:14:21,930 --> 00:14:27,660
and so depending on how many does you

00:14:24,750 --> 00:14:29,580
have you have an aggregate so it's one

00:14:27,660 --> 00:14:31,620
point six gigabytes a second of read

00:14:29,580 --> 00:14:34,890
throughput so now I'm going to explain

00:14:31,620 --> 00:14:36,870
the chart on the x-axis it's at time

00:14:34,890 --> 00:14:40,800
right progression of time and on the

00:14:36,870 --> 00:14:42,870
y-axis it's megabytes a second so now we

00:14:40,800 --> 00:14:44,490
can go to one point six gigabytes a

00:14:42,870 --> 00:14:46,200
second so the chart really is sort of

00:14:44,490 --> 00:14:48,810
chopped off halfway in the middle right

00:14:46,200 --> 00:14:51,870
so we're really just getting ten percent

00:14:48,810 --> 00:14:54,600
total throughput utilization and we're

00:14:51,870 --> 00:14:57,510
not troubling r io subsystem whatsoever

00:14:54,600 --> 00:14:59,730
right so we've actually over provisioned

00:14:57,510 --> 00:15:01,620
the IO subsystem we could save a lot of

00:14:59,730 --> 00:15:04,050
costs by tuning this down because we're

00:15:01,620 --> 00:15:07,010
not really using it much the next thing

00:15:04,050 --> 00:15:10,680
to check is the network subsystem and

00:15:07,010 --> 00:15:12,150
similarly am we can drive for gigabits a

00:15:10,680 --> 00:15:14,280
second because the necks are bonded

00:15:12,150 --> 00:15:17,070
roughly there abouts which if you do

00:15:14,280 --> 00:15:19,020
some simple math maps to 400 megabytes a

00:15:17,070 --> 00:15:22,860
second right and so that's what you're

00:15:19,020 --> 00:15:26,130
seeing on the y-axis is in increments of

00:15:22,860 --> 00:15:28,170
20 right so again should go out to 400

00:15:26,130 --> 00:15:31,260
but you know we're just going to chop it

00:15:28,170 --> 00:15:34,230
off at 160 and on the x-axis again it's

00:15:31,260 --> 00:15:37,590
elapsed time so you're seeing the net

00:15:34,230 --> 00:15:40,590
net is that we're at a quarter of our

00:15:37,590 --> 00:15:42,660
network capacity here and so the key

00:15:40,590 --> 00:15:44,550
thing to keep in mind is tarah so it's

00:15:42,660 --> 00:15:48,090
not a particularly Network intensive

00:15:44,550 --> 00:15:51,510
workload if you change this to test DFS

00:15:48,090 --> 00:15:53,220
i/o which basically rights to the HDFS

00:15:51,510 --> 00:15:55,020
and it creates files and in those files

00:15:53,220 --> 00:15:57,780
are then blocks and then those blocks or

00:15:55,020 --> 00:15:59,670
replicated across nodes you'll light up

00:15:57,780 --> 00:16:02,220
the next right and it'll you know start

00:15:59,670 --> 00:16:04,440
pig in your neck infrastructure but if

00:16:02,220 --> 00:16:06,060
you're not network intensive you know

00:16:04,440 --> 00:16:07,140
you can kind of see that like a lot of

00:16:06,060 --> 00:16:09,450
people are saying oh you need to go to

00:16:07,140 --> 00:16:11,130
10 gig e or InfiniBand for Hadoop you

00:16:09,450 --> 00:16:13,470
can see here for a 10 terabyte tear

00:16:11,130 --> 00:16:15,120
assault we're barely you know scratching

00:16:13,470 --> 00:16:17,430
at the surface of what our next can do

00:16:15,120 --> 00:16:18,900
on a one gig e so just keep in mind you

00:16:17,430 --> 00:16:21,990
know you should profile and you will

00:16:18,900 --> 00:16:23,940
understand but better so on the cpu

00:16:21,990 --> 00:16:25,560
subsystem this is you know theoretically

00:16:23,940 --> 00:16:27,810
you could short cut this all and just

00:16:25,560 --> 00:16:29,850
look at this guy and there's two metrics

00:16:27,810 --> 00:16:32,790
here and which is you want to keep the

00:16:29,850 --> 00:16:34,920
cpu as busy as possible but the io wait

00:16:32,790 --> 00:16:37,380
time has got to be really low right

00:16:34,920 --> 00:16:39,930
which means that the CPUs not waiting on

00:16:37,380 --> 00:16:43,800
instructions from the network or just

00:16:39,930 --> 00:16:46,710
subsystem right and so you can see here

00:16:43,800 --> 00:16:48,900
are I OS wait times near negligible and

00:16:46,710 --> 00:16:51,570
you cannot most likely guess when the

00:16:48,900 --> 00:16:53,250
map phase ends and when the the reduced

00:16:51,570 --> 00:16:55,440
thoughts and the shuffle on sorts get

00:16:53,250 --> 00:16:59,010
gets a little busier there but you know

00:16:55,440 --> 00:17:00,270
it's it's it's very low that's right so

00:16:59,010 --> 00:17:03,200
this is this is a good shot this is

00:17:00,270 --> 00:17:05,520
exactly where we want to be on our CPU

00:17:03,200 --> 00:17:08,250
then the memory subsystem is an

00:17:05,520 --> 00:17:10,560
interesting anecdotal and cha ta which

00:17:08,250 --> 00:17:12,840
is basically that we're cashing a lot of

00:17:10,560 --> 00:17:14,910
data in memory right so what you're

00:17:12,840 --> 00:17:18,540
seeing here on x-axis is elapsed time

00:17:14,910 --> 00:17:20,880
and y-axis is memory utilization and

00:17:18,540 --> 00:17:22,680
you're comparing the blue which is the

00:17:20,880 --> 00:17:24,750
memory usage with the red which is the

00:17:22,680 --> 00:17:26,310
percentage cash so there's lots of data

00:17:24,750 --> 00:17:27,480
that's being cached in memory which

00:17:26,310 --> 00:17:29,040
means that there's a lot of memory

00:17:27,480 --> 00:17:31,830
available to cache data and they want

00:17:29,040 --> 00:17:34,520
processes consuming that memory right so

00:17:31,830 --> 00:17:37,460
this is probably a stronger indicators

00:17:34,520 --> 00:17:39,540
correlated I want to say it's causal but

00:17:37,460 --> 00:17:42,900
correlated to why there's probably a lot

00:17:39,540 --> 00:17:45,750
as a minimal I oh wait time on the this

00:17:42,900 --> 00:17:48,820
slide below because if it needs data

00:17:45,750 --> 00:17:51,470
it's in memory right

00:17:48,820 --> 00:17:54,020
okay so now at this point we're like

00:17:51,470 --> 00:17:55,850
well you know we did a great job pat

00:17:54,020 --> 00:17:59,300
ourselves on the back where we've

00:17:55,850 --> 00:18:02,630
designed a smoking fast Hadoop worker

00:17:59,300 --> 00:18:04,190
node but it's an expensive one right so

00:18:02,630 --> 00:18:06,890
if you're in this position what are some

00:18:04,190 --> 00:18:08,480
of the types of things that you know say

00:18:06,890 --> 00:18:10,300
you're in the position where you could

00:18:08,480 --> 00:18:14,650
found that you were actually i/o bound

00:18:10,300 --> 00:18:18,080
or CPU bound or something to that effect

00:18:14,650 --> 00:18:20,900
what could you do to fix it and so this

00:18:18,080 --> 00:18:23,290
slide is sort of like the sort of

00:18:20,900 --> 00:18:27,050
high-level best practice slide around

00:18:23,290 --> 00:18:28,660
tuning right so the first thing to do is

00:18:27,050 --> 00:18:31,180
to talk to your data center folks

00:18:28,660 --> 00:18:33,530
there's a really interesting stat that

00:18:31,180 --> 00:18:36,860
generally true across all data centers

00:18:33,530 --> 00:18:38,960
which is that if your rack cost you

00:18:36,860 --> 00:18:40,400
three hundred thousand dollars it's

00:18:38,960 --> 00:18:41,900
going to cost you a hundred thousand

00:18:40,400 --> 00:18:44,270
dollars in power and cooling costs per

00:18:41,900 --> 00:18:45,860
year to maintain it right that's pretty

00:18:44,270 --> 00:18:48,680
amazing that when I learned that I was

00:18:45,860 --> 00:18:50,660
sort of flabbergasted so it's you know

00:18:48,680 --> 00:18:53,240
getting a correct power and cooling

00:18:50,660 --> 00:18:55,310
footprint is key in a large scale art

00:18:53,240 --> 00:18:56,780
environment and so the first thing you

00:18:55,310 --> 00:18:58,790
should do is go and talk to your data

00:18:56,780 --> 00:19:01,460
center ops folks and ask them how much

00:18:58,790 --> 00:19:03,380
floor space do I have and typically you

00:19:01,460 --> 00:19:05,840
put a rack on a particular floor tile

00:19:03,380 --> 00:19:07,910
and so if the guys is or gal says

00:19:05,840 --> 00:19:11,090
there's only five floor tiles available

00:19:07,910 --> 00:19:13,640
and you can only fit in five racks but

00:19:11,090 --> 00:19:15,980
yet you know you're still constrained

00:19:13,640 --> 00:19:18,020
for a Hadoop job to complete in a

00:19:15,980 --> 00:19:20,120
particular amount of time and so you

00:19:18,020 --> 00:19:23,510
can't scale out to improve performance

00:19:20,120 --> 00:19:26,930
and by using parallelization you have to

00:19:23,510 --> 00:19:30,050
buy beefier notes right so you basically

00:19:26,930 --> 00:19:32,000
have to buy more compute power and you

00:19:30,050 --> 00:19:34,070
know that because you constrained you

00:19:32,000 --> 00:19:35,180
can't scale up so that that's key and

00:19:34,070 --> 00:19:38,020
something you don't want to find out

00:19:35,180 --> 00:19:40,820
after the fact and the next thing is

00:19:38,020 --> 00:19:43,760
power so that the the data center

00:19:40,820 --> 00:19:45,200
persons usually only power provide a

00:19:43,760 --> 00:19:48,590
certain amount of power to each floor

00:19:45,200 --> 00:19:50,750
tile and so you got to figure out how

00:19:48,590 --> 00:19:52,330
you constrain there so now assuming you

00:19:50,750 --> 00:19:56,300
have enough power for your particular

00:19:52,330 --> 00:19:58,910
rack architecture when it's all powered

00:19:56,300 --> 00:20:00,860
up you then have to cool that thing so

00:19:58,910 --> 00:20:02,809
there's there's a particular thermal

00:20:00,860 --> 00:20:04,010
footprint that they can support and

00:20:02,809 --> 00:20:05,540
they're quite you can't exceed it

00:20:04,010 --> 00:20:07,580
otherwise they can't cool it and then

00:20:05,540 --> 00:20:09,950
things start catching on fire which

00:20:07,580 --> 00:20:11,510
isn't good so that's something else that

00:20:09,950 --> 00:20:13,370
you have to figure out so you figure

00:20:11,510 --> 00:20:17,059
power cooling floor space and this

00:20:13,370 --> 00:20:18,530
dictates everything else right so okay

00:20:17,059 --> 00:20:20,679
so now let's talk about the i/o

00:20:18,530 --> 00:20:25,070
subsystem so there's two sort of key

00:20:20,679 --> 00:20:27,559
disk types for Hadoop right there's SATA

00:20:25,070 --> 00:20:32,870
and there is midline SAS so it's really

00:20:27,559 --> 00:20:34,640
7200 RPM versus 10,000 RPM and SATA will

00:20:32,870 --> 00:20:37,520
give you a midline SAS will give you

00:20:34,640 --> 00:20:39,919
about a forty percent boost in

00:20:37,520 --> 00:20:42,290
performance on Io workloads at about a

00:20:39,919 --> 00:20:44,240
twenty percent increase in price so you

00:20:42,290 --> 00:20:46,880
know if you can afford medline SAS good

00:20:44,240 --> 00:20:48,980
for you it's but if you can't if you

00:20:46,880 --> 00:20:51,169
really have a huge environment you

00:20:48,980 --> 00:20:54,500
should just go with large form factor

00:20:51,169 --> 00:20:57,350
setup so so those are your different

00:20:54,500 --> 00:20:59,960
types of disks and the amount of disk is

00:20:57,350 --> 00:21:03,860
the next thing right so why did I say 12

00:20:59,960 --> 00:21:05,660
disks versus four disks so with the you

00:21:03,860 --> 00:21:07,549
know if you've got dual socket six cores

00:21:05,660 --> 00:21:09,919
so you've got 12 cores these are Intel

00:21:07,549 --> 00:21:12,950
processors they're hyper threaded so you

00:21:09,919 --> 00:21:15,740
can you know sort of like 1.5 times the

00:21:12,950 --> 00:21:18,290
amount of cores or up to two that you

00:21:15,740 --> 00:21:20,270
can get if you double the amount but

00:21:18,290 --> 00:21:22,700
this comes down to MapReduce slides

00:21:20,270 --> 00:21:24,320
right so how many MapReduce sites do you

00:21:22,700 --> 00:21:28,010
want to configure you can configure a

00:21:24,320 --> 00:21:31,580
lot I mean we have 30 configured on on

00:21:28,010 --> 00:21:33,080
this particular run on Terra sort now

00:21:31,580 --> 00:21:35,630
imagine you just had two discs right

00:21:33,080 --> 00:21:37,340
those discs both have blocks for a

00:21:35,630 --> 00:21:38,660
number of different file sets right and

00:21:37,340 --> 00:21:41,260
you could be running depending on your

00:21:38,660 --> 00:21:44,299
scheduler a number of different jobs

00:21:41,260 --> 00:21:46,460
where you have tasks on a particular

00:21:44,299 --> 00:21:47,929
node to say you have 10 slots with tyce

00:21:46,460 --> 00:21:49,910
training in each slide from a different

00:21:47,929 --> 00:21:53,000
job or running against a different block

00:21:49,910 --> 00:21:54,380
all hitting the same disk right so then

00:21:53,000 --> 00:21:56,390
basically in that case you're going to

00:21:54,380 --> 00:21:59,720
create interleaved i/o and queuing right

00:21:56,390 --> 00:22:01,429
on the disk access right so you want to

00:21:59,720 --> 00:22:03,049
be have many disks that you can spread

00:22:01,429 --> 00:22:04,580
those requests out and reduce the

00:22:03,049 --> 00:22:06,380
likelihood of that happening it's going

00:22:04,580 --> 00:22:08,390
to happen but you can just reduce the

00:22:06,380 --> 00:22:11,240
amount of it happening so that's why you

00:22:08,390 --> 00:22:13,850
want a lot of dis then you want then the

00:22:11,240 --> 00:22:14,840
discussion happens on type of dust right

00:22:13,850 --> 00:22:16,670
so you get to know

00:22:14,840 --> 00:22:18,440
half inch discs that the industry

00:22:16,670 --> 00:22:20,780
typically refers to a small form-factor

00:22:18,440 --> 00:22:22,970
discs and three and a half inch discs

00:22:20,780 --> 00:22:25,610
that you're the bigger so their costs

00:22:22,970 --> 00:22:28,040
the same but the small form factor does

00:22:25,610 --> 00:22:29,660
select one terabyte and the lodge form

00:22:28,040 --> 00:22:31,700
factor discs are about two to three

00:22:29,660 --> 00:22:34,040
terabytes right so you get much more

00:22:31,700 --> 00:22:36,680
storage capacity and a large form factor

00:22:34,040 --> 00:22:38,660
this versus a small form factor this so

00:22:36,680 --> 00:22:44,090
that's typically why I recommend going

00:22:38,660 --> 00:22:45,860
with a a to you server with at least 12

00:22:44,090 --> 00:22:48,170
large form factor drives because that

00:22:45,860 --> 00:22:50,570
way you have the drive count to improve

00:22:48,170 --> 00:22:52,340
spread the iOS around but you also have

00:22:50,570 --> 00:22:54,410
the storage capacity that you want an

00:22:52,340 --> 00:22:58,700
HDFS while keeping the price constant

00:22:54,410 --> 00:23:00,890
right so the next asked next thing is

00:22:58,700 --> 00:23:04,070
number of controllers right so I spoke

00:23:00,890 --> 00:23:07,610
earlier about you want to dedicated lean

00:23:04,070 --> 00:23:09,170
to each disk right and so the second

00:23:07,610 --> 00:23:11,360
controller is going to add a couple of

00:23:09,170 --> 00:23:13,280
hundred dollars right depending on how

00:23:11,360 --> 00:23:18,080
nice the controller is right and the

00:23:13,280 --> 00:23:19,960
types of features it provides you you

00:23:18,080 --> 00:23:23,060
can save money I mean if you're buying

00:23:19,960 --> 00:23:26,780
600 servers you know that that 500 bucks

00:23:23,060 --> 00:23:30,590
per server adds up right so you can save

00:23:26,780 --> 00:23:33,350
costs by going with just a single

00:23:30,590 --> 00:23:36,110
controller instead of two and with with

00:23:33,350 --> 00:23:37,730
the small loss on Io performance that

00:23:36,110 --> 00:23:39,560
you're going to run into for i/o

00:23:37,730 --> 00:23:40,940
intensive workloads because what's going

00:23:39,560 --> 00:23:42,320
to happen is you'll start sharing lanes

00:23:40,940 --> 00:23:44,750
which means you lose a bit of bandwidth

00:23:42,320 --> 00:23:46,250
than your throughput decreases of it so

00:23:44,750 --> 00:23:47,630
just something to think about but that's

00:23:46,250 --> 00:23:50,120
actually what I'd recommend is one

00:23:47,630 --> 00:23:52,430
controller with 12 disks for a good

00:23:50,120 --> 00:23:55,930
price point so then the next aspect is

00:23:52,430 --> 00:23:58,340
power and compute right so intel and

00:23:55,930 --> 00:23:59,690
i'll go into non intel as well in this

00:23:58,340 --> 00:24:01,850
presentation later because there's some

00:23:59,690 --> 00:24:05,690
really exciting stuff happening around

00:24:01,850 --> 00:24:07,520
md sorry arm an intel atom like the

00:24:05,690 --> 00:24:12,140
low-power processors that have moved

00:24:07,520 --> 00:24:14,540
over from mobile devices so there's the

00:24:12,140 --> 00:24:17,270
socket our processes which are like 130

00:24:14,540 --> 00:24:20,210
wat chips that's the 2.9 gigahertz one

00:24:17,270 --> 00:24:22,610
that I Hughes but the fact our footprint

00:24:20,210 --> 00:24:25,190
is quite a bit higher than the socket B

00:24:22,610 --> 00:24:26,540
which are like 95 wat so the basic

00:24:25,190 --> 00:24:28,860
difference there is they're both six

00:24:26,540 --> 00:24:31,679
core chips the higher power

00:24:28,860 --> 00:24:33,740
powered ones run at 2.9 gigahertz the

00:24:31,679 --> 00:24:36,720
lower powered ones run at 2.4 gigahertz

00:24:33,740 --> 00:24:38,400
for Hadoop at scale for a generic

00:24:36,720 --> 00:24:40,980
architecture I would remain regularly

00:24:38,400 --> 00:24:43,200
recommend the lower power the 95 white I

00:24:40,980 --> 00:24:44,990
don't think the half a gigahertz is

00:24:43,200 --> 00:24:48,000
going to make that much of a difference

00:24:44,990 --> 00:24:51,150
amount of course four to six to eight

00:24:48,000 --> 00:24:53,700
cores at a price point at what commodity

00:24:51,150 --> 00:24:57,030
price point i'd recommend six cores at

00:24:53,700 --> 00:24:58,799
this point in time another thing that

00:24:57,030 --> 00:25:00,570
you need a factor in is memory channels

00:24:58,799 --> 00:25:04,049
different processor types have different

00:25:00,570 --> 00:25:07,440
memory channels so the ones that so the

00:25:04,049 --> 00:25:10,320
the socket are the 131 what processors

00:25:07,440 --> 00:25:12,600
have full memory channels per processor

00:25:10,320 --> 00:25:15,059
right you have two processors so you

00:25:12,600 --> 00:25:16,890
have eight right so if you're using

00:25:15,059 --> 00:25:20,370
eight gigabyte dems that allow you to

00:25:16,890 --> 00:25:22,620
have a 64 gigabytes ram for that node if

00:25:20,370 --> 00:25:24,240
you're using HBase or some sort of no

00:25:22,620 --> 00:25:26,070
sequel solution that's really memory

00:25:24,240 --> 00:25:28,919
intensive you might want to go with that

00:25:26,070 --> 00:25:31,380
option if you go with the socket be down

00:25:28,919 --> 00:25:34,679
you have three channels per node and so

00:25:31,380 --> 00:25:37,770
you know at eight gigabytes that's forty

00:25:34,679 --> 00:25:42,870
eight gig of ram right so you have sorry

00:25:37,770 --> 00:25:44,220
24 yeah so so you have less memory

00:25:42,870 --> 00:25:45,450
that's available to you you know you

00:25:44,220 --> 00:25:48,210
just need to factor that in and the

00:25:45,450 --> 00:25:50,790
processor that you pick something that I

00:25:48,210 --> 00:25:53,970
don't think is that well-known is there

00:25:50,790 --> 00:25:56,100
are special kinds of dims so you know

00:25:53,970 --> 00:25:58,950
people freak out a bit about the name

00:25:56,100 --> 00:26:02,429
node limitation around you know each

00:25:58,950 --> 00:26:04,350
block is like 150 bites and you know

00:26:02,429 --> 00:26:07,200
when you run out of memory you can't you

00:26:04,350 --> 00:26:11,010
know you can't you know your fault you

00:26:07,200 --> 00:26:13,530
namespaces sort of frozen rights and you

00:26:11,010 --> 00:26:16,110
know the standard commodity service can

00:26:13,530 --> 00:26:17,669
go up to almost 800 gig of ram so you

00:26:16,110 --> 00:26:19,559
know it's just the dims are a lot more

00:26:17,669 --> 00:26:21,120
expensive but you can do that so if you

00:26:19,559 --> 00:26:24,390
want a super beefy name node you're not

00:26:21,120 --> 00:26:28,530
blocked at 64 you can I think one of

00:26:24,390 --> 00:26:29,850
them goes up to 968 so that that's just

00:26:28,530 --> 00:26:31,290
something I don't feel like the

00:26:29,850 --> 00:26:35,549
community is particularly well aware of

00:26:31,290 --> 00:26:37,169
okay next thing is the network remember

00:26:35,549 --> 00:26:39,540
I mentioned about the difference between

00:26:37,169 --> 00:26:42,240
enterprise and web scale if you talk to

00:26:39,540 --> 00:26:44,010
the yahoo guys that ran opposite

00:26:42,240 --> 00:26:46,679
for their Hadoop cluster they always

00:26:44,010 --> 00:26:48,480
only had one switch in their rack but

00:26:46,679 --> 00:26:50,370
you know in the same way that if you

00:26:48,480 --> 00:26:52,559
lose the node if you lose a disk on a

00:26:50,370 --> 00:26:55,200
node that has your OS that whole node

00:26:52,559 --> 00:26:58,110
goes offline well imagine what happens

00:26:55,200 --> 00:27:00,120
if you've got you know 20 nodes in Iraq

00:26:58,110 --> 00:27:01,710
all connected to a single top-of-rack

00:27:00,120 --> 00:27:03,630
switch that's connected to your core

00:27:01,710 --> 00:27:06,809
aggregation switch and you lose that

00:27:03,630 --> 00:27:08,760
switch the whole rack goes offline what

00:27:06,809 --> 00:27:10,679
does who do when the rep when a node

00:27:08,760 --> 00:27:12,450
goes offline it starts rebalancing

00:27:10,679 --> 00:27:14,640
you're going to create a network storm

00:27:12,450 --> 00:27:17,330
that if your network isn't big enough it

00:27:14,640 --> 00:27:19,830
will crash your Hadoop cluster right so

00:27:17,330 --> 00:27:22,260
what you want is if you have a small

00:27:19,830 --> 00:27:24,690
cluster by two switches right have a

00:27:22,260 --> 00:27:27,240
redundant switch so that you're covered

00:27:24,690 --> 00:27:29,309
in that event right now the yahoo guys

00:27:27,240 --> 00:27:32,340
will say not at you know different scale

00:27:29,309 --> 00:27:34,950
dynamics they had so many so many nodes

00:27:32,340 --> 00:27:36,480
and so many racks that if they lost iraq

00:27:34,950 --> 00:27:38,690
it wasn't a big deal it's kind of like

00:27:36,480 --> 00:27:42,510
losing just one node which is impressive

00:27:38,690 --> 00:27:44,640
i mentioned server necks again you want

00:27:42,510 --> 00:27:46,260
to have at least two right so if you

00:27:44,640 --> 00:27:47,880
lose a neck your note doesn't go offline

00:27:46,260 --> 00:27:49,770
and you start reading it so have at

00:27:47,880 --> 00:27:51,059
least two but you'll find that the

00:27:49,770 --> 00:27:53,490
commodity most of the time they just

00:27:51,059 --> 00:27:56,309
come with for a flexible land on

00:27:53,490 --> 00:27:58,559
motherboard right has a network Clyde

00:27:56,309 --> 00:28:02,340
and that card is for necks or two tank

00:27:58,559 --> 00:28:03,929
for one gig e or to 10 gig in X so

00:28:02,340 --> 00:28:05,550
that's good because it improves your

00:28:03,929 --> 00:28:09,150
network throughput as well as gives you

00:28:05,550 --> 00:28:11,400
additional redundancy something you need

00:28:09,150 --> 00:28:14,520
to figure out you need to make sure that

00:28:11,400 --> 00:28:18,240
if you're using four ports net for

00:28:14,520 --> 00:28:21,480
network ports on each server and you

00:28:18,240 --> 00:28:24,300
have enough ports available in your

00:28:21,480 --> 00:28:26,970
switches for all of that I mean we were

00:28:24,300 --> 00:28:29,429
when we what cabled up those clusters

00:28:26,970 --> 00:28:33,690
you saw in the demo there there was like

00:28:29,429 --> 00:28:35,429
four ports left open right so you know

00:28:33,690 --> 00:28:36,720
if and those were for to you servers if

00:28:35,429 --> 00:28:39,120
you're going to one you serve as we were

00:28:36,720 --> 00:28:41,220
to run out of port right so just factor

00:28:39,120 --> 00:28:42,840
that in in your design the nice thing is

00:28:41,220 --> 00:28:44,640
I don't actually have any data points

00:28:42,840 --> 00:28:46,200
about this so this is a one of those

00:28:44,640 --> 00:28:50,669
dangerous paper exercises that I

00:28:46,200 --> 00:28:52,860
mentioned but the switches with deep

00:28:50,669 --> 00:28:55,260
packet buffering Hadoop is a bursty

00:28:52,860 --> 00:28:58,020
framework right it's basically

00:28:55,260 --> 00:29:00,360
if you watch watch a Hadoop job

00:28:58,020 --> 00:29:02,640
visualize you'll basically see you know

00:29:00,360 --> 00:29:05,010
the iOS spins up and the CPU spins up

00:29:02,640 --> 00:29:06,750
the network stays quiet and then it gets

00:29:05,010 --> 00:29:09,690
to the shuffle and sort and boom the

00:29:06,750 --> 00:29:12,000
next slider right and so switches with

00:29:09,690 --> 00:29:13,410
the buffering will ensure that packets

00:29:12,000 --> 00:29:15,570
don't get dropped right it can just

00:29:13,410 --> 00:29:18,240
buffer the packets coming in and then

00:29:15,570 --> 00:29:22,830
process them later with that before the

00:29:18,240 --> 00:29:24,420
packets timeout okay so however you want

00:29:22,830 --> 00:29:26,490
to avoid introducing this new phenomenon

00:29:24,420 --> 00:29:29,670
what I call it who do Bob's Wacom all

00:29:26,490 --> 00:29:31,440
right which is I just explained how to

00:29:29,670 --> 00:29:35,190
tune a cluster for one particular

00:29:31,440 --> 00:29:36,180
workload well imagine you're responsible

00:29:35,190 --> 00:29:38,460
for maintaining the Hadoop

00:29:36,180 --> 00:29:40,740
infrastructure in your large company and

00:29:38,460 --> 00:29:42,810
you have Hadoop clusters popping up

00:29:40,740 --> 00:29:44,670
everywhere you get this wacom elephant

00:29:42,810 --> 00:29:46,830
phenomenon right it's much more

00:29:44,670 --> 00:29:49,500
cost-effective and efficient to

00:29:46,830 --> 00:29:51,150
basically have a single shared managed

00:29:49,500 --> 00:29:53,040
service right so you have one large

00:29:51,150 --> 00:29:55,050
cluster that you can share equitably

00:29:53,040 --> 00:29:57,600
among all the different internal

00:29:55,050 --> 00:30:00,150
customers right so I'm going to explain

00:29:57,600 --> 00:30:02,340
how to do this right part of the multi

00:30:00,150 --> 00:30:06,030
tenant multitenancy within Hadoop is

00:30:02,340 --> 00:30:08,430
really immature so I'm gonna explain the

00:30:06,030 --> 00:30:10,320
infrastructure but there's different

00:30:08,430 --> 00:30:15,030
schedulers that you can use to do this

00:30:10,320 --> 00:30:17,460
and you can you know ensure that you can

00:30:15,030 --> 00:30:19,950
assign you know you can constrain uses

00:30:17,460 --> 00:30:21,480
of HDFS to certain customers that you

00:30:19,950 --> 00:30:23,040
can correlate to the ones that you're

00:30:21,480 --> 00:30:25,170
scheduling jobsworth right in your

00:30:23,040 --> 00:30:28,340
scheduler but it's not that much more

00:30:25,170 --> 00:30:30,840
sophisticated but okay so if you want to

00:30:28,340 --> 00:30:32,790
build a nice scale out cluster this is

00:30:30,840 --> 00:30:34,230
really what i recommend right you want

00:30:32,790 --> 00:30:38,130
for your name node and job tracker you

00:30:34,230 --> 00:30:41,520
want a 1u 64 gig of ram to dual socket

00:30:38,130 --> 00:30:45,240
2.9 gigahertz six core with four small

00:30:41,520 --> 00:30:47,670
form-factor disks raid murud right so

00:30:45,240 --> 00:30:49,950
basically knit knit this is like beefy

00:30:47,670 --> 00:30:52,110
CPU beefy memory it's going to do great

00:30:49,950 --> 00:30:54,000
for your job tracker it could do great

00:30:52,110 --> 00:30:56,520
for your name node but doesn't need a

00:30:54,000 --> 00:30:58,590
lot of disks for your Hadoop slaves you

00:30:56,520 --> 00:31:01,800
want forty eight gig of ram will work

00:30:58,590 --> 00:31:03,780
well with HBase and MapReduce and Impala

00:31:01,800 --> 00:31:05,820
and you know whatever else other

00:31:03,780 --> 00:31:07,220
computer yarn or whatever the

00:31:05,820 --> 00:31:12,110
computational framework store

00:31:07,220 --> 00:31:15,049
running on top of HDFS dual socket six

00:31:12,110 --> 00:31:17,390
core low-power the socket be processors

00:31:15,049 --> 00:31:19,760
so you keep your steady state a data

00:31:17,390 --> 00:31:21,620
center costs as low as possible you have

00:31:19,760 --> 00:31:24,590
one high performing disk controller that

00:31:21,620 --> 00:31:26,090
I mentioned earlier not to 12 2 terabyte

00:31:24,590 --> 00:31:28,850
lodge form factor disks in Jade

00:31:26,090 --> 00:31:30,830
configuration that gives you 24

00:31:28,850 --> 00:31:33,080
terabytes of storage capacity per node

00:31:30,830 --> 00:31:35,120
with a pretty performance set up and

00:31:33,080 --> 00:31:36,890
it's low power consumption now I

00:31:35,120 --> 00:31:40,940
mentioned Jay Boyd and I thought I

00:31:36,890 --> 00:31:43,400
should clarify this right typically you

00:31:40,940 --> 00:31:45,230
know this the servers that you get 12

00:31:43,400 --> 00:31:47,059
dusts usually come in 14 disks as well

00:31:45,230 --> 00:31:48,860
you'll typically have two at the back of

00:31:47,059 --> 00:31:51,110
the server that are specifically

00:31:48,860 --> 00:31:54,740
intended to mirror the OS and the other

00:31:51,110 --> 00:31:56,360
12 to be data drives now when you buy

00:31:54,740 --> 00:31:58,429
that controller it's typically a raid

00:31:56,360 --> 00:32:00,530
controller right and it can be tricky to

00:31:58,429 --> 00:32:02,120
get raid controllers to do j-bot setup

00:32:00,530 --> 00:32:04,520
so what you do is you set up each

00:32:02,120 --> 00:32:06,919
individual disk as its own individual

00:32:04,520 --> 00:32:08,840
raid 0 volume so you don't take or 12

00:32:06,919 --> 00:32:11,179
and create one raid 0 volume from them

00:32:08,840 --> 00:32:13,480
you have raid 0 bottom raid 0 volume

00:32:11,179 --> 00:32:17,530
raid 0 it's effectively a j board and

00:32:13,480 --> 00:32:20,539
you will see papers out there there's a

00:32:17,530 --> 00:32:22,970
section in Tom White's book there's a

00:32:20,539 --> 00:32:25,610
recent blog post from Cloudera about

00:32:22,970 --> 00:32:28,190
raid 0 configuration and how j-bot is

00:32:25,610 --> 00:32:30,440
faster they are talking about taking all

00:32:28,190 --> 00:32:32,120
12 disks and creating a single j-bot so

00:32:30,440 --> 00:32:33,919
there's a clarification there so you're

00:32:32,120 --> 00:32:37,400
creating a single red zero volume from

00:32:33,919 --> 00:32:38,600
all type 12 this so just to clarify so

00:32:37,400 --> 00:32:41,659
it's not a bad thing to create

00:32:38,600 --> 00:32:43,460
individual raid 0 volumes okay so

00:32:41,659 --> 00:32:44,990
assuming that those are your two server

00:32:43,460 --> 00:32:47,659
building blocks now let's talk about the

00:32:44,990 --> 00:32:50,270
rack building blocks so what I'm going

00:32:47,659 --> 00:32:55,130
to propose here is is that 10 minutes

00:32:50,270 --> 00:32:56,570
until questions or 10 minutes tops okay

00:32:55,130 --> 00:33:01,549
okay right so I'm going to finish up

00:32:56,570 --> 00:33:03,110
here real quick and the so if we set up

00:33:01,549 --> 00:33:04,490
what I'm going to pause it is if you set

00:33:03,110 --> 00:33:06,860
your rack up this way that you can

00:33:04,490 --> 00:33:08,720
basically scale up until the Hadoop one

00:33:06,860 --> 00:33:10,490
dodos current scalability limit without

00:33:08,720 --> 00:33:12,440
ever having to go back and change this

00:33:10,490 --> 00:33:14,690
configuration right so that's what I

00:33:12,440 --> 00:33:16,429
mean by a building block so what you

00:33:14,690 --> 00:33:19,970
have is two redundant one gigabit

00:33:16,429 --> 00:33:20,990
switches and those will connect to a 10

00:33:19,970 --> 00:33:25,150
gigabit switch

00:33:20,990 --> 00:33:27,440
as an you for your core aggregation

00:33:25,150 --> 00:33:30,650
network interconnects that connect all

00:33:27,440 --> 00:33:34,490
your acts together you have you know a

00:33:30,650 --> 00:33:36,740
management node so this the job tracker

00:33:34,490 --> 00:33:38,660
and the name node set up that I showed

00:33:36,740 --> 00:33:41,420
earlier you have a third one that's a

00:33:38,660 --> 00:33:43,309
management mode and this node basically

00:33:41,420 --> 00:33:45,140
is multihomed it sits between your user

00:33:43,309 --> 00:33:46,760
network and your private network that

00:33:45,140 --> 00:33:49,220
you create for your Hadoop cluster and

00:33:46,760 --> 00:33:51,500
that way you isolate the traffic within

00:33:49,220 --> 00:33:53,510
your Hadoop cluster from outside traffic

00:33:51,500 --> 00:33:55,850
right so your Hadoop cluster is in its

00:33:53,510 --> 00:33:57,320
own private network the user networks in

00:33:55,850 --> 00:33:59,059
their own private network how do they

00:33:57,320 --> 00:34:00,920
get and talk and move data back and

00:33:59,059 --> 00:34:02,570
forwards to Hadoop you've got to have

00:34:00,920 --> 00:34:04,429
something that's multihomed and sits on

00:34:02,570 --> 00:34:06,230
both networks and that's your management

00:34:04,429 --> 00:34:09,379
no that's where you'll stick pig and

00:34:06,230 --> 00:34:11,119
hive and your Hadoop clients that's

00:34:09,379 --> 00:34:13,040
where you'll put your job tracker or

00:34:11,119 --> 00:34:14,629
York la tierra manager will your ambari

00:34:13,040 --> 00:34:15,919
on there right sorry not your job

00:34:14,629 --> 00:34:18,560
tracker that's what you put in barrio

00:34:15,919 --> 00:34:20,810
cloudera manager the applications that

00:34:18,560 --> 00:34:23,540
users can see the progress of their jobs

00:34:20,810 --> 00:34:25,399
and then basically the Hadoop slave

00:34:23,540 --> 00:34:27,820
you'll just fill the rack up with the

00:34:25,399 --> 00:34:31,190
rest of those leaves one you open for a

00:34:27,820 --> 00:34:32,840
keyboard video monitor kvm switch for

00:34:31,190 --> 00:34:35,510
people to sort of debug what's going on

00:34:32,840 --> 00:34:36,649
this is the scale up now so basically

00:34:35,510 --> 00:34:38,929
what you're going to do is you're going

00:34:36,649 --> 00:34:40,520
to add one or so a single rack reference

00:34:38,929 --> 00:34:42,050
architectural building block that was

00:34:40,520 --> 00:34:44,389
the first line so the block on the left

00:34:42,050 --> 00:34:46,280
is what I just showed you the block on

00:34:44,389 --> 00:34:47,899
the right is you're going to add one or

00:34:46,280 --> 00:34:50,540
more of those and just keep scaling out

00:34:47,899 --> 00:34:52,639
rack by rack to grow your who do cluster

00:34:50,540 --> 00:34:55,040
and so that's pretty much the same thing

00:34:52,639 --> 00:34:56,929
two switches to top Iraq switches and

00:34:55,040 --> 00:34:59,180
then you just fill the cluster with

00:34:56,929 --> 00:35:03,230
Hadoop slaves and you connect that to a

00:34:59,180 --> 00:35:05,540
10 gigabit aggregation network at the

00:35:03,230 --> 00:35:07,430
back and so that and basically if you

00:35:05,540 --> 00:35:11,150
just follow this process a really simple

00:35:07,430 --> 00:35:13,850
way of just designing a cluster right so

00:35:11,150 --> 00:35:15,380
my last slide and here is something

00:35:13,850 --> 00:35:17,480
that's interesting Hadoop has come full

00:35:15,380 --> 00:35:21,470
circle so if you look at what I started

00:35:17,480 --> 00:35:24,380
on my first slide which was the Hadoop

00:35:21,470 --> 00:35:26,359
in 2006 and a setup right now if you

00:35:24,380 --> 00:35:28,820
look at what people are doing with on

00:35:26,359 --> 00:35:31,070
processors and intel atom and creating

00:35:28,820 --> 00:35:33,170
server cartridges for those it sort of

00:35:31,070 --> 00:35:34,370
maps right over to the same spec so four

00:35:33,170 --> 00:35:36,680
to eight gig of ram

00:35:34,370 --> 00:35:39,260
four cores 1.8 giga hertz two to four

00:35:36,680 --> 00:35:41,710
terabytes in a single neck so I'm going

00:35:39,260 --> 00:35:44,180
to wonder over here for for a little bit

00:35:41,710 --> 00:35:49,720
what you see here is a tray from a

00:35:44,180 --> 00:35:53,360
server it looks like it's a essentially

00:35:49,720 --> 00:35:55,160
normal Enterprise racks you have these

00:35:53,360 --> 00:35:57,170
things called j-bot drawers you pull

00:35:55,160 --> 00:36:00,140
this draw out and it's just full of

00:35:57,170 --> 00:36:01,820
discs right this is kind of similar you

00:36:00,140 --> 00:36:03,320
pull this tray out of the rack but it's

00:36:01,820 --> 00:36:06,020
not full of distance full of service

00:36:03,320 --> 00:36:07,670
each one of these servers which sits

00:36:06,020 --> 00:36:11,480
about this big it's about as big as a

00:36:07,670 --> 00:36:13,340
large form factor SATA Drive right is a

00:36:11,480 --> 00:36:17,120
full server with those specs that I just

00:36:13,340 --> 00:36:21,680
mentioned and they can have a number of

00:36:17,120 --> 00:36:24,680
disks attached to them at varies and and

00:36:21,680 --> 00:36:29,060
we've we were running a Hadoop on this

00:36:24,680 --> 00:36:30,800
setup at HP right so I left before our

00:36:29,060 --> 00:36:32,930
findings were conclusive but it

00:36:30,800 --> 00:36:35,240
definitely does work and there on the

00:36:32,930 --> 00:36:37,970
trajectory for this to become an

00:36:35,240 --> 00:36:40,790
architecture for Hadoop so now something

00:36:37,970 --> 00:36:42,800
to keep in mind right and you see

00:36:40,790 --> 00:36:45,050
virtualization as a technology that has

00:36:42,800 --> 00:36:47,540
taken off because in some cases the

00:36:45,050 --> 00:36:49,280
current x86 commodity architecture is

00:36:47,540 --> 00:36:51,560
over provisioned for the apps that run

00:36:49,280 --> 00:36:54,680
on them right so if you've got too much

00:36:51,560 --> 00:36:56,360
resources on a server there's a couple

00:36:54,680 --> 00:36:59,270
of things that you can do one you could

00:36:56,360 --> 00:37:01,220
basically redesign servers and make them

00:36:59,270 --> 00:37:03,170
smaller and fit the workloads better or

00:37:01,220 --> 00:37:05,660
you could use virtualization and cough

00:37:03,170 --> 00:37:07,130
up the resources and and partition the

00:37:05,660 --> 00:37:08,510
resources that way so there's sort of

00:37:07,130 --> 00:37:10,850
two approaches going on in the market

00:37:08,510 --> 00:37:12,500
and this is a more Hardware centric

00:37:10,850 --> 00:37:14,360
approach versus a software centric

00:37:12,500 --> 00:37:16,100
approach but it's pretty cool and here's

00:37:14,360 --> 00:37:19,250
something that'll blow your mind you can

00:37:16,100 --> 00:37:23,420
fit 270 Hadoop slaves in a single same

00:37:19,250 --> 00:37:28,070
42 42 you rack compared to 20 that's

00:37:23,420 --> 00:37:30,860
pretty amazing right so at an increase

00:37:28,070 --> 00:37:33,020
in storage capacity right so this is a

00:37:30,860 --> 00:37:35,980
pretty revolutionary architecture in the

00:37:33,020 --> 00:37:39,530
industry it's known as system-on-a-chip

00:37:35,980 --> 00:37:43,790
this is a this is a photo of Gerald

00:37:39,530 --> 00:37:46,520
client from HP Houston showing he's the

00:37:43,790 --> 00:37:48,349
lab manager for this and there's a third

00:37:46,520 --> 00:37:50,029
party company calzada that

00:37:48,349 --> 00:37:53,720
are in austin texas which is where i

00:37:50,029 --> 00:37:56,630
live and they're actually partnering

00:37:53,720 --> 00:37:57,950
with HP but they they create these

00:37:56,630 --> 00:37:59,690
cartridges themselves and Trevor

00:37:57,950 --> 00:38:02,089
Robinson's a member of the Apache Hadoop

00:37:59,690 --> 00:38:04,880
community and he would love to vent to

00:38:02,089 --> 00:38:06,140
you about Hawaii hoodie community should

00:38:04,880 --> 00:38:07,700
care a little bit more about memory

00:38:06,140 --> 00:38:10,099
management because he only has four to

00:38:07,700 --> 00:38:13,309
eight gigs that he's dealing with right

00:38:10,099 --> 00:38:15,470
so anyways his contact info is there so

00:38:13,309 --> 00:38:17,239
that's my talk thank you very much for

00:38:15,470 --> 00:38:19,460
attending I know there's free beer out

00:38:17,239 --> 00:38:21,109
there and that's your reward for sitting

00:38:19,460 --> 00:38:28,279
through this so come chat to me are

00:38:21,109 --> 00:38:29,390
there any questions sure go ahead was

00:38:28,279 --> 00:38:35,269
that a hand that you wanted to go to

00:38:29,390 --> 00:38:37,819
beer or you had a question yes I'm just

00:38:35,269 --> 00:38:40,660
curious cost-wise you factory in order

00:38:37,819 --> 00:38:43,249
the energy caused is the atom based

00:38:40,660 --> 00:38:46,069
servers more cost-effective they haven't

00:38:43,249 --> 00:38:48,979
priced them yet I to the best of my

00:38:46,069 --> 00:38:51,499
knowledge i'm not so I'm well I can say

00:38:48,979 --> 00:38:53,839
is I don't know because when I lost was

00:38:51,499 --> 00:38:55,839
at HP it we haven't priced them yet but

00:38:53,839 --> 00:38:58,819
they're the power of footprint is like

00:38:55,839 --> 00:39:00,769
exponentially cheaper than so the steady

00:38:58,819 --> 00:39:03,049
state courts is definitely cheaper I'm

00:39:00,769 --> 00:39:05,930
not sure what their capital you know the

00:39:03,049 --> 00:39:07,640
initial order cost is but I believe

00:39:05,930 --> 00:39:09,589
that's part of the premise right is that

00:39:07,640 --> 00:39:17,329
they're cheaper steady state an

00:39:09,589 --> 00:39:20,390
additional expenditure any more

00:39:17,329 --> 00:39:23,619
questions well thanks for attending

00:39:20,390 --> 00:39:23,619
folks I appreciate you staying

00:39:25,480 --> 00:39:27,540

YouTube URL: https://www.youtube.com/watch?v=MW77p0c3c0k


