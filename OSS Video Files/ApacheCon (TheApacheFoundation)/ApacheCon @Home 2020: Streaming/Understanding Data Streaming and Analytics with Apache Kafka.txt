Title: Understanding Data Streaming and Analytics with Apache Kafka
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Understanding Data Streaming and Analytics with Apache Kafka
Ricardo Ferreira

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

The use of distributed streaming platforms is becoming increasingly popular among developers, but have you ever wonder what exactly this is? Part Pub/Sub messaging system, partly distributed storage, partly event processing engine, the usage of this type of technology brings a whole new perspective on how developers capture, store, and process events. This talk will explain what distributed streaming platforms are and how it can be a game changer for modern data architectures. It will be discussed the road in IT that led to the need of this type of plataform, the current state of Apache Kafka, as well as scenarios where this technology can be implemented.

Ricardo is a Developer Advocate at Confluent, the company founded by the original co-creators of Apache Kafka. He has over 20 years of experience where he specializes in streaming data architectures, big data, cloud, and serverless. Prior to Confluent, he worked for other vendors, such as Oracle, Red Hat, and IONA Technologies, as well as several consulting firms. When not working, he enjoys grilling steaks in his backyard with his family and friends, where he gets the chance to talk about anything that is not IT related. Currently, he lives in Raleigh, North Carolina, with his wife and son. Follow Ricardo on Twitter: @riferrei
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,519 --> 00:00:29,279
okay

00:00:26,080 --> 00:00:31,920
so let's start it

00:00:29,279 --> 00:00:32,559
okay so well for those of you that just

00:00:31,920 --> 00:00:34,559
joined it

00:00:32,559 --> 00:00:36,160
thank uh thanks for joining the session

00:00:34,559 --> 00:00:37,600
um i was

00:00:36,160 --> 00:00:38,960
just delivering another session right

00:00:37,600 --> 00:00:40,719
now about building your own connector

00:00:38,960 --> 00:00:43,440
with kafka connect which is

00:00:40,719 --> 00:00:44,399
it was extremely democentric right and

00:00:43,440 --> 00:00:45,920
code centric

00:00:44,399 --> 00:00:48,239
this one's going to be a little bit more

00:00:45,920 --> 00:00:50,960
high level right so for those of you

00:00:48,239 --> 00:00:52,640
that know kafka in a very high level or

00:00:50,960 --> 00:00:54,000
maybe you don't know kafka i think this

00:00:52,640 --> 00:00:57,360
is going to be your thing

00:00:54,000 --> 00:01:01,359
right um okay

00:00:57,360 --> 00:01:03,600
so let me just set the

00:01:01,359 --> 00:01:04,559
connector here so let me present again

00:01:03,600 --> 00:01:06,080
for those of you that

00:01:04,559 --> 00:01:08,400
was not in the first session my name is

00:01:06,080 --> 00:01:10,000
ricardo right i'm a developer advocate

00:01:08,400 --> 00:01:11,280
working in this company here called

00:01:10,000 --> 00:01:12,720
elastic right

00:01:11,280 --> 00:01:15,040
you probably know elastic from the

00:01:12,720 --> 00:01:18,400
elastic stack like uh elasticsearch

00:01:15,040 --> 00:01:20,080
kibana beats and log stash right

00:01:18,400 --> 00:01:21,600
uh and the last time part of the

00:01:20,080 --> 00:01:23,920
community team right

00:01:21,600 --> 00:01:25,360
and um i'm all open source i mean i love

00:01:23,920 --> 00:01:26,720
the technology that i've been made

00:01:25,360 --> 00:01:28,159
available by open source specifically

00:01:26,720 --> 00:01:30,000
the ones from apache right

00:01:28,159 --> 00:01:31,280
that's one of the reasons why i decided

00:01:30,000 --> 00:01:34,079
to apply for this conference

00:01:31,280 --> 00:01:34,400
and i'm a fan of kafka right uh but you

00:01:34,079 --> 00:01:35,840
you

00:01:34,400 --> 00:01:38,320
for those of you that know my career

00:01:35,840 --> 00:01:40,159
i've i've used to work at conflict which

00:01:38,320 --> 00:01:40,799
is one of the companies behind kafka so

00:01:40,159 --> 00:01:44,479
that's why

00:01:40,799 --> 00:01:46,320
i'm so eager about this subject and

00:01:44,479 --> 00:01:47,680
i'm also part of the kafka summon

00:01:46,320 --> 00:01:49,680
program committee member

00:01:47,680 --> 00:01:51,040
right that's uh one of the things that i

00:01:49,680 --> 00:01:52,840
started doing last year

00:01:51,040 --> 00:01:54,960
and those are my contacts so rifari and

00:01:52,840 --> 00:01:57,040
elastic.co and referee

00:01:54,960 --> 00:01:58,240
rivalry.com if you want to contact me

00:01:57,040 --> 00:02:00,159
for anything that

00:01:58,240 --> 00:02:02,320
we cannot discuss here in the q a in the

00:02:00,159 --> 00:02:04,719
end this is something that you can

00:02:02,320 --> 00:02:06,320
also reach out and this is my twitter

00:02:04,719 --> 00:02:08,640
handler so all these lines

00:02:06,320 --> 00:02:10,080
will contain my career handle uh if you

00:02:08,640 --> 00:02:12,080
care to follow me on twitter

00:02:10,080 --> 00:02:13,760
from time to time i usually share some

00:02:12,080 --> 00:02:14,800
good content over here whether it's the

00:02:13,760 --> 00:02:17,040
talks that i've done

00:02:14,800 --> 00:02:19,440
or something that i've built right so

00:02:17,040 --> 00:02:22,160
without further ado let's get started

00:02:19,440 --> 00:02:23,440
i think if we ask anyone in the world

00:02:22,160 --> 00:02:24,800
right now about what they're repeating

00:02:23,440 --> 00:02:27,440
about kafka they're going to say that

00:02:24,800 --> 00:02:30,160
kafka is a messaging technology right

00:02:27,440 --> 00:02:31,120
it is no brainer writers right most of

00:02:30,160 --> 00:02:33,440
the people

00:02:31,120 --> 00:02:36,000
uh think about this but the reality is

00:02:33,440 --> 00:02:38,080
that kafka is not a messaging technology

00:02:36,000 --> 00:02:39,440
right it looks like one but it isn't

00:02:38,080 --> 00:02:41,280
right so

00:02:39,440 --> 00:02:42,800
the best way i found it to explain what

00:02:41,280 --> 00:02:44,319
kafka is is

00:02:42,800 --> 00:02:46,080
using some of the passages from this

00:02:44,319 --> 00:02:46,560
book here called kafka the definitive

00:02:46,080 --> 00:02:48,239
guide

00:02:46,560 --> 00:02:50,720
um this is probably one of the best

00:02:48,239 --> 00:02:54,080
resources for you to learn kafka in

00:02:50,720 --> 00:02:54,400
greater details and one of the passages

00:02:54,080 --> 00:02:56,720
that

00:02:54,400 --> 00:02:57,519
is part of preface which is has been

00:02:56,720 --> 00:02:59,280
written by the

00:02:57,519 --> 00:03:00,720
one of the co-creators of kafka jay

00:02:59,280 --> 00:03:03,040
crabs he said

00:03:00,720 --> 00:03:04,000
uh back his time when he was working on

00:03:03,040 --> 00:03:05,760
linkedin

00:03:04,000 --> 00:03:07,519
he said there was a lot of databases in

00:03:05,760 --> 00:03:09,040
other systems built to store data

00:03:07,519 --> 00:03:11,440
what was something missing in the

00:03:09,040 --> 00:03:13,440
architecture that would help us to

00:03:11,440 --> 00:03:14,400
handle continuous flows of data right so

00:03:13,440 --> 00:03:16,720
they had this need

00:03:14,400 --> 00:03:18,480
to have a continuous flow of data and

00:03:16,720 --> 00:03:19,840
basically their realizations that right

00:03:18,480 --> 00:03:22,000
we have this bunch of databases and

00:03:19,840 --> 00:03:24,400
technologies in place but none of them

00:03:22,000 --> 00:03:25,040
seems to be very helpful for that right

00:03:24,400 --> 00:03:27,040
so

00:03:25,040 --> 00:03:28,720
let's talk about their their story on

00:03:27,040 --> 00:03:31,440
linkedin so we can better understand

00:03:28,720 --> 00:03:32,720
where kafka came from right so when they

00:03:31,440 --> 00:03:34,159
were building

00:03:32,720 --> 00:03:36,080
the architecture of linkedin not

00:03:34,159 --> 00:03:38,000
linkedin specific but the architecture

00:03:36,080 --> 00:03:40,400
behind linkedin right the the portal

00:03:38,000 --> 00:03:42,319
linkedin um they've decided that

00:03:40,400 --> 00:03:44,720
everything needs to be even driven

00:03:42,319 --> 00:03:46,000
right so basically if you if you ever

00:03:44,720 --> 00:03:48,080
care to see how linkedin

00:03:46,000 --> 00:03:50,159
works or everything that linkedin does

00:03:48,080 --> 00:03:50,720
is to detect when an event happened and

00:03:50,159 --> 00:03:53,200
then

00:03:50,720 --> 00:03:54,720
it is acute some interested applications

00:03:53,200 --> 00:03:56,080
that are going to be

00:03:54,720 --> 00:03:58,480
are going to do something useful with

00:03:56,080 --> 00:04:00,720
that data right so for example you

00:03:58,480 --> 00:04:02,720
you change jobs you're gonna execute for

00:04:00,720 --> 00:04:04,080
example the optimization of the search

00:04:02,720 --> 00:04:06,080
engine right

00:04:04,080 --> 00:04:07,680
which is probably advancing on

00:04:06,080 --> 00:04:10,799
elasticsearch right

00:04:07,680 --> 00:04:11,920
and this even dreaming architecture

00:04:10,799 --> 00:04:14,080
right uh when you were building

00:04:11,920 --> 00:04:16,479
fundamentally the first draft

00:04:14,080 --> 00:04:18,239
of the linkedin architecture right what

00:04:16,479 --> 00:04:20,000
these what they saw at the time is that

00:04:18,239 --> 00:04:22,479
right let's not reinvent the wheel

00:04:20,000 --> 00:04:23,120
right let's just use what everybody's

00:04:22,479 --> 00:04:25,120
doing for

00:04:23,120 --> 00:04:27,199
things like this so let's use a sql

00:04:25,120 --> 00:04:29,680
database for starter data

00:04:27,199 --> 00:04:30,479
and then for for the notification part

00:04:29,680 --> 00:04:32,240
we're going to actually

00:04:30,479 --> 00:04:34,080
write a bunch of micro services that are

00:04:32,240 --> 00:04:35,440
going to execute sql queries against the

00:04:34,080 --> 00:04:39,040
database to read the data from

00:04:35,440 --> 00:04:40,720
right right basically that's uh 99 of

00:04:39,040 --> 00:04:42,400
the developers on the award would start

00:04:40,720 --> 00:04:43,120
developing their applications these days

00:04:42,400 --> 00:04:45,199
right

00:04:43,120 --> 00:04:46,720
and back then wasn't different uh what

00:04:45,199 --> 00:04:49,840
they realized is that

00:04:46,720 --> 00:04:51,199
by using this approach they they saw

00:04:49,840 --> 00:04:53,040
that the database was not keeping up

00:04:51,199 --> 00:04:55,199
with the volume right because

00:04:53,040 --> 00:04:56,639
uh there is a small number of

00:04:55,199 --> 00:04:58,479
transactional events

00:04:56,639 --> 00:04:59,840
by transaction events i'm referring to

00:04:58,479 --> 00:05:03,280
this data like customers

00:04:59,840 --> 00:05:04,880
products or uh things that are not

00:05:03,280 --> 00:05:06,800
change a lot or doesn't happen a lot

00:05:04,880 --> 00:05:08,720
they're basically static data right

00:05:06,800 --> 00:05:10,639
and there's a huge considerable amount

00:05:08,720 --> 00:05:13,199
of non-transactional events or just

00:05:10,639 --> 00:05:14,720
events as we could call it right that

00:05:13,199 --> 00:05:17,919
the database was not keeping up

00:05:14,720 --> 00:05:19,360
right um so the reality is that for that

00:05:17,919 --> 00:05:21,039
type of system databases are not

00:05:19,360 --> 00:05:22,800
appropriate right if you think about how

00:05:21,039 --> 00:05:23,440
databases used to be like three years

00:05:22,800 --> 00:05:25,360
ago

00:05:23,440 --> 00:05:27,120
it was just like this right but these

00:05:25,360 --> 00:05:29,120
days when you're building microservices

00:05:27,120 --> 00:05:29,680
that has to handle like huge amount of

00:05:29,120 --> 00:05:31,360
data

00:05:29,680 --> 00:05:33,440
has to be horizontally scalable and all

00:05:31,360 --> 00:05:35,120
of that databases are not keeping up

00:05:33,440 --> 00:05:36,720
i'm not saying the database are bad

00:05:35,120 --> 00:05:38,560
right uh there's a

00:05:36,720 --> 00:05:39,919
very good databases out there the

00:05:38,560 --> 00:05:41,919
reality is that

00:05:39,919 --> 00:05:43,759
there are some problems that databases

00:05:41,919 --> 00:05:46,240
only cannot be solved anymore

00:05:43,759 --> 00:05:46,880
right and the reality is that databases

00:05:46,240 --> 00:05:49,680
are limited

00:05:46,880 --> 00:05:50,160
right uh a while some people might uh

00:05:49,680 --> 00:05:52,479
defer

00:05:50,160 --> 00:05:54,000
from this from this uh statement right

00:05:52,479 --> 00:05:55,120
the reality is that databases are

00:05:54,000 --> 00:05:58,560
limited because

00:05:55,120 --> 00:06:00,800
think about this what reason what other

00:05:58,560 --> 00:06:03,680
reason for a database be limited

00:06:00,800 --> 00:06:06,000
because we have to this this process of

00:06:03,680 --> 00:06:09,039
reading from the transactional databases

00:06:06,000 --> 00:06:10,800
and move to a very dedicated database

00:06:09,039 --> 00:06:12,639
only for the person doing analytics

00:06:10,800 --> 00:06:15,840
right if the databases were not

00:06:12,639 --> 00:06:17,600
limited we should we should be able

00:06:15,840 --> 00:06:19,280
right that's the statement we should be

00:06:17,600 --> 00:06:20,800
able to run analytics on top of the

00:06:19,280 --> 00:06:22,479
transaction databases right

00:06:20,800 --> 00:06:24,000
but if you do this what's going to

00:06:22,479 --> 00:06:25,520
happen is that the data transaction

00:06:24,000 --> 00:06:27,039
database is going to slow down

00:06:25,520 --> 00:06:29,120
right and that's the limitations all

00:06:27,039 --> 00:06:31,759
about right so

00:06:29,120 --> 00:06:32,639
uh the realization that the guys from

00:06:31,759 --> 00:06:35,199
linkedin

00:06:32,639 --> 00:06:35,840
came is that right let's let me go back

00:06:35,199 --> 00:06:38,080
to this

00:06:35,840 --> 00:06:39,360
picture over here and then i can explain

00:06:38,080 --> 00:06:40,880
better so

00:06:39,360 --> 00:06:42,960
behind every database there's this

00:06:40,880 --> 00:06:44,880
concept of log right and

00:06:42,960 --> 00:06:46,080
this this is basically any district that

00:06:44,880 --> 00:06:48,319
system paper

00:06:46,080 --> 00:06:49,840
will touch base the concept of logs

00:06:48,319 --> 00:06:52,080
right so

00:06:49,840 --> 00:06:52,880
the this paper here specifically talks

00:06:52,080 --> 00:06:54,960
about that

00:06:52,880 --> 00:06:56,720
the truth is the log right the database

00:06:54,960 --> 00:06:59,120
is basically a cache

00:06:56,720 --> 00:07:01,759
and comprising of a subset of that log

00:06:59,120 --> 00:07:02,720
so all databases organize that very in

00:07:01,759 --> 00:07:04,800
the logs the

00:07:02,720 --> 00:07:07,120
status turquoise right and what the

00:07:04,800 --> 00:07:09,759
tables does is to periodically kind of

00:07:07,120 --> 00:07:11,680
read chunks of that log and materialize

00:07:09,759 --> 00:07:13,759
in memory for read consumption right

00:07:11,680 --> 00:07:15,440
that's what database does basically

00:07:13,759 --> 00:07:18,240
right so

00:07:15,440 --> 00:07:18,880
what linkedin did back then is that they

00:07:18,240 --> 00:07:21,120
kind of uh

00:07:18,880 --> 00:07:22,720
got rid of the whole concept of database

00:07:21,120 --> 00:07:24,880
per se right so

00:07:22,720 --> 00:07:26,560
specifically the concept of tables right

00:07:24,880 --> 00:07:29,759
let's get rid of tables

00:07:26,560 --> 00:07:32,080
and let's write code specifically for

00:07:29,759 --> 00:07:32,800
the data structure that is behind tables

00:07:32,080 --> 00:07:35,520
which is

00:07:32,800 --> 00:07:36,960
the log right and conceptually the log

00:07:35,520 --> 00:07:39,199
is a very simple data circuitry it

00:07:36,960 --> 00:07:41,520
basically is a sequential set of data

00:07:39,199 --> 00:07:42,880
where new data always goes to the tail

00:07:41,520 --> 00:07:44,879
of the log right

00:07:42,880 --> 00:07:46,560
and from time to time you might have

00:07:44,879 --> 00:07:48,560
concurrent readers right

00:07:46,560 --> 00:07:49,759
and one of the beauties of this uh

00:07:48,560 --> 00:07:51,440
architecture is that

00:07:49,759 --> 00:07:53,120
no matter how many concurrent readers

00:07:51,440 --> 00:07:56,639
you might have the

00:07:53,120 --> 00:07:59,120
latency or the performance of each read

00:07:56,639 --> 00:08:01,199
is going to be custom time right because

00:07:59,120 --> 00:08:02,960
basically all the readers are referring

00:08:01,199 --> 00:08:04,720
to a specific piece of data

00:08:02,960 --> 00:08:06,800
given the position of the data and the

00:08:04,720 --> 00:08:08,080
log right for for those of you that know

00:08:06,800 --> 00:08:10,560
very well data structures

00:08:08,080 --> 00:08:11,280
think about an array right why really an

00:08:10,560 --> 00:08:13,599
array

00:08:11,280 --> 00:08:14,400
is constant time because given the index

00:08:13,599 --> 00:08:16,000
of the array

00:08:14,400 --> 00:08:18,160
you go straight to the position where

00:08:16,000 --> 00:08:18,560
the data is located so it is constant

00:08:18,160 --> 00:08:20,800
time

00:08:18,560 --> 00:08:22,800
if you know where the data is right here

00:08:20,800 --> 00:08:25,360
is the same concept right we just don't

00:08:22,800 --> 00:08:26,800
refer to indexes but we call this in a

00:08:25,360 --> 00:08:28,639
log architect we call those

00:08:26,800 --> 00:08:30,080
offsets if you know the offset of a

00:08:28,639 --> 00:08:31,759
given piece of information

00:08:30,080 --> 00:08:33,599
you can move the offset and log

00:08:31,759 --> 00:08:34,000
specifically then and then you can read

00:08:33,599 --> 00:08:37,279
it

00:08:34,000 --> 00:08:38,880
it's just extremely scalable right so

00:08:37,279 --> 00:08:40,959
this is basically what the linkedin

00:08:38,880 --> 00:08:43,200
folks did on the past and they built

00:08:40,959 --> 00:08:44,640
this what they call commit log right

00:08:43,200 --> 00:08:46,399
they've augmented

00:08:44,640 --> 00:08:48,640
the concept of log for a commit log

00:08:46,399 --> 00:08:49,760
because now this log has to be kind of

00:08:48,640 --> 00:08:52,080
distributed right

00:08:49,760 --> 00:08:54,160
for for handles things like scalability

00:08:52,080 --> 00:08:56,399
and full tolerance right

00:08:54,160 --> 00:08:58,080
and then there's the second passage of

00:08:56,399 --> 00:08:59,680
the book where j crab said

00:08:58,080 --> 00:09:01,360
we've come to think of kafka as a

00:08:59,680 --> 00:09:03,200
streaming platform right the

00:09:01,360 --> 00:09:04,959
system that lets you probably subscribe

00:09:03,200 --> 00:09:08,320
to streams of data

00:09:04,959 --> 00:09:10,800
but not only that right

00:09:08,320 --> 00:09:12,160
um until there you might remember on the

00:09:10,800 --> 00:09:13,760
messaging technology messaging

00:09:12,160 --> 00:09:16,000
technology that's what it does right

00:09:13,760 --> 00:09:17,120
it lets you publish and subscribe to

00:09:16,000 --> 00:09:20,160
streams of data

00:09:17,120 --> 00:09:22,640
but kafka allows you to store the data

00:09:20,160 --> 00:09:24,880
right right here and

00:09:22,640 --> 00:09:26,640
to process the data and this is exactly

00:09:24,880 --> 00:09:29,680
what apache cover is built to be

00:09:26,640 --> 00:09:32,399
right so the reality is that

00:09:29,680 --> 00:09:34,080
if we kind of join the best and the best

00:09:32,399 --> 00:09:36,000
things about each architect refer

00:09:34,080 --> 00:09:37,760
databases what they do best right

00:09:36,000 --> 00:09:39,440
they're how scalable they're durable

00:09:37,760 --> 00:09:40,640
they're persistent and it provides

00:09:39,440 --> 00:09:42,320
ordering right

00:09:40,640 --> 00:09:44,399
and messaging technology what they do

00:09:42,320 --> 00:09:45,920
best they bad they basically provide low

00:09:44,399 --> 00:09:48,640
latency right they're fast

00:09:45,920 --> 00:09:50,480
right and if you take aside all the bad

00:09:48,640 --> 00:09:51,279
things about databases and messaging

00:09:50,480 --> 00:09:52,800
system

00:09:51,279 --> 00:09:54,480
we're going to end up with a thing

00:09:52,800 --> 00:09:56,320
called distributed commute log

00:09:54,480 --> 00:09:57,680
right this is basically what apache

00:09:56,320 --> 00:10:00,320
kafka is in

00:09:57,680 --> 00:10:02,240
essence okay this is the philosophy

00:10:00,320 --> 00:10:04,399
about behind kafka

00:10:02,240 --> 00:10:06,399
it is saying that it's highly scalable

00:10:04,399 --> 00:10:07,279
durable persistent in order and it's

00:10:06,399 --> 00:10:09,040
fast

00:10:07,279 --> 00:10:10,800
but remember that the description for

00:10:09,040 --> 00:10:12,320
jkranz he said all right

00:10:10,800 --> 00:10:14,640
it is a system allowed to pump to

00:10:12,320 --> 00:10:17,519
describe and store data right

00:10:14,640 --> 00:10:18,720
but also it lets you process them right

00:10:17,519 --> 00:10:20,480
so

00:10:18,720 --> 00:10:22,560
if you have a platform a distributed

00:10:20,480 --> 00:10:23,680
commit log that offers you all the

00:10:22,560 --> 00:10:25,920
capabilities of

00:10:23,680 --> 00:10:27,680
uh processing streams of data and store

00:10:25,920 --> 00:10:28,560
them this is what basically what we have

00:10:27,680 --> 00:10:30,160
right now

00:10:28,560 --> 00:10:32,240
we can store them because it provides

00:10:30,160 --> 00:10:34,079
persistency capabilities right

00:10:32,240 --> 00:10:35,839
but what about processing the data

00:10:34,079 --> 00:10:37,519
that's the beauty of using streaming

00:10:35,839 --> 00:10:38,720
data architecture is that you have to be

00:10:37,519 --> 00:10:41,200
able that

00:10:38,720 --> 00:10:42,480
as the data happens you have to be able

00:10:41,200 --> 00:10:44,399
to plug what we call

00:10:42,480 --> 00:10:46,320
processors or stream processors we're

00:10:44,399 --> 00:10:48,880
not talking about this later on

00:10:46,320 --> 00:10:50,480
and then everything that every new data

00:10:48,880 --> 00:10:51,920
that arrives you can actually process

00:10:50,480 --> 00:10:52,560
them as it happens you don't need to

00:10:51,920 --> 00:10:55,440
actually

00:10:52,560 --> 00:10:56,320
wait until the data is stored until you

00:10:55,440 --> 00:10:59,040
actually to start

00:10:56,320 --> 00:11:00,240
acting upon that data right so this is

00:10:59,040 --> 00:11:02,000
where the part

00:11:00,240 --> 00:11:04,240
if you add string processing

00:11:02,000 --> 00:11:05,839
capabilities and scalable integration

00:11:04,240 --> 00:11:07,680
remember the kafka connection that i've

00:11:05,839 --> 00:11:09,279
just did so this is the arena

00:11:07,680 --> 00:11:10,800
scalable integration right we're gonna

00:11:09,279 --> 00:11:14,079
talk this uh

00:11:10,800 --> 00:11:16,240
more about this uh in a moment but now

00:11:14,079 --> 00:11:17,839
if you if you if you group all those

00:11:16,240 --> 00:11:19,920
capabilities you're not talking about

00:11:17,839 --> 00:11:22,320
something a technology that

00:11:19,920 --> 00:11:23,279
everybody else right this is not a

00:11:22,320 --> 00:11:26,480
accepted

00:11:23,279 --> 00:11:28,079
commonly accepted term right but it's

00:11:26,480 --> 00:11:29,839
becoming common and common

00:11:28,079 --> 00:11:31,360
for other companies to call this a

00:11:29,839 --> 00:11:34,800
distributed streaming platform

00:11:31,360 --> 00:11:35,279
right and if you were in apache car

00:11:34,800 --> 00:11:37,839
right

00:11:35,279 --> 00:11:38,959
so it's all about apache uh software

00:11:37,839 --> 00:11:41,680
technologies

00:11:38,959 --> 00:11:43,040
if you go to the kafka website right now

00:11:41,680 --> 00:11:45,360
in apache

00:11:43,040 --> 00:11:46,720
you're going to see that right below the

00:11:45,360 --> 00:11:49,680
logo of kafka

00:11:46,720 --> 00:11:50,320
this is the description of what kafka is

00:11:49,680 --> 00:11:53,519
so

00:11:50,320 --> 00:11:55,760
as you can see here right off the bat

00:11:53,519 --> 00:11:56,880
the apache website is telling you

00:11:55,760 --> 00:11:59,279
upfront

00:11:56,880 --> 00:12:00,959
what kafka is so this is the expectation

00:11:59,279 --> 00:12:01,600
that you should have on top of kafka

00:12:00,959 --> 00:12:04,000
right so

00:12:01,600 --> 00:12:04,880
kafka is not a messaging technology for

00:12:04,000 --> 00:12:06,639
crying out loud

00:12:04,880 --> 00:12:09,120
it is a distributed streaming platform

00:12:06,639 --> 00:12:11,760
and this is my answer a little bit

00:12:09,120 --> 00:12:12,959
i i've i've been working for kafka for

00:12:11,760 --> 00:12:16,000
some time now

00:12:12,959 --> 00:12:17,600
and one of the main uh criticisms

00:12:16,000 --> 00:12:19,519
and complaints that i've heard from

00:12:17,600 --> 00:12:21,040
developers uh

00:12:19,519 --> 00:12:23,440
as a developer advocate i've talked with

00:12:21,040 --> 00:12:25,360
a lot of developers on the world so

00:12:23,440 --> 00:12:27,360
one of the main criticisms that oh kafka

00:12:25,360 --> 00:12:28,639
is so much complicated compared to other

00:12:27,360 --> 00:12:31,839
messaging systems

00:12:28,639 --> 00:12:32,880
such as for example wrapped mq or active

00:12:31,839 --> 00:12:34,720
mq or

00:12:32,880 --> 00:12:35,920
typical or any other messaging system

00:12:34,720 --> 00:12:39,200
you name it right

00:12:35,920 --> 00:12:40,399
so dear or yeah right i give you that

00:12:39,200 --> 00:12:42,079
yeah kafka is

00:12:40,399 --> 00:12:43,680
a little bit more complicated than other

00:12:42,079 --> 00:12:45,360
messaging systems but there's a reason

00:12:43,680 --> 00:12:47,279
there's a fundamental reason for this

00:12:45,360 --> 00:12:49,200
because kafka was built to be a

00:12:47,279 --> 00:12:51,519
discriminatory platform right

00:12:49,200 --> 00:12:52,320
if you know this if you set this

00:12:51,519 --> 00:12:55,200
expectation

00:12:52,320 --> 00:12:55,920
in your head right off right off the bat

00:12:55,200 --> 00:12:58,079
right

00:12:55,920 --> 00:13:00,160
you're actually going to afford that

00:12:58,079 --> 00:13:02,000
complexity easier right uh

00:13:00,160 --> 00:13:04,079
all right there is a an inherited

00:13:02,000 --> 00:13:05,360
complexity that comes with kafka right

00:13:04,079 --> 00:13:07,760
but the reality is that it is a

00:13:05,360 --> 00:13:10,160
complexity that you should expect

00:13:07,760 --> 00:13:11,360
right it's just like you you cannot

00:13:10,160 --> 00:13:14,480
compare for example

00:13:11,360 --> 00:13:16,560
how to deploy and manage a database to

00:13:14,480 --> 00:13:17,600
how to deploy and manage a messaging

00:13:16,560 --> 00:13:19,040
system because there

00:13:17,600 --> 00:13:20,560
those are two different technologies

00:13:19,040 --> 00:13:22,720
right this is the same happens with

00:13:20,560 --> 00:13:24,399
kafka when you start comparing with uh

00:13:22,720 --> 00:13:26,399
other messaging technology you should

00:13:24,399 --> 00:13:28,000
not uh set

00:13:26,399 --> 00:13:30,240
you should not set the same expectations

00:13:28,000 --> 00:13:30,800
right and then this is the third and

00:13:30,240 --> 00:13:32,480
last

00:13:30,800 --> 00:13:34,959
uh passage from the book kafka the

00:13:32,480 --> 00:13:36,480
definitive guide that has been said by j

00:13:34,959 --> 00:13:38,399
craft where he said the ability to

00:13:36,480 --> 00:13:40,399
provide these three areas

00:13:38,399 --> 00:13:41,920
to bring all the streams of data

00:13:40,399 --> 00:13:44,800
together across

00:13:41,920 --> 00:13:46,079
all the use cases is what makes the idea

00:13:44,800 --> 00:13:49,680
of a streaming platform

00:13:46,079 --> 00:13:51,360
so appealing to people right and

00:13:49,680 --> 00:13:53,199
if you think about it not sure if you

00:13:51,360 --> 00:13:54,240
asked your question this question before

00:13:53,199 --> 00:13:57,199
but

00:13:54,240 --> 00:13:58,560
why kafka has become so popular in the

00:13:57,199 --> 00:14:00,639
last five years

00:13:58,560 --> 00:14:02,399
right i'm pretty sure that you've heard

00:14:00,639 --> 00:14:04,399
that right every new developer these

00:14:02,399 --> 00:14:06,880
days are basically using kafka

00:14:04,399 --> 00:14:08,800
for doing something uh every new

00:14:06,880 --> 00:14:10,720
applications are basically relying on

00:14:08,800 --> 00:14:13,199
kafka for doing what they have to do

00:14:10,720 --> 00:14:14,000
so why why this popularity of kafka is

00:14:13,199 --> 00:14:17,279
coming from

00:14:14,000 --> 00:14:20,079
it's coming from the the vision

00:14:17,279 --> 00:14:21,360
that the creators of kafka had like when

00:14:20,079 --> 00:14:23,279
they were building linkedin

00:14:21,360 --> 00:14:25,279
like probably more than 10 years ago

00:14:23,279 --> 00:14:28,160
right now uh

00:14:25,279 --> 00:14:30,000
it became these days a tendency and a

00:14:28,160 --> 00:14:33,040
reality that most developers are

00:14:30,000 --> 00:14:33,600
actually uh in need the need for

00:14:33,040 --> 00:14:36,160
handling

00:14:33,600 --> 00:14:38,079
data not using databases not using

00:14:36,160 --> 00:14:40,000
messaging but using streaming platforms

00:14:38,079 --> 00:14:41,680
right so serving platforms are becoming

00:14:40,000 --> 00:14:43,680
very popular these days right

00:14:41,680 --> 00:14:45,519
and of course this presentation is all

00:14:43,680 --> 00:14:46,880
about kafka because kafka it is my

00:14:45,519 --> 00:14:49,199
background this is where

00:14:46,880 --> 00:14:51,040
i'm probably best at it but there are

00:14:49,199 --> 00:14:52,880
other distribution platforms out there

00:14:51,040 --> 00:14:54,399
right front apache as well there's

00:14:52,880 --> 00:14:57,760
apache pulsar

00:14:54,399 --> 00:14:58,000
which is a very good piece of technology

00:14:57,760 --> 00:15:00,480
that

00:14:58,000 --> 00:15:02,320
uh it does conceptually and virtually

00:15:00,480 --> 00:15:05,760
pretty much what the kafka does as well

00:15:02,320 --> 00:15:07,199
uh it is not so common and uh that

00:15:05,760 --> 00:15:10,240
doesn't have the same

00:15:07,199 --> 00:15:12,320
uh wider community that kafka created

00:15:10,240 --> 00:15:14,320
for for the last five years but it's a

00:15:12,320 --> 00:15:15,760
very decent piece of technology right so

00:15:14,320 --> 00:15:17,279
apache booster is something that

00:15:15,760 --> 00:15:20,240
probably will catch on

00:15:17,279 --> 00:15:22,240
on the next years right and there's a

00:15:20,240 --> 00:15:23,600
whole bunch of other technologies that

00:15:22,240 --> 00:15:26,160
not from apache but

00:15:23,600 --> 00:15:27,760
are either open source project for other

00:15:26,160 --> 00:15:30,160
vendors or perhaps some

00:15:27,760 --> 00:15:32,800
cloud services from vendors such as

00:15:30,160 --> 00:15:35,360
azure aws or google so all of them

00:15:32,800 --> 00:15:36,800
they probably have some string services

00:15:35,360 --> 00:15:38,800
so just to give an

00:15:36,800 --> 00:15:40,560
uh an example of a few of them if the

00:15:38,800 --> 00:15:42,560
blast has kinesis and

00:15:40,560 --> 00:15:44,000
uh a managed version of for kafka as

00:15:42,560 --> 00:15:47,279
well uh

00:15:44,000 --> 00:15:47,680
azure has even hubs uh google has google

00:15:47,279 --> 00:15:50,000
pubs

00:15:47,680 --> 00:15:52,240
so all of those are streaming technology

00:15:50,000 --> 00:15:53,680
so just this is only for you just to

00:15:52,240 --> 00:15:55,440
understand that yeah so streaming

00:15:53,680 --> 00:15:57,440
technologies are catching on

00:15:55,440 --> 00:15:58,720
so you as a developer should be uh at

00:15:57,440 --> 00:16:01,120
least more interested

00:15:58,720 --> 00:16:02,959
on this type of technology right kafka

00:16:01,120 --> 00:16:05,120
is probably one of the best one of them

00:16:02,959 --> 00:16:07,600
uh but the reality is that there's

00:16:05,120 --> 00:16:09,360
others so you should pursue this uh

00:16:07,600 --> 00:16:10,639
path of learning more about streaming

00:16:09,360 --> 00:16:13,920
technologies right

00:16:10,639 --> 00:16:15,759
so now that we've discussed uh the

00:16:13,920 --> 00:16:17,920
regions of kafka and we

00:16:15,759 --> 00:16:19,600
came to a realization that kafka is not

00:16:17,920 --> 00:16:20,880
a messaging technology it's a streaming

00:16:19,600 --> 00:16:23,360
data platform right

00:16:20,880 --> 00:16:23,920
we can start discussing some of the uh

00:16:23,360 --> 00:16:26,320
uh

00:16:23,920 --> 00:16:28,320
specifically capabilities of kaka right

00:16:26,320 --> 00:16:28,720
so what i'm going to expand the next

00:16:28,320 --> 00:16:32,160
like

00:16:28,720 --> 00:16:32,800
20 minutes is to explain a little bit

00:16:32,160 --> 00:16:35,839
about

00:16:32,800 --> 00:16:38,079
uh what i call the three main pillars of

00:16:35,839 --> 00:16:39,519
a distributed screen platform which is

00:16:38,079 --> 00:16:42,399
the ability for the streaming platform

00:16:39,519 --> 00:16:44,320
to handle data streams like messaging

00:16:42,399 --> 00:16:45,920
the ability to handle data analytics or

00:16:44,320 --> 00:16:48,320
stream processing right

00:16:45,920 --> 00:16:49,360
and using the streaming platform as a

00:16:48,320 --> 00:16:51,519
storage

00:16:49,360 --> 00:16:53,120
system right a system that allows you to

00:16:51,519 --> 00:16:55,680
store data

00:16:53,120 --> 00:16:55,680
preferably

00:17:00,959 --> 00:17:07,199
okay so let's start by discussing

00:17:04,079 --> 00:17:11,760
data streams um in

00:17:07,199 --> 00:17:11,760
in the context of messaging right so

00:17:12,640 --> 00:17:17,919
let's if we think about it how

00:17:15,679 --> 00:17:19,600
messaging technologies works out there

00:17:17,919 --> 00:17:21,520
you name it it doesn't i'm not referring

00:17:19,600 --> 00:17:23,760
to any specific messaging technology but

00:17:21,520 --> 00:17:24,319
all messaging technologies are comprised

00:17:23,760 --> 00:17:26,640
by

00:17:24,319 --> 00:17:27,839
three main building blocks right there's

00:17:26,640 --> 00:17:31,360
this producer

00:17:27,839 --> 00:17:33,440
that writes data there's the broker that

00:17:31,360 --> 00:17:34,480
receives the data that has been written

00:17:33,440 --> 00:17:36,160
by the producer

00:17:34,480 --> 00:17:38,720
right and there's a consumer that

00:17:36,160 --> 00:17:41,039
basically uh receives the data

00:17:38,720 --> 00:17:42,720
that originally has been written by the

00:17:41,039 --> 00:17:44,400
producer but the broker actually

00:17:42,720 --> 00:17:47,200
delivers the data to the consumer

00:17:44,400 --> 00:17:49,120
right all of the technologies are there

00:17:47,200 --> 00:17:49,919
uses this architectural called pushing

00:17:49,120 --> 00:17:53,039
model

00:17:49,919 --> 00:17:55,120
right why this is push because it is the

00:17:53,039 --> 00:17:57,520
responsibility of the broker

00:17:55,120 --> 00:17:59,679
to actually get the data that has been

00:17:57,520 --> 00:18:02,640
received by the producer

00:17:59,679 --> 00:18:04,960
temporarily store the data somehow and

00:18:02,640 --> 00:18:07,200
by the time the consumer asks for it

00:18:04,960 --> 00:18:08,960
it's going to the broker is going to

00:18:07,200 --> 00:18:11,600
push the data down to the consumers

00:18:08,960 --> 00:18:13,440
right so in the end of the day it's all

00:18:11,600 --> 00:18:14,480
about the broker the responsibility to

00:18:13,440 --> 00:18:17,760
actually

00:18:14,480 --> 00:18:19,679
deliver the data to the consumers right

00:18:17,760 --> 00:18:21,760
what that means to us is that eventually

00:18:19,679 --> 00:18:24,240
the broker might become a bottleneck

00:18:21,760 --> 00:18:25,679
right this is why if you have worked

00:18:24,240 --> 00:18:28,720
with some other technologies in the past

00:18:25,679 --> 00:18:30,880
such as uh wrapped and queue or fmq

00:18:28,720 --> 00:18:32,799
you might have read some bottlenecks in

00:18:30,880 --> 00:18:34,160
errors where the broker could simply not

00:18:32,799 --> 00:18:36,480
scale out anymore

00:18:34,160 --> 00:18:38,640
it became the bottleneck right so it

00:18:36,480 --> 00:18:41,039
compromised things like throughput

00:18:38,640 --> 00:18:42,799
but when compared to kafka like any

00:18:41,039 --> 00:18:44,960
other messaging technology kind of a

00:18:42,799 --> 00:18:46,320
take a very serious beat from kafka

00:18:44,960 --> 00:18:49,520
because kafka

00:18:46,320 --> 00:18:50,720
somehow is able to provide way more

00:18:49,520 --> 00:18:52,559
throughput than the other massive

00:18:50,720 --> 00:18:55,440
technology and here's the reason

00:18:52,559 --> 00:18:56,080
because in kafka there is no push model

00:18:55,440 --> 00:18:59,600
there is a

00:18:56,080 --> 00:19:01,600
pool one right what that means is that

00:18:59,600 --> 00:19:03,039
it is not the broker in kafka that

00:19:01,600 --> 00:19:03,679
actually pushes the data down to the

00:19:03,039 --> 00:19:05,840
consumer

00:19:03,679 --> 00:19:07,600
but in reality it is the consumer that

00:19:05,840 --> 00:19:08,960
actually established tcp connections

00:19:07,600 --> 00:19:12,320
with the kafka broker

00:19:08,960 --> 00:19:15,039
and pulls the data out right

00:19:12,320 --> 00:19:15,440
so in other words if the number of the

00:19:15,039 --> 00:19:17,919
volume

00:19:15,440 --> 00:19:19,760
data volume grows right for the kafka

00:19:17,919 --> 00:19:21,200
broker it won't make any difference

00:19:19,760 --> 00:19:23,520
the only thing that the kafka broker

00:19:21,200 --> 00:19:24,000
needs to be able to do is to be able to

00:19:23,520 --> 00:19:26,400
establish

00:19:24,000 --> 00:19:27,360
as many tcp connections as possible

00:19:26,400 --> 00:19:28,640
right so

00:19:27,360 --> 00:19:30,480
if you're talking about operating

00:19:28,640 --> 00:19:32,000
systems uh this means

00:19:30,480 --> 00:19:34,160
increasing the number of file handles

00:19:32,000 --> 00:19:37,039
can that can be opened simultaneously

00:19:34,160 --> 00:19:39,120
right as well as you can have you can

00:19:37,039 --> 00:19:40,559
scale the number of consumers for a very

00:19:39,120 --> 00:19:42,559
very high number right so

00:19:40,559 --> 00:19:43,679
this is one of the reasons why it does

00:19:42,559 --> 00:19:45,280
make a difference for

00:19:43,679 --> 00:19:46,799
for example any other messaging

00:19:45,280 --> 00:19:50,320
technology if they have 1

00:19:46,799 --> 00:19:52,000
10 100 or 1000 of consumers concurrently

00:19:50,320 --> 00:19:54,240
and for kafka you can it doesn't make

00:19:52,000 --> 00:19:55,760
any difference at least

00:19:54,240 --> 00:19:57,280
not i'm going to talk about this

00:19:55,760 --> 00:19:59,440
scalability reasoning

00:19:57,280 --> 00:20:00,559
later on but it doesn't make so many

00:19:59,440 --> 00:20:02,159
difference if you have

00:20:00,559 --> 00:20:04,080
hundreds thousands of hundreds of

00:20:02,159 --> 00:20:05,840
thousands of consumers concurrently

00:20:04,080 --> 00:20:07,120
right because kafka can handle this

00:20:05,840 --> 00:20:10,320
right it's all about

00:20:07,120 --> 00:20:13,520
networking less than about the uh

00:20:10,320 --> 00:20:15,760
the broker uh spending some of these

00:20:13,520 --> 00:20:18,000
resources to actually pushing data down

00:20:15,760 --> 00:20:19,919
to the consumers right

00:20:18,000 --> 00:20:22,400
so this is one of the good things uh

00:20:19,919 --> 00:20:25,360
another aspect about kafka is that

00:20:22,400 --> 00:20:26,880
uh kafka it might not look like right

00:20:25,360 --> 00:20:28,720
because kafka has this

00:20:26,880 --> 00:20:30,000
naming convention called everything for

00:20:28,720 --> 00:20:32,400
kafka are topics

00:20:30,000 --> 00:20:33,039
right so basically topics are the unit

00:20:32,400 --> 00:20:35,840
where you're

00:20:33,039 --> 00:20:36,159
to start it right uh it might not look

00:20:35,840 --> 00:20:37,840
like

00:20:36,159 --> 00:20:39,360
especially if you come from the jms

00:20:37,840 --> 00:20:41,440
world where

00:20:39,360 --> 00:20:42,640
the name topic means that you are going

00:20:41,440 --> 00:20:44,720
to do a pub sub

00:20:42,640 --> 00:20:46,880
right like publish and subscribe to many

00:20:44,720 --> 00:20:48,720
uh consumers right

00:20:46,880 --> 00:20:50,559
but it might not look like that kafka

00:20:48,720 --> 00:20:50,960
doesn't provide queuing support but it

00:20:50,559 --> 00:20:53,120
does

00:20:50,960 --> 00:20:55,440
right everything in cafe is about is

00:20:53,120 --> 00:20:58,799
about this concept of groups

00:20:55,440 --> 00:21:01,120
if you put a bunch of consumers

00:20:58,799 --> 00:21:02,159
sharing a proper call group identifier

00:21:01,120 --> 00:21:04,159
or group id

00:21:02,159 --> 00:21:05,679
right in other words if all of them

00:21:04,159 --> 00:21:07,600
belong to the same group

00:21:05,679 --> 00:21:09,840
they're all actually going to queue it

00:21:07,600 --> 00:21:12,480
up bro so they're going to low balance

00:21:09,840 --> 00:21:15,520
themselves to process the messages right

00:21:12,480 --> 00:21:18,240
so in other words uh only one

00:21:15,520 --> 00:21:19,760
consumer at a time will process records

00:21:18,240 --> 00:21:21,840
right not all of them

00:21:19,760 --> 00:21:22,799
there won't be any broadcast of the

00:21:21,840 --> 00:21:25,200
message for

00:21:22,799 --> 00:21:26,240
the consumers within a group right

00:21:25,200 --> 00:21:28,320
however

00:21:26,240 --> 00:21:30,240
if you put your consumers on different

00:21:28,320 --> 00:21:33,280
groups this is going to be a pub sub

00:21:30,240 --> 00:21:34,559
right so whatever record has been stored

00:21:33,280 --> 00:21:36,960
here in the broker

00:21:34,559 --> 00:21:38,720
if we won't pull this guy is going to

00:21:36,960 --> 00:21:40,480
receive a copy of the data

00:21:38,720 --> 00:21:42,159
as well as the copy of the data for this

00:21:40,480 --> 00:21:44,960
one right so

00:21:42,159 --> 00:21:46,400
there are ways to implement both pub sub

00:21:44,960 --> 00:21:50,240
and qe in kafka

00:21:46,400 --> 00:21:53,039
right and kafka does something very very

00:21:50,240 --> 00:21:54,640
uh important that concept called

00:21:53,039 --> 00:21:57,600
partitioning which is

00:21:54,640 --> 00:22:00,080
how kafka is going to scale out right

00:21:57,600 --> 00:22:01,120
partitions are probably one of the best

00:22:00,080 --> 00:22:03,280
conventions

00:22:01,120 --> 00:22:04,960
naming conventions i've seen out there

00:22:03,280 --> 00:22:07,520
for example for those of you that has

00:22:04,960 --> 00:22:09,039
elasticsearch background you might be

00:22:07,520 --> 00:22:11,360
familiar with the concept of

00:22:09,039 --> 00:22:12,159
shards in elasticsearch so basically an

00:22:11,360 --> 00:22:14,159
index

00:22:12,159 --> 00:22:15,520
is broken down into multiple charts and

00:22:14,159 --> 00:22:18,400
each chart lives

00:22:15,520 --> 00:22:19,200
in a given uh server or jvm or process

00:22:18,400 --> 00:22:21,919
right

00:22:19,200 --> 00:22:22,880
elasticsearch server uh and kafka it

00:22:21,919 --> 00:22:25,679
doesn't use the name

00:22:22,880 --> 00:22:26,320
shards but it has the same effect right

00:22:25,679 --> 00:22:28,880
in

00:22:26,320 --> 00:22:30,400
kafka a topic is broken down into

00:22:28,880 --> 00:22:31,760
multiple partitions

00:22:30,400 --> 00:22:33,679
and the reason why i like the name

00:22:31,760 --> 00:22:36,000
partition is because partitions

00:22:33,679 --> 00:22:38,080
makes me remember the concept of parts

00:22:36,000 --> 00:22:40,720
right so basically you you picked up a

00:22:38,080 --> 00:22:43,600
topic and broken down into multiple

00:22:40,720 --> 00:22:45,120
parts right and those parts are going to

00:22:43,600 --> 00:22:46,320
be spread over the cluster right so

00:22:45,120 --> 00:22:49,120
that's this is how

00:22:46,320 --> 00:22:50,960
kafka addresses uh scalability right if

00:22:49,120 --> 00:22:52,640
you put as many brokers and a cluster as

00:22:50,960 --> 00:22:56,159
you might need

00:22:52,640 --> 00:22:58,400
this uh let's go spreading of

00:22:56,159 --> 00:23:00,240
parts is gonna happen automatically for

00:22:58,400 --> 00:23:03,360
you so you don't need to control

00:23:00,240 --> 00:23:05,039
this right uh and

00:23:03,360 --> 00:23:07,039
each one of the consumers is going to

00:23:05,039 --> 00:23:08,159
read from one given partition at a time

00:23:07,039 --> 00:23:10,320
so this is uh

00:23:08,159 --> 00:23:11,840
this is how i mentioned previously that

00:23:10,320 --> 00:23:12,720
it doesn't make any difference for kafka

00:23:11,840 --> 00:23:14,799
if you are handled

00:23:12,720 --> 00:23:16,559
if you're handling hundreds of thousands

00:23:14,799 --> 00:23:19,760
of concurrent consumers at a time

00:23:16,559 --> 00:23:22,080
as as much you have partitions enough to

00:23:19,760 --> 00:23:22,960
for each one of these consumers you

00:23:22,080 --> 00:23:26,000
might be able to

00:23:22,960 --> 00:23:26,640
serve right each one of these consumers

00:23:26,000 --> 00:23:29,280
right

00:23:26,640 --> 00:23:30,559
in uh right concurrently right so none

00:23:29,280 --> 00:23:32,720
of them will actually concur

00:23:30,559 --> 00:23:34,720
the resources from the kafka broker in

00:23:32,720 --> 00:23:35,360
order to achieve uh receive the records

00:23:34,720 --> 00:23:37,919
right

00:23:35,360 --> 00:23:39,280
so it is all about you have the enough

00:23:37,919 --> 00:23:41,440
number of partitions

00:23:39,280 --> 00:23:42,400
in your coffee cluster to serve all the

00:23:41,440 --> 00:23:44,080
consumers right

00:23:42,400 --> 00:23:45,520
if you have fewer partitions and more

00:23:44,080 --> 00:23:47,440
consumers objects

00:23:45,520 --> 00:23:50,080
is going to work but then the consumers

00:23:47,440 --> 00:23:52,880
are going to kind of uh

00:23:50,080 --> 00:23:53,440
work through the number of partitions

00:23:52,880 --> 00:23:56,400
and

00:23:53,440 --> 00:23:58,159
you won't you won't have the actual

00:23:56,400 --> 00:23:59,039
parallelism that you should have in a

00:23:58,159 --> 00:24:02,320
kafka cluster

00:23:59,039 --> 00:24:04,400
right and for the producer side

00:24:02,320 --> 00:24:06,559
kafka provides some very unique ways for

00:24:04,400 --> 00:24:08,640
you to decide which partition or

00:24:06,559 --> 00:24:09,760
which part of the topic you are going to

00:24:08,640 --> 00:24:12,480
write the data right

00:24:09,760 --> 00:24:13,520
so kafka is just like a nosql database

00:24:12,480 --> 00:24:15,919
it has a key value

00:24:13,520 --> 00:24:17,279
right each record is composed by a key

00:24:15,919 --> 00:24:19,760
and a value

00:24:17,279 --> 00:24:21,520
although it is most commonly uh

00:24:19,760 --> 00:24:22,799
developers kind of write code to produce

00:24:21,520 --> 00:24:25,360
data without a key

00:24:22,799 --> 00:24:25,840
just the value right if you specify a

00:24:25,360 --> 00:24:27,600
key

00:24:25,840 --> 00:24:30,159
you actually going to control a little

00:24:27,600 --> 00:24:33,360
bit more which partition

00:24:30,159 --> 00:24:33,919
uh the data is going to be written right

00:24:33,360 --> 00:24:35,760
so

00:24:33,919 --> 00:24:39,039
you how to accomplish this using the

00:24:35,760 --> 00:24:39,039
concept of keys

00:24:42,240 --> 00:24:46,080
and one of the cool things about kafka

00:24:43,760 --> 00:24:46,880
as well is that kafka has no format it's

00:24:46,080 --> 00:24:48,799
schemeless

00:24:46,880 --> 00:24:50,080
right everything in kafka is an array of

00:24:48,799 --> 00:24:53,039
bytes

00:24:50,080 --> 00:24:53,520
because of this the producer can pretty

00:24:53,039 --> 00:24:55,919
much

00:24:53,520 --> 00:24:57,200
write and serialize any type of format

00:24:55,919 --> 00:24:59,679
they want it

00:24:57,200 --> 00:25:00,880
and the consumers they can pull and

00:24:59,679 --> 00:25:03,600
deserialize

00:25:00,880 --> 00:25:04,559
any type of format right so this is cool

00:25:03,600 --> 00:25:06,559
because

00:25:04,559 --> 00:25:08,240
it gives you the ability for multiple

00:25:06,559 --> 00:25:10,480
programming language to share and

00:25:08,240 --> 00:25:13,039
consume data simultaneously but

00:25:10,480 --> 00:25:14,480
you can easily like serialize something

00:25:13,039 --> 00:25:16,799
using.net

00:25:14,480 --> 00:25:19,200
uh and write it to kafka and then you

00:25:16,799 --> 00:25:22,480
can have a consumer written in java

00:25:19,200 --> 00:25:24,320
or python or go or any other programming

00:25:22,480 --> 00:25:25,600
language and deserialize the same data

00:25:24,320 --> 00:25:26,720
and being able to handle that data

00:25:25,600 --> 00:25:29,760
efficiently right

00:25:26,720 --> 00:25:30,320
so in order to accomplish this what you

00:25:29,760 --> 00:25:32,400
have to do

00:25:30,320 --> 00:25:34,640
is to rely on this concept about

00:25:32,400 --> 00:25:35,600
serialization and deserialization so you

00:25:34,640 --> 00:25:37,919
have

00:25:35,600 --> 00:25:39,520
uh ultimately you you have to your

00:25:37,919 --> 00:25:40,400
producer and consumers are going to use

00:25:39,520 --> 00:25:42,640
some sort of uh

00:25:40,400 --> 00:25:43,679
format right for example they can decide

00:25:42,640 --> 00:25:46,320
to use protobuf

00:25:43,679 --> 00:25:47,760
right uh so it's going to be the up to

00:25:46,320 --> 00:25:50,799
the producer to use some

00:25:47,760 --> 00:25:51,760
serializer that can pick up the data and

00:25:50,799 --> 00:25:53,679
protobuf

00:25:51,760 --> 00:25:54,960
and transform to the array of bytes that

00:25:53,679 --> 00:25:57,760
kakis expects

00:25:54,960 --> 00:25:59,440
right as well as the consumer needs to

00:25:57,760 --> 00:26:00,480
be able to deserialize data from the

00:25:59,440 --> 00:26:02,880
array of bytes

00:26:00,480 --> 00:26:04,400
into the protobuf format so they can

00:26:02,880 --> 00:26:06,320
start acting upon the data

00:26:04,400 --> 00:26:08,840
right but the reality is that this is

00:26:06,320 --> 00:26:11,440
possible so this is

00:26:08,840 --> 00:26:13,279
good

00:26:11,440 --> 00:26:14,880
and one of the key things about kafka is

00:26:13,279 --> 00:26:17,360
that kafka data is all

00:26:14,880 --> 00:26:18,240
persistent so persistency in kafka is

00:26:17,360 --> 00:26:20,000
interesting because

00:26:18,240 --> 00:26:21,840
it is not something you can disable

00:26:20,000 --> 00:26:23,679
right different from some messaging

00:26:21,840 --> 00:26:25,440
technology for example that uh yeah data

00:26:23,679 --> 00:26:26,400
is persistent but persistency is a

00:26:25,440 --> 00:26:28,960
feature right

00:26:26,400 --> 00:26:31,039
you toggle on and off right and kafka

00:26:28,960 --> 00:26:31,840
all the data is persistent they expire

00:26:31,039 --> 00:26:35,039
eventually

00:26:31,840 --> 00:26:37,520
because it has a retention period

00:26:35,039 --> 00:26:39,520
by before seven days but if you want the

00:26:37,520 --> 00:26:41,760
data to actually live for three months

00:26:39,520 --> 00:26:43,279
or a year just set the retention date

00:26:41,760 --> 00:26:46,559
that is a per topic

00:26:43,279 --> 00:26:48,159
to to that duration so uh it's going to

00:26:46,559 --> 00:26:51,360
be persistent for up there

00:26:48,159 --> 00:26:52,880
right uh another aspect from kafka

00:26:51,360 --> 00:26:54,880
is the ability to provide stream

00:26:52,880 --> 00:26:57,279
processing capabilities right so stream

00:26:54,880 --> 00:26:59,360
processing came from the need

00:26:57,279 --> 00:27:00,880
right for how do you actually do

00:26:59,360 --> 00:27:02,640
computation with kafka

00:27:00,880 --> 00:27:04,880
so basically you would have to write a

00:27:02,640 --> 00:27:05,760
consumer that periodically pulls data

00:27:04,880 --> 00:27:08,640
from the broker

00:27:05,760 --> 00:27:08,960
and bring the data in to its own cache

00:27:08,640 --> 00:27:11,440
or

00:27:08,960 --> 00:27:12,720
memory or somewhere where the data can

00:27:11,440 --> 00:27:14,799
be available for

00:27:12,720 --> 00:27:15,919
for for processing purposes and then

00:27:14,799 --> 00:27:17,279
you're going to do the processing

00:27:15,919 --> 00:27:19,600
right so for example you're going to

00:27:17,279 --> 00:27:21,919
filter all the records that has

00:27:19,600 --> 00:27:23,919
is less than four or maybe the records

00:27:21,919 --> 00:27:27,039
that are more than five so

00:27:23,919 --> 00:27:27,679
the result of that processing the common

00:27:27,039 --> 00:27:30,960
technique

00:27:27,679 --> 00:27:33,440
is to write the results back into kafka

00:27:30,960 --> 00:27:35,279
likely in another topic right let's call

00:27:33,440 --> 00:27:37,120
there there was the input topic

00:27:35,279 --> 00:27:38,880
and there was the output topic right so

00:27:37,120 --> 00:27:39,360
the results of the processing would be

00:27:38,880 --> 00:27:42,399
written

00:27:39,360 --> 00:27:43,919
into the output topic right so any

00:27:42,399 --> 00:27:46,960
application that is interested

00:27:43,919 --> 00:27:48,480
on the not only raw data but the data

00:27:46,960 --> 00:27:51,600
computed

00:27:48,480 --> 00:27:53,919
would read or pull data straight

00:27:51,600 --> 00:27:54,960
from the output topic right but the

00:27:53,919 --> 00:27:58,080
reality is that

00:27:54,960 --> 00:27:59,360
uh making this process work scalably is

00:27:58,080 --> 00:28:01,520
very complicated because

00:27:59,360 --> 00:28:04,080
there's too much code to write so that's

00:28:01,520 --> 00:28:06,000
why in the kafka ecosystem

00:28:04,080 --> 00:28:07,120
they decided to provide a framework to

00:28:06,000 --> 00:28:09,360
do all this

00:28:07,120 --> 00:28:10,799
process of the process here and then

00:28:09,360 --> 00:28:13,039
they end up with something called

00:28:10,799 --> 00:28:14,720
stream processors right so stream

00:28:13,039 --> 00:28:18,159
processors are basically

00:28:14,720 --> 00:28:18,960
frameworks or sdks or just technologies

00:28:18,159 --> 00:28:20,960
out there

00:28:18,960 --> 00:28:22,960
that allows you to concentrate your

00:28:20,960 --> 00:28:24,960
logic of processing the data

00:28:22,960 --> 00:28:26,880
without necessarily having to handle the

00:28:24,960 --> 00:28:28,399
whole plumbing of kaffir for example

00:28:26,880 --> 00:28:30,320
you don't have to handle details of

00:28:28,399 --> 00:28:32,240
partitioning or fade over

00:28:30,320 --> 00:28:33,679
or full tolerancy or how the

00:28:32,240 --> 00:28:36,799
serialization works

00:28:33,679 --> 00:28:38,720
things like that are very abstracted

00:28:36,799 --> 00:28:40,480
away from the developer right so

00:28:38,720 --> 00:28:42,320
there will be still the producers

00:28:40,480 --> 00:28:44,880
writing the data the consumers

00:28:42,320 --> 00:28:47,279
reading the data and in between we might

00:28:44,880 --> 00:28:49,440
have string processor that will hook up

00:28:47,279 --> 00:28:51,760
into the kafka cluster and then they

00:28:49,440 --> 00:28:54,000
will be concurrently as the data happens

00:28:51,760 --> 00:28:55,039
or as the data flows the brokers or the

00:28:54,000 --> 00:28:57,360
kafka cluster

00:28:55,039 --> 00:28:58,480
they will process them and flush the

00:28:57,360 --> 00:29:02,240
results out

00:28:58,480 --> 00:29:04,159
into output topics right uh namely

00:29:02,240 --> 00:29:05,919
there are two main technologies that you

00:29:04,159 --> 00:29:08,320
can use for implement your own string

00:29:05,919 --> 00:29:11,039
processor the first and most obvious one

00:29:08,320 --> 00:29:12,960
is what comes with kafka out of the box

00:29:11,039 --> 00:29:15,120
and as part of the kafka uh

00:29:12,960 --> 00:29:17,039
framework you have this guy called

00:29:15,120 --> 00:29:19,760
skafix streams right which is basically

00:29:17,039 --> 00:29:22,399
a java or jvm compatible library

00:29:19,760 --> 00:29:23,440
that you can implement a code similar to

00:29:22,399 --> 00:29:26,240
this one over here

00:29:23,440 --> 00:29:28,159
for example what a filter all the red uh

00:29:26,240 --> 00:29:29,840
squares over here so this is probably

00:29:28,159 --> 00:29:30,640
the type of code you would have to write

00:29:29,840 --> 00:29:32,960
in java

00:29:30,640 --> 00:29:33,840
in order to make that happen right and

00:29:32,960 --> 00:29:36,000
then

00:29:33,840 --> 00:29:37,679
there is also this uh implementation

00:29:36,000 --> 00:29:41,600
call is an open source project

00:29:37,679 --> 00:29:44,240
as well called key sql db that basically

00:29:41,600 --> 00:29:46,399
it provides an abstraction layer that

00:29:44,240 --> 00:29:47,360
has been built on top of kafka stream so

00:29:46,399 --> 00:29:49,919
ultimately

00:29:47,360 --> 00:29:50,880
it is all kafka streams running running

00:29:49,919 --> 00:29:52,559
underneath

00:29:50,880 --> 00:29:54,240
but for the developer standpoint you can

00:29:52,559 --> 00:29:56,080
actually express your stream processing

00:29:54,240 --> 00:29:58,320
using this sql

00:29:56,080 --> 00:30:00,080
type of language right okay security b

00:29:58,320 --> 00:30:01,679
is a fairly new project

00:30:00,080 --> 00:30:03,760
it's a very good one it is being

00:30:01,679 --> 00:30:06,159
maintained it's not from apache

00:30:03,760 --> 00:30:07,600
but it's been maintained and is uh

00:30:06,159 --> 00:30:10,880
developed by confluent

00:30:07,600 --> 00:30:13,440
right so ultimately is not available on

00:30:10,880 --> 00:30:13,840
the apache license of 2.0 license right

00:30:13,440 --> 00:30:16,399
so

00:30:13,840 --> 00:30:18,000
keep that in mind uh there are other

00:30:16,399 --> 00:30:19,840
frameworks that can be used as

00:30:18,000 --> 00:30:21,200
string processors as well which is our

00:30:19,840 --> 00:30:24,080
equally powerful

00:30:21,200 --> 00:30:25,360
uh arguably sometimes even better than

00:30:24,080 --> 00:30:27,840
uh k super db

00:30:25,360 --> 00:30:28,480
which is apache flink for example apache

00:30:27,840 --> 00:30:30,799
flink

00:30:28,480 --> 00:30:32,159
is a more mature string processing

00:30:30,799 --> 00:30:32,720
framework that has been around for some

00:30:32,159 --> 00:30:36,000
time

00:30:32,720 --> 00:30:38,960
right uh and has some capabilities that

00:30:36,000 --> 00:30:40,320
key sql db is catching up right very

00:30:38,960 --> 00:30:42,880
quickly but

00:30:40,320 --> 00:30:44,799
if you if all you need is a very mature

00:30:42,880 --> 00:30:46,000
implementation on top of some stream

00:30:44,799 --> 00:30:48,720
processing framework that

00:30:46,000 --> 00:30:49,520
allows you to not concentrate of oh yeah

00:30:48,720 --> 00:30:50,960
we have to kind of

00:30:49,520 --> 00:30:53,279
create a workaround because this is not

00:30:50,960 --> 00:30:55,120
supported yet apache flink might be your

00:30:53,279 --> 00:30:56,159
thing right or kafka streams because

00:30:55,120 --> 00:30:59,519
kafka streams

00:30:56,159 --> 00:31:03,120
is a very mature project as well right

00:30:59,519 --> 00:31:04,880
uh and then there is this concept of

00:31:03,120 --> 00:31:06,960
scalable data integration so scalable

00:31:04,880 --> 00:31:08,880
data integration and kafka is handled by

00:31:06,960 --> 00:31:11,039
this firmware called kafkaconnect

00:31:08,880 --> 00:31:12,720
right which i've just delivered

00:31:11,039 --> 00:31:14,000
presentations showing you how to run

00:31:12,720 --> 00:31:17,360
your own connector for

00:31:14,000 --> 00:31:19,600
right kafka connect basically the job of

00:31:17,360 --> 00:31:22,000
kafka connect is to

00:31:19,600 --> 00:31:24,159
have this concept of connectors which

00:31:22,000 --> 00:31:26,399
basically is going to allow you to read

00:31:24,159 --> 00:31:28,880
the data from the brokers or from kafka

00:31:26,399 --> 00:31:29,519
and for send somewhere else for example

00:31:28,880 --> 00:31:31,120
i want to

00:31:29,519 --> 00:31:32,559
want to read data from the kafka broker

00:31:31,120 --> 00:31:35,760
and then maybe right

00:31:32,559 --> 00:31:36,880
into uh elasticsearch right so

00:31:35,760 --> 00:31:40,159
elasticsearch

00:31:36,880 --> 00:31:41,440
is a nosql database as you know right so

00:31:40,159 --> 00:31:43,200
if you want to do this you can use

00:31:41,440 --> 00:31:44,159
connectors for kafka or maybe it's the

00:31:43,200 --> 00:31:46,399
inverse right

00:31:44,159 --> 00:31:47,519
you have data available on elasticsearch

00:31:46,399 --> 00:31:49,760
and you can use

00:31:47,519 --> 00:31:51,600
one of these connectors for rated data

00:31:49,760 --> 00:31:52,640
from from elasticsearch and bring it

00:31:51,600 --> 00:31:54,720
into kafka right

00:31:52,640 --> 00:31:56,720
oh why do you do this ricardo oh maybe

00:31:54,720 --> 00:31:58,399
because the data that is available in

00:31:56,720 --> 00:32:00,240
elasticsearch you want to make available

00:31:58,399 --> 00:32:02,080
for somewhere else right that

00:32:00,240 --> 00:32:04,720
in that somewhere else doesn't

00:32:02,080 --> 00:32:05,679
necessarily has the ability or cannot or

00:32:04,720 --> 00:32:07,600
should not

00:32:05,679 --> 00:32:09,200
read directly from elasticsearch for

00:32:07,600 --> 00:32:10,720
some reason i don't know i can't seek

00:32:09,200 --> 00:32:12,480
out of any motive from this because

00:32:10,720 --> 00:32:14,000
elasticsearch is a pretty open uh

00:32:12,480 --> 00:32:16,799
framework as well but

00:32:14,000 --> 00:32:18,960
you can do this using kafka if you want

00:32:16,799 --> 00:32:21,279
right

00:32:18,960 --> 00:32:22,480
and then let's talk about uh we have

00:32:21,279 --> 00:32:25,919
about three minutes

00:32:22,480 --> 00:32:28,640
until our q a let this is our last topic

00:32:25,919 --> 00:32:31,120
about uh this trickery of kafka which is

00:32:28,640 --> 00:32:32,480
kafka as a storage system right so

00:32:31,120 --> 00:32:34,320
one of the things you have to know about

00:32:32,480 --> 00:32:38,240
kafka is that kafka currently

00:32:34,320 --> 00:32:40,720
right uh october 2020 currently kafka

00:32:38,240 --> 00:32:43,919
uses the store system that is based on a

00:32:40,720 --> 00:32:45,840
virtual stars later that is composed by

00:32:43,919 --> 00:32:47,360
all the kafka brokers right so

00:32:45,840 --> 00:32:49,440
if you want to have one kafka broker

00:32:47,360 --> 00:32:51,519
that has one terabyte of storage

00:32:49,440 --> 00:32:53,760
and if you put the same kafka broker in

00:32:51,519 --> 00:32:56,240
a cluster with another kafka broker with

00:32:53,760 --> 00:32:57,279
1.5 terabytes of storage they're going

00:32:56,240 --> 00:33:00,640
to form a

00:32:57,279 --> 00:33:02,000
theoretical 2.5 terabytes of storage

00:33:00,640 --> 00:33:05,279
copper cluster right

00:33:02,000 --> 00:33:07,279
so if you come from radu apache hadoop

00:33:05,279 --> 00:33:08,720
this is the same storage implementation

00:33:07,279 --> 00:33:11,679
that arguably

00:33:08,720 --> 00:33:13,120
kafka kind of uh was inspired on hadoop

00:33:11,679 --> 00:33:15,200
when they came up with this uh

00:33:13,120 --> 00:33:17,360
approach uh when he was originally

00:33:15,200 --> 00:33:18,559
created so keep that in mind that's how

00:33:17,360 --> 00:33:20,480
you kind of increase

00:33:18,559 --> 00:33:22,240
elastically storage of your kafka

00:33:20,480 --> 00:33:24,080
cluster right

00:33:22,240 --> 00:33:26,080
i mentioned that uh this is right now

00:33:24,080 --> 00:33:27,600
because kafka has in place some

00:33:26,080 --> 00:33:29,760
something called a tiered storage

00:33:27,600 --> 00:33:32,320
there's a kafka improvement proposal

00:33:29,760 --> 00:33:33,279
for implementing something that apache

00:33:32,320 --> 00:33:35,840
pulsar

00:33:33,279 --> 00:33:38,240
has already in place which is uh they

00:33:35,840 --> 00:33:38,880
kind of separate the storage layer into

00:33:38,240 --> 00:33:42,000
a more

00:33:38,880 --> 00:33:43,200
permanent storage layer uh usually based

00:33:42,000 --> 00:33:45,200
on apache boot keeper

00:33:43,200 --> 00:33:47,039
right which is pretty cool so you can

00:33:45,200 --> 00:33:50,399
have a code hot and code

00:33:47,039 --> 00:33:52,720
storage and you can kind of grow more

00:33:50,399 --> 00:33:54,159
economically efficiently your coffer

00:33:52,720 --> 00:33:55,840
cluster so in other words

00:33:54,159 --> 00:33:57,200
if you want to want to grow your storage

00:33:55,840 --> 00:33:58,000
you don't necessarily have to put more

00:33:57,200 --> 00:34:00,880
compute

00:33:58,000 --> 00:34:02,880
you just put more storage so this is one

00:34:00,880 --> 00:34:04,720
of the drawbacks from kafka right now so

00:34:02,880 --> 00:34:06,000
keep that in mind when you're sizing

00:34:04,720 --> 00:34:08,720
your kafka cluster

00:34:06,000 --> 00:34:10,079
right and remember when i'm going to

00:34:08,720 --> 00:34:11,839
explain about partitions right

00:34:10,079 --> 00:34:12,800
partitions is how kafka addresses the

00:34:11,839 --> 00:34:14,800
scalability

00:34:12,800 --> 00:34:16,320
each partition in a kafka cluster can

00:34:14,800 --> 00:34:18,480
also have replicas right so

00:34:16,320 --> 00:34:20,240
those replicas are going to be spread

00:34:18,480 --> 00:34:22,960
also in a coffee cluster

00:34:20,240 --> 00:34:24,639
preferably in a different broker that is

00:34:22,960 --> 00:34:25,280
not where the primary partition is

00:34:24,639 --> 00:34:27,520
running

00:34:25,280 --> 00:34:28,879
for failover purposes so if you ever

00:34:27,520 --> 00:34:31,200
lose a partition

00:34:28,879 --> 00:34:32,399
that partition can actually be restored

00:34:31,200 --> 00:34:36,079
for another broker

00:34:32,399 --> 00:34:37,599
okay and kafka starts system is constant

00:34:36,079 --> 00:34:40,079
time as i mentioned before remember the

00:34:37,599 --> 00:34:41,599
example of the array and indexes right

00:34:40,079 --> 00:34:43,119
so it doesn't make any difference if

00:34:41,599 --> 00:34:45,119
you're handling in kafka

00:34:43,119 --> 00:34:47,280
five kilobytes of data or five terabytes

00:34:45,119 --> 00:34:48,720
of data if you ever have to read like

00:34:47,280 --> 00:34:52,079
100 records

00:34:48,720 --> 00:34:55,040
uh the the time spent for

00:34:52,079 --> 00:34:56,000
for that is going to be the same right

00:34:55,040 --> 00:34:57,359
and finally

00:34:56,000 --> 00:34:59,920
this is one of the key things about

00:34:57,359 --> 00:35:02,480
kafka but how why kafk is so fast

00:34:59,920 --> 00:35:03,760
because kafka serves all the data from

00:35:02,480 --> 00:35:05,760
the page cache so

00:35:03,760 --> 00:35:07,200
remember when i mentioned that kafka is

00:35:05,760 --> 00:35:10,000
persistent by nature

00:35:07,200 --> 00:35:11,119
so all the data is kind of stored in a

00:35:10,000 --> 00:35:13,920
file system

00:35:11,119 --> 00:35:15,680
right in a bunch of file called segments

00:35:13,920 --> 00:35:17,680
and a copy of those segments

00:35:15,680 --> 00:35:19,440
is kind of a reading up into the the

00:35:17,680 --> 00:35:21,200
main memory from the parenting system

00:35:19,440 --> 00:35:23,040
which is the page cache right

00:35:21,200 --> 00:35:24,800
every time a consumer tries to pull the

00:35:23,040 --> 00:35:26,480
data the data is served

00:35:24,800 --> 00:35:28,160
directly from the page cache so

00:35:26,480 --> 00:35:31,119
optimization number one

00:35:28,160 --> 00:35:33,119
optimization number two kafka has this

00:35:31,119 --> 00:35:36,560
makes heavily used the send file

00:35:33,119 --> 00:35:39,040
api which basically does a bypass of the

00:35:36,560 --> 00:35:40,640
kernel space and basically serve the

00:35:39,040 --> 00:35:43,359
data directly to the

00:35:40,640 --> 00:35:44,960
network socket buffer which in turn came

00:35:43,359 --> 00:35:46,240
directly from the page cache so

00:35:44,960 --> 00:35:48,800
for those of you that know what send

00:35:46,240 --> 00:35:50,480
file api does it basically bypass

00:35:48,800 --> 00:35:52,640
all the copies that the kernel needs to

00:35:50,480 --> 00:35:54,160
do every time it will serve

00:35:52,640 --> 00:35:56,320
some data that is on the application

00:35:54,160 --> 00:35:58,160
layer to the network card layer so

00:35:56,320 --> 00:36:00,079
this is why one of the reasons kafka

00:35:58,160 --> 00:36:03,520
provides a very good throughput

00:36:00,079 --> 00:36:05,520
um i think we're running out of time and

00:36:03,520 --> 00:36:07,440
luckily this is actually the the last

00:36:05,520 --> 00:36:10,160
thing i would like to share about kafka

00:36:07,440 --> 00:36:11,280
so i'm gonna stop presenting right now

00:36:10,160 --> 00:36:16,800
and take a look

00:36:11,280 --> 00:36:18,320
on the chat for a second

00:36:16,800 --> 00:36:20,079
if there are any questions so let's

00:36:18,320 --> 00:36:20,560
spend about let's spend the next five

00:36:20,079 --> 00:36:22,960
minutes

00:36:20,560 --> 00:36:24,160
uh with any questions that you might

00:36:22,960 --> 00:36:29,280
have

00:36:24,160 --> 00:36:29,280
okay so let me take a look at the chat

00:36:32,560 --> 00:36:37,680
oh for now there's only felix comments

00:36:35,359 --> 00:36:37,680
okay

00:36:38,079 --> 00:36:42,720
okay so if you have if you have any

00:36:40,880 --> 00:36:45,280
questions feel free to ask right now

00:36:42,720 --> 00:36:46,640
um i'm gonna be around available for for

00:36:45,280 --> 00:36:50,480
any questions you might have

00:36:46,640 --> 00:36:51,839
uh but hopefully you um you were able to

00:36:50,480 --> 00:36:54,400
understand about a little bit more about

00:36:51,839 --> 00:36:54,960
how kafka works and what kafka really is

00:36:54,400 --> 00:36:57,359
i think the

00:36:54,960 --> 00:36:58,800
the the main value of this presentation

00:36:57,359 --> 00:37:00,720
is for you to understand

00:36:58,800 --> 00:37:02,640
what kafka really is and what

00:37:00,720 --> 00:37:03,520
expectations you should have around

00:37:02,640 --> 00:37:06,560
kafka

00:37:03,520 --> 00:37:10,720
because um it is an amazing technology

00:37:06,560 --> 00:37:10,720
but sometimes is uh misinterpreted

00:37:12,160 --> 00:37:15,839
okay so andy has a question you

00:37:13,839 --> 00:37:19,200
mentioned kc could be b huh

00:37:15,839 --> 00:37:24,079
how does that compare with kafka plus

00:37:19,200 --> 00:37:27,760
flink sql for example uh it is

00:37:24,079 --> 00:37:31,119
it is basically a uh equivalent approach

00:37:27,760 --> 00:37:32,960
um because flink sql is basically

00:37:31,119 --> 00:37:34,400
running on this their own infrastructure

00:37:32,960 --> 00:37:36,000
that connects to kafka right

00:37:34,400 --> 00:37:37,760
so key sequel squared b has the same

00:37:36,000 --> 00:37:40,240
approach you would have your own

00:37:37,760 --> 00:37:42,160
k sequel to be infrastructuring right

00:37:40,240 --> 00:37:43,440
which is connecting to kafka to do the

00:37:42,160 --> 00:37:45,839
processing right

00:37:43,440 --> 00:37:47,680
but i would say that flink sql is a bit

00:37:45,839 --> 00:37:49,680
more mature than k sql db

00:37:47,680 --> 00:37:51,599
right but fundamentally they offer the

00:37:49,680 --> 00:37:52,400
same thing right they basically does the

00:37:51,599 --> 00:37:54,640
same job

00:37:52,400 --> 00:37:54,640
right

00:37:55,440 --> 00:37:59,520
uh can you please explain why the

00:37:58,000 --> 00:38:01,680
throughput of kafka

00:37:59,520 --> 00:38:05,119
is independent of the number of records

00:38:01,680 --> 00:38:08,880
being requested

00:38:05,119 --> 00:38:10,800
okay so when i when i mentioned that

00:38:08,880 --> 00:38:11,920
i was referring for the constant time

00:38:10,800 --> 00:38:14,720
performance so

00:38:11,920 --> 00:38:16,320
what i was mentioning is what if you

00:38:14,720 --> 00:38:18,320
have a database right for example let's

00:38:16,320 --> 00:38:19,920
compare with the database

00:38:18,320 --> 00:38:21,680
if you have five kilobytes of data

00:38:19,920 --> 00:38:24,160
stored on the database right

00:38:21,680 --> 00:38:25,040
reading one record from the database is

00:38:24,160 --> 00:38:28,640
gonna be fast

00:38:25,040 --> 00:38:31,119
okay if you have five terabytes

00:38:28,640 --> 00:38:33,680
store it in the database if you try to

00:38:31,119 --> 00:38:36,160
read one record from the database

00:38:33,680 --> 00:38:37,040
it's going to be a little slower okay so

00:38:36,160 --> 00:38:40,160
in other words

00:38:37,040 --> 00:38:41,920
the volume of the data storage is going

00:38:40,160 --> 00:38:44,240
to impact the performance of one

00:38:41,920 --> 00:38:46,800
individual request in a database

00:38:44,240 --> 00:38:48,560
in kafka this doesn't happen right so

00:38:46,800 --> 00:38:51,520
what i've mentioned is that

00:38:48,560 --> 00:38:52,079
in the regardless of the the data volume

00:38:51,520 --> 00:38:55,280
reading

00:38:52,079 --> 00:38:57,520
one record right that's the key thing

00:38:55,280 --> 00:38:59,359
one record in five kilobytes and one

00:38:57,520 --> 00:39:00,960
records and five terabytes

00:38:59,359 --> 00:39:03,040
the performance is going to be the same

00:39:00,960 --> 00:39:04,800
because it's constant time based

00:39:03,040 --> 00:39:06,240
on the offset if you know the offset of

00:39:04,800 --> 00:39:07,680
the data the log

00:39:06,240 --> 00:39:09,520
has been positioned that and you just

00:39:07,680 --> 00:39:10,960
read so

00:39:09,520 --> 00:39:12,800
it doesn't matter how much data you have

00:39:10,960 --> 00:39:15,839
stored that's what i meant to do

00:39:12,800 --> 00:39:15,839
meant to say

00:39:16,800 --> 00:39:21,839
okay um anything you have to add about

00:39:19,680 --> 00:39:23,599
data flow management orchestration

00:39:21,839 --> 00:39:25,520
any need to manage back pressure for

00:39:23,599 --> 00:39:29,359
example like scifi

00:39:25,520 --> 00:39:31,040
yes lauren so back pressure is probably

00:39:29,359 --> 00:39:33,680
one of the main

00:39:31,040 --> 00:39:35,280
uh characteristics that kafka doesn't

00:39:33,680 --> 00:39:37,280
provide out of the box that i've seen

00:39:35,280 --> 00:39:39,920
many developers complaining about it

00:39:37,280 --> 00:39:41,680
uh mostly because it's kind of a good

00:39:39,920 --> 00:39:44,160
problem to have like kafka

00:39:41,680 --> 00:39:46,640
it's so performance and being able to

00:39:44,160 --> 00:39:48,480
deliver data so fast that sometimes

00:39:46,640 --> 00:39:49,680
the downstream system cannot keep up

00:39:48,480 --> 00:39:50,480
with the throughput that kafka is

00:39:49,680 --> 00:39:53,119
delivering right

00:39:50,480 --> 00:39:54,400
and employment back pressure techniques

00:39:53,119 --> 00:39:56,400
is something that yes

00:39:54,400 --> 00:39:58,079
you should be aware about it and kafka

00:39:56,400 --> 00:39:59,520
doesn't provide a very good story about

00:39:58,079 --> 00:40:01,440
it so far

00:39:59,520 --> 00:40:02,560
because it's all about all up to the

00:40:01,440 --> 00:40:05,920
consumers to

00:40:02,560 --> 00:40:07,359
kind of pull the data uh and

00:40:05,920 --> 00:40:09,119
and the frequencies that they can handle

00:40:07,359 --> 00:40:11,839
it right so it's

00:40:09,119 --> 00:40:11,839
yeah that's a good question

00:40:12,640 --> 00:40:15,440
okay

00:40:17,119 --> 00:40:21,359
this is a similar to that okay what are

00:40:19,680 --> 00:40:25,359
the main kafka production bottlenecks

00:40:21,359 --> 00:40:25,359
before posh pulsar's greater termite

00:40:26,400 --> 00:40:30,960
is a great alternative that's for sure

00:40:28,079 --> 00:40:33,680
that's a given uh i would say the kafka

00:40:30,960 --> 00:40:35,280
i mean usually what the main problems of

00:40:33,680 --> 00:40:37,520
scalability of kafka is not

00:40:35,280 --> 00:40:39,200
sizing properly the number of partitions

00:40:37,520 --> 00:40:41,839
right that's number one

00:40:39,200 --> 00:40:42,880
number two uh kafka architecture is

00:40:41,839 --> 00:40:46,800
extremely

00:40:42,880 --> 00:40:49,280
disk io bound and network io bound

00:40:46,800 --> 00:40:50,400
right it is is way less about cpu and

00:40:49,280 --> 00:40:53,359
memory right

00:40:50,400 --> 00:40:53,920
so with that said uh what i've seen out

00:40:53,359 --> 00:40:56,400
there is that

00:40:53,920 --> 00:40:57,200
most kafka deployments not uh sizing

00:40:56,400 --> 00:40:59,440
properly the

00:40:57,200 --> 00:41:01,839
infrastructure layer thinking about disk

00:40:59,440 --> 00:41:03,760
and network performance right so

00:41:01,839 --> 00:41:05,520
if you're in referendum for circuit

00:41:03,760 --> 00:41:07,359
background you know that you have to

00:41:05,520 --> 00:41:09,119
serious consideration when regards

00:41:07,359 --> 00:41:11,440
networking so that's something that

00:41:09,119 --> 00:41:15,200
makes a difference in the kafka cluster

00:41:11,440 --> 00:41:18,640
um but yeah kafka pulsar is

00:41:15,200 --> 00:41:19,200
a great alternative can i use kafka to

00:41:18,640 --> 00:41:22,160
build

00:41:19,200 --> 00:41:23,200
pipeline between cloud and primitive yes

00:41:22,160 --> 00:41:24,880
elders yes

00:41:23,200 --> 00:41:26,800
definitely this is a very popular use

00:41:24,880 --> 00:41:30,000
case from kafka and

00:41:26,800 --> 00:41:31,520
primarily because kafka can plug in into

00:41:30,000 --> 00:41:33,200
and pass through fires very easily

00:41:31,520 --> 00:41:36,160
because it's tcp basic

00:41:33,200 --> 00:41:37,839
and also it provides very resiliency for

00:41:36,160 --> 00:41:39,440
the data so yeah

00:41:37,839 --> 00:41:42,560
and matter of fact there's a lot of

00:41:39,440 --> 00:41:44,560
migration scenarios from

00:41:42,560 --> 00:41:45,920
leaving one cloud provider to another

00:41:44,560 --> 00:41:49,280
using kafka for that so

00:41:45,920 --> 00:41:50,960
yeah i have a doubt about testing those

00:41:49,280 --> 00:41:52,880
string processors for microservice for

00:41:50,960 --> 00:41:53,839
instance we can apply content-driven

00:41:52,880 --> 00:41:55,839
testing

00:41:53,839 --> 00:41:58,079
to kind of integration tests and of the

00:41:55,839 --> 00:42:00,319
without a full supply of

00:41:58,079 --> 00:42:02,160
borgia i don't think i follow your

00:42:00,319 --> 00:42:04,319
question sorry

00:42:02,160 --> 00:42:05,520
okay so there's a continuation build

00:42:04,319 --> 00:42:06,319
sorry i said without finishing the

00:42:05,520 --> 00:42:08,480
question

00:42:06,319 --> 00:42:09,599
is there a good pr approach of events

00:42:08,480 --> 00:42:13,440
and stream processor

00:42:09,599 --> 00:42:15,119
or is it important yes yes no definitely

00:42:13,440 --> 00:42:17,359
contract driven development still

00:42:15,119 --> 00:42:17,760
applies for string processors basically

00:42:17,359 --> 00:42:20,960
uh

00:42:17,760 --> 00:42:24,079
if you're building an api that has to

00:42:20,960 --> 00:42:25,359
obey a contract uh yeah your stream

00:42:24,079 --> 00:42:27,040
processors definitely have to follow

00:42:25,359 --> 00:42:27,760
that design and there is some testing

00:42:27,040 --> 00:42:30,079
frameworks

00:42:27,760 --> 00:42:31,520
that you can use in order to help with

00:42:30,079 --> 00:42:33,839
that as well so

00:42:31,520 --> 00:42:34,960
that but yeah the concern applies your

00:42:33,839 --> 00:42:37,280
stream processors

00:42:34,960 --> 00:42:39,599
needs to follow a counter driven design

00:42:37,280 --> 00:42:39,599
design

00:42:40,400 --> 00:42:44,079
uh gents what's does it make sense to

00:42:43,359 --> 00:42:46,560
use kafka

00:42:44,079 --> 00:42:47,920
on single computer node thinking of edge

00:42:46,560 --> 00:42:49,280
computing processing data from

00:42:47,920 --> 00:42:53,359
prediction machines

00:42:49,280 --> 00:42:56,400
uh jens let me put this way

00:42:53,359 --> 00:42:59,680
it technically works right

00:42:56,400 --> 00:43:01,440
but i think it would be preferable if

00:42:59,680 --> 00:43:04,960
you use a more lightweight

00:43:01,440 --> 00:43:07,760
broker technology such as mqtt right

00:43:04,960 --> 00:43:08,800
and why is that uh because kafka is an

00:43:07,760 --> 00:43:11,440
extremely

00:43:08,800 --> 00:43:12,000
disk driven disc io bound and network io

00:43:11,440 --> 00:43:15,040
bound

00:43:12,000 --> 00:43:15,680
and in the edge or in the fog as we call

00:43:15,040 --> 00:43:17,359
it

00:43:15,680 --> 00:43:18,800
is not a very kind of an interesting

00:43:17,359 --> 00:43:21,119
technology to have so

00:43:18,800 --> 00:43:22,319
i i would bet on more lightweight broker

00:43:21,119 --> 00:43:27,520
technologies such as

00:43:22,319 --> 00:43:30,560
mqtt for example

00:43:27,520 --> 00:43:33,680
and then there's a question here from

00:43:30,560 --> 00:43:35,760
dion jaiden on pusar

00:43:33,680 --> 00:43:37,599
since they are similar and now under

00:43:35,760 --> 00:43:39,839
apache software foundation

00:43:37,599 --> 00:43:41,280
are they likely to merge on some point

00:43:39,839 --> 00:43:44,880
in the future

00:43:41,280 --> 00:43:47,359
uh i juden i really don't know but

00:43:44,880 --> 00:43:49,119
uh that's a very interesting question

00:43:47,359 --> 00:43:50,480
because

00:43:49,119 --> 00:43:52,880
they have they definitely share

00:43:50,480 --> 00:43:53,680
similarities i if i would bet right now

00:43:52,880 --> 00:43:56,640
i would say

00:43:53,680 --> 00:43:58,319
no that's very unlikely mostly because

00:43:56,640 --> 00:44:01,440
there are different code bases

00:43:58,319 --> 00:44:01,920
right and because of the different code

00:44:01,440 --> 00:44:03,839
base

00:44:01,920 --> 00:44:05,599
merging this type of project is not a

00:44:03,839 --> 00:44:08,400
very easy task right

00:44:05,599 --> 00:44:09,520
so it's more about the amount of work

00:44:08,400 --> 00:44:12,079
that you would have

00:44:09,520 --> 00:44:12,640
for merging those projects then actually

00:44:12,079 --> 00:44:14,560
if

00:44:12,640 --> 00:44:16,079
if it makes sense about yes it does make

00:44:14,560 --> 00:44:17,920
sense right but

00:44:16,079 --> 00:44:23,839
yeah i think the merge would be very

00:44:17,920 --> 00:44:23,839
complicated to have

00:44:24,000 --> 00:44:29,760
i think i'm answering all the questions

00:44:27,280 --> 00:44:30,880
but if someone has another one feel free

00:44:29,760 --> 00:44:34,319
to shoot

00:44:30,880 --> 00:44:34,319
let me just see if i haven't

00:44:34,720 --> 00:44:38,079
oh lori has one anything you have to say

00:44:36,560 --> 00:44:41,200
about time windowing

00:44:38,079 --> 00:44:43,200
uh okay yeah so lauren

00:44:41,200 --> 00:44:44,640
asked about time windows so time window

00:44:43,200 --> 00:44:46,240
is something that you can implement

00:44:44,640 --> 00:44:49,520
using kafka streams and kc

00:44:46,240 --> 00:44:50,960
db or apache flink as well so usually

00:44:49,520 --> 00:44:52,480
all those three frameworks

00:44:50,960 --> 00:44:54,319
there's very good support for time

00:44:52,480 --> 00:44:57,040
rendering for example uh

00:44:54,319 --> 00:44:57,839
tumbling windows session windows hoping

00:44:57,040 --> 00:45:00,480
windows

00:44:57,839 --> 00:45:01,839
so yeah windowing is something that

00:45:00,480 --> 00:45:04,319
definitely you have to use a stream

00:45:01,839 --> 00:45:06,400
processor framework for it right uh

00:45:04,319 --> 00:45:07,760
the client's api from kafka doesn't

00:45:06,400 --> 00:45:09,680
support for that so

00:45:07,760 --> 00:45:11,200
this is something you would find

00:45:09,680 --> 00:45:14,640
natively on the

00:45:11,200 --> 00:45:17,040
stream processing frameworks um

00:45:14,640 --> 00:45:19,119
and then andy asked any guess on the

00:45:17,040 --> 00:45:22,160
timeline for the kitchen storage

00:45:19,119 --> 00:45:25,440
andy i had hopes that

00:45:22,160 --> 00:45:26,319
the tiered storage would come by the end

00:45:25,440 --> 00:45:28,640
of this year

00:45:26,319 --> 00:45:29,839
right but the reality is that they're

00:45:28,640 --> 00:45:32,560
still under discussion

00:45:29,839 --> 00:45:33,440
right um and if if i it could provide

00:45:32,560 --> 00:45:35,359
some advice

00:45:33,440 --> 00:45:37,280
this is something that i've done for

00:45:35,359 --> 00:45:40,640
some time is that there is the uh

00:45:37,280 --> 00:45:43,119
apache kafka mailing list where you can

00:45:40,640 --> 00:45:45,599
subscribe and follow up with the kip uh

00:45:43,119 --> 00:45:47,359
more often instead of going to the jira

00:45:45,599 --> 00:45:48,000
and then just see the status of the

00:45:47,359 --> 00:45:50,880
approach

00:45:48,000 --> 00:45:53,359
so what i have seen it and i've just

00:45:50,880 --> 00:45:55,359
read uh the description recently

00:45:53,359 --> 00:45:58,240
they're still discussing like uh

00:45:55,359 --> 00:46:00,560
implementation details of the skip so

00:45:58,240 --> 00:46:01,599
my expectation is that it may take may

00:46:00,560 --> 00:46:03,200
take some time based

00:46:01,599 --> 00:46:04,800
on that because on the rise they would

00:46:03,200 --> 00:46:06,800
be discussing something like uh

00:46:04,800 --> 00:46:08,240
okay the code is done and that just

00:46:06,800 --> 00:46:10,480
merged the code so

00:46:08,240 --> 00:46:11,839
that type of discussion hasn't been i

00:46:10,480 --> 00:46:15,839
have i haven't seen so far

00:46:11,839 --> 00:46:15,839
yeah that's the reality

00:46:18,480 --> 00:46:22,319
uh what's your opinion about k sequel

00:46:20,319 --> 00:46:22,560
and experience prediction uh luis yeah

00:46:22,319 --> 00:46:25,200
que

00:46:22,560 --> 00:46:26,319
sequo is a former name now it's called

00:46:25,200 --> 00:46:28,800
kc pdb

00:46:26,319 --> 00:46:30,480
it's a very good technology right um it

00:46:28,800 --> 00:46:32,160
yes definitely has a lot of

00:46:30,480 --> 00:46:34,000
i've seen a lot of people using in

00:46:32,160 --> 00:46:37,599
production it works fine

00:46:34,000 --> 00:46:38,880
i um but so yeah uh if i were you i

00:46:37,599 --> 00:46:42,400
should give a try in kc

00:46:38,880 --> 00:46:46,720
db because now it's under kc db dot

00:46:42,400 --> 00:46:46,720
io just use this website is there

00:46:47,200 --> 00:46:50,880
uh does kafka streams run at the same

00:46:49,280 --> 00:46:54,240
jvm as a kafka broker

00:46:50,880 --> 00:46:54,880
no yeah so you answer your own question

00:46:54,240 --> 00:46:57,359
so

00:46:54,880 --> 00:46:58,160
nicholas kafka streams just like his

00:46:57,359 --> 00:47:00,720
sequel db

00:46:58,160 --> 00:47:02,480
they run on their dedicated set of jvms

00:47:00,720 --> 00:47:05,280
that is outside the kafka broker

00:47:02,480 --> 00:47:07,200
right uh this is important because kafka

00:47:05,280 --> 00:47:09,359
is extremely i o bound

00:47:07,200 --> 00:47:11,119
and kafka streams and kc db are

00:47:09,359 --> 00:47:14,079
extremely cpu

00:47:11,119 --> 00:47:14,960
bound so it is a good thing to separate

00:47:14,079 --> 00:47:16,720
the workloads

00:47:14,960 --> 00:47:19,040
of course technically you can run both

00:47:16,720 --> 00:47:20,960
at the same machine but

00:47:19,040 --> 00:47:22,480
because of this hardware characteristics

00:47:20,960 --> 00:47:23,040
it's better to separate the workloads

00:47:22,480 --> 00:47:24,960
for

00:47:23,040 --> 00:47:27,680
better performance and resource

00:47:24,960 --> 00:47:27,680
utilization

00:47:27,920 --> 00:47:32,000
with streaming solutions better for

00:47:29,520 --> 00:47:35,520
writing data from kafka

00:47:32,000 --> 00:47:38,160
to ignite the b um

00:47:35,520 --> 00:47:38,559
from kafka to ignite to b ignite to be

00:47:38,160 --> 00:47:42,079
uh

00:47:38,559 --> 00:47:45,040
it is apache ignite if uh in those

00:47:42,079 --> 00:47:46,079
that's that's the one you were asking if

00:47:45,040 --> 00:47:49,280
it is

00:47:46,079 --> 00:47:51,839
i know apache unite for being this uh

00:47:49,280 --> 00:47:53,040
in memory data grid in memory database

00:47:51,839 --> 00:47:55,200
technology which is

00:47:53,040 --> 00:47:56,960
looks like a lock with a nosql database

00:47:55,200 --> 00:47:58,880
if that's the case

00:47:56,960 --> 00:48:01,119
i would recommend taking a look on kafka

00:47:58,880 --> 00:48:02,079
connect which is remember the scalable

00:48:01,119 --> 00:48:04,319
integration part

00:48:02,079 --> 00:48:06,960
that i mentioned before so i know that's

00:48:04,319 --> 00:48:09,359
apache ignite has a connector for it

00:48:06,960 --> 00:48:11,040
that you can simply deploy and then it

00:48:09,359 --> 00:48:13,200
would continuously reading data from

00:48:11,040 --> 00:48:13,839
kafka and then sending to apache ignite

00:48:13,200 --> 00:48:16,880
so

00:48:13,839 --> 00:48:18,079
if that's what you're talking about i

00:48:16,880 --> 00:48:20,400
don't know any other

00:48:18,079 --> 00:48:23,680
ignite tb out there other than the

00:48:20,400 --> 00:48:23,680
apache ignite so

00:48:27,280 --> 00:48:32,079
you're welcome andy no worries my

00:48:32,839 --> 00:48:35,839
pleasure

00:48:40,800 --> 00:48:46,839
uh so haru has a

00:48:44,079 --> 00:48:48,079
when you say that the broker caches

00:48:46,839 --> 00:48:50,000
partitions

00:48:48,079 --> 00:48:51,359
are you talking about the file system

00:48:50,000 --> 00:48:53,040
cache yes how

00:48:51,359 --> 00:48:55,119
i was talking about the file system

00:48:53,040 --> 00:48:57,440
cache so the page cache

00:48:55,119 --> 00:48:59,440
every file that the operating system

00:48:57,440 --> 00:49:01,040
flushes and to

00:48:59,440 --> 00:49:02,960
every file that the operating system

00:49:01,040 --> 00:49:05,839
handles it flushes

00:49:02,960 --> 00:49:07,520
into disk and makes a copy of that file

00:49:05,839 --> 00:49:08,559
on the page cache which is has a

00:49:07,520 --> 00:49:12,160
watermark

00:49:08,559 --> 00:49:16,480
of 85 of your virtual available memory

00:49:12,160 --> 00:49:20,000
right so yeah

00:49:16,480 --> 00:49:22,720
won't so this is the fire system cache

00:49:20,000 --> 00:49:26,079
so yeah if yes won't large memory map

00:49:22,720 --> 00:49:30,160
and file calls heavy memory usage

00:49:26,079 --> 00:49:33,200
uh yes yes it it it will cause

00:49:30,160 --> 00:49:34,640
file heavy usage that's for sure and

00:49:33,200 --> 00:49:36,480
that's one of the reason why

00:49:34,640 --> 00:49:37,680
when you are considering doing a sizing

00:49:36,480 --> 00:49:40,079
for kafka

00:49:37,680 --> 00:49:41,119
you have to think really really well in

00:49:40,079 --> 00:49:44,240
how much data

00:49:41,119 --> 00:49:44,800
each node individually is going to

00:49:44,240 --> 00:49:46,800
sustain

00:49:44,800 --> 00:49:48,240
right so you don't overpass that

00:49:46,800 --> 00:49:51,119
watermark of

00:49:48,240 --> 00:49:52,880
more than 85 percent of your available

00:49:51,119 --> 00:49:54,400
rand memory right so

00:49:52,880 --> 00:49:57,040
obviously if you're thinking about a

00:49:54,400 --> 00:49:58,720
cluster for example

00:49:57,040 --> 00:50:00,800
if you're thinking about storing in a

00:49:58,720 --> 00:50:03,359
given point in time one terabyte of data

00:50:00,800 --> 00:50:05,359
and you have four nodes you might be

00:50:03,359 --> 00:50:06,400
kind of a reasoning to think that each

00:50:05,359 --> 00:50:10,319
one of those nodes

00:50:06,400 --> 00:50:13,520
might need to be able to handle 250

00:50:10,319 --> 00:50:16,000
gigabytes of uh data right per node

00:50:13,520 --> 00:50:16,800
because remember kafka has elastic

00:50:16,000 --> 00:50:19,440
storage

00:50:16,800 --> 00:50:21,280
and then if they need to handle 250

00:50:19,440 --> 00:50:23,839
gigabytes of per node

00:50:21,280 --> 00:50:25,680
you have to have at least the double of

00:50:23,839 --> 00:50:28,079
that memory per node

00:50:25,680 --> 00:50:29,920
so this is more or less the reasoning

00:50:28,079 --> 00:50:31,839
yet you should have but you were right

00:50:29,920 --> 00:50:33,680
if you passed through the watermark you

00:50:31,839 --> 00:50:34,880
might have some very memory pressure

00:50:33,680 --> 00:50:36,960
problems that

00:50:34,880 --> 00:50:41,839
my like you said might trigger some

00:50:36,960 --> 00:50:41,839
alerts on the on the system

00:50:50,480 --> 00:50:57,200
you're welcome

00:50:54,400 --> 00:50:58,160
right so um i think we are running out

00:50:57,200 --> 00:51:01,359
of time

00:50:58,160 --> 00:51:04,880
um if there are any no more questions

00:51:01,359 --> 00:51:08,400
um i think we can wrap it up right now

00:51:04,880 --> 00:51:09,440
i would like to uh thank everybody that

00:51:08,400 --> 00:51:12,319
was

00:51:09,440 --> 00:51:14,400
with me all this time uh even though the

00:51:12,319 --> 00:51:16,240
one that came from the the previous talk

00:51:14,400 --> 00:51:17,680
as well before uh like i mentioned

00:51:16,240 --> 00:51:19,680
before i'm available

00:51:17,680 --> 00:51:20,880
uh if you have any further questions uh

00:51:19,680 --> 00:51:23,920
you can reach me out

00:51:20,880 --> 00:51:25,520
on my email or my twitter handle and i

00:51:23,920 --> 00:51:26,880
will be more than glad to answer any

00:51:25,520 --> 00:51:30,079
questions that you might have

00:51:26,880 --> 00:51:37,839
so thank you and enjoy and wrestle with

00:51:30,079 --> 00:51:37,839
the conference

00:52:05,280 --> 00:52:07,359

YouTube URL: https://www.youtube.com/watch?v=7Me4nD4beh4


