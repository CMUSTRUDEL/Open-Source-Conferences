Title: Fresh updates about the new Beam Spark Structured Streaming runner
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Fresh updates about the new Beam Spark Structured Streaming runner
Etienne Chauchot

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Beam provides a unified programming model to execute batch and streaming pipelines on all the popular big data engines. The translation layer from Beam to the chosen big data engine is called a runner. A little more than one year ago, a new Spark runner based on Spark Structured Streaming framework was started and it has been merged to Beam master since. This talk will give updates about this new runner showing some added features, some performance improvements and also things that are yet to come.

Etienne has been working in software engineering and architecture for more than 16 years. He is focused on Big Data subjects. He is an Open Source fan and contributes to Apache projects such as Apache Beam, Apache Flink or Apache Spark. He is a Beam committer and PMC member.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:29,039 --> 00:00:33,120
hi everyone

00:00:30,400 --> 00:00:34,079
uh i'm attention and i'm gonna give you

00:00:33,120 --> 00:00:37,280
fresh updates

00:00:34,079 --> 00:00:40,239
about uh the new beam runner based on

00:00:37,280 --> 00:00:42,079
spark structure streaming framework uh

00:00:40,239 --> 00:00:44,320
this talk is a continuation

00:00:42,079 --> 00:00:45,680
of the talk i gave last year at the

00:00:44,320 --> 00:00:47,840
apache con

00:00:45,680 --> 00:00:48,800
to present the building of this new

00:00:47,840 --> 00:00:52,000
runner

00:00:48,800 --> 00:00:52,559
and uh it belongs to uh the big data

00:00:52,000 --> 00:00:56,840
track

00:00:52,559 --> 00:00:58,000
even though it was put in the streaming

00:00:56,840 --> 00:01:00,800
track

00:00:58,000 --> 00:01:02,719
let me introduce myself i'm a software

00:01:00,800 --> 00:01:04,879
engineer at talend

00:01:02,719 --> 00:01:06,479
uh and i'm an apache beam committer and

00:01:04,879 --> 00:01:09,119
pmc member

00:01:06,479 --> 00:01:11,680
i'm also contributing to apache flink

00:01:09,119 --> 00:01:15,280
and a bit to apache

00:01:11,680 --> 00:01:19,119
you can catch me on twitter by email

00:01:15,280 --> 00:01:19,119
or with my blog

00:01:19,439 --> 00:01:24,560
at talon we do data integration software

00:01:22,400 --> 00:01:25,520
in that case the screenshot is pipeline

00:01:24,560 --> 00:01:28,479
designer

00:01:25,520 --> 00:01:30,000
it's a tool that allows to be to build

00:01:28,479 --> 00:01:33,520
big data pipelines

00:01:30,000 --> 00:01:37,600
by assembling ui components uh and it's

00:01:33,520 --> 00:01:38,400
based on bim so this is the agenda for

00:01:37,600 --> 00:01:40,400
today

00:01:38,400 --> 00:01:42,479
uh i will start with an introduction to

00:01:40,400 --> 00:01:43,040
bim for those who don't know it uh i

00:01:42,479 --> 00:01:46,079
will then

00:01:43,040 --> 00:01:47,759
do a very brief uh introduction to spark

00:01:46,079 --> 00:01:51,119
structure streaming framework

00:01:47,759 --> 00:01:52,720
i will then uh give you uh the overall

00:01:51,119 --> 00:01:54,479
beam runner architecture so that you

00:01:52,720 --> 00:01:56,159
could know what a runner is

00:01:54,479 --> 00:01:58,320
i will then switch to latest

00:01:56,159 --> 00:02:01,119
improvements in terms of features

00:01:58,320 --> 00:02:01,520
i will then dig into a problem that we

00:02:01,119 --> 00:02:03,280
have

00:02:01,520 --> 00:02:04,960
with the sparks torture streaming

00:02:03,280 --> 00:02:08,000
framework and

00:02:04,960 --> 00:02:11,599
um i will then give updates on

00:02:08,000 --> 00:02:15,599
performances and a conclusion

00:02:11,599 --> 00:02:18,959
so as an introduction to bim i will just

00:02:15,599 --> 00:02:21,360
give this code bim

00:02:18,959 --> 00:02:22,239
is a programming model that allows to

00:02:21,360 --> 00:02:25,280
create

00:02:22,239 --> 00:02:27,120
big data pipelines and it's unified

00:02:25,280 --> 00:02:30,640
between batch and streaming

00:02:27,120 --> 00:02:32,480
with a unified api i mean so

00:02:30,640 --> 00:02:34,879
that's because we believe that there is

00:02:32,480 --> 00:02:36,560
no real difference between a batch

00:02:34,879 --> 00:02:39,040
pipeline and a streaming pipeline

00:02:36,560 --> 00:02:40,959
the only difference is that uh the

00:02:39,040 --> 00:02:43,040
streaming pipeline deals with

00:02:40,959 --> 00:02:45,040
an infinite collection of data whereas

00:02:43,040 --> 00:02:47,280
the pipe the batch pipeline

00:02:45,040 --> 00:02:48,560
deals with infinite collection of data

00:02:47,280 --> 00:02:52,160
and all you need to do

00:02:48,560 --> 00:02:53,360
is to divide this infinite data into

00:02:52,160 --> 00:02:56,959
finite chunks

00:02:53,360 --> 00:02:59,040
so the overall batch problem can be seen

00:02:56,959 --> 00:03:00,959
as a subset of the streaming problem

00:02:59,040 --> 00:03:02,239
and to divide the data all you need is

00:03:00,959 --> 00:03:06,480
windowing and

00:03:02,239 --> 00:03:08,879
being provided uh that's the beam stack

00:03:06,480 --> 00:03:10,480
uh on top of it you have the user code

00:03:08,879 --> 00:03:14,319
that that's the pipeline

00:03:10,480 --> 00:03:15,519
written by the user using the libraries

00:03:14,319 --> 00:03:17,840
of transforms

00:03:15,519 --> 00:03:19,760
and the ios that are provided by the

00:03:17,840 --> 00:03:22,319
language sdks

00:03:19,760 --> 00:03:24,640
there are several language sdks one for

00:03:22,319 --> 00:03:27,680
java one for python one for go

00:03:24,640 --> 00:03:29,920
and they all expose um the beam

00:03:27,680 --> 00:03:32,080
model concepts like windowing we will

00:03:29,920 --> 00:03:34,239
see that later on

00:03:32,080 --> 00:03:35,599
and the bottom layer is the focus of

00:03:34,239 --> 00:03:38,879
interest of this talk

00:03:35,599 --> 00:03:41,440
it's the translation layer between a

00:03:38,879 --> 00:03:42,640
pipeline written with the beam api and a

00:03:41,440 --> 00:03:45,360
native

00:03:42,640 --> 00:03:47,280
pipeline for spark for flink for data

00:03:45,360 --> 00:03:51,760
flow that would be executed by the

00:03:47,280 --> 00:03:54,720
actual engine um the beam transforms

00:03:51,760 --> 00:03:56,720
actually there are only three beam

00:03:54,720 --> 00:03:58,799
transforms three primitives

00:03:56,720 --> 00:04:00,000
uh there's the purdue which has the good

00:03:58,799 --> 00:04:03,920
old flat map

00:04:00,000 --> 00:04:06,480
that is basically um an event uh

00:04:03,920 --> 00:04:08,080
wise processing an element-wise

00:04:06,480 --> 00:04:11,519
processing sorry

00:04:08,080 --> 00:04:14,239
and the group by key which is

00:04:11,519 --> 00:04:14,959
a way to group elements into groups that

00:04:14,239 --> 00:04:17,040
could be

00:04:14,959 --> 00:04:19,199
later on processed in parallel

00:04:17,040 --> 00:04:21,680
downstream the pipeline

00:04:19,199 --> 00:04:22,400
and there is the read that is there to

00:04:21,680 --> 00:04:25,600
ingest

00:04:22,400 --> 00:04:29,280
data into the pipeline all the other

00:04:25,600 --> 00:04:31,759
um transforms available in the beam sdks

00:04:29,280 --> 00:04:34,560
are composites of bardu and group by key

00:04:31,759 --> 00:04:35,120
even the reducer equivalent which is

00:04:34,560 --> 00:04:38,560
called

00:04:35,120 --> 00:04:40,320
combining bim uh is uh implemented by

00:04:38,560 --> 00:04:43,520
bmsdk as a pardo

00:04:40,320 --> 00:04:47,280
and a group id this

00:04:43,520 --> 00:04:49,360
is the traditional usual

00:04:47,280 --> 00:04:50,479
example very straight very simple

00:04:49,360 --> 00:04:52,639
pipeline that

00:04:50,479 --> 00:04:53,680
reads text file counts the occurrences

00:04:52,639 --> 00:04:57,120
of the words

00:04:53,680 --> 00:04:59,440
and prints the output to a text file

00:04:57,120 --> 00:05:00,560
uh it is there just to show you uh the

00:04:59,440 --> 00:05:03,840
beam api

00:05:00,560 --> 00:05:05,680
so we have uh the the pipeline

00:05:03,840 --> 00:05:06,880
which is the user interaction with the

00:05:05,680 --> 00:05:09,360
pipeline

00:05:06,880 --> 00:05:10,720
we have the p collection object which is

00:05:09,360 --> 00:05:14,400
an abstraction

00:05:10,720 --> 00:05:16,880
of the data spread across the cluster

00:05:14,400 --> 00:05:18,240
we have several transforms flatmap

00:05:16,880 --> 00:05:20,960
counts map elements

00:05:18,240 --> 00:05:21,600
and text style that allows to read from

00:05:20,960 --> 00:05:24,800
text file

00:05:21,600 --> 00:05:27,919
and write to text file so

00:05:24,800 --> 00:05:30,960
um this slide is

00:05:27,919 --> 00:05:33,440
the bim vision um it's

00:05:30,960 --> 00:05:34,560
to say that you can write your pipeline

00:05:33,440 --> 00:05:37,520
using whatever

00:05:34,560 --> 00:05:39,440
language sdk in java python go and run

00:05:37,520 --> 00:05:42,639
it on

00:05:39,440 --> 00:05:44,720
any big data engine that is supported

00:05:42,639 --> 00:05:46,880
no matter what the language was used to

00:05:44,720 --> 00:05:50,080
write it for example you can write

00:05:46,880 --> 00:05:51,680
a python pipeline and run it on spark

00:05:50,080 --> 00:05:55,199
which is coded in scala

00:05:51,680 --> 00:05:57,199
so uh this is quite out of the scope of

00:05:55,199 --> 00:05:59,600
this talk because it's portability

00:05:57,199 --> 00:06:00,960
it would be dealt with tomorrow but by

00:05:59,600 --> 00:06:04,319
alex romanenko

00:06:00,960 --> 00:06:07,919
but i needed to mention it um

00:06:04,319 --> 00:06:11,759
so this is the first um model

00:06:07,919 --> 00:06:14,800
uh concept of bim which is windowing

00:06:11,759 --> 00:06:18,000
windowing is a way to temporarily

00:06:14,800 --> 00:06:18,800
group elements based on their even

00:06:18,000 --> 00:06:20,639
timestamp

00:06:18,800 --> 00:06:22,160
the timestamp at which at which the

00:06:20,639 --> 00:06:24,880
element occurred

00:06:22,160 --> 00:06:26,479
and and it's mandatory for streaming

00:06:24,880 --> 00:06:26,960
because you need to divide the infinite

00:06:26,479 --> 00:06:30,720
data

00:06:26,960 --> 00:06:32,880
as i said before but it's optional

00:06:30,720 --> 00:06:34,720
uh for batch mode because even in batch

00:06:32,880 --> 00:06:35,360
mode you can set your timestamp on your

00:06:34,720 --> 00:06:38,560
data

00:06:35,360 --> 00:06:40,240
and still apply windowing um we have

00:06:38,560 --> 00:06:42,639
several types of windows

00:06:40,240 --> 00:06:44,319
we have the fixed windows that that

00:06:42,639 --> 00:06:46,240
allows to

00:06:44,319 --> 00:06:48,960
express things like i want to have the

00:06:46,240 --> 00:06:51,039
last 30 seconds of data

00:06:48,960 --> 00:06:52,319
we have the sliding windows that allow

00:06:51,039 --> 00:06:55,199
to express things like

00:06:52,319 --> 00:06:56,319
i want to have the last 60 seconds of

00:06:55,199 --> 00:07:00,160
data

00:06:56,319 --> 00:07:03,360
every 30 seconds we also have the

00:07:00,160 --> 00:07:04,639
session windows that are very similar to

00:07:03,360 --> 00:07:06,639
web session

00:07:04,639 --> 00:07:08,639
meaning that they are defined by a gap

00:07:06,639 --> 00:07:09,199
duration so let's say we have a 30

00:07:08,639 --> 00:07:12,639
seconds

00:07:09,199 --> 00:07:15,520
gap duration set um if we receive no

00:07:12,639 --> 00:07:16,400
elements for more than 30 seconds in

00:07:15,520 --> 00:07:19,919
even time

00:07:16,400 --> 00:07:23,680
then the next element will be considered

00:07:19,919 --> 00:07:25,759
as belonging to the next session

00:07:23,680 --> 00:07:26,800
we also provide two special kind of

00:07:25,759 --> 00:07:28,720
windows

00:07:26,800 --> 00:07:30,000
the global window which is the default

00:07:28,720 --> 00:07:32,560
one the one you

00:07:30,000 --> 00:07:34,319
get when you don't specify your window

00:07:32,560 --> 00:07:37,759
that's the window that contains

00:07:34,319 --> 00:07:39,680
all the elements in batch mode

00:07:37,759 --> 00:07:41,919
uh all the elements of the pipeline of

00:07:39,680 --> 00:07:44,800
course and we also have a special

00:07:41,919 --> 00:07:45,840
custom window that is basically the way

00:07:44,800 --> 00:07:49,199
for the user

00:07:45,840 --> 00:07:49,520
to specify for a given element based on

00:07:49,199 --> 00:07:53,039
its

00:07:49,520 --> 00:07:54,800
timestamp to what a window it should

00:07:53,039 --> 00:07:57,280
belong

00:07:54,800 --> 00:07:58,800
so that's a function to implement for

00:07:57,280 --> 00:08:02,080
the user

00:07:58,800 --> 00:08:04,160
uh another very very important concept

00:08:02,080 --> 00:08:05,680
of bim is watermarks we will deal with

00:08:04,160 --> 00:08:07,680
that later on

00:08:05,680 --> 00:08:08,720
but i need to say that for the

00:08:07,680 --> 00:08:11,840
watermarks

00:08:08,720 --> 00:08:12,479
um that for all the streaming systems we

00:08:11,840 --> 00:08:15,840
always

00:08:12,479 --> 00:08:16,560
have problems like lag between the event

00:08:15,840 --> 00:08:18,639
timestamp

00:08:16,560 --> 00:08:20,960
so it's the time at which the element

00:08:18,639 --> 00:08:22,400
occurred and the processing time which

00:08:20,960 --> 00:08:24,960
is the time at which

00:08:22,400 --> 00:08:26,160
the element was processed by the system

00:08:24,960 --> 00:08:28,879
and we also have

00:08:26,160 --> 00:08:29,919
out of order data and to deal with those

00:08:28,879 --> 00:08:32,159
two points

00:08:29,919 --> 00:08:33,760
we provide what we call a watermark a

00:08:32,159 --> 00:08:36,479
watermark is

00:08:33,760 --> 00:08:37,039
a point in time where we should not

00:08:36,479 --> 00:08:40,080
receive

00:08:37,039 --> 00:08:41,599
older elements in other words uh it's

00:08:40,080 --> 00:08:44,240
the system notion

00:08:41,599 --> 00:08:46,720
that all the data for a certain window

00:08:44,240 --> 00:08:49,040
is expected to have arrived to have come

00:08:46,720 --> 00:08:50,480
in the pipeline so in fact if i take a

00:08:49,040 --> 00:08:53,920
very simple example

00:08:50,480 --> 00:08:59,040
very naive of a 10 minute

00:08:53,920 --> 00:09:02,720
lag watermark very static watermark

00:08:59,040 --> 00:09:05,839
at 210 the system will consider

00:09:02,720 --> 00:09:06,959
that no element with timestamp older

00:09:05,839 --> 00:09:09,040
than 2 o'clock

00:09:06,959 --> 00:09:10,560
can arrive so it's a lag of 10 minutes

00:09:09,040 --> 00:09:13,200
watermark

00:09:10,560 --> 00:09:14,720
the watermark is set by the source and

00:09:13,200 --> 00:09:16,959
it's propagated

00:09:14,720 --> 00:09:18,399
throughout the pipeline through the

00:09:16,959 --> 00:09:22,000
different uh

00:09:18,399 --> 00:09:26,480
transforms and a corollary

00:09:22,000 --> 00:09:29,440
to that is leg data leg data is data

00:09:26,480 --> 00:09:31,680
that arrives after the watermark so if i

00:09:29,440 --> 00:09:34,720
take my previous example of this

00:09:31,680 --> 00:09:37,120
simple 10 minutes lag watermark

00:09:34,720 --> 00:09:39,680
for a given element if it has a

00:09:37,120 --> 00:09:43,600
timestamp of 159

00:09:39,680 --> 00:09:47,040
and it arrives at 211

00:09:43,600 --> 00:09:49,120
then as at 210 the watermark

00:09:47,040 --> 00:09:50,399
will consider that no elements older

00:09:49,120 --> 00:09:53,440
than two can arrive

00:09:50,399 --> 00:09:55,680
then this element with timestamp 159

00:09:53,440 --> 00:09:56,560
will be considered late and it will be

00:09:55,680 --> 00:09:59,120
dropped

00:09:56,560 --> 00:10:00,000
unless we set what we call a load

00:09:59,120 --> 00:10:02,640
lateness

00:10:00,000 --> 00:10:03,920
and a low likeness is a tolerance in

00:10:02,640 --> 00:10:07,120
time

00:10:03,920 --> 00:10:08,000
and in that case it avoids the elements

00:10:07,120 --> 00:10:10,640
dropping

00:10:08,000 --> 00:10:13,279
so for the previous example of this

00:10:10,640 --> 00:10:15,839
element with timestamp 159

00:10:13,279 --> 00:10:18,399
to avoid that it's dropped you need to

00:10:15,839 --> 00:10:22,560
set a load lightness of at least

00:10:18,399 --> 00:10:25,920
two minutes another

00:10:22,560 --> 00:10:28,480
model concept important is triggering

00:10:25,920 --> 00:10:29,839
triggering is what tells bim when to

00:10:28,480 --> 00:10:32,399
output data

00:10:29,839 --> 00:10:33,519
the default trigger is when the

00:10:32,399 --> 00:10:36,320
watermark

00:10:33,519 --> 00:10:37,600
past the end of the window so with my

00:10:36,320 --> 00:10:41,360
previous knife static

00:10:37,600 --> 00:10:41,360
watermark example of 10 minutes lag

00:10:42,160 --> 00:10:46,480
a given fixed window of one hour from

00:10:45,120 --> 00:10:50,160
one to two

00:10:46,480 --> 00:10:52,880
will be closed at 2 10 because at 2 10

00:10:50,160 --> 00:10:54,640
the watermark value will be 2 o'clock

00:10:52,880 --> 00:10:55,600
and it will pass the end of the window

00:10:54,640 --> 00:10:58,640
so that's when

00:10:55,600 --> 00:11:00,480
the output will come and the

00:10:58,640 --> 00:11:01,680
buffers will be cleaned and all these

00:11:00,480 --> 00:11:04,399
things and

00:11:01,680 --> 00:11:06,079
uh in the data output um there are

00:11:04,399 --> 00:11:08,880
several types of triggers

00:11:06,079 --> 00:11:09,920
events based uh triggers even time-based

00:11:08,880 --> 00:11:11,760
triggers

00:11:09,920 --> 00:11:13,920
based on timestamp of elements

00:11:11,760 --> 00:11:14,640
processing time triggers that are based

00:11:13,920 --> 00:11:16,720
on

00:11:14,640 --> 00:11:18,560
the time at which the elements was seen

00:11:16,720 --> 00:11:21,040
or processed by the system

00:11:18,560 --> 00:11:22,079
and data-driven triggers that are based

00:11:21,040 --> 00:11:25,279
on the count

00:11:22,079 --> 00:11:28,399
of elements but we also can

00:11:25,279 --> 00:11:30,640
have early triggers that emit data

00:11:28,399 --> 00:11:33,920
before the window closes

00:11:30,640 --> 00:11:35,279
it's useful for preview and also late

00:11:33,920 --> 00:11:38,640
data triggers that

00:11:35,279 --> 00:11:40,480
emit their data

00:11:38,640 --> 00:11:43,040
after the watermark has passed the end

00:11:40,480 --> 00:11:45,760
of the window

00:11:43,040 --> 00:11:47,519
i will now do a very quick introduction

00:11:45,760 --> 00:11:48,640
uh to the spot structure streaming

00:11:47,519 --> 00:11:51,440
framework

00:11:48,640 --> 00:11:53,360
uh it's a framework that was released in

00:11:51,440 --> 00:11:56,560
beta in with spark tool

00:11:53,360 --> 00:11:59,279
in july 16. it's based on spark sql

00:11:56,560 --> 00:12:00,160
and it provides an api unified for batch

00:11:59,279 --> 00:12:03,440
and streaming

00:12:00,160 --> 00:12:06,399
that is called data set of data frame

00:12:03,440 --> 00:12:07,200
it is it has a type and a schema

00:12:06,399 --> 00:12:09,760
enforcement

00:12:07,200 --> 00:12:10,560
you actually define you the structure of

00:12:09,760 --> 00:12:13,920
your data

00:12:10,560 --> 00:12:16,880
and it gets enforced uh by the uh

00:12:13,920 --> 00:12:17,680
the the framework but it's still micro

00:12:16,880 --> 00:12:20,720
batched

00:12:17,680 --> 00:12:23,360
and even even though they

00:12:20,720 --> 00:12:24,000
defined uh the continuous processing

00:12:23,360 --> 00:12:26,560
mode that

00:12:24,000 --> 00:12:29,519
is not micro batched and that that is

00:12:26,560 --> 00:12:31,920
available for low latency pipelines

00:12:29,519 --> 00:12:32,880
uh this mode does not support

00:12:31,920 --> 00:12:35,040
aggregation

00:12:32,880 --> 00:12:36,480
in streaming at all you cannot do a

00:12:35,040 --> 00:12:38,639
group buy

00:12:36,480 --> 00:12:40,240
with the continuous processing mode so

00:12:38,639 --> 00:12:41,680
it's not useful for bim

00:12:40,240 --> 00:12:43,680
because obviously we need to do

00:12:41,680 --> 00:12:46,399
aggregations um

00:12:43,680 --> 00:12:48,079
as uh it has also an optimizer that is

00:12:46,399 --> 00:12:50,959
called catalyst that is

00:12:48,079 --> 00:12:52,000
very similar to a sql optimizer its job

00:12:50,959 --> 00:12:55,120
is to optimize

00:12:52,000 --> 00:12:57,200
the execution plan of the spark queries

00:12:55,120 --> 00:12:58,160
so that will be all for this back

00:12:57,200 --> 00:13:01,279
interaction

00:12:58,160 --> 00:13:03,200
i will uh dig into the subject of the

00:13:01,279 --> 00:13:05,440
beam runner itself

00:13:03,200 --> 00:13:07,120
so you remember the pipeline i gave in

00:13:05,440 --> 00:13:09,040
the introduction the very simple one

00:13:07,120 --> 00:13:14,240
that reads text file

00:13:09,040 --> 00:13:17,279
actually this uh example this pipeline

00:13:14,240 --> 00:13:20,560
will result when the

00:13:17,279 --> 00:13:23,920
sdk will pass it it will

00:13:20,560 --> 00:13:26,240
uh the sdk will construct a graph

00:13:23,920 --> 00:13:27,440
such as this one on the right hand side

00:13:26,240 --> 00:13:31,120
of the screen

00:13:27,440 --> 00:13:35,519
uh it's the direct aci click graph

00:13:31,120 --> 00:13:38,800
so uh so the short is dag

00:13:35,519 --> 00:13:39,600
it contains only beam transforms so read

00:13:38,800 --> 00:13:42,720
flat map

00:13:39,600 --> 00:13:45,680
count map elements and write

00:13:42,720 --> 00:13:47,519
and this is what is used as an input for

00:13:45,680 --> 00:13:49,920
the runner translation

00:13:47,519 --> 00:13:51,519
the job of the runner is to translate

00:13:49,920 --> 00:13:54,639
this egg

00:13:51,519 --> 00:13:56,959
to the native

00:13:54,639 --> 00:13:57,680
pipeline code that you see on the right

00:13:56,959 --> 00:13:59,760
hand side

00:13:57,680 --> 00:14:01,279
of course it's pseudocode but you see

00:13:59,760 --> 00:14:04,079
that it's native spar

00:14:01,279 --> 00:14:06,240
because there is back session read data

00:14:04,079 --> 00:14:10,079
source v2 flat map and all these things

00:14:06,240 --> 00:14:11,440
and this is this actual native pipeline

00:14:10,079 --> 00:14:15,680
that will be run

00:14:11,440 --> 00:14:19,040
uh in the spark cluster and um

00:14:15,680 --> 00:14:22,240
let's take a look at the left-hand side

00:14:19,040 --> 00:14:25,360
for a bit um you remember that i

00:14:22,240 --> 00:14:29,279
said in the introduction that

00:14:25,360 --> 00:14:32,399
there were only a few primitives in bim

00:14:29,279 --> 00:14:34,959
so read is a primitive but flat map

00:14:32,399 --> 00:14:36,880
is not flat map is implemented in the

00:14:34,959 --> 00:14:39,279
sdk as a pardo

00:14:36,880 --> 00:14:40,399
account is not a primitive either it's

00:14:39,279 --> 00:14:43,040
implemented as

00:14:40,399 --> 00:14:44,880
a beam combined which is itself not a

00:14:43,040 --> 00:14:47,120
primitive as i said before it's

00:14:44,880 --> 00:14:48,320
implemented in the sdk as a group i key

00:14:47,120 --> 00:14:50,079
and a pardo

00:14:48,320 --> 00:14:51,440
and map element is a border and right is

00:14:50,079 --> 00:14:54,639
a border

00:14:51,440 --> 00:14:57,600
what happens there is that

00:14:54,639 --> 00:14:59,519
the runner job is to translate so it

00:14:57,600 --> 00:15:02,079
will visit this tag

00:14:59,519 --> 00:15:04,480
but it can choose the level of

00:15:02,079 --> 00:15:07,760
translation it's the object of the

00:15:04,480 --> 00:15:08,880
these green boxes um for the read

00:15:07,760 --> 00:15:11,199
there's no point because it's a

00:15:08,880 --> 00:15:11,519
primitive so it gets directly translated

00:15:11,199 --> 00:15:14,800
to

00:15:11,519 --> 00:15:18,720
a spark read with data so v2 api

00:15:14,800 --> 00:15:21,600
and the pardo is translated as a spark

00:15:18,720 --> 00:15:22,959
flat map and but that where it's

00:15:21,600 --> 00:15:25,600
interesting is with

00:15:22,959 --> 00:15:28,000
the combine because with the combine we

00:15:25,600 --> 00:15:31,040
could have chosen to translate

00:15:28,000 --> 00:15:32,000
the beam a group by key to a spark ruby

00:15:31,040 --> 00:15:34,639
key and a pardo

00:15:32,000 --> 00:15:35,040
to a spark flat map but it would have

00:15:34,639 --> 00:15:38,320
been

00:15:35,040 --> 00:15:39,279
less performant than uh directly

00:15:38,320 --> 00:15:41,920
translating

00:15:39,279 --> 00:15:42,880
the beam combined to a spark aggregator

00:15:41,920 --> 00:15:45,120
because they are

00:15:42,880 --> 00:15:46,720
equivalent so that that is more

00:15:45,120 --> 00:15:49,759
performant to

00:15:46,720 --> 00:15:54,000
do the translation at that level

00:15:49,759 --> 00:15:56,639
so when the runner visits this deck

00:15:54,000 --> 00:15:57,360
it creates an input data set as a result

00:15:56,639 --> 00:16:01,120
of the read

00:15:57,360 --> 00:16:03,199
stores uh this data set and applies it

00:16:01,120 --> 00:16:04,320
the first transform stores the output

00:16:03,199 --> 00:16:06,880
data set

00:16:04,320 --> 00:16:08,320
and use it as an input for next step and

00:16:06,880 --> 00:16:12,959
so on and so on and so on

00:16:08,320 --> 00:16:15,839
and until we reach the output data

00:16:12,959 --> 00:16:16,959
that will be the final data set of the

00:16:15,839 --> 00:16:20,160
pipeline

00:16:16,959 --> 00:16:23,360
and we will apply to to it an action

00:16:20,160 --> 00:16:26,800
in that case it will be uh

00:16:23,360 --> 00:16:30,959
a forage uh action

00:16:26,800 --> 00:16:30,959
uh because it's a it's a batch pipeline

00:16:31,040 --> 00:16:36,480
uh so now how do you run this uh new

00:16:34,800 --> 00:16:38,800
a pipeline with this new runner you

00:16:36,480 --> 00:16:42,160
simply replace the dash dash runner

00:16:38,800 --> 00:16:44,959
equals spark runner in bim command line

00:16:42,160 --> 00:16:46,079
by dash dash runner equals power social

00:16:44,959 --> 00:16:50,480
streaming runner

00:16:46,079 --> 00:16:52,959
and you will get an output such as this

00:16:50,480 --> 00:16:54,240
in that case it's a very simple example

00:16:52,959 --> 00:16:57,360
of a pardo that is

00:16:54,240 --> 00:16:59,920
executed on data and

00:16:57,360 --> 00:17:00,720
this screenshot is the output of

00:16:59,920 --> 00:17:03,279
catalyst

00:17:00,720 --> 00:17:05,439
it's the physical plan that is actually

00:17:03,279 --> 00:17:07,199
executed by the spark engine so it's

00:17:05,439 --> 00:17:10,079
readable from bottom to top

00:17:07,199 --> 00:17:11,520
so you can see it starts with a source

00:17:10,079 --> 00:17:15,039
uh in batch mode

00:17:11,520 --> 00:17:18,079
it then applies the binary schema

00:17:15,039 --> 00:17:23,439
uh defined uh the source

00:17:18,079 --> 00:17:28,799
um creates row so it deserializes to row

00:17:23,439 --> 00:17:28,799
and then we with a map element

00:17:29,120 --> 00:17:32,720
this allies to window value because when

00:17:31,039 --> 00:17:33,520
the value is the object that the beam

00:17:32,720 --> 00:17:36,240
pipeline

00:17:33,520 --> 00:17:37,200
deal with in the inside it's window plus

00:17:36,240 --> 00:17:40,720
data

00:17:37,200 --> 00:17:44,080
uh and we then apply the part that is

00:17:40,720 --> 00:17:47,120
defined by the user in the bim pipeline

00:17:44,080 --> 00:17:50,559
and after that it's the the

00:17:47,120 --> 00:17:53,679
the result is resale realized uh

00:17:50,559 --> 00:17:56,240
two bytes so that uh spark could

00:17:53,679 --> 00:17:58,799
uh deal with the data for downstream the

00:17:56,240 --> 00:18:01,840
pipeline

00:17:58,799 --> 00:18:04,000
uh i will now uh give you the

00:18:01,840 --> 00:18:05,280
latest improvements uh in case you

00:18:04,000 --> 00:18:08,320
already tested this

00:18:05,280 --> 00:18:11,360
sparkler uh there were

00:18:08,320 --> 00:18:12,080
some minor fixes uh like the flatten the

00:18:11,360 --> 00:18:15,280
flatten is

00:18:12,080 --> 00:18:15,919
a merge of two collection and it now

00:18:15,280 --> 00:18:19,039
works

00:18:15,919 --> 00:18:22,559
on mtp collections we also

00:18:19,039 --> 00:18:23,840
avoided staging of the jars in spark

00:18:22,559 --> 00:18:28,000
local mode because

00:18:23,840 --> 00:18:30,160
in this mode the the spark engine runs

00:18:28,000 --> 00:18:31,840
inside the same jvm so there is already

00:18:30,160 --> 00:18:33,760
the lips in the casper so we don't need

00:18:31,840 --> 00:18:36,640
to stage the jars

00:18:33,760 --> 00:18:38,400
we also improved the pipeline options um

00:18:36,640 --> 00:18:42,559
by splitting them

00:18:38,400 --> 00:18:45,039
between the the current rdd streams

00:18:42,559 --> 00:18:46,880
uh based runner options and the

00:18:45,039 --> 00:18:49,919
structure streaming

00:18:46,880 --> 00:18:51,039
uh based uh for for the structural

00:18:49,919 --> 00:18:54,559
streaming runner

00:18:51,039 --> 00:18:56,240
options um and uh we introduced uh the

00:18:54,559 --> 00:18:58,720
test mode uh

00:18:56,240 --> 00:19:00,000
that you saw in the previous slide the

00:18:58,720 --> 00:19:02,000
log that you saw

00:19:00,000 --> 00:19:04,559
is the test mode output for the the

00:19:02,000 --> 00:19:07,840
execution plans of spark

00:19:04,559 --> 00:19:10,240
and we also fixed uh the motorization

00:19:07,840 --> 00:19:11,760
in batch mode because at the end spot

00:19:10,240 --> 00:19:14,480
needs to materialize data

00:19:11,760 --> 00:19:16,320
and for that it needs to apply an action

00:19:14,480 --> 00:19:16,799
to the output data set so in batch we

00:19:16,320 --> 00:19:20,240
chose

00:19:16,799 --> 00:19:24,000
to apply the forage um

00:19:20,240 --> 00:19:26,960
we did us also a good amount of work

00:19:24,000 --> 00:19:28,160
on the combined translation so as i said

00:19:26,960 --> 00:19:31,679
the combine in bim

00:19:28,160 --> 00:19:34,400
is the equivalent of reduce in the data

00:19:31,679 --> 00:19:35,120
let's say that we have a cluster with

00:19:34,400 --> 00:19:39,280
three

00:19:35,120 --> 00:19:41,200
executors so as i said the p collection

00:19:39,280 --> 00:19:42,720
is spread across the cluster so it's

00:19:41,200 --> 00:19:46,000
divided into three

00:19:42,720 --> 00:19:48,720
and for each executor

00:19:46,000 --> 00:19:49,440
beam creates an accumulator a local

00:19:48,720 --> 00:19:52,080
accumulator

00:19:49,440 --> 00:19:54,240
to which the data of the local part of

00:19:52,080 --> 00:19:56,240
the pay collection gets added

00:19:54,240 --> 00:19:57,919
and then the accumulators get merged

00:19:56,240 --> 00:20:00,400
between each other into a single

00:19:57,919 --> 00:20:03,840
accumulator that resides on a single

00:20:00,400 --> 00:20:06,559
uh a single uh executor and

00:20:03,840 --> 00:20:09,520
it is its accumulator that gets

00:20:06,559 --> 00:20:12,400
extracted to produce the output data

00:20:09,520 --> 00:20:13,919
so what's important in that is that you

00:20:12,400 --> 00:20:15,120
saw in the introduction that we have

00:20:13,919 --> 00:20:18,240
complex windowing

00:20:15,120 --> 00:20:21,600
for that we cannot rely on

00:20:18,240 --> 00:20:23,600
spark windowing we use our own

00:20:21,600 --> 00:20:25,520
uh windowing with the windowed value

00:20:23,600 --> 00:20:27,679
that you saw in the in the logs

00:20:25,520 --> 00:20:29,280
uh that contains windowing information

00:20:27,679 --> 00:20:31,440
so as a consequence

00:20:29,280 --> 00:20:33,440
when we merge we need when we merge the

00:20:31,440 --> 00:20:34,240
data in the accumulators we need to

00:20:33,440 --> 00:20:37,200
merge

00:20:34,240 --> 00:20:38,320
uh by the windows so we need to merge

00:20:37,200 --> 00:20:40,480
the accumulators by

00:20:38,320 --> 00:20:41,760
by that target windows and the data as

00:20:40,480 --> 00:20:44,400
well so

00:20:41,760 --> 00:20:46,400
uh it's important that to know that we

00:20:44,400 --> 00:20:49,520
had a lot of work to deal with

00:20:46,400 --> 00:20:51,919
window merging there is also

00:20:49,520 --> 00:20:53,120
now the sliding window type that is

00:20:51,919 --> 00:20:55,679
support

00:20:53,120 --> 00:20:57,200
and there were minor fixes on timestamp

00:20:55,679 --> 00:20:59,679
and instance reuse

00:20:57,200 --> 00:21:00,960
uh it's not three years because um beam

00:20:59,679 --> 00:21:04,559
tries to

00:21:00,960 --> 00:21:07,679
spare as a lesser memory as possible so

00:21:04,559 --> 00:21:09,640
it reuses instances of the accumulators

00:21:07,679 --> 00:21:11,760
and we need proper real

00:21:09,640 --> 00:21:15,840
re-initialization

00:21:11,760 --> 00:21:18,480
um we also now support bim coders

00:21:15,840 --> 00:21:19,600
what are bim coders beam coder is simply

00:21:18,480 --> 00:21:22,960
a serializer

00:21:19,600 --> 00:21:26,559
decelerator it's an in it's an interface

00:21:22,960 --> 00:21:28,559
uh so the encode for acceleration

00:21:26,559 --> 00:21:30,799
of an element to a bytes array and

00:21:28,559 --> 00:21:33,840
decodes for the deceleration

00:21:30,799 --> 00:21:37,280
of this byte array to the

00:21:33,840 --> 00:21:40,320
elements these coders

00:21:37,280 --> 00:21:43,360
are either provided by the user

00:21:40,320 --> 00:21:46,240
which just implements this interface and

00:21:43,360 --> 00:21:47,600
it can be also provided by the sdk so

00:21:46,240 --> 00:21:51,600
the sdk provides several

00:21:47,600 --> 00:21:53,919
colors beam colors for for several types

00:21:51,600 --> 00:21:56,159
and these colors are very very similar

00:21:53,919 --> 00:21:58,240
to spark encoders

00:21:56,159 --> 00:22:00,880
but before uh talking about sparkling

00:21:58,240 --> 00:22:03,679
colors i will just do a quick process of

00:22:00,880 --> 00:22:05,039
on the catalyst optimizer uh spark

00:22:03,679 --> 00:22:06,720
catalyst optimizer

00:22:05,039 --> 00:22:08,960
like i said before it's similar to a

00:22:06,720 --> 00:22:12,720
cyclops optimizer uh

00:22:08,960 --> 00:22:16,159
its job is to iteratively apply

00:22:12,720 --> 00:22:19,200
rules uh to the user dag to generate

00:22:16,159 --> 00:22:22,159
at the end the best physical plan uh

00:22:19,200 --> 00:22:22,480
possible so it iteratively applies rules

00:22:22,159 --> 00:22:25,919
to

00:22:22,480 --> 00:22:27,280
to end up with that um and catalyst sees

00:22:25,919 --> 00:22:30,000
the the input pipeline

00:22:27,280 --> 00:22:31,840
has a tree made of three nodes and there

00:22:30,000 --> 00:22:34,960
is a special kind of three nodes

00:22:31,840 --> 00:22:35,600
that is called catalyst expressions and

00:22:34,960 --> 00:22:38,640
there is

00:22:35,600 --> 00:22:41,840
a special type of catalyst expressions

00:22:38,640 --> 00:22:44,240
that are expression encoders and that's

00:22:41,840 --> 00:22:45,120
the those expression encoders are

00:22:44,240 --> 00:22:48,400
actually

00:22:45,120 --> 00:22:49,440
custom spark encoders and that's what we

00:22:48,400 --> 00:22:52,480
used

00:22:49,440 --> 00:22:55,520
to uh wire up to wrap

00:22:52,480 --> 00:22:57,039
the beam colors into a custom spark

00:22:55,520 --> 00:23:00,320
encoders

00:22:57,039 --> 00:23:03,280
as an expression encoder

00:23:00,320 --> 00:23:03,679
but what's important to notice there is

00:23:03,280 --> 00:23:06,720
that

00:23:03,679 --> 00:23:07,360
all catalyst expressions generate java

00:23:06,720 --> 00:23:11,039
codes

00:23:07,360 --> 00:23:14,400
actually they generate strings

00:23:11,039 --> 00:23:17,840
of java and these strings of java

00:23:14,400 --> 00:23:21,679
are compiled by the channel compiler

00:23:17,840 --> 00:23:24,799
in at the spark site when catalyst

00:23:21,679 --> 00:23:27,919
process system and this is this code

00:23:24,799 --> 00:23:31,360
uh these java strings that call

00:23:27,919 --> 00:23:32,799
the beam coder uh so what's the

00:23:31,360 --> 00:23:36,559
appearance state of this

00:23:32,799 --> 00:23:38,880
neurona it was merged to be master

00:23:36,559 --> 00:23:40,159
uh it covers a hundred percent of the

00:23:38,880 --> 00:23:43,279
bimcore features

00:23:40,159 --> 00:23:43,919
there is still optional features uh that

00:23:43,279 --> 00:23:46,480
are not yet

00:23:43,919 --> 00:23:47,520
implemented like the state api like the

00:23:46,480 --> 00:23:50,720
timer api

00:23:47,520 --> 00:23:53,120
like the splitable dfn api and also

00:23:50,720 --> 00:23:55,039
side inputs in combine are an ongoing

00:23:53,120 --> 00:23:58,480
work

00:23:55,039 --> 00:24:02,159
the beam schemers are not wired up

00:23:58,480 --> 00:24:06,320
with spark schemers we use for now

00:24:02,159 --> 00:24:08,720
spark binary schemas also the runner

00:24:06,320 --> 00:24:10,240
can execute next mark in batch mode so

00:24:08,720 --> 00:24:12,799
what is next mark nexmark

00:24:10,240 --> 00:24:13,440
is our internal beam performance

00:24:12,799 --> 00:24:16,320
framework

00:24:13,440 --> 00:24:17,120
that covers 100 of the model and that

00:24:16,320 --> 00:24:19,600
simulates

00:24:17,120 --> 00:24:20,880
an auction system it simulates persons

00:24:19,600 --> 00:24:24,080
creating auctions

00:24:20,880 --> 00:24:27,120
placing bids on items and then it

00:24:24,080 --> 00:24:28,240
queries the system for statistical data

00:24:27,120 --> 00:24:31,600
such as

00:24:28,240 --> 00:24:35,279
what's the average uh selling price

00:24:31,600 --> 00:24:39,520
in the united states and these queries

00:24:35,279 --> 00:24:42,960
are actually uh beam pipelines

00:24:39,520 --> 00:24:45,440
this runner is only in batch mode

00:24:42,960 --> 00:24:46,000
only available in batch mode because the

00:24:45,440 --> 00:24:49,120
streaming

00:24:46,000 --> 00:24:50,240
mode is not supported yet because we

00:24:49,120 --> 00:24:52,320
have a problem

00:24:50,240 --> 00:24:53,520
uh with the spark structure streaming

00:24:52,320 --> 00:24:56,320
framework

00:24:53,520 --> 00:24:56,880
which is at the spark side that will be

00:24:56,320 --> 00:25:00,720
uh

00:24:56,880 --> 00:25:03,760
the focus of nexbarc next part

00:25:00,720 --> 00:25:06,480
so the multiple aggregation problem in

00:25:03,760 --> 00:25:10,080
spot structure streaming framework

00:25:06,480 --> 00:25:13,279
well the problem is that

00:25:10,080 --> 00:25:14,000
this framework does not support multiple

00:25:13,279 --> 00:25:17,200
aggregation

00:25:14,000 --> 00:25:20,400
it does not support more than one

00:25:17,200 --> 00:25:23,360
aggregation in a streaming pipeline

00:25:20,400 --> 00:25:24,320
so there's an open ticket and there is

00:25:23,360 --> 00:25:28,320
also

00:25:24,320 --> 00:25:31,440
an ongoing pr but it has not received

00:25:28,320 --> 00:25:34,159
uh an update since august 19

00:25:31,440 --> 00:25:36,159
and there's also an ongoing spark design

00:25:34,159 --> 00:25:39,520
but that received no update since

00:25:36,159 --> 00:25:41,760
june 19. so the bing project

00:25:39,520 --> 00:25:43,760
is stuck with the new runner based on

00:25:41,760 --> 00:25:46,240
spark structure streaming framework

00:25:43,760 --> 00:25:47,200
waiting for the support for more than

00:25:46,240 --> 00:25:50,400
one aggregation

00:25:47,200 --> 00:25:52,640
in a streaming mode uh

00:25:50,400 --> 00:25:55,440
so what's the underlying problem that

00:25:52,640 --> 00:25:59,039
that is at the sparse side

00:25:55,440 --> 00:25:59,760
actually the problem uh is the watermark

00:25:59,039 --> 00:26:02,880
scope

00:25:59,760 --> 00:26:05,039
in spark this scope is global

00:26:02,880 --> 00:26:06,159
for the old pipeline there is no

00:26:05,039 --> 00:26:09,279
watermark pair

00:26:06,159 --> 00:26:11,840
operation and how it works is that

00:26:09,279 --> 00:26:13,600
at the end of each micro batch the

00:26:11,840 --> 00:26:16,640
watermark is updated

00:26:13,600 --> 00:26:17,200
with the newest timestamp received so if

00:26:16,640 --> 00:26:19,840
i take

00:26:17,200 --> 00:26:20,799
an example like below a streaming

00:26:19,840 --> 00:26:23,600
pipeline

00:26:20,799 --> 00:26:24,559
with two aggregations in a row so that's

00:26:23,600 --> 00:26:28,159
not supported

00:26:24,559 --> 00:26:29,360
we will see i i doesn't work so there is

00:26:28,159 --> 00:26:32,159
the first aggregation

00:26:29,360 --> 00:26:32,880
that outputs the highest value in the

00:26:32,159 --> 00:26:36,400
window

00:26:32,880 --> 00:26:39,360
of three seconds so

00:26:36,400 --> 00:26:41,120
let's say that we receive three elements

00:26:39,360 --> 00:26:43,440
with values six four and five

00:26:41,120 --> 00:26:44,400
and timestamp one two and three so they

00:26:43,440 --> 00:26:47,840
get perfect

00:26:44,400 --> 00:26:48,400
in the uh operation one the operation

00:26:47,840 --> 00:26:52,159
one

00:26:48,400 --> 00:26:54,400
updates the global watermark to value

00:26:52,159 --> 00:26:55,520
three because it's the newest timestamp

00:26:54,400 --> 00:26:58,559
scene

00:26:55,520 --> 00:26:59,279
so as a result when the operation one

00:26:58,559 --> 00:27:02,400
outputs

00:26:59,279 --> 00:27:06,000
its data no luck the highest

00:27:02,400 --> 00:27:09,039
is value six but it has timestamp one

00:27:06,000 --> 00:27:09,919
and as the global watermark is value

00:27:09,039 --> 00:27:13,679
three

00:27:09,919 --> 00:27:16,480
then this element with timestamp one

00:27:13,679 --> 00:27:17,360
will be considered by operation two as

00:27:16,480 --> 00:27:20,080
late

00:27:17,360 --> 00:27:22,000
so then it will be dropped and this will

00:27:20,080 --> 00:27:25,200
lead to incorrect results

00:27:22,000 --> 00:27:26,080
so as a consequence to avoid incorrect

00:27:25,200 --> 00:27:29,279
results

00:27:26,080 --> 00:27:30,080
then the spark projects deactivate the

00:27:29,279 --> 00:27:31,840
support of

00:27:30,080 --> 00:27:33,279
more than one aggregation in streaming

00:27:31,840 --> 00:27:36,320
pipeline with this

00:27:33,279 --> 00:27:40,000
structural streaming framework uh

00:27:36,320 --> 00:27:43,039
a possible solution that we discussed

00:27:40,000 --> 00:27:46,480
with the spark community is to replace

00:27:43,039 --> 00:27:50,000
uh this global watermark by watermark's

00:27:46,480 --> 00:27:50,880
pair transform and these watermark

00:27:50,000 --> 00:27:53,360
values

00:27:50,880 --> 00:27:55,600
will be propagated through the pipeline

00:27:53,360 --> 00:27:58,799
from the source throughout the different

00:27:55,600 --> 00:28:02,080
spark operations and each operation

00:27:58,799 --> 00:28:03,919
will update their local watermark as

00:28:02,080 --> 00:28:05,919
they receive data

00:28:03,919 --> 00:28:08,399
and we define for each of these

00:28:05,919 --> 00:28:11,279
operations an input watermark

00:28:08,399 --> 00:28:13,440
which is defined as the minimum of the

00:28:11,279 --> 00:28:15,840
output watermark of the previous

00:28:13,440 --> 00:28:16,480
operations of course this minimum is

00:28:15,840 --> 00:28:18,799
only for

00:28:16,480 --> 00:28:19,520
why pipelines if you have a straight

00:28:18,799 --> 00:28:22,080
pipeline

00:28:19,520 --> 00:28:22,720
then the the input water mark is the

00:28:22,080 --> 00:28:26,399
minimum

00:28:22,720 --> 00:28:28,880
of the output water is the is directly

00:28:26,399 --> 00:28:30,640
uh the output watermark of the previous

00:28:28,880 --> 00:28:33,200
step

00:28:30,640 --> 00:28:34,320
and the output watermark is the minimum

00:28:33,200 --> 00:28:36,799
between the local

00:28:34,320 --> 00:28:38,320
input watermark and the oldest processed

00:28:36,799 --> 00:28:42,399
so let's say

00:28:38,320 --> 00:28:44,720
now in an example how it works

00:28:42,399 --> 00:28:45,919
so we take the very same example like

00:28:44,720 --> 00:28:49,440
before

00:28:45,919 --> 00:28:51,360
only this time we define a watermark

00:28:49,440 --> 00:28:53,120
pair operation so there is one for the

00:28:51,360 --> 00:28:55,840
source there is one

00:28:53,120 --> 00:28:57,120
for operation one and one for operation

00:28:55,840 --> 00:28:59,760
two

00:28:57,120 --> 00:29:01,600
and also let's say that the source at

00:28:59,760 --> 00:29:04,799
some point says that the watermark

00:29:01,600 --> 00:29:08,399
is 1 meaning that we should not receive

00:29:04,799 --> 00:29:10,880
older elements than time stomp 1.

00:29:08,399 --> 00:29:12,960
this watermark gets propagated through

00:29:10,880 --> 00:29:14,080
the operation like the operations like i

00:29:12,960 --> 00:29:16,640
said so

00:29:14,080 --> 00:29:18,480
the input watermark of operation one is

00:29:16,640 --> 00:29:21,440
the output watermark of previous step

00:29:18,480 --> 00:29:22,399
so it's one the output watermark of

00:29:21,440 --> 00:29:24,640
operation one

00:29:22,399 --> 00:29:25,440
is the minimum between the oldest

00:29:24,640 --> 00:29:28,880
process

00:29:25,440 --> 00:29:32,320
in the input watermark so it's one and

00:29:28,880 --> 00:29:34,240
for operation two the input watermark is

00:29:32,320 --> 00:29:35,600
the output watermark of previous steps

00:29:34,240 --> 00:29:38,480
so it's one

00:29:35,600 --> 00:29:40,399
and the output watermark of operation

00:29:38,480 --> 00:29:41,279
two is the minimum between the oldest

00:29:40,399 --> 00:29:44,399
processed

00:29:41,279 --> 00:29:45,279
and the local input watermark so it's

00:29:44,399 --> 00:29:47,600
one

00:29:45,279 --> 00:29:48,640
so let's say now that we receive the

00:29:47,600 --> 00:29:50,399
very same

00:29:48,640 --> 00:29:52,799
three elements with values six four and

00:29:50,399 --> 00:29:55,600
five and times two point two and three

00:29:52,799 --> 00:29:57,279
so they get buffered inside operation

00:29:55,600 --> 00:30:00,480
one

00:29:57,279 --> 00:30:03,520
so the operation one updates its

00:30:00,480 --> 00:30:06,159
current oldest process counter

00:30:03,520 --> 00:30:07,440
to the oldest time stop because it's the

00:30:06,159 --> 00:30:11,360
oldest process counter

00:30:07,440 --> 00:30:15,120
so it's time stop one uh so

00:30:11,360 --> 00:30:16,880
it does not update its output watermark

00:30:15,120 --> 00:30:18,559
because the output watermark is the

00:30:16,880 --> 00:30:20,799
minimum between the

00:30:18,559 --> 00:30:22,000
input watermark and the oldest process

00:30:20,799 --> 00:30:24,080
so it's still one

00:30:22,000 --> 00:30:26,000
so if the output watermark of operation

00:30:24,080 --> 00:30:28,880
one is not updated then

00:30:26,000 --> 00:30:29,840
the input and output watermark of

00:30:28,880 --> 00:30:33,120
operation two

00:30:29,840 --> 00:30:37,039
will not be updated either so

00:30:33,120 --> 00:30:39,760
now let's say that the source says now

00:30:37,039 --> 00:30:40,799
that the watermark is three meaning that

00:30:39,760 --> 00:30:44,159
we should not receive

00:30:40,799 --> 00:30:46,000
all the elements that timestamp three

00:30:44,159 --> 00:30:48,320
what happens is that this watermark is

00:30:46,000 --> 00:30:51,200
propagated to the input watermark of

00:30:48,320 --> 00:30:52,480
operation one and now something

00:30:51,200 --> 00:30:54,720
interesting happens

00:30:52,480 --> 00:30:56,240
because the input watermark of operation

00:30:54,720 --> 00:30:59,120
one is value three

00:30:56,240 --> 00:31:00,640
so the watermark has past the end of the

00:30:59,120 --> 00:31:04,000
window so if you remember

00:31:00,640 --> 00:31:05,600
what i said on the triggers then um the

00:31:04,000 --> 00:31:06,960
watermark has passed the end of the

00:31:05,600 --> 00:31:08,960
window of three seconds

00:31:06,960 --> 00:31:11,200
so it should output its data so

00:31:08,960 --> 00:31:14,320
operation one should output its data

00:31:11,200 --> 00:31:16,880
so it will do so so cleans this buffer

00:31:14,320 --> 00:31:17,600
output is data so it will output as

00:31:16,880 --> 00:31:21,519
before

00:31:17,600 --> 00:31:24,880
the value six times stomp one but now

00:31:21,519 --> 00:31:28,000
as the watermark is local

00:31:24,880 --> 00:31:30,480
the up the input watermark of operation

00:31:28,000 --> 00:31:33,279
two has not changed it's still one

00:31:30,480 --> 00:31:33,840
this element with timestamp one is no

00:31:33,279 --> 00:31:37,600
more

00:31:33,840 --> 00:31:38,080
uh considered uh late and it will not be

00:31:37,600 --> 00:31:40,640
dropped

00:31:38,080 --> 00:31:41,440
it will just be buffered and processed

00:31:40,640 --> 00:31:43,440
so

00:31:41,440 --> 00:31:46,240
uh operation two will just update its

00:31:43,440 --> 00:31:49,919
current processed counter to one

00:31:46,240 --> 00:31:52,720
and now you may wonder when do the

00:31:49,919 --> 00:31:54,159
these watermarks get updated well let's

00:31:52,720 --> 00:31:56,720
say we receive another

00:31:54,159 --> 00:31:57,440
element with timestamp four with value

00:31:56,720 --> 00:32:00,720
seven

00:31:57,440 --> 00:32:03,200
it gets buffered in operation one so it

00:32:00,720 --> 00:32:05,279
this time the operation one updates is

00:32:03,200 --> 00:32:08,640
all this process counter to four

00:32:05,279 --> 00:32:10,559
so now four is higher than three so

00:32:08,640 --> 00:32:12,159
the output water mark with which is the

00:32:10,559 --> 00:32:15,840
minimum between the two

00:32:12,159 --> 00:32:17,760
uh gets updated to three so as the

00:32:15,840 --> 00:32:18,640
output watermark of operation one is

00:32:17,760 --> 00:32:21,919
updated

00:32:18,640 --> 00:32:23,279
then the input watermark of operation

00:32:21,919 --> 00:32:26,320
two will be updated to

00:32:23,279 --> 00:32:27,200
its value so it will be now three and

00:32:26,320 --> 00:32:29,039
the local

00:32:27,200 --> 00:32:31,600
output watermark of operation two will

00:32:29,039 --> 00:32:34,880
be also updated to three so now you know

00:32:31,600 --> 00:32:36,799
how could the watermark be updated with

00:32:34,880 --> 00:32:41,039
the arrival of value and for

00:32:36,799 --> 00:32:45,279
the operations i will now give updates

00:32:41,039 --> 00:32:49,760
about the the performances

00:32:45,279 --> 00:32:49,760
of the new runner

00:32:51,120 --> 00:32:59,039
well what do we do for performances

00:32:55,440 --> 00:33:03,519
actually we

00:32:59,039 --> 00:33:06,880
schedule load tests per transform

00:33:03,519 --> 00:33:10,240
on a schedule basis we also

00:33:06,880 --> 00:33:13,679
run nexmark uh on

00:33:10,240 --> 00:33:16,559
each commit on master to follow

00:33:13,679 --> 00:33:17,440
the performance of the runner and we

00:33:16,559 --> 00:33:19,840
also

00:33:17,440 --> 00:33:20,880
did some profiling on the next mark

00:33:19,840 --> 00:33:24,640
queries

00:33:20,880 --> 00:33:27,840
uh to spot places to improve

00:33:24,640 --> 00:33:31,360
then as a development process

00:33:27,840 --> 00:33:34,640
we also for each new change um

00:33:31,360 --> 00:33:38,320
run load tests to ensure that

00:33:34,640 --> 00:33:42,880
there is no performance regression

00:33:38,320 --> 00:33:45,440
so um it led to some improvements

00:33:42,880 --> 00:33:46,399
but also some failures uh so the

00:33:45,440 --> 00:33:50,240
improvements

00:33:46,399 --> 00:33:53,600
uh brought by the beam cutters um

00:33:50,240 --> 00:33:56,640
when well previously uh before

00:33:53,600 --> 00:34:00,159
being compatible with bim coders we used

00:33:56,640 --> 00:34:02,640
cryo encoders that uh are generic

00:34:00,159 --> 00:34:05,600
encoders that generic spark encoders

00:34:02,640 --> 00:34:07,600
uh to summarize data and we replaced it

00:34:05,600 --> 00:34:11,040
by the things i presented

00:34:07,600 --> 00:34:14,480
and it gained 40 in execution

00:34:11,040 --> 00:34:18,079
time uh for that we minimized uh

00:34:14,480 --> 00:34:19,280
generated codes so we used as much jdk

00:34:18,079 --> 00:34:21,839
compiled codes

00:34:19,280 --> 00:34:22,399
and as little january compile code you

00:34:21,839 --> 00:34:25,440
remember

00:34:22,399 --> 00:34:28,720
the java strings are mentioned

00:34:25,440 --> 00:34:32,320
for the group by key we

00:34:28,720 --> 00:34:34,960
managed to remove an internal beam

00:34:32,320 --> 00:34:36,399
flat map in the translation from beam to

00:34:34,960 --> 00:34:39,839
spark and it gained

00:34:36,399 --> 00:34:42,480
20 percent um we also

00:34:39,839 --> 00:34:43,440
are on our refactoring right now the

00:34:42,480 --> 00:34:46,480
combined

00:34:43,440 --> 00:34:48,480
beam translation to support side inputs

00:34:46,480 --> 00:34:49,679
so what are the what are side inputs

00:34:48,480 --> 00:34:52,960
being side inputs

00:34:49,679 --> 00:34:55,679
are like a view of on

00:34:52,960 --> 00:34:56,560
on a p collection and this view can be

00:34:55,679 --> 00:34:59,680
injected

00:34:56,560 --> 00:35:02,400
to a transform combine in that case

00:34:59,680 --> 00:35:04,800
so it could be useful for something like

00:35:02,400 --> 00:35:07,760
a reference table that you want to cross

00:35:04,800 --> 00:35:09,520
with data coming to a combine for

00:35:07,760 --> 00:35:13,520
example

00:35:09,520 --> 00:35:16,880
and these stem views actually

00:35:13,520 --> 00:35:17,920
right now use broadcast variables and to

00:35:16,880 --> 00:35:19,520
use spark

00:35:17,920 --> 00:35:20,960
bro cause variables you need to

00:35:19,520 --> 00:35:23,119
serialize data

00:35:20,960 --> 00:35:24,880
so you need to materialize your data to

00:35:23,119 --> 00:35:27,839
set the broadcast variable

00:35:24,880 --> 00:35:29,040
and this could have been avoided we

00:35:27,839 --> 00:35:31,920
could have been avoided

00:35:29,040 --> 00:35:32,800
materializing the data by using spartan

00:35:31,920 --> 00:35:38,400
views

00:35:32,800 --> 00:35:38,400
and spark 10 views are like sql views

00:35:38,640 --> 00:35:42,839
sorry they can be cached and they can be

00:35:41,200 --> 00:35:46,480
also lazily

00:35:42,839 --> 00:35:50,400
evaluated and it could have been

00:35:46,480 --> 00:35:55,280
then a better performance candidate

00:35:50,400 --> 00:35:55,280
for the beam side input translation

00:35:55,599 --> 00:36:03,920
and it has failed because spark sequel

00:35:59,839 --> 00:36:07,280
context cannot be passed to a map

00:36:03,920 --> 00:36:10,960
because it it is not serializable so

00:36:07,280 --> 00:36:14,960
we cannot use temp views uh to translate

00:36:10,960 --> 00:36:18,079
the beam side inputs so we start to

00:36:14,960 --> 00:36:18,079
broadcast variables

00:36:18,880 --> 00:36:22,079
updated performance

00:36:22,480 --> 00:36:30,720
this performance shot is the um

00:36:27,599 --> 00:36:31,520
execution time of um the next mark

00:36:30,720 --> 00:36:34,079
queries

00:36:31,520 --> 00:36:34,720
so there are 14 of them so you remember

00:36:34,079 --> 00:36:37,599
next mark

00:36:34,720 --> 00:36:39,599
is uh the simulation of the auction

00:36:37,599 --> 00:36:41,680
system and the queries are

00:36:39,599 --> 00:36:42,720
let's say statistical questions on the

00:36:41,680 --> 00:36:46,880
system

00:36:42,720 --> 00:36:50,480
um it's a run that was done on a hundred

00:36:46,880 --> 00:36:54,240
thousand events uh uh

00:36:50,480 --> 00:36:57,440
with uh let's say uh um

00:36:54,240 --> 00:37:00,560
a local spark of uh false threads

00:36:57,440 --> 00:37:03,920
and um we see in blue uh

00:37:00,560 --> 00:37:07,200
the uh execution time of each query

00:37:03,920 --> 00:37:08,720
uh for in blue for the new spark runner

00:37:07,200 --> 00:37:09,359
based on spark software streaming

00:37:08,720 --> 00:37:13,200
framework

00:37:09,359 --> 00:37:14,800
and in red the the response time for the

00:37:13,200 --> 00:37:19,040
current spark runner

00:37:14,800 --> 00:37:23,200
based on rdd and d string

00:37:19,040 --> 00:37:25,200
and you can see that in some places

00:37:23,200 --> 00:37:26,640
the the new runner behaves better than

00:37:25,200 --> 00:37:30,079
the current one

00:37:26,640 --> 00:37:32,800
and because the the bars are lower uh

00:37:30,079 --> 00:37:34,079
in other places they behave similarly

00:37:32,800 --> 00:37:37,200
like in query number

00:37:34,079 --> 00:37:39,040
zero or one and in some cases the new

00:37:37,200 --> 00:37:40,320
runner is worse in performance than the

00:37:39,040 --> 00:37:45,119
current trainer

00:37:40,320 --> 00:37:47,520
that could be explained by the fact that

00:37:45,119 --> 00:37:48,240
there was a lot more performance

00:37:47,520 --> 00:37:51,280
improvement

00:37:48,240 --> 00:37:53,119
on the current rdd stream runner because

00:37:51,280 --> 00:37:54,079
it is still considered by the beam

00:37:53,119 --> 00:37:57,440
community

00:37:54,079 --> 00:37:58,640
as the main spark runner uh also that's

00:37:57,440 --> 00:38:01,839
that can be explained

00:37:58,640 --> 00:38:02,640
by the use of binary schemas in this new

00:38:01,839 --> 00:38:06,240
runner

00:38:02,640 --> 00:38:08,400
as a consequence catalyst cannot

00:38:06,240 --> 00:38:10,400
do some optimizations the the

00:38:08,400 --> 00:38:13,760
optimizations that are based on fields

00:38:10,400 --> 00:38:17,359
as it sees only by sorry uh

00:38:13,760 --> 00:38:20,720
it cannot do a field based

00:38:17,359 --> 00:38:21,440
optimizations uh and also another thing

00:38:20,720 --> 00:38:23,839
is that

00:38:21,440 --> 00:38:25,440
as we use binary schemas we serialize

00:38:23,839 --> 00:38:29,200
the old contents of data

00:38:25,440 --> 00:38:32,640
and in some cases we we could have

00:38:29,200 --> 00:38:33,920
serialized only uh one field uh

00:38:32,640 --> 00:38:36,240
so it could have been counter

00:38:33,920 --> 00:38:40,560
performance uh in that case

00:38:36,240 --> 00:38:44,160
um so i will as jump to the conclusion

00:38:40,560 --> 00:38:48,000
uh as a conclusion i will just say that

00:38:44,160 --> 00:38:51,359
contributions are welcome um

00:38:48,000 --> 00:38:52,400
also feel free to join uh the spark

00:38:51,359 --> 00:38:56,079
runner

00:38:52,400 --> 00:38:59,119
beam channel uh beam channel and also

00:38:56,079 --> 00:39:02,560
take a look at the apache beam

00:38:59,119 --> 00:39:06,400
website you can join the bml list

00:39:02,560 --> 00:39:09,520
you can follow us on the beam

00:39:06,400 --> 00:39:11,359
handle and also if you need some more

00:39:09,520 --> 00:39:13,599
details

00:39:11,359 --> 00:39:14,720
on some of the topics of the talk you

00:39:13,599 --> 00:39:16,720
can take a look

00:39:14,720 --> 00:39:18,240
at my previous thoughts the the first

00:39:16,720 --> 00:39:19,359
one on this particular swimming

00:39:18,240 --> 00:39:22,800
framework

00:39:19,359 --> 00:39:25,440
runner that explains the building uh

00:39:22,800 --> 00:39:26,480
you can also take a look at this nexmark

00:39:25,440 --> 00:39:28,880
talk

00:39:26,480 --> 00:39:30,480
and uh if you need some more details on

00:39:28,880 --> 00:39:31,119
the spark and queries you can take a

00:39:30,480 --> 00:39:34,320
look

00:39:31,119 --> 00:39:37,920
at uh the blog i wrote on

00:39:34,320 --> 00:39:40,960
this topic um i will also

00:39:37,920 --> 00:39:44,079
say that my friend alexa romanenko

00:39:40,960 --> 00:39:48,000
will present a talk tomorrow

00:39:44,079 --> 00:39:51,280
about portability um

00:39:48,000 --> 00:39:54,400
so that's cross language uh pipelines

00:39:51,280 --> 00:39:58,720
uh so i i urge you to uh take a

00:39:54,400 --> 00:39:58,720
a look at his session

00:39:59,200 --> 00:40:03,280
if you have some questions i am a bit

00:40:02,400 --> 00:40:04,910
late

00:40:03,280 --> 00:40:06,880
i maybe can

00:40:04,910 --> 00:40:09,280
[Music]

00:40:06,880 --> 00:40:11,040
take one question is it possible to

00:40:09,280 --> 00:40:11,680
estimate do you have a rate of beam

00:40:11,040 --> 00:40:14,400
using

00:40:11,680 --> 00:40:15,440
structural spars native uh structural

00:40:14,400 --> 00:40:18,480
stock performance

00:40:15,440 --> 00:40:20,800
well very good question um actually

00:40:18,480 --> 00:40:21,839
as far as i know this overhead was never

00:40:20,800 --> 00:40:25,839
measured

00:40:21,839 --> 00:40:29,760
um a good way to do so will be

00:40:25,839 --> 00:40:32,319
uh to pick up uh some of the

00:40:29,760 --> 00:40:34,000
the the more important next mark queries

00:40:32,319 --> 00:40:37,119
and recode them

00:40:34,000 --> 00:40:37,760
using spark native and compare the the

00:40:37,119 --> 00:40:41,119
response

00:40:37,760 --> 00:40:42,560
time of the two with the the runner with

00:40:41,119 --> 00:40:45,280
the native pipeline

00:40:42,560 --> 00:40:47,520
um but what i can say i don't have

00:40:45,280 --> 00:40:50,960
figures to show but what i can say

00:40:47,520 --> 00:40:54,160
is that when beam

00:40:50,960 --> 00:40:56,720
executes pipeline.run uh it

00:40:54,160 --> 00:40:57,359
uh does first the translation that i

00:40:56,720 --> 00:40:59,839
presented

00:40:57,359 --> 00:41:01,760
in the architecture and then when the

00:40:59,839 --> 00:41:04,400
native pipeline that i showed

00:41:01,760 --> 00:41:05,359
in the native park is produced then it's

00:41:04,400 --> 00:41:08,400
that

00:41:05,359 --> 00:41:10,560
that is run in the spark uh

00:41:08,400 --> 00:41:12,000
engine itself so there is no more beam

00:41:10,560 --> 00:41:14,880
at that point

00:41:12,000 --> 00:41:15,920
of course there are still uh the uh uh

00:41:14,880 --> 00:41:18,880
the wrappers

00:41:15,920 --> 00:41:20,079
uh to call a beam code but no more

00:41:18,880 --> 00:41:23,440
translation

00:41:20,079 --> 00:41:27,359
and uh you must know that

00:41:23,440 --> 00:41:29,440
we try to use as minimum wrappers

00:41:27,359 --> 00:41:30,560
to reduce memory as many more wrappers

00:41:29,440 --> 00:41:34,960
as possible

00:41:30,560 --> 00:41:38,000
we also use as minimum internal steps

00:41:34,960 --> 00:41:40,319
like uh in the beam translation forcing

00:41:38,000 --> 00:41:41,119
your civilization we tried not to do so

00:41:40,319 --> 00:41:44,319
not to

00:41:41,119 --> 00:41:47,599
have these internal steps and also tried

00:41:44,319 --> 00:41:50,480
uh to be as negative as possible like

00:41:47,599 --> 00:41:52,079
i said in the architecture part with the

00:41:50,480 --> 00:41:53,040
combine translation that is more

00:41:52,079 --> 00:41:56,800
performant

00:41:53,040 --> 00:41:57,359
to let's say translate at the combined

00:41:56,800 --> 00:42:00,640
level

00:41:57,359 --> 00:42:01,280
rather than translating the group by key

00:42:00,640 --> 00:42:04,160
the inner

00:42:01,280 --> 00:42:04,800
group by key with asparabiki and dina

00:42:04,160 --> 00:42:06,800
padu

00:42:04,800 --> 00:42:07,920
with a star flat map it is more

00:42:06,800 --> 00:42:10,640
performant to

00:42:07,920 --> 00:42:11,599
to use the native spark aggregator to

00:42:10,640 --> 00:42:14,640
translate

00:42:11,599 --> 00:42:16,880
uh the uh beam combine

00:42:14,640 --> 00:42:18,160
so that's it i hope i've answered your

00:42:16,880 --> 00:42:21,599
question

00:42:18,160 --> 00:42:24,800
um i will i'm late for by a minute

00:42:21,599 --> 00:42:25,839
so i will uh give a big thanks uh for

00:42:24,800 --> 00:42:28,960
joining

00:42:25,839 --> 00:42:29,599
um thank you uh for attending uh the

00:42:28,960 --> 00:42:45,839
talk

00:42:29,599 --> 00:42:45,839
and see you around in the conference

00:42:52,319 --> 00:42:54,400

YouTube URL: https://www.youtube.com/watch?v=_dCmV1ZW3M4


