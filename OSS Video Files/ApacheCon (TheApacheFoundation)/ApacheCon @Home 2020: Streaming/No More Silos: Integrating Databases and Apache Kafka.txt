Title: No More Silos: Integrating Databases and Apache Kafka
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	No More Silos: Integrating Databases and Apache Kafka
Robin Moffatt

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Companies new and old are all recognising the importance of a low-latency, scalable, fault-tolerant data backbone, in the form of the Apache Kafka streaming platform. With Kafka, developers can integrate multiple sources and systems, which enables low latency analytics, event-driven architectures and the population of multiple downstream systems. In this talk, we’ll look at one of the most common integration requirements - connecting databases to Kafka. We’ll consider the concept that all data is a stream of events, including that residing within a database. We’ll look at why we’d want to stream data from a database, including driving applications in Kafka from events upstream. We’ll discuss the different methods for connecting databases to Kafka, and the pros and cons of each. Techniques including Change-Data-Capture (CDC) and Kafka Connect will be covered. Attendees of this talk will learn: * That all data is event streams; databases are just a materialised view of a stream of events. * The best ways to integrate databases with Kafka. * Anti-patterns of which to be aware. * The power of ksqlDB for transforming streams of data in Kafka.

Robin is a Senior Developer Advocate at Confluent, the company founded by the original creators of Apache Kafka, as well as an Oracle ACE Director (Alumnus). He has been speaking at conferences since 2009 including QCon, Devoxx, Strata, Kafka Summit, and Øredev. You can find many of his talks online at http://rmoff.net/talks/, and his blog articles at http://cnfl.io/rmoff and http://rmoff.net/. Outside of work he enjoys drinking good beer and eating fried breakfasts, although generally not at the same time.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,519 --> 00:00:27,840
okay

00:00:25,920 --> 00:00:29,679
good afternoon everyone i'll make it a

00:00:27,840 --> 00:00:30,800
quarter past five here in the uk so i'm

00:00:29,679 --> 00:00:32,239
gonna get started

00:00:30,800 --> 00:00:33,920
and if you could just give me a plus one

00:00:32,239 --> 00:00:34,559
or something in the chat uh so i can see

00:00:33,920 --> 00:00:36,000
that you can

00:00:34,559 --> 00:00:37,760
hear me and see a screen which you

00:00:36,000 --> 00:00:38,800
should see the uh the title slide for

00:00:37,760 --> 00:00:40,719
this video

00:00:38,800 --> 00:00:43,360
good stuff thanks very much so my name

00:00:40,719 --> 00:00:45,520
is robin moffat and i work at confluence

00:00:43,360 --> 00:00:46,879
um i say developer advocates i'm at i'm

00:00:45,520 --> 00:00:47,520
off on twitter if you'd like to follow

00:00:46,879 --> 00:00:49,680
me

00:00:47,520 --> 00:00:50,719
um and today i want to talk about

00:00:49,680 --> 00:00:53,039
integrating

00:00:50,719 --> 00:00:55,120
databases into kafka which kind of

00:00:53,039 --> 00:00:56,399
sounds simple enough

00:00:55,120 --> 00:00:58,480
and i guess we might want to start off

00:00:56,399 --> 00:00:59,359
by thinking about why do we even want to

00:00:58,480 --> 00:01:01,120
do it because

00:00:59,359 --> 00:01:02,559
databases and kafka like they don't

00:01:01,120 --> 00:01:04,879
necessarily obviously

00:01:02,559 --> 00:01:06,240
go together but maybe by the end of this

00:01:04,879 --> 00:01:07,439
talk you'll hopefully agree with me that

00:01:06,240 --> 00:01:10,240
they do kind of

00:01:07,439 --> 00:01:11,600
obviously go together so people want to

00:01:10,240 --> 00:01:14,000
do it for different reasons

00:01:11,600 --> 00:01:16,240
people want to hook up databases through

00:01:14,000 --> 00:01:18,000
kafka to be able to offload their data

00:01:16,240 --> 00:01:20,320
somewhere else so we've got

00:01:18,000 --> 00:01:22,000
transactional data in a database and we

00:01:20,320 --> 00:01:23,680
say we want to take this data

00:01:22,000 --> 00:01:25,680
and put it somewhere else maybe for

00:01:23,680 --> 00:01:29,119
analytics purposes go and put it into

00:01:25,680 --> 00:01:29,600
hdfs or s3 or something somewhere like

00:01:29,119 --> 00:01:31,119
that

00:01:29,600 --> 00:01:32,560
and we'll use kafka to do it so we've

00:01:31,119 --> 00:01:33,360
got that bit of the pipeline there the

00:01:32,560 --> 00:01:36,240
database

00:01:33,360 --> 00:01:37,600
into kafka that we want to be able to do

00:01:36,240 --> 00:01:38,320
then we want to take it a step further

00:01:37,600 --> 00:01:40,320
and say well

00:01:38,320 --> 00:01:41,600
we've got data flowing through kafka

00:01:40,320 --> 00:01:42,240
that we're going to push down somewhere

00:01:41,600 --> 00:01:43,920
else

00:01:42,240 --> 00:01:45,600
but we want to take data from a database

00:01:43,920 --> 00:01:47,280
to help us enrich events

00:01:45,600 --> 00:01:48,640
that we're getting from somewhere else

00:01:47,280 --> 00:01:50,479
so we've written a service and it's

00:01:48,640 --> 00:01:52,000
pushing its events into kafka

00:01:50,479 --> 00:01:53,920
but we want to take those events and

00:01:52,000 --> 00:01:55,439
enrich them maybe we've got things like

00:01:53,920 --> 00:01:56,880
an order being placed and you want to

00:01:55,439 --> 00:01:57,600
say well we've got information about the

00:01:56,880 --> 00:01:59,280
customer

00:01:57,600 --> 00:02:01,119
and that's held in a database so we

00:01:59,280 --> 00:02:03,439
could do lockups you can do those joins

00:02:01,119 --> 00:02:04,399
within kafka itself so again wanting to

00:02:03,439 --> 00:02:06,399
take data

00:02:04,399 --> 00:02:07,520
out of a database and stream it into

00:02:06,399 --> 00:02:10,000
kafka

00:02:07,520 --> 00:02:11,039
or maybe we want to build applications

00:02:10,000 --> 00:02:13,120
that are going to

00:02:11,039 --> 00:02:14,160
move functionality away from an existing

00:02:13,120 --> 00:02:15,599
application and

00:02:14,160 --> 00:02:17,599
most applications are built around

00:02:15,599 --> 00:02:19,680
databases in some way or another

00:02:17,599 --> 00:02:21,280
so say we've got an existing application

00:02:19,680 --> 00:02:23,520
it's using a database

00:02:21,280 --> 00:02:25,440
and we can capture what happens in that

00:02:23,520 --> 00:02:27,120
database so it's a

00:02:25,440 --> 00:02:28,480
stock management system and so it moves

00:02:27,120 --> 00:02:30,480
a piece of stock around and

00:02:28,480 --> 00:02:32,080
under the covers in a database somewhere

00:02:30,480 --> 00:02:34,239
something gets updated

00:02:32,080 --> 00:02:36,160
or we can capture those events out of

00:02:34,239 --> 00:02:37,920
the database into kafka

00:02:36,160 --> 00:02:40,239
and build new applications against it

00:02:37,920 --> 00:02:41,920
that way so all of these different ways

00:02:40,239 --> 00:02:43,680
we're using databases and wanting to

00:02:41,920 --> 00:02:46,000
take that information into kafka

00:02:43,680 --> 00:02:47,920
whether to build new applications driven

00:02:46,000 --> 00:02:50,080
by events happening in the database

00:02:47,920 --> 00:02:51,680
or whether we want to take events or

00:02:50,080 --> 00:02:55,040
data out of a database

00:02:51,680 --> 00:02:56,640
to build new applications but we keep on

00:02:55,040 --> 00:02:59,120
talking about streaming these things

00:02:56,640 --> 00:03:00,239
and we keep talking about databases and

00:02:59,120 --> 00:03:02,800
databases

00:03:00,239 --> 00:03:04,560
certainly feel like lumps of data and

00:03:02,800 --> 00:03:06,080
kind of quite a long way away from

00:03:04,560 --> 00:03:08,319
streams of events which is what we're

00:03:06,080 --> 00:03:10,959
talking about when we talk about kafka

00:03:08,319 --> 00:03:12,000
but it turns out that events which is

00:03:10,959 --> 00:03:13,680
what we're streaming

00:03:12,000 --> 00:03:15,360
are actually all around us whether we

00:03:13,680 --> 00:03:17,280
realize it or not

00:03:15,360 --> 00:03:19,200
a lot of the data that we work with in a

00:03:17,280 --> 00:03:21,040
database actually starts life as an

00:03:19,200 --> 00:03:23,280
event we just choose to store it

00:03:21,040 --> 00:03:24,720
in a database and if we think about the

00:03:23,280 --> 00:03:26,319
fields in which we work and the

00:03:24,720 --> 00:03:27,280
businesses and the kind of the domains

00:03:26,319 --> 00:03:28,879
of data

00:03:27,280 --> 00:03:30,799
we can easily think of lots of different

00:03:28,879 --> 00:03:32,799
events that are created by

00:03:30,799 --> 00:03:34,640
interactions with our business humans

00:03:32,799 --> 00:03:35,519
interacting with our business these are

00:03:34,640 --> 00:03:36,720
all events

00:03:35,519 --> 00:03:38,159
and if you look at those things you

00:03:36,720 --> 00:03:38,640
could probably think well i would put

00:03:38,159 --> 00:03:41,200
that

00:03:38,640 --> 00:03:42,799
in a database but it starts life as an

00:03:41,200 --> 00:03:44,959
event something happened and

00:03:42,799 --> 00:03:46,480
what happened that's what an event is so

00:03:44,959 --> 00:03:49,200
someone buys something they walk into a

00:03:46,480 --> 00:03:50,959
shop opera nowadays they buy it online

00:03:49,200 --> 00:03:52,640
or we have some infantry and it moves

00:03:50,959 --> 00:03:54,239
around between warehouses these

00:03:52,640 --> 00:03:56,319
are events and we can capture these

00:03:54,239 --> 00:03:56,799
events or we have machine generated

00:03:56,319 --> 00:03:58,560
events

00:03:56,799 --> 00:04:01,040
we have things happening on firewalls

00:03:58,560 --> 00:04:02,720
and readings coming from iot devices or

00:04:01,040 --> 00:04:04,799
applications that we're building also

00:04:02,720 --> 00:04:08,000
generate events

00:04:04,799 --> 00:04:11,120
and then databases themselves are also a

00:04:08,000 --> 00:04:12,560
source of events so we can take a

00:04:11,120 --> 00:04:15,680
database and think about it

00:04:12,560 --> 00:04:17,519
in terms of events and like i said

00:04:15,680 --> 00:04:20,000
it looks and it smells like a static

00:04:17,519 --> 00:04:22,960
lump of data and we think we're querying

00:04:20,000 --> 00:04:23,759
a table but if we think carefully about

00:04:22,960 --> 00:04:25,360
that data

00:04:23,759 --> 00:04:27,759
that we're working with it's not

00:04:25,360 --> 00:04:29,360
necessarily quite straightforward

00:04:27,759 --> 00:04:31,440
if you imagine you've got a table so

00:04:29,360 --> 00:04:32,639
obviously a very simple abstracted idea

00:04:31,440 --> 00:04:34,560
of a bank account

00:04:32,639 --> 00:04:36,000
table so here's our table for the

00:04:34,560 --> 00:04:38,880
balance on a bank account so

00:04:36,000 --> 00:04:40,320
for a given key for a given account id

00:04:38,880 --> 00:04:41,120
what's the balance the balance is 50

00:04:40,320 --> 00:04:44,400
euros so

00:04:41,120 --> 00:04:46,000
select balance from uh accounts where id

00:04:44,400 --> 00:04:47,520
equals one two three four five it's a

00:04:46,000 --> 00:04:49,360
database table

00:04:47,520 --> 00:04:50,560
but how did that balance get there well

00:04:49,360 --> 00:04:51,360
the balance got there because we put

00:04:50,560 --> 00:04:53,199
some money

00:04:51,360 --> 00:04:54,560
into the account and then we put some

00:04:53,199 --> 00:04:55,199
more money into the account and now the

00:04:54,560 --> 00:04:57,040
balance

00:04:55,199 --> 00:04:58,479
has changed if we go to the table and

00:04:57,040 --> 00:05:00,800
query the table the balance

00:04:58,479 --> 00:05:03,280
has changed and if we spend some money

00:05:00,800 --> 00:05:05,280
the balance will change again

00:05:03,280 --> 00:05:07,280
so if we look at these two things here

00:05:05,280 --> 00:05:10,639
you can see you've got a table

00:05:07,280 --> 00:05:12,400
and what builds that table is a stream

00:05:10,639 --> 00:05:14,160
so you can take a stream of events and

00:05:12,400 --> 00:05:14,800
you can play them and replay them over

00:05:14,160 --> 00:05:16,400
time

00:05:14,800 --> 00:05:18,000
to give you the state which is what we

00:05:16,400 --> 00:05:20,000
hold in a table

00:05:18,000 --> 00:05:21,199
so it's called the stream table duality

00:05:20,000 --> 00:05:23,759
because you go from

00:05:21,199 --> 00:05:24,800
a stream to a table and it's called a

00:05:23,759 --> 00:05:26,479
duality

00:05:24,800 --> 00:05:28,400
because you can actually say well let's

00:05:26,479 --> 00:05:30,800
take things that happen to a table

00:05:28,400 --> 00:05:31,440
and capture those into a stream of

00:05:30,800 --> 00:05:32,880
events

00:05:31,440 --> 00:05:34,560
so you can go like back and forth and

00:05:32,880 --> 00:05:36,320
full circle with us

00:05:34,560 --> 00:05:38,160
and based on this we have this great

00:05:36,320 --> 00:05:40,960
quotation from pat helens

00:05:38,160 --> 00:05:41,440
the truth is the log the database is a

00:05:40,960 --> 00:05:44,720
cache

00:05:41,440 --> 00:05:45,360
of a subset of the log all you actually

00:05:44,720 --> 00:05:47,199
need

00:05:45,360 --> 00:05:49,120
when you're working with data are the

00:05:47,199 --> 00:05:50,960
events which you can capture in an

00:05:49,120 --> 00:05:53,440
immutable distributed commit log like

00:05:50,960 --> 00:05:54,800
apache kafka and from that you can

00:05:53,440 --> 00:05:56,720
replay those events

00:05:54,800 --> 00:05:57,919
either to drive an application based on

00:05:56,720 --> 00:06:00,800
things that have happened

00:05:57,919 --> 00:06:01,840
or you can use it to build state but all

00:06:00,800 --> 00:06:04,960
you actually need

00:06:01,840 --> 00:06:06,880
is this log of events so

00:06:04,960 --> 00:06:09,280
this is why we might want to think about

00:06:06,880 --> 00:06:10,639
getting data from a database into kafka

00:06:09,280 --> 00:06:12,000
and why it actually makes a huge amount

00:06:10,639 --> 00:06:14,240
of sense to be able to relate

00:06:12,000 --> 00:06:16,080
a database to kafka because they're both

00:06:14,240 --> 00:06:17,840
sources of events or both ways of

00:06:16,080 --> 00:06:19,360
working with events

00:06:17,840 --> 00:06:20,800
but how do we actually go about doing

00:06:19,360 --> 00:06:21,360
this what are the bits and pieces that

00:06:20,800 --> 00:06:24,000
we need

00:06:21,360 --> 00:06:24,400
in our toolbox so i'll pause for a drink

00:06:24,000 --> 00:06:26,400
and

00:06:24,400 --> 00:06:27,919
ask questions in the uh the chat box if

00:06:26,400 --> 00:06:28,880
you want to i've got to open a separate

00:06:27,919 --> 00:06:30,639
thing down here

00:06:28,880 --> 00:06:34,479
um or i'll kind of post some questions

00:06:30,639 --> 00:06:38,000
at the end as well also

00:06:34,479 --> 00:06:39,919
so the kind of things that we want to do

00:06:38,000 --> 00:06:41,759
or the the tools that we're going to use

00:06:39,919 --> 00:06:44,319
in our toolbox for getting data

00:06:41,759 --> 00:06:45,120
from a database into kafka it's probably

00:06:44,319 --> 00:06:48,160
going to be based

00:06:45,120 --> 00:06:50,720
around kafka connect so kafka connect

00:06:48,160 --> 00:06:51,759
is part of apache kafka if you're using

00:06:50,720 --> 00:06:54,319
apache kafka

00:06:51,759 --> 00:06:55,919
you already have kafka connect so it's

00:06:54,319 --> 00:06:57,360
one of the apis it was added

00:06:55,919 --> 00:06:59,039
quite a while ago now i think it was

00:06:57,360 --> 00:07:01,919
version 0.10 but

00:06:59,039 --> 00:07:03,440
like way back then and it enables us to

00:07:01,919 --> 00:07:06,560
do streaming integration

00:07:03,440 --> 00:07:07,440
between systems upstream into kafka and

00:07:06,560 --> 00:07:10,240
from kafka

00:07:07,440 --> 00:07:11,120
downstream to other systems so we can

00:07:10,240 --> 00:07:13,039
build ourselves

00:07:11,120 --> 00:07:14,720
end-to-end pipelines strengthening data

00:07:13,039 --> 00:07:16,800
from one source system or

00:07:14,720 --> 00:07:18,639
message queue or file or rest endpoint

00:07:16,800 --> 00:07:21,520
or anywhere else we've got data

00:07:18,639 --> 00:07:23,599
and we can stream it into kafka and then

00:07:21,520 --> 00:07:27,039
from kafka we can stream it down to

00:07:23,599 --> 00:07:28,639
any number of other places it's just

00:07:27,039 --> 00:07:30,720
configuration file to use

00:07:28,639 --> 00:07:31,919
so you set up a bit of json that says

00:07:30,720 --> 00:07:33,280
this is the connector

00:07:31,919 --> 00:07:34,880
i would like to use and then the

00:07:33,280 --> 00:07:35,919
appropriate information for that

00:07:34,880 --> 00:07:37,599
connector to work

00:07:35,919 --> 00:07:39,599
so if we're connecting to a database

00:07:37,599 --> 00:07:42,720
where is that database which tables

00:07:39,599 --> 00:07:45,840
would we like to ingest into kafka

00:07:42,720 --> 00:07:46,720
and kafka connects is a fantastically uh

00:07:45,840 --> 00:07:49,599
designed

00:07:46,720 --> 00:07:51,919
uh api and it's modular so it's based

00:07:49,599 --> 00:07:53,199
around the idea of connector plugins

00:07:51,919 --> 00:07:55,280
and this is something that someone has

00:07:53,199 --> 00:07:56,160
gone and written which describes how to

00:07:55,280 --> 00:07:58,400
integrate

00:07:56,160 --> 00:08:00,240
with the source or target technology so

00:07:58,400 --> 00:08:01,280
it uses the appropriate apis for that

00:08:00,240 --> 00:08:03,120
technology so

00:08:01,280 --> 00:08:04,800
if we talk into a database one option

00:08:03,120 --> 00:08:06,000
would be using jdbc

00:08:04,800 --> 00:08:08,160
but someone's actually written a

00:08:06,000 --> 00:08:10,080
connector that goes and speaks the jdbc

00:08:08,160 --> 00:08:11,680
to the database to get the data in

00:08:10,080 --> 00:08:13,280
and we're going to talk all about how it

00:08:11,680 --> 00:08:13,840
does that and our options around it in a

00:08:13,280 --> 00:08:15,039
moment

00:08:13,840 --> 00:08:17,520
but this is kind of the basics of

00:08:15,039 --> 00:08:18,800
connect here we've got a connect plug-in

00:08:17,520 --> 00:08:20,639
and it says i'm going to integrate with

00:08:18,800 --> 00:08:22,720
that particular technology

00:08:20,639 --> 00:08:24,560
but then it hands off the actual piece

00:08:22,720 --> 00:08:26,479
of data to another piece within the

00:08:24,560 --> 00:08:27,919
framework because it's very modular for

00:08:26,479 --> 00:08:29,280
very very good reasons

00:08:27,919 --> 00:08:31,440
because what we get from the source

00:08:29,280 --> 00:08:33,519
system we now need to write

00:08:31,440 --> 00:08:35,279
to kafka and we're going to write it to

00:08:33,519 --> 00:08:36,719
kafka as bytes

00:08:35,279 --> 00:08:38,640
because messengers and kafka are just

00:08:36,719 --> 00:08:40,399
bytes we need to serialize it

00:08:38,640 --> 00:08:41,919
in some way and that's where our

00:08:40,399 --> 00:08:44,480
converter plug-in

00:08:41,919 --> 00:08:45,839
comes so someone writes a connector

00:08:44,480 --> 00:08:47,440
plug-in they say like i'm going to

00:08:45,839 --> 00:08:49,680
sort out getting the data from this

00:08:47,440 --> 00:08:50,320
source and i'm going to bring that data

00:08:49,680 --> 00:08:52,160
in

00:08:50,320 --> 00:08:53,600
but then someone else could write a

00:08:52,160 --> 00:08:55,040
plug-in which says okay i'm going to

00:08:53,600 --> 00:08:56,320
take data that's come from

00:08:55,040 --> 00:08:58,399
somewhere that like the connector

00:08:56,320 --> 00:09:00,560
plug-in person figured out how to do

00:08:58,399 --> 00:09:02,320
and i'm going to serialize that into a

00:09:00,560 --> 00:09:02,959
suitable way that we can store it in

00:09:02,320 --> 00:09:05,600
kafka

00:09:02,959 --> 00:09:06,720
but you can mix and match these things

00:09:05,600 --> 00:09:08,240
you can use various different

00:09:06,720 --> 00:09:09,519
serialization methods

00:09:08,240 --> 00:09:11,120
there you can tell i've got like

00:09:09,519 --> 00:09:12,560
opinions on this based on the emojis

00:09:11,120 --> 00:09:14,240
i've used there

00:09:12,560 --> 00:09:15,600
when it comes to taking data

00:09:14,240 --> 00:09:17,519
particularly when we're using kafka

00:09:15,600 --> 00:09:20,560
connect and building pipelines

00:09:17,519 --> 00:09:22,320
schemas matter a huge amount so if we

00:09:20,560 --> 00:09:23,440
just say here's our payload of data and

00:09:22,320 --> 00:09:26,080
they're like the data says

00:09:23,440 --> 00:09:27,920
42 and fred we want to know what those

00:09:26,080 --> 00:09:29,920
fields are what the data types are are

00:09:27,920 --> 00:09:30,880
there any defaults or any fields missing

00:09:29,920 --> 00:09:32,880
are they nullable

00:09:30,880 --> 00:09:34,800
all that kind of stuff we have a schema

00:09:32,880 --> 00:09:35,680
around it because when we come to use

00:09:34,800 --> 00:09:38,160
that data

00:09:35,680 --> 00:09:40,880
we need to know that so something that

00:09:38,160 --> 00:09:42,800
afro or protobuf or json schema

00:09:40,880 --> 00:09:44,720
these are great ways to serialize your

00:09:42,800 --> 00:09:47,120
data json is like

00:09:44,720 --> 00:09:49,200
so so it's like grimace emoji because

00:09:47,120 --> 00:09:50,880
you don't have an explicitly declared

00:09:49,200 --> 00:09:52,880
schema you have like a string that you

00:09:50,880 --> 00:09:54,720
can eyeball and guess at what the schema

00:09:52,880 --> 00:09:56,320
is but it's not explicitly declared

00:09:54,720 --> 00:09:57,760
and thus it's not enforceable so you end

00:09:56,320 --> 00:09:59,519
up with brittle pipelines

00:09:57,760 --> 00:10:00,880
and csv like i hope you're kidding if

00:09:59,519 --> 00:10:02,720
you're suggesting that

00:10:00,880 --> 00:10:04,240
so if you're going to use something with

00:10:02,720 --> 00:10:05,440
strong support for schemas

00:10:04,240 --> 00:10:07,440
you need somewhere to store those

00:10:05,440 --> 00:10:09,040
schemas and one example is the

00:10:07,440 --> 00:10:10,959
confidence scheme registry

00:10:09,040 --> 00:10:13,120
it's community licensed and it gives you

00:10:10,959 --> 00:10:14,600
somewhere for your schemas to reside

00:10:13,120 --> 00:10:17,200
it provides serializers and

00:10:14,600 --> 00:10:18,079
deserializers so in this example kafka

00:10:17,200 --> 00:10:19,760
connect to say

00:10:18,079 --> 00:10:21,839
i'm going to serialize that message

00:10:19,760 --> 00:10:23,839
using avro for example

00:10:21,839 --> 00:10:24,959
onto kafka we get a nice small binary

00:10:23,839 --> 00:10:26,640
payload and then

00:10:24,959 --> 00:10:28,320
the schema just restore the actual

00:10:26,640 --> 00:10:30,560
schema itself

00:10:28,320 --> 00:10:32,079
when we come to use the data it could be

00:10:30,560 --> 00:10:34,399
kafka connect could be

00:10:32,079 --> 00:10:36,480
just a consuming application the scheme

00:10:34,399 --> 00:10:38,000
is available for use by the consumer

00:10:36,480 --> 00:10:39,600
we read the message from kafka

00:10:38,000 --> 00:10:40,320
deserialize it and now we have the

00:10:39,600 --> 00:10:42,399
schema

00:10:40,320 --> 00:10:44,320
so we could say we're taking data from

00:10:42,399 --> 00:10:46,640
one place and we're pushing it into

00:10:44,320 --> 00:10:48,399
hdfs for example well if we have the

00:10:46,640 --> 00:10:48,959
schema we can go and build the hive

00:10:48,399 --> 00:10:52,000
table

00:10:48,959 --> 00:10:53,760
on top of it because we have the schema

00:10:52,000 --> 00:10:55,519
we can also do transformation work as

00:10:53,760 --> 00:10:57,200
part of the pipeline and again single

00:10:55,519 --> 00:10:59,040
message transforms are pluggable

00:10:57,200 --> 00:11:01,120
so you can say as the data passes

00:10:59,040 --> 00:11:03,200
through drop these particular fields

00:11:01,120 --> 00:11:04,640
or change the data type of these so

00:11:03,200 --> 00:11:05,839
single message transforms are really

00:11:04,640 --> 00:11:07,920
powerful

00:11:05,839 --> 00:11:09,760
but it gives us a nice extensible

00:11:07,920 --> 00:11:12,160
powerful framework for building these

00:11:09,760 --> 00:11:14,320
integrations of getting data into kafka

00:11:12,160 --> 00:11:15,519
and getting data from kafka down to

00:11:14,320 --> 00:11:16,959
other places

00:11:15,519 --> 00:11:18,560
you can get the connectors and plugins

00:11:16,959 --> 00:11:20,240
from various different places

00:11:18,560 --> 00:11:21,600
one of those is confluence hub so you

00:11:20,240 --> 00:11:23,120
can have pulsing together

00:11:21,600 --> 00:11:24,880
um you can go and search for the

00:11:23,120 --> 00:11:27,120
particular technology that you'd like to

00:11:24,880 --> 00:11:29,120
integrate with

00:11:27,120 --> 00:11:30,160
now let's think about our databases

00:11:29,120 --> 00:11:31,920
specifically

00:11:30,160 --> 00:11:33,839
so the reason i've spoken about kafka

00:11:31,920 --> 00:11:34,640
connect so much is that it's key to what

00:11:33,839 --> 00:11:36,480
we're doing

00:11:34,640 --> 00:11:38,000
because kafka connect is generally how

00:11:36,480 --> 00:11:38,560
you're going to build your integrations

00:11:38,000 --> 00:11:40,720
or

00:11:38,560 --> 00:11:42,079
generally how you should be building

00:11:40,720 --> 00:11:43,760
your integration so i'm kind of

00:11:42,079 --> 00:11:45,600
strong with my opinions here and feel

00:11:43,760 --> 00:11:47,600
free to to question or

00:11:45,600 --> 00:11:49,839
challenge them but in my opinion kafka

00:11:47,600 --> 00:11:50,959
connect is the best default assumption

00:11:49,839 --> 00:11:53,120
for how you're going to do your

00:11:50,959 --> 00:11:55,360
integrations otherwise exist

00:11:53,120 --> 00:11:57,200
kafka connect is widely accepted as a

00:11:55,360 --> 00:11:58,560
very good way to do it

00:11:57,200 --> 00:12:00,320
but now we need to think about well

00:11:58,560 --> 00:12:00,880
which connected plug-in are we going to

00:12:00,320 --> 00:12:03,839
use

00:12:00,880 --> 00:12:06,320
i mentioned jdbc earlier on but that's

00:12:03,839 --> 00:12:08,399
not the only way in which to do it

00:12:06,320 --> 00:12:09,440
and when it comes to getting data out of

00:12:08,399 --> 00:12:11,519
a database

00:12:09,440 --> 00:12:13,279
we're going to use a kind of technique

00:12:11,519 --> 00:12:15,279
called change data capture

00:12:13,279 --> 00:12:17,279
which people understand in different

00:12:15,279 --> 00:12:18,079
ways so to start off with i'm going to

00:12:17,279 --> 00:12:20,320
explain

00:12:18,079 --> 00:12:22,160
what i mean by change data capture and

00:12:20,320 --> 00:12:23,680
in my view there are two different types

00:12:22,160 --> 00:12:25,680
of change data capture

00:12:23,680 --> 00:12:26,959
one is query based change data capture

00:12:25,680 --> 00:12:28,480
i'm going to start saying cdc because

00:12:26,959 --> 00:12:32,480
i've only got 40 minutes so

00:12:28,480 --> 00:12:35,440
query based cdc on log based cdc

00:12:32,480 --> 00:12:36,160
query based cdc is based on an idea of

00:12:35,440 --> 00:12:38,240
querying

00:12:36,160 --> 00:12:39,920
the database to try and figure out

00:12:38,240 --> 00:12:41,279
what's changed since we last queried it

00:12:39,920 --> 00:12:41,839
so we're going to pull the database and

00:12:41,279 --> 00:12:44,160
say

00:12:41,839 --> 00:12:45,279
what's changed since we were last here

00:12:44,160 --> 00:12:46,000
and we could do that based on a

00:12:45,279 --> 00:12:47,519
timestamp

00:12:46,000 --> 00:12:49,040
or based on an id column that's

00:12:47,519 --> 00:12:51,440
incrementing each time

00:12:49,040 --> 00:12:53,120
so we go to the database we say what

00:12:51,440 --> 00:12:55,040
changed since we last checked it says

00:12:53,120 --> 00:12:56,320
okay these two rows here have changed

00:12:55,040 --> 00:12:58,399
and then something else changes in the

00:12:56,320 --> 00:12:59,839
database and we pull the database again

00:12:58,399 --> 00:13:01,839
this is okay now there's this additional

00:12:59,839 --> 00:13:04,160
row has also changed

00:13:01,839 --> 00:13:05,279
so we can extract rows from the database

00:13:04,160 --> 00:13:08,720
as they change

00:13:05,279 --> 00:13:11,920
into kafka that's query based cdc

00:13:08,720 --> 00:13:13,360
log based cdc is based on the databases

00:13:11,920 --> 00:13:14,720
transaction log or bid and logger

00:13:13,360 --> 00:13:15,519
whichever flavor of database you're

00:13:14,720 --> 00:13:17,600
using

00:13:15,519 --> 00:13:18,720
the log that the database internally

00:13:17,600 --> 00:13:20,720
writes stuff to

00:13:18,720 --> 00:13:22,079
when things happen the log that if the

00:13:20,720 --> 00:13:23,519
database goes bang

00:13:22,079 --> 00:13:25,519
you cross your fingers and cross your

00:13:23,519 --> 00:13:26,240
toes and hope that you've got a copy of

00:13:25,519 --> 00:13:28,320
because that's what you're going to

00:13:26,240 --> 00:13:30,160
recover the database with by rolling

00:13:28,320 --> 00:13:30,560
those transactions forwards and applying

00:13:30,160 --> 00:13:34,000
them

00:13:30,560 --> 00:13:35,760
to the database so with log based cdc

00:13:34,000 --> 00:13:37,279
we're actually using that transaction

00:13:35,760 --> 00:13:39,199
log to take those events

00:13:37,279 --> 00:13:41,360
as they happen in the database and

00:13:39,199 --> 00:13:43,279
stream them into a kafka topic

00:13:41,360 --> 00:13:44,399
so something happens in the database we

00:13:43,279 --> 00:13:47,279
stream it over

00:13:44,399 --> 00:13:48,000
into kafka so let me show you these two

00:13:47,279 --> 00:13:50,320
in action

00:13:48,000 --> 00:13:51,120
you can go and get the uh the demo that

00:13:50,320 --> 00:13:53,120
i'm going to show you

00:13:51,120 --> 00:13:54,399
uh online in fact let me show you where

00:13:53,120 --> 00:13:56,959
it actually is

00:13:54,399 --> 00:13:57,680
so here's my window here and if i put it

00:13:56,959 --> 00:14:00,160
on the screen

00:13:57,680 --> 00:14:00,880
you can see that so there's a repository

00:14:00,160 --> 00:14:02,720
here called

00:14:00,880 --> 00:14:03,920
a demo scene i'm not sharing like one

00:14:02,720 --> 00:14:05,519
yes i am so

00:14:03,920 --> 00:14:07,760
it's called demo scene and there's a

00:14:05,519 --> 00:14:09,600
whole bunch of different uh demos here

00:14:07,760 --> 00:14:11,120
so one of them is this one called no

00:14:09,600 --> 00:14:12,800
more silos and

00:14:11,120 --> 00:14:14,720
within it it's actually got the step by

00:14:12,800 --> 00:14:16,639
step so because i'm not very good at

00:14:14,720 --> 00:14:18,560
memorizing things or live demos suck if

00:14:16,639 --> 00:14:19,680
the person keeps on fat fingering things

00:14:18,560 --> 00:14:21,600
i'm actually just going to copy and

00:14:19,680 --> 00:14:22,800
paste from this but it also means that

00:14:21,600 --> 00:14:24,800
you can follow it along

00:14:22,800 --> 00:14:25,920
and try it for yourselves so it's based

00:14:24,800 --> 00:14:27,600
on docker so

00:14:25,920 --> 00:14:28,800
we've brought the stack up and we're

00:14:27,600 --> 00:14:29,680
just going to check that everything is

00:14:28,800 --> 00:14:33,600
running

00:14:29,680 --> 00:14:36,320
so if i do this and do that

00:14:33,600 --> 00:14:37,760
we can see that's all working and it

00:14:36,320 --> 00:14:38,720
says the kafka connect is running so

00:14:37,760 --> 00:14:39,519
we're now going to check that our

00:14:38,720 --> 00:14:42,079
plugins

00:14:39,519 --> 00:14:43,360
have been installed correctly so i'm

00:14:42,079 --> 00:14:44,560
going to share some links afterwards

00:14:43,360 --> 00:14:46,240
including the slides

00:14:44,560 --> 00:14:48,160
but one of them is a talk all about the

00:14:46,240 --> 00:14:49,839
kind of kafka connect and how you run it

00:14:48,160 --> 00:14:51,519
and kafka connect workers and all that

00:14:49,839 --> 00:14:53,440
kind of stuff so that's not this talk

00:14:51,519 --> 00:14:54,639
this talk is about databases

00:14:53,440 --> 00:14:56,480
but we're going to use kafka connect it

00:14:54,639 --> 00:14:58,160
has these plugins that i mentioned and

00:14:56,480 --> 00:14:59,920
we can go to the worker and can say

00:14:58,160 --> 00:15:02,079
has it got the particular connectors

00:14:59,920 --> 00:15:03,279
that i want to show you and it does

00:15:02,079 --> 00:15:05,839
so now we're going to say well let's

00:15:03,279 --> 00:15:08,320
have a look at some data in the database

00:15:05,839 --> 00:15:09,199
that we want to pull into kafka so in

00:15:08,320 --> 00:15:10,959
our database

00:15:09,199 --> 00:15:12,320
we've got some tables let's say show

00:15:10,959 --> 00:15:13,680
tables it says you've got one called

00:15:12,320 --> 00:15:15,600
customers

00:15:13,680 --> 00:15:17,199
so let's have a look at that customers

00:15:15,600 --> 00:15:19,760
table so we can say describe

00:15:17,199 --> 00:15:19,760
customers

00:15:20,639 --> 00:15:24,079
and it says there's your schema so we've

00:15:22,240 --> 00:15:25,279
got things like an id column and a first

00:15:24,079 --> 00:15:26,399
name last name

00:15:25,279 --> 00:15:28,399
we've got this one here which is going

00:15:26,399 --> 00:15:29,040
to be important when was the row last

00:15:28,399 --> 00:15:30,800
updated

00:15:29,040 --> 00:15:31,920
and we can see it's got a default and

00:15:30,800 --> 00:15:34,480
it's also got a thing on it which is

00:15:31,920 --> 00:15:36,240
going to update it automatically for us

00:15:34,480 --> 00:15:38,320
and if we query the database we've got

00:15:36,240 --> 00:15:41,199
five rows it's just a small little table

00:15:38,320 --> 00:15:42,399
of information about our customers so

00:15:41,199 --> 00:15:44,959
let's pull that

00:15:42,399 --> 00:15:45,440
into kafka and the first one we're going

00:15:44,959 --> 00:15:48,079
to do

00:15:45,440 --> 00:15:49,759
is we're going to use the query based

00:15:48,079 --> 00:15:52,160
change data capture

00:15:49,759 --> 00:15:53,759
so this says i would like to use the in

00:15:52,160 --> 00:15:54,399
this case it's called the gdbc source

00:15:53,759 --> 00:15:55,759
connector

00:15:54,399 --> 00:15:57,440
i'll go into the details of these at the

00:15:55,759 --> 00:16:00,959
very end for now we'll just talk about

00:15:57,440 --> 00:16:01,519
query based cdc we say here is my jdbc

00:16:00,959 --> 00:16:03,519
connection

00:16:01,519 --> 00:16:05,680
because we're using jdbc to talk to the

00:16:03,519 --> 00:16:06,720
database here's my credentials his

00:16:05,680 --> 00:16:09,440
particular table

00:16:06,720 --> 00:16:10,480
i want to pull the data in from and we

00:16:09,440 --> 00:16:13,199
hit enter

00:16:10,480 --> 00:16:13,759
and it says okay i've created that

00:16:13,199 --> 00:16:16,560
hopefully

00:16:13,759 --> 00:16:17,440
now we can say sort and we can see that

00:16:16,560 --> 00:16:21,279
it's actually

00:16:17,440 --> 00:16:22,720
running that particular connector here

00:16:21,279 --> 00:16:24,800
like that so that shows that particular

00:16:22,720 --> 00:16:28,160
connector it's been created

00:16:24,800 --> 00:16:30,079
and it's running so that has taken a

00:16:28,160 --> 00:16:31,519
select against that source table and

00:16:30,079 --> 00:16:33,199
streamed it into kafka and now it's

00:16:31,519 --> 00:16:34,320
running so now it's pulling the database

00:16:33,199 --> 00:16:36,880
for any changes

00:16:34,320 --> 00:16:37,440
to that table so we can go over to kafka

00:16:36,880 --> 00:16:39,600
and we can say

00:16:37,440 --> 00:16:41,920
show me the contents of that table oh

00:16:39,600 --> 00:16:42,959
that's sorry of that topic

00:16:41,920 --> 00:16:45,680
topics tables the kind of

00:16:42,959 --> 00:16:48,639
interchangeable data entities

00:16:45,680 --> 00:16:49,120
and here i'm going to use kafkacat which

00:16:48,639 --> 00:16:51,680
just acts as

00:16:49,120 --> 00:16:53,199
a consumer against the kafka topic to

00:16:51,680 --> 00:16:53,680
pull in the data from this particular

00:16:53,199 --> 00:16:55,839
topic

00:16:53,680 --> 00:16:57,680
to which we stream the data i'm going to

00:16:55,839 --> 00:16:58,880
type it through jq just to pretty print

00:16:57,680 --> 00:17:01,199
it on the screen so you can

00:16:58,880 --> 00:17:02,880
see what's going on so you can see we've

00:17:01,199 --> 00:17:04,799
got a bunch of different messages

00:17:02,880 --> 00:17:06,400
so this message here is for customer

00:17:04,799 --> 00:17:09,199
five and you can see that

00:17:06,400 --> 00:17:10,319
that sorry that there let me get my

00:17:09,199 --> 00:17:13,679
selections right

00:17:10,319 --> 00:17:16,319
matches this in the database

00:17:13,679 --> 00:17:18,000
so database copied into a kafka topic

00:17:16,319 --> 00:17:20,240
which is kind of neat

00:17:18,000 --> 00:17:21,520
but now let's see what happens if we

00:17:20,240 --> 00:17:23,839
insert a row

00:17:21,520 --> 00:17:26,559
in our database so let's clear that

00:17:23,839 --> 00:17:28,079
there and insert a row into the database

00:17:26,559 --> 00:17:29,600
and if you blink you'll miss it over in

00:17:28,079 --> 00:17:31,600
our kafka topic here

00:17:29,600 --> 00:17:32,880
we've now got that row that was inserted

00:17:31,600 --> 00:17:34,720
into the database

00:17:32,880 --> 00:17:36,320
and if i make a change in the database

00:17:34,720 --> 00:17:37,200
over here in my sql i'm going to do an

00:17:36,320 --> 00:17:39,360
update

00:17:37,200 --> 00:17:41,120
so paste that update in and on the right

00:17:39,360 --> 00:17:42,400
hand side there we can see we picked up

00:17:41,120 --> 00:17:44,320
that update

00:17:42,400 --> 00:17:47,679
if i page on the right hand side up

00:17:44,320 --> 00:17:49,200
through the topic and do that

00:17:47,679 --> 00:17:50,880
you can see there was our insert and it

00:17:49,200 --> 00:17:52,320
had no email address

00:17:50,880 --> 00:17:54,000
and we paged down and we can see the

00:17:52,320 --> 00:17:54,559
next message it did have an email

00:17:54,000 --> 00:17:57,120
address

00:17:54,559 --> 00:17:58,080
because in the database we did an insert

00:17:57,120 --> 00:18:00,720
and then we did

00:17:58,080 --> 00:18:01,200
an update so that's using the query

00:18:00,720 --> 00:18:02,799
based

00:18:01,200 --> 00:18:04,400
change data capture we're going to the

00:18:02,799 --> 00:18:05,360
database and we're pulling the database

00:18:04,400 --> 00:18:07,679
for changes

00:18:05,360 --> 00:18:09,120
and it's based on this update timestamp

00:18:07,679 --> 00:18:10,400
field incrementing

00:18:09,120 --> 00:18:13,039
uh so we can tell when things have

00:18:10,400 --> 00:18:16,160
changed now let's have a look

00:18:13,039 --> 00:18:18,240
at the log based change data capture

00:18:16,160 --> 00:18:19,280
so i'm going to clear that screen on the

00:18:18,240 --> 00:18:20,559
right hand side

00:18:19,280 --> 00:18:21,280
we're going to create another connector

00:18:20,559 --> 00:18:24,320
this time we're going to use the

00:18:21,280 --> 00:18:25,840
division my sql connector

00:18:24,320 --> 00:18:28,559
so here we're saying the my sql

00:18:25,840 --> 00:18:30,480
connector and here's how we connect to

00:18:28,559 --> 00:18:32,000
the my sql database here's the

00:18:30,480 --> 00:18:32,720
particular table that would like to

00:18:32,000 --> 00:18:35,840
ingest

00:18:32,720 --> 00:18:36,799
into our kafka topic so we're going to

00:18:35,840 --> 00:18:38,799
create that

00:18:36,799 --> 00:18:40,000
and we say sort and make sure that's

00:18:38,799 --> 00:18:40,880
running and we can see we've got this

00:18:40,000 --> 00:18:43,600
one here

00:18:40,880 --> 00:18:44,720
our source connector and it says it's

00:18:43,600 --> 00:18:47,760
running which is

00:18:44,720 --> 00:18:49,360
again good news so again let's go and

00:18:47,760 --> 00:18:50,799
have a look at the kafka topic

00:18:49,360 --> 00:18:52,720
and it's right into a different kafka

00:18:50,799 --> 00:18:54,960
topic this time so we're

00:18:52,720 --> 00:18:56,000
using kafka cat against this kafka topic

00:18:54,960 --> 00:18:59,120
that we're streaming the data

00:18:56,000 --> 00:19:00,880
to again we're piping it through jq

00:18:59,120 --> 00:19:02,720
and this time we see that we get a

00:19:00,880 --> 00:19:04,960
payload that we would expect

00:19:02,720 --> 00:19:06,799
is mr rick astley who's the bureau that

00:19:04,960 --> 00:19:08,799
we updated and changed

00:19:06,799 --> 00:19:10,240
but now we get a bunch more information

00:19:08,799 --> 00:19:11,520
we get a bunch of metadata we get all

00:19:10,240 --> 00:19:12,880
this lovely stuff here so

00:19:11,520 --> 00:19:14,400
what was the version of the connector

00:19:12,880 --> 00:19:15,520
what was the server it came from which

00:19:14,400 --> 00:19:17,120
was the table

00:19:15,520 --> 00:19:19,039
what was the file bin log that it came

00:19:17,120 --> 00:19:19,679
from what was the siri the scn on the

00:19:19,039 --> 00:19:22,320
system

00:19:19,679 --> 00:19:23,760
what operation was it was a create so

00:19:22,320 --> 00:19:24,640
we're doing a snapshot against that

00:19:23,760 --> 00:19:27,760
existing

00:19:24,640 --> 00:19:29,360
database table and if i now cancel that

00:19:27,760 --> 00:19:30,400
so that topic on the right hand side

00:19:29,360 --> 00:19:31,760
it's continually

00:19:30,400 --> 00:19:34,400
it's going to show us any updates to

00:19:31,760 --> 00:19:36,320
that topic if on the left hand side

00:19:34,400 --> 00:19:38,160
we're going to make some more changes to

00:19:36,320 --> 00:19:40,559
that database

00:19:38,160 --> 00:19:42,240
and we're going to do this so we're

00:19:40,559 --> 00:19:43,679
going to update rick astley again so

00:19:42,240 --> 00:19:46,480
we've set his email address

00:19:43,679 --> 00:19:47,840
and now we need to fix the email address

00:19:46,480 --> 00:19:48,320
so i'm going to change the email address

00:19:47,840 --> 00:19:49,840
there

00:19:48,320 --> 00:19:51,039
on the right hand side we've pulled that

00:19:49,840 --> 00:19:52,320
through and you can see the operation

00:19:51,039 --> 00:19:53,760
was an update

00:19:52,320 --> 00:19:55,520
and now here's where we get into the

00:19:53,760 --> 00:19:56,320
difference the real differences between

00:19:55,520 --> 00:19:59,280
log based

00:19:56,320 --> 00:20:01,760
and query based because query base says

00:19:59,280 --> 00:20:05,039
this row changed here is the row

00:20:01,760 --> 00:20:06,000
log based says i got an update i got an

00:20:05,039 --> 00:20:08,080
event in the database

00:20:06,000 --> 00:20:09,840
transaction log here's what it is and in

00:20:08,080 --> 00:20:12,400
the case of an update it says well

00:20:09,840 --> 00:20:13,440
this throw here is updated and this is

00:20:12,400 --> 00:20:15,280
what it looked like

00:20:13,440 --> 00:20:16,720
beforehand we'll just page down and show

00:20:15,280 --> 00:20:17,520
you that this is what it looked like

00:20:16,720 --> 00:20:19,760
beforehand

00:20:17,520 --> 00:20:21,760
okay so here's your email address and

00:20:19,760 --> 00:20:24,640
this is what it looked like afterwards

00:20:21,760 --> 00:20:25,120
and here is your email address and by

00:20:24,640 --> 00:20:26,640
the way

00:20:25,120 --> 00:20:28,000
here's all your other lovely metadata

00:20:26,640 --> 00:20:30,080
and where it was from the bin log and

00:20:28,000 --> 00:20:31,840
all the rest of it and it was an update

00:20:30,080 --> 00:20:34,480
so you can now build applications which

00:20:31,840 --> 00:20:36,240
are driven by when something changes in

00:20:34,480 --> 00:20:38,080
that source system whether it underpins

00:20:36,240 --> 00:20:40,240
a third-party application or whatever

00:20:38,080 --> 00:20:41,760
you can say this got changed this got

00:20:40,240 --> 00:20:44,559
inserted this got updated

00:20:41,760 --> 00:20:46,000
what was the before what was the after

00:20:44,559 --> 00:20:47,440
we can do this we can say let's go to

00:20:46,000 --> 00:20:49,360
the customers table

00:20:47,440 --> 00:20:50,720
and make an update to the name so we

00:20:49,360 --> 00:20:52,159
update the name to bob and you can see

00:20:50,720 --> 00:20:54,400
you get another update through

00:20:52,159 --> 00:20:56,080
we can also do this and this you can't

00:20:54,400 --> 00:20:59,520
do with a query based

00:20:56,080 --> 00:21:01,760
cdc we can say delete from customers

00:20:59,520 --> 00:21:02,640
and over here in our kafka topic we've

00:21:01,760 --> 00:21:04,960
got a delete

00:21:02,640 --> 00:21:06,480
come through and because it's a delete

00:21:04,960 --> 00:21:08,720
it says well afterwards

00:21:06,480 --> 00:21:09,760
it was no it's been deleted it's not it

00:21:08,720 --> 00:21:11,360
doesn't exist

00:21:09,760 --> 00:21:13,600
what was it before it got deleted well

00:21:11,360 --> 00:21:15,280
this is the record that got deleted

00:21:13,600 --> 00:21:17,520
so now we can build applications that

00:21:15,280 --> 00:21:18,159
say well regardless if it got deleted

00:21:17,520 --> 00:21:19,840
upstream

00:21:18,159 --> 00:21:23,360
we can actually capture those deletes we

00:21:19,840 --> 00:21:26,000
can build really cool things with it

00:21:23,360 --> 00:21:27,120
so let me head back over to the slides

00:21:26,000 --> 00:21:29,840
and we can talk about this

00:21:27,120 --> 00:21:30,799
just a little bit more we've got to

00:21:29,840 --> 00:21:32,960
decide

00:21:30,799 --> 00:21:34,960
how we're going to choose between log

00:21:32,960 --> 00:21:36,240
based and query base and we could go

00:21:34,960 --> 00:21:38,240
eeny meeny miny mo

00:21:36,240 --> 00:21:39,919
and that kind of suffices to start with

00:21:38,240 --> 00:21:42,400
but say we need to justify

00:21:39,919 --> 00:21:43,679
how did we choose each one so let's take

00:21:42,400 --> 00:21:46,559
a bit more of a look

00:21:43,679 --> 00:21:49,120
at the differences between them query

00:21:46,559 --> 00:21:51,919
based change data capture relies on

00:21:49,120 --> 00:21:54,400
the source schema having a field that we

00:21:51,919 --> 00:21:56,559
can use to identify what changed

00:21:54,400 --> 00:21:57,679
so that either means adding a timestamp

00:21:56,559 --> 00:21:59,840
to the table

00:21:57,679 --> 00:22:01,120
or it means adding an id field to the

00:21:59,840 --> 00:22:03,440
table and that id

00:22:01,120 --> 00:22:04,400
field needs to go up each time so the

00:22:03,440 --> 00:22:07,039
connector will say

00:22:04,400 --> 00:22:08,480
is it greater than it was last time so

00:22:07,039 --> 00:22:09,280
if you've got an id column that gets

00:22:08,480 --> 00:22:10,880
created and like

00:22:09,280 --> 00:22:13,200
increments each time you're going to

00:22:10,880 --> 00:22:14,480
capture all of your uh inserts because

00:22:13,200 --> 00:22:15,039
it's going to go up so it'll be a new

00:22:14,480 --> 00:22:17,039
one

00:22:15,039 --> 00:22:18,799
you won't capture your updates unless

00:22:17,039 --> 00:22:21,039
that id field increases

00:22:18,799 --> 00:22:22,559
when we do an update so a timestamp is a

00:22:21,039 --> 00:22:24,000
much more obvious one you have a field

00:22:22,559 --> 00:22:26,400
and a team which says when was it

00:22:24,000 --> 00:22:27,039
last updated when was it created and so

00:22:26,400 --> 00:22:30,000
you can base

00:22:27,039 --> 00:22:31,120
your polling based on that so a rule

00:22:30,000 --> 00:22:32,559
gets inserted

00:22:31,120 --> 00:22:34,799
we can capture that because we can

00:22:32,559 --> 00:22:37,039
compare it to the timestamp previously

00:22:34,799 --> 00:22:38,960
a row gets updated we can capture that

00:22:37,039 --> 00:22:42,559
because again the update timestamp is

00:22:38,960 --> 00:22:42,559
greater than it was previously

00:22:42,960 --> 00:22:46,000
a rule gets deleted we can't capture

00:22:45,360 --> 00:22:47,840
that

00:22:46,000 --> 00:22:49,760
because how can you go to a database and

00:22:47,840 --> 00:22:52,000
say select all the rows

00:22:49,760 --> 00:22:53,360
that don't exist well you can't you get

00:22:52,000 --> 00:22:55,120
kind of like well no rows were found

00:22:53,360 --> 00:22:57,360
because they don't exist anymore

00:22:55,120 --> 00:22:58,559
so here's the big difference query based

00:22:57,360 --> 00:23:02,000
change data capture

00:22:58,559 --> 00:23:03,919
you can't capture guides here's another

00:23:02,000 --> 00:23:05,520
more subtle but equally important

00:23:03,919 --> 00:23:07,760
difference

00:23:05,520 --> 00:23:08,880
if we're pulling the database there's

00:23:07,760 --> 00:23:10,720
going to be an interval

00:23:08,880 --> 00:23:12,240
in which we're not querying the database

00:23:10,720 --> 00:23:14,960
for things that have changed

00:23:12,240 --> 00:23:16,080
so let's say we've got a 30-second poll

00:23:14,960 --> 00:23:18,159
on our connector

00:23:16,080 --> 00:23:19,760
and we do a select against the orders

00:23:18,159 --> 00:23:21,280
table and say what order's been created

00:23:19,760 --> 00:23:23,120
in the last 30 seconds

00:23:21,280 --> 00:23:25,200
and he says well here's this one order i

00:23:23,120 --> 00:23:27,919
thought it ordered id 42

00:23:25,200 --> 00:23:28,880
it's been shipped it was last updated at

00:23:27,919 --> 00:23:31,120
10 54

00:23:28,880 --> 00:23:32,960
and 29 seconds you say okay great we've

00:23:31,120 --> 00:23:35,919
captured the event

00:23:32,960 --> 00:23:36,400
but if you actually think about how that

00:23:35,919 --> 00:23:37,760
event

00:23:36,400 --> 00:23:40,080
could have come about how that rule

00:23:37,760 --> 00:23:42,159
could have come about maybe at one

00:23:40,080 --> 00:23:43,600
seconds past the minute so like just

00:23:42,159 --> 00:23:45,679
after we last polled

00:23:43,600 --> 00:23:47,840
the polling every 30 seconds it got

00:23:45,679 --> 00:23:48,640
created so the order was created and it

00:23:47,840 --> 00:23:50,480
was pending

00:23:48,640 --> 00:23:52,240
and we didn't have an address associated

00:23:50,480 --> 00:23:53,679
with us and then another application

00:23:52,240 --> 00:23:56,000
comes along and it updates

00:23:53,679 --> 00:23:56,880
the address so this is our second change

00:23:56,000 --> 00:23:58,640
to the table

00:23:56,880 --> 00:23:59,840
and then another system comes along and

00:23:58,640 --> 00:24:01,279
says well actually i kind of need to

00:23:59,840 --> 00:24:03,679
retro change this data

00:24:01,279 --> 00:24:04,799
now we change the data again and then

00:24:03,679 --> 00:24:08,000
finally we ship it

00:24:04,799 --> 00:24:09,440
and we change the data again well all of

00:24:08,000 --> 00:24:12,240
these things have happened

00:24:09,440 --> 00:24:13,440
to the data yet when we pull the table

00:24:12,240 --> 00:24:15,600
every 30 seconds

00:24:13,440 --> 00:24:16,720
we only get to see the last one you

00:24:15,600 --> 00:24:19,760
don't catch those

00:24:16,720 --> 00:24:20,720
intermediate events you may not care you

00:24:19,760 --> 00:24:22,159
may simply say

00:24:20,720 --> 00:24:24,080
i just want to make sure that whatever

00:24:22,159 --> 00:24:27,200
the table currently looks like

00:24:24,080 --> 00:24:28,480
is what i have currently in my topic but

00:24:27,200 --> 00:24:30,480
you may say

00:24:28,480 --> 00:24:32,480
i would like to build an application

00:24:30,480 --> 00:24:33,200
that if someone changes the address on

00:24:32,480 --> 00:24:35,600
an order

00:24:33,200 --> 00:24:37,760
i need to rerun the fraud detection or

00:24:35,600 --> 00:24:38,240
when an order moves from pending to

00:24:37,760 --> 00:24:39,760
shipped

00:24:38,240 --> 00:24:41,919
i need to go and ring a bell somewhere

00:24:39,760 --> 00:24:44,159
and do something based on that event

00:24:41,919 --> 00:24:45,360
happening and if we're using query based

00:24:44,159 --> 00:24:48,080
changing to capture

00:24:45,360 --> 00:24:49,840
we cannot capture all of those events we

00:24:48,080 --> 00:24:51,919
could make the polling frequency

00:24:49,840 --> 00:24:53,520
uh more often and say like let's pull a

00:24:51,919 --> 00:24:54,559
database every second

00:24:53,520 --> 00:24:56,240
but you can imagine what that's going to

00:24:54,559 --> 00:24:57,279
do to your scalability and also to your

00:24:56,240 --> 00:24:58,480
dba's temper

00:24:57,279 --> 00:25:00,080
because you say well what are you doing

00:24:58,480 --> 00:25:01,279
to my database you're pulling it all

00:25:00,080 --> 00:25:03,440
this time

00:25:01,279 --> 00:25:05,200
and also even within a second stuff's

00:25:03,440 --> 00:25:06,720
going to happen things get updated

00:25:05,200 --> 00:25:10,320
automatically so we simply cannot

00:25:06,720 --> 00:25:12,480
guarantee that we capture every event

00:25:10,320 --> 00:25:14,559
so it's generally easier to set up it's

00:25:12,480 --> 00:25:15,279
a great one for prototyping and setting

00:25:14,559 --> 00:25:17,360
up systems

00:25:15,279 --> 00:25:18,320
where i was simply saying let's see what

00:25:17,360 --> 00:25:20,400
we can build

00:25:18,320 --> 00:25:22,080
if we've got data from this database in

00:25:20,400 --> 00:25:23,760
kafka that's great for that

00:25:22,080 --> 00:25:25,039
but when it comes down to nitty gritty

00:25:23,760 --> 00:25:26,240
it doesn't actually do everything you

00:25:25,039 --> 00:25:28,320
need it to

00:25:26,240 --> 00:25:29,919
so it's certainly easier but you're

00:25:28,320 --> 00:25:30,960
going to find if you're building event

00:25:29,919 --> 00:25:33,440
driven applications

00:25:30,960 --> 00:25:34,159
certainly that log-based change data

00:25:33,440 --> 00:25:36,320
capture

00:25:34,159 --> 00:25:37,840
is a more solid option because it gives

00:25:36,320 --> 00:25:39,919
you the fidelity of the data

00:25:37,840 --> 00:25:42,080
that query base doesn't or put another

00:25:39,919 --> 00:25:45,039
way it's simply a more refined way

00:25:42,080 --> 00:25:47,120
of capturing your data so log based

00:25:45,039 --> 00:25:49,360
change data capture as we mentioned

00:25:47,120 --> 00:25:50,400
it worked against the transaction log of

00:25:49,360 --> 00:25:52,799
a database

00:25:50,400 --> 00:25:53,679
so as something happens in the database

00:25:52,799 --> 00:25:55,279
the user does an

00:25:53,679 --> 00:25:57,360
insert they do an update they do a

00:25:55,279 --> 00:25:59,360
delete they commit the transaction

00:25:57,360 --> 00:26:01,440
that database writes it down to the log

00:25:59,360 --> 00:26:02,240
first and then flushes it to the data

00:26:01,440 --> 00:26:04,559
files

00:26:02,240 --> 00:26:07,120
the log in the database as any dba will

00:26:04,559 --> 00:26:08,880
know is supremely valuable important you

00:26:07,120 --> 00:26:10,240
really hope you don't lose that

00:26:08,880 --> 00:26:11,919
because if you lose it you cannot

00:26:10,240 --> 00:26:13,200
rebuild your database back to a given

00:26:11,919 --> 00:26:13,440
point in time you just have to roll back

00:26:13,200 --> 00:26:16,159
to

00:26:13,440 --> 00:26:17,360
like a snapshot backup database logs

00:26:16,159 --> 00:26:19,039
that would roll back and forwards

00:26:17,360 --> 00:26:20,320
because they capture a stream of events

00:26:19,039 --> 00:26:22,400
that happened in the database and you

00:26:20,320 --> 00:26:24,400
can apply that stream of events

00:26:22,400 --> 00:26:25,600
and you may well start to see parallels

00:26:24,400 --> 00:26:28,480
between how

00:26:25,600 --> 00:26:29,840
kafka gives us that fundamental ability

00:26:28,480 --> 00:26:31,600
of building a commit log

00:26:29,840 --> 00:26:34,000
on which we can build other things

00:26:31,600 --> 00:26:35,360
whereas databases based on a commit log

00:26:34,000 --> 00:26:37,520
and then they say and now here is your

00:26:35,360 --> 00:26:38,960
state on top of it

00:26:37,520 --> 00:26:41,039
so something happens in the database we

00:26:38,960 --> 00:26:42,640
do an insert we capture that into kafka

00:26:41,039 --> 00:26:44,640
we do an update it gets written into the

00:26:42,640 --> 00:26:47,520
transaction log we can capture us

00:26:44,640 --> 00:26:48,799
into kafka we do a delete something gets

00:26:47,520 --> 00:26:50,240
deleted from the database

00:26:48,799 --> 00:26:52,000
that's still something that gets written

00:26:50,240 --> 00:26:53,520
to the transaction log and because it's

00:26:52,000 --> 00:26:56,720
written to the transaction log

00:26:53,520 --> 00:26:58,799
we can also capture it to kafka so

00:26:56,720 --> 00:27:00,799
there is this parallel between a

00:26:58,799 --> 00:27:02,720
database which is built on

00:27:00,799 --> 00:27:04,320
a commit log and like takes that and

00:27:02,720 --> 00:27:05,679
says well here is an api

00:27:04,320 --> 00:27:07,039
it's called sql and here's all these

00:27:05,679 --> 00:27:08,559
different ways of working with your data

00:27:07,039 --> 00:27:11,840
and clearing it and so on

00:27:08,559 --> 00:27:12,720
kafka says i am a distributed immutable

00:27:11,840 --> 00:27:15,520
commitment

00:27:12,720 --> 00:27:16,799
go and build cool things so kafka gives

00:27:15,520 --> 00:27:18,240
us fundamentals this

00:27:16,799 --> 00:27:20,320
this is great talk by martin patman

00:27:18,240 --> 00:27:22,320
about the database inside out

00:27:20,320 --> 00:27:24,080
and kafka gives us those fundamentals of

00:27:22,320 --> 00:27:24,720
a distributed connect log on which we

00:27:24,080 --> 00:27:27,279
can build

00:27:24,720 --> 00:27:28,880
super cool things so log base changes

00:27:27,279 --> 00:27:29,679
capture it gives us much greater

00:27:28,880 --> 00:27:31,600
fidelity

00:27:29,679 --> 00:27:33,039
on the data it's lower latency because

00:27:31,600 --> 00:27:33,919
we're going against a transaction log

00:27:33,039 --> 00:27:35,120
we're not pulling

00:27:33,919 --> 00:27:37,279
the database and because we're not

00:27:35,120 --> 00:27:39,120
pulling the database we've got a lower

00:27:37,279 --> 00:27:41,760
impact on the source

00:27:39,120 --> 00:27:43,679
it needs more work setting it up and you

00:27:41,760 --> 00:27:46,320
usually need to go and talk nicely to

00:27:43,679 --> 00:27:47,919
your dba team because you usually need

00:27:46,320 --> 00:27:48,960
enhanced privileges to do so because

00:27:47,919 --> 00:27:50,799
it's a lower level

00:27:48,960 --> 00:27:52,480
operation and then here's the bit that

00:27:50,799 --> 00:27:54,320
kind of like can stick in the throat

00:27:52,480 --> 00:27:55,679
depending on the database which you're

00:27:54,320 --> 00:27:57,679
querying from

00:27:55,679 --> 00:27:59,360
often there's costs associated with log

00:27:57,679 --> 00:28:02,880
based change data capture tools

00:27:59,360 --> 00:28:03,600
not always but sometimes so how do we

00:28:02,880 --> 00:28:06,000
decide

00:28:03,600 --> 00:28:06,799
which one we're going to use we can use

00:28:06,000 --> 00:28:09,039
if we're using

00:28:06,799 --> 00:28:10,320
query based change data capture then the

00:28:09,039 --> 00:28:13,440
kafka connect

00:28:10,320 --> 00:28:14,480
source connector is kind of the the de

00:28:13,440 --> 00:28:16,000
facto one

00:28:14,480 --> 00:28:17,679
um it's community licensed from

00:28:16,000 --> 00:28:20,000
confluence you can go and download that

00:28:17,679 --> 00:28:20,799
and you can use that if you want to use

00:28:20,000 --> 00:28:23,120
log based

00:28:20,799 --> 00:28:24,320
change data capture it all gets a little

00:28:23,120 --> 00:28:26,000
bit more murky

00:28:24,320 --> 00:28:27,440
there are commercial tools available

00:28:26,000 --> 00:28:29,440
there are some great open source tools

00:28:27,440 --> 00:28:31,520
available debesium is kind of like the

00:28:29,440 --> 00:28:32,799
leader of the pack by far it's amazing

00:28:31,520 --> 00:28:33,919
and they're developing loads of

00:28:32,799 --> 00:28:35,760
different connectors

00:28:33,919 --> 00:28:38,159
so there's postgres there's my sequel

00:28:35,760 --> 00:28:39,600
there's uh i saw gunner one of the

00:28:38,159 --> 00:28:40,880
authors of it's on the chat so he'll

00:28:39,600 --> 00:28:43,440
tell me which ones i've missed

00:28:40,880 --> 00:28:44,399
they've also got sql server there's uh

00:28:43,440 --> 00:28:46,320
oracle

00:28:44,399 --> 00:28:47,679
um and beta so there's loads of

00:28:46,320 --> 00:28:48,960
different connectors within it it's a

00:28:47,679 --> 00:28:50,559
very very good tool

00:28:48,960 --> 00:28:52,320
um let's say there are other proprietary

00:28:50,559 --> 00:28:52,799
ones oracle golden gate people have

00:28:52,320 --> 00:28:54,480
heard

00:28:52,799 --> 00:28:56,320
of um if only because it's kind of

00:28:54,480 --> 00:28:57,760
infamous but it's also very very good

00:28:56,320 --> 00:28:59,520
and what it does but there's loads of

00:28:57,760 --> 00:29:02,640
different options when it does come to

00:28:59,520 --> 00:29:05,039
log base change data capture

00:29:02,640 --> 00:29:06,320
so i think by my reckoning i've got 10

00:29:05,039 --> 00:29:07,440
minutes until the end of this i'd love

00:29:06,320 --> 00:29:09,360
to do some questions

00:29:07,440 --> 00:29:10,480
but i want to show you one more example

00:29:09,360 --> 00:29:13,279
of what we can do

00:29:10,480 --> 00:29:14,000
with this i showed you this slide

00:29:13,279 --> 00:29:15,440
earlier

00:29:14,000 --> 00:29:17,760
where i talked about this idea of

00:29:15,440 --> 00:29:20,320
enriching streams of data

00:29:17,760 --> 00:29:22,399
and i want to show you this because

00:29:20,320 --> 00:29:24,799
people sometimes think of kafka

00:29:22,399 --> 00:29:26,000
as just a dumb pipeline it's like it's a

00:29:24,799 --> 00:29:28,480
super scalable

00:29:26,000 --> 00:29:29,200
and flexible and brilliant pipeline but

00:29:28,480 --> 00:29:30,720
they just kind of

00:29:29,200 --> 00:29:32,320
set their horizons understanding of

00:29:30,720 --> 00:29:34,720
kafka as like well i can use it for

00:29:32,320 --> 00:29:37,279
shunting data from here to here

00:29:34,720 --> 00:29:38,240
kafka has built-in stream processing

00:29:37,279 --> 00:29:41,120
capabilities

00:29:38,240 --> 00:29:42,240
but kafka streams api is part of apache

00:29:41,120 --> 00:29:44,399
kafka

00:29:42,240 --> 00:29:46,480
so we can say here is a topic with an

00:29:44,399 --> 00:29:47,840
event like maybe some ratings or some

00:29:46,480 --> 00:29:49,360
orders were left

00:29:47,840 --> 00:29:51,200
and we can say well i would like to

00:29:49,360 --> 00:29:52,960
enrich that and push that

00:29:51,200 --> 00:29:54,399
downstream or make it available

00:29:52,960 --> 00:29:56,159
downstream i want to build

00:29:54,399 --> 00:29:57,679
a real-time dashboard i want to have an

00:29:56,159 --> 00:29:58,559
application that's got access to this

00:29:57,679 --> 00:30:00,320
enriched

00:29:58,559 --> 00:30:01,919
data rather than an application which

00:30:00,320 --> 00:30:03,679
has to consume the raw data and then do

00:30:01,919 --> 00:30:05,760
its own lookups out to a database and it

00:30:03,679 --> 00:30:06,960
gets messy and laggy and so on

00:30:05,760 --> 00:30:09,760
so we can actually say well let's pull

00:30:06,960 --> 00:30:11,200
the data from a database into kafka

00:30:09,760 --> 00:30:13,120
and then we can do our stream processing

00:30:11,200 --> 00:30:14,080
within this so i'm going to show you

00:30:13,120 --> 00:30:16,880
another demo

00:30:14,080 --> 00:30:18,240
in this case i don't code java so to

00:30:16,880 --> 00:30:20,240
those who do my apologies

00:30:18,240 --> 00:30:22,159
i'm going to need a tool called k sql db

00:30:20,240 --> 00:30:23,919
it's community licensed from confluence

00:30:22,159 --> 00:30:25,760
but it's an abstraction on top of kafka

00:30:23,919 --> 00:30:26,880
streams so i'm showing you you can do

00:30:25,760 --> 00:30:28,399
with kafka streams

00:30:26,880 --> 00:30:30,399
i'm going to cheat and use a sequel

00:30:28,399 --> 00:30:34,320
based interface though

00:30:30,399 --> 00:30:36,240
so again the examples for this are on

00:30:34,320 --> 00:30:37,840
the github repository again i'll share

00:30:36,240 --> 00:30:39,520
the links for it afterwards

00:30:37,840 --> 00:30:40,880
and what we're going to do here is if i

00:30:39,520 --> 00:30:44,640
close this window

00:30:40,880 --> 00:30:48,080
and come out of this one and

00:30:44,640 --> 00:30:50,080
we're going to say case equal h and this

00:30:48,080 --> 00:30:51,360
is going to bring us into a key sql db

00:30:50,080 --> 00:30:53,600
command prompt

00:30:51,360 --> 00:30:54,960
so key sequel db is the blurb says event

00:30:53,600 --> 00:30:57,360
streaming database blah blah blah

00:30:54,960 --> 00:30:58,799
it's built on top of kafka streams and

00:30:57,360 --> 00:31:00,080
it lets you express the streaming

00:30:58,799 --> 00:31:03,440
transformations that you want to do

00:31:00,080 --> 00:31:05,360
on your data using sql so to start off

00:31:03,440 --> 00:31:08,159
with we're going to create ourselves

00:31:05,360 --> 00:31:09,200
an object which sits on top of our

00:31:08,159 --> 00:31:10,720
stream of data

00:31:09,200 --> 00:31:12,640
that we got through from the beginning

00:31:10,720 --> 00:31:14,880
so we're taking this kafka topic here

00:31:12,640 --> 00:31:15,679
it's in avro because it's in afro we

00:31:14,880 --> 00:31:17,039
have the schema

00:31:15,679 --> 00:31:18,640
okay remember how to talk about how

00:31:17,039 --> 00:31:20,159
schemes are super important they're

00:31:18,640 --> 00:31:20,799
super important to people who want to

00:31:20,159 --> 00:31:22,399
use

00:31:20,799 --> 00:31:23,919
the data and presumably we're not just

00:31:22,399 --> 00:31:26,799
collecting data for the heck of it

00:31:23,919 --> 00:31:27,919
we want to use the data so i'm a user i

00:31:26,799 --> 00:31:29,200
come along i've got

00:31:27,919 --> 00:31:31,679
appropriate permissions to access the

00:31:29,200 --> 00:31:34,159
topic i say i'm about to create a stream

00:31:31,679 --> 00:31:34,799
against this topic so casey called eb

00:31:34,159 --> 00:31:37,840
says well

00:31:34,799 --> 00:31:39,279
i can do that and i have the schema

00:31:37,840 --> 00:31:40,960
because you use avro

00:31:39,279 --> 00:31:42,960
so i can go to scheme registry pull down

00:31:40,960 --> 00:31:44,320
the schema same with protobuf same with

00:31:42,960 --> 00:31:46,799
json schema

00:31:44,320 --> 00:31:48,399
and here we can see here is my schema

00:31:46,799 --> 00:31:50,399
we've got the before and after which is

00:31:48,399 --> 00:31:52,320
a struct a nested object

00:31:50,399 --> 00:31:53,600
with a particular payload within it

00:31:52,320 --> 00:31:54,559
we've got information about the source

00:31:53,600 --> 00:31:55,919
we've got information about the

00:31:54,559 --> 00:31:57,279
operation

00:31:55,919 --> 00:32:00,720
and now that we have a schema on top of

00:31:57,279 --> 00:32:03,600
the data we can query the data sorry

00:32:00,720 --> 00:32:05,279
select start from the customer's cdc

00:32:03,600 --> 00:32:06,880
stream

00:32:05,279 --> 00:32:08,480
and make changes show me everything

00:32:06,880 --> 00:32:09,120
that's happening in that topic and it

00:32:08,480 --> 00:32:10,960
says okay

00:32:09,120 --> 00:32:12,159
here's the contents of us because it's a

00:32:10,960 --> 00:32:14,240
nested thing that's

00:32:12,159 --> 00:32:16,080
kind of not so useful to look at so

00:32:14,240 --> 00:32:19,200
let's say let's select

00:32:16,080 --> 00:32:22,720
the operation and the after uh

00:32:19,200 --> 00:32:23,919
first name okay there's it was a create

00:32:22,720 --> 00:32:25,120
and there was an update and it was a

00:32:23,919 --> 00:32:27,039
delete

00:32:25,120 --> 00:32:29,039
but because we have the before and after

00:32:27,039 --> 00:32:32,080
we could actually say before

00:32:29,039 --> 00:32:32,559
first name and here we can see this one

00:32:32,080 --> 00:32:35,360
here this

00:32:32,559 --> 00:32:35,840
update actually changed the first name

00:32:35,360 --> 00:32:37,760
so you can see

00:32:35,840 --> 00:32:39,440
before after you get the delta between

00:32:37,760 --> 00:32:41,279
it this one here the deletion

00:32:39,440 --> 00:32:43,279
it was this and now it's null why is it

00:32:41,279 --> 00:32:43,760
null because we deleted it it doesn't

00:32:43,279 --> 00:32:46,000
exist

00:32:43,760 --> 00:32:47,039
anymore but because we're using log base

00:32:46,000 --> 00:32:49,919
change data capture

00:32:47,039 --> 00:32:50,880
we actually capture the before record we

00:32:49,919 --> 00:32:52,799
can do another thing

00:32:50,880 --> 00:32:54,320
using kafka streams and in this case

00:32:52,799 --> 00:32:57,039
case equal db

00:32:54,320 --> 00:32:57,919
we can actually materialize a view of

00:32:57,039 --> 00:32:59,039
the state

00:32:57,919 --> 00:33:01,360
so you know i've talked at the beginning

00:32:59,039 --> 00:33:04,799
about how databases are actually

00:33:01,360 --> 00:33:05,679
based on events well kafka gives you the

00:33:04,799 --> 00:33:08,399
commit log

00:33:05,679 --> 00:33:09,840
and then using kafka streams or kc or db

00:33:08,399 --> 00:33:11,360
you can actually take those events and

00:33:09,840 --> 00:33:13,440
replay them and rebuild

00:33:11,360 --> 00:33:15,279
the state so here i'm going to say

00:33:13,440 --> 00:33:18,320
create myself a table

00:33:15,279 --> 00:33:19,919
within k sqldb called customers and it's

00:33:18,320 --> 00:33:23,760
going to hold the current state

00:33:19,919 --> 00:33:25,760
of our customers so if i create that

00:33:23,760 --> 00:33:26,799
and then i open up another tab here

00:33:25,760 --> 00:33:30,880
which is going to be for

00:33:26,799 --> 00:33:35,840
my sequel let me say select

00:33:30,880 --> 00:33:37,440
these from the table so select

00:33:35,840 --> 00:33:39,679
select that from customers what's the

00:33:37,440 --> 00:33:42,320
current state in our customers table of

00:33:39,679 --> 00:33:44,960
id 42 i'm going to go and do the same

00:33:42,320 --> 00:33:47,200
thing in case sequel bb

00:33:44,960 --> 00:33:49,279
this is well there is your current state

00:33:47,200 --> 00:33:50,159
it's got id 42 that's the first name and

00:33:49,279 --> 00:33:52,240
the last name

00:33:50,159 --> 00:33:54,080
and the email let's go to mysql and

00:33:52,240 --> 00:33:56,240
let's say well i'm sure it's not called

00:33:54,080 --> 00:33:57,600
uh bob i'm pretty sure it's called rick

00:33:56,240 --> 00:33:58,640
so let's go along here and say well

00:33:57,600 --> 00:34:01,039
let's fix that

00:33:58,640 --> 00:34:02,320
in the source database so i'm using sql

00:34:01,039 --> 00:34:04,480
but any application to

00:34:02,320 --> 00:34:06,320
write into my sql database right it's

00:34:04,480 --> 00:34:08,079
the transaction log it goes over to our

00:34:06,320 --> 00:34:10,560
catholic topic our kafka topic

00:34:08,079 --> 00:34:12,560
is read by our stream processor kafka

00:34:10,560 --> 00:34:14,399
streams in this case k sequel db

00:34:12,560 --> 00:34:16,000
we say what's the current state of that

00:34:14,399 --> 00:34:19,200
table it says well current state

00:34:16,000 --> 00:34:20,960
is now this but because we're using a

00:34:19,200 --> 00:34:21,520
kafka topic which captures all of our

00:34:20,960 --> 00:34:23,040
events

00:34:21,520 --> 00:34:24,560
we can rebuild the state into a

00:34:23,040 --> 00:34:25,520
materialized view like this that we can

00:34:24,560 --> 00:34:27,919
query

00:34:25,520 --> 00:34:29,839
but we can also do this let me show you

00:34:27,919 --> 00:34:32,159
this

00:34:29,839 --> 00:34:33,119
i can say will show me those stream of

00:34:32,159 --> 00:34:35,839
changes

00:34:33,119 --> 00:34:37,200
so id42 first off they got created and

00:34:35,839 --> 00:34:38,720
the email address was this and then we

00:34:37,200 --> 00:34:39,359
made an update and the email address was

00:34:38,720 --> 00:34:40,800
this

00:34:39,359 --> 00:34:42,560
and then we made an update and the first

00:34:40,800 --> 00:34:43,119
name changed and then we made another

00:34:42,560 --> 00:34:45,919
update

00:34:43,119 --> 00:34:47,359
and it was like this so by capturing

00:34:45,919 --> 00:34:49,119
your stream of events

00:34:47,359 --> 00:34:51,040
you can actually do both things you can

00:34:49,119 --> 00:34:52,720
have an audit of all of the things that

00:34:51,040 --> 00:34:53,440
happened and you can go back and replay

00:34:52,720 --> 00:34:54,800
those and

00:34:53,440 --> 00:34:56,159
how many times did they change email

00:34:54,800 --> 00:34:57,680
address when was the last time they

00:34:56,159 --> 00:34:59,520
moved house and so on

00:34:57,680 --> 00:35:01,200
but you can also say i would like to

00:34:59,520 --> 00:35:03,119
query that and what the current state

00:35:01,200 --> 00:35:04,960
is you can use that current state to

00:35:03,119 --> 00:35:06,240
join to streams and so on

00:35:04,960 --> 00:35:08,000
i'm saying that i've remembered the

00:35:06,240 --> 00:35:10,800
point of that then i was supposed to be

00:35:08,000 --> 00:35:12,240
joining it so let me briefly show you

00:35:10,800 --> 00:35:14,160
that because i got carried away with

00:35:12,240 --> 00:35:16,720
materializing my state

00:35:14,160 --> 00:35:17,760
so here we're going to create ourselves

00:35:16,720 --> 00:35:19,839
a stream

00:35:17,760 --> 00:35:21,280
of ratings so like people leave ratings

00:35:19,839 --> 00:35:22,320
something was good it was bad against

00:35:21,280 --> 00:35:24,000
the topic

00:35:22,320 --> 00:35:26,160
and we can query that and ensures it's

00:35:24,000 --> 00:35:27,760
working and it's got a live feed of data

00:35:26,160 --> 00:35:28,400
people saying something was good it was

00:35:27,760 --> 00:35:30,720
bad

00:35:28,400 --> 00:35:32,320
and it's got a user id field so we're

00:35:30,720 --> 00:35:34,560
going to join this stream of events

00:35:32,320 --> 00:35:36,560
that's being produced by a producer api

00:35:34,560 --> 00:35:39,200
somewhere and we're going to create

00:35:36,560 --> 00:35:41,359
ourselves a stream

00:35:39,200 --> 00:35:43,119
of enriched ratings so we're going to do

00:35:41,359 --> 00:35:45,440
a join this is just using

00:35:43,119 --> 00:35:47,040
create a stream as the results of this

00:35:45,440 --> 00:35:49,520
select statement so select

00:35:47,040 --> 00:35:50,400
the rating and information about our

00:35:49,520 --> 00:35:52,240
customer

00:35:50,400 --> 00:35:54,560
from ratings in adjoined customers in

00:35:52,240 --> 00:35:56,800
the joint customers table

00:35:54,560 --> 00:35:58,480
so we go and create that and we can say

00:35:56,800 --> 00:36:03,680
select start from

00:35:58,480 --> 00:36:03,680
ratings enriched changes

00:36:04,160 --> 00:36:08,320
it says there you go there is your

00:36:06,160 --> 00:36:10,560
ratings as they come in here is the user

00:36:08,320 --> 00:36:12,480
who left that rating and the star rating

00:36:10,560 --> 00:36:13,599
and so on and so on and all of this

00:36:12,480 --> 00:36:15,920
under the covers

00:36:13,599 --> 00:36:17,280
is actually just a kafka topic so we say

00:36:15,920 --> 00:36:20,160
print this topic just

00:36:17,280 --> 00:36:21,839
act as a consumer against that topic it

00:36:20,160 --> 00:36:23,040
says here is let's put that here is

00:36:21,839 --> 00:36:26,880
that's topic

00:36:23,040 --> 00:36:30,480
and if i unpause it so 1651 utc we're on

00:36:26,880 --> 00:36:33,680
um bst over here still so an hour ahead

00:36:30,480 --> 00:36:35,119
this is a live stream of events arriving

00:36:33,680 --> 00:36:36,800
on a topic

00:36:35,119 --> 00:36:38,640
going through that select doing the join

00:36:36,800 --> 00:36:40,800
out to current state of customers

00:36:38,640 --> 00:36:42,000
and writing down to a new kafka topic so

00:36:40,800 --> 00:36:43,920
we could take that kafka topic and

00:36:42,000 --> 00:36:45,760
consume it with an application

00:36:43,920 --> 00:36:46,720
we could take that kafka topic and push

00:36:45,760 --> 00:36:48,320
it down somewhere else like

00:36:46,720 --> 00:36:51,440
elasticsearch or whatever else

00:36:48,320 --> 00:36:51,440
using kafka connect

00:36:52,560 --> 00:36:55,839
so i hope that's been useful i hope that

00:36:55,200 --> 00:36:57,839
kind of

00:36:55,839 --> 00:37:00,240
filled out some of the gaps in like the

00:36:57,839 --> 00:37:00,800
different types of cdc and how do you

00:37:00,240 --> 00:37:03,040
choose

00:37:00,800 --> 00:37:04,720
which type of cdc i'll put all of the

00:37:03,040 --> 00:37:05,680
slides for this on the chats straight

00:37:04,720 --> 00:37:07,680
after this

00:37:05,680 --> 00:37:09,119
and you can follow me on twitter at i'm

00:37:07,680 --> 00:37:10,480
off i'll be sharing the slides and all

00:37:09,119 --> 00:37:12,320
the resources there

00:37:10,480 --> 00:37:13,520
you can get a great set of books uh

00:37:12,320 --> 00:37:16,160
download them from that

00:37:13,520 --> 00:37:18,240
link for free scan the qr code all about

00:37:16,160 --> 00:37:19,760
kafka about the principles of logs and

00:37:18,240 --> 00:37:21,440
why they're so important

00:37:19,760 --> 00:37:24,000
if you want to learn more kafka go along

00:37:21,440 --> 00:37:25,760
to developer.confluent.io you'll find a

00:37:24,000 --> 00:37:27,359
bunch of tutorials and podcasts and

00:37:25,760 --> 00:37:28,880
videos and so on back

00:37:27,359 --> 00:37:30,000
i'll share these links afterwards just

00:37:28,880 --> 00:37:30,880
other talks that i've done if you want

00:37:30,000 --> 00:37:33,119
to learn more about

00:37:30,880 --> 00:37:34,560
kafka connect or things about key sql db

00:37:33,119 --> 00:37:36,960
or stream processing and

00:37:34,560 --> 00:37:38,400
all that kind of stuff there with that

00:37:36,960 --> 00:37:39,440
thank you very much

00:37:38,400 --> 00:37:42,160
and i'll head over and see if they've

00:37:39,440 --> 00:37:42,160
been any questions

00:37:43,359 --> 00:37:46,640
marta says do you have any uh idea of

00:37:45,119 --> 00:37:48,079
how to deal with rollbacks when using

00:37:46,640 --> 00:37:50,240
log base change rate capture

00:37:48,079 --> 00:37:51,760
no i have no idea i'd be delighted for

00:37:50,240 --> 00:37:52,800
your input if anyone else has got

00:37:51,760 --> 00:37:54,800
suggestions

00:37:52,800 --> 00:37:56,400
um i just built these like hello world

00:37:54,800 --> 00:37:57,119
demos which kind of good enough to get

00:37:56,400 --> 00:37:58,640
you to there

00:37:57,119 --> 00:38:00,480
and then only like the nitty gritty like

00:37:58,640 --> 00:38:02,079
devil in the detail of using it for real

00:38:00,480 --> 00:38:04,320
that's what other people can figure out

00:38:02,079 --> 00:38:06,000
um i've been slightly flippant um

00:38:04,320 --> 00:38:07,440
but in this case here i've not actually

00:38:06,000 --> 00:38:08,720
gone into that scenario but it's

00:38:07,440 --> 00:38:09,359
obviously a super important one because

00:38:08,720 --> 00:38:10,720
then you've got

00:38:09,359 --> 00:38:13,200
uh consistency and things like that to

00:38:10,720 --> 00:38:17,119
worry about

00:38:13,200 --> 00:38:20,480
um okay thanks you've answered that one

00:38:17,119 --> 00:38:21,920
um do you have any tips how to handle

00:38:20,480 --> 00:38:23,599
the right to be forgotten requirements

00:38:21,920 --> 00:38:24,800
when data needs to be deleted from kafka

00:38:23,599 --> 00:38:25,680
topic that captures the data from

00:38:24,800 --> 00:38:28,720
database

00:38:25,680 --> 00:38:29,440
yes so right to be forgotten gdpr over

00:38:28,720 --> 00:38:30,720
here

00:38:29,440 --> 00:38:33,359
all sorts of different legislation

00:38:30,720 --> 00:38:35,359
saying well this fantastic immutable

00:38:33,359 --> 00:38:36,960
uh commit log of events that you've got

00:38:35,359 --> 00:38:38,240
you can kind of go back in time and

00:38:36,960 --> 00:38:39,839
replay these things

00:38:38,240 --> 00:38:41,040
that thing that's immutable it now needs

00:38:39,839 --> 00:38:42,480
to be mutable you need to get rid of

00:38:41,040 --> 00:38:44,800
that particular thing there

00:38:42,480 --> 00:38:45,599
there's different approaches kafka

00:38:44,800 --> 00:38:47,440
topics

00:38:45,599 --> 00:38:48,880
have a retention policy that you can set

00:38:47,440 --> 00:38:52,160
to based on time

00:38:48,880 --> 00:38:55,119
or size like um size on desk

00:38:52,160 --> 00:38:56,800
or something called compaction um so you

00:38:55,119 --> 00:38:58,880
can also say to retain it forever

00:38:56,800 --> 00:39:00,960
and that's commonly used with compaction

00:38:58,880 --> 00:39:02,640
where you say for every single key

00:39:00,960 --> 00:39:04,880
i would like to keep the latest value of

00:39:02,640 --> 00:39:06,800
that key and if you send a null

00:39:04,880 --> 00:39:08,880
value for that key it acts as a

00:39:06,800 --> 00:39:10,400
tombstone and it gets deleted

00:39:08,880 --> 00:39:12,160
so if you're using compacted topics

00:39:10,400 --> 00:39:14,160
that's one approach you send a null

00:39:12,160 --> 00:39:15,680
for that particular user and thus they

00:39:14,160 --> 00:39:16,640
get forgotten when the compaction next

00:39:15,680 --> 00:39:19,920
runs

00:39:16,640 --> 00:39:21,200
another option is to use um crypto

00:39:19,920 --> 00:39:23,200
shredding i think is the term

00:39:21,200 --> 00:39:25,040
where you basically use keys associated

00:39:23,200 --> 00:39:26,320
with each user and if that user wants

00:39:25,040 --> 00:39:28,560
those data to be lost

00:39:26,320 --> 00:39:29,839
you basically ditch those those um

00:39:28,560 --> 00:39:31,680
encryption keys

00:39:29,839 --> 00:39:33,599
um which means you can never decrypt it

00:39:31,680 --> 00:39:35,359
unless it's logically forgotten so those

00:39:33,599 --> 00:39:37,200
are the two kind of approaches

00:39:35,359 --> 00:39:39,119
kafka summit has had some really good

00:39:37,200 --> 00:39:40,400
talks about that particular subject

00:39:39,119 --> 00:39:42,320
so go and check out kafka summit

00:39:40,400 --> 00:39:43,440
recordings you find them on google and

00:39:42,320 --> 00:39:44,079
there's a bunch of really good talks

00:39:43,440 --> 00:39:47,200
around

00:39:44,079 --> 00:39:48,880
that kind of specific thing okay i think

00:39:47,200 --> 00:39:49,599
i'm pretty much at time so i'll turn off

00:39:48,880 --> 00:39:51,040
my camera

00:39:49,599 --> 00:39:52,320
and whatnot now and hand over to the

00:39:51,040 --> 00:39:52,800
next person but i'll stay on the chat

00:39:52,320 --> 00:39:54,560
room

00:39:52,800 --> 00:39:56,160
uh i'll be on the streaming slack

00:39:54,560 --> 00:39:57,680
channel as well and

00:39:56,160 --> 00:39:59,359
also you can always find me on twitter

00:39:57,680 --> 00:40:00,960
shameless plug head over to my youtube

00:39:59,359 --> 00:40:02,720
channel as well and subscribe there

00:40:00,960 --> 00:40:04,400
there's a ton of kafka connect talks in

00:40:02,720 --> 00:40:11,839
particular so thank you very much and

00:40:04,400 --> 00:40:11,839
have a great day everyone

00:51:52,160 --> 00:51:54,240

YouTube URL: https://www.youtube.com/watch?v=8rv-8UQ7BfI


