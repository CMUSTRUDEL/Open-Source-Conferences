Title: Story of moving our 4 Trillion Event Log Pipeline from Batch to Streaming
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Story of moving our 4 Trillion Event Log Pipeline from Batch to Streaming
Lohit VijayaRenu, Zhenzhao Wang, Praveen Killamsetti

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Twitter's LogPipeline handle more than 4Trillion events per day. This complex pipeline has evolved over the years to support Twitter's scale of data. This pipeline is designed to be resilient, support high throughput and use resources efficiently. Because of its legacy architecture, it was still batch pipeline at scale. For some time, our team has been redesigning this to support streaming use cases and have done significant architecture changes for this pipeline In this talk we deep dive into our old architecture, highlight pros and cons of that and describe how we are making changes for it to be more streaming friendly. We talk about various open source projects such as Apache Hadoop, Apache Flume, Apache Tez, Apache Beam and cloud technologies which tie together to form our large scale event LogPipeline.

Lohit VijayaRenu:
Lohit is part of DataPlatform team at Twitter. He concentrates on projects around storage, compute and log pipeline for Twitter scale both on premise and cloud. He has worked at several startups before joining Twitter. He has a Masters degree in Computer Science from Stony Brook University.
Zhenzhao Wang:
Zhenzhao works at Twitter as part of Hadoop and Log Management team. He is currently concentrating on Twitter Log Ingestion Pipeline which scales to handle trillions of events per day. Previously he was a member of DFS(Pangu) team in Alibaba Cloud where he focused on feature for random file access file in Pangu used as storage for Virtual Machines. He has Bachelor's degree from Nankai University and Master's degree from Tsinghua University.
Praveen Killamsetti
Praveen works at Twitter as part of the DataPlatform organization. In his current role, he is working on scaling the log ingestion pipeline to trillions of events in the streaming model and building a data set lifecycle management system for analytical data sets. He has a master degree in computer science from IIT Madras. Before joining Twitter, Praveen worked on building distributed storage systems at Nimble Storage, NetApp and built various products including Synchronous Replication across multiple data centers with automatic failover, Write Optimized KV stores, Dedupe and Compression stack, Efficient Cloning features, Archiving Storage Snapshots to S3 efficiently etc.
YouTube URL: https://www.youtube.com/watch?v=jOHWAuGoVp4


