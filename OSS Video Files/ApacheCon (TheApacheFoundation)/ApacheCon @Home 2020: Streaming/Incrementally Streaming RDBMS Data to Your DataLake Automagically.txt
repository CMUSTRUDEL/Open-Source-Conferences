Title: Incrementally Streaming RDBMS Data to Your DataLake Automagically
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Incrementally Streaming RDBMS Data to Your DataLake Automagically
Timothy Spann

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

There is often data locked in transactional relational systems that you would like to ingest, transform, parse, aggregate, and store forever in Hadoop as wide tables. With the new features in Apache NiFi, Cloudera Schema Registry, HBase 2, Phoenix, Hive 3, Kudu, Spark 2, Kafka, Ranger, Atlas, Zeppelin and Hue this becomes something you can do at scale without the heavy hand processing of yore. Now with the hybrid cloud, you may want to securely ingest to multiple clusters with new tools including Streams Replication Manager. They told me it's not ETL or ELT, exactly it is so much more. You now have full control over global data assets with full management, full control and smart dashboards to allow a true enterprise open source solution for all your data. With materialized views and the ability to federate queries to JDBC and other data sources your fully ACID Hive 3 tables allow for you to escape the small scale EDW and be reborn in unlimited scale data worlds. References: https://community.cloudera.com/t5/Community-Articles/ETL-With-Lookups-with-Apache-HBase-and-Apache-NiFi/ta-p/248243 https://community.cloudera.com/t5/Community-Articles/Ingesting-RDBMS-Data-As-New-Tables-Arrive-Automagically-into/ta-p/246214 https://community.cloudera.com/t5/Community-Articles/Incrementally-Streaming-RDBMS-Data-to-Your-Hadoop-DataLake/ta-p/247927 https://community.cloudera.com/t5/Community-Articles/Ingesting-Golden-Gate-Records-From-Apache-Kafka-and/ta-p/247557 https://www.datainmotion.dev/2020/05/cloudera-flow-management-101-lets-build.html

Tim Spann is a Principal DataFlow Field Engineer at Cloudera, the Big Data Zone leader and blogger at DZone and an experienced data engineer with 15 years of experience. He runs the Future of Data Princeton meetup as well as other events. He has spoken at Philly Open SOurce, ApacheCon in Montreal, Strata NYC, Oracle Code NYC, IoT Fusion in Philly, meetups in Princeton, NYC, Philly, Berlin and Prague, DataWorks Summits in San Jose, Berlin and Sydney.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,240 --> 00:00:30,560
my clockwork

00:00:26,720 --> 00:00:32,880
appropriate time so this is me i'm at a

00:00:30,560 --> 00:00:35,200
cool house here

00:00:32,880 --> 00:00:36,640
so i put the agenda together we're

00:00:35,200 --> 00:00:39,520
flexible so if anyone

00:00:36,640 --> 00:00:40,879
chats anything or it's i don't know if

00:00:39,520 --> 00:00:42,480
people are allowed to talk i'm not sure

00:00:40,879 --> 00:00:45,200
how that goes but

00:00:42,480 --> 00:00:45,760
we're we do a meet up together so we're

00:00:45,200 --> 00:00:49,360
very

00:00:45,760 --> 00:00:52,320
uh interactive so

00:00:49,360 --> 00:00:53,600
feel free to interrupt us you know this

00:00:52,320 --> 00:00:56,000
is uh

00:00:53,600 --> 00:00:58,160
time for everyone to learn about the

00:00:56,000 --> 00:01:00,719
projects how they work together and a

00:00:58,160 --> 00:01:04,080
really cool use case of how you can

00:01:00,719 --> 00:01:07,680
very easily move data from

00:01:04,080 --> 00:01:11,360
any relational database and push it into

00:01:07,680 --> 00:01:14,880
hdfs push it into kudu push it into hive

00:01:11,360 --> 00:01:16,640
really straightforward we run the future

00:01:14,880 --> 00:01:19,680
data meetup in princeton

00:01:16,640 --> 00:01:23,439
and we work with philly in new york

00:01:19,680 --> 00:01:25,600
and boston and another one in providence

00:01:23,439 --> 00:01:26,479
but tonight we're doing a meet up on

00:01:25,600 --> 00:01:29,360
some of the

00:01:26,479 --> 00:01:31,360
streaming uh apache projects it's gonna

00:01:29,360 --> 00:01:34,000
be pretty much an open forum

00:01:31,360 --> 00:01:35,759
a couple people discussing things we'll

00:01:34,000 --> 00:01:39,520
dive deeper into this demo

00:01:35,759 --> 00:01:41,600
some other demos give people an idea

00:01:39,520 --> 00:01:42,880
how to work with the uh projects

00:01:41,600 --> 00:01:45,600
together

00:01:42,880 --> 00:01:47,600
and you know share out all the uh source

00:01:45,600 --> 00:01:50,799
codes you could do it yourself

00:01:47,600 --> 00:01:53,040
these are some of the projects we cover

00:01:50,799 --> 00:01:55,439
uh we can post the link in there i post

00:01:53,040 --> 00:01:58,799
it in the general event section

00:01:55,439 --> 00:01:58,799
for the meetup tonight

00:02:04,079 --> 00:02:07,439
so it looks like tim froze a little bit

00:02:05,680 --> 00:02:09,039
so this is me uh my name is john

00:02:07,439 --> 00:02:10,640
couchmack i'm actually a senior

00:02:09,039 --> 00:02:13,840
solutions engineer

00:02:10,640 --> 00:02:15,520
out of south jersey

00:02:13,840 --> 00:02:17,599
tim you're you're froze you're frozen

00:02:15,520 --> 00:02:22,080
little buddy so did i

00:02:17,599 --> 00:02:25,120
yeah am i okay now yeah you're good

00:02:22,080 --> 00:02:26,800
i shouldn't uh street uh do myself twice

00:02:25,120 --> 00:02:29,280
here we'll see if that slows me down

00:02:26,800 --> 00:02:32,560
we'll get rid of that

00:02:29,280 --> 00:02:34,640
just let me know so kudu

00:02:32,560 --> 00:02:36,080
will walk through that it's a pretty

00:02:34,640 --> 00:02:38,000
common use case sometimes

00:02:36,080 --> 00:02:39,280
you don't want to just grab it from a

00:02:38,000 --> 00:02:40,640
database

00:02:39,280 --> 00:02:43,599
sometimes you're going to grab it from

00:02:40,640 --> 00:02:44,800
files very easy to take whatever kind of

00:02:43,599 --> 00:02:48,239
file it is

00:02:44,800 --> 00:02:50,879
do whatever processing with apache nifi

00:02:48,239 --> 00:02:53,120
maybe send it through kafka got a couple

00:02:50,879 --> 00:02:55,519
of ways to do that

00:02:53,120 --> 00:02:56,239
it's very straightforward all you really

00:02:55,519 --> 00:02:59,360
need to

00:02:56,239 --> 00:03:01,280
have for these uh services is

00:02:59,360 --> 00:03:04,000
the basic knowledge of where your data

00:03:01,280 --> 00:03:06,480
is you know what tables you want

00:03:04,000 --> 00:03:07,360
those sort of things i've got an article

00:03:06,480 --> 00:03:10,480
here

00:03:07,360 --> 00:03:12,400
grabbing stuff from a maria database and

00:03:10,480 --> 00:03:14,159
pushing into kudu

00:03:12,400 --> 00:03:16,239
it's very straightforward with the

00:03:14,159 --> 00:03:19,120
newest versions of nifi

00:03:16,239 --> 00:03:21,360
make it really easy to work with records

00:03:19,120 --> 00:03:23,280
but i think that's uh

00:03:21,360 --> 00:03:24,480
makes things easy just get through a

00:03:23,280 --> 00:03:27,120
couple of these

00:03:24,480 --> 00:03:29,040
uh one thing that's helpful and very

00:03:27,120 --> 00:03:30,879
thankful to another apache project

00:03:29,040 --> 00:03:34,480
called apache calcite

00:03:30,879 --> 00:03:35,680
is within nifi now we can automatically

00:03:34,480 --> 00:03:38,799
query

00:03:35,680 --> 00:03:39,519
events and data as it's in stream this

00:03:38,799 --> 00:03:43,120
comes in

00:03:39,519 --> 00:03:45,360
really nice when you want to

00:03:43,120 --> 00:03:47,519
say you're getting events in but you

00:03:45,360 --> 00:03:48,400
only want a subset of them that you want

00:03:47,519 --> 00:03:50,720
to deal with

00:03:48,400 --> 00:03:52,080
or perhaps you want to route your data

00:03:50,720 --> 00:03:53,120
something we could have added to the

00:03:52,080 --> 00:03:56,560
demo

00:03:53,120 --> 00:04:00,080
if it's an even record send it to kudu

00:03:56,560 --> 00:04:02,000
if it's an odd record send it to hive

00:04:00,080 --> 00:04:04,000
you know that's we would just add two

00:04:02,000 --> 00:04:06,159
sql queries here to do that

00:04:04,000 --> 00:04:07,040
and that would happen in the event

00:04:06,159 --> 00:04:09,360
stream

00:04:07,040 --> 00:04:10,159
doesn't have to land in a storage place

00:04:09,360 --> 00:04:12,239
first

00:04:10,159 --> 00:04:13,439
and it doesn't matter if it's json

00:04:12,239 --> 00:04:17,440
events

00:04:13,439 --> 00:04:19,280
avro csv xml

00:04:17,440 --> 00:04:22,079
a couple different types there makes it

00:04:19,280 --> 00:04:24,000
uh really nice you could also do it

00:04:22,079 --> 00:04:26,320
with something you can analyze like a

00:04:24,000 --> 00:04:28,720
log

00:04:26,320 --> 00:04:30,400
that comes in handy uh these slides will

00:04:28,720 --> 00:04:32,400
be available

00:04:30,400 --> 00:04:33,680
uh through apache i'll also make sure

00:04:32,400 --> 00:04:36,400
that they're

00:04:33,680 --> 00:04:37,600
shared through either slideshare or

00:04:36,400 --> 00:04:40,000
through github

00:04:37,600 --> 00:04:41,280
so everyone can get the links to all the

00:04:40,000 --> 00:04:43,600
articles and

00:04:41,280 --> 00:04:45,919
all the details if you're interested on

00:04:43,600 --> 00:04:48,080
how they might want to do this

00:04:45,919 --> 00:04:49,759
i've got some details here here i pulled

00:04:48,080 --> 00:04:52,720
from

00:04:49,759 --> 00:04:54,479
a sql server database you list to what

00:04:52,720 --> 00:04:56,639
the server is

00:04:54,479 --> 00:04:58,240
you know if you've done any kind of jdbc

00:04:56,639 --> 00:04:59,520
programming it's going to look really

00:04:58,240 --> 00:05:01,440
familiar

00:04:59,520 --> 00:05:03,520
this will just uh set up those

00:05:01,440 --> 00:05:06,400
connection pools for you

00:05:03,520 --> 00:05:09,199
you know you select your record output

00:05:06,400 --> 00:05:11,199
in what format you want

00:05:09,199 --> 00:05:13,759
you know if later you want to change it

00:05:11,199 --> 00:05:16,000
to something else very easy to do

00:05:13,759 --> 00:05:16,880
the query record processor makes that

00:05:16,000 --> 00:05:20,240
very uh

00:05:16,880 --> 00:05:22,960
straightforward and again there's a lot

00:05:20,240 --> 00:05:26,000
of different ways we can easily

00:05:22,960 --> 00:05:26,400
ingest these database tables john has

00:05:26,000 --> 00:05:29,680
come

00:05:26,400 --> 00:05:31,919
up with uh what i think is a really

00:05:29,680 --> 00:05:34,960
awesome way to do that

00:05:31,919 --> 00:05:35,759
so let me stop to share here this demo

00:05:34,960 --> 00:05:38,240
has

00:05:35,759 --> 00:05:39,360
to uh a real i think is the cleanest way

00:05:38,240 --> 00:05:42,160
to do it

00:05:39,360 --> 00:05:44,240
and seems to be pretty performance that

00:05:42,160 --> 00:05:45,120
seems like a good paradigm if you want

00:05:44,240 --> 00:05:47,520
to

00:05:45,120 --> 00:05:50,000
walk through that demo oh we got our

00:05:47,520 --> 00:05:52,960
buddy abdulfriend from druid here

00:05:50,000 --> 00:05:55,840
which is cool in five minutes we'll have

00:05:52,960 --> 00:05:59,120
knife i connect to druid

00:05:55,840 --> 00:06:01,039
for him we do this interactive demo

00:05:59,120 --> 00:06:02,639
send it over the pipe through kafka to

00:06:01,039 --> 00:06:06,800
him

00:06:02,639 --> 00:06:08,479
no pressure so um so what i set up here

00:06:06,800 --> 00:06:10,720
is i mean it's it's pretty simple

00:06:08,479 --> 00:06:14,319
actually the overall process and

00:06:10,720 --> 00:06:16,479
uh i decided to use an oracle uh

00:06:14,319 --> 00:06:18,400
database rather than using like postgres

00:06:16,479 --> 00:06:18,960
or mysql where you see a lot of that

00:06:18,400 --> 00:06:22,319
online

00:06:18,960 --> 00:06:24,560
especially pulling out a data with nifi

00:06:22,319 --> 00:06:26,400
i created a believe it or not i use nifi

00:06:24,560 --> 00:06:29,440
to generate data as well

00:06:26,400 --> 00:06:33,280
so i'll go through that real quick

00:06:29,440 --> 00:06:35,120
there is an api where it has random

00:06:33,280 --> 00:06:36,160
users so i actually can get a list of

00:06:35,120 --> 00:06:39,360
people's names

00:06:36,160 --> 00:06:42,240
um it's pretty cool because it's pretty

00:06:39,360 --> 00:06:43,600
global like you'll see names in english

00:06:42,240 --> 00:06:46,160
as well as other languages

00:06:43,600 --> 00:06:48,800
so you'll get all that just doing a get

00:06:46,160 --> 00:06:51,840
command from there uh

00:06:48,800 --> 00:06:53,599
i'm pulling the json out and storing it

00:06:51,840 --> 00:06:56,240
as attributes

00:06:53,599 --> 00:06:57,919
and then i'm actually using the internal

00:06:56,240 --> 00:06:59,280
expression language of nifi which is

00:06:57,919 --> 00:07:02,000
pretty cool right so

00:06:59,280 --> 00:07:03,440
i want to generate a key value i wanted

00:07:02,000 --> 00:07:06,880
to generate

00:07:03,440 --> 00:07:10,080
um like a decimal uh you know and

00:07:06,880 --> 00:07:12,160
essentially i add the schema name and

00:07:10,080 --> 00:07:12,639
also i wanted to get a time stamp right

00:07:12,160 --> 00:07:15,039
so

00:07:12,639 --> 00:07:17,759
to actually get when the data is being

00:07:15,039 --> 00:07:20,639
ingested

00:07:17,759 --> 00:07:21,440
from here uh i essentially then just put

00:07:20,639 --> 00:07:24,240
this

00:07:21,440 --> 00:07:25,840
into an oracle database now the oracle

00:07:24,240 --> 00:07:28,080
database is just using what's called a

00:07:25,840 --> 00:07:32,400
database connection pool

00:07:28,080 --> 00:07:36,400
if you go into the controller services

00:07:32,400 --> 00:07:38,639
and i show you this um it's pretty easy

00:07:36,400 --> 00:07:40,080
i'm not gonna expand it too much because

00:07:38,639 --> 00:07:42,080
then you're gonna get my entire

00:07:40,080 --> 00:07:44,240
database name which i don't want that to

00:07:42,080 --> 00:07:46,319
happen because the aws costs would be

00:07:44,240 --> 00:07:50,000
insane

00:07:46,319 --> 00:07:52,479
just using a normal driver of course

00:07:50,000 --> 00:07:55,960
had to download the driver since it's

00:07:52,479 --> 00:07:58,560
java 8 make sure that you're using

00:07:55,960 --> 00:07:59,919
ojdbc8.jar for the newer versions of

00:07:58,560 --> 00:08:01,520
nifi it does support

00:07:59,919 --> 00:08:04,400
newer versions of java so you could

00:08:01,520 --> 00:08:06,080
utilize those jars as well

00:08:04,400 --> 00:08:07,759
and then just the database user and the

00:08:06,080 --> 00:08:08,639
password for those that aren't familiar

00:08:07,759 --> 00:08:10,960
with nifi

00:08:08,639 --> 00:08:12,319
the password actually even me the person

00:08:10,960 --> 00:08:14,000
who put it in

00:08:12,319 --> 00:08:15,680
if i go to click on this it says

00:08:14,000 --> 00:08:18,319
sensitive value set it doesn't actually

00:08:15,680 --> 00:08:21,599
show me what the value is so

00:08:18,319 --> 00:08:24,400
one of the things that i like about nifi

00:08:21,599 --> 00:08:25,440
so once i start this up then we have two

00:08:24,400 --> 00:08:27,759
different ways to

00:08:25,440 --> 00:08:28,720
kind of push the data downstream right

00:08:27,759 --> 00:08:30,160
so

00:08:28,720 --> 00:08:31,919
it's important to note that this is not

00:08:30,160 --> 00:08:32,719
real change data capture because we're

00:08:31,919 --> 00:08:35,519
not going

00:08:32,719 --> 00:08:36,880
to the log level right what we're doing

00:08:35,519 --> 00:08:38,000
is we're

00:08:36,880 --> 00:08:40,080
we're actually just querying the

00:08:38,000 --> 00:08:42,560
database but if the data

00:08:40,080 --> 00:08:44,480
has a timestamp component we essentially

00:08:42,560 --> 00:08:47,680
can use that timestamp component

00:08:44,480 --> 00:08:49,760
now quick background of where i came

00:08:47,680 --> 00:08:51,519
from i actually was a data engineer for

00:08:49,760 --> 00:08:53,120
a company called american water and

00:08:51,519 --> 00:08:55,440
one of the things that we had to do was

00:08:53,120 --> 00:08:57,200
try to get data out of sap

00:08:55,440 --> 00:08:59,120
and it was incredibly expensive to use

00:08:57,200 --> 00:09:00,880
cdc tools in order to do so

00:08:59,120 --> 00:09:03,120
so what we did is we turned to the open

00:09:00,880 --> 00:09:05,200
source we turned to apache nifi

00:09:03,120 --> 00:09:06,160
and we found that there was a way that

00:09:05,200 --> 00:09:09,200
we could

00:09:06,160 --> 00:09:12,000
essentially inject the time stamp

00:09:09,200 --> 00:09:13,279
through the slt process from you know

00:09:12,000 --> 00:09:16,240
the ecc system

00:09:13,279 --> 00:09:17,920
into the runtime system and then we

00:09:16,240 --> 00:09:19,040
could do the sql off of the runtime

00:09:17,920 --> 00:09:21,440
instance

00:09:19,040 --> 00:09:23,200
utilizing that time stamp and nifi gives

00:09:21,440 --> 00:09:25,200
us that capability because there are

00:09:23,200 --> 00:09:26,640
stateful processors in order to pull the

00:09:25,200 --> 00:09:30,000
data in

00:09:26,640 --> 00:09:31,920
so first i'm going to actually show this

00:09:30,000 --> 00:09:33,519
in uh let me make this a little bit

00:09:31,920 --> 00:09:35,760
bigger

00:09:33,519 --> 00:09:36,560
i'm going to show this pushing to kudu

00:09:35,760 --> 00:09:38,640
and you're gonna see

00:09:36,560 --> 00:09:40,720
the kudu design and the hive design are

00:09:38,640 --> 00:09:43,680
a little bit different

00:09:40,720 --> 00:09:45,120
this is it it's literally four four

00:09:43,680 --> 00:09:46,720
processors

00:09:45,120 --> 00:09:48,240
and actually one of them is just in case

00:09:46,720 --> 00:09:50,080
there's a failure i'm routing it to a

00:09:48,240 --> 00:09:50,560
log message so that i can go and take a

00:09:50,080 --> 00:09:53,680
look at it

00:09:50,560 --> 00:09:56,399
i could route the failure of the data

00:09:53,680 --> 00:09:58,000
to a file system so that way i never

00:09:56,399 --> 00:09:58,720
lose that data i can come in and take a

00:09:58,000 --> 00:10:02,240
look and see

00:09:58,720 --> 00:10:03,440
what exactly happened to it but in the

00:10:02,240 --> 00:10:04,959
query database

00:10:03,440 --> 00:10:07,440
table record i'm going to show the

00:10:04,959 --> 00:10:08,880
configuration i'm using that same

00:10:07,440 --> 00:10:12,240
database

00:10:08,880 --> 00:10:13,600
connection pool into oracle i'm allowed

00:10:12,240 --> 00:10:15,519
to pick the database type so there's

00:10:13,600 --> 00:10:17,680
several different database types

00:10:15,519 --> 00:10:20,959
one of them is actually you know the

00:10:17,680 --> 00:10:24,000
oracle 12 plus i'm using an oracle 19c

00:10:20,959 --> 00:10:24,399
uh and aws table name i called it apache

00:10:24,000 --> 00:10:27,120
con

00:10:24,399 --> 00:10:29,040
i figured it was uh it was fitting um

00:10:27,120 --> 00:10:29,760
and all i'm doing is just doing a select

00:10:29,040 --> 00:10:33,120
star from it

00:10:29,760 --> 00:10:35,360
and one of the things i like about um

00:10:33,120 --> 00:10:37,600
about nifi is that the record processors

00:10:35,360 --> 00:10:40,079
are awesome right so

00:10:37,600 --> 00:10:42,160
i had the data uh when i pull it out i

00:10:40,079 --> 00:10:44,480
could use a json record set writer

00:10:42,160 --> 00:10:46,560
so i'm actually writing the data to a

00:10:44,480 --> 00:10:48,959
json format

00:10:46,560 --> 00:10:51,440
the max value column is set so this max

00:10:48,959 --> 00:10:53,120
value column is called ingest time

00:10:51,440 --> 00:10:54,640
right it's what i created when i created

00:10:53,120 --> 00:10:56,320
the oracle table so

00:10:54,640 --> 00:10:58,000
essentially this is going to store the

00:10:56,320 --> 00:11:00,640
state and we'll show you that

00:10:58,000 --> 00:11:02,800
in a minute as well one important thing

00:11:00,640 --> 00:11:04,560
to note a lot of times when you do sql

00:11:02,800 --> 00:11:05,680
out of databases you'll notice that

00:11:04,560 --> 00:11:08,640
everything will come in as

00:11:05,680 --> 00:11:09,920
uh you know envar char or you know some

00:11:08,640 --> 00:11:13,279
sort of character set

00:11:09,920 --> 00:11:15,839
so use avro logical types it'll actually

00:11:13,279 --> 00:11:17,920
detect the average logical type

00:11:15,839 --> 00:11:19,920
if i set this to true out of the box

00:11:17,920 --> 00:11:21,760
this is going to come set as false

00:11:19,920 --> 00:11:23,120
so as you're going to pull the data you

00:11:21,760 --> 00:11:24,800
want to set this as true and what that

00:11:23,120 --> 00:11:26,560
will do is it'll actually map like your

00:11:24,800 --> 00:11:27,920
integers as integers

00:11:26,560 --> 00:11:30,800
one thing that was a little weird is the

00:11:27,920 --> 00:11:33,839
actual integer mapped to a float

00:11:30,800 --> 00:11:36,160
which is fine um but i'm able to use

00:11:33,839 --> 00:11:37,839
um you know an avro schema registry to

00:11:36,160 --> 00:11:38,320
do that conversion when i go push it

00:11:37,839 --> 00:11:42,959
into

00:11:38,320 --> 00:11:45,360
um into kudu in the same regard

00:11:42,959 --> 00:11:47,040
when you look at the configuration i got

00:11:45,360 --> 00:11:48,240
a little uh happy there

00:11:47,040 --> 00:11:50,880
i'm going to go and view the state

00:11:48,240 --> 00:11:53,920
you're going to see that the epoch

00:11:50,880 --> 00:11:56,079
timestamp is there right for the cluster

00:11:53,920 --> 00:11:57,839
if i were to clear this state i would

00:11:56,079 --> 00:11:59,839
essentially get all of the data that i

00:11:57,839 --> 00:12:01,120
have in the oracle database so it's not

00:11:59,839 --> 00:12:02,880
something i want to do now

00:12:01,120 --> 00:12:05,519
it's not a lot i you know i built this

00:12:02,880 --> 00:12:08,560
yesterday i'd say this whole process

00:12:05,519 --> 00:12:10,800
took about six hours to build everything

00:12:08,560 --> 00:12:12,399
in the demo and of which probably about

00:12:10,800 --> 00:12:13,600
four of them were just trying to get the

00:12:12,399 --> 00:12:16,240
oracle database

00:12:13,600 --> 00:12:17,600
stood up correctly and you know all the

00:12:16,240 --> 00:12:19,360
different tables set up and everything

00:12:17,600 --> 00:12:21,120
like that

00:12:19,360 --> 00:12:22,959
from here i'm going to update attribute

00:12:21,120 --> 00:12:23,680
all this is adding a schema name

00:12:22,959 --> 00:12:25,279
property

00:12:23,680 --> 00:12:26,800
and i'm going to put it in a kudu right

00:12:25,279 --> 00:12:30,079
and so if you look at the

00:12:26,800 --> 00:12:30,720
kudu processor um essentially you just

00:12:30,079 --> 00:12:33,360
have the

00:12:30,720 --> 00:12:34,560
kudu master create the table in impala

00:12:33,360 --> 00:12:36,079
and i'm going to show you the table in

00:12:34,560 --> 00:12:37,360
hue as well

00:12:36,079 --> 00:12:39,440
and then like i said we're using that

00:12:37,360 --> 00:12:40,959
json tree reader and

00:12:39,440 --> 00:12:42,639
for those that aren't familiar right

00:12:40,959 --> 00:12:45,040
with uh readers and writers

00:12:42,639 --> 00:12:47,680
so there's if i look at the

00:12:45,040 --> 00:12:50,959
configuration

00:12:47,680 --> 00:12:53,040
it's not not showing so

00:12:50,959 --> 00:12:54,320
the readers and the writers i actually

00:12:53,040 --> 00:12:57,120
actually can use

00:12:54,320 --> 00:12:58,639
an external schema nifi has its own

00:12:57,120 --> 00:13:03,200
internal schema

00:12:58,639 --> 00:13:06,800
right so if i come here to um

00:13:03,200 --> 00:13:08,000
the controller settings will configure

00:13:06,800 --> 00:13:10,639
you'll see this average scheme of

00:13:08,000 --> 00:13:11,519
registries up here right so nifi has its

00:13:10,639 --> 00:13:15,519
own internal

00:13:11,519 --> 00:13:17,120
schema um but i could use so hortonworks

00:13:15,519 --> 00:13:20,320
schema registry or cloudera schema

00:13:17,120 --> 00:13:20,959
registry um you can use a lot of

00:13:20,320 --> 00:13:22,720
different uh

00:13:20,959 --> 00:13:24,959
external schema registries right so

00:13:22,720 --> 00:13:27,600
there's there's other ones as well

00:13:24,959 --> 00:13:28,639
if i go ahead and start this flow um

00:13:27,600 --> 00:13:30,079
what i'm going to do is i'm going to

00:13:28,639 --> 00:13:35,440
start this

00:13:30,079 --> 00:13:37,839
oracle generator and you're going to see

00:13:35,440 --> 00:13:40,320
the data will start flowing in nifi

00:13:37,839 --> 00:13:40,320
pretty quick

00:13:41,120 --> 00:13:45,120
and that's just constantly flowing in

00:13:42,720 --> 00:13:47,040
right the region rights are relatively

00:13:45,120 --> 00:13:50,320
quick so doing something like this

00:13:47,040 --> 00:13:51,760
on a heavy transactional table it's not

00:13:50,320 --> 00:13:53,360
that bad actually

00:13:51,760 --> 00:13:55,920
one thing that i would recommend is

00:13:53,360 --> 00:13:58,240
probably you know don't go against the

00:13:55,920 --> 00:13:59,760
source system transactional table

00:13:58,240 --> 00:14:00,800
usually when you go against like you

00:13:59,760 --> 00:14:03,680
know

00:14:00,800 --> 00:14:05,680
larger system databases you want to push

00:14:03,680 --> 00:14:07,279
it to a to a copy so that way the

00:14:05,680 --> 00:14:08,560
transaction system never really gets

00:14:07,279 --> 00:14:10,560
upset and essentially do that

00:14:08,560 --> 00:14:12,639
replication from there and then use that

00:14:10,560 --> 00:14:14,160
to do the pulling

00:14:12,639 --> 00:14:16,000
for hive it's a little bit different

00:14:14,160 --> 00:14:17,920
right so for hive what we did is we

00:14:16,000 --> 00:14:20,720
set up a two-stage process and this is

00:14:17,920 --> 00:14:23,199
something very similar to what

00:14:20,720 --> 00:14:24,880
i did at american water if i come in

00:14:23,199 --> 00:14:27,440
here you're gonna notice that there's a

00:14:24,880 --> 00:14:30,880
lot more going on right

00:14:27,440 --> 00:14:32,880
still the same query the query database

00:14:30,880 --> 00:14:35,920
table record processor

00:14:32,880 --> 00:14:37,120
the big difference here is i'm using a

00:14:35,920 --> 00:14:40,560
conversion

00:14:37,120 --> 00:14:42,480
from avro so like when i when i use

00:14:40,560 --> 00:14:44,000
the poll here i'm using an avro record

00:14:42,480 --> 00:14:45,760
set writer to make sure that it comes in

00:14:44,000 --> 00:14:48,800
as avro

00:14:45,760 --> 00:14:50,480
i'm then converting that to orc format

00:14:48,800 --> 00:14:52,079
and i'm putting it into hdfs you're

00:14:50,480 --> 00:14:53,279
going to notice there's two hdfs

00:14:52,079 --> 00:14:55,040
processors right and

00:14:53,279 --> 00:14:56,720
and so you might be wondering why am i

00:14:55,040 --> 00:14:59,519
doing that well

00:14:56,720 --> 00:15:00,240
i'm creating a staging table right and

00:14:59,519 --> 00:15:01,920
so

00:15:00,240 --> 00:15:03,519
what i wanted to do is i wanted to show

00:15:01,920 --> 00:15:05,279
that this is only going to run

00:15:03,519 --> 00:15:09,120
the scheduling of this processor is to

00:15:05,279 --> 00:15:10,480
run every 60 seconds so one minute

00:15:09,120 --> 00:15:12,560
part of the reason why i do that is

00:15:10,480 --> 00:15:16,560
there's a merge statement into

00:15:12,560 --> 00:15:18,240
a overarching hive table um

00:15:16,560 --> 00:15:20,959
but i have this external table right

00:15:18,240 --> 00:15:22,480
here so what i wanted to do is you know

00:15:20,959 --> 00:15:24,399
when you do something like this and this

00:15:22,480 --> 00:15:26,240
is where we say it's not real cdc right

00:15:24,399 --> 00:15:29,360
it's incremental batch

00:15:26,240 --> 00:15:32,079
micro batching we were able to

00:15:29,360 --> 00:15:34,880
with with our our relatively small

00:15:32,079 --> 00:15:36,959
hadoop cluster we were able to use nifi

00:15:34,880 --> 00:15:38,399
and then the hadoop cluster together to

00:15:36,959 --> 00:15:40,480
you know basically get hundreds of

00:15:38,399 --> 00:15:40,959
thousands of records merging against um

00:15:40,480 --> 00:15:42,320
you know

00:15:40,959 --> 00:15:44,079
hundreds of millions like six hundred

00:15:42,320 --> 00:15:47,199
seven hundred million records

00:15:44,079 --> 00:15:49,519
in a matter of five minutes so

00:15:47,199 --> 00:15:51,600
we did our incremental batches in ten

00:15:49,519 --> 00:15:54,160
minute batches so that way we don't have

00:15:51,600 --> 00:15:55,920
we never had to worry about the merge

00:15:54,160 --> 00:15:57,519
you know taking a little bit of time and

00:15:55,920 --> 00:15:58,959
we didn't have to worry about backup but

00:15:57,519 --> 00:16:00,079
you can actually control a lot of the

00:15:58,959 --> 00:16:03,279
back pressure

00:16:00,079 --> 00:16:05,120
in the flow as well but

00:16:03,279 --> 00:16:07,279
what i wanted to do too is i wanted to

00:16:05,120 --> 00:16:08,000
actually store every copy of the data or

00:16:07,279 --> 00:16:10,480
data change that

00:16:08,000 --> 00:16:11,360
existed within the table as well and so

00:16:10,480 --> 00:16:14,560
this second

00:16:11,360 --> 00:16:16,399
put hdfs is for me to essentially have a

00:16:14,560 --> 00:16:18,560
historical table

00:16:16,399 --> 00:16:20,240
uh important very important you know a

00:16:18,560 --> 00:16:23,360
lot of enterprises

00:16:20,240 --> 00:16:25,360
that i've dealt with they want basically

00:16:23,360 --> 00:16:27,519
a copy of the source system

00:16:25,360 --> 00:16:28,639
in hadoop but then they also want a copy

00:16:27,519 --> 00:16:30,320
of the history

00:16:28,639 --> 00:16:32,959
right so that way they can go back and

00:16:30,320 --> 00:16:34,800
see how is the change happening how

00:16:32,959 --> 00:16:36,079
often is the change happening

00:16:34,800 --> 00:16:37,440
what can they do maybe there's

00:16:36,079 --> 00:16:39,279
information in that that they want to

00:16:37,440 --> 00:16:41,600
use machine learning on

00:16:39,279 --> 00:16:43,199
so i i store that off and and you see

00:16:41,600 --> 00:16:44,959
it's really easy i actually just have

00:16:43,199 --> 00:16:46,959
the success cues going in both different

00:16:44,959 --> 00:16:49,920
directions

00:16:46,959 --> 00:16:51,360
once it's in the staging table what i'm

00:16:49,920 --> 00:16:53,519
doing here is i'm actually just

00:16:51,360 --> 00:16:56,320
replacing the text

00:16:53,519 --> 00:16:59,199
and i'm doing a merge statement i'm

00:16:56,320 --> 00:16:59,199
going to make this bigger

00:17:00,720 --> 00:17:04,160
now i wanted to make sure and i'm glad

00:17:02,720 --> 00:17:06,240
so tim and i were testing this out

00:17:04,160 --> 00:17:08,959
earlier today and of course you know

00:17:06,240 --> 00:17:10,559
problems always happen when you test and

00:17:08,959 --> 00:17:11,360
notice that because i'm waiting a little

00:17:10,559 --> 00:17:13,760
bit of time

00:17:11,360 --> 00:17:15,520
i essentially have multiple records with

00:17:13,760 --> 00:17:17,520
the same key value

00:17:15,520 --> 00:17:19,679
that we're showing up in the staging

00:17:17,520 --> 00:17:21,760
table so a little bit of sql fixes

00:17:19,679 --> 00:17:24,559
everything so my select statement

00:17:21,760 --> 00:17:25,919
i essentially make sure that i just get

00:17:24,559 --> 00:17:29,200
you know the one that has

00:17:25,919 --> 00:17:30,160
the highest time ingest time value

00:17:29,200 --> 00:17:32,320
meaning that it's going to have the

00:17:30,160 --> 00:17:33,520
highest rank and so i'm using that as a

00:17:32,320 --> 00:17:36,000
basis to push that

00:17:33,520 --> 00:17:36,880
into and merge the data into a base

00:17:36,000 --> 00:17:40,640
table

00:17:36,880 --> 00:17:43,360
or you know my hive table

00:17:40,640 --> 00:17:44,480
and then that's just using a put hive ql

00:17:43,360 --> 00:17:46,320
processor

00:17:44,480 --> 00:17:47,760
so if you come here there's just a hive

00:17:46,320 --> 00:17:49,200
connection pool

00:17:47,760 --> 00:17:50,799
this one i can show you the connection

00:17:49,200 --> 00:17:52,880
string because it doesn't really matter

00:17:50,799 --> 00:17:54,799
yeah for some odd reason um i've got a

00:17:52,880 --> 00:17:56,480
lot of stuff running on one node so it's

00:17:54,799 --> 00:17:59,120
taking a little bit of time you'll see

00:17:56,480 --> 00:18:01,360
the ui is lagging a little bit

00:17:59,120 --> 00:18:03,200
but all this is using is just the jdbc

00:18:01,360 --> 00:18:06,400
connection string into hive

00:18:03,200 --> 00:18:08,720
um and then using the

00:18:06,400 --> 00:18:10,160
you know the hive site dot xml so

00:18:08,720 --> 00:18:11,760
there's there's uh

00:18:10,160 --> 00:18:13,679
there's xml files that you need the

00:18:11,760 --> 00:18:17,200
configuration files within hive

00:18:13,679 --> 00:18:18,080
the user exists so if i come in here to

00:18:17,200 --> 00:18:21,840
hue

00:18:18,080 --> 00:18:24,880
right and you'll see so i have

00:18:21,840 --> 00:18:28,960
what i call my production table

00:18:24,880 --> 00:18:32,240
is my apache con underscore hive table

00:18:28,960 --> 00:18:35,600
so all i'm doing is i'm doing a select

00:18:32,240 --> 00:18:37,360
key count star so what i'm showing you

00:18:35,600 --> 00:18:39,919
is that essentially i'm ordering it by

00:18:37,360 --> 00:18:42,000
the count right in descending order

00:18:39,919 --> 00:18:45,200
is that i have no duplicates of data

00:18:42,000 --> 00:18:45,200
right based on that key

00:18:45,440 --> 00:18:52,720
if i were to just do a select

00:18:48,640 --> 00:18:56,080
star from hive

00:18:52,720 --> 00:18:59,120
you'll see what the data looks like

00:18:56,080 --> 00:19:00,080
so this is all just made up data but

00:18:59,120 --> 00:19:01,520
you'll see i mean

00:19:00,080 --> 00:19:03,200
the generator is actually pretty cool

00:19:01,520 --> 00:19:05,120
it's a pretty cool api

00:19:03,200 --> 00:19:07,120
right and i have my value set and i have

00:19:05,120 --> 00:19:08,320
my ingest time as well

00:19:07,120 --> 00:19:11,840
if i were to go ahead and look at the

00:19:08,320 --> 00:19:11,840
history table

00:19:14,160 --> 00:19:24,960
you're going to see looks the same right

00:19:17,360 --> 00:19:27,520
except one of the things is

00:19:24,960 --> 00:19:29,200
if i go ahead and do that same now i

00:19:27,520 --> 00:19:32,160
have five copies of the same key

00:19:29,200 --> 00:19:34,000
five copies of the same key right so

00:19:32,160 --> 00:19:35,760
that random generator that i made that

00:19:34,000 --> 00:19:37,360
key using the expression language i

00:19:35,760 --> 00:19:38,559
basically said pick a number between one

00:19:37,360 --> 00:19:39,919
and a thousand

00:19:38,559 --> 00:19:41,679
and you're going to see i have multiple

00:19:39,919 --> 00:19:45,200
values with the same

00:19:41,679 --> 00:19:46,799
key right so primary key

00:19:45,200 --> 00:19:48,480
and i made the table as simple as

00:19:46,799 --> 00:19:50,880
possible right it's obviously not

00:19:48,480 --> 00:19:52,400
very large it's very easy to do because

00:19:50,880 --> 00:19:54,559
it's quick to build it's quick to

00:19:52,400 --> 00:19:57,280
demonstrate the capability

00:19:54,559 --> 00:19:59,600
in the same regard i can switch now to

00:19:57,280 --> 00:20:02,640
impala

00:19:59,600 --> 00:20:07,600
and i can go ahead and look at

00:20:02,640 --> 00:20:11,520
the kudu apache con table and you'll see

00:20:07,600 --> 00:20:16,480
very similar the cool thing about

00:20:11,520 --> 00:20:18,960
nifi is that when doing the kudu piece

00:20:16,480 --> 00:20:19,600
the pukudu processor actually allows you

00:20:18,960 --> 00:20:22,880
to

00:20:19,600 --> 00:20:26,080
make it an upser or an insert right

00:20:22,880 --> 00:20:27,440
so i think this is great um i would love

00:20:26,080 --> 00:20:28,960
to see that in hive

00:20:27,440 --> 00:20:31,760
you know so that you don't have to use

00:20:28,960 --> 00:20:34,240
the staging method to the push method

00:20:31,760 --> 00:20:35,679
but um this is this is a one of the

00:20:34,240 --> 00:20:39,520
fantastic things about this

00:20:35,679 --> 00:20:42,080
kudu processor so i'm a huge fan

00:20:39,520 --> 00:20:43,760
also you'll notice that with nifi you

00:20:42,080 --> 00:20:45,679
know tim pointed out the registry i have

00:20:43,760 --> 00:20:47,600
green check marks here that exist

00:20:45,679 --> 00:20:49,440
right and that's because over here in

00:20:47,600 --> 00:20:52,880
the registry i have these buckets

00:20:49,440 --> 00:20:56,799
right so each one of these buckets

00:20:52,880 --> 00:20:58,480
come here each one of these buckets i

00:20:56,799 --> 00:20:59,280
can go in and take a look at the version

00:20:58,480 --> 00:21:00,799
of the buckets

00:20:59,280 --> 00:21:02,559
i actually made the buckets while tim

00:21:00,799 --> 00:21:03,600
was talking because i forgot to do it

00:21:02,559 --> 00:21:06,320
earlier

00:21:03,600 --> 00:21:06,640
that's how fast it is to make a bucket

00:21:06,320 --> 00:21:08,559
and

00:21:06,640 --> 00:21:10,559
set the processor group for version

00:21:08,559 --> 00:21:14,000
control and you can see

00:21:10,559 --> 00:21:17,039
141 pm is when when i made this bucket

00:21:14,000 --> 00:21:17,440
so that's pretty much it um tim i don't

00:21:17,039 --> 00:21:19,200
know

00:21:17,440 --> 00:21:20,960
if you uh you want to go through some

00:21:19,200 --> 00:21:23,600
other stuff

00:21:20,960 --> 00:21:24,720
it's a pretty quick demo but this this

00:21:23,600 --> 00:21:27,760
works for oracle

00:21:24,720 --> 00:21:29,520
i've done it in production for

00:21:27,760 --> 00:21:31,280
the largest publicly traded water

00:21:29,520 --> 00:21:34,240
utility for sap

00:21:31,280 --> 00:21:36,480
um easily can do it with postgres easily

00:21:34,240 --> 00:21:40,640
can do it with uh

00:21:36,480 --> 00:21:44,400
you know with with mariadb or mysql

00:21:40,640 --> 00:21:45,840
so that's pretty much it

00:21:44,400 --> 00:21:47,200
yeah i was just going to say people

00:21:45,840 --> 00:21:49,200
could chat but somebody's been

00:21:47,200 --> 00:21:52,559
dominating the catboy

00:21:49,200 --> 00:21:55,600
i know man this guy can span

00:21:52,559 --> 00:21:55,600
i get i get bored

00:21:56,559 --> 00:22:00,400
whatever that's called i was just

00:21:58,240 --> 00:22:03,120
mentioning some of the things related to

00:22:00,400 --> 00:22:04,400
uh what we're doing in the meetup

00:22:03,120 --> 00:22:06,559
tonight which

00:22:04,400 --> 00:22:08,720
we'll have random people talking about

00:22:06,559 --> 00:22:12,080
random streaming stuff

00:22:08,720 --> 00:22:16,000
or they'll be very interactive uh

00:22:12,080 --> 00:22:19,280
in what time is it in

00:22:16,000 --> 00:22:23,120
10 15 minutes i have another session

00:22:19,280 --> 00:22:26,559
that's on the flank stack

00:22:23,120 --> 00:22:29,039
which is knifi kafka flink

00:22:26,559 --> 00:22:30,559
kudu and some of the other apache

00:22:29,039 --> 00:22:33,440
friends there

00:22:30,559 --> 00:22:35,760
it follows uh some of this and some

00:22:33,440 --> 00:22:38,320
other use cases for nifi

00:22:35,760 --> 00:22:39,440
if people do have questions please put

00:22:38,320 --> 00:22:43,120
them out there

00:22:39,440 --> 00:22:45,600
uh john will share that uh that flow

00:22:43,120 --> 00:22:46,480
we'll put that on a github somewhere i'm

00:22:45,600 --> 00:22:48,480
actually gonna write an

00:22:46,480 --> 00:22:50,480
article i'm gonna write step-by-step

00:22:48,480 --> 00:22:51,200
instructions and that way it's even

00:22:50,480 --> 00:22:53,600
better right

00:22:51,200 --> 00:22:54,960
how to set it up in aws how to how to

00:22:53,600 --> 00:22:56,799
set up um

00:22:54,960 --> 00:22:58,480
how to set up everything so that way you

00:22:56,799 --> 00:23:00,000
know it's easier to do you can step

00:22:58,480 --> 00:23:02,400
through it yourself

00:23:00,000 --> 00:23:03,600
that's very cool yeah this is a good way

00:23:02,400 --> 00:23:06,960
to uh

00:23:03,600 --> 00:23:08,320
get started what seems like a complex

00:23:06,960 --> 00:23:11,360
use case

00:23:08,320 --> 00:23:14,799
is pretty straightforward in nifi

00:23:11,360 --> 00:23:17,919
someone mentioned redshift really

00:23:14,799 --> 00:23:19,600
to switch as redshift as the source is

00:23:17,919 --> 00:23:24,400
redshift as a sync

00:23:19,600 --> 00:23:24,400
it's switching a processor which

00:23:24,799 --> 00:23:30,320
is not it's not complex i mean for

00:23:27,919 --> 00:23:31,600
uh john to pick another processor to

00:23:30,320 --> 00:23:34,640
drop in there

00:23:31,600 --> 00:23:36,320
or for me to grab uh

00:23:34,640 --> 00:23:38,880
you know we want to send it somewhere

00:23:36,320 --> 00:23:39,280
else i mean kudos a nice one because of

00:23:38,880 --> 00:23:42,400
that

00:23:39,280 --> 00:23:43,279
upsert but we can just push somewhere

00:23:42,400 --> 00:23:46,799
else

00:23:43,279 --> 00:23:50,000
like if i wanted to push to redshift

00:23:46,799 --> 00:23:53,760
or to oracle or postgres

00:23:50,000 --> 00:23:56,640
or mysql or any uh jdbc

00:23:53,760 --> 00:23:57,760
data source put database record works

00:23:56,640 --> 00:23:59,600
really well

00:23:57,760 --> 00:24:01,440
and you don't have to worry about

00:23:59,600 --> 00:24:05,279
manually converting

00:24:01,440 --> 00:24:07,840
whatever your data is into sql

00:24:05,279 --> 00:24:09,279
which could be messy or you know sql

00:24:07,840 --> 00:24:10,400
injection is always something to worry

00:24:09,279 --> 00:24:13,679
about

00:24:10,400 --> 00:24:15,200
here i just pick a sequel reader

00:24:13,679 --> 00:24:18,720
which i don't know if my screen is big

00:24:15,200 --> 00:24:21,919
enough so we've got things like avro

00:24:18,720 --> 00:24:22,559
csv grok is cool if you've seen grock

00:24:21,919 --> 00:24:26,799
for

00:24:22,559 --> 00:24:29,279
any of the log chunks rock is great

00:24:26,799 --> 00:24:31,840
you know parse your stuff two different

00:24:29,279 --> 00:24:33,840
ones for jason i usually use jason tree

00:24:31,840 --> 00:24:36,480
we could read parquet

00:24:33,840 --> 00:24:37,360
you could write a little script to parse

00:24:36,480 --> 00:24:40,480
your data

00:24:37,360 --> 00:24:43,279
grab syslog xml those are pretty

00:24:40,480 --> 00:24:44,080
pretty good set of data there so say you

00:24:43,279 --> 00:24:46,320
had

00:24:44,080 --> 00:24:48,480
uh the person mentioned jason so i have

00:24:46,320 --> 00:24:51,679
jason coming in

00:24:48,480 --> 00:24:52,960
i want to do you know insert

00:24:51,679 --> 00:24:55,360
then i'm going to have to create a

00:24:52,960 --> 00:24:56,000
database connection uh we've got a

00:24:55,360 --> 00:24:59,200
couple

00:24:56,000 --> 00:25:01,919
uh options here one if it's something

00:24:59,200 --> 00:25:02,960
related to uh hadoop or hive or hive

00:25:01,919 --> 00:25:06,320
three

00:25:02,960 --> 00:25:09,679
or for just a regular jdbc one

00:25:06,320 --> 00:25:10,080
i would just create that put in as you

00:25:09,679 --> 00:25:13,600
saw

00:25:10,080 --> 00:25:15,679
john do he put in some parameters

00:25:13,600 --> 00:25:17,279
and it's really these are the ones you'd

00:25:15,679 --> 00:25:21,360
use in a java program

00:25:17,279 --> 00:25:22,559
what's the jdbc url where's that class

00:25:21,360 --> 00:25:25,360
you know would you put the driver what's

00:25:22,559 --> 00:25:26,799
the name of the class login

00:25:25,360 --> 00:25:28,720
you know things you'd expect in a

00:25:26,799 --> 00:25:31,440
database nothing uh

00:25:28,720 --> 00:25:34,080
too hard here put a table name in what's

00:25:31,440 --> 00:25:37,200
nice within nifi which we didn't mention

00:25:34,080 --> 00:25:40,640
is everything scriptable so i can

00:25:37,200 --> 00:25:42,000
derive this table with code so i can

00:25:40,640 --> 00:25:44,960
look at that file coming

00:25:42,000 --> 00:25:46,000
in and decide oh i know this one this

00:25:44,960 --> 00:25:49,279
should go to table

00:25:46,000 --> 00:25:50,720
five because the the field names

00:25:49,279 --> 00:25:53,039
so you don't have to have this

00:25:50,720 --> 00:25:54,559
hard-coded this could be uh

00:25:53,039 --> 00:25:56,080
you know passed in and you can

00:25:54,559 --> 00:25:59,200
manipulate that

00:25:56,080 --> 00:25:59,520
and it'll automatically figure out you

00:25:59,200 --> 00:26:01,840
know

00:25:59,520 --> 00:26:03,200
which field goes where based on that

00:26:01,840 --> 00:26:05,840
schema

00:26:03,200 --> 00:26:07,440
or if you don't want to do a schema

00:26:05,840 --> 00:26:09,679
because maybe your data changes

00:26:07,440 --> 00:26:10,640
every single time which that'd be

00:26:09,679 --> 00:26:13,120
painful

00:26:10,640 --> 00:26:14,880
it probably doesn't we have infer schema

00:26:13,120 --> 00:26:17,919
that'll figure it out

00:26:14,880 --> 00:26:19,600
you usually better off if you can use a

00:26:17,919 --> 00:26:21,520
schema

00:26:19,600 --> 00:26:24,480
you know if you have them if you don't

00:26:21,520 --> 00:26:28,400
have them knife i can help you build one

00:26:24,480 --> 00:26:31,440
uh we generally put them in the adjacent

00:26:28,400 --> 00:26:32,960
style of avro very straightforward

00:26:31,440 --> 00:26:34,799
supports nulls

00:26:32,960 --> 00:26:36,640
those sort of thing you even have

00:26:34,799 --> 00:26:38,640
complex types but

00:26:36,640 --> 00:26:39,760
i don't know if i really think that's a

00:26:38,640 --> 00:26:41,279
great idea

00:26:39,760 --> 00:26:43,279
because you could have a raise in there

00:26:41,279 --> 00:26:46,240
nested records and

00:26:43,279 --> 00:26:48,240
that gets a little complex but that's

00:26:46,240 --> 00:26:49,919
something you could do

00:26:48,240 --> 00:26:51,679
you got a couple of minutes left is

00:26:49,919 --> 00:26:54,720
there a question there

00:26:51,679 --> 00:26:54,720
they're asking about kate

00:26:55,279 --> 00:26:58,960
i sent him a link to orange's open

00:26:56,960 --> 00:27:02,240
source of the nifi cop

00:26:58,960 --> 00:27:04,720
yeah yeah there's a couple of

00:27:02,240 --> 00:27:06,320
people have put them out there i don't

00:27:04,720 --> 00:27:07,919
know if they'll ever be an apache

00:27:06,320 --> 00:27:12,000
official one

00:27:07,919 --> 00:27:13,679
but i i know uh the people we work with

00:27:12,000 --> 00:27:15,360
uh are building one that's pretty

00:27:13,679 --> 00:27:18,240
awesome that does more than just

00:27:15,360 --> 00:27:20,159
run them it you know you take your thing

00:27:18,240 --> 00:27:22,080
out of the registry

00:27:20,159 --> 00:27:23,279
put in some parameters and it'll auto

00:27:22,080 --> 00:27:26,320
deploy it on uh

00:27:23,279 --> 00:27:28,640
kubernetes in a smart manner with auto

00:27:26,320 --> 00:27:31,279
scaling

00:27:28,640 --> 00:27:34,640
now if i support deleting up sir if your

00:27:31,279 --> 00:27:39,360
source or sync if your sync does we do

00:27:34,640 --> 00:27:42,480
so kudu we can uh phoenix we can

00:27:39,360 --> 00:27:45,200
i can do it in hive with the sql command

00:27:42,480 --> 00:27:47,039
i mean that's it's you're seeing you're

00:27:45,200 --> 00:27:48,640
seeing an update you're seeing upserts

00:27:47,039 --> 00:27:51,600
that's that's where

00:27:48,640 --> 00:27:51,600
let me go back here

00:27:51,760 --> 00:27:57,360
yeah so you're actually in kudu

00:27:54,799 --> 00:27:58,880
it's kudu is the easiest um it was

00:27:57,360 --> 00:27:59,679
designed really well this processor in

00:27:58,880 --> 00:28:02,880
the sense that

00:27:59,679 --> 00:28:04,320
it just has upsert right here right so i

00:28:02,880 --> 00:28:05,360
could just make this an insert or make

00:28:04,320 --> 00:28:08,080
this an upset

00:28:05,360 --> 00:28:10,000
and in hive it's a little different in

00:28:08,080 --> 00:28:10,960
hive what you do is you actually build

00:28:10,000 --> 00:28:12,399
your query

00:28:10,960 --> 00:28:14,399
i use the replace text to build the

00:28:12,399 --> 00:28:16,720
query but that's essentially what i'm

00:28:14,399 --> 00:28:19,120
doing here is i'm doing an upset right

00:28:16,720 --> 00:28:21,360
so a merge

00:28:19,120 --> 00:28:22,399
where the keys are the same and what i'm

00:28:21,360 --> 00:28:24,080
doing is i

00:28:22,399 --> 00:28:25,840
my historical data i have multiple

00:28:24,080 --> 00:28:26,799
copies i kind of showed this if you're a

00:28:25,840 --> 00:28:28,840
little late

00:28:26,799 --> 00:28:31,279
i showed that there were multiple copies

00:28:28,840 --> 00:28:33,600
of

00:28:31,279 --> 00:28:34,960
you know of the same key value right

00:28:33,600 --> 00:28:36,799
with different timestamps and different

00:28:34,960 --> 00:28:38,799
values associated with it

00:28:36,799 --> 00:28:40,480
i'm using just the sql in order to make

00:28:38,799 --> 00:28:43,120
sure that when i merge into

00:28:40,480 --> 00:28:43,520
uh you know what i call production table

00:28:43,120 --> 00:28:45,600
that

00:28:43,520 --> 00:28:47,039
i'm i'm only taking the latest time

00:28:45,600 --> 00:28:48,480
stamped value

00:28:47,039 --> 00:28:50,559
and doing that but that's essentially

00:28:48,480 --> 00:28:52,960
what this is this is an upser so

00:28:50,559 --> 00:28:55,120
if the system can can handle it nifi is

00:28:52,960 --> 00:28:57,360
knifi is not doing the upsert

00:28:55,120 --> 00:28:59,200
there it's it's doing it on the on the

00:28:57,360 --> 00:29:00,880
system database right whether it be the

00:28:59,200 --> 00:29:04,480
source or the sync

00:29:00,880 --> 00:29:06,799
so yeah i think the

00:29:04,480 --> 00:29:07,679
put database directory doesn't have it

00:29:06,799 --> 00:29:11,440
because

00:29:07,679 --> 00:29:14,399
upstairs got standard to jdbc

00:29:11,440 --> 00:29:16,480
it's kind of specific to certain data

00:29:14,399 --> 00:29:18,399
stores have that some don't

00:29:16,480 --> 00:29:20,799
so you might have to use something one

00:29:18,399 --> 00:29:26,960
of our things that does raw sql

00:29:20,799 --> 00:29:30,080
like john was doing yeah

00:29:26,960 --> 00:29:32,000
i think we have a couple minutes left

00:29:30,080 --> 00:29:33,279
i don't want to make for my own session

00:29:32,000 --> 00:29:34,880
you want to you want to you want to

00:29:33,279 --> 00:29:37,360
share anything else or

00:29:34,880 --> 00:29:37,919
um yeah let me while we're here since we

00:29:37,360 --> 00:29:41,120
got a couple

00:29:37,919 --> 00:29:42,799
of sure yeah sure let me see here do we

00:29:41,120 --> 00:29:44,159
have a do you have anything cool to show

00:29:42,799 --> 00:29:46,159
well the one thing i was going to show

00:29:44,159 --> 00:29:47,679
is i have a way to visualize

00:29:46,159 --> 00:29:49,679
we could have put that data in a nice

00:29:47,679 --> 00:29:52,960
visualization but

00:29:49,679 --> 00:29:54,240
iq also has some visualization i haven't

00:29:52,960 --> 00:29:56,559
really played with it but

00:29:54,240 --> 00:29:57,760
use a really nice apache project for

00:29:56,559 --> 00:30:00,159
doing

00:29:57,760 --> 00:30:02,480
you know queries but it can also do some

00:30:00,159 --> 00:30:05,279
basic visualization here

00:30:02,480 --> 00:30:06,399
not a ton of them but you could do some

00:30:05,279 --> 00:30:08,799
so that's something to

00:30:06,399 --> 00:30:09,840
to think about is uh you know how you

00:30:08,799 --> 00:30:12,559
display your data

00:30:09,840 --> 00:30:15,039
also apache zeppelin is a very

00:30:12,559 --> 00:30:18,480
underrated project which has

00:30:15,039 --> 00:30:21,440
some really nice abilities to do uh sql

00:30:18,480 --> 00:30:23,200
and to do running uh spark and python

00:30:21,440 --> 00:30:24,720
and r

00:30:23,200 --> 00:30:26,799
if you haven't checked that apache

00:30:24,720 --> 00:30:27,760
zeppelin i i highly recommend it same

00:30:26,799 --> 00:30:29,360
with hue

00:30:27,760 --> 00:30:31,360
they've also added a bunch of new

00:30:29,360 --> 00:30:34,480
connectors which i don't have there yet

00:30:31,360 --> 00:30:36,159
one for flink sql one for phoenix

00:30:34,480 --> 00:30:38,559
so those those are pretty exciting

00:30:36,159 --> 00:30:41,279
because this is totally used

00:30:38,559 --> 00:30:42,000
well i guess we we glanced over the the

00:30:41,279 --> 00:30:45,039
question

00:30:42,000 --> 00:30:47,360
um uh

00:30:45,039 --> 00:30:49,279
basically can you use it for long

00:30:47,360 --> 00:30:50,399
running jobs or should we use kate's

00:30:49,279 --> 00:30:54,000
only to run some small

00:30:50,399 --> 00:30:54,000
jobs but i mean

00:30:54,320 --> 00:30:58,799
my opinion could be wrong uh the

00:30:57,200 --> 00:31:00,640
kubernetes stuff the whole point of

00:30:58,799 --> 00:31:01,440
having kubernetes is not to have the

00:31:00,640 --> 00:31:04,080
long running

00:31:01,440 --> 00:31:05,600
right unless you you don't know what

00:31:04,080 --> 00:31:07,919
your data looks like right where you

00:31:05,600 --> 00:31:09,919
need that scalability of resources

00:31:07,919 --> 00:31:12,640
but essentially if you're making

00:31:09,919 --> 00:31:14,240
long-running containers

00:31:12,640 --> 00:31:15,679
then that means that that's time where

00:31:14,240 --> 00:31:16,799
those containers aren't available to

00:31:15,679 --> 00:31:18,640
other jobs to run

00:31:16,799 --> 00:31:20,880
right so the whole point in my opinion

00:31:18,640 --> 00:31:23,120
of kubernetes is essentially

00:31:20,880 --> 00:31:25,120
you know have it run give it back have

00:31:23,120 --> 00:31:25,600
it run give it back have it run give it

00:31:25,120 --> 00:31:28,480
back

00:31:25,600 --> 00:31:29,679
um i don't know tim what are your

00:31:28,480 --> 00:31:32,000
thoughts

00:31:29,679 --> 00:31:33,360
yeah that's that's generally the idea we

00:31:32,000 --> 00:31:36,159
have

00:31:33,360 --> 00:31:36,960
a new way to run this and even in that

00:31:36,159 --> 00:31:39,279
one

00:31:36,960 --> 00:31:40,240
it's going to be running for as long as

00:31:39,279 --> 00:31:42,960
the job runs

00:31:40,240 --> 00:31:45,200
or run on demand when an event comes in

00:31:42,960 --> 00:31:46,799
that also brings up something else that

00:31:45,200 --> 00:31:50,720
might be of interest

00:31:46,799 --> 00:31:53,679
uh to them is the nifi stateless engine

00:31:50,720 --> 00:31:54,799
which uh i didn't think to mention and

00:31:53,679 --> 00:31:58,480
that will run

00:31:54,799 --> 00:32:00,559
as event at a time or job at a time

00:31:58,480 --> 00:32:02,640
and that works out really nice for

00:32:00,559 --> 00:32:03,760
running that on kubernetes or yarn or

00:32:02,640 --> 00:32:06,159
docker

00:32:03,760 --> 00:32:08,320
uh it really depends on your use case if

00:32:06,159 --> 00:32:10,720
you've got data that you're waiting on

00:32:08,320 --> 00:32:12,080
or you're running i i have nine five

00:32:10,720 --> 00:32:14,720
running as a web server

00:32:12,080 --> 00:32:17,519
obviously i want that always running i i

00:32:14,720 --> 00:32:20,000
probably want just a standard cluster

00:32:17,519 --> 00:32:22,880
you know whether that's you know just

00:32:20,000 --> 00:32:26,480
apache and i do it myself or get it from

00:32:22,880 --> 00:32:27,840
uh a vendor that's uh that's an option

00:32:26,480 --> 00:32:29,279
because if you're always going to be

00:32:27,840 --> 00:32:31,519
running

00:32:29,279 --> 00:32:32,960
you probably want something that's uh

00:32:31,519 --> 00:32:35,039
designed for that if i put it on

00:32:32,960 --> 00:32:37,760
kubernetes and run it all the time

00:32:35,039 --> 00:32:38,480
i don't get the advantage of reusing

00:32:37,760 --> 00:32:41,200
those uh

00:32:38,480 --> 00:32:42,080
pods which is nice but you could do it

00:32:41,200 --> 00:32:44,480
and the new version

00:32:42,080 --> 00:32:46,880
will i mean you know is the long-running

00:32:44,480 --> 00:32:49,200
job until it doesn't need to run anymore

00:32:46,880 --> 00:32:52,559
you know maybe if no data shows up for

00:32:49,200 --> 00:32:55,840
an hour i shut it down to maybe one node

00:32:52,559 --> 00:32:58,080
or turn it off and have my operators

00:32:55,840 --> 00:33:00,720
waiting for some signal to spin it up

00:32:58,080 --> 00:33:02,480
as long as you're okay with that delay i

00:33:00,720 --> 00:33:03,760
mean the amount of resources and money

00:33:02,480 --> 00:33:06,880
you save might make it

00:33:03,760 --> 00:33:07,679
worthwhile depends on your use case not

00:33:06,880 --> 00:33:11,840
if it's

00:33:07,679 --> 00:33:11,840
life critical data though

00:33:13,760 --> 00:33:17,360
that's pretty cool i think we are out of

00:33:15,919 --> 00:33:20,399
time because it's

00:33:17,360 --> 00:33:22,640
2 10 eastern time all right

00:33:20,399 --> 00:33:24,880
yeah you got another session so yeah

00:33:22,640 --> 00:33:26,799
mine's at 215 if people are interested

00:33:24,880 --> 00:33:30,000
there'll be more nifi

00:33:26,799 --> 00:33:30,640
and there'll be kafka and kudu and as

00:33:30,000 --> 00:33:33,039
many

00:33:30,640 --> 00:33:36,000
apache projects as i could stuff in 40

00:33:33,039 --> 00:33:38,960
minutes how about that

00:33:36,000 --> 00:33:40,880
thanks a lot for uh coming thanks again

00:33:38,960 --> 00:33:43,919
to john for doing all the work

00:33:40,880 --> 00:33:46,000
that way i only had to do part of five

00:33:43,919 --> 00:33:47,600
talks and not six complete talks by

00:33:46,000 --> 00:33:49,279
myself

00:33:47,600 --> 00:33:51,679
when you when you submit a talk to

00:33:49,279 --> 00:33:53,840
apache sometimes they approve more than

00:33:51,679 --> 00:33:53,840
one

00:33:56,640 --> 00:34:02,080
thanks for coming uh if you we'll uh

00:33:59,360 --> 00:34:04,320
post the slides and some

00:34:02,080 --> 00:34:05,519
and the demos and probably john will do

00:34:04,320 --> 00:34:07,279
an article

00:34:05,519 --> 00:34:09,040
and we'll make sure that's tagged with

00:34:07,279 --> 00:34:09,760
apache con so people know where to find

00:34:09,040 --> 00:34:11,440
it

00:34:09,760 --> 00:34:14,399
yeah definitely it might take me a few

00:34:11,440 --> 00:34:16,480
days to to write up the instructions but

00:34:14,399 --> 00:34:18,720
yeah they think that's cool then i need

00:34:16,480 --> 00:34:22,159
it now

00:34:18,720 --> 00:34:23,919
i'll get on it man thanks for kevin

00:34:22,159 --> 00:34:25,280
hopefully see in the next session and i

00:34:23,919 --> 00:34:28,639
also got one

00:34:25,280 --> 00:34:31,839
with a committer for apache nifi at

00:34:28,639 --> 00:34:35,839
3 35 eastern time

00:34:31,839 --> 00:34:39,040
real yeah pierre real time uh ingesting

00:34:35,839 --> 00:34:41,040
very similar to this but you've got a

00:34:39,040 --> 00:34:42,159
a high level uh committer there which

00:34:41,040 --> 00:34:44,320
will be very nice

00:34:42,159 --> 00:34:46,480
to ask questions pierre knows his stuff

00:34:44,320 --> 00:34:49,040
man so pierre's pair is awesome

00:34:46,480 --> 00:34:50,639
yeah i get smarter standing next to him

00:34:49,040 --> 00:34:52,720
it's very nice

00:34:50,639 --> 00:34:53,679
so thanks everybody hope to see you in

00:34:52,720 --> 00:35:03,839
the next session

00:34:53,679 --> 00:35:03,839
see you guys

00:35:08,240 --> 00:35:10,320

YouTube URL: https://www.youtube.com/watch?v=nPewLy7Zjkg


