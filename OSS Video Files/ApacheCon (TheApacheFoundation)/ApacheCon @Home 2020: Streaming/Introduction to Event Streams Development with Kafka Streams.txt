Title: Introduction to Event Streams Development with Kafka Streams
Publication date: 2020-10-22
Playlist: ApacheCon @Home 2020: Streaming
Description: 
	Introduction to Event Streams Development with Kafka Streams
Bill Bejeck

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Developers today work with a lot of data. Much of this data is available near real-time. And it presents the opportunity for businesses and organizations to improve service and deliver more value to users of today's applications. But the question is, how to manage this incoming stream of records? Viewing the incoming data as event streams is one way to think about working with data. In recent years, Apache Kafka has become a defacto standard for ingesting record streams. To work with the incoming data, Apache Kafka provides a Producer and Consumer interface as the basic building blocks for sending to and reading records from Kafka. When building a Kafka-based microservice, using the Producer and Consumer clients means handling all the details of communicating yourself. To enable building event-driven applications, Apache Kafka provides Kafka Streams. Kafka Streams is the native stream procession library for Apache Kafka In this talk, we'll review Kafka and how it can function as a central nervous system for incoming data. From there, we'll cover how Kafka Producers and Consumers work and how developers can build a microservice using these building blocks. Finally, we'll transition our application to a Kafka Streams application and demonstrate how using Kafka Streams can simplify building a Kafka based microservice. Attendees of this presentation will gain the knowledge needed to understand how Kafka Streams works and how they can get started using it to simplify the development of applications involving Apache Kafka. Additionally, developers in attendance that aren't familiar with Apache Kafka itself will gain an understanding of how it can help their business or organization make effective use of available incoming event streams.

Bill Bejeck is working at Confluent as an integration architect on the Developer Relations Team before that Bill was a software engineer on the Kafka Streams team for three years. He has been a software engineer for over 17 years and has regularly contributed to Kafka Streams. Before Confluent, he worked on various ingest applications as a U.S. Government contractor using distributed software such as Apache Kafka, Apache Spark™, and Apache™ Hadoop®. Bill is a committer to Apache Kafka and has also written a book about Kafka Streams titled Kafka Streams in Action.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:29,679 --> 00:00:34,239
hi

00:00:30,960 --> 00:00:35,120
welcome to um lovely gaithersburg

00:00:34,239 --> 00:00:37,200
maryland

00:00:35,120 --> 00:00:40,160
uh this is event streams development

00:00:37,200 --> 00:00:40,160
with kafka streams

00:00:41,200 --> 00:00:46,079
um so first i'll just briefly introduce

00:00:45,440 --> 00:00:49,360
myself

00:00:46,079 --> 00:00:50,800
uh my name is bill b jack i'm on the

00:00:49,360 --> 00:00:51,840
integration i'm an integration architect

00:00:50,800 --> 00:00:54,480
on the devx team

00:00:51,840 --> 00:00:56,079
uh part of that i spent three years as a

00:00:54,480 --> 00:00:56,719
software engineer on the kafka streams

00:00:56,079 --> 00:00:59,520
team

00:00:56,719 --> 00:01:00,800
i'm an apache kafka committer and author

00:00:59,520 --> 00:01:02,559
of kafka streams in action

00:01:00,800 --> 00:01:05,520
and i actually have a second edition of

00:01:02,559 --> 00:01:05,520
that underway

00:01:07,040 --> 00:01:14,320
um so what are we going to cover today

00:01:11,280 --> 00:01:15,759
basically what is an event stream then

00:01:14,320 --> 00:01:18,159
we're gonna do

00:01:15,759 --> 00:01:19,759
uh a co an overview of what kafka is and

00:01:18,159 --> 00:01:21,759
then we're gonna talk about uh kafka

00:01:19,759 --> 00:01:24,240
streams concepts and architectures

00:01:21,759 --> 00:01:25,840
uh kafka streams api and features and

00:01:24,240 --> 00:01:28,000
we're gonna see an example of an event

00:01:25,840 --> 00:01:30,880
stream processing before and after

00:01:28,000 --> 00:01:30,880
kafka streams

00:01:31,600 --> 00:01:37,119
so what makes what makes up an event

00:01:34,720 --> 00:01:37,119
streak

00:01:39,119 --> 00:01:42,560
my answer to that's pretty

00:01:40,240 --> 00:01:46,960
straightforward

00:01:42,560 --> 00:01:50,000
everything everything is an event stream

00:01:46,960 --> 00:01:50,479
you have sales you have an invoice uh a

00:01:50,000 --> 00:01:53,040
trade

00:01:50,479 --> 00:01:55,680
customer experience and then you can

00:01:53,040 --> 00:01:57,840
also have a software to software events

00:01:55,680 --> 00:01:58,960
so everything that that occurs at a

00:01:57,840 --> 00:02:00,960
certain point of time

00:01:58,960 --> 00:02:03,600
we're going to consider an event which

00:02:00,960 --> 00:02:03,600
leads me to

00:02:04,560 --> 00:02:11,039
all your data are event streams

00:02:07,680 --> 00:02:12,400
and so this is important because uh you

00:02:11,039 --> 00:02:14,640
like to think of i like to think of

00:02:12,400 --> 00:02:16,160
kafka as the central nervous system for

00:02:14,640 --> 00:02:17,840
your data so it's gonna

00:02:16,160 --> 00:02:20,879
it's gonna collect data you're gonna

00:02:17,840 --> 00:02:22,959
send in records come into kafka but

00:02:20,879 --> 00:02:23,920
you need to process it somehow so let's

00:02:22,959 --> 00:02:25,920
take a quick look

00:02:23,920 --> 00:02:27,920
but these event streams might mean

00:02:25,920 --> 00:02:31,360
different things at different people

00:02:27,920 --> 00:02:37,120
so as an example let's consider

00:02:31,360 --> 00:02:40,000
the humble page view event

00:02:37,120 --> 00:02:40,879
now let's just consider let's just say

00:02:40,000 --> 00:02:44,000
for this example

00:02:40,879 --> 00:02:44,879
it is a banking example it's uh you go

00:02:44,000 --> 00:02:47,040
to your

00:02:44,879 --> 00:02:48,800
your bank your main website and then you

00:02:47,040 --> 00:02:51,519
see a

00:02:48,800 --> 00:02:52,480
certain page comes up and you click on

00:02:51,519 --> 00:02:55,120
it click through

00:02:52,480 --> 00:02:56,560
uh you're going to have the session id

00:02:55,120 --> 00:02:59,440
time stamp

00:02:56,560 --> 00:03:00,879
um what type it is it's it's an equity

00:02:59,440 --> 00:03:01,840
loan offer you clicked on or an

00:03:00,879 --> 00:03:04,080
advertisement

00:03:01,840 --> 00:03:04,879
it's got an offer id cut your customer

00:03:04,080 --> 00:03:09,440
id

00:03:04,879 --> 00:03:11,200
so basically lots of metadata

00:03:09,440 --> 00:03:14,000
but this page event means different

00:03:11,200 --> 00:03:16,879
things to different different people

00:03:14,000 --> 00:03:18,640
um you've got you know several other

00:03:16,879 --> 00:03:21,280
people taking a look at it

00:03:18,640 --> 00:03:22,560
uh underwriting might be underwriting

00:03:21,280 --> 00:03:24,560
might be interested in this as a

00:03:22,560 --> 00:03:25,920
pre-qualification you clicked on it

00:03:24,560 --> 00:03:27,920
since your customer they're gonna look

00:03:25,920 --> 00:03:30,400
up real quick and run some fast numbers

00:03:27,920 --> 00:03:32,159
and maybe send a follow-up email uh

00:03:30,400 --> 00:03:35,280
asking you about that

00:03:32,159 --> 00:03:36,879
um data science might be interested in

00:03:35,280 --> 00:03:38,319
uh people that looked at this loan

00:03:36,879 --> 00:03:40,480
officer offer

00:03:38,319 --> 00:03:41,519
ended up doing something else this was

00:03:40,480 --> 00:03:44,799
their next steps

00:03:41,519 --> 00:03:46,400
and then the promotion department uh

00:03:44,799 --> 00:03:48,640
is going to be interested in this as

00:03:46,400 --> 00:03:49,840
well uh from the standpoint of are we

00:03:48,640 --> 00:03:50,959
hitting the right you know are the right

00:03:49,840 --> 00:03:52,319
people clicking on this right

00:03:50,959 --> 00:03:53,760
demographics you know

00:03:52,319 --> 00:03:55,680
or are people seeing it are they

00:03:53,760 --> 00:03:57,760
clicking on it

00:03:55,680 --> 00:03:59,120
so what this what this means is since

00:03:57,760 --> 00:04:01,920
you have

00:03:59,120 --> 00:04:02,640
uh this simple page view event in

00:04:01,920 --> 00:04:05,760
different people

00:04:02,640 --> 00:04:06,319
you need a way to to process this data

00:04:05,760 --> 00:04:08,799
differently

00:04:06,319 --> 00:04:10,640
to process you know each one of these if

00:04:08,799 --> 00:04:12,640
we consider each one of these boxes

00:04:10,640 --> 00:04:14,400
we're part of separate departments that

00:04:12,640 --> 00:04:15,920
want to analyze this data

00:04:14,400 --> 00:04:17,440
you want them to be able to process it

00:04:15,920 --> 00:04:20,479
and handle it in the way that makes the

00:04:17,440 --> 00:04:20,479
best sense for them

00:04:25,840 --> 00:04:29,280
okay so i'm going to start with my

00:04:27,280 --> 00:04:32,880
overview of kafka

00:04:29,280 --> 00:04:36,320
this will be a a quick one kafka

00:04:32,880 --> 00:04:37,199
is essentially it's based on the idea of

00:04:36,320 --> 00:04:40,400
a log

00:04:37,199 --> 00:04:41,840
and not the application logs that

00:04:40,400 --> 00:04:44,880
developers are used to when you're

00:04:41,840 --> 00:04:47,280
reading the state of your application or

00:04:44,880 --> 00:04:50,560
the series of events that are happening

00:04:47,280 --> 00:04:53,759
in the application itself now the log is

00:04:50,560 --> 00:04:57,199
uh is there it is

00:04:53,759 --> 00:04:59,360
the record of transaction it's the core

00:04:57,199 --> 00:05:01,520
entity or the core data structure that

00:04:59,360 --> 00:05:02,639
holds all the series of events that have

00:05:01,520 --> 00:05:04,639
happened

00:05:02,639 --> 00:05:05,919
and each record that comes in just gets

00:05:04,639 --> 00:05:07,360
appended at the end of the log it's a

00:05:05,919 --> 00:05:09,919
very simple data structure

00:05:07,360 --> 00:05:11,520
a new record comes in and gets appended

00:05:09,919 --> 00:05:13,680
it's assigned

00:05:11,520 --> 00:05:15,759
a number where it is in the log which is

00:05:13,680 --> 00:05:17,840
considered

00:05:15,759 --> 00:05:20,240
an offset number and just keeps going

00:05:17,840 --> 00:05:22,080
and goes to the end of the log

00:05:20,240 --> 00:05:25,120
and the nice thing is about this is you

00:05:22,080 --> 00:05:26,240
get automatically get time ordering of

00:05:25,120 --> 00:05:28,800
events

00:05:26,240 --> 00:05:30,960
and this also again this represents if

00:05:28,800 --> 00:05:33,759
you want to replay this this represents

00:05:30,960 --> 00:05:35,360
the source of truth of data for how

00:05:33,759 --> 00:05:37,120
things have occurred up to a certain

00:05:35,360 --> 00:05:41,039
point in time

00:05:37,120 --> 00:05:41,039
so what makes up a record in kafka

00:05:41,840 --> 00:05:45,199
so basically you have a header which can

00:05:43,759 --> 00:05:47,120
have

00:05:45,199 --> 00:05:48,800
different types of metadata and you have

00:05:47,120 --> 00:05:53,120
a key and a value

00:05:48,800 --> 00:05:55,600
now the key does not have to be

00:05:53,120 --> 00:05:57,280
populated i think key could be no which

00:05:55,600 --> 00:06:00,240
most likely you're going to have

00:05:57,280 --> 00:06:01,280
you're always going to have a value uh

00:06:00,240 --> 00:06:02,720
there's a little difference here with

00:06:01,280 --> 00:06:05,680
the compacted topics but

00:06:02,720 --> 00:06:07,039
that's for another discussion but one of

00:06:05,680 --> 00:06:09,199
the other things that's

00:06:07,039 --> 00:06:10,560
part of a kafka record which is

00:06:09,199 --> 00:06:12,319
important we'll talk about this later

00:06:10,560 --> 00:06:15,680
the kafka streams

00:06:12,319 --> 00:06:15,680
is the concept of time

00:06:15,759 --> 00:06:20,319
um you have event time and log append

00:06:18,840 --> 00:06:22,400
time

00:06:20,319 --> 00:06:24,319
the producer when you create a producer

00:06:22,400 --> 00:06:25,520
record you have the opportunity to

00:06:24,319 --> 00:06:28,080
timestamp yourself

00:06:25,520 --> 00:06:29,600
if you don't the kafka producer will set

00:06:28,080 --> 00:06:31,759
the timestamp and that is considered

00:06:29,600 --> 00:06:34,080
event time

00:06:31,759 --> 00:06:35,520
now there's a configuration you can the

00:06:34,080 --> 00:06:37,280
default is event time there's a

00:06:35,520 --> 00:06:40,160
configuration you can set

00:06:37,280 --> 00:06:41,280
where the broker itself will set the

00:06:40,160 --> 00:06:43,600
timestamp

00:06:41,280 --> 00:06:44,800
and that's considered log depend time so

00:06:43,600 --> 00:06:47,120
what happens there

00:06:44,800 --> 00:06:49,840
is the broker will overwrite whatever

00:06:47,120 --> 00:06:52,560
stamp was put on the kafka record

00:06:49,840 --> 00:06:53,759
the broker will override that and set

00:06:52,560 --> 00:06:55,360
the time stamp at the time it's

00:06:53,759 --> 00:06:57,360
appending it to the law

00:06:55,360 --> 00:06:59,199
now a key distinction i want to make

00:06:57,360 --> 00:07:02,319
here is when

00:06:59,199 --> 00:07:04,400
the record you have itself over here

00:07:02,319 --> 00:07:05,919
you can have a time stamp embedded in

00:07:04,400 --> 00:07:07,919
that

00:07:05,919 --> 00:07:09,120
that could be you know let's say you're

00:07:07,919 --> 00:07:10,960
ingesting

00:07:09,120 --> 00:07:12,560
records there's a lag between when the

00:07:10,960 --> 00:07:15,840
events actually happen and when you get

00:07:12,560 --> 00:07:17,599
them to produce the producer

00:07:15,840 --> 00:07:19,919
when i talk about studying time stamps

00:07:17,599 --> 00:07:22,080
this is on the kafka record itself not

00:07:19,919 --> 00:07:25,039
this doesn't touch any timestamps you

00:07:22,080 --> 00:07:25,039
might have in your values

00:07:25,360 --> 00:07:28,960
so the next concept i want to talk about

00:07:28,000 --> 00:07:32,800
in kafka

00:07:28,960 --> 00:07:37,120
is the idea of partitions

00:07:32,800 --> 00:07:40,400
kafka uses partitions for throughput

00:07:37,120 --> 00:07:44,000
and to distribute the load every

00:07:40,400 --> 00:07:46,479
topic is a topic you can every topic has

00:07:44,000 --> 00:07:48,160
to have partitions even if it's just

00:07:46,479 --> 00:07:50,479
when you create a topic you have to

00:07:48,160 --> 00:07:52,160
specify the number of partitions which

00:07:50,479 --> 00:07:55,039
could be just one

00:07:52,160 --> 00:07:57,680
so in that case you have a topic

00:07:55,039 --> 00:07:59,120
partition where the partition is zero

00:07:57,680 --> 00:08:01,440
we're going to consider this simple case

00:07:59,120 --> 00:08:04,960
here you've got three partitions

00:08:01,440 --> 00:08:06,720
so every record comes in and it's going

00:08:04,960 --> 00:08:09,360
to based on the key

00:08:06,720 --> 00:08:10,720
it's going to go to a certain partition

00:08:09,360 --> 00:08:12,639
and this increases

00:08:10,720 --> 00:08:14,560
again this is done for throughput it

00:08:12,639 --> 00:08:18,800
also helps spread the load because

00:08:14,560 --> 00:08:21,120
every broker is in a kafka cluster

00:08:18,800 --> 00:08:23,039
a topic a broker is considered the lead

00:08:21,120 --> 00:08:26,160
broker for topic partition

00:08:23,039 --> 00:08:28,720
so this allows and then the other very

00:08:26,160 --> 00:08:30,479
assuming a three broker cluster

00:08:28,720 --> 00:08:32,080
you would have a broker that's a lead

00:08:30,479 --> 00:08:33,279
for a topic partition and the other two

00:08:32,080 --> 00:08:34,959
would be followers

00:08:33,279 --> 00:08:37,360
so that helps spread out the load so you

00:08:34,959 --> 00:08:39,200
don't have all type of partitions

00:08:37,360 --> 00:08:40,479
on a particular broker that helps spread

00:08:39,200 --> 00:08:42,959
it out

00:08:40,479 --> 00:08:45,120
and then the the way the assignment per

00:08:42,959 --> 00:08:48,560
partition works if you have a key

00:08:45,120 --> 00:08:50,080
it's the standard hash of the key um

00:08:48,560 --> 00:08:52,720
modular the number of partitions and

00:08:50,080 --> 00:08:54,399
that determines uh

00:08:52,720 --> 00:08:56,560
the determine is the partition to send

00:08:54,399 --> 00:08:59,440
that right to put that record on

00:08:56,560 --> 00:09:01,920
uh if you don't provide a key there's a

00:08:59,440 --> 00:09:06,399
new strategy i think it was in

00:09:01,920 --> 00:09:07,680
kafka 242.4 it is the sticky partitioner

00:09:06,399 --> 00:09:10,720
it used to be

00:09:07,680 --> 00:09:13,040
in this case here uh

00:09:10,720 --> 00:09:14,560
or each record that came in it would be

00:09:13,040 --> 00:09:16,000
assigned to this partition and then the

00:09:14,560 --> 00:09:18,320
next one and the next one

00:09:16,000 --> 00:09:19,920
as the records go into a producer and

00:09:18,320 --> 00:09:21,440
what doesn't producers don't send

00:09:19,920 --> 00:09:21,760
records automatically they buffer them

00:09:21,440 --> 00:09:24,000
up

00:09:21,760 --> 00:09:25,760
and send them in a batch so the way what

00:09:24,000 --> 00:09:28,399
would happen here

00:09:25,760 --> 00:09:30,320
is each record would come in get

00:09:28,399 --> 00:09:32,240
assigned partition zero then the next

00:09:30,320 --> 00:09:35,519
one partition one and partition two

00:09:32,240 --> 00:09:38,800
start back at zero now what happens

00:09:35,519 --> 00:09:40,560
is when records come in they go

00:09:38,800 --> 00:09:42,720
they're assigned to a partition per

00:09:40,560 --> 00:09:45,279
batch so

00:09:42,720 --> 00:09:46,880
what would happen here is you get a

00:09:45,279 --> 00:09:48,720
batch of records come in

00:09:46,880 --> 00:09:50,399
they would first that first batch would

00:09:48,720 --> 00:09:52,560
go just to this partition

00:09:50,399 --> 00:09:53,680
then your second batch goes to this the

00:09:52,560 --> 00:09:56,560
second partition

00:09:53,680 --> 00:09:58,080
and then the third batch goes to this

00:09:56,560 --> 00:09:59,279
partition and then that starts over

00:09:58,080 --> 00:10:02,240
again so you still get

00:09:59,279 --> 00:10:03,760
a balance of records across partitions

00:10:02,240 --> 00:10:06,800
but it's more eventual

00:10:03,760 --> 00:10:10,640
it's it's instead of instead and the

00:10:06,800 --> 00:10:11,920
advantage to this is you're for a batch

00:10:10,640 --> 00:10:14,079
you would only send

00:10:11,920 --> 00:10:15,760
one batch per part you're only going to

00:10:14,079 --> 00:10:16,720
send one batch as opposed to sending

00:10:15,760 --> 00:10:22,399
multiple batches

00:10:16,720 --> 00:10:22,399
for partitions now i had mentioned

00:10:22,800 --> 00:10:29,360
kafka being assigned a

00:10:26,959 --> 00:10:31,120
being the lead for topic partition but

00:10:29,360 --> 00:10:33,120
also assuming a three broker cluster

00:10:31,120 --> 00:10:33,760
here it's also going to have two

00:10:33,120 --> 00:10:35,360
followers

00:10:33,760 --> 00:10:37,440
and this is if you set your replication

00:10:35,360 --> 00:10:40,160
factor to three

00:10:37,440 --> 00:10:41,440
and this is your first this broker right

00:10:40,160 --> 00:10:44,079
here is your first

00:10:41,440 --> 00:10:45,519
is considered the first replica these

00:10:44,079 --> 00:10:47,920
two over here

00:10:45,519 --> 00:10:48,640
are replicas two and three so what

00:10:47,920 --> 00:10:51,120
happens is

00:10:48,640 --> 00:10:52,640
all interactions for this topic

00:10:51,120 --> 00:10:54,720
partition presented here

00:10:52,640 --> 00:10:56,640
go through the lead broker and you

00:10:54,720 --> 00:10:57,519
produce it goes to this broker you

00:10:56,640 --> 00:11:00,000
consume those

00:10:57,519 --> 00:11:01,519
consumer requests go to this broker but

00:11:00,000 --> 00:11:03,760
the two followers are going to

00:11:01,519 --> 00:11:05,519
continually send fetch requests

00:11:03,760 --> 00:11:07,760
and under normal circumstances they'll

00:11:05,519 --> 00:11:09,760
be in sync so at any time point in time

00:11:07,760 --> 00:11:12,480
again under normal circumstances

00:11:09,760 --> 00:11:13,040
you'll have three copies of your data

00:11:12,480 --> 00:11:14,880
with this

00:11:13,040 --> 00:11:16,480
configuration with you know considered

00:11:14,880 --> 00:11:20,560
configured for three

00:11:16,480 --> 00:11:20,560
uh bubble because

00:11:20,640 --> 00:11:27,200
so now let's talk about clients how does

00:11:24,160 --> 00:11:29,440
the data get into kafka where you have

00:11:27,200 --> 00:11:31,519
producers i mentioned that before

00:11:29,440 --> 00:11:33,760
uh just a few seconds ago you have a

00:11:31,519 --> 00:11:36,160
producer you create a producer record

00:11:33,760 --> 00:11:38,000
and you call producer.send now that is

00:11:36,160 --> 00:11:40,240
an asynchronous call

00:11:38,000 --> 00:11:41,680
uh and so that allows you to not have to

00:11:40,240 --> 00:11:46,320
block to wait

00:11:41,680 --> 00:11:48,720
for the for those calls to be successful

00:11:46,320 --> 00:11:50,720
and again when you call producer.send

00:11:48,720 --> 00:11:52,639
when you put a record on there

00:11:50,720 --> 00:11:54,000
it doesn't send it right away they go

00:11:52,639 --> 00:11:56,320
into a buffer

00:11:54,000 --> 00:11:57,200
and then when either when the buffer is

00:11:56,320 --> 00:11:59,760
full or it's

00:11:57,200 --> 00:12:00,959
it's considered time to send then a

00:11:59,760 --> 00:12:03,839
batch is sent

00:12:00,959 --> 00:12:05,519
and again it will go to a batch goes to

00:12:03,839 --> 00:12:06,320
one partition and then the next case it

00:12:05,519 --> 00:12:09,360
would

00:12:06,320 --> 00:12:12,240
go to a second purchase

00:12:09,360 --> 00:12:14,480
but on the other side of the coin you

00:12:12,240 --> 00:12:16,320
have consumers

00:12:14,480 --> 00:12:17,600
and what we have here we've got two

00:12:16,320 --> 00:12:19,760
partitions

00:12:17,600 --> 00:12:22,320
and we've got two consumers but i've got

00:12:19,760 --> 00:12:25,680
this kafka has a notion of a

00:12:22,320 --> 00:12:28,560
consumer group and what that is is both

00:12:25,680 --> 00:12:31,440
of these consumers

00:12:28,560 --> 00:12:32,320
are part of group a so logically what

00:12:31,440 --> 00:12:35,440
this looks like

00:12:32,320 --> 00:12:36,880
is one it's a logically one consumer

00:12:35,440 --> 00:12:38,639
there's multiple consumers

00:12:36,880 --> 00:12:40,320
and you can have as many consumers as

00:12:38,639 --> 00:12:42,560
our topic partitions

00:12:40,320 --> 00:12:43,600
you can allocate more but those will be

00:12:42,560 --> 00:12:45,920
idle

00:12:43,600 --> 00:12:49,440
uh maybe for failover reasons you might

00:12:45,920 --> 00:12:51,839
have more that are that are there

00:12:49,440 --> 00:12:53,760
but again so we have we've got two

00:12:51,839 --> 00:12:56,880
consumers each one is assigned

00:12:53,760 --> 00:12:58,720
a topic partition so what happens in the

00:12:56,880 --> 00:12:59,680
event of a failure what happens if a

00:12:58,720 --> 00:13:02,720
consumer

00:12:59,680 --> 00:13:04,800
is unrepo unresponsive

00:13:02,720 --> 00:13:07,760
group coordinator is going to kick this

00:13:04,800 --> 00:13:10,079
consumer out of the group

00:13:07,760 --> 00:13:12,000
and then what happens next is a

00:13:10,079 --> 00:13:14,800
rebalance

00:13:12,000 --> 00:13:17,360
and when you have a rebalance because of

00:13:14,800 --> 00:13:19,200
a consumer being unresponsive

00:13:17,360 --> 00:13:22,000
it gets kicked out of group the topic

00:13:19,200 --> 00:13:23,680
partitions that that particular consumer

00:13:22,000 --> 00:13:26,320
in the group was responsible for are

00:13:23,680 --> 00:13:28,240
taken away and reassigned to the other

00:13:26,320 --> 00:13:29,519
active members of the group in this case

00:13:28,240 --> 00:13:30,399
it's very simple we just had two

00:13:29,519 --> 00:13:32,560
consumers

00:13:30,399 --> 00:13:33,920
so the one consumer that failed it's

00:13:32,560 --> 00:13:35,040
going to get assigned the topic

00:13:33,920 --> 00:13:39,040
partition

00:13:35,040 --> 00:13:39,040
of the member that failed

00:13:42,000 --> 00:13:49,760
okay so now let's move on to

00:13:46,800 --> 00:13:50,480
i'm gonna introduce kafka streams what

00:13:49,760 --> 00:13:54,000
is

00:13:50,480 --> 00:13:54,959
kafka streams uh it's an application it

00:13:54,000 --> 00:13:57,279
does not run

00:13:54,959 --> 00:13:59,279
inside the brokers it's at the end of

00:13:57,279 --> 00:13:59,920
the day you build a jar file and deploy

00:13:59,279 --> 00:14:03,279
it

00:13:59,920 --> 00:14:06,720
um one of the configuration parameters

00:14:03,279 --> 00:14:10,240
is the

00:14:06,720 --> 00:14:11,199
bootstrap url for the broker or brokers

00:14:10,240 --> 00:14:13,839
you can

00:14:11,199 --> 00:14:15,040
supply multiple so it's going to connect

00:14:13,839 --> 00:14:16,880
but

00:14:15,040 --> 00:14:18,079
embedded within a kafka stream's

00:14:16,880 --> 00:14:20,079
application

00:14:18,079 --> 00:14:21,680
it has a consumer and a producer but

00:14:20,079 --> 00:14:22,320
that's abstracted away and we'll talk

00:14:21,680 --> 00:14:24,959
about that

00:14:22,320 --> 00:14:26,720
a little bit later how many you have but

00:14:24,959 --> 00:14:29,839
the key point is this is not running

00:14:26,720 --> 00:14:31,199
inside of a broker it's external and you

00:14:29,839 --> 00:14:34,720
can have

00:14:31,199 --> 00:14:37,120
uh analogous to a consumer having a

00:14:34,720 --> 00:14:39,519
group id uh you can have multiple

00:14:37,120 --> 00:14:42,880
streams applications that have the same

00:14:39,519 --> 00:14:45,120
application id so if we went if we were

00:14:42,880 --> 00:14:47,279
just thinking quickly about

00:14:45,120 --> 00:14:48,880
a topic with three partitions if you

00:14:47,279 --> 00:14:49,760
spun up three copter streams

00:14:48,880 --> 00:14:53,120
applications

00:14:49,760 --> 00:14:55,760
with the same app id then all three of

00:14:53,120 --> 00:14:58,800
those would be assigned one partition

00:14:55,760 --> 00:15:00,959
it's a process one and so what this

00:14:58,800 --> 00:15:02,639
what this does for us this leads us to a

00:15:00,959 --> 00:15:04,880
unique feature

00:15:02,639 --> 00:15:05,920
um usually when you're doing it when you

00:15:04,880 --> 00:15:08,079
have

00:15:05,920 --> 00:15:08,079
a

00:15:10,560 --> 00:15:17,360
when you have a um distributed system

00:15:15,440 --> 00:15:19,040
to expand it to add a node or take a

00:15:17,360 --> 00:15:20,480
node usually involves like

00:15:19,040 --> 00:15:22,160
stopping when you have a processing

00:15:20,480 --> 00:15:25,440
cluster you have to stop it

00:15:22,160 --> 00:15:28,160
at a node or uh to remove a new

00:15:25,440 --> 00:15:30,079
with kafka streams because under the

00:15:28,160 --> 00:15:32,000
covers it's using consumers it relies on

00:15:30,079 --> 00:15:35,040
the consumer rebalance protocol

00:15:32,000 --> 00:15:38,399
so again with our simple example here

00:15:35,040 --> 00:15:41,440
you have a single app

00:15:38,399 --> 00:15:42,959
and there's three partitions uh your

00:15:41,440 --> 00:15:45,839
the number of records the volumes of

00:15:42,959 --> 00:15:48,000
records increases and you wanna

00:15:45,839 --> 00:15:49,120
improve your uh processing you need to

00:15:48,000 --> 00:15:52,079
ramp up

00:15:49,120 --> 00:15:53,600
you just simply spin up two other

00:15:52,079 --> 00:15:55,360
applications

00:15:53,600 --> 00:15:56,800
rebounds occurs and then each

00:15:55,360 --> 00:15:58,240
application is going to be assigned the

00:15:56,800 --> 00:16:00,639
topic partition

00:15:58,240 --> 00:16:02,399
and then the converse is true uh

00:16:00,639 --> 00:16:04,160
processing slow you don't need all these

00:16:02,399 --> 00:16:07,360
applications running

00:16:04,160 --> 00:16:09,680
you would simply take down two of the

00:16:07,360 --> 00:16:11,839
app instances shut them down gracefully

00:16:09,680 --> 00:16:13,600
rebalance occurs again and now you have

00:16:11,839 --> 00:16:23,839
a single app processing

00:16:13,600 --> 00:16:23,839
all three topic partitions

00:16:26,240 --> 00:16:31,360
and one other unique thing about kafka

00:16:28,480 --> 00:16:34,639
streams is

00:16:31,360 --> 00:16:36,800
it allows for uh well

00:16:34,639 --> 00:16:38,320
let me take a step back so typically you

00:16:36,800 --> 00:16:39,920
have a processing cluster

00:16:38,320 --> 00:16:42,000
you're ingesting records from conquering

00:16:39,920 --> 00:16:44,720
your processing you start pushing those

00:16:42,000 --> 00:16:46,320
results out to an external database

00:16:44,720 --> 00:16:48,000
and naturally you want to view those

00:16:46,320 --> 00:16:49,519
results so you might have some sort of

00:16:48,000 --> 00:16:51,759
dashboard application

00:16:49,519 --> 00:16:54,639
that's going to repeatedly query that

00:16:51,759 --> 00:16:58,160
database and poll for update you know

00:16:54,639 --> 00:17:01,120
select updates that are happening

00:16:58,160 --> 00:17:05,439
on the as a result of your processing

00:17:01,120 --> 00:17:10,480
kafka streams has the notion of

00:17:05,439 --> 00:17:10,480
interactive queries that

00:17:14,799 --> 00:17:19,120
interactive queries what that allows you

00:17:17,120 --> 00:17:22,400
to do kafka streams has

00:17:19,120 --> 00:17:24,480
um stateful applications or stateful

00:17:22,400 --> 00:17:28,079
operations

00:17:24,480 --> 00:17:28,720
and you can expose those you can expose

00:17:28,079 --> 00:17:30,640
the

00:17:28,720 --> 00:17:33,440
stored in the state store by default

00:17:30,640 --> 00:17:35,760
kafka streams uses rocksdb

00:17:33,440 --> 00:17:37,520
for persistence and you can expose that

00:17:35,760 --> 00:17:38,400
we all as kafka streams also has in

00:17:37,520 --> 00:17:40,799
memory stores

00:17:38,400 --> 00:17:41,440
and those can be exposed for quartering

00:17:40,799 --> 00:17:43,840
as well

00:17:41,440 --> 00:17:45,840
but you can expose that store and then

00:17:43,840 --> 00:17:47,760
you can query directly so you can as

00:17:45,840 --> 00:17:49,200
your street as your stream processing as

00:17:47,760 --> 00:17:51,440
your event streams

00:17:49,200 --> 00:17:52,880
are doing their thing you can query and

00:17:51,440 --> 00:17:55,440
get that state

00:17:52,880 --> 00:17:57,280
so that kind of that changes the

00:17:55,440 --> 00:17:58,880
architecture needs a little bit of not

00:17:57,280 --> 00:18:00,880
having to have

00:17:58,880 --> 00:18:02,880
an external data not that it would

00:18:00,880 --> 00:18:04,640
completely replace an external database

00:18:02,880 --> 00:18:07,280
but in some cases for a dashboard

00:18:04,640 --> 00:18:10,480
application you don't need that other

00:18:07,280 --> 00:18:12,840
tier you can just directly query the

00:18:10,480 --> 00:18:14,080
state store of your streaming

00:18:12,840 --> 00:18:18,400
application

00:18:14,080 --> 00:18:21,120
so now i'd like to cover kafka streams

00:18:18,400 --> 00:18:21,120
apis

00:18:24,160 --> 00:18:27,840
the first idea in the conference

00:18:26,480 --> 00:18:30,640
exchange api

00:18:27,840 --> 00:18:32,160
is that of a k stream case stream isn't

00:18:30,640 --> 00:18:35,039
under well it's a stream

00:18:32,160 --> 00:18:36,640
kafka streams deals with uh event

00:18:35,039 --> 00:18:38,960
streams so the records are always

00:18:36,640 --> 00:18:43,840
infinite it's just a non-ending

00:18:38,960 --> 00:18:43,840
stream of records

00:18:47,840 --> 00:18:52,720
and what we have is with the difference

00:18:51,440 --> 00:18:56,559
with the case stream

00:18:52,720 --> 00:18:58,640
we consider it a record stream is that

00:18:56,559 --> 00:19:00,000
even if you have we've got three we've

00:18:58,640 --> 00:19:02,320
got four records here three of them have

00:19:00,000 --> 00:19:04,480
the same key

00:19:02,320 --> 00:19:06,480
those are considered unrelated in an

00:19:04,480 --> 00:19:07,520
event in a record stream even though

00:19:06,480 --> 00:19:09,039
they have the same key

00:19:07,520 --> 00:19:12,720
they're independent of each other and

00:19:09,039 --> 00:19:12,720
that is the idea behind the case stream

00:19:13,600 --> 00:19:19,600
but then we also had the

00:19:16,799 --> 00:19:21,200
analog to that is an update stream which

00:19:19,600 --> 00:19:24,720
is the k table

00:19:21,200 --> 00:19:26,000
and and with an update stream the keys

00:19:24,720 --> 00:19:28,240
do matter

00:19:26,000 --> 00:19:30,720
later keys become updates for the

00:19:28,240 --> 00:19:32,320
previous record so in this case

00:19:30,720 --> 00:19:34,320
if you were to query for the latest

00:19:32,320 --> 00:19:36,240
record you would get

00:19:34,320 --> 00:19:38,240
this one right here because it replaced

00:19:36,240 --> 00:19:41,440
the other each one that comes in

00:19:38,240 --> 00:19:43,440
replaces um

00:19:41,440 --> 00:19:46,000
replaces the previous record with the

00:19:43,440 --> 00:19:46,000
same key

00:19:46,160 --> 00:19:50,640
now i'd like to show i'd like to talk a

00:19:48,880 --> 00:19:52,480
little bit about the code of building a

00:19:50,640 --> 00:19:55,200
coffee strings application

00:19:52,480 --> 00:19:57,840
but before we do that i want to this is

00:19:55,200 --> 00:20:00,000
going to be a very simple application

00:19:57,840 --> 00:20:01,440
we're going to use i'm going to show you

00:20:00,000 --> 00:20:02,640
life before competitions how would you

00:20:01,440 --> 00:20:04,000
do this with plain producers and

00:20:02,640 --> 00:20:05,840
consumers

00:20:04,000 --> 00:20:07,200
and hopefully the point being showing

00:20:05,840 --> 00:20:09,200
you uh

00:20:07,200 --> 00:20:10,240
how kafka streams makes things a bit

00:20:09,200 --> 00:20:11,919
easier for you

00:20:10,240 --> 00:20:13,440
so we're just going to have real simple

00:20:11,919 --> 00:20:16,720
uh application here's your

00:20:13,440 --> 00:20:19,200
main method and we'll get to

00:20:16,720 --> 00:20:20,480
what these counter and set interval

00:20:19,200 --> 00:20:21,919
variables are four

00:20:20,480 --> 00:20:23,200
but you're just going to group you're

00:20:21,919 --> 00:20:24,640
just going to do count and you're going

00:20:23,200 --> 00:20:26,080
to group by key and just do a simple

00:20:24,640 --> 00:20:27,679
count

00:20:26,080 --> 00:20:30,159
so you create your consumer you create a

00:20:27,679 --> 00:20:34,799
producer and then you subscribe

00:20:30,159 --> 00:20:36,480
to your topics a and b and then just

00:20:34,799 --> 00:20:38,640
basically you're just off and running

00:20:36,480 --> 00:20:41,440
you're going to consume your records

00:20:38,640 --> 00:20:43,039
and then as records come in you just

00:20:41,440 --> 00:20:46,080
basically loop over

00:20:43,039 --> 00:20:47,679
uh your hashmap and then just

00:20:46,080 --> 00:20:49,840
increment the count per key pretty

00:20:47,679 --> 00:20:49,840
simple

00:20:50,559 --> 00:20:54,480
but it's not really useful unless you

00:20:52,640 --> 00:20:55,840
share it with the outside world so

00:20:54,480 --> 00:20:58,320
what you want to do and this is where

00:20:55,840 --> 00:21:01,360
our counter and send interval come in

00:20:58,320 --> 00:21:04,400
is every so often uh you

00:21:01,360 --> 00:21:07,919
want to share these

00:21:04,400 --> 00:21:10,240
uh you want to share these counts

00:21:07,919 --> 00:21:11,919
so you're going to when you hit the when

00:21:10,240 --> 00:21:13,120
you hit the number of required records

00:21:11,919 --> 00:21:16,000
or it could be by time

00:21:13,120 --> 00:21:18,159
you're going to loop over your map and

00:21:16,000 --> 00:21:20,480
just create producer records

00:21:18,159 --> 00:21:22,000
uh and then add the key and the value

00:21:20,480 --> 00:21:24,880
and then send that out

00:21:22,000 --> 00:21:27,039
so this is how you would do that under

00:21:24,880 --> 00:21:28,640
and i've left a few details out

00:21:27,039 --> 00:21:31,440
configuration and other things but

00:21:28,640 --> 00:21:36,159
that's how you would do this via

00:21:31,440 --> 00:21:39,679
kafka streams now how you would do this

00:21:36,159 --> 00:21:43,200
with doctor streams the dsl

00:21:39,679 --> 00:21:44,720
okay this is an example of the dsl api

00:21:43,200 --> 00:21:47,120
again i've left out some details here

00:21:44,720 --> 00:21:48,640
but you create a stream builder instance

00:21:47,120 --> 00:21:51,200
and then you call stream on it and then

00:21:48,640 --> 00:21:52,159
you pass in a collection of topics and

00:21:51,200 --> 00:21:54,880
this is your

00:21:52,159 --> 00:21:56,400
this is analogous to the subscribe then

00:21:54,880 --> 00:21:57,679
you take that stream and you just start

00:21:56,400 --> 00:21:59,679
doing it you just start

00:21:57,679 --> 00:22:00,799
putting operations on it you want to

00:21:59,679 --> 00:22:04,159
group by key

00:22:00,799 --> 00:22:06,799
you want to count and this materialized

00:22:04,159 --> 00:22:08,559
count store this expo and i talked spoke

00:22:06,799 --> 00:22:11,039
about interactive queries before

00:22:08,559 --> 00:22:12,400
this exposes it by calling materialize

00:22:11,039 --> 00:22:14,400
giving it a name

00:22:12,400 --> 00:22:16,080
that's a signal that i want to expose

00:22:14,400 --> 00:22:20,000
this for

00:22:16,080 --> 00:22:22,640
query now count returns i mentioned in

00:22:20,000 --> 00:22:25,200
update stream account returns a k table

00:22:22,640 --> 00:22:27,440
because every time you do a count you're

00:22:25,200 --> 00:22:29,280
going to update the previous record

00:22:27,440 --> 00:22:31,120
now to send this back out we want to

00:22:29,280 --> 00:22:34,640
turn this into event stream

00:22:31,120 --> 00:22:37,919
so we call two stream and then you

00:22:34,640 --> 00:22:39,760
call two which is the two operation

00:22:37,919 --> 00:22:41,120
two method and you just have your output

00:22:39,760 --> 00:22:44,480
topic there

00:22:41,120 --> 00:22:47,520
so this is just just nice concise

00:22:44,480 --> 00:22:50,400
uh four or five lines of code

00:22:47,520 --> 00:22:50,400
and then this is your

00:22:51,120 --> 00:22:56,559
um this is your

00:22:54,400 --> 00:22:58,559
stream's application now this is an

00:22:56,559 --> 00:23:02,320
example of the dsl the dsl

00:22:58,559 --> 00:23:02,320
is um

00:23:06,720 --> 00:23:11,360
the dsl it gives you the most

00:23:08,640 --> 00:23:11,360
flexibility

00:23:11,440 --> 00:23:16,240
uh but if you need to deviate outside of

00:23:14,960 --> 00:23:17,600
the dsl

00:23:16,240 --> 00:23:19,520
every if you can do everything within

00:23:17,600 --> 00:23:21,120
the dsl x frame sometimes you have to go

00:23:19,520 --> 00:23:24,400
outside the dsl

00:23:21,120 --> 00:23:25,679
so kafka streams offers the processor

00:23:24,400 --> 00:23:29,280
api

00:23:25,679 --> 00:23:32,559
the trade-off with that being

00:23:29,280 --> 00:23:35,919
that with the dsl you get

00:23:32,559 --> 00:23:36,880
maximum flexibility programming wise and

00:23:35,919 --> 00:23:39,280
you saw it's just

00:23:36,880 --> 00:23:40,240
it's a you know fluent interface just

00:23:39,280 --> 00:23:42,080
with a few

00:23:40,240 --> 00:23:43,679
keystrokes you can get a streaming

00:23:42,080 --> 00:23:46,720
application running

00:23:43,679 --> 00:23:48,320
uh with the processor api and all i

00:23:46,720 --> 00:23:50,080
guess i should say the dsl does all the

00:23:48,320 --> 00:23:50,799
wiring up because we'll talk about this

00:23:50,080 --> 00:23:53,360
in a second

00:23:50,799 --> 00:23:54,720
but under the covers kafka streams uh

00:23:53,360 --> 00:23:57,919
generates a

00:23:54,720 --> 00:23:59,039
a dag directed acycle graph of

00:23:57,919 --> 00:24:00,799
processors

00:23:59,039 --> 00:24:02,480
with the processor api you've got

00:24:00,799 --> 00:24:03,200
maximum flexibility you can do whatever

00:24:02,480 --> 00:24:04,960
you want

00:24:03,200 --> 00:24:08,159
but you have to do all the wiring so

00:24:04,960 --> 00:24:10,400
here you create a topology object

00:24:08,159 --> 00:24:11,360
uh and this is actually this word count

00:24:10,400 --> 00:24:14,400
process for demo

00:24:11,360 --> 00:24:16,799
is from kafka streams examples

00:24:14,400 --> 00:24:18,320
but you add a source you're adding so

00:24:16,799 --> 00:24:20,720
this is your source topic and you have a

00:24:18,320 --> 00:24:22,880
source

00:24:20,720 --> 00:24:24,320
next you're going to add a processor and

00:24:22,880 --> 00:24:27,440
you always add you

00:24:24,320 --> 00:24:30,640
you use this with

00:24:27,440 --> 00:24:32,720
um you give it a name

00:24:30,640 --> 00:24:34,080
give the processor name processor

00:24:32,720 --> 00:24:36,080
supplier and

00:24:34,080 --> 00:24:37,520
the suppliers should always return a new

00:24:36,080 --> 00:24:40,799
instance of this of

00:24:37,520 --> 00:24:41,840
the uh processor implementation and then

00:24:40,799 --> 00:24:44,240
you specify

00:24:41,840 --> 00:24:45,600
a parent so basically what this is

00:24:44,240 --> 00:24:48,080
saying is over here

00:24:45,600 --> 00:24:49,840
was the name of source source but here

00:24:48,080 --> 00:24:51,760
you're saying it's it's parent

00:24:49,840 --> 00:24:54,000
so the importance of that relationship

00:24:51,760 --> 00:24:54,559
is everything that comes into the source

00:24:54,000 --> 00:24:56,640
node

00:24:54,559 --> 00:24:58,799
is going to get forward to the process

00:24:56,640 --> 00:24:59,600
now and i've got a simple example here

00:24:58,799 --> 00:25:01,520
that only has

00:24:59,600 --> 00:25:03,200
one parent child but you could have

00:25:01,520 --> 00:25:05,520
multiple children

00:25:03,200 --> 00:25:06,640
that source could have multiple children

00:25:05,520 --> 00:25:08,320
and it would just forward all the

00:25:06,640 --> 00:25:10,000
records to each processor that it's a

00:25:08,320 --> 00:25:12,880
child up

00:25:10,000 --> 00:25:14,000
so next we're going to add a state store

00:25:12,880 --> 00:25:16,400
and here we're specifying

00:25:14,000 --> 00:25:17,600
in memory stage store but again we're

00:25:16,400 --> 00:25:20,080
exposing it

00:25:17,600 --> 00:25:22,720
for queries by calling by giving it the

00:25:20,080 --> 00:25:22,720
name counts

00:25:23,120 --> 00:25:27,279
and we're saying here it's not so much a

00:25:25,520 --> 00:25:28,799
parent-child relationship as we're

00:25:27,279 --> 00:25:32,400
giving the name

00:25:28,799 --> 00:25:35,679
or names this is of our org

00:25:32,400 --> 00:25:38,799
parameter a name or names of store

00:25:35,679 --> 00:25:43,840
of processors that would have access to

00:25:38,799 --> 00:25:43,840
this store

00:25:45,200 --> 00:25:52,159
and then finally we add a sink

00:25:49,120 --> 00:25:54,080
which is just writing the records out to

00:25:52,159 --> 00:25:56,159
an output topic

00:25:54,080 --> 00:25:57,120
and again we see the parent-child

00:25:56,159 --> 00:25:59,760
relationship

00:25:57,120 --> 00:26:01,600
here's the name of the parent or parents

00:25:59,760 --> 00:26:02,880
uh a single

00:26:01,600 --> 00:26:04,960
processor known could have multiple

00:26:02,880 --> 00:26:08,720
parents that feed into it

00:26:04,960 --> 00:26:11,840
um and that is so you can see that's

00:26:08,720 --> 00:26:13,600
not too much extra code but if you've

00:26:11,840 --> 00:26:15,440
got a lot of processing nodes if you've

00:26:13,600 --> 00:26:17,279
got something more complicated

00:26:15,440 --> 00:26:18,720
it can involve a lot because you're

00:26:17,279 --> 00:26:20,240
doing all the wiring yourself

00:26:18,720 --> 00:26:22,400
but again it gives you maximum

00:26:20,240 --> 00:26:26,159
flexibility

00:26:22,400 --> 00:26:28,640
now for the cat times that the dsl

00:26:26,159 --> 00:26:30,880
pretty much does say 90 of what you want

00:26:28,640 --> 00:26:33,440
but you've got one thing you want

00:26:30,880 --> 00:26:35,760
there's just not an operator uh kafka

00:26:33,440 --> 00:26:37,640
streams give you the ability

00:26:35,760 --> 00:26:40,159
to

00:26:37,640 --> 00:26:43,279
[Music]

00:26:40,159 --> 00:26:44,480
mix the two mix the two approaches you

00:26:43,279 --> 00:26:46,080
can have a dsl

00:26:44,480 --> 00:26:48,559
and then you can add a processor

00:26:46,080 --> 00:26:51,600
operation via

00:26:48,559 --> 00:26:55,200
one of the transform methods or process

00:26:51,600 --> 00:26:56,640
so here we have um

00:26:55,200 --> 00:26:58,960
you know we're doing our standard group

00:26:56,640 --> 00:27:00,720
by key count then once we call we

00:26:58,960 --> 00:27:03,279
convert it back to a record stream

00:27:00,720 --> 00:27:06,159
we're adding a transform on it and this

00:27:03,279 --> 00:27:07,039
is the trend a transformer of the

00:27:06,159 --> 00:27:10,480
transform

00:27:07,039 --> 00:27:11,840
interface has an init a process and a

00:27:10,480 --> 00:27:14,000
closed method

00:27:11,840 --> 00:27:16,080
and you init is just where you would do

00:27:14,000 --> 00:27:19,440
any kind of setup work

00:27:16,080 --> 00:27:20,799
but the process method is where you do

00:27:19,440 --> 00:27:22,640
whatever it is you want to do in that

00:27:20,799 --> 00:27:23,840
processor and that's where the maximum

00:27:22,640 --> 00:27:26,080
flexibility comes in

00:27:23,840 --> 00:27:28,080
you can just do whatever you need to in

00:27:26,080 --> 00:27:31,760
that processor

00:27:28,080 --> 00:27:34,880
an interesting use case for this

00:27:31,760 --> 00:27:38,000
recently came about is twitter uses

00:27:34,880 --> 00:27:40,159
um kafka streams for

00:27:38,000 --> 00:27:41,120
part of its it's got a machine learning

00:27:40,159 --> 00:27:44,080
pipeline

00:27:41,120 --> 00:27:45,919
where it keeps track of user preferences

00:27:44,080 --> 00:27:46,640
and things for i don't remember all the

00:27:45,919 --> 00:27:49,679
details

00:27:46,640 --> 00:27:51,520
but using kafka streams to keep track of

00:27:49,679 --> 00:27:53,440
uh for the machine learning pipeline for

00:27:51,520 --> 00:27:55,600
user preferences

00:27:53,440 --> 00:27:58,320
uh and to do with join they had an

00:27:55,600 --> 00:27:58,320
instance where

00:27:58,720 --> 00:28:03,039
the model data would come in if i'm

00:28:01,440 --> 00:28:04,799
getting this correctly

00:28:03,039 --> 00:28:06,720
certain data would come in and then tags

00:28:04,799 --> 00:28:09,200
would come in the tags always lagged

00:28:06,720 --> 00:28:10,399
could lag behind so they needed to be

00:28:09,200 --> 00:28:13,679
able to

00:28:10,399 --> 00:28:16,320
do a join but have it wait do a join and

00:28:13,679 --> 00:28:17,919
then have it set it aside

00:28:16,320 --> 00:28:19,440
to not forward it because you really

00:28:17,919 --> 00:28:21,120
don't want some of that user data to

00:28:19,440 --> 00:28:25,279
come in without a tag

00:28:21,120 --> 00:28:27,760
so the solution was to integrate

00:28:25,279 --> 00:28:29,600
the stream's dsl with the processor

00:28:27,760 --> 00:28:32,159
integration and then within

00:28:29,600 --> 00:28:33,360
those transform methods that i think

00:28:32,159 --> 00:28:35,360
they use transform

00:28:33,360 --> 00:28:36,559
uh they built their custom the

00:28:35,360 --> 00:28:40,399
operations around their

00:28:36,559 --> 00:28:40,399
state stores to do their custom behavior

00:28:43,039 --> 00:28:48,480
now kafka streams offers

00:28:46,399 --> 00:28:50,399
offers stateless operations you've got

00:28:48,480 --> 00:28:54,000
the standard things you would expect

00:28:50,399 --> 00:28:56,399
filter map uh map values

00:28:54,000 --> 00:28:57,120
um map value you'd use map values over

00:28:56,399 --> 00:28:58,720
map

00:28:57,120 --> 00:29:00,080
if you're not going to change the key

00:28:58,720 --> 00:29:01,200
value page you're just going to change

00:29:00,080 --> 00:29:04,559
value

00:29:01,200 --> 00:29:06,559
transform and then k table has some

00:29:04,559 --> 00:29:08,799
methods as well filtered group by map

00:29:06,559 --> 00:29:12,240
values

00:29:08,799 --> 00:29:15,600
now there's also stateful operations

00:29:12,240 --> 00:29:19,039
uh we saw group by key count and before

00:29:15,600 --> 00:29:20,640
but there's also a reduce in aggregate

00:29:19,039 --> 00:29:23,840
and then you also have

00:29:20,640 --> 00:29:25,520
group by methods on a k table

00:29:23,840 --> 00:29:27,760
but you would have to change the key

00:29:25,520 --> 00:29:28,640
because k tables by default have to have

00:29:27,760 --> 00:29:31,679
a key

00:29:28,640 --> 00:29:34,799
but there's also stateful operations on

00:29:31,679 --> 00:29:36,480
a k table as well now some of the more

00:29:34,799 --> 00:29:40,159
interesting

00:29:36,480 --> 00:29:42,399
stateful operations

00:29:40,159 --> 00:29:43,200
uh you can have a stream stream join

00:29:42,399 --> 00:29:45,279
you've got two

00:29:43,200 --> 00:29:46,799
you've got two streams and it's pretty

00:29:45,279 --> 00:29:48,240
straightforward you have

00:29:46,799 --> 00:29:50,640
you're calling left stream join on the

00:29:48,240 --> 00:29:52,080
right stream you give a value joiner

00:29:50,640 --> 00:29:52,799
because they have to have the same key

00:29:52,080 --> 00:29:55,039
so you don't

00:29:52,799 --> 00:29:58,080
worry about specifying the key you have

00:29:55,039 --> 00:30:00,240
a value joiner which is where

00:29:58,080 --> 00:30:02,399
what is the result of joining these two

00:30:00,240 --> 00:30:03,440
records together it could be a brand new

00:30:02,399 --> 00:30:06,080
type

00:30:03,440 --> 00:30:08,000
whatever it's up to you what it does and

00:30:06,080 --> 00:30:11,039
then you give it a join windows of 10

00:30:08,000 --> 00:30:11,039
seconds so what that

00:30:11,279 --> 00:30:15,440
let me before i explain what joint

00:30:12,559 --> 00:30:18,960
windows are you could have an inner join

00:30:15,440 --> 00:30:20,640
a left outer join or just an outer join

00:30:18,960 --> 00:30:23,200
an outer join

00:30:20,640 --> 00:30:24,240
uh if a record doesn't show up on either

00:30:23,200 --> 00:30:26,880
side within

00:30:24,240 --> 00:30:27,520
the given time of the join windows it'll

00:30:26,880 --> 00:30:30,000
emit

00:30:27,520 --> 00:30:31,679
a record uh inner join only emits if

00:30:30,000 --> 00:30:34,640
there's a match on both sides

00:30:31,679 --> 00:30:37,200
and left outer as you expect if there's

00:30:34,640 --> 00:30:40,320
no match from on the right side it

00:30:37,200 --> 00:30:41,919
admits the left side record uh join

00:30:40,320 --> 00:30:45,279
windows of 10 seconds

00:30:41,919 --> 00:30:48,399
what that means is kind of gives you a

00:30:45,279 --> 00:30:51,360
if a record comes in either 10 seconds

00:30:48,399 --> 00:30:53,279
before or after that record so that

00:30:51,360 --> 00:30:54,880
that's your window of time and you can

00:30:53,279 --> 00:30:57,919
actually even

00:30:54,880 --> 00:30:59,760
get more specific with um

00:30:57,919 --> 00:31:01,840
joining windows gives you a more

00:30:59,760 --> 00:31:06,320
specificity more granularity

00:31:01,840 --> 00:31:08,640
with compared to um

00:31:06,320 --> 00:31:11,679
how soon how like how pre like how early

00:31:08,640 --> 00:31:15,360
or how much after it can come

00:31:11,679 --> 00:31:17,120
then we have stream table joins um

00:31:15,360 --> 00:31:18,720
pretty straightforward and then here you

00:31:17,120 --> 00:31:19,120
don't give a join window the stream

00:31:18,720 --> 00:31:21,840
table

00:31:19,120 --> 00:31:24,320
join is only triggered by the stream

00:31:21,840 --> 00:31:27,679
side so if updates come to the

00:31:24,320 --> 00:31:29,519
uh k table it won't trigger it won't

00:31:27,679 --> 00:31:30,960
emit a result it's the stream table

00:31:29,519 --> 00:31:32,640
joins are only

00:31:30,960 --> 00:31:35,360
there's talk about changing this but

00:31:32,640 --> 00:31:37,760
right now and

00:31:35,360 --> 00:31:38,799
result is only triggered by something

00:31:37,760 --> 00:31:44,399
coming on the stream

00:31:38,799 --> 00:31:44,399
side and then we've got a table table

00:31:44,840 --> 00:31:49,360
joint

00:31:46,320 --> 00:31:52,799
now within stateful operations

00:31:49,360 --> 00:31:54,000
usually you know just getting a count

00:31:52,799 --> 00:31:56,159
you want

00:31:54,000 --> 00:31:58,000
just getting a plain count isn't always

00:31:56,159 --> 00:32:00,000
interesting you want to know like

00:31:58,000 --> 00:32:01,760
time parameters when did these things

00:32:00,000 --> 00:32:04,159
happen

00:32:01,760 --> 00:32:06,240
um now one thing i've mentioned time

00:32:04,159 --> 00:32:09,840
stamps before

00:32:06,240 --> 00:32:11,919
um this time is is essential

00:32:09,840 --> 00:32:14,080
to kafka streams and for windowing

00:32:11,919 --> 00:32:16,240
driving the events

00:32:14,080 --> 00:32:16,240
so

00:32:17,440 --> 00:32:21,600
you set you have a you have a time stamp

00:32:19,919 --> 00:32:24,000
on your record when you produce it

00:32:21,600 --> 00:32:25,760
now you could have a different time

00:32:24,000 --> 00:32:27,760
stamp and better than the value

00:32:25,760 --> 00:32:29,600
kafka streams provides a time stamp

00:32:27,760 --> 00:32:32,559
extractor so if you want to use

00:32:29,600 --> 00:32:35,120
an embedded timestamp within the record

00:32:32,559 --> 00:32:36,559
you're sending

00:32:35,120 --> 00:32:38,640
this is you would use a timestamp

00:32:36,559 --> 00:32:39,840
extractor now the key thing is about

00:32:38,640 --> 00:32:41,919
timestamps is

00:32:39,840 --> 00:32:43,840
kafka streams is driven by the

00:32:41,919 --> 00:32:45,519
timestamps on their records not wall

00:32:43,840 --> 00:32:47,360
clock time

00:32:45,519 --> 00:32:49,840
so it's only moving and processing

00:32:47,360 --> 00:32:52,960
forward as long as the time stamps

00:32:49,840 --> 00:32:58,080
on the records are increasing

00:32:52,960 --> 00:32:58,080
um kafka streams has a notion of

00:32:58,720 --> 00:33:02,159
stream time which is the highest time

00:33:00,320 --> 00:33:05,200
stamp and seen so far

00:33:02,159 --> 00:33:06,159
now for windowing uh we've got hopping

00:33:05,200 --> 00:33:09,200
windows

00:33:06,159 --> 00:33:10,559
where you've got a window you define a

00:33:09,200 --> 00:33:13,440
windows size

00:33:10,559 --> 00:33:14,720
and how far in an advanced time so here

00:33:13,440 --> 00:33:16,720
we've got a minute window and it's going

00:33:14,720 --> 00:33:21,039
to advance by 30 seconds

00:33:16,720 --> 00:33:22,640
hopping windows give you um

00:33:21,039 --> 00:33:25,679
what will give you overlapping results

00:33:22,640 --> 00:33:28,399
because here we see in our first window

00:33:25,679 --> 00:33:30,240
we've got all of these five results it

00:33:28,399 --> 00:33:32,080
advances 30 seconds

00:33:30,240 --> 00:33:34,080
two of the results from the previous

00:33:32,080 --> 00:33:35,679
window are included and so this

00:33:34,080 --> 00:33:37,519
so you're going to have overlapping

00:33:35,679 --> 00:33:40,399
results

00:33:37,519 --> 00:33:41,600
the other type of window we have are

00:33:40,399 --> 00:33:43,519
tumbling windows

00:33:41,600 --> 00:33:44,880
tumbling windows are just a special case

00:33:43,519 --> 00:33:47,039
of hopping windows

00:33:44,880 --> 00:33:48,880
where the windows size where the where

00:33:47,039 --> 00:33:52,000
the slot the advanced time

00:33:48,880 --> 00:33:54,559
is the same as the windows slide window

00:33:52,000 --> 00:33:55,519
size excuse me so you're not going to

00:33:54,559 --> 00:33:58,559
have

00:33:55,519 --> 00:34:01,120
overlapping records

00:33:58,559 --> 00:34:02,320
so each window is going to have distinct

00:34:01,120 --> 00:34:05,360
records

00:34:02,320 --> 00:34:06,640
now there's a new type of window that

00:34:05,360 --> 00:34:09,200
is going to be coming out it's not

00:34:06,640 --> 00:34:10,079
available right now it's going to be

00:34:09,200 --> 00:34:12,879
coming out in two

00:34:10,079 --> 00:34:14,000
seven it's the notion of sliding windows

00:34:12,879 --> 00:34:16,720
now you could

00:34:14,000 --> 00:34:18,240
simulate this with hopping windows uh

00:34:16,720 --> 00:34:21,839
let me back this up sliding windows by

00:34:18,240 --> 00:34:21,839
default advance one millisecond

00:34:23,040 --> 00:34:26,159
you could do this with hopping windows

00:34:24,720 --> 00:34:27,040
but as i you could see from the previous

00:34:26,159 --> 00:34:30,560
slide

00:34:27,040 --> 00:34:32,079
you would end up with many many windows

00:34:30,560 --> 00:34:33,440
with the exact same results

00:34:32,079 --> 00:34:35,119
and you'd be doing these calculations

00:34:33,440 --> 00:34:35,839
over and over and over again on the same

00:34:35,119 --> 00:34:38,720
results

00:34:35,839 --> 00:34:40,320
sliding windows guarantees you in each

00:34:38,720 --> 00:34:43,679
window it's going to have a unique

00:34:40,320 --> 00:34:45,839
set of records there might be records

00:34:43,679 --> 00:34:48,560
from the same window

00:34:45,839 --> 00:34:49,919
in there but each window is going to be

00:34:48,560 --> 00:34:53,919
a unique set

00:34:49,919 --> 00:34:56,399
so here uh record a comes in at time a

00:34:53,919 --> 00:34:57,040
and that sets the it comes in it's got a

00:34:56,399 --> 00:35:00,240
time of

00:34:57,040 --> 00:35:01,200
10 that sets the end window every record

00:35:00,240 --> 00:35:04,560
that comes in

00:35:01,200 --> 00:35:05,680
it's going to create a starter window

00:35:04,560 --> 00:35:09,599
and then they're going to get combined

00:35:05,680 --> 00:35:09,599
as they go along but a comes in

00:35:09,760 --> 00:35:18,160
and is the record now b comes in at 14

00:35:14,839 --> 00:35:19,280
since you just defined a time difference

00:35:18,160 --> 00:35:21,359
of 10 seconds

00:35:19,280 --> 00:35:23,520
a is going to be included in that window

00:35:21,359 --> 00:35:26,000
now and then the same thing when c comes

00:35:23,520 --> 00:35:27,520
in it comes in at time of 16

00:35:26,000 --> 00:35:29,680
you've got 10 seconds so it's going to

00:35:27,520 --> 00:35:31,040
pull in a and b so you

00:35:29,680 --> 00:35:32,960
you're going to have these distinct

00:35:31,040 --> 00:35:34,800
windows and it keeps advancing then it

00:35:32,960 --> 00:35:38,240
gets to the point we only have

00:35:34,800 --> 00:35:38,240
the one record left in it

00:35:40,480 --> 00:35:46,400
and then we have um

00:35:45,040 --> 00:35:47,680
data driven window the other windows are

00:35:46,400 --> 00:35:49,680
driven by time you have the data driven

00:35:47,680 --> 00:35:51,599
window which is session windows

00:35:49,680 --> 00:35:53,680
a session windows you just define an

00:35:51,599 --> 00:35:56,000
inactivity gap

00:35:53,680 --> 00:35:56,960
and as long as records arrive within the

00:35:56,000 --> 00:35:58,240
nativity gun

00:35:56,960 --> 00:36:00,560
the window just keeps running so

00:35:58,240 --> 00:36:01,760
theoretically your window could just

00:36:00,560 --> 00:36:05,280
keep going for an ever and every

00:36:01,760 --> 00:36:08,160
session windows here

00:36:05,280 --> 00:36:09,359
records uh stop coming for a few for the

00:36:08,160 --> 00:36:12,400
length of time greater than

00:36:09,359 --> 00:36:13,680
the inactivity gap so you end up with

00:36:12,400 --> 00:36:16,320
two windows but down here

00:36:13,680 --> 00:36:19,040
this bottom one records keep coming in

00:36:16,320 --> 00:36:23,920
so the window just keeps growing

00:36:19,040 --> 00:36:23,920
now for stateful operation in windows

00:36:25,440 --> 00:36:29,520
you would like to have fault thomas the

00:36:26,960 --> 00:36:32,240
way kafka streams achieves this

00:36:29,520 --> 00:36:34,240
is you have a change log topic that

00:36:32,240 --> 00:36:37,040
backs a state store

00:36:34,240 --> 00:36:37,599
um skipping over some finer details here

00:36:37,040 --> 00:36:40,720
there's

00:36:37,599 --> 00:36:42,400
cash involved but when records are

00:36:40,720 --> 00:36:42,960
written to the state store those records

00:36:42,400 --> 00:36:45,119
are also

00:36:42,960 --> 00:36:46,720
written to the changelog topic and so

00:36:45,119 --> 00:36:49,920
that is your resilience

00:36:46,720 --> 00:36:52,640
for um that state store

00:36:49,920 --> 00:36:54,240
so if you were to that instance where to

00:36:52,640 --> 00:36:56,880
migrate to another machine or you

00:36:54,240 --> 00:36:58,000
lose that machine start up another one

00:36:56,880 --> 00:36:59,839
the state store

00:36:58,000 --> 00:37:01,040
for that and we'll cover tasks in a

00:36:59,839 --> 00:37:05,119
second

00:37:01,040 --> 00:37:06,960
that would um

00:37:05,119 --> 00:37:09,599
start reading would read from the change

00:37:06,960 --> 00:37:11,680
log

00:37:09,599 --> 00:37:13,839
and restore the state to where it was

00:37:11,680 --> 00:37:15,520
from before now this is and now

00:37:13,839 --> 00:37:17,280
this is the advantage again i want to

00:37:15,520 --> 00:37:19,359
say if you're using roxdb if you're

00:37:17,280 --> 00:37:21,280
using persistent state source

00:37:19,359 --> 00:37:23,760
if you were to take a machine down and

00:37:21,280 --> 00:37:25,680
then bring it back up

00:37:23,760 --> 00:37:27,280
it wouldn't it doesn't need to recover

00:37:25,680 --> 00:37:28,480
because it stays permanent and memory

00:37:27,280 --> 00:37:29,359
stores you're always going to read from

00:37:28,480 --> 00:37:31,520
a change log this

00:37:29,359 --> 00:37:33,520
is what the scenario here i'm showing is

00:37:31,520 --> 00:37:38,800
a failure scenario

00:37:33,520 --> 00:37:38,800
now for um

00:37:39,599 --> 00:37:42,640
if someone mentioned in the chat uh

00:37:41,200 --> 00:37:45,920
checkpointing

00:37:42,640 --> 00:37:48,720
what kafka streams has which is

00:37:45,920 --> 00:37:50,640
kind of analogous to that is this notion

00:37:48,720 --> 00:37:52,240
of standby tasks i'll talk about tasks

00:37:50,640 --> 00:37:56,560
in a second

00:37:52,240 --> 00:37:58,160
but you have a task on machine a

00:37:56,560 --> 00:37:59,920
and you've got machine b and you've

00:37:58,160 --> 00:38:01,359
specified your configurations to use

00:37:59,920 --> 00:38:03,200
standby tasks

00:38:01,359 --> 00:38:04,480
well what that is that's a shadow task

00:38:03,200 --> 00:38:05,200
it's not really going to be processing

00:38:04,480 --> 00:38:07,680
records

00:38:05,200 --> 00:38:09,119
but as the task on machine a as it

00:38:07,680 --> 00:38:09,920
writes to the store and it writes the

00:38:09,119 --> 00:38:12,560
change law

00:38:09,920 --> 00:38:13,920
the standby task is consuming and

00:38:12,560 --> 00:38:15,520
building up the state store

00:38:13,920 --> 00:38:17,760
so if you were to have a failure

00:38:15,520 --> 00:38:19,839
scenario

00:38:17,760 --> 00:38:21,839
the standby task on machine b is that

00:38:19,839 --> 00:38:23,839
you take machine a away machine b that

00:38:21,839 --> 00:38:25,440
standby task becomes the primary

00:38:23,839 --> 00:38:26,640
but the state is already caught up you

00:38:25,440 --> 00:38:28,240
don't have to do or it would be a

00:38:26,640 --> 00:38:30,720
minimal amount of state to catch up for

00:38:28,240 --> 00:38:32,720
the change i'm talking

00:38:30,720 --> 00:38:34,160
i'm getting close on time so i'll pick

00:38:32,720 --> 00:38:35,680
up the pace um

00:38:34,160 --> 00:38:37,760
we've met i've mentioned streams

00:38:35,680 --> 00:38:41,119
generating a graph so this

00:38:37,760 --> 00:38:43,040
is our group by um

00:38:41,119 --> 00:38:44,240
application that i had spoken about

00:38:43,040 --> 00:38:46,400
before

00:38:44,240 --> 00:38:47,760
and it generates a graph like we see

00:38:46,400 --> 00:38:50,320
here these are your two

00:38:47,760 --> 00:38:51,119
topics and these are this is the graph

00:38:50,320 --> 00:38:54,560
that

00:38:51,119 --> 00:38:58,240
is generated under the covers um

00:38:54,560 --> 00:39:02,000
by this topology defined here

00:38:58,240 --> 00:39:04,880
now um how are tasks assigned how

00:39:02,000 --> 00:39:06,480
our output our top how are the

00:39:04,880 --> 00:39:09,359
partitions grouped up

00:39:06,480 --> 00:39:11,200
so basically when you have when you

00:39:09,359 --> 00:39:14,240
subscribe to multiple partitions

00:39:11,200 --> 00:39:15,119
multiple topics the number of tasks is

00:39:14,240 --> 00:39:17,359
the max

00:39:15,119 --> 00:39:19,040
of the number of impartitions across all

00:39:17,359 --> 00:39:22,240
the topics so in this case

00:39:19,040 --> 00:39:25,200
we've got topic a and topic b a has four

00:39:22,240 --> 00:39:27,119
partitions b has two so then you're

00:39:25,200 --> 00:39:29,440
going to have four tasks

00:39:27,119 --> 00:39:31,040
and a task is the lowest level is the

00:39:29,440 --> 00:39:33,599
unit of work in the kafka streams

00:39:31,040 --> 00:39:33,599
application

00:39:35,280 --> 00:39:38,320
so since we've got four tasks but six

00:39:37,760 --> 00:39:41,359
import

00:39:38,320 --> 00:39:43,599
partitions how does the assignment work

00:39:41,359 --> 00:39:47,040
how does data get parsed out

00:39:43,599 --> 00:39:49,920
so uh this first task

00:39:47,040 --> 00:39:51,200
is going to get which is zero zero and

00:39:49,920 --> 00:39:52,400
the second number represents the

00:39:51,200 --> 00:39:55,280
partitions

00:39:52,400 --> 00:39:56,720
is it's going to be assigned two

00:39:55,280 --> 00:39:59,520
partitions both with the

00:39:56,720 --> 00:40:00,400
zero partition zeroth partition of our

00:39:59,520 --> 00:40:01,839
topics

00:40:00,400 --> 00:40:03,760
it's gonna have two tests the second one

00:40:01,839 --> 00:40:06,319
is gonna have two tasks of one

00:40:03,760 --> 00:40:07,920
and then the second and the third and

00:40:06,319 --> 00:40:10,079
fourth task are only gonna have one

00:40:07,920 --> 00:40:11,760
topic partition

00:40:10,079 --> 00:40:13,359
that's a unit of work but we need

00:40:11,760 --> 00:40:17,040
execution so

00:40:13,359 --> 00:40:21,200
execution is determined by a concept of

00:40:17,040 --> 00:40:25,040
a stream thread stream threads

00:40:21,200 --> 00:40:26,480
you can have analogous to

00:40:25,040 --> 00:40:29,119
each stream thread is going to have an

00:40:26,480 --> 00:40:31,359
embedded consumer and embedded producer

00:40:29,119 --> 00:40:33,200
so each stream thread in this case we're

00:40:31,359 --> 00:40:36,800
going to have two stream threads

00:40:33,200 --> 00:40:39,440
and this way it's going to

00:40:36,800 --> 00:40:42,800
each have two tasks so you could spin up

00:40:39,440 --> 00:40:46,000
up to four string threads

00:40:42,800 --> 00:40:48,960
and they're you

00:40:46,000 --> 00:40:50,400
each strengthener would have one task uh

00:40:48,960 --> 00:40:51,599
you could take this down to one thread

00:40:50,400 --> 00:40:53,040
and then that thread is going to have

00:40:51,599 --> 00:40:56,560
four tasks

00:40:53,040 --> 00:40:57,839
if you go above four threads

00:40:56,560 --> 00:41:00,240
then you're going to have a thread

00:40:57,839 --> 00:41:03,040
that's idle and this is the example here

00:41:00,240 --> 00:41:07,839
of you expand out to four threads

00:41:03,040 --> 00:41:10,960
and each thread has a task um

00:41:07,839 --> 00:41:13,359
and that is

00:41:10,960 --> 00:41:14,560
going to skip a review i think on that

00:41:13,359 --> 00:41:16,800
time here

00:41:14,560 --> 00:41:18,839
um so i want to thank you all for your

00:41:16,800 --> 00:41:22,240
time

00:41:18,839 --> 00:41:23,920
and here's some resource links

00:41:22,240 --> 00:41:25,599
there's a question there about how this

00:41:23,920 --> 00:41:29,520
compares to flank

00:41:25,599 --> 00:41:33,359
um well they're both stream processing

00:41:29,520 --> 00:41:34,079
um for late arrival i think a covered

00:41:33,359 --> 00:41:35,839
window

00:41:34,079 --> 00:41:38,079
down under dana did i cover when do

00:41:35,839 --> 00:41:41,599
competitions and joins enough

00:41:38,079 --> 00:41:45,119
for you um records that are

00:41:41,599 --> 00:41:48,720
alive arriving late streams

00:41:45,119 --> 00:41:50,560
has a concept of a grace period

00:41:48,720 --> 00:41:52,240
so when you define a window i think i

00:41:50,560 --> 00:41:54,079
had one of my previous slides when you

00:41:52,240 --> 00:41:56,079
define

00:41:54,079 --> 00:41:57,839
a window and the size of the window you

00:41:56,079 --> 00:42:00,240
can define a grace period

00:41:57,839 --> 00:42:01,599
so if you said my window let's just take

00:42:00,240 --> 00:42:03,040
a tumbling window since that's the

00:42:01,599 --> 00:42:04,640
easier case you've got a tumbling window

00:42:03,040 --> 00:42:06,079
of one minute

00:42:04,640 --> 00:42:08,400
you could say i've got a tumbling window

00:42:06,079 --> 00:42:12,560
of one minute but a grace period

00:42:08,400 --> 00:42:14,480
of um 20 seconds so that means if

00:42:12,560 --> 00:42:16,319
something shows up

00:42:14,480 --> 00:42:17,839
that is outside the time window but

00:42:16,319 --> 00:42:18,960
within the grace period it'll be

00:42:17,839 --> 00:42:22,400
included

00:42:18,960 --> 00:42:24,880
and we consider that out of order data

00:42:22,400 --> 00:42:25,440
late arriving records are outside the

00:42:24,880 --> 00:42:29,040
window

00:42:25,440 --> 00:42:30,079
and outside the grace period and those

00:42:29,040 --> 00:42:33,599
records are just

00:42:30,079 --> 00:42:34,480
dropped uh and then check pointing like

00:42:33,599 --> 00:42:38,240
i said we don't

00:42:34,480 --> 00:42:42,400
have specifically checkpointing um

00:42:38,240 --> 00:42:44,079
i guess that's i guess that's it for me

00:42:42,400 --> 00:42:59,839
thank you thank you everybody for

00:42:44,079 --> 00:42:59,839
attending the time hope it was enjoyable

00:43:16,480 --> 00:43:21,599
um should i stick around answer

00:43:18,319 --> 00:43:21,599
questions or just go ahead and just

00:43:24,839 --> 00:43:27,839
leave

00:43:54,319 --> 00:43:57,359
okay i guess that's it there's no more

00:43:56,079 --> 00:43:58,400
questions i'll go ahead and leave the

00:43:57,359 --> 00:44:09,839
session in

00:43:58,400 --> 00:44:09,839
thanks again everyone

00:44:23,200 --> 00:44:25,280

YouTube URL: https://www.youtube.com/watch?v=V4HbPuy8XQU


