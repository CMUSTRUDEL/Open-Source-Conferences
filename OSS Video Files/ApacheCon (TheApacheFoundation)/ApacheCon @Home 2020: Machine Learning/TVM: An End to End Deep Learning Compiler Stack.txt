Title: TVM: An End to End Deep Learning Compiler Stack
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	TVM: An End to End Deep Learning Compiler Stack
Tianqi Chen

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache(incubating) TVM is an open deep learning compiler stack for CPUs, GPUs, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends. TVM provides the following main features: - Compilation of deep learning models in Keras, MXNet, PyTorch, Tensorflow, CoreML, DarkNet into minimum deployable modules on diverse hardware backends. - Infrastructure to automatic generate and optimize tensor operators on more backend with better performance. In this talk, I will cover the new developments in TVM in the past year around the areas of more backend, automation and model support.

Tianqi Chen received his PhD. from the Paul G. Allen School of Computer Science & Engineering at the University of Washington, working with Carlos Guestrin on the intersection of machine learning and systems. He has created three major learning systems that are widely adopted: XGBoost, TVM, and MXNet(co-creator). He is a recipient of the Google Ph.D. Fellowship in Machine Learning. He is currently the CTO of OctoML.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,880 --> 00:00:30,960
so hello everyone it's great to be here

00:00:28,840 --> 00:00:32,320
and you know today i'm going to tell you

00:00:30,960 --> 00:00:35,280
about

00:00:32,320 --> 00:00:36,559
country dvm so so to give you a sense of

00:00:35,280 --> 00:00:39,840
what apache tvm

00:00:36,559 --> 00:00:41,920
is it is uh you know end-to-end

00:00:39,840 --> 00:00:44,640
uh machine learning framework for

00:00:41,920 --> 00:00:47,920
deploying diploma models onto the areas

00:00:44,640 --> 00:00:49,520
of hardware platforms and you know

00:00:47,920 --> 00:00:51,199
to see like where we are at if you look

00:00:49,520 --> 00:00:53,680
at the current deep learning landscape

00:00:51,199 --> 00:00:54,559
there are different frameworks and and

00:00:53,680 --> 00:00:56,800
engines

00:00:54,559 --> 00:00:58,079
there are compiler and then finally

00:00:56,800 --> 00:01:01,440
there are also like a kernel

00:00:58,079 --> 00:01:04,559
libraries out there so

00:01:01,440 --> 00:01:06,720
the the main question that uh that

00:01:04,559 --> 00:01:08,240
a lot of you know that that a lot of

00:01:06,720 --> 00:01:10,000
people have is you know right now

00:01:08,240 --> 00:01:12,960
a lot of the approaches that people take

00:01:10,000 --> 00:01:15,680
are using hand optimized libraries

00:01:12,960 --> 00:01:17,280
and tvm really assisting this uh

00:01:15,680 --> 00:01:18,960
spectrum of you know being able to try

00:01:17,280 --> 00:01:21,920
to generate automatic

00:01:18,960 --> 00:01:22,479
uh kernel libraries that help us to

00:01:21,920 --> 00:01:24,560
deploy

00:01:22,479 --> 00:01:26,400
machine learning model to various kind

00:01:24,560 --> 00:01:28,960
of power platform

00:01:26,400 --> 00:01:30,159
so so to give us as so you know how tvm

00:01:28,960 --> 00:01:31,280
is different from the current machine

00:01:30,159 --> 00:01:33,680
learning systems

00:01:31,280 --> 00:01:34,560
most of a current learning system tries

00:01:33,680 --> 00:01:37,200
to build

00:01:34,560 --> 00:01:38,720
machine learning solutions to you know

00:01:37,200 --> 00:01:40,000
enable machine learning on the other

00:01:38,720 --> 00:01:41,840
hand

00:01:40,000 --> 00:01:44,000
there's also this question of you know

00:01:41,840 --> 00:01:45,439
uh we we need to spend a lot of effort

00:01:44,000 --> 00:01:47,439
building those systems so can we

00:01:45,439 --> 00:01:51,040
actually use machine learning to

00:01:47,439 --> 00:01:52,880
to optimize the system themselves

00:01:51,040 --> 00:01:54,880
so that you know i can spend less effort

00:01:52,880 --> 00:01:55,840
or you know the the tbm community can

00:01:54,880 --> 00:01:57,920
spend less effort

00:01:55,840 --> 00:01:59,040
to go and optimize this system the

00:01:57,920 --> 00:02:02,240
answer is yes

00:01:59,040 --> 00:02:03,759
and that we will build what we call

00:02:02,240 --> 00:02:06,240
learning based learning system

00:02:03,759 --> 00:02:07,600
that not only use machines that not only

00:02:06,240 --> 00:02:08,720
build system that enables machine

00:02:07,600 --> 00:02:10,239
learning but also use

00:02:08,720 --> 00:02:12,080
machine learning to optimize those

00:02:10,239 --> 00:02:13,440
system integration

00:02:12,080 --> 00:02:15,680
so in particular i'm going to tell you a

00:02:13,440 --> 00:02:17,920
bit about tvn and the task

00:02:15,680 --> 00:02:19,920
that we are looking at when building

00:02:17,920 --> 00:02:21,280
tvms we want to build be able to deploy

00:02:19,920 --> 00:02:23,120
machine learning models onto

00:02:21,280 --> 00:02:26,160
various kind of power backers including

00:02:23,120 --> 00:02:28,879
your data center cpu gpu raspberry pi's

00:02:26,160 --> 00:02:29,920
all those specialized accelerators the

00:02:28,879 --> 00:02:32,640
main gap

00:02:29,920 --> 00:02:34,400
of of these this deployment is actually

00:02:32,640 --> 00:02:36,319
huge so to give a sense of how

00:02:34,400 --> 00:02:38,720
does existing diploma in framework solve

00:02:36,319 --> 00:02:41,599
this problem most of existing frameworks

00:02:38,720 --> 00:02:42,160
represent the high-level computation of

00:02:41,599 --> 00:02:43,519
those

00:02:42,160 --> 00:02:45,920
frameworks in what we call high-level

00:02:43,519 --> 00:02:48,000
data flow graphs and each of the colors

00:02:45,920 --> 00:02:50,000
correspond to a primitive test operator

00:02:48,000 --> 00:02:52,879
such as that conversion 3d or conversion

00:02:50,000 --> 00:02:54,640
2d and in each of the color operators

00:02:52,879 --> 00:02:56,480
actually there have to be someone

00:02:54,640 --> 00:02:58,400
to go and implement those libraries for

00:02:56,480 --> 00:03:00,080
example in the case of nvidia there's a

00:02:58,400 --> 00:03:00,959
library called cudn that go and

00:03:00,080 --> 00:03:04,000
implement

00:03:00,959 --> 00:03:05,760
those operations the limitation of this

00:03:04,000 --> 00:03:07,519
approach is of course you know it will

00:03:05,760 --> 00:03:09,440
cause human resources

00:03:07,519 --> 00:03:11,680
and in a lot of cases you also want to

00:03:09,440 --> 00:03:14,400
be able to combine

00:03:11,680 --> 00:03:16,560
a few uh node together into a new what

00:03:14,400 --> 00:03:19,040
we call fused operation which gives you

00:03:16,560 --> 00:03:20,800
potential speed up actually on the on

00:03:19,040 --> 00:03:23,280
the on the execution

00:03:20,800 --> 00:03:25,440
however the question now is we have to

00:03:23,280 --> 00:03:27,760
implement a blue node in the new

00:03:25,440 --> 00:03:28,879
library and it happened exactly to us a

00:03:27,760 --> 00:03:30,879
few years ago and

00:03:28,879 --> 00:03:32,560
well what we did then is we call nvidia

00:03:30,879 --> 00:03:33,440
and then you know after a year they add

00:03:32,560 --> 00:03:35,360
it back

00:03:33,440 --> 00:03:37,599
as you can see it's very time consuming

00:03:35,360 --> 00:03:38,799
and human engineering results is

00:03:37,599 --> 00:03:41,040
consuming

00:03:38,799 --> 00:03:42,959
and if you multiply that by the amount

00:03:41,040 --> 00:03:45,360
of power devices you want to support

00:03:42,959 --> 00:03:47,360
it's a very engineering test process

00:03:45,360 --> 00:03:48,799
so that's why we want to try to build a

00:03:47,360 --> 00:03:51,120
more automated solution

00:03:48,799 --> 00:03:51,920
i just want to be able to replace the

00:03:51,120 --> 00:03:54,480
human

00:03:51,920 --> 00:03:55,760
based effort by a bit more automation to

00:03:54,480 --> 00:03:57,760
build a machine learning program

00:03:55,760 --> 00:04:00,080
optimizer that automatically generates

00:03:57,760 --> 00:04:00,959
the high-level data flow graphs and

00:04:00,080 --> 00:04:03,280
optimize

00:04:00,959 --> 00:04:05,519
those programs and directly generate

00:04:03,280 --> 00:04:09,120
code that that can be deployed onto

00:04:05,519 --> 00:04:12,000
you know new hardware and new workloads

00:04:09,120 --> 00:04:13,200
so specifically for this in order to do

00:04:12,000 --> 00:04:15,360
that we will try to

00:04:13,200 --> 00:04:16,959
you know describe the high-level growing

00:04:15,360 --> 00:04:20,720
node in what we call

00:04:16,959 --> 00:04:23,040
uh tensor expression language and

00:04:20,720 --> 00:04:25,120
and then we will try to you know build a

00:04:23,040 --> 00:04:26,639
space of possible program optimizations

00:04:25,120 --> 00:04:28,240
based on specification

00:04:26,639 --> 00:04:30,080
and we can generate low level variance

00:04:28,240 --> 00:04:32,160
from that for example to give a sense

00:04:30,080 --> 00:04:34,639
you can directly write a for loop that

00:04:32,160 --> 00:04:36,400
runs this matrix application if you have

00:04:34,639 --> 00:04:37,680
written like cpu based optimization

00:04:36,400 --> 00:04:38,560
before you might know that you want to

00:04:37,680 --> 00:04:40,880
be able to

00:04:38,560 --> 00:04:42,400
tile the loop a bit so that it enjoys

00:04:40,880 --> 00:04:44,320
better cache locality

00:04:42,400 --> 00:04:46,000
or if you want to map two specialized

00:04:44,320 --> 00:04:47,360
accelerators that you like

00:04:46,000 --> 00:04:48,720
you might want to map some of the

00:04:47,360 --> 00:04:49,759
instructions through the accelerator

00:04:48,720 --> 00:04:53,120
instructions that

00:04:49,759 --> 00:04:55,280
that these programs provide

00:04:53,120 --> 00:04:56,880
so you're not sure we want to be able to

00:04:55,280 --> 00:04:58,240
you know combine billions of possible

00:04:56,880 --> 00:05:00,240
optimization choices

00:04:58,240 --> 00:05:02,560
and search over that space in order for

00:05:00,240 --> 00:05:05,680
us to get a better program

00:05:02,560 --> 00:05:07,520
so to formalize this we can you know use

00:05:05,680 --> 00:05:09,360
sse to denote the search space

00:05:07,520 --> 00:05:11,520
and our goal is to be able to generate

00:05:09,360 --> 00:05:12,560
the optimized configuration for the

00:05:11,520 --> 00:05:15,039
optimizer

00:05:12,560 --> 00:05:16,639
that gives you a program now in our

00:05:15,039 --> 00:05:19,199
objectives you want to minimize

00:05:16,639 --> 00:05:20,800
the execution time so there are a few

00:05:19,199 --> 00:05:22,639
ways we can do that for example one of

00:05:20,800 --> 00:05:24,080
the way to do that is you can always try

00:05:22,639 --> 00:05:25,759
called black box auto tuning

00:05:24,080 --> 00:05:27,440
you just try a different configuration

00:05:25,759 --> 00:05:28,080
send it to that send it to the target

00:05:27,440 --> 00:05:29,840
highway

00:05:28,080 --> 00:05:31,360
and try it again again until you know

00:05:29,840 --> 00:05:33,039
you find a good solution

00:05:31,360 --> 00:05:34,800
the problem of this approach is because

00:05:33,039 --> 00:05:38,080
it's very time consuming you need to

00:05:34,800 --> 00:05:40,479
write a lot of experiments before you

00:05:38,080 --> 00:05:41,919
hey it works or it does not work another

00:05:40,479 --> 00:05:42,639
approach we can do is you know we can

00:05:41,919 --> 00:05:45,280
learn from

00:05:42,639 --> 00:05:46,160
typical database community by defining

00:05:45,280 --> 00:05:48,400
what we call

00:05:46,160 --> 00:05:50,800
a cost model that estimates the cost of

00:05:48,400 --> 00:05:52,639
a program use that to drive the search

00:05:50,800 --> 00:05:53,919
the problem of this approach is that

00:05:52,639 --> 00:05:55,919
usually it's very hard to

00:05:53,919 --> 00:05:57,120
design a very accurate reliable cost

00:05:55,919 --> 00:05:59,280
model for each of

00:05:57,120 --> 00:06:01,440
how kind of however interesting so

00:05:59,280 --> 00:06:03,360
instead we take what we call prop uh

00:06:01,440 --> 00:06:05,360
you know machine learning based approach

00:06:03,360 --> 00:06:06,880
the idea is that we want to be able to

00:06:05,360 --> 00:06:09,440
you know use the high level

00:06:06,880 --> 00:06:11,520
configuration to generate low level asd

00:06:09,440 --> 00:06:13,280
and extract statistical features from a

00:06:11,520 --> 00:06:15,199
program and then use the machine

00:06:13,280 --> 00:06:17,680
learning based predictor to predict

00:06:15,199 --> 00:06:19,199
the cost of the running program the

00:06:17,680 --> 00:06:20,880
advantage of this approach is

00:06:19,199 --> 00:06:22,639
of of course we can also use like a

00:06:20,880 --> 00:06:23,039
neural network based approach that you

00:06:22,639 --> 00:06:26,319
know

00:06:23,039 --> 00:06:29,919
based use a tree gio model to encode the

00:06:26,319 --> 00:06:31,199
input program um the advantage of these

00:06:29,919 --> 00:06:33,199
approaches actually you know

00:06:31,199 --> 00:06:34,720
chip based model gives you a pretty good

00:06:33,199 --> 00:06:37,120
predictive accuracy

00:06:34,720 --> 00:06:38,319
it does also kind of task invariant in

00:06:37,120 --> 00:06:40,240
us as you know you can

00:06:38,319 --> 00:06:41,759
you can transfer the model across

00:06:40,240 --> 00:06:43,919
multiple uh

00:06:41,759 --> 00:06:46,080
program domains because the ast is

00:06:43,919 --> 00:06:47,840
shared

00:06:46,080 --> 00:06:49,360
by using this machine learning based

00:06:47,840 --> 00:06:50,800
cost model approach

00:06:49,360 --> 00:06:52,160
we can actually get pretty decent

00:06:50,800 --> 00:06:53,360
performance for example here's a

00:06:52,160 --> 00:06:56,160
benchmark on

00:06:53,360 --> 00:06:56,800
a single layer of uh running conversion

00:06:56,160 --> 00:06:59,599
2d

00:06:56,800 --> 00:07:00,240
operator or titan x gpu if you directly

00:06:59,599 --> 00:07:02,240
use

00:07:00,240 --> 00:07:03,759
a black box approach it can give you

00:07:02,240 --> 00:07:05,360
pretty decent performance

00:07:03,759 --> 00:07:07,520
but if you use the machine learning

00:07:05,360 --> 00:07:09,360
based optimization you can find that

00:07:07,520 --> 00:07:11,680
it will give you better performance than

00:07:09,360 --> 00:07:13,840
the cdm baseline in this particular case

00:07:11,680 --> 00:07:15,280
the reason is not because you know

00:07:13,840 --> 00:07:16,560
machines are better than human

00:07:15,280 --> 00:07:19,199
it's simply because you know machines

00:07:16,560 --> 00:07:21,280
can specialize particular case

00:07:19,199 --> 00:07:24,400
to get to find a specific code that

00:07:21,280 --> 00:07:26,639
optimizes for this particular operator

00:07:24,400 --> 00:07:28,319
another important perspective of this is

00:07:26,639 --> 00:07:29,120
that not only we can build a course

00:07:28,319 --> 00:07:31,759
model for

00:07:29,120 --> 00:07:32,960
a single search purposes we can reuse

00:07:31,759 --> 00:07:35,840
the cost model

00:07:32,960 --> 00:07:36,160
to to predict the cost of a new task so

00:07:35,840 --> 00:07:39,039
as

00:07:36,160 --> 00:07:40,720
the as as the you know the compiler

00:07:39,039 --> 00:07:41,520
starts to collect more data it becomes

00:07:40,720 --> 00:07:44,240
smarter

00:07:41,520 --> 00:07:46,080
and you will learn the less to predict

00:07:44,240 --> 00:07:47,360
the cost more accurately and find better

00:07:46,080 --> 00:07:49,120
programs faster

00:07:47,360 --> 00:07:50,720
so give it a sense of you know what kind

00:07:49,120 --> 00:07:51,440
of this transfer learning approach can

00:07:50,720 --> 00:07:53,440
bring you

00:07:51,440 --> 00:07:54,720
this is the original curve and if you

00:07:53,440 --> 00:07:55,360
use transforming for this particular

00:07:54,720 --> 00:07:56,560
case that

00:07:55,360 --> 00:07:59,360
actually gives you a pretty good

00:07:56,560 --> 00:08:00,800
performance boost on average is not as

00:07:59,360 --> 00:08:02,560
good as this one but on average you can

00:08:00,800 --> 00:08:04,319
get a three extra tax speed up over a

00:08:02,560 --> 00:08:06,160
non-transfer case

00:08:04,319 --> 00:08:08,160
so you know not sure we by using a

00:08:06,160 --> 00:08:10,720
learning based approach actually we can

00:08:08,160 --> 00:08:12,479
you know scale the automatic program

00:08:10,720 --> 00:08:15,840
optimization to

00:08:12,479 --> 00:08:17,280
to a pretty large scale and speed up the

00:08:15,840 --> 00:08:19,919
speed up for the speed of program

00:08:17,280 --> 00:08:22,000
optimization using transfer learning

00:08:19,919 --> 00:08:24,160
so so far i've talked about this kind of

00:08:22,000 --> 00:08:25,759
machine learning component of this

00:08:24,160 --> 00:08:28,000
tbm is actually an end-to-end deep

00:08:25,759 --> 00:08:30,240
learning compiler that that contains a

00:08:28,000 --> 00:08:33,360
larger system component with

00:08:30,240 --> 00:08:34,800
machine learning as its core so uh

00:08:33,360 --> 00:08:36,560
i will talk a bit more about you know

00:08:34,800 --> 00:08:37,360
how do we define a search spacing here

00:08:36,560 --> 00:08:39,519
for example

00:08:37,360 --> 00:08:41,039
uh you know for a test expression to

00:08:39,519 --> 00:08:41,919
recall we want to define search ways

00:08:41,039 --> 00:08:43,760
that maps

00:08:41,919 --> 00:08:45,600
from high level expression to the low

00:08:43,760 --> 00:08:47,519
level val power programs

00:08:45,600 --> 00:08:49,680
and in here one of the most important

00:08:47,519 --> 00:08:51,760
things how can we define a search space

00:08:49,680 --> 00:08:53,839
for cpu based programs actually we can

00:08:51,760 --> 00:08:56,959
reuse a lot of existing primitives

00:08:53,839 --> 00:08:57,760
by existing frameworks like halide or

00:08:56,959 --> 00:09:00,560
loopy

00:08:57,760 --> 00:09:02,320
to be able to do root transformations to

00:09:00,560 --> 00:09:04,640
make use of cache locality

00:09:02,320 --> 00:09:06,080
and do better vectorization things

00:09:04,640 --> 00:09:06,640
become more interesting as we start to

00:09:06,080 --> 00:09:09,360
look at

00:09:06,640 --> 00:09:10,240
other custom hardware like you know gpus

00:09:09,360 --> 00:09:13,680
or

00:09:10,240 --> 00:09:14,880
emerging new accelerators for gpus one

00:09:13,680 --> 00:09:17,120
of the important things we want to be

00:09:14,880 --> 00:09:20,160
able to make use of the shared memory

00:09:17,120 --> 00:09:22,800
and do threat corporation effectively

00:09:20,160 --> 00:09:23,680
across the gpu strike to cooperate with

00:09:22,800 --> 00:09:26,000
fetch data

00:09:23,680 --> 00:09:27,519
onto the gpu shield memory since

00:09:26,000 --> 00:09:28,640
becoming mostly interesting when you

00:09:27,519 --> 00:09:31,279
start to think about

00:09:28,640 --> 00:09:32,800
tpu-like accelerators these accelerators

00:09:31,279 --> 00:09:34,080
are both special in terms of their

00:09:32,800 --> 00:09:37,440
compute primitives

00:09:34,080 --> 00:09:39,040
and the memory subsistently exposed so

00:09:37,440 --> 00:09:40,480
let's talk about compute primitives so

00:09:39,040 --> 00:09:42,399
traditionally when you write programs

00:09:40,480 --> 00:09:44,080
you can just write a scalar program

00:09:42,399 --> 00:09:46,399
you just write for loops to compute each

00:09:44,080 --> 00:09:48,640
element of a target it's very flexible

00:09:46,399 --> 00:09:50,959
and as you start to use the vector

00:09:48,640 --> 00:09:53,440
instructions like avx 512 or

00:09:50,959 --> 00:09:54,480
arms neo instruction we will need to be

00:09:53,440 --> 00:09:56,800
able to you know

00:09:54,480 --> 00:09:58,560
write uh programs that make use of the

00:09:56,800 --> 00:10:01,040
vector instruction that imposes some

00:09:58,560 --> 00:10:03,440
kind of constraints on your program

00:10:01,040 --> 00:10:05,040
if you look at the new access readers or

00:10:03,440 --> 00:10:07,279
like you know the instruction like a

00:10:05,040 --> 00:10:10,079
tensor construction in a media gpu

00:10:07,279 --> 00:10:10,480
since becoming even more complicated we

00:10:10,079 --> 00:10:12,399
can

00:10:10,480 --> 00:10:13,680
write a single instruction that performs

00:10:12,399 --> 00:10:16,959
matrix vector

00:10:13,680 --> 00:10:18,399
product or matrix matrix product these

00:10:16,959 --> 00:10:19,920
these instructions are usually high

00:10:18,399 --> 00:10:21,600
dimensional and it's really hard to

00:10:19,920 --> 00:10:23,279
enumerate all the possible tensor

00:10:21,600 --> 00:10:24,880
instructions out there

00:10:23,279 --> 00:10:26,640
so my challenge is how can we build a

00:10:24,880 --> 00:10:28,560
generic system

00:10:26,640 --> 00:10:29,680
or all the potential emerging tensor

00:10:28,560 --> 00:10:31,680
instruction

00:10:29,680 --> 00:10:32,880
in order to do that what we do is you

00:10:31,680 --> 00:10:34,959
know besides using

00:10:32,880 --> 00:10:36,000
a declarative language for computer

00:10:34,959 --> 00:10:38,160
specification

00:10:36,000 --> 00:10:39,040
we also use the same language to specify

00:10:38,160 --> 00:10:41,200
the hardware

00:10:39,040 --> 00:10:43,279
interface specification in this case it

00:10:41,200 --> 00:10:46,880
describes the matrix vector product

00:10:43,279 --> 00:10:49,040
of a 8x8 matrix vector instruction

00:10:46,880 --> 00:10:50,880
and what we do is we will employ what we

00:10:49,040 --> 00:10:52,160
call transformation process that will

00:10:50,880 --> 00:10:54,000
try to mix and match

00:10:52,160 --> 00:10:56,079
the computer specification to the

00:10:54,000 --> 00:10:57,440
hardware interface specification in

00:10:56,079 --> 00:10:59,360
order to lower the code

00:10:57,440 --> 00:11:01,040
to make use of the howard instructions

00:10:59,360 --> 00:11:02,800
out there

00:11:01,040 --> 00:11:05,200
and i i don't have time to talk about

00:11:02,800 --> 00:11:07,839
the the hardware uh memory scope but

00:11:05,200 --> 00:11:07,839
you know you know

00:11:08,240 --> 00:11:12,800
to combine the steps from prior works as

00:11:10,959 --> 00:11:13,200
well as you know bringing new parameters

00:11:12,800 --> 00:11:16,000
from

00:11:13,200 --> 00:11:16,399
gpus and other accelerators to enable

00:11:16,000 --> 00:11:18,880
new

00:11:16,399 --> 00:11:19,680
new acceleration so so far i've talked

00:11:18,880 --> 00:11:22,560
about you know

00:11:19,680 --> 00:11:24,640
how does the tvm works in both the

00:11:22,560 --> 00:11:26,000
machine learning optimizer component as

00:11:24,640 --> 00:11:27,519
well as the

00:11:26,000 --> 00:11:29,360
as well as the code search based

00:11:27,519 --> 00:11:31,440
component let's talk about what we can

00:11:29,360 --> 00:11:34,720
do by bringing everything together

00:11:31,440 --> 00:11:37,040
so here's the benchmark from entry and

00:11:34,720 --> 00:11:38,800
inference on a media contact gpu

00:11:37,040 --> 00:11:40,560
this benchmark is a bit old if you go to

00:11:38,800 --> 00:11:41,440
the tvm blog post there are a lot of

00:11:40,560 --> 00:11:44,880
interesting

00:11:41,440 --> 00:11:47,279
new latest results as well so um

00:11:44,880 --> 00:11:49,040
both tensorflow apache mxnet are backed

00:11:47,279 --> 00:11:51,600
by cdn library

00:11:49,040 --> 00:11:52,160
and if you run tvm you can find that you

00:11:51,600 --> 00:11:54,160
can get

00:11:52,160 --> 00:11:56,000
better performance and there are a few

00:11:54,160 --> 00:11:56,720
highlights in here but if you look at

00:11:56,000 --> 00:11:59,279
the left

00:11:56,720 --> 00:12:00,079
side it's a it's a model on resonant 18

00:11:59,279 --> 00:12:02,079
and we can get

00:12:00,079 --> 00:12:03,600
pretty competitive performance on those

00:12:02,079 --> 00:12:05,680
kind of standard models

00:12:03,600 --> 00:12:07,680
what's most exciting takeaways though is

00:12:05,680 --> 00:12:09,519
that we look at far right

00:12:07,680 --> 00:12:11,360
and this is the model called deep q

00:12:09,519 --> 00:12:14,160
learning model and we get a 3x

00:12:11,360 --> 00:12:16,639
better performance then the state then

00:12:14,160 --> 00:12:19,760
the standard deepening framework

00:12:16,639 --> 00:12:22,079
what is the reason from here the reason

00:12:19,760 --> 00:12:23,680
is the reason is that you know we will

00:12:22,079 --> 00:12:27,440
be able to

00:12:23,680 --> 00:12:29,600
use the we want to be able to you know

00:12:27,440 --> 00:12:32,800
we want to be able to

00:12:29,600 --> 00:12:35,040
in this case because the library vendors

00:12:32,800 --> 00:12:36,720
are heavily optimized for standard

00:12:35,040 --> 00:12:38,160
benchmark like rest not 18 because they

00:12:36,720 --> 00:12:40,399
are paid to do so

00:12:38,160 --> 00:12:42,480
again on those emerging workloads like a

00:12:40,399 --> 00:12:43,360
deep qree model or you know if you are

00:12:42,480 --> 00:12:46,639
thinking about

00:12:43,360 --> 00:12:48,720
any model that runs on your local uh

00:12:46,639 --> 00:12:51,120
or on your local company a lot of cases

00:12:48,720 --> 00:12:53,120
people are developing customized models

00:12:51,120 --> 00:12:54,800
and load cyber is not well optimized for

00:12:53,120 --> 00:12:57,279
those customized models

00:12:54,800 --> 00:12:59,040
so so what we can what's happening here

00:12:57,279 --> 00:13:01,600
is that tvm based solution

00:12:59,040 --> 00:13:03,360
can give us automatic optimization all

00:13:01,600 --> 00:13:04,240
those emerging models as well so that

00:13:03,360 --> 00:13:07,040
you can see this

00:13:04,240 --> 00:13:08,399
3x performance boost or other cases

00:13:07,040 --> 00:13:09,200
where i've seen a lot of successful

00:13:08,399 --> 00:13:11,440
stories

00:13:09,200 --> 00:13:13,680
of the community deployment case of

00:13:11,440 --> 00:13:15,680
apache tdm

00:13:13,680 --> 00:13:17,360
similarly you know another interesting

00:13:15,680 --> 00:13:17,680
thing is we can use the same solution

00:13:17,360 --> 00:13:19,680
and

00:13:17,680 --> 00:13:21,120
get portable performance across hardware

00:13:19,680 --> 00:13:23,680
platforms in this case

00:13:21,120 --> 00:13:24,160
um you know we can we can run tvm on

00:13:23,680 --> 00:13:26,160
both

00:13:24,160 --> 00:13:27,279
arm cpu and gpu and get better

00:13:26,160 --> 00:13:31,200
performance than the

00:13:27,279 --> 00:13:33,920
native solutions on those platforms

00:13:31,200 --> 00:13:36,240
and finally we can we can also build

00:13:33,920 --> 00:13:39,040
solutions that optimizes for

00:13:36,240 --> 00:13:40,000
latest power this is an example on video

00:13:39,040 --> 00:13:42,160
tensor called

00:13:40,000 --> 00:13:43,680
again we can find that in this case this

00:13:42,160 --> 00:13:46,160
is a speed up

00:13:43,680 --> 00:13:48,000
over the existing solution so we can

00:13:46,160 --> 00:13:50,079
find that we can get 1. 1.5

00:13:48,000 --> 00:13:51,760
x better on transformer related

00:13:50,079 --> 00:13:53,519
workloads on this case

00:13:51,760 --> 00:13:56,560
and again this is the takeaway on how

00:13:53,519 --> 00:13:58,320
can we build automatic optimization

00:13:56,560 --> 00:14:01,440
that gives you you know better

00:13:58,320 --> 00:14:04,079
performance or emerging workloads

00:14:01,440 --> 00:14:04,720
so so far we have talked about you know

00:14:04,079 --> 00:14:07,360
what is

00:14:04,720 --> 00:14:07,760
tvm and how much benefit it can bring

00:14:07,360 --> 00:14:10,000
you

00:14:07,760 --> 00:14:10,800
let me switch gears to talk a bit about

00:14:10,000 --> 00:14:13,519
you know

00:14:10,800 --> 00:14:15,519
where the tvm community is going in the

00:14:13,519 --> 00:14:18,000
in the last year actually

00:14:15,519 --> 00:14:19,440
so there are a few important directions

00:14:18,000 --> 00:14:21,199
we are looking into

00:14:19,440 --> 00:14:23,279
um the first direction is you know we

00:14:21,199 --> 00:14:26,079
are we are building a unified

00:14:23,279 --> 00:14:28,000
runtime for deploying heterogeneous

00:14:26,079 --> 00:14:30,399
devices

00:14:28,000 --> 00:14:31,279
and in this case you know one of the

00:14:30,399 --> 00:14:32,959
important things

00:14:31,279 --> 00:14:35,040
in a lot of deployment cases where we're

00:14:32,959 --> 00:14:36,480
starting to see a need to be able to

00:14:35,040 --> 00:14:40,560
deploy on multiple devices

00:14:36,480 --> 00:14:42,639
including things like you know cudas

00:14:40,560 --> 00:14:43,680
mpu or some external runtimes like test

00:14:42,639 --> 00:14:45,519
rt

00:14:43,680 --> 00:14:47,440
what we build so far is we will

00:14:45,519 --> 00:14:50,480
introduce a unified interface

00:14:47,440 --> 00:14:52,160
for all these uh all these device

00:14:50,480 --> 00:14:54,480
specific runtimes that's what we call a

00:14:52,160 --> 00:14:57,519
device module and we can plug them back

00:14:54,480 --> 00:15:00,399
into the tv around town and exposing

00:14:57,519 --> 00:15:02,720
them into a single binary file

00:15:00,399 --> 00:15:03,760
so by having this unified runtime we'll

00:15:02,720 --> 00:15:06,399
be able to

00:15:03,760 --> 00:15:07,600
generate programs that that runs on

00:15:06,399 --> 00:15:11,199
multiple

00:15:07,600 --> 00:15:12,800
devices including cpu gpu and mpu

00:15:11,199 --> 00:15:14,639
very effectively using a single

00:15:12,800 --> 00:15:16,720
interface

00:15:14,639 --> 00:15:18,480
and it also gives you some free benefit

00:15:16,720 --> 00:15:20,240
like you know tvm have a

00:15:18,480 --> 00:15:22,320
have language mining so all those major

00:15:20,240 --> 00:15:24,079
languages including python java go

00:15:22,320 --> 00:15:26,000
we even have a web assembly interface

00:15:24,079 --> 00:15:28,959
that allows you to directly call

00:15:26,000 --> 00:15:30,800
from a web browser and you can you can

00:15:28,959 --> 00:15:33,120
directly get those

00:15:30,800 --> 00:15:35,279
free api bindings by making use of the

00:15:33,120 --> 00:15:37,199
unified runtime

00:15:35,279 --> 00:15:39,680
you can also get things like automatic

00:15:37,199 --> 00:15:42,000
rpc support that allows you to

00:15:39,680 --> 00:15:42,880
upload your programs onto a remote

00:15:42,000 --> 00:15:45,120
devices and

00:15:42,880 --> 00:15:46,560
and do remote benchmarking so that you

00:15:45,120 --> 00:15:48,160
can say you know

00:15:46,560 --> 00:15:49,759
automatically generate an optimized

00:15:48,160 --> 00:15:52,240
program on your android devices

00:15:49,759 --> 00:15:55,839
without worrying about you know how to

00:15:52,240 --> 00:15:55,839
how to directly access

00:15:55,920 --> 00:15:59,199
the second thing that we are actively

00:15:57,600 --> 00:16:01,199
going on is we want to build

00:15:59,199 --> 00:16:02,959
unified intermediate orientation it

00:16:01,199 --> 00:16:05,440
allows us to unify both

00:16:02,959 --> 00:16:06,000
high level and low level optimization

00:16:05,440 --> 00:16:09,360
together

00:16:06,000 --> 00:16:12,079
into what we call a common module and

00:16:09,360 --> 00:16:12,800
in order to do by doing that it allows

00:16:12,079 --> 00:16:15,040
us to

00:16:12,800 --> 00:16:17,040
you know mix and match different levels

00:16:15,040 --> 00:16:21,040
of orientations in a single

00:16:17,040 --> 00:16:24,320
ir module and allows us to do some cross

00:16:21,040 --> 00:16:26,079
cross implementation optimization

00:16:24,320 --> 00:16:27,920
finally we are also looking into what we

00:16:26,079 --> 00:16:29,440
call first class python support

00:16:27,920 --> 00:16:31,680
which means that you know we want to be

00:16:29,440 --> 00:16:33,680
able to use python

00:16:31,680 --> 00:16:35,920
as the first class citizen to be able to

00:16:33,680 --> 00:16:36,399
represent the intermediary orientation

00:16:35,920 --> 00:16:38,800
and

00:16:36,399 --> 00:16:39,920
write as a right as the intermediate

00:16:38,800 --> 00:16:41,759
recognition so

00:16:39,920 --> 00:16:44,399
developers will be able to directly

00:16:41,759 --> 00:16:46,800
write their ir programs in python

00:16:44,399 --> 00:16:48,079
and manipulate them using python api

00:16:46,800 --> 00:16:50,079
this allows us to

00:16:48,079 --> 00:16:52,240
extrude innovation because ml-based

00:16:50,079 --> 00:16:54,880
compilation is still a very young view

00:16:52,240 --> 00:16:56,560
and we can mix and match programs in

00:16:54,880 --> 00:16:57,199
both pricing and superclassplus if you

00:16:56,560 --> 00:16:59,680
want

00:16:57,199 --> 00:17:00,800
and you know you can also shift to

00:16:59,680 --> 00:17:02,959
c-class plus when

00:17:00,800 --> 00:17:04,720
your python prototype is product ready

00:17:02,959 --> 00:17:05,360
so that it gives you a path from

00:17:04,720 --> 00:17:09,439
research

00:17:05,360 --> 00:17:10,400
to production finally we also want to be

00:17:09,439 --> 00:17:13,039
able to rethink

00:17:10,400 --> 00:17:14,799
the low level tensori to introduce

00:17:13,039 --> 00:17:16,839
things like automatic scheduling

00:17:14,799 --> 00:17:18,160
and better transformation support to

00:17:16,839 --> 00:17:21,600
support

00:17:18,160 --> 00:17:24,559
more heterogeneous heterogeneous tensor

00:17:21,600 --> 00:17:24,559
specialized hardware

00:17:25,039 --> 00:17:27,919
another thing that we're actively

00:17:26,240 --> 00:17:29,120
looking at is be able to you know as

00:17:27,919 --> 00:17:31,360
open source project

00:17:29,120 --> 00:17:32,960
to be able to be able to interpolate

00:17:31,360 --> 00:17:34,799
with other existing machine learning

00:17:32,960 --> 00:17:37,679
compiler infrastructure in particular

00:17:34,799 --> 00:17:38,960
we are working on bringing in models

00:17:37,679 --> 00:17:41,280
from both tensorflow

00:17:38,960 --> 00:17:42,640
and pytorch and be able to also you know

00:17:41,280 --> 00:17:44,960
generate code that

00:17:42,640 --> 00:17:46,559
contains external functions and make use

00:17:44,960 --> 00:17:50,000
of customized packaging

00:17:46,559 --> 00:17:52,720
and customized code generation

00:17:50,000 --> 00:17:54,320
so besides the the both unification in

00:17:52,720 --> 00:17:55,679
terms of runtime and ir

00:17:54,320 --> 00:17:57,520
one of the final things we are looking

00:17:55,679 --> 00:17:58,480
into is trying to bring in full stack

00:17:57,520 --> 00:18:00,880
automation

00:17:58,480 --> 00:18:02,320
in this case our current plane auto tv

00:18:00,880 --> 00:18:04,559
and charge automate

00:18:02,320 --> 00:18:06,480
the the optimize operator generation

00:18:04,559 --> 00:18:08,400
stage and we want to be able to

00:18:06,480 --> 00:18:10,559
you know enlarge their support to use

00:18:08,400 --> 00:18:12,080
other tv across all the layers including

00:18:10,559 --> 00:18:13,760
the high level optimization

00:18:12,080 --> 00:18:16,240
as well as the highway specific

00:18:13,760 --> 00:18:18,640
optimizations

00:18:16,240 --> 00:18:20,400
so um so if i've talked i'll tell you a

00:18:18,640 --> 00:18:21,919
bit about you know tvm and i would like

00:18:20,400 --> 00:18:23,600
to spend the last minute

00:18:21,919 --> 00:18:25,440
talk about to talk about community

00:18:23,600 --> 00:18:27,440
perspective and as you know that

00:18:25,440 --> 00:18:29,200
partially tvm is apache project

00:18:27,440 --> 00:18:31,360
and this means that you know it really

00:18:29,200 --> 00:18:32,240
enables independent governance allows

00:18:31,360 --> 00:18:34,640
multiple

00:18:32,240 --> 00:18:37,120
uh organizations to to to even

00:18:34,640 --> 00:18:40,000
competitors to collaborate on a project

00:18:37,120 --> 00:18:40,960
and so far tvm is very active and we

00:18:40,000 --> 00:18:43,039
developed

00:18:40,960 --> 00:18:44,080
the project on the apache way which

00:18:43,039 --> 00:18:46,799
means that you know

00:18:44,080 --> 00:18:48,160
all every depart every developing events

00:18:46,799 --> 00:18:50,000
are in open you can

00:18:48,160 --> 00:18:52,080
follow a mail list and discuss forum on

00:18:50,000 --> 00:18:52,480
that um and there's open governance

00:18:52,080 --> 00:18:53,919
model

00:18:52,480 --> 00:18:56,240
followed by the apache software

00:18:53,919 --> 00:18:58,880
foundation guideline

00:18:56,240 --> 00:19:00,080
it has also been widely adopted by a lot

00:18:58,880 --> 00:19:02,880
of the

00:19:00,080 --> 00:19:04,720
industrial partners including um you

00:19:02,880 --> 00:19:05,600
know academia and industries such as you

00:19:04,720 --> 00:19:08,720
know

00:19:05,600 --> 00:19:12,080
amazon intel facebook are

00:19:08,720 --> 00:19:14,400
qualcomm uh to name a few and uh

00:19:12,080 --> 00:19:15,120
and you know you have uh has already

00:19:14,400 --> 00:19:18,160
powered

00:19:15,120 --> 00:19:20,160
uh some of the industrial applications

00:19:18,160 --> 00:19:22,960
uh one of the notable example is that

00:19:20,160 --> 00:19:26,559
you know alexa's weak world actually is

00:19:22,960 --> 00:19:28,480
optimized by tbms optimization

00:19:26,559 --> 00:19:30,400
we are also holding an annual data

00:19:28,480 --> 00:19:33,440
conference this year is virtual

00:19:30,400 --> 00:19:35,120
so if you go to tvmconf.org you will

00:19:33,440 --> 00:19:37,280
find more information there

00:19:35,120 --> 00:19:39,760
with that i would like to conclude my

00:19:37,280 --> 00:19:53,440
talk and i'll be more than happy to

00:19:39,760 --> 00:19:57,440
take any questions okay thank you

00:19:53,440 --> 00:19:57,440
yeah any questions uh

00:20:02,960 --> 00:20:07,039
yeah so i forgot to include the link to

00:20:05,280 --> 00:20:08,240
the tvm conference so if you are

00:20:07,039 --> 00:20:10,400
interested

00:20:08,240 --> 00:20:12,400
you can i will post it on the chat and

00:20:10,400 --> 00:20:14,799
please go and check it out

00:20:12,400 --> 00:20:16,559
there's also links to last two years

00:20:14,799 --> 00:20:19,200
conference including

00:20:16,559 --> 00:20:20,080
video recordings and you can check out

00:20:19,200 --> 00:20:22,559
the existing

00:20:20,080 --> 00:20:23,760
applications from different industrial

00:20:22,559 --> 00:20:25,200
partners

00:20:23,760 --> 00:20:34,000
different industrial organizations that

00:20:25,200 --> 00:20:36,320
are contributing to the community

00:20:34,000 --> 00:20:37,840
yes so uh one of the questions you know

00:20:36,320 --> 00:20:40,960
what is the focus right so

00:20:37,840 --> 00:20:43,440
first of all tvm itself is uh

00:20:40,960 --> 00:20:45,520
the is nate is natively building c plus

00:20:43,440 --> 00:20:46,720
plus so so we we really want to enable

00:20:45,520 --> 00:20:49,360
productivity

00:20:46,720 --> 00:20:50,000
on that perspective and it's also cross

00:20:49,360 --> 00:20:52,159
language

00:20:50,000 --> 00:20:54,480
you know says you know right now you can

00:20:52,159 --> 00:20:56,480
you can you know access tvm

00:20:54,480 --> 00:20:58,480
from languages like you know when you

00:20:56,480 --> 00:20:59,360
deploy tvm you can deploy it into

00:20:58,480 --> 00:21:03,520
bindings like

00:20:59,360 --> 00:21:06,480
you know uh uh uh c plus plus python

00:21:03,520 --> 00:21:08,080
javascript go and any other languages

00:21:06,480 --> 00:21:09,679
but python is indeed one of the first

00:21:08,080 --> 00:21:11,280
class citizen world support because we

00:21:09,679 --> 00:21:12,480
we see a lot of use cases where people

00:21:11,280 --> 00:21:14,080
want to directly

00:21:12,480 --> 00:21:15,919
write machine learning program in python

00:21:14,080 --> 00:21:18,960
and directly call compilation and

00:21:15,919 --> 00:21:18,960
deployment from here

00:21:19,120 --> 00:21:22,240
it means that tbm is not developed in a

00:21:20,880 --> 00:21:23,760
core to be cross language

00:21:22,240 --> 00:21:25,360
so for example there's also a recent

00:21:23,760 --> 00:21:26,960
movement to bring rust

00:21:25,360 --> 00:21:28,960
as a first-class citizen because there

00:21:26,960 --> 00:21:30,240
are some interest in security and

00:21:28,960 --> 00:21:33,039
deployment community

00:21:30,240 --> 00:21:33,039
are doing that

00:21:43,679 --> 00:21:47,919
yeah if you have any additional

00:21:46,400 --> 00:21:48,640
questions you are more than welcome to

00:21:47,919 --> 00:21:51,360
post

00:21:48,640 --> 00:21:52,240
to the to the chat or directly speak out

00:21:51,360 --> 00:21:59,840
i believe

00:21:52,240 --> 00:21:59,840
and that's also possible

00:22:08,240 --> 00:22:12,400
yes the question is about you know is

00:22:10,480 --> 00:22:13,679
there any use cases where it is being

00:22:12,400 --> 00:22:15,520
used in production

00:22:13,679 --> 00:22:18,640
the answer is definitely yes so if you

00:22:15,520 --> 00:22:19,600
go to tvm conference 2019 you can find

00:22:18,640 --> 00:22:22,400
that

00:22:19,600 --> 00:22:23,760
you can find talks from uh different

00:22:22,400 --> 00:22:26,559
companies like

00:22:23,760 --> 00:22:28,559
elixirs weak word is one example that

00:22:26,559 --> 00:22:30,240
you know it's already tbm already powers

00:22:28,559 --> 00:22:33,120
the aws

00:22:30,240 --> 00:22:34,799
siege maker neo services uh it also

00:22:33,120 --> 00:22:38,320
drives some of the facebook

00:22:34,799 --> 00:22:40,400
as as click through product pipeline and

00:22:38,320 --> 00:22:42,480
you can find quite a few use cases

00:22:40,400 --> 00:22:43,280
actually nowadays using tvi in

00:22:42,480 --> 00:22:46,320
production

00:22:43,280 --> 00:22:46,799
on various use cases in both the cloud

00:22:46,320 --> 00:22:55,840
setting

00:22:46,799 --> 00:22:55,840
as well as the embedded settings

00:23:00,960 --> 00:23:04,799
and and we will also love to hear a lot

00:23:03,440 --> 00:23:08,000
more use cases

00:23:04,799 --> 00:23:23,840
uh if you are interested in uh using

00:23:08,000 --> 00:23:23,840
tbm in your product

00:23:43,919 --> 00:23:47,120
okay so uh let me know if it's a if

00:23:45,919 --> 00:23:49,279
there's any

00:23:47,120 --> 00:23:50,960
okay so the there's the additional

00:23:49,279 --> 00:23:52,720
question about is there is a hello world

00:23:50,960 --> 00:23:53,520
or docker container type demo to get

00:23:52,720 --> 00:23:56,200
started

00:23:53,520 --> 00:23:58,000
um tvm uh so if you go to

00:23:56,200 --> 00:24:00,640
tvm.parachute.org there is

00:23:58,000 --> 00:24:02,080
documents about how do you get started

00:24:00,640 --> 00:24:05,120
unfortunately right now

00:24:02,080 --> 00:24:07,600
uh there's well there is a docker

00:24:05,120 --> 00:24:10,159
container but on the other hand because

00:24:07,600 --> 00:24:11,919
uh the door container is is kind of a

00:24:10,159 --> 00:24:13,840
convenient generator so you can you can

00:24:11,919 --> 00:24:14,320
try to go and try to get quick started

00:24:13,840 --> 00:24:18,000
on

00:24:14,320 --> 00:24:20,320
the tutorials um um because there's a

00:24:18,000 --> 00:24:21,679
dependency on your vm so you kind of

00:24:20,320 --> 00:24:23,600
need to build from source

00:24:21,679 --> 00:24:25,440
but there are also third-party binaries

00:24:23,600 --> 00:24:26,320
that you can go and check out and

00:24:25,440 --> 00:24:29,520
install

00:24:26,320 --> 00:24:31,919
uh from from the

00:24:29,520 --> 00:24:33,279
from from a third party on the other

00:24:31,919 --> 00:24:35,600
hand the community

00:24:33,279 --> 00:24:37,760
usually maintains a fully apache

00:24:35,600 --> 00:24:40,640
compatible

00:24:37,760 --> 00:24:47,840
source release that you can go and build

00:24:40,640 --> 00:24:47,840
for your local use cases

00:24:53,200 --> 00:25:01,840
yeah welcome

00:25:08,320 --> 00:25:13,200
yeah so i think i will just stick around

00:25:10,559 --> 00:25:15,279
for a few more minutes since the

00:25:13,200 --> 00:25:17,200
officially the session still wouldn't

00:25:15,279 --> 00:25:20,640
end until

00:25:17,200 --> 00:25:21,600
9 40. if you have actions feel free to

00:25:20,640 --> 00:25:28,480
type in

00:25:21,600 --> 00:25:43,840
and yeah

00:25:28,480 --> 00:25:43,840
thank you felix

00:27:46,880 --> 00:27:50,880
okay thank you again for coming to work

00:27:48,799 --> 00:27:52,720
and then i'm going to leave for now but

00:27:50,880 --> 00:28:03,840
enjoy the rest of

00:27:52,720 --> 00:28:03,840
thanksgiving thanks folks for hosting

00:28:15,760 --> 00:28:17,840

YouTube URL: https://www.youtube.com/watch?v=QXp5ebZzLuE


