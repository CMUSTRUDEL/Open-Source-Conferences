Title: Deep Learning in Java
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	Deep Learning in Java
Qing Lan

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

AI is evolving rapidly, and is used widely in a variety of industries. Machine learning (ML) applications ranging from basic text classification to complex applications such as object detection and pose estimation are being developed to be used in enterprise applications. Currently, software engineers using Java have a large barrier to entry when they try to adopt Deep Learning (DL) for their applications. Python being the de-facto programming language for ML adds additional gradient to an already steep learning curve. This tutorial will introduce an open-source, framework-agnostic Java library â€” Deep Java Library (DJL) for high-performance training and inference in production. DJL supports a variety of Deep Learning engines (including but not limited to Apache MXNet, TensorFlow and PyTorch) and provides a simple and clean Java API that will work the same with each engine. Additionally, DJL offers the DJL Model Zoo - a repository of models that makes it easy to share models across teams. This tutorial will walk software engineers through the core features of DJL and demonstrate how it can be used to simplify experience of serving models. By the end of the session, users will be able to train and deploy DL models from a variety of DL frameworks into production environments and serve user requests using Java. Website: https://djl.ai/

Qing is a SDE II in the AWS Deep Learning Toolkits team. He is one of the co-authors of DJL (djl.ai) and PPMC member of Apache MXNet. He graduated from Columbia University in 2017 with a MS degree in Computer Engineering and has worked on model training and inference. Qing has presented a workshop about Apache MXNet in ApacheCon 2019(Las Vegas) about using Java for Deep Learning inference.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:26,880 --> 00:00:30,000
okay

00:00:27,519 --> 00:00:31,279
and hello everyone thanks for attending

00:00:30,000 --> 00:00:33,280
apachecon

00:00:31,279 --> 00:00:34,880
and good morning good afternoon and good

00:00:33,280 --> 00:00:35,840
evening to the people in the different

00:00:34,880 --> 00:00:37,440
time zone

00:00:35,840 --> 00:00:39,360
my name is ching lang i'm a software

00:00:37,440 --> 00:00:41,120
engineer inside of amazon machine

00:00:39,360 --> 00:00:42,879
learning machine learning group

00:00:41,120 --> 00:00:44,480
and today i'm going to present the topic

00:00:42,879 --> 00:00:47,039
for deep learning in java

00:00:44,480 --> 00:00:48,800
and everything with people learning and

00:00:47,039 --> 00:00:50,960
also java language

00:00:48,800 --> 00:00:52,480
firstly beforehand i saw the people in

00:00:50,960 --> 00:00:53,760
the chat room saying they're not

00:00:52,480 --> 00:00:56,160
familiar with deep lear

00:00:53,760 --> 00:00:58,000
they're learning the concept let's go

00:00:56,160 --> 00:01:01,840
over the right hand graph first

00:00:58,000 --> 00:01:04,320
so what is actually like a deep learning

00:01:01,840 --> 00:01:05,680
you may be familiar with ai like the ai

00:01:04,320 --> 00:01:09,600
application people tell

00:01:05,680 --> 00:01:11,119
in the in the in the normal region from

00:01:09,600 --> 00:01:12,799
the basic concept

00:01:11,119 --> 00:01:15,680
uh to do a simple dog and cat

00:01:12,799 --> 00:01:16,000
classification into the other tasks like

00:01:15,680 --> 00:01:17,280
uh

00:01:16,000 --> 00:01:19,200
they're they're recognizing your

00:01:17,280 --> 00:01:21,600
handwritten digit

00:01:19,200 --> 00:01:22,799
or or like the store like the the

00:01:21,600 --> 00:01:24,720
features snapchat

00:01:22,799 --> 00:01:26,320
and instagram brings that can actually

00:01:24,720 --> 00:01:29,200
putting a dog facing her

00:01:26,320 --> 00:01:31,759
interface these are all ai technology

00:01:29,200 --> 00:01:33,759
and below that is actually the machine

00:01:31,759 --> 00:01:35,759
learning technology it's a smaller

00:01:33,759 --> 00:01:38,720
domain inside of the ai because ai

00:01:35,759 --> 00:01:40,000
is very big it contains a basic

00:01:38,720 --> 00:01:43,040
rule-based system

00:01:40,000 --> 00:01:44,720
stuff and also the machine machine

00:01:43,040 --> 00:01:46,640
learning task

00:01:44,720 --> 00:01:48,240
and below the machine learning class is

00:01:46,640 --> 00:01:50,640
actually the deep learning

00:01:48,240 --> 00:01:52,960
region and deep learning is a subset for

00:01:50,640 --> 00:01:54,720
machine learning machine learning

00:01:52,960 --> 00:01:56,240
and typically machine learning can be

00:01:54,720 --> 00:01:58,240
categorized as

00:01:56,240 --> 00:01:59,439
a supervised learning and also

00:01:58,240 --> 00:02:01,360
unsupervised

00:01:59,439 --> 00:02:03,200
and deep learn deep learning fall into

00:02:01,360 --> 00:02:03,680
the supervised learning by people

00:02:03,200 --> 00:02:05,840
feeding

00:02:03,680 --> 00:02:07,840
in the corresponding data and you can

00:02:05,840 --> 00:02:10,959
generate food

00:02:07,840 --> 00:02:11,280
and on the left hand side is actually

00:02:10,959 --> 00:02:13,440
the

00:02:11,280 --> 00:02:15,280
the development trend inside of our

00:02:13,440 --> 00:02:18,080
current academia and although

00:02:15,280 --> 00:02:19,680
also industrial field so for the

00:02:18,080 --> 00:02:22,640
traditional

00:02:19,680 --> 00:02:23,840
programming side at the beginning people

00:02:22,640 --> 00:02:27,200
building up the rules

00:02:23,840 --> 00:02:29,599
simply by defining uh how how you want

00:02:27,200 --> 00:02:30,480
to define this by by writing up even the

00:02:29,599 --> 00:02:33,599
simple

00:02:30,480 --> 00:02:35,680
if statement they can say if the people

00:02:33,599 --> 00:02:38,080
type in some kind of words are going to

00:02:35,680 --> 00:02:40,480
give them the corresponding answers

00:02:38,080 --> 00:02:42,319
and this purely rule-based system has

00:02:40,480 --> 00:02:43,280
been long long-lasting for the first

00:02:42,319 --> 00:02:46,400
generation

00:02:43,280 --> 00:02:48,319
applications and and and surprisingly

00:02:46,400 --> 00:02:50,319
and i have here even hear the rumors

00:02:48,319 --> 00:02:52,720
that people talking about series before

00:02:50,319 --> 00:02:53,519
2012 they're also using the traditional

00:02:52,720 --> 00:02:55,599
programming

00:02:53,519 --> 00:02:57,599
by written up the thousand lines of

00:02:55,599 --> 00:03:00,879
rules by defining what was

00:02:57,599 --> 00:03:03,120
the user trying to type and and and

00:03:00,879 --> 00:03:04,720
talk when when they're talking to to the

00:03:03,120 --> 00:03:07,599
agent

00:03:04,720 --> 00:03:09,360
and after that uh there are machine

00:03:07,599 --> 00:03:11,680
learning machine learning technology

00:03:09,360 --> 00:03:12,640
and it's a simple concept that people

00:03:11,680 --> 00:03:15,920
can having

00:03:12,640 --> 00:03:17,280
the data and also answer feeding the

00:03:15,920 --> 00:03:19,680
machine learning models

00:03:17,280 --> 00:03:21,200
and it can generate for the rules it's a

00:03:19,680 --> 00:03:23,760
simple feeding algorithm

00:03:21,200 --> 00:03:25,040
you can imagine a simple algorithm like

00:03:23,760 --> 00:03:28,560
y equals to

00:03:25,040 --> 00:03:30,000
a x plus b and by by feeding the

00:03:28,560 --> 00:03:31,840
corresponding x and y

00:03:30,000 --> 00:03:34,159
they can use the feeding algorithm to

00:03:31,840 --> 00:03:37,360
find the best weight and bias

00:03:34,159 --> 00:03:38,400
inside of this algorithm and and the

00:03:37,360 --> 00:03:41,440
rules in here

00:03:38,400 --> 00:03:43,120
is corresponding to the to the w to to

00:03:41,440 --> 00:03:45,200
the weight and the bias

00:03:43,120 --> 00:03:46,879
and by generating these kinds of the

00:03:45,200 --> 00:03:49,200
algorithm they by

00:03:46,879 --> 00:03:51,920
after that they can just simply fitting

00:03:49,200 --> 00:03:53,120
up the the data inside and the algorithm

00:03:51,920 --> 00:03:56,319
will generate

00:03:53,120 --> 00:03:58,400
the the best gas for the answers

00:03:56,319 --> 00:04:00,319
and deep learn deep learning is also

00:03:58,400 --> 00:04:02,319
using the similar terminology

00:04:00,319 --> 00:04:04,959
that doing the supervised learning by

00:04:02,319 --> 00:04:07,760
having this in mind

00:04:04,959 --> 00:04:09,280
um i i won't be going too much for the

00:04:07,760 --> 00:04:11,840
deep learning itself because

00:04:09,280 --> 00:04:14,480
uh our major roles in here is all about

00:04:11,840 --> 00:04:16,880
the like deep learnings of java

00:04:14,480 --> 00:04:18,320
and if you have any questions with deep

00:04:16,880 --> 00:04:20,239
learning itself please raise

00:04:18,320 --> 00:04:22,479
your questions in the chat room and i'm

00:04:20,239 --> 00:04:25,120
going to handle that after the session

00:04:22,479 --> 00:04:25,919
and why java for deep learning in our

00:04:25,120 --> 00:04:27,840
cases

00:04:25,919 --> 00:04:30,400
and i saw there there's actually like a

00:04:27,840 --> 00:04:32,080
trend inside the enterprise customers

00:04:30,400 --> 00:04:34,080
where they're trying to use java for

00:04:32,080 --> 00:04:35,680
their deep learning use cases because

00:04:34,080 --> 00:04:37,040
most of the services

00:04:35,680 --> 00:04:39,199
inside the world they're actually

00:04:37,040 --> 00:04:40,160
building itself java like tomcat-based

00:04:39,199 --> 00:04:42,720
services

00:04:40,160 --> 00:04:44,720
and also the big data processing like

00:04:42,720 --> 00:04:46,080
people trying to use a party spark to

00:04:44,720 --> 00:04:49,360
solve their problem

00:04:46,080 --> 00:04:51,120
and also the stream data processing uh

00:04:49,360 --> 00:04:53,759
problem people are trying to use apache

00:04:51,120 --> 00:04:54,720
flink and these are all built inside of

00:04:53,759 --> 00:04:58,720
the

00:04:54,720 --> 00:05:02,320
existing jvm domain and also the

00:04:58,720 --> 00:05:02,880
existing java of for existing java api

00:05:02,320 --> 00:05:05,120
for deep

00:05:02,880 --> 00:05:05,919
learning is kind of painful because

00:05:05,120 --> 00:05:09,440
mostly

00:05:05,919 --> 00:05:10,479
there are kind of low level apis and

00:05:09,440 --> 00:05:12,880
also

00:05:10,479 --> 00:05:13,840
it is kind of raw for them to use uh

00:05:12,880 --> 00:05:17,039
it's there there

00:05:13,840 --> 00:05:18,880
are broken pieces and uh and the memory

00:05:17,039 --> 00:05:23,120
cost is very hard to

00:05:18,880 --> 00:05:24,880
to to to to keep and also by having that

00:05:23,120 --> 00:05:27,360
java api deploying

00:05:24,880 --> 00:05:29,440
your machine learning um stuff inside of

00:05:27,360 --> 00:05:31,039
java is also very challenging

00:05:29,440 --> 00:05:33,600
and because if you're considered with

00:05:31,039 --> 00:05:35,680
cloud environment you really need to

00:05:33,600 --> 00:05:37,440
keep the environment safe without any

00:05:35,680 --> 00:05:40,479
kind of memory overflow

00:05:37,440 --> 00:05:42,639
or or the

00:05:40,479 --> 00:05:44,160
the other issues like sag fault when

00:05:42,639 --> 00:05:46,000
you're trying to do the modest threading

00:05:44,160 --> 00:05:48,000
inference or training task

00:05:46,000 --> 00:05:49,360
so if the maintenance cost is also very

00:05:48,000 --> 00:05:52,000
high for the

00:05:49,360 --> 00:05:53,919
existing enterprise users they are

00:05:52,000 --> 00:05:58,000
trying to leverage on the java

00:05:53,919 --> 00:05:59,600
and the second problem is uh there's

00:05:58,000 --> 00:06:01,759
deep learning standard in the java

00:05:59,600 --> 00:06:03,680
community um

00:06:01,759 --> 00:06:05,360
they are actually one inside of python

00:06:03,680 --> 00:06:07,680
right for now they're people trying to

00:06:05,360 --> 00:06:09,039
uh getting their api closer to the numpy

00:06:07,680 --> 00:06:09,759
but if you're trying to use the java

00:06:09,039 --> 00:06:11,759
like

00:06:09,759 --> 00:06:13,600
well when you're trying to run in the

00:06:11,759 --> 00:06:15,280
with the different java machine learning

00:06:13,600 --> 00:06:16,000
and deep learning package you'll find

00:06:15,280 --> 00:06:18,639
that

00:06:16,000 --> 00:06:20,560
the standard for for the java is all is

00:06:18,639 --> 00:06:22,560
all different

00:06:20,560 --> 00:06:24,080
and thirdly deep learning with strongly

00:06:22,560 --> 00:06:26,319
typed languages

00:06:24,080 --> 00:06:28,080
because uh if you're trying to look into

00:06:26,319 --> 00:06:29,280
the code especially written in python

00:06:28,080 --> 00:06:31,120
you'll figure that

00:06:29,280 --> 00:06:32,400
the readability for the python code

00:06:31,120 --> 00:06:35,120
isn't very good

00:06:32,400 --> 00:06:35,600
because they didn't provide us like the

00:06:35,120 --> 00:06:38,720
type

00:06:35,600 --> 00:06:39,919
of the of the the data set although we

00:06:38,720 --> 00:06:41,680
can infer that during the

00:06:39,919 --> 00:06:43,440
the runtime but if you're considering

00:06:41,680 --> 00:06:45,759
with a java service code

00:06:43,440 --> 00:06:47,600
that the readability and reliability is

00:06:45,759 --> 00:06:49,280
truly important because we really don't

00:06:47,600 --> 00:06:50,880
want to make any mistakes

00:06:49,280 --> 00:06:52,319
before we throw that code into the

00:06:50,880 --> 00:06:53,759
product environment and running for

00:06:52,319 --> 00:06:57,680
hundreds of hours

00:06:53,759 --> 00:06:59,360
right so by having that in mind i'm

00:06:57,680 --> 00:07:01,360
going to introduce you the

00:06:59,360 --> 00:07:03,520
the toolkit that amazon web service

00:07:01,360 --> 00:07:03,919
created called deep job library trying

00:07:03,520 --> 00:07:06,800
to

00:07:03,919 --> 00:07:09,120
solve the problem with deep learning in

00:07:06,800 --> 00:07:09,120
java

00:07:09,759 --> 00:07:14,319
so like the reason that deep learning

00:07:12,400 --> 00:07:16,240
and the dvd library uh

00:07:14,319 --> 00:07:17,360
came in here is because there are

00:07:16,240 --> 00:07:19,919
actually teams not

00:07:17,360 --> 00:07:21,759
amazon they ask us if there is a single

00:07:19,919 --> 00:07:25,280
solutions we can write

00:07:21,759 --> 00:07:27,280
we can use java on in our product system

00:07:25,280 --> 00:07:28,560
with all different kind of deep learning

00:07:27,280 --> 00:07:31,599
framework and

00:07:28,560 --> 00:07:34,080
they have spent days and even months to

00:07:31,599 --> 00:07:36,160
to build up their own java api just to

00:07:34,080 --> 00:07:38,880
solve the problem to run inference

00:07:36,160 --> 00:07:39,759
within in their pro system and that's

00:07:38,880 --> 00:07:42,319
the and

00:07:39,759 --> 00:07:44,319
and that is the initial reason why deep

00:07:42,319 --> 00:07:47,599
java library came in here

00:07:44,319 --> 00:07:48,319
and draw library is is simply designed

00:07:47,599 --> 00:07:50,639
for java

00:07:48,319 --> 00:07:52,400
it's support for multi-threading it also

00:07:50,639 --> 00:07:55,280
have the memory con

00:07:52,400 --> 00:07:56,160
control system and all the api signature

00:07:55,280 --> 00:07:58,479
are

00:07:56,160 --> 00:07:59,440
developed into the java friendly way

00:07:58,479 --> 00:08:01,680
where you

00:07:59,440 --> 00:08:02,479
you can do everything inside the java

00:08:01,680 --> 00:08:04,960
way and

00:08:02,479 --> 00:08:06,319
also having the benefit doing everything

00:08:04,960 --> 00:08:09,199
that is actually for deep

00:08:06,319 --> 00:08:09,680
deep learning tasks and second benefit

00:08:09,199 --> 00:08:11,520
is

00:08:09,680 --> 00:08:13,680
we're following the java standard like

00:08:11,520 --> 00:08:16,000
right right once run anywhere

00:08:13,680 --> 00:08:17,759
we have a single layer front end api

00:08:16,000 --> 00:08:19,039
that applies to the all deep learning

00:08:17,759 --> 00:08:20,319
engine will currently support like

00:08:19,039 --> 00:08:22,879
apache mxnet

00:08:20,319 --> 00:08:23,759
pytorch and tensorflow whereas if you're

00:08:22,879 --> 00:08:26,479
trying to run

00:08:23,759 --> 00:08:27,440
an image classification task with um

00:08:26,479 --> 00:08:30,639
with with

00:08:27,440 --> 00:08:31,680
let's say with apache mxnet and if we're

00:08:30,639 --> 00:08:33,440
trying to switch into

00:08:31,680 --> 00:08:35,039
into the pi torch you barely need to

00:08:33,440 --> 00:08:37,200
change any part of the code

00:08:35,039 --> 00:08:39,120
you only need to to change your your

00:08:37,200 --> 00:08:40,719
your dependency import

00:08:39,120 --> 00:08:42,479
and the same code can be run with

00:08:40,719 --> 00:08:45,279
different learning engines and they will

00:08:42,479 --> 00:08:47,920
also have the same behavior

00:08:45,279 --> 00:08:48,880
we also bring something called moto zoo

00:08:47,920 --> 00:08:52,320
it contains

00:08:48,880 --> 00:08:53,040
70 different pre-trained models out of

00:08:52,320 --> 00:08:55,279
box where

00:08:53,040 --> 00:08:57,600
you can use them directly in your

00:08:55,279 --> 00:08:59,680
inference and training applications

00:08:57,600 --> 00:09:01,120
and the models came from the variety

00:08:59,680 --> 00:09:04,320
sources like luang cv

00:09:01,120 --> 00:09:06,800
grand nlp and also

00:09:04,320 --> 00:09:09,279
torch hub hogging phase and also keras

00:09:06,800 --> 00:09:11,920
preaching model zoo

00:09:09,279 --> 00:09:14,320
and amazon has actively contributed open

00:09:11,920 --> 00:09:15,920
source machine learning projects

00:09:14,320 --> 00:09:17,440
including the following like tensorflow

00:09:15,920 --> 00:09:21,600
and mxn and and

00:09:17,440 --> 00:09:22,720
and pytorch learning from the community

00:09:21,600 --> 00:09:24,560
we gradually

00:09:22,720 --> 00:09:26,480
understand what are the demand for the

00:09:24,560 --> 00:09:28,080
java customers and we finally

00:09:26,480 --> 00:09:30,000
built up the single layer deep draw

00:09:28,080 --> 00:09:32,800
library here and also made it

00:09:30,000 --> 00:09:33,360
open source to benefit for the for the

00:09:32,800 --> 00:09:35,279
all

00:09:33,360 --> 00:09:37,760
java developer developers and deep

00:09:35,279 --> 00:09:40,160
learning developers

00:09:37,760 --> 00:09:43,680
and and all the lessons we use inside is

00:09:40,160 --> 00:09:45,360
apache 2.0 because we we understand that

00:09:43,680 --> 00:09:48,320
the true open source

00:09:45,360 --> 00:09:49,600
that is actually fall under the apache

00:09:48,320 --> 00:09:51,920
lessons

00:09:49,600 --> 00:09:53,200
and we also supporting for different

00:09:51,920 --> 00:09:55,519
apache projects

00:09:53,200 --> 00:09:56,720
like we we have the packages installed

00:09:55,519 --> 00:09:58,880
inside of

00:09:56,720 --> 00:10:00,000
apache camel we have a closed

00:09:58,880 --> 00:10:04,160
integration with

00:10:00,000 --> 00:10:06,560
apache mxnet and also we have the the

00:10:04,160 --> 00:10:07,760
example integration with apache spark

00:10:06,560 --> 00:10:10,480
apache flink

00:10:07,760 --> 00:10:12,560
and apache nifi and also there are

00:10:10,480 --> 00:10:13,440
upcoming other projects that coming

00:10:12,560 --> 00:10:16,480
towards

00:10:13,440 --> 00:10:19,839
that we're going to bring uh to benefit

00:10:16,480 --> 00:10:19,839
for the whole apache community

00:10:20,480 --> 00:10:24,480
so now let's move on into the

00:10:22,160 --> 00:10:26,720
architecture for detroit library

00:10:24,480 --> 00:10:28,640
so firstly different library has a

00:10:26,720 --> 00:10:31,600
single layer java interface

00:10:28,640 --> 00:10:32,000
that been defined universal across

00:10:31,600 --> 00:10:34,480
different

00:10:32,000 --> 00:10:35,200
deep learning engines so you you can't

00:10:34,480 --> 00:10:36,480
expect

00:10:35,200 --> 00:10:38,560
whatever you're going to write you will

00:10:36,480 --> 00:10:39,440
have the same behavior across the

00:10:38,560 --> 00:10:42,000
different deep

00:10:39,440 --> 00:10:42,480
learning engine and the first thing at

00:10:42,000 --> 00:10:45,760
first

00:10:42,480 --> 00:10:49,200
is called nd array and the array here

00:10:45,760 --> 00:10:49,839
un-dimensional uh rate that representing

00:10:49,200 --> 00:10:52,480
some

00:10:49,839 --> 00:10:54,079
higher dimensional data set instead of a

00:10:52,480 --> 00:10:57,839
java.java domain

00:10:54,079 --> 00:10:59,200
the original array representation in

00:10:57,839 --> 00:11:01,440
java is kind of weak

00:10:59,200 --> 00:11:03,279
if you're trying to use it to represent

00:11:01,440 --> 00:11:07,519
some complicated

00:11:03,279 --> 00:11:08,240
use cases and also we have some other

00:11:07,519 --> 00:11:10,880
stuff like

00:11:08,240 --> 00:11:11,760
model and inside of the model is

00:11:10,880 --> 00:11:14,320
essentially

00:11:11,760 --> 00:11:16,399
you can load and train the model with

00:11:14,320 --> 00:11:17,040
the two classes that spawn from from the

00:11:16,399 --> 00:11:19,440
model

00:11:17,040 --> 00:11:21,200
cases so first thing called is trainer

00:11:19,440 --> 00:11:23,920
you can train the model with trainer

00:11:21,200 --> 00:11:24,240
and second thing is called predictor

00:11:23,920 --> 00:11:27,279
where

00:11:24,240 --> 00:11:29,200
you can run the inference with that and

00:11:27,279 --> 00:11:31,760
also there's another key classic we call

00:11:29,200 --> 00:11:33,760
this block it's actually the con

00:11:31,760 --> 00:11:35,200
the container for the different new

00:11:33,760 --> 00:11:37,920
neural networks

00:11:35,200 --> 00:11:38,720
by having these threes it it builds up

00:11:37,920 --> 00:11:40,320
the

00:11:38,720 --> 00:11:42,399
whole building blocks for free for you

00:11:40,320 --> 00:11:42,640
to actually try to to run the training

00:11:42,399 --> 00:11:45,279
and

00:11:42,640 --> 00:11:47,279
influence with with java inside of the

00:11:45,279 --> 00:11:49,920
deep learning domain

00:11:47,279 --> 00:11:50,720
and below that we have the java engine

00:11:49,920 --> 00:11:52,880
layer

00:11:50,720 --> 00:11:56,000
which corresponding to the different

00:11:52,880 --> 00:11:59,200
implementation inside of java

00:11:56,000 --> 00:12:00,480
uh that's corresponding with mxn pytorch

00:11:59,200 --> 00:12:02,320
and tensorflow in here

00:12:00,480 --> 00:12:04,320
we're basically doing the translation

00:12:02,320 --> 00:12:07,200
work that going to translate

00:12:04,320 --> 00:12:10,079
their our original behavior to the

00:12:07,200 --> 00:12:13,040
universal behavior on the front end

00:12:10,079 --> 00:12:13,920
and below that we have the c plus us

00:12:13,040 --> 00:12:16,240
layer

00:12:13,920 --> 00:12:17,120
we have built or used the corresponding

00:12:16,240 --> 00:12:20,079
uh gi

00:12:17,120 --> 00:12:22,160
and jna layers that are actually calling

00:12:20,079 --> 00:12:25,360
directly with the c and c process

00:12:22,160 --> 00:12:27,920
api with with different learning engine

00:12:25,360 --> 00:12:30,000
this will ensure us to have the best uh

00:12:27,920 --> 00:12:32,320
performance comparing to python

00:12:30,000 --> 00:12:34,480
because we are all using the same and

00:12:32,320 --> 00:12:36,320
the same api in here and it will also

00:12:34,480 --> 00:12:38,240
bring us the same behavior

00:12:36,320 --> 00:12:40,000
if you're trying to run the training and

00:12:38,240 --> 00:12:41,120
inverse test comparing with java and

00:12:40,000 --> 00:12:43,040
python

00:12:41,120 --> 00:12:44,560
and we also support four different

00:12:43,040 --> 00:12:47,120
hardwares like mac os

00:12:44,560 --> 00:12:48,079
linux windows and also even android

00:12:47,120 --> 00:12:51,040
devices

00:12:48,079 --> 00:12:53,279
and we support for both cpu and gpu use

00:12:51,040 --> 00:12:55,040
cases

00:12:53,279 --> 00:12:56,880
and let's now move on to something

00:12:55,040 --> 00:12:58,399
that's really fun um

00:12:56,880 --> 00:13:00,000
on the left hand side you can actually

00:12:58,399 --> 00:13:01,120
see this and it's an android

00:13:00,000 --> 00:13:02,880
applications

00:13:01,120 --> 00:13:05,040
it's a doodle draw application that

00:13:02,880 --> 00:13:06,720
trains from from from from the

00:13:05,040 --> 00:13:09,360
pie torch with the google quick draw

00:13:06,720 --> 00:13:09,680
data set and it will actually recognize

00:13:09,360 --> 00:13:11,519
the

00:13:09,680 --> 00:13:13,760
the the doodle you're drawing on the

00:13:11,519 --> 00:13:14,639
screen like here i'm drawing i'm trying

00:13:13,760 --> 00:13:17,680
to draw like a

00:13:14,639 --> 00:13:19,839
like a like a rabbit in here and

00:13:17,680 --> 00:13:22,399
below that it's actually pre-presenting

00:13:19,839 --> 00:13:24,079
the inference result

00:13:22,399 --> 00:13:26,079
while you're trying to try to run to run

00:13:24,079 --> 00:13:28,320
this kind of thing and uh

00:13:26,079 --> 00:13:30,240
surprisingly the total inference code

00:13:28,320 --> 00:13:32,639
inside of this android application i

00:13:30,240 --> 00:13:34,959
mean on the i mean the inference side is

00:13:32,639 --> 00:13:37,040
less than 20 lines of code and it's also

00:13:34,959 --> 00:13:38,639
available from our demo site

00:13:37,040 --> 00:13:40,560
and it's been running on the on the

00:13:38,639 --> 00:13:43,120
android device mostly

00:13:40,560 --> 00:13:44,720
and uh to build up the the deep jaw

00:13:43,120 --> 00:13:47,040
library with the

00:13:44,720 --> 00:13:48,160
with the ai application it's very easy

00:13:47,040 --> 00:13:50,320
basically you

00:13:48,160 --> 00:13:51,839
you really need to have much more

00:13:50,320 --> 00:13:54,560
machine learning learning experiences

00:13:51,839 --> 00:13:56,959
we have the different packages ready for

00:13:54,560 --> 00:13:59,519
you to import and also we have uh

00:13:56,959 --> 00:14:00,079
already provided different uh pre

00:13:59,519 --> 00:14:03,279
picturing

00:14:00,079 --> 00:14:05,519
models where and and and even you can

00:14:03,279 --> 00:14:06,399
you use the model that is already

00:14:05,519 --> 00:14:08,560
trained um

00:14:06,399 --> 00:14:10,240
by using some other sources and and

00:14:08,560 --> 00:14:11,760
import to deepdraw library

00:14:10,240 --> 00:14:13,920
for your own inference and training

00:14:11,760 --> 00:14:17,440
cases

00:14:13,920 --> 00:14:17,760
so and deep dry library also supporting

00:14:17,440 --> 00:14:20,320
for

00:14:17,760 --> 00:14:21,279
the advanced use cases such as the

00:14:20,320 --> 00:14:23,680
training

00:14:21,279 --> 00:14:25,920
item has been designed for the engine uh

00:14:23,680 --> 00:14:28,959
for the for the apache mxn and

00:14:25,920 --> 00:14:31,120
and high torch we have the 40 data set

00:14:28,959 --> 00:14:32,000
that's supporting for both cv and nlp

00:14:31,120 --> 00:14:33,920
training

00:14:32,000 --> 00:14:36,160
similarly we provide the different

00:14:33,920 --> 00:14:37,199
building blocks for the cv and nlp

00:14:36,160 --> 00:14:38,880
models

00:14:37,199 --> 00:14:40,560
you can use the pre-trained model for

00:14:38,880 --> 00:14:42,639
the transfer learning or simply train

00:14:40,560 --> 00:14:44,880
from scratch

00:14:42,639 --> 00:14:47,120
we also have the advanced feature like

00:14:44,880 --> 00:14:48,320
multi gpu support you can run with multi

00:14:47,120 --> 00:14:51,440
gpu training

00:14:48,320 --> 00:14:52,079
and also the transfer learning uh with

00:14:51,440 --> 00:14:55,440
your

00:14:52,079 --> 00:14:57,360
existing model and apart from that we

00:14:55,440 --> 00:14:59,839
also have the built-in metric system

00:14:57,360 --> 00:15:01,440
which allow you to quickly collect the

00:14:59,839 --> 00:15:03,360
the key information you want to get like

00:15:01,440 --> 00:15:04,959
the speed of training like the accuracy

00:15:03,360 --> 00:15:06,720
and the loss information

00:15:04,959 --> 00:15:08,320
so these are crucial for the training

00:15:06,720 --> 00:15:09,440
use cases to get the better

00:15:08,320 --> 00:15:11,519
representation about

00:15:09,440 --> 00:15:14,399
what are the actual outcome of your

00:15:11,519 --> 00:15:16,480
training workflow

00:15:14,399 --> 00:15:18,240
and before we we jump into the next

00:15:16,480 --> 00:15:20,720
sections uh we would like to

00:15:18,240 --> 00:15:21,680
introduce you some some book that amazon

00:15:20,720 --> 00:15:25,040
created

00:15:21,680 --> 00:15:27,440
uh for the different use cases so

00:15:25,040 --> 00:15:30,240
we inside of the amazon science team we

00:15:27,440 --> 00:15:32,880
created the dive into deep learning book

00:15:30,240 --> 00:15:33,759
it contains 150 runnable jupiter

00:15:32,880 --> 00:15:36,000
notebook

00:15:33,759 --> 00:15:38,240
from the model architecture to the

00:15:36,000 --> 00:15:40,079
applications instead of different cv and

00:15:38,240 --> 00:15:42,959
alp use cases

00:15:40,079 --> 00:15:44,160
it also are adopted as a textbook and

00:15:42,959 --> 00:15:47,600
and the reference book

00:15:44,160 --> 00:15:50,480
in the at uc berkeley cmu mit

00:15:47,600 --> 00:15:51,199
and 70 more universities worldwide it's

00:15:50,480 --> 00:15:53,199
very it's

00:15:51,199 --> 00:15:54,560
been designed very easy to learn and

00:15:53,199 --> 00:15:56,959
people with no

00:15:54,560 --> 00:15:58,480
deep learning technology they can they

00:15:56,959 --> 00:16:00,639
can make a starting here

00:15:58,480 --> 00:16:03,440
by running with the jupiter notebook as

00:16:00,639 --> 00:16:04,880
long and also following the flow

00:16:03,440 --> 00:16:06,800
that we want to teach about the

00:16:04,880 --> 00:16:08,000
different concepts you will learn how

00:16:06,800 --> 00:16:10,720
you can creating

00:16:08,000 --> 00:16:12,560
like a simple uh neural network like

00:16:10,720 --> 00:16:16,079
like with with a single layer

00:16:12,560 --> 00:16:17,759
and and also to the modern uh like the

00:16:16,079 --> 00:16:19,120
convolutional neural networks with the

00:16:17,759 --> 00:16:22,240
cv application

00:16:19,120 --> 00:16:25,199
and also it contains nlp information

00:16:22,240 --> 00:16:26,399
so now we also bring something similar

00:16:25,199 --> 00:16:29,440
we we we're currently

00:16:26,399 --> 00:16:32,639
implementing the it's a dive into deep

00:16:29,440 --> 00:16:34,320
learning this book instead of java and i

00:16:32,639 --> 00:16:36,160
think it's also recognized the first

00:16:34,320 --> 00:16:37,199
java book for the for the deep learning

00:16:36,160 --> 00:16:40,000
in our

00:16:37,199 --> 00:16:41,839
use cases and before the session started

00:16:40,000 --> 00:16:43,600
i already put the link

00:16:41,839 --> 00:16:44,880
in the in the chat room where we can

00:16:43,600 --> 00:16:47,120
check it out

00:16:44,880 --> 00:16:48,240
and we currently finish up the first

00:16:47,120 --> 00:16:50,639
seven chapters

00:16:48,240 --> 00:16:52,639
that all design inside of java where you

00:16:50,639 --> 00:16:53,360
can use deep divide library along with

00:16:52,639 --> 00:16:56,560
the book

00:16:53,360 --> 00:16:58,399
to learn the basic use cases with deep

00:16:56,560 --> 00:17:00,240
learning itself and also the java

00:16:58,399 --> 00:17:02,079
technology

00:17:00,240 --> 00:17:04,079
and we're trying to make this book as an

00:17:02,079 --> 00:17:06,079
industrial industrial solution

00:17:04,079 --> 00:17:07,919
for the for the deep learning use cases

00:17:06,079 --> 00:17:10,880
because for its simple assistance

00:17:07,919 --> 00:17:12,319
for its simplicity and also for it at

00:17:10,880 --> 00:17:15,520
once

00:17:12,319 --> 00:17:18,480
usages you can check it offline after

00:17:15,520 --> 00:17:19,600
i can i can also share the slides uh you

00:17:18,480 --> 00:17:21,839
can check out this book

00:17:19,600 --> 00:17:24,000
it's very good it's very useful you can

00:17:21,839 --> 00:17:25,439
run it online and also locally with your

00:17:24,000 --> 00:17:27,439
own joker notebook

00:17:25,439 --> 00:17:28,799
so there's a lot of fun to do that all

00:17:27,439 --> 00:17:31,360
right

00:17:28,799 --> 00:17:32,480
and now let's move on some real meat how

00:17:31,360 --> 00:17:34,400
deep your library

00:17:32,480 --> 00:17:36,240
solves the deep learning solutions for

00:17:34,400 --> 00:17:39,120
the customer

00:17:36,240 --> 00:17:39,520
production use cases before we jump into

00:17:39,120 --> 00:17:41,440
that

00:17:39,520 --> 00:17:42,559
let's talk about the key advantages that

00:17:41,440 --> 00:17:45,600
the library

00:17:42,559 --> 00:17:46,160
actually bring firstly it's very easy to

00:17:45,600 --> 00:17:48,480
set up

00:17:46,160 --> 00:17:50,480
you you can you can use you can just

00:17:48,480 --> 00:17:53,520
simply write 10 lines of code

00:17:50,480 --> 00:17:56,160
to run for your inference task

00:17:53,520 --> 00:17:57,919
and uh for sometimes it's usually last

00:17:56,160 --> 00:18:00,080
belong like 10 to 20 lines

00:17:57,919 --> 00:18:02,320
and i always say most of the cases won't

00:18:00,080 --> 00:18:05,280
be more than 20 lines of code

00:18:02,320 --> 00:18:06,640
um and it also has very very minimum

00:18:05,280 --> 00:18:08,480
dependency requirements

00:18:06,640 --> 00:18:09,919
do okay expect if you want to try and

00:18:08,480 --> 00:18:12,400
use mx dams

00:18:09,919 --> 00:18:13,520
tensorflow and and pi torch the total

00:18:12,400 --> 00:18:16,000
dependency

00:18:13,520 --> 00:18:17,120
for for to import is only two three

00:18:16,000 --> 00:18:19,039
different packages

00:18:17,120 --> 00:18:21,200
with the total package size less than

00:18:19,039 --> 00:18:23,520
200 max

00:18:21,200 --> 00:18:25,760
and debra library is designed to be very

00:18:23,520 --> 00:18:28,160
fast especially on the java service

00:18:25,760 --> 00:18:29,520
it can get up to two times performance

00:18:28,160 --> 00:18:31,840
boost on some small

00:18:29,520 --> 00:18:33,760
model inference and the terms for the

00:18:31,840 --> 00:18:36,720
small modeling heroes means this

00:18:33,760 --> 00:18:37,360
model has less of the less of the

00:18:36,720 --> 00:18:40,000
features

00:18:37,360 --> 00:18:41,840
such as squeeze net we have benchmark on

00:18:40,000 --> 00:18:42,240
the squeeze and it can bring us two to

00:18:41,840 --> 00:18:43,840
seven

00:18:42,240 --> 00:18:47,280
times performance boost if you're trying

00:18:43,840 --> 00:18:50,400
to run it on the server comparing to the

00:18:47,280 --> 00:18:52,400
existing motor server solution and this

00:18:50,400 --> 00:18:53,120
technology has been used widely by the

00:18:52,400 --> 00:18:55,600
azim car

00:18:53,120 --> 00:18:57,679
understanding inside of amazon and also

00:18:55,600 --> 00:19:00,240
some small image classification from our

00:18:57,679 --> 00:19:03,039
external customers

00:19:00,240 --> 00:19:04,640
and besides that uh deepdraw library

00:19:03,039 --> 00:19:05,360
also have the support for the large

00:19:04,640 --> 00:19:07,520
scale

00:19:05,360 --> 00:19:09,919
where you can run the offline data

00:19:07,520 --> 00:19:11,039
processes with more than 800 million

00:19:09,919 --> 00:19:13,840
inverse load

00:19:11,039 --> 00:19:15,760
with apache spark uh and and on a

00:19:13,840 --> 00:19:19,440
cluster of machines

00:19:15,760 --> 00:19:20,160
and uh and also you can try to use dpr

00:19:19,440 --> 00:19:22,960
library

00:19:20,160 --> 00:19:23,440
with online streaming tasks such as you

00:19:22,960 --> 00:19:25,440
can

00:19:23,440 --> 00:19:26,559
you can use difficult library as a as a

00:19:25,440 --> 00:19:29,280
middleware

00:19:26,559 --> 00:19:30,160
instead of uh apache flink since it's a

00:19:29,280 --> 00:19:32,320
java package

00:19:30,160 --> 00:19:33,200
so you simply just import this package

00:19:32,320 --> 00:19:35,840
and do all the

00:19:33,200 --> 00:19:37,600
all the inference tasks and we also

00:19:35,840 --> 00:19:40,640
provide the corresponding

00:19:37,600 --> 00:19:41,840
examples in our in our demo site where

00:19:40,640 --> 00:19:44,559
you can try

00:19:41,840 --> 00:19:46,559
you can try that to get the integration

00:19:44,559 --> 00:19:48,240
and finally the key reason that people

00:19:46,559 --> 00:19:51,039
choose deep down library is is

00:19:48,240 --> 00:19:52,080
because it's very stable we have doing

00:19:51,039 --> 00:19:55,200
the consistent

00:19:52,080 --> 00:19:57,679
uh in continuous inference call

00:19:55,200 --> 00:19:58,880
based on all different releases we're

00:19:57,679 --> 00:20:02,559
trying to bring there

00:19:58,880 --> 00:20:04,559
with more than 100 hours benchmark tax

00:20:02,559 --> 00:20:06,240
in the real production use cases we

00:20:04,559 --> 00:20:08,480
really don't want things crashed

00:20:06,240 --> 00:20:11,200
right if if your inference or training

00:20:08,480 --> 00:20:12,320
service is continuous crashing you won't

00:20:11,200 --> 00:20:15,440
be thinking

00:20:12,320 --> 00:20:15,919
this is a re reliable solution you want

00:20:15,440 --> 00:20:18,559
to choose

00:20:15,919 --> 00:20:20,159
so we want to keep it very stable and

00:20:18,559 --> 00:20:22,799
the long-lasting service we

00:20:20,159 --> 00:20:23,360
have been now it's been lasting for more

00:20:22,799 --> 00:20:28,240
than six

00:20:23,360 --> 00:20:30,559
days um without any problem

00:20:28,240 --> 00:20:33,840
so let's jump into the first use cases

00:20:30,559 --> 00:20:37,200
by deep jaw library on page

00:20:33,840 --> 00:20:38,080
and again it has very minimum dependency

00:20:37,200 --> 00:20:40,080
requirements

00:20:38,080 --> 00:20:41,360
which are actually saving times once

00:20:40,080 --> 00:20:43,039
you're trying to run everything inside

00:20:41,360 --> 00:20:44,880
of a spark machine

00:20:43,039 --> 00:20:47,120
and apart from that the modest writing

00:20:44,880 --> 00:20:49,679
support now came into place

00:20:47,120 --> 00:20:50,799
because the the spark they have been

00:20:49,679 --> 00:20:53,360
designed every part

00:20:50,799 --> 00:20:55,120
inside of the mod is ready so you can

00:20:53,360 --> 00:20:57,360
leverage that benefit with deepdraw

00:20:55,120 --> 00:20:59,280
library since we already support that

00:20:57,360 --> 00:21:02,640
you can use modis threading with jvm

00:20:59,280 --> 00:21:04,000
inside of spark and and and

00:21:02,640 --> 00:21:06,000
if you're trying to compare the deep

00:21:04,000 --> 00:21:07,760
draw library solution it's actually

00:21:06,000 --> 00:21:09,600
much faster than the multi-processing

00:21:07,760 --> 00:21:12,480
solution by leveraging spot

00:21:09,600 --> 00:21:13,200
by leveraging uh pi spark on small

00:21:12,480 --> 00:21:15,039
models

00:21:13,200 --> 00:21:17,039
and here we mentioned about small models

00:21:15,039 --> 00:21:17,520
again so if you're talking about some

00:21:17,039 --> 00:21:20,720
large

00:21:17,520 --> 00:21:21,919
models such as the the birth model it's

00:21:20,720 --> 00:21:24,480
usually like

00:21:21,919 --> 00:21:25,120
800 max to one to one gigs of memory and

00:21:24,480 --> 00:21:27,280
total

00:21:25,120 --> 00:21:28,400
influence cost will last around like 200

00:21:27,280 --> 00:21:31,679
milliseconds

00:21:28,400 --> 00:21:33,200
so we we're kind of having like 10 to 20

00:21:31,679 --> 00:21:34,559
milliseconds performance boost if you're

00:21:33,200 --> 00:21:36,799
trying to compare that

00:21:34,559 --> 00:21:37,919
so it's the benefits is crucial inside

00:21:36,799 --> 00:21:41,039
of the small

00:21:37,919 --> 00:21:42,240
model side and the different library is

00:21:41,039 --> 00:21:44,720
also designed to be very

00:21:42,240 --> 00:21:45,520
easy to use you can elaborate the our

00:21:44,720 --> 00:21:48,559
plugin

00:21:45,520 --> 00:21:50,400
with hdfs and s3 you can load the model

00:21:48,559 --> 00:21:51,760
directly from this domain so you don't

00:21:50,400 --> 00:21:52,400
worry about where you want to store the

00:21:51,760 --> 00:21:54,559
model

00:21:52,400 --> 00:21:55,919
and how you want to clean on the models

00:21:54,559 --> 00:21:58,320
and this kind of

00:21:55,919 --> 00:22:00,240
stuff is being used by the amazon retail

00:21:58,320 --> 00:22:02,640
system and also the chinese firm called

00:22:00,240 --> 00:22:05,600
talking data for big data processing

00:22:02,640 --> 00:22:06,880
so the story from the and the retail

00:22:05,600 --> 00:22:08,720
system is before

00:22:06,880 --> 00:22:10,880
using different library they spend like

00:22:08,720 --> 00:22:13,360
several weeks trying to fine tuning

00:22:10,880 --> 00:22:14,960
their their models and and deploy into

00:22:13,360 --> 00:22:17,200
their infant service

00:22:14,960 --> 00:22:18,880
and the total inference time is around

00:22:17,200 --> 00:22:20,720
like 24 hours

00:22:18,880 --> 00:22:22,240
by having deep drive library building

00:22:20,720 --> 00:22:23,919
there they can live through a different

00:22:22,240 --> 00:22:26,240
deep learning model easily

00:22:23,919 --> 00:22:28,240
without any i mean spending much of time

00:22:26,240 --> 00:22:29,919
in the into the tuning of the memory

00:22:28,240 --> 00:22:32,080
and the total inference time has been

00:22:29,919 --> 00:22:35,200
lasted just inside of

00:22:32,080 --> 00:22:38,240
um eight hours on 800

00:22:35,200 --> 00:22:39,679
million of the the influence workload to

00:22:38,240 --> 00:22:42,240
provide the best

00:22:39,679 --> 00:22:45,120
to provide better experiences once the

00:22:42,240 --> 00:22:46,799
user trying to search on something

00:22:45,120 --> 00:22:48,159
and for serving solutions deep down

00:22:46,799 --> 00:22:50,320
library um

00:22:48,159 --> 00:22:51,200
is a java based solution so you can

00:22:50,320 --> 00:22:54,480
integrate that

00:22:51,200 --> 00:22:56,799
with your exiting services by simply

00:22:54,480 --> 00:22:58,240
import the package and and use that

00:22:56,799 --> 00:23:01,280
directly

00:22:58,240 --> 00:23:03,520
and and

00:23:01,280 --> 00:23:06,799
and this technology has been used by the

00:23:03,520 --> 00:23:10,159
amazon s team and and also netflix

00:23:06,799 --> 00:23:12,720
so talking about amazon as use cases

00:23:10,159 --> 00:23:14,159
uh before that there's actually a team

00:23:12,720 --> 00:23:17,360
came into us like saying

00:23:14,159 --> 00:23:20,080
um can you help us uh because we really

00:23:17,360 --> 00:23:23,360
have a very critical task want to solve

00:23:20,080 --> 00:23:24,880
and uh and they want us to bring us

00:23:23,360 --> 00:23:26,960
solutions that can

00:23:24,880 --> 00:23:28,320
uh run the inference inside of one

00:23:26,960 --> 00:23:30,320
millisecond

00:23:28,320 --> 00:23:32,960
because they're they're they're as query

00:23:30,320 --> 00:23:36,400
they they require very low

00:23:32,960 --> 00:23:38,080
latency cost and all the existing model

00:23:36,400 --> 00:23:39,520
service solutions cannot solve their

00:23:38,080 --> 00:23:41,440
problem and they came to us

00:23:39,520 --> 00:23:42,720
and say if there's a if there are the

00:23:41,440 --> 00:23:44,400
solutions

00:23:42,720 --> 00:23:46,159
and we have been trying on that

00:23:44,400 --> 00:23:47,279
beforehand and we didn't know there are

00:23:46,159 --> 00:23:49,279
even customers

00:23:47,279 --> 00:23:50,559
like like like that so we do the

00:23:49,279 --> 00:23:52,480
benchmark

00:23:50,559 --> 00:23:53,919
and the phone lines the outcome is

00:23:52,480 --> 00:23:56,159
always surprised us as well

00:23:53,919 --> 00:23:58,320
we to finish all the tasks for their

00:23:56,159 --> 00:23:59,600
their their model instead of 400

00:23:58,320 --> 00:24:02,400
nanoseconds

00:23:59,600 --> 00:24:04,080
it's like half of the millisecond and

00:24:02,400 --> 00:24:07,679
they're also like very

00:24:04,080 --> 00:24:10,799
exciting about this result and now

00:24:07,679 --> 00:24:11,200
moving to prod so you you can't expect

00:24:10,799 --> 00:24:13,039
like

00:24:11,200 --> 00:24:14,720
uh all the query when you're trying to

00:24:13,039 --> 00:24:17,120
search on amazon ask

00:24:14,720 --> 00:24:18,720
we we run this crew we run this crew for

00:24:17,120 --> 00:24:22,240
services and bring you the

00:24:18,720 --> 00:24:23,279
the inference result by bringing you the

00:24:22,240 --> 00:24:25,600
sponsor prop

00:24:23,279 --> 00:24:28,159
by bringing the sponsor product it all

00:24:25,600 --> 00:24:31,120
came from from this kind of services

00:24:28,159 --> 00:24:33,279
so um so i would say that deep dial

00:24:31,120 --> 00:24:35,840
library is also very tough into the use

00:24:33,279 --> 00:24:38,559
cases if you have a really tight budget

00:24:35,840 --> 00:24:39,279
in your infant use cases so that's

00:24:38,559 --> 00:24:40,799
pretty much

00:24:39,279 --> 00:24:42,799
about the deep draw libraries

00:24:40,799 --> 00:24:45,200
introduction since time is tight i won't

00:24:42,799 --> 00:24:46,799
have so much time to reduce our stuff

00:24:45,200 --> 00:24:48,240
so if you want to check it out please go

00:24:46,799 --> 00:24:51,520
to our demo page

00:24:48,240 --> 00:24:52,720
uh also our our website and the doodle

00:24:51,520 --> 00:24:55,120
draw applications

00:24:52,720 --> 00:24:57,120
is also available in the google play

00:24:55,120 --> 00:24:58,400
store where we can try to download that

00:24:57,120 --> 00:25:00,799
and test it

00:24:58,400 --> 00:25:02,320
locally and please feel free to tell us

00:25:00,799 --> 00:25:04,320
any problem you're having when you're

00:25:02,320 --> 00:25:07,840
trying to play with that application

00:25:04,320 --> 00:25:07,840
so thanks for listening

00:25:08,640 --> 00:25:13,520
okay all right so

00:25:13,679 --> 00:25:19,440
that's actually okay so i hope my flow

00:25:17,600 --> 00:25:23,440
isn't going very quick in here

00:25:19,440 --> 00:25:29,039
um so is there any questions

00:25:23,440 --> 00:25:31,760
or anything

00:25:29,039 --> 00:25:34,000
so let me try to bring up the questions

00:25:31,760 --> 00:25:34,000
so

00:25:34,720 --> 00:25:37,360
um

00:25:40,840 --> 00:25:46,400
basically

00:25:43,760 --> 00:25:46,400
thanks guys

00:25:48,080 --> 00:25:51,440
oh hi can i start a notebook deep

00:25:49,919 --> 00:25:53,440
learning good question so

00:25:51,440 --> 00:25:55,679
uh basically there is a solution that

00:25:53,440 --> 00:25:58,000
frank actually posted below

00:25:55,679 --> 00:26:00,080
above in the in the chat room i will

00:25:58,000 --> 00:26:02,480
just paste the link again

00:26:00,080 --> 00:26:03,679
you can you actually can make a start

00:26:02,480 --> 00:26:07,360
with the

00:26:03,679 --> 00:26:09,360
with our d2l d2l book which these books

00:26:07,360 --> 00:26:11,279
bring you the basic terminology where

00:26:09,360 --> 00:26:14,159
you can learn from scratch

00:26:11,279 --> 00:26:15,279
for the basic deep learning concept as

00:26:14,159 --> 00:26:16,720
well as the

00:26:15,279 --> 00:26:18,480
like how you can leverage deep learning

00:26:16,720 --> 00:26:21,200
instead of java

00:26:18,480 --> 00:26:21,760
so this book is still under development

00:26:21,200 --> 00:26:23,760
and we're

00:26:21,760 --> 00:26:25,840
hoping if there are more contributions

00:26:23,760 --> 00:26:31,039
there is actually welcome

00:26:25,840 --> 00:26:31,039
because we are open source and uh yeah

00:26:31,520 --> 00:26:35,440
and we only finished currently like

00:26:33,440 --> 00:26:36,080
seven chapters and we're hoping we can

00:26:35,440 --> 00:26:39,279
finish

00:26:36,080 --> 00:26:42,640
all the book uh this

00:26:39,279 --> 00:26:43,919
uh in in this winter so you will have a

00:26:42,640 --> 00:26:46,240
comprehensive

00:26:43,919 --> 00:26:47,520
like the first book that been decided uh

00:26:46,240 --> 00:26:51,360
that's been defined in the

00:26:47,520 --> 00:26:51,360
in the deep learning world with java

00:26:51,440 --> 00:26:53,840
um

00:27:14,400 --> 00:27:17,760
how to use deep jaw locker with mx tech

00:27:16,400 --> 00:27:21,039
comparing to use advanced

00:27:17,760 --> 00:27:23,039
java api directly okay

00:27:21,039 --> 00:27:24,640
so i think i'm i'm the person that

00:27:23,039 --> 00:27:28,720
answered that question so

00:27:24,640 --> 00:27:33,039
i'm also i'm excited java authors so

00:27:28,720 --> 00:27:34,720
basically um comparing that is

00:27:33,039 --> 00:27:36,880
deep down library is the second

00:27:34,720 --> 00:27:38,159
generation for the for the deep deep

00:27:36,880 --> 00:27:41,679
learning stuff for the mx

00:27:38,159 --> 00:27:44,320
java and for msn java apis we

00:27:41,679 --> 00:27:46,480
actually brings too much of the old

00:27:44,320 --> 00:27:49,919
schemes like you you actually you

00:27:46,480 --> 00:27:53,120
using a lot of deprecated feature inside

00:27:49,919 --> 00:27:55,440
and also because uh it's actually like a

00:27:53,120 --> 00:27:56,559
problem because we firstly defined the

00:27:55,440 --> 00:27:58,960
mx scala

00:27:56,559 --> 00:28:00,720
api so we're actually building up the

00:27:58,960 --> 00:28:03,919
java layer on top of that

00:28:00,720 --> 00:28:06,159
and we we we began to feel like the

00:28:03,919 --> 00:28:08,320
the signate nature for that is not very

00:28:06,159 --> 00:28:11,760
friendly for people to use

00:28:08,320 --> 00:28:15,120
uh and also people find it hard to

00:28:11,760 --> 00:28:18,480
to do their memory management with the

00:28:15,120 --> 00:28:20,559
mxl java apis

00:28:18,480 --> 00:28:22,159
so beside of that so that's why we're

00:28:20,559 --> 00:28:24,480
trying to leverage with the second

00:28:22,159 --> 00:28:27,120
generation java parting here

00:28:24,480 --> 00:28:28,720
to bring up a better java experiences uh

00:28:27,120 --> 00:28:30,320
by having the deepdraw library

00:28:28,720 --> 00:28:33,440
implemented newer java

00:28:30,320 --> 00:28:35,919
a api here so

00:28:33,440 --> 00:28:37,840
you you can think about that so the

00:28:35,919 --> 00:28:39,120
default library with mxn is actually

00:28:37,840 --> 00:28:41,520
bringing much of the

00:28:39,120 --> 00:28:43,520
the features we're using the same

00:28:41,520 --> 00:28:46,720
operators as python use

00:28:43,520 --> 00:28:48,799
we provide the similar numpy experiences

00:28:46,720 --> 00:28:51,760
and also it is support for both

00:28:48,799 --> 00:28:55,200
imperative and symbolic inferences

00:28:51,760 --> 00:28:57,600
which mxn java cannot bring and the most

00:28:55,200 --> 00:29:00,880
important question is very designed

00:28:57,600 --> 00:29:01,679
to be very simple to use especially on

00:29:00,880 --> 00:29:04,799
the on the map

00:29:01,679 --> 00:29:06,320
on the on the package management

00:29:04,799 --> 00:29:07,919
uh before that if you're trying to

00:29:06,320 --> 00:29:11,039
leverage with mxs

00:29:07,919 --> 00:29:12,159
mx java a api is kind of painful i would

00:29:11,039 --> 00:29:14,880
say

00:29:12,159 --> 00:29:15,440
even their team saying is very hard to

00:29:14,880 --> 00:29:17,520
use so

00:29:15,440 --> 00:29:19,120
we always spend too much too long time

00:29:17,520 --> 00:29:22,320
to help people

00:29:19,120 --> 00:29:26,159
to onboard without packages um

00:29:22,320 --> 00:29:27,279
so yeah i hope that answer questions and

00:29:26,159 --> 00:29:29,840
the trade-off is

00:29:27,279 --> 00:29:30,559
um i always think there will be

00:29:29,840 --> 00:29:32,080
basically

00:29:30,559 --> 00:29:33,760
no trade-off you're trying to use

00:29:32,080 --> 00:29:37,120
deepdraw library

00:29:33,760 --> 00:29:39,600
um it basically what's

00:29:37,120 --> 00:29:40,720
what's uh i'm excited api can bring deep

00:29:39,600 --> 00:29:44,799
draw library with

00:29:40,720 --> 00:29:49,840
mxn can also bring that so yeah

00:29:44,799 --> 00:29:53,120
and also it's in sync with mx mxn 2.0

00:29:49,840 --> 00:29:56,159
with upcoming features that happened

00:29:53,120 --> 00:29:56,159
by the end of this year

00:29:58,559 --> 00:30:01,760
right uh

00:30:03,679 --> 00:30:09,840
any more questions from people

00:30:20,840 --> 00:30:23,840
hmm

00:30:28,240 --> 00:30:31,919
okay i will wait for another five

00:30:29,600 --> 00:30:36,480
minutes so this will be

00:30:31,919 --> 00:30:40,080
the end of my session of that so

00:30:36,480 --> 00:30:41,360
have you worked with pipelines of

00:30:40,080 --> 00:30:44,880
which pipelines are actually mentioned

00:30:41,360 --> 00:30:47,600
here so uh i'm not quite understand

00:30:44,880 --> 00:30:48,960
uh can you explain more about like the

00:30:47,600 --> 00:30:52,240
meaning of pipelines

00:30:48,960 --> 00:30:53,360
you mean the amazon web service pipeline

00:30:52,240 --> 00:30:56,880
or

00:30:53,360 --> 00:30:56,880
or some other pipelines

00:31:01,279 --> 00:31:07,919
oh sk learn pipelines uh

00:31:05,360 --> 00:31:10,240
nope uh the actual answer is no but we

00:31:07,919 --> 00:31:12,480
haven't worked so much on that

00:31:10,240 --> 00:31:12,480
uh

00:31:13,600 --> 00:31:17,679
yeah i'll i'll definitely take a look on

00:31:15,679 --> 00:31:19,760
what is escalant

00:31:17,679 --> 00:31:22,960
do you know what are the key benefits by

00:31:19,760 --> 00:31:22,960
having that inside java

00:31:28,480 --> 00:31:33,039
so so talking about that we're also

00:31:30,559 --> 00:31:36,000
supporting for the uh

00:31:33,039 --> 00:31:37,600
uh for the on its runtime which actually

00:31:36,000 --> 00:31:40,000
uh bring us to import

00:31:37,600 --> 00:31:41,039
is killer model directly instead of java

00:31:40,000 --> 00:31:43,600
domain

00:31:41,039 --> 00:31:45,519
uh you can basically convert your

00:31:43,600 --> 00:31:48,799
executor model into

00:31:45,519 --> 00:31:49,360
into the java and then and then run the

00:31:48,799 --> 00:31:52,240
inference

00:31:49,360 --> 00:31:52,240
cases in there

00:31:52,480 --> 00:31:58,480
so i will definitely take a look on the

00:31:55,120 --> 00:31:58,480
on sk learn pipelines

00:32:16,799 --> 00:32:19,120
okay

00:32:23,519 --> 00:32:28,240
so thanks again for everyone um thanks

00:32:26,320 --> 00:32:30,399
for your time especially i would say

00:32:28,240 --> 00:32:33,519
that's really hard for the people who is

00:32:30,399 --> 00:32:35,760
in the china region and also

00:32:33,519 --> 00:32:36,559
uh inside of the west coast because it's

00:32:35,760 --> 00:32:38,320
very

00:32:36,559 --> 00:32:39,760
early in the morning and a very deep in

00:32:38,320 --> 00:32:43,679
the evening there

00:32:39,760 --> 00:32:45,600
so if you got a chance to learn more so

00:32:43,679 --> 00:32:48,000
and thank you everyone to join this

00:32:45,600 --> 00:32:48,000
session

00:32:48,080 --> 00:32:52,799
right thank you

00:32:53,120 --> 00:32:56,399
yeah and and if you have more of the

00:32:54,799 --> 00:32:57,120
questions please feel free to go to our

00:32:56,399 --> 00:32:58,960
slack room

00:32:57,120 --> 00:33:01,200
and i'm going to answer the question

00:32:58,960 --> 00:33:04,480
there um

00:33:01,200 --> 00:33:06,240
so i mean to keep us connected because

00:33:04,480 --> 00:33:06,720
once i guess the session ends there

00:33:06,240 --> 00:33:09,120
won't be

00:33:06,720 --> 00:33:10,080
any chances for us to actually see the

00:33:09,120 --> 00:33:25,840
questions you are

00:33:10,080 --> 00:33:25,840
you actually ask all right

00:33:54,399 --> 00:34:00,320
okay any more of questions so

00:33:58,399 --> 00:34:01,760
yeah so so pretty much this is the end

00:34:00,320 --> 00:34:05,679
of session so if you

00:34:01,760 --> 00:34:06,000
have uh if if you just lose your chance

00:34:05,679 --> 00:34:08,159
to get

00:34:06,000 --> 00:34:10,320
it through it please go to please join

00:34:08,159 --> 00:34:11,359
our slack channel where we can answer

00:34:10,320 --> 00:34:13,679
the question there

00:34:11,359 --> 00:34:15,760
and probably since this uh session is

00:34:13,679 --> 00:34:18,879
also recorded i guess

00:34:15,760 --> 00:34:20,159
it will also be shared once the session

00:34:18,879 --> 00:34:22,960
is in please

00:34:20,159 --> 00:34:25,040
do please do doing that catch up and

00:34:22,960 --> 00:34:26,560
also go to our deepdraw library website

00:34:25,040 --> 00:34:28,560
we're also providing you like a 10

00:34:26,560 --> 00:34:31,119
minutes uh basic to

00:34:28,560 --> 00:34:32,560
tutorial about what deep learning is

00:34:31,119 --> 00:34:35,919
they have the similar

00:34:32,560 --> 00:34:38,480
content in there to understand uh what

00:34:35,919 --> 00:34:39,839
what we actually covered in some of

00:34:38,480 --> 00:34:43,919
these cases

00:34:39,839 --> 00:34:47,200
yeah okay

00:34:43,919 --> 00:34:49,040
so um thanks everyone i'm

00:34:47,200 --> 00:34:50,560
i'm going to close the session and if

00:34:49,040 --> 00:34:55,200
you have more questions please

00:34:50,560 --> 00:34:55,200
go to our slack run and um

00:34:55,599 --> 00:35:03,760
um slacker

00:34:59,760 --> 00:35:07,359
if you have more questions

00:35:03,760 --> 00:35:07,359
or go to

00:35:07,480 --> 00:35:17,839
djl.ai to

00:35:10,800 --> 00:35:17,839
understand more

00:35:36,480 --> 00:35:45,839
all right so see you guys bye bye

00:35:59,680 --> 00:36:01,760

YouTube URL: https://www.youtube.com/watch?v=K1n_96l4Rlw


