Title: Streaming Machine Learning with Apache Kafka and TensorFlow (without a Data Lake)
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	Streaming Machine Learning with Apache Kafka and TensorFlow (without a Data Lake)
Kai Waehner

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Machine Learning (ML) is separated into model training and model inference. ML frameworks typically load historical data from a data store like HDFS or S3 to train models. This talk shows how you can avoid such a data store by ingesting streaming data directly via Apache Kafka from any source system into TensorFlow for model training and model inference using the capabilities of “TensorFlow I/O”. The talk compares this modern streaming architecture to traditional batch and big data alternatives and explains benefits like the simplified architecture, the ability of reprocessing events for training different models, and the possibility to build a scalable, mission-critical, real time ML architecture with muss less headaches and problems

Kai Waehner is a Technology Evangelist at Confluent. He works with customers across Europe, US, Middle East and Asia and internal teams like engineering and marketing. Kai’s main area of expertise lies within the fields of Big Data Analytics, Machine Learning, Hybrid Cloud Architectures, Event Stream Processing and Internet of Things. He is regular speaker at international conferences such as ApacheCon and Kafka Summit, writes articles for professional journals, and shares his experiences with new technologies on his blog: www.kai-waehner.de.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,320 --> 00:00:28,960
great to be at apache con today so

00:00:26,400 --> 00:00:30,080
let's get started with this topic i will

00:00:28,960 --> 00:00:33,440
today talk about

00:00:30,080 --> 00:00:34,480
apache kafka and t storage to use

00:00:33,440 --> 00:00:36,160
together with machine learning

00:00:34,480 --> 00:00:38,399
frameworks like tensorflow

00:00:36,160 --> 00:00:40,239
for streaming machine learning so this

00:00:38,399 --> 00:00:42,000
is really some kind of new architecture

00:00:40,239 --> 00:00:43,200
which many people have not seen yet or

00:00:42,000 --> 00:00:44,960
used yet um

00:00:43,200 --> 00:00:46,480
but this is really an idea to show you

00:00:44,960 --> 00:00:48,160
how you can leverage machine learning

00:00:46,480 --> 00:00:50,719
with kafka without using

00:00:48,160 --> 00:00:51,680
a data lake like hdfs or something like

00:00:50,719 --> 00:00:53,600
that

00:00:51,680 --> 00:00:54,800
my name is kai i'm working for

00:00:53,600 --> 00:00:57,120
confluence i'm

00:00:54,800 --> 00:00:58,320
doing use cases around kafka for four

00:00:57,120 --> 00:00:59,840
years so i've seen

00:00:58,320 --> 00:01:01,760
plenty of different projects and

00:00:59,840 --> 00:01:02,879
architectures and

00:01:01,760 --> 00:01:04,479
machine learning is one of the hot

00:01:02,879 --> 00:01:07,200
topics i think and therefore i think

00:01:04,479 --> 00:01:09,760
this is a great discussion today

00:01:07,200 --> 00:01:11,280
first of all really the primer um what's

00:01:09,760 --> 00:01:12,000
very important because i talk about tf

00:01:11,280 --> 00:01:13,600
storage today

00:01:12,000 --> 00:01:15,600
so first of all what i will show you

00:01:13,600 --> 00:01:17,680
today um is a team storage

00:01:15,600 --> 00:01:19,520
implementation from confluence which is

00:01:17,680 --> 00:01:20,640
already ga and you can use that in

00:01:19,520 --> 00:01:22,720
production

00:01:20,640 --> 00:01:25,119
but this is a commercial tool and having

00:01:22,720 --> 00:01:26,159
said that um confluence is also working

00:01:25,119 --> 00:01:28,080
together

00:01:26,159 --> 00:01:29,280
with the open source community around

00:01:28,080 --> 00:01:31,600
kafka to

00:01:29,280 --> 00:01:33,360
provide tf storage as part of the apache

00:01:31,600 --> 00:01:36,159
kafka open source project

00:01:33,360 --> 00:01:37,040
so the current status in september 2020

00:01:36,159 --> 00:01:39,200
is that

00:01:37,040 --> 00:01:41,040
um uber has the lead on the open source

00:01:39,200 --> 00:01:42,560
tier storage implementation

00:01:41,040 --> 00:01:45,520
and is working together with confluent

00:01:42,560 --> 00:01:48,320
and others and it's expected that um

00:01:45,520 --> 00:01:51,360
tf storage will come into kafka 3.0 um

00:01:48,320 --> 00:01:53,600
which will release early 2021

00:01:51,360 --> 00:01:55,119
so then there will be different storage

00:01:53,600 --> 00:01:56,479
options under the hood like

00:01:55,119 --> 00:01:58,640
what i will show you today is

00:01:56,479 --> 00:02:00,880
implemented for example for s3 and

00:01:58,640 --> 00:02:02,880
google cloud storage and we also

00:02:00,880 --> 00:02:03,920
certified for example pure storage for

00:02:02,880 --> 00:02:06,240
on-premise already

00:02:03,920 --> 00:02:07,920
and others coming and the open source

00:02:06,240 --> 00:02:10,160
will be more like an interface so that

00:02:07,920 --> 00:02:12,080
different storages can also be adopted

00:02:10,160 --> 00:02:13,599
and uber as far as i know is working for

00:02:12,080 --> 00:02:16,000
example on an hdfs

00:02:13,599 --> 00:02:17,360
option for the storage so this is really

00:02:16,000 --> 00:02:19,200
the background um for you guys to

00:02:17,360 --> 00:02:21,040
understand this about tip storage what's

00:02:19,200 --> 00:02:22,879
available and what's coming

00:02:21,040 --> 00:02:25,440
but this is definitely a real game

00:02:22,879 --> 00:02:27,599
changer for how you can use kafka

00:02:25,440 --> 00:02:29,360
and so um just as an introduction i will

00:02:27,599 --> 00:02:30,800
not do a deep dive into kafka i hope you

00:02:29,360 --> 00:02:32,640
know that already

00:02:30,800 --> 00:02:34,319
my main point here is really that you

00:02:32,640 --> 00:02:35,760
understand that when i mean

00:02:34,319 --> 00:02:38,000
event streaming i really talk about

00:02:35,760 --> 00:02:39,440
continuously processing data

00:02:38,000 --> 00:02:41,440
that's where we see most of the use

00:02:39,440 --> 00:02:43,599
cases where the real added value comes

00:02:41,440 --> 00:02:45,200
from so of course you can use kafka just

00:02:43,599 --> 00:02:47,360
as ingestion layer into

00:02:45,200 --> 00:02:49,519
a data lake like hadoop and do a lot of

00:02:47,360 --> 00:02:51,519
mapreduce and spark there

00:02:49,519 --> 00:02:53,680
but the really added value for

00:02:51,519 --> 00:02:56,000
innovative use cases is to correlate the

00:02:53,680 --> 00:02:57,920
data in real time with stream processing

00:02:56,000 --> 00:02:59,840
streaming analytics this can be

00:02:57,920 --> 00:03:01,200
something like kafka streams okay sql in

00:02:59,840 --> 00:03:02,879
a kafka native way

00:03:01,200 --> 00:03:05,760
or of course also something like flink

00:03:02,879 --> 00:03:07,680
for example

00:03:05,760 --> 00:03:08,959
and today i don't want to talk just

00:03:07,680 --> 00:03:11,280
about

00:03:08,959 --> 00:03:13,599
kafka for event streaming but really how

00:03:11,280 --> 00:03:15,519
to use it together with machine learning

00:03:13,599 --> 00:03:17,040
and in the real world we see a lot of

00:03:15,519 --> 00:03:18,400
different use cases here where this

00:03:17,040 --> 00:03:20,400
makes sense together

00:03:18,400 --> 00:03:22,319
and this is of course some really

00:03:20,400 --> 00:03:24,080
cutting-edge use cases like if you do

00:03:22,319 --> 00:03:26,799
something like speech translation for

00:03:24,080 --> 00:03:29,040
building chatbots or a customer service

00:03:26,799 --> 00:03:30,959
this is where neural networks are used

00:03:29,040 --> 00:03:33,120
for deep learning and so on this is of

00:03:30,959 --> 00:03:35,280
course what you can do and it works well

00:03:33,120 --> 00:03:37,120
but on the other side in in the real

00:03:35,280 --> 00:03:38,560
world today i would say that still 80

00:03:37,120 --> 00:03:39,120
percent of the machine learning use

00:03:38,560 --> 00:03:41,760
cases

00:03:39,120 --> 00:03:43,200
are more about um solving existing

00:03:41,760 --> 00:03:45,519
problems or improving them

00:03:43,200 --> 00:03:46,799
like cross selling or customer churn or

00:03:45,519 --> 00:03:48,319
fraud detection

00:03:46,799 --> 00:03:50,000
that's use cases which exists for many

00:03:48,319 --> 00:03:51,680
years already and there are already

00:03:50,000 --> 00:03:54,080
business rules implemented

00:03:51,680 --> 00:03:56,080
but uh but adding machine learning here

00:03:54,080 --> 00:03:59,200
can simply make the

00:03:56,080 --> 00:04:01,040
behavior better like selling more or

00:03:59,200 --> 00:04:03,120
reducing the risk and these kind of

00:04:01,040 --> 00:04:04,640
things so whatever your use case is

00:04:03,120 --> 00:04:06,480
kafka and machine learning are really

00:04:04,640 --> 00:04:09,519
very complementary to each other for

00:04:06,480 --> 00:04:12,000
many different use cases

00:04:09,519 --> 00:04:13,360
in this talk today um i will use the

00:04:12,000 --> 00:04:15,840
example of a

00:04:13,360 --> 00:04:16,799
connected car infrastructure the reason

00:04:15,840 --> 00:04:18,560
is that

00:04:16,799 --> 00:04:20,880
this is really a use case which is

00:04:18,560 --> 00:04:22,720
interesting because this is

00:04:20,880 --> 00:04:24,960
needs to be highly available highly

00:04:22,720 --> 00:04:27,040
scalable and real time for high volume

00:04:24,960 --> 00:04:29,199
of throughput so we have customers

00:04:27,040 --> 00:04:30,880
in the automotive industry which really

00:04:29,199 --> 00:04:32,160
build connected car infrastructures

00:04:30,880 --> 00:04:34,080
across the globe

00:04:32,160 --> 00:04:36,000
and this really means millions of cars

00:04:34,080 --> 00:04:37,199
which stream data in real time and need

00:04:36,000 --> 00:04:39,919
to be connected

00:04:37,199 --> 00:04:41,120
so that you can correlate the data and

00:04:39,919 --> 00:04:42,720
therefore um

00:04:41,120 --> 00:04:44,479
at many companies this is already the

00:04:42,720 --> 00:04:46,240
reality like um one of our public

00:04:44,479 --> 00:04:46,800
references references for this is audi

00:04:46,240 --> 00:04:48,400
which

00:04:46,800 --> 00:04:50,400
has started building this together with

00:04:48,400 --> 00:04:52,560
us um five years ago which is

00:04:50,400 --> 00:04:54,479
already in production for some years now

00:04:52,560 --> 00:04:55,360
and the status quo for this talk here is

00:04:54,479 --> 00:04:57,759
that we expect

00:04:55,360 --> 00:04:59,360
we as a automotive company already have

00:04:57,759 --> 00:05:00,240
the connected car infrastructure in

00:04:59,360 --> 00:05:02,000
place

00:05:00,240 --> 00:05:04,160
um this is important because we now want

00:05:02,000 --> 00:05:06,560
to improve this with machine learning

00:05:04,160 --> 00:05:08,000
to make our use cases even better so

00:05:06,560 --> 00:05:09,759
that's the main idea

00:05:08,000 --> 00:05:11,919
and so what i will show you today is if

00:05:09,759 --> 00:05:14,240
you have a kafka streaming platform

00:05:11,919 --> 00:05:14,960
how can you use machine learning on top

00:05:14,240 --> 00:05:17,039
of that

00:05:14,960 --> 00:05:18,000
in our case we use um it for detecting

00:05:17,039 --> 00:05:20,240
animals for

00:05:18,000 --> 00:05:21,759
predictive maintenance but this really

00:05:20,240 --> 00:05:22,880
doesn't matter much right for any kind

00:05:21,759 --> 00:05:26,560
of industry um

00:05:22,880 --> 00:05:26,560
the architecture looks very similar

00:05:26,800 --> 00:05:30,400
and therefore um this is as i said um

00:05:28,960 --> 00:05:31,840
the foundation so let's assume we

00:05:30,400 --> 00:05:33,120
already have a streaming platform in

00:05:31,840 --> 00:05:35,440
place with kafka

00:05:33,120 --> 00:05:37,039
we integrate to some edge devices like

00:05:35,440 --> 00:05:39,199
in this case it's cars or in

00:05:37,039 --> 00:05:41,199
other cases it might be mobile apps or

00:05:39,199 --> 00:05:42,880
machines or plc's

00:05:41,199 --> 00:05:44,880
um and then on the other side we also

00:05:42,880 --> 00:05:47,120
integrate with real-time systems i mean

00:05:44,880 --> 00:05:49,199
that's what many people use kafka for um

00:05:47,120 --> 00:05:50,880
like a real-time monitoring system

00:05:49,199 --> 00:05:53,199
that consumes the data with very low

00:05:50,880 --> 00:05:55,199
latency but in parallel to that and

00:05:53,199 --> 00:05:57,120
that's one of the key strengths of kafka

00:05:55,199 --> 00:05:59,360
and you can also consume it for example

00:05:57,120 --> 00:06:01,039
from a batch platform in parallel

00:05:59,360 --> 00:06:02,639
the great thing about kafka is because

00:06:01,039 --> 00:06:03,759
it's not just a messaging system like

00:06:02,639 --> 00:06:05,919
rabbitmq

00:06:03,759 --> 00:06:07,600
but also a storage system so we really

00:06:05,919 --> 00:06:09,199
decouple the different producers and

00:06:07,600 --> 00:06:10,639
consumers from each other

00:06:09,199 --> 00:06:12,800
and that's important because the car

00:06:10,639 --> 00:06:15,120
sensors they produce data continuously

00:06:12,800 --> 00:06:16,639
and they don't care if all consumers can

00:06:15,120 --> 00:06:18,800
consume the data in real time

00:06:16,639 --> 00:06:21,039
maybe one other system is done or maybe

00:06:18,800 --> 00:06:21,520
a batch system only processes overnight

00:06:21,039 --> 00:06:23,039
so

00:06:21,520 --> 00:06:25,440
this is where kafka is really great in

00:06:23,039 --> 00:06:25,440
the middle

00:06:25,600 --> 00:06:28,720
and now we want to add machine learning

00:06:27,120 --> 00:06:29,840
to that so my definition of machine

00:06:28,720 --> 00:06:32,160
learning is that

00:06:29,840 --> 00:06:33,680
um it's algorithms which allow computers

00:06:32,160 --> 00:06:35,600
to find hidden insights

00:06:33,680 --> 00:06:36,720
without being explicitly programmed

00:06:35,600 --> 00:06:38,639
where to look

00:06:36,720 --> 00:06:40,639
and machine learning is also nothing new

00:06:38,639 --> 00:06:42,400
so many of these algorithms exist for

00:06:40,639 --> 00:06:44,160
for tens of years right

00:06:42,400 --> 00:06:46,479
and of course now we also have these

00:06:44,160 --> 00:06:48,720
neural networks which are

00:06:46,479 --> 00:06:49,840
now we have to compute power for doing

00:06:48,720 --> 00:06:51,520
something like a

00:06:49,840 --> 00:06:54,319
convolutional neural network for image

00:06:51,520 --> 00:06:56,160
recognition or in our case um

00:06:54,319 --> 00:06:58,880
i will talk about an auto encoder to do

00:06:56,160 --> 00:07:01,280
animated anomaly detection this is an

00:06:58,880 --> 00:07:02,160
unsupervised deep learning model but it

00:07:01,280 --> 00:07:03,759
really doesn't matter

00:07:02,160 --> 00:07:05,759
depending on the use case choose the

00:07:03,759 --> 00:07:06,800
right algorithms and embed them into

00:07:05,759 --> 00:07:08,479
your infrastructure

00:07:06,800 --> 00:07:11,360
there is no single algorithm which

00:07:08,479 --> 00:07:11,360
solves every problem

00:07:11,759 --> 00:07:16,400
so back to our architecture we simply

00:07:14,720 --> 00:07:18,160
use the streaming architecture we

00:07:16,400 --> 00:07:19,599
already have in place often

00:07:18,160 --> 00:07:22,160
like in our case our connected kind

00:07:19,599 --> 00:07:24,639
infrastructure and we add additional

00:07:22,160 --> 00:07:25,280
infrastructure and use cases so in this

00:07:24,639 --> 00:07:27,599
case you see

00:07:25,280 --> 00:07:29,599
two green boxes this is the ml which we

00:07:27,599 --> 00:07:30,400
had in the middle we see the model

00:07:29,599 --> 00:07:33,199
training

00:07:30,400 --> 00:07:34,720
so this model training consumes data

00:07:33,199 --> 00:07:36,560
from the streaming platform and then

00:07:34,720 --> 00:07:38,240
trains a model

00:07:36,560 --> 00:07:40,319
most of the model training is typically

00:07:38,240 --> 00:07:40,880
done in batch there is a few online

00:07:40,319 --> 00:07:42,639
model

00:07:40,880 --> 00:07:44,879
models available like if you do online

00:07:42,639 --> 00:07:46,960
clustering but almost all of them

00:07:44,879 --> 00:07:49,360
in reality are batch training but still

00:07:46,960 --> 00:07:51,199
you consume the data from this pipeline

00:07:49,360 --> 00:07:52,960
and then you take maybe the last 10

00:07:51,199 --> 00:07:53,680
minutes maybe the last hour or the last

00:07:52,960 --> 00:07:55,440
day

00:07:53,680 --> 00:07:56,960
and then you train your model and when

00:07:55,440 --> 00:07:59,360
the model is trained after

00:07:56,960 --> 00:08:01,039
10 minutes or an hour or a day you have

00:07:59,360 --> 00:08:02,080
a binary which you can deploy somewhere

00:08:01,039 --> 00:08:03,919
if you use it

00:08:02,080 --> 00:08:06,400
and in this case on the right side it's

00:08:03,919 --> 00:08:08,639
another streaming real-time application

00:08:06,400 --> 00:08:10,879
where we embed our analytic model into

00:08:08,639 --> 00:08:12,240
that for example to do predictive

00:08:10,879 --> 00:08:13,680
maintenance

00:08:12,240 --> 00:08:15,039
and in addition to that we still have

00:08:13,680 --> 00:08:16,240
this other batch platform which does

00:08:15,039 --> 00:08:19,919
some other stuff like

00:08:16,240 --> 00:08:19,919
creating reports for a dashboard

00:08:20,960 --> 00:08:24,720
and really in reality this is much

00:08:23,199 --> 00:08:26,319
harder than many people think

00:08:24,720 --> 00:08:28,720
because there is a huge impedance

00:08:26,319 --> 00:08:30,639
mismatch between the data science team

00:08:28,720 --> 00:08:32,320
which typically uses python and things

00:08:30,639 --> 00:08:34,880
like jupiter notebooks to

00:08:32,320 --> 00:08:35,360
do rapid prototyping on historical data

00:08:34,880 --> 00:08:37,519
and then

00:08:35,360 --> 00:08:38,719
building a model and that's what data

00:08:37,519 --> 00:08:41,279
scientists are great in

00:08:38,719 --> 00:08:43,120
but i've seen so many customers across

00:08:41,279 --> 00:08:45,440
the globe where they had the problem

00:08:43,120 --> 00:08:47,680
that they were not able to deploy their

00:08:45,440 --> 00:08:50,240
great and accurate models to production

00:08:47,680 --> 00:08:51,120
so in production this typically means um

00:08:50,240 --> 00:08:54,880
in real time

00:08:51,120 --> 00:08:56,080
and at scale and most often with a 24 7

00:08:54,880 --> 00:08:58,000
uptime requirement

00:08:56,080 --> 00:08:59,680
and this is not what you do with python

00:08:58,000 --> 00:09:01,760
and jupiter typically and so there is

00:08:59,680 --> 00:09:04,560
this huge impedance mismatch between

00:09:01,760 --> 00:09:04,959
data science and production deployments

00:09:04,560 --> 00:09:07,600
and

00:09:04,959 --> 00:09:08,800
that's what many people now use kafka

00:09:07,600 --> 00:09:11,360
for to solve this

00:09:08,800 --> 00:09:11,360
mismatch

00:09:12,240 --> 00:09:15,600
and a key here really to understand is

00:09:14,000 --> 00:09:17,440
that there is this hidden technical

00:09:15,600 --> 00:09:19,839
depth in machine learning systems

00:09:17,440 --> 00:09:21,040
this is a great paper by google already

00:09:19,839 --> 00:09:23,760
a few years old

00:09:21,040 --> 00:09:25,120
where you see that the ml code the small

00:09:23,760 --> 00:09:26,560
box in the middle

00:09:25,120 --> 00:09:28,880
this is the thing where you write the

00:09:26,560 --> 00:09:30,959
python code um for training a model

00:09:28,880 --> 00:09:32,959
so this is great and then every tutorial

00:09:30,959 --> 00:09:34,880
you use today for tensorflow or others

00:09:32,959 --> 00:09:36,480
you write 20 lines of paper code and

00:09:34,880 --> 00:09:38,880
then you have a great model which can do

00:09:36,480 --> 00:09:42,080
amazing things like image recognition

00:09:38,880 --> 00:09:44,240
or translation or anything else but this

00:09:42,080 --> 00:09:45,920
is just a small part of the problem and

00:09:44,240 --> 00:09:48,080
even the data scientist typically spends

00:09:45,920 --> 00:09:49,760
most of the time on other tasks

00:09:48,080 --> 00:09:51,360
and as you can see here something like

00:09:49,760 --> 00:09:52,800
data collection is really much harder

00:09:51,360 --> 00:09:54,560
than you think um so

00:09:52,800 --> 00:09:55,920
um the data scientists of course you can

00:09:54,560 --> 00:09:57,360
take the historical data from the

00:09:55,920 --> 00:09:59,200
connected car infrastructure if it's

00:09:57,360 --> 00:10:00,959
already stored somewhere in a data lake

00:09:59,200 --> 00:10:03,040
but collecting the data from a million

00:10:00,959 --> 00:10:03,600
cars um that's a little bit harder thing

00:10:03,040 --> 00:10:05,200
to do

00:10:03,600 --> 00:10:06,880
and that's why this box is bigger than

00:10:05,200 --> 00:10:08,880
the ml code box

00:10:06,880 --> 00:10:11,040
and also for serving the models and for

00:10:08,880 --> 00:10:11,920
doing monitoring and finding a process

00:10:11,040 --> 00:10:13,440
around that

00:10:11,920 --> 00:10:16,000
that's so many things which you have to

00:10:13,440 --> 00:10:17,600
solve around just writing a model code

00:10:16,000 --> 00:10:19,839
that's what many people are not aware of

00:10:17,600 --> 00:10:19,839
in the beginning

00:10:20,720 --> 00:10:24,399
so let's take a few examples where we

00:10:23,279 --> 00:10:26,640
already have

00:10:24,399 --> 00:10:28,640
a scalable and technology agnostic

00:10:26,640 --> 00:10:30,399
machine learning infrastructure

00:10:28,640 --> 00:10:32,079
netflix for example has been a very

00:10:30,399 --> 00:10:34,880
powerful recommendation

00:10:32,079 --> 00:10:37,040
engine so that every user sees context

00:10:34,880 --> 00:10:39,440
specific recommendations depending on

00:10:37,040 --> 00:10:41,600
the device he uses the behavior and

00:10:39,440 --> 00:10:44,079
history of what he watches

00:10:41,600 --> 00:10:45,120
the ratings he does the time of the day

00:10:44,079 --> 00:10:46,959
the location

00:10:45,120 --> 00:10:50,160
and all these kind of things so that you

00:10:46,959 --> 00:10:52,160
get context-specific recommendations

00:10:50,160 --> 00:10:53,200
uber and also other ride-sharing

00:10:52,160 --> 00:10:55,200
applications like

00:10:53,200 --> 00:10:57,680
lyft and all the same in asia and in

00:10:55,200 --> 00:11:00,480
europe they all use machine learning

00:10:57,680 --> 00:11:02,240
a lot to do things like calculating the

00:11:00,480 --> 00:11:04,640
estimated time of arrival

00:11:02,240 --> 00:11:07,120
calculating the estimated cost

00:11:04,640 --> 00:11:08,640
calculating the right routing

00:11:07,120 --> 00:11:10,560
and all of this has to happen in real

00:11:08,640 --> 00:11:11,760
time because otherwise

00:11:10,560 --> 00:11:14,480
you will move from the uber app to the

00:11:11,760 --> 00:11:16,240
lift app for example and this has to run

00:11:14,480 --> 00:11:18,399
24 7 because if your

00:11:16,240 --> 00:11:20,079
uber ad is down for an hour you don't

00:11:18,399 --> 00:11:21,440
just lose revenue but people will go to

00:11:20,079 --> 00:11:23,040
the competition

00:11:21,440 --> 00:11:24,800
and in addition to that they will also

00:11:23,040 --> 00:11:26,160
complain on twitter and facebook about

00:11:24,800 --> 00:11:27,839
that

00:11:26,160 --> 00:11:29,760
and the last example is paypal so this

00:11:27,839 --> 00:11:31,760
is really critical business transactions

00:11:29,760 --> 00:11:33,680
every payment is important and so

00:11:31,760 --> 00:11:35,839
the fraud detection infrastructure from

00:11:33,680 --> 00:11:37,600
from paypal is not just about one model

00:11:35,839 --> 00:11:39,839
but this is many different models to

00:11:37,600 --> 00:11:41,360
detect fraud and this is really a huge

00:11:39,839 --> 00:11:43,760
architecture

00:11:41,360 --> 00:11:44,800
and what these three examples have in

00:11:43,760 --> 00:11:46,399
common is that

00:11:44,800 --> 00:11:48,720
again though all of them need to act in

00:11:46,399 --> 00:11:50,800
real time at scale for millions of

00:11:48,720 --> 00:11:52,639
events and with zero downtime

00:11:50,800 --> 00:11:53,920
and so this is a critical infrastructure

00:11:52,639 --> 00:11:55,680
that's not what you do with python and

00:11:53,920 --> 00:11:58,160
jupyter notebooks right

00:11:55,680 --> 00:11:59,920
and no surprise here right so all three

00:11:58,160 --> 00:12:00,800
of them therefore use kafka under the

00:11:59,920 --> 00:12:02,639
hood as a

00:12:00,800 --> 00:12:05,360
central system for this infrastructure

00:12:02,639 --> 00:12:07,600
and combine kafka with machine learning

00:12:05,360 --> 00:12:09,040
to build such a reliable and scalable

00:12:07,600 --> 00:12:11,200
infrastructure

00:12:09,040 --> 00:12:14,560
and um now let's think about how we can

00:12:11,200 --> 00:12:14,560
do this in other approaches

00:12:14,800 --> 00:12:18,240
so this is where kafka comes into play

00:12:16,720 --> 00:12:19,600
in general so um

00:12:18,240 --> 00:12:21,279
on a high level it's really important to

00:12:19,600 --> 00:12:22,959
understand that kafka is not just a

00:12:21,279 --> 00:12:24,639
messaging layer but it's really also a

00:12:22,959 --> 00:12:25,760
storage layer to decouple the different

00:12:24,639 --> 00:12:27,760
systems

00:12:25,760 --> 00:12:29,120
some can consume the data in real time

00:12:27,760 --> 00:12:31,040
some others and batch

00:12:29,120 --> 00:12:33,040
and some artists with request response

00:12:31,040 --> 00:12:36,000
via rest api from their mobile app

00:12:33,040 --> 00:12:36,720
that's totally fine and therefore kafka

00:12:36,000 --> 00:12:38,399
is really

00:12:36,720 --> 00:12:40,800
a streaming platform messaging and

00:12:38,399 --> 00:12:42,560
storage but also it provides data

00:12:40,800 --> 00:12:43,360
integration capabilities with kafka

00:12:42,560 --> 00:12:45,760
connect

00:12:43,360 --> 00:12:48,079
both to legacy systems like a mainframe

00:12:45,760 --> 00:12:51,120
and to an ibm mq system

00:12:48,079 --> 00:12:53,200
and also to modern systems like mqtt or

00:12:51,120 --> 00:12:54,959
to machine learning frameworks and any

00:12:53,200 --> 00:12:56,959
other application

00:12:54,959 --> 00:12:59,279
and also it provides um processing

00:12:56,959 --> 00:13:01,440
capabilities with kafka streams

00:12:59,279 --> 00:13:03,279
um so this means that you can have a

00:13:01,440 --> 00:13:04,959
whole platform to do all this data

00:13:03,279 --> 00:13:07,760
correlation and integration

00:13:04,959 --> 00:13:09,120
in real time at scale highly available

00:13:07,760 --> 00:13:12,800
that's why so many people use

00:13:09,120 --> 00:13:12,800
the kafka ecosystem more and more

00:13:12,880 --> 00:13:16,880
and now if you think about machine

00:13:14,480 --> 00:13:17,519
learning um this really complements this

00:13:16,880 --> 00:13:19,440
right

00:13:17,519 --> 00:13:20,800
so machine learning in the end is always

00:13:19,440 --> 00:13:22,240
two things on a high level

00:13:20,800 --> 00:13:23,360
on the one side you need to model

00:13:22,240 --> 00:13:24,560
training which you see here on the

00:13:23,360 --> 00:13:26,160
bottom right

00:13:24,560 --> 00:13:27,600
model training means that you take

00:13:26,160 --> 00:13:29,279
historical data

00:13:27,600 --> 00:13:30,639
coming from all these data sources where

00:13:29,279 --> 00:13:32,480
you collect it from

00:13:30,639 --> 00:13:34,480
and then you take this data and put your

00:13:32,480 --> 00:13:36,000
algorithm on top of that and then you do

00:13:34,480 --> 00:13:38,560
some computation and then you get a

00:13:36,000 --> 00:13:40,720
model out of that which is a binary

00:13:38,560 --> 00:13:42,560
and this is the model so this is good

00:13:40,720 --> 00:13:44,240
but this is only half the part because

00:13:42,560 --> 00:13:45,040
then you need to deploy this model into

00:13:44,240 --> 00:13:46,240
production

00:13:45,040 --> 00:13:48,160
which you can see here on the bottom

00:13:46,240 --> 00:13:49,839
left where you do these

00:13:48,160 --> 00:13:51,920
predictions and often this has to be

00:13:49,839 --> 00:13:53,360
done in real time reliably at scale with

00:13:51,920 --> 00:13:55,440
low latency

00:13:53,360 --> 00:13:57,360
so very often model training and model

00:13:55,440 --> 00:13:58,079
deployment are completely separated from

00:13:57,360 --> 00:14:00,320
each other

00:13:58,079 --> 00:14:02,000
and this is okay right you can do model

00:14:00,320 --> 00:14:03,839
training in a data lake and deploy

00:14:02,000 --> 00:14:05,839
your models to a smaller lightweight

00:14:03,839 --> 00:14:08,720
application

00:14:05,839 --> 00:14:10,399
and so used um of course as the

00:14:08,720 --> 00:14:12,000
ingestion layer into a data lake

00:14:10,399 --> 00:14:13,519
that's one of the first use cases in

00:14:12,000 --> 00:14:16,079
analytics but

00:14:13,519 --> 00:14:16,720
you can do so much more with kafka like

00:14:16,079 --> 00:14:18,880
as i said

00:14:16,720 --> 00:14:20,639
um for example there is kafka connect to

00:14:18,880 --> 00:14:21,600
integrate to all of these data sources

00:14:20,639 --> 00:14:22,880
and things

00:14:21,600 --> 00:14:24,720
you don't want to integrate this

00:14:22,880 --> 00:14:26,240
integration by yourself

00:14:24,720 --> 00:14:28,399
and you can even produce from other

00:14:26,240 --> 00:14:31,360
systems like from a python consumer

00:14:28,399 --> 00:14:33,120
or from a go consumer or producer and so

00:14:31,360 --> 00:14:34,800
as you see here on the bottom right um

00:14:33,120 --> 00:14:36,399
the data science team probably prefers

00:14:34,800 --> 00:14:38,320
the python client to use

00:14:36,399 --> 00:14:40,320
and connect to the data um from the

00:14:38,320 --> 00:14:41,760
jupyter notebook instead of using java

00:14:40,320 --> 00:14:44,079
or something like this

00:14:41,760 --> 00:14:46,000
but on the bottom left often you use

00:14:44,079 --> 00:14:47,199
another technology like kafka streams or

00:14:46,000 --> 00:14:49,839
kcra libby

00:14:47,199 --> 00:14:51,839
and which is more uh well it's based on

00:14:49,839 --> 00:14:52,399
java and it simply has different kind of

00:14:51,839 --> 00:14:55,519
of

00:14:52,399 --> 00:14:57,040
um latency and and and performance slas

00:14:55,519 --> 00:14:58,560
than than python has

00:14:57,040 --> 00:15:01,120
but but their choice is up to you that's

00:14:58,560 --> 00:15:01,120
a great thing

00:15:01,600 --> 00:15:05,519
so let's think about this a little bit

00:15:03,279 --> 00:15:06,160
um more from a step-by-step approach um

00:15:05,519 --> 00:15:08,079
about

00:15:06,160 --> 00:15:10,320
what do you do to to build this now for

00:15:08,079 --> 00:15:12,079
our connected kind infrastructure

00:15:10,320 --> 00:15:14,079
so first of all we need to do a data

00:15:12,079 --> 00:15:16,320
collection so we ingest the iot data

00:15:14,079 --> 00:15:18,320
somehow into the kafka cluster

00:15:16,320 --> 00:15:20,160
and um in our case we can use kafka

00:15:18,320 --> 00:15:22,079
connect for example with an mqtt

00:15:20,160 --> 00:15:23,760
connector to do that

00:15:22,079 --> 00:15:26,320
and then from there we ingest it into

00:15:23,760 --> 00:15:28,480
the analytics pipeline right

00:15:26,320 --> 00:15:30,079
however having said that um a very

00:15:28,480 --> 00:15:31,680
common approach we see and this is true

00:15:30,079 --> 00:15:32,639
in automotive but also in many other

00:15:31,680 --> 00:15:34,079
industries

00:15:32,639 --> 00:15:35,920
that actually we even here see a

00:15:34,079 --> 00:15:38,079
separation of concerns so

00:15:35,920 --> 00:15:39,759
we see a mission critical kafka cluster

00:15:38,079 --> 00:15:41,440
where you do your

00:15:39,759 --> 00:15:43,920
business transactions and workloads

00:15:41,440 --> 00:15:44,639
which have to run 24 7 with zero data

00:15:43,920 --> 00:15:46,720
laws

00:15:44,639 --> 00:15:49,360
and separated from that we replicate the

00:15:46,720 --> 00:15:50,959
data into another kafka cluster where we

00:15:49,360 --> 00:15:53,040
do the analytics

00:15:50,959 --> 00:15:55,440
this is often for slas and uptime

00:15:53,040 --> 00:15:56,800
requirements and latency requirements

00:15:55,440 --> 00:15:58,720
but also for other reasons that you

00:15:56,800 --> 00:16:00,320
simply want to separate the business

00:15:58,720 --> 00:16:01,040
transactions from the analytical

00:16:00,320 --> 00:16:03,759
workloads

00:16:01,040 --> 00:16:05,360
um so we even have customers which use

00:16:03,759 --> 00:16:06,480
confluent only for the mission critical

00:16:05,360 --> 00:16:07,920
workloads and then

00:16:06,480 --> 00:16:09,839
they replicate replicated data into a

00:16:07,920 --> 00:16:11,360
not so critical cluster which might run

00:16:09,839 --> 00:16:14,079
on cloudera that's totally fine

00:16:11,360 --> 00:16:15,600
right um so there's different options

00:16:14,079 --> 00:16:16,880
how you do that but in the end you need

00:16:15,600 --> 00:16:19,519
to get your data

00:16:16,880 --> 00:16:22,160
from the data sources into kafka so that

00:16:19,519 --> 00:16:23,680
you can do the next steps on that

00:16:22,160 --> 00:16:25,279
and the next step typically is

00:16:23,680 --> 00:16:26,720
processing the data

00:16:25,279 --> 00:16:28,480
and if you think about that it's the

00:16:26,720 --> 00:16:30,160
same impedance mismatch um

00:16:28,480 --> 00:16:31,600
of course a data scientist can use

00:16:30,160 --> 00:16:32,399
python and the jupiter notebook for

00:16:31,600 --> 00:16:35,759
doing some

00:16:32,399 --> 00:16:37,759
some processing however if you want to

00:16:35,759 --> 00:16:38,560
connect for example to the connected car

00:16:37,759 --> 00:16:40,240
data um

00:16:38,560 --> 00:16:41,759
this is millions of events per seconds

00:16:40,240 --> 00:16:43,600
from from thousands or hundreds of

00:16:41,759 --> 00:16:44,800
thousands of cars so

00:16:43,600 --> 00:16:47,440
you don't want to do that in your

00:16:44,800 --> 00:16:47,920
jupyter notebook so um ideally you do

00:16:47,440 --> 00:16:50,959
this with

00:16:47,920 --> 00:16:52,399
scalable streaming technologies and um

00:16:50,959 --> 00:16:54,000
here you now have to ask yourself the

00:16:52,399 --> 00:16:55,600
question well um

00:16:54,000 --> 00:16:57,440
of course you could also just ingest

00:16:55,600 --> 00:16:57,920
everything into a spark and your spark

00:16:57,440 --> 00:16:59,759
streaming

00:16:57,920 --> 00:17:01,279
with that or you can use flink as an

00:16:59,759 --> 00:17:03,360
external cluster

00:17:01,279 --> 00:17:05,280
or like in this example you use kafka

00:17:03,360 --> 00:17:06,880
native technologies like kafka streams

00:17:05,280 --> 00:17:08,640
or k sql db

00:17:06,880 --> 00:17:10,559
this has a huge advantage that you have

00:17:08,640 --> 00:17:12,799
just one single infrastructure

00:17:10,559 --> 00:17:14,160
not just for data ingestion but also for

00:17:12,799 --> 00:17:16,400
data processing

00:17:14,160 --> 00:17:18,400
because these tools use kafka under hood

00:17:16,400 --> 00:17:20,880
just one single infrastructure to run it

00:17:18,400 --> 00:17:23,520
operate 24 7 instead of a separate

00:17:20,880 --> 00:17:25,120
fling or spark cluster but whatever is

00:17:23,520 --> 00:17:26,400
your choice you do the pre-processing

00:17:25,120 --> 00:17:29,200
with these tools

00:17:26,400 --> 00:17:30,960
and then as a next step you need to

00:17:29,200 --> 00:17:32,720
ingest it somewhere

00:17:30,960 --> 00:17:34,400
before that here is one example of such

00:17:32,720 --> 00:17:36,720
a k-sql query

00:17:34,400 --> 00:17:37,840
and um with this sql query we do some

00:17:36,720 --> 00:17:38,960
streaming etl

00:17:37,840 --> 00:17:41,600
in this case it's more or less like

00:17:38,960 --> 00:17:43,919
hello world right and we filter out

00:17:41,600 --> 00:17:45,200
of the car data only the data for one

00:17:43,919 --> 00:17:48,400
specific model type

00:17:45,200 --> 00:17:49,360
a very simple um filtering approach but

00:17:48,400 --> 00:17:51,840
of course you can do

00:17:49,360 --> 00:17:53,600
many more powerful steps here and the

00:17:51,840 --> 00:17:55,200
great thing about this query here is

00:17:53,600 --> 00:17:56,720
that um this is pretty straightforward

00:17:55,200 --> 00:17:58,559
and everybody can do that

00:17:56,720 --> 00:18:00,720
but you can use this on the one side for

00:17:58,559 --> 00:18:02,320
rapid prototyping like you see here you

00:18:00,720 --> 00:18:03,280
can even embed this instantly in a

00:18:02,320 --> 00:18:05,919
jupyter notebook

00:18:03,280 --> 00:18:07,679
to combine this with your python code so

00:18:05,919 --> 00:18:09,840
you can consume the data from a kafka

00:18:07,679 --> 00:18:11,280
cluster with kcq like here

00:18:09,840 --> 00:18:12,799
or you could also use the confluent

00:18:11,280 --> 00:18:15,120
python client if you want to do more

00:18:12,799 --> 00:18:17,200
python code instead of um sql

00:18:15,120 --> 00:18:19,280
and then you um use your other favorite

00:18:17,200 --> 00:18:21,200
python tools like numpy or scikit-learn

00:18:19,280 --> 00:18:23,120
and then your tensorflow api

00:18:21,200 --> 00:18:26,240
so this can all be combined in a jupyter

00:18:23,120 --> 00:18:28,640
notebook to be very agile and rapid

00:18:26,240 --> 00:18:29,440
but then this also solves this impedance

00:18:28,640 --> 00:18:32,480
mismatch

00:18:29,440 --> 00:18:34,400
because you can deploy this query into a

00:18:32,480 --> 00:18:35,840
production cluster to process millions

00:18:34,400 --> 00:18:37,760
of events per second

00:18:35,840 --> 00:18:38,960
because even this simple query is just

00:18:37,760 --> 00:18:40,240
kafka under the hood

00:18:38,960 --> 00:18:42,000
which you don't see when you do this

00:18:40,240 --> 00:18:44,320
right but in this way you

00:18:42,000 --> 00:18:46,000
can do the data science team do their

00:18:44,320 --> 00:18:47,840
work but then when it's done with the

00:18:46,000 --> 00:18:48,400
prototyping you can deploy the same

00:18:47,840 --> 00:18:50,160
process

00:18:48,400 --> 00:18:51,440
into a production cluster without any

00:18:50,160 --> 00:18:53,600
code changes or any

00:18:51,440 --> 00:18:54,880
wrapper code or something like that so

00:18:53,600 --> 00:18:57,280
this is why they think this is a great

00:18:54,880 --> 00:18:58,720
example

00:18:57,280 --> 00:19:00,720
and then um when you have done your

00:18:58,720 --> 00:19:02,640
processing with something like a sql or

00:19:00,720 --> 00:19:04,720
maybe with link or spark streaming

00:19:02,640 --> 00:19:06,160
then you'll need to ingest it into your

00:19:04,720 --> 00:19:07,120
data store where you're doing model

00:19:06,160 --> 00:19:08,880
training

00:19:07,120 --> 00:19:10,320
in our case we're on google cloud so we

00:19:08,880 --> 00:19:12,080
ingest everything into google blob

00:19:10,320 --> 00:19:14,080
storage google cloud storage

00:19:12,080 --> 00:19:15,440
so that we can run our model training on

00:19:14,080 --> 00:19:17,600
top of that

00:19:15,440 --> 00:19:19,039
so um in addition to that also we have

00:19:17,600 --> 00:19:20,720
other consumers right so that's the

00:19:19,039 --> 00:19:21,840
beauty about kafka some are real time

00:19:20,720 --> 00:19:23,679
some are batch

00:19:21,840 --> 00:19:27,280
and in this case we do ingestion into

00:19:23,679 --> 00:19:27,280
the cloud store for a model training

00:19:27,600 --> 00:19:30,640
and then we train our model finally

00:19:29,200 --> 00:19:32,720
after all these steps

00:19:30,640 --> 00:19:33,840
and this is where the cloud is great and

00:19:32,720 --> 00:19:35,760
um here you can

00:19:33,840 --> 00:19:38,080
do extreme scale and scale up and then

00:19:35,760 --> 00:19:40,240
scale down after the model training

00:19:38,080 --> 00:19:42,080
and and this is not related to kafka at

00:19:40,240 --> 00:19:44,559
all so here also to be clear though

00:19:42,080 --> 00:19:46,320
kafka is not machine learning kafka is

00:19:44,559 --> 00:19:47,600
complementary to machine learning

00:19:46,320 --> 00:19:50,400
right and here in this case we use

00:19:47,600 --> 00:19:51,840
tensorflow to train a model

00:19:50,400 --> 00:19:54,240
as i said in our case we use an

00:19:51,840 --> 00:19:57,200
autoencoder this is an unsupervised

00:19:54,240 --> 00:19:58,720
concept for um detecting animally but

00:19:57,200 --> 00:19:59,679
whatever you use this is just one

00:19:58,720 --> 00:20:01,919
example of

00:19:59,679 --> 00:20:03,280
of having a model the big point here now

00:20:01,919 --> 00:20:07,840
is that we have a model and this is a

00:20:03,280 --> 00:20:07,840
binary which we can deploy anywhere else

00:20:08,320 --> 00:20:11,679
and now i want to talk a little bit more

00:20:09,679 --> 00:20:13,280
about this architecture because as i

00:20:11,679 --> 00:20:15,039
mentioned before

00:20:13,280 --> 00:20:16,400
you don't necessarily need another data

00:20:15,039 --> 00:20:18,400
lake to the model training

00:20:16,400 --> 00:20:20,000
so you can directly consume the data

00:20:18,400 --> 00:20:22,720
from kafka into your ml

00:20:20,000 --> 00:20:24,720
framework like here we use tensorflow

00:20:22,720 --> 00:20:27,360
and its io kafka plugin

00:20:24,720 --> 00:20:29,360
to directly consume from tensorflow and

00:20:27,360 --> 00:20:31,039
get it from kafka look to train a model

00:20:29,360 --> 00:20:32,159
with that so it's pretty straightforward

00:20:31,039 --> 00:20:34,000
and completely

00:20:32,159 --> 00:20:36,320
simplifies your architecture because you

00:20:34,000 --> 00:20:37,840
don't need an additional s3 or hdfs

00:20:36,320 --> 00:20:39,919
interface for that

00:20:37,840 --> 00:20:41,360
so this is really huge and it's great

00:20:39,919 --> 00:20:43,600
and then later you can also

00:20:41,360 --> 00:20:44,480
replay the data so many people use kafka

00:20:43,600 --> 00:20:46,240
for that

00:20:44,480 --> 00:20:47,679
because in kafka the events are appended

00:20:46,240 --> 00:20:49,440
in an even based way

00:20:47,679 --> 00:20:51,200
with timestamps and with guaranteed

00:20:49,440 --> 00:20:52,480
ordering and so you can replay the data

00:20:51,200 --> 00:20:54,480
later again

00:20:52,480 --> 00:20:55,919
maybe use another kafka another

00:20:54,480 --> 00:20:57,280
tensorflow configuration with other

00:20:55,919 --> 00:20:59,440
hyper parameters

00:20:57,280 --> 00:21:01,520
or maybe a month later you want to try

00:20:59,440 --> 00:21:03,440
out a completely different ml framework

00:21:01,520 --> 00:21:05,120
like data robot for outlier ml

00:21:03,440 --> 00:21:08,000
and compare it to your customized

00:21:05,120 --> 00:21:08,000
tensorflow model

00:21:08,080 --> 00:21:11,360
and this is then typically also where

00:21:09,600 --> 00:21:14,000
tip storage comes into play

00:21:11,360 --> 00:21:15,919
because um of course um you can store

00:21:14,000 --> 00:21:17,919
data in kafka forever with retention

00:21:15,919 --> 00:21:19,520
time -1 so that's not the problem from a

00:21:17,919 --> 00:21:22,240
technical perspective

00:21:19,520 --> 00:21:24,000
but the big problem is with kafka only

00:21:22,240 --> 00:21:25,840
and this can get pretty expensive and

00:21:24,000 --> 00:21:27,679
also the scalability is harder when you

00:21:25,840 --> 00:21:28,960
have terabytes of data in your kafka

00:21:27,679 --> 00:21:32,320
cluster

00:21:28,960 --> 00:21:33,039
and um still we want to simplify this

00:21:32,320 --> 00:21:35,600
and that's why

00:21:33,039 --> 00:21:36,640
um tear storage makes so much sense with

00:21:35,600 --> 00:21:39,280
tier storage

00:21:36,640 --> 00:21:40,799
you can avoid having another data lake

00:21:39,280 --> 00:21:43,679
in addition to kafka

00:21:40,799 --> 00:21:45,440
to pay and operate and integrate and

00:21:43,679 --> 00:21:47,600
with this you can reduce the cost but

00:21:45,440 --> 00:21:49,200
still have this long-term storage

00:21:47,600 --> 00:21:51,280
and you still have this performance

00:21:49,200 --> 00:21:52,720
isolation so this means you can still

00:21:51,280 --> 00:21:54,799
have your real-time consumers which

00:21:52,720 --> 00:21:56,640
continuously process the data

00:21:54,799 --> 00:21:58,480
but your data science team can spin up a

00:21:56,640 --> 00:22:00,720
jupyter notebook and consume historical

00:21:58,480 --> 00:22:04,000
data and train a model on top of that

00:22:00,720 --> 00:22:06,000
from the same kafka cluster and a huge

00:22:04,000 --> 00:22:08,480
advantage of tiered storage is now

00:22:06,000 --> 00:22:11,200
that you offload most of the data from

00:22:08,480 --> 00:22:12,720
the kafka brokers into an object store

00:22:11,200 --> 00:22:14,799
so as i said in the beginning of this

00:22:12,720 --> 00:22:17,039
talk um with confluential storage which

00:22:14,799 --> 00:22:18,880
is ga already you can use things like s3

00:22:17,039 --> 00:22:21,679
or google cloud storage or even

00:22:18,880 --> 00:22:22,320
pure storage for example on premise and

00:22:21,679 --> 00:22:25,039
um

00:22:22,320 --> 00:22:27,159
soon this will also get into the open

00:22:25,039 --> 00:22:29,919
source kafka is part of the kip

00:22:27,159 --> 00:22:31,360
405 um which confluence is working on

00:22:29,919 --> 00:22:33,039
with the community

00:22:31,360 --> 00:22:34,880
and with tier storage then you can save

00:22:33,039 --> 00:22:37,360
a lot of money but still store

00:22:34,880 --> 00:22:40,000
data long term in kafka to reprocess

00:22:37,360 --> 00:22:41,200
historical data in an even based manner

00:22:40,000 --> 00:22:43,280
and that's really huge and also

00:22:41,200 --> 00:22:44,880
scalability gets much better because

00:22:43,280 --> 00:22:47,200
if you think about a disk crash for

00:22:44,880 --> 00:22:49,039
example if you need to synchronize

00:22:47,200 --> 00:22:50,480
terabytes of data in a kafka cluster

00:22:49,039 --> 00:22:52,400
this can take hours

00:22:50,480 --> 00:22:54,000
but with teal storage it doesn't because

00:22:52,400 --> 00:22:55,200
only one percent or so of your data

00:22:54,000 --> 00:22:57,280
is actually in the brokers which you

00:22:55,200 --> 00:22:59,520
need to synchronize everything else is

00:22:57,280 --> 00:23:01,760
offloaded to a tiered storage

00:22:59,520 --> 00:23:03,760
and the big advantage however is that

00:23:01,760 --> 00:23:04,960
tier storage does not change your client

00:23:03,760 --> 00:23:07,440
applications

00:23:04,960 --> 00:23:08,000
so the client api is exactly the same so

00:23:07,440 --> 00:23:10,240
no code

00:23:08,000 --> 00:23:12,000
breaking changes here and that's really

00:23:10,240 --> 00:23:13,520
huge so as a client you don't even know

00:23:12,000 --> 00:23:14,400
if you're using teeth storage under the

00:23:13,520 --> 00:23:16,080
hood

00:23:14,400 --> 00:23:17,760
and also because this question comes up

00:23:16,080 --> 00:23:19,760
all the time you don't have to be

00:23:17,760 --> 00:23:22,799
worried about this throughput so

00:23:19,760 --> 00:23:24,480
if you consume historical data um the

00:23:22,799 --> 00:23:25,600
difference between consuming from the

00:23:24,480 --> 00:23:27,679
disks on the broker

00:23:25,600 --> 00:23:29,280
and consuming from the object store so

00:23:27,679 --> 00:23:31,600
from the tests we did um

00:23:29,280 --> 00:23:33,360
this is really neglectable so if you

00:23:31,600 --> 00:23:35,520
consume it from s3 or from the

00:23:33,360 --> 00:23:39,520
ebs volumes um this doesn't make much

00:23:35,520 --> 00:23:39,520
change um if you consume historical data

00:23:40,559 --> 00:23:43,840
and there are so many use cases where

00:23:42,000 --> 00:23:45,520
you want to reprocess data in this talk

00:23:43,840 --> 00:23:47,440
i focus much on machine learning so you

00:23:45,520 --> 00:23:49,360
can take a look at all data and consume

00:23:47,440 --> 00:23:51,760
it to train new models on that

00:23:49,360 --> 00:23:54,000
um this might be for um you found out

00:23:51,760 --> 00:23:55,760
that a fraud happened a year ago and so

00:23:54,000 --> 00:23:57,840
the data scientists have to take a look

00:23:55,760 --> 00:23:59,039
at the old data and train models also on

00:23:57,840 --> 00:24:00,559
the old data

00:23:59,039 --> 00:24:02,159
um but there is many other use cases

00:24:00,559 --> 00:24:04,159
like for compliance and regular

00:24:02,159 --> 00:24:04,880
reprocessing you also need to consume

00:24:04,159 --> 00:24:07,600
more data

00:24:04,880 --> 00:24:09,039
and there is really many uh use cases

00:24:07,600 --> 00:24:10,640
why you want to keep data in an

00:24:09,039 --> 00:24:13,360
event-based manner for long-term to

00:24:10,640 --> 00:24:14,720
reprocess it again and again later

00:24:13,360 --> 00:24:16,240
and this is where tier storage is really

00:24:14,720 --> 00:24:17,120
helpful because you can even store

00:24:16,240 --> 00:24:20,640
bigger data

00:24:17,120 --> 00:24:22,480
in a cost efficient way so now after i

00:24:20,640 --> 00:24:24,000
talked about the model training a lot um

00:24:22,480 --> 00:24:25,120
of course we also want to deploy our

00:24:24,000 --> 00:24:26,960
model right

00:24:25,120 --> 00:24:28,640
no matter if we deployed it in the cloud

00:24:26,960 --> 00:24:29,200
in a data lake with spark or something

00:24:28,640 --> 00:24:31,039
else

00:24:29,200 --> 00:24:32,720
then the model deployment often happens

00:24:31,039 --> 00:24:35,520
somewhere completely different

00:24:32,720 --> 00:24:36,559
like in a factory or in a car or on the

00:24:35,520 --> 00:24:38,240
mobile app

00:24:36,559 --> 00:24:40,640
and this is no problem because kafka is

00:24:38,240 --> 00:24:42,000
just a binary the model is just a binary

00:24:40,640 --> 00:24:43,600
right

00:24:42,000 --> 00:24:46,080
there is two options for model

00:24:43,600 --> 00:24:48,320
deployment and i cannot go into detail

00:24:46,080 --> 00:24:49,360
today i have another kafka summit talk

00:24:48,320 --> 00:24:51,440
which you might look up

00:24:49,360 --> 00:24:53,279
where i talk 60 minutes just about model

00:24:51,440 --> 00:24:54,880
deployment with kafka because there's a

00:24:53,279 --> 00:24:55,679
lot of trade-offs you need to understand

00:24:54,880 --> 00:24:57,679
better

00:24:55,679 --> 00:24:59,279
but on a high level there's two options

00:24:57,679 --> 00:25:01,120
um that's the data science

00:24:59,279 --> 00:25:03,120
version here so um you use a model

00:25:01,120 --> 00:25:04,720
server right um every machine learning

00:25:03,120 --> 00:25:06,640
framework or product has one

00:25:04,720 --> 00:25:08,320
so you deploy your model there and then

00:25:06,640 --> 00:25:11,760
from your kafka application

00:25:08,320 --> 00:25:13,520
you do an rpc call to that model server

00:25:11,760 --> 00:25:15,039
the big problem here from a kafka

00:25:13,520 --> 00:25:16,480
perspective is that while you have a

00:25:15,039 --> 00:25:18,960
streaming application

00:25:16,480 --> 00:25:20,400
you always have to do an rpc call to do

00:25:18,960 --> 00:25:22,799
the model prediction

00:25:20,400 --> 00:25:24,559
and this is not a good architecture it

00:25:22,799 --> 00:25:25,760
still works for some use cases right

00:25:24,559 --> 00:25:27,679
it's still good enough

00:25:25,760 --> 00:25:29,200
but for example like for connected car

00:25:27,679 --> 00:25:30,640
infrastructure where you have millions

00:25:29,200 --> 00:25:32,559
of events per second

00:25:30,640 --> 00:25:34,559
and you need to to do model predictions

00:25:32,559 --> 00:25:35,760
in real time in a robust way with low

00:25:34,559 --> 00:25:37,600
latency

00:25:35,760 --> 00:25:40,080
and the much better architecture is to

00:25:37,600 --> 00:25:41,360
directly embed your model into the kafka

00:25:40,080 --> 00:25:43,120
application

00:25:41,360 --> 00:25:44,880
and this is also pretty straightforward

00:25:43,120 --> 00:25:45,840
to do many people are surprised how easy

00:25:44,880 --> 00:25:48,240
that is but

00:25:45,840 --> 00:25:50,080
a model is just a binary and you can

00:25:48,240 --> 00:25:51,200
easily load that into your kafka

00:25:50,080 --> 00:25:53,279
application so

00:25:51,200 --> 00:25:54,720
that you don't have this rpc call to

00:25:53,279 --> 00:25:55,919
another server

00:25:54,720 --> 00:25:58,159
because you have to think about the

00:25:55,919 --> 00:25:59,760
trade of using a model server like

00:25:58,159 --> 00:26:02,080
what do you do if the rpc is not

00:25:59,760 --> 00:26:04,320
possible or if the latency is not good

00:26:02,080 --> 00:26:06,080
how do you do error handling what do you

00:26:04,320 --> 00:26:07,760
do if you use exactly one semantics in

00:26:06,080 --> 00:26:08,960
kafka and you have a failure with the

00:26:07,760 --> 00:26:10,559
model server

00:26:08,960 --> 00:26:12,799
there's many trade-offs you have to

00:26:10,559 --> 00:26:14,960
compare here um so um

00:26:12,799 --> 00:26:15,919
know that the pros and cons of both and

00:26:14,960 --> 00:26:19,279
make the right call

00:26:15,919 --> 00:26:21,360
depending on your slas and requirements

00:26:19,279 --> 00:26:22,799
here is one example again with k-square

00:26:21,360 --> 00:26:25,279
so this is so powerful

00:26:22,799 --> 00:26:26,480
here is a very simple query we create an

00:26:25,279 --> 00:26:28,640
anomaly detection

00:26:26,480 --> 00:26:29,600
where we consume the sensor id from the

00:26:28,640 --> 00:26:31,840
cars

00:26:29,600 --> 00:26:32,799
and we use a user-defined function which

00:26:31,840 --> 00:26:35,200
i implemented

00:26:32,799 --> 00:26:36,080
detect anomaly which uses the sensor

00:26:35,200 --> 00:26:38,000
values

00:26:36,080 --> 00:26:39,919
and under the hood we embedded this

00:26:38,000 --> 00:26:41,440
anomaly detection with the tensorflow

00:26:39,919 --> 00:26:42,320
model which we trained before in the

00:26:41,440 --> 00:26:43,840
cloud

00:26:42,320 --> 00:26:45,840
and with this we can do model

00:26:43,840 --> 00:26:47,360
predictions in real time at scale for

00:26:45,840 --> 00:26:49,120
millions of events

00:26:47,360 --> 00:26:51,520
and and this is really huge because this

00:26:49,120 --> 00:26:52,240
query can deploy like this into k sql

00:26:51,520 --> 00:26:54,320
servers

00:26:52,240 --> 00:26:55,840
without any additional wrapper code or

00:26:54,320 --> 00:26:57,679
configuration

00:26:55,840 --> 00:27:00,000
and because this is just kafka under the

00:26:57,679 --> 00:27:01,919
hood using partitions and replication

00:27:00,000 --> 00:27:03,360
and kafka topics and all these things

00:27:01,919 --> 00:27:05,120
you don't have to worry at all about

00:27:03,360 --> 00:27:06,400
high availability or latency or

00:27:05,120 --> 00:27:08,080
scalability

00:27:06,400 --> 00:27:10,240
this is just a kafka application like

00:27:08,080 --> 00:27:12,960
anything else but it's very simple to

00:27:10,240 --> 00:27:12,960
build this right

00:27:13,520 --> 00:27:16,799
and so now here we see again our

00:27:15,120 --> 00:27:18,480
architecture i talked before now with

00:27:16,799 --> 00:27:20,000
technologies mapped

00:27:18,480 --> 00:27:22,320
typically when we talk about connected

00:27:20,000 --> 00:27:24,640
cars um the interface is mqtt because

00:27:22,320 --> 00:27:25,919
this works also in bad networks or

00:27:24,640 --> 00:27:27,840
if you're offline when you're driving

00:27:25,919 --> 00:27:28,480
through a tunnel and from that we

00:27:27,840 --> 00:27:30,320
integrate

00:27:28,480 --> 00:27:32,000
for example with kafka connect to get

00:27:30,320 --> 00:27:34,320
the data into the classroom

00:27:32,000 --> 00:27:36,159
and then we use something like k sql for

00:27:34,320 --> 00:27:38,559
pre-processing the data

00:27:36,159 --> 00:27:39,600
and as i said before um in this case we

00:27:38,559 --> 00:27:41,600
directly

00:27:39,600 --> 00:27:42,720
consume the data with tensorflow from

00:27:41,600 --> 00:27:44,559
kafka

00:27:42,720 --> 00:27:46,640
without any other data lake in the

00:27:44,559 --> 00:27:47,120
middle and then we train our model and

00:27:46,640 --> 00:27:49,120
deploy

00:27:47,120 --> 00:27:51,360
us and the model scoring is also

00:27:49,120 --> 00:27:53,600
happening in another kafka application

00:27:51,360 --> 00:27:55,039
this can for example be a kafka streams

00:27:53,600 --> 00:27:57,360
java application

00:27:55,039 --> 00:27:59,440
completely separate from model training

00:27:57,360 --> 00:28:00,960
and in addition to that in this example

00:27:59,440 --> 00:28:03,120
we have also built a digital twin with

00:28:00,960 --> 00:28:04,960
mongodb which is simply another use case

00:28:03,120 --> 00:28:06,240
completely separated from the analytics

00:28:04,960 --> 00:28:08,080
use cases

00:28:06,240 --> 00:28:10,000
in kafka typically as soon as you have a

00:28:08,080 --> 00:28:11,760
pipeline built in your organization

00:28:10,000 --> 00:28:13,279
many other business units come and want

00:28:11,760 --> 00:28:14,799
to use some of the data because

00:28:13,279 --> 00:28:17,760
and that's what the strength of kafka

00:28:14,799 --> 00:28:20,080
because everything is decoupled

00:28:17,760 --> 00:28:21,919
so um with this architecture in mind we

00:28:20,080 --> 00:28:23,919
have actually exactly built this so you

00:28:21,919 --> 00:28:26,240
can check out the github link

00:28:23,919 --> 00:28:28,159
we have built this demo in our case we

00:28:26,240 --> 00:28:30,480
have built it on kubernetes so that um

00:28:28,159 --> 00:28:32,480
it's an example which you can build by

00:28:30,480 --> 00:28:33,440
yourself and also on-premise or in any

00:28:32,480 --> 00:28:34,960
cloud

00:28:33,440 --> 00:28:36,880
but of course you could also do the same

00:28:34,960 --> 00:28:38,000
for example with fully managed confluent

00:28:36,880 --> 00:28:41,440
cloud

00:28:38,000 --> 00:28:44,480
um the big point here is also that um

00:28:41,440 --> 00:28:46,480
we even used python for the production

00:28:44,480 --> 00:28:48,880
deployment of the model scoring

00:28:46,480 --> 00:28:49,760
so you you don't have to use java for

00:28:48,880 --> 00:28:52,159
that right um

00:28:49,760 --> 00:28:53,120
even this application um can be done

00:28:52,159 --> 00:28:55,279
with python

00:28:53,120 --> 00:28:57,520
so in our case we build a python

00:28:55,279 --> 00:28:58,159
container which has just two libraries

00:28:57,520 --> 00:29:00,559
in there

00:28:58,159 --> 00:29:01,840
it has the kafka consumer to consume the

00:29:00,559 --> 00:29:03,919
streaming data

00:29:01,840 --> 00:29:05,679
and we have the model from tensorflow

00:29:03,919 --> 00:29:07,039
which is embedded to do the predictions

00:29:05,679 --> 00:29:08,559
in real time

00:29:07,039 --> 00:29:10,320
and while i said in the beginning that

00:29:08,559 --> 00:29:11,919
of course it does not perform as well as

00:29:10,320 --> 00:29:13,679
a java application because it's not

00:29:11,919 --> 00:29:15,679
compiled by code under the hood

00:29:13,679 --> 00:29:17,039
but still it performs very well for many

00:29:15,679 --> 00:29:18,640
use cases so

00:29:17,039 --> 00:29:21,279
we have customers which just use the

00:29:18,640 --> 00:29:22,880
python client and process over 100 000

00:29:21,279 --> 00:29:24,720
messages per second

00:29:22,880 --> 00:29:26,799
so for many use cases this is still good

00:29:24,720 --> 00:29:28,480
enough having said this

00:29:26,799 --> 00:29:30,640
in our use case like connected cars with

00:29:28,480 --> 00:29:32,000
millions of cars um maybe a java app is

00:29:30,640 --> 00:29:34,000
better but that's up to you and your

00:29:32,000 --> 00:29:35,279
architecture or maybe use golang instead

00:29:34,000 --> 00:29:38,399
that's all up to you

00:29:35,279 --> 00:29:40,159
you're very flexible here

00:29:38,399 --> 00:29:42,000
and as i said so i have this demo and i

00:29:40,159 --> 00:29:43,919
have also many other demos for

00:29:42,000 --> 00:29:46,559
um kafka together with machine learning

00:29:43,919 --> 00:29:49,120
so i have built examples for deploying

00:29:46,559 --> 00:29:50,559
things like um tensorflow and h2o and

00:29:49,120 --> 00:29:53,039
deep learning for j

00:29:50,559 --> 00:29:54,399
within kafka streams and k sql apps so

00:29:53,039 --> 00:29:56,159
this is pretty straightforward if you're

00:29:54,399 --> 00:29:57,679
a developer and you can do this easily

00:29:56,159 --> 00:30:01,360
by yourself and it really doesn't matter

00:29:57,679 --> 00:30:01,360
which machine learning framework to use

00:30:02,240 --> 00:30:06,159
so to conclude this session so in the

00:30:04,480 --> 00:30:07,360
end really a huge advantage is that with

00:30:06,159 --> 00:30:09,679
kafka you can

00:30:07,360 --> 00:30:11,039
um build one pipeline to rule them all

00:30:09,679 --> 00:30:12,240
that's of course intentionally a little

00:30:11,039 --> 00:30:14,159
bit provoking right

00:30:12,240 --> 00:30:15,760
um of course you can still combine this

00:30:14,159 --> 00:30:16,880
with something like spark or fling or

00:30:15,760 --> 00:30:18,720
anything else

00:30:16,880 --> 00:30:20,320
i'm just saying you really should always

00:30:18,720 --> 00:30:22,640
evaluate um

00:30:20,320 --> 00:30:23,919
to keep your architecture as simple as

00:30:22,640 --> 00:30:25,679
possible because

00:30:23,919 --> 00:30:27,919
every additional cluster you add this

00:30:25,679 --> 00:30:29,360
makes the complexity much harder

00:30:27,919 --> 00:30:31,440
and especially for the more mission

00:30:29,360 --> 00:30:33,360
critical use cases with high slas and

00:30:31,440 --> 00:30:35,600
low latency requirements

00:30:33,360 --> 00:30:37,039
um the less infrastructure you use the

00:30:35,600 --> 00:30:39,840
better it is for you um

00:30:37,039 --> 00:30:41,520
regarding an operations perspective and

00:30:39,840 --> 00:30:43,279
especially now where i introduce you to

00:30:41,520 --> 00:30:45,360
tf storage today

00:30:43,279 --> 00:30:46,720
it's pretty straightforward to also use

00:30:45,360 --> 00:30:49,200
kafka not just for

00:30:46,720 --> 00:30:50,000
real-time applications but also for

00:30:49,200 --> 00:30:53,120
example for

00:30:50,000 --> 00:30:55,440
using data science teams to consume old

00:30:53,120 --> 00:30:57,840
data from kafka historical data

00:30:55,440 --> 00:30:59,840
and use your favorite machine learning

00:30:57,840 --> 00:31:01,760
infrastructure like python and jupiter

00:30:59,840 --> 00:31:03,760
together with something like nine pi and

00:31:01,760 --> 00:31:06,000
cycle learn and the tensorflow api

00:31:03,760 --> 00:31:07,120
to train models and then deploy them so

00:31:06,000 --> 00:31:09,279
this is really

00:31:07,120 --> 00:31:11,279
the main lesson learned here from that

00:31:09,279 --> 00:31:13,120
and with that i'm at the end of my talk

00:31:11,279 --> 00:31:15,200
i hope you liked it and got an overview

00:31:13,120 --> 00:31:17,120
about what you can do together with

00:31:15,200 --> 00:31:19,200
kafka and machine learning

00:31:17,120 --> 00:31:20,799
and um also feel free to connect to me

00:31:19,200 --> 00:31:22,159
on linkedin and on twitter to stay in

00:31:20,799 --> 00:31:23,919
touch about this

00:31:22,159 --> 00:31:26,000
um i'm happy to also discuss with you

00:31:23,919 --> 00:31:28,880
and engage more and therefore i

00:31:26,000 --> 00:31:30,480
i really hope you like this and now if

00:31:28,880 --> 00:31:32,799
you also have any question i'm

00:31:30,480 --> 00:31:34,320
monitoring the chat window so um feel

00:31:32,799 --> 00:31:38,880
free to also ask um

00:31:34,320 --> 00:31:41,360
any questions more so um

00:31:38,880 --> 00:31:42,640
yeah exactly here's the one question um

00:31:41,360 --> 00:31:44,799
which scenario is better in terms of

00:31:42,640 --> 00:31:47,600
data analysis performance so um

00:31:44,799 --> 00:31:48,320
data to kafka intent to tensorflow or

00:31:47,600 --> 00:31:50,559
data to

00:31:48,320 --> 00:31:52,000
kafka and then to spark and this really

00:31:50,559 --> 00:31:55,120
depends exactly on on

00:31:52,000 --> 00:31:56,000
what you want to do i mean um if you are

00:31:55,120 --> 00:31:58,559
thinking about

00:31:56,000 --> 00:32:00,080
um you definitely want to use tensorflow

00:31:58,559 --> 00:32:01,840
then there is no need to ingest it into

00:32:00,080 --> 00:32:02,640
another data lake where you also can use

00:32:01,840 --> 00:32:04,159
spark

00:32:02,640 --> 00:32:05,760
so this is actually the question it's

00:32:04,159 --> 00:32:07,120
not so much about performance but about

00:32:05,760 --> 00:32:08,399
really what consumers you need

00:32:07,120 --> 00:32:10,720
if you want to train models with

00:32:08,399 --> 00:32:13,039
tensorflow um then there is no added

00:32:10,720 --> 00:32:15,440
value if another data lake with spark

00:32:13,039 --> 00:32:17,279
having said this um spark also does some

00:32:15,440 --> 00:32:19,360
things very well for example

00:32:17,279 --> 00:32:20,559
if you want to do more like batch based

00:32:19,360 --> 00:32:22,720
processing um

00:32:20,559 --> 00:32:24,000
so um or if you wanted to shuffling all

00:32:22,720 --> 00:32:25,679
of the data at once

00:32:24,000 --> 00:32:27,200
this is where another data lake makes

00:32:25,679 --> 00:32:28,799
sense because that was they were built

00:32:27,200 --> 00:32:31,760
for and where they are better

00:32:28,799 --> 00:32:33,200
so most of our customers actually um do

00:32:31,760 --> 00:32:35,679
not have just one single

00:32:33,200 --> 00:32:36,480
um solution for that so they have this

00:32:35,679 --> 00:32:37,519
pipeline

00:32:36,480 --> 00:32:39,360
and then they build their own

00:32:37,519 --> 00:32:40,640
materialized views on top of that some

00:32:39,360 --> 00:32:42,720
use cases with spark

00:32:40,640 --> 00:32:44,399
some others with tensorflow and then

00:32:42,720 --> 00:32:46,159
maybe in the future also without time

00:32:44,399 --> 00:32:48,000
error with other use cases

00:32:46,159 --> 00:32:49,840
so if you want to see a great example

00:32:48,000 --> 00:32:53,519
for that um you can google for

00:32:49,840 --> 00:32:54,960
bmw kafka summit because bmw presented a

00:32:53,519 --> 00:32:57,279
framework where they built an

00:32:54,960 --> 00:32:58,240
nlp solution so natural language

00:32:57,279 --> 00:33:00,240
processing

00:32:58,240 --> 00:33:01,760
on top of kafka because from the

00:33:00,240 --> 00:33:03,679
beginning bmw knew

00:33:01,760 --> 00:33:05,200
that they don't have just one one ml

00:33:03,679 --> 00:33:06,960
framework for that they had different

00:33:05,200 --> 00:33:08,080
use cases and for some they used some

00:33:06,960 --> 00:33:09,519
python frameworks

00:33:08,080 --> 00:33:11,600
for some others they used java

00:33:09,519 --> 00:33:13,279
frameworks and for some others they use

00:33:11,600 --> 00:33:14,960
native cloud services

00:33:13,279 --> 00:33:16,799
and so they build an orchestration

00:33:14,960 --> 00:33:18,480
pipeline around kafka tour

00:33:16,799 --> 00:33:20,000
and be very flexible and technology

00:33:18,480 --> 00:33:22,720
agnostic here so again

00:33:20,000 --> 00:33:24,080
um just google for bmw and kafka summit

00:33:22,720 --> 00:33:26,320
they talked about this i think one or

00:33:24,080 --> 00:33:27,679
two years ago and this is a great talk

00:33:26,320 --> 00:33:29,919
with slides in video recording where

00:33:27,679 --> 00:33:31,440
they explain in detail why they combined

00:33:29,919 --> 00:33:32,159
kafka together with the machine learning

00:33:31,440 --> 00:33:34,080
frameworks

00:33:32,159 --> 00:33:35,919
so this is really um important to

00:33:34,080 --> 00:33:38,000
understand

00:33:35,919 --> 00:33:39,919
and the other question is do you have

00:33:38,000 --> 00:33:41,760
any benchmark of running a heavy model

00:33:39,919 --> 00:33:43,679
inference on milliseconds in that case

00:33:41,760 --> 00:33:45,039
you recommend going away from dedicated

00:33:43,679 --> 00:33:47,360
services

00:33:45,039 --> 00:33:49,279
i mean um this is really not much

00:33:47,360 --> 00:33:50,799
related to kafka now and actually

00:33:49,279 --> 00:33:53,039
i talked to a customer about such a

00:33:50,799 --> 00:33:54,880
problem yesterday so the first important

00:33:53,039 --> 00:33:57,200
thing is you need to really find out

00:33:54,880 --> 00:33:58,159
and where the blocker is is the blocker

00:33:57,200 --> 00:34:00,080
the um

00:33:58,159 --> 00:34:02,240
processing pipeline or is it the model

00:34:00,080 --> 00:34:03,760
itself because in many cases it's

00:34:02,240 --> 00:34:05,279
actually the model interference which

00:34:03,760 --> 00:34:07,440
takes much more time

00:34:05,279 --> 00:34:08,800
um the the the model processing from

00:34:07,440 --> 00:34:11,839
kafka perspective this

00:34:08,800 --> 00:34:12,480
typically is really end to end in 10 20

00:34:11,839 --> 00:34:14,639
00:34:12,480 --> 00:34:15,760
40 milliseconds but then if your model

00:34:14,639 --> 00:34:17,520
takes much longer

00:34:15,760 --> 00:34:18,879
or even the model pipeline including

00:34:17,520 --> 00:34:20,079
some other pre-processing

00:34:18,879 --> 00:34:22,000
then that's typically where the

00:34:20,079 --> 00:34:23,919
bottleneck is so really find out where

00:34:22,000 --> 00:34:26,320
your bottleneck is to fix that

00:34:23,919 --> 00:34:28,240
and based on that um we definitely have

00:34:26,320 --> 00:34:30,000
seen this challenge a lot also for these

00:34:28,240 --> 00:34:31,599
low latency use cases

00:34:30,000 --> 00:34:33,280
and here it's also important from the

00:34:31,599 --> 00:34:34,240
beginning to choose the right machine

00:34:33,280 --> 00:34:35,760
learning technology

00:34:34,240 --> 00:34:37,599
so this is not really a kafka question

00:34:35,760 --> 00:34:39,119
because kafka end-to-end latency it can

00:34:37,599 --> 00:34:40,000
be very low if your infrastructure is

00:34:39,119 --> 00:34:41,599
good enough

00:34:40,000 --> 00:34:43,040
but for the machine learning frameworks

00:34:41,599 --> 00:34:45,200
we have seen customers

00:34:43,040 --> 00:34:46,159
which have built great python models um

00:34:45,200 --> 00:34:48,720
but they didn't

00:34:46,159 --> 00:34:50,720
perform and execute well so here um we

00:34:48,720 --> 00:34:52,960
have seen customers um where they used

00:34:50,720 --> 00:34:54,639
frameworks like h2o for example

00:34:52,960 --> 00:34:56,320
this is one of my favorite open source

00:34:54,639 --> 00:34:57,440
frameworks for for low latency

00:34:56,320 --> 00:34:59,280
requirements

00:34:57,440 --> 00:35:00,800
because you can do the model training

00:34:59,280 --> 00:35:03,119
with python apis

00:35:00,800 --> 00:35:04,880
but the generated code is java code so

00:35:03,119 --> 00:35:07,599
it's compiled bytecode

00:35:04,880 --> 00:35:08,800
and then um the the model infuriance can

00:35:07,599 --> 00:35:10,720
really be in

00:35:08,800 --> 00:35:12,640
less than a millisecond so this doesn't

00:35:10,720 --> 00:35:14,640
really have any latency

00:35:12,640 --> 00:35:16,400
and this really simply depends on your

00:35:14,640 --> 00:35:17,760
use case we have seen use cases for

00:35:16,400 --> 00:35:19,839
example in manufacturing in the

00:35:17,760 --> 00:35:21,440
production lines assembly lines

00:35:19,839 --> 00:35:24,000
where people did things like image

00:35:21,440 --> 00:35:26,160
recognition or things like um

00:35:24,000 --> 00:35:27,280
on quality assurance with deep learning

00:35:26,160 --> 00:35:29,280
and there really it's about

00:35:27,280 --> 00:35:31,680
milliseconds and there they use h2o

00:35:29,280 --> 00:35:33,119
because that can be deployed at the edge

00:35:31,680 --> 00:35:34,720
while you still can do the model

00:35:33,119 --> 00:35:35,200
training in the cloud or in a data

00:35:34,720 --> 00:35:36,480
center

00:35:35,200 --> 00:35:38,240
and this is the great thing about the

00:35:36,480 --> 00:35:41,040
separation between model training and

00:35:38,240 --> 00:35:41,040
model scoring

00:35:41,839 --> 00:35:45,520
okay um with that i think i covered the

00:35:44,640 --> 00:35:46,960
questions so

00:35:45,520 --> 00:35:48,400
thank you a lot for watching this and

00:35:46,960 --> 00:35:50,079
again feel free to connect to me on

00:35:48,400 --> 00:35:51,760
linkedin to stay in touch

00:35:50,079 --> 00:35:59,839
thanks for watching and have a great

00:35:51,760 --> 00:35:59,839
apache home goodbye

00:36:28,079 --> 00:36:30,160

YouTube URL: https://www.youtube.com/watch?v=ELfz7Qpe9IY


