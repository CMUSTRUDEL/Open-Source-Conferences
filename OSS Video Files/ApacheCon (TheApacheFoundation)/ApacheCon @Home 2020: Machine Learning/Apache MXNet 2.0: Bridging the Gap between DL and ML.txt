Title: Apache MXNet 2.0: Bridging the Gap between DL and ML
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	Apache MXNet 2.0: Bridging the Gap between DL and ML
Sheng Zha

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Deep learning community has largely evolved independently from the prior community of data science and machine learning community in NumPy. While most deep learning frameworks now provide NumPy-like math and array library, they differ in the definition of the operations which creates a steeper learning curve of deep learning for machine learning practitioners and data scientists. This creates a chasm not only in the skillsets of the two different communities, but also hinders the exchange of knowledge. The next major version, 2.0, of Apache MXNet (incubating) seeks to bridge the fragmented deep learning and machine learning ecosystem. It provides NumPy-compatible programming experiences and simple enhancements to NumPy for deep learning with the new Gluon 2.0 interface. The NumPy-compatible array API also brings the advances in GPU acceleration, auto-differentiation, and high-performance one-click deployment to the NumPy ecosystem.

Sheng Zha is an Applied Scientist at Amazon AI. Heâ€™s also a committer and PPMC member of Apache MXNet (Incubating), steering committee member of Linux AI Foundation ONNX, and maintainer of the GluonNLP project. In his research, Sheng focuses on the intersection between deep learning-based natural language processing and computing systems, with the aim of enabling learning from large-scale language data and making it accessible.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,480 --> 00:00:29,359
so

00:00:25,039 --> 00:00:33,120
uh hi i'm sean i'm a member of

00:00:29,359 --> 00:00:34,399
the apache mxnet ppmc and also a senior

00:00:33,120 --> 00:00:37,040
applied scientist at

00:00:34,399 --> 00:00:38,399
amazon ai i'm really glad to be

00:00:37,040 --> 00:00:42,000
presenting at my first

00:00:38,399 --> 00:00:45,840
apache call in my favorite track about

00:00:42,000 --> 00:00:49,039
mxnet so in today's talk

00:00:45,840 --> 00:00:50,160
i'll be first sharing the state of our

00:00:49,039 --> 00:00:52,480
community

00:00:50,160 --> 00:00:54,399
and then we'll be looking at the current

00:00:52,480 --> 00:00:57,360
status in ai frameworks

00:00:54,399 --> 00:00:58,000
and examine the fragmentation problem

00:00:57,360 --> 00:01:01,840
afterwards

00:00:58,000 --> 00:01:04,000
i'll be introducing how mxn 2.0

00:01:01,840 --> 00:01:06,000
is uh helping to bridge the gap that's

00:01:04,000 --> 00:01:08,240
caused by the fragmentation

00:01:06,000 --> 00:01:09,600
and finally i'll share an update on the

00:01:08,240 --> 00:01:12,880
ecosystem of

00:01:09,600 --> 00:01:16,159
mx network if we have time

00:01:12,880 --> 00:01:19,040
so um mxnet is a

00:01:16,159 --> 00:01:21,119
pretty large community we've had over

00:01:19,040 --> 00:01:24,560
900 contributors

00:01:21,119 --> 00:01:27,759
that helped on mxnet

00:01:24,560 --> 00:01:31,759
we have over 19k stars

00:01:27,759 --> 00:01:34,960
on github and almost 7000 forks

00:01:31,759 --> 00:01:38,159
um since joining the apache incubator

00:01:34,960 --> 00:01:42,720
we've progressed with almost

00:01:38,159 --> 00:01:45,680
6000 commits and it's a widely adopted

00:01:42,720 --> 00:01:48,320
deep learning framework by the industry

00:01:45,680 --> 00:01:51,840
and trusted by many companies

00:01:48,320 --> 00:01:54,960
so um here are some information how

00:01:51,840 --> 00:01:58,640
on how you can stay connected

00:01:54,960 --> 00:02:01,759
in mxnet so as usual you can

00:01:58,640 --> 00:02:04,399
join and subscribe to our dev list

00:02:01,759 --> 00:02:06,000
and you can also connect with us on the

00:02:04,399 --> 00:02:09,679
asf slack

00:02:06,000 --> 00:02:13,120
in the mxnet channel um there are also

00:02:09,679 --> 00:02:14,640
a good first issues that we identify for

00:02:13,120 --> 00:02:15,840
people who want to get started in the

00:02:14,640 --> 00:02:18,879
project

00:02:15,840 --> 00:02:22,560
and also we label our roadmap

00:02:18,879 --> 00:02:24,560
issues on github with the roadmap label

00:02:22,560 --> 00:02:26,720
so at the moment we're looking for

00:02:24,560 --> 00:02:27,760
improvement in class plus and the java

00:02:26,720 --> 00:02:31,599
bindings

00:02:27,760 --> 00:02:34,879
and also we want to integrate

00:02:31,599 --> 00:02:37,280
more closely with the tvm relay which

00:02:34,879 --> 00:02:38,800
tenshi talked about it in the morning

00:02:37,280 --> 00:02:41,920
and then finally there are

00:02:38,800 --> 00:02:43,040
um social media channels um to which you

00:02:41,920 --> 00:02:46,160
can subscribe to

00:02:43,040 --> 00:02:49,360
our news and announcements okay so

00:02:46,160 --> 00:02:52,319
now let's dive in to discuss about the

00:02:49,360 --> 00:02:56,000
ai framework fragmentation

00:02:52,319 --> 00:02:58,640
so here's a an ai open source landscape

00:02:56,000 --> 00:03:00,400
that um that's produced by linux ai

00:02:58,640 --> 00:03:02,879
foundation

00:03:00,400 --> 00:03:05,040
so ai is really among the fastest

00:03:02,879 --> 00:03:07,280
growing field that spreads into

00:03:05,040 --> 00:03:08,239
every industry and the demand for ai

00:03:07,280 --> 00:03:11,440
talents

00:03:08,239 --> 00:03:13,440
almost doubles every year since 2015.

00:03:11,440 --> 00:03:14,560
similarly there is an explosion in the

00:03:13,440 --> 00:03:18,080
diversity of

00:03:14,560 --> 00:03:21,040
ai tools that touches every aspect of

00:03:18,080 --> 00:03:23,200
ai and data science in data processing

00:03:21,040 --> 00:03:25,440
analysis and modeling

00:03:23,200 --> 00:03:27,200
and because of the high practical value

00:03:25,440 --> 00:03:29,360
and even higher potential

00:03:27,200 --> 00:03:31,040
the industry invests a lot in the ai

00:03:29,360 --> 00:03:33,280
tooling so

00:03:31,040 --> 00:03:36,959
in this picture you may recognize um the

00:03:33,280 --> 00:03:39,599
the logos of many popular ai tools here

00:03:36,959 --> 00:03:40,959
um while impressive there's a problem in

00:03:39,599 --> 00:03:43,599
this picture

00:03:40,959 --> 00:03:45,360
it's the fragmentation in the ai open

00:03:43,599 --> 00:03:48,480
source softwares

00:03:45,360 --> 00:03:49,280
most notably the deep learning

00:03:48,480 --> 00:03:51,519
software's

00:03:49,280 --> 00:03:53,680
evolved to become a whole independent

00:03:51,519 --> 00:03:54,319
group from earlier machine learning

00:03:53,680 --> 00:03:56,480
tools

00:03:54,319 --> 00:03:58,799
despite that deep learning is just a new

00:03:56,480 --> 00:04:01,680
way of modeling in machine learning

00:03:58,799 --> 00:04:02,959
so let's see why this fragmentation is a

00:04:01,680 --> 00:04:06,799
problem

00:04:02,959 --> 00:04:08,560
so first what do i mean by fragmentation

00:04:06,799 --> 00:04:10,959
in the context of machine learning and

00:04:08,560 --> 00:04:13,040
deep learning open source software

00:04:10,959 --> 00:04:14,560
i'm referring to the lack of

00:04:13,040 --> 00:04:17,759
interoperability

00:04:14,560 --> 00:04:20,400
and lack of common interface design

00:04:17,759 --> 00:04:23,040
so next i'll try to convince you that

00:04:20,400 --> 00:04:26,639
this problem is really costly

00:04:23,040 --> 00:04:29,919
so why is it costly let's take a look at

00:04:26,639 --> 00:04:32,080
its effect on three groups of people

00:04:29,919 --> 00:04:34,560
that participate in the open source

00:04:32,080 --> 00:04:38,080
software

00:04:34,560 --> 00:04:40,240
so first to the users

00:04:38,080 --> 00:04:41,680
the lack of interoperability and common

00:04:40,240 --> 00:04:45,120
design among frameworks

00:04:41,680 --> 00:04:49,280
creates a lock-in in two ways

00:04:45,120 --> 00:04:52,320
first is the learning curve so

00:04:49,280 --> 00:04:55,440
the user needs to choose a tool to learn

00:04:52,320 --> 00:04:58,400
and that learning takes time

00:04:55,440 --> 00:05:00,320
when the user needs to switch from one

00:04:58,400 --> 00:05:02,320
framework to another

00:05:00,320 --> 00:05:04,400
the user would have to relearn so the

00:05:02,320 --> 00:05:08,800
time cost is locked into

00:05:04,400 --> 00:05:11,759
the the tool of choice the second

00:05:08,800 --> 00:05:13,280
part is that um whatever user code base

00:05:11,759 --> 00:05:16,240
the user creates

00:05:13,280 --> 00:05:17,600
it's locked in that tool of choice too

00:05:16,240 --> 00:05:20,800
that's um because of

00:05:17,600 --> 00:05:22,320
lack of common interface design the code

00:05:20,800 --> 00:05:24,080
cannot be transferred

00:05:22,320 --> 00:05:26,160
to another framework without being

00:05:24,080 --> 00:05:29,360
migrated to it manually

00:05:26,160 --> 00:05:33,360
so this login makes it hard to benefit

00:05:29,360 --> 00:05:33,360
from the strings of different frameworks

00:05:33,520 --> 00:05:39,280
second to the framework developers

00:05:36,880 --> 00:05:40,080
the lack of interoperability creates the

00:05:39,280 --> 00:05:42,800
need to

00:05:40,080 --> 00:05:44,880
develop and maintain independent stacks

00:05:42,800 --> 00:05:48,400
in each of the framework

00:05:44,880 --> 00:05:49,840
so because of the lack of

00:05:48,400 --> 00:05:52,080
interoperability

00:05:49,840 --> 00:05:53,440
the frameworks are essentially competing

00:05:52,080 --> 00:05:56,960
with each other

00:05:53,440 --> 00:06:00,000
and the user would have to choose among

00:05:56,960 --> 00:06:02,240
the many choices for just one of them

00:06:00,000 --> 00:06:04,400
so this would force the developers and

00:06:02,240 --> 00:06:05,919
contributor resources to be focused down

00:06:04,400 --> 00:06:09,280
on

00:06:05,919 --> 00:06:12,560
the shortest plank so to speak in this

00:06:09,280 --> 00:06:14,400
barrel of framework instead of focusing

00:06:12,560 --> 00:06:16,800
on making the needed progress for the

00:06:14,400 --> 00:06:16,800
field

00:06:17,520 --> 00:06:24,479
so the third party developers would be

00:06:21,680 --> 00:06:26,319
developing hardware and runtime or other

00:06:24,479 --> 00:06:29,199
third-party extensions for

00:06:26,319 --> 00:06:30,319
for example distributed training um they

00:06:29,199 --> 00:06:34,000
must invest in

00:06:30,319 --> 00:06:35,840
each of the framework in integration

00:06:34,000 --> 00:06:37,280
and this results in duplicated

00:06:35,840 --> 00:06:39,919
investments and

00:06:37,280 --> 00:06:41,520
higher engineering costs so this would

00:06:39,919 --> 00:06:44,720
become a barrier for

00:06:41,520 --> 00:06:47,840
innovation and hinders progress

00:06:44,720 --> 00:06:49,120
because of the the higher cost and often

00:06:47,840 --> 00:06:50,880
framework agnostic

00:06:49,120 --> 00:06:52,880
extensions would suffer from

00:06:50,880 --> 00:06:53,759
maintainability issues as a result of

00:06:52,880 --> 00:06:57,680
managing

00:06:53,759 --> 00:06:59,680
complex dependencies so how did

00:06:57,680 --> 00:07:01,280
the frame fragmentation happen in the

00:06:59,680 --> 00:07:04,720
first place

00:07:01,280 --> 00:07:05,919
it comes down to the different design

00:07:04,720 --> 00:07:09,520
choices

00:07:05,919 --> 00:07:14,720
in the frameworks

00:07:09,520 --> 00:07:16,720
goals and so part of the

00:07:14,720 --> 00:07:19,759
so deep learning frameworks would

00:07:16,720 --> 00:07:22,560
usually focus on the array

00:07:19,759 --> 00:07:24,000
interface whereas in addition to array

00:07:22,560 --> 00:07:24,880
machine learning frameworks also

00:07:24,000 --> 00:07:28,000
provides

00:07:24,880 --> 00:07:31,680
data frames such as in the

00:07:28,000 --> 00:07:33,680
spark project and

00:07:31,680 --> 00:07:35,440
machine learning frameworks often value

00:07:33,680 --> 00:07:38,639
higher precision whereas

00:07:35,440 --> 00:07:39,840
in deep learning the precision is less

00:07:38,639 --> 00:07:43,440
of an issue

00:07:39,840 --> 00:07:46,560
but we focus more on speed so

00:07:43,440 --> 00:07:48,639
the that's why deep learning frameworks

00:07:46,560 --> 00:07:51,919
usually provide half precision

00:07:48,639 --> 00:07:54,960
um choices so

00:07:51,919 --> 00:07:57,280
um and also in machine learning

00:07:54,960 --> 00:07:58,240
frameworks they would optimize for scala

00:07:57,280 --> 00:08:00,240
programs

00:07:58,240 --> 00:08:01,840
which is not usually a focus of deep

00:08:00,240 --> 00:08:04,879
learning frameworks

00:08:01,840 --> 00:08:05,440
and also deep learning frameworks

00:08:04,879 --> 00:08:08,000
provide

00:08:05,440 --> 00:08:09,440
automatic differentiation whereas

00:08:08,000 --> 00:08:11,280
machine learning frameworks usually

00:08:09,440 --> 00:08:13,759
don't

00:08:11,280 --> 00:08:14,800
also um most of the time machine

00:08:13,759 --> 00:08:17,520
learning frameworks

00:08:14,800 --> 00:08:18,080
follows the imperative program paradigm

00:08:17,520 --> 00:08:19,599
whereas

00:08:18,080 --> 00:08:21,599
deep learning now provides both

00:08:19,599 --> 00:08:24,879
imperative and symbolic

00:08:21,599 --> 00:08:28,240
um optimization uh as

00:08:24,879 --> 00:08:28,720
as paradigms and acceleration is more of

00:08:28,240 --> 00:08:32,399
a

00:08:28,720 --> 00:08:35,599
focus uh for deep learning on gpus and

00:08:32,399 --> 00:08:40,719
accelerators nowadays um so

00:08:35,599 --> 00:08:44,320
um i'll be focusing on the um array

00:08:40,719 --> 00:08:46,800
uh api the automatic differentiation

00:08:44,320 --> 00:08:47,760
and the acceleration so that i can show

00:08:46,800 --> 00:08:51,600
you the key

00:08:47,760 --> 00:08:54,959
ingredients of deep learning frameworks

00:08:51,600 --> 00:08:58,800
so first on the array api

00:08:54,959 --> 00:08:59,120
the modern array library is popularized

00:08:58,800 --> 00:09:02,320
by

00:08:59,120 --> 00:09:04,959
numpy starting 15 years ago

00:09:02,320 --> 00:09:05,920
it's a combination of data structure as

00:09:04,959 --> 00:09:09,040
well as

00:09:05,920 --> 00:09:09,760
the manipulation methods the data

00:09:09,040 --> 00:09:12,320
structure

00:09:09,760 --> 00:09:12,800
captures the data the data type the

00:09:12,320 --> 00:09:16,800
shape

00:09:12,800 --> 00:09:20,080
and also in different layouts we would

00:09:16,800 --> 00:09:23,200
describe them as strides and

00:09:20,080 --> 00:09:25,279
there's also um ways of

00:09:23,200 --> 00:09:26,560
indexing the array to create a view and

00:09:25,279 --> 00:09:29,839
copy

00:09:26,560 --> 00:09:32,880
and the there is also vectorized

00:09:29,839 --> 00:09:35,200
arithmetics broadcasting as well as

00:09:32,880 --> 00:09:36,959
reduction by certain axes

00:09:35,200 --> 00:09:38,959
so for those who are already familiar

00:09:36,959 --> 00:09:40,959
with numpy

00:09:38,959 --> 00:09:42,640
related programs this example should

00:09:40,959 --> 00:09:45,600
look familiar

00:09:42,640 --> 00:09:46,640
so now that we talked about array let's

00:09:45,600 --> 00:09:50,320
talk about

00:09:46,640 --> 00:09:52,160
automatic differentiation first i'd like

00:09:50,320 --> 00:09:55,279
to share a quote from uh

00:09:52,160 --> 00:09:56,480
the 2018 touring awards laureate young

00:09:55,279 --> 00:09:58,560
lacun

00:09:56,480 --> 00:10:00,800
um so he's saying people are now

00:09:58,560 --> 00:10:03,200
building a new kind of software

00:10:00,800 --> 00:10:04,079
by assembling networks of parameterized

00:10:03,200 --> 00:10:06,399
functional

00:10:04,079 --> 00:10:08,640
blocks and by training them from

00:10:06,399 --> 00:10:10,480
examples using some form of gradient

00:10:08,640 --> 00:10:12,720
based optimization

00:10:10,480 --> 00:10:15,200
so what he's referring to is

00:10:12,720 --> 00:10:19,040
differentiable programming or sometimes

00:10:15,200 --> 00:10:21,120
some people call it software 2.0

00:10:19,040 --> 00:10:22,880
this is enabled by automatic

00:10:21,120 --> 00:10:27,360
differentiation

00:10:22,880 --> 00:10:27,360
so what is ad

00:10:27,519 --> 00:10:33,200
so in the old way of programming

00:10:31,040 --> 00:10:34,240
i believe everyone is familiar with that

00:10:33,200 --> 00:10:37,760
and we we all do

00:10:34,240 --> 00:10:40,480
plenty of it um so uh

00:10:37,760 --> 00:10:41,120
here the programmers would hard code the

00:10:40,480 --> 00:10:43,680
function

00:10:41,120 --> 00:10:44,560
with the goal of generating desirable

00:10:43,680 --> 00:10:47,680
output

00:10:44,560 --> 00:10:50,640
given the input data

00:10:47,680 --> 00:10:52,079
in the differentiable programming

00:10:50,640 --> 00:10:54,079
however

00:10:52,079 --> 00:10:55,680
we would be talking about programming

00:10:54,079 --> 00:10:58,000
with data

00:10:55,680 --> 00:10:58,959
so now suppose we want to have a

00:10:58,000 --> 00:11:03,120
computer tell

00:10:58,959 --> 00:11:06,320
whether an image depicts a cat or a dog

00:11:03,120 --> 00:11:08,480
so more or more generally it's given

00:11:06,320 --> 00:11:10,399
some input data we want to estimate the

00:11:08,480 --> 00:11:12,839
output

00:11:10,399 --> 00:11:14,079
so often the logic would be quite

00:11:12,839 --> 00:11:16,720
complicated

00:11:14,079 --> 00:11:18,240
it might be vague but we have plenty of

00:11:16,720 --> 00:11:20,640
examples

00:11:18,240 --> 00:11:23,120
on the other hand as a programmer it's

00:11:20,640 --> 00:11:27,040
almost impossible to write code

00:11:23,120 --> 00:11:29,920
but we can train an estimator to do that

00:11:27,040 --> 00:11:31,519
so in order to do it we feed the image

00:11:29,920 --> 00:11:34,800
data of the kitty

00:11:31,519 --> 00:11:38,240
into a network that is uh parameterized

00:11:34,800 --> 00:11:40,240
by the weights and the network

00:11:38,240 --> 00:11:41,440
would generate a prediction through a

00:11:40,240 --> 00:11:44,640
weighted sum of

00:11:41,440 --> 00:11:48,560
the input data and a series of

00:11:44,640 --> 00:11:49,680
mass operations it produces a score of

00:11:48,560 --> 00:11:52,639
probability

00:11:49,680 --> 00:11:54,800
that the image represents a cat or that

00:11:52,639 --> 00:11:57,360
of a dog

00:11:54,800 --> 00:11:58,160
so what automatic differentiation

00:11:57,360 --> 00:12:01,440
enables

00:11:58,160 --> 00:12:03,360
is that um um there there would be an

00:12:01,440 --> 00:12:04,240
easy way of describing the

00:12:03,360 --> 00:12:07,839
differentiable

00:12:04,240 --> 00:12:11,440
function as the advanced network um

00:12:07,839 --> 00:12:13,920
and uh an easy way of producing

00:12:11,440 --> 00:12:14,720
the prediction from it so since the

00:12:13,920 --> 00:12:16,639
program

00:12:14,720 --> 00:12:19,279
already knows everything about the

00:12:16,639 --> 00:12:21,360
function once we know the arrow signal

00:12:19,279 --> 00:12:24,800
from the output 10 label

00:12:21,360 --> 00:12:27,839
we can generate the the arrow signal

00:12:24,800 --> 00:12:31,040
and the the gradients in which

00:12:27,839 --> 00:12:33,360
the weights should be adjusted so that

00:12:31,040 --> 00:12:36,240
it could produce a higher score for for

00:12:33,360 --> 00:12:36,240
cat in this case

00:12:36,959 --> 00:12:41,279
so um now that we looked at um array

00:12:40,160 --> 00:12:42,320
library and the automatic

00:12:41,279 --> 00:12:44,560
differentiation

00:12:42,320 --> 00:12:46,240
let's also look at acceleration on

00:12:44,560 --> 00:12:49,040
modern hardware

00:12:46,240 --> 00:12:49,440
so the driving factors behind this need

00:12:49,040 --> 00:12:52,639
are

00:12:49,440 --> 00:12:53,440
really the three parts we want larger

00:12:52,639 --> 00:12:56,079
networks

00:12:53,440 --> 00:12:56,959
the function f we want to train on more

00:12:56,079 --> 00:12:59,760
data

00:12:56,959 --> 00:13:01,600
and we want to iterate faster so that

00:12:59,760 --> 00:13:02,800
scientist time can be better utilized

00:13:01,600 --> 00:13:05,680
for science

00:13:02,800 --> 00:13:07,440
and not for sword fights on office

00:13:05,680 --> 00:13:10,240
shares

00:13:07,440 --> 00:13:11,760
so first on larger models here i'm

00:13:10,240 --> 00:13:14,399
sharing a plot

00:13:11,760 --> 00:13:16,160
for the amount of compute of well-known

00:13:14,399 --> 00:13:18,160
deep learning models

00:13:16,160 --> 00:13:19,200
you may recognize some of the recent

00:13:18,160 --> 00:13:21,680
feats in

00:13:19,200 --> 00:13:23,600
deep learning such as alphago and neural

00:13:21,680 --> 00:13:27,519
neural machine translation

00:13:23,600 --> 00:13:30,880
so the plot x-axis is a timeline

00:13:27,519 --> 00:13:32,120
and the y-axis is the logarithmic plot

00:13:30,880 --> 00:13:35,279
of

00:13:32,120 --> 00:13:36,880
a unit that represents compute it's the

00:13:35,279 --> 00:13:40,000
petaflops per second

00:13:36,880 --> 00:13:41,519
times the number of days um so you can

00:13:40,000 --> 00:13:44,160
see a linear growth

00:13:41,519 --> 00:13:45,440
in this plot and because if it's a

00:13:44,160 --> 00:13:47,199
logarithmic plot

00:13:45,440 --> 00:13:48,880
we know that it's it's been growing

00:13:47,199 --> 00:13:52,480
exponentially since um

00:13:48,880 --> 00:13:55,680
the 2012 deep learning boom

00:13:52,480 --> 00:13:57,920
um and uh on more data so

00:13:55,680 --> 00:13:59,199
let's take a look at the amount of data

00:13:57,920 --> 00:14:02,560
human generates

00:13:59,199 --> 00:14:05,680
every minute so this is uh domo's

00:14:02,560 --> 00:14:06,720
data never sleeps infographics um you

00:14:05,680 --> 00:14:09,040
can see from

00:14:06,720 --> 00:14:10,720
on the left that the internet population

00:14:09,040 --> 00:14:13,839
has grown from 3 billion

00:14:10,720 --> 00:14:17,760
in 2014 to 4.57 billion

00:14:13,839 --> 00:14:19,040
in 2017. every minute there is a value

00:14:17,760 --> 00:14:21,680
of data that's

00:14:19,040 --> 00:14:23,360
generated across all the different apps

00:14:21,680 --> 00:14:26,959
and services on the right

00:14:23,360 --> 00:14:30,320
for example for youtube on the um

00:14:26,959 --> 00:14:33,279
top right corner there is uh 20 days

00:14:30,320 --> 00:14:33,760
worth of video generated every minute

00:14:33,279 --> 00:14:37,440
and

00:14:33,760 --> 00:14:40,399
on bottom left there is a amazon where

00:14:37,440 --> 00:14:42,480
over 6000 packages are shipped so

00:14:40,399 --> 00:14:45,760
nowadays assume that um

00:14:42,480 --> 00:14:49,040
many of those are probably toilet papers

00:14:45,760 --> 00:14:51,920
um so um

00:14:49,040 --> 00:14:54,000
given the the need for a larger model

00:14:51,920 --> 00:14:56,800
and

00:14:54,000 --> 00:14:59,760
more data processing more data the

00:14:56,800 --> 00:15:02,720
hardware is also innovating very fast

00:14:59,760 --> 00:15:04,880
so on the left it's a plot for the

00:15:02,720 --> 00:15:08,399
compute power of gpus

00:15:04,880 --> 00:15:10,480
the top-of-the-line gpus over the years

00:15:08,399 --> 00:15:12,880
the gpu performance has increased nine

00:15:10,480 --> 00:15:15,279
volts since 2016.

00:15:12,880 --> 00:15:16,079
and on the right it's a comparison of

00:15:15,279 --> 00:15:18,480
the two

00:15:16,079 --> 00:15:20,079
most recent generations of tpu from

00:15:18,480 --> 00:15:23,360
google

00:15:20,079 --> 00:15:27,680
so on average across these different

00:15:23,360 --> 00:15:31,360
models they improve in performance by

00:15:27,680 --> 00:15:31,839
2.7 times so deep learning frameworks

00:15:31,360 --> 00:15:34,079
need to

00:15:31,839 --> 00:15:38,320
not only keep up with the fast pace in

00:15:34,079 --> 00:15:38,320
every aspect but also to lead the future

00:15:39,120 --> 00:15:43,360
um so now that we talked about the

00:15:42,000 --> 00:15:45,759
fragmentation where it

00:15:43,360 --> 00:15:47,279
came from uh let's talk about the

00:15:45,759 --> 00:15:49,600
solution to it

00:15:47,279 --> 00:15:51,199
here i introduced two standardization

00:15:49,600 --> 00:15:52,399
efforts in machine learning and deep

00:15:51,199 --> 00:15:55,519
learning that i modeled

00:15:52,399 --> 00:15:58,480
i'm really honored to participate in um

00:15:55,519 --> 00:16:00,240
so the first one is uh python data api

00:15:58,480 --> 00:16:01,920
consortium

00:16:00,240 --> 00:16:03,680
the goal of the consortium is to

00:16:01,920 --> 00:16:06,839
standard standardized

00:16:03,680 --> 00:16:08,240
array and data frame api to address the

00:16:06,839 --> 00:16:11,680
fragmentation

00:16:08,240 --> 00:16:13,440
of libraries that offer them we assemble

00:16:11,680 --> 00:16:15,600
a consortium of people from

00:16:13,440 --> 00:16:16,800
interested companies and key community

00:16:15,600 --> 00:16:20,639
contributors

00:16:16,800 --> 00:16:22,959
to examine the design choices in in them

00:16:20,639 --> 00:16:23,839
and to propose the standards that are

00:16:22,959 --> 00:16:27,519
suitable for

00:16:23,839 --> 00:16:28,480
modern day tools the proposed standards

00:16:27,519 --> 00:16:31,040
are shared

00:16:28,480 --> 00:16:32,560
to the public as a request for comments

00:16:31,040 --> 00:16:35,120
or ifcs

00:16:32,560 --> 00:16:36,000
that library maintainers can provide

00:16:35,120 --> 00:16:39,040
feedback early

00:16:36,000 --> 00:16:41,759
on and adopt afterwards

00:16:39,040 --> 00:16:45,440
and the community participates in the

00:16:41,759 --> 00:16:45,440
reviews throughout this process

00:16:45,519 --> 00:16:49,519
the other effort is open neural network

00:16:48,720 --> 00:16:52,720
exchange

00:16:49,519 --> 00:16:56,160
or onyx so

00:16:52,720 --> 00:16:59,440
onyx is an effort that was

00:16:56,160 --> 00:17:02,079
started by facebook and microsoft

00:16:59,440 --> 00:17:03,519
it facilitates the exchange of deep

00:17:02,079 --> 00:17:06,319
learning models

00:17:03,519 --> 00:17:06,640
and it does so by providing a definition

00:17:06,319 --> 00:17:09,760
of

00:17:06,640 --> 00:17:11,839
extensible computation graph model

00:17:09,760 --> 00:17:15,120
as well as definitions of built-in

00:17:11,839 --> 00:17:18,240
operators and standard data types

00:17:15,120 --> 00:17:21,919
so now let's move on to

00:17:18,240 --> 00:17:24,000
talk about mxnet um we've um

00:17:21,919 --> 00:17:25,120
covered what what the fragmentation

00:17:24,000 --> 00:17:26,880
problem is uh

00:17:25,120 --> 00:17:28,319
as well as the the knees in deep

00:17:26,880 --> 00:17:31,440
learning so

00:17:28,319 --> 00:17:34,799
uh we're prepared for this so um

00:17:31,440 --> 00:17:36,880
mxnet is a truly open source

00:17:34,799 --> 00:17:38,240
deep learning framework developed in the

00:17:36,880 --> 00:17:40,160
apache way

00:17:38,240 --> 00:17:41,280
uh it's a community of deep learning

00:17:40,160 --> 00:17:43,520
enthusiasts

00:17:41,280 --> 00:17:45,280
with the goal of democratizing ai and

00:17:43,520 --> 00:17:46,799
making sure that it's accessible to

00:17:45,280 --> 00:17:48,400
everyone

00:17:46,799 --> 00:17:52,400
it's designed to be flexible and

00:17:48,400 --> 00:17:55,679
efficient and ready for production

00:17:52,400 --> 00:17:57,520
also as i described before

00:17:55,679 --> 00:17:59,120
there are the two standards that are

00:17:57,520 --> 00:18:03,280
both adopted by

00:17:59,120 --> 00:18:03,280
mxnet so

00:18:03,520 --> 00:18:09,360
in 2.0 we aim to provide the bridge

00:18:06,880 --> 00:18:10,240
between numpy based machine learning

00:18:09,360 --> 00:18:12,640
tools

00:18:10,240 --> 00:18:14,240
and deep learning and the two most

00:18:12,640 --> 00:18:16,000
notable features are the numpy

00:18:14,240 --> 00:18:18,080
compatible programming as well as the

00:18:16,000 --> 00:18:20,400
gluon 2.0

00:18:18,080 --> 00:18:22,400
the numpy compatible programming

00:18:20,400 --> 00:18:25,280
provides the np module

00:18:22,400 --> 00:18:27,280
which is a numpy compatible array

00:18:25,280 --> 00:18:29,520
library with the enhancement of

00:18:27,280 --> 00:18:31,200
auto differentiation and gpu

00:18:29,520 --> 00:18:33,679
acceleration

00:18:31,200 --> 00:18:35,039
we also provide npx which is the neural

00:18:33,679 --> 00:18:38,880
network extension

00:18:35,039 --> 00:18:42,480
to the numpy compatible array api

00:18:38,880 --> 00:18:45,200
for glon 2.0 we provide a simple

00:18:42,480 --> 00:18:48,240
high-level programming model that can

00:18:45,200 --> 00:18:50,320
optimize non-time enhanced deep learning

00:18:48,240 --> 00:18:52,000
overall the goal is to make deep

00:18:50,320 --> 00:18:56,640
learning and machine learning

00:18:52,000 --> 00:19:00,240
fast and also flexible

00:18:56,640 --> 00:19:02,640
so here's an example of such combination

00:19:00,240 --> 00:19:04,880
um we write the neural network from the

00:19:02,640 --> 00:19:06,080
cat and dog example in just a few lines

00:19:04,880 --> 00:19:09,600
of code

00:19:06,080 --> 00:19:12,720
so overall the the class structure

00:19:09,600 --> 00:19:16,880
is uh what it's like uh for programming

00:19:12,720 --> 00:19:19,760
in gluon 2.0 we can declare parameters

00:19:16,880 --> 00:19:21,760
for for weights and biases for that

00:19:19,760 --> 00:19:24,559
linear combination

00:19:21,760 --> 00:19:26,000
and in the forward function we define

00:19:24,559 --> 00:19:29,120
the compute

00:19:26,000 --> 00:19:32,640
where we can use the numpy interface

00:19:29,120 --> 00:19:35,919
for the arithmetic

00:19:32,640 --> 00:19:36,960
operations and we also can use the

00:19:35,919 --> 00:19:40,000
neural network

00:19:36,960 --> 00:19:43,120
extension to to numpy for example for

00:19:40,000 --> 00:19:46,240
the softmax activation so

00:19:43,120 --> 00:19:46,880
once we have the the network uh we can

00:19:46,240 --> 00:19:51,039
create

00:19:46,880 --> 00:19:54,080
an instance of that and feed the data of

00:19:51,039 --> 00:19:57,520
a kitty through it

00:19:54,080 --> 00:20:00,080
so um in gluon there is the concept

00:19:57,520 --> 00:20:02,320
called hybridization which is our

00:20:00,080 --> 00:20:06,400
just-in-time compilation

00:20:02,320 --> 00:20:09,120
it can optimize and export numpy and

00:20:06,400 --> 00:20:10,480
neural network models to different

00:20:09,120 --> 00:20:14,080
bindings

00:20:10,480 --> 00:20:17,360
of programming languages and we can also

00:20:14,080 --> 00:20:20,559
deploy them through tvm tensor rt

00:20:17,360 --> 00:20:23,760
or move refer to openvmo

00:20:20,559 --> 00:20:27,120
and also we can exchange the model with

00:20:23,760 --> 00:20:30,000
even wider range of tools through onyx

00:20:27,120 --> 00:20:31,679
and once the python data api consortium

00:20:30,000 --> 00:20:34,880
standard

00:20:31,679 --> 00:20:37,360
is widely adopted it will be able to

00:20:34,880 --> 00:20:40,240
interoperate with even more tools that

00:20:37,360 --> 00:20:40,240
are participating

00:20:40,720 --> 00:20:45,360
so this makes mxnet really portable

00:20:45,520 --> 00:20:49,600
and in 2.0 we also make it easier to

00:20:48,720 --> 00:20:51,520
customize

00:20:49,600 --> 00:20:54,159
which is an important need in the

00:20:51,520 --> 00:20:55,840
industry here i'll focus on the first

00:20:54,159 --> 00:20:58,960
two items of

00:20:55,840 --> 00:21:02,000
custom operators so

00:20:58,960 --> 00:21:04,799
for c-plus plus operators we

00:21:02,000 --> 00:21:05,360
used to require that these operators are

00:21:04,799 --> 00:21:08,480
written

00:21:05,360 --> 00:21:10,880
with our interface and it needs to be

00:21:08,480 --> 00:21:13,919
compiled and linked

00:21:10,880 --> 00:21:14,480
to mxnet directly so this creates a need

00:21:13,919 --> 00:21:16,640
for

00:21:14,480 --> 00:21:18,720
maintaining a fork if anyone wants to

00:21:16,640 --> 00:21:21,760
add custom operations

00:21:18,720 --> 00:21:22,720
so in order to make it flexible we now

00:21:21,760 --> 00:21:25,360
offer

00:21:22,720 --> 00:21:26,880
a custom operator library that can be

00:21:25,360 --> 00:21:29,919
loaded at runtime

00:21:26,880 --> 00:21:32,880
that can be registered into our operator

00:21:29,919 --> 00:21:35,840
registry in the api compatible way

00:21:32,880 --> 00:21:38,159
so this removes the need to maintain the

00:21:35,840 --> 00:21:38,159
fork

00:21:38,480 --> 00:21:43,120
also we now support defining operators

00:21:41,440 --> 00:21:46,000
using tbm

00:21:43,120 --> 00:21:46,880
which then she talked about in this

00:21:46,000 --> 00:21:49,679
morning

00:21:46,880 --> 00:21:51,440
so here i have a comparison of the same

00:21:49,679 --> 00:21:54,720
operator that's written

00:21:51,440 --> 00:21:55,520
with tvm in python and also the hand

00:21:54,720 --> 00:21:58,320
optimized

00:21:55,520 --> 00:22:00,480
operator that's written in c plus plus

00:21:58,320 --> 00:22:02,559
so

00:22:00,480 --> 00:22:04,159
as you can see in in terms of the

00:22:02,559 --> 00:22:07,600
lengths it really

00:22:04,159 --> 00:22:10,480
improves the development efficiency

00:22:07,600 --> 00:22:12,960
and also dvm offers many ways of

00:22:10,480 --> 00:22:15,679
automatically optimizing for

00:22:12,960 --> 00:22:17,360
this compute which makes it uh really

00:22:15,679 --> 00:22:20,960
desirable for

00:22:17,360 --> 00:22:20,960
um development

00:22:21,600 --> 00:22:27,280
um so in 2.0 we also enhanced

00:22:24,640 --> 00:22:28,000
a lot in in terms of performance as well

00:22:27,280 --> 00:22:31,760
as

00:22:28,000 --> 00:22:34,640
adding more hardware support so um

00:22:31,760 --> 00:22:35,280
we now support cuda graph for scheduling

00:22:34,640 --> 00:22:38,159
so that

00:22:35,280 --> 00:22:39,360
it removes one of the the largest

00:22:38,159 --> 00:22:41,440
overhead

00:22:39,360 --> 00:22:42,799
in scheduling from the deep learning

00:22:41,440 --> 00:22:45,600
framework

00:22:42,799 --> 00:22:46,799
we also support runtime compilation with

00:22:45,600 --> 00:22:49,840
mvrtc

00:22:46,799 --> 00:22:51,280
which enables the runtime fusion of gpu

00:22:49,840 --> 00:22:53,440
operators

00:22:51,280 --> 00:22:54,799
so this would improve the speed as well

00:22:53,440 --> 00:22:57,600
as reduce

00:22:54,799 --> 00:22:58,880
the the memory consumption we also

00:22:57,600 --> 00:23:02,159
enhanced our dynamic

00:22:58,880 --> 00:23:03,360
graph execution so that graphs with a

00:23:02,159 --> 00:23:06,080
combination of

00:23:03,360 --> 00:23:07,360
static and dynamic parts it would be

00:23:06,080 --> 00:23:11,200
optimized

00:23:07,360 --> 00:23:12,720
as much as possible so for the static

00:23:11,200 --> 00:23:15,520
part it will be optimized

00:23:12,720 --> 00:23:16,799
in the exact same way as a static

00:23:15,520 --> 00:23:20,080
network

00:23:16,799 --> 00:23:23,600
and finally we enable automatic mixed

00:23:20,080 --> 00:23:24,559
precision which is uh automatically

00:23:23,600 --> 00:23:28,240
switching to

00:23:24,559 --> 00:23:30,480
uh fp16 for tensor core compute

00:23:28,240 --> 00:23:31,919
so this would improve the speed and

00:23:30,480 --> 00:23:33,280
since we offer it in the numpy

00:23:31,919 --> 00:23:37,039
compatible api

00:23:33,280 --> 00:23:39,520
it can be used for phenomena as well

00:23:37,039 --> 00:23:40,720
so here i share some of the performance

00:23:39,520 --> 00:23:44,400
highlights of

00:23:40,720 --> 00:23:47,600
mxnet um so mxnet

00:23:44,400 --> 00:23:51,520
here i'm sharing the highlights from

00:23:47,600 --> 00:23:53,520
malpur 0.7 which is released recently

00:23:51,520 --> 00:23:55,520
it's a standard benchmark in deep

00:23:53,520 --> 00:23:56,480
learning with very strict competition

00:23:55,520 --> 00:24:00,400
rules

00:23:56,480 --> 00:24:04,000
mxnet is used in 23 of the 52

00:24:00,400 --> 00:24:06,960
available on-premise submissions so

00:24:04,000 --> 00:24:07,360
for two of the tasks that are well known

00:24:06,960 --> 00:24:10,320
um

00:24:07,360 --> 00:24:12,559
here for image classification i imagenet

00:24:10,320 --> 00:24:17,360
with less than 50.

00:24:12,559 --> 00:24:20,240
nvidia submitted a record breaking

00:24:17,360 --> 00:24:21,039
submission which is uh 46 seconds with

00:24:20,240 --> 00:24:25,600
um

00:24:21,039 --> 00:24:29,200
uh 1840 a 100 gpus

00:24:25,600 --> 00:24:31,440
that's a lot of gpus and there's also a

00:24:29,200 --> 00:24:35,200
pretty good result from

00:24:31,440 --> 00:24:37,840
intel cpu where

00:24:35,200 --> 00:24:39,440
the the speed record was a little over

00:24:37,840 --> 00:24:42,640
18 minutes

00:24:39,440 --> 00:24:46,000
was just eight machines and uh

00:24:42,640 --> 00:24:49,760
on object detection um coco

00:24:46,000 --> 00:24:50,360
with ssd the speed record was 49 seconds

00:24:49,760 --> 00:24:54,640
with

00:24:50,360 --> 00:24:58,159
1024 a100 gpus

00:24:54,640 --> 00:25:02,880
so uh we also now integrate with

00:24:58,159 --> 00:25:05,840
one dnn which was called mkldnn before

00:25:02,880 --> 00:25:06,559
so it helped a lot on the cpu

00:25:05,840 --> 00:25:10,000
performance

00:25:06,559 --> 00:25:12,799
so here i'm sharing a

00:25:10,000 --> 00:25:13,919
bar chart for different models on the

00:25:12,799 --> 00:25:17,200
relative speed up

00:25:13,919 --> 00:25:20,320
of one dnn version versus the

00:25:17,200 --> 00:25:23,120
native uh version that we had before

00:25:20,320 --> 00:25:23,440
so overall we can see that it achieves

00:25:23,120 --> 00:25:26,480
an

00:25:23,440 --> 00:25:27,440
over five times uh improvement across

00:25:26,480 --> 00:25:30,960
the board

00:25:27,440 --> 00:25:34,320
and uh for mobilenet it achieved even

00:25:30,960 --> 00:25:37,679
higher speed up for over 40 times um so

00:25:34,320 --> 00:25:40,799
this really is um

00:25:37,679 --> 00:25:41,520
attributed to the optimization in one

00:25:40,799 --> 00:25:46,480
dnn

00:25:41,520 --> 00:25:46,480
for avx 512 optimization

00:25:47,279 --> 00:25:52,880
so let's also talk about the low cost

00:25:50,320 --> 00:25:55,120
which is important in machine learning

00:25:52,880 --> 00:25:58,080
so i'm sharing a global nlp

00:25:55,120 --> 00:25:59,360
birth real-time inference result the

00:25:58,080 --> 00:26:02,400
blog was posted

00:25:59,360 --> 00:26:04,559
uh just yesterday so um

00:26:02,400 --> 00:26:05,520
the model is bert which is uh the

00:26:04,559 --> 00:26:08,640
bi-directional

00:26:05,520 --> 00:26:09,279
embedding from transformers it's one of

00:26:08,640 --> 00:26:12,400
the

00:26:09,279 --> 00:26:13,600
state of the art transformer based

00:26:12,400 --> 00:26:17,520
models

00:26:13,600 --> 00:26:21,360
that first broke the human record for

00:26:17,520 --> 00:26:23,919
natural language understanding task so

00:26:21,360 --> 00:26:26,880
i'm sharing i'm going to share the

00:26:23,919 --> 00:26:28,480
results on the sentiment analysis task

00:26:26,880 --> 00:26:31,279
which is showing in the upper right

00:26:28,480 --> 00:26:33,840
corner so suppose that

00:26:31,279 --> 00:26:34,720
we're looking at a book review comment

00:26:33,840 --> 00:26:38,159
which is

00:26:34,720 --> 00:26:41,360
saying that it's great for insomniacs

00:26:38,159 --> 00:26:43,760
so this is not something that

00:26:41,360 --> 00:26:44,640
we would look for in the quality of a

00:26:43,760 --> 00:26:48,000
book

00:26:44,640 --> 00:26:53,120
but even though it says it's great

00:26:48,000 --> 00:26:56,000
um so it's a hard task so

00:26:53,120 --> 00:26:56,640
for the inference cost here i'm showing

00:26:56,000 --> 00:27:00,480
a

00:26:56,640 --> 00:27:03,440
bar chart for um the the birth

00:27:00,480 --> 00:27:05,440
sentiment classification task for

00:27:03,440 --> 00:27:08,480
different sequence lengths

00:27:05,440 --> 00:27:09,760
and each bar of different color

00:27:08,480 --> 00:27:14,080
represents

00:27:09,760 --> 00:27:14,960
the cost in dollars per 1 million

00:27:14,080 --> 00:27:18,240
requests

00:27:14,960 --> 00:27:21,440
on four different types of

00:27:18,240 --> 00:27:24,559
hardware instances ec2

00:27:21,440 --> 00:27:28,960
so um as we can see for

00:27:24,559 --> 00:27:32,080
the g4dn x large with the half precision

00:27:28,960 --> 00:27:34,799
inference we can achieve a million

00:27:32,080 --> 00:27:38,240
requests for just 20 cents

00:27:34,799 --> 00:27:40,159
which is you know a pretty

00:27:38,240 --> 00:27:42,000
nice cost reduction over the the

00:27:40,159 --> 00:27:44,399
previous results

00:27:42,000 --> 00:27:45,600
in comparison some of the publicly

00:27:44,399 --> 00:27:47,840
available nlp

00:27:45,600 --> 00:27:48,960
services such as hugging phase offers

00:27:47,840 --> 00:27:51,200
the

00:27:48,960 --> 00:27:52,159
same amount of influence at a monthly

00:27:51,200 --> 00:27:57,120
cost of

00:27:52,159 --> 00:27:57,120
200 so a thousand times

00:27:57,440 --> 00:28:03,600
um looks like i still have some time so

00:28:01,120 --> 00:28:05,840
last but not least let me introduce the

00:28:03,600 --> 00:28:08,559
ecosystem and the

00:28:05,840 --> 00:28:10,480
uh the different projects that support

00:28:08,559 --> 00:28:12,720
mxnet

00:28:10,480 --> 00:28:14,240
first one is auto glue on so autocoulomb

00:28:12,720 --> 00:28:16,640
is an automl tool that

00:28:14,240 --> 00:28:19,039
enables automatic machine learning with

00:28:16,640 --> 00:28:21,760
just three lines of code

00:28:19,039 --> 00:28:22,559
it automatically ensembles various

00:28:21,760 --> 00:28:24,399
models

00:28:22,559 --> 00:28:25,600
and it performs hyper parameter

00:28:24,399 --> 00:28:28,559
optimization

00:28:25,600 --> 00:28:30,320
to pick the best combination of models

00:28:28,559 --> 00:28:32,320
for the task at hand

00:28:30,320 --> 00:28:33,600
as shown in the table so in the

00:28:32,320 --> 00:28:36,640
comparison

00:28:33,600 --> 00:28:39,679
uh to other rtml tools on

00:28:36,640 --> 00:28:42,000
11 kaggle competitions

00:28:39,679 --> 00:28:42,960
otto guloan performed the best on seven

00:28:42,000 --> 00:28:46,399
of them

00:28:42,960 --> 00:28:46,399
waste the least amount of time

00:28:46,480 --> 00:28:49,760
um the other project i'd like to share

00:28:48,880 --> 00:28:51,919
is that

00:28:49,760 --> 00:28:52,960
it's a dive into deep learning it's a

00:28:51,919 --> 00:28:55,200
deep learning book

00:28:52,960 --> 00:28:56,880
with the perfect combination of

00:28:55,200 --> 00:29:00,000
knowledge as well as

00:28:56,880 --> 00:29:02,159
hands-on practice so it's written as

00:29:00,000 --> 00:29:04,559
jupiter notebooks to help build

00:29:02,159 --> 00:29:06,720
on the hands-on skills and the solid

00:29:04,559 --> 00:29:09,200
foundation in deep learning

00:29:06,720 --> 00:29:10,080
and it also offers a very active

00:29:09,200 --> 00:29:12,159
community of

00:29:10,080 --> 00:29:14,559
learners that help each other to create

00:29:12,159 --> 00:29:16,960
the best learning experience

00:29:14,559 --> 00:29:19,919
jason huang the ceo of nvidia also

00:29:16,960 --> 00:29:19,919
highly recommends it

00:29:20,640 --> 00:29:24,799
it's also used in more than 100

00:29:22,880 --> 00:29:27,200
universities worldwide

00:29:24,799 --> 00:29:28,960
as textbooks for teaching deep learning

00:29:27,200 --> 00:29:31,679
and machine learning

00:29:28,960 --> 00:29:34,240
and half of the top 30 universities use

00:29:31,679 --> 00:29:36,720
d2l

00:29:34,240 --> 00:29:37,279
in computer vision we have glow on cv

00:29:36,720 --> 00:29:39,520
which is

00:29:37,279 --> 00:29:40,640
a versatile deep learning for computer

00:29:39,520 --> 00:29:42,559
vision toolkit

00:29:40,640 --> 00:29:44,799
that provides training and deployment

00:29:42,559 --> 00:29:47,279
for a wide range of tasks

00:29:44,799 --> 00:29:48,480
they include image classification

00:29:47,279 --> 00:29:50,880
detection

00:29:48,480 --> 00:29:51,760
segmentation on the right action

00:29:50,880 --> 00:29:55,679
recognition

00:29:51,760 --> 00:30:00,240
and post estimation on videos

00:29:55,679 --> 00:30:02,880
so more recently it

00:30:00,240 --> 00:30:03,440
also offers the depth estimation as well

00:30:02,880 --> 00:30:05,600
as

00:30:03,440 --> 00:30:07,760
generative adversarial networks for

00:30:05,600 --> 00:30:11,440
various image generation tasks

00:30:07,760 --> 00:30:11,440
so all these faces are fake

00:30:11,919 --> 00:30:15,440
um another great computer vision toolkit

00:30:14,240 --> 00:30:17,760
i'd like to highlight is

00:30:15,440 --> 00:30:18,799
inside face it's um deep learning

00:30:17,760 --> 00:30:21,760
toolkit for face

00:30:18,799 --> 00:30:22,720
analysis it provides implementations and

00:30:21,760 --> 00:30:25,200
the pre-trained

00:30:22,720 --> 00:30:27,440
state-of-the-art models um the two

00:30:25,200 --> 00:30:27,760
killer features are the arc phase model

00:30:27,440 --> 00:30:31,120
for

00:30:27,760 --> 00:30:34,480
face recognition and the award-winning

00:30:31,120 --> 00:30:37,520
retina phase model for face localization

00:30:34,480 --> 00:30:39,520
so inside face is a famous and widely

00:30:37,520 --> 00:30:40,559
used toolkit in both academia and

00:30:39,520 --> 00:30:42,640
industry

00:30:40,559 --> 00:30:43,840
it aims to become the the center for

00:30:42,640 --> 00:30:47,120
innovation

00:30:43,840 --> 00:30:50,240
and uh deep face analysis

00:30:47,120 --> 00:30:51,360
now moving on to nlp we have glo iop

00:30:50,240 --> 00:30:53,679
which provides

00:30:51,360 --> 00:30:56,240
training and deployment for a wide

00:30:53,679 --> 00:30:59,200
variety of natural language tasks

00:30:56,240 --> 00:30:59,760
such as sentiment analysis natural

00:30:59,200 --> 00:31:03,440
language

00:30:59,760 --> 00:31:06,399
inference text generation translation

00:31:03,440 --> 00:31:08,000
and sequence labeling it offers over a

00:31:06,399 --> 00:31:10,640
thousand pre-trained models

00:31:08,000 --> 00:31:12,640
and as i shared before it offers the

00:31:10,640 --> 00:31:15,279
low-cost inference too

00:31:12,640 --> 00:31:16,880
in the next major version of global nlp

00:31:15,279 --> 00:31:18,960
we provide the numpy based

00:31:16,880 --> 00:31:19,919
implementation and also more backbone

00:31:18,960 --> 00:31:23,039
networks

00:31:19,919 --> 00:31:26,480
and the data processing tools for faster

00:31:23,039 --> 00:31:29,120
iteration so gloapy focuses on the

00:31:26,480 --> 00:31:31,760
industry and powers many of the

00:31:29,120 --> 00:31:33,440
nlp services in in the best as well as

00:31:31,760 --> 00:31:36,559
in alexa's

00:31:33,440 --> 00:31:40,559
natural language understanding

00:31:36,559 --> 00:31:43,600
um another great nlp toolkit is sockeye

00:31:40,559 --> 00:31:47,200
it's a sequence to sequence toolkit that

00:31:43,600 --> 00:31:48,799
specializes in translation it provides

00:31:47,200 --> 00:31:50,240
state-of-the-art's translation models

00:31:48,799 --> 00:31:52,880
and powers uh

00:31:50,240 --> 00:31:55,440
the amazon translate service in the

00:31:52,880 --> 00:31:58,480
latest version it adopts gloom api

00:31:55,440 --> 00:31:59,279
that achieved 14 increased training

00:31:58,480 --> 00:32:02,320
speed with

00:31:59,279 --> 00:32:05,039
one quarter less lines of code it offers

00:32:02,320 --> 00:32:06,240
a faster model training now with inter

00:32:05,039 --> 00:32:09,279
integration

00:32:06,240 --> 00:32:12,320
of amp and horrible

00:32:09,279 --> 00:32:13,440
it also achieves 3.4 times faster

00:32:12,320 --> 00:32:17,679
translation

00:32:13,440 --> 00:32:17,679
with context matrix multiplication

00:32:17,840 --> 00:32:21,919
um gluon ts is a deep learning for time

00:32:20,960 --> 00:32:24,720
series toolkit

00:32:21,919 --> 00:32:26,960
that powers amazon forecast it's

00:32:24,720 --> 00:32:28,880
designed to be modular and scalable with

00:32:26,960 --> 00:32:30,960
the production stability

00:32:28,880 --> 00:32:32,720
you can mix different modules of

00:32:30,960 --> 00:32:34,960
distributions

00:32:32,720 --> 00:32:36,640
probabilistic components and the neural

00:32:34,960 --> 00:32:41,279
network structures for different

00:32:36,640 --> 00:32:41,279
approaches in time series modeling

00:32:41,600 --> 00:32:48,480
and uh next is dgl this is more of a

00:32:45,200 --> 00:32:52,000
cutting edge research topic so

00:32:48,480 --> 00:32:54,080
deep graph library or dgl is a flexible

00:32:52,000 --> 00:32:56,080
graph neural network toolkit

00:32:54,080 --> 00:32:57,120
it's widely adopted in the research

00:32:56,080 --> 00:32:59,600
community and

00:32:57,120 --> 00:33:01,279
has high performance it also offers

00:32:59,600 --> 00:33:02,880
domain specific tools for knowledge

00:33:01,279 --> 00:33:07,519
graph embedding and

00:33:02,880 --> 00:33:07,519
life science in chemistry and biology

00:33:07,840 --> 00:33:13,679
um yeah again so that's the end of uh

00:33:10,880 --> 00:33:14,399
my my talk again uh if you want to get

00:33:13,679 --> 00:33:17,760
involved

00:33:14,399 --> 00:33:18,480
in mxnet here are the different channels

00:33:17,760 --> 00:33:21,360
uh that

00:33:18,480 --> 00:33:23,279
you can connect with uh stand uh reach

00:33:21,360 --> 00:33:26,480
out to us

00:33:23,279 --> 00:33:29,840
all right with that um i'll

00:33:26,480 --> 00:33:32,320
switch back from the full screen so that

00:33:29,840 --> 00:33:38,000
i can see the questions

00:33:32,320 --> 00:33:40,240
thank you

00:33:38,000 --> 00:33:40,240
so

00:33:48,080 --> 00:33:51,200
all right so uh i saw a question from

00:33:50,399 --> 00:33:54,960
croak

00:33:51,200 --> 00:33:57,840
um so um you mean with

00:33:54,960 --> 00:33:59,679
mxnet we can build a deep learning model

00:33:57,840 --> 00:34:03,360
and then export the trained model for

00:33:59,679 --> 00:34:06,720
mobile apps yes that's the case

00:34:03,360 --> 00:34:09,040
and since we integrate with tbm um

00:34:06,720 --> 00:34:09,760
dvm can actually optimize for different

00:34:09,040 --> 00:34:12,560
hardwares

00:34:09,760 --> 00:34:13,520
for that mobile so that you can get the

00:34:12,560 --> 00:34:21,839
performance

00:34:13,520 --> 00:34:21,839
benefits from it too

00:34:27,760 --> 00:34:32,320
yeah any other question also uh remember

00:34:30,800 --> 00:34:35,599
to check out the

00:34:32,320 --> 00:34:38,240
dji library talk

00:34:35,599 --> 00:34:40,639
from tomorrow so qing a colleague of

00:34:38,240 --> 00:34:42,000
mine and also a ppmc member of mxnet

00:34:40,639 --> 00:34:45,919
will be talking about

00:34:42,000 --> 00:34:45,919
deep learning in java extensively

00:34:46,839 --> 00:34:49,839
tomorrow

00:34:50,159 --> 00:34:55,839
um can you say something more about the

00:34:53,119 --> 00:34:59,680
new 2.0 versions what are the changes

00:34:55,839 --> 00:35:03,200
so the two most notable changes are

00:34:59,680 --> 00:35:06,880
the numpy compatible api for programming

00:35:03,200 --> 00:35:09,839
this would enable numpy models to

00:35:06,880 --> 00:35:10,160
be optimized and deployed in a similar

00:35:09,839 --> 00:35:13,200
way

00:35:10,160 --> 00:35:14,720
as deep learning models from mxnet in

00:35:13,200 --> 00:35:17,520
one point x

00:35:14,720 --> 00:35:20,000
and also in 2.0 we simplified our gluon

00:35:17,520 --> 00:35:23,200
interface so that um

00:35:20,000 --> 00:35:27,040
it's more flexible

00:35:23,200 --> 00:35:30,320
um yeah it's more flexible and

00:35:27,040 --> 00:35:45,839
can optimize different models so that

00:35:30,320 --> 00:35:45,839
we can help the speed of innovation

00:35:57,839 --> 00:36:01,040
um any other question

00:36:04,960 --> 00:36:11,920
um so uh 2.0 release date

00:36:08,800 --> 00:36:14,320
we're going through a series of beta

00:36:11,920 --> 00:36:17,440
releases for 2.0 first

00:36:14,320 --> 00:36:18,880
we want to make sure that it's of a good

00:36:17,440 --> 00:36:21,680
quality

00:36:18,880 --> 00:36:23,760
before we call it an official release so

00:36:21,680 --> 00:36:27,839
we don't have a date set in stone but we

00:36:23,760 --> 00:36:27,839
do have a quality bar in mind

00:36:29,119 --> 00:36:41,839
the first beta release would be

00:36:30,560 --> 00:36:41,839
happening in the following months

00:37:01,599 --> 00:37:09,839
all right thank you

00:37:46,079 --> 00:37:48,160

YouTube URL: https://www.youtube.com/watch?v=gwPG90afGoI


