Title: Apache Submarine: State of the union
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Machine Learning
Description: 
	Apache Submarine: State of the union
Wangda Tan, Zhankun Tang

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Submarine is the ONE PLATFORM to allow Data Scientists to create end-to-end machine learning workflow. ONE PLATFORM means it supports Data Scientists to finish their jobs on the same platform without frequently switching their toolsets. From dataset exploring data pipeline creation, model training (experiments), and push model to production (model serving and monitoring). All these steps can be completed within the ONE PLATFORM. In this talk, we’ll start with the current status of Apache Submarine – how it is used today in deployments large and small. We'll then move on to the exciting present & future of Submarine – features that are further strengthening Submarine as the ONE PLATFORM for data scientists to train/manage machine learning models. We’ll discuss highlight of the newly released 0.4.0 version, and new features 0.5.0 release which is planned in 2020 Q3: - New features to run model training (experiments) on K8s, submit mode training job by using easy-to-use Python/REST API or UI. - Integration to Jupyter notebook, and allows Data-Scientists to provision, manage notebook session, and submit offline machine learning jobs from notebooks. - Integration with Conda kernel, Docker images to make hassle-free experiences to manage reusable notebook/mode-training experiments within a team/company. - Pre-packaged Training Template for Data-Scientists to focus on domain-specific tasks (like using DeepFM to build a CTR prediction model). We will also share mid-term/long-term roadmap for Submarine, including Model management for model-serving/versioning/monitoring, etc.

Wangda Tan:
Wangda Tan is Sr. Manager of Compute Platform engineering team @ Cloudera, responsible for all engineering efforts related to Kubernetes, Apache Hadoop YARN, Resource Scheduling, and internal container cloud. In open-source world, he's a member of Apache Software Foundation (ASF), PMC Chair of Apache Submarine project, He is also project management committee (PMC) members of Apache Hadoop, Apache YuniKorn (incubating). Before joining Cloudera, he leads High-performance-computing on Hadoop related work in EMC/Pivotal. Before that, he worked in Alibaba Cloud and participated in the development of a distributed machine learning platform (later became ODPS XLIB).

Zhankun Tang:
Zhankun Tang is Staff Software Engineer @Cloudera. He’s interested in big data, cloud computing, and operating system. Now focus on contributing new features to Hadoop as well as customer engagement. Zhankun is PMC member of Apache Hadoop/Submarine, prior to Cloudera/Hortonworks, he works for Intel.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,240 --> 00:00:28,880
nice

00:00:25,519 --> 00:00:29,760
yeah so so hello everyone um today we're

00:00:28,880 --> 00:00:32,880
going to talk about

00:00:29,760 --> 00:00:34,320
apache submarine um which is a unified

00:00:32,880 --> 00:00:37,040
machine learning platform

00:00:34,320 --> 00:00:39,120
um my name is wanda 10 and we have a

00:00:37,040 --> 00:00:42,160
house speaker jenkin tong so both of

00:00:39,120 --> 00:00:43,760
us um are from caldera so today jenkins

00:00:42,160 --> 00:00:46,480
cannot join in person

00:00:43,760 --> 00:00:47,520
he is in in china which is a really bad

00:00:46,480 --> 00:00:50,800
time zone

00:00:47,520 --> 00:00:53,440
um for to this slot

00:00:50,800 --> 00:00:54,079
um so he recorded a demo and i will play

00:00:53,440 --> 00:00:57,680
the demo

00:00:54,079 --> 00:01:00,079
later so let's get started

00:00:57,680 --> 00:01:01,039
so this is agenda today so first we're

00:01:00,079 --> 00:01:02,879
going to

00:01:01,039 --> 00:01:05,040
talk about machine learning in

00:01:02,879 --> 00:01:06,560
production and requirements

00:01:05,040 --> 00:01:08,159
and then we will talk about what is

00:01:06,560 --> 00:01:11,360
submarine and what's the

00:01:08,159 --> 00:01:12,960
problem of summary trying to solve

00:01:11,360 --> 00:01:14,880
goes through the future highlights of

00:01:12,960 --> 00:01:17,280
submarine and we also

00:01:14,880 --> 00:01:19,840
talk about the submarine community and

00:01:17,280 --> 00:01:21,520
the release plans

00:01:19,840 --> 00:01:23,680
so first let's see the machine learning

00:01:21,520 --> 00:01:26,799
in production

00:01:23,680 --> 00:01:30,159
so if you check some

00:01:26,799 --> 00:01:30,799
tutorials then on the tensorflow or

00:01:30,159 --> 00:01:34,240
other

00:01:30,799 --> 00:01:36,720
website um you can see it's it's fairly

00:01:34,240 --> 00:01:40,479
simple you import import a tensorflow

00:01:36,720 --> 00:01:43,520
and you create some some

00:01:40,479 --> 00:01:45,759
some constants or vertex

00:01:43,520 --> 00:01:47,200
and you multiply them yeah and you feel

00:01:45,759 --> 00:01:47,840
like you are learning a lot of things

00:01:47,200 --> 00:01:50,880
you are

00:01:47,840 --> 00:01:54,000
already mastering much learning

00:01:50,880 --> 00:01:56,399
and if you search youtube you can see

00:01:54,000 --> 00:01:57,439
a lot of tutorials like handwritten

00:01:56,399 --> 00:02:01,280
character image

00:01:57,439 --> 00:02:04,640
classifier in 49 build an image

00:02:01,280 --> 00:02:06,560
classifier in certain eyes and you can

00:02:04,640 --> 00:02:10,720
print it apple stock in forty nights

00:02:06,560 --> 00:02:12,560
and of that so sometimes you will feel

00:02:10,720 --> 00:02:14,160
very excited because machine learning

00:02:12,560 --> 00:02:17,120
sounds very simple

00:02:14,160 --> 00:02:17,680
however sometimes you will feel is that

00:02:17,120 --> 00:02:21,440
true

00:02:17,680 --> 00:02:22,959
this sounds too oversimplified

00:02:21,440 --> 00:02:25,440
so if you look at the machine learning

00:02:22,959 --> 00:02:27,200
um training life cycle so this

00:02:25,440 --> 00:02:28,959
this picture is showing everywhere i

00:02:27,200 --> 00:02:32,239
don't want to repeat too much

00:02:28,959 --> 00:02:32,640
so basically what this says is the code

00:02:32,239 --> 00:02:34,480
you

00:02:32,640 --> 00:02:36,239
you want to write for the machine

00:02:34,480 --> 00:02:39,120
learning it's just a very

00:02:36,239 --> 00:02:40,640
tiny part of your whole training um

00:02:39,120 --> 00:02:44,480
whole machine learning training

00:02:40,640 --> 00:02:46,080
and um and how much learning life cycle

00:02:44,480 --> 00:02:48,160
there's a lot of things we have to build

00:02:46,080 --> 00:02:51,840
around that in order to make

00:02:48,160 --> 00:02:51,840
machine learning useful

00:02:52,000 --> 00:02:57,200
so if you look at the typical data

00:02:55,360 --> 00:03:00,480
pipeline for machine learning

00:02:57,200 --> 00:03:03,360
uh you will see the data first is come

00:03:00,480 --> 00:03:05,840
from some edge like some

00:03:03,360 --> 00:03:06,959
some sensors or this also can come from

00:03:05,840 --> 00:03:10,159
some

00:03:06,959 --> 00:03:13,519
click lock of the website and everything

00:03:10,159 --> 00:03:16,720
will be landed in a data lake

00:03:13,519 --> 00:03:20,239
and the data scientists can use

00:03:16,720 --> 00:03:23,120
tools like zeppelin or jupiter or other

00:03:20,239 --> 00:03:24,400
since the tableau to explore the data

00:03:23,120 --> 00:03:27,360
and trying to get

00:03:24,400 --> 00:03:29,120
the sense of the data once they have

00:03:27,360 --> 00:03:32,720
some understanding of the data

00:03:29,120 --> 00:03:36,000
they can run some etl applications like

00:03:32,720 --> 00:03:36,959
like join like sampling feature

00:03:36,000 --> 00:03:39,680
stretching

00:03:36,959 --> 00:03:40,239
and typically they they use tools like

00:03:39,680 --> 00:03:44,159
spark

00:03:40,239 --> 00:03:47,440
or hive and once they get the data

00:03:44,159 --> 00:03:50,560
they can do the model training

00:03:47,440 --> 00:03:52,799
and they can typically use things like

00:03:50,560 --> 00:03:53,680
tensorflow or pi torch to do the model

00:03:52,799 --> 00:03:55,840
training

00:03:53,680 --> 00:03:57,120
and once the training is done the model

00:03:55,840 --> 00:03:59,680
will be saved

00:03:57,120 --> 00:04:00,799
in some storage and later you can deploy

00:03:59,680 --> 00:04:04,159
this model in

00:04:00,799 --> 00:04:06,879
production and so once you visit a bank

00:04:04,159 --> 00:04:07,519
once you want to get a long funnel bank

00:04:06,879 --> 00:04:09,840
um

00:04:07,519 --> 00:04:11,439
and your risk number will be will be

00:04:09,840 --> 00:04:15,840
predicted by the

00:04:11,439 --> 00:04:15,840
by the mercenary models

00:04:16,880 --> 00:04:20,720
so for the data scientists so let's try

00:04:18,959 --> 00:04:22,160
to define what is data scientist and

00:04:20,720 --> 00:04:25,600
what c wants to

00:04:22,160 --> 00:04:28,320
uh they want to expect from the

00:04:25,600 --> 00:04:30,160
pro from the product um so we have

00:04:28,320 --> 00:04:32,160
talked to many data scientists

00:04:30,160 --> 00:04:34,000
and typically they are experts of

00:04:32,160 --> 00:04:36,800
machine learning algorithms

00:04:34,000 --> 00:04:37,360
and they are very familiar with models

00:04:36,800 --> 00:04:39,840
and

00:04:37,360 --> 00:04:41,120
libraries to build these models and

00:04:39,840 --> 00:04:44,479
feature engineering

00:04:41,120 --> 00:04:47,280
right they need tools and platforms to

00:04:44,479 --> 00:04:48,000
gain insight of the data and they want

00:04:47,280 --> 00:04:51,280
to build

00:04:48,000 --> 00:04:54,160
models very efficiently

00:04:51,280 --> 00:04:55,759
and they also want to create ml pipeline

00:04:54,160 --> 00:04:58,160
so which i just mentioned

00:04:55,759 --> 00:04:59,040
before like data labeling transformation

00:04:58,160 --> 00:05:02,160
like that

00:04:59,040 --> 00:05:02,720
um very easily they are mostly familiar

00:05:02,160 --> 00:05:06,400
with

00:05:02,720 --> 00:05:08,880
uh the tools like python spark or

00:05:06,400 --> 00:05:12,800
some of them familiar with hive they are

00:05:08,880 --> 00:05:12,800
not very familiar with platform stuffs

00:05:13,120 --> 00:05:19,440
so what they will expect so for the

00:05:16,560 --> 00:05:21,039
model exploration uh they typically use

00:05:19,440 --> 00:05:22,960
pre-process using spark

00:05:21,039 --> 00:05:25,039
or hive or sometimes they will use

00:05:22,960 --> 00:05:27,919
pandas which is some smaller

00:05:25,039 --> 00:05:30,479
scale alternatives and they will use

00:05:27,919 --> 00:05:33,919
experiment to use sample dataset

00:05:30,479 --> 00:05:34,880
right and they will will do the

00:05:33,919 --> 00:05:37,759
experiment

00:05:34,880 --> 00:05:38,800
with a full data set once they get some

00:05:37,759 --> 00:05:41,360
confidence

00:05:38,800 --> 00:05:42,400
and typically they need a distributed

00:05:41,360 --> 00:05:45,440
algorithm or

00:05:42,400 --> 00:05:48,479
they need to run the single node

00:05:45,440 --> 00:05:51,440
application in a faster faster resources

00:05:48,479 --> 00:05:52,000
too wrong and they need to make sure

00:05:51,440 --> 00:05:55,440
that

00:05:52,000 --> 00:05:58,560
the experiment can be easier um

00:05:55,440 --> 00:06:01,759
and reproducible they can record

00:05:58,560 --> 00:06:04,960
parameters codes and metrics of

00:06:01,759 --> 00:06:07,680
experiment and they want to manage their

00:06:04,960 --> 00:06:08,240
uh dependency easier right they want to

00:06:07,680 --> 00:06:12,000
really

00:06:08,240 --> 00:06:13,759
coding once and run everywhere they also

00:06:12,000 --> 00:06:16,479
wants to make the parameters can be

00:06:13,759 --> 00:06:19,840
easier tuned

00:06:16,479 --> 00:06:21,440
and they can use tools like automl right

00:06:19,840 --> 00:06:22,400
they also want you very easy to manage

00:06:21,440 --> 00:06:24,880
the models

00:06:22,400 --> 00:06:25,759
to uh easier to push the models to

00:06:24,880 --> 00:06:27,919
production

00:06:25,759 --> 00:06:29,440
and also they want to do the model

00:06:27,919 --> 00:06:33,039
assurance monitoring

00:06:29,440 --> 00:06:35,600
like that so what's it don't

00:06:33,039 --> 00:06:36,639
expect to know so we also talk to a lot

00:06:35,600 --> 00:06:39,840
of

00:06:36,639 --> 00:06:40,960
data scientists they really uh lack of

00:06:39,840 --> 00:06:44,160
understanding

00:06:40,960 --> 00:06:46,319
of resource management concepts like yar

00:06:44,160 --> 00:06:47,280
or kubernetes if you ask them to

00:06:46,319 --> 00:06:49,840
troubleshooting

00:06:47,280 --> 00:06:51,120
why application cannot start it on yarn

00:06:49,840 --> 00:06:54,800
or kubernetes

00:06:51,120 --> 00:06:56,720
they cannot figure it out they also

00:06:54,800 --> 00:06:59,520
many of them are not familiar with

00:06:56,720 --> 00:07:03,039
tooling compute engine

00:06:59,520 --> 00:07:05,039
in some deeper deeper fine tuning

00:07:03,039 --> 00:07:06,080
such as tuning the memory configuration

00:07:05,039 --> 00:07:08,800
or shuffle

00:07:06,080 --> 00:07:09,840
performance of a smart job so what they

00:07:08,800 --> 00:07:13,520
really want to

00:07:09,840 --> 00:07:15,199
to have is some working compute engine

00:07:13,520 --> 00:07:16,560
they are not really worried about the

00:07:15,199 --> 00:07:17,280
performance they just want to make it

00:07:16,560 --> 00:07:20,880
working

00:07:17,280 --> 00:07:22,639
make it available and not feel

00:07:20,880 --> 00:07:24,000
so they don't like any nitty-gritty

00:07:22,639 --> 00:07:28,000
details of under

00:07:24,000 --> 00:07:28,000
9 infra it should just work

00:07:29,120 --> 00:07:35,680
so what is apache summary so summary

00:07:32,479 --> 00:07:36,240
um by definition we want to uh see it's

00:07:35,680 --> 00:07:39,440
a

00:07:36,240 --> 00:07:40,000
one platform to allow data scientists to

00:07:39,440 --> 00:07:42,400
create

00:07:40,000 --> 00:07:44,479
uh end-to-end machine learning workflow

00:07:42,400 --> 00:07:47,120
so when we see it's a one platform means

00:07:44,479 --> 00:07:47,120
we want to

00:07:50,240 --> 00:07:53,360
um not sure if audio will start

00:07:51,919 --> 00:07:55,520
automatically if there's ever do

00:07:53,360 --> 00:07:57,840
something

00:07:55,520 --> 00:07:59,039
um so so i just want to confirm um

00:07:57,840 --> 00:08:02,479
thoughts on the

00:07:59,039 --> 00:08:02,479
nine and you can't hear me right

00:08:03,840 --> 00:08:08,479
okay sounds good uh yeah basically what

00:08:06,960 --> 00:08:12,080
we want to have is

00:08:08,479 --> 00:08:13,360
we can do the do the um have a single

00:08:12,080 --> 00:08:16,000
platform to

00:08:13,360 --> 00:08:16,800
allow data scientists to exploring data

00:08:16,000 --> 00:08:19,039
pipeline

00:08:16,800 --> 00:08:22,479
model training tooling push model to

00:08:19,039 --> 00:08:25,360
production in the same platform

00:08:22,479 --> 00:08:26,160
so what submarine can do so later we

00:08:25,360 --> 00:08:29,680
will show a

00:08:26,160 --> 00:08:32,159
uh demo from gentlemen about all these

00:08:29,680 --> 00:08:32,719
things but i want to introduce about

00:08:32,159 --> 00:08:35,120
this

00:08:32,719 --> 00:08:36,159
um and this for now to make sure you

00:08:35,120 --> 00:08:38,959
understand

00:08:36,159 --> 00:08:40,959
uh how this feature work so first is we

00:08:38,959 --> 00:08:43,680
can very easy to install summary

00:08:40,959 --> 00:08:45,680
a single pay-on command can install

00:08:43,680 --> 00:08:48,160
everything needed for summary our

00:08:45,680 --> 00:08:51,360
kubernetes cluster

00:08:48,160 --> 00:08:54,399
and when we do the experiment which is a

00:08:51,360 --> 00:08:56,240
model training we have very nice python

00:08:54,399 --> 00:08:58,959
sdk and api

00:08:56,240 --> 00:09:00,800
to submit and manage experiment so

00:08:58,959 --> 00:09:04,000
currently we support

00:09:00,800 --> 00:09:04,720
um pytorch and tensorflow so for the

00:09:04,000 --> 00:09:07,839
notebook

00:09:04,720 --> 00:09:09,519
we have managed jupyter network service

00:09:07,839 --> 00:09:10,640
to allow modern users to share the same

00:09:09,519 --> 00:09:13,120
environment

00:09:10,640 --> 00:09:14,560
and also we have a first-class

00:09:13,120 --> 00:09:18,320
environment

00:09:14,560 --> 00:09:21,600
concept to allow a user to manage their

00:09:18,320 --> 00:09:23,120
darker and content environment easier so

00:09:21,600 --> 00:09:25,200
basically we don't want your data

00:09:23,120 --> 00:09:27,680
scientists get into trouble with

00:09:25,200 --> 00:09:30,080
building their own data image or

00:09:27,680 --> 00:09:31,519
handle their conda dependencies like

00:09:30,080 --> 00:09:34,720
that we want to make

00:09:31,519 --> 00:09:38,080
a environment can be easier shared

00:09:34,720 --> 00:09:41,279
within the team and to other other

00:09:38,080 --> 00:09:42,240
members of the team also we have ditch

00:09:41,279 --> 00:09:45,360
integration

00:09:42,240 --> 00:09:47,920
for experiment and notebook

00:09:45,360 --> 00:09:49,600
which you can easily download the code

00:09:47,920 --> 00:09:52,640
directly from

00:09:49,600 --> 00:09:53,519
from from your private git repo or

00:09:52,640 --> 00:09:55,600
github

00:09:53,519 --> 00:09:56,720
so that can help you to do the trimming

00:09:55,600 --> 00:09:58,480
easier

00:09:56,720 --> 00:09:59,760
um for the most things like model and

00:09:58,480 --> 00:10:03,200
matrix management

00:09:59,760 --> 00:10:05,680
this part is is in planning but we

00:10:03,200 --> 00:10:06,560
our plans to finish the first of four

00:10:05,680 --> 00:10:09,120
buckets

00:10:06,560 --> 00:10:11,360
uh first in a live series and then we

00:10:09,120 --> 00:10:15,680
can move to the model and matrix

00:10:11,360 --> 00:10:19,600
management so here are the features of

00:10:15,680 --> 00:10:21,600
submarine so using one command you can

00:10:19,600 --> 00:10:22,959
install helm install submarine and

00:10:21,600 --> 00:10:27,120
install the summary on

00:10:22,959 --> 00:10:29,200
existing kubernetes and

00:10:27,120 --> 00:10:30,480
user can go to your notebook page to

00:10:29,200 --> 00:10:33,760
create notebook

00:10:30,480 --> 00:10:34,320
and choose our environment and you can

00:10:33,760 --> 00:10:37,519
get your

00:10:34,320 --> 00:10:42,720
book running in the cluster and you can

00:10:37,519 --> 00:10:42,720
submit your applications in the notebook

00:10:43,120 --> 00:10:49,200
we also have training support we have

00:10:46,160 --> 00:10:49,839
python sdk you can create an environment

00:10:49,200 --> 00:10:52,160
here

00:10:49,839 --> 00:10:53,120
you choose what is the environment which

00:10:52,160 --> 00:10:56,160
is a

00:10:53,120 --> 00:10:59,360
docker and anaconda dependencies you

00:10:56,160 --> 00:11:01,519
choose where is your training code

00:10:59,360 --> 00:11:02,720
located so this training code is located

00:11:01,519 --> 00:11:06,079
in s3

00:11:02,720 --> 00:11:06,399
and you specify the parameters to train

00:11:06,079 --> 00:11:09,839
the

00:11:06,399 --> 00:11:12,959
teaching model and specify some

00:11:09,839 --> 00:11:14,880
parameters and also input output

00:11:12,959 --> 00:11:17,279
location to this application once you

00:11:14,880 --> 00:11:20,800
call the experiment.run

00:11:17,279 --> 00:11:24,160
and run this python script this can

00:11:20,800 --> 00:11:26,640
run submit this job to to your

00:11:24,160 --> 00:11:28,320
distributed cluster like a kubernetes

00:11:26,640 --> 00:11:30,240
and around this without user

00:11:28,320 --> 00:11:32,640
understanding there's anything

00:11:30,240 --> 00:11:34,560
running and so so it is running on

00:11:32,640 --> 00:11:36,880
kubernetes or not so user doesn't

00:11:34,560 --> 00:11:39,360
worry about that doesn't care about

00:11:36,880 --> 00:11:39,360
about that

00:11:40,079 --> 00:11:45,279
so we also have ui support so later

00:11:42,000 --> 00:11:48,320
we're going to show a real demo for this

00:11:45,279 --> 00:11:52,560
and for the environment profile

00:11:48,320 --> 00:11:56,800
um for the data scientists or the

00:11:52,560 --> 00:11:59,200
the model model engineers they can

00:11:56,800 --> 00:12:00,160
create an environment like let's put a

00:11:59,200 --> 00:12:02,800
name like my

00:12:00,160 --> 00:12:04,000
submarine environment specify a base

00:12:02,800 --> 00:12:07,360
docker image

00:12:04,000 --> 00:12:09,279
and what is the anaconda kernel to use

00:12:07,360 --> 00:12:11,040
you put a kernel name what is the

00:12:09,279 --> 00:12:11,920
channel and what is the dependencies

00:12:11,040 --> 00:12:15,040
here

00:12:11,920 --> 00:12:16,399
and when you want to use the environment

00:12:15,040 --> 00:12:19,120
in your experiment

00:12:16,399 --> 00:12:19,839
you can just refer the name of my

00:12:19,120 --> 00:12:22,639
submarine

00:12:19,839 --> 00:12:22,880
environment as an environment parameter

00:12:22,639 --> 00:12:24,959
and

00:12:22,880 --> 00:12:26,480
submit your application to summary so

00:12:24,959 --> 00:12:28,880
submarine can

00:12:26,480 --> 00:12:29,920
automatically download the darker image

00:12:28,880 --> 00:12:31,440
for you and

00:12:29,920 --> 00:12:33,279
set up the other counter dependencies

00:12:31,440 --> 00:12:36,480
for you so user don't need to worry

00:12:33,279 --> 00:12:36,480
about all these dependencies

00:12:36,800 --> 00:12:44,959
and this can be also used in in notebook

00:12:40,800 --> 00:12:44,959
so all the scenes can be shared

00:12:45,600 --> 00:12:53,680
so for the training support

00:12:48,720 --> 00:12:57,839
we have the date repo integration

00:12:53,680 --> 00:13:01,120
a way an user can so there are several

00:12:57,839 --> 00:13:05,360
ways you can get your training code in

00:13:01,120 --> 00:13:06,639
training code into your application

00:13:05,360 --> 00:13:09,040
when you're running applications on

00:13:06,639 --> 00:13:09,760
kubernetes so first option is to pass

00:13:09,040 --> 00:13:12,560
the code

00:13:09,760 --> 00:13:13,600
into a docker image so the downside is

00:13:12,560 --> 00:13:17,440
very difficult for

00:13:13,600 --> 00:13:19,519
data scientists they have

00:13:17,440 --> 00:13:21,040
they will be pretty struggle to figure

00:13:19,519 --> 00:13:22,320
out how to build a doctor image

00:13:21,040 --> 00:13:25,440
themselves

00:13:22,320 --> 00:13:26,720
in a right way and second options is to

00:13:25,440 --> 00:13:30,160
mount a volume

00:13:26,720 --> 00:13:32,560
like a nfs to a jar so the downside is

00:13:30,160 --> 00:13:33,279
a lot of data scientists they just

00:13:32,560 --> 00:13:36,000
change

00:13:33,279 --> 00:13:38,160
their code on the disk frequently so

00:13:36,000 --> 00:13:41,440
it's very hard to trace back

00:13:38,160 --> 00:13:43,760
when this change happens so what we

00:13:41,440 --> 00:13:44,880
suggest the user to do is to use a data

00:13:43,760 --> 00:13:46,880
integration

00:13:44,880 --> 00:13:48,000
uh when you run your experiment you

00:13:46,880 --> 00:13:51,760
specify a

00:13:48,000 --> 00:13:54,800
sync mode to say git specify a url

00:13:51,760 --> 00:13:57,199
and this will be submitted and so when

00:13:54,800 --> 00:14:00,000
this job is submitted to kubernetes

00:13:57,199 --> 00:14:00,880
the date will be automatically checked

00:14:00,000 --> 00:14:03,120
out

00:14:00,880 --> 00:14:04,639
by summary and put this to the slash

00:14:03,120 --> 00:14:07,760
code so you can just

00:14:04,639 --> 00:14:10,000
refer your training code under the slash

00:14:07,760 --> 00:14:10,000
code

00:14:11,440 --> 00:14:17,839
so here we are going to show a demo from

00:14:14,839 --> 00:14:17,839
gentlemen

00:14:22,480 --> 00:14:27,680
so let me make this time run faster

00:14:28,000 --> 00:14:33,279
okay so so here's the demo um this is a

00:14:31,360 --> 00:14:37,680
login page of summary

00:14:33,279 --> 00:14:40,079
and so we can user can log in here using

00:14:37,680 --> 00:14:41,360
their own credentials and like we

00:14:40,079 --> 00:14:45,040
mentioned we can use

00:14:41,360 --> 00:14:47,680
the um dodger desktop

00:14:45,040 --> 00:14:49,199
communities to install submarine and use

00:14:47,680 --> 00:14:50,800
the hem install to

00:14:49,199 --> 00:14:52,959
install the submarine in one single

00:14:50,800 --> 00:14:53,760
command after uh maybe two minutes you

00:14:52,959 --> 00:14:57,600
can get this

00:14:53,760 --> 00:15:00,320
up and running and this is a dashboard

00:14:57,600 --> 00:15:03,120
of your submarine you can see the

00:15:00,320 --> 00:15:06,320
resources available in the cluster

00:15:03,120 --> 00:15:08,720
and you can how many gpus and

00:15:06,320 --> 00:15:09,600
cpus like that and you can see the

00:15:08,720 --> 00:15:11,600
lookbook

00:15:09,600 --> 00:15:14,160
experiments and the environment you can

00:15:11,600 --> 00:15:17,360
manage the resources here

00:15:14,160 --> 00:15:19,279
so let's go to the um let's go to the

00:15:17,360 --> 00:15:20,720
environment first

00:15:19,279 --> 00:15:22,399
so in the environment you can see there

00:15:20,720 --> 00:15:25,519
are a list of environments

00:15:22,399 --> 00:15:28,079
and you have a darker image here

00:15:25,519 --> 00:15:28,880
and you have a watch python dependencies

00:15:28,079 --> 00:15:31,360
here

00:15:28,880 --> 00:15:32,240
you can also create your own environment

00:15:31,360 --> 00:15:35,279
put a name

00:15:32,240 --> 00:15:36,880
put a doctor image name and you can add

00:15:35,279 --> 00:15:39,199
some other dependencies

00:15:36,880 --> 00:15:40,639
so once this is created this can be

00:15:39,199 --> 00:15:44,639
shared

00:15:40,639 --> 00:15:44,639
across different team members

00:15:45,519 --> 00:15:49,040
so let's go to experiment and try to run

00:15:47,279 --> 00:15:50,480
science experiment so currently there's

00:15:49,040 --> 00:15:53,199
no experiment running

00:15:50,480 --> 00:15:54,480
and you create an experiment first you

00:15:53,199 --> 00:15:57,120
can

00:15:54,480 --> 00:15:58,480
define your own experiment and you can

00:15:57,120 --> 00:16:01,839
put a name

00:15:58,480 --> 00:16:02,320
um and and put what is the command to

00:16:01,839 --> 00:16:05,920
use

00:16:02,320 --> 00:16:08,639
so here is a very simple minist command

00:16:05,920 --> 00:16:09,920
to with some parameters and put what is

00:16:08,639 --> 00:16:12,959
the image

00:16:09,920 --> 00:16:12,959
for this experiment

00:16:14,000 --> 00:16:17,519
so here's also some advanced options you

00:16:17,040 --> 00:16:20,160
can put

00:16:17,519 --> 00:16:21,279
like what's name space to use but people

00:16:20,160 --> 00:16:23,839
should not hear

00:16:21,279 --> 00:16:25,040
so the second step is to choose your

00:16:23,839 --> 00:16:27,680
training job type

00:16:25,040 --> 00:16:29,759
here is a distributed tensorflow or

00:16:27,680 --> 00:16:32,720
distributed high torch or

00:16:29,759 --> 00:16:34,320
standalone script right so for here we

00:16:32,720 --> 00:16:34,959
are running a distributed tensorflow job

00:16:34,320 --> 00:16:37,279
so we

00:16:34,959 --> 00:16:39,040
choose distributed tensorflow and we

00:16:37,279 --> 00:16:41,519
have one water

00:16:39,040 --> 00:16:43,120
how many memories here and we can add a

00:16:41,519 --> 00:16:45,680
parameter server here

00:16:43,120 --> 00:16:46,320
with some memory definition for the next

00:16:45,680 --> 00:16:49,040
step

00:16:46,320 --> 00:16:50,800
we can review the training job uh here's

00:16:49,040 --> 00:16:53,600
the command line we're going to use

00:16:50,800 --> 00:16:54,480
and here's the image of the doctor image

00:16:53,600 --> 00:16:56,399
and we can see

00:16:54,480 --> 00:16:57,920
we have one water and one parameter

00:16:56,399 --> 00:17:01,120
server it's a very simple

00:16:57,920 --> 00:17:04,720
distributed tensorflow application

00:17:01,120 --> 00:17:07,919
and so once you make sure this is right

00:17:04,720 --> 00:17:10,959
we can we can create and submit

00:17:07,919 --> 00:17:12,959
and this will be submitted to the the

00:17:10,959 --> 00:17:17,360
communities cluster running in the back

00:17:12,959 --> 00:17:20,799
you can see the status is is accepted

00:17:17,360 --> 00:17:23,919
and if you want to

00:17:20,799 --> 00:17:26,480
um to retrain your job you can click a

00:17:23,919 --> 00:17:27,919
clone and you can change some of the

00:17:26,480 --> 00:17:30,240
parameters you have

00:17:27,919 --> 00:17:32,080
and you can resubmit this again so this

00:17:30,240 --> 00:17:36,640
can help you to

00:17:32,080 --> 00:17:39,360
to run jobs with different setup easier

00:17:36,640 --> 00:17:39,679
so we can see this is running and if you

00:17:39,360 --> 00:17:42,480
click

00:17:39,679 --> 00:17:43,600
output you can see the output of the of

00:17:42,480 --> 00:17:46,240
the

00:17:43,600 --> 00:17:46,640
training job right there's also metrics

00:17:46,240 --> 00:17:48,640
uh

00:17:46,640 --> 00:17:50,880
if there's any matrix data here it will

00:17:48,640 --> 00:17:54,000
be showing here

00:17:50,880 --> 00:17:57,440
in the metrics so

00:17:54,000 --> 00:17:57,440
let's uh um

00:17:58,480 --> 00:18:04,640
so so here we we go to the notebook

00:18:01,679 --> 00:18:07,440
so here we have two notebooks running um

00:18:04,640 --> 00:18:10,400
and also we can create new notes for you

00:18:07,440 --> 00:18:11,679
uh you can create a notebook with a name

00:18:10,400 --> 00:18:14,400
choose an environment

00:18:11,679 --> 00:18:16,000
which we pre and defined choose how many

00:18:14,400 --> 00:18:18,799
cpus and the memories

00:18:16,000 --> 00:18:19,840
and once you click create this will be

00:18:18,799 --> 00:18:22,480
submitted to the

00:18:19,840 --> 00:18:23,039
kubernetes cluster and it will be up and

00:18:22,480 --> 00:18:24,960
running

00:18:23,039 --> 00:18:26,480
so let's go to one of the running

00:18:24,960 --> 00:18:29,840
jupyter notebook

00:18:26,480 --> 00:18:31,280
so let's first shoot some submarine sdk

00:18:29,840 --> 00:18:33,520
example

00:18:31,280 --> 00:18:34,880
so in the sdk example we first will

00:18:33,520 --> 00:18:38,080
input some of the

00:18:34,880 --> 00:18:41,440
um submarine sdk objects

00:18:38,080 --> 00:18:45,520
to the notebook and we create

00:18:41,440 --> 00:18:47,840
uh and clients and here we specify

00:18:45,520 --> 00:18:48,960
select we we mentioned before we will

00:18:47,840 --> 00:18:52,240
specify an

00:18:48,960 --> 00:18:53,360
environment spec and what is the

00:18:52,240 --> 00:18:56,240
environment you use

00:18:53,360 --> 00:18:58,080
and what is the name namespace it's a

00:18:56,240 --> 00:18:59,679
tensorflow or high torch

00:18:58,080 --> 00:19:02,400
what is the common like command knight

00:18:59,679 --> 00:19:04,960
to use uh we have we create one worker

00:19:02,400 --> 00:19:07,520
here one parameter server here

00:19:04,960 --> 00:19:08,320
and we will specify what is the code

00:19:07,520 --> 00:19:11,919
spec here

00:19:08,320 --> 00:19:13,440
it's it's it's from um john quinn's

00:19:11,919 --> 00:19:16,480
github repo

00:19:13,440 --> 00:19:19,520
and once we define everything we

00:19:16,480 --> 00:19:23,440
create uh experiment spec

00:19:19,520 --> 00:19:25,919
which included all these items

00:19:23,440 --> 00:19:27,200
and once this is done we can call the

00:19:25,919 --> 00:19:30,559
submarine client

00:19:27,200 --> 00:19:33,919
creates experiment and this can

00:19:30,559 --> 00:19:35,200
can be submitted so we can see this

00:19:33,919 --> 00:19:38,960
application is

00:19:35,200 --> 00:19:42,240
accepted so we can also use the

00:19:38,960 --> 00:19:44,799
environment uh experiment apis to

00:19:42,240 --> 00:19:46,559
list the experiment to see what is the

00:19:44,799 --> 00:19:49,280
status for now

00:19:46,559 --> 00:19:50,000
right and if we call the wait for finish

00:19:49,280 --> 00:19:53,039
this will just

00:19:50,000 --> 00:19:54,960
wait here and it will showing that

00:19:53,039 --> 00:19:56,240
it will stream in the output of the

00:19:54,960 --> 00:19:58,320
applications to the

00:19:56,240 --> 00:20:00,720
to the notebook and we can get what's

00:19:58,320 --> 00:20:02,960
the status here

00:20:00,720 --> 00:20:04,000
and if we want to get a lot we can also

00:20:02,960 --> 00:20:08,159
get a lot from

00:20:04,000 --> 00:20:08,159
from the from the sdk itself

00:20:08,480 --> 00:20:13,919
yeah and once we don't need this

00:20:12,080 --> 00:20:16,159
experiment anymore we can delete and we

00:20:13,919 --> 00:20:18,000
can delete the experiment

00:20:16,159 --> 00:20:19,440
so if we go back to the experiment we

00:20:18,000 --> 00:20:22,080
can also see

00:20:19,440 --> 00:20:23,280
the new experiment we just submitted

00:20:22,080 --> 00:20:26,159
from notebook

00:20:23,280 --> 00:20:27,440
is list is listed in the ui right the

00:20:26,159 --> 00:20:30,080
user can

00:20:27,440 --> 00:20:30,960
easily check what is the experiment they

00:20:30,080 --> 00:20:33,919
have wrong

00:20:30,960 --> 00:20:34,960
and they can track the status of each

00:20:33,919 --> 00:20:37,679
experiment

00:20:34,960 --> 00:20:39,360
they can also in the future we plan to

00:20:37,679 --> 00:20:43,520
add the support

00:20:39,360 --> 00:20:45,919
to get the metrics and that's

00:20:43,520 --> 00:20:49,360
that the model can be downloaded from

00:20:45,919 --> 00:20:51,440
each experiment

00:20:49,360 --> 00:20:53,360
so the other example is a it's a

00:20:51,440 --> 00:20:54,000
pre-defined template but because of the

00:20:53,360 --> 00:20:57,280
time

00:20:54,000 --> 00:20:59,760
limits today so let's skip this one

00:20:57,280 --> 00:21:01,760
um yeah so so that's pretty much for the

00:20:59,760 --> 00:21:04,400
demo and let's go back to the

00:21:01,760 --> 00:21:04,400
presentation

00:21:10,240 --> 00:21:13,760
so what is the relationship of submarine

00:21:12,960 --> 00:21:16,559
with other

00:21:13,760 --> 00:21:18,640
open source projects so first of all we

00:21:16,559 --> 00:21:20,480
are not trying to reinvent of whales

00:21:18,640 --> 00:21:21,760
we are building on top of other open

00:21:20,480 --> 00:21:24,320
source projects

00:21:21,760 --> 00:21:26,400
and so here is a model where we are

00:21:24,320 --> 00:21:28,080
using other open source projects

00:21:26,400 --> 00:21:30,000
so for the environment profile

00:21:28,080 --> 00:21:32,720
management we are using daughter and

00:21:30,000 --> 00:21:33,520
counter and for the notebook we're using

00:21:32,720 --> 00:21:35,280
jupiter

00:21:33,520 --> 00:21:37,280
and for the uh when we run the

00:21:35,280 --> 00:21:39,440
experiment on yarn we're using

00:21:37,280 --> 00:21:40,559
tensorflow on yarn which is from

00:21:39,440 --> 00:21:42,240
linkedin

00:21:40,559 --> 00:21:44,159
and when we run experiments on

00:21:42,240 --> 00:21:47,520
kubernetes we use uh

00:21:44,159 --> 00:21:49,840
keep flows tf job and pi torch job

00:21:47,520 --> 00:21:51,520
so our focus is not to replicate all

00:21:49,840 --> 00:21:54,960
this underlying

00:21:51,520 --> 00:21:58,000
platform and platform and

00:21:54,960 --> 00:22:00,960
layer and we what our focus is

00:21:58,000 --> 00:22:02,159
to provide a better api and ui support

00:22:00,960 --> 00:22:05,520
for data scientists

00:22:02,159 --> 00:22:07,120
and much learning engineers so for the

00:22:05,520 --> 00:22:09,039
submarine community

00:22:07,120 --> 00:22:10,799
um so currently we have a pretty

00:22:09,039 --> 00:22:13,919
diversified community

00:22:10,799 --> 00:22:17,280
um we we have a committee

00:22:13,919 --> 00:22:20,640
and contributors from linkedin facebook

00:22:17,280 --> 00:22:22,960
cadera alibaba netease and we have many

00:22:20,640 --> 00:22:24,559
contributors from many other companies

00:22:22,960 --> 00:22:27,679
currently we have 36

00:22:24,559 --> 00:22:28,400
code contributors which have any code

00:22:27,679 --> 00:22:31,600
merged

00:22:28,400 --> 00:22:34,960
in the in the community which is

00:22:31,600 --> 00:22:38,240
grow from 18 uh at the um

00:22:34,960 --> 00:22:41,520
beginning of 2020. so

00:22:38,240 --> 00:22:45,440
for the releases um for the last release

00:22:41,520 --> 00:22:47,440
is 0.4 release which supports deploy

00:22:45,440 --> 00:22:50,320
submarine server on kubernetes

00:22:47,440 --> 00:22:51,600
and this can also run single node

00:22:50,320 --> 00:22:54,240
distributed

00:22:51,600 --> 00:22:56,799
tensorflow and pi ports on kubernetes

00:22:54,240 --> 00:23:00,320
the ongoing release

00:22:56,799 --> 00:23:02,240
is 0.5 which we target um

00:23:00,320 --> 00:23:03,919
yeah so so this this is september to

00:23:02,240 --> 00:23:04,320
october but it's already october it's

00:23:03,919 --> 00:23:06,400
almost

00:23:04,320 --> 00:23:07,440
october now so we where we should be

00:23:06,400 --> 00:23:10,559
able to get this

00:23:07,440 --> 00:23:12,159
released sometime in middle or late

00:23:10,559 --> 00:23:14,559
october

00:23:12,159 --> 00:23:15,520
so what we demo here in the notebook

00:23:14,559 --> 00:23:18,880
support

00:23:15,520 --> 00:23:19,200
environment profile uis distance will be

00:23:18,880 --> 00:23:22,799
all

00:23:19,200 --> 00:23:24,720
included in the 0.5 release so for

00:23:22,799 --> 00:23:26,480
for the next we plan to have a 0.6

00:23:24,720 --> 00:23:29,600
release to enhance

00:23:26,480 --> 00:23:32,000
the um the the parts and

00:23:29,600 --> 00:23:34,640
for the better user user experiences and

00:23:32,000 --> 00:23:35,360
post 0.6 we are looking at a plan to do

00:23:34,640 --> 00:23:39,200
a ga

00:23:35,360 --> 00:23:42,480
so probably for the ga plan will be

00:23:39,200 --> 00:23:42,480
45 months from now

00:23:43,279 --> 00:23:47,200
so we have a bunch of community use

00:23:44,880 --> 00:23:50,320
cases for example

00:23:47,200 --> 00:23:53,360
netease is is one of the largest

00:23:50,320 --> 00:23:54,159
online gaming news and music provider in

00:23:53,360 --> 00:23:56,960
china

00:23:54,159 --> 00:23:57,760
so they are running uh running summary

00:23:56,960 --> 00:24:01,840
on yarn

00:23:57,760 --> 00:24:05,360
on two 45 gpu gpu nodes

00:24:01,840 --> 00:24:06,159
clustered so on the submarine they are

00:24:05,360 --> 00:24:09,679
building

00:24:06,159 --> 00:24:12,880
one of the major music

00:24:09,679 --> 00:24:15,279
recommendation model which is invoked

00:24:12,880 --> 00:24:16,159
more than one billion times a day and

00:24:15,279 --> 00:24:19,200
for the next thing

00:24:16,159 --> 00:24:21,360
they are running uh summary on 255

00:24:19,200 --> 00:24:22,240
gpu machines and they're also running on

00:24:21,360 --> 00:24:24,400
yarn

00:24:22,240 --> 00:24:25,600
they are they have 500 tensorflow

00:24:24,400 --> 00:24:29,200
training per day

00:24:25,600 --> 00:24:30,559
and this is a data um a while back

00:24:29,200 --> 00:24:32,960
probably

00:24:30,559 --> 00:24:33,919
nine months back and there are service

00:24:32,960 --> 00:24:37,360
applications

00:24:33,919 --> 00:24:39,520
in recommendation systems and nlp

00:24:37,360 --> 00:24:41,120
they collaborate with us on runtime and

00:24:39,520 --> 00:24:44,000
sdk development

00:24:41,120 --> 00:24:44,960
intensively and there's another company

00:24:44,000 --> 00:24:48,080
called the tong

00:24:44,960 --> 00:24:51,039
so they recently listed in nasdaq

00:24:48,080 --> 00:24:51,760
so it is the largest online real estate

00:24:51,039 --> 00:24:55,039
brokerage

00:24:51,760 --> 00:24:56,080
website in china they have 50 gpu

00:24:55,039 --> 00:24:59,279
machines

00:24:56,080 --> 00:25:00,880
and it is based on hadoop trunk so

00:24:59,279 --> 00:25:02,400
as they are served applications like

00:25:00,880 --> 00:25:05,120
image and the voice

00:25:02,400 --> 00:25:05,120
recognition

00:25:05,760 --> 00:25:08,880
yeah so that's pretty much for the demo

00:25:07,679 --> 00:25:11,679
and so

00:25:08,880 --> 00:25:12,799
here's our website and here is a github

00:25:11,679 --> 00:25:15,520
so

00:25:12,799 --> 00:25:17,679
if you have interest into trying summary

00:25:15,520 --> 00:25:20,480
and trying to join this community

00:25:17,679 --> 00:25:22,400
you will be always welcome and here are

00:25:20,480 --> 00:25:25,840
all the

00:25:22,400 --> 00:25:26,880
major contributors to this presentation

00:25:25,840 --> 00:25:28,159
and to this project

00:25:26,880 --> 00:25:29,919
and there are a lot more we haven't

00:25:28,159 --> 00:25:32,000
listed here and yet

00:25:29,919 --> 00:25:33,760
we have lots of people help to push the

00:25:32,000 --> 00:25:36,960
community forward

00:25:33,760 --> 00:25:43,840
so if you have any questions

00:25:36,960 --> 00:25:43,840
please feel free to ask

00:25:47,760 --> 00:25:57,840
so i think question will be asked in the

00:25:50,080 --> 00:25:57,840
chat right

00:26:04,720 --> 00:26:11,840
so any questions

00:26:14,480 --> 00:26:20,080
yeah so so i will be staying here for um

00:26:17,679 --> 00:26:21,840
another five minutes and so if you have

00:26:20,080 --> 00:26:24,960
any questions feel free to

00:26:21,840 --> 00:26:28,480
um post in the um

00:26:24,960 --> 00:26:29,279
in the event events uh in in a session

00:26:28,480 --> 00:26:30,960
chat

00:26:29,279 --> 00:26:33,120
okay so the first question is a lot of

00:26:30,960 --> 00:26:35,679
documents referred to jupiter

00:26:33,120 --> 00:26:36,400
uh their plans to include apache sibling

00:26:35,679 --> 00:26:38,400
as well

00:26:36,400 --> 00:26:40,559
so yeah this is a this is a good

00:26:38,400 --> 00:26:41,520
question so actually initially we are

00:26:40,559 --> 00:26:44,640
looking at

00:26:41,520 --> 00:26:45,120
zeppelin uh rather than jupiter but

00:26:44,640 --> 00:26:47,520
later

00:26:45,120 --> 00:26:49,039
we found the uh jupiter has very good

00:26:47,520 --> 00:26:53,279
integration for

00:26:49,039 --> 00:26:55,600
um data scientist tools and

00:26:53,279 --> 00:26:57,279
and zeppelin and compared to that has

00:26:55,600 --> 00:26:59,919
very good integration to spark

00:26:57,279 --> 00:27:00,960
and other data engineering and and

00:26:59,919 --> 00:27:04,159
engineering

00:27:00,960 --> 00:27:04,840
um and that flink as well so yeah

00:27:04,159 --> 00:27:08,000
assessing

00:27:04,840 --> 00:27:09,840
um to your question dave so um there is

00:27:08,000 --> 00:27:12,159
a plan to look into

00:27:09,840 --> 00:27:13,840
a patch deeply in the future um but we

00:27:12,159 --> 00:27:17,279
want to put some

00:27:13,840 --> 00:27:19,919
thieves um and give a useful

00:27:17,279 --> 00:27:20,880
notebook experiences to the users first

00:27:19,919 --> 00:27:24,080
before we

00:27:20,880 --> 00:27:27,840
exploring other local

00:27:24,080 --> 00:27:27,840
notebook projects

00:27:28,720 --> 00:27:31,760
yes thank you thanks

00:27:32,880 --> 00:27:36,480
any questions feel free to type in a

00:27:46,840 --> 00:27:49,840
chat

00:28:15,039 --> 00:28:19,200
yeah so there's one there's one thing um

00:28:17,600 --> 00:28:23,760
i actually state

00:28:19,200 --> 00:28:27,279
um maybe i can i can show this to you

00:28:23,760 --> 00:28:28,559
um and so there's a concept where i'm

00:28:27,279 --> 00:28:30,640
building right now

00:28:28,559 --> 00:28:31,760
it's called the predefined it's

00:28:30,640 --> 00:28:35,200
predefined

00:28:31,760 --> 00:28:37,200
experiment template um yeah so feel free

00:28:35,200 --> 00:28:40,880
to give your feedback for that

00:28:37,200 --> 00:28:43,279
and so for them for the normal training

00:28:40,880 --> 00:28:44,399
support when the user wants to run their

00:28:43,279 --> 00:28:46,960
application

00:28:44,399 --> 00:28:48,720
they will have to figure out what is

00:28:46,960 --> 00:28:51,120
their

00:28:48,720 --> 00:28:52,000
where is their training script located

00:28:51,120 --> 00:28:55,840
how many

00:28:52,000 --> 00:28:59,279
how much memory cpu to to use

00:28:55,840 --> 00:29:01,919
and what is their um their

00:28:59,279 --> 00:29:04,000
environment like the doctor image or

00:29:01,919 --> 00:29:07,120
python dependencies to use

00:29:04,000 --> 00:29:10,159
so that is still very trivial for

00:29:07,120 --> 00:29:12,960
data scientists to specify and it is

00:29:10,159 --> 00:29:13,279
it is not error proof it's very easy to

00:29:12,960 --> 00:29:16,799
uh

00:29:13,279 --> 00:29:19,760
specify some errors in the in the

00:29:16,799 --> 00:29:21,600
um in the training and the job could

00:29:19,760 --> 00:29:23,840
fail and they have to rerun this at the

00:29:21,600 --> 00:29:26,240
end again to figure out why

00:29:23,840 --> 00:29:28,080
so once you we are currently building is

00:29:26,240 --> 00:29:30,000
called predefined template

00:29:28,080 --> 00:29:31,120
uh this is probably a bit too late for

00:29:30,000 --> 00:29:33,679
the um

00:29:31,120 --> 00:29:35,600
0.5 but we are targeting this returning

00:29:33,679 --> 00:29:37,840
this to 0.6

00:29:35,600 --> 00:29:38,880
so what in the in the predefined

00:29:37,840 --> 00:29:41,520
template is

00:29:38,880 --> 00:29:43,279
it is just a normal script but we only

00:29:41,520 --> 00:29:45,520
allow user to change

00:29:43,279 --> 00:29:47,279
some of these parameters like where is

00:29:45,520 --> 00:29:49,760
the input data located

00:29:47,279 --> 00:29:50,640
and where is output data located and

00:29:49,760 --> 00:29:54,240
there are some

00:29:50,640 --> 00:29:57,279
training parameters you specify

00:29:54,240 --> 00:30:01,200
um this is for example fm

00:29:57,279 --> 00:30:04,640
ctr training and you submit this json

00:30:01,200 --> 00:30:07,919
json json file with these parameters

00:30:04,640 --> 00:30:11,120
to submarine and somebody can fill

00:30:07,919 --> 00:30:13,600
the deep fmctr applications

00:30:11,120 --> 00:30:15,200
to run so you don't have to worry about

00:30:13,600 --> 00:30:17,200
where is this code located

00:30:15,200 --> 00:30:20,720
you don't have to worry about where is

00:30:17,200 --> 00:30:23,760
this how much resource to use

00:30:20,720 --> 00:30:25,279
so this can be very easy to tune by by

00:30:23,760 --> 00:30:28,159
the data scientists

00:30:25,279 --> 00:30:36,640
um so yeah so this is something where

00:30:28,159 --> 00:30:39,200
we're also building right now

00:30:36,640 --> 00:30:41,200
okay so if there's no more questions we

00:30:39,200 --> 00:30:44,960
can probably end this

00:30:41,200 --> 00:30:46,320
um and this um session

00:30:44,960 --> 00:30:48,000
if you have any other questions feel

00:30:46,320 --> 00:30:51,039
free to

00:30:48,000 --> 00:30:54,640
send an email to submarine and

00:30:51,039 --> 00:30:58,320
apache email list or

00:30:54,640 --> 00:31:00,799
drop me some email like that or ping me

00:30:58,320 --> 00:31:00,799
on the chat

00:31:02,080 --> 00:31:05,840
okay cool thank you guys

00:32:18,080 --> 00:32:20,159

YouTube URL: https://www.youtube.com/watch?v=raXfcdLqPt0


