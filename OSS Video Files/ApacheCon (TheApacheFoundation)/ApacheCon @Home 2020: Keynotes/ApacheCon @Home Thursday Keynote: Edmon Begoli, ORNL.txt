Title: ApacheCon @Home Thursday Keynote: Edmon Begoli, ORNL
Publication date: 2020-10-09
Playlist: ApacheCon @Home 2020: Keynotes
Description: 
	Edmon Begoli

Oak Ridge National Labs


High Performance Computing with Apache Spark and Parquet on Mission Critical Tasks


Oak Ridge National Laboratory (ORNL) is known for its deployment of some of the world's fastest supercomputers. This legacy brings us opportunities to work on some of the most challenging societal problems. Often, these problems require approaches that are more comprehensive than what specific high-performance computing solutions can solve. In this talk, we will talk about the essential role that Apache Spark and Parquet played in solving some of these problems. We will illustrate Apache Spark and Parquet's uses with a case study related to suicide and overdose risk where prevention. The result is a 300x speedup in processing from 75+ hours for the original algorithm to 15 minutes with a new one. We will discuss specific techniques behind this accomplishment and the lessons learned.


Edmon Begoli, PhD works at Oak Ridge National Laboratory (ORNL), where he leads research and development programs aimed at scaling and improving the resilience of critical decision making.


Edmon is a committer with Apache Software Foundation, and is a joint faculty professor of Computer Science at the University of Tennessee, EECS department.


ApacheCon @Home 2020 - https://apachecon.com/acah2020/
The Apache Software Foundation - https://apache.org
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,720 --> 00:00:29,439
thank you for your patience

00:00:26,640 --> 00:00:30,160
and once again i'm really excited to

00:00:29,439 --> 00:00:32,000
introduce

00:00:30,160 --> 00:00:33,440
edmund who will be giving us our first

00:00:32,000 --> 00:00:37,040
keynote of the day

00:00:33,440 --> 00:00:38,879
and i won't take any of your time so

00:00:37,040 --> 00:00:40,559
when you're ready i will step off stage

00:00:38,879 --> 00:00:43,520
thank you thank you

00:00:40,559 --> 00:00:44,079
i'm ready and i want to thank you for

00:00:43,520 --> 00:00:46,320
the invite

00:00:44,079 --> 00:00:47,360
to give it this keynote and i am

00:00:46,320 --> 00:00:49,600
sincerely

00:00:47,360 --> 00:00:51,840
extremely excited to present it because

00:00:49,600 --> 00:00:54,239
you know even though i work

00:00:51,840 --> 00:00:56,079
uh at of course oakridge national lab

00:00:54,239 --> 00:00:57,840
the apache foundation and the

00:00:56,079 --> 00:00:59,440
entire movement and all the open source

00:00:57,840 --> 00:01:00,960
projects have been something very

00:00:59,440 --> 00:01:02,719
special to me and

00:01:00,960 --> 00:01:04,559
i'm very grateful for the community and

00:01:02,719 --> 00:01:06,880
for the work that came

00:01:04,559 --> 00:01:08,320
out of it so that my talk is going to be

00:01:06,880 --> 00:01:10,240
about the role of

00:01:08,320 --> 00:01:12,240
specifically apache parque and apache

00:01:10,240 --> 00:01:13,520
spark in the role that they played on

00:01:12,240 --> 00:01:15,600
our project

00:01:13,520 --> 00:01:17,600
in frequently mission critical tasks or

00:01:15,600 --> 00:01:17,920
at least a life impacting tasks so it's

00:01:17,600 --> 00:01:19,759
a

00:01:17,920 --> 00:01:21,840
it's it's a really important stuff and

00:01:19,759 --> 00:01:23,520
so um i want to share

00:01:21,840 --> 00:01:26,000
a couple of case studies and certainly

00:01:23,520 --> 00:01:28,960
how did we get to the point of using

00:01:26,000 --> 00:01:29,520
those projects using those libraries on

00:01:28,960 --> 00:01:32,799
the project

00:01:29,520 --> 00:01:35,200
so um a quick outline what i'm going to

00:01:32,799 --> 00:01:37,119
tell you about and that is

00:01:35,200 --> 00:01:38,880
a background on a little bit about who

00:01:37,119 --> 00:01:40,240
am i but more importantly what is oak

00:01:38,880 --> 00:01:41,200
ridge national live bridge has given

00:01:40,240 --> 00:01:44,320
really

00:01:41,200 --> 00:01:45,680
a a great a few sentences intro i'm

00:01:44,320 --> 00:01:47,360
going to expand on it a little bit

00:01:45,680 --> 00:01:49,200
because you know there's this dichotomy

00:01:47,360 --> 00:01:52,720
of high performance computing shop

00:01:49,200 --> 00:01:55,520
and use of not necessarily

00:01:52,720 --> 00:01:57,280
hpc traditional libraries and i'm going

00:01:55,520 --> 00:01:58,799
to talk about the

00:01:57,280 --> 00:02:00,479
i think it's an interesting story talk

00:01:58,799 --> 00:02:02,000
about how did we got to use actually

00:02:00,479 --> 00:02:03,840
spark and parquet because it had

00:02:02,000 --> 00:02:05,200
such a significant impact on the

00:02:03,840 --> 00:02:07,360
projects and yet

00:02:05,200 --> 00:02:08,640
uh it came at the right time when he

00:02:07,360 --> 00:02:11,200
needed it

00:02:08,640 --> 00:02:12,720
and i'm gonna connect it to what i call

00:02:11,200 --> 00:02:13,520
at least in some of the papers i wrote

00:02:12,720 --> 00:02:16,720
magic

00:02:13,520 --> 00:02:19,280
triangle of analytics because that

00:02:16,720 --> 00:02:21,280
ultimately combination of write open

00:02:19,280 --> 00:02:24,480
source tools and algorithms

00:02:21,280 --> 00:02:26,959
result in some serious

00:02:24,480 --> 00:02:28,400
advancements on our projects when i say

00:02:26,959 --> 00:02:29,920
our project these are usually nationally

00:02:28,400 --> 00:02:30,879
significant projects i'm going to touch

00:02:29,920 --> 00:02:33,840
on that

00:02:30,879 --> 00:02:35,599
and what were our experiences through a

00:02:33,840 --> 00:02:39,040
particular case study

00:02:35,599 --> 00:02:40,959
so briefly about myself um

00:02:39,040 --> 00:02:42,400
just briefly i work at oak ridge

00:02:40,959 --> 00:02:44,720
national lab as a

00:02:42,400 --> 00:02:46,800
as a scientist i'm also connected to

00:02:44,720 --> 00:02:48,720
academia where i teach as a joint

00:02:46,800 --> 00:02:50,080
faculty professor of computer science at

00:02:48,720 --> 00:02:52,800
university of tennessee

00:02:50,080 --> 00:02:53,200
and i'm also visiting scientist scholar

00:02:52,800 --> 00:02:55,760
at

00:02:53,200 --> 00:02:56,640
uc berkeley's prize lab which has a

00:02:55,760 --> 00:02:58,400
significance

00:02:56,640 --> 00:03:00,400
and the reason why i'm there is still a

00:02:58,400 --> 00:03:01,760
lot to do with with our experiences with

00:03:00,400 --> 00:03:03,360
sparks so far

00:03:01,760 --> 00:03:05,519
and i'm a member of apache software

00:03:03,360 --> 00:03:07,360
foundation committed on a

00:03:05,519 --> 00:03:10,720
apache call side project and certainly

00:03:07,360 --> 00:03:10,720
connected to other related

00:03:10,959 --> 00:03:16,080
projects and i have had experienced an

00:03:14,720 --> 00:03:17,440
industrial reason i bring this up is

00:03:16,080 --> 00:03:19,200
because it's kind of good to know what's

00:03:17,440 --> 00:03:19,920
happening in industry not just strictly

00:03:19,200 --> 00:03:23,440
live

00:03:19,920 --> 00:03:25,760
in the uh ivory tower of research

00:03:23,440 --> 00:03:26,560
but more interestingly uh for some of

00:03:25,760 --> 00:03:29,760
you that don't know

00:03:26,560 --> 00:03:31,680
what what is oak ridge national lab um

00:03:29,760 --> 00:03:33,840
it's it was formed in the hills of

00:03:31,680 --> 00:03:35,440
tennessee uh in the middle of november

00:03:33,840 --> 00:03:37,760
and i still say it looks like an

00:03:35,440 --> 00:03:38,480
alien ship landed in the middle of

00:03:37,760 --> 00:03:40,239
nowhere

00:03:38,480 --> 00:03:41,760
it was formed in 1943 during the

00:03:40,239 --> 00:03:44,319
manhattan project and it was

00:03:41,760 --> 00:03:45,440
purposefully placed far away from nazi

00:03:44,319 --> 00:03:48,400
spies

00:03:45,440 --> 00:03:49,599
uh to where it was um and it was a place

00:03:48,400 --> 00:03:51,360
of the first

00:03:49,599 --> 00:03:53,200
uh continuously operating nuclear

00:03:51,360 --> 00:03:56,640
reactor and this is how it looked like

00:03:53,200 --> 00:03:58,840
uh in 1943 uh and this is how it looked

00:03:56,640 --> 00:04:00,000
like it looks like today it's a much

00:03:58,840 --> 00:04:03,040
larger

00:04:00,000 --> 00:04:05,680
institution it's a largest multi

00:04:03,040 --> 00:04:07,360
multi-purpose research lab within doe

00:04:05,680 --> 00:04:09,280
complex united states

00:04:07,360 --> 00:04:11,360
and you can still see here in the right

00:04:09,280 --> 00:04:12,720
two chimney stacks i'm not sure if

00:04:11,360 --> 00:04:14,239
there's exactly the same ones

00:04:12,720 --> 00:04:15,680
like the one on previous picture i think

00:04:14,239 --> 00:04:17,199
they are because that's the site where

00:04:15,680 --> 00:04:18,799
everything developed from

00:04:17,199 --> 00:04:20,639
and today we have four thousand six

00:04:18,799 --> 00:04:21,680
hundred people and three thousand some

00:04:20,639 --> 00:04:24,880
researchers

00:04:21,680 --> 00:04:26,960
uh very inclusive inviting

00:04:24,880 --> 00:04:29,360
open research place or lots of

00:04:26,960 --> 00:04:32,240
interesting things happen

00:04:29,360 --> 00:04:34,080
but in terms of where i come from and

00:04:32,240 --> 00:04:38,000
what

00:04:34,080 --> 00:04:38,720
perhaps matters the to the subject of of

00:04:38,000 --> 00:04:42,400
computing

00:04:38,720 --> 00:04:44,560
is that many know rnl by its

00:04:42,400 --> 00:04:45,440
high performance computing progress so

00:04:44,560 --> 00:04:48,240
you know

00:04:45,440 --> 00:04:49,280
department of energy is the american

00:04:48,240 --> 00:04:52,479
custodian

00:04:49,280 --> 00:04:53,280
of computing excellence and runs the

00:04:52,479 --> 00:04:54,720
largest

00:04:53,280 --> 00:04:56,240
high performance computing systems

00:04:54,720 --> 00:04:58,240
united states and usually it's in the

00:04:56,240 --> 00:04:59,840
world and then these rankings change

00:04:58,240 --> 00:05:02,160
so historically it's been like that it's

00:04:59,840 --> 00:05:04,800
been you know even before with that list

00:05:02,160 --> 00:05:06,479
here 2010 we had a jaguar system then

00:05:04,800 --> 00:05:07,360
titan and titan was the first one that

00:05:06,479 --> 00:05:10,080
used

00:05:07,360 --> 00:05:10,960
gpus and cpus combined and at a time it

00:05:10,080 --> 00:05:12,720
was for

00:05:10,960 --> 00:05:14,479
power saving believe it or not because

00:05:12,720 --> 00:05:16,479
gpus were more efficient for certain

00:05:14,479 --> 00:05:18,240
types of computation and then

00:05:16,479 --> 00:05:19,520
we all don't claim that we were that

00:05:18,240 --> 00:05:22,240
smart to come up with

00:05:19,520 --> 00:05:24,160
gpus for deep learning but it happened

00:05:22,240 --> 00:05:26,240
you know like i said it didn't start uh

00:05:24,160 --> 00:05:27,840
by design it happened by the

00:05:26,240 --> 00:05:30,320
in anyway so summit is our current

00:05:27,840 --> 00:05:32,320
system it's very large it has a 27 000

00:05:30,320 --> 00:05:33,360
gpus it was the fastest super computer

00:05:32,320 --> 00:05:36,320
in the world up until

00:05:33,360 --> 00:05:36,880
two months ago and then it got seated by

00:05:36,320 --> 00:05:39,360
um

00:05:36,880 --> 00:05:41,120
a japanese system and that's how these

00:05:39,360 --> 00:05:44,080
things work and we're right now

00:05:41,120 --> 00:05:45,520
in a process of developing uh deploying

00:05:44,080 --> 00:05:46,800
frontier which is going to be the access

00:05:45,520 --> 00:05:49,600
scale system that has a

00:05:46,800 --> 00:05:51,360
you know tremendous parallelization

00:05:49,600 --> 00:05:53,680
billion billion way parallelism

00:05:51,360 --> 00:05:55,600
and it's going to be the as far as i

00:05:53,680 --> 00:05:58,880
know the first exaskill system

00:05:55,600 --> 00:06:01,120
operating ssql system um and

00:05:58,880 --> 00:06:03,280
these are great systems for modeling and

00:06:01,120 --> 00:06:05,759
simulation and increasingly running some

00:06:03,280 --> 00:06:09,039
large-scale ai jobs

00:06:05,759 --> 00:06:09,520
but in reality the projects that i got

00:06:09,039 --> 00:06:12,960
to

00:06:09,520 --> 00:06:16,160
get involved on um were different

00:06:12,960 --> 00:06:19,199
so it started in 2008 9

00:06:16,160 --> 00:06:20,800
10 really thin with us being approached

00:06:19,199 --> 00:06:22,479
by centers for medicare medicaid

00:06:20,800 --> 00:06:24,319
services to help us with the fraud

00:06:22,479 --> 00:06:27,280
problem and you can see here from

00:06:24,319 --> 00:06:28,160
the economist article title 272 billion

00:06:27,280 --> 00:06:29,759
dollar swindle

00:06:28,160 --> 00:06:31,199
the massive fraud problem and they

00:06:29,759 --> 00:06:33,840
wanted us to help them

00:06:31,199 --> 00:06:34,479
analyze and find and address some of

00:06:33,840 --> 00:06:37,600
those

00:06:34,479 --> 00:06:38,880
some of those issues then um five six

00:06:37,600 --> 00:06:41,440
years later we were approached by the

00:06:38,880 --> 00:06:44,000
department of veteran affairs

00:06:41,440 --> 00:06:46,000
that runs the large world's largest uh

00:06:44,000 --> 00:06:49,199
mega bio bank called mvp

00:06:46,000 --> 00:06:50,080
to see how we can help them manage and

00:06:49,199 --> 00:06:53,919
run and

00:06:50,080 --> 00:06:56,160
really use use the data for a important

00:06:53,919 --> 00:06:57,440
purposes answering some critical

00:06:56,160 --> 00:07:02,160
research problems

00:06:57,440 --> 00:07:05,360
including um the suicide epidemic

00:07:02,160 --> 00:07:08,000
prior to pandemic of covet 19 2

00:07:05,360 --> 00:07:08,880
things that we were all looking in were

00:07:08,000 --> 00:07:11,199
opioid

00:07:08,880 --> 00:07:12,000
epidemic and suicide and so this is what

00:07:11,199 --> 00:07:16,160
we worked on

00:07:12,000 --> 00:07:16,880
up until covet 19 problems started and

00:07:16,160 --> 00:07:18,479
so and then

00:07:16,880 --> 00:07:20,479
in addition to solving some of those

00:07:18,479 --> 00:07:21,280
critical societal problems it's also

00:07:20,479 --> 00:07:23,520
advancing the

00:07:21,280 --> 00:07:25,120
state of research working with all these

00:07:23,520 --> 00:07:26,560
data sets there's all kinds of

00:07:25,120 --> 00:07:28,240
important things that can be derived

00:07:26,560 --> 00:07:31,120
from

00:07:28,240 --> 00:07:32,560
so the historically the problems that so

00:07:31,120 --> 00:07:33,280
so the point of previous slides is rule

00:07:32,560 --> 00:07:35,039
to say that

00:07:33,280 --> 00:07:36,479
even though we have you had those high

00:07:35,039 --> 00:07:37,680
performance computing systems at that

00:07:36,479 --> 00:07:39,759
time we had a titan

00:07:37,680 --> 00:07:41,520
and it could solve problems that nobody

00:07:39,759 --> 00:07:43,199
else on the planet could

00:07:41,520 --> 00:07:44,560
the issue with those kinds of systems

00:07:43,199 --> 00:07:47,440
was that they were not

00:07:44,560 --> 00:07:49,280
optimized for solving problems that our

00:07:47,440 --> 00:07:50,800
partner agencies brought to us to solve

00:07:49,280 --> 00:07:54,080
and these were critical national

00:07:50,800 --> 00:07:55,680
or global problems and in terms of what

00:07:54,080 --> 00:07:57,759
what for the typical promise that we had

00:07:55,680 --> 00:07:58,479
to solve at least from my point of view

00:07:57,759 --> 00:08:00,479
and what my

00:07:58,479 --> 00:08:02,160
expertise was and the expertise of my

00:08:00,479 --> 00:08:04,879
team was for

00:08:02,160 --> 00:08:05,759
us and my team it's really a team called

00:08:04,879 --> 00:08:08,720
pierce

00:08:05,759 --> 00:08:09,919
um typical these agencies would bring

00:08:08,720 --> 00:08:12,400
data and they would need

00:08:09,919 --> 00:08:13,759
answers either faster or to answer

00:08:12,400 --> 00:08:15,120
questions better or they

00:08:13,759 --> 00:08:16,879
they knew there was something in it that

00:08:15,120 --> 00:08:19,280
just didn't know i mean typical kind of

00:08:16,879 --> 00:08:20,160
archetypical really a a data science

00:08:19,280 --> 00:08:23,360
problem

00:08:20,160 --> 00:08:24,720
uh the data was usually uh or always is

00:08:23,360 --> 00:08:26,400
tabular

00:08:24,720 --> 00:08:28,319
um it's either comes out a relational

00:08:26,400 --> 00:08:30,879
database or

00:08:28,319 --> 00:08:32,800
it's flat files that can be converted to

00:08:30,879 --> 00:08:33,680
some kind of code or format but it's

00:08:32,800 --> 00:08:35,360
unlinked

00:08:33,680 --> 00:08:37,519
and it's at the very large volumes you

00:08:35,360 --> 00:08:38,880
know this is not a facebook and twitter

00:08:37,519 --> 00:08:41,919
volumes that you have

00:08:38,880 --> 00:08:43,680
you know billions of tweets or clicks or

00:08:41,919 --> 00:08:45,920
something but this is a

00:08:43,680 --> 00:08:47,760
have here any informational very dense

00:08:45,920 --> 00:08:49,680
data i mean these are for instance

00:08:47,760 --> 00:08:51,760
a medicare claim where every field

00:08:49,680 --> 00:08:54,160
matters correlation between the

00:08:51,760 --> 00:08:55,839
age and gender and some other

00:08:54,160 --> 00:08:57,120
demographic is critically important for

00:08:55,839 --> 00:08:59,040
instance for conducting

00:08:57,120 --> 00:09:00,800
breast cancer prostate cancer studies

00:08:59,040 --> 00:09:02,240
and then you have thousands of fields

00:09:00,800 --> 00:09:03,279
like that so it's a very informational

00:09:02,240 --> 00:09:05,120
dentist there's no

00:09:03,279 --> 00:09:06,720
much noise in this data it's all very

00:09:05,120 --> 00:09:09,360
very useful it's just

00:09:06,720 --> 00:09:10,959
how do you how do you use it and that's

00:09:09,360 --> 00:09:12,560
really what i call the material for

00:09:10,959 --> 00:09:16,399
discovery we needed to to

00:09:12,560 --> 00:09:19,519
this this is called like a muddy pond

00:09:16,399 --> 00:09:21,120
um a parable is that is that you know

00:09:19,519 --> 00:09:22,320
you have all this like in classical

00:09:21,120 --> 00:09:24,240
belgian microsoft

00:09:22,320 --> 00:09:25,600
microscope was invented you have to

00:09:24,240 --> 00:09:26,880
discover something you know that there

00:09:25,600 --> 00:09:28,880
is something in that

00:09:26,880 --> 00:09:30,240
muddy water you just don't know what all

00:09:28,880 --> 00:09:32,160
organisms are there

00:09:30,240 --> 00:09:33,760
and in words of some of our partner

00:09:32,160 --> 00:09:35,600
agency leads you know they call

00:09:33,760 --> 00:09:37,200
themselves data rich and knowledge poor

00:09:35,600 --> 00:09:39,279
and they needed help and they needed

00:09:37,200 --> 00:09:41,920
help in discovering that knowledge and

00:09:39,279 --> 00:09:44,000
they needed help in in some instances

00:09:41,920 --> 00:09:47,600
like case studies solving some

00:09:44,000 --> 00:09:50,640
critical problems the reality that we

00:09:47,600 --> 00:09:51,360
had in 2010 that we started from was

00:09:50,640 --> 00:09:54,320
that

00:09:51,360 --> 00:09:56,160
um we had two choices and we really

00:09:54,320 --> 00:09:58,959
experimented with both

00:09:56,160 --> 00:09:59,920
one was hadoop we were all very excited

00:09:58,959 --> 00:10:03,120
we were very

00:09:59,920 --> 00:10:04,880
early adopters of of hadoop and

00:10:03,120 --> 00:10:07,360
specifically hive because

00:10:04,880 --> 00:10:08,959
given the nature of the data and type of

00:10:07,360 --> 00:10:11,600
analysis they had to do

00:10:08,959 --> 00:10:13,040
or go with some massive massively

00:10:11,600 --> 00:10:14,800
parallel processing database

00:10:13,040 --> 00:10:16,880
there were many commercial offerings and

00:10:14,800 --> 00:10:19,360
they come in cabinets and

00:10:16,880 --> 00:10:21,519
they cost millions of dollars and the

00:10:19,360 --> 00:10:22,800
issue with going with mpp commercial mpp

00:10:21,519 --> 00:10:23,680
is that it's very much like a mainframe

00:10:22,800 --> 00:10:25,200
you get locked in

00:10:23,680 --> 00:10:27,440
especially once you put your data and

00:10:25,200 --> 00:10:28,320
you get locked in and pay lots of money

00:10:27,440 --> 00:10:30,880
and you're on it

00:10:28,320 --> 00:10:32,000
for a long long time and the question is

00:10:30,880 --> 00:10:34,320
how much can you really do

00:10:32,000 --> 00:10:35,680
with it in addition to just doing

00:10:34,320 --> 00:10:37,360
standard business intelligence

00:10:35,680 --> 00:10:39,040
on the hadoop side it was much more

00:10:37,360 --> 00:10:40,640
flexible free

00:10:39,040 --> 00:10:41,839
i remember approaching the cms at the

00:10:40,640 --> 00:10:43,839
time and says we're going to build you a

00:10:41,839 --> 00:10:45,360
zero dollar architecture and we did

00:10:43,839 --> 00:10:47,040
but the thing with adobe at that time

00:10:45,360 --> 00:10:49,279
was i mean still

00:10:47,040 --> 00:10:51,519
to some degree holds was it was you know

00:10:49,279 --> 00:10:54,959
engineering intensive a little messy

00:10:51,519 --> 00:10:56,399
slow um queries would run for days and

00:10:54,959 --> 00:10:57,440
yes we would get answers that nobody

00:10:56,399 --> 00:10:59,680
else could but

00:10:57,440 --> 00:11:01,360
it took some time and so always an

00:10:59,680 --> 00:11:02,880
unsatisfying feel like wish there is

00:11:01,360 --> 00:11:05,680
something better

00:11:02,880 --> 00:11:07,440
and we were like hawks watching the

00:11:05,680 --> 00:11:09,120
publications and interacting with the

00:11:07,440 --> 00:11:11,040
peers in the community

00:11:09,120 --> 00:11:12,480
and really two things that at least

00:11:11,040 --> 00:11:16,079
flashed in front of my eyes

00:11:12,480 --> 00:11:17,680
very encouragingly were a work that came

00:11:16,079 --> 00:11:20,000
out of amplab

00:11:17,680 --> 00:11:21,600
um spark i remember just sitting and

00:11:20,000 --> 00:11:23,279
watching their web pages and

00:11:21,600 --> 00:11:24,640
trying to follow what's going on with

00:11:23,279 --> 00:11:27,120
them and

00:11:24,640 --> 00:11:28,560
uh spark was announced there's mata

00:11:27,120 --> 00:11:32,160
zahari and others

00:11:28,560 --> 00:11:33,839
dr stoica students um started publishing

00:11:32,160 --> 00:11:36,720
their work and presenting

00:11:33,839 --> 00:11:38,399
and we've seen these figures comparison

00:11:36,720 --> 00:11:40,079
between spark and hadoop and how much

00:11:38,399 --> 00:11:40,640
faster it is and made total sense to us

00:11:40,079 --> 00:11:42,320
because

00:11:40,640 --> 00:11:44,160
you know almost like wish we were the

00:11:42,320 --> 00:11:46,160
ones writing it's just because we felt

00:11:44,160 --> 00:11:47,680
that was the right thing to do and then

00:11:46,160 --> 00:11:48,560
at the same time google dremel paper

00:11:47,680 --> 00:11:51,040
came out

00:11:48,560 --> 00:11:51,839
and then afterwards julian led them from

00:11:51,040 --> 00:11:55,360
of course

00:11:51,839 --> 00:11:58,959
apache uh parque started

00:11:55,360 --> 00:12:00,720
parquet open source rewrite of dremel

00:11:58,959 --> 00:12:02,880
and i'll learn through actually looking

00:12:00,720 --> 00:12:04,079
at apache impala because that was an mpp

00:12:02,880 --> 00:12:05,519
that was open source that's how i looked

00:12:04,079 --> 00:12:07,360
for it like is there any

00:12:05,519 --> 00:12:08,560
open source mpp rather than paying

00:12:07,360 --> 00:12:11,680
millions of dollars

00:12:08,560 --> 00:12:13,120
and so those two things really looked

00:12:11,680 --> 00:12:14,480
very very promising i remember how

00:12:13,120 --> 00:12:15,760
impressed i was specifically with the

00:12:14,480 --> 00:12:17,519
dremel paper and and

00:12:15,760 --> 00:12:19,200
approached that implant which is now

00:12:17,519 --> 00:12:20,800
rise live at berkeley start that's why

00:12:19,200 --> 00:12:21,839
i'm affiliated now because i really

00:12:20,800 --> 00:12:24,399
really

00:12:21,839 --> 00:12:27,040
was so grateful and impressed by the

00:12:24,399 --> 00:12:30,079
what employed did for the community

00:12:27,040 --> 00:12:32,880
um so our early work was we took

00:12:30,079 --> 00:12:33,920
spark and put it on a hpc system

00:12:32,880 --> 00:12:36,720
implemented it

00:12:33,920 --> 00:12:38,800
this is on the left through the pbs you

00:12:36,720 --> 00:12:39,839
know put it on a classical hpc scheduler

00:12:38,800 --> 00:12:42,560
and queueing system

00:12:39,839 --> 00:12:44,079
and ran it and run it on a large memory

00:12:42,560 --> 00:12:44,880
system and we've seen what you see here

00:12:44,079 --> 00:12:46,959
in the bottom

00:12:44,880 --> 00:12:48,320
consistent high performance in the large

00:12:46,959 --> 00:12:50,480
block memory just

00:12:48,320 --> 00:12:52,320
we've seen that spark is kind of memory

00:12:50,480 --> 00:12:54,320
has an affinity for for lots of memory

00:12:52,320 --> 00:12:54,959
but it performs really well and you can

00:12:54,320 --> 00:12:57,680
run it

00:12:54,959 --> 00:12:58,399
on the hpc platform and then two working

00:12:57,680 --> 00:13:00,639
with this

00:12:58,399 --> 00:13:02,160
industry role and industry colleagues we

00:13:00,639 --> 00:13:04,800
also use the apache per k

00:13:02,160 --> 00:13:06,240
through apache drill to for instance

00:13:04,800 --> 00:13:08,320
solve the problem you know in

00:13:06,240 --> 00:13:11,040
in healthcare you have a problem of this

00:13:08,320 --> 00:13:12,720
byzantine ehr electronic health records

00:13:11,040 --> 00:13:14,480
system where each one have a different

00:13:12,720 --> 00:13:15,839
format they're all ultimately structured

00:13:14,480 --> 00:13:17,200
data but they put it in different

00:13:15,839 --> 00:13:20,399
repositories different

00:13:17,200 --> 00:13:22,160
different layouts and we experimented

00:13:20,399 --> 00:13:24,959
how to pull all of that

00:13:22,160 --> 00:13:26,240
and rather than doing some complicated

00:13:24,959 --> 00:13:29,120
you know

00:13:26,240 --> 00:13:31,120
real-time integration we would collapse

00:13:29,120 --> 00:13:32,480
it all into parquet put a drill on top

00:13:31,120 --> 00:13:34,320
and be able to query

00:13:32,480 --> 00:13:36,399
and still do these conversions very

00:13:34,320 --> 00:13:38,480
quickly and make it available and so

00:13:36,399 --> 00:13:39,519
that was that's where drill and parquet

00:13:38,480 --> 00:13:40,720
were just

00:13:39,519 --> 00:13:42,000
amazing and i couldn't believe the

00:13:40,720 --> 00:13:43,440
numbers i remember i was running on

00:13:42,000 --> 00:13:46,079
command line

00:13:43,440 --> 00:13:46,880
some conversions from csv to relational

00:13:46,079 --> 00:13:49,760
to parquet

00:13:46,880 --> 00:13:51,440
and queries and i really thought i'm

00:13:49,760 --> 00:13:53,279
doing something wrong because

00:13:51,440 --> 00:13:54,959
one is running 40 seconds another one is

00:13:53,279 --> 00:13:56,959
running zero point some

00:13:54,959 --> 00:13:58,320
and so to make sure that i'm not

00:13:56,959 --> 00:14:02,320
completely crazy

00:13:58,320 --> 00:14:05,040
um colleagues and myself um

00:14:02,320 --> 00:14:05,440
did the experiment and so we tested how

00:14:05,040 --> 00:14:08,720
much

00:14:05,440 --> 00:14:10,880
parquet compresses the data how much

00:14:08,720 --> 00:14:13,839
parquet speeds up in this case it was

00:14:10,880 --> 00:14:15,839
really orc and per case orc is another

00:14:13,839 --> 00:14:17,360
optimized row column columnar format

00:14:15,839 --> 00:14:18,800
also trapachi through

00:14:17,360 --> 00:14:20,800
phoenix i believe in highway you know

00:14:18,800 --> 00:14:22,320
using it um

00:14:20,800 --> 00:14:24,320
we did a formal experiment published

00:14:22,320 --> 00:14:25,920
this paper to show the improvements and

00:14:24,320 --> 00:14:28,800
as you can see here just it's an

00:14:25,920 --> 00:14:29,120
orders of magnitude difference and i'm

00:14:28,800 --> 00:14:30,880
still

00:14:29,120 --> 00:14:32,639
amazed and we're still reaping benefits

00:14:30,880 --> 00:14:34,320
and so the second half of

00:14:32,639 --> 00:14:36,800
this short keynote is going to be a

00:14:34,320 --> 00:14:39,519
little bit about what are these benefits

00:14:36,800 --> 00:14:41,040
but overall and at the time i had a both

00:14:39,519 --> 00:14:43,680
research and industry

00:14:41,040 --> 00:14:45,440
role uh when i thought about when people

00:14:43,680 --> 00:14:46,560
people talk about analytics and use of

00:14:45,440 --> 00:14:49,760
all kinds of machine

00:14:46,560 --> 00:14:51,519
learning tools i really thought of now

00:14:49,760 --> 00:14:53,040
equipped with all these components what

00:14:51,519 --> 00:14:55,839
is the a

00:14:53,040 --> 00:14:56,880
really strong approach to doing

00:14:55,839 --> 00:14:59,519
analytics

00:14:56,880 --> 00:15:00,079
in an optimal or as optimal as possible

00:14:59,519 --> 00:15:03,440
fashion

00:15:00,079 --> 00:15:04,959
and it's a three axis or three

00:15:03,440 --> 00:15:07,040
components or three dimensions i call

00:15:04,959 --> 00:15:09,040
this a magic triangle of analytics

00:15:07,040 --> 00:15:10,560
one is of course analytic methods and

00:15:09,040 --> 00:15:12,480
algorithms that's what most people think

00:15:10,560 --> 00:15:14,800
about upfront is that what kind of

00:15:12,480 --> 00:15:17,440
machine learning algorithm is going to

00:15:14,800 --> 00:15:18,639
be applied or exploratory data analysis

00:15:17,440 --> 00:15:20,480
and so on

00:15:18,639 --> 00:15:22,399
two is really apparel and distributed

00:15:20,480 --> 00:15:23,920
platforms that's a another critical

00:15:22,399 --> 00:15:24,800
component and that is being able to

00:15:23,920 --> 00:15:28,079
scale

00:15:24,800 --> 00:15:28,800
up and wide and paralyzed processing and

00:15:28,079 --> 00:15:30,720
then three

00:15:28,800 --> 00:15:32,480
and this is the most overlooked one is

00:15:30,720 --> 00:15:34,079
data placement format this is for me

00:15:32,480 --> 00:15:34,639
most revealing one and that's the role

00:15:34,079 --> 00:15:37,920
that parque

00:15:34,639 --> 00:15:40,560
played and spark is using it um

00:15:37,920 --> 00:15:42,160
how to store data in a way that it's

00:15:40,560 --> 00:15:45,759
fast to read fast to scan

00:15:42,160 --> 00:15:48,560
fast to compute against and in case of

00:15:45,759 --> 00:15:50,160
our experiences you know in this case

00:15:48,560 --> 00:15:52,079
spark was giving us through it's

00:15:50,160 --> 00:15:53,360
directed at sickle graph or dag based

00:15:52,079 --> 00:15:57,600
computation

00:15:53,360 --> 00:15:58,720
uh a opportunity to experiment with

00:15:57,600 --> 00:16:00,160
various different

00:15:58,720 --> 00:16:01,920
improvements to the algorithms and of

00:16:00,160 --> 00:16:03,360
course use the data science and aiml

00:16:01,920 --> 00:16:04,880
libraries that we can do either through

00:16:03,360 --> 00:16:09,120
spark or outside

00:16:04,880 --> 00:16:11,279
but it gives us that platform um

00:16:09,120 --> 00:16:12,880
we use spark as a parallel distributed

00:16:11,279 --> 00:16:14,160
platform and this is in addition to mpi

00:16:12,880 --> 00:16:16,480
that we do in openmp

00:16:14,160 --> 00:16:17,759
anyway through hpc world and then now we

00:16:16,480 --> 00:16:19,759
are also looking to task

00:16:17,759 --> 00:16:20,800
but as really as a as a general purpose

00:16:19,759 --> 00:16:23,040
starting point

00:16:20,800 --> 00:16:24,959
spark is great and then finally data

00:16:23,040 --> 00:16:26,639
placement formats and this is the parkay

00:16:24,959 --> 00:16:29,600
and arrow as i said earlier

00:16:26,639 --> 00:16:30,800
really shown and given us so all three

00:16:29,600 --> 00:16:34,160
together

00:16:30,800 --> 00:16:36,320
i kind of felt that once we put analysis

00:16:34,160 --> 00:16:38,240
in the context or we improve in all

00:16:36,320 --> 00:16:41,759
three areas that something

00:16:38,240 --> 00:16:44,800
important uh could happen

00:16:41,759 --> 00:16:48,079
additionally what really helped us with

00:16:44,800 --> 00:16:51,040
parquet and uh spark

00:16:48,079 --> 00:16:52,000
is that for some of our life sciences

00:16:51,040 --> 00:16:54,079
projects or

00:16:52,000 --> 00:16:56,000
you know human research related projects

00:16:54,079 --> 00:16:58,639
it created this unifying

00:16:56,000 --> 00:17:00,399
platform that brings together two core

00:16:58,639 --> 00:17:02,800
workflows and one is

00:17:00,399 --> 00:17:04,559
data engineering workflow taking data

00:17:02,800 --> 00:17:06,319
from its native

00:17:04,559 --> 00:17:07,600
uh repositories which we received from

00:17:06,319 --> 00:17:11,199
who knows there you know you can get

00:17:07,600 --> 00:17:14,959
data from va you can get it from nih cms

00:17:11,199 --> 00:17:17,600
uh clearing houses but having

00:17:14,959 --> 00:17:18,000
knowing that we have a workflow to take

00:17:17,600 --> 00:17:20,720
it from

00:17:18,000 --> 00:17:21,679
that native format and put it in parquet

00:17:20,720 --> 00:17:23,760
and then use

00:17:21,679 --> 00:17:25,280
various different ai and ml methods and

00:17:23,760 --> 00:17:29,679
we have machines for that to

00:17:25,280 --> 00:17:32,000
refine the data so that you know we drop

00:17:29,679 --> 00:17:33,600
or optimize the data structures and then

00:17:32,000 --> 00:17:35,120
the second and then prepare it for data

00:17:33,600 --> 00:17:36,480
science and then the second workflow is

00:17:35,120 --> 00:17:38,799
what i call data science workflow and

00:17:36,480 --> 00:17:40,480
that is now that we put this data that

00:17:38,799 --> 00:17:44,080
make data available to both

00:17:40,480 --> 00:17:46,880
hpc classical hpc or big data platforms

00:17:44,080 --> 00:17:47,919
uh we enable data scientists to use our

00:17:46,880 --> 00:17:50,640
python

00:17:47,919 --> 00:17:52,080
scala java whatever that they wish to do

00:17:50,640 --> 00:17:53,600
and oh in open source

00:17:52,080 --> 00:17:56,240
this was a big thing because many of

00:17:53,600 --> 00:17:58,880
these organizations are so locked onto

00:17:56,240 --> 00:18:00,799
uh very expensive commercial software in

00:17:58,880 --> 00:18:03,200
our case we gave them

00:18:00,799 --> 00:18:04,960
everything they needed through the

00:18:03,200 --> 00:18:08,400
really in open source

00:18:04,960 --> 00:18:10,000
um and apache specifically tool sets

00:18:08,400 --> 00:18:11,919
and gave them freedom to do it either

00:18:10,000 --> 00:18:13,120
hpc way or big data when big data in

00:18:11,919 --> 00:18:15,760
this case was really

00:18:13,120 --> 00:18:17,760
sparking hdfs back back and over time we

00:18:15,760 --> 00:18:20,240
have evolved over hdfs but

00:18:17,760 --> 00:18:23,039
you know it was a great inexpensive

00:18:20,240 --> 00:18:26,480
starting point

00:18:23,039 --> 00:18:28,640
so um i'm gonna then

00:18:26,480 --> 00:18:30,000
i talked about the you know magic

00:18:28,640 --> 00:18:33,120
triangle of analytics

00:18:30,000 --> 00:18:33,760
and and all this kind of hunch that this

00:18:33,120 --> 00:18:36,480
is gonna

00:18:33,760 --> 00:18:38,880
do something really good and it's gonna

00:18:36,480 --> 00:18:41,200
be really useful but it was

00:18:38,880 --> 00:18:42,720
almost yeah it was intuition but at that

00:18:41,200 --> 00:18:43,840
time we didn't have change to rule test

00:18:42,720 --> 00:18:45,919
it and so we did

00:18:43,840 --> 00:18:48,160
work on various different aspects of the

00:18:45,919 --> 00:18:50,720
project but where we

00:18:48,160 --> 00:18:52,000
succeeded and had the most striking

00:18:50,720 --> 00:18:53,440
results and i'm borrowing some of these

00:18:52,000 --> 00:18:55,039
slides i already spoke about this

00:18:53,440 --> 00:18:56,640
data palooza this year and we had a

00:18:55,039 --> 00:18:58,400
little article written but i won't

00:18:56,640 --> 00:19:00,640
highlight this case study

00:18:58,400 --> 00:19:01,919
is for what's called a retweet and storm

00:19:00,640 --> 00:19:03,520
project that we have with the department

00:19:01,919 --> 00:19:04,880
of veteran affairs

00:19:03,520 --> 00:19:07,280
i'll just give you quick background

00:19:04,880 --> 00:19:10,799
again um suicide is a

00:19:07,280 --> 00:19:11,520
a as is an epidemic problem specifically

00:19:10,799 --> 00:19:13,760
in the

00:19:11,520 --> 00:19:16,160
among the veteran population in the u.s

00:19:13,760 --> 00:19:17,200
um 20 veterans on average commit suicide

00:19:16,160 --> 00:19:19,440
a day

00:19:17,200 --> 00:19:20,559
not all 20 veterans are ever part of the

00:19:19,440 --> 00:19:21,919
ba system they

00:19:20,559 --> 00:19:24,240
can exist their whole life without

00:19:21,919 --> 00:19:25,919
visiting any of the va systems but for

00:19:24,240 --> 00:19:28,000
go through the system but va as an

00:19:25,919 --> 00:19:30,000
organization is focused on

00:19:28,000 --> 00:19:31,360
helping of course and assisting in

00:19:30,000 --> 00:19:33,760
alleviating this problem

00:19:31,360 --> 00:19:34,960
and so there is organization retrained

00:19:33,760 --> 00:19:38,880
we work with and storm

00:19:34,960 --> 00:19:39,520
specifically a tool to um attempting to

00:19:38,880 --> 00:19:42,080
highlight

00:19:39,520 --> 00:19:43,600
and identify people who might be at risk

00:19:42,080 --> 00:19:47,600
and this is done both risk

00:19:43,600 --> 00:19:49,280
of opioid overdose and of course suicide

00:19:47,600 --> 00:19:52,160
and then they needed our help to speed

00:19:49,280 --> 00:19:55,600
and optimize this platform and so

00:19:52,160 --> 00:19:57,840
um we had to we

00:19:55,600 --> 00:19:59,919
we looked into their current approaches

00:19:57,840 --> 00:20:01,520
and they had both clinical notes a very

00:19:59,919 --> 00:20:02,880
large repository of clinical notes and

00:20:01,520 --> 00:20:04,960
electronic health records

00:20:02,880 --> 00:20:07,039
and idea is to take this data combine

00:20:04,960 --> 00:20:08,720
and analyze and improve

00:20:07,039 --> 00:20:10,159
uh how is that being reported so we

00:20:08,720 --> 00:20:12,000
would do something and then send it back

00:20:10,159 --> 00:20:15,039
to va so they can use in their own

00:20:12,000 --> 00:20:16,159
clinical practice the issues were

00:20:15,039 --> 00:20:18,000
so first to tell you a little bit about

00:20:16,159 --> 00:20:18,480
this particular predictive algorithm is

00:20:18,000 --> 00:20:21,200
called

00:20:18,480 --> 00:20:22,720
npr or medication possession ratio the

00:20:21,200 --> 00:20:27,039
issue with suicide

00:20:22,720 --> 00:20:29,360
risk is that there's no um a particular

00:20:27,039 --> 00:20:30,400
surveillance instrument like we have in

00:20:29,360 --> 00:20:33,280
you know

00:20:30,400 --> 00:20:34,799
diabetes care or heart disease and

00:20:33,280 --> 00:20:36,320
cholesterol to tell us

00:20:34,799 --> 00:20:38,720
the person is progressing towards a

00:20:36,320 --> 00:20:40,640
certain state either people don't

00:20:38,720 --> 00:20:42,400
see therapists or if they see them it's

00:20:40,640 --> 00:20:44,559
recorded in a very peculiar way

00:20:42,400 --> 00:20:46,000
or it's not easy to analyze and one of

00:20:44,559 --> 00:20:48,080
the ways to try to

00:20:46,000 --> 00:20:49,600
one of the proxies for estimating risk

00:20:48,080 --> 00:20:53,600
is something called medication position

00:20:49,600 --> 00:20:56,880
possession ratio and that is um

00:20:53,600 --> 00:20:57,520
it looks at the ratio of fills of the

00:20:56,880 --> 00:20:59,760
drugs

00:20:57,520 --> 00:21:00,559
and how they occur over time and what is

00:20:59,760 --> 00:21:02,480
the

00:21:00,559 --> 00:21:04,080
expected adherence to those drugs and

00:21:02,480 --> 00:21:05,840
then in that way correlate with the

00:21:04,080 --> 00:21:07,520
visits and correlate with the

00:21:05,840 --> 00:21:09,760
amount of drugs being taken and their

00:21:07,520 --> 00:21:11,679
interactions and so it's this is

00:21:09,760 --> 00:21:13,280
npr is being calculated daily and it's

00:21:11,679 --> 00:21:14,480
used as one of the components of the

00:21:13,280 --> 00:21:15,520
whole dashboard so there's lots of

00:21:14,480 --> 00:21:17,919
factors

00:21:15,520 --> 00:21:19,280
that indicate the risk of suicide but

00:21:17,919 --> 00:21:20,559
npr is one of those that can be

00:21:19,280 --> 00:21:22,400
calculated daily

00:21:20,559 --> 00:21:23,600
and it's the closest to something that

00:21:22,400 --> 00:21:26,799
can be monitored

00:21:23,600 --> 00:21:28,880
the problem with the npr was that uh

00:21:26,799 --> 00:21:30,080
va was running computational into

00:21:28,880 --> 00:21:33,520
computational bottlenecks

00:21:30,080 --> 00:21:36,720
so uh they were running only on a

00:21:33,520 --> 00:21:38,640
limited number of drugs the npr

00:21:36,720 --> 00:21:40,080
for a limited population because of the

00:21:38,640 --> 00:21:41,600
computational bottlenecks i mean it

00:21:40,080 --> 00:21:42,799
would take them at some point in their

00:21:41,600 --> 00:21:44,799
relational database even though they

00:21:42,799 --> 00:21:47,360
have a great relational database

00:21:44,799 --> 00:21:48,640
um 24-hour almost 24-hour window to

00:21:47,360 --> 00:21:49,440
compute something they just couldn't

00:21:48,640 --> 00:21:51,840
advance

00:21:49,440 --> 00:21:53,120
so they approached us for help and so we

00:21:51,840 --> 00:21:54,720
had a copy of their

00:21:53,120 --> 00:21:56,720
their house and our approach for this

00:21:54,720 --> 00:21:58,320
was to pull data i mean we experimented

00:21:56,720 --> 00:22:01,840
we want to experiment with the mpi c

00:21:58,320 --> 00:22:03,520
plus plus um i'm trying to think

00:22:01,840 --> 00:22:05,600
any other approaches i mean really but

00:22:03,520 --> 00:22:07,760
we felt that

00:22:05,600 --> 00:22:09,200
what we know magic triangle is pull the

00:22:07,760 --> 00:22:12,159
data out of the relational database

00:22:09,200 --> 00:22:14,480
convert it to per k and then work to

00:22:12,159 --> 00:22:16,559
optimize this algorithm in spark using

00:22:14,480 --> 00:22:17,919
just spark out of the box but also using

00:22:16,559 --> 00:22:20,799
the the

00:22:17,919 --> 00:22:22,559
dag approach to reducing number of

00:22:20,799 --> 00:22:23,360
computation steps and then paralyzing

00:22:22,559 --> 00:22:26,480
them

00:22:23,360 --> 00:22:27,520
so we did that we ran it through the

00:22:26,480 --> 00:22:29,760
parquet spark

00:22:27,520 --> 00:22:31,919
and then fed back results and in terms

00:22:29,760 --> 00:22:35,440
of

00:22:31,919 --> 00:22:36,799
speed up it was original job was running

00:22:35,440 --> 00:22:40,000
5.5 hours

00:22:36,799 --> 00:22:41,600
we sped it up to 15 minutes there are

00:22:40,000 --> 00:22:43,280
certain jobs that couldn't be able

00:22:41,600 --> 00:22:44,640
they couldn't execute at all in the

00:22:43,280 --> 00:22:45,120
environment so for instance to run

00:22:44,640 --> 00:22:48,960
against

00:22:45,120 --> 00:22:52,400
all drugs and all patients

00:22:48,960 --> 00:22:55,039
it would take over 75 hours in instances

00:22:52,400 --> 00:22:57,280
it ran out of completion doing the

00:22:55,039 --> 00:22:58,799
their methodology because it was just

00:22:57,280 --> 00:23:01,760
running out of memory and just

00:22:58,799 --> 00:23:03,360
taking so long and over tasking cpus and

00:23:01,760 --> 00:23:04,080
what we did once we pence we did our

00:23:03,360 --> 00:23:07,280
package

00:23:04,080 --> 00:23:09,039
spark approach is that it ran in as i

00:23:07,280 --> 00:23:11,200
said sub 15 minutes

00:23:09,039 --> 00:23:12,880
run times and so it we we're right now

00:23:11,200 --> 00:23:14,159
looking how to move it even to the real

00:23:12,880 --> 00:23:15,200
time that that's going to approach

00:23:14,159 --> 00:23:17,120
that's going to require different

00:23:15,200 --> 00:23:18,000
approaches we're exploring some other

00:23:17,120 --> 00:23:20,320
open source

00:23:18,000 --> 00:23:22,159
libraries but you know we we're here to

00:23:20,320 --> 00:23:26,240
point almost where we can run this

00:23:22,159 --> 00:23:29,280
in a minute um to move towards

00:23:26,240 --> 00:23:32,880
conclusions how we got to this point so

00:23:29,280 --> 00:23:34,080
what was the formula for speeding this

00:23:32,880 --> 00:23:36,400
up

00:23:34,080 --> 00:23:38,320
first as i said restructuring data into

00:23:36,400 --> 00:23:42,000
parque in a later

00:23:38,320 --> 00:23:42,400
era formats um that just itself is just

00:23:42,000 --> 00:23:44,480
as a

00:23:42,400 --> 00:23:46,559
report in that paper it gives you

00:23:44,480 --> 00:23:46,880
multiple orders of magnitude improvement

00:23:46,559 --> 00:23:48,480
or

00:23:46,880 --> 00:23:50,880
reading data from classical relational

00:23:48,480 --> 00:23:52,720
database it's a call oriented format

00:23:50,880 --> 00:23:55,279
it's a data compressing format

00:23:52,720 --> 00:23:56,559
it's very clever how it does these uh

00:23:55,279 --> 00:23:59,440
basic statistical

00:23:56,559 --> 00:24:00,080
um aggregate functions sums and mints

00:23:59,440 --> 00:24:02,159
and counts

00:24:00,080 --> 00:24:03,520
using the sketch data structure so just

00:24:02,159 --> 00:24:06,240
using parquet

00:24:03,520 --> 00:24:06,960
out of box you get the speed up two with

00:24:06,240 --> 00:24:08,799
spark and

00:24:06,960 --> 00:24:10,960
using larger memory machines now these

00:24:08,799 --> 00:24:11,679
are not 10 terabyte memory machines

00:24:10,960 --> 00:24:14,000
these were

00:24:11,679 --> 00:24:14,720
i don't know 500 gig maybe a terabyte at

00:24:14,000 --> 00:24:16,559
most but

00:24:14,720 --> 00:24:18,880
even more than that just moving some of

00:24:16,559 --> 00:24:20,559
that computing because what you get with

00:24:18,880 --> 00:24:22,720
parking arrow you get a very compact

00:24:20,559 --> 00:24:24,640
data set so moving that in memory

00:24:22,720 --> 00:24:26,000
you are preserving memory and all the

00:24:24,640 --> 00:24:28,240
computations happening there

00:24:26,000 --> 00:24:29,120
there's another level of a performance

00:24:28,240 --> 00:24:32,960
improvement

00:24:29,120 --> 00:24:35,039
and then um using

00:24:32,960 --> 00:24:36,960
efficient data structures and directed

00:24:35,039 --> 00:24:38,640
directed the cyclograph because for npr

00:24:36,960 --> 00:24:39,600
there's a lot of three computations

00:24:38,640 --> 00:24:41,200
there's lots of

00:24:39,600 --> 00:24:42,640
redundant computations that happen

00:24:41,200 --> 00:24:45,039
because you have to keep going for every

00:24:42,640 --> 00:24:47,200
patient you have to do certain things

00:24:45,039 --> 00:24:49,360
restructuring computation using either

00:24:47,200 --> 00:24:49,679
what spark gives you out of box with dag

00:24:49,360 --> 00:24:51,440
and

00:24:49,679 --> 00:24:54,000
doing our own algorithmic work we were

00:24:51,440 --> 00:24:55,120
able to drastically reduce number of

00:24:54,000 --> 00:24:58,240
recomputations and

00:24:55,120 --> 00:25:00,080
compute daily only what's new and

00:24:58,240 --> 00:25:02,240
the wonderful thing again is that many

00:25:00,080 --> 00:25:02,960
of that was just done so easily with

00:25:02,240 --> 00:25:04,799
spark

00:25:02,960 --> 00:25:07,360
and then being being able to innovate on

00:25:04,799 --> 00:25:11,919
top of it and so

00:25:07,360 --> 00:25:13,919
um judging on this experience really as

00:25:11,919 --> 00:25:15,360
i said this is a very stark example of

00:25:13,919 --> 00:25:18,559
how this works

00:25:15,360 --> 00:25:19,840
and you know it doesn't require 100 phds

00:25:18,559 --> 00:25:21,520
working on this problem yeah we had

00:25:19,840 --> 00:25:23,279
three phds working on a problem but you

00:25:21,520 --> 00:25:24,240
know we work with a great team from from

00:25:23,279 --> 00:25:27,360
va

00:25:24,240 --> 00:25:29,039
um just by by using

00:25:27,360 --> 00:25:31,200
in 2010 when i tell you about hadoop

00:25:29,039 --> 00:25:32,640
versus mpp that we did this in mpp

00:25:31,200 --> 00:25:35,039
it would have required probably a 15

00:25:32,640 --> 00:25:36,559
million dollar appliance

00:25:35,039 --> 00:25:38,080
and i'm not even sure we'll be able to

00:25:36,559 --> 00:25:40,159
do everything we're able to do with

00:25:38,080 --> 00:25:41,039
spark on a commodity hardware in part

00:25:40,159 --> 00:25:43,520
okay

00:25:41,039 --> 00:25:44,080
so spark and parquet have been our swiss

00:25:43,520 --> 00:25:46,720
knife

00:25:44,080 --> 00:25:48,720
solutions for lots of different things i

00:25:46,720 --> 00:25:50,080
presented this particular stark use case

00:25:48,720 --> 00:25:50,720
but there are many many others where we

00:25:50,080 --> 00:25:53,279
do it

00:25:50,720 --> 00:25:55,600
daily as i said i mean really and so for

00:25:53,279 --> 00:25:57,200
instance very extending is

00:25:55,600 --> 00:25:58,880
converting unstructured data like a

00:25:57,200 --> 00:26:01,360
medical notes extracting

00:25:58,880 --> 00:26:03,039
either for instance developing creating

00:26:01,360 --> 00:26:05,039
word embeddings or extracting

00:26:03,039 --> 00:26:06,400
particular terms of interest and then

00:26:05,039 --> 00:26:07,200
once we extract them put them in a

00:26:06,400 --> 00:26:09,039
parquet and

00:26:07,200 --> 00:26:10,480
reduce the storage and we get again all

00:26:09,039 --> 00:26:13,279
those great benefits

00:26:10,480 --> 00:26:14,880
um the things we're exploring which are

00:26:13,279 --> 00:26:15,360
very obvious and that is using error

00:26:14,880 --> 00:26:17,360
which is

00:26:15,360 --> 00:26:18,480
in memory optimized and then also using

00:26:17,360 --> 00:26:20,640
memory mapped

00:26:18,480 --> 00:26:21,760
file systems like galaxy and others that

00:26:20,640 --> 00:26:25,120
you know give us that

00:26:21,760 --> 00:26:26,480
i o uh high performance um

00:26:25,120 --> 00:26:28,640
we have also moved in terms of data

00:26:26,480 --> 00:26:31,679
management this is something to just to

00:26:28,640 --> 00:26:34,400
to be aware to know uh spark and

00:26:31,679 --> 00:26:35,679
parquet can be also a little loose in

00:26:34,400 --> 00:26:36,559
terms of data management because you

00:26:35,679 --> 00:26:39,360
have to manage those

00:26:36,559 --> 00:26:40,799
i mean is some of you know it's a it's a

00:26:39,360 --> 00:26:44,240
folder that has

00:26:40,799 --> 00:26:46,400
files in it which are chunks and

00:26:44,240 --> 00:26:48,159
as you keep doing this over time it

00:26:46,400 --> 00:26:49,679
becomes a little bit more complicated to

00:26:48,159 --> 00:26:51,520
manage all those files especially

00:26:49,679 --> 00:26:52,960
updates so we are right now using

00:26:51,520 --> 00:26:54,559
delta lake which is another it's not

00:26:52,960 --> 00:26:56,960
apache project but it's open source

00:26:54,559 --> 00:26:59,440
project coming from

00:26:56,960 --> 00:27:01,440
databricks and this is early phase

00:26:59,440 --> 00:27:01,840
evaluation so far so good i don't have

00:27:01,440 --> 00:27:04,159
anything

00:27:01,840 --> 00:27:05,200
special to report but it's it's going

00:27:04,159 --> 00:27:08,159
well

00:27:05,200 --> 00:27:09,600
but you know judging from my experience

00:27:08,159 --> 00:27:11,760
um

00:27:09,600 --> 00:27:13,360
and what we have seen in our work is

00:27:11,760 --> 00:27:14,240
that it's just an incredibly useful

00:27:13,360 --> 00:27:15,760
general purpose

00:27:14,240 --> 00:27:17,279
set of open source libraries that can

00:27:15,760 --> 00:27:20,240
take you very far

00:27:17,279 --> 00:27:20,559
out of the box and then open the room

00:27:20,240 --> 00:27:22,320
for

00:27:20,559 --> 00:27:23,760
another things which of course this was

00:27:22,320 --> 00:27:24,399
on topic of this talk but you know we

00:27:23,760 --> 00:27:26,880
are

00:27:24,399 --> 00:27:28,000
big now machine learning deep learning

00:27:26,880 --> 00:27:30,880
lab with all these

00:27:28,000 --> 00:27:32,559
monster gpu machines and so that also

00:27:30,880 --> 00:27:33,520
helps there because it helps us prepare

00:27:32,559 --> 00:27:37,200
the data and

00:27:33,520 --> 00:27:38,880
don't have to worry about you know all

00:27:37,200 --> 00:27:40,240
complicated ways to prepare it it's a

00:27:38,880 --> 00:27:42,080
kind of good general purpose

00:27:40,240 --> 00:27:43,360
set of things to get start and then use

00:27:42,080 --> 00:27:44,960
the data for

00:27:43,360 --> 00:27:46,640
either deep learning or some other

00:27:44,960 --> 00:27:50,159
scientific computing jobs

00:27:46,640 --> 00:27:52,080
so overall um great experience very

00:27:50,159 --> 00:27:54,159
thankful as i said i got myself

00:27:52,080 --> 00:27:56,000
affiliated and i got to know ian soyka

00:27:54,159 --> 00:27:59,360
now very well and we work

00:27:56,000 --> 00:28:00,720
very closely very grateful to the uh

00:27:59,360 --> 00:28:02,880
to the vision that came out of

00:28:00,720 --> 00:28:05,279
originally amp lab and now rise lab

00:28:02,880 --> 00:28:05,919
and certainly grateful to the apache

00:28:05,279 --> 00:28:09,840
community

00:28:05,919 --> 00:28:11,679
for uh creating ecosystem

00:28:09,840 --> 00:28:12,960
where people can innovate and provide

00:28:11,679 --> 00:28:15,919
things like this

00:28:12,960 --> 00:28:18,640
that support life critical mission in an

00:28:15,919 --> 00:28:21,679
entirely open source setting

00:28:18,640 --> 00:28:22,640
so as i before i stopped my sharing in

00:28:21,679 --> 00:28:25,760
the presentation

00:28:22,640 --> 00:28:26,399
uh just to thank dr josh arnold who was

00:28:25,760 --> 00:28:28,720
the

00:28:26,399 --> 00:28:31,600
a key data engineer and machine learning

00:28:28,720 --> 00:28:33,840
engineer behind the npr improvement

00:28:31,600 --> 00:28:35,039
and dr john trafton amy robinson and

00:28:33,840 --> 00:28:36,960
susana martens on the

00:28:35,039 --> 00:28:38,080
side work with us as a subject matter

00:28:36,960 --> 00:28:40,320
expert to help us

00:28:38,080 --> 00:28:41,200
understand how npr works and then help

00:28:40,320 --> 00:28:43,600
convert it

00:28:41,200 --> 00:28:45,120
the way we did in both in the entire of

00:28:43,600 --> 00:28:46,480
course va and rnl team

00:28:45,120 --> 00:28:49,520
that's involved in the rich light and

00:28:46,480 --> 00:28:51,840
storm collaboration and of course my

00:28:49,520 --> 00:28:53,360
you know homeplace oakridge national lab

00:28:51,840 --> 00:28:54,080
managed by ut battelle and power

00:28:53,360 --> 00:28:57,520
advantage

00:28:54,080 --> 00:29:00,159
so um with that

00:28:57,520 --> 00:29:00,559
thank you for your attention and that

00:29:00,159 --> 00:29:03,039
was

00:29:00,559 --> 00:29:03,919
the experience i was very excited to

00:29:03,039 --> 00:29:06,640
share with you

00:29:03,919 --> 00:29:07,200
and i hope you get to experience some

00:29:06,640 --> 00:29:10,720
some of this

00:29:07,200 --> 00:29:12,799
as well thank you so much for sharing

00:29:10,720 --> 00:29:14,880
with us you know one of the things that

00:29:12,799 --> 00:29:16,000
was was exciting while watching you in

00:29:14,880 --> 00:29:18,240
addition to the

00:29:16,000 --> 00:29:20,159
the cool stuff that you're working on

00:29:18,240 --> 00:29:21,840
when you work on apache software

00:29:20,159 --> 00:29:23,279
you're literally changing the world you

00:29:21,840 --> 00:29:26,240
are making life better

00:29:23,279 --> 00:29:26,559
for for people in the world and that's

00:29:26,240 --> 00:29:28,559
just

00:29:26,559 --> 00:29:29,760
it's so inspiring thank you for sharing

00:29:28,559 --> 00:29:39,679
your time with us

00:29:29,760 --> 00:29:39,679

YouTube URL: https://www.youtube.com/watch?v=b5fpiDzScRU


