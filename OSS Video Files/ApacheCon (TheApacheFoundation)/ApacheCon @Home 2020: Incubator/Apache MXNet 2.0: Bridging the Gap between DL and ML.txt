Title: Apache MXNet 2.0: Bridging the Gap between DL and ML
Publication date: 2020-10-16
Playlist: ApacheCon @Home 2020: Incubator
Description: 
	Apache MXNet 2.0: Bridging the Gap between DL and ML
Sheng Zha

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Deep learning community has largely evolved independently from the prior community of data science and machine learning community in NumPy. While most deep learning frameworks now provide NumPy-like math and array library, they differ in the definition of the operations which creates a steeper learning curve of deep learning for machine learning practitioners and data scientists. This creates a chasm not only in the skillsets of the two different communities, but also hinders the exchange of knowledge. The next major version, 2.0, of Apache MXNet (incubating) seeks to bridge the fragmented deep learning and machine learning ecosystem. It provides NumPy-compatible programming experiences and simple enhancements to NumPy for deep learning with the new Gluon 2.0 interface. The NumPy-compatible array API also brings the advances in GPU acceleration, auto-differentiation, and high-performance one-click deployment to the NumPy ecosystem.

Sheng Zha is an Applied Scientist at Amazon AI, a committer and PPMC member of Apache MXNet (Incubating), maintainer of GluonNLP project, and a steering committee member of ONNX project. In his research, Sheng focuses on the intersection between deep learning, natural language processing, and machine learning systems, with the goal of reducing the cost of developing deep learning and machine learning applications for NLP.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:26,960 --> 00:00:31,920
all right

00:00:27,519 --> 00:00:35,360
we are time and i'm getting started now

00:00:31,920 --> 00:00:39,520
so hi i'm a

00:00:35,360 --> 00:00:41,840
member of the apache mxnet ppmc

00:00:39,520 --> 00:00:43,360
and also a senior applied scientist at

00:00:41,840 --> 00:00:45,120
amazon ai

00:00:43,360 --> 00:00:47,120
i'm really glad to be presenting at my

00:00:45,120 --> 00:00:50,879
first apache call

00:00:47,120 --> 00:00:54,719
in my favorite track about mxnet

00:00:50,879 --> 00:00:57,520
so in today's talk i'll be

00:00:54,719 --> 00:00:58,160
first sharing the state of our community

00:00:57,520 --> 00:01:00,480
and then

00:00:58,160 --> 00:01:01,840
we'll be looking at the current status

00:01:00,480 --> 00:01:04,799
in ai frameworks

00:01:01,840 --> 00:01:06,080
and examine the fragmentation problem

00:01:04,799 --> 00:01:09,200
afterwards i'll be

00:01:06,080 --> 00:01:11,360
introducing how mxn 2.0

00:01:09,200 --> 00:01:13,360
is helping to bridge the gap that's

00:01:11,360 --> 00:01:15,600
caused by the fragmentation

00:01:13,360 --> 00:01:17,040
and finally i'll share an update on the

00:01:15,600 --> 00:01:20,240
ecosystem of

00:01:17,040 --> 00:01:23,520
mx network if we have time

00:01:20,240 --> 00:01:26,400
so mxnet is a

00:01:23,520 --> 00:01:28,560
pretty large community we've had over

00:01:26,400 --> 00:01:31,920
900 contributors

00:01:28,560 --> 00:01:35,520
that helped on mxnet

00:01:31,920 --> 00:01:39,840
we have over 19k stars on

00:01:35,520 --> 00:01:43,360
github and almost 7000 forks

00:01:39,840 --> 00:01:47,600
since joining the apache incubator we've

00:01:43,360 --> 00:01:50,640
progressed with almost 6000 commits

00:01:47,600 --> 00:01:51,680
and it's a widely adopted deep learning

00:01:50,640 --> 00:01:54,000
framework

00:01:51,680 --> 00:01:55,680
by the industry and trusted by many

00:01:54,000 --> 00:01:59,200
companies

00:01:55,680 --> 00:02:02,399
so um here are some information how

00:01:59,200 --> 00:02:06,000
on how you can stay connected

00:02:02,399 --> 00:02:09,119
in mxnet so as usual you can

00:02:06,000 --> 00:02:11,599
join and subscribe to our dev list

00:02:09,119 --> 00:02:13,360
and um you can also connect with us on

00:02:11,599 --> 00:02:17,360
the asf slack

00:02:13,360 --> 00:02:20,560
in the mxnet channel um there are also

00:02:17,360 --> 00:02:22,000
good first issues that we identify for

00:02:20,560 --> 00:02:23,200
people who want to get started in the

00:02:22,000 --> 00:02:26,239
project

00:02:23,200 --> 00:02:29,920
and also we label our roadmap

00:02:26,239 --> 00:02:31,920
issues on github with the roadmap label

00:02:29,920 --> 00:02:34,080
so at the moment we're looking for

00:02:31,920 --> 00:02:35,120
improvement in c plus plus and the java

00:02:34,080 --> 00:02:38,959
bindings

00:02:35,120 --> 00:02:42,239
and also we want to integrate

00:02:38,959 --> 00:02:44,640
more closely with the tbm relay which

00:02:42,239 --> 00:02:46,560
tenshi talked about it in the morning

00:02:44,640 --> 00:02:49,440
and finally there are

00:02:46,560 --> 00:02:50,080
social media channels to which you can

00:02:49,440 --> 00:02:53,519
subscribe

00:02:50,080 --> 00:02:56,480
to our news and announcements okay so

00:02:53,519 --> 00:02:56,800
now let's dive in to uh discuss about

00:02:56,480 --> 00:03:00,159
the

00:02:56,800 --> 00:03:03,440
ai framework fragmentation so

00:03:00,159 --> 00:03:06,000
here's a an ai open source landscape

00:03:03,440 --> 00:03:07,280
that that's produced by linux ai

00:03:06,000 --> 00:03:10,239
foundation

00:03:07,280 --> 00:03:12,400
um so ai is really among the fastest

00:03:10,239 --> 00:03:14,640
growing field that spreads into

00:03:12,400 --> 00:03:15,599
every industry and the demand for ai

00:03:14,640 --> 00:03:18,800
talents

00:03:15,599 --> 00:03:20,800
almost doubles every year since 2015.

00:03:18,800 --> 00:03:21,920
similarly there is an explosion in the

00:03:20,800 --> 00:03:25,440
diversity of

00:03:21,920 --> 00:03:28,480
ai tools that touches every aspect of

00:03:25,440 --> 00:03:31,599
ai and data science in data processing

00:03:28,480 --> 00:03:32,879
analysis and modeling and because of the

00:03:31,599 --> 00:03:35,200
high practical value

00:03:32,879 --> 00:03:37,360
and even higher potential the industry

00:03:35,200 --> 00:03:39,920
invests a lot in the ai tooling

00:03:37,360 --> 00:03:40,480
so um in this picture you may recognize

00:03:39,920 --> 00:03:42,799
um

00:03:40,480 --> 00:03:44,319
the the logos of many popular ai tools

00:03:42,799 --> 00:03:47,040
here

00:03:44,319 --> 00:03:48,400
um while impressive there's a problem in

00:03:47,040 --> 00:03:51,040
this picture

00:03:48,400 --> 00:03:52,720
it's the fragmentation in the ai open

00:03:51,040 --> 00:03:55,840
source softwares

00:03:52,720 --> 00:03:56,720
most notably the deep learning

00:03:55,840 --> 00:03:58,879
software's

00:03:56,720 --> 00:04:00,000
evolved to become a whole independent

00:03:58,879 --> 00:04:02,159
group from

00:04:00,000 --> 00:04:04,319
earlier machine learning tools despite

00:04:02,159 --> 00:04:06,239
that deep learning is just a new way of

00:04:04,319 --> 00:04:09,040
modeling in machine learning

00:04:06,239 --> 00:04:10,319
so let's see why this fragmentation is a

00:04:09,040 --> 00:04:14,239
problem

00:04:10,319 --> 00:04:15,920
so first what do i mean by fragmentation

00:04:14,239 --> 00:04:18,320
in the context of machine learning and

00:04:15,920 --> 00:04:20,440
deep learning open source software

00:04:18,320 --> 00:04:21,919
i'm referring to the lack of

00:04:20,440 --> 00:04:25,199
interoperability

00:04:21,919 --> 00:04:27,759
and lack of common interface design

00:04:25,199 --> 00:04:30,400
so next i'll try to convince you that

00:04:27,759 --> 00:04:33,759
this problem is really costly

00:04:30,400 --> 00:04:37,280
so why is it costly um let's take a look

00:04:33,759 --> 00:04:40,840
at its effect on three groups of people

00:04:37,280 --> 00:04:42,000
that participate in the open source

00:04:40,840 --> 00:04:45,520
software

00:04:42,000 --> 00:04:47,600
so um first to the users

00:04:45,520 --> 00:04:49,120
the lack of interoperability and common

00:04:47,600 --> 00:04:52,479
design among frameworks

00:04:49,120 --> 00:04:56,720
creates a lock-in in two ways

00:04:52,479 --> 00:04:59,680
first is the learning curve so

00:04:56,720 --> 00:05:00,320
the user needs to choose a tool to learn

00:04:59,680 --> 00:05:03,840
and

00:05:00,320 --> 00:05:04,479
that learning takes time when the user

00:05:03,840 --> 00:05:07,680
needs to

00:05:04,479 --> 00:05:09,680
switch from one framework to another

00:05:07,680 --> 00:05:11,759
the user would have to relearn so the

00:05:09,680 --> 00:05:16,160
time cost is locked into

00:05:11,759 --> 00:05:19,280
the the tool of choice the second

00:05:16,160 --> 00:05:20,639
part is that whatever user code base the

00:05:19,280 --> 00:05:23,600
user creates

00:05:20,639 --> 00:05:24,240
it's locked in that tool of choice two

00:05:23,600 --> 00:05:27,039
that's

00:05:24,240 --> 00:05:27,759
because of lack of common interface

00:05:27,039 --> 00:05:29,919
design

00:05:27,759 --> 00:05:32,000
the code cannot be transferred to

00:05:29,919 --> 00:05:33,520
another framework without being migrated

00:05:32,000 --> 00:05:36,720
to it manually

00:05:33,520 --> 00:05:38,160
so this login makes it hard to benefit

00:05:36,720 --> 00:05:40,720
from the strengths of different

00:05:38,160 --> 00:05:40,720
frameworks

00:05:40,880 --> 00:05:46,639
second to the framework developers

00:05:44,240 --> 00:05:47,440
the lack of interoperability creates the

00:05:46,639 --> 00:05:50,160
need to

00:05:47,440 --> 00:05:52,320
develop and maintain independent stacks

00:05:50,160 --> 00:05:55,759
in each of the framework

00:05:52,320 --> 00:05:57,199
so um because of the lack of

00:05:55,759 --> 00:05:59,520
interoperability

00:05:57,199 --> 00:06:00,880
the frameworks are essentially competing

00:05:59,520 --> 00:06:04,319
with each other

00:06:00,880 --> 00:06:07,360
and the user would have to choose among

00:06:04,319 --> 00:06:09,680
the many choices for just one of them

00:06:07,360 --> 00:06:12,880
so this would force the developers and

00:06:09,680 --> 00:06:16,080
contributor resources to be focused on

00:06:12,880 --> 00:06:19,440
um the shortest plank so to speak in

00:06:16,080 --> 00:06:21,520
this barrel of framework instead of

00:06:19,440 --> 00:06:24,160
focusing on making the needed progress

00:06:21,520 --> 00:06:24,160
for the field

00:06:24,960 --> 00:06:31,840
so the third party developers would be

00:06:29,039 --> 00:06:33,680
developing hardware and runtime or other

00:06:31,840 --> 00:06:36,560
third-party extensions for

00:06:33,680 --> 00:06:36,960
for example distributed training they

00:06:36,560 --> 00:06:40,080
must

00:06:36,960 --> 00:06:41,360
invest in each of the framework in

00:06:40,080 --> 00:06:43,199
integration

00:06:41,360 --> 00:06:44,639
and this results in duplicated

00:06:43,199 --> 00:06:47,280
investments and

00:06:44,639 --> 00:06:48,880
higher engineering costs so this would

00:06:47,280 --> 00:06:52,080
become a barrier for

00:06:48,880 --> 00:06:55,199
innovation and hinders progress

00:06:52,080 --> 00:06:57,520
because of the the higher cost and often

00:06:55,199 --> 00:06:58,319
framework agnostic extensions would

00:06:57,520 --> 00:07:00,319
suffer from

00:06:58,319 --> 00:07:03,440
maintainability issues as a result of

00:07:00,319 --> 00:07:06,560
managing complex dependencies

00:07:03,440 --> 00:07:08,639
so how did the frame fragmentation

00:07:06,560 --> 00:07:12,080
happen in the first place

00:07:08,639 --> 00:07:13,280
it comes down to the different design

00:07:12,080 --> 00:07:16,880
choices

00:07:13,280 --> 00:07:21,759
in the the frameworks um

00:07:16,880 --> 00:07:24,080
goals and um so part of the

00:07:21,759 --> 00:07:25,039
um so deep learning frameworks would

00:07:24,080 --> 00:07:28,080
usually

00:07:25,039 --> 00:07:30,240
focus on the array interface

00:07:28,080 --> 00:07:32,240
whereas in addition to array machine

00:07:30,240 --> 00:07:35,280
learning frameworks also provides

00:07:32,240 --> 00:07:39,039
data frames such as um in the

00:07:35,280 --> 00:07:41,039
the spark project um and um

00:07:39,039 --> 00:07:42,080
machine learning frameworks often value

00:07:41,039 --> 00:07:45,039
higher precision

00:07:42,080 --> 00:07:47,280
whereas in deep learning um the

00:07:45,039 --> 00:07:50,400
precision is less of an issue

00:07:47,280 --> 00:07:53,440
but we focus more on speed so

00:07:50,400 --> 00:07:55,039
um the that's why deep learning

00:07:53,440 --> 00:07:56,840
frameworks usually provide half

00:07:55,039 --> 00:08:00,400
precision

00:07:56,840 --> 00:08:03,199
choices so um

00:08:00,400 --> 00:08:05,599
and also in machine learning frameworks

00:08:03,199 --> 00:08:07,599
they would optimize for scala programs

00:08:05,599 --> 00:08:09,280
which is not usually a focus of deep

00:08:07,599 --> 00:08:12,240
learning frameworks

00:08:09,280 --> 00:08:12,800
and also deep learning frameworks

00:08:12,240 --> 00:08:15,360
provide

00:08:12,800 --> 00:08:16,800
automatic differentiation whereas

00:08:15,360 --> 00:08:18,080
machine learning frameworks usually

00:08:16,800 --> 00:08:21,199
don't

00:08:18,080 --> 00:08:22,240
um also um most of the time machine

00:08:21,199 --> 00:08:23,919
learning frameworks

00:08:22,240 --> 00:08:25,440
follows the imperative programming

00:08:23,919 --> 00:08:27,039
paradigm whereas

00:08:25,440 --> 00:08:28,960
deep learning now provides both

00:08:27,039 --> 00:08:32,240
imperative and symbolic

00:08:28,960 --> 00:08:35,599
um optimization uh as

00:08:32,240 --> 00:08:36,159
as paradigms and acceleration is more of

00:08:35,599 --> 00:08:39,760
a

00:08:36,159 --> 00:08:43,279
focus for deep learning on gpus and

00:08:39,760 --> 00:08:46,880
accelerators nowadays um so

00:08:43,279 --> 00:08:46,880
i'll be focusing on the

00:08:47,279 --> 00:08:53,519
array api the automatic

00:08:50,399 --> 00:08:55,200
differentiation and the acceleration so

00:08:53,519 --> 00:08:59,040
that i can show you the key

00:08:55,200 --> 00:09:02,320
ingredients of deep learning frameworks

00:08:59,040 --> 00:09:06,160
so first on the array api

00:09:02,320 --> 00:09:06,560
the modern array library is popularized

00:09:06,160 --> 00:09:09,760
by

00:09:06,560 --> 00:09:12,399
numpy starting 15 years ago

00:09:09,760 --> 00:09:13,279
it's a combination of data structure as

00:09:12,399 --> 00:09:16,480
well as

00:09:13,279 --> 00:09:17,120
the manipulation methods the data

00:09:16,480 --> 00:09:19,680
structure

00:09:17,120 --> 00:09:20,240
captures the data the data type the

00:09:19,680 --> 00:09:24,240
shape

00:09:20,240 --> 00:09:27,440
and also in different layouts we would

00:09:24,240 --> 00:09:30,560
describe them as strides and

00:09:27,440 --> 00:09:32,720
there's also ways of

00:09:30,560 --> 00:09:33,920
indexing the array to create a view and

00:09:32,720 --> 00:09:37,200
copy

00:09:33,920 --> 00:09:40,240
and the there is also vectorized

00:09:37,200 --> 00:09:42,560
arithmetics uh broadcasting as well as

00:09:40,240 --> 00:09:44,399
reduction by certain axes

00:09:42,560 --> 00:09:46,320
so for those who are already familiar

00:09:44,399 --> 00:09:48,320
with numpy

00:09:46,320 --> 00:09:50,000
related programs this example should

00:09:48,320 --> 00:09:53,040
look familiar

00:09:50,000 --> 00:09:54,000
so now that we talked about array let's

00:09:53,040 --> 00:09:57,519
talk about

00:09:54,000 --> 00:09:59,519
automatic differentiation um first i'd

00:09:57,519 --> 00:10:02,839
like to share a quote from

00:09:59,519 --> 00:10:04,240
the 2018 touring awards laureate young

00:10:02,839 --> 00:10:06,640
lacombe

00:10:04,240 --> 00:10:08,160
so he's saying people are now building a

00:10:06,640 --> 00:10:10,640
new kind of software

00:10:08,160 --> 00:10:11,440
by assembling networks of parameterized

00:10:10,640 --> 00:10:13,760
functional

00:10:11,440 --> 00:10:16,000
blocks and by training them from

00:10:13,760 --> 00:10:18,240
examples using some form of gradient

00:10:16,000 --> 00:10:20,800
based optimization so

00:10:18,240 --> 00:10:22,560
what he's referring to is differentiable

00:10:20,800 --> 00:10:26,399
programming or sometimes

00:10:22,560 --> 00:10:28,480
some people call it software 2.0

00:10:26,399 --> 00:10:30,240
this is enabled by automatic

00:10:28,480 --> 00:10:34,480
differentiation

00:10:30,240 --> 00:10:37,920
so uh what is uh ad

00:10:34,480 --> 00:10:40,240
um so in the old way of programming

00:10:37,920 --> 00:10:41,600
um i believe everyone is familiar with

00:10:40,240 --> 00:10:45,120
that and we we all do

00:10:41,600 --> 00:10:47,839
plenty of it um so uh

00:10:45,120 --> 00:10:48,480
here the programmers would hard code the

00:10:47,839 --> 00:10:51,120
function

00:10:48,480 --> 00:10:52,000
with the goal of generating desirable

00:10:51,120 --> 00:10:55,040
output

00:10:52,000 --> 00:10:58,000
given the input data um

00:10:55,040 --> 00:10:59,440
in the differentiable programming

00:10:58,000 --> 00:11:01,519
however

00:10:59,440 --> 00:11:03,040
we would be talking about programming

00:11:01,519 --> 00:11:05,360
with data

00:11:03,040 --> 00:11:06,320
so now suppose we want to have a

00:11:05,360 --> 00:11:09,839
computer tell

00:11:06,320 --> 00:11:12,959
whether an image depicts a cat or a dog

00:11:09,839 --> 00:11:15,200
um so more or more generally

00:11:12,959 --> 00:11:17,760
it's given some input data we want to

00:11:15,200 --> 00:11:19,920
estimate the output

00:11:17,760 --> 00:11:21,440
so often the logic would be quite

00:11:19,920 --> 00:11:24,079
complicated

00:11:21,440 --> 00:11:25,680
it might be vague but we have plenty of

00:11:24,079 --> 00:11:28,000
examples

00:11:25,680 --> 00:11:30,480
on the other hand as a programmer it's

00:11:28,000 --> 00:11:34,399
almost impossible to write code

00:11:30,480 --> 00:11:37,360
but we can train an estimator to do that

00:11:34,399 --> 00:11:38,880
so in order to do it we feed the image

00:11:37,360 --> 00:11:42,160
data of the kitty

00:11:38,880 --> 00:11:45,680
into a network that is parameterized

00:11:42,160 --> 00:11:47,680
by the weights and the network

00:11:45,680 --> 00:11:50,560
would generate a prediction through a

00:11:47,680 --> 00:11:53,839
weighted sum of from the input data

00:11:50,560 --> 00:11:57,040
and a series of mass operations

00:11:53,839 --> 00:12:00,000
it produces a score of probability

00:11:57,040 --> 00:12:02,160
that the image represents a cat or that

00:12:00,000 --> 00:12:04,720
of a dog

00:12:02,160 --> 00:12:05,519
so what automatic differentiation

00:12:04,720 --> 00:12:09,839
enables

00:12:05,519 --> 00:12:11,680
is that um there would be an easy way of

00:12:09,839 --> 00:12:15,200
describing the differentiable

00:12:11,680 --> 00:12:18,800
function as the advanced network um

00:12:15,200 --> 00:12:21,279
and an easy way of producing

00:12:18,800 --> 00:12:22,079
the prediction from it so since the

00:12:21,279 --> 00:12:24,000
program

00:12:22,079 --> 00:12:26,720
already knows everything about the

00:12:24,000 --> 00:12:28,720
function once we know the arrow signal

00:12:26,720 --> 00:12:32,160
from the output end label

00:12:28,720 --> 00:12:35,200
we can generate the the arrow signal

00:12:32,160 --> 00:12:38,399
and the the gradients in which

00:12:35,200 --> 00:12:40,800
the weights should be adjusted so that

00:12:38,399 --> 00:12:43,600
it could produce a higher score for for

00:12:40,800 --> 00:12:43,600
cat in this case

00:12:44,320 --> 00:12:49,760
so now that we looked at array library

00:12:48,000 --> 00:12:51,920
and the automatic differentiation

00:12:49,760 --> 00:12:53,680
let's also look at acceleration on

00:12:51,920 --> 00:12:56,399
modern hardware

00:12:53,680 --> 00:12:56,800
so the driving factors behind this need

00:12:56,399 --> 00:13:00,000
are

00:12:56,800 --> 00:13:00,880
really the three parts we want larger

00:13:00,000 --> 00:13:03,440
networks

00:13:00,880 --> 00:13:04,399
the function f we want to train on more

00:13:03,440 --> 00:13:07,120
data

00:13:04,399 --> 00:13:08,959
and we want to iterate faster so that

00:13:07,120 --> 00:13:10,160
scientists time can be better utilized

00:13:08,959 --> 00:13:13,040
for science

00:13:10,160 --> 00:13:14,560
and uh not for sword fights on office

00:13:13,040 --> 00:13:17,440
shares

00:13:14,560 --> 00:13:19,120
um so first on on larger models um here

00:13:17,440 --> 00:13:21,760
i'm sharing a plot

00:13:19,120 --> 00:13:23,519
for the amount of compute of well-known

00:13:21,760 --> 00:13:25,519
deep learning models

00:13:23,519 --> 00:13:26,560
you may recognize some of the recent

00:13:25,519 --> 00:13:29,040
feats in

00:13:26,560 --> 00:13:30,959
deep learning such as alphago and your

00:13:29,040 --> 00:13:34,880
neural machine translation

00:13:30,959 --> 00:13:38,240
so the plot x-axis is a timeline

00:13:34,880 --> 00:13:39,480
and the y-axis is the logarithmic plot

00:13:38,240 --> 00:13:42,639
of

00:13:39,480 --> 00:13:44,240
a unit that represents compute it's the

00:13:42,639 --> 00:13:47,680
petaflops per second

00:13:44,240 --> 00:13:48,959
times the number of days so you can see

00:13:47,680 --> 00:13:51,519
a linear growth

00:13:48,959 --> 00:13:52,800
in this plot and because if it's a

00:13:51,519 --> 00:13:54,560
logarithmic plot

00:13:52,800 --> 00:13:56,240
we know that it's it's been growing

00:13:54,560 --> 00:13:59,839
exponentially since um

00:13:56,240 --> 00:14:03,120
the 2012 deep learning boom

00:13:59,839 --> 00:14:05,279
um and uh on more data so

00:14:03,120 --> 00:14:06,560
let's take a look at the amount of data

00:14:05,279 --> 00:14:09,920
human generates

00:14:06,560 --> 00:14:13,040
every minute so this is uh domo's

00:14:09,920 --> 00:14:14,079
data never sleeps infographics um you

00:14:13,040 --> 00:14:16,480
can see from

00:14:14,079 --> 00:14:18,079
on the left that the internet population

00:14:16,480 --> 00:14:23,279
has grown from 3 billion

00:14:18,079 --> 00:14:25,279
in 2014 to 4.57 billion in 2017.

00:14:23,279 --> 00:14:26,399
and every minute there is a dilution of

00:14:25,279 --> 00:14:29,120
data that's

00:14:26,399 --> 00:14:30,720
generated across all the different apps

00:14:29,120 --> 00:14:34,320
and services on the right

00:14:30,720 --> 00:14:37,680
for example for youtube on the

00:14:34,320 --> 00:14:40,639
top right corner there is 20 days

00:14:37,680 --> 00:14:41,120
worth of video generated every minute

00:14:40,639 --> 00:14:44,800
and

00:14:41,120 --> 00:14:47,760
on bottom left there is amazon where

00:14:44,800 --> 00:14:49,839
over 6000 packages are shipped so

00:14:47,760 --> 00:14:53,839
nowadays assume that

00:14:49,839 --> 00:14:57,120
many of those are probably toilet papers

00:14:53,839 --> 00:15:01,360
so given the

00:14:57,120 --> 00:15:04,160
the need for a larger model and

00:15:01,360 --> 00:15:07,199
more data processing more data the

00:15:04,160 --> 00:15:10,079
hardware is also innovating very fast

00:15:07,199 --> 00:15:12,320
so on the left it's a plot for the

00:15:10,079 --> 00:15:15,760
compute power of gpus

00:15:12,320 --> 00:15:17,839
the top of the line gpus over the years

00:15:15,760 --> 00:15:20,240
the gpu performance has increased nine

00:15:17,839 --> 00:15:22,639
volts since 2016.

00:15:20,240 --> 00:15:25,680
and on the right it's a comparison of

00:15:22,639 --> 00:15:27,440
the two most recent generations of tpu

00:15:25,680 --> 00:15:30,720
from google

00:15:27,440 --> 00:15:35,040
so on average across these different

00:15:30,720 --> 00:15:38,720
models they improve in performance by

00:15:35,040 --> 00:15:40,880
2.7 times so deep learning frameworks

00:15:38,720 --> 00:15:41,279
need to not only keep up with the fast

00:15:40,880 --> 00:15:43,360
pace

00:15:41,279 --> 00:15:45,759
in every aspect but also to lead the

00:15:43,360 --> 00:15:45,759
future

00:15:47,120 --> 00:15:50,720
so not that we talked about the

00:15:49,360 --> 00:15:53,600
fragmentation where it

00:15:50,720 --> 00:15:54,639
came from let's talk about the solution

00:15:53,600 --> 00:15:56,959
to it

00:15:54,639 --> 00:15:58,560
here i introduced two standardization

00:15:56,959 --> 00:15:59,759
efforts in machine learning and deep

00:15:58,560 --> 00:16:02,880
learning that i'm

00:15:59,759 --> 00:16:05,839
i'm really honored to participate in

00:16:02,880 --> 00:16:07,600
so the first one is uh python data api

00:16:05,839 --> 00:16:09,279
consortium

00:16:07,600 --> 00:16:11,040
the goal of the consortium is to

00:16:09,279 --> 00:16:13,920
standard standardized

00:16:11,040 --> 00:16:15,600
array and data frame api to address the

00:16:13,920 --> 00:16:19,040
fragmentation

00:16:15,600 --> 00:16:20,800
of libraries that offer them we assemble

00:16:19,040 --> 00:16:22,959
a consortium of people from

00:16:20,800 --> 00:16:24,160
interested companies and key community

00:16:22,959 --> 00:16:28,000
contributors

00:16:24,160 --> 00:16:30,320
to examine the design choices in in them

00:16:28,000 --> 00:16:31,279
and to propose the standards that are

00:16:30,320 --> 00:16:34,399
suitable for

00:16:31,279 --> 00:16:37,279
modern day tools the proposed

00:16:34,399 --> 00:16:39,920
standards are shared to the public as a

00:16:37,279 --> 00:16:42,560
request for comments or ifcs

00:16:39,920 --> 00:16:44,560
that library maintainers can provide

00:16:42,560 --> 00:16:47,519
feedback early on

00:16:44,560 --> 00:16:48,480
and adopt afterwards and the community

00:16:47,519 --> 00:16:52,800
participates

00:16:48,480 --> 00:16:52,800
in the reviews throughout this process

00:16:52,959 --> 00:16:59,199
the other effort is open neural network

00:16:56,160 --> 00:17:03,519
exchange or onyx

00:16:59,199 --> 00:17:06,880
so onyx is an effort that was

00:17:03,519 --> 00:17:09,520
started by facebook and microsoft

00:17:06,880 --> 00:17:10,959
it facilitates the exchange of deep

00:17:09,520 --> 00:17:13,760
learning models

00:17:10,959 --> 00:17:14,000
and it does so by providing a definition

00:17:13,760 --> 00:17:17,120
of

00:17:14,000 --> 00:17:19,199
extensible computation graph model

00:17:17,120 --> 00:17:22,480
as well as definitions of built-in

00:17:19,199 --> 00:17:25,679
operators and standard data types

00:17:22,480 --> 00:17:29,280
so um now let's move on to

00:17:25,679 --> 00:17:31,360
talk about mxnet we've

00:17:29,280 --> 00:17:32,480
covered what what the fragmentation

00:17:31,360 --> 00:17:34,320
problem is

00:17:32,480 --> 00:17:35,679
as well as the the knees in deep

00:17:34,320 --> 00:17:38,880
learning so

00:17:35,679 --> 00:17:42,160
uh we're prepared for this so um

00:17:38,880 --> 00:17:44,240
mxnet is a truly open source

00:17:42,160 --> 00:17:45,840
deep learning framework developed in the

00:17:44,240 --> 00:17:47,600
apache way

00:17:45,840 --> 00:17:49,520
it's a community of deep learning

00:17:47,600 --> 00:17:51,760
enthusiasts with the goal of

00:17:49,520 --> 00:17:54,160
democratizing ai and making sure that

00:17:51,760 --> 00:17:55,840
it's accessible to everyone

00:17:54,160 --> 00:17:59,280
it's designed to be flexible and

00:17:55,840 --> 00:18:03,039
efficient and ready for production

00:17:59,280 --> 00:18:04,960
um also um as i described before

00:18:03,039 --> 00:18:06,559
there are the two standards that are

00:18:04,960 --> 00:18:10,960
both adopted by

00:18:06,559 --> 00:18:14,240
mxnet so um

00:18:10,960 --> 00:18:16,720
in 2.0 we aim to provide the bridge

00:18:14,240 --> 00:18:17,600
between numpy based machine learning

00:18:16,720 --> 00:18:20,000
tools

00:18:17,600 --> 00:18:20,960
and deep learning and the two most

00:18:20,000 --> 00:18:22,960
notable features

00:18:20,960 --> 00:18:25,440
are the numpy compatible programming as

00:18:22,960 --> 00:18:27,760
well as the gluon 2.0

00:18:25,440 --> 00:18:28,640
the numpy compatible programming

00:18:27,760 --> 00:18:32,320
provides the

00:18:28,640 --> 00:18:34,640
np module which is a numpy compatible

00:18:32,320 --> 00:18:36,240
array library with the enhancement of

00:18:34,640 --> 00:18:39,520
auto differentiation

00:18:36,240 --> 00:18:41,520
and gpu acceleration we also provide

00:18:39,520 --> 00:18:43,360
npx which is the neural network

00:18:41,520 --> 00:18:48,000
extension to the

00:18:43,360 --> 00:18:50,480
numpy compatible array api for gluon 2.0

00:18:48,000 --> 00:18:52,160
we provide a simple high-level

00:18:50,480 --> 00:18:54,320
programming model

00:18:52,160 --> 00:18:55,600
that can optimize numpy enhanced deep

00:18:54,320 --> 00:18:57,679
learning

00:18:55,600 --> 00:18:59,360
overall the goal is to make deep

00:18:57,679 --> 00:19:02,880
learning and machine learning

00:18:59,360 --> 00:19:02,880
fast and also flexible

00:19:04,080 --> 00:19:10,240
so here's an example of such combination

00:19:07,919 --> 00:19:12,400
we write the neural network from the cat

00:19:10,240 --> 00:19:13,440
and dog example in just a few lines of

00:19:12,400 --> 00:19:17,039
code

00:19:13,440 --> 00:19:20,080
so overall the the class structure

00:19:17,039 --> 00:19:24,240
is what it's like for programming

00:19:20,080 --> 00:19:27,200
in glon 2.0 we can declare parameters

00:19:24,240 --> 00:19:28,880
for for weights and biases for that

00:19:27,200 --> 00:19:31,120
linear combination

00:19:28,880 --> 00:19:32,000
um and um in the forward function we

00:19:31,120 --> 00:19:35,280
define

00:19:32,000 --> 00:19:38,720
the compute where um we can use the

00:19:35,280 --> 00:19:41,919
numpy interface for the

00:19:38,720 --> 00:19:44,400
arithmetic operations and

00:19:41,919 --> 00:19:45,520
we also can use the neural network

00:19:44,400 --> 00:19:48,240
extension to

00:19:45,520 --> 00:19:49,760
to numpy for example for the softmax

00:19:48,240 --> 00:19:53,280
activation

00:19:49,760 --> 00:19:55,520
so once we have the the network

00:19:53,280 --> 00:19:57,039
we can create an instance of that and

00:19:55,520 --> 00:20:01,440
feed the

00:19:57,039 --> 00:20:01,440
data of a kitty through it

00:20:01,520 --> 00:20:07,440
so in gluon there is the concept

00:20:04,880 --> 00:20:09,679
called hybridization which is our

00:20:07,440 --> 00:20:13,200
just-in-time compilation

00:20:09,679 --> 00:20:16,480
it can optimize and export numpy

00:20:13,200 --> 00:20:17,840
and neural network models to different

00:20:16,480 --> 00:20:21,440
bindings

00:20:17,840 --> 00:20:24,720
of programming languages and we can also

00:20:21,440 --> 00:20:27,919
deploy them through tvm tensor rt

00:20:24,720 --> 00:20:31,200
or move refer to openvino

00:20:27,919 --> 00:20:34,480
and also we can exchange the model with

00:20:31,200 --> 00:20:37,360
even wider range of tools through onyx

00:20:34,480 --> 00:20:39,039
and once the python data api consortium

00:20:37,360 --> 00:20:42,240
standard

00:20:39,039 --> 00:20:44,799
is widely adopted it will be able to

00:20:42,240 --> 00:20:47,600
interoperate with even more tools that

00:20:44,799 --> 00:20:47,600
are participating

00:20:47,679 --> 00:20:52,720
um so this makes mxnet really portable

00:20:52,960 --> 00:20:56,960
and in 2.0 we also make it easier to

00:20:56,080 --> 00:20:58,880
customize

00:20:56,960 --> 00:21:01,520
which is an important need in the

00:20:58,880 --> 00:21:02,240
industry here i would focus on the first

00:21:01,520 --> 00:21:05,760
two items

00:21:02,240 --> 00:21:09,200
of um custom operators

00:21:05,760 --> 00:21:12,000
so for uh c plus plus operators

00:21:09,200 --> 00:21:12,799
we used to require that these operators

00:21:12,000 --> 00:21:15,840
are written

00:21:12,799 --> 00:21:16,960
with our interface and it needs to be

00:21:15,840 --> 00:21:20,799
compiled and

00:21:16,960 --> 00:21:21,919
linked to mxnet directly so this creates

00:21:20,799 --> 00:21:24,000
a need for

00:21:21,919 --> 00:21:26,080
maintaining a fork if anyone wants to

00:21:24,000 --> 00:21:29,120
add custom operations

00:21:26,080 --> 00:21:30,080
so in order to make it flexible we now

00:21:29,120 --> 00:21:32,799
offer

00:21:30,080 --> 00:21:34,240
a custom operator library that can be

00:21:32,799 --> 00:21:37,280
loaded at runtime

00:21:34,240 --> 00:21:38,080
that can be registered into our operator

00:21:37,280 --> 00:21:41,280
registry

00:21:38,080 --> 00:21:43,280
in the api compatible way so

00:21:41,280 --> 00:21:45,840
this removes the need to maintain the

00:21:43,280 --> 00:21:48,799
fork

00:21:45,840 --> 00:21:50,480
also we now support defining operators

00:21:48,799 --> 00:21:53,440
using tbm

00:21:50,480 --> 00:21:54,320
which then she talked about in this

00:21:53,440 --> 00:21:57,039
morning

00:21:54,320 --> 00:21:58,880
so here i have a comparison of the same

00:21:57,039 --> 00:22:02,080
operator that's written

00:21:58,880 --> 00:22:02,880
with tvm in python and also the hand

00:22:02,080 --> 00:22:05,679
optimized

00:22:02,880 --> 00:22:07,840
operator that's written in c plus plus

00:22:05,679 --> 00:22:10,640
so

00:22:07,840 --> 00:22:11,520
as you can see in in terms of the length

00:22:10,640 --> 00:22:14,960
it really

00:22:11,520 --> 00:22:17,840
improves the development efficiency

00:22:14,960 --> 00:22:20,320
and also tvm offers many ways of

00:22:17,840 --> 00:22:23,120
automatically optimizing for

00:22:20,320 --> 00:22:25,760
this compute which makes it really

00:22:23,120 --> 00:22:28,400
desirable for

00:22:25,760 --> 00:22:28,400
development

00:22:29,280 --> 00:22:35,440
so in 2.0 we also enhanced a lot in

00:22:32,799 --> 00:22:37,679
in terms of performance as well as

00:22:35,440 --> 00:22:41,200
adding more hardware support

00:22:37,679 --> 00:22:44,320
so we now support cuda graph

00:22:41,200 --> 00:22:44,880
for scheduling so that it removes uh one

00:22:44,320 --> 00:22:47,840
of the

00:22:44,880 --> 00:22:50,159
the largest overhead um in scheduling

00:22:47,840 --> 00:22:52,960
from the deep learning framework

00:22:50,159 --> 00:22:54,240
we also support runtime compilation with

00:22:52,960 --> 00:22:57,200
mvrtc

00:22:54,240 --> 00:22:58,720
which enables the runtime fusion of gpu

00:22:57,200 --> 00:23:00,799
operators

00:22:58,720 --> 00:23:02,159
so this would improve the speed as well

00:23:00,799 --> 00:23:05,039
as reduce

00:23:02,159 --> 00:23:06,240
the the memory consumption we also

00:23:05,039 --> 00:23:09,600
enhanced our dynamic

00:23:06,240 --> 00:23:10,320
graph execution so that graphs with a

00:23:09,600 --> 00:23:13,440
combination

00:23:10,320 --> 00:23:17,280
of static and dynamic parts it would be

00:23:13,440 --> 00:23:19,200
optimized as much as possible

00:23:17,280 --> 00:23:22,320
so for the static part it would be

00:23:19,200 --> 00:23:26,159
optimized in the exact same way as

00:23:22,320 --> 00:23:30,320
a static network and finally we enable

00:23:26,159 --> 00:23:34,080
automatic mixed precision which is

00:23:30,320 --> 00:23:36,480
automatically switching to fp16 for

00:23:34,080 --> 00:23:37,520
tensor core compute so this would

00:23:36,480 --> 00:23:39,360
improve the speed

00:23:37,520 --> 00:23:40,720
and since we offer it in the numpy

00:23:39,360 --> 00:23:44,400
compatible api

00:23:40,720 --> 00:23:46,880
it can be used for phenomena as well

00:23:44,400 --> 00:23:48,080
so here i share some of the performance

00:23:46,880 --> 00:23:51,760
highlights of

00:23:48,080 --> 00:23:54,960
mxnet um so mxnet

00:23:51,760 --> 00:23:58,559
here i'm sharing the uh highlights from

00:23:54,960 --> 00:24:00,880
malpur 0.7 which is released recently

00:23:58,559 --> 00:24:02,880
um it's a standard benchmark in deep

00:24:00,880 --> 00:24:06,159
learning with very strict competition

00:24:02,880 --> 00:24:08,799
rules mxnet is used in 23

00:24:06,159 --> 00:24:10,159
of the 52 available on-premise

00:24:08,799 --> 00:24:13,600
submissions

00:24:10,159 --> 00:24:14,720
so for two of the tasks that are well

00:24:13,600 --> 00:24:17,120
known

00:24:14,720 --> 00:24:20,000
here for image classification i on

00:24:17,120 --> 00:24:24,159
imagenet with less than 50.

00:24:20,000 --> 00:24:27,279
nvda submitted a record-breaking

00:24:24,159 --> 00:24:31,440
um submission which is 46 seconds

00:24:27,279 --> 00:24:35,120
with 1840

00:24:31,440 --> 00:24:37,679
a 100 gpus that's a lot of gpus

00:24:35,120 --> 00:24:38,799
and there's also a pretty good result

00:24:37,679 --> 00:24:42,559
from

00:24:38,799 --> 00:24:45,200
intel cpu where

00:24:42,559 --> 00:24:46,799
the the speed record was a little over

00:24:45,200 --> 00:24:50,080
18 minutes

00:24:46,799 --> 00:24:53,360
with just eight machines and uh

00:24:50,080 --> 00:24:57,200
on object detection on cocoa

00:24:53,360 --> 00:24:57,720
with ssd the speed record was 49 seconds

00:24:57,200 --> 00:25:02,000
with

00:24:57,720 --> 00:25:05,520
1024 a a100 gpus

00:25:02,000 --> 00:25:10,240
so uh we also now integrate with

00:25:05,520 --> 00:25:13,200
1dn which was called mkl dnn before

00:25:10,240 --> 00:25:14,720
so it helped a lot on the cpu

00:25:13,200 --> 00:25:18,559
performance so here i'm

00:25:14,720 --> 00:25:20,960
sharing a a bar chart for

00:25:18,559 --> 00:25:21,840
different models on the relative speed

00:25:20,960 --> 00:25:25,760
up of

00:25:21,840 --> 00:25:28,720
one dnn version versus the native

00:25:25,760 --> 00:25:29,760
version that we had before so overall we

00:25:28,720 --> 00:25:32,960
can see that

00:25:29,760 --> 00:25:35,679
it achieves an over five times uh

00:25:32,960 --> 00:25:37,039
improvement across the board and uh for

00:25:35,679 --> 00:25:41,520
mobilenet it achieved

00:25:37,039 --> 00:25:44,559
even higher speed up for over 40 times

00:25:41,520 --> 00:25:44,559
so this really is

00:25:45,120 --> 00:25:53,919
attributed to the optimization in 1dnn

00:25:48,880 --> 00:25:53,919
for avx 512 optimization

00:25:54,640 --> 00:26:00,240
so um let's also talk about the low cost

00:25:57,679 --> 00:26:00,720
which is important in machine learning

00:26:00,240 --> 00:26:03,600
so

00:26:00,720 --> 00:26:04,400
i'm sharing a global nlp birth real-time

00:26:03,600 --> 00:26:07,279
inference

00:26:04,400 --> 00:26:08,559
result the blog was posted uh just

00:26:07,279 --> 00:26:11,919
yesterday

00:26:08,559 --> 00:26:12,880
so um the model is bert which is uh the

00:26:11,919 --> 00:26:15,919
bi-directional

00:26:12,880 --> 00:26:16,640
embedding from transformers uh it's one

00:26:15,919 --> 00:26:19,760
of the

00:26:16,640 --> 00:26:21,039
state of the art transformer based

00:26:19,760 --> 00:26:24,880
models

00:26:21,039 --> 00:26:28,720
that first broke the human record for

00:26:24,880 --> 00:26:31,279
natural language understanding task so

00:26:28,720 --> 00:26:34,240
i'm sharing i'm going to share the

00:26:31,279 --> 00:26:35,919
result on the sentiment analysis task

00:26:34,240 --> 00:26:38,640
which is showing in the upper right

00:26:35,919 --> 00:26:41,200
corner so suppose that uh

00:26:38,640 --> 00:26:41,840
we're looking at a book review comment

00:26:41,200 --> 00:26:45,520
which is

00:26:41,840 --> 00:26:48,720
uh saying that it's great for insomniacs

00:26:45,520 --> 00:26:51,120
so uh this is not something that

00:26:48,720 --> 00:26:52,000
we would look for in the quality of the

00:26:51,120 --> 00:26:55,360
book

00:26:52,000 --> 00:26:59,200
but uh even though it says it's great

00:26:55,360 --> 00:27:02,480
um so it's a hard task

00:26:59,200 --> 00:27:06,559
so for the inference cost

00:27:02,480 --> 00:27:09,440
here i'm showing a bar chart for

00:27:06,559 --> 00:27:10,640
the the bird sentiment classification

00:27:09,440 --> 00:27:13,840
task

00:27:10,640 --> 00:27:17,120
for different sequence lengths and

00:27:13,840 --> 00:27:21,520
each bar of different color represents

00:27:17,120 --> 00:27:22,399
the cost in dollars per 1 million

00:27:21,520 --> 00:27:25,600
requests

00:27:22,399 --> 00:27:28,880
on four different types of

00:27:25,600 --> 00:27:31,919
hardware instances ec2

00:27:28,880 --> 00:27:36,320
so um as we can see for

00:27:31,919 --> 00:27:39,440
the g4dn x large with the half precision

00:27:36,320 --> 00:27:42,159
inference we can achieve a million

00:27:39,440 --> 00:27:45,600
requests for just 20 cents

00:27:42,159 --> 00:27:47,520
which is you know a pretty

00:27:45,600 --> 00:27:49,360
nice cost reduction over the the

00:27:47,520 --> 00:27:51,760
previous results

00:27:49,360 --> 00:27:52,399
in comparison some of the publicly

00:27:51,760 --> 00:27:54,880
available

00:27:52,399 --> 00:27:56,320
nlp services such as hugging phase

00:27:54,880 --> 00:27:58,559
offers the

00:27:56,320 --> 00:27:59,600
same amount of influence at a monthly

00:27:58,559 --> 00:28:04,480
cost of

00:27:59,600 --> 00:28:04,480
200 dollars so a thousand times

00:28:04,799 --> 00:28:10,960
um looks like i still have some time so

00:28:08,480 --> 00:28:12,000
last but not least let me introduce the

00:28:10,960 --> 00:28:15,120
ecosystem

00:28:12,000 --> 00:28:17,840
and the uh the different projects that

00:28:15,120 --> 00:28:20,320
support mxnet

00:28:17,840 --> 00:28:22,720
first one is autogluon so autocoulon is

00:28:20,320 --> 00:28:23,760
an automl tool that enables automatic

00:28:22,720 --> 00:28:26,799
machine learning

00:28:23,760 --> 00:28:29,919
with just three lines of code it

00:28:26,799 --> 00:28:30,320
automatically ensembles various models

00:28:29,919 --> 00:28:32,960
and

00:28:30,320 --> 00:28:35,919
it performs hyper parameter optimization

00:28:32,960 --> 00:28:38,480
to pick the best combination of models

00:28:35,919 --> 00:28:39,120
for the task at hand as shown in the

00:28:38,480 --> 00:28:43,360
table

00:28:39,120 --> 00:28:47,120
so in the comparison to other hml tools

00:28:43,360 --> 00:28:49,520
on 11 kaggle competitions

00:28:47,120 --> 00:28:50,399
autogrown performed the best on seven of

00:28:49,520 --> 00:28:53,760
them

00:28:50,399 --> 00:28:53,760
was the least amount of time

00:28:54,240 --> 00:28:59,120
the other project i like to share is

00:28:56,399 --> 00:29:00,320
that it's a dive into deep learning it's

00:28:59,120 --> 00:29:02,559
a deep learning book

00:29:00,320 --> 00:29:04,240
with the perfect combination of

00:29:02,559 --> 00:29:07,440
knowledge as well as

00:29:04,240 --> 00:29:10,480
hands-on practice so it's written as

00:29:07,440 --> 00:29:12,480
jupiter notebooks to help build on the

00:29:10,480 --> 00:29:14,159
hands-on skills and the solid foundation

00:29:12,480 --> 00:29:16,640
in deep learning

00:29:14,159 --> 00:29:18,559
and it also offers a very active

00:29:16,640 --> 00:29:19,039
community of learners that help each

00:29:18,559 --> 00:29:21,919
other

00:29:19,039 --> 00:29:24,320
to create the best learning experience

00:29:21,919 --> 00:29:27,840
jason huang the ceo of nvidia also

00:29:24,320 --> 00:29:27,840
highly recommends it

00:29:28,080 --> 00:29:32,240
it's also used in more than 100

00:29:30,240 --> 00:29:34,559
universities worldwide

00:29:32,240 --> 00:29:36,320
as textbooks for teaching deep learning

00:29:34,559 --> 00:29:39,039
and machine learning

00:29:36,320 --> 00:29:41,679
and half of the top 30 universities use

00:29:39,039 --> 00:29:44,399
d2l

00:29:41,679 --> 00:29:46,559
in computer vision we have glon cv which

00:29:44,399 --> 00:29:48,080
is a versatile deep learning for

00:29:46,559 --> 00:29:49,919
computer vision toolkit

00:29:48,080 --> 00:29:52,159
that provides training and deployment

00:29:49,919 --> 00:29:54,720
for a wide range of tasks

00:29:52,159 --> 00:29:55,919
they include image classification

00:29:54,720 --> 00:29:58,320
detection

00:29:55,919 --> 00:29:59,200
segmentation on the right action

00:29:58,320 --> 00:30:03,120
recognition

00:29:59,200 --> 00:30:07,600
and post estimation on videos

00:30:03,120 --> 00:30:10,240
so more recently it

00:30:07,600 --> 00:30:10,799
also offers the depth estimation as well

00:30:10,240 --> 00:30:12,960
as

00:30:10,799 --> 00:30:15,120
generative adversarial networks for

00:30:12,960 --> 00:30:18,799
various image generation tasks

00:30:15,120 --> 00:30:18,799
so all these spaces are fake

00:30:19,840 --> 00:30:23,200
another great computer vision toolkit

00:30:21,600 --> 00:30:25,760
i'd like to highlight is inside

00:30:23,200 --> 00:30:27,360
face it's um deep learning toolkit for

00:30:25,760 --> 00:30:29,360
face analysis

00:30:27,360 --> 00:30:32,080
it provides implementations and the

00:30:29,360 --> 00:30:34,080
pre-trained state-of-the-art models

00:30:32,080 --> 00:30:35,120
the two killer features are the arc

00:30:34,080 --> 00:30:38,480
phase model for

00:30:35,120 --> 00:30:41,840
face recognition and the award-winning

00:30:38,480 --> 00:30:44,880
retina phase model for face localization

00:30:41,840 --> 00:30:45,679
so inside face is a famous and widely

00:30:44,880 --> 00:30:48,640
used toolkit

00:30:45,679 --> 00:30:51,200
in both academia and industry it aims to

00:30:48,640 --> 00:30:54,480
become the the center for innovation

00:30:51,200 --> 00:30:57,679
in uh deep phase analysis

00:30:54,480 --> 00:31:00,240
now moving on to nlp uh we have glow iop

00:30:57,679 --> 00:31:02,399
which provides training and deployment

00:31:00,240 --> 00:31:03,600
for a wide variety of natural language

00:31:02,399 --> 00:31:06,559
tasks

00:31:03,600 --> 00:31:07,120
such as sentiment analysis natural

00:31:06,559 --> 00:31:10,880
language

00:31:07,120 --> 00:31:13,840
inference text generation translation

00:31:10,880 --> 00:31:15,360
and sequence labeling it offers over a

00:31:13,840 --> 00:31:18,159
thousand pre-trained models

00:31:15,360 --> 00:31:20,000
and as i shared before it offers the low

00:31:18,159 --> 00:31:22,640
cost influence too

00:31:20,000 --> 00:31:24,240
in the next major version of global nlp

00:31:22,640 --> 00:31:26,320
we provide the numpy based

00:31:24,240 --> 00:31:27,279
implementation and also more backbone

00:31:26,320 --> 00:31:30,840
networks

00:31:27,279 --> 00:31:32,080
and the data processing tools for faster

00:31:30,840 --> 00:31:34,640
iteration

00:31:32,080 --> 00:31:36,559
so 1lp focuses on the industry and

00:31:34,640 --> 00:31:39,360
powers many of the

00:31:36,559 --> 00:31:40,799
nlp services in the west as well as in

00:31:39,360 --> 00:31:44,080
alexa's

00:31:40,799 --> 00:31:44,080
natural language understanding

00:31:44,240 --> 00:31:51,039
another great nlp toolkit is sockeye

00:31:47,919 --> 00:31:53,760
it's a sequence to sequence toolkit that

00:31:51,039 --> 00:31:54,640
specializes in translation uh it

00:31:53,760 --> 00:31:56,399
provides

00:31:54,640 --> 00:32:00,000
state-of-the-art translation models and

00:31:56,399 --> 00:32:02,880
powers uh the amazon translate service

00:32:00,000 --> 00:32:03,279
in the latest version it adopts glon api

00:32:02,880 --> 00:32:06,240
that

00:32:03,279 --> 00:32:06,640
achieved 14 increased training speed

00:32:06,240 --> 00:32:09,679
with

00:32:06,640 --> 00:32:12,399
one quarter less lines of code it offers

00:32:09,679 --> 00:32:13,679
a faster model training now with inter

00:32:12,399 --> 00:32:17,279
integration

00:32:13,679 --> 00:32:20,799
of amp and horror mod it also

00:32:17,279 --> 00:32:25,120
achieves 3.4 times faster translation

00:32:20,799 --> 00:32:25,120
with quantized matrix multiplication

00:32:25,519 --> 00:32:29,279
guantis is a deep learning for time

00:32:28,399 --> 00:32:32,080
series toolkit

00:32:29,279 --> 00:32:34,320
that powers amazon forecast it's

00:32:32,080 --> 00:32:36,320
designed to be modular and scalable with

00:32:34,320 --> 00:32:38,320
the production stability

00:32:36,320 --> 00:32:40,080
you can mix different modules of

00:32:38,320 --> 00:32:42,320
distributions

00:32:40,080 --> 00:32:44,000
probabilistic components and the neural

00:32:42,320 --> 00:32:48,720
network structures for different

00:32:44,000 --> 00:32:48,720
approaches in time series modeling

00:32:48,960 --> 00:32:56,080
and uh next is dji this is more of a

00:32:52,640 --> 00:32:58,399
cutting-edge research topic so deep

00:32:56,080 --> 00:33:00,480
graph library or dgl

00:32:58,399 --> 00:33:01,519
is a flexible graph neural network

00:33:00,480 --> 00:33:03,440
toolkit

00:33:01,519 --> 00:33:06,000
it's widely adopted in the research

00:33:03,440 --> 00:33:08,320
community and has high performance

00:33:06,000 --> 00:33:10,240
it also offers domain specific tools for

00:33:08,320 --> 00:33:14,880
knowledge graph embedding and

00:33:10,240 --> 00:33:14,880
life science in chemistry and biology

00:33:15,200 --> 00:33:21,039
um yeah again so that's the end of

00:33:18,240 --> 00:33:21,760
my my talk again uh if you want to get

00:33:21,039 --> 00:33:25,440
involved

00:33:21,760 --> 00:33:28,480
in mxnet here are the different channels

00:33:25,440 --> 00:33:31,919
that you can connect with us and

00:33:28,480 --> 00:33:36,320
reach out to us all right with that

00:33:31,919 --> 00:33:38,000
um i'll switch back from the

00:33:36,320 --> 00:33:39,679
full screen so that i can see the

00:33:38,000 --> 00:33:42,159
questions

00:33:39,679 --> 00:33:42,159
thank you

00:33:50,840 --> 00:33:53,840
so

00:33:55,519 --> 00:33:58,559
all right so uh i saw a question from

00:33:57,840 --> 00:34:03,440
crook

00:33:58,559 --> 00:34:05,679
um so um you mean with mxnet we can

00:34:03,440 --> 00:34:07,440
build a deep learning model and then

00:34:05,679 --> 00:34:10,720
exports the trained model for mobile

00:34:07,440 --> 00:34:14,079
apps yes that's the case

00:34:10,720 --> 00:34:16,399
and since we integrate with tbm

00:34:14,079 --> 00:34:17,200
dvm can actually optimize for different

00:34:16,399 --> 00:34:19,919
hardwares

00:34:17,200 --> 00:34:20,879
for that mobile so that you can get the

00:34:19,919 --> 00:34:35,119
performance

00:34:20,879 --> 00:34:38,159
benefits from it too

00:34:35,119 --> 00:34:39,760
yeah any other question also uh remember

00:34:38,159 --> 00:34:42,960
to check out the

00:34:39,760 --> 00:34:45,599
dji library uh talk

00:34:42,960 --> 00:34:48,000
from tomorrow so chain a colleague of

00:34:45,599 --> 00:34:50,159
mine and also a ppmc member of mxnet

00:34:48,000 --> 00:34:53,760
will be talking about deep learning in

00:34:50,159 --> 00:34:53,760
java extensively tomorrow

00:34:58,320 --> 00:35:03,200
can you say something more about the new

00:35:00,720 --> 00:35:07,040
2.0 versions what are the changes

00:35:03,200 --> 00:35:10,560
so the two most notable changes are

00:35:07,040 --> 00:35:14,240
the numpy compatible api for programming

00:35:10,560 --> 00:35:17,200
this would enable numpy models to

00:35:14,240 --> 00:35:18,079
be optimized and deployed in a similar

00:35:17,200 --> 00:35:22,079
way as

00:35:18,079 --> 00:35:24,880
deep learning models from mxnet in 1.x

00:35:22,079 --> 00:35:27,359
and also in 2.0 we simplified our gluon

00:35:24,880 --> 00:35:31,839
interface so that

00:35:27,359 --> 00:35:35,119
it's more flexible

00:35:31,839 --> 00:35:38,960
yeah it's more flexible and can

00:35:35,119 --> 00:35:45,839
optimize different models so that we can

00:35:38,960 --> 00:35:45,839
help the speed of innovation

00:36:05,200 --> 00:36:15,839
um any other question

00:36:12,320 --> 00:36:19,040
um so uh 2.0 release date

00:36:15,839 --> 00:36:22,000
um we're going through a series of um

00:36:19,040 --> 00:36:23,040
beta releases for 2.0 first um we want

00:36:22,000 --> 00:36:26,320
to

00:36:23,040 --> 00:36:29,040
make sure that it's of a good quality

00:36:26,320 --> 00:36:31,200
before we call it an official release so

00:36:29,040 --> 00:36:34,640
we don't have a date set in stone but we

00:36:31,200 --> 00:36:34,640
do have a quality bar in mind

00:36:36,560 --> 00:36:41,839
the first beta release would be

00:36:38,000 --> 00:36:41,839
happening in the following months

00:37:09,040 --> 00:37:23,839
all right thank you

00:37:27,119 --> 00:37:29,200

YouTube URL: https://www.youtube.com/watch?v=lVLV9XLFnJI


