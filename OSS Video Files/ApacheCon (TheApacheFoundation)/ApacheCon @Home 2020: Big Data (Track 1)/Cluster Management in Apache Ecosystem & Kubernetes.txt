Title: Cluster Management in Apache Ecosystem & Kubernetes
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Cluster Management in Apache Ecosystem & Kubernetes
Shekhar Prasad Rajak

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache have powerful cluster & resource manager already, so do we really need to use Kubernetes for the deployment while using Apache projects ? Let's find out what type of cluster management system Apache already have ,How cluster management works in each of below cases and when we don't need any other cluster management top of it and when we can leverage the power of both this apache cluster modes and Kubernetes in resource & cluster management. * Apache Spark Standalone: A simple cluster manager available as part of the Spark distribution.. * Apache Mesos: A general purpose distributed OS level push based scheduler & resource manager. * Apache Hadoop YARN: A distributed computing framework for monolithic job scheduling and cluster resource management for Hadoop cluster (Apache/CDH/HDP) We will see some benchmarks and features that kubernetes can provide but it is not present(or not mature enough) in the Apache ecosystem, but still using, one or both can improve the performance. We will deep dive into fundamentals of Kubernetes and Apache distribution, resource & cluster management system, Job scheduling, to get clear cut idea behind both ecosystems and why they are best in particular cases like Big Data, Machine Learning, Load balancer, and so on. Applications are containerised in Kubernetes Pod, Kubernetes Service is used as Load balancer, Kubernetes High availability is because of distribution of Pods in worker nodes, Local Storage, Persistent volume & Networking and many other features will be compared side by side with Apache Ecosystem. Like in Mesos, Application Group models dependencies as a tree of groups and Components are started in dependency order, Mesos-DNS works as basic load balancer, applications distribution among slave nodes, two-level scheduling mechanism, modern kernel "cgroups" in Linux & "zones" in Solaris, and so on. Along with the comparison & benchmark the talk will provide practical guide to use the Apache project with Kubernetes. Audience will understand the Software System design and generic problems of processing the request through the cluster & resource managers and why it is important to have modular, micro service based, loosely coupled software design, so that it can easily go through the container or OS level cluster management systems. This talk is clearly not to show who is winning but how can you win in your time, in the dark situation.

Shekhar is passionate about Open Source Softwares and active in various Open Source Projects. During college days he has contributed SymPy - Python library for symbolic mathematics , Data Science related Ruby gems like: daru, dart-view(Author), nyaplot - which is under Ruby Science Foundation (SciRuby), Bundler: a gem to bundle gems, NumPy & SciPy for creating the interactive website and documentation website using sphinx and Hugo framework, CloudCV for migrating the Angular JS application to Angular 8, and few others. He has successfully completed Google Summer of Code 2016 & 2017 and mentored students after that on 2018, 2019. Shekhar also talked about daru-view gem in RubyConf India 2018 and PyCon India 2017 on SymPy & SymEngine.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,160 --> 00:00:27,279
so

00:00:25,199 --> 00:00:29,599
okay thanks everyone for joining me in

00:00:27,279 --> 00:00:31,760
this talk titled cluster management in

00:00:29,599 --> 00:00:34,719
apache ecosystem and kubernetes

00:00:31,760 --> 00:00:36,640
so in this and different cluster

00:00:34,719 --> 00:00:38,239
managers available in apache ecosystem

00:00:36,640 --> 00:00:42,000
and do we really need

00:00:38,239 --> 00:00:42,000
kubernetes on top of it

00:00:43,520 --> 00:00:50,480
so we will focus more on apache mesos

00:00:47,360 --> 00:00:52,960
hadoop yarn and kubernetes

00:00:50,480 --> 00:00:54,239
but we will not be comparing it we will

00:00:52,960 --> 00:00:57,120
be understanding

00:00:54,239 --> 00:00:58,320
the initial implementation and what are

00:00:57,120 --> 00:01:01,039
the research people

00:00:58,320 --> 00:01:01,600
tell us about uh the logic behind these

00:01:01,039 --> 00:01:03,840
uh

00:01:01,600 --> 00:01:04,799
customer managers and resource managers

00:01:03,840 --> 00:01:07,520
we will see some

00:01:04,799 --> 00:01:10,080
uh promising features and how they are

00:01:07,520 --> 00:01:10,080
implemented

00:01:10,400 --> 00:01:16,560
we pick spark framework and see how

00:01:14,000 --> 00:01:17,920
different these are different approaches

00:01:16,560 --> 00:01:21,280
to deploy it in

00:01:17,920 --> 00:01:22,880
this cluster managers

00:01:21,280 --> 00:01:25,119
also we will understand why

00:01:22,880 --> 00:01:26,720
containerization is very important and

00:01:25,119 --> 00:01:28,880
what are the software design patterns

00:01:26,720 --> 00:01:31,759
and principles we should

00:01:28,880 --> 00:01:34,320
be careful while building the micro

00:01:31,759 --> 00:01:34,320
services

00:01:35,680 --> 00:01:40,560
about me my name is shekhar prasad i am

00:01:38,799 --> 00:01:42,479
very passionate about open source

00:01:40,560 --> 00:01:44,799
softwares currently i am working as a

00:01:42,479 --> 00:01:47,439
software engineer apple

00:01:44,799 --> 00:01:48,159
i have completed my btec in computer

00:01:47,439 --> 00:01:50,399
science and

00:01:48,159 --> 00:01:54,399
engineering department from nasa

00:01:50,399 --> 00:01:56,640
institute of technology warangal

00:01:54,399 --> 00:01:57,439
my open source career started during my

00:01:56,640 --> 00:02:00,960
college days

00:01:57,439 --> 00:02:03,040
only so i successfully completed my

00:02:00,960 --> 00:02:06,640
google summer of code project

00:02:03,040 --> 00:02:10,000
twice in 2016 i worked under simpli

00:02:06,640 --> 00:02:12,800
which is python library for symbolic

00:02:10,000 --> 00:02:13,840
mathematics and 2017 under ruby science

00:02:12,800 --> 00:02:15,920
foundation

00:02:13,840 --> 00:02:17,440
who built the various scientific

00:02:15,920 --> 00:02:21,120
computing rubygem

00:02:17,440 --> 00:02:23,440
i also mentored various

00:02:21,120 --> 00:02:24,640
students under different organization

00:02:23,440 --> 00:02:27,599
and

00:02:24,640 --> 00:02:28,400
i have also contributed more than 10

00:02:27,599 --> 00:02:31,840
organization

00:02:28,400 --> 00:02:31,840
in very different way

00:02:32,560 --> 00:02:39,280
you can find me

00:02:36,080 --> 00:02:41,680
twitter github or linkedin with this

00:02:39,280 --> 00:02:41,680
handler

00:02:41,760 --> 00:02:47,840
or iphone music

00:02:44,800 --> 00:02:48,879
let's back into the talk so let's

00:02:47,840 --> 00:02:52,560
understand

00:02:48,879 --> 00:02:52,560
the history behind this cluster manager

00:02:55,920 --> 00:02:59,440
start with very simple cluster manager

00:02:58,000 --> 00:03:02,560
is part of standalone

00:02:59,440 --> 00:03:04,239
which is not a separate standalone which

00:03:02,560 --> 00:03:08,159
is not the cluster management

00:03:04,239 --> 00:03:10,400
tool for production this build for

00:03:08,159 --> 00:03:12,959
spark specifically so this is the

00:03:10,400 --> 00:03:16,319
requirement to run the spark jobs uh

00:03:12,959 --> 00:03:16,319
under a cluster manager

00:03:16,640 --> 00:03:23,280
so as we know spark is a

00:03:20,080 --> 00:03:24,640
framework for uh for doing data

00:03:23,280 --> 00:03:27,519
processing very effective

00:03:24,640 --> 00:03:27,519
and efficient way

00:03:30,560 --> 00:03:38,159
so let's understand that cluster man's

00:03:33,760 --> 00:03:40,239
manager in spark first

00:03:38,159 --> 00:03:42,799
so there are different components spark

00:03:40,239 --> 00:03:45,760
master spock worker and other companies

00:03:42,799 --> 00:03:50,159
like history server

00:03:45,760 --> 00:03:52,400
so once we have uh client

00:03:50,159 --> 00:03:54,080
submit our spark application to spark

00:03:52,400 --> 00:03:57,280
master

00:03:54,080 --> 00:03:58,560
master first talk to one of the spark

00:03:57,280 --> 00:04:00,799
worker node

00:03:58,560 --> 00:04:02,239
to start the driver for this application

00:04:00,799 --> 00:04:05,760
and schedule the task

00:04:02,239 --> 00:04:09,120
it wants to run so once driver

00:04:05,760 --> 00:04:10,400
uh give the metadata about what are the

00:04:09,120 --> 00:04:15,840
tasks

00:04:10,400 --> 00:04:15,840
it wants to run and what are the

00:04:16,079 --> 00:04:22,160
guess sorry so once

00:04:20,239 --> 00:04:24,240
travel provides the metadata about what

00:04:22,160 --> 00:04:27,280
is the task it wants to run

00:04:24,240 --> 00:04:27,600
and what are the what are the cpu core

00:04:27,280 --> 00:04:30,800
or

00:04:27,600 --> 00:04:33,199
memory it needs to the master

00:04:30,800 --> 00:04:34,960
spark master spark master then

00:04:33,199 --> 00:04:38,400
communicate the spark worker node to

00:04:34,960 --> 00:04:38,400
execute those stars parallely

00:04:38,800 --> 00:04:43,199
and once they are started executing the

00:04:41,040 --> 00:04:46,000
executors will directly communicate to

00:04:43,199 --> 00:04:49,199
spark scheduler

00:04:46,000 --> 00:04:56,080
and update about the task execution and

00:04:49,199 --> 00:04:59,120
what is the status of it

00:04:56,080 --> 00:05:00,000
so this works only for spark application

00:04:59,120 --> 00:05:02,400
which is simple

00:05:00,000 --> 00:05:05,280
and quick we don't have to do any

00:05:02,400 --> 00:05:07,759
unnecessary configuration for it

00:05:05,280 --> 00:05:08,400
various recovery configuration like file

00:05:07,759 --> 00:05:10,800
system

00:05:08,400 --> 00:05:11,440
uh and zookeeper so if worker node goes

00:05:10,800 --> 00:05:13,199
down it

00:05:11,440 --> 00:05:14,880
will be restarted automatically if

00:05:13,199 --> 00:05:17,919
master node goes down

00:05:14,880 --> 00:05:19,440
we can recover it using uh file system

00:05:17,919 --> 00:05:23,600
mode or zookeeper we will see

00:05:19,440 --> 00:05:27,120
this configuration in later slides

00:05:23,600 --> 00:05:27,919
dynamic allocation is promising feature

00:05:27,120 --> 00:05:31,520
here because

00:05:27,919 --> 00:05:34,720
uh if we have spark cell started

00:05:31,520 --> 00:05:38,000
on the xu executors have all

00:05:34,720 --> 00:05:39,919
already started the allocated the

00:05:38,000 --> 00:05:42,400
resources to run the task

00:05:39,919 --> 00:05:44,639
but if we are not using it then it

00:05:42,400 --> 00:05:48,240
should be

00:05:44,639 --> 00:05:50,080
usable to other spark driver or spark

00:05:48,240 --> 00:05:52,800
application

00:05:50,080 --> 00:05:53,440
so if it is idle there it temporarily

00:05:52,800 --> 00:05:57,039
free the

00:05:53,440 --> 00:05:59,759
executor uh or resources so that other

00:05:57,039 --> 00:06:02,960
application can use it

00:05:59,759 --> 00:06:04,000
so that that makes it a very memory

00:06:02,960 --> 00:06:07,840
efficient and

00:06:04,000 --> 00:06:07,840
uh utilization of resources

00:06:09,360 --> 00:06:15,840
okay let's go ahead with

00:06:12,720 --> 00:06:16,479
another uh customer manager you know in

00:06:15,840 --> 00:06:19,759
apache

00:06:16,479 --> 00:06:21,759
ecosystem watching missiles this is the

00:06:19,759 --> 00:06:23,440
initial vapor i found a platform for

00:06:21,759 --> 00:06:25,759
fine grained resource sharing in data

00:06:23,440 --> 00:06:28,880
centers so this provide basically a

00:06:25,759 --> 00:06:31,600
linux kernel to run our uh processes

00:06:28,880 --> 00:06:32,479
in isolated way so it provides the

00:06:31,600 --> 00:06:36,639
isolation

00:06:32,479 --> 00:06:36,639
file system and the network itself

00:06:39,360 --> 00:06:45,280
and spark was initially

00:06:42,639 --> 00:06:49,199
started as a project exemplary project

00:06:45,280 --> 00:06:52,240
for apache mesos

00:06:49,199 --> 00:06:53,520
the main feature here is uh apache

00:06:52,240 --> 00:06:56,960
messages is very

00:06:53,520 --> 00:07:00,080
scalable it can run um

00:06:56,960 --> 00:07:03,840
10 000 more than 2050 nodes

00:07:00,080 --> 00:07:03,840
as per the research paper

00:07:04,000 --> 00:07:09,360
what makes this more scalable uh the

00:07:07,199 --> 00:07:13,120
concept of resource offer

00:07:09,360 --> 00:07:14,800
and utilization strategy so

00:07:13,120 --> 00:07:16,720
the apache missiles are two main

00:07:14,800 --> 00:07:20,960
components of

00:07:16,720 --> 00:07:24,720
message master and its worker notes

00:07:20,960 --> 00:07:28,800
so mesos worker knows keep

00:07:24,720 --> 00:07:28,800
give the update about the resources it

00:07:28,840 --> 00:07:33,599
has

00:07:30,080 --> 00:07:36,319
let's say uh here

00:07:33,599 --> 00:07:37,599
the worker node is saying i have four

00:07:36,319 --> 00:07:40,639
cpu and 4gb

00:07:37,599 --> 00:07:41,520
memory here worker node is saying uh i

00:07:40,639 --> 00:07:44,960
have

00:07:41,520 --> 00:07:47,440
another cpu 4gb memory

00:07:44,960 --> 00:07:48,800
so once mesos master knows all the

00:07:47,440 --> 00:07:52,319
resources available in

00:07:48,800 --> 00:07:53,440
our worker nodes it offers the resources

00:07:52,319 --> 00:07:55,360
to the framework

00:07:53,440 --> 00:07:56,560
frameworks that have registered to

00:07:55,360 --> 00:08:00,560
message master

00:07:56,560 --> 00:08:01,360
so frameworks are um either accepted or

00:08:00,560 --> 00:08:03,840
rejected

00:08:01,360 --> 00:08:05,919
as per their con constrained of running

00:08:03,840 --> 00:08:08,000
the task

00:08:05,919 --> 00:08:09,680
so once it accept the resources it

00:08:08,000 --> 00:08:11,039
provides the metadata whatever the task

00:08:09,680 --> 00:08:12,160
it wants to run and what are the

00:08:11,039 --> 00:08:15,199
resources it wants to

00:08:12,160 --> 00:08:15,840
use to message master and mrs master

00:08:15,199 --> 00:08:17,680
then

00:08:15,840 --> 00:08:20,400
talk to mesos exhibitor to run those

00:08:17,680 --> 00:08:24,560
tasks parallelly

00:08:20,400 --> 00:08:27,520
so it has it uses the zookeeper

00:08:24,560 --> 00:08:29,840
for recovery system so if any master

00:08:27,520 --> 00:08:32,719
node goes down

00:08:29,840 --> 00:08:34,719
all the metadata already stored in

00:08:32,719 --> 00:08:38,240
zookeeper

00:08:34,719 --> 00:08:40,800
will be restored so let's say

00:08:38,240 --> 00:08:42,000
message master goes down and there is a

00:08:40,800 --> 00:08:45,600
multiple

00:08:42,000 --> 00:08:49,200
uh master standby mode so all the

00:08:45,600 --> 00:08:51,680
mesos worker node get to know that

00:08:49,200 --> 00:08:54,320
uh current message master is gold down

00:08:51,680 --> 00:08:57,200
they elected the new mesos master

00:08:54,320 --> 00:08:57,920
and start communicating to that message

00:08:57,200 --> 00:09:02,000
master

00:08:57,920 --> 00:09:02,000
which is recovered from the zookeeper

00:09:02,720 --> 00:09:06,720
so this makes it highly scalable in

00:09:05,760 --> 00:09:10,320
terms of

00:09:06,720 --> 00:09:12,560
using the resources and

00:09:10,320 --> 00:09:14,160
if there is new node came into picture

00:09:12,560 --> 00:09:17,920
it can easily

00:09:14,160 --> 00:09:21,360
add that into our mesos worker node

00:09:17,920 --> 00:09:22,000
and this makes is more available and

00:09:21,360 --> 00:09:24,399
reliable

00:09:22,000 --> 00:09:24,399
system

00:09:25,920 --> 00:09:31,360
it can run any kind of task homogeneous

00:09:28,480 --> 00:09:31,360
or heterogeneous

00:09:31,600 --> 00:09:38,480
okay so let's see how patchy mesos will

00:09:35,279 --> 00:09:41,839
run the spark application

00:09:38,480 --> 00:09:44,959
so here we see main

00:09:41,839 --> 00:09:48,399
component mrs master missus executor

00:09:44,959 --> 00:09:51,920
the slave node and our client

00:09:48,399 --> 00:09:55,360
here spark driver is present

00:09:51,920 --> 00:09:58,160
in client jvm itself that means it is a

00:09:55,360 --> 00:10:00,720
client mode uh deployment of spark

00:09:58,160 --> 00:10:04,720
application

00:10:00,720 --> 00:10:06,959
so once um spark driver

00:10:04,720 --> 00:10:09,279
uh created for the spark application it

00:10:06,959 --> 00:10:11,760
registered to message master

00:10:09,279 --> 00:10:13,279
who knows all the resources available in

00:10:11,760 --> 00:10:16,480
meso slaves

00:10:13,279 --> 00:10:19,680
in real time it

00:10:16,480 --> 00:10:22,800
messes master one uh talks to

00:10:19,680 --> 00:10:26,800
any slave mode any

00:10:22,800 --> 00:10:30,000
uh meso slave and

00:10:26,800 --> 00:10:33,519
the metadata about running the job we

00:10:30,000 --> 00:10:35,839
got from the spark scheduler it executed

00:10:33,519 --> 00:10:38,399
missus executed

00:10:35,839 --> 00:10:40,000
so once the spark executor started

00:10:38,399 --> 00:10:42,079
running the task parallely

00:10:40,000 --> 00:10:43,600
it directly communicate to spark

00:10:42,079 --> 00:10:46,640
scheduler

00:10:43,600 --> 00:10:49,040
and update it

00:10:46,640 --> 00:10:52,560
independently from the cluster manager

00:10:49,040 --> 00:10:52,560
which is spark message here

00:10:53,600 --> 00:10:58,880
so spark mesos is very low labor

00:10:57,040 --> 00:10:59,360
infrastructure as a service so if you

00:10:58,880 --> 00:11:03,040
want to

00:10:59,360 --> 00:11:04,800
build or if you want to deploy something

00:11:03,040 --> 00:11:09,200
we have to do

00:11:04,800 --> 00:11:11,360
we have to configure it accordingly so

00:11:09,200 --> 00:11:13,120
various projects on top of mesos

00:11:11,360 --> 00:11:15,279
mesosphere one of them which provides

00:11:13,120 --> 00:11:17,600
platform as a service

00:11:15,279 --> 00:11:18,640
so that we can run any message queue

00:11:17,600 --> 00:11:23,279
service or

00:11:18,640 --> 00:11:23,279
database or any application on top of it

00:11:24,880 --> 00:11:32,240
also um early version of the mesos

00:11:28,640 --> 00:11:35,600
we see uh it provides default container

00:11:32,240 --> 00:11:36,640
system but once doc uh in later version

00:11:35,600 --> 00:11:38,480
we see

00:11:36,640 --> 00:11:40,160
uh it supports started supporting docker

00:11:38,480 --> 00:11:42,320
container as well

00:11:40,160 --> 00:11:43,760
so now it is programming language

00:11:42,320 --> 00:11:45,519
agnostic

00:11:43,760 --> 00:11:47,040
if we can dockerize this at that

00:11:45,519 --> 00:11:49,839
application

00:11:47,040 --> 00:11:51,120
so various projects on top of mesos one

00:11:49,839 --> 00:11:54,320
of

00:11:51,120 --> 00:11:57,600
them is marathon which is uh

00:11:54,320 --> 00:12:00,320
la which is which provides

00:11:57,600 --> 00:12:01,360
law which helps in running long-running

00:12:00,320 --> 00:12:06,079
uh

00:12:01,360 --> 00:12:09,040
services scheduling the task

00:12:06,079 --> 00:12:10,880
storage system stateful application it

00:12:09,040 --> 00:12:13,760
also have mattresses

00:12:10,880 --> 00:12:15,040
and check we can easily monitor what's

00:12:13,760 --> 00:12:18,480
happening in our system

00:12:15,040 --> 00:12:21,519
it provides a good ui

00:12:18,480 --> 00:12:23,680
as well for the monitoring kronos for

00:12:21,519 --> 00:12:28,000
scheduling job

00:12:23,680 --> 00:12:30,480
we can run bad jobs with metronome

00:12:28,000 --> 00:12:32,880
and there are other different uh

00:12:30,480 --> 00:12:33,600
projects or frameworks on top of mesos

00:12:32,880 --> 00:12:36,880
that helps

00:12:33,600 --> 00:12:40,240
running the application and give us a

00:12:36,880 --> 00:12:43,120
good load balancer and

00:12:40,240 --> 00:12:45,920
fault tolerance and available

00:12:43,120 --> 00:12:45,920
availability

00:12:47,440 --> 00:12:53,040
let's move ahead with apache hadoop yarn

00:12:50,839 --> 00:12:56,399
here

00:12:53,040 --> 00:12:59,279
so as we all know adobe started and

00:12:56,399 --> 00:13:00,000
started as a very good data processing

00:12:59,279 --> 00:13:04,079
system

00:13:00,000 --> 00:13:07,440
and provides a resource resource manager

00:13:04,079 --> 00:13:11,200
inbuilt in hadoop one

00:13:07,440 --> 00:13:14,240
but the company started using hadoop

00:13:11,200 --> 00:13:17,040
as a state stretches these

00:13:14,240 --> 00:13:19,120
capabilities and they wanted to run

00:13:17,040 --> 00:13:21,519
different frameworks on top of hadoop

00:13:19,120 --> 00:13:22,639
so hadoop two came into picture and they

00:13:21,519 --> 00:13:26,240
divided

00:13:22,639 --> 00:13:28,639
the sdfs and uh

00:13:26,240 --> 00:13:30,639
adobe on for the cluster manager sdfs

00:13:28,639 --> 00:13:33,839
for the distributed storage

00:13:30,639 --> 00:13:35,920
and we can run more than uh map reduce

00:13:33,839 --> 00:13:38,399
other frameworks like spark also in top

00:13:35,920 --> 00:13:38,399
of yarn

00:13:41,680 --> 00:13:45,279
so in apache hadoop

00:13:45,360 --> 00:13:50,160
we have main components resource manager

00:13:47,680 --> 00:13:52,160
and resource manager node manager and

00:13:50,160 --> 00:13:54,320
application master

00:13:52,160 --> 00:13:55,440
here application master will be actually

00:13:54,320 --> 00:13:58,240
running the application

00:13:55,440 --> 00:14:00,079
and talks to resource manager and known

00:13:58,240 --> 00:14:02,320
advisor directly

00:14:00,079 --> 00:14:04,320
in give the snapshot of the world of

00:14:02,320 --> 00:14:06,639
application running

00:14:04,320 --> 00:14:09,600
it also have failure recovery system

00:14:06,639 --> 00:14:09,600
using zookeeper

00:14:11,839 --> 00:14:17,199
and also the it provide isolation cpu

00:14:14,959 --> 00:14:21,120
memory so

00:14:17,199 --> 00:14:24,800
only the authentic user can use it

00:14:21,120 --> 00:14:24,800
there is use the resources

00:14:25,199 --> 00:14:29,120
node manager basically monitor the

00:14:27,519 --> 00:14:32,880
resources in the node

00:14:29,120 --> 00:14:34,639
and and the application it is running

00:14:32,880 --> 00:14:37,519
and provides the all the logging and

00:14:34,639 --> 00:14:37,519
monitoring system

00:14:39,279 --> 00:14:46,480
so let's see how spock will work on yarn

00:14:44,240 --> 00:14:48,880
so again this is a client mode

00:14:46,480 --> 00:14:52,320
deployment and driver is running

00:14:48,880 --> 00:14:55,680
uh on client jvm

00:14:52,320 --> 00:14:58,480
once uh spark spark

00:14:55,680 --> 00:14:59,600
application is submitted the driver

00:14:58,480 --> 00:15:02,160
schedule the

00:14:59,600 --> 00:15:04,560
task and ask resource managers to

00:15:02,160 --> 00:15:07,440
provide those resources

00:15:04,560 --> 00:15:08,560
containers to run those tasks resource

00:15:07,440 --> 00:15:12,079
managers talks to

00:15:08,560 --> 00:15:13,920
node manager and

00:15:12,079 --> 00:15:16,320
spark application master will be

00:15:13,920 --> 00:15:16,320
triggered

00:15:18,880 --> 00:15:22,320
and this way all the

00:15:25,519 --> 00:15:29,759
and once uh spark application master get

00:15:28,240 --> 00:15:31,360
the resources

00:15:29,759 --> 00:15:33,519
from the resource manager resource

00:15:31,360 --> 00:15:35,600
manager will talk to node manager to

00:15:33,519 --> 00:15:37,199
start the containers for executing the

00:15:35,600 --> 00:15:40,079
task

00:15:37,199 --> 00:15:40,480
parallely and once the executor started

00:15:40,079 --> 00:15:43,120
it

00:15:40,480 --> 00:15:44,160
start communicating to spark scheduler

00:15:43,120 --> 00:15:46,959
and update them

00:15:44,160 --> 00:15:48,240
about the uh application that is running

00:15:46,959 --> 00:15:52,160
what is the status

00:15:48,240 --> 00:15:55,839
success server and so on so

00:15:52,160 --> 00:15:57,040
the note point here is mesos offer the

00:15:55,839 --> 00:16:01,440
resources

00:15:57,040 --> 00:16:03,279
so it is to waste scheduling

00:16:01,440 --> 00:16:05,920
the framework either accept the

00:16:03,279 --> 00:16:07,519
resources or reject the resources so

00:16:05,920 --> 00:16:09,759
there are different filters and

00:16:07,519 --> 00:16:14,959
constraint that mesos put

00:16:09,759 --> 00:16:14,959
so that it got the resources on time

00:16:15,279 --> 00:16:18,480
yan application master asked for the

00:16:17,040 --> 00:16:22,160
containers

00:16:18,480 --> 00:16:24,560
resource manager to run this task

00:16:22,160 --> 00:16:26,000
but also yarn have some powerful

00:16:24,560 --> 00:16:29,360
authentication system

00:16:26,000 --> 00:16:32,399
and you roll constraint as well

00:16:29,360 --> 00:16:35,680
to authorize the user

00:16:32,399 --> 00:16:38,160
kerberos required sdfs one of them

00:16:35,680 --> 00:16:39,120
which is not built-in message but messes

00:16:38,160 --> 00:16:42,480
have its own

00:16:39,120 --> 00:16:42,480
authenticating system

00:16:43,759 --> 00:16:51,759
so this was all about a cluster manager

00:16:48,079 --> 00:16:54,000
and resource manager but can we

00:16:51,759 --> 00:16:55,759
also simplify the dependency management

00:16:54,000 --> 00:16:58,880
and deployment part

00:16:55,759 --> 00:17:01,839
for the application let's think about

00:16:58,880 --> 00:17:01,839
docker containers

00:17:03,279 --> 00:17:06,400
so just a recap it is not a vm it is a

00:17:05,919 --> 00:17:09,199
pro

00:17:06,400 --> 00:17:12,400
isolated process which has its own

00:17:09,199 --> 00:17:12,400
network and file system

00:17:13,919 --> 00:17:18,319
which is fast and quick and unlike the

00:17:16,559 --> 00:17:20,559
vm we can't put it

00:17:18,319 --> 00:17:22,559
put a different os on top of it so it

00:17:20,559 --> 00:17:24,480
runs on a host operating system and we

00:17:22,559 --> 00:17:26,959
can easily see all the processes

00:17:24,480 --> 00:17:29,760
all the container running using ps

00:17:26,959 --> 00:17:31,520
command in the host machine itself

00:17:29,760 --> 00:17:33,919
but they are all isolated because it

00:17:31,520 --> 00:17:37,120
provides a layer on top of it if you are

00:17:33,919 --> 00:17:40,160
using a file system in a container

00:17:37,120 --> 00:17:42,970
it creates a new layer that is only

00:17:40,160 --> 00:17:44,799
accessible to the specific container

00:17:42,970 --> 00:17:47,919
[Applause]

00:17:44,799 --> 00:17:50,880
so when we talk about containers

00:17:47,919 --> 00:17:53,039
the kubernetes come into picture to

00:17:50,880 --> 00:17:57,760
manage all the micro services

00:17:53,039 --> 00:17:57,760
and running on the containers

00:17:59,200 --> 00:18:04,559
this simplify our job for the deployment

00:18:02,320 --> 00:18:05,679
scaling the services and managing the

00:18:04,559 --> 00:18:09,679
dependency

00:18:05,679 --> 00:18:12,559
or all the variables or configuration

00:18:09,679 --> 00:18:15,360
in the containers it comes from real

00:18:12,559 --> 00:18:18,640
years of experience and

00:18:15,360 --> 00:18:18,640
from the googlebot

00:18:20,400 --> 00:18:29,840
okay so let's say what are the main

00:18:22,720 --> 00:18:29,840
features in kubernetes

00:18:31,440 --> 00:18:34,480
running the application

00:18:34,640 --> 00:18:38,160
so on top of docker containers it

00:18:37,280 --> 00:18:41,360
provides

00:18:38,160 --> 00:18:44,640
a abstract

00:18:41,360 --> 00:18:47,760
layer which is called pod so

00:18:44,640 --> 00:18:48,480
the pod basically running the uh pod is

00:18:47,760 --> 00:18:51,120
basically on

00:18:48,480 --> 00:18:52,559
uh inside the part we will be running

00:18:51,120 --> 00:18:54,720
docker containers

00:18:52,559 --> 00:18:56,559
and we want to replicate the instance of

00:18:54,720 --> 00:19:00,000
the application we use replication

00:18:56,559 --> 00:19:00,000
controller or replica set

00:19:00,160 --> 00:19:03,280
and there is name space if you want to

00:19:02,559 --> 00:19:06,400
logically

00:19:03,280 --> 00:19:06,400
divide the clusters

00:19:07,919 --> 00:19:11,600
various discovery and communicate

00:19:10,240 --> 00:19:13,919
communication between the

00:19:11,600 --> 00:19:17,120
parts that provides service disk

00:19:13,919 --> 00:19:19,280
discovery and load balancer in build

00:19:17,120 --> 00:19:21,600
we we can have file system and disk

00:19:19,280 --> 00:19:24,799
storage

00:19:21,600 --> 00:19:28,240
so different ways to persist the volume

00:19:24,799 --> 00:19:32,400
and claiming the volume in the

00:19:28,240 --> 00:19:32,400
locally or any cloud provider as well

00:19:33,039 --> 00:19:36,880
it simplifies the configuration of the

00:19:34,720 --> 00:19:41,520
application

00:19:36,880 --> 00:19:41,520
using config map uh secrets

00:19:44,320 --> 00:19:47,600
we can deploy stateful services

00:19:46,640 --> 00:19:50,960
application

00:19:47,600 --> 00:19:52,880
and we can easily roll it rolling out

00:19:50,960 --> 00:19:54,640
the new really a new version of the

00:19:52,880 --> 00:19:58,640
application

00:19:54,640 --> 00:20:01,360
or roll back them if there is any favor

00:19:58,640 --> 00:20:01,840
we can also get the metadata about the

00:20:01,360 --> 00:20:04,159
pod

00:20:01,840 --> 00:20:06,240
as well so what are the resources uh

00:20:04,159 --> 00:20:10,400
what are the memory and cpu

00:20:06,240 --> 00:20:10,400
the node have or the pod is running

00:20:12,159 --> 00:20:16,880
so along with these features kubernetes

00:20:14,720 --> 00:20:16,880
is

00:20:17,520 --> 00:20:21,120
have other resources as well so if

00:20:20,480 --> 00:20:23,600
anyone

00:20:21,120 --> 00:20:24,240
is solving the similar problem that you

00:20:23,600 --> 00:20:26,400
want to

00:20:24,240 --> 00:20:28,240
try to solve you can easily find it out

00:20:26,400 --> 00:20:30,080
in

00:20:28,240 --> 00:20:32,080
lmsr which is just like a package

00:20:30,080 --> 00:20:35,760
manager for kubernetes

00:20:32,080 --> 00:20:37,440
which give us the kubernetes operators

00:20:35,760 --> 00:20:39,760
so let's say if you don't like

00:20:37,440 --> 00:20:42,480
kubernetes inbuilt load balancer

00:20:39,760 --> 00:20:43,360
you can build a new load balancing on

00:20:42,480 --> 00:20:46,640
top of

00:20:43,360 --> 00:20:50,000
ingress with

00:20:46,640 --> 00:20:53,280
bits like if you want to engine x um

00:20:50,000 --> 00:20:56,240
load balancer you can use uh nginx

00:20:53,280 --> 00:20:56,960
ingress operator here and different

00:20:56,240 --> 00:21:00,080
service

00:20:56,960 --> 00:21:04,159
um mesh operators like invoiced you

00:21:00,080 --> 00:21:07,200
also a label that you can use directly

00:21:04,159 --> 00:21:08,880
for the application in the system qflow

00:21:07,200 --> 00:21:11,760
is another

00:21:08,880 --> 00:21:13,679
good platform where we can run any

00:21:11,760 --> 00:21:14,960
machine learning

00:21:13,679 --> 00:21:18,000
algorithm or machine learning

00:21:14,960 --> 00:21:20,159
application prometheus grafana provides

00:21:18,000 --> 00:21:23,039
monitoring

00:21:20,159 --> 00:21:23,039
and logging system

00:21:25,120 --> 00:21:30,240
i will be more talking about kiloflow i

00:21:27,840 --> 00:21:32,960
already talked about qflo in my previous

00:21:30,240 --> 00:21:34,240
talk in machine learning track running

00:21:32,960 --> 00:21:35,840
machine learning algorithms

00:21:34,240 --> 00:21:37,840
reaching machine learning tools

00:21:35,840 --> 00:21:39,440
available in apache ecosystem

00:21:37,840 --> 00:21:40,880
that will more focus on running the

00:21:39,440 --> 00:21:45,600
application rather than

00:21:40,880 --> 00:21:48,400
running the machine learning application

00:21:45,600 --> 00:21:49,039
kubernetes a single platform where

00:21:48,400 --> 00:21:51,679
developer

00:21:49,039 --> 00:21:52,080
system that helps developer system admin

00:21:51,679 --> 00:21:54,960
of

00:21:52,080 --> 00:21:54,960
operation teams

00:21:56,159 --> 00:21:59,520
so developers can easily test it their

00:21:58,799 --> 00:22:01,600
application

00:21:59,520 --> 00:22:02,640
system admin can easily configure their

00:22:01,600 --> 00:22:06,000
infrastructure and

00:22:02,640 --> 00:22:08,159
operation team can easily monitor or

00:22:06,000 --> 00:22:09,440
set up the alert manage manager

00:22:08,159 --> 00:22:12,960
management tool

00:22:09,440 --> 00:22:17,360
on top of the kubernetes platform

00:22:12,960 --> 00:22:17,360
the platform that kubernetes give us

00:22:18,000 --> 00:22:24,840
this is one of the feature that i like

00:22:21,360 --> 00:22:27,679
so i just want to show so if we want to

00:22:24,840 --> 00:22:31,039
rolling running out the new version

00:22:27,679 --> 00:22:35,600
spark app version two

00:22:31,039 --> 00:22:40,640
we can just set the new image in our

00:22:35,600 --> 00:22:44,880
replica set or deployment resources

00:22:40,640 --> 00:22:47,600
and as per the rolling update strategy

00:22:44,880 --> 00:22:47,600
started the

00:22:48,640 --> 00:22:55,440
new it started to replicate the new

00:22:52,000 --> 00:22:58,159
image version to template

00:22:55,440 --> 00:23:00,000
we can we can pause it as well so that

00:22:58,159 --> 00:23:01,039
we can check if there is any error

00:23:00,000 --> 00:23:04,640
coming for

00:23:01,039 --> 00:23:06,480
that updated pod once everything is

00:23:04,640 --> 00:23:09,760
looking fine we can resume the

00:23:06,480 --> 00:23:11,520
rollout here but we just we kubernetes

00:23:09,760 --> 00:23:13,280
we want kubernetes to do all those

00:23:11,520 --> 00:23:14,320
things we don't we just say to

00:23:13,280 --> 00:23:18,000
kubernetes and

00:23:14,320 --> 00:23:20,960
it should be capable of doing things

00:23:18,000 --> 00:23:21,919
so it provides various probe readiness

00:23:20,960 --> 00:23:24,960
leveling is

00:23:21,919 --> 00:23:27,840
top and it automatically block rolling

00:23:24,960 --> 00:23:30,960
out if there is any failure

00:23:27,840 --> 00:23:32,880
according to the policy we policy or

00:23:30,960 --> 00:23:36,720
strategy type we

00:23:32,880 --> 00:23:39,679
put in our configuration

00:23:36,720 --> 00:23:40,080
so it makes sure that that there is only

00:23:39,679 --> 00:23:43,039
one

00:23:40,080 --> 00:23:44,240
uh rolling out is happening and there is

00:23:43,039 --> 00:23:47,200
no

00:23:44,240 --> 00:23:47,200
downtime here

00:23:48,080 --> 00:23:54,320
so it will check for uh every second if

00:23:51,279 --> 00:23:57,440
if uh we can use that pod or not

00:23:54,320 --> 00:23:58,320
once once it is available for serving

00:23:57,440 --> 00:24:01,919
the traffic

00:23:58,320 --> 00:24:05,200
it roll out the next uh

00:24:01,919 --> 00:24:06,080
next replica of the next instance of the

00:24:05,200 --> 00:24:09,600
instance

00:24:06,080 --> 00:24:11,440
app and so on

00:24:09,600 --> 00:24:13,200
and if there is any failure it will

00:24:11,440 --> 00:24:18,400
block the role

00:24:13,200 --> 00:24:20,960
or rolling the new release

00:24:18,400 --> 00:24:23,679
so let's see how we can run smart on

00:24:20,960 --> 00:24:23,679
kubernetes

00:24:24,240 --> 00:24:30,480
so these are the uh main components in

00:24:27,840 --> 00:24:32,000
spark spark driver executor master and

00:24:30,480 --> 00:24:35,039
worker nodes

00:24:32,000 --> 00:24:37,440
we either run the executor in a

00:24:35,039 --> 00:24:40,080
kubernetes cluster or we can run driver

00:24:37,440 --> 00:24:42,480
and executor in kubernetes cluster or

00:24:40,080 --> 00:24:44,480
we can run client as well in kubernetes

00:24:42,480 --> 00:24:47,120
cluster so i think the best way is to

00:24:44,480 --> 00:24:47,919
isolate every component and run them in

00:24:47,120 --> 00:24:50,480
kubernetes

00:24:47,919 --> 00:24:50,480
cluster

00:24:50,570 --> 00:24:53,869
[Music]

00:24:57,679 --> 00:25:04,320
okay so let's once in

00:25:00,960 --> 00:25:07,600
let's take one scenario why kubernetes

00:25:04,320 --> 00:25:09,360
such a powerful reform for cluster

00:25:07,600 --> 00:25:12,000
manager and resource manager

00:25:09,360 --> 00:25:12,000
point of view

00:25:13,840 --> 00:25:20,159
let's say we have a number of nodes

00:25:17,120 --> 00:25:24,159
and we want to run

00:25:20,159 --> 00:25:27,120
a number of executors in each node

00:25:24,159 --> 00:25:29,600
and we have a distributed data storage

00:25:27,120 --> 00:25:32,799
and we want to

00:25:29,600 --> 00:25:33,279
have each executor to talk to a data

00:25:32,799 --> 00:25:36,159
node

00:25:33,279 --> 00:25:37,279
here i'm talking about in terms of sdfs

00:25:36,159 --> 00:25:39,360
hdfs is

00:25:37,279 --> 00:25:41,200
to mean component name node and data

00:25:39,360 --> 00:25:45,679
node data node is basically where

00:25:41,200 --> 00:25:47,919
actual data is present in the node

00:25:45,679 --> 00:25:49,120
so we we want to make sure that if

00:25:47,919 --> 00:25:52,159
executor one talks

00:25:49,120 --> 00:25:56,240
one two talks to uh the storage system

00:25:52,159 --> 00:25:58,960
it should talk to data node one here

00:25:56,240 --> 00:25:59,679
and if exeter 2 wants to talk to storage

00:25:58,960 --> 00:26:04,159
system then

00:25:59,679 --> 00:26:07,279
it should talk to uh data node 2 here

00:26:04,159 --> 00:26:11,360
so that we can uh optimize we

00:26:07,279 --> 00:26:15,679
the network latency will be reduced

00:26:11,360 --> 00:26:18,720
because if executor one want to talks to

00:26:15,679 --> 00:26:21,919
outside of the its node then

00:26:18,720 --> 00:26:24,320
uh that that will

00:26:21,919 --> 00:26:26,159
that will be costly and expensive

00:26:24,320 --> 00:26:29,279
operation

00:26:26,159 --> 00:26:31,279
so kubernetes provides different levels

00:26:29,279 --> 00:26:33,120
like we can label out the port we can

00:26:31,279 --> 00:26:36,320
label out the node

00:26:33,120 --> 00:26:38,559
we can have a number of naming spaces

00:26:36,320 --> 00:26:39,679
as per our requirement like if we want

00:26:38,559 --> 00:26:42,159
to run

00:26:39,679 --> 00:26:43,840
spark application one we can create a

00:26:42,159 --> 00:26:45,120
separate name space for it which is

00:26:43,840 --> 00:26:47,440
isolated from other name

00:26:45,120 --> 00:26:49,919
spaces and parts running in the same

00:26:47,440 --> 00:26:53,200
cluster

00:26:49,919 --> 00:26:53,679
so here we want in each node we want to

00:26:53,200 --> 00:26:57,679
run

00:26:53,679 --> 00:26:59,919
one data node so that means in each node

00:26:57,679 --> 00:27:01,440
one data node should be always always

00:26:59,919 --> 00:27:04,159
available

00:27:01,440 --> 00:27:06,559
even though if there is fellow community

00:27:04,159 --> 00:27:08,880
should restart a new node in that

00:27:06,559 --> 00:27:08,880
node

00:27:10,240 --> 00:27:14,480
new pod in that node so demon set

00:27:15,440 --> 00:27:18,880
helps here which makes sure that there

00:27:17,919 --> 00:27:22,399
is

00:27:18,880 --> 00:27:26,240
at least one instance running in the uh

00:27:22,399 --> 00:27:26,240
each node in the namespace

00:27:28,880 --> 00:27:36,240
so we can level out our

00:27:32,159 --> 00:27:40,320
kubernetes node and we

00:27:36,240 --> 00:27:42,960
make sure that label

00:27:40,320 --> 00:27:43,919
uh present in our pods as well so when

00:27:42,960 --> 00:27:46,000
we deployment

00:27:43,919 --> 00:27:47,039
we do deployment the node selector will

00:27:46,000 --> 00:27:49,120
pick that

00:27:47,039 --> 00:27:52,080
label and execute it in the

00:27:49,120 --> 00:27:54,960
corresponding node

00:27:52,080 --> 00:27:55,440
so in this way if uh executed know about

00:27:54,960 --> 00:27:58,640
the

00:27:55,440 --> 00:28:00,320
uh node ip and it talks to the data node

00:27:58,640 --> 00:28:04,960
which is present in

00:28:00,320 --> 00:28:04,960
its kubernetes node

00:28:06,320 --> 00:28:13,279
spark on kubernetes so

00:28:09,919 --> 00:28:15,360
we have seen how complex it could be

00:28:13,279 --> 00:28:16,559
to do everything from scratch in

00:28:15,360 --> 00:28:18,640
kubernetes weird

00:28:16,559 --> 00:28:19,760
we have to write so much configuration

00:28:18,640 --> 00:28:22,559
yaml file

00:28:19,760 --> 00:28:23,520
for it but if every other people have

00:28:22,559 --> 00:28:27,039
already done it

00:28:23,520 --> 00:28:30,000
and published in helmchart package

00:28:27,039 --> 00:28:32,000
manager a kind of package manager then

00:28:30,000 --> 00:28:34,640
we can easily use it on top of

00:28:32,000 --> 00:28:35,279
it we can do some modification as well

00:28:34,640 --> 00:28:37,520
so we

00:28:35,279 --> 00:28:39,200
it is better to use spark on kubernetes

00:28:37,520 --> 00:28:42,240
operator here

00:28:39,200 --> 00:28:45,760
so we will see the operator to i

00:28:42,240 --> 00:28:47,760
need spark cluster and magically it

00:28:45,760 --> 00:28:51,200
creates a sparkle stun gives to you

00:28:47,760 --> 00:28:52,880
and you can run your application as per

00:28:51,200 --> 00:28:54,399
the your configuration you provided to

00:28:52,880 --> 00:28:57,360
the operator

00:28:54,399 --> 00:29:00,799
a different cloud service cloud platform

00:28:57,360 --> 00:29:04,960
also provide kubernetes as a service and

00:29:00,799 --> 00:29:04,960
spark are already installed into it

00:29:07,120 --> 00:29:12,640
so can we use our kubernetes and

00:29:10,000 --> 00:29:15,760
messages together

00:29:12,640 --> 00:29:18,159
messes provide distributed data center

00:29:15,760 --> 00:29:21,120
distributed operating data center and

00:29:18,159 --> 00:29:21,120
operating systems

00:29:22,159 --> 00:29:25,440
and kubernetes can be used as a cluster

00:29:24,240 --> 00:29:27,440
manager here so

00:29:25,440 --> 00:29:29,440
all kind of dependency on configuration

00:29:27,440 --> 00:29:29,919
management and deployment part will be

00:29:29,440 --> 00:29:33,279
handled

00:29:29,919 --> 00:29:36,480
by kubernetes and messers will be

00:29:33,279 --> 00:29:39,600
providing the whole data center

00:29:36,480 --> 00:29:40,080
to run the kubernetes and all it

00:29:39,600 --> 00:29:43,279
provides

00:29:40,080 --> 00:29:46,480
the different um cli and ui

00:29:43,279 --> 00:29:48,240
gui uh for monitoring all the tasks

00:29:46,480 --> 00:29:50,000
it already support different packages

00:29:48,240 --> 00:29:53,679
like spark after cassandra

00:29:50,000 --> 00:29:56,720
and so on yan

00:29:53,679 --> 00:29:59,120
and messes so yarn mostly

00:29:56,720 --> 00:30:00,799
can be used as a resource manager and

00:29:59,120 --> 00:30:02,159
measures again as a disability data

00:30:00,799 --> 00:30:04,799
center

00:30:02,159 --> 00:30:05,440
so once any application is registered to

00:30:04,799 --> 00:30:10,399
mesos

00:30:05,440 --> 00:30:13,279
it talks to yan resource manager

00:30:10,399 --> 00:30:14,240
for providing the resources for offering

00:30:13,279 --> 00:30:18,480
the resources

00:30:14,240 --> 00:30:21,600
to run the application once

00:30:18,480 --> 00:30:22,000
the metadata about the task came to a

00:30:21,600 --> 00:30:24,399
message

00:30:22,000 --> 00:30:27,200
master it triggered the message slave

00:30:24,399 --> 00:30:29,200
for the execution of this task

00:30:27,200 --> 00:30:30,399
there is already one produce project

00:30:29,200 --> 00:30:34,240
going on

00:30:30,399 --> 00:30:37,840
watching it which has uh the scheduling

00:30:34,240 --> 00:30:37,840
and executed part

00:30:38,799 --> 00:30:43,120
so if you again think if you can't

00:30:41,120 --> 00:30:45,200
isolate your micro services that

00:30:43,120 --> 00:30:46,880
you may need to understand the designing

00:30:45,200 --> 00:30:48,640
and designing of your software and

00:30:46,880 --> 00:30:51,039
microservice

00:30:48,640 --> 00:30:51,760
it should be loosely coupled so that it

00:30:51,039 --> 00:30:54,399
can be

00:30:51,760 --> 00:30:55,600
easily dockerized and all the

00:30:54,399 --> 00:30:59,120
dependencies are

00:30:55,600 --> 00:31:03,039
independent so

00:30:59,120 --> 00:31:06,640
when i see uh designing the software i

00:31:03,039 --> 00:31:10,240
also think how we do normalization in

00:31:06,640 --> 00:31:11,679
in uh database system like one nf or if

00:31:10,240 --> 00:31:13,679
we don't

00:31:11,679 --> 00:31:17,039
we should have we should have one

00:31:13,679 --> 00:31:19,679
feature in each micro services

00:31:17,039 --> 00:31:21,360
two and f we don't want any dependency

00:31:19,679 --> 00:31:24,880
inside the

00:31:21,360 --> 00:31:28,559
one table right also

00:31:24,880 --> 00:31:28,559
in three nf we see how uh

00:31:28,720 --> 00:31:32,720
functional functional dependency could

00:31:30,720 --> 00:31:35,200
be in same micro services

00:31:32,720 --> 00:31:36,960
those kind of logic or concept can be

00:31:35,200 --> 00:31:38,799
applied here as well and we should use

00:31:36,960 --> 00:31:40,640
different design patterns for it like if

00:31:38,799 --> 00:31:44,080
we have two

00:31:40,640 --> 00:31:46,720
incompatible interfaces and we want to

00:31:44,080 --> 00:31:47,200
build a bridge we need a micro service

00:31:46,720 --> 00:31:50,480
that

00:31:47,200 --> 00:31:52,480
uses adapter design pattern and there

00:31:50,480 --> 00:31:55,200
the other design pattern can be used

00:31:52,480 --> 00:31:57,919
like strategy design pattern so

00:31:55,200 --> 00:31:59,039
it change the behavior as per the

00:31:57,919 --> 00:32:01,120
requirement

00:31:59,039 --> 00:32:03,279
and solid principles we all know about

00:32:01,120 --> 00:32:03,279
it

00:32:04,159 --> 00:32:09,279
and how we can uh disturb how we can

00:32:07,600 --> 00:32:11,760
effectively manage the distributed

00:32:09,279 --> 00:32:15,840
transaction as well

00:32:11,760 --> 00:32:15,840
using a saga pattern

00:32:16,880 --> 00:32:21,360
but it's not always simple we could have

00:32:19,440 --> 00:32:21,840
different upstream and downstream system

00:32:21,360 --> 00:32:24,720
so that

00:32:21,840 --> 00:32:26,000
all should be uh could should be

00:32:24,720 --> 00:32:27,919
containerized

00:32:26,000 --> 00:32:30,080
or will be running in different cluster

00:32:27,919 --> 00:32:31,760
managers so here in this

00:32:30,080 --> 00:32:34,000
case spark application is running

00:32:31,760 --> 00:32:36,320
kubernetes cluster and

00:32:34,000 --> 00:32:37,600
uh apache ignite which is distributed in

00:32:36,320 --> 00:32:40,640
memory storage

00:32:37,600 --> 00:32:43,039
also running in uh kubernetes cluster

00:32:40,640 --> 00:32:44,720
it it boosts the actual acceleration

00:32:43,039 --> 00:32:49,039
accelerate the spark

00:32:44,720 --> 00:32:49,039
capabilities of data processing

00:32:53,919 --> 00:32:57,360
okay so other distributed frameworks can

00:32:57,039 --> 00:33:02,880
be

00:32:57,360 --> 00:33:06,320
also used in this cluster managers

00:33:02,880 --> 00:33:08,480
so we have to see like mesos and yarn

00:33:06,320 --> 00:33:09,440
in in terms of mesos and yarn we should

00:33:08,480 --> 00:33:12,000
have a

00:33:09,440 --> 00:33:13,919
scheduler in the framework in in terms

00:33:12,000 --> 00:33:16,080
of kubernetes we

00:33:13,919 --> 00:33:17,440
just have to containerize it and

00:33:16,080 --> 00:33:20,960
kubernetes will do

00:33:17,440 --> 00:33:20,960
all this stuff for us

00:33:21,840 --> 00:33:26,159
so some main points here if you already

00:33:24,399 --> 00:33:28,320
have any cluster

00:33:26,159 --> 00:33:29,600
manager set up like yarn messes or

00:33:28,320 --> 00:33:32,640
kubernetes you don't

00:33:29,600 --> 00:33:35,360
have to actually go from

00:33:32,640 --> 00:33:36,799
scratch and tune the all the hyper

00:33:35,360 --> 00:33:40,720
parameters

00:33:36,799 --> 00:33:40,720
for the for the optimization

00:33:41,360 --> 00:33:45,440
if you don't have a new requirement

00:33:47,200 --> 00:33:51,120
if you are starting a micro service or

00:33:49,039 --> 00:33:53,440
project from scratch then you definitely

00:33:51,120 --> 00:33:55,120
have to listed on all the requirements

00:33:53,440 --> 00:33:57,039
what are the dependency management

00:33:55,120 --> 00:34:00,559
management you need

00:33:57,039 --> 00:34:00,559
what are the configuration you need

00:34:01,279 --> 00:34:07,760
and use it on this customizer

00:34:05,600 --> 00:34:09,520
if you don't have if you have legacy

00:34:07,760 --> 00:34:12,560
system let's say they are not

00:34:09,520 --> 00:34:16,960
containerized and you you actually not

00:34:12,560 --> 00:34:16,960
do containerize uh straight away

00:34:17,119 --> 00:34:21,760
so you might be using jan or message on

00:34:19,520 --> 00:34:24,079
it and once it is containerized

00:34:21,760 --> 00:34:26,240
you can actually use continue using

00:34:24,079 --> 00:34:30,480
mesos because missiles use

00:34:26,240 --> 00:34:30,480
both type of systems

00:34:31,760 --> 00:34:35,599
also if we set up new cluster manager we

00:34:34,079 --> 00:34:38,800
have to

00:34:35,599 --> 00:34:42,720
have a skillful school skillful

00:34:38,800 --> 00:34:46,639
people that can do the monitoring

00:34:42,720 --> 00:34:46,639
and the development and the admin part

00:34:47,280 --> 00:34:51,040
so that's all i had i took different

00:34:50,240 --> 00:34:54,240
reference

00:34:51,040 --> 00:34:57,520
the various research papers i was books

00:34:54,240 --> 00:34:59,040
and the official websites

00:34:57,520 --> 00:35:00,720
thank you everyone for joining me in

00:34:59,040 --> 00:35:03,200
this talk and listening i hope you

00:35:00,720 --> 00:35:05,839
learned something new here

00:35:03,200 --> 00:35:06,480
i hope i will definitely collect all the

00:35:05,839 --> 00:35:09,520
questions

00:35:06,480 --> 00:35:12,000
i will also label in apache

00:35:09,520 --> 00:35:12,720
apple booth you can definitely check out

00:35:12,000 --> 00:35:15,520
what are the

00:35:12,720 --> 00:35:16,800
jaw openings and if you have any

00:35:15,520 --> 00:35:19,760
question feel free to

00:35:16,800 --> 00:35:21,119
bring me in this like chat for this

00:35:19,760 --> 00:35:23,359
track

00:35:21,119 --> 00:35:25,119
in big data i will be also available in

00:35:23,359 --> 00:35:28,839
machine learning try because i have

00:35:25,119 --> 00:35:31,839
one talk on running machine learning

00:35:28,839 --> 00:35:31,839
algorithms

00:35:33,040 --> 00:35:36,560
you can always find me in twitter

00:35:35,119 --> 00:35:38,880
linkedin or github

00:35:36,560 --> 00:35:40,240
you can see my website for more

00:35:38,880 --> 00:35:42,000
information

00:35:40,240 --> 00:35:43,440
thank you thank you so much apache khan

00:35:42,000 --> 00:35:59,839
and apple for

00:35:43,440 --> 00:35:59,839
sponsoring this conference

00:38:41,040 --> 00:38:43,119

YouTube URL: https://www.youtube.com/watch?v=_cOc3yZbnu8


