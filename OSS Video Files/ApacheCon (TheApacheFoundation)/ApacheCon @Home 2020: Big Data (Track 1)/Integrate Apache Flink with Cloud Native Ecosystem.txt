Title: Integrate Apache Flink with Cloud Native Ecosystem
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Integrate Apache Flink with Cloud Native Ecosystem
Yang Wang, Tao Yang

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

With the vigorous development of cloud-native and serverless computing, there are more and more big-data workloads, especially Apache Flink workloads, running in Alibaba cloud for better deployment and management, backed by Kubernetes. This presentation introduces the experiences of intergrating Flink with cloud-native ecosystem, including the improvements in Flink to support elasticity and natively running on Kubernetes, the experiences about managing dependent components like ZooKeeper, HDFS etc. and leveraging Kubernetes service/network/storage extensions, better supports for big-data workloads to satisfy requirements on multi-tanent management, resource fairness, resource elasticity etc. and achieve high scheduling performance via Apache YuniKorn.

Yang Wang:
Technical Expert of Alibaba's realtime computing team, Apache Flink Contributor, focusing on the direction of resource scheduling in Flink.
Tao Yang:
Technical Expert of Alibaba's realtime computing team, Apache Hadoop Committer, Apache YuniKorn Committer, focusing on the direction of resource scheduling in YARN and Kubernetes.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:28,160 --> 00:00:32,960
hi everyone

00:00:29,119 --> 00:00:36,320
thanks for joining this session today

00:00:32,960 --> 00:00:38,559
me and my colleague we are going to talk

00:00:36,320 --> 00:00:40,960
about how to integrate things with

00:00:38,559 --> 00:00:44,079
cognitive ecosystem

00:00:40,960 --> 00:00:45,360
a quick introduction to myself my name

00:00:44,079 --> 00:00:48,800
is

00:00:45,360 --> 00:00:50,800
i come from alibaba and i have more than

00:00:48,800 --> 00:00:52,559
five years of experience and creator at

00:00:50,800 --> 00:00:57,120
this field system

00:00:52,559 --> 00:01:00,160
now recently i'm focusing on

00:00:57,120 --> 00:01:02,879
communities and operations in production

00:01:00,160 --> 00:01:02,879
and life skill

00:01:04,400 --> 00:01:09,840
this is today's antenna first i will

00:01:07,760 --> 00:01:10,960
quickly go through the introduction of

00:01:09,840 --> 00:01:14,000
quantities

00:01:10,960 --> 00:01:16,000
second i will tell you why alibaba is

00:01:14,000 --> 00:01:17,439
trying to migrate the thing workflows

00:01:16,000 --> 00:01:21,200
from part of vr

00:01:17,439 --> 00:01:24,560
to quantities and how to do that next

00:01:21,200 --> 00:01:27,840
i will share some challenges when we

00:01:24,560 --> 00:01:30,079
deploy free on quantities in production

00:01:27,840 --> 00:01:31,520
finally my colleague will give some

00:01:30,079 --> 00:01:36,400
insight about

00:01:31,520 --> 00:01:36,400
scaling and follow the file conclusion

00:01:36,479 --> 00:01:41,840
okay let's come to the first part

00:01:41,920 --> 00:01:47,119
so what is kubernetes this is a very

00:01:44,960 --> 00:01:49,360
simple definition i covered from the

00:01:47,119 --> 00:01:52,079
official website

00:01:49,360 --> 00:01:52,479
let's check out some pinpoint here the

00:01:52,079 --> 00:01:55,759
first

00:01:52,479 --> 00:01:56,880
is resource management kubernetes is a

00:01:55,759 --> 00:02:00,000
resource management

00:01:56,880 --> 00:02:03,200
framework which could help us to manage

00:02:00,000 --> 00:02:05,520
thousands of machines

00:02:03,200 --> 00:02:07,600
then you can run your distributed jobs

00:02:05,520 --> 00:02:10,879
on the queen's cluster

00:02:07,600 --> 00:02:13,440
the second is contender orchestration

00:02:10,879 --> 00:02:16,640
all the workloads on our jobs running on

00:02:13,440 --> 00:02:19,680
quantities is containerized

00:02:16,640 --> 00:02:22,800
the third is operation automatic

00:02:19,680 --> 00:02:26,000
automation kubernetes provide

00:02:22,800 --> 00:02:28,560
a simple declarative api

00:02:26,000 --> 00:02:29,280
then you could just write a simple yaml

00:02:28,560 --> 00:02:33,120
file

00:02:29,280 --> 00:02:36,640
then apply it your your your coordinates

00:02:33,120 --> 00:02:37,440
application will be launched uh all's

00:02:36,640 --> 00:02:39,599
operation

00:02:37,440 --> 00:02:41,120
uh all the uh creating resource and

00:02:39,599 --> 00:02:42,879
other operations will be

00:02:41,120 --> 00:02:44,720
uh will be done by kubernetes

00:02:42,879 --> 00:02:48,000
automatically

00:02:44,720 --> 00:02:51,440
uh number four is uh cloud native

00:02:48,000 --> 00:02:51,920
uh also all the cloud vendors could

00:02:51,440 --> 00:02:55,280
provide

00:02:51,920 --> 00:02:58,480
outpost out of box uh quincy cluster

00:02:55,280 --> 00:03:01,040
then it is very easy to set up a queen's

00:02:58,480 --> 00:03:01,040
cluster

00:03:02,319 --> 00:03:06,879
so what kind of our clothes we could run

00:03:04,560 --> 00:03:09,760
on kubernetes

00:03:06,879 --> 00:03:10,800
uh in the very beginning uh we are

00:03:09,760 --> 00:03:14,400
running a

00:03:10,800 --> 00:03:16,840
website or the mobile banking service

00:03:14,400 --> 00:03:18,319
on the kubernetes they are all still

00:03:16,840 --> 00:03:20,879
researchers

00:03:18,319 --> 00:03:22,800
when the poor failed we just need to

00:03:20,879 --> 00:03:25,440
launch them again

00:03:22,800 --> 00:03:26,560
not after too long deep learning is

00:03:25,440 --> 00:03:29,760
become

00:03:26,560 --> 00:03:31,120
become very popular and the engineers

00:03:29,760 --> 00:03:33,840
start to running

00:03:31,120 --> 00:03:35,599
differently overall extension flow on

00:03:33,840 --> 00:03:37,599
the quantities

00:03:35,599 --> 00:03:39,599
at the same time some companies are

00:03:37,599 --> 00:03:43,120
trying to deploy the storage

00:03:39,599 --> 00:03:46,480
uh on the queries um for example

00:03:43,120 --> 00:03:50,319
the my circle or database uh

00:03:46,480 --> 00:03:54,000
or on the message queue on kafka

00:03:50,319 --> 00:03:55,120
under the elasticsearch um the big data

00:03:54,000 --> 00:03:57,840
seems to be the

00:03:55,120 --> 00:04:00,080
last one to join the family uh uh

00:03:57,840 --> 00:04:03,360
include include uh including

00:04:00,080 --> 00:04:05,360
spark and flink however i think it is a

00:04:03,360 --> 00:04:10,080
very important signal

00:04:05,360 --> 00:04:10,080
because we always believe that

00:04:10,400 --> 00:04:16,799
the big data workflows are more

00:04:12,560 --> 00:04:20,479
appreciated to run on hadoop clusters

00:04:16,799 --> 00:04:24,240
now many companies are starting to

00:04:20,479 --> 00:04:27,360
run regulator workloads on kubernetes

00:04:24,240 --> 00:04:28,800
in alibaba we are just in the same

00:04:27,360 --> 00:04:32,400
situation

00:04:28,800 --> 00:04:35,440
in the very beginning the alibaba

00:04:32,400 --> 00:04:38,479
e-commercial website and the mobile

00:04:35,440 --> 00:04:42,080
backend service has been deployed on

00:04:38,479 --> 00:04:44,479
kubernetes it is our internal furnace

00:04:42,080 --> 00:04:44,479
version

00:04:45,600 --> 00:04:50,160
of the death story team they are trying

00:04:48,880 --> 00:04:53,120
to deploy the

00:04:50,160 --> 00:04:54,639
uh the story series on the communities

00:04:53,120 --> 00:04:58,240
too

00:04:54,639 --> 00:05:01,759
we are from the figure team and we have

00:04:58,240 --> 00:05:02,800
started to uh migrate to the quantities

00:05:01,759 --> 00:05:07,759
from yara

00:05:02,800 --> 00:05:07,759
uh from from last year

00:05:08,840 --> 00:05:13,840
so why uh alba is

00:05:11,600 --> 00:05:18,160
trying to migrate uh flink workloads

00:05:13,840 --> 00:05:18,160
from hot vr to quantities cluster

00:05:19,600 --> 00:05:27,039
first uh let's have a look uh what is a

00:05:23,360 --> 00:05:30,479
current situation or in alibaba

00:05:27,039 --> 00:05:30,479
for the flink onyar

00:05:31,280 --> 00:05:37,120
about two years ago all the free jobs in

00:05:34,240 --> 00:05:39,600
alibaba are running on hadoop

00:05:37,120 --> 00:05:41,680
internally we have more than 10 thousand

00:05:39,600 --> 00:05:42,960
flink applications running on different

00:05:41,680 --> 00:05:45,919
yacht clusters

00:05:42,960 --> 00:05:46,960
with ten thousand nodes uh on the public

00:05:45,919 --> 00:05:49,919
cloud

00:05:46,960 --> 00:05:52,639
we also have more than ten uh more than

00:05:49,919 --> 00:05:55,440
one hundred customers are using their

00:05:52,639 --> 00:05:56,800
real-time computer service we are

00:05:55,440 --> 00:05:59,840
managing a

00:05:56,800 --> 00:06:00,880
shared cluster and provide managed think

00:05:59,840 --> 00:06:04,560
service

00:06:00,880 --> 00:06:07,360
however because of security reasons

00:06:04,560 --> 00:06:08,400
we could not support uh user default

00:06:07,360 --> 00:06:11,440
function

00:06:08,400 --> 00:06:15,759
under the just submission in the

00:06:11,440 --> 00:06:19,520
public cloud for the shared cluster

00:06:15,759 --> 00:06:20,880
also another limit another limitation is

00:06:19,520 --> 00:06:23,600
a user could

00:06:20,880 --> 00:06:24,319
connect cannot connect to the on their

00:06:23,600 --> 00:06:27,360
own

00:06:24,319 --> 00:06:29,280
storage service in their vpc system we

00:06:27,360 --> 00:06:32,160
do not have a

00:06:29,280 --> 00:06:34,080
disk isolation when some jobs flash many

00:06:32,160 --> 00:06:36,639
data to the local disk

00:06:34,080 --> 00:06:37,440
the node manager will choose into

00:06:36,639 --> 00:06:40,479
unhealthy

00:06:37,440 --> 00:06:42,240
and all the draw all the jobs uh

00:06:40,479 --> 00:06:43,520
running all the jobs have a connected

00:06:42,240 --> 00:06:46,800
running on the same

00:06:43,520 --> 00:06:49,840
manager will have to favor i think

00:06:46,800 --> 00:06:53,440
it is uh not reasonable um for

00:06:49,840 --> 00:06:53,759
mountain tennis support uh on the right

00:06:53,440 --> 00:06:56,960
uh

00:06:53,759 --> 00:06:58,800
it is a yama cluster yeah max yama

00:06:56,960 --> 00:07:02,319
cluster is running in the

00:06:58,800 --> 00:07:04,960
user vpc network user is uh

00:07:02,319 --> 00:07:06,240
responsible for managing the clusters

00:07:04,960 --> 00:07:09,039
and we could

00:07:06,240 --> 00:07:09,520
we provide some tools uh for example

00:07:09,039 --> 00:07:12,639
class

00:07:09,520 --> 00:07:17,840
manager to help us to help them

00:07:12,639 --> 00:07:17,840
with managing upgrading

00:07:20,800 --> 00:07:27,440
so why we choose to

00:07:24,240 --> 00:07:31,840
migrate uh from happier to companies

00:07:27,440 --> 00:07:35,520
i see at least we have 14 reasons

00:07:31,840 --> 00:07:38,319
the first one is a container environment

00:07:35,520 --> 00:07:41,039
it is very easy to set up a cleanup and

00:07:38,319 --> 00:07:44,639
reproducible

00:07:41,039 --> 00:07:47,759
image that you should email that

00:07:44,639 --> 00:07:50,080
you you have a

00:07:47,759 --> 00:07:51,120
command if you have a command running in

00:07:50,080 --> 00:07:55,120
the local

00:07:51,120 --> 00:07:59,199
then uh uh then accept a theme

00:07:55,120 --> 00:08:02,720
event will be created in the cluster

00:07:59,199 --> 00:08:06,960
if you if you run the same command

00:08:02,720 --> 00:08:09,919
the num2 is a multiple thinness uh

00:08:06,960 --> 00:08:10,479
docker connector quantities could

00:08:09,919 --> 00:08:13,520
provide

00:08:10,479 --> 00:08:17,199
better resource or network isolation

00:08:13,520 --> 00:08:20,080
also it could provide better security

00:08:17,199 --> 00:08:21,199
benefit from the maintenance and

00:08:20,080 --> 00:08:24,800
isolation

00:08:21,199 --> 00:08:26,240
with running mixed workloads in our same

00:08:24,800 --> 00:08:28,560
company's cluster

00:08:26,240 --> 00:08:29,280
for you know we could run online home

00:08:28,560 --> 00:08:31,759
service

00:08:29,280 --> 00:08:32,959
the machine learning approach the search

00:08:31,759 --> 00:08:36,000
engine

00:08:32,959 --> 00:08:36,959
and there's a big database in a sim

00:08:36,000 --> 00:08:40,000
cluster

00:08:36,959 --> 00:08:43,200
so or we could get a better resource

00:08:40,000 --> 00:08:43,919
utilization the last one is we could

00:08:43,200 --> 00:08:46,720
leverage

00:08:43,919 --> 00:08:48,399
the rich communities involving the

00:08:46,720 --> 00:08:51,120
ecosystem

00:08:48,399 --> 00:08:51,440
most of most of them are open source and

00:08:51,120 --> 00:08:55,680
we

00:08:51,440 --> 00:08:58,560
could use them directory

00:08:55,680 --> 00:08:58,560
um so

00:08:58,800 --> 00:09:05,120
this is how uh we are this is

00:09:02,399 --> 00:09:06,240
our new uh architecture for free on

00:09:05,120 --> 00:09:09,680
quantities

00:09:06,240 --> 00:09:10,720
and it is very similar uh to uh the anya

00:09:09,680 --> 00:09:13,839
applicator

00:09:10,720 --> 00:09:17,519
uh we could provide a service

00:09:13,839 --> 00:09:20,720
link and self-managed cluster

00:09:17,519 --> 00:09:24,560
the the the improvement is uh

00:09:20,720 --> 00:09:28,000
now we could provide we will support

00:09:24,560 --> 00:09:30,320
a user uef and

00:09:28,000 --> 00:09:33,200
just submission in the public account

00:09:30,320 --> 00:09:36,640
benefit from the

00:09:33,200 --> 00:09:40,000
better isolation also

00:09:36,640 --> 00:09:42,640
we have the exact same architecture

00:09:40,000 --> 00:09:43,680
in the service spring and self-managed

00:09:42,640 --> 00:09:47,200
cluster

00:09:43,680 --> 00:09:49,360
we have now we are using kubernetes

00:09:47,200 --> 00:09:53,279
operators to

00:09:49,360 --> 00:09:56,080
to manage their fling cluster

00:09:53,279 --> 00:09:56,399
all the other series on the components

00:09:56,080 --> 00:09:59,680
are

00:09:56,399 --> 00:10:00,399
also running are also developed as a

00:09:59,680 --> 00:10:02,880
kubernetes

00:10:00,399 --> 00:10:04,160
operator that are already on the

00:10:02,880 --> 00:10:07,279
kubernetes cluster

00:10:04,160 --> 00:10:08,000
we will not have bare processes running

00:10:07,279 --> 00:10:12,399
in

00:10:08,000 --> 00:10:12,399
in the machines

00:10:14,480 --> 00:10:18,480
so now we can know why we choose to

00:10:17,760 --> 00:10:21,440
migrate

00:10:18,480 --> 00:10:22,160
from how we are to quantities uh how to

00:10:21,440 --> 00:10:25,279
do that

00:10:22,160 --> 00:10:27,200
uh the easiest way is to stand on

00:10:25,279 --> 00:10:30,240
cluster on quantities

00:10:27,200 --> 00:10:34,000
uh first it is very simple and

00:10:30,240 --> 00:10:37,360
we do not need to uh change a theme code

00:10:34,000 --> 00:10:41,279
we just need to apply some yamos

00:10:37,360 --> 00:10:43,839
then the kubernetes will help us to

00:10:41,279 --> 00:10:44,480
launch the job manager deployment and

00:10:43,839 --> 00:10:47,200
some

00:10:44,480 --> 00:10:49,760
task managers when the task manager

00:10:47,200 --> 00:10:53,040
registers through the job manager

00:10:49,760 --> 00:10:55,279
uh or and an empty uh companies uh

00:10:53,040 --> 00:10:57,279
fringe operating station will be ready

00:10:55,279 --> 00:10:58,839
you could use flink client to submit a

00:10:57,279 --> 00:11:01,920
job to the

00:10:58,839 --> 00:11:04,000
existing spin cluster

00:11:01,920 --> 00:11:06,240
please remember that the c4 is

00:11:04,000 --> 00:11:08,480
unnecessary in drop of allocation

00:11:06,240 --> 00:11:11,440
cluster

00:11:08,480 --> 00:11:12,000
the grp since we have the therapy in the

00:11:11,440 --> 00:11:16,079
image

00:11:12,000 --> 00:11:20,320
and when the distance is launched

00:11:16,079 --> 00:11:20,320
the job will be recovered automatically

00:11:21,839 --> 00:11:24,959
uh you may still believe that you must

00:11:24,000 --> 00:11:26,959
do things that

00:11:24,959 --> 00:11:28,240
it is too difficult for me because i

00:11:26,959 --> 00:11:31,440
have to

00:11:28,240 --> 00:11:32,480
apply so many ammos do i have an easy

00:11:31,440 --> 00:11:35,600
way yes

00:11:32,480 --> 00:11:38,800
it is a kubernetes operator

00:11:35,600 --> 00:11:41,760
quinnis operator is very easy to use and

00:11:38,800 --> 00:11:44,800
could help us to manage multiple

00:11:41,760 --> 00:11:44,800
offering clusters

00:11:45,200 --> 00:11:51,760
for for a specific job it is

00:11:48,560 --> 00:11:54,880
it is it could help us to manage the

00:11:51,760 --> 00:11:57,279
whole life circle for example

00:11:54,880 --> 00:11:58,399
when we update the flink evaluation and

00:11:57,279 --> 00:12:02,079
restart as a

00:11:58,399 --> 00:12:03,200
thing cluster uh we upgrade the user and

00:12:02,079 --> 00:12:05,839
dependencies

00:12:03,200 --> 00:12:06,480
of three versions uh operators could

00:12:05,839 --> 00:12:09,519
help us

00:12:06,480 --> 00:12:12,079
to trigger a same point and then

00:12:09,519 --> 00:12:13,200
delete the flame cluster and create a

00:12:12,079 --> 00:12:16,399
new one

00:12:13,200 --> 00:12:19,040
okay it could recover from the latest

00:12:16,399 --> 00:12:20,959
latest starting point of same point and

00:12:19,040 --> 00:12:23,680
and keeps running

00:12:20,959 --> 00:12:24,880
each spring application is a single drop

00:12:23,680 --> 00:12:29,120
it is

00:12:24,880 --> 00:12:29,120
it could provide a better isolation

00:12:30,160 --> 00:12:35,519
uh so uh what what limitations uh do we

00:12:34,079 --> 00:12:38,240
still have uh

00:12:35,519 --> 00:12:38,959
the first is not aware of coordinates

00:12:38,240 --> 00:12:42,639
cluster

00:12:38,959 --> 00:12:45,200
we are using for control or operators to

00:12:42,639 --> 00:12:46,560
apply some ammos to launch the drone

00:12:45,200 --> 00:12:50,000
manual a task manager

00:12:46,560 --> 00:12:52,320
but flick is not it's not aware

00:12:50,000 --> 00:12:53,920
the class free cluster is running on

00:12:52,320 --> 00:12:56,560
quantities

00:12:53,920 --> 00:12:57,839
uh the second is step and studying

00:12:56,560 --> 00:13:00,560
resource allocation

00:12:57,839 --> 00:13:01,440
we have to we have to use uh external

00:13:00,560 --> 00:13:03,760
tools to

00:13:01,440 --> 00:13:06,320
launch the task manager bring do not

00:13:03,760 --> 00:13:08,000
have have the ability to

00:13:06,320 --> 00:13:10,320
adapt to the task manual port

00:13:08,000 --> 00:13:13,519
dynamically

00:13:10,320 --> 00:13:15,920
the store is uh the users have to

00:13:13,519 --> 00:13:19,040
request require some upfront knowledge

00:13:15,920 --> 00:13:21,760
about continuous operators

00:13:19,040 --> 00:13:22,160
but i think uh some friend users are

00:13:21,760 --> 00:13:25,360
they

00:13:22,160 --> 00:13:27,600
and they are more like to uh

00:13:25,360 --> 00:13:29,519
use spring run offering run application

00:13:27,600 --> 00:13:31,839
to submit job

00:13:29,519 --> 00:13:33,440
the last one is it is not convenient for

00:13:31,839 --> 00:13:37,200
the best shop and the

00:13:33,440 --> 00:13:39,760
amount of jobs in station because uh

00:13:37,200 --> 00:13:40,240
the job manager on the the quality and

00:13:39,760 --> 00:13:43,040
the

00:13:40,240 --> 00:13:43,600
resource manager could not advocate and

00:13:43,040 --> 00:13:47,120
port

00:13:43,600 --> 00:13:49,360
from police automatically on demand

00:13:47,120 --> 00:13:52,000
so let's take a look at the native

00:13:49,360 --> 00:13:52,000
integration

00:13:54,880 --> 00:13:58,639
what does uh what does anything mean i

00:13:57,440 --> 00:14:01,839
think here uh here

00:13:58,639 --> 00:14:05,519
that means we have an embedded

00:14:01,839 --> 00:14:08,560
queries class in flink line under

00:14:05,519 --> 00:14:11,279
the tremendous so

00:14:08,560 --> 00:14:13,440
friend khan could directly talk to

00:14:11,279 --> 00:14:16,320
kuani's api server

00:14:13,440 --> 00:14:17,360
to tell him to create the job manager

00:14:16,320 --> 00:14:20,880
deployment

00:14:17,360 --> 00:14:24,800
when the job manager department launched

00:14:20,880 --> 00:14:30,079
and the user could submit the job

00:14:24,800 --> 00:14:33,440
through the exposed service

00:14:30,079 --> 00:14:34,240
the job is submitted uh automatically

00:14:33,440 --> 00:14:37,440
spawn

00:14:34,240 --> 00:14:39,760
and it adds an electric resource from

00:14:37,440 --> 00:14:42,800
the opponent's resource manager

00:14:39,760 --> 00:14:46,160
the resource manager will allocate

00:14:42,800 --> 00:14:49,839
task report from quantities api server

00:14:46,160 --> 00:14:52,240
and demand when the

00:14:49,839 --> 00:14:54,399
task manager is launched it released to

00:14:52,240 --> 00:14:54,399
the

00:14:55,040 --> 00:15:00,800
master resource manager and

00:14:58,240 --> 00:15:03,279
then the job master will then the drug

00:15:00,800 --> 00:15:05,920
master will deploy the task and then mix

00:15:03,279 --> 00:15:05,920
jobs running

00:15:08,000 --> 00:15:11,199
of course we still of course we will

00:15:10,240 --> 00:15:15,120
have a

00:15:11,199 --> 00:15:17,360
frequency operator for the net mode

00:15:15,120 --> 00:15:18,320
it is very similar to the standalone

00:15:17,360 --> 00:15:21,440
components

00:15:18,320 --> 00:15:25,120
operator the only difference

00:15:21,440 --> 00:15:28,320
is the operator is in general

00:15:25,120 --> 00:15:29,839
implemented and we we are using the

00:15:28,320 --> 00:15:33,440
fabric8

00:15:29,839 --> 00:15:35,759
companies client the

00:15:33,440 --> 00:15:36,639
native companies operator will help us

00:15:35,759 --> 00:15:39,600
to create

00:15:36,639 --> 00:15:41,120
ingress the job manager deployment and

00:15:39,600 --> 00:15:44,240
all other resources

00:15:41,120 --> 00:15:48,000
the users just need to apply a

00:15:44,240 --> 00:15:51,600
custom resource yaml it is i think it is

00:15:48,000 --> 00:15:54,720
more friendly to the queries users

00:15:51,600 --> 00:15:57,839
or companies expert but he's not

00:15:54,720 --> 00:15:57,839
familiar with flink

00:15:59,199 --> 00:16:04,560
um so far we have new on why we

00:16:02,399 --> 00:16:06,720
uh we are we are mirroring flinging

00:16:04,560 --> 00:16:10,399
gloves from yar to quantities

00:16:06,720 --> 00:16:10,959
and how to do that then i will share

00:16:10,399 --> 00:16:14,480
some

00:16:10,959 --> 00:16:15,120
challenges in the production when we try

00:16:14,480 --> 00:16:18,880
to

00:16:15,120 --> 00:16:18,880
deploy flink on quantities

00:16:20,639 --> 00:16:25,360
the first oppo the first challenge is

00:16:22,959 --> 00:16:28,639
clarinetic problem platform

00:16:25,360 --> 00:16:31,040
uh i think uh

00:16:28,639 --> 00:16:31,920
it won't if we want to deploy things up

00:16:31,040 --> 00:16:35,120
when it is

00:16:31,920 --> 00:16:38,560
we just we at least we have two

00:16:35,120 --> 00:16:41,920
uh we have at least we we must have 14

00:16:38,560 --> 00:16:44,560
components the first is uh abb

00:16:41,920 --> 00:16:45,600
manager it is very similar to kubernetes

00:16:44,560 --> 00:16:48,800
operator

00:16:45,600 --> 00:16:52,000
and it could provide a web ui help us

00:16:48,800 --> 00:16:55,199
to manage the screen clusters

00:16:52,000 --> 00:16:59,120
we could use a circle

00:16:55,199 --> 00:17:01,360
under visual jazz to submit a free job

00:16:59,120 --> 00:17:03,519
also it could help us to upgrade the

00:17:01,360 --> 00:17:05,679
free job if you want to

00:17:03,519 --> 00:17:07,600
upgrade their user you just need to

00:17:05,679 --> 00:17:11,120
upload under

00:17:07,600 --> 00:17:13,679
and click the upgrade button

00:17:11,120 --> 00:17:14,959
all all other operations will be done

00:17:13,679 --> 00:17:18,319
the ap manager

00:17:14,959 --> 00:17:21,760
automatically internally

00:17:18,319 --> 00:17:24,079
the second is uh auto scaling service

00:17:21,760 --> 00:17:24,959
uh when we want to deploy the class on

00:17:24,079 --> 00:17:28,960
quantities

00:17:24,959 --> 00:17:31,679
uh we may have such following questions

00:17:28,960 --> 00:17:32,480
the first is how many task managers do i

00:17:31,679 --> 00:17:35,919
need

00:17:32,480 --> 00:17:39,200
uh how many strokes or i have to set

00:17:35,919 --> 00:17:40,320
for per tm uh how to set the parentism

00:17:39,200 --> 00:17:43,280
for job

00:17:40,320 --> 00:17:44,720
it is just for a single syndrome uh it

00:17:43,280 --> 00:17:47,840
may be that you have

00:17:44,720 --> 00:17:50,080
more than thousands of jobs to you and

00:17:47,840 --> 00:17:51,679
it will be a disaster

00:17:50,080 --> 00:17:54,000
all the scanning service could help us

00:17:51,679 --> 00:17:55,600
to do this it will start the job and

00:17:54,000 --> 00:17:58,640
keeps monitoring the

00:17:55,600 --> 00:17:59,520
delay the latency dc account and

00:17:58,640 --> 00:18:02,640
disaccounts

00:17:59,520 --> 00:18:05,280
and as a matrix

00:18:02,640 --> 00:18:06,160
and the restart drop with new pair uh

00:18:05,280 --> 00:18:10,160
parameters

00:18:06,160 --> 00:18:11,520
uh until it is stable jimmy stephen is a

00:18:10,160 --> 00:18:13,120
commercial uh

00:18:11,520 --> 00:18:15,280
commercial on plugin for step the

00:18:13,120 --> 00:18:17,840
backhand is pure jamma implementing it

00:18:15,280 --> 00:18:21,120
with better performance than roxdb

00:18:17,840 --> 00:18:23,120
for matrix unlocks we do not have a

00:18:21,120 --> 00:18:24,960
special requirements we are using

00:18:23,120 --> 00:18:28,000
local4j2 appender

00:18:24,960 --> 00:18:30,960
to directly to write the launch tool to

00:18:28,000 --> 00:18:31,919
remote storage buffering and we are

00:18:30,960 --> 00:18:34,400
using

00:18:31,919 --> 00:18:35,120
permissions and promises for the metric

00:18:34,400 --> 00:18:38,559
storage

00:18:35,120 --> 00:18:38,559
and graphene for the view

00:18:40,880 --> 00:18:45,200
the second challenge is uh have high

00:18:43,919 --> 00:18:47,520
availability

00:18:45,200 --> 00:18:49,200
high availability is a very basic

00:18:47,520 --> 00:18:51,200
requirement in production

00:18:49,200 --> 00:18:53,600
it helps to eliminate the single point

00:18:51,200 --> 00:18:56,480
of failure for free clusters

00:18:53,600 --> 00:18:57,440
for flink uh high availability and

00:18:56,480 --> 00:18:59,600
communication

00:18:57,440 --> 00:19:01,520
it is necessary to have more than one

00:18:59,600 --> 00:19:04,240
job manager in the cluster

00:19:01,520 --> 00:19:07,039
known as the active and staff and

00:19:04,240 --> 00:19:10,320
standby commanders

00:19:07,039 --> 00:19:10,960
so how the of flink test thing has

00:19:10,320 --> 00:19:15,039
provided

00:19:10,960 --> 00:19:18,240
to cable uh higher quality service uh

00:19:15,039 --> 00:19:21,440
it it has been used in production

00:19:18,240 --> 00:19:22,960
by many companies however

00:19:21,440 --> 00:19:25,039
when we want to drive through our

00:19:22,960 --> 00:19:25,600
kubernetes cluster we still have to

00:19:25,039 --> 00:19:28,880
manage

00:19:25,600 --> 00:19:31,919
a new cable cluster it take

00:19:28,880 --> 00:19:35,039
it takes additional cost for us

00:19:31,919 --> 00:19:37,200
so at the same time quantities has

00:19:35,039 --> 00:19:40,320
provided some

00:19:37,200 --> 00:19:43,520
the election api and the company map

00:19:40,320 --> 00:19:46,640
for the meta storage so uh we

00:19:43,520 --> 00:19:47,600
i think uh and we better if we if we

00:19:46,640 --> 00:19:50,080
have a

00:19:47,600 --> 00:19:51,200
native companies high availability so

00:19:50,080 --> 00:19:55,200
how the high

00:19:51,200 --> 00:19:55,200
availability works first uh

00:19:55,360 --> 00:20:01,200
first we started multiple germanders

00:19:58,240 --> 00:20:04,559
they are all trying to contain the law

00:20:01,200 --> 00:20:07,679
when when a specific determiner

00:20:04,559 --> 00:20:10,480
succeeds it is elected as a active

00:20:07,679 --> 00:20:11,440
leader all other uh traumatic becomes

00:20:10,480 --> 00:20:14,720
standby

00:20:11,440 --> 00:20:18,240
and tries to uh contender uh uh

00:20:14,720 --> 00:20:21,280
try uh tries to continue continuously

00:20:18,240 --> 00:20:24,080
the active uh also leader uh

00:20:21,280 --> 00:20:24,799
manager will periodically renew the

00:20:24,080 --> 00:20:28,240
token

00:20:24,799 --> 00:20:29,679
to keep the leadership the task manager

00:20:28,240 --> 00:20:31,840
will retrieve the

00:20:29,679 --> 00:20:34,799
german leader address from the company

00:20:31,840 --> 00:20:38,400
map and then register them to the

00:20:34,799 --> 00:20:41,440
active job manager manager

00:20:38,400 --> 00:20:44,559
we are directly using the computer map

00:20:41,440 --> 00:20:46,559
for the meta store for example the drop

00:20:44,559 --> 00:20:49,679
drop store a chamber store

00:20:46,559 --> 00:20:52,960
please remember that only the meta is

00:20:49,679 --> 00:20:55,919
stored in the map the real

00:20:52,960 --> 00:20:56,960
data is stored in the distributed system

00:20:55,919 --> 00:21:02,240
such as

00:20:56,960 --> 00:21:05,760
hdfs s3

00:21:02,240 --> 00:21:08,960
uh the next uh the next uh challenge is

00:21:05,760 --> 00:21:08,960
network performance

00:21:09,280 --> 00:21:12,720
uh the network architecture in phoenix

00:21:11,520 --> 00:21:15,840
is very different

00:21:12,720 --> 00:21:16,400
from yara since the yard do not have the

00:21:15,840 --> 00:21:18,799
network

00:21:16,400 --> 00:21:20,080
virtualization and isolation all the

00:21:18,799 --> 00:21:22,480
containers are running

00:21:20,080 --> 00:21:23,360
in the host machine network for

00:21:22,480 --> 00:21:26,559
communities

00:21:23,360 --> 00:21:29,760
and provide overlaid and work by default

00:21:26,559 --> 00:21:33,440
each port will have their own ip address

00:21:29,760 --> 00:21:36,799
so in the small deployment and

00:21:33,440 --> 00:21:37,840
we we we could use the flannel network

00:21:36,799 --> 00:21:40,159
backend

00:21:37,840 --> 00:21:40,960
however in our internal use case we

00:21:40,159 --> 00:21:43,039
found that

00:21:40,960 --> 00:21:44,400
for some payment network applications

00:21:43,039 --> 00:21:46,960
the performance of ram

00:21:44,400 --> 00:21:48,400
is about 10 percent lower than the yard

00:21:46,960 --> 00:21:51,280
of standard deployments

00:21:48,400 --> 00:21:52,320
solving job frankly filled with

00:21:51,280 --> 00:21:55,200
connection

00:21:52,320 --> 00:21:56,320
received appear or some other network

00:21:55,200 --> 00:21:59,039
problems

00:21:56,320 --> 00:22:01,039
so we enable the host network for the

00:21:59,039 --> 00:22:04,000
tremendous task managers

00:22:01,039 --> 00:22:04,799
uh in such case we should use a random

00:22:04,000 --> 00:22:08,799
port

00:22:04,799 --> 00:22:11,440
or for rent for the rpc rest

00:22:08,799 --> 00:22:13,280
broadband ports and the native uh

00:22:11,440 --> 00:22:15,679
queen's high ability

00:22:13,280 --> 00:22:17,760
could help a lot in such cases for the

00:22:15,679 --> 00:22:20,799
middle finger

00:22:17,760 --> 00:22:24,000
however in the current environment

00:22:20,799 --> 00:22:25,840
since we need to provide we need to

00:22:24,000 --> 00:22:26,720
provide isolation for different

00:22:25,840 --> 00:22:29,120
teammates

00:22:26,720 --> 00:22:30,880
the host network is not reasonable

00:22:29,120 --> 00:22:33,840
fortunately

00:22:30,880 --> 00:22:34,559
all the current vendors current vendors

00:22:33,840 --> 00:22:38,000
will provide

00:22:34,559 --> 00:22:40,240
the their own singing plugin

00:22:38,000 --> 00:22:42,159
using the cn plugin allows kubernetes

00:22:40,240 --> 00:22:45,440
ports to have the same

00:22:42,159 --> 00:22:47,280
iq address uh in the in their in the

00:22:45,440 --> 00:22:49,840
user vdc network

00:22:47,280 --> 00:22:51,200
the single eye plugin is responsible for

00:22:49,840 --> 00:22:54,159
attaching the

00:22:51,200 --> 00:22:56,240
elastic network interface and allocating

00:22:54,159 --> 00:22:58,799
the ikea address in vpc

00:22:56,240 --> 00:23:00,559
it could provide the same performance as

00:22:58,799 --> 00:23:04,159
host network with

00:23:00,559 --> 00:23:04,159
external asset isolation

00:23:04,720 --> 00:23:14,000
the number takes a

00:23:09,120 --> 00:23:17,440
support this is a manufacturer

00:23:14,000 --> 00:23:20,480
we have a silver master uh on the bottom

00:23:17,440 --> 00:23:22,640
uh on top of the supermaster is

00:23:20,480 --> 00:23:25,679
on this on top of the supermarket ponies

00:23:22,640 --> 00:23:28,240
cluster we provide a different

00:23:25,679 --> 00:23:30,880
uh virtual cluster for different users

00:23:28,240 --> 00:23:31,840
each broadcaster will have dedicated

00:23:30,880 --> 00:23:35,280
access server

00:23:31,840 --> 00:23:38,880
etc and coordinates so

00:23:35,280 --> 00:23:42,320
even some users are keeping uh uh

00:23:38,880 --> 00:23:43,360
are coming to this course uh list the

00:23:42,320 --> 00:23:46,559
large course

00:23:43,360 --> 00:23:48,840
it will not affect users also we could

00:23:46,559 --> 00:23:51,200
enable the security uh

00:23:48,840 --> 00:23:52,000
functionality for example the service

00:23:51,200 --> 00:23:57,039
account

00:23:52,000 --> 00:24:01,120
or the secret for the data planes

00:23:57,039 --> 00:24:03,760
we are using vpc and ei force network

00:24:01,120 --> 00:24:08,159
acceleration and just like we have

00:24:03,760 --> 00:24:10,000
we haven't seen in the last page uh

00:24:08,159 --> 00:24:11,600
we are using connect container for the

00:24:10,000 --> 00:24:15,279
resource isolation

00:24:11,600 --> 00:24:18,480
card and canon is uh provides strong

00:24:15,279 --> 00:24:22,159
isolation and then the docker container

00:24:18,480 --> 00:24:25,360
it is uh secure as a virtual machine

00:24:22,159 --> 00:24:27,919
but a fast answer container

00:24:25,360 --> 00:24:29,360
you can find more information uh on the

00:24:27,919 --> 00:24:32,080
website

00:24:29,360 --> 00:24:32,880
uh the last one is elastic counters we

00:24:32,080 --> 00:24:36,000
use

00:24:32,880 --> 00:24:39,440
each port will will have their own

00:24:36,000 --> 00:24:41,679
elastic disc uh it could provide

00:24:39,440 --> 00:24:43,360
uh it could provide better isolation

00:24:41,679 --> 00:24:46,799
between different ports

00:24:43,360 --> 00:24:49,520
for the disk uh disk of

00:24:46,799 --> 00:24:49,520
isolation

00:24:52,159 --> 00:24:57,520
the next challenge is gary my colleague

00:24:55,520 --> 00:25:00,960
taurian will give more introduction

00:24:57,520 --> 00:25:04,960
about it welcome

00:25:00,960 --> 00:25:04,960
thank you hello hello everyone

00:25:05,760 --> 00:25:09,600
okay

00:25:07,050 --> 00:25:12,480
[Music]

00:25:09,600 --> 00:25:14,799
okay hello everyone this is how i'm

00:25:12,480 --> 00:25:14,799
going to

00:25:25,279 --> 00:25:28,960
the background and the requirements of

00:25:27,440 --> 00:25:31,679
our usage

00:25:28,960 --> 00:25:34,080
scenarios our classrooms are providing

00:25:31,679 --> 00:25:38,159
resources for multi-tenant

00:25:34,080 --> 00:25:43,039
which will present videos such as

00:25:38,159 --> 00:25:43,039
advertisements meanwhile we

00:25:53,679 --> 00:25:57,840
for scheduling

00:26:07,360 --> 00:26:13,520
some of our clusters are actually

00:26:10,600 --> 00:26:16,480
heterogeneous so that they may contain

00:26:13,520 --> 00:26:17,120
devices with different computational or

00:26:16,480 --> 00:26:20,480
network

00:26:17,120 --> 00:26:23,600
capabilities thus we have higher

00:26:20,480 --> 00:26:27,600
requirements on node aware schedule

00:26:23,600 --> 00:26:30,880
link a unified data processing

00:26:27,600 --> 00:26:31,760
for both spec and streaming and our

00:26:30,880 --> 00:26:34,320
clusters

00:26:31,760 --> 00:26:35,120
are actually serving from mixed

00:26:34,320 --> 00:26:37,919
workloads

00:26:35,120 --> 00:26:39,279
and are expected to manage thousands of

00:26:37,919 --> 00:26:41,600
nodes later

00:26:39,279 --> 00:26:44,240
so higher throughput is the most

00:26:41,600 --> 00:26:47,760
important performance target

00:26:44,240 --> 00:26:48,480
last but not least observability helps

00:26:47,760 --> 00:26:51,679
users

00:26:48,480 --> 00:26:54,960
understand the internals of complex

00:26:51,679 --> 00:26:56,640
systems that is useful for figuring out

00:26:54,960 --> 00:26:59,440
the scheduling problem

00:26:56,640 --> 00:27:00,880
or analyzing performance bottleneck

00:26:59,440 --> 00:27:04,240
overall

00:27:00,880 --> 00:27:07,679
lattice default schedule cannot tackle

00:27:04,240 --> 00:27:10,880
these challenges it was filled for

00:27:07,679 --> 00:27:14,880
normal services when it comes to such a

00:27:10,880 --> 00:27:18,399
complex scenario it just doesn't work

00:27:14,880 --> 00:27:23,679
so here comes a question how can we

00:27:18,399 --> 00:27:23,679
tackle those challenges for flink

00:27:24,240 --> 00:27:27,279
then in the next section i will

00:27:26,559 --> 00:27:30,320
introduce

00:27:27,279 --> 00:27:32,880
unicorn how we optimize scheduling with

00:27:30,320 --> 00:27:32,880
unicorn

00:27:33,679 --> 00:27:39,919
so what is unicom it's a night wheat and

00:27:36,720 --> 00:27:43,039
standalone resource scheduler for

00:27:39,919 --> 00:27:44,960
lattice it's focused on building the

00:27:43,039 --> 00:27:48,000
scheduling capabilities to

00:27:44,960 --> 00:27:51,760
empower big data on kubernetes

00:27:48,000 --> 00:27:55,279
and is very easy to use and can co-exist

00:27:51,760 --> 00:27:55,279
with default scheduler

00:27:56,080 --> 00:28:00,559
now let's take a look at multi-tenant

00:27:58,559 --> 00:28:03,919
and workloads management

00:28:00,559 --> 00:28:07,919
based unicorn the brown flow shows

00:28:03,919 --> 00:28:10,640
how to manage your tenant results

00:28:07,919 --> 00:28:11,440
cluster domain can manage your tenant

00:28:10,640 --> 00:28:14,480
results

00:28:11,440 --> 00:28:16,000
via cluster management platform which

00:28:14,480 --> 00:28:19,039
will update a config

00:28:16,000 --> 00:28:20,080
map that keeps the configurations for

00:28:19,039 --> 00:28:22,799
unicorn

00:28:20,080 --> 00:28:24,720
then scheduler is notified and will

00:28:22,799 --> 00:28:27,760
update the internal stage

00:28:24,720 --> 00:28:29,039
accordingly the green flow shows how to

00:28:27,760 --> 00:28:31,520
schedule calls

00:28:29,039 --> 00:28:32,640
for the workloads talents can run

00:28:31,520 --> 00:28:36,159
workloads

00:28:32,640 --> 00:28:39,600
while workloads management platform

00:28:36,159 --> 00:28:42,320
which will ask api server to

00:28:39,600 --> 00:28:43,520
create pause the scheduler name of parts

00:28:42,320 --> 00:28:47,120
will be mutated

00:28:43,520 --> 00:28:50,559
by webhook then scheduler will schedule

00:28:47,120 --> 00:28:53,039
for parts with considering talent or

00:28:50,559 --> 00:28:55,279
workloads ordering according to the

00:28:53,039 --> 00:28:58,080
internal state

00:28:55,279 --> 00:29:00,640
unicorn uses queues with guaranteed

00:28:58,080 --> 00:29:03,600
results and maximum results to

00:29:00,640 --> 00:29:04,960
represent tenants guaranteed results

00:29:03,600 --> 00:29:08,240
means tenant

00:29:04,960 --> 00:29:11,919
can get get the results at any time

00:29:08,240 --> 00:29:14,080
and also can be seen as a bonus factor

00:29:11,919 --> 00:29:17,120
between tenants

00:29:14,080 --> 00:29:17,919
max results refers to the results

00:29:17,120 --> 00:29:20,240
quarter

00:29:17,919 --> 00:29:21,600
resource sharing can help some tenants

00:29:20,240 --> 00:29:25,360
to use elastic

00:29:21,600 --> 00:29:27,919
results comes from idle resource pool

00:29:25,360 --> 00:29:29,039
meanwhile the elastic results is not

00:29:27,919 --> 00:29:31,679
guaranteed

00:29:29,039 --> 00:29:34,320
since it can be taken away when the idle

00:29:31,679 --> 00:29:37,600
resource score is exhausted

00:29:34,320 --> 00:29:38,159
resource failures besides tenants over

00:29:37,600 --> 00:29:41,039
close

00:29:38,159 --> 00:29:41,840
ordering will help to avoid stopping

00:29:41,039 --> 00:29:45,200
some tenants

00:29:41,840 --> 00:29:48,000
or workloads priority aware scheduling

00:29:45,200 --> 00:29:48,399
besides workloads ordering and makes

00:29:48,000 --> 00:29:51,279
them

00:29:48,399 --> 00:29:53,200
well organized by which the scheduler

00:29:51,279 --> 00:29:55,760
can guarantee that

00:29:53,200 --> 00:29:56,240
important workloads can be scheduled

00:29:55,760 --> 00:29:59,760
before

00:29:56,240 --> 00:30:02,799
others and the primitive resources

00:29:59,760 --> 00:30:05,520
from less important running workloads if

00:30:02,799 --> 00:30:05,520
necessary

00:30:06,640 --> 00:30:10,000
then let's talk about new device

00:30:09,200 --> 00:30:13,919
scheduling

00:30:10,000 --> 00:30:16,880
some of our clusters have heterogeneous

00:30:13,919 --> 00:30:17,919
environment with class computing results

00:30:16,880 --> 00:30:21,919
like gpu

00:30:17,919 --> 00:30:23,200
fpga or scarce network resources such as

00:30:21,919 --> 00:30:25,360
emi

00:30:23,200 --> 00:30:26,240
here comes a big challenge how to

00:30:25,360 --> 00:30:29,279
schedule for

00:30:26,240 --> 00:30:31,760
mixed workloads with high support

00:30:29,279 --> 00:30:34,799
while the new sorting is the most time

00:30:31,760 --> 00:30:34,799
consuming part

00:30:35,200 --> 00:30:39,600
we have we have designed a flexible

00:30:38,240 --> 00:30:43,039
mechanism to meet

00:30:39,600 --> 00:30:46,399
requirements in complex

00:30:43,039 --> 00:30:49,279
environment the core is no starting

00:30:46,399 --> 00:30:50,799
algorithm it has an incremental

00:30:49,279 --> 00:30:53,840
implementation

00:30:50,799 --> 00:30:57,519
with a cache to keep a list of certain

00:30:53,840 --> 00:31:01,279
nodes node evaluator can give the static

00:30:57,519 --> 00:31:03,360
and the dynamic scores only the static

00:31:01,279 --> 00:31:04,960
costs are taken into account for the

00:31:03,360 --> 00:31:07,360
cash

00:31:04,960 --> 00:31:08,320
the green flow shows key actions of

00:31:07,360 --> 00:31:11,360
incremental

00:31:08,320 --> 00:31:14,880
implementation of node sorting

00:31:11,360 --> 00:31:18,240
algorithm for a common request

00:31:14,880 --> 00:31:20,960
algorithm will directly send sorted

00:31:18,240 --> 00:31:25,200
nodes from the cache to the scheduler

00:31:20,960 --> 00:31:28,080
but for requests with class results nike

00:31:25,200 --> 00:31:30,159
with the dynamics host for nodes with

00:31:28,080 --> 00:31:33,440
constant results

00:31:30,159 --> 00:31:37,600
we arranged them generally

00:31:33,440 --> 00:31:40,480
we can use flexible configurations to

00:31:37,600 --> 00:31:42,000
customize their node array scheduling

00:31:40,480 --> 00:31:45,120
for different

00:31:42,000 --> 00:31:48,320
generals knowing

00:31:45,120 --> 00:31:48,960
implementation has improved a lot on the

00:31:48,320 --> 00:31:51,519
scheduling

00:31:48,960 --> 00:31:51,519
performance

00:31:53,200 --> 00:31:57,519
in unicom we have done lots of

00:31:55,440 --> 00:31:58,640
optimizations to improve your

00:31:57,519 --> 00:32:02,000
performance

00:31:58,640 --> 00:32:05,600
as request management incremental

00:32:02,000 --> 00:32:08,480
node sorting algorithm and so on

00:32:05,600 --> 00:32:09,519
we have built a performance testing

00:32:08,480 --> 00:32:12,880
environment

00:32:09,519 --> 00:32:14,960
based on cookmark which is a performance

00:32:12,880 --> 00:32:17,600
testing tool

00:32:14,960 --> 00:32:18,960
and allows users to run tests on

00:32:17,600 --> 00:32:21,679
simulated

00:32:18,960 --> 00:32:24,240
cluster the testing environment has a

00:32:21,679 --> 00:32:27,600
cookie mark master and a real

00:32:24,240 --> 00:32:30,960
lattice cluster with one master and 18

00:32:27,600 --> 00:32:33,760
nodes google maps script can set up the

00:32:30,960 --> 00:32:34,000
honor nodes as calls on the real cluster

00:32:33,760 --> 00:32:36,799
and

00:32:34,000 --> 00:32:38,640
let them talk to the kubernetes api

00:32:36,799 --> 00:32:42,480
server

00:32:38,640 --> 00:32:45,200
following charts reveal how many seconds

00:32:42,480 --> 00:32:46,240
twenty thousand pounds can be scheduled

00:32:45,200 --> 00:32:49,039
on to

00:32:46,240 --> 00:32:49,760
two thousand or four thousand loads with

00:32:49,039 --> 00:32:53,440
unicorn

00:32:49,760 --> 00:32:55,919
default schedule the red line represents

00:32:53,440 --> 00:32:59,039
for unicorn and the green line

00:32:55,919 --> 00:33:02,080
represents for the default scheduler

00:32:59,039 --> 00:33:05,279
roughly we can get a four times better

00:33:02,080 --> 00:33:07,840
performance comparing to the default

00:33:05,279 --> 00:33:07,840
scheduler

00:33:08,679 --> 00:33:14,159
observability is very essential to users

00:33:12,000 --> 00:33:15,600
which can make them better understand

00:33:14,159 --> 00:33:18,880
workloads

00:33:15,600 --> 00:33:19,760
figure out why workloads fail or start

00:33:18,880 --> 00:33:22,840
smoothly

00:33:19,760 --> 00:33:24,000
and discover underlying problems of

00:33:22,840 --> 00:33:26,880
fortinet

00:33:24,000 --> 00:33:27,840
houses may be rised by different

00:33:26,880 --> 00:33:31,120
components

00:33:27,840 --> 00:33:34,240
we leverage open tracing api and yoga

00:33:31,120 --> 00:33:38,240
implementation to build a cheese

00:33:34,240 --> 00:33:42,240
architecture as a top picture show

00:33:38,240 --> 00:33:45,679
to enhance the observability

00:33:42,240 --> 00:33:48,480
there are two key cheese scopes

00:33:45,679 --> 00:33:50,240
including pot life cycle and the

00:33:48,480 --> 00:33:52,799
scheduling cycle

00:33:50,240 --> 00:33:54,320
and the following picture shows the

00:33:52,799 --> 00:33:57,919
traces of those two

00:33:54,320 --> 00:34:01,039
scopes we can not only track

00:33:57,919 --> 00:34:04,080
key transitions and internal details

00:34:01,039 --> 00:34:05,039
for any single workload but also can

00:34:04,080 --> 00:34:09,200
figure out

00:34:05,039 --> 00:34:13,040
underlying problem of our bottleneck

00:34:09,200 --> 00:34:13,040
while analyzing the 36

00:34:14,000 --> 00:34:17,440
then let's talk about the conclusion and

00:34:16,720 --> 00:34:20,399
some

00:34:17,440 --> 00:34:22,480
some of the roadmap we have done a lot

00:34:20,399 --> 00:34:25,599
of works to continuously

00:34:22,480 --> 00:34:29,679
improve the experience of integrating

00:34:25,599 --> 00:34:32,399
link with cloud native ecosystem

00:34:29,679 --> 00:34:34,000
and most of them has been contributed

00:34:32,399 --> 00:34:37,200
upstream to the compute

00:34:34,000 --> 00:34:40,320
community for the link

00:34:37,200 --> 00:34:43,679
both session and application mode with

00:34:40,320 --> 00:34:46,879
native integration have been supported

00:34:43,679 --> 00:34:49,359
in 1.11 version some

00:34:46,879 --> 00:34:50,639
remaining details and native high

00:34:49,359 --> 00:34:54,639
availability

00:34:50,639 --> 00:34:57,520
are planned in version 1.12

00:34:54,639 --> 00:34:59,440
and we have open source poc version of

00:34:57,520 --> 00:35:02,240
native link operator

00:34:59,440 --> 00:35:03,280
which is a control plan for running for

00:35:02,240 --> 00:35:06,400
inflative

00:35:03,280 --> 00:35:09,520
application on privileges moreover

00:35:06,400 --> 00:35:12,320
we are expecting to support external

00:35:09,520 --> 00:35:14,160
shutter service onto lettuce which is

00:35:12,320 --> 00:35:17,920
included in flip

00:35:14,160 --> 00:35:20,640
31 to enhance the rate of resource

00:35:17,920 --> 00:35:24,160
utilization

00:35:20,640 --> 00:35:27,200
for scheduling optimizations with unicom

00:35:24,160 --> 00:35:30,800
priority aware scheduling is planned

00:35:27,200 --> 00:35:32,520
in version 0.10

00:35:30,800 --> 00:35:35,119
now the aware scheduling and

00:35:32,520 --> 00:35:38,240
observabilities are in progress

00:35:35,119 --> 00:35:39,599
then later we will pay more attention on

00:35:38,240 --> 00:35:44,480
auto scanning

00:35:39,599 --> 00:35:46,320
results utilization and so on

00:35:44,480 --> 00:35:49,839
that's all thank you so much for your

00:35:46,320 --> 00:35:49,839
interest and attention

00:35:53,119 --> 00:35:59,839
any questions

00:36:10,839 --> 00:36:13,839
oh

00:36:24,839 --> 00:36:27,839
uh

00:36:56,320 --> 00:37:09,839
no questions this is

00:37:14,400 --> 00:37:23,839
thanks for your attention

00:37:33,520 --> 00:37:36,960
okay that's what it

00:37:48,839 --> 00:37:53,599
uh

00:37:51,359 --> 00:37:53,599
yes

00:38:03,680 --> 00:38:06,160
thank you

00:38:16,839 --> 00:38:19,839
okay

00:38:35,599 --> 00:38:37,680

YouTube URL: https://www.youtube.com/watch?v=4hghJCuZk5M


