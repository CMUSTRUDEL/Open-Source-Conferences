Title: GFS View Across all Hadoop Compatible File Systems w  the Lightweight Client Side Mount Points
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Global File System View Across all Hadoop Compatible File Systems with the LightWeight Client Side Mount Points.
Uma Maheswara Rao Gangumalla

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Hadoop File System layer has integrations to many popular storage systems including cloud storages like S3, Azure Data Lake Storage etc, along with in-house Apache Hadoop Distributed File System. When users want to migrate data between file systems, it’s very difficult for them to update their meta storages when they persist file system paths with schemes. For Example the Apache Hive persists the URI paths in meta-store. In Apache Hadoop, we came up with a solution(HDFS-15289) for this problem, i.e, the View FileSystem Overload Scheme with the configurable scheme and mount points. In this talk, we will cover in details, how users can enable it and how easily users can migrate data between file systems without modifying their meta-store. It’s completely transparent to users with respective to the file paths. We will present one of the use cases with Apache Hive partitioning, that is the user can move one/some of their partition data to a remote file system and just add a mount point on the default file system(ex: HDFS) where the user was working with. Here Hive queries will work transparently from the user point of view even though the data resides in a remote storage cluster ex: Apache Hadoop Ozone or S3. This will be very useful when users want to move certain kinds of data, ex: Cold Partitions, Small Files can be moved to remote clusters from a primary HDFS cluster without affecting applications. The Mount tables are maintained at the central server, all clients will load the tables while initializing the file system and also can refresh on modification of mount points. So, that all the initializing clients will be in sync. This will make user’s life easier to migrate data between cloud and on-premise storages in a much flexible way.

Uma Maheswara Rao Gangumalla is an Apache Software Foundation Member[1]. An Apache Hadoop, BookKeeper, Incubator committer and a member of the Project Management Committee[2], and a long-term active contributor to the Apache Hadoop project. He is also mentoring several incubator projects at Apache. Uma holds a bachelor's degree in Electronics and Communications Engineering. He has more than 13 years of experience in large scale Distributed Software Platforms design and development. Currently, Uma is working as a Principal Software Engineer at Cloudera, Inc, California, and primarily focuses on open source big data technologies. Prior to this, Uma worked as a Software Architect in Intel Corporation, California. [1] https://www.apache.org/foundation/members.html [2] http://people.apache.org/phonebook.html?uid=umamahesh
Captions: 
	
YouTube URL: https://www.youtube.com/watch?v=DzLLPwA6ETI


