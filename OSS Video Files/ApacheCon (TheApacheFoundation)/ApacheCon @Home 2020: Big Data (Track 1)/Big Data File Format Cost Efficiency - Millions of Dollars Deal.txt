Title: Big Data File Format Cost Efficiency - Millions of Dollars Deal
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	Big Data File Format Cost Efficiency - Millions of Dollars Deal
Xinli Shang, Juncheng Ma

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Reducing the size of data at rest and in transit is critical to many organizations, not only because it can save the cost of storage but also can improve the IO usage and traffic volume in the network. We will present how to translate hundreds of petabytes data compressed in GZIP in the data lake to a more efficient compression method - ZSTD which can reduce the data size by 10% and save millions of dollars. We will show the recent Apache Parquet format improvement that makes ZSTD compression to be easily set up (PARQUET-1866). The tech talk will also demonstrate how we solve the challenges of the compression translation speed by improving the throughput by 5X (PARQUET-1872). As a result, the translation time of large scale data sets can be reduced from months to days and save compute vCores correspondingly. The translation needs to be in a safe way to prevent data corruption and incompatibility. We will also show the technicals that are built into the compression translation tool to prevent them from happening. Another significant storage size reduction (up to 80%) can be done by 1) reordering the columns in Parquet to make it more friendly to encoding and compression, 2) encoding with BYTE_STREAM_SPLIT (PARQUET-1622) that is more efficient for floating type data, 3) reducing geolocation data precision to make RLE more efficient, 4) pruning unused columns (PARQUET-1800). We will show above every technique, the effectiveness of each and the reason behind it. We will also show the tools like Parquet column-size (PARQUET-1821) that can help users to identify the candidate tables to apply the above techniques.

Xinli Shang:
Xinli Shang is a tech lead on the Uber Data Infra team, Apache Parquet Committer. He is passionate about big data file format for efficiency, performance and security, tuning large scale services for performance, throughput, and reliability. He is an active contributor to Apache Parquet. He also has many years of experience developing large scale distributed systems like S3 Index, and operating system Windows.
Juncheng Ma :
Software Developer at Uber
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,439 --> 00:00:27,760
all right

00:00:28,560 --> 00:00:32,960
hello everybody can everybody hear me

00:00:33,680 --> 00:00:37,280
let's talk about their storage cost

00:00:36,000 --> 00:00:39,680
efficiency

00:00:37,280 --> 00:00:41,040
at the big data fair format level

00:00:39,680 --> 00:00:44,079
particularly

00:00:41,040 --> 00:00:46,000
either apache proclaim will show you

00:00:44,079 --> 00:00:47,600
the recent work with date inside the

00:00:46,000 --> 00:00:50,079
uber

00:00:47,600 --> 00:00:52,559
using the property file size to save the

00:00:50,079 --> 00:00:52,559
storage

00:00:56,840 --> 00:00:59,840
cost

00:01:02,960 --> 00:01:07,600
my name is sushini i'm a tech lead

00:01:04,799 --> 00:01:09,600
manager at the uber data info located at

00:01:07,600 --> 00:01:11,520
seattle i'm also a party public

00:01:09,600 --> 00:01:13,760
committer my colleague

00:01:11,520 --> 00:01:16,159
clinton he worked with me for this

00:01:13,760 --> 00:01:16,159
project

00:01:16,320 --> 00:01:21,920
so this talk is organized this way first

00:01:19,920 --> 00:01:23,840
what the problem that we are trying to

00:01:21,920 --> 00:01:26,479
solve and second

00:01:23,840 --> 00:01:27,280
what techniques that we use to solve

00:01:26,479 --> 00:01:29,280
this problem

00:01:27,280 --> 00:01:31,200
particularly to reduce how to reduce the

00:01:29,280 --> 00:01:32,880
broken file size

00:01:31,200 --> 00:01:35,600
we will talk about this standard

00:01:32,880 --> 00:01:37,920
compression column pruning

00:01:35,600 --> 00:01:38,880
column reordering decimal place

00:01:37,920 --> 00:01:41,280
reduction

00:01:38,880 --> 00:01:42,640
so all of these techniques together can

00:01:41,280 --> 00:01:45,439
bring down the file size

00:01:42,640 --> 00:01:46,079
dramatically so those technologies are

00:01:45,439 --> 00:01:49,119
cool

00:01:46,079 --> 00:01:50,399
but when you apply these technologies to

00:01:49,119 --> 00:01:52,640
the production

00:01:50,399 --> 00:01:54,399
you will see all different problems now

00:01:52,640 --> 00:01:55,600
we will talk about how to roll up to the

00:01:54,399 --> 00:01:59,600
production

00:01:55,600 --> 00:01:59,600
now we look forward to the next step

00:02:00,000 --> 00:02:03,520
will you focus on the discussion about

00:02:02,079 --> 00:02:06,240
the technologies

00:02:03,520 --> 00:02:08,319
and other than the data itself we will

00:02:06,240 --> 00:02:10,239
talk about the different technology

00:02:08,319 --> 00:02:11,440
different technologies how to bring down

00:02:10,239 --> 00:02:14,160
the file size

00:02:11,440 --> 00:02:16,319
but we are not going to talk about what

00:02:14,160 --> 00:02:18,160
data it is

00:02:16,319 --> 00:02:19,599
we are going to talk about apache pro

00:02:18,160 --> 00:02:23,840
query but we are not

00:02:19,599 --> 00:02:23,840
going to cover oracy

00:02:24,000 --> 00:02:27,920
the problem we're trying to solve is

00:02:26,800 --> 00:02:31,120
very straightforward

00:02:27,920 --> 00:02:31,920
it's about money saving so as a business

00:02:31,120 --> 00:02:34,640
grows

00:02:31,920 --> 00:02:36,720
so does the data particularly for the

00:02:34,640 --> 00:02:39,040
data lake

00:02:36,720 --> 00:02:40,239
people just ingest all different data

00:02:39,040 --> 00:02:43,760
into the link

00:02:40,239 --> 00:02:46,400
the data grows very fast

00:02:43,760 --> 00:02:47,120
if you if you store your data in the

00:02:46,400 --> 00:02:50,959
cloud

00:02:47,120 --> 00:02:54,640
like s3 azure or gcs

00:02:50,959 --> 00:02:56,879
you need to pay the cost every month

00:02:54,640 --> 00:02:58,800
not only the storage cost but also we

00:02:56,879 --> 00:03:00,720
access the data when you transport the

00:02:58,800 --> 00:03:03,599
data you also need to pay for it

00:03:00,720 --> 00:03:05,360
you will see your monthly bill growth if

00:03:03,599 --> 00:03:08,800
you store your data

00:03:05,360 --> 00:03:09,680
on premise for example in the hdfs data

00:03:08,800 --> 00:03:12,000
lake

00:03:09,680 --> 00:03:14,879
you probably need to add the hardware

00:03:12,000 --> 00:03:18,400
periodically that's also a lot of money

00:03:14,879 --> 00:03:21,840
not only about the money saving but also

00:03:18,400 --> 00:03:23,840
the scale pressure the real funds i o

00:03:21,840 --> 00:03:26,640
slowing you we found the

00:03:23,840 --> 00:03:27,760
network congestion high gc long latency

00:03:26,640 --> 00:03:30,720
etc

00:03:27,760 --> 00:03:33,840
all of these issues is because of data

00:03:30,720 --> 00:03:33,840
is so large

00:03:34,720 --> 00:03:39,200
so we we have the same problem either

00:03:36,959 --> 00:03:43,680
uber so we will show you what we did

00:03:39,200 --> 00:03:43,680
how do we reduce the data size

00:03:45,680 --> 00:03:51,840
before we started let's have a quick

00:03:47,840 --> 00:03:55,360
review of the big data file format

00:03:51,840 --> 00:03:58,560
in the big data analytics world

00:03:55,360 --> 00:04:01,360
is many of the two categories rule level

00:03:58,560 --> 00:04:02,959
and the column level rule rule so row

00:04:01,360 --> 00:04:05,680
storage and the column storage

00:04:02,959 --> 00:04:06,319
file format for the column storage file

00:04:05,680 --> 00:04:09,840
format

00:04:06,319 --> 00:04:13,200
it's mainly about opaque and oracy

00:04:09,840 --> 00:04:16,560
they are used very widely they spark

00:04:13,200 --> 00:04:17,120
presto hive etc all need to deal with

00:04:16,560 --> 00:04:20,639
this

00:04:17,120 --> 00:04:23,840
file format let's zoom in

00:04:20,639 --> 00:04:26,720
a particular query for a little bit

00:04:23,840 --> 00:04:27,680
more detail to build up the context here

00:04:26,720 --> 00:04:30,800
as i said

00:04:27,680 --> 00:04:34,080
a particular query is a column storage

00:04:30,800 --> 00:04:36,000
that means the same column data will be

00:04:34,080 --> 00:04:38,479
put together

00:04:36,000 --> 00:04:40,000
so that the retrieve the data the query

00:04:38,479 --> 00:04:41,120
will be very fast because you can

00:04:40,000 --> 00:04:44,479
retrieve the data

00:04:41,120 --> 00:04:47,199
at once and also it will make their

00:04:44,479 --> 00:04:49,759
data set smaller because the same column

00:04:47,199 --> 00:04:52,000
value usually has a lot of similarities

00:04:49,759 --> 00:04:57,600
we apply the encoding et cetera that

00:04:52,000 --> 00:05:00,720
will be more efficient

00:04:57,600 --> 00:05:01,440
now a proqu file is divided into

00:05:00,720 --> 00:05:05,120
different

00:05:01,440 --> 00:05:05,919
row groups each row groups is divided

00:05:05,120 --> 00:05:08,560
into different

00:05:05,919 --> 00:05:10,639
column chunks so each column channel is

00:05:08,560 --> 00:05:13,199
corresponding to a color

00:05:10,639 --> 00:05:13,840
so inside a column chunk is further

00:05:13,199 --> 00:05:16,960
divided

00:05:13,840 --> 00:05:19,199
into pages so the page is

00:05:16,960 --> 00:05:21,840
your need for the encoding and

00:05:19,199 --> 00:05:21,840
compression

00:05:22,560 --> 00:05:29,360
cochlea several ways for the encoding

00:05:26,240 --> 00:05:30,479
and compression plain coding is less

00:05:29,360 --> 00:05:34,000
efficient

00:05:30,479 --> 00:05:36,960
but whenever the modification techniques

00:05:34,000 --> 00:05:38,240
is not able to be used the plane coding

00:05:36,960 --> 00:05:40,479
will be used

00:05:38,240 --> 00:05:41,919
for thinking the coding it will build up

00:05:40,479 --> 00:05:44,720
a dictionary

00:05:41,919 --> 00:05:47,199
from their column data instead of

00:05:44,720 --> 00:05:50,560
storing their

00:05:47,199 --> 00:05:53,840
install the index of the data inside the

00:05:50,560 --> 00:05:57,280
directory inside the dictionary

00:05:53,840 --> 00:05:58,880
runs encoding is very good at encoding

00:05:57,280 --> 00:06:01,120
the repeated values

00:05:58,880 --> 00:06:02,240
data encoding is similar to the release

00:06:01,120 --> 00:06:04,880
encoding

00:06:02,240 --> 00:06:05,759
but it's better to deal with the data

00:06:04,880 --> 00:06:08,479
has

00:06:05,759 --> 00:06:10,080
you know the larger data ranges but we

00:06:08,479 --> 00:06:11,520
also provide a variety of the

00:06:10,080 --> 00:06:14,560
compression magnesiums

00:06:11,520 --> 00:06:18,319
snatchy easily lead standard etc

00:06:14,560 --> 00:06:18,319
so let's talk about this standard

00:06:18,639 --> 00:06:22,880
two or three years ago facebook

00:06:20,639 --> 00:06:25,120
published to use this standard to

00:06:22,880 --> 00:06:28,319
optimize the compatible ratio

00:06:25,120 --> 00:06:28,960
and the speed compression ratio is

00:06:28,319 --> 00:06:32,400
defined

00:06:28,960 --> 00:06:35,039
as the uncompressed data divided by

00:06:32,400 --> 00:06:36,400
the compressed data the larger of the

00:06:35,039 --> 00:06:39,680
compression ratio

00:06:36,400 --> 00:06:43,280
the more efficient of the storage

00:06:39,680 --> 00:06:44,880
on the other side there is a speed for

00:06:43,280 --> 00:06:46,720
writing and reading which is

00:06:44,880 --> 00:06:49,919
corresponding to the compression

00:06:46,720 --> 00:06:51,440
and decompression usually is a trade-off

00:06:49,919 --> 00:06:54,080
between the company ratio

00:06:51,440 --> 00:06:56,080
and speed if you want to optimize the

00:06:54,080 --> 00:06:56,639
competitive ratio you can sacrifice the

00:06:56,080 --> 00:07:00,479
speed

00:06:56,639 --> 00:07:03,360
and vice versa so here's a diagram

00:07:00,479 --> 00:07:03,840
published by the facebook uh the left

00:07:03,360 --> 00:07:06,960
chart

00:07:03,840 --> 00:07:08,160
is the comparison of the comprehensive

00:07:06,960 --> 00:07:12,160
speed and

00:07:08,160 --> 00:07:15,360
ratio between the z standard and z link

00:07:12,160 --> 00:07:18,479
so you can see the system outperform

00:07:15,360 --> 00:07:21,360
the z-leaf the blue line is a z standard

00:07:18,479 --> 00:07:24,240
and green line is a ceiling

00:07:21,360 --> 00:07:26,000
if you look at it vertically for a given

00:07:24,240 --> 00:07:28,800
compression speed

00:07:26,000 --> 00:07:30,560
the up either this standard meaning the

00:07:28,800 --> 00:07:32,479
company ratio is larger

00:07:30,560 --> 00:07:34,000
than the delay if you look at the

00:07:32,479 --> 00:07:36,880
horizontally

00:07:34,000 --> 00:07:37,520
given for a given component ratio the

00:07:36,880 --> 00:07:39,840
speed

00:07:37,520 --> 00:07:40,800
of the comfort of the z standard is

00:07:39,840 --> 00:07:43,919
larger

00:07:40,800 --> 00:07:44,879
than the z-link on the right chart is a

00:07:43,919 --> 00:07:47,840
decompression

00:07:44,879 --> 00:07:50,080
so decompress speed is kind of constant

00:07:47,840 --> 00:07:53,599
about this then all the other perform

00:07:50,080 --> 00:07:53,599
the delay

00:07:54,160 --> 00:08:01,759
so we did our in-house experiment

00:07:58,639 --> 00:08:05,599
we translate 65 files each

00:08:01,759 --> 00:08:08,720
file has one gig file size

00:08:05,599 --> 00:08:12,080
so those files are comprised by the gzip

00:08:08,720 --> 00:08:14,479
currently and we translate

00:08:12,080 --> 00:08:16,400
to the z standard to see how much file

00:08:14,479 --> 00:08:19,599
size can be reduced

00:08:16,400 --> 00:08:20,080
so we did it for each file we did it 22

00:08:19,599 --> 00:08:22,000
times

00:08:20,080 --> 00:08:24,160
for each time we use a different

00:08:22,000 --> 00:08:25,360
compression level here's a concept of

00:08:24,160 --> 00:08:29,120
combination level

00:08:25,360 --> 00:08:31,520
that the larger of the conversion level

00:08:29,120 --> 00:08:33,440
the more comparison ratio it can achieve

00:08:31,520 --> 00:08:37,440
so currently this standards about

00:08:33,440 --> 00:08:37,440
from 1 to 22.

00:08:38,000 --> 00:08:42,080
so after we have done all of this

00:08:40,719 --> 00:08:45,040
experiment

00:08:42,080 --> 00:08:45,680
so we just draw a diagram the reduced

00:08:45,040 --> 00:08:48,160
size

00:08:45,680 --> 00:08:49,519
versus the compression level the reduced

00:08:48,160 --> 00:08:52,880
side is the average

00:08:49,519 --> 00:08:55,200
across the 65 files

00:08:52,880 --> 00:08:56,160
so that will change as the compression

00:08:55,200 --> 00:08:59,040
level

00:08:56,160 --> 00:09:00,640
changes so those files are token from

00:08:59,040 --> 00:09:03,279
the production file

00:09:00,640 --> 00:09:04,880
with all different data tags make sure

00:09:03,279 --> 00:09:08,240
that we have a pretty good

00:09:04,880 --> 00:09:10,320
coverage so

00:09:08,240 --> 00:09:12,320
from the chart you can see as the

00:09:10,320 --> 00:09:15,839
composition level goes up

00:09:12,320 --> 00:09:18,000
the reduced size percentage also goes up

00:09:15,839 --> 00:09:19,600
roughly but there are some exceptions

00:09:18,000 --> 00:09:20,800
sometimes they decrease a little bit

00:09:19,600 --> 00:09:23,839
sometimes they fly

00:09:20,800 --> 00:09:27,519
but the overall train we can see is

00:09:23,839 --> 00:09:31,680
it will increase as the comprehension

00:09:27,519 --> 00:09:33,760
level goes up now speaking of

00:09:31,680 --> 00:09:36,640
the compression and decompressing time

00:09:33,760 --> 00:09:36,640
also the speed

00:09:38,160 --> 00:09:41,760
we refer the right time and the real

00:09:40,560 --> 00:09:44,640
time as an

00:09:41,760 --> 00:09:46,000
intuit duration of the file reading on

00:09:44,640 --> 00:09:49,279
real time because that's

00:09:46,000 --> 00:09:52,480
they're more appropriate to evaluate as

00:09:49,279 --> 00:09:54,160
end-to-end scenario the chart here you

00:09:52,480 --> 00:09:56,240
can see on the left one

00:09:54,160 --> 00:09:57,279
which is the right time is corresponding

00:09:56,240 --> 00:10:01,120
to the

00:09:57,279 --> 00:10:03,600
different component level from 1 to 22.

00:10:01,120 --> 00:10:06,079
as the level goes up the writing time

00:10:03,600 --> 00:10:09,120
also goes up

00:10:06,079 --> 00:10:12,160
the right side is a decomposition

00:10:09,120 --> 00:10:15,360
time of the read time which is

00:10:12,160 --> 00:10:16,240
kind of a constant so this number this

00:10:15,360 --> 00:10:19,519
chain

00:10:16,240 --> 00:10:23,360
in line with the numbers that

00:10:19,519 --> 00:10:26,800
the facebook web page published

00:10:23,360 --> 00:10:28,079
from this two diagram along with the

00:10:26,800 --> 00:10:30,839
last slides

00:10:28,079 --> 00:10:33,920
so we decided to use comprehensive level

00:10:30,839 --> 00:10:37,200
19 because it can give us

00:10:33,920 --> 00:10:46,079
a pretty decent compression ratio

00:10:37,200 --> 00:10:49,040
also with reasonable writing speed

00:10:46,079 --> 00:10:50,880
now when we translate the gzip to uh

00:10:49,040 --> 00:10:53,600
this standard in the production

00:10:50,880 --> 00:10:56,800
we started with query engine we use

00:10:53,600 --> 00:10:58,880
spark job to do so

00:10:56,800 --> 00:11:01,360
now the spark job just called the poke

00:10:58,880 --> 00:11:04,800
library to do all of these steps

00:11:01,360 --> 00:11:07,760
what we found is it's very slow

00:11:04,800 --> 00:11:08,880
so let's walk through how the the flow

00:11:07,760 --> 00:11:11,440
it goes

00:11:08,880 --> 00:11:13,440
first the api they call the procreate if

00:11:11,440 --> 00:11:14,480
you have to do the decompression from

00:11:13,440 --> 00:11:17,200
the adhesive

00:11:14,480 --> 00:11:18,000
i then do the decoding and reassembling

00:11:17,200 --> 00:11:20,320
the record

00:11:18,000 --> 00:11:21,200
you get the value the spark also do the

00:11:20,320 --> 00:11:23,360
shuffle

00:11:21,200 --> 00:11:24,240
and then during the writing pass you

00:11:23,360 --> 00:11:27,040
have to do the

00:11:24,240 --> 00:11:28,880
assembly of the record by encoding and

00:11:27,040 --> 00:11:31,200
the compression using this standard

00:11:28,880 --> 00:11:32,160
so these steps are pretty long that's

00:11:31,200 --> 00:11:36,320
why i take

00:11:32,160 --> 00:11:38,640
a lot of time now if you have

00:11:36,320 --> 00:11:41,040
hundreds of parabolic data you can

00:11:38,640 --> 00:11:42,800
imagine how much time you need it

00:11:41,040 --> 00:11:44,560
it probably is not realistic that you

00:11:42,800 --> 00:11:46,959
can finish the task in a short time

00:11:44,560 --> 00:11:46,959
window

00:11:47,519 --> 00:11:53,760
now how do we increase the speed

00:11:50,800 --> 00:11:56,000
now the question is our goal is just

00:11:53,760 --> 00:11:58,079
translate the compression

00:11:56,000 --> 00:11:59,680
is there a way that we can bypass all

00:11:58,079 --> 00:12:01,680
the underlying steps

00:11:59,680 --> 00:12:03,040
except for the compression and

00:12:01,680 --> 00:12:06,240
decompression

00:12:03,040 --> 00:12:09,040
so we make changes to the pocket library

00:12:06,240 --> 00:12:09,440
so we bypass all of these steps what we

00:12:09,040 --> 00:12:13,279
do

00:12:09,440 --> 00:12:16,160
is we step into the um the

00:12:13,279 --> 00:12:16,639
the reading path that when the page is

00:12:16,160 --> 00:12:19,600
really

00:12:16,639 --> 00:12:20,480
first into the decompression then we

00:12:19,600 --> 00:12:23,040
stop there

00:12:20,480 --> 00:12:25,120
so we do the compression immediately

00:12:23,040 --> 00:12:27,920
using this standard and ready to back

00:12:25,120 --> 00:12:28,720
so we bypass all of these steps so we

00:12:27,920 --> 00:12:31,519
see

00:12:28,720 --> 00:12:33,519
five times faster of the speed than

00:12:31,519 --> 00:12:36,240
using the spark oranges

00:12:33,519 --> 00:12:40,480
so this work is merged to the upstream

00:12:36,240 --> 00:12:40,480
it will be released in the property 12.

00:12:41,760 --> 00:12:45,200
now we conclude the discussion of this

00:12:44,480 --> 00:12:47,600
standard

00:12:45,200 --> 00:12:48,240
the second techniques we use is column

00:12:47,600 --> 00:12:52,000
pruning

00:12:48,240 --> 00:12:54,160
which is to remove the unused colors

00:12:52,000 --> 00:12:56,320
so most organizations they do not have

00:12:54,160 --> 00:12:59,519
governance for adding a columns

00:12:56,320 --> 00:13:02,880
for instance when a table owner they add

00:12:59,519 --> 00:13:04,480
a table they decide what column they use

00:13:02,880 --> 00:13:06,480
they don't really have a think of it do

00:13:04,480 --> 00:13:08,880
i really need this column but let's just

00:13:06,480 --> 00:13:08,880
add it

00:13:08,959 --> 00:13:12,880
at the end of the day a lot of columns

00:13:11,600 --> 00:13:15,360
are they may not need it

00:13:12,880 --> 00:13:16,639
at all but they take a lot of space to

00:13:15,360 --> 00:13:19,279
give you an example

00:13:16,639 --> 00:13:22,079
we just see one table we remove one

00:13:19,279 --> 00:13:26,160
column we see two parabolas data cvs

00:13:22,079 --> 00:13:29,760
which is huge now the question is

00:13:26,160 --> 00:13:32,560
which column we want to remove

00:13:29,760 --> 00:13:33,440
there's two factors here one is the

00:13:32,560 --> 00:13:36,399
usage

00:13:33,440 --> 00:13:37,279
do we still need that color second is

00:13:36,399 --> 00:13:40,240
how large

00:13:37,279 --> 00:13:41,760
is a color for the first one you may

00:13:40,240 --> 00:13:43,920
want to query the audio log

00:13:41,760 --> 00:13:46,160
of the company probably or the linux

00:13:43,920 --> 00:13:49,440
data etc whatever that you can

00:13:46,160 --> 00:13:51,680
use to get the usage data a second

00:13:49,440 --> 00:13:52,720
factor is column size estimator so we

00:13:51,680 --> 00:13:56,079
write a tool

00:13:52,720 --> 00:13:58,240
uh to estimate the column size

00:13:56,079 --> 00:14:00,399
this also merge to the upstream and will

00:13:58,240 --> 00:14:03,600
be releasing probably 12.

00:14:00,399 --> 00:14:05,199
so after you decide this column or let a

00:14:03,600 --> 00:14:06,079
collection of the column need to be

00:14:05,199 --> 00:14:08,320
removed

00:14:06,079 --> 00:14:11,360
now you need a tool to remove those

00:14:08,320 --> 00:14:14,320
columns in a high throughput way

00:14:11,360 --> 00:14:14,320
let's talk about that

00:14:14,720 --> 00:14:19,760
now when we remove a column we started

00:14:17,680 --> 00:14:22,079
with the quarantines as we did

00:14:19,760 --> 00:14:23,040
sensing for the trans compression from

00:14:22,079 --> 00:14:26,959
this from the

00:14:23,040 --> 00:14:29,120
gdp to this standard what we found is

00:14:26,959 --> 00:14:30,399
it's very slow you can imagine let's do

00:14:29,120 --> 00:14:33,120
a lot of steps you can

00:14:30,399 --> 00:14:35,519
you have to fully read the data you know

00:14:33,120 --> 00:14:37,600
assembly so yeah many many steps sharply

00:14:35,519 --> 00:14:41,600
et cetera

00:14:37,600 --> 00:14:44,959
now we just developed a pruning tool

00:14:41,600 --> 00:14:48,720
the idea of the tool is let's do the

00:14:44,959 --> 00:14:51,360
file copy during you calling the file

00:14:48,720 --> 00:14:53,600
you can skip those columns that you

00:14:51,360 --> 00:14:55,760
don't need you want to prompt

00:14:53,600 --> 00:14:57,360
so at the end of the day the speed of

00:14:55,760 --> 00:14:59,519
the column priming

00:14:57,360 --> 00:15:00,639
will be the failed copying speed so you

00:14:59,519 --> 00:15:02,560
can imagine

00:15:00,639 --> 00:15:05,519
that it will be very fast we see 10

00:15:02,560 --> 00:15:12,320
times faster than using the spark

00:15:05,519 --> 00:15:15,199
this work is also merged to upstream

00:15:12,320 --> 00:15:16,320
another techniques we use to reduce the

00:15:15,199 --> 00:15:19,440
profi file size

00:15:16,320 --> 00:15:20,320
is called column reordering it will make

00:15:19,440 --> 00:15:23,680
the encoding

00:15:20,320 --> 00:15:25,839
more efficient because after the column

00:15:23,680 --> 00:15:28,000
reordering of column sorting

00:15:25,839 --> 00:15:28,959
the data has the same value or similar

00:15:28,000 --> 00:15:32,079
value will be

00:15:28,959 --> 00:15:33,440
put together or closer now the encoding

00:15:32,079 --> 00:15:35,920
like a valence encoding

00:15:33,440 --> 00:15:37,519
et cetera that would be more efficient

00:15:35,920 --> 00:15:40,639
so there's a paper

00:15:37,519 --> 00:15:42,000
talk about column reordering and how to

00:15:40,639 --> 00:15:44,959
minimize the length

00:15:42,000 --> 00:15:45,839
of the reordering so we inspired by this

00:15:44,959 --> 00:15:47,199
paper

00:15:45,839 --> 00:15:49,680
but we don't initially actually

00:15:47,199 --> 00:15:54,639
implement uh exactly the algorithm

00:15:49,680 --> 00:15:57,839
of it here's our experiment

00:15:54,639 --> 00:16:01,199
so there's a table have four colors

00:15:57,839 --> 00:16:04,240
uuid which is stream timestamp which is

00:16:01,199 --> 00:16:05,680
big inch latitude longitude with a

00:16:04,240 --> 00:16:09,120
double

00:16:05,680 --> 00:16:11,040
now we started with no salty we get the

00:16:09,120 --> 00:16:12,720
side we just take one partition of the

00:16:11,040 --> 00:16:15,600
table and do the experiment

00:16:12,720 --> 00:16:17,600
we start with no saltine and then we

00:16:15,600 --> 00:16:20,000
move on with certain different

00:16:17,600 --> 00:16:21,440
columns and then multiple columns

00:16:20,000 --> 00:16:25,680
eventually we get

00:16:21,440 --> 00:16:27,920
this number 523 from 776

00:16:25,680 --> 00:16:29,199
macbeth's data for this partition

00:16:27,920 --> 00:16:32,320
overall we see

00:16:29,199 --> 00:16:34,839
32 percent of the drop of the data size

00:16:32,320 --> 00:16:37,839
by applying the multiple column

00:16:34,839 --> 00:16:37,839
reordering

00:16:38,240 --> 00:16:41,839
another technique we use is called

00:16:40,480 --> 00:16:45,519
decimal precession

00:16:41,839 --> 00:16:50,000
reduction the idea is

00:16:45,519 --> 00:16:53,279
for the decimal if we can increase

00:16:50,000 --> 00:16:56,880
some least significant bytes then we get

00:16:53,279 --> 00:16:59,120
more zeros now more zeros will make

00:16:56,880 --> 00:17:01,519
their encoding later running encoding

00:16:59,120 --> 00:17:06,319
more efficient

00:17:01,519 --> 00:17:06,319
particularly for the latitude longitude

00:17:06,400 --> 00:17:12,559
the precession like eight seven or six

00:17:10,480 --> 00:17:14,079
this corresponding to zero point zero

00:17:12,559 --> 00:17:16,240
zero one or zero point

00:17:14,079 --> 00:17:17,280
zero one or zero point one meters you

00:17:16,240 --> 00:17:21,360
can imagine

00:17:17,280 --> 00:17:25,120
for the pre-session age 0.001

00:17:21,360 --> 00:17:27,679
is very accurate but the question is

00:17:25,120 --> 00:17:28,880
do we really need that accuracy a lot of

00:17:27,679 --> 00:17:30,799
machine learning jobs

00:17:28,880 --> 00:17:33,120
for instance where to drop off the

00:17:30,799 --> 00:17:35,520
customer where to pick up the customer

00:17:33,120 --> 00:17:37,120
probably is we don't need that accurate

00:17:35,520 --> 00:17:39,840
probably like a meter level

00:17:37,120 --> 00:17:40,880
is sufficient at the most 0.1 meters in

00:17:39,840 --> 00:17:43,919
this case

00:17:40,880 --> 00:17:45,840
precision 6 will be sufficient

00:17:43,919 --> 00:17:47,760
another way to think of is if you use

00:17:45,840 --> 00:17:50,640
the gps data

00:17:47,760 --> 00:17:51,600
so gps itself doesn't provide that

00:17:50,640 --> 00:17:53,679
accuracy

00:17:51,600 --> 00:17:55,440
so it's a waste if you use the high

00:17:53,679 --> 00:17:58,799
accuracy of the

00:17:55,440 --> 00:18:02,000
of the decimal representation

00:17:58,799 --> 00:18:04,559
so we did the experiment

00:18:02,000 --> 00:18:06,080
so we reduced the precision from eight

00:18:04,559 --> 00:18:08,640
all the way to six

00:18:06,080 --> 00:18:09,600
we see the data size with release

00:18:08,640 --> 00:18:13,520
encoding

00:18:09,600 --> 00:18:17,440
dropped from 354 meg to

00:18:13,520 --> 00:18:20,799
what eleven make for four partition

00:18:17,440 --> 00:18:22,559
overall we see 12 percent drops now

00:18:20,799 --> 00:18:24,559
there's another encoding technique

00:18:22,559 --> 00:18:26,960
called better speed encoding

00:18:24,559 --> 00:18:28,880
that way if we apply that we are

00:18:26,960 --> 00:18:30,799
expecting more but in the experiment i

00:18:28,880 --> 00:18:32,160
think we only use the releasing coding

00:18:30,799 --> 00:18:35,200
we talked about that in the

00:18:32,160 --> 00:18:35,200
looking forward steps

00:18:37,440 --> 00:18:42,080
now the techniques is cool but when you

00:18:40,400 --> 00:18:44,720
apply it in the production

00:18:42,080 --> 00:18:45,679
handling of paradise data hundreds of

00:18:44,720 --> 00:18:48,000
paragraphs data

00:18:45,679 --> 00:18:49,360
you will see all different problems now

00:18:48,000 --> 00:18:51,679
i will hand it over to

00:18:49,360 --> 00:18:54,559
my colleague youtube to talk about how

00:18:51,679 --> 00:18:57,840
we relate to the production

00:18:54,559 --> 00:18:57,840
uh sure can you hear me

00:18:58,000 --> 00:19:01,280
uh hello everyone my name is jin chen

00:19:00,080 --> 00:19:04,400
i'm working with cine

00:19:01,280 --> 00:19:06,160
in uber data info team uh so far we have

00:19:04,400 --> 00:19:08,960
talked about this standard

00:19:06,160 --> 00:19:10,320
calm pruning calm reordering and the

00:19:08,960 --> 00:19:13,440
decimal precision

00:19:10,320 --> 00:19:16,880
reduction now let's talk about how to

00:19:13,440 --> 00:19:19,440
execute it in real production

00:19:16,880 --> 00:19:20,880
there are two chains here the first one

00:19:19,440 --> 00:19:23,760
is about like how to

00:19:20,880 --> 00:19:24,160
deal with large-scale data like hundreds

00:19:23,760 --> 00:19:27,679
of

00:19:24,160 --> 00:19:30,720
petabytes the second one is how to

00:19:27,679 --> 00:19:34,640
verify the data safety to avoid any

00:19:30,720 --> 00:19:39,840
uh to avoid any data corruption

00:19:34,640 --> 00:19:39,840
next slide please

00:19:42,880 --> 00:19:48,080
for the first challenge with the lab we

00:19:45,679 --> 00:19:50,640
developed a collection of spark jobs

00:19:48,080 --> 00:19:53,440
and used script to control the kickoff

00:19:50,640 --> 00:19:56,640
and rerun of those back jobs

00:19:53,440 --> 00:19:58,640
the job is execute configure tasks

00:19:56,640 --> 00:19:59,760
like column pruning the standard

00:19:58,640 --> 00:20:02,960
transcompression

00:19:59,760 --> 00:20:05,760
and precision reduction center each

00:20:02,960 --> 00:20:08,240
spark job can handle multiple tables

00:20:05,760 --> 00:20:12,080
and also each spark job can repairable

00:20:08,240 --> 00:20:12,080
can be repairable with checkpoints

00:20:12,320 --> 00:20:15,120
next last please

00:20:16,320 --> 00:20:20,960
for the second challenge we added the

00:20:18,480 --> 00:20:24,080
verification at different levels

00:20:20,960 --> 00:20:25,600
at pakistan level we developed a tool

00:20:24,080 --> 00:20:27,600
called packet div

00:20:25,600 --> 00:20:29,440
it compares the data content and the

00:20:27,600 --> 00:20:32,480
metadata but ignores

00:20:29,440 --> 00:20:34,799
compression and encoding essential

00:20:32,480 --> 00:20:35,919
this is because during the translation

00:20:34,799 --> 00:20:39,679
the compression tab

00:20:35,919 --> 00:20:42,400
is changed but we are not expecting any

00:20:39,679 --> 00:20:43,520
changes in the data itself and same for

00:20:42,400 --> 00:20:45,760
the metadata

00:20:43,520 --> 00:20:47,200
we are not expecting any change in the

00:20:45,760 --> 00:20:50,480
number of rows

00:20:47,200 --> 00:20:53,760
static statistics column index

00:20:50,480 --> 00:20:56,880
offset index room filter essential

00:20:53,760 --> 00:20:59,840
they should not be changed

00:20:56,880 --> 00:21:00,880
the benefit of this tool is it is high

00:20:59,840 --> 00:21:03,520
throughput

00:21:00,880 --> 00:21:04,960
it reads and compares the data directly

00:21:03,520 --> 00:21:07,360
at the page level

00:21:04,960 --> 00:21:08,960
it's like a block operation instead of

00:21:07,360 --> 00:21:11,919
record by record

00:21:08,960 --> 00:21:11,919
so it's very fast

00:21:14,240 --> 00:21:18,960
at a higher level after partition all

00:21:17,280 --> 00:21:21,280
table is translated

00:21:18,960 --> 00:21:22,159
we will verify at the partition or table

00:21:21,280 --> 00:21:25,280
level

00:21:22,159 --> 00:21:28,559
we verify all the file names per account

00:21:25,280 --> 00:21:31,360
file permission it should be exactly

00:21:28,559 --> 00:21:34,159
same as before the translation

00:21:31,360 --> 00:21:36,799
and the common data that are not prone

00:21:34,159 --> 00:21:39,200
need to be same as before

00:21:36,799 --> 00:21:40,720
we also calculate the sum of hash values

00:21:39,200 --> 00:21:43,760
for some columns

00:21:40,720 --> 00:21:48,240
and compares the result before and after

00:21:43,760 --> 00:21:48,240
the hash value should be same as well

00:21:48,720 --> 00:21:51,760
that's all about the execution in

00:21:51,039 --> 00:21:54,159
production and

00:21:51,760 --> 00:21:58,559
verifications now i will hand over back

00:21:54,159 --> 00:22:01,919
to chine

00:21:58,559 --> 00:22:04,559
all right let's have a quick recap uh

00:22:01,919 --> 00:22:06,159
we talked about this standard for better

00:22:04,559 --> 00:22:08,240
compression ratio

00:22:06,159 --> 00:22:09,600
we talked about the head durable column

00:22:08,240 --> 00:22:12,960
pruning to remove

00:22:09,600 --> 00:22:14,559
those unused columns column reordering

00:22:12,960 --> 00:22:16,720
to save the space

00:22:14,559 --> 00:22:19,600
decimal precision reduction for

00:22:16,720 --> 00:22:21,280
randomness encoding to be more efficient

00:22:19,600 --> 00:22:23,280
we talked about how to deploy to

00:22:21,280 --> 00:22:25,520
production with high throughput

00:22:23,280 --> 00:22:28,159
and a careful verification to avoid the

00:22:25,520 --> 00:22:31,360
data corruption

00:22:28,159 --> 00:22:33,200
local forward we want to use data

00:22:31,360 --> 00:22:35,520
include data encoding which

00:22:33,200 --> 00:22:36,320
will be more appropriate for the data

00:22:35,520 --> 00:22:39,280
has a

00:22:36,320 --> 00:22:40,000
larger range value also better speed

00:22:39,280 --> 00:22:42,480
encoding is

00:22:40,000 --> 00:22:43,679
another uh new techniques just check

00:22:42,480 --> 00:22:46,480
into the property

00:22:43,679 --> 00:22:48,159
uh recently uh so this tech this

00:22:46,480 --> 00:22:51,200
encoding is very good as i

00:22:48,159 --> 00:22:53,679
if you have a lot of like zeros

00:22:51,200 --> 00:22:54,720
uh they can do better than the current

00:22:53,679 --> 00:22:57,200
runners encoding

00:22:54,720 --> 00:22:58,080
the idea here is instead of doing like

00:22:57,200 --> 00:23:00,559
one by one

00:22:58,080 --> 00:23:02,640
they can do like a chunk they can put a

00:23:00,559 --> 00:23:04,880
lot of like a most significant bit

00:23:02,640 --> 00:23:05,679
which is most likely all zeros all

00:23:04,880 --> 00:23:08,240
together

00:23:05,679 --> 00:23:11,360
you get more zeros be motivation for the

00:23:08,240 --> 00:23:11,360
future compression

00:23:12,080 --> 00:23:17,679
uh we are hiring a play online or you

00:23:15,120 --> 00:23:22,320
can follow your resume to me directly

00:23:17,679 --> 00:23:26,559
i would like to uh follow your email

00:23:22,320 --> 00:23:26,559
thanks everybody questions

00:23:34,840 --> 00:23:39,679
um

00:23:36,720 --> 00:23:41,840
so the first question is about uh u did

00:23:39,679 --> 00:23:44,320
issue numbers between z standard and

00:23:41,840 --> 00:23:45,600
t z but what does this standard compare

00:23:44,320 --> 00:23:49,039
to snappy

00:23:45,600 --> 00:23:50,000
uh we don't have the number uh the

00:23:49,039 --> 00:23:53,120
reason for that is

00:23:50,000 --> 00:23:56,400
we see more using the gdp in

00:23:53,120 --> 00:23:59,600
our usage than the snappy uh

00:23:56,400 --> 00:24:01,360
but i'm effecting the reduction probably

00:23:59,600 --> 00:24:03,440
similar

00:24:01,360 --> 00:24:04,880
i mean the percentage of the drug

00:24:03,440 --> 00:24:06,960
probably similar

00:24:04,880 --> 00:24:08,799
because usually we see the similar

00:24:06,960 --> 00:24:09,679
between the disease standards maybe in

00:24:08,799 --> 00:24:12,799
terms of

00:24:09,679 --> 00:24:12,799
the comprehension ratio

00:24:15,279 --> 00:24:19,679
second can you put the paper link uh i'm

00:24:18,159 --> 00:24:22,200
not sure this slides

00:24:19,679 --> 00:24:24,480
you know will be published by the

00:24:22,200 --> 00:24:28,640
organization if it is published

00:24:24,480 --> 00:24:32,000
there is a link uh uh the embedded link

00:24:28,640 --> 00:24:35,279
is inside these slides

00:24:32,000 --> 00:24:35,840
you can just um you can just search

00:24:35,279 --> 00:24:39,200
online

00:24:35,840 --> 00:24:42,000
by the title of this paper you should be

00:24:39,200 --> 00:24:44,080
able to get it

00:24:42,000 --> 00:24:46,720
the third question is in addition to the

00:24:44,080 --> 00:24:48,559
slides is a code available for the

00:24:46,720 --> 00:24:50,080
benchmarking with a different existing

00:24:48,559 --> 00:24:53,760
accomplishment level

00:24:50,080 --> 00:24:54,799
um so the bench part so the procreate

00:24:53,760 --> 00:24:58,960
itself

00:24:54,799 --> 00:25:02,559
has some benchmark test suite

00:24:58,960 --> 00:25:05,840
but for the benchmark test that we use

00:25:02,559 --> 00:25:08,400
uh we didn't uh publish it

00:25:05,840 --> 00:25:09,679
uh it is just a spark job uh if you

00:25:08,400 --> 00:25:12,640
wanna write it by yourself

00:25:09,679 --> 00:25:13,279
it's pretty straightforward uh you just

00:25:12,640 --> 00:25:15,520
take the

00:25:13,279 --> 00:25:16,799
your data uh it should be from your

00:25:15,520 --> 00:25:18,159
product you need that because that's

00:25:16,799 --> 00:25:21,600
your target

00:25:18,159 --> 00:25:23,840
now you just do the translation uh

00:25:21,600 --> 00:25:25,600
from the system from the gzip to the

00:25:23,840 --> 00:25:26,240
this standard and then compile the file

00:25:25,600 --> 00:25:29,360
size

00:25:26,240 --> 00:25:29,600
and measure the duration of the reading

00:25:29,360 --> 00:25:32,640
and

00:25:29,600 --> 00:25:37,279
writing and third question

00:25:32,640 --> 00:25:40,400
is uh is procreative open sourced

00:25:37,279 --> 00:25:42,799
no but we do have a plan to

00:25:40,400 --> 00:25:43,600
open sausage we are just getting busy

00:25:42,799 --> 00:25:46,799
keep it up

00:25:43,600 --> 00:25:46,799
so we'll do it later

00:25:48,960 --> 00:25:52,559
another question is is column priming

00:25:51,760 --> 00:25:55,760
open source

00:25:52,559 --> 00:25:56,000
the answer is yes it is open source it

00:25:55,760 --> 00:26:00,240
will

00:25:56,000 --> 00:26:00,240
be released in program 12.

00:26:04,320 --> 00:26:11,840
more questions

00:26:35,840 --> 00:26:53,840
we still have time we can still accept

00:26:39,279 --> 00:26:53,840
more questions if you have

00:26:56,880 --> 00:26:59,919
all right uh if no more questions uh

00:26:59,440 --> 00:27:03,279
thanks

00:26:59,919 --> 00:27:05,279
everybody uh keep in touch i have the

00:27:03,279 --> 00:27:07,039
contact in the slide feel free to

00:27:05,279 --> 00:27:11,520
contact me for any

00:27:07,039 --> 00:27:11,520
further questions have a good day

00:27:18,840 --> 00:27:21,840
everybody

00:27:58,159 --> 00:28:00,240

YouTube URL: https://www.youtube.com/watch?v=PcWaiXHuSt4


