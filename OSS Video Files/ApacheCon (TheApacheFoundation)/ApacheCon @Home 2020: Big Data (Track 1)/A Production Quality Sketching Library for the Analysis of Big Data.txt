Title: A Production Quality Sketching Library for the Analysis of Big Data
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 1)
Description: 
	A Production Quality Sketching Library for the Analysis of Big Data
Lee Rhodes

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

In the analysis of big data there are often problem queries that don’t scale because they require huge compute resources to generate exact results, or don’t parallelize well. Examples include count-distinct, quantiles, most frequent items, joins, matrix computations, and graph analysis. Algorithms that can produce accuracy guaranteed approximate answers for these problem queries are a required toolkit for modern analysis systems that need to process massive amounts of data quickly. For interactive queries there may not be other viable alternatives, and in the case of real­-time streams, these specialized algorithms, called stochastic, streaming, sublinear algorithms, or 'sketches', are the only known solution. This technology has helped Yahoo successfully reduce data processing times from days to hours or minutes on a number of its internal platforms and has enabled subsecond queries on real-time platforms that would have been infeasible without sketches. This talk provides an introduction to sketching and to Apache DataSketches, an open source library in C++, Java and Python of algorithms designed for large production analysis systems.

Lee Rhodes is a Distinguished Architect at Yahoo (now Verizon Media). He created the DataSketches project in 2012 to address analysis problems in Yahoo's large data processing pipelines. DataSketches was Open Sourced in 2015 and is in incubation at Apache Software Foundation. He was an author or coauthor on sketching work published in ICDT, IMC, and JCGS. He obtained his Master's Degree in Electrical Engineering from Stanford University.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:31,760 --> 00:00:36,160
greetings

00:00:32,559 --> 00:00:39,520
um everyone it's uh 9 55 and it's uh

00:00:36,160 --> 00:00:42,719
time to start the session um

00:00:39,520 --> 00:00:45,200
i'm lee rhodes uh i'm with verizon media

00:00:42,719 --> 00:00:46,640
uh am a distinguished architect at

00:00:45,200 --> 00:00:50,320
verizon media

00:00:46,640 --> 00:00:54,000
and i started this project in

00:00:50,320 --> 00:00:56,399
about 2012 i was an architect on one of

00:00:54,000 --> 00:00:59,760
our big data systems at the time

00:00:56,399 --> 00:01:01,680
and noticed that we were spending a huge

00:00:59,760 --> 00:01:06,080
amount of our

00:01:01,680 --> 00:01:08,880
compute resources on solving um

00:01:06,080 --> 00:01:11,280
some problems like counting unique

00:01:08,880 --> 00:01:14,240
identifiers for example

00:01:11,280 --> 00:01:16,400
it was a very difficult problem and uh

00:01:14,240 --> 00:01:19,119
consuming a lot of resources so i i

00:01:16,400 --> 00:01:21,680
started this project

00:01:19,119 --> 00:01:22,560
and was seeking better ways to solve

00:01:21,680 --> 00:01:25,200
this uh

00:01:22,560 --> 00:01:26,720
this fundamental problem but uh turns

00:01:25,200 --> 00:01:27,680
out there are a lot of other similar

00:01:26,720 --> 00:01:30,479
problems that

00:01:27,680 --> 00:01:31,040
we'll be discussing here so what we have

00:01:30,479 --> 00:01:33,840
developed

00:01:31,040 --> 00:01:34,240
we went originally open source in 2015

00:01:33,840 --> 00:01:38,079
and

00:01:34,240 --> 00:01:41,439
joined uh apache uh last year

00:01:38,079 --> 00:01:43,840
and our project is growing and

00:01:41,439 --> 00:01:45,840
we're really excited about it so this is

00:01:43,840 --> 00:01:54,560
a production quality sketching library

00:01:45,840 --> 00:01:56,960
for the analysis of big data

00:01:54,560 --> 00:01:58,640
all right so there's um three major

00:01:56,960 --> 00:02:02,159
topics i'm going to talk about

00:01:58,640 --> 00:02:03,439
um the first is problematic queries of

00:02:02,159 --> 00:02:05,280
big data

00:02:03,439 --> 00:02:07,600
and this is where traditional analysis

00:02:05,280 --> 00:02:10,000
methods don't work well

00:02:07,600 --> 00:02:11,280
the second major topic is approximate

00:02:10,000 --> 00:02:15,120
analysis

00:02:11,280 --> 00:02:18,239
using sketches and how using stochastic

00:02:15,120 --> 00:02:21,520
processes and probabilistic analysis

00:02:18,239 --> 00:02:25,280
wins in a systems architecture context

00:02:21,520 --> 00:02:27,680
and the third topic is i'll briefly

00:02:25,280 --> 00:02:29,280
review the open source apache data

00:02:27,680 --> 00:02:32,080
sketches library

00:02:29,280 --> 00:02:34,560
and this is a unique library dedicated

00:02:32,080 --> 00:02:37,599
to production systems that process

00:02:34,560 --> 00:02:40,160
big data

00:02:37,599 --> 00:02:43,920
so the data analysis challenge this is

00:02:40,160 --> 00:02:45,920
just a template here but

00:02:43,920 --> 00:02:47,599
generally in our data we have billions

00:02:45,920 --> 00:02:51,280
of billions of rows

00:02:47,599 --> 00:02:54,720
or key value pairs or

00:02:51,280 --> 00:02:58,239
billions of vectors in our data and

00:02:54,720 --> 00:02:59,840
we need to still analyze that and

00:02:58,239 --> 00:03:02,400
the big course the big data problem is

00:02:59,840 --> 00:03:04,720
you can't store it all in memory

00:03:02,400 --> 00:03:05,680
even large distributed systems aren't

00:03:04,720 --> 00:03:08,000
aren't big enough

00:03:05,680 --> 00:03:11,120
to contain all the data at once and so

00:03:08,000 --> 00:03:14,239
we need some means of analyzing

00:03:11,120 --> 00:03:14,239
this data quickly

00:03:15,200 --> 00:03:19,519
here's some typical and very common

00:03:18,560 --> 00:03:23,040
queries we

00:03:19,519 --> 00:03:26,000
would like to apply to our big data

00:03:23,040 --> 00:03:26,560
uh unique identifiers is certainly one

00:03:26,000 --> 00:03:28,480
uh

00:03:26,560 --> 00:03:30,000
we want to count uh people or

00:03:28,480 --> 00:03:33,120
identifiers or maybe

00:03:30,000 --> 00:03:36,400
uh devices or uh

00:03:33,120 --> 00:03:37,680
our sources uh of uh or maybe different

00:03:36,400 --> 00:03:40,400
uh browsers or

00:03:37,680 --> 00:03:41,280
or whatever but we're counting unique

00:03:40,400 --> 00:03:44,000
instances

00:03:41,280 --> 00:03:44,959
of these uh identifiers because there

00:03:44,000 --> 00:03:47,840
are many

00:03:44,959 --> 00:03:48,640
duplicates of these identifiers in our

00:03:47,840 --> 00:03:51,680
stream

00:03:48,640 --> 00:03:54,799
uh of data or in the data set

00:03:51,680 --> 00:03:58,239
that we want to analyze but we not only

00:03:54,799 --> 00:04:00,720
would like to count just one stream but

00:03:58,239 --> 00:04:03,040
we would like to be able to apply this

00:04:00,720 --> 00:04:05,760
to multiple different streams

00:04:03,040 --> 00:04:07,280
and do set operations on those streams

00:04:05,760 --> 00:04:10,480
so when we view

00:04:07,280 --> 00:04:11,120
the set of unique identifiers in in set

00:04:10,480 --> 00:04:13,439
a

00:04:11,120 --> 00:04:14,640
we'd like to do a union with b or an

00:04:13,439 --> 00:04:17,919
intersection

00:04:14,640 --> 00:04:21,359
with a c union d and then

00:04:17,919 --> 00:04:23,040
excluding all the identifiers from set e

00:04:21,359 --> 00:04:25,600
for instance in just using set

00:04:23,040 --> 00:04:25,600
expressions

00:04:25,919 --> 00:04:29,440
so that's a very powerful capability and

00:04:27,919 --> 00:04:32,960
analysis

00:04:29,440 --> 00:04:35,280
the second kind of uh queries that are

00:04:32,960 --> 00:04:37,280
very common is quantiles

00:04:35,280 --> 00:04:40,000
where we want to say what is the 95th

00:04:37,280 --> 00:04:40,000
percentile

00:04:40,320 --> 00:04:44,400
of some data stream or what's the median

00:04:42,639 --> 00:04:46,800
what's the 10th percentile

00:04:44,400 --> 00:04:48,000
uh from these quantile numbers we can

00:04:46,800 --> 00:04:50,000
get an idea

00:04:48,000 --> 00:04:52,160
of what the our distribution of data

00:04:50,000 --> 00:04:55,120
really really looks like

00:04:52,160 --> 00:04:56,080
um this is very closely related to the

00:04:55,120 --> 00:04:58,400
next uh

00:04:56,080 --> 00:04:59,680
little icon on the right which is

00:04:58,400 --> 00:05:01,199
histograms and

00:04:59,680 --> 00:05:03,280
probably probably what we call

00:05:01,199 --> 00:05:05,120
probability mass functions

00:05:03,280 --> 00:05:08,160
where we want to see actually the shape

00:05:05,120 --> 00:05:08,160
of the distribution

00:05:08,880 --> 00:05:12,400
in and be able to compute that uh in

00:05:11,680 --> 00:05:15,520
real time

00:05:12,400 --> 00:05:17,039
or certainly as a stream uh the next one

00:05:15,520 --> 00:05:20,320
on the on the lower left

00:05:17,039 --> 00:05:21,199
is frequent items so we may have uh many

00:05:20,320 --> 00:05:23,199
occurrences

00:05:21,199 --> 00:05:26,479
of something but we want to find out

00:05:23,199 --> 00:05:29,759
what what is the most popular

00:05:26,479 --> 00:05:32,160
if you are selling tunes

00:05:29,759 --> 00:05:33,280
on your website we you may want to say

00:05:32,160 --> 00:05:35,360
okay

00:05:33,280 --> 00:05:37,440
within the last hour what are the most

00:05:35,360 --> 00:05:40,479
frequently occurring

00:05:37,440 --> 00:05:42,800
tunes that then you can adjust your

00:05:40,479 --> 00:05:45,520
advertising or adjust your

00:05:42,800 --> 00:05:47,440
priorities based on what's popular or

00:05:45,520 --> 00:05:50,960
most frequent

00:05:47,440 --> 00:05:52,320
for the moment another kind of

00:05:50,960 --> 00:05:56,400
operations that are

00:05:52,320 --> 00:05:58,720
also applicable for approximate analysis

00:05:56,400 --> 00:06:02,400
is vector of matrix operations

00:05:58,720 --> 00:06:05,919
singular value decomposition is also

00:06:02,400 --> 00:06:06,720
a very key process for analyzing our

00:06:05,919 --> 00:06:09,680
data

00:06:06,720 --> 00:06:11,039
graph analysis we all live in networks

00:06:09,680 --> 00:06:13,600
and

00:06:11,039 --> 00:06:16,319
of various types and we want to be able

00:06:13,600 --> 00:06:19,039
to understand the structure

00:06:16,319 --> 00:06:20,240
and density of the networks at various

00:06:19,039 --> 00:06:22,960
times

00:06:20,240 --> 00:06:23,360
another kinds of analysis where we want

00:06:22,960 --> 00:06:25,039
to do

00:06:23,360 --> 00:06:27,280
sampling and either weighted

00:06:25,039 --> 00:06:28,639
probabilistic sampling or uniform

00:06:27,280 --> 00:06:31,759
sampling of our data

00:06:28,639 --> 00:06:32,319
and do that in a very efficient way and

00:06:31,759 --> 00:06:36,000
uh

00:06:32,319 --> 00:06:37,440
in a streaming way now when the data

00:06:36,000 --> 00:06:40,479
gets large

00:06:37,440 --> 00:06:42,800
or your resources are limited

00:06:40,479 --> 00:06:44,160
all of these queries become problematic

00:06:42,800 --> 00:06:46,240
because the aggregations are

00:06:44,160 --> 00:06:48,400
non-additive and that is a particular

00:06:46,240 --> 00:06:50,720
quality of these aggregations

00:06:48,400 --> 00:06:52,560
you can't take some result from a group

00:06:50,720 --> 00:06:55,120
a and result from a group b

00:06:52,560 --> 00:06:57,599
and combine those results in a linear

00:06:55,120 --> 00:06:57,599
fashion

00:06:57,759 --> 00:07:00,800
and similarly you may have some current

00:07:00,160 --> 00:07:03,280
result

00:07:00,800 --> 00:07:04,639
and you want to add no one new item to

00:07:03,280 --> 00:07:07,759
it

00:07:04,639 --> 00:07:11,039
you can't update that that result

00:07:07,759 --> 00:07:13,360
easily and obtain the correct answer

00:07:11,039 --> 00:07:16,000
and this is due to the non-additive

00:07:13,360 --> 00:07:18,960
property of all the queries i discussed

00:07:16,000 --> 00:07:18,960
on the previous page

00:07:20,000 --> 00:07:25,039
so the traditional approach or exact

00:07:23,599 --> 00:07:27,199
analysis methods

00:07:25,039 --> 00:07:28,560
for all these different types of queries

00:07:27,199 --> 00:07:32,080
require

00:07:28,560 --> 00:07:32,479
local copies um and what i mean by local

00:07:32,080 --> 00:07:34,880
is

00:07:32,479 --> 00:07:36,479
uh suppose your big data exists on the

00:07:34,880 --> 00:07:40,080
cloud somewhere

00:07:36,479 --> 00:07:42,080
and uh or even if it's all on disk

00:07:40,080 --> 00:07:44,800
and your query engine wants to be able

00:07:42,080 --> 00:07:46,720
to analyze

00:07:44,800 --> 00:07:48,000
any of these queries whether it's

00:07:46,720 --> 00:07:51,199
quantile analysis

00:07:48,000 --> 00:07:52,639
or it's a most frequent or heavy header

00:07:51,199 --> 00:07:56,479
analysis or

00:07:52,639 --> 00:07:59,520
unique items analysis

00:07:56,479 --> 00:08:01,360
it has to copy all this data into memory

00:07:59,520 --> 00:08:03,599
or into the local system

00:08:01,360 --> 00:08:05,440
for processing and it often requires

00:08:03,599 --> 00:08:06,479
sorting of that data which is very very

00:08:05,440 --> 00:08:09,680
slow

00:08:06,479 --> 00:08:11,360
um this is a very difficult process is

00:08:09,680 --> 00:08:15,280
very

00:08:11,360 --> 00:08:17,280
time consuming now i i want to

00:08:15,280 --> 00:08:18,720
make sure we don't confuse the kind of

00:08:17,280 --> 00:08:20,639
streaming we're talking about here with

00:08:18,720 --> 00:08:23,280
streaming platforms

00:08:20,639 --> 00:08:24,400
um because streaming platforms really

00:08:23,280 --> 00:08:27,599
are mostly

00:08:24,400 --> 00:08:30,479
micro batch systems they

00:08:27,599 --> 00:08:31,440
don't it doesn't solve this fundamental

00:08:30,479 --> 00:08:34,800
problem

00:08:31,440 --> 00:08:39,200
of uh non nonlinear uh

00:08:34,800 --> 00:08:39,200
addition of these various quantities

00:08:39,599 --> 00:08:43,760
parallelization doesn't help much either

00:08:41,760 --> 00:08:45,519
uh you can throw a lot of resources at

00:08:43,760 --> 00:08:47,440
this problem but you still need local

00:08:45,519 --> 00:08:50,720
copies of all your data

00:08:47,440 --> 00:08:52,160
and uh you end up with brute force

00:08:50,720 --> 00:08:54,720
analysis in each of the different

00:08:52,160 --> 00:08:56,720
partitions you set up so you may be

00:08:54,720 --> 00:08:58,720
throwing a thousand times the resources

00:08:56,720 --> 00:09:01,600
at it and getting

00:08:58,720 --> 00:09:05,040
a very very little benefit from it in

00:09:01,600 --> 00:09:05,040
terms of exact analysis

00:09:05,120 --> 00:09:09,920
traditional time time windowing this is

00:09:07,839 --> 00:09:12,080
a very popular

00:09:09,920 --> 00:09:12,959
and common analysis method where you

00:09:12,080 --> 00:09:16,399
want to look at

00:09:12,959 --> 00:09:18,240
a window of time going back days

00:09:16,399 --> 00:09:20,000
or it could be minutes or it could be

00:09:18,240 --> 00:09:23,279
hours

00:09:20,000 --> 00:09:26,240
but you want to say analyze uh in this

00:09:23,279 --> 00:09:26,880
in this picture uh i have a three-day

00:09:26,240 --> 00:09:30,000
window

00:09:26,880 --> 00:09:30,720
and using traditional analysis you end

00:09:30,000 --> 00:09:34,160
up

00:09:30,720 --> 00:09:34,720
touching every data item uh three times

00:09:34,160 --> 00:09:37,519
uh

00:09:34,720 --> 00:09:38,240
or in the case of an uh window of length

00:09:37,519 --> 00:09:40,800
in

00:09:38,240 --> 00:09:41,760
you end up processing your data in times

00:09:40,800 --> 00:09:45,040
which is

00:09:41,760 --> 00:09:46,480
exorbitantly expensive when processing

00:09:45,040 --> 00:09:51,839
these

00:09:46,480 --> 00:09:51,839
data using traditional methods

00:09:52,000 --> 00:09:56,480
so one of the fundamental challenges or

00:09:53,920 --> 00:09:59,440
or fundamental premise i should say

00:09:56,480 --> 00:10:00,160
as that we want to challenge is that our

00:09:59,440 --> 00:10:03,519
results

00:10:00,160 --> 00:10:05,760
must be exact um if we allow for some

00:10:03,519 --> 00:10:07,040
approximation along with some accuracy

00:10:05,760 --> 00:10:09,120
guarantees

00:10:07,040 --> 00:10:10,959
we can achieve orders of magnitude

00:10:09,120 --> 00:10:11,839
improvement in speed and reduction of

00:10:10,959 --> 00:10:14,959
resources

00:10:11,839 --> 00:10:18,640
this is the fundamental premise of

00:10:14,959 --> 00:10:20,480
our technology here

00:10:18,640 --> 00:10:21,839
so i want to introduce the concept of a

00:10:20,480 --> 00:10:25,440
sketch uh

00:10:21,839 --> 00:10:27,600
it's the common name for a set of uh

00:10:25,440 --> 00:10:30,240
technologies known as stochastic

00:10:27,600 --> 00:10:32,959
streaming algorithm algorithms

00:10:30,240 --> 00:10:33,839
and we model the problem as a stochastic

00:10:32,959 --> 00:10:37,920
process

00:10:33,839 --> 00:10:39,200
and analyze uh uh using probability and

00:10:37,920 --> 00:10:42,320
statistics

00:10:39,200 --> 00:10:42,880
and these sketches have roughly four

00:10:42,320 --> 00:10:44,720
major

00:10:42,880 --> 00:10:46,880
components to them it has a stream

00:10:44,720 --> 00:10:47,680
processor at the front end that takes

00:10:46,880 --> 00:10:50,240
the stream of

00:10:47,680 --> 00:10:52,560
of items coming in uh those could be a

00:10:50,240 --> 00:10:55,920
live stream or it could be off of a disc

00:10:52,560 --> 00:10:59,040
or or whatever and it

00:10:55,920 --> 00:11:02,800
uses stochastic and random processes uh

00:10:59,040 --> 00:11:03,839
in the selection or examination of that

00:11:02,800 --> 00:11:06,320
stream

00:11:03,839 --> 00:11:07,760
and in that process then it keeps some

00:11:06,320 --> 00:11:09,839
elements or aspects

00:11:07,760 --> 00:11:13,200
of the data in a data structure very

00:11:09,839 --> 00:11:13,200
specialized data structure

00:11:13,279 --> 00:11:16,880
and then uh this data structure can be

00:11:15,760 --> 00:11:19,120
queried

00:11:16,880 --> 00:11:20,079
uh at any time during the streaming

00:11:19,120 --> 00:11:23,120
process or

00:11:20,079 --> 00:11:26,160
later after the uh sketch has been built

00:11:23,120 --> 00:11:29,200
you can query the uh the sketch

00:11:26,160 --> 00:11:29,680
and obtain uh results and the results

00:11:29,200 --> 00:11:33,040
you get

00:11:29,680 --> 00:11:35,279
of course have some uh probabilistic uh

00:11:33,040 --> 00:11:36,240
distribution of error about it but it's

00:11:35,279 --> 00:11:39,680
generally uh

00:11:36,240 --> 00:11:42,079
pretty small the fourth

00:11:39,680 --> 00:11:42,720
element that is very important in in all

00:11:42,079 --> 00:11:45,120
these

00:11:42,720 --> 00:11:45,920
sketches is what we call mergeability

00:11:45,120 --> 00:11:48,000
where this

00:11:45,920 --> 00:11:49,120
the stream coming in is a stream of

00:11:48,000 --> 00:11:52,720
sketches actually

00:11:49,120 --> 00:11:55,440
uh completed uh sketches that have

00:11:52,720 --> 00:11:57,839
been analyzed before and you're able to

00:11:55,440 --> 00:12:01,120
merge these together in a linear

00:11:57,839 --> 00:12:04,800
fashion and so now the result sketch

00:12:01,120 --> 00:12:08,000
that comes from this process is

00:12:04,800 --> 00:12:12,079
the same as if you had

00:12:08,000 --> 00:12:12,399
merged all the data into one stream and

00:12:12,079 --> 00:12:15,200
put

00:12:12,399 --> 00:12:17,600
into a single sketch so you don't lose

00:12:15,200 --> 00:12:19,839
any accuracy

00:12:17,600 --> 00:12:21,440
particularly for union type merge

00:12:19,839 --> 00:12:25,760
operations

00:12:21,440 --> 00:12:28,880
in this process

00:12:25,760 --> 00:12:29,360
so we want to ask how and why sketches

00:12:28,880 --> 00:12:31,519
achieve

00:12:29,360 --> 00:12:35,040
this superior performance for systems

00:12:31,519 --> 00:12:35,040
processing this massive data

00:12:35,279 --> 00:12:41,360
so the key properties of these sketches

00:12:38,079 --> 00:12:43,440
as they have small stored size

00:12:41,360 --> 00:12:45,680
that is they're small certainly compared

00:12:43,440 --> 00:12:49,200
to the original

00:12:45,680 --> 00:12:51,760
big data that you're looking at and

00:12:49,200 --> 00:12:53,200
by small it varies a lot but small small

00:12:51,760 --> 00:12:55,680
can be from

00:12:53,200 --> 00:12:58,480
hundreds of bytes to kilobytes to

00:12:55,680 --> 00:13:00,399
megabytes but compared to the terabytes

00:12:58,480 --> 00:13:02,320
or petabytes of your big data that's

00:13:00,399 --> 00:13:03,839
still quite small

00:13:02,320 --> 00:13:05,440
the other key property is that they're

00:13:03,839 --> 00:13:07,519
sublinear in space

00:13:05,440 --> 00:13:09,040
and what that means is that as the

00:13:07,519 --> 00:13:11,920
stream size grows

00:13:09,040 --> 00:13:12,959
as shown in this little diagram that the

00:13:11,920 --> 00:13:15,440
sketch size

00:13:12,959 --> 00:13:16,720
uh does not grow even proportionately

00:13:15,440 --> 00:13:19,279
grows

00:13:16,720 --> 00:13:20,160
in a sublinear fashion and sometimes

00:13:19,279 --> 00:13:22,399
even

00:13:20,160 --> 00:13:23,440
uh comes to a limit and doesn't grow

00:13:22,399 --> 00:13:27,040
anymore so it's

00:13:23,440 --> 00:13:28,880
it uh it grows very slowly and small to

00:13:27,040 --> 00:13:31,600
start with

00:13:28,880 --> 00:13:32,959
uh these are by definition this is these

00:13:31,600 --> 00:13:33,760
are streaming algorithms so they're

00:13:32,959 --> 00:13:37,519
single pass

00:13:33,760 --> 00:13:40,720
they touch the data only once and

00:13:37,519 --> 00:13:44,079
and then never look at it again data

00:13:40,720 --> 00:13:46,079
insensitivity is very important

00:13:44,079 --> 00:13:47,120
you don't want to depend on the order of

00:13:46,079 --> 00:13:50,240
the data

00:13:47,120 --> 00:13:52,399
um so that

00:13:50,240 --> 00:13:53,360
you you get basically the same answer no

00:13:52,399 --> 00:13:56,160
matter how the

00:13:53,360 --> 00:13:57,040
the data is presented to the sketch and

00:13:56,160 --> 00:13:59,199
ideally you

00:13:57,040 --> 00:14:00,160
you really don't need to know much about

00:13:59,199 --> 00:14:03,199
the data at all

00:14:00,160 --> 00:14:04,800
with the min the max or or data range

00:14:03,199 --> 00:14:06,639
and that sort of thing

00:14:04,800 --> 00:14:08,720
uh probably one of the most important

00:14:06,639 --> 00:14:10,639
properties of sketches is that they're

00:14:08,720 --> 00:14:14,399
mergeable

00:14:10,639 --> 00:14:17,600
and the mergibility allows us to

00:14:14,399 --> 00:14:19,120
process our data in an embarrassingly

00:14:17,600 --> 00:14:21,199
parallel way

00:14:19,120 --> 00:14:22,560
they are by definition approximate and

00:14:21,199 --> 00:14:24,800
probabilistic

00:14:22,560 --> 00:14:26,480
and one of the keys to the sketches that

00:14:24,800 --> 00:14:28,399
we have in our library is that they're

00:14:26,480 --> 00:14:31,279
all mathematically proven

00:14:28,399 --> 00:14:32,959
uh in terms their error properties now

00:14:31,279 --> 00:14:33,839
you might ask how is it different from

00:14:32,959 --> 00:14:36,720
sampling well

00:14:33,839 --> 00:14:38,480
it's it's similar in some ways uh

00:14:36,720 --> 00:14:41,279
sketches overlap

00:14:38,480 --> 00:14:43,120
with sampling there are some sampling

00:14:41,279 --> 00:14:45,199
algorithms that are also sketching but

00:14:43,120 --> 00:14:48,000
they're also sketching algorithms

00:14:45,199 --> 00:14:49,120
that are not at all sampling and so it

00:14:48,000 --> 00:14:50,880
it

00:14:49,120 --> 00:14:53,920
the determination is based on the

00:14:50,880 --> 00:14:53,920
specific sketch

00:14:54,000 --> 00:14:58,639
the first win of course with sketching

00:14:56,240 --> 00:15:02,000
is that this we have a very small

00:14:58,639 --> 00:15:04,959
uh stored space small amount of memory

00:15:02,000 --> 00:15:06,639
requirements they start small sublinear

00:15:04,959 --> 00:15:08,399
means they stay small

00:15:06,639 --> 00:15:10,399
and the single pass simplifies

00:15:08,399 --> 00:15:12,720
processing so now instead of

00:15:10,399 --> 00:15:13,519
all this big data that you have to store

00:15:12,720 --> 00:15:15,920
into your

00:15:13,519 --> 00:15:19,199
pull into your query engine the sketch

00:15:15,920 --> 00:15:21,920
is orders of magnitude smaller

00:15:19,199 --> 00:15:22,639
and because it is much smaller that

00:15:21,920 --> 00:15:26,310
means you can

00:15:22,639 --> 00:15:27,760
it can process itself much faster

00:15:26,310 --> 00:15:30,880
[Music]

00:15:27,760 --> 00:15:34,880
because the less data it has in it the

00:15:30,880 --> 00:15:37,519
more the faster it can process the data

00:15:34,880 --> 00:15:38,720
the second major win for sketches is

00:15:37,519 --> 00:15:41,279
merge ability

00:15:38,720 --> 00:15:43,440
and this enables parallelism with no

00:15:41,279 --> 00:15:45,680
additional loss of accuracy

00:15:43,440 --> 00:15:47,680
and the sketches transform these

00:15:45,680 --> 00:15:49,199
non-additive metrics into additive

00:15:47,680 --> 00:15:51,279
objects

00:15:49,199 --> 00:15:52,800
and the result of a sketch merge is yet

00:15:51,279 --> 00:15:55,440
another sketch and that

00:15:52,800 --> 00:15:58,000
enables set expressions for selected

00:15:55,440 --> 00:15:58,000
sketches

00:15:59,920 --> 00:16:03,600
the next couple of wins is near

00:16:02,160 --> 00:16:06,880
real-time query speed

00:16:03,600 --> 00:16:09,519
so now if you have

00:16:06,880 --> 00:16:10,399
a back end system that that produces

00:16:09,519 --> 00:16:14,240
your sketches

00:16:10,399 --> 00:16:17,360
say like a hadoop infrastructure

00:16:14,240 --> 00:16:19,360
produces the sketches offline then those

00:16:17,360 --> 00:16:21,600
can be stored into a data mart or

00:16:19,360 --> 00:16:23,279
hypercube database

00:16:21,600 --> 00:16:25,440
where the sketches are small enough to

00:16:23,279 --> 00:16:28,720
be stored as elements in a row

00:16:25,440 --> 00:16:30,880
of a database now the query

00:16:28,720 --> 00:16:33,199
then query process then chooses the

00:16:30,880 --> 00:16:35,600
appropriate rows and dimensions

00:16:33,199 --> 00:16:36,880
that you're interested interested in for

00:16:35,600 --> 00:16:39,199
that query

00:16:36,880 --> 00:16:40,079
then merges them into the result sketch

00:16:39,199 --> 00:16:43,680
and produces

00:16:40,079 --> 00:16:47,199
result and this allows a sub second

00:16:43,680 --> 00:16:52,399
analysis of um of queries

00:16:47,199 --> 00:16:52,399
and achieved near real-time analysis

00:16:54,240 --> 00:16:58,320
the time when doing becomes vastly

00:16:56,399 --> 00:17:01,759
simpler now

00:16:58,320 --> 00:17:05,120
and it includes light data processing so

00:17:01,759 --> 00:17:08,240
the data is only touched once in this uh

00:17:05,120 --> 00:17:10,400
in this diagram and if any data comes in

00:17:08,240 --> 00:17:11,520
late it can be very easily merged into

00:17:10,400 --> 00:17:14,799
the appropriate

00:17:11,520 --> 00:17:18,400
time window in real time and

00:17:14,799 --> 00:17:20,319
then the sketches once uh once the

00:17:18,400 --> 00:17:21,679
day is finished or an hour is finished

00:17:20,319 --> 00:17:23,600
or a minute is finished

00:17:21,679 --> 00:17:25,360
it can then the same sketch can roll

00:17:23,600 --> 00:17:27,600
back to the beginning the window and be

00:17:25,360 --> 00:17:30,000
uh reused for the next uh for the next

00:17:27,600 --> 00:17:30,000
cycle

00:17:31,520 --> 00:17:37,120
uh this is actually a case study uh

00:17:34,720 --> 00:17:38,080
one of our internal systems is called

00:17:37,120 --> 00:17:41,440
flurry

00:17:38,080 --> 00:17:44,160
um and um

00:17:41,440 --> 00:17:45,039
here the the internal architecture of

00:17:44,160 --> 00:17:48,400
this allowed

00:17:45,039 --> 00:17:49,520
real-time analysis and it was achieved

00:17:48,400 --> 00:17:51,760
this way where

00:17:49,520 --> 00:17:53,679
in the top diagram you have a continuous

00:17:51,760 --> 00:17:56,799
stream from our edge web servers

00:17:53,679 --> 00:17:57,919
of data streaming in uh through a storm

00:17:56,799 --> 00:18:01,280
process which

00:17:57,919 --> 00:18:03,360
the storm server basically did this

00:18:01,280 --> 00:18:05,280
partitioning uh into different

00:18:03,360 --> 00:18:08,480
dimensions

00:18:05,280 --> 00:18:11,360
those were fed into a druid uh

00:18:08,480 --> 00:18:13,360
system uh where the sketches were built

00:18:11,360 --> 00:18:14,000
with a 48 hour history and these

00:18:13,360 --> 00:18:17,520
sketches had

00:18:14,000 --> 00:18:20,559
one minute resolution and

00:18:17,520 --> 00:18:23,679
then it was able to update queries

00:18:20,559 --> 00:18:26,240
about uh every 15 seconds so so the the

00:18:23,679 --> 00:18:26,960
freshness of the data was in 15 seconds

00:18:26,240 --> 00:18:30,240
of

00:18:26,960 --> 00:18:33,520
real time and then for

00:18:30,240 --> 00:18:37,039
longer term storage and

00:18:33,520 --> 00:18:40,320
analysis we had hadoop

00:18:37,039 --> 00:18:43,600
with hive and pig in the back back end

00:18:40,320 --> 00:18:46,480
that allow the query process to merge in

00:18:43,600 --> 00:18:47,760
history as old as hours days or weeks or

00:18:46,480 --> 00:18:51,600
months

00:18:47,760 --> 00:18:54,080
so literally the user a interface

00:18:51,600 --> 00:18:54,799
was able to see data that was as fresh

00:18:54,080 --> 00:18:57,919
as

00:18:54,799 --> 00:19:00,480
as a few seconds all the way to hours

00:18:57,919 --> 00:19:00,480
and months

00:19:02,720 --> 00:19:06,880
the next uh major win was the lower

00:19:05,520 --> 00:19:10,000
system cost

00:19:06,880 --> 00:19:12,880
so we had the chance since we did this

00:19:10,000 --> 00:19:13,679
redesign uh in-house uh we had the

00:19:12,880 --> 00:19:16,720
chance to

00:19:13,679 --> 00:19:17,120
analyze the overall system cost in terms

00:19:16,720 --> 00:19:19,280
of

00:19:17,120 --> 00:19:20,720
uh virtual course seconds and so this is

00:19:19,280 --> 00:19:23,760
a reflective of the

00:19:20,720 --> 00:19:24,880
of the actual hardware costs um and

00:19:23,760 --> 00:19:28,480
before sketches

00:19:24,880 --> 00:19:28,960
it was uh we it consumed about 80

00:19:28,480 --> 00:19:31,760
billion

00:19:28,960 --> 00:19:33,679
virtual core seconds per month and this

00:19:31,760 --> 00:19:35,200
is by the way for all the processing not

00:19:33,679 --> 00:19:37,200
just the uh

00:19:35,200 --> 00:19:39,679
analysis that was used by sketching but

00:19:37,200 --> 00:19:41,840
for everything and after sketches it

00:19:39,679 --> 00:19:42,720
reduced the entire system cost down to

00:19:41,840 --> 00:19:44,480
00:19:42,720 --> 00:19:45,919
20 billion virtual core seconds that's

00:19:44,480 --> 00:19:51,200
about one-fourth the

00:19:45,919 --> 00:19:52,960
hardware costs uh and investment

00:19:51,200 --> 00:19:54,960
so it's something something to think

00:19:52,960 --> 00:19:58,240
about uh

00:19:54,960 --> 00:20:01,280
is as an incentive

00:19:58,240 --> 00:20:04,559
so the data sketches library i'm going

00:20:01,280 --> 00:20:04,559
to talk about here for a moment

00:20:05,200 --> 00:20:09,360
our mission is to combine uh deep

00:20:07,600 --> 00:20:11,280
science with exceptional engineering to

00:20:09,360 --> 00:20:13,600
produce production quality sketches that

00:20:11,280 --> 00:20:15,440
can address these difficult queries

00:20:13,600 --> 00:20:17,120
and these can be integrated in many

00:20:15,440 --> 00:20:20,559
different types of

00:20:17,120 --> 00:20:23,200
systems our team

00:20:20,559 --> 00:20:24,880
is a combination of scientists uh

00:20:23,200 --> 00:20:28,000
theoretical and academics

00:20:24,880 --> 00:20:30,000
uh around the world that are very

00:20:28,000 --> 00:20:33,039
interested in this set of problems

00:20:30,000 --> 00:20:35,840
and uh work with the engineers

00:20:33,039 --> 00:20:38,320
to develop this library and you'll

00:20:35,840 --> 00:20:39,760
you'll see that they're

00:20:38,320 --> 00:20:41,919
some of the some of these scientists are

00:20:39,760 --> 00:20:44,799
the leading scientists in

00:20:41,919 --> 00:20:47,360
the theoretical analysis of streaming

00:20:44,799 --> 00:20:47,360
algorithms

00:20:47,760 --> 00:20:51,679
the library uh is divided into a number

00:20:51,039 --> 00:20:54,799
of uh

00:20:51,679 --> 00:20:55,440
different families of sketches we have a

00:20:54,799 --> 00:20:58,480
number of

00:20:55,440 --> 00:20:59,039
four sub-families in cardinality the

00:20:58,480 --> 00:21:01,600
famous

00:20:59,039 --> 00:21:03,760
hyper log log sketch is uh implemented

00:21:01,600 --> 00:21:04,960
with it's an excellent implementation in

00:21:03,760 --> 00:21:07,039
our library

00:21:04,960 --> 00:21:08,080
we've developed some of our own sketches

00:21:07,039 --> 00:21:10,400
the cp sketch

00:21:08,080 --> 00:21:12,640
actually beats the hyperlog log in terms

00:21:10,400 --> 00:21:14,320
of accuracy per stored space

00:21:12,640 --> 00:21:16,080
and we have the very popular theta

00:21:14,320 --> 00:21:20,240
sketches and tuple sketches

00:21:16,080 --> 00:21:20,240
which enable set expressions

00:21:20,799 --> 00:21:26,480
then we have our quantile sketches we

00:21:23,520 --> 00:21:26,480
have three families

00:21:26,559 --> 00:21:31,840
traditional quantiles and a leading

00:21:29,760 --> 00:21:33,039
probably the best known uh best

00:21:31,840 --> 00:21:36,240
performing sketch

00:21:33,039 --> 00:21:39,280
in terms of quantile analysis is kll

00:21:36,240 --> 00:21:40,000
and uh we have a new sketch which i'm

00:21:39,280 --> 00:21:42,400
talking about

00:21:40,000 --> 00:21:44,720
in a minute is a relative error quantile

00:21:42,400 --> 00:21:46,559
sketch to be released soon

00:21:44,720 --> 00:21:49,120
we have frequent items heavy hitter

00:21:46,559 --> 00:21:52,320
sketches

00:21:49,120 --> 00:21:54,240
that are also very popular and the

00:21:52,320 --> 00:21:57,120
frequent directions sketch

00:21:54,240 --> 00:22:00,159
which is very useful in artificial

00:21:57,120 --> 00:22:01,840
intelligence and machine learning

00:22:00,159 --> 00:22:04,159
sampling we have a number of sampling

00:22:01,840 --> 00:22:05,360
sketches reservoir and the var up sketch

00:22:04,159 --> 00:22:08,559
which is developed by

00:22:05,360 --> 00:22:11,120
edith cohen at

00:22:08,559 --> 00:22:12,799
google and we have a number of specialty

00:22:11,120 --> 00:22:16,400
sketches for

00:22:12,799 --> 00:22:19,919
doing customer engagement uh frequent

00:22:16,400 --> 00:22:21,919
distinct tuples hlr maps and so on um

00:22:19,919 --> 00:22:23,200
all of our sketches are developed in

00:22:21,919 --> 00:22:26,720
both java and c

00:22:23,200 --> 00:22:29,360
plus and accessible from python and

00:22:26,720 --> 00:22:31,280
i we have binary compatibility across

00:22:29,360 --> 00:22:34,799
languages and cross systems

00:22:31,280 --> 00:22:37,919
so you can develop a sketch in c

00:22:34,799 --> 00:22:40,320
plus and then also be able to read it

00:22:37,919 --> 00:22:43,840
and merge and do all the operations in

00:22:40,320 --> 00:22:43,840
java as well and back and forth

00:22:45,120 --> 00:22:48,400
so here's a little peek at our new

00:22:47,600 --> 00:22:50,159
relative

00:22:48,400 --> 00:22:53,120
error quantile sketch and this will take

00:22:50,159 --> 00:22:56,080
me a minute to explain

00:22:53,120 --> 00:22:56,400
on the left is the kll float sketch it's

00:22:56,080 --> 00:22:59,840
a

00:22:56,400 --> 00:23:01,840
um probably the uh most powerful

00:22:59,840 --> 00:23:04,559
quantile sketch to date

00:23:01,840 --> 00:23:06,480
um that has been published it was

00:23:04,559 --> 00:23:08,960
published a couple years ago

00:23:06,480 --> 00:23:10,240
and uh what i'm showing here is the

00:23:08,960 --> 00:23:12,880
error analysis

00:23:10,240 --> 00:23:13,360
over the ranks and so if you're looking

00:23:12,880 --> 00:23:15,440
at the

00:23:13,360 --> 00:23:17,840
say the 50th percentile that would be in

00:23:15,440 --> 00:23:20,840
the middle the 90th percentile

00:23:17,840 --> 00:23:22,559
is what we call the 0.9 rank of your

00:23:20,840 --> 00:23:25,440
data and

00:23:22,559 --> 00:23:27,200
uh the errors on the left and you see

00:23:25,440 --> 00:23:29,679
it's pretty flat so

00:23:27,200 --> 00:23:31,600
these represent standard deviations in

00:23:29,679 --> 00:23:33,520
the error distribution

00:23:31,600 --> 00:23:35,679
so between the red curve and the blue

00:23:33,520 --> 00:23:38,400
curve represents

00:23:35,679 --> 00:23:41,039
95 percent confidence and between the

00:23:38,400 --> 00:23:44,320
the brown curve and the purple curve

00:23:41,039 --> 00:23:46,400
represents 99 confidence

00:23:44,320 --> 00:23:48,000
and you can see it's flat over the range

00:23:46,400 --> 00:23:49,520
over the range of ranks so it doesn't

00:23:48,000 --> 00:23:51,120
matter whether you're asking for the

00:23:49,520 --> 00:23:54,400
10th percentile or the

00:23:51,120 --> 00:23:57,039
99th percentile basically the error

00:23:54,400 --> 00:23:58,400
rank error is about the same the new

00:23:57,039 --> 00:24:01,600
sketch though

00:23:58,400 --> 00:24:03,200
solves a a problem that we've heard

00:24:01,600 --> 00:24:06,240
quite a bit about where

00:24:03,200 --> 00:24:07,600
users are saying okay well uh this is

00:24:06,240 --> 00:24:09,279
about one percent

00:24:07,600 --> 00:24:11,600
the one on the left is one percent

00:24:09,279 --> 00:24:13,600
accurate they say well i need a much

00:24:11,600 --> 00:24:14,320
more accuracy but i only care about the

00:24:13,600 --> 00:24:18,480
99th

00:24:14,320 --> 00:24:20,480
or the 99.99 quantiles

00:24:18,480 --> 00:24:22,400
so we've developed this what's called

00:24:20,480 --> 00:24:23,440
the relative error quantile sketch so

00:24:22,400 --> 00:24:25,840
you can

00:24:23,440 --> 00:24:27,840
configure the sketch so at the high end

00:24:25,840 --> 00:24:28,240
you get extremely high accuracy all the

00:24:27,840 --> 00:24:32,320
way

00:24:28,240 --> 00:24:34,799
down to exact so that um uh

00:24:32,320 --> 00:24:36,240
for instance you asked for the 99th for

00:24:34,799 --> 00:24:39,919
instance that i'm sorry

00:24:36,240 --> 00:24:42,400
then the 90th percentile

00:24:39,919 --> 00:24:43,039
and you can see it's a 14 four tenths of

00:24:42,400 --> 00:24:46,720
a percent

00:24:43,039 --> 00:24:49,440
at the 99.9 0.9 it's it's

00:24:46,720 --> 00:24:50,400
uh extremely accurate uh down and you

00:24:49,440 --> 00:24:52,559
can also switch it

00:24:50,400 --> 00:24:54,240
uh switch it for the low end so this is

00:24:52,559 --> 00:24:55,279
for those cases where you really don't

00:24:54,240 --> 00:24:57,679
care about the

00:24:55,279 --> 00:25:02,320
error at the low end of your ranks you

00:24:57,679 --> 00:25:05,840
only care about say the 99 or 99.99

00:25:02,320 --> 00:25:10,159
percentiles this is common in a lot of

00:25:05,840 --> 00:25:10,159
systems metric analysis

00:25:11,120 --> 00:25:14,799
there's a bright future for sketching

00:25:12,720 --> 00:25:15,679
technology and solutions which we show

00:25:14,799 --> 00:25:18,880
here in red

00:25:15,679 --> 00:25:21,760
is areas where we've

00:25:18,880 --> 00:25:23,520
addressed uh sketching already like in

00:25:21,760 --> 00:25:24,400
count distinct frequent items heavy

00:25:23,520 --> 00:25:26,960
hitters

00:25:24,400 --> 00:25:28,000
quantile ranks set operations sampling

00:25:26,960 --> 00:25:31,200
and so on

00:25:28,000 --> 00:25:33,840
but there are other other areas where um

00:25:31,200 --> 00:25:36,000
we're still waiting for some

00:25:33,840 --> 00:25:38,000
contributions

00:25:36,000 --> 00:25:40,559
in connectivity cut specification of

00:25:38,000 --> 00:25:43,520
graphs covariance estimation

00:25:40,559 --> 00:25:45,679
uh low rank approximation sparsification

00:25:43,520 --> 00:25:47,440
clustering k-means k-median

00:25:45,679 --> 00:25:50,240
uh these are all areas that we're

00:25:47,440 --> 00:25:51,679
looking at for the future

00:25:50,240 --> 00:25:53,360
and a lot of these will be very

00:25:51,679 --> 00:25:56,559
interesting for

00:25:53,360 --> 00:26:00,720
machine learning and

00:25:56,559 --> 00:26:04,159
artificial intelligence thank you

00:26:00,720 --> 00:26:05,919
we want to invite open invitation to

00:26:04,159 --> 00:26:08,840
collaboration in our project

00:26:05,919 --> 00:26:11,840
and you can visit us at

00:26:08,840 --> 00:26:11,840
datasketches.apache.org

00:26:13,360 --> 00:26:24,080
okay so i guess i'm open for

00:26:17,919 --> 00:26:27,760
some questions okay

00:26:24,080 --> 00:26:30,240
the distribution of the data affect the

00:26:27,760 --> 00:26:32,240
error on the upper percentiles with that

00:26:30,240 --> 00:26:35,679
new sketch no

00:26:32,240 --> 00:26:37,679
uh by uh all right so

00:26:35,679 --> 00:26:39,280
let me clarify what you mean by

00:26:37,679 --> 00:26:41,279
distribution

00:26:39,280 --> 00:26:44,480
[Music]

00:26:41,279 --> 00:26:45,679
you'll have some distribution um and if

00:26:44,480 --> 00:26:48,720
you're

00:26:45,679 --> 00:26:52,000
looking at the rank distribution

00:26:48,720 --> 00:26:55,520
then certainly with a new sketch

00:26:52,000 --> 00:26:57,279
you're interested in say the uh most

00:26:55,520 --> 00:26:59,679
you want the most accuracy at the high

00:26:57,279 --> 00:27:02,159
end of your rank distribution

00:26:59,679 --> 00:27:04,080
then yeah that will that will be far

00:27:02,159 --> 00:27:04,799
more accurate but in terms of how the

00:27:04,080 --> 00:27:07,200
data is

00:27:04,799 --> 00:27:07,200
is

00:27:08,000 --> 00:27:13,120
actually other than that it doesn't

00:27:11,200 --> 00:27:20,159
really depend on how the errors just

00:27:13,120 --> 00:27:24,080
how the data is distributed um

00:27:20,159 --> 00:27:24,080
okay any any other questions

00:27:27,600 --> 00:27:32,080
oh some other uh some other news were

00:27:30,159 --> 00:27:35,520
developing a docker

00:27:32,080 --> 00:27:38,480
uh container application

00:27:35,520 --> 00:27:40,159
um i'm interested in getting some

00:27:38,480 --> 00:27:42,960
feedback on that

00:27:40,159 --> 00:27:44,240
so this docker could be virtually

00:27:42,960 --> 00:27:48,720
installed anywhere

00:27:44,240 --> 00:27:51,760
and uh you'll be able to um

00:27:48,720 --> 00:27:53,840
use in this any of the sketches now this

00:27:51,760 --> 00:27:56,480
docker implication course

00:27:53,840 --> 00:27:58,559
won't be anywhere near as efficient as

00:27:56,480 --> 00:27:59,520
where this library is integrated into a

00:27:58,559 --> 00:28:03,760
system

00:27:59,520 --> 00:28:06,559
but it makes it very easy to

00:28:03,760 --> 00:28:17,840
do experiments with your data and see

00:28:06,559 --> 00:28:17,840
how useful it would be

00:28:23,600 --> 00:28:28,320
yes there are mathematical proofs if you

00:28:26,720 --> 00:28:32,159
look at the library there's a

00:28:28,320 --> 00:28:35,679
a research tab that you can

00:28:32,159 --> 00:28:38,720
go through and all the different

00:28:35,679 --> 00:28:45,279
research papers that we leverage are

00:28:38,720 --> 00:28:45,870
on that page

00:28:45,279 --> 00:28:48,640
um

00:28:45,870 --> 00:28:50,050
[Music]

00:28:48,640 --> 00:28:53,210
okay um

00:28:50,050 --> 00:28:53,210
[Music]

00:28:56,799 --> 00:29:03,039
uh that we're hoping that the container

00:28:59,919 --> 00:29:06,720
um on the next month or so

00:29:03,039 --> 00:29:10,159
uh we hope to have the container um

00:29:06,720 --> 00:29:14,799
implementation available

00:29:10,159 --> 00:29:14,799
um we're working on that right now

00:29:21,039 --> 00:29:27,600
any other feedback about interest in the

00:29:24,840 --> 00:29:30,720
container

00:29:27,600 --> 00:29:33,520
what data does it uh

00:29:30,720 --> 00:29:34,480
what data format does the library take

00:29:33,520 --> 00:29:36,399
data as

00:29:34,480 --> 00:29:38,080
input uh okay so if you're using the

00:29:36,399 --> 00:29:41,440
library itself

00:29:38,080 --> 00:29:42,159
um either and all right so this is

00:29:41,440 --> 00:29:46,000
complicated

00:29:42,159 --> 00:29:48,799
in in java we have um

00:29:46,000 --> 00:29:50,480
for example our our uh distinct counting

00:29:48,799 --> 00:29:54,480
sketches

00:29:50,480 --> 00:29:57,840
take uh strings uh ends floats

00:29:54,480 --> 00:30:01,039
doubles uh longs all the various

00:29:57,840 --> 00:30:01,039
different primitive types

00:30:01,520 --> 00:30:06,399
because it is establishing it needs to

00:30:03,919 --> 00:30:10,559
establish a uniqueness

00:30:06,399 --> 00:30:10,559
and it then hashes that input

00:30:10,799 --> 00:30:14,080
we also some of the libraries have what

00:30:13,520 --> 00:30:17,840
we call

00:30:14,080 --> 00:30:19,600
their their generic

00:30:17,840 --> 00:30:21,120
in their implementation both in c plus

00:30:19,600 --> 00:30:23,919
plus and in java

00:30:21,120 --> 00:30:24,559
so that you can have abstract objects as

00:30:23,919 --> 00:30:27,039
long as

00:30:24,559 --> 00:30:29,039
you can either produce a hash of it and

00:30:27,039 --> 00:30:31,200
identify using it you

00:30:29,039 --> 00:30:33,520
in terms of quantiles you have the

00:30:31,200 --> 00:30:35,360
ability to compare

00:30:33,520 --> 00:30:39,039
two items together then otherwise we

00:30:35,360 --> 00:30:39,039
don't care what what the object is

00:30:42,159 --> 00:30:45,360
so it can be all all different types in

00:30:44,000 --> 00:30:49,200
terms of data input

00:30:45,360 --> 00:30:52,880
now in the container more than likely

00:30:49,200 --> 00:30:55,919
we'll be standardizing on strings and

00:30:52,880 --> 00:30:57,840
[Music]

00:30:55,919 --> 00:30:59,120
just to make it uh just to make it

00:30:57,840 --> 00:31:03,039
simple

00:30:59,120 --> 00:31:04,559
yes okay uh data format i suggest you go

00:31:03,039 --> 00:31:06,960
and look at our website

00:31:04,559 --> 00:31:10,159
and uh it becomes clearer i think what

00:31:06,960 --> 00:31:10,159
the different data types are

00:31:14,399 --> 00:31:19,840
uh any other questions

00:31:28,480 --> 00:31:35,919
we have a

00:31:33,200 --> 00:31:37,600
we have different uh ways of

00:31:35,919 --> 00:31:39,279
communicating with us we have a slack

00:31:37,600 --> 00:31:43,919
channel we also have

00:31:39,279 --> 00:31:48,080
of course our um apache mailing lists

00:31:43,919 --> 00:31:51,279
please check in with us

00:31:48,080 --> 00:31:53,360
to learn and

00:31:51,279 --> 00:32:01,840
we're eager eager to meet with you and

00:31:53,360 --> 00:32:01,840
talk with you

00:32:13,600 --> 00:32:29,840
i'm not sure when this session was

00:32:15,200 --> 00:32:29,840
supposed to end here

00:32:30,799 --> 00:32:34,159
well going back to i don't know if chris

00:32:32,640 --> 00:32:37,200
is still on

00:32:34,159 --> 00:32:40,799
uh he was asking about

00:32:37,200 --> 00:32:41,600
uh we don't assume any distribution of

00:32:40,799 --> 00:32:43,279
the input

00:32:41,600 --> 00:32:44,960
input data it does not have to be

00:32:43,279 --> 00:32:46,960
normally distributed

00:32:44,960 --> 00:32:48,320
it's just that the properties of the

00:32:46,960 --> 00:32:52,640
sketches are that the

00:32:48,320 --> 00:32:54,480
error pro the properties of error are

00:32:52,640 --> 00:32:56,159
tend to be normally distributed from

00:32:54,480 --> 00:32:57,200
most of the sketches but it has nothing

00:32:56,159 --> 00:33:11,840
to do with the

00:32:57,200 --> 00:33:11,840
distribution of the data itself

00:33:37,760 --> 00:33:39,840
you

00:34:19,200 --> 00:34:23,119
oh dan uh you might

00:34:24,480 --> 00:34:28,000
a number of databases that have adopted

00:34:26,720 --> 00:34:31,839
the library for

00:34:28,000 --> 00:34:35,440
doing query optimization um

00:34:31,839 --> 00:34:40,320
so i um

00:34:35,440 --> 00:34:44,720
yes in fact the query optimization is a

00:34:40,320 --> 00:34:44,720
one of the major uses of this kind of a

00:34:46,839 --> 00:34:49,839
library

00:35:01,680 --> 00:35:05,280
in fact we have one of our database uh

00:35:04,320 --> 00:35:08,560
customers has

00:35:05,280 --> 00:35:10,640
um they use uh all three main

00:35:08,560 --> 00:35:11,760
three of the major families they use um

00:35:10,640 --> 00:35:15,040
unique

00:35:11,760 --> 00:35:18,560
uh you know uh unique analysis

00:35:15,040 --> 00:35:20,160
as well as most frequent occurrences as

00:35:18,560 --> 00:35:22,800
well as quantiles they use

00:35:20,160 --> 00:35:24,400
they take their data and apply all

00:35:22,800 --> 00:35:27,520
sketches

00:35:24,400 --> 00:35:31,839
at the same time uh for

00:35:27,520 --> 00:35:31,839
uh query optimization

00:35:39,839 --> 00:35:47,920
uh samuel i'm not sure i understand

00:35:44,480 --> 00:35:51,359
can i change the theta sketch

00:35:47,920 --> 00:35:53,599
if i have a mapping between old ids and

00:35:51,359 --> 00:35:57,200
new ids

00:35:53,599 --> 00:35:59,839
uh if your id

00:35:57,200 --> 00:35:59,839
changes

00:36:01,520 --> 00:36:05,119
then you're talking it as far as the

00:36:03,040 --> 00:36:05,599
sketches can turn concerned it is a

00:36:05,119 --> 00:36:08,640
different

00:36:05,599 --> 00:36:12,480
unique identity so i

00:36:08,640 --> 00:36:18,480
i don't think that will work

00:36:12,480 --> 00:36:18,480
um if i understand the question

00:36:21,599 --> 00:36:26,160
in other words your data has to have a

00:36:24,320 --> 00:36:29,200
an identity is an identity

00:36:26,160 --> 00:36:30,400
and uh if that changes then it treats it

00:36:29,200 --> 00:36:41,839
as if it were a separate

00:36:30,400 --> 00:36:41,839
unique object

00:36:50,720 --> 00:36:55,200
um samuel i presume you are familiar

00:36:53,520 --> 00:36:57,920
with the library if you're already using

00:36:55,200 --> 00:36:57,920
theta sketches

00:37:06,960 --> 00:37:09,839
excellent

00:37:13,760 --> 00:37:18,960
i'm not familiar with um iceberg but

00:37:21,200 --> 00:37:25,520
yeah i can't help you samuel in terms if

00:37:23,040 --> 00:37:28,320
you have to change the id

00:37:25,520 --> 00:37:29,839
uh you basically have to regenerate your

00:37:28,320 --> 00:37:37,839
sketches

00:37:29,839 --> 00:37:37,839
i'm sorry

00:38:05,680 --> 00:38:10,240
ah i see but i'm not familiar with

00:38:07,760 --> 00:38:10,240
iceberg

00:38:10,960 --> 00:38:15,440
i suggest uh all of you

00:38:17,119 --> 00:38:20,320
come and talk to us uh either on slack

00:38:20,000 --> 00:38:22,480
or

00:38:20,320 --> 00:38:22,480
on

00:38:23,359 --> 00:38:33,839
in our users group users mail list

00:38:49,920 --> 00:38:53,599
okay i think i uh

00:38:56,240 --> 00:38:59,599
i'm running in the next stock so i have

00:38:57,839 --> 00:39:15,839
to leave

00:38:59,599 --> 00:39:15,839
thank you thank you folks

00:39:18,079 --> 00:39:20,160

YouTube URL: https://www.youtube.com/watch?v=nO9pauS-mGQ


