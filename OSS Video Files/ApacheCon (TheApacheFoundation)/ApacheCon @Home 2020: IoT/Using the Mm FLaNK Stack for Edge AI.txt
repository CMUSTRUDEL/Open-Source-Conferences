Title: Using the Mm FLaNK Stack for Edge AI
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020: IoT
Description: 
	Using the Mm FLaNK Stack for Edge AI (Apache MXNet, Apache Flink, Apache NiFi, Apache Kafka, Apache Kudu)
Timothy Spann

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Today, data is being generated from devices and containers living at the edge of networks, clouds and data centers. We need to run business logic, analytics and deep learning at the edge before we start our real-time streaming flows. Fortunately using the all Apache Mm FLaNK stack we can do this with ease! Streaming AI Powered Analytics From the Edge to the Data Center is now a simple use case. With MiNiFi we can ingest the data, do data checks, cleansing, run machine learning and deep learning models and route our data in real-time to Apache NiFi and/or Apache Kafka for further transformations and processing. Apache Flink will provide our advanced streaming capabilities fed real-time via Apache Kafka topics. Apache MXNet models will run both at the edge and in our data centers via Apache NiFi and MiNiFi. Our final data will be stored in Apache Kudu via Apache NiFi for final SQL analytics. We can now solve IoT problems with a scalable all Apache solution that incorporates real-time streaming, analytics and AI. Tools Apache Flink, Apache Kafka, Apache NiFi, MiNiFi, Apache MXNet, Apache Kudu, Apache Impala, Apache HDFS References https://www.datainmotion.dev/2019/08/rapid-iot-development-with-cloudera.html https://www.datainmotion.dev/2019/09/powering-edge-ai-for-sensor-reading.html https://www.datainmotion.dev/2019/05/dataworks-summit-dc-2019-report.html https://www.datainmotion.dev/2019/03/using-raspberry-pi-3b-with-apache-nifi.html

Tim Spann is a Field Engineer at Cloudera in the Data in Motion Team where he works with Apache NiFi, MiniFi, Kafka, Kafka Streams, Edge Flow Manager, MXNet, TensorFlow, Apache Spark, Big Data, IoT, Cloud, Machine Learning, and Deep Learning. Tim has over a decade of experience with the IoT, big data, distributed computing, streaming technologies, and Java programming. Previously, he was a senior solutions architect at AirisData and a senior field engineer at Pivotal. He blogs for DZone, where he is the Big Data Zone leader, and runs a popular meetup in Princeton on big data, IoT, deep learning, streaming, NiFi, blockchain, and Spark. Tim is a frequent speaker at conferences such as IoT Fusion, Strata, ApacheCon, Data Works Summit Berlin, DataWorks Summit Sydney, DataWorks Summit DC, DataWorks Summit Barcelona and Oracle Code NYC. He holds a BS and MS in computer science.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,119 --> 00:00:28,720
so this

00:00:25,680 --> 00:00:31,840
is using the blank stack

00:00:28,720 --> 00:00:34,880
for nji uh flank is

00:00:31,840 --> 00:00:35,680
uh an acronym i came up with that

00:00:34,880 --> 00:00:44,000
combined

00:00:35,680 --> 00:00:47,039
apache flink apache 95

00:00:44,000 --> 00:00:49,760
so we got one minute so wait for that to

00:00:47,039 --> 00:00:49,760
get started

00:00:50,879 --> 00:00:57,120
if you're uh interested in streaming

00:00:54,320 --> 00:00:58,480
or i'm working with edge devices if you

00:00:57,120 --> 00:01:00,879
look behind me

00:00:58,480 --> 00:01:03,120
i've got a number of devices here that

00:01:00,879 --> 00:01:06,720
uh we'll be interacting with

00:01:03,120 --> 00:01:09,360
i've got uh an nvda figure

00:01:06,720 --> 00:01:12,159
we've got a couple of different nanos

00:01:09,360 --> 00:01:15,280
got a raspberry pi with a

00:01:12,159 --> 00:01:18,240
uh pc2 stick on it

00:01:15,280 --> 00:01:18,960
a couple other raspberry pies here some

00:01:18,240 --> 00:01:21,439
with uh

00:01:18,960 --> 00:01:23,280
ai accelerators i've got a coral over

00:01:21,439 --> 00:01:25,680
there

00:01:23,280 --> 00:01:27,119
they're all running trying to get us

00:01:25,680 --> 00:01:29,119
some data in there

00:01:27,119 --> 00:01:31,439
so we could do a little bit of machine

00:01:29,119 --> 00:01:33,360
learning and deep learning at the edge

00:01:31,439 --> 00:01:35,439
and then bringing that data in and we

00:01:33,360 --> 00:01:37,360
could do additional

00:01:35,439 --> 00:01:39,280
deep learning while those events are

00:01:37,360 --> 00:01:41,040
coming through the pipe

00:01:39,280 --> 00:01:43,280
and then do more when they land in

00:01:41,040 --> 00:01:45,680
whatever

00:01:43,280 --> 00:01:47,520
place they land whether that's a cloud

00:01:45,680 --> 00:01:50,320
space cluster or something

00:01:47,520 --> 00:01:50,880
uh on premise in a set of servers or

00:01:50,320 --> 00:01:55,040
even just

00:01:50,880 --> 00:01:56,719
in a laptop uh missing new orleans here

00:01:55,040 --> 00:02:00,079
but at least we've got the uh

00:01:56,719 --> 00:02:02,240
slides here i kept it a very uh

00:02:00,079 --> 00:02:04,320
orleans scene if you haven't seen the

00:02:02,240 --> 00:02:06,240
thing to build your own badges

00:02:04,320 --> 00:02:07,600
the link is out there it's really cool

00:02:06,240 --> 00:02:10,160
you just type in your thing

00:02:07,600 --> 00:02:13,280
build it up i'll probably print one out

00:02:10,160 --> 00:02:13,280
for tomorrow's sessions

00:02:14,480 --> 00:02:20,319
you just missed the first talk that i

00:02:16,560 --> 00:02:22,080
had with john crutch or you were there

00:02:20,319 --> 00:02:24,560
hopefully you were there it was more fun

00:02:22,080 --> 00:02:26,400
with uh john joining me next time i

00:02:24,560 --> 00:02:28,319
should have brought him on this one

00:02:26,400 --> 00:02:30,560
with the rest of my talks i'm doing it

00:02:28,319 --> 00:02:32,959
with a cool presenter

00:02:30,560 --> 00:02:33,760
and the exciting one for today after

00:02:32,959 --> 00:02:36,160
this

00:02:33,760 --> 00:02:38,239
is the real-time stock processing one

00:02:36,160 --> 00:02:41,680
that's with pierre villard

00:02:38,239 --> 00:02:43,760
the knife like emitter and pmc

00:02:41,680 --> 00:02:45,440
so we'll have some really good insights

00:02:43,760 --> 00:02:48,879
there and we'll show

00:02:45,440 --> 00:02:52,319
uh a demo of using knife i don't

00:02:48,879 --> 00:02:53,599
grab in stock feeds do some processing

00:02:52,319 --> 00:02:55,440
with them

00:02:53,599 --> 00:02:57,920
and then do a couple of different event

00:02:55,440 --> 00:02:58,800
streaming frameworks swimming storage do

00:02:57,920 --> 00:03:01,599
some uh

00:02:58,800 --> 00:03:02,800
simple analytics and then tomorrow i

00:03:01,599 --> 00:03:06,159
pretty much have

00:03:02,800 --> 00:03:09,680
almost the same uh lineup

00:03:06,159 --> 00:03:11,360
we uh happy to launch uh post lunch uh

00:03:09,680 --> 00:03:12,720
meetings here i've got a package

00:03:11,360 --> 00:03:15,760
learning 301

00:03:12,720 --> 00:03:16,239
with dr brooks uh we'll be covering some

00:03:15,760 --> 00:03:18,800
general

00:03:16,239 --> 00:03:21,200
data science and then from my side i'll

00:03:18,800 --> 00:03:24,080
show you some data engineering

00:03:21,200 --> 00:03:24,400
and how to run compute learning within

00:03:24,080 --> 00:03:28,080
nine

00:03:24,400 --> 00:03:29,280
five with egl and mxnet and after that

00:03:28,080 --> 00:03:31,519
i've got to talk with

00:03:29,280 --> 00:03:33,599
paul vidal he's doing something very

00:03:31,519 --> 00:03:34,239
similar to what i'm talking about today

00:03:33,599 --> 00:03:37,920
but he's got

00:03:34,239 --> 00:03:40,720
uh interesting uh shifts on that

00:03:37,920 --> 00:03:41,680
and then finally a very interesting iot

00:03:40,720 --> 00:03:44,720
talk

00:03:41,680 --> 00:03:46,400
with uh sunil from texas going over some

00:03:44,720 --> 00:03:49,599
of the things you could do with

00:03:46,400 --> 00:03:51,440
iot using apache projects

00:03:49,599 --> 00:03:54,080
and some different connectors from

00:03:51,440 --> 00:03:54,080
third-party

00:03:55,760 --> 00:04:02,720
so what is flank if you've heard of lamp

00:03:59,519 --> 00:04:06,560
or smack they kind of get the idea

00:04:02,720 --> 00:04:09,519
i i saw a lack of a cool

00:04:06,560 --> 00:04:11,760
acronym for the streaming frameworks

00:04:09,519 --> 00:04:15,040
that i use from apache

00:04:11,760 --> 00:04:18,000
and very often use them together

00:04:15,040 --> 00:04:20,000
because they really flow together nicely

00:04:18,000 --> 00:04:22,639
and i'll show you why i decided

00:04:20,000 --> 00:04:24,560
let me put them together and the names

00:04:22,639 --> 00:04:26,320
kind of fit together nice

00:04:24,560 --> 00:04:27,919
you know we can outline some of those

00:04:26,320 --> 00:04:31,680
other streaming things but uh

00:04:27,919 --> 00:04:33,600
this is funny since it was a k and i

00:04:31,680 --> 00:04:35,840
keep pushing the truth a lot like you

00:04:33,600 --> 00:04:38,080
could do in there but it could also be a

00:04:35,840 --> 00:04:42,240
space but that doesn't sound as good

00:04:38,080 --> 00:04:44,639
or i could just push the hdfs or ozone

00:04:42,240 --> 00:04:46,800
a whole bunch of other data stores but

00:04:44,639 --> 00:04:49,840
having uh the kids kudo and kafka

00:04:46,800 --> 00:04:49,840
together makes it nice

00:04:50,000 --> 00:04:54,840
this is a common flow and this is pretty

00:04:53,280 --> 00:04:59,520
much what we do for the

00:04:54,840 --> 00:05:02,720
aji i'll have a minify engine which is a

00:04:59,520 --> 00:05:06,320
apache sub project for

00:05:02,720 --> 00:05:09,360
uh apache 95 and that one

00:05:06,320 --> 00:05:12,320
is running on a device but it could run

00:05:09,360 --> 00:05:13,280
kubernetes docker you could just run on

00:05:12,320 --> 00:05:14,880
a server

00:05:13,280 --> 00:05:16,880
some people will run this on a sql

00:05:14,880 --> 00:05:20,080
server to grab those logs

00:05:16,880 --> 00:05:23,120
and then you send them to nifi over http

00:05:20,080 --> 00:05:25,520
with a couple of vertical choices there

00:05:23,120 --> 00:05:27,199
i can run from mxnet you don't have to

00:05:25,520 --> 00:05:29,280
run pythords i don't know what their

00:05:27,199 --> 00:05:32,560
license is but i can run that

00:05:29,280 --> 00:05:34,800
and then uh commonly once i clean up the

00:05:32,560 --> 00:05:36,960
data figure out where i want it to go

00:05:34,800 --> 00:05:38,800
i'm going to push a cleaned up version

00:05:36,960 --> 00:05:41,280
of that data data

00:05:38,800 --> 00:05:42,960
to a topic and i'm going to convert that

00:05:41,280 --> 00:05:46,080
usually in avro

00:05:42,960 --> 00:05:48,479
the nice format for data also apache and

00:05:46,080 --> 00:05:50,840
that'll have a schema on it so

00:05:48,479 --> 00:05:52,000
when i'm talking to as many of these

00:05:50,840 --> 00:05:55,120
consumers

00:05:52,000 --> 00:05:55,600
they'll understand what the records look

00:05:55,120 --> 00:05:58,720
like

00:05:55,600 --> 00:06:00,240
what this data looks like makes it uh

00:05:58,720 --> 00:06:02,400
very easy for them

00:06:00,240 --> 00:06:04,240
i'm just checking my other stream to see

00:06:02,400 --> 00:06:06,639
if people have questions

00:06:04,240 --> 00:06:08,479
feel free to uh synthetic or ask

00:06:06,639 --> 00:06:11,280
questions at any time

00:06:08,479 --> 00:06:13,680
to worry about it so once it comes to

00:06:11,280 --> 00:06:16,000
kafka because we have that schema

00:06:13,680 --> 00:06:17,759
link can easily understand what that

00:06:16,000 --> 00:06:20,479
data is

00:06:17,759 --> 00:06:22,800
and then often i'll do some processing

00:06:20,479 --> 00:06:26,160
in flink which could be

00:06:22,800 --> 00:06:28,080
just a sql query it could be a join

00:06:26,160 --> 00:06:29,840
or i think when i'm done whatever

00:06:28,080 --> 00:06:33,039
processing i'm doing i could just

00:06:29,840 --> 00:06:36,160
insert it into another topic and then

00:06:33,039 --> 00:06:38,160
have nine five push it to a data store

00:06:36,160 --> 00:06:41,600
i could have linked stream into a data

00:06:38,160 --> 00:06:43,600
store i could have counter connect to it

00:06:41,600 --> 00:06:44,720
uh there's a lot of flexibility in the

00:06:43,600 --> 00:06:48,160
blank spec

00:06:44,720 --> 00:06:49,120
it really depends on what connectors you

00:06:48,160 --> 00:06:53,039
have

00:06:49,120 --> 00:06:56,240
with data stores you have what paradigm

00:06:53,039 --> 00:06:57,360
the storage is you know what you want to

00:06:56,240 --> 00:06:59,599
check

00:06:57,360 --> 00:07:02,080
they all work together and they have

00:06:59,599 --> 00:07:04,880
some overlapping roles

00:07:02,080 --> 00:07:05,199
scifi is really the best one along with

00:07:04,880 --> 00:07:08,000
its

00:07:05,199 --> 00:07:09,919
little buddy minify you start getting

00:07:08,000 --> 00:07:13,360
that data at the beginning

00:07:09,919 --> 00:07:16,400
find the sources of data bring them in

00:07:13,360 --> 00:07:17,280
validate them know get that source of

00:07:16,400 --> 00:07:19,599
truth going

00:07:17,280 --> 00:07:20,639
know what they are and then start them

00:07:19,599 --> 00:07:22,880
through there

00:07:20,639 --> 00:07:23,840
i'll actually show you part of that is

00:07:22,880 --> 00:07:28,319
while i'm doing this

00:07:23,840 --> 00:07:31,360
i'm pushing uh metadata to apache atlas

00:07:28,319 --> 00:07:32,960
so that i can see a lineage of how i got

00:07:31,360 --> 00:07:34,639
data into the system

00:07:32,960 --> 00:07:36,160
and again i'm pushing to kudu but i

00:07:34,639 --> 00:07:38,400
could just do

00:07:36,160 --> 00:07:39,840
where we saw in the last talk you can

00:07:38,400 --> 00:07:42,319
push it to hive

00:07:39,840 --> 00:07:44,000
and push it through the database

00:07:42,319 --> 00:07:45,759
whatever makes sense

00:07:44,000 --> 00:07:47,360
as part of this and sticking with the

00:07:45,759 --> 00:07:51,120
apache license

00:07:47,360 --> 00:07:54,240
i'm using uh mxnet ddli

00:07:51,120 --> 00:07:57,360
as part of the

00:07:54,240 --> 00:07:59,599
ai data science machine learning deep

00:07:57,360 --> 00:08:02,080
learning whatever you want to call it

00:07:59,599 --> 00:08:03,840
as it comes through the data flows and

00:08:02,080 --> 00:08:06,080
i'll show you that more tomorrow in a

00:08:03,840 --> 00:08:08,479
patchy deep learning 301

00:08:06,080 --> 00:08:09,759
but we'll touch on this and this is also

00:08:08,479 --> 00:08:11,680
in uh

00:08:09,759 --> 00:08:13,440
i did help i'll post out you can look

00:08:11,680 --> 00:08:16,479
through all the examples

00:08:13,440 --> 00:08:18,080
see documentation see uh why this might

00:08:16,479 --> 00:08:19,919
be interesting

00:08:18,080 --> 00:08:21,520
so this makes this nice is i can run

00:08:19,919 --> 00:08:24,400
deep learning on the edge

00:08:21,520 --> 00:08:26,360
i mentioned i've got a xavier and a nano

00:08:24,400 --> 00:08:27,759
those are really great for running

00:08:26,360 --> 00:08:30,000
classification

00:08:27,759 --> 00:08:32,640
for a lot of different models there

00:08:30,000 --> 00:08:34,320
isaiah has three cameras on it

00:08:32,640 --> 00:08:36,399
they're feeding a couple different

00:08:34,320 --> 00:08:38,159
algorithms and then they're sending that

00:08:36,399 --> 00:08:41,839
data through a minified agent

00:08:38,159 --> 00:08:41,839
into nifi

00:08:47,519 --> 00:08:52,640
and then if i want to connect to you

00:08:50,640 --> 00:08:55,120
know some cloud available

00:08:52,640 --> 00:08:55,920
machine learning i could do that i

00:08:55,120 --> 00:08:57,680
connected it

00:08:55,920 --> 00:08:59,920
with clutter machine learning also done

00:08:57,680 --> 00:09:02,160
it with aws and azure

00:08:59,920 --> 00:09:03,839
ibm you know there's a lot of options

00:09:02,160 --> 00:09:06,399
out there

00:09:03,839 --> 00:09:07,279
here's a little diagram of what's

00:09:06,399 --> 00:09:09,839
happening

00:09:07,279 --> 00:09:12,560
so i've got a number of devices on the

00:09:09,839 --> 00:09:15,360
edge one of them has some sensors

00:09:12,560 --> 00:09:17,200
that's that little device behind me

00:09:15,360 --> 00:09:19,279
where you can see that little screen

00:09:17,200 --> 00:09:21,360
that's a nice raspberry pi hat that

00:09:19,279 --> 00:09:24,640
pulls in a bunch of sensors

00:09:21,360 --> 00:09:26,640
i'm getting some sensor readings

00:09:24,640 --> 00:09:28,080
i'm getting some device data because

00:09:26,640 --> 00:09:30,240
when you're doing

00:09:28,080 --> 00:09:31,760
anything with these edge devices you

00:09:30,240 --> 00:09:33,360
want to know their health am i running

00:09:31,760 --> 00:09:35,040
out of disk space

00:09:33,360 --> 00:09:36,640
you know how's my processing doing

00:09:35,040 --> 00:09:37,440
what's the temperature of the device i'm

00:09:36,640 --> 00:09:39,760
going to overheat

00:09:37,440 --> 00:09:40,480
should i shut things down those sort of

00:09:39,760 --> 00:09:44,320
things i get

00:09:40,480 --> 00:09:46,720
all that data format it nicely and json

00:09:44,320 --> 00:09:48,320
and send some strings in through minify

00:09:46,720 --> 00:09:50,800
and deny 5

00:09:48,320 --> 00:09:52,560
and if i'm running in a small private

00:09:50,800 --> 00:09:54,720
cloud instance

00:09:52,560 --> 00:09:56,240
and from there i'm pushing to a number

00:09:54,720 --> 00:09:58,080
of

00:09:56,240 --> 00:09:59,519
data stores that i have there like

00:09:58,080 --> 00:10:02,160
pagespace or polar

00:09:59,519 --> 00:10:04,560
kudu and then from there i'm using a

00:10:02,160 --> 00:10:07,760
schema to make sure that it's

00:10:04,560 --> 00:10:08,560
formatted correctly for uh the topic

00:10:07,760 --> 00:10:11,200
that it's going

00:10:08,560 --> 00:10:12,880
through and those could be read by

00:10:11,200 --> 00:10:15,360
connect apps

00:10:12,880 --> 00:10:16,320
link app can be read by another nifi

00:10:15,360 --> 00:10:18,880
consumer

00:10:16,320 --> 00:10:20,560
especially if i want to appear from an

00:10:18,880 --> 00:10:23,279
i5 that's local or

00:10:20,560 --> 00:10:24,320
at the edge or the gateway and have that

00:10:23,279 --> 00:10:26,640
used in

00:10:24,320 --> 00:10:28,480
a public cloud or just another cluster

00:10:26,640 --> 00:10:30,880
yeah i don't care where it's running

00:10:28,480 --> 00:10:33,839
and then maybe have swing dealing with

00:10:30,880 --> 00:10:35,680
some object stores or kudu

00:10:33,839 --> 00:10:36,959
and then i could do visualization with a

00:10:35,680 --> 00:10:40,480
patch eq or

00:10:36,959 --> 00:10:42,320
some other reporting tool

00:10:40,480 --> 00:10:43,839
pretty standard way you might want to

00:10:42,320 --> 00:10:48,079
get in

00:10:43,839 --> 00:10:48,079
you know machine data from the edge

00:10:48,800 --> 00:10:52,079
fancy arrows there

00:10:52,560 --> 00:10:55,920
now i have links to almost everything

00:10:54,959 --> 00:10:57,519
i'm doing

00:10:55,920 --> 00:11:00,000
and i'll make sure these slides are

00:10:57,519 --> 00:11:02,079
shared out into today

00:11:00,000 --> 00:11:03,519
and that you have all the links if

00:11:02,079 --> 00:11:06,240
you're interested in learning more

00:11:03,519 --> 00:11:07,839
trying things out it's there for you on

00:11:06,240 --> 00:11:10,640
the left i'm just showing you how

00:11:07,839 --> 00:11:12,320
uh the minify flows work so all that's

00:11:10,640 --> 00:11:14,880
running on an edge device

00:11:12,320 --> 00:11:16,640
i'm telling some logs i'm listing a

00:11:14,880 --> 00:11:19,279
directory of images

00:11:16,640 --> 00:11:20,800
sending off a camera setting up another

00:11:19,279 --> 00:11:25,120
camera one of them is doing

00:11:20,800 --> 00:11:27,680
thermal scans and then from knife

00:11:25,120 --> 00:11:29,680
consuming some cops to push it to kudu

00:11:27,680 --> 00:11:32,240
you saw how easy it was

00:11:29,680 --> 00:11:34,079
to pull from a database and put to uh

00:11:32,240 --> 00:11:36,640
another apache data store

00:11:34,079 --> 00:11:38,399
from top to the canoe is just one step

00:11:36,640 --> 00:11:40,800
it's all record-based

00:11:38,399 --> 00:11:42,800
so send me a different type of record

00:11:40,800 --> 00:11:44,640
just tell me what the school is

00:11:42,800 --> 00:11:46,160
and i'll automatically be able to push

00:11:44,640 --> 00:11:47,040
you to the right table in the right

00:11:46,160 --> 00:11:49,680
format

00:11:47,040 --> 00:11:50,399
no change in code no stopping the change

00:11:49,680 --> 00:11:52,959
diversion

00:11:50,399 --> 00:11:53,519
pass me that new version we're good to

00:11:52,959 --> 00:11:55,279
go

00:11:53,519 --> 00:11:56,880
i can work with the old one and the new

00:11:55,279 --> 00:11:58,480
one in different types without having to

00:11:56,880 --> 00:12:00,160
change anything

00:11:58,480 --> 00:12:02,480
very straightforward to do that we'll

00:12:00,160 --> 00:12:04,880
walk through those examples

00:12:02,480 --> 00:12:06,480
so this is a cleaner picture of what's

00:12:04,880 --> 00:12:08,880
right behind me

00:12:06,480 --> 00:12:10,000
you can see some of the devices there so

00:12:08,880 --> 00:12:14,000
there's sensors and

00:12:10,000 --> 00:12:16,079
cameras a bunch of different protocols

00:12:14,000 --> 00:12:18,000
here i'm reading some sensors i'm also

00:12:16,079 --> 00:12:22,880
using some uh

00:12:18,000 --> 00:12:25,600
native uh ai on there from openvino

00:12:22,880 --> 00:12:26,720
this is important so when i have that

00:12:25,600 --> 00:12:29,120
data

00:12:26,720 --> 00:12:30,240
i have a standard thing i like to do i

00:12:29,120 --> 00:12:34,079
like to give it a new

00:12:30,240 --> 00:12:35,680
id for every event and i like to put in

00:12:34,079 --> 00:12:38,320
some hostname

00:12:35,680 --> 00:12:40,079
mac address device information so i know

00:12:38,320 --> 00:12:41,760
which one it came from

00:12:40,079 --> 00:12:43,920
because i don't just have one edge

00:12:41,760 --> 00:12:45,600
device i don't have just one raspberry

00:12:43,920 --> 00:12:48,079
pi

00:12:45,600 --> 00:12:49,519
over the years they keep me to build up

00:12:48,079 --> 00:12:52,720
so there's a lot of them so it's good

00:12:49,519 --> 00:12:54,639
for me to know where the data came from

00:12:52,720 --> 00:12:55,920
and then here i've got sensors and then

00:12:54,639 --> 00:13:00,639
i've got some of that

00:12:55,920 --> 00:13:00,639
device metrics that i mentioned like cpu

00:13:01,040 --> 00:13:04,639
that's important to have so it's

00:13:03,120 --> 00:13:06,160
important to make sure you have some

00:13:04,639 --> 00:13:08,959
time stamps in there

00:13:06,160 --> 00:13:10,880
very important for event processing also

00:13:08,959 --> 00:13:11,839
so you figure out okay are things still

00:13:10,880 --> 00:13:13,760
running

00:13:11,839 --> 00:13:15,839
okay yeah things are still running i can

00:13:13,760 --> 00:13:18,959
see the data coming in

00:13:15,839 --> 00:13:21,839
helpful jason is widely accepted

00:13:18,959 --> 00:13:23,360
very easy to work with and i find and

00:13:21,839 --> 00:13:26,560
it's very simple for me

00:13:23,360 --> 00:13:27,440
to infer a school or i've already built

00:13:26,560 --> 00:13:31,120
this schema

00:13:27,440 --> 00:13:31,120
let me convert it into avro

00:13:31,360 --> 00:13:36,079
send that to a kafka topic and then i

00:13:33,920 --> 00:13:38,240
could have a spring booth consumer

00:13:36,079 --> 00:13:39,600
or i could have a copy of streams one

00:13:38,240 --> 00:13:43,279
top ticket

00:13:39,600 --> 00:13:45,199
nine five spark a lot of apache projects

00:13:43,279 --> 00:13:47,040
to read it a lot of others

00:13:45,199 --> 00:13:48,720
so these are some of the sensors to give

00:13:47,040 --> 00:13:51,760
you an idea

00:13:48,720 --> 00:13:52,399
or if you're running a xavier i just

00:13:51,760 --> 00:13:56,079
took this

00:13:52,399 --> 00:13:59,199
behind me i can run live ones and

00:13:56,079 --> 00:14:01,760
output a whole lot of data but

00:13:59,199 --> 00:14:03,199
not much going on here this is more fun

00:14:01,760 --> 00:14:05,279
when we're in person

00:14:03,199 --> 00:14:07,360
i could i usually set this up on the

00:14:05,279 --> 00:14:08,639
stage some different things going on at

00:14:07,360 --> 00:14:10,639
the conference

00:14:08,639 --> 00:14:12,000
not much going on in this conference

00:14:10,639 --> 00:14:15,279
space which is

00:14:12,000 --> 00:14:17,360
my little lab here and from there

00:14:15,279 --> 00:14:20,240
very easy for me to route that data to

00:14:17,360 --> 00:14:22,880
wherever it needs to go

00:14:20,240 --> 00:14:24,800
let me get past some of this you know

00:14:22,880 --> 00:14:28,000
you'll have these slides

00:14:24,800 --> 00:14:28,000
better off to get into the

00:14:29,680 --> 00:14:32,959
a demo of i get a lot of slides and a

00:14:32,000 --> 00:14:35,279
lot of links

00:14:32,959 --> 00:14:36,079
so you can get into those anytime you

00:14:35,279 --> 00:14:38,079
want

00:14:36,079 --> 00:14:40,399
but let's show you some real things

00:14:38,079 --> 00:14:41,199
going on let's see some real data coming

00:14:40,399 --> 00:14:44,160
in

00:14:41,199 --> 00:14:46,720
hopefully nothing's crashed while we're

00:14:44,160 --> 00:14:49,920
well i've been doing in two sessions

00:14:46,720 --> 00:14:51,680
and i can find the right vendors we have

00:14:49,920 --> 00:14:53,279
a i have a lot of

00:14:51,680 --> 00:14:55,600
a lot of different apps running because

00:14:53,279 --> 00:14:57,519
we've got uh three talks today

00:14:55,600 --> 00:14:59,120
for three tomorrow there's a bunch of

00:14:57,519 --> 00:15:02,639
different apps running

00:14:59,120 --> 00:15:06,079
hopefully i didn't lose my network here

00:15:02,639 --> 00:15:06,079
while i've been dabbing on here

00:15:06,399 --> 00:15:11,120
oh nice nice and slow here everything

00:15:08,399 --> 00:15:14,240
more like okay so i'll show you this one

00:15:11,120 --> 00:15:17,040
this one on grabbing weather data

00:15:14,240 --> 00:15:18,480
is pretty straightforward you know most

00:15:17,040 --> 00:15:21,279
people

00:15:18,480 --> 00:15:22,079
are used to grabbing a rest api what's

00:15:21,279 --> 00:15:25,440
different here

00:15:22,079 --> 00:15:26,639
is i'm grabbing an entire zip file of

00:15:25,440 --> 00:15:29,759
xml

00:15:26,639 --> 00:15:33,519
and then within knife i am unzipping it

00:15:29,759 --> 00:15:35,759
unpacking it uh routing based on what

00:15:33,519 --> 00:15:38,320
that data looks like

00:15:35,759 --> 00:15:38,880
and then doing a real time query to

00:15:38,320 --> 00:15:42,720
convert

00:15:38,880 --> 00:15:43,759
xml into json and then from those events

00:15:42,720 --> 00:15:46,240
i do a

00:15:43,759 --> 00:15:48,000
query on it you know to throw away some

00:15:46,240 --> 00:15:51,519
of the bad data

00:15:48,000 --> 00:15:55,360
and i've got this data that's coming in

00:15:51,519 --> 00:15:58,320
i pull it into my uh consumer here

00:15:55,360 --> 00:15:59,920
i can start the whole thing i'm just

00:15:58,320 --> 00:16:02,000
setting a schema here to say

00:15:59,920 --> 00:16:03,440
which which scheme i should use to look

00:16:02,000 --> 00:16:06,560
up so this is

00:16:03,440 --> 00:16:09,279
weather data so i have a

00:16:06,560 --> 00:16:11,040
weather scheme right here stored in my

00:16:09,279 --> 00:16:13,120
schema registry

00:16:11,040 --> 00:16:14,800
so it tells me you know what are all the

00:16:13,120 --> 00:16:18,639
fields what are the types

00:16:14,800 --> 00:16:22,320
they support no that sort of thing

00:16:18,639 --> 00:16:24,800
and then what i want to do is if those

00:16:22,320 --> 00:16:28,079
records that are coming in

00:16:24,800 --> 00:16:30,160
are not valid against the schema

00:16:28,079 --> 00:16:33,040
and here there's a couple of options i

00:16:30,160 --> 00:16:35,360
said don't make the type too strict

00:16:33,040 --> 00:16:37,519
you know some extra fields don't blow up

00:16:35,360 --> 00:16:39,199
it's not going to blow up 95.

00:16:37,519 --> 00:16:40,880
so if it didn't fall what i'm sending in

00:16:39,199 --> 00:16:42,160
here maybe i want to put that in the

00:16:40,880 --> 00:16:44,160
storage

00:16:42,160 --> 00:16:45,360
maybe i want to uh send a message to

00:16:44,160 --> 00:16:47,040
someone

00:16:45,360 --> 00:16:48,320
i i know this data is coming from the

00:16:47,040 --> 00:16:49,279
government and not what they can do

00:16:48,320 --> 00:16:52,079
about it

00:16:49,279 --> 00:16:54,079
i'm just going to not use those and here

00:16:52,079 --> 00:16:57,040
i'm going to push to my

00:16:54,079 --> 00:16:59,120
topic this is a parameter this is

00:16:57,040 --> 00:17:01,360
extracted from 95

00:16:59,120 --> 00:17:02,240
so if i want to move to production with

00:17:01,360 --> 00:17:05,919
the devops

00:17:02,240 --> 00:17:08,720
process that's separate from my code

00:17:05,919 --> 00:17:10,959
the code and configuration get combined

00:17:08,720 --> 00:17:13,600
and i push it to another server

00:17:10,959 --> 00:17:14,799
i posted a link to an article how to do

00:17:13,600 --> 00:17:17,520
that using either the

00:17:14,799 --> 00:17:18,160
knife like cleaning or the rest api

00:17:17,520 --> 00:17:21,760
everything in

00:17:18,160 --> 00:17:23,679
ifi is a rest api if you turn on your

00:17:21,760 --> 00:17:25,039
developer console you see all these rest

00:17:23,679 --> 00:17:26,959
calls happening

00:17:25,039 --> 00:17:28,160
it's a pretty good way to learn if you

00:17:26,959 --> 00:17:30,799
want to do uh

00:17:28,160 --> 00:17:33,679
knife programming be arrested it's

00:17:30,799 --> 00:17:35,520
really straightforward

00:17:33,679 --> 00:17:36,880
so what we're doing here is taking the

00:17:35,520 --> 00:17:40,080
weather

00:17:36,880 --> 00:17:43,760
and pushing this out as

00:17:40,080 --> 00:17:45,679
avro so we have avro with a schema

00:17:43,760 --> 00:17:48,720
and i'm going to read that in another

00:17:45,679 --> 00:17:52,720
server hopefully this one is up

00:17:48,720 --> 00:17:56,240
this one is up so i i shifted it from

00:17:52,720 --> 00:18:00,400
my laptop here in my little office

00:17:56,240 --> 00:18:04,160
and i'm reading that here in

00:18:00,400 --> 00:18:07,360
the uh amazon cluster pulling in from uh

00:18:04,160 --> 00:18:10,480
kafka broker for that weather

00:18:07,360 --> 00:18:14,480
i get avro in i'm gonna push jason out

00:18:10,480 --> 00:18:18,160
why not and then i'm gonna push that

00:18:14,480 --> 00:18:20,160
into a crew to the table with an upshirt

00:18:18,160 --> 00:18:21,840
again i don't have to know the fields i

00:18:20,160 --> 00:18:23,760
don't have to write any sequel

00:18:21,840 --> 00:18:25,760
i don't even really need to know that

00:18:23,760 --> 00:18:29,120
much i could switch it to a regular

00:18:25,760 --> 00:18:31,600
jbc uh just by doing that

00:18:29,120 --> 00:18:32,880
or i can decide to put it somewhere else

00:18:31,600 --> 00:18:33,679
i don't want to tell you to put it

00:18:32,880 --> 00:18:36,559
somewhere that's

00:18:33,679 --> 00:18:37,760
not in a patchy place but i can store it

00:18:36,559 --> 00:18:41,280
in a lot of different

00:18:37,760 --> 00:18:46,240
uh places like high

00:18:41,280 --> 00:18:50,799
like uh hdfs

00:18:46,240 --> 00:18:53,679
you know any of the amazon data stores

00:18:50,799 --> 00:18:56,559
azure yeah you kind of get the idea

00:18:53,679 --> 00:18:58,480
there's a lot of places i can put data

00:18:56,559 --> 00:19:01,840
but uh cuda's a nice place to do that

00:18:58,480 --> 00:19:03,840
and linked with the client staff

00:19:01,840 --> 00:19:05,520
one thing that's very cool about nifly

00:19:03,840 --> 00:19:07,600
we didn't mention is the same data

00:19:05,520 --> 00:19:11,280
providence so i could see

00:19:07,600 --> 00:19:14,000
as that data is coming in what happened

00:19:11,280 --> 00:19:14,720
and i get detailed information what was

00:19:14,000 --> 00:19:18,559
the topic

00:19:14,720 --> 00:19:22,000
offset what was the time stamp topic

00:19:18,559 --> 00:19:24,160
you know which team i used i had 800

00:19:22,000 --> 00:19:24,960
records that processed in this one batch

00:19:24,160 --> 00:19:27,120
here

00:19:24,960 --> 00:19:29,919
i could even look at the data before and

00:19:27,120 --> 00:19:32,240
after a couple things supposed to get a

00:19:29,919 --> 00:19:34,240
retry here in case it fails

00:19:32,240 --> 00:19:35,440
i've decided in code what i should do

00:19:34,240 --> 00:19:38,000
with that

00:19:35,440 --> 00:19:38,720
and then when i come in here i'm just

00:19:38,000 --> 00:19:42,160
going to

00:19:38,720 --> 00:19:43,520
uh parse the data so i could send it to

00:19:42,160 --> 00:19:45,919
a slack channel

00:19:43,520 --> 00:19:46,880
pretty straightforward let's see if we

00:19:45,919 --> 00:19:50,480
can get to those

00:19:46,880 --> 00:19:55,120
other ones okay this one's helpful

00:19:50,480 --> 00:19:58,480
so what i do with data

00:19:55,120 --> 00:19:59,600
you'll uh i have part of my data is

00:19:58,480 --> 00:20:02,799
images

00:19:59,600 --> 00:20:04,320
as we mentioned before so here i've got

00:20:02,799 --> 00:20:07,280
let me show you how this works

00:20:04,320 --> 00:20:07,760
so i've got that minify agent and i've

00:20:07,280 --> 00:20:10,640
got

00:20:07,760 --> 00:20:11,679
uh one list there here this is the easy

00:20:10,640 --> 00:20:14,720
way to do it

00:20:11,679 --> 00:20:18,240
so as stuff to push in

00:20:14,720 --> 00:20:20,559
from any of these engines to nifi

00:20:18,240 --> 00:20:22,400
i'm just gonna have it come in one place

00:20:20,559 --> 00:20:24,559
and then looking at the user agent i'll

00:20:22,400 --> 00:20:26,320
decide where i want to send it

00:20:24,559 --> 00:20:27,840
so this is pushing in from a bunch of

00:20:26,320 --> 00:20:30,080
different devices

00:20:27,840 --> 00:20:31,760
i see one and it's just doing that auto

00:20:30,080 --> 00:20:34,400
routing for me

00:20:31,760 --> 00:20:35,120
pretty easy to do but again remember

00:20:34,400 --> 00:20:37,600
when i said i

00:20:35,120 --> 00:20:39,520
care about you know what what that

00:20:37,600 --> 00:20:41,760
information is so when i'm

00:20:39,520 --> 00:20:44,320
processing a device i want to know what

00:20:41,760 --> 00:20:47,520
the ip is with the mac address

00:20:44,320 --> 00:20:48,720
and when that data comes in i don't know

00:20:47,520 --> 00:20:51,919
what it is

00:20:48,720 --> 00:20:53,760
again here doing some processing

00:20:51,919 --> 00:20:55,039
looking at the data decide what i want

00:20:53,760 --> 00:20:57,120
to do with it

00:20:55,039 --> 00:20:58,400
uh there's some data i have here that's

00:20:57,120 --> 00:21:00,240
standard out

00:20:58,400 --> 00:21:02,000
it's good for me pretty plugging so i'm

00:21:00,240 --> 00:21:04,240
keeping it but

00:21:02,000 --> 00:21:05,520
if you look at the data it's not very

00:21:04,240 --> 00:21:07,280
useful

00:21:05,520 --> 00:21:08,880
but uh the kind of data you get when

00:21:07,280 --> 00:21:11,440
you're running

00:21:08,880 --> 00:21:12,960
these uh deep learning processes uh in

00:21:11,440 --> 00:21:15,280
the nvidia on the

00:21:12,960 --> 00:21:16,720
standard out there's a ton of

00:21:15,280 --> 00:21:19,919
information there if i was trying to

00:21:16,720 --> 00:21:23,360
debug why a model was giving me a result

00:21:19,919 --> 00:21:24,960
i can uh look at that closely again or i

00:21:23,360 --> 00:21:26,320
could parse it with something like say

00:21:24,960 --> 00:21:28,000
apache

00:21:26,320 --> 00:21:29,440
or tesseract and figure out what i want

00:21:28,000 --> 00:21:32,000
to do with it

00:21:29,440 --> 00:21:33,600
another thing i'm getting is images from

00:21:32,000 --> 00:21:34,799
those cameras i have i have three

00:21:33,600 --> 00:21:36,960
cameras

00:21:34,799 --> 00:21:38,960
on that device and i'm pulling in a

00:21:36,960 --> 00:21:41,760
bunch of them some of them raw

00:21:38,960 --> 00:21:43,679
some of them processed by the framework

00:21:41,760 --> 00:21:46,000
and then i just send them

00:21:43,679 --> 00:21:47,440
to uh another bit of knife to process

00:21:46,000 --> 00:21:50,559
them

00:21:47,440 --> 00:21:54,320
over here i'm doing the similar thing

00:21:50,559 --> 00:21:56,720
we've got data coming from that device

00:21:54,320 --> 00:21:57,600
it's jason i didn't build a schema for

00:21:56,720 --> 00:21:59,919
this

00:21:57,600 --> 00:22:01,520
i don't really need to as you see here i

00:21:59,919 --> 00:22:03,039
got a bunch of records waiting to get

00:22:01,520 --> 00:22:05,280
pushed to kafka

00:22:03,039 --> 00:22:06,240
we've got these configurable cues and

00:22:05,280 --> 00:22:08,400
iphone

00:22:06,240 --> 00:22:09,919
but when i'm ready to do it i'll just

00:22:08,400 --> 00:22:10,640
turn it on i'm not going to lose any

00:22:09,919 --> 00:22:13,679
data

00:22:10,640 --> 00:22:16,080
if my cube fills up it could be

00:22:13,679 --> 00:22:18,240
i could turn it on so it'll auto resize

00:22:16,080 --> 00:22:20,480
based on some machine learning

00:22:18,240 --> 00:22:22,320
or i could just make it larger or i

00:22:20,480 --> 00:22:23,280
could just decide to turn on what i'm

00:22:22,320 --> 00:22:25,520
doing

00:22:23,280 --> 00:22:28,480
so this will look familiar i get that

00:22:25,520 --> 00:22:30,480
unique idea i like to put in there

00:22:28,480 --> 00:22:31,679
this one i also put the name of the

00:22:30,480 --> 00:22:33,360
camera i'm using

00:22:31,679 --> 00:22:36,320
since i have three different cameras i

00:22:33,360 --> 00:22:38,000
want to know which image it was

00:22:36,320 --> 00:22:39,520
uh the xavier supports a lot of

00:22:38,000 --> 00:22:42,400
concurrent cameras

00:22:39,520 --> 00:22:44,240
so if you had this as a security system

00:22:42,400 --> 00:22:44,559
or as something in an office or whatever

00:22:44,240 --> 00:22:46,480
you're

00:22:44,559 --> 00:22:49,120
using your camera for such important

00:22:46,480 --> 00:22:52,320
information

00:22:49,120 --> 00:22:53,360
got my ip in there got how much time it

00:22:52,320 --> 00:22:56,320
took

00:22:53,360 --> 00:22:58,400
you know percentage of match you know

00:22:56,320 --> 00:23:00,880
that sort of thing there's a couple

00:22:58,400 --> 00:23:02,559
different types of data i get in there

00:23:00,880 --> 00:23:04,799
and i'm just going to push that to

00:23:02,559 --> 00:23:07,120
confidence why not

00:23:04,799 --> 00:23:08,960
and then over here like i mentioned i

00:23:07,120 --> 00:23:12,240
have that

00:23:08,960 --> 00:23:13,120
there's images coming in i keep a local

00:23:12,240 --> 00:23:15,919
copy of them

00:23:13,120 --> 00:23:17,120
i like to have images i'm also doing

00:23:15,919 --> 00:23:20,880
something which is

00:23:17,120 --> 00:23:25,360
sending in i can upload

00:23:20,880 --> 00:23:27,120
those images to uh a slack channel

00:23:25,360 --> 00:23:28,880
so i'm just gonna change this one and

00:23:27,120 --> 00:23:31,520
put that in the

00:23:28,880 --> 00:23:33,919
apache john i have my own slack so i'm

00:23:31,520 --> 00:23:35,520
not spamming you there

00:23:33,919 --> 00:23:36,960
with people you're wondering where these

00:23:35,520 --> 00:23:40,159
weird images coming from

00:23:36,960 --> 00:23:42,480
i'm also sending a message here this is

00:23:40,159 --> 00:23:45,039
a different probability and some of the

00:23:42,480 --> 00:23:48,320
things i want to display in a message

00:23:45,039 --> 00:23:51,919
i'll start this this is a cgl

00:23:48,320 --> 00:23:54,480
processor using mxnet hdmi

00:23:51,919 --> 00:23:56,640
do it analytics on these images while it

00:23:54,480 --> 00:23:59,200
comes through nine five

00:23:56,640 --> 00:24:01,120
and then you get pushed to some of them

00:23:59,200 --> 00:24:04,159
were pushed to that old one

00:24:01,120 --> 00:24:07,039
uh i didn't switch that one and then the

00:24:04,159 --> 00:24:19,840
images should start showing up over here

00:24:07,039 --> 00:24:24,159
to see who it figured out

00:24:19,840 --> 00:24:26,080
so yeah so it's starting to up already

00:24:24,159 --> 00:24:28,960
and while it's sending in that other

00:24:26,080 --> 00:24:31,120
data system that comes to data

00:24:28,960 --> 00:24:33,360
pushed in those results as you see it

00:24:31,120 --> 00:24:36,000
sticking those images

00:24:33,360 --> 00:24:38,159
it's uh posting the results of them i

00:24:36,000 --> 00:24:39,120
turn down how many i send the slack at a

00:24:38,159 --> 00:24:42,480
time

00:24:39,120 --> 00:24:44,640
otherwise it gets a little crazy

00:24:42,480 --> 00:24:46,159
you see here it found the tie which is

00:24:44,640 --> 00:24:49,360
pretty cool

00:24:46,159 --> 00:24:51,279
why i put a tie in that so we can see

00:24:49,360 --> 00:24:53,039
the results down here

00:24:51,279 --> 00:24:55,520
this is the probability of what i

00:24:53,039 --> 00:24:56,320
thought it was it uh whatever that one

00:24:55,520 --> 00:24:57,919
is it got my

00:24:56,320 --> 00:24:59,520
chair in the picture which is pretty

00:24:57,919 --> 00:25:02,559
cool

00:24:59,520 --> 00:25:03,679
so we're running that we get those

00:25:02,559 --> 00:25:06,320
results

00:25:03,679 --> 00:25:07,520
this gives you an idea i push some of

00:25:06,320 --> 00:25:10,720
the data

00:25:07,520 --> 00:25:12,320
to copter so let's take a look make sure

00:25:10,720 --> 00:25:14,799
we're getting that data

00:25:12,320 --> 00:25:16,080
i want to make sure getting data coming

00:25:14,799 --> 00:25:17,840
in

00:25:16,080 --> 00:25:20,000
we have a lot of different sources of

00:25:17,840 --> 00:25:21,440
data uh later we're going to talk about

00:25:20,000 --> 00:25:24,559
the stock data

00:25:21,440 --> 00:25:26,320
uh one of the sources with weather uh

00:25:24,559 --> 00:25:29,679
we'll get a lot of options here

00:25:26,320 --> 00:25:31,840
a lot of different topics uh which one

00:25:29,679 --> 00:25:36,080
is important

00:25:31,840 --> 00:25:38,640
you know we have many sources of data

00:25:36,080 --> 00:25:40,840
you know whichever one makes sense here

00:25:38,640 --> 00:25:43,840
uh let me push some more of that weather

00:25:40,840 --> 00:25:47,200
data

00:25:43,840 --> 00:25:48,159
that is a useful one to have uh that

00:25:47,200 --> 00:25:50,320
weather data

00:25:48,159 --> 00:25:51,440
is nice because it comes from all over

00:25:50,320 --> 00:25:53,360
the country

00:25:51,440 --> 00:25:54,720
and then when i get that i'm pushing

00:25:53,360 --> 00:25:57,520
that to kudu

00:25:54,720 --> 00:25:58,159
so i could push that in some graphs so i

00:25:57,520 --> 00:26:00,640
could see

00:25:58,159 --> 00:26:03,600
all the different results from different

00:26:00,640 --> 00:26:06,559
uh airports around the country

00:26:03,600 --> 00:26:07,919
interesting information so let's look

00:26:06,559 --> 00:26:10,880
back

00:26:07,919 --> 00:26:13,840
at our constant topic he's keeping my

00:26:10,880 --> 00:26:15,679
eye out for questions there

00:26:13,840 --> 00:26:17,520
so we've got scott i'm pushing it to a

00:26:15,679 --> 00:26:17,840
lot of different places let's look at

00:26:17,520 --> 00:26:20,720
the

00:26:17,840 --> 00:26:22,000
weather again something i mentioned with

00:26:20,720 --> 00:26:25,919
this

00:26:22,000 --> 00:26:28,559
is that i'm doing this in avro

00:26:25,919 --> 00:26:30,400
so i can connect that with the schema

00:26:28,559 --> 00:26:31,039
again it's nice to use the schema so i

00:26:30,400 --> 00:26:33,440
can use that

00:26:31,039 --> 00:26:34,480
everywhere so that's the weather data

00:26:33,440 --> 00:26:38,320
coming in

00:26:34,480 --> 00:26:41,279
i have that in kafka and then from kafka

00:26:38,320 --> 00:26:42,640
i'm going to consume that in nine five

00:26:41,279 --> 00:26:45,679
push that to a true

00:26:42,640 --> 00:26:50,960
table where i can read that

00:26:45,679 --> 00:26:53,279
from now let's do an order here

00:26:50,960 --> 00:26:56,159
observation time descending we'll just

00:26:53,279 --> 00:26:59,200
see uh whatever is the newest stuff

00:26:56,159 --> 00:27:00,159
uh that's coming out of uh using

00:26:59,200 --> 00:27:02,960
impala's

00:27:00,159 --> 00:27:03,360
interface there and we can see different

00:27:02,960 --> 00:27:06,480
uh

00:27:03,360 --> 00:27:07,200
weather information i could take a look

00:27:06,480 --> 00:27:10,000
at the

00:27:07,200 --> 00:27:11,679
columns that are more interesting for me

00:27:10,000 --> 00:27:16,480
i'm concerned uh

00:27:11,679 --> 00:27:19,520
location observation time

00:27:16,480 --> 00:27:21,919
yeah don't use reserved words otherwise

00:27:19,520 --> 00:27:24,559
you gotta use the

00:27:21,919 --> 00:27:26,080
things to hide it and then i could just

00:27:24,559 --> 00:27:27,600
do things like maybe temperature

00:27:26,080 --> 00:27:30,320
strength

00:27:27,600 --> 00:27:31,919
a couple of uh values there that are

00:27:30,320 --> 00:27:34,240
interesting

00:27:31,919 --> 00:27:35,360
so you see different temperatures across

00:27:34,240 --> 00:27:38,640
the country

00:27:35,360 --> 00:27:40,399
you know when i did this read in uh

00:27:38,640 --> 00:27:42,480
fahrenheit and celsius whichever one

00:27:40,399 --> 00:27:44,240
you're happy with

00:27:42,480 --> 00:27:46,159
and then i just use that same date it

00:27:44,240 --> 00:27:49,120
also has last long

00:27:46,159 --> 00:27:50,480
so i could push that to uh mature and

00:27:49,120 --> 00:27:54,240
i'm grabbing that

00:27:50,480 --> 00:27:57,600
we were supposed to be down here

00:27:54,240 --> 00:28:00,720
and it's pretty hot down there

00:27:57,600 --> 00:28:04,320
so as much as i missed it you know

00:28:00,720 --> 00:28:04,640
the temperature in uh new orleans here

00:28:04,320 --> 00:28:06,799
is

00:28:04,640 --> 00:28:06,799
uh

00:28:08,480 --> 00:28:11,440
it must have been earlier

00:28:12,320 --> 00:28:15,760
but it depends on what time we did that

00:28:14,240 --> 00:28:18,240
reading

00:28:15,760 --> 00:28:20,880
so let's take a look i want to see if um

00:28:18,240 --> 00:28:22,880
you guys saying

00:28:20,880 --> 00:28:24,720
what do we have here question how could

00:28:22,880 --> 00:28:26,559
you call a deep learning python script

00:28:24,720 --> 00:28:28,720
that needs to run on a separate server

00:28:26,559 --> 00:28:31,760
receiving the results from that

00:28:28,720 --> 00:28:35,440
denied ifi that is

00:28:31,760 --> 00:28:38,640
pretty easy now we have a couple options

00:28:35,440 --> 00:28:41,279
set right to a file

00:28:38,640 --> 00:28:42,720
which i do that on some of the devices

00:28:41,279 --> 00:28:44,640
right to a file

00:28:42,720 --> 00:28:46,960
and i can put a minify agent on there

00:28:44,640 --> 00:28:49,679
and read that file either as a sale or

00:28:46,960 --> 00:28:51,200
the whole file and send that to an ipi

00:28:49,679 --> 00:28:52,960
that's one option

00:28:51,200 --> 00:28:54,960
if that other server has a rest

00:28:52,960 --> 00:28:56,960
interface i can call that

00:28:54,960 --> 00:28:58,240
i can wrap it in something like apache

00:28:56,960 --> 00:28:59,919
spark

00:28:58,240 --> 00:29:03,039
or in any of these machine learning

00:28:59,919 --> 00:29:05,200
runners to do it with zeppelin

00:29:03,039 --> 00:29:06,080
uh if it's just a python script i

00:29:05,200 --> 00:29:11,200
usually have

00:29:06,080 --> 00:29:14,720
unify running on there so i can show

00:29:11,200 --> 00:29:17,840
let you that to uh command line

00:29:14,720 --> 00:29:17,840
everyone's favorite place

00:29:19,120 --> 00:29:22,880
actually how i'm running

00:29:23,039 --> 00:29:30,480
the uh that process i have

00:29:27,200 --> 00:29:32,640
a directory here

00:29:30,480 --> 00:29:34,159
with a couple of shell scripts yeah

00:29:32,640 --> 00:29:37,200
filters

00:29:34,159 --> 00:29:39,679
and you see here it's calling python

00:29:37,200 --> 00:29:41,600
so this is not where my knife server is

00:29:39,679 --> 00:29:42,159
this is where that little minify engine

00:29:41,600 --> 00:29:44,240
is

00:29:42,159 --> 00:29:46,480
it's small i'm using the java one you

00:29:44,240 --> 00:29:48,399
can use the c plus plus one

00:29:46,480 --> 00:29:49,919
pretty small it's running on xavier

00:29:48,399 --> 00:29:52,320
running on minifi

00:29:49,919 --> 00:29:53,679
and i'm using that to coordinate running

00:29:52,320 --> 00:29:57,679
that script

00:29:53,679 --> 00:30:00,960
but i also have other places where

00:29:57,679 --> 00:30:02,799
i just have something running on its own

00:30:00,960 --> 00:30:05,520
if i could find that one again we have a

00:30:02,799 --> 00:30:08,000
lot of i have a lot of service here

00:30:05,520 --> 00:30:10,320
okay yeah this one's running so here i'm

00:30:08,000 --> 00:30:11,520
doing just what you said i have a python

00:30:10,320 --> 00:30:14,640
script running

00:30:11,520 --> 00:30:17,600
and it's just writing to a file and then

00:30:14,640 --> 00:30:21,520
minifies reading that file

00:30:17,600 --> 00:30:24,880
so that's one option yeah whether you

00:30:21,520 --> 00:30:28,080
print those results to a file or have

00:30:24,880 --> 00:30:30,559
minify execute that python

00:30:28,080 --> 00:30:32,880
it really depends i usually put minify

00:30:30,559 --> 00:30:35,039
on that device if it's possible

00:30:32,880 --> 00:30:36,240
with anything that runs linux mac or

00:30:35,039 --> 00:30:39,200
windows

00:30:36,240 --> 00:30:40,559
and has at least a couple megs of ram

00:30:39,200 --> 00:30:43,360
obviously it's something like

00:30:40,559 --> 00:30:45,039
an arduino it's too small for that but

00:30:43,360 --> 00:30:48,240
those sometimes will have

00:30:45,039 --> 00:30:49,520
you know i've got a adafruit device over

00:30:48,240 --> 00:30:51,760
there on the wall

00:30:49,520 --> 00:30:52,799
that pushes out the blue low energy

00:30:51,760 --> 00:30:55,360
messages

00:30:52,799 --> 00:30:57,600
and i can read them from another device

00:30:55,360 --> 00:30:59,440
or a gateway or a server

00:30:57,600 --> 00:31:00,960
uh your other option is some of these

00:30:59,440 --> 00:31:04,000
will have wi-fi and they can

00:31:00,960 --> 00:31:07,600
make a push out whether that's mcpc

00:31:04,000 --> 00:31:10,480
or rest or just uh some other

00:31:07,600 --> 00:31:12,720
protocol you can usually read that or

00:31:10,480 --> 00:31:14,559
have someone read it for you

00:31:12,720 --> 00:31:16,399
uh if you look at my examples i

00:31:14,559 --> 00:31:19,039
generally have minify

00:31:16,399 --> 00:31:20,399
run that python script or read the

00:31:19,039 --> 00:31:22,399
results of it

00:31:20,399 --> 00:31:24,320
if you look at the i'll share the github

00:31:22,399 --> 00:31:26,799
it's very easy to write to a file

00:31:24,320 --> 00:31:28,640
in python you just write those results

00:31:26,799 --> 00:31:30,080
of those keep learning python scripts

00:31:28,640 --> 00:31:34,240
you'll file

00:31:30,080 --> 00:31:35,519
uh you can also call uh depends on what

00:31:34,240 --> 00:31:38,880
framework it is

00:31:35,519 --> 00:31:39,919
mxnet also has the ability to run within

00:31:38,880 --> 00:31:44,240
java

00:31:39,919 --> 00:31:47,840
which uh i showed you very slightly here

00:31:44,240 --> 00:31:49,840
it was uh a little if you look here this

00:31:47,840 --> 00:31:53,760
information's gonna look familiar

00:31:49,840 --> 00:31:56,320
i'm running an mx net uh there's a java

00:31:53,760 --> 00:31:59,600
connector for mxnet there's also if i

00:31:56,320 --> 00:32:02,399
use a dcl this one for some other

00:31:59,600 --> 00:32:04,399
frameworks like pytorch and tensorflow

00:32:02,399 --> 00:32:06,880
so i can run a lot of that

00:32:04,399 --> 00:32:08,880
through knives as you see here that

00:32:06,880 --> 00:32:11,760
makes it pretty straightforward

00:32:08,880 --> 00:32:13,600
i get back all those results and i have

00:32:11,760 --> 00:32:16,240
knives by doing it for me

00:32:13,600 --> 00:32:18,480
if i look here i wrote this one it's

00:32:16,240 --> 00:32:21,440
giving me back the class results in a

00:32:18,480 --> 00:32:23,360
bounding box around that image

00:32:21,440 --> 00:32:25,600
that's pretty straightforward and then

00:32:23,360 --> 00:32:28,480
i'm pushing that image to slack as we

00:32:25,600 --> 00:32:30,080
saw before

00:32:28,480 --> 00:32:31,440
so we had a couple of new messages

00:32:30,080 --> 00:32:31,919
because i just have that running in a

00:32:31,440 --> 00:32:36,080
flow

00:32:31,919 --> 00:32:39,279
what it's doing did we get any more uh

00:32:36,080 --> 00:32:40,960
images uploaded a couple more

00:32:39,279 --> 00:32:42,720
got a couple different cameras and as

00:32:40,960 --> 00:32:45,039
you can see they look uh

00:32:42,720 --> 00:32:45,840
slightly different angles what i've got

00:32:45,039 --> 00:32:47,360
pointed towards

00:32:45,840 --> 00:32:49,039
me when i've got pointed towards the

00:32:47,360 --> 00:32:51,360
screen

00:32:49,039 --> 00:32:53,200
uh let's see what i thought this one is

00:32:51,360 --> 00:32:54,640
but that was a chair i guess if you're a

00:32:53,200 --> 00:32:56,159
really small person

00:32:54,640 --> 00:32:57,760
i mean it just kind of looked like it's

00:32:56,159 --> 00:32:59,440
here i guess they can't fault the

00:32:57,760 --> 00:33:02,799
algorithm for that

00:32:59,440 --> 00:33:02,799
you know that's kind of legit

00:33:03,519 --> 00:33:06,559
any other questions

00:33:07,200 --> 00:33:10,399
and i'll share the github i've got a

00:33:09,840 --> 00:33:13,760
master

00:33:10,399 --> 00:33:16,880
github i have here for apache house

00:33:13,760 --> 00:33:19,440
that has a lot of things including trips

00:33:16,880 --> 00:33:23,120
to build all this material

00:33:19,440 --> 00:33:24,320
and links to a couple of uh deep

00:33:23,120 --> 00:33:28,240
learning processes

00:33:24,320 --> 00:33:29,360
here these uh have these in java but

00:33:28,240 --> 00:33:31,279
i've got some

00:33:29,360 --> 00:33:32,880
other links here on how to run the

00:33:31,279 --> 00:33:36,320
python ones

00:33:32,880 --> 00:33:39,519
so that's pretty straightforward

00:33:36,320 --> 00:33:40,240
so let's get back to the program here so

00:33:39,519 --> 00:33:43,919
it's pushing

00:33:40,240 --> 00:33:44,880
data to kafka from nine five so we've

00:33:43,919 --> 00:33:47,840
gone through that

00:33:44,880 --> 00:33:48,960
step where we've got the data from the

00:33:47,840 --> 00:33:50,960
edge

00:33:48,960 --> 00:33:52,480
which could be that deep learning script

00:33:50,960 --> 00:33:55,279
could be sensors

00:33:52,480 --> 00:33:56,559
could just be grabbing from a log the

00:33:55,279 --> 00:33:58,480
definition if i

00:33:56,559 --> 00:34:01,360
cleaned it up validated against the

00:33:58,480 --> 00:34:04,640
schema with sister kafka

00:34:01,360 --> 00:34:06,320
we saw data coming through kafka lots of

00:34:04,640 --> 00:34:07,440
different topics depending on what they

00:34:06,320 --> 00:34:10,480
were

00:34:07,440 --> 00:34:12,240
those are coming in we get that data and

00:34:10,480 --> 00:34:14,159
then from there

00:34:12,240 --> 00:34:15,919
i'm going to take this and i just could

00:34:14,159 --> 00:34:17,599
put it into an app

00:34:15,919 --> 00:34:20,960
and i've got a couple of different apps

00:34:17,599 --> 00:34:24,159
here for whether

00:34:20,960 --> 00:34:26,320
a uh cop can connect that i drop the

00:34:24,159 --> 00:34:29,359
connect app it's really simple

00:34:26,320 --> 00:34:30,320
it's just waiting on that topic and

00:34:29,359 --> 00:34:33,440
reading those

00:34:30,320 --> 00:34:35,679
apache avro messages and just dumping

00:34:33,440 --> 00:34:38,800
them to a directory

00:34:35,679 --> 00:34:40,480
uh in hdfs pretty straightforward but

00:34:38,800 --> 00:34:42,000
just to show you one of the things you

00:34:40,480 --> 00:34:44,960
can do

00:34:42,000 --> 00:34:46,240
so we can also take a look at a link

00:34:44,960 --> 00:34:48,399
process here

00:34:46,240 --> 00:34:50,960
i've got one that's reading from one of

00:34:48,399 --> 00:34:53,119
the other

00:34:50,960 --> 00:34:54,800
devices i have which i didn't show you

00:34:53,119 --> 00:34:57,440
yet i should show you that one too

00:34:54,800 --> 00:34:58,400
i guess that have a lot of devices here

00:34:57,440 --> 00:35:01,599
uh besides that

00:34:58,400 --> 00:35:02,000
xavier i also have a raspberry pi 4 with

00:35:01,599 --> 00:35:06,400
all those

00:35:02,000 --> 00:35:08,560
sensors now i have this on pause

00:35:06,400 --> 00:35:10,240
uh which is nice with these queues you

00:35:08,560 --> 00:35:12,160
can see some of the data coming in

00:35:10,240 --> 00:35:13,680
some of that from that log that that

00:35:12,160 --> 00:35:15,520
python script writing

00:35:13,680 --> 00:35:17,520
and you can see it kind of has that name

00:35:15,520 --> 00:35:20,640
that suggests it

00:35:17,520 --> 00:35:22,160
uh some of them is the uh

00:35:20,640 --> 00:35:24,160
what is this data this might be the

00:35:22,160 --> 00:35:27,200
standard out from

00:35:24,160 --> 00:35:30,480
not just another row from a different

00:35:27,200 --> 00:35:32,079
uh python process so those are going to

00:35:30,480 --> 00:35:33,599
come in

00:35:32,079 --> 00:35:35,520
and we're going to sort them out based

00:35:33,599 --> 00:35:36,320
on what they are of their energy ones

00:35:35,520 --> 00:35:40,400
i'm looking at

00:35:36,320 --> 00:35:42,960
uh some energy sensors uh over here

00:35:40,400 --> 00:35:44,960
i've got some different sensors and i'm

00:35:42,960 --> 00:35:45,520
going to uh do some queries on them

00:35:44,960 --> 00:35:49,280
decide

00:35:45,520 --> 00:35:52,320
what i want to do with them and then for

00:35:49,280 --> 00:35:56,400
uh the more important ones which are the

00:35:52,320 --> 00:35:59,440
uh iot style ones pushing it to capture

00:35:56,400 --> 00:36:02,320
again sounds pretty familiar this is

00:35:59,440 --> 00:36:04,960
pretty much the flank paradigm

00:36:02,320 --> 00:36:06,160
data it's a deny if i could be for

00:36:04,960 --> 00:36:09,520
minifies

00:36:06,160 --> 00:36:10,720
from 100 other sources i clean it up

00:36:09,520 --> 00:36:13,280
validate it

00:36:10,720 --> 00:36:14,960
put it in a format that makes sense try

00:36:13,280 --> 00:36:18,160
to align it with a schema

00:36:14,960 --> 00:36:19,920
tabular if it's images or something else

00:36:18,160 --> 00:36:22,160
maybe send that system keep learning

00:36:19,920 --> 00:36:26,160
within java or elsewhere

00:36:22,160 --> 00:36:29,119
but if it looks like a table as a schema

00:36:26,160 --> 00:36:30,240
let me treat it as such and send that

00:36:29,119 --> 00:36:32,320
through the system

00:36:30,240 --> 00:36:34,160
so i've got on this side i have energy

00:36:32,320 --> 00:36:38,160
data this side i have

00:36:34,160 --> 00:36:40,320
data data some sort of iot

00:36:38,160 --> 00:36:42,880
and if we look i've got a couple of

00:36:40,320 --> 00:36:45,520
things that are going to process those

00:36:42,880 --> 00:36:47,440
again i automatically built those using

00:36:45,520 --> 00:36:49,440
a script in that github

00:36:47,440 --> 00:36:51,040
i have a schema for all these sort of

00:36:49,440 --> 00:36:55,839
things

00:36:51,040 --> 00:36:55,839
you know like energy

00:36:56,079 --> 00:37:02,240
and whatever other yeah there is energy

00:36:59,839 --> 00:37:03,839
and one for stata so i could process

00:37:02,240 --> 00:37:05,599
that as it comes in

00:37:03,839 --> 00:37:07,440
let's make sure we have some of that

00:37:05,599 --> 00:37:09,040
data coming in that we just started

00:37:07,440 --> 00:37:11,760
sending

00:37:09,040 --> 00:37:12,880
so we see some energy data let's see if

00:37:11,760 --> 00:37:15,040
anything showed up

00:37:12,880 --> 00:37:17,200
got an alert it's been off for a while

00:37:15,040 --> 00:37:18,160
so i paused it so i'd have a nice patch

00:37:17,200 --> 00:37:22,000
of data

00:37:18,160 --> 00:37:24,640
again it's an avro format as a schema

00:37:22,000 --> 00:37:26,240
i could see here i put it a copy in

00:37:24,640 --> 00:37:27,839
there that makes it easier to work with

00:37:26,240 --> 00:37:29,680
top to connect

00:37:27,839 --> 00:37:32,079
so that's something i learned and it

00:37:29,680 --> 00:37:33,520
makes sense to have a key for copter

00:37:32,079 --> 00:37:36,000
it makes it easier if you want to

00:37:33,520 --> 00:37:38,560
correlate between what showed up in your

00:37:36,000 --> 00:37:40,640
sink what was in your source what's

00:37:38,560 --> 00:37:42,720
still in kafka

00:37:40,640 --> 00:37:44,400
don't go along with knowing the offset

00:37:42,720 --> 00:37:44,880
and the time stamp for having that key

00:37:44,400 --> 00:37:47,680
is

00:37:44,880 --> 00:37:48,320
especially i use the key that's the uuid

00:37:47,680 --> 00:37:51,040
here

00:37:48,320 --> 00:37:53,280
so i could track it back to the uh event

00:37:51,040 --> 00:37:58,000
coming off that device

00:37:53,280 --> 00:38:00,960
very helpful so let me uh

00:37:58,000 --> 00:38:01,440
show you what happens next so what

00:38:00,960 --> 00:38:04,480
happens

00:38:01,440 --> 00:38:05,839
next is a print again as part of that

00:38:04,480 --> 00:38:08,960
blank stack

00:38:05,839 --> 00:38:12,400
this is perhaps the easiest distributed

00:38:08,960 --> 00:38:15,839
application that reads from multiple

00:38:12,400 --> 00:38:18,320
topics and populates the third uh

00:38:15,839 --> 00:38:19,680
that you'll ever see and that anyone

00:38:18,320 --> 00:38:22,880
will ever write

00:38:19,680 --> 00:38:25,760
uh that's why i highly recommend sql

00:38:22,880 --> 00:38:27,920
if you see here this statement here is

00:38:25,760 --> 00:38:30,960
all i need

00:38:27,920 --> 00:38:32,720
for the uh blank stack to be populating

00:38:30,960 --> 00:38:34,640
a constant topic

00:38:32,720 --> 00:38:37,040
so what i'm doing is i'm selecting a

00:38:34,640 --> 00:38:40,240
couple of fields

00:38:37,040 --> 00:38:43,440
from that uh state of topic you see here

00:38:40,240 --> 00:38:45,520
and some of them from that energy topic

00:38:43,440 --> 00:38:47,440
and we we can see both of those you can

00:38:45,520 --> 00:38:49,040
see data's coming in we that's the

00:38:47,440 --> 00:38:52,320
energy one

00:38:49,040 --> 00:38:52,320
then we'll look at scada

00:38:52,640 --> 00:38:56,880
and we see they both have data coming in

00:38:55,520 --> 00:38:57,680
again they were both off for a little

00:38:56,880 --> 00:39:01,359
while i had a

00:38:57,680 --> 00:39:03,599
uh alert set up there

00:39:01,359 --> 00:39:04,960
again put a key in there this one's

00:39:03,599 --> 00:39:07,920
coming off

00:39:04,960 --> 00:39:09,440
as a different id okay there's all the

00:39:07,920 --> 00:39:12,560
fields from there

00:39:09,440 --> 00:39:15,599
so i see that coming in so flink is

00:39:12,560 --> 00:39:17,760
taking it from both of those

00:39:15,599 --> 00:39:20,000
things we have there to make it a little

00:39:17,760 --> 00:39:23,599
easier to read here

00:39:20,000 --> 00:39:27,359
okay coming in as we see here i've

00:39:23,599 --> 00:39:28,320
processed for the join about 26 thousand

00:39:27,359 --> 00:39:31,839
records

00:39:28,320 --> 00:39:35,839
and they're getting pushed into this

00:39:31,839 --> 00:39:39,200
third topic which has the schema here

00:39:35,839 --> 00:39:40,800
this is the uh global sensor that's what

00:39:39,200 --> 00:39:43,200
i'm calling it

00:39:40,800 --> 00:39:44,720
uh also i i like to use the word globals

00:39:43,200 --> 00:39:45,839
it's something i'm going to use maybe

00:39:44,720 --> 00:39:48,640
top just

00:39:45,839 --> 00:39:50,320
schema replication to uh change

00:39:48,640 --> 00:39:50,800
replication to push it somewhere else to

00:39:50,320 --> 00:39:54,160
another

00:39:50,800 --> 00:39:55,040
uh cluster and so this is a join of

00:39:54,160 --> 00:39:56,640
those two

00:39:55,040 --> 00:39:59,200
it's an inner joint i could have done an

00:39:56,640 --> 00:40:01,200
outer join but i i wanted to get ones

00:39:59,200 --> 00:40:03,359
where they match up on time

00:40:01,200 --> 00:40:05,280
so i got some sensor readings at the

00:40:03,359 --> 00:40:08,000
same time i did the energy

00:40:05,280 --> 00:40:08,560
again maybe this is i joined a bunch of

00:40:08,000 --> 00:40:11,599
different

00:40:08,560 --> 00:40:12,000
uh sensors in the field together you

00:40:11,599 --> 00:40:13,839
know

00:40:12,000 --> 00:40:16,960
you could do a lot with these people

00:40:13,839 --> 00:40:19,520
within points so this is just running

00:40:16,960 --> 00:40:20,400
i didn't have to do much to launch this

00:40:19,520 --> 00:40:23,599
i'll show you the

00:40:20,400 --> 00:40:27,520
shell script for that later and i'm also

00:40:23,599 --> 00:40:28,640
running a uh sql command line so i could

00:40:27,520 --> 00:40:31,040
query it

00:40:28,640 --> 00:40:33,119
yeah i know command line but it's apache

00:40:31,040 --> 00:40:35,920
con i can create command line

00:40:33,119 --> 00:40:37,440
so to show you the different catalogs

00:40:35,920 --> 00:40:40,319
that we could use

00:40:37,440 --> 00:40:41,440
i'm going to use the registry one if you

00:40:40,319 --> 00:40:43,440
can't guess

00:40:41,440 --> 00:40:44,640
that's that schema registry when you saw

00:40:43,440 --> 00:40:46,240
those schema

00:40:44,640 --> 00:40:48,160
you'll guess it better when i show you a

00:40:46,240 --> 00:40:50,160
list of tables here

00:40:48,160 --> 00:40:52,400
so all these make sense right now i'm

00:40:50,160 --> 00:40:55,839
populating data and energy

00:40:52,400 --> 00:40:58,880
and that insert link sql is populating

00:40:55,839 --> 00:41:00,079
that table so let's take a look at that

00:40:58,880 --> 00:41:02,640
table

00:41:00,079 --> 00:41:04,160
uh it's not a table though this is a

00:41:02,640 --> 00:41:06,560
talk to topic

00:41:04,160 --> 00:41:08,319
with real time event data so i want

00:41:06,560 --> 00:41:12,880
another

00:41:08,319 --> 00:41:15,760
copy job so pretty shortly you'll see

00:41:12,880 --> 00:41:17,760
another job show up here and this is

00:41:15,760 --> 00:41:20,000
that sql statement

00:41:17,760 --> 00:41:21,839
and you can get all the kind of metadata

00:41:20,000 --> 00:41:24,960
there that's nice to have

00:41:21,839 --> 00:41:25,599
and here i can see the old data the new

00:41:24,960 --> 00:41:27,280
data

00:41:25,599 --> 00:41:28,960
and this is the data that's happening

00:41:27,280 --> 00:41:31,440
now

00:41:28,960 --> 00:41:32,960
so as events come in this is a

00:41:31,440 --> 00:41:35,520
continuous query

00:41:32,960 --> 00:41:36,000
it'll just start showing up so we're

00:41:35,520 --> 00:41:40,560
here

00:41:36,000 --> 00:41:43,280
at 14.54 let's see if we have more data

00:41:40,560 --> 00:41:46,800
coming into the system

00:41:43,280 --> 00:41:49,839
or uh for this guy

00:41:46,800 --> 00:41:51,760
see a couple coming through uh they do

00:41:49,839 --> 00:41:54,480
have to match up though

00:41:51,760 --> 00:41:55,119
that's why can i do it in a batch there

00:41:54,480 --> 00:41:58,160
uh because

00:41:55,119 --> 00:42:01,119
i only have it coming into one a second

00:41:58,160 --> 00:42:02,720
do we have that data coming through

00:42:01,119 --> 00:42:04,880
coming into kafka

00:42:02,720 --> 00:42:05,920
we should be starting to get some more

00:42:04,880 --> 00:42:09,520
messages

00:42:05,920 --> 00:42:12,079
and we look down here 54

00:42:09,520 --> 00:42:13,040
getting some new events popping in again

00:42:12,079 --> 00:42:16,400
we could also

00:42:13,040 --> 00:42:19,440
look at different queries i can look at

00:42:16,400 --> 00:42:20,480
the energy data as it's coming in pretty

00:42:19,440 --> 00:42:22,480
straightforward

00:42:20,480 --> 00:42:24,400
now if you want to deploy this in an

00:42:22,480 --> 00:42:27,599
environment

00:42:24,400 --> 00:42:28,960
as an application i can write a java app

00:42:27,599 --> 00:42:31,839
in flink

00:42:28,960 --> 00:42:33,680
wrap this sql in it and deploy it and do

00:42:31,839 --> 00:42:34,079
whatever i want with this query i could

00:42:33,680 --> 00:42:37,599
push

00:42:34,079 --> 00:42:41,200
to any of uh blank uh

00:42:37,599 --> 00:42:43,680
stores or very commonly i'll read this

00:42:41,200 --> 00:42:43,680
data

00:42:46,319 --> 00:42:51,119
or other sources do some processing

00:42:49,520 --> 00:42:53,520
maybe do some uh

00:42:51,119 --> 00:42:54,640
machine learning and then push that to

00:42:53,520 --> 00:42:57,040
another topic

00:42:54,640 --> 00:42:58,640
you see here it just wrote a new record

00:42:57,040 --> 00:43:01,040
here it's coming in

00:42:58,640 --> 00:43:02,319
the new date is coming in starting to

00:43:01,040 --> 00:43:05,040
use more

00:43:02,319 --> 00:43:07,359
data on my disk at some point they're

00:43:05,040 --> 00:43:09,440
going to worry if i'm going to run out

00:43:07,359 --> 00:43:11,119
i cancel that job and we got this other

00:43:09,440 --> 00:43:13,440
one where i'm going for managing

00:43:11,119 --> 00:43:16,400
this one's still running this is on an

00:43:13,440 --> 00:43:16,400
apache yarn

00:43:16,560 --> 00:43:23,599
cluster here pretty straightforward

00:43:20,400 --> 00:43:24,800
yes i am using flink sql to join and

00:43:23,599 --> 00:43:27,839
distribute well

00:43:24,800 --> 00:43:33,040
at of time sorry about that

00:43:27,839 --> 00:43:33,040
i will uh we'll talk to you soon thank

00:43:38,839 --> 00:43:42,800
you

00:43:40,720 --> 00:43:42,800

YouTube URL: https://www.youtube.com/watch?v=VYNRo0pbFtg


