Title: Buddhist Digital Archives (BUDA), RDF and jena-text
Publication date: 2020-10-14
Playlist: ApacheCon @Home 2020: Jena
Description: 
	Buddhist Digital Archives (BUDA), RDF and jena-text
Chris Tomlinson, Élie Roux

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

BUDA is s Linked Data Platform built on Jena-Fuseki using RDF and Jena’s Lucene integration, jena-text. The platform enables collaboration in digital humanities among a variety of partners and leverages RDF and IIIF to provide open-access to a vast collection of textual materials and cultural heritage metadata about these materials.

Chris Tomlinson:
Senior Technologist, working with BDRC (tbrc.org) for 18 years developing systems for the preservation, access and distribution of Buddhist texts and their cultural context. Developed contributions to Jena in support of the multilingual needs of BUDA.
Élie Roux:
Project Lead, working with BDRC for 4 years, with experience in open-source cultural preservation projects (such as Gregorian Chant score engraving software . Developed contributions to IIIF and other open-source activities for use in BUDA.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,920 --> 00:00:29,840
okay well

00:00:27,840 --> 00:00:31,840
thanks everyone for joining in this

00:00:29,840 --> 00:00:34,640
presentation we're going to

00:00:31,840 --> 00:00:37,120
introduce our work on the buddhist

00:00:34,640 --> 00:00:39,360
digital archives platform or buddha for

00:00:37,120 --> 00:00:39,360
sure

00:00:39,760 --> 00:00:43,840
so this pronunciation will have two

00:00:41,440 --> 00:00:44,960
parts in the first part i'm going to

00:00:43,840 --> 00:00:48,239
introduce the

00:00:44,960 --> 00:00:48,960
the the way we implemented a full rdf

00:00:48,239 --> 00:00:51,120
workflow

00:00:48,960 --> 00:00:52,800
uh so fully rdf in the sense that we

00:00:51,120 --> 00:00:55,280
don't have uh

00:00:52,800 --> 00:00:56,640
data in a non-rdf format we don't have

00:00:55,280 --> 00:00:59,840
like a sql database or an

00:00:56,640 --> 00:01:03,359
nsxr database and um

00:00:59,840 --> 00:01:05,360
which is uh and our platform is based on

00:01:03,359 --> 00:01:07,439
genome and then in the second part

00:01:05,360 --> 00:01:12,320
we're going to present the contribution

00:01:07,439 --> 00:01:14,640
we made to the genetics

00:01:12,320 --> 00:01:14,640
so

00:01:15,920 --> 00:01:19,840
so first of all completely production

00:01:18,240 --> 00:01:20,400
the buddha platform has been developed

00:01:19,840 --> 00:01:24,000
by the

00:01:20,400 --> 00:01:27,200
british digital resource center so

00:01:24,000 --> 00:01:30,000
to serve as the main you know

00:01:27,200 --> 00:01:31,840
platform for the database and uh so the

00:01:30,000 --> 00:01:35,439
database is constituted about

00:01:31,840 --> 00:01:38,880
on two parts one we have

00:01:35,439 --> 00:01:40,000
some bibliographical data on about

00:01:38,880 --> 00:01:43,280
eighty 000

00:01:40,000 --> 00:01:46,640
volumes of tibetan manuscripts and

00:01:43,280 --> 00:01:47,680
that represents about 20 million images

00:01:46,640 --> 00:01:51,200
that we serve

00:01:47,680 --> 00:01:52,240
through if we also have a biographical

00:01:51,200 --> 00:01:55,439
part

00:01:52,240 --> 00:01:58,799
so about 40 000 records

00:01:55,439 --> 00:02:02,320
um bdrc has always been um

00:01:58,799 --> 00:02:04,399
quite strong on the open access so we

00:02:02,320 --> 00:02:06,000
are only using and developing open

00:02:04,399 --> 00:02:08,479
source software so everything is on

00:02:06,000 --> 00:02:11,599
github

00:02:08,479 --> 00:02:12,640
the data that we serve is under the cc0

00:02:11,599 --> 00:02:14,959
license

00:02:12,640 --> 00:02:17,040
and most of our images are in the public

00:02:14,959 --> 00:02:19,520
domain

00:02:17,040 --> 00:02:21,680
one of the particularity is that our

00:02:19,520 --> 00:02:24,239
database is very multilingual

00:02:21,680 --> 00:02:26,000
we have some tibetan in different

00:02:24,239 --> 00:02:28,160
configurations from chinese

00:02:26,000 --> 00:02:30,800
some indic languages and some southeast

00:02:28,160 --> 00:02:33,840
asian languages

00:02:30,800 --> 00:02:34,319
um so now the the buddha platform itself

00:02:33,840 --> 00:02:37,360
has

00:02:34,319 --> 00:02:39,840
the development started in 2017

00:02:37,360 --> 00:02:42,519
uh it we're reading at the end of the

00:02:39,840 --> 00:02:45,599
development with a public data on

00:02:42,519 --> 00:02:46,720
library.bdrc.io so as i said we are

00:02:45,599 --> 00:02:49,519
using rdf

00:02:46,720 --> 00:02:51,840
natively we have about 33 million

00:02:49,519 --> 00:02:51,840
triples

00:02:51,920 --> 00:02:56,080
and um one of the requirements i mean

00:02:54,640 --> 00:02:57,920
two strong requirements

00:02:56,080 --> 00:02:59,440
of the platform was that we were able to

00:02:57,920 --> 00:03:03,120
search in different

00:02:59,440 --> 00:03:06,560
translations for each language and

00:03:03,120 --> 00:03:10,319
also to be able to uh find results

00:03:06,560 --> 00:03:13,360
in some in the databases of our partners

00:03:10,319 --> 00:03:16,959
we have about 15 partners who are

00:03:13,360 --> 00:03:19,840
collaborating on sharing some data

00:03:16,959 --> 00:03:20,400
and so one of the reasons we used rdf is

00:03:19,840 --> 00:03:22,319
to

00:03:20,400 --> 00:03:26,159
align with trends in the digital

00:03:22,319 --> 00:03:26,159
amenities and the library science

00:03:26,560 --> 00:03:31,040
so now from a more engineering

00:03:29,280 --> 00:03:34,080
perspective i'm going to

00:03:31,040 --> 00:03:35,920
describe the way

00:03:34,080 --> 00:03:37,840
i mean what we came up with in terms of

00:03:35,920 --> 00:03:41,760
architecture and why

00:03:37,840 --> 00:03:42,640
so initially in 2017 what we had in mind

00:03:41,760 --> 00:03:45,200
was

00:03:42,640 --> 00:03:45,760
something along these lines so we would

00:03:45,200 --> 00:03:48,000
have an

00:03:45,760 --> 00:03:48,879
rdf editor that would run some sparkle

00:03:48,000 --> 00:03:50,640
queries so

00:03:48,879 --> 00:03:52,319
all our data would be you know in a

00:03:50,640 --> 00:03:55,280
triple star behind a

00:03:52,319 --> 00:03:55,760
sparkle endpoint um the website will

00:03:55,280 --> 00:03:58,080
also

00:03:55,760 --> 00:03:59,920
run some sparkle queries uh we would

00:03:58,080 --> 00:04:03,840
have other services such as

00:03:59,920 --> 00:04:06,720
if server etc and also some users

00:04:03,840 --> 00:04:07,360
would run some sparkle queries directly

00:04:06,720 --> 00:04:09,680
and

00:04:07,360 --> 00:04:11,519
so most of the sparkle queries would be

00:04:09,680 --> 00:04:13,760
federated queries so using

00:04:11,519 --> 00:04:15,760
the federation protocol and so they

00:04:13,760 --> 00:04:18,000
would you know

00:04:15,760 --> 00:04:20,400
be federated among different triple

00:04:18,000 --> 00:04:23,600
stores

00:04:20,400 --> 00:04:25,919
so the first

00:04:23,600 --> 00:04:27,360
thing that we had to change from this

00:04:25,919 --> 00:04:31,360
architecture was the

00:04:27,360 --> 00:04:33,919
federation one of the the reasons

00:04:31,360 --> 00:04:37,120
is that you know when the website makes

00:04:33,919 --> 00:04:39,120
federated query we have some partners

00:04:37,120 --> 00:04:41,120
that are pretty far away so our servers

00:04:39,120 --> 00:04:42,800
are on the us east coast and we have

00:04:41,120 --> 00:04:43,680
some partners in new zealand for

00:04:42,800 --> 00:04:47,680
instance

00:04:43,680 --> 00:04:51,680
so this arrow is actually

00:04:47,680 --> 00:04:54,720
physically very long so and the the data

00:04:51,680 --> 00:04:56,720
of our partners is very much static and

00:04:54,720 --> 00:04:59,919
it doesn't change every day

00:04:56,720 --> 00:05:01,680
so uh it didn't make a lot of sense

00:04:59,919 --> 00:05:04,000
to run internal recovery from the

00:05:01,680 --> 00:05:07,600
website uh you know basically

00:05:04,000 --> 00:05:10,080
through the whole world so what we do is

00:05:07,600 --> 00:05:11,600
we have an offline import of our

00:05:10,080 --> 00:05:14,240
partners data set

00:05:11,600 --> 00:05:14,800
so we we sort of cache it in our triple

00:05:14,240 --> 00:05:17,759
store

00:05:14,800 --> 00:05:18,240
and run the the federation locally if

00:05:17,759 --> 00:05:21,680
you want

00:05:18,240 --> 00:05:25,759
so it's it's a cached federation

00:05:21,680 --> 00:05:28,320
so another change that we made was

00:05:25,759 --> 00:05:30,639
to add a middleware between you know all

00:05:28,320 --> 00:05:33,759
the services and the sparkle endpoints

00:05:30,639 --> 00:05:35,840
one of the reasons is that

00:05:33,759 --> 00:05:38,240
we discovered it was very easy even for

00:05:35,840 --> 00:05:40,560
us you know what intention users

00:05:38,240 --> 00:05:42,160
to make some sparkle queries that were

00:05:40,560 --> 00:05:43,919
extremely inefficient and that were

00:05:42,160 --> 00:05:46,639
taking you know 10 minutes

00:05:43,919 --> 00:05:47,759
to finish and returning megabytes of

00:05:46,639 --> 00:05:50,240
data

00:05:47,759 --> 00:05:51,199
and that wasn't a really ideal that you

00:05:50,240 --> 00:05:52,800
know put some

00:05:51,199 --> 00:05:55,280
load on the server and you know if

00:05:52,800 --> 00:05:57,759
everyone starts to do that

00:05:55,280 --> 00:05:59,680
that can put the server on his knees so

00:05:57,759 --> 00:06:03,039
in order to avoid that

00:05:59,680 --> 00:06:04,080
we we have this system which is quite

00:06:03,039 --> 00:06:05,759
similar to what

00:06:04,080 --> 00:06:08,800
cloud described in this presentation we

00:06:05,759 --> 00:06:13,440
have some sparkle query templates

00:06:08,800 --> 00:06:16,319
and with some variables that we can

00:06:13,440 --> 00:06:18,720
bind basically through some arguments to

00:06:16,319 --> 00:06:20,800
the query and you know we make sure that

00:06:18,720 --> 00:06:22,960
all the variables are bound before we

00:06:20,800 --> 00:06:26,080
send the sparkle query to the sparkling

00:06:22,960 --> 00:06:28,319
endpoint and return the data

00:06:26,080 --> 00:06:29,199
the dismissor will also implement some

00:06:28,319 --> 00:06:33,360
caching

00:06:29,199 --> 00:06:36,479
and it provides some

00:06:33,360 --> 00:06:38,319
so-called cool uris you know that's uh

00:06:36,479 --> 00:06:40,800
some different serialization according

00:06:38,319 --> 00:06:43,440
to the accept header etc

00:06:40,800 --> 00:06:43,919
so yeah that was also a major change not

00:06:43,440 --> 00:06:46,319
to

00:06:43,919 --> 00:06:49,199
use to allow anyone to use the sparkling

00:06:46,319 --> 00:06:51,840
point directly

00:06:49,199 --> 00:06:52,880
and finally the one of the biggest

00:06:51,840 --> 00:06:56,479
change was that

00:06:52,880 --> 00:07:00,840
um we didn't

00:06:56,479 --> 00:07:04,319
really think that the i said the tools

00:07:00,840 --> 00:07:04,960
around fuseki or any other triple store

00:07:04,319 --> 00:07:07,360
were

00:07:04,960 --> 00:07:10,080
mature enough to provide some very solid

00:07:07,360 --> 00:07:13,199
backup and replication etc

00:07:10,080 --> 00:07:14,400
so instead we thought well we probably

00:07:13,199 --> 00:07:17,360
want our data

00:07:14,400 --> 00:07:18,400
in you know somewhere else so that if

00:07:17,360 --> 00:07:21,680
crashes

00:07:18,400 --> 00:07:23,440
you know we can quickly we have a backup

00:07:21,680 --> 00:07:27,840
somewhere and we can make a new

00:07:23,440 --> 00:07:30,080
instance of ruseki so um

00:07:27,840 --> 00:07:31,280
what we ended up with is storing our

00:07:30,080 --> 00:07:34,160
data on some

00:07:31,280 --> 00:07:36,000
git repositories and we're going to

00:07:34,160 --> 00:07:38,960
explain a bit how we

00:07:36,000 --> 00:07:40,639
achieved that so basically the workflow

00:07:38,960 --> 00:07:42,720
is that our editor

00:07:40,639 --> 00:07:44,400
only modifies the data on the git

00:07:42,720 --> 00:07:46,560
repositories and then there's an

00:07:44,400 --> 00:07:50,479
automatic synchronization process

00:07:46,560 --> 00:07:50,479
from the git repositories to fusaki

00:07:52,080 --> 00:07:59,280
so in order to achieve that we

00:07:56,160 --> 00:08:00,000
also thought about uh you know the way

00:07:59,280 --> 00:08:03,120
we

00:08:00,000 --> 00:08:03,919
we think about our data basically and uh

00:08:03,120 --> 00:08:06,879
you know rdf

00:08:03,919 --> 00:08:09,120
is this sort of giant uh graph of

00:08:06,879 --> 00:08:12,160
interconnected entities

00:08:09,120 --> 00:08:12,800
and uh so that's uh you know that's very

00:08:12,160 --> 00:08:14,960
useful but

00:08:12,800 --> 00:08:16,240
at the same time it's not really how we

00:08:14,960 --> 00:08:18,639
think of our data

00:08:16,240 --> 00:08:20,879
uh in our mind the way we relate to our

00:08:18,639 --> 00:08:23,199
data is we have some data about a person

00:08:20,879 --> 00:08:24,720
that's constituted of some triples we

00:08:23,199 --> 00:08:27,120
have some data about the place let's

00:08:24,720 --> 00:08:29,599
consider the further triples et cetera

00:08:27,120 --> 00:08:31,280
and so instead of having one giant graph

00:08:29,599 --> 00:08:32,800
we sort of slice it into different

00:08:31,280 --> 00:08:37,680
pieces

00:08:32,800 --> 00:08:40,800
and so this is what we did to

00:08:37,680 --> 00:08:41,519
you know in this sort of move towards

00:08:40,800 --> 00:08:44,159
the

00:08:41,519 --> 00:08:44,800
files on the git repository so we

00:08:44,159 --> 00:08:47,600
basically

00:08:44,800 --> 00:08:49,440
took these set of triples and we put

00:08:47,600 --> 00:08:52,320
them into some rdf graphs

00:08:49,440 --> 00:08:53,680
so we have you know one rdf graph per

00:08:52,320 --> 00:08:55,839
what we perceive as

00:08:53,680 --> 00:08:57,200
information about one person one place

00:08:55,839 --> 00:09:01,040
etc

00:08:57,200 --> 00:09:04,160
and uh we serialize these graphs

00:09:01,040 --> 00:09:06,399
as some trig files uh on the git

00:09:04,160 --> 00:09:09,680
repositories

00:09:06,399 --> 00:09:12,240
um so uh in order for

00:09:09,680 --> 00:09:12,959
uh you know gibbs not to create a lot of

00:09:12,240 --> 00:09:15,920
noise

00:09:12,959 --> 00:09:17,920
when you know we update one file uh what

00:09:15,920 --> 00:09:20,640
we developed is a

00:09:17,920 --> 00:09:23,040
very straightforward uh so it's a jenna

00:09:20,640 --> 00:09:26,320
plugin that serializes

00:09:23,040 --> 00:09:27,360
uh some rdf triples in the turtle or

00:09:26,320 --> 00:09:30,800
trig

00:09:27,360 --> 00:09:34,160
in a stable way in the sense that

00:09:30,800 --> 00:09:36,560
the same graph will always be serialized

00:09:34,160 --> 00:09:40,080
in the same exact same way

00:09:36,560 --> 00:09:42,800
you know in a file so

00:09:40,080 --> 00:09:43,760
that way you know it's a very useful uh

00:09:42,800 --> 00:09:45,360
thing because when

00:09:43,760 --> 00:09:47,040
when the graph changes we can

00:09:45,360 --> 00:09:48,880
immediately see with a git

00:09:47,040 --> 00:09:51,360
div for instance what change in the

00:09:48,880 --> 00:09:54,480
graph

00:09:51,360 --> 00:09:56,959
and one consequence of that is that we

00:09:54,480 --> 00:09:59,040
are not using blank nodes

00:09:56,959 --> 00:09:59,740
because you can't really sort blank

00:09:59,040 --> 00:10:01,120
nodes

00:09:59,740 --> 00:10:03,279
[Music]

00:10:01,120 --> 00:10:04,720
it can get quite complicated and there

00:10:03,279 --> 00:10:06,800
is another reason

00:10:04,720 --> 00:10:09,839
why why we are not using blank nodes is

00:10:06,800 --> 00:10:12,959
the rdf verification

00:10:09,839 --> 00:10:16,720
and so anyway so we have

00:10:12,959 --> 00:10:18,079
these trig files and we

00:10:16,720 --> 00:10:20,320
so the way they are synchronized with

00:10:18,079 --> 00:10:21,839
fuseki is just through a simple http

00:10:20,320 --> 00:10:23,760
put you know using the graph store

00:10:21,839 --> 00:10:26,800
protocol

00:10:23,760 --> 00:10:28,160
and uh so there is a little intermediate

00:10:26,800 --> 00:10:32,320
step here that

00:10:28,160 --> 00:10:34,240
i'm going to discuss so it's um

00:10:32,320 --> 00:10:35,519
it's really the way we keep track you

00:10:34,240 --> 00:10:38,320
know in fuseki

00:10:35,519 --> 00:10:39,839
what graph we have when they were

00:10:38,320 --> 00:10:42,000
synchronized etc

00:10:39,839 --> 00:10:44,079
so that we can we can make sure that

00:10:42,000 --> 00:10:46,000
everything is up to date on fuseki

00:10:44,079 --> 00:10:48,000
uh based on what we have in the git

00:10:46,000 --> 00:10:51,040
repositories

00:10:48,000 --> 00:10:54,959
so basically we developed a very

00:10:51,040 --> 00:10:56,399
small vocabulary that says the the

00:10:54,959 --> 00:10:58,480
following so for each

00:10:56,399 --> 00:10:59,440
graph so here that's the graph where we

00:10:58,480 --> 00:11:02,320
have

00:10:59,440 --> 00:11:04,000
some triples and inside that graph we

00:11:02,320 --> 00:11:06,399
you know when we

00:11:04,000 --> 00:11:07,920
synchronize the graph to hussekin we add

00:11:06,399 --> 00:11:10,560
this entity

00:11:07,920 --> 00:11:11,680
which is of the type admin data and we

00:11:10,560 --> 00:11:14,399
say it's about

00:11:11,680 --> 00:11:16,160
you know that graph so it's really the

00:11:14,399 --> 00:11:18,800
data about the graph

00:11:16,160 --> 00:11:20,480
and we say okay it's in that git

00:11:18,800 --> 00:11:25,200
repository so we

00:11:20,480 --> 00:11:28,320
assign some uris to our repositories and

00:11:25,200 --> 00:11:30,320
it's that file in the git repository

00:11:28,320 --> 00:11:33,120
and it has been synchronized in that

00:11:30,320 --> 00:11:36,240
revision of the file

00:11:33,120 --> 00:11:38,800
um we also have you know a

00:11:36,240 --> 00:11:40,480
sort of hard-coded graph with the latest

00:11:38,800 --> 00:11:42,079
revision that has been synchronized on

00:11:40,480 --> 00:11:45,200
fuseki for each

00:11:42,079 --> 00:11:49,279
repository basically

00:11:45,200 --> 00:11:51,440
so once you have that kind of data so

00:11:49,279 --> 00:11:54,079
we have the first part for each graph

00:11:51,440 --> 00:11:55,920
and the second part for each repository

00:11:54,079 --> 00:11:57,519
so that way you know if a

00:11:55,920 --> 00:12:00,560
synchronization fails

00:11:57,519 --> 00:12:03,440
or something like that we can you know

00:12:00,560 --> 00:12:05,040
just redo it and it will just update

00:12:03,440 --> 00:12:08,320
what needs to be updated

00:12:05,040 --> 00:12:10,480
and something that's also quite

00:12:08,320 --> 00:12:14,000
interesting is that we can synchronize

00:12:10,480 --> 00:12:17,600
multiple fuseki from the same

00:12:14,000 --> 00:12:19,920
you know git repository and we can

00:12:17,600 --> 00:12:20,639
very easily respawn a fuselate asset so

00:12:19,920 --> 00:12:23,440
if the

00:12:20,639 --> 00:12:24,000
hard drive fails or just the files get

00:12:23,440 --> 00:12:27,440
corrupted

00:12:24,000 --> 00:12:28,240
or something we can very um easily fix

00:12:27,440 --> 00:12:30,959
that

00:12:28,240 --> 00:12:32,480
uh it allows us also to perform some

00:12:30,959 --> 00:12:36,000
offline queries

00:12:32,480 --> 00:12:37,839
sometimes we want to do some very

00:12:36,000 --> 00:12:39,440
complex queries that take you know one

00:12:37,839 --> 00:12:41,760
hour to complete or something like that

00:12:39,440 --> 00:12:44,399
and we don't have to

00:12:41,760 --> 00:12:45,680
put the put that on fuseki we can just

00:12:44,399 --> 00:12:47,360
you know get a copy of the key

00:12:45,680 --> 00:12:49,120
repositories and just parse them all

00:12:47,360 --> 00:12:50,639
with a python file or something like

00:12:49,120 --> 00:12:52,800
that

00:12:50,639 --> 00:12:55,200
and uh it provides obviously some backup

00:12:52,800 --> 00:12:57,680
and some versioning

00:12:55,200 --> 00:12:58,880
and so all our git repositories are

00:12:57,680 --> 00:13:00,880
publicly available

00:12:58,880 --> 00:13:03,200
uh they're on git lab and so that

00:13:00,880 --> 00:13:06,720
creates also a nice way to export

00:13:03,200 --> 00:13:09,519
our data and for uh you know that

00:13:06,720 --> 00:13:10,000
the people who want to look of to our

00:13:09,519 --> 00:13:11,839
data

00:13:10,000 --> 00:13:13,040
it's also very easy because it's very

00:13:11,839 --> 00:13:15,120
sort of low tech

00:13:13,040 --> 00:13:16,720
it's just some text files in the git

00:13:15,120 --> 00:13:17,920
repository that you can download as a

00:13:16,720 --> 00:13:20,240
zip file

00:13:17,920 --> 00:13:23,360
so in a way our that makes also our data

00:13:20,240 --> 00:13:26,320
very accessible

00:13:23,360 --> 00:13:28,240
and yeah so this is you know the

00:13:26,320 --> 00:13:31,360
solution that we

00:13:28,240 --> 00:13:34,320
found to achieve a full rdf workflow

00:13:31,360 --> 00:13:35,600
and so we're happy to have some feedback

00:13:34,320 --> 00:13:37,680
on that

00:13:35,600 --> 00:13:40,160
because it's we had to get a little bit

00:13:37,680 --> 00:13:42,480
creative to do that kind of things

00:13:40,160 --> 00:13:43,760
so in the the second part of the

00:13:42,480 --> 00:13:46,639
presentation will be

00:13:43,760 --> 00:13:46,639
made by chris

00:13:47,760 --> 00:13:52,800
okay um yeah turn off that

00:13:52,870 --> 00:13:55,889
[Music]

00:13:58,480 --> 00:14:05,040
okay so um

00:14:02,240 --> 00:14:05,600
in order to support the multi-language

00:14:05,040 --> 00:14:08,639
uh

00:14:05,600 --> 00:14:10,000
needs that we have uh we made a variety

00:14:08,639 --> 00:14:14,720
of

00:14:10,000 --> 00:14:16,720
extensions to the existing jenna lucine

00:14:14,720 --> 00:14:19,279
integration which was present we started

00:14:16,720 --> 00:14:22,720
the project around 2017

00:14:19,279 --> 00:14:25,279
and um so it was a very uh

00:14:22,720 --> 00:14:27,600
good platform for us to begin with our

00:14:25,279 --> 00:14:31,920
prior platform we had worked on did not

00:14:27,600 --> 00:14:35,680
provide a useful integration with lucine

00:14:31,920 --> 00:14:39,040
so the key thing is that the

00:14:35,680 --> 00:14:39,760
laying strings carry the the language

00:14:39,040 --> 00:14:42,320
and the tag

00:14:39,760 --> 00:14:43,760
information and that's important from

00:14:42,320 --> 00:14:44,240
our perspective that we have many

00:14:43,760 --> 00:14:46,320
different

00:14:44,240 --> 00:14:48,399
encodings for given language and so

00:14:46,320 --> 00:14:50,959
forth that we have to deal with

00:14:48,399 --> 00:14:51,760
so the list of extensions that i'll go

00:14:50,959 --> 00:14:55,440
over

00:14:51,760 --> 00:14:57,600
some of them very quickly are

00:14:55,440 --> 00:15:00,320
first of all an ability to define

00:14:57,600 --> 00:15:04,560
analyzers and property lists

00:15:00,320 --> 00:15:05,839
that the configurations of analyzers can

00:15:04,560 --> 00:15:07,279
often be very

00:15:05,839 --> 00:15:09,120
messy and you don't want to have to keep

00:15:07,279 --> 00:15:11,120
repeating those

00:15:09,120 --> 00:15:12,240
over and over again similarly properly

00:15:11,120 --> 00:15:15,040
list may be

00:15:12,240 --> 00:15:16,480
fairly longish in terms of the number of

00:15:15,040 --> 00:15:21,839
properties we want a given

00:15:16,480 --> 00:15:23,839
uh jenna queried text query to run over

00:15:21,839 --> 00:15:25,360
we also wanted to provide highlighting

00:15:23,839 --> 00:15:28,480
uh integration

00:15:25,360 --> 00:15:30,560
so we've added that uh so now the

00:15:28,480 --> 00:15:32,399
queries can actually be highlighted

00:15:30,560 --> 00:15:33,759
using the normal leucine highlighting

00:15:32,399 --> 00:15:37,040
facility

00:15:33,759 --> 00:15:39,839
uh the query results the property lists

00:15:37,040 --> 00:15:40,320
uh extension in the text query itself

00:15:39,839 --> 00:15:43,440
i'll

00:15:40,320 --> 00:15:45,519
illustrate that briefly uh then

00:15:43,440 --> 00:15:47,519
we added generic analyzers we have a

00:15:45,519 --> 00:15:48,000
number of analyzers that we've developed

00:15:47,519 --> 00:15:50,959
for

00:15:48,000 --> 00:15:51,600
tibetan sanskrit chinese and so forth

00:15:50,959 --> 00:15:54,079
and

00:15:51,600 --> 00:15:56,240
um we don't want to i mean these are not

00:15:54,079 --> 00:15:58,320
of general interest to the

00:15:56,240 --> 00:15:59,680
general user base so we don't see any

00:15:58,320 --> 00:16:01,839
point in trying to

00:15:59,680 --> 00:16:04,560
insert those into every jenna

00:16:01,839 --> 00:16:08,639
distribution and so we provide a way to

00:16:04,560 --> 00:16:11,759
attach and configure those analyzers um

00:16:08,639 --> 00:16:13,440
outside of having them in the general

00:16:11,759 --> 00:16:15,600
distribution

00:16:13,440 --> 00:16:17,360
the main things are all of this is in

00:16:15,600 --> 00:16:20,399
the interest of supporting

00:16:17,360 --> 00:16:22,560
searching over transliterations uh so

00:16:20,399 --> 00:16:25,759
that if somebody types in tibetan

00:16:22,560 --> 00:16:28,800
script and unicode and uh

00:16:25,759 --> 00:16:30,959
somebody else's has entered data in in

00:16:28,800 --> 00:16:33,199
one of the many transliterations of

00:16:30,959 --> 00:16:34,079
of tibetan then we'd like to have

00:16:33,199 --> 00:16:37,040
queries find

00:16:34,079 --> 00:16:38,560
all of those results without uh undo you

00:16:37,040 --> 00:16:39,440
know taking up a bunch of time trying to

00:16:38,560 --> 00:16:42,639
do it

00:16:39,440 --> 00:16:43,199
then we have uh further lenient search

00:16:42,639 --> 00:16:46,560
support

00:16:43,199 --> 00:16:50,000
uh which covers the case of uh

00:16:46,560 --> 00:16:52,480
where somebody is using a lossy uh

00:16:50,000 --> 00:16:54,160
kind of a transliteration like opinion

00:16:52,480 --> 00:16:57,920
without diacritics and

00:16:54,160 --> 00:17:01,040
and so forth uh so

00:16:57,920 --> 00:17:03,040
just quickly the the reason why

00:17:01,040 --> 00:17:04,559
integrating uh leucine in the first

00:17:03,040 --> 00:17:07,600
place is because

00:17:04,559 --> 00:17:11,199
jenna's uh native support for

00:17:07,600 --> 00:17:11,919
for searching um will involve having to

00:17:11,199 --> 00:17:15,439
use some

00:17:11,919 --> 00:17:17,360
uh i'll say fairly crude mechanisms for

00:17:15,439 --> 00:17:19,120
for getting out the content of the

00:17:17,360 --> 00:17:21,280
literal because otherwise liberals are

00:17:19,120 --> 00:17:24,079
effectively blobs i mean

00:17:21,280 --> 00:17:24,880
uh there are some some facilities but

00:17:24,079 --> 00:17:27,520
they're not as

00:17:24,880 --> 00:17:29,760
generalized as leucine leucine provides

00:17:27,520 --> 00:17:32,799
extensive capabilities for

00:17:29,760 --> 00:17:36,160
uh getting out the structure and

00:17:32,799 --> 00:17:37,919
jenna uh added a

00:17:36,160 --> 00:17:39,600
generic property function extension

00:17:37,919 --> 00:17:44,480
mechanism to uh

00:17:39,600 --> 00:17:47,520
get into um the the leucine system

00:17:44,480 --> 00:17:50,000
and there's a url to the

00:17:47,520 --> 00:17:51,840
documentation online for all of the

00:17:50,000 --> 00:17:55,120
integration stuff

00:17:51,840 --> 00:17:58,000
essentially if you wanted to just use

00:17:55,120 --> 00:17:59,919
uh jenna without lucine to find all the

00:17:58,000 --> 00:18:01,039
products that have printer in their

00:17:59,919 --> 00:18:04,240
labels

00:18:01,039 --> 00:18:04,720
uh then you end up uh retrieving all of

00:18:04,240 --> 00:18:07,919
the

00:18:04,720 --> 00:18:10,240
uh labels for old products and then

00:18:07,919 --> 00:18:11,280
uh stepping through them looking one by

00:18:10,240 --> 00:18:13,679
one to find

00:18:11,280 --> 00:18:14,640
whether the regex matches now that that

00:18:13,679 --> 00:18:16,960
works for some

00:18:14,640 --> 00:18:18,240
uh situations fairly nicely but when

00:18:16,960 --> 00:18:20,160
you're dealing with large amounts of

00:18:18,240 --> 00:18:22,720
data or things that are not easily done

00:18:20,160 --> 00:18:25,919
with a regex for other kind of matching

00:18:22,720 --> 00:18:26,480
uh mechanisms then another solution is

00:18:25,919 --> 00:18:30,480
needed

00:18:26,480 --> 00:18:33,520
and essentially the text query uh

00:18:30,480 --> 00:18:35,360
drives uh input information about

00:18:33,520 --> 00:18:36,559
what the query string is and what

00:18:35,360 --> 00:18:39,200
properties

00:18:36,559 --> 00:18:40,080
uh we're looking for so the key thing to

00:18:39,200 --> 00:18:43,520
understand here

00:18:40,080 --> 00:18:45,520
is that uh jenna triple becomes a

00:18:43,520 --> 00:18:48,799
leucine document and that's a key

00:18:45,520 --> 00:18:51,200
thing to the to to the entire mechanism

00:18:48,799 --> 00:18:52,880
uh so you got to think in terms of

00:18:51,200 --> 00:18:55,840
really small documents

00:18:52,880 --> 00:18:56,559
these are not like taking globs of text

00:18:55,840 --> 00:18:59,679
files and

00:18:56,559 --> 00:19:02,799
shoveling and lucian as many uh

00:18:59,679 --> 00:19:04,880
other applications will

00:19:02,799 --> 00:19:06,880
and the net result of this is that you

00:19:04,880 --> 00:19:07,840
find all the products which is a trivial

00:19:06,880 --> 00:19:11,919
query

00:19:07,840 --> 00:19:14,559
and then you just simply uh

00:19:11,919 --> 00:19:14,960
tell text query that you're wanting to

00:19:14,559 --> 00:19:19,039
get

00:19:14,960 --> 00:19:22,000
uh uh all of the uh uh

00:19:19,039 --> 00:19:22,640
query all of the old product resources

00:19:22,000 --> 00:19:25,200
that uh

00:19:22,640 --> 00:19:25,840
have a printer in the label and and this

00:19:25,200 --> 00:19:27,440
is

00:19:25,840 --> 00:19:29,200
something that leucine knows how to do

00:19:27,440 --> 00:19:32,320
very well very well

00:19:29,200 --> 00:19:34,720
um and as i mentioned it treats uh

00:19:32,320 --> 00:19:36,799
the the integration strategy treats each

00:19:34,720 --> 00:19:40,080
uh triple as a document

00:19:36,799 --> 00:19:42,240
and the predicate

00:19:40,080 --> 00:19:44,720
uh or property of the triple is what's

00:19:42,240 --> 00:19:47,840
used to actually pick out an analyzer

00:19:44,720 --> 00:19:49,200
and then the language tag uh can be used

00:19:47,840 --> 00:19:52,960
to pick out even more

00:19:49,200 --> 00:19:54,559
uh uh specific uh analyzers and so forth

00:19:52,960 --> 00:19:56,160
and there was a mechanism already

00:19:54,559 --> 00:19:59,120
in the integration called the

00:19:56,160 --> 00:20:01,440
multilingual uh analyzer that had been

00:19:59,120 --> 00:20:03,840
been there and we've just extended uh

00:20:01,440 --> 00:20:07,280
the way that that can be configured and

00:20:03,840 --> 00:20:10,080
uh made use of and um

00:20:07,280 --> 00:20:11,200
so the first extension that i wanted to

00:20:10,080 --> 00:20:14,400
mention is the

00:20:11,200 --> 00:20:18,480
uh adding of definitional uh

00:20:14,400 --> 00:20:22,000
properties uh so in the

00:20:18,480 --> 00:20:24,960
text index lucine configuration area

00:20:22,000 --> 00:20:26,240
we've added the ability to define

00:20:24,960 --> 00:20:29,120
property lists

00:20:26,240 --> 00:20:30,640
for reuse and to define analyzers for

00:20:29,120 --> 00:20:34,159
reuse

00:20:30,640 --> 00:20:36,159
so this allows to uh say

00:20:34,159 --> 00:20:37,360
that you know you have an ascii analyzer

00:20:36,159 --> 00:20:39,039
and you're going to use it in several

00:20:37,360 --> 00:20:39,760
different places so we can just give it

00:20:39,039 --> 00:20:42,799
a name

00:20:39,760 --> 00:20:45,120
it's a straightforward idea similarly

00:20:42,799 --> 00:20:45,840
we'll show an example an extension later

00:20:45,120 --> 00:20:48,480
where we

00:20:45,840 --> 00:20:49,760
take a list of properties and ask

00:20:48,480 --> 00:20:52,480
leucine to do

00:20:49,760 --> 00:20:52,880
essentially an or query over all of

00:20:52,480 --> 00:20:56,080
those

00:20:52,880 --> 00:20:59,600
uh properties the generic analyzer

00:20:56,080 --> 00:21:02,159
extension that i mentioned a while ago

00:20:59,600 --> 00:21:02,640
simply adds the ability to instead of

00:21:02,159 --> 00:21:05,679
doing

00:21:02,640 --> 00:21:07,600
a built-in analyzer such as a

00:21:05,679 --> 00:21:09,360
configurable analyzer which is built

00:21:07,600 --> 00:21:12,720
into jenna

00:21:09,360 --> 00:21:14,559
we can make use of the

00:21:12,720 --> 00:21:17,919
the idea of a generic analyzer and as

00:21:14,559 --> 00:21:21,600
long as this class is on the class path

00:21:17,919 --> 00:21:22,720
then we can integrate access to that

00:21:21,600 --> 00:21:25,039
analyzer without

00:21:22,720 --> 00:21:27,679
putting it into the jenna distribution

00:21:25,039 --> 00:21:31,120
and we have a variety of

00:21:27,679 --> 00:21:33,520
typed parameter forms we can use

00:21:31,120 --> 00:21:35,280
these are all implied based on the

00:21:33,520 --> 00:21:36,159
values over here we know what the type

00:21:35,280 --> 00:21:39,360
is

00:21:36,159 --> 00:21:42,880
and that way we can build a

00:21:39,360 --> 00:21:44,640
configuration of an external analyzer

00:21:42,880 --> 00:21:48,159
that can be reused nicely

00:21:44,640 --> 00:21:50,240
this illustrates associating uh

00:21:48,159 --> 00:21:52,240
an analyzer definition rather than

00:21:50,240 --> 00:21:56,159
defining giving it a name

00:21:52,240 --> 00:21:58,080
we just associate it with a line tag

00:21:56,159 --> 00:22:00,960
and the way we normally use this though

00:21:58,080 --> 00:22:04,080
is we do a defined analyzer

00:22:00,960 --> 00:22:06,240
for this and then simply refer to

00:22:04,080 --> 00:22:07,919
uh when it says text analyzer we would

00:22:06,240 --> 00:22:11,760
simply refer to the

00:22:07,919 --> 00:22:15,440
previously defined configuration

00:22:11,760 --> 00:22:19,360
uh generic analyzer in this case the

00:22:15,440 --> 00:22:22,000
tibetan uh analyzer is able to

00:22:19,360 --> 00:22:23,360
uh essentially take any of the the

00:22:22,000 --> 00:22:26,400
common input

00:22:23,360 --> 00:22:29,760
uh transliterations which are one-to-one

00:22:26,400 --> 00:22:32,799
with a tibetan unicode uh

00:22:29,760 --> 00:22:36,080
script script's not the right word but

00:22:32,799 --> 00:22:37,039
uh encoding and everything is

00:22:36,080 --> 00:22:39,840
transformed

00:22:37,039 --> 00:22:41,039
into the tibetan unicode essentially and

00:22:39,840 --> 00:22:43,280
that's how we actually

00:22:41,039 --> 00:22:44,559
index the thing so leucine is indexing

00:22:43,280 --> 00:22:48,880
tibetan unicode

00:22:44,559 --> 00:22:52,960
in any case and uh we also normalize

00:22:48,880 --> 00:22:52,960
uh in many of these

00:22:53,039 --> 00:22:58,880
uh language systems there are uh

00:22:56,480 --> 00:23:00,320
a variety of ways that the unicode can

00:22:58,880 --> 00:23:03,360
be written and so

00:23:00,320 --> 00:23:06,799
uh we normalize that so that

00:23:03,360 --> 00:23:07,520
we're not dealing with a problem that we

00:23:06,799 --> 00:23:10,880
don't match

00:23:07,520 --> 00:23:12,720
because one thing used

00:23:10,880 --> 00:23:13,919
a in front of a b instead of a b in

00:23:12,720 --> 00:23:16,799
front of an a that's

00:23:13,919 --> 00:23:17,280
like a diacritic in front of a letter in

00:23:16,799 --> 00:23:20,400
a letter

00:23:17,280 --> 00:23:22,159
in front of the diacritic um and then

00:23:20,400 --> 00:23:23,600
there's lemmatization support and that

00:23:22,159 --> 00:23:24,880
sort of thing these are fairly

00:23:23,600 --> 00:23:27,919
conventional

00:23:24,880 --> 00:23:31,039
um the uh

00:23:27,919 --> 00:23:34,880
text entity map is the way that uh

00:23:31,039 --> 00:23:38,400
the jenna integration associates uh

00:23:34,880 --> 00:23:41,120
the um properties like

00:23:38,400 --> 00:23:41,760
sp skills prep label alt label and so

00:23:41,120 --> 00:23:44,960
forth

00:23:41,760 --> 00:23:46,720
with field names that are used inside uh

00:23:44,960 --> 00:23:48,640
leucine documents that's that's the

00:23:46,720 --> 00:23:52,320
essential thing that goes on here

00:23:48,640 --> 00:23:56,799
uh the full syntax allows for adding

00:23:52,320 --> 00:24:00,640
the analyzer to be used at this point

00:23:56,799 --> 00:24:03,279
but what in our particular configuration

00:24:00,640 --> 00:24:05,679
everything's built into a single

00:24:03,279 --> 00:24:06,400
gigantic multilingual analyzer which is

00:24:05,679 --> 00:24:08,559
the default

00:24:06,400 --> 00:24:09,520
analyzer so we don't even mention that

00:24:08,559 --> 00:24:14,159
in in the

00:24:09,520 --> 00:24:14,159
entity map the entity map is referred to

00:24:14,720 --> 00:24:21,360
up here in the text index lucine

00:24:18,400 --> 00:24:23,120
configuration so that just points off to

00:24:21,360 --> 00:24:25,679
the entity map which essentially

00:24:23,120 --> 00:24:28,320
associates all the different properties

00:24:25,679 --> 00:24:29,600
uh with the uh that are going to be

00:24:28,320 --> 00:24:33,520
indexed

00:24:29,600 --> 00:24:35,919
uh with their uh document field names

00:24:33,520 --> 00:24:36,559
uh and then there's some other fields up

00:24:35,919 --> 00:24:40,559
here

00:24:36,559 --> 00:24:41,440
that refer to uh you know the graph and

00:24:40,559 --> 00:24:44,960
so forth

00:24:41,440 --> 00:24:48,880
to give an example we have a triple here

00:24:44,960 --> 00:24:52,240
so we have the resource uh which is some

00:24:48,880 --> 00:24:53,360
uh in this case it's person it has a

00:24:52,240 --> 00:24:55,760
label

00:24:53,360 --> 00:24:56,799
and this is the label in one of the

00:24:55,760 --> 00:25:00,559
common

00:24:56,799 --> 00:25:02,400
uh uh transliterations of tibetan

00:25:00,559 --> 00:25:03,600
the document that results from that in

00:25:02,400 --> 00:25:05,760
lucine

00:25:03,600 --> 00:25:08,320
is essentially this kind of thing

00:25:05,760 --> 00:25:08,799
there's stuff i've deleted from this but

00:25:08,320 --> 00:25:10,720
the

00:25:08,799 --> 00:25:11,840
uh the general thing is that we have the

00:25:10,720 --> 00:25:14,960
resource

00:25:11,840 --> 00:25:17,440
uh uh uri uh

00:25:14,960 --> 00:25:18,480
for the the the resource of the triple

00:25:17,440 --> 00:25:22,000
we have the graph

00:25:18,480 --> 00:25:25,039
that triple lives in uh and we have

00:25:22,000 --> 00:25:26,880
uh the string uh for the label

00:25:25,039 --> 00:25:29,279
and the language tag for the labels we

00:25:26,880 --> 00:25:32,720
just take apart the laying string and

00:25:29,279 --> 00:25:35,919
a couple of pieces here then we have

00:25:32,720 --> 00:25:38,960
a little illustrate later we have fields

00:25:35,919 --> 00:25:42,000
that include the original property name

00:25:38,960 --> 00:25:46,480
with the language tag

00:25:42,000 --> 00:25:48,799
for that uh property instance so this is

00:25:46,480 --> 00:25:50,960
repeating the data we actually use some

00:25:48,799 --> 00:25:51,360
uh configuration stuff so that only one

00:25:50,960 --> 00:25:54,480
of these

00:25:51,360 --> 00:25:56,880
is actually stored in the index

00:25:54,480 --> 00:25:58,400
and then this uid is used to manage

00:25:56,880 --> 00:26:01,679
synchronization between

00:25:58,400 --> 00:26:05,200
the graphs in uh jenna and the

00:26:01,679 --> 00:26:05,840
uh index contents and uh lucien so if

00:26:05,200 --> 00:26:08,159
the

00:26:05,840 --> 00:26:10,320
triple is deleted then this information

00:26:08,159 --> 00:26:13,840
allows to delete that from the

00:26:10,320 --> 00:26:17,039
uh from the index

00:26:13,840 --> 00:26:19,760
uh we added highlight as i mentioned and

00:26:17,039 --> 00:26:21,360
essentially that just integrates the

00:26:19,760 --> 00:26:25,039
normal lucine mechanism

00:26:21,360 --> 00:26:27,360
for using the query a second time over

00:26:25,039 --> 00:26:29,919
the query results in order to identify

00:26:27,360 --> 00:26:31,600
where the matches occurred within the

00:26:29,919 --> 00:26:34,799
overall query result

00:26:31,600 --> 00:26:36,960
uh which is the literal here and this

00:26:34,799 --> 00:26:38,720
just specifies things like the start and

00:26:36,960 --> 00:26:41,440
end character to use for

00:26:38,720 --> 00:26:44,159
uh indicating the match and and so forth

00:26:41,440 --> 00:26:46,880
and that's used in ui processing

00:26:44,159 --> 00:26:48,480
uh then i mentioned the property list

00:26:46,880 --> 00:26:51,039
extension

00:26:48,480 --> 00:26:51,600
oftentimes you want to search over

00:26:51,039 --> 00:26:54,640
multiple

00:26:51,600 --> 00:26:57,039
properties for essentially the same

00:26:54,640 --> 00:26:58,080
content as you may want to look and find

00:26:57,039 --> 00:27:00,880
all the instances

00:26:58,080 --> 00:27:03,919
that uh all the triples that have labels

00:27:00,880 --> 00:27:06,880
pref labels are all labels that

00:27:03,919 --> 00:27:08,000
have printer in uh in the label

00:27:06,880 --> 00:27:11,039
somewhere

00:27:08,000 --> 00:27:14,240
and uh you can do that with

00:27:11,039 --> 00:27:16,720
with the original uh facility

00:27:14,240 --> 00:27:19,120
in in general seeing with unions because

00:27:16,720 --> 00:27:21,440
then you can union over rdf label

00:27:19,120 --> 00:27:23,840
over the label union over the prep label

00:27:21,440 --> 00:27:26,399
and the alt label and the sparkle

00:27:23,840 --> 00:27:27,600
but it seems so much nicer to be able to

00:27:26,399 --> 00:27:30,000
simply list

00:27:27,600 --> 00:27:31,039
the properties that we want to work with

00:27:30,000 --> 00:27:34,799
and

00:27:31,039 --> 00:27:37,440
then using the definitional mechanism i

00:27:34,799 --> 00:27:37,919
i started off the extensions with then

00:27:37,440 --> 00:27:41,360
we can

00:27:37,919 --> 00:27:44,159
simply define these three labels as as

00:27:41,360 --> 00:27:45,200
x labels and then we can simply do a

00:27:44,159 --> 00:27:48,159
query that

00:27:45,200 --> 00:27:50,399
that does that that'll expand out as

00:27:48,159 --> 00:27:53,600
i'll show in a moment

00:27:50,399 --> 00:27:57,440
we also extended the output

00:27:53,600 --> 00:28:00,159
arguments of the text query

00:27:57,440 --> 00:28:00,640
property function extension so we can

00:28:00,159 --> 00:28:03,600
grab

00:28:00,640 --> 00:28:05,760
not only the graph that the triple

00:28:03,600 --> 00:28:06,720
resides in but also the property that

00:28:05,760 --> 00:28:11,039
matched

00:28:06,720 --> 00:28:12,960
and that's obviously of relevance when

00:28:11,039 --> 00:28:14,399
the property is actually a list of

00:28:12,960 --> 00:28:18,399
properties

00:28:14,399 --> 00:28:21,200
the um uh

00:28:18,399 --> 00:28:21,840
situation that i mentioned at the start

00:28:21,200 --> 00:28:23,600
about

00:28:21,840 --> 00:28:25,440
you know our motivation is really we

00:28:23,600 --> 00:28:28,240
want to be able to provide users with

00:28:25,440 --> 00:28:30,640
the ability to get into the material

00:28:28,240 --> 00:28:31,600
in any transliteration that they may be

00:28:30,640 --> 00:28:34,399
using

00:28:31,600 --> 00:28:34,960
and uh ultimately in terms of lossy

00:28:34,399 --> 00:28:37,159
things

00:28:34,960 --> 00:28:39,080
uh so just to handle the

00:28:37,159 --> 00:28:41,360
transliterations and what i mean by a

00:28:39,080 --> 00:28:45,440
transliteration here is a one

00:28:41,360 --> 00:28:48,559
one for one uh uh transformation

00:28:45,440 --> 00:28:51,600
so this is the tibetan for

00:28:48,559 --> 00:28:54,640
uh the name of the uh individual

00:28:51,600 --> 00:28:58,000
and this is transliteration that's uh

00:28:54,640 --> 00:29:00,960
uniquely transformable back into this uh

00:28:58,000 --> 00:29:02,559
tibetan form and we want to be able to

00:29:00,960 --> 00:29:06,159
allow users to enter

00:29:02,559 --> 00:29:08,720
this information in any form and

00:29:06,159 --> 00:29:11,039
retrieve all the different results they

00:29:08,720 --> 00:29:13,080
may have been cataloged or

00:29:11,039 --> 00:29:14,640
entered with any of these

00:29:13,080 --> 00:29:17,440
transliterations

00:29:14,640 --> 00:29:19,200
uh and we don't want to have to tell the

00:29:17,440 --> 00:29:20,960
librarians oh you have to do everything

00:29:19,200 --> 00:29:21,919
in one particular form and so forth so

00:29:20,960 --> 00:29:24,960
we take care of that

00:29:21,919 --> 00:29:28,960
in the uh uh in the

00:29:24,960 --> 00:29:31,440
analyzer and so our tibetan analyzer

00:29:28,960 --> 00:29:33,200
as i mentioned earlier uh will take any

00:29:31,440 --> 00:29:36,799
of these forms

00:29:33,200 --> 00:29:39,840
the uh these are two common uh

00:29:36,799 --> 00:29:41,679
romanized transliterations uh and then

00:29:39,840 --> 00:29:42,640
this is just off the tibetan unicode

00:29:41,679 --> 00:29:47,120
page

00:29:42,640 --> 00:29:50,159
then the analyzer based on the tag

00:29:47,120 --> 00:29:51,600
will have been configured to recognize

00:29:50,159 --> 00:29:54,880
these input forms

00:29:51,600 --> 00:29:57,279
and that will generate the same output

00:29:54,880 --> 00:29:58,240
which is what's actually indexed and we

00:29:57,279 --> 00:30:01,760
retain

00:29:58,240 --> 00:30:04,080
the knowledge of what the original uh

00:30:01,760 --> 00:30:06,399
form was the original transliteration

00:30:04,080 --> 00:30:09,120
form the encoding was

00:30:06,399 --> 00:30:10,000
by attaching the essentially the

00:30:09,120 --> 00:30:13,200
encoding

00:30:10,000 --> 00:30:16,480
to the uh label field in the

00:30:13,200 --> 00:30:19,919
in the uh document as we configured it

00:30:16,480 --> 00:30:22,960
and so when you're querying uh

00:30:19,919 --> 00:30:26,799
uh and and this is to

00:30:22,960 --> 00:30:30,399
uh illustrate the the the situation we

00:30:26,799 --> 00:30:33,760
we can configure the uh

00:30:30,399 --> 00:30:37,600
the the multilingual analyzers so that

00:30:33,760 --> 00:30:40,080
it will whenever it's trying to search

00:30:37,600 --> 00:30:40,720
uh uh whenever the query string that's

00:30:40,080 --> 00:30:44,480
coming in

00:30:40,720 --> 00:30:46,080
is any one of these three uh forms

00:30:44,480 --> 00:30:47,600
then it will search the other two as

00:30:46,080 --> 00:30:49,360
well and

00:30:47,600 --> 00:30:51,039
so the query string that's generated for

00:30:49,360 --> 00:30:54,080
leucine is simply an or

00:30:51,039 --> 00:30:56,399
it just looks for all of these or these

00:30:54,080 --> 00:30:57,679
or these and returns them the parsed

00:30:56,399 --> 00:31:01,120
query

00:30:57,679 --> 00:31:04,240
uh is not terribly different from the

00:31:01,120 --> 00:31:07,039
uh the the basic query that comes in

00:31:04,240 --> 00:31:08,080
uh to loosen but this is its parsed

00:31:07,039 --> 00:31:11,200
result

00:31:08,080 --> 00:31:15,360
uh and it's still an horror and

00:31:11,200 --> 00:31:18,799
uh so essentially you were able to then

00:31:15,360 --> 00:31:21,279
uh grab any material that was

00:31:18,799 --> 00:31:23,279
indexed originally in any of the

00:31:21,279 --> 00:31:26,720
transliteration

00:31:23,279 --> 00:31:29,760
forms the lenient support

00:31:26,720 --> 00:31:34,080
uh requires that we have some way of

00:31:29,760 --> 00:31:36,399
uh being able to um

00:31:34,080 --> 00:31:37,440
handle what are what we consider to be

00:31:36,399 --> 00:31:39,760
lossy

00:31:37,440 --> 00:31:40,640
uh forms of input as i mentioned like

00:31:39,760 --> 00:31:43,679
pinion

00:31:40,640 --> 00:31:46,000
without diacritics sanskrit without

00:31:43,679 --> 00:31:47,039
diacritics poly without diacritics all

00:31:46,000 --> 00:31:49,840
of these

00:31:47,039 --> 00:31:50,880
languages in their transliteration their

00:31:49,840 --> 00:31:53,600
sort of their

00:31:50,880 --> 00:31:55,519
formal one-to-one transliterations all

00:31:53,600 --> 00:31:58,480
involved uh

00:31:55,519 --> 00:31:59,360
diacritics in general in order to be

00:31:58,480 --> 00:32:02,480
very

00:31:59,360 --> 00:32:05,200
precise about what the the modifiers on

00:32:02,480 --> 00:32:05,679
characters are and that kind of thing uh

00:32:05,200 --> 00:32:09,360
and

00:32:05,679 --> 00:32:12,880
uh so we have uh developed a

00:32:09,360 --> 00:32:14,159
yet another layer of extension on this

00:32:12,880 --> 00:32:17,519
which i'll try to illustrate

00:32:14,159 --> 00:32:21,840
here the uh

00:32:17,519 --> 00:32:25,360
this is associating a uh a definition

00:32:21,840 --> 00:32:27,279
uh a defined analyzer uh

00:32:25,360 --> 00:32:30,080
we won't look at what that definition is

00:32:27,279 --> 00:32:33,279
but it's associating that with

00:32:30,080 --> 00:32:36,720
uh the language tag uh for

00:32:33,279 --> 00:32:38,559
non-diacritical uh uh sanskrit

00:32:36,720 --> 00:32:39,760
so this is sanskrits with without the

00:32:38,559 --> 00:32:43,039
diacritics

00:32:39,760 --> 00:32:46,720
and when you're searching uh

00:32:43,039 --> 00:32:50,159
for a string that is uh uh in

00:32:46,720 --> 00:32:53,519
non-diacritic sanskrit then we

00:32:50,159 --> 00:32:54,960
also have several auxiliary indices that

00:32:53,519 --> 00:32:57,440
we've defined

00:32:54,960 --> 00:32:59,120
uh in particular one that we call roman

00:32:57,440 --> 00:33:02,480
to ndia

00:32:59,120 --> 00:33:03,039
and um to kind of connect that into the

00:33:02,480 --> 00:33:06,080
picture

00:33:03,039 --> 00:33:07,679
consider the the uh exact

00:33:06,080 --> 00:33:08,640
transliteration the international

00:33:07,679 --> 00:33:10,799
alphabet for

00:33:08,640 --> 00:33:12,000
sanskrit transliteration which is a

00:33:10,799 --> 00:33:15,440
common one

00:33:12,000 --> 00:33:17,519
um and these are other common uh

00:33:15,440 --> 00:33:18,880
one-for-one uh transliterations of

00:33:17,519 --> 00:33:22,080
sanskrit

00:33:18,880 --> 00:33:25,200
and um what we do is we specify

00:33:22,080 --> 00:33:25,760
that when when we uh are confronted with

00:33:25,200 --> 00:33:29,519
strings

00:33:25,760 --> 00:33:31,279
of uh iast then we want to add them to

00:33:29,519 --> 00:33:34,720
the auxiliary index

00:33:31,279 --> 00:33:38,000
uh which is handled by uh the tag

00:33:34,720 --> 00:33:42,000
saox roman ndia and

00:33:38,000 --> 00:33:45,360
uh the definition of how we handle

00:33:42,000 --> 00:33:46,640
that tag is down here uh and we indicate

00:33:45,360 --> 00:33:51,120
that we search

00:33:46,640 --> 00:33:51,120
whenever we were searching for this

00:33:51,600 --> 00:33:55,120
kind of thing then we would search these

00:33:54,080 --> 00:33:59,039
uh

00:33:55,120 --> 00:34:01,600
three uh uh uh indices as well

00:33:59,039 --> 00:34:02,559
and uh sort of the key thing that makes

00:34:01,600 --> 00:34:06,080
that all play

00:34:02,559 --> 00:34:09,839
is the text uh index the

00:34:06,080 --> 00:34:13,679
index analyzer as opposed to the

00:34:09,839 --> 00:34:17,520
uh query analyzer essentially

00:34:13,679 --> 00:34:19,599
and we have a special roman lenient uh

00:34:17,520 --> 00:34:20,960
query analyzer configuration it's just a

00:34:19,599 --> 00:34:24,560
configuration of the

00:34:20,960 --> 00:34:25,200
sanskrit analyzer that handles doing the

00:34:24,560 --> 00:34:28,399
actual

00:34:25,200 --> 00:34:31,040
uh indexing and

00:34:28,399 --> 00:34:31,839
um the way that uh you can think of that

00:34:31,040 --> 00:34:35,359
working we have

00:34:31,839 --> 00:34:38,480
punderica with the diacritics on

00:34:35,359 --> 00:34:41,440
the end the d and the i and

00:34:38,480 --> 00:34:42,240
the configuration that i illustrated

00:34:41,440 --> 00:34:46,240
above

00:34:42,240 --> 00:34:50,240
ends up creating two

00:34:46,240 --> 00:34:51,520
indices uh uh index entries uh one with

00:34:50,240 --> 00:34:53,440
the diacritics uh

00:34:51,520 --> 00:34:55,359
syllableized and one without the

00:34:53,440 --> 00:34:58,560
diacritics syllabized

00:34:55,359 --> 00:35:01,760
and then when query time comes then

00:34:58,560 --> 00:35:05,040
the uh if the query is formed with

00:35:01,760 --> 00:35:08,560
iast then it goes against

00:35:05,040 --> 00:35:11,839
the iast field

00:35:08,560 --> 00:35:15,359
in the index and on the other hand

00:35:11,839 --> 00:35:18,560
if we come in with

00:35:15,359 --> 00:35:19,119
non-diacritic content then we search

00:35:18,560 --> 00:35:21,760
this way

00:35:19,119 --> 00:35:23,200
in each case we'll get back the same set

00:35:21,760 --> 00:35:25,200
of documents

00:35:23,200 --> 00:35:27,200
plus more in the case of the

00:35:25,200 --> 00:35:28,880
non-diacritic you generally

00:35:27,200 --> 00:35:30,400
being lossy you're generally going to

00:35:28,880 --> 00:35:32,160
get more results

00:35:30,400 --> 00:35:34,480
uh than the ones you would get if you

00:35:32,160 --> 00:35:37,599
were using a precise diacritic

00:35:34,480 --> 00:35:40,960
uh query so that's

00:35:37,599 --> 00:35:44,720
uh i i realize this is fairly rapid but

00:35:40,960 --> 00:35:47,760
yeah great that's it um

00:35:44,720 --> 00:35:51,839
so the uh uh

00:35:47,760 --> 00:35:54,800
uh that that's that's sort of the

00:35:51,839 --> 00:35:56,400
the talk so if there are additional

00:35:54,800 --> 00:36:00,640
questions other than ones that

00:35:56,400 --> 00:36:00,640
uh ellie may have already answered then

00:36:00,880 --> 00:36:06,240
glad to answer them at least one of us

00:36:10,839 --> 00:36:13,839
will

00:36:24,839 --> 00:36:27,839
so

00:36:29,920 --> 00:36:40,079
so i guess that's uh it doesn't look

00:36:32,960 --> 00:36:41,920
like there are any questions

00:36:40,079 --> 00:36:43,359
yeah i can answer the other question of

00:36:41,920 --> 00:36:46,400
the rdf div

00:36:43,359 --> 00:36:48,560
so we did look at it and

00:36:46,400 --> 00:36:49,839
we actually considered using it and

00:36:48,560 --> 00:36:54,160
creating a

00:36:49,839 --> 00:36:54,160
javascript library to handle it

00:36:54,400 --> 00:37:01,520
but in the current state of

00:36:57,520 --> 00:37:04,160
our architecture i don't think

00:37:01,520 --> 00:37:05,200
it would be really a key component

00:37:04,160 --> 00:37:07,119
because

00:37:05,200 --> 00:37:09,359
even if you start to consider okay we

00:37:07,119 --> 00:37:12,079
have our data and we sort of accumulate

00:37:09,359 --> 00:37:13,040
some you know rdf diff but you know

00:37:12,079 --> 00:37:15,440
where you put i mean

00:37:13,040 --> 00:37:17,040
files etc but then the same problem

00:37:15,440 --> 00:37:18,560
around this you know where you put the

00:37:17,040 --> 00:37:22,000
files if it's on the hard drive what

00:37:18,560 --> 00:37:26,560
happens if the hard drive fails etc

00:37:22,000 --> 00:37:26,560
um yeah

00:37:27,119 --> 00:37:30,880
yeah and and as we've answered earlier

00:37:30,000 --> 00:37:34,960
the

00:37:30,880 --> 00:37:36,880
that stable turtle is available

00:37:34,960 --> 00:37:39,040
all of all of the stuff that we've

00:37:36,880 --> 00:37:42,880
developed our entire project is on

00:37:39,040 --> 00:37:46,320
github uh under the uh buddha base

00:37:42,880 --> 00:37:50,240
uh organization and

00:37:46,320 --> 00:37:53,599
so it's all there for for the taking

00:37:50,240 --> 00:37:53,599
and so forth

00:37:57,520 --> 00:38:01,520
okay well i think we've run a minute or

00:38:00,000 --> 00:38:03,200
two over i think the birds of the

00:38:01,520 --> 00:38:06,160
feather session

00:38:03,200 --> 00:38:06,160
presumably is

00:38:06,240 --> 00:38:10,960
started so unless there's any more

00:38:15,599 --> 00:38:19,839
any more questions

00:38:26,850 --> 00:38:30,190
[Music]

00:38:30,320 --> 00:38:46,320
okay okay well thanks everyone okay yeah

00:38:33,520 --> 00:38:46,320

YouTube URL: https://www.youtube.com/watch?v=757ipoFhttU


