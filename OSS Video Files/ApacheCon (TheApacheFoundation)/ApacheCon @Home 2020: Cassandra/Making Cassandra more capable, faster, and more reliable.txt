Title: Making Cassandra more capable, faster, and more reliable
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Cassandra
Description: 
	Making Cassandra more capable, faster, and more reliable
Hiroyuki Yamada, Yuji Ito

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Cassandra is widely adopted in real-world applications and used by large and sometimes mission-critical applications because of its high performance, high availability and high scalability. However, there is still some room for improvement to take Cassandra to the next level. We have been contributing to Cassandra to make it more capable, faster, and more reliable by, for example, proposing non-invasive ACID transaction library, adding GroupCommitLogService, and maintaining and conducting Jepsen testing for lightweight transactions. This talk will present the contributions we have done including the latest updates in more detail, and the reasons why we made such contributions. This talk will be one of the good starting points for discussing the next generation Cassandra.

Hiroyuki Yamada:
Hiroyuki Yamada is CTO and CEO at Scalar, Inc. He has been passionate about parallel and distributed data management systems for more than 15 years. Prior to Scalar, he worked at IIS UTokyo, Yahoo, IBM. Ph.D. from the University of Tokyo.
Yuji Ito:
Working on distributed database/storage. Formerly, worked on SSD firmware. Master's degree in Information Science and Technology from The University of Tokyo.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:29,519 --> 00:00:32,160
hi everyone

00:00:32,800 --> 00:00:38,800
welcome to this session uh for

00:00:35,920 --> 00:00:41,120
making cassandra compatible faster and

00:00:38,800 --> 00:00:44,800
more reliable

00:00:41,120 --> 00:00:48,640
uh i guess we're on time so we can

00:00:44,800 --> 00:00:58,640
make a start current jeremy

00:00:48,640 --> 00:01:02,000
yes we can thank you

00:00:58,640 --> 00:01:02,000
okay uh let's start um

00:01:08,640 --> 00:01:15,600
uh hello everyone i'm shirokiyama

00:01:11,760 --> 00:01:17,439
cto at skater so we're talking about

00:01:15,600 --> 00:01:19,840
our contributions to apache cassandra

00:01:17,439 --> 00:01:19,840
today

00:01:20,159 --> 00:01:26,000
so um i'm hereki and then i love

00:01:22,960 --> 00:01:28,240
database systems distributed systems and

00:01:26,000 --> 00:01:32,079
then yuji is the next speaker

00:01:28,240 --> 00:01:32,079
he's going to introduce himself later on

00:01:33,040 --> 00:01:37,680
so uh at skater we've been doing a lot

00:01:36,720 --> 00:01:40,960
for cassandra

00:01:37,680 --> 00:01:44,799
to make it at the next level

00:01:40,960 --> 00:01:47,920
so we propose a library called scale db

00:01:44,799 --> 00:01:51,520
that makes easy that makes cassandra is

00:01:47,920 --> 00:01:55,119
easy compliant and then we proposed

00:01:51,520 --> 00:01:59,200
and implemented the

00:01:55,119 --> 00:02:02,079
group commit logs sync to make it faster

00:01:59,200 --> 00:02:03,600
and then we've been doing lots of uh

00:02:02,079 --> 00:02:04,799
jefferson testing for lighter

00:02:03,600 --> 00:02:08,239
transactions

00:02:04,799 --> 00:02:11,200
for to make it more reliable so

00:02:08,239 --> 00:02:11,599
this talk we're going to talk one by one

00:02:11,200 --> 00:02:13,599
uh

00:02:11,599 --> 00:02:16,720
and explaining about why we're doing

00:02:13,599 --> 00:02:16,720
this and then what we do

00:02:18,160 --> 00:02:23,840
so first uh ac transactions on cassandra

00:02:20,560 --> 00:02:23,840
with scale db

00:02:24,080 --> 00:02:28,160
so most of you don't know scale db so

00:02:27,120 --> 00:02:31,040
scale db is

00:02:28,160 --> 00:02:32,000
a universal transaction manager written

00:02:31,040 --> 00:02:34,800
in java

00:02:32,000 --> 00:02:36,720
that makes a non-easy compliant database

00:02:34,800 --> 00:02:38,800
is it compliant

00:02:36,720 --> 00:02:41,280
and then xander is the first supported

00:02:38,800 --> 00:02:44,560
database

00:02:41,280 --> 00:02:46,720
so uh architecture of scale db is uh

00:02:44,560 --> 00:02:48,239
like a layered architecture inspired by

00:02:46,720 --> 00:02:50,080
deuteronomy

00:02:48,239 --> 00:02:51,519
so uh there is a universal transaction

00:02:50,080 --> 00:02:53,760
manager uh

00:02:51,519 --> 00:02:54,879
on the top and then there is a straight

00:02:53,760 --> 00:02:57,760
abstraction layer

00:02:54,879 --> 00:02:58,640
underneath and then there uh that

00:02:57,760 --> 00:03:00,319
implements

00:02:58,640 --> 00:03:03,280
that kind of abstracts the three

00:03:00,319 --> 00:03:05,519
specific implementations

00:03:03,280 --> 00:03:06,800
and for cassandra there's a cassandra

00:03:05,519 --> 00:03:09,360
adapter

00:03:06,800 --> 00:03:10,080
that implements a stretch abstraction

00:03:09,360 --> 00:03:12,800
and then that

00:03:10,080 --> 00:03:14,640
uses a dataset java driver to talk with

00:03:12,800 --> 00:03:16,959
cassandra

00:03:14,640 --> 00:03:18,000
so from cassandra's perspective it

00:03:16,959 --> 00:03:21,040
doesn't even know

00:03:18,000 --> 00:03:25,280
if it's doing transactions or not

00:03:21,040 --> 00:03:25,280
and all the tricks are done by scale db

00:03:26,959 --> 00:03:32,879
so um why we're doing this kind of is

00:03:30,799 --> 00:03:35,120
transactions with cassandra

00:03:32,879 --> 00:03:37,200
even though cassandra doesn't support

00:03:35,120 --> 00:03:39,200
transaction by itself

00:03:37,200 --> 00:03:41,519
and then why we're doing this with scale

00:03:39,200 --> 00:03:44,560
db instead of actually

00:03:41,519 --> 00:03:48,239
updating cassandra itself

00:03:44,560 --> 00:03:51,200
so for the first question um as you know

00:03:48,239 --> 00:03:54,080
ac is a kind of mess-up feature in some

00:03:51,200 --> 00:03:57,519
mission critical applications

00:03:54,080 --> 00:04:00,480
and uh cassandra

00:03:57,519 --> 00:04:02,480
used to be uh more like the scalable

00:04:00,480 --> 00:04:03,599
database for web applications web

00:04:02,480 --> 00:04:06,239
services

00:04:03,599 --> 00:04:07,680
but it has been getting widely used for

00:04:06,239 --> 00:04:09,360
enterprise or mission critical

00:04:07,680 --> 00:04:11,360
applications

00:04:09,360 --> 00:04:14,480
and also uh cassandra is one of the

00:04:11,360 --> 00:04:18,560
major open source distributed databases

00:04:14,480 --> 00:04:18,560
so that's why we decided to do it

00:04:18,639 --> 00:04:23,919
and for the second question um

00:04:22,160 --> 00:04:26,160
it'd be great if you can update the

00:04:23,919 --> 00:04:28,880
cassandra itself

00:04:26,160 --> 00:04:29,199
but it's got a lot of risks and burden

00:04:28,880 --> 00:04:32,000
for

00:04:29,199 --> 00:04:33,440
actually modifying cassandra so we

00:04:32,000 --> 00:04:35,840
decided not to do it

00:04:33,440 --> 00:04:37,360
and then we created a library called

00:04:35,840 --> 00:04:41,120
scaledbb

00:04:37,360 --> 00:04:42,080
to enable ac transactions without

00:04:41,120 --> 00:04:45,280
actually modifying

00:04:42,080 --> 00:04:48,080
it and then that's done because

00:04:45,280 --> 00:04:48,880
um scalability being only dependent on

00:04:48,080 --> 00:04:52,639
the

00:04:48,880 --> 00:04:53,600
exposed apis so there are no risks for

00:04:52,639 --> 00:04:57,840
breaking

00:04:53,600 --> 00:04:57,840
existing code

00:04:58,400 --> 00:05:01,840
so there are pros and cons in this

00:05:00,639 --> 00:05:06,000
approach

00:05:01,840 --> 00:05:08,840
so for pros first it's a non-invasive

00:05:06,000 --> 00:05:10,560
so no modifications in xander is

00:05:08,840 --> 00:05:13,840
required

00:05:10,560 --> 00:05:17,360
so second it can achieve highest

00:05:13,840 --> 00:05:20,320
availability and high scalability

00:05:17,360 --> 00:05:21,600
even though you do transactions so those

00:05:20,320 --> 00:05:23,759
properties are kind of

00:05:21,600 --> 00:05:26,240
essential cassandra properties and then

00:05:23,759 --> 00:05:29,440
those properties are properly sustained

00:05:26,240 --> 00:05:31,120
even due to transactions

00:05:29,440 --> 00:05:32,720
and then you can achieve flexible

00:05:31,120 --> 00:05:34,960
deployment

00:05:32,720 --> 00:05:39,440
so transactional layer and then straight

00:05:34,960 --> 00:05:43,360
layer can be independently scaled

00:05:39,440 --> 00:05:45,919
so for cons it could be slower

00:05:43,360 --> 00:05:46,560
than new sequels like cockroachdb type

00:05:45,919 --> 00:05:50,479
db

00:05:46,560 --> 00:05:54,160
gigabyte db so ones of the reasons

00:05:50,479 --> 00:05:55,759
are that scale db has more abstraction

00:05:54,160 --> 00:05:58,800
layers

00:05:55,759 --> 00:06:01,440
and also uh transaction manager

00:05:58,800 --> 00:06:02,800
doesn't know doesn't know much about the

00:06:01,440 --> 00:06:06,639
straight uh strange

00:06:02,800 --> 00:06:09,360
implementation storage layouts

00:06:06,639 --> 00:06:10,240
and then for a second it's similar to

00:06:09,360 --> 00:06:12,240
the first one but

00:06:10,240 --> 00:06:13,280
it's hard to optimize since transaction

00:06:12,240 --> 00:06:16,960
manager doesn't

00:06:13,280 --> 00:06:20,319
not much information about strategy

00:06:16,960 --> 00:06:22,560
and the third nosql is supported

00:06:20,319 --> 00:06:25,280
so you need to write a program to do

00:06:22,560 --> 00:06:25,280
transactions

00:06:27,360 --> 00:06:31,600
so programming interface of scale db is

00:06:29,680 --> 00:06:35,919
basically a cloud interface

00:06:31,600 --> 00:06:38,479
so you can do put get scan delete

00:06:35,919 --> 00:06:40,160
and then it's got the beginning and the

00:06:38,479 --> 00:06:42,720
comment semantics

00:06:40,160 --> 00:06:44,400
so it actually uh has a start and then

00:06:42,720 --> 00:06:46,639
commit method

00:06:44,400 --> 00:06:48,000
so uh and you can have arbitrarily

00:06:46,639 --> 00:06:51,520
number of

00:06:48,000 --> 00:06:54,880
operations in between and then those are

00:06:51,520 --> 00:06:57,280
done executed atomically

00:06:54,880 --> 00:06:58,880
and in the architecture of scale db is a

00:06:57,280 --> 00:07:01,120
client coordinated

00:06:58,880 --> 00:07:02,639
so there is no dedicated middleware for

00:07:01,120 --> 00:07:05,680
scaledbb

00:07:02,639 --> 00:07:09,680
and it is usually used integrate

00:07:05,680 --> 00:07:09,680
integrated with web applications

00:07:11,120 --> 00:07:15,440
so the data model of scale db is a

00:07:13,360 --> 00:07:19,280
multi-dimensional map

00:07:15,440 --> 00:07:21,360
used in bigtable and as you know

00:07:19,280 --> 00:07:22,800
cassandra is also a successor of

00:07:21,360 --> 00:07:26,160
bigtable

00:07:22,800 --> 00:07:31,199
so basically alexandra and scldb

00:07:26,160 --> 00:07:33,199
share the same data model

00:07:31,199 --> 00:07:35,360
so let's look at the how it does

00:07:33,199 --> 00:07:37,599
transactions

00:07:35,360 --> 00:07:39,840
and then the transition protocol is

00:07:37,599 --> 00:07:44,160
based on cherry gastric alcohol

00:07:39,840 --> 00:07:47,520
uh proposed in icd 2015

00:07:44,160 --> 00:07:50,479
and roughly speaking um

00:07:47,520 --> 00:07:51,440
it's got that it does a two-phase commit

00:07:50,479 --> 00:07:55,199
to his comment

00:07:51,440 --> 00:07:56,319
on linearizable operations and then it

00:07:55,199 --> 00:07:58,879
uses a wall

00:07:56,319 --> 00:08:01,759
but the wall right there is logging but

00:07:58,879 --> 00:08:04,240
while records are all distributed

00:08:01,759 --> 00:08:06,720
and it is a single version optimistic

00:08:04,240 --> 00:08:10,160
concurrency control

00:08:06,720 --> 00:08:12,560
but the original protocol original work

00:08:10,160 --> 00:08:13,919
is kind of broken a little bit so we've

00:08:12,560 --> 00:08:16,960
extended the work

00:08:13,919 --> 00:08:21,120
by actually correcting the protocol and

00:08:16,960 --> 00:08:23,440
then adding the serializability support

00:08:21,120 --> 00:08:25,039
and the original protocol kind of

00:08:23,440 --> 00:08:27,840
requires two features

00:08:25,039 --> 00:08:28,240
in the underneath underlining databases

00:08:27,840 --> 00:08:31,759
one

00:08:28,240 --> 00:08:33,680
is a analyzable read and realizable

00:08:31,759 --> 00:08:36,959
conditional update

00:08:33,680 --> 00:08:37,919
and the other is an ability for each

00:08:36,959 --> 00:08:40,240
record

00:08:37,919 --> 00:08:42,479
and they both are fine for xander of

00:08:40,240 --> 00:08:42,479
course

00:08:43,680 --> 00:08:50,320
so let's look at the one by one

00:08:47,040 --> 00:08:53,040
so first transaction commit protocol

00:08:50,320 --> 00:08:54,000
and like i explained it's it's doing a

00:08:53,040 --> 00:08:55,519
two-phase commit

00:08:54,000 --> 00:08:58,160
protocol on you know lightable

00:08:55,519 --> 00:08:59,519
operations and the since cassandra using

00:08:58,160 --> 00:09:00,399
a pack source for lightweight

00:08:59,519 --> 00:09:02,800
transactions

00:09:00,399 --> 00:09:05,200
uh it is similar pretty similar to paxos

00:09:02,800 --> 00:09:05,200
commit

00:09:05,279 --> 00:09:08,800
and then the protocol is basically

00:09:07,279 --> 00:09:11,839
composed of two phases

00:09:08,800 --> 00:09:13,760
prepare phase and commit phase and

00:09:11,839 --> 00:09:16,080
in previous phase uh its prevalent

00:09:13,760 --> 00:09:18,480
regards to be committed

00:09:16,080 --> 00:09:19,440
and in the commit phase actually quite

00:09:18,480 --> 00:09:22,800
phased composer

00:09:19,440 --> 00:09:26,160
is composed of two phases sub phases and

00:09:22,800 --> 00:09:29,680
in commit phase one it's uh

00:09:26,160 --> 00:09:32,399
committing the status of transaction

00:09:29,680 --> 00:09:35,440
and in commit phase two is actually

00:09:32,399 --> 00:09:37,760
committing records

00:09:35,440 --> 00:09:39,040
but the in commit phase one this is the

00:09:37,760 --> 00:09:40,560
where this is the point

00:09:39,040 --> 00:09:43,040
where transaction is regarded as

00:09:40,560 --> 00:09:45,920
committed or pocket so it can be seen as

00:09:43,040 --> 00:09:48,959
dfs commit

00:09:45,920 --> 00:09:52,560
and for recovery uh it

00:09:48,959 --> 00:09:54,880
applies a lazy recovery so uncommitted

00:09:52,560 --> 00:09:56,320
transactions and committed records will

00:09:54,880 --> 00:09:58,880
be rolled forwarded

00:09:56,320 --> 00:10:02,399
or rolled back based on the status of

00:09:58,880 --> 00:10:02,399
transactions when they're ready

00:10:02,800 --> 00:10:07,279
i'm going to explain the protocol later

00:10:04,560 --> 00:10:07,279
with example

00:10:07,839 --> 00:10:15,200
so for a while so uh in scale dbe

00:10:12,160 --> 00:10:19,279
while records are all distributed into

00:10:15,200 --> 00:10:21,519
multiple records so for each record

00:10:19,279 --> 00:10:23,440
there is a after image and then before

00:10:21,519 --> 00:10:26,320
image

00:10:23,440 --> 00:10:26,959
and there is a table called coordinator

00:10:26,320 --> 00:10:31,360
table

00:10:26,959 --> 00:10:33,760
that manages a statuses of transactions

00:10:31,360 --> 00:10:35,440
and for each record there is application

00:10:33,760 --> 00:10:37,920
data of course

00:10:35,440 --> 00:10:39,120
and then there is a scalar db managed

00:10:37,920 --> 00:10:42,640
metadata

00:10:39,120 --> 00:10:44,640
marked as blue like a status version

00:10:42,640 --> 00:10:46,640
transaction id for after image

00:10:44,640 --> 00:10:48,399
and then for b4 image it's got the

00:10:46,640 --> 00:10:54,079
previous

00:10:48,399 --> 00:10:54,079
versions of the record to roll back

00:10:56,320 --> 00:11:00,079
so as for concurrency control it uses a

00:10:59,279 --> 00:11:04,079
single version

00:11:00,079 --> 00:11:06,000
optimistic concurrency control and then

00:11:04,079 --> 00:11:07,200
it can be seen as a simple

00:11:06,000 --> 00:11:10,880
implementation of

00:11:07,200 --> 00:11:11,839
snapshot isolation and then conflicts

00:11:10,880 --> 00:11:14,079
are detected by

00:11:11,839 --> 00:11:15,600
a lightweight transactions so we don't

00:11:14,079 --> 00:11:18,560
really use

00:11:15,600 --> 00:11:20,800
clocks not even using a hybrid logical

00:11:18,560 --> 00:11:23,360
clock

00:11:20,800 --> 00:11:24,720
and then it supports two isolation

00:11:23,360 --> 00:11:27,200
levels

00:11:24,720 --> 00:11:28,640
one is a weaker variant of natural

00:11:27,200 --> 00:11:31,680
snapshot isolation

00:11:28,640 --> 00:11:33,680
sometimes called really committed steps

00:11:31,680 --> 00:11:36,959
of isolation

00:11:33,680 --> 00:11:37,600
and then in addition to the animal is

00:11:36,959 --> 00:11:40,560
happening

00:11:37,600 --> 00:11:42,959
in snapshot isolation there is a chance

00:11:40,560 --> 00:11:46,160
of rediscu

00:11:42,959 --> 00:11:51,519
and it supports serializable as well so

00:11:46,160 --> 00:11:51,519
in that case there there is no anomalies

00:11:52,639 --> 00:11:56,320
so let's look at how it works with

00:11:54,839 --> 00:11:59,279
example

00:11:56,320 --> 00:12:02,320
so let's assume uh alexander aman is a

00:11:59,279 --> 00:12:04,160
cassandra manages a banking application

00:12:02,320 --> 00:12:06,240
and then there are two accounts one and

00:12:04,160 --> 00:12:09,839
two and they both have

00:12:06,240 --> 00:12:12,800
uh balance

00:12:09,839 --> 00:12:14,079
whatever the currency unit is and then

00:12:12,800 --> 00:12:15,680
client one comes

00:12:14,079 --> 00:12:17,440
and then he wants to do a payment

00:12:15,680 --> 00:12:20,560
transactions

00:12:17,440 --> 00:12:24,079
so uh since it's a snapshot isolation it

00:12:20,560 --> 00:12:26,079
first reads both records into its local

00:12:24,079 --> 00:12:28,639
memory space

00:12:26,079 --> 00:12:29,920
and then it's doing a transactions

00:12:28,639 --> 00:12:32,320
locally

00:12:29,920 --> 00:12:34,079
so in this case transferring 20 from one

00:12:32,320 --> 00:12:37,600
to two

00:12:34,079 --> 00:12:40,480
the actual data is application data

00:12:37,600 --> 00:12:42,399
is updated and then the status

00:12:40,480 --> 00:12:44,079
transmission id versions are also

00:12:42,399 --> 00:12:47,120
updated

00:12:44,079 --> 00:12:47,839
so in the previous case the updated

00:12:47,120 --> 00:12:51,839
records

00:12:47,839 --> 00:12:55,040
will be reflected to alexandra

00:12:51,839 --> 00:12:58,000
with conditional update and

00:12:55,040 --> 00:12:58,880
it's up it's gonna update only if the

00:12:58,000 --> 00:13:01,440
versions

00:12:58,880 --> 00:13:03,760
and the transaction ids are the same as

00:13:01,440 --> 00:13:07,120
the ones it read

00:13:03,760 --> 00:13:10,480
so in this case uh the versions

00:13:07,120 --> 00:13:11,600
and transaction ids are xx yy five and

00:13:10,480 --> 00:13:14,079
four

00:13:11,600 --> 00:13:15,360
which is the same as the versions

00:13:14,079 --> 00:13:18,880
transaction ids

00:13:15,360 --> 00:13:21,040
it read so it's fine so it's properly

00:13:18,880 --> 00:13:22,880
prepared

00:13:21,040 --> 00:13:24,240
but let's assume there is another

00:13:22,880 --> 00:13:28,800
transaction

00:13:24,240 --> 00:13:28,800
just come around at the same time

00:13:28,880 --> 00:13:34,880
and then it read the same record as

00:13:32,880 --> 00:13:37,200
transaction one

00:13:34,880 --> 00:13:38,320
and it does the similar transaction

00:13:37,200 --> 00:13:41,279
transferring 10

00:13:38,320 --> 00:13:41,279
from one to two

00:13:41,360 --> 00:13:45,199
and then those changes will be reflected

00:13:43,600 --> 00:13:47,600
to the cassandra

00:13:45,199 --> 00:13:48,399
but in this case it's gonna be failed

00:13:47,600 --> 00:13:52,000
because

00:13:48,399 --> 00:13:53,360
uh the condition mismatch uh the data is

00:13:52,000 --> 00:13:56,959
already updated

00:13:53,360 --> 00:13:58,560
by transmission one so it's a tx one and

00:13:56,959 --> 00:14:01,680
six and five

00:13:58,560 --> 00:14:04,560
which is not same as the

00:14:01,680 --> 00:14:05,040
the version it's red so in this case it

00:14:04,560 --> 00:14:08,560
failed

00:14:05,040 --> 00:14:11,680
and then it's gonna be aborted

00:14:08,560 --> 00:14:13,760
so now client one wins for the prepare

00:14:11,680 --> 00:14:15,839
and it's now going to the commit phase

00:14:13,760 --> 00:14:18,240
one

00:14:15,839 --> 00:14:20,399
uh client one tries to commit the

00:14:18,240 --> 00:14:22,639
transaction one

00:14:20,399 --> 00:14:25,040
and then it's gonna update the status

00:14:22,639 --> 00:14:27,440
record like this

00:14:25,040 --> 00:14:28,240
and then it also uses a lightweight

00:14:27,440 --> 00:14:30,880
transaction

00:14:28,240 --> 00:14:34,000
and then that condition is updating only

00:14:30,880 --> 00:14:35,600
if the transmission id does not exist

00:14:34,000 --> 00:14:37,360
so in this case there is no

00:14:35,600 --> 00:14:41,120
transactional id1 so

00:14:37,360 --> 00:14:41,120
it can be successfully committed

00:14:42,320 --> 00:14:49,040
and commit phase two uh trend one

00:14:46,000 --> 00:14:52,800
is going to update the records

00:14:49,040 --> 00:14:54,639
from prepared to committed

00:14:52,800 --> 00:14:56,800
and in this case also it uses a

00:14:54,639 --> 00:15:00,000
conditional update

00:14:56,800 --> 00:15:03,600
and it updates only if the record

00:15:00,000 --> 00:15:05,360
is prepared by the transaction id

00:15:03,600 --> 00:15:06,800
and then the reason why we are doing

00:15:05,360 --> 00:15:10,000
checking the transaction id

00:15:06,800 --> 00:15:11,839
is that uh you know um there might be a

00:15:10,000 --> 00:15:15,120
case where

00:15:11,839 --> 00:15:17,440
they are prepared and after that

00:15:15,120 --> 00:15:18,959
and the transaction comes in and then

00:15:17,440 --> 00:15:21,440
recover the data

00:15:18,959 --> 00:15:23,680
and they also issued another transaction

00:15:21,440 --> 00:15:28,079
to update those data

00:15:23,680 --> 00:15:30,399
so by checking the transaction id we can

00:15:28,079 --> 00:15:32,800
avoid overwriting the all the updated

00:15:30,399 --> 00:15:32,800
data

00:15:34,160 --> 00:15:39,680
so for recovery

00:15:37,279 --> 00:15:41,920
recovery is lazily done basically when

00:15:39,680 --> 00:15:46,240
oracle is read

00:15:41,920 --> 00:15:49,839
so let's see how it's going to handle

00:15:46,240 --> 00:15:50,800
so there is a phase and then what if a

00:15:49,839 --> 00:15:54,399
transaction

00:15:50,800 --> 00:15:56,480
crashes before prepare phase

00:15:54,399 --> 00:15:57,600
in that case nothing happened in the

00:15:56,480 --> 00:15:59,839
database

00:15:57,600 --> 00:16:02,079
so nothing needs to be done basically

00:15:59,839 --> 00:16:04,000
just leave it as it is

00:16:02,079 --> 00:16:05,600
and then the local memory space will be

00:16:04,000 --> 00:16:08,800
cleared out by

00:16:05,600 --> 00:16:11,839
automatically by jvm and then

00:16:08,800 --> 00:16:14,959
what if a transaction

00:16:11,839 --> 00:16:18,800
crashes after prepare phase

00:16:14,959 --> 00:16:22,320
but before preparing commit phase one

00:16:18,800 --> 00:16:24,959
in that case records are prepared but

00:16:22,320 --> 00:16:25,600
there is no status for the transaction

00:16:24,959 --> 00:16:28,079
so

00:16:25,600 --> 00:16:30,079
the records will be rolled back by other

00:16:28,079 --> 00:16:33,360
transactions lazily

00:16:30,079 --> 00:16:33,360
using before image

00:16:33,440 --> 00:16:41,040
so what if a transaction crash is after

00:16:36,880 --> 00:16:43,360
commit phase one before commit phase two

00:16:41,040 --> 00:16:44,399
in that case records are prepared and

00:16:43,360 --> 00:16:47,519
then there is a

00:16:44,399 --> 00:16:50,639
record in transactional status table

00:16:47,519 --> 00:16:54,240
so those records will be all forwarded

00:16:50,639 --> 00:16:55,120
by another transaction easily updating

00:16:54,240 --> 00:16:57,839
status to

00:16:55,120 --> 00:16:57,839
committed

00:16:59,519 --> 00:17:03,839
so um regarding industrializable

00:17:02,000 --> 00:17:06,799
strategy

00:17:03,839 --> 00:17:08,480
the basic strategy is we're trying to

00:17:06,799 --> 00:17:12,000
avoid the anti-dependency

00:17:08,480 --> 00:17:14,559
which is a root cause of the um

00:17:12,000 --> 00:17:16,480
breaking industrializability in snapshot

00:17:14,559 --> 00:17:20,000
isolation

00:17:16,480 --> 00:17:20,400
so we have two implementations one is

00:17:20,000 --> 00:17:23,120
called

00:17:20,400 --> 00:17:25,360
extra right which is uh actually

00:17:23,120 --> 00:17:30,559
converting breeze into rights

00:17:25,360 --> 00:17:34,240
so there is no anti-dependencies

00:17:30,559 --> 00:17:37,840
and then the other is called extra read

00:17:34,240 --> 00:17:40,799
which is actually checking the reset

00:17:37,840 --> 00:17:42,720
after prepared to see if it's not

00:17:40,799 --> 00:17:44,840
updated by other transactions

00:17:42,720 --> 00:17:46,240
so it's actually checking the

00:17:44,840 --> 00:17:49,039
anti-dependency

00:17:46,240 --> 00:17:51,840
and if there is anti-dependency it's

00:17:49,039 --> 00:17:51,840
going to be imported

00:17:52,880 --> 00:17:59,039
so here's a benchmark result with

00:17:56,080 --> 00:18:00,400
scale db on cassandra in a hundred node

00:17:59,039 --> 00:18:04,160
cluster

00:18:00,400 --> 00:18:07,679
so um with two different workloads

00:18:04,160 --> 00:18:11,280
uh as you can see um it's almost almost

00:18:07,679 --> 00:18:14,799
linearly scaled and then it achieves

00:18:11,280 --> 00:18:17,760
uh 190 scalability

00:18:14,799 --> 00:18:17,760
in both workloads

00:18:19,200 --> 00:18:23,200
so um we've done lots of we've done lots

00:18:21,919 --> 00:18:26,320
of lots of verification

00:18:23,200 --> 00:18:29,280
tests uh with jackson

00:18:26,320 --> 00:18:31,840
and then you're just gonna one who is

00:18:29,280 --> 00:18:35,120
speaking about the jepsen test in our

00:18:31,840 --> 00:18:38,160
uh use cases and

00:18:35,120 --> 00:18:40,320
um we've also done the

00:18:38,160 --> 00:18:42,960
transition commit protocol verification

00:18:40,320 --> 00:18:47,039
with dla plus tla plus is a

00:18:42,960 --> 00:18:50,559
formal verification language and then

00:18:47,039 --> 00:18:52,880
we've verified lots of lots of uh cldb

00:18:50,559 --> 00:18:52,880
code

00:18:53,600 --> 00:18:57,280
i think i'm gonna stop here and then

00:18:56,480 --> 00:19:00,400
switching to

00:18:57,280 --> 00:19:03,919
yuji yes uh

00:19:00,400 --> 00:19:05,919
hi i'm yuji i'm working on

00:19:03,919 --> 00:19:09,280
improving the performance and the

00:19:05,919 --> 00:19:12,720
reliability of the scale db

00:19:09,280 --> 00:19:15,679
hi i am going to talk about

00:19:12,720 --> 00:19:16,720
a new control sync mode called group

00:19:15,679 --> 00:19:21,280
mode

00:19:16,720 --> 00:19:21,280
and json test for lwt

00:19:22,240 --> 00:19:29,120
why we need a new mode

00:19:26,160 --> 00:19:30,720
scalar db transaction relies on custom

00:19:29,120 --> 00:19:34,480
graph durability

00:19:30,720 --> 00:19:36,080
and performance while the functions to

00:19:34,480 --> 00:19:39,840
protect our data

00:19:36,080 --> 00:19:42,640
is controlled and also the commentary

00:19:39,840 --> 00:19:43,039
thing affects the performance because

00:19:42,640 --> 00:19:47,440
the

00:19:43,039 --> 00:19:53,360
thing because the sync causes many i o

00:19:47,440 --> 00:19:56,799
so i proposed a new mode

00:19:53,360 --> 00:20:01,280
we can choose group mode in cassandra

00:19:56,799 --> 00:20:04,960
4.0 group mode satisfies

00:20:01,280 --> 00:20:08,640
data durability and vdss right

00:20:04,960 --> 00:20:11,760
i o to the material disk

00:20:08,640 --> 00:20:12,320
group mode since merge for controls at

00:20:11,760 --> 00:20:15,200
once

00:20:12,320 --> 00:20:15,200
and periodically

00:20:16,159 --> 00:20:23,520
as you know commit road is the role of

00:20:20,000 --> 00:20:26,960
all mutations to a cassandra node

00:20:23,520 --> 00:20:29,600
control provides durability in the case

00:20:26,960 --> 00:20:33,200
of class

00:20:29,600 --> 00:20:36,400
all right applies from applied mutations

00:20:33,200 --> 00:20:37,360
to the name table and appends commit

00:20:36,400 --> 00:20:40,000
roles

00:20:37,360 --> 00:20:40,000
to the disk

00:20:40,960 --> 00:20:45,679
after crash happens the node recovers

00:20:44,720 --> 00:20:49,760
the data

00:20:45,679 --> 00:20:54,480
from the macros because all data

00:20:49,760 --> 00:20:54,480
on the name table have been gone

00:20:55,679 --> 00:21:03,840
commit raw sync mode is how to sync

00:20:59,039 --> 00:21:03,840
the controls to the disk

00:21:04,320 --> 00:21:12,480
i would like to introduce the existing

00:21:07,440 --> 00:21:15,600
modes before the detail of group mode

00:21:12,480 --> 00:21:16,400
we can choose the two modes which are

00:21:15,600 --> 00:21:21,600
periodic

00:21:16,400 --> 00:21:24,799
and bad so the hot mode is periodic

00:21:21,600 --> 00:21:28,080
this mode sinks controls to the disk

00:21:24,799 --> 00:21:31,440
periodically it's

00:21:28,080 --> 00:21:34,640
um asynchronous thing

00:21:31,440 --> 00:21:37,200
not to wait for the completion of the

00:21:34,640 --> 00:21:41,440
automator sync

00:21:37,200 --> 00:21:44,480
this mode can minimize the number of io

00:21:41,440 --> 00:21:45,280
because it seems much for controls at

00:21:44,480 --> 00:21:49,440
once

00:21:45,280 --> 00:21:54,640
at the point the diagram shows

00:21:49,440 --> 00:21:54,640
5 controls can be synced at once

00:21:56,480 --> 00:22:00,320
however it is possible to lose some

00:21:59,280 --> 00:22:04,080
controls

00:22:00,320 --> 00:22:04,640
when crash in this mode appended commit

00:22:04,080 --> 00:22:06,799
rolls

00:22:04,640 --> 00:22:08,559
after the rushing point are not

00:22:06,799 --> 00:22:11,280
processed

00:22:08,559 --> 00:22:12,480
and this mode responds with arc

00:22:11,280 --> 00:22:15,039
immediately

00:22:12,480 --> 00:22:15,600
regardless results of commit roles are

00:22:15,039 --> 00:22:18,080
synced

00:22:15,600 --> 00:22:18,080
or not

00:22:19,440 --> 00:22:24,840
the second mode is batch

00:22:22,480 --> 00:22:26,400
batch mode thinks commit runs

00:22:24,840 --> 00:22:29,760
immediately

00:22:26,400 --> 00:22:34,880
it's synchronous thing so the

00:22:29,760 --> 00:22:38,080
arc means the lights are being persisted

00:22:34,880 --> 00:22:41,120
if a crash happens they can be covered

00:22:38,080 --> 00:22:42,159
from the commit drugs the data

00:22:41,120 --> 00:22:45,840
durability

00:22:42,159 --> 00:22:49,200
is satisfied by this mode however

00:22:45,840 --> 00:22:50,640
a lot of ios might degrade the

00:22:49,200 --> 00:22:54,000
throughput

00:22:50,640 --> 00:22:57,840
a parameter batch window is the

00:22:54,000 --> 00:23:01,280
maximum length of your window to sync

00:22:57,840 --> 00:23:05,520
it seems not working from

00:23:01,280 --> 00:23:08,080
the mode name but you'd expect

00:23:05,520 --> 00:23:08,720
this mode tries to think more commit

00:23:08,080 --> 00:23:12,400
roles

00:23:08,720 --> 00:23:15,760
at once but much more

00:23:12,400 --> 00:23:18,799
thinks only few commit roles which

00:23:15,760 --> 00:23:22,640
which are requested concurrently at that

00:23:18,799 --> 00:23:25,200
same point that's why many

00:23:22,640 --> 00:23:27,520
small i o are requested to the commuter

00:23:25,200 --> 00:23:27,520
disk

00:23:28,960 --> 00:23:33,120
i summarize the issues of the existing

00:23:31,600 --> 00:23:36,640
modes

00:23:33,120 --> 00:23:38,240
first periodic mode might lose comet

00:23:36,640 --> 00:23:42,080
rocks

00:23:38,240 --> 00:23:44,320
second patch mode increases a lot of

00:23:42,080 --> 00:23:47,679
small io

00:23:44,320 --> 00:23:49,760
to store critical data we cannot choose

00:23:47,679 --> 00:23:51,520
periodic mode due to the lack of

00:23:49,760 --> 00:23:54,559
durability

00:23:51,520 --> 00:23:58,000
in fact we recommend batch mode

00:23:54,559 --> 00:24:01,039
for scale db however

00:23:58,000 --> 00:24:04,240
we have a concern about the

00:24:01,039 --> 00:24:06,000
degradation by increasing i o in batch

00:24:04,240 --> 00:24:10,720
mode

00:24:06,000 --> 00:24:10,720
so i propose a new sync mode

00:24:11,440 --> 00:24:19,360
group mode things commit roles advance

00:24:15,039 --> 00:24:23,279
and periodically like periodic mode

00:24:19,360 --> 00:24:25,600
like batch mode its synchronous sync

00:24:23,279 --> 00:24:26,880
which responds after sinking the

00:24:25,600 --> 00:24:30,720
controls

00:24:26,880 --> 00:24:34,400
does not return the arc immediately

00:24:30,720 --> 00:24:39,679
group mode achieves both durability

00:24:34,400 --> 00:24:42,880
and reducing io by grouping control zinc

00:24:39,679 --> 00:24:47,039
the diagram shows the first commit drop

00:24:42,880 --> 00:24:50,880
is synced at the past thing point

00:24:47,039 --> 00:24:54,559
after the thing the right is completed

00:24:50,880 --> 00:24:58,320
the last four controls are synced

00:24:54,559 --> 00:25:01,440
at once at the next thing point

00:24:58,320 --> 00:25:03,679
after the thing the four rights are

00:25:01,440 --> 00:25:07,360
completed

00:25:03,679 --> 00:25:14,080
in this example this node issued only

00:25:07,360 --> 00:25:17,279
two io or five commitures

00:25:14,080 --> 00:25:18,320
i evaluated the performance of three

00:25:17,279 --> 00:25:22,799
node cluster

00:25:18,320 --> 00:25:26,559
with slow disks which are 200 iops

00:25:22,799 --> 00:25:27,600
like hard disk drive i configured the

00:25:26,559 --> 00:25:30,960
interval time

00:25:27,600 --> 00:25:35,679
to think the controls 10 milliseconds

00:25:30,960 --> 00:25:35,679
and 15 milliseconds for group mode

00:25:38,159 --> 00:25:42,080
the left graph shows the throughput

00:25:40,880 --> 00:25:44,480
which is

00:25:42,080 --> 00:25:45,919
the number of upgrade operations per

00:25:44,480 --> 00:25:49,279
second

00:25:45,919 --> 00:25:52,559
in batch mode the throughput of

00:25:49,279 --> 00:25:56,400
2 milliseconds and 10 milliseconds but

00:25:52,559 --> 00:26:00,880
window are the same group mode

00:25:56,400 --> 00:26:00,880
looks a bit better than batch mode

00:26:01,760 --> 00:26:08,320
between 10 and 32 threads the throughput

00:26:05,760 --> 00:26:09,600
of group mode is better than certain

00:26:08,320 --> 00:26:12,640
department

00:26:09,600 --> 00:26:15,440
up to 75 percent

00:26:12,640 --> 00:26:16,559
the latency is also better than that of

00:26:15,440 --> 00:26:19,679
patch mode

00:26:16,559 --> 00:26:22,960
or lower support

00:26:19,679 --> 00:26:26,000
as lwt many incoming draws

00:26:22,960 --> 00:26:26,960
are issued to update not only the user

00:26:26,000 --> 00:26:31,279
table

00:26:26,960 --> 00:26:31,279
but also the purchase table

00:26:32,799 --> 00:26:39,039
without lwt the latency

00:26:36,559 --> 00:26:40,080
of batch mode is better than that of

00:26:39,039 --> 00:26:44,559
group mode

00:26:40,080 --> 00:26:47,279
in 100 oops that's because

00:26:44,559 --> 00:26:48,000
few commit draws are issued and batch

00:26:47,279 --> 00:26:53,840
mode

00:26:48,000 --> 00:26:53,840
does not have to wait for a single point

00:26:54,480 --> 00:27:03,360
when to use when to use group mode

00:26:58,880 --> 00:27:03,360
it's when durability is important

00:27:03,679 --> 00:27:06,720
in the case where we cannot lose

00:27:05,679 --> 00:27:11,919
critical data

00:27:06,720 --> 00:27:15,440
even if all nodes are down unexpectedly

00:27:11,919 --> 00:27:17,360
and when the disk iops is lower than the

00:27:15,440 --> 00:27:20,799
request arrival time

00:27:17,360 --> 00:27:24,080
arrival rate the slope degradation

00:27:20,799 --> 00:27:27,120
can be avoided by decreasing i o

00:27:24,080 --> 00:27:27,840
in my aberration it's when the number of

00:27:27,120 --> 00:27:32,000
threads

00:27:27,840 --> 00:27:35,039
is between 8 and 32

00:27:32,000 --> 00:27:36,080
we can choose batch mode to make each

00:27:35,039 --> 00:27:40,159
latency

00:27:36,080 --> 00:27:44,000
go with our password disk

00:27:40,159 --> 00:27:47,600
batch mode can group as many commuters

00:27:44,000 --> 00:27:50,799
as group mode in high concurrency

00:27:47,600 --> 00:27:51,279
each latency is lower by returning the

00:27:50,799 --> 00:27:53,919
arc

00:27:51,279 --> 00:27:53,919
immediately

00:27:56,399 --> 00:28:07,840
my next topic is jefferson test for lwg

00:28:03,120 --> 00:28:10,880
why we do japan test for lwt

00:28:07,840 --> 00:28:14,840
scale tv transaction relies on

00:28:10,880 --> 00:28:16,320
lwt on cassandra how can we check the

00:28:14,840 --> 00:28:19,760
correctness

00:28:16,320 --> 00:28:23,279
gpsm provides functions to inject ports

00:28:19,760 --> 00:28:26,320
and to verify the linearizability

00:28:23,279 --> 00:28:28,000
data starts made gypsum tests about five

00:28:26,320 --> 00:28:31,760
years ago

00:28:28,000 --> 00:28:37,120
however it has not been maintained

00:28:31,760 --> 00:28:37,120
that's why i made japan test for lwp

00:28:38,880 --> 00:28:45,840
our test is based on data stocks test

00:28:42,320 --> 00:28:49,120
it has lwd patch

00:28:45,840 --> 00:28:52,399
set map and counter test

00:28:49,120 --> 00:28:53,360
with various ports which are network

00:28:52,399 --> 00:28:56,559
ports

00:28:53,360 --> 00:28:59,840
node crash clock drift

00:28:56,559 --> 00:28:59,840
and node joining

00:29:01,919 --> 00:29:05,919
i will introduce our contributions to

00:29:04,720 --> 00:29:09,440
jefferson testing

00:29:05,919 --> 00:29:12,480
for cassandra first

00:29:09,440 --> 00:29:15,760
we replaced casa quality this area as

00:29:12,480 --> 00:29:18,720
groja rapper or cassandra

00:29:15,760 --> 00:29:20,960
that's because casa quality has not been

00:29:18,720 --> 00:29:23,440
maintained

00:29:20,960 --> 00:29:24,080
i faced on some buzz about getting

00:29:23,440 --> 00:29:27,600
results

00:29:24,080 --> 00:29:31,200
and cassandra driver

00:29:27,600 --> 00:29:32,799
second we reload tests with the latest

00:29:31,200 --> 00:29:36,240
steps

00:29:32,799 --> 00:29:39,520
the previous lwt test held

00:29:36,240 --> 00:29:41,440
failed due to auto memory to check the

00:29:39,520 --> 00:29:44,880
long loads

00:29:41,440 --> 00:29:48,559
but new gypsum can check the loads

00:29:44,880 --> 00:29:54,159
by dividing the test to some parts

00:29:48,559 --> 00:29:57,120
so the current lwd test works well

00:29:54,159 --> 00:29:59,960
i'm honestly i'm not matched some

00:29:57,120 --> 00:30:03,360
modifications for the latest gypsy

00:29:59,960 --> 00:30:06,559
0.2.0 due to that sense bar

00:30:03,360 --> 00:30:10,200
i i reported the bug and

00:30:06,559 --> 00:30:13,360
it was fixed the new version

00:30:10,200 --> 00:30:17,600
0.2.1 is released today

00:30:13,360 --> 00:30:17,600
so i will match them soon

00:30:19,520 --> 00:30:23,200
i'm reporting the result of cassandra

00:30:22,480 --> 00:30:26,000
tests

00:30:23,200 --> 00:30:27,520
to the mailing list where a new button

00:30:26,000 --> 00:30:29,840
is released

00:30:27,520 --> 00:30:31,760
there are simple tests which are one

00:30:29,840 --> 00:30:35,279
minute one minutes testing

00:30:31,760 --> 00:30:38,480
without quote injection

00:30:35,279 --> 00:30:42,880
i'm testing cassandra 4.0

00:30:38,480 --> 00:30:47,039
beta with 4 injections every week

00:30:42,880 --> 00:30:49,919
where test runs repeatedly sometimes

00:30:47,039 --> 00:30:52,480
a node couldn't join general xenocluster

00:30:49,919 --> 00:30:55,600
before starting a test

00:30:52,480 --> 00:31:00,159
i'm not sure what happens this

00:30:55,600 --> 00:31:05,840
issue didn't happen with 4.0 output

00:31:00,159 --> 00:31:05,840
so i i will i will indicate it

00:31:06,080 --> 00:31:11,360
and the current test is maintained by

00:31:08,960 --> 00:31:14,720
scala repository

00:31:11,360 --> 00:31:19,039
we think the test can be migrated

00:31:14,720 --> 00:31:19,039
to the official cassandra because

00:31:19,200 --> 00:31:27,279
since it is not specific to scalar tv

00:31:22,960 --> 00:31:29,919
so we would we would like to know

00:31:27,279 --> 00:31:29,919
your thought

00:31:31,039 --> 00:31:47,840
okay that's the one thank you

00:31:48,399 --> 00:31:55,519
all right uh thank you uh hiroyuki

00:31:52,399 --> 00:31:56,720
and yuji for that presentation um just

00:31:55,519 --> 00:31:58,480
throw it out to the floor now if there

00:31:56,720 --> 00:32:01,840
are any questions please drop them into

00:31:58,480 --> 00:32:01,840
the chat channel

00:32:32,399 --> 00:32:39,360
all right so i've got one question um

00:32:36,640 --> 00:32:40,640
will the where whereabouts is the code

00:32:39,360 --> 00:32:44,320
located

00:32:40,640 --> 00:32:46,960
for scala is it going will it be made

00:32:44,320 --> 00:32:46,960
open source

00:32:48,240 --> 00:32:52,720
i'm sorry i couldn't get it like what's

00:32:50,320 --> 00:32:52,720
the question

00:32:53,519 --> 00:32:59,120
uh is the is whereabouts is the code

00:32:56,559 --> 00:33:02,000
located for scala

00:32:59,120 --> 00:33:04,640
the code is located in scalar labs rebel

00:33:02,000 --> 00:33:04,640
and github

00:33:08,640 --> 00:33:12,799
cool and there are instructions to build

00:33:10,880 --> 00:33:16,080
and compile it

00:33:12,799 --> 00:33:18,960
um and run it and plug it in

00:33:16,080 --> 00:33:20,559
right right yeah like uh we have a

00:33:18,960 --> 00:33:23,039
getting started document we have a

00:33:20,559 --> 00:33:26,399
design document and then

00:33:23,039 --> 00:33:30,159
yeah so basically you can start um

00:33:26,399 --> 00:33:30,159
like after this

00:33:30,399 --> 00:33:38,159
fantastic yeah and we have also uh

00:33:33,760 --> 00:33:41,760
stack overflow um tags so that uh

00:33:38,159 --> 00:33:44,640
if you have any questions you can

00:33:41,760 --> 00:33:44,640
feel free to ask

00:33:50,240 --> 00:33:55,120
all right um well i think we'll wind the

00:33:54,000 --> 00:33:58,960
session up here then

00:33:55,120 --> 00:34:00,720
um so thank you again uh hiroyoki and

00:33:58,960 --> 00:34:01,760
yuji for an excellent presentation and

00:34:00,720 --> 00:34:02,720
thank you for everyone else that

00:34:01,760 --> 00:34:05,919
presented as well

00:34:02,720 --> 00:34:07,840
today um had some really interesting

00:34:05,919 --> 00:34:09,919
presentations so

00:34:07,840 --> 00:34:10,960
yeah thanks again everyone thank you

00:34:09,919 --> 00:34:21,839
very much

00:34:10,960 --> 00:34:21,839
thank you

00:34:37,520 --> 00:34:39,599

YouTube URL: https://www.youtube.com/watch?v=r16ZYS-SIFw


