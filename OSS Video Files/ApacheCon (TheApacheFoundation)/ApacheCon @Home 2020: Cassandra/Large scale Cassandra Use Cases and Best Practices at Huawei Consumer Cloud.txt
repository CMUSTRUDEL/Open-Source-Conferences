Title: Large scale Cassandra Use Cases and Best Practices at Huawei Consumer Cloud
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Cassandra
Description: 
	Large scale Cassandra Use Cases and Best Practices at Huawei Consumer Cloud
Duican Huang

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Cassandra is widely used in key business scenarios in Huawei Consumer Cloud. You can find Cassandra databases serving as the real-time data store behind almost all Huawei consumer electronic products that are used by billions of people in China and the rest of the world. With a long history of Cassandra adoption ever since 2010, Huawei Consumer Cloudâ€™s Cassandra deployments have grown to 30,000+ nodes, supporting more than 10 million operations per second with average latency of 4ms, and the maximum number of table records reaches 300 billion. Along this journey, we have gained a lot of experience in data modeling, fine-tuning leveled compaction with high node density, day-to-day operations such as repair and handling tombstones, monitoring and problem identification and quick resolution under very tight SLA, which we are thrilled to share with the community. We also summarized our lessons learned and best practices in managing those low-latency, high-concurrency and mission-critical use cases.

Duican Huang is a Huawei Senior R&D Engineer
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,160 --> 00:00:31,119
i can start

00:00:24,960 --> 00:00:36,399
right okay

00:00:31,119 --> 00:00:40,399
okay hello everyone this is wanduytan

00:00:36,399 --> 00:00:40,399
i'm from huawei consumer cloud

00:00:40,719 --> 00:00:45,039
i'm very happy and honest to share our

00:00:43,920 --> 00:00:48,320
experience

00:00:45,039 --> 00:00:48,320
in using cassandra

00:00:50,239 --> 00:00:53,360
this is also the first time i

00:00:52,160 --> 00:00:56,719
participate in

00:00:53,360 --> 00:00:59,920
the rpg conference so

00:00:56,719 --> 00:01:03,440
go straight to the topics i share

00:00:59,920 --> 00:01:04,159
my topic is large-scale cassandra use

00:01:03,440 --> 00:01:07,119
case

00:01:04,159 --> 00:01:09,520
and the best practice at huawei consumer

00:01:07,119 --> 00:01:09,520
cloud

00:01:11,040 --> 00:01:15,920
this sharing is mainly divided into

00:01:14,000 --> 00:01:19,520
three parts

00:01:15,920 --> 00:01:22,799
the first part is

00:01:19,520 --> 00:01:26,400
mainly is using cassandra

00:01:22,799 --> 00:01:30,079
in huawei consumer cloud this part

00:01:26,400 --> 00:01:34,159
meaning talk about how we use cassandra

00:01:30,079 --> 00:01:36,079
the second part is lessons learned with

00:01:34,159 --> 00:01:38,400
using cassandra in production

00:01:36,079 --> 00:01:41,520
environments

00:01:38,400 --> 00:01:42,799
this parts talk about some issues we

00:01:41,520 --> 00:01:46,079
have encountered

00:01:42,799 --> 00:01:46,960
in the production environment and the

00:01:46,079 --> 00:01:50,479
last part

00:01:46,960 --> 00:01:54,399
is best practice for using cassandra

00:01:50,479 --> 00:01:56,640
in huawei consumer cloud for this part

00:01:54,399 --> 00:01:57,920
i would like to talk about some

00:01:56,640 --> 00:02:01,520
experience

00:01:57,920 --> 00:02:06,000
from all the issues we have encountered

00:02:01,520 --> 00:02:06,000
some of them became our application

00:02:06,840 --> 00:02:12,480
rules

00:02:08,319 --> 00:02:15,040
so why did we choose cassandra

00:02:12,480 --> 00:02:16,800
the reason for choosing cassandra is

00:02:15,040 --> 00:02:20,560
probably the same as

00:02:16,800 --> 00:02:23,760
most cassandra user cassandra has many

00:02:20,560 --> 00:02:25,440
nice features such as decentralized

00:02:23,760 --> 00:02:29,440
architecture

00:02:25,440 --> 00:02:32,319
simple deployment ease of maintenance

00:02:29,440 --> 00:02:33,519
and it has good scalability high

00:02:32,319 --> 00:02:36,800
performance

00:02:33,519 --> 00:02:39,760
multi-site high availability

00:02:36,800 --> 00:02:41,519
active open source community stable

00:02:39,760 --> 00:02:44,239
evolution version

00:02:41,519 --> 00:02:45,200
and it is easy to monitor and it

00:02:44,239 --> 00:02:51,840
provides

00:02:45,200 --> 00:02:51,840
microscope level monitoring interface

00:02:52,160 --> 00:02:59,920
we also have made a monitoring system

00:02:55,599 --> 00:02:59,920
for cassandra based on jmx

00:03:02,720 --> 00:03:08,080
our process of using cassandra is

00:03:05,680 --> 00:03:11,599
divided into two phase

00:03:08,080 --> 00:03:15,760
the original version of cassandra we use

00:03:11,599 --> 00:03:15,760
was still a very early version

00:03:16,720 --> 00:03:23,519
the version at that time was not as much

00:03:20,000 --> 00:03:27,200
as it is now at that time

00:03:23,519 --> 00:03:31,120
our understanding of nosql was still

00:03:27,200 --> 00:03:34,159
unclear and cassandra was used as

00:03:31,120 --> 00:03:38,319
a relational database

00:03:34,159 --> 00:03:41,519
we made a lot of change to cassandra

00:03:38,319 --> 00:03:44,879
we modified the source code to deeply

00:03:41,519 --> 00:03:45,360
customize the storage layer to adapt to

00:03:44,879 --> 00:03:49,440
the

00:03:45,360 --> 00:03:52,560
telecom level applications

00:03:49,440 --> 00:03:55,599
on the driver side we in

00:03:52,560 --> 00:03:59,040
capture the interface to

00:03:55,599 --> 00:04:02,159
of stripped defining jdbc light

00:03:59,040 --> 00:04:05,439
interface and provide

00:04:02,159 --> 00:04:05,439
sqlite statements

00:04:05,680 --> 00:04:12,720
on the access layer we provide

00:04:08,879 --> 00:04:15,760
sql pathing module which

00:04:12,720 --> 00:04:19,919
used to pass ddl and dml statements

00:04:15,760 --> 00:04:24,320
of sql syntax and convert them

00:04:19,919 --> 00:04:28,280
into native mutation objects

00:04:24,320 --> 00:04:31,440
on the storage layer we modified the

00:04:28,280 --> 00:04:32,000
serialization and the serialization for

00:04:31,440 --> 00:04:35,680
mates

00:04:32,000 --> 00:04:39,360
to reduce storage space

00:04:35,680 --> 00:04:43,759
and modify the query iterators

00:04:39,360 --> 00:04:47,600
logic to adapt to the new serialization

00:04:43,759 --> 00:04:51,840
and deserialization for mate

00:04:47,600 --> 00:04:56,240
in addition we provide a series

00:04:51,840 --> 00:04:56,880
of relational database capabilities such

00:04:56,240 --> 00:05:00,479
as

00:04:56,880 --> 00:05:03,120
secondary indexes stores procedure

00:05:00,479 --> 00:05:03,120
and trigger

00:05:03,680 --> 00:05:11,039
the result is that we only use cassandra

00:05:07,199 --> 00:05:13,680
on a small scale and even get up at last

00:05:11,039 --> 00:05:14,320
not that did it help the business

00:05:13,680 --> 00:05:17,280
success

00:05:14,320 --> 00:05:17,280
through cassandra

00:05:18,160 --> 00:05:22,240
the experience of using cassandra for

00:05:20,880 --> 00:05:25,520
the first time

00:05:22,240 --> 00:05:28,639
also taught us two lessons

00:05:25,520 --> 00:05:32,320
firstly cassandra is essential

00:05:28,639 --> 00:05:36,479
a nosql database don't change

00:05:32,320 --> 00:05:39,680
it to sql secondly modify the

00:05:36,479 --> 00:05:40,240
source code leading to the further and

00:05:39,680 --> 00:05:44,560
further

00:05:40,240 --> 00:05:44,560
drift from the open source version

00:05:46,560 --> 00:05:53,280
the experience of the first phase

00:05:49,680 --> 00:05:56,960
will play a great role in our subsequent

00:05:53,280 --> 00:06:00,240
large-scale use of cassandra

00:05:56,960 --> 00:06:04,720
in the second phase from 2040

00:06:00,240 --> 00:06:07,680
to present we sum up the experience

00:06:04,720 --> 00:06:10,800
and the lessons and find the best use

00:06:07,680 --> 00:06:14,240
scenario of cassandra

00:06:10,800 --> 00:06:17,199
so how to use first do not

00:06:14,240 --> 00:06:18,400
modify the source code continue to

00:06:17,199 --> 00:06:21,919
explore the

00:06:18,400 --> 00:06:22,720
optimal use of cassandra then according

00:06:21,919 --> 00:06:25,919
to the

00:06:22,720 --> 00:06:26,560
characteristic of cassandra match the

00:06:25,919 --> 00:06:30,800
best

00:06:26,560 --> 00:06:34,319
application scenario only use the most

00:06:30,800 --> 00:06:37,360
outstanding feature of cassandra

00:06:34,319 --> 00:06:38,639
and and we rewrite the driver routing

00:06:37,360 --> 00:06:42,160
strategy

00:06:38,639 --> 00:06:49,280
to realize the switching between dc's

00:06:42,160 --> 00:06:51,280
and improve the reliability of cassandra

00:06:49,280 --> 00:06:52,479
and we also built cluster level

00:06:51,280 --> 00:06:55,599
deployment

00:06:52,479 --> 00:06:56,960
monitoring and lengthening consult to

00:06:55,599 --> 00:07:00,160
comprehensive

00:06:56,960 --> 00:07:01,120
monitor the running status of cassandra

00:07:00,160 --> 00:07:05,280
node

00:07:01,120 --> 00:07:05,280
without ops center

00:07:05,360 --> 00:07:08,479
continue to follow up on the new

00:07:07,680 --> 00:07:12,319
versions

00:07:08,479 --> 00:07:16,000
and the new feature of the community and

00:07:12,319 --> 00:07:19,680
quickly launch new version to improve

00:07:16,000 --> 00:07:22,880
system performance and reliability

00:07:19,680 --> 00:07:25,919
and we also specified cassandra

00:07:22,880 --> 00:07:28,800
usage rule constraint

00:07:25,919 --> 00:07:29,759
application use scenario and made a

00:07:28,800 --> 00:07:34,560
perfect

00:07:29,759 --> 00:07:34,560
combination of application and cassandra

00:07:36,319 --> 00:07:43,360
then we got such outcome

00:07:40,000 --> 00:07:46,560
cassandra is widely used in various

00:07:43,360 --> 00:07:49,919
scenarios of consumer cloud service

00:07:46,560 --> 00:07:53,120
effectively supporting the rapid

00:07:49,919 --> 00:07:55,520
deployment of cloud service in recent

00:07:53,120 --> 00:07:55,520
years

00:07:57,440 --> 00:08:06,000
so how large the scale we are using now

00:08:02,800 --> 00:08:09,919
we have a total of 30 000 nodes

00:08:06,000 --> 00:08:09,919
and more than 300 clusters

00:08:10,639 --> 00:08:16,240
the large cluster has more than 600

00:08:14,000 --> 00:08:19,440
nodes

00:08:16,240 --> 00:08:23,840
all of our cassandra cluster process

00:08:19,440 --> 00:08:27,520
more than 10 million requests per second

00:08:23,840 --> 00:08:29,599
however the average latency is only 4

00:08:27,520 --> 00:08:32,800
milliseconds

00:08:29,599 --> 00:08:36,399
and the largest table has more than

00:08:32,800 --> 00:08:36,399
300 billion records

00:08:37,279 --> 00:08:41,360
cassandra is behind almost all huawei

00:08:40,320 --> 00:08:44,399
mobile

00:08:41,360 --> 00:08:47,839
applications and terminal equipment

00:08:44,399 --> 00:08:51,920
applications including photo

00:08:47,839 --> 00:08:55,920
albums radios and

00:08:51,920 --> 00:08:58,800
small healthy etc every day

00:08:55,920 --> 00:08:59,760
thousands of huawei mobile phone users

00:08:58,800 --> 00:09:02,480
benefit from

00:08:59,760 --> 00:09:02,480
cassandra

00:09:03,200 --> 00:09:08,160
so next i will share the issue we have

00:09:06,160 --> 00:09:10,720
encountered in the production

00:09:08,160 --> 00:09:10,720
environment

00:09:15,120 --> 00:09:18,640
as your cluster becomes larger and

00:09:17,440 --> 00:09:21,360
larger

00:09:18,640 --> 00:09:22,720
there will be some problems in other

00:09:21,360 --> 00:09:25,360
words

00:09:22,720 --> 00:09:27,360
you need to face some challenges under

00:09:25,360 --> 00:09:29,680
large clusters

00:09:27,360 --> 00:09:31,839
i can summarize it to the following

00:09:29,680 --> 00:09:35,440
challenges

00:09:31,839 --> 00:09:37,760
change 1 cluster stability

00:09:35,440 --> 00:09:39,360
caused by large scale and large data

00:09:37,760 --> 00:09:43,920
volume

00:09:39,360 --> 00:09:47,760
challenge two data consistency issue

00:09:43,920 --> 00:09:51,120
challenges three infrastructure issue

00:09:47,760 --> 00:09:52,720
and last challenge quickly problem

00:09:51,120 --> 00:09:56,399
identification

00:09:52,720 --> 00:09:56,399
narrow down and recovery

00:09:58,640 --> 00:10:05,600
i think most cassandra users may

00:10:01,760 --> 00:10:09,200
have encountered this strategy as well

00:10:05,600 --> 00:10:11,839
next i will share our experience

00:10:09,200 --> 00:10:14,630
according to some typical cassandra

00:10:11,839 --> 00:10:16,560
issue list below

00:10:14,630 --> 00:10:20,320
[Music]

00:10:16,560 --> 00:10:25,680
such as too many cluster nodes

00:10:20,320 --> 00:10:25,680
large partition key hotkey and tombstone

00:10:30,880 --> 00:10:38,800
that's one first one case is the

00:10:35,120 --> 00:10:39,680
capacity issue this issue caused by too

00:10:38,800 --> 00:10:42,399
many nodes

00:10:39,680 --> 00:10:42,399
in a cluster

00:10:44,320 --> 00:10:47,600
a production cluster has a single

00:10:46,800 --> 00:10:50,720
cluster

00:10:47,600 --> 00:10:54,720
of 600.

00:10:50,720 --> 00:10:58,839
during the expansion process

00:10:54,720 --> 00:11:02,000
the cpu and the load of all nodes

00:10:58,839 --> 00:11:05,360
stored causing all application

00:11:02,000 --> 00:11:05,360
requests to be blocked

00:11:05,600 --> 00:11:12,839
after stopping the expansion node

00:11:09,440 --> 00:11:15,839
the cluster node cpu and node return to

00:11:12,839 --> 00:11:15,839
normal

00:11:18,240 --> 00:11:22,480
we can see from the screenshot here

00:11:24,160 --> 00:11:33,279
the disk until you this utah and the

00:11:28,320 --> 00:11:33,279
node of glass node has risen sharply

00:11:33,760 --> 00:11:36,880
okay i have a drink

00:11:38,160 --> 00:11:43,120
the success rate of requests have

00:11:40,320 --> 00:11:45,519
dropped by 15

00:11:43,120 --> 00:11:46,320
facing the problem of large number of

00:11:45,519 --> 00:11:49,360
nodes

00:11:46,320 --> 00:11:53,440
there are two solutions first

00:11:49,360 --> 00:11:56,240
control the scale of the single cluster

00:11:53,440 --> 00:11:56,959
mainly number of the virtual token try

00:11:56,240 --> 00:12:00,800
not to

00:11:56,959 --> 00:12:03,600
exit 100 000

00:12:00,800 --> 00:12:04,399
after the cluster is too large it is

00:12:03,600 --> 00:12:08,079
necessary

00:12:04,399 --> 00:12:10,120
to consider the spill of the cluster

00:12:08,079 --> 00:12:11,360
so that a cluster cannot be

00:12:10,120 --> 00:12:13,920
significantly

00:12:11,360 --> 00:12:13,920
expanded

00:12:18,560 --> 00:12:24,720
this case is about issue of too much

00:12:21,120 --> 00:12:28,000
data on a single node

00:12:24,720 --> 00:12:29,200
in a production cluster the data volume

00:12:28,000 --> 00:12:32,240
of a single node

00:12:29,200 --> 00:12:35,360
reached 5 tb

00:12:32,240 --> 00:12:40,480
the cluster became very unstable

00:12:35,360 --> 00:12:40,480
frequent full gc and even out of memory

00:12:41,279 --> 00:12:46,079
data migration is also very easy to view

00:12:49,279 --> 00:12:53,040
the amount of no data is large and

00:12:51,920 --> 00:12:56,320
objects such as

00:12:53,040 --> 00:12:59,600
chrome filter and the index summary

00:12:56,320 --> 00:13:03,680
occupy a large amount of memory which

00:12:59,600 --> 00:13:07,839
eventually leads to frequent fuji and

00:13:03,680 --> 00:13:07,839
even out of memory

00:13:08,639 --> 00:13:15,360
when expanding if lcs is used but the

00:13:11,839 --> 00:13:18,000
stcs is used for level zero layer

00:13:15,360 --> 00:13:19,120
it will be very easy to have its

00:13:18,000 --> 00:13:22,160
frictions disk

00:13:19,120 --> 00:13:22,959
during the level 0 layer compaction

00:13:22,160 --> 00:13:26,480
process

00:13:22,959 --> 00:13:26,480
and the cost expansion view

00:13:27,839 --> 00:13:34,959
the solution is simple method 1

00:13:31,279 --> 00:13:37,440
expand the disk method 2

00:13:34,959 --> 00:13:38,560
temp rarely modified the compaction

00:13:37,440 --> 00:13:44,079
strategy

00:13:38,560 --> 00:13:44,079
of level 0 from stcs to lcs

00:13:45,040 --> 00:13:52,000
set this parameter here

00:13:48,240 --> 00:13:55,199
to true in addition

00:13:52,000 --> 00:13:55,839
the data volume of a node should be

00:13:55,199 --> 00:13:58,639
monitored

00:13:55,839 --> 00:13:58,639
in real time

00:13:58,880 --> 00:14:05,519
it is recommend that the data

00:14:02,079 --> 00:14:08,639
volume of a single node now is it

00:14:05,519 --> 00:14:08,639
1.5 tb

00:14:12,240 --> 00:14:20,240
and this case is the issue

00:14:15,680 --> 00:14:20,240
caused by compassion accumulation

00:14:23,279 --> 00:14:27,440
atlas production cluster no there are a

00:14:26,880 --> 00:14:30,639
lot of

00:14:27,440 --> 00:14:34,000
compaction pending tasks this

00:14:30,639 --> 00:14:37,920
will affect rate performance and also

00:14:34,000 --> 00:14:45,199
cause the data volume of this node

00:14:37,920 --> 00:14:48,720
to be much larger than of the other node

00:14:45,199 --> 00:14:52,399
use the compaction status command

00:14:48,720 --> 00:14:54,959
to view node compaction it is found that

00:14:52,399 --> 00:14:57,040
the node has a large number of pending

00:14:54,959 --> 00:14:59,360
tasks

00:14:57,040 --> 00:15:01,600
a large amount of the pen compaction

00:14:59,360 --> 00:15:03,839
painting shows that

00:15:01,600 --> 00:15:05,600
compaction cannot keep up with the

00:15:03,839 --> 00:15:09,360
writing speed

00:15:05,600 --> 00:15:12,480
and the loss of small file cannot be

00:15:09,360 --> 00:15:16,399
compacted in time which affect the rate

00:15:12,480 --> 00:15:16,399
performance of the application

00:15:16,720 --> 00:15:24,240
this figure here shows that more than

00:15:20,639 --> 00:15:27,199
30 000 compaction appending

00:15:24,240 --> 00:15:29,760
and it results in slow read requests to

00:15:27,199 --> 00:15:29,760
this node

00:15:32,240 --> 00:15:38,959
and this two figure shows the

00:15:35,360 --> 00:15:44,399
comparison between the compaction

00:15:38,959 --> 00:15:44,399
task without pending and with pending

00:15:45,120 --> 00:15:52,560
the graphic here is normal the

00:15:48,320 --> 00:15:56,880
this is the without painting

00:15:52,560 --> 00:16:01,440
task shows a read latency of 0.5

00:15:56,880 --> 00:16:04,480
milliseconds and

00:16:01,440 --> 00:16:04,880
the graphic of write show with pending

00:16:04,480 --> 00:16:11,680
test

00:16:04,880 --> 00:16:11,680
shows a read latency of 161 milliseconds

00:16:16,720 --> 00:16:23,839
there are many two methods to slow

00:16:19,839 --> 00:16:23,839
this issue

00:16:24,399 --> 00:16:29,839
adjust the compaction speed through node

00:16:27,600 --> 00:16:29,839
00:16:30,720 --> 00:16:34,480
if the accumulation reduce is not

00:16:33,120 --> 00:16:38,079
obviously

00:16:34,480 --> 00:16:41,360
after using master one try to

00:16:38,079 --> 00:16:45,839
adjust to these two

00:16:41,360 --> 00:16:45,839
prime merit parameter

00:16:46,560 --> 00:16:54,079
such for example this one is configured

00:16:50,800 --> 00:16:58,000
in the cassandra.ml

00:16:54,079 --> 00:17:05,839
and this parameter is jvm

00:16:58,000 --> 00:17:05,839
start parameter

00:17:07,760 --> 00:17:11,520
finish finish some of the issue

00:17:10,319 --> 00:17:14,880
encounter of

00:17:11,520 --> 00:17:19,839
cassandra server let's talk about some

00:17:14,880 --> 00:17:19,839
application related issue

00:17:20,559 --> 00:17:28,559
this case is about large partition key

00:17:25,360 --> 00:17:30,080
a production cluster has a large key

00:17:28,559 --> 00:17:32,960
issue

00:17:30,080 --> 00:17:33,840
even the amount of the data under some

00:17:32,960 --> 00:17:37,039
partition

00:17:33,840 --> 00:17:40,720
reach 2gb and there

00:17:37,039 --> 00:17:43,840
are continuous requests to assess

00:17:40,720 --> 00:17:47,360
access the launch partition key which

00:17:43,840 --> 00:17:53,039
leads to frequent fuji of some node

00:17:47,360 --> 00:17:53,039
and eventually leads to out of memory

00:17:53,760 --> 00:17:58,960
as can be seen from the figure there are

00:17:57,120 --> 00:18:03,120
many alarm logs

00:17:58,960 --> 00:18:03,120
in the cassandra system logs

00:18:05,520 --> 00:18:10,400
the solution is to design the table

00:18:08,160 --> 00:18:13,919
structure together

00:18:10,400 --> 00:18:17,280
with the applica application site

00:18:13,919 --> 00:18:18,480
and standardize the weight cassandra in

00:18:17,280 --> 00:18:23,760
use

00:18:18,480 --> 00:18:23,760
then avoid large key from the design

00:18:24,400 --> 00:18:28,240
and what we need to pay special

00:18:26,960 --> 00:18:32,320
attention

00:18:28,240 --> 00:18:36,320
to is that large partition key issue

00:18:32,320 --> 00:18:36,320
can only be solved on the design

00:18:37,200 --> 00:18:41,120
it is too late to think about how to

00:18:40,160 --> 00:18:43,360
avoid it

00:18:41,120 --> 00:18:44,400
after the large partition key has an

00:18:43,360 --> 00:18:48,880
impact

00:18:44,400 --> 00:18:48,880
on the production environment

00:18:52,880 --> 00:18:59,760
there are two ways to solve the large

00:18:56,000 --> 00:19:04,559
partition key issue from the design

00:18:59,760 --> 00:19:07,360
first don't modify the table structure

00:19:04,559 --> 00:19:08,559
but so the large position kit by

00:19:07,360 --> 00:19:13,600
changing the

00:19:08,559 --> 00:19:13,600
application usage scenario

00:19:13,760 --> 00:19:21,039
or you can modify the table structure

00:19:17,440 --> 00:19:25,840
of application to achieve the purpose

00:19:21,039 --> 00:19:25,840
of discrete large partition key

00:19:30,000 --> 00:19:37,200
this is the case for method one

00:19:34,720 --> 00:19:38,960
a storage application has a file

00:19:37,200 --> 00:19:41,760
deletion records table

00:19:38,960 --> 00:19:43,679
for recording the deletion record of the

00:19:41,760 --> 00:19:47,440
file

00:19:43,679 --> 00:19:47,440
the partition key is

00:19:48,240 --> 00:19:53,840
file id

00:19:56,960 --> 00:20:00,400
for personal file the deletion record

00:19:59,840 --> 00:20:03,760
under

00:20:00,400 --> 00:20:06,799
a file will not be very large but for

00:20:03,760 --> 00:20:10,159
public file such as lock screen

00:20:06,799 --> 00:20:13,280
picture of huawei forms there will be a

00:20:10,159 --> 00:20:18,080
large partition key issue

00:20:13,280 --> 00:20:18,080
and then we did the optimization

00:20:18,960 --> 00:20:27,200
before each file is deleted first

00:20:22,480 --> 00:20:31,600
determine whether the file is a hot file

00:20:27,200 --> 00:20:31,600
if it is a hot file

00:20:32,000 --> 00:20:39,039
no more delete record will be end

00:20:35,520 --> 00:20:49,840
in this way there will not be too many

00:20:39,039 --> 00:20:49,840
deletion records under public file

00:20:51,120 --> 00:20:55,280
and this is a case for method 2.

00:20:58,080 --> 00:21:04,880
a radio application use a table

00:21:01,280 --> 00:21:08,240
to record the reservation detail of a

00:21:04,880 --> 00:21:11,440
radio resource for example

00:21:08,240 --> 00:21:13,360
a movie is about to be shown and the

00:21:11,440 --> 00:21:18,960
user can be subscribed

00:21:13,360 --> 00:21:18,960
to news in evidence at in advance

00:21:19,200 --> 00:21:25,919
after the movie goes online

00:21:22,799 --> 00:21:27,919
the application push the message to each

00:21:25,919 --> 00:21:29,760
user

00:21:27,919 --> 00:21:31,120
the primary key of this table are

00:21:29,760 --> 00:21:34,159
resource id

00:21:31,120 --> 00:21:34,159
and user id

00:21:35,440 --> 00:21:40,400
the partition key is resource id

00:21:40,720 --> 00:21:48,640
if you use resource id to record

00:21:44,720 --> 00:21:52,000
a popular movie then the

00:21:48,640 --> 00:21:55,200
user id of the resource id will reach

00:21:52,000 --> 00:21:57,520
10 of millions or even hundreds of

00:21:55,200 --> 00:22:00,799
million

00:21:57,520 --> 00:22:03,919
this will definitely cause a large

00:22:00,799 --> 00:22:03,919
partition key issue

00:22:05,280 --> 00:22:10,640
the type of application has two

00:22:07,760 --> 00:22:13,919
characteristics

00:22:10,640 --> 00:22:17,520
first popular resource will cause the

00:22:13,919 --> 00:22:21,840
large partition key issue

00:22:17,520 --> 00:22:22,240
the frequency of access to resource will

00:22:21,840 --> 00:22:24,880
not

00:22:22,240 --> 00:22:24,880
to be high

00:22:25,600 --> 00:22:30,320
a common scenario is to push

00:22:28,400 --> 00:22:33,760
notification to user

00:22:30,320 --> 00:22:36,799
after the results are online

00:22:33,760 --> 00:22:41,840
this kind of things does not have

00:22:36,799 --> 00:22:41,840
high requirement for timeliness

00:22:42,400 --> 00:22:45,120
in this case

00:22:45,679 --> 00:22:51,120
the solution is to discrete the

00:22:48,640 --> 00:22:53,919
partition key

00:22:51,120 --> 00:22:54,240
detail about distributizing partition

00:22:53,919 --> 00:22:57,039
key

00:22:54,240 --> 00:22:57,039
and omitting

00:22:57,600 --> 00:23:00,880
the method in the slide is just an

00:22:59,600 --> 00:23:04,400
example you

00:23:00,880 --> 00:23:05,600
can discretize partition key in other

00:23:04,400 --> 00:23:08,640
way

00:23:05,600 --> 00:23:09,840
here just an example the resource id

00:23:08,640 --> 00:23:14,720
continues to be

00:23:09,840 --> 00:23:14,720
discrete such as decrease into

00:23:15,760 --> 00:23:23,840
10 000.

00:23:28,159 --> 00:23:32,640
and this case is about pocket

00:23:33,200 --> 00:23:39,679
frequent operation

00:23:36,880 --> 00:23:40,159
of the same kids in a short period of

00:23:39,679 --> 00:23:43,200
time

00:23:40,159 --> 00:23:44,080
will cause the cpu and the load of the

00:23:43,200 --> 00:23:48,880
node

00:23:44,080 --> 00:23:48,880
where the kit is located to behind

00:23:49,039 --> 00:23:52,799
it will affect the request sent to this

00:23:51,919 --> 00:23:57,200
node

00:23:52,799 --> 00:23:57,200
and cause the success rate to draw

00:23:58,559 --> 00:24:03,600
the monitoring system shows the cpu and

00:24:01,360 --> 00:24:08,400
the load of some nodes are very high

00:24:03,600 --> 00:24:08,400
you can see the figure on the right here

00:24:09,440 --> 00:24:16,000
so how to recovery

00:24:12,720 --> 00:24:19,600
first find the hotket login to the

00:24:16,000 --> 00:24:22,480
abnormal node and use top partition

00:24:19,600 --> 00:24:23,440
to check which partition key under which

00:24:22,480 --> 00:24:29,600
table

00:24:23,440 --> 00:24:32,720
has the highest operating frequency

00:24:29,600 --> 00:24:33,520
then shell the hotkey the application

00:24:32,720 --> 00:24:37,039
side

00:24:33,520 --> 00:24:38,559
can use the blacklist capability on the

00:24:37,039 --> 00:24:41,520
sdk site

00:24:38,559 --> 00:24:42,400
that we deployed based on the cassandra

00:24:41,520 --> 00:24:47,840
driver

00:24:42,400 --> 00:24:47,840
to block the hotkey directly

00:24:48,960 --> 00:24:56,799
and finally it is

00:24:53,200 --> 00:25:01,840
designed to design to avoid

00:24:56,799 --> 00:25:01,840
causing hotkey problems in cassandra

00:25:02,080 --> 00:25:06,159
first the identified application

00:25:04,720 --> 00:25:10,559
scenario

00:25:06,159 --> 00:25:14,080
that may general hockey during design

00:25:10,559 --> 00:25:17,279
and then according to the identified

00:25:14,080 --> 00:25:20,799
scene set up a cache on the

00:25:17,279 --> 00:25:26,159
application site to reduce the impact

00:25:20,799 --> 00:25:26,159
on the database when the hotkey

00:25:26,840 --> 00:25:29,840
appears

00:25:33,520 --> 00:25:38,159
and this case is about tombstone

00:25:38,400 --> 00:25:42,080
frequent deletion or partition key in a

00:25:41,679 --> 00:25:45,360
short

00:25:42,080 --> 00:25:46,559
period of time will result in a very

00:25:45,360 --> 00:25:51,919
large number of

00:25:46,559 --> 00:25:51,919
tombstones in a period of gc grades

00:25:54,000 --> 00:26:00,720
this is this in turns

00:25:57,200 --> 00:26:01,919
lead to slow queries high cpu

00:26:00,720 --> 00:26:06,000
utilization

00:26:01,919 --> 00:26:06,000
load and frequent gc

00:26:06,640 --> 00:26:10,080
it can be seen from the monitoring

00:26:08,840 --> 00:26:13,600
system

00:26:10,080 --> 00:26:17,919
that the cpu utilization and the

00:26:13,600 --> 00:26:21,679
load of some nodes are very high

00:26:17,919 --> 00:26:21,679
the right figure here

00:26:22,000 --> 00:26:29,600
at the same time we receive a lot of

00:26:26,159 --> 00:26:29,600
warning message

00:26:30,559 --> 00:26:35,440
and then we use the jstact command to

00:26:33,840 --> 00:26:38,480
print stacked information

00:26:35,440 --> 00:26:42,320
and find that threat with

00:26:38,480 --> 00:26:45,360
high cpu usage are basically awkwardly

00:26:42,320 --> 00:26:47,840
the red which are all doing tombstone

00:26:45,360 --> 00:26:47,840
filtering

00:26:50,880 --> 00:26:56,640
the emergency measure is that

00:26:54,559 --> 00:26:58,240
first delete the data under the

00:26:56,640 --> 00:27:01,760
partition

00:26:58,240 --> 00:27:05,279
key and

00:27:01,760 --> 00:27:08,799
then reduce gc grades in this

00:27:05,279 --> 00:27:10,400
scenario this value can be reduced to

00:27:08,799 --> 00:27:13,600
speed up the

00:27:10,400 --> 00:27:16,400
physical clint and recycle time

00:27:13,600 --> 00:27:16,400
of tombstone

00:27:16,880 --> 00:27:23,120
through this event we believe that

00:27:20,159 --> 00:27:24,080
cassandra is not suitable for

00:27:23,120 --> 00:27:29,120
applications

00:27:24,080 --> 00:27:29,120
that are frequently deleted and

00:27:32,840 --> 00:27:37,200
queried

00:27:34,559 --> 00:27:40,080
the last few cases are about

00:27:37,200 --> 00:27:44,240
infrastructure

00:27:40,080 --> 00:27:44,240
this is about network backlog

00:27:47,440 --> 00:27:51,360
this case is mainly to share how to

00:27:50,080 --> 00:27:55,919
identify

00:27:51,360 --> 00:27:56,799
such issue the latency of a certain

00:27:55,919 --> 00:27:59,919
application

00:27:56,799 --> 00:28:00,799
increase significant the cassandra

00:27:59,919 --> 00:28:04,880
driver side

00:28:00,799 --> 00:28:07,120
prints a lot a large num amount of snow

00:28:04,880 --> 00:28:07,120
lock

00:28:09,039 --> 00:28:16,159
here was the pointing process

00:28:12,240 --> 00:28:18,399
at that time first check the cassandra

00:28:16,159 --> 00:28:21,840
cluster status

00:28:18,399 --> 00:28:25,200
such as cpu io

00:28:21,840 --> 00:28:29,200
accumulation of the threat put

00:28:25,200 --> 00:28:29,200
gossip information etc

00:28:29,360 --> 00:28:37,919
if there is no problem after first step

00:28:33,520 --> 00:28:37,919
you can check the network for problem

00:28:38,720 --> 00:28:42,799
so how to find the volte node

00:28:43,200 --> 00:28:48,559
you can use get endpoints to query the

00:28:46,559 --> 00:28:51,760
copy nodes of the key

00:28:48,559 --> 00:28:54,080
to find the public key

00:28:51,760 --> 00:28:56,960
to find the public node of the key in

00:28:54,080 --> 00:29:02,240
the slow query

00:28:56,960 --> 00:29:02,240
you can see the figure on the right

00:29:03,120 --> 00:29:08,559
and from the picture on the right

00:29:06,320 --> 00:29:10,000
you can see all the partition key

00:29:08,559 --> 00:29:15,039
involved

00:29:10,000 --> 00:29:15,039
in slow query at the common replica node

00:29:15,880 --> 00:29:21,039
23.20

00:29:17,919 --> 00:29:24,960
finally it was find that this node

00:29:21,039 --> 00:29:28,080
has a issue with the network card

00:29:24,960 --> 00:29:31,120
and there was a packet loss

00:29:28,080 --> 00:29:32,399
problem after the packet loss is

00:29:31,120 --> 00:29:41,840
repaired

00:29:32,399 --> 00:29:41,840
the application latency is normal

00:29:48,080 --> 00:29:55,600
this case is about the issue of

00:29:51,279 --> 00:30:00,480
an event load caused by inconsistent

00:29:55,600 --> 00:30:00,480
machines specific specifications

00:30:01,200 --> 00:30:04,399
the application upgrade caused the

00:30:03,679 --> 00:30:08,240
amount of

00:30:04,399 --> 00:30:09,760
query requests to double that user cause

00:30:08,240 --> 00:30:12,240
the load of some node

00:30:09,760 --> 00:30:14,159
in the cassandra cluster to reach the

00:30:12,240 --> 00:30:17,440
limit

00:30:14,159 --> 00:30:19,440
resulting in a large number of requests

00:30:17,440 --> 00:30:22,159
timeout

00:30:19,440 --> 00:30:25,760
the applications cost of the traffic and

00:30:22,159 --> 00:30:25,760
the cassandra cluster resume

00:30:26,320 --> 00:30:29,919
symptom is that the cluster has a total

00:30:28,799 --> 00:30:33,279
10 nodes

00:30:29,919 --> 00:30:36,720
of which fine cassandra node has a

00:30:33,279 --> 00:30:40,159
load several times higher than the other

00:30:36,720 --> 00:30:43,440
5 node after our

00:30:40,159 --> 00:30:45,120
analysis we find that when cassandra

00:30:43,440 --> 00:30:47,760
selects a replica

00:30:45,120 --> 00:30:48,880
it will sort it according to the

00:30:47,760 --> 00:30:52,480
specification

00:30:48,880 --> 00:30:55,440
of each node and the higher of

00:30:52,480 --> 00:30:57,440
at the higher densification the higher

00:30:55,440 --> 00:30:59,519
the priority

00:30:57,440 --> 00:31:01,039
and the more internal requests it

00:30:59,519 --> 00:31:04,080
received

00:31:01,039 --> 00:31:08,240
there are many parameters that

00:31:04,080 --> 00:31:13,200
affect the selection strategy including

00:31:08,240 --> 00:31:13,200
memory disk cpu etc

00:31:13,360 --> 00:31:20,399
to this end we did test

00:31:18,000 --> 00:31:23,039
you can see the test result of the end

00:31:20,399 --> 00:31:28,159
of this slide

00:31:23,039 --> 00:31:31,360
in a cluster the comparison of the node

00:31:28,159 --> 00:31:35,840
latency between 16 chord and a

00:31:31,360 --> 00:31:39,279
chord is shown in this figure here

00:31:35,840 --> 00:31:42,640
the latency of this code is higher the

00:31:39,279 --> 00:31:46,000
the machine latency of a chord

00:31:42,640 --> 00:31:49,600
so we recommend that when the cassandra

00:31:46,000 --> 00:31:54,399
is deployed the hardware configuration

00:31:49,600 --> 00:31:54,399
of all node must be consistency

00:31:55,519 --> 00:32:00,640
this can prevent cassandra from

00:31:58,000 --> 00:32:05,120
calculating the difference in the no

00:32:00,640 --> 00:32:08,720
score and routing request to note which

00:32:05,120 --> 00:32:11,440
with higher priorities which leads to

00:32:08,720 --> 00:32:11,440
this issue

00:32:13,440 --> 00:32:16,960
the last case is a issue encountered

00:32:16,159 --> 00:32:20,320
during

00:32:16,960 --> 00:32:20,320
data migration

00:32:20,640 --> 00:32:27,840
when a production cluster is expand

00:32:24,000 --> 00:32:31,120
data migration is always stuck

00:32:27,840 --> 00:32:34,880
gb data migration is either stock

00:32:31,120 --> 00:32:38,399
or it takes a day or two to complete

00:32:34,880 --> 00:32:43,840
expansion which serious seriously

00:32:38,399 --> 00:32:43,840
affect the progress of expansion

00:32:44,000 --> 00:32:47,039
view the stream progress through the

00:32:45,840 --> 00:32:50,159
node 2 status

00:32:47,039 --> 00:32:53,200
you can see the picture on the right

00:32:50,159 --> 00:32:53,200
first one picture

00:32:54,559 --> 00:32:57,840
and we use notice to nestle that query

00:32:57,519 --> 00:33:02,720
the

00:32:57,840 --> 00:33:05,760
result as shown in the figure

00:33:02,720 --> 00:33:10,320
it has always been the status

00:33:05,760 --> 00:33:13,120
in the figures and has no change

00:33:10,320 --> 00:33:13,919
on the receiving side checked where the

00:33:13,120 --> 00:33:17,360
streaming

00:33:13,919 --> 00:33:20,480
threat is stuck through jstack

00:33:17,360 --> 00:33:23,600
you can see here the second picture

00:33:20,480 --> 00:33:26,240
then it was find out the

00:33:23,600 --> 00:33:26,799
receiving threat try to read the data

00:33:26,240 --> 00:33:31,279
stream

00:33:26,799 --> 00:33:31,679
from the socket use netstatus to check

00:33:31,279 --> 00:33:36,159
the

00:33:31,679 --> 00:33:39,919
migration port 7000 on the sending

00:33:36,159 --> 00:33:42,720
site and find find that

00:33:39,919 --> 00:33:45,279
the data packet with 6 million cannot be

00:33:42,720 --> 00:33:45,279
sent out

00:33:45,360 --> 00:33:52,960
in responsible to this problem

00:33:48,720 --> 00:33:56,080
our analysis find that the parameter

00:33:52,960 --> 00:34:00,080
net ipv4 tcp stacks

00:33:56,080 --> 00:34:02,960
need to be set one set to one in

00:34:00,080 --> 00:34:05,200
in an alignment with poor network

00:34:02,960 --> 00:34:08,399
condition

00:34:05,200 --> 00:34:12,560
after setting 200gb data

00:34:08,399 --> 00:34:16,399
can be migration in 20 minutes

00:34:12,560 --> 00:34:18,839
finally i will share our best practice

00:34:16,399 --> 00:34:21,839
for using cassandra in huawei consumer

00:34:18,839 --> 00:34:21,839
cloud

00:34:22,639 --> 00:34:26,000
our team is responsible for all consumer

00:34:25,520 --> 00:34:30,320
cloud

00:34:26,000 --> 00:34:30,320
service accessing cassandra

00:34:30,879 --> 00:34:35,359
based on our experience and using the

00:34:34,879 --> 00:34:39,520
best

00:34:35,359 --> 00:34:42,159
feature of cassandra as far as possible

00:34:39,520 --> 00:34:44,879
we have specified some routes for

00:34:42,159 --> 00:34:44,879
applications

00:34:47,679 --> 00:34:54,879
i think this is very important reason

00:34:51,520 --> 00:34:57,599
why we can support cassandra on such a

00:34:54,879 --> 00:34:57,599
large scale

00:34:58,560 --> 00:35:02,079
we will review we will review table

00:35:01,359 --> 00:35:05,839
structure

00:35:02,079 --> 00:35:09,280
of application if the table structure

00:35:05,839 --> 00:35:12,880
does not conform to the to this road

00:35:09,280 --> 00:35:12,880
it is not allowed to be great

00:35:12,960 --> 00:35:20,079
the form of for and for

00:35:16,000 --> 00:35:23,200
online application cassandra use

00:35:20,079 --> 00:35:28,079
on a large scale application use

00:35:23,200 --> 00:35:28,079
scenario must be restricted

00:35:29,200 --> 00:35:34,800
and the form on the right is our access

00:35:32,480 --> 00:35:34,800
rule

00:35:35,280 --> 00:35:42,839
such as a single partition key is not

00:35:38,480 --> 00:35:45,839
discrete enough lead to orchid

00:35:42,839 --> 00:35:45,839
and

00:35:46,720 --> 00:35:50,320
the number of records for a single

00:35:49,440 --> 00:35:54,880
partition key

00:35:50,320 --> 00:35:54,880
does not exist 100 000

00:35:55,200 --> 00:36:01,599
and last

00:35:58,400 --> 00:36:13,839
capability application need to consider

00:36:01,599 --> 00:36:13,839
ttl to prevent unlimited data growth

00:36:17,680 --> 00:36:21,520
in order to simplify the operation and

00:36:20,240 --> 00:36:24,960
maintenance

00:36:21,520 --> 00:36:27,839
we have also made a set of our own

00:36:24,960 --> 00:36:27,839
monitoring system

00:36:29,839 --> 00:36:38,839
here's machine monitoring

00:36:33,839 --> 00:36:41,839
we monitor the cpu io disk and memory

00:36:38,839 --> 00:36:44,400
and the second feature is the request

00:36:41,839 --> 00:36:47,520
monitoring

00:36:44,400 --> 00:36:51,520
we monitor the request volume

00:36:47,520 --> 00:36:55,200
latency success rate etc

00:36:51,520 --> 00:37:00,480
and the third

00:36:55,200 --> 00:37:03,920
picture is the the red food monitoring

00:37:00,480 --> 00:37:04,560
and we monitor the cassandra's red proof

00:37:03,920 --> 00:37:09,280
such as

00:37:04,560 --> 00:37:12,079
read write compaction and etc

00:37:09,280 --> 00:37:15,040
and the last picture is the cluster

00:37:12,079 --> 00:37:15,040
scale monitoring

00:37:16,000 --> 00:37:19,760
and we also collect some log for

00:37:19,119 --> 00:37:23,440
monitoring

00:37:19,760 --> 00:37:26,720
include tombstone large platinum key

00:37:23,440 --> 00:37:26,720
hints etc

00:37:27,599 --> 00:37:32,640
that's all i have shared this time thank

00:37:34,839 --> 00:37:40,720
you

00:37:36,640 --> 00:37:44,160
uh questions uh thank you firstly for

00:37:40,720 --> 00:37:46,480
the presentation very informative um so

00:37:44,160 --> 00:37:49,520
the first one is

00:37:46,480 --> 00:37:53,839
is there any rdb ms

00:37:49,520 --> 00:37:53,839
being used or is it only cassandra

00:37:54,400 --> 00:37:58,160
rdb are there other use cases

00:37:59,119 --> 00:38:07,839
on our team we only use cassandra's but

00:38:04,240 --> 00:38:11,200
i know other teams use other rtb

00:38:07,839 --> 00:38:14,800
such as mysql and

00:38:11,200 --> 00:38:14,800
oracle database

00:38:17,200 --> 00:38:25,760
so uh my question is clear your

00:38:22,480 --> 00:38:28,960
ah i think so yeah yeah

00:38:25,760 --> 00:38:30,720
okay so that was uh from the audience i

00:38:28,960 --> 00:38:33,839
actually have a question

00:38:30,720 --> 00:38:37,520
um okay you you mentioned

00:38:33,839 --> 00:38:40,720
we should never exceed a hundred

00:38:37,520 --> 00:38:43,440
or num tokens in cluster do you mean

00:38:40,720 --> 00:38:44,400
the total number of v nodes across all

00:38:43,440 --> 00:38:47,359
the nodes

00:38:44,400 --> 00:38:49,119
so if you summed the tokens for all the

00:38:47,359 --> 00:38:51,520
nodes it must be less than a hundred

00:38:49,119 --> 00:38:51,520
thousand

00:39:00,079 --> 00:39:02,400
all right

00:39:05,200 --> 00:39:12,800
okay i think okay we are out of time so

00:39:09,359 --> 00:39:29,839
thank you again thank you thank you okay

00:39:12,800 --> 00:39:29,839
thank you bye bye thanks everyone

00:39:38,240 --> 00:39:40,320

YouTube URL: https://www.youtube.com/watch?v=nFMblxoqjAg


