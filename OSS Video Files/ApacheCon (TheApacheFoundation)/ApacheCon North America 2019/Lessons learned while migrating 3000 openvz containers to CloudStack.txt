Title: Lessons learned while migrating 3000 openvz containers to CloudStack
Publication date: 2019-09-20
Playlist: ApacheCon North America 2019
Description: 
	When Leaseweb acquired a new company 3000 public cloud customers we had to come up with a strategy to move all these VMs to our CloudStack setup. During this talk we will share: how we migrated a large foreign openvz environment into our kvm cloudstack environment in an automated fashion minimizing customer impact. The lessons we learned along the way and how we would do it again in the future.
Captions: 
	00:00:04,650 --> 00:00:09,340
what more today we're going to talk

00:00:07,900 --> 00:00:11,590
about the lessons we learned while

00:00:09,340 --> 00:00:15,070
migrating 3000 open fishes containers to

00:00:11,590 --> 00:00:16,390
CloudStack i'm jeffrey from senior

00:00:15,070 --> 00:00:20,440
system engineer for leech web for the

00:00:16,390 --> 00:00:22,779
last 13 years working on these last

00:00:20,440 --> 00:00:23,910
three years in the club stack team with

00:00:22,779 --> 00:00:28,320
me in a style of Wilson

00:00:23,910 --> 00:00:28,320
Deb's inference specialist pulleys web

00:00:29,740 --> 00:00:36,280
[Music]

00:00:32,460 --> 00:00:41,860
previously and now I've been inoculated

00:00:36,280 --> 00:00:43,780
into classic and that ever since okay so

00:00:41,860 --> 00:00:46,360
we work for LeaseWeb Alicia is a global

00:00:43,780 --> 00:00:49,170
hybrid hosting company we're currently

00:00:46,360 --> 00:00:52,449
located in 21 data centers around six

00:00:49,170 --> 00:00:54,070
for confidence we've been in business

00:00:52,449 --> 00:00:55,719
for twenty years and we offering

00:00:54,070 --> 00:00:59,680
services like dedicated servers

00:00:55,719 --> 00:01:05,050
colocation PPSS cloud hosting both

00:00:59,680 --> 00:01:07,960
public and private so the agenda for

00:01:05,050 --> 00:01:10,570
today today we're going to talk about

00:01:07,960 --> 00:01:12,549
why we migrated three thousands of fish

00:01:10,570 --> 00:01:15,790
at containers to CloudStack the

00:01:12,549 --> 00:01:18,490
migration methods we found were suitable

00:01:15,790 --> 00:01:21,520
to actually migrate these containers how

00:01:18,490 --> 00:01:24,549
we ended up migrating them are we

00:01:21,520 --> 00:01:26,829
migrated to network between our

00:01:24,549 --> 00:01:29,250
platforms a little bit about

00:01:26,829 --> 00:01:31,930
policy-based routing to get the network

00:01:29,250 --> 00:01:33,130
films in a network running and the

00:01:31,930 --> 00:01:39,390
lessons we learned while doing all of

00:01:33,130 --> 00:01:41,860
this so I'm gonna talk about how we

00:01:39,390 --> 00:01:43,180
mÃ©rida those stuff first of all the

00:01:41,860 --> 00:01:46,030
context of the however thing is

00:01:43,180 --> 00:01:51,070
happening here at least have acquired a

00:01:46,030 --> 00:01:52,960
company called servant in 2018 then one

00:01:51,070 --> 00:01:54,759
of the objectives there is to migrate

00:01:52,960 --> 00:01:56,110
all their stuff into loose hubs

00:01:54,759 --> 00:01:59,759
infrastructure so we don't have to

00:01:56,110 --> 00:02:02,350
manage two different platforms and

00:01:59,759 --> 00:02:04,420
servant was obviously at what women meta

00:02:02,350 --> 00:02:06,460
hosting cloud provider so more the same

00:02:04,420 --> 00:02:07,990
kind of things that Lisa does so we want

00:02:06,460 --> 00:02:11,440
to just line up one of those services

00:02:07,990 --> 00:02:13,990
into one instruction they're the goals

00:02:11,440 --> 00:02:16,450
there is to migrate the data for the

00:02:13,990 --> 00:02:18,870
customers in question I keep the

00:02:16,450 --> 00:02:20,760
downtime as minimal is awesome

00:02:18,870 --> 00:02:22,200
I keep their current IP address so they

00:02:20,760 --> 00:02:24,959
don't have any kind of sort of downtime

00:02:22,200 --> 00:02:27,090
for their customers and automated as

00:02:24,959 --> 00:02:28,890
much as possible since it is 3000 plus

00:02:27,090 --> 00:02:31,379
containers since a lot of work to do

00:02:28,890 --> 00:02:33,360
manually and still be able to use all

00:02:31,379 --> 00:02:38,459
the context features as normal after the

00:02:33,360 --> 00:02:39,510
migration so the source were and all

00:02:38,459 --> 00:02:40,920
these containers are coming from are

00:02:39,510 --> 00:02:43,620
coming from two platforms that are

00:02:40,920 --> 00:02:45,750
running at surfing one for parallel

00:02:43,620 --> 00:02:48,659
structure side pre 1206 platform that

00:02:45,750 --> 00:02:50,609
have 3000 containers running and one

00:02:48,659 --> 00:02:54,480
parallels for 1207 platform that had

00:02:50,609 --> 00:02:56,489
about 250 containers running so we

00:02:54,480 --> 00:02:59,220
migrated these machines to our leisure

00:02:56,489 --> 00:03:01,109
plastic platform we which we built a

00:02:59,220 --> 00:03:02,389
fully redundant high available including

00:03:01,109 --> 00:03:05,879
meta

00:03:02,389 --> 00:03:09,359
attached storage full flash for

00:03:05,879 --> 00:03:11,040
performance yeah Network all redundant

00:03:09,359 --> 00:03:14,519
power redundant make sure that

00:03:11,040 --> 00:03:19,560
everything stays up and running in case

00:03:14,519 --> 00:03:22,680
of failure migration methods so we

00:03:19,560 --> 00:03:26,609
investigated on how to migrate the

00:03:22,680 --> 00:03:29,190
containers from open platform to cloud

00:03:26,609 --> 00:03:31,889
stack we came up with three possible

00:03:29,190 --> 00:03:35,430
solutions one was contained two

00:03:31,889 --> 00:03:38,310
fertilization conversion since most of

00:03:35,430 --> 00:03:39,889
the I think about 90 95 percent of the

00:03:38,310 --> 00:03:42,750
containers were actually running cPanel

00:03:39,889 --> 00:03:45,930
we looked into the option through cPanel

00:03:42,750 --> 00:03:46,979
the cPanel migration and of course there

00:03:45,930 --> 00:03:49,680
is always the option to give the

00:03:46,979 --> 00:03:54,229
customer its own method of migrating its

00:03:49,680 --> 00:03:58,470
own data so the container to fertilize

00:03:54,229 --> 00:04:00,030
the VM migration so that is what we will

00:03:58,470 --> 00:04:02,790
do is we will create an exact copy of

00:04:00,030 --> 00:04:06,239
the customers container and then deploy

00:04:02,790 --> 00:04:09,090
it in a virtual machine the advantage is

00:04:06,239 --> 00:04:11,040
that no customer there's no customer

00:04:09,090 --> 00:04:12,840
actually needed so we do everything for

00:04:11,040 --> 00:04:15,989
the customer copy today I bring it back

00:04:12,840 --> 00:04:19,470
online for us it means since we don't

00:04:15,989 --> 00:04:21,870
want to work a lot of manual tasks means

00:04:19,470 --> 00:04:23,699
a lot of automation for us so for us

00:04:21,870 --> 00:04:25,500
will initially be more work to actually

00:04:23,699 --> 00:04:26,130
do the automation with them all the

00:04:25,500 --> 00:04:29,460
migrations

00:04:26,130 --> 00:04:31,380
itself will go automatically yeah we are

00:04:29,460 --> 00:04:33,740
reliant on accessing tooling to perform

00:04:31,380 --> 00:04:33,740
this start

00:04:33,810 --> 00:04:38,220
and we need much support legacy

00:04:35,100 --> 00:04:40,140
operating systems so we containments

00:04:38,220 --> 00:04:43,200
were running from different operating

00:04:40,140 --> 00:04:45,480
systems from centralized for all the way

00:04:43,200 --> 00:04:50,940
up to the latest central seven go to 10

00:04:45,480 --> 00:04:53,370
- boom - 18 and all these methods needed

00:04:50,940 --> 00:04:54,470
to be supported for us this was the

00:04:53,370 --> 00:04:57,660
preferred option to do the migration

00:04:54,470 --> 00:05:00,320
since that would be requiring the least

00:04:57,660 --> 00:05:02,250
amount of work for the customer itself

00:05:00,320 --> 00:05:04,200
the other option

00:05:02,250 --> 00:05:06,900
investigate was cPanel to cpanel

00:05:04,200 --> 00:05:09,870
migration so here we will deploy a new

00:05:06,900 --> 00:05:11,640
fee M on the cloud stack platform and

00:05:09,870 --> 00:05:16,170
then leisure support will do the

00:05:11,640 --> 00:05:18,840
migration between sheep and throw panels

00:05:16,170 --> 00:05:20,400
where they will migrate the data to

00:05:18,840 --> 00:05:22,530
initial migration and then change the

00:05:20,400 --> 00:05:25,320
DNS records to possibly new IP addresses

00:05:22,530 --> 00:05:25,770
and advantage for the customer to do the

00:05:25,320 --> 00:05:27,450
cPanel

00:05:25,770 --> 00:05:30,360
GP no migration is that you would get a

00:05:27,450 --> 00:05:32,760
new machine meaning latest OS latest

00:05:30,360 --> 00:05:34,500
updates and if customers still haven't

00:05:32,760 --> 00:05:36,600
all the ways that means yeah more

00:05:34,500 --> 00:05:40,230
security for him as well since it will

00:05:36,600 --> 00:05:42,300
have the latest patches however as I

00:05:40,230 --> 00:05:44,760
said it's about 95 percent of the peak

00:05:42,300 --> 00:05:46,470
instances that have cPanel running that

00:05:44,760 --> 00:05:47,850
means that there still some machines

00:05:46,470 --> 00:05:52,410
that will not be able to be migrating

00:05:47,850 --> 00:05:54,690
this way and then we have the option of

00:05:52,410 --> 00:05:56,190
the customer migrating its own data in

00:05:54,690 --> 00:05:58,620
this case we will deploy a new virtual

00:05:56,190 --> 00:05:59,880
machine on a cloud stack platform will

00:05:58,620 --> 00:06:01,500
give the customer the credentials to

00:05:59,880 --> 00:06:04,050
this new machine and then the customer

00:06:01,500 --> 00:06:05,970
code migrate the data between all

00:06:04,050 --> 00:06:07,890
instance and his new instance of myself

00:06:05,970 --> 00:06:09,360
for us that means the least amount of

00:06:07,890 --> 00:06:12,750
work since we just have to deploy a new

00:06:09,360 --> 00:06:15,870
fee M however there's a high chance of

00:06:12,750 --> 00:06:17,700
client loss since for a customer

00:06:15,870 --> 00:06:19,920
copying his data from one machine to

00:06:17,700 --> 00:06:21,210
another machine if it Sweden leaves up

00:06:19,920 --> 00:06:23,130
or with at least a competitor for

00:06:21,210 --> 00:06:24,180
customer it's the same amount of work so

00:06:23,130 --> 00:06:28,070
for us this is the least preferred

00:06:24,180 --> 00:06:30,390
method so I'm going to talk about

00:06:28,070 --> 00:06:34,910
different migration strategies we did

00:06:30,390 --> 00:06:37,860
first first thing we tried was there's a

00:06:34,910 --> 00:06:42,210
utility within parallels that is called

00:06:37,860 --> 00:06:43,680
PTV or PMI great initially that's this

00:06:42,210 --> 00:06:45,480
was the preferred option because it was

00:06:43,680 --> 00:06:49,050
the one suggested by

00:06:45,480 --> 00:06:50,160
as a very tool but it wasn't really

00:06:49,050 --> 00:06:52,350
built for

00:06:50,160 --> 00:06:54,240
kayvyun migrations it was built for

00:06:52,350 --> 00:06:56,460
parallels via migrations which is

00:06:54,240 --> 00:07:00,660
actually a different kind of thing but

00:06:56,460 --> 00:07:03,870
we tried it nonetheless we found it's so

00:07:00,660 --> 00:07:07,920
mostly unreliable I did it did it

00:07:03,870 --> 00:07:09,810
migrates things sometimes otherwise it

00:07:07,920 --> 00:07:13,410
would you give if they you know heir 51

00:07:09,810 --> 00:07:16,370
heir 48 and we have to call support at a

00:07:13,410 --> 00:07:19,080
pretty long execution time some vm's

00:07:16,370 --> 00:07:22,110
with it with 30 gigs would happen within

00:07:19,080 --> 00:07:24,120
our so but some would take days because

00:07:22,110 --> 00:07:26,460
customers had quite a few quite a few

00:07:24,120 --> 00:07:30,030
files I notes and it had her all those

00:07:26,460 --> 00:07:31,350
inodes up it used a local disk space on

00:07:30,030 --> 00:07:33,920
the high res or uses a template in

00:07:31,350 --> 00:07:37,260
partition so it limited our use case

00:07:33,920 --> 00:07:39,180
basically to stuff that had enough space

00:07:37,260 --> 00:07:42,780
within the hypervisor to store the VM

00:07:39,180 --> 00:07:45,120
again it can only process one container

00:07:42,780 --> 00:07:47,670
at a time per hypervisor just the

00:07:45,120 --> 00:07:49,410
limitations of the process you know yeah

00:07:47,670 --> 00:07:52,260
it doesn't really didn't really support

00:07:49,410 --> 00:07:54,540
Nate can I parallel processing and it's

00:07:52,260 --> 00:07:56,040
we still had to process that image to be

00:07:54,540 --> 00:07:58,410
compatible with a closet environment so

00:07:56,040 --> 00:07:59,970
we still had to upload all of all the

00:07:58,410 --> 00:08:03,390
DCP help scripts we still had to

00:07:59,970 --> 00:08:05,760
configure all the interfaces as a normal

00:08:03,390 --> 00:08:07,680
matrix you would I'm gonna talk about

00:08:05,760 --> 00:08:12,330
the process flow free using that tool

00:08:07,680 --> 00:08:14,490
that we found out here so first first we

00:08:12,330 --> 00:08:17,220
just executed the process there we

00:08:14,490 --> 00:08:19,230
transferred the parallels VM disk image

00:08:17,220 --> 00:08:21,080
on the source hypervisor so the hi

00:08:19,230 --> 00:08:24,810
browser storing those containers and

00:08:21,080 --> 00:08:28,080
then we converted that parallels VM to

00:08:24,810 --> 00:08:30,960
VM by the qmu a me image tool which does

00:08:28,080 --> 00:08:33,330
have support for parallel VM and KVM so

00:08:30,960 --> 00:08:35,849
in theory did would be able to work but

00:08:33,330 --> 00:08:39,360
I think that's semi non know and other

00:08:35,849 --> 00:08:42,240
issues and finally we find a free NBD

00:08:39,360 --> 00:08:45,930
disk on the mountain in then mount the

00:08:42,240 --> 00:08:48,330
image we have a working high pressure we

00:08:45,930 --> 00:08:52,380
upload that that - you might we mount

00:08:48,330 --> 00:08:56,100
that converted Canadian disk and then we

00:08:52,380 --> 00:08:57,330
use guest fish to OS detect us so some

00:08:56,100 --> 00:08:58,290
of these were sent to us some of these

00:08:57,330 --> 00:09:01,050
or

00:08:58,290 --> 00:09:03,600
- we need to figure out programmatically

00:09:01,050 --> 00:09:04,980
how which ones which without going into

00:09:03,600 --> 00:09:07,230
the OS and you know digging through

00:09:04,980 --> 00:09:09,529
files and guess fish was able to do that

00:09:07,230 --> 00:09:09,529
for us

00:09:10,130 --> 00:09:14,490
then we backed up the existing

00:09:12,870 --> 00:09:15,839
configuration files because we didn't

00:09:14,490 --> 00:09:18,480
really want to overwrite anything that

00:09:15,839 --> 00:09:20,850
would break something so we backed up

00:09:18,480 --> 00:09:21,600
the network configuration Paz boot boot

00:09:20,850 --> 00:09:23,370
settings

00:09:21,600 --> 00:09:25,199
jung-hee rich felt anything that we

00:09:23,370 --> 00:09:27,029
anything we've changed we backed up

00:09:25,199 --> 00:09:28,079
first and we wrote new files in

00:09:27,029 --> 00:09:31,709
according to our new class tank

00:09:28,079 --> 00:09:36,120
environment so that's that's everything

00:09:31,709 --> 00:09:37,800
to do with OS repositories file system

00:09:36,120 --> 00:09:40,589
settings boot settings everything and

00:09:37,800 --> 00:09:42,810
then finally uploading the call stack DT

00:09:40,589 --> 00:09:46,769
hug scripts to get past a reset

00:09:42,810 --> 00:09:48,240
functionality working to get SSH keys

00:09:46,769 --> 00:09:50,610
working although all that fun stuff and

00:09:48,240 --> 00:09:52,440
then finally registering the he have

00:09:50,610 --> 00:09:58,079
template and starting it with the IPS

00:09:52,440 --> 00:10:01,050
the other virtual VM had and then after

00:09:58,079 --> 00:10:04,350
dealing with this thing with the school

00:10:01,050 --> 00:10:05,959
for a number of weeks I finally went to

00:10:04,350 --> 00:10:08,449
the went to the drawing board again and

00:10:05,959 --> 00:10:11,819
found my own method of doing this

00:10:08,449 --> 00:10:14,990
through our sink it was quite a bit

00:10:11,819 --> 00:10:17,819
faster than he a migrate or PTV

00:10:14,990 --> 00:10:18,990
by the order of us exponentially faster

00:10:17,819 --> 00:10:20,730
within 30 minutes

00:10:18,990 --> 00:10:23,490
I could get a container on container to

00:10:20,730 --> 00:10:26,639
McLaws tech it was developed entirely

00:10:23,490 --> 00:10:29,010
within within LeaseWeb and it used it

00:10:26,639 --> 00:10:32,899
entirely I think all open-source

00:10:29,010 --> 00:10:35,850
software it has minimal downtime before

00:10:32,899 --> 00:10:38,610
with the PTV tool that the container was

00:10:35,850 --> 00:10:41,279
down as soon as the tools ran now I can

00:10:38,610 --> 00:10:43,410
sync all data and then on the second I

00:10:41,279 --> 00:10:47,760
can sync twice and the second sync is

00:10:43,410 --> 00:10:50,089
obviously offline syncing data it's

00:10:47,760 --> 00:10:52,649
tailored to support each specific OS so

00:10:50,089 --> 00:10:54,540
it's a little more work but it ended up

00:10:52,649 --> 00:10:55,740
being a little more reliable for us

00:10:54,540 --> 00:10:59,310
because we could actually tail a tool

00:10:55,740 --> 00:11:00,959
for each OS and we could reuse all the

00:10:59,310 --> 00:11:02,040
codes that we used for the closet I can

00:11:00,959 --> 00:11:04,259
merge preparation so all the

00:11:02,040 --> 00:11:05,850
configuration files you rewrote all the

00:11:04,259 --> 00:11:06,680
DS for us we uploaded and all that stuff

00:11:05,850 --> 00:11:09,980
can be rewritten

00:11:06,680 --> 00:11:09,980
narvi we used

00:11:10,300 --> 00:11:15,760
so the process for the rsync is a little

00:11:13,900 --> 00:11:18,880
more a little more complex but along the

00:11:15,760 --> 00:11:21,070
same lines first you have to install the

00:11:18,880 --> 00:11:23,920
kernel and grub packages on the source

00:11:21,070 --> 00:11:26,110
container for open Z they actually don't

00:11:23,920 --> 00:11:27,610
really use a real boot sector bootloader

00:11:26,110 --> 00:11:28,930
and they also don't really use them with

00:11:27,610 --> 00:11:30,730
Santa they don't use their own kernel

00:11:28,930 --> 00:11:33,190
for the VM and like we do in closed tech

00:11:30,730 --> 00:11:34,480
they use the hybrid kernel so we have to

00:11:33,190 --> 00:11:37,300
install that even though it's not gonna

00:11:34,480 --> 00:11:41,560
boot from it yet all the container is

00:11:37,300 --> 00:11:45,460
still run make sure that and then on our

00:11:41,560 --> 00:11:47,910
working on our converter boxes we create

00:11:45,460 --> 00:11:50,710
a rod disk with the same container size

00:11:47,910 --> 00:11:53,560
using guest fish again guest fish

00:11:50,710 --> 00:11:58,420
amazing tool we you can create a blank

00:11:53,560 --> 00:12:02,820
disk just as a Q&A image I can and then

00:11:58,420 --> 00:12:07,510
we convert that that disk to a QQ image

00:12:02,820 --> 00:12:09,580
and then we mount that using NBD and

00:12:07,510 --> 00:12:11,950
then we start the hour sync process and

00:12:09,580 --> 00:12:15,460
that will sync all the data from the

00:12:11,950 --> 00:12:18,910
live container onto the this empty disk

00:12:15,460 --> 00:12:20,200
there and then finally the downtime the

00:12:18,910 --> 00:12:22,150
downtime starts and we stop the

00:12:20,200 --> 00:12:24,820
container and resync the data so you

00:12:22,150 --> 00:12:29,710
cover anything missing this process

00:12:24,820 --> 00:12:32,320
usually much shorter now well it's still

00:12:29,710 --> 00:12:35,110
now the downtime started we detect the

00:12:32,320 --> 00:12:36,700
OS by a guest fish again we need to see

00:12:35,110 --> 00:12:38,140
exactly what version of centum what

00:12:36,700 --> 00:12:40,540
version is sent to us but vaunteth

00:12:38,140 --> 00:12:43,930
running to you you know right this is

00:12:40,540 --> 00:12:46,000
our files and all that stuff i backup

00:12:43,930 --> 00:12:48,550
the existing configuration files again

00:12:46,000 --> 00:12:51,780
and this is the code that we use right

00:12:48,550 --> 00:12:54,040
and we wrote the new classic environment

00:12:51,780 --> 00:12:57,810
and then this is something we actually

00:12:54,040 --> 00:13:00,340
add had had to add is writing our own

00:12:57,810 --> 00:13:02,530
well executing the inner inner amethyst

00:13:00,340 --> 00:13:05,590
and creating the in and creating the

00:13:02,530 --> 00:13:08,730
kernel environment - with all the

00:13:05,590 --> 00:13:11,380
Vertigo drivers that cloud tech needs

00:13:08,730 --> 00:13:13,120
and that it was dependent entirely on

00:13:11,380 --> 00:13:16,780
the different types different type of

00:13:13,120 --> 00:13:18,640
version of the OS because of unto sent

00:13:16,780 --> 00:13:21,589
to us and all the different versions to

00:13:18,640 --> 00:13:24,740
do that you do this differently

00:13:21,589 --> 00:13:26,899
and then again right the cloth safety

00:13:24,740 --> 00:13:29,660
hook scripts to get the pass to reset

00:13:26,899 --> 00:13:31,310
functionality working SSH key support

00:13:29,660 --> 00:13:33,649
all that stuff then the same thing

00:13:31,310 --> 00:13:35,600
uploading the key code template and then

00:13:33,649 --> 00:13:39,949
starting out he's from the source

00:13:35,600 --> 00:13:43,879
container now there's a special caveat

00:13:39,949 --> 00:13:47,120
to to this we see we found we had a

00:13:43,879 --> 00:13:49,519
CentOS 7 working box as a converter box

00:13:47,120 --> 00:13:52,399
and it did not support really the older

00:13:49,519 --> 00:13:55,610
grub installation if you ever have to

00:13:52,399 --> 00:13:58,009
work with grub 1.0 and then having an

00:13:55,610 --> 00:14:00,949
effing a newer us install group 1.0 -

00:13:58,009 --> 00:14:05,449
for an older Wes it never works almost

00:14:00,949 --> 00:14:09,949
so what we ended up doing is I created a

00:14:05,449 --> 00:14:13,579
custom ISO that booted into a CentOS 5

00:14:09,949 --> 00:14:17,120
environment or six one of them - and

00:14:13,579 --> 00:14:19,699
installed grub and the kernel drivers

00:14:17,120 --> 00:14:23,089
needed and then rebooted itself

00:14:19,699 --> 00:14:26,120
automatically so if the version is this

00:14:23,089 --> 00:14:28,910
version is CentOS four five and six it's

00:14:26,120 --> 00:14:30,980
changed the OS type-c IDE so they could

00:14:28,910 --> 00:14:32,300
boot it from the older it didn't have

00:14:30,980 --> 00:14:36,439
any kernel drives the moments a good

00:14:32,300 --> 00:14:38,540
older auto grab by so then it changes

00:14:36,439 --> 00:14:41,990
the OS type to vertigo scuzzy and then

00:14:38,540 --> 00:14:45,199
moved from being disk so after the auto

00:14:41,990 --> 00:14:47,000
grub ISO installs the grub installs the

00:14:45,199 --> 00:14:49,129
kernel drivers it reboots it back to

00:14:47,000 --> 00:14:51,620
Verdasco's because it has it has his new

00:14:49,129 --> 00:14:53,629
Verdejo drivers and then it called that

00:14:51,620 --> 00:14:58,819
calls back to the our migration manager

00:14:53,629 --> 00:15:00,860
and then says drop to complete now

00:14:58,819 --> 00:15:04,370
because we have so many issues with the

00:15:00,860 --> 00:15:06,319
original PTV migration we decided to

00:15:04,370 --> 00:15:07,610
start with a smaller batch I think this

00:15:06,319 --> 00:15:08,600
is a good strategy employed to all

00:15:07,610 --> 00:15:10,639
projects

00:15:08,600 --> 00:15:12,019
you always put on the starts to the

00:15:10,639 --> 00:15:13,819
smaller batch to make sure you get your

00:15:12,019 --> 00:15:16,660
all your bugs fixed before you start

00:15:13,819 --> 00:15:19,309
taking large swaths of servers offline

00:15:16,660 --> 00:15:22,240
but we started with ten it seem to be

00:15:19,309 --> 00:15:25,999
good number and then we ended up doing

00:15:22,240 --> 00:15:30,480
200 200 of time as soon as we start to

00:15:25,999 --> 00:15:32,999
get the ground rolling and to recap

00:15:30,480 --> 00:15:34,709
originally we did you did use that

00:15:32,999 --> 00:15:36,540
parallel tool but I wasn't really

00:15:34,709 --> 00:15:39,119
sufficient for our needs and we ended up

00:15:36,540 --> 00:15:40,529
building a own solution out of just open

00:15:39,119 --> 00:15:43,109
source tool switches which is always a

00:15:40,529 --> 00:15:48,209
preferred and Jeffrey's going to talk

00:15:43,109 --> 00:15:48,869
about network migration yeah so network

00:15:48,209 --> 00:15:52,049
migration

00:15:48,869 --> 00:15:55,079
so in short network existed out of about

00:15:52,049 --> 00:15:59,040
Holman shut Maps different sizes less 24

00:15:55,079 --> 00:16:02,160
23 21 all these subnets were divided up

00:15:59,040 --> 00:16:05,519
for freelance and one of the challenges

00:16:02,160 --> 00:16:07,559
we had was that I piece from multiple

00:16:05,519 --> 00:16:12,119
subnets and feelin's were all used on

00:16:07,559 --> 00:16:14,600
single instances which I'll explain

00:16:12,119 --> 00:16:16,859
later on how we solve that challenge so

00:16:14,600 --> 00:16:18,089
for the network interconnection the

00:16:16,859 --> 00:16:20,249
network migration we created an

00:16:18,089 --> 00:16:23,279
interconnect between the server network

00:16:20,249 --> 00:16:27,149
and the least rep Network we used the

00:16:23,279 --> 00:16:28,559
wave linked basically shared path over

00:16:27,149 --> 00:16:31,709
at r5 connection between two data

00:16:28,559 --> 00:16:36,439
centers here over this network we create

00:16:31,709 --> 00:16:38,669
the layer 2 network with some feelings

00:16:36,439 --> 00:16:40,829
yeah we use this to transport a few

00:16:38,669 --> 00:16:43,970
lines between the two sides and we also

00:16:40,829 --> 00:16:46,499
use this to do some feel and mapping

00:16:43,970 --> 00:16:48,839
because servant used certain feel and

00:16:46,499 --> 00:16:51,059
numbering in their network we used field

00:16:48,839 --> 00:16:53,609
and numbering in our network and they

00:16:51,059 --> 00:16:55,980
overlap so therefore we had to renumber

00:16:53,609 --> 00:16:57,779
some feelings on certain sides to a

00:16:55,980 --> 00:17:01,350
different number only strap so we will

00:16:57,779 --> 00:17:03,149
not get conflict there so this is an

00:17:01,350 --> 00:17:06,510
image on how the network connection

00:17:03,149 --> 00:17:08,220
looks like we have on the left side we

00:17:06,510 --> 00:17:10,949
have five servant network connected

00:17:08,220 --> 00:17:12,569
through a router in the internet on the

00:17:10,949 --> 00:17:14,760
LeaseWeb side we will have our cloud

00:17:12,569 --> 00:17:16,980
platform as well connected to the router

00:17:14,760 --> 00:17:19,860
and the internet and we created a wave

00:17:16,980 --> 00:17:23,970
connection between the two routers on

00:17:19,860 --> 00:17:26,579
both sides this is how the traffic flow

00:17:23,970 --> 00:17:28,769
looks like so traffic only existing

00:17:26,579 --> 00:17:29,909
virtual machines we don't serve and that

00:17:28,769 --> 00:17:31,200
we're not migrated yet we'll still

00:17:29,909 --> 00:17:33,659
connect to the internet from the servant

00:17:31,200 --> 00:17:37,289
router and all the machines that were

00:17:33,659 --> 00:17:39,510
migrated and will also connect over the

00:17:37,289 --> 00:17:43,420
internet interconnect towards her friend

00:17:39,510 --> 00:17:46,210
routers and enter the internet there

00:17:43,420 --> 00:17:48,640
after migration was completed we

00:17:46,210 --> 00:17:50,260
disabled to the interconnect and from

00:17:48,640 --> 00:17:52,680
there we switch the gateways from the

00:17:50,260 --> 00:17:54,700
surf and routers to the reverb routers

00:17:52,680 --> 00:17:56,470
start announcing the traffic for these

00:17:54,700 --> 00:17:58,240
subnets on the leash up side and then

00:17:56,470 --> 00:18:03,610
all the traffic will flow to the

00:17:58,240 --> 00:18:06,310
internet over these republics for a

00:18:03,610 --> 00:18:07,930
feline remapping for a female we use

00:18:06,310 --> 00:18:09,940
this wave connection also for the VLAN

00:18:07,930 --> 00:18:12,670
remapping so this is an example on how

00:18:09,940 --> 00:18:14,260
we did it as you can see on the left

00:18:12,670 --> 00:18:16,750
side we have one switch that has two

00:18:14,260 --> 00:18:19,810
villains which are connected to another

00:18:16,750 --> 00:18:21,640
switch over one link with a trunk

00:18:19,810 --> 00:18:24,850
so both feelings will be transported off

00:18:21,640 --> 00:18:28,060
this link to the new switch on the lease

00:18:24,850 --> 00:18:32,500
website we connected to switch with two

00:18:28,060 --> 00:18:36,400
separate links to an additional switch

00:18:32,500 --> 00:18:39,370
and here we sent the link untagged over

00:18:36,400 --> 00:18:42,820
the line so we gave both sides the port

00:18:39,370 --> 00:18:44,760
an access mode where on the one side it

00:18:42,820 --> 00:18:48,400
will have fur in this example feed on 10

00:18:44,760 --> 00:18:50,650
will be untaek over the the link to a

00:18:48,400 --> 00:18:52,420
new switch and here the we will tag that

00:18:50,650 --> 00:18:55,150
feed line again or the network again

00:18:52,420 --> 00:18:57,970
with a new feline number what this

00:18:55,150 --> 00:18:59,200
example ad is for to feelin's we have

00:18:57,970 --> 00:19:01,120
depending on the amount of cables you

00:18:59,200 --> 00:19:04,500
pull between your switches you can do

00:19:01,120 --> 00:19:07,630
this up to 48 or it's switch 48 villains

00:19:04,500 --> 00:19:09,940
we had a depends bit on your need so

00:19:07,630 --> 00:19:15,580
that's the way we translated the feel

00:19:09,940 --> 00:19:17,940
and an additional feature we built yes

00:19:15,580 --> 00:19:20,050
the classic multi neck support

00:19:17,940 --> 00:19:22,090
we talked about this a little bit

00:19:20,050 --> 00:19:25,840
yesterday in our presentation

00:19:22,090 --> 00:19:28,720
so because the instances from we

00:19:25,840 --> 00:19:31,750
migrated had IP addresses from multiple

00:19:28,720 --> 00:19:34,090
networks from multiple VLANs email

00:19:31,750 --> 00:19:37,000
Teaneck support was built in which

00:19:34,090 --> 00:19:39,520
security groups this allowed us to add

00:19:37,000 --> 00:19:40,750
multiple IP addresses from the same

00:19:39,520 --> 00:19:43,750
subnet

00:19:40,750 --> 00:19:45,370
on one interface and we were able to add

00:19:43,750 --> 00:19:47,920
multiple network interfaces to the same

00:19:45,370 --> 00:19:52,510
virtual machine each from its own feel

00:19:47,920 --> 00:19:54,220
and with network addresses including

00:19:52,510 --> 00:19:56,530
having the support for the firewalling

00:19:54,220 --> 00:19:58,330
of the advanced security groups so that

00:19:56,530 --> 00:20:05,260
customers could still allow certain

00:19:58,330 --> 00:20:08,230
ports only to network so in order to get

00:20:05,260 --> 00:20:10,120
this working with multiple IPS on

00:20:08,230 --> 00:20:12,730
multiple different multiple interfaces

00:20:10,120 --> 00:20:14,740
of the authorities or gateways you

00:20:12,730 --> 00:20:18,370
actually have to implement what's called

00:20:14,740 --> 00:20:21,460
this policy based routing which is news

00:20:18,370 --> 00:20:23,650
truce to some people but it's basically

00:20:21,460 --> 00:20:26,710
used to make routing decisions based on

00:20:23,650 --> 00:20:28,150
policy seven through the creation it's

00:20:26,710 --> 00:20:30,880
designed for multiple network interfaces

00:20:28,150 --> 00:20:32,950
on different networks and inbound

00:20:30,880 --> 00:20:34,930
packets are to return to the configure

00:20:32,950 --> 00:20:37,840
interface not just I'll just eat zero

00:20:34,930 --> 00:20:40,300
but whatever it's if you have a package

00:20:37,840 --> 00:20:44,140
bound from eath one it'll come back and

00:20:40,300 --> 00:20:45,910
each one if it needs to in order to

00:20:44,140 --> 00:20:48,010
configure this with santo with sent OS

00:20:45,910 --> 00:20:51,580
which was our primary target for doing

00:20:48,010 --> 00:20:54,040
this you have to actually add each

00:20:51,580 --> 00:20:56,950
interface to the Artie tables IP route

00:20:54,040 --> 00:21:00,130
configuration that this will create a

00:20:56,950 --> 00:21:05,140
table for each interface which will be

00:21:00,130 --> 00:21:06,640
configuring later right now next you

00:21:05,140 --> 00:21:10,120
have to add the gateway routing

00:21:06,640 --> 00:21:13,690
configuration this is sent to us this is

00:21:10,120 --> 00:21:17,530
actually a route - Heath configuration

00:21:13,690 --> 00:21:20,230
file so each wanting to whatever the

00:21:17,530 --> 00:21:22,330
configuration here is default table 0x0

00:21:20,230 --> 00:21:25,570
gateway obviously you replace T 0 gate

00:21:22,330 --> 00:21:26,860
with your either and the table name is

00:21:25,570 --> 00:21:29,920
right there and that's the same one you

00:21:26,860 --> 00:21:31,000
added to the RT tables file and again

00:21:29,920 --> 00:21:34,000
you need to add this - for every

00:21:31,000 --> 00:21:35,710
interface you have this will define the

00:21:34,000 --> 00:21:40,840
gateway which traffic should wrap

00:21:35,710 --> 00:21:43,720
through for that interface next for each

00:21:40,840 --> 00:21:47,770
IP on that interface and we had quite a

00:21:43,720 --> 00:21:50,200
few IPS you actually have to add an

00:21:47,770 --> 00:21:52,630
alias file for a loose wheel file and

00:21:50,200 --> 00:21:56,170
that's rule - eath whatever interface

00:21:52,630 --> 00:21:57,730
you have so from IP address one table

00:21:56,170 --> 00:22:00,010
these arose again that's the same table

00:21:57,730 --> 00:22:01,540
than the Artie table as well and that's

00:22:00,010 --> 00:22:03,010
is in addition to the normal

00:22:01,540 --> 00:22:05,880
configuration files you'd have to write

00:22:03,010 --> 00:22:09,490
for the interfaces incentive us so the

00:22:05,880 --> 00:22:11,370
if config - eath whatever

00:22:09,490 --> 00:22:13,559
and this will get your gateway routing

00:22:11,370 --> 00:22:15,549
correctly through the right interface

00:22:13,559 --> 00:22:21,480
which we had to do in order to get this

00:22:15,549 --> 00:22:23,740
working so well that since we learned

00:22:21,480 --> 00:22:28,029
and the mistakes we made so you don't

00:22:23,740 --> 00:22:31,960
have to the first one is get a proper

00:22:28,029 --> 00:22:33,490
test environment we set up initially a

00:22:31,960 --> 00:22:37,149
test environment to test in migrations

00:22:33,490 --> 00:22:39,880
and we received the test hypervisor with

00:22:37,149 --> 00:22:42,909
firchau so we shut up test CloudStack

00:22:39,880 --> 00:22:45,370
environments started to convert with

00:22:42,909 --> 00:22:48,820
this e 2 v 2 from parallels article

00:22:45,370 --> 00:22:50,710
further machine it writes the converted

00:22:48,820 --> 00:22:53,770
disk on the local hypervisor where the

00:22:50,710 --> 00:22:54,279
container was running and then we copied

00:22:53,770 --> 00:22:57,220
it over

00:22:54,279 --> 00:22:58,330
however in the end we found out that the

00:22:57,220 --> 00:22:59,620
source hypervisor is where the

00:22:58,330 --> 00:23:02,770
containers running we're not all the

00:22:59,620 --> 00:23:04,690
same some of them had hundreds of

00:23:02,770 --> 00:23:06,490
gigabytes of free disk space others had

00:23:04,690 --> 00:23:08,429
five gigabytes of free disk space and

00:23:06,490 --> 00:23:10,600
then we tried to pull the container

00:23:08,429 --> 00:23:12,429
where it tries to look right locally and

00:23:10,600 --> 00:23:14,080
there is no disk space well that will

00:23:12,429 --> 00:23:16,149
give you into problem and trouble so

00:23:14,080 --> 00:23:17,740
after 5 gigabytes filling it up it will

00:23:16,149 --> 00:23:20,110
have a full hard drive and then your

00:23:17,740 --> 00:23:21,850
migration would stop so make sure you

00:23:20,110 --> 00:23:25,029
check this before you start the

00:23:21,850 --> 00:23:33,010
migration so that everything is covered

00:23:25,029 --> 00:23:35,049
so one thing I needed stress is you

00:23:33,010 --> 00:23:37,299
should try to use open source tools

00:23:35,049 --> 00:23:42,580
whenever possible there's a lot more

00:23:37,299 --> 00:23:44,710
community support around them and do not

00:23:42,580 --> 00:23:46,210
use parallelize PTV for a KVM conversion

00:23:44,710 --> 00:23:48,730
I'm sure it works fine for apparels vm

00:23:46,210 --> 00:23:52,350
for conversion but I would learn from

00:23:48,730 --> 00:23:55,840
this and would not try to use that again

00:23:52,350 --> 00:23:58,360
the reason why we really promote open

00:23:55,840 --> 00:23:59,890
source tools is because there's there's

00:23:58,360 --> 00:24:01,390
always a community backing it there's

00:23:59,890 --> 00:24:04,000
always if you can see the source code

00:24:01,390 --> 00:24:05,860
obviously but you can actually get

00:24:04,000 --> 00:24:07,419
support a lot quicker actually out of

00:24:05,860 --> 00:24:15,340
the community then you can usually out

00:24:07,419 --> 00:24:17,399
of vendors I found yeah one lesson we

00:24:15,340 --> 00:24:20,470
learned while creating the internet

00:24:17,399 --> 00:24:22,419
interconnection between the two sides is

00:24:20,470 --> 00:24:23,060
that make sure when this thing has been

00:24:22,419 --> 00:24:24,380
set up

00:24:23,060 --> 00:24:27,410
the routing policy on your router is

00:24:24,380 --> 00:24:32,020
also correctly updated the first batch

00:24:27,410 --> 00:24:35,090
of migrations we started to migrate

00:24:32,020 --> 00:24:36,140
after 5-10 minutes our first arts we

00:24:35,090 --> 00:24:39,380
received a call from the network

00:24:36,140 --> 00:24:41,030
engineers in the US telling us what we

00:24:39,380 --> 00:24:43,790
were doing they saw up links being

00:24:41,030 --> 00:24:46,820
congested they like lost all the links

00:24:43,790 --> 00:24:48,080
and after a short investigation it

00:24:46,820 --> 00:24:50,390
turned out that the data was not

00:24:48,080 --> 00:24:53,060
migrated over the internal link but over

00:24:50,390 --> 00:24:55,420
the public internet links which is not a

00:24:53,060 --> 00:24:57,860
good way area traffic pulse lot of money

00:24:55,420 --> 00:24:59,300
so make sure your router policies are

00:24:57,860 --> 00:25:01,900
set up correctly before starting the

00:24:59,300 --> 00:25:07,430
migration and this prevents of links

00:25:01,900 --> 00:25:11,450
congestion or your network so one thing

00:25:07,430 --> 00:25:13,420
we tried to implement while I was trying

00:25:11,450 --> 00:25:16,370
to figure out in the process between

00:25:13,420 --> 00:25:20,450
setting up the arsenic method and the

00:25:16,370 --> 00:25:22,490
PCB school method is setting up the file

00:25:20,450 --> 00:25:23,690
system transfer locally of INFs because

00:25:22,490 --> 00:25:25,220
I was running into issues where

00:25:23,690 --> 00:25:28,310
hypervisors didn't have any disk space

00:25:25,220 --> 00:25:32,000
do you actually convert this process so

00:25:28,310 --> 00:25:34,730
what I did was mount just tried mounting

00:25:32,000 --> 00:25:38,930
the NFS an NFS amount on the local

00:25:34,730 --> 00:25:40,910
hypervisor itself and - - in places that

00:25:38,930 --> 00:25:44,530
temporary directory and what happened

00:25:40,910 --> 00:25:49,730
was ACL problems everywhere basically

00:25:44,530 --> 00:25:52,720
and I'm sure this is possible but it's

00:25:49,730 --> 00:25:54,860
just not really recommended I say I

00:25:52,720 --> 00:25:56,210
recommend just using rsync whenever

00:25:54,860 --> 00:25:58,430
possible because it's really designed to

00:25:56,210 --> 00:26:00,470
transfer file systems or just straight

00:25:58,430 --> 00:26:06,830
copy the disk if you can but that really

00:26:00,470 --> 00:26:09,170
was an option for us and don't be afraid

00:26:06,830 --> 00:26:11,420
to automate with guest FS and yes fish

00:26:09,170 --> 00:26:13,130
guest fish is a part of guest of us the

00:26:11,420 --> 00:26:16,190
guest FS is my fave moment one of my

00:26:13,130 --> 00:26:18,290
favorite open-source projects but it

00:26:16,190 --> 00:26:20,450
allows you to modify this images to

00:26:18,290 --> 00:26:23,620
modify and automate anything within the

00:26:20,450 --> 00:26:25,850
virtualized environment it'll set up a

00:26:23,620 --> 00:26:28,490
set up a virtualized environment for you

00:26:25,850 --> 00:26:31,400
it'll detect your OS it will upload

00:26:28,490 --> 00:26:32,720
files to a disk it will anything you

00:26:31,400 --> 00:26:36,310
want to do with virtualized environments

00:26:32,720 --> 00:26:36,310
guests of us can do it

00:26:38,000 --> 00:26:44,700
yeah another lesson we learn is when we

00:26:41,640 --> 00:26:46,680
started to import the templates on a new

00:26:44,700 --> 00:26:48,990
cloud platform is that you should not be

00:26:46,680 --> 00:26:52,830
confused when after 30 minutes of blood

00:26:48,990 --> 00:26:55,050
with this data it suddenly stops by

00:26:52,830 --> 00:26:57,120
default cloud stack has some global

00:26:55,050 --> 00:27:00,660
settings specifying the maximum upload

00:26:57,120 --> 00:27:03,510
size for a templates and a maximum

00:27:00,660 --> 00:27:05,220
storage volume size make sure that you

00:27:03,510 --> 00:27:08,790
change these settings to the needs you

00:27:05,220 --> 00:27:10,680
have to check which size is your largest

00:27:08,790 --> 00:27:15,870
polish and make sure you update the

00:27:10,680 --> 00:27:17,970
settings accordingly yeah because we had

00:27:15,870 --> 00:27:20,160
a YouTube price off the first a block it

00:27:17,970 --> 00:27:23,580
gave us an error message said blow

00:27:20,160 --> 00:27:24,810
filters like okay why did it fail after

00:27:23,580 --> 00:27:26,910
inspecting the log files we actually

00:27:24,810 --> 00:27:30,300
notice that the storage sizes where the

00:27:26,910 --> 00:27:34,380
problem so make sure you check this

00:27:30,300 --> 00:27:35,880
before you start migrating data another

00:27:34,380 --> 00:27:39,560
lesson we learned while secondary

00:27:35,880 --> 00:27:42,570
storage volumes and the way this worked

00:27:39,560 --> 00:27:44,970
all the templates converted containers

00:27:42,570 --> 00:27:46,920
we started to a blood worker blood as a

00:27:44,970 --> 00:27:49,770
template which means they were uploaded

00:27:46,920 --> 00:27:53,570
to the secondary storage volume after a

00:27:49,770 --> 00:27:57,510
few uploads are fully quite big with

00:27:53,570 --> 00:28:00,930
uploaded data and we ended up adding

00:27:57,510 --> 00:28:02,400
more secondary search volumes and that's

00:28:00,930 --> 00:28:04,080
that cassis nice button at secondary

00:28:02,400 --> 00:28:04,760
storage you can just add as many as you

00:28:04,080 --> 00:28:08,100
want

00:28:04,760 --> 00:28:10,710
we added a few more ended up creating 10

00:28:08,100 --> 00:28:13,800
secondary storage volumes and we started

00:28:10,710 --> 00:28:16,020
our blood the problem we found out here

00:28:13,800 --> 00:28:18,870
is that when you start the blood and you

00:28:16,020 --> 00:28:20,340
have 10 secondary storage volumes the

00:28:18,870 --> 00:28:22,140
file will be downloaded 10 times and

00:28:20,340 --> 00:28:26,130
will write 10 times to each different

00:28:22,140 --> 00:28:27,630
volume which gives you a bottleneck on

00:28:26,130 --> 00:28:29,940
your secondary starts p.m. because it

00:28:27,630 --> 00:28:31,680
will handle all these downloads the up

00:28:29,940 --> 00:28:33,990
links only secondary starts p.m. book on

00:28:31,680 --> 00:28:36,900
you start to congest the downloads will

00:28:33,990 --> 00:28:38,370
start to slow down and people give you

00:28:36,900 --> 00:28:40,500
eventually very poor performance on your

00:28:38,370 --> 00:28:42,690
cloud platform when deploying

00:28:40,500 --> 00:28:44,340
secondary source films here what we

00:28:42,690 --> 00:28:46,040
ended up doing is removing

00:28:44,340 --> 00:28:49,230
nine secondary storage volumes again and

00:28:46,040 --> 00:28:50,500
after we imported the template or a

00:28:49,230 --> 00:28:52,179
machine

00:28:50,500 --> 00:28:54,429
and deploy the machine did the

00:28:52,179 --> 00:28:55,990
deployment with this machine we end up

00:28:54,429 --> 00:28:58,510
throwing away the template again to make

00:28:55,990 --> 00:29:01,840
space for new web loads

00:28:58,510 --> 00:29:03,490
yeah so one of the features we have one

00:29:01,840 --> 00:29:05,590
our backlog for our programmers to work

00:29:03,490 --> 00:29:07,120
on it so actually half the secondary

00:29:05,590 --> 00:29:08,799
storage volume worked in the same way as

00:29:07,120 --> 00:29:11,409
the primary starts volume that means

00:29:08,799 --> 00:29:13,320
when a download is triggered the

00:29:11,409 --> 00:29:15,460
secondary start storage VM should

00:29:13,320 --> 00:29:16,929
actually look at which volume has the

00:29:15,460 --> 00:29:20,530
most capacity available and should just

00:29:16,929 --> 00:29:20,980
save it once indefinitely so that's a

00:29:20,530 --> 00:29:25,780
nice feature

00:29:20,980 --> 00:29:28,450
yeah we want to add in cloud stack so I

00:29:25,780 --> 00:29:30,429
think we found out is when we're off so

00:29:28,450 --> 00:29:33,159
we had to upload all of these container

00:29:30,429 --> 00:29:35,289
templates right yeah we modified them we

00:29:33,159 --> 00:29:36,730
loaded our scripts we are changing

00:29:35,289 --> 00:29:38,950
aggression files we installed grow up

00:29:36,730 --> 00:29:42,309
all that stuff so we had to upload all

00:29:38,950 --> 00:29:43,720
these things torque low spec and we

00:29:42,309 --> 00:29:47,679
found that when you're doing this in

00:29:43,720 --> 00:29:50,650
mass some of these some of the secondary

00:29:47,679 --> 00:29:54,159
storage being issues were brought to

00:29:50,650 --> 00:29:54,940
light basically the internet isn't

00:29:54,159 --> 00:29:58,030
perfect right

00:29:54,940 --> 00:30:00,659
we have timeouts we have TCP drops we

00:29:58,030 --> 00:30:03,309
have just connection drops and we found

00:30:00,659 --> 00:30:04,630
there's Centers of these we're just

00:30:03,309 --> 00:30:07,000
we'll just failed on the secondary

00:30:04,630 --> 00:30:08,110
storage again the download will just

00:30:07,000 --> 00:30:10,690
time out and we had to go in the

00:30:08,110 --> 00:30:13,539
database and actually just manually air

00:30:10,690 --> 00:30:15,460
these out there is no ability ability to

00:30:13,539 --> 00:30:18,700
retry the down Bluetooth lead retry the

00:30:15,460 --> 00:30:20,230
upload there and that would be something

00:30:18,700 --> 00:30:22,750
on other than the future we'd like to

00:30:20,230 --> 00:30:25,650
have into contact so another thing we

00:30:22,750 --> 00:30:28,270
should we're probably gonna look into

00:30:25,650 --> 00:30:30,730
since again this is not really a normal

00:30:28,270 --> 00:30:32,799
use case since you usually just upload a

00:30:30,730 --> 00:30:34,840
small ISO or a small you know template

00:30:32,799 --> 00:30:37,659
but we're talking about about hundreds

00:30:34,840 --> 00:30:39,880
of gigabytes here but it would be would

00:30:37,659 --> 00:30:42,220
be good to retry that upload or at least

00:30:39,880 --> 00:30:47,650
a set number of retries all the

00:30:42,220 --> 00:30:50,710
configuration that'd be nice and then

00:30:47,650 --> 00:30:52,419
the last less concerned it's not the

00:30:50,710 --> 00:30:56,049
conversion time for the secondary starts

00:30:52,419 --> 00:30:59,049
VM by default is really slow we started

00:30:56,049 --> 00:31:02,860
web love templates deploy for virtual

00:30:59,049 --> 00:31:03,970
machines from these templates and our

00:31:02,860 --> 00:31:05,080
downloads

00:31:03,970 --> 00:31:07,210
our secondary star schemas connect

00:31:05,080 --> 00:31:10,090
Houston on our hypervisors which have 20

00:31:07,210 --> 00:31:12,340
gigabytes of internet connection using

00:31:10,090 --> 00:31:13,750
the fertile driver we should be able to

00:31:12,340 --> 00:31:16,510
get reasonable speeds at least up to 10

00:31:13,750 --> 00:31:19,080
gigabits but we sure saw that the

00:31:16,510 --> 00:31:21,190
downloads were going at under 200 am bit

00:31:19,080 --> 00:31:23,920
and after looking into the secondary

00:31:21,190 --> 00:31:26,580
storage vm we found out that the default

00:31:23,920 --> 00:31:28,170
deployment size was I think one core and

00:31:26,580 --> 00:31:31,540
265 mm

00:31:28,170 --> 00:31:34,060
megabytes of memory or 512 MB sure it's

00:31:31,540 --> 00:31:35,920
really really love a month of CPU

00:31:34,060 --> 00:31:38,050
resources and memory secondary starts VM

00:31:35,920 --> 00:31:40,360
has and if you give it a lot of tasks to

00:31:38,050 --> 00:31:41,440
do this resource settings are just not

00:31:40,360 --> 00:31:42,880
enough

00:31:41,440 --> 00:31:46,090
we ended up shutting down this machine

00:31:42,880 --> 00:31:47,650
upgrading it to 1600 rpm with 16

00:31:46,090 --> 00:31:49,930
gigabytes of memory turning it back on

00:31:47,650 --> 00:31:52,210
and when we started the new downloads

00:31:49,930 --> 00:31:55,750
they all started to download that max

00:31:52,210 --> 00:31:57,910
out at 10 15 gigabits so make sure when

00:31:55,750 --> 00:31:58,990
you start to really use your secondary

00:31:57,910 --> 00:32:01,120
storage vm and you do need the

00:31:58,990 --> 00:32:03,700
performance to increase the resources

00:32:01,120 --> 00:32:06,100
for this you don't have to do 16

00:32:03,700 --> 00:32:08,170
gigabytes of memory and 16 cores like we

00:32:06,100 --> 00:32:10,780
did we had it available on the platform

00:32:08,170 --> 00:32:15,880
service out there why not but make sure

00:32:10,780 --> 00:32:18,780
you adjust it to your needs that's it

00:32:15,880 --> 00:32:18,780
any questions

00:32:27,090 --> 00:32:33,460
initially it was for this project so

00:32:29,410 --> 00:32:34,600
leisure quad servant in 2018 since we

00:32:33,460 --> 00:32:36,460
want to standardize on one cloud

00:32:34,600 --> 00:32:38,380
platform we migrated it from this

00:32:36,460 --> 00:32:41,140
company it's open for us platform to

00:32:38,380 --> 00:32:42,370
hourly staff dunsack platform so in this

00:32:41,140 --> 00:32:44,410
case yeah it is one-off

00:32:42,370 --> 00:32:46,870
however if these rep decides to buy

00:32:44,410 --> 00:32:48,690
another company in the future we could

00:32:46,870 --> 00:32:51,130
reuse most of the methods in code

00:32:48,690 --> 00:33:03,190
depending on the platform which that

00:32:51,130 --> 00:33:05,430
company has so yeah yeah so we can in

00:33:03,190 --> 00:33:09,340
the current state as it's currently

00:33:05,430 --> 00:33:10,600
probably not yeah sure if there's an

00:33:09,340 --> 00:33:13,290
interest on the community to use these

00:33:10,600 --> 00:33:15,610
two links we can optimize it make sure

00:33:13,290 --> 00:33:17,929
the you have some bug fixes and

00:33:15,610 --> 00:33:33,200
unpublish it or some will get up

00:33:17,929 --> 00:33:36,860
yeah yeah we have all the codes on our

00:33:33,200 --> 00:33:38,570
internal git repository so yeah we you

00:33:36,860 --> 00:33:42,649
can clean this open and publish it yeah

00:33:38,570 --> 00:33:44,509
we actually I use the code too

00:33:42,649 --> 00:33:47,629
it's a great dedicated servers into

00:33:44,509 --> 00:33:49,190
LeaseWeb garden into plastic as well so

00:33:47,629 --> 00:33:53,450
very much pretty much if I can rsync it

00:33:49,190 --> 00:33:55,460
it'll work with an obvious data

00:33:53,450 --> 00:33:57,289
limitations of your converter box and

00:33:55,460 --> 00:33:58,580
all this all that stuff yeah it's

00:33:57,289 --> 00:34:08,799
something that we've really cold here

00:33:58,580 --> 00:34:08,799
for any questions okay thank you

00:34:09,489 --> 00:34:12,179

YouTube URL: https://www.youtube.com/watch?v=mmWs9H7dnNA


