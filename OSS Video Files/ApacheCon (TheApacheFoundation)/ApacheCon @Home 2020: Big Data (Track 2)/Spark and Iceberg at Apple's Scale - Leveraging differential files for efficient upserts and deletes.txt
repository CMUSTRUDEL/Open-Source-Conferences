Title: Spark and Iceberg at Apple's Scale - Leveraging differential files for efficient upserts and deletes
Publication date: 2020-10-21
Playlist: ApacheCon @Home 2020: Big Data (Track 2)
Description: 
	Spark and Iceberg at Apple's Scale - Leveraging differential files for efficient upserts and deletes
Anton Okolnychyi, Vishwanath Lakkundi

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apple leverages Apache Spark for processing large datasets to power key components of Apple’s production services. As users begin to use Apache Spark in a bigger range of data processing scenarios, it is essential to support efficient and transactional update/delete/merge operations even in read-mostly data lake environments. For example, such functionality is required to implement change data capture, support some forms of slowly changing dimensions in data warehousing, fix corrupted records without rewriting complete partitions. The original implementation of update/delete/merge operations in Apple's internal version of Apache Spark relied on snapshot isolation in Apache Iceberg and rewriting complete files if at least one record had to be changed. This approach performs well if we can limit the scope of updates/deletes to a small number of files using indexing. However, modifying a couple of records in a large number of files is still expensive as all unmodified records in touched files have to be copied over. Therefore, Apple collaborates with other members of the Apache Iceberg and Apache Spark communities on a way to leverage differential files, an efficient method for storing large and volatile databases, for update/delete/merge operations. This approach allows us to reduce write amplification, support online updates to data warehouses and sustain more concurrent operations on the same table. This talk will briefly describe common ways to implement updates in analytical databases, challenges between providing updates and optimizing data structures for reading, outline the proposed solution alongside its benefits and drawbacks.

Anton is a committer and PMC member of Apache Iceberg as well as an Apache Spark contributor at Apple. He has been dealing with internals of various Big Data systems for the last 5 years. At Apple, Anton is working on data lakes and an elastic, on-demand, secure, and fully managed Spark as a service. Prior to joining Apple, he optimized and extended a proprietary Spark distribution at SAP. Anton holds a Master’s degree in Computer Science from RWTH Aachen University.
Vishwanath Lakkundi is the engineering lead for the team that focuses on Data Orchestration and Data Lake at Apple. This team is responsible for development of an elastic fully managed Apache Spark as a service, a Data Lake engine based on Apache Iceberg and a data pipelines product based on Apache Airflow. He has been working with Apple since the lastseven years focusing on various analytics infrastructure and platform products.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,720 --> 00:00:29,599
all right

00:00:26,880 --> 00:00:30,960
uh good morning everybody i hope uh you

00:00:29,599 --> 00:00:33,280
can see my screen here

00:00:30,960 --> 00:00:34,000
um we'll be also switching the screens

00:00:33,280 --> 00:00:37,040
in between

00:00:34,000 --> 00:00:40,239
um as anton takes over in between and

00:00:37,040 --> 00:00:42,480
uh so please bear with us um

00:00:40,239 --> 00:00:44,399
so uh we're gonna be talking about spark

00:00:42,480 --> 00:00:46,719
and iceberg at apple uh

00:00:44,399 --> 00:00:47,840
at apple scale a little bit and we'll

00:00:46,719 --> 00:00:50,640
we'll talk a bit about

00:00:47,840 --> 00:00:50,960
um why do we need upsets and deletes and

00:00:50,640 --> 00:00:52,800
uh

00:00:50,960 --> 00:00:54,079
what's the plan for doing upsets and

00:00:52,800 --> 00:00:57,520
deletes using

00:00:54,079 --> 00:01:00,320
iceberg um a little bit about us

00:00:57,520 --> 00:01:00,320
uh and all

00:01:02,960 --> 00:01:08,320
we can't hear you hey guys

00:01:06,320 --> 00:01:10,560
so my name is anthon i am a software

00:01:08,320 --> 00:01:12,720
engineer focusing on data leaks

00:01:10,560 --> 00:01:14,240
and we ensure that data lakes are

00:01:12,720 --> 00:01:16,799
efficient and reliable

00:01:14,240 --> 00:01:18,640
at apple cloud services and i'm also tmc

00:01:16,799 --> 00:01:22,320
member for apache iceberg

00:01:18,640 --> 00:01:22,320
and apache spark contributor

00:01:23,280 --> 00:01:26,479
thanks anton my name is rishwa i'm an

00:01:25,520 --> 00:01:29,119
engineering manager

00:01:26,479 --> 00:01:32,159
and lead here at apple cloud services

00:01:29,119 --> 00:01:35,439
data orchestration and data lake teams

00:01:32,159 --> 00:01:37,439
um a little bit about us so we are part

00:01:35,439 --> 00:01:39,840
of data orchestration and data lake team

00:01:37,439 --> 00:01:40,640
um the overarching arc that we are part

00:01:39,840 --> 00:01:44,000
of

00:01:40,640 --> 00:01:44,560
provides a data platform uh to all of

00:01:44,000 --> 00:01:48,560
apple

00:01:44,560 --> 00:01:51,840
uh we primarily focus on apache spark

00:01:48,560 --> 00:01:53,119
iceberg and airflow we have committers

00:01:51,840 --> 00:01:56,079
and pmc members for

00:01:53,119 --> 00:01:57,439
all three of these we contribute to open

00:01:56,079 --> 00:02:00,640
source park heavily

00:01:57,439 --> 00:02:04,560
we have contribute to iceberg

00:02:00,640 --> 00:02:06,560
and as well as to airflow

00:02:04,560 --> 00:02:08,640
with respect to customers we have users

00:02:06,560 --> 00:02:11,520
from all over apple starting from

00:02:08,640 --> 00:02:12,239
siri to maps to apple media products

00:02:11,520 --> 00:02:15,280
icloud

00:02:12,239 --> 00:02:17,120
itunes hardware engineering and various

00:02:15,280 --> 00:02:20,319
different data engineering and data

00:02:17,120 --> 00:02:22,560
science teams all across apple

00:02:20,319 --> 00:02:23,440
agenda today uh we're going to be

00:02:22,560 --> 00:02:25,440
talking about

00:02:23,440 --> 00:02:26,879
uh spark and iceberg infrastructure at

00:02:25,440 --> 00:02:29,200
apple um

00:02:26,879 --> 00:02:30,000
upsets and delete related requirements

00:02:29,200 --> 00:02:31,280
uh uh

00:02:30,000 --> 00:02:34,000
how do we do upsets and deletes an

00:02:31,280 --> 00:02:37,040
iceberg and lukey takeaways

00:02:34,000 --> 00:02:38,720
um spark and iceberg at apple um we do

00:02:37,040 --> 00:02:41,200
have another session later today at

00:02:38,720 --> 00:02:42,959
10 35 where we'll be talking more about

00:02:41,200 --> 00:02:43,519
uh the history of data infrastructure at

00:02:42,959 --> 00:02:45,280
apple

00:02:43,519 --> 00:02:47,040
how we have evolved over a couple of

00:02:45,280 --> 00:02:48,560
years um focusing on

00:02:47,040 --> 00:02:50,239
all the other pieces of data

00:02:48,560 --> 00:02:52,239
infrastructure as well

00:02:50,239 --> 00:02:54,720
however in this talk we'll primarily

00:02:52,239 --> 00:02:58,080
focus just on spark and iceberg so

00:02:54,720 --> 00:03:00,720
diving right into spark we provide

00:02:58,080 --> 00:03:01,519
scalable elastic on demand spark as a

00:03:00,720 --> 00:03:04,400
service

00:03:01,519 --> 00:03:05,440
we launched it back in 2018 uh since

00:03:04,400 --> 00:03:07,920
then we have

00:03:05,440 --> 00:03:10,000
about a few hundred thousand jobs that

00:03:07,920 --> 00:03:11,760
we run every day we have over

00:03:10,000 --> 00:03:13,599
three million executives that start

00:03:11,760 --> 00:03:16,159
every day um they use

00:03:13,599 --> 00:03:17,120
somewhere around four petabytes of ram

00:03:16,159 --> 00:03:19,280
um

00:03:17,120 --> 00:03:20,239
across various different data centers of

00:03:19,280 --> 00:03:22,400
apple

00:03:20,239 --> 00:03:23,360
we do have a lot of spark that runs on

00:03:22,400 --> 00:03:25,120
yarn as well

00:03:23,360 --> 00:03:27,040
so as we are trying to modernize the

00:03:25,120 --> 00:03:27,680
infrastructure a lot of these workloads

00:03:27,040 --> 00:03:32,000
have moved

00:03:27,680 --> 00:03:33,760
off of yarn onto the spark as a service

00:03:32,000 --> 00:03:35,120
platform that we are providing but

00:03:33,760 --> 00:03:37,120
however we do have a lot of

00:03:35,120 --> 00:03:39,519
yarn workloads spark workloads that run

00:03:37,120 --> 00:03:41,599
on yarn we do provide an internal

00:03:39,519 --> 00:03:44,640
distribution of spark

00:03:41,599 --> 00:03:46,480
that people use on yarn basically

00:03:44,640 --> 00:03:48,239
few features that we provide for spark

00:03:46,480 --> 00:03:49,760
as a service uh one of the core

00:03:48,239 --> 00:03:52,000
principles here is to make sure

00:03:49,760 --> 00:03:52,959
we enable disaggregated architecture

00:03:52,000 --> 00:03:55,519
disaggregation

00:03:52,959 --> 00:03:56,400
is basically about desegregating storage

00:03:55,519 --> 00:03:57,920
and compute

00:03:56,400 --> 00:04:00,080
so that you can scale up storage

00:03:57,920 --> 00:04:02,720
separately and compute separately more

00:04:00,080 --> 00:04:04,239
about it in the next stock

00:04:02,720 --> 00:04:06,640
we provide an enhanced power current

00:04:04,239 --> 00:04:08,480
time a lot of libraries that we

00:04:06,640 --> 00:04:10,080
develop internally some of them are

00:04:08,480 --> 00:04:13,200
available externally as well

00:04:10,080 --> 00:04:14,959
are part of the spark runtime we support

00:04:13,200 --> 00:04:17,280
multiple versions of spark users can

00:04:14,959 --> 00:04:19,199
choose to uh jump between the versions

00:04:17,280 --> 00:04:19,919
test out different questions and upgrade

00:04:19,199 --> 00:04:22,560
as

00:04:19,919 --> 00:04:23,919
needed we provide secrets and network

00:04:22,560 --> 00:04:25,199
apples so that you can connect to

00:04:23,919 --> 00:04:28,560
restricted networks

00:04:25,199 --> 00:04:31,440
private networks and such

00:04:28,560 --> 00:04:32,080
service discovery and routing you can

00:04:31,440 --> 00:04:34,720
discover

00:04:32,080 --> 00:04:35,360
and connect to different drivers and

00:04:34,720 --> 00:04:37,680
jobs

00:04:35,360 --> 00:04:40,080
uh through service discovery and routing

00:04:37,680 --> 00:04:42,240
application identities

00:04:40,080 --> 00:04:43,600
each job that comes up comes with its

00:04:42,240 --> 00:04:46,560
own uh

00:04:43,600 --> 00:04:47,440
cryptographically signed certificates so

00:04:46,560 --> 00:04:49,199
um

00:04:47,440 --> 00:04:51,360
you can be sure when you connect to a

00:04:49,199 --> 00:04:54,400
particular job what connectivity it has

00:04:51,360 --> 00:04:56,160
in between uh deployments and version

00:04:54,400 --> 00:04:59,360
control we support deployments and

00:04:56,160 --> 00:05:01,440
version control to

00:04:59,360 --> 00:05:02,479
uh production is running the dev and

00:05:01,440 --> 00:05:03,680
integration environments and

00:05:02,479 --> 00:05:05,280
productionize it

00:05:03,680 --> 00:05:07,600
in prior environments scheduling and

00:05:05,280 --> 00:05:09,039
grant support

00:05:07,600 --> 00:05:11,919
authentication handling and

00:05:09,039 --> 00:05:13,600
authorization handling users who

00:05:11,919 --> 00:05:15,520
own certain jobs they can only see their

00:05:13,600 --> 00:05:16,479
own jobs so it comes as part of the

00:05:15,520 --> 00:05:18,240
platform here

00:05:16,479 --> 00:05:19,919
audit and efficiency we provide a lot of

00:05:18,240 --> 00:05:20,960
audit logs and also with respect to

00:05:19,919 --> 00:05:22,160
efficiency

00:05:20,960 --> 00:05:23,840
a lot of tools that we give to

00:05:22,160 --> 00:05:25,600
automatically right size your jobs

00:05:23,840 --> 00:05:27,919
basically

00:05:25,600 --> 00:05:28,800
some of the jobs that run uh use or

00:05:27,919 --> 00:05:32,479
petabyte of

00:05:28,800 --> 00:05:35,600
data and and they do shuffle which is in

00:05:32,479 --> 00:05:38,639
500 to 600 terabytes of shuffle for

00:05:35,600 --> 00:05:41,360
each run so efficiency kind of helps

00:05:38,639 --> 00:05:42,160
efficiency tools help them to figure out

00:05:41,360 --> 00:05:44,479
how they can

00:05:42,160 --> 00:05:45,680
tune their spark jobs profile their jobs

00:05:44,479 --> 00:05:48,720
and such

00:05:45,680 --> 00:05:50,639
switching gears to iceberg

00:05:48,720 --> 00:05:52,320
the current status of iceberg we have

00:05:50,639 --> 00:05:53,039
several produce cases that are running

00:05:52,320 --> 00:05:56,240
in fraud

00:05:53,039 --> 00:05:56,720
um uh one of the use cases that is in

00:05:56,240 --> 00:05:58,880
prod

00:05:56,720 --> 00:06:00,560
has about two to three petabytes of data

00:05:58,880 --> 00:06:03,600
per table um

00:06:00,560 --> 00:06:05,280
tens of tables there and over 2.5

00:06:03,600 --> 00:06:07,440
million files

00:06:05,280 --> 00:06:09,520
are more in each of these um each of

00:06:07,440 --> 00:06:10,400
these tables and and most queries are

00:06:09,520 --> 00:06:12,240
returned within

00:06:10,400 --> 00:06:13,680
somewhere about five second response

00:06:12,240 --> 00:06:17,199
time so the

00:06:13,680 --> 00:06:19,440
the the uh the query response times we

00:06:17,199 --> 00:06:22,639
are aiming for is is somewhere in

00:06:19,440 --> 00:06:24,080
few seconds um so this kind of fits the

00:06:22,639 --> 00:06:26,080
slows there

00:06:24,080 --> 00:06:27,440
um some of the primary requirements that

00:06:26,080 --> 00:06:30,000
we have to

00:06:27,440 --> 00:06:31,280
build data lake on top of iceberg

00:06:30,000 --> 00:06:33,120
obviously

00:06:31,280 --> 00:06:34,800
the obvious requirements here are

00:06:33,120 --> 00:06:36,720
lightning fast queries

00:06:34,800 --> 00:06:38,479
uh transactional upsets and deletes

00:06:36,720 --> 00:06:40,639
we're going to be doubling down on this

00:06:38,479 --> 00:06:42,000
later in the talk here asset components

00:06:40,639 --> 00:06:45,120
on any object store

00:06:42,000 --> 00:06:47,360
or distributed file systems um and

00:06:45,120 --> 00:06:49,120
of course automatic compaction and table

00:06:47,360 --> 00:06:51,039
management so we have a separate service

00:06:49,120 --> 00:06:53,919
that kind of automatically keeps

00:06:51,039 --> 00:06:55,360
uh tables compacted manages the table

00:06:53,919 --> 00:06:58,639
data as well as

00:06:55,360 --> 00:07:01,280
metadata to prune it um keep it clean

00:06:58,639 --> 00:07:02,160
um expire snapshots and and things like

00:07:01,280 --> 00:07:03,840
that

00:07:02,160 --> 00:07:05,360
some of the current features that we

00:07:03,840 --> 00:07:07,919
support as part of this

00:07:05,360 --> 00:07:08,800
are our asset compliance for all

00:07:07,919 --> 00:07:10,720
operations

00:07:08,800 --> 00:07:12,000
uh flexible indexing capabilities that

00:07:10,720 --> 00:07:14,000
comes out of the box for

00:07:12,000 --> 00:07:16,080
iceberg here uh support for update

00:07:14,000 --> 00:07:17,759
deletes and absurds support for batch

00:07:16,080 --> 00:07:20,319
and streaming use cases

00:07:17,759 --> 00:07:22,080
data and metadata compaction commands as

00:07:20,319 --> 00:07:24,000
well as automatic compaction here

00:07:22,080 --> 00:07:25,360
support for time travel data prominence

00:07:24,000 --> 00:07:26,479
we have metadata tables that you can

00:07:25,360 --> 00:07:28,240
query

00:07:26,479 --> 00:07:30,479
utilities that can migrate existing

00:07:28,240 --> 00:07:32,960
parking and avro tables without any

00:07:30,479 --> 00:07:33,840
edl jobs basically uh support for scale

00:07:32,960 --> 00:07:35,520
and interactive

00:07:33,840 --> 00:07:36,880
queries using notebooks is is quite

00:07:35,520 --> 00:07:39,680
important as well

00:07:36,880 --> 00:07:40,240
so um so the data lake platform that we

00:07:39,680 --> 00:07:42,240
have built

00:07:40,240 --> 00:07:43,440
currently supports all these features as

00:07:42,240 --> 00:07:46,479
you currently know

00:07:43,440 --> 00:07:48,400
uh iceberg is in

00:07:46,479 --> 00:07:50,639
is integrating in better ways with spark

00:07:48,400 --> 00:07:53,919
as part 3.0 comes out and

00:07:50,639 --> 00:07:54,720
evolves itself out with spark 2.x the

00:07:53,919 --> 00:07:56,400
integration is

00:07:54,720 --> 00:07:58,080
is not so great some of these commands

00:07:56,400 --> 00:07:59,599
are not supported in open source yet so

00:07:58,080 --> 00:08:03,680
we have support for them

00:07:59,599 --> 00:08:05,759
internally as well um

00:08:03,680 --> 00:08:07,680
absurd and delete requirements some of

00:08:05,759 --> 00:08:11,280
the functional requirements here

00:08:07,680 --> 00:08:13,120
are to support read and write heavy olap

00:08:11,280 --> 00:08:14,879
use cases

00:08:13,120 --> 00:08:17,199
one of the primary focus that we have

00:08:14,879 --> 00:08:19,360
been having here at apple is to make

00:08:17,199 --> 00:08:22,560
sure that we support tables

00:08:19,360 --> 00:08:26,800
which are multi-petabytes in size and

00:08:22,560 --> 00:08:30,720
and and updates and

00:08:26,800 --> 00:08:33,279
deletes that happen affect multiple

00:08:30,720 --> 00:08:34,560
um partitions a large number of

00:08:33,279 --> 00:08:37,039
partitions and

00:08:34,560 --> 00:08:38,880
a large subset of data so that has been

00:08:37,039 --> 00:08:42,159
the primary focus basically so

00:08:38,880 --> 00:08:42,800
so we have to be supporting um updates

00:08:42,159 --> 00:08:45,519
that can

00:08:42,800 --> 00:08:47,600
update a large portion of the tables

00:08:45,519 --> 00:08:48,320
full support for update delete and merge

00:08:47,600 --> 00:08:50,080
commands

00:08:48,320 --> 00:08:52,240
one of the things here is we want our

00:08:50,080 --> 00:08:54,240
users to be able to use the same

00:08:52,240 --> 00:08:56,080
sql commands they are familiar with so

00:08:54,240 --> 00:08:57,279
we have added full update delete and

00:08:56,080 --> 00:08:58,959
merge support this is going to be

00:08:57,279 --> 00:09:00,720
available in spark 3.0 as

00:08:58,959 --> 00:09:02,399
it comes out as well anton is going to

00:09:00,720 --> 00:09:04,959
be talking more about it

00:09:02,399 --> 00:09:06,160
um efficient non-conflicting concurrent

00:09:04,959 --> 00:09:09,040
operations

00:09:06,160 --> 00:09:11,040
we have tons of use cases where a data

00:09:09,040 --> 00:09:13,360
flows in from multiple regions

00:09:11,040 --> 00:09:15,440
where you would be updating same table

00:09:13,360 --> 00:09:18,320
with from multiple streams

00:09:15,440 --> 00:09:19,440
so um one of the it's possible that the

00:09:18,320 --> 00:09:22,640
updates that come in

00:09:19,440 --> 00:09:26,480
can be updating two different parts of

00:09:22,640 --> 00:09:29,200
uh the tables so uh having non-cliff

00:09:26,480 --> 00:09:29,519
non-conflicting concurrent operations um

00:09:29,200 --> 00:09:31,839
is

00:09:29,519 --> 00:09:33,680
is very very important configurable

00:09:31,839 --> 00:09:35,279
isolation levels iceberg already

00:09:33,680 --> 00:09:37,600
provides

00:09:35,279 --> 00:09:39,760
a really good isolation level in the

00:09:37,600 --> 00:09:41,760
form of all operations of serializable

00:09:39,760 --> 00:09:44,320
in iceberg so that's the best form of

00:09:41,760 --> 00:09:46,959
isolation that any database can provide

00:09:44,320 --> 00:09:48,800
however when when you have large amount

00:09:46,959 --> 00:09:50,480
of writes and reads happening you can

00:09:48,800 --> 00:09:53,600
also switch to

00:09:50,480 --> 00:09:56,640
a snapshot isolation and that provides

00:09:53,600 --> 00:09:59,920
better write performance

00:09:56,640 --> 00:10:02,320
while happening

00:09:59,920 --> 00:10:04,079
ability to stream out row level changes

00:10:02,320 --> 00:10:06,160
incrementally is also quite important

00:10:04,079 --> 00:10:07,120
feature that we've been looking for so

00:10:06,160 --> 00:10:10,079
that if

00:10:07,120 --> 00:10:12,000
you you can basically replay the um

00:10:10,079 --> 00:10:13,120
events that has happened on one table

00:10:12,000 --> 00:10:14,880
onto another table of

00:10:13,120 --> 00:10:16,399
iceberg basically some of the

00:10:14,880 --> 00:10:19,600
non-functional requirements

00:10:16,399 --> 00:10:22,160
are the the architecture

00:10:19,600 --> 00:10:24,560
one of the main goals is to have us have

00:10:22,160 --> 00:10:25,920
a simple and scalable architecture

00:10:24,560 --> 00:10:28,079
the first thing that comes to mind when

00:10:25,920 --> 00:10:30,640
you think about absurds and

00:10:28,079 --> 00:10:32,480
acid acid compliance and upsets is that

00:10:30,640 --> 00:10:34,880
why not build a system

00:10:32,480 --> 00:10:36,880
that uh that can use log structured

00:10:34,880 --> 00:10:38,640
merge trees or something similar to what

00:10:36,880 --> 00:10:41,839
cassandra hbase and

00:10:38,640 --> 00:10:44,560
other columnar databases currently uh

00:10:41,839 --> 00:10:46,320
provide and and it often comes at a cost

00:10:44,560 --> 00:10:49,120
basically the architecture becomes

00:10:46,320 --> 00:10:49,920
somewhat complicated it can only um it

00:10:49,120 --> 00:10:53,040
can only

00:10:49,920 --> 00:10:53,680
serve some uh amount of data the tables

00:10:53,040 --> 00:10:56,000
cannot be

00:10:53,680 --> 00:10:57,839
in petabytes it can only be in in

00:10:56,000 --> 00:10:59,040
hundreds of terabytes if if we go with

00:10:57,839 --> 00:11:00,640
that architecture

00:10:59,040 --> 00:11:02,079
it also comes with the cost that you

00:11:00,640 --> 00:11:03,760
have to manage different set of

00:11:02,079 --> 00:11:06,320
processes the processes that

00:11:03,760 --> 00:11:07,600
are uh that may manage quorum that has

00:11:06,320 --> 00:11:09,120
caching and everything

00:11:07,600 --> 00:11:11,279
so the prime requirement for building

00:11:09,120 --> 00:11:13,200
the data lake for us is to have a simple

00:11:11,279 --> 00:11:14,560
and scalable architecture and iceberg

00:11:13,200 --> 00:11:16,800
provides that

00:11:14,560 --> 00:11:18,640
um and and and the next requirement here

00:11:16,800 --> 00:11:18,959
is no change for file power file format

00:11:18,640 --> 00:11:21,920
so

00:11:18,959 --> 00:11:23,040
we apple already has exabytes of data

00:11:21,920 --> 00:11:25,200
stored in our data

00:11:23,040 --> 00:11:26,399
uh in our data where in our data lakes

00:11:25,200 --> 00:11:29,279
and we don't want to be

00:11:26,399 --> 00:11:30,800
um replaying the entire data set into a

00:11:29,279 --> 00:11:33,279
new format basically so

00:11:30,800 --> 00:11:34,160
if you're using a parquet and avro we

00:11:33,279 --> 00:11:36,399
want to be able to

00:11:34,160 --> 00:11:37,839
continue to use same file formats

00:11:36,399 --> 00:11:41,360
actually i think the open

00:11:37,839 --> 00:11:42,079
um open data architecture and open file

00:11:41,360 --> 00:11:45,920
formats are

00:11:42,079 --> 00:11:47,600
quite important um absurds and deletes

00:11:45,920 --> 00:11:49,440
in iceberg so

00:11:47,600 --> 00:11:51,680
that's what that was all about how we

00:11:49,440 --> 00:11:55,760
have been looking at

00:11:51,680 --> 00:11:56,399
spark and iceberg within within apple

00:11:55,760 --> 00:11:58,079
and

00:11:56,399 --> 00:11:59,839
from this point on i let anton talk a

00:11:58,079 --> 00:12:00,959
little bit about upsets and deletes uh

00:11:59,839 --> 00:12:03,040
implementation of

00:12:00,959 --> 00:12:04,800
how it's implemented in iceberg and what

00:12:03,040 --> 00:12:06,959
our plans are for future

00:12:04,800 --> 00:12:10,880
thank you so much from um let me share

00:12:06,959 --> 00:12:12,720
my screen

00:12:10,880 --> 00:12:15,040
are you guys able are you guys able to

00:12:12,720 --> 00:12:15,040
see it

00:12:15,839 --> 00:12:21,600
yes all right cool

00:12:19,040 --> 00:12:23,120
um so before i actually dive into the

00:12:21,600 --> 00:12:26,800
actual implementation i want to

00:12:23,120 --> 00:12:29,680
also walk through the sql

00:12:26,800 --> 00:12:31,680
syntax for update deleting mergent

00:12:29,680 --> 00:12:34,480
because i will refer to that syntax

00:12:31,680 --> 00:12:36,160
later in my presentation so delete

00:12:34,480 --> 00:12:39,200
statement is fairly simple we

00:12:36,160 --> 00:12:40,320
are familiar with it from the relational

00:12:39,200 --> 00:12:42,240
world where you have

00:12:40,320 --> 00:12:44,000
specified which tables you want to

00:12:42,240 --> 00:12:44,959
modify and then you provide the

00:12:44,000 --> 00:12:48,639
condition which

00:12:44,959 --> 00:12:51,600
may or may not contain subquery

00:12:48,639 --> 00:12:52,800
delete or update is kind of similar so

00:12:51,600 --> 00:12:54,480
you also provide

00:12:52,800 --> 00:12:56,639
a very condition which may or may not

00:12:54,480 --> 00:12:58,800
contain subquery and in addition to that

00:12:56,639 --> 00:13:01,519
you provide a set of assignments

00:12:58,800 --> 00:13:04,480
that define how you would like to modify

00:13:01,519 --> 00:13:04,480
the existing table

00:13:05,600 --> 00:13:10,480
mergent is a bit more complicated um

00:13:08,480 --> 00:13:11,519
there are different variations to start

00:13:10,480 --> 00:13:13,200
with

00:13:11,519 --> 00:13:15,279
i'll use the one that's available in

00:13:13,200 --> 00:13:17,680
spark which i think is was also copied

00:13:15,279 --> 00:13:20,000
from hype

00:13:17,680 --> 00:13:21,680
here you first start by modifying by

00:13:20,000 --> 00:13:22,880
specifying which table you would like to

00:13:21,680 --> 00:13:24,480
modify

00:13:22,880 --> 00:13:26,160
then you have to provide the source

00:13:24,480 --> 00:13:27,120
relation that actually contains your

00:13:26,160 --> 00:13:29,279
changes

00:13:27,120 --> 00:13:30,880
this can be a reference to temporary

00:13:29,279 --> 00:13:35,760
view or temporary table

00:13:30,880 --> 00:13:37,200
or any arbitrary subquery you may want

00:13:35,760 --> 00:13:40,000
next you have to provide the merge

00:13:37,200 --> 00:13:42,079
condition and in merge condition defines

00:13:40,000 --> 00:13:46,959
but the rows in the source relation

00:13:42,079 --> 00:13:46,959
actually match row's target relation

00:13:47,120 --> 00:13:51,680
alongside that you may specify an

00:13:49,199 --> 00:13:54,160
optional predator on the target column

00:13:51,680 --> 00:13:55,040
um which will be used to pre-filter the

00:13:54,160 --> 00:13:57,600
target table

00:13:55,040 --> 00:13:59,120
before executing the statement and this

00:13:57,600 --> 00:14:00,079
is really important for the overall

00:13:59,120 --> 00:14:02,959
performance of this

00:14:00,079 --> 00:14:06,240
operation and i'll come back to this

00:14:02,959 --> 00:14:08,320
later in my presentation

00:14:06,240 --> 00:14:09,519
if you have a match between a record in

00:14:08,320 --> 00:14:12,320
your source relation

00:14:09,519 --> 00:14:12,720
and an existing record in your table you

00:14:12,320 --> 00:14:15,120
have

00:14:12,720 --> 00:14:16,480
three options you can update the

00:14:15,120 --> 00:14:19,440
existing record

00:14:16,480 --> 00:14:21,680
you can delete it or you can keep it as

00:14:19,440 --> 00:14:21,680
if

00:14:22,160 --> 00:14:27,600
if there is no match between

00:14:25,279 --> 00:14:29,360
the source row and the target row then

00:14:27,600 --> 00:14:30,160
you can decide to insert the row that

00:14:29,360 --> 00:14:32,240
you didn't

00:14:30,160 --> 00:14:33,519
find the match for or you can also

00:14:32,240 --> 00:14:35,040
ignore it

00:14:33,519 --> 00:14:37,760
keep in mind that not all of the

00:14:35,040 --> 00:14:39,440
branches are required at the same time

00:14:37,760 --> 00:14:41,360
so you need at least one in order to

00:14:39,440 --> 00:14:42,560
successfully execute emerging to

00:14:41,360 --> 00:14:44,880
statement

00:14:42,560 --> 00:14:46,240
so to summarize merge into is a very

00:14:44,880 --> 00:14:48,880
flexible way to

00:14:46,240 --> 00:14:49,680
perform inserts updates and deletes in

00:14:48,880 --> 00:14:52,079
the same

00:14:49,680 --> 00:14:52,079
command

00:14:53,120 --> 00:14:56,240
now if you take a look how different

00:14:54,880 --> 00:14:58,399
systems

00:14:56,240 --> 00:15:00,000
handle updates and deletes internally

00:14:58,399 --> 00:15:02,399
you will be able to split their

00:15:00,000 --> 00:15:05,519
approaches into two main groups

00:15:02,399 --> 00:15:06,399
copy and write and merchant read in copy

00:15:05,519 --> 00:15:09,760
and write you all

00:15:06,399 --> 00:15:12,079
you do all of the work um on right

00:15:09,760 --> 00:15:13,519
and in merging read you do as less work

00:15:12,079 --> 00:15:15,519
as possible on the right

00:15:13,519 --> 00:15:17,680
and then you apply the differences while

00:15:15,519 --> 00:15:19,519
reading the data set

00:15:17,680 --> 00:15:21,440
each of these approaches has its own

00:15:19,519 --> 00:15:24,079
benefits and drawbacks and they really

00:15:21,440 --> 00:15:26,560
work well in specific use cases

00:15:24,079 --> 00:15:27,839
as we wanted to have a generic solution

00:15:26,560 --> 00:15:30,079
inside iceberg

00:15:27,839 --> 00:15:32,000
the community has decided to support

00:15:30,079 --> 00:15:33,759
both implementations

00:15:32,000 --> 00:15:35,920
and for merging weed we've decided to

00:15:33,759 --> 00:15:37,920
use differential files

00:15:35,920 --> 00:15:39,600
it's a concept that was introduced by

00:15:37,920 --> 00:15:41,839
the research community around

00:15:39,600 --> 00:15:43,440
40 years ago but is still widely widely

00:15:41,839 --> 00:15:45,920
adopted

00:15:43,440 --> 00:15:46,720
it allows you to avoid materializing the

00:15:45,920 --> 00:15:49,920
updates

00:15:46,720 --> 00:15:50,720
on write but also it does not require

00:15:49,920 --> 00:15:53,279
you to run

00:15:50,720 --> 00:15:54,639
a set of standalone processes somewhere

00:15:53,279 --> 00:15:56,480
in your cluster

00:15:54,639 --> 00:15:58,639
and coordinate make sure they are full

00:15:56,480 --> 00:16:01,040
tolerant and so on

00:15:58,639 --> 00:16:02,959
so it kind of contributes to the overall

00:16:01,040 --> 00:16:06,399
simplicity in the architecture

00:16:02,959 --> 00:16:08,399
while actually achieving the same goal

00:16:06,399 --> 00:16:10,079
and this was again really important for

00:16:08,399 --> 00:16:13,120
us as we didn't want to build

00:16:10,079 --> 00:16:15,759
another another database and

00:16:13,120 --> 00:16:17,680
to give even more flexibility to our

00:16:15,759 --> 00:16:18,399
users we actually support different

00:16:17,680 --> 00:16:20,480
types

00:16:18,399 --> 00:16:22,160
of uh differential files which i will

00:16:20,480 --> 00:16:25,120
describe later in the presentation

00:16:22,160 --> 00:16:25,920
as well so a quick summary of this slide

00:16:25,120 --> 00:16:28,639
is that

00:16:25,920 --> 00:16:30,560
iceberg will support three ways to

00:16:28,639 --> 00:16:32,399
update and delete your data

00:16:30,560 --> 00:16:34,160
and that should cover all reasonable

00:16:32,399 --> 00:16:36,240
analytical use cases

00:16:34,160 --> 00:16:38,560
as wishful mentioned so we want to cover

00:16:36,240 --> 00:16:39,920
both read and write heavy use cases in

00:16:38,560 --> 00:16:43,519
the analytical

00:16:39,920 --> 00:16:44,560
world the first approach i want to talk

00:16:43,519 --> 00:16:47,680
about in detail

00:16:44,560 --> 00:16:50,160
is copy and write

00:16:47,680 --> 00:16:51,920
so the main idea here is that if you

00:16:50,160 --> 00:16:54,240
have to modify

00:16:51,920 --> 00:16:56,959
if you have to delete a record from a

00:16:54,240 --> 00:16:59,279
file you have to copy that file

00:16:56,959 --> 00:17:01,600
delete the record produce a new data

00:16:59,279 --> 00:17:06,079
file and then atomically replace

00:17:01,600 --> 00:17:08,799
the old file with the with the new one

00:17:06,079 --> 00:17:10,959
it's a fairly simple concept but yet uh

00:17:08,799 --> 00:17:12,319
combined with all other logic in iceberg

00:17:10,959 --> 00:17:14,240
you can use it to build

00:17:12,319 --> 00:17:17,039
a relatively efficient merge into

00:17:14,240 --> 00:17:17,039
implementation

00:17:17,120 --> 00:17:22,319
the first step during this under this

00:17:19,360 --> 00:17:25,280
model would be to

00:17:22,319 --> 00:17:27,600
do partition and file pruning whenever a

00:17:25,280 --> 00:17:30,000
user executes a merging to statement

00:17:27,600 --> 00:17:31,919
in a query engine like spark icebrick

00:17:30,000 --> 00:17:32,720
will look for predicates on the target

00:17:31,919 --> 00:17:35,760
column

00:17:32,720 --> 00:17:38,400
and it will use those predicates to uh

00:17:35,760 --> 00:17:38,880
to actually prune partitions and even

00:17:38,400 --> 00:17:40,960
more

00:17:38,880 --> 00:17:43,039
prune files within those partitions

00:17:40,960 --> 00:17:45,840
using the minmax statistics that is

00:17:43,039 --> 00:17:47,840
that it has in its metadata and i would

00:17:45,840 --> 00:17:49,600
like to emphasize that this filtering

00:17:47,840 --> 00:17:52,400
happens without actually touching

00:17:49,600 --> 00:17:54,320
data so it gives us a cheap way to

00:17:52,400 --> 00:17:57,360
pre-filter the target table

00:17:54,320 --> 00:17:59,760
and find out which files may have

00:17:57,360 --> 00:18:01,760
matches

00:17:59,760 --> 00:18:04,080
so therefore i highly advise you to

00:18:01,760 --> 00:18:05,919
structure your table

00:18:04,080 --> 00:18:08,160
in a way that you will be able to

00:18:05,919 --> 00:18:11,440
benefit from partitioning file

00:18:08,160 --> 00:18:11,440
file skipping in iceberg

00:18:12,640 --> 00:18:17,600
once you have this information the next

00:18:14,880 --> 00:18:19,760
step is to perform the cardinality check

00:18:17,600 --> 00:18:21,039
and the sql standard requires an

00:18:19,760 --> 00:18:23,919
exception to be thrown

00:18:21,039 --> 00:18:26,480
if multiple rows in the source relation

00:18:23,919 --> 00:18:28,400
match the same row in the target

00:18:26,480 --> 00:18:30,400
this happens the result of your command

00:18:28,400 --> 00:18:34,160
is actually indeterministic

00:18:30,400 --> 00:18:36,160
um so you cannot really like

00:18:34,160 --> 00:18:37,440
decide what's the correct way to execute

00:18:36,160 --> 00:18:39,919
it and therefore

00:18:37,440 --> 00:18:41,919
um while it's still expensive because

00:18:39,919 --> 00:18:44,000
you have to do an extra inner joint it's

00:18:41,919 --> 00:18:46,000
always a good idea to perform this

00:18:44,000 --> 00:18:48,880
because otherwise you risk to actually

00:18:46,000 --> 00:18:48,880
corrupt your data

00:18:49,600 --> 00:18:53,919
the good part about this that while

00:18:52,160 --> 00:18:54,799
doing the cardinality check we can

00:18:53,919 --> 00:18:58,000
actually collect

00:18:54,799 --> 00:19:00,080
file names that contain matches and this

00:18:58,000 --> 00:19:01,120
can help us to limit the scope of this

00:19:00,080 --> 00:19:04,640
operation

00:19:01,120 --> 00:19:05,039
even further so in step one we started

00:19:04,640 --> 00:19:07,679
with

00:19:05,039 --> 00:19:09,520
files that may have matches which we

00:19:07,679 --> 00:19:10,640
kind of determine based on the available

00:19:09,520 --> 00:19:13,039
metadata

00:19:10,640 --> 00:19:14,880
and during step 3 we actually know which

00:19:13,039 --> 00:19:17,840
files must be rewritten

00:19:14,880 --> 00:19:18,880
so after this step every file that we

00:19:17,840 --> 00:19:22,840
will rewrite will

00:19:18,880 --> 00:19:25,840
actually have a record that must be

00:19:22,840 --> 00:19:25,840
updated

00:19:25,919 --> 00:19:29,840
so once we have this information uh we

00:19:28,480 --> 00:19:32,960
need to perform

00:19:29,840 --> 00:19:35,679
an outer join in the worst case this

00:19:32,960 --> 00:19:37,200
will be a full outer join

00:19:35,679 --> 00:19:39,200
in certain cases depending how you

00:19:37,200 --> 00:19:40,960
structure your branches this can be a

00:19:39,200 --> 00:19:43,600
right outer join as well

00:19:40,960 --> 00:19:45,600
but in generic case it's a full outer

00:19:43,600 --> 00:19:49,360
join because you have to copy

00:19:45,600 --> 00:19:51,440
over the records that didn't match um

00:19:49,360 --> 00:19:53,280
and once you have the joint relation you

00:19:51,440 --> 00:19:55,120
have actually have to materialize

00:19:53,280 --> 00:19:57,039
different branches you have you have to

00:19:55,120 --> 00:19:58,080
update the records you have to insert

00:19:57,039 --> 00:20:00,400
new records

00:19:58,080 --> 00:20:01,280
and you have to delete some of them as

00:20:00,400 --> 00:20:04,640
as

00:20:01,280 --> 00:20:06,880
as per user configuration once you have

00:20:04,640 --> 00:20:09,600
the new state you have to write it into

00:20:06,880 --> 00:20:12,080
a set of new data files

00:20:09,600 --> 00:20:14,480
all of this is done by clear engine like

00:20:12,080 --> 00:20:14,480
spark

00:20:14,640 --> 00:20:17,919
and in the final step you actually have

00:20:16,559 --> 00:20:20,080
to atomically replace

00:20:17,919 --> 00:20:21,440
the files that you read was the files

00:20:20,080 --> 00:20:24,960
that you produced

00:20:21,440 --> 00:20:26,960
that contain updated records and during

00:20:24,960 --> 00:20:27,679
this step iceberg will verify what

00:20:26,960 --> 00:20:30,000
happened

00:20:27,679 --> 00:20:31,120
since the moment you read till the time

00:20:30,000 --> 00:20:32,640
you commit

00:20:31,120 --> 00:20:36,159
and it will act according to the

00:20:32,640 --> 00:20:36,159
configured isolation level

00:20:38,000 --> 00:20:41,679
um the main benefit of this approach is

00:20:40,960 --> 00:20:44,799
it's

00:20:41,679 --> 00:20:46,400
easy to implement there is no extra work

00:20:44,799 --> 00:20:48,320
and read so your reads are as

00:20:46,400 --> 00:20:49,440
efficient as they were before updates

00:20:48,320 --> 00:20:51,120
and deletes

00:20:49,440 --> 00:20:53,679
and it works really well for bulk

00:20:51,120 --> 00:20:56,480
updates if you have

00:20:53,679 --> 00:20:57,120
use cases where you update 80 of the

00:20:56,480 --> 00:20:59,280
file

00:20:57,120 --> 00:21:00,720
um then it's not a big deal to actually

00:20:59,280 --> 00:21:04,799
copy over 20

00:21:00,720 --> 00:21:07,200
extra and write it in one go because

00:21:04,799 --> 00:21:08,040
after that you basically have the same

00:21:07,200 --> 00:21:09,200
efficient

00:21:08,040 --> 00:21:12,640
[Music]

00:21:09,200 --> 00:21:14,080
representation of your data the worst

00:21:12,640 --> 00:21:17,280
case for this approach is

00:21:14,080 --> 00:21:19,919
if you have to modify um a couple of

00:21:17,280 --> 00:21:22,640
records in a large number of files

00:21:19,919 --> 00:21:25,280
in this scenario you actually copy over

00:21:22,640 --> 00:21:27,679
a lot of files that you didn't want

00:21:25,280 --> 00:21:29,360
or did you you didn't intend to modify

00:21:27,679 --> 00:21:32,480
in the first place right

00:21:29,360 --> 00:21:34,080
and this makes it too expensive for

00:21:32,480 --> 00:21:39,679
right heavy use cases

00:21:34,080 --> 00:21:39,679
because rights are taking too much time

00:21:40,799 --> 00:21:44,799
so i have a couple of tips how to make

00:21:42,720 --> 00:21:47,520
this implementation scalable

00:21:44,799 --> 00:21:49,760
um first of all i definitely advise to

00:21:47,520 --> 00:21:50,880
try to cache the relation that contains

00:21:49,760 --> 00:21:54,000
your changes

00:21:50,880 --> 00:21:55,760
that you would like to merge um it's a

00:21:54,000 --> 00:21:57,919
good idea for a number of reasons so

00:21:55,760 --> 00:22:00,159
first of all it will ensure that you get

00:21:57,919 --> 00:22:02,240
the same consistent result every time

00:22:00,159 --> 00:22:04,480
you query the source relation

00:22:02,240 --> 00:22:06,640
and it's actually required by the

00:22:04,480 --> 00:22:08,720
execution because the source relation

00:22:06,640 --> 00:22:10,159
is scanned multiple times so it must

00:22:08,720 --> 00:22:13,280
produce the same result

00:22:10,159 --> 00:22:16,080
so caching is one way to achieve that um

00:22:13,280 --> 00:22:17,760
apart from that uh because the source

00:22:16,080 --> 00:22:19,600
relation is kind of joined multiple

00:22:17,760 --> 00:22:20,960
times with the target table during the

00:22:19,600 --> 00:22:23,200
execution

00:22:20,960 --> 00:22:25,200
if you can pre-cache it then it also

00:22:23,200 --> 00:22:26,880
gives you a better performance

00:22:25,200 --> 00:22:28,480
because you don't want to re-evaluate

00:22:26,880 --> 00:22:30,880
some of the predicates on

00:22:28,480 --> 00:22:31,520
on that sub-query you just want to use

00:22:30,880 --> 00:22:34,720
the cache

00:22:31,520 --> 00:22:36,559
variant of it

00:22:34,720 --> 00:22:38,880
apart from that i definitely advise to

00:22:36,559 --> 00:22:42,159
think about what partition and sort the

00:22:38,880 --> 00:22:42,159
predicates you may have

00:22:42,480 --> 00:22:46,720
in many many cases you're actually not

00:22:45,039 --> 00:22:51,600
targeting

00:22:46,720 --> 00:22:51,600
updates across all of the table

00:22:51,919 --> 00:22:55,440
in many cases what i've seen in practice

00:22:54,400 --> 00:22:58,400
is that you're

00:22:55,440 --> 00:22:59,919
are trying to update some recent part of

00:22:58,400 --> 00:23:03,280
the table let's say

00:22:59,919 --> 00:23:05,520
five ten months last months of data

00:23:03,280 --> 00:23:06,480
and if you have data for let's say five

00:23:05,520 --> 00:23:08,480
years

00:23:06,480 --> 00:23:10,159
by thinking about correct partition and

00:23:08,480 --> 00:23:13,280
sort key predicates you can

00:23:10,159 --> 00:23:15,200
actually um uh

00:23:13,280 --> 00:23:19,200
limit the scope of that operation and

00:23:15,200 --> 00:23:22,720
discard a lot of data without actually

00:23:19,200 --> 00:23:24,480
touching it also i would definitely

00:23:22,720 --> 00:23:25,840
advise to look into the broadcast

00:23:24,480 --> 00:23:29,679
thresholds you have

00:23:25,840 --> 00:23:32,559
um for your uh query engines

00:23:29,679 --> 00:23:34,080
if you can leverage broadcast joins uh

00:23:32,559 --> 00:23:37,200
this will make your life much

00:23:34,080 --> 00:23:40,080
much easier and last point

00:23:37,200 --> 00:23:40,880
is try to explore the metadata tables in

00:23:40,080 --> 00:23:44,240
iceberg

00:23:40,880 --> 00:23:46,080
they contain a ton of useful information

00:23:44,240 --> 00:23:48,559
and especially the snapshots table in

00:23:46,080 --> 00:23:51,360
this case it will tell you

00:23:48,559 --> 00:23:53,360
what operation happened um how many

00:23:51,360 --> 00:23:55,120
records you modified how many records

00:23:53,360 --> 00:23:56,480
you added what's the total number of

00:23:55,120 --> 00:23:58,320
factors right now

00:23:56,480 --> 00:24:00,640
and this will be a good way to kind of

00:23:58,320 --> 00:24:05,600
understand what happened in the

00:24:00,640 --> 00:24:08,559
merge operation

00:24:05,600 --> 00:24:10,000
all right that was copy and write in the

00:24:08,559 --> 00:24:12,640
next section i'm going to cover

00:24:10,000 --> 00:24:12,640
marginally

00:24:17,279 --> 00:24:21,200
and the main concept we had to introduce

00:24:19,360 --> 00:24:22,559
to support margin reading iceberg is a

00:24:21,200 --> 00:24:25,200
delete file

00:24:22,559 --> 00:24:25,760
so you delete a delete file tells us

00:24:25,200 --> 00:24:28,400
which

00:24:25,760 --> 00:24:29,360
records were removed from our data set

00:24:28,400 --> 00:24:31,200
and instead of

00:24:29,360 --> 00:24:33,120
copying and modifying data files

00:24:31,200 --> 00:24:35,600
directly you can now

00:24:33,120 --> 00:24:38,159
mark deleted records by writing and

00:24:35,600 --> 00:24:40,159
producing your delete files

00:24:38,159 --> 00:24:43,279
those delete files they will be applied

00:24:40,159 --> 00:24:46,640
while you're reading the data set

00:24:43,279 --> 00:24:49,039
and producing a delete file is um

00:24:46,640 --> 00:24:51,520
substantially more efficient than

00:24:49,039 --> 00:24:53,600
modifying the data file directly

00:24:51,520 --> 00:24:55,600
so under this model an update is

00:24:53,600 --> 00:24:58,159
represented by a pandembos

00:24:55,600 --> 00:24:59,360
delete file which kind of removes old

00:24:58,159 --> 00:25:01,440
version of the row

00:24:59,360 --> 00:25:03,760
and data files which contains the

00:25:01,440 --> 00:25:05,919
updated record

00:25:03,760 --> 00:25:08,159
and at selected points in time you

00:25:05,919 --> 00:25:10,159
actually have to perform compactions

00:25:08,159 --> 00:25:12,480
and a compaction may be a minor

00:25:10,159 --> 00:25:14,799
compaction or a major compaction

00:25:12,480 --> 00:25:15,520
the goal of a minor compaction is to

00:25:14,799 --> 00:25:17,600
optimize

00:25:15,520 --> 00:25:20,080
the number and layout of small delete

00:25:17,600 --> 00:25:21,039
files and the goal of a major compaction

00:25:20,080 --> 00:25:23,520
is to actually

00:25:21,039 --> 00:25:24,559
get rid of delete files completely by

00:25:23,520 --> 00:25:29,840
merging them

00:25:24,559 --> 00:25:29,840
into the corresponding base files

00:25:30,320 --> 00:25:34,960
your debate files must somehow reference

00:25:33,039 --> 00:25:36,320
rows in the target table and this can be

00:25:34,960 --> 00:25:38,240
done using two ways

00:25:36,320 --> 00:25:42,159
either using natural keys or using

00:25:38,240 --> 00:25:44,480
syntactic keys

00:25:42,159 --> 00:25:45,360
a natural key is a key that consists of

00:25:44,480 --> 00:25:47,919
one or more

00:25:45,360 --> 00:25:49,760
column values and if you want to

00:25:47,919 --> 00:25:51,200
represent a generic update using a

00:25:49,760 --> 00:25:53,600
natural key

00:25:51,200 --> 00:25:55,520
you face a couple of problems and by

00:25:53,600 --> 00:25:58,240
generic use case i mean

00:25:55,520 --> 00:25:59,200
um an update like like you see in sql

00:25:58,240 --> 00:26:01,520
command

00:25:59,200 --> 00:26:03,679
where you can modify an arbitrary field

00:26:01,520 --> 00:26:06,080
and you may have an arbitrary condition

00:26:03,679 --> 00:26:07,919
so you are not limited to condition on

00:26:06,080 --> 00:26:09,600
on only on the natural key

00:26:07,919 --> 00:26:11,919
so in this case it's it's hard to

00:26:09,600 --> 00:26:12,799
represent that operation using an atrial

00:26:11,919 --> 00:26:16,000
key

00:26:12,799 --> 00:26:19,120
um first of all not all of the datasets

00:26:16,000 --> 00:26:20,559
actually have a unique globally unique

00:26:19,120 --> 00:26:23,840
natural key

00:26:20,559 --> 00:26:24,799
second you have to reason about key

00:26:23,840 --> 00:26:27,360
uniqueness

00:26:24,799 --> 00:26:29,200
uh and it's challenging and expansive

00:26:27,360 --> 00:26:30,960
especially at scale

00:26:29,200 --> 00:26:32,480
if you already do this track on right

00:26:30,960 --> 00:26:34,000
this would mean you would have to join

00:26:32,480 --> 00:26:36,720
the existing dataset

00:26:34,000 --> 00:26:38,720
with the incoming dataset find if there

00:26:36,720 --> 00:26:40,000
are duplicates which is expensive

00:26:38,720 --> 00:26:41,600
because you need to execute that

00:26:40,000 --> 00:26:45,120
distributed join

00:26:41,600 --> 00:26:46,799
um if you are to do this check on read

00:26:45,120 --> 00:26:49,039
you would have to probably sort the

00:26:46,799 --> 00:26:52,159
records not only by the natural keys but

00:26:49,039 --> 00:26:53,760
also by some additional column

00:26:52,159 --> 00:26:56,240
that would represent the version of the

00:26:53,760 --> 00:26:59,279
row and then you would pick the last one

00:26:56,240 --> 00:27:01,039
in both cases it's expensive and it adds

00:26:59,279 --> 00:27:03,440
more complexity to the overall

00:27:01,039 --> 00:27:05,679
implementation

00:27:03,440 --> 00:27:07,200
in addition the size of delete files in

00:27:05,679 --> 00:27:09,600
case of neutral keys

00:27:07,200 --> 00:27:10,480
depends on how many columns you use as

00:27:09,600 --> 00:27:13,679
natural key

00:27:10,480 --> 00:27:15,520
and on their size so the more columns

00:27:13,679 --> 00:27:17,919
you use as a natural key

00:27:15,520 --> 00:27:19,679
the bigger size you have for your delete

00:27:17,919 --> 00:27:22,320
files

00:27:19,679 --> 00:27:24,480
and final it kind of limits the read

00:27:22,320 --> 00:27:26,720
optimizations you may have

00:27:24,480 --> 00:27:27,760
um so the consensus we have arrived in

00:27:26,720 --> 00:27:31,520
the community

00:27:27,760 --> 00:27:33,520
is that supporting natural keys

00:27:31,520 --> 00:27:35,520
for generic use cases is probably not

00:27:33,520 --> 00:27:37,440
the best idea

00:27:35,520 --> 00:27:39,200
but but there are definitely very

00:27:37,440 --> 00:27:41,120
important use cases where you want to

00:27:39,200 --> 00:27:43,919
use them

00:27:41,120 --> 00:27:45,919
over positional keys and you still want

00:27:43,919 --> 00:27:49,840
to cover them so therefore the decision

00:27:45,919 --> 00:27:49,840
was that you will support those

00:27:51,200 --> 00:27:56,399
a synthetic key in case of iceberg is

00:27:54,399 --> 00:27:57,760
actually a combination of a file pass

00:27:56,399 --> 00:28:00,399
and row position

00:27:57,760 --> 00:28:02,559
and such keys ensure uniqueness of rows

00:28:00,399 --> 00:28:05,760
without any extra work on the read

00:28:02,559 --> 00:28:08,159
or write so it's a very efficient way to

00:28:05,760 --> 00:28:10,080
uniquely identify row

00:28:08,159 --> 00:28:11,679
and also they have fixed size of the

00:28:10,080 --> 00:28:13,039
delete files because you always have

00:28:11,679 --> 00:28:15,440
just two columns and they

00:28:13,039 --> 00:28:17,279
kind of always the same size and because

00:28:15,440 --> 00:28:20,480
all of that flexibility

00:28:17,279 --> 00:28:22,799
that that you get um most of the

00:28:20,480 --> 00:28:23,520
systems like if you consider one db

00:28:22,799 --> 00:28:26,559
vertica

00:28:23,520 --> 00:28:29,120
others uh they leverage synthetic keys

00:28:26,559 --> 00:28:31,279
even higher acid tables in v1 and v2

00:28:29,120 --> 00:28:33,520
version they all leverage synthetic keys

00:28:31,279 --> 00:28:35,760
but there is very important distinction

00:28:33,520 --> 00:28:36,159
in synthetic keys in iceberg compared to

00:28:35,760 --> 00:28:39,440
hive

00:28:36,159 --> 00:28:42,559
asset hive assets stores an extra column

00:28:39,440 --> 00:28:43,919
in the data files for iceberg synthetic

00:28:42,559 --> 00:28:45,919
key is implicit

00:28:43,919 --> 00:28:48,880
so there is no extra metadata

00:28:45,919 --> 00:28:50,640
information in the data files

00:28:48,880 --> 00:28:52,080
which is again a very important point

00:28:50,640 --> 00:28:54,080
for us

00:28:52,080 --> 00:28:56,000
the only drawback of your synthetic key

00:28:54,080 --> 00:28:56,720
is that you have to read the target

00:28:56,000 --> 00:29:00,320
table

00:28:56,720 --> 00:29:02,159
in order to produce a positional delete

00:29:00,320 --> 00:29:04,320
which may not be a big deal if you have

00:29:02,159 --> 00:29:05,679
to read the table anyway to execute a

00:29:04,320 --> 00:29:07,679
merger into statement

00:29:05,679 --> 00:29:10,399
and this is actually true in a generic

00:29:07,679 --> 00:29:12,640
case but there are edge cases where you

00:29:10,399 --> 00:29:15,039
can produce your delete files without

00:29:12,640 --> 00:29:16,960
actually reading the target table and

00:29:15,039 --> 00:29:18,799
this is something that you can address

00:29:16,960 --> 00:29:21,440
with natural keys and you cannot do with

00:29:18,799 --> 00:29:23,919
synthetic keys

00:29:21,440 --> 00:29:25,919
so as i said before therefore the

00:29:23,919 --> 00:29:29,200
community has decided to use synthetic

00:29:25,919 --> 00:29:31,679
keys for generic use cases

00:29:29,200 --> 00:29:33,360
and use natural keys for some edge cases

00:29:31,679 --> 00:29:35,200
where you can generate delete files

00:29:33,360 --> 00:29:36,480
without without touching the target

00:29:35,200 --> 00:29:38,159
table

00:29:36,480 --> 00:29:40,320
and to give you a couple of examples

00:29:38,159 --> 00:29:43,039
where what i actually mean by

00:29:40,320 --> 00:29:44,640
generic use case um let's explore this

00:29:43,039 --> 00:29:47,760
one

00:29:44,640 --> 00:29:50,799
so here we have a merge into statement

00:29:47,760 --> 00:29:54,399
with equality on partition column

00:29:50,799 --> 00:29:56,960
and id and if there is a match

00:29:54,399 --> 00:29:57,760
between the source and target table we

00:29:56,960 --> 00:30:00,880
would like to

00:29:57,760 --> 00:30:03,679
check if the value of the existing row

00:30:00,880 --> 00:30:05,520
in in the count field is less than 10

00:30:03,679 --> 00:30:06,320
and then if that's true then we'd like

00:30:05,520 --> 00:30:09,840
to modify

00:30:06,320 --> 00:30:13,679
one single time column in the existing

00:30:09,840 --> 00:30:15,440
record in order to find out the new

00:30:13,679 --> 00:30:18,080
representation of the row

00:30:15,440 --> 00:30:19,039
with all updates you actually need to

00:30:18,080 --> 00:30:21,200
know

00:30:19,039 --> 00:30:22,399
the values for other columns that that

00:30:21,200 --> 00:30:24,399
row has

00:30:22,399 --> 00:30:26,320
the only way to find that out is by

00:30:24,399 --> 00:30:27,840
reading the target table

00:30:26,320 --> 00:30:29,760
in addition you have to apply that

00:30:27,840 --> 00:30:32,080
predicate less than 10

00:30:29,760 --> 00:30:33,440
on the count field which is again only

00:30:32,080 --> 00:30:37,360
possible if you

00:30:33,440 --> 00:30:39,360
read the target table so in this case

00:30:37,360 --> 00:30:41,600
you have to read the target table anyway

00:30:39,360 --> 00:30:42,000
so there is no extra penalty to actually

00:30:41,600 --> 00:30:45,200
read

00:30:42,000 --> 00:30:48,000
the synthetic key while

00:30:45,200 --> 00:30:48,000
reading the data

00:30:48,399 --> 00:30:54,320
now compare this to this example

00:30:51,919 --> 00:30:55,840
we have a quality only on id and we are

00:30:54,320 --> 00:30:58,640
doing this merge operation

00:30:55,840 --> 00:30:59,200
only a specific partition and we are

00:30:58,640 --> 00:31:02,480
saying that

00:30:59,200 --> 00:31:05,200
update all matching records with

00:31:02,480 --> 00:31:06,399
the record i have in my source relation

00:31:05,200 --> 00:31:08,159
so here it doesn't really

00:31:06,399 --> 00:31:10,880
care about the uniqueness of the key you

00:31:08,159 --> 00:31:13,279
can have uh 10 records with the same id

00:31:10,880 --> 00:31:15,120
and you just want to update all of them

00:31:13,279 --> 00:31:17,360
right and you actually have the new

00:31:15,120 --> 00:31:19,519
representation of row you want to

00:31:17,360 --> 00:31:20,399
replace it with it's in your source

00:31:19,519 --> 00:31:22,159
relation

00:31:20,399 --> 00:31:23,519
so in this case what you can do and what

00:31:22,159 --> 00:31:26,159
the iceberg will do

00:31:23,519 --> 00:31:28,240
is it will produce a delete file that

00:31:26,159 --> 00:31:30,799
will contain unique ids

00:31:28,240 --> 00:31:32,320
uh so that it will discard all versions

00:31:30,799 --> 00:31:36,240
of those rows

00:31:32,320 --> 00:31:36,240
and it will produce new data files

00:31:36,320 --> 00:31:40,480
with updated records and all of that

00:31:38,559 --> 00:31:41,200
without actually reading the target

00:31:40,480 --> 00:31:44,840
table

00:31:41,200 --> 00:31:46,320
which makes the write as lightweight as

00:31:44,840 --> 00:31:48,159
possible

00:31:46,320 --> 00:31:50,320
now i would like to talk a bit about how

00:31:48,159 --> 00:31:53,519
that that is actually represented

00:31:50,320 --> 00:31:53,519
in the table format

00:31:54,000 --> 00:31:57,519
as you know iceberg keeps both forward

00:31:55,840 --> 00:31:59,200
and backward compatibility within the

00:31:57,519 --> 00:32:04,480
same table for that version

00:31:59,200 --> 00:32:04,480
which which means that

00:32:04,559 --> 00:32:10,240
he had to introduce the v2 table format

00:32:07,440 --> 00:32:10,240
for merge and read

00:32:10,720 --> 00:32:14,799
but before diving into v2 i'd like to

00:32:12,720 --> 00:32:17,840
actually walk through the v1 format and

00:32:14,799 --> 00:32:20,880
make sure we are on the same table

00:32:17,840 --> 00:32:23,200
and just one side note is the upgrade to

00:32:20,880 --> 00:32:26,000
v2 is a simple configuration change

00:32:23,200 --> 00:32:27,120
you just bump the table format version

00:32:26,000 --> 00:32:30,559
and you're done

00:32:27,120 --> 00:32:32,000
the the migration so the v2 format is

00:32:30,559 --> 00:32:34,080
backward compatible

00:32:32,000 --> 00:32:35,840
but once you migrate you cannot actually

00:32:34,080 --> 00:32:38,480
use your v1 writers

00:32:35,840 --> 00:32:40,880
to write to v2 tables but the opposite

00:32:38,480 --> 00:32:44,640
is kind of true

00:32:40,880 --> 00:32:47,519
so in v1 there is a notion of a snapshot

00:32:44,640 --> 00:32:49,039
in iceberg and a snapshot tells us which

00:32:47,519 --> 00:32:51,760
data files were present

00:32:49,039 --> 00:32:54,080
at a particular point in time and then

00:32:51,760 --> 00:32:55,360
the metadata for different data files in

00:32:54,080 --> 00:32:58,960
a particular snapshot

00:32:55,360 --> 00:33:00,799
is spread across different manifest

00:32:58,960 --> 00:33:02,399
files

00:33:00,799 --> 00:33:04,320
and the manifest file is a really

00:33:02,399 --> 00:33:06,720
important concept in icebreck it

00:33:04,320 --> 00:33:07,440
provides us metadata for a group of data

00:33:06,720 --> 00:33:09,600
files

00:33:07,440 --> 00:33:10,480
and each entry in the manifest file

00:33:09,600 --> 00:33:13,760
actually

00:33:10,480 --> 00:33:14,559
contains the file location the partition

00:33:13,760 --> 00:33:17,519
tuple

00:33:14,559 --> 00:33:19,440
min max statistics file size and other

00:33:17,519 --> 00:33:22,840
useful information that may be used

00:33:19,440 --> 00:33:27,279
during job planning or other operations

00:33:22,840 --> 00:33:30,559
um usually a single manifest covers from

00:33:27,279 --> 00:33:33,279
two to five thousands of data files

00:33:30,559 --> 00:33:35,440
and whenever we write a new version

00:33:33,279 --> 00:33:38,320
whenever we produce a new snapshot

00:33:35,440 --> 00:33:40,320
we will have a new manifest list but the

00:33:38,320 --> 00:33:42,320
important part is that manifest list

00:33:40,320 --> 00:33:44,480
might actually reference some of the old

00:33:42,320 --> 00:33:46,720
manifests

00:33:44,480 --> 00:33:48,080
and actually new ones as well this

00:33:46,720 --> 00:33:50,159
allows us to have

00:33:48,080 --> 00:33:52,559
cheap commits in iceberg and rewrite

00:33:50,159 --> 00:33:54,640
only what changed in terms of metadata

00:33:52,559 --> 00:33:57,760
and inherit most of the table state from

00:33:54,640 --> 00:33:57,760
the previous snapshot

00:33:58,799 --> 00:34:04,080
in order to support positional deletes

00:34:02,320 --> 00:34:05,919
or deletes in general we had to

00:34:04,080 --> 00:34:08,720
introduce a notion of the delete file

00:34:05,919 --> 00:34:08,720
for v2 spec

00:34:08,800 --> 00:34:16,000
and delete file actually contains

00:34:12,800 --> 00:34:18,159
the the rows that that were removed

00:34:16,000 --> 00:34:20,879
we also had to extend the manifest

00:34:18,159 --> 00:34:22,720
metadata with the field called content

00:34:20,879 --> 00:34:24,159
it now tells whether the manifest

00:34:22,720 --> 00:34:27,040
contains either data

00:34:24,159 --> 00:34:29,200
or delete files you cannot have the same

00:34:27,040 --> 00:34:31,119
manifest that provides you both data and

00:34:29,200 --> 00:34:34,240
delete files because of the way how job

00:34:31,119 --> 00:34:36,240
planning works

00:34:34,240 --> 00:34:37,839
we've also extended the manifest entry

00:34:36,240 --> 00:34:39,839
metadata with

00:34:37,839 --> 00:34:41,760
content field as well and it tells us

00:34:39,839 --> 00:34:45,280
whether it's a data file

00:34:41,760 --> 00:34:48,159
positional delete or inequality delete

00:34:45,280 --> 00:34:50,159
and a single manifest can actually

00:34:48,159 --> 00:34:53,280
contain multiple delete files

00:34:50,159 --> 00:34:55,040
so that the same table might have uh

00:34:53,280 --> 00:34:57,040
different delete types as well so you're

00:34:55,040 --> 00:34:59,200
not limited to one approach within a

00:34:57,040 --> 00:35:01,760
table you can actually use all three

00:34:59,200 --> 00:35:02,800
approaches inside and what is also

00:35:01,760 --> 00:35:05,920
important to note

00:35:02,800 --> 00:35:07,040
is that delete files are scoped to

00:35:05,920 --> 00:35:09,200
partitions

00:35:07,040 --> 00:35:11,280
um so that you can print partitions

00:35:09,200 --> 00:35:13,359
while while job planning just as you do

00:35:11,280 --> 00:35:16,320
for data files

00:35:13,359 --> 00:35:18,160
and in addition all records uh in

00:35:16,320 --> 00:35:19,520
positional delete files they are sorted

00:35:18,160 --> 00:35:22,160
by the file name

00:35:19,520 --> 00:35:23,520
and we keep min max statistics for

00:35:22,160 --> 00:35:25,599
delete files

00:35:23,520 --> 00:35:27,920
in the metadata so that we can not only

00:35:25,599 --> 00:35:30,480
print partitions but also filter out

00:35:27,920 --> 00:35:31,119
delete files as we can filter our data

00:35:30,480 --> 00:35:33,599
files

00:35:31,119 --> 00:35:35,119
and this ensures that we kind of filter

00:35:33,599 --> 00:35:36,720
out as much as we can

00:35:35,119 --> 00:35:38,240
during job planning and don't touch

00:35:36,720 --> 00:35:41,359
deletes that do not apply

00:35:38,240 --> 00:35:41,359
to specific query

00:35:42,560 --> 00:35:47,280
in order to support the quality deletes

00:35:44,400 --> 00:35:49,280
we had to introduce the sequence number

00:35:47,280 --> 00:35:50,400
and the sequence number tells us the

00:35:49,280 --> 00:35:53,359
relative age

00:35:50,400 --> 00:35:54,720
of a delete or data file it is necessary

00:35:53,359 --> 00:35:56,560
because once you produce

00:35:54,720 --> 00:35:58,079
those delete files you actually want

00:35:56,560 --> 00:36:01,040
them to apply

00:35:58,079 --> 00:36:02,000
uh to data files that were older than

00:36:01,040 --> 00:36:05,280
this

00:36:02,000 --> 00:36:07,200
and uh the quality deletes are also

00:36:05,280 --> 00:36:10,240
scoped to partitions

00:36:07,200 --> 00:36:12,880
so that you can do partition filtering

00:36:10,240 --> 00:36:15,200
as well

00:36:12,880 --> 00:36:16,800
in case of equality deletes you also

00:36:15,200 --> 00:36:20,320
have a metadata column

00:36:16,800 --> 00:36:22,480
called equality ids it specifies which

00:36:20,320 --> 00:36:23,359
column ids were used as the key for this

00:36:22,480 --> 00:36:26,079
specific

00:36:23,359 --> 00:36:28,000
delete operation it gives you extra

00:36:26,079 --> 00:36:30,880
flexibility so that you are not

00:36:28,000 --> 00:36:32,320
really limited to a specific key to

00:36:30,880 --> 00:36:34,480
perform your deletes

00:36:32,320 --> 00:36:36,240
right you can have different deleted

00:36:34,480 --> 00:36:38,000
operations and you can still encode them

00:36:36,240 --> 00:36:40,480
using equality difference

00:36:38,000 --> 00:36:41,440
and the content of uh equality delete

00:36:40,480 --> 00:36:44,480
files

00:36:41,440 --> 00:36:46,000
actually contains the values for uh for

00:36:44,480 --> 00:36:48,320
the columns that were used as

00:36:46,000 --> 00:36:48,320
keys

00:36:50,000 --> 00:36:54,160
the main benefit of merging read is that

00:36:52,640 --> 00:36:57,359
it makes your rights

00:36:54,160 --> 00:36:59,440
lightweight and it it

00:36:57,359 --> 00:37:00,400
ensures that the amount of data you have

00:36:59,440 --> 00:37:02,320
to write

00:37:00,400 --> 00:37:03,599
is proportional to the number of records

00:37:02,320 --> 00:37:06,240
you modify

00:37:03,599 --> 00:37:08,480
which was not really the case for uh

00:37:06,240 --> 00:37:10,880
copy and write

00:37:08,480 --> 00:37:12,720
the main drawback is that it does affect

00:37:10,880 --> 00:37:15,280
the read performance you have

00:37:12,720 --> 00:37:15,920
um because there will be an extra work

00:37:15,280 --> 00:37:17,599
on

00:37:15,920 --> 00:37:19,440
entering job planning to locate which

00:37:17,599 --> 00:37:22,000
data files you need to read

00:37:19,440 --> 00:37:23,440
plus there is extra work while reading

00:37:22,000 --> 00:37:24,960
the file

00:37:23,440 --> 00:37:26,480
because you have to filter the records

00:37:24,960 --> 00:37:28,320
that were deleted

00:37:26,480 --> 00:37:30,880
also it's harder to implement it

00:37:28,320 --> 00:37:32,720
requires v2 format in iceberg

00:37:30,880 --> 00:37:34,240
and it works well only if the size of

00:37:32,720 --> 00:37:36,400
the delete file is

00:37:34,240 --> 00:37:38,079
small enough if you're kind of deleting

00:37:36,400 --> 00:37:40,880
this approach using this approach

00:37:38,079 --> 00:37:43,200
to delete 80 of the records in the file

00:37:40,880 --> 00:37:45,280
this is not the best use case

00:37:43,200 --> 00:37:46,640
and in addition it requires minor and

00:37:45,280 --> 00:37:48,880
major compaction

00:37:46,640 --> 00:37:51,839
which adds more complexity to the way

00:37:48,880 --> 00:37:51,839
you manage your tables

00:37:52,480 --> 00:37:55,680
now i want to talk a bit about the

00:37:54,320 --> 00:37:57,599
status of this

00:37:55,680 --> 00:37:58,720
in open source and both internally at

00:37:57,599 --> 00:38:01,040
apple

00:37:58,720 --> 00:38:02,880
um we've been running the copy and write

00:38:01,040 --> 00:38:04,240
implementation for more than a year

00:38:02,880 --> 00:38:05,760
internally

00:38:04,240 --> 00:38:07,920
based on the design dock that we've

00:38:05,760 --> 00:38:09,599
created in open source

00:38:07,920 --> 00:38:11,760
the only reason why we didn't do this in

00:38:09,599 --> 00:38:14,880
open source directly is because of the

00:38:11,760 --> 00:38:17,920
data source v2 limitation and spark 2.

00:38:14,880 --> 00:38:20,000
spark 3 has proper data source v2 and

00:38:17,920 --> 00:38:21,359
also has the logical plans for update

00:38:20,000 --> 00:38:23,520
deleted merge

00:38:21,359 --> 00:38:24,880
so we are already working to kind of

00:38:23,520 --> 00:38:26,880
contribute this back to

00:38:24,880 --> 00:38:28,640
open source so you can expect a fully

00:38:26,880 --> 00:38:30,000
available and working solution in

00:38:28,640 --> 00:38:33,440
iceberg directly

00:38:30,000 --> 00:38:35,280
was in one month from now

00:38:33,440 --> 00:38:37,040
merge and read is still kind of in

00:38:35,280 --> 00:38:38,960
progress so if you've completed all of

00:38:37,040 --> 00:38:42,079
the table format changes

00:38:38,960 --> 00:38:43,760
you can create a delete file in

00:38:42,079 --> 00:38:45,119
both equality and those positional

00:38:43,760 --> 00:38:46,960
delete files

00:38:45,119 --> 00:38:48,880
job planning is in place so there will

00:38:46,960 --> 00:38:50,560
be different filtering

00:38:48,880 --> 00:38:52,000
and actually that delete file will be

00:38:50,560 --> 00:38:54,160
applied if you read from

00:38:52,000 --> 00:38:55,040
from spark the missing part that we

00:38:54,160 --> 00:38:56,640
currently have

00:38:55,040 --> 00:38:58,320
is the logic and spark that would

00:38:56,640 --> 00:38:59,599
produce those delete files in the

00:38:58,320 --> 00:39:01,920
distributed

00:38:59,599 --> 00:39:03,839
fashion right the logic that would do

00:39:01,920 --> 00:39:06,000
those uh joins between the source and

00:39:03,839 --> 00:39:08,000
relation determine different conditions

00:39:06,000 --> 00:39:09,040
and what what must be written into a

00:39:08,000 --> 00:39:11,119
delete file

00:39:09,040 --> 00:39:12,839
and also we don't have um the

00:39:11,119 --> 00:39:15,599
implementation combining major

00:39:12,839 --> 00:39:16,400
compaction we have a plan how to do this

00:39:15,599 --> 00:39:19,760
but

00:39:16,400 --> 00:39:22,720
it's not yet implemented so my personal

00:39:19,760 --> 00:39:23,839
um prediction is that there will be a

00:39:22,720 --> 00:39:28,320
working prototype

00:39:23,839 --> 00:39:29,760
and trend by the end of the year

00:39:28,320 --> 00:39:32,000
so i'd like to finish with a few key

00:39:29,760 --> 00:39:33,520
takeaways um

00:39:32,000 --> 00:39:35,200
that i want you to remember from this

00:39:33,520 --> 00:39:37,760
presentation

00:39:35,200 --> 00:39:39,760
first and foremost different use cases

00:39:37,760 --> 00:39:42,400
require different approaches

00:39:39,760 --> 00:39:43,200
um you cannot have a single approach

00:39:42,400 --> 00:39:45,359
that would

00:39:43,200 --> 00:39:47,359
work equally well for different use

00:39:45,359 --> 00:39:49,520
cases and therefore it is important

00:39:47,359 --> 00:39:50,960
for iceberg to support different

00:39:49,520 --> 00:39:53,839
implementations

00:39:50,960 --> 00:39:56,560
um and this gives extra flexibility to

00:39:53,839 --> 00:39:58,160
you to meet the requirements you have

00:39:56,560 --> 00:40:00,320
we would advise to use copy and write

00:39:58,160 --> 00:40:01,520
for bulk updates if you have weak heavy

00:40:00,320 --> 00:40:03,359
use cases

00:40:01,520 --> 00:40:05,760
we would advise to use positional merge

00:40:03,359 --> 00:40:06,720
and read for generic like heavy use

00:40:05,760 --> 00:40:09,599
cases

00:40:06,720 --> 00:40:11,680
and we would advise you to use equality

00:40:09,599 --> 00:40:13,280
merchandise for cases where you can skip

00:40:11,680 --> 00:40:15,680
reading the target table

00:40:13,280 --> 00:40:19,760
to produce your tv files and one of such

00:40:15,680 --> 00:40:19,760
examples is absorbed by primary key

00:40:20,480 --> 00:40:24,400
and definitely check out our opportunity

00:40:23,040 --> 00:40:26,880
so we hire in different

00:40:24,400 --> 00:40:28,880
uh different roles uh we're looking for

00:40:26,880 --> 00:40:30,560
people who are experts in prairie angels

00:40:28,880 --> 00:40:32,560
like spark and presto

00:40:30,560 --> 00:40:34,160
we're looking for experts in file

00:40:32,560 --> 00:40:37,839
formats like or c

00:40:34,160 --> 00:40:40,079
park avro and people who are also

00:40:37,839 --> 00:40:42,079
experts in apache arrow as well

00:40:40,079 --> 00:40:44,240
all that is really needed for our team

00:40:42,079 --> 00:40:46,240
and we also be available in the boost

00:40:44,240 --> 00:40:48,480
after the presentation if you have any

00:40:46,240 --> 00:40:50,640
any questions just feel free to drop

00:40:48,480 --> 00:40:52,160
and as we before said we also have

00:40:50,640 --> 00:40:54,720
another presentation

00:40:52,160 --> 00:40:55,520
where we will talk about icebreaks in

00:40:54,720 --> 00:40:58,079
detail about

00:40:55,520 --> 00:40:59,440
why we chose iceberg and why what are

00:40:58,079 --> 00:41:00,960
the features that are really important

00:40:59,440 --> 00:41:03,520
for us

00:41:00,960 --> 00:41:03,520
thanks a lot

00:41:04,000 --> 00:41:09,440
thanks a lot guys and

00:41:07,520 --> 00:41:12,000
question with for if we if anybody has

00:41:09,440 --> 00:41:12,000
any questions

00:41:17,280 --> 00:41:23,760
there was one question in the chat about

00:41:20,640 --> 00:41:26,560
uh when would we have merge on reits

00:41:23,760 --> 00:41:29,680
available i think as uh anton mentioned

00:41:26,560 --> 00:41:33,599
we are hoping to get the mvp out by

00:41:29,680 --> 00:41:36,880
end of the year as much as possible

00:41:33,599 --> 00:41:38,880
i think spark 3 and

00:41:36,880 --> 00:41:41,040
uh spark please integration so we're

00:41:38,880 --> 00:41:42,960
working on that

00:41:41,040 --> 00:41:45,359
uh yeah there there is even like

00:41:42,960 --> 00:41:47,599
consensus on how that will look like so

00:41:45,359 --> 00:41:48,720
um there is a lot of progress boson

00:41:47,599 --> 00:41:50,480
virgin read

00:41:48,720 --> 00:41:52,640
uh i'm copying right i mean it's done a

00:41:50,480 --> 00:41:54,960
year ago so we just were waiting for the

00:41:52,640 --> 00:41:57,920
deserts uh v2 to be out

00:41:54,960 --> 00:41:59,680
um and i think the design for mercenary

00:41:57,920 --> 00:42:00,880
kind of stabilizes during the last

00:41:59,680 --> 00:42:03,359
months

00:42:00,880 --> 00:42:04,640
and brian blue it did a lot of work to

00:42:03,359 --> 00:42:07,200
actually implement this

00:42:04,640 --> 00:42:08,079
from the table perspective and i will

00:42:07,200 --> 00:42:10,560
add

00:42:08,079 --> 00:42:12,240
the spark related parts and it actually

00:42:10,560 --> 00:42:14,079
will share a lot of this copy and write

00:42:12,240 --> 00:42:16,319
in terms of what spark does

00:42:14,079 --> 00:42:18,560
um so i don't expect that to take much

00:42:16,319 --> 00:42:18,560
time

00:42:19,119 --> 00:42:23,760
awesome um i i don't see any other

00:42:21,359 --> 00:42:26,160
questions so please do try out iceberg

00:42:23,760 --> 00:42:28,079
um uh watch out for the changes that

00:42:26,160 --> 00:42:31,280
we're gonna be putting out in oss

00:42:28,079 --> 00:42:33,520
for absurds it's quite exciting

00:42:31,280 --> 00:42:34,720
um let us know if you have any other

00:42:33,520 --> 00:42:36,560
questions we'll be in booth

00:42:34,720 --> 00:42:38,000
uh feel free to reach out to us through

00:42:36,560 --> 00:42:41,040
slack

00:42:38,000 --> 00:42:42,880
or through email as well uh thanks a lot

00:42:41,040 --> 00:42:59,760
for joining thank you

00:42:42,880 --> 00:43:01,839
thanks a lot guys bye-bye thanks

00:42:59,760 --> 00:43:01,839

YouTube URL: https://www.youtube.com/watch?v=IzkSGKoUxcQ


