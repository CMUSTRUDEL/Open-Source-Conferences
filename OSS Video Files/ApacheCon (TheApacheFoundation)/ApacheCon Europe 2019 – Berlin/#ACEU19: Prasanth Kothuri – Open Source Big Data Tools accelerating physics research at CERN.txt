Title: #ACEU19: Prasanth Kothuri – Open Source Big Data Tools accelerating physics research at CERN
Publication date: 2019-10-31
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	More: https://aceu19.apachecon.com/session/open-source-big-data-tools-accelerating-physics-research-cern

The number of CERN teams utilising big data frameworks – Apache Hadoop, Spark and Kafka to develop their systems has grown significantly in recent years. These systems include the next generation CERN Accelerator Logging Service (NxCALS) which logs data from 20,000 devices that monitor the CERN accelerator complex for online and offline analysis, the monitoring system for the CERN IT Data Center infrastructure and the Worldwide LHC Computing Grid (WLCG) and recently High Energy Physics core data processing software package ROOT can perform parallel and distributed execution using Apache Spark Engine.

This talk will provide an overview of the current deployments of Apache Hadoop, Spark and Kafka and challenges faced in supporting demanding needs from various CERN communities.
Captions: 
	00:00:05,540 --> 00:00:11,090
good afternoon everyone my name is

00:00:07,460 --> 00:00:13,190
Prashant Kotori I work at soul this is

00:00:11,090 --> 00:00:16,160
the physics Research Center in Europe

00:00:13,190 --> 00:00:18,590
I'm going to talk about how big data

00:00:16,160 --> 00:00:21,650
tools are helping us accelerate physics

00:00:18,590 --> 00:00:23,780
research at CERN I give you a quick

00:00:21,650 --> 00:00:25,730
overview of what we are up to at CERN

00:00:23,780 --> 00:00:27,470
and then I will go through the tools

00:00:25,730 --> 00:00:32,059
that are pushing the frontiers of

00:00:27,470 --> 00:00:34,940
physics research so Sun is a European

00:00:32,059 --> 00:00:37,550
laboratory for particle physics so this

00:00:34,940 --> 00:00:40,760
is this was the place where web was

00:00:37,550 --> 00:00:43,250
invented by Tim berners-lee today it's

00:00:40,760 --> 00:00:45,200
home to what is the biggest particle

00:00:43,250 --> 00:00:48,920
accelerator in the world it's called LHC

00:00:45,200 --> 00:00:50,780
Large Hadron Collider so the on in the

00:00:48,920 --> 00:00:52,610
Hadron Collider the particles are

00:00:50,780 --> 00:00:54,620
accelerated to the speed of light and

00:00:52,610 --> 00:00:57,440
they are collided at four different

00:00:54,620 --> 00:00:59,840
points and in each of these points we

00:00:57,440 --> 00:01:01,370
have a big experiment so the experiment

00:00:59,840 --> 00:01:03,920
when I say the experiment it means there

00:01:01,370 --> 00:01:05,329
is a big detector that is detecting the

00:01:03,920 --> 00:01:08,659
collision of these particles and

00:01:05,329 --> 00:01:12,200
collecting data so these are called

00:01:08,659 --> 00:01:14,719
Atlas CMS LHCb and ali's it's a

00:01:12,200 --> 00:01:16,909
collaboration across many countries so

00:01:14,719 --> 00:01:19,520
today we have what we call 22 member

00:01:16,909 --> 00:01:21,409
states and six associate members and

00:01:19,520 --> 00:01:24,859
several worldwide collaborations with

00:01:21,409 --> 00:01:26,810
research institutes etcetera so we have

00:01:24,859 --> 00:01:30,109
a roll 3,000 people who put together

00:01:26,810 --> 00:01:32,270
this LHC and we have more than that like

00:01:30,109 --> 00:01:35,869
12,000 people who actually use our

00:01:32,270 --> 00:01:38,869
facilities to do the research and we

00:01:35,869 --> 00:01:44,659
have around a billion francs per year to

00:01:38,869 --> 00:01:47,060
play with so where are we based so we

00:01:44,659 --> 00:01:50,569
are in the border of Switzerland and

00:01:47,060 --> 00:01:52,459
France so the LHC which is 27 kilometers

00:01:50,569 --> 00:01:56,799
in circumference goes across Switzerland

00:01:52,459 --> 00:01:59,450
and France where in Geneva

00:01:56,799 --> 00:02:00,739
so this is the largest machine in the

00:01:59,450 --> 00:02:02,899
world so there are lot of

00:02:00,739 --> 00:02:05,479
superconducting magnets that actually

00:02:02,899 --> 00:02:10,670
orient the beam so that it goes in a

00:02:05,479 --> 00:02:13,100
circular path and this is the fastest

00:02:10,670 --> 00:02:16,260
racetrack on earth so the protons go and

00:02:13,100 --> 00:02:18,569
almost this almost the speed of light

00:02:16,260 --> 00:02:21,239
and since we want to understand what

00:02:18,569 --> 00:02:23,519
happens when the collisions happen so

00:02:21,239 --> 00:02:30,170
there is absolutely no noise so it's a

00:02:23,519 --> 00:02:30,170
very high vacuum place something is

00:02:42,860 --> 00:02:46,240
something is wrong with

00:02:52,330 --> 00:02:56,500
so let me exit and

00:03:01,380 --> 00:03:08,960
I can use this

00:03:15,980 --> 00:03:22,560
so that is this so as I said it's the

00:03:20,220 --> 00:03:24,989
emptiest place in the solar system so we

00:03:22,560 --> 00:03:28,170
it's a very high vacuum place so that

00:03:24,989 --> 00:03:30,180
the collisions are like what we want but

00:03:28,170 --> 00:03:31,620
there is no noise so when the collisions

00:03:30,180 --> 00:03:33,180
happen it creates a very high

00:03:31,620 --> 00:03:38,250
temperature so it is the hottest spot

00:03:33,180 --> 00:03:40,709
also in the galaxy so at this four

00:03:38,250 --> 00:03:44,160
places so you see the experiments the

00:03:40,709 --> 00:03:49,800
LHC be on the top alleys on your right

00:03:44,160 --> 00:03:51,660
and then there is Atlas and CMS so we

00:03:49,800 --> 00:03:53,760
have detectors so these are particle

00:03:51,660 --> 00:03:55,890
detectors that detect the collisions and

00:03:53,760 --> 00:03:58,920
collect the data so this you can imagine

00:03:55,890 --> 00:04:01,290
as like IOT sensors so this data is

00:03:58,920 --> 00:04:03,959
initially filtered we use what are

00:04:01,290 --> 00:04:06,780
called FPGAs to do the filtering and

00:04:03,959 --> 00:04:08,730
then send to the CERN data center so

00:04:06,780 --> 00:04:10,260
that's why I mentioned the numbers of

00:04:08,730 --> 00:04:12,930
the CERN data centers so here the

00:04:10,260 --> 00:04:14,010
physics data is aggregated in the Sun

00:04:12,930 --> 00:04:16,320
data center where the initial

00:04:14,010 --> 00:04:18,870
reconstruction happens so you can

00:04:16,320 --> 00:04:21,120
imagine each collision as an event which

00:04:18,870 --> 00:04:23,100
is a big record but when the data

00:04:21,120 --> 00:04:25,110
collection happens we collect bits and

00:04:23,100 --> 00:04:27,870
pieces and then these are reconstructed

00:04:25,110 --> 00:04:30,000
using our batch form and stored in our

00:04:27,870 --> 00:04:31,890
data center so in addition we also have

00:04:30,000 --> 00:04:34,500
a remote extension of our data center

00:04:31,890 --> 00:04:36,900
that is in Budapest Hungary so we

00:04:34,500 --> 00:04:38,910
collect when the LHC is running now it

00:04:36,900 --> 00:04:41,370
is in the shutdown mode so there are

00:04:38,910 --> 00:04:43,919
maintenance work going on but when it is

00:04:41,370 --> 00:04:46,530
running we collect around 12.5 petabytes

00:04:43,919 --> 00:04:48,390
of data every month only a fraction of

00:04:46,530 --> 00:04:50,400
it can be analyzed because we are

00:04:48,390 --> 00:04:52,890
restricted by the amount of compute that

00:04:50,400 --> 00:04:54,900
we can we have so as you see we do not

00:04:52,890 --> 00:04:57,330
have the we have around 200 thousand

00:04:54,900 --> 00:04:59,460
course but that that can only you know

00:04:57,330 --> 00:05:01,140
analyze their own 2 petabytes of data so

00:04:59,460 --> 00:05:05,280
when I say 2 petabytes access dates

00:05:01,140 --> 00:05:07,140
mostly reading reading that and if you

00:05:05,280 --> 00:05:10,020
wonder all this data is actually stored

00:05:07,140 --> 00:05:13,050
in the tape we also have some copy on

00:05:10,020 --> 00:05:14,940
the disk but the long-term retention is

00:05:13,050 --> 00:05:17,010
in the tape cartridges and if you see

00:05:14,940 --> 00:05:23,309
the density of tape well nowadays it can

00:05:17,010 --> 00:05:26,309
store up to 200 terabytes each tape

00:05:23,309 --> 00:05:28,439
so in a broader view you can classify it

00:05:26,309 --> 00:05:31,259
data we have at CERN into two buckets so

00:05:28,439 --> 00:05:35,009
what is called physics data so these are

00:05:31,259 --> 00:05:37,409
the data from the collisions for this

00:05:35,009 --> 00:05:39,829
Sun has developed a internal framework

00:05:37,409 --> 00:05:43,619
to analyze this data it's called route

00:05:39,829 --> 00:05:46,319
this is in the 1980s when there is no

00:05:43,619 --> 00:05:48,539
you know big data tools available today

00:05:46,319 --> 00:05:51,269
here we don't use a lot of open source

00:05:48,539 --> 00:05:53,549
stuff so this what we use here is a root

00:05:51,269 --> 00:05:57,659
framework which is internally developed

00:05:53,549 --> 00:06:00,299
so we are trying to use apache spark but

00:05:57,659 --> 00:06:01,949
it is in the early experimental stage so

00:06:00,299 --> 00:06:03,959
I'm going to focus on the bottom part so

00:06:01,949 --> 00:06:05,699
which is the infrastructure of data so

00:06:03,959 --> 00:06:08,069
this is the data that is coming from the

00:06:05,699 --> 00:06:10,049
IOT see in there let's see and we also

00:06:08,069 --> 00:06:12,179
have lot of data catalogues so the

00:06:10,049 --> 00:06:13,559
people will you know catalog data saying

00:06:12,179 --> 00:06:16,319
that you know I am interested in the in

00:06:13,559 --> 00:06:19,769
this dataset and we also use Hadoop and

00:06:16,319 --> 00:06:22,319
spark to monitor our so data centers so

00:06:19,769 --> 00:06:25,139
all the metrics on the logs are

00:06:22,319 --> 00:06:27,869
collected you know various so various

00:06:25,139 --> 00:06:30,539
pipelines and eventually ends up on HDFS

00:06:27,869 --> 00:06:33,419
and it's also available for the users

00:06:30,539 --> 00:06:35,399
where through various means like you

00:06:33,419 --> 00:06:40,679
know graph on Jupiter and things like

00:06:35,399 --> 00:06:43,019
that Kabbalah okay so why did we choose

00:06:40,679 --> 00:06:47,819
Hadoop and spark for big data analysis

00:06:43,019 --> 00:06:50,299
so when initially the LHC was started

00:06:47,819 --> 00:06:52,349
the luminosity was not that high so

00:06:50,299 --> 00:06:54,809
luminosity refers to number of

00:06:52,349 --> 00:06:58,469
collisions that happen in a particular

00:06:54,809 --> 00:07:00,179
beam run so as the years progressed the

00:06:58,469 --> 00:07:01,709
luminosity has increased so which means

00:07:00,179 --> 00:07:03,869
the number of collisions have increased

00:07:01,709 --> 00:07:06,419
so that means the amount of data that we

00:07:03,869 --> 00:07:07,919
could acquire has drastically gone up so

00:07:06,419 --> 00:07:10,829
as I said now it's in the maintenance

00:07:07,919 --> 00:07:12,959
phase where the luminosity is going to

00:07:10,829 --> 00:07:15,539
increase further in the next year so the

00:07:12,959 --> 00:07:17,279
data is going to increase further so we

00:07:15,539 --> 00:07:20,459
cannot store all these in relational

00:07:17,279 --> 00:07:21,659
databases so we need to you know even if

00:07:20,459 --> 00:07:23,519
you are able to store

00:07:21,659 --> 00:07:26,969
we cannot process it in a reasonable

00:07:23,519 --> 00:07:29,429
manner so that's when in 2011 we were

00:07:26,969 --> 00:07:31,919
trying to see what is available and we

00:07:29,429 --> 00:07:35,459
did some early prototype work with

00:07:31,919 --> 00:07:37,680
Hadoop and HBase and we decided together

00:07:35,459 --> 00:07:39,960
with the team that is responsible

00:07:37,680 --> 00:07:41,100
for a let's see data that we should give

00:07:39,960 --> 00:07:43,620
this a go

00:07:41,100 --> 00:07:45,270
today the LHC accelerator logging

00:07:43,620 --> 00:07:48,600
project actually relies heavily on

00:07:45,270 --> 00:07:52,050
Hadoop HBase to store their data and use

00:07:48,600 --> 00:07:54,270
Park to process their data and we choose

00:07:52,050 --> 00:07:55,979
it for this reason so as you know that

00:07:54,270 --> 00:07:58,560
you know we have multiple data

00:07:55,979 --> 00:08:00,449
processing interfaces so it can operate

00:07:58,560 --> 00:08:03,240
at scale by design because the you are

00:08:00,449 --> 00:08:04,620
not sharing anything in comparison with

00:08:03,240 --> 00:08:05,190
the engineered systems where you have

00:08:04,620 --> 00:08:08,550
you know

00:08:05,190 --> 00:08:11,669
InfiniBand or things like that it can

00:08:08,550 --> 00:08:14,729
run on commodity type servers so we can

00:08:11,669 --> 00:08:17,550
just buy x86 servers and it can run and

00:08:14,729 --> 00:08:20,400
we have also many solutions that target

00:08:17,550 --> 00:08:21,660
data analytics and data warehousing we

00:08:20,400 --> 00:08:24,090
are also interested in the stream

00:08:21,660 --> 00:08:26,550
processing so we use the spark streaming

00:08:24,090 --> 00:08:29,910
heavily and for machine learning we

00:08:26,550 --> 00:08:31,949
don't use area we don't yet use Parker

00:08:29,910 --> 00:08:33,620
what we use is mostly carers and

00:08:31,949 --> 00:08:35,849
tensorflow

00:08:33,620 --> 00:08:37,950
and we also went because you know

00:08:35,849 --> 00:08:40,469
several people in the industry were

00:08:37,950 --> 00:08:45,420
using this so there is a good community

00:08:40,469 --> 00:08:46,950
around Hadoop at that time so just to

00:08:45,420 --> 00:08:48,600
give you an idea this is the timeline as

00:08:46,950 --> 00:08:51,660
I said we did some early prototype in

00:08:48,600 --> 00:08:54,510
2012 and then we first deployed our

00:08:51,660 --> 00:08:58,230
cluster in 2013 so we run our cluster

00:08:54,510 --> 00:09:01,290
with Kerberos and we had a big project

00:08:58,230 --> 00:09:03,480
in 2014 that is the monitoring of our IT

00:09:01,290 --> 00:09:05,550
infrastructure and that's when we also

00:09:03,480 --> 00:09:08,430
migrated everything to Hadoop two that

00:09:05,550 --> 00:09:10,830
runs on the arm and we also started a

00:09:08,430 --> 00:09:14,820
little bit of Apache spark in 2014 and

00:09:10,830 --> 00:09:19,440
we we evaluated multiple solutions for

00:09:14,820 --> 00:09:23,940
sequel on let's say unstructured data so

00:09:19,440 --> 00:09:26,520
we we have I mean we mainly use hive but

00:09:23,940 --> 00:09:28,860
we have used Impala in the past but the

00:09:26,520 --> 00:09:30,570
usage has gone down recently nowadays we

00:09:28,860 --> 00:09:33,779
are trying to see if presto can help in

00:09:30,570 --> 00:09:37,230
this area and then we moved on to you

00:09:33,779 --> 00:09:38,910
know having Kafka and then recently we

00:09:37,230 --> 00:09:40,440
have a data science platform that is

00:09:38,910 --> 00:09:42,480
built around Jupiter notebooks so

00:09:40,440 --> 00:09:44,480
internally we call it as Swan

00:09:42,480 --> 00:09:48,839
so this is a service for web based

00:09:44,480 --> 00:09:50,339
analysis and we have done some testing

00:09:48,839 --> 00:09:51,280
the beginning of this year of our Hadoop

00:09:50,339 --> 00:09:54,100
tree

00:09:51,280 --> 00:09:56,920
we plan to migrate to Hadoop 3 early

00:09:54,100 --> 00:09:59,230
next year mostly in January this is our

00:09:56,920 --> 00:10:01,270
let's say our production stuff and

00:09:59,230 --> 00:10:04,810
nowadays we are also running spark on

00:10:01,270 --> 00:10:07,030
kubernetes as you will see that not all

00:10:04,810 --> 00:10:09,910
of our data store is stored on HDFS it

00:10:07,030 --> 00:10:13,600
is stored on some internal CERN storage

00:10:09,910 --> 00:10:16,030
systems so if we can decouple storage

00:10:13,600 --> 00:10:18,670
and compute which you know SPARC can

00:10:16,030 --> 00:10:21,490
help and we can run spark on kubernetes

00:10:18,670 --> 00:10:27,310
on our internal cloud this allows us for

00:10:21,490 --> 00:10:28,810
the elasticity of the resources so I am

00:10:27,310 --> 00:10:31,630
part of the Hadoop and spark service

00:10:28,810 --> 00:10:33,580
together with 8 other colleagues so we

00:10:31,630 --> 00:10:36,040
set up and run the infrastructure not

00:10:33,580 --> 00:10:38,200
only that we also you know heavily

00:10:36,040 --> 00:10:39,880
support our user community because all

00:10:38,200 --> 00:10:41,860
these tools are not very easy for them

00:10:39,880 --> 00:10:44,350
to get started with so we do a lot of

00:10:41,860 --> 00:10:47,530
you know consultancy and we work with

00:10:44,350 --> 00:10:50,800
them in the initial prototype of the you

00:10:47,530 --> 00:10:52,600
know data pipeline or the project and

00:10:50,800 --> 00:10:54,970
then we have a checkpoint you know to

00:10:52,600 --> 00:10:57,880
see whether to go ahead or not and

00:10:54,970 --> 00:11:01,330
having this deployed is not enough

00:10:57,880 --> 00:11:02,770
because people have to use it so this is

00:11:01,330 --> 00:11:05,620
where we had a lot of challenges

00:11:02,770 --> 00:11:08,260
initially that we had acquired a lot of

00:11:05,620 --> 00:11:09,880
data we had a data later but it was not

00:11:08,260 --> 00:11:12,010
very easy to access this data and

00:11:09,880 --> 00:11:14,260
process this data so this is where we

00:11:12,010 --> 00:11:16,780
pivoted little bit and we developed a

00:11:14,260 --> 00:11:19,420
data science platform around Jupiter

00:11:16,780 --> 00:11:22,420
notebooks and then integrating with all

00:11:19,420 --> 00:11:25,030
the Sun resources starting from single

00:11:22,420 --> 00:11:27,250
sign-on to making a spark connection

00:11:25,030 --> 00:11:29,950
with a click of a button and things like

00:11:27,250 --> 00:11:31,480
that and our clients are obviously can

00:11:29,950 --> 00:11:36,670
also you know we provide some docker

00:11:31,480 --> 00:11:39,610
images libraries so this is how it looks

00:11:36,670 --> 00:11:42,280
in numbers so we have around 6 clusters

00:11:39,610 --> 00:11:45,850
so we run it on the physical servers for

00:11:42,280 --> 00:11:48,850
the most critical ones but for let's say

00:11:45,850 --> 00:11:53,800
the reasonable for the others we run it

00:11:48,850 --> 00:11:55,420
on virtual machines on OpenStack so

00:11:53,800 --> 00:11:59,080
today we have around you know 18

00:11:55,420 --> 00:12:03,220
petabytes of storage that is available

00:11:59,080 --> 00:12:06,880
around half of it is used so we have

00:12:03,220 --> 00:12:08,650
leave it we don't have that many coats

00:12:06,880 --> 00:12:10,720
as you see we are migrating more and

00:12:08,650 --> 00:12:13,540
more into spark

00:12:10,720 --> 00:12:15,970
so here the users decide what to do with

00:12:13,540 --> 00:12:18,430
their course at CERN each projects gets

00:12:15,970 --> 00:12:20,410
a certain number of you know course and

00:12:18,430 --> 00:12:22,840
they decide what to do so as you will

00:12:20,410 --> 00:12:25,900
see that we are also provide an easy way

00:12:22,840 --> 00:12:28,360
to have this disposable spark clusters

00:12:25,900 --> 00:12:30,820
so they can create them you know run a

00:12:28,360 --> 00:12:36,580
spark analysis and immediately use it

00:12:30,820 --> 00:12:40,270
for something else so these are the

00:12:36,580 --> 00:12:42,730
components that we use but some of them

00:12:40,270 --> 00:12:45,790
are very low usage for example pig is

00:12:42,730 --> 00:12:48,640
it's very low usage the main ones are

00:12:45,790 --> 00:12:51,910
for compute is mostly spark and little

00:12:48,640 --> 00:12:54,670
bit of hype and for storage it's HDFS

00:12:51,910 --> 00:12:56,770
for large-scale analytics and if you

00:12:54,670 --> 00:13:00,580
want like a index kind of access it's on

00:12:56,770 --> 00:13:01,960
HBase and we also run it Kafka Kafka

00:13:00,580 --> 00:13:05,740
service but I don't talk about it much

00:13:01,960 --> 00:13:11,710
here but that's how people get the data

00:13:05,740 --> 00:13:13,360
into our systems ok so what do we do in

00:13:11,710 --> 00:13:15,790
terms of deployment so initially we

00:13:13,360 --> 00:13:19,990
started with CDH so the cloud era

00:13:15,790 --> 00:13:22,870
distribution for Hadoop but recently we

00:13:19,990 --> 00:13:24,610
have moved to vanilla pacie mainly

00:13:22,870 --> 00:13:27,220
because the cloud era was mixing with

00:13:24,610 --> 00:13:29,980
things from different versions and it is

00:13:27,220 --> 00:13:32,620
hard to know what we are running and we

00:13:29,980 --> 00:13:35,170
also the management was also not willing

00:13:32,620 --> 00:13:37,720
to like support license from cloud era

00:13:35,170 --> 00:13:41,080
so we we now mainly run it on apache

00:13:37,720 --> 00:13:42,550
hadoop we still have one or two

00:13:41,080 --> 00:13:44,560
instances of cloud era which will

00:13:42,550 --> 00:13:47,470
eventually migrate to you know vanilla

00:13:44,560 --> 00:13:49,600
party hadoop and we run it on CentOS so

00:13:47,470 --> 00:13:52,840
we do all the configuration management

00:13:49,600 --> 00:13:55,660
with puppet so when it comes to security

00:13:52,840 --> 00:13:57,940
we use Kerberos and it is fully

00:13:55,660 --> 00:14:00,040
integrated with our Active Directory and

00:13:57,940 --> 00:14:02,410
we also have fine-grained authorization

00:14:00,040 --> 00:14:06,250
with you know integration with LDAP any

00:14:02,410 --> 00:14:09,700
groups we run it in hi available mode

00:14:06,250 --> 00:14:13,899
so all these master services whether it

00:14:09,700 --> 00:14:17,139
is name node or resource manager or

00:14:13,899 --> 00:14:21,250
HBase master they all run in you know hi

00:14:17,139 --> 00:14:23,350
available mode because when the LHC is

00:14:21,250 --> 00:14:25,470
running this the system that relies on

00:14:23,350 --> 00:14:27,930
this infrastructure is highly critical

00:14:25,470 --> 00:14:30,339
so that's why we run it in this mode

00:14:27,930 --> 00:14:32,019
that means we can also do you know

00:14:30,339 --> 00:14:33,550
rolling deployment for some minor

00:14:32,019 --> 00:14:36,220
updates we don't have to bring the whole

00:14:33,550 --> 00:14:39,399
service down we also developed a lot of

00:14:36,220 --> 00:14:42,550
stuff to collect our data for alerting

00:14:39,399 --> 00:14:45,160
and you know long-term analysis we also

00:14:42,550 --> 00:14:47,829
do backups of our HDFS data so it's

00:14:45,160 --> 00:14:50,019
another custom-built solution so we try

00:14:47,829 --> 00:14:53,050
to identify the new files that landed in

00:14:50,019 --> 00:14:58,630
our HDFS and we ship it or tape system

00:14:53,050 --> 00:15:00,639
that's called caster so why did we move

00:14:58,630 --> 00:15:02,649
to Apache Hadoop distribution so it is

00:15:00,639 --> 00:15:04,810
better control of the core software

00:15:02,649 --> 00:15:07,569
stack so we want to be a little bit

00:15:04,810 --> 00:15:10,029
independent from the from the vendor and

00:15:07,569 --> 00:15:11,649
we can also pick some commits that we

00:15:10,029 --> 00:15:14,440
want and you know do our own compilation

00:15:11,649 --> 00:15:16,480
so for example we were early adopters of

00:15:14,440 --> 00:15:19,420
for Sparkle kubernetes so even even

00:15:16,480 --> 00:15:22,660
before it was in two to three we were

00:15:19,420 --> 00:15:24,279
using it internally because we if you

00:15:22,660 --> 00:15:29,920
are able to you know build it yourself

00:15:24,279 --> 00:15:32,050
you have more leaving we we have

00:15:29,920 --> 00:15:34,480
developed everything this this build

00:15:32,050 --> 00:15:38,709
platform internally so it's available I

00:15:34,480 --> 00:15:41,889
can I can provide these details okay

00:15:38,709 --> 00:15:44,949
next this is what is our data sense

00:15:41,889 --> 00:15:47,639
platform so we call it as so on so it is

00:15:44,949 --> 00:15:51,279
essentially jupiter notebooks on demand

00:15:47,639 --> 00:15:54,010
so this is developed at at cern it is

00:15:51,279 --> 00:15:56,529
tightly integrated with our resources so

00:15:54,010 --> 00:15:57,910
if you heard of google collab so when

00:15:56,529 --> 00:16:00,399
you when you use something on google

00:15:57,910 --> 00:16:02,410
collab you use the G Drive to store your

00:16:00,399 --> 00:16:04,839
files so when you use like you know or

00:16:02,410 --> 00:16:06,850
small we have our internal Dropbox like

00:16:04,839 --> 00:16:09,759
thing that is one box where we store our

00:16:06,850 --> 00:16:11,740
files and all the you know high energy

00:16:09,759 --> 00:16:14,170
physics software is also provided in the

00:16:11,740 --> 00:16:16,209
docker images and we also have a

00:16:14,170 --> 00:16:18,670
facility to offload the computation from

00:16:16,209 --> 00:16:21,370
the notebook to the kubernetes cluster

00:16:18,670 --> 00:16:24,110
or if it is a distributed computation to

00:16:21,370 --> 00:16:26,569
a spark cluster on kubernetes

00:16:24,110 --> 00:16:29,389
if you don't know about notebooks or so

00:16:26,569 --> 00:16:31,399
they're very good for you know early

00:16:29,389 --> 00:16:33,170
early stage of analysis or late stage of

00:16:31,399 --> 00:16:35,540
analysis it's mostly interactive so you

00:16:33,170 --> 00:16:37,160
have a cell you write a little code and

00:16:35,540 --> 00:16:39,230
you execute you have your output and

00:16:37,160 --> 00:16:40,879
this is widely used at CERN because it's

00:16:39,230 --> 00:16:42,290
also a research organization so there's

00:16:40,879 --> 00:16:45,019
a lot of trainings that are delivered

00:16:42,290 --> 00:16:46,670
using this platform a lot of initial

00:16:45,019 --> 00:16:48,920
phases of analysis like you know

00:16:46,670 --> 00:16:52,489
exploring your data and the final phase

00:16:48,920 --> 00:16:54,019
of analysis is actually done but it is

00:16:52,489 --> 00:16:55,610
not used for let's say batch because

00:16:54,019 --> 00:16:58,549
that's not the purpose so if there if

00:16:55,610 --> 00:17:01,339
there is like a big ETL job to be run we

00:16:58,549 --> 00:17:03,709
have a batch system for that so we have

00:17:01,339 --> 00:17:06,589
also integrated these notebooks with

00:17:03,709 --> 00:17:08,809
let's say spark and Hadoop that allows

00:17:06,589 --> 00:17:11,179
users you know if they can paralyze

00:17:08,809 --> 00:17:13,250
their computation which is actually

00:17:11,179 --> 00:17:16,309
quite easy at CERN because most of our

00:17:13,250 --> 00:17:18,559
data or physics events and each event is

00:17:16,309 --> 00:17:20,899
discrete so all the data for a

00:17:18,559 --> 00:17:22,909
particular event is in a single record

00:17:20,899 --> 00:17:25,610
so each record can be processed in

00:17:22,909 --> 00:17:27,589
parallel so most of the analysis are

00:17:25,610 --> 00:17:31,460
like you know highly highly scalable

00:17:27,589 --> 00:17:33,380
highly parallelizable let's say and yeah

00:17:31,460 --> 00:17:35,600
it's web-based so we don't have to

00:17:33,380 --> 00:17:37,510
manage you know things that people have

00:17:35,600 --> 00:17:40,070
to install as I said we have around

00:17:37,510 --> 00:17:45,470
12,000 users at CERN who use our

00:17:40,070 --> 00:17:47,960
facilities and this is how it looks if

00:17:45,470 --> 00:17:49,789
you are using if you don't know how the

00:17:47,960 --> 00:17:52,700
Jupiter notebook looks like so this is

00:17:49,789 --> 00:17:54,590
like a spark sequel query so what you

00:17:52,700 --> 00:17:57,350
get with the Jupiter notebook is like

00:17:54,590 --> 00:17:59,899
you know your mark your mark now on your

00:17:57,350 --> 00:18:01,750
code and your visualization so we have

00:17:59,899 --> 00:18:04,669
developed a Jupiter notebook extension

00:18:01,750 --> 00:18:06,549
so if you are uploading a large

00:18:04,669 --> 00:18:08,809
computation to an external cluster

00:18:06,549 --> 00:18:10,309
instead of showing a busy screen we

00:18:08,809 --> 00:18:13,240
thought it's a good idea to show some

00:18:10,309 --> 00:18:15,500
information so we have developed a

00:18:13,240 --> 00:18:17,600
extension that tells them you know at

00:18:15,500 --> 00:18:21,500
what stage it is what is completed and

00:18:17,600 --> 00:18:23,809
it also shows that number of parallel

00:18:21,500 --> 00:18:25,909
active tasks so if somebody has done a

00:18:23,809 --> 00:18:28,730
static allocation of 200 course if he's

00:18:25,909 --> 00:18:30,500
actually using all 200 course or not so

00:18:28,730 --> 00:18:33,500
and we also provide some troubleshooting

00:18:30,500 --> 00:18:35,659
tools from here from here so that you

00:18:33,500 --> 00:18:37,180
know he knows a little bit these are

00:18:35,659 --> 00:18:39,370
people who are let's say you

00:18:37,180 --> 00:18:41,380
of spark they are not experts so our

00:18:39,370 --> 00:18:47,290
team is supposed to be let's say experts

00:18:41,380 --> 00:18:49,660
of this some of these technologies so

00:18:47,290 --> 00:18:51,760
when we say the integration of Jupiter

00:18:49,660 --> 00:18:53,800
notebooks with spark so as you know a

00:18:51,760 --> 00:18:55,240
spark is a distributed computer and work

00:18:53,800 --> 00:18:57,880
right you have what are called drivers

00:18:55,240 --> 00:19:00,460
and executors so all of them should have

00:18:57,880 --> 00:19:03,760
the same equalized delivery on went

00:19:00,460 --> 00:19:06,250
across so for this we externalize our

00:19:03,760 --> 00:19:08,590
software into a read-only file system

00:19:06,250 --> 00:19:11,470
called C VMFS and make sure that all of

00:19:08,590 --> 00:19:13,450
our executors and drivers all are using

00:19:11,470 --> 00:19:15,400
it and you have seen one of the

00:19:13,450 --> 00:19:17,710
extensions that is called spark Monitor

00:19:15,400 --> 00:19:21,490
that is all of our stuff is fully open

00:19:17,710 --> 00:19:23,200
source I have put some github links you

00:19:21,490 --> 00:19:25,000
can try to use it if it is useful for

00:19:23,200 --> 00:19:26,800
you and we have also developed what is

00:19:25,000 --> 00:19:28,990
called a spark connector so with a click

00:19:26,800 --> 00:19:30,790
of a button you make a spark session so

00:19:28,990 --> 00:19:33,610
this this means that you know people

00:19:30,790 --> 00:19:39,310
don't have to write any code for at

00:19:33,610 --> 00:19:40,900
least getting a spark objector so why

00:19:39,310 --> 00:19:43,300
did we develop smart connector so

00:19:40,900 --> 00:19:45,160
initially we just gave them a notebook

00:19:43,300 --> 00:19:48,340
and asked them to create their own spark

00:19:45,160 --> 00:19:50,830
session and we thought and then we had

00:19:48,340 --> 00:19:52,780
many questions and people were really

00:19:50,830 --> 00:19:54,400
confused what they have to do how can

00:19:52,780 --> 00:19:57,720
how they can ship packages from the

00:19:54,400 --> 00:20:00,580
driver to the executors you know how to

00:19:57,720 --> 00:20:03,610
inject additional Python modules and all

00:20:00,580 --> 00:20:06,310
these things so we thought it's not a

00:20:03,610 --> 00:20:08,370
bad idea to take away all this

00:20:06,310 --> 00:20:11,020
configuration complexity from the users

00:20:08,370 --> 00:20:13,210
and hide it in an extension so that's

00:20:11,020 --> 00:20:15,480
what it does so when you click on the

00:20:13,210 --> 00:20:18,100
button you can select a cluster and then

00:20:15,480 --> 00:20:19,720
we also bundle some configuration

00:20:18,100 --> 00:20:22,090
depending on the projects you say there

00:20:19,720 --> 00:20:23,830
are Takai this project needs all these

00:20:22,090 --> 00:20:25,810
six parameters all the time so we just

00:20:23,830 --> 00:20:27,640
bundle it and we also provide the

00:20:25,810 --> 00:20:29,860
flexibility to inject additional

00:20:27,640 --> 00:20:31,420
configuration and then connect and they

00:20:29,860 --> 00:20:36,280
should have a spark object at the end of

00:20:31,420 --> 00:20:39,310
it and the second one that is also

00:20:36,280 --> 00:20:40,840
widely popular is this spark monitor so

00:20:39,310 --> 00:20:42,370
this is another jupiter notebook

00:20:40,840 --> 00:20:45,310
extension that allows for the live

00:20:42,370 --> 00:20:47,320
monitoring of the spark jobs that are

00:20:45,310 --> 00:20:49,210
spawned from the notebook and as i said

00:20:47,320 --> 00:20:51,840
you can also access the spark web UI

00:20:49,210 --> 00:20:51,840
from here

00:20:52,409 --> 00:20:55,809
these are actually if you are using

00:20:54,279 --> 00:20:57,820
Jupiter notebooks these are very very

00:20:55,809 --> 00:21:00,190
easy to install these extensions and

00:20:57,820 --> 00:21:05,649
they should work pretty much out of the

00:21:00,190 --> 00:21:07,659
box and recently we also developed in

00:21:05,649 --> 00:21:11,019
yet another extension that's that we

00:21:07,659 --> 00:21:14,529
internally call it as ket selection so

00:21:11,019 --> 00:21:19,240
this is to create a femoral spark

00:21:14,529 --> 00:21:21,720
kubernetes clusters on demand so okay

00:21:19,240 --> 00:21:25,000
it's possible because internally we run

00:21:21,720 --> 00:21:27,070
sunglass on cloud runs on OpenStack so

00:21:25,000 --> 00:21:29,230
in OpenStack there is an easy way

00:21:27,070 --> 00:21:31,240
through a component called Magnum to

00:21:29,230 --> 00:21:34,779
create kubernetes cluster so so what

00:21:31,240 --> 00:21:37,659
this extension does is it checks if the

00:21:34,779 --> 00:21:40,809
user has privileges to create a cluster

00:21:37,659 --> 00:21:43,179
in this computer sources and then amount

00:21:40,809 --> 00:21:44,740
of quota he has and with a click of a

00:21:43,179 --> 00:21:49,000
button he can create a kubernetes

00:21:44,740 --> 00:21:50,950
cluster and then he can also add a

00:21:49,000 --> 00:21:54,070
cluster that was created by another

00:21:50,950 --> 00:21:56,529
person and he can also grant access from

00:21:54,070 --> 00:21:59,289
one user to another user and also

00:21:56,529 --> 00:22:01,389
immediately attach the notebook to this

00:21:59,289 --> 00:22:06,159
cluster so that you know any of your

00:22:01,389 --> 00:22:07,899
spark tasks can be offloaded to this and

00:22:06,159 --> 00:22:11,110
this is also a jupiter notebook

00:22:07,899 --> 00:22:15,460
extension this will mostly work if you

00:22:11,110 --> 00:22:17,350
are running OpenStack but we also tested

00:22:15,460 --> 00:22:20,080
with the G cloud because as you can

00:22:17,350 --> 00:22:21,669
imagine Sun is kind of restricted with

00:22:20,080 --> 00:22:24,789
the amount of compute that we have the

00:22:21,669 --> 00:22:28,240
research so there are also plans to go

00:22:24,789 --> 00:22:33,519
to some of the clouds like telecom cloud

00:22:28,240 --> 00:22:35,860
or Azure or AWS so we are also it's not

00:22:33,519 --> 00:22:38,740
fully ready we also want users who have

00:22:35,860 --> 00:22:40,899
created a G cloud cluster to be able to

00:22:38,740 --> 00:22:42,970
use it from the notebook so that you

00:22:40,899 --> 00:22:49,720
know everything is in one place so this

00:22:42,970 --> 00:22:52,059
is the idea and this is this is our

00:22:49,720 --> 00:22:53,950
let's say outlook so it's always all

00:22:52,059 --> 00:22:55,570
these things mostly they are ready but

00:22:53,950 --> 00:22:58,179
at the same time they're all work in

00:22:55,570 --> 00:23:01,520
progress so we want to integrate all the

00:22:58,179 --> 00:23:04,360
all the services at CERN

00:23:01,520 --> 00:23:07,610
with this let's go data science platform

00:23:04,360 --> 00:23:10,970
so we have internal storage that's

00:23:07,610 --> 00:23:13,429
called use and we have let's say the

00:23:10,970 --> 00:23:16,460
industry standard storage that is HDFS

00:23:13,429 --> 00:23:16,940
and we have personal storage that is Sun

00:23:16,460 --> 00:23:19,280
box

00:23:16,940 --> 00:23:21,410
it's called Rob it's named after let's

00:23:19,280 --> 00:23:24,190
say Dropbox and we have the software

00:23:21,410 --> 00:23:28,010
that is also available from this and

00:23:24,190 --> 00:23:31,700
users can use long-running clusters like

00:23:28,010 --> 00:23:33,380
ER or they can use kubernetes of course

00:23:31,700 --> 00:23:35,500
today it's only spark but with

00:23:33,380 --> 00:23:38,809
kubernetes you could do a lot of things

00:23:35,500 --> 00:23:40,580
so for example you can you can have your

00:23:38,809 --> 00:23:45,710
airflow running on kubernetes and things

00:23:40,580 --> 00:23:48,260
like this so what you see spark there

00:23:45,710 --> 00:23:55,160
you will have new our intention is we'll

00:23:48,260 --> 00:24:00,950
have multiple things going on there yeah

00:23:55,160 --> 00:24:02,690
so what we don't see many users apart

00:24:00,950 --> 00:24:05,420
from the three for big projects that we

00:24:02,690 --> 00:24:07,160
have using HDFS because they are not

00:24:05,420 --> 00:24:08,600
actually responsible for collecting the

00:24:07,160 --> 00:24:11,420
data someone else is collecting this

00:24:08,600 --> 00:24:12,650
data and it is kept in its own storage

00:24:11,420 --> 00:24:14,300
that's called EOS

00:24:12,650 --> 00:24:16,309
but they do want to analyze data and

00:24:14,300 --> 00:24:18,830
they do want to analyze this data with

00:24:16,309 --> 00:24:20,450
spark because as I said many of the

00:24:18,830 --> 00:24:22,730
analysis is highly parallelizable

00:24:20,450 --> 00:24:25,010
because each event is discrete and a

00:24:22,730 --> 00:24:28,429
file typically has you know thousands of

00:24:25,010 --> 00:24:33,830
events and these since these people want

00:24:28,429 --> 00:24:36,140
to use spark so we have we we are trying

00:24:33,830 --> 00:24:38,270
to we don't want to use a spark onion

00:24:36,140 --> 00:24:40,670
because then you're going to you know

00:24:38,270 --> 00:24:43,130
waste your you know storage resources

00:24:40,670 --> 00:24:45,890
right so we just want to like you know

00:24:43,130 --> 00:24:48,020
this cloud model of separating storage

00:24:45,890 --> 00:24:51,020
with compute so this we run it on

00:24:48,020 --> 00:24:52,070
kubernetes but on top of OpenStack we

00:24:51,020 --> 00:24:54,380
don't need to on top of OpenStack

00:24:52,070 --> 00:24:58,040
because that's like the CERN internal

00:24:54,380 --> 00:25:00,500
private cloud and today we are targeting

00:24:58,040 --> 00:25:03,500
these users like even if people have

00:25:00,500 --> 00:25:05,990
users that does some spark streaming

00:25:03,500 --> 00:25:07,910
with Apache kafka they don't have to be

00:25:05,990 --> 00:25:11,480
on your low they can just as well run it

00:25:07,910 --> 00:25:13,880
on kubernetes and then if they have you

00:25:11,480 --> 00:25:15,050
know highly demand computing so if

00:25:13,880 --> 00:25:17,150
somebody wants

00:25:15,050 --> 00:25:18,650
like a thousand course we cannot you

00:25:17,150 --> 00:25:22,340
know provision this on the physical

00:25:18,650 --> 00:25:24,950
servers in a few months because son has

00:25:22,340 --> 00:25:26,780
a big process of procuring any equipment

00:25:24,950 --> 00:25:29,440
so it typically takes six to eight

00:25:26,780 --> 00:25:32,030
months to buy any physical hardware

00:25:29,440 --> 00:25:33,830
because there is only two times a year

00:25:32,030 --> 00:25:36,980
the Finance Committee meets and science

00:25:33,830 --> 00:25:38,690
of projects so for them you know this

00:25:36,980 --> 00:25:41,270
this kind of things are very very cool

00:25:38,690 --> 00:25:43,400
because they can decide what do how to

00:25:41,270 --> 00:25:45,080
use their computer sources whether they

00:25:43,400 --> 00:25:46,910
want to run spark or they want to run

00:25:45,080 --> 00:25:52,490
let's say internal CERN bad system

00:25:46,910 --> 00:25:54,380
that's based on counter okay I just

00:25:52,490 --> 00:25:56,180
briefly tell you what are the Big Data

00:25:54,380 --> 00:25:57,800
projects we have at CERN so that you

00:25:56,180 --> 00:26:01,340
have like you know some understanding of

00:25:57,800 --> 00:26:03,290
how we are trying to use this so the

00:26:01,340 --> 00:26:06,470
headline project that we have for our

00:26:03,290 --> 00:26:09,410
group is what we call so an accelerator

00:26:06,470 --> 00:26:11,870
logging system so these are the IOT

00:26:09,410 --> 00:26:14,930
login data that is coming from thousands

00:26:11,870 --> 00:26:18,020
of devices that is in this LHC it's a

00:26:14,930 --> 00:26:21,130
critical control system for calibration

00:26:18,020 --> 00:26:24,070
of a lot of things there are radioactive

00:26:21,130 --> 00:26:26,780
cavities that accelerate to particles

00:26:24,070 --> 00:26:30,140
there are superconducting magnets that

00:26:26,780 --> 00:26:31,850
orient the beam and there are wolves if

00:26:30,140 --> 00:26:35,840
there is a beam dump so there's a lot of

00:26:31,850 --> 00:26:38,570
devices here and these people decided

00:26:35,840 --> 00:26:40,490
together with us so that the relational

00:26:38,570 --> 00:26:42,080
database is no longer serve the purpose

00:26:40,490 --> 00:26:43,580
because they are mount of data they're

00:26:42,080 --> 00:26:47,150
going to acquire is going to grow and

00:26:43,580 --> 00:26:48,950
grow so today in the past it was like I

00:26:47,150 --> 00:26:52,010
don't know a couple of terabytes per

00:26:48,950 --> 00:26:55,580
year but nowadays it can be up to 200

00:26:52,010 --> 00:26:57,830
terabytes per year and this is let's say

00:26:55,580 --> 00:27:00,380
critical approachin system that we run

00:26:57,830 --> 00:27:04,310
in our service the components that we

00:27:00,380 --> 00:27:06,800
run is obviously the HDFS and HBase and

00:27:04,310 --> 00:27:08,360
we also provide the tools the front-end

00:27:06,800 --> 00:27:11,210
tools for the programmers and the

00:27:08,360 --> 00:27:14,420
scientists and the applications connect

00:27:11,210 --> 00:27:18,290
with their API so they so we do we are

00:27:14,420 --> 00:27:21,760
not mainly into the picture and they

00:27:18,290 --> 00:27:24,440
also land data into Kafka and then

00:27:21,760 --> 00:27:28,460
process them and store it into HBase and

00:27:24,440 --> 00:27:32,659
HDFS and to reduce the

00:27:28,460 --> 00:27:34,519
latency they you write every every

00:27:32,659 --> 00:27:36,619
second but then there is also a

00:27:34,519 --> 00:27:39,529
compactor that complex the jobs and

00:27:36,619 --> 00:27:41,960
keeps them in nice reasonable sizes as

00:27:39,529 --> 00:27:44,179
you can imagine you know HTF is not is

00:27:41,960 --> 00:27:51,289
not very good if you have lots of small

00:27:44,179 --> 00:27:54,100
files and this is another for a nightly

00:27:51,289 --> 00:27:56,779
monitoring infrastructure so we get data

00:27:54,100 --> 00:28:00,499
let's say not only from our Sun

00:27:56,779 --> 00:28:03,080
datacenter Sun also runs the grid this

00:28:00,499 --> 00:28:06,740
is called hep computing grid so it's

00:28:03,080 --> 00:28:09,070
called WL CG LHC computing grid where

00:28:06,740 --> 00:28:12,110
there are a lot of great services that

00:28:09,070 --> 00:28:15,289
produce logs and those logs are also

00:28:12,110 --> 00:28:18,259
sent to Kafka and then eventually they

00:28:15,289 --> 00:28:20,389
are also stored not only in HDFS but

00:28:18,259 --> 00:28:22,610
also in elasticsearch and Lex DB so that

00:28:20,389 --> 00:28:25,249
people have this nice graphical

00:28:22,610 --> 00:28:27,289
interfaces like key banana fana to just

00:28:25,249 --> 00:28:29,480
see a plot and get an alert simple

00:28:27,289 --> 00:28:31,759
alerts and if they want to have some

00:28:29,480 --> 00:28:34,309
long-term or trending so for example

00:28:31,759 --> 00:28:36,980
this group we have committed I don't

00:28:34,309 --> 00:28:39,259
know this amount of resources but let's

00:28:36,980 --> 00:28:41,539
do an accounting and see you know if

00:28:39,259 --> 00:28:43,639
they have used more or less so for this

00:28:41,539 --> 00:28:46,279
kind of you know along like analysis

00:28:43,639 --> 00:28:48,429
that's when they will use you know spark

00:28:46,279 --> 00:28:51,350
together with the data that is on HDFS

00:28:48,429 --> 00:28:53,720
so in elastic search and in Flex DB the

00:28:51,350 --> 00:28:56,509
data is kept depending on the amount

00:28:53,720 --> 00:29:00,549
from six weeks to three months but in

00:28:56,509 --> 00:29:03,289
HDFS forever it so it's like an archival

00:29:00,549 --> 00:29:04,610
here the scale is not as because yeah

00:29:03,289 --> 00:29:08,629
let's see because we are just talking

00:29:04,610 --> 00:29:12,169
about computers I mean the servers which

00:29:08,629 --> 00:29:17,480
are like in thousands not not like more

00:29:12,169 --> 00:29:19,279
lots of thousands ok so that is all

00:29:17,480 --> 00:29:21,019
about the infrastructure but we also

00:29:19,279 --> 00:29:23,509
want to see if some of the industry

00:29:21,019 --> 00:29:26,029
tools can be used for the physics actual

00:29:23,509 --> 00:29:28,669
physics analysis so this is how a

00:29:26,029 --> 00:29:30,740
typical LHC data pipeline looks like so

00:29:28,669 --> 00:29:33,350
those are the pictures of the detectors

00:29:30,740 --> 00:29:36,019
for all the experiments so they detect

00:29:33,350 --> 00:29:39,200
the path of the particles and collect

00:29:36,019 --> 00:29:41,880
this data and send and we have their

00:29:39,200 --> 00:29:43,880
what is called data acquisitions

00:29:41,880 --> 00:29:46,769
they do lot of event filtering because

00:29:43,880 --> 00:29:49,620
we don't want to collect a lot of known

00:29:46,769 --> 00:29:52,139
physics what they call known physics so

00:29:49,620 --> 00:29:53,970
which means almost to 90% of the data is

00:29:52,139 --> 00:29:56,970
filtered out so when you are getting

00:29:53,970 --> 00:29:58,500
this volume of data the filtering is

00:29:56,970 --> 00:30:01,769
typically done with hardware

00:29:58,500 --> 00:30:04,799
accelerators like FPGAs and then

00:30:01,769 --> 00:30:07,409
whatever is filtered out is the

00:30:04,799 --> 00:30:09,090
remaining 10% you send to the data

00:30:07,409 --> 00:30:11,760
center where the reconstruction happens

00:30:09,090 --> 00:30:16,500
and then we have this huge data sets

00:30:11,760 --> 00:30:19,139
that are made available and from the

00:30:16,500 --> 00:30:20,639
reconstructed data also you know there

00:30:19,139 --> 00:30:23,760
is a process called skimming

00:30:20,639 --> 00:30:27,090
so essentially you know to choose events

00:30:23,760 --> 00:30:29,549
based on you know a run based on the

00:30:27,090 --> 00:30:31,049
amount of energy and things like this so

00:30:29,549 --> 00:30:33,210
that's what they do in skimming is

00:30:31,049 --> 00:30:36,720
essentially yet another filtering of the

00:30:33,210 --> 00:30:38,549
data and they create a smaller data

00:30:36,720 --> 00:30:42,090
files those are called analysis objects

00:30:38,549 --> 00:30:45,690
so today spark plays a role in this

00:30:42,090 --> 00:30:48,539
skimming which is like a big ETL and

00:30:45,690 --> 00:30:51,090
then in the final step of analysis where

00:30:48,539 --> 00:30:53,490
you do interactive analysis on you know

00:30:51,090 --> 00:31:00,539
small analysis data objects which are

00:30:53,490 --> 00:31:02,779
typically two gig to gig of size okay so

00:31:00,539 --> 00:31:06,899
what is this CMS data addiction facility

00:31:02,779 --> 00:31:09,779
so typically what they were doing before

00:31:06,899 --> 00:31:12,269
cm is one of our experiments is they

00:31:09,779 --> 00:31:15,750
were running this skimming process the

00:31:12,269 --> 00:31:16,679
four times a year so people from this

00:31:15,750 --> 00:31:19,649
collaboration

00:31:16,679 --> 00:31:21,570
send the request for the data how the

00:31:19,649 --> 00:31:24,360
data should be skimmed so essentially

00:31:21,570 --> 00:31:26,220
specifying the filtering conditions and

00:31:24,360 --> 00:31:28,289
it is done once a quarter so it's not

00:31:26,220 --> 00:31:29,820
run on demand so with this data

00:31:28,289 --> 00:31:32,279
reduction facility with the help of

00:31:29,820 --> 00:31:34,980
spark of what we want to do is can we do

00:31:32,279 --> 00:31:36,899
on demand reduction of data it was done

00:31:34,980 --> 00:31:38,940
only four times here because there was

00:31:36,899 --> 00:31:42,720
not enough you know compute resources

00:31:38,940 --> 00:31:44,940
and also the process to take skimming is

00:31:42,720 --> 00:31:47,070
is little at least it was a bit

00:31:44,940 --> 00:31:50,059
sequential so with this data reduction

00:31:47,070 --> 00:31:54,900
facility so we were trying to

00:31:50,059 --> 00:31:58,730
essentially you know have a large park

00:31:54,900 --> 00:32:02,490
to reduce one petabyte of data into

00:31:58,730 --> 00:32:06,840
hundreds of gigabytes so we were able to

00:32:02,490 --> 00:32:11,250
do this with spark we were using I think

00:32:06,840 --> 00:32:15,660
around the 2000 2000 course and it was

00:32:11,250 --> 00:32:17,160
done in 48 hours we have a proof of

00:32:15,660 --> 00:32:19,559
concept available even though we were

00:32:17,160 --> 00:32:22,050
managed to do the reduction of one bit

00:32:19,559 --> 00:32:23,730
of a data but it is still in discussion

00:32:22,050 --> 00:32:25,770
whether they want to go ahead in this

00:32:23,730 --> 00:32:28,200
direction or not whether you do it with

00:32:25,770 --> 00:32:31,170
spark or with something else even with

00:32:28,200 --> 00:32:33,120
spark the world the number of CPU

00:32:31,170 --> 00:32:40,559
seconds is still the same what you

00:32:33,120 --> 00:32:42,570
reduces the wall time ok so to do this

00:32:40,559 --> 00:32:45,660
we had to you know develop some

00:32:42,570 --> 00:32:47,610
components the data the physics data is

00:32:45,660 --> 00:32:50,670
stored in what are called root files so

00:32:47,610 --> 00:32:52,350
these are not Parker files or orc or any

00:32:50,670 --> 00:32:56,190
of these files that you guys generally

00:32:52,350 --> 00:32:58,800
know and to load this root file into a

00:32:56,190 --> 00:33:01,290
spark data frame we had to write a spark

00:32:58,800 --> 00:33:04,410
data source so that is called the spark

00:33:01,290 --> 00:33:08,100
root so there is a link and in addition

00:33:04,410 --> 00:33:09,809
this data is not stored in HDFS or s3

00:33:08,100 --> 00:33:12,150
where you have connectors available and

00:33:09,809 --> 00:33:14,460
you can access data so this is stored in

00:33:12,150 --> 00:33:17,220
yours and it speaks let's say a

00:33:14,460 --> 00:33:22,559
scientific protocol called X rudy so we

00:33:17,220 --> 00:33:24,870
had to extend the HDFS and create this

00:33:22,559 --> 00:33:27,690
root colon slash slash protocol so we

00:33:24,870 --> 00:33:30,570
have to write this so that we can

00:33:27,690 --> 00:33:36,900
directly access the data or x0t protocol

00:33:30,570 --> 00:33:39,540
from sparker ok that is the first part

00:33:36,900 --> 00:33:42,240
so this is that was basically here in

00:33:39,540 --> 00:33:44,070
the in the CMS data reduction where the

00:33:42,240 --> 00:33:49,200
spark was used for the offline

00:33:44,070 --> 00:33:51,960
processing and for the physics analysis

00:33:49,200 --> 00:33:54,600
most of the phd's what they do is they

00:33:51,960 --> 00:33:57,150
use a analysis package called route so

00:33:54,600 --> 00:33:59,220
it was written in C++ but nowadays

00:33:57,150 --> 00:34:02,010
everybody is more comfortable to work

00:33:59,220 --> 00:34:05,160
with the Python so those guys have made

00:34:02,010 --> 00:34:07,620
a Python API for a lot of this C++ code

00:34:05,160 --> 00:34:08,190
they have but now what we are trying to

00:34:07,620 --> 00:34:12,389
do

00:34:08,190 --> 00:34:14,730
sir whether we can push we can have like

00:34:12,389 --> 00:34:18,119
multiple back-end so a back-end that

00:34:14,730 --> 00:34:19,710
runs locally a back-end that can you

00:34:18,119 --> 00:34:21,679
know run in spark so essentially it's

00:34:19,710 --> 00:34:24,450
the same philosophy so we build a dag

00:34:21,679 --> 00:34:25,710
based on the products your projections

00:34:24,450 --> 00:34:30,060
and the filter pushdowns

00:34:25,710 --> 00:34:33,980
and defining additional columns and the

00:34:30,060 --> 00:34:33,980
scent is dragged to spark for execution

00:34:34,970 --> 00:34:40,079
so this is still in the experimental

00:34:38,159 --> 00:34:43,500
phase

00:34:40,079 --> 00:34:47,159
the idea is to either you know send it

00:34:43,500 --> 00:34:50,250
to spark mostly on the kubernetes it is

00:34:47,159 --> 00:34:53,339
in the experimental phase because even

00:34:50,250 --> 00:34:57,000
though a basic things work if you know a

00:34:53,339 --> 00:34:59,250
C++ object you also have your methods

00:34:57,000 --> 00:35:02,060
stored inside that yeah so some of the

00:34:59,250 --> 00:35:05,069
scan not be accessed now directly so

00:35:02,060 --> 00:35:07,140
further work is going on to make sure

00:35:05,069 --> 00:35:08,700
that the our data frame what we call can

00:35:07,140 --> 00:35:14,790
run in the distributed fashion using

00:35:08,700 --> 00:35:18,930
spark and the way it runs is we take the

00:35:14,790 --> 00:35:21,960
full data set and depending on the

00:35:18,930 --> 00:35:24,420
amount of events that we have we on the

00:35:21,960 --> 00:35:27,450
amount of compute that you can get so

00:35:24,420 --> 00:35:30,690
these are divided into sub ranges so

00:35:27,450 --> 00:35:32,700
that each of the spark executors can run

00:35:30,690 --> 00:35:41,190
these particular rain ranges of the

00:35:32,700 --> 00:35:42,480
events our team is still little bit in

00:35:41,190 --> 00:35:46,970
the beginning phase of the machine

00:35:42,480 --> 00:35:50,760
learning but we had like some initial

00:35:46,970 --> 00:35:53,130
prototyping for machine learning so we

00:35:50,760 --> 00:35:55,230
are trying to see if spark can help us

00:35:53,130 --> 00:35:58,260
in the mostly on the featured

00:35:55,230 --> 00:36:00,150
engineering side not on the model

00:35:58,260 --> 00:36:01,770
training yet but even though we have

00:36:00,150 --> 00:36:03,960
used the model training together with

00:36:01,770 --> 00:36:07,890
what is called a big deal from Intel

00:36:03,960 --> 00:36:10,750
with the spark we are mainly trying to

00:36:07,890 --> 00:36:12,970
see if you know the initial phases where

00:36:10,750 --> 00:36:15,130
there is a lot of amount a amount of

00:36:12,970 --> 00:36:17,440
data and the features that we want to

00:36:15,130 --> 00:36:21,190
extract from this data this part can

00:36:17,440 --> 00:36:23,650
help us or not so there was also my

00:36:21,190 --> 00:36:25,510
colleague has given a sonic data break

00:36:23,650 --> 00:36:31,119
so if you're interested you can follow

00:36:25,510 --> 00:36:34,440
that okay so what are the conclusions so

00:36:31,119 --> 00:36:37,270
we see that there is a big demand for

00:36:34,440 --> 00:36:41,650
you know let's say industry big data

00:36:37,270 --> 00:36:44,500
tools at it's all we have multiple

00:36:41,650 --> 00:36:46,839
critical projects in production these

00:36:44,500 --> 00:36:50,550
are around the monitoring and we also

00:36:46,839 --> 00:36:52,810
use it at Convention for the security so

00:36:50,550 --> 00:36:55,720
to monitor what is happening in our

00:36:52,810 --> 00:36:58,290
networks as well and we are in the early

00:36:55,720 --> 00:37:01,960
phases of using it for the physics data

00:36:58,290 --> 00:37:04,869
so we are part of the Hadoop and spark

00:37:01,960 --> 00:37:06,520
service at IT so we are kind of stable

00:37:04,869 --> 00:37:08,589
in some areas like you know in the

00:37:06,520 --> 00:37:11,319
storage and things like that but we are

00:37:08,589 --> 00:37:13,869
always trying to see now we are trying

00:37:11,319 --> 00:37:17,109
to evaluate whether pressed or Drake and

00:37:13,869 --> 00:37:21,430
help you know some kind of a JDBC access

00:37:17,109 --> 00:37:24,040
from these storage systems and we

00:37:21,430 --> 00:37:26,230
because it's our research organization

00:37:24,040 --> 00:37:27,880
we also spend a lot of effort in

00:37:26,230 --> 00:37:31,300
building a community we deliver

00:37:27,880 --> 00:37:33,700
trainings on SPARC and Hadoop and we

00:37:31,300 --> 00:37:35,710
also write a lot of blog articles so the

00:37:33,700 --> 00:37:39,069
link is here so if you are interested

00:37:35,710 --> 00:37:41,290
you can go through that and if you are

00:37:39,069 --> 00:37:42,940
using similar technologies around the

00:37:41,290 --> 00:37:45,869
data science platform or the Hadoop

00:37:42,940 --> 00:37:50,619
tools we are also happy to exchange

00:37:45,869 --> 00:37:52,780
experience and things like that and most

00:37:50,619 --> 00:37:55,119
of the things I mean put here or open

00:37:52,780 --> 00:38:01,030
source if they are useful you can use

00:37:55,119 --> 00:38:03,660
them ok so that's all I have so if you

00:38:01,030 --> 00:38:03,660
have any questions

00:38:06,390 --> 00:38:13,650
I have two questions regarding sparks

00:38:11,640 --> 00:38:16,470
over kubernetes the first one is you

00:38:13,650 --> 00:38:18,330
said that it worked on OpenStack is it

00:38:16,470 --> 00:38:20,520
just because it's the only thing you

00:38:18,330 --> 00:38:22,590
have or is it a technical limitation

00:38:20,520 --> 00:38:25,560
that you see it's not the technical

00:38:22,590 --> 00:38:27,510
limitation we run OpenStack so that

00:38:25,560 --> 00:38:31,410
essentially virtualizes our physical

00:38:27,510 --> 00:38:33,450
servers so we at the moment we only run

00:38:31,410 --> 00:38:35,070
kubernetes on these VMs we don't run

00:38:33,450 --> 00:38:37,770
kubernetes on physical machines you can

00:38:35,070 --> 00:38:39,330
run them but it's an internal decision

00:38:37,770 --> 00:38:42,930
to optimize the resources and the

00:38:39,330 --> 00:38:46,620
configuration and my second one is do

00:38:42,930 --> 00:38:48,120
you see specific usages of the people

00:38:46,620 --> 00:38:51,540
that are using and spark of our

00:38:48,120 --> 00:38:53,550
kubernetes for instance are they using

00:38:51,540 --> 00:38:54,840
them so because you said that there was

00:38:53,550 --> 00:38:56,730
a limit in the course that you could

00:38:54,840 --> 00:39:01,800
allocate so yes this was a way for them

00:38:56,730 --> 00:39:03,450
to yes yeah but do they for instance I

00:39:01,800 --> 00:39:05,700
was told that it was not very efficient

00:39:03,450 --> 00:39:07,170
to run small queries in Chianti's

00:39:05,700 --> 00:39:13,470
because there is a start of time that is

00:39:07,170 --> 00:39:15,710
not efficient so yeah we try to ensure

00:39:13,470 --> 00:39:18,660
we have done lot of optimizations

00:39:15,710 --> 00:39:21,000
especially if you have very fat images

00:39:18,660 --> 00:39:23,250
and if you have to pull them you know

00:39:21,000 --> 00:39:26,250
there is a higher startup time to start

00:39:23,250 --> 00:39:28,440
your executor on your pod so we have

00:39:26,250 --> 00:39:30,060
some stuff that warms up our cluster so

00:39:28,440 --> 00:39:32,640
that what users need are always

00:39:30,060 --> 00:39:34,890
available and in terms of answering your

00:39:32,640 --> 00:39:36,840
first part so we have a lot of users

00:39:34,890 --> 00:39:39,510
nowadays using sparkin kubernetes so

00:39:36,840 --> 00:39:41,850
mainly because their data is not in HDFS

00:39:39,510 --> 00:39:44,220
so all the physics data is in CERN

00:39:41,850 --> 00:39:48,030
storage system that's calling us and

00:39:44,220 --> 00:39:51,240
they can only use kubernetes to access

00:39:48,030 --> 00:39:52,920
the data and we have few users from time

00:39:51,240 --> 00:39:56,280
to time because our Hadoop clusters are

00:39:52,920 --> 00:39:57,860
storage dense clusters so the ratio

00:39:56,280 --> 00:40:02,190
between the amount of storage and course

00:39:57,860 --> 00:40:05,640
is very high so though if we have like

00:40:02,190 --> 00:40:07,890
some ad hoc big analysis that cannot be

00:40:05,640 --> 00:40:10,800
fit because we have let's say 40 percent

00:40:07,890 --> 00:40:13,260
of the cluster that's always occupied

00:40:10,800 --> 00:40:15,360
with some streaming jobs and some

00:40:13,260 --> 00:40:17,370
production jobs all the time and if the

00:40:15,360 --> 00:40:19,200
analysis cannot be fit in the remaining

00:40:17,370 --> 00:40:19,800
60 percent we send them to the component

00:40:19,200 --> 00:40:22,060
is

00:40:19,800 --> 00:40:23,980
and we are also planning to move more

00:40:22,060 --> 00:40:27,340
and more towards kubernetes because this

00:40:23,980 --> 00:40:39,460
helps us to optimize the resources for

00:40:27,340 --> 00:40:43,060
various needs so you put in your data

00:40:39,460 --> 00:40:46,330
from Kafka to Hadoop as you said and so

00:40:43,060 --> 00:40:50,140
is this data for like offline analysis

00:40:46,330 --> 00:40:52,960
or do you need it real-time intuitive so

00:40:50,140 --> 00:40:55,840
the data is also required for a

00:40:52,960 --> 00:40:58,120
real-time so the real-time data so we

00:40:55,840 --> 00:41:00,670
keep the data for the last one week in

00:40:58,120 --> 00:41:03,670
HBase so that it can be immediately

00:41:00,670 --> 00:41:07,930
accessed and the same data is also kept

00:41:03,670 --> 00:41:09,790
in HDFS for offline processing and we

00:41:07,930 --> 00:41:12,190
have used goblin I don't know if you

00:41:09,790 --> 00:41:16,030
heard of goblin so we use goblin to pump

00:41:12,190 --> 00:41:17,950
data both to HBase and HDFS so that the

00:41:16,030 --> 00:41:20,830
frequency at which it runs is

00:41:17,950 --> 00:41:24,400
configurable today it runs every five

00:41:20,830 --> 00:41:25,930
seconds Oh every five seconds so that's

00:41:24,400 --> 00:41:27,970
what my question is to with goblin you

00:41:25,930 --> 00:41:29,950
run it every five seconds right and at

00:41:27,970 --> 00:41:31,780
your rate like you would be creating a

00:41:29,950 --> 00:41:34,300
lot of small files with that yeah so we

00:41:31,780 --> 00:41:37,800
have a compact job that compacts all the

00:41:34,300 --> 00:41:40,030
small files into good enough size files

00:41:37,800 --> 00:41:42,910
every day every night we run this every

00:41:40,030 --> 00:41:46,750
night so so how do your users get those

00:41:42,910 --> 00:41:49,510
so the users so the users are hidden by

00:41:46,750 --> 00:41:51,820
our API so our API depending on the

00:41:49,510 --> 00:41:54,760
timestamp will redirect them either to

00:41:51,820 --> 00:41:56,440
HBase or HDFS so the users will never

00:41:54,760 --> 00:41:58,420
see these small files because the latest

00:41:56,440 --> 00:42:01,330
data is always in HP HBase and they go

00:41:58,420 --> 00:42:02,560
to each pacer and the HDFS by the time

00:42:01,330 --> 00:42:05,680
the user see it they are nicely

00:42:02,560 --> 00:42:08,440
compacted and we we kind of target

00:42:05,680 --> 00:42:09,880
around you know 2 gig size but that's

00:42:08,440 --> 00:42:11,740
not always possible because there is

00:42:09,880 --> 00:42:14,680
some partitioning scheme and some

00:42:11,740 --> 00:42:17,070
devices send less data than others and

00:42:14,680 --> 00:42:17,070
things like that

00:42:20,980 --> 00:42:29,619
so let's tank buttons mission present

00:42:26,270 --> 00:42:29,619
okay the second he runs one

00:42:29,900 --> 00:42:34,510

YouTube URL: https://www.youtube.com/watch?v=9TaTHKeNxDc


