Title: #ACEU19: Apache Beam: Running Big Data Pipelines in Python and Go with Spark
Publication date: 2019-10-31
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	Speakers: Ismaël Mejía & Kyle Weaver
More: https://aceu19.apachecon.com/session/apache-beam-running-big-data-pipelines-python-and-go-spark

Apache Spark is the most popular open source analytics engine for large-scale data processing. Spark is not only a mature system, but thanks to its support of multiple resource managers like Hadoop, Mesos, and Kubernetes it has become a popular choice for both batch and streaming workloads in the industry.

Apache Beam has included a Spark runner since its inception to allow users to execute Beam pipelines on Spark, but until recently the Spark runner could only execute pipelines written in Java. In this talk we will introduce the portability framework and how we adapted it into the existing Spark runner translation to make the Spark runner portable.

We will show you how to execute Beam pipelines written in Python and Golang in Spark with Beam and invite you to use the new Spark Portable Runner. We will mention the use case of Tensorflow Extended, the end-to-end platform for data validation and transformation and ML model analysis. Finally we will discuss ongoing work and some future plans for the portable runner.
Captions: 
	00:00:05,190 --> 00:00:11,200
hi everybody thanks for being here this

00:00:07,750 --> 00:00:14,139
morning I'm Kyle and I'm a software

00:00:11,200 --> 00:00:18,340
engineer at Google and an Apache beam

00:00:14,139 --> 00:00:21,099
committer hi I'm you smile I'm a

00:00:18,340 --> 00:00:22,240
software engineer at talent I'm part of

00:00:21,099 --> 00:00:24,460
this project

00:00:22,240 --> 00:00:27,250
Apache beam and also I contribute to

00:00:24,460 --> 00:00:31,330
other projects in the Big Data space a

00:00:27,250 --> 00:00:35,140
patch so today we're going to talk about

00:00:31,330 --> 00:00:37,870
a patchy beam and a little bit not tiny

00:00:35,140 --> 00:00:40,180
introduction to Apache beam and but we

00:00:37,870 --> 00:00:44,050
are going to focus more on how we do the

00:00:40,180 --> 00:00:46,539
magic of running beam code in part

00:00:44,050 --> 00:00:48,760
runner or it's just concretely the main

00:00:46,539 --> 00:00:51,250
focus of the talk and with the goal of

00:00:48,760 --> 00:00:53,739
showing how we support languages like

00:00:51,250 --> 00:00:55,899
Python and especially go that may not be

00:00:53,739 --> 00:00:59,079
supported in the target system but we'll

00:00:55,899 --> 00:01:01,629
go there but before I start just one

00:00:59,079 --> 00:01:04,090
question who knows

00:01:01,629 --> 00:01:09,060
MapReduce spark basic stuff like

00:01:04,090 --> 00:01:11,890
working-level please okay that's good

00:01:09,060 --> 00:01:14,799
who knows been a little bitter has done

00:01:11,890 --> 00:01:18,310
something with beam okay so that's less

00:01:14,799 --> 00:01:21,850
than I expected but still good okay so

00:01:18,310 --> 00:01:25,150
we are going to talk then and we hope

00:01:21,850 --> 00:01:28,329
this this introduction to be model and

00:01:25,150 --> 00:01:31,960
and how what is this be shown and what

00:01:28,329 --> 00:01:34,000
is how it works but suddenly the that we

00:01:31,960 --> 00:01:35,140
don't go deep into the details of the

00:01:34,000 --> 00:01:38,409
v-model because there would be a

00:01:35,140 --> 00:01:40,509
complete presentation by itself so beam

00:01:38,409 --> 00:01:44,860
is an Apache open source project that is

00:01:40,509 --> 00:01:46,990
a top-level project since 2016 it was

00:01:44,860 --> 00:01:49,000
donated by Google and other partners

00:01:46,990 --> 00:01:54,610
like Luthor and that artisans now

00:01:49,000 --> 00:01:56,649
barbarika and general propose parallel

00:01:54,610 --> 00:01:58,689
distributed programming framework which

00:01:56,649 --> 00:02:01,210
means that is in the family of spark

00:01:58,689 --> 00:02:04,689
fleeing and all of these things but this

00:02:01,210 --> 00:02:06,820
is more is mostly two things it has

00:02:04,689 --> 00:02:09,009
mostly two goals one to have a unified

00:02:06,820 --> 00:02:12,280
programming model so it's a unified API

00:02:09,009 --> 00:02:15,190
let's say to do batch and streaming with

00:02:12,280 --> 00:02:17,950
the same code and the second one is to

00:02:15,190 --> 00:02:19,989
be portable and portable means

00:02:17,950 --> 00:02:22,000
things being portable in the sense that

00:02:19,989 --> 00:02:24,310
we translate the code to writing beam

00:02:22,000 --> 00:02:27,280
into the target execution system for

00:02:24,310 --> 00:02:29,650
example spark and also that we support

00:02:27,280 --> 00:02:34,000
multiple languages to thus thus the

00:02:29,650 --> 00:02:36,280
overall growth of beam and to for the

00:02:34,000 --> 00:02:38,830
dispersed part of having a unified model

00:02:36,280 --> 00:02:41,890
for batching streaming beam introduces

00:02:38,830 --> 00:02:42,300
some extra concepts into the programming

00:02:41,890 --> 00:02:46,420
model

00:02:42,300 --> 00:02:48,129
concretely every element that you are

00:02:46,420 --> 00:02:50,680
going to processing beam has an

00:02:48,129 --> 00:02:52,840
Associated timestamp and the idea of

00:02:50,680 --> 00:02:56,620
this timestamp is that you can use it to

00:02:52,840 --> 00:02:59,079
define when you do aggregations or

00:02:56,620 --> 00:03:00,670
operations downstream why because we are

00:02:59,079 --> 00:03:02,860
supporting streaming so we don't know

00:03:00,670 --> 00:03:06,269
when is the end of the stream so we have

00:03:02,860 --> 00:03:09,849
to put some way to decide when we do

00:03:06,269 --> 00:03:11,709
operations and calculate results and we

00:03:09,849 --> 00:03:13,540
do that with we do this with three

00:03:11,709 --> 00:03:15,190
concepts well the concept of window win

00:03:13,540 --> 00:03:16,690
when we define windows that can be

00:03:15,190 --> 00:03:19,209
closed with certain condition for

00:03:16,690 --> 00:03:22,030
example a 10-minute window and I get the

00:03:19,209 --> 00:03:23,859
results that I have from 10 minutes we

00:03:22,030 --> 00:03:26,200
have also the concept of watermark that

00:03:23,859 --> 00:03:28,480
defines that we sum is a progress

00:03:26,200 --> 00:03:31,060
concept that defines that we are not

00:03:28,480 --> 00:03:34,510
going to receive more elements but this

00:03:31,060 --> 00:03:37,000
is not this is an estimation this is not

00:03:34,510 --> 00:03:38,829
something exactly accurate and to adjust

00:03:37,000 --> 00:03:41,530
this estimation we have the concept of

00:03:38,829 --> 00:03:44,380
triggers where we can define let's wait

00:03:41,530 --> 00:03:47,049
for more lemons or let's close this

00:03:44,380 --> 00:03:49,090
computation at some point so what this

00:03:47,049 --> 00:03:51,579
is something you can find more in detail

00:03:49,090 --> 00:03:53,319
in a presentation of beam but in general

00:03:51,579 --> 00:03:56,819
the idea to have here is that all these

00:03:53,319 --> 00:03:59,470
let's say transforms and and operators

00:03:56,819 --> 00:04:02,440
learn to play with the trade-off of

00:03:59,470 --> 00:04:05,769
latency and correctness so when we want

00:04:02,440 --> 00:04:08,410
to have output produced so does latency

00:04:05,769 --> 00:04:10,840
and how exact we need this output to be

00:04:08,410 --> 00:04:14,919
and so we have like knobs to play with

00:04:10,840 --> 00:04:16,930
that so as I said the goal of the beam

00:04:14,919 --> 00:04:19,510
project is to support multiple languages

00:04:16,930 --> 00:04:21,099
especially so any any user will write

00:04:19,510 --> 00:04:23,860
his program in one of these languages

00:04:21,099 --> 00:04:26,890
and then these languages are translated

00:04:23,860 --> 00:04:29,740
by what we call in groaners include the

00:04:26,890 --> 00:04:32,050
target destination system

00:04:29,740 --> 00:04:34,780
and that's what that's some some of the

00:04:32,050 --> 00:04:36,580
languages we support today and they and

00:04:34,780 --> 00:04:39,310
the target systems as you can see most

00:04:36,580 --> 00:04:41,500
of the Apache most popular Big Data

00:04:39,310 --> 00:04:44,319
frameworks out there so you have a spar

00:04:41,500 --> 00:04:49,780
flinging you have some Samsung gear pump

00:04:44,319 --> 00:04:52,930
all the suspects okay so how do we do

00:04:49,780 --> 00:04:55,389
that we do this with with what we call

00:04:52,930 --> 00:04:57,280
the concept of a pipeline the idea here

00:04:55,389 --> 00:05:01,030
is that we have an input that normally

00:04:57,280 --> 00:05:04,539
is distributed database or distributed

00:05:01,030 --> 00:05:06,940
log system like Kafka and with this

00:05:04,539 --> 00:05:09,789
input we just apply a series of

00:05:06,940 --> 00:05:12,550
transforms so we can do operation like

00:05:09,789 --> 00:05:14,949
map in MapReduce or we can do like

00:05:12,550 --> 00:05:17,020
reduce group IDs

00:05:14,949 --> 00:05:20,380
but also we can do time for Ian Tate

00:05:17,020 --> 00:05:22,360
operations on these streams but as you

00:05:20,380 --> 00:05:28,030
can see is just a direct attack cyclic

00:05:22,360 --> 00:05:30,099
graph so no not that is special so it's

00:05:28,030 --> 00:05:33,610
just remind this is the classical world

00:05:30,099 --> 00:05:36,370
count and what we are trying to do here

00:05:33,610 --> 00:05:38,289
is just to count words in a text and in

00:05:36,370 --> 00:05:40,570
classical Hadoop what we do is these

00:05:38,289 --> 00:05:44,320
three operations we do a map just to

00:05:40,570 --> 00:05:47,710
produce one one topple of word and

00:05:44,320 --> 00:05:49,840
number of currencies that is one then we

00:05:47,710 --> 00:05:52,960
use do a shuffle so we can have in

00:05:49,840 --> 00:05:57,159
different machine the same resource for

00:05:52,960 --> 00:05:58,930
each key this thing and then we reduce

00:05:57,159 --> 00:06:01,270
that in the in the destination machine

00:05:58,930 --> 00:06:04,949
so that's that's basically how MapReduce

00:06:01,270 --> 00:06:07,509
works in the case of beam we have two

00:06:04,949 --> 00:06:09,430
keywords to do this kind of operation we

00:06:07,509 --> 00:06:12,759
have map elements that is just a map

00:06:09,430 --> 00:06:15,610
that receives a map function and we have

00:06:12,759 --> 00:06:18,610
some by key integrals by key which just

00:06:15,610 --> 00:06:22,120
applies to some so that's that's not so

00:06:18,610 --> 00:06:25,810
different so if you want to see that in

00:06:22,120 --> 00:06:29,130
code what is happening we are reading a

00:06:25,810 --> 00:06:31,900
text file with what we call an i/o beam

00:06:29,130 --> 00:06:33,909
textile and we read this file then we

00:06:31,900 --> 00:06:36,669
have each line of the file and then we

00:06:33,909 --> 00:06:38,409
split those just by space with a flat

00:06:36,669 --> 00:06:41,260
map function that for each line produces

00:06:38,409 --> 00:06:43,410
multiple words then we produce the total

00:06:41,260 --> 00:06:46,770
that is a key value pair in

00:06:43,410 --> 00:06:48,810
in Beemer KETV and then we sum this and

00:06:46,770 --> 00:06:50,160
finally we output the stuff as you can

00:06:48,810 --> 00:06:53,100
see all these supplies are like the

00:06:50,160 --> 00:07:00,500
arrows that connect the things that the

00:06:53,100 --> 00:07:03,810
pipes sorry so but in reality beam has

00:07:00,500 --> 00:07:05,310
has like lower level operators or

00:07:03,810 --> 00:07:08,520
transforms that we call primitive

00:07:05,310 --> 00:07:10,140
transforms so this map elements is

00:07:08,520 --> 00:07:13,290
something that in beam we call pardhu

00:07:10,140 --> 00:07:15,990
and there is a more way way more

00:07:13,290 --> 00:07:19,170
advanced map function that includes

00:07:15,990 --> 00:07:21,360
other concept of beam we have this group

00:07:19,170 --> 00:07:23,790
by key that is just essentially grew by

00:07:21,360 --> 00:07:26,760
key and then we have that also just

00:07:23,790 --> 00:07:28,700
introduced in part why I am talking

00:07:26,760 --> 00:07:31,560
about these primitive transforms so

00:07:28,700 --> 00:07:34,880
because when you write the code that I'd

00:07:31,560 --> 00:07:37,850
show before it gets expanded into this

00:07:34,880 --> 00:07:39,870
so this is something that in end users

00:07:37,850 --> 00:07:41,520
normally don't see but this is an

00:07:39,870 --> 00:07:45,120
internal expansion of the transforms

00:07:41,520 --> 00:07:48,360
that everybody uses and why we have this

00:07:45,120 --> 00:07:50,190
primitive transform because for the end

00:07:48,360 --> 00:07:52,290
users you have all these available

00:07:50,190 --> 00:07:53,760
classes all these transforms and

00:07:52,290 --> 00:07:55,890
elements you can use to spread your

00:07:53,760 --> 00:07:59,270
computation and you have of course the

00:07:55,890 --> 00:08:02,190
classic expects like some kind combines

00:07:59,270 --> 00:08:05,250
that are reducers in beam and you have

00:08:02,190 --> 00:08:07,080
also map flatmap all these things but

00:08:05,250 --> 00:08:10,410
you also have the new ones in the right

00:08:07,080 --> 00:08:12,690
that are the ones that beam uses to

00:08:10,410 --> 00:08:15,000
adjust the time operation side show in

00:08:12,690 --> 00:08:18,000
the second slide so for windowing to

00:08:15,000 --> 00:08:19,560
define three girls and but if you see

00:08:18,000 --> 00:08:20,970
all of these would be too hard to

00:08:19,560 --> 00:08:22,920
translate if we are going to translate

00:08:20,970 --> 00:08:24,870
each one individual of those is like an

00:08:22,920 --> 00:08:28,050
impossible task especially if we have to

00:08:24,870 --> 00:08:31,680
target all these runners so that's why

00:08:28,050 --> 00:08:34,590
we have primitive transforms that are

00:08:31,680 --> 00:08:37,830
inside the model but we also have

00:08:34,590 --> 00:08:41,010
connectors to all the systems we have of

00:08:37,830 --> 00:08:43,640
course also all most of the Apache data

00:08:41,010 --> 00:08:48,960
stores we have Cassandra HBase

00:08:43,640 --> 00:08:50,850
Casca etc and of course also some of the

00:08:48,960 --> 00:08:52,320
cloud ones like of course why within

00:08:50,850 --> 00:08:53,940
Google because Google is the main

00:08:52,320 --> 00:08:56,230
contributor in this project and we have

00:08:53,940 --> 00:08:58,660
a good chunk of Amazon stuff

00:08:56,230 --> 00:09:02,770
and we support the classical form as

00:08:58,660 --> 00:09:11,410
avro park also such products so this

00:09:02,770 --> 00:09:17,140
this this this operators and I use in

00:09:11,410 --> 00:09:20,140
the end are just five core primitive

00:09:17,140 --> 00:09:22,810
transforms in beam are these five so we

00:09:20,140 --> 00:09:24,760
have the retransform that basically is

00:09:22,810 --> 00:09:26,440
going to read data in parallel so it's

00:09:24,760 --> 00:09:28,200
going to look for the partitions in data

00:09:26,440 --> 00:09:31,120
in the data stores and read the data

00:09:28,200 --> 00:09:33,550
quite essential one we have purdue that

00:09:31,120 --> 00:09:36,010
is just map per element processing but

00:09:33,550 --> 00:09:38,290
with some tricks we have grouped by key

00:09:36,010 --> 00:09:40,540
that in the case of beam has a special

00:09:38,290 --> 00:09:43,030
thing is that since we have this concept

00:09:40,540 --> 00:09:45,550
of windowing we need also to group by

00:09:43,030 --> 00:09:46,990
window so we group at key and windows so

00:09:45,550 --> 00:09:50,770
we have all the results in the right

00:09:46,990 --> 00:09:52,120
place we have flattened adjust union of

00:09:50,770 --> 00:09:55,360
collections that we have assigned

00:09:52,120 --> 00:09:57,610
windows that allows us to assign the

00:09:55,360 --> 00:10:00,370
elements into the correct windows they

00:09:57,610 --> 00:10:04,420
should be this is also to cover this

00:10:00,370 --> 00:10:08,230
trimming part so why I am talking about

00:10:04,420 --> 00:10:10,080
all of this because this is I'm going to

00:10:08,230 --> 00:10:12,730
show me essentially how the rawness work

00:10:10,080 --> 00:10:15,400
but in the case of a spark is going to

00:10:12,730 --> 00:10:17,560
be like that you just received a Java

00:10:15,400 --> 00:10:19,690
code and then there is this translation

00:10:17,560 --> 00:10:22,330
process that's going to create my

00:10:19,690 --> 00:10:24,580
classic RDD dot map blah blah blah dot

00:10:22,330 --> 00:10:27,330
map group by key all these spark

00:10:24,580 --> 00:10:30,400
operations and this is relatively

00:10:27,330 --> 00:10:33,700
straightforward to do because we are

00:10:30,400 --> 00:10:36,220
calling Java code and while all these

00:10:33,700 --> 00:10:37,990
target systems are Java based so it's

00:10:36,220 --> 00:10:43,000
just a question of wrapping stuff and

00:10:37,990 --> 00:10:45,340
calling it outside so in the end what we

00:10:43,000 --> 00:10:47,200
want to do is something like that of

00:10:45,340 --> 00:10:49,540
course this is not exactly how is in

00:10:47,200 --> 00:10:51,610
reality but just to give you an idea you

00:10:49,540 --> 00:10:55,270
can see in the down part the equivalent

00:10:51,610 --> 00:10:58,870
code of the word count we had in in beam

00:10:55,270 --> 00:11:01,780
but now in spark and as you can see is

00:10:58,870 --> 00:11:04,030
like it kinda translates directly now

00:11:01,780 --> 00:11:06,760
you have all these operators in in all

00:11:04,030 --> 00:11:08,480
these transforms in beam sorry that

00:11:06,760 --> 00:11:11,570
become this action

00:11:08,480 --> 00:11:13,760
and transformations in in spark so

00:11:11,570 --> 00:11:16,160
that's the general idea of what runners

00:11:13,760 --> 00:11:18,470
do and why you are talking about all of

00:11:16,160 --> 00:11:22,000
this because we are our goal is to run

00:11:18,470 --> 00:11:24,380
Python and go but as you see we have not

00:11:22,000 --> 00:11:28,940
we are not there yet we have all this

00:11:24,380 --> 00:11:30,110
Java stuff that Maps not so now I'm

00:11:28,940 --> 00:11:32,930
going to talk a little bit about this

00:11:30,110 --> 00:11:35,899
perp runner so of course most most

00:11:32,930 --> 00:11:38,269
people probably here now spark and why

00:11:35,899 --> 00:11:40,130
why spark is a destination system for

00:11:38,269 --> 00:11:42,860
being pipelines well first because it's

00:11:40,130 --> 00:11:45,019
the most popular a big data framework so

00:11:42,860 --> 00:11:46,699
something that if you do have a spark in

00:11:45,019 --> 00:11:48,769
your company you know what spark loss

00:11:46,699 --> 00:11:50,589
the Hadoop cluster probably that would

00:11:48,769 --> 00:11:52,699
be the easiest way to get into BIM

00:11:50,589 --> 00:11:55,610
second because the spark supports

00:11:52,699 --> 00:11:57,980
multiple resource managers so target

00:11:55,610 --> 00:11:59,149
systems so now even kubernetes so you

00:11:57,980 --> 00:12:00,829
can running Hadoop but also in

00:11:59,149 --> 00:12:02,600
kubernetes so that's a great thing to

00:12:00,829 --> 00:12:04,910
have also because there is a rich

00:12:02,600 --> 00:12:07,820
ecosystem of tools in a spark does

00:12:04,910 --> 00:12:09,110
something that most people I mean don't

00:12:07,820 --> 00:12:10,760
consider that much but is super

00:12:09,110 --> 00:12:13,130
important we have seen projects like

00:12:10,760 --> 00:12:16,250
levy that you can use to submit

00:12:13,130 --> 00:12:21,170
pipelines in a REST API or tools to

00:12:16,250 --> 00:12:23,839
monitor like creepiest barbar so spark

00:12:21,170 --> 00:12:25,490
also has good tooling around and all of

00:12:23,839 --> 00:12:27,470
course because it is running in your

00:12:25,490 --> 00:12:30,019
company you have some super contract

00:12:27,470 --> 00:12:32,540
with one of these vendors well as

00:12:30,019 --> 00:12:36,260
somehow obliged to live with respect to

00:12:32,540 --> 00:12:39,470
so and even now there is a service offer

00:12:36,260 --> 00:12:43,880
by data bricks that we also have tested

00:12:39,470 --> 00:12:45,589
with Beaman and works correctly so this

00:12:43,880 --> 00:12:48,290
part one as I said at the beginning was

00:12:45,589 --> 00:12:50,930
donated back clue theorem and it

00:12:48,290 --> 00:12:52,910
originally was based in an RDD and the

00:12:50,930 --> 00:12:56,540
stream translation that was the older

00:12:52,910 --> 00:12:58,790
API for spark at this moment there is an

00:12:56,540 --> 00:13:01,490
ongoing working to a new runner based on

00:12:58,790 --> 00:13:03,560
the newer api's but in for the sake of

00:13:01,490 --> 00:13:05,899
this presentation will focus on RDD on

00:13:03,560 --> 00:13:07,670
this stream is one of the most mature

00:13:05,899 --> 00:13:10,519
runners let's say there's been battle

00:13:07,670 --> 00:13:14,600
tested somehow by buyers people in all

00:13:10,519 --> 00:13:17,480
these companies and and there are really

00:13:14,600 --> 00:13:20,110
interesting use cases already that of

00:13:17,480 --> 00:13:22,339
users of the runner especially I mean

00:13:20,110 --> 00:13:25,059
this system group from

00:13:22,339 --> 00:13:27,649
the public check they are like the an

00:13:25,059 --> 00:13:29,990
insertion gene and they index a lot of

00:13:27,649 --> 00:13:34,600
data there we have this G beef people

00:13:29,990 --> 00:13:37,220
who also do do have is the name of that

00:13:34,600 --> 00:13:43,730
biology observation of a species based

00:13:37,220 --> 00:13:46,329
on that so cool use cases okay so I am

00:13:43,730 --> 00:13:48,980
just going to mention this quickly

00:13:46,329 --> 00:13:51,139
because I I would probably you will

00:13:48,980 --> 00:13:53,120
probably see this after well says these

00:13:51,139 --> 00:13:55,189
are the operations in a spark basically

00:13:53,120 --> 00:13:57,079
spark also represents the date has a

00:13:55,189 --> 00:14:00,470
distributed collection that they call

00:13:57,079 --> 00:14:02,480
RDD and with this oddity you can do you

00:14:00,470 --> 00:14:04,339
can do transformations of the data that

00:14:02,480 --> 00:14:06,980
there is there and and actions that

00:14:04,339 --> 00:14:09,559
materialize this data if you see these

00:14:06,980 --> 00:14:11,749
concepts maps with with the with the

00:14:09,559 --> 00:14:14,360
beam concepts and they also have this

00:14:11,749 --> 00:14:18,139
concept of broadcast variables that is

00:14:14,360 --> 00:14:20,959
there are read-only state let's say that

00:14:18,139 --> 00:14:22,879
you can get into each worker and we have

00:14:20,959 --> 00:14:26,809
these contexts that allows you to

00:14:22,879 --> 00:14:28,279
connect all this stuff together why I

00:14:26,809 --> 00:14:31,129
mention this because we are going to see

00:14:28,279 --> 00:14:35,029
now how we translate from beam to spark

00:14:31,129 --> 00:14:37,100
in the general way and this is important

00:14:35,029 --> 00:14:39,110
because this is like the first step that

00:14:37,100 --> 00:14:41,779
we have with the runner and we'll

00:14:39,110 --> 00:14:45,170
connect with the new translation for

00:14:41,779 --> 00:14:47,779
Python and go so as we mentioned before

00:14:45,170 --> 00:14:51,589
we had these five primitives so we are

00:14:47,779 --> 00:14:54,949
going to go into each one quickly the

00:14:51,589 --> 00:14:56,420
first primitive is great so as I said

00:14:54,949 --> 00:14:58,879
what we were trying to do is just to

00:14:56,420 --> 00:15:04,149
read each partition in parallel and and

00:14:58,879 --> 00:15:07,790
we have this computation executed I mean

00:15:04,149 --> 00:15:10,879
it are ready to be computed sorry there

00:15:07,790 --> 00:15:13,370
is one hidden C internal secret of beam

00:15:10,879 --> 00:15:17,089
is that every LM every element on beam

00:15:13,370 --> 00:15:18,920
is mapped or lift into a window value

00:15:17,089 --> 00:15:20,990
representation why is this because I

00:15:18,920 --> 00:15:23,179
said that we should have always a

00:15:20,990 --> 00:15:25,160
timestamp we have the original value

00:15:23,179 --> 00:15:27,259
that you are play with all this is

00:15:25,160 --> 00:15:29,089
internal is just to give you an idea of

00:15:27,259 --> 00:15:31,339
how it works and that you can measure

00:15:29,089 --> 00:15:34,339
also they all the the miner overhead

00:15:31,339 --> 00:15:35,220
there is there we have also the windows

00:15:34,339 --> 00:15:38,910
this element

00:15:35,220 --> 00:15:43,770
belongs to and pain information - for

00:15:38,910 --> 00:15:45,540
material for motivation and say what we

00:15:43,770 --> 00:15:48,360
do in read actually is just to create a

00:15:45,540 --> 00:15:51,150
job an RDD this exactly how now didi

00:15:48,360 --> 00:15:53,550
works these has animated get partition

00:15:51,150 --> 00:15:56,940
so we are going to map the grid function

00:15:53,550 --> 00:15:58,350
from beam into their get partitions

00:15:56,940 --> 00:16:01,200
method of spark so it's pretty

00:15:58,350 --> 00:16:04,530
straightforward we have them to

00:16:01,200 --> 00:16:07,140
translate Purdue Purdue is more

00:16:04,530 --> 00:16:10,950
complicated function Purdue in beam has

00:16:07,140 --> 00:16:13,380
life cycle and well if we translate

00:16:10,950 --> 00:16:15,720
Purdue we have to respect like this that

00:16:13,380 --> 00:16:18,810
laser cycle we have also the conceived

00:16:15,720 --> 00:16:21,810
of bundles and also we have to to to

00:16:18,810 --> 00:16:23,250
decide how we bundle the elements a

00:16:21,810 --> 00:16:25,860
bundle is like a little group of

00:16:23,250 --> 00:16:27,660
elements that can be small for example

00:16:25,860 --> 00:16:30,870
in the case of streaming because we want

00:16:27,660 --> 00:16:32,670
to produce data downstream all as fast

00:16:30,870 --> 00:16:35,280
as possible or can be larger in the case

00:16:32,670 --> 00:16:37,740
of batch we have all this stuff like

00:16:35,280 --> 00:16:42,300
matrix state timers it's like a big

00:16:37,740 --> 00:16:45,450
Swiss Army knife of beam I need this

00:16:42,300 --> 00:16:48,900
part is basically this is basically a

00:16:45,450 --> 00:16:52,440
big flat map and some other maps minor

00:16:48,900 --> 00:16:55,950
maps traduce things not partitions

00:16:52,440 --> 00:16:59,160
especially and also use broadcast to get

00:16:55,950 --> 00:17:00,870
this side inputs that can be collections

00:16:59,160 --> 00:17:06,949
that you need to compute things for

00:17:00,870 --> 00:17:09,900
example like for joints that we have to

00:17:06,949 --> 00:17:13,199
move thing to spark is group by key as I

00:17:09,900 --> 00:17:17,010
mentioned we have Mikey and by the

00:17:13,199 --> 00:17:19,829
window so the cleaver hack here is just

00:17:17,010 --> 00:17:22,319
that we add the wing the window to the

00:17:19,829 --> 00:17:24,959
key represent that as bytes and thus the

00:17:22,319 --> 00:17:28,230
new key and that's mostly what it does

00:17:24,959 --> 00:17:30,870
but group by key also has to take care

00:17:28,230 --> 00:17:33,210
of all these time stamp based semantics

00:17:30,870 --> 00:17:36,030
so he has what just times you have to

00:17:33,210 --> 00:17:37,890
remove drop lay data because we don't

00:17:36,030 --> 00:17:40,560
want to shuffle more data than we should

00:17:37,890 --> 00:17:42,230
and you have to decide also how we're

00:17:40,560 --> 00:17:44,640
going to triggers the results

00:17:42,230 --> 00:17:51,080
essentially all of this is done by some

00:17:44,640 --> 00:17:51,080
maps add a map and then by the group by

00:17:52,429 --> 00:17:55,740
finally no I don't know we have another

00:17:55,139 --> 00:17:59,159
one

00:17:55,740 --> 00:18:01,409
this windows is the simplest one let's

00:17:59,159 --> 00:18:05,159
say we are just going to put one element

00:18:01,409 --> 00:18:09,389
into multiple windows so basically some

00:18:05,159 --> 00:18:11,370
is a malfunction to that does this why

00:18:09,389 --> 00:18:13,200
this because well as you defined the

00:18:11,370 --> 00:18:17,850
window strategy that that's something

00:18:13,200 --> 00:18:19,320
you may need finally the flat in

00:18:17,850 --> 00:18:21,600
transform is you say union of spec

00:18:19,320 --> 00:18:23,370
collections though this is quite

00:18:21,600 --> 00:18:25,380
straightforward to translate in a spark

00:18:23,370 --> 00:18:27,179
because we already have rdd's of the

00:18:25,380 --> 00:18:29,340
same type and Mintzberg there is this

00:18:27,179 --> 00:18:34,409
Union operation so translation is just

00:18:29,340 --> 00:18:37,080
an unit cool so we have finally do the

00:18:34,409 --> 00:18:39,389
complete translation but the key part is

00:18:37,080 --> 00:18:42,029
that we are calling Java code from Java

00:18:39,389 --> 00:18:43,710
so this is straightforward more more

00:18:42,029 --> 00:18:45,510
concretely we have this user-defined

00:18:43,710 --> 00:18:48,240
functions is the most critical part of

00:18:45,510 --> 00:18:50,789
the of the code that we end up putting

00:18:48,240 --> 00:18:53,789
into the executor machines thanks to a

00:18:50,789 --> 00:18:56,100
spark but what will happen if we need to

00:18:53,789 --> 00:18:58,110
do now this with Python or would go

00:18:56,100 --> 00:19:05,490
so now Kyle is going to talk with you

00:18:58,110 --> 00:19:08,580
about that Thanks ok so yeah so that was

00:19:05,490 --> 00:19:12,990
the spark Runner basically as it existed

00:19:08,580 --> 00:19:17,970
until earlier this year it worked well

00:19:12,990 --> 00:19:20,580
for everything Java but and beam one of

00:19:17,970 --> 00:19:22,799
the like s mail said we have these two

00:19:20,580 --> 00:19:24,600
levels of portability right so we had

00:19:22,799 --> 00:19:27,210
all of these we had all of these

00:19:24,600 --> 00:19:29,130
different execution engines we had we

00:19:27,210 --> 00:19:31,529
had a spark we had flink we had cloud

00:19:29,130 --> 00:19:34,200
data flown a whole bunch of others but

00:19:31,529 --> 00:19:37,139
then we also add additional SDKs which

00:19:34,200 --> 00:19:39,210
are different implementations of the

00:19:37,139 --> 00:19:44,010
beam model in different languages so

00:19:39,210 --> 00:19:46,110
Java Python and then go is another SDK

00:19:44,010 --> 00:19:49,019
that's been added recently so then the

00:19:46,110 --> 00:19:51,090
question becomes how are we going to

00:19:49,019 --> 00:19:53,039
then support all of these different

00:19:51,090 --> 00:19:57,750
languages on all of these different

00:19:53,039 --> 00:19:58,260
runners so that that is a tricky

00:19:57,750 --> 00:20:02,100
question

00:19:58,260 --> 00:20:02,520
so with spark of course you may know

00:20:02,100 --> 00:20:04,200
that there

00:20:02,520 --> 00:20:08,550
something called PI spark which is

00:20:04,200 --> 00:20:11,580
sparks Python integration so the your

00:20:08,550 --> 00:20:13,950
first thought when translating from beam

00:20:11,580 --> 00:20:16,490
to spark might be well why don't we just

00:20:13,950 --> 00:20:18,810
use PI spark and so this is kind of like

00:20:16,490 --> 00:20:22,170
maybe a little example of what that

00:20:18,810 --> 00:20:25,190
might look like but the problem there

00:20:22,170 --> 00:20:28,830
are a few problems with that first off

00:20:25,190 --> 00:20:31,920
doing this for every target language and

00:20:28,830 --> 00:20:34,710
runtime is completely not scalable if

00:20:31,920 --> 00:20:37,740
you multiply the amount of SDKs we have

00:20:34,710 --> 00:20:39,630
by the number of execution engines we

00:20:37,740 --> 00:20:42,090
have it just became completely

00:20:39,630 --> 00:20:45,990
unrealistic to build and maintain all of

00:20:42,090 --> 00:20:48,330
that the second thing is that some

00:20:45,990 --> 00:20:52,470
target systems don't even have the

00:20:48,330 --> 00:20:54,540
targeted API so for example with spark

00:20:52,470 --> 00:20:59,610
we have PI spark but then what about go

00:20:54,540 --> 00:21:01,800
there's no go spark so that would just

00:20:59,610 --> 00:21:05,460
be impossible there would be no way to

00:21:01,800 --> 00:21:07,620
do that so and then for that was also

00:21:05,460 --> 00:21:10,470
the case for example with flank which

00:21:07,620 --> 00:21:13,320
did not have Python support I believe

00:21:10,470 --> 00:21:16,380
they're adding it but in the past it

00:21:13,320 --> 00:21:21,930
didn't so the question becomes how do we

00:21:16,380 --> 00:21:23,730
do this and the answer is basically the

00:21:21,930 --> 00:21:28,800
subject of most of the rest of this talk

00:21:23,730 --> 00:21:31,500
the beam portability API so this is a

00:21:28,800 --> 00:21:35,250
well defined language agnostic way to

00:21:31,500 --> 00:21:37,950
represent and execute beam pipelines so

00:21:35,250 --> 00:21:41,100
basically what that means the key word

00:21:37,950 --> 00:21:44,610
here is language agnostic so this is

00:21:41,100 --> 00:21:51,210
gonna work for all of the SDKs and it

00:21:44,610 --> 00:21:53,160
will scale and then we also need to have

00:21:51,210 --> 00:21:55,560
a bunch of infrastructure related to

00:21:53,160 --> 00:22:00,240
this that we can reuse which I'll talk

00:21:55,560 --> 00:22:02,430
about in a minute in order to get things

00:22:00,240 --> 00:22:04,800
running on a language specific

00:22:02,430 --> 00:22:08,760
environment and then there are bunch of

00:22:04,800 --> 00:22:10,890
challenges with us - some of which we

00:22:08,760 --> 00:22:14,740
have pretty much resolved and others we

00:22:10,890 --> 00:22:17,140
are still working on but a basic idea is

00:22:14,740 --> 00:22:20,289
so we have to we have to have an

00:22:17,140 --> 00:22:22,659
execution environment for this stuff if

00:22:20,289 --> 00:22:27,010
you know about spark you'll know that

00:22:22,659 --> 00:22:29,440
it's it's basically all JVM based and

00:22:27,010 --> 00:22:31,720
even things like PI spark are often

00:22:29,440 --> 00:22:33,669
going to use things like Jai thon which

00:22:31,720 --> 00:22:38,080
are still JVM based even though they're

00:22:33,669 --> 00:22:41,020
Python so for for doing languages like

00:22:38,080 --> 00:22:43,630
go this becomes a little more tricky

00:22:41,020 --> 00:22:45,580
because there is no again no go

00:22:43,630 --> 00:22:49,990
integration with Java as far as I'm

00:22:45,580 --> 00:22:52,029
aware so the challenge then is we need

00:22:49,990 --> 00:22:56,409
to support multiple languages we need to

00:22:52,029 --> 00:22:59,890
somehow run this Python go or whatever

00:22:56,409 --> 00:23:02,830
it may be on this in this spark

00:22:59,890 --> 00:23:04,390
environment and then there's also some

00:23:02,830 --> 00:23:07,120
other things to consider like of course

00:23:04,390 --> 00:23:10,059
performance is something that everyone

00:23:07,120 --> 00:23:12,789
is naturally always worried about and we

00:23:10,059 --> 00:23:16,360
need to make it easy to evolve easy to

00:23:12,789 --> 00:23:19,450
scale not too much of a pain to develop

00:23:16,360 --> 00:23:22,120
and another thing that I might add to

00:23:19,450 --> 00:23:23,770
this is we need to keep the operational

00:23:22,120 --> 00:23:25,899
complexity to a reasonable minimum

00:23:23,770 --> 00:23:27,789
because as you'll see here in a minute

00:23:25,899 --> 00:23:32,260
it gets it gets a little bit complicated

00:23:27,789 --> 00:23:37,980
so so basically the the way this works

00:23:32,260 --> 00:23:42,809
is we're going to we're going to end up

00:23:37,980 --> 00:23:45,640
using the this concept of an SDK worker

00:23:42,809 --> 00:23:48,490
which all elaborate on here in a second

00:23:45,640 --> 00:23:52,659
but basically we're going to shell out

00:23:48,490 --> 00:23:57,610
the work of doing user code to this SDK

00:23:52,659 --> 00:23:59,770
worker yeah so this is again what I

00:23:57,610 --> 00:24:04,029
talked about a little bit just now but

00:23:59,770 --> 00:24:06,250
basically we had Java and Scala and

00:24:04,029 --> 00:24:09,789
sequel all running on all of these

00:24:06,250 --> 00:24:13,240
runners which was great but then earlier

00:24:09,789 --> 00:24:16,120
we only had Python and go running on

00:24:13,240 --> 00:24:22,570
data flow and flank and not all of these

00:24:16,120 --> 00:24:26,529
other things so inter the portable SPARC

00:24:22,570 --> 00:24:28,420
runner so this is very similar to what

00:24:26,529 --> 00:24:32,559
is male just talking about with

00:24:28,420 --> 00:24:36,010
existing spark Runner but there are some

00:24:32,559 --> 00:24:39,309
key differences as we will see but a lot

00:24:36,010 --> 00:24:42,330
of this I have to give credit to the

00:24:39,309 --> 00:24:46,000
folks who made the flink portable Runner

00:24:42,330 --> 00:24:49,150
because flink again as I mentioned the

00:24:46,000 --> 00:24:53,290
last slide was the first open source

00:24:49,150 --> 00:24:56,559
runner to use this new portability

00:24:53,290 --> 00:24:58,929
framework so they got Python working

00:24:56,559 --> 00:25:01,570
first and we were able to leverage a lot

00:24:58,929 --> 00:25:04,900
of that work to get the spark Runner

00:25:01,570 --> 00:25:07,000
working and it's basically proves the

00:25:04,900 --> 00:25:10,690
concept of portability that like yes

00:25:07,000 --> 00:25:14,500
this is tenable to develop this for all

00:25:10,690 --> 00:25:16,870
of our desired execution engines so it's

00:25:14,500 --> 00:25:19,059
like I said it's going to use I'll reuse

00:25:16,870 --> 00:25:24,549
a lot of stuff that I'll talked about

00:25:19,059 --> 00:25:26,799
earlier and it is still based on RTD and

00:25:24,549 --> 00:25:29,110
for now it is batch only although

00:25:26,799 --> 00:25:31,059
streaming support may be coming in the

00:25:29,110 --> 00:25:35,410
future we will we will see about that

00:25:31,059 --> 00:25:37,900
and it's been around since the beam 2.14

00:25:35,410 --> 00:25:40,030
release and I believe we're on that 2.16

00:25:37,900 --> 00:25:45,790
release now so it's still relatively new

00:25:40,030 --> 00:25:48,450
but yeah so this is this is the same 5

00:25:45,790 --> 00:25:53,140
transforms that we talked about earlier

00:25:48,450 --> 00:25:55,929
and out of these there are at least two

00:25:53,140 --> 00:25:58,200
that are pretty much the same those

00:25:55,929 --> 00:26:01,390
being grouped by key and flattened so

00:25:58,200 --> 00:26:03,130
the implementations of group I can

00:26:01,390 --> 00:26:05,860
flatten that he talked about are pretty

00:26:03,130 --> 00:26:09,070
much the same and the new affordable

00:26:05,860 --> 00:26:11,559
runner but then we also have a few

00:26:09,070 --> 00:26:13,929
differences with Reed which I won't go

00:26:11,559 --> 00:26:16,390
into because that's a whole talk unto

00:26:13,929 --> 00:26:19,840
itself the main thing I want to focus on

00:26:16,390 --> 00:26:24,820
is the pardhu which is basically how do

00:26:19,840 --> 00:26:28,360
we execute the user's arbitrary code and

00:26:24,820 --> 00:26:32,320
their transforms how do we run that and

00:26:28,360 --> 00:26:36,610
so that is that is the big question and

00:26:32,320 --> 00:26:38,800
so the answer is well first first things

00:26:36,610 --> 00:26:41,590
first first we need to be able to

00:26:38,800 --> 00:26:44,529
represent a beam pipeline

00:26:41,590 --> 00:26:46,600
language agnostic way and so the the way

00:26:44,529 --> 00:26:48,640
we do that is with protocol buffers

00:26:46,600 --> 00:26:50,830
which is you may know we use for many

00:26:48,640 --> 00:26:54,039
many things at Google but basically

00:26:50,830 --> 00:26:56,799
protocol buffer so our pipeline becomes

00:26:54,039 --> 00:27:01,020
a protocol buffer and then we are going

00:26:56,799 --> 00:27:04,299
to so we so we have all these api's here

00:27:01,020 --> 00:27:08,200
three api's are listed these are all g

00:27:04,299 --> 00:27:11,460
RPC and protocol buffer based so we are

00:27:08,200 --> 00:27:13,960
going to first translate the pipeline

00:27:11,460 --> 00:27:16,990
and to a protocol buffer we're gonna

00:27:13,960 --> 00:27:21,010
submit it to this thing here that we

00:27:16,990 --> 00:27:25,179
call a job server and then we are going

00:27:21,010 --> 00:27:28,600
to then from there we are going to

00:27:25,179 --> 00:27:33,760
translate the pipeline into spark and

00:27:28,600 --> 00:27:37,539
run it on spark and so this is kind of

00:27:33,760 --> 00:27:39,610
an expansion of the previous slide this

00:27:37,539 --> 00:27:43,600
is what happens after we submit it to

00:27:39,610 --> 00:27:46,029
spark so you'll see here that I've coded

00:27:43,600 --> 00:27:48,549
things as language specific and language

00:27:46,029 --> 00:27:50,679
agnostic so basically what would happen

00:27:48,549 --> 00:27:53,620
is you would submit your pipe on Python

00:27:50,679 --> 00:27:54,970
pipeline and then it would go to the job

00:27:53,620 --> 00:27:57,039
server and then it's going to go to

00:27:54,970 --> 00:28:01,390
spark and basically what spark will do

00:27:57,039 --> 00:28:04,480
is when it sees a part to function it

00:28:01,390 --> 00:28:07,840
will then shell out to these SDK

00:28:04,480 --> 00:28:11,110
harnesses via another one of our api's

00:28:07,840 --> 00:28:16,120
which again is mostly G RPC and protocol

00:28:11,110 --> 00:28:18,610
buffer based so that's the are so called

00:28:16,120 --> 00:28:21,580
executor will stage function that's

00:28:18,610 --> 00:28:24,370
going to run user transforms it is

00:28:21,580 --> 00:28:27,909
basically just a very very fancy spark

00:28:24,370 --> 00:28:31,179
flatmap function and it's going to

00:28:27,909 --> 00:28:34,690
handle all of your user code whether

00:28:31,179 --> 00:28:38,770
it's Python or Co or whatever and so

00:28:34,690 --> 00:28:41,049
what will happen is we have a few

00:28:38,770 --> 00:28:43,179
special things for side inputs again

00:28:41,049 --> 00:28:46,210
still going to use spark broadcast

00:28:43,179 --> 00:28:49,210
variables as was mentioned earlier and

00:28:46,210 --> 00:28:51,580
then there's going to be some special

00:28:49,210 --> 00:28:53,470
stuff with windowing and watermarks but

00:28:51,580 --> 00:28:55,240
that's pretty straightforward because

00:28:53,470 --> 00:28:58,600
like I said we're only doing batch for

00:28:55,240 --> 00:29:02,429
and most of that kind of stuff is useful

00:28:58,600 --> 00:29:07,360
more for screaming so we're going to

00:29:02,429 --> 00:29:09,309
essentially send it from here from the

00:29:07,360 --> 00:29:13,090
spark flatmap function we are going to

00:29:09,309 --> 00:29:17,080
send all of our work requests and input

00:29:13,090 --> 00:29:18,880
data to the SDK harness which I've

00:29:17,080 --> 00:29:21,820
mentioned a few times and that's going

00:29:18,880 --> 00:29:27,070
to actually execute the user code so the

00:29:21,820 --> 00:29:29,580
SDK harness is a say rather involved

00:29:27,070 --> 00:29:33,900
piece of machinery so shall I say

00:29:29,580 --> 00:29:36,280
there's a little basic diagram here of

00:29:33,900 --> 00:29:39,550
how it works but essentially what it's

00:29:36,280 --> 00:29:42,190
doing is it's going to going to send all

00:29:39,550 --> 00:29:45,010
of its data from spark to the SDK

00:29:42,190 --> 00:29:46,390
harness and then all the works going to

00:29:45,010 --> 00:29:50,110
happen and then that's going to send

00:29:46,390 --> 00:29:52,540
back a result essentially so this is

00:29:50,110 --> 00:29:54,360
where there's a little bit of

00:29:52,540 --> 00:29:57,400
configuration to be done by the user

00:29:54,360 --> 00:29:59,110
because the SDK harness can run in a

00:29:57,400 --> 00:30:02,350
number of different places we wanted to

00:29:59,110 --> 00:30:05,440
give users the flexibility to do this

00:30:02,350 --> 00:30:07,420
and different setups so the main way in

00:30:05,440 --> 00:30:10,570
the perk kind of the preferred way is to

00:30:07,420 --> 00:30:14,410
run it in a docker container and we are

00:30:10,570 --> 00:30:17,200
now providing container base images that

00:30:14,410 --> 00:30:20,500
contain all of the beam stuff that it

00:30:17,200 --> 00:30:22,420
needs to run transforms so basically you

00:30:20,500 --> 00:30:24,160
just have to add in your dependencies

00:30:22,420 --> 00:30:28,270
like your Python packages or whatever

00:30:24,160 --> 00:30:30,700
for example but then that will take care

00:30:28,270 --> 00:30:33,730
of the rest for you there's also a few

00:30:30,700 --> 00:30:37,950
other options for different deployment

00:30:33,730 --> 00:30:43,120
strategies but doctors the main one so

00:30:37,950 --> 00:30:46,030
so this runner we've we've tested it we

00:30:43,120 --> 00:30:49,120
just we just wanted to point that out so

00:30:46,030 --> 00:30:51,490
basically when you submit a pr2 beam

00:30:49,120 --> 00:30:53,530
that's going to be validated now on both

00:30:51,490 --> 00:30:56,650
the classic and portable runners and we

00:30:53,530 --> 00:30:59,200
also have go integration tests which is

00:30:56,650 --> 00:31:01,570
cool so runs on go to and now go

00:30:59,200 --> 00:31:03,640
integration was basically free because

00:31:01,570 --> 00:31:06,669
once you once you have all of this

00:31:03,640 --> 00:31:08,350
portability stuff in place it like I

00:31:06,669 --> 00:31:08,740
said it becomes a lot more scalable you

00:31:08,350 --> 00:31:11,920
can do

00:31:08,740 --> 00:31:15,040
stat on new languages so if anyone wants

00:31:11,920 --> 00:31:19,840
to write a new SDK provided they keep

00:31:15,040 --> 00:31:22,120
the whole beam SDK contract it will be

00:31:19,840 --> 00:31:24,730
able to just work with us basically

00:31:22,120 --> 00:31:29,140
without any effort on our part which is

00:31:24,730 --> 00:31:32,770
great yeah so for a little bit of an

00:31:29,140 --> 00:31:35,590
idea of how this might be used I'd like

00:31:32,770 --> 00:31:40,300
to bring up the example of tensorflow

00:31:35,590 --> 00:31:41,200
extended which is a sub project of

00:31:40,300 --> 00:31:44,170
tensorflow

00:31:41,200 --> 00:31:47,610
which I'm sure pretty much everyone here

00:31:44,170 --> 00:31:51,550
sort of but basically the idea of

00:31:47,610 --> 00:31:54,250
tensorflow extended is it's going to

00:31:51,550 --> 00:31:58,360
take care of a complete end-to-end

00:31:54,250 --> 00:32:00,610
machine learning pipeline so where's

00:31:58,360 --> 00:32:03,370
tensorflow itself is mostly concerned

00:32:00,610 --> 00:32:06,730
with just the training of an ml model

00:32:03,370 --> 00:32:10,840
tensorflow extended is going to be the

00:32:06,730 --> 00:32:14,260
whole pipeline and we use this within

00:32:10,840 --> 00:32:16,570
Google quite a bit and it's also started

00:32:14,260 --> 00:32:21,820
to get used by some of our partners in

00:32:16,570 --> 00:32:25,390
industry so basically T FX pipelines are

00:32:21,820 --> 00:32:27,400
going to be a set of components which

00:32:25,390 --> 00:32:31,630
I'll have a diagram of in a second but

00:32:27,400 --> 00:32:33,580
basically you are free to use T FX and a

00:32:31,630 --> 00:32:35,290
complete pipeline or if you just have

00:32:33,580 --> 00:32:37,740
pieces of it that you would like to use

00:32:35,290 --> 00:32:41,590
you can also use them individually so

00:32:37,740 --> 00:32:45,130
kind of cool and then beam beam is

00:32:41,590 --> 00:32:47,530
really what powers T FX hence why I

00:32:45,130 --> 00:32:50,050
brought it up in this talk because a lot

00:32:47,530 --> 00:32:53,320
of the T FX components are actually

00:32:50,050 --> 00:32:55,210
based on beam so much so that um in a

00:32:53,320 --> 00:32:58,120
previous presentation I think this

00:32:55,210 --> 00:33:00,100
summer here in Berlin there is this

00:32:58,120 --> 00:33:02,950
quote without being we wouldn't have an

00:33:00,100 --> 00:33:04,929
open-source T FX because it's um it's

00:33:02,950 --> 00:33:08,309
really just what makes that possible so

00:33:04,929 --> 00:33:09,690
for as an idea here we have all of these

00:33:08,309 --> 00:33:13,240
tensorflow

00:33:09,690 --> 00:33:17,230
extended components so we have data

00:33:13,240 --> 00:33:21,160
validation we have transforms and then

00:33:17,230 --> 00:33:22,400
we have model analysis and all of all of

00:33:21,160 --> 00:33:24,170
these oh sorry

00:33:22,400 --> 00:33:28,340
data ingestion - so all of these are

00:33:24,170 --> 00:33:30,410
based on beam these are all libraries in

00:33:28,340 --> 00:33:32,809
tensorflow extended but they all use

00:33:30,410 --> 00:33:35,120
beam under the hood to provide a lot of

00:33:32,809 --> 00:33:37,280
flexibility and this is really important

00:33:35,120 --> 00:33:41,570
because like I said we do this in Google

00:33:37,280 --> 00:33:44,690
and we also offer it as an open source

00:33:41,570 --> 00:33:46,610
product so that means that it's

00:33:44,690 --> 00:33:48,950
important that it runs both on Google's

00:33:46,610 --> 00:33:51,170
own proprietary infrastructure as well

00:33:48,950 --> 00:33:53,030
as whatever kind of setup you might

00:33:51,170 --> 00:33:56,090
happen to have at your company or

00:33:53,030 --> 00:33:59,240
organization and so in order to do that

00:33:56,090 --> 00:34:01,130
we use beam because again it is portable

00:33:59,240 --> 00:34:05,630
between all of these different execution

00:34:01,130 --> 00:34:09,139
engines so the the thing also about tf-x

00:34:05,630 --> 00:34:12,200
is that it is all Python libraries so

00:34:09,139 --> 00:34:14,750
when we wanted to run it on SPARC before

00:34:12,200 --> 00:34:16,639
we couldn't because it only supported

00:34:14,750 --> 00:34:19,909
Java but now that we support Python as

00:34:16,639 --> 00:34:24,080
well you are able to run all of these

00:34:19,909 --> 00:34:26,840
tf-x libraries without modification on a

00:34:24,080 --> 00:34:29,090
smart cluster which is great because no

00:34:26,840 --> 00:34:31,790
one wants to have to rewrite a library

00:34:29,090 --> 00:34:37,250
that is this involved for every

00:34:31,790 --> 00:34:39,470
execution engine so just a little bit an

00:34:37,250 --> 00:34:45,020
idea where we're going with this whole

00:34:39,470 --> 00:34:47,659
project we have a few more features to

00:34:45,020 --> 00:34:50,149
implement it is mostly feature complete

00:34:47,659 --> 00:34:51,260
with the essential features of the B

00:34:50,149 --> 00:34:52,820
model but there are some new

00:34:51,260 --> 00:34:55,940
cutting-edge features we would like to

00:34:52,820 --> 00:35:00,890
add streaming support is the big one and

00:34:55,940 --> 00:35:04,550
both there yeah so we do have a

00:35:00,890 --> 00:35:06,710
streaming support for ver Python on

00:35:04,550 --> 00:35:09,800
flink but we haven't added it to SPARC

00:35:06,710 --> 00:35:13,070
yet so maybe we will get there at some

00:35:09,800 --> 00:35:15,130
point but and then performance analysis

00:35:13,070 --> 00:35:19,310
is something everyone always asks about

00:35:15,130 --> 00:35:21,430
naturally and we are working on that and

00:35:19,310 --> 00:35:25,810
then of course better documentation

00:35:21,430 --> 00:35:25,810
probably could be said of anything but

00:35:26,080 --> 00:35:31,340
yeah we also want to make some usability

00:35:29,390 --> 00:35:36,170
improvement so that it's easy for people

00:35:31,340 --> 00:35:38,390
to get up and running with us as as you

00:35:36,170 --> 00:35:40,210
might have noticed from the previous

00:35:38,390 --> 00:35:42,640
slides there is a little bit of

00:35:40,210 --> 00:35:44,960
complexity added with this whole

00:35:42,640 --> 00:35:47,930
portability infrastructure so we want to

00:35:44,960 --> 00:35:51,500
make sure that that is as seamless as

00:35:47,930 --> 00:35:52,970
possible from end users so they don't

00:35:51,500 --> 00:35:57,070
have to worry about that stuff as much

00:35:52,970 --> 00:35:59,840
as we possibly can and we'd also like to

00:35:57,070 --> 00:36:01,550
kind of document how to use this and

00:35:59,840 --> 00:36:06,980
cloud providers another thing we're

00:36:01,550 --> 00:36:09,260
working on so yeah so if you are

00:36:06,980 --> 00:36:11,750
interested then we have the link here

00:36:09,260 --> 00:36:18,380
for the landing page for the spark

00:36:11,750 --> 00:36:21,230
runner and we we eagerly anticipate your

00:36:18,380 --> 00:36:25,450
code ideas documentation x' test bug

00:36:21,230 --> 00:36:28,850
reports complaints opinions anything

00:36:25,450 --> 00:36:31,640
yeah so you also can subscribe to our

00:36:28,850 --> 00:36:34,310
user and dev lists and we'll be happy to

00:36:31,640 --> 00:36:38,360
answer any questions there or you can

00:36:34,310 --> 00:36:40,600
join the slack channel so yeah thanks

00:36:38,360 --> 00:36:40,600
everyone

00:36:47,910 --> 00:36:53,480
Oh any question anyone

00:36:57,900 --> 00:37:03,180
as far as I know beam is still in the

00:37:00,630 --> 00:37:05,130
preview mode or version so my question

00:37:03,180 --> 00:37:12,810
is where is it plant basically to

00:37:05,130 --> 00:37:14,130
release stable version actually I don't

00:37:12,810 --> 00:37:15,840
know if I understood you said that

00:37:14,130 --> 00:37:21,300
there's not a stable version of Benjamin

00:37:15,840 --> 00:37:25,230
I think on the website I saw that we

00:37:21,300 --> 00:37:27,960
have a stable version since we did the

00:37:25,230 --> 00:37:32,120
top level process in Apache that was the

00:37:27,960 --> 00:37:35,040
release version to 2.0 we have however

00:37:32,120 --> 00:37:37,530
some api's that are experimental notably

00:37:35,040 --> 00:37:39,840
all of this stuff is still experimental

00:37:37,530 --> 00:37:43,230
because it's ongoing work but all the

00:37:39,840 --> 00:37:47,810
core transforms are quite stable so no

00:37:43,230 --> 00:37:47,810
this is already stable okay thank you

00:37:56,030 --> 00:38:00,660
just a question to to YouTube Google

00:37:58,740 --> 00:38:02,880
colleague could you explain a bit oh I

00:38:00,660 --> 00:38:05,400
didn't get how you achieve the language

00:38:02,880 --> 00:38:07,890
agnostic so from my understanding and

00:38:05,400 --> 00:38:11,250
one in in some place you should somehow

00:38:07,890 --> 00:38:13,170
either translate the code from one

00:38:11,250 --> 00:38:16,410
language to another or use some

00:38:13,170 --> 00:38:19,920
independent format could you please

00:38:16,410 --> 00:38:23,250
explain me more in this yeah so actually

00:38:19,920 --> 00:38:26,010
we do know no translation whatsoever the

00:38:23,250 --> 00:38:28,170
code is just exactly as you write it it

00:38:26,010 --> 00:38:30,300
is run so I

00:38:28,170 --> 00:38:32,310
it's basically what's going to happen

00:38:30,300 --> 00:38:36,330
and the SDK harness that I talked about

00:38:32,310 --> 00:38:38,490
is it's going to its its language

00:38:36,330 --> 00:38:40,290
specific so based on what language

00:38:38,490 --> 00:38:42,900
you're using so like there's a Python

00:38:40,290 --> 00:38:44,610
SDK harness or Java SDK harness it go

00:38:42,900 --> 00:38:46,470
SDK harness and so on

00:38:44,610 --> 00:38:49,350
and so when you submit it to for

00:38:46,470 --> 00:38:51,030
instance a Python SDK harness that SDK

00:38:49,350 --> 00:38:54,320
harness is going to know how to run

00:38:51,030 --> 00:38:57,930
Python code so it will just run

00:38:54,320 --> 00:39:00,240
basically a bit bundles Python on a very

00:38:57,930 --> 00:39:02,010
basic in this case you basically need to

00:39:00,240 --> 00:39:05,850
support all combinations between

00:39:02,010 --> 00:39:08,220
languages and target systems or I'm

00:39:05,850 --> 00:39:09,400
wrong now so if if you support a new

00:39:08,220 --> 00:39:13,590
language or

00:39:09,400 --> 00:39:18,370
say Scala is already in the list and

00:39:13,590 --> 00:39:20,410
with sparks or love watches Park for

00:39:18,370 --> 00:39:24,010
example supporter but if link how you

00:39:20,410 --> 00:39:26,200
how you do it so the the answer is that

00:39:24,010 --> 00:39:29,530
the SDK harness is actually shared

00:39:26,200 --> 00:39:34,060
between all execution engines it's not

00:39:29,530 --> 00:39:36,250
it's not really spark based yeah pretty

00:39:34,060 --> 00:39:38,200
much an event from spark there there

00:39:36,250 --> 00:39:39,670
naturally has to be some integration

00:39:38,200 --> 00:39:42,490
there and that is going to be runner

00:39:39,670 --> 00:39:48,040
specific but the SDK harness itself is

00:39:42,490 --> 00:39:49,570
its own thing I just wanted to show this

00:39:48,040 --> 00:39:50,980
is light because it could maybe help to

00:39:49,570 --> 00:39:53,560
make sense

00:39:50,980 --> 00:39:55,350
this is decay code that you write in

00:39:53,560 --> 00:39:57,760
Python in the end is going to be

00:39:55,350 --> 00:39:59,950
executed inside the harness not despite

00:39:57,760 --> 00:40:02,260
on a specific so let's say you want to

00:39:59,950 --> 00:40:04,960
create a dotnet support for example that

00:40:02,260 --> 00:40:07,540
should be pretty cool then do you have

00:40:04,960 --> 00:40:09,820
to write this SDK harness and provide

00:40:07,540 --> 00:40:11,830
the api's to connect but all the rest of

00:40:09,820 --> 00:40:13,660
the machinery you see only the pieces in

00:40:11,830 --> 00:40:15,850
orange is what you have to do and then

00:40:13,660 --> 00:40:19,360
it will support it by bringing the

00:40:15,850 --> 00:40:20,740
sparkle of decay IPI is the same you

00:40:19,360 --> 00:40:22,480
have to get is the same do you have

00:40:20,740 --> 00:40:26,560
pretty much to define the same yin and

00:40:22,480 --> 00:40:29,620
how logic how big how many lines of code

00:40:26,560 --> 00:40:32,920
is in a mystic and their harness I mean

00:40:29,620 --> 00:40:34,630
when I say SDK is the end user API the

00:40:32,920 --> 00:40:37,360
harness is as is the set of services

00:40:34,630 --> 00:40:40,210
that Kyle show that the allowed to pass

00:40:37,360 --> 00:40:42,930
data and control commands the sector

00:40:40,210 --> 00:40:46,980
execute this data with this function

00:40:42,930 --> 00:40:49,720
well III don't know the size but this is

00:40:46,980 --> 00:40:58,030
definitely big yes a lot is not so

00:40:49,720 --> 00:40:59,800
trivial to do it so continuing on the

00:40:58,030 --> 00:41:01,300
same question like you also mentioned

00:40:59,800 --> 00:41:11,110
that you had thought above so how is

00:41:01,300 --> 00:41:14,410
that helping yeah so basically you're

00:41:11,110 --> 00:41:17,590
just serialize you you write a pipeline

00:41:14,410 --> 00:41:19,830
and python or whatever or java and then

00:41:17,590 --> 00:41:22,170
it serializes to kind of the same

00:41:19,830 --> 00:41:25,670
protocol buffer format no matter

00:41:22,170 --> 00:41:28,410
so it's been it's basically you're just

00:41:25,670 --> 00:41:32,900
essentially serializing the pipeline in

00:41:28,410 --> 00:41:32,900
a way that is common between languages

00:41:33,349 --> 00:41:37,950
complement for example you are not

00:41:36,000 --> 00:41:40,500
written as we mentioned before we're not

00:41:37,950 --> 00:41:42,059
rewriting stuff so is the world code

00:41:40,500 --> 00:41:47,369
that is just passed us bytes and

00:41:42,059 --> 00:41:49,200
abstract it in the other side it might

00:41:47,369 --> 00:41:52,039
just have been JSON as well it doesn't

00:41:49,200 --> 00:42:03,960
make a difference here right the like

00:41:52,039 --> 00:42:06,750
yeah yeah I have a question if it's

00:42:03,960 --> 00:42:09,210
possible in the future to create a

00:42:06,750 --> 00:42:14,000
service for example a cloud service

00:42:09,210 --> 00:42:16,589
which would be the beam execution

00:42:14,000 --> 00:42:19,559
environment and you would also you would

00:42:16,589 --> 00:42:22,619
also have to create a pipeline and a jar

00:42:19,559 --> 00:42:25,589
without any runner inside and just

00:42:22,619 --> 00:42:27,539
deploy the jar with the pipeline to such

00:42:25,589 --> 00:42:33,450
a service a cloud service for example

00:42:27,539 --> 00:42:35,130
and google out or AWS maybe similar to

00:42:33,450 --> 00:42:36,960
something we're working on

00:42:35,130 --> 00:42:40,019
I don't know if it's exactly what you

00:42:36,960 --> 00:42:43,920
have in mind but basically you can you

00:42:40,019 --> 00:42:45,390
can essentially now bundle this is

00:42:43,920 --> 00:42:47,210
something we have first link but not

00:42:45,390 --> 00:42:50,400
first part yet but we're working on that

00:42:47,210 --> 00:42:53,220
basically you can bundle everything into

00:42:50,400 --> 00:42:55,380
a jar that is essentially the same kind

00:42:53,220 --> 00:42:58,619
of jar that you would execute on any

00:42:55,380 --> 00:43:01,259
spark cluster and so there's basically

00:42:58,619 --> 00:43:05,430
nothing that you would have to add to

00:43:01,259 --> 00:43:12,000
that yeah because I currently if I want

00:43:05,430 --> 00:43:16,819
to deploy my beam pipeline to aw gasps I

00:43:12,000 --> 00:43:20,190
use Flint runner so I have to create a

00:43:16,819 --> 00:43:24,000
llamar cluster then I have to run fling

00:43:20,190 --> 00:43:26,490
inside configure detect link so the

00:43:24,000 --> 00:43:29,549
memory and storage for the flink and so

00:43:26,490 --> 00:43:33,990
on and the thing is running on Hadoop

00:43:29,549 --> 00:43:35,940
and I deploy the jar file with the flink

00:43:33,990 --> 00:43:38,670
runner inside the jar file

00:43:35,940 --> 00:43:41,640
yeah so the question is if it's possible

00:43:38,670 --> 00:43:43,680
to create the service we were I wouldn't

00:43:41,640 --> 00:43:48,059
have to carry all about all of the stuff

00:43:43,680 --> 00:43:50,630
in between and just to give a Java file

00:43:48,059 --> 00:43:53,220
with the pipeline and don't care what is

00:43:50,630 --> 00:43:56,099
underneath if it's linked if it's Hadoop

00:43:53,220 --> 00:43:57,750
and so on so on okay so so actually

00:43:56,099 --> 00:43:59,549
Google offers a service for that there's

00:43:57,750 --> 00:44:01,440
dataflow is your in the work platform

00:43:59,549 --> 00:44:02,910
it's exactly how you described you don't

00:44:01,440 --> 00:44:05,700
care about the clothes it makes

00:44:02,910 --> 00:44:08,069
everything by itself of course if you

00:44:05,700 --> 00:44:09,960
are in using one of the source projects

00:44:08,069 --> 00:44:12,119
what you have to realign that manually I

00:44:09,960 --> 00:44:13,680
just mentioned at some point that for

00:44:12,119 --> 00:44:16,140
this part owner you can also use data

00:44:13,680 --> 00:44:17,609
breaks the the cloud service they have

00:44:16,140 --> 00:44:19,589
they do exactly the same thing you

00:44:17,609 --> 00:44:23,160
propose but you have to package a

00:44:19,589 --> 00:44:26,369
sparking all this stuff in but there is

00:44:23,160 --> 00:44:27,869
not as far as I know anyone working on a

00:44:26,369 --> 00:44:29,609
service like that but definitely you can

00:44:27,869 --> 00:44:31,589
do it I mean and you can even create a

00:44:29,609 --> 00:44:34,230
new runner that doesn't have any of this

00:44:31,589 --> 00:44:36,869
legacy but is created just to work on

00:44:34,230 --> 00:44:38,460
being can imagine thing doing that in

00:44:36,869 --> 00:44:44,520
whoa but this is a huge project that I

00:44:38,460 --> 00:44:47,069
don't know if somebody will take so as I

00:44:44,520 --> 00:44:50,520
understand that portability stuff also

00:44:47,069 --> 00:44:53,789
is for Java right now right so also the

00:44:50,520 --> 00:44:55,740
Java is decay so does it mean that if I

00:44:53,789 --> 00:44:59,609
have like I don't know the spark cluster

00:44:55,740 --> 00:45:01,680
and I want to run

00:44:59,609 --> 00:45:03,809
beam pipeline written in Java on it I

00:45:01,680 --> 00:45:06,660
still have to install something that

00:45:03,809 --> 00:45:09,089
spark raster right this is decay

00:45:06,660 --> 00:45:11,579
hardness for example um so you don't

00:45:09,089 --> 00:45:13,619
actually there's there's two options

00:45:11,579 --> 00:45:16,020
there you could just do the non portable

00:45:13,619 --> 00:45:18,690
version because that's still around and

00:45:16,020 --> 00:45:21,599
if you're using Java then there is you

00:45:18,690 --> 00:45:26,210
know no reason not to do it but then the

00:45:21,599 --> 00:45:28,859
second thing is that yeah so you don't I

00:45:26,210 --> 00:45:32,490
won't see you actually have to install

00:45:28,859 --> 00:45:34,589
the SDK harness yourself because again

00:45:32,490 --> 00:45:36,750
it's just a it's just a docker container

00:45:34,589 --> 00:45:39,180
and we are publishing images for them

00:45:36,750 --> 00:45:43,109
now so it will get them automatically

00:45:39,180 --> 00:45:44,970
and it should be able to if it's Java

00:45:43,109 --> 00:45:46,619
should be able to get your dependencies

00:45:44,970 --> 00:45:49,069
for you so you shouldn't have to worry

00:45:46,619 --> 00:45:49,069
about that

00:45:49,620 --> 00:45:55,020
there is one one thing that we already

00:45:52,260 --> 00:45:56,790
heard from some customer that is that I

00:45:55,020 --> 00:45:59,610
mean people who running Hadoop for

00:45:56,790 --> 00:46:01,590
example some of them are with Christian

00:45:59,610 --> 00:46:04,310
to have docker at the moment in the same

00:46:01,590 --> 00:46:08,280
machine in worker so but we with

00:46:04,310 --> 00:46:09,240
interview we you can also use processes

00:46:08,280 --> 00:46:11,070
with this I mean there are like

00:46:09,240 --> 00:46:13,140
different environments so that could be

00:46:11,070 --> 00:46:15,300
an alternative but that's the only

00:46:13,140 --> 00:46:18,120
dependency are part of the docker images

00:46:15,300 --> 00:46:20,220
that are already packaged and this is

00:46:18,120 --> 00:46:23,370
again kind of an ongoing discussion

00:46:20,220 --> 00:46:26,010
about how to make this more usable so if

00:46:23,370 --> 00:46:28,020
you can if you want to you can share a

00:46:26,010 --> 00:46:29,850
message on the dev list and we'd be

00:46:28,020 --> 00:46:31,710
happy to discuss like you know your

00:46:29,850 --> 00:46:33,990
specific deployment needs and all that

00:46:31,710 --> 00:46:36,120
because we want again we want this to

00:46:33,990 --> 00:46:39,950
work for everybody but yeah again

00:46:36,120 --> 00:46:39,950

YouTube URL: https://www.youtube.com/watch?v=ortl8sGZjYM


