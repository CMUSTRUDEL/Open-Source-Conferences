Title: #ACEU19: Extensible Autonomous Transactions in Microservices
Publication date: 2019-10-29
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	Speakers: Divya Nagar & Nicola Giacchetta

More: https://aceu19.apachecon.com/session/extensible-autonomous-transactions-world-microservices-using-apache-kafka

Over the last decade or so, there has been a strong movement toward a flexible style of building large systems that have lately been called microservices. When you move to microservices, the data distribution becomes the biggest problem. How to move data transactionally in a distributed architecture of hundreds of microservices while keeping the system in a consistent state. how do we provide services with the data they require correctly in minimal time. This talk describes the similar problem we faced while scaling the services at Nexmo, The Vonage API Platform and how are we solving it.

Other ways to solve these problems are either keep a huge shared global database, which is the violation of microservice fundamentals or create an internal Death star in which failure in a single machine you didn’t even know existed can crash your entire system. These solutions are already in place and eventually becomes more and more difficult to modify and scale.

In this talk, I will present a solution which we are using at Nexmo by deconstructing the database and creating an application independent scale and data aware layer. By using this the application can run independently without worrying about the data. By implementing the practices and architectural choices described in this talk audience will be able to decouple the applications from the data layer and other data dependencies to reduce cascading system failures.

This talk will present the following Data handling patterns:

Replica management for Consistent & Partition Tolerant in data stores
Command and Query Responsibility Separation to handle high throughout writes and virtual buckets based replication to manage read hot-spotting on a subset of the data corpus
Data pre-compute and time-travel to create timeline consistent view of data across multiple distributed systems
Data sourcing from Database replication mechanisms, local transactions and async relaying to an analytics platform
By-passing network bottlenecks like centralized load-balancers by using data-placement-aware smart local proxies/sidecar
Captions: 
	00:00:05,569 --> 00:00:12,240
hundreds o microservices sounds like a

00:00:08,670 --> 00:00:14,549
lot but we we are a little bit less than

00:00:12,240 --> 00:00:15,930
that but yes we select the problem about

00:00:14,549 --> 00:00:18,150
on a muslin section in were the

00:00:15,930 --> 00:00:20,279
microservices thank you for joining

00:00:18,150 --> 00:00:23,220
first of all because there is other talk

00:00:20,279 --> 00:00:24,150
in parallel so it means that you had to

00:00:23,220 --> 00:00:29,579
choose us somehow

00:00:24,150 --> 00:00:32,310
oh yes so we are Nicola in Libya we work

00:00:29,579 --> 00:00:35,489
in a team of three people along with

00:00:32,310 --> 00:00:37,260
Narayan we had the luxury to mention the

00:00:35,489 --> 00:00:39,960
world team because the team is small so

00:00:37,260 --> 00:00:41,430
and we were connects more the one at API

00:00:39,960 --> 00:00:44,010
be working a team that deal with the

00:00:41,430 --> 00:00:49,020
services I availability data migration

00:00:44,010 --> 00:00:53,040
and data replication so for those of you

00:00:49,020 --> 00:00:55,500
that don't know how our mission and next

00:00:53,040 --> 00:00:59,700
mode vanity VI platform is to build a

00:00:55,500 --> 00:01:02,480
cloud-based communication platform to

00:00:59,700 --> 00:01:05,460
allow developers to interconnect

00:01:02,480 --> 00:01:07,710
applications their applications with

00:01:05,460 --> 00:01:10,320
communication infrastructure we kind of

00:01:07,710 --> 00:01:14,340
abstract all the problem of this

00:01:10,320 --> 00:01:16,170
integration and in general no matter if

00:01:14,340 --> 00:01:18,420
you are a single developer in his own

00:01:16,170 --> 00:01:21,030
room trying to make a POC or if you are

00:01:18,420 --> 00:01:23,369
a big enterprise with few line of codes

00:01:21,030 --> 00:01:28,109
and an API requests you can dispatch

00:01:23,369 --> 00:01:29,399
your traffic and and provide your

00:01:28,109 --> 00:01:32,030
application with the capability to

00:01:29,399 --> 00:01:35,700
interact with all this different kind of

00:01:32,030 --> 00:01:37,350
communication tool globally and without

00:01:35,700 --> 00:01:41,490
having the problem of deal with country

00:01:37,350 --> 00:01:46,770
specific regulation local careers which

00:01:41,490 --> 00:01:49,619
I can tell you it's a big burden yeah so

00:01:46,770 --> 00:01:51,780
cloud platform communication platform

00:01:49,619 --> 00:01:54,959
as-a-service azure would be quickest

00:01:51,780 --> 00:01:57,119
these days so yeah maybe who knows you

00:01:54,959 --> 00:02:01,469
are already using us somehow and you

00:01:57,119 --> 00:02:03,420
don't even know something about the

00:02:01,469 --> 00:02:07,529
agenda today we are gonna a little bit

00:02:03,420 --> 00:02:09,720
talk about the fairy tales of how we

00:02:07,529 --> 00:02:11,340
ended up having started with a small

00:02:09,720 --> 00:02:16,590
back-end application we ended up in a

00:02:11,340 --> 00:02:19,080
monolid then always try to scale these

00:02:16,590 --> 00:02:21,570
application which was basically a read

00:02:19,080 --> 00:02:23,520
intensive application so we scaled our

00:02:21,570 --> 00:02:26,400
read workload with the tool internal to

00:02:23,520 --> 00:02:28,980
that we have developed called Volga then

00:02:26,400 --> 00:02:30,900
that supported dazzles so for the

00:02:28,980 --> 00:02:33,120
migration to micro services and then

00:02:30,900 --> 00:02:35,610
when we were in the micro services and

00:02:33,120 --> 00:02:37,200
we thought that the worst was that was

00:02:35,610 --> 00:02:39,480
passed we had the problem of

00:02:37,200 --> 00:02:42,720
transactions there and we are gonna try

00:02:39,480 --> 00:02:46,370
to talk about our strategy to fix this

00:02:42,720 --> 00:02:49,410
problem yeah so there you go we have

00:02:46,370 --> 00:02:52,440
once upon a time our small back-end

00:02:49,410 --> 00:02:54,060
application I believe from an

00:02:52,440 --> 00:02:56,250
engineering software engineering

00:02:54,060 --> 00:02:59,490
distributed engineering standpoint this

00:02:56,250 --> 00:03:02,010
best perfect place to be you can easily

00:02:59,490 --> 00:03:04,530
delegate or consistency and isolation

00:03:02,010 --> 00:03:06,780
problems to your my sequel engine of

00:03:04,530 --> 00:03:12,030
course this is a fairy tale so I'm not

00:03:06,780 --> 00:03:14,250
adding into this diagram proxies cluster

00:03:12,030 --> 00:03:16,740
replication for this hysterical very old

00:03:14,250 --> 00:03:18,900
I and I availability all this stuff it's

00:03:16,740 --> 00:03:20,459
just front-end application calling a

00:03:18,900 --> 00:03:24,390
back-end application with the database

00:03:20,459 --> 00:03:26,030
oh yeah so as I already mentioned we are

00:03:24,390 --> 00:03:28,709
talking about the read intense

00:03:26,030 --> 00:03:32,970
application back to buy a my sequel

00:03:28,709 --> 00:03:35,850
database Mexico yeah so the company

00:03:32,970 --> 00:03:38,550
start to grow grew across region as well

00:03:35,850 --> 00:03:40,140
so at some point in other data center we

00:03:38,550 --> 00:03:43,110
had the other application interacting

00:03:40,140 --> 00:03:46,530
with this back-end application that was

00:03:43,110 --> 00:03:48,930
not any more tiny but it was evolving

00:03:46,530 --> 00:03:51,150
into a monolith and we reached at this

00:03:48,930 --> 00:03:55,080
point where it was time to deal with

00:03:51,150 --> 00:03:57,959
scalability and the main problem were

00:03:55,080 --> 00:04:01,410
read workload as I already mentioned and

00:03:57,959 --> 00:04:03,150
monolithic architecture so let's see how

00:04:01,410 --> 00:04:05,850
do we try to solve the problem or read

00:04:03,150 --> 00:04:09,959
workload and the problem can be

00:04:05,850 --> 00:04:13,920
decomposed into problems there is too

00:04:09,959 --> 00:04:17,430
much load on that back and database

00:04:13,920 --> 00:04:19,410
which is again putting in jeopardy the

00:04:17,430 --> 00:04:24,300
availability of the right traffic from

00:04:19,410 --> 00:04:25,830
that up on that data store and of course

00:04:24,300 --> 00:04:27,120
there is a problem or latency for the

00:04:25,830 --> 00:04:29,250
application that are in the other data

00:04:27,120 --> 00:04:29,900
center above all that are doing this

00:04:29,250 --> 00:04:34,790
network

00:04:29,900 --> 00:04:37,190
to reach the primary data store yeah so

00:04:34,790 --> 00:04:40,820
the solution are the common solution we

00:04:37,190 --> 00:04:42,770
add in caches we try to spin up in

00:04:40,820 --> 00:04:47,420
reading sync replicas in other region

00:04:42,770 --> 00:04:50,930
and that's where cap theorem comes into

00:04:47,420 --> 00:04:52,670
the game because of course this is kind

00:04:50,930 --> 00:04:56,410
of to set up a little bit the

00:04:52,670 --> 00:04:59,300
expectation to say guys there is no way

00:04:56,410 --> 00:05:02,020
you can have at the same time in

00:04:59,300 --> 00:05:05,570
disability application high availability

00:05:02,020 --> 00:05:08,000
high consistency and partition tolerant

00:05:05,570 --> 00:05:11,630
partition tolerance being resilient to

00:05:08,000 --> 00:05:15,920
network failure but actually we cannot

00:05:11,630 --> 00:05:17,480
avoid in real world to deal with the

00:05:15,920 --> 00:05:20,950
network failure because networks are

00:05:17,480 --> 00:05:23,690
unreliable so the choice is between two

00:05:20,950 --> 00:05:25,880
of the triangle and it's between

00:05:23,690 --> 00:05:27,440
consistency availability basically of to

00:05:25,880 --> 00:05:30,370
choose when you have a network partition

00:05:27,440 --> 00:05:34,490
if you want to send back to your to your

00:05:30,370 --> 00:05:39,470
application a 500 or an inconsistent

00:05:34,490 --> 00:05:41,810
view of the data and yet the first real

00:05:39,470 --> 00:05:43,160
choice here in that trade-off is

00:05:41,810 --> 00:05:46,220
basically when you are adding this

00:05:43,160 --> 00:05:48,050
caches around and the problem is how do

00:05:46,220 --> 00:05:50,270
you how do you notify discussion for

00:05:48,050 --> 00:05:51,980
updates or cache invalidation abuse and

00:05:50,270 --> 00:05:54,740
cache invalidation notification and

00:05:51,980 --> 00:05:58,070
that's where the quote from field

00:05:54,740 --> 00:05:59,810
captain start jumping into your mind

00:05:58,070 --> 00:06:01,460
that there are only two are things in

00:05:59,810 --> 00:06:04,280
computer science cache invalidation and

00:06:01,460 --> 00:06:06,230
naming things and you start

00:06:04,280 --> 00:06:10,160
understanding why cache invalidation was

00:06:06,230 --> 00:06:11,090
such a problem and the reason and kind

00:06:10,160 --> 00:06:13,040
of an anti-pattern

00:06:11,090 --> 00:06:14,360
that I've seen very often opting and

00:06:13,040 --> 00:06:16,280
there is this problem of do our rights

00:06:14,360 --> 00:06:18,080
were basically to notify your cache from

00:06:16,280 --> 00:06:20,900
Europe you manage you manage these

00:06:18,080 --> 00:06:22,850
updates from your application so

00:06:20,900 --> 00:06:24,380
basically the application in parallel

00:06:22,850 --> 00:06:27,950
writes to the primary data store and

00:06:24,380 --> 00:06:29,450
then write one message queue or to

00:06:27,950 --> 00:06:32,930
whatever other technology you have

00:06:29,450 --> 00:06:37,990
available to dispatch changes to the

00:06:32,930 --> 00:06:41,780
cashews but this problem of duo rights

00:06:37,990 --> 00:06:43,310
has two main problems and it's that

00:06:41,780 --> 00:06:45,380
without a complex

00:06:43,310 --> 00:06:47,840
coordination protocol like for example

00:06:45,380 --> 00:06:52,870
two-phase commit it's very difficult for

00:06:47,840 --> 00:06:55,700
us to guarantee that the two rights are

00:06:52,870 --> 00:06:57,890
consistent and the ordering of the

00:06:55,700 --> 00:06:59,930
change wherever your application needs

00:06:57,890 --> 00:07:02,720
this requirement your caches needs to

00:06:59,930 --> 00:07:04,610
this requirement is maintained and also

00:07:02,720 --> 00:07:06,800
there is another problem that the logic

00:07:04,610 --> 00:07:08,810
tries to the messaging system is

00:07:06,800 --> 00:07:10,640
strictly coupled with your application

00:07:08,810 --> 00:07:12,260
that means that once you want in the

00:07:10,640 --> 00:07:14,480
future when you want to refactor that

00:07:12,260 --> 00:07:17,330
back and monnel it and maybe the compose

00:07:14,480 --> 00:07:19,670
and segregated into micro services yeah

00:07:17,330 --> 00:07:21,020
this is going to be this is going to be

00:07:19,670 --> 00:07:25,420
kicking you it's a problem that you're

00:07:21,020 --> 00:07:29,480
only postponing so there is actually in

00:07:25,420 --> 00:07:32,510
a pattern for solving this issue one on

00:07:29,480 --> 00:07:35,810
I mean there are several pattern

00:07:32,510 --> 00:07:40,010
actually but the one that we went for

00:07:35,810 --> 00:07:42,890
was log mining lot- means that you

00:07:40,010 --> 00:07:45,230
basically take your primary data store

00:07:42,890 --> 00:07:47,660
data store as the source of truth you

00:07:45,230 --> 00:07:51,140
extraction we extract change form its

00:07:47,660 --> 00:07:52,730
transaction log and with I'm sure you

00:07:51,140 --> 00:07:55,730
are aware that with my sequel we have

00:07:52,730 --> 00:07:59,870
this locked row of transaction available

00:07:55,730 --> 00:08:01,880
and therefore we connect directly to it

00:07:59,870 --> 00:08:04,010
so we try to create a sequence with we

00:08:01,880 --> 00:08:05,600
instead of writing in parallel to the

00:08:04,010 --> 00:08:10,880
two sides so we connect as a replication

00:08:05,600 --> 00:08:12,919
slave to this source of truth we stream

00:08:10,880 --> 00:08:14,660
the changes through Kafka and then on

00:08:12,919 --> 00:08:17,450
the other side we apply the changes to

00:08:14,660 --> 00:08:19,810
caches and not so why not since we are

00:08:17,450 --> 00:08:24,500
there we also use the dis solution for

00:08:19,810 --> 00:08:26,240
replicating creating in sync replica of

00:08:24,500 --> 00:08:28,820
the primary data store in the other data

00:08:26,240 --> 00:08:30,919
center this is exactly the way my sequel

00:08:28,820 --> 00:08:36,380
does replication if you are familiar

00:08:30,919 --> 00:08:38,750
with it and so our design evolved in

00:08:36,380 --> 00:08:41,950
something like this now we don't have

00:08:38,750 --> 00:08:45,800
anymore the problem of dual writes plus

00:08:41,950 --> 00:08:48,530
we also got something more out of it

00:08:45,800 --> 00:08:51,380
once the changes are there floating in

00:08:48,530 --> 00:08:53,510
this I'll I way that we have built it's

00:08:51,380 --> 00:08:56,530
very easy for us to create materialized

00:08:53,510 --> 00:09:02,140
views of our data from our prime

00:08:56,530 --> 00:09:06,940
restoring to other into other data

00:09:02,140 --> 00:09:09,310
stores we can use it to push changes to

00:09:06,940 --> 00:09:11,650
secondary index exchanging data with

00:09:09,310 --> 00:09:14,110
other teams by sending data to analytic

00:09:11,650 --> 00:09:16,840
platform again we can send them to

00:09:14,110 --> 00:09:20,110
different caches in sync replica data

00:09:16,840 --> 00:09:21,640
Ramon genius and heterogeneous so this

00:09:20,110 --> 00:09:26,430
is a great tool to support also

00:09:21,640 --> 00:09:29,920
migration as we will see afterwards but

00:09:26,430 --> 00:09:31,800
yes I mentioned cap theorem before and

00:09:29,920 --> 00:09:36,220
there was something we had to give up

00:09:31,800 --> 00:09:38,260
and this was for respect to the scenario

00:09:36,220 --> 00:09:40,990
we were we were on the single data store

00:09:38,260 --> 00:09:45,640
we had to give up that kind of

00:09:40,990 --> 00:09:48,010
capability to delegates consistency and

00:09:45,640 --> 00:09:50,560
isolation to the data store into a new

00:09:48,010 --> 00:09:52,300
consistency model which was sequential

00:09:50,560 --> 00:09:55,510
consistency which if you're not familiar

00:09:52,300 --> 00:09:57,580
it's just and one of the paradigm of

00:09:55,510 --> 00:09:59,680
event or consistency where you are also

00:09:57,580 --> 00:10:05,860
adding the constraint of the dispatching

00:09:59,680 --> 00:10:09,370
the changes in order to your other end

00:10:05,860 --> 00:10:12,990
of your event or consistency stream in

00:10:09,370 --> 00:10:15,160
this case we were able to achieve this

00:10:12,990 --> 00:10:19,290
leveraging of course the intrinsic

00:10:15,160 --> 00:10:23,230
ordering within the bin log file and

00:10:19,290 --> 00:10:26,430
using Kafka order delivery and the other

00:10:23,230 --> 00:10:32,050
thing that we have to give up is like

00:10:26,430 --> 00:10:35,920
there was a relaxed semantics because of

00:10:32,050 --> 00:10:40,660
course in order to achieve reliable

00:10:35,920 --> 00:10:42,760
delivery we had to we have to pay the

00:10:40,660 --> 00:10:45,820
cost of some time or release patching

00:10:42,760 --> 00:10:48,580
some messages for example if one of the

00:10:45,820 --> 00:10:50,740
two stages of our pipeline goes down

00:10:48,580 --> 00:10:54,540
source or sink as I have mentioned

00:10:50,740 --> 00:10:57,370
before goes down when we bring this

00:10:54,540 --> 00:11:00,100
stages up we could dispatch again some

00:10:57,370 --> 00:11:02,350
of the messages yeah other the future

00:11:00,100 --> 00:11:05,770
with that we have a method within Volga

00:11:02,350 --> 00:11:06,649
where a snapshotting of we are creating

00:11:05,770 --> 00:11:12,499
work

00:11:06,649 --> 00:11:15,259
views of the data also for installed

00:11:12,499 --> 00:11:17,119
somewhere so that if we have a disaster

00:11:15,259 --> 00:11:19,519
we can recovery the state of our

00:11:17,119 --> 00:11:22,850
application we can bootstrap in sync

00:11:19,519 --> 00:11:25,279
replica in slave very quick through our

00:11:22,850 --> 00:11:28,220
snapshots and also we use this nap shot

00:11:25,279 --> 00:11:32,480
for correctness so periodically we check

00:11:28,220 --> 00:11:36,069
that the snapshot that at given a given

00:11:32,480 --> 00:11:40,519
of the offset is equal at this oldest

00:11:36,069 --> 00:11:42,889
destination and the source also there is

00:11:40,519 --> 00:11:45,350
another important thing is that true

00:11:42,889 --> 00:11:49,009
once your data in this eye way you are

00:11:45,350 --> 00:11:52,220
able to collect a metric about right

00:11:49,009 --> 00:11:54,769
access pattern which is very important

00:11:52,220 --> 00:11:56,600
tool again to support the migration

00:11:54,769 --> 00:11:59,389
afterward and the segregation into micro

00:11:56,600 --> 00:12:02,029
services we are also for respect to

00:11:59,389 --> 00:12:03,679
other tool that do the same thing we are

00:12:02,029 --> 00:12:07,850
also supporting different level of

00:12:03,679 --> 00:12:09,980
partitioning because since this tool is

00:12:07,850 --> 00:12:11,660
all is mainly was mainly starting the

00:12:09,980 --> 00:12:13,360
designing and not test change data

00:12:11,660 --> 00:12:17,569
capture but was mainly designed for

00:12:13,360 --> 00:12:21,199
replication yeah we added a problem like

00:12:17,569 --> 00:12:23,480
having a partition level that was around

00:12:21,199 --> 00:12:25,279
the transaction and then we are using

00:12:23,480 --> 00:12:27,649
gnome amorphic caching for integrity

00:12:25,279 --> 00:12:31,540
checks to check that the correctness of

00:12:27,649 --> 00:12:34,129
the transaction online is is respected

00:12:31,540 --> 00:12:37,699
yeah so we solved the problem of the

00:12:34,129 --> 00:12:39,259
reads and then we have the drop back

00:12:37,699 --> 00:12:42,350
home the monolith

00:12:39,259 --> 00:12:45,519
I go very fast on this is a big problem

00:12:42,350 --> 00:12:48,860
that we have already seen around but

00:12:45,519 --> 00:12:51,139
there was some specific issue that we

00:12:48,860 --> 00:12:52,639
have in our organization in particular

00:12:51,139 --> 00:12:55,449
we had the den capacity to scale

00:12:52,639 --> 00:12:58,220
business domain independently and

00:12:55,449 --> 00:13:00,679
barrier to innovation even just a simple

00:12:58,220 --> 00:13:03,589
task of upgrading a library in that in

00:13:00,679 --> 00:13:08,410
that model it was becoming a painful

00:13:03,589 --> 00:13:12,949
Gurgaon toin operation involving a team

00:13:08,410 --> 00:13:16,160
negotiating negotiation and yet

00:13:12,949 --> 00:13:17,720
difficult coordination then also because

00:13:16,160 --> 00:13:20,140
it was difficult very difficult for us

00:13:17,720 --> 00:13:22,870
to understand that the

00:13:20,140 --> 00:13:25,480
and segregated the scope of ownership of

00:13:22,870 --> 00:13:28,810
different team within that code base so

00:13:25,480 --> 00:13:31,930
all this was creating just very bad mood

00:13:28,810 --> 00:13:34,750
the developers to work on that codebase

00:13:31,930 --> 00:13:37,140
and barrier to innovation also for

00:13:34,750 --> 00:13:40,150
product from the product side that

00:13:37,140 --> 00:13:41,830
product was really scared to go to our

00:13:40,150 --> 00:13:45,880
developer and ask for new features

00:13:41,830 --> 00:13:47,770
because of the bad moon yeah so he was

00:13:45,880 --> 00:13:49,450
time to migrate to micro services here

00:13:47,770 --> 00:13:51,940
again another pattern that is very

00:13:49,450 --> 00:13:53,590
common in this migration I don't know if

00:13:51,940 --> 00:13:54,940
you're familiar with the strangle

00:13:53,590 --> 00:13:56,440
pattern by the way how many of you guys

00:13:54,940 --> 00:13:58,720
are dealing with Mon elite at the moment

00:13:56,440 --> 00:14:01,960
and very good use to do this food survey

00:13:58,720 --> 00:14:06,430
is any of Ewing also in the in this face

00:14:01,960 --> 00:14:09,040
of migration to micro services okay so

00:14:06,430 --> 00:14:12,430
yeah strangled pattern is like very

00:14:09,040 --> 00:14:15,160
common solution it's patterned at the

00:14:12,430 --> 00:14:17,530
bolts entry basically entry phase in the

00:14:15,160 --> 00:14:21,640
first phase you try to build like an

00:14:17,530 --> 00:14:24,130
application in parallel to your existing

00:14:21,640 --> 00:14:27,010
logic then for some time you keep these

00:14:24,130 --> 00:14:28,930
two reality living at the same time and

00:14:27,010 --> 00:14:32,620
then at the end you cut off you strangle

00:14:28,930 --> 00:14:34,750
the old the old application this is very

00:14:32,620 --> 00:14:37,720
common there is like this analogy with

00:14:34,750 --> 00:14:39,870
these fixed trees that since they are in

00:14:37,720 --> 00:14:42,970
this very dark forest they have to

00:14:39,870 --> 00:14:45,160
leverage lighting from the mother tree

00:14:42,970 --> 00:14:47,410
so they basically starts blossoming

00:14:45,160 --> 00:14:48,850
their seed from the top of this tree and

00:14:47,410 --> 00:14:53,140
then they goes down creating this

00:14:48,850 --> 00:14:56,710
wonderful natural architecture but then

00:14:53,140 --> 00:14:59,320
the O's also strangled so they kind of

00:14:56,710 --> 00:15:04,350
destroy it for their survival nature is

00:14:59,320 --> 00:15:06,250
amazing strangled pattern yes so I

00:15:04,350 --> 00:15:07,960
talked about the first phase of

00:15:06,250 --> 00:15:09,640
transformation where we are building the

00:15:07,960 --> 00:15:11,710
first stop and this phase we really deal

00:15:09,640 --> 00:15:13,360
with all the problems that are strictly

00:15:11,710 --> 00:15:17,560
related to the app we built the

00:15:13,360 --> 00:15:21,880
deployment pipelines we start writing

00:15:17,560 --> 00:15:24,340
creating a test pre prod environments we

00:15:21,880 --> 00:15:26,590
set up all the monitoring tools that we

00:15:24,340 --> 00:15:28,750
are going to need for the life food of

00:15:26,590 --> 00:15:31,240
these micro service but all the traffic

00:15:28,750 --> 00:15:33,450
is this goes on the other side into the

00:15:31,240 --> 00:15:37,720
all the monolith

00:15:33,450 --> 00:15:39,730
then there is very painful phase which

00:15:37,720 --> 00:15:41,800
is the migration of the dependency to

00:15:39,730 --> 00:15:43,660
the new API of course I didn't mention

00:15:41,800 --> 00:15:47,560
in the back end the new API is still

00:15:43,660 --> 00:15:49,870
going through the monolith so and this

00:15:47,560 --> 00:15:51,460
is a very painful migration because this

00:15:49,870 --> 00:15:52,960
whether the migration becomes also

00:15:51,460 --> 00:15:57,610
social you have to go to the different

00:15:52,960 --> 00:15:59,230
team advocated the new API push them to

00:15:57,610 --> 00:16:01,450
migrate as soon as possible start

00:15:59,230 --> 00:16:05,800
deprecating the API in the old model or

00:16:01,450 --> 00:16:08,110
monolith and yeah maybe support them and

00:16:05,800 --> 00:16:16,870
bringing your new dependency into their

00:16:08,110 --> 00:16:19,360
own tests environments and now you start

00:16:16,870 --> 00:16:22,000
having this problem into our mind now am

00:16:19,360 --> 00:16:24,700
I gonna migrate then the real problem

00:16:22,000 --> 00:16:27,520
that is the data under the hood and

00:16:24,700 --> 00:16:32,230
that's where another famous patterns

00:16:27,520 --> 00:16:35,350
comes to your mind is the command query

00:16:32,230 --> 00:16:37,360
responsibility segregation so basically

00:16:35,350 --> 00:16:39,160
the read model into your application is

00:16:37,360 --> 00:16:41,740
completely the capital the couple for

00:16:39,160 --> 00:16:44,830
the right from the right models and to

00:16:41,740 --> 00:16:46,930
achieve this again we go back to the

00:16:44,830 --> 00:16:49,510
previous conversation about consistency

00:16:46,930 --> 00:16:51,520
you cannot achieve this by keeping the

00:16:49,510 --> 00:16:53,530
consistent the strictly consistency

00:16:51,520 --> 00:17:00,100
between the right store and the rid

00:16:53,530 --> 00:17:02,050
store so yeah this is very very good

00:17:00,100 --> 00:17:05,380
because with this pattern you can start

00:17:02,050 --> 00:17:07,270
migrating your read workload and in

00:17:05,380 --> 00:17:10,000
parallel still keeping the source of

00:17:07,270 --> 00:17:13,030
truth in your main main database so that

00:17:10,000 --> 00:17:16,089
you can also make incremental the

00:17:13,030 --> 00:17:18,240
migration of the the of the data from

00:17:16,089 --> 00:17:21,160
your data store which is the most

00:17:18,240 --> 00:17:25,390
sensitive piece of your of your

00:17:21,160 --> 00:17:27,850
migration and yes the benefit of common

00:17:25,390 --> 00:17:31,000
query responsibility a clinician are a

00:17:27,850 --> 00:17:33,450
lot I added it at the end the support

00:17:31,000 --> 00:17:37,230
for incremental migration but of course

00:17:33,450 --> 00:17:43,810
you can create read model that are

00:17:37,230 --> 00:17:46,659
optimized for your queries that are yeah

00:17:43,810 --> 00:17:49,029
completely they can scale independent

00:17:46,659 --> 00:17:51,129
for respect to your right model you can

00:17:49,029 --> 00:17:53,249
of course create separation of concern

00:17:51,129 --> 00:17:55,629
which is always good in our in our work

00:17:53,249 --> 00:17:57,700
you can improve security because then

00:17:55,629 --> 00:17:59,919
it's very easy for you to the coupled

00:17:57,700 --> 00:18:04,929
security policy for right model and the

00:17:59,919 --> 00:18:08,909
and read the model and yes you can

00:18:04,929 --> 00:18:12,549
basically start the loose coupling

00:18:08,909 --> 00:18:15,639
between your data store in the back end

00:18:12,549 --> 00:18:19,090
so basically you reach something like

00:18:15,639 --> 00:18:21,220
this at some point and this is exactly

00:18:19,090 --> 00:18:25,409
the second phase the cosy space has

00:18:21,220 --> 00:18:30,129
already has definitely started now then

00:18:25,409 --> 00:18:32,289
in our case we got rid of the monolith

00:18:30,129 --> 00:18:36,279
incrementally by migrating the business

00:18:32,289 --> 00:18:39,190
logical the a monolith into our new

00:18:36,279 --> 00:18:41,080
application and that's when basically

00:18:39,190 --> 00:18:43,389
you can start getting a little death

00:18:41,080 --> 00:18:46,599
dependency for us now in this phase we

00:18:43,389 --> 00:18:48,820
can achieve mark the migration is

00:18:46,599 --> 00:18:51,009
completed but then there is still the

00:18:48,820 --> 00:18:56,279
right traffic that is going to that

00:18:51,009 --> 00:18:58,749
other data store and that's when the

00:18:56,279 --> 00:19:00,669
matrix that we collected before with

00:18:58,749 --> 00:19:02,619
with Volga come into the game because

00:19:00,669 --> 00:19:04,090
did then depending on the right access

00:19:02,619 --> 00:19:05,679
pattern to your data store you can

00:19:04,090 --> 00:19:07,899
define different strategy to migrate

00:19:05,679 --> 00:19:09,700
this data you can also decide not to

00:19:07,899 --> 00:19:14,259
migrate the data because the data find

00:19:09,700 --> 00:19:16,090
there it's up to you you could also the

00:19:14,259 --> 00:19:18,369
site works at a small down time and

00:19:16,090 --> 00:19:19,899
decide where to go with the nine-time

00:19:18,369 --> 00:19:22,989
because now you have graph of

00:19:19,899 --> 00:19:29,169
distribution of requests on your master

00:19:22,989 --> 00:19:31,330
so yeah finally you get rid of the

00:19:29,169 --> 00:19:33,340
monolith and you I guess you guys get

00:19:31,330 --> 00:19:43,809
results on me because I'm gonna end over

00:19:33,340 --> 00:19:47,169
to Divya hello yeah so yeah so we have

00:19:43,809 --> 00:19:51,249
solved a lot of problems till now but

00:19:47,169 --> 00:19:53,289
let's let's go back to the basic topic

00:19:51,249 --> 00:19:55,899
of the presentation so we said that hey

00:19:53,289 --> 00:19:57,429
we will talk about transaction and how

00:19:55,899 --> 00:19:59,619
do you handle all these things

00:19:57,429 --> 00:20:00,740
consistency isolation and and everything

00:19:59,619 --> 00:20:02,210
so

00:20:00,740 --> 00:20:07,760
can someone tell me what is an

00:20:02,210 --> 00:20:10,340
autonomous transaction how do you define

00:20:07,760 --> 00:20:12,799
an autonomous transaction and how that

00:20:10,340 --> 00:20:20,090
definition is important in the context

00:20:12,799 --> 00:20:21,649
of transactions anyone okay so if I

00:20:20,090 --> 00:20:23,659
start with an autonomous transactions so

00:20:21,649 --> 00:20:26,390
autonomous transaction is that let's say

00:20:23,659 --> 00:20:27,649
you have a transaction that in which you

00:20:26,390 --> 00:20:29,299
are doing multiple things you are doing

00:20:27,649 --> 00:20:31,940
let's say some insertion updates and

00:20:29,299 --> 00:20:33,440
whatever operations so the system

00:20:31,940 --> 00:20:35,570
divides your transaction in two

00:20:33,440 --> 00:20:36,950
different parts so now a transaction is

00:20:35,570 --> 00:20:39,169
being divided into transaction one

00:20:36,950 --> 00:20:42,169
transaction 2 transaction 3 now you

00:20:39,169 --> 00:20:44,510
apply these transactions you get the

00:20:42,169 --> 00:20:46,370
result and you pass on this result to

00:20:44,510 --> 00:20:48,740
the actual transaction and that will be

00:20:46,370 --> 00:20:50,960
your final result so an autonomous

00:20:48,740 --> 00:20:53,929
transaction is nothing but a division of

00:20:50,960 --> 00:20:56,390
your request and then completing each of

00:20:53,929 --> 00:20:59,330
them one by one and then finally giving

00:20:56,390 --> 00:21:01,130
back to the actual request so and based

00:20:59,330 --> 00:21:04,190
on this definition there are different

00:21:01,130 --> 00:21:06,470
patterns which came into picture for

00:21:04,190 --> 00:21:07,880
example whenever you go in a distributed

00:21:06,470 --> 00:21:12,409
system and you perform two-phase commit

00:21:07,880 --> 00:21:14,090
or any other thing you see that so now

00:21:12,409 --> 00:21:16,309
let's come back to this monolith

00:21:14,090 --> 00:21:18,529
application so you know there are a lot

00:21:16,309 --> 00:21:19,970
of problems with monoliths I agree but

00:21:18,529 --> 00:21:22,610
there are some good things about it and

00:21:19,970 --> 00:21:24,559
this is one of them so when you have

00:21:22,610 --> 00:21:27,980
let's start with an example so let's say

00:21:24,559 --> 00:21:29,630
you work in uber and the simplest thing

00:21:27,980 --> 00:21:33,049
you do is like okay you have a booking

00:21:29,630 --> 00:21:35,149
you have a driver and you whenever you

00:21:33,049 --> 00:21:38,840
do a booking you mark the driver as busy

00:21:35,149 --> 00:21:40,700
and you mark the booking as allotted so

00:21:38,840 --> 00:21:42,890
and this is this is very like a simple

00:21:40,700 --> 00:21:45,289
application so you just if you have

00:21:42,890 --> 00:21:47,059
single database you just say hey begin

00:21:45,289 --> 00:21:49,789
this transaction do these these two

00:21:47,059 --> 00:21:53,330
things and end this transaction now it

00:21:49,789 --> 00:21:55,789
just it up at this point your database

00:21:53,330 --> 00:21:57,559
will make sure that this transaction is

00:21:55,789 --> 00:21:59,720
either succeeds or fails there won't be

00:21:57,559 --> 00:22:01,640
any transient states in between there

00:21:59,720 --> 00:22:02,870
won't be any inconsistent states in

00:22:01,640 --> 00:22:05,480
between which you need to handle

00:22:02,870 --> 00:22:08,539
everything is taken care of by your

00:22:05,480 --> 00:22:11,270
database engine so now well you decide

00:22:08,539 --> 00:22:13,070
that hey we want to you know do fast

00:22:11,270 --> 00:22:13,910
delivery we want to add new features you

00:22:13,070 --> 00:22:16,790
move to this

00:22:13,910 --> 00:22:18,800
new place where you divided your

00:22:16,790 --> 00:22:21,470
monolith application into multiple

00:22:18,800 --> 00:22:25,490
micro-services and each one is backed by

00:22:21,470 --> 00:22:27,950
the database and you design your domains

00:22:25,490 --> 00:22:29,420
in a way that ok we don't need to you

00:22:27,950 --> 00:22:30,920
know solve this problem we we I don't

00:22:29,420 --> 00:22:33,050
need to do the transactions within the

00:22:30,920 --> 00:22:35,330
micro services so that means that you

00:22:33,050 --> 00:22:37,130
don't need this entire thing so let's

00:22:35,330 --> 00:22:39,470
say now you have a driver so we have a

00:22:37,130 --> 00:22:42,050
booking service but you made these in a

00:22:39,470 --> 00:22:44,480
way that the domain are not as separated

00:22:42,050 --> 00:22:46,610
but let's say tomorrow your product is

00:22:44,480 --> 00:22:48,410
coming and asking you that hey we have

00:22:46,610 --> 00:22:50,090
this feature one we have this feature

00:22:48,410 --> 00:22:51,380
too let's start this feature three which

00:22:50,090 --> 00:22:54,320
can do the feature one and feature two

00:22:51,380 --> 00:22:56,000
together and you are like yeah we can do

00:22:54,320 --> 00:22:57,710
that but I think it's not that

00:22:56,000 --> 00:22:59,900
straightforward because then if you

00:22:57,710 --> 00:23:01,460
create this different service then now

00:22:59,900 --> 00:23:04,430
you have to deal with transient state

00:23:01,460 --> 00:23:06,470
you have to think about the consistency

00:23:04,430 --> 00:23:09,050
levels okay how do you handle the

00:23:06,470 --> 00:23:10,730
eventual consistency between services

00:23:09,050 --> 00:23:12,710
what will be your isolation level how

00:23:10,730 --> 00:23:14,270
would you roll back your transactions so

00:23:12,710 --> 00:23:16,490
because if you are using let's say my

00:23:14,270 --> 00:23:18,440
sequel or something if a transaction

00:23:16,490 --> 00:23:22,070
fails it will automatically roll back

00:23:18,440 --> 00:23:24,110
the success which ever succeeded so now

00:23:22,070 --> 00:23:27,140
you have a lot of a new set of problems

00:23:24,110 --> 00:23:29,270
to deal with so one way or other at some

00:23:27,140 --> 00:23:30,440
point of time you might come into a

00:23:29,270 --> 00:23:31,100
scenario where you have to deal with

00:23:30,440 --> 00:23:33,620
this problem

00:23:31,100 --> 00:23:35,630
so now obviously this is not a new

00:23:33,620 --> 00:23:38,120
problem and there are many companies out

00:23:35,630 --> 00:23:41,150
there who have already solved it so what

00:23:38,120 --> 00:23:45,230
are the possible solutions of it so this

00:23:41,150 --> 00:23:46,730
is what we could look on the internet so

00:23:45,230 --> 00:23:49,160
there are very two standard approaches

00:23:46,730 --> 00:23:51,770
of solving the transaction is one is

00:23:49,160 --> 00:23:53,630
two-phase commit and so this two-phase

00:23:51,770 --> 00:23:55,700
commit is not the this is the

00:23:53,630 --> 00:23:57,950
distributive two-phase commit and the

00:23:55,700 --> 00:24:01,490
sagas so how many of you have already

00:23:57,950 --> 00:24:04,280
heard of saga pattern and coordinators

00:24:01,490 --> 00:24:06,500
and how do you deal with it okay nice so

00:24:04,280 --> 00:24:08,360
I think so for the rest of you I will

00:24:06,500 --> 00:24:09,880
explain what what is saga pattern what

00:24:08,360 --> 00:24:13,580
is two-phase commit and I'll also

00:24:09,880 --> 00:24:15,830
discuss that what is this we we also

00:24:13,580 --> 00:24:18,260
have some approach which is basically in

00:24:15,830 --> 00:24:21,860
the middle of both of these so let's

00:24:18,260 --> 00:24:23,840
start with two-phase commit so now this

00:24:21,860 --> 00:24:25,160
is the diagram of two-phase commit so

00:24:23,840 --> 00:24:27,360
let's start with a very simple example

00:24:25,160 --> 00:24:30,059
again we have this driver database and

00:24:27,360 --> 00:24:32,100
booking database and the coordinator now

00:24:30,059 --> 00:24:34,230
when you do a distributed two-phase

00:24:32,100 --> 00:24:36,000
commit you need this sum coordinator to

00:24:34,230 --> 00:24:37,620
coordinate the entire transaction to

00:24:36,000 --> 00:24:40,590
rollback and to handle all these

00:24:37,620 --> 00:24:42,720
consistency of yours so you start the

00:24:40,590 --> 00:24:44,670
transaction now as it says there are two

00:24:42,720 --> 00:24:47,460
phases of it so we start in the prepare

00:24:44,670 --> 00:24:49,830
phase so your request goes to the

00:24:47,460 --> 00:24:51,090
prepare phase and when it the

00:24:49,830 --> 00:24:52,530
coordinator asks that hey this

00:24:51,090 --> 00:24:55,740
transaction started between this driver

00:24:52,530 --> 00:24:57,690
and booking so these two databases will

00:24:55,740 --> 00:24:59,640
look at the request they'll say hey yeah

00:24:57,690 --> 00:25:02,490
I can make this change I have enough

00:24:59,640 --> 00:25:04,320
resources to do ok this change this is

00:25:02,490 --> 00:25:07,020
the redo log of this choice or this

00:25:04,320 --> 00:25:09,450
change so just in case if later I have

00:25:07,020 --> 00:25:12,450
to rollback and also it will lock the

00:25:09,450 --> 00:25:14,610
tables so because now here I'm talking

00:25:12,450 --> 00:25:16,710
about that we should have these strict

00:25:14,610 --> 00:25:19,169
consistency so that means if you do not

00:25:16,710 --> 00:25:20,970
lock the tables people will be able to

00:25:19,169 --> 00:25:22,440
read the uncommitted data what if the

00:25:20,970 --> 00:25:22,830
later transaction fails and you have to

00:25:22,440 --> 00:25:24,330
rollback

00:25:22,830 --> 00:25:27,750
so we don't want to go into that

00:25:24,330 --> 00:25:30,090
scenario so you lock the tables and then

00:25:27,750 --> 00:25:32,610
your database is that hey it's prepared

00:25:30,090 --> 00:25:35,280
and then you send the commit request and

00:25:32,610 --> 00:25:38,100
if let's everything goes well you

00:25:35,280 --> 00:25:40,530
committed your data is committed and if

00:25:38,100 --> 00:25:42,419
just in case if any of the commit

00:25:40,530 --> 00:25:44,610
request fails or any of the prepare

00:25:42,419 --> 00:25:46,860
phase fails then it will just use the

00:25:44,610 --> 00:25:48,720
redo log to rollback the transaction if

00:25:46,860 --> 00:25:52,169
it is committed or made some changes

00:25:48,720 --> 00:25:55,080
already but what it's a good approach

00:25:52,169 --> 00:25:56,790
you get a lot of benefits of it but the

00:25:55,080 --> 00:26:00,360
only problem is that during this entire

00:25:56,790 --> 00:26:03,540
time your database tables or rows are

00:26:00,360 --> 00:26:06,450
logged so which makes this particular

00:26:03,540 --> 00:26:09,480
approach slower and not very useful in

00:26:06,450 --> 00:26:11,880
the current state of architectures where

00:26:09,480 --> 00:26:14,730
we have so much of late and it's so much

00:26:11,880 --> 00:26:17,309
of throughput of the requests so and

00:26:14,730 --> 00:26:19,650
another problem is the deadlock now what

00:26:17,309 --> 00:26:21,630
is the develop problem so so in a

00:26:19,650 --> 00:26:22,890
transaction when you are doing these two

00:26:21,630 --> 00:26:24,630
things so there are like let's say you

00:26:22,890 --> 00:26:26,400
log two tables and there is another

00:26:24,630 --> 00:26:28,559
concurrent transaction which also want

00:26:26,400 --> 00:26:30,120
to make changes in the same tables so

00:26:28,559 --> 00:26:31,559
there is a possibility that you can go

00:26:30,120 --> 00:26:33,179
in and read locks in error when these

00:26:31,559 --> 00:26:35,580
two transactions are waiting for each

00:26:33,179 --> 00:26:37,200
other to unlock the tables now you also

00:26:35,580 --> 00:26:39,690
need to think okay how do you avoid this

00:26:37,200 --> 00:26:40,710
deadlock how do you maybe if there is a

00:26:39,690 --> 00:26:43,260
deadlock then how do you saw

00:26:40,710 --> 00:26:45,870
these problems so but good part is that

00:26:43,260 --> 00:26:48,179
yes you get hurt a strong consistency

00:26:45,870 --> 00:26:51,210
you get read/write isolation so all

00:26:48,179 --> 00:26:53,610
these things are good so then this as

00:26:51,210 --> 00:26:55,260
this was slow and people needed a bit

00:26:53,610 --> 00:26:56,640
faster things obviously there are some

00:26:55,260 --> 00:26:58,950
tweaks you can do on two-phase commit

00:26:56,640 --> 00:27:01,260
which we will discuss later in the same

00:26:58,950 --> 00:27:01,679
presentation um let's let's move to the

00:27:01,260 --> 00:27:05,070
next one

00:27:01,679 --> 00:27:06,659
which is like sagas so now let's start

00:27:05,070 --> 00:27:08,220
again the same now there is a subtle

00:27:06,659 --> 00:27:10,230
difference between the two-phase commit

00:27:08,220 --> 00:27:12,179
and sagas can anyone tell me what is

00:27:10,230 --> 00:27:15,990
this this diagram and we had the exactly

00:27:12,179 --> 00:27:18,899
similar diagram in two-phase commit so

00:27:15,990 --> 00:27:20,700
if you look at here its if you look at

00:27:18,899 --> 00:27:23,159
the two-phase commit it was not the

00:27:20,700 --> 00:27:25,799
application it was the database and in

00:27:23,159 --> 00:27:28,440
sagas you have the application so the

00:27:25,799 --> 00:27:30,390
one fundamental difference is that sagas

00:27:28,440 --> 00:27:33,419
happen at the application level that

00:27:30,390 --> 00:27:34,770
means that the compensation or the the

00:27:33,419 --> 00:27:36,450
redo log which you are using in

00:27:34,770 --> 00:27:38,730
two-phase commit to reword or to

00:27:36,450 --> 00:27:40,320
rollback your transaction in sagas your

00:27:38,730 --> 00:27:41,610
application has to support that your

00:27:40,320 --> 00:27:44,309
application it's like an apology

00:27:41,610 --> 00:27:46,320
protocol hey we wanted to be in this

00:27:44,309 --> 00:27:47,940
state but we are not in this state so

00:27:46,320 --> 00:27:49,260
later you go to customer and apology or

00:27:47,940 --> 00:27:50,990
sorry we can't make your booking or

00:27:49,260 --> 00:27:54,960
something like that so it more like

00:27:50,990 --> 00:27:56,279
works in that way so now again we do we

00:27:54,960 --> 00:27:58,980
start with the same thing so you start a

00:27:56,279 --> 00:28:00,390
transaction to the coordinator and the

00:27:58,980 --> 00:28:02,399
you don't prepare or anything you just

00:28:00,390 --> 00:28:04,679
say that hey I'll go I'll make this

00:28:02,399 --> 00:28:06,779
change in if everything goes well then I

00:28:04,679 --> 00:28:10,020
don't need to worry but what happens

00:28:06,779 --> 00:28:11,220
when something fails so let's say if

00:28:10,020 --> 00:28:13,590
everything goes well I don't need to

00:28:11,220 --> 00:28:15,570
worry but when this particular request

00:28:13,590 --> 00:28:18,870
failed then what you do you sing the

00:28:15,570 --> 00:28:20,909
compensation that hey sorry we made this

00:28:18,870 --> 00:28:23,190
booking but we had to cancel it because

00:28:20,909 --> 00:28:25,350
we kind of overbooked or somewhat like

00:28:23,190 --> 00:28:28,500
that and then you apologize and you are

00:28:25,350 --> 00:28:30,240
done so that's what you do with saga and

00:28:28,500 --> 00:28:32,490
when the compensation is success you end

00:28:30,240 --> 00:28:34,350
the transaction now the good thing is

00:28:32,490 --> 00:28:36,210
that you don't do any locking or

00:28:34,350 --> 00:28:38,429
anything on your database so the UI you

00:28:36,210 --> 00:28:40,429
are allowed to read the data which is

00:28:38,429 --> 00:28:42,500
not yet committed or which might be

00:28:40,429 --> 00:28:45,090
rolled back eventually

00:28:42,500 --> 00:28:47,640
so now what are the problems with this

00:28:45,090 --> 00:28:49,470
thing so the good thing is that yes you

00:28:47,640 --> 00:28:50,610
don't have logs you have long do

00:28:49,470 --> 00:28:53,130
transaction you can have multiple

00:28:50,610 --> 00:28:54,540
services so you can add any services to

00:28:53,130 --> 00:28:56,550
the coordinator and start

00:28:54,540 --> 00:28:58,830
diction between them and it will wait to

00:28:56,550 --> 00:29:01,440
the protocol we'll make sure that your

00:28:58,830 --> 00:29:04,770
system comes into an eventual consistent

00:29:01,440 --> 00:29:06,510
state so those are the good things and

00:29:04,770 --> 00:29:08,340
obviously you don't have any tight

00:29:06,510 --> 00:29:10,800
coupling between services so every

00:29:08,340 --> 00:29:13,650
services can be implemented on their own

00:29:10,800 --> 00:29:15,900
but they need to have few things so now

00:29:13,650 --> 00:29:17,970
the few things is that first your

00:29:15,900 --> 00:29:20,360
isolation is relaxed it's very difficult

00:29:17,970 --> 00:29:24,240
to isolate the concurrent transactions

00:29:20,360 --> 00:29:26,310
now another thing is that your

00:29:24,240 --> 00:29:28,650
application now has to support this

00:29:26,310 --> 00:29:30,720
compensation logic which means that

00:29:28,650 --> 00:29:32,910
let's say you have an existing

00:29:30,720 --> 00:29:35,010
infrastructure of micro-services and now

00:29:32,910 --> 00:29:37,080
you go to every micro service ends and

00:29:35,010 --> 00:29:39,180
you go to every micro service and add

00:29:37,080 --> 00:29:41,520
that hey if this request happened this

00:29:39,180 --> 00:29:43,380
is how we compensate it if we did one

00:29:41,520 --> 00:29:45,810
booking we will basically cancel this

00:29:43,380 --> 00:29:50,720
booking and tell giver this coupon too

00:29:45,810 --> 00:29:54,180
because customer sorry so yeah another

00:29:50,720 --> 00:29:57,180
when a big problem is that now here you

00:29:54,180 --> 00:29:59,580
are basically allowing the user to read

00:29:57,180 --> 00:30:02,100
a data which might be rolled back

00:29:59,580 --> 00:30:04,230
eventually so that means so if you go

00:30:02,100 --> 00:30:07,740
back to the same example you made a

00:30:04,230 --> 00:30:09,360
booking in over your driver and booking

00:30:07,740 --> 00:30:11,760
so you need to mark your driver as busy

00:30:09,360 --> 00:30:12,870
your booking has to be allocated so the

00:30:11,760 --> 00:30:15,750
first thing which happened is your

00:30:12,870 --> 00:30:18,870
booking application gets the request it

00:30:15,750 --> 00:30:20,790
marks the booking is allocated but the

00:30:18,870 --> 00:30:22,590
driver is not busy yet so that thing is

00:30:20,790 --> 00:30:24,660
still in process at the same time

00:30:22,590 --> 00:30:26,520
another concurrent transaction comes

00:30:24,660 --> 00:30:28,410
into picture he says that hey give me

00:30:26,520 --> 00:30:30,000
all the free drivers available so now as

00:30:28,410 --> 00:30:32,250
the driver is not busy your database

00:30:30,000 --> 00:30:33,540
will eho this driver is free and it will

00:30:32,250 --> 00:30:36,210
alow kate the same booking to the same

00:30:33,540 --> 00:30:37,740
driver now what happens here that there

00:30:36,210 --> 00:30:39,540
is one driver who gets to booking

00:30:37,740 --> 00:30:40,890
so one booking he has to cancel because

00:30:39,540 --> 00:30:43,080
he can't complete two bookings at the

00:30:40,890 --> 00:30:45,480
same time there is one eligible driver

00:30:43,080 --> 00:30:47,310
who could not get the booking at all but

00:30:45,480 --> 00:30:50,010
because we've had the problem of new

00:30:47,310 --> 00:30:52,560
bookings and we I've seen this problem

00:30:50,010 --> 00:30:54,360
of having dual bookings or bad

00:30:52,560 --> 00:30:58,710
experiences with the customers due to

00:30:54,360 --> 00:31:00,060
all these things so now how do you solve

00:30:58,710 --> 00:31:02,670
this problem so this is good thing for

00:31:00,060 --> 00:31:04,440
example a lot of companies already how

00:31:02,670 --> 00:31:06,510
these things in place that hey sometimes

00:31:04,440 --> 00:31:08,370
you over book due to these reasons and

00:31:06,510 --> 00:31:11,549
then you eventually compensate

00:31:08,370 --> 00:31:14,669
but we could not do this because let's

00:31:11,549 --> 00:31:16,169
say there is one more bad thing is that

00:31:14,669 --> 00:31:18,059
you can't use this if you have some

00:31:16,169 --> 00:31:19,679
banking transactions so for example

00:31:18,059 --> 00:31:21,960
let's say you have a credit and you have

00:31:19,679 --> 00:31:23,220
a debit now you can always credit in

00:31:21,960 --> 00:31:25,260
someone's account but you can't debit

00:31:23,220 --> 00:31:27,320
without their concern so that means that

00:31:25,260 --> 00:31:31,289
there is no way you can compensate this

00:31:27,320 --> 00:31:33,270
so what if the kind of application you

00:31:31,289 --> 00:31:35,279
have does not support the compensation

00:31:33,270 --> 00:31:37,409
so then it again becomes very difficult

00:31:35,279 --> 00:31:40,590
to use this and you are then the only

00:31:37,409 --> 00:31:42,149
way you can do is two-phase commit so we

00:31:40,590 --> 00:31:44,730
were kind of in a similar situation that

00:31:42,149 --> 00:31:46,110
this was very difficult for us to use so

00:31:44,730 --> 00:31:48,690
we were just thinking what what else you

00:31:46,110 --> 00:31:51,539
can do is there any way that we can kind

00:31:48,690 --> 00:31:53,610
of make sure that the user all never

00:31:51,539 --> 00:31:55,799
reads this uncommitted data can we can

00:31:53,610 --> 00:32:00,120
we tweak these protocols to come to a

00:31:55,799 --> 00:32:04,080
middle ground so we read a lot of papers

00:32:00,120 --> 00:32:07,470
and there is this Dropbox paper on edge

00:32:04,080 --> 00:32:08,970
store so it talks about how they had

00:32:07,470 --> 00:32:12,480
similar problems and how did this all

00:32:08,970 --> 00:32:15,840
wait so based on that we also kind of

00:32:12,480 --> 00:32:17,880
designed a similar thing so it's not new

00:32:15,840 --> 00:32:20,730
it's just if you go out on the internet

00:32:17,880 --> 00:32:22,559
you surf for it there are many places

00:32:20,730 --> 00:32:24,659
you can find similar thing but we call

00:32:22,559 --> 00:32:27,510
it shadow so it's a distributed

00:32:24,659 --> 00:32:31,500
two-phase commit now let's see how we go

00:32:27,510 --> 00:32:33,450
about it so again we start with the same

00:32:31,500 --> 00:32:35,309
place where we were in saga so you again

00:32:33,450 --> 00:32:37,169
have a coordinator you have the driver

00:32:35,309 --> 00:32:39,840
application and you have the bookings

00:32:37,169 --> 00:32:42,210
application now we introduce one more

00:32:39,840 --> 00:32:45,570
loss unit here which is called business

00:32:42,210 --> 00:32:48,029
logic unit so what is business logic

00:32:45,570 --> 00:32:50,029
unit the business logic unit is

00:32:48,029 --> 00:32:52,710
basically we want to decouple the

00:32:50,029 --> 00:32:55,289
business logic from the actual

00:32:52,710 --> 00:32:57,390
transaction so this means that for

00:32:55,289 --> 00:32:59,460
example if you have to compensate or

00:32:57,390 --> 00:33:00,870
let's say you have to rollback all these

00:32:59,460 --> 00:33:03,450
things can be put into business logic

00:33:00,870 --> 00:33:06,870
unit and your coordinator can perform

00:33:03,450 --> 00:33:08,010
independently from that so if let's say

00:33:06,870 --> 00:33:09,960
tomorrow you want to add some

00:33:08,010 --> 00:33:12,000
application delete some application your

00:33:09,960 --> 00:33:14,220
coordinator or internal things doesn't

00:33:12,000 --> 00:33:16,530
have to change so that was the reason

00:33:14,220 --> 00:33:17,789
now also this unit it doesn't have to be

00:33:16,530 --> 00:33:20,510
a separate service or something it can

00:33:17,789 --> 00:33:22,340
be just a library or which you can maybe

00:33:20,510 --> 00:33:24,810
sidecar with the coordinate

00:33:22,340 --> 00:33:26,910
so now they request you get the request

00:33:24,810 --> 00:33:29,310
so it comes to the business logic unit

00:33:26,910 --> 00:33:30,480
now the business logic use unit is aware

00:33:29,310 --> 00:33:32,400
that hey there are these two

00:33:30,480 --> 00:33:34,980
applications and the transaction will

00:33:32,400 --> 00:33:37,170
start between these two applications so

00:33:34,980 --> 00:33:39,090
the first thing it does is it will go to

00:33:37,170 --> 00:33:42,660
the coordinator and it will ask for the

00:33:39,090 --> 00:33:45,660
transaction ID now this transaction ID

00:33:42,660 --> 00:33:47,460
is the key of the entire transaction so

00:33:45,660 --> 00:33:49,440
every time you want to know any detail

00:33:47,460 --> 00:33:52,080
about this transaction you can get with

00:33:49,440 --> 00:33:53,760
the transaction ID that is one the

00:33:52,080 --> 00:33:55,440
second thing is that the coordinator is

00:33:53,760 --> 00:33:56,820
the one who is always aware that what

00:33:55,440 --> 00:33:58,500
are the current of ongoing transaction

00:33:56,820 --> 00:34:00,210
what are the transaction which are in

00:33:58,500 --> 00:34:01,790
process because the first step in the

00:34:00,210 --> 00:34:05,610
transaction starts from the coordinator

00:34:01,790 --> 00:34:07,860
so you take this transaction ID and then

00:34:05,610 --> 00:34:09,390
you said that hey you go to the driver

00:34:07,860 --> 00:34:10,890
and bookings happened say hey there is

00:34:09,390 --> 00:34:13,530
this transaction which needs to be done

00:34:10,890 --> 00:34:16,040
between drivers and bookings so can you

00:34:13,530 --> 00:34:18,420
do this can you fulfill this request so

00:34:16,040 --> 00:34:20,850
now the driver application is obviously

00:34:18,420 --> 00:34:23,610
it's you're already billed microservice

00:34:20,850 --> 00:34:25,920
application and what our goal was that

00:34:23,610 --> 00:34:27,780
we don't want to change in any of the

00:34:25,920 --> 00:34:29,250
macro services if there is a possible

00:34:27,780 --> 00:34:31,560
way to do that we don't want to go to

00:34:29,250 --> 00:34:33,180
every team and tell them hey can we

00:34:31,560 --> 00:34:34,640
start this compensation can you support

00:34:33,180 --> 00:34:38,280
this for us can you support that for us

00:34:34,640 --> 00:34:42,270
so how do you do that so we introduced

00:34:38,280 --> 00:34:45,420
the shadow now what is shadow so shadow

00:34:42,270 --> 00:34:47,490
is basically a table which is exactly

00:34:45,420 --> 00:34:48,990
same or the schema same as the actual

00:34:47,490 --> 00:34:51,630
table so actual table is actually where

00:34:48,990 --> 00:34:53,730
your data gets inserted and shadow table

00:34:51,630 --> 00:34:56,340
is basically kind of the prepare phase

00:34:53,730 --> 00:34:57,630
of the two-phase commit so what is the

00:34:56,340 --> 00:34:59,790
purpose of the prepare phase in

00:34:57,630 --> 00:35:01,890
two-phase commit you want to save the

00:34:59,790 --> 00:35:03,380
data that hey this is these are the

00:35:01,890 --> 00:35:06,510
changes which will happen eventually

00:35:03,380 --> 00:35:09,060
then also you don't want to you don't

00:35:06,510 --> 00:35:10,710
want your user to see this data but if

00:35:09,060 --> 00:35:12,630
your user is reading from the actual

00:35:10,710 --> 00:35:14,010
table and if you put this retain the

00:35:12,630 --> 00:35:15,600
actual table he will read it from in

00:35:14,010 --> 00:35:18,060
there and then you it will be difficult

00:35:15,600 --> 00:35:20,100
for you to handle this so instead of

00:35:18,060 --> 00:35:22,140
putting it to the actual table you put

00:35:20,100 --> 00:35:24,060
it to the shadow table so exactly the

00:35:22,140 --> 00:35:25,590
same data he put into the Scituate able

00:35:24,060 --> 00:35:26,310
you add extra column with transaction ID

00:35:25,590 --> 00:35:28,560
and timestamp

00:35:26,310 --> 00:35:30,840
now the data will go into shadow table

00:35:28,560 --> 00:35:32,220
now what is shadow klein so now

00:35:30,840 --> 00:35:33,720
obviously as we said that we did not

00:35:32,220 --> 00:35:35,609
want make much changes in the

00:35:33,720 --> 00:35:37,109
application so this client will

00:35:35,609 --> 00:35:39,420
basically the request will come to your

00:35:37,109 --> 00:35:40,950
driver application then it will use the

00:35:39,420 --> 00:35:42,960
shadow client and shadow client will

00:35:40,950 --> 00:35:44,819
make the changes the all the change so

00:35:42,960 --> 00:35:46,950
for example if you had hey mark this

00:35:44,819 --> 00:35:48,329
driver is busy in this table shadow

00:35:46,950 --> 00:35:51,059
client will change that mark this driver

00:35:48,329 --> 00:35:53,519
as busy for the table for the shadow

00:35:51,059 --> 00:35:56,009
table of this actual table so it will

00:35:53,519 --> 00:35:58,920
change this and then finally the data

00:35:56,009 --> 00:36:01,319
will go to the shadow table so now what

00:35:58,920 --> 00:36:03,989
happened here so now you have at the

00:36:01,319 --> 00:36:06,029
data available with your system and now

00:36:03,989 --> 00:36:08,130
if I see that let's say that your

00:36:06,029 --> 00:36:09,869
transaction is success and later at some

00:36:08,130 --> 00:36:11,940
point of time I will make sure that it

00:36:09,869 --> 00:36:14,309
the data goes to the actual table

00:36:11,940 --> 00:36:16,319
eventually for you but now how do you

00:36:14,309 --> 00:36:18,059
read this data how do you how do you

00:36:16,319 --> 00:36:20,069
know that when it will get committed how

00:36:18,059 --> 00:36:22,019
do you know that when how do the user

00:36:20,069 --> 00:36:24,900
will get that if it succeeds or failed

00:36:22,019 --> 00:36:27,359
so we had this one more unit you know

00:36:24,900 --> 00:36:30,029
everyone is aware what is vulgar we just

00:36:27,359 --> 00:36:32,609
we already had this thing in our

00:36:30,029 --> 00:36:34,859
previous slides so the purpose of Allah

00:36:32,609 --> 00:36:37,170
is that it can take the data it's like

00:36:34,859 --> 00:36:39,630
the bin log mining so it will read from

00:36:37,170 --> 00:36:42,809
the bin log of the shadow table so and

00:36:39,630 --> 00:36:44,880
then it will put them into this Kafka so

00:36:42,809 --> 00:36:46,289
we already had this cough car cluster

00:36:44,880 --> 00:36:48,690
across so we'll put it into Kafka in a

00:36:46,289 --> 00:36:51,269
specific topic that hey there is this

00:36:48,690 --> 00:36:53,759
transaction which was running and this

00:36:51,269 --> 00:36:55,789
is now in this topic and the coordinator

00:36:53,759 --> 00:36:57,720
will be reading from the same topic so

00:36:55,789 --> 00:36:59,999
now as I already told you that

00:36:57,720 --> 00:37:02,039
coordinator is the one who gave the

00:36:59,999 --> 00:37:03,509
initial transaction ID so that means it

00:37:02,039 --> 00:37:05,249
keeps it that hey there is this

00:37:03,509 --> 00:37:06,749
transaction started few milliseconds ago

00:37:05,249 --> 00:37:10,019
few seconds ago based on your latency

00:37:06,749 --> 00:37:13,440
and this was still in process state so

00:37:10,019 --> 00:37:16,499
the coordinator gets these two requests

00:37:13,440 --> 00:37:18,509
these two events from the Kafka and when

00:37:16,499 --> 00:37:21,539
the coordinator gives get these two

00:37:18,509 --> 00:37:24,569
events now it knows that hey this

00:37:21,539 --> 00:37:25,829
transaction the it gets the event that

00:37:24,569 --> 00:37:27,839
means that the transaction data is

00:37:25,829 --> 00:37:29,880
available in the shadow table and at

00:37:27,839 --> 00:37:32,339
that point of time you just mark it

00:37:29,880 --> 00:37:35,309
commit that okay we got the data we

00:37:32,339 --> 00:37:37,859
successfully done with your request and

00:37:35,309 --> 00:37:39,569
we mark it as commit and it will put

00:37:37,859 --> 00:37:40,890
back this you went into Kafka that hey

00:37:39,569 --> 00:37:42,749
this transaction is committed

00:37:40,890 --> 00:37:44,430
taking it do whatever should you want to

00:37:42,749 --> 00:37:46,230
do and the job of the coordinator and

00:37:44,430 --> 00:37:48,299
business logic unit is done here and

00:37:46,230 --> 00:37:48,990
your user gets the that hey this

00:37:48,299 --> 00:37:50,250
transaction

00:37:48,990 --> 00:37:53,430
this booking you did and you were

00:37:50,250 --> 00:37:55,470
successfully able to do it now how does

00:37:53,430 --> 00:37:58,830
it actually move to the actual table is

00:37:55,470 --> 00:38:00,810
basically now my transition is complete

00:37:58,830 --> 00:38:03,960
and then again the volga will read back

00:38:00,810 --> 00:38:06,210
this event from Kafka and it will go to

00:38:03,960 --> 00:38:09,210
the shadow table and mark this

00:38:06,210 --> 00:38:11,010
transaction as complete so now the

00:38:09,210 --> 00:38:12,750
transition is complete now the only

00:38:11,010 --> 00:38:15,030
thing you need to worry about how do you

00:38:12,750 --> 00:38:17,640
make the user to read this data such

00:38:15,030 --> 00:38:21,089
that you always make sure that user

00:38:17,640 --> 00:38:22,680
reads the committed data so it goes to

00:38:21,089 --> 00:38:24,480
the shadow table mask so there is one

00:38:22,680 --> 00:38:26,550
more component to this which we haven't

00:38:24,480 --> 00:38:29,790
added in these slides but it's like more

00:38:26,550 --> 00:38:32,280
an a synchronous application or library

00:38:29,790 --> 00:38:34,680
or maybe job or whatever you see it but

00:38:32,280 --> 00:38:36,480
it's more like it will go to the shadow

00:38:34,680 --> 00:38:37,740
table it will read from the shadow table

00:38:36,480 --> 00:38:39,420
that what are the transition which are

00:38:37,740 --> 00:38:41,760
incomplete state and move to the actual

00:38:39,420 --> 00:38:43,859
table so that means that eventually as

00:38:41,760 --> 00:38:45,000
we previously said that eventually we

00:38:43,859 --> 00:38:47,460
will make sure that your data is

00:38:45,000 --> 00:38:50,310
available into actual table and we'll

00:38:47,460 --> 00:38:52,200
keep removing the data from shadow table

00:38:50,310 --> 00:38:54,210
so at any point of time the shadow table

00:38:52,200 --> 00:38:56,369
will have some transactions which are in

00:38:54,210 --> 00:38:58,770
processing state some transactions which

00:38:56,369 --> 00:39:00,950
are incomplete state and that will be

00:38:58,770 --> 00:39:03,660
the state of the shadow tables so now

00:39:00,950 --> 00:39:06,270
let's see how do you handle reads from

00:39:03,660 --> 00:39:09,030
how do you define that we have a

00:39:06,270 --> 00:39:12,330
consistent read so just to simplify this

00:39:09,030 --> 00:39:13,770
diagram so again you start with the

00:39:12,330 --> 00:39:14,910
booking app you have the shadow client

00:39:13,770 --> 00:39:17,160
you have the coordinator and you have

00:39:14,910 --> 00:39:18,630
your database so the request comes to

00:39:17,160 --> 00:39:22,020
this application hey can you give me

00:39:18,630 --> 00:39:23,460
this booking with the ID one so what the

00:39:22,020 --> 00:39:24,720
first thing we have this year declined

00:39:23,460 --> 00:39:26,339
so the booking app will send it to

00:39:24,720 --> 00:39:27,839
shadow plying that hey there is this

00:39:26,339 --> 00:39:30,330
request to read this data

00:39:27,839 --> 00:39:32,099
so what shadow client will do is it will

00:39:30,330 --> 00:39:34,140
go to the coordinator and it will ask

00:39:32,099 --> 00:39:36,450
hey what was the last committed ID you

00:39:34,140 --> 00:39:39,630
had and now coordinator is aware that

00:39:36,450 --> 00:39:41,849
hey this was the last committed ID I had

00:39:39,630 --> 00:39:43,680
and that means that before this

00:39:41,849 --> 00:39:46,020
committed ID all the IDS are committed

00:39:43,680 --> 00:39:48,119
or failed or they have they're not in

00:39:46,020 --> 00:39:50,369
some transient state or they are not in

00:39:48,119 --> 00:39:52,349
between they have a final status already

00:39:50,369 --> 00:39:55,560
before this committed ID so shadow

00:39:52,349 --> 00:39:58,260
grants gets this committed ID and what

00:39:55,560 --> 00:39:59,820
it does it does few things so now what

00:39:58,260 --> 00:40:02,040
it first do is okay this was my

00:39:59,820 --> 00:40:03,210
committed idea this is the requests okay

00:40:02,040 --> 00:40:05,790
so it will work

00:40:03,210 --> 00:40:08,580
give me all the requests let's say which

00:40:05,790 --> 00:40:10,020
has with IDs equation one or ID less

00:40:08,580 --> 00:40:12,780
than equal to one or whatever is your

00:40:10,020 --> 00:40:14,910
where clause so it will get this data

00:40:12,780 --> 00:40:17,310
from both the actual table and shadow

00:40:14,910 --> 00:40:18,990
table then it will join that data and

00:40:17,310 --> 00:40:22,470
then it will filter based only last

00:40:18,990 --> 00:40:24,180
committed ID so that means that now it

00:40:22,470 --> 00:40:25,980
has to get the data from the actual and

00:40:24,180 --> 00:40:28,560
shadow table because there might be a

00:40:25,980 --> 00:40:30,810
point where your data is not yet in the

00:40:28,560 --> 00:40:33,150
actual table it is still in shadow we we

00:40:30,810 --> 00:40:34,589
are still in the process of moving it to

00:40:33,150 --> 00:40:36,480
the actual table so you have to read

00:40:34,589 --> 00:40:39,270
from both so that you you make sure that

00:40:36,480 --> 00:40:41,040
your data is being read and even as soon

00:40:39,270 --> 00:40:43,619
as you send these success status to the

00:40:41,040 --> 00:40:45,270
user then it will Union the result you

00:40:43,619 --> 00:40:48,630
have to do it in order to get the final

00:40:45,270 --> 00:40:50,580
result and it will filter so the

00:40:48,630 --> 00:40:52,020
filtering will make sure that you don't

00:40:50,580 --> 00:40:54,359
read anything which are still in

00:40:52,020 --> 00:40:56,520
processing state so the filtering will

00:40:54,359 --> 00:40:58,859
make sure that just read the

00:40:56,520 --> 00:41:02,010
transactions which are either which

00:40:58,859 --> 00:41:03,750
completely success and that's how it

00:41:02,010 --> 00:41:06,540
will get this data and finally returned

00:41:03,750 --> 00:41:08,220
to the user so now in this way you make

00:41:06,540 --> 00:41:10,170
sure that you always read the committed

00:41:08,220 --> 00:41:12,570
data so now this cannot happen that if

00:41:10,170 --> 00:41:14,369
there is a driver which are or there is

00:41:12,570 --> 00:41:16,859
a transaction which are still ongoing

00:41:14,369 --> 00:41:19,140
it will never read that or it will wait

00:41:16,859 --> 00:41:21,150
until the transaction is complete if

00:41:19,140 --> 00:41:25,710
this right request happened before any

00:41:21,150 --> 00:41:27,510
read so now the question becomes what

00:41:25,710 --> 00:41:30,839
are the good things about it you have a

00:41:27,510 --> 00:41:32,280
slow reads that's true because you might

00:41:30,839 --> 00:41:33,660
say that maybe when we do the join

00:41:32,280 --> 00:41:36,180
operation and filtering it will take a

00:41:33,660 --> 00:41:38,520
lot of time but that not true because

00:41:36,180 --> 00:41:39,900
your shadow table is always small as we

00:41:38,520 --> 00:41:42,270
are continuously moving the data from

00:41:39,900 --> 00:41:44,240
shadow table so it will mostly have the

00:41:42,270 --> 00:41:46,920
ongoing transaction most of the time

00:41:44,240 --> 00:41:49,170
it's complex to implement reason being

00:41:46,920 --> 00:41:51,810
that you now have to implement this

00:41:49,170 --> 00:41:53,520
coordinator the business logic unit you

00:41:51,810 --> 00:41:56,430
have to implement this different client

00:41:53,520 --> 00:42:00,119
you need to have this either this Kafka

00:41:56,430 --> 00:42:02,450
pipeline in place already so those are

00:42:00,119 --> 00:42:04,470
like there are many components to it

00:42:02,450 --> 00:42:06,780
good part is that you don't lock

00:42:04,470 --> 00:42:08,700
anything so your protocol make sure that

00:42:06,780 --> 00:42:11,520
you always get get the committed or

00:42:08,700 --> 00:42:12,900
consistent data you get the read

00:42:11,520 --> 00:42:16,410
committed isolation you get the read

00:42:12,900 --> 00:42:18,059
committed consistency levels so now

00:42:16,410 --> 00:42:20,579
this is the comparison between the two

00:42:18,059 --> 00:42:22,410
phase commit sagas and shadows so the

00:42:20,579 --> 00:42:23,759
consistency in two phase commit here we

00:42:22,410 --> 00:42:25,950
are discussing about when you lock

00:42:23,759 --> 00:42:28,650
everything so it will be linearizable in

00:42:25,950 --> 00:42:30,900
sagas you get eventually it will come to

00:42:28,650 --> 00:42:32,640
a consistent state shadows is Detroit's

00:42:30,900 --> 00:42:34,200
route committed when we talk about

00:42:32,640 --> 00:42:35,819
isolation levels two-phase commit is

00:42:34,200 --> 00:42:37,680
serializable less rhythmic use we are

00:42:35,819 --> 00:42:39,299
locking everything we are not allowing

00:42:37,680 --> 00:42:39,920
two transaction to interfere with each

00:42:39,299 --> 00:42:42,660
other

00:42:39,920 --> 00:42:44,880
sagas is mostly relaxed isolation

00:42:42,660 --> 00:42:46,619
obviously you can maybe I'm not sure if

00:42:44,880 --> 00:42:49,950
you can change some implementation and

00:42:46,619 --> 00:42:52,769
make sure that okay we somehow if you go

00:42:49,950 --> 00:42:53,910
to a good or better isolation levels but

00:42:52,769 --> 00:42:55,829
shadows will make sure that you always

00:42:53,910 --> 00:42:59,609
get the lead committed isolation levels

00:42:55,829 --> 00:43:02,970
now what to use it's it's very difficult

00:42:59,609 --> 00:43:05,630
question to answer that which protocol

00:43:02,970 --> 00:43:08,069
might be a good fit for your system so

00:43:05,630 --> 00:43:09,990
there are like there are so many factors

00:43:08,069 --> 00:43:12,599
to it you know that you need to know

00:43:09,990 --> 00:43:16,470
that what is the consistency levels you

00:43:12,599 --> 00:43:18,980
want to sorry what is the consistency

00:43:16,470 --> 00:43:21,869
levels you want to have in your system

00:43:18,980 --> 00:43:25,079
what is the isolation they will you need

00:43:21,869 --> 00:43:28,349
what is the latency and throughput

00:43:25,079 --> 00:43:29,970
system if maybe you can it's okay for

00:43:28,349 --> 00:43:31,380
you to have some latency you don't have

00:43:29,970 --> 00:43:33,420
that much of throughput but you want

00:43:31,380 --> 00:43:35,220
like very strong consistency and

00:43:33,420 --> 00:43:36,630
serializable isolation so then maybe

00:43:35,220 --> 00:43:39,720
your two-phase commit can be better fit

00:43:36,630 --> 00:43:43,049
for you it depends what you can

00:43:39,720 --> 00:43:44,369
compromise it as we know that in capture

00:43:43,049 --> 00:43:46,140
and partition tolerance will always be

00:43:44,369 --> 00:43:49,650
there so you have to choose one between

00:43:46,140 --> 00:43:51,390
the consistency or high availability so

00:43:49,650 --> 00:43:54,089
what is more important to you and where

00:43:51,390 --> 00:43:56,519
you do the trade-offs so I think based

00:43:54,089 --> 00:43:58,230
on all these factors and if you are know

00:43:56,519 --> 00:44:00,059
if you know that okay these these are my

00:43:58,230 --> 00:44:01,650
requirements so I think if you if we are

00:44:00,059 --> 00:44:04,410
perfectly aware of all of these then you

00:44:01,650 --> 00:44:08,519
can choose a protocol whatever suit

00:44:04,410 --> 00:44:10,820
suits your needs so that's all from our

00:44:08,519 --> 00:44:17,689
side thank you

00:44:10,820 --> 00:44:17,689
[Applause]

00:44:27,740 --> 00:44:33,839
could you please show the slide of

00:44:30,920 --> 00:44:37,740
regarding the microcell immigration or

00:44:33,839 --> 00:44:44,000
in the modern lead and microservices

00:44:37,740 --> 00:44:44,000
working in parallel with MySQL database

00:44:45,230 --> 00:44:56,099
yeah is this 100 or even previous yeah

00:44:51,750 --> 00:44:59,040
this one this one the question are all

00:44:56,099 --> 00:45:01,829
the right operations are working in this

00:44:59,040 --> 00:45:05,970
case so imagine you have two tables

00:45:01,829 --> 00:45:07,970
driver and begins some on a little write

00:45:05,970 --> 00:45:11,910
something in this table how the

00:45:07,970 --> 00:45:13,440
microservices - in so they have own copy

00:45:11,910 --> 00:45:18,569
of the table and here somehow

00:45:13,440 --> 00:45:21,540
synchronize the data or its use other

00:45:18,569 --> 00:45:23,400
instance of the database or write data

00:45:21,540 --> 00:45:26,940
in the same table it's just not quite

00:45:23,400 --> 00:45:29,730
clear for me okay sorry for the lack of

00:45:26,940 --> 00:45:31,680
clarity first of all the idea is it here

00:45:29,730 --> 00:45:34,079
is that of course as you're pointing out

00:45:31,680 --> 00:45:35,609
you don't want to end oh you're right

00:45:34,079 --> 00:45:40,490
traffic in two different point because

00:45:35,609 --> 00:45:44,849
then you have a problem of merging this

00:45:40,490 --> 00:45:48,299
state which is updated in two different

00:45:44,849 --> 00:45:51,000
sides so the catch here is that in this

00:45:48,299 --> 00:45:54,270
in this case in the back end of your

00:45:51,000 --> 00:45:58,170
microservices you still are wearing the

00:45:54,270 --> 00:46:00,750
right api from the hold monolith so

00:45:58,170 --> 00:46:01,890
basically still the part of the request

00:46:00,750 --> 00:46:04,170
the request comes in to your

00:46:01,890 --> 00:46:07,740
micro-services but then is dispatched on

00:46:04,170 --> 00:46:09,270
in your monolith like this you don't

00:46:07,740 --> 00:46:12,089
have actually - you don't have to have

00:46:09,270 --> 00:46:13,470
this problem because the right goes only

00:46:12,089 --> 00:46:15,510
there is only one source of truth for

00:46:13,470 --> 00:46:18,000
right there is no way we can handle

00:46:15,510 --> 00:46:21,290
without like conflict resolution

00:46:18,000 --> 00:46:21,290
algorithm there is no way we can answer

00:46:24,289 --> 00:46:34,680
yes yes that's that's why the migration

00:46:32,969 --> 00:46:36,869
is incremental in the in this in this

00:46:34,680 --> 00:46:41,910
face the microservice is still a very

00:46:36,869 --> 00:46:44,940
thin layer which is is already on the

00:46:41,910 --> 00:46:47,369
link maybe so if your question is about

00:46:44,940 --> 00:46:50,489
if you I have to add the new API when

00:46:47,369 --> 00:46:52,739
I'm a-gonna had that that's yes you can

00:46:50,489 --> 00:46:55,049
see lab this was a simplified model here

00:46:52,739 --> 00:46:55,799
of course the my other purpose of the

00:46:55,049 --> 00:46:57,900
micro-services

00:46:55,799 --> 00:47:00,660
which is already there it is not to

00:46:57,900 --> 00:47:03,449
prevent new feature to come means and

00:47:00,660 --> 00:47:05,099
being served but it's you don't want at

00:47:03,449 --> 00:47:06,749
the same time to go and implement this

00:47:05,099 --> 00:47:08,549
feature in this monolith that you'd want

00:47:06,749 --> 00:47:10,109
to touch as little as possible so yes

00:47:08,549 --> 00:47:12,630
you're right if you're ready a - a

00:47:10,109 --> 00:47:13,979
strategy of a new right model that you

00:47:12,630 --> 00:47:15,479
want to implement in the future yes

00:47:13,979 --> 00:47:18,989
there would be another database there

00:47:15,479 --> 00:47:21,509
with your right model of course this

00:47:18,989 --> 00:47:25,709
prevent you to do operational relational

00:47:21,509 --> 00:47:28,589
operation like foreign key Zappa datum

00:47:25,709 --> 00:47:30,449
yes exactly so for that for that that's

00:47:28,589 --> 00:47:32,009
when the transactions are coming into

00:47:30,449 --> 00:47:33,809
place you have to have like sort of

00:47:32,009 --> 00:47:36,239
coordination protocol lets us allow you

00:47:33,809 --> 00:47:40,319
to write in two different place in a

00:47:36,239 --> 00:47:42,420
lockstep that kind right immigration

00:47:40,319 --> 00:47:43,979
becomes very complicated so for us the

00:47:42,420 --> 00:47:47,759
approach that we use the most often was

00:47:43,979 --> 00:47:53,069
to keep the data in the in the old in

00:47:47,759 --> 00:47:55,650
the world right model and and keep using

00:47:53,069 --> 00:47:57,769
the new model for for only for reading

00:47:55,650 --> 00:48:00,569
that was the approach that for the

00:47:57,769 --> 00:48:02,249
business critical application we took in

00:48:00,569 --> 00:48:05,430
place because for us the real deal was

00:48:02,249 --> 00:48:08,759
there to get rid of that big monolith

00:48:05,430 --> 00:48:11,779
from the from the dependency but more

00:48:08,759 --> 00:48:15,029
than the data model itself in my sequel

00:48:11,779 --> 00:48:16,410
but yeah they're the worst the worst

00:48:15,029 --> 00:48:17,699
team the word team that we're dealing

00:48:16,410 --> 00:48:20,940
with the problem of writing the

00:48:17,699 --> 00:48:22,949
migration sorry migrating the right

00:48:20,940 --> 00:48:25,999
model in parallel and for those they

00:48:22,949 --> 00:48:28,650
created very complicated tool to migrate

00:48:25,999 --> 00:48:31,289
accounts were performing this migration

00:48:28,650 --> 00:48:33,150
Allah Perkins basis yes but this was

00:48:31,289 --> 00:48:35,190
like just a simplified version of this

00:48:33,150 --> 00:48:36,660
migration of course I didn't go into the

00:48:35,190 --> 00:48:38,670
details about migrate

00:48:36,660 --> 00:48:43,910
the right model I agree with you it's

00:48:38,670 --> 00:48:43,910
very complicated problem to solve yeah

00:48:50,180 --> 00:48:56,059
my question was is Volga open source and

00:48:53,730 --> 00:49:00,569
it says something we can all use and

00:48:56,059 --> 00:49:03,000
second when it replicates bin logs does

00:49:00,569 --> 00:49:05,099
it support replicating this statement

00:49:03,000 --> 00:49:15,359
bin logs or do you have to replicate the

00:49:05,099 --> 00:49:17,579
entire row record correctly Volga is not

00:49:15,359 --> 00:49:20,130
open source there are a bunch of project

00:49:17,579 --> 00:49:22,530
that do lock mining with different

00:49:20,130 --> 00:49:24,960
strategy we differ for example I see

00:49:22,530 --> 00:49:27,539
there is a tool pirate that called the B

00:49:24,960 --> 00:49:29,369
zoom there is plenty of tool that have

00:49:27,539 --> 00:49:31,049
the same thing the vision does something

00:49:29,369 --> 00:49:33,390
a little bit different than us it's

00:49:31,049 --> 00:49:38,069
tailored for change data capture we

00:49:33,390 --> 00:49:42,660
tolerate Volga to support to support in

00:49:38,069 --> 00:49:45,059
sync replicas so about the role based a

00:49:42,660 --> 00:49:47,369
statement so we use a row based the

00:49:45,059 --> 00:49:49,200
replication under the hood and reason

00:49:47,369 --> 00:49:51,539
being is that actually we don't want to

00:49:49,200 --> 00:49:55,160
include in this issue when we replicate

00:49:51,539 --> 00:50:00,059
for example foreign keys or transactions

00:49:55,160 --> 00:50:01,109
the tool yes tool currently sorry I did

00:50:00,059 --> 00:50:03,089
answer the previous question the tool

00:50:01,109 --> 00:50:05,490
currently is not open source we're

00:50:03,089 --> 00:50:07,049
working on it let's see if we handle the

00:50:05,490 --> 00:50:11,250
bureaucracy within the company to do

00:50:07,049 --> 00:50:16,430
that probably follow us we'll keep you

00:50:11,250 --> 00:50:16,430
updated thank you

00:50:21,980 --> 00:50:27,570
thanks for the representation I think

00:50:24,480 --> 00:50:29,940
you made a very good let's say overview

00:50:27,570 --> 00:50:33,510
of how hard is to keep consistency in

00:50:29,940 --> 00:50:37,130
the support assistance with the Salvos

00:50:33,510 --> 00:50:39,690
implementation I see that you are

00:50:37,130 --> 00:50:42,960
becoming even more dissipated okay

00:50:39,690 --> 00:50:47,490
because you are having in Sado tables

00:50:42,960 --> 00:50:49,200
you are reading Kafka so if I think in a

00:50:47,490 --> 00:50:51,810
recovery scenario for instance you need

00:50:49,200 --> 00:50:53,580
you need to take a backup and then you

00:50:51,810 --> 00:50:56,190
need to restore it then I mean

00:50:53,580 --> 00:50:57,540
everything is even less clear okay

00:50:56,190 --> 00:50:59,700
because you have this other tables you

00:50:57,540 --> 00:51:01,470
have some state in the Kafka so I don't

00:50:59,700 --> 00:51:04,140
know you have come you can come in

00:51:01,470 --> 00:51:06,120
something about that or how to solve

00:51:04,140 --> 00:51:15,150
that disaster recovery in this

00:51:06,120 --> 00:51:16,890
implementation haven't done that so the

00:51:15,150 --> 00:51:20,160
disaster recovery thing so right now I

00:51:16,890 --> 00:51:22,410
think as we are using Volga to basically

00:51:20,160 --> 00:51:24,960
replicate the data so we don't really

00:51:22,410 --> 00:51:26,970
what you can do is basically when you

00:51:24,960 --> 00:51:29,100
replicate the actual table so the

00:51:26,970 --> 00:51:32,150
snapshot are like time snapshots so at

00:51:29,100 --> 00:51:36,120
any point of time you take the entire

00:51:32,150 --> 00:51:37,620
backup of your database and then of what

00:51:36,120 --> 00:51:38,940
we are doing right now we are doing we

00:51:37,620 --> 00:51:40,860
are currently in the implementation of

00:51:38,940 --> 00:51:44,700
this checkpointing thing so what we do

00:51:40,860 --> 00:51:46,740
is we keep this data in the rocks DB in

00:51:44,700 --> 00:51:49,260
the local machine and then after every

00:51:46,740 --> 00:51:53,190
five minutes we upload this entire rocks

00:51:49,260 --> 00:51:55,320
Ruby to AWS and then from LW s you can

00:51:53,190 --> 00:51:56,760
create a my rocks instance and then

00:51:55,320 --> 00:51:59,100
which you can use just in case if

00:51:56,760 --> 00:52:01,140
something fails so that's what we are

00:51:59,100 --> 00:52:04,200
trying to do that is still in progress

00:52:01,140 --> 00:52:06,570
so it will be very difficult that how it

00:52:04,200 --> 00:52:09,180
will turn out but I think we did some

00:52:06,570 --> 00:52:11,640
tests with it and it should be doable

00:52:09,180 --> 00:52:13,550
but right now we don't have that in

00:52:11,640 --> 00:52:18,780
place yet we are it's still in progress

00:52:13,550 --> 00:52:21,840
so I want to comment also so this these

00:52:18,780 --> 00:52:25,410
are are not output this is not an

00:52:21,840 --> 00:52:27,750
algorithm that we invented let's say

00:52:25,410 --> 00:52:29,490
this is a well-known strategy that a lot

00:52:27,750 --> 00:52:31,710
of companies using I can mention some

00:52:29,490 --> 00:52:33,780
product that is the previous called fast

00:52:31,710 --> 00:52:35,960
car in Alibaba now becoming as

00:52:33,780 --> 00:52:40,290
at a project there is edge store for

00:52:35,960 --> 00:52:42,060
used a Dropbox for cross charting comets

00:52:40,290 --> 00:52:44,310
so there is a lot of of tools that are

00:52:42,060 --> 00:52:46,140
going into this direction there is there

00:52:44,310 --> 00:52:47,820
should be a parallel triangle in my

00:52:46,140 --> 00:52:49,890
opinion for aspect to the capture mm

00:52:47,820 --> 00:52:52,260
that is our three dimension that is

00:52:49,890 --> 00:52:54,480
complexity latency and I mean you have

00:52:52,260 --> 00:52:55,800
to mention other stuff other other stuff

00:52:54,480 --> 00:52:58,050
into the game of course for a small

00:52:55,800 --> 00:53:00,960
presentation like this that cap theorem

00:52:58,050 --> 00:53:05,220
is very useful tool to explain the

00:53:00,960 --> 00:53:07,350
trade-off yes it's very complex but if

00:53:05,220 --> 00:53:10,380
you want to gain that consistency step

00:53:07,350 --> 00:53:12,720
and that isolation step usually you

00:53:10,380 --> 00:53:15,140
really have to do manage this complexity

00:53:12,720 --> 00:53:20,670
or you have to pay the cost of latency

00:53:15,140 --> 00:53:24,300
for the two-phase commit but yes these

00:53:20,670 --> 00:53:28,740
are very complex problem and that's why

00:53:24,300 --> 00:53:31,260
databases or product that solve this

00:53:28,740 --> 00:53:34,770
problem for us and nowadays are getting

00:53:31,260 --> 00:53:38,930
so so popular see the motto in the club

00:53:34,770 --> 00:53:41,250
all the ecosystem of the cloud the cloud

00:53:38,930 --> 00:53:44,310
data stores that are solving this

00:53:41,250 --> 00:53:46,650
problem for us so because it's not not

00:53:44,310 --> 00:53:50,670
everything can afford to invest money in

00:53:46,650 --> 00:53:52,950
in solving this issue so yeah yeah I

00:53:50,670 --> 00:53:54,480
think if we manage to come again next

00:53:52,950 --> 00:53:57,660
year maybe we'll be able to give you

00:53:54,480 --> 00:53:59,340
better statistics on the performance and

00:53:57,660 --> 00:54:00,900
data recovery and on everything but

00:53:59,340 --> 00:54:09,930
right now the work is still in progress

00:54:00,900 --> 00:54:14,610
so thank you oh no I think he hired

00:54:09,930 --> 00:54:16,860
occasion could you could you switch to

00:54:14,610 --> 00:54:19,340
the slide we've distributed to face

00:54:16,860 --> 00:54:19,340
commit

00:54:25,330 --> 00:54:30,610
distributed the distributor to face

00:54:28,220 --> 00:54:30,610
carriers

00:54:34,900 --> 00:54:41,150
okay that's fine so the way I understand

00:54:38,720 --> 00:54:42,980
this is the and micro service for

00:54:41,150 --> 00:54:45,200
everything strongly relies on

00:54:42,980 --> 00:54:47,540
coordinator so what happens if the

00:54:45,200 --> 00:54:49,880
coordinator fries because the way I see

00:54:47,540 --> 00:54:52,730
it if coordinator fries then basically

00:54:49,880 --> 00:54:55,550
the whole system fails so the

00:54:52,730 --> 00:54:57,410
coordinator so first of all it's not so

00:54:55,550 --> 00:54:59,090
the coordinator is like think about it

00:54:57,410 --> 00:55:02,240
it's a very small application which just

00:54:59,090 --> 00:55:04,940
keeps the transaction data so what you

00:55:02,240 --> 00:55:07,520
do is basically after every some time

00:55:04,940 --> 00:55:08,330
period first you have multiple instances

00:55:07,520 --> 00:55:11,930
of it

00:55:08,330 --> 00:55:14,690
so which can reduce the failure scenario

00:55:11,930 --> 00:55:16,430
second we try to make it as a catalyst

00:55:14,690 --> 00:55:17,840
stateless as possible so you keep the

00:55:16,430 --> 00:55:20,630
check points in your database that this

00:55:17,840 --> 00:55:22,940
is an ongoing and these are this is what

00:55:20,630 --> 00:55:25,010
currently having so what happens if the

00:55:22,940 --> 00:55:26,210
coordinator goes down if there is a

00:55:25,010 --> 00:55:29,030
transition which are already committed

00:55:26,210 --> 00:55:30,380
it want to fit that but it will affect

00:55:29,030 --> 00:55:33,020
the ongoing transactions

00:55:30,380 --> 00:55:35,210
so whatever transactions will be in

00:55:33,020 --> 00:55:36,830
ongoing there will always be a timeout

00:55:35,210 --> 00:55:39,170
on those transactions so in worst case

00:55:36,830 --> 00:55:40,820
scenario if all the instances goes down

00:55:39,170 --> 00:55:42,860
and you are not able to collect your

00:55:40,820 --> 00:55:44,840
data there is a possibility that some of

00:55:42,860 --> 00:55:46,490
the ongoing transactions will fail but

00:55:44,840 --> 00:55:48,650
eventually when it will come back it

00:55:46,490 --> 00:55:52,450
will read the state and it will start

00:55:48,650 --> 00:55:52,450
from the state where it went down

00:56:00,750 --> 00:56:05,109
and again this is another problem that

00:56:03,369 --> 00:56:07,510
is very common I do know for example if

00:56:05,109 --> 00:56:09,820
you have ever run into a software tool

00:56:07,510 --> 00:56:11,470
called Orchestrator that my sequel that

00:56:09,820 --> 00:56:17,050
get abused for I availability of there

00:56:11,470 --> 00:56:19,420
my sequel that a master the whenever you

00:56:17,050 --> 00:56:21,160
have like a tool the does coordination

00:56:19,420 --> 00:56:23,200
as you say it becomes very easily single

00:56:21,160 --> 00:56:27,010
point of failure so that application

00:56:23,200 --> 00:56:29,349
must be itself being designed to be

00:56:27,010 --> 00:56:31,079
highly available so there is different

00:56:29,349 --> 00:56:34,150
instance of this guy going in parallel

00:56:31,079 --> 00:56:35,710
working in parallel if as DBA was saying

00:56:34,150 --> 00:56:38,140
we don't number we don't we didn't want

00:56:35,710 --> 00:56:40,630
whether like a coordination protocol

00:56:38,140 --> 00:56:42,880
within the coordinator so if you lose

00:56:40,630 --> 00:56:44,230
the coordinator while the transaction is

00:56:42,880 --> 00:56:45,460
ongoing you lose the state of that

00:56:44,230 --> 00:56:48,010
transaction but the other transaction

00:56:45,460 --> 00:56:50,380
observed by other applications running

00:56:48,010 --> 00:56:55,690
in parallel if this was your question

00:56:50,380 --> 00:56:57,910
but yes if at the moment to reduce the

00:56:55,690 --> 00:57:02,890
complexity of that piece of code we

00:56:57,910 --> 00:57:05,140
didn't want to be earlier to do

00:57:02,890 --> 00:57:07,210
something about the the state of young

00:57:05,140 --> 00:57:10,450
online transaction which is in progress

00:57:07,210 --> 00:57:12,550
so we failed the transaction if this was

00:57:10,450 --> 00:57:14,170
your question I think how the

00:57:12,550 --> 00:57:17,109
transaction fails is let's say at any

00:57:14,170 --> 00:57:18,730
point of time you are not able to send

00:57:17,109 --> 00:57:20,440
this data to Kafka or let's say the

00:57:18,730 --> 00:57:21,910
final come it did not happen so then

00:57:20,440 --> 00:57:23,710
every transaction will have a time out

00:57:21,910 --> 00:57:25,720
of your system for example let's say if

00:57:23,710 --> 00:57:27,640
the transaction does not go in a

00:57:25,720 --> 00:57:28,900
committed state after let's say one

00:57:27,640 --> 00:57:31,630
second then we will just fail the

00:57:28,900 --> 00:57:32,980
transaction because it also if we do not

00:57:31,630 --> 00:57:34,720
fail the transaction then we will also

00:57:32,980 --> 00:57:36,460
create a problem with the concurrent

00:57:34,720 --> 00:57:37,750
transaction when reads and writes are

00:57:36,460 --> 00:57:41,829
happening at the same time

00:57:37,750 --> 00:57:43,119
so in order to solve that you just we

00:57:41,829 --> 00:57:44,920
just fail the transition so that the

00:57:43,119 --> 00:57:47,500
user can retry the transaction instead

00:57:44,920 --> 00:57:50,190
of going in an inconsistent state or for

00:57:47,500 --> 00:57:50,190
the entire system

00:58:00,589 --> 00:58:07,319
Sarris and what you are saying is that

00:58:03,779 --> 00:58:09,869
this strategy approach or whatever is

00:58:07,319 --> 00:58:14,609
used by AWS or other

00:58:09,869 --> 00:58:17,249
no not AWS it is being used by so the

00:58:14,609 --> 00:58:19,130
edge store which is by Dropbox it is

00:58:17,249 --> 00:58:22,440
being used by Fisker which is from

00:58:19,130 --> 00:58:24,599
Alibaba so they do a similar thing so

00:58:22,440 --> 00:58:26,489
they so what we did is basically they

00:58:24,599 --> 00:58:28,349
have this entire store which internally

00:58:26,489 --> 00:58:30,749
does this thing we just kind of D

00:58:28,349 --> 00:58:32,880
structure eyes it separated out the

00:58:30,749 --> 00:58:36,900
component and use the same strategy at

00:58:32,880 --> 00:58:38,339
the application level so if you go there

00:58:36,900 --> 00:58:40,710
then you can actually read the same

00:58:38,339 --> 00:58:42,660
protocol that's how they are managing

00:58:40,710 --> 00:58:43,619
this consistency and oscillations for

00:58:42,660 --> 00:58:45,630
their application

00:58:43,619 --> 00:58:47,789
so therefore entire database or didn't

00:58:45,630 --> 00:58:49,739
entire so for that we don't have the

00:58:47,789 --> 00:58:51,210
entire thing we are just using different

00:58:49,739 --> 00:59:02,670
different components to get the similar

00:58:51,210 --> 00:59:05,279
results and not really get this shadow

00:59:02,670 --> 00:59:08,549
client stuff is the sort of client above

00:59:05,279 --> 00:59:09,989
the Dre drivers up so every request to

00:59:08,549 --> 00:59:12,900
the drivers of goes through the shuttle

00:59:09,989 --> 00:59:15,029
client yeah so for example in your java

00:59:12,900 --> 00:59:17,549
application you have the my sequel light

00:59:15,029 --> 00:59:19,559
or you have let's say your JDBC Cline so

00:59:17,549 --> 00:59:21,210
the shadow client is exactly similar it

00:59:19,559 --> 00:59:23,910
will support all the my sequel requests

00:59:21,210 --> 00:59:26,069
it will just tweak the request based on

00:59:23,910 --> 00:59:27,779
the protocols so you instead of using it

00:59:26,069 --> 00:59:30,119
as a separate client or library you just

00:59:27,779 --> 00:59:31,980
change your my sequel light so if there

00:59:30,119 --> 00:59:33,029
is any transaction the shadow client

00:59:31,980 --> 00:59:34,680
will make the changes if there is

00:59:33,029 --> 00:59:37,049
nothing it will work as it is and let

00:59:34,680 --> 00:59:41,339
your request go through so the shadow

00:59:37,049 --> 00:59:44,519
client injecting into there really cool

00:59:41,339 --> 00:59:47,369
thanks also in the next

00:59:44,519 --> 00:59:50,400
I probably twas some of the next slides

00:59:47,369 --> 00:59:53,400
you said last committed transaction of

00:59:50,400 --> 00:59:55,650
does that mean that the transaction

00:59:53,400 --> 00:59:57,210
should be committed in order yes

00:59:55,650 --> 00:59:59,369
transition should be committed in order

00:59:57,210 --> 01:00:02,369
so it depends on the coordinator so what

00:59:59,369 --> 01:00:04,079
coordinator does is basically every so

01:00:02,369 --> 01:00:05,700
this transaction ID technically this

01:00:04,079 --> 01:00:07,620
transaction ID should be an

01:00:05,700 --> 01:00:08,910
mendl's thing so let's say our time

01:00:07,620 --> 01:00:10,890
stamp so let's say you get a two

01:00:08,910 --> 01:00:13,050
concurrent request and if your

01:00:10,890 --> 01:00:15,180
concurrent request you have a read and

01:00:13,050 --> 01:00:16,830
write at the same time now you will only

01:00:15,180 --> 01:00:18,960
face the problem when the reads and

01:00:16,830 --> 01:00:20,310
writes are happening on the same key not

01:00:18,960 --> 01:00:21,870
on different keys so if they are owned

01:00:20,310 --> 01:00:23,910
differently it doesn't matter but if

01:00:21,870 --> 01:00:26,430
they are on the same key so let's say

01:00:23,910 --> 01:00:27,810
the coordinator gave a number to the

01:00:26,430 --> 01:00:29,280
right transaction let's say the

01:00:27,810 --> 01:00:30,630
coordinator gave our times time to write

01:00:29,280 --> 01:00:32,700
transaction is five and the read

01:00:30,630 --> 01:00:34,770
transition is six so in the read

01:00:32,700 --> 01:00:36,630
committed isolation we have to make sure

01:00:34,770 --> 01:00:40,170
that this reads sick should write or

01:00:36,630 --> 01:00:41,670
read the right five as well so in order

01:00:40,170 --> 01:00:44,190
to make sure that's why we have this

01:00:41,670 --> 01:00:46,110
timeout thing so if you varied about the

01:00:44,190 --> 01:00:47,910
Google spanner it also have the seven

01:00:46,110 --> 01:00:49,200
milliseconds of gap between every

01:00:47,910 --> 01:00:51,150
transaction that means that every

01:00:49,200 --> 01:00:52,950
transaction should complete within this

01:00:51,150 --> 01:00:54,510
time period and if it does not complete

01:00:52,950 --> 01:00:56,550
within this time period we will fail it

01:00:54,510 --> 01:00:59,700
because then there is no way you can

01:00:56,550 --> 01:01:02,730
deal with concurrent transaction so you

01:00:59,700 --> 01:01:04,560
you it has to be incremental and it will

01:01:02,730 --> 01:01:06,240
so the read transaction will say what

01:01:04,560 --> 01:01:08,340
was the last committed ID so for example

01:01:06,240 --> 01:01:09,780
my ID is six the righty of the read is

01:01:08,340 --> 01:01:11,790
six that mean I should be able to read

01:01:09,780 --> 01:01:13,110
everything before this and if there is

01:01:11,790 --> 01:01:14,400
anything in the waiting state during

01:01:13,110 --> 01:01:15,780
that period that if there is any

01:01:14,400 --> 01:01:18,390
transaction recharge which is still

01:01:15,780 --> 01:01:19,980
ongoing the read will went wait until

01:01:18,390 --> 01:01:21,630
the transaction is complete and if the

01:01:19,980 --> 01:01:23,670
transition is not complete within that

01:01:21,630 --> 01:01:26,010
time frame which you can decide based on

01:01:23,670 --> 01:01:27,720
your latency of this system then your

01:01:26,010 --> 01:01:29,430
read has to fail because otherwise then

01:01:27,720 --> 01:01:31,350
you will go in trouble for example

01:01:29,430 --> 01:01:33,330
instead of read what is what if it's in

01:01:31,350 --> 01:01:34,890
an other way around like you first read

01:01:33,330 --> 01:01:36,120
and then you write then it's okay

01:01:34,890 --> 01:01:38,910
because your right is happening after

01:01:36,120 --> 01:01:40,290
the read so that's how we are making

01:01:38,910 --> 01:01:42,330
sure that you get the read committed

01:01:40,290 --> 01:01:44,520
isolation so basically serializing the

01:01:42,330 --> 01:01:48,110
transactions yes basically see relating

01:01:44,520 --> 01:01:48,110
the transaction is okay thanks

01:02:00,490 --> 01:02:07,690
oh yes so so for distributed transaction

01:02:06,790 --> 01:02:09,400
if you if you

01:02:07,690 --> 01:02:11,770
it depends what implementation you want

01:02:09,400 --> 01:02:14,080
so if we are looking for some two-phase

01:02:11,770 --> 01:02:16,240
commit implementations there are already

01:02:14,080 --> 01:02:18,190
in Java I think the exit transactions

01:02:16,240 --> 01:02:21,340
and do those things if we are looking

01:02:18,190 --> 01:02:23,920
for sagas I know there is this company

01:02:21,340 --> 01:02:25,480
called eventually so do you know the guy

01:02:23,920 --> 01:02:28,330
Chris Richardson he's a guy from

01:02:25,480 --> 01:02:30,369
micro-services dot IO so he has this

01:02:28,330 --> 01:02:31,869
company called eventuate dot IO and they

01:02:30,369 --> 01:02:33,460
have an implementation of saga

01:02:31,869 --> 01:02:35,050
coordinator and they call it the

01:02:33,460 --> 01:02:37,619
transaction manager and something like

01:02:35,050 --> 01:02:40,150
that so if you are looking for those

01:02:37,619 --> 01:02:41,560
sagem emendation you can go and find on

01:02:40,150 --> 01:02:43,930
their website there is this github

01:02:41,560 --> 01:02:45,940
repository I don't know the name of the

01:02:43,930 --> 01:02:48,100
repository but if we just search

01:02:45,940 --> 01:02:49,660
eventuate dot IO saga protocol

01:02:48,100 --> 01:02:52,390
implementation so you can find the

01:02:49,660 --> 01:02:54,430
implementation of saga coordinator you

01:02:52,390 --> 01:02:57,250
cannot find the implementation for the

01:02:54,430 --> 01:02:59,200
shadows coordinator as such because this

01:02:57,250 --> 01:03:01,720
is something which we are still in

01:02:59,200 --> 01:03:04,090
progress you can find the entire store

01:03:01,720 --> 01:03:05,710
with all the components but it's as I

01:03:04,090 --> 01:03:08,619
said that we are more likely structure

01:03:05,710 --> 01:03:10,600
izing it so you can find for the shadows

01:03:08,619 --> 01:03:13,390
but for other two protocols you can

01:03:10,600 --> 01:03:15,160
easily find and later if you manage to

01:03:13,390 --> 01:03:16,510
get these things out maybe we will try

01:03:15,160 --> 01:03:26,970
to open source the coordinator or the

01:03:16,510 --> 01:03:26,970
manager for the shadows yeah

01:03:33,920 --> 01:03:38,930
that is doing this kind of solving this

01:03:37,519 --> 01:03:40,759
kind of problem to be honest

01:03:38,930 --> 01:03:43,130
Jaipur said the driver as well but I

01:03:40,759 --> 01:03:46,789
don't I don't know if you meant for

01:03:43,130 --> 01:03:49,269
Jakarta meant a product by Apache I'm

01:03:46,789 --> 01:03:49,269
not sure

01:03:51,369 --> 01:03:56,240
the there is a standard the meant you

01:03:55,309 --> 01:03:59,769
were talking about the implementation

01:03:56,240 --> 01:04:10,779
right implementation under the Apache

01:03:59,769 --> 01:04:10,779
committee flag oh yeah of course yeah

01:04:11,200 --> 01:04:16,869
and I know I don't know it's a good

01:04:13,910 --> 01:04:16,869
question to ask around

01:04:30,910 --> 01:04:47,359
yeah yeah so I think right so the I

01:04:45,230 --> 01:04:49,880
think what I understood is that is there

01:04:47,359 --> 01:04:51,320
any standard protocol to handle the

01:04:49,880 --> 01:04:54,590
problem of distributed transactions

01:04:51,320 --> 01:04:57,770
right so I kind of there is no such

01:04:54,590 --> 01:05:00,140
standard protocol but from what I have

01:04:57,770 --> 01:05:02,090
read so if we look at spanner if you

01:05:00,140 --> 01:05:03,619
look at cockroach DB and all these

01:05:02,090 --> 01:05:05,960
different databases they solve the

01:05:03,619 --> 01:05:08,210
similar problems so there is I think

01:05:05,960 --> 01:05:11,119
this thing is a bit new right now

01:05:08,210 --> 01:05:12,080
because people recently it's not so for

01:05:11,119 --> 01:05:13,880
example two-phase commit is very

01:05:12,080 --> 01:05:16,220
standard in databases it's been there

01:05:13,880 --> 01:05:18,140
for years so that's why the two-phase

01:05:16,220 --> 01:05:20,119
commit is also very standard and there

01:05:18,140 --> 01:05:21,680
are a lot of implementation you can find

01:05:20,119 --> 01:05:22,970
the tweaked implementation of two-phase

01:05:21,680 --> 01:05:25,480
commit which are like faster than

01:05:22,970 --> 01:05:27,920
locking all in the entire tables and

01:05:25,480 --> 01:05:31,160
there people are still trying to make it

01:05:27,920 --> 01:05:33,740
I think better and fast so there are

01:05:31,160 --> 01:05:35,510
like some databases around to solve this

01:05:33,740 --> 01:05:36,980
problem but in order to make those

01:05:35,510 --> 01:05:39,230
changes you have to change your database

01:05:36,980 --> 01:05:42,740
which is like very difficult or painful

01:05:39,230 --> 01:05:44,810
so there is not very standard thing yet

01:05:42,740 --> 01:05:46,640
sagas is a bit standard so if you just

01:05:44,810 --> 01:05:48,619
go on the Google and search for sagas

01:05:46,640 --> 01:05:50,180
you can find like hundreds of talks in

01:05:48,619 --> 01:05:52,369
different conferences and there are some

01:05:50,180 --> 01:05:54,770
implementations around so that is kind

01:05:52,369 --> 01:05:56,660
of going towards the standardization

01:05:54,770 --> 01:06:07,010
process but I don't think there is a

01:05:56,660 --> 01:06:09,760
completely one way to do this yet thank

01:06:07,010 --> 01:06:09,760

YouTube URL: https://www.youtube.com/watch?v=O1gIum_LmhM


