Title: #ACEU19: Pierre Villard – Running visual quality inspection at the edge with Apache NiFi & MiNiFi
Publication date: 2019-10-31
Playlist: ApacheCon Europe 2019 – Berlin
Description: 
	More: https://aceu19.apachecon.com/session/running-visual-quality-inspection-edge-apache-nifi-minifi

Apache NiFi is an easy to use, powerful, and reliable system to process and distribute data. Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. MiNiFi — a subproject of Apache NiFi — is a complementary data collection approach that supplements the core tenets of NiFi in data flow management, focusing on the collection of data at the source of its creation.

In this talk I present how Apache NiFi & MiNiFi can be used in combination with Google Cloud AutoML Vision to implement a visual quality inspection system. The idea is to demonstrate the possibilities to interact with edge devices while leveraging cloud services in a real world use case
Captions: 
	00:00:04,600 --> 00:00:11,450
well thank you for everyone being in the

00:00:07,790 --> 00:00:14,450
room today I'm gonna talk about a

00:00:11,450 --> 00:00:18,529
bachina IFI obviously and how to use it

00:00:14,450 --> 00:00:20,960
to run visual quality inspection so I'm

00:00:18,529 --> 00:00:23,599
gonna go through we all use case we have

00:00:20,960 --> 00:00:25,910
in the manufacturing industry but it can

00:00:23,599 --> 00:00:29,869
actually apply to many industries

00:00:25,910 --> 00:00:31,640
so about myself very quickly I'm

00:00:29,869 --> 00:00:35,210
currently a customer engineer working at

00:00:31,640 --> 00:00:36,920
Google and I'm involved in the Apache

00:00:35,210 --> 00:00:42,440
know fi community for the last four

00:00:36,920 --> 00:00:44,809
years my Twitter and github accounts and

00:00:42,440 --> 00:00:48,649
if you already know about knife I maybe

00:00:44,809 --> 00:00:50,480
you went across my websites by raising

00:00:48,649 --> 00:00:54,350
your hands how many of you already know

00:00:50,480 --> 00:00:57,649
about a batch knife I cool and how many

00:00:54,350 --> 00:01:01,280
of you used it in production or we all

00:00:57,649 --> 00:01:03,229
use case great okay so in this talk I'm

00:01:01,280 --> 00:01:06,890
assuming you already know a little bit

00:01:03,229 --> 00:01:08,900
about knife I so I won't go too deep but

00:01:06,890 --> 00:01:09,940
if you have any questions we can discuss

00:01:08,900 --> 00:01:13,460
at the end

00:01:09,940 --> 00:01:15,230
you're free to ask any question so a

00:01:13,460 --> 00:01:17,930
little introduction about 95

00:01:15,230 --> 00:01:20,840
nevertheless so initially it's been

00:01:17,930 --> 00:01:23,270
developed by the NSA 30 years ago and it

00:01:20,840 --> 00:01:28,070
has been open sourced and donated to the

00:01:23,270 --> 00:01:32,230
ASF in 2014 and it became a top little

00:01:28,070 --> 00:01:35,330
project top-level project four years ago

00:01:32,230 --> 00:01:37,550
what is now if I use force

00:01:35,330 --> 00:01:42,380
I was actually discussing it with

00:01:37,550 --> 00:01:44,810
someone at lunch usually people I talked

00:01:42,380 --> 00:01:48,350
with have a lot of difficulties to

00:01:44,810 --> 00:01:51,260
position IFI against ETL tools trimming

00:01:48,350 --> 00:01:54,860
tools pepsin missing gene tools

00:01:51,260 --> 00:01:57,080
so it's let's say from my point of view

00:01:54,860 --> 00:02:00,530
if I had to give a definition of knife I

00:01:57,080 --> 00:02:03,080
it's it's a toolbox with a lot of tools

00:02:00,530 --> 00:02:05,930
to move that around from a place a to a

00:02:03,080 --> 00:02:07,670
place be very efficiently with a lot of

00:02:05,930 --> 00:02:11,120
features like multi-tenancy

00:02:07,670 --> 00:02:11,900
data line age data provenance stuff like

00:02:11,120 --> 00:02:15,079
that

00:02:11,900 --> 00:02:18,500
so you really need to see knife I as the

00:02:15,079 --> 00:02:21,130
tool you will use to get that of

00:02:18,500 --> 00:02:23,420
and cleaned processed to make it

00:02:21,130 --> 00:02:26,840
available for consumption to someone

00:02:23,420 --> 00:02:28,940
else that's how I would picture it so

00:02:26,840 --> 00:02:31,700
usually yeah you can use no Fi to move

00:02:28,940 --> 00:02:35,750
that around from the edge to your core

00:02:31,700 --> 00:02:39,470
infrastructure and you can use a lot of

00:02:35,750 --> 00:02:41,450
knife knife I instances along the way so

00:02:39,470 --> 00:02:44,720
knife is a big project we have a lot of

00:02:41,450 --> 00:02:48,140
sub projects very quickly so now fi is

00:02:44,720 --> 00:02:51,080
the core project that's the one I'm

00:02:48,140 --> 00:02:53,390
gonna use extensively today minify as

00:02:51,080 --> 00:02:58,730
well so it's a low footprint version of

00:02:53,390 --> 00:03:00,500
knife I aim to run on IOT devices so I'm

00:02:58,730 --> 00:03:05,420
gonna use the Java version for this talk

00:03:00,500 --> 00:03:07,910
but obviously the C++ version is way

00:03:05,420 --> 00:03:09,709
more adapted for real IOT use case

00:03:07,910 --> 00:03:13,610
because obviously you don't have always

00:03:09,709 --> 00:03:15,560
a GVM available then you have the knife

00:03:13,610 --> 00:03:17,840
I registry which is used for everything

00:03:15,560 --> 00:03:20,569
around CIT d4 knife I so when you need

00:03:17,840 --> 00:03:23,209
to move your workflows from an

00:03:20,569 --> 00:03:26,120
environment to another one you will use

00:03:23,209 --> 00:03:28,549
it to version your work when you publish

00:03:26,120 --> 00:03:32,060
new version of your workflows you can

00:03:28,549 --> 00:03:34,970
also version your extensions because now

00:03:32,060 --> 00:03:37,120
fi is a very extensive extensible

00:03:34,970 --> 00:03:39,709
framework so you can actually build

00:03:37,120 --> 00:03:42,380
everything by yourself if you have very

00:03:39,709 --> 00:03:44,750
specific needs into knife I see two

00:03:42,380 --> 00:03:47,690
server is command and control for all

00:03:44,750 --> 00:03:51,079
the minnow Fi agents and three design

00:03:47,690 --> 00:03:53,420
system is for the UI parts that we are

00:03:51,079 --> 00:03:57,829
trying to make reusable across all the

00:03:53,420 --> 00:04:01,160
projects so right now in nine file you

00:03:57,829 --> 00:04:04,730
have about 300 processors to integrate

00:04:01,160 --> 00:04:06,709
with a lot of systems that's the process

00:04:04,730 --> 00:04:08,900
we are shipping with knife I when you

00:04:06,709 --> 00:04:10,880
are downloading the binary files from

00:04:08,900 --> 00:04:14,780
the websites but you can actually find a

00:04:10,880 --> 00:04:17,840
lot more processors on the Internet

00:04:14,780 --> 00:04:23,030
one I can think of and we had to talk

00:04:17,840 --> 00:04:25,490
about it yesterday or today is PLC 4x we

00:04:23,030 --> 00:04:28,370
also have a parable so so a lot of

00:04:25,490 --> 00:04:30,770
projects half process owns not shipped

00:04:28,370 --> 00:04:33,099
by default but still available on the

00:04:30,770 --> 00:04:33,099
Internet

00:04:33,210 --> 00:04:39,190
usually I put the slides at the end but

00:04:35,949 --> 00:04:42,340
I wanted to put it right now so we don't

00:04:39,190 --> 00:04:43,720
skip it at the end so the Apache way

00:04:42,340 --> 00:04:46,660
community overcoat

00:04:43,720 --> 00:04:49,210
I believe about unifying as a very great

00:04:46,660 --> 00:04:51,160
community so if you want to subscribe to

00:04:49,210 --> 00:04:53,320
the mailing list or contribute or

00:04:51,160 --> 00:04:56,410
participate or ask questions you have

00:04:53,320 --> 00:04:59,169
everything here usually we will use a

00:04:56,410 --> 00:05:00,789
mailing list as a primary way to reach

00:04:59,169 --> 00:05:04,180
out but you can also use the slack

00:05:00,789 --> 00:05:06,610
channel few numbers so right now on the

00:05:04,180 --> 00:05:10,840
track slack channel we have over 500

00:05:06,610 --> 00:05:13,120
people 260 contributors on github across

00:05:10,840 --> 00:05:17,020
the repositories because as I said we

00:05:13,120 --> 00:05:19,030
have multiple projects 4500 meters since

00:05:17,020 --> 00:05:22,150
yesterday we have the Apache now if I

00:05:19,030 --> 00:05:24,610
1.10 release candidate under vote so if

00:05:22,150 --> 00:05:27,810
you want to download it build it and

00:05:24,610 --> 00:05:31,720
cast your vote feel free to do to do it

00:05:27,810 --> 00:05:33,940
and on docker hub we are reaching 1

00:05:31,720 --> 00:05:37,599
million docker pools for the Apache 9 5

00:05:33,940 --> 00:05:41,830
docker image ok let's go into the use

00:05:37,599 --> 00:05:45,550
case so I took a use case that I could

00:05:41,830 --> 00:05:49,300
easily reproduce at my place so in this

00:05:45,550 --> 00:05:52,030
case I'm going to detect if cookies are

00:05:49,300 --> 00:05:54,070
broken or not broken so picture a

00:05:52,030 --> 00:05:57,159
factory where you have productions lines

00:05:54,070 --> 00:05:59,500
and you are making cookies and obviously

00:05:57,159 --> 00:06:01,750
you don't want to have an epic customer

00:05:59,500 --> 00:06:04,270
so before the cookie gets to the

00:06:01,750 --> 00:06:07,120
packaging you want to get the broken

00:06:04,270 --> 00:06:11,770
cookies out of the factory line so to do

00:06:07,120 --> 00:06:14,770
that we can use a knife I and with my

00:06:11,770 --> 00:06:17,020
file we are going to use some tools

00:06:14,770 --> 00:06:20,110
provided by the Google cloud platform

00:06:17,020 --> 00:06:25,110
and specifically we are going to use a

00:06:20,110 --> 00:06:27,880
combination of o2 ml and cloud vision so

00:06:25,110 --> 00:06:30,699
well that's usually what I present to

00:06:27,880 --> 00:06:34,449
customers when they ask about machine

00:06:30,699 --> 00:06:37,030
learning at Google so we provide a wide

00:06:34,449 --> 00:06:38,710
range of solutions and depending on how

00:06:37,030 --> 00:06:43,360
much you want to do everything by

00:06:38,710 --> 00:06:45,190
yourself you have the solutions Auto ml

00:06:43,360 --> 00:06:48,850
is in the middle of cloud

00:06:45,190 --> 00:06:53,770
claudinho is where you build your custom

00:06:48,850 --> 00:06:57,130
models but you need to know about models

00:06:53,770 --> 00:06:59,710
about your data about what you want to

00:06:57,130 --> 00:07:02,320
achieve and on the right you have the

00:06:59,710 --> 00:07:06,220
API is ready to consume where you don't

00:07:02,320 --> 00:07:10,570
have anything to do and you probably

00:07:06,220 --> 00:07:12,430
know about translates etc Auto ml is in

00:07:10,570 --> 00:07:14,470
the middle so you are going to provide

00:07:12,430 --> 00:07:15,940
your data and we are going to train your

00:07:14,470 --> 00:07:19,390
customer though but you don't have to do

00:07:15,940 --> 00:07:22,300
anything else you don't have to to say

00:07:19,390 --> 00:07:25,360
anything about about your data it's

00:07:22,300 --> 00:07:27,790
going to train and optimize the best

00:07:25,360 --> 00:07:32,440
machine learning model available for you

00:07:27,790 --> 00:07:34,740
need what we are trying to do here is to

00:07:32,440 --> 00:07:38,230
have a continuous model we train

00:07:34,740 --> 00:07:40,060
including a feedback loop so when we

00:07:38,230 --> 00:07:42,250
have train a machine learning model

00:07:40,060 --> 00:07:46,420
that's great we are deploying it on the

00:07:42,250 --> 00:07:49,210
edge amazing but obviously your model is

00:07:46,420 --> 00:07:53,740
going to evolve because maybe you are

00:07:49,210 --> 00:07:55,660
going to detect new features with broken

00:07:53,740 --> 00:07:57,970
cookies that your model is not able to

00:07:55,660 --> 00:08:00,460
detect so you want to retrain your model

00:07:57,970 --> 00:08:03,610
with that new picture so we actually

00:08:00,460 --> 00:08:06,400
want to have this full automatic loop

00:08:03,610 --> 00:08:09,370
where the pictures we are taking our

00:08:06,400 --> 00:08:11,440
interesting ingested into Google Cloud

00:08:09,370 --> 00:08:14,020
and we are training again a new model

00:08:11,440 --> 00:08:15,760
and exporting and deploying this new

00:08:14,020 --> 00:08:18,430
version of the model so that's actually

00:08:15,760 --> 00:08:23,200
the model gets better and better over

00:08:18,430 --> 00:08:25,060
time now how we can do that and if

00:08:23,200 --> 00:08:28,330
possible in an efficient way

00:08:25,060 --> 00:08:30,640
so first dummy solution so we are

00:08:28,330 --> 00:08:34,419
running minify on the edge on a

00:08:30,640 --> 00:08:36,789
raspberry pi here just quickly I won't

00:08:34,419 --> 00:08:39,460
do any demo because I don't want to take

00:08:36,789 --> 00:08:40,870
broken cookies everywhere here but if we

00:08:39,460 --> 00:08:44,169
want to discuss it I have everything

00:08:40,870 --> 00:08:46,660
running so we can talk about it I will

00:08:44,169 --> 00:08:47,770
stick around after the talk so we have

00:08:46,660 --> 00:08:50,830
midnight fire running on the Raspberry

00:08:47,770 --> 00:08:53,650
Pi talking with Google Cloud IOT core

00:08:50,830 --> 00:08:57,250
through mqtt which is the gateway for

00:08:53,650 --> 00:08:58,810
IOT basically the data is sent to a pub

00:08:57,250 --> 00:09:01,720
sub topic which is the

00:08:58,810 --> 00:09:06,010
Google equivalents let's say of Apache

00:09:01,720 --> 00:09:08,500
Kafka then we have a knife I running in

00:09:06,010 --> 00:09:10,140
GCP that will take this that do some

00:09:08,500 --> 00:09:12,820
processing we will go into the details

00:09:10,140 --> 00:09:15,340
make it available to Google Cloud vision

00:09:12,820 --> 00:09:18,670
and then I'm using the first option

00:09:15,340 --> 00:09:21,640
which is Google Cloud vision is actually

00:09:18,670 --> 00:09:25,870
exporting exposing the API that I can

00:09:21,640 --> 00:09:28,030
request to classify my images so in this

00:09:25,870 --> 00:09:30,690
case I am NOT deploying the model on the

00:09:28,030 --> 00:09:34,090
edge it's available in the cloud and

00:09:30,690 --> 00:09:36,760
minify is doing an HTTP call each time

00:09:34,090 --> 00:09:40,240
we want to classify an image it works

00:09:36,760 --> 00:09:43,720
obviously we will see that it's not

00:09:40,240 --> 00:09:46,390
efficient and it's assumed that's your

00:09:43,720 --> 00:09:48,910
minify is able to do an HTTP call to

00:09:46,390 --> 00:09:52,870
clone and obviously most of the time we

00:09:48,910 --> 00:09:57,420
don't want to do that second second

00:09:52,870 --> 00:09:59,800
architecture so it's it's quite the same

00:09:57,420 --> 00:10:02,170
communication through MQTT with Google

00:09:59,800 --> 00:10:03,910
Cloud IOT core but when we train the

00:10:02,170 --> 00:10:05,950
model in Google Cloud vision we are

00:10:03,910 --> 00:10:09,250
exporting the model and make it

00:10:05,950 --> 00:10:11,290
available directly on the edge and the

00:10:09,250 --> 00:10:13,360
inference the classification of our

00:10:11,290 --> 00:10:17,350
image is happening directly on the

00:10:13,360 --> 00:10:22,110
raspberry pi which is much faster ok so

00:10:17,350 --> 00:10:27,340
the first step is to actually create a

00:10:22,110 --> 00:10:30,070
data set and register your device so

00:10:27,340 --> 00:10:31,990
that's the device I won't go into the

00:10:30,070 --> 00:10:34,900
details but in Google IOT core you can

00:10:31,990 --> 00:10:36,670
set up your registry which is the

00:10:34,900 --> 00:10:39,360
collection of devices you want to manage

00:10:36,670 --> 00:10:42,390
then you register your device with the

00:10:39,360 --> 00:10:46,420
TLS certificates for identification

00:10:42,390 --> 00:10:51,070
authentication and stuff liner so not

00:10:46,420 --> 00:10:53,290
very important then here is a workflow

00:10:51,070 --> 00:10:56,140
that is actually running on the mini

00:10:53,290 --> 00:10:59,110
iPad so each time when you see a 9-5

00:10:56,140 --> 00:11:02,530
workflow i'm specifying on the top where

00:10:59,110 --> 00:11:07,420
it is running so this one is on minify

00:11:02,530 --> 00:11:09,940
so very easily i'm saying to the pine

00:11:07,420 --> 00:11:11,910
camera to take pictures with the code

00:11:09,940 --> 00:11:15,390
over there nothing no

00:11:11,910 --> 00:11:17,520
fancy very very basic stuff I'm taking

00:11:15,390 --> 00:11:24,540
the files and sending the the pictures

00:11:17,520 --> 00:11:27,030
through MQTT that's it so once the image

00:11:24,540 --> 00:11:29,070
is arrived to Google Cloud IOT Co as I

00:11:27,030 --> 00:11:31,710
said you don't have to do anything it's

00:11:29,070 --> 00:11:34,980
automatically pushed to a pub sub topic

00:11:31,710 --> 00:11:37,140
and then on my knife I running on DCP

00:11:34,980 --> 00:11:39,420
here I have another workflow which is

00:11:37,140 --> 00:11:42,330
pulling the data from the pub sub topic

00:11:39,420 --> 00:11:44,850
and putting this data into Google Cloud

00:11:42,330 --> 00:11:46,650
storage and then on the right I'm doing

00:11:44,850 --> 00:11:49,770
some API calls to make this data

00:11:46,650 --> 00:11:53,940
available to Google Cloud vision right

00:11:49,770 --> 00:11:57,060
so here is what I did during my last few

00:11:53,940 --> 00:11:59,990
days I have extracting days took hours

00:11:57,060 --> 00:12:03,720
to take pictures of cookies really great

00:11:59,990 --> 00:12:06,510
but here are the pictures this is the UI

00:12:03,720 --> 00:12:09,000
of Google Cloud vision which is

00:12:06,510 --> 00:12:12,390
currently in beta but it will be GA

00:12:09,000 --> 00:12:15,840
before the end of the year and you can

00:12:12,390 --> 00:12:18,420
actually go over the pictures and label

00:12:15,840 --> 00:12:22,320
your pictures so in my case I said that

00:12:18,420 --> 00:12:25,320
I have only two levels not okay or okay

00:12:22,320 --> 00:12:28,140
I have labelled the images on level

00:12:25,320 --> 00:12:30,750
images that I just ingested but I didn't

00:12:28,140 --> 00:12:34,890
say it's if it's okay or not and that's

00:12:30,750 --> 00:12:36,870
it once you have at least ten images per

00:12:34,890 --> 00:12:44,220
level you can actually start training

00:12:36,870 --> 00:12:47,400
your models then what I want to do is to

00:12:44,220 --> 00:12:49,980
be able to switch from the architecture

00:12:47,400 --> 00:12:53,370
one to the architecture two just by

00:12:49,980 --> 00:12:56,940
configuring my device from the GCP

00:12:53,370 --> 00:12:58,710
platform so on my knife I if you

00:12:56,940 --> 00:13:02,070
remember this is the processor where I'm

00:12:58,710 --> 00:13:05,670
sending the images through MQTT so when

00:13:02,070 --> 00:13:07,800
you connect to mqtt to Google IOT core

00:13:05,670 --> 00:13:11,010
through mqtt you can also receive

00:13:07,800 --> 00:13:14,070
configuration changes and comments so in

00:13:11,010 --> 00:13:17,460
that case this is the workflow I define

00:13:14,070 --> 00:13:20,520
to take care of configuration changes

00:13:17,460 --> 00:13:22,590
and here is a JSON representing my

00:13:20,520 --> 00:13:25,010
configuration so you don't see anything

00:13:22,590 --> 00:13:26,750
but basically I'm saying I

00:13:25,010 --> 00:13:30,530
in which mode the device is running

00:13:26,750 --> 00:13:33,080
either cloud mode so on the Raspberry Pi

00:13:30,530 --> 00:13:37,130
will make an HTTP call each time I

00:13:33,080 --> 00:13:39,050
capture an image or edge mode the model

00:13:37,130 --> 00:13:45,320
is running directly on the device and

00:13:39,050 --> 00:13:48,590
I'm doing the inference locally then on

00:13:45,320 --> 00:13:52,190
my workflows running on the knife I on

00:13:48,590 --> 00:13:55,280
DCPI externalized all of the variables I

00:13:52,190 --> 00:13:57,170
needed so I won't go into the details

00:13:55,280 --> 00:14:00,410
but basically basically you also have

00:13:57,170 --> 00:14:04,100
the mode here which is either edge or

00:14:00,410 --> 00:14:06,890
cloud the other variables are just some

00:14:04,100 --> 00:14:11,060
parameters are externalized so that I

00:14:06,890 --> 00:14:14,660
can manage everything in one place okay

00:14:11,060 --> 00:14:17,180
model training so as I said I don't want

00:14:14,660 --> 00:14:19,430
to do anything manually so I have a

00:14:17,180 --> 00:14:23,930
workflow taking care of the model

00:14:19,430 --> 00:14:27,020
training so every day at midnight I'm

00:14:23,930 --> 00:14:29,240
triggering a new model training to get a

00:14:27,020 --> 00:14:33,410
new version of my machine learning model

00:14:29,240 --> 00:14:35,300
so I start the model training by making

00:14:33,410 --> 00:14:37,000
some API calls waiting for the

00:14:35,300 --> 00:14:42,100
completion of the model training and

00:14:37,000 --> 00:14:42,100
then I will take care of the deployments

00:14:42,400 --> 00:14:50,870
so once you have made the API call if

00:14:48,320 --> 00:14:53,450
you go into the UI it will look like

00:14:50,870 --> 00:14:55,940
this so you actually see that the model

00:14:53,450 --> 00:14:58,490
is currently being trained and at the

00:14:55,940 --> 00:15:00,500
end you will receive an adjacent answer

00:14:58,490 --> 00:15:05,240
like that saying that your model is

00:15:00,500 --> 00:15:07,580
ready and you can use it so once a model

00:15:05,240 --> 00:15:10,430
is trained on the UI you can add a lot

00:15:07,580 --> 00:15:12,950
of information about how efficient the

00:15:10,430 --> 00:15:16,130
model is going to be regarding

00:15:12,950 --> 00:15:18,200
confidence you also have the details if

00:15:16,130 --> 00:15:21,470
you if you had true positives or two

00:15:18,200 --> 00:15:23,900
negatives etc during the validation of

00:15:21,470 --> 00:15:26,180
the model so everything is available I

00:15:23,900 --> 00:15:30,740
won't go too much into the details here

00:15:26,180 --> 00:15:34,040
so architecture one so as I said in this

00:15:30,740 --> 00:15:36,950
case we are just deploying the model on

00:15:34,040 --> 00:15:38,360
Google cloud and an API is made

00:15:36,950 --> 00:15:40,579
available for you and you can

00:15:38,360 --> 00:15:45,649
called the API with your image to get

00:15:40,579 --> 00:15:47,329
the Lebel on your image so this is the

00:15:45,649 --> 00:15:50,300
second part of my walkthrough running on

00:15:47,329 --> 00:15:53,870
knife ice in case I'm in the cloud mode

00:15:50,300 --> 00:15:56,899
I'm doing the API calls to deploy the

00:15:53,870 --> 00:15:59,720
model on Google Cloud and it's it men

00:15:56,899 --> 00:16:02,690
available so it's it's in server list so

00:15:59,720 --> 00:16:04,940
you don't have to care about servers how

00:16:02,690 --> 00:16:08,329
it is made available on what it is

00:16:04,940 --> 00:16:13,940
running Google Cloud is taking care of

00:16:08,329 --> 00:16:17,300
everything okay but then once the model

00:16:13,940 --> 00:16:21,920
is trained I have to search my device

00:16:17,300 --> 00:16:25,310
which model to use so at the end at the

00:16:21,920 --> 00:16:28,550
end of the of the workflow actually here

00:16:25,310 --> 00:16:31,070
at the end of the workflow I'm executing

00:16:28,550 --> 00:16:34,130
a MQTT comment to send the information

00:16:31,070 --> 00:16:37,430
back to my device so I'm sending to the

00:16:34,130 --> 00:16:41,360
device the model ID that I just trained

00:16:37,430 --> 00:16:44,240
and I make this information available on

00:16:41,360 --> 00:16:47,630
my device again I won't go into the

00:16:44,240 --> 00:16:49,459
details that's not really necessary but

00:16:47,630 --> 00:16:52,699
just a few processors receive the

00:16:49,459 --> 00:16:54,620
command and extract information so that

00:16:52,699 --> 00:16:57,350
was the architecture one then the

00:16:54,620 --> 00:16:59,630
architecture 2 so in this case we want

00:16:57,350 --> 00:17:03,199
to actually export the model directly on

00:16:59,630 --> 00:17:05,240
the device so quite similarly we are

00:17:03,199 --> 00:17:07,549
calling the API to make the model

00:17:05,240 --> 00:17:10,610
available on GCS Google Cloud Storage

00:17:07,549 --> 00:17:12,919
and then we are sending MQTT comment to

00:17:10,610 --> 00:17:14,799
the device to say ok my model is

00:17:12,919 --> 00:17:20,299
available and you can download it here

00:17:14,799 --> 00:17:22,309
whenever you want so same I'm receiving

00:17:20,299 --> 00:17:24,290
on the device this information with the

00:17:22,309 --> 00:17:26,900
Google Cloud Storage path - down on my

00:17:24,290 --> 00:17:28,549
mother and I'm downloading the model

00:17:26,900 --> 00:17:33,919
directly on the device and now I can use

00:17:28,549 --> 00:17:38,980
it local inference so now we have

00:17:33,919 --> 00:17:41,390
everything we are able to start the loop

00:17:38,980 --> 00:17:43,070
we have the first version of our model

00:17:41,390 --> 00:17:46,309
so we can actually start doing some

00:17:43,070 --> 00:17:50,390
inference so here is what it looks like

00:17:46,309 --> 00:17:51,980
on the minify part on the on the

00:17:50,390 --> 00:17:55,610
Raspberry Pi so

00:17:51,980 --> 00:17:58,700
in this case I'm doing the inference in

00:17:55,610 --> 00:18:01,190
the cloud so making the API calls and we

00:17:58,700 --> 00:18:04,040
can see some examples of the results we

00:18:01,190 --> 00:18:06,200
are getting with the Ladell we are

00:18:04,040 --> 00:18:08,630
computing on the picture confidence

00:18:06,200 --> 00:18:14,870
score and the mode in which the device

00:18:08,630 --> 00:18:18,470
is running okay now we can do the same

00:18:14,870 --> 00:18:23,360
if I switch the device to the other mode

00:18:18,470 --> 00:18:26,270
edge mode I'm using the model directly

00:18:23,360 --> 00:18:28,309
locally on the device and I can do the

00:18:26,270 --> 00:18:31,960
same so I'm executing the model it's a

00:18:28,309 --> 00:18:37,460
tensor flow lights model so you can

00:18:31,960 --> 00:18:40,400
execute it using a very short Speight on

00:18:37,460 --> 00:18:43,370
script and you have the same result so

00:18:40,400 --> 00:18:46,240
each level with the confidence score and

00:18:43,370 --> 00:18:51,080
and and that's it basically

00:18:46,240 --> 00:18:56,150
okay that's great but now we want to do

00:18:51,080 --> 00:18:58,610
more so actually you can get even faster

00:18:56,150 --> 00:19:02,030
in your inference using Google's GPU so

00:18:58,610 --> 00:19:06,110
TPU stands for tensor flow tensor

00:19:02,030 --> 00:19:08,540
processing units that's hardware that

00:19:06,110 --> 00:19:11,179
Google is making and only Google is

00:19:08,540 --> 00:19:14,660
making this hardware and you can use it

00:19:11,179 --> 00:19:17,929
to actually have even better performance

00:19:14,660 --> 00:19:21,710
for all your tensor flow models so in

00:19:17,929 --> 00:19:27,760
this case I used USB accelerator which

00:19:21,710 --> 00:19:31,610
is here and great news yesterday this

00:19:27,760 --> 00:19:33,940
website came out of beta so it has been

00:19:31,610 --> 00:19:36,740
announced yesterday it was not planned

00:19:33,940 --> 00:19:41,080
you can look at all the examples

00:19:36,740 --> 00:19:45,429
resources on the website and get some

00:19:41,080 --> 00:19:48,799
some board looking like raspberry and

00:19:45,429 --> 00:19:55,549
also the USB accelerators camera and

00:19:48,799 --> 00:19:58,660
stuff like that so some results when I'm

00:19:55,549 --> 00:20:03,049
running what I just show you before so

00:19:58,660 --> 00:20:05,460
when I'm actually you okay running my

00:20:03,049 --> 00:20:07,799
workflow on my Raspberry Pi

00:20:05,460 --> 00:20:10,140
and doing the inference using the first

00:20:07,799 --> 00:20:12,059
architecture so between the time I take

00:20:10,140 --> 00:20:15,779
the picture and the time I have the

00:20:12,059 --> 00:20:18,450
Lebel it's about 6 seconds the HTTP call

00:20:15,779 --> 00:20:21,450
itself is about two and a half seconds

00:20:18,450 --> 00:20:24,179
so obviously in a factory if you have

00:20:21,450 --> 00:20:26,789
tens of cookies per second going on your

00:20:24,179 --> 00:20:30,149
factory line you you cannot walk like

00:20:26,789 --> 00:20:32,760
that so then if you go with the model on

00:20:30,149 --> 00:20:35,250
the edge the processing time between the

00:20:32,760 --> 00:20:37,230
picture I take the picture and the

00:20:35,250 --> 00:20:39,690
moment I have done the classification

00:20:37,230 --> 00:20:42,720
which is the cookie is okay or not okay

00:20:39,690 --> 00:20:45,840
it's about 550 milliseconds and the

00:20:42,720 --> 00:20:49,230
inference time itself is a bit more 100

00:20:45,840 --> 00:20:54,059
milliseconds and then when I use the USB

00:20:49,230 --> 00:20:57,270
accelerator using the HTTP you features

00:20:54,059 --> 00:21:00,450
of Google the processing time between

00:20:57,270 --> 00:21:03,029
the pitcher and the labeling it's half a

00:21:00,450 --> 00:21:08,820
second and the inference time itself is

00:21:03,029 --> 00:21:12,779
10 milliseconds so here is an example

00:21:08,820 --> 00:21:16,470
just not to throw a number like that so

00:21:12,779 --> 00:21:21,690
the first one is when I'm using the

00:21:16,470 --> 00:21:23,640
model that you will download from the

00:21:21,690 --> 00:21:26,909
Google cloud engine API which is not

00:21:23,640 --> 00:21:29,820
optimized for HTTP you and you would get

00:21:26,909 --> 00:21:32,970
the numbers here for the inference and

00:21:29,820 --> 00:21:36,179
then you can also download another model

00:21:32,970 --> 00:21:38,909
which is optimized for HTTP you and you

00:21:36,179 --> 00:21:44,270
have the inference time which is around

00:21:38,909 --> 00:21:51,510
nine milliseconds okay so that's great

00:21:44,270 --> 00:21:54,690
but you could say I don't know what's

00:21:51,510 --> 00:21:57,870
going on on my device I don't have any

00:21:54,690 --> 00:22:00,779
monitoring capabilities with what you

00:21:57,870 --> 00:22:02,880
showed it means that any each time I'm

00:22:00,779 --> 00:22:04,740
taking a picture the picture is ingested

00:22:02,880 --> 00:22:07,529
into my data set in Google Cloud vision

00:22:04,740 --> 00:22:09,750
I would have to manually level it to

00:22:07,529 --> 00:22:13,620
actually use it for the next version of

00:22:09,750 --> 00:22:17,039
my model so we we have some rooms for

00:22:13,620 --> 00:22:19,350
improvement so first what I'm going to

00:22:17,039 --> 00:22:21,600
do is to automatically level

00:22:19,350 --> 00:22:23,490
data if I'm confident enough on the

00:22:21,600 --> 00:22:26,250
label that I inferred on the picture so

00:22:23,490 --> 00:22:29,010
for instance if I take a picture of a

00:22:26,250 --> 00:22:31,799
cookie which is okay and I'm confident

00:22:29,010 --> 00:22:34,169
at 99% that the cookie is okay I don't

00:22:31,799 --> 00:22:36,000
want someone to level it I want to push

00:22:34,169 --> 00:22:37,650
it into google flood vision directly

00:22:36,000 --> 00:22:39,630
with the leather and take it for the

00:22:37,650 --> 00:22:43,350
next training of my machine learning

00:22:39,630 --> 00:22:45,390
models also something I can do is to

00:22:43,350 --> 00:22:48,450
send the inference results along with

00:22:45,390 --> 00:22:51,480
the pictures through mqtt so that I can

00:22:48,450 --> 00:22:54,240
make monitoring on Google cloud and also

00:22:51,480 --> 00:22:57,990
detect outliers inference performance

00:22:54,240 --> 00:23:00,330
and stuff like that so I did it it's

00:22:57,990 --> 00:23:02,549
making the full walk flows a little but

00:23:00,330 --> 00:23:04,230
a little bit more complex but I won't go

00:23:02,549 --> 00:23:07,740
too much into the details but basically

00:23:04,230 --> 00:23:10,020
before going to the IOT device MQTT

00:23:07,740 --> 00:23:14,000
process so I'm doing the inference here

00:23:10,020 --> 00:23:18,720
and I'm sending this this information

00:23:14,000 --> 00:23:22,409
through MQTT to Google calm so once I do

00:23:18,720 --> 00:23:24,990
that I'm able to do some monitoring -

00:23:22,409 --> 00:23:26,760
bolts in stackdriver and you can

00:23:24,990 --> 00:23:30,450
actually see what I was showing before

00:23:26,760 --> 00:23:32,789
so meaning that here the inference is

00:23:30,450 --> 00:23:37,350
happening in the cloud it's architecture

00:23:32,789 --> 00:23:39,330
one and it's about six seconds and then

00:23:37,350 --> 00:23:42,330
I switch the mode and the inference

00:23:39,330 --> 00:23:47,100
happens on the on the edge directly on

00:23:42,330 --> 00:23:51,000
the device and it's it's below two

00:23:47,100 --> 00:23:52,950
seconds same for the confidence so you

00:23:51,000 --> 00:23:56,490
can notice that when you are doing an

00:23:52,950 --> 00:24:00,960
API call to Google Cloud vision to get

00:23:56,490 --> 00:24:04,289
the level we we almost get a score of

00:24:00,960 --> 00:24:07,679
one so 99% of confidence but as soon as

00:24:04,289 --> 00:24:10,400
we build a machine learning model that

00:24:07,679 --> 00:24:13,470
is designed to run on the device we are

00:24:10,400 --> 00:24:16,620
we have to reduce the size of the

00:24:13,470 --> 00:24:19,380
machine learning model so it's gonna be

00:24:16,620 --> 00:24:21,750
efficient with the low latency for the

00:24:19,380 --> 00:24:24,030
inference but obviously you lose in

00:24:21,750 --> 00:24:26,730
accuracy of the model so once we are

00:24:24,030 --> 00:24:30,090
doing the inference on the edge usually

00:24:26,730 --> 00:24:31,540
we are wrong 90 percent of confidence

00:24:30,090 --> 00:24:35,460
when we are doing an inference

00:24:31,540 --> 00:24:37,930
on the right you see the switch between

00:24:35,460 --> 00:24:38,650
using the accelerator and not the

00:24:37,930 --> 00:24:42,250
accelerator

00:24:38,650 --> 00:24:44,800
so first the accelerator on the left so

00:24:42,250 --> 00:24:46,330
I don't know why to be honest but

00:24:44,800 --> 00:24:48,580
actually when we use the accelerator

00:24:46,330 --> 00:24:51,190
with the model which is optimized for

00:24:48,580 --> 00:24:53,890
HTTP you we have a confidence score

00:24:51,190 --> 00:24:55,360
which is higher than before and we have

00:24:53,890 --> 00:24:57,400
an inference time which is lower than

00:24:55,360 --> 00:25:04,720
before so you have no reason not to use

00:24:57,400 --> 00:25:06,790
it obviously but I won't go into the

00:25:04,720 --> 00:25:08,650
details of that once you have order of

00:25:06,790 --> 00:25:15,240
the data into Google Cloud you can

00:25:08,650 --> 00:25:17,350
perform analytics you can do whatever

00:25:15,240 --> 00:25:20,460
processing you want on the data you are

00:25:17,350 --> 00:25:24,070
collecting if you want to do something

00:25:20,460 --> 00:25:27,370
ok so a conclusion so with knife I in

00:25:24,070 --> 00:25:32,850
combination with GCP we we did implement

00:25:27,370 --> 00:25:35,140
this feedback loop which is working

00:25:32,850 --> 00:25:39,100
automatically actually you don't have to

00:25:35,140 --> 00:25:41,680
do anything once it's started so you you

00:25:39,100 --> 00:25:44,770
did everything without writing any code

00:25:41,680 --> 00:25:47,680
except the Python script I showed at the

00:25:44,770 --> 00:25:50,530
beginning to take pictures you have the

00:25:47,680 --> 00:25:53,560
feedback loop you don't have to manually

00:25:50,530 --> 00:25:55,480
do anything except for labeling the

00:25:53,560 --> 00:25:57,310
pictures that I'm below a given

00:25:55,480 --> 00:26:01,420
threshold that you want to actually look

00:25:57,310 --> 00:26:03,690
into and you can do the processing on

00:26:01,420 --> 00:26:05,890
the edge using tensorflow lights and

00:26:03,690 --> 00:26:10,680
boosting your performance using the

00:26:05,890 --> 00:26:14,820
coral edge TPU and that's it very

00:26:10,680 --> 00:26:18,760
quickly before you go to the questions I

00:26:14,820 --> 00:26:21,250
went through the slides very quickly and

00:26:18,760 --> 00:26:23,290
I did not go into the details of the

00:26:21,250 --> 00:26:25,930
workflow but I will will be publishing

00:26:23,290 --> 00:26:28,870
everything on github repository I

00:26:25,930 --> 00:26:32,110
started writing it yesterday evening so

00:26:28,870 --> 00:26:34,480
right now not too much on it but I will

00:26:32,110 --> 00:26:37,540
walk on it tonight and tomorrow in the

00:26:34,480 --> 00:26:39,850
plane anyway everything will be

00:26:37,540 --> 00:26:42,970
available on this represent or a code

00:26:39,850 --> 00:26:45,360
walkthroughs process halls I will make

00:26:42,970 --> 00:26:48,310
everything available and

00:26:45,360 --> 00:26:52,960
so you can deploy it and play with it

00:26:48,310 --> 00:26:56,730
very easily and that's it for me thank

00:26:52,960 --> 00:27:05,200
[Applause]

00:26:56,730 --> 00:27:06,730
you thank you questions must be why I

00:27:05,200 --> 00:27:15,760
know it was easy but come on your

00:27:06,730 --> 00:27:19,090
operations Thanks here you had this was

00:27:15,760 --> 00:27:21,670
a custom processor for the inference

00:27:19,090 --> 00:27:23,830
yourself it's just a Python script so

00:27:21,670 --> 00:27:26,860
it's actually available on the Corolla

00:27:23,830 --> 00:27:30,700
websites I didn't change anything on

00:27:26,860 --> 00:27:34,150
that just yes done and you give the

00:27:30,700 --> 00:27:36,460
parameters I showed to the model levels

00:27:34,150 --> 00:27:39,850
and the image you want to classify and

00:27:36,460 --> 00:27:42,370
that's it okay and when you use HTTP you

00:27:39,850 --> 00:27:45,790
you have a little bit of installation to

00:27:42,370 --> 00:27:51,220
perform a setup but I mean it took five

00:27:45,790 --> 00:27:56,140
minutes very easy okay thanks a lot of

00:27:51,220 --> 00:27:57,460
questions over there yeah the processing

00:27:56,140 --> 00:27:59,560
time that you had that you can multitask

00:27:57,460 --> 00:28:02,140
the same module yes these multiple

00:27:59,560 --> 00:28:05,200
cameras you can schedule yes okay you

00:28:02,140 --> 00:28:07,360
call and I remember that I have to

00:28:05,200 --> 00:28:09,430
repeat the questions I've been asked to

00:28:07,360 --> 00:28:17,760
repeat the question just a side note for

00:28:09,430 --> 00:28:17,760
the previous question leave though bit

00:28:18,000 --> 00:28:23,500
so just a side note for the previous

00:28:20,200 --> 00:28:25,420
question that in c++ 2005 we added the

00:28:23,500 --> 00:28:27,190
opencv module so you can connect to

00:28:25,420 --> 00:28:29,260
whatever video source and capture image

00:28:27,190 --> 00:28:31,870
and detect motion without needing to

00:28:29,260 --> 00:28:34,660
write a line of code yeah yeah yeah I

00:28:31,870 --> 00:28:38,200
mean obviously it's because I'm lazy

00:28:34,660 --> 00:28:40,750
that I use a mean I find Java option but

00:28:38,200 --> 00:28:43,510
you can do exactly the same with minify

00:28:40,750 --> 00:28:45,640
C++ version and you will be able to run

00:28:43,510 --> 00:28:51,220
it on something else that Raspberry Pi

00:28:45,640 --> 00:28:53,380
and on very small devices or IOT stuff I

00:28:51,220 --> 00:28:55,240
wanted to know if there are significant

00:28:53,380 --> 00:28:58,930
cost differences with the three

00:28:55,240 --> 00:29:02,920
different ways in GCP so right now

00:28:58,930 --> 00:29:07,600
even the fact it's in beta it's not very

00:29:02,920 --> 00:29:09,400
costly but actually as a user of Google

00:29:07,600 --> 00:29:12,670
Cloud vision I received an email

00:29:09,400 --> 00:29:14,560
yesterday or two days ago saying we

00:29:12,670 --> 00:29:16,300
remind you that this product is going to

00:29:14,560 --> 00:29:20,620
be GA before the end of the year and

00:29:16,300 --> 00:29:25,660
this is going to cost that much so I

00:29:20,620 --> 00:29:28,360
don't remember the prices but you pay

00:29:25,660 --> 00:29:31,210
for the training of the model which is

00:29:28,360 --> 00:29:32,950
not very expensive obviously depends how

00:29:31,210 --> 00:29:35,110
much images you have in your data sets

00:29:32,950 --> 00:29:37,720
and how long it takes to train the model

00:29:35,110 --> 00:29:40,680
and you have a lot of parameters you can

00:29:37,720 --> 00:29:43,780
tune the model based on your needs and

00:29:40,680 --> 00:29:47,320
also based on where your model is going

00:29:43,780 --> 00:29:51,760
to run and you if you deploy the

00:29:47,320 --> 00:29:54,760
workflow in Google Cloud and expose API

00:29:51,760 --> 00:30:00,550
you also pay for each cone that is made

00:29:54,760 --> 00:30:12,850
with the API but it's it's very low as

00:30:00,550 --> 00:30:16,630
far as I know yep could you elaborate a

00:30:12,850 --> 00:30:24,130
bit on the qtg protocol it's used for

00:30:16,630 --> 00:30:26,830
and why mqtt is basically one of the

00:30:24,130 --> 00:30:29,890
protocol which is the most used for IOT

00:30:26,830 --> 00:30:32,520
communication and if I remember

00:30:29,890 --> 00:30:36,820
correctly you have two options for

00:30:32,520 --> 00:30:38,460
Google Cloud IOT MQTT and HTTP I'm not

00:30:36,820 --> 00:30:45,890
sure about this one

00:30:38,460 --> 00:30:48,720
so actually I find go back very quickly

00:30:45,890 --> 00:30:54,640
[Music]

00:30:48,720 --> 00:30:57,280
sorry here so actually this processor

00:30:54,640 --> 00:31:00,340
right now is not available in 95 by

00:30:57,280 --> 00:31:02,920
default this is a pull request I met a

00:31:00,340 --> 00:31:05,500
few months ago it's not in the codebase

00:31:02,920 --> 00:31:09,940
yet but I will make it available on my

00:31:05,500 --> 00:31:12,670
repository that you have you already

00:31:09,940 --> 00:31:12,909
have a MQTT processor available that you

00:31:12,670 --> 00:31:17,139
can

00:31:12,909 --> 00:31:19,539
use with edible us-asia IOT and an NG

00:31:17,139 --> 00:31:21,609
City I mean mqtt is the protocol that

00:31:19,539 --> 00:31:23,859
you would use with IOT for most of the

00:31:21,609 --> 00:31:27,840
cloud providers to get the data and also

00:31:23,859 --> 00:31:27,840
to send information back to the devices

00:31:30,509 --> 00:31:35,639
well it's a protocol so you send

00:31:33,249 --> 00:31:38,349
whatever payload you want in this case

00:31:35,639 --> 00:31:41,349
in this case in the first version when

00:31:38,349 --> 00:31:44,440
I'm not sending the inference I'm just

00:31:41,349 --> 00:31:46,840
sending the bytes of the picture really

00:31:44,440 --> 00:31:50,139
the picture itself and in the version

00:31:46,840 --> 00:31:52,269
later when I'm setting also the

00:31:50,139 --> 00:31:55,679
inference results I'm sending a JSON

00:31:52,269 --> 00:32:04,139
payload with the inference results and

00:31:55,679 --> 00:32:04,139
and the image with the base64 encoded

00:32:05,759 --> 00:32:10,720
yeah I hope this is not too basic

00:32:08,229 --> 00:32:12,429
question but you had a lot of steps and

00:32:10,720 --> 00:32:13,809
some of them were sort of executed on

00:32:12,429 --> 00:32:17,019
the edge somewhere in the cloud and so

00:32:13,809 --> 00:32:19,809
how do you sort of define in knife I

00:32:17,019 --> 00:32:22,570
tell it where to run which parts yeah

00:32:19,809 --> 00:32:27,999
there there is no dumb question all

00:32:22,570 --> 00:32:30,460
right so well basically mean I find is a

00:32:27,999 --> 00:32:33,629
low footprint version of knife I and on

00:32:30,460 --> 00:32:36,159
minify you don't have any UI okay so

00:32:33,629 --> 00:32:39,279
when you want to deploy your workflow on

00:32:36,159 --> 00:32:41,559
minify usual you usually develop it on a

00:32:39,279 --> 00:32:44,519
knife I instance then you export it

00:32:41,559 --> 00:32:47,889
convert it into an old file and then

00:32:44,519 --> 00:32:49,960
deploy it on your on your device that's

00:32:47,889 --> 00:32:54,639
usually what you do and if you also want

00:32:49,960 --> 00:32:57,549
to actually deploy new versions of your

00:32:54,639 --> 00:32:59,710
workflow on your devices you would use a

00:32:57,549 --> 00:33:02,979
combination of nine five registry and

00:32:59,710 --> 00:33:07,629
ific to serve that's something I

00:33:02,979 --> 00:33:10,269
published an article I published a while

00:33:07,629 --> 00:33:14,200
back but yeah that's something you can

00:33:10,269 --> 00:33:16,539
do but here it's just for clarity this

00:33:14,200 --> 00:33:19,019
is what I design on my knife I instance

00:33:16,539 --> 00:33:22,749
but then for the minify part I'm just

00:33:19,019 --> 00:33:26,499
exporting it and deploying because every

00:33:22,749 --> 00:33:27,040
slide I've seen on this part where there

00:33:26,499 --> 00:33:29,530
is a doctor

00:33:27,040 --> 00:33:31,900
line and says well this is on mini mini

00:33:29,530 --> 00:33:35,140
fire now and I just thought you sort of

00:33:31,900 --> 00:33:39,490
configure that in the IDE you know

00:33:35,140 --> 00:33:44,050
no actually so in this case our when I

00:33:39,490 --> 00:33:47,920
did it I developed my workflows exported

00:33:44,050 --> 00:33:50,010
it on my laptop and then SCP the file on

00:33:47,920 --> 00:33:53,560
the device restarted me now if I very

00:33:50,010 --> 00:33:56,080
very basic stuff but you can do it fully

00:33:53,560 --> 00:33:57,700
automatically with the knife our

00:33:56,080 --> 00:34:02,170
registry and knife I see two server

00:33:57,700 --> 00:34:04,300
which means that actually in knife I we

00:34:02,170 --> 00:34:07,210
have a notion of what we call templates

00:34:04,300 --> 00:34:10,000
so you can actually save a workflow as a

00:34:07,210 --> 00:34:12,190
template then the c2 server is gonna

00:34:10,000 --> 00:34:14,620
detect a new version of your template

00:34:12,190 --> 00:34:17,290
this template is going to be downloaded

00:34:14,620 --> 00:34:21,460
and forwarded to all the devices and

00:34:17,290 --> 00:34:24,040
then the new workflow is used on the

00:34:21,460 --> 00:34:26,440
devices as the new workflow that mean I

00:34:24,040 --> 00:34:28,750
if I should use but again there is no UI

00:34:26,440 --> 00:34:30,910
on minify so you have to give up the

00:34:28,750 --> 00:34:33,820
workflow somewhere else and then deploy

00:34:30,910 --> 00:34:43,240
the workflows on minify agents thank you

00:34:33,820 --> 00:34:45,100
welcome thank you someone suggested that

00:34:43,240 --> 00:34:47,590
I should have brought cookies and and

00:34:45,100 --> 00:34:51,310
shared cookies with you that I didn't

00:34:47,590 --> 00:34:53,560
real real-time classification what thank

00:34:51,310 --> 00:34:58,050
you thank you again Pierre

00:34:53,560 --> 00:34:58,050

YouTube URL: https://www.youtube.com/watch?v=QQks0KAvT58


