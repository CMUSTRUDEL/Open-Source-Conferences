Title: Introduction to Apache DataFu - William Vaughan
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,030 --> 00:00:07,140
okay hi everyone my name is Wil Vaughn I

00:00:04,020 --> 00:00:09,000
am an engineer at LinkedIn I've been

00:00:07,140 --> 00:00:11,420
part of the applied data products team

00:00:09,000 --> 00:00:14,790
for about the last three years or so and

00:00:11,420 --> 00:00:18,000
we build data products like people you

00:00:14,790 --> 00:00:20,160
may know and skills and endorsements and

00:00:18,000 --> 00:00:23,369
during the process of this we end up

00:00:20,160 --> 00:00:25,050
using Hadoop fairly extensively and a

00:00:23,369 --> 00:00:29,039
lot of high-level languages on top of

00:00:25,050 --> 00:00:31,590
Hadoop so I'm here to introduce Apache

00:00:29,039 --> 00:00:33,660
Day tofu which is a collection of

00:00:31,590 --> 00:00:35,780
libraries for working with high-level

00:00:33,660 --> 00:00:39,510
languages such as say Apache Pig

00:00:35,780 --> 00:00:41,640
currently we only support two libraries

00:00:39,510 --> 00:00:45,000
right now there's an apache pig library

00:00:41,640 --> 00:00:48,629
and an hourglass library and we began

00:00:45,000 --> 00:00:51,300
incubating this year so we'll start with

00:00:48,629 --> 00:00:52,890
the brief history a couple of years ago

00:00:51,300 --> 00:00:56,460
LinkedIn had a number of teams hood to

00:00:52,890 --> 00:00:58,320
develop generally useful UDF's

00:00:56,460 --> 00:01:01,289
and these ETF's were being shared across

00:00:58,320 --> 00:01:02,760
the company but there were a few

00:01:01,289 --> 00:01:05,640
problems they weren't in a centralized

00:01:02,760 --> 00:01:07,229
library which means that the latest

00:01:05,640 --> 00:01:09,990
version of the UDF was sometimes

00:01:07,229 --> 00:01:12,750
communicated via email it wasn't always

00:01:09,990 --> 00:01:15,270
properly maintained updates didn't

00:01:12,750 --> 00:01:17,189
distribute well and there was really no

00:01:15,270 --> 00:01:19,080
automated testing for these people Rotem

00:01:17,189 --> 00:01:22,520
once they worked then they assumed that

00:01:19,080 --> 00:01:24,780
they would continue to work this

00:01:22,520 --> 00:01:27,060
assumption wasn't exactly true

00:01:24,780 --> 00:01:28,710
and all the other problems that you

00:01:27,060 --> 00:01:31,220
would assume from things that are passed

00:01:28,710 --> 00:01:33,900
around via email started happening so

00:01:31,220 --> 00:01:36,710
okay we decided we can fix this so we

00:01:33,900 --> 00:01:39,329
created a single centralized library and

00:01:36,710 --> 00:01:41,460
we added in unit tests thanks to Pig

00:01:39,329 --> 00:01:44,369
unit which had come out and we started

00:01:41,460 --> 00:01:46,020
doing code coverage on this and we

00:01:44,369 --> 00:01:47,430
realized after a while that we had

00:01:46,020 --> 00:01:50,250
something that was actually pretty

00:01:47,430 --> 00:01:53,780
decent and would be generally useful to

00:01:50,250 --> 00:01:56,490
anyone who was working in Apache payeth

00:01:53,780 --> 00:01:58,520
anywhere so we decided to open source it

00:01:56,490 --> 00:02:02,030
and we open sourced this and github in

00:01:58,520 --> 00:02:05,420
September 2011 and then

00:02:02,030 --> 00:02:08,479
in September last year

00:02:05,420 --> 00:02:10,819
we hit a 1.0 at that point we decided

00:02:08,479 --> 00:02:12,950
that we wanted to bring it in to Apache

00:02:10,819 --> 00:02:16,400
to give it to the community and to help

00:02:12,950 --> 00:02:20,360
build up a community so what is data fou

00:02:16,400 --> 00:02:22,880
about it's about making it easier to

00:02:20,360 --> 00:02:26,000
work with large-scale data what does

00:02:22,880 --> 00:02:29,120
this mean it sort of depends but it's a

00:02:26,000 --> 00:02:31,340
combination of providing general purpose

00:02:29,120 --> 00:02:34,610
things that may make a particular

00:02:31,340 --> 00:02:36,800
framework easier and about providing

00:02:34,610 --> 00:02:39,440
some specific things so for example

00:02:36,800 --> 00:02:42,140
stats calculations were sampling

00:02:39,440 --> 00:02:45,140
algorithms things that are much more

00:02:42,140 --> 00:02:47,269
complicated to implement without some

00:02:45,140 --> 00:02:50,780
experience in that but which a lot of

00:02:47,269 --> 00:02:52,610
people can use right out of the box what

00:02:50,780 --> 00:02:57,110
else do we want to do we want it to be

00:02:52,610 --> 00:03:00,500
easy to contribute this is we want a

00:02:57,110 --> 00:03:02,780
community that our users of these

00:03:00,500 --> 00:03:04,910
large-scale library all right look users

00:03:02,780 --> 00:03:07,420
who work with large-scale data this

00:03:04,910 --> 00:03:09,920
could be data scientist data engineers

00:03:07,420 --> 00:03:12,079
software engineers we don't want it

00:03:09,920 --> 00:03:13,310
specialized to just people who are good

00:03:12,079 --> 00:03:15,620
at working on the internals of

00:03:13,310 --> 00:03:20,390
distributed systems or people who are

00:03:15,620 --> 00:03:21,530
just statisticians so what we've done is

00:03:20,390 --> 00:03:23,600
we've created a fair amount of

00:03:21,530 --> 00:03:26,090
documentation so far we've created the

00:03:23,600 --> 00:03:28,400
getting started guides and we're trying

00:03:26,090 --> 00:03:31,190
to maintain that so for example for data

00:03:28,400 --> 00:03:32,510
Phu peg if you want to add a UT out that

00:03:31,190 --> 00:03:34,730
you've written and you think would be

00:03:32,510 --> 00:03:37,220
useful to the community that you can

00:03:34,730 --> 00:03:39,019
just add the UDF add a test to go along

00:03:37,220 --> 00:03:41,570
with it and then ship it pretty easily

00:03:39,019 --> 00:03:45,709
so the barrier of entry for this

00:03:41,570 --> 00:03:48,620
shouldn't be that high so like I said

00:03:45,709 --> 00:03:51,140
the community is the people who used to

00:03:48,620 --> 00:03:53,630
do for working with data people who are

00:03:51,140 --> 00:03:55,489
using data food right now well it's used

00:03:53,630 --> 00:03:58,310
extensively at LinkedIn for a lot of

00:03:55,489 --> 00:04:00,500
these large-scale data products right

00:03:58,310 --> 00:04:05,810
now it's also including cloud era CDH

00:04:00,500 --> 00:04:08,600
and Apache big top so for the rest of

00:04:05,810 --> 00:04:11,510
this talk I'm going to sort of give this

00:04:08,600 --> 00:04:14,920
introduction as a series of examples of

00:04:11,510 --> 00:04:18,130
how to use data through inside of Pik

00:04:14,920 --> 00:04:21,669
because that is the first library that

00:04:18,130 --> 00:04:25,210
we came up with support for and the most

00:04:21,669 --> 00:04:28,150
complete at this point so what is data

00:04:25,210 --> 00:04:30,639
food pick well it's a collection of EDS

00:04:28,150 --> 00:04:33,340
for data analysis covering statistics

00:04:30,639 --> 00:04:36,460
bag operation set operations sessions

00:04:33,340 --> 00:04:38,650
sampling a whole bevy of things that a

00:04:36,460 --> 00:04:42,160
lot of Pig workflows are likely to do

00:04:38,650 --> 00:04:45,840
and the idea is to make it simpler to do

00:04:42,160 --> 00:04:47,919
these things so some of this is just

00:04:45,840 --> 00:04:50,680
general-purpose things that happen

00:04:47,919 --> 00:04:52,810
frequently one really common case is

00:04:50,680 --> 00:04:54,430
that you have a script you've got a

00:04:52,810 --> 00:04:55,960
bunch of nil values from your input data

00:04:54,430 --> 00:04:58,900
it you want to replace it with some

00:04:55,960 --> 00:05:00,970
value currently you can do that in Pig

00:04:58,900 --> 00:05:03,220
but it requires a writing out

00:05:00,970 --> 00:05:06,310
extraordinary operator the syntax is not

00:05:03,220 --> 00:05:08,949
very concise and you've got to be a

00:05:06,310 --> 00:05:10,330
little careful with schemas also if you

00:05:08,949 --> 00:05:11,919
have a bunch of values and you want to

00:05:10,330 --> 00:05:12,220
take the first non know value out of

00:05:11,919 --> 00:05:15,190
that

00:05:12,220 --> 00:05:17,800
well okay that's a big nested ternary

00:05:15,190 --> 00:05:20,650
operator there is no clean way to do it

00:05:17,800 --> 00:05:23,080
in pig latin how do you clean that up

00:05:20,650 --> 00:05:25,450
well you write a UDF that returns the

00:05:23,080 --> 00:05:28,990
first non null value so we have

00:05:25,450 --> 00:05:32,050
coalesced to do that so in the first

00:05:28,990 --> 00:05:35,080
example if the vowel is null we return

00:05:32,050 --> 00:05:37,960
zero and the second example we return

00:05:35,080 --> 00:05:40,539
the first of those values so that is non

00:05:37,960 --> 00:05:42,870
null we could add a comma zero and there

00:05:40,539 --> 00:05:46,030
in return to zero if they're all now

00:05:42,870 --> 00:05:49,479
this is sort of the example of a really

00:05:46,030 --> 00:05:52,660
simple general-purpose UDF that a bunch

00:05:49,479 --> 00:05:55,180
of people can pick up and use easily we

00:05:52,660 --> 00:05:56,889
want more of these in so we want to

00:05:55,180 --> 00:05:59,800
encourage the community to contribute

00:05:56,889 --> 00:06:04,030
these types of UDF's as we you know can

00:05:59,800 --> 00:06:05,500
tune you to build out this library an

00:06:04,030 --> 00:06:07,060
example of something a little bit more

00:06:05,500 --> 00:06:10,030
complicated is if we want to compute

00:06:07,060 --> 00:06:12,280
session statistics so say you've got a

00:06:10,030 --> 00:06:14,889
website and on that website you have

00:06:12,280 --> 00:06:17,110
members who browse the website for some

00:06:14,889 --> 00:06:18,880
period of time you want to see who are

00:06:17,110 --> 00:06:21,370
your most engaged members on the site

00:06:18,880 --> 00:06:22,870
and how long are they spending doing

00:06:21,370 --> 00:06:25,860
what and how long are your normal

00:06:22,870 --> 00:06:28,150
members spending doing various things so

00:06:25,860 --> 00:06:29,770
all right we've got some raw data

00:06:28,150 --> 00:06:32,200
which is a clickstream of what the users

00:06:29,770 --> 00:06:34,030
are doing the first thing is is we

00:06:32,200 --> 00:06:37,300
really wanted to find what's the session

00:06:34,030 --> 00:06:40,660
so a session is sustained user activity

00:06:37,300 --> 00:06:42,280
so in this case let's say that we will

00:06:40,660 --> 00:06:44,950
say a session ends once there's no

00:06:42,280 --> 00:06:46,870
activity for 10 minutes the users

00:06:44,950 --> 00:06:49,780
probably stopped browsing at that point

00:06:46,870 --> 00:06:53,350
and gone off and done something else so

00:06:49,780 --> 00:06:57,490
okay we'll define a session with that 10

00:06:53,350 --> 00:06:59,830
minute perm parameter expects isoform at

00:06:57,490 --> 00:07:03,220
a time so define another EDF that can

00:06:59,830 --> 00:07:06,580
convert to this then we can use session

00:07:03,220 --> 00:07:10,840
eyes so specialized simply goes in it

00:07:06,580 --> 00:07:13,440
takes ordered input data and a pins or a

00:07:10,840 --> 00:07:17,500
pre yeah pin so every tuple a session ID

00:07:13,440 --> 00:07:19,480
so the idea is that everything that is

00:07:17,500 --> 00:07:21,910
within this same session will get the

00:07:19,480 --> 00:07:25,240
same session ID so we can then go back

00:07:21,910 --> 00:07:28,510
and group by this session to apply some

00:07:25,240 --> 00:07:32,170
statistics so data foo also provides a

00:07:28,510 --> 00:07:37,150
number of standard statistics so median

00:07:32,170 --> 00:07:39,160
quantile variance so we also have

00:07:37,150 --> 00:07:41,560
streaming and non streaming versions of

00:07:39,160 --> 00:07:42,850
a number of our UDF's so the difference

00:07:41,560 --> 00:07:44,830
between this is for the streaming

00:07:42,850 --> 00:07:47,280
version there are approximations they

00:07:44,830 --> 00:07:51,300
don't require order to input data and

00:07:47,280 --> 00:07:54,760
they're streaming in the sense of

00:07:51,300 --> 00:07:56,500
working inside of MapReduce not in the

00:07:54,760 --> 00:07:58,900
sense of working with a true stream of

00:07:56,500 --> 00:08:01,210
data but they don't require ordered data

00:07:58,900 --> 00:08:03,520
we do have the exact versions as well

00:08:01,210 --> 00:08:05,230
they're a little bit slower they require

00:08:03,520 --> 00:08:09,340
you to go ahead and sort your input data

00:08:05,230 --> 00:08:10,870
before passing it in so okay we've

00:08:09,340 --> 00:08:13,150
defined these sessions that we want for

00:08:10,870 --> 00:08:16,570
our or these statistics that we want for

00:08:13,150 --> 00:08:18,640
our session so now we compute the

00:08:16,570 --> 00:08:22,330
session length it's fairly standard Pig

00:08:18,640 --> 00:08:26,770
and then compute the statistics by just

00:08:22,330 --> 00:08:30,400
passing it into the various arguments or

00:08:26,770 --> 00:08:32,500
into the various UDF's we can then find

00:08:30,400 --> 00:08:35,800
the most engaged users using the

00:08:32,500 --> 00:08:39,039
quantiles where we defined everyone in

00:08:35,800 --> 00:08:43,479
the 95th percentile or better as highly

00:08:39,039 --> 00:08:45,120
gaged so this is a very simple report

00:08:43,479 --> 00:08:48,610
that you could use for data analysis

00:08:45,120 --> 00:08:51,339
also since this is a pretty well tested

00:08:48,610 --> 00:08:55,269
UDF you can use this for production and

00:08:51,339 --> 00:08:57,399
we would encourage you to so another

00:08:55,269 --> 00:09:01,839
area that we've spent a lot of time

00:08:57,399 --> 00:09:06,209
addressing our bags so pig represents

00:09:01,839 --> 00:09:09,459
all collections as bags inside of a tube

00:09:06,209 --> 00:09:11,319
but the ways in standard pig latin that

00:09:09,459 --> 00:09:14,259
you can manipulate bags are pretty

00:09:11,319 --> 00:09:16,930
limited it's especially difficult if

00:09:14,259 --> 00:09:19,540
you're working with an inner bag so you

00:09:16,930 --> 00:09:21,940
want to do something to this bag inside

00:09:19,540 --> 00:09:25,810
of a nested block you can do a few

00:09:21,940 --> 00:09:27,790
things such as project it and there are

00:09:25,810 --> 00:09:30,730
a few operators that do work on it cross

00:09:27,790 --> 00:09:32,500
filter distinct a couple others but

00:09:30,730 --> 00:09:35,769
there are a number of common things that

00:09:32,500 --> 00:09:39,370
you might want to do that you can't so

00:09:35,769 --> 00:09:42,459
we have provided a number of UDF to app

00:09:39,370 --> 00:09:45,940
into a bag and preap into a bag at a

00:09:42,459 --> 00:09:48,639
tube on either end to catenate bags so

00:09:45,940 --> 00:09:50,620
that if you have two bags you can

00:09:48,639 --> 00:09:52,779
combine them into one or if you have a

00:09:50,620 --> 00:09:58,540
bag of bags you can combine it into a

00:09:52,779 --> 00:10:00,670
single bag and also to split a bag up so

00:09:58,540 --> 00:10:03,339
we also provide UDF's that lets you

00:10:00,670 --> 00:10:04,810
operate on bags in much the same way

00:10:03,339 --> 00:10:08,649
that you would work with the top level

00:10:04,810 --> 00:10:11,490
relation so why is this useful well this

00:10:08,649 --> 00:10:14,949
means that inside of your nested block

00:10:11,490 --> 00:10:17,139
you can do things like group a bag or

00:10:14,949 --> 00:10:19,510
count the number of distinct items in

00:10:17,139 --> 00:10:22,600
the bag or join two bags together and a

00:10:19,510 --> 00:10:24,760
key when would you want to do this well

00:10:22,600 --> 00:10:26,380
it will save you a MapReduce job when

00:10:24,760 --> 00:10:29,350
you would otherwise probably have to

00:10:26,380 --> 00:10:31,029
flatten this relationship out do the

00:10:29,350 --> 00:10:33,399
MapReduce job to do your group or your

00:10:31,029 --> 00:10:37,569
join or such and then maybe regroup

00:10:33,399 --> 00:10:41,139
again so let's consider an example for

00:10:37,569 --> 00:10:44,519
this alright we've got a system where

00:10:41,139 --> 00:10:47,319
we're recommending the user items and

00:10:44,519 --> 00:10:50,139
the user can act to accept or reject to

00:10:47,319 --> 00:10:52,000
these recommendations so we're going to

00:10:50,139 --> 00:10:53,240
read in impressions which are things

00:10:52,000 --> 00:10:57,020
that we show to the

00:10:53,240 --> 00:10:59,000
except things that the user decided yes

00:10:57,020 --> 00:11:00,830
I'll accept that recommendation and the

00:10:59,000 --> 00:11:04,640
things that the user explicitly said no

00:11:00,830 --> 00:11:07,430
I don't want that okay what do we want

00:11:04,640 --> 00:11:12,460
to do we want to know for each user all

00:11:07,430 --> 00:11:15,470
the items and how many times the user

00:11:12,460 --> 00:11:16,910
saw it how many times user accepted the

00:11:15,470 --> 00:11:21,080
recommendation and how many times user

00:11:16,910 --> 00:11:23,210
said nope I'm not interested so there's

00:11:21,080 --> 00:11:27,080
a naive approach to doing this which

00:11:23,210 --> 00:11:29,840
would be to take this data that you've

00:11:27,080 --> 00:11:32,420
loaded in group it up by item and by

00:11:29,840 --> 00:11:34,910
user for each of impressions accepts and

00:11:32,420 --> 00:11:36,770
rejects then do a left join on

00:11:34,910 --> 00:11:40,100
impressions to bring this together then

00:11:36,770 --> 00:11:43,520
do another left join to bring in rejects

00:11:40,100 --> 00:11:45,320
and this ends up being a large number of

00:11:43,520 --> 00:11:47,360
MapReduce job so it ends up being a

00:11:45,320 --> 00:11:51,230
fairly slow series of things to pull

00:11:47,360 --> 00:11:53,480
together so okay well a better approach

00:11:51,230 --> 00:11:56,930
is to use Co group so you can Co group

00:11:53,480 --> 00:11:58,670
by the user ID and the item ID and then

00:11:56,930 --> 00:12:01,430
pull these together then you can do your

00:11:58,670 --> 00:12:06,350
counts and then you group again to get

00:12:01,430 --> 00:12:10,640
it to the user level it's a bit wasteful

00:12:06,350 --> 00:12:12,230
if I have to do this so because you know

00:12:10,640 --> 00:12:14,330
that you're processing the same data

00:12:12,230 --> 00:12:16,180
over and over all you're really doing is

00:12:14,330 --> 00:12:18,350
transforming the way it looks and

00:12:16,180 --> 00:12:23,090
slicing and dicing on slightly different

00:12:18,350 --> 00:12:25,970
levels one thing that I've learned while

00:12:23,090 --> 00:12:29,270
working in these data products is that

00:12:25,970 --> 00:12:32,480
even really big data once you start

00:12:29,270 --> 00:12:35,150
slicing and dicing it becomes small or

00:12:32,480 --> 00:12:37,280
at least manageably small where in this

00:12:35,150 --> 00:12:39,800
case management only small may mean if

00:12:37,280 --> 00:12:43,250
it's in memory well I wouldn't expect

00:12:39,800 --> 00:12:47,480
that any particular users set of items

00:12:43,250 --> 00:12:50,330
to ever run out of memory I mean a

00:12:47,480 --> 00:12:51,410
couple gigs the data would be a lot so

00:12:50,330 --> 00:12:54,110
all right

00:12:51,410 --> 00:12:55,880
let's consider that one user at a time

00:12:54,110 --> 00:12:59,720
is there a way that we can make this a

00:12:55,880 --> 00:13:02,060
little bit more optimal so let's only

00:12:59,720 --> 00:13:05,390
group once let's group for the user then

00:13:02,060 --> 00:13:06,810
we have all the users items now can't we

00:13:05,390 --> 00:13:10,080
just in mmm

00:13:06,810 --> 00:13:12,900
through bag manipulation do all the

00:13:10,080 --> 00:13:14,700
other aggregation that we want so okay

00:13:12,900 --> 00:13:17,010
we're going to define three UDF's we'll

00:13:14,700 --> 00:13:19,590
reuse colas and because it's convenient

00:13:17,010 --> 00:13:20,970
and then we'll have count each and

00:13:19,590 --> 00:13:23,070
bhagath outer join

00:13:20,970 --> 00:13:24,630
count each does pretty much what you

00:13:23,070 --> 00:13:26,880
might think it counts the number of

00:13:24,630 --> 00:13:28,740
times a particular tuple appears in the

00:13:26,880 --> 00:13:31,170
bag and returns that tuple with the

00:13:28,740 --> 00:13:33,570
count the bag left outer join

00:13:31,170 --> 00:13:36,990
well performs a left outer join using

00:13:33,570 --> 00:13:39,890
bags and set of relations so here's how

00:13:36,990 --> 00:13:43,380
it works we have this large co group

00:13:39,890 --> 00:13:47,400
where we pull in everything by the user

00:13:43,380 --> 00:13:50,430
ID we then count the number of times

00:13:47,400 --> 00:13:53,460
every item appears as an impression and

00:13:50,430 --> 00:13:57,930
accept or reject and then we do a bag

00:13:53,460 --> 00:14:03,420
outer join on these to get all of these

00:13:57,930 --> 00:14:06,480
combined into one tuple so then we can

00:14:03,420 --> 00:14:11,250
revisit coalesce and take the results of

00:14:06,480 --> 00:14:13,290
this to get our counts and add in zeros

00:14:11,250 --> 00:14:16,800
as default values for things that were

00:14:13,290 --> 00:14:21,030
never accepted or rejected so this

00:14:16,800 --> 00:14:24,060
approach allows us to trim this previous

00:14:21,030 --> 00:14:26,430
job which was to MapReduce jobs once the

00:14:24,060 --> 00:14:29,520
big plan execute down to one MapReduce

00:14:26,430 --> 00:14:32,990
job and a couple of experiments that

00:14:29,520 --> 00:14:35,640
I've run this is anywhere between a

00:14:32,990 --> 00:14:40,080
one-third and one-half wall clock

00:14:35,640 --> 00:14:43,560
savings and about a third to a quarter

00:14:40,080 --> 00:14:45,870
of actual total cluster resources so I

00:14:43,560 --> 00:14:48,360
mean it is beneficial in terms of

00:14:45,870 --> 00:14:50,700
performance in some ways it's also

00:14:48,360 --> 00:14:53,280
easier to read because you're keeping

00:14:50,700 --> 00:14:59,190
everything grouped together a little bit

00:14:53,280 --> 00:15:00,870
more logically okay so another area that

00:14:59,190 --> 00:15:05,100
we're trying to tackle with data foo is

00:15:00,870 --> 00:15:06,510
sampling frequently when you're working

00:15:05,100 --> 00:15:07,680
with large scale data you don't want to

00:15:06,510 --> 00:15:10,380
work with all of it you want to only

00:15:07,680 --> 00:15:12,720
bring it down to Sun Pig Latin has a

00:15:10,380 --> 00:15:15,300
sample operator but it is basically

00:15:12,720 --> 00:15:18,510
random so sometimes

00:15:15,300 --> 00:15:19,940
random sampling doesn't work exactly for

00:15:18,510 --> 00:15:21,620
your case so

00:15:19,940 --> 00:15:23,180
let's say we have this same previous

00:15:21,620 --> 00:15:26,720
example where we are letting in the

00:15:23,180 --> 00:15:29,330
impressions accepts and rejects but

00:15:26,720 --> 00:15:31,520
we've got a problem because if we just

00:15:29,330 --> 00:15:34,250
randomly sample each of those we're not

00:15:31,520 --> 00:15:35,570
guaranteed that the same keys are going

00:15:34,250 --> 00:15:38,540
to show up in the correct distribution

00:15:35,570 --> 00:15:41,030
and all of these so we want to make sure

00:15:38,540 --> 00:15:44,660
that if I have the particular user ID

00:15:41,030 --> 00:15:46,760
and that exists in impression so that we

00:15:44,660 --> 00:15:51,410
it also is going to exist in accepts and

00:15:46,760 --> 00:15:54,140
rejects so we provide sample by key so

00:15:51,410 --> 00:15:58,520
sample by key we'll take a salt and a

00:15:54,140 --> 00:16:01,820
percentage and then works as a filter

00:15:58,520 --> 00:16:03,650
func that can go through and process all

00:16:01,820 --> 00:16:06,680
of these relations and we'll pull out

00:16:03,650 --> 00:16:08,840
things and you will guarantee that keys

00:16:06,680 --> 00:16:13,430
that appear in a will also appear in B

00:16:08,840 --> 00:16:17,360
if it exists okay

00:16:13,430 --> 00:16:19,790
all right let's tackle one other common

00:16:17,360 --> 00:16:22,810
thing that happens in pig that is

00:16:19,790 --> 00:16:24,470
actually sort of difficult to deal with

00:16:22,810 --> 00:16:28,040
left outer joins

00:16:24,470 --> 00:16:30,340
so pig latin has support for a left

00:16:28,040 --> 00:16:30,340
outer join

00:16:30,400 --> 00:16:35,120
unfortunately what it does not have

00:16:32,300 --> 00:16:37,850
support for is left outer joining more

00:16:35,120 --> 00:16:41,140
than two relations together this

00:16:37,850 --> 00:16:44,270
actually ends up being really common and

00:16:41,140 --> 00:16:47,600
it would be nice to just write this but

00:16:44,270 --> 00:16:51,230
it's not legal so how can you deal with

00:16:47,600 --> 00:16:53,720
this one way to do it is to write a

00:16:51,230 --> 00:16:56,600
series of outer joins

00:16:53,720 --> 00:16:58,850
that becomes expensive both in terms of

00:16:56,600 --> 00:17:01,670
MapReduce operations that it requires

00:16:58,850 --> 00:17:03,680
one per and it also becomes really

00:17:01,670 --> 00:17:05,000
unpleasant to write if you're actually

00:17:03,680 --> 00:17:07,910
trying to project out and keep your

00:17:05,000 --> 00:17:10,400
namespaces because you have to do this

00:17:07,910 --> 00:17:16,370
after every join or at least once at the

00:17:10,400 --> 00:17:19,430
end so it's not the most elegant way to

00:17:16,370 --> 00:17:23,750
go about it you can use Co group to do

00:17:19,430 --> 00:17:25,520
something very similar however if once

00:17:23,750 --> 00:17:28,910
you if you want it in the same format

00:17:25,520 --> 00:17:31,470
that you would get as a join so without

00:17:28,910 --> 00:17:35,220
a bunch of bags involved

00:17:31,470 --> 00:17:37,860
and you've got to flatten it out if you

00:17:35,220 --> 00:17:41,190
flatten an empty bag and pick then that

00:17:37,860 --> 00:17:43,500
road disappears so you want to convert

00:17:41,190 --> 00:17:46,740
the bag to null and you want the correct

00:17:43,500 --> 00:17:50,159
fields out of the bag so the syntax to

00:17:46,740 --> 00:17:52,129
do that is a little bit unpretty if you

00:17:50,159 --> 00:17:55,649
want to keep your schemas correct

00:17:52,129 --> 00:17:56,789
it's also very error-prone because you

00:17:55,649 --> 00:18:01,350
have to keep your schemas correct

00:17:56,789 --> 00:18:03,600
manually so we wrote a very simple UDF

00:18:01,350 --> 00:18:06,600
that just takes an empty bag and

00:18:03,600 --> 00:18:09,240
generates no fields for everything in

00:18:06,600 --> 00:18:12,960
the tuple of that bag it's a lot cleaner

00:18:09,240 --> 00:18:14,700
and easier to use so now you can

00:18:12,960 --> 00:18:18,679
actually write a macro for a left outer

00:18:14,700 --> 00:18:22,759
join and pass in relations and keys and

00:18:18,679 --> 00:18:22,759
then perform this operation

00:18:22,789 --> 00:18:32,730
very simply okay now onto a personal pet

00:18:30,059 --> 00:18:38,009
peeve of mine schemas and aliases and

00:18:32,730 --> 00:18:41,759
Pig a very common bad practice in pig is

00:18:38,009 --> 00:18:44,370
to use positional annotation I'm sure

00:18:41,759 --> 00:18:47,129
most people have had a CS 101 lecture

00:18:44,370 --> 00:18:50,909
about why you should use decent variable

00:18:47,129 --> 00:18:55,110
names and I don't know why Pig

00:18:50,909 --> 00:18:58,080
developers love disregarding this this

00:18:55,110 --> 00:19:00,149
is especially bad in situations where

00:18:58,080 --> 00:19:03,269
you don't control all of your data

00:19:00,149 --> 00:19:05,429
exactly if you're ingesting data from

00:19:03,269 --> 00:19:07,769
another source that you didn't create

00:19:05,429 --> 00:19:10,769
it's very possible that at some point in

00:19:07,769 --> 00:19:12,629
time this data might change slightly if

00:19:10,769 --> 00:19:15,450
you're using positional notation this

00:19:12,629 --> 00:19:18,210
means if they put anything new in the

00:19:15,450 --> 00:19:20,639
data that's not at the end something is

00:19:18,210 --> 00:19:22,919
going to break it's especially bad when

00:19:20,639 --> 00:19:25,220
it's something that otherwise fits the

00:19:22,919 --> 00:19:28,320
schema and things just operate normally

00:19:25,220 --> 00:19:30,509
okay it's bad enough in Pig scripts it

00:19:28,320 --> 00:19:32,820
gets a little bit worse even in UTS

00:19:30,509 --> 00:19:34,950
because the code for your UDF is

00:19:32,820 --> 00:19:37,879
separated from your script it's very

00:19:34,950 --> 00:19:42,809
easy to make a change in the script that

00:19:37,879 --> 00:19:44,480
looks good and executes and does not

00:19:42,809 --> 00:19:46,610
make the UDF do the right

00:19:44,480 --> 00:19:49,250
thing anymore so let's give an example

00:19:46,610 --> 00:19:52,820
of how that works let's say that we're

00:19:49,250 --> 00:19:55,070
calculating mortgage payments okay great

00:19:52,820 --> 00:19:57,140
so we've got a big file with principles

00:19:55,070 --> 00:19:58,070
number of payments and some a bag of

00:19:57,140 --> 00:20:02,299
interest rates that we're going

00:19:58,070 --> 00:20:04,220
calculate payments for all right so we

00:20:02,299 --> 00:20:05,990
write a UDF for this it looks something

00:20:04,220 --> 00:20:08,210
like this you start by getting the

00:20:05,990 --> 00:20:11,240
inputs to your UTF you pull them out of

00:20:08,210 --> 00:20:15,049
the position by tuples you cast them to

00:20:11,240 --> 00:20:16,809
whatever you expect them to be then you

00:20:15,049 --> 00:20:19,549
do some sort of computation with that

00:20:16,809 --> 00:20:21,710
and you're generating a bunch of tuples

00:20:19,549 --> 00:20:27,530
you toss them into a bag and you return

00:20:21,710 --> 00:20:29,450
the bag okay great so now you have this

00:20:27,530 --> 00:20:33,640
bag coming out you've got your UDF

00:20:29,450 --> 00:20:36,169
working you include it in the script and

00:20:33,640 --> 00:20:38,030
you maybe you do better than you

00:20:36,169 --> 00:20:39,200
actually doing positional notation in

00:20:38,030 --> 00:20:42,710
the script maybe you actually use

00:20:39,200 --> 00:20:45,650
aliases okay so this executes for a

00:20:42,710 --> 00:20:47,600
while it's running the data is good so

00:20:45,650 --> 00:20:49,400
people are now using it later on in a

00:20:47,600 --> 00:20:52,660
pipeline maybe it's flowing into

00:20:49,400 --> 00:20:56,240
production somehow directly okay

00:20:52,660 --> 00:20:59,450
somebody comes in a few months down the

00:20:56,240 --> 00:21:02,270
road and needs to change things for some

00:20:59,450 --> 00:21:04,850
reason so or maybe the input data

00:21:02,270 --> 00:21:07,040
changes and a field is prepended to the

00:21:04,850 --> 00:21:08,900
tuples and the interest rate back and

00:21:07,040 --> 00:21:11,150
let's say this is something like a week

00:21:08,900 --> 00:21:12,650
over week change that happens to be the

00:21:11,150 --> 00:21:17,590
same type as the field that you were

00:21:12,650 --> 00:21:21,260
reading so now what's happened is that

00:21:17,590 --> 00:21:23,390
your UDF is taking different data as

00:21:21,260 --> 00:21:25,940
input no error is going to generate

00:21:23,390 --> 00:21:29,480
because well it's approximately the

00:21:25,940 --> 00:21:30,980
right type and your scripts going to

00:21:29,480 --> 00:21:32,660
continue to work you're not going to

00:21:30,980 --> 00:21:36,080
know that anything is wrong until

00:21:32,660 --> 00:21:40,070
someone else later down the line notices

00:21:36,080 --> 00:21:42,549
when the data is weird because the right

00:21:40,070 --> 00:21:44,690
results are no longer coming out and

00:21:42,549 --> 00:21:47,990
it's going to be difficult for you to

00:21:44,690 --> 00:21:49,880
figure out why the data is weird perhaps

00:21:47,990 --> 00:21:51,530
maybe you can trace it back to a

00:21:49,880 --> 00:21:52,760
particular point in time but you're

00:21:51,530 --> 00:21:55,790
going to have to do a little bit of data

00:21:52,760 --> 00:21:58,040
archeology to to come back and find oh

00:21:55,790 --> 00:21:59,810
this is the simple change

00:21:58,040 --> 00:22:01,730
and at some point here I have to take

00:21:59,810 --> 00:22:04,250
your UDF on one hand your script on the

00:22:01,730 --> 00:22:07,540
other and say it's no longer exactly

00:22:04,250 --> 00:22:10,420
where I expect these type of problems

00:22:07,540 --> 00:22:15,470
make the software engineer and me cringe

00:22:10,420 --> 00:22:18,680
and they happen far too frequently so

00:22:15,470 --> 00:22:20,870
there are ways to get around this one

00:22:18,680 --> 00:22:23,090
and the easiest way is to write the EDF

00:22:20,870 --> 00:22:27,260
to fetch arguments by name using the

00:22:23,090 --> 00:22:30,380
schema so now you do have a dependency

00:22:27,260 --> 00:22:32,150
on your schema names not changing but at

00:22:30,380 --> 00:22:34,580
least in this point if you change the

00:22:32,150 --> 00:22:38,090
name it breaks you're much less likely

00:22:34,580 --> 00:22:40,520
to make a sort of incompatible change

00:22:38,090 --> 00:22:43,970
that accidentally goes through without

00:22:40,520 --> 00:22:46,220
breaking so we have a leasable of alpha

00:22:43,970 --> 00:22:48,160
and data food that can help with this so

00:22:46,220 --> 00:22:51,800
we have a bunch of helper methods that

00:22:48,160 --> 00:22:56,090
lets you say get a double from a tuple

00:22:51,800 --> 00:22:57,560
by the name of the schema and this is

00:22:56,090 --> 00:23:01,040
how you could rewrite that mortgage

00:22:57,560 --> 00:23:03,290
payment UDF doing that instead of using

00:23:01,040 --> 00:23:06,620
positional and notation you just use the

00:23:03,290 --> 00:23:08,840
schema name and then inside of the bag

00:23:06,620 --> 00:23:12,850
you can also use the same sort of schema

00:23:08,840 --> 00:23:12,850
name and this would solve your problem

00:23:13,030 --> 00:23:19,250
so there are few other awesome things

00:23:15,620 --> 00:23:21,770
that are in data foo Pig so we've got

00:23:19,250 --> 00:23:24,140
functions for calculating entropy open

00:23:21,770 --> 00:23:26,720
NLP wrappers some new and improved

00:23:24,140 --> 00:23:29,480
sampling UDF's for various styles of

00:23:26,720 --> 00:23:32,780
sampling some additional bag UDF's that

00:23:29,480 --> 00:23:35,240
are coming on something for in hash set

00:23:32,780 --> 00:23:37,490
that functions very similar to the bloom

00:23:35,240 --> 00:23:41,900
filter UDF's that are available right

00:23:37,490 --> 00:23:43,820
now and a few more things but I think

00:23:41,900 --> 00:23:48,730
next I'm going to talk about a data foo

00:23:43,820 --> 00:23:52,280
hourglass so data food hourglass is

00:23:48,730 --> 00:23:53,960
another library that we have and the

00:23:52,280 --> 00:23:56,950
purpose of hourglass is to make it

00:23:53,960 --> 00:24:00,260
easier to calculate or to do incremental

00:23:56,950 --> 00:24:03,830
calculations on data so why would you

00:24:00,260 --> 00:24:09,670
want to do this well it's very typical

00:24:03,830 --> 00:24:11,690
that in an online website you have

00:24:09,670 --> 00:24:14,270
services that are instrumented to

00:24:11,690 --> 00:24:17,300
collect events and these events are

00:24:14,270 --> 00:24:20,120
going to get ETL into Hadoop for later

00:24:17,300 --> 00:24:23,150
analysis so you know really common

00:24:20,120 --> 00:24:24,380
things are things like page views maybe

00:24:23,150 --> 00:24:27,080
you want to calculate the number of

00:24:24,380 --> 00:24:30,050
active users maybe you're going to end

00:24:27,080 --> 00:24:32,270
up using it in some sort of machine

00:24:30,050 --> 00:24:35,120
learning system later on to discount

00:24:32,270 --> 00:24:36,650
based on impressions or whatever so the

00:24:35,120 --> 00:24:39,080
events come in they float through some

00:24:36,650 --> 00:24:41,500
ETL they end up in Hadoop and they're

00:24:39,080 --> 00:24:43,340
going to end up likely by topics and

00:24:41,500 --> 00:24:48,230
potentially they're going to end up

00:24:43,340 --> 00:24:50,570
there sorted by day maybe they're even

00:24:48,230 --> 00:24:52,520
sorted by hour but let's just use the

00:24:50,570 --> 00:24:54,440
day for now so you have these various

00:24:52,520 --> 00:24:58,040
topics the types of events that are

00:24:54,440 --> 00:25:00,650
firing off you have this sort of schema

00:24:58,040 --> 00:25:03,260
where they're stored with the topic and

00:25:00,650 --> 00:25:06,680
a daily format and the day referencing

00:25:03,260 --> 00:25:09,460
what the data files represent and you

00:25:06,680 --> 00:25:13,730
want to do computations overtime windows

00:25:09,460 --> 00:25:16,280
so most of these computations generally

00:25:13,730 --> 00:25:18,710
take one of two forms they're either a

00:25:16,280 --> 00:25:20,810
fixed start window which says well I

00:25:18,710 --> 00:25:22,460
began producing data on this day and I

00:25:20,810 --> 00:25:24,920
want to consider everything going

00:25:22,460 --> 00:25:28,280
forwards or they're a fixed window

00:25:24,920 --> 00:25:34,100
length so I want to consider the last 30

00:25:28,280 --> 00:25:35,840
days of data for example the easiest way

00:25:34,100 --> 00:25:38,060
to deal with this especially if you're

00:25:35,840 --> 00:25:43,160
using something like Pig is to just

00:25:38,060 --> 00:25:45,980
recompute all this data daily so you go

00:25:43,160 --> 00:25:49,160
in you say load all the data or load the

00:25:45,980 --> 00:25:51,770
last 30 days of data and you do a group

00:25:49,160 --> 00:25:56,930
an account for example this would be a

00:25:51,770 --> 00:25:59,390
really standard operation but from day

00:25:56,930 --> 00:26:04,070
to day your inputs not changing very

00:25:59,390 --> 00:26:07,450
much you know one out of the one day has

00:26:04,070 --> 00:26:10,550
changed 29 days are still the same so

00:26:07,450 --> 00:26:12,230
you're being fairly inefficient if you

00:26:10,550 --> 00:26:14,810
add this up over a bunch of different

00:26:12,230 --> 00:26:17,000
jobs for a bunch of groups for a bunch

00:26:14,810 --> 00:26:18,770
of topics it can put a lot more load on

00:26:17,000 --> 00:26:21,010
your cluster than you actually really

00:26:18,770 --> 00:26:21,010
need

00:26:22,010 --> 00:26:28,669
same thing so fixed-length wind up

00:26:26,269 --> 00:26:31,340
you've got this one day extra you want

00:26:28,669 --> 00:26:38,330
to lose an old day okay how do you do

00:26:31,340 --> 00:26:42,260
this if more effectively so hourglass

00:26:38,330 --> 00:26:47,929
has this idea of a partition collapsing

00:26:42,260 --> 00:26:50,360
job so you want to go through and say

00:26:47,929 --> 00:26:51,500
count the number of pageviews per member

00:26:50,360 --> 00:26:55,399
okay

00:26:51,500 --> 00:26:59,139
fairly standard count and pig group by

00:26:55,399 --> 00:27:02,149
member ID generate member ID and count

00:26:59,139 --> 00:27:04,220
so this is what you want as they are

00:27:02,149 --> 00:27:07,429
output a very incremental job you're

00:27:04,220 --> 00:27:12,789
going to read in say your you know last

00:27:07,429 --> 00:27:16,340
in number of days and output this so

00:27:12,789 --> 00:27:19,130
this in this case this is an arithmetic

00:27:16,340 --> 00:27:21,529
operation we can merge the data with the

00:27:19,130 --> 00:27:23,659
previous output so we can take just the

00:27:21,529 --> 00:27:27,200
new day's data calculate our aggregates

00:27:23,659 --> 00:27:32,480
on that merge it with the old and print

00:27:27,200 --> 00:27:35,269
it out so the architecture looks

00:27:32,480 --> 00:27:38,059
something like this you add in the new

00:27:35,269 --> 00:27:41,470
day you map back through all of the

00:27:38,059 --> 00:27:45,470
stuff that you've calculated you then

00:27:41,470 --> 00:27:53,570
reduce it in this merge step and you

00:27:45,470 --> 00:27:55,220
have an output on your day so for a

00:27:53,570 --> 00:27:58,940
fixed length job you can do something

00:27:55,220 --> 00:28:02,360
similar in this case if I've got thirty

00:27:58,940 --> 00:28:06,260
days of data then I want to take the 30

00:28:02,360 --> 00:28:08,389
days that I had previously I get one new

00:28:06,260 --> 00:28:11,600
day I want to calculate my your gets on

00:28:08,389 --> 00:28:15,230
that I want to take my oldest day drop

00:28:11,600 --> 00:28:19,700
those off and add the new day in so it's

00:28:15,230 --> 00:28:23,559
an added in a subtract so the framework

00:28:19,700 --> 00:28:23,559
supports doing this job very easily

00:28:26,149 --> 00:28:29,929
and here's an architecture diagram of

00:28:28,669 --> 00:28:32,829
what it's going to look like once you've

00:28:29,929 --> 00:28:37,399
applied it to a fixed length window

00:28:32,829 --> 00:28:40,489
composition so the key here is that for

00:28:37,399 --> 00:28:46,999
the hourglass jobs you don't have to do

00:28:40,489 --> 00:28:49,849
a lot of work you have to write the map

00:28:46,999 --> 00:28:52,309
and they accumulate and then we've got

00:28:49,849 --> 00:28:54,769
the merge in the unmerge which are if

00:28:52,309 --> 00:29:01,189
you want to collapse or uncollapse your

00:28:54,769 --> 00:29:03,529
partitions those are optional so that's

00:29:01,189 --> 00:29:06,349
the easy example for doing something

00:29:03,529 --> 00:29:09,439
that's basically an algebraic or

00:29:06,349 --> 00:29:11,689
accumulative operation so some

00:29:09,439 --> 00:29:16,249
operations you can't do this

00:29:11,689 --> 00:29:18,049
so maximum men are a good example if you

00:29:16,249 --> 00:29:21,049
can't take the max at the last 30 days

00:29:18,049 --> 00:29:23,419
see if the nude newest day is a max and

00:29:21,049 --> 00:29:25,819
then drop off the last max it doesn't

00:29:23,419 --> 00:29:28,879
quite work you have to store all the

00:29:25,819 --> 00:29:35,719
intermediate data still somehow so you

00:29:28,879 --> 00:29:40,009
can use the output still per day but you

00:29:35,719 --> 00:29:42,289
have to partition the output data so we

00:29:40,009 --> 00:29:45,529
have a diagram here that shows sort of

00:29:42,289 --> 00:29:47,539
how that works so you instead of having

00:29:45,529 --> 00:29:52,699
a single collapsed partition you have to

00:29:47,539 --> 00:29:54,559
have multiple partitions one per day and

00:29:52,699 --> 00:29:56,719
this is what that architecture looks

00:29:54,559 --> 00:29:59,899
like for every day it runs its own

00:29:56,719 --> 00:30:01,909
pipeline through the advantage here

00:29:59,899 --> 00:30:04,249
though is that you don't have to repeat

00:30:01,909 --> 00:30:07,039
days that you've already processed so

00:30:04,249 --> 00:30:08,629
you'll have a cost of say 30 the first

00:30:07,039 --> 00:30:10,729
time you run it and then it's one day

00:30:08,629 --> 00:30:11,929
per and then you've got your cost in

00:30:10,729 --> 00:30:16,239
your aggregates but you've already

00:30:11,929 --> 00:30:16,239
reduced your data size tremendously

00:30:18,900 --> 00:30:24,240
so in summary our glass supports two

00:30:22,110 --> 00:30:28,620
types of jobs one is partitioned

00:30:24,240 --> 00:30:30,570
preserving where consume partition input

00:30:28,620 --> 00:30:32,790
data and it produces partitioned output

00:30:30,570 --> 00:30:35,910
data the other one is a partition

00:30:32,790 --> 00:30:37,740
collapsing job where it consumes the

00:30:35,910 --> 00:30:46,470
partition input data and produces a

00:30:37,740 --> 00:30:48,150
single output file so now the advantage

00:30:46,470 --> 00:30:50,160
of this framework is that what do you

00:30:48,150 --> 00:30:52,830
have to do all you have to do is

00:30:50,160 --> 00:30:55,380
implement map and accumulate so how do I

00:30:52,830 --> 00:30:58,080
read my data in and what is the

00:30:55,380 --> 00:31:01,290
accumulating function for it

00:30:58,080 --> 00:31:03,900
optionally depending on if you want to

00:31:01,290 --> 00:31:06,590
do a partition collapsing you can

00:31:03,900 --> 00:31:09,590
implement the merge in the unmerge

00:31:06,590 --> 00:31:09,590
so

00:31:11,630 --> 00:31:16,960
that is about all I have so do we have

00:31:14,480 --> 00:31:16,960
any questions

00:31:29,070 --> 00:31:33,909
okay

00:31:31,100 --> 00:31:33,909

YouTube URL: https://www.youtube.com/watch?v=JWI9tVsQ1cY


