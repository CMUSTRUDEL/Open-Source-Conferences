Title: Scaling MQTT Using Kafka - Tim Kellogg
Publication date: 2014-04-21
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,030 --> 00:00:05,549
yes I'm a software engineer with

00:00:01,829 --> 00:00:06,899
telemetry thanks for coming to there no

00:00:05,549 --> 00:00:09,120
I'm like the last thing standing between

00:00:06,899 --> 00:00:13,620
you and happy hour so just kind of bear

00:00:09,120 --> 00:00:16,139
with me for a second so anyway just to

00:00:13,620 --> 00:00:18,080
get into it this is going to be a little

00:00:16,139 --> 00:00:21,109
bit different than the other talks today

00:00:18,080 --> 00:00:23,539
first of all telemetry deals mostly with

00:00:21,109 --> 00:00:25,439
something called the Internet of Things

00:00:23,539 --> 00:00:28,109
which is a little bit different than

00:00:25,439 --> 00:00:31,529
your typical cloud platform the Internet

00:00:28,109 --> 00:00:33,510
of Things is a concept that devices will

00:00:31,529 --> 00:00:36,690
you have a connected home connected car

00:00:33,510 --> 00:00:38,579
and devices will opt to talk to each

00:00:36,690 --> 00:00:42,239
other rather than talk to you so it's

00:00:38,579 --> 00:00:44,070
kind of reducing a human interface so in

00:00:42,239 --> 00:00:47,520
order to make that possible that world

00:00:44,070 --> 00:00:49,289
of things talking to each other and not

00:00:47,520 --> 00:00:51,960
talking to you and interfacing with your

00:00:49,289 --> 00:00:53,910
computer less we need to have good

00:00:51,960 --> 00:00:56,100
communication lines good communication

00:00:53,910 --> 00:00:57,989
protocols things that communicate well

00:00:56,100 --> 00:01:01,109
when you have a constrained environment

00:00:57,989 --> 00:01:02,910
when you having to a bit little a bit

00:01:01,109 --> 00:01:04,409
processor with two cavemen of RAM or

00:01:02,910 --> 00:01:06,439
something

00:01:04,409 --> 00:01:09,830
so it's a little bit different of the

00:01:06,439 --> 00:01:12,210
environment than might be used to so

00:01:09,830 --> 00:01:13,500
telemetry is kind of provides the

00:01:12,210 --> 00:01:16,670
backbone to the internet of things

00:01:13,500 --> 00:01:20,070
that's kind of our little tagline is

00:01:16,670 --> 00:01:22,409
provides a MQTT broker which we'll get

00:01:20,070 --> 00:01:25,020
into that in a minute and we also some

00:01:22,409 --> 00:01:27,470
like enterprise services like protocol

00:01:25,020 --> 00:01:31,500
onboarding for like custom one-off

00:01:27,470 --> 00:01:33,299
protocols lastly we're a startup so we

00:01:31,500 --> 00:01:34,829
do use the cloud pretty extensively I

00:01:33,299 --> 00:01:36,990
don't think some of the stuff we do

00:01:34,829 --> 00:01:38,700
would be possible without the cloud we

00:01:36,990 --> 00:01:40,380
use Amazon but it's not particularly

00:01:38,700 --> 00:01:44,420
important it just could be Rackspace

00:01:40,380 --> 00:01:46,799
could be anything so a little bit about

00:01:44,420 --> 00:01:49,250
MQTT first of all it's a pub/sub

00:01:46,799 --> 00:01:53,759
protocol much like Kafka

00:01:49,250 --> 00:01:55,500
publish/subscribe so in a pub subsystem

00:01:53,759 --> 00:01:57,869
you have a publisher that's the kind of

00:01:55,500 --> 00:01:59,100
source of messages and you have the

00:01:57,869 --> 00:02:02,250
subscriber that's kind of the sync

00:01:59,100 --> 00:02:04,979
subscribers so it's consuming and the

00:02:02,250 --> 00:02:06,240
broker is in the middle now some people

00:02:04,979 --> 00:02:08,030
like pointing out but yeah there's a

00:02:06,240 --> 00:02:11,390
single point of failure there's a broker

00:02:08,030 --> 00:02:11,390
I'll get into

00:02:11,500 --> 00:02:16,030
but aside from that single point of

00:02:13,840 --> 00:02:18,129
failure it's a pretty nice paradigm

00:02:16,030 --> 00:02:20,349
because your publishers are kind of

00:02:18,129 --> 00:02:21,879
insulated from your subscribers if some

00:02:20,349 --> 00:02:23,409
the publishers die the subscribers don't

00:02:21,879 --> 00:02:24,790
really notice this it's kind of nice

00:02:23,409 --> 00:02:26,560
especially in a constrained environment

00:02:24,790 --> 00:02:28,299
where your publishers frequently go

00:02:26,560 --> 00:02:33,099
offline or change IP addresses

00:02:28,299 --> 00:02:34,900
unexpectedly so another some other

00:02:33,099 --> 00:02:37,780
reasons why we like MQTT is first of all

00:02:34,900 --> 00:02:41,379
it's a standard IBM first created it

00:02:37,780 --> 00:02:43,930
like 10 15 20 years ago a lot of people

00:02:41,379 --> 00:02:46,269
have adopted that sense just WebSphere

00:02:43,930 --> 00:02:48,220
support for it it's also a really

00:02:46,269 --> 00:02:52,720
lightweight protocol so it runs on top

00:02:48,220 --> 00:02:54,609
of TCP and on top of TCP it only has

00:02:52,720 --> 00:02:59,169
like 2 by overhead uses a lot of bit

00:02:54,609 --> 00:03:03,459
shifting so unlike say HTTP that uses

00:02:59,169 --> 00:03:08,349
plain text em PTT we use very compact

00:03:03,459 --> 00:03:10,239
format it's also very easy to parse by

00:03:08,349 --> 00:03:12,340
easy to parse I don't mean for humans

00:03:10,239 --> 00:03:14,579
like HTTP is easy to parse for humans

00:03:12,340 --> 00:03:19,060
because it's just plain text headers

00:03:14,579 --> 00:03:21,699
MQTT is easy to parse that computer one

00:03:19,060 --> 00:03:25,540
of the reasons is something called

00:03:21,699 --> 00:03:27,129
length prefix strings so when you're

00:03:25,540 --> 00:03:29,590
reading through the the network packet

00:03:27,129 --> 00:03:31,239
you come to the length first and you can

00:03:29,590 --> 00:03:32,799
make an array you know exactly how much

00:03:31,239 --> 00:03:35,049
data you need to read in to read in that

00:03:32,799 --> 00:03:36,780
string and that actually helps a lot

00:03:35,049 --> 00:03:40,079
when you're in a constrained environment

00:03:36,780 --> 00:03:43,989
especially those you know you only have

00:03:40,079 --> 00:03:49,209
two to four K of memory available to you

00:03:43,989 --> 00:03:52,329
total also a big another big point is

00:03:49,209 --> 00:03:55,689
that unlike Kafka it has very

00:03:52,329 --> 00:03:58,509
lightweight clients the broker is the

00:03:55,689 --> 00:04:03,489
MQTT brokers complex and the client is

00:03:58,509 --> 00:04:05,079
very very thin Kafka the the clients are

00:04:03,489 --> 00:04:07,509
very thick very smart very intelligent

00:04:05,079 --> 00:04:10,109
and the broker is pretty thin so it's

00:04:07,509 --> 00:04:10,109
quite a bit different

00:04:10,979 --> 00:04:19,719
lastly just on PTT is also very reliable

00:04:15,630 --> 00:04:21,820
so TCP guarantees message delivery in

00:04:19,719 --> 00:04:22,930
that if it doesn't get delivered to let

00:04:21,820 --> 00:04:25,580
you know that if

00:04:22,930 --> 00:04:27,919
mqtt actually does guarantee message

00:04:25,580 --> 00:04:30,290
delivery it'll keep on trying connection

00:04:27,919 --> 00:04:32,660
breaks IP address changes which happens

00:04:30,290 --> 00:04:35,289
frequently if you're on a cell network

00:04:32,660 --> 00:04:38,300
say you have a connected car that's or

00:04:35,289 --> 00:04:40,880
Mack truck or something that's driving

00:04:38,300 --> 00:04:42,380
across country it's going to change IP

00:04:40,880 --> 00:04:50,539
addresses several times it goes through

00:04:42,380 --> 00:04:53,330
different cell areas so QoS the quality

00:04:50,539 --> 00:04:57,139
of service zero with just TCP guarantees

00:04:53,330 --> 00:04:58,940
and there's a QoS one and two that gives

00:04:57,139 --> 00:05:02,930
extra additional guarantees on top of

00:04:58,940 --> 00:05:04,400
that and then finally it's secure I mean

00:05:02,930 --> 00:05:07,550
this isn't a great amount of security

00:05:04,400 --> 00:05:09,830
but combined with TLS TLS gives you the

00:05:07,550 --> 00:05:12,199
encryption and with the username and

00:05:09,830 --> 00:05:13,750
password you have a identities between

00:05:12,199 --> 00:05:16,130
the two of them you can kind of

00:05:13,750 --> 00:05:19,070
guarantee the integrity of the system so

00:05:16,130 --> 00:05:22,449
it's it's a nice little protocol small

00:05:19,070 --> 00:05:25,789
simple the the spec is only a few pages

00:05:22,449 --> 00:05:29,630
pretty neat implementations and just

00:05:25,789 --> 00:05:32,090
just about every language so again with

00:05:29,630 --> 00:05:33,889
pub subsystems just a quick run-through

00:05:32,090 --> 00:05:36,320
I'm sure you've already done this

00:05:33,889 --> 00:05:39,440
million times but give a set of

00:05:36,320 --> 00:05:41,590
publishers and subscribers and kind of

00:05:39,440 --> 00:05:46,310
like Kafka we publish to topics

00:05:41,590 --> 00:05:49,400
however unlike Kafka the topics are what

00:05:46,310 --> 00:05:53,030
we call dynamic topics so we have topic

00:05:49,400 --> 00:05:58,190
patterns so you might have an exam a a

00:05:53,030 --> 00:05:59,960
topic like calmed example / device / you

00:05:58,190 --> 00:06:02,180
might put a little identifier and

00:05:59,960 --> 00:06:04,159
they're like a device ID and then

00:06:02,180 --> 00:06:06,860
something else so there's two different

00:06:04,159 --> 00:06:08,900
wildcards there's a plus and a pound

00:06:06,860 --> 00:06:11,720
they're not particularly important but

00:06:08,900 --> 00:06:14,870
if you want to know the pound matches

00:06:11,720 --> 00:06:16,880
several topic segments and a plus match

00:06:14,870 --> 00:06:20,960
is just one single topic segment this is

00:06:16,880 --> 00:06:23,330
an important point because the the

00:06:20,960 --> 00:06:24,830
topics are very rich you don't need to

00:06:23,330 --> 00:06:27,979
declare it it kind of makes it kind of

00:06:24,830 --> 00:06:31,580
adds to the simplicity of mqtt the

00:06:27,979 --> 00:06:32,710
reason why we like it let's get into

00:06:31,580 --> 00:06:35,390
this

00:06:32,710 --> 00:06:38,360
okay so the problem we were addressing

00:06:35,390 --> 00:06:40,640
at telemetry is we need you to scale up

00:06:38,360 --> 00:06:44,060
to a certain level we're in the cloud

00:06:40,640 --> 00:06:46,970
yeah we need to scale up to say you know

00:06:44,060 --> 00:06:48,530
millions of connected devices I have two

00:06:46,970 --> 00:06:50,150
million written down here but it's

00:06:48,530 --> 00:06:51,950
actually going to be you know four

00:06:50,150 --> 00:06:54,650
million eight million skin.if month

00:06:51,950 --> 00:06:56,120
going higher and higher so we need to be

00:06:54,650 --> 00:06:59,240
able to scale linearly that's the most

00:06:56,120 --> 00:07:00,710
important thing as far as messages per

00:06:59,240 --> 00:07:02,270
second it's actually not that high

00:07:00,710 --> 00:07:03,980
considering how many connected clients

00:07:02,270 --> 00:07:06,110
there are mostly because a lot of

00:07:03,980 --> 00:07:08,900
clients just hop on the network for just

00:07:06,110 --> 00:07:10,910
a little blip publish much messages and

00:07:08,900 --> 00:07:14,000
go back off or they might come back on

00:07:10,910 --> 00:07:18,230
and receive much messages and it goes

00:07:14,000 --> 00:07:21,200
both ways but the single hardest part of

00:07:18,230 --> 00:07:23,560
this is the single subscriber I'll get

00:07:21,200 --> 00:07:30,440
into them a bit why that's so difficult

00:07:23,560 --> 00:07:32,450
it's yeah so again with the scaling

00:07:30,440 --> 00:07:35,660
goals we're in were in the cloud so

00:07:32,450 --> 00:07:38,360
we're we're shooting for some horizontal

00:07:35,660 --> 00:07:39,710
scaling and sure everyone in this room

00:07:38,360 --> 00:07:44,450
probably had a fully understands the

00:07:39,710 --> 00:07:46,010
perks of horizontal scaling so like that

00:07:44,450 --> 00:07:47,930
that broker isn't a single point of

00:07:46,010 --> 00:07:50,720
failure anymore now we can have a little

00:07:47,930 --> 00:07:53,690
more availability and reduce costs or

00:07:50,720 --> 00:08:01,610
actually increase cost in some cases and

00:07:53,690 --> 00:08:04,490
then so problems with scaling MQTT first

00:08:01,610 --> 00:08:06,680
of all load balancing when you when you

00:08:04,490 --> 00:08:09,260
first address horizontal scaling problem

00:08:06,680 --> 00:08:11,390
you gotta spread the load across to all

00:08:09,260 --> 00:08:13,250
your servers you have necks just go into

00:08:11,390 --> 00:08:16,400
one you're not really utilizing the

00:08:13,250 --> 00:08:19,430
extra hardware so we use something

00:08:16,400 --> 00:08:21,440
called a chi proxy it's frequently users

00:08:19,430 --> 00:08:23,630
another just several other you know

00:08:21,440 --> 00:08:26,270
proxies out there that can do that the

00:08:23,630 --> 00:08:28,130
way that load balancing works is it

00:08:26,270 --> 00:08:31,400
forms the connection from the device the

00:08:28,130 --> 00:08:35,590
client to the load balancer and then

00:08:31,400 --> 00:08:39,290
another TCP connection to the server

00:08:35,590 --> 00:08:44,960
with mqtt TCP connections can frequently

00:08:39,290 --> 00:08:45,900
be long lasting usually more than splits

00:08:44,960 --> 00:08:48,990
that can like hu

00:08:45,900 --> 00:08:51,840
- usually longer like days or weeks

00:08:48,990 --> 00:08:53,640
sometimes however long you can maintain

00:08:51,840 --> 00:08:55,760
a TCP connection so usually how long it

00:08:53,640 --> 00:08:55,760
will last

00:08:56,420 --> 00:09:07,440
so the the H I proxy in itself is not a

00:09:03,630 --> 00:09:10,050
great date it's gonna require a little

00:09:07,440 --> 00:09:12,540
more than that so we also use another

00:09:10,050 --> 00:09:15,230
concept called DNS load balancing which

00:09:12,540 --> 00:09:17,760
is not a really great load balancing

00:09:15,230 --> 00:09:20,820
strategy on its own

00:09:17,760 --> 00:09:23,190
what if you think about DNS it's kind of

00:09:20,820 --> 00:09:25,530
a hash map you have hosts you have IP

00:09:23,190 --> 00:09:29,190
addresses so you send a request the DNS

00:09:25,530 --> 00:09:31,140
server resolves to resolve a hostname

00:09:29,190 --> 00:09:32,970
and gives you back an IP address with

00:09:31,140 --> 00:09:35,190
DNS load balancing you actually load the

00:09:32,970 --> 00:09:37,620
DNS server with a list of IP addresses

00:09:35,190 --> 00:09:39,420
all the IP addresses of your server and

00:09:37,620 --> 00:09:41,700
so when you resolve an IP address it

00:09:39,420 --> 00:09:44,040
just goes and hands you the next item in

00:09:41,700 --> 00:09:46,040
the list and just kind of rotates the

00:09:44,040 --> 00:09:49,020
whole list so it's kind of round robin

00:09:46,040 --> 00:09:50,700
way of load balancing it's not great

00:09:49,020 --> 00:09:55,920
because you you can only really do

00:09:50,700 --> 00:09:58,080
round-robin you can't really can't use

00:09:55,920 --> 00:10:00,030
some statistics like how much load

00:09:58,080 --> 00:10:03,330
you're under so it's not it's not a

00:10:00,030 --> 00:10:05,340
terribly great way so as far as load

00:10:03,330 --> 00:10:09,510
balancing goes it wasn't really all that

00:10:05,340 --> 00:10:11,640
effective we we did it all we could with

00:10:09,510 --> 00:10:13,740
a tree proxy and DNS load balancing and

00:10:11,640 --> 00:10:16,680
you can kind of sort of balance the load

00:10:13,740 --> 00:10:20,270
but it in the end it's just kind of luck

00:10:16,680 --> 00:10:22,950
of the draw but what you end up getting

00:10:20,270 --> 00:10:24,990
as far as DNS load balancing another

00:10:22,950 --> 00:10:27,000
point to mention is it's a lot more

00:10:24,990 --> 00:10:29,850
effective if you force the clients to

00:10:27,000 --> 00:10:31,860
not cache the DNS so that goes to server

00:10:29,850 --> 00:10:37,200
every time and the balancing a little

00:10:31,860 --> 00:10:39,300
better last point on there with qs1 and

00:10:37,200 --> 00:10:42,360
to since the reliability is better than

00:10:39,300 --> 00:10:45,060
TCP we have to use some sort of file

00:10:42,360 --> 00:10:47,960
store or persistent store of some sort

00:10:45,060 --> 00:10:50,520
we use Cassandra in this case because

00:10:47,960 --> 00:10:53,130
Cassandra has a consistent hash ring

00:10:50,520 --> 00:10:56,970
that's it's a horizontal scaling a

00:10:53,130 --> 00:10:59,979
linear linear rate so

00:10:56,970 --> 00:11:07,209
it can help us scale as far as we need

00:10:59,979 --> 00:11:09,040
to scale at yeah also back to the the

00:11:07,209 --> 00:11:10,299
single subscriber this is this is kinda

00:11:09,040 --> 00:11:12,249
how the SUBSCRIBE single subscriber

00:11:10,299 --> 00:11:14,739
looks like you have a lot of publishers

00:11:12,249 --> 00:11:17,649
they go through the broker and then you

00:11:14,739 --> 00:11:20,739
have a wild card consumer that called a

00:11:17,649 --> 00:11:22,869
firehose consumer seems all the messages

00:11:20,739 --> 00:11:24,819
that are being published when you break

00:11:22,869 --> 00:11:26,679
that down a little bit it's a little bit

00:11:24,819 --> 00:11:28,239
more tricky because now we have this

00:11:26,679 --> 00:11:31,449
load balancing solution that's kind of

00:11:28,239 --> 00:11:33,459
hacked into there so you have a bunch of

00:11:31,449 --> 00:11:35,049
publishers publishing to ole when they

00:11:33,459 --> 00:11:36,489
connect they they have to go through

00:11:35,049 --> 00:11:38,049
some sort of load balancing solutions

00:11:36,489 --> 00:11:39,759
there's no way to predict exactly which

00:11:38,049 --> 00:11:42,309
broker they're connecting to connect to

00:11:39,759 --> 00:11:44,949
you so they kind of connect to an

00:11:42,309 --> 00:11:47,789
arbitrary broker and then the message

00:11:44,949 --> 00:11:50,289
just gets published out to a subscriber

00:11:47,789 --> 00:11:51,999
you break down break that down a little

00:11:50,289 --> 00:11:53,169
bit further it's not even that simple

00:11:51,999 --> 00:11:55,689
because you think about it the

00:11:53,169 --> 00:11:58,509
subscriber how else has to go through

00:11:55,689 --> 00:12:01,869
the same load balancing mechanism and so

00:11:58,509 --> 00:12:03,699
we can't really guarantee what broker

00:12:01,869 --> 00:12:06,069
the the subscriber is going to connect

00:12:03,699 --> 00:12:09,279
to so what you end up with is you end up

00:12:06,069 --> 00:12:11,919
a hot node so when messages are

00:12:09,279 --> 00:12:13,779
published on any particular broker they

00:12:11,919 --> 00:12:16,749
all end up going through that broker

00:12:13,779 --> 00:12:19,029
that the subscriber is attached to so

00:12:16,749 --> 00:12:20,859
this kind of killed our goals of

00:12:19,029 --> 00:12:23,169
horizontal scaling so this is like the

00:12:20,859 --> 00:12:25,619
crux the problem and kind of need to get

00:12:23,169 --> 00:12:25,619
past this

00:12:26,279 --> 00:12:32,079
so our first first thought is let's just

00:12:29,980 --> 00:12:35,439
use HTTP I mean it's pretty simple

00:12:32,079 --> 00:12:36,789
everyone's quite familiar with HTTP so

00:12:35,439 --> 00:12:39,879
this is kind of how it looks when it's

00:12:36,789 --> 00:12:41,980
setup same sort of okay there's lots of

00:12:39,879 --> 00:12:43,749
publishers get load balance into the

00:12:41,980 --> 00:12:45,970
brokers and then when we get when the

00:12:43,749 --> 00:12:50,739
message comes into the broker it gets

00:12:45,970 --> 00:12:53,919
published directly out to a URL another

00:12:50,739 --> 00:12:55,689
HTTP server which effectively is a load

00:12:53,919 --> 00:13:00,339
balancer but it's more of a normal sense

00:12:55,689 --> 00:13:02,979
a load balancer so this this actually

00:13:00,339 --> 00:13:04,899
worked pretty well whose benefits is

00:13:02,979 --> 00:13:06,360
really simple very well studied like

00:13:04,899 --> 00:13:08,690
everyone does it

00:13:06,360 --> 00:13:14,070
anybody coming out of college usually

00:13:08,690 --> 00:13:15,690
understands generally how this works and

00:13:14,070 --> 00:13:17,610
it was it was relatively efficient and

00:13:15,690 --> 00:13:20,220
it did scale horizontally so it did it

00:13:17,610 --> 00:13:22,500
did achieve those goals so the the

00:13:20,220 --> 00:13:24,959
problems we're having with it is HTTP

00:13:22,500 --> 00:13:27,269
tends to be heavy one of the big reasons

00:13:24,959 --> 00:13:30,240
big things I mean by that is when you

00:13:27,269 --> 00:13:32,040
make an HTTP call you have to establish

00:13:30,240 --> 00:13:33,870
the connection and you have to send the

00:13:32,040 --> 00:13:36,360
request out and acknowledge the request

00:13:33,870 --> 00:13:38,100
and the response acknowledged response

00:13:36,360 --> 00:13:40,350
and finalize a connection all in all

00:13:38,100 --> 00:13:42,899
you're looking at like nine TCP packets

00:13:40,350 --> 00:13:45,240
which we can we can do a lot less that

00:13:42,899 --> 00:13:47,910
than that with them qgt so it was kind

00:13:45,240 --> 00:13:50,399
of like kind of stinks that's not that's

00:13:47,910 --> 00:13:51,890
not that big of an issue realistically

00:13:50,399 --> 00:13:54,720
we can just throw more money at it and

00:13:51,890 --> 00:13:57,690
get past it however some of the bigger

00:13:54,720 --> 00:13:59,550
issues for us was the subscribers have

00:13:57,690 --> 00:14:02,220
to always be available and it's the

00:13:59,550 --> 00:14:04,740
retry logic and all the corner cases for

00:14:02,220 --> 00:14:08,820
the retrial aaj ik so if the server if

00:14:04,740 --> 00:14:10,470
the consumer server is down what do we

00:14:08,820 --> 00:14:12,420
do do we what we store it for a little

00:14:10,470 --> 00:14:16,050
while when do we retry what do we know

00:14:12,420 --> 00:14:18,029
to retry they can't just pull so we have

00:14:16,050 --> 00:14:24,300
to push it all the time so it's a little

00:14:18,029 --> 00:14:27,240
tricky how to how to do that so then we

00:14:24,300 --> 00:14:29,010
I was reading the linkedin's paper on

00:14:27,240 --> 00:14:31,140
Kafka at the time and I was like wow

00:14:29,010 --> 00:14:33,540
this would be like really really easy if

00:14:31,140 --> 00:14:35,070
we just threw Kafka into the mix that's

00:14:33,540 --> 00:14:37,199
I know it's another you know piece and

00:14:35,070 --> 00:14:39,420
makes it a little more technology stack

00:14:37,199 --> 00:14:42,839
but so let's let's try it out

00:14:39,420 --> 00:14:44,180
so again LinkedIn last presentation is

00:14:42,839 --> 00:14:49,320
really good if you're around for that

00:14:44,180 --> 00:14:50,399
LinkedIn made Kafka for something to

00:14:49,320 --> 00:14:53,279
call distributed log aggregation

00:14:50,399 --> 00:14:55,110
framework now first is confused me like

00:14:53,279 --> 00:14:57,329
a log aggregation well it's not what I'm

00:14:55,110 --> 00:14:58,560
looking for but then dug down into it's

00:14:57,329 --> 00:15:00,120
just a pub sub system you have

00:14:58,560 --> 00:15:03,269
publishers and subscribers and you have

00:15:00,120 --> 00:15:04,890
brokers then I'll dug into a little more

00:15:03,269 --> 00:15:06,089
because I was like well this is curious

00:15:04,890 --> 00:15:08,490
because I mean it scales a lot better

00:15:06,089 --> 00:15:10,769
than MQTT does how come how come it

00:15:08,490 --> 00:15:12,570
scales so well well I mean it's really

00:15:10,769 --> 00:15:14,010
meant for server to server environments

00:15:12,570 --> 00:15:16,020
it's not meant for constrained

00:15:14,010 --> 00:15:18,240
environments one of the big reasons is

00:15:16,020 --> 00:15:19,360
it's got really really smart clients in

00:15:18,240 --> 00:15:21,130
fact it

00:15:19,360 --> 00:15:23,079
kind of inverts that the load so the

00:15:21,130 --> 00:15:26,500
clients bear all the load and the

00:15:23,079 --> 00:15:30,040
brokers are relatively thin on one of

00:15:26,500 --> 00:15:31,540
the reasons wise it uses zookeeper and I

00:15:30,040 --> 00:15:35,560
don't think you could ever run zookeeper

00:15:31,540 --> 00:15:39,430
on a little 8-bit sensor actuator or

00:15:35,560 --> 00:15:46,450
something so so it's a really cool

00:15:39,430 --> 00:15:49,959
little framework or tool again Kafka

00:15:46,450 --> 00:15:52,450
does not have dynamic topics it has

00:15:49,959 --> 00:15:53,160
something called as a smurf of static

00:15:52,450 --> 00:15:55,959
topic

00:15:53,160 --> 00:15:58,930
static topics actually are represented

00:15:55,959 --> 00:16:01,959
as actual files on the file system so

00:15:58,930 --> 00:16:03,430
when a message comes in it gets just

00:16:01,959 --> 00:16:05,649
simply appended to the end of that file

00:16:03,430 --> 00:16:08,950
and you keep on up adding messages as

00:16:05,649 --> 00:16:12,670
you go along and then when the consumer

00:16:08,950 --> 00:16:14,980
requests a message when subscribes it

00:16:12,670 --> 00:16:17,769
just gives it a message offset and so

00:16:14,980 --> 00:16:19,709
the message the message ID is the same

00:16:17,769 --> 00:16:22,180
thing as a message offset and so it just

00:16:19,709 --> 00:16:24,760
scans to that place in the file and it

00:16:22,180 --> 00:16:27,339
starts reading messages this is really

00:16:24,760 --> 00:16:28,990
really efficient way to write a pub sub

00:16:27,339 --> 00:16:31,839
system and I really thought was

00:16:28,990 --> 00:16:33,550
brilliant again it probably wouldn't

00:16:31,839 --> 00:16:38,199
work for Internet of Things applications

00:16:33,550 --> 00:16:40,810
but if you're trying to replace a MTP or

00:16:38,199 --> 00:16:42,850
one of the other like XMPP one of the

00:16:40,810 --> 00:16:47,949
other cups of systems it might actually

00:16:42,850 --> 00:16:50,079
work really love that potentially so

00:16:47,949 --> 00:16:52,630
another challenge we had like now we

00:16:50,079 --> 00:16:55,110
have MQTT with dynamic topics and we

00:16:52,630 --> 00:16:58,480
have Kafka with static topics

00:16:55,110 --> 00:17:00,610
luckily Kafka also has something that's

00:16:58,480 --> 00:17:04,089
called key so you have a topic and you

00:17:00,610 --> 00:17:08,439
have a key the topic is static but the

00:17:04,089 --> 00:17:11,290
key can be dynamic so we took the static

00:17:08,439 --> 00:17:13,900
information out of the MQTT topic made

00:17:11,290 --> 00:17:18,130
that into the Kafka topic and then we

00:17:13,900 --> 00:17:20,700
took the dynamic parts of the mqtt topic

00:17:18,130 --> 00:17:23,550
like the device ID and what

00:17:20,700 --> 00:17:30,180
and whatever else and threw that into

00:17:23,550 --> 00:17:31,650
the key so the key the keys used also

00:17:30,180 --> 00:17:35,310
kind of like a sander it's kind of a

00:17:31,650 --> 00:17:36,750
consistent hash ring and so it's used

00:17:35,310 --> 00:17:40,530
for balancing in the brokers but it's

00:17:36,750 --> 00:17:44,040
also used for called a subscriber group

00:17:40,530 --> 00:17:47,070
so with MQTT you can subscribe and pull

00:17:44,040 --> 00:17:48,720
messages with kafka you can actually

00:17:47,070 --> 00:17:51,510
take one subscription you can take that

00:17:48,720 --> 00:17:54,360
firehose subscription and split it out

00:17:51,510 --> 00:17:58,140
so say like you have four nodes and your

00:17:54,360 --> 00:17:59,850
kafka subscriber group you can take that

00:17:58,140 --> 00:18:02,250
firehose description and kind of kind of

00:17:59,850 --> 00:18:06,570
throw a quarter the messages at each of

00:18:02,250 --> 00:18:10,410
the members of the Scrabble group it's a

00:18:06,570 --> 00:18:13,380
cool little feature and it helps it

00:18:10,410 --> 00:18:19,200
scale really well so this is kind of how

00:18:13,380 --> 00:18:20,610
we changed our our architecture to deal

00:18:19,200 --> 00:18:22,080
with that so again you have publishers

00:18:20,610 --> 00:18:24,810
publishing through load-balanced here

00:18:22,080 --> 00:18:27,930
into the broker with a message when the

00:18:24,810 --> 00:18:30,600
broker receives a message it publishes

00:18:27,930 --> 00:18:33,630
it straight to Kafka and Kafka is able

00:18:30,600 --> 00:18:36,240
to balance well because the consistent

00:18:33,630 --> 00:18:38,730
hash ring and it doesn't have the same

00:18:36,240 --> 00:18:41,330
problem those are mqtt brokers did so

00:18:38,730 --> 00:18:44,850
it's a pretty pretty elegant solution

00:18:41,330 --> 00:18:47,070
and it's pretty easy to grasp your head

00:18:44,850 --> 00:18:49,800
grasp the only thing that's missing from

00:18:47,070 --> 00:18:51,630
this picture is the subscribers so Kafka

00:18:49,800 --> 00:18:57,750
and that box is just the brokers and

00:18:51,630 --> 00:18:59,790
subscribers can pull from there so as

00:18:57,750 --> 00:19:01,860
far as results we reach evil results

00:18:59,790 --> 00:19:05,790
we're looking for and better yet it was

00:19:01,860 --> 00:19:11,510
a horizontal linear scaling so kind of

00:19:05,790 --> 00:19:14,190
that near near infinite scaling so and

00:19:11,510 --> 00:19:17,580
the bigger thing is we didn't have any

00:19:14,190 --> 00:19:21,810
of those tricky corner cases with like

00:19:17,580 --> 00:19:24,030
the HTTP retry when you publish to Kafka

00:19:21,810 --> 00:19:26,370
you just simply publish and it just keep

00:19:24,030 --> 00:19:29,130
his messages forever and subscriber can

00:19:26,370 --> 00:19:31,230
just pull when it's ready and so if the

00:19:29,130 --> 00:19:33,000
publisher and subscriber aren't alive at

00:19:31,230 --> 00:19:34,540
the same time it's not a big deal they

00:19:33,000 --> 00:19:36,520
can go offline for we

00:19:34,540 --> 00:19:37,900
can come back online and be processing

00:19:36,520 --> 00:19:42,580
for a few days while they catch up or

00:19:37,900 --> 00:19:45,490
whatever compare so some of the things

00:19:42,580 --> 00:19:50,230
that work on my wish list and yeah I'm

00:19:45,490 --> 00:19:53,410
saying it security is a big thing right

00:19:50,230 --> 00:19:58,780
now we just set up a VPN from Clapp

00:19:53,410 --> 00:20:02,020
between two clusters and it works the

00:19:58,780 --> 00:20:03,880
the problems our clients aren't part of

00:20:02,020 --> 00:20:05,230
our company so we have to set it up

00:20:03,880 --> 00:20:07,840
individually and run through some

00:20:05,230 --> 00:20:10,090
processes so that's kind of a pain I

00:20:07,840 --> 00:20:12,910
wish there was a better security story

00:20:10,090 --> 00:20:15,970
and also kafka configuration is a lot

00:20:12,910 --> 00:20:19,450
more of a headache than TCP other than

00:20:15,970 --> 00:20:23,500
MQTT configuration Lemkin ET it's mostly

00:20:19,450 --> 00:20:24,970
just get up set it up go hands-off even

00:20:23,500 --> 00:20:27,990
even our own brokers we create ourselves

00:20:24,970 --> 00:20:30,670
there's not much configuration when you

00:20:27,990 --> 00:20:32,470
subscribe to a client it's it's really

00:20:30,670 --> 00:20:34,870
easy to just kind of give it a URL and

00:20:32,470 --> 00:20:36,790
subscribe to our top topic and start

00:20:34,870 --> 00:20:39,880
getting messages with kafka you got to

00:20:36,790 --> 00:20:41,560
set up the zookeeper configuration you

00:20:39,880 --> 00:20:43,300
gotta spend time as tune the garbage

00:20:41,560 --> 00:20:46,090
collector if you really look into high

00:20:43,300 --> 00:20:51,430
loads and so there's a lot more

00:20:46,090 --> 00:20:55,200
configuration there so us but although I

00:20:51,430 --> 00:20:58,090
think it was a very successful little

00:20:55,200 --> 00:21:00,580
experiment and it went well and I think

00:20:58,090 --> 00:21:04,000
I'd recommend Kafka for a lot of people

00:21:00,580 --> 00:21:05,950
trying to scale as far as I mentioned

00:21:04,000 --> 00:21:10,990
Internet of Things I just want to throw

00:21:05,950 --> 00:21:13,750
in a little kind of selling point I in

00:21:10,990 --> 00:21:16,450
my opinion some the most exciting things

00:21:13,750 --> 00:21:20,950
open source are probably in the Internet

00:21:16,450 --> 00:21:22,900
of Things arena right now some so

00:21:20,950 --> 00:21:24,820
Internet of Things is kind of split up

00:21:22,900 --> 00:21:26,470
into two areas you have like the home

00:21:24,820 --> 00:21:28,780
automation side where you're connecting

00:21:26,470 --> 00:21:30,970
your TV to your refrigerator to switch

00:21:28,780 --> 00:21:33,510
it to your car and someone's kind of

00:21:30,970 --> 00:21:36,850
stupid but some of them make sense yeah

00:21:33,510 --> 00:21:40,330
and in that area there's a project

00:21:36,850 --> 00:21:43,260
called openhab that's basically just

00:21:40,330 --> 00:21:43,260
coming up with a lot of

00:21:43,340 --> 00:21:49,940
tools and standards to facilitate that

00:21:46,490 --> 00:21:52,220
happening also if you haven't heard of

00:21:49,940 --> 00:21:52,519
all join I'd really suggest looking it

00:21:52,220 --> 00:21:56,149
up

00:21:52,519 --> 00:21:58,999
I mean I talk a lot about like protocols

00:21:56,149 --> 00:22:00,980
stuff but all join kind of makes all

00:21:58,999 --> 00:22:03,649
this device-to-device communication is

00:22:00,980 --> 00:22:07,509
easy as object-oriented programming it's

00:22:03,649 --> 00:22:10,429
pretty neat they just a device exposes a

00:22:07,509 --> 00:22:12,740
standard interface and any device can

00:22:10,429 --> 00:22:17,389
consume it as if it already knows how it

00:22:12,740 --> 00:22:21,529
operates so really cool concept suggest

00:22:17,389 --> 00:22:24,889
getting into also on the the connected

00:22:21,529 --> 00:22:27,110
car open XC if you know it at obd2

00:22:24,889 --> 00:22:28,669
connector is slow connector it's

00:22:27,110 --> 00:22:30,490
underneath the dashboard car and it

00:22:28,669 --> 00:22:32,749
pumps out lots of diagnostic information

00:22:30,490 --> 00:22:34,789
could be boring to a lot of people but

00:22:32,749 --> 00:22:38,360
some people get excited about lots of

00:22:34,789 --> 00:22:42,289
data in a big data so open XC is a

00:22:38,360 --> 00:22:43,940
little project that just helps you

00:22:42,289 --> 00:22:45,759
figure out how to get the data out of

00:22:43,940 --> 00:22:51,710
your car and make sense that graph it

00:22:45,759 --> 00:22:54,169
and again I hub is a a project written

00:22:51,710 --> 00:22:56,210
around around security of Internet of

00:22:54,169 --> 00:22:59,990
Things and I just want to give a shout

00:22:56,210 --> 00:23:01,999
out to IOT clips org it's not really a

00:22:59,990 --> 00:23:03,950
product in the normal sense it's just a

00:23:01,999 --> 00:23:06,440
kind of aggregation of a lot of places

00:23:03,950 --> 00:23:12,230
to start a lot of projects a lot of

00:23:06,440 --> 00:23:15,440
standards and whatnot so with that I

00:23:12,230 --> 00:23:17,419
actually for some reason writing a book

00:23:15,440 --> 00:23:20,570
about the Internet of Things I hope it

00:23:17,419 --> 00:23:22,580
turns out well but I just want to tell

00:23:20,570 --> 00:23:25,990
that for a little bit but after that

00:23:22,580 --> 00:23:25,990
anybody have any questions

00:23:28,269 --> 00:23:33,959
I explained things perfectly

00:23:39,000 --> 00:23:43,320
just apart from Kafka what else would

00:23:40,860 --> 00:23:44,880
you evaluate outside of the HTTP if you

00:23:43,320 --> 00:23:49,110
looked at kind of other things in the in

00:23:44,880 --> 00:23:51,059
the Kafka space apart from the HTTP kind

00:23:49,110 --> 00:23:53,400
of in kind of interim solution that you

00:23:51,059 --> 00:23:55,440
mentioned so is there any kind of other

00:23:53,400 --> 00:24:01,049
cluster based solution that you looked

00:23:55,440 --> 00:24:02,940
at ah no I kind of saw Kafka paper out I

00:24:01,049 --> 00:24:09,330
read it I kind of spoiled the other

00:24:02,940 --> 00:24:10,890
options um you mentioned one of the

00:24:09,330 --> 00:24:12,179
things was being able to be offline and

00:24:10,890 --> 00:24:14,549
then the consumer would come back and

00:24:12,179 --> 00:24:16,350
process and you phrased it as it will be

00:24:14,549 --> 00:24:17,760
there forever but like what sort of

00:24:16,350 --> 00:24:20,370
retention policies are you actually

00:24:17,760 --> 00:24:22,289
doing because Kafka will drop data right

00:24:20,370 --> 00:24:23,669
yes Kafka will drop it and it's not

00:24:22,289 --> 00:24:25,440
smart to hold on to forever because you

00:24:23,669 --> 00:24:27,650
eventually will fill up yeah unless you

00:24:25,440 --> 00:24:30,330
have infinite storage like Google does

00:24:27,650 --> 00:24:32,159
um what do you guys use like what are

00:24:30,330 --> 00:24:34,020
your kind of settings to make sure that

00:24:32,159 --> 00:24:36,000
some of the data will be there the next

00:24:34,020 --> 00:24:39,740
time though we connectivity app we hold

00:24:36,000 --> 00:24:44,419
data for month but it's configurable so

00:24:39,740 --> 00:24:44,419
we have a lot of a lot of big data here

00:24:48,190 --> 00:24:52,930
so are you still using HTTP from each

00:24:50,380 --> 00:24:57,010
one of the devices to send the messages

00:24:52,930 --> 00:24:59,590
in no we use mqtt just saving kids yeah

00:24:57,010 --> 00:25:00,880
MQTT is a really light protocol it's

00:24:59,590 --> 00:25:03,250
really well fitted for that sort of

00:25:00,880 --> 00:25:06,940
thing there's a few other IOT protocols

00:25:03,250 --> 00:25:08,950
and if you went back to eclipse org IOT

00:25:06,940 --> 00:25:13,630
valance org there's a few other

00:25:08,950 --> 00:25:17,670
protocols LinkedIn link to that page or

00:25:13,630 --> 00:25:20,620
Co app as well as PTT Co app is a

00:25:17,670 --> 00:25:22,420
slightly different protocol it's it's

00:25:20,620 --> 00:25:25,060
actually more like a lightweight HTTP

00:25:22,420 --> 00:25:29,380
it's like compact binary representation

00:25:25,060 --> 00:25:30,490
runs over UDP so there's a few other

00:25:29,380 --> 00:25:34,540
options when you're talking about

00:25:30,490 --> 00:25:39,600
sending telemetry data or data bending

00:25:34,540 --> 00:25:39,600
sort from constrained environments

00:25:42,890 --> 00:25:47,360
just in terms of the subscribers and the

00:25:45,050 --> 00:25:48,620
consumers what are your use cases at

00:25:47,360 --> 00:25:49,550
that side of things

00:25:48,620 --> 00:25:52,190
do you use any sort of statistics

00:25:49,550 --> 00:25:54,250
warehouse or you know I'm a dupe you you

00:25:52,190 --> 00:25:58,490
know what are you crunching at that end

00:25:54,250 --> 00:26:00,020
that you can talk about I mean you

00:25:58,490 --> 00:26:01,310
mentioned the start the reason I ask is

00:26:00,020 --> 00:26:02,390
you mentioned the start that the

00:26:01,310 --> 00:26:05,330
Internet of Things is largely about

00:26:02,390 --> 00:26:07,220
things talking to each other but it

00:26:05,330 --> 00:26:08,210
seems to me that you you are obviously

00:26:07,220 --> 00:26:10,700
using that data

00:26:08,210 --> 00:26:11,870
you know internally or that there's some

00:26:10,700 --> 00:26:13,370
value to be gained out of it without

00:26:11,870 --> 00:26:15,740
just these devices sending telemetry

00:26:13,370 --> 00:26:18,680
data to one another just clarity now

00:26:15,740 --> 00:26:19,910
absolutely yes there this is what those

00:26:18,680 --> 00:26:20,810
are why I'm so fascinated by the

00:26:19,910 --> 00:26:22,880
Internet of Things there's so many

00:26:20,810 --> 00:26:25,640
different facets to it so you have this

00:26:22,880 --> 00:26:27,590
yeah inner device communication which a

00:26:25,640 --> 00:26:28,880
little device like a Raspberry Pi and

00:26:27,590 --> 00:26:30,830
Arduino isn't going to be holding a lot

00:26:28,880 --> 00:26:32,080
of data but there are other things where

00:26:30,830 --> 00:26:34,130
you do want a whole lot of data

00:26:32,080 --> 00:26:36,530
especially when you're in the talking

00:26:34,130 --> 00:26:42,710
about MQT and the telemetry paloma tree

00:26:36,530 --> 00:26:44,750
data so yes so we store so we have a we

00:26:42,710 --> 00:26:47,090
have some mechanisms to automatically

00:26:44,750 --> 00:26:50,720
store data from it's published our

00:26:47,090 --> 00:26:53,120
brokers you mentioned Hadoop yeah so we

00:26:50,720 --> 00:26:54,350
have might store things to HDFS actually

00:26:53,120 --> 00:26:56,720
we don't store it at HDFS we store it

00:26:54,350 --> 00:26:58,910
Cassandra and then we run a spark

00:26:56,720 --> 00:27:01,790
cluster on top of Cassandra because

00:26:58,910 --> 00:27:04,790
there's plugins to do that that works

00:27:01,790 --> 00:27:07,730
pretty well spark I we write in Scout

00:27:04,790 --> 00:27:10,400
Scala anyway so spark just makes a lot

00:27:07,730 --> 00:27:14,080
of sense to us it's a little bit easier

00:27:10,400 --> 00:27:14,080
than writing rahu deep jobs anyway

00:27:22,110 --> 00:27:25,130
any more questions

00:27:28,299 --> 00:27:35,249
okay well thanks for coming you can go

00:27:32,649 --> 00:27:35,249

YouTube URL: https://www.youtube.com/watch?v=VoTclkxSago


