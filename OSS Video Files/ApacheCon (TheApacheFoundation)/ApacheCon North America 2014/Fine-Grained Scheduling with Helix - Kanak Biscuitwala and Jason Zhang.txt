Title: Fine-Grained Scheduling with Helix - Kanak Biscuitwala and Jason Zhang
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,000 --> 00:00:05,940
so hi everyone so my name is Jason and

00:00:03,589 --> 00:00:08,420
today connect and I are going to talk

00:00:05,940 --> 00:00:10,650
about fine going scheduling with helix

00:00:08,420 --> 00:00:13,500
both connect and I are working at

00:00:10,650 --> 00:00:17,760
linking data infrastructure team and we

00:00:13,500 --> 00:00:19,740
are both an Apache helix commuters so

00:00:17,760 --> 00:00:22,890
for those who don't know helix yet here

00:00:19,740 --> 00:00:25,710
is a few words about helix so helix is a

00:00:22,890 --> 00:00:27,990
generic class manager so it automates

00:00:25,710 --> 00:00:30,599
this assignment of partitioned

00:00:27,990 --> 00:00:33,719
replicated distributed tasks in the face

00:00:30,599 --> 00:00:36,660
of node failure know the recovery class

00:00:33,719 --> 00:00:40,260
expansion and rec infusion of the

00:00:36,660 --> 00:00:41,940
clusters so I want to show a story so

00:00:40,260 --> 00:00:46,739
yesterday connect and I went to this

00:00:41,940 --> 00:00:50,370
solar cloud talk and basically the solar

00:00:46,739 --> 00:00:54,719
talk is distributed solar service that

00:00:50,370 --> 00:00:57,480
do with is failure and and elasticity of

00:00:54,719 --> 00:00:59,730
the clusters so what we discovered is

00:00:57,480 --> 00:01:03,780
that they are almost using the identical

00:00:59,730 --> 00:01:06,150
design and aesthetics and even we can

00:01:03,780 --> 00:01:08,610
find one to one mapping from the

00:01:06,150 --> 00:01:12,960
terminologies used in solar cloud to

00:01:08,610 --> 00:01:15,150
like helix but except that this this

00:01:12,960 --> 00:01:19,350
these terminologies are tightly coupled

00:01:15,150 --> 00:01:21,150
to their applications so instead helix

00:01:19,350 --> 00:01:24,799
is trying to abstract all these

00:01:21,150 --> 00:01:27,299
terminologies for distributed resources

00:01:24,799 --> 00:01:30,090
to handling failure to handling this

00:01:27,299 --> 00:01:32,869
class elasticity so basically for

00:01:30,090 --> 00:01:35,189
example we allow user to plugging

00:01:32,869 --> 00:01:37,950
arbitrary state machines so for example

00:01:35,189 --> 00:01:40,890
you can have a leader standby state

00:01:37,950 --> 00:01:42,930
model but for resort for database you

00:01:40,890 --> 00:01:45,030
can have like a master slave state model

00:01:42,930 --> 00:01:47,670
and for many other applications you can

00:01:45,030 --> 00:01:50,549
come up with any kind of state machines

00:01:47,670 --> 00:01:53,130
you have and also we can let user to

00:01:50,549 --> 00:01:57,360
specify their placement strategies so

00:01:53,130 --> 00:02:02,450
where this resources are placed and so

00:01:57,360 --> 00:02:05,729
on so here is a simplified valve

00:02:02,450 --> 00:02:09,629
architecture of the of the data

00:02:05,729 --> 00:02:12,000
infrastructure used at linking so helix

00:02:09,629 --> 00:02:13,590
is pretty successful at linking so here

00:02:12,000 --> 00:02:15,810
in the middle of this

00:02:13,590 --> 00:02:18,180
diagram we see this TV this is the data

00:02:15,810 --> 00:02:21,030
store this is sort of choose and it

00:02:18,180 --> 00:02:24,150
comes user writes and reads and beneath

00:02:21,030 --> 00:02:26,160
the DB we see this data change capture

00:02:24,150 --> 00:02:28,800
system so it's captured a change of the

00:02:26,160 --> 00:02:31,440
database and fits into the downstream

00:02:28,800 --> 00:02:33,209
applications so we have this change

00:02:31,440 --> 00:02:36,360
consumers that consumers changes from

00:02:33,209 --> 00:02:38,540
the database and downstream applications

00:02:36,360 --> 00:02:41,610
may be a search index so they build

00:02:38,540 --> 00:02:45,120
these search index with this theta

00:02:41,610 --> 00:02:47,220
changes and also we have data replicator

00:02:45,120 --> 00:02:50,489
that's replicating this changes to

00:02:47,220 --> 00:02:52,410
another data center and on the left of

00:02:50,489 --> 00:02:55,099
this diagram we have this et al job not

00:02:52,410 --> 00:02:59,640
take snapshots from the database and

00:02:55,099 --> 00:03:02,220
transform to HDFS for analytics so we

00:02:59,640 --> 00:03:04,290
can see that in the for the DB the helix

00:03:02,220 --> 00:03:07,410
use is using helix to manage the

00:03:04,290 --> 00:03:09,920
database clusters for this theta chi ang

00:03:07,410 --> 00:03:14,670
capture we also Skrillex to manage and

00:03:09,920 --> 00:03:17,160
the data change clusters to to capture

00:03:14,670 --> 00:03:19,170
the changes and on the data change

00:03:17,160 --> 00:03:21,840
consumer part we also use helix to do

00:03:19,170 --> 00:03:24,780
the to do the load load balance among

00:03:21,840 --> 00:03:26,900
the the consumers and for search index

00:03:24,780 --> 00:03:31,079
is also make sense to use helix to

00:03:26,900 --> 00:03:34,319
manage the index chart and to do with

00:03:31,079 --> 00:03:37,500
feel feel feel failure and class

00:03:34,319 --> 00:03:40,440
expansion and these are replicated also

00:03:37,500 --> 00:03:44,489
use helix fulfill failover and ETS ETL

00:03:40,440 --> 00:03:46,680
also use helix so pretty much everything

00:03:44,489 --> 00:03:48,359
like distributed systems in these things

00:03:46,680 --> 00:03:53,519
of structure use helix to do this class

00:03:48,359 --> 00:03:56,790
management see so here is a few numbers

00:03:53,519 --> 00:04:00,120
in production about helix so over band

00:03:56,790 --> 00:04:02,849
sound instance is covering like 330

00:04:00,120 --> 00:04:06,030
sound databases partitions managed by

00:04:02,849 --> 00:04:11,040
helix and over 1000 instance for change

00:04:06,030 --> 00:04:13,980
capture consumers and for single helix

00:04:11,040 --> 00:04:16,590
cluster we can handle as many as 500

00:04:13,980 --> 00:04:20,970
instances and all these numbers are

00:04:16,590 --> 00:04:23,370
Perdita center so helix is extensively

00:04:20,970 --> 00:04:24,889
used inside linking and we also got a

00:04:23,370 --> 00:04:28,310
fuel

00:04:24,889 --> 00:04:31,999
a few external users cases so here we

00:04:28,310 --> 00:04:39,319
Instagram you skate helix box and gee

00:04:31,999 --> 00:04:42,409
boss so in a data center with a lot of

00:04:39,319 --> 00:04:44,090
diversity so here example we have

00:04:42,409 --> 00:04:45,919
different types of jobs running a data

00:04:44,090 --> 00:04:49,819
center so we have for example this

00:04:45,919 --> 00:04:52,669
database service we have two partitions

00:04:49,819 --> 00:04:55,180
and each partition made running backup

00:04:52,669 --> 00:04:57,590
jobs like periodically antic backups and

00:04:55,180 --> 00:04:59,960
also we may have ETF jobs that take

00:04:57,590 --> 00:05:02,990
snapshots and transfer the snapshots to

00:04:59,960 --> 00:05:04,939
HDFS so we can see that the different

00:05:02,990 --> 00:05:07,189
types of jobs they're both long running

00:05:04,939 --> 00:05:10,610
services and they can be bad jobs

00:05:07,189 --> 00:05:13,180
running together in a data center so

00:05:10,610 --> 00:05:15,020
here is a broader picture so we have

00:05:13,180 --> 00:05:17,270
applications with diverse requirements

00:05:15,020 --> 00:05:19,969
running together in the data center so

00:05:17,270 --> 00:05:22,849
we can have like online service we can

00:05:19,969 --> 00:05:25,370
have near lying applications we can have

00:05:22,849 --> 00:05:27,349
batch applications they are distributed

00:05:25,370 --> 00:05:29,689
across the data center and on each

00:05:27,349 --> 00:05:33,770
physical note we can running multiple

00:05:29,689 --> 00:05:35,719
types of these jobs and for example the

00:05:33,770 --> 00:05:38,779
database is the online job and the badge

00:05:35,719 --> 00:05:41,990
is a new line job and it's et our batch

00:05:38,779 --> 00:05:44,960
jobs so how these processes are running

00:05:41,990 --> 00:05:47,240
each machines so we can have each

00:05:44,960 --> 00:05:50,060
process running natively on the physical

00:05:47,240 --> 00:05:52,849
machine so the problem is there's no

00:05:50,060 --> 00:05:55,129
resource isolation so process we will

00:05:52,849 --> 00:05:58,460
interfere with each other and they will

00:05:55,129 --> 00:06:00,589
compete for the resources so another

00:05:58,460 --> 00:06:02,479
solution is to use virtual virtual

00:06:00,589 --> 00:06:04,580
machines so these virtual machines we

00:06:02,479 --> 00:06:07,610
can create multiple ones on each

00:06:04,580 --> 00:06:11,360
physical machine and let the process

00:06:07,610 --> 00:06:13,610
running on each virtual machines so this

00:06:11,360 --> 00:06:17,060
provides strict isolation on the

00:06:13,610 --> 00:06:19,189
resources so authoritative ly we can

00:06:17,060 --> 00:06:21,020
have this containers running on multiple

00:06:19,189 --> 00:06:23,180
containers running on the physical

00:06:21,020 --> 00:06:25,460
machine and running process inside these

00:06:23,180 --> 00:06:26,960
physical machines so the containers

00:06:25,460 --> 00:06:29,960
compelled to the virtual machines they

00:06:26,960 --> 00:06:31,520
are more flexible they are easy they're

00:06:29,960 --> 00:06:33,649
more like light we're here to start and

00:06:31,520 --> 00:06:37,039
stop and also the provide isolation zone

00:06:33,649 --> 00:06:39,279
resources like memory CPU and disk

00:06:37,039 --> 00:06:39,279
storage

00:06:39,430 --> 00:06:44,870
so it's a summary like we can run

00:06:41,840 --> 00:06:47,449
processes individually this tends to be

00:06:44,870 --> 00:06:49,520
either poor isolation or pro utilization

00:06:47,449 --> 00:06:52,039
we can have virtual machines they

00:06:49,520 --> 00:06:54,710
provide better isolation examples as

00:06:52,039 --> 00:06:56,840
then and hyper 5 etc and we cannot use

00:06:54,710 --> 00:06:58,819
containers so these containers use

00:06:56,840 --> 00:07:01,639
control groups provided by the kernel to

00:06:58,819 --> 00:07:04,729
do the resource isolations examples are

00:07:01,639 --> 00:07:07,310
young and meses so compared to virtual

00:07:04,729 --> 00:07:10,220
machines this container super

00:07:07,310 --> 00:07:15,530
lightweight they are more dynamic based

00:07:10,220 --> 00:07:16,729
on application requirements so both

00:07:15,530 --> 00:07:19,460
virtualization and the container

00:07:16,729 --> 00:07:22,069
container containerization significantly

00:07:19,460 --> 00:07:24,380
improves the process isolation and opens

00:07:22,069 --> 00:07:28,699
up possibilities for a better

00:07:24,380 --> 00:07:30,710
utilization of the physical resources so

00:07:28,699 --> 00:07:33,080
let's see how the container with how

00:07:30,710 --> 00:07:36,919
continue can solve to deploy and manage

00:07:33,080 --> 00:07:40,340
systems inside data center so here are

00:07:36,919 --> 00:07:43,969
an example so let's see we have 33

00:07:40,340 --> 00:07:47,000
applications ABC and for application a

00:07:43,969 --> 00:07:49,729
wee requires 3 contains a continuous

00:07:47,000 --> 00:07:52,520
running and each container requires 64

00:07:49,729 --> 00:07:54,320
megabytes of memory for application p we

00:07:52,520 --> 00:07:57,590
require two containers each of them

00:07:54,320 --> 00:08:00,080
requires 128 megabytes of memory and for

00:07:57,590 --> 00:08:02,330
continuously we have full application

00:08:00,080 --> 00:08:04,789
see we have one container requires 256

00:08:02,330 --> 00:08:06,560
megabytes so this is just a simple

00:08:04,789 --> 00:08:08,930
example that use one time mention of the

00:08:06,560 --> 00:08:10,729
resources but in reality you can have

00:08:08,930 --> 00:08:13,940
multiple dimensions of resources when

00:08:10,729 --> 00:08:16,849
you trying to specify the application

00:08:13,940 --> 00:08:18,860
requirements so once we come up with

00:08:16,849 --> 00:08:21,319
these replication requirements we can

00:08:18,860 --> 00:08:23,750
locate these containers inside physical

00:08:21,319 --> 00:08:25,310
machines so here is the example we start

00:08:23,750 --> 00:08:27,880
all these continuous and then we can

00:08:25,310 --> 00:08:31,130
start the process inside the containers

00:08:27,880 --> 00:08:33,110
so continuously is very powerful but the

00:08:31,130 --> 00:08:35,630
problem is to process always fit so

00:08:33,110 --> 00:08:39,669
nicely into the internal resources that

00:08:35,630 --> 00:08:42,500
that's had that has been really reserved

00:08:39,669 --> 00:08:44,839
so one power may be over utilization so

00:08:42,500 --> 00:08:47,930
first we locate a container with this

00:08:44,839 --> 00:08:49,790
much memory and graduate the process run

00:08:47,930 --> 00:08:50,300
inside this container the use up all the

00:08:49,790 --> 00:08:53,450
memories

00:08:50,300 --> 00:08:56,990
and goes beyond the limit so in this

00:08:53,450 --> 00:08:59,570
case we need to preempt and relaunch the

00:08:56,990 --> 00:09:02,060
container in somewhere else with a large

00:08:59,570 --> 00:09:05,829
allocation and start the process inside

00:09:02,060 --> 00:09:09,980
it and in more common cases this is

00:09:05,829 --> 00:09:12,050
under utilization because the code base

00:09:09,980 --> 00:09:14,750
is complicated the workload is dynamic

00:09:12,050 --> 00:09:16,820
people don't people tends to

00:09:14,750 --> 00:09:18,920
overestimate the resources is the

00:09:16,820 --> 00:09:21,800
application of the applications trying

00:09:18,920 --> 00:09:23,149
to use so we can start processing

00:09:21,800 --> 00:09:26,029
containers but there are some resources

00:09:23,149 --> 00:09:27,829
with waste in the containers this our

00:09:26,029 --> 00:09:30,019
provisioning will remain there until we

00:09:27,829 --> 00:09:33,850
started the container with a more

00:09:30,019 --> 00:09:37,160
appropriate estimation of the resources

00:09:33,850 --> 00:09:39,230
to handle failures so physical machines

00:09:37,160 --> 00:09:41,720
can go down for example in this case we

00:09:39,230 --> 00:09:43,790
can have the second node goes down so to

00:09:41,720 --> 00:09:45,709
handle videos we need to start the

00:09:43,790 --> 00:09:48,500
application I started container someone

00:09:45,709 --> 00:09:51,589
somewhere else in the health machines

00:09:48,500 --> 00:09:55,370
and restart the process so this is cool

00:09:51,589 --> 00:09:57,320
but what about stateful systems so here

00:09:55,370 --> 00:09:59,630
we show that for application a there may

00:09:57,320 --> 00:10:02,660
be some state associated with these

00:09:59,630 --> 00:10:04,550
containers or this this processes so

00:10:02,660 --> 00:10:06,500
these three containers one of them are

00:10:04,550 --> 00:10:10,250
running in master master mode and two of

00:10:06,500 --> 00:10:12,230
them running slave mode so if the second

00:10:10,250 --> 00:10:13,970
node fails without the additional

00:10:12,230 --> 00:10:15,950
information the Master will become

00:10:13,970 --> 00:10:20,630
unavailable into it the container

00:10:15,950 --> 00:10:23,180
restarted somewhere else to do with

00:10:20,630 --> 00:10:24,980
scouting so workload may grow and we

00:10:23,180 --> 00:10:27,529
need to add new machines to the cluster

00:10:24,980 --> 00:10:30,380
so we need to allocate new containers

00:10:27,529 --> 00:10:31,790
and scales to the workload so here

00:10:30,380 --> 00:10:36,230
example we add a new machine to the

00:10:31,790 --> 00:10:38,360
cluster so in order to skeletal system

00:10:36,230 --> 00:10:40,790
we need to repartition the workload we

00:10:38,360 --> 00:10:42,920
shut down the containers and we locate

00:10:40,790 --> 00:10:49,519
three new containers each of them will

00:10:42,920 --> 00:10:50,870
take one third of the new shouting so in

00:10:49,519 --> 00:10:53,269
a summary for the container based

00:10:50,870 --> 00:10:56,209
solution so in terms of the utilization

00:10:53,269 --> 00:10:59,390
the application need to specify the

00:10:56,209 --> 00:11:01,339
resource requirement upfront and also to

00:10:59,390 --> 00:11:03,770
do with for tolerance which we mainly

00:11:01,339 --> 00:11:04,320
just start a new containers in someone

00:11:03,770 --> 00:11:06,060
else

00:11:04,320 --> 00:11:09,210
and to scouting we need to repartition

00:11:06,060 --> 00:11:11,250
the workload and also we need to bring

00:11:09,210 --> 00:11:14,640
up the new containers to holding the new

00:11:11,250 --> 00:11:16,710
new shots and for service discovery

00:11:14,640 --> 00:11:18,690
because we only know that where other

00:11:16,710 --> 00:11:20,730
containers are running so we can detect

00:11:18,690 --> 00:11:28,560
the existence of the of the container

00:11:20,730 --> 00:11:30,780
but by default nothing more than that so

00:11:28,560 --> 00:11:32,850
the container model provides flexibility

00:11:30,780 --> 00:11:35,520
within machines but within each

00:11:32,850 --> 00:11:38,310
containers the workload is more or less

00:11:35,520 --> 00:11:40,280
homogeneous and with the containers

00:11:38,310 --> 00:11:42,090
doesn't know the types of the

00:11:40,280 --> 00:11:44,460
applications on the inside the

00:11:42,090 --> 00:11:47,460
applications inside the containers and

00:11:44,460 --> 00:11:49,410
there's no concept of rows or States

00:11:47,460 --> 00:11:51,960
associates with these applications

00:11:49,410 --> 00:11:56,370
running inside the containers so when

00:11:51,960 --> 00:11:58,380
you something like final grinned so

00:11:56,370 --> 00:12:01,440
essentially we need to some design that

00:11:58,380 --> 00:12:04,790
use task as a unifying abstraction of

00:12:01,440 --> 00:12:07,800
the workload unit inside the data center

00:12:04,790 --> 00:12:11,010
so let's look at this ABC again so in

00:12:07,800 --> 00:12:13,860
this case instead of specifying the real

00:12:11,010 --> 00:12:16,170
physical requirements for for

00:12:13,860 --> 00:12:18,090
application a we just see that it should

00:12:16,170 --> 00:12:21,390
be completed in less than five hours and

00:12:18,090 --> 00:12:24,480
4b we see that should always have

00:12:21,390 --> 00:12:26,610
to continuous running & 4 cbc that for

00:12:24,480 --> 00:12:29,280
each request the response time should be

00:12:26,610 --> 00:12:31,860
less than 50 milliseconds so instead of

00:12:29,280 --> 00:12:34,350
specifying these physical resources we

00:12:31,860 --> 00:12:37,920
are trying to specify the real

00:12:34,350 --> 00:12:40,410
requirements of the applications so

00:12:37,920 --> 00:12:42,900
using the abstraction of a task we can

00:12:40,410 --> 00:12:45,810
have multiple tasks running inside each

00:12:42,900 --> 00:12:49,980
containers and this tasks can be of

00:12:45,810 --> 00:12:53,340
different types so how to do with over

00:12:49,980 --> 00:12:57,930
utilization so we have task one that

00:12:53,340 --> 00:13:00,330
used up is resources and it tries to go

00:12:57,930 --> 00:13:01,680
beyond the limit of the containers so in

00:13:00,330 --> 00:13:03,960
this case we can allocate a new

00:13:01,680 --> 00:13:08,340
container somewhere else with a large

00:13:03,960 --> 00:13:10,800
larger community larger size and wrigley

00:13:08,340 --> 00:13:14,130
long to the task inside there and shut

00:13:10,800 --> 00:13:16,910
down the original one so in this way we

00:13:14,130 --> 00:13:16,910
can hide the

00:13:18,730 --> 00:13:24,290
we can hide the overhead of the

00:13:21,080 --> 00:13:28,000
continual restart so how about under

00:13:24,290 --> 00:13:30,950
utilization so again we can consolidate

00:13:28,000 --> 00:13:33,590
the tasks into one container and shut

00:13:30,950 --> 00:13:35,480
down the the the other container so in

00:13:33,590 --> 00:13:38,060
this case we can optimize the container

00:13:35,480 --> 00:13:42,260
allocation based on the rules usage of

00:13:38,060 --> 00:13:44,690
the application to deal with state for

00:13:42,260 --> 00:13:48,800
systems like this for example we have

00:13:44,690 --> 00:13:51,860
three tasks each of them has three

00:13:48,800 --> 00:13:54,320
containers running wine leader mode the

00:13:51,860 --> 00:13:56,720
other two in standby mode so if some

00:13:54,320 --> 00:13:59,840
wood goes down we can let the leader the

00:13:56,720 --> 00:14:03,770
leader the standby node assumes the

00:13:59,840 --> 00:14:06,800
leadership so this is more desirable for

00:14:03,770 --> 00:14:12,110
systems that cannot wait for a new

00:14:06,800 --> 00:14:15,440
container to start so for the so forth

00:14:12,110 --> 00:14:17,270
for discovery since we have this state

00:14:15,440 --> 00:14:21,350
associated with these tasks we can not

00:14:17,270 --> 00:14:24,110
only discovery by the by the existence

00:14:21,350 --> 00:14:27,710
of the container but we can also do the

00:14:24,110 --> 00:14:30,290
discovery on state so we cannot learn in

00:14:27,710 --> 00:14:32,480
when we learn where everything is

00:14:30,290 --> 00:14:35,900
running as well as what state each task

00:14:32,480 --> 00:14:39,230
seemed so how about scaling so again we

00:14:35,900 --> 00:14:42,680
have two containers running six tasks

00:14:39,230 --> 00:14:46,610
and the start of workload is gross and

00:14:42,680 --> 00:14:49,460
we want to add a new you know to discuss

00:14:46,610 --> 00:14:52,040
to rebalance the workload so we can you

00:14:49,460 --> 00:14:54,740
we can start a new container in the new

00:14:52,040 --> 00:14:58,670
node and just move the move the task

00:14:54,740 --> 00:15:01,280
over there so here is a summary

00:14:58,670 --> 00:15:04,040
compelling this this container business

00:15:01,280 --> 00:15:08,180
solution task resolution so use the task

00:15:04,040 --> 00:15:10,310
as the abstraction we can distribute the

00:15:08,180 --> 00:15:12,710
task as media to a minimum set of

00:15:10,310 --> 00:15:15,650
containers so this provides more

00:15:12,710 --> 00:15:18,710
flexible and dynamic way to use

00:15:15,650 --> 00:15:21,470
containers and to do with for tolerance

00:15:18,710 --> 00:15:23,570
the existing task assume a new state

00:15:21,470 --> 00:15:25,940
without while waiting for the new

00:15:23,570 --> 00:15:28,310
container to start and for scouting we

00:15:25,940 --> 00:15:28,850
don't need to repartition the workload

00:15:28,310 --> 00:15:30,829
which as

00:15:28,850 --> 00:15:33,709
to move the task student across

00:15:30,829 --> 00:15:36,800
containers and for discovery since we

00:15:33,709 --> 00:15:38,500
know more about what what tasks only

00:15:36,800 --> 00:15:41,569
inside each container so we can do

00:15:38,500 --> 00:15:46,130
discovery not only on existence but also

00:15:41,569 --> 00:15:48,050
on the state so what's the benefit of

00:15:46,130 --> 00:15:50,540
task based solution so we can reuse

00:15:48,050 --> 00:15:53,269
containers and also we can minimize the

00:15:50,540 --> 00:15:56,569
overhead of relaunch containers and it

00:15:53,269 --> 00:15:59,060
provides a fine grain scheduling so we

00:15:56,569 --> 00:16:00,860
can compare and task two containers in a

00:15:59,060 --> 00:16:04,009
similar way we compel thread to

00:16:00,860 --> 00:16:08,630
processes so tasks will be the more

00:16:04,009 --> 00:16:10,190
appropriate level of abstraction so we

00:16:08,630 --> 00:16:12,410
see that we need a more performance

00:16:10,190 --> 00:16:14,300
centric approach to do the resource and

00:16:12,410 --> 00:16:17,420
management and working at task

00:16:14,300 --> 00:16:20,420
granularities is more powerful so how

00:16:17,420 --> 00:16:22,370
helix can help so systems like young and

00:16:20,420 --> 00:16:25,579
resource they provide these containers

00:16:22,370 --> 00:16:28,670
which brings flexibility inside each

00:16:25,579 --> 00:16:32,480
virtual inside each machines and while

00:16:28,670 --> 00:16:35,420
helix can bring these concepts of tasks

00:16:32,480 --> 00:16:37,819
which brings more flexibility inside in

00:16:35,420 --> 00:16:42,980
terms of resource allocation inside a

00:16:37,819 --> 00:16:45,410
container so connect we're going to

00:16:42,980 --> 00:16:48,100
continue to talk about task management

00:16:45,410 --> 00:16:48,100
with helix

00:16:51,920 --> 00:16:58,579
noticed we haven't really spent that

00:16:56,269 --> 00:17:00,260
much time talking about helix we just

00:16:58,579 --> 00:17:01,670
talked about this hypothetical solution

00:17:00,260 --> 00:17:04,819
if we had this magical thing that can

00:17:01,670 --> 00:17:05,929
work with a task granularity so I'm

00:17:04,819 --> 00:17:07,939
going to talk a little bit about what

00:17:05,929 --> 00:17:10,910
we've added to helix to kind of support

00:17:07,939 --> 00:17:13,220
going in this direction before I do that

00:17:10,910 --> 00:17:14,750
I'm going to talk about you know things

00:17:13,220 --> 00:17:17,000
that an application generally always

00:17:14,750 --> 00:17:19,339
needs to be able to do so the first

00:17:17,000 --> 00:17:20,360
thing is capacity planning so we

00:17:19,339 --> 00:17:22,100
basically need to make sure that we have

00:17:20,360 --> 00:17:24,290
enough physical resources up and running

00:17:22,100 --> 00:17:26,839
so that it can handle our application

00:17:24,290 --> 00:17:28,130
next we need to actually deploy and

00:17:26,839 --> 00:17:31,070
launch things on those physical

00:17:28,130 --> 00:17:32,660
resources once they're running we need

00:17:31,070 --> 00:17:34,809
to make sure they stay available so we

00:17:32,660 --> 00:17:37,820
need some concept of fault tolerance and

00:17:34,809 --> 00:17:39,850
as Jason alluded to we also in care

00:17:37,820 --> 00:17:42,260
about the states of these systems

00:17:39,850 --> 00:17:44,000
because we actually want the code to be

00:17:42,260 --> 00:17:45,740
doing something useful and we care as

00:17:44,000 --> 00:17:49,040
external observers off and what things

00:17:45,740 --> 00:17:52,549
are doing so let me just talk a little

00:17:49,040 --> 00:17:54,500
bit about helix concepts first the

00:17:52,549 --> 00:17:56,929
unifying concept in helix is called a

00:17:54,500 --> 00:17:59,270
resource and this is not to be confused

00:17:56,929 --> 00:18:01,400
with the physical resource this is more

00:17:59,270 --> 00:18:03,080
like a just like a logical entity that

00:18:01,400 --> 00:18:05,419
you're distributing in your cluster and

00:18:03,080 --> 00:18:09,500
we say the resource can be partitioned

00:18:05,419 --> 00:18:11,059
and you know for the purposes is talk

00:18:09,500 --> 00:18:12,500
you can think of a partition of resource

00:18:11,059 --> 00:18:15,080
sort of like what we've been calling it

00:18:12,500 --> 00:18:18,919
task the entire time and that sense of

00:18:15,080 --> 00:18:21,140
resource is sort of a group of tasks all

00:18:18,919 --> 00:18:23,480
partitions can be replicated and all

00:18:21,140 --> 00:18:25,880
replicas can have some state associated

00:18:23,480 --> 00:18:28,520
with them so maybe you have a master

00:18:25,880 --> 00:18:30,140
slave state model you'll see that for

00:18:28,520 --> 00:18:31,309
each partition you have a master and

00:18:30,140 --> 00:18:37,400
maybe something else in a different

00:18:31,309 --> 00:18:39,380
state and you might ask how do we decide

00:18:37,400 --> 00:18:41,720
you know what state things should be in

00:18:39,380 --> 00:18:43,910
and it's all comes down to this state

00:18:41,720 --> 00:18:45,500
model with constraints so the state

00:18:43,910 --> 00:18:48,049
model defines the states that you have

00:18:45,500 --> 00:18:50,270
in your system and the possible

00:18:48,049 --> 00:18:51,500
transitions you can have in it and then

00:18:50,270 --> 00:18:54,049
you have the constraints which say

00:18:51,500 --> 00:18:55,400
things like at various scopes you know

00:18:54,049 --> 00:18:58,850
for a partition there should only be

00:18:55,400 --> 00:19:00,559
exactly one replica that's master or at

00:18:58,850 --> 00:19:03,200
a node level there should be no more

00:19:00,559 --> 00:19:05,149
than ten partitions hosted on this node

00:19:03,200 --> 00:19:06,679
and transition constraints or things

00:19:05,149 --> 00:19:08,269
like oh you know we should only be doing

00:19:06,679 --> 00:19:09,889
this many transitions at this scope

00:19:08,269 --> 00:19:14,210
because otherwise it's gonna be too much

00:19:09,889 --> 00:19:16,309
entropy in the cluster so put simply

00:19:14,210 --> 00:19:21,409
helix was all about managing the state

00:19:16,309 --> 00:19:23,360
of tasks in the system and the way the

00:19:21,409 --> 00:19:25,789
helix does this is it takes these tasks

00:19:23,360 --> 00:19:28,370
and it associate and it assigns them to

00:19:25,789 --> 00:19:29,720
what we call participants which are

00:19:28,370 --> 00:19:32,090
roughly you can think of them as

00:19:29,720 --> 00:19:35,899
containers that are running code that

00:19:32,090 --> 00:19:38,269
can accept these tasks and this mapping

00:19:35,899 --> 00:19:40,669
is computed by our controller which is

00:19:38,269 --> 00:19:42,080
thought tolerant distributed etc and

00:19:40,669 --> 00:19:45,110
then we have these things called

00:19:42,080 --> 00:19:47,539
spectators your kind of hard to see here

00:19:45,110 --> 00:19:50,480
but they're basically able to see what

00:19:47,539 --> 00:19:54,110
this mapping is because it's this kind

00:19:50,480 --> 00:19:55,279
of the service discovery side of it so

00:19:54,110 --> 00:19:57,799
diving a little bit deeper into the

00:19:55,279 --> 00:20:00,590
controller here's what it's doing at a

00:19:57,799 --> 00:20:02,600
high level it's taking those constraints

00:20:00,590 --> 00:20:04,309
and the live nodes in your system and

00:20:02,600 --> 00:20:06,559
speeding them until we call the

00:20:04,309 --> 00:20:08,210
rebalance ER and the rebalance ER is

00:20:06,559 --> 00:20:11,260
responsible for coming up with some

00:20:08,210 --> 00:20:13,250
assignment of those tasks to nodes and

00:20:11,260 --> 00:20:15,110
as I said the constraints could be

00:20:13,250 --> 00:20:18,230
something like single master or no more

00:20:15,110 --> 00:20:19,130
than three task for instance etc so this

00:20:18,230 --> 00:20:21,350
weird balance or interface is pretty

00:20:19,130 --> 00:20:24,529
simple it just takes a snapshot of the

00:20:21,350 --> 00:20:25,880
cluster some configuration some

00:20:24,529 --> 00:20:27,740
constraints and it comes up with a

00:20:25,880 --> 00:20:29,120
mapping and we have a few default

00:20:27,740 --> 00:20:32,769
implementations and we allow you to plug

00:20:29,120 --> 00:20:35,419
in your own if they don't work for you

00:20:32,769 --> 00:20:36,889
but you know as you know we've been

00:20:35,419 --> 00:20:38,659
bringing up all these new concepts

00:20:36,889 --> 00:20:41,570
related task based assignments and all

00:20:38,659 --> 00:20:42,529
that working with containers and so we

00:20:41,570 --> 00:20:45,440
haven't really talked about any of that

00:20:42,529 --> 00:20:47,600
yet so what else do we need we need some

00:20:45,440 --> 00:20:49,549
way to allocate containers we need some

00:20:47,600 --> 00:20:52,880
way to deploy services on Lucas on those

00:20:49,549 --> 00:20:54,710
containers we need isolation and we need

00:20:52,880 --> 00:20:56,299
some way of monitoring how our resource

00:20:54,710 --> 00:20:59,809
how are the physical resources in our

00:20:56,299 --> 00:21:01,789
cluster are being used so the first

00:20:59,809 --> 00:21:03,889
thing we added to our controller was

00:21:01,789 --> 00:21:06,470
something we call a target provider and

00:21:03,889 --> 00:21:08,990
what this does is it based on some

00:21:06,470 --> 00:21:10,429
strategy it determines how many

00:21:08,990 --> 00:21:12,620
containers we should have running in our

00:21:10,429 --> 00:21:14,899
system and this strategy could be fixed

00:21:12,620 --> 00:21:16,370
which is which is to say we should

00:21:14,899 --> 00:21:18,710
always have K containers running in

00:21:16,370 --> 00:21:22,040
system no matter what it could be like

00:21:18,710 --> 00:21:24,020
CPU which does make sure you have enough

00:21:22,040 --> 00:21:27,950
containers running such that the usage

00:21:24,020 --> 00:21:29,690
is never this much CPU and then you can

00:21:27,950 --> 00:21:32,420
have some combination of that so far

00:21:29,690 --> 00:21:33,770
we've only implemented fixed but we're

00:21:32,420 --> 00:21:34,910
working on integrating with monitoring

00:21:33,770 --> 00:21:37,880
systems so we can get enough information

00:21:34,910 --> 00:21:41,090
to do some of the other ones and here's

00:21:37,880 --> 00:21:42,470
the interface again pretty simple given

00:21:41,090 --> 00:21:44,210
a snapshot of the cluster and some

00:21:42,470 --> 00:21:45,890
constraints just figure out how many

00:21:44,210 --> 00:21:49,790
containers to acquire release start and

00:21:45,890 --> 00:21:52,250
stop so going back to this picture I'm

00:21:49,790 --> 00:21:53,600
kind of on a parallel track we have the

00:21:52,250 --> 00:21:55,850
constraints and nodes feeding into this

00:21:53,600 --> 00:21:58,790
target provider so what do we do with

00:21:55,850 --> 00:22:01,280
this target provide a response well

00:21:58,790 --> 00:22:02,510
that's the second thing that we've this

00:22:01,280 --> 00:22:04,010
concept they've introduced into the

00:22:02,510 --> 00:22:07,220
controller it's called that container

00:22:04,010 --> 00:22:09,020
provider and this is where you think of

00:22:07,220 --> 00:22:10,700
like yard and mesa so this is where we

00:22:09,020 --> 00:22:12,080
say okay so we have these requirements

00:22:10,700 --> 00:22:14,900
or things we need to start up in our

00:22:12,080 --> 00:22:16,250
cluster let's go to one of these systems

00:22:14,900 --> 00:22:21,050
that's actually good at doing those

00:22:16,250 --> 00:22:23,260
things and the interface is pretty

00:22:21,050 --> 00:22:25,820
simple it's like you pass these in

00:22:23,260 --> 00:22:27,410
basically for each container that you

00:22:25,820 --> 00:22:31,850
want to change the state of you passed

00:22:27,410 --> 00:22:34,880
it in calling one of these methods so I

00:22:31,850 --> 00:22:37,610
also mentioned this logical container

00:22:34,880 --> 00:22:39,800
provider which is to say sometimes we

00:22:37,610 --> 00:22:40,940
have containers running in our system

00:22:39,800 --> 00:22:43,130
where we you know we're not using

00:22:40,940 --> 00:22:44,660
containerization or whatever this is

00:22:43,130 --> 00:22:46,190
this contest pretty general in the sense

00:22:44,660 --> 00:22:48,410
that you can take things that are

00:22:46,190 --> 00:22:50,090
running and maybe not even start

00:22:48,410 --> 00:22:52,820
something but even like mark something

00:22:50,090 --> 00:22:56,559
is I should be using it so in this in

00:22:52,820 --> 00:22:56,559
that sense it's like a logical mapping

00:22:56,830 --> 00:23:01,220
so back to this picture again target

00:22:59,660 --> 00:23:03,320
provider feeds in the container provider

00:23:01,220 --> 00:23:05,240
which actually affects the number of

00:23:03,320 --> 00:23:06,830
nodes in the system which goes back into

00:23:05,240 --> 00:23:08,900
the rebound sore task assignment part of

00:23:06,830 --> 00:23:10,130
helix so it's kind of like you have

00:23:08,900 --> 00:23:11,840
these two parallel things that are

00:23:10,130 --> 00:23:13,820
affecting affecting each other and

00:23:11,840 --> 00:23:15,650
there's this food feedback loop that's

00:23:13,820 --> 00:23:18,650
constantly happening as your cluster

00:23:15,650 --> 00:23:20,120
changes and we call a target provider

00:23:18,650 --> 00:23:24,650
plus the container provider our

00:23:20,120 --> 00:23:27,560
provisioner so mapping respects the

00:23:24,650 --> 00:23:29,000
application lifecycle you can think of

00:23:27,560 --> 00:23:30,140
the target providers taking care of

00:23:29,000 --> 00:23:32,180
capacity planning

00:23:30,140 --> 00:23:34,190
the container provider taking care of

00:23:32,180 --> 00:23:35,330
the provisioning part of it and then the

00:23:34,190 --> 00:23:36,890
fault tolerance and state management

00:23:35,330 --> 00:23:38,900
that's sort of what helix was good at

00:23:36,890 --> 00:23:44,300
already and it's only being enhanced by

00:23:38,900 --> 00:23:45,410
the provisioner so at a high level what

00:23:44,300 --> 00:23:46,730
kind of cysts like how would we

00:23:45,410 --> 00:23:50,600
integrate with things like yarn and

00:23:46,730 --> 00:23:52,400
maces to make this work so in general we

00:23:50,600 --> 00:23:54,310
need some sort of resource provider like

00:23:52,400 --> 00:23:56,390
a master which is responsible for

00:23:54,310 --> 00:23:58,520
keeping track of all the resources in

00:23:56,390 --> 00:24:01,910
the system so we assume that all the

00:23:58,520 --> 00:24:04,550
container providers have this we're able

00:24:01,910 --> 00:24:06,950
to submit work to these resources which

00:24:04,550 --> 00:24:09,530
starts up a container which runs our

00:24:06,950 --> 00:24:12,260
helix controller now once the seal those

00:24:09,530 --> 00:24:13,700
controllers running up running we can go

00:24:12,260 --> 00:24:14,960
back to this resource provider and start

00:24:13,700 --> 00:24:17,870
asking for containers for actual

00:24:14,960 --> 00:24:20,270
applications and I should mention that

00:24:17,870 --> 00:24:21,470
this controller is meant to multi-tenant

00:24:20,270 --> 00:24:25,370
so it should we be able to manage

00:24:21,470 --> 00:24:27,230
multiple applications so it starts up a

00:24:25,370 --> 00:24:29,870
container which will be running our

00:24:27,230 --> 00:24:31,430
application code and this will be

00:24:29,870 --> 00:24:33,620
running you know this will be able to

00:24:31,430 --> 00:24:37,670
accept whatever tasks at the helix

00:24:33,620 --> 00:24:40,430
controller signs to it so if we look at

00:24:37,670 --> 00:24:43,310
yarn by itself it's actually you can see

00:24:40,430 --> 00:24:45,410
a lot of overlap here so we have an

00:24:43,310 --> 00:24:47,810
application master which is started up

00:24:45,410 --> 00:24:51,620
when an application is submitted by a

00:24:47,810 --> 00:24:53,180
client and then that's kind of

00:24:51,620 --> 00:24:55,940
responsible for starting containers for

00:24:53,180 --> 00:24:58,970
its application so if we add he looks to

00:24:55,940 --> 00:25:00,080
this it mats pretty well it's a in the

00:24:58,970 --> 00:25:02,060
application master were running the

00:25:00,080 --> 00:25:03,500
controller and then the regular

00:25:02,060 --> 00:25:05,660
containers were running the helix

00:25:03,500 --> 00:25:07,910
participants and those except tests and

00:25:05,660 --> 00:25:11,750
we also have you know HDFS we can pull

00:25:07,910 --> 00:25:14,390
our application packages from now going

00:25:11,750 --> 00:25:16,430
to Mesa fundamentally it's actually not

00:25:14,390 --> 00:25:19,190
that different the one big difference is

00:25:16,430 --> 00:25:20,870
that it may so that your kind of how you

00:25:19,190 --> 00:25:24,830
have a schedule is negotiating offers

00:25:20,870 --> 00:25:27,410
with the master so here it's like you

00:25:24,830 --> 00:25:29,420
know once and offers sort of deemed to

00:25:27,410 --> 00:25:31,180
be fit for your application then you're

00:25:29,420 --> 00:25:34,360
able to run an executor on your slave

00:25:31,180 --> 00:25:36,650
which will be a run tests within that so

00:25:34,360 --> 00:25:37,970
if you add helix here we can say that

00:25:36,650 --> 00:25:40,040
the scheduler sort of running in the

00:25:37,970 --> 00:25:42,440
helix controller maybe this controller

00:25:40,040 --> 00:25:43,399
were started up using some you know

00:25:42,440 --> 00:25:45,019
method of

00:25:43,399 --> 00:25:50,149
scheduler failover fault tolerance

00:25:45,019 --> 00:25:51,619
there's a few ways to do that and from

00:25:50,149 --> 00:25:53,509
then on you know the scheduler is one of

00:25:51,619 --> 00:25:55,849
the schedulers that it's talking to May

00:25:53,509 --> 00:25:57,979
sauce and it can negotiate the starting

00:25:55,849 --> 00:25:59,239
of a slave machine which can start up

00:25:57,979 --> 00:26:06,469
executors which can hold a few

00:25:59,239 --> 00:26:10,279
participants so let me just give a brief

00:26:06,469 --> 00:26:14,509
example let's go back to what we started

00:26:10,279 --> 00:26:17,779
with at the beginning we have a database

00:26:14,509 --> 00:26:19,489
which is holding a few partitions again

00:26:17,779 --> 00:26:22,190
we say that a partition sort of maps to

00:26:19,489 --> 00:26:23,809
a task here and we say that frees

00:26:22,190 --> 00:26:25,549
partition we should have exactly one

00:26:23,809 --> 00:26:27,039
master and we should be roughly

00:26:25,549 --> 00:26:29,779
distributing our masters in some

00:26:27,039 --> 00:26:32,450
reasonable way so there's no machine

00:26:29,779 --> 00:26:33,889
that as too many masters at the same

00:26:32,450 --> 00:26:36,019
time we have these backup tasks which

00:26:33,889 --> 00:26:38,259
would be running for each partition on

00:26:36,019 --> 00:26:40,549
one of the machines that has a slave and

00:26:38,259 --> 00:26:44,450
we have etl which is taking snapshots

00:26:40,549 --> 00:26:47,749
and pushing the HDFS so if something

00:26:44,450 --> 00:26:49,849
goes down one of the slaves in this case

00:26:47,749 --> 00:26:53,690
we're Perdition one gets promoted to

00:26:49,849 --> 00:26:56,299
master and the p0 backup moves to the

00:26:53,690 --> 00:27:02,869
other machine that or container here

00:26:56,299 --> 00:27:04,549
that has been hosting a slave for p0 so

00:27:02,869 --> 00:27:06,049
showing how that maps to the yarn

00:27:04,549 --> 00:27:09,679
picture I showed a couple minutes ago

00:27:06,049 --> 00:27:11,659
here we're just saying that all the

00:27:09,679 --> 00:27:14,749
tests will be started up are living in

00:27:11,659 --> 00:27:18,369
this container as the abstraction and

00:27:14,749 --> 00:27:20,570
these are assigned by the controller

00:27:18,369 --> 00:27:22,309
we're still working on this part but

00:27:20,570 --> 00:27:25,429
we're thinking we can specify this with

00:27:22,309 --> 00:27:27,080
some sort of Y Amal so here we say

00:27:25,429 --> 00:27:30,799
things like you know how do we get to

00:27:27,080 --> 00:27:33,139
our packages what kind of services are

00:27:30,799 --> 00:27:34,609
we actually playing to run and then we

00:27:33,139 --> 00:27:36,919
have this thing called a service config

00:27:34,609 --> 00:27:39,379
map which is what you think of as the

00:27:36,919 --> 00:27:41,179
specification for our target provider so

00:27:39,379 --> 00:27:42,739
here we could so say things like oh we

00:27:41,179 --> 00:27:44,690
want to have three containers with some

00:27:42,739 --> 00:27:46,519
requirements something you might see

00:27:44,690 --> 00:27:47,839
traditionally or if you've implemented

00:27:46,519 --> 00:27:50,269
the right target provider you could even

00:27:47,839 --> 00:27:51,739
go a lot more fine-grained and say

00:27:50,269 --> 00:27:53,419
things like oh you know the service

00:27:51,739 --> 00:27:55,369
should be taking no more than five hours

00:27:53,419 --> 00:27:56,550
to complete so start up as many

00:27:55,369 --> 00:28:01,160
containers you can eat as

00:27:56,550 --> 00:28:03,030
need to in order to honor that request

00:28:01,160 --> 00:28:04,740
so once you've started up your

00:28:03,030 --> 00:28:09,440
application your container is going to

00:28:04,740 --> 00:28:11,430
start and we have a few callback based

00:28:09,440 --> 00:28:12,960
interfaces that you can implement in

00:28:11,430 --> 00:28:14,730
order to you know be able to do things

00:28:12,960 --> 00:28:17,010
when this happens so from a container

00:28:14,730 --> 00:28:19,290
scope we have this thing that's like we

00:28:17,010 --> 00:28:20,850
have initialization and then we're told

00:28:19,290 --> 00:28:23,220
when we're going online or when we're

00:28:20,850 --> 00:28:24,450
going offline so you have any startup or

00:28:23,220 --> 00:28:27,390
clean up you need to do this is where

00:28:24,450 --> 00:28:28,890
you can do it and then within the for

00:28:27,390 --> 00:28:31,290
the test within the containers we have

00:28:28,890 --> 00:28:32,820
our own callbacks so there's something

00:28:31,290 --> 00:28:35,130
like a backup task it makes sense to

00:28:32,820 --> 00:28:36,360
have callbacks like this where it's you

00:28:35,130 --> 00:28:41,190
know it says when it's time to start

00:28:36,360 --> 00:28:42,990
pause resume cancel etc but if your task

00:28:41,190 --> 00:28:44,160
is like a database partition you

00:28:42,990 --> 00:28:47,340
actually want something a little bit

00:28:44,160 --> 00:28:49,380
more detailed so in this case we support

00:28:47,340 --> 00:28:50,490
and this is something that we've been

00:28:49,380 --> 00:28:52,410
sweet support basically from the

00:28:50,490 --> 00:28:55,050
beginning with helix where for each

00:28:52,410 --> 00:28:57,600
state transition we get a call back so

00:28:55,050 --> 00:28:59,700
you know when you become a slave from a

00:28:57,600 --> 00:29:02,430
master or you know any other transition

00:28:59,700 --> 00:29:04,740
so from your application perspective you

00:29:02,430 --> 00:29:11,210
know how to get ready to handle that

00:29:04,740 --> 00:29:13,170
task and just a note on discovery

00:29:11,210 --> 00:29:16,080
basically we have these methods I can

00:29:13,170 --> 00:29:17,760
say okay for this partition or task you

00:29:16,080 --> 00:29:19,830
know I want to only want the Masters

00:29:17,760 --> 00:29:23,700
because this is a database and i want to

00:29:19,830 --> 00:29:24,690
send rights to the master or for you

00:29:23,700 --> 00:29:26,250
know if I'm doing a reading it's a

00:29:24,690 --> 00:29:28,260
database maybe i don't care i can send

00:29:26,250 --> 00:29:33,900
it to a slave so then I just care about

00:29:28,260 --> 00:29:37,470
who's hosting this partition so as a

00:29:33,900 --> 00:29:39,450
kind of overall summary of this talk

00:29:37,470 --> 00:29:41,790
it's it's basically that you know

00:29:39,450 --> 00:29:44,370
containerisation has been huge for data

00:29:41,790 --> 00:29:46,440
center utilization isolation etc and

00:29:44,370 --> 00:29:48,660
with helix we just want to take it a

00:29:46,440 --> 00:29:51,390
step further and make you know fine

00:29:48,660 --> 00:29:53,010
grain scheduling possible by really you

00:29:51,390 --> 00:29:55,890
know focusing on on the tasks that are

00:29:53,010 --> 00:29:57,300
running inside containers and with the

00:29:55,890 --> 00:30:00,900
target provider and container provider

00:29:57,300 --> 00:30:04,920
abstractions any popular provisioner can

00:30:00,900 --> 00:30:06,810
be plugged in so we have a little bit of

00:30:04,920 --> 00:30:10,500
time so I'm going to try to do a quick

00:30:06,810 --> 00:30:12,960
demo and this is what i would call

00:30:10,500 --> 00:30:14,250
the hello world service so this is sort

00:30:12,960 --> 00:30:17,310
of just like showing that we can

00:30:14,250 --> 00:30:18,900
dynamically change containers based on

00:30:17,310 --> 00:30:23,810
our fixed requirements we always say

00:30:18,900 --> 00:30:26,160
that k containers must be running and if

00:30:23,810 --> 00:30:27,990
that number ever changes like something

00:30:26,160 --> 00:30:33,690
fails then you know will automatically

00:30:27,990 --> 00:30:40,370
start up a new container so we find out

00:30:33,690 --> 00:30:44,840
okay so first thing I'm going to do is

00:30:40,370 --> 00:30:44,840
start up yarn

00:31:02,250 --> 00:31:10,549
ok

00:31:04,669 --> 00:31:18,730
now i'm going to start up a hello world

00:31:10,549 --> 00:31:18,730
service i gotta wait

00:31:31,800 --> 00:31:35,550
now if this works right on the dock on

00:31:34,350 --> 00:31:40,530
the left you're going to see a bunch of

00:31:35,550 --> 00:31:42,840
random java processes start up and right

00:31:40,530 --> 00:31:47,970
now I'm just saying that we should have

00:31:42,840 --> 00:31:51,600
exactly three containers running and so

00:31:47,970 --> 00:31:53,040
we've started up we sent requests to

00:31:51,600 --> 00:31:55,350
start those three containers let's see

00:31:53,040 --> 00:31:56,760
if they there they are okay so now we

00:31:55,350 --> 00:32:03,660
have three containers running for our

00:31:56,760 --> 00:32:06,300
system now we didn't make this clear but

00:32:03,660 --> 00:32:10,200
helix uses zookeeper to maintain its

00:32:06,300 --> 00:32:12,060
State so what i can do is connect to

00:32:10,200 --> 00:32:20,430
this local zoo keeper writing the helos

00:32:12,060 --> 00:32:24,570
controller i can see that we have three

00:32:20,430 --> 00:32:28,230
live instances running and they're all

00:32:24,570 --> 00:32:29,370
online for this container so i'm not

00:32:28,230 --> 00:32:30,720
going to be running any tests on this

00:32:29,370 --> 00:32:36,830
container but i'm going to show that we

00:32:30,720 --> 00:32:36,830
can vary these containers dynamically so

00:32:40,930 --> 00:32:45,770
I'll use this tool to change the number

00:32:44,480 --> 00:32:55,700
of containers that should be running to

00:32:45,770 --> 00:32:58,250
two and one just died and so now we see

00:32:55,700 --> 00:33:04,100
that these two containers are available

00:32:58,250 --> 00:33:12,380
to serve this search service and I can

00:33:04,100 --> 00:33:16,820
take it back to three and now we have

00:33:12,380 --> 00:33:18,410
three again so yeah so basically what

00:33:16,820 --> 00:33:19,910
this shows is that we've basically done

00:33:18,410 --> 00:33:22,310
most of the plumbing required in order

00:33:19,910 --> 00:33:24,370
to work with this kind of task based

00:33:22,310 --> 00:33:27,350
abstraction and let helix control

00:33:24,370 --> 00:33:29,090
something like yarn in order to tell it

00:33:27,350 --> 00:33:32,240
that you know more or fewer containers

00:33:29,090 --> 00:33:34,040
are necessary so our kind of our current

00:33:32,240 --> 00:33:35,900
progress is basically we've got this

00:33:34,040 --> 00:33:37,810
plumbing done with the yarn it's about a

00:33:35,900 --> 00:33:40,400
third of the way done with May sews and

00:33:37,810 --> 00:33:42,350
we're working on some better api's for

00:33:40,400 --> 00:33:43,820
actually being able to start you know

00:33:42,350 --> 00:33:45,890
batch tasks because right now you can

00:33:43,820 --> 00:33:48,850
run like helix has traditionally been

00:33:45,890 --> 00:33:51,740
for more like long-running things and

00:33:48,850 --> 00:33:53,750
highly staple services so we're trying

00:33:51,740 --> 00:33:59,090
to make helix kind of a little bit more

00:33:53,750 --> 00:34:00,950
flexible in that sense so with that be

00:33:59,090 --> 00:34:03,280
happy to take any questions thanks for

00:34:00,950 --> 00:34:03,280
listening

00:34:15,619 --> 00:34:22,109
so do you actually use helix with both

00:34:18,539 --> 00:34:25,499
yarn and besos at LinkedIn currently

00:34:22,109 --> 00:34:27,149
like at LinkedIn we're sort of in our

00:34:25,499 --> 00:34:29,190
offline data processing side there

00:34:27,149 --> 00:34:30,779
they're moving towards yarn so we need

00:34:29,190 --> 00:34:32,970
to get this built before they actually

00:34:30,779 --> 00:34:34,230
did this integration so we're just going

00:34:32,970 --> 00:34:36,059
to be ready by the time they do that

00:34:34,230 --> 00:34:38,549
that's that's sort of where we're at it

00:34:36,059 --> 00:34:39,960
linked in with my sauce I think Ben is

00:34:38,549 --> 00:34:42,299
actually going to talk to some people

00:34:39,960 --> 00:34:51,119
over there so right now we're not using

00:34:42,299 --> 00:34:53,429
mace us at all in you and LinkedIn does

00:34:51,119 --> 00:34:56,220
it kind of make sense eventually for

00:34:53,429 --> 00:35:01,140
helix to be folded into yarn or fold it

00:34:56,220 --> 00:35:02,609
into a sauce uh possibly I mean it's

00:35:01,140 --> 00:35:05,160
sort of like right now they're solving

00:35:02,609 --> 00:35:07,710
more or less orthogonal problems so he

00:35:05,160 --> 00:35:09,599
looks is all about you know making sure

00:35:07,710 --> 00:35:11,640
that things like your system does

00:35:09,599 --> 00:35:15,299
something useful when things are running

00:35:11,640 --> 00:35:16,710
that conserve your system and now we're

00:35:15,299 --> 00:35:19,019
just doing this integration so that

00:35:16,710 --> 00:35:22,920
helix can give hints to these systems so

00:35:19,019 --> 00:35:25,859
that applications can have more you know

00:35:22,920 --> 00:35:27,509
Richard have richer requirements that we

00:35:25,859 --> 00:35:30,809
can actually act on so that's sort of

00:35:27,509 --> 00:35:34,289
the motivation for all this in terms of

00:35:30,809 --> 00:35:35,849
which direction that should be I think

00:35:34,289 --> 00:35:37,910
it's possibly reasonable that they could

00:35:35,849 --> 00:35:40,609
be integrated into them one day but

00:35:37,910 --> 00:35:44,779
that's that's something that I think

00:35:40,609 --> 00:35:44,779
requires some design work and we'll see

00:36:05,900 --> 00:36:12,269
can you like integrated helix with

00:36:08,519 --> 00:36:14,549
others let's say management software is

00:36:12,269 --> 00:36:18,089
for example amber embody from

00:36:14,549 --> 00:36:21,059
Hortonworks could you repeat the

00:36:18,089 --> 00:36:23,190
question sorry can you integrate helix

00:36:21,059 --> 00:36:25,579
with other management software for

00:36:23,190 --> 00:36:28,650
example I'm buzzy from Hortonworks

00:36:25,579 --> 00:36:33,990
Ojinaga Lizzie I'm personally not

00:36:28,650 --> 00:36:36,630
familiar with that particular system at

00:36:33,990 --> 00:36:39,990
a high level what does it do they'd like

00:36:36,630 --> 00:36:45,720
to install and provision install manage

00:36:39,990 --> 00:36:48,329
Hadoop clusters yeah I think at some at

00:36:45,720 --> 00:36:56,069
some level there's some some things we'd

00:36:48,329 --> 00:36:58,349
get out of that so for instance okay

00:36:56,069 --> 00:37:01,140
let's take a closer manager I mean if

00:36:58,349 --> 00:37:04,230
you're familiar with that so can I

00:37:01,140 --> 00:37:06,690
integrate helix with Florida manager

00:37:04,230 --> 00:37:08,220
who's vice versa I mean fundamentally if

00:37:06,690 --> 00:37:09,720
you can implement these four functions

00:37:08,220 --> 00:37:12,539
using your system then we can integrate

00:37:09,720 --> 00:37:15,900
helix with it become so that's a Java

00:37:12,539 --> 00:37:19,980
interface yeah I mean right now helix is

00:37:15,900 --> 00:37:22,769
more or less jvm specific so i think we

00:37:19,980 --> 00:37:26,279
have some bindings in clojure that are

00:37:22,769 --> 00:37:28,920
more or less experimental but it's yeah

00:37:26,279 --> 00:37:33,119
he looks at right now is mostly Java we

00:37:28,920 --> 00:37:34,470
have some client-side Python support but

00:37:33,119 --> 00:37:35,910
yeah right now we're we're calling for

00:37:34,470 --> 00:37:39,779
people if they're interested to write

00:37:35,910 --> 00:37:44,099
new language bindings okay one last

00:37:39,779 --> 00:37:46,739
question one last question so I think I

00:37:44,099 --> 00:37:49,710
saw a slight where you displayed all the

00:37:46,739 --> 00:37:52,829
available containers oh I'm a collector

00:37:49,710 --> 00:37:55,319
did I see properly do you have a tool to

00:37:52,829 --> 00:37:57,359
see all the containers I'm sorry I can

00:37:55,319 --> 00:38:00,900
hear do you have a tool to see all the

00:37:57,359 --> 00:38:04,079
containers which you're managing stuff

00:38:00,900 --> 00:38:05,910
like that no uh helix doesn't really

00:38:04,079 --> 00:38:08,339
have a dashboard so we're kind of stuck

00:38:05,910 --> 00:38:10,980
using zoo inspector which is a zookeeper

00:38:08,339 --> 00:38:13,319
tool and since helix uses zookeeper to

00:38:10,980 --> 00:38:15,180
manage its state we can piggyback off of

00:38:13,319 --> 00:38:16,549
that so with this thing called an

00:38:15,180 --> 00:38:18,170
external view

00:38:16,549 --> 00:38:19,759
so for our service we can see exactly

00:38:18,170 --> 00:38:24,789
which containers are available for that

00:38:19,759 --> 00:38:24,789
service okay thanks thanks

00:38:45,130 --> 00:38:47,730
so

00:38:49,780 --> 00:38:55,420
I saw that you have some sort of moving

00:38:53,890 --> 00:38:58,960
one task from one container to another

00:38:55,420 --> 00:39:02,820
container so is it the task is completed

00:38:58,960 --> 00:39:06,430
relaunched from scratch so we kind of

00:39:02,820 --> 00:39:08,230
hid what that's actually doing generally

00:39:06,430 --> 00:39:10,300
speaking what we're doing is we're when

00:39:08,230 --> 00:39:11,860
we think it's time to move something we

00:39:10,300 --> 00:39:13,570
actually start a new task in the

00:39:11,860 --> 00:39:15,040
container and you know start

00:39:13,570 --> 00:39:16,720
bootstrapping it from and start

00:39:15,040 --> 00:39:18,220
bootstrapping it and when we feel it's

00:39:16,720 --> 00:39:20,050
sufficiently bootstrap then we can shut

00:39:18,220 --> 00:39:22,660
down the other one and then transition

00:39:20,050 --> 00:39:23,920
that one to the primary so because we

00:39:22,660 --> 00:39:25,360
have these state transitions that our

00:39:23,920 --> 00:39:27,760
disposal we can kind of hide that

00:39:25,360 --> 00:39:31,290
overhead we're starting new tasking

00:39:27,760 --> 00:39:31,290
containers or starting new containers

00:39:37,339 --> 00:39:42,519

YouTube URL: https://www.youtube.com/watch?v=ZJbMDhMq43c


