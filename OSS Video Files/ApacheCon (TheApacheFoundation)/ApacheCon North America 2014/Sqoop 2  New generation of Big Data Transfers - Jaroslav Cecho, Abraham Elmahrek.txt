Title: Sqoop 2  New generation of Big Data Transfers - Jaroslav Cecho, Abraham Elmahrek
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,030 --> 00:00:05,790
okay so welcome everyone we have almost

00:00:02,700 --> 00:00:07,319
full room which I'm glad to see before

00:00:05,790 --> 00:00:09,480
we will dive into the into the

00:00:07,319 --> 00:00:11,730
presentation I would like to see you

00:00:09,480 --> 00:00:13,259
know how is the audience familiar with

00:00:11,730 --> 00:00:15,599
all the different technologies that we

00:00:13,259 --> 00:00:18,060
are going to talk about so just a quick

00:00:15,599 --> 00:00:20,100
question app let's do a poll who knows

00:00:18,060 --> 00:00:24,840
is familiar with a relational databases

00:00:20,100 --> 00:00:26,460
you know use them cool so some guys are

00:00:24,840 --> 00:00:30,179
little sleeping or not putting their

00:00:26,460 --> 00:00:33,149
hands up so um how many of you are

00:00:30,179 --> 00:00:35,130
actually using relational databases in a

00:00:33,149 --> 00:00:37,200
production environment is a part of a

00:00:35,130 --> 00:00:39,750
you know for clothes that are placed on

00:00:37,200 --> 00:00:41,670
your business interesting more people is

00:00:39,750 --> 00:00:43,469
using the production the databases in a

00:00:41,670 --> 00:00:49,050
production environment then using the

00:00:43,469 --> 00:00:51,120
databases at all interesting the last

00:00:49,050 --> 00:00:53,550
question for a relation database is how

00:00:51,120 --> 00:00:55,980
many of you are using more than one

00:00:53,550 --> 00:01:00,930
vendor in a production environment you

00:00:55,980 --> 00:01:04,320
know considerably less let's say like 10

00:01:00,930 --> 00:01:06,270
percent of the room okay so that you

00:01:04,320 --> 00:01:10,950
know has been databases now let's move

00:01:06,270 --> 00:01:13,290
to a to a Hadoop Hadoop war who here is

00:01:10,950 --> 00:01:14,150
familiar with Hadoop or you know her the

00:01:13,290 --> 00:01:17,820
key word before

00:01:14,150 --> 00:01:19,650
almost everyone I'm excited actually and

00:01:17,820 --> 00:01:24,150
who here uses Hadoop in a production

00:01:19,650 --> 00:01:26,759
environment Wow almost everyone guys I

00:01:24,150 --> 00:01:29,460
like you and now coming to you know

00:01:26,759 --> 00:01:31,110
scoop right more than half of the room

00:01:29,460 --> 00:01:33,150
is using databases in a production

00:01:31,110 --> 00:01:34,500
environment more than half of the room

00:01:33,150 --> 00:01:37,140
is using how to be in a production

00:01:34,500 --> 00:01:41,159
environment so who already is using

00:01:37,140 --> 00:01:43,229
scoop in a production environment one

00:01:41,159 --> 00:01:45,810
single guys come on what are you doing

00:01:43,229 --> 00:01:47,399
how are you transferring the data well I

00:01:45,810 --> 00:01:51,930
guess that's why you are here right

00:01:47,399 --> 00:01:55,350
after all okay so the pilots of today's

00:01:51,930 --> 00:01:56,700
session is Abra Hamel Marek he's my very

00:01:55,350 --> 00:01:57,290
good friend and colleague back at

00:01:56,700 --> 00:02:00,689
Cloudera

00:01:57,290 --> 00:02:03,180
he said core hue engineer and also a

00:02:00,689 --> 00:02:06,659
very very frequent contributor to scoop

00:02:03,180 --> 00:02:07,560
and also your records check on my friend

00:02:06,659 --> 00:02:09,330
yard sites

00:02:07,560 --> 00:02:12,030
he is the co-author of the scoop

00:02:09,330 --> 00:02:13,430
cookbook as well as a PMC member and

00:02:12,030 --> 00:02:17,739
committer on several Hadoop

00:02:13,430 --> 00:02:19,939
excuse me several projects and Apache so

00:02:17,739 --> 00:02:21,159
we're gonna start off a little bit of a

00:02:19,939 --> 00:02:25,609
little bit of history

00:02:21,159 --> 00:02:27,799
so before Hadoop there were data based

00:02:25,609 --> 00:02:29,930
systems data systems if you will that

00:02:27,799 --> 00:02:33,109
did one job in that one have one job

00:02:29,930 --> 00:02:34,489
very well it's stored data you basically

00:02:33,109 --> 00:02:36,319
had all of your data in these data

00:02:34,489 --> 00:02:38,870
systems and then you had a compute

00:02:36,319 --> 00:02:40,700
system somewhere else and maybe you

00:02:38,870 --> 00:02:43,400
transferred that data from the data

00:02:40,700 --> 00:02:44,780
system to the compute systems via a

00:02:43,400 --> 00:02:48,980
network attached storage or something

00:02:44,780 --> 00:02:51,260
and so this worked pretty well and what

00:02:48,980 --> 00:02:53,930
was found was for these systems a lot of

00:02:51,260 --> 00:02:56,450
them required specific hardware that

00:02:53,930 --> 00:02:59,060
specific hardware was generally very

00:02:56,450 --> 00:03:01,519
very very very large and if you wanted

00:02:59,060 --> 00:03:04,549
to actually scale you had to scale

00:03:01,519 --> 00:03:08,870
vertically this meant get larger boxes

00:03:04,549 --> 00:03:11,389
and so a dip came along and changed

00:03:08,870 --> 00:03:15,769
things up a little bit so Hadoop

00:03:11,389 --> 00:03:18,229
actually took computation to the data

00:03:15,769 --> 00:03:21,229
rather moving the data from computation

00:03:18,229 --> 00:03:24,229
Hadoop took moving computation to data

00:03:21,229 --> 00:03:26,900
that was really big also it didn't

00:03:24,229 --> 00:03:29,569
require any special hardware it could

00:03:26,900 --> 00:03:32,689
use just your generic PC any commodity

00:03:29,569 --> 00:03:35,599
hardware that was also very big scaling

00:03:32,689 --> 00:03:37,159
was built into the system so instead of

00:03:35,599 --> 00:03:39,470
having to worry about partitioning or

00:03:37,159 --> 00:03:41,060
scaling vertically it actually took care

00:03:39,470 --> 00:03:43,879
of it for you it was a forethought

00:03:41,060 --> 00:03:46,340
rather than an afterthought so the way

00:03:43,879 --> 00:03:48,500
you do this is horizontally it applies

00:03:46,340 --> 00:03:50,979
horizontal scaling all you have to do is

00:03:48,500 --> 00:03:54,019
add an entirely new node and your scale

00:03:50,979 --> 00:03:56,750
so was to dupe the first to do

00:03:54,019 --> 00:04:01,819
horizontal scaling no but it does a

00:03:56,750 --> 00:04:04,250
pretty good job so who were the early

00:04:01,819 --> 00:04:07,220
adopters of Hadoop well excuse me the

00:04:04,250 --> 00:04:09,949
early adopters of Hadoop how what what

00:04:07,220 --> 00:04:12,650
problems did they have so they had you

00:04:09,949 --> 00:04:14,720
know Hadoop cluster lined around with a

00:04:12,650 --> 00:04:16,459
lot of resources and then they had these

00:04:14,720 --> 00:04:18,169
data systems in their data warehouse

00:04:16,459 --> 00:04:20,599
that had all of the data but no

00:04:18,169 --> 00:04:24,380
resources so the problem was how do you

00:04:20,599 --> 00:04:25,639
move that data to Hadoop and so there

00:04:24,380 --> 00:04:27,650
were a couple of you know custom

00:04:25,639 --> 00:04:29,630
solutions out there

00:04:27,650 --> 00:04:30,770
generally speaking these these companies

00:04:29,630 --> 00:04:33,020
that had these problems they would build

00:04:30,770 --> 00:04:35,120
their own stuff you know it would be

00:04:33,020 --> 00:04:37,370
very specific they'd have like my sequel

00:04:35,120 --> 00:04:38,870
dump or something they dump it they dump

00:04:37,370 --> 00:04:40,490
it into a Hadoop you know but then there

00:04:38,870 --> 00:04:43,310
were a lot of problems with that

00:04:40,490 --> 00:04:46,669
Hadoop has a variety of different

00:04:43,310 --> 00:04:48,020
components a hive HBase etc you might

00:04:46,669 --> 00:04:48,889
want to represent that data in a

00:04:48,020 --> 00:04:51,380
different way you know

00:04:48,889 --> 00:04:52,009
Avro see Avro sequence files something

00:04:51,380 --> 00:04:53,690
of that nature

00:04:52,009 --> 00:04:56,630
and that didn't that didn't really you

00:04:53,690 --> 00:04:58,100
know really really helped or it didn't

00:04:56,630 --> 00:04:59,990
really do it for them right if they

00:04:58,100 --> 00:05:02,930
wanted to do that or do it themselves it

00:04:59,990 --> 00:05:04,310
was a lot of work that's a problem and

00:05:02,930 --> 00:05:07,160
then on top of that you may have

00:05:04,310 --> 00:05:09,229
multiple database systems and doing that

00:05:07,160 --> 00:05:11,720
for every database system is on top of

00:05:09,229 --> 00:05:12,949
that also a lot of work and then you

00:05:11,720 --> 00:05:14,360
know there's the problem of monitoring

00:05:12,949 --> 00:05:15,530
and potentially you have other

00:05:14,360 --> 00:05:21,530
guarantees and things of that nature

00:05:15,530 --> 00:05:25,160
it's just a lot of work mmm and so

00:05:21,530 --> 00:05:28,340
that's where scoop came in so scoop is

00:05:25,160 --> 00:05:30,740
the one tool to rule them all if you can

00:05:28,340 --> 00:05:32,720
imagine it in a database agnostic

00:05:30,740 --> 00:05:34,220
fashion will connect to any database

00:05:32,720 --> 00:05:35,570
that you can think of any relational

00:05:34,220 --> 00:05:37,990
database that you can think of and then

00:05:35,570 --> 00:05:40,340
enable you to transfer data into Hadoop

00:05:37,990 --> 00:05:42,560
also it integrates with all of the

00:05:40,340 --> 00:05:44,260
Hadoop components or the majority of the

00:05:42,560 --> 00:05:46,370
new components such as HBase and hive

00:05:44,260 --> 00:05:48,860
and also if you just want to like

00:05:46,370 --> 00:05:50,960
transfer data to HDFS you can choose

00:05:48,860 --> 00:05:57,919
sequence files or a profiles or just

00:05:50,960 --> 00:06:02,630
plain text files etc etc so who here is

00:05:57,919 --> 00:06:06,289
familiar with hackathons cool just about

00:06:02,630 --> 00:06:09,169
everybody so scoop was actually a

00:06:06,289 --> 00:06:11,570
hackathon project from a former cloud I

00:06:09,169 --> 00:06:15,470
remember Aaron Kimball that's how it

00:06:11,570 --> 00:06:17,570
started off and so really scoop had a

00:06:15,470 --> 00:06:20,570
couple of goals in mind when Aaron

00:06:17,570 --> 00:06:23,000
Kimball was building this it was for

00:06:20,570 --> 00:06:24,800
one-time transfers you have a bunch of

00:06:23,000 --> 00:06:26,599
data in your relational database systems

00:06:24,800 --> 00:06:29,840
and you just want to transfer that data

00:06:26,599 --> 00:06:32,229
to Hadoop one time then you know wait

00:06:29,840 --> 00:06:35,800
around a week two weeks maybe a month

00:06:32,229 --> 00:06:38,270
don't use scoop and then you do it again

00:06:35,800 --> 00:06:39,110
also you know you had to be a super user

00:06:38,270 --> 00:06:43,550
to do this kind

00:06:39,110 --> 00:06:44,750
thing so fast-forward to the future or

00:06:43,550 --> 00:06:47,570
excuse me fast-forward to the present

00:06:44,750 --> 00:06:49,250
and you know there's a couple of new

00:06:47,570 --> 00:06:51,800
workflows out there that we need to

00:06:49,250 --> 00:06:53,990
support you know you've you've imported

00:06:51,800 --> 00:06:55,850
all of your data and then you've waited

00:06:53,990 --> 00:06:57,200
like a week or two and then you have

00:06:55,850 --> 00:06:59,300
more data to the relational relational

00:06:57,200 --> 00:07:00,650
database system and so you want to

00:06:59,300 --> 00:07:02,570
import the rest of that data rather than

00:07:00,650 --> 00:07:04,160
duplicate that data and this group world

00:07:02,570 --> 00:07:06,680
we call that an incremental import and

00:07:04,160 --> 00:07:09,440
then on top of that you know we needed

00:07:06,680 --> 00:07:12,650
to provide scoop as a service we're

00:07:09,440 --> 00:07:15,230
finding that companies actually have a

00:07:12,650 --> 00:07:16,340
division of roles division of

00:07:15,230 --> 00:07:19,250
responsibilities things of that nature

00:07:16,340 --> 00:07:21,440
so database administrators Hadoop

00:07:19,250 --> 00:07:23,600
administrators the common user they all

00:07:21,440 --> 00:07:25,280
do slightly different things potentially

00:07:23,600 --> 00:07:27,410
a database administrator would manage

00:07:25,280 --> 00:07:29,180
how you connect to these database

00:07:27,410 --> 00:07:31,610
systems and then the common user would

00:07:29,180 --> 00:07:34,220
defined from where you're transferring

00:07:31,610 --> 00:07:41,030
data and then from to where you're

00:07:34,220 --> 00:07:43,840
transferring data to so it required a

00:07:41,030 --> 00:07:47,270
couple of massive changes to do that

00:07:43,840 --> 00:07:49,070
we've investigated the the scoop a

00:07:47,270 --> 00:07:51,170
community has investigated improving

00:07:49,070 --> 00:07:54,650
scoop one turns out it requires a couple

00:07:51,170 --> 00:07:57,800
of radical changes so they decided to

00:07:54,650 --> 00:08:00,890
start from the ground up and that is

00:07:57,800 --> 00:08:04,040
scoop two it was designed with through

00:08:00,890 --> 00:08:06,830
three new main objectives which is

00:08:04,040 --> 00:08:09,410
ease-of-use basically having a very

00:08:06,830 --> 00:08:12,740
simple client and representing objects

00:08:09,410 --> 00:08:14,270
in the database ease of security which

00:08:12,740 --> 00:08:16,040
is basically having roles that are

00:08:14,270 --> 00:08:17,570
clearly defined such as your database ed

00:08:16,040 --> 00:08:20,080
month that administrators here Hadoop

00:08:17,570 --> 00:08:23,420
the dupe administrators and common users

00:08:20,080 --> 00:08:25,910
also ease of extensibility which is

00:08:23,420 --> 00:08:29,300
basically having a very clearly defined

00:08:25,910 --> 00:08:31,430
interface for your connectors so the

00:08:29,300 --> 00:08:33,470
connectors in scoop one by the way they

00:08:31,430 --> 00:08:36,020
did just about everything in scoop too

00:08:33,470 --> 00:08:37,790
that's no longer the case really all the

00:08:36,020 --> 00:08:40,070
connectors are responsible for in scoop

00:08:37,790 --> 00:08:43,130
two is fetching and storing data from

00:08:40,070 --> 00:08:45,500
the databases and transforming them to

00:08:43,130 --> 00:08:48,110
the intermediate data format which is

00:08:45,500 --> 00:08:50,000
the representation the data takes takes

00:08:48,110 --> 00:08:51,130
the form of while it goes through the

00:08:50,000 --> 00:08:52,750
system

00:08:51,130 --> 00:08:54,070
and for the rest of the technical

00:08:52,750 --> 00:08:58,300
details I'm going to pass the torch to

00:08:54,070 --> 00:09:00,700
your it says so the very structure the

00:08:58,300 --> 00:09:02,560
presentation I will continue

00:09:00,700 --> 00:09:05,020
you know describing how scoop works oh

00:09:02,560 --> 00:09:06,820
it's good to word I will start in with

00:09:05,020 --> 00:09:09,880
ten thousand feet and we will you know

00:09:06,820 --> 00:09:12,010
continue landing all the way down so on

00:09:09,880 --> 00:09:15,040
really high level you know how scoop

00:09:12,010 --> 00:09:16,990
this scripture works well you have a

00:09:15,040 --> 00:09:18,640
server client model right you can see

00:09:16,990 --> 00:09:21,880
several clients everywhere so I'm

00:09:18,640 --> 00:09:26,010
assuming that you're not surprised the

00:09:21,880 --> 00:09:26,010
server is doing all the heavy lifting

00:09:26,190 --> 00:09:33,700
whereas the client is very light very

00:09:28,780 --> 00:09:35,560
light place would beef notice on the

00:09:33,700 --> 00:09:37,930
original scoop on it if mentioned and

00:09:35,560 --> 00:09:39,580
the one user that before you know raise

00:09:37,930 --> 00:09:43,150
the hand it is already using scoop on in

00:09:39,580 --> 00:09:46,630
production the scoop one is a very thick

00:09:43,150 --> 00:09:49,660
client when you run scoop one you had

00:09:46,630 --> 00:09:51,550
need to access to the hadoop to your

00:09:49,660 --> 00:09:53,950
Hadoop cluster you need to have access

00:09:51,550 --> 00:09:56,800
to your database directly from the node

00:09:53,950 --> 00:09:58,180
that is executing school so the new

00:09:56,800 --> 00:09:59,920
thing about you know how would you

00:09:58,180 --> 00:10:02,290
actually use it in a production right

00:09:59,920 --> 00:10:05,830
you're a user you might be working from

00:10:02,290 --> 00:10:08,020
home over VPN but you need direct

00:10:05,830 --> 00:10:09,940
connectivity to the database well then

00:10:08,020 --> 00:10:11,890
it's a security concern right you don't

00:10:09,940 --> 00:10:14,830
want to expose all your databases over

00:10:11,890 --> 00:10:17,350
the VPN so what we did in scoop to the

00:10:14,830 --> 00:10:19,660
client is very very lightweight all it

00:10:17,350 --> 00:10:22,390
does it just contains to the server over

00:10:19,660 --> 00:10:24,820
VPN and then it's the server job to

00:10:22,390 --> 00:10:29,350
connect to the database is to the Hadoop

00:10:24,820 --> 00:10:31,210
cluster etc editor on the I would say

00:10:29,350 --> 00:10:35,590
most important portion of this picture

00:10:31,210 --> 00:10:37,600
is the data flow the scoop server drives

00:10:35,590 --> 00:10:39,460
the entire approaches right it will say

00:10:37,600 --> 00:10:42,640
now is the time to transfer the data

00:10:39,460 --> 00:10:44,590
let's do it but the data themselves are

00:10:42,640 --> 00:10:47,260
not going through the server right

00:10:44,590 --> 00:10:48,970
because there is just one server so that

00:10:47,260 --> 00:10:53,650
would be like a bottleneck on the entire

00:10:48,970 --> 00:10:56,440
transfer instead all the data are coming

00:10:53,650 --> 00:11:00,930
directly between the database and the

00:10:56,440 --> 00:11:00,930
Hadoop cluster I can't

00:11:01,110 --> 00:11:06,550
so we have been on 10,000 feet you know

00:11:04,329 --> 00:11:08,440
server client model very simple now we

00:11:06,550 --> 00:11:11,889
are going to lend a little bit more

00:11:08,440 --> 00:11:13,569
5,000 feet as you can imagine those

00:11:11,889 --> 00:11:16,029
groups however have a lot of different

00:11:13,569 --> 00:11:17,740
components I don't you know even try to

00:11:16,029 --> 00:11:19,779
read the picture that's just you know an

00:11:17,740 --> 00:11:22,000
understanding of the high level we will

00:11:19,779 --> 00:11:23,740
actually dive into all the components

00:11:22,000 --> 00:11:26,009
and describe how they are working and

00:11:23,740 --> 00:11:28,930
interacting with each other

00:11:26,009 --> 00:11:31,569
the first component that I want to

00:11:28,930 --> 00:11:33,819
mention is a connector but before we

00:11:31,569 --> 00:11:35,920
will dive into how its you know in the

00:11:33,819 --> 00:11:37,810
bigger picture let's just stop and talk

00:11:35,920 --> 00:11:40,990
what is what it is actually would it

00:11:37,810 --> 00:11:43,779
represent I mentioned that the power of

00:11:40,990 --> 00:11:46,240
scoop assist that is one tool to rule

00:11:43,779 --> 00:11:48,550
them all right with one two you can

00:11:46,240 --> 00:11:52,000
transfer data from any database in the

00:11:48,550 --> 00:11:54,519
world to your Hadoop cluster well that

00:11:52,000 --> 00:11:56,649
seems like a magic right disappear 300

00:11:54,519 --> 00:11:58,389
you go from this world there is no such

00:11:56,649 --> 00:12:00,459
thing as you know one magical tool that

00:11:58,389 --> 00:12:03,639
can do everything right you would call a

00:12:00,459 --> 00:12:05,230
be SME the way Scoob actually achieved

00:12:03,639 --> 00:12:07,750
this functionality is through a

00:12:05,230 --> 00:12:10,779
pluggable interface called connector and

00:12:07,750 --> 00:12:13,600
each connector is responsible to talk to

00:12:10,779 --> 00:12:16,149
one database so we have a my secure

00:12:13,600 --> 00:12:18,459
connector we have an Oracle connector we

00:12:16,149 --> 00:12:21,189
have a Tara data connector etc etc

00:12:18,459 --> 00:12:25,180
editor and that's how Scoob has the

00:12:21,189 --> 00:12:30,970
power to to you know talk with all the

00:12:25,180 --> 00:12:33,160
other databases the in scoop one we had

00:12:30,970 --> 00:12:35,620
also a you know concept of a connector

00:12:33,160 --> 00:12:37,779
but at that time the connector for Mel

00:12:35,620 --> 00:12:40,089
yeah Hecky thing how to do it yes you

00:12:37,779 --> 00:12:43,329
have a connector but they were very big

00:12:40,089 --> 00:12:45,100
very far doing a lot of things and well

00:12:43,329 --> 00:12:48,759
it wasn't working very well

00:12:45,100 --> 00:12:51,819
so in scoop 2 would be you know how we

00:12:48,759 --> 00:12:54,610
did was hey connector very small thing

00:12:51,819 --> 00:12:56,800
all you're responsible for is just would

00:12:54,610 --> 00:12:59,290
get me the data or just to get the data

00:12:56,800 --> 00:13:05,559
to database nothing else everything else

00:12:59,290 --> 00:13:08,019
is my responsibility now when we've

00:13:05,559 --> 00:13:09,339
defined the connectors let's move to the

00:13:08,019 --> 00:13:12,120
additional component so called

00:13:09,339 --> 00:13:12,120
repository

00:13:12,320 --> 00:13:15,840
the knife mentioned that you know we

00:13:14,580 --> 00:13:17,820
have a different connectors for

00:13:15,840 --> 00:13:19,560
different databases it's probably not

00:13:17,820 --> 00:13:21,870
hard to imagine that different

00:13:19,560 --> 00:13:24,570
connectors different databases have a

00:13:21,870 --> 00:13:26,910
different requirement what they need in

00:13:24,570 --> 00:13:29,430
order to transfer the data right the

00:13:26,910 --> 00:13:32,700
basic example I have a generate JDBC

00:13:29,430 --> 00:13:35,790
connector all I know is a JDBC interface

00:13:32,700 --> 00:13:37,830
and in order to do so I need JDBC URL

00:13:35,790 --> 00:13:42,240
some credentials

00:13:37,830 --> 00:13:44,880
perhaps some connection properties I was

00:13:42,240 --> 00:13:47,070
saying am I'm using SSL and I'm using

00:13:44,880 --> 00:13:49,530
you know some encryption or how the hell

00:13:47,070 --> 00:13:51,660
I actually can get to the database so

00:13:49,530 --> 00:13:54,420
the connector itself you know a java

00:13:51,660 --> 00:13:57,960
code piece of a piece of a code plugin

00:13:54,420 --> 00:13:59,340
is exposing what it means in order to do

00:13:57,960 --> 00:14:02,340
it job

00:13:59,340 --> 00:14:04,800
all those metadata parameters are

00:14:02,340 --> 00:14:06,870
actually storing scoop too we have a you

00:14:04,800 --> 00:14:12,330
know repository where we can put

00:14:06,870 --> 00:14:16,170
additional additional object um in scoop

00:14:12,330 --> 00:14:18,930
voila we had a we had you know concept

00:14:16,170 --> 00:14:21,420
of a of a connector we had the concept

00:14:18,930 --> 00:14:24,540
of parameters but we have like gazillion

00:14:21,420 --> 00:14:26,460
of them so my colleague of mine actually

00:14:24,540 --> 00:14:28,860
mentioned that scoop won't have a

00:14:26,460 --> 00:14:32,790
parameter he'll because you had like you

00:14:28,860 --> 00:14:34,680
know 69 I believe last time different

00:14:32,790 --> 00:14:36,360
parameters were parameter a might not

00:14:34,680 --> 00:14:38,160
work with parameter B that will not work

00:14:36,360 --> 00:14:42,150
parameter C considering that you're

00:14:38,160 --> 00:14:44,070
using connector a and blah blah blah the

00:14:42,150 --> 00:14:46,350
reason why we had this parameter hell

00:14:44,070 --> 00:14:48,450
was that the parameters were exposed by

00:14:46,350 --> 00:14:50,850
the scoop frame or scoop said hey I need

00:14:48,450 --> 00:14:52,860
all this information then the kinetic

00:14:50,850 --> 00:14:54,720
parameters were passed to the connector

00:14:52,860 --> 00:14:57,390
and it was up to the connector what to

00:14:54,720 --> 00:14:59,490
do with them in scoop two we've reversed

00:14:57,390 --> 00:15:01,710
this process it's a connector we're

00:14:59,490 --> 00:15:05,100
saying hey I need those parameters in

00:15:01,710 --> 00:15:08,160
order to run now all those parameters

00:15:05,100 --> 00:15:10,500
have two different families or two

00:15:08,160 --> 00:15:13,680
different sets you can you can think

00:15:10,500 --> 00:15:15,960
about it this way the first set is

00:15:13,680 --> 00:15:18,450
actually changing a lot whereas the

00:15:15,960 --> 00:15:22,140
second set is not changing at all or

00:15:18,450 --> 00:15:24,510
almost so we've promoted and created two

00:15:22,140 --> 00:15:26,310
high level objects

00:15:24,510 --> 00:15:28,560
objects the first object is something

00:15:26,310 --> 00:15:30,390
that we're calling a connection please

00:15:28,560 --> 00:15:32,670
do not think about it as a you know

00:15:30,390 --> 00:15:35,370
actual TCP connection to some server

00:15:32,670 --> 00:15:37,650
it's just a set of metadata you know an

00:15:35,370 --> 00:15:41,220
hash table somewhere in memory saying

00:15:37,650 --> 00:15:43,620
for this key JDBC URL I have this value

00:15:41,220 --> 00:15:48,000
JDBC semicolon blah blah blah blah blah

00:15:43,620 --> 00:15:50,730
blah the logical representation of a

00:15:48,000 --> 00:15:53,730
connection is how to get two dead source

00:15:50,730 --> 00:15:57,120
to that database to death ftp server to

00:15:53,730 --> 00:15:58,980
that mainframe the intention is that you

00:15:57,120 --> 00:16:01,020
will have one connection object per

00:15:58,980 --> 00:16:03,180
physical server per physical database

00:16:01,020 --> 00:16:07,040
that will be created by some DB

00:16:03,180 --> 00:16:07,040
administrators and maintained by them

00:16:07,160 --> 00:16:13,410
the second high-level object more meant

00:16:10,440 --> 00:16:16,380
for for a users rather than

00:16:13,410 --> 00:16:19,290
administrators is called a job and it

00:16:16,380 --> 00:16:21,450
contains set of parameters again you

00:16:19,290 --> 00:16:24,030
know it's the same metadata object a

00:16:21,450 --> 00:16:26,520
hash table that are moment what I want

00:16:24,030 --> 00:16:28,140
to do with the database boss so for

00:16:26,520 --> 00:16:31,170
example I have a table

00:16:28,140 --> 00:16:34,260
what table I want to import what is the

00:16:31,170 --> 00:16:36,810
target system is a type is it HBase is

00:16:34,260 --> 00:16:41,630
it H catalog is it you know name your

00:16:36,810 --> 00:16:43,620
own thing and all those objects are

00:16:41,630 --> 00:16:45,810
represented and stored in a repository

00:16:43,620 --> 00:16:47,910
so that you can query them you can

00:16:45,810 --> 00:16:51,240
change them you can delete them at that

00:16:47,910 --> 00:16:54,000
sir editor up until this point I was

00:16:51,240 --> 00:16:56,040
just talking about very fancy you know

00:16:54,000 --> 00:16:57,750
why how to enter parameters right yes I

00:16:56,040 --> 00:16:59,910
can store them in a database you know

00:16:57,750 --> 00:17:01,920
everybody's doing that right up until

00:16:59,910 --> 00:17:04,320
this point we did not do any

00:17:01,920 --> 00:17:06,959
transformation sorry any transfers or

00:17:04,320 --> 00:17:10,199
data you're just representing metadata

00:17:06,959 --> 00:17:14,160
somewhere then there is again something

00:17:10,199 --> 00:17:17,130
that we learned from our users in scoop

00:17:14,160 --> 00:17:19,589
one you know I've mentioned that

00:17:17,130 --> 00:17:21,449
originally it was mentis I will transfer

00:17:19,589 --> 00:17:24,180
data one that I will wait you know a

00:17:21,449 --> 00:17:26,130
month here and do it again but the users

00:17:24,180 --> 00:17:28,500
iterated to a solution where they needed

00:17:26,130 --> 00:17:30,930
to do the transfers daily right I mean

00:17:28,500 --> 00:17:34,020
Hadoop has become from a more data

00:17:30,930 --> 00:17:35,730
warehouse active our high system the

00:17:34,020 --> 00:17:38,130
thing over daily used right

00:17:35,730 --> 00:17:40,500
I'm transferring the data daily I'm

00:17:38,130 --> 00:17:41,210
doing computation my business depends on

00:17:40,500 --> 00:17:45,450
it

00:17:41,210 --> 00:17:47,640
so what we've seen was that user had a

00:17:45,450 --> 00:17:50,280
long scooped command you know seven

00:17:47,640 --> 00:17:52,770
lines of bash and they copy and paste

00:17:50,280 --> 00:17:56,130
this command all over again and then hit

00:17:52,770 --> 00:17:59,040
enter to execute it change one line one

00:17:56,130 --> 00:18:02,220
letter been executed again very

00:17:59,040 --> 00:18:04,740
troublesome process right so what we

00:18:02,220 --> 00:18:07,170
didn't skip to we said let's distinguish

00:18:04,740 --> 00:18:09,480
between I am changing a metadata and I'm

00:18:07,170 --> 00:18:11,670
running the transfer and that is

00:18:09,480 --> 00:18:15,060
represented by the submission object on

00:18:11,670 --> 00:18:17,490
this line all you know creating a job

00:18:15,060 --> 00:18:19,680
again don't imagine that something is

00:18:17,490 --> 00:18:21,930
running anywhere it's just a set of

00:18:19,680 --> 00:18:24,120
parameters and I have a different

00:18:21,930 --> 00:18:26,580
command hey now is the time to do the

00:18:24,120 --> 00:18:29,670
transfer and a submission object will

00:18:26,580 --> 00:18:31,680
get created the very kind of thing about

00:18:29,670 --> 00:18:34,380
the submission is just you know I've

00:18:31,680 --> 00:18:36,660
executed the job yesterday night it

00:18:34,380 --> 00:18:39,900
finished yesterday at midnight I've

00:18:36,660 --> 00:18:42,390
transferred three million rows in 15

00:18:39,900 --> 00:18:48,690
minutes that doesn't do the math but you

00:18:42,390 --> 00:18:51,390
see the point so we described what is a

00:18:48,690 --> 00:18:54,780
connector general interface for a

00:18:51,390 --> 00:18:56,550
particular database user is creating all

00:18:54,780 --> 00:18:58,500
the various metadata stored in

00:18:56,550 --> 00:19:00,240
repository and now we can say that

00:18:58,500 --> 00:19:05,240
server is everything that I've just

00:19:00,240 --> 00:19:08,040
described the purpose of server is to

00:19:05,240 --> 00:19:12,240
two main purposes the first one

00:19:08,040 --> 00:19:14,010
obviously is the CR UD logic on top of

00:19:12,240 --> 00:19:16,230
the metadata right you need to create

00:19:14,010 --> 00:19:19,950
those objects you need to update them

00:19:16,230 --> 00:19:23,640
you need to delete them the second

00:19:19,950 --> 00:19:25,050
portion is to drive the transfers right

00:19:23,640 --> 00:19:27,840
I've mentioned before we've separated

00:19:25,050 --> 00:19:31,050
the execution from giving the actual

00:19:27,840 --> 00:19:35,160
parameters so Randall job is the second

00:19:31,050 --> 00:19:38,250
important portion the way they design

00:19:35,160 --> 00:19:40,080
the system intentionally when you go say

00:19:38,250 --> 00:19:41,780
hey scoop your server please transfer

00:19:40,080 --> 00:19:45,170
this table to do

00:19:41,780 --> 00:19:48,320
hdfs the script to several EC okay

00:19:45,170 --> 00:19:50,270
let's do it and we'll create a job you

00:19:48,320 --> 00:19:51,890
know this time it's actually not the you

00:19:50,270 --> 00:19:53,480
know object that I described before it's

00:19:51,890 --> 00:19:55,640
really something that will execute on

00:19:53,480 --> 00:19:59,630
some cluster and it will submit it to a

00:19:55,640 --> 00:20:01,940
remote cluster at that point two servo

00:19:59,630 --> 00:20:05,000
will stop driving it and just checking

00:20:01,940 --> 00:20:07,220
how it does the again the weight of

00:20:05,000 --> 00:20:09,350
thing about it I've submitted a job and

00:20:07,220 --> 00:20:11,540
I'm just checking hey did you finish now

00:20:09,350 --> 00:20:17,720
okay continue running did you finish

00:20:11,540 --> 00:20:19,730
you failed Oh too bad why um so the

00:20:17,720 --> 00:20:22,250
benefit of the hiss texture is if you

00:20:19,730 --> 00:20:24,260
will put goop server down while jobs are

00:20:22,250 --> 00:20:26,300
running everything will continue running

00:20:24,260 --> 00:20:27,860
right because the jobs are self

00:20:26,300 --> 00:20:30,350
sufficient to scoop to server is just

00:20:27,860 --> 00:20:32,960
overseeing it so it's not a single point

00:20:30,350 --> 00:20:35,330
of failure right if that dies everything

00:20:32,960 --> 00:20:37,160
runs you cannot run new jobs yeah for

00:20:35,330 --> 00:20:43,700
the bummer but at least the existing

00:20:37,160 --> 00:20:46,490
ones are still running so now we have

00:20:43,700 --> 00:20:48,380
been at 5,000 feet which describe all

00:20:46,490 --> 00:20:50,360
the server architecture and now we're

00:20:48,380 --> 00:20:54,410
going to land now we are going to

00:20:50,360 --> 00:20:56,390
transfer the data again a lesson that we

00:20:54,410 --> 00:20:59,420
learn from our own history when working

00:20:56,390 --> 00:21:01,610
on scoop one in school I was like yeah

00:20:59,420 --> 00:21:04,790
please transfer the data do it whatever

00:21:01,610 --> 00:21:08,630
means necessary it was somehow work well

00:21:04,790 --> 00:21:11,000
it didn't it had a lot of short bags a

00:21:08,630 --> 00:21:13,430
lot of key we had a lot of things

00:21:11,000 --> 00:21:16,160
weren't working together how to follow

00:21:13,430 --> 00:21:17,960
connectors in scoop one have actually up

00:21:16,160 --> 00:21:20,240
until this day don't support HBase

00:21:17,960 --> 00:21:24,290
because we have to do it explicitly and

00:21:20,240 --> 00:21:27,800
a lot of a lot of hiccups so in order to

00:21:24,290 --> 00:21:30,170
avoid that in scoop two we've defined a

00:21:27,800 --> 00:21:32,210
workflow every single job that is

00:21:30,170 --> 00:21:35,330
transferring data have a predefined

00:21:32,210 --> 00:21:39,800
workflow a set of actions that needs to

00:21:35,330 --> 00:21:43,070
happen in the in the order what is also

00:21:39,800 --> 00:21:46,310
important is that for each portion a

00:21:43,070 --> 00:21:48,980
different part is responsible for so

00:21:46,310 --> 00:21:50,420
let's take an email example of an import

00:21:48,980 --> 00:21:51,470
job because you know that's the simplest

00:21:50,420 --> 00:21:53,990
way

00:21:51,470 --> 00:21:56,510
I haven't remove database and I have a

00:21:53,990 --> 00:22:00,230
table there let's say credit card data

00:21:56,510 --> 00:22:02,960
and then I have a Hadoop HDFS file my

00:22:00,230 --> 00:22:07,460
goal get that credit card data to the

00:22:02,960 --> 00:22:09,409
HDFS the first step well is

00:22:07,460 --> 00:22:10,880
initialization right not a big deal I

00:22:09,409 --> 00:22:12,710
mean let's we are all here are

00:22:10,880 --> 00:22:17,690
developers right so we I need to

00:22:12,710 --> 00:22:20,270
initialize something the first important

00:22:17,690 --> 00:22:23,240
portion is a partition all right I have

00:22:20,270 --> 00:22:25,760
one table it's a huge thing and I want

00:22:23,240 --> 00:22:27,980
to transfer it in a parallel so I need

00:22:25,760 --> 00:22:30,409
somehow slice it up into independent

00:22:27,980 --> 00:22:32,480
pieces so that they can be transferred

00:22:30,409 --> 00:22:37,100
in parallel and that's the job of a

00:22:32,480 --> 00:22:39,200
partition er basically slicing data when

00:22:37,100 --> 00:22:41,809
I slice the entire table oh now I can

00:22:39,200 --> 00:22:44,090
you know see hey test now the point when

00:22:41,809 --> 00:22:46,700
I can transfer them and I will create a

00:22:44,090 --> 00:22:49,940
multiple extractor things multiple

00:22:46,700 --> 00:22:52,370
extractor well object or tasks every

00:22:49,940 --> 00:22:54,409
extractor or Egypt's tractor will get

00:22:52,370 --> 00:22:59,030
one partition from the previous phase

00:22:54,409 --> 00:23:01,820
and do the data transfers the important

00:22:59,030 --> 00:23:05,030
thing here that I'm transferring data to

00:23:01,820 --> 00:23:08,090
scoop framework I am NOT at this point

00:23:05,030 --> 00:23:10,010
storing them anywhere and it's all that

00:23:08,090 --> 00:23:13,250
connector does the connector will

00:23:10,010 --> 00:23:16,280
partition the table and get the data to

00:23:13,250 --> 00:23:18,890
the SKU framework in the extractor from

00:23:16,280 --> 00:23:22,640
now on is the job of the framework to

00:23:18,890 --> 00:23:24,500
finish the data data transfer the

00:23:22,640 --> 00:23:27,710
framework will submit a piece cause a

00:23:24,500 --> 00:23:29,960
loader that will load all the data to

00:23:27,710 --> 00:23:32,840
the final destination in this case HDFS

00:23:29,960 --> 00:23:35,510
and finally I have some destroyer that

00:23:32,840 --> 00:23:38,210
will you know finish everything clean up

00:23:35,510 --> 00:23:41,690
perhaps a temporary tables at that's

00:23:38,210 --> 00:23:43,909
right that's right at zero when you do

00:23:41,690 --> 00:23:46,730
the step back and look at it you see a

00:23:43,909 --> 00:23:49,190
basic of a usual ETL system right I have

00:23:46,730 --> 00:23:51,470
an extractor I have a transfer face and

00:23:49,190 --> 00:23:53,179
I have a loader in this case there is no

00:23:51,470 --> 00:23:57,770
transformation phase right so it's a

00:23:53,179 --> 00:23:59,140
sort of a year system however however we

00:23:57,770 --> 00:24:02,390
are planning to add a small

00:23:59,140 --> 00:24:05,130
transformation phase here so that you

00:24:02,390 --> 00:24:08,340
know the scoop can be used for a single

00:24:05,130 --> 00:24:14,220
tl processing not just for any l but you

00:24:08,340 --> 00:24:16,470
know right now just just yeah so I will

00:24:14,220 --> 00:24:17,700
do a small pause and I will be talking

00:24:16,470 --> 00:24:19,530
about something completely different

00:24:17,700 --> 00:24:23,760
because you know I want to confuse all

00:24:19,530 --> 00:24:28,110
of you and I will go back to hey to my

00:24:23,760 --> 00:24:30,240
polling polling system before I ask who

00:24:28,110 --> 00:24:33,030
in the room knows had you and basically

00:24:30,240 --> 00:24:37,080
everyone put their hands up so who in

00:24:33,030 --> 00:24:38,880
the room knows how MapReduce works you

00:24:37,080 --> 00:24:41,700
know what is a mapper what is a reducer

00:24:38,880 --> 00:24:44,340
almost everyone in the room cool how

00:24:41,700 --> 00:24:47,820
many of you wrote as MapReduce job in

00:24:44,340 --> 00:24:50,190
their life at least one majority of

00:24:47,820 --> 00:24:52,050
their own so what I'm going to say not a

00:24:50,190 --> 00:24:55,470
big surprise but I will repeat it anyway

00:24:52,050 --> 00:24:57,840
I'm a pretty job the way you can think

00:24:55,470 --> 00:25:00,570
about it is that it consists from four

00:24:57,840 --> 00:25:02,760
different steps you have an input format

00:25:00,570 --> 00:25:04,380
that will say hey I have a directory

00:25:02,760 --> 00:25:07,920
with all the files that I need to

00:25:04,380 --> 00:25:10,050
process I will split it per input block

00:25:07,920 --> 00:25:12,240
for every single block

00:25:10,050 --> 00:25:14,700
I will call the second stage which is a

00:25:12,240 --> 00:25:17,370
mapper I will run several mappers they

00:25:14,700 --> 00:25:19,500
will do some data computation then I

00:25:17,370 --> 00:25:21,630
will have the reduce phase but I will do

00:25:19,500 --> 00:25:24,270
some aggregation of the data generated

00:25:21,630 --> 00:25:26,580
in mappers and finally using an output

00:25:24,270 --> 00:25:30,210
format I will you know put the data back

00:25:26,580 --> 00:25:32,370
to HDFS store them somewhere now when

00:25:30,210 --> 00:25:35,160
you take a look at you know the workflow

00:25:32,370 --> 00:25:37,830
that we define here with the MapReduce

00:25:35,160 --> 00:25:40,230
one on one you can immediately imagine

00:25:37,830 --> 00:25:42,600
hey that seems like the same thing right

00:25:40,230 --> 00:25:44,910
I can basically take the workflow and

00:25:42,600 --> 00:25:47,490
put it to the MapReduce and voila

00:25:44,910 --> 00:25:49,800
everything will be working and indeed

00:25:47,490 --> 00:25:51,840
that's exactly what we've done we've

00:25:49,800 --> 00:25:56,310
defined that you know a generic workflow

00:25:51,840 --> 00:26:00,660
and applied MapReduce now you might be

00:25:56,310 --> 00:26:02,850
asking why bother doing that right why

00:26:00,660 --> 00:26:04,560
I'm bothering to do some abstraction

00:26:02,850 --> 00:26:07,920
when the abstraction is already there

00:26:04,560 --> 00:26:10,920
I'm actually using the answer to that is

00:26:07,920 --> 00:26:12,780
actually very very interesting what we

00:26:10,920 --> 00:26:15,360
see right now is that MapReduce is

00:26:12,780 --> 00:26:17,280
really heavily used so scoop 2 is using

00:26:15,360 --> 00:26:19,920
MapReduce to do data transfer

00:26:17,280 --> 00:26:23,010
but looking you know into the future

00:26:19,920 --> 00:26:24,780
there is much more different projects

00:26:23,010 --> 00:26:28,140
that are trying to do the same job that

00:26:24,780 --> 00:26:31,500
just differently there is a spark there

00:26:28,140 --> 00:26:33,540
is a yarn indeed there is a test and all

00:26:31,500 --> 00:26:36,020
those things are using the same things

00:26:33,540 --> 00:26:38,670
you know trans execute I would say

00:26:36,020 --> 00:26:40,680
execution engines on top of a hadoop

00:26:38,670 --> 00:26:43,170
they are doing the job differently right

00:26:40,680 --> 00:26:45,660
so what we didn't want to do is tie

00:26:43,170 --> 00:26:47,760
ourselves to MapReduce when you know a

00:26:45,660 --> 00:26:49,230
year from now the users can say hey you

00:26:47,760 --> 00:26:51,120
know scoop is the only thing that is

00:26:49,230 --> 00:26:52,830
using MapReduce why are you even border

00:26:51,120 --> 00:26:53,520
you know using it can you just rewrite

00:26:52,830 --> 00:26:55,800
it for me

00:26:53,520 --> 00:26:59,700
but that bollywood happy yeah if you're

00:26:55,800 --> 00:27:01,710
using so deeply so it's not so easy so

00:26:59,700 --> 00:27:04,020
that is the reason why if an abstract

00:27:01,710 --> 00:27:05,880
workflow and the application the

00:27:04,020 --> 00:27:07,020
execution layer is just as pluggable

00:27:05,880 --> 00:27:08,490
piece another plugin

00:27:07,020 --> 00:27:11,520
I can just say yeah run of MapReduce

00:27:08,490 --> 00:27:14,430
Reynolds part run on yarn random tasks

00:27:11,520 --> 00:27:18,510
and that's why we have the we have the

00:27:14,430 --> 00:27:20,850
separation so now we've transfer all the

00:27:18,510 --> 00:27:22,830
data to HDFS so we've succeeded all

00:27:20,850 --> 00:27:26,550
right the last piece that we want to

00:27:22,830 --> 00:27:31,670
describe is the client interface write

00:27:26,550 --> 00:27:34,320
or spend a couple of words there sorry

00:27:31,670 --> 00:27:37,230
the client as I've mentioned is very

00:27:34,320 --> 00:27:40,470
thin the server exposes a rest interface

00:27:37,230 --> 00:27:42,690
you will just say hey HTTP get me the

00:27:40,470 --> 00:27:45,420
connector data you will get back a JSON

00:27:42,690 --> 00:27:47,370
object representing the connector or hey

00:27:45,420 --> 00:27:50,280
submit a job you will do a post request

00:27:47,370 --> 00:27:53,370
to some URL and voila Joe will be spawn

00:27:50,280 --> 00:27:55,680
and that's it nothing more the client is

00:27:53,370 --> 00:27:57,570
not connecting to the database it's not

00:27:55,680 --> 00:28:00,630
connecting to the Map Reduce to the

00:27:57,570 --> 00:28:06,480
Hadoop all it does is just talk to you

00:28:00,630 --> 00:28:08,390
know HTTP port and we have two clients

00:28:06,480 --> 00:28:11,840
right now that are using this interface

00:28:08,390 --> 00:28:13,980
we have a built-in shell based on groovy

00:28:11,840 --> 00:28:16,110
variable you know type all the commands

00:28:13,980 --> 00:28:18,480
in the command line it's actually very

00:28:16,110 --> 00:28:20,370
similar to this group one you know by

00:28:18,480 --> 00:28:23,100
using different parameters you go change

00:28:20,370 --> 00:28:25,920
different objects and then we have a

00:28:23,100 --> 00:28:29,400
very very cool web interface in the view

00:28:25,920 --> 00:28:35,920
project that Abe will describe further

00:28:29,400 --> 00:28:38,950
thanks for it Seth so we have a mmm a

00:28:35,920 --> 00:28:41,700
scoop to you I in Hue so quick question

00:28:38,950 --> 00:28:45,160
has anyone here heard of you

00:28:41,700 --> 00:28:47,770
cool some people so to give a brief

00:28:45,160 --> 00:28:48,550
overview of hue hue is the Hadoop user

00:28:47,770 --> 00:28:52,210
experience

00:28:48,550 --> 00:28:54,970
it's a UI for Hadoop but really anything

00:28:52,210 --> 00:28:57,880
Big Data so it comes it comes with you

00:28:54,970 --> 00:29:00,550
is four components like hive Impala HDFS

00:28:57,880 --> 00:29:03,730
etc etc really anything big data it has

00:29:00,550 --> 00:29:05,500
a has a UI floor and so the goal of you

00:29:03,730 --> 00:29:08,290
is to provide an integrated experience

00:29:05,500 --> 00:29:09,820
and this project hue really wouldn't

00:29:08,290 --> 00:29:11,920
have been really wouldn't be complete

00:29:09,820 --> 00:29:13,840
without scoop if you think about it you

00:29:11,920 --> 00:29:15,430
need a way to get your data into Hadoop

00:29:13,840 --> 00:29:17,200
such that you can perform

00:29:15,430 --> 00:29:20,050
transformations and data computations

00:29:17,200 --> 00:29:23,200
etc etc so if you look at the screen

00:29:20,050 --> 00:29:27,790
you'll see basically the job listings

00:29:23,200 --> 00:29:29,200
page from hue of scoop to and so if you

00:29:27,790 --> 00:29:31,900
look at the top right you'll see manage

00:29:29,200 --> 00:29:33,880
connections and jobs essentially it

00:29:31,900 --> 00:29:37,830
allows you to create connections and

00:29:33,880 --> 00:29:40,570
create jobs pretty straightforward and

00:29:37,830 --> 00:29:43,120
this is what it currently looks like to

00:29:40,570 --> 00:29:44,890
create a job essentially you've defined

00:29:43,120 --> 00:29:46,950
from where you're getting your data and

00:29:44,890 --> 00:29:49,330
to where you want to put your data and

00:29:46,950 --> 00:29:51,220
as you can see there are a bunch of

00:29:49,330 --> 00:29:52,480
fields that you need to fill out it's

00:29:51,220 --> 00:29:55,120
essentially what you'd have to fill out

00:29:52,480 --> 00:29:56,680
for scoop to in the near future we

00:29:55,120 --> 00:29:58,840
intend on improving that we actually

00:29:56,680 --> 00:30:01,720
want to go one level up and make it

00:29:58,840 --> 00:30:02,980
really easy really simple who knows

00:30:01,720 --> 00:30:04,810
maybe all you have to do is a drag and

00:30:02,980 --> 00:30:10,450
drop what table you want to use who

00:30:04,810 --> 00:30:12,760
knows and so that would be the end of

00:30:10,450 --> 00:30:15,340
this presentation any questions comments

00:30:12,760 --> 00:30:17,830
or concerns if so please direct them to

00:30:15,340 --> 00:30:20,530
new york sets and i would be more than

00:30:17,830 --> 00:30:23,110
happy to answer them

00:30:20,530 --> 00:30:25,110
we have a demo so don't add us in the

00:30:23,110 --> 00:30:28,440
case that will be any further questions

00:30:25,110 --> 00:30:28,440
there is one

00:30:32,620 --> 00:30:39,460
so when you're moving the data from your

00:30:36,990 --> 00:30:41,860
relational database management systems

00:30:39,460 --> 00:30:45,120
into Hadoop how do you hold all these

00:30:41,860 --> 00:30:48,940
objects in memory do you do any special

00:30:45,120 --> 00:30:50,380
optimization on this so I don't have to

00:30:48,940 --> 00:30:55,390
repeat a question right it will be on

00:30:50,380 --> 00:30:57,820
the on the recording so the it actually

00:30:55,390 --> 00:31:01,059
depends on the connector it's up to the

00:30:57,820 --> 00:31:04,510
connector how to get the data from from

00:31:01,059 --> 00:31:07,210
database a couple of examples generic

00:31:04,510 --> 00:31:09,730
JDBC connector will open a JDBC

00:31:07,210 --> 00:31:13,559
connection will do select query to get

00:31:09,730 --> 00:31:16,240
the data right my sequel down for it or

00:31:13,559 --> 00:31:18,669
differently we have a special connector

00:31:16,240 --> 00:31:21,340
for my sequel a direct one that can take

00:31:18,669 --> 00:31:22,809
advantage of a Mexico dump utility that

00:31:21,340 --> 00:31:25,029
will you know running different process

00:31:22,809 --> 00:31:26,590
to get the data differently you can

00:31:25,029 --> 00:31:30,100
think about different approaches you

00:31:26,590 --> 00:31:32,950
know how to get how to get the data now

00:31:30,100 --> 00:31:40,230
on that picture or you can you actually

00:31:32,950 --> 00:31:42,789
scroll up on the work flow picture sorry

00:31:40,230 --> 00:31:44,679
you have you know different extractors

00:31:42,789 --> 00:31:46,029
and different loader phase right and the

00:31:44,679 --> 00:31:47,710
question has been how are you

00:31:46,029 --> 00:31:50,470
representing all the data into memory

00:31:47,710 --> 00:31:52,770
right so the first important thing is

00:31:50,470 --> 00:31:55,419
it's just trimming it's not that I am

00:31:52,770 --> 00:31:58,029
materializing the entire table in a

00:31:55,419 --> 00:32:00,640
memory on once in at the extractor phase

00:31:58,029 --> 00:32:03,820
then moving to do loader and storing it

00:32:00,640 --> 00:32:05,980
finally right I am just loading couple

00:32:03,820 --> 00:32:07,899
of roads some badge through the

00:32:05,980 --> 00:32:09,490
extractor passing it to the loader and

00:32:07,899 --> 00:32:12,610
finally putting it to the final

00:32:09,490 --> 00:32:15,130
destination the in-memory representation

00:32:12,610 --> 00:32:16,570
actually depends on the connector the

00:32:15,130 --> 00:32:18,909
connector is saying there's the

00:32:16,570 --> 00:32:21,630
representation that is most fat for me

00:32:18,909 --> 00:32:25,510
that's how I can interact in the most

00:32:21,630 --> 00:32:27,429
efficient speed for the MySQL dump it

00:32:25,510 --> 00:32:28,929
probably won't be text right because my

00:32:27,429 --> 00:32:31,240
see code and fill get text from the

00:32:28,929 --> 00:32:32,320
database some other connectors can use

00:32:31,240 --> 00:32:34,149
you know a different binary

00:32:32,320 --> 00:32:36,720
representation at this point I don't

00:32:34,149 --> 00:32:39,730
really care and it's up to the loader

00:32:36,720 --> 00:32:41,030
loader to actually sterilize the data

00:32:39,730 --> 00:32:43,520
based on what the user

00:32:41,030 --> 00:32:46,430
set I mean if I want to transfer data to

00:32:43,520 --> 00:32:49,310
text I will do the conversion there if I

00:32:46,430 --> 00:32:50,630
want to import data to HBase then I have

00:32:49,310 --> 00:32:52,610
to you know convert them to a put

00:32:50,630 --> 00:32:59,630
statements and put into each place

00:32:52,610 --> 00:33:01,790
did that answer the question so you're

00:32:59,630 --> 00:33:04,640
saying that everything you just get

00:33:01,790 --> 00:33:07,430
everything out as text and the u.s.

00:33:04,640 --> 00:33:09,500
really relying on the loaders to

00:33:07,430 --> 00:33:13,040
translate everything to whatever is

00:33:09,500 --> 00:33:16,220
needed now texts of one format very

00:33:13,040 --> 00:33:18,980
common for a you know at least fast

00:33:16,220 --> 00:33:22,280
export tools from a database perspective

00:33:18,980 --> 00:33:25,100
such as my sequel peach dam or a fast

00:33:22,280 --> 00:33:28,310
export on terror attack is the point I'm

00:33:25,100 --> 00:33:30,680
trying to make is that we as a developer

00:33:28,310 --> 00:33:32,600
for school really don't care it's up to

00:33:30,680 --> 00:33:36,290
the connector but it's his native

00:33:32,600 --> 00:33:38,780
interface so it can be bytes it can be I

00:33:36,290 --> 00:33:42,230
don't know Avro Orca it can be my own

00:33:38,780 --> 00:33:44,780
proprietary format all I need to do is

00:33:42,230 --> 00:33:46,190
just run the load Rufus to say Adric

00:33:44,780 --> 00:33:48,200
give me the row as a logical

00:33:46,190 --> 00:33:50,870
representation give me a text somehow

00:33:48,200 --> 00:33:52,910
get it in a format that I understand but

00:33:50,870 --> 00:33:55,070
as as long as the extract Rufus is

00:33:52,910 --> 00:34:00,250
concerned anything that you can work

00:33:55,070 --> 00:34:00,250
with I don't really care okay thank you

00:34:05,290 --> 00:34:11,450
does it support the incremental data

00:34:07,790 --> 00:34:14,720
transfers as well so definitely right we

00:34:11,450 --> 00:34:16,550
will add incremental support because

00:34:14,720 --> 00:34:19,220
that's one of the major use cases right

00:34:16,550 --> 00:34:22,040
I believe I've mention it when we

00:34:19,220 --> 00:34:24,020
firstly work on scoop it was like yeah

00:34:22,040 --> 00:34:25,220
we just need to get the data mahants we

00:34:24,020 --> 00:34:27,740
don't really care if you will type

00:34:25,220 --> 00:34:29,450
comment every fill and be fine but

00:34:27,740 --> 00:34:32,450
without what you've seen our users

00:34:29,450 --> 00:34:35,179
actually doing is really like hey I'm

00:34:32,450 --> 00:34:37,100
having an hourly partitions and as long

00:34:35,179 --> 00:34:38,540
as the hour is done I need to transfer

00:34:37,100 --> 00:34:40,700
the data because you know rest of my

00:34:38,540 --> 00:34:43,940
business depends on it so incremental

00:34:40,700 --> 00:34:45,800
enforced very very important and will be

00:34:43,940 --> 00:34:47,960
there right now

00:34:45,800 --> 00:34:51,450
the I will finish the question this

00:34:47,960 --> 00:34:53,760
group too is I would say very literally

00:34:51,450 --> 00:34:56,369
of the development so the incremental

00:34:53,760 --> 00:34:59,130
imports are not there but we will add

00:34:56,369 --> 00:35:02,790
them very shortly it's you know one of

00:34:59,130 --> 00:35:04,560
the major major features that depends on

00:35:02,790 --> 00:35:06,180
the partitions being available and they

00:35:04,560 --> 00:35:08,730
being available in different partitions

00:35:06,180 --> 00:35:11,760
but what happens to the data which keeps

00:35:08,730 --> 00:35:14,460
getting updated that to orden be taken

00:35:11,760 --> 00:35:19,020
care by incremental very very good

00:35:14,460 --> 00:35:21,540
question so you have a different place

00:35:19,020 --> 00:35:24,420
how to describe the incremental import

00:35:21,540 --> 00:35:25,950
right I will actually go into scoop one

00:35:24,420 --> 00:35:28,349
because that's where actually all the

00:35:25,950 --> 00:35:30,329
things are already working and in scoop

00:35:28,349 --> 00:35:33,780
one we had two ways how to define

00:35:30,329 --> 00:35:36,510
incremental import the first way is so

00:35:33,780 --> 00:35:40,109
called append mode you have some column

00:35:36,510 --> 00:35:42,960
primary ID you know incremental base

00:35:40,109 --> 00:35:45,570
plus one plus one and what group does it

00:35:42,960 --> 00:35:47,790
say hey what was the last I did it I've

00:35:45,570 --> 00:35:50,730
transferred that can be addressed by a

00:35:47,790 --> 00:35:52,230
user or store inside zone repository

00:35:50,730 --> 00:35:54,990
right that's why we have the first

00:35:52,230 --> 00:35:57,030
repository and every time you have a

00:35:54,990 --> 00:35:58,920
table that you will fill scoop will

00:35:57,030 --> 00:36:01,740
transpersonal data and as you will

00:35:58,920 --> 00:36:03,780
continue importing later the next time

00:36:01,740 --> 00:36:05,970
we will run through google realized here

00:36:03,780 --> 00:36:08,550
is when I end it and I will transfer the

00:36:05,970 --> 00:36:11,460
rest but this is assuming that the data

00:36:08,550 --> 00:36:13,950
are immutable right because if I have a

00:36:11,460 --> 00:36:15,599
table that I transfer here and hey by

00:36:13,950 --> 00:36:17,520
the way I've change this row it will

00:36:15,599 --> 00:36:19,710
never get imported again right a huge

00:36:17,520 --> 00:36:22,609
problem so the second way how to

00:36:19,710 --> 00:36:26,520
transfer data is so called last updated

00:36:22,609 --> 00:36:28,170
this group you know the magic really

00:36:26,520 --> 00:36:30,210
disappear on this world and I feel sorry

00:36:28,170 --> 00:36:32,790
for that but we need to have a way how

00:36:30,210 --> 00:36:35,819
to know what Rose has changed so the

00:36:32,790 --> 00:36:37,980
second way is to say hey keep updated

00:36:35,819 --> 00:36:39,750
some column let's call it last update

00:36:37,980 --> 00:36:41,970
date I don't really care about the name

00:36:39,750 --> 00:36:44,520
the seven things of that column is

00:36:41,970 --> 00:36:47,099
that's where I lastly changed at row and

00:36:44,520 --> 00:36:49,950
then I can run a query right last time I

00:36:47,099 --> 00:36:52,260
run today right now give me all rows

00:36:49,950 --> 00:36:55,619
that has been inserted or change after

00:36:52,260 --> 00:36:58,290
this time I will import into HDFS or

00:36:55,619 --> 00:37:00,119
into HBase then I can merge them the

00:36:58,290 --> 00:37:00,720
previous data set you know based on the

00:37:00,119 --> 00:37:03,630
primary key

00:37:00,720 --> 00:37:05,869
and finally end up in the final final

00:37:03,630 --> 00:37:10,550
state does that answer the question yes

00:37:05,869 --> 00:37:12,839
but at the destination or the target

00:37:10,550 --> 00:37:14,460
with the incremental data transfer it

00:37:12,839 --> 00:37:17,790
will just have the incremental data if

00:37:14,460 --> 00:37:18,930
you choose a different location every R

00:37:17,790 --> 00:37:22,530
or every day right

00:37:18,930 --> 00:37:24,569
is it possible to support a snapshot of

00:37:22,530 --> 00:37:26,220
the data on the I loop cluster but you

00:37:24,569 --> 00:37:30,810
would still end up transferring mingi

00:37:26,220 --> 00:37:34,530
only the incremental data so scoop right

00:37:30,810 --> 00:37:37,560
now will always work with the one

00:37:34,530 --> 00:37:39,540
directory for the target I can imagine

00:37:37,560 --> 00:37:41,730
that you can do a higher level logic on

00:37:39,540 --> 00:37:44,069
top of that using a scoop as you know

00:37:41,730 --> 00:37:45,750
the way of transferring data but right

00:37:44,069 --> 00:37:47,730
now the scoop doesn't have a built-in

00:37:45,750 --> 00:37:50,040
support for this particular use case I

00:37:47,730 --> 00:37:52,940
would however encourage you to go to

00:37:50,040 --> 00:37:56,180
Apache JIRA and file a request for it

00:37:52,940 --> 00:37:59,460
patches are welcome thank you very much

00:37:56,180 --> 00:38:01,680
why would you need a reducer for the

00:37:59,460 --> 00:38:06,569
Lord things I was hoping that someone

00:38:01,680 --> 00:38:09,150
will ask actually I don't

00:38:06,569 --> 00:38:11,310
and if you will just run the school it

00:38:09,150 --> 00:38:12,990
will be scoop on us or a scoop on me it

00:38:11,310 --> 00:38:16,950
will be map only joke we are not using

00:38:12,990 --> 00:38:19,829
reducers by default the reason why I do

00:38:16,950 --> 00:38:22,950
have the loader on the reducer side is

00:38:19,829 --> 00:38:25,619
that in scoop to you have the ability to

00:38:22,950 --> 00:38:28,440
for scoop to put the loader into a

00:38:25,619 --> 00:38:29,760
separate separate stage what is the

00:38:28,440 --> 00:38:33,960
reasoning for it and that is the

00:38:29,760 --> 00:38:35,460
question right well right now not a bit

00:38:33,960 --> 00:38:38,280
reasoning right because I'm just

00:38:35,460 --> 00:38:40,200
stirring it at HDFS when there will be

00:38:38,280 --> 00:38:41,819
an HBase you might want to limit the

00:38:40,200 --> 00:38:43,829
number of connections going to your

00:38:41,819 --> 00:38:45,420
HBase right and the number of

00:38:43,829 --> 00:38:46,470
connections for the database might be

00:38:45,420 --> 00:38:48,960
different right

00:38:46,470 --> 00:38:51,900
so now you need two stages if two

00:38:48,960 --> 00:38:54,420
different number of tasks and other use

00:38:51,900 --> 00:38:56,310
cases that in the after we will add the

00:38:54,420 --> 00:38:58,619
transformation phase that in this case

00:38:56,310 --> 00:39:01,099
would be really on the reducer side you

00:38:58,619 --> 00:39:04,290
want to limit the number of concurrent

00:39:01,099 --> 00:39:06,300
transformations right why you might be

00:39:04,290 --> 00:39:07,980
using some remote resources and I can

00:39:06,300 --> 00:39:10,740
you don't want to overload them

00:39:07,980 --> 00:39:13,220
so to summarize it right now it's map

00:39:10,740 --> 00:39:17,210
only job the reducer there is for a

00:39:13,220 --> 00:39:20,050
future I had a question on

00:39:17,210 --> 00:39:20,050
transformation

00:39:20,109 --> 00:39:28,700
so you're planning are you planning a

00:39:23,990 --> 00:39:33,550
way to convert from text or the

00:39:28,700 --> 00:39:36,380
proprietary DBE storage format into

00:39:33,550 --> 00:39:39,410
something like you know is your RC file

00:39:36,380 --> 00:39:41,119
something that sorry so I guess from

00:39:39,410 --> 00:39:44,710
this perspective you you can think about

00:39:41,119 --> 00:39:47,030
two different transformations data

00:39:44,710 --> 00:39:48,890
transformation like a formats right on

00:39:47,030 --> 00:39:51,619
the input I have some proprietary format

00:39:48,890 --> 00:39:53,330
on the output I have RC file Avro that's

00:39:51,619 --> 00:39:55,010
I would say Oh back to you as a user

00:39:53,330 --> 00:39:56,690
that's Morris coupe internal

00:39:55,010 --> 00:39:58,880
implementation right you will configure

00:39:56,690 --> 00:40:01,340
hey I want my data I said parkette file

00:39:58,880 --> 00:40:03,020
and coop will do that for you the

00:40:01,340 --> 00:40:06,410
transformation phase that we are talking

00:40:03,020 --> 00:40:10,640
about is more for you as a user right

00:40:06,410 --> 00:40:13,730
I'm thinking about an example imagine

00:40:10,640 --> 00:40:16,040
that you are having a date on the

00:40:13,730 --> 00:40:19,400
database and on the Hadoop you need to

00:40:16,040 --> 00:40:21,589
multiply them by 100 why I don't know

00:40:19,400 --> 00:40:23,420
you're storing the database number of

00:40:21,589 --> 00:40:24,800
seconds and you need to store number of

00:40:23,420 --> 00:40:26,900
milliseconds so we need to you know

00:40:24,800 --> 00:40:28,670
multiply by 100 those are the

00:40:26,900 --> 00:40:30,470
transformations

00:40:28,670 --> 00:40:32,330
you can ask hit all those

00:40:30,470 --> 00:40:34,339
transformations I can done them on the

00:40:32,330 --> 00:40:36,230
database I try they can change my query

00:40:34,339 --> 00:40:38,089
to say yeah give me that column multiply

00:40:36,230 --> 00:40:40,160
by 1 right right but then you were

00:40:38,089 --> 00:40:44,810
started thinking I have one database and

00:40:40,160 --> 00:40:47,180
I have one you know I'm 100 cluster but

00:40:44,810 --> 00:40:49,760
I have 16 cores on the head under on the

00:40:47,180 --> 00:40:52,510
database and 200 course on the had you

00:40:49,760 --> 00:40:54,589
right where you want to put the load

00:40:52,510 --> 00:40:59,060
similarly what we actually seen in the

00:40:54,589 --> 00:41:02,810
past some of the users we've seen are

00:40:59,060 --> 00:41:04,940
doing a shenanigans when selecting rows

00:41:02,810 --> 00:41:07,849
you know a better statement with five

00:41:04,940 --> 00:41:09,920
different rows doing a huge huge logic

00:41:07,849 --> 00:41:12,230
that is putting a lot of load in the

00:41:09,920 --> 00:41:14,720
database so another reason for the

00:41:12,230 --> 00:41:17,119
transformation phase is to do sort of a

00:41:14,720 --> 00:41:20,330
filtering where you can say this row I

00:41:17,119 --> 00:41:22,940
don't really want to import it another

00:41:20,330 --> 00:41:25,640
use case for the transformations in

00:41:22,940 --> 00:41:25,970
financial industry on the source you can

00:41:25,640 --> 00:41:27,770
have a

00:41:25,970 --> 00:41:29,480
credit card data right you can have the

00:41:27,770 --> 00:41:32,090
credit card number you might want to

00:41:29,480 --> 00:41:33,859
report it as a you know as the reason

00:41:32,090 --> 00:41:36,430
just keeping the last four digit or

00:41:33,859 --> 00:41:36,430
something like that

00:41:41,080 --> 00:41:49,360
so I have a two question the first one

00:41:46,110 --> 00:41:55,420
regarding the export from the database

00:41:49,360 --> 00:41:58,780
tables to what happens in case if some

00:41:55,420 --> 00:42:01,570
tables related one with others or have

00:41:58,780 --> 00:42:04,810
one too many or many too many relations

00:42:01,570 --> 00:42:08,590
with foreign keys to do have some

00:42:04,810 --> 00:42:14,160
semantics to applies it or so connector

00:42:08,590 --> 00:42:14,160
just export a table independently

00:42:14,370 --> 00:42:21,250
excellent question it actually depends

00:42:17,790 --> 00:42:23,560
in the most naive case you can specify

00:42:21,250 --> 00:42:26,110
let's say that you have you know a fact

00:42:23,560 --> 00:42:27,430
sorry it's called fact table and to

00:42:26,110 --> 00:42:29,770
dictionary tables right that's what

00:42:27,430 --> 00:42:31,570
you're describing in the most naive case

00:42:29,770 --> 00:42:34,330
you can say hey import three tables

00:42:31,570 --> 00:42:35,830
enjoy them on Hadoop later depend you

00:42:34,330 --> 00:42:40,660
might want to need to do it you might

00:42:35,830 --> 00:42:42,340
not or you can join them in scooped I

00:42:40,660 --> 00:42:44,410
know that we were just talking about an

00:42:42,340 --> 00:42:46,660
example I have a table but that's just

00:42:44,410 --> 00:42:49,270
you know most I would say easily use

00:42:46,660 --> 00:42:51,820
case in Tsukuba case you can say hey

00:42:49,270 --> 00:42:54,040
here is my query a result set that I

00:42:51,820 --> 00:42:56,470
want to import and in that query you can

00:42:54,040 --> 00:42:59,140
specify any arbitrary joint and the

00:42:56,470 --> 00:43:02,320
query I can configure some somehow in

00:42:59,140 --> 00:43:04,090
the connectors yes what's the question

00:43:02,320 --> 00:43:07,450
yeah then again it depends on the

00:43:04,090 --> 00:43:09,460
connector what exactly is supported the

00:43:07,450 --> 00:43:12,010
generic JDBC connector allows you to

00:43:09,460 --> 00:43:15,840
submit any arbitrary query and the

00:43:12,010 --> 00:43:20,830
second question does a curious so you

00:43:15,840 --> 00:43:23,410
talked about SQL and no SQL database if

00:43:20,830 --> 00:43:27,040
I have my date somewhere in messaging

00:43:23,410 --> 00:43:30,820
system or in FTP is it any plans to

00:43:27,040 --> 00:43:33,280
supports it so I just think in perhaps a

00:43:30,820 --> 00:43:35,680
camel could be a good way so to

00:43:33,280 --> 00:43:37,240
implement connector that is a second

00:43:35,680 --> 00:43:40,240
question that I hope somebody will I

00:43:37,240 --> 00:43:43,600
will ask me thank you very much so in

00:43:40,240 --> 00:43:46,360
scoop one we were basically forcing

00:43:43,600 --> 00:43:48,220
everyone to follow the JDBC model we

00:43:46,360 --> 00:43:50,350
were expecting and every connector will

00:43:48,220 --> 00:43:52,400
know about JDBC connection about

00:43:50,350 --> 00:43:54,020
username password blah blah blah

00:43:52,400 --> 00:43:56,450
and we actually had problem of that

00:43:54,020 --> 00:43:59,150
guess what CouchDB doesn't have the

00:43:56,450 --> 00:44:01,400
concept of table so you know we forced

00:43:59,150 --> 00:44:04,099
them hey give me a table parameter even

00:44:01,400 --> 00:44:05,660
though he knows that's why he did a lot

00:44:04,099 --> 00:44:07,039
of to change that I talked about when

00:44:05,660 --> 00:44:10,010
the parameters are exposed by the

00:44:07,039 --> 00:44:13,039
connectors and blah blah blah the

00:44:10,010 --> 00:44:15,770
connect or the connected themselves they

00:44:13,039 --> 00:44:17,779
can you know get post anything and

00:44:15,770 --> 00:44:21,109
transfer data from anywhere we don't

00:44:17,779 --> 00:44:24,049
really care the reason why we imported

00:44:21,109 --> 00:44:25,700
or design Aidid this way that we are

00:44:24,049 --> 00:44:29,270
actually planning to add and

00:44:25,700 --> 00:44:32,559
non-relational sources for example

00:44:29,270 --> 00:44:35,359
source for a mainframe or source for FTP

00:44:32,559 --> 00:44:38,660
or source for yeah I don't know anything

00:44:35,359 --> 00:44:41,150
else so yes you can do it there is one

00:44:38,660 --> 00:44:43,970
thing that I actually want to add scoop

00:44:41,150 --> 00:44:46,700
in the nature is a batch system so

00:44:43,970 --> 00:44:49,460
loading data from FTP very good use case

00:44:46,700 --> 00:44:52,819
loading latest order mainframe very good

00:44:49,460 --> 00:44:54,829
use case loading data from JMS not so

00:44:52,819 --> 00:44:56,990
much in as a real time you know queuing

00:44:54,829 --> 00:44:59,270
system right but guess what we have a

00:44:56,990 --> 00:45:01,880
different component called flow that is

00:44:59,270 --> 00:45:03,950
meant for a real-time our data access

00:45:01,880 --> 00:45:07,250
and actually I can promise you that

00:45:03,950 --> 00:45:10,420
right now the flume does support JMS so

00:45:07,250 --> 00:45:10,420
it's just a different use case

00:45:12,310 --> 00:45:18,710
what you might ask the question to the

00:45:14,480 --> 00:45:20,990
microphone so do you think in order to

00:45:18,710 --> 00:45:25,120
reuse some connector from camera for

00:45:20,990 --> 00:45:28,910
example you have a from we have already

00:45:25,120 --> 00:45:32,260
Big Bend of the connectors basically

00:45:28,910 --> 00:45:34,460
it's not necessary to the right everyone

00:45:32,260 --> 00:45:37,400
I'm not sure that I've understood the

00:45:34,460 --> 00:45:40,430
question we use camera projects you use

00:45:37,400 --> 00:45:41,930
a Pecha camel as a connector I'm not

00:45:40,430 --> 00:45:46,280
familiar with camera so I can't really

00:45:41,930 --> 00:45:48,050
comment I'm sorry but if it's just some

00:45:46,280 --> 00:45:50,270
sort of storage you know we I'm sure

00:45:48,050 --> 00:45:52,690
that we can create a connector for it or

00:45:50,270 --> 00:45:55,160
actually you can if you want

00:45:52,690 --> 00:45:57,620
how will you limit the number of

00:45:55,160 --> 00:46:00,590
connections from extractor to the DB

00:45:57,620 --> 00:46:02,720
there are 200 extractors all of them

00:46:00,590 --> 00:46:09,350
make simultaneous connections then the

00:46:02,720 --> 00:46:11,960
database will die right so it's I

00:46:09,350 --> 00:46:13,940
usually user responsibility one of the

00:46:11,960 --> 00:46:16,190
reasons why we've introduced a central

00:46:13,940 --> 00:46:20,150
component cook to server is to do

00:46:16,190 --> 00:46:22,280
resource management in scoop one you

00:46:20,150 --> 00:46:24,050
know you had the ability to say hey use

00:46:22,280 --> 00:46:25,460
only 50 connection we use only 10

00:46:24,050 --> 00:46:27,620
connection use only five connections

00:46:25,460 --> 00:46:28,640
that's cool one has been a command-line

00:46:27,620 --> 00:46:31,190
utility right

00:46:28,640 --> 00:46:33,800
I have user bear I have used it there so

00:46:31,190 --> 00:46:36,470
you know an example it actually happened

00:46:33,800 --> 00:46:39,920
to me I knew that I can run in order to

00:46:36,470 --> 00:46:42,230
run a data transfer and not put a lot of

00:46:39,920 --> 00:46:44,600
burden on the database I knew that I can

00:46:42,230 --> 00:46:46,730
run ten connections at the same time

00:46:44,600 --> 00:46:48,680
transfer data protection traffic and

00:46:46,730 --> 00:46:50,990
effective so you know one day I've

00:46:48,680 --> 00:46:52,970
decided to do it transport the data but

00:46:50,990 --> 00:46:54,650
cat was my friend from Russia had

00:46:52,970 --> 00:46:57,860
exactly the same idea at the same time

00:46:54,650 --> 00:46:59,780
we had 20 connections at the same time

00:46:57,860 --> 00:47:03,170
and the production dropping has been

00:46:59,780 --> 00:47:05,420
affected so in scoop to when you're

00:47:03,170 --> 00:47:08,350
creating the connection object part of

00:47:05,420 --> 00:47:10,910
that is to say I want to allow maximally

00:47:08,350 --> 00:47:13,790
20 connections on this data base and

00:47:10,910 --> 00:47:16,670
then scoop server can do the resource

00:47:13,790 --> 00:47:19,490
management can say hey you want to start

00:47:16,670 --> 00:47:21,130
a job with 10 map 10 connections I'm

00:47:19,490 --> 00:47:24,900
sorry I cannot really

00:47:21,130 --> 00:47:24,900
because I don't have them available

00:47:33,279 --> 00:47:45,750
is this cook just for shot JDBC or are

00:47:40,210 --> 00:47:49,779
there any initiatives to to able to

00:47:45,750 --> 00:47:56,170
MongoDB for example or another that data

00:47:49,779 --> 00:47:58,210
source so scoop 2 is open to anything we

00:47:56,170 --> 00:48:01,210
made a lot of changes to allow basically

00:47:58,210 --> 00:48:03,940
any arbitrary source in Mora batch mode

00:48:01,210 --> 00:48:07,210
scope is not really a real-time written

00:48:03,940 --> 00:48:10,960
real-time tool specifically for the

00:48:07,210 --> 00:48:12,670
MongoDB I know we have a connector for

00:48:10,960 --> 00:48:16,059
CouchDB we don't have a connector for

00:48:12,670 --> 00:48:24,450
MongoDB yet but the cost of writing one

00:48:16,059 --> 00:48:24,450
is relatively cheap any other questions

00:48:28,140 --> 00:48:32,730
we're interested using patchy kafka as

00:48:31,170 --> 00:48:34,740
our message boss I'm just wondering if

00:48:32,730 --> 00:48:37,530
you guys have any kind of ideas with

00:48:34,740 --> 00:48:39,690
regards to implementing a connector of

00:48:37,530 --> 00:48:40,950
sorts for Apache Kafka it will be more

00:48:39,690 --> 00:48:43,740
along the lines of loom but at the same

00:48:40,950 --> 00:48:45,720
time Apache Kafka could potentially be

00:48:43,740 --> 00:48:47,580
storing weeks worth of logs you would

00:48:45,720 --> 00:48:50,550
want to initially import for handing off

00:48:47,580 --> 00:48:53,490
the flume for the real-time stuff so my

00:48:50,550 --> 00:48:57,540
personal thoughts just me personally I

00:48:53,490 --> 00:48:59,970
do see a Kafka in the more real-time use

00:48:57,540 --> 00:49:02,310
so you know I don't see immediately

00:48:59,970 --> 00:49:03,480
creating a scope connector for Kafka but

00:49:02,310 --> 00:49:05,400
you're correct right you can be

00:49:03,480 --> 00:49:07,140
representing their weeks of data and you

00:49:05,400 --> 00:49:11,430
might want to pull them at the same time

00:49:07,140 --> 00:49:15,540
so yes I can imagine such connector am i

00:49:11,430 --> 00:49:18,380
correctly working on one no I think we

00:49:15,540 --> 00:49:18,380
can take one last question

00:49:21,690 --> 00:49:27,890
okay thank you very much for for being

00:49:24,359 --> 00:49:27,890

YouTube URL: https://www.youtube.com/watch?v=hg683-GOWP4


