Title: Apache Hadoop YARN  The Next generation Distributed Operating System - Zhijie Shen and Jian He
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,030 --> 00:00:03,270
hello everyone sing for all coming to

00:00:02,159 --> 00:00:06,240
attend our talk

00:00:03,270 --> 00:00:09,480
my name is association this is my

00:00:06,240 --> 00:00:10,230
colleague urging her we are working for

00:00:09,480 --> 00:00:14,580
Hortonworks

00:00:10,230 --> 00:00:19,020
and we are the committee and had in the

00:00:14,580 --> 00:00:21,260
article from the foundation and today we

00:00:19,020 --> 00:00:24,510
are going to give a general talk about

00:00:21,260 --> 00:00:28,189
Apache Hadoop young the next generation

00:00:24,510 --> 00:00:31,289
operating systems for general

00:00:28,189 --> 00:00:33,480
distributions and we are coming here

00:00:31,289 --> 00:00:36,600
because we are actually working on young

00:00:33,480 --> 00:00:41,489
so we are come to speak the cool stuff

00:00:36,600 --> 00:00:43,980
about young today our talk is going to

00:00:41,489 --> 00:00:46,230
cover the following four aspects first

00:00:43,980 --> 00:00:48,899
I'm going to give a brief introduction

00:00:46,230 --> 00:00:52,980
about the background and the motivation

00:00:48,899 --> 00:00:55,370
of our young and next I will go through

00:00:52,980 --> 00:00:58,020
some basic concepts about young and

00:00:55,370 --> 00:01:01,079
afterwards my colleague urchin is going

00:00:58,020 --> 00:01:03,270
to introduce you guys about the recent

00:01:01,079 --> 00:01:05,850
development in the English project and

00:01:03,270 --> 00:01:11,790
also introduce how to write a young

00:01:05,850 --> 00:01:15,090
application so let's go to the first

00:01:11,790 --> 00:01:17,640
part these are the two questions that I

00:01:15,090 --> 00:01:20,490
have a birth observed on the user

00:01:17,640 --> 00:01:23,460
mailing list of Hadoop frequently why is

00:01:20,490 --> 00:01:27,000
a what is Hadoop to the RT is what is

00:01:23,460 --> 00:01:30,150
young so the simple answer is that

00:01:27,000 --> 00:01:32,220
Hadoop 2 is young and it is the

00:01:30,150 --> 00:01:34,579
operating system to run various

00:01:32,220 --> 00:01:38,070
distributed data processing applications

00:01:34,579 --> 00:01:41,340
so I think a lot of people here have

00:01:38,070 --> 00:01:45,840
tried them Hadoop one before he'll run

00:01:41,340 --> 00:01:47,970
MapReduce jobs before before or we you

00:01:45,840 --> 00:01:50,009
can well think the MapReduce have

00:01:47,970 --> 00:01:53,329
already serve the people there well why

00:01:50,009 --> 00:01:55,860
do is do need to have young in this

00:01:53,329 --> 00:01:57,420
situation is that the word has been

00:01:55,860 --> 00:02:00,420
changed a lot

00:01:57,420 --> 00:02:03,390
Hadoop is invented at 8 years ago but

00:02:00,420 --> 00:02:06,270
now the hardware has been upgraded it is

00:02:03,390 --> 00:02:08,879
much powerful than before and the most

00:02:06,270 --> 00:02:11,610
important thing is that more and more

00:02:08,879 --> 00:02:12,400
companies and the Institute has adopted

00:02:11,610 --> 00:02:15,849
had to

00:02:12,400 --> 00:02:18,400
as they processing platform and they

00:02:15,849 --> 00:02:21,310
have different amount for example they

00:02:18,400 --> 00:02:23,739
want to use the interactive date

00:02:21,310 --> 00:02:26,980
processing model and they won't used

00:02:23,739 --> 00:02:29,530
streaming data processing models well if

00:02:26,980 --> 00:02:32,650
they will have this kind of requirements

00:02:29,530 --> 00:02:37,090
the traditional MapReduce job which is

00:02:32,650 --> 00:02:44,709
bad best for the batch job execution is

00:02:37,090 --> 00:02:47,250
not good in this case anymore so the the

00:02:44,709 --> 00:02:50,049
first way and are the most important

00:02:47,250 --> 00:02:53,560
motivations for our first to England

00:02:50,049 --> 00:02:56,909
young is to enable these processing

00:02:53,560 --> 00:03:02,250
models other than MapReduce you can see

00:02:56,909 --> 00:03:06,069
in the following diagram the Hadoop one

00:03:02,250 --> 00:03:09,220
architecture is a bit simple the HDFS is

00:03:06,069 --> 00:03:12,879
at the bottom on top of yet it is a

00:03:09,220 --> 00:03:16,870
MapReduce in the pig in hive are two

00:03:12,879 --> 00:03:19,840
high-level secret like they processing

00:03:16,870 --> 00:03:23,410
language on top of it but when we

00:03:19,840 --> 00:03:26,669
migrate to Hadoop 2 and as the Hadoop

00:03:23,410 --> 00:03:30,220
stack has been grown a lot

00:03:26,669 --> 00:03:32,470
hdhdhd FSS to the important layer but on

00:03:30,220 --> 00:03:39,549
top of it it's no longer a MapReduce

00:03:32,470 --> 00:03:43,829
instead it's a young hello the system

00:03:39,549 --> 00:03:46,239
can run different kinds of applications

00:03:43,829 --> 00:03:49,120
MapReduce is still one computation

00:03:46,239 --> 00:03:51,879
framework that can be run on young but

00:03:49,120 --> 00:03:55,720
in addition to that we can support the

00:03:51,879 --> 00:03:57,699
real time today process applications the

00:03:55,720 --> 00:04:00,540
streaming data process applications the

00:03:57,699 --> 00:04:03,400
graph process applications as well as

00:04:00,540 --> 00:04:09,760
some some kinds of long-running service

00:04:03,400 --> 00:04:13,569
such as HBase and actually our colleague

00:04:09,760 --> 00:04:16,739
Ted I'm not sure whether he is here he

00:04:13,569 --> 00:04:19,810
is going to give a separate talk about

00:04:16,739 --> 00:04:21,609
HBase I'm young if you are interest is

00:04:19,810 --> 00:04:25,169
interested in this part you can pay

00:04:21,609 --> 00:04:25,169
attention to that talk as well

00:04:25,920 --> 00:04:32,080
the second motivation is that we have

00:04:29,410 --> 00:04:36,430
upward observer some drawbacks in that

00:04:32,080 --> 00:04:39,940
the old MapReduce if you know a

00:04:36,430 --> 00:04:42,520
MapReduce well you probably know the job

00:04:39,940 --> 00:04:44,830
tracker is taking a lot of workload it

00:04:42,520 --> 00:04:48,280
is doing the resource management

00:04:44,830 --> 00:04:50,860
it is also managing the jobs life cycles

00:04:48,280 --> 00:04:54,940
and the more returning them so given we

00:04:50,860 --> 00:04:58,120
have a big cluster say we have one in

00:04:54,940 --> 00:05:00,160
some applications tensile applications

00:04:58,120 --> 00:05:03,820
or even more applications running on

00:05:00,160 --> 00:05:07,600
this cluster that the job tracker is

00:05:03,820 --> 00:05:09,700
going to tackling so many things such

00:05:07,600 --> 00:05:13,270
that it is going to be the bottleneck

00:05:09,700 --> 00:05:15,970
for the whole class to scale the second

00:05:13,270 --> 00:05:19,270
things which are back to that we find is

00:05:15,970 --> 00:05:21,640
the efficiency the the cut of the

00:05:19,270 --> 00:05:25,390
traditional MapReduce class is always

00:05:21,640 --> 00:05:28,240
underutilized this is because a host is

00:05:25,390 --> 00:05:32,110
always configured with some fixed map

00:05:28,240 --> 00:05:35,860
and the reduce resource slots so if a

00:05:32,110 --> 00:05:39,310
Map Reduce job is at the mapper phase

00:05:35,860 --> 00:05:42,550
they'll reduce slot is going to be

00:05:39,310 --> 00:05:46,120
wasted and when the MapReduce job

00:05:42,550 --> 00:05:48,100
reached reduced phase in the map mappers

00:05:46,120 --> 00:05:51,540
loss is going to be wasted

00:05:48,100 --> 00:05:55,690
so the idea things is that we have some

00:05:51,540 --> 00:05:58,060
general resource loss such that it is

00:05:55,690 --> 00:06:01,030
not associated to map her is not

00:05:58,060 --> 00:06:05,260
associated to reducer well when a mapper

00:06:01,030 --> 00:06:08,350
over reduce tasks want to be execute it

00:06:05,260 --> 00:06:14,020
can take one slot on the Hausa tool to

00:06:08,350 --> 00:06:17,740
do what is it want to do so the last but

00:06:14,020 --> 00:06:20,830
not least is a resource shearing as I

00:06:17,740 --> 00:06:23,320
mentioned young is going to run

00:06:20,830 --> 00:06:24,940
different kind of applications and the

00:06:23,320 --> 00:06:28,350
different applications can be submitted

00:06:24,940 --> 00:06:32,040
by different users different groups

00:06:28,350 --> 00:06:35,080
departments are even companies and each

00:06:32,040 --> 00:06:37,860
holder want to at least a guest some

00:06:35,080 --> 00:06:40,919
sheer of the risk resource to run

00:06:37,860 --> 00:06:43,490
their application and another

00:06:40,919 --> 00:06:46,050
requirement is that different

00:06:43,490 --> 00:06:49,349
stakeholders don't want to be disturbed

00:06:46,050 --> 00:06:52,169
by each other's so we were like we want

00:06:49,349 --> 00:06:56,490
to have some kind of mechanisms to

00:06:52,169 --> 00:06:58,979
ensure that one at mistake holder can

00:06:56,490 --> 00:07:03,000
get a share of the cluster and they also

00:06:58,979 --> 00:07:05,479
ensured different stakeholders are

00:07:03,000 --> 00:07:10,520
isolated and then we are not being

00:07:05,479 --> 00:07:15,539
affected each other so given these

00:07:10,520 --> 00:07:18,569
motivations we have a spin of the job

00:07:15,539 --> 00:07:21,599
tracker in a task a tracker logic out of

00:07:18,569 --> 00:07:25,199
our MapReduce and now the MapReduce is

00:07:21,599 --> 00:07:27,599
complete a computation framework and

00:07:25,199 --> 00:07:30,210
though we have a generalized the job

00:07:27,599 --> 00:07:33,599
tracker and task of tracking logic to be

00:07:30,210 --> 00:07:36,150
as general as possible to support all

00:07:33,599 --> 00:07:39,870
kinds of applications and as well as

00:07:36,150 --> 00:07:42,629
long-running services fortunately in in

00:07:39,870 --> 00:07:45,629
the second half of last year we have a

00:07:42,629 --> 00:07:48,599
release our first stable version of

00:07:45,629 --> 00:07:51,810
apertium but we didn't just stop there

00:07:48,599 --> 00:07:55,919
we are do working hard to make this

00:07:51,810 --> 00:07:58,349
project as bad as possible and in this

00:07:55,919 --> 00:08:00,810
month the Hadoop $2 for is going to be

00:07:58,349 --> 00:08:03,440
released there will be the following

00:08:00,810 --> 00:08:07,289
cool features that come with this

00:08:03,440 --> 00:08:09,839
release resource manager high

00:08:07,289 --> 00:08:12,240
availability application history

00:08:09,839 --> 00:08:18,569
historic data services and the long

00:08:12,240 --> 00:08:22,139
running application optimization here I

00:08:18,569 --> 00:08:27,300
will also like to mention some young

00:08:22,139 --> 00:08:30,029
ecosystems MapReduce is for sure - able

00:08:27,300 --> 00:08:33,240
to run on young but in addition to that

00:08:30,029 --> 00:08:35,610
in the Apache foundation we have already

00:08:33,240 --> 00:08:39,000
a sort of other application that can run

00:08:35,610 --> 00:08:42,000
young directly one as a project as it is

00:08:39,000 --> 00:08:45,540
either a batch or interact here with job

00:08:42,000 --> 00:08:49,160
is cuter another one in the storm it is

00:08:45,540 --> 00:08:51,240
user to processing streaming data and

00:08:49,160 --> 00:08:54,839
Apache giraffe to

00:08:51,240 --> 00:08:59,430
process graph spark to do the iterative

00:08:54,839 --> 00:09:06,779
application and each base which is going

00:08:59,430 --> 00:09:09,360
to run as a long-running service next I

00:09:06,779 --> 00:09:13,170
would like to introduce something about

00:09:09,360 --> 00:09:16,050
our young community actually Jana is a

00:09:13,170 --> 00:09:19,920
part of Hadoop Hadoop is a very big

00:09:16,050 --> 00:09:24,660
community we have a tens of PMC's tens

00:09:19,920 --> 00:09:28,080
of commuters but among these commuters

00:09:24,660 --> 00:09:31,470
we have a Templars dedicated community

00:09:28,080 --> 00:09:33,600
walking on the young project and in

00:09:31,470 --> 00:09:35,970
addition to that we have more than

00:09:33,600 --> 00:09:39,149
twenty contributors who working on this

00:09:35,970 --> 00:09:44,880
project as well in the last two years we

00:09:39,149 --> 00:09:48,180
have already fired around 1800 plus your

00:09:44,880 --> 00:09:50,250
ticket and among these dirty kids we

00:09:48,180 --> 00:09:53,610
have already resolved or closed or

00:09:50,250 --> 00:09:56,370
eleven hundred plus so it's a really

00:09:53,610 --> 00:10:00,870
amazing achievement in the past years in

00:09:56,370 --> 00:10:03,620
this thank you all the contributors to

00:10:00,870 --> 00:10:03,620
this project

00:10:06,140 --> 00:10:13,110
ok next I were like to go through some

00:10:09,390 --> 00:10:15,450
basic basic stuffs about young to make

00:10:13,110 --> 00:10:22,350
sure you have some knowledge about how

00:10:15,450 --> 00:10:26,910
it what it is and how it's run there are

00:10:22,350 --> 00:10:30,390
four basic concept or a young young is

00:10:26,910 --> 00:10:34,050
actually running as master slave

00:10:30,390 --> 00:10:36,890
paradigm resource manager is the mass of

00:10:34,050 --> 00:10:40,260
the whole cluster it has a control has a

00:10:36,890 --> 00:10:44,190
of all the resources in this cluster and

00:10:40,260 --> 00:10:47,760
it is also going to be the main access

00:10:44,190 --> 00:10:51,149
point for all the other components no

00:10:47,760 --> 00:10:54,810
the manager is the slave and it is going

00:10:51,149 --> 00:10:57,570
to take care of a specific host to

00:10:54,810 --> 00:11:01,440
monitoring the containers that running

00:10:57,570 --> 00:11:04,310
on this host and also monitoring this

00:11:01,440 --> 00:11:07,440
resource on this host

00:11:04,310 --> 00:11:10,470
application master is actually a user

00:11:07,440 --> 00:11:14,490
land stuff but it is also an important

00:11:10,470 --> 00:11:17,100
part of young user are free to implement

00:11:14,490 --> 00:11:20,190
the application master logic but

00:11:17,100 --> 00:11:22,470
basically it is it is required to

00:11:20,190 --> 00:11:26,250
contact resource manager and another

00:11:22,470 --> 00:11:28,890
manager to get the resource in the

00:11:26,250 --> 00:11:32,310
cluster and you started a container to

00:11:28,890 --> 00:11:35,840
run the application code also the user

00:11:32,310 --> 00:11:41,220
is free to implement other logic to

00:11:35,840 --> 00:11:43,710
monitoring the the progress of the its

00:11:41,220 --> 00:11:47,640
application and the respond to that

00:11:43,710 --> 00:11:52,170
client about the application status and

00:11:47,640 --> 00:11:55,050
there's a last important concept is

00:11:52,170 --> 00:11:57,330
container we can explain to the

00:11:55,050 --> 00:12:02,550
container in two ways the first thing is

00:11:57,330 --> 00:12:05,430
that it's a abstraction or resource we

00:12:02,550 --> 00:12:10,580
can consider it as a bunch of resource

00:12:05,430 --> 00:12:14,790
such as some mount of memory a number of

00:12:10,580 --> 00:12:19,110
CPU cores and in the future we might

00:12:14,790 --> 00:12:22,620
consider this the secondary storage into

00:12:19,110 --> 00:12:25,100
the resource as well and from the other

00:12:22,620 --> 00:12:28,140
side we can consider the container as a

00:12:25,100 --> 00:12:34,850
process that is running some specific

00:12:28,140 --> 00:12:38,070
application code after introducing the

00:12:34,850 --> 00:12:40,980
four basic concepts I would like to go a

00:12:38,070 --> 00:12:44,250
bit deeper about a resource manager the

00:12:40,980 --> 00:12:48,570
resource manager actually have four

00:12:44,250 --> 00:12:51,000
major parts the core part is about a

00:12:48,570 --> 00:12:54,150
resource manager is to manage managing

00:12:51,000 --> 00:12:56,730
the lifecycle of applications but

00:12:54,150 --> 00:12:59,640
another important thing it is doing it

00:12:56,730 --> 00:13:02,940
to scheduler the resource of the cluster

00:12:59,640 --> 00:13:05,880
and doing the correct allocation to the

00:13:02,940 --> 00:13:10,230
each individual applications in addition

00:13:05,880 --> 00:13:13,380
to that there are three interfacing part

00:13:10,230 --> 00:13:16,830
one of them is a client I'm service the

00:13:13,380 --> 00:13:17,960
client arm service is a poser to talk to

00:13:16,830 --> 00:13:22,520
a client

00:13:17,960 --> 00:13:24,830
the client can submit application cure

00:13:22,520 --> 00:13:28,120
application and the get application

00:13:24,830 --> 00:13:29,870
status and then you get class or status

00:13:28,120 --> 00:13:32,360
weirdest service

00:13:29,870 --> 00:13:35,540
another one is application master

00:13:32,360 --> 00:13:38,360
service this this interfacing part is

00:13:35,540 --> 00:13:41,120
used to communicate with application

00:13:38,360 --> 00:13:44,540
master that when the application master

00:13:41,120 --> 00:13:47,330
is a started it can do the registration

00:13:44,540 --> 00:13:50,000
in the unread restoration with resource

00:13:47,330 --> 00:13:52,970
manager in that some in deserves our

00:13:50,000 --> 00:13:54,890
nothing important thing is to accept as

00:13:52,970 --> 00:13:59,300
a resource to request a from the

00:13:54,890 --> 00:14:03,140
application master mmm the last part is

00:13:59,300 --> 00:14:06,430
a resource tracker service this part is

00:14:03,140 --> 00:14:09,520
a monitoring is listening to the

00:14:06,430 --> 00:14:13,550
heartbeat from the node man node manager

00:14:09,520 --> 00:14:16,370
non-manager is supposed to contact a

00:14:13,550 --> 00:14:19,550
resource manager at regular interval to

00:14:16,370 --> 00:14:23,600
update the host information with

00:14:19,550 --> 00:14:26,960
resource manager I will talk about the

00:14:23,600 --> 00:14:31,430
details next about the node manager in

00:14:26,960 --> 00:14:35,090
an in a node manager the components a

00:14:31,430 --> 00:14:37,910
bit simpler it has three major parts the

00:14:35,090 --> 00:14:40,820
core part is to managing the

00:14:37,910 --> 00:14:44,390
applications that has running containers

00:14:40,820 --> 00:14:46,940
on this of this specific host and the

00:14:44,390 --> 00:14:49,820
and also many during all the running

00:14:46,940 --> 00:14:52,820
containers on this host in addition to

00:14:49,820 --> 00:14:55,490
that there are two other parts one is

00:14:52,820 --> 00:14:59,570
the container as cuter the container as

00:14:55,490 --> 00:15:03,380
cuter is talking to the opera assistant

00:14:59,570 --> 00:15:06,800
directly to start the process to execute

00:15:03,380 --> 00:15:09,500
the exact application code in the

00:15:06,800 --> 00:15:12,340
another whoo the other one is and know

00:15:09,500 --> 00:15:16,400
the status update her this is a one that

00:15:12,340 --> 00:15:18,710
node manager used to talk to you

00:15:16,400 --> 00:15:21,860
resource manager to a legend resource

00:15:18,710 --> 00:15:23,780
manager know what containers is running

00:15:21,860 --> 00:15:25,970
on this coaster and what container has

00:15:23,780 --> 00:15:28,360
been finished and whether this house is

00:15:25,970 --> 00:15:30,670
as still healthy still have them

00:15:28,360 --> 00:15:33,760
available

00:15:30,670 --> 00:15:40,570
spare discus bases such a kind of stuff

00:15:33,760 --> 00:15:42,880
so given you know we have introduced the

00:15:40,570 --> 00:15:45,400
resource manager know the manager and

00:15:42,880 --> 00:15:47,080
application I would like to grow through

00:15:45,400 --> 00:15:51,490
a typical workflow of a young

00:15:47,080 --> 00:15:54,160
application first the young application

00:15:51,490 --> 00:15:56,650
is submitted from client to the resource

00:15:54,160 --> 00:15:59,950
manager when the resource manager

00:15:56,650 --> 00:16:03,990
received that application it is going to

00:15:59,950 --> 00:16:07,990
start a specific container to start the

00:16:03,990 --> 00:16:11,410
application master after the application

00:16:07,990 --> 00:16:14,410
master is successfully started it is

00:16:11,410 --> 00:16:17,740
going to do registration or against

00:16:14,410 --> 00:16:19,570
resource manager this is a step 2 letter

00:16:17,740 --> 00:16:21,690
the resource manager know the

00:16:19,570 --> 00:16:25,720
application master has been successfully

00:16:21,690 --> 00:16:29,410
started then the application master can

00:16:25,720 --> 00:16:32,260
go Kinkos go on to request the

00:16:29,410 --> 00:16:34,870
containers from resource managers the

00:16:32,260 --> 00:16:37,840
resource manager has the scheduler to do

00:16:34,870 --> 00:16:42,370
the calculation on top of the available

00:16:37,840 --> 00:16:44,820
resource in the cluster and I checked

00:16:42,370 --> 00:16:47,350
some policies in the respondent the

00:16:44,820 --> 00:16:51,970
application master with some allocated

00:16:47,350 --> 00:16:54,280
containers after that when application

00:16:51,970 --> 00:16:58,540
was received allocate containers it will

00:16:54,280 --> 00:17:02,640
come tackle node manager to start the

00:16:58,540 --> 00:17:05,710
container to run the application code in

00:17:02,640 --> 00:17:08,440
the during this period when the

00:17:05,710 --> 00:17:10,540
application is running client is a free

00:17:08,440 --> 00:17:14,320
to contact resource manager and the

00:17:10,540 --> 00:17:18,910
application man master to get the status

00:17:14,320 --> 00:17:22,660
a report of the application mmm but on

00:17:18,910 --> 00:17:27,510
the application master site developers

00:17:22,660 --> 00:17:29,830
you have to implement some kind of

00:17:27,510 --> 00:17:33,670
information encourage service such that

00:17:29,830 --> 00:17:38,740
the client can ask for some application

00:17:33,670 --> 00:17:40,950
specific information so after all the

00:17:38,740 --> 00:17:44,050
containers has been done in the

00:17:40,950 --> 00:17:44,550
application master has been notified of

00:17:44,050 --> 00:17:48,120
the come

00:17:44,550 --> 00:17:50,850
vision it's a way out do on registration

00:17:48,120 --> 00:17:53,250
with resource manager to letter the

00:17:50,850 --> 00:17:56,990
resource management manager know the

00:17:53,250 --> 00:18:00,810
application is safe to finish then

00:17:56,990 --> 00:18:05,000
afterwards application master is a safe

00:18:00,810 --> 00:18:08,610
to exceed that is complete flow of the

00:18:05,000 --> 00:18:11,070
application another thing I would like

00:18:08,610 --> 00:18:13,830
to mention here is a scheduler it is

00:18:11,070 --> 00:18:17,580
actually a part of resource manager but

00:18:13,830 --> 00:18:21,450
it is there so important to users so I

00:18:17,580 --> 00:18:24,300
would like to speak about it separately

00:18:21,450 --> 00:18:29,130
users should have actually the resource

00:18:24,300 --> 00:18:31,380
scheduler is is a part that decide what

00:18:29,130 --> 00:18:33,900
resource is going to be allocated to app

00:18:31,380 --> 00:18:37,290
an application so users should pay

00:18:33,900 --> 00:18:39,450
attention to it and make a good decision

00:18:37,290 --> 00:18:42,690
on what schedule is they are going to

00:18:39,450 --> 00:18:45,660
use and what configurations they they

00:18:42,690 --> 00:18:47,700
want to do and the currently in urine we

00:18:45,660 --> 00:18:50,790
have implemented a three type of a

00:18:47,700 --> 00:18:53,220
scheduler the first one is if IIF all

00:18:50,790 --> 00:18:55,140
scheduler the second woman is fair

00:18:53,220 --> 00:18:59,850
scheduling the sort of woman is a

00:18:55,140 --> 00:19:04,230
capacity scheduler the if I fo scheduler

00:18:59,850 --> 00:19:07,170
is the most think as the simplest one if

00:19:04,230 --> 00:19:12,150
for example if a user have a private

00:19:07,170 --> 00:19:15,120
cluster and he is their only application

00:19:12,150 --> 00:19:18,470
submitter and he doesn't care about in a

00:19:15,120 --> 00:19:22,110
parallelism of it it is the he submitted

00:19:18,470 --> 00:19:26,820
applications he can simply choose a fi

00:19:22,110 --> 00:19:29,760
fo scheduler such that all the resource

00:19:26,820 --> 00:19:32,790
requests will be handled us satisfied

00:19:29,760 --> 00:19:36,090
one by one but for example he really

00:19:32,790 --> 00:19:39,360
cares about the parallelism about his

00:19:36,090 --> 00:19:42,980
applications he might want to choose a

00:19:39,360 --> 00:19:44,490
fair scheduler to ensure the

00:19:42,980 --> 00:19:46,950
applications that are running

00:19:44,490 --> 00:19:48,690
simultaneously can get a fair share of

00:19:46,950 --> 00:19:52,230
the resourcing the cluster

00:19:48,690 --> 00:19:54,279
the first scheduler is a bit smarter it

00:19:52,230 --> 00:19:57,729
can

00:19:54,279 --> 00:20:00,700
as distributed the whole class of

00:19:57,729 --> 00:20:03,609
resources you know evenly to different

00:20:00,700 --> 00:20:05,769
application cues and even within one

00:20:03,609 --> 00:20:08,440
application cues is going on to

00:20:05,769 --> 00:20:12,519
distributed resource evenly among

00:20:08,440 --> 00:20:15,999
different applications in the most

00:20:12,519 --> 00:20:18,729
sophisticated schedule is a capacity

00:20:15,999 --> 00:20:22,299
scheduling actually this one is supposed

00:20:18,729 --> 00:20:24,849
to use of force a multi potency scenario

00:20:22,299 --> 00:20:27,969
give our cluster is shared by different

00:20:24,849 --> 00:20:30,009
groups departments or companies

00:20:27,969 --> 00:20:34,809
you should stakeholders who want to make

00:20:30,009 --> 00:20:37,059
sure they can always get share from this

00:20:34,809 --> 00:20:39,009
cluster to make sure their when they

00:20:37,059 --> 00:20:41,529
have application to and they can run yet

00:20:39,009 --> 00:20:45,339
so this capacity scheduler has a bunch

00:20:41,529 --> 00:20:48,099
of sophisticated lower bound and upper

00:20:45,339 --> 00:20:52,320
bound a check for the resource to

00:20:48,099 --> 00:20:57,580
allocate your once one plication and

00:20:52,320 --> 00:21:00,700
also users have a bunch of options to

00:20:57,580 --> 00:21:04,330
configure the capacity scheduler to do

00:21:00,700 --> 00:21:07,239
the right things to share the resource

00:21:04,330 --> 00:21:08,739
and the controls a resource to infer the

00:21:07,239 --> 00:21:15,999
resource isolations for each

00:21:08,739 --> 00:21:18,909
stakeholders so I'm done for from my

00:21:15,999 --> 00:21:21,399
part and my colleague R is going to

00:21:18,909 --> 00:21:25,379
introduce something about is a recent

00:21:21,399 --> 00:21:25,379
development of the young project

00:21:29,919 --> 00:21:35,379
yeah thanks everyone being here okay in

00:21:33,369 --> 00:21:38,200
the remaini i'm gonna talk about the

00:21:35,379 --> 00:21:40,239
most original departments for the last

00:21:38,200 --> 00:21:43,259
few months we have we have done volume

00:21:40,239 --> 00:21:45,669
project and also i'm gonna talk about

00:21:43,259 --> 00:21:51,609
the simplest way to write on your

00:21:45,669 --> 00:21:53,499
application okay uh yeah yeah i said we

00:21:51,609 --> 00:21:57,330
said cut about two dollar is coming out

00:21:53,499 --> 00:22:01,149
i will say by the end of this week and

00:21:57,330 --> 00:22:03,999
if we only one interested check it out

00:22:01,149 --> 00:22:06,039
so resource manager her so and we also

00:22:03,999 --> 00:22:08,170
included these features in the new

00:22:06,039 --> 00:22:10,600
release resource magic

00:22:08,170 --> 00:22:12,280
very very bit and the application he

00:22:10,600 --> 00:22:15,130
started the Peter service which is quite

00:22:12,280 --> 00:22:21,250
similar to the job Jesus server in the

00:22:15,130 --> 00:22:23,650
Odum Apple to use version 1 okay as we

00:22:21,250 --> 00:22:25,960
know our resource manager is the central

00:22:23,650 --> 00:22:28,750
authority to care resources and given

00:22:25,960 --> 00:22:31,150
resource to the to each each individual

00:22:28,750 --> 00:22:34,330
application so it is a potential a

00:22:31,150 --> 00:22:35,860
single point of failure and and also

00:22:34,330 --> 00:22:37,870
people want to manually take down the

00:22:35,860 --> 00:22:40,690
resource manager so just for upgrades

00:22:37,870 --> 00:22:43,390
for upgrading from an older version to a

00:22:40,690 --> 00:22:45,340
newer version and so at the goal the

00:22:43,390 --> 00:22:48,130
goal is to make resource manager

00:22:45,340 --> 00:22:50,800
downtown invisible to the outside users

00:22:48,130 --> 00:22:52,660
the outside users should not be aware of

00:22:50,800 --> 00:22:54,970
the downtown of the resource manager

00:22:52,660 --> 00:22:57,190
they they don't need to resubmit the

00:22:54,970 --> 00:22:59,680
application if the resource manager

00:22:57,190 --> 00:23:02,980
fails or crushed biscuits to pass the

00:22:59,680 --> 00:23:06,910
resource measure restart it set up the

00:23:02,980 --> 00:23:09,250
ground work for assisting the I'm state

00:23:06,910 --> 00:23:11,800
and also pick up the state from the

00:23:09,250 --> 00:23:15,490
state's a kind of stay store and and

00:23:11,800 --> 00:23:18,250
sort of kick off the early application

00:23:15,490 --> 00:23:19,840
and the odd part is that fear over how

00:23:18,250 --> 00:23:21,880
the resource manager is moving from one

00:23:19,840 --> 00:23:27,280
resource major instance to another

00:23:21,880 --> 00:23:30,850
instance so typically if we want to save

00:23:27,280 --> 00:23:33,250
the data we want to look at how what

00:23:30,850 --> 00:23:34,810
other state the resource manager has so

00:23:33,250 --> 00:23:36,820
typically it has two kind of two

00:23:34,810 --> 00:23:38,380
categories that winds the static state

00:23:36,820 --> 00:23:40,060
and the others are running state

00:23:38,380 --> 00:23:42,430
standard state in the state the Napper

00:23:40,060 --> 00:23:45,220
changes like the the user submits an

00:23:42,430 --> 00:23:48,000
application it tells a user sorry I tell

00:23:45,220 --> 00:23:50,290
the user tells I am these are the

00:23:48,000 --> 00:23:52,180
context for this application these are

00:23:50,290 --> 00:23:54,490
the command you should have run for this

00:23:52,180 --> 00:23:56,770
job these are the environment variables

00:23:54,490 --> 00:23:59,950
these are the cross paths and also the

00:23:56,770 --> 00:24:02,830
local jobs jobs jobs which cannot be

00:23:59,950 --> 00:24:05,260
downloaded from the wishes first put on

00:24:02,830 --> 00:24:07,780
the HDFS and then will be downloaded

00:24:05,260 --> 00:24:11,290
from the HTF when the nodal manager is

00:24:07,780 --> 00:24:12,880
launching the application master and the

00:24:11,290 --> 00:24:16,420
other part is the running state to disk

00:24:12,880 --> 00:24:18,310
it's dangerous it's a internal

00:24:16,420 --> 00:24:20,860
schedulers scheduler state of the

00:24:18,310 --> 00:24:21,880
resource manager and this this I will

00:24:20,860 --> 00:24:24,850
say is the standard

00:24:21,880 --> 00:24:28,809
of the world and how can we reconstruct

00:24:24,850 --> 00:24:30,820
the state of the scheduler such that we

00:24:28,809 --> 00:24:33,520
don't have to rescale all the

00:24:30,820 --> 00:24:35,490
application from scratch so first we we

00:24:33,520 --> 00:24:38,289
want to compare with the job chatter

00:24:35,490 --> 00:24:40,990
architecture that the first the old

00:24:38,289 --> 00:24:43,270
version of how to so in Java it

00:24:40,990 --> 00:24:44,679
basically does two things the resource

00:24:43,270 --> 00:24:46,450
measurement which young resource

00:24:44,679 --> 00:24:48,220
measured us today and also the

00:24:46,450 --> 00:24:50,919
application management which your

00:24:48,220 --> 00:24:52,480
application mustered us today so if we

00:24:50,919 --> 00:24:55,380
want to implement the same visual job

00:24:52,480 --> 00:24:57,970
Charter we sort of have to persist post

00:24:55,380 --> 00:24:59,320
application management and some sort of

00:24:57,970 --> 00:25:02,470
stated in post application management

00:24:59,320 --> 00:25:07,270
and resource management and and that

00:25:02,470 --> 00:25:09,789
there can be a match overhead for the

00:25:07,270 --> 00:25:12,309
job chart itself as people know that

00:25:09,789 --> 00:25:16,330
persistently it's always the bottleneck

00:25:12,309 --> 00:25:18,280
in the whole cluster so by inyoung

00:25:16,330 --> 00:25:20,110
because we separate out this logic

00:25:18,280 --> 00:25:22,929
application management from the resource

00:25:20,110 --> 00:25:24,640
management so in yon we only want to

00:25:22,929 --> 00:25:26,559
persist the state which process

00:25:24,640 --> 00:25:30,850
resource manager itself in the case of

00:25:26,559 --> 00:25:32,380
resource merge is feeling a crest so and

00:25:30,850 --> 00:25:34,210
when the resource manager comes up

00:25:32,380 --> 00:25:35,799
because the application master is

00:25:34,210 --> 00:25:37,750
running on a different machine then the

00:25:35,799 --> 00:25:39,580
resource major it's on running only the

00:25:37,750 --> 00:25:41,470
know the manager in century the other

00:25:39,580 --> 00:25:44,559
node managers and application masters

00:25:41,470 --> 00:25:47,289
can tell the the newly started resource

00:25:44,559 --> 00:25:50,650
manager that these are the disarmer are

00:25:47,289 --> 00:25:52,090
my outstanding requests and the nor

00:25:50,650 --> 00:25:56,100
managers can also say these are the

00:25:52,090 --> 00:25:58,809
containers which I have been running and

00:25:56,100 --> 00:26:01,360
you don't have to risk at the roost can

00:25:58,809 --> 00:26:02,830
you reschedule this containers again and

00:26:01,360 --> 00:26:04,929
the resource manager sort of can

00:26:02,830 --> 00:26:07,679
reconstruct such schedulers data for

00:26:04,929 --> 00:26:12,130
each application individual application

00:26:07,679 --> 00:26:14,919
and that and I will say so the

00:26:12,130 --> 00:26:18,010
architecture not only just makes how

00:26:14,919 --> 00:26:20,230
make sure the makes the performance

00:26:18,010 --> 00:26:22,780
boost or it also gives us more

00:26:20,230 --> 00:26:25,710
opportunities to increment the feature

00:26:22,780 --> 00:26:25,710
in various ways

00:26:28,530 --> 00:26:39,390
oh this this this this gravel generally

00:26:36,480 --> 00:26:41,550
talks about how the we start in the

00:26:39,390 --> 00:26:45,750
failover works on the left-hand side we

00:26:41,550 --> 00:26:48,120
have a bunch of Noda not managers which

00:26:45,750 --> 00:26:51,150
maintains that continues the process of

00:26:48,120 --> 00:26:53,400
each application and on the on the

00:26:51,150 --> 00:26:56,400
middle column we have - manager -

00:26:53,400 --> 00:26:58,580
resource managers wines active and the

00:26:56,400 --> 00:27:01,110
others

00:26:58,580 --> 00:27:02,850
otherwise the condom resource manager

00:27:01,110 --> 00:27:06,420
and the other the previous one and on

00:27:02,850 --> 00:27:08,850
the right hand side we have a crime the

00:27:06,420 --> 00:27:11,430
crane who submits the job and also

00:27:08,850 --> 00:27:13,260
curing the state of the application and

00:27:11,430 --> 00:27:15,540
as we can see it both resource measures

00:27:13,260 --> 00:27:18,150
sure that since they store and this list

00:27:15,540 --> 00:27:20,220
or is kind of a Provost a store user can

00:27:18,150 --> 00:27:23,820
define will have state so they want we

00:27:20,220 --> 00:27:26,490
the available option today we have HDFS

00:27:23,820 --> 00:27:30,770
based a store and also the zookeeper

00:27:26,490 --> 00:27:34,050
business distal so the failover

00:27:30,770 --> 00:27:36,060
is pretty cheerful because the so in

00:27:34,050 --> 00:27:40,290
resource measures you pull of resource

00:27:36,060 --> 00:27:45,050
pages we have we have a active resource

00:27:40,290 --> 00:27:47,940
manager which recently all the requests

00:27:45,050 --> 00:27:50,790
from all the managers and clients and

00:27:47,940 --> 00:27:54,570
also we all the other normal resource

00:27:50,790 --> 00:27:56,340
managers are in standby mode so we are

00:27:54,570 --> 00:28:00,540
using the zookeepers were - in the

00:27:56,340 --> 00:28:03,150
leader in actions and whenever the

00:28:00,540 --> 00:28:04,860
active resource many fields one of the

00:28:03,150 --> 00:28:09,380
standby resource measures will come up

00:28:04,860 --> 00:28:12,660
and the claim to be the active one and

00:28:09,380 --> 00:28:15,660
the on the on the transition from the

00:28:12,660 --> 00:28:18,990
standby mode to the active mode the

00:28:15,660 --> 00:28:21,570
resource manager the the newest are

00:28:18,990 --> 00:28:23,280
resource manager will load a state from

00:28:21,570 --> 00:28:24,810
the state store which is state is

00:28:23,280 --> 00:28:27,240
written by the previous result measure

00:28:24,810 --> 00:28:29,790
about the application method has the

00:28:27,240 --> 00:28:32,850
application summation contacts the

00:28:29,790 --> 00:28:35,130
direct the so when the active resource

00:28:32,850 --> 00:28:35,970
manager when the new resource managers

00:28:35,130 --> 00:28:38,790
newly started

00:28:35,970 --> 00:28:42,000
they will also recent the listen to all

00:28:38,790 --> 00:28:45,120
these reports from both normal measures

00:28:42,000 --> 00:28:47,280
application masters so in case of

00:28:45,120 --> 00:28:49,830
failover so both not matches the

00:28:47,280 --> 00:28:51,810
application masters clients should be

00:28:49,830 --> 00:28:54,090
obvious are redirected to the new new

00:28:51,810 --> 00:28:57,960
resource manager and we sort of

00:28:54,090 --> 00:28:59,640
implemented our library the sort of a

00:28:57,960 --> 00:29:01,230
application they are between the

00:28:59,640 --> 00:29:05,370
resource manager and outer standard

00:29:01,230 --> 00:29:08,280
entities so people use this this proxy

00:29:05,370 --> 00:29:11,190
so it's so there are multiple options so

00:29:08,280 --> 00:29:13,260
today we implement I prick a lot of pre

00:29:11,190 --> 00:29:16,110
complete complete list of resource

00:29:13,260 --> 00:29:18,570
managers so the the trainer's typically

00:29:16,110 --> 00:29:21,870
just keep a polling and look into the

00:29:18,570 --> 00:29:23,490
list of active I look in the looking

00:29:21,870 --> 00:29:25,650
into the list of the resource managers

00:29:23,490 --> 00:29:31,050
and the keeper pony until we find the

00:29:25,650 --> 00:29:32,640
one who is active okay

00:29:31,050 --> 00:29:34,380
nothing I want to say the application

00:29:32,640 --> 00:29:38,250
history server

00:29:34,380 --> 00:29:41,160
this is motivated by the job historian

00:29:38,250 --> 00:29:43,260
job chatter so I want to give more

00:29:41,160 --> 00:29:48,090
context about the job history server so

00:29:43,260 --> 00:29:51,210
the MCUs job works is it while he's

00:29:48,090 --> 00:29:55,650
running it periodically would write the

00:29:51,210 --> 00:29:57,930
historic data on to HDFS and the one

00:29:55,650 --> 00:30:00,690
when a job finished or the job is

00:29:57,930 --> 00:30:03,480
removed from the memory so the client

00:30:00,690 --> 00:30:05,790
will make a request will be made a

00:30:03,480 --> 00:30:07,350
request to the job chatter and and ask

00:30:05,790 --> 00:30:08,970
for this application again but this

00:30:07,350 --> 00:30:11,840
approach is completed come from the

00:30:08,970 --> 00:30:14,670
memory so this request will be

00:30:11,840 --> 00:30:18,000
redirected to a job history server which

00:30:14,670 --> 00:30:20,450
is a separated demon sitting beyond a

00:30:18,000 --> 00:30:23,280
MapReduce and the dis job your server

00:30:20,450 --> 00:30:25,380
reads the persistent data which is

00:30:23,280 --> 00:30:28,500
returned by the are running MapReduce

00:30:25,380 --> 00:30:32,760
jobs and only when the user asked for

00:30:28,500 --> 00:30:34,800
this application the job mister the job

00:30:32,760 --> 00:30:37,350
his server will load this data and to

00:30:34,800 --> 00:30:41,580
serve such a request so it's sort of an

00:30:37,350 --> 00:30:43,740
on-demand refresher so the goal is we

00:30:41,580 --> 00:30:46,290
want to stop such applications were in a

00:30:43,740 --> 00:30:48,240
more generic way so each application so

00:30:46,290 --> 00:30:51,840
we can imagine so each application

00:30:48,240 --> 00:30:54,120
running on top of young if people want

00:30:51,840 --> 00:30:55,740
to implement each server such kind of

00:30:54,120 --> 00:30:57,510
histories of each application

00:30:55,740 --> 00:31:00,360
running on top of young fellah will be

00:30:57,510 --> 00:31:03,000
clearly another scalable solution though

00:31:00,360 --> 00:31:05,549
so we want to make that such a more

00:31:03,000 --> 00:31:09,299
generic server history server to such to

00:31:05,549 --> 00:31:11,070
serve all the applications application

00:31:09,299 --> 00:31:15,470
for operations who want to save the

00:31:11,070 --> 00:31:18,919
historic data so also typically we have

00:31:15,470 --> 00:31:21,210
we have two kinds of two categories of

00:31:18,919 --> 00:31:23,610
information we want save the resource

00:31:21,210 --> 00:31:26,220
major has its own the generic

00:31:23,610 --> 00:31:28,200
information the application state what

00:31:26,220 --> 00:31:32,070
is the tag list of this application will

00:31:28,200 --> 00:31:35,010
either the command the contacts and also

00:31:32,070 --> 00:31:37,950
similarly the atom the containers what

00:31:35,010 --> 00:31:41,340
is the state what is the the usage

00:31:37,950 --> 00:31:43,140
always the the resources in the resource

00:31:41,340 --> 00:31:45,750
consumption of this container and the

00:31:43,140 --> 00:31:47,610
application master itself I also write

00:31:45,750 --> 00:31:50,940
to because this is the user encode an

00:31:47,610 --> 00:31:55,159
application must exceed itself has to

00:31:50,940 --> 00:31:58,080
write some sort of his specific

00:31:55,159 --> 00:32:01,110
information which is specific to itself

00:31:58,080 --> 00:32:05,220
and and we also provide some interface

00:32:01,110 --> 00:32:08,210
for user to curie we have the up RPC

00:32:05,220 --> 00:32:12,059
implementation and also we have the

00:32:08,210 --> 00:32:15,120
restful service implementation for the

00:32:12,059 --> 00:32:16,980
for the user to make yourself and also

00:32:15,120 --> 00:32:18,899
the history survey the self has a

00:32:16,980 --> 00:32:22,200
history surfer UI where UI

00:32:18,899 --> 00:32:26,090
I wish it's quite similar to the web you

00:32:22,200 --> 00:32:28,590
of the resource measure this is a

00:32:26,090 --> 00:32:31,380
general graph of how this whole thing

00:32:28,590 --> 00:32:34,380
works on the left side the application

00:32:31,380 --> 00:32:35,880
master uses so we will provide a write

00:32:34,380 --> 00:32:38,190
interface for each application master

00:32:35,880 --> 00:32:40,620
I'm Chris Amato used such a ready writer

00:32:38,190 --> 00:32:44,399
interface to write into a strata storage

00:32:40,620 --> 00:32:47,100
and similarly on the drive side on the

00:32:44,399 --> 00:32:50,070
right side the resource manager also

00:32:47,100 --> 00:32:52,710
writes the generic state the app the

00:32:50,070 --> 00:32:55,770
attempt containers using the interface

00:32:52,710 --> 00:33:00,059
to a shared storage and on the middle

00:32:55,770 --> 00:33:02,370
column the the history server only when

00:33:00,059 --> 00:33:04,740
the user asks for this particular

00:33:02,370 --> 00:33:07,649
application the history server load such

00:33:04,740 --> 00:33:09,419
state for this application from the

00:33:07,649 --> 00:33:13,349
state of taste-off

00:33:09,419 --> 00:33:19,589
and a brand ADA in the web UI or give a

00:33:13,349 --> 00:33:23,219
RESTful API so wrong one in surveys

00:33:19,589 --> 00:33:26,190
though so yeah yeah yeah it's supposed

00:33:23,219 --> 00:33:29,519
to be a very generic perform for all

00:33:26,190 --> 00:33:31,259
kinds of applications the time

00:33:29,519 --> 00:33:32,940
christian-like member it just launched

00:33:31,259 --> 00:33:36,029
four hours

00:33:32,940 --> 00:33:37,619
taste it it's eventually finished but

00:33:36,029 --> 00:33:39,690
there are other applications we should

00:33:37,619 --> 00:33:42,179
never to miss it's just sets up its own

00:33:39,690 --> 00:33:45,570
server and the reason for sound requests

00:33:42,179 --> 00:33:50,190
so we want to make the disrupt impactive

00:33:45,570 --> 00:33:51,359
impact to keep the impact minimum to

00:33:50,190 --> 00:33:53,759
order on the applications there are

00:33:51,359 --> 00:33:56,639
multiple works going on here so the what

00:33:53,759 --> 00:33:58,200
prism Amen restart so today so be what

00:33:56,639 --> 00:34:00,329
this feature so this is halfway

00:33:58,200 --> 00:34:02,339
incremental feature so before this

00:34:00,329 --> 00:34:07,169
feature whether we have a some way to

00:34:02,339 --> 00:34:08,909
restart the am we when am crushed we

00:34:07,169 --> 00:34:12,510
have sort of configured the max I am

00:34:08,909 --> 00:34:14,609
attempt so they're the new there will be

00:34:12,510 --> 00:34:17,639
a new am study but this new am study is

00:34:14,609 --> 00:34:20,730
basically a new application which is

00:34:17,639 --> 00:34:22,679
start from scratch and in the meanwhile

00:34:20,730 --> 00:34:24,480
the resource manager on the crash of

00:34:22,679 --> 00:34:26,519
this application will kill all the

00:34:24,480 --> 00:34:29,220
containers that are launched and

00:34:26,519 --> 00:34:32,009
monitored by this application by the

00:34:29,220 --> 00:34:34,559
previous a.m. so when the newest are am

00:34:32,009 --> 00:34:38,940
comes up it allows all these containers

00:34:34,559 --> 00:34:41,730
it has a previously has so although we

00:34:38,940 --> 00:34:43,759
have what we have done is to not killing

00:34:41,730 --> 00:34:45,960
these containers to have a way to

00:34:43,759 --> 00:34:48,929
rebound this previously running

00:34:45,960 --> 00:34:52,200
containers to the application master so

00:34:48,929 --> 00:34:55,200
this is this again has to do with our

00:34:52,200 --> 00:34:57,329
new new architecture so the resource

00:34:55,200 --> 00:34:59,250
managers teens has resource managers

00:34:57,329 --> 00:35:00,690
state and where the am crash because

00:34:59,250 --> 00:35:02,609
these two things are separately

00:35:00,690 --> 00:35:04,259
ronald information when the new am

00:35:02,609 --> 00:35:07,529
concept it can cure is a resource

00:35:04,259 --> 00:35:09,950
manager that we are my we are my

00:35:07,529 --> 00:35:12,089
previous containers and please give

00:35:09,950 --> 00:35:14,910
please giving me back and under

00:35:12,089 --> 00:35:17,460
registration with the resource measure

00:35:14,910 --> 00:35:19,829
this was man you can tell the the new am

00:35:17,460 --> 00:35:23,570
that oh these are the containers you

00:35:19,829 --> 00:35:26,930
have and you are non you

00:35:23,570 --> 00:35:30,320
- you don't need to stop start new

00:35:26,930 --> 00:35:33,110
containers and another students going on

00:35:30,320 --> 00:35:34,700
is the no the manual restart so it it's

00:35:33,110 --> 00:35:36,800
quite a mirror so when the normal

00:35:34,700 --> 00:35:38,750
measured process dies because the

00:35:36,800 --> 00:35:40,970
containers are running on different the

00:35:38,750 --> 00:35:46,010
process when the normal magic process

00:35:40,970 --> 00:35:47,660
dies we can all so people so today when

00:35:46,010 --> 00:35:49,280
another magic dies all the containers

00:35:47,660 --> 00:35:52,640
belong to this non average Joe

00:35:49,280 --> 00:35:55,490
there are we with the normal so we want

00:35:52,640 --> 00:35:57,950
to make sure even he would not measure

00:35:55,490 --> 00:35:59,530
itself dice the containers that are

00:35:57,950 --> 00:36:05,060
running on different process than the

00:35:59,530 --> 00:36:07,610
measure I'm not actually killed but is

00:36:05,060 --> 00:36:09,500
no measure so the new style nor magic

00:36:07,610 --> 00:36:15,200
can adjust repent to these containers

00:36:09,500 --> 00:36:19,030
that and also again this this is still

00:36:15,200 --> 00:36:21,440
annoying not in progress and not yet

00:36:19,030 --> 00:36:26,780
implemented so nothing I want to know

00:36:21,440 --> 00:36:30,410
how to write a young application also

00:36:26,780 --> 00:36:32,630
look at this grab fight for user there's

00:36:30,410 --> 00:36:34,700
only two TC user needs to write the

00:36:32,630 --> 00:36:37,280
current the current zombies job kind of

00:36:34,700 --> 00:36:41,510
get the status of the application and

00:36:37,280 --> 00:36:43,730
also the the centerpiece of work is the

00:36:41,510 --> 00:36:45,710
application master writes the user

00:36:43,730 --> 00:36:47,990
encode to gather resource from the

00:36:45,710 --> 00:36:50,420
resource measure and to launch launch

00:36:47,990 --> 00:36:57,560
containers on specific machines for its

00:36:50,420 --> 00:37:01,070
tasks these are worth so there's one two

00:36:57,560 --> 00:37:07,900
three seven seven PS disability native

00:37:01,070 --> 00:37:07,900
ApS to use to write an application code

00:37:09,160 --> 00:37:16,430
we have we have beyond that we have a

00:37:12,740 --> 00:37:19,010
more higher level library - for people

00:37:16,430 --> 00:37:21,290
to use the young kind which deals with

00:37:19,010 --> 00:37:23,420
the abstract the interaction between the

00:37:21,290 --> 00:37:24,860
client and the resource manager and am

00:37:23,420 --> 00:37:26,150
I'm trying which deals the interactive

00:37:24,860 --> 00:37:28,190
research application master and the

00:37:26,150 --> 00:37:30,590
resource manager and also the nether not

00:37:28,190 --> 00:37:32,420
a magic trunk which deals the the

00:37:30,590 --> 00:37:35,320
interaction between the application must

00:37:32,420 --> 00:37:35,320
end and not imagine

00:37:37,970 --> 00:37:45,330
okay r22 right a young application

00:37:42,570 --> 00:37:49,560
typically we so first of the kind gets

00:37:45,330 --> 00:37:51,210
approached RIT which is the which the

00:37:49,560 --> 00:37:55,650
car used to identify this application

00:37:51,210 --> 00:37:57,510
and end up the kind of says the current

00:37:55,650 --> 00:37:59,609
construct the application contacts this

00:37:57,510 --> 00:38:01,230
is essentially the meditate about this

00:37:59,609 --> 00:38:05,250
application were laid the command only

00:38:01,230 --> 00:38:07,710
the crossed paths and the job just like

00:38:05,250 --> 00:38:12,420
sector and there it makes code and use

00:38:07,710 --> 00:38:16,190
the submit adhere together to to make

00:38:12,420 --> 00:38:19,380
the request to submit the application to

00:38:16,190 --> 00:38:21,150
use the young con this is a simplified

00:38:19,380 --> 00:38:23,310
code so use the young kind of first we

00:38:21,150 --> 00:38:25,680
create a young using the young

00:38:23,310 --> 00:38:29,160
client to create the application and

00:38:25,680 --> 00:38:31,890
then within the application we construct

00:38:29,160 --> 00:38:35,280
the application context and we set up at

00:38:31,890 --> 00:38:38,220
the command the environments the jars

00:38:35,280 --> 00:38:39,900
and etc and then we say young grant

00:38:38,220 --> 00:38:44,220
thought submit application with the

00:38:39,900 --> 00:38:46,230
given context the other polish the given

00:38:44,220 --> 00:38:49,410
the application master is started on one

00:38:46,230 --> 00:38:51,750
of the Noda machine or manager

00:38:49,410 --> 00:38:53,520
so in the approach must code up

00:38:51,750 --> 00:38:55,800
typically we just or they repeat the

00:38:53,520 --> 00:38:58,109
water what we have done for the

00:38:55,800 --> 00:39:00,540
application master we construct the

00:38:58,109 --> 00:39:02,510
content container launch contacts that

00:39:00,540 --> 00:39:08,160
we set up with the commands the

00:39:02,510 --> 00:39:10,290
environment variables so so the first

00:39:08,160 --> 00:39:12,060
thing M does is to register which I'm

00:39:10,290 --> 00:39:14,250
using the register application master

00:39:12,060 --> 00:39:16,109
API and there it harpy's with the

00:39:14,250 --> 00:39:21,000
resource manager at the DISA help is

00:39:16,109 --> 00:39:22,980
also serves as a our request allocation

00:39:21,000 --> 00:39:26,190
request it's tender request to resource

00:39:22,980 --> 00:39:28,800
manager and the resource manager returns

00:39:26,190 --> 00:39:31,050
the containers back to the application

00:39:28,800 --> 00:39:33,270
master and the milk application master

00:39:31,050 --> 00:39:35,490
used these containers to launch its

00:39:33,270 --> 00:39:38,010
tasks on a specific machine after the

00:39:35,490 --> 00:39:41,099
application started uses the finished

00:39:38,010 --> 00:39:45,180
application master API to say I'm done

00:39:41,099 --> 00:39:47,810
with this job but there are couple

00:39:45,180 --> 00:39:50,280
things which you are not too obvious the

00:39:47,810 --> 00:39:52,980
containers are

00:39:50,280 --> 00:39:55,410
at all features completed a signal so if

00:39:52,980 --> 00:39:59,430
you say we can want to contain we want

00:39:55,410 --> 00:40:02,520
to get some continuous so because it

00:39:59,430 --> 00:40:04,950
depends on the on the schedule of the

00:40:02,520 --> 00:40:06,510
application sort of the resource manager

00:40:04,950 --> 00:40:09,119
in the may or may not give you such

00:40:06,510 --> 00:40:11,069
containers depending on the consumption

00:40:09,119 --> 00:40:13,530
of the whole cluster and what is the

00:40:11,069 --> 00:40:16,890
limit this air pollution has so the user

00:40:13,530 --> 00:40:18,000
typically needs to to wait make such a

00:40:16,890 --> 00:40:20,940
request okay okay

00:40:18,000 --> 00:40:23,160
with zero containers nor with more

00:40:20,940 --> 00:40:25,500
containers with their o containers and

00:40:23,160 --> 00:40:28,290
so using the same API which I'll talk

00:40:25,500 --> 00:40:30,630
about the allocation API it serves as a

00:40:28,290 --> 00:40:33,230
heartbeat also and a wait until we

00:40:30,630 --> 00:40:37,940
gather request and the record and I

00:40:33,230 --> 00:40:40,079
don't think of okay this is simplified

00:40:37,940 --> 00:40:44,099
so the northern Magister we have

00:40:40,079 --> 00:40:45,780
discussed three ApS to to start

00:40:44,099 --> 00:40:48,690
container gather container status and

00:40:45,780 --> 00:40:52,050
stop container so these are typical

00:40:48,690 --> 00:40:54,119
example so the i i'm i used to creates

00:40:52,050 --> 00:40:56,190
the air mom crank and the constructor

00:40:54,119 --> 00:40:58,560
container request and put it in the

00:40:56,190 --> 00:41:01,230
crime say add a continuing quest and

00:40:58,560 --> 00:41:04,260
then we wait we inside the roof we wait

00:41:01,230 --> 00:41:07,010
until we catch some containers until the

00:41:04,260 --> 00:41:10,560
continuous stress roger the bureau and

00:41:07,010 --> 00:41:12,900
and then we use the NM crime to create a

00:41:10,560 --> 00:41:15,020
crime and a for each response returned

00:41:12,900 --> 00:41:17,849
by the am inclined we construct the

00:41:15,020 --> 00:41:20,490
container launch context and we set up

00:41:17,849 --> 00:41:23,099
with the commands and also we can do

00:41:20,490 --> 00:41:24,990
monitor say NM crontab dollar care

00:41:23,099 --> 00:41:27,319
container status and say a stop

00:41:24,990 --> 00:41:29,250
container which then given container ID

00:41:27,319 --> 00:41:33,630
so i'll get rolling

00:41:29,250 --> 00:41:36,300
also here are some few takeaways after

00:41:33,630 --> 00:41:36,869
this i want we would recommend people to

00:41:36,300 --> 00:41:38,369
take away

00:41:36,869 --> 00:41:41,069
so yeah it's supposed to be average

00:41:38,369 --> 00:41:43,319
natural resource management purple that

00:41:41,069 --> 00:41:45,630
k host a different a distributed

00:41:43,319 --> 00:41:49,980
application tell you can think of this

00:41:45,630 --> 00:41:54,450
is supposed to go in this this whole

00:41:49,980 --> 00:41:56,369
architecture allows user to go beyond a

00:41:54,450 --> 00:42:00,480
MapReduce like a we we can defined even

00:41:56,369 --> 00:42:03,040
batch processing the inaccurate models

00:42:00,480 --> 00:42:05,560
and the stream models and auto the

00:42:03,040 --> 00:42:08,170
a big piece this allows us to share a

00:42:05,560 --> 00:42:10,240
single hot tub a cluster so imagine you

00:42:08,170 --> 00:42:12,070
already have a MapReduce how to be

00:42:10,240 --> 00:42:14,410
trusted I hope you already have a young

00:42:12,070 --> 00:42:17,400
cluster and you already have some

00:42:14,410 --> 00:42:21,100
habitus job and you want to you want to

00:42:17,400 --> 00:42:24,880
you want to make you want to write a

00:42:21,100 --> 00:42:28,150
submit some application like the stone

00:42:24,880 --> 00:42:30,100
spark you can just show the same data on

00:42:28,150 --> 00:42:34,320
each Davis you don't have to establish

00:42:30,100 --> 00:42:37,240
that have a new cluster and running the

00:42:34,320 --> 00:42:40,210
corresponding frameworks and so and also

00:42:37,240 --> 00:42:44,860
yeah in production so Yahoo is the the

00:42:40,210 --> 00:42:47,800
the earliest one who atop young as the

00:42:44,860 --> 00:42:50,350
production so they they have thousands

00:42:47,800 --> 00:42:53,200
of nose running daily basis and also you

00:42:50,350 --> 00:42:57,360
bid I think I recently shift from the -

00:42:53,200 --> 00:42:57,360
/ 1 - ha - boo - so how do - not only

00:42:57,420 --> 00:43:04,390
gives us the the performance boost and

00:43:02,260 --> 00:43:08,230
also a week we work on your contribute

00:43:04,390 --> 00:43:10,540
and attach any test any if you want to

00:43:08,230 --> 00:43:13,060
write application and telling us what

00:43:10,540 --> 00:43:17,320
other things missing what other things

00:43:13,060 --> 00:43:20,860
are still without beans our redundancy

00:43:17,320 --> 00:43:25,510
what are these we should do and also we

00:43:20,860 --> 00:43:27,190
have this link doing the drawing people

00:43:25,510 --> 00:43:29,740
if we people want to learn the very

00:43:27,190 --> 00:43:32,980
detail code of the of the young cluster

00:43:29,740 --> 00:43:35,890
and you can just mode it's a very pure

00:43:32,980 --> 00:43:37,540
way to learn this - it's very cool way

00:43:35,890 --> 00:43:39,160
to learn the very detailed

00:43:37,540 --> 00:43:42,550
implementation of the young just about

00:43:39,160 --> 00:43:44,500
you can this in different zeroes pick up

00:43:42,550 --> 00:43:47,710
whichever you are interested in and

00:43:44,500 --> 00:43:50,910
there's a this simple young app on the

00:43:47,710 --> 00:43:54,330
github and people may want checkout and

00:43:50,910 --> 00:43:57,850
nothing this this book is out this book

00:43:54,330 --> 00:44:02,650
gives a very good detailed description

00:43:57,850 --> 00:44:05,050
of how young works how the what is the

00:44:02,650 --> 00:44:07,030
individual component doing and what are

00:44:05,050 --> 00:44:10,510
the new rationality behind the design

00:44:07,030 --> 00:44:16,240
and also how to write a young

00:44:10,510 --> 00:44:17,020
application yeah and it's probably

00:44:16,240 --> 00:44:23,880
caught up

00:44:17,020 --> 00:44:23,880
thirty bucks yeah question

00:44:35,510 --> 00:44:39,640
and any questions any questions

00:44:41,520 --> 00:44:44,270
you

00:44:50,510 --> 00:44:55,430
excuse me what is the difference so you

00:44:53,840 --> 00:45:04,610
are what is the difference between the

00:44:55,430 --> 00:45:06,850
traditional MapReduce and young masters

00:45:04,610 --> 00:45:06,850
like

00:45:15,910 --> 00:45:31,220
laughs sorry so we do have an idea about

00:45:22,880 --> 00:45:33,260
talkin actually I don't I'm not aware of

00:45:31,220 --> 00:45:37,340
it a sister you are mention the park

00:45:33,260 --> 00:45:40,630
yeah but for further resource managers a

00:45:37,340 --> 00:45:43,670
young it is actually a spinoff from the

00:45:40,630 --> 00:45:46,970
original had to project in it is

00:45:43,670 --> 00:45:50,150
supposed to be first working for a

00:45:46,970 --> 00:45:52,490
MapReduce job and you know to you to

00:45:50,150 --> 00:45:56,780
that we find that we we have a lot of

00:45:52,490 --> 00:46:01,010
other kind of worker loads such as storm

00:45:56,780 --> 00:46:02,900
the test and even spark so we want to

00:46:01,010 --> 00:46:06,710
support different a saying that that is

00:46:02,900 --> 00:46:10,580
a reasons we we speak of the resource

00:46:06,710 --> 00:46:13,730
management platform MapReduce and we

00:46:10,580 --> 00:46:16,010
make a generic layer and bring other

00:46:13,730 --> 00:46:27,110
kind of application into the for app

00:46:16,010 --> 00:46:30,620
Hadoop stack so what languages have you

00:46:27,110 --> 00:46:32,570
seen - oh no your application being

00:46:30,620 --> 00:46:36,170
written in have seen other KVM languages

00:46:32,570 --> 00:46:38,690
like Clojure and Scala actually there

00:46:36,170 --> 00:46:43,490
are two parts the first is the client

00:46:38,690 --> 00:46:46,190
and application master for the kind in

00:46:43,490 --> 00:46:51,280
application master now we are supposed

00:46:46,190 --> 00:46:54,110
to use Java 2 to write because we

00:46:51,280 --> 00:46:56,960
currently we just have the Java API is

00:46:54,110 --> 00:46:59,960
for writing application master in the

00:46:56,960 --> 00:47:02,870
client but we're still working on it to

00:46:59,960 --> 00:47:06,380
make it better for example we have

00:47:02,870 --> 00:47:09,110
already made the job submission a REST

00:47:06,380 --> 00:47:10,910
API it's such that in the future when

00:47:09,110 --> 00:47:13,520
you submit application you don't need a

00:47:10,910 --> 00:47:16,310
Java client you can have some other kind

00:47:13,520 --> 00:47:19,960
of written in other languages and all

00:47:16,310 --> 00:47:22,880
the other part is the the code of your

00:47:19,960 --> 00:47:26,620
application logic that part can be

00:47:22,880 --> 00:47:28,590
written in any kind of language you want

00:47:26,620 --> 00:47:32,490
to

00:47:28,590 --> 00:47:34,350
to add something so because yeah we use

00:47:32,490 --> 00:47:35,910
the protocol the Google protocol which

00:47:34,350 --> 00:47:40,140
typically and in our language you can

00:47:35,910 --> 00:47:42,240
interact with so given that we it's very

00:47:40,140 --> 00:47:43,800
likely that very possible we you can use

00:47:42,240 --> 00:47:46,380
different language to use the prototype

00:47:43,800 --> 00:47:49,050
and to inkatha with the young young

00:47:46,380 --> 00:47:52,440
crosser and I think we already have a

00:47:49,050 --> 00:47:54,300
example using the code language to to do

00:47:52,440 --> 00:47:56,700
all the other things application master

00:47:54,300 --> 00:48:00,660
and the contest and it's on the github

00:47:56,700 --> 00:48:04,100
you can just search for Go Go young and

00:48:00,660 --> 00:48:04,100
go ahead to passing yes

00:48:24,830 --> 00:48:32,710
okay okay if you have any questions we

00:48:28,340 --> 00:48:32,710

YouTube URL: https://www.youtube.com/watch?v=9Bm6EBQ8t8U


