Title: Building Highly Flexible, High Performance Query Engines -- Neeraja Rentachintala
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,000 --> 00:00:05,940
okay hi everyone my name is neeraja a

00:00:03,419 --> 00:00:07,980
little bit about myself I work at map

00:00:05,940 --> 00:00:10,200
our technologies it's a Hadoop

00:00:07,980 --> 00:00:12,300
distribution company and then I'm

00:00:10,200 --> 00:00:16,049
contributor for the Apache drill in the

00:00:12,300 --> 00:00:18,750
past I have worked on few areas such as

00:00:16,049 --> 00:00:20,220
data integration bi reporting and other

00:00:18,750 --> 00:00:23,670
data and application infrastructure

00:00:20,220 --> 00:00:25,680
related products so did I am excited to

00:00:23,670 --> 00:00:28,050
share with you a little bit about apache

00:00:25,680 --> 00:00:30,689
drill we'll start with an overview on

00:00:28,050 --> 00:00:32,669
what drill is why is it interesting and

00:00:30,689 --> 00:00:35,280
what are the use cases in which you can

00:00:32,669 --> 00:00:37,230
consider using the product right and

00:00:35,280 --> 00:00:39,360
then I will explain a little bit about

00:00:37,230 --> 00:00:42,570
various features of drill from a user

00:00:39,360 --> 00:00:44,250
point of view and then I will cover very

00:00:42,570 --> 00:00:46,289
briefly on what are the different

00:00:44,250 --> 00:00:49,140
architectural components that make up

00:00:46,289 --> 00:00:51,030
drill and then wrap up with the status

00:00:49,140 --> 00:00:53,280
on where we are and where we are heading

00:00:51,030 --> 00:00:55,559
towards right and in the end if I really

00:00:53,280 --> 00:00:58,050
have time which i doubt i will try to

00:00:55,559 --> 00:01:00,270
spend few minutes just showcasing few

00:00:58,050 --> 00:01:02,489
queries running few queries in the drill

00:01:00,270 --> 00:01:06,960
on one of the hot bills that i have got

00:01:02,489 --> 00:01:10,170
in the morning okay so a standard poll

00:01:06,960 --> 00:01:16,350
how many of you are considering to use

00:01:10,170 --> 00:01:18,540
or using sequel on hadoop today okay how

00:01:16,350 --> 00:01:23,220
many of you have heard about drill

00:01:18,540 --> 00:01:26,280
before at least the name ok this is

00:01:23,220 --> 00:01:28,680
encouraging little bit so for the folks

00:01:26,280 --> 00:01:31,259
who don't know right so let me start

00:01:28,680 --> 00:01:33,270
with a very quick overview on the

00:01:31,259 --> 00:01:34,740
different types of Hadoop use cases and

00:01:33,270 --> 00:01:36,720
the workloads and what are the different

00:01:34,740 --> 00:01:39,150
API is that are available in order to

00:01:36,720 --> 00:01:40,380
achieve these use cases right so this is

00:01:39,150 --> 00:01:41,880
certainly not intended to be a

00:01:40,380 --> 00:01:43,590
comprehensive list of all the different

00:01:41,880 --> 00:01:45,979
things you can do and all the different

00:01:43,590 --> 00:01:49,079
technologies but I just want to give a

00:01:45,979 --> 00:01:51,930
picture on where does drill fit right so

00:01:49,079 --> 00:01:53,700
drill doesn't do batch processing drill

00:01:51,930 --> 00:01:55,259
is certainly not in the area where it

00:01:53,700 --> 00:01:57,540
can give like 10 millisecond response

00:01:55,259 --> 00:01:58,790
time right that is where Phoenix I

00:01:57,540 --> 00:02:01,380
attended one of the talks yesterday

00:01:58,790 --> 00:02:03,170
drill is mainly at least in the Wonder

00:02:01,380 --> 00:02:06,329
diversion is focusing on the interactive

00:02:03,170 --> 00:02:08,729
analytics use case so what this means is

00:02:06,329 --> 00:02:10,649
a business user or a technical user who

00:02:08,729 --> 00:02:13,170
has sequel skill set is sitting before

00:02:10,649 --> 00:02:13,680
his computer and trying to interact and

00:02:13,170 --> 00:02:15,569
iterate

00:02:13,680 --> 00:02:17,969
third of data right that's the core use

00:02:15,569 --> 00:02:19,500
case so you can think about it from they

00:02:17,969 --> 00:02:21,750
are trying to explore the data they are

00:02:19,500 --> 00:02:23,040
trying to perform some ad hoc queries or

00:02:21,750 --> 00:02:24,900
you are trying to build some simple

00:02:23,040 --> 00:02:26,609
reporting applications so those are the

00:02:24,900 --> 00:02:29,010
use cases in which you can consider

00:02:26,609 --> 00:02:31,379
drill the other products at a broader

00:02:29,010 --> 00:02:34,680
category level that fit into this our

00:02:31,379 --> 00:02:36,829
shark Impala the new version of hive

00:02:34,680 --> 00:02:39,299
that is coming up on test framework and

00:02:36,829 --> 00:02:43,889
presto by Facebook so they are all

00:02:39,299 --> 00:02:45,659
trying to do similar things right so the

00:02:43,889 --> 00:02:47,790
next thing is why is interactive sequel

00:02:45,659 --> 00:02:49,290
interesting so in the past 12 1 and half

00:02:47,790 --> 00:02:51,480
years I would say there is a lot of

00:02:49,290 --> 00:02:53,099
momentum on interactive sequel so I

00:02:51,480 --> 00:02:54,689
mentioned few products which are open

00:02:53,099 --> 00:02:56,760
source already but there are also

00:02:54,689 --> 00:02:59,340
proprietary vendors like IBM Microsoft

00:02:56,760 --> 00:03:01,290
so everybody has a sequel on Hadoop kind

00:02:59,340 --> 00:03:04,409
of product so the main thing it is

00:03:01,290 --> 00:03:06,599
interesting is because it can open up

00:03:04,409 --> 00:03:08,579
Hadoop data to a broader audience right

00:03:06,599 --> 00:03:10,500
there is so sequel is pretty old sequel

00:03:08,579 --> 00:03:12,180
is pretty easy to do and there is so

00:03:10,500 --> 00:03:14,219
much equal talent in the companies and

00:03:12,180 --> 00:03:16,409
they want to leverage short of data and

00:03:14,219 --> 00:03:18,209
then there is also a sequel ecosystem of

00:03:16,409 --> 00:03:19,739
tools tableaus click views data made

00:03:18,209 --> 00:03:22,199
plat for there are so many tools out

00:03:19,739 --> 00:03:24,090
there that can speak sequel they can all

00:03:22,199 --> 00:03:26,370
be leveraged now with Hadoop right which

00:03:24,090 --> 00:03:28,739
is driving the need and then suddenly it

00:03:26,370 --> 00:03:30,449
can the Hadoop data itself can enable

00:03:28,739 --> 00:03:32,760
new types of use cases around customer

00:03:30,449 --> 00:03:34,590
satisfaction a good customer experience

00:03:32,760 --> 00:03:36,689
it's a bunch of new things that you can

00:03:34,590 --> 00:03:38,970
do with Hadoop and certainly there is a

00:03:36,689 --> 00:03:40,859
cost savings component to it because any

00:03:38,970 --> 00:03:42,299
how do best solution is certainly going

00:03:40,859 --> 00:03:45,419
to be cheaper than some of the

00:03:42,299 --> 00:03:47,220
traditional solutions out there so the

00:03:45,419 --> 00:03:48,989
first question that I commonly here is

00:03:47,220 --> 00:03:51,000
ok you just mentioned there are so many

00:03:48,989 --> 00:03:53,729
products out there why do you need one

00:03:51,000 --> 00:03:55,620
more sequel and harder product right so

00:03:53,729 --> 00:03:58,199
if you think about Big Data what really

00:03:55,620 --> 00:04:00,090
has driven the need for big data there

00:03:58,199 --> 00:04:01,799
are all these new types of applications

00:04:00,090 --> 00:04:03,569
right so i was attending the talks in

00:04:01,799 --> 00:04:06,810
the past couple of days there are social

00:04:03,569 --> 00:04:09,269
company's mobile web the whole sensor

00:04:06,810 --> 00:04:11,400
kind of data that is coming in and then

00:04:09,269 --> 00:04:13,769
there are cloud based companies right

00:04:11,400 --> 00:04:16,259
cloud-based applications so they all

00:04:13,769 --> 00:04:18,959
have a couple of things in common first

00:04:16,259 --> 00:04:21,419
of all they try to meet the users of a

00:04:18,959 --> 00:04:23,490
lot more number of users now right than

00:04:21,419 --> 00:04:24,740
the traditional applications and so

00:04:23,490 --> 00:04:27,349
there is certainly a scale

00:04:24,740 --> 00:04:28,880
aspect to it the second aspect that is

00:04:27,349 --> 00:04:30,770
interesting is they are very iterative

00:04:28,880 --> 00:04:32,180
in nature so you want to put out the

00:04:30,770 --> 00:04:34,160
application you want to get the feedback

00:04:32,180 --> 00:04:36,620
you want to make changes you want to add

00:04:34,160 --> 00:04:38,120
features right you want to maybe cut out

00:04:36,620 --> 00:04:40,280
some of the features that people are not

00:04:38,120 --> 00:04:42,319
using right so you need this kind of a

00:04:40,280 --> 00:04:44,419
quick feedback and turn around when you

00:04:42,319 --> 00:04:46,940
are working with these applications that

00:04:44,419 --> 00:04:49,669
makes it another key aspect that is

00:04:46,940 --> 00:04:51,710
unique about these applications so but

00:04:49,669 --> 00:04:53,360
these applications are driving new types

00:04:51,710 --> 00:04:55,280
of requirements on the underlying data

00:04:53,360 --> 00:04:57,020
layers right which is what has given

00:04:55,280 --> 00:04:58,759
birth to Hadoop and no sequel stores

00:04:57,020 --> 00:05:01,220
like Cassandra and all these

00:04:58,759 --> 00:05:03,650
things so one aspect is the schema

00:05:01,220 --> 00:05:06,139
itself is pretty flexible right people

00:05:03,650 --> 00:05:08,030
call it schema-less basically the schema

00:05:06,139 --> 00:05:09,530
is not managed in a central store it's

00:05:08,030 --> 00:05:11,630
managed within the applications right

00:05:09,530 --> 00:05:14,150
you all know this so the flexible

00:05:11,630 --> 00:05:16,340
flexibility is one part of it and it's

00:05:14,150 --> 00:05:18,860
rapidly changing because the application

00:05:16,340 --> 00:05:20,750
itself is changing very fast and the

00:05:18,860 --> 00:05:22,970
data type themselves are more semi

00:05:20,750 --> 00:05:25,370
structured and nested type of data

00:05:22,970 --> 00:05:26,360
structures they closely represent the

00:05:25,370 --> 00:05:28,820
objects that you have in your

00:05:26,360 --> 00:05:32,590
programming languages right so I just

00:05:28,820 --> 00:05:35,240
listed an example of a JSON here and

00:05:32,590 --> 00:05:37,099
Hadoop people are trying to use her two

00:05:35,240 --> 00:05:38,539
more and more as the central place where

00:05:37,099 --> 00:05:41,030
you would put together all this data and

00:05:38,539 --> 00:05:43,009
start analyzing the data right the new

00:05:41,030 --> 00:05:44,990
datasets clearly fit in there the new

00:05:43,009 --> 00:05:46,699
data says that we just talked about but

00:05:44,990 --> 00:05:48,409
there are also traditional data sets

00:05:46,699 --> 00:05:50,300
like your transactional application kind

00:05:48,409 --> 00:05:52,550
of data sets that are coming in from

00:05:50,300 --> 00:05:54,620
Oracle my sequel they are also going

00:05:52,550 --> 00:05:56,479
into Hadoop right because you want to do

00:05:54,620 --> 00:05:58,310
large-scale ETL processing on it you

00:05:56,479 --> 00:06:00,800
want to use Hadoop as a cost-effective

00:05:58,310 --> 00:06:02,240
historical kind of storage place there

00:06:00,800 --> 00:06:05,360
are different reasons why traditional

00:06:02,240 --> 00:06:07,039
data sets go in here but if you look at

00:06:05,360 --> 00:06:08,719
the characteristics of these data sets

00:06:07,039 --> 00:06:11,030
they have good schemas like very

00:06:08,719 --> 00:06:14,030
well-defined schemas and the schemas do

00:06:11,030 --> 00:06:16,819
not change pretty often and the data is

00:06:14,030 --> 00:06:17,690
fairly flat from these applications so

00:06:16,819 --> 00:06:19,819
the main thing I am trying to

00:06:17,690 --> 00:06:22,849
communicate in this point is Hadoop has

00:06:19,819 --> 00:06:25,400
is giving an opportunity to save these

00:06:22,849 --> 00:06:27,259
two types of data sets which have two

00:06:25,400 --> 00:06:31,130
distinct type of characteristics in

00:06:27,259 --> 00:06:33,169
terms of data models and data types so

00:06:31,130 --> 00:06:34,940
now if you look at the existing sequel

00:06:33,169 --> 00:06:35,660
approaches whether it is adam ms best

00:06:34,940 --> 00:06:37,820
solution

00:06:35,660 --> 00:06:39,260
a sequel and Hadoop solution I have

00:06:37,820 --> 00:06:41,720
collected this all this data in Hadoop

00:06:39,260 --> 00:06:43,850
and all my users are waiting to analyze

00:06:41,720 --> 00:06:45,770
this data the first thing I have to do

00:06:43,850 --> 00:06:47,720
is model the data for a centralized

00:06:45,770 --> 00:06:49,940
schema right I have to define tables I

00:06:47,720 --> 00:06:51,920
have to define views and I have to

00:06:49,940 --> 00:06:54,200
define it if I know the questions that

00:06:51,920 --> 00:06:56,090
people are going to ask and if I'm using

00:06:54,200 --> 00:06:58,340
a traditional system like post dress or

00:06:56,090 --> 00:06:59,810
some other system for analysis then I

00:06:58,340 --> 00:07:02,990
have to actually move the data through

00:06:59,810 --> 00:07:04,790
etl right Hadoop basically gave the

00:07:02,990 --> 00:07:06,830
flexibility to store the data without

00:07:04,790 --> 00:07:08,690
schema but it when it comes to analysis

00:07:06,830 --> 00:07:09,920
you have to specify the schema right

00:07:08,690 --> 00:07:11,780
that's the current state of this

00:07:09,920 --> 00:07:13,820
technology and if there are new

00:07:11,780 --> 00:07:15,890
questions you have to change if there

00:07:13,820 --> 00:07:18,770
are new data sources or modified schemas

00:07:15,890 --> 00:07:21,110
then there is a change so this problem

00:07:18,770 --> 00:07:22,820
as such is there today right even

00:07:21,110 --> 00:07:25,070
without big data this is one of the

00:07:22,820 --> 00:07:27,560
problems the only thing is the scale of

00:07:25,070 --> 00:07:29,660
this problem becomes very severe when

00:07:27,560 --> 00:07:31,910
you consider all these new types of data

00:07:29,660 --> 00:07:34,880
that you are talking about right so the

00:07:31,910 --> 00:07:37,010
main thing is the new data types do not

00:07:34,880 --> 00:07:39,920
really map well to the relational models

00:07:37,010 --> 00:07:42,410
I have a HBase schema every row has

00:07:39,920 --> 00:07:44,360
different schema I have a JSON document

00:07:42,410 --> 00:07:46,280
every element has different attributes

00:07:44,360 --> 00:07:48,320
in it how do you map this to a

00:07:46,280 --> 00:07:52,220
relational model even if you had somehow

00:07:48,320 --> 00:07:54,830
managed to map it then the schema might

00:07:52,220 --> 00:07:56,410
itself change very often right so the

00:07:54,830 --> 00:08:00,050
centralized schemas could become pretty

00:07:56,410 --> 00:08:01,610
quickly an overhead to manage so why

00:08:00,050 --> 00:08:03,260
another reason is I was talking to one

00:08:01,610 --> 00:08:05,510
of the users actually at the conference

00:08:03,260 --> 00:08:07,640
they try to analyze the Social Feeds

00:08:05,510 --> 00:08:09,710
Twitter feeds and this is third party

00:08:07,640 --> 00:08:12,650
data that they don't even own right so

00:08:09,710 --> 00:08:15,230
it's hard to impose a schema and a data

00:08:12,650 --> 00:08:17,270
which is coming from elsewhere right so

00:08:15,230 --> 00:08:20,030
the centralized schemas can can become

00:08:17,270 --> 00:08:21,710
an overhead pretty quickly so this is

00:08:20,030 --> 00:08:23,540
where drill fits in this is like a

00:08:21,710 --> 00:08:26,390
little marketing kind of a picture here

00:08:23,540 --> 00:08:28,880
but the main goal of drill is to allow

00:08:26,390 --> 00:08:31,370
the users to analyze the data and do it

00:08:28,880 --> 00:08:33,050
quickly right without having to spend

00:08:31,370 --> 00:08:34,849
weeks and months of preparing the data

00:08:33,050 --> 00:08:37,400
being able to analyze the data quickly

00:08:34,849 --> 00:08:38,780
so there are three components to it the

00:08:37,400 --> 00:08:41,360
first one is the flexible schema

00:08:38,780 --> 00:08:43,460
management so if you have your data in

00:08:41,360 --> 00:08:45,980
files and HBase you can analyze the data

00:08:43,460 --> 00:08:47,040
without defining a schema in hive for

00:08:45,980 --> 00:08:48,899
example

00:08:47,040 --> 00:08:50,699
but at the same time we do realize that

00:08:48,899 --> 00:08:52,740
their people have taken time to actually

00:08:50,699 --> 00:08:54,540
define the schemas right so if there is

00:08:52,740 --> 00:08:56,699
schema already available then you can

00:08:54,540 --> 00:08:58,529
query the data using drill as well so

00:08:56,699 --> 00:09:00,709
the giving that flexibility when it

00:08:58,529 --> 00:09:03,149
comes to schema says the core aspect

00:09:00,709 --> 00:09:05,220
there are spec see is the new data types

00:09:03,149 --> 00:09:07,079
that I just talked about I am just

00:09:05,220 --> 00:09:08,699
referring to it as no sequel data right

00:09:07,079 --> 00:09:10,709
nested data and semi-structured data

00:09:08,699 --> 00:09:13,019
types you RN allah you are able to

00:09:10,709 --> 00:09:15,630
analyze it more natively rather than

00:09:13,019 --> 00:09:16,949
trying to flatten it or model it so I'll

00:09:15,630 --> 00:09:19,259
talk about some of the features out

00:09:16,949 --> 00:09:21,089
there and that the third aspect is it's

00:09:19,259 --> 00:09:23,009
a sequel based tool so it has to work

00:09:21,089 --> 00:09:24,600
with the BI tools it has to provide good

00:09:23,009 --> 00:09:28,380
sequel supports so that's the third

00:09:24,600 --> 00:09:30,569
aspect which is kiefer drill and this is

00:09:28,380 --> 00:09:32,459
this all has to be done and we are we

00:09:30,569 --> 00:09:33,750
are talking about seconds response time

00:09:32,459 --> 00:09:35,730
right when you are talking about the BI

00:09:33,750 --> 00:09:37,380
reporting kind of applications then you

00:09:35,730 --> 00:09:39,480
are talking about seconds response time

00:09:37,380 --> 00:09:41,610
so being able to achieve this response

00:09:39,480 --> 00:09:44,790
time set scale is also a fundamental

00:09:41,610 --> 00:09:46,709
criteria for the product so flexible

00:09:44,790 --> 00:09:48,329
schema management very quickly if you

00:09:46,709 --> 00:09:50,009
want to query a JSON document that's

00:09:48,329 --> 00:09:52,050
coming from a web application or a click

00:09:50,009 --> 00:09:54,209
stream kind of a data I have to specify

00:09:52,050 --> 00:09:56,100
a schema with existing solution with

00:09:54,209 --> 00:09:59,399
drill basically you can do sequel

00:09:56,100 --> 00:10:01,740
queries directly on Jason and if you

00:09:59,399 --> 00:10:03,630
already have schema defined in hive or

00:10:01,740 --> 00:10:08,760
some other system then you you are able

00:10:03,630 --> 00:10:10,649
to leverage that using drill so let's

00:10:08,760 --> 00:10:12,990
see how how these things work right what

00:10:10,649 --> 00:10:15,209
are the features that contribute to some

00:10:12,990 --> 00:10:18,899
of the things that the benefit that I

00:10:15,209 --> 00:10:20,610
just talked about the first one is let

00:10:18,899 --> 00:10:23,040
me go back here the dynamic and the

00:10:20,610 --> 00:10:24,839
schema less queries so this is no need

00:10:23,040 --> 00:10:27,449
to define the centralized schema upfront

00:10:24,839 --> 00:10:29,790
you can specify what columns you want to

00:10:27,449 --> 00:10:32,250
retrieve at the query time right so that

00:10:29,790 --> 00:10:35,579
that is mainly the dynamic queries so

00:10:32,250 --> 00:10:37,709
let's let's see how this works so the

00:10:35,579 --> 00:10:40,980
first capability here is being able to

00:10:37,709 --> 00:10:43,439
query the files directly right so if you

00:10:40,980 --> 00:10:45,720
look at an example here there is I'm

00:10:43,439 --> 00:10:49,769
just doing a select star from BFS dot

00:10:45,720 --> 00:10:51,480
users dot a file named BFS is mainly

00:10:49,769 --> 00:10:53,100
referring to with my file system it

00:10:51,480 --> 00:10:55,230
could be your local file system or it

00:10:53,100 --> 00:10:56,839
could be Hadoop compatible file system

00:10:55,230 --> 00:10:59,430
right s3 or HD of

00:10:56,839 --> 00:11:00,810
BFS is there is no configuration there

00:10:59,430 --> 00:11:02,730
is no big configuration there it's

00:11:00,810 --> 00:11:05,579
simply like HDFS colon slash slash

00:11:02,730 --> 00:11:07,589
that's all it is and then user sees like

00:11:05,579 --> 00:11:09,959
a convenient kind of work space that you

00:11:07,589 --> 00:11:12,269
can create so users simply is pointing

00:11:09,959 --> 00:11:14,430
to a directory on my file system that's

00:11:12,269 --> 00:11:16,980
all it is you can think of it similar to

00:11:14,430 --> 00:11:18,839
like a database kind of a modular thing

00:11:16,980 --> 00:11:21,480
that you have created it's not a

00:11:18,839 --> 00:11:24,899
mandatory thing and then you specify the

00:11:21,480 --> 00:11:26,910
location of your file right and then I

00:11:24,899 --> 00:11:28,410
am just doing select start because I

00:11:26,910 --> 00:11:30,209
don't even know what the columns are out

00:11:28,410 --> 00:11:32,640
there in the JSON so I want to discover

00:11:30,209 --> 00:11:37,380
that columns first and then I can do for

00:11:32,640 --> 00:11:39,899
the sequel processing on it so more

00:11:37,380 --> 00:11:41,730
examples I am able to query a single

00:11:39,899 --> 00:11:44,040
file right just like I showed in the

00:11:41,730 --> 00:11:46,920
previous query or I can actually show a

00:11:44,040 --> 00:11:49,920
query a directory right i can say that

00:11:46,920 --> 00:11:51,630
in because in hadoop it's not a single

00:11:49,920 --> 00:11:53,160
file right in the file system it's

00:11:51,630 --> 00:11:54,630
always like a bunch of files you

00:11:53,160 --> 00:11:56,640
partition you put the data in a

00:11:54,630 --> 00:11:58,230
particular directory so you want you

00:11:56,640 --> 00:12:01,070
need these operations at a directory

00:11:58,230 --> 00:12:04,350
level so i want to say give me all the

00:12:01,070 --> 00:12:06,300
errors that have occurred in the january

00:12:04,350 --> 00:12:08,160
month so this could be going over i

00:12:06,300 --> 00:12:11,399
don't know 30 days of worth of data

00:12:08,160 --> 00:12:13,110
every day one log file and then you can

00:12:11,399 --> 00:12:18,300
do sequel processing on this data just

00:12:13,110 --> 00:12:20,040
like you are doing a ton tables so a

00:12:18,300 --> 00:12:22,380
similar thing I let it show this shows

00:12:20,040 --> 00:12:25,860
is in a demo as well so the next part is

00:12:22,380 --> 00:12:27,990
HBase right unlike files HPS actually

00:12:25,860 --> 00:12:30,420
have little schemer with it they do have

00:12:27,990 --> 00:12:32,220
this concept of namespaces they do have

00:12:30,420 --> 00:12:34,290
some tables they have column families

00:12:32,220 --> 00:12:37,199
the only thing that they don't have is

00:12:34,290 --> 00:12:39,089
the column specification and it is not

00:12:37,199 --> 00:12:41,519
there for a reason the reason is the

00:12:39,089 --> 00:12:42,839
columns actually can be flexible across

00:12:41,519 --> 00:12:45,570
different rows that's why it's not

00:12:42,839 --> 00:12:48,180
defined so basically what dylan allows

00:12:45,570 --> 00:12:50,850
you to do is query this hash based table

00:12:48,180 --> 00:12:53,279
data directly I'm not we're defining

00:12:50,850 --> 00:12:55,380
anything in hive right so I am a

00:12:53,279 --> 00:12:58,290
directly going after the HBase table and

00:12:55,380 --> 00:13:00,089
there is a column family the month and

00:12:58,290 --> 00:13:02,490
year all these are columns that I know

00:13:00,089 --> 00:13:04,110
exists in his face if a particular

00:13:02,490 --> 00:13:06,300
column does not exist i will get back a

00:13:04,110 --> 00:13:08,699
null value right but you can always

00:13:06,300 --> 00:13:10,470
discover what is there in HBase table

00:13:08,699 --> 00:13:13,300
and then you start varying it

00:13:10,470 --> 00:13:17,050
no upfront metadata definition is needed

00:13:13,300 --> 00:13:19,900
and then if for example this is this is

00:13:17,050 --> 00:13:21,580
an interesting thing so if so drill

00:13:19,900 --> 00:13:24,010
needs basically any sequel based

00:13:21,580 --> 00:13:26,890
engineer its data types right if the

00:13:24,010 --> 00:13:29,650
data type doesn't exist drill try to

00:13:26,890 --> 00:13:32,080
determine it but data types can also

00:13:29,650 --> 00:13:33,940
exist within the file for example if i

00:13:32,080 --> 00:13:36,700
pick parquet right there are certain

00:13:33,940 --> 00:13:39,040
data types that park identifies so that

00:13:36,700 --> 00:13:41,230
thing dil can leverage so the way to

00:13:39,040 --> 00:13:43,120
think about it is you are not having a

00:13:41,230 --> 00:13:46,060
central method at metadata definitions

00:13:43,120 --> 00:13:48,160
but if the metadata is there in the

00:13:46,060 --> 00:13:50,830
associated data store drill can leverage

00:13:48,160 --> 00:13:53,470
that right thus that's the way to think

00:13:50,830 --> 00:13:55,870
about it in case of HBase there is no

00:13:53,470 --> 00:13:58,450
data type so it will actually give some

00:13:55,870 --> 00:13:59,770
auxiliary kind of functions to interpret

00:13:58,450 --> 00:14:01,810
the data that you are getting back from

00:13:59,770 --> 00:14:03,160
hitch base because if you purely query

00:14:01,810 --> 00:14:04,810
it from his page table you will get a

00:14:03,160 --> 00:14:07,180
bunch of bytes right that's all you'll

00:14:04,810 --> 00:14:08,890
get so your drill provides this convert

00:14:07,180 --> 00:14:11,290
to and convert from function so that he

00:14:08,890 --> 00:14:13,210
encode and decode the data so the goal

00:14:11,290 --> 00:14:15,760
of these features like direct queries on

00:14:13,210 --> 00:14:18,340
files and his pace years my business

00:14:15,760 --> 00:14:20,380
user is has come in he wants to ask a

00:14:18,340 --> 00:14:22,060
question on the log files maybe it's a

00:14:20,380 --> 00:14:23,830
one-time question he doesn't even need

00:14:22,060 --> 00:14:24,970
this information every day like it's not

00:14:23,830 --> 00:14:27,310
like a report that i need to put

00:14:24,970 --> 00:14:28,780
together so how do you help him answer

00:14:27,310 --> 00:14:30,670
this question and how do you do this

00:14:28,780 --> 00:14:34,620
quickly right that's the main goal

00:14:30,670 --> 00:14:37,390
allowing this exploration kind of usage

00:14:34,620 --> 00:14:39,490
the second part of it is the nested data

00:14:37,390 --> 00:14:41,290
so nested data is a first-class entity

00:14:39,490 --> 00:14:43,210
in drill right the whole drill

00:14:41,290 --> 00:14:46,000
architecture is designed ground up for

00:14:43,210 --> 00:14:49,060
nested data the in-memory representation

00:14:46,000 --> 00:14:51,760
for drills execution it is a columnar

00:14:49,060 --> 00:14:53,620
hierarchical model so you don't need to

00:14:51,760 --> 00:14:55,750
flatten this nested data at design time

00:14:53,620 --> 00:14:58,300
you don't need to the drill doesn't

00:14:55,750 --> 00:15:01,780
flatten this data at runtime so we are

00:14:58,300 --> 00:15:02,980
processing this data completely keeping

00:15:01,780 --> 00:15:06,240
it nested there is there are no

00:15:02,980 --> 00:15:08,290
inefficiencies at any point of time and

00:15:06,240 --> 00:15:10,810
I don't know how many of you are

00:15:08,290 --> 00:15:14,970
familiar with Google bigquery have you

00:15:10,810 --> 00:15:14,970
anybody here a little bit

00:15:15,020 --> 00:15:18,680
so they have edges on base data model as

00:15:16,880 --> 00:15:20,180
well they do require schema but they do

00:15:18,680 --> 00:15:22,190
provide some nice functions around

00:15:20,180 --> 00:15:24,980
nested data like count with in and

00:15:22,190 --> 00:15:27,020
flatten so drill provides some similar

00:15:24,980 --> 00:15:29,330
functions to operate on the nested data

00:15:27,020 --> 00:15:31,160
right so we have in roadmap to build a

00:15:29,330 --> 00:15:33,590
more broader comprehensive nested data

00:15:31,160 --> 00:15:35,810
API but in the B 1 dot 0 you will have

00:15:33,590 --> 00:15:37,550
simple scalar functions and simple kind

00:15:35,810 --> 00:15:39,950
of aggregated functions that you can do

00:15:37,550 --> 00:15:44,810
on nested data so here is an example

00:15:39,950 --> 00:15:47,510
where a hash based column has JSON data

00:15:44,810 --> 00:15:49,040
type and I am trying to get the value of

00:15:47,510 --> 00:15:51,590
it and count the number of children that

00:15:49,040 --> 00:15:52,670
is there any JSON document so i can do

00:15:51,590 --> 00:15:54,440
something like this i do not need to

00:15:52,670 --> 00:15:57,350
define anything anywhere in order to

00:15:54,440 --> 00:15:59,060
perform a query like this and drills

00:15:57,350 --> 00:16:01,610
design for nested data is also pretty

00:15:59,060 --> 00:16:03,680
broad it's not tied to json or anything

00:16:01,610 --> 00:16:06,800
it's basically a hierarchical data model

00:16:03,680 --> 00:16:08,660
so you can always in the b1 we are

00:16:06,800 --> 00:16:11,150
supporting JSON Parkway and txt file

00:16:08,660 --> 00:16:13,580
formats but it can always be extended to

00:16:11,150 --> 00:16:17,840
the subsequently for a more broader kind

00:16:13,580 --> 00:16:19,760
of hierarchical data formats as i

00:16:17,840 --> 00:16:21,530
mentioned flexible schema management

00:16:19,760 --> 00:16:23,720
doesn't mean just dynamic varies

00:16:21,530 --> 00:16:25,310
flexible schema management means giving

00:16:23,720 --> 00:16:27,380
the option to the users on what they

00:16:25,310 --> 00:16:29,540
want to do with the data if you don't

00:16:27,380 --> 00:16:31,340
have schema then you can query using the

00:16:29,540 --> 00:16:33,440
dynamic queries but you have already

00:16:31,340 --> 00:16:35,210
spent time to actually create the schema

00:16:33,440 --> 00:16:37,210
in hive you can leverage that with drill

00:16:35,210 --> 00:16:39,800
right so it's like a plug-and-play

00:16:37,210 --> 00:16:41,600
integration so you have the data rate

00:16:39,800 --> 00:16:43,490
tables and views defined in high metal

00:16:41,600 --> 00:16:46,670
store you can do sequel queries using

00:16:43,490 --> 00:16:48,560
drill on the high metadata right and

00:16:46,670 --> 00:16:51,320
this is one of the nice features i like

00:16:48,560 --> 00:16:53,510
drill is not tied to one hive meta store

00:16:51,320 --> 00:16:55,910
it's like a decentralized meta data

00:16:53,510 --> 00:16:58,790
model so you can have multiple hive meta

00:16:55,910 --> 00:17:00,580
stores H bases files and drill is like a

00:16:58,790 --> 00:17:02,990
distributed query engine on top of it

00:17:00,580 --> 00:17:05,930
right so you can query data from my hive

00:17:02,990 --> 00:17:07,339
test hype rod whatever like different

00:17:05,930 --> 00:17:09,589
different types of data sources you can

00:17:07,339 --> 00:17:12,730
query and do the sequel processing on it

00:17:09,589 --> 00:17:15,110
and return the data to your client and

00:17:12,730 --> 00:17:17,360
the drill supports all the different

00:17:15,110 --> 00:17:20,450
high file format so integrate using

00:17:17,360 --> 00:17:21,770
hypster DS so any high file formats even

00:17:20,450 --> 00:17:23,510
the custom formats that have been

00:17:21,770 --> 00:17:26,120
implemented for hive you can leverage

00:17:23,510 --> 00:17:27,470
with drill queries so what that means is

00:17:26,120 --> 00:17:28,940
you have do a data set

00:17:27,470 --> 00:17:30,980
they're in that particular format you

00:17:28,940 --> 00:17:33,200
have your tables defined in hive you can

00:17:30,980 --> 00:17:34,940
do low latency queries on the data using

00:17:33,200 --> 00:17:39,470
drill without having to worry about the

00:17:34,940 --> 00:17:41,720
data formats and any any any other

00:17:39,470 --> 00:17:43,520
things you have done with high right hi

00:17:41,720 --> 00:17:45,650
beauty ups is a pretty common thing to

00:17:43,520 --> 00:17:47,090
use so anything that you have built you

00:17:45,650 --> 00:17:49,640
can leverage in the drill queries as

00:17:47,090 --> 00:17:52,090
well so it's very closely and compatible

00:17:49,640 --> 00:17:54,260
with hype so you can consider

00:17:52,090 --> 00:17:56,270
complementing the hive environments for

00:17:54,260 --> 00:17:58,700
low-latency queries with drill right

00:17:56,270 --> 00:18:01,610
that's a common thing command request we

00:17:58,700 --> 00:18:03,350
hear the other aspect is cross data

00:18:01,610 --> 00:18:05,240
source queries this is like a side

00:18:03,350 --> 00:18:07,730
effect of the fact that drill has

00:18:05,240 --> 00:18:09,679
decentralized metadata model so you can

00:18:07,730 --> 00:18:11,600
bring together data from files eh pairs

00:18:09,679 --> 00:18:14,030
hive in a single query you don't need to

00:18:11,600 --> 00:18:15,950
do anything so here I am trying to

00:18:14,030 --> 00:18:18,500
combine the data for coming from a

00:18:15,950 --> 00:18:21,140
customer database with some social data

00:18:18,500 --> 00:18:22,970
that is coming from HBase and I am able

00:18:21,140 --> 00:18:24,440
to do that in a single query right

00:18:22,970 --> 00:18:31,610
without without defining anything

00:18:24,440 --> 00:18:33,860
upfront bi tool integration so for any

00:18:31,610 --> 00:18:35,990
sequel best tool you need to be able to

00:18:33,860 --> 00:18:39,049
connect to the BI tools so we do provide

00:18:35,990 --> 00:18:42,409
jdbc and whoa DBC drivers so drill has

00:18:39,049 --> 00:18:45,799
its own RPC format we don't support the

00:18:42,409 --> 00:18:49,039
drift yet but we do have a c++ client

00:18:45,799 --> 00:18:51,559
api so that is used to build the odbc

00:18:49,039 --> 00:18:53,960
driver on top of drill so you can use it

00:18:51,559 --> 00:18:55,640
from Excel MicroStrategy tour squirrels

00:18:53,960 --> 00:18:59,679
or whatever is the bi tool that you are

00:18:55,640 --> 00:18:59,679
comfortable with you can leverage drill

00:18:59,980 --> 00:19:05,240
sequel support this is just like the

00:19:02,870 --> 00:19:06,919
Hadoop ecosystem compatibility for drill

00:19:05,240 --> 00:19:09,230
sequel support is one of the primary

00:19:06,919 --> 00:19:11,720
design goals right so you want to be

00:19:09,230 --> 00:19:13,400
compatible with ansys equal that is kind

00:19:11,720 --> 00:19:15,679
of one of the poor things we started

00:19:13,400 --> 00:19:18,049
with so to that extent we do support the

00:19:15,679 --> 00:19:20,480
various sequel data types workers by

00:19:18,049 --> 00:19:23,570
about binary decimals doubles all the

00:19:20,480 --> 00:19:25,490
different sequel data types and most

00:19:23,570 --> 00:19:27,409
common sequel constructs are supported

00:19:25,490 --> 00:19:29,210
right so this is an again an area that

00:19:27,409 --> 00:19:31,400
needs to evolve over the time the sequel

00:19:29,210 --> 00:19:33,590
constructs but I just highlighted few

00:19:31,400 --> 00:19:35,630
things that we have in the one at a time

00:19:33,590 --> 00:19:38,600
frame so you can see that there is also

00:19:35,630 --> 00:19:40,429
some ddl included there so create table

00:19:38,600 --> 00:19:42,139
as insert into select

00:19:40,429 --> 00:19:45,799
so there are some there is some ddl

00:19:42,139 --> 00:19:47,690
functionality as well and information

00:19:45,799 --> 00:19:49,639
schema is ansi standard way of

00:19:47,690 --> 00:19:51,919
discovering the metadata within a

00:19:49,639 --> 00:19:53,480
database so you can ask what are the

00:19:51,919 --> 00:19:55,070
different schemas in it what are the

00:19:53,480 --> 00:19:56,509
different tables needs came out of the

00:19:55,070 --> 00:19:58,639
different views in each schema what are

00:19:56,509 --> 00:20:00,559
the columns in each table so you can get

00:19:58,639 --> 00:20:03,470
all the metadata information about the

00:20:00,559 --> 00:20:06,470
database using the information schema

00:20:03,470 --> 00:20:10,820
it's like a metadata database in inside

00:20:06,470 --> 00:20:14,360
drill and another key point that we are

00:20:10,820 --> 00:20:16,340
trying to make it possible is basically

00:20:14,360 --> 00:20:18,169
being able to execute the data that

00:20:16,340 --> 00:20:21,230
doesn't fit in the memory right we are

00:20:18,169 --> 00:20:25,070
talking about big data so drill has

00:20:21,230 --> 00:20:27,110
different types of join operators so we

00:20:25,070 --> 00:20:29,210
do have for example has joined we do

00:20:27,110 --> 00:20:31,820
have much join so there are situations

00:20:29,210 --> 00:20:35,149
if a particular query cannot be

00:20:31,820 --> 00:20:37,190
fulfilled by hash join we might choose

00:20:35,149 --> 00:20:38,960
to go with another type of join or we

00:20:37,190 --> 00:20:40,580
might choose the spooling to disk

00:20:38,960 --> 00:20:43,009
feature that is available with the hash

00:20:40,580 --> 00:20:44,990
join so basically the goal is to try to

00:20:43,009 --> 00:20:47,210
do it as much as possible in memory

00:20:44,990 --> 00:20:48,799
because it speeds up things but at some

00:20:47,210 --> 00:20:50,659
point if the data doesn't fit in memory

00:20:48,799 --> 00:20:52,369
you should be able to spill to disk but

00:20:50,659 --> 00:20:54,710
still complete the query right at the

00:20:52,369 --> 00:20:56,419
end of the day this is again i would say

00:20:54,710 --> 00:20:58,580
another area that will will evolve over

00:20:56,419 --> 00:21:00,710
time but in the one at a time frame we

00:20:58,580 --> 00:21:03,230
are looking at joins aggregation and

00:21:00,710 --> 00:21:06,049
sought to be able to spill to disk so

00:21:03,230 --> 00:21:10,669
those are the operators will try to fill

00:21:06,049 --> 00:21:12,379
spill to disk from very basic

00:21:10,669 --> 00:21:13,850
installation point of view it is

00:21:12,379 --> 00:21:15,619
designed to work on all hard of

00:21:13,850 --> 00:21:19,100
distributions no distribution specific

00:21:15,619 --> 00:21:20,929
features are utilized and another cool

00:21:19,100 --> 00:21:22,999
thing is is pretty easy to ramp up with

00:21:20,929 --> 00:21:25,369
drill right because it has this nice

00:21:22,999 --> 00:21:28,249
functionality to query files directly

00:21:25,369 --> 00:21:30,049
you can actually enter drill put it on

00:21:28,249 --> 00:21:32,779
your machine like in two minutes and

00:21:30,049 --> 00:21:34,309
then start querying the json text parka

00:21:32,779 --> 00:21:35,419
files that you have on your machine you

00:21:34,309 --> 00:21:36,769
don't need Hadoop you don't need

00:21:35,419 --> 00:21:40,039
zookeeper you don't need anything right

00:21:36,769 --> 00:21:46,549
this is a way to just get a sense of how

00:21:40,039 --> 00:21:48,529
drill works and under the hood drill is

00:21:46,549 --> 00:21:51,529
a distributed query engine so the

00:21:48,529 --> 00:21:53,250
typical query flow for drill is the

00:21:51,529 --> 00:21:57,240
query comes in it

00:21:53,250 --> 00:21:58,800
come from JDBC odbc or CLI and a drill

00:21:57,240 --> 00:22:00,570
bit is running on each node of the

00:21:58,800 --> 00:22:02,850
Hadoop cluster it is not a requirement

00:22:00,570 --> 00:22:04,710
but it is good so that you can get the

00:22:02,850 --> 00:22:06,960
data locality without having to move the

00:22:04,710 --> 00:22:11,190
data around so drill bit is the key

00:22:06,960 --> 00:22:13,380
service in drill and then any drill bit

00:22:11,190 --> 00:22:15,210
can take the query right every every

00:22:13,380 --> 00:22:17,310
drill bit has this artisan point it can

00:22:15,210 --> 00:22:20,070
access at it can access the request from

00:22:17,310 --> 00:22:22,980
the clients and once the request comes

00:22:20,070 --> 00:22:24,540
in it does a set of optimizations in

00:22:22,980 --> 00:22:26,520
order to determine what is the best way

00:22:24,540 --> 00:22:29,700
to execute this query right so we do

00:22:26,520 --> 00:22:31,890
support some cost based optimization in

00:22:29,700 --> 00:22:33,900
case of schema-less queries we do we do

00:22:31,890 --> 00:22:36,660
look at some sample data we do look at

00:22:33,900 --> 00:22:38,550
the file sizes so you try to basically

00:22:36,660 --> 00:22:40,860
manipulate the query so that it can be

00:22:38,550 --> 00:22:43,590
efficiently executed that's the goal and

00:22:40,860 --> 00:22:45,480
another part the optimization does is it

00:22:43,590 --> 00:22:48,360
looks at the data locality information

00:22:45,480 --> 00:22:51,060
right to look to see where to schedule

00:22:48,360 --> 00:22:53,310
this particular query execution on which

00:22:51,060 --> 00:22:54,930
notes to carry this this is a pretty

00:22:53,310 --> 00:22:57,390
standard stuff most of the sequel on

00:22:54,930 --> 00:23:00,000
Hadoop solutions do this so this is a

00:22:57,390 --> 00:23:03,180
standard distributed query engine that

00:23:00,000 --> 00:23:05,430
can handle this and the query is split

00:23:03,180 --> 00:23:07,770
into query fragments it is spread across

00:23:05,430 --> 00:23:09,660
the different nodes right so that you

00:23:07,770 --> 00:23:12,030
can scale out you complete the query

00:23:09,660 --> 00:23:14,490
execution you return the results to the

00:23:12,030 --> 00:23:16,200
driving or the coordinator node and the

00:23:14,490 --> 00:23:19,830
results are turned back to the client

00:23:16,200 --> 00:23:22,770
drill also has this hazel cast based

00:23:19,830 --> 00:23:25,470
distributed cache it's not a data cache

00:23:22,770 --> 00:23:27,720
it's like a metadata cash basically the

00:23:25,470 --> 00:23:30,630
query plan fragments some configuration

00:23:27,720 --> 00:23:34,770
information so some metadata basically

00:23:30,630 --> 00:23:36,900
is cached so that it can be leveraged by

00:23:34,770 --> 00:23:39,900
various nodes so we do use hazel cast

00:23:36,900 --> 00:23:42,690
for this reason and zookeeper helps with

00:23:39,900 --> 00:23:44,910
the cluster the nodes coordination

00:23:42,690 --> 00:23:48,690
determining the node health and pick a

00:23:44,910 --> 00:23:50,370
picking up the right set of nodes so

00:23:48,690 --> 00:23:53,220
these are the code modules within a

00:23:50,370 --> 00:23:55,770
drill bit so as I mentioned every drill

00:23:53,220 --> 00:23:57,660
bit has an RPC endpoint so it can accept

00:23:55,770 --> 00:23:59,460
the queries from the client it has a

00:23:57,660 --> 00:24:02,810
sequel parser so we love raise

00:23:59,460 --> 00:24:05,130
open-source optic for sequel parsing and

00:24:02,810 --> 00:24:05,940
the parsing would generate a logical

00:24:05,130 --> 00:24:08,100
plan

00:24:05,940 --> 00:24:10,470
optimizer kicks in it creates a physical

00:24:08,100 --> 00:24:13,679
plan and the execution is scheduled on

00:24:10,470 --> 00:24:16,679
various nodes of the cluster the storage

00:24:13,679 --> 00:24:19,559
engine interfaces are basically kind of

00:24:16,679 --> 00:24:21,029
the wrappers on top of data source these

00:24:19,559 --> 00:24:23,370
storage engine interfaces are

00:24:21,029 --> 00:24:25,860
responsible to provide the metadata of

00:24:23,370 --> 00:24:28,379
the data source they provide a reader

00:24:25,860 --> 00:24:29,879
and writer so that drill can communicate

00:24:28,379 --> 00:24:32,820
with the data sources using these

00:24:29,879 --> 00:24:35,370
interfaces it can expose information

00:24:32,820 --> 00:24:37,440
like for example hive storage engine it

00:24:35,370 --> 00:24:39,960
can say okay this is a partition table

00:24:37,440 --> 00:24:41,879
right so the drill can leverage the fact

00:24:39,960 --> 00:24:44,519
that it is a partition table and it can

00:24:41,879 --> 00:24:46,080
do optimizations appropriately so the

00:24:44,519 --> 00:24:47,820
storage engine interfaces are also

00:24:46,080 --> 00:24:49,830
responsible for exposing the

00:24:47,820 --> 00:24:55,259
optimization capabilities of the

00:24:49,830 --> 00:24:56,820
underlying data sources from a query

00:24:55,259 --> 00:24:59,009
engine standpoint I think this this

00:24:56,820 --> 00:25:00,779
particular topic itself is a separate

00:24:59,009 --> 00:25:03,779
park by itself you can find a lot of

00:25:00,779 --> 00:25:06,750
material on how drill does the execution

00:25:03,779 --> 00:25:09,210
on slideshare and youtube but at pretty

00:25:06,750 --> 00:25:11,309
high level there there are four key

00:25:09,210 --> 00:25:13,470
aspects the first one is the optimal

00:25:11,309 --> 00:25:15,269
stake out the pipelined execution so

00:25:13,470 --> 00:25:18,870
basically drill is designed for short

00:25:15,269 --> 00:25:21,149
queries so it doesn't assume that the

00:25:18,870 --> 00:25:23,009
failures are going to happen right so it

00:25:21,149 --> 00:25:25,200
doesn't plan for it it doesn't write to

00:25:23,009 --> 00:25:27,389
disk it doesn't create checkpoints like

00:25:25,200 --> 00:25:30,000
mapreduce which are more focused on the

00:25:27,389 --> 00:25:32,279
batch oriented paradigms and fault

00:25:30,000 --> 00:25:35,789
tolerance is critical whereas we drill

00:25:32,279 --> 00:25:37,950
it assumes failure but it's a failure

00:25:35,789 --> 00:25:39,629
then you have to restart the request you

00:25:37,950 --> 00:25:42,210
in the sense drill restarts the request

00:25:39,629 --> 00:25:44,340
not the user but so so most of the

00:25:42,210 --> 00:25:46,019
execution happens in memory unless there

00:25:44,340 --> 00:25:48,059
is memory war flow and you have to spill

00:25:46,019 --> 00:25:50,340
it to the disk and the data is moved

00:25:48,059 --> 00:25:52,350
from one stage to another in memory

00:25:50,340 --> 00:25:54,000
right there is no serialization to the

00:25:52,350 --> 00:25:57,690
disk which again can impact your

00:25:54,000 --> 00:26:00,299
latencies and response times the second

00:25:57,690 --> 00:26:02,879
aspect is the columnar execution the

00:26:00,299 --> 00:26:04,470
vectorization so drill averages some of

00:26:02,879 --> 00:26:06,059
I think the recent kind of researchers

00:26:04,470 --> 00:26:09,090
that are happening around this space

00:26:06,059 --> 00:26:11,460
there are probably pretty limited

00:26:09,090 --> 00:26:13,590
databases out there who does columnar

00:26:11,460 --> 00:26:15,389
execution I think vector wise is one

00:26:13,590 --> 00:26:15,929
company I know vertica I think does

00:26:15,389 --> 00:26:18,119
something

00:26:15,929 --> 00:26:20,549
this so the main idea is when you are

00:26:18,119 --> 00:26:22,289
reading the data from disk into drill in

00:26:20,549 --> 00:26:24,230
order to process it you do not

00:26:22,289 --> 00:26:27,389
immediately convert that into rows

00:26:24,230 --> 00:26:30,029
drills execution model is based on a

00:26:27,389 --> 00:26:32,039
columnar hierarchical model right so it

00:26:30,029 --> 00:26:34,619
can do all the sequel processing joins

00:26:32,039 --> 00:26:37,019
aggregations on this columnar structures

00:26:34,619 --> 00:26:39,840
so you don't materialize it into rows

00:26:37,019 --> 00:26:42,480
yet as sutra it tries to delay it as

00:26:39,840 --> 00:26:44,789
much as possible right so the benefit of

00:26:42,480 --> 00:26:46,649
that is the memory usage is lot more

00:26:44,789 --> 00:26:47,940
efficient right you are trying to manage

00:26:46,649 --> 00:26:49,769
this kind of columnar structure

00:26:47,940 --> 00:26:52,409
throughout your execution which means

00:26:49,769 --> 00:26:54,299
that you are consuming a lot less memory

00:26:52,409 --> 00:26:56,429
than what you could have with a row

00:26:54,299 --> 00:26:59,279
based execution the other part is

00:26:56,429 --> 00:27:02,639
vectorization so this is basically being

00:26:59,279 --> 00:27:05,159
able to process not one record at a time

00:27:02,639 --> 00:27:07,200
but doing the operation on a set of

00:27:05,159 --> 00:27:09,720
batch of records if you are trying to

00:27:07,200 --> 00:27:11,669
perform like a null check you you are

00:27:09,720 --> 00:27:13,559
doing it on a bunch of rows at a time

00:27:11,669 --> 00:27:15,539
you are not doing one at a time this is

00:27:13,559 --> 00:27:17,580
again like a standard research thing

00:27:15,539 --> 00:27:19,889
that is going on and the benefit of

00:27:17,580 --> 00:27:22,499
vectorization is the CPU efficiencies

00:27:19,889 --> 00:27:24,779
that you are getting so drills execution

00:27:22,499 --> 00:27:26,909
model is based on what is called record

00:27:24,779 --> 00:27:28,860
batches right so it's not like a one

00:27:26,909 --> 00:27:30,869
record but it's a bunch of Records you

00:27:28,860 --> 00:27:32,460
are trying to do an operation on it then

00:27:30,869 --> 00:27:34,649
you get your next record bash and you

00:27:32,460 --> 00:27:37,529
try to do the operation on it so you try

00:27:34,649 --> 00:27:39,690
to save the CPU and memory at the same

00:27:37,529 --> 00:27:43,110
time using these two things mixed

00:27:39,690 --> 00:27:44,580
together the next aspect late binding

00:27:43,110 --> 00:27:46,499
this is something I covered in the

00:27:44,580 --> 00:27:49,080
schema less execution it doesn't expect

00:27:46,499 --> 00:27:51,539
the schema all the sequel operators in

00:27:49,080 --> 00:27:54,450
drill they are prepared to handle the

00:27:51,539 --> 00:27:56,249
schema change right so for example if I

00:27:54,450 --> 00:27:58,230
am trying to filter a day to filter data

00:27:56,249 --> 00:28:00,690
on a particular column and the column

00:27:58,230 --> 00:28:02,610
data type changed or the course of query

00:28:00,690 --> 00:28:05,009
execution right you started getting more

00:28:02,610 --> 00:28:07,740
rows and you realize that okay my column

00:28:05,009 --> 00:28:09,600
has changed from into string right so

00:28:07,740 --> 00:28:12,240
the filter operation the filter operator

00:28:09,600 --> 00:28:13,769
is ready to handle it one way it can do

00:28:12,240 --> 00:28:15,360
is if it doesn't know what to do it can

00:28:13,769 --> 00:28:17,340
throw an error saying that I don't know

00:28:15,360 --> 00:28:18,929
how to handle this schema change the

00:28:17,340 --> 00:28:21,419
second thing it can do is okay the

00:28:18,929 --> 00:28:23,929
filtering is not happening on the column

00:28:21,419 --> 00:28:26,549
that has actually changed the data type

00:28:23,929 --> 00:28:28,169
right some some other column change the

00:28:26,549 --> 00:28:28,809
data type but not the filtered column

00:28:28,169 --> 00:28:31,179
itself so

00:28:28,809 --> 00:28:33,039
it can continue the execution this is

00:28:31,179 --> 00:28:35,559
again an ongoing evolution kind of an

00:28:33,039 --> 00:28:37,360
area but in the one at a time frame will

00:28:35,559 --> 00:28:39,399
make sure that some of the key operators

00:28:37,360 --> 00:28:42,490
can handle this kind of a schema change

00:28:39,399 --> 00:28:44,889
in a in a manner such that the user

00:28:42,490 --> 00:28:46,539
never gets wrong results like they might

00:28:44,889 --> 00:28:49,629
get failure but not a wrong result

00:28:46,539 --> 00:28:52,120
that's the goal and extensibility is

00:28:49,629 --> 00:28:54,539
another key point drill again design

00:28:52,120 --> 00:28:56,980
ground up for complete extensibility

00:28:54,539 --> 00:28:59,649
there are a bunch of integration points

00:28:56,980 --> 00:29:02,889
if you are looking at drill and looking

00:28:59,649 --> 00:29:05,409
to extend it one has udfs so it provides

00:29:02,889 --> 00:29:08,799
a Java API to create your ears and you

00:29:05,409 --> 00:29:11,799
das and then it's not tied to Hadoop

00:29:08,799 --> 00:29:14,350
right basically I'll show the storage

00:29:11,799 --> 00:29:16,149
engine interfaces it is getting all the

00:29:14,350 --> 00:29:18,700
information from the storage engine

00:29:16,149 --> 00:29:21,129
interface so you can always in implement

00:29:18,700 --> 00:29:22,990
a mango or a cousin or whatever there is

00:29:21,129 --> 00:29:26,049
a no sequel type of interface or Oracle

00:29:22,990 --> 00:29:27,700
my sequel interface and implement those

00:29:26,049 --> 00:29:30,429
API is provide the information it needs

00:29:27,700 --> 00:29:32,080
so there is no Hadoop tying it to Hadoop

00:29:30,429 --> 00:29:36,960
kind of an architecture at any at any

00:29:32,080 --> 00:29:39,399
point of the stack the third aspect is

00:29:36,960 --> 00:29:40,840
the storage engines can be extensible

00:29:39,399 --> 00:29:44,529
the functions and operators are

00:29:40,840 --> 00:29:46,330
extensible and at basically we have the

00:29:44,529 --> 00:29:47,950
ability to look at the classpath and

00:29:46,330 --> 00:29:49,779
basically dynamically load them right

00:29:47,950 --> 00:29:51,610
rather than users have to configure them

00:29:49,779 --> 00:29:54,519
or a create wrapper in them you have

00:29:51,610 --> 00:29:57,850
ability to automatically detect that

00:29:54,519 --> 00:29:59,769
dynamically and load it and another k

00:29:57,850 --> 00:30:02,350
thing this we find it pretty handy in

00:29:59,769 --> 00:30:05,049
testing is your logical plan and

00:30:02,350 --> 00:30:07,960
physical plan they have fixed JSON

00:30:05,049 --> 00:30:09,970
structures so we do generate this

00:30:07,960 --> 00:30:11,499
structure and drill execute it but if

00:30:09,970 --> 00:30:13,749
you want to implement your own query

00:30:11,499 --> 00:30:15,970
layer not sequel I don't know some

00:30:13,749 --> 00:30:18,009
QR a lot some other ql that you want to

00:30:15,970 --> 00:30:19,929
implement as long as you can generate

00:30:18,009 --> 00:30:21,549
this logical and physical plans you can

00:30:19,929 --> 00:30:24,100
execute them using the distributed

00:30:21,549 --> 00:30:25,809
engine capabilities of drill right so I

00:30:24,100 --> 00:30:28,929
just wanted to highlight this because I

00:30:25,809 --> 00:30:30,549
see requests on the drill mailing list

00:30:28,929 --> 00:30:32,559
okay does it have a connector does

00:30:30,549 --> 00:30:34,840
it have a cassandra connector so the

00:30:32,559 --> 00:30:36,580
architecture is built for that right so

00:30:34,840 --> 00:30:38,179
if we want to extend that it still it's

00:30:36,580 --> 00:30:42,299
possible to do that

00:30:38,179 --> 00:30:43,950
so where we are it's heavy active

00:30:42,299 --> 00:30:46,380
development there is a significant

00:30:43,950 --> 00:30:49,380
momentum from the communique community

00:30:46,380 --> 00:30:53,159
and the the mailing lists are pretty

00:30:49,380 --> 00:30:54,990
active and we had recent meet up in the

00:30:53,159 --> 00:30:57,690
Bay Area and we'll be having another

00:30:54,990 --> 00:30:59,490
meetup in a couple of months but there

00:30:57,690 --> 00:31:01,950
is a lot of good interest and momentum

00:30:59,490 --> 00:31:04,590
around around a project current state is

00:31:01,950 --> 00:31:08,700
alpha the plan is to have a bond out of

00:31:04,590 --> 00:31:10,049
beta out by end of q2 and it all depends

00:31:08,700 --> 00:31:11,970
on the feedback but the current thinking

00:31:10,049 --> 00:31:13,620
is that maybe we will have like eight to

00:31:11,970 --> 00:31:16,049
ten we kind of a better duration and

00:31:13,620 --> 00:31:18,690
then product can go ga in the one dot in

00:31:16,049 --> 00:31:20,220
the q3 like around September kind of a

00:31:18,690 --> 00:31:24,710
timeframe september-october kind of

00:31:20,220 --> 00:31:28,169
timeframe so this is a high-level

00:31:24,710 --> 00:31:29,460
roadmap so one berto is basically covers

00:31:28,169 --> 00:31:34,100
all the key features that i just

00:31:29,460 --> 00:31:36,690
mentioned and in one dot one looking to

00:31:34,100 --> 00:31:38,970
spend more time on optimizing the

00:31:36,690 --> 00:31:40,559
performance on top of HBase so there are

00:31:38,970 --> 00:31:42,929
some pushed on functionality that is

00:31:40,559 --> 00:31:45,210
coming in the one at a time frame but we

00:31:42,929 --> 00:31:47,429
want to expand on that and do lot more

00:31:45,210 --> 00:31:50,549
optimizations around HBase and in

00:31:47,429 --> 00:31:53,070
general optimizations as well and then

00:31:50,549 --> 00:31:55,260
we want to provide a comprehensive API

00:31:53,070 --> 00:31:57,450
for the nested data so drill has this

00:31:55,260 --> 00:31:58,799
amazing architecture for nested data we

00:31:57,450 --> 00:32:00,870
want to make sure that it is available

00:31:58,799 --> 00:32:02,850
for the users to actually leverage that

00:32:00,870 --> 00:32:04,500
so that will come in and as well and

00:32:02,850 --> 00:32:06,090
some of their standard enterprise

00:32:04,500 --> 00:32:08,760
features like yarn integration and

00:32:06,090 --> 00:32:12,000
security I just put in a version number

00:32:08,760 --> 00:32:14,370
so you will have a path to go but there

00:32:12,000 --> 00:32:16,320
is no nothing hard hard coded or

00:32:14,370 --> 00:32:19,380
anything for the numbers of the versions

00:32:16,320 --> 00:32:20,760
and eventually the idea is have a single

00:32:19,380 --> 00:32:22,350
query layer that can tackle the

00:32:20,760 --> 00:32:24,690
analytics and the operational kind of

00:32:22,350 --> 00:32:27,299
use case when I say operation here it's

00:32:24,690 --> 00:32:30,870
more like user-facing applications more

00:32:27,299 --> 00:32:32,309
point queries so this is this is this is

00:32:30,870 --> 00:32:34,980
the idea like 10 millisecond kind of

00:32:32,309 --> 00:32:37,620
response time things is what we want to

00:32:34,980 --> 00:32:38,700
go after and nicely there are a bunch of

00:32:37,620 --> 00:32:41,549
products we were trying to do the

00:32:38,700 --> 00:32:43,200
similar things tref Orion by HP is one

00:32:41,549 --> 00:32:46,179
of them Phoenix is trying to do the

00:32:43,200 --> 00:32:48,490
operational as well so we will be

00:32:46,179 --> 00:32:52,360
to have conversations to collaborate

00:32:48,490 --> 00:32:54,610
with those teams as well so if you are

00:32:52,360 --> 00:32:57,369
interested in Apache drill join the

00:32:54,610 --> 00:32:59,440
community there are two mailing list

00:32:57,369 --> 00:33:01,629
that you can subscribe to and several

00:32:59,440 --> 00:33:03,779
ways to contribute one of the key things

00:33:01,629 --> 00:33:06,190
is now that we are approaching beta

00:33:03,779 --> 00:33:07,720
writers will be very valuable if there

00:33:06,190 --> 00:33:09,340
are some queries that you can share so

00:33:07,720 --> 00:33:11,710
that we can try out and make sure drill

00:33:09,340 --> 00:33:13,749
wats with them and there is a lot of

00:33:11,710 --> 00:33:17,590
documentation that you can look at as

00:33:13,749 --> 00:33:21,909
well so with that I will experiment with

00:33:17,590 --> 00:33:26,340
my demo and we will see how it goes any

00:33:21,909 --> 00:33:26,340
questions meanwhile I bring up this

00:33:30,559 --> 00:33:36,490
I don't even know how do you zoom this

00:33:33,490 --> 00:33:36,490
bigger

00:33:41,190 --> 00:33:45,510
okay so I just executed the queerest few

00:33:43,650 --> 00:33:48,270
minutes back it all seemed fine so let

00:33:45,510 --> 00:33:50,970
us try it out otherwise you will get a

00:33:48,270 --> 00:33:53,240
chance to try it out yourself so that's

00:33:50,970 --> 00:33:53,240
a good thing

00:34:01,549 --> 00:34:06,830
okay so what I want to show you is few

00:34:04,309 --> 00:34:09,409
queries running on files a couple of

00:34:06,830 --> 00:34:11,119
queries running on hive and a couple of

00:34:09,409 --> 00:34:13,819
queries running on HBase right that does

00:34:11,119 --> 00:34:17,270
the main thing I want to show so I have

00:34:13,819 --> 00:34:23,530
written my queries down here so the

00:34:17,270 --> 00:34:23,530
first thing is a very simple JSON file

00:34:25,569 --> 00:34:30,500
it just as one record I wanted to show

00:34:28,129 --> 00:34:32,450
how the columns are returned so it has a

00:34:30,500 --> 00:34:34,639
contact name account email like how

00:34:32,450 --> 00:34:36,649
address street so if there are complex

00:34:34,639 --> 00:34:38,419
types basically you will get back a JSON

00:34:36,649 --> 00:34:40,190
and you need to use further functions in

00:34:38,419 --> 00:34:43,899
order to operate on them right so you

00:34:40,190 --> 00:34:46,909
are basically querying a file here and

00:34:43,899 --> 00:34:48,919
we can try out I think a couple of

00:34:46,909 --> 00:34:51,049
examples here which I showed in the

00:34:48,919 --> 00:34:53,149
slide deck so basically these examples

00:34:51,049 --> 00:34:56,419
that I am going to run now or based on

00:34:53,149 --> 00:34:58,880
the standard t PCH type of data but

00:34:56,419 --> 00:35:00,109
instead of creating a hive schema for it

00:34:58,880 --> 00:35:05,510
I am just running off of the files

00:35:00,109 --> 00:35:08,210
directly so this is going after a

00:35:05,510 --> 00:35:11,960
customer Park a file that i have on my

00:35:08,210 --> 00:35:15,650
Hadoop file system it has around 15,000

00:35:11,960 --> 00:35:17,660
records here and my orders has 150,000

00:35:15,650 --> 00:35:20,869
records and then I am trying to do a

00:35:17,660 --> 00:35:23,210
join on this two tables and trying to

00:35:20,869 --> 00:35:25,250
determine who are who which customers

00:35:23,210 --> 00:35:27,049
have the highest order totals write a

00:35:25,250 --> 00:35:30,380
simple join query that I'm trying to do

00:35:27,049 --> 00:35:32,630
here so you get back some results which

00:35:30,380 --> 00:35:36,770
is what you would expect now if i create

00:35:32,630 --> 00:35:39,619
a hive schema on top of this files right

00:35:36,770 --> 00:35:41,809
so just for customer parking i create a

00:35:39,619 --> 00:35:43,970
customer table for orders i create an

00:35:41,809 --> 00:35:49,369
order table and i try to do the same

00:35:43,970 --> 00:35:51,559
exact where is using hive same queries

00:35:49,369 --> 00:35:58,010
just the table names are replaced with

00:35:51,559 --> 00:36:00,589
the hive so one thing to notice here is

00:35:58,010 --> 00:36:02,390
the hive that I mentioned is basically

00:36:00,589 --> 00:36:03,130
the storage engine instance that is

00:36:02,390 --> 00:36:05,920
registered

00:36:03,130 --> 00:36:07,960
drill so it simply points to the thrift

00:36:05,920 --> 00:36:10,780
you are a of 5 along with the port

00:36:07,960 --> 00:36:13,150
that's all it is right and you can

00:36:10,780 --> 00:36:14,800
represent that using dot notation so

00:36:13,150 --> 00:36:16,720
that you can put all these things in one

00:36:14,800 --> 00:36:21,010
query and you can do like a cross data

00:36:16,720 --> 00:36:24,940
source query so the you got the five

00:36:21,010 --> 00:36:26,910
customer say 3620 695 i-695 wait pretty

00:36:24,940 --> 00:36:29,680
similar to what we got for the hype

00:36:26,910 --> 00:36:31,540
right so it is just highlighting you can

00:36:29,680 --> 00:36:35,760
query some of this data without actually

00:36:31,540 --> 00:36:40,210
having to define a scheme as upfront and

00:36:35,760 --> 00:36:42,960
HBase it is just started working so I am

00:36:40,210 --> 00:36:45,580
demonstrating a very simple feature here

00:36:42,960 --> 00:36:48,250
so I'm wearing his pace user profile

00:36:45,580 --> 00:36:50,860
right it's a simple table it's in his

00:36:48,250 --> 00:36:53,260
pace directly I am querying it and I got

00:36:50,860 --> 00:36:56,110
a bunch of bytes because HPS does not

00:36:53,260 --> 00:36:58,660
have any data types so as a user you can

00:36:56,110 --> 00:37:00,910
use convert functions which I haven't

00:36:58,660 --> 00:37:05,160
used or you can use the cast functions

00:37:00,910 --> 00:37:08,170
in order to cast this data into whatever

00:37:05,160 --> 00:37:11,110
the data type that you would like to see

00:37:08,170 --> 00:37:13,150
the data in right so basically it is

00:37:11,110 --> 00:37:14,650
pretty it's pretty handy when you have a

00:37:13,150 --> 00:37:16,240
bunch of data you are trying to make it

00:37:14,650 --> 00:37:18,400
available to the users to explore the

00:37:16,240 --> 00:37:20,350
data negative and a kit and start

00:37:18,400 --> 00:37:22,060
running queries right away at some point

00:37:20,350 --> 00:37:24,370
if the queries seem to be repeatable

00:37:22,060 --> 00:37:26,020
they seem to be valuable to share with

00:37:24,370 --> 00:37:28,660
other users then you can consider

00:37:26,020 --> 00:37:32,950
creating schemas on it that's the goal

00:37:28,660 --> 00:37:35,200
of drill so any any questions it's an

00:37:32,950 --> 00:37:37,660
interesting project lot of what to do so

00:37:35,200 --> 00:37:39,160
appreciate contributions and you are

00:37:37,660 --> 00:37:44,730
welcome to join the mailing list ask

00:37:39,160 --> 00:37:44,730
questions provide feedback thank you

00:37:51,440 --> 00:37:59,780
can one of the data sources be just a

00:37:55,940 --> 00:38:02,270
standard JDBC data source so you could

00:37:59,780 --> 00:38:06,079
potentially join relational data

00:38:02,270 --> 00:38:07,490
together with it yeah interesting so

00:38:06,079 --> 00:38:08,960
basically it has to implement the

00:38:07,490 --> 00:38:10,579
storage engine interface that I

00:38:08,960 --> 00:38:12,710
mentioned as long as it can give

00:38:10,579 --> 00:38:14,480
metadata it can be similar to one of the

00:38:12,710 --> 00:38:17,420
previous talks we heard right it exposes

00:38:14,480 --> 00:38:19,160
a standard API you give the metadata you

00:38:17,420 --> 00:38:20,810
give like what to use for reading what

00:38:19,160 --> 00:38:24,170
you use for writing and then you are off

00:38:20,810 --> 00:38:27,349
to go right and so it's the data is all

00:38:24,170 --> 00:38:29,270
in Hadoop for now but theoretically like

00:38:27,349 --> 00:38:32,060
it's like a federation kind of a concept

00:38:29,270 --> 00:38:33,589
but because it's all in Hadoop you are

00:38:32,060 --> 00:38:36,849
able to actually leverage that and

00:38:33,589 --> 00:38:39,109
provide more faster performance but

00:38:36,849 --> 00:38:41,869
product why is it can certainly do that

00:38:39,109 --> 00:38:44,599
it's not part of eccentric and then

00:38:41,869 --> 00:38:47,000
another question you you showed some

00:38:44,599 --> 00:38:48,710
create table and create view statements

00:38:47,000 --> 00:38:51,800
that you could execute what does that

00:38:48,710 --> 00:38:53,839
meta data go when you execute those so a

00:38:51,800 --> 00:38:55,819
drill doesn't have a meta data store it

00:38:53,839 --> 00:38:57,290
is not yet right so in the one at a time

00:38:55,819 --> 00:38:59,660
frame and that's the goal like have a

00:38:57,290 --> 00:39:01,400
decentralized meta data model so any

00:38:59,660 --> 00:39:03,680
metadata that you create you have to

00:39:01,400 --> 00:39:05,540
point it to one of the storage engines

00:39:03,680 --> 00:39:07,790
right so you can create a table in hive

00:39:05,540 --> 00:39:10,400
you can create a table actually as a

00:39:07,790 --> 00:39:12,020
file right because it's ultimately data

00:39:10,400 --> 00:39:14,420
you are trying to mashing together data

00:39:12,020 --> 00:39:15,920
and you are putting it so we are talking

00:39:14,420 --> 00:39:18,020
about actually this might not come in

00:39:15,920 --> 00:39:19,970
one at a time frame but you will have

00:39:18,020 --> 00:39:23,060
this more of a distributed little

00:39:19,970 --> 00:39:25,520
metadata if the target is a filesystem

00:39:23,060 --> 00:39:26,869
right so you might have something like I

00:39:25,520 --> 00:39:30,680
don't know dot drill file or something

00:39:26,869 --> 00:39:32,240
like that right or you can choose not to

00:39:30,680 --> 00:39:35,900
have the metadata itself just purchase

00:39:32,240 --> 00:39:38,180
the data only like as a park FL so but

00:39:35,900 --> 00:39:45,310
you always specify a storage engine when

00:39:38,180 --> 00:39:45,310
you are executing ddl any question yeah

00:39:49,350 --> 00:39:56,890
if there are different data sources how

00:39:53,020 --> 00:40:03,280
how do you define and dro there is a

00:39:56,890 --> 00:40:08,470
simple JSON file it's pretty easy it's a

00:40:03,280 --> 00:40:14,430
very this is a sequel line jdbc told

00:40:08,470 --> 00:40:14,430
matt i just used so you can go to conf

00:40:14,940 --> 00:40:20,560
something like this so if I'm varying

00:40:18,270 --> 00:40:22,630
filesystem a local file system you can

00:40:20,560 --> 00:40:24,280
just simply like specify like this if I

00:40:22,630 --> 00:40:26,920
am specifying hive then I am specifying

00:40:24,280 --> 00:40:29,530
a trip to RI so every basically storage

00:40:26,920 --> 00:40:32,500
engine tells like how to connect right

00:40:29,530 --> 00:40:35,230
for HBase is a zookeeper quorum so you

00:40:32,500 --> 00:40:37,150
define how you want to be connected for

00:40:35,230 --> 00:40:43,780
a JDBC a typical type of data source

00:40:37,150 --> 00:40:49,330
it's a JDBC URL that you specify and the

00:40:43,780 --> 00:40:51,790
driver gone for like how how somebody

00:40:49,330 --> 00:40:54,760
will in case of jdbc how howell's

00:40:51,790 --> 00:40:56,200
somebody can figure the driver we

00:40:54,760 --> 00:40:58,150
haven't thought about it but it's

00:40:56,200 --> 00:41:00,250
certainly a configuration thing right so

00:40:58,150 --> 00:41:01,930
you can specify it I would you need to

00:41:00,250 --> 00:41:08,440
have that and yeah you have that in

00:41:01,930 --> 00:41:11,250
order to talk / / + per person any other

00:41:08,440 --> 00:41:11,250
questions

00:41:12,650 --> 00:41:15,849

YouTube URL: https://www.youtube.com/watch?v=jaLObfrl7Lk


