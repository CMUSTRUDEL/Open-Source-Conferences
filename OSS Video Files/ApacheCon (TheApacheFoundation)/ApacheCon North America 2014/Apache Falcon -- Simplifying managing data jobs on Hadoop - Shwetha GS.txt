Title: Apache Falcon -- Simplifying managing data jobs on Hadoop - Shwetha GS
Publication date: 2014-04-22
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:01,129 --> 00:00:07,380
hi I'm Rita today I'm going to talk

00:00:05,160 --> 00:00:09,389
about a party Falcon which is basically

00:00:07,380 --> 00:00:15,599
a data management platform on top of

00:00:09,389 --> 00:00:19,669
Hadoop but me I'm being within mobile

00:00:15,599 --> 00:00:23,369
for the last little more than two years

00:00:19,669 --> 00:00:27,539
I've worked on the data management

00:00:23,369 --> 00:00:30,119
efforts at in mobile and also committed

00:00:27,539 --> 00:00:32,390
at Apache Falcon and I've also

00:00:30,119 --> 00:00:35,280
contributed to Apache Vusi as well

00:00:32,390 --> 00:00:38,190
before in Mobius with Amazon for almost

00:00:35,280 --> 00:00:39,809
five years so working on a imagine

00:00:38,190 --> 00:00:44,640
Amazon payment service which is a web

00:00:39,809 --> 00:00:48,030
service and in Amazon this is the agenda

00:00:44,640 --> 00:00:50,699
for today first I will explain about the

00:00:48,030 --> 00:00:54,210
motivation because to understand and

00:00:50,699 --> 00:00:56,359
appreciate any any solution right you

00:00:54,210 --> 00:00:59,219
need to understand the challenges and

00:00:56,359 --> 00:01:00,960
why we did it right so I'm going to take

00:00:59,219 --> 00:01:02,969
some time and explain the motivation for

00:01:00,960 --> 00:01:06,960
it and then I'll give a brief overview

00:01:02,969 --> 00:01:08,880
of Falcon we will go through a few case

00:01:06,960 --> 00:01:11,729
studies where we have used Falcon on

00:01:08,880 --> 00:01:18,689
production systems and at the end we can

00:01:11,729 --> 00:01:21,570
take questions and answers coming to the

00:01:18,689 --> 00:01:23,700
motivation this is the typical data

00:01:21,570 --> 00:01:24,750
processing landscape on top of Hadoop

00:01:23,700 --> 00:01:27,119
right

00:01:24,750 --> 00:01:29,759
you have Hadoop cluster and there is

00:01:27,119 --> 00:01:33,180
data which comes into the Hadoop cluster

00:01:29,759 --> 00:01:35,070
the data can stream from daraa from the

00:01:33,180 --> 00:01:37,979
application servers into the Hadoop

00:01:35,070 --> 00:01:39,600
cluster or a slow changing data like the

00:01:37,979 --> 00:01:42,060
data in the DB or things like that

00:01:39,600 --> 00:01:45,810
you can import the data into the Hadoop

00:01:42,060 --> 00:01:50,100
cluster and once the data is available

00:01:45,810 --> 00:01:52,939
on the cluster you you basically design

00:01:50,100 --> 00:01:55,170
data pipelines which transform the data

00:01:52,939 --> 00:01:57,000
into the data that you want for

00:01:55,170 --> 00:02:00,750
reporting systems or for feedback

00:01:57,000 --> 00:02:03,869
systems or for your prediction or

00:02:00,750 --> 00:02:05,850
anything right and there is a complexity

00:02:03,869 --> 00:02:08,300
involved in designing the data pipeline

00:02:05,850 --> 00:02:10,740
itself you need to handle the various

00:02:08,300 --> 00:02:13,020
different stages of the data processing

00:02:10,740 --> 00:02:13,220
you need to take care of any failures in

00:02:13,020 --> 00:02:15,050
the

00:02:13,220 --> 00:02:17,930
depressing and you need to rerun them

00:02:15,050 --> 00:02:20,660
and things like that and once the data

00:02:17,930 --> 00:02:22,700
is available data is processed and it's

00:02:20,660 --> 00:02:24,290
available on the Hadoop cluster you need

00:02:22,700 --> 00:02:26,510
to even worry about the eviction of the

00:02:24,290 --> 00:02:28,220
data or the archival of the data because

00:02:26,510 --> 00:02:30,470
you can't keep the data forever on the

00:02:28,220 --> 00:02:32,990
Hadoop cluster as it might cause

00:02:30,470 --> 00:02:35,600
instability in the cluster as well so

00:02:32,990 --> 00:02:38,300
once the data is available on the

00:02:35,600 --> 00:02:40,790
cluster it it's you might want to even

00:02:38,300 --> 00:02:44,390
replicate the data for BCP reasons or

00:02:40,790 --> 00:02:45,920
for any business reasons as well so so

00:02:44,390 --> 00:02:48,530
there are different functionalities that

00:02:45,920 --> 00:02:50,720
are that are required over here right

00:02:48,530 --> 00:02:52,640
there is a import that is eviction there

00:02:50,720 --> 00:02:56,209
is archival there is replication and the

00:02:52,640 --> 00:02:58,070
data transformation so instead of every

00:02:56,209 --> 00:03:00,080
data processing team it needs to worry

00:02:58,070 --> 00:03:02,420
about all these all these can be

00:03:00,080 --> 00:03:05,750
abstracted out into a platform service

00:03:02,420 --> 00:03:07,850
so that not everyone has to worry about

00:03:05,750 --> 00:03:10,250
these basic stuffs if all these are

00:03:07,850 --> 00:03:13,580
available as a platform in for you and

00:03:10,250 --> 00:03:15,800
the data processing team can work on the

00:03:13,580 --> 00:03:17,660
business implementation of the data

00:03:15,800 --> 00:03:20,180
pipelines that are required so which is

00:03:17,660 --> 00:03:22,580
more important as compared to all that

00:03:20,180 --> 00:03:25,209
so instead of everybody having to worry

00:03:22,580 --> 00:03:28,400
about all these things if we if we

00:03:25,209 --> 00:03:30,440
hacked out all these into a platform it

00:03:28,400 --> 00:03:32,330
gives us two advantages one is the

00:03:30,440 --> 00:03:34,190
processing team doesn't have to worry

00:03:32,330 --> 00:03:36,890
about these basic stuff and the other

00:03:34,190 --> 00:03:39,140
one is if it's a platform it's quite

00:03:36,890 --> 00:03:41,720
possible you can use best practices and

00:03:39,140 --> 00:03:47,269
implement them generally in the platform

00:03:41,720 --> 00:03:49,070
itself so these are the typical core

00:03:47,269 --> 00:03:52,549
services that are required for any data

00:03:49,070 --> 00:03:55,880
management you have the process relay

00:03:52,549 --> 00:03:57,799
switch which basically about how you

00:03:55,880 --> 00:04:00,500
process the data in different stages

00:03:57,799 --> 00:04:03,380
there's lineage information data lineage

00:04:00,500 --> 00:04:05,239
with char which basically looks at how

00:04:03,380 --> 00:04:08,000
the data is transformed at each stage

00:04:05,239 --> 00:04:10,100
this data anonymization and there's

00:04:08,000 --> 00:04:12,170
operability around the data and how do

00:04:10,100 --> 00:04:14,000
you detect if the data comes in late

00:04:12,170 --> 00:04:16,030
which is the late data management

00:04:14,000 --> 00:04:17,870
there's retention replication and

00:04:16,030 --> 00:04:20,209
acquisition and things like that

00:04:17,870 --> 00:04:24,020
and explain each of these has a separate

00:04:20,209 --> 00:04:26,190
service as we move along so this is

00:04:24,020 --> 00:04:27,780
basically a typical how you

00:04:26,190 --> 00:04:29,640
have different stages of the data

00:04:27,780 --> 00:04:31,170
processing in the system right there is

00:04:29,640 --> 00:04:34,620
the raw data that comes into the system

00:04:31,170 --> 00:04:37,170
and you you have different layers of

00:04:34,620 --> 00:04:40,320
processing on top of it and finally you

00:04:37,170 --> 00:04:42,690
have the data which is available at the

00:04:40,320 --> 00:04:46,170
end which is basically the reports or

00:04:42,690 --> 00:04:48,240
anything for that matter and which are

00:04:46,170 --> 00:04:52,200
the final stages of your data pipeline

00:04:48,240 --> 00:04:55,110
all this data the last stage of the data

00:04:52,200 --> 00:04:57,090
is pretty SLA critical so which means

00:04:55,110 --> 00:04:59,010
that you need to make sure that each of

00:04:57,090 --> 00:05:01,110
these stages of the pipeline's run as

00:04:59,010 --> 00:05:05,250
soon as the data before that is

00:05:01,110 --> 00:05:07,470
available right and some systems like

00:05:05,250 --> 00:05:08,730
kuzey Apache who see already take care

00:05:07,470 --> 00:05:11,700
of that the gate on the data being

00:05:08,730 --> 00:05:13,200
available and automatically spawn a data

00:05:11,700 --> 00:05:15,600
process the next stage of the data

00:05:13,200 --> 00:05:18,750
processing what you also need to take a

00:05:15,600 --> 00:05:20,550
over here is what happens if there is

00:05:18,750 --> 00:05:22,380
any failure in the first stage of the

00:05:20,550 --> 00:05:24,300
data so you need to make sure that you

00:05:22,380 --> 00:05:26,280
read on the that first stage and make

00:05:24,300 --> 00:05:30,450
sure that all the further stages are

00:05:26,280 --> 00:05:31,980
also run smoothly so there are different

00:05:30,450 --> 00:05:34,350
challenges that come into picture when

00:05:31,980 --> 00:05:39,390
you have the data pipelines designed

00:05:34,350 --> 00:05:43,230
this way another thing is the lay data

00:05:39,390 --> 00:05:45,930
management typically there are two ways

00:05:43,230 --> 00:05:48,990
of consuming the data right this can be

00:05:45,930 --> 00:05:51,330
human consumption or it's it's a mess a

00:05:48,990 --> 00:05:54,690
machine consumption so a machine

00:05:51,330 --> 00:05:56,190
consumption the typically of it it works

00:05:54,690 --> 00:05:58,650
with the data that's available it

00:05:56,190 --> 00:06:02,700
doesn't wait for the complete set of

00:05:58,650 --> 00:06:04,980
data being available for example assume

00:06:02,700 --> 00:06:07,890
that there is a person who is looking at

00:06:04,980 --> 00:06:10,050
all the what happened yesterday so all

00:06:07,890 --> 00:06:13,050
the data that was generated yesterday

00:06:10,050 --> 00:06:17,160
what happened he wants to analyze the

00:06:13,050 --> 00:06:18,660
report on that so because the data flows

00:06:17,160 --> 00:06:20,910
through various systems it's quite

00:06:18,660 --> 00:06:24,540
possible the data gets delayed and it

00:06:20,910 --> 00:06:26,190
comes in late so for a human consumption

00:06:24,540 --> 00:06:28,020
they need to make sure that the data is

00:06:26,190 --> 00:06:31,200
complete before they look at the reports

00:06:28,020 --> 00:06:33,240
so it's important that we detect when

00:06:31,200 --> 00:06:35,940
the glade data comes and read on the

00:06:33,240 --> 00:06:37,770
rear and the reports so that the data is

00:06:35,940 --> 00:06:39,260
complete so it's not about really

00:06:37,770 --> 00:06:40,760
running just one step in

00:06:39,260 --> 00:06:45,880
data pipeline you need to make sure that

00:06:40,760 --> 00:06:48,410
the whole related data pipeline is run

00:06:45,880 --> 00:06:50,000
if we go back to the previous one so

00:06:48,410 --> 00:06:51,020
assume that the lady ADA comes in the

00:06:50,000 --> 00:06:53,240
first stage you need to make sure that

00:06:51,020 --> 00:06:55,580
all the dependent pipelines as well are

00:06:53,240 --> 00:06:57,110
rerun and that becomes very important

00:06:55,580 --> 00:07:03,170
when you want to make sure that there is

00:06:57,110 --> 00:07:05,180
complete set of data that's available so

00:07:03,170 --> 00:07:06,830
what is important that is you need to

00:07:05,180 --> 00:07:08,080
detect when there is late date and you

00:07:06,830 --> 00:07:10,370
need to make sure that you rerun

00:07:08,080 --> 00:07:13,780
everything that depends on the data all

00:07:10,370 --> 00:07:18,200
the pipelines that depend on the data

00:07:13,780 --> 00:07:21,230
other thing is the data retention data

00:07:18,200 --> 00:07:23,120
retention so when you have huge amount

00:07:21,230 --> 00:07:27,350
of data coming into the cluster you need

00:07:23,120 --> 00:07:30,410
to make sure that you you delete the

00:07:27,350 --> 00:07:32,870
data which you don't want so various

00:07:30,410 --> 00:07:35,150
different types of data may be retained

00:07:32,870 --> 00:07:38,120
for different duration for example a raw

00:07:35,150 --> 00:07:40,280
data that comes in may be required to be

00:07:38,120 --> 00:07:42,680
retain for almost three years for audit

00:07:40,280 --> 00:07:45,800
purposes or if you want to make sure for

00:07:42,680 --> 00:07:47,570
BCP and things like that and a summary a

00:07:45,800 --> 00:07:49,190
report that's generated at the end of

00:07:47,570 --> 00:07:51,740
the pipeline may be required only for a

00:07:49,190 --> 00:07:55,700
month for example so what is important

00:07:51,740 --> 00:07:59,300
is a way for you to define retention on

00:07:55,700 --> 00:08:01,150
different set of data separately and you

00:07:59,300 --> 00:08:04,250
need to make sure that there are other

00:08:01,150 --> 00:08:05,930
other services like the archival and all

00:08:04,250 --> 00:08:09,170
things like that run before the data

00:08:05,930 --> 00:08:11,030
retention and another thing that's

00:08:09,170 --> 00:08:12,800
important for data retention is you

00:08:11,030 --> 00:08:15,140
don't want a superuser which goes and

00:08:12,800 --> 00:08:17,390
deletes the data for everybody right you

00:08:15,140 --> 00:08:24,850
need to have access controls around the

00:08:17,390 --> 00:08:28,520
data that's available data replication

00:08:24,850 --> 00:08:30,410
so when a data is available on a cluster

00:08:28,520 --> 00:08:32,780
you might want to replicate the data to

00:08:30,410 --> 00:08:36,620
another cluster for two reasons one is

00:08:32,780 --> 00:08:38,900
because of the because of the global

00:08:36,620 --> 00:08:40,630
scale of the application you might have

00:08:38,900 --> 00:08:43,930
different data centers that are there in

00:08:40,630 --> 00:08:46,430
data centers that serve the business so

00:08:43,930 --> 00:08:48,350
each of these data centers might have

00:08:46,430 --> 00:08:50,750
their own herd local Hadoop clusters

00:08:48,350 --> 00:08:52,310
that are defined over there so you might

00:08:50,750 --> 00:08:52,860
want to replicate the data for two

00:08:52,310 --> 00:08:56,160
reasons

00:08:52,860 --> 00:08:58,050
for the business guy for the BCP in case

00:08:56,160 --> 00:08:59,519
of things go wrong with one cluster you

00:08:58,050 --> 00:09:03,690
should be able to resume the business

00:08:59,519 --> 00:09:05,250
from the another cluster or because you

00:09:03,690 --> 00:09:07,050
might want a grenade that data across

00:09:05,250 --> 00:09:08,640
all different clusters you might want to

00:09:07,050 --> 00:09:10,740
summarize the data in each of the local

00:09:08,640 --> 00:09:12,660
clusters and ship out the summary and do

00:09:10,740 --> 00:09:15,269
a global accurate aggregation on on a

00:09:12,660 --> 00:09:17,279
global cluster so the diff the

00:09:15,269 --> 00:09:20,760
challenges that are that in the

00:09:17,279 --> 00:09:22,740
replication are how do you control

00:09:20,760 --> 00:09:25,079
bandwidth across different sets of data

00:09:22,740 --> 00:09:26,670
one might be a critical data where who

00:09:25,079 --> 00:09:29,430
you want to give more bandwidth and one

00:09:26,670 --> 00:09:32,250
might be a not so critical data where

00:09:29,430 --> 00:09:34,890
you want to you want to replicate it in

00:09:32,250 --> 00:09:36,750
a slower path and you need to make sure

00:09:34,890 --> 00:09:40,980
that the data is replicated correctly

00:09:36,750 --> 00:09:42,600
the data correctness for example and if

00:09:40,980 --> 00:09:44,040
there is a minute Li data that's

00:09:42,600 --> 00:09:45,779
available on the cluster you don't want

00:09:44,040 --> 00:09:48,329
to replicate the data every minute

00:09:45,779 --> 00:09:50,459
instead you want to do data bulking and

00:09:48,329 --> 00:09:53,220
replicate the data every five minutes or

00:09:50,459 --> 00:09:56,339
every half an hour for that matter so

00:09:53,220 --> 00:09:57,420
these things need to be figured out if

00:09:56,339 --> 00:10:03,269
you want to just implement data

00:09:57,420 --> 00:10:08,910
replication another part is a data

00:10:03,269 --> 00:10:10,680
acquisition so it's so far here for

00:10:08,910 --> 00:10:12,870
machine consumption the data available

00:10:10,680 --> 00:10:14,550
on the cluster can be directly consumed

00:10:12,870 --> 00:10:18,120
right but for human consumption you

00:10:14,550 --> 00:10:20,360
might wanna it import the data into the

00:10:18,120 --> 00:10:22,980
cluster and export the data as well so

00:10:20,360 --> 00:10:24,269
hang import and export from different

00:10:22,980 --> 00:10:26,970
sources and to different destinations

00:10:24,269 --> 00:10:34,760
this also becomes critical in any data

00:10:26,970 --> 00:10:37,380
management system so so what we need is

00:10:34,760 --> 00:10:39,959
so the various things that I explained

00:10:37,380 --> 00:10:43,110
like the retention or the data pipelines

00:10:39,959 --> 00:10:45,690
each of these can be implemented using

00:10:43,110 --> 00:10:49,070
the existing systems like for example a

00:10:45,690 --> 00:10:52,949
data pipeline can be implemented as a

00:10:49,070 --> 00:10:54,480
coordinator in Guzzi so Guzzi provides

00:10:52,949 --> 00:10:58,230
the gating on the data being available

00:10:54,480 --> 00:11:00,690
and you can define a coordinator which

00:10:58,230 --> 00:11:02,430
basically runs every AR and you can

00:11:00,690 --> 00:11:04,660
define what are the input data sets and

00:11:02,430 --> 00:11:06,069
the output data sets and you

00:11:04,660 --> 00:11:09,699
be able to achieve the data pipeline

00:11:06,069 --> 00:11:11,290
using Guzzi as well then why why why are

00:11:09,699 --> 00:11:13,690
we trying to develop a new system new

00:11:11,290 --> 00:11:17,920
platform if you have one available right

00:11:13,690 --> 00:11:20,680
there are a few shortcomings one is what

00:11:17,920 --> 00:11:22,779
woozi exposés is a way for you to for

00:11:20,680 --> 00:11:24,790
for a given coordinator for example you

00:11:22,779 --> 00:11:26,620
need to define what the input dataset is

00:11:24,790 --> 00:11:30,459
which means that you need to define the

00:11:26,620 --> 00:11:33,339
frequency and the location of the data

00:11:30,459 --> 00:11:35,709
on the Hadoop cluster so if you have two

00:11:33,339 --> 00:11:37,930
processes two pipelines which consume

00:11:35,709 --> 00:11:41,110
the same data you have to define this

00:11:37,930 --> 00:11:43,060
twice and if you want to change this

00:11:41,110 --> 00:11:45,040
data if this data changes the location

00:11:43,060 --> 00:11:46,810
changes or the frequency changes you

00:11:45,040 --> 00:11:50,439
need to remember to go change it in all

00:11:46,810 --> 00:11:52,779
the places all right and that's just one

00:11:50,439 --> 00:11:54,819
example for a data pipeline if you want

00:11:52,779 --> 00:11:57,399
to implement a data retention for

00:11:54,819 --> 00:12:00,459
example you can probably write your own

00:11:57,399 --> 00:12:01,959
cron which does the retention depending

00:12:00,459 --> 00:12:03,730
on the data being available so you might

00:12:01,959 --> 00:12:06,220
have different cross depending on the

00:12:03,730 --> 00:12:10,329
day different data frequencies but the

00:12:06,220 --> 00:12:12,699
problem and that is the whole details of

00:12:10,329 --> 00:12:14,680
the data management or when the data was

00:12:12,699 --> 00:12:17,439
created when it's archived or when it's

00:12:14,680 --> 00:12:19,509
deleted is it spans across multiple

00:12:17,439 --> 00:12:21,910
systems that it becomes difficult to

00:12:19,509 --> 00:12:23,800
manage even for from a developer

00:12:21,910 --> 00:12:26,769
perspective or even from a support

00:12:23,800 --> 00:12:29,170
perspective on the production system so

00:12:26,769 --> 00:12:31,750
if all this information were available

00:12:29,170 --> 00:12:33,759
in a single place it becomes easier for

00:12:31,750 --> 00:12:36,670
anybody to manage the pipeline's

00:12:33,759 --> 00:12:40,800
so what we need is a cleaner abstraction

00:12:36,670 --> 00:12:43,509
of all this in a single place so that it

00:12:40,800 --> 00:12:45,839
life becomes easier and so that the

00:12:43,509 --> 00:12:48,519
business team can focus on the right

00:12:45,839 --> 00:12:51,959
problems which is basically on the

00:12:48,519 --> 00:12:51,959
business implementations right

00:12:53,939 --> 00:13:00,670
another thing that's missing with all

00:12:57,220 --> 00:13:03,550
this is the dashboard this becomes very

00:13:00,670 --> 00:13:05,970
critical when you are managing a big a

00:13:03,550 --> 00:13:09,040
complex data pipeline for example right

00:13:05,970 --> 00:13:10,720
the production support system is trying

00:13:09,040 --> 00:13:13,329
to look at this and try to figure out

00:13:10,720 --> 00:13:15,519
why the data is not available instead if

00:13:13,329 --> 00:13:18,019
there is a single view of the whole data

00:13:15,519 --> 00:13:19,670
pipelines which shows which stage of the

00:13:18,019 --> 00:13:21,709
pipeline isn't red which means that

00:13:19,670 --> 00:13:23,629
something is wrong over that or which

00:13:21,709 --> 00:13:25,879
pipeline is green which means everything

00:13:23,629 --> 00:13:27,949
is okay if there was a single dashboard

00:13:25,879 --> 00:13:31,129
which could give all this information it

00:13:27,949 --> 00:13:34,189
becomes a lot easier for any Support

00:13:31,129 --> 00:13:37,399
Engineer to manage these so what Falcon

00:13:34,189 --> 00:13:39,470
tries to address this address is trying

00:13:37,399 --> 00:13:42,939
to do is address all these concerns and

00:13:39,470 --> 00:13:53,179
give a very clean and abstract way of

00:13:42,939 --> 00:13:56,259
managing this at the art of Falcon are

00:13:53,179 --> 00:13:59,679
the three entities that Falcon defines

00:13:56,259 --> 00:14:03,110
one is a cluster feed and a process a

00:13:59,679 --> 00:14:05,329
cluster defines the Hadoop

00:14:03,110 --> 00:14:07,699
infrastructure like for example the name

00:14:05,329 --> 00:14:10,009
node the job track and endpoints the

00:14:07,699 --> 00:14:12,170
workflow engine the scheduler for

00:14:10,009 --> 00:14:15,170
example woozi or a catalog service

00:14:12,170 --> 00:14:18,499
endpoint like a chat log or the

00:14:15,170 --> 00:14:22,369
messaging service as well a feed defines

00:14:18,499 --> 00:14:24,619
the data on the cluster it can be a data

00:14:22,369 --> 00:14:26,569
that's directly on the cluster or it's

00:14:24,619 --> 00:14:28,429
even possible to define the data through

00:14:26,569 --> 00:14:31,549
each catalog catalog service as well and

00:14:28,429 --> 00:14:33,619
it defines what the frequency is it

00:14:31,549 --> 00:14:36,439
defines which all clusters this data is

00:14:33,619 --> 00:14:38,089
available on and what's the duration of

00:14:36,439 --> 00:14:42,649
the data which is available on the

00:14:38,089 --> 00:14:44,540
cluster and a processes defines a

00:14:42,649 --> 00:14:47,149
configuration for your data pipeline

00:14:44,540 --> 00:14:49,819
make for it it defines what are the

00:14:47,149 --> 00:14:51,980
input data sets input field input data

00:14:49,819 --> 00:14:55,309
that it consumes what are the output

00:14:51,980 --> 00:14:57,619
feed that it generates and it also

00:14:55,309 --> 00:14:59,749
defines the frequency at which the

00:14:57,619 --> 00:15:03,799
process should run which are clusters it

00:14:59,749 --> 00:15:05,899
should run on and more I will explain

00:15:03,799 --> 00:15:08,480
the details of each of these later so

00:15:05,899 --> 00:15:11,809
the important thing is each of these

00:15:08,480 --> 00:15:14,059
data is defined once and you never do it

00:15:11,809 --> 00:15:16,699
multiple times and each of these

00:15:14,059 --> 00:15:19,129
entities have a name which is a user

00:15:16,699 --> 00:15:21,649
specified name and the references are

00:15:19,129 --> 00:15:23,509
using the name so all the data related

00:15:21,649 --> 00:15:25,790
to the Hadoop infrastructure is defined

00:15:23,509 --> 00:15:28,920
in the cluster all the data related to

00:15:25,790 --> 00:15:31,410
the dare the data is defined in a fee

00:15:28,920 --> 00:15:33,450
and all the processing logic is defined

00:15:31,410 --> 00:15:35,550
in a process so there is a one place

00:15:33,450 --> 00:15:36,990
some one-stroke for you to look at the

00:15:35,550 --> 00:15:40,290
data and see what's happening in the

00:15:36,990 --> 00:15:41,870
system so if you want to for example

00:15:40,290 --> 00:15:46,320
given a data if you want to figure out

00:15:41,870 --> 00:15:48,510
which all pipe which are data pipelines

00:15:46,320 --> 00:15:51,270
consume this data or which are clusters

00:15:48,510 --> 00:15:52,680
it's defined in what happen what is the

00:15:51,270 --> 00:15:54,630
retention on the data what is the

00:15:52,680 --> 00:15:56,040
replication on the data all that you

00:15:54,630 --> 00:15:57,300
need to know is look at the feed

00:15:56,040 --> 00:15:59,100
definition and you will get all the

00:15:57,300 --> 00:16:01,410
details so this is the kind of

00:15:59,100 --> 00:16:03,480
abstraction that Falcon tries to give it

00:16:01,410 --> 00:16:08,100
obstructs out all the data management

00:16:03,480 --> 00:16:13,560
functionality so that the the developers

00:16:08,100 --> 00:16:16,410
can work on the right right things like

00:16:13,560 --> 00:16:18,780
the business functionality so this is an

00:16:16,410 --> 00:16:22,320
example this is a DSL that Falcon

00:16:18,780 --> 00:16:26,550
internally uses this is a cluster XML

00:16:22,320 --> 00:16:29,490
lab that is there as you can see each of

00:16:26,550 --> 00:16:31,770
them has a name and it defines the end

00:16:29,490 --> 00:16:33,420
point for the different hadoop

00:16:31,770 --> 00:16:35,100
infrastructure like the read-only

00:16:33,420 --> 00:16:37,050
endpoint right endpoint is the name

00:16:35,100 --> 00:16:42,660
lower end point the jobtracker end point

00:16:37,050 --> 00:16:45,750
and the work flow is the so the rear end

00:16:42,660 --> 00:16:49,410
point is used for application the right

00:16:45,750 --> 00:16:52,860
end point is the it DFS endpoint where

00:16:49,410 --> 00:16:56,700
the data is stored and that's a where

00:16:52,860 --> 00:16:58,940
the execute end point is the is where

00:16:56,700 --> 00:17:01,680
the process is launched for execution

00:16:58,940 --> 00:17:06,120
the workflow endpoint is the scheduler

00:17:01,680 --> 00:17:07,980
which in this case is Guzzi and the

00:17:06,120 --> 00:17:11,340
registry endpoint is the catalog service

00:17:07,980 --> 00:17:13,460
which can be either a chat log and there

00:17:11,340 --> 00:17:18,120
is a messaging interface as well

00:17:13,460 --> 00:17:23,100
Falkon internally supports messages as

00:17:18,120 --> 00:17:26,070
well these messages are messages are

00:17:23,100 --> 00:17:28,560
published on every data that's created

00:17:26,070 --> 00:17:30,690
or every data that's deleted or when a

00:17:28,560 --> 00:17:33,420
pipeline succeeds or fails or things

00:17:30,690 --> 00:17:35,160
like that so this is the messaging end

00:17:33,420 --> 00:17:37,110
point the JMS messaging in point that's

00:17:35,160 --> 00:17:41,670
used and those are some of the paths

00:17:37,110 --> 00:17:42,670
that are internally used by Falcon this

00:17:41,670 --> 00:17:47,320
is an example of

00:17:42,670 --> 00:17:49,830
feed so feed has a name again and it

00:17:47,320 --> 00:17:51,700
defines the frequency the freak

00:17:49,830 --> 00:17:54,040
frequency at which the data is available

00:17:51,700 --> 00:17:56,140
on the cluster and the late arrival

00:17:54,040 --> 00:17:59,400
which basically defines how late this

00:17:56,140 --> 00:18:02,250
data can come and it also supports

00:17:59,400 --> 00:18:05,590
groups and tags which are basically

00:18:02,250 --> 00:18:08,200
useful if you want to tag the feed and

00:18:05,590 --> 00:18:11,080
it also defines which are clusters this

00:18:08,200 --> 00:18:13,090
data is available from what time to what

00:18:11,080 --> 00:18:14,980
time this data is available what is the

00:18:13,090 --> 00:18:17,560
retention on the data in each of the

00:18:14,980 --> 00:18:19,150
different clusters and the type equal to

00:18:17,560 --> 00:18:21,340
source and the target defines the

00:18:19,150 --> 00:18:23,920
replication so it defines this data

00:18:21,340 --> 00:18:25,840
should be replicated from the cluster

00:18:23,920 --> 00:18:29,680
primary to cluster secondary whenever

00:18:25,840 --> 00:18:32,440
the data is available finally there is

00:18:29,680 --> 00:18:34,780
locations which defines the path of this

00:18:32,440 --> 00:18:36,850
data on the different clusters it's

00:18:34,780 --> 00:18:38,620
possible to define this at a global

00:18:36,850 --> 00:18:40,900
equation but it is also possible to

00:18:38,620 --> 00:18:42,370
override it and define it specifically

00:18:40,900 --> 00:18:44,500
for each and every cluster separately

00:18:42,370 --> 00:18:46,570
and finally you have the Arkell which

00:18:44,500 --> 00:18:48,780
basically defines the owners and things

00:18:46,570 --> 00:18:48,780
like that

00:18:55,100 --> 00:19:03,470
and the third entity is the process sir

00:18:59,690 --> 00:19:06,320
process has a name again and it defines

00:19:03,470 --> 00:19:08,930
the clusters where this data pipeline

00:19:06,320 --> 00:19:12,050
should run the start and the end date

00:19:08,930 --> 00:19:14,420
where it should run a parallel defines

00:19:12,050 --> 00:19:17,180
the concurrency of running the pipeline

00:19:14,420 --> 00:19:18,770
how many instances of the pipeline can I

00:19:17,180 --> 00:19:20,270
run in parallel on the cluster so

00:19:18,770 --> 00:19:22,250
depending on the throttling and the

00:19:20,270 --> 00:19:25,970
capacity available you can define your

00:19:22,250 --> 00:19:28,130
own concurrency and it's possible to

00:19:25,970 --> 00:19:32,240
define frequency which is basically the

00:19:28,130 --> 00:19:34,340
frequency at which the data is the

00:19:32,240 --> 00:19:38,210
process should run it also defines

00:19:34,340 --> 00:19:40,220
inputs and outputs so the inputs is is

00:19:38,210 --> 00:19:43,040
the feed name which is which goes as the

00:19:40,220 --> 00:19:44,720
input see that you what you define in

00:19:43,040 --> 00:19:46,700
the input and output is just the field

00:19:44,720 --> 00:19:48,830
names you don't specify that all the

00:19:46,700 --> 00:19:50,840
locations or the frequency anymore what

00:19:48,830 --> 00:19:53,630
you define is just once in the date in

00:19:50,840 --> 00:19:55,340
the feed definition so as part of the

00:19:53,630 --> 00:19:59,990
process you specify the inputs and

00:19:55,340 --> 00:20:01,970
output so then now of 0 2 and today of 0

00:19:59,990 --> 00:20:04,070
0 are basically the instances of the

00:20:01,970 --> 00:20:06,800
data that you want to consume it for

00:20:04,070 --> 00:20:09,380
example if you have a daily processing

00:20:06,800 --> 00:20:11,090
that happens and the input data is

00:20:09,380 --> 00:20:13,160
hourly I want to consume the whole of

00:20:11,090 --> 00:20:17,570
yesterday's data and I want to process

00:20:13,160 --> 00:20:19,880
it and generate a generate daily data

00:20:17,570 --> 00:20:22,790
which is output data right so the story

00:20:19,880 --> 00:20:24,920
of 0 0 and today for 0 0 so these are

00:20:22,790 --> 00:20:27,380
the yield expressions that Falken

00:20:24,920 --> 00:20:30,850
exposes to define what are the input

00:20:27,380 --> 00:20:30,850
data sets and the output data set

00:20:32,020 --> 00:20:37,910
finally you define the workflow engine

00:20:35,360 --> 00:20:40,810
which can be either a whose epoch flow

00:20:37,910 --> 00:20:44,450
that you can specify or a pig or hive

00:20:40,810 --> 00:20:47,140
also which basically contains the

00:20:44,450 --> 00:20:50,350
processing logic for your data and you

00:20:47,140 --> 00:20:53,630
can even specify the retry policy which

00:20:50,350 --> 00:20:56,390
which explains how you want a handle

00:20:53,630 --> 00:20:59,090
lock in case of failures do you want to

00:20:56,390 --> 00:21:01,220
retry how many times you want to retry

00:20:59,090 --> 00:21:03,200
and what is the policy with which you

00:21:01,220 --> 00:21:06,620
coil you can retry like a periodic or a

00:21:03,200 --> 00:21:08,360
back of policy at the end of the process

00:21:06,620 --> 00:21:10,580
pacification you also specify

00:21:08,360 --> 00:21:12,770
how do you wanna handle a data which are

00:21:10,580 --> 00:21:15,890
the late inputs and when the data

00:21:12,770 --> 00:21:17,240
arrives late how do you handle that do

00:21:15,890 --> 00:21:19,460
you want to rerun the whole pipeline

00:21:17,240 --> 00:21:21,020
again or you want to do incremental

00:21:19,460 --> 00:21:25,400
processing and things like that that you

00:21:21,020 --> 00:21:27,380
can specify in the late data so that's

00:21:25,400 --> 00:21:35,990
basically the different entities that

00:21:27,380 --> 00:21:40,160
are defined in Falcon this is a

00:21:35,990 --> 00:21:43,490
high-level architecture of Falcon Falcon

00:21:40,160 --> 00:21:46,990
is a very lightweight application it has

00:21:43,490 --> 00:21:50,450
a CLI and rest ApS to manage different

00:21:46,990 --> 00:21:52,370
entities that are that and it also

00:21:50,450 --> 00:21:56,240
exposes instance functions which

00:21:52,370 --> 00:21:58,850
basically you can look at the one

00:21:56,240 --> 00:22:00,710
instance of the pipeline hardly process

00:21:58,850 --> 00:22:03,559
for example you want to check what

00:22:00,710 --> 00:22:06,830
happened to that process which were

00:22:03,559 --> 00:22:08,059
supposed to run today so or what

00:22:06,830 --> 00:22:10,790
happened to that process we just

00:22:08,059 --> 00:22:12,320
supposed to run from last ten days so

00:22:10,790 --> 00:22:14,120
these are the instance functions which

00:22:12,320 --> 00:22:15,890
basically look at each instance of the

00:22:14,120 --> 00:22:18,260
process or each instance of the feed and

00:22:15,890 --> 00:22:20,150
gives you status of that so those are

00:22:18,260 --> 00:22:23,360
the different instance functions so it's

00:22:20,150 --> 00:22:26,710
possible to all these functions are

00:22:23,360 --> 00:22:30,169
available through both CLI and REST API

00:22:26,710 --> 00:22:32,510
so what Falcon is just a high-level

00:22:30,169 --> 00:22:36,490
abstraction he doesn't do any of the

00:22:32,510 --> 00:22:40,760
heavy lifting all this the process

00:22:36,490 --> 00:22:42,440
process orchestration and the feed

00:22:40,760 --> 00:22:45,770
retention and the replication all this

00:22:42,440 --> 00:22:49,400
is scheduled on a scheduler in this

00:22:45,770 --> 00:22:51,110
example we have taken it up as Suzy but

00:22:49,400 --> 00:22:56,809
it's possible to plug in any other

00:22:51,110 --> 00:22:59,299
workflow engine if available so Falcon

00:22:56,809 --> 00:23:02,480
distracts out and all this heavy lifting

00:22:59,299 --> 00:23:05,150
is done by Uzi and the different entity

00:23:02,480 --> 00:23:08,929
xml's are stored in config store which

00:23:05,150 --> 00:23:11,090
is part of a particular of Falcon it can

00:23:08,929 --> 00:23:13,580
be configured to store it in a DFS or

00:23:11,090 --> 00:23:15,290
any chain file system as well and many

00:23:13,580 --> 00:23:17,960
other Hadoop per file system which is

00:23:15,290 --> 00:23:20,800
HDFS and MapReduce and whose intern can

00:23:17,960 --> 00:23:26,700
talk to catalog service for the data

00:23:20,800 --> 00:23:29,710
availability so in addition to

00:23:26,700 --> 00:23:31,720
orchestrating the different entity life

00:23:29,710 --> 00:23:35,800
cycles like retention and replication or

00:23:31,720 --> 00:23:37,300
a process it also customizes the

00:23:35,800 --> 00:23:40,600
workflow with a pre-processing and a

00:23:37,300 --> 00:23:43,450
post-processing step which is which is

00:23:40,600 --> 00:23:47,710
used by falcon internally for retries

00:23:43,450 --> 00:23:49,540
for lay data handling so as part of the

00:23:47,710 --> 00:23:52,990
post-processing of the workflow it sends

00:23:49,540 --> 00:23:56,860
out DMS messages which falconry listens

00:23:52,990 --> 00:23:59,590
on and it does the retry in case there

00:23:56,860 --> 00:24:02,110
are failures it also has the late data

00:23:59,590 --> 00:24:04,990
mechanism which basically monitors this

00:24:02,110 --> 00:24:07,200
and depending on the configured late

00:24:04,990 --> 00:24:10,720
data detection it checks if there is any

00:24:07,200 --> 00:24:15,120
delay late data and then reruns the

00:24:10,720 --> 00:24:15,120
whole of the box the processing again

00:24:18,750 --> 00:24:24,220
this is the basic entity shade you love

00:24:21,940 --> 00:24:26,920
how the data flows across different

00:24:24,220 --> 00:24:29,260
systems so you you have a cluster XML a

00:24:26,920 --> 00:24:32,680
feed XML and a process XML that are

00:24:29,260 --> 00:24:34,930
submitted to Falcon and when you submit

00:24:32,680 --> 00:24:37,350
these entities into Falcon this entities

00:24:34,930 --> 00:24:40,360
are stored in configuration store and

00:24:37,350 --> 00:24:44,050
when you schedule this feed or a process

00:24:40,360 --> 00:24:46,630
a retention or a replication for a feed

00:24:44,050 --> 00:24:50,710
or a process lifecycle kicks off which

00:24:46,630 --> 00:24:54,250
basically are orchestrates the lifecycle

00:24:50,710 --> 00:24:56,500
into a Woozie workflow and it is

00:24:54,250 --> 00:24:59,890
sheduled on who's different coordinators

00:24:56,500 --> 00:25:01,360
in woozi and it also has a

00:24:59,890 --> 00:25:02,800
pre-processing and a post processing

00:25:01,360 --> 00:25:06,790
step which basically takes care of

00:25:02,800 --> 00:25:09,100
sending the required notifications and

00:25:06,790 --> 00:25:11,560
then there are instance management ApS

00:25:09,100 --> 00:25:15,130
that are available which looks at the

00:25:11,560 --> 00:25:18,370
data and you can get the status so

00:25:15,130 --> 00:25:19,750
status or a suspension or a resuming the

00:25:18,370 --> 00:25:23,340
workflows and things like that at a in

00:25:19,750 --> 00:25:23,340
each and every instance level as well

00:25:26,510 --> 00:25:31,970
so this is the physical architecture of

00:25:29,090 --> 00:25:35,980
how falcon is deployed across multiple

00:25:31,970 --> 00:25:40,010
clusters when you have a global business

00:25:35,980 --> 00:25:41,690
with spans across multiple geographical

00:25:40,010 --> 00:25:44,570
locations you end up setting up

00:25:41,690 --> 00:25:46,940
different data centers for latency

00:25:44,570 --> 00:25:48,770
reasons so you'll have different

00:25:46,940 --> 00:25:50,330
clusters that are available in different

00:25:48,770 --> 00:25:52,190
Hadoop clusters that are available in

00:25:50,330 --> 00:25:55,370
different data centers and the

00:25:52,190 --> 00:25:57,290
application servers stream the data into

00:25:55,370 --> 00:25:58,850
the Hadoop clusters right so and you

00:25:57,290 --> 00:26:01,760
want to do processing on each of the

00:25:58,850 --> 00:26:03,950
local clusters as well so it's possible

00:26:01,760 --> 00:26:07,669
to have multiple floods set up as well

00:26:03,950 --> 00:26:09,650
with Falcon just like each Hadoop

00:26:07,669 --> 00:26:12,799
cluster in each of the data center each

00:26:09,650 --> 00:26:17,120
Falcon service is deployed in each of

00:26:12,799 --> 00:26:19,010
the data centers again and because this

00:26:17,120 --> 00:26:21,410
Falcon can work across multiple clusters

00:26:19,010 --> 00:26:23,720
there is a another component of Falcon

00:26:21,410 --> 00:26:27,830
called prism which provides a global

00:26:23,720 --> 00:26:32,049
view of all the feeds and processes so

00:26:27,830 --> 00:26:34,370
all the entity operations like a submit

00:26:32,049 --> 00:26:39,200
everything happens through the prism

00:26:34,370 --> 00:26:40,669
prism doesn't do any of the it doesn't

00:26:39,200 --> 00:26:44,809
have any other functionality other than

00:26:40,669 --> 00:26:47,179
figuring out which all clusters the feed

00:26:44,809 --> 00:26:48,950
or a process should should be submitted

00:26:47,179 --> 00:26:51,650
and it just forwards the request to the

00:26:48,950 --> 00:26:54,049
different application the Falcon servers

00:26:51,650 --> 00:26:56,510
so each of the Falcon servers work

00:26:54,049 --> 00:27:01,760
independently they don't know of each

00:26:56,510 --> 00:27:04,960
other and they they take care of the

00:27:01,760 --> 00:27:04,960
entity life circles

00:27:08,320 --> 00:27:15,139
another important thing that's required

00:27:12,440 --> 00:27:18,320
in a data management is lineage data

00:27:15,139 --> 00:27:21,379
lineage right assume that you have

00:27:18,320 --> 00:27:22,969
generated a report and somebody looks at

00:27:21,379 --> 00:27:24,889
it and comes back after a month says

00:27:22,969 --> 00:27:27,889
this report is wrong how did you come up

00:27:24,889 --> 00:27:31,219
with this report so you need a way of

00:27:27,889 --> 00:27:33,109
tracking down which which version of

00:27:31,219 --> 00:27:35,419
your application ran which produced this

00:27:33,109 --> 00:27:37,340
data and it's also important for you to

00:27:35,419 --> 00:27:41,089
know which data it consumed to produce

00:27:37,340 --> 00:27:42,799
this data so it's important to record

00:27:41,089 --> 00:27:46,429
this information so that it's available

00:27:42,799 --> 00:27:49,759
readily for the users so what Falcon

00:27:46,429 --> 00:27:52,940
does is part of the post-processing step

00:27:49,759 --> 00:27:54,919
of the different entity life cycles it

00:27:52,940 --> 00:27:56,989
also records which instance ran which

00:27:54,919 --> 00:28:00,229
version of the application ran and all

00:27:56,989 --> 00:28:02,749
this data is available in a graph in

00:28:00,229 --> 00:28:06,499
Falcon and there are ApS to get this

00:28:02,749 --> 00:28:10,879
information we use Titan to store this

00:28:06,499 --> 00:28:14,349
information in a graph in Falcon so

00:28:10,879 --> 00:28:16,429
there is also uuy which gives different

00:28:14,349 --> 00:28:18,649
instances and their dependencies like

00:28:16,429 --> 00:28:21,950
for example over here for each of the

00:28:18,649 --> 00:28:23,809
field instance it says when is the which

00:28:21,950 --> 00:28:27,289
is a process instance that consume this

00:28:23,809 --> 00:28:29,210
and when was the data available the

00:28:27,289 --> 00:28:30,710
output data available and if there was

00:28:29,210 --> 00:28:33,169
any retention in it when the data was

00:28:30,710 --> 00:28:35,659
deleted and things like that it becomes

00:28:33,169 --> 00:28:37,389
many have a complex data pipeline it

00:28:35,659 --> 00:28:40,489
becomes very important for you to track

00:28:37,389 --> 00:28:44,539
what happened at each step and how the

00:28:40,489 --> 00:28:47,419
data is data moves across different

00:28:44,539 --> 00:28:49,519
steps how the data is finally deleted so

00:28:47,419 --> 00:28:58,460
lineage gives you all this information

00:28:49,519 --> 00:29:02,719
at one place another component of Falcon

00:28:58,460 --> 00:29:06,950
is the designer so what happens

00:29:02,719 --> 00:29:09,109
typically in an organization is you have

00:29:06,950 --> 00:29:12,049
one team which is which has all the

00:29:09,109 --> 00:29:14,979
expertise to develop data pipelines who

00:29:12,049 --> 00:29:17,839
know about Hadoop who know about how to

00:29:14,979 --> 00:29:19,190
write pipelines using peg or a MapReduce

00:29:17,839 --> 00:29:21,710
job right

00:29:19,190 --> 00:29:23,330
and when you have different products and

00:29:21,710 --> 00:29:25,520
which are which have different

00:29:23,330 --> 00:29:28,160
requirements the data processing team

00:29:25,520 --> 00:29:30,920
becomes a bottleneck and it's the only

00:29:28,160 --> 00:29:33,440
team it knows how to do things so that

00:29:30,920 --> 00:29:39,290
becomes a bottleneck for any new feature

00:29:33,440 --> 00:29:43,730
additions right so and that won't scale

00:29:39,290 --> 00:29:46,940
so for you to scale what you need is you

00:29:43,730 --> 00:29:50,720
which it doesn't have any details of the

00:29:46,940 --> 00:29:53,360
Hadoop which gives an option for anybody

00:29:50,720 --> 00:29:56,030
to go ahead and discover what data is

00:29:53,360 --> 00:29:58,640
available on the cluster and enables you

00:29:56,030 --> 00:30:01,940
to design a pipeline without knowing the

00:29:58,640 --> 00:30:06,590
details about how many hadoop clusters

00:30:01,940 --> 00:30:10,280
are available or or how many how many

00:30:06,590 --> 00:30:12,740
MapReduce slots do I need the hell is

00:30:10,280 --> 00:30:14,870
this data stored in the cluster like on

00:30:12,740 --> 00:30:18,290
educate log or is there an H pace or a

00:30:14,870 --> 00:30:21,020
DFS and things like that so a designer

00:30:18,290 --> 00:30:22,430
gives a UI for you the left-hand side

00:30:21,020 --> 00:30:24,730
what you can see is the different data

00:30:22,430 --> 00:30:26,990
that's available and the right side you

00:30:24,730 --> 00:30:29,300
define the different transformations

00:30:26,990 --> 00:30:31,550
that are possible on the data like you

00:30:29,300 --> 00:30:33,860
can have filters or you can have

00:30:31,550 --> 00:30:35,660
aggregations you have you can define

00:30:33,860 --> 00:30:37,820
your own transformations and things like

00:30:35,660 --> 00:30:40,910
that and it's also possible for you to

00:30:37,820 --> 00:30:42,560
define different actions like email

00:30:40,910 --> 00:30:44,300
action when the data is available I just

00:30:42,560 --> 00:30:47,030
want to send out an email and things

00:30:44,300 --> 00:30:49,670
like that so what this UI gives you is

00:30:47,030 --> 00:30:52,130
way for anybody without any knowledge of

00:30:49,670 --> 00:30:55,520
Hadoop to go ahead and design a pipeline

00:30:52,130 --> 00:30:58,010
and verify that the pipeline works fine

00:30:55,520 --> 00:31:01,670
and go ahead and deploy this pipeline on

00:30:58,010 --> 00:31:03,920
production environment this is a work in

00:31:01,670 --> 00:31:08,930
progress it's still not fully completed

00:31:03,920 --> 00:31:10,910
these are just a marks of it so those

00:31:08,930 --> 00:31:15,500
are the different functionalities that

00:31:10,910 --> 00:31:17,240
Falken provides we'll just go through

00:31:15,500 --> 00:31:21,250
some of the case studies where we have

00:31:17,240 --> 00:31:21,250
used falcon on production environments

00:31:23,890 --> 00:31:29,240
so this is the multi cluster failover

00:31:26,410 --> 00:31:32,120
use case where you have the raw data

00:31:29,240 --> 00:31:33,629
that that comes into the cluster and it

00:31:32,120 --> 00:31:36,479
goes through various

00:31:33,629 --> 00:31:38,459
where you clean up the data and you have

00:31:36,479 --> 00:31:40,109
some processing of the data and you have

00:31:38,459 --> 00:31:43,919
the final presentable data that's

00:31:40,109 --> 00:31:46,789
available on the cluster right for for

00:31:43,919 --> 00:31:49,469
BCP reasons you want to you want to

00:31:46,789 --> 00:31:51,689
replicate this data replicate the raw

00:31:49,469 --> 00:31:53,999
data into a different cluster so that if

00:31:51,689 --> 00:31:56,159
things go wrong if the cluster goes down

00:31:53,999 --> 00:31:58,589
for any reason your business continues

00:31:56,159 --> 00:32:04,919
as usual so this is achieved using a

00:31:58,589 --> 00:32:07,469
feeder application directly and so so if

00:32:04,919 --> 00:32:09,839
if one of the cluster goes down you can

00:32:07,469 --> 00:32:11,699
set up a process and you can just remove

00:32:09,839 --> 00:32:13,319
out the cluster which doesn't work and

00:32:11,699 --> 00:32:15,929
add another cluster and submit it to

00:32:13,319 --> 00:32:18,089
Falcon everything will work as ease so

00:32:15,929 --> 00:32:22,709
it requires very less configuration from

00:32:18,089 --> 00:32:30,269
the user and it lets you operate very

00:32:22,709 --> 00:32:32,489
easily and the feed retention because

00:32:30,269 --> 00:32:34,409
the feed is defined for each and every

00:32:32,489 --> 00:32:36,689
data that's available on the cluster is

00:32:34,409 --> 00:32:38,689
possible for you to define the feed

00:32:36,689 --> 00:32:42,299
retention for every data that's there so

00:32:38,689 --> 00:32:46,729
feed retention comes in handy and it

00:32:42,299 --> 00:32:46,729
gives a single view of the data for you

00:32:49,879 --> 00:32:56,339
another case study that we have is the

00:32:52,649 --> 00:32:58,369
distributed processing that we are doing

00:32:56,339 --> 00:33:04,319
that in mobi

00:32:58,369 --> 00:33:08,219
so in mobi is leading mobile advertising

00:33:04,319 --> 00:33:10,019
network and this is the Hadoop usage at

00:33:08,219 --> 00:33:12,419
in mobi we have around seven production

00:33:10,019 --> 00:33:15,719
clusters there is more than one PB of

00:33:12,419 --> 00:33:17,609
storage available on this clusters there

00:33:15,719 --> 00:33:20,069
is around fighty B of data that keeps

00:33:17,609 --> 00:33:22,679
the raw data that keeps coming in every

00:33:20,069 --> 00:33:26,819
day there's 20 DB of data processing

00:33:22,679 --> 00:33:29,009
that happens every day and the clusters

00:33:26,819 --> 00:33:32,279
are run of three hundred nodes cluster

00:33:29,009 --> 00:33:34,739
and we use HBase as well which is 1502 a

00:33:32,279 --> 00:33:36,689
store that we have we are more than 200

00:33:34,739 --> 00:33:40,619
kg aduke jobs that run every day and

00:33:36,689 --> 00:33:42,779
there are 80 k wu z workflows that we

00:33:40,619 --> 00:33:43,950
have and we have 500 plus fee

00:33:42,779 --> 00:33:47,370
definitions and

00:33:43,950 --> 00:33:54,120
over 200 Falcon process definitions as

00:33:47,370 --> 00:33:58,260
well so we have various kinds of data

00:33:54,120 --> 00:34:00,600
now when a user requests for an ad all

00:33:58,260 --> 00:34:04,080
the details are logged as a request data

00:34:00,600 --> 00:34:06,149
request logs and when an ad is solved

00:34:04,080 --> 00:34:08,129
for that request it's called an

00:34:06,149 --> 00:34:09,990
impression and all the impression

00:34:08,129 --> 00:34:13,050
details are stored as an impression log

00:34:09,990 --> 00:34:15,770
in the application server and when I

00:34:13,050 --> 00:34:18,149
click happens on that ad there is a

00:34:15,770 --> 00:34:21,179
beacon that is sent back to the server

00:34:18,149 --> 00:34:23,010
and the click servers again log all this

00:34:21,179 --> 00:34:24,869
click information and there can be

00:34:23,010 --> 00:34:27,300
various beacon events that can come in

00:34:24,869 --> 00:34:32,369
later as well with the service log again

00:34:27,300 --> 00:34:34,770
so we have all this data available the

00:34:32,369 --> 00:34:37,829
application service log all this data

00:34:34,770 --> 00:34:43,139
and this data is streamed me neatly into

00:34:37,829 --> 00:34:45,000
the Hadoop cluster and there are so this

00:34:43,139 --> 00:34:47,099
is basically what happens in a single

00:34:45,000 --> 00:34:51,599
data center we have five different data

00:34:47,099 --> 00:34:53,339
centers that are available and there are

00:34:51,599 --> 00:34:55,649
Hadoop clusters set up in each of these

00:34:53,339 --> 00:34:57,300
data centers and there are application

00:34:55,649 --> 00:34:59,310
servers again in each of these data

00:34:57,300 --> 00:35:01,400
centers so this is the processing that

00:34:59,310 --> 00:35:05,520
happens in each of the data centers

00:35:01,400 --> 00:35:07,230
locally in each of the clusters and of

00:35:05,520 --> 00:35:09,270
course we can ship out all this data

00:35:07,230 --> 00:35:12,540
into the global cluster and do all the

00:35:09,270 --> 00:35:15,030
processing globally on one single

00:35:12,540 --> 00:35:17,460
cluster but the talent there is the huge

00:35:15,030 --> 00:35:19,440
amount of raw data that you want ship to

00:35:17,460 --> 00:35:21,240
the single cluster so there is huge

00:35:19,440 --> 00:35:24,329
bandwidth consumption right to optimize

00:35:21,240 --> 00:35:27,390
on all that what we do is we process

00:35:24,329 --> 00:35:29,970
this data in in each of the local

00:35:27,390 --> 00:35:32,010
clusters and only the summary which is

00:35:29,970 --> 00:35:34,319
very small in size is shipped to the

00:35:32,010 --> 00:35:36,300
global cluster so because the summary

00:35:34,319 --> 00:35:40,230
size is very small we save on the

00:35:36,300 --> 00:35:43,530
bandwidth as well and so this is the

00:35:40,230 --> 00:35:45,150
various stages of the data processing

00:35:43,530 --> 00:35:51,869
that happens in each of the local

00:35:45,150 --> 00:35:54,390
clusters so the same processing actually

00:35:51,869 --> 00:35:56,549
is actually duplicated across the five

00:35:54,390 --> 00:35:59,059
different data centers that's a five

00:35:56,549 --> 00:36:03,119
certain clusters that are available so

00:35:59,059 --> 00:36:06,059
so instead of you having to define all

00:36:03,119 --> 00:36:07,769
this data and the processing multiple

00:36:06,059 --> 00:36:10,589
times on each of these five different

00:36:07,769 --> 00:36:13,799
available clusters what falconeer

00:36:10,589 --> 00:36:15,900
enables is defining a process once and

00:36:13,799 --> 00:36:18,119
define which our clusters it's defined

00:36:15,900 --> 00:36:20,219
in it's just a single XML where we

00:36:18,119 --> 00:36:23,849
define the processing and we submit this

00:36:20,219 --> 00:36:25,499
to a prism Falcon prism and prism takes

00:36:23,849 --> 00:36:28,709
care of going and sharing all these

00:36:25,499 --> 00:36:33,599
clusters in the relevant relevant

00:36:28,709 --> 00:36:36,089
clusters so Falcon helps a long way in

00:36:33,599 --> 00:36:39,719
managing all these complicated pipelines

00:36:36,089 --> 00:36:42,689
and automatically the feed replication

00:36:39,719 --> 00:36:45,839
takes care of replicating this data into

00:36:42,689 --> 00:36:47,459
the global cluster where where we

00:36:45,839 --> 00:36:51,900
aggregate over the data across different

00:36:47,459 --> 00:36:53,729
clusters and have different reports for

00:36:51,900 --> 00:37:00,539
both the human consumption and the

00:36:53,729 --> 00:37:02,309
machine on the highlights so these are

00:37:00,539 --> 00:37:05,179
some things that we have planned for the

00:37:02,309 --> 00:37:10,160
future one is the data governance which

00:37:05,179 --> 00:37:13,799
basically covers the data anonymity and

00:37:10,160 --> 00:37:16,849
each data can define the each data

00:37:13,799 --> 00:37:19,400
pipeline can define the statistics

00:37:16,849 --> 00:37:22,289
statistics and Falcon should be able to

00:37:19,400 --> 00:37:25,410
look at those statistics and look at the

00:37:22,289 --> 00:37:28,109
data quality and figure out if this data

00:37:25,410 --> 00:37:32,279
is a valid data or not so you don't want

00:37:28,109 --> 00:37:39,269
some wrong data getting consumed by self

00:37:32,279 --> 00:37:44,359
by some business people and so so this

00:37:39,269 --> 00:37:46,469
data governance covers all that and

00:37:44,359 --> 00:37:49,319
another one is the data pipeline

00:37:46,469 --> 00:37:52,349
designer that I showed the mock on which

00:37:49,319 --> 00:37:54,390
gives you a UI where in people who don't

00:37:52,349 --> 00:37:55,739
even have any knowledge of Hadoop should

00:37:54,390 --> 00:38:01,349
be able to go ahead and design a

00:37:55,739 --> 00:38:04,499
pipeline and the third one is the data

00:38:01,349 --> 00:38:06,499
acquisition which basically about how do

00:38:04,499 --> 00:38:08,380
you get the data into the cluster

00:38:06,499 --> 00:38:10,720
depending on

00:38:08,380 --> 00:38:16,060
if it's a slow changing data or a stream

00:38:10,720 --> 00:38:18,580
data the final thing which is missing is

00:38:16,060 --> 00:38:24,790
the dashboard that we need which

00:38:18,580 --> 00:38:27,460
basically enables you to define define a

00:38:24,790 --> 00:38:29,500
feed or a process easily through the UI

00:38:27,460 --> 00:38:32,560
and you don't have to write XML for it

00:38:29,500 --> 00:38:34,900
and how do you manage the different

00:38:32,560 --> 00:38:36,610
pipelines and get status of each and

00:38:34,900 --> 00:38:38,620
every instance and things like that so

00:38:36,610 --> 00:38:40,830
that's a dashboard up which is coming up

00:38:38,620 --> 00:38:40,830
soon

00:38:41,070 --> 00:38:47,740
to summarize on what we saw till now

00:38:45,060 --> 00:38:50,200
what Falcon gives you is the data

00:38:47,740 --> 00:38:55,120
management solution which is abstracted

00:38:50,200 --> 00:38:56,830
out into a platform and all this is

00:38:55,120 --> 00:39:00,940
abstracted onto a platform so that

00:38:56,830 --> 00:39:04,680
people can worry about the business

00:39:00,940 --> 00:39:08,530
details rather than worry about this

00:39:04,680 --> 00:39:10,660
this every retention or a replication

00:39:08,530 --> 00:39:13,030
and failures and things like that all

00:39:10,660 --> 00:39:15,970
this is handled by Falcon you have the

00:39:13,030 --> 00:39:18,070
import export and the serialization the

00:39:15,970 --> 00:39:19,750
schema or the replication retention and

00:39:18,070 --> 00:39:22,210
archival for data which is managed by

00:39:19,750 --> 00:39:24,700
Falcon or coming to the data processing

00:39:22,210 --> 00:39:27,700
it takes care of the data dependencies

00:39:24,700 --> 00:39:29,230
and late data handling or what happens

00:39:27,700 --> 00:39:32,260
if there are any failures in the data

00:39:29,230 --> 00:39:33,940
pipelines and how do you handle the be

00:39:32,260 --> 00:39:38,170
centralized processing across multiple

00:39:33,940 --> 00:39:43,390
clusters and about running your pipeline

00:39:38,170 --> 00:39:44,080
all together that's basically what I

00:39:43,390 --> 00:39:48,870
have for today

00:39:44,080 --> 00:39:48,870
Oh any questions

00:39:56,900 --> 00:40:02,700
it can be anything for that matter

00:39:59,760 --> 00:40:05,660
so what feed GC is an abstraction of the

00:40:02,700 --> 00:40:08,010
data that can data can be on ad FS or

00:40:05,660 --> 00:40:09,900
anywhere for that matter or you can use

00:40:08,010 --> 00:40:11,970
H catalog to abstract out the data as

00:40:09,900 --> 00:40:14,400
well and the processing again can be a

00:40:11,970 --> 00:40:16,230
pig or a hi or you can define your kuzey

00:40:14,400 --> 00:40:18,000
workflow or a MapReduce job anything for

00:40:16,230 --> 00:40:21,450
that matter so there's not specific to

00:40:18,000 --> 00:40:26,810
any anything in anything in particular

00:40:21,450 --> 00:40:26,810
and it just gives a clean abstraction

00:40:27,170 --> 00:40:33,080
any other questions

00:40:29,780 --> 00:40:33,080

YouTube URL: https://www.youtube.com/watch?v=ZjpTGljWcZo


