Title: Interoperability in the Apache Hive Ecosystem - Sushanth Sowmyan, Mithun Radhakrishnan
Publication date: 2014-04-25
Playlist: ApacheCon North America 2014
Description: 
	ApacheCon North America 2014
Captions: 
	00:00:00,260 --> 00:00:08,429
hello and so Sean I work for Hortonworks

00:00:04,680 --> 00:00:11,429
and this is Nathan who way I work in

00:00:08,429 --> 00:00:12,900
yahoo and we both been working on the

00:00:11,429 --> 00:00:14,969
hedge catalog project which was an

00:00:12,900 --> 00:00:18,359
Apache incubating project for a while

00:00:14,969 --> 00:00:22,199
that got absorbed into the hive Apache

00:00:18,359 --> 00:00:23,760
project and to a large extent it's been

00:00:22,199 --> 00:00:25,439
an interesting journey and we hope we

00:00:23,760 --> 00:00:28,199
can kind of share a bit of that with you

00:00:25,439 --> 00:00:31,470
and also talk about what

00:00:28,199 --> 00:00:35,760
interoperability means for people in the

00:00:31,470 --> 00:00:38,610
hive ecosystem roughly what we are going

00:00:35,760 --> 00:00:40,860
to try to cover in this talk is kind of

00:00:38,610 --> 00:00:42,690
start off with you know the broader

00:00:40,860 --> 00:00:47,730
ecosystem we do live in which is the

00:00:42,690 --> 00:00:50,340
Apache Hadoop well and then try to work

00:00:47,730 --> 00:00:51,660
out where hi fits into that what hedge

00:00:50,340 --> 00:00:53,719
catalog brings to that picture and

00:00:51,660 --> 00:00:56,610
generally what kind of AP Eisen and

00:00:53,719 --> 00:00:59,820
services help you connect to various of

00:00:56,610 --> 00:01:03,120
the tools then we'll quickly talk

00:00:59,820 --> 00:01:06,240
methanol workers through a use case that

00:01:03,120 --> 00:01:08,939
he's been working on at Yahoo it's

00:01:06,240 --> 00:01:10,409
called Project Starling and if we have

00:01:08,939 --> 00:01:12,780
time at the end of it will also talk

00:01:10,409 --> 00:01:16,110
about a new recent initiative called

00:01:12,780 --> 00:01:19,890
Project stinger which is trying to kind

00:01:16,110 --> 00:01:21,930
of bring hive to the update high with

00:01:19,890 --> 00:01:24,840
the latest workings and apache tails and

00:01:21,930 --> 00:01:27,270
things like that before I continue I

00:01:24,840 --> 00:01:30,270
just like to get a quick you know show

00:01:27,270 --> 00:01:31,710
of hands how many few people are very

00:01:30,270 --> 00:01:37,680
familiar with what happens inside

00:01:31,710 --> 00:01:39,810
MapReduce okay so well good we'll

00:01:37,680 --> 00:01:41,729
actually go through and describe what it

00:01:39,810 --> 00:01:45,659
is that we wind up doing in MapReduce

00:01:41,729 --> 00:01:47,340
about the guess so a good way to start

00:01:45,659 --> 00:01:49,649
really is usually the Wikipedia

00:01:47,340 --> 00:01:53,100
definition of what an ecosystem is

00:01:49,649 --> 00:01:54,720
supposed to be and what what really

00:01:53,100 --> 00:01:56,640
speaks to me about the definition here

00:01:54,720 --> 00:01:59,219
is that we're talking about a community

00:01:56,640 --> 00:02:00,899
of living organisms and their

00:01:59,219 --> 00:02:04,920
interactions with other non living

00:02:00,899 --> 00:02:07,740
components and it is useful to me to

00:02:04,920 --> 00:02:11,760
kind of think of Hadoop as the biosphere

00:02:07,740 --> 00:02:13,590
the the base a system that is in place I

00:02:11,760 --> 00:02:14,700
think of HDFS as

00:02:13,590 --> 00:02:16,620
actually the resources that are

00:02:14,700 --> 00:02:18,209
available and all the projects like I

00:02:16,620 --> 00:02:19,410
even pig and whatnot are you living

00:02:18,209 --> 00:02:21,780
beings trying to interact with each

00:02:19,410 --> 00:02:26,750
other and living off what resources you

00:02:21,780 --> 00:02:30,510
have available to you and you know so

00:02:26,750 --> 00:02:33,599
basically what happens is as with any

00:02:30,510 --> 00:02:35,040
other system you want to not be a silo

00:02:33,599 --> 00:02:39,840
you want to be able to communicate

00:02:35,040 --> 00:02:41,790
across different platforms the Apache

00:02:39,840 --> 00:02:45,780
Hadoop ecosystem broadly started off

00:02:41,790 --> 00:02:48,329
with two primary projects one was HDFS

00:02:45,780 --> 00:02:52,560
the others the MapReduce framework the

00:02:48,329 --> 00:02:55,380
HDFS is pretty cool to everything that

00:02:52,560 --> 00:02:57,150
we do in the hadoop world it gives you a

00:02:55,380 --> 00:02:59,549
good fault tolerant distributed file

00:02:57,150 --> 00:03:01,349
system it takes care of things like

00:02:59,549 --> 00:03:03,989
trying to figure out data locality and

00:03:01,349 --> 00:03:05,940
trying to figure out you know that your

00:03:03,989 --> 00:03:08,129
your processing moves to your data in

00:03:05,940 --> 00:03:10,260
extent in conjunction with something

00:03:08,129 --> 00:03:13,079
like mapreduce the MapReduce framework

00:03:10,260 --> 00:03:17,780
itself is kind of the magic behind the

00:03:13,079 --> 00:03:20,040
scenes and I know everyone starts off

00:03:17,780 --> 00:03:22,769
describing Hadoop with a word count

00:03:20,040 --> 00:03:24,090
example and never really spoke to me so

00:03:22,769 --> 00:03:26,209
I will start it off with a slightly

00:03:24,090 --> 00:03:28,709
different example one of anagrams and

00:03:26,209 --> 00:03:31,319
let's say we were trying to you know

00:03:28,709 --> 00:03:33,000
talk about what it is that MapReduce

00:03:31,319 --> 00:03:35,190
really does for us and we had a

00:03:33,000 --> 00:03:38,370
dictionary of words these particular

00:03:35,190 --> 00:03:39,930
words that you know we are trying to

00:03:38,370 --> 00:03:43,739
figure out groups of anagrams in this

00:03:39,930 --> 00:03:45,630
list and so well I mean just eyeballing

00:03:43,739 --> 00:03:47,669
it we can see act and cat are anagrams

00:03:45,630 --> 00:03:50,010
dog doesn't have anything to go along

00:03:47,669 --> 00:03:52,049
with it in this list and you know

00:03:50,010 --> 00:03:53,609
there's a bunch of others that exist as

00:03:52,049 --> 00:03:57,810
well and we want to group them

00:03:53,609 --> 00:03:59,730
essentially as groups so you know the

00:03:57,810 --> 00:04:02,160
first thing that you think of when you

00:03:59,730 --> 00:04:03,959
come across something like this is okay

00:04:02,160 --> 00:04:07,349
so you can come up with a hash map of

00:04:03,959 --> 00:04:09,180
what he say entries where key is

00:04:07,349 --> 00:04:10,799
something that is common across the

00:04:09,180 --> 00:04:13,139
entire groups that you're trying to

00:04:10,799 --> 00:04:14,970
identify and the value is just a list of

00:04:13,139 --> 00:04:19,049
all the things that have that same key

00:04:14,970 --> 00:04:21,620
and the trick is really coming up with

00:04:19,049 --> 00:04:24,450
what's appropriate for that key and

00:04:21,620 --> 00:04:25,950
essentially you know if we wanted to

00:04:24,450 --> 00:04:27,360
have a representation of what our

00:04:25,950 --> 00:04:29,789
desired result is it's

00:04:27,360 --> 00:04:31,830
so great you know it's easy enough to

00:04:29,789 --> 00:04:34,139
code up something that would you say

00:04:31,830 --> 00:04:36,840
puts words into a hashmap by say

00:04:34,139 --> 00:04:39,270
something like sorting the list of what

00:04:36,840 --> 00:04:41,159
do you say the words that we have and

00:04:39,270 --> 00:04:43,169
getting keys for that but what if your

00:04:41,159 --> 00:04:45,090
hash map is too big to fit in memory

00:04:43,169 --> 00:04:46,919
well one of the first things that you

00:04:45,090 --> 00:04:49,439
can do is clearly you can kind of just

00:04:46,919 --> 00:04:51,150
make sure that you're emitting only one

00:04:49,439 --> 00:04:52,860
key value at a time and not trying to do

00:04:51,150 --> 00:04:54,840
any sort of grouping and then you can

00:04:52,860 --> 00:04:56,460
try and sort it later and the

00:04:54,840 --> 00:04:58,800
distributed merge sort something slide

00:04:56,460 --> 00:05:00,599
that will help you do that and then once

00:04:58,800 --> 00:05:03,960
you have that you can you can kind of be

00:05:00,599 --> 00:05:06,060
assured that adjacent values in that

00:05:03,960 --> 00:05:07,439
stream are all going to have the same

00:05:06,060 --> 00:05:09,479
key and then you can do some sort of

00:05:07,439 --> 00:05:11,129
grouping which is a lot more memory

00:05:09,479 --> 00:05:16,590
efficient than trying to put everything

00:05:11,129 --> 00:05:19,800
in one large hash map and so well this

00:05:16,590 --> 00:05:21,719
form of thinking about things where your

00:05:19,800 --> 00:05:23,729
first step is essentially some sort of

00:05:21,719 --> 00:05:26,879
mapping some sort of finding a

00:05:23,729 --> 00:05:29,879
commonality that kind of helps you group

00:05:26,879 --> 00:05:34,259
or classify a bunch of what do you say

00:05:29,879 --> 00:05:35,729
you know items and then to split split

00:05:34,259 --> 00:05:37,949
that problem up so that it is

00:05:35,729 --> 00:05:40,710
parallelizable and then for each

00:05:37,949 --> 00:05:42,659
parallel fork so to speak you kind of

00:05:40,710 --> 00:05:45,060
can apply some sort of other function

00:05:42,659 --> 00:05:48,330
that basically tries to summarize or try

00:05:45,060 --> 00:05:50,819
to what'd he say reduce and your

00:05:48,330 --> 00:05:54,960
vocabulary is going to differ based on

00:05:50,819 --> 00:05:58,169
which world you come from you know so

00:05:54,960 --> 00:06:00,139
this sort of computation is what makes

00:05:58,169 --> 00:06:03,659
MapReduce tick it's what MapReduce

00:06:00,139 --> 00:06:05,189
really is and MapReduce is also a

00:06:03,659 --> 00:06:06,870
framework that enables the sort of

00:06:05,189 --> 00:06:09,900
computation where you can simply write

00:06:06,870 --> 00:06:11,610
one function or a class that implements

00:06:09,900 --> 00:06:15,800
a mapper and another that implements a

00:06:11,610 --> 00:06:17,879
reducer and then set it in motion and

00:06:15,800 --> 00:06:21,240
associated with that is also this notion

00:06:17,879 --> 00:06:24,419
of data locality which is primal and you

00:06:21,240 --> 00:06:26,729
know obviously this is important for

00:06:24,419 --> 00:06:30,240
jobs that are large enough so that

00:06:26,729 --> 00:06:32,219
you're you know data size do not fit in

00:06:30,240 --> 00:06:36,240
memory and that's really where I draw

00:06:32,219 --> 00:06:38,370
the line at what makes big data so

00:06:36,240 --> 00:06:40,709
essentially in a broad basis it will

00:06:38,370 --> 00:06:41,169
come back to this later on you have some

00:06:40,709 --> 00:06:44,080
sort of

00:06:41,169 --> 00:06:46,090
operation and then you wind up splitting

00:06:44,080 --> 00:06:48,819
your input data and there's an input

00:06:46,090 --> 00:06:50,740
phase so to speak that happens and I'm

00:06:48,819 --> 00:06:53,050
deliberately calling it an input phase

00:06:50,740 --> 00:06:55,060
rather than a map phase for now and

00:06:53,050 --> 00:06:56,919
there are map of functions that

00:06:55,060 --> 00:06:59,199
basically run on that input data that

00:06:56,919 --> 00:07:02,069
are doing various you know data

00:06:59,199 --> 00:07:04,449
massaging all right and then you have

00:07:02,069 --> 00:07:06,069
what I consider to be the elephant to

00:07:04,449 --> 00:07:09,099
the room that is a sort shuffle phase

00:07:06,069 --> 00:07:12,090
that no one really talks much about but

00:07:09,099 --> 00:07:14,469
we'll get to that in a bit and basically

00:07:12,090 --> 00:07:16,960
there's some sort of data massaging that

00:07:14,469 --> 00:07:19,180
I mean movement that happens across the

00:07:16,960 --> 00:07:20,620
board and then you have some sort of

00:07:19,180 --> 00:07:22,240
grouping and aggregation that happens

00:07:20,620 --> 00:07:24,129
and maybe you have something else or

00:07:22,240 --> 00:07:26,879
here so you have an input side and an

00:07:24,129 --> 00:07:30,129
output side to this and associated that

00:07:26,879 --> 00:07:32,800
MapReduce defines a couple of formats a

00:07:30,129 --> 00:07:35,189
couple of interfaces called input format

00:07:32,800 --> 00:07:38,050
and output format which if you implement

00:07:35,189 --> 00:07:41,580
you can actually do a lot you can do

00:07:38,050 --> 00:07:44,830
what is essentially i/o in Hadoop from a

00:07:41,580 --> 00:07:49,509
perspective of a higher you know higher

00:07:44,830 --> 00:07:52,180
level two so you know what is hive hive

00:07:49,509 --> 00:07:54,460
if I put it down really concisely is a

00:07:52,180 --> 00:07:55,990
database engine that works on her do and

00:07:54,460 --> 00:07:57,460
I like the term database engine because

00:07:55,990 --> 00:07:59,589
it speaks to a lot more than saying that

00:07:57,460 --> 00:08:01,990
it's a data warehousing infrastructure

00:07:59,589 --> 00:08:04,000
which is where these comes from I mean a

00:08:01,990 --> 00:08:05,860
data warehousing infrastructure broadly

00:08:04,000 --> 00:08:08,860
speaking has this notion of feeds and

00:08:05,860 --> 00:08:10,990
data and input and output and you know

00:08:08,860 --> 00:08:15,550
extract transform loading that sort of a

00:08:10,990 --> 00:08:17,379
thing so basically you have that kind of

00:08:15,550 --> 00:08:18,669
an ocean but when you say something is

00:08:17,379 --> 00:08:20,560
the database engine you're talking about

00:08:18,669 --> 00:08:22,240
a little more you're talking about the

00:08:20,560 --> 00:08:24,999
notion that your data is fundamentally

00:08:22,240 --> 00:08:29,199
tabular in nature that there is some

00:08:24,999 --> 00:08:30,550
sort of strong schema to things and you

00:08:29,199 --> 00:08:32,560
also have this notion of storage in

00:08:30,550 --> 00:08:34,810
abstractions remember when I said that

00:08:32,560 --> 00:08:36,940
you know you have an input side under an

00:08:34,810 --> 00:08:38,889
output side well there are multiple

00:08:36,940 --> 00:08:41,229
implementations of input formats and

00:08:38,889 --> 00:08:43,029
output formats and some people might

00:08:41,229 --> 00:08:45,550
want to store data and a columnar format

00:08:43,029 --> 00:08:47,079
some people may want to store it as JSON

00:08:45,550 --> 00:08:48,970
because they just imported it from some

00:08:47,079 --> 00:08:51,370
other external system some other people

00:08:48,970 --> 00:08:54,310
may want to what'd he say export it to a

00:08:51,370 --> 00:08:55,140
database so that is kind of freeform to

00:08:54,310 --> 00:08:57,830
an extent

00:08:55,140 --> 00:09:00,900
we want it to be marinated driven and

00:08:57,830 --> 00:09:04,440
and when I say metadata driven I mean

00:09:00,900 --> 00:09:06,630
that the notion of what your table is

00:09:04,440 --> 00:09:09,420
and how to read it and how to write it

00:09:06,630 --> 00:09:11,970
what kind of optimizations exist on it

00:09:09,420 --> 00:09:14,820
are all stored in some sort of meta

00:09:11,970 --> 00:09:17,430
store and this is available to anybody

00:09:14,820 --> 00:09:20,520
that is using that matter store it need

00:09:17,430 --> 00:09:23,910
not be you know hive itself and not all

00:09:20,520 --> 00:09:27,090
metadata is evil it's it's based on

00:09:23,910 --> 00:09:30,180
sequel which means well traditionally

00:09:27,090 --> 00:09:32,160
hive has always called itself hive ql I

00:09:30,180 --> 00:09:34,170
mean the query language that works with

00:09:32,160 --> 00:09:36,120
a high fuel and it's not claimed sequel

00:09:34,170 --> 00:09:37,980
but you know that's kind of just mostly

00:09:36,120 --> 00:09:39,750
kidding ourselves there it's forever

00:09:37,980 --> 00:09:42,270
been trying to get closer and closer to

00:09:39,750 --> 00:09:44,520
sequel it's been adding new features

00:09:42,270 --> 00:09:46,350
most recently especially even things

00:09:44,520 --> 00:09:49,500
like authorization all that that try to

00:09:46,350 --> 00:09:51,840
work on a sequel standard form and it's

00:09:49,500 --> 00:09:54,450
whatever trying to build that and so

00:09:51,840 --> 00:09:57,090
I've essentially is trying to work on

00:09:54,450 --> 00:10:01,170
this notion of being a database engine

00:09:57,090 --> 00:10:03,540
that works on top of Hadoop and you know

00:10:01,170 --> 00:10:06,900
other tools that exist that have also

00:10:03,540 --> 00:10:10,230
kind of been compensated in some sense

00:10:06,900 --> 00:10:12,150
for hi other kinds of metaphors and

00:10:10,230 --> 00:10:14,820
communities that kind of compete with

00:10:12,150 --> 00:10:17,040
high if you will you know let's look at

00:10:14,820 --> 00:10:21,210
each of them to an extent you have well

00:10:17,040 --> 00:10:23,340
peg biggest I used to work in yahoo and

00:10:21,210 --> 00:10:26,730
you know one of the things is Yahoo is a

00:10:23,340 --> 00:10:29,400
huge big shot and the notion of being

00:10:26,730 --> 00:10:32,000
able to that pig can kind of eat

00:10:29,400 --> 00:10:35,040
anything is kind of fundamental to ping

00:10:32,000 --> 00:10:36,840
hive on the other hand basically comes

00:10:35,040 --> 00:10:38,820
from this notion of saying you know we

00:10:36,840 --> 00:10:40,740
want to be formal database kind of

00:10:38,820 --> 00:10:44,100
anything we want to speak sequel a

00:10:40,740 --> 00:10:46,290
sequel like at least pig comes from an

00:10:44,100 --> 00:10:48,900
ETL well it appeals to scripters and

00:10:46,290 --> 00:10:51,110
programmers hive up yields to those that

00:10:48,900 --> 00:10:53,490
are more familiar with sequel and

00:10:51,110 --> 00:10:55,320
fundamentally also and I think this is

00:10:53,490 --> 00:10:57,600
crucial that business intelligence tools

00:10:55,320 --> 00:11:00,720
are very often written against an

00:10:57,600 --> 00:11:03,960
assumed sequel they written against jdbc

00:11:00,720 --> 00:11:05,790
drivers or DBC drivers you know and that

00:11:03,960 --> 00:11:09,209
actually makes a huge difference so even

00:11:05,790 --> 00:11:11,040
if big were to add metadata

00:11:09,209 --> 00:11:12,779
associated with all the data that it

00:11:11,040 --> 00:11:15,929
stores which is something we tried doing

00:11:12,779 --> 00:11:17,879
actually at the end of the day pig is

00:11:15,929 --> 00:11:21,839
trying to solve a different use case

00:11:17,879 --> 00:11:23,699
than what I was trying to solve and it

00:11:21,839 --> 00:11:25,110
comes down really to stylistic

00:11:23,699 --> 00:11:28,499
preferences and differences in the

00:11:25,110 --> 00:11:31,230
communities what about HBase I mean with

00:11:28,499 --> 00:11:34,230
a name like HBase which proclaims Hadoop

00:11:31,230 --> 00:11:35,850
database is that not a database as well

00:11:34,230 --> 00:11:38,610
well depends on what you mean by

00:11:35,850 --> 00:11:40,769
database HBase is excellent for random

00:11:38,610 --> 00:11:43,910
read rights please do not try to use

00:11:40,769 --> 00:11:46,170
high for a random read/write use case

00:11:43,910 --> 00:11:49,110
HBase is excellent for millions of

00:11:46,170 --> 00:11:51,689
Rosalind millions of columns again

00:11:49,110 --> 00:11:53,189
please if you ever have a use case that

00:11:51,689 --> 00:11:55,439
requires millions of columns do not use

00:11:53,189 --> 00:11:58,589
hive you're just asking for a world of

00:11:55,439 --> 00:12:01,769
pain and they're not common use cases

00:11:58,589 --> 00:12:05,249
for for relational operations that hive

00:12:01,769 --> 00:12:07,769
is really trying to appeal to and so in

00:12:05,249 --> 00:12:10,589
terms of optimizations in terms of what

00:12:07,769 --> 00:12:13,679
kind of problems are unique to the kind

00:12:10,589 --> 00:12:17,579
of project the kind of push in the hive

00:12:13,679 --> 00:12:20,429
community is a different kind of push so

00:12:17,579 --> 00:12:24,720
now well talk a little bit about his

00:12:20,429 --> 00:12:27,240
catalog which started off essentially as

00:12:24,720 --> 00:12:29,279
a project called owl which was supposed

00:12:27,240 --> 00:12:31,920
to try to which is trying to be another

00:12:29,279 --> 00:12:34,019
meta store and it was being developed

00:12:31,920 --> 00:12:39,240
for pigs so that pig could speak sequel

00:12:34,019 --> 00:12:40,499
and we went a way into this and then we

00:12:39,240 --> 00:12:42,749
realized that the hive meta stored

00:12:40,499 --> 00:12:45,929
already had quite a bit of adoption in

00:12:42,749 --> 00:12:47,490
the wider community and we also have it

00:12:45,929 --> 00:12:50,009
also had quite a few features that were

00:12:47,490 --> 00:12:51,559
really useful and we didn't want to

00:12:50,009 --> 00:12:54,990
necessarily split the community between

00:12:51,559 --> 00:12:57,360
hive and pig worlds we wanted them to

00:12:54,990 --> 00:12:58,619
talk to each other so we said okay we

00:12:57,360 --> 00:13:00,980
will blew the same kind of storage

00:12:58,619 --> 00:13:03,329
abstraction notion that we have with

00:13:00,980 --> 00:13:05,279
that we're trying to go for that allows

00:13:03,329 --> 00:13:07,559
interoperability between various

00:13:05,279 --> 00:13:09,809
projects and we will try to build it on

00:13:07,559 --> 00:13:12,899
top of the hive meta store and so we

00:13:09,809 --> 00:13:14,160
called it hull and as with many of the

00:13:12,899 --> 00:13:16,769
people that are familiar with apache

00:13:14,160 --> 00:13:18,480
trademark processes the other projects

00:13:16,769 --> 00:13:21,360
call all those other projects call house

00:13:18,480 --> 00:13:24,020
okay we come up with h catalog and no

00:13:21,360 --> 00:13:26,880
other project has that name

00:13:24,020 --> 00:13:29,100
so we incubated for a little over a year

00:13:26,880 --> 00:13:33,320
and then we got absorbed into the high

00:13:29,100 --> 00:13:36,360
project and what is his catalog really

00:13:33,320 --> 00:13:38,490
this line here which says that it's a

00:13:36,360 --> 00:13:41,330
metadata back storage abstraction is

00:13:38,490 --> 00:13:43,470
what it was built to be but

00:13:41,330 --> 00:13:45,150
fundamentally his catalog is many

00:13:43,470 --> 00:13:47,220
different things to many different users

00:13:45,150 --> 00:13:49,380
it's very much the case of you know

00:13:47,220 --> 00:13:51,690
blind people touching different parts of

00:13:49,380 --> 00:13:55,260
an elephant and feeling that it's many

00:13:51,690 --> 00:13:58,260
different things and what it was built

00:13:55,260 --> 00:14:01,500
to be was so that you can use the hive

00:13:58,260 --> 00:14:04,560
meta store you can use information about

00:14:01,500 --> 00:14:07,170
tables and and data and basically build

00:14:04,560 --> 00:14:08,880
on build many other high-level tools on

00:14:07,170 --> 00:14:10,830
top of that which all communicate with

00:14:08,880 --> 00:14:13,920
the same notion of the same kind of

00:14:10,830 --> 00:14:15,930
table and data and so your notion is

00:14:13,920 --> 00:14:19,530
that big will be able to read and write

00:14:15,930 --> 00:14:23,370
five tables HBase will be able to kind

00:14:19,530 --> 00:14:25,560
of be a storage engine for hive or other

00:14:23,370 --> 00:14:27,540
custom tools that are written for kind

00:14:25,560 --> 00:14:30,360
of workflow management can then

00:14:27,540 --> 00:14:34,740
understand tables and processes that

00:14:30,360 --> 00:14:36,810
work on it so and another in core

00:14:34,740 --> 00:14:39,750
feature that hedge catalog was built for

00:14:36,810 --> 00:14:41,370
was for data migrations again it was

00:14:39,750 --> 00:14:43,530
kind of a yahoo specific use case

00:14:41,370 --> 00:14:45,420
because at that time you were heavily

00:14:43,530 --> 00:14:47,970
invested in a data format called zebra

00:14:45,420 --> 00:14:50,370
whereas I've used to live in a world

00:14:47,970 --> 00:14:53,520
entirely deploying on a file format

00:14:50,370 --> 00:14:56,610
colossi file and when you have hundreds

00:14:53,520 --> 00:14:59,070
of you know terabytes of data that are

00:14:56,610 --> 00:15:01,530
in one format it is not easy to go ahead

00:14:59,070 --> 00:15:03,470
and say okay create a new table last

00:15:01,530 --> 00:15:06,560
select everything from the old table

00:15:03,470 --> 00:15:10,200
yeah your IT is going to come up to you

00:15:06,560 --> 00:15:11,970
so what you can do with a and in

00:15:10,200 --> 00:15:13,380
addition to that in a lot of cases what

00:15:11,970 --> 00:15:16,170
winds up happening is you wind up hard

00:15:13,380 --> 00:15:18,030
coding dependencies and your program is

00:15:16,170 --> 00:15:20,940
hard-coded to work with or not with our

00:15:18,030 --> 00:15:25,290
C files and it's a huge production asst

00:15:20,940 --> 00:15:28,080
to ask people to shift the wrong so well

00:15:25,290 --> 00:15:29,580
what has catalog allows you to do is you

00:15:28,080 --> 00:15:31,260
write a MapReduce program to work with

00:15:29,580 --> 00:15:34,650
hedge catalog and you don't care what

00:15:31,260 --> 00:15:36,520
the underlying storage system is and you

00:15:34,650 --> 00:15:38,830
can just quickly matter

00:15:36,520 --> 00:15:41,560
phew metadata bits switch it over to a

00:15:38,830 --> 00:15:43,950
new system and I mean say zebra or

00:15:41,560 --> 00:15:46,570
whatever rather have you nowadays arc

00:15:43,950 --> 00:15:48,490
and what would happen is you would let

00:15:46,570 --> 00:15:50,590
retention manage your data migration

00:15:48,490 --> 00:15:53,620
where all your old data continues to be

00:15:50,590 --> 00:15:56,310
in our C file all your new data is

00:15:53,620 --> 00:15:58,780
written out as ark and as far as tables

00:15:56,310 --> 00:16:01,870
concern it still sees it as a stream of

00:15:58,780 --> 00:16:03,610
data and over time your old data will

00:16:01,870 --> 00:16:06,610
have been deleted or collected by a

00:16:03,610 --> 00:16:10,690
retention what easy tools and your new

00:16:06,610 --> 00:16:12,550
data will now be arc and fundamental to

00:16:10,690 --> 00:16:15,430
this is this notion of you know

00:16:12,550 --> 00:16:17,920
interoperability I keep talking about

00:16:15,430 --> 00:16:21,120
common shared metadata and about how

00:16:17,920 --> 00:16:23,920
that's crucial to what we're doing well

00:16:21,120 --> 00:16:25,390
you you need to be able to access that

00:16:23,920 --> 00:16:27,760
metadata how do you go about doing that

00:16:25,390 --> 00:16:29,470
well services are one way of doing it

00:16:27,760 --> 00:16:31,360
and in fact it is the way of going about

00:16:29,470 --> 00:16:34,960
it so this is our well-known abstraction

00:16:31,360 --> 00:16:38,800
so at the core of it before H catalog

00:16:34,960 --> 00:16:40,720
before you know many of these services

00:16:38,800 --> 00:16:43,300
what you could do is if you wrote a tool

00:16:40,720 --> 00:16:44,920
you could well connect to the my sequel

00:16:43,300 --> 00:16:48,460
database or whatever database was

00:16:44,920 --> 00:16:51,220
touring the Metis the body saved

00:16:48,460 --> 00:16:54,250
metadata and you could write out your

00:16:51,220 --> 00:16:56,740
sequel calls this is bad for several

00:16:54,250 --> 00:17:00,340
reasons and the least of which is that

00:16:56,740 --> 00:17:02,800
metadata changes over time basically you

00:17:00,340 --> 00:17:04,900
you are asking for data corruptions you

00:17:02,800 --> 00:17:06,930
are asking for assumptions on data

00:17:04,900 --> 00:17:09,760
models that change over time and

00:17:06,930 --> 00:17:12,610
basically this is asking for production

00:17:09,760 --> 00:17:14,830
issues well luckily you have a meta

00:17:12,610 --> 00:17:17,080
store associated with it so highly used

00:17:14,830 --> 00:17:18,910
to be kind of a fat client which did a

00:17:17,080 --> 00:17:20,800
lot of the parsing and any other

00:17:18,910 --> 00:17:23,470
metadata access and then just connect it

00:17:20,800 --> 00:17:25,900
to a database to do it you know

00:17:23,470 --> 00:17:27,760
background store well we evolved that's

00:17:25,900 --> 00:17:29,890
likely saying no a metadata server is

00:17:27,760 --> 00:17:32,470
important and this server would work on

00:17:29,890 --> 00:17:34,630
a thrift interface and basically to take

00:17:32,470 --> 00:17:36,670
connections from other tools or other

00:17:34,630 --> 00:17:38,950
developers and basically do whatever

00:17:36,670 --> 00:17:41,500
metadata operations are needed so

00:17:38,950 --> 00:17:43,780
associated with that we have one one

00:17:41,500 --> 00:17:46,690
more step of indirection and r1 and one

00:17:43,780 --> 00:17:48,730
more step of safety there's another tool

00:17:46,690 --> 00:17:49,720
called web hedge cat that we've what he

00:17:48,730 --> 00:17:51,700
say

00:17:49,720 --> 00:17:54,490
that is part of his catalogue and it's

00:17:51,700 --> 00:17:57,190
kind of arrest services API on top of a

00:17:54,490 --> 00:17:59,799
lot of tasks fundamentally written for

00:17:57,190 --> 00:18:02,140
things like you know your basic ddl

00:17:59,799 --> 00:18:03,549
operations like creating a table and you

00:18:02,140 --> 00:18:05,320
know describing a table showing

00:18:03,549 --> 00:18:06,970
partitions and things like that which

00:18:05,320 --> 00:18:10,480
allows other tools to interact directly

00:18:06,970 --> 00:18:12,970
with what he say the meta store and

00:18:10,480 --> 00:18:14,200
that's a useful kind of abstraction for

00:18:12,970 --> 00:18:17,260
people that don't want to connect

00:18:14,200 --> 00:18:18,880
directly to thrift or deal with what

00:18:17,260 --> 00:18:20,950
looks like a lower level interface

00:18:18,880 --> 00:18:24,450
because essentially the high meta store

00:18:20,950 --> 00:18:28,390
even if you know it's a thrift based

00:18:24,450 --> 00:18:30,970
server what it does is it is written at

00:18:28,390 --> 00:18:33,700
a very low level you have things like

00:18:30,970 --> 00:18:36,700
add partitions no transaction it's like

00:18:33,700 --> 00:18:38,559
okay so it is too low level access and

00:18:36,700 --> 00:18:40,510
you want to kind of have that at a

00:18:38,559 --> 00:18:43,600
higher level and where get solves

00:18:40,510 --> 00:18:46,809
a bunch of that and what web which can

00:18:43,600 --> 00:18:48,010
does for medial operations well web

00:18:46,809 --> 00:18:49,690
which can also does some other

00:18:48,010 --> 00:18:53,620
operations it allows you to launch hive

00:18:49,690 --> 00:18:55,330
queries launch pig queries basically

00:18:53,620 --> 00:18:57,159
also launched a custom MapReduce job if

00:18:55,330 --> 00:18:59,440
you have it it acts as a generic web

00:18:57,159 --> 00:19:01,870
service as well hive survey two is a

00:18:59,440 --> 00:19:04,809
more recent development which also is

00:19:01,870 --> 00:19:07,929
trying to provide services hive survey

00:19:04,809 --> 00:19:10,030
to really helps out in the kind of JDBC

00:19:07,929 --> 00:19:13,059
kind of use cases or it helps out in

00:19:10,030 --> 00:19:15,520
cases where you have a sequel string and

00:19:13,059 --> 00:19:17,559
you just want to send that along to hive

00:19:15,520 --> 00:19:23,080
as a service and that's what hive

00:19:17,559 --> 00:19:24,460
servitude does fundamentally again one

00:19:23,080 --> 00:19:28,770
other thing that his catalog tries to

00:19:24,460 --> 00:19:31,390
solve is this notion of API points and

00:19:28,770 --> 00:19:33,220
it provides an input format outperform

00:19:31,390 --> 00:19:35,320
at abstraction as I mentioned earlier it

00:19:33,220 --> 00:19:37,059
also forms a record abstraction so that

00:19:35,320 --> 00:19:39,280
you can pretty much treat it as a list

00:19:37,059 --> 00:19:41,830
of job I mean a java list of object and

00:19:39,280 --> 00:19:46,059
you know read your row it also provides

00:19:41,830 --> 00:19:47,740
a schema notion associated with that it

00:19:46,059 --> 00:19:51,150
provides the reader and writer interface

00:19:47,740 --> 00:19:54,360
for those that do not want to you know

00:19:51,150 --> 00:19:56,650
deploy their jobs in a MapReduce task

00:19:54,360 --> 00:19:58,299
essentially for example teradata is

00:19:56,650 --> 00:20:01,390
interested in this because they have

00:19:58,299 --> 00:20:03,490
connectivity requirements with Hadoop to

00:20:01,390 --> 00:20:04,929
be able to access data that's on how do

00:20:03,490 --> 00:20:07,330
but they don't necessarily want to run

00:20:04,929 --> 00:20:09,580
inside a MapReduce job so hedge cat

00:20:07,330 --> 00:20:11,140
reader-writer abstracts away what hedge

00:20:09,580 --> 00:20:13,660
cat and perform at output formats ends

00:20:11,140 --> 00:20:16,360
and allows you to do it on a single

00:20:13,660 --> 00:20:18,670
thread on a separate machine there's a

00:20:16,360 --> 00:20:20,470
notification center face hedge cat can

00:20:18,670 --> 00:20:22,210
provide JMS notifications for when

00:20:20,470 --> 00:20:23,890
certain tasks are done when a partition

00:20:22,210 --> 00:20:26,170
gets published and things like this is

00:20:23,890 --> 00:20:27,940
really useful in a larger use case from

00:20:26,170 --> 00:20:29,530
say you see where you have a workflow

00:20:27,940 --> 00:20:31,540
that you want to set up and you want to

00:20:29,530 --> 00:20:33,700
say run this process when that finishes

00:20:31,540 --> 00:20:36,130
trigger this win this and that are

00:20:33,700 --> 00:20:38,200
finished trigger this so that's really

00:20:36,130 --> 00:20:41,250
useful web which cat again has its own

00:20:38,200 --> 00:20:43,570
you know REST API associated with it

00:20:41,250 --> 00:20:45,970
hedge cat client was actually initially

00:20:43,570 --> 00:20:49,540
written to test web cat but

00:20:45,970 --> 00:20:53,320
primarily has become a neater Java API

00:20:49,540 --> 00:20:56,950
to connect to the meta store and so we

00:20:53,320 --> 00:20:59,740
have a ton of EP ice which is kind of

00:20:56,950 --> 00:21:01,000
both useful and concerning because you

00:20:59,740 --> 00:21:04,690
kind of want to make sure that you're

00:21:01,000 --> 00:21:06,429
not you're not kind of splitting your

00:21:04,690 --> 00:21:08,860
efforts and just creating a hodgepodge

00:21:06,429 --> 00:21:10,630
of various use cases that people don't

00:21:08,860 --> 00:21:12,040
know what they're going to use and more

00:21:10,630 --> 00:21:14,290
importantly is more difficult to

00:21:12,040 --> 00:21:16,030
maintain storage handle is another

00:21:14,290 --> 00:21:19,420
abstraction that private self provides

00:21:16,030 --> 00:21:22,600
which allows other systems to kind of

00:21:19,420 --> 00:21:25,390
manage storage for hive we've talked

00:21:22,600 --> 00:21:27,760
about HBase interacting with high for

00:21:25,390 --> 00:21:30,120
example HBase implements a storage

00:21:27,760 --> 00:21:31,929
handling and this is kind of

00:21:30,120 --> 00:21:35,140
interoperability from the bottom level

00:21:31,929 --> 00:21:37,870
and in all you know everything to do

00:21:35,140 --> 00:21:39,850
with standards and AP ice this is

00:21:37,870 --> 00:21:42,880
actually a part that you have to kind of

00:21:39,850 --> 00:21:44,800
be careful with and you know there's a

00:21:42,880 --> 00:21:47,530
there's always a relevant obligatory

00:21:44,800 --> 00:21:49,420
XKCD associated with this and this is a

00:21:47,530 --> 00:21:53,050
problem that we kind of try to deal with

00:21:49,420 --> 00:21:55,179
and minimize interaction points but

00:21:53,050 --> 00:21:57,130
there are that many relevant use cases

00:21:55,179 --> 00:21:59,830
that have driven developments on each of

00:21:57,130 --> 00:22:02,350
those things this I'll pass it over to

00:21:59,830 --> 00:22:07,270
meth on and so I will tell ago but I

00:22:02,350 --> 00:22:09,580
will talk about government so right now

00:22:07,270 --> 00:22:12,820
so chance is mentioned a fair few

00:22:09,580 --> 00:22:16,270
components in the ecosystem as well as

00:22:12,820 --> 00:22:16,700
how they interact I thought I would give

00:22:16,270 --> 00:22:19,940
you an

00:22:16,700 --> 00:22:21,770
to illustrate this and the example I use

00:22:19,940 --> 00:22:26,000
is a program that's running in

00:22:21,770 --> 00:22:28,760
production at Yahoo today this project

00:22:26,000 --> 00:22:33,290
is called Starling a little bit of

00:22:28,760 --> 00:22:36,560
background Yahoo runs Hadoop at scale so

00:22:33,290 --> 00:22:39,980
we have about 25 Hadoop clusters each

00:22:36,560 --> 00:22:42,800
cluster has several thousand nodes and

00:22:39,980 --> 00:22:47,150
between them they process millions and

00:22:42,800 --> 00:22:51,920
millions of Hadoop jobs every day now an

00:22:47,150 --> 00:22:54,370
interesting byproduct of the Hadoop jobs

00:22:51,920 --> 00:22:57,800
running is that you get a ton of logs

00:22:54,370 --> 00:23:02,450
which are specific to Hadoop for example

00:22:57,800 --> 00:23:06,200
you have job history job summaries file

00:23:02,450 --> 00:23:07,910
system access audit logs you have stack

00:23:06,200 --> 00:23:11,300
traces from task attempts that failed

00:23:07,910 --> 00:23:13,460
and so on so as a data geek you have to

00:23:11,300 --> 00:23:19,670
ask yourself is there anything to be

00:23:13,460 --> 00:23:22,490
gained by analyzing that data now since

00:23:19,670 --> 00:23:25,520
this data is in these several hundreds

00:23:22,490 --> 00:23:28,550
of gigabytes in itself you might want to

00:23:25,520 --> 00:23:31,160
actually analyze the Hadoop logs in

00:23:28,550 --> 00:23:33,380
Hadoop which then produces further

00:23:31,160 --> 00:23:38,750
Hadoop blogs which can then be processed

00:23:33,380 --> 00:23:41,300
again Turtles all the way down so what

00:23:38,750 --> 00:23:43,640
do we gain from running the sort of

00:23:41,300 --> 00:23:46,160
analysis and and how would you run this

00:23:43,640 --> 00:23:51,560
kind of analysis let's take a few use

00:23:46,160 --> 00:23:54,470
cases the first one is probably the

00:23:51,560 --> 00:23:57,920
easiest one metering you might want to

00:23:54,470 --> 00:24:00,650
go through the job history logs you can

00:23:57,920 --> 00:24:02,450
group by user and some runtimes and

00:24:00,650 --> 00:24:06,170
figure out who is using the most amount

00:24:02,450 --> 00:24:10,630
of computer on your grids and you can

00:24:06,170 --> 00:24:14,750
charge him if you need to you can you

00:24:10,630 --> 00:24:17,990
can graph the distribution of job types

00:24:14,750 --> 00:24:21,850
that is raw MapReduce versus pig in hive

00:24:17,990 --> 00:24:26,030
and you can gain some insight there

00:24:21,850 --> 00:24:30,200
suppose the grid team has shipped pig 11

00:24:26,030 --> 00:24:30,559
to the clusters now you might want to go

00:24:30,200 --> 00:24:32,210
back

00:24:30,559 --> 00:24:36,200
look at adoption you might want to see

00:24:32,210 --> 00:24:38,749
who's using pig 10 or pig 9 and have

00:24:36,200 --> 00:24:43,720
them migrator pig 11 at gunpoint if need

00:24:38,749 --> 00:24:48,200
be further down a little less trivial

00:24:43,720 --> 00:24:50,559
you might want to you might want to look

00:24:48,200 --> 00:24:54,320
at your job failures as well as your

00:24:50,559 --> 00:24:57,080
poorly performing jobs and go into why

00:24:54,320 --> 00:25:00,649
for example you might have jobs whose

00:24:57,080 --> 00:25:03,350
iOS or mb is set to too low so your

00:25:00,649 --> 00:25:05,990
records are being spilled to disk and so

00:25:03,350 --> 00:25:08,539
you're losing performance so you can

00:25:05,990 --> 00:25:12,559
identify these jobs and either alert the

00:25:08,539 --> 00:25:15,350
users or a possible future thing is to

00:25:12,559 --> 00:25:19,190
tune the job so that the next time they

00:25:15,350 --> 00:25:21,230
run they use a greater iOS or mb in a

00:25:19,190 --> 00:25:24,499
similar way you might have job failures

00:25:21,230 --> 00:25:30,259
entirely because your yarn containers

00:25:24,499 --> 00:25:32,809
are too small to accommodate the larger

00:25:30,259 --> 00:25:35,389
heap size so you might want to either

00:25:32,809 --> 00:25:39,409
auto-tune that or you might want to

00:25:35,389 --> 00:25:45,980
alert users or change your cluster

00:25:39,409 --> 00:25:49,549
defaults which brings me to probably my

00:25:45,980 --> 00:25:52,850
favorite use case here let's let's take

00:25:49,549 --> 00:25:57,320
an example let's say there's a data set

00:25:52,850 --> 00:26:01,480
that captures search logs across yahoo

00:25:57,320 --> 00:26:04,220
properties in the u.s. aggregated daily

00:26:01,480 --> 00:26:08,990
now if you were to ask the producer of

00:26:04,220 --> 00:26:12,169
this data how long do you want this to

00:26:08,990 --> 00:26:15,440
be retained on HDFS and this is

00:26:12,169 --> 00:26:21,679
terabytes vlogs so he's likely to say oh

00:26:15,440 --> 00:26:28,759
maybe a year now that's a number he

00:26:21,679 --> 00:26:30,710
probably pulled out his mind and it's a

00:26:28,759 --> 00:26:32,720
pessimistic view it's probably something

00:26:30,710 --> 00:26:37,669
that he didn't spend too much time

00:26:32,720 --> 00:26:40,549
thinking about us on the other hand we

00:26:37,669 --> 00:26:43,040
would like to reduce this to a number

00:26:40,549 --> 00:26:46,070
that's as small as is viable

00:26:43,040 --> 00:26:49,610
now how would you identify what the

00:26:46,070 --> 00:26:52,610
ideal retention period should be so what

00:26:49,610 --> 00:26:56,230
we did was we let things run with the

00:26:52,610 --> 00:26:58,610
years retention and then over time we

00:26:56,230 --> 00:27:00,860
rechecked the file system access and

00:26:58,610 --> 00:27:04,370
audit logs we figured out the file

00:27:00,860 --> 00:27:07,640
creation times and the file last access

00:27:04,370 --> 00:27:09,830
times and we are plotted graphs we saw

00:27:07,640 --> 00:27:13,070
that for the most part for most data

00:27:09,830 --> 00:27:15,230
sets data that is produced on a certain

00:27:13,070 --> 00:27:19,090
day is probably hard for that week at

00:27:15,230 --> 00:27:21,590
best and and then it just tapers off

00:27:19,090 --> 00:27:23,810
after three months you probably don't

00:27:21,590 --> 00:27:26,090
even need that data no one's accessing

00:27:23,810 --> 00:27:28,880
it so we reduce retention times from a

00:27:26,090 --> 00:27:34,940
year to three months and we have saved

00:27:28,880 --> 00:27:37,220
millions plural in HDFS storage now you

00:27:34,940 --> 00:27:38,930
can choose to make up you can choose to

00:27:37,220 --> 00:27:42,040
make a policy decision here you could

00:27:38,930 --> 00:27:45,710
choose to drop replication factor for

00:27:42,040 --> 00:27:48,350
data that is called you can ship it to

00:27:45,710 --> 00:27:51,080
higher latency storage or probably a

00:27:48,350 --> 00:27:53,270
faraway cluster that no one's using you

00:27:51,080 --> 00:27:56,660
can delete the data entirely but all

00:27:53,270 --> 00:27:58,850
that has to be based on on hard numbers

00:27:56,660 --> 00:28:03,820
and this is kind of how we get our

00:27:58,850 --> 00:28:05,840
numbers all right enough of the use case

00:28:03,820 --> 00:28:09,670
let's go into how you would build

00:28:05,840 --> 00:28:09,670
something like this what would you need

00:28:09,730 --> 00:28:15,500
the most boring part of this is probably

00:28:12,950 --> 00:28:18,890
the parser now the logs for the most

00:28:15,500 --> 00:28:23,350
part our text some of those adjacent I

00:28:18,890 --> 00:28:27,050
believe our Hadoop two uses a bro for

00:28:23,350 --> 00:28:28,970
job history logs i'm not sure but all of

00:28:27,050 --> 00:28:31,220
these data is on the HDFS you could

00:28:28,970 --> 00:28:34,550
write a passer yourself or you could use

00:28:31,220 --> 00:28:38,390
or adapt one of the passes that ships

00:28:34,550 --> 00:28:40,100
with Hadoop today that's probably the

00:28:38,390 --> 00:28:44,900
least interesting part of your solution

00:28:40,100 --> 00:28:48,470
what else do you need let's let's look

00:28:44,900 --> 00:28:51,800
at the end game you probably want to run

00:28:48,470 --> 00:28:54,830
your analytics in seeker you probably

00:28:51,800 --> 00:28:56,690
want the data and

00:28:54,830 --> 00:28:59,990
after you parse the data you probably

00:28:56,690 --> 00:29:03,350
want to store it in say my sequel you

00:28:59,990 --> 00:29:07,390
can then run sequel queries on it and

00:29:03,350 --> 00:29:10,340
you can visualize the data using tableau

00:29:07,390 --> 00:29:13,630
MicroStrategy or a business intelligence

00:29:10,340 --> 00:29:17,950
tool of your choice connecting to

00:29:13,630 --> 00:29:23,899
connecting to my sequel probably using a

00:29:17,950 --> 00:29:26,690
odbc so we went back and calculated how

00:29:23,899 --> 00:29:31,580
much are processing we would require for

00:29:26,690 --> 00:29:34,549
given the volumes and after some amount

00:29:31,580 --> 00:29:37,490
of mathematical dexterity we decided

00:29:34,549 --> 00:29:41,110
that the heavy lifting probably ought to

00:29:37,490 --> 00:29:44,240
be done using Hadoop and hive so

00:29:41,110 --> 00:29:46,820
specifically instead of creating tables

00:29:44,240 --> 00:29:50,919
in my sequel we would create those

00:29:46,820 --> 00:29:54,620
tables in each catalog using hive ddl

00:29:50,919 --> 00:29:56,029
you would use MapReduce the parser I was

00:29:54,620 --> 00:30:02,139
talking about you could convert that

00:29:56,029 --> 00:30:05,990
into a MapReduce job use Hadoop jobs to

00:30:02,139 --> 00:30:09,710
parse the data use the H get output

00:30:05,990 --> 00:30:12,409
format to store the data into hive into

00:30:09,710 --> 00:30:15,679
the H catalog tables thereafter you can

00:30:12,409 --> 00:30:20,450
query using hive ql in the same way that

00:30:15,679 --> 00:30:24,320
you would have used sequel now since the

00:30:20,450 --> 00:30:25,789
data happens to be in h catalog because

00:30:24,320 --> 00:30:27,769
you have the connectors that sushanth

00:30:25,789 --> 00:30:30,169
was talking about you can consume that

00:30:27,769 --> 00:30:35,539
using pig as well or MapReduce if you

00:30:30,169 --> 00:30:38,600
prefer and and finally where you would

00:30:35,539 --> 00:30:40,820
have used a tableau and MicroStrategy

00:30:38,600 --> 00:30:42,850
you continue to use tableau and

00:30:40,820 --> 00:30:50,480
MicroStrategy except it is connected

00:30:42,850 --> 00:30:53,179
over odbc to hive server to directly so

00:30:50,480 --> 00:30:54,440
so let's call that the end game so this

00:30:53,179 --> 00:30:57,830
is probably how you're going to run your

00:30:54,440 --> 00:31:03,549
analysis the solution is not yet

00:30:57,830 --> 00:31:03,549
complete you probably want a way to

00:31:03,760 --> 00:31:07,740
schedule this processing and probably do

00:31:07,100 --> 00:31:11,970
the processing

00:31:07,740 --> 00:31:14,730
one place now you probably don't want to

00:31:11,970 --> 00:31:16,800
run your parcels in your production

00:31:14,730 --> 00:31:19,260
clusters because that's taking away

00:31:16,800 --> 00:31:22,170
resources from I mean you're affecting

00:31:19,260 --> 00:31:26,450
sl8 so you probably want to pull the raw

00:31:22,170 --> 00:31:29,490
logs from the HDFS into a single

00:31:26,450 --> 00:31:34,230
Starling cluster that you can then

00:31:29,490 --> 00:31:37,940
process offline the way to copy data

00:31:34,230 --> 00:31:41,040
between grids between clusters up

00:31:37,940 --> 00:31:45,300
incidentally this is the sample code

00:31:41,040 --> 00:31:47,490
that you would use the HK ddl the the

00:31:45,300 --> 00:31:51,179
hive query to find the user that is

00:31:47,490 --> 00:31:54,360
consuming the most compute the same

00:31:51,179 --> 00:31:56,820
query transliterated into Pig and the

00:31:54,360 --> 00:32:00,870
JDBC URL you would use to connect your

00:31:56,820 --> 00:32:04,110
bi tool for hives over to so back to

00:32:00,870 --> 00:32:10,170
copying the data to copy the raw logs

00:32:04,110 --> 00:32:15,540
you're probably going to use TCP DCP

00:32:10,170 --> 00:32:18,390
ships with Hadoop in it's very simple to

00:32:15,540 --> 00:32:21,179
what it does is it uses MapReduce to

00:32:18,390 --> 00:32:25,650
copy files between clusters create a

00:32:21,179 --> 00:32:27,380
list break it up and pass that to

00:32:25,650 --> 00:32:31,290
different mappers and it does the copy

00:32:27,380 --> 00:32:33,179
that's the sample command line if you're

00:32:31,290 --> 00:32:38,880
familiar with Hadoop you likely already

00:32:33,179 --> 00:32:43,050
used DCP incidentally if you've had

00:32:38,880 --> 00:32:46,800
trouble with a DCP on dot twenty-three

00:32:43,050 --> 00:32:52,650
or two dot oh please ping me later that

00:32:46,800 --> 00:32:58,380
really is my code and finally you need a

00:32:52,650 --> 00:33:00,120
way to orchestrate all of this you need

00:32:58,380 --> 00:33:03,090
a way to make sure that this runs

00:33:00,120 --> 00:33:06,679
periodically perhaps every day you could

00:33:03,090 --> 00:33:09,270
have put this app a cron job the

00:33:06,679 --> 00:33:12,690
suggested method for doing this is to

00:33:09,270 --> 00:33:16,530
use boozie boozie is a workflow manager

00:33:12,690 --> 00:33:18,840
for Hadoop it allows you to specify a

00:33:16,530 --> 00:33:20,790
workflow which contains several actions

00:33:18,840 --> 00:33:25,250
in this case this is a

00:33:20,790 --> 00:33:29,280
sort of a pared-down workflow that we

00:33:25,250 --> 00:33:33,000
use in Starling the first action as you

00:33:29,280 --> 00:33:36,480
can see runs a DCP job to copy from

00:33:33,000 --> 00:33:40,410
source to the Starling cluster and when

00:33:36,480 --> 00:33:43,980
that succeeds the process logs action is

00:33:40,410 --> 00:33:47,250
called takes which calls your

00:33:43,980 --> 00:33:50,610
parser code so now you have everything

00:33:47,250 --> 00:33:54,570
in place you your Uzi workflow takes

00:33:50,610 --> 00:34:00,620
care of running the CP copying the logs

00:33:54,570 --> 00:34:04,850
over kicking off your your purse jobs

00:34:00,620 --> 00:34:08,400
the past job then registers records into

00:34:04,850 --> 00:34:14,220
hive tables and then you you query and

00:34:08,400 --> 00:34:18,840
you visualize and so on so that's how we

00:34:14,220 --> 00:34:22,650
put Starling together here's an example

00:34:18,840 --> 00:34:24,179
of the sort of analytics that we can do

00:34:22,650 --> 00:34:27,510
with something like this now now this

00:34:24,179 --> 00:34:32,940
data is fairly old and therefore safe to

00:34:27,510 --> 00:34:35,700
share with you this data shows a job

00:34:32,940 --> 00:34:40,620
type distributions between clusters you

00:34:35,700 --> 00:34:42,540
can see three clumps there you can see

00:34:40,620 --> 00:34:45,900
that the cluster in the middle seems to

00:34:42,540 --> 00:34:49,440
have more spikes and that's simply

00:34:45,900 --> 00:34:55,110
because it is a busier cluster you can

00:34:49,440 --> 00:34:59,960
see the green bits a pig and the orange

00:34:55,110 --> 00:34:59,960
our MapReduce and the blue bits are hive

00:35:00,500 --> 00:35:05,580
wasn't really used too much at the time

00:35:03,360 --> 00:35:11,150
i made this extract so this is about a

00:35:05,580 --> 00:35:14,040
fortnight worth of data split daily so

00:35:11,150 --> 00:35:17,120
we can look at the same data diced

00:35:14,040 --> 00:35:21,360
differently this time grouped by user

00:35:17,120 --> 00:35:25,770
what we see here is that so the top and

00:35:21,360 --> 00:35:28,710
the bottom are I think one is the number

00:35:25,770 --> 00:35:30,990
of jobs in the other is total run times

00:35:28,710 --> 00:35:32,630
as you can see there's two users who

00:35:30,990 --> 00:35:37,400
seem to be using

00:35:32,630 --> 00:35:44,450
were compute than the others and they

00:35:37,400 --> 00:35:47,420
both seem to prefer pink right so I'm

00:35:44,450 --> 00:35:50,390
going to hand this back to search now

00:35:47,420 --> 00:35:56,360
here and he'll take you back to the

00:35:50,390 --> 00:35:59,000
future so one of the things that I also

00:35:56,360 --> 00:36:00,770
wanted to talk to talk about in this top

00:35:59,000 --> 00:36:03,080
if you had enough time and looks like we

00:36:00,770 --> 00:36:05,420
do thankfully is what's happening with

00:36:03,080 --> 00:36:07,190
hive itself and some of the changes that

00:36:05,420 --> 00:36:09,470
I've been happening in the Hadoop

00:36:07,190 --> 00:36:12,860
ecosystem that inspired changes in the

00:36:09,470 --> 00:36:15,560
hive ecosystem and one of which is

00:36:12,860 --> 00:36:18,830
called you know this called Project

00:36:15,560 --> 00:36:22,850
stinger and its goal essentially is to

00:36:18,830 --> 00:36:25,100
try to bring hive to a point where it

00:36:22,850 --> 00:36:27,710
can take you make use of the recent

00:36:25,100 --> 00:36:32,390
changes in done with yarn and if you've

00:36:27,710 --> 00:36:33,950
heard of Apache tase so let's go back to

00:36:32,390 --> 00:36:36,080
this picture where I talked about how

00:36:33,950 --> 00:36:40,100
you know a typical MapReduce job kind of

00:36:36,080 --> 00:36:43,160
spills out I mentioned about how we wind

00:36:40,100 --> 00:36:46,130
up preparing for a job and then we wind

00:36:43,160 --> 00:36:48,290
up doing in an input phase where we do

00:36:46,130 --> 00:36:49,820
various mapping and then I said that

00:36:48,290 --> 00:36:51,910
there was the elephant in the room which

00:36:49,820 --> 00:36:56,090
basically is a short shuffle shuffle

00:36:51,910 --> 00:36:58,310
stage which basically is you know mostly

00:36:56,090 --> 00:36:59,720
hidden there's a reduced stage where

00:36:58,310 --> 00:37:02,330
you're doing some sort of aggregation

00:36:59,720 --> 00:37:05,470
and you're finally out putting you know

00:37:02,330 --> 00:37:07,910
whatever you have go here and

00:37:05,470 --> 00:37:09,650
realistically this part is actually

00:37:07,910 --> 00:37:13,070
significant in terms of higher costs if

00:37:09,650 --> 00:37:14,660
you consider if you consider one of the

00:37:13,070 --> 00:37:17,150
things that you hear off in Big Data

00:37:14,660 --> 00:37:19,700
stories most of the time it is that I

00:37:17,150 --> 00:37:22,220
owed dominates things it is not about

00:37:19,700 --> 00:37:24,920
cpu utilization most of the time it is

00:37:22,220 --> 00:37:26,690
about you being io block whether it's

00:37:24,920 --> 00:37:28,880
local hardness or network read rights

00:37:26,690 --> 00:37:31,220
you should never be network reread

00:37:28,880 --> 00:37:32,750
blocked as far as possible and you

00:37:31,220 --> 00:37:36,080
should always try to keep things local

00:37:32,750 --> 00:37:40,730
but even local hardest are only that

00:37:36,080 --> 00:37:43,880
fast so what you have is in many cases

00:37:40,730 --> 00:37:45,410
when you run a hive query what winds up

00:37:43,880 --> 00:37:46,190
happening is it winds up getting

00:37:45,410 --> 00:37:50,210
represent

00:37:46,190 --> 00:37:51,740
as a sequence of MapReduce jobs going

00:37:50,210 --> 00:37:54,200
back to when I said that high was a

00:37:51,740 --> 00:37:56,060
database engine well it has this notion

00:37:54,200 --> 00:37:58,370
of storage abstractions it has this

00:37:56,060 --> 00:38:01,130
notion of what tables are logically and

00:37:58,370 --> 00:38:04,100
then it's able to try and say okay if

00:38:01,130 --> 00:38:07,100
this person wanted me to select this

00:38:04,100 --> 00:38:09,620
filter by that group this this

00:38:07,100 --> 00:38:11,120
translates to me to a MapReduce job

00:38:09,620 --> 00:38:13,700
followed by another MapReduce job

00:38:11,120 --> 00:38:15,800
followed by another MapReduce job I mean

00:38:13,700 --> 00:38:18,050
for example it can be much more complex

00:38:15,800 --> 00:38:22,400
than this but let's look at this picture

00:38:18,050 --> 00:38:25,240
again here as for the previous slide you

00:38:22,400 --> 00:38:27,320
remember that after a mapper you have to

00:38:25,240 --> 00:38:30,080
dump two discs and you wind up doing

00:38:27,320 --> 00:38:33,020
this whole sort shuffle thing before it

00:38:30,080 --> 00:38:34,940
goes to the reducer right now if this is

00:38:33,020 --> 00:38:37,460
temporary data that we are concerned

00:38:34,940 --> 00:38:39,980
about then the reducer does need not

00:38:37,460 --> 00:38:42,680
necessarily strictly need to write out

00:38:39,980 --> 00:38:47,360
to disk but you will have a scenario

00:38:42,680 --> 00:38:49,670
where this is necessary so in something

00:38:47,360 --> 00:38:51,920
like a MapReduce MapReduce MapReduce

00:38:49,670 --> 00:38:54,650
chain that rides up Appling like this if

00:38:51,920 --> 00:38:57,590
you could just rearrange your blocks so

00:38:54,650 --> 00:39:01,520
that you had a mapper followed by this

00:38:57,590 --> 00:39:03,710
reducer and fed that data in memory to

00:39:01,520 --> 00:39:06,020
the next mapper and so on so forth and

00:39:03,710 --> 00:39:08,840
made it more like a chain like an m RM

00:39:06,020 --> 00:39:11,420
RM are kind of a chain then what you

00:39:08,840 --> 00:39:13,580
wind up with is you don't need to spill

00:39:11,420 --> 00:39:15,260
to disk at this in between this reducer

00:39:13,580 --> 00:39:17,750
the mapper you don't need to spell two

00:39:15,260 --> 00:39:20,120
discs in between the seducer an apple so

00:39:17,750 --> 00:39:21,920
you save on a bunch of i/o costs

00:39:20,120 --> 00:39:25,730
associated with this the first picture

00:39:21,920 --> 00:39:29,770
here actually has something like 1 2 3 4

00:39:25,730 --> 00:39:32,570
5 6 essentially after every block

00:39:29,770 --> 00:39:35,300
hardest right in the second block you

00:39:32,570 --> 00:39:36,830
only need to write four times and that

00:39:35,300 --> 00:39:39,740
is actually the savings that you get on

00:39:36,830 --> 00:39:41,570
that actually huge but when you try to

00:39:39,740 --> 00:39:44,030
shoehorn everything into a MapReduce

00:39:41,570 --> 00:39:46,040
framework classically you wind up having

00:39:44,030 --> 00:39:47,900
to say oh I launched a MapReduce job I

00:39:46,040 --> 00:39:50,900
have to wait for that to finish before I

00:39:47,900 --> 00:39:52,490
can you know shoot off another but you

00:39:50,900 --> 00:39:55,400
realize that these kind of chains are

00:39:52,490 --> 00:39:57,560
much more useful so with yarn that is a

00:39:55,400 --> 00:40:00,059
new project called Apache tastes which

00:39:57,560 --> 00:40:03,809
basically tries to be a yarn content

00:40:00,059 --> 00:40:05,430
that deals with input processing output

00:40:03,809 --> 00:40:08,849
it deals more with the notion of

00:40:05,430 --> 00:40:12,499
vertices and edges of a dag of a

00:40:08,849 --> 00:40:15,209
workflow dag then with what he say

00:40:12,499 --> 00:40:19,109
strictly MapReduce kind of paradigm so

00:40:15,209 --> 00:40:21,180
now we start looking at maps and reduce

00:40:19,109 --> 00:40:22,619
tasks as simply some sort of processing

00:40:21,180 --> 00:40:23,939
and the question of what kind of

00:40:22,619 --> 00:40:26,880
processing do you need to take before

00:40:23,939 --> 00:40:29,549
you do something else and in this

00:40:26,880 --> 00:40:32,249
scenario for example I have I listed out

00:40:29,549 --> 00:40:35,219
a slightly more complex tag for some

00:40:32,249 --> 00:40:36,719
sort of hive query and you know we wind

00:40:35,219 --> 00:40:40,079
up saying that we wanted to have

00:40:36,719 --> 00:40:41,969
MapReduce job followed by two other

00:40:40,079 --> 00:40:44,219
MapReduce jobs which I have similar

00:40:41,969 --> 00:40:46,309
mapping so if you had for example a

00:40:44,219 --> 00:40:49,559
group by or you had a filter that

00:40:46,309 --> 00:40:51,989
resulted in creating a map task that did

00:40:49,559 --> 00:40:53,819
the same operation but the reduce tasks

00:40:51,989 --> 00:40:56,910
did something completely different and

00:40:53,819 --> 00:40:58,709
then you use these to the resultant data

00:40:56,910 --> 00:41:00,869
of both of those together in a joint

00:40:58,709 --> 00:41:03,569
that eventually fed to another MapReduce

00:41:00,869 --> 00:41:06,900
program then you might get some dag that

00:41:03,569 --> 00:41:08,789
looked like this but again we have one

00:41:06,900 --> 00:41:11,999
right here one of those right here

00:41:08,789 --> 00:41:14,099
another another another another and one

00:41:11,999 --> 00:41:15,779
more and one more so every block here is

00:41:14,099 --> 00:41:17,729
writing out and so that's a total of

00:41:15,779 --> 00:41:20,729
eight disk writes that we're looking at

00:41:17,729 --> 00:41:22,859
if you rearrange that picture again to

00:41:20,729 --> 00:41:25,170
kind of an m RM cycle kind of a thing

00:41:22,859 --> 00:41:27,630
then we can actually arrange it in this

00:41:25,170 --> 00:41:29,789
way where the m2 which is common between

00:41:27,630 --> 00:41:31,880
both of these can actually be attached

00:41:29,789 --> 00:41:34,739
to the r1 and this becomes an in-memory

00:41:31,880 --> 00:41:36,749
data transfer and same with this and

00:41:34,739 --> 00:41:40,859
same with this so now we are actually

00:41:36,749 --> 00:41:42,749
down to one right to rights three four

00:41:40,859 --> 00:41:45,719
and five rights so from eight rights we

00:41:42,749 --> 00:41:48,150
come down to Phi rights and essentially

00:41:45,719 --> 00:41:51,630
these kinds of optimizations are huge

00:41:48,150 --> 00:41:55,170
when in terms of the kind of savings

00:41:51,630 --> 00:41:57,719
that hive actually has and in a sense we

00:41:55,170 --> 00:42:00,239
are moving with Apache thays to a more

00:41:57,719 --> 00:42:03,689
conventional when he say input

00:42:00,239 --> 00:42:05,880
processing output kennefa idea and what

00:42:03,689 --> 00:42:07,920
you can do for something like a hive

00:42:05,880 --> 00:42:10,589
database engine is it is strictly not a

00:42:07,920 --> 00:42:12,150
MapReduce engine anymore and you know

00:42:10,589 --> 00:42:13,780
the future will tell what kind of

00:42:12,150 --> 00:42:16,810
additional things we do for this

00:42:13,780 --> 00:42:18,910
but this already is what he say making a

00:42:16,810 --> 00:42:21,670
huge amount of performance gains with

00:42:18,910 --> 00:42:23,650
hive 02 13 that's just coming out you

00:42:21,670 --> 00:42:26,620
should be able to set execution engine

00:42:23,650 --> 00:42:28,060
equal to tease in days and you should

00:42:26,620 --> 00:42:30,570
actually see quite a bit of faster

00:42:28,060 --> 00:42:33,850
performance with a bunch of things and

00:42:30,570 --> 00:42:37,810
associated with this coming back to the

00:42:33,850 --> 00:42:41,650
theme of interoperability potentially

00:42:37,810 --> 00:42:45,280
hive is now just producing phase blocks

00:42:41,650 --> 00:42:47,680
and there is no reason why we cannot try

00:42:45,280 --> 00:42:50,620
to adapt this to additional kinds of

00:42:47,680 --> 00:42:53,650
processing that what do you you know

00:42:50,620 --> 00:42:56,350
other other projects are able to also

00:42:53,650 --> 00:42:58,480
provide for these blocks and so

00:42:56,350 --> 00:43:03,100
essentially we will become much more

00:42:58,480 --> 00:43:05,470
expressive as things go on that

00:43:03,100 --> 00:43:13,090
concludes our talk and I open the floor

00:43:05,470 --> 00:43:16,290
to any questions that you might have all

00:43:13,090 --> 00:43:16,290

YouTube URL: https://www.youtube.com/watch?v=SA9no60BZEI


