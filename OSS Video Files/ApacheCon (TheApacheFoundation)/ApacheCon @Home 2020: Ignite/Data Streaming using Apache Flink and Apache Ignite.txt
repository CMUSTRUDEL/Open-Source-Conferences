Title: Data Streaming using Apache Flink and Apache Ignite
Publication date: 2020-10-14
Playlist: ApacheCon @Home 2020: Ignite
Description: 
	Data Streaming using Apache Flink and Apache Ignite
Saikat Maitra

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Apache Ignite is a powerful in-memory computing platform. The Apache IgniteSink streaming connector enables users to inject Flink data into the Ignite cache. Join Saikat Maitra to learn how to build a simple data streaming application using Apache Flink and Apache Ignite. This stream processing topology will allow data streaming in a distributed, scalable, and fault-tolerant manner, which can process data sets consisting of virtually unlimited streams of events. Apache IgniteSink offers a streaming connector to inject Flink data into the Ignite cache. The sink emits its input data to the Ignite cache. The key feature to note is the performance and scale both Apache Flink and Apache Ignite offer. Apache Flink can process unbounded and bounded data sets and has been designed to run stateful streaming applications at scale. Application computation is distributed and concurrently executed in clusters. Apache Flink is also optimized for local state access for tasks and does checkpointing of local state for durability. Apache Ignite provides streaming capabilities that allow data ingestion at a high scale in its in-memory data grid.

Saikat Maitra is Lead Engineer at Target and Apache Ignite Committer and PMC Member. Prior to Target, he worked for Flipkart and AOL (America Online) to build retail and e-commerce systems. Saikat received his Master of Technology in Software Systems from BITS, Pilani.
Captions: 
	00:00:03,370 --> 00:00:08,089
[Music]

00:00:11,120 --> 00:00:14,190
[Music]

00:00:22,320 --> 00:00:26,720
hi

00:00:23,279 --> 00:00:27,439
i'm shakat i'm part of apache ignite

00:00:26,720 --> 00:00:30,480
committer

00:00:27,439 --> 00:00:33,840
and pmc member

00:00:30,480 --> 00:00:35,280
and i have contributed to multiple

00:00:33,840 --> 00:00:38,320
modules in

00:00:35,280 --> 00:00:40,079
apache ignite some of them are

00:00:38,320 --> 00:00:42,800
ignite sync that i'll be talking about

00:00:40,079 --> 00:00:45,039
today and also

00:00:42,800 --> 00:00:47,280
about the rest apis uh some of the rest

00:00:45,039 --> 00:00:49,120
api that apache ignite support

00:00:47,280 --> 00:00:51,440
so today i'll be talking about data

00:00:49,120 --> 00:00:52,079
streaming using apache flink and apache

00:00:51,440 --> 00:00:55,600
ignite

00:00:52,079 --> 00:00:57,520
and how do we build a real-time

00:00:55,600 --> 00:00:59,440
data streaming pipeline application

00:00:57,520 --> 00:01:02,879
using two of these

00:00:59,440 --> 00:01:02,879
apache projects

00:01:05,119 --> 00:01:12,479
so before we begin i just wanted to also

00:01:09,360 --> 00:01:15,600
touch about what is strata streaming

00:01:12,479 --> 00:01:18,320
and what is stream processing in itself

00:01:15,600 --> 00:01:20,000
so stream processing is about processing

00:01:18,320 --> 00:01:22,720
unbounded stream of data

00:01:20,000 --> 00:01:23,920
contrast to bounded data set process by

00:01:22,720 --> 00:01:26,880
batch processor

00:01:23,920 --> 00:01:28,479
so what it means is that a continuous

00:01:26,880 --> 00:01:31,600
set of events are

00:01:28,479 --> 00:01:33,920
being published to your system and

00:01:31,600 --> 00:01:35,040
you have clusters of nodes which is

00:01:33,920 --> 00:01:37,280
processing

00:01:35,040 --> 00:01:39,040
and aggregating applying its functions

00:01:37,280 --> 00:01:41,360
and analyzing

00:01:39,040 --> 00:01:42,960
and storing the result sets into a data

00:01:41,360 --> 00:01:44,720
storage system

00:01:42,960 --> 00:01:46,880
so as you could see in the image that

00:01:44,720 --> 00:01:48,560
there is like a continuous stream of

00:01:46,880 --> 00:01:50,399
events which is coming in and there is

00:01:48,560 --> 00:01:53,040
no bound to it in a sense that

00:01:50,399 --> 00:01:54,799
there is no start and end point for this

00:01:53,040 --> 00:01:57,040
data set

00:01:54,799 --> 00:01:58,960
application events for example like real

00:01:57,040 --> 00:02:01,360
time scores for multiplayer game

00:01:58,960 --> 00:02:03,360
or it could be also like order volume

00:02:01,360 --> 00:02:06,399
information for a retail

00:02:03,360 --> 00:02:09,440
web application or it could be like

00:02:06,399 --> 00:02:12,000
what are the best selling items in a

00:02:09,440 --> 00:02:14,640
in a web application uh what are the

00:02:12,000 --> 00:02:17,120
best selling items for a retail web

00:02:14,640 --> 00:02:17,760
site or uh what are the top ten products

00:02:17,120 --> 00:02:20,720
which are

00:02:17,760 --> 00:02:22,080
uh highest preferred items uh which are

00:02:20,720 --> 00:02:24,000
getting sold

00:02:22,080 --> 00:02:25,760
so stream processor process events as

00:02:24,000 --> 00:02:28,160
soon as they arrive there is no lag

00:02:25,760 --> 00:02:29,920
there is no wait period for that events

00:02:28,160 --> 00:02:32,959
to get process and

00:02:29,920 --> 00:02:34,959
depending on your cluster size you can

00:02:32,959 --> 00:02:36,640
also you know add more nodes to increase

00:02:34,959 --> 00:02:40,000
its capacity and to

00:02:36,640 --> 00:02:43,280
improve its processing volume

00:02:40,000 --> 00:02:45,200
or you could reduce so and depending on

00:02:43,280 --> 00:02:46,959
your needs if your resource utilization

00:02:45,200 --> 00:02:47,599
is very low you can also reduce nodes

00:02:46,959 --> 00:02:49,840
and

00:02:47,599 --> 00:02:51,280
it's very elastic in nature that your

00:02:49,840 --> 00:02:53,680
cluster is

00:02:51,280 --> 00:02:54,640
can be scaled up or scaled down as and

00:02:53,680 --> 00:02:58,000
when

00:02:54,640 --> 00:03:00,239
your need arises

00:02:58,000 --> 00:03:01,360
so some of the other example would be

00:03:00,239 --> 00:03:04,080
for

00:03:01,360 --> 00:03:05,519
stream processing is considered like

00:03:04,080 --> 00:03:08,239
ride sharing app where

00:03:05,519 --> 00:03:10,080
the ride sharing apps are sending events

00:03:08,239 --> 00:03:13,360
about a specific

00:03:10,080 --> 00:03:16,080
location demand and based on that the

00:03:13,360 --> 00:03:16,879
price of the right fare can go up or

00:03:16,080 --> 00:03:20,319
down

00:03:16,879 --> 00:03:20,800
and that that get used as an that can be

00:03:20,319 --> 00:03:22,720
used as

00:03:20,800 --> 00:03:24,400
another example where uh stream

00:03:22,720 --> 00:03:26,799
processing can be used where

00:03:24,400 --> 00:03:28,879
each of the mobile apps are sending a

00:03:26,799 --> 00:03:29,680
demand information and depending on that

00:03:28,879 --> 00:03:31,280
the

00:03:29,680 --> 00:03:32,959
maybe the price of that specific

00:03:31,280 --> 00:03:36,560
location or the fare for that specific

00:03:32,959 --> 00:03:38,720
location can go up or down

00:03:36,560 --> 00:03:39,920
so why do we want to use stream

00:03:38,720 --> 00:03:42,799
processor

00:03:39,920 --> 00:03:44,239
so it is it provides a lot less latency

00:03:42,799 --> 00:03:48,239
compared to batch processor

00:03:44,239 --> 00:03:49,920
there is no periodic scheduler jobs your

00:03:48,239 --> 00:03:52,400
and you don't have a bounded data set

00:03:49,920 --> 00:03:53,439
where in case of a batch processor which

00:03:52,400 --> 00:03:56,159
always processes

00:03:53,439 --> 00:03:58,080
events in such a way that you know it

00:03:56,159 --> 00:04:01,120
starts at a specific

00:03:58,080 --> 00:04:03,200
time in period and then it takes up

00:04:01,120 --> 00:04:04,799
whatever data has it has received by

00:04:03,200 --> 00:04:07,200
that time

00:04:04,799 --> 00:04:08,159
window and then it start processing

00:04:07,200 --> 00:04:09,760
those events

00:04:08,159 --> 00:04:11,680
but in case of real time streaming

00:04:09,760 --> 00:04:12,000
application you do not have such wait

00:04:11,680 --> 00:04:14,159
time

00:04:12,000 --> 00:04:16,160
there is no schedule job which is

00:04:14,159 --> 00:04:17,919
waiting for events to arrive and get

00:04:16,160 --> 00:04:19,919
accumulated and then get process it's

00:04:17,919 --> 00:04:22,320
continuously running system

00:04:19,919 --> 00:04:23,520
and data can be processed partitioned in

00:04:22,320 --> 00:04:26,080
event windows

00:04:23,520 --> 00:04:27,280
so what event window provide us a

00:04:26,080 --> 00:04:29,919
context in time

00:04:27,280 --> 00:04:31,360
which can be used to process those event

00:04:29,919 --> 00:04:33,440
an example

00:04:31,360 --> 00:04:34,560
for that would be what is the order

00:04:33,440 --> 00:04:37,759
volume for

00:04:34,560 --> 00:04:38,320
a application retail application per

00:04:37,759 --> 00:04:40,080
hour

00:04:38,320 --> 00:04:41,919
what what's the order volume for last

00:04:40,080 --> 00:04:42,560
one hour or what's the order volume for

00:04:41,919 --> 00:04:45,440
last

00:04:42,560 --> 00:04:46,080
seven days this gives us a context of

00:04:45,440 --> 00:04:48,800
time

00:04:46,080 --> 00:04:49,520
in which we the event time are being

00:04:48,800 --> 00:04:52,720
used

00:04:49,520 --> 00:04:55,520
to you know distribute and

00:04:52,720 --> 00:04:56,000
segregate those events so i i'm only

00:04:55,520 --> 00:04:58,400
interested

00:04:56,000 --> 00:05:00,639
in last 10 minutes of data or i am

00:04:58,400 --> 00:05:02,720
interested in last one hour of data

00:05:00,639 --> 00:05:03,919
and that is when we will start

00:05:02,720 --> 00:05:05,680
processing those event

00:05:03,919 --> 00:05:07,039
and as then when we see the demo

00:05:05,680 --> 00:05:09,759
application it will more

00:05:07,039 --> 00:05:11,600
get a clearer picture like how event

00:05:09,759 --> 00:05:15,199
window can be used

00:05:11,600 --> 00:05:17,360
so the another advantage is like

00:05:15,199 --> 00:05:18,720
it can as i said before that it can be

00:05:17,360 --> 00:05:21,039
easily scalable

00:05:18,720 --> 00:05:24,320
uh system where you can add more and

00:05:21,039 --> 00:05:26,000
more nodes to the job cluster if your

00:05:24,320 --> 00:05:28,639
data volume is very high and

00:05:26,000 --> 00:05:29,199
you need faster processing or if your

00:05:28,639 --> 00:05:31,440
data

00:05:29,199 --> 00:05:32,479
volume is very large and you have a

00:05:31,440 --> 00:05:34,880
large data set

00:05:32,479 --> 00:05:39,840
then you can also add nodes to your data

00:05:34,880 --> 00:05:39,840
store to increase its capacity

00:05:40,560 --> 00:05:48,320
so apache ignite data streamer is

00:05:44,880 --> 00:05:50,800
the system looks something similar where

00:05:48,320 --> 00:05:53,120
ignite streamers are continuously

00:05:50,800 --> 00:05:55,039
sending events to ignite nodes

00:05:53,120 --> 00:05:56,639
and ignite nodes is considered to be a

00:05:55,039 --> 00:05:58,880
data storage or database

00:05:56,639 --> 00:05:59,840
where it is accepting those events and

00:05:58,880 --> 00:06:03,360
continuously

00:05:59,840 --> 00:06:04,800
storing those result sets and then we

00:06:03,360 --> 00:06:08,000
can also use ignite

00:06:04,800 --> 00:06:09,039
thick or thin clients to attach or you

00:06:08,000 --> 00:06:11,120
know connect to those

00:06:09,039 --> 00:06:13,600
ignite nodes and then do further

00:06:11,120 --> 00:06:16,160
analytics and process those events or

00:06:13,600 --> 00:06:21,840
run some kind of a compute function in

00:06:16,160 --> 00:06:21,840
the distributed ignite cluster

00:06:23,120 --> 00:06:27,039
so this is an example of how data

00:06:25,840 --> 00:06:30,080
streaming pipeline

00:06:27,039 --> 00:06:31,199
can be built and this resembles very

00:06:30,080 --> 00:06:34,160
similar to

00:06:31,199 --> 00:06:35,039
pipeline architecture in systems where

00:06:34,160 --> 00:06:37,039
you could see like

00:06:35,039 --> 00:06:38,880
as i said before like the apps are

00:06:37,039 --> 00:06:39,680
publishing events it could be a mobile

00:06:38,880 --> 00:06:41,199
app which is

00:06:39,680 --> 00:06:43,680
publishing demand for a specific

00:06:41,199 --> 00:06:45,680
location it could be a mobile game which

00:06:43,680 --> 00:06:49,280
is publishing the real time

00:06:45,680 --> 00:06:51,759
scores for players or it could also be

00:06:49,280 --> 00:06:52,400
a retail web where the order volume or

00:06:51,759 --> 00:06:55,520
the

00:06:52,400 --> 00:06:57,360
uh top selling these top selling skus

00:06:55,520 --> 00:06:59,919
information are being published

00:06:57,360 --> 00:07:01,599
and it all get published to a message

00:06:59,919 --> 00:07:03,120
queue or a message broker system it

00:07:01,599 --> 00:07:06,720
could be apache kafka or

00:07:03,120 --> 00:07:07,599
it could be any other messaging queue

00:07:06,720 --> 00:07:10,880
system

00:07:07,599 --> 00:07:14,160
so apache link provides connector to

00:07:10,880 --> 00:07:16,479
apache kafka rabbit mq redace

00:07:14,160 --> 00:07:17,599
you can use any kind of a data storage

00:07:16,479 --> 00:07:20,240
uh system

00:07:17,599 --> 00:07:22,479
to which the events can be published

00:07:20,240 --> 00:07:24,160
apache link also support file based uh

00:07:22,479 --> 00:07:26,720
connectors so you can also use those

00:07:24,160 --> 00:07:30,000
also if depending on your system's need

00:07:26,720 --> 00:07:30,800
so and then as and when the events are

00:07:30,000 --> 00:07:33,039
being published

00:07:30,800 --> 00:07:33,919
so in this example it is getting

00:07:33,039 --> 00:07:36,720
published to a

00:07:33,919 --> 00:07:38,560
apache kafka message broker and then

00:07:36,720 --> 00:07:41,680
apache flink

00:07:38,560 --> 00:07:43,039
is built as a consumer which start

00:07:41,680 --> 00:07:46,400
collecting those events

00:07:43,039 --> 00:07:48,479
and start processing them and here in

00:07:46,400 --> 00:07:50,639
apache link can be used to you know

00:07:48,479 --> 00:07:54,000
aggregate apply functions like

00:07:50,639 --> 00:07:56,400
flat map transform partition the data

00:07:54,000 --> 00:07:58,240
or distribute them into a specific time

00:07:56,400 --> 00:08:01,280
window to process them

00:07:58,240 --> 00:08:03,039
and or map those results

00:08:01,280 --> 00:08:06,160
into another data structure that your

00:08:03,039 --> 00:08:10,400
application needs before its storage

00:08:06,160 --> 00:08:12,560
so apache flink get used as a

00:08:10,400 --> 00:08:14,160
processing engine i would say in this

00:08:12,560 --> 00:08:17,440
context and then

00:08:14,160 --> 00:08:20,160
the result sets that are getting built

00:08:17,440 --> 00:08:21,039
that are being stored into the apache

00:08:20,160 --> 00:08:23,440
ignite nodes

00:08:21,039 --> 00:08:24,479
and in apache ignite nodes here it is

00:08:23,440 --> 00:08:27,199
acting as a

00:08:24,479 --> 00:08:27,840
data storage system where the ignite

00:08:27,199 --> 00:08:30,000
sync

00:08:27,840 --> 00:08:33,200
comes in play and it accepts all of

00:08:30,000 --> 00:08:40,159
those results and get published into the

00:08:33,200 --> 00:08:43,279
ignites data store

00:08:40,159 --> 00:08:47,600
so this is another example where

00:08:43,279 --> 00:08:48,959
apache link is used to consume data from

00:08:47,600 --> 00:08:52,160
ignite data sources

00:08:48,959 --> 00:08:56,320
so as i said before in the

00:08:52,160 --> 00:08:59,360
data streaming application there are um

00:08:56,320 --> 00:09:01,360
you can use apache ignite as a data sync

00:08:59,360 --> 00:09:02,399
or you could also use apache ignite as a

00:09:01,360 --> 00:09:05,519
data source

00:09:02,399 --> 00:09:08,080
what it means is that in this

00:09:05,519 --> 00:09:09,600
pipeline application which you you could

00:09:08,080 --> 00:09:12,640
build using this

00:09:09,600 --> 00:09:15,680
uh projects you can use apache ignite

00:09:12,640 --> 00:09:16,480
as either of that like you can be at at

00:09:15,680 --> 00:09:18,560
the

00:09:16,480 --> 00:09:21,200
end where it is storing those result

00:09:18,560 --> 00:09:24,160
sets or it could also be a system where

00:09:21,200 --> 00:09:25,760
it is it already has those information

00:09:24,160 --> 00:09:27,680
and then it is

00:09:25,760 --> 00:09:28,800
act as a data source which flink can

00:09:27,680 --> 00:09:30,720
consume it from

00:09:28,800 --> 00:09:32,480
and then finally it can publish those

00:09:30,720 --> 00:09:34,640
result sets into

00:09:32,480 --> 00:09:36,560
a visualization app something like

00:09:34,640 --> 00:09:38,320
grafana where we can build dashboard out

00:09:36,560 --> 00:09:40,800
of the result set for visualization

00:09:38,320 --> 00:09:40,800
purposes

00:09:43,680 --> 00:09:46,720
another uh area that i wanted to cover

00:09:46,240 --> 00:09:50,320
is

00:09:46,720 --> 00:09:54,560
stream safe point so stream safe point

00:09:50,320 --> 00:09:55,760
is about storing snapshot of a job

00:09:54,560 --> 00:09:58,320
including its

00:09:55,760 --> 00:09:59,920
source offset and job state what it

00:09:58,320 --> 00:10:02,959
means is that

00:09:59,920 --> 00:10:05,920
let us say your job has ran and process

00:10:02,959 --> 00:10:07,680
certain set of events and now um we

00:10:05,920 --> 00:10:11,040
would like to migrate that

00:10:07,680 --> 00:10:14,079
some of the nodes to another cluster

00:10:11,040 --> 00:10:15,519
or we would like to migrate the job into

00:10:14,079 --> 00:10:17,839
another system

00:10:15,519 --> 00:10:19,040
with a higher capacity so what could

00:10:17,839 --> 00:10:22,320
happen is that

00:10:19,040 --> 00:10:24,160
in point in time at p1

00:10:22,320 --> 00:10:26,560
we can say like okay i would like to

00:10:24,160 --> 00:10:29,839
take a snapshot of this job

00:10:26,560 --> 00:10:32,160
and i would like to take the complete

00:10:29,839 --> 00:10:35,120
data set that has already been processed

00:10:32,160 --> 00:10:37,120
and migrate them to another system or i

00:10:35,120 --> 00:10:37,600
can i can increase my cluster size and

00:10:37,120 --> 00:10:40,560
we can

00:10:37,600 --> 00:10:42,399
we can use the stream save point as a

00:10:40,560 --> 00:10:44,399
starting point what if

00:10:42,399 --> 00:10:45,519
what it will do is that instead of

00:10:44,399 --> 00:10:47,760
processing events

00:10:45,519 --> 00:10:48,880
again from the beginning of from the

00:10:47,760 --> 00:10:51,600
message

00:10:48,880 --> 00:10:53,760
queue system it will start the

00:10:51,600 --> 00:10:55,920
processing from the last save point

00:10:53,760 --> 00:10:57,440
so that that advantage it provides like

00:10:55,920 --> 00:11:00,000
when when you're migrating

00:10:57,440 --> 00:11:01,040
jobs to a different cluster or it acts

00:11:00,000 --> 00:11:03,839
as like

00:11:01,040 --> 00:11:05,279
a save point which is created by user

00:11:03,839 --> 00:11:08,399
which is user defined

00:11:05,279 --> 00:11:10,800
and you can save a state of your job

00:11:08,399 --> 00:11:14,320
including its offsets so that reduces

00:11:10,800 --> 00:11:16,720
the processing time when there are

00:11:14,320 --> 00:11:18,480
systems which need to be migrated so

00:11:16,720 --> 00:11:19,680
stream set point is very similar to

00:11:18,480 --> 00:11:22,480
checkpointing

00:11:19,680 --> 00:11:24,079
but it is more controlled by user

00:11:22,480 --> 00:11:26,320
whereas checkpointing

00:11:24,079 --> 00:11:27,200
is another way in which apache link

00:11:26,320 --> 00:11:30,079
takes a

00:11:27,200 --> 00:11:31,440
periodic snapshot of the job and if any

00:11:30,079 --> 00:11:34,240
node goes down

00:11:31,440 --> 00:11:36,240
in that case apache link and you add

00:11:34,240 --> 00:11:39,760
another node to the cluster

00:11:36,240 --> 00:11:41,360
then the next job will start from the

00:11:39,760 --> 00:11:43,440
last checkpoint it say

00:11:41,360 --> 00:11:45,519
so that's typically the difference where

00:11:43,440 --> 00:11:48,640
checkpointing is system control

00:11:45,519 --> 00:11:50,320
whereas the safe points are very much

00:11:48,640 --> 00:11:52,720
user defined for the

00:11:50,320 --> 00:11:52,720
project

00:11:53,360 --> 00:11:59,279
so in apache ignite we have a couple of

00:11:57,120 --> 00:12:01,440
already built connectors or

00:11:59,279 --> 00:12:03,040
extensions that are available for apache

00:12:01,440 --> 00:12:05,760
ignite data streamers

00:12:03,040 --> 00:12:06,880
uh flink is one of them apache camel

00:12:05,760 --> 00:12:10,160
floom

00:12:06,880 --> 00:12:12,160
kafka jms mqtt strom these are like

00:12:10,160 --> 00:12:14,160
some of the apache ignite data streamer

00:12:12,160 --> 00:12:17,120
which are already available into the

00:12:14,160 --> 00:12:19,279
code base and can be used as as a

00:12:17,120 --> 00:12:21,680
connector to build your project so

00:12:19,279 --> 00:12:24,079
you could choose to use apache ignite as

00:12:21,680 --> 00:12:25,839
a data sync or data sources which

00:12:24,079 --> 00:12:29,440
has connectivity to either of this

00:12:25,839 --> 00:12:32,480
system to build your data pipeline

00:12:29,440 --> 00:12:34,000
i'll next go and build a demo

00:12:32,480 --> 00:12:35,519
application and then we can

00:12:34,000 --> 00:12:39,839
walk through there to see like how we

00:12:35,519 --> 00:12:39,839
can build a data pipeline application

00:12:48,320 --> 00:12:54,399
we'll start with a

00:12:52,000 --> 00:12:54,399
function

00:13:00,959 --> 00:13:07,839
and we can say l

00:13:04,720 --> 00:13:07,839
environment equal to

00:13:52,720 --> 00:14:03,839
let's look

00:14:07,040 --> 00:14:10,560
so here we defined a environment stream

00:14:09,199 --> 00:14:13,760
execution environment

00:14:10,560 --> 00:14:15,279
which which provides a sandbox kind of

00:14:13,760 --> 00:14:17,360
an environment where

00:14:15,279 --> 00:14:19,360
you it abstract away from the developer

00:14:17,360 --> 00:14:21,120
like where the job is running

00:14:19,360 --> 00:14:23,120
so it could mean is that either you

00:14:21,120 --> 00:14:26,320
could be running your job in a local

00:14:23,120 --> 00:14:28,320
uh node or you could be running it

00:14:26,320 --> 00:14:30,560
in a cluster of system where multiple

00:14:28,320 --> 00:14:34,000
nodes are being used to

00:14:30,560 --> 00:14:35,760
process your job so that specifically

00:14:34,000 --> 00:14:37,680
the environment provides it it gives you

00:14:35,760 --> 00:14:40,800
an environment to run your job

00:14:37,680 --> 00:14:41,440
safely and and it abstract away the fact

00:14:40,800 --> 00:14:43,040
that okay

00:14:41,440 --> 00:14:45,279
some of the nodes may go down or it

00:14:43,040 --> 00:14:48,240
could have certain other issues so

00:14:45,279 --> 00:14:49,920
that kind of issues are being abstracted

00:14:48,240 --> 00:14:51,680
away from a developer where you

00:14:49,920 --> 00:14:53,519
your job get executed within that

00:14:51,680 --> 00:14:55,360
environment it could be a cluster of

00:14:53,519 --> 00:14:56,800
nodes or it could be a single node as

00:14:55,360 --> 00:15:00,240
well

00:14:56,800 --> 00:15:04,720
so out of this environment now

00:15:00,240 --> 00:15:04,720
we can define a stream of events

00:15:10,480 --> 00:15:13,839
and here i'm defining a statically set

00:15:12,720 --> 00:15:17,920
defined data but

00:15:13,839 --> 00:15:19,440
it could be a uh dynamic

00:15:17,920 --> 00:15:21,120
set of events also which is like

00:15:19,440 --> 00:15:22,160
real-time events which are coming into

00:15:21,120 --> 00:15:24,399
your system

00:15:22,160 --> 00:15:27,360
so here let us say i'll give certain

00:15:24,399 --> 00:15:29,920
skus which

00:15:27,360 --> 00:15:31,279
get process and then we will also see

00:15:29,920 --> 00:15:34,320
another example where

00:15:31,279 --> 00:15:35,600
we can also how we can send real time

00:15:34,320 --> 00:15:38,000
events to this system

00:15:35,600 --> 00:15:39,199
which will process those events so let

00:15:38,000 --> 00:15:41,440
us say

00:15:39,199 --> 00:15:43,839
these and the typical application that

00:15:41,440 --> 00:15:47,199
i'm building is like trying to get a

00:15:43,839 --> 00:15:48,240
top selling products for a specific

00:15:47,199 --> 00:15:51,360
retail web

00:15:48,240 --> 00:15:54,560
right so i'll

00:15:51,360 --> 00:16:08,160
take this and

00:15:54,560 --> 00:16:10,720
add a few more instances

00:16:08,160 --> 00:16:12,480
let us say these screens has came stream

00:16:10,720 --> 00:16:14,880
of event has came

00:16:12,480 --> 00:16:17,279
and on top of that i'll apply a flatmap

00:16:14,880 --> 00:16:17,279
function

00:16:20,000 --> 00:16:24,160
and here what i did is i applied a flat

00:16:23,440 --> 00:16:26,000
map function

00:16:24,160 --> 00:16:27,600
and this platform function is very

00:16:26,000 --> 00:16:30,639
simple it just takes a

00:16:27,600 --> 00:16:32,160
set of input and based on that input it

00:16:30,639 --> 00:16:34,560
will just count that

00:16:32,160 --> 00:16:37,839
number of time it has came and attach it

00:16:34,560 --> 00:16:37,839
as a value of one

00:16:41,440 --> 00:16:45,519
so on that flat map now the next

00:16:44,320 --> 00:16:49,040
function i wanted to

00:16:45,519 --> 00:16:51,360
use is key by and the key by function is

00:16:49,040 --> 00:16:54,880
more like a partition

00:16:51,360 --> 00:16:56,399
partition function to distribute your

00:16:54,880 --> 00:17:00,639
data into its key

00:16:56,399 --> 00:17:03,600
so here if you see the flatmap function

00:17:00,639 --> 00:17:05,439
took a input which is string and then it

00:17:03,600 --> 00:17:07,919
gave us out as a tuple

00:17:05,439 --> 00:17:10,240
and in tuple two there are two elements

00:17:07,919 --> 00:17:12,400
in that tuple two

00:17:10,240 --> 00:17:14,480
one is the input value which came in

00:17:12,400 --> 00:17:16,079
which is the first element in the tuple

00:17:14,480 --> 00:17:18,000
and the second is the count number of

00:17:16,079 --> 00:17:21,439
time it has arrived into

00:17:18,000 --> 00:17:23,760
that system so this tuple 2 on that

00:17:21,439 --> 00:17:25,199
we can apply when this tuple 2 came to

00:17:23,760 --> 00:17:27,600
this next function

00:17:25,199 --> 00:17:28,400
the key by function will now take that

00:17:27,600 --> 00:17:30,080
tuple and

00:17:28,400 --> 00:17:32,160
it will partition the data based on the

00:17:30,080 --> 00:17:32,799
key and the key now we will consider

00:17:32,160 --> 00:17:34,559
like

00:17:32,799 --> 00:17:35,840
this element which was the input to the

00:17:34,559 --> 00:17:39,840
platform function

00:17:35,840 --> 00:17:42,880
becomes a key to the key by function

00:17:39,840 --> 00:17:46,160
and once we have partitioned that

00:17:42,880 --> 00:17:50,000
we'll apply a sum function and

00:17:46,160 --> 00:17:52,240
here we will just count number of time

00:17:50,000 --> 00:17:56,240
bit has came

00:17:52,240 --> 00:17:56,240
and next one would be print

00:17:56,320 --> 00:18:01,600
so this is a very similar to the

00:17:59,520 --> 00:18:03,200
word count example that we see in data

00:18:01,600 --> 00:18:06,480
streaming application

00:18:03,200 --> 00:18:08,480
uh here it's just that you can assume

00:18:06,480 --> 00:18:09,520
that this is also can be used as a top

00:18:08,480 --> 00:18:11,840
selling products

00:18:09,520 --> 00:18:13,039
uh for your retail app and you are just

00:18:11,840 --> 00:18:15,280
counting

00:18:13,039 --> 00:18:16,080
which is the best selling products for

00:18:15,280 --> 00:18:19,600
your application

00:18:16,080 --> 00:18:23,440
for your application and now

00:18:19,600 --> 00:18:26,960
the next function would be event dot

00:18:23,440 --> 00:18:30,799
execute and here

00:18:26,960 --> 00:18:33,919
to touch upon few things here

00:18:30,799 --> 00:18:36,720
is you can consider from elements

00:18:33,919 --> 00:18:37,520
and passing that set of input that i

00:18:36,720 --> 00:18:40,720
have given

00:18:37,520 --> 00:18:41,440
is like a source it's your data source

00:18:40,720 --> 00:18:44,320
although it is

00:18:41,440 --> 00:18:45,200
statically defined it is your data

00:18:44,320 --> 00:18:48,880
source which

00:18:45,200 --> 00:18:51,200
get processed by your application

00:18:48,880 --> 00:18:52,320
in that environment it applies certain

00:18:51,200 --> 00:18:54,400
set of function

00:18:52,320 --> 00:18:56,080
it's kind of like you know water flowing

00:18:54,400 --> 00:18:56,880
through a set of functions so it's like

00:18:56,080 --> 00:19:00,000
a waterfall

00:18:56,880 --> 00:19:03,039
where a certain set of functions

00:19:00,000 --> 00:19:04,320
transform that information it

00:19:03,039 --> 00:19:07,360
partitioned its sum

00:19:04,320 --> 00:19:10,000
it it maps them into different

00:19:07,360 --> 00:19:10,640
data structure and the last one is the

00:19:10,000 --> 00:19:12,720
print

00:19:10,640 --> 00:19:14,480
and the print here is a console print

00:19:12,720 --> 00:19:15,600
which will just print out the result set

00:19:14,480 --> 00:19:18,640
into console

00:19:15,600 --> 00:19:19,600
but consider this in a real time real

00:19:18,640 --> 00:19:21,919
world application

00:19:19,600 --> 00:19:22,640
you would be attaching a sync to this

00:19:21,919 --> 00:19:24,640
application

00:19:22,640 --> 00:19:26,320
so instead of using print you will be

00:19:24,640 --> 00:19:28,480
using ignite sync

00:19:26,320 --> 00:19:30,240
as a replacement to store that result

00:19:28,480 --> 00:19:33,440
set into your node of

00:19:30,240 --> 00:19:33,440
apache ignite cluster

00:19:34,720 --> 00:19:41,120
into the cluster of apache ignite node

00:19:38,080 --> 00:19:53,840
sorry my bad

00:19:41,120 --> 00:19:53,840
and i'll execute that

00:19:58,240 --> 00:20:03,600
so now you could see the when when i ran

00:20:04,840 --> 00:20:20,880
this

00:20:06,640 --> 00:20:35,840
from this function

00:20:20,880 --> 00:20:35,840
it is the previous little set

00:20:52,720 --> 00:20:57,360
so now i ran this code and then it

00:21:00,840 --> 00:21:17,840
printed

00:21:03,760 --> 00:21:17,840
yeah that's coming up

00:21:21,760 --> 00:21:24,960
so here you could see that one two three

00:21:24,240 --> 00:21:26,799
four five

00:21:24,960 --> 00:21:28,480
which was the first reason that arrived

00:21:26,799 --> 00:21:30,880
twice into this

00:21:28,480 --> 00:21:33,120
application it it was counted as two

00:21:30,880 --> 00:21:37,440
because it it came twice so it it

00:21:33,120 --> 00:21:41,280
it becomes the you know top uh selling

00:21:37,440 --> 00:21:44,880
product for your retail application

00:21:41,280 --> 00:21:47,360
so with that i'll i'll move into another

00:21:44,880 --> 00:21:47,360
example

00:21:48,400 --> 00:21:52,720
now this is um this example is very

00:21:51,600 --> 00:21:57,280
similar to the

00:21:52,720 --> 00:22:00,480
pipeline we have shared earlier

00:21:57,280 --> 00:22:04,720
so here the example was like

00:22:00,480 --> 00:22:08,000
app is sending a set of events into your

00:22:04,720 --> 00:22:09,760
apache kafka broker and then your flink

00:22:08,000 --> 00:22:11,679
is consuming those events

00:22:09,760 --> 00:22:13,840
processing them and then storing the

00:22:11,679 --> 00:22:18,000
result set into ignite sync

00:22:13,840 --> 00:22:21,200
so to run this i'll just quickly

00:22:18,000 --> 00:22:23,360
uh go through the code so here you could

00:22:21,200 --> 00:22:26,559
see like we have defined

00:22:23,360 --> 00:22:29,120
ignite sync and

00:22:26,559 --> 00:22:30,480
the sync has few functions and

00:22:29,120 --> 00:22:33,440
attributes where

00:22:30,480 --> 00:22:34,240
we say set allow overwrite true which

00:22:33,440 --> 00:22:36,400
means like

00:22:34,240 --> 00:22:37,360
whether we are allowed to overwrite the

00:22:36,400 --> 00:22:40,400
key

00:22:37,360 --> 00:22:42,480
in which the result was written and

00:22:40,400 --> 00:22:44,640
if you say to false then it will just

00:22:42,480 --> 00:22:46,559
accept the first result for that key

00:22:44,640 --> 00:22:47,679
and it will not allow you to write it

00:22:46,559 --> 00:22:49,760
again

00:22:47,679 --> 00:22:51,440
and you could also set auto flush

00:22:49,760 --> 00:22:54,080
frequency which means like

00:22:51,440 --> 00:22:55,840
i said here five seconds so which means

00:22:54,080 --> 00:22:58,559
like at every five seconds you

00:22:55,840 --> 00:22:59,679
from your streamer the data will be sent

00:22:58,559 --> 00:23:02,799
to your

00:22:59,679 --> 00:23:03,760
cluster that that's the flush frequency

00:23:02,799 --> 00:23:07,200
of your

00:23:03,760 --> 00:23:08,559
application and this example is very

00:23:07,200 --> 00:23:11,120
similar to the previous one

00:23:08,559 --> 00:23:12,320
we just seen where you have stream

00:23:11,120 --> 00:23:14,159
execution environment

00:23:12,320 --> 00:23:15,440
you get the environment in which you

00:23:14,159 --> 00:23:18,559
will be running your

00:23:15,440 --> 00:23:22,799
application and then in this environment

00:23:18,559 --> 00:23:25,120
you attach a data source and

00:23:22,799 --> 00:23:25,919
this data source contains a flink kafka

00:23:25,120 --> 00:23:28,559
consumer

00:23:25,919 --> 00:23:29,520
so here we are using a kafka consumer

00:23:28,559 --> 00:23:31,120
which will

00:23:29,520 --> 00:23:33,919
you know consume the events which is

00:23:31,120 --> 00:23:37,360
being published to the kafka broker

00:23:33,919 --> 00:23:39,760
then we have window counts and

00:23:37,360 --> 00:23:40,400
it takes a set of events which are

00:23:39,760 --> 00:23:43,440
coming in

00:23:40,400 --> 00:23:44,240
to that system then applies a flat map

00:23:43,440 --> 00:23:47,360
function

00:23:44,240 --> 00:23:48,080
and here we split the data so what split

00:23:47,360 --> 00:23:50,080
means like

00:23:48,080 --> 00:23:52,320
you could give a sentence and then it

00:23:50,080 --> 00:23:54,320
will split by white space and take those

00:23:52,320 --> 00:23:57,520
words and it will just attach

00:23:54,320 --> 00:24:00,960
count to that once you do that then

00:23:57,520 --> 00:24:02,880
we apply a key by function based on the

00:24:00,960 --> 00:24:04,000
first element in the tuple which is

00:24:02,880 --> 00:24:07,760
coming in

00:24:04,000 --> 00:24:08,480
and then we process those events in time

00:24:07,760 --> 00:24:11,279
window

00:24:08,480 --> 00:24:12,320
so here the context of application we

00:24:11,279 --> 00:24:14,880
are saying like

00:24:12,320 --> 00:24:15,360
we are interested in aggregation of data

00:24:14,880 --> 00:24:17,840
or

00:24:15,360 --> 00:24:19,279
processing of data every 10 seconds so

00:24:17,840 --> 00:24:21,679
all the result sets are

00:24:19,279 --> 00:24:23,440
getting processed within that 10 second

00:24:21,679 --> 00:24:26,559
period

00:24:23,440 --> 00:24:27,600
then we sum those events result sets on

00:24:26,559 --> 00:24:30,320
the position 1

00:24:27,600 --> 00:24:32,640
and then we map it and here we also use

00:24:30,320 --> 00:24:34,480
another formatter function which is

00:24:32,640 --> 00:24:36,640
we are formatting the result set of

00:24:34,480 --> 00:24:40,320
tuple and store it as a

00:24:36,640 --> 00:24:42,480
hash map to ignite ignite accepts hash

00:24:40,320 --> 00:24:43,039
map as a as a data streamer input so

00:24:42,480 --> 00:24:45,120
here

00:24:43,039 --> 00:24:46,080
we just formatted that data structure

00:24:45,120 --> 00:24:49,520
from tuple

00:24:46,080 --> 00:24:51,360
to we a to a hash map

00:24:49,520 --> 00:24:52,720
with a string and the in which was the

00:24:51,360 --> 00:24:54,880
value so

00:24:52,720 --> 00:24:56,000
here as i said before like you know we

00:24:54,880 --> 00:24:57,840
can apply

00:24:56,000 --> 00:24:59,919
uh different function that allows you to

00:24:57,840 --> 00:25:00,720
transform a data structure from one to

00:24:59,919 --> 00:25:04,240
another

00:25:00,720 --> 00:25:04,240
depending on your system's need

00:25:04,880 --> 00:25:11,440
so once we do that then we again

00:25:08,960 --> 00:25:13,919
attach a sink here here the ignite sync

00:25:11,440 --> 00:25:15,360
was attached which which will act as a

00:25:13,919 --> 00:25:19,279
uh

00:25:15,360 --> 00:25:20,880
sync to capture all of those result sets

00:25:19,279 --> 00:25:24,240
and get stored into the

00:25:20,880 --> 00:25:26,159
ignite database and it will be it can be

00:25:24,240 --> 00:25:28,400
then again

00:25:26,159 --> 00:25:29,440
processed further if you are considering

00:25:28,400 --> 00:25:32,320
enough

00:25:29,440 --> 00:25:34,240
in the previous example where i have

00:25:32,320 --> 00:25:36,400
shown

00:25:34,240 --> 00:25:38,799
that ignite streamers are publishing

00:25:36,400 --> 00:25:41,360
those events into ignite nodes

00:25:38,799 --> 00:25:42,000
and now you could also build another

00:25:41,360 --> 00:25:44,159
application

00:25:42,000 --> 00:25:46,000
where ignite clients thin or thick

00:25:44,159 --> 00:25:46,720
either of those type of clients can be

00:25:46,000 --> 00:25:48,720
connect

00:25:46,720 --> 00:25:50,720
can be used to connect to those data

00:25:48,720 --> 00:25:52,880
store nodes of ignite and

00:25:50,720 --> 00:25:54,799
start building another set of analytical

00:25:52,880 --> 00:25:56,000
apps which in which you would be running

00:25:54,799 --> 00:25:58,320
your compute jobs

00:25:56,000 --> 00:25:59,520
so now that one set of results are being

00:25:58,320 --> 00:26:01,520
stored in ignite node

00:25:59,520 --> 00:26:03,279
now you could also run a compute job

00:26:01,520 --> 00:26:05,760
which gives you

00:26:03,279 --> 00:26:06,320
more detail historical data in a sense

00:26:05,760 --> 00:26:08,480
like

00:26:06,320 --> 00:26:10,000
in the previous example where i've seen

00:26:08,480 --> 00:26:11,679
like every 10 second we

00:26:10,000 --> 00:26:13,679
are interested in certain set of events

00:26:11,679 --> 00:26:15,120
getting aggregated but then once those

00:26:13,679 --> 00:26:17,039
results are being stored

00:26:15,120 --> 00:26:18,799
the way you format the data you can also

00:26:17,039 --> 00:26:20,240
use that data which is stored for

00:26:18,799 --> 00:26:23,200
historical

00:26:20,240 --> 00:26:24,880
uh or archival purposes now you can

00:26:23,200 --> 00:26:27,520
attach your ignite clients

00:26:24,880 --> 00:26:29,200
to now build another application or

00:26:27,520 --> 00:26:32,559
build reports for example

00:26:29,200 --> 00:26:35,840
and use those reports for um

00:26:32,559 --> 00:26:35,840
further analytics right

00:26:37,120 --> 00:26:42,480
so here we add that sync

00:26:40,400 --> 00:26:43,520
to store those result set into ignite

00:26:42,480 --> 00:26:45,600
sync

00:26:43,520 --> 00:26:48,080
and we finally did an environment

00:26:45,600 --> 00:26:52,159
execute so now i'll run this

00:26:48,080 --> 00:26:58,000
example just to show how it works at

00:26:52,159 --> 00:26:58,000
zookeeper start a kafka server

00:26:59,520 --> 00:27:07,840
already have the topic created think

00:27:15,279 --> 00:27:21,840
okay i think it's already created now

00:27:18,480 --> 00:27:21,840
here we will start the cluster

00:27:41,279 --> 00:27:45,840
so here we are just starting a link job

00:27:44,080 --> 00:28:02,159
cluster

00:27:45,840 --> 00:28:04,480
and we could see it should come up so

00:28:02,159 --> 00:28:06,399
here we have a available task slot one

00:28:04,480 --> 00:28:08,159
which means and we don't have a running

00:28:06,399 --> 00:28:10,240
jobs as of yet

00:28:08,159 --> 00:28:11,600
and one once we start the job it will

00:28:10,240 --> 00:28:14,320
start showing the results that's coming

00:28:11,600 --> 00:28:18,000
in and if the job will get executed

00:28:14,320 --> 00:28:20,240
go back to terminal and i'll start the

00:28:18,000 --> 00:28:20,240
job

00:28:21,679 --> 00:28:25,840
so this is uh running the same job that

00:28:24,320 --> 00:28:29,440
we have

00:28:25,840 --> 00:28:32,480
discussed earlier for the

00:28:29,440 --> 00:28:35,120
event counts and in this job

00:28:32,480 --> 00:28:35,600
we have also we could see that ignite

00:28:35,120 --> 00:28:38,159
sync

00:28:35,600 --> 00:28:38,880
is being used so which is like ignite

00:28:38,159 --> 00:28:41,600
node which

00:28:38,880 --> 00:28:43,039
gets started and it this node will start

00:28:41,600 --> 00:28:45,120
acting as a

00:28:43,039 --> 00:28:46,159
data sync you could see like there is

00:28:45,120 --> 00:28:50,960
only one server

00:28:46,159 --> 00:28:50,960
defined here and will now start

00:28:51,200 --> 00:28:56,240
publishing events into this so as this

00:28:54,480 --> 00:28:57,200
job started we can also start seeing

00:28:56,240 --> 00:28:59,200
some

00:28:57,200 --> 00:29:00,960
events you know these jobs started

00:28:59,200 --> 00:29:02,640
running and the available tax slot

00:29:00,960 --> 00:29:03,760
becomes zero and we have now a running

00:29:02,640 --> 00:29:05,679
job

00:29:03,760 --> 00:29:08,080
right and now we will start publishing

00:29:05,679 --> 00:29:11,200
some events

00:29:08,080 --> 00:29:13,919
and this is a simple kafka

00:29:11,200 --> 00:29:16,320
producer application which just take a

00:29:13,919 --> 00:29:16,320
set of

00:29:16,799 --> 00:29:21,360
item information consider this like a

00:29:19,120 --> 00:29:22,159
product id or skus which are getting fed

00:29:21,360 --> 00:29:26,320
into that

00:29:22,159 --> 00:29:29,679
system and the data publisher will then

00:29:26,320 --> 00:29:30,640
process them so we'll just quickly run

00:29:29,679 --> 00:29:34,159
this

00:29:30,640 --> 00:29:34,159
and as i started running it

00:29:34,799 --> 00:29:42,640
you could see that

00:29:38,320 --> 00:29:45,279
it has started running those events and

00:29:42,640 --> 00:29:47,120
with that it also showing number of

00:29:45,279 --> 00:29:50,080
record being sent

00:29:47,120 --> 00:29:51,279
and number of record being processed by

00:29:50,080 --> 00:29:53,120
the system

00:29:51,279 --> 00:29:54,559
so you could see like real time the data

00:29:53,120 --> 00:29:57,279
set is continuously

00:29:54,559 --> 00:29:57,279
getting changed

00:29:57,840 --> 00:30:02,799
and as in when more and more data

00:30:00,799 --> 00:30:03,840
getting fed to the system it it get

00:30:02,799 --> 00:30:07,600
analyzed and

00:30:03,840 --> 00:30:12,159
you know stored into the ignite thing

00:30:07,600 --> 00:30:14,880
here we will use the ignite

00:30:12,159 --> 00:30:15,760
rest api and we will just see what

00:30:14,880 --> 00:30:18,240
what's happening

00:30:15,760 --> 00:30:19,279
to the result set so this is a step api

00:30:18,240 --> 00:30:22,799
to get the

00:30:19,279 --> 00:30:25,520
uh to run a command query scan and

00:30:22,799 --> 00:30:26,559
we just running it in a test cache and

00:30:25,520 --> 00:30:29,279
these are the items

00:30:26,559 --> 00:30:30,399
information that are getting stored and

00:30:29,279 --> 00:30:32,640
we could see like

00:30:30,399 --> 00:30:33,520
all of those item information count are

00:30:32,640 --> 00:30:36,880
same but as

00:30:33,520 --> 00:30:39,039
i refresh continuously

00:30:36,880 --> 00:30:41,360
you would see that few of those values

00:30:39,039 --> 00:30:44,320
will continue to change

00:30:41,360 --> 00:30:44,320
every 10 seconds

00:30:46,240 --> 00:30:49,440
so now you see like this element has

00:30:48,399 --> 00:30:53,120
gone down

00:30:49,440 --> 00:30:55,039
from those result sets so

00:30:53,120 --> 00:30:57,440
this is this is pretty much the demo for

00:30:55,039 --> 00:31:01,679
the pipeline application

00:30:57,440 --> 00:31:01,679
that we discuss

00:31:02,000 --> 00:31:05,519
so the apps which was the python app

00:31:04,880 --> 00:31:08,000
which was

00:31:05,519 --> 00:31:10,399
a simple data producer which was

00:31:08,000 --> 00:31:14,240
publishing events to a kafka broker

00:31:10,399 --> 00:31:16,960
and from there we use apache flink

00:31:14,240 --> 00:31:17,519
job cluster to process those events from

00:31:16,960 --> 00:31:20,000
that

00:31:17,519 --> 00:31:21,440
message queue and then we publish those

00:31:20,000 --> 00:31:24,000
result sets

00:31:21,440 --> 00:31:26,000
applied certain functions to change

00:31:24,000 --> 00:31:28,080
those result sets and then

00:31:26,000 --> 00:31:30,320
finally at the 10 second window time

00:31:28,080 --> 00:31:32,799
window we took all of those result sets

00:31:30,320 --> 00:31:33,679
and started writing it to the ignite

00:31:32,799 --> 00:31:36,559
sync

00:31:33,679 --> 00:31:38,720
and now that can even be used for

00:31:36,559 --> 00:31:40,799
building out reports or any kind of

00:31:38,720 --> 00:31:42,640
another analytical application that you

00:31:40,799 --> 00:31:44,480
would like to build

00:31:42,640 --> 00:31:46,080
with that i think that's pretty much i

00:31:44,480 --> 00:31:51,200
had in

00:31:46,080 --> 00:31:51,200
the demo and i can take questions uh now

00:31:53,919 --> 00:31:57,600
okay so uh the question is how would you

00:31:56,240 --> 00:32:00,320
compare flink and

00:31:57,600 --> 00:32:00,960
spark stream in general i think there is

00:32:00,320 --> 00:32:02,960
a

00:32:00,960 --> 00:32:04,799
small difference i think both projects

00:32:02,960 --> 00:32:06,720
are very popular spark and

00:32:04,799 --> 00:32:08,080
flink but there is a very small

00:32:06,720 --> 00:32:12,880
difference in a sense that

00:32:08,080 --> 00:32:15,919
flink allows you to process events

00:32:12,880 --> 00:32:16,960
in even if you are interested at a very

00:32:15,919 --> 00:32:19,120
granular

00:32:16,960 --> 00:32:20,880
single even point of view so you can you

00:32:19,120 --> 00:32:23,279
can start processing even

00:32:20,880 --> 00:32:24,559
one single event at a time whereas in

00:32:23,279 --> 00:32:26,480
case of spark

00:32:24,559 --> 00:32:28,240
it actually you know micro batches those

00:32:26,480 --> 00:32:30,480
events and within that micro batch

00:32:28,240 --> 00:32:32,640
a set of events will be processed so

00:32:30,480 --> 00:32:32,960
that's that's more of a difference but i

00:32:32,640 --> 00:32:35,279
think

00:32:32,960 --> 00:32:36,399
both of those projects are very popular

00:32:35,279 --> 00:32:42,240
and can be used

00:32:36,399 --> 00:32:44,640
alternatively depending on your needs

00:32:42,240 --> 00:32:46,640
yeah so the next question was like is it

00:32:44,640 --> 00:32:48,559
advisable to have flink stream and

00:32:46,640 --> 00:32:49,919
transform the data that is fed into

00:32:48,559 --> 00:32:52,399
ignite real time

00:32:49,919 --> 00:32:52,960
by some other technology essentially

00:32:52,399 --> 00:32:56,320
keeping

00:32:52,960 --> 00:32:58,399
flink after ignite so

00:32:56,320 --> 00:32:59,840
i think that is the another example that

00:32:58,399 --> 00:33:03,760
we have seen

00:32:59,840 --> 00:33:06,399
in the notes and i'll just

00:33:03,760 --> 00:33:06,399
bring that up

00:33:08,720 --> 00:33:12,080
so this is typically the example that

00:33:10,799 --> 00:33:15,600
you are looking for where

00:33:12,080 --> 00:33:18,559
you use ignite data store

00:33:15,600 --> 00:33:19,679
nodes as a data sources and then use

00:33:18,559 --> 00:33:22,240
flink

00:33:19,679 --> 00:33:23,120
to consume those information from ignite

00:33:22,240 --> 00:33:25,840
and

00:33:23,120 --> 00:33:26,399
process them and store those result set

00:33:25,840 --> 00:33:28,320
into

00:33:26,399 --> 00:33:30,320
or visualize those result sets into

00:33:28,320 --> 00:33:32,240
another application like grafana where

00:33:30,320 --> 00:33:34,159
you use them for building out dashboards

00:33:32,240 --> 00:33:36,799
or any other reports

00:33:34,159 --> 00:33:38,320
so that is that that is another example

00:33:36,799 --> 00:33:41,600
where

00:33:38,320 --> 00:33:43,760
we already have ignite data sources

00:33:41,600 --> 00:33:45,200
and those provides connectors to which

00:33:43,760 --> 00:33:46,080
you can use to connect to a flink

00:33:45,200 --> 00:33:49,919
cluster and

00:33:46,080 --> 00:33:49,919
start processing those result sets

00:33:50,480 --> 00:33:53,919
all right i think that's about time and

00:33:53,120 --> 00:33:56,799
um

00:33:53,919 --> 00:33:58,399
thank you so much for joining today i

00:33:56,799 --> 00:34:02,159
really appreciate

00:33:58,399 --> 00:34:06,559
and and thanks for

00:34:02,159 --> 00:34:09,200
listening to me all right

00:34:06,559 --> 00:34:10,480
have a good day and enjoy rest of the

00:34:09,200 --> 00:34:20,800
apache conversations

00:34:10,480 --> 00:34:22,879
thank you so much bye

00:34:20,800 --> 00:34:22,879

YouTube URL: https://www.youtube.com/watch?v=n74HMmTz5i0


