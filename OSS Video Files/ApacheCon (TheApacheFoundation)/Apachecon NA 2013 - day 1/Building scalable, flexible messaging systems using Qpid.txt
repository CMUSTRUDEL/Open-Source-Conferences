Title: Building scalable, flexible messaging systems using Qpid
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Jack Gibson ApacheCon NA 2013
Tapping the Stream
Captions: 
	00:00:00,000 --> 00:00:07,890
so please welcome Jack Gibson he's going

00:00:03,419 --> 00:00:10,590
to talk about cupid so uh this is the

00:00:07,890 --> 00:00:12,330
third cupid amqp presentation of the day

00:00:10,590 --> 00:00:13,710
so I'm going to try not to overlap with

00:00:12,330 --> 00:00:15,809
everything else are we also talked about

00:00:13,710 --> 00:00:19,980
I said talk about actually what we built

00:00:15,809 --> 00:00:23,460
versus vendor speak so trying to get

00:00:19,980 --> 00:00:26,279
fast out of it so so I'm Jack Gibson I'm

00:00:23,460 --> 00:00:28,529
from paypal we are in the process of

00:00:26,279 --> 00:00:31,019
rolling out n QP and keep it in

00:00:28,529 --> 00:00:32,520
particular to a large portion of our

00:00:31,019 --> 00:00:34,320
infrastructure and we'll talk it a

00:00:32,520 --> 00:00:35,700
little bit about what we found and some

00:00:34,320 --> 00:00:37,829
of the lessons learned through our

00:00:35,700 --> 00:00:40,020
implementation and then hopefully put up

00:00:37,829 --> 00:00:42,570
for questions as you guys go through so

00:00:40,020 --> 00:00:43,980
a little bit of background of paypal and

00:00:42,570 --> 00:00:45,960
i'll talk about why we're actually

00:00:43,980 --> 00:00:48,809
looking at cupid and what we're doing

00:00:45,960 --> 00:00:50,700
there so we handle actually sixty

00:00:48,809 --> 00:00:53,850
percent of bald web transactions that

00:00:50,700 --> 00:00:55,199
involve money worldwide so it's a pretty

00:00:53,850 --> 00:00:56,820
significant number we actually handle

00:00:55,199 --> 00:01:00,930
more monetary transactions than Google

00:00:56,820 --> 00:01:02,489
and Yahoo combined so we actually have

00:01:00,930 --> 00:01:03,750
one of the largest Oracle instances in

00:01:02,489 --> 00:01:07,830
the world so when you think about

00:01:03,750 --> 00:01:10,770
databases our databases are massive we

00:01:07,830 --> 00:01:13,110
built everything from scratch except for

00:01:10,770 --> 00:01:15,560
maybe Oracle we build our own HUP

00:01:13,110 --> 00:01:17,790
servers who they are messaging systems

00:01:15,560 --> 00:01:19,229
only thing they don't think we've built

00:01:17,790 --> 00:01:22,740
there's our own routers and our own

00:01:19,229 --> 00:01:26,520
operating system yeah and I don't say

00:01:22,740 --> 00:01:29,729
that's a good thing either we have a mix

00:01:26,520 --> 00:01:31,950
of thousands of stateless processes our

00:01:29,729 --> 00:01:35,640
system was originally CGI based

00:01:31,950 --> 00:01:37,890
application that got 2 / 4 gig so can

00:01:35,640 --> 00:01:40,049
imagine a single runtime executable

00:01:37,890 --> 00:01:41,759
sitting in apache that you're forking

00:01:40,049 --> 00:01:44,600
and dispatching tearing up and tearing

00:01:41,759 --> 00:01:47,990
down it's four gig it's a massive fire

00:01:44,600 --> 00:01:51,030
we have over 20 million lines of code

00:01:47,990 --> 00:01:53,220
support payments we said we're going to

00:01:51,030 --> 00:01:54,840
break that up and great services we

00:01:53,220 --> 00:01:56,700
created services the fact that we have

00:01:54,840 --> 00:01:59,009
thousands of individual damon's running

00:01:56,700 --> 00:02:00,509
between everything now every single line

00:01:59,009 --> 00:02:03,090
of code that was in that four-game

00:02:00,509 --> 00:02:03,790
executable ended up being a single

00:02:03,090 --> 00:02:05,410
individual day

00:02:03,790 --> 00:02:06,910
that we broke up and now with

00:02:05,410 --> 00:02:08,770
interconnected ugh so we have a massive

00:02:06,910 --> 00:02:10,660
network of interconnection between

00:02:08,770 --> 00:02:13,540
things and you can imagine managing that

00:02:10,660 --> 00:02:15,310
is pretty painful it also creates really

00:02:13,540 --> 00:02:16,960
interesting load balancing of routing

00:02:15,310 --> 00:02:18,790
issues trying to connect all these

00:02:16,960 --> 00:02:20,230
individual things together so if you

00:02:18,790 --> 00:02:22,680
imagine a front-end application needs to

00:02:20,230 --> 00:02:24,760
connect to 1,300 individual services

00:02:22,680 --> 00:02:26,940
that's going to be pretty painful for a

00:02:24,760 --> 00:02:30,430
connection pooling festival resumption

00:02:26,940 --> 00:02:31,930
grouting how do I handle security and

00:02:30,430 --> 00:02:34,240
then you have to do that both ways so

00:02:31,930 --> 00:02:37,450
ends up with a really really complex

00:02:34,240 --> 00:02:39,100
hairball we have traditional g2 the

00:02:37,450 --> 00:02:42,520
applications we're migrating from sue

00:02:39,100 --> 00:02:44,100
plus plus the Java from our core systems

00:02:42,520 --> 00:02:46,810
as well as a lot of our service tears

00:02:44,100 --> 00:02:48,820
and then payments are generally

00:02:46,810 --> 00:02:50,100
asynchronous everybody thinks they swipe

00:02:48,820 --> 00:02:53,890
a credit card and it happens right now

00:02:50,100 --> 00:02:55,989
it's actually an asynchronous process so

00:02:53,890 --> 00:02:58,030
you think about your experiences you're

00:02:55,989 --> 00:03:00,700
waiting but in the back end is actually

00:02:58,030 --> 00:03:03,310
sending a message off to somebody coming

00:03:00,700 --> 00:03:04,780
back round over a period of time maybe

00:03:03,310 --> 00:03:08,200
in a different order maybe to a

00:03:04,780 --> 00:03:10,600
different destination a CGI application

00:03:08,200 --> 00:03:12,730
that is completely blocking isn't built

00:03:10,600 --> 00:03:14,230
to do that so not a will be changing our

00:03:12,730 --> 00:03:16,180
architecture from a synchronous

00:03:14,230 --> 00:03:17,709
point-to-point architecture but to a

00:03:16,180 --> 00:03:24,910
message based architecture that fits our

00:03:17,709 --> 00:03:26,860
business model or requirements were as

00:03:24,910 --> 00:03:28,780
part of this transformation was we won

00:03:26,860 --> 00:03:30,459
something this highly scalable partition

00:03:28,780 --> 00:03:33,160
of all we don't want one thing to take

00:03:30,459 --> 00:03:34,810
down anything else in the system it had

00:03:33,160 --> 00:03:37,209
to be cloud friendly eventually when

00:03:34,810 --> 00:03:39,100
expand beyond a parameter the perimeter

00:03:37,209 --> 00:03:41,380
of PayPal we also expand beyond the

00:03:39,100 --> 00:03:42,880
perimeter of the United States so we

00:03:41,380 --> 00:03:44,620
need to be able to break things up into

00:03:42,880 --> 00:03:48,150
different segments and be able to write

00:03:44,620 --> 00:03:53,110
between those partitions really well

00:03:48,150 --> 00:03:54,880
failure we can't go down no single point

00:03:53,110 --> 00:03:55,989
of failure so if I take one thing out

00:03:54,880 --> 00:03:59,890
it's not going to cause everything else

00:03:55,989 --> 00:04:01,239
it to go down nothing shared so we don't

00:03:59,890 --> 00:04:03,370
want anything sharing between anything

00:04:01,239 --> 00:04:05,519
else so that means hard drives that

00:04:03,370 --> 00:04:08,820
means switches that means databases

00:04:05,519 --> 00:04:11,260
nothing and then latency near real time

00:04:08,820 --> 00:04:12,910
so if you think of what paypal is it's

00:04:11,260 --> 00:04:15,600
like a stock exchange you're taking

00:04:12,910 --> 00:04:17,550
traders and brokers and bringing them

00:04:15,600 --> 00:04:19,140
we're taking customers and merchants of

00:04:17,550 --> 00:04:21,420
bring them together and you expect your

00:04:19,140 --> 00:04:25,560
payment to happen like that not on a

00:04:21,420 --> 00:04:30,720
couple minutes so that's where we

00:04:25,560 --> 00:04:33,330
started and why did we go down to keep

00:04:30,720 --> 00:04:35,280
it and keep you up we wanted an open

00:04:33,330 --> 00:04:36,780
messaging protocol we knew that we had

00:04:35,280 --> 00:04:38,400
this proprietary stuff that we're gonna

00:04:36,780 --> 00:04:39,360
have to plumb together but we didn't

00:04:38,400 --> 00:04:42,210
want to do is put ourselves in a

00:04:39,360 --> 00:04:43,800
non-proprietary mess we didn't want to

00:04:42,210 --> 00:04:45,810
go down the m key series route where we

00:04:43,800 --> 00:04:48,300
can only use IBM products or only use

00:04:45,810 --> 00:04:49,950
JMS we want the ability to say hey we

00:04:48,300 --> 00:04:51,380
want to go to nodejs and we want to

00:04:49,950 --> 00:04:53,340
connect that into our service framework

00:04:51,380 --> 00:04:54,570
there's no climb out there we want to be

00:04:53,340 --> 00:04:55,860
scholar but there's nothing out there

00:04:54,570 --> 00:04:58,350
from a scholar client for that

00:04:55,860 --> 00:05:00,210
particular messaging system amqp gives

00:04:58,350 --> 00:05:02,520
us the ability to build new clients and

00:05:00,210 --> 00:05:04,020
new implementations based on the wire

00:05:02,520 --> 00:05:05,790
protocol and not be bound to a

00:05:04,020 --> 00:05:07,380
particular vendor and something we're

00:05:05,790 --> 00:05:10,310
actually ready taking advantage of today

00:05:07,380 --> 00:05:12,930
with our know Jas implementation

00:05:10,310 --> 00:05:16,470
cross-platform interoperability massive

00:05:12,930 --> 00:05:18,270
amount of C++ and Java we've looked at

00:05:16,470 --> 00:05:20,910
other gamekeeper face brokers like

00:05:18,270 --> 00:05:23,220
RabbitMQ VMware wouldn't actually

00:05:20,910 --> 00:05:27,180
support C++ which is how we ended up on

00:05:23,220 --> 00:05:30,180
Cupid in particular we require very

00:05:27,180 --> 00:05:31,830
little agency near real-time so the

00:05:30,180 --> 00:05:34,170
difference between point-to-point and a

00:05:31,830 --> 00:05:37,260
message based architecture it almost

00:05:34,170 --> 00:05:39,420
have to be identical if that we're not

00:05:37,260 --> 00:05:41,400
going to go there eventual

00:05:39,420 --> 00:05:43,670
interoperability of activemq we have a

00:05:41,400 --> 00:05:46,410
very very large active implementation

00:05:43,670 --> 00:05:48,930
then we use for all our pub sub topic

00:05:46,410 --> 00:05:50,640
based event distributions so any kind of

00:05:48,930 --> 00:05:52,470
state changes with an object or anything

00:05:50,640 --> 00:05:53,910
else we publish off an event that

00:05:52,470 --> 00:05:57,780
eventually goes off to another system

00:05:53,910 --> 00:06:01,020
that we make risk decisions on we were

00:05:57,780 --> 00:06:02,820
customers and possible fraud we slot you

00:06:01,020 --> 00:06:04,140
lock your account all kinds of other

00:06:02,820 --> 00:06:06,630
needs for saying that paypal does for

00:06:04,140 --> 00:06:08,520
you so

00:06:06,630 --> 00:06:11,460
we need to integrate with that actual

00:06:08,520 --> 00:06:13,500
network as well and then the ability to

00:06:11,460 --> 00:06:15,030
influence of community it's one thing to

00:06:13,500 --> 00:06:16,440
say that we're going to download this

00:06:15,030 --> 00:06:19,140
product we also want the ability to

00:06:16,440 --> 00:06:20,610
influence where it's going we do not

00:06:19,140 --> 00:06:22,410
work that we think that we should be

00:06:20,610 --> 00:06:25,020
able to help guide where things go a

00:06:22,410 --> 00:06:27,000
little bit and if we went with an IV I

00:06:25,020 --> 00:06:29,460
man on open source capabilities that we

00:06:27,000 --> 00:06:31,620
weren't going to be able to do that so

00:06:29,460 --> 00:06:33,390
that's how we ended up with an AP

00:06:31,620 --> 00:06:38,100
implementation and we'll keep it in

00:06:33,390 --> 00:06:40,050
particular questions interrupt if you

00:06:38,100 --> 00:06:43,950
guys have questions on the fly I'm not

00:06:40,050 --> 00:06:46,110
going to wait till the end so so where

00:06:43,950 --> 00:06:49,860
we started we started out with a very

00:06:46,110 --> 00:06:52,650
simple implementation couple of brokers

00:06:49,860 --> 00:06:53,940
we throw a later melfin f5 there were

00:06:52,650 --> 00:06:58,170
five switch and photo Bowman were low

00:06:53,940 --> 00:07:00,090
bouncer round-robin least min connection

00:06:58,170 --> 00:07:01,560
rule everybody at least got one

00:07:00,090 --> 00:07:03,270
connection to each individual broker

00:07:01,560 --> 00:07:05,340
every program in a minimal one

00:07:03,270 --> 00:07:08,660
connection that way if something went

00:07:05,340 --> 00:07:10,950
down and things didn't break it worked

00:07:08,660 --> 00:07:13,800
but when you try to scale that out to a

00:07:10,950 --> 00:07:16,890
very large infrastructure it didn't work

00:07:13,800 --> 00:07:17,760
in a woman and it kind of created the

00:07:16,890 --> 00:07:20,100
same problems we have from a

00:07:17,760 --> 00:07:21,330
point-to-point architecture the illness

00:07:20,100 --> 00:07:24,480
number of connections and everything

00:07:21,330 --> 00:07:25,260
else we found that it didn't scale and

00:07:24,480 --> 00:07:27,450
it actually from a performance

00:07:25,260 --> 00:07:29,040
perspective it caused us to actually

00:07:27,450 --> 00:07:29,970
make hops through different things that

00:07:29,040 --> 00:07:31,800
we didn't need to make a hot through

00:07:29,970 --> 00:07:35,310
because we're always forcing it to go to

00:07:31,800 --> 00:07:36,750
that load balancer so what we ended up

00:07:35,310 --> 00:07:41,190
doing if you look think back to the

00:07:36,750 --> 00:07:43,560
conversation earlier we wanted to change

00:07:41,190 --> 00:07:45,270
that so what we found is that we could

00:07:43,560 --> 00:07:47,520
actually go to about 20 billion messages

00:07:45,270 --> 00:07:51,210
a day around 2k average message size

00:07:47,520 --> 00:07:53,640
which is a pretty large number we get

00:07:51,210 --> 00:07:56,370
variation message to size introduces

00:07:53,640 --> 00:07:58,340
latency so smaller messages were very

00:07:56,370 --> 00:08:01,670
consistent larger the pay little enough

00:07:58,340 --> 00:08:05,970
with larger latency on the message

00:08:01,670 --> 00:08:07,710
connections shortly processes CGI's type

00:08:05,970 --> 00:08:10,890
processes at terrapin teardown killed

00:08:07,710 --> 00:08:13,380
brokers so we would have five to six

00:08:10,890 --> 00:08:14,890
styles and connections on a broker it

00:08:13,380 --> 00:08:17,840
would cause the brokers are

00:08:14,890 --> 00:08:20,420
and latency will start going on it could

00:08:17,840 --> 00:08:21,800
handle it but it wasn't very fast and

00:08:20,420 --> 00:08:26,120
then routing concerns we had a hard time

00:08:21,800 --> 00:08:30,170
distributing connections we also had no

00:08:26,120 --> 00:08:31,670
ability to in vm memory nvm messaging so

00:08:30,170 --> 00:08:33,140
if your typical java app and you stand

00:08:31,670 --> 00:08:35,150
up and i had a boss or i got Geronimo

00:08:33,140 --> 00:08:37,099
and it comes with an instance of horny q

00:08:35,150 --> 00:08:40,070
or activemq you want to write messages

00:08:37,099 --> 00:08:41,750
to it there was no way to connect that

00:08:40,070 --> 00:08:43,220
back to our existing system with that

00:08:41,750 --> 00:08:44,690
architecture and no way to write back

00:08:43,220 --> 00:08:46,340
because I threw that load balancer in

00:08:44,690 --> 00:08:48,560
the middle so I had no way to actually

00:08:46,340 --> 00:08:51,650
wrap back through a low bouncer back to

00:08:48,560 --> 00:08:53,840
my original destination so creator were

00:08:51,650 --> 00:08:55,670
some really interesting routing problems

00:08:53,840 --> 00:08:58,670
around how we were connected things

00:08:55,670 --> 00:09:00,290
together long-term ok so we realized

00:08:58,670 --> 00:09:05,330
that that architecture wasn't going to

00:09:00,290 --> 00:09:06,980
work so we're reading it once is we

00:09:05,330 --> 00:09:09,740
decide to create the stink layers of

00:09:06,980 --> 00:09:11,840
brokers and instances of cupid a

00:09:09,740 --> 00:09:13,280
frontier set of brokers and mid-tier set

00:09:11,840 --> 00:09:15,320
of brokers and of course I brokers and

00:09:13,280 --> 00:09:18,290
those are really do statements they

00:09:15,320 --> 00:09:19,640
allow us to have a pipe between the

00:09:18,290 --> 00:09:21,290
different layers themselves we don't

00:09:19,640 --> 00:09:23,540
actually put anything on those brokers

00:09:21,290 --> 00:09:26,630
for say all the endpoints actually

00:09:23,540 --> 00:09:29,630
contain or the endpoint brokers contain

00:09:26,630 --> 00:09:31,550
all the implementations so we also

00:09:29,630 --> 00:09:33,830
partition each layer by function and

00:09:31,550 --> 00:09:36,050
action so we have a functional

00:09:33,830 --> 00:09:37,820
functional area for users consumers or

00:09:36,050 --> 00:09:40,940
functional area for merchants we don't

00:09:37,820 --> 00:09:43,100
have one big heterogeneous network per

00:09:40,940 --> 00:09:44,870
se we have individual networks that we

00:09:43,100 --> 00:09:47,360
then plumb together and then create

00:09:44,870 --> 00:09:49,520
routes between those networks we have

00:09:47,360 --> 00:09:53,420
different networks for business function

00:09:49,520 --> 00:09:54,980
risk payments account servicing we have

00:09:53,420 --> 00:09:57,050
for system functions we have the event

00:09:54,980 --> 00:09:59,300
network that's active in queue we have

00:09:57,050 --> 00:10:00,920
our services network which is Cupid and

00:09:59,300 --> 00:10:04,220
we have a logging network that right now

00:10:00,920 --> 00:10:07,460
is Tivoli but we're interconnecting all

00:10:04,220 --> 00:10:10,040
those together and then it's cloud from

00:10:07,460 --> 00:10:12,200
them we need to be able to stare up tear

00:10:10,040 --> 00:10:15,590
up stand up and tear down instances

00:10:12,200 --> 00:10:17,600
instantaneously based upon capacity and

00:10:15,590 --> 00:10:19,460
then we want to isolate partitions

00:10:17,600 --> 00:10:22,850
within the broker so what I mean by that

00:10:19,460 --> 00:10:24,860
is that within Cupid you have the idea

00:10:22,850 --> 00:10:26,710
of exchanges and you have direct

00:10:24,860 --> 00:10:29,800
exchanges you have top of the exchanges

00:10:26,710 --> 00:10:31,540
you have fan out exchanges we create

00:10:29,800 --> 00:10:33,370
multiple exchanges for different types

00:10:31,540 --> 00:10:37,000
of functions so i'm going to send out a

00:10:33,370 --> 00:10:38,860
service request I a exchange for service

00:10:37,000 --> 00:10:40,300
requests only if I'm going to create

00:10:38,860 --> 00:10:42,370
responses I create an exchange for

00:10:40,300 --> 00:10:43,840
service responses only and the reason

00:10:42,370 --> 00:10:45,820
why we do that is that way we can scale

00:10:43,840 --> 00:10:48,700
up or scale down based upon the exchange

00:10:45,820 --> 00:10:50,650
type as well as prey rats for individual

00:10:48,700 --> 00:10:53,200
exchanges and that way I can handle

00:10:50,650 --> 00:10:57,820
different capacity influxes for those

00:10:53,200 --> 00:11:00,220
different types of workloads so from an

00:10:57,820 --> 00:11:01,600
architectural perspective instead of

00:11:00,220 --> 00:11:04,690
having individual clients each of the

00:11:01,600 --> 00:11:06,520
consumer is now an amv provider so we

00:11:04,690 --> 00:11:09,100
have a local broker on every single node

00:11:06,520 --> 00:11:11,800
that they talk to locally and then they

00:11:09,100 --> 00:11:13,720
connect to a mid-tier set of brokers

00:11:11,800 --> 00:11:15,520
that allow us to ask you to act as a

00:11:13,720 --> 00:11:17,470
gateway to the next year down below that

00:11:15,520 --> 00:11:19,000
and the next town cheer down below that

00:11:17,470 --> 00:11:22,900
is a set of local brokers on each

00:11:19,000 --> 00:11:24,850
individual box which gives us a lot of

00:11:22,900 --> 00:11:26,620
flexibility we don't have to worry about

00:11:24,850 --> 00:11:28,930
connections anymore because only that

00:11:26,620 --> 00:11:31,330
box is connecting to excel processes and

00:11:28,930 --> 00:11:33,190
that box only then connects to the

00:11:31,330 --> 00:11:35,440
mid-tier so you have a very small number

00:11:33,190 --> 00:11:38,110
of connections even with thousands of

00:11:35,440 --> 00:11:40,030
boxes in your data center each broker

00:11:38,110 --> 00:11:41,890
only has maybe a thousand two thousand

00:11:40,030 --> 00:11:43,750
connections on that instead of that five

00:11:41,890 --> 00:11:46,030
thousand and since we're partitioning

00:11:43,750 --> 00:11:47,740
the worst case scenario we've seen so

00:11:46,030 --> 00:11:50,500
far and our data centers with about

00:11:47,740 --> 00:11:52,720
10,000 servers is 500 connections /

00:11:50,500 --> 00:11:54,310
Broker which is much different than

00:11:52,720 --> 00:11:58,230
we're at before we're getting six to

00:11:54,310 --> 00:11:58,230
seven thousand per broker so

00:11:59,730 --> 00:12:04,800
the other thing that we found was

00:12:01,230 --> 00:12:06,600
interfaces on the old model Federation

00:12:04,800 --> 00:12:08,490
semantics is part of what we were doing

00:12:06,600 --> 00:12:11,160
before because of that routing with the

00:12:08,490 --> 00:12:13,320
the switch in front we're having to

00:12:11,160 --> 00:12:15,269
expose out the routing characteristics

00:12:13,320 --> 00:12:18,630
of the message itself saying bind to

00:12:15,269 --> 00:12:20,519
this exchange bind to this route go to

00:12:18,630 --> 00:12:22,740
this broker and we didn't really want to

00:12:20,519 --> 00:12:24,449
do that we wanted the clients to say you

00:12:22,740 --> 00:12:26,100
know just send a message to your local

00:12:24,449 --> 00:12:27,600
instance and the brokers emselves net

00:12:26,100 --> 00:12:29,880
that network of brokers will figure out

00:12:27,600 --> 00:12:31,529
where to send it so we wanted to strip

00:12:29,880 --> 00:12:33,149
out the whole rounding semantics from

00:12:31,529 --> 00:12:34,800
the class I have to be aware of it they

00:12:33,149 --> 00:12:36,570
have to say I'm going to send it to this

00:12:34,800 --> 00:12:38,310
queue and they don't need to know which

00:12:36,570 --> 00:12:40,170
exchange is bound to what destination is

00:12:38,310 --> 00:12:41,910
bound to or anything else and then the

00:12:40,170 --> 00:12:43,560
brokers and the links in the between the

00:12:41,910 --> 00:12:46,889
brokers will actually then handle the

00:12:43,560 --> 00:12:47,910
routing between everything else the

00:12:46,889 --> 00:12:50,760
other thing is we externalize the

00:12:47,910 --> 00:12:53,430
addressing we put the addressing

00:12:50,760 --> 00:12:54,959
actually into zookeeper we pull the

00:12:53,430 --> 00:12:56,459
addresses out a zookeeper then inject

00:12:54,959 --> 00:12:58,529
them into the applications and then we

00:12:56,459 --> 00:13:00,089
start up the brokers the brokers are

00:12:58,529 --> 00:13:01,829
just basic containers and when they

00:13:00,089 --> 00:13:04,260
start up they pull their configuration

00:13:01,829 --> 00:13:07,319
from zookeeper as well the other thing

00:13:04,260 --> 00:13:10,170
is use pure npp and JMS wherever

00:13:07,319 --> 00:13:12,269
possible we done did some interesting

00:13:10,170 --> 00:13:14,579
problems we're trying to spring specific

00:13:12,269 --> 00:13:17,040
semantics and it caused lots of things

00:13:14,579 --> 00:13:18,300
to break and not be interoperable we

00:13:17,040 --> 00:13:21,240
also ran some very interesting things

00:13:18,300 --> 00:13:23,160
when we tried to use a and QP specific

00:13:21,240 --> 00:13:24,779
semantics based upon the brokers there's

00:13:23,160 --> 00:13:26,850
actually differences between the Java

00:13:24,779 --> 00:13:29,040
implementation of Cupid and the c++

00:13:26,850 --> 00:13:31,610
implementation of Cupid and we got ding

00:13:29,040 --> 00:13:31,610
there a couple times

00:13:33,560 --> 00:13:40,829
you were able to use a JMS in the

00:13:38,070 --> 00:13:43,889
clients but not spring genus correctness

00:13:40,829 --> 00:13:47,160
and they're just really just because

00:13:43,889 --> 00:13:49,980
spring expects certain behaviors in the

00:13:47,160 --> 00:13:51,360
JMS implementation and not all JMS

00:13:49,980 --> 00:13:52,949
providers actually complimented the same

00:13:51,360 --> 00:13:55,620
way they actually basers off of that

00:13:52,949 --> 00:13:57,360
their implementation off of activemq and

00:13:55,620 --> 00:13:59,490
they make a couple activemq assumptions

00:13:57,360 --> 00:14:02,639
in theirs and their spring day ms

00:13:59,490 --> 00:14:08,100
clients which kind of fall down onto

00:14:02,639 --> 00:14:09,899
general JMS base implementation so the

00:14:08,100 --> 00:14:11,010
other thing is when we looked at the

00:14:09,899 --> 00:14:13,889
interface and what we're trying to build

00:14:11,010 --> 00:14:16,199
it the layers that we talked about

00:14:13,889 --> 00:14:18,180
around amqp and then jca sitting on top

00:14:16,199 --> 00:14:20,490
of that and JMS edina thava gave us a

00:14:18,180 --> 00:14:21,870
lot of isolation it allowed us to make

00:14:20,490 --> 00:14:24,300
decisions based how we want to interact

00:14:21,870 --> 00:14:26,519
so from most clients they interact at

00:14:24,300 --> 00:14:27,839
the gym at the JMS level they don't

00:14:26,519 --> 00:14:29,940
interact with anything else or they

00:14:27,839 --> 00:14:31,350
interact with a spring client or they

00:14:29,940 --> 00:14:33,420
interact with camel or whatever

00:14:31,350 --> 00:14:34,440
component they want to use they don't

00:14:33,420 --> 00:14:36,990
know they're actually doing any cooking

00:14:34,440 --> 00:14:38,190
on the covers we do have a couple java

00:14:36,990 --> 00:14:42,360
based applications they're actually

00:14:38,190 --> 00:14:43,290
coding directly to the amt pap is under

00:14:42,360 --> 00:14:45,360
doing that because they wanted these

00:14:43,290 --> 00:14:48,630
things that are very very specific but

00:14:45,360 --> 00:14:50,220
those so very isolate on the security

00:14:48,630 --> 00:14:52,860
side we have our own proprietary

00:14:50,220 --> 00:14:54,720
security mechanisms so we're not using

00:14:52,860 --> 00:14:56,940
the default out-of-the-box capabilities

00:14:54,720 --> 00:14:58,800
that come with with Cupid we're using

00:14:56,940 --> 00:15:01,170
our own implementation which created

00:14:58,800 --> 00:15:02,550
sazzle components that we plug in to

00:15:01,170 --> 00:15:05,310
that infrastructure and then we use

00:15:02,550 --> 00:15:06,510
those under the covers so and the

00:15:05,310 --> 00:15:09,380
clients would need to know that and the

00:15:06,510 --> 00:15:12,380
broker doesn't know that either

00:15:09,380 --> 00:15:12,380
questions

00:15:13,900 --> 00:15:18,830
so this is actually what we ended up

00:15:16,190 --> 00:15:21,380
creating we have a bunch of local

00:15:18,830 --> 00:15:23,570
instances that then connect on the

00:15:21,380 --> 00:15:25,520
gateways those local instances are also

00:15:23,570 --> 00:15:27,970
connected out to a set of infrastructure

00:15:25,520 --> 00:15:30,980
nodes that do logging and Fantine

00:15:27,970 --> 00:15:32,240
statistic gathering they listen for

00:15:30,980 --> 00:15:34,490
fraud events and some other things that

00:15:32,240 --> 00:15:37,010
are going on the gateways then connect

00:15:34,490 --> 00:15:38,600
down to remote destinations and in

00:15:37,010 --> 00:15:41,210
reality every remote destination is a

00:15:38,600 --> 00:15:43,190
local destination unto themselves and

00:15:41,210 --> 00:15:45,200
then we can expand this app or Rosanna

00:15:43,190 --> 00:15:48,080
Lee and we can interconnect the gateways

00:15:45,200 --> 00:15:49,730
between different functional areas as

00:15:48,080 --> 00:15:52,340
well as different geographic regions so

00:15:49,730 --> 00:15:53,690
think of it we have a and implementation

00:15:52,340 --> 00:15:55,940
in our data center in Salt Lake we have

00:15:53,690 --> 00:15:58,370
an implementation of Phoenix they're

00:15:55,940 --> 00:15:59,750
connected the Gateway level that's it's

00:15:58,370 --> 00:16:02,960
the only places they're connected our

00:15:59,750 --> 00:16:06,080
connection between salt lake city and in

00:16:02,960 --> 00:16:08,270
Dublin is at the gateway level and as we

00:16:06,080 --> 00:16:10,100
can send the message locally to our

00:16:08,270 --> 00:16:11,570
local broker it could end up in double

00:16:10,100 --> 00:16:13,100
they don't even need to know that we're

00:16:11,570 --> 00:16:15,500
routing at the devil we look at the

00:16:13,100 --> 00:16:17,660
message header stands up on the exchange

00:16:15,500 --> 00:16:20,900
this change expects inspects it and it

00:16:17,660 --> 00:16:22,880
forwards it off so every client thinks

00:16:20,900 --> 00:16:27,200
they're writing locally but in essence

00:16:22,880 --> 00:16:29,320
they're writing globally questions on

00:16:27,200 --> 00:16:29,320
them

00:16:31,010 --> 00:16:35,900
so we think about Federation and the Red

00:16:34,610 --> 00:16:37,850
Hat can keep you guys brought this up

00:16:35,900 --> 00:16:40,100
this is a cupid implementation so this

00:16:37,850 --> 00:16:41,870
is this a layer above amqp this is what

00:16:40,100 --> 00:16:43,400
we've built within Cupid out of the

00:16:41,870 --> 00:16:47,480
cover this is am keeping links what's

00:16:43,400 --> 00:16:49,550
going on so we create distinct request

00:16:47,480 --> 00:16:51,440
and response exchanges so everything

00:16:49,550 --> 00:16:53,540
gets separated like I said earlier so we

00:16:51,440 --> 00:16:54,890
want to be able to isolate performance

00:16:53,540 --> 00:16:56,870
characteristics as well as routing

00:16:54,890 --> 00:16:59,450
English between those those ones suppose

00:16:56,870 --> 00:17:01,400
request request we want to go the

00:16:59,450 --> 00:17:02,990
specific destination responses may vary

00:17:01,400 --> 00:17:05,300
based upon failures and everything else

00:17:02,990 --> 00:17:07,220
so we want to have different kinds of

00:17:05,300 --> 00:17:10,550
routing algorithms but depending on how

00:17:07,220 --> 00:17:12,260
they are local and remote destinations

00:17:10,550 --> 00:17:13,459
so what you see here is actually this is

00:17:12,260 --> 00:17:16,699
an actual implementation that I'll run

00:17:13,459 --> 00:17:20,839
through you'll see that the queue set up

00:17:16,699 --> 00:17:23,390
here so you see a direct service gateway

00:17:20,839 --> 00:17:25,970
and then you see exchange for direct

00:17:23,390 --> 00:17:27,650
implementation the implementation is

00:17:25,970 --> 00:17:30,800
where the actual service is consuming

00:17:27,650 --> 00:17:33,350
messages from the service gateways we're

00:17:30,800 --> 00:17:35,360
actually finding those two rather so

00:17:33,350 --> 00:17:38,000
client publish to the service gateway

00:17:35,360 --> 00:17:40,160
the implementation consumes from the

00:17:38,000 --> 00:17:41,960
implementation game so the clients

00:17:40,160 --> 00:17:43,940
always right to service they don't need

00:17:41,960 --> 00:17:45,860
to know implications and only then it

00:17:43,940 --> 00:17:47,480
needs to know the implementation is is

00:17:45,860 --> 00:17:50,540
the implementation and he actually

00:17:47,480 --> 00:17:52,400
creates that gateway that exchange and

00:17:50,540 --> 00:17:53,810
his you himself when he starts up so you

00:17:52,400 --> 00:17:56,900
could strap development Ischl ization

00:17:53,810 --> 00:17:58,970
say i'm a new service but it's a user me

00:17:56,900 --> 00:18:01,730
or lifecycle service when he starts up

00:17:58,970 --> 00:18:03,020
he creates his own cue he creates his

00:18:01,730 --> 00:18:04,940
own gateway and then he fed raised

00:18:03,020 --> 00:18:07,100
himself into the Gateway vector because

00:18:04,940 --> 00:18:09,740
of exchange and then he Federation

00:18:07,100 --> 00:18:11,350
yourself into the network and as soon as

00:18:09,740 --> 00:18:13,669
he does it than anybody

00:18:11,350 --> 00:18:15,289
so there is no configuration of a

00:18:13,669 --> 00:18:19,539
pre-populated out for service

00:18:15,289 --> 00:18:19,539
implementations only the clients

00:18:20,920 --> 00:18:32,560
so okay sorry request cues we use cue

00:18:29,980 --> 00:18:33,970
brats so we want to load balance so we

00:18:32,560 --> 00:18:36,790
want to make sure that we can distribute

00:18:33,970 --> 00:18:39,040
messages across the network so between

00:18:36,790 --> 00:18:41,560
the local and the gateways we have Q

00:18:39,040 --> 00:18:42,640
runs we publish to the local and then it

00:18:41,560 --> 00:18:44,170
will actually load balance out the

00:18:42,640 --> 00:18:46,660
number of messages to the individual

00:18:44,170 --> 00:18:48,580
gateway destinations the implementations

00:18:46,660 --> 00:18:50,200
have Q routes from the gateways to the

00:18:48,580 --> 00:18:52,120
implementations and then they load

00:18:50,200 --> 00:18:53,850
balance on to the next level so you're

00:18:52,120 --> 00:18:56,250
essentially creating this large tree

00:18:53,850 --> 00:18:59,830
that distributes the messages out to

00:18:56,250 --> 00:19:02,350
individual implementations which gives

00:18:59,830 --> 00:19:04,450
us a massive amount of scalability it

00:19:02,350 --> 00:19:06,490
also allows us instead of load balancing

00:19:04,450 --> 00:19:08,530
connections we're actually load

00:19:06,490 --> 00:19:10,300
balancing messages so the faster the

00:19:08,530 --> 00:19:12,970
consumer is the faster they're going to

00:19:10,300 --> 00:19:15,070
pull tool messages so we're not actually

00:19:12,970 --> 00:19:15,820
going to unfairly distribute load which

00:19:15,070 --> 00:19:17,530
is something we had on our

00:19:15,820 --> 00:19:18,990
point-to-point model we would route

00:19:17,530 --> 00:19:22,480
messages to a service implementation

00:19:18,990 --> 00:19:24,460
that was completely underwater but could

00:19:22,480 --> 00:19:26,740
handle another connection and you will

00:19:24,460 --> 00:19:29,710
get really really bad response time and

00:19:26,740 --> 00:19:31,720
now we can say the variability between a

00:19:29,710 --> 00:19:33,610
good response and a bad response is much

00:19:31,720 --> 00:19:36,250
lower now because they're pulling when

00:19:33,610 --> 00:19:40,030
they have capacity instead of pushing

00:19:36,250 --> 00:19:42,130
traffic to them responses use dynamic

00:19:40,030 --> 00:19:43,480
routes and this is something we were

00:19:42,130 --> 00:19:46,870
actually talking about a little bit

00:19:43,480 --> 00:19:51,250
offline we do this to prevent failures

00:19:46,870 --> 00:19:54,880
so when we create a response destination

00:19:51,250 --> 00:19:59,530
save JMS post reply we create one for

00:19:54,880 --> 00:20:01,600
each container not for each session so

00:19:59,530 --> 00:20:04,240
that when we connect to a broker and say

00:20:01,600 --> 00:20:06,430
here's my destination if that broker

00:20:04,240 --> 00:20:08,050
fails and we reconnect to another broker

00:20:06,430 --> 00:20:10,810
we want that destination to still be

00:20:08,050 --> 00:20:12,250
valid if I use the session identifier it

00:20:10,810 --> 00:20:14,680
would no longer be validated get a new

00:20:12,250 --> 00:20:17,020
session because I'm reconnecting by

00:20:14,680 --> 00:20:19,650
using a unique destination for each VM

00:20:17,020 --> 00:20:22,510
or spring contacts as brokers fail

00:20:19,650 --> 00:20:24,190
because we have that dynamic route it

00:20:22,510 --> 00:20:25,510
will actually route the message to

00:20:24,190 --> 00:20:27,670
wherever that client reconnects to it

00:20:25,510 --> 00:20:29,920
and the only messages that we ever lose

00:20:27,670 --> 00:20:31,409
or when a message is in memory on a

00:20:29,920 --> 00:20:33,759
broker during a failure

00:20:31,409 --> 00:20:35,950
we get around a persistence if we really

00:20:33,759 --> 00:20:37,210
want to but everything's out in potent

00:20:35,950 --> 00:20:39,849
so we really worried about that so much

00:20:37,210 --> 00:20:41,769
but it gives its ability to essentially

00:20:39,849 --> 00:20:43,899
we can take brokers out of service and

00:20:41,769 --> 00:20:46,149
not really impact anybody at that point

00:20:43,899 --> 00:20:49,929
in time so it's kind of self annealing

00:20:46,149 --> 00:20:51,669
around the architecture and then

00:20:49,929 --> 00:20:52,929
responses use unique binding addresses

00:20:51,669 --> 00:20:55,059
for routing that's kind of what I talked

00:20:52,929 --> 00:20:58,570
about so the routing key the by the

00:20:55,059 --> 00:21:03,389
binding key for each response q is

00:20:58,570 --> 00:21:03,389
unique to that vm when I spring contacts

00:21:04,049 --> 00:21:07,960
so ox you walk through an example and

00:21:06,549 --> 00:21:09,580
I'll show you the code this is a small

00:21:07,960 --> 00:21:11,649
subset of what our system actually looks

00:21:09,580 --> 00:21:14,109
like granel paypal stuff is stripped out

00:21:11,649 --> 00:21:15,700
of it but you can actually see little

00:21:14,109 --> 00:21:17,529
balancing the messages and the routing

00:21:15,700 --> 00:21:19,779
and how everything goes down so we'll

00:21:17,529 --> 00:21:21,009
start by creating the whole network and

00:21:19,779 --> 00:21:22,809
this is essentially what we do a

00:21:21,009 --> 00:21:25,629
bootstrap free service for each client

00:21:22,809 --> 00:21:27,039
and then we'll publish messages to the

00:21:25,629 --> 00:21:29,590
Gateway and then we'll load balance

00:21:27,039 --> 00:21:31,179
between the two destinations and then

00:21:29,590 --> 00:21:32,710
i'll publish to one of the destinations

00:21:31,179 --> 00:21:35,729
and it'll load balance between himself

00:21:32,710 --> 00:21:35,729
in the other destination

00:21:41,050 --> 00:21:48,790
maybe I'll start with the the basic

00:21:45,980 --> 00:21:48,790
script

00:21:56,330 --> 00:21:59,620
that's not good

00:22:03,619 --> 00:22:07,279
is it the curse of

00:22:12,020 --> 00:22:19,930
sorry I'll do it on the sideline

00:22:21,940 --> 00:22:24,000
you

00:22:36,999 --> 00:22:40,779
so this is that this is our source code

00:22:39,249 --> 00:22:43,719
so if you walk through this we tear it

00:22:40,779 --> 00:22:46,989
we create couple da directors restore

00:22:43,719 --> 00:22:50,799
process data we create a couple cupid

00:22:46,989 --> 00:22:54,579
Damon's they're out there 53 5673 which

00:22:50,799 --> 00:22:56,889
is the gateway 5674 5675 which for the

00:22:54,579 --> 00:22:58,809
remote instances from there we actually

00:22:56,889 --> 00:23:02,499
though could then create links between

00:22:58,809 --> 00:23:06,309
all these brokers and then from the

00:23:02,499 --> 00:23:08,019
links we would create the queues so like

00:23:06,309 --> 00:23:09,369
I said we create a local q that they

00:23:08,019 --> 00:23:11,349
would write to that they don't even know

00:23:09,369 --> 00:23:12,549
it's remote and then we create an

00:23:11,349 --> 00:23:14,679
implementation queue which is actually

00:23:12,549 --> 00:23:17,499
remote to that broker and every broker

00:23:14,679 --> 00:23:20,079
gets a copy of those and then we create

00:23:17,499 --> 00:23:21,279
a set of exchanges I no scheme exchanges

00:23:20,079 --> 00:23:23,349
are there's a gateway exchange and

00:23:21,279 --> 00:23:25,209
there's implementation exchange clients

00:23:23,349 --> 00:23:26,499
right to the gateway exchange service is

00:23:25,209 --> 00:23:30,789
consumed from the implementation

00:23:26,499 --> 00:23:34,659
exchange and then from there we bind the

00:23:30,789 --> 00:23:36,159
queues to the to the exchanges and then

00:23:34,659 --> 00:23:38,049
we create the rats between them so we've

00:23:36,159 --> 00:23:39,249
heard a queue route between each one of

00:23:38,049 --> 00:23:41,589
these I wasn't up load balancing

00:23:39,249 --> 00:23:45,309
connections and then we throw off a

00:23:41,589 --> 00:23:48,059
bunch of messages each one questions on

00:23:45,309 --> 00:23:48,059
this have I seen this before

00:24:00,620 --> 00:24:04,360
it takes a little time stand up

00:24:07,010 --> 00:24:09,670
close

00:25:04,979 --> 00:25:12,609
certainly you guys could make this

00:25:06,580 --> 00:25:17,649
faster by the way are you want to be 18

00:25:12,609 --> 00:25:21,099
yeah a lot of that stuff bastard now you

00:25:17,649 --> 00:25:24,159
tell me I believe it on the fact i'm

00:25:21,099 --> 00:25:27,519
using yours he said it's some network no

00:25:24,159 --> 00:25:31,269
that's all local ok so we're start

00:25:27,519 --> 00:25:36,389
spouting some messages off ah son of

00:25:31,269 --> 00:25:36,389
them what that

00:25:38,900 --> 00:25:42,010
that's gonna suck

00:25:56,640 --> 00:25:59,900
figures let me uh

00:26:12,650 --> 00:26:16,420
I thought Michael run through this hole

00:26:14,180 --> 00:26:16,420
again

00:26:31,820 --> 00:26:34,540
ok

00:26:35,620 --> 00:26:39,490
so I just threw out thousands out to the

00:26:38,020 --> 00:26:42,730
brokers and now you're going to see that

00:26:39,490 --> 00:26:47,650
on 5673 which is the gateway then it got

00:26:42,730 --> 00:26:51,610
a thousand messages and then on 5674

00:26:47,650 --> 00:26:53,080
with one got 359 and one at 641 so we

00:26:51,610 --> 00:26:56,230
load balance the messages between those

00:26:53,080 --> 00:26:57,730
two individual instances and those are

00:26:56,230 --> 00:26:59,110
the reason that there's a little bit

00:26:57,730 --> 00:27:01,150
difference the processes are running off

00:26:59,110 --> 00:27:04,960
of them one is a faster process versus

00:27:01,150 --> 00:27:06,610
the other one so and then we're going to

00:27:04,960 --> 00:27:09,100
rewrite on the other way so I'm going to

00:27:06,610 --> 00:27:11,050
publish to 5674 and it's going to wrap

00:27:09,100 --> 00:27:15,120
all the way back up to 56 73 then reload

00:27:11,050 --> 00:27:15,120
balance back out to beck to themselves

00:27:22,370 --> 00:27:25,330
so we published

00:27:25,670 --> 00:27:34,550
six hundred messages on this one one got

00:27:27,770 --> 00:27:36,590
241 any other one at 359 so this basic

00:27:34,550 --> 00:27:39,290
pattern is what we build our whole

00:27:36,590 --> 00:27:41,300
network on so you can actually go up to

00:27:39,290 --> 00:27:42,980
thousands and thousands of individual

00:27:41,300 --> 00:27:45,830
brokers or if you're on the proton side

00:27:42,980 --> 00:27:47,780
of things individual just a MTP

00:27:45,830 --> 00:27:55,250
listeners and scale the mouth be of

00:27:47,780 --> 00:27:58,040
building a set of routes like this you

00:27:55,250 --> 00:28:06,820
don't need to drink so that's our basic

00:27:58,040 --> 00:28:06,820
architecture a paypal to play

00:28:09,090 --> 00:28:12,940
so the other things that we noticed in

00:28:11,500 --> 00:28:16,300
their initial implementation here is

00:28:12,940 --> 00:28:18,610
that message size is important pay a lot

00:28:16,300 --> 00:28:20,260
of attention the message size don't send

00:28:18,610 --> 00:28:22,240
any more data you really have to because

00:28:20,260 --> 00:28:24,780
it actually drastically affects latency

00:28:22,240 --> 00:28:27,370
and we're very very latency sensitive

00:28:24,780 --> 00:28:29,200
the other thing is keep if you can't

00:28:27,370 --> 00:28:31,270
keep your messages lower than your frame

00:28:29,200 --> 00:28:33,460
size on the network as much as we can

00:28:31,270 --> 00:28:35,890
say that they are full duplex and

00:28:33,460 --> 00:28:38,130
multiplex connections and they can

00:28:35,890 --> 00:28:40,570
interweave the messages between them on

00:28:38,130 --> 00:28:42,730
latency sentences of application that's

00:28:40,570 --> 00:28:44,050
not a good thing you really want to fit

00:28:42,730 --> 00:28:46,690
every message into a single Network

00:28:44,050 --> 00:28:48,940
packing to move on interleaving causes

00:28:46,690 --> 00:28:52,420
pausing believe it or not so it's not

00:28:48,940 --> 00:28:55,570
going to be as fast avoid default reply

00:28:52,420 --> 00:28:58,300
to one implementation implementations we

00:28:55,570 --> 00:29:00,340
don't use the JMS reply to so that most

00:28:58,300 --> 00:29:04,810
people normally do and the reason for

00:29:00,340 --> 00:29:05,980
that is it's really really slow and for

00:29:04,810 --> 00:29:09,160
us really loose those two milliseconds

00:29:05,980 --> 00:29:11,830
so every time you create a reply to

00:29:09,160 --> 00:29:13,240
destination in JMS it's about two

00:29:11,830 --> 00:29:15,010
milliseconds sometimes even longer

00:29:13,240 --> 00:29:17,710
depending if there's SSL depending on

00:29:15,010 --> 00:29:20,500
how the configuration setup we actually

00:29:17,710 --> 00:29:22,420
create a reply to for every JVM and then

00:29:20,500 --> 00:29:23,830
we used futures to dispatch messages

00:29:22,420 --> 00:29:27,820
back to the individual threads that are

00:29:23,830 --> 00:29:28,960
actually waiting for the response versus

00:29:27,820 --> 00:29:30,460
letting it happen on the broker and

00:29:28,960 --> 00:29:33,040
creating thousands of individual reply

00:29:30,460 --> 00:29:34,630
to destinations it's a much more

00:29:33,040 --> 00:29:37,780
efficient from a connections perspective

00:29:34,630 --> 00:29:40,300
but also from a performance perspective

00:29:37,780 --> 00:29:44,800
we avoid that that latency had to create

00:29:40,300 --> 00:29:46,720
that reply to so and the other thing is

00:29:44,800 --> 00:29:49,180
so pull configurations versus

00:29:46,720 --> 00:29:50,800
pre-packaging we started out by

00:29:49,180 --> 00:29:52,030
pre-packaging everything and we ended up

00:29:50,800 --> 00:29:53,890
with pulling everything in and

00:29:52,030 --> 00:29:57,270
initializing a bootstrapping when the

00:29:53,890 --> 00:29:57,270
brokers in the JVM start up

00:30:01,870 --> 00:30:07,340
so this is how we actually manage our

00:30:04,820 --> 00:30:08,779
config like I said we started out with a

00:30:07,340 --> 00:30:11,179
configuration where we put everything

00:30:08,779 --> 00:30:13,490
into a properties file bundle that up

00:30:11,179 --> 00:30:15,980
into our rpms we push the RPMs out with

00:30:13,490 --> 00:30:19,279
puppet and distributor that's all 10,000

00:30:15,980 --> 00:30:20,690
servers and anytime we want to change it

00:30:19,279 --> 00:30:23,510
was pushing that all out to come out

00:30:20,690 --> 00:30:25,010
service what we do now is we have a

00:30:23,510 --> 00:30:27,230
basic config that we push out the

00:30:25,010 --> 00:30:28,460
zookeeper and then each of the

00:30:27,230 --> 00:30:30,769
individual Cupid instance this is

00:30:28,460 --> 00:30:32,779
wrapped with a Python script that looks

00:30:30,769 --> 00:30:35,480
for events coming from zookeepers as odd

00:30:32,779 --> 00:30:36,889
that configures change update Cupid and

00:30:35,480 --> 00:30:39,470
apply that out there and we use that for

00:30:36,889 --> 00:30:41,600
apples we use that for creating queues

00:30:39,470 --> 00:30:43,039
and direct links and everything else and

00:30:41,600 --> 00:30:45,860
then that way the only thing that we're

00:30:43,039 --> 00:30:47,480
pushing out to each server is a basic

00:30:45,860 --> 00:30:50,210
bootstrap that says here's where Zoo

00:30:47,480 --> 00:30:51,740
keepers at go start yourself up versus

00:30:50,210 --> 00:30:53,179
it happen to be aware of the whole

00:30:51,740 --> 00:30:55,480
network of brokers and a changes that we

00:30:53,179 --> 00:30:55,480
made to

00:30:58,370 --> 00:31:07,620
the instances are holding the Cuban

00:31:05,250 --> 00:31:09,390
assistant themselves aren't there if you

00:31:07,620 --> 00:31:12,929
look at what you keep it positive keeper

00:31:09,390 --> 00:31:15,600
essentially says there is a event

00:31:12,929 --> 00:31:17,700
listener within zookeeper that will get

00:31:15,600 --> 00:31:20,580
a notification from zookeeper anytime a

00:31:17,700 --> 00:31:22,050
particular note of the tree is of

00:31:20,580 --> 00:31:24,150
interest so if you say that I'm

00:31:22,050 --> 00:31:26,700
interested in the services config it

00:31:24,150 --> 00:31:28,950
will notify its listener that they

00:31:26,700 --> 00:31:30,210
change has happened and then you get the

00:31:28,950 --> 00:31:32,550
change that's happened as a result of

00:31:30,210 --> 00:31:35,429
that as part of the basic configuration

00:31:32,550 --> 00:31:37,050
para dylon zookeeper so what we've done

00:31:35,429 --> 00:31:38,640
is you're taking zookeeper and keep it

00:31:37,050 --> 00:31:40,350
and said okay we start or keep it

00:31:38,640 --> 00:31:41,550
instance up by the way we also have a

00:31:40,350 --> 00:31:43,020
zookeeper listener and it's just a

00:31:41,550 --> 00:31:45,059
Python script this is anytime there's a

00:31:43,020 --> 00:31:47,429
change that relates to these Cupid

00:31:45,059 --> 00:31:49,460
instances pull those changes down and

00:31:47,429 --> 00:31:52,230
apply them to keep it so that we're not

00:31:49,460 --> 00:31:53,970
distributing out all these changes we

00:31:52,230 --> 00:31:56,040
can update them one central location and

00:31:53,970 --> 00:31:59,240
then they feder a top with relative

00:31:56,040 --> 00:32:01,260
consistency across the whole network

00:31:59,240 --> 00:32:04,910
zookeepers telling all the keeping notes

00:32:01,260 --> 00:32:07,429
it will do that so it's a pretty common

00:32:04,910 --> 00:32:10,050
implementation I think we do that

00:32:07,429 --> 00:32:11,280
linkedin does that as well there's a

00:32:10,050 --> 00:32:14,240
couple other folks that are out there

00:32:11,280 --> 00:32:14,240
doing some kind of configuration

00:32:15,559 --> 00:32:19,890
monitoring putting some in production

00:32:18,480 --> 00:32:22,580
that actually moves a lot of money we

00:32:19,890 --> 00:32:24,690
have to pay attention to what's going on

00:32:22,580 --> 00:32:26,940
so the keeping management frameworks

00:32:24,690 --> 00:32:29,820
actually pretty cool each object in the

00:32:26,940 --> 00:32:31,920
broker is manageable and it publishes

00:32:29,820 --> 00:32:33,270
events and statistics about itself the

00:32:31,920 --> 00:32:36,030
question is is figuring out what they

00:32:33,270 --> 00:32:37,170
are and what you can subscribe to but

00:32:36,030 --> 00:32:38,250
once you figure that out it's actually

00:32:37,170 --> 00:32:39,570
pretty easy then you can actually

00:32:38,250 --> 00:32:42,179
subscribe to things and rear out the

00:32:39,570 --> 00:32:44,340
events to somewhere else so one of our

00:32:42,179 --> 00:32:46,559
implementations we take advanced and

00:32:44,340 --> 00:32:51,030
we're at them over our tiburon in the

00:32:46,559 --> 00:32:52,440
network to ebays not so they're curious

00:32:51,030 --> 00:32:54,390
about certain kind of events and so we

00:32:52,440 --> 00:32:57,150
have a cow logging infrastructure that

00:32:54,390 --> 00:32:58,320
says oh this kind of event brought this

00:32:57,150 --> 00:33:00,210
from cupid to

00:32:58,320 --> 00:33:03,330
to leave sorry not to leave a tip toe

00:33:00,210 --> 00:33:05,250
and send it on we can describe

00:33:03,330 --> 00:33:07,440
interested in vets so you can listen for

00:33:05,250 --> 00:33:11,009
particular things within the broker

00:33:07,440 --> 00:33:13,590
itself and then like I said we can

00:33:11,009 --> 00:33:16,440
dispatch out to Vanessa new traps SNMP

00:33:13,590 --> 00:33:18,750
traps Nadia's to the NOC internal

00:33:16,440 --> 00:33:22,679
logging cetera and I'll show you what

00:33:18,750 --> 00:33:24,509
that looks like as well so what the can

00:33:22,679 --> 00:33:26,820
you monitor here's the things that we

00:33:24,509 --> 00:33:29,970
monitor there might be more maybe the

00:33:26,820 --> 00:33:32,519
Red Hat guys can tell us but agents

00:33:29,970 --> 00:33:34,590
bindings so any time somebody buys with

00:33:32,519 --> 00:33:38,009
your topic or bind so particular

00:33:34,590 --> 00:33:39,149
exchange connections so somebody creates

00:33:38,009 --> 00:33:40,830
a connection you can listen for any

00:33:39,149 --> 00:33:43,289
connection as a tow your client

00:33:40,830 --> 00:33:46,019
connected to my network concerned about

00:33:43,289 --> 00:33:49,230
that any modifications to exchanges or

00:33:46,019 --> 00:33:52,799
statistics on that links between brokers

00:33:49,230 --> 00:33:54,210
or from clients / brokers queues which

00:33:52,799 --> 00:33:57,120
is the most common thing that people

00:33:54,210 --> 00:33:59,940
look at subscriptions the systems in the

00:33:57,120 --> 00:34:01,830
D host through the human framework we

00:33:59,940 --> 00:34:04,019
actually create a complete map of our

00:34:01,830 --> 00:34:05,370
whole broker infrastructure and we only

00:34:04,019 --> 00:34:07,080
do that by discovering one initial

00:34:05,370 --> 00:34:09,210
broker from that broker we describe the

00:34:07,080 --> 00:34:10,889
new links the next broker like some ex

00:34:09,210 --> 00:34:13,349
broker and Lisa next broken links next

00:34:10,889 --> 00:34:14,760
broker we can actually put a whole grid

00:34:13,349 --> 00:34:16,530
of saying here's everything we have out

00:34:14,760 --> 00:34:18,389
there and then we can actually then go

00:34:16,530 --> 00:34:20,129
to the came out framework and say here's

00:34:18,389 --> 00:34:21,720
what's performing in each one and then

00:34:20,129 --> 00:34:23,790
we've set it to auto discovery through

00:34:21,720 --> 00:34:27,089
all the links through PMF it's actually

00:34:23,790 --> 00:34:31,139
pretty cool but generally we listen for

00:34:27,089 --> 00:34:33,839
mainly just events new objects and

00:34:31,139 --> 00:34:35,639
statistics you know two thresholds how

00:34:33,839 --> 00:34:38,190
many n QT queues was the latency for a

00:34:35,639 --> 00:34:41,069
particular message type those kinds of

00:34:38,190 --> 00:34:42,780
things until realistic for and we do

00:34:41,069 --> 00:34:48,260
thou all that with a basic set of Python

00:34:42,780 --> 00:34:48,260
scripts and i will show you those

00:34:48,300 --> 00:34:52,220
if my systems up and running again

00:35:03,430 --> 00:35:12,350
so I'll show you a very exemple very

00:35:06,200 --> 00:35:14,930
simple monitoring script so so here's a

00:35:12,350 --> 00:35:21,110
basic monitoring script that we use I'll

00:35:14,930 --> 00:35:23,540
kind of go through a bit this is

00:35:21,110 --> 00:35:25,610
actually taken directly from the cupid

00:35:23,540 --> 00:35:28,460
site at least initial part of it is and

00:35:25,610 --> 00:35:31,070
then we've extended that basic script to

00:35:28,460 --> 00:35:32,390
do some other things but essentially

00:35:31,070 --> 00:35:34,280
within the script you register for

00:35:32,390 --> 00:35:36,620
interest into into a particular broker

00:35:34,280 --> 00:35:38,270
and then you look for different objects

00:35:36,620 --> 00:35:40,220
and those different objects come in the

00:35:38,270 --> 00:35:42,080
form of a record you take that record

00:35:40,220 --> 00:35:44,000
and then we've we actually fire that off

00:35:42,080 --> 00:35:45,590
to an event handler that is based on a

00:35:44,000 --> 00:35:47,150
configuration and then that

00:35:45,590 --> 00:35:50,120
configuration says I'm interested in

00:35:47,150 --> 00:35:52,670
this event or not this event within the

00:35:50,120 --> 00:35:54,980
event handler this is for cues here's

00:35:52,670 --> 00:35:57,500
all the different attributes so we look

00:35:54,980 --> 00:36:00,050
for so we say we handle an event and if

00:35:57,500 --> 00:36:01,820
that object is in the configuration it

00:36:00,050 --> 00:36:04,570
will fire up things for total message

00:36:01,820 --> 00:36:06,290
DQ's this was totally in queues whatever

00:36:04,570 --> 00:36:08,180
attribute that we're interested in

00:36:06,290 --> 00:36:09,980
changing we can actually inspect that

00:36:08,180 --> 00:36:12,680
events and take action on it whether

00:36:09,980 --> 00:36:14,930
it's an SNMP trap or whether it's maybe

00:36:12,680 --> 00:36:17,840
restarting a broker or actually adding

00:36:14,930 --> 00:36:19,670
capacity based upon the performance of

00:36:17,840 --> 00:36:22,450
the mutual broker or the events that are

00:36:19,670 --> 00:36:22,450
coming across than that

00:36:26,750 --> 00:36:31,450
and our simple config I'll show you

00:36:44,049 --> 00:36:51,940
we use a really simple config file to do

00:36:46,790 --> 00:36:54,799
this so each section is a different

00:36:51,940 --> 00:36:56,809
object in the broker so we have a

00:36:54,799 --> 00:36:59,480
transaction test which is an object in

00:36:56,809 --> 00:37:01,579
the broker it's actually a cube and then

00:36:59,480 --> 00:37:04,549
we're interested individual message in

00:37:01,579 --> 00:37:06,380
cuse there's a threshold that's set

00:37:04,549 --> 00:37:08,540
where though and send it out to an SMP

00:37:06,380 --> 00:37:11,329
trap to our centralized logging trap or

00:37:08,540 --> 00:37:12,740
less it out to a log file and you can do

00:37:11,329 --> 00:37:17,440
that for any kind of object how we've

00:37:12,740 --> 00:37:17,440
set this up and then to run it

00:37:26,200 --> 00:37:29,070
I want to do

00:37:45,200 --> 00:37:48,990
so you can see as it went through

00:37:47,670 --> 00:37:51,240
there's all the things I don't really

00:37:48,990 --> 00:37:53,220
care about that published out and then

00:37:51,240 --> 00:37:54,510
the one thing on transaction test I'm

00:37:53,220 --> 00:37:56,780
actually monitoring that one and it's

00:37:54,510 --> 00:37:59,310
looking for individual events on that so

00:37:56,780 --> 00:38:01,020
just see a basic Python script we can

00:37:59,310 --> 00:38:03,210
inspect anything within the broker and

00:38:01,020 --> 00:38:04,890
then take action upon anywhere in our

00:38:03,210 --> 00:38:06,270
network and then using the same

00:38:04,890 --> 00:38:08,010
monitoring the same scripting

00:38:06,270 --> 00:38:09,840
capabilities we actually discover our

00:38:08,010 --> 00:38:11,850
whole network through just inspecting

00:38:09,840 --> 00:38:18,140
one broker and then going out the

00:38:11,850 --> 00:38:18,140
individual ones questions

00:38:22,859 --> 00:38:26,309
as required

00:38:29,500 --> 00:38:36,540
so performance figure the activemq got

00:38:33,280 --> 00:38:40,480
put a performance on why should as well

00:38:36,540 --> 00:38:43,120
so these are our round trip times these

00:38:40,480 --> 00:38:47,770
are actually live numbers for us the top

00:38:43,120 --> 00:38:49,540
graph shows just basic on the wire no

00:38:47,770 --> 00:38:51,210
paypal specific stuff on it what we

00:38:49,540 --> 00:38:54,640
measure on our keep it infrastructure

00:38:51,210 --> 00:38:58,350
and each the top line is bites so number

00:38:54,640 --> 00:39:01,450
of bytes so over our one gig network

00:38:58,350 --> 00:39:04,900
we're looking at for a1k message about

00:39:01,450 --> 00:39:06,100
point six six milliseconds for multi

00:39:04,900 --> 00:39:08,530
nodes which means i'm hopping between

00:39:06,100 --> 00:39:10,540
multiple machines we're looking at about

00:39:08,530 --> 00:39:13,020
point nine eight milliseconds round trip

00:39:10,540 --> 00:39:15,820
so this is from the client to the broker

00:39:13,020 --> 00:39:18,820
to the service implementation back to

00:39:15,820 --> 00:39:21,670
the broker back to the client and then

00:39:18,820 --> 00:39:23,560
over RDMA we're looking about point four

00:39:21,670 --> 00:39:26,440
four little second so we'd already made

00:39:23,560 --> 00:39:28,630
for our back-end payment systems as we

00:39:26,440 --> 00:39:30,970
actually care about and then the the

00:39:28,630 --> 00:39:33,190
second graph is adding our stuff into it

00:39:30,970 --> 00:39:34,510
and our different behavioral use cases

00:39:33,190 --> 00:39:39,070
that we run through so that's

00:39:34,510 --> 00:39:41,470
serialization SSL message signing all

00:39:39,070 --> 00:39:42,760
the other things that we do on top of it

00:39:41,470 --> 00:39:47,160
you'll notice the numbers go up quite a

00:39:42,760 --> 00:39:47,160
bit they're still relatively quick

00:39:52,990 --> 00:40:02,980
look concerned back there no concern so

00:40:01,390 --> 00:40:04,570
how can we use it so we're actually

00:40:02,980 --> 00:40:07,180
using the Inca p stuff in our cloud

00:40:04,570 --> 00:40:09,400
environment so what we're doing is we're

00:40:07,180 --> 00:40:11,920
developing cartridges so with an open

00:40:09,400 --> 00:40:13,390
shift there is a notion of cartridges so

00:40:11,920 --> 00:40:17,380
we have a JBoss cartridge now we're

00:40:13,390 --> 00:40:19,270
developing a a QP Cupid cartridge and

00:40:17,380 --> 00:40:21,280
that Cupid cartridge developers pull in

00:40:19,270 --> 00:40:24,070
when it starts out that pulls itself it

00:40:21,280 --> 00:40:26,890
registers itself in for the network and

00:40:24,070 --> 00:40:28,270
it starts listening within the OpenStack

00:40:26,890 --> 00:40:30,910
infrastructure that we use in production

00:40:28,270 --> 00:40:33,730
we listen for those few events coming in

00:40:30,910 --> 00:40:35,680
and the Layton sees and we say oh for

00:40:33,730 --> 00:40:37,600
this kind of cue we need more capacity

00:40:35,680 --> 00:40:40,810
and it will actually walk and stand up a

00:40:37,600 --> 00:40:42,660
whole nother implementation an instance

00:40:40,810 --> 00:40:45,220
because our queue depth went up too high

00:40:42,660 --> 00:40:47,530
if the latency gets too low we had stand

00:40:45,220 --> 00:40:49,540
up additional service implementations to

00:40:47,530 --> 00:40:52,119
address the latency issues or we send

00:40:49,540 --> 00:40:53,830
off alerts as a result of that so we

00:40:52,119 --> 00:40:56,260
actually build a dynamically scaling

00:40:53,830 --> 00:41:02,140
infrastructure based upon monitoring of

00:40:56,260 --> 00:41:03,430
our amqp infrastructure some possible

00:41:02,140 --> 00:41:07,660
applications that we're looking at right

00:41:03,430 --> 00:41:10,290
now mobile via JavaScript so we're we're

00:41:07,660 --> 00:41:15,190
actually developing mobile interfaces

00:41:10,290 --> 00:41:17,890
right now on no Jas with backbone JS

00:41:15,190 --> 00:41:20,280
connecting into on the clients

00:41:17,890 --> 00:41:22,780
JavaScript application so you have

00:41:20,280 --> 00:41:24,280
javascript in the browser connecting

00:41:22,780 --> 00:41:27,130
back to nodejs over backbone.js

00:41:24,280 --> 00:41:28,840
connecting to a MPP as a messaging

00:41:27,130 --> 00:41:33,130
infrastructure under the covers right

00:41:28,840 --> 00:41:36,250
now we're wrapping the native C++

00:41:33,130 --> 00:41:37,450
libraries with JavaScript using v8 so

00:41:36,250 --> 00:41:40,300
we're exposing everything I would create

00:41:37,450 --> 00:41:42,550
prototypes and that job nodejs interacts

00:41:40,300 --> 00:41:44,320
directly with it we're hoping to get a

00:41:42,550 --> 00:41:47,920
native implementation so we can bet it

00:41:44,320 --> 00:41:50,440
in the browsers payment devices if in

00:41:47,920 --> 00:41:53,140
Vegas at Home Depot you know type your

00:41:50,440 --> 00:41:54,910
car once use paypal right now we're

00:41:53,140 --> 00:41:59,950
using HTTP we're thinking about swapping

00:41:54,910 --> 00:42:01,029
that over the MPP so I think american

00:41:59,950 --> 00:42:07,119
eagle

00:42:01,029 --> 00:42:08,829
fitch Home Depot those complexes what

00:42:07,119 --> 00:42:12,880
we're looking to actually use it some

00:42:08,829 --> 00:42:14,769
more linking npp directly between our

00:42:12,880 --> 00:42:17,559
keep and implementation or active and

00:42:14,769 --> 00:42:18,969
active implementation and that's just a

00:42:17,559 --> 00:42:20,919
timing thing based upon the versions

00:42:18,969 --> 00:42:22,659
that we're on but we want to build a

00:42:20,919 --> 00:42:24,789
heterogenous fabric so we can distribute

00:42:22,659 --> 00:42:27,099
one set of clients out to everybody and

00:42:24,789 --> 00:42:28,179
we can choose based upon quality of

00:42:27,099 --> 00:42:30,189
service and where they need to go to

00:42:28,179 --> 00:42:32,049
which messaging infrastructure they're

00:42:30,189 --> 00:42:34,839
connecting into or whether they're just

00:42:32,049 --> 00:42:36,279
a messaging provider themselves we're

00:42:34,839 --> 00:42:41,049
looking at embedded messaging engines

00:42:36,279 --> 00:42:42,699
into different devices taxi magic etc so

00:42:41,049 --> 00:42:44,619
a lot of the things you guys are seeing

00:42:42,699 --> 00:42:47,589
the startups we're trying to get paypal

00:42:44,619 --> 00:42:49,119
into those things we're thinking ap will

00:42:47,589 --> 00:42:51,219
be late for us to do that to bridge back

00:42:49,119 --> 00:42:53,919
into our network first injuries

00:42:51,219 --> 00:42:57,219
basically you're trying to look your

00:42:53,919 --> 00:43:00,459
your new messaging infrastructure with

00:42:57,219 --> 00:43:02,890
your legacy excited performance issues

00:43:00,459 --> 00:43:06,459
didn't acai how performance issues um so

00:43:02,890 --> 00:43:08,349
maybe I should clarify that one and I'll

00:43:06,459 --> 00:43:10,509
get into the weeds a little bit on

00:43:08,349 --> 00:43:14,679
paypal history we actually have 27

00:43:10,509 --> 00:43:16,929
messaging systems of those 71 is tibco

00:43:14,679 --> 00:43:19,959
that we acquired from you today that we

00:43:16,929 --> 00:43:22,150
use for alerts fanaka we have two

00:43:19,959 --> 00:43:24,609
versions of activemq both of which are

00:43:22,150 --> 00:43:30,489
forked then aren't the same as everybody

00:43:24,609 --> 00:43:32,409
else's don't ask we have three

00:43:30,489 --> 00:43:35,709
proprietary systems that we wrote

00:43:32,409 --> 00:43:37,890
ourselves we're merging all those into

00:43:35,709 --> 00:43:40,119
academic you so activemq is our main

00:43:37,890 --> 00:43:42,429
messaging platform from the bulk of what

00:43:40,119 --> 00:43:45,519
we do we don't use it for our low

00:43:42,429 --> 00:43:48,369
latency request reply payment processing

00:43:45,519 --> 00:43:49,859
type stuff and we don't intend to the

00:43:48,369 --> 00:43:51,909
performance differences between the two

00:43:49,859 --> 00:43:55,479
significant enough that we care about

00:43:51,909 --> 00:43:58,040
that and they're microseconds

00:43:55,479 --> 00:44:00,920
differences they're not huge differences

00:43:58,040 --> 00:44:02,930
but you add that up to our volumes it

00:44:00,920 --> 00:44:04,930
gets significant so we don't have

00:44:02,930 --> 00:44:08,270
performance problems with activemq

00:44:04,930 --> 00:44:10,820
they're like this I mean so I wouldn't

00:44:08,270 --> 00:44:12,680
brand it that way it's really about

00:44:10,820 --> 00:44:14,060
placing our proprietary stuff and

00:44:12,680 --> 00:44:18,500
choosing quality service based upon the

00:44:14,060 --> 00:44:20,630
things that we're trying to do so the

00:44:18,500 --> 00:44:26,150
other thing we're looking at is energy

00:44:20,630 --> 00:44:28,790
wife when you say it's yeah you do kill

00:44:26,150 --> 00:44:31,070
a disease good yeah and we're talking

00:44:28,790 --> 00:44:32,300
the difference like so to go to one side

00:44:31,070 --> 00:44:33,950
I house we're at point six six

00:44:32,300 --> 00:44:37,310
milliseconds activemq that point nine

00:44:33,950 --> 00:44:40,070
eight I mean it's you know 320

00:44:37,310 --> 00:44:42,140
microseconds to most people that's in

00:44:40,070 --> 00:44:43,400
everything consequential when you're

00:44:42,140 --> 00:44:45,200
doing sixty percent of the total

00:44:43,400 --> 00:44:48,140
financial transactions on the internet

00:44:45,200 --> 00:44:54,200
that's a big deal that adds up quite a

00:44:48,140 --> 00:44:56,840
bit so we're split hairs but most people

00:44:54,200 --> 00:44:57,980
never notice the other thing we're

00:44:56,840 --> 00:45:00,110
placing some proprietary service

00:44:57,980 --> 00:45:03,380
framework we proton we wrote our own

00:45:00,110 --> 00:45:04,970
HTTP server on web service network we

00:45:03,380 --> 00:45:08,960
think there's an opportunity to get rid

00:45:04,970 --> 00:45:11,360
of that and replace that which is native

00:45:08,960 --> 00:45:13,070
aim QP interface the other thing is

00:45:11,360 --> 00:45:14,960
replacing to keep a library is with no

00:45:13,070 --> 00:45:17,420
protons specific libraries in the

00:45:14,960 --> 00:45:19,900
messenger messenger API versus the

00:45:17,420 --> 00:45:22,010
out-of-the-box one that came with Cupid

00:45:19,900 --> 00:45:24,890
and the reasonable is just for some

00:45:22,010 --> 00:45:29,290
generic interfaces but I'm not finding

00:45:24,890 --> 00:45:29,290
us directly to how red packages thanks

00:45:29,400 --> 00:45:39,800
sorry guys but that's it so questions I

00:45:43,790 --> 00:45:48,050

YouTube URL: https://www.youtube.com/watch?v=8TsXuuBK_RY


