Title: Getting Hadoop, Hive and HBase up and running in less than 15 minutes
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Mark Grover
ApacheCon NA 2013
Overture and Beginners
Captions: 
	00:00:00,000 --> 00:00:03,929
well welcome everybody thank you for

00:00:01,170 --> 00:00:06,150
coming out so like Justin set my talk is

00:00:03,929 --> 00:00:08,099
on getting Hadoop hive and HP's up and

00:00:06,150 --> 00:00:09,929
running in less than 15 minutes my name

00:00:08,099 --> 00:00:12,780
is Mark Grover I work as a software

00:00:09,929 --> 00:00:15,450
engineer at Cloudera there will be a

00:00:12,780 --> 00:00:17,460
demo in this presentation and the code

00:00:15,450 --> 00:00:20,460
that I used for the demo is available on

00:00:17,460 --> 00:00:22,260
my github profile gadaa.com / mark rover

00:00:20,460 --> 00:00:23,550
so feel free to check it out during the

00:00:22,260 --> 00:00:25,529
presentation after the presentation

00:00:23,550 --> 00:00:28,109
whenever you feel like alright let's get

00:00:25,529 --> 00:00:29,460
started so a little about me I'm a

00:00:28,109 --> 00:00:32,130
contributor to a patch you top-level

00:00:29,460 --> 00:00:33,329
project called big top that's what we

00:00:32,130 --> 00:00:35,399
are going to talk about mostly in this

00:00:33,329 --> 00:00:37,320
presentation I'm also contributed to

00:00:35,399 --> 00:00:38,790
another project called Apache hive there

00:00:37,320 --> 00:00:40,649
will be a little bit of hive stuff in

00:00:38,790 --> 00:00:43,800
this presentation and said before I'm a

00:00:40,649 --> 00:00:46,670
engineer at clutter so this presentation

00:00:43,800 --> 00:00:49,440
is not about me it's about my buddy Bart

00:00:46,670 --> 00:00:51,539
Bart not too long ago heard that big

00:00:49,440 --> 00:00:54,570
data rocks right so he wanted a piece of

00:00:51,539 --> 00:00:56,430
it more than a big chunk so he did some

00:00:54,570 --> 00:00:57,719
google searching try to figure out what

00:00:56,430 --> 00:01:00,390
the whole big data i think is all about

00:00:57,719 --> 00:01:03,510
and he heard of this elephant called

00:01:00,390 --> 00:01:05,309
Apache Hadoop right so turns out an

00:01:03,510 --> 00:01:07,439
Apache Hadoop as a distributed batch

00:01:05,309 --> 00:01:09,770
processing system allows you to run

00:01:07,439 --> 00:01:12,420
large scale data manipulation queries on

00:01:09,770 --> 00:01:14,580
commodity hardware and scale just by

00:01:12,420 --> 00:01:17,700
adding more nodes to the cluster right

00:01:14,580 --> 00:01:20,970
so I'm guessing most of you have heard

00:01:17,700 --> 00:01:24,720
of a do or try to install it felt some

00:01:20,970 --> 00:01:27,360
pain it's not the case the show of hands

00:01:24,720 --> 00:01:29,130
how many of you have heard if I do all

00:01:27,360 --> 00:01:33,329
right how many of you have tried to

00:01:29,130 --> 00:01:35,250
install Hadoop all right okay so for

00:01:33,329 --> 00:01:36,659
that was a sizable chunk who have heard

00:01:35,250 --> 00:01:38,130
if I do but haven't tried to install a

00:01:36,659 --> 00:01:41,670
tube and that's perfectly the audience

00:01:38,130 --> 00:01:43,259
I'm looking for here so Hadoop is

00:01:41,670 --> 00:01:45,540
divided into layers I'm not going to go

00:01:43,259 --> 00:01:47,549
into very deep details but the lower

00:01:45,540 --> 00:01:50,100
layer is the storage lyrics the HDFS

00:01:47,549 --> 00:01:52,350
layer provides robust scalable storage

00:01:50,100 --> 00:01:54,990
that's fault tolerant rakha where you

00:01:52,350 --> 00:01:56,670
store your data on it and then the top

00:01:54,990 --> 00:01:59,219
layer as you see is MapReduce layer

00:01:56,670 --> 00:02:01,350
that's the compute layer that will

00:01:59,219 --> 00:02:04,020
perform the computation on it anyways

00:02:01,350 --> 00:02:05,490
moving on so Bart those two Hadoop's

00:02:04,020 --> 00:02:09,330
website and tries to figure out how he's

00:02:05,490 --> 00:02:12,209
going to install a tube right so he goes

00:02:09,330 --> 00:02:13,200
to apache doric / a dupe and reads

00:02:12,209 --> 00:02:13,740
instructions and these are the

00:02:13,200 --> 00:02:15,750
instructions

00:02:13,740 --> 00:02:17,280
fine he finds that he is to go download

00:02:15,750 --> 00:02:18,690
the Hadoop tarball create a bunch of

00:02:17,280 --> 00:02:20,400
working directories that her dupes going

00:02:18,690 --> 00:02:22,530
to store some temporary data in and read

00:02:20,400 --> 00:02:24,750
it from there it's going to populate

00:02:22,530 --> 00:02:26,430
some config files that will set up the

00:02:24,750 --> 00:02:28,230
cluster in a mode called pseudo

00:02:26,430 --> 00:02:30,420
distributed mode that's the mode where

00:02:28,230 --> 00:02:31,980
when you're installing a dupont only one

00:02:30,420 --> 00:02:34,770
machine that's the more you need to run

00:02:31,980 --> 00:02:37,980
Hadoop on then he has to format the name

00:02:34,770 --> 00:02:40,170
node start the Hadoop demons and pray to

00:02:37,980 --> 00:02:42,780
God that his MapReduce job works right

00:02:40,170 --> 00:02:44,880
so note that these are all logical steps

00:02:42,780 --> 00:02:46,650
it's not like one command that you run

00:02:44,880 --> 00:02:48,000
that will populate your configs you

00:02:46,650 --> 00:02:51,450
actually had to go manually probably

00:02:48,000 --> 00:02:53,730
these configs so he tries this and of

00:02:51,450 --> 00:02:57,360
course he sees this error in Red says

00:02:53,730 --> 00:02:58,680
Java home not set right so turns out

00:02:57,360 --> 00:03:00,180
they forgot to set a whole bunch of

00:02:58,680 --> 00:03:04,110
environment variables that he needs to

00:03:00,180 --> 00:03:05,700
set before he tries to run I do wait so

00:03:04,110 --> 00:03:07,620
he does that but then he gets this

00:03:05,700 --> 00:03:10,080
letter error says permission denied your

00:03:07,620 --> 00:03:11,640
user does not have write permissions to

00:03:10,080 --> 00:03:14,670
the root directory and the permissions

00:03:11,640 --> 00:03:16,940
are this right so turns out that he has

00:03:14,670 --> 00:03:21,050
to create a bunch of directories on HDFS

00:03:16,940 --> 00:03:23,720
before he launches the yarn demons right

00:03:21,050 --> 00:03:26,100
finally he's able to run a MapReduce job

00:03:23,720 --> 00:03:28,410
but this is really frustrating process I

00:03:26,100 --> 00:03:30,450
mean you would hope there is a one-click

00:03:28,410 --> 00:03:32,850
install sort of method or one script

00:03:30,450 --> 00:03:35,820
install method to install your Hadoop in

00:03:32,850 --> 00:03:37,620
HDFS and MapReduce but there isn't so

00:03:35,820 --> 00:03:39,810
what do we see wouldn't it be nice to

00:03:37,620 --> 00:03:45,150
have an easier process to install and

00:03:39,810 --> 00:03:46,800
configure her do so next thing Hadoop is

00:03:45,150 --> 00:03:49,590
like this framework that allows you

00:03:46,800 --> 00:03:53,190
distributed computation hive is another

00:03:49,590 --> 00:03:55,620
top-level project that will provide you

00:03:53,190 --> 00:03:58,530
a sequel front end will compile your

00:03:55,620 --> 00:04:01,770
sequel like queries into MapReduce and

00:03:58,530 --> 00:04:04,740
then return the results right so Bart

00:04:01,770 --> 00:04:08,100
finds that Hadoop and MapReduce on its

00:04:04,740 --> 00:04:10,650
own is not great for him he wants hive

00:04:08,100 --> 00:04:12,840
to be installed on top of a do so he

00:04:10,650 --> 00:04:14,370
goes to the high mailing list right he

00:04:12,840 --> 00:04:15,630
says howdy Hivers can you tell me the

00:04:14,370 --> 00:04:18,180
latest version of Hadoop that I

00:04:15,630 --> 00:04:19,799
installed on my laptop will work with

00:04:18,180 --> 00:04:22,500
the latest version of hive that you have

00:04:19,799 --> 00:04:24,030
available on your website and the hive

00:04:22,500 --> 00:04:26,250
dude replies on the high mailing list

00:04:24,030 --> 00:04:27,690
right he says we only tested the latest

00:04:26,250 --> 00:04:31,350
version of hive with an

00:04:27,690 --> 00:04:32,790
older version of Hadoop but the new

00:04:31,350 --> 00:04:34,320
version should work right now I push it

00:04:32,790 --> 00:04:37,770
in red because every time I hear on

00:04:34,320 --> 00:04:39,630
mailing lists it's a red flag right okay

00:04:37,770 --> 00:04:42,870
so Bart cries it out and tries it oh my

00:04:39,630 --> 00:04:45,570
god another error right so at this point

00:04:42,870 --> 00:04:47,340
it's ready to kill somebody so wouldn't

00:04:45,570 --> 00:04:49,350
be nice if somebody actually spent the

00:04:47,340 --> 00:04:51,360
effort in integration testing all these

00:04:49,350 --> 00:04:53,040
projects right so there are like 10

00:04:51,360 --> 00:04:54,420
different versions of 5 available 10

00:04:53,040 --> 00:04:57,630
different versions of Hadoop available

00:04:54,420 --> 00:04:58,980
has someone even tried to even test you

00:04:57,630 --> 00:05:03,270
know particular virgins to each other

00:04:58,980 --> 00:05:05,310
and see if they work so what do we see

00:05:03,270 --> 00:05:07,020
we see two problems first installing and

00:05:05,310 --> 00:05:09,630
deploying a dupe and related projects is

00:05:07,020 --> 00:05:11,160
complicated second there's a lack of

00:05:09,630 --> 00:05:14,100
integration testing between different

00:05:11,160 --> 00:05:16,320
versions of these projects and apache

00:05:14,100 --> 00:05:18,720
big top is the project that solves it

00:05:16,320 --> 00:05:23,040
these exact two problems so what is big

00:05:18,720 --> 00:05:24,570
top it makes two things it makes you

00:05:23,040 --> 00:05:26,880
guessed it makes installation and

00:05:24,570 --> 00:05:28,350
configuration easier and it makes it

00:05:26,880 --> 00:05:30,090
does integration testing amongst various

00:05:28,350 --> 00:05:33,840
projects so they work very nicely with

00:05:30,090 --> 00:05:36,630
each other so here's about big top Vic

00:05:33,840 --> 00:05:39,060
top is a top level Apache project it

00:05:36,630 --> 00:05:40,830
generates packages for various dis pros

00:05:39,060 --> 00:05:43,800
so you will get Debian on our p.m. at

00:05:40,830 --> 00:05:45,800
artifacts of Hadoop and hive and HBase

00:05:43,800 --> 00:05:49,620
and scoop and flume and you name it and

00:05:45,800 --> 00:05:51,240
these artifacts are integration testing

00:05:49,620 --> 00:05:53,040
each other so you can rely on these

00:05:51,240 --> 00:05:56,880
artifacts install them on your cluster

00:05:53,040 --> 00:05:59,310
and you know seriously rely on them to

00:05:56,880 --> 00:06:00,270
work and not what happened with Bart

00:05:59,310 --> 00:06:01,410
when he's ready to use the latest

00:06:00,270 --> 00:06:04,560
version of high with the latest version

00:06:01,410 --> 00:06:06,390
for Duprey big top also provides

00:06:04,560 --> 00:06:08,460
deployment code so this is code that's

00:06:06,390 --> 00:06:10,260
you know code like puppet code puppet

00:06:08,460 --> 00:06:13,620
recipes that you can use to deploy

00:06:10,260 --> 00:06:15,960
artifacts on your clusters we also

00:06:13,620 --> 00:06:17,490
distribute convenience artifacts so

00:06:15,960 --> 00:06:19,800
these are Hadoop con student that's the

00:06:17,490 --> 00:06:21,840
package name so all you do when you had

00:06:19,800 --> 00:06:23,940
to install a pseudo distributors do

00:06:21,840 --> 00:06:25,470
cluster on your laptop is go ahead and

00:06:23,940 --> 00:06:27,630
install this package and that's your

00:06:25,470 --> 00:06:29,640
very familiar sudo apt-get install

00:06:27,630 --> 00:06:31,970
Hadoop consciously dough and it will

00:06:29,640 --> 00:06:34,440
install Hadoop on your machine right and

00:06:31,970 --> 00:06:35,910
lastly and I mentioned before it does

00:06:34,440 --> 00:06:37,800
integration testing so you have some

00:06:35,910 --> 00:06:39,450
sort of reliability and reliance on the

00:06:37,800 --> 00:06:40,800
package that you're installing it's just

00:06:39,450 --> 00:06:41,040
not something you just directly pulled

00:06:40,800 --> 00:06:43,770
up

00:06:41,040 --> 00:06:45,900
patchy and hope it works right I my

00:06:43,770 --> 00:06:47,880
impression is that the upstream projects

00:06:45,900 --> 00:06:49,800
do a really good job and making sure

00:06:47,880 --> 00:06:52,500
your unit testing what they are actually

00:06:49,800 --> 00:06:54,750
contributing to the community but they

00:06:52,500 --> 00:06:57,180
aren't necessarily particular about

00:06:54,750 --> 00:06:58,740
testing all versions their component

00:06:57,180 --> 00:07:03,120
with the other project data interact

00:06:58,740 --> 00:07:04,380
with right so here's a recap of what had

00:07:03,120 --> 00:07:06,390
to do when he was installing Hadoop

00:07:04,380 --> 00:07:07,710
through tar balls from Apache right

00:07:06,390 --> 00:07:09,990
downloaded tarball quit working

00:07:07,710 --> 00:07:11,730
directories popular configs format the

00:07:09,990 --> 00:07:13,500
name node started oop demon set

00:07:11,730 --> 00:07:16,500
environment variables career directories

00:07:13,500 --> 00:07:18,450
in HDFS and hope it works this is no

00:07:16,500 --> 00:07:20,190
longer required now we have a five step

00:07:18,450 --> 00:07:22,290
process that even a code monkey can run

00:07:20,190 --> 00:07:23,970
Mary note that these are all actual

00:07:22,290 --> 00:07:27,000
instructions so you do sudo apt-get

00:07:23,970 --> 00:07:28,440
install Hadoop constitute Oh sudo

00:07:27,000 --> 00:07:30,690
service your start your name node you

00:07:28,440 --> 00:07:34,110
start your data node and you initialize

00:07:30,690 --> 00:07:37,230
the HDFS and you're all set okay so I'm

00:07:34,110 --> 00:07:39,180
not all talk we'll do a live demo of how

00:07:37,230 --> 00:07:43,260
this works if it doesn't work we'll just

00:07:39,180 --> 00:07:44,910
pretend Adam whatever happened that will

00:07:43,260 --> 00:07:46,830
just corroborate my case that Hadoop is

00:07:44,910 --> 00:07:50,100
in fact harder to install all right so

00:07:46,830 --> 00:07:52,830
what I have here is a vagrant vm this is

00:07:50,100 --> 00:07:56,640
a cleanly M has nothing on it except

00:07:52,830 --> 00:07:59,190
Java ok so job was there but if I die

00:07:56,640 --> 00:08:00,870
pad OOP there's nothing called hit you

00:07:59,190 --> 00:08:03,870
all right and I swear to God I did not

00:08:00,870 --> 00:08:07,140
install it it's not in the path it's not

00:08:03,870 --> 00:08:10,410
there alright so the first thing we're

00:08:07,140 --> 00:08:12,150
going to do all these instructions are

00:08:10,410 --> 00:08:14,550
by the way on the big-top wiki page but

00:08:12,150 --> 00:08:16,830
you went to comes with repositories that

00:08:14,550 --> 00:08:18,120
you can install stuff from we would love

00:08:16,830 --> 00:08:20,430
for you Bunty to come with the

00:08:18,120 --> 00:08:21,750
repository for big top right so all I

00:08:20,430 --> 00:08:24,180
had to do is do sudo apt-get install

00:08:21,750 --> 00:08:26,370
this package from big top but it's not

00:08:24,180 --> 00:08:30,240
there so first of all we need to add a

00:08:26,370 --> 00:08:32,880
repository on my this lucid box to add a

00:08:30,240 --> 00:08:35,550
repository where it can pull big top

00:08:32,880 --> 00:08:37,530
artifacts from but before then you want

00:08:35,550 --> 00:08:38,880
to need to trust that repository so

00:08:37,530 --> 00:08:39,960
we'll go ahead and add a key so I'm

00:08:38,880 --> 00:08:42,720
going to copy paste this command

00:08:39,960 --> 00:08:46,200
essentially getting the big top key from

00:08:42,720 --> 00:08:48,690
archive data Patchett org and adding it

00:08:46,200 --> 00:08:51,000
on the app list so now we trust the big

00:08:48,690 --> 00:08:53,209
top repository and then the second thing

00:08:51,000 --> 00:08:59,619
we're going to do

00:08:53,209 --> 00:09:01,939
is get this file called whether it go

00:08:59,619 --> 00:09:03,259
sources that list that's going to

00:09:01,939 --> 00:09:04,639
contain where are we going to pull the

00:09:03,259 --> 00:09:08,629
artifacts from so let's go look at that

00:09:04,639 --> 00:09:11,240
the sources that list d big top on file

00:09:08,629 --> 00:09:13,819
list so in here we have an amazon s3

00:09:11,240 --> 00:09:16,490
bucket where our big top artifacts

00:09:13,819 --> 00:09:17,569
recite and i just told app that you

00:09:16,490 --> 00:09:21,589
should be pulling the artifacts from

00:09:17,569 --> 00:09:23,240
there anyways that's read-only so just

00:09:21,589 --> 00:09:29,480
because i add a new repository i need to

00:09:23,240 --> 00:09:31,879
do sudo had got update for for you want

00:09:29,480 --> 00:09:34,040
you to remember the repository and after

00:09:31,879 --> 00:09:35,869
that we'll be done with the linux see

00:09:34,040 --> 00:09:39,019
stuff and move on to the real mumbo

00:09:35,869 --> 00:09:41,559
jumbo here anyways so it's adding the

00:09:39,019 --> 00:09:41,559
repository

00:09:50,420 --> 00:09:55,020
ok

00:09:52,080 --> 00:09:56,580
alright sweet so now next step is are

00:09:55,020 --> 00:10:04,470
installing of that package so sudo

00:09:56,580 --> 00:10:06,110
apt-get install Hadoop conf to toe all

00:10:04,470 --> 00:10:08,220
right do I want install this I sure do

00:10:06,110 --> 00:10:09,600
okay this is going to take a while right

00:10:08,220 --> 00:10:11,520
so it needs to install a whole bunch of

00:10:09,600 --> 00:10:14,160
packages these are big top utilities

00:10:11,520 --> 00:10:15,930
packages who do packages HDFS packages

00:10:14,160 --> 00:10:17,760
MapReduce packages in the iron packages

00:10:15,930 --> 00:10:19,440
so I'm going to let this simmer in the

00:10:17,760 --> 00:10:23,790
back burner and move on with my

00:10:19,440 --> 00:10:24,840
presentation alright so let's talk a

00:10:23,790 --> 00:10:26,340
little bit more about integration

00:10:24,840 --> 00:10:29,190
testing while our demo is trying to

00:10:26,340 --> 00:10:30,390
install Hadoop in the background most

00:10:29,190 --> 00:10:32,070
individual projects as I said before

00:10:30,390 --> 00:10:33,360
don't perform integration testing they

00:10:32,070 --> 00:10:35,100
do a real good job doing the inner

00:10:33,360 --> 00:10:37,200
testing not necessarily integration

00:10:35,100 --> 00:10:38,760
testing an example of that is H page

00:10:37,200 --> 00:10:41,130
which is a distributed database on top

00:10:38,760 --> 00:10:43,290
of dupe does not presently have a

00:10:41,130 --> 00:10:44,640
release tarball that works with a dupe

00:10:43,290 --> 00:10:46,350
to which is the latest version of for

00:10:44,640 --> 00:10:49,260
Duke right so you can't download from

00:10:46,350 --> 00:10:50,910
anything from H based org H based on

00:10:49,260 --> 00:10:52,770
Apache Doric that will work with a dupe

00:10:50,910 --> 00:10:54,330
two out of the box but Vic top tries to

00:10:52,770 --> 00:10:56,910
install that exact problem right it will

00:10:54,330 --> 00:10:58,920
make a binary for HBase that will run

00:10:56,910 --> 00:11:00,390
directly with a deep too but if you

00:10:58,920 --> 00:11:02,400
think about it's a complex combinatorial

00:11:00,390 --> 00:11:03,840
problem right I mean you've got ten

00:11:02,400 --> 00:11:05,340
different versions of Project X that you

00:11:03,840 --> 00:11:07,110
want to test with 15 different versions

00:11:05,340 --> 00:11:09,180
of project by how are you going to do

00:11:07,110 --> 00:11:11,160
this n squared problem how you going to

00:11:09,180 --> 00:11:14,280
solve it right in the short answer that

00:11:11,160 --> 00:11:16,860
we do in in big top as we don't we pick

00:11:14,280 --> 00:11:18,870
particular versions that are latest and

00:11:16,860 --> 00:11:22,050
greatest at that time and we test them

00:11:18,870 --> 00:11:24,660
together so big top has quarterly

00:11:22,050 --> 00:11:26,040
release model every quarter will look at

00:11:24,660 --> 00:11:29,160
what is the latest greatest version of a

00:11:26,040 --> 00:11:31,410
dupe I've HBase crew flume whatever will

00:11:29,160 --> 00:11:33,180
pull it all out integration test it we

00:11:31,410 --> 00:11:35,190
work directly with the upstream projects

00:11:33,180 --> 00:11:38,040
to perform various levels of testing so

00:11:35,190 --> 00:11:40,560
Vic top has integration testing that

00:11:38,040 --> 00:11:42,810
will do platform you know distro level

00:11:40,560 --> 00:11:46,530
of grade level testing and make sure all

00:11:42,810 --> 00:11:48,270
these components work together so if you

00:11:46,530 --> 00:11:51,600
think about what debian did and the next

00:11:48,270 --> 00:11:53,700
space before debian there was you know

00:11:51,600 --> 00:11:54,860
some software that ran on the linux

00:11:53,700 --> 00:11:56,940
kernel and then there was a colonel

00:11:54,860 --> 00:11:58,980
debian came together put these all

00:11:56,940 --> 00:12:00,960
together in a cutting-edge distribution

00:11:58,980 --> 00:12:03,570
and allowed other distributions to

00:12:00,960 --> 00:12:04,590
derive from it so out came you bunt you

00:12:03,570 --> 00:12:06,420
out canopic

00:12:04,590 --> 00:12:08,070
right and big top is trying to do

00:12:06,420 --> 00:12:10,470
exactly the same thing before big top

00:12:08,070 --> 00:12:13,560
there was a Hadoop and there was a bunch

00:12:10,470 --> 00:12:16,500
of software hive HBase scoop flume that

00:12:13,560 --> 00:12:19,290
ran on her dude but with big top we're

00:12:16,500 --> 00:12:21,270
going to package it in a cutting-edge

00:12:19,290 --> 00:12:23,220
repository that other repositories that

00:12:21,270 --> 00:12:25,020
other distributions can derive from an

00:12:23,220 --> 00:12:26,130
example to those attributions are you

00:12:25,020 --> 00:12:28,980
know clatters distribution or

00:12:26,130 --> 00:12:32,160
Hortonworks distribution our van discos

00:12:28,980 --> 00:12:33,690
distribution and so here's an example of

00:12:32,160 --> 00:12:35,700
companies that use big top they clearly

00:12:33,690 --> 00:12:40,620
fall into categories first category of

00:12:35,700 --> 00:12:41,760
companies that rely on big tops

00:12:40,620 --> 00:12:43,920
distribution to create their own

00:12:41,760 --> 00:12:46,440
distribution example of that is cloudera

00:12:43,920 --> 00:12:48,780
and rayon disco other set of companies

00:12:46,440 --> 00:12:50,580
that rely on big tops deployment code to

00:12:48,780 --> 00:12:53,280
deploy whatever artifacts they want to

00:12:50,580 --> 00:12:55,020
deploy right an example that is EMC they

00:12:53,280 --> 00:12:57,000
use big tops puppet deployment code to

00:12:55,020 --> 00:12:59,850
actually deploy artifacts on their

00:12:57,000 --> 00:13:01,050
cluster alright so back to our demo

00:12:59,850 --> 00:13:03,960
let's see what's going on there whoa

00:13:01,050 --> 00:13:06,450
this is really slow so I was hoping in

00:13:03,960 --> 00:13:07,830
those three slides this thing would have

00:13:06,450 --> 00:13:09,600
downloaded but apparently the internet

00:13:07,830 --> 00:13:15,330
connection is slow so I'm going to sing

00:13:09,600 --> 00:13:16,740
a song no I'm kidding let's carry on so

00:13:15,330 --> 00:13:19,110
by this time you've all heard the

00:13:16,740 --> 00:13:21,060
MongoDB or seen the MongoDB video right

00:13:19,110 --> 00:13:24,540
and we've only talked about installing

00:13:21,060 --> 00:13:28,290
big top on one node right I took my

00:13:24,540 --> 00:13:30,720
laptop and tried to install it on I do

00:13:28,290 --> 00:13:32,160
Suter distribute configuration there but

00:13:30,720 --> 00:13:34,440
you asked the question is picked up

00:13:32,160 --> 00:13:35,640
really scalable right and of course I

00:13:34,440 --> 00:13:38,720
wouldn't be asking that question if the

00:13:35,640 --> 00:13:40,890
answer wasn't yes so the answer is yes

00:13:38,720 --> 00:13:42,480
big top comes with a whole bunch of

00:13:40,890 --> 00:13:44,130
deployment recipe so it obviously comes

00:13:42,480 --> 00:13:46,170
with artifacts that you can install on a

00:13:44,130 --> 00:13:47,880
fully distributed cluster but it comes

00:13:46,170 --> 00:13:50,250
with deployment code that you can use to

00:13:47,880 --> 00:13:53,070
deploy things and that's the example of

00:13:50,250 --> 00:13:55,200
customers i gave previously moreover the

00:13:53,070 --> 00:13:58,800
next version of big top is going to have

00:13:55,200 --> 00:14:00,780
integration with apache were which some

00:13:58,800 --> 00:14:03,510
of you might know will help you install

00:14:00,780 --> 00:14:09,260
Hadoop and related projects on you know

00:14:03,510 --> 00:14:12,330
amazon AWS or CloudStack or Rackspace

00:14:09,260 --> 00:14:13,500
third-party cloud providers and big top

00:14:12,330 --> 00:14:15,390
will have integration with that to help

00:14:13,500 --> 00:14:17,160
you do that install big top integration

00:14:15,390 --> 00:14:19,570
tested artifacts on that as well

00:14:17,160 --> 00:14:20,620
alright so I'm going to keep went back

00:14:19,570 --> 00:14:21,990
and forth here or see what my

00:14:20,620 --> 00:14:24,430
installation progress is like

00:14:21,990 --> 00:14:25,959
ninety-eight percent all right you get

00:14:24,430 --> 00:14:29,140
to hear some more so why use big top

00:14:25,959 --> 00:14:31,899
first of all it's an easier deployment

00:14:29,140 --> 00:14:33,640
of tested upstream components like I

00:14:31,899 --> 00:14:35,140
said before there isn't much an

00:14:33,640 --> 00:14:36,520
integration testing going on between

00:14:35,140 --> 00:14:38,500
various components in the Hadoop

00:14:36,520 --> 00:14:42,760
ecosystem and Big Top does that for you

00:14:38,500 --> 00:14:45,270
it will also supply you with puppet

00:14:42,760 --> 00:14:47,170
recipes it will supply you with

00:14:45,270 --> 00:14:48,580
convenience artifacts like Hadoop

00:14:47,170 --> 00:14:51,040
consider which will help you install

00:14:48,580 --> 00:14:52,870
things in a much more easier fashion

00:14:51,040 --> 00:14:55,300
than what you get from the upstream

00:14:52,870 --> 00:14:56,709
component and the third thing and I

00:14:55,300 --> 00:14:59,380
think this is more like a meta point

00:14:56,709 --> 00:15:00,580
it's a distribution of the community by

00:14:59,380 --> 00:15:03,279
the community for the community sounds

00:15:00,580 --> 00:15:05,920
like the Constitution rate but it really

00:15:03,279 --> 00:15:07,720
is I mean you shape how this

00:15:05,920 --> 00:15:10,450
distribution look you don't like

00:15:07,720 --> 00:15:11,649
something and there will be something in

00:15:10,450 --> 00:15:14,140
the demo you won't like I'll point it

00:15:11,649 --> 00:15:15,640
out when when we get there all you had

00:15:14,140 --> 00:15:18,730
to do is you know come provide your

00:15:15,640 --> 00:15:20,560
feedback on the mailing list you know if

00:15:18,730 --> 00:15:22,450
you're willing provide a patch we would

00:15:20,560 --> 00:15:23,920
love to see more patches and say you

00:15:22,450 --> 00:15:25,480
know what this is really a pain point we

00:15:23,920 --> 00:15:27,310
should fix it yeah we totally should

00:15:25,480 --> 00:15:30,579
right there's a lot of sucky things out

00:15:27,310 --> 00:15:33,010
there in the ecosystem that will we'll

00:15:30,579 --> 00:15:34,300
be glad to fix so you really shape how

00:15:33,010 --> 00:15:36,220
the distribution look like and can't

00:15:34,300 --> 00:15:38,620
emphasize it enough to have your

00:15:36,220 --> 00:15:40,720
contribution providing that feedback and

00:15:38,620 --> 00:15:42,730
providing your contributions to make the

00:15:40,720 --> 00:15:44,649
deployment and integration testing off

00:15:42,730 --> 00:15:46,589
the various projects easier all right

00:15:44,649 --> 00:15:51,459
let's see what's going on here who

00:15:46,589 --> 00:15:52,600
finally all right so we are here we

00:15:51,459 --> 00:15:54,250
installed our pseudo distributed

00:15:52,600 --> 00:15:57,490
configuration package we need to do some

00:15:54,250 --> 00:16:00,640
installation so what happens is in the

00:15:57,490 --> 00:16:05,320
etsy nad directory you're going to d's

00:16:00,640 --> 00:16:07,720
get these files dupe HDFS star files and

00:16:05,320 --> 00:16:11,500
we need to initialize these files before

00:16:07,720 --> 00:16:16,690
we do anything so sudo service Hadoop

00:16:11,500 --> 00:16:18,760
HDFS name node in it alright that will

00:16:16,690 --> 00:16:24,270
initialize our name note then we'll go

00:16:18,760 --> 00:16:28,380
sudo service a dupe HDFS name node start

00:16:24,270 --> 00:16:28,380
hopefully that will start things up

00:16:33,670 --> 00:16:38,470
alright and now we need to do the same

00:16:36,850 --> 00:16:44,220
thing on data node so we'll do sudo

00:16:38,470 --> 00:16:44,220
service Hadoop HDFS data node start

00:16:47,570 --> 00:16:52,040
it would help if I named things

00:16:49,620 --> 00:16:52,040
correctly

00:16:58,160 --> 00:17:11,900
okay then we'll start the yarn Beeman's

00:17:00,589 --> 00:17:13,130
so dupe yarn node manager restart yeah I

00:17:11,900 --> 00:17:15,140
guess at this point you kind of have to

00:17:13,130 --> 00:17:16,459
know what you had to start but if you

00:17:15,140 --> 00:17:18,380
look at the init scripts that the

00:17:16,459 --> 00:17:22,039
package installed these were the unit

00:17:18,380 --> 00:17:24,919
scripts that were installed so I'm just

00:17:22,039 --> 00:17:33,980
going to start all of them a dupe yarn

00:17:24,919 --> 00:17:36,289
resource manager start we start all

00:17:33,980 --> 00:17:41,960
right so when this command completes we

00:17:36,289 --> 00:17:44,120
have started all the all the components

00:17:41,960 --> 00:17:47,900
that got installed with that pseudo

00:17:44,120 --> 00:17:50,990
distributed config package and if i did

00:17:47,900 --> 00:17:53,330
a dupe FS LS command there's nothing

00:17:50,990 --> 00:17:56,980
install right now so what we have to do

00:17:53,330 --> 00:17:59,539
is we have to initialize our HDFS and

00:17:56,980 --> 00:18:02,840
there is a script that shipped with big

00:17:59,539 --> 00:18:04,400
top that's called very promptly in it

00:18:02,840 --> 00:18:06,440
hdfs so I'm going to run that script

00:18:04,400 --> 00:18:11,440
make sure I look at my notes make sure I

00:18:06,440 --> 00:18:11,440
didn't forget anything okay

00:18:21,039 --> 00:18:24,080
alright so just creating a bunch of

00:18:22,909 --> 00:18:25,279
directories but you don't have to create

00:18:24,080 --> 00:18:27,489
the manually descrip will do that for

00:18:25,279 --> 00:18:27,489
you

00:18:46,380 --> 00:18:50,060
all right almost here

00:18:55,910 --> 00:18:59,080
ok and

00:19:00,690 --> 00:19:04,950
just too confusing i'm going to restart

00:19:02,610 --> 00:19:07,890
node managers and resource managers I'll

00:19:04,950 --> 00:19:09,210
tell you why later so it turns out that

00:19:07,890 --> 00:19:11,100
these directories need to be created

00:19:09,210 --> 00:19:13,080
before you start the node manager and

00:19:11,100 --> 00:19:14,520
resource manager and I know that it's an

00:19:13,080 --> 00:19:18,090
orchestration problem again that's

00:19:14,520 --> 00:19:20,220
something that Vic top deployment code

00:19:18,090 --> 00:19:23,040
deals with but given that i installed

00:19:20,220 --> 00:19:25,410
the package by hand I'll had to do them

00:19:23,040 --> 00:19:28,380
manually anyway so the sequence was I

00:19:25,410 --> 00:19:32,820
install the package I install I in it

00:19:28,380 --> 00:19:36,360
hdfs I start HDFS that I restart the

00:19:32,820 --> 00:19:40,440
yarn demons and now we are ready to run

00:19:36,360 --> 00:19:42,750
some MapReduce code and hope it works so

00:19:40,440 --> 00:19:44,520
this is essentially Hadoop comes with

00:19:42,750 --> 00:19:46,920
this examples jar file that has a bunch

00:19:44,520 --> 00:19:48,390
of things you can run with there's a PI

00:19:46,920 --> 00:19:50,070
job that will let you calculate the

00:19:48,390 --> 00:19:51,660
value of pi up to a particular precision

00:19:50,070 --> 00:19:54,390
so I'm just going to run that and that's

00:19:51,660 --> 00:19:56,370
it you command Hadoop jar location of

00:19:54,390 --> 00:19:59,240
where my MapReduce and ample jars is and

00:19:56,370 --> 00:20:02,010
I want to run the PI job there and

00:19:59,240 --> 00:20:03,390
that's like the precision and scale that

00:20:02,010 --> 00:20:09,120
I want to use to calculate the value of

00:20:03,390 --> 00:20:10,740
pi so that's gonna all right so that is

00:20:09,120 --> 00:20:12,300
right something's not running so now you

00:20:10,740 --> 00:20:16,290
get to see a live debugging session of

00:20:12,300 --> 00:20:18,120
what's going on isn't this exciting all

00:20:16,290 --> 00:20:23,280
right let's see what services are up and

00:20:18,120 --> 00:20:26,280
which are not Hadoop yarn load managers

00:20:23,280 --> 00:20:29,550
status all right our note manager is

00:20:26,280 --> 00:20:33,480
dead let's see our resource manager

00:20:29,550 --> 00:20:34,740
status resource manager is running so

00:20:33,480 --> 00:20:39,680
what are we going to do are you going to

00:20:34,740 --> 00:20:39,680
try to start the node manager again

00:20:46,880 --> 00:20:56,060
all right let's resource manager still

00:20:49,760 --> 00:21:00,410
up node manager is right okay let's see

00:20:56,060 --> 00:21:02,390
if you have better luck all right that

00:21:00,410 --> 00:21:04,700
obviously did not work let's give you a

00:21:02,390 --> 00:21:08,120
better lug running our Hadoop ooh okay

00:21:04,700 --> 00:21:11,210
so because I ran a job once before the

00:21:08,120 --> 00:21:14,030
output directory was already there so we

00:21:11,210 --> 00:21:19,540
just have to remove that Hadoop FS Mr

00:21:14,030 --> 00:21:23,180
okay all right let's drive one more time

00:21:19,540 --> 00:21:27,110
okay finally I will not embarrass myself

00:21:23,180 --> 00:21:29,480
see that word so again this is an

00:21:27,110 --> 00:21:30,920
example to show how you would the job is

00:21:29,480 --> 00:21:32,690
actually running right now but this is

00:21:30,920 --> 00:21:35,570
an example to show how you would you

00:21:32,690 --> 00:21:37,040
know install Hadoop MapReduce on your on

00:21:35,570 --> 00:21:40,460
your single node laptop whatever you

00:21:37,040 --> 00:21:41,840
have there's certain steps you how to

00:21:40,460 --> 00:21:43,310
follow all these steps are listed on the

00:21:41,840 --> 00:21:45,620
wiki page and these are just matter of

00:21:43,310 --> 00:21:47,420
the inherent complexity that comes when

00:21:45,620 --> 00:21:50,060
to do right you have to start HDFS first

00:21:47,420 --> 00:21:51,740
you're in initialize HDFS so that it

00:21:50,060 --> 00:21:54,020
creates a bunch of directories which

00:21:51,740 --> 00:21:57,020
yarn is going to use and then you start

00:21:54,020 --> 00:21:59,960
your run demons so and then there comes

00:21:57,020 --> 00:22:03,020
your value of pi rate so all this is

00:21:59,960 --> 00:22:05,090
great but this is like a very

00:22:03,020 --> 00:22:06,860
fundamental example so let's dig in a

00:22:05,090 --> 00:22:13,400
little more we'll install two more

00:22:06,860 --> 00:22:17,330
projects scoop and hive all right so

00:22:13,400 --> 00:22:19,820
scoop is this project that lets you pull

00:22:17,330 --> 00:22:22,280
in things from a relational land into

00:22:19,820 --> 00:22:24,310
Hadoop world and pull out things from

00:22:22,280 --> 00:22:28,960
the hadoop world back to relational and

00:22:24,310 --> 00:22:33,110
import export right lucrative business

00:22:28,960 --> 00:22:34,730
hive is a sequel like layer on top of a

00:22:33,110 --> 00:22:38,840
tube that will take sequel like queries

00:22:34,730 --> 00:22:41,360
and compile them into MapReduce and then

00:22:38,840 --> 00:22:43,640
result returns the results from whatever

00:22:41,360 --> 00:22:46,270
MapReduce computation that you did all

00:22:43,640 --> 00:22:51,350
right so while that's being installed

00:22:46,270 --> 00:22:56,360
I'm going to see my notes again okay so

00:22:51,350 --> 00:22:58,180
I'm gonna what am I going to do I might

00:22:56,360 --> 00:23:03,710
just carry on with my presentation is

00:22:58,180 --> 00:23:07,700
less user all right you know what I'm

00:23:03,710 --> 00:23:11,120
going to wait around okay so what I'm

00:23:07,700 --> 00:23:16,250
going to do with this is I have a data

00:23:11,120 --> 00:23:19,880
set loaded on this VM that's from 2000

00:23:16,250 --> 00:23:22,970
census of the united states and it has

00:23:19,880 --> 00:23:25,790
data of salary information per zip code

00:23:22,970 --> 00:23:27,470
right so it's just a simple data set I

00:23:25,790 --> 00:23:30,620
think maybe three thousand lines it's

00:23:27,470 --> 00:23:33,380
basically got salary information for

00:23:30,620 --> 00:23:34,880
various up codes this isn't my sequel so

00:23:33,380 --> 00:23:39,170
we're going to use scoop to transfer

00:23:34,880 --> 00:23:41,810
data from my sequel on to Hadoop and

00:23:39,170 --> 00:23:45,170
HDFS then we're going to run a query on

00:23:41,810 --> 00:23:46,400
hive to see whatever we wanted to see in

00:23:45,170 --> 00:23:49,460
this case we'll see an example of that

00:23:46,400 --> 00:23:51,320
so all I'm trying to do is show you with

00:23:49,460 --> 00:23:53,210
big top how easy it is to install

00:23:51,320 --> 00:23:54,770
artifacts and take a data set that

00:23:53,210 --> 00:23:58,190
you're very familiar with in my sequel

00:23:54,770 --> 00:24:01,190
convert in her do plan and run a hive

00:23:58,190 --> 00:24:02,540
query on top of that okay that was

00:24:01,190 --> 00:24:04,040
enough filler of God so another

00:24:02,540 --> 00:24:10,100
twenty-five percent is going to be all

00:24:04,040 --> 00:24:14,050
quiet alright so maybe we can browse

00:24:10,100 --> 00:24:14,050
through the my sequel data that we have

00:24:42,179 --> 00:24:48,049
alright this is almost done all right

00:24:44,220 --> 00:24:48,049
let's go browse to where my sequel data

00:24:50,090 --> 00:24:53,090
right

00:24:54,010 --> 00:25:03,250
okay so here's our table zip code

00:24:57,610 --> 00:25:06,850
incomes right so there's some sort of ID

00:25:03,250 --> 00:25:08,470
then zip code code then some description

00:25:06,850 --> 00:25:10,960
and then the end you see the income

00:25:08,470 --> 00:25:15,160
right so the query I'm going to run

00:25:10,960 --> 00:25:17,530
what's going to be select have what we

00:25:15,160 --> 00:25:19,570
do average income actually before I even

00:25:17,530 --> 00:25:20,710
go there let's do a describe table so

00:25:19,570 --> 00:25:27,640
you know what the schema looks like

00:25:20,710 --> 00:25:29,590
describe zip code incomes okay yeah so

00:25:27,640 --> 00:25:31,780
like I said id zip code description

00:25:29,590 --> 00:25:35,740
description income I'm going to do this

00:25:31,780 --> 00:25:41,620
query we're going to do average sub str

00:25:35,740 --> 00:25:49,080
I think it's called off step 13 comma

00:25:41,620 --> 00:25:56,800
average income from zip code incomes

00:25:49,080 --> 00:25:58,810
group by substring zip three so I'm

00:25:56,800 --> 00:26:01,780
essentially grouping things by regions

00:25:58,810 --> 00:26:03,760
to do some sort of analogy with regions

00:26:01,780 --> 00:26:05,980
I'm taking the first three letters of

00:26:03,760 --> 00:26:09,700
the zip code and calling that a region

00:26:05,980 --> 00:26:12,310
right all right that work I sequel isn't

00:26:09,700 --> 00:26:15,100
all that bad all right so look here

00:26:12,310 --> 00:26:18,310
let's go through some data well you know

00:26:15,100 --> 00:26:22,120
usually is 30 28,000 range 45,000 range

00:26:18,310 --> 00:26:24,490
whatever so you get some answer here now

00:26:22,120 --> 00:26:28,180
we're going to transfer this data onto a

00:26:24,490 --> 00:26:30,790
dupe right we have our dupe running

00:26:28,180 --> 00:26:33,510
there's some stuff in HDFS because we

00:26:30,790 --> 00:26:35,920
can edit it and we ran up high job but

00:26:33,510 --> 00:26:38,020
let's use scoop to transfer this data

00:26:35,920 --> 00:26:42,280
from my sequel alright but there's a

00:26:38,020 --> 00:26:43,780
catch scoop thanks to licensing doesn't

00:26:42,280 --> 00:26:45,760
come with a my sequel connector so scoop

00:26:43,780 --> 00:26:47,920
uses Java JDBC to connect to my sequel

00:26:45,760 --> 00:26:49,210
to pull data off for licensing reasons

00:26:47,920 --> 00:26:51,340
we don't ship the connector with scoop

00:26:49,210 --> 00:26:53,110
so first thing you need to do is pull

00:26:51,340 --> 00:26:54,970
the connector from my sequel website and

00:26:53,110 --> 00:26:56,680
put it in the appropriate location where

00:26:54,970 --> 00:26:58,360
scoop can find it in the classpath right

00:26:56,680 --> 00:27:01,540
so that's what I'm going to do I'm going

00:26:58,360 --> 00:27:03,640
to curl it from my sequels website and

00:27:01,540 --> 00:27:05,710
this is a pain point I was referring to

00:27:03,640 --> 00:27:07,200
like I mean we shouldn't really had to

00:27:05,710 --> 00:27:09,149
do that perhaps there should be a script

00:27:07,200 --> 00:27:10,289
does that you know and these are the

00:27:09,149 --> 00:27:11,519
kind of things where you can shape the

00:27:10,289 --> 00:27:13,139
deployment code the way you want to see

00:27:11,519 --> 00:27:16,370
it I personally see it as a pain point I

00:27:13,139 --> 00:27:19,500
hope you do too and so things like these

00:27:16,370 --> 00:27:21,539
need to be dealt outside off the parent

00:27:19,500 --> 00:27:24,419
project and big top is essentially the

00:27:21,539 --> 00:27:26,010
place to deal with them okay so that

00:27:24,419 --> 00:27:27,450
download is going on the next command is

00:27:26,010 --> 00:27:30,210
just going to copy it from where where I

00:27:27,450 --> 00:27:31,769
copied it to user lip scoop lip so every

00:27:30,210 --> 00:27:33,419
project usually has Lib directory where

00:27:31,769 --> 00:27:35,399
it pulls the stuff puts in the classpath

00:27:33,419 --> 00:27:38,010
and we are essentially going to drop

00:27:35,399 --> 00:27:40,490
this my sequel connector jar in scoops

00:27:38,010 --> 00:27:40,490
classpath

00:27:49,680 --> 00:27:55,769
and

00:27:52,190 --> 00:28:00,989
you know what maybe I should try that

00:27:55,769 --> 00:28:03,179
again oh yeah so I was trying to

00:28:00,989 --> 00:28:05,219
download that in the etsy netd directory

00:28:03,179 --> 00:28:07,649
did without pseudo access so obviously

00:28:05,219 --> 00:28:08,700
that was expected so I changed director

00:28:07,649 --> 00:28:11,299
to my home directory I'm trying to

00:28:08,700 --> 00:28:11,299
download that again

00:28:46,620 --> 00:28:51,700
alright so that God downloaded now i'm

00:28:49,510 --> 00:28:52,870
going to copy it over and we're all set

00:28:51,700 --> 00:28:59,730
so now we were ready for our scoop

00:28:52,870 --> 00:28:59,730
command I am NOT a scoop group Vicki

00:29:45,590 --> 00:29:47,650
I

00:30:13,549 --> 00:30:17,659
as flag there is hype over right means

00:30:15,739 --> 00:30:19,610
if there is a table with that name if

00:30:17,659 --> 00:30:22,460
there is a table with that name that

00:30:19,610 --> 00:30:24,980
have already has cotton you go ahead and

00:30:22,460 --> 00:30:27,799
override that content right okay so

00:30:24,980 --> 00:30:31,489
fantastic it says hive important

00:30:27,799 --> 00:30:33,350
complete and we go start high again when

00:30:31,489 --> 00:30:36,049
i installed scoop i said to sudo apt-get

00:30:33,350 --> 00:30:38,509
install scoop and high so we have high

00:30:36,049 --> 00:30:40,129
ready we have not even logged on to high

00:30:38,509 --> 00:30:45,110
one till now the first thing we do is

00:30:40,129 --> 00:30:47,529
show tables and and there is the zipcode

00:30:45,110 --> 00:30:51,019
incomes table how did it get here scoop

00:30:47,529 --> 00:30:54,649
created it for us so if you want to go

00:30:51,019 --> 00:30:59,080
through this table it's the same data

00:30:54,649 --> 00:31:02,090
you want to do a described zip code

00:30:59,080 --> 00:31:04,369
incomes it's the same schema right so

00:31:02,090 --> 00:31:06,739
it's got ID zyp two descriptions and an

00:31:04,369 --> 00:31:08,919
income and if you want to run the same

00:31:06,739 --> 00:31:15,070
query that we ran there so we wanted to

00:31:08,919 --> 00:31:17,989
buy a region group find average income

00:31:15,070 --> 00:31:20,090
so zip 13 is actually starting this is

00:31:17,989 --> 00:31:22,340
not zero indexed for some reason

00:31:20,090 --> 00:31:24,470
substrings in hive and scoop and my

00:31:22,340 --> 00:31:26,749
sequel are one referenced so I'm saying

00:31:24,470 --> 00:31:34,580
take the first three letters three is

00:31:26,749 --> 00:31:36,649
the length from zip code in comes again

00:31:34,580 --> 00:31:40,450
like you can see I've syntax is really

00:31:36,649 --> 00:31:46,669
similar to sequels and tax group by

00:31:40,450 --> 00:31:47,929
substring zip 13 all right okay so now

00:31:46,669 --> 00:31:50,629
it's going to launch a MapReduce job

00:31:47,929 --> 00:31:52,399
essentially hive uses this job XML file

00:31:50,629 --> 00:31:55,940
that it will create send a bunch of

00:31:52,399 --> 00:31:57,859
properties Paula MapReduce and ask do to

00:31:55,940 --> 00:32:00,139
to run this MapReduce job and you're

00:31:57,859 --> 00:32:02,090
going to do all the legwork if you

00:32:00,139 --> 00:32:03,980
wanted to track this job there's a URL

00:32:02,090 --> 00:32:05,690
right here to tracking URL just go to

00:32:03,980 --> 00:32:06,950
your browser and look at that URL will

00:32:05,690 --> 00:32:09,919
show your progress of how many percent

00:32:06,950 --> 00:32:10,850
the job is done anyway so the mappers

00:32:09,919 --> 00:32:12,679
are hundred percent complete the

00:32:10,850 --> 00:32:15,320
reducers happened to open though we got

00:32:12,679 --> 00:32:18,169
some data alright the same data that we

00:32:15,320 --> 00:32:23,299
saw in scoop that's already in my sequel

00:32:18,169 --> 00:32:24,859
as president hi anyways so I want to

00:32:23,299 --> 00:32:26,779
point out that a lot of people spend

00:32:24,859 --> 00:32:28,759
days is trying to set up hive scoop and

00:32:26,779 --> 00:32:30,409
Hadoop to run together and with the

00:32:28,759 --> 00:32:32,539
artifacts that you get from Big Top just

00:32:30,409 --> 00:32:34,159
becomes you know relatively easy there

00:32:32,539 --> 00:32:35,480
are obviously plain points like going

00:32:34,159 --> 00:32:36,799
and downloading the my sequel jar and

00:32:35,480 --> 00:32:38,240
putting in the appropriate directory I

00:32:36,799 --> 00:32:41,059
mean how you're going to figure that out

00:32:38,240 --> 00:32:43,100
right but they're these are definitely

00:32:41,059 --> 00:32:44,509
points that we have dealt with big top

00:32:43,100 --> 00:32:46,070
to make things better and there's

00:32:44,509 --> 00:32:48,769
obviously a lot more we can do in and

00:32:46,070 --> 00:32:50,389
that's why we're trying here to ask you

00:32:48,769 --> 00:32:52,309
to use BIC to provide your feedback tell

00:32:50,389 --> 00:32:54,919
us about what the pain points are and if

00:32:52,309 --> 00:32:56,690
you can't help us fix them all right so

00:32:54,919 --> 00:32:58,840
let's recap the top does two things

00:32:56,690 --> 00:33:01,580
makes installation and configuration

00:32:58,840 --> 00:33:02,809
alpha do projects easier second performs

00:33:01,580 --> 00:33:04,249
integration testing amongst various

00:33:02,809 --> 00:33:05,600
projects so if you slept through the

00:33:04,249 --> 00:33:07,249
entire presentation you can't remember

00:33:05,600 --> 00:33:08,779
even a single thing these are the two

00:33:07,249 --> 00:33:11,210
things you should remember right so if

00:33:08,779 --> 00:33:14,690
you want to try out big top this is your

00:33:11,210 --> 00:33:16,159
time all right questions so first of all

00:33:14,690 --> 00:33:18,289
before i go into questions my twitter

00:33:16,159 --> 00:33:21,590
account is mark underscore over the code

00:33:18,289 --> 00:33:23,179
for the demo so the data set and the

00:33:21,590 --> 00:33:25,519
code that i used to bring up the wii m

00:33:23,179 --> 00:33:28,460
sort of do some basic provisioning

00:33:25,519 --> 00:33:31,119
before we install big top is also on to

00:33:28,460 --> 00:33:34,369
get a profile alright so question time

00:33:31,119 --> 00:33:37,070
yes sir there is a microphone just used

00:33:34,369 --> 00:33:40,549
a please so we'll picked a boil

00:33:37,070 --> 00:33:42,040
integration with such as in bari as a

00:33:40,549 --> 00:33:47,470
deployment and

00:33:42,040 --> 00:33:49,750
monitoring so they are complementary

00:33:47,470 --> 00:33:52,810
products at the moment right Vic top

00:33:49,750 --> 00:33:54,670
produces artifacts that projects like in

00:33:52,810 --> 00:33:56,350
Bari or clutter and manager could use to

00:33:54,670 --> 00:33:58,870
install everything instead of fusing

00:33:56,350 --> 00:34:01,750
them from car balls but as such big tub

00:33:58,870 --> 00:34:03,910
doesn't you know particularly help

00:34:01,750 --> 00:34:06,370
projects in that way the idea is to help

00:34:03,910 --> 00:34:08,380
the community with the installation and

00:34:06,370 --> 00:34:10,510
deployment projects and that's people

00:34:08,380 --> 00:34:19,140
like me or you know code like and bar

00:34:10,510 --> 00:34:19,140
your cloud area manager more questions

00:34:28,520 --> 00:34:35,899
so one of the pain points that we went

00:34:33,590 --> 00:34:39,110
through was just knowing what the right

00:34:35,899 --> 00:34:41,210
machine selection was is there any is

00:34:39,110 --> 00:34:45,230
there any efforts on that front I mean

00:34:41,210 --> 00:34:48,560
just like you know what kind of machine

00:34:45,230 --> 00:34:51,980
should we allocate for you know a three

00:34:48,560 --> 00:34:54,260
machine HBase cluster you know and and I

00:34:51,980 --> 00:34:56,840
mean one of the things we found is some

00:34:54,260 --> 00:34:59,600
of the information we got online was you

00:34:56,840 --> 00:35:01,520
know basically wrong you know or

00:34:59,600 --> 00:35:03,020
contradictory like oh these machines are

00:35:01,520 --> 00:35:05,120
too small this is odd about this is

00:35:03,020 --> 00:35:07,210
plenty right that sort of thing and just

00:35:05,120 --> 00:35:10,310
keeping keeping everything up and stable

00:35:07,210 --> 00:35:12,530
you know was was definitely a huge issue

00:35:10,310 --> 00:35:14,000
right and are you talking mostly about

00:35:12,530 --> 00:35:16,400
hardware configurations or these

00:35:14,000 --> 00:35:19,850
hardware configurations yeah right those

00:35:16,400 --> 00:35:24,560
are usually dictated by the upstream

00:35:19,850 --> 00:35:25,910
project and your your workloads the

00:35:24,560 --> 00:35:27,650
amount of data you're storing what kind

00:35:25,910 --> 00:35:29,390
of queries you're running so big tub

00:35:27,650 --> 00:35:31,010
doesn't necessarily come up with some

00:35:29,390 --> 00:35:35,210
sort of guidelines that you you do have

00:35:31,010 --> 00:35:37,640
but we do have some sort of hardware

00:35:35,210 --> 00:35:39,710
level actually specifications so Vic top

00:35:37,640 --> 00:35:40,940
artifacts only work with 64-bit machines

00:35:39,710 --> 00:35:43,160
we assume that you know if you're using

00:35:40,940 --> 00:35:46,810
32-bit probably not good enough for

00:35:43,160 --> 00:35:46,810
Hadoop Java

00:35:49,560 --> 00:35:53,160
that primarily because upstream projects

00:35:51,690 --> 00:35:55,980
haven't really started supporting you

00:35:53,160 --> 00:35:59,040
know openjdk or things of that nature

00:35:55,980 --> 00:36:00,690
but in terms of how much memory

00:35:59,040 --> 00:36:02,100
requirement or what hardware required

00:36:00,690 --> 00:36:06,170
for the particular project there is not

00:36:02,100 --> 00:36:09,390
much big top can mandate or does mandate

00:36:06,170 --> 00:36:11,160
but in terms of hardware architecture as

00:36:09,390 --> 00:36:13,140
i said before 64-bit machines and having

00:36:11,160 --> 00:36:19,470
the proper java installed is what we do

00:36:13,140 --> 00:36:26,730
mandy any other questions we've got one

00:36:19,470 --> 00:36:29,190
here Justin yeah so the question was are

00:36:26,730 --> 00:36:31,110
there any automated tests in the big top

00:36:29,190 --> 00:36:33,240
codebase yes there is every time a new

00:36:31,110 --> 00:36:36,270
component gets checked in we we ask that

00:36:33,240 --> 00:36:38,790
you check in integration tests of course

00:36:36,270 --> 00:36:40,920
more tests are always welcome but it's

00:36:38,790 --> 00:36:44,610
kind of mandatory for a new component to

00:36:40,920 --> 00:36:45,720
get added that we get integration tests

00:36:44,610 --> 00:36:47,160
because that's kind of the purpose of

00:36:45,720 --> 00:36:50,220
back top right make sure everything

00:36:47,160 --> 00:36:53,610
integrates nicely the components have to

00:36:50,220 --> 00:36:55,500
be Hadoop related in some shape sense so

00:36:53,610 --> 00:36:58,200
obviously this includes the really

00:36:55,500 --> 00:37:01,230
popular ones hive at dupage base but

00:36:58,200 --> 00:37:03,180
also they can be searched for example

00:37:01,230 --> 00:37:05,610
solar is integrated in the big top so

00:37:03,180 --> 00:37:07,680
most people can't really recognize solar

00:37:05,610 --> 00:37:10,500
energy together but you know if it's

00:37:07,680 --> 00:37:12,330
remotely related we would consider it to

00:37:10,500 --> 00:37:18,440
be added integration tested oh hey why

00:37:12,330 --> 00:37:18,440
not great more questions

00:37:19,540 --> 00:37:23,180
alright well thank you for coming if you

00:37:21,590 --> 00:37:25,550
have any more questions there is a big

00:37:23,180 --> 00:37:27,020
top user mailing list that gentleman

00:37:25,550 --> 00:37:28,280
there Roman is the VP other projects so

00:37:27,020 --> 00:37:30,710
if you don't like something please blame

00:37:28,280 --> 00:37:34,070
him if you do like something come to me

00:37:30,710 --> 00:37:35,660
and congratulate me but yeah thank you

00:37:34,070 --> 00:37:38,380
for attending please provide your

00:37:35,660 --> 00:37:38,380

YouTube URL: https://www.youtube.com/watch?v=94nfp5gR568


