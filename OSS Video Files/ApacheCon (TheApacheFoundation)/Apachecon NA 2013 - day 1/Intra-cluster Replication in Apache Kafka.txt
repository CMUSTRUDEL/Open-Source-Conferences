Title: Intra-cluster Replication in Apache Kafka
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Jun Rao
ApacheCon NA 2013
Overture and Beginners
Captions: 
	00:00:01,639 --> 00:00:06,480
okay thanks thanks everyone for coming

00:00:04,100 --> 00:00:09,870
today I'm going to give a presentation

00:00:06,480 --> 00:00:12,719
on Apache Kafka and it's mostly about

00:00:09,870 --> 00:00:15,929
our newly introduced feature his intro

00:00:12,719 --> 00:00:18,090
class replication within the Kafka so

00:00:15,929 --> 00:00:20,670
just a little bit the introductory

00:00:18,090 --> 00:00:22,470
contraption of myself I'm currently a

00:00:20,670 --> 00:00:25,380
software software engineer at Lincoln

00:00:22,470 --> 00:00:28,800
I've been there since 2010 where I

00:00:25,380 --> 00:00:31,650
mostly work on developing Kafka as well

00:00:28,800 --> 00:00:33,870
as supporting the operation and usage of

00:00:31,650 --> 00:00:36,630
Kafka at Lincoln so just a little

00:00:33,870 --> 00:00:38,750
history of Kafka Kafka was originally

00:00:36,630 --> 00:00:43,640
developed and then used at Lincoln and

00:00:38,750 --> 00:00:46,590
in 2011 Lincoln decided to donate

00:00:43,640 --> 00:00:49,649
conference code to Apache when we became

00:00:46,590 --> 00:00:51,930
an incubator project and was the end of

00:00:49,649 --> 00:00:55,350
last year we actually graduated as a

00:00:51,930 --> 00:00:58,859
top-level Apache project so that's where

00:00:55,350 --> 00:01:01,440
we are now so other than Apache Kafka I

00:00:58,859 --> 00:01:06,270
also worked on another Apache project

00:01:01,440 --> 00:01:09,990
Cassandra befallen clean and earlier I

00:01:06,270 --> 00:01:13,799
think I was also database researcher in

00:01:09,990 --> 00:01:16,020
one of the IBM research labs so here's

00:01:13,799 --> 00:01:18,750
the outline of the talk so I'm going to

00:01:16,020 --> 00:01:22,229
start with a quick overview of Kafka and

00:01:18,750 --> 00:01:25,439
its application and after that error

00:01:22,229 --> 00:01:28,110
give you a quick description of the

00:01:25,439 --> 00:01:32,729
overall of architecture of Kafka and

00:01:28,110 --> 00:01:35,549
it's a bit internal details and after

00:01:32,729 --> 00:01:37,439
that error explain how we extended the

00:01:35,549 --> 00:01:40,140
current system to support this new

00:01:37,439 --> 00:01:42,420
replication feature our talk about our

00:01:40,140 --> 00:01:46,320
performance and if he happy to answer

00:01:42,420 --> 00:01:49,159
any questions was the end so what is

00:01:46,320 --> 00:01:52,920
Kafka in one sentence Kafka is a

00:01:49,159 --> 00:01:54,000
distributed pop sub messaging system but

00:01:52,920 --> 00:01:56,969
what's really more important to

00:01:54,000 --> 00:01:59,219
understand what people use Kappa for the

00:01:56,969 --> 00:02:02,369
since coffee is donated to Apache we

00:01:59,219 --> 00:02:05,540
have seen usage in many places outside

00:02:02,369 --> 00:02:07,950
Lincoln and for example Twitter box

00:02:05,540 --> 00:02:10,080
Foursquare all of those web 2.0

00:02:07,950 --> 00:02:12,629
companies have been using coffee

00:02:10,080 --> 00:02:12,970
internally and more recently we have

00:02:12,629 --> 00:02:14,830
seen

00:02:12,970 --> 00:02:18,100
some more traditional enterprise

00:02:14,830 --> 00:02:21,040
companies starting to use Kafka so what

00:02:18,100 --> 00:02:22,990
do people really use Kaka for it turns

00:02:21,040 --> 00:02:27,310
out there are different people are using

00:02:22,990 --> 00:02:29,890
Kafka for different purposes so in one

00:02:27,310 --> 00:02:33,040
typical use case people use Kafka as a

00:02:29,890 --> 00:02:34,870
long aggregator so a lot of people have

00:02:33,040 --> 00:02:37,270
those problem they have various kinds of

00:02:34,870 --> 00:02:39,940
log sitting in the very servers they

00:02:37,270 --> 00:02:41,680
need a place to aggregate those data

00:02:39,940 --> 00:02:44,950
together and then potentially eventually

00:02:41,680 --> 00:02:48,040
load those data into Hadoop for batch

00:02:44,950 --> 00:02:50,670
processing for ad hoc analysis and this

00:02:48,040 --> 00:02:53,080
is a place that coffee can be useful

00:02:50,670 --> 00:02:55,420
there are some of the people where they

00:02:53,080 --> 00:02:58,290
just want to create a live stream of

00:02:55,420 --> 00:03:02,020
data and have some kind of processing

00:02:58,290 --> 00:03:03,790
code behind this live stream so they

00:03:02,020 --> 00:03:07,180
will consume those data in real time as

00:03:03,790 --> 00:03:11,440
they can another important use case of

00:03:07,180 --> 00:03:14,260
Kaka is monitoring this cup has been

00:03:11,440 --> 00:03:17,380
used in many places for monitoring the

00:03:14,260 --> 00:03:19,269
health needs of individual services can

00:03:17,380 --> 00:03:22,209
be used to monitor the house needs of

00:03:19,269 --> 00:03:24,940
emissions and for collecting various

00:03:22,209 --> 00:03:27,670
kind of logs so monitoring is another

00:03:24,940 --> 00:03:28,180
big use case of Kafka so last but not

00:03:27,670 --> 00:03:29,920
least

00:03:28,180 --> 00:03:32,320
so Kafka can also be used for

00:03:29,920 --> 00:03:34,780
traditional Message Queuing so there are

00:03:32,320 --> 00:03:37,209
lots of applications where they want to

00:03:34,780 --> 00:03:39,489
generate a large of number of events or

00:03:37,209 --> 00:03:41,380
messages but they don't want to person

00:03:39,489 --> 00:03:44,019
process those immediately they won't

00:03:41,380 --> 00:03:46,690
have a buffer or place to queue those

00:03:44,019 --> 00:03:48,820
things and asynchronously there's some

00:03:46,690 --> 00:03:52,690
demon that were processed those later

00:03:48,820 --> 00:03:55,380
so kava can serve as a big queue for

00:03:52,690 --> 00:03:58,060
buffering or those outstanding messages

00:03:55,380 --> 00:04:00,790
so since Lincoln is one of the early

00:03:58,060 --> 00:04:03,760
adopters of Kafka to all those use cases

00:04:00,790 --> 00:04:05,350
have always been used and exercised at

00:04:03,760 --> 00:04:07,570
Lincoln it's in the next couple of

00:04:05,350 --> 00:04:10,090
slides what I'm going to do is just walk

00:04:07,570 --> 00:04:14,430
through with you some of the use cases

00:04:10,090 --> 00:04:17,590
and the deployment at LinkedIn for Kafka

00:04:14,430 --> 00:04:19,540
so if you are linking user you probably

00:04:17,590 --> 00:04:22,660
have seen this page this is if a lot

00:04:19,540 --> 00:04:24,849
into Lincoln this is the network update

00:04:22,660 --> 00:04:26,800
stream in your main home page that we

00:04:24,849 --> 00:04:29,319
will see those get updated

00:04:26,800 --> 00:04:33,340
as new updates Commons like connections

00:04:29,319 --> 00:04:37,419
showing up this is a second screenshot

00:04:33,340 --> 00:04:39,759
this is actually the internal graph we

00:04:37,419 --> 00:04:43,120
call which we call in graph that we use

00:04:39,759 --> 00:04:45,849
for monitoring or the link internal

00:04:43,120 --> 00:04:47,800
services and it houses some of the basic

00:04:45,849 --> 00:04:49,720
metric in this particular case is our

00:04:47,800 --> 00:04:53,050
key value store Baltimore we want to

00:04:49,720 --> 00:04:57,039
measure its QPS over time in both those

00:04:53,050 --> 00:04:59,409
cases the copper is used as one of the

00:04:57,039 --> 00:05:01,389
underlying technology to provide you

00:04:59,409 --> 00:05:04,139
know the feeds and all provide the

00:05:01,389 --> 00:05:06,430
monitoring capability

00:05:04,139 --> 00:05:09,729
so just a bit more detail this is

00:05:06,430 --> 00:05:14,050
actually a bit more detailed deployment

00:05:09,729 --> 00:05:17,500
of caca within Lincoln so at Lincoln our

00:05:14,050 --> 00:05:19,810
mission for kakÃ¡ is to collect all kinds

00:05:17,500 --> 00:05:23,349
of interactive data and deliver this

00:05:19,810 --> 00:05:26,340
data to wherever they want and this

00:05:23,349 --> 00:05:28,870
interact interactive data some of them

00:05:26,340 --> 00:05:30,819
human generated so for example the

00:05:28,870 --> 00:05:33,639
network upstream data that you saw

00:05:30,819 --> 00:05:35,770
earlier is some kind of human

00:05:33,639 --> 00:05:39,150
interactive data but we also track

00:05:35,770 --> 00:05:41,800
things like pageviews impressions and

00:05:39,150 --> 00:05:45,159
searches people search keywords people

00:05:41,800 --> 00:05:46,659
typed in some other interaction

00:05:45,159 --> 00:05:49,150
interactive data can be machine

00:05:46,659 --> 00:05:51,849
generated these are some monitoring data

00:05:49,150 --> 00:05:55,000
that we talked about that can be the

00:05:51,849 --> 00:05:58,779
internal matrix of a java service can be

00:05:55,000 --> 00:06:01,509
the matric collect on the i/o CPU and

00:05:58,779 --> 00:06:05,169
network stats and can be some of your

00:06:01,509 --> 00:06:07,389
service log as well so we collect those

00:06:05,169 --> 00:06:09,819
data and we don't feed those to all the

00:06:07,389 --> 00:06:12,009
consumers so some of the consumers are

00:06:09,819 --> 00:06:14,020
real-time as you will see some of those

00:06:12,009 --> 00:06:16,000
live services have those arrows pointing

00:06:14,020 --> 00:06:19,449
back to them so that was out of services

00:06:16,000 --> 00:06:22,029
that consume some of those live streams

00:06:19,449 --> 00:06:24,550
in real time a lot of those are news

00:06:22,029 --> 00:06:27,279
rated applications or security wray trap

00:06:24,550 --> 00:06:29,740
with related applications and monitoring

00:06:27,279 --> 00:06:31,900
is exam prop is real-time

00:06:29,740 --> 00:06:34,690
consumer because it wants to consume all

00:06:31,900 --> 00:06:37,060
the matrix data we lot and then plot all

00:06:34,690 --> 00:06:39,520
those graphs in real time there's

00:06:37,060 --> 00:06:40,680
another consumers that's more offline so

00:06:39,520 --> 00:06:43,500
you can see the other five

00:06:40,680 --> 00:06:46,380
the horizontal pipe where we take that

00:06:43,500 --> 00:06:48,810
feed and push that into our offline

00:06:46,380 --> 00:06:50,850
analysis data center eventually I'm

00:06:48,810 --> 00:06:53,910
going to load this data into our Hadoop

00:06:50,850 --> 00:06:56,639
cluster and maybe our data warehouse

00:06:53,910 --> 00:06:58,590
system so that we can run reporting we

00:06:56,639 --> 00:07:00,660
can run batch queries and we can run all

00:06:58,590 --> 00:07:03,720
those sort of ad hoc analysis this is a

00:07:00,660 --> 00:07:06,000
server offline use case of the same set

00:07:03,720 --> 00:07:08,250
of data we must have the reverse

00:07:06,000 --> 00:07:10,199
pipeline please you receive the arrows I

00:07:08,250 --> 00:07:12,509
think from Hadoop to kafka points the

00:07:10,199 --> 00:07:15,240
other way as well so this in some sense

00:07:12,509 --> 00:07:17,220
is like the Qun usage where we have lost

00:07:15,240 --> 00:07:19,979
applications where they want to generate

00:07:17,220 --> 00:07:22,050
some data in Hadoop but they want to

00:07:19,979 --> 00:07:25,050
deliver those data eventually through

00:07:22,050 --> 00:07:27,990
the live system so we provide a pipe to

00:07:25,050 --> 00:07:30,180
buffer all those messages that to be

00:07:27,990 --> 00:07:32,729
processed asynchronously and we deliver

00:07:30,180 --> 00:07:34,289
them from the offline data center to the

00:07:32,729 --> 00:07:36,240
online data center and eventually those

00:07:34,289 --> 00:07:40,259
Bri can be consumed by some of the live

00:07:36,240 --> 00:07:41,729
services there so since summer I think

00:07:40,259 --> 00:07:43,949
we have we are collecting all those

00:07:41,729 --> 00:07:47,280
interactive data and we deliver to Ordos

00:07:43,949 --> 00:07:49,620
data to BOS the real-time use case as

00:07:47,280 --> 00:07:54,000
well as the offline use case using a

00:07:49,620 --> 00:07:57,690
single system in terms of volume is

00:07:54,000 --> 00:08:00,090
pretty sizable on a typical day we are

00:07:57,690 --> 00:08:03,000
collecting in the order of tens of

00:08:00,090 --> 00:08:06,360
billions of events which translates to a

00:08:03,000 --> 00:08:10,710
couple of terabytes compress of data in

00:08:06,360 --> 00:08:12,960
our system and on the consumer inside we

00:08:10,710 --> 00:08:14,909
are consuming about five times of

00:08:12,960 --> 00:08:16,500
messages so each message on average

00:08:14,909 --> 00:08:18,210
consumed five times were consuming a

00:08:16,500 --> 00:08:23,820
total of in the order of 50 billion

00:08:18,210 --> 00:08:27,330
messages on a given day so why do people

00:08:23,820 --> 00:08:28,380
choose to use Kafka as a matter of fact

00:08:27,330 --> 00:08:30,300
there are lots of other messaging

00:08:28,380 --> 00:08:33,510
systems in the open source world and

00:08:30,300 --> 00:08:36,289
then the commercial world also I think

00:08:33,510 --> 00:08:38,909
half in terms of feature is lacking

00:08:36,289 --> 00:08:41,070
compared with some of other systems in

00:08:38,909 --> 00:08:42,570
particular we don't really support JMS

00:08:41,070 --> 00:08:45,029
protocol and then we don't have a lot of

00:08:42,570 --> 00:08:46,740
the richness in terms of features but

00:08:45,029 --> 00:08:50,130
here are some of the reasons why people

00:08:46,740 --> 00:08:53,190
choose to use Kafka the first thing is

00:08:50,130 --> 00:08:54,800
Kaka is actually billed as a skier out

00:08:53,190 --> 00:08:57,710
distributed system

00:08:54,800 --> 00:08:59,660
from ground up we have lucky because we

00:08:57,710 --> 00:09:02,120
start this project just only two to

00:08:59,660 --> 00:09:03,950
three years ago and at a time when we

00:09:02,120 --> 00:09:06,290
started we actually have access to

00:09:03,950 --> 00:09:08,900
things like zookeeper which allows us to

00:09:06,290 --> 00:09:13,700
build such a distributed system easier

00:09:08,900 --> 00:09:15,650
and the second thing is unlike some of

00:09:13,700 --> 00:09:17,690
other messaging system which only

00:09:15,650 --> 00:09:20,600
supports us or the real-time use case

00:09:17,690 --> 00:09:24,080
Kafka actually persists or is messages

00:09:20,600 --> 00:09:25,910
on disk so we persist to disk so this

00:09:24,080 --> 00:09:28,160
allows us to support not only the

00:09:25,910 --> 00:09:29,480
real-time use cases where you can

00:09:28,160 --> 00:09:31,730
actually just probably pop for a lot of

00:09:29,480 --> 00:09:34,280
data in memory but also for some of the

00:09:31,730 --> 00:09:36,050
offline use cases for example the et our

00:09:34,280 --> 00:09:38,810
use case is a good offline use case

00:09:36,050 --> 00:09:41,630
because really used for batch analysis

00:09:38,810 --> 00:09:45,170
and sometimes your back your offline

00:09:41,630 --> 00:09:47,360
system can be can be done for a longer

00:09:45,170 --> 00:09:49,100
period of time for repairing and by

00:09:47,360 --> 00:09:51,830
having those data as a persisted it is

00:09:49,100 --> 00:09:55,520
give us a bigger buffer for queuing

00:09:51,830 --> 00:09:57,260
things a third thing we have is we have

00:09:55,520 --> 00:09:59,120
to now system to deliver this high

00:09:57,260 --> 00:10:01,430
throughput and you have seen some of the

00:09:59,120 --> 00:10:04,190
numbers we use at Lincoln but for

00:10:01,430 --> 00:10:06,260
benchmark each each kafka server is

00:10:04,190 --> 00:10:11,210
capable of delivering tens of megabytes

00:10:06,260 --> 00:10:13,340
of throughput propose the sort of a

00:10:11,210 --> 00:10:15,080
publishing side as well as the

00:10:13,340 --> 00:10:18,380
subscribing side depending on your

00:10:15,080 --> 00:10:21,410
configuration last but not least

00:10:18,380 --> 00:10:23,900
socata is a multi subscription system as

00:10:21,410 --> 00:10:26,330
you have seen earlier each published

00:10:23,900 --> 00:10:29,090
message can be potentially consumed by a

00:10:26,330 --> 00:10:31,460
multiple independent consumers and this

00:10:29,090 --> 00:10:33,230
actually useful feature for stream

00:10:31,460 --> 00:10:36,640
processing for example Twitter storm

00:10:33,230 --> 00:10:43,010
system uses Kafka for because of this

00:10:36,640 --> 00:10:44,960
subscription feature so it's a cover

00:10:43,010 --> 00:10:46,880
that the old quick overview of Kafka

00:10:44,960 --> 00:10:49,880
now I'm going to really into the

00:10:46,880 --> 00:10:52,460
internal architecture of Kaka in

00:10:49,880 --> 00:10:53,990
particular how do we support some of the

00:10:52,460 --> 00:10:59,330
features that I mentioned in the

00:10:53,990 --> 00:11:03,040
previous slide so this is a typical sub

00:10:59,330 --> 00:11:03,040
deployment architecture of Kaka

00:11:03,200 --> 00:11:08,510
Kafka we have those server tier and

00:11:06,050 --> 00:11:09,080
client here each our server is called a

00:11:08,510 --> 00:11:11,330
broker

00:11:09,080 --> 00:11:13,760
okay and the for the clients we have two

00:11:11,330 --> 00:11:16,010
types of clients one is produce a client

00:11:13,760 --> 00:11:19,130
which published data to the server to

00:11:16,010 --> 00:11:22,100
the brokers and we have those consumer

00:11:19,130 --> 00:11:25,010
clients which makes a subscription to

00:11:22,100 --> 00:11:27,110
our data stream and one subscription is

00:11:25,010 --> 00:11:29,060
made they just keep fetching data from

00:11:27,110 --> 00:11:31,400
our brokers as you can see from this

00:11:29,060 --> 00:11:33,950
picture because we designed a system as

00:11:31,400 --> 00:11:37,190
sort of scared out or distributed system

00:11:33,950 --> 00:11:39,710
from ground up all of those tiers are

00:11:37,190 --> 00:11:41,900
distributed so we can have a list a set

00:11:39,710 --> 00:11:45,320
of brokers formula Kafka cluster and

00:11:41,900 --> 00:11:49,160
both our producer side and the consumer

00:11:45,320 --> 00:11:50,900
side can be running unmarked thread on

00:11:49,160 --> 00:11:53,660
multiple instances and it's fully

00:11:50,900 --> 00:11:55,010
distributed and you can see orders

00:11:53,660 --> 00:11:56,600
brokering information and consumer

00:11:55,010 --> 00:11:59,420
information they are registering

00:11:56,600 --> 00:12:01,520
zookeeper and yours and by using

00:11:59,420 --> 00:12:03,200
zookeeper I think we can handle some of

00:12:01,520 --> 00:12:05,930
the failures and we can do load

00:12:03,200 --> 00:12:10,370
balancing when some of failures occur to

00:12:05,930 --> 00:12:12,650
keep the load even so before I get into

00:12:10,370 --> 00:12:16,340
some other details this is Regis limited

00:12:12,650 --> 00:12:18,620
interaction of the terminologies Kafka

00:12:16,340 --> 00:12:20,510
provides those massive streams so each

00:12:18,620 --> 00:12:23,240
message stream is defined by this

00:12:20,510 --> 00:12:25,730
concept called topic and then in the

00:12:23,240 --> 00:12:28,070
cover a topic is petitioned each tie

00:12:25,730 --> 00:12:31,970
each topic can have one or more

00:12:28,070 --> 00:12:33,410
partitions underneath of it and all

00:12:31,970 --> 00:12:35,660
those petitions are sort of spread

00:12:33,410 --> 00:12:39,770
around among those brokers that you have

00:12:35,660 --> 00:12:42,350
seen earlier and each partition has you

00:12:39,770 --> 00:12:45,110
as you were see later has a logical log

00:12:42,350 --> 00:12:47,570
associate with it on disk and those

00:12:45,110 --> 00:12:51,020
messages once they are published sort of

00:12:47,570 --> 00:12:55,460
added to this lock and each message has

00:12:51,020 --> 00:12:59,060
a unique identifier which record is

00:12:55,460 --> 00:13:01,820
offset and given offset we can fetch the

00:12:59,060 --> 00:13:04,190
message for you and then you can you can

00:13:01,820 --> 00:13:07,300
make use of this message so this is the

00:13:04,190 --> 00:13:10,360
basic terminology

00:13:07,300 --> 00:13:12,580
in terms of api's so in Kafka I think

00:13:10,360 --> 00:13:15,400
the API is pretty simple we have just to

00:13:12,580 --> 00:13:17,460
settle api's one for a producer and one

00:13:15,400 --> 00:13:19,540
for the consumer see if a producer

00:13:17,460 --> 00:13:22,840
basically what it can do is you can

00:13:19,540 --> 00:13:25,170
create one or more messages and send

00:13:22,840 --> 00:13:28,120
those messages in a single send request

00:13:25,170 --> 00:13:30,250
as you can see in this particular case

00:13:28,120 --> 00:13:33,790
we are trying to create a message for a

00:13:30,250 --> 00:13:35,980
topic one and it was a message yourself

00:13:33,790 --> 00:13:39,370
in this particular case the message is a

00:13:35,980 --> 00:13:42,700
string message one but it can be

00:13:39,370 --> 00:13:46,600
actually any type as long as you define

00:13:42,700 --> 00:13:48,700
an encoder that can convert from your

00:13:46,600 --> 00:13:52,060
original data type into a byte array

00:13:48,700 --> 00:13:53,680
then we can send those data optionally

00:13:52,060 --> 00:13:56,170
you can also give us a key in this case

00:13:53,680 --> 00:13:58,990
is not used the key is now but if you

00:13:56,170 --> 00:14:00,580
want you can provide as a key associate

00:13:58,990 --> 00:14:03,190
with this message then the difference

00:14:00,580 --> 00:14:05,980
there is in addition instead of picking

00:14:03,190 --> 00:14:09,370
a random partition to store this message

00:14:05,980 --> 00:14:12,940
we are going to take this key and hash

00:14:09,370 --> 00:14:15,340
it and determines deterministically pick

00:14:12,940 --> 00:14:16,630
a petition to store your message so some

00:14:15,340 --> 00:14:19,720
other applications want to do this

00:14:16,630 --> 00:14:22,680
addition way based on key because they

00:14:19,720 --> 00:14:25,360
want to group the data in certain way

00:14:22,680 --> 00:14:26,980
now in terms consumer the API is also

00:14:25,360 --> 00:14:29,640
rather simple if you want to consume

00:14:26,980 --> 00:14:33,340
some data again you just pray this

00:14:29,640 --> 00:14:34,900
message stream on a particular topic you

00:14:33,340 --> 00:14:36,970
can actually give us a set of topics or

00:14:34,900 --> 00:14:39,340
you can give us a regex pattern which

00:14:36,970 --> 00:14:41,980
match one or more topics you can pick up

00:14:39,340 --> 00:14:43,990
some of the topics dynamically in

00:14:41,980 --> 00:14:46,440
addition to specifying a topic you can

00:14:43,990 --> 00:14:48,640
also tell us how many substring you want

00:14:46,440 --> 00:14:50,800
out of this stream in this particular

00:14:48,640 --> 00:14:53,530
case we just want to order data to be

00:14:50,800 --> 00:14:55,510
fed into a single stream so once you get

00:14:53,530 --> 00:14:57,160
a stream the stream supports our

00:14:55,510 --> 00:14:58,870
iterator API so you can just score

00:14:57,160 --> 00:15:00,520
iterate around it and as you are

00:14:58,870 --> 00:15:02,920
iterating this you get each other

00:15:00,520 --> 00:15:06,610
message back and you can process it so

00:15:02,920 --> 00:15:08,470
it's pretty simple so in the next slide

00:15:06,610 --> 00:15:11,110
we're going to do is just drew into a

00:15:08,470 --> 00:15:13,630
particular area in terms of throughput I

00:15:11,110 --> 00:15:16,510
want to cover how we design our system

00:15:13,630 --> 00:15:19,270
to deliver this high throughput

00:15:16,510 --> 00:15:23,950
there are few things that we we have

00:15:19,270 --> 00:15:27,370
done to make our throughput throughput

00:15:23,950 --> 00:15:29,740
reasonable the first thing we we have is

00:15:27,370 --> 00:15:32,410
the underlying persistent storage we

00:15:29,740 --> 00:15:35,560
have within the cava is pretty simple if

00:15:32,410 --> 00:15:38,260
you look into a particular broker there

00:15:35,560 --> 00:15:43,300
is a there's a log associated with each

00:15:38,260 --> 00:15:45,850
topic partition each log is formed by

00:15:43,300 --> 00:15:47,400
one or more of those segments okay and

00:15:45,850 --> 00:15:50,950
if you zoom into a particular segment

00:15:47,400 --> 00:15:53,890
it's also pretty simple it has a beta

00:15:50,950 --> 00:15:56,260
beta lock which where we just have

00:15:53,890 --> 00:15:58,420
messages sort of accumulated one after

00:15:56,260 --> 00:16:01,180
another and we have a smaller flower

00:15:58,420 --> 00:16:04,270
which is the index file that maps from

00:16:01,180 --> 00:16:07,360
the ID which is the offset of message

00:16:04,270 --> 00:16:10,000
into the location of their farm okay

00:16:07,360 --> 00:16:13,150
this is a really full read and in terms

00:16:10,000 --> 00:16:14,290
of operation when some message is

00:16:13,150 --> 00:16:16,480
published

00:16:14,290 --> 00:16:19,930
eventually they will be appended to one

00:16:16,480 --> 00:16:22,480
of two the last segment it was in a lock

00:16:19,930 --> 00:16:24,820
and if you are consuming a message

00:16:22,480 --> 00:16:26,830
basically you'll give us an offset to

00:16:24,820 --> 00:16:28,210
see you know from from which officer you

00:16:26,830 --> 00:16:30,580
want to start consuming a message and

00:16:28,210 --> 00:16:32,740
we'll use this index to find the

00:16:30,580 --> 00:16:37,510
location of this message and we'll give

00:16:32,740 --> 00:16:40,360
you this data back normally those slot

00:16:37,510 --> 00:16:42,250
segments are kept for certain configure

00:16:40,360 --> 00:16:45,190
of here of time sometimes you know it

00:16:42,250 --> 00:16:46,180
can be a few days or a week once those

00:16:45,190 --> 00:16:48,130
data get old

00:16:46,180 --> 00:16:51,250
those old segments simply just get

00:16:48,130 --> 00:16:55,300
deleted so as you can see all those log

00:16:51,250 --> 00:16:57,130
segments they are append only and they

00:16:55,300 --> 00:16:58,870
never get updated in place so in terms

00:16:57,130 --> 00:17:01,630
access pattern is actually pretty simple

00:16:58,870 --> 00:17:02,920
and once they are get old they're

00:17:01,630 --> 00:17:06,190
deleted as a whole so it's pretty

00:17:02,920 --> 00:17:08,770
efficient and earlier you have seen our

00:17:06,190 --> 00:17:10,990
producer API we have dispatched API so

00:17:08,770 --> 00:17:12,699
when you send some data you can choose

00:17:10,990 --> 00:17:14,500
to send us a set of messages that a

00:17:12,699 --> 00:17:17,320
single one so that amortized the

00:17:14,500 --> 00:17:19,630
overhead across the network and we have

00:17:17,320 --> 00:17:22,089
the same thing on the consumer side when

00:17:19,630 --> 00:17:25,000
we do a read our consumer library can

00:17:22,089 --> 00:17:27,480
fetch a big chunk of data to amortize

00:17:25,000 --> 00:17:29,490
the RPC overhead

00:17:27,480 --> 00:17:32,580
a third thing we have which is a pretty

00:17:29,490 --> 00:17:34,020
interesting we are using this sand of

00:17:32,580 --> 00:17:37,140
our API it is available in a lot of

00:17:34,020 --> 00:17:40,950
Linux and UNIX system what that allows

00:17:37,140 --> 00:17:45,510
us to do is do this zero copy transfer

00:17:40,950 --> 00:17:48,210
which means it can take some buys in the

00:17:45,510 --> 00:17:51,299
local fire Channel and send it directly

00:17:48,210 --> 00:17:53,910
to a remote socket directly who is not

00:17:51,299 --> 00:17:56,760
going through the application space so

00:17:53,910 --> 00:17:59,960
this is efficient because it give us a

00:17:56,760 --> 00:18:02,910
bypass another kernel overhead it also

00:17:59,960 --> 00:18:04,320
reduce a lot of extra copying back and

00:18:02,910 --> 00:18:07,410
forth between the kernel and a user

00:18:04,320 --> 00:18:10,260
space so that's also pretty big win for

00:18:07,410 --> 00:18:13,950
us last but not least we support

00:18:10,260 --> 00:18:16,380
compression all compression is end to

00:18:13,950 --> 00:18:18,210
end the compression typically happens on

00:18:16,380 --> 00:18:20,700
a producer side where we take a set of

00:18:18,210 --> 00:18:23,070
messages and compress them and those

00:18:20,700 --> 00:18:25,320
compress message set so stays all away

00:18:23,070 --> 00:18:27,179
in a broker and into the consumer

00:18:25,320 --> 00:18:28,799
library it's only when the consumer

00:18:27,179 --> 00:18:31,650
wants to start it ratings messages

00:18:28,799 --> 00:18:33,870
that's when they get decompressed so we

00:18:31,650 --> 00:18:37,650
say a lot of bandwidth to transfer the

00:18:33,870 --> 00:18:39,419
data across those various layers from

00:18:37,650 --> 00:18:41,760
the producer to prop rocker and to the

00:18:39,419 --> 00:18:44,429
consumer and on this it also stays in

00:18:41,760 --> 00:18:48,990
this compressed format so that's how we

00:18:44,429 --> 00:18:51,150
achieve this high throughput so now I

00:18:48,990 --> 00:18:56,190
covered sort of the basic architecture

00:18:51,150 --> 00:18:59,580
of link of Kafka in the next ten or

00:18:56,190 --> 00:19:03,480
twelve slides what I'm going to do is to

00:18:59,580 --> 00:19:05,910
go through our design of adding this

00:19:03,480 --> 00:19:09,000
replication feature and how we extended

00:19:05,910 --> 00:19:13,950
our current system to support this intro

00:19:09,000 --> 00:19:17,370
class the replication so just to start

00:19:13,950 --> 00:19:19,260
with why do we need replication well

00:19:17,370 --> 00:19:22,200
first of all I think just like any

00:19:19,260 --> 00:19:25,770
system our brokers or server can go down

00:19:22,200 --> 00:19:29,340
and most common reason while brokers

00:19:25,770 --> 00:19:32,660
will go down is actually expect it

00:19:29,340 --> 00:19:35,220
because we want to bring down our broker

00:19:32,660 --> 00:19:37,640
because we have to deploy some new code

00:19:35,220 --> 00:19:40,460
we have to deploy some new context

00:19:37,640 --> 00:19:43,780
this is actually a majority of our

00:19:40,460 --> 00:19:45,920
failure cases then occasionally

00:19:43,780 --> 00:19:48,380
individual broker can go down because

00:19:45,920 --> 00:19:52,340
you know your operating system crashes

00:19:48,380 --> 00:19:55,220
or there's a bug in our calculator where

00:19:52,340 --> 00:19:56,630
we have to shut down the server or maybe

00:19:55,220 --> 00:20:00,590
your underlying storage has a problem

00:19:56,630 --> 00:20:03,740
okay now what happens if a broker goes

00:20:00,590 --> 00:20:05,810
down in our currently in our existing

00:20:03,740 --> 00:20:08,540
releases if a broker goes down that

00:20:05,810 --> 00:20:10,130
means all partitions and that broker I

00:20:08,540 --> 00:20:12,860
have none available because we don't

00:20:10,130 --> 00:20:14,420
have redundancy across brokers so that

00:20:12,860 --> 00:20:16,520
means you can't

00:20:14,420 --> 00:20:19,010
the producers cannot publish to these

00:20:16,520 --> 00:20:21,470
brokers and consumers cannot consume or

00:20:19,010 --> 00:20:25,570
catch from those brokers so neither you

00:20:21,470 --> 00:20:28,910
side you and if you have a really hard

00:20:25,570 --> 00:20:30,110
storage problem which means your data is

00:20:28,910 --> 00:20:32,810
gone forever right

00:20:30,110 --> 00:20:34,880
then whatever unconsumed data you have

00:20:32,810 --> 00:20:36,650
on that broker is permanent second its

00:20:34,880 --> 00:20:40,030
permanent lost so you have data loss in

00:20:36,650 --> 00:20:42,500
this case so by adding replication we

00:20:40,030 --> 00:20:44,960
our goal is to improve ours the

00:20:42,500 --> 00:20:49,520
availability as well as durability of

00:20:44,960 --> 00:20:51,560
our system so before I talk about a

00:20:49,520 --> 00:20:53,270
specific limitation of Coffman

00:20:51,560 --> 00:20:57,230
replication it's probably useful to

00:20:53,270 --> 00:20:59,000
revisit this cap theorem so some people

00:20:57,230 --> 00:21:02,210
probably a fan already familiar with

00:20:59,000 --> 00:21:04,970
this is a concept developed proposed to

00:21:02,210 --> 00:21:08,450
by Berkeley professor I think Eric

00:21:04,970 --> 00:21:11,680
Brewer so basically idea is if you are

00:21:08,450 --> 00:21:14,390
building a replicated distributed system

00:21:11,680 --> 00:21:17,300
typically you can only pick two out of

00:21:14,390 --> 00:21:20,060
the three features these three features

00:21:17,300 --> 00:21:23,060
are strong consistency among your

00:21:20,060 --> 00:21:26,360
replicas high availability of your

00:21:23,060 --> 00:21:29,500
system tolerating of natural

00:21:26,360 --> 00:21:33,020
partitioning so among those three things

00:21:29,500 --> 00:21:34,910
you it's it's almost impossible or very

00:21:33,020 --> 00:21:36,830
hard to build a system that can satisfy

00:21:34,910 --> 00:21:39,050
all three but you can build a system

00:21:36,830 --> 00:21:42,110
that sets that two out of three and you

00:21:39,050 --> 00:21:45,320
have to pick which two you want Singh

00:21:42,110 --> 00:21:48,770
Kafka what we picked is the first two we

00:21:45,320 --> 00:21:51,410
pick consistency and availability the

00:21:48,770 --> 00:21:53,930
final amendment reason is our

00:21:51,410 --> 00:21:56,150
intraclass replication is designed for a

00:21:53,930 --> 00:21:59,210
cluster that sits within the same data

00:21:56,150 --> 00:22:01,280
center and in that scenario we expect

00:21:59,210 --> 00:22:03,710
natural partition to be really rare

00:22:01,280 --> 00:22:05,360
within the same data center but in

00:22:03,710 --> 00:22:08,720
return what we can do is we can optimize

00:22:05,360 --> 00:22:11,390
the other tool in particular this means

00:22:08,720 --> 00:22:13,940
we can build this strongly consistent

00:22:11,390 --> 00:22:16,840
replicas meaning those replicas are by

00:22:13,940 --> 00:22:19,850
DeWyze consistent or time not eventually

00:22:16,840 --> 00:22:23,030
we can build a system that's highly

00:22:19,850 --> 00:22:26,110
available and in some our preliminary

00:22:23,030 --> 00:22:29,030
experiments a typical failure over time

00:22:26,110 --> 00:22:32,000
when when there's a failure you have to

00:22:29,030 --> 00:22:34,370
fire over the unavailable in doe during

00:22:32,000 --> 00:22:38,270
this fire over is typically less than a

00:22:34,370 --> 00:22:42,770
few milliseconds which is a which is a

00:22:38,270 --> 00:22:44,420
pretty good so so now we need to extend

00:22:42,770 --> 00:22:46,310
now existing system to support its

00:22:44,420 --> 00:22:50,000
replication so the first thing we have

00:22:46,310 --> 00:22:52,520
to do is to add replicas now a petition

00:22:50,000 --> 00:22:55,160
will have one or more replicas based on

00:22:52,520 --> 00:22:57,620
your configuration and those replicas

00:22:55,160 --> 00:23:01,250
that can see in this picture as spread

00:22:57,620 --> 00:23:03,290
around in those brokers so for those

00:23:01,250 --> 00:23:05,870
little box of the same color they are

00:23:03,290 --> 00:23:07,610
replicas of the same topic partition and

00:23:05,870 --> 00:23:10,040
we try to distribute them so more or

00:23:07,610 --> 00:23:15,260
less evenly amount of brokers for load

00:23:10,040 --> 00:23:17,540
balancing once we have those replicas we

00:23:15,260 --> 00:23:18,800
have to keep them consistent and since

00:23:17,540 --> 00:23:20,840
we want to be on this one consistent

00:23:18,800 --> 00:23:23,320
system we have to make sure they abide

00:23:20,840 --> 00:23:25,790
wise consistent all the time

00:23:23,320 --> 00:23:27,410
so typically the way you do that if you

00:23:25,790 --> 00:23:29,600
want to be or Distronic consistent

00:23:27,410 --> 00:23:33,080
system is you have to pick one or

00:23:29,600 --> 00:23:34,970
wrapping her as the leader so authorites

00:23:33,080 --> 00:23:37,580
have to go through that leader and

00:23:34,970 --> 00:23:40,430
leader is responsible for ordering those

00:23:37,580 --> 00:23:43,040
rights and is responsible for having

00:23:40,430 --> 00:23:44,750
this data propagated to all the resin

00:23:43,040 --> 00:23:47,570
replicas which are for followers

00:23:44,750 --> 00:23:49,940
typically in the exact same order only

00:23:47,570 --> 00:23:52,250
in this way I think you can make sure

00:23:49,940 --> 00:23:54,640
the graphic is identical

00:23:52,250 --> 00:23:58,610
another thing that the leader has do is

00:23:54,640 --> 00:24:01,460
to decide when to commitment a commit a

00:23:58,610 --> 00:24:04,669
piece of data which means when is

00:24:01,460 --> 00:24:07,039
considered a piece of data is has

00:24:04,669 --> 00:24:09,169
reached enough replicas and safe and

00:24:07,039 --> 00:24:11,710
won't be lost when there are failures

00:24:09,169 --> 00:24:15,409
and when they are replicas the failures

00:24:11,710 --> 00:24:18,020
so in the literature there are also a

00:24:15,409 --> 00:24:19,820
couple of ways of doing that how how the

00:24:18,020 --> 00:24:24,590
leader decides how to commit the message

00:24:19,820 --> 00:24:26,600
and the most popular way is to do this

00:24:24,590 --> 00:24:30,679
quorum list commit this actual approach

00:24:26,600 --> 00:24:33,770
used in Apache zookeeper the basic idea

00:24:30,679 --> 00:24:36,559
is when the leader gets a piece of data

00:24:33,770 --> 00:24:38,690
it waits until annajura T of the

00:24:36,559 --> 00:24:42,710
replicas have received that piece of

00:24:38,690 --> 00:24:45,770
data before it can needs that data okay

00:24:42,710 --> 00:24:49,250
CID waits for majority not everybody so

00:24:45,770 --> 00:24:51,159
the plus the plus part of this approach

00:24:49,250 --> 00:24:54,140
is it gives you a pretty good latency

00:24:51,159 --> 00:24:56,929
which means if one of the replicas for

00:24:54,140 --> 00:24:59,570
some reason is slow it doesn't affect

00:24:56,929 --> 00:25:02,720
the time to commit a message because it

00:24:59,570 --> 00:25:05,270
doesn't need everybody but the downside

00:25:02,720 --> 00:25:07,549
of this approach is it means actually

00:25:05,270 --> 00:25:10,669
can tolerate less failure than you would

00:25:07,549 --> 00:25:12,919
hope in it does in particular if you

00:25:10,669 --> 00:25:15,409
have to F +1 replicas you can only

00:25:12,919 --> 00:25:17,390
tolerate F failures that means you have

00:25:15,409 --> 00:25:19,820
three replicas you can only tolerate one

00:25:17,390 --> 00:25:21,080
failure you have two replicas you can

00:25:19,820 --> 00:25:23,750
tolerate a zero failure you can tolerate

00:25:21,080 --> 00:25:25,700
anything in this case so this actually

00:25:23,750 --> 00:25:27,409
although this is OK for a system like

00:25:25,700 --> 00:25:29,630
zookeeper because it's really intended

00:25:27,409 --> 00:25:31,340
to store state information it's not a

00:25:29,630 --> 00:25:33,770
lot of data which means you can actually

00:25:31,340 --> 00:25:37,010
Forge to use more replicas you want more

00:25:33,770 --> 00:25:39,530
redundancy to support failure cases but

00:25:37,010 --> 00:25:41,240
for Kafka we instead of hard for us to

00:25:39,530 --> 00:25:43,640
do that because caucus stores real data

00:25:41,240 --> 00:25:45,740
and you have seen Simon ourian numbers a

00:25:43,640 --> 00:25:48,710
lot of data are flown through Kafka

00:25:45,740 --> 00:25:50,600
having actual replicas will simply

00:25:48,710 --> 00:25:53,090
increase the storage overhead so we

00:25:50,600 --> 00:25:56,150
can't afford to do that so ideally what

00:25:53,090 --> 00:25:57,530
we want to have is you have to f+ one

00:25:56,150 --> 00:25:59,390
rep because we want to be able tolerate

00:25:57,530 --> 00:26:01,410
two F failures you can tolerate more

00:25:59,390 --> 00:26:03,270
failures

00:26:01,410 --> 00:26:06,540
and if that means you have to try a

00:26:03,270 --> 00:26:09,570
little bit of latency that maybe is a

00:26:06,540 --> 00:26:13,830
good trade-off so the next slide I'm

00:26:09,570 --> 00:26:18,810
gonna cover how Kafka did this commit

00:26:13,830 --> 00:26:22,170
protocol so this slide sort of roughly

00:26:18,810 --> 00:26:26,450
describes how we committed data so that

00:26:22,170 --> 00:26:29,700
leader we the leader maintains this

00:26:26,450 --> 00:26:32,220
concept of court eyes are which is in

00:26:29,700 --> 00:26:34,020
sync replicas so this is sort of the set

00:26:32,220 --> 00:26:37,860
of a replica replicas

00:26:34,020 --> 00:26:40,290
there are alive and that have caught up

00:26:37,860 --> 00:26:42,390
fully with the current leader as why

00:26:40,290 --> 00:26:45,630
it's cordless in sync replicas so

00:26:42,390 --> 00:26:48,390
initially when the top when a topic

00:26:45,630 --> 00:26:50,910
petition is created every replica is in

00:26:48,390 --> 00:26:53,130
this in sync replica set okay and

00:26:50,910 --> 00:26:54,930
usually as a message coming in the

00:26:53,130 --> 00:26:57,570
leader basically wait and curious

00:26:54,930 --> 00:27:00,300
message is received by every replica

00:26:57,570 --> 00:27:02,970
okay once the message received by

00:27:00,300 --> 00:27:04,740
everybody it can miss this message then

00:27:02,970 --> 00:27:07,200
at some point he may have failures now

00:27:04,740 --> 00:27:10,020
in particular one of the follower

00:27:07,200 --> 00:27:12,270
epicness could fail right in this case

00:27:10,020 --> 00:27:13,980
the leader can wait forever so what we

00:27:12,270 --> 00:27:18,690
do is in this case the leader will

00:27:13,980 --> 00:27:21,060
recognize one of the follower is gone so

00:27:18,690 --> 00:27:22,950
take it out of this is our because it's

00:27:21,060 --> 00:27:24,150
not it can no longer keep it in sync

00:27:22,950 --> 00:27:28,440
with itself

00:27:24,150 --> 00:27:30,690
okay now delete once it has shrink this

00:27:28,440 --> 00:27:33,000
in sync replica set the leader can start

00:27:30,690 --> 00:27:35,310
committing new messages to the reference

00:27:33,000 --> 00:27:38,490
rest of the replicas using the current

00:27:35,310 --> 00:27:41,250
in sync replica set as you can see in

00:27:38,490 --> 00:27:43,500
this case we don't block for the

00:27:41,250 --> 00:27:45,930
committing of the message but trade of

00:27:43,500 --> 00:27:48,210
is now the system is actually running in

00:27:45,930 --> 00:27:49,740
this under replicated case right because

00:27:48,210 --> 00:27:52,890
there are fewer replicas analyse the

00:27:49,740 --> 00:27:55,080
system is still available so the main

00:27:52,890 --> 00:27:57,950
benefit of this approach is now we do

00:27:55,080 --> 00:28:00,770
can tolerate more failures with F

00:27:57,950 --> 00:28:04,230
replicas we can tolerate F minus 1

00:28:00,770 --> 00:28:07,320
failures in terms of latency it because

00:28:04,230 --> 00:28:09,240
you have to wait for all replicas in the

00:28:07,320 --> 00:28:12,790
steady state right on all the replicas

00:28:09,240 --> 00:28:14,920
are up to date and in sync with the lead

00:28:12,790 --> 00:28:17,050
the latency could be a bit long but

00:28:14,920 --> 00:28:18,460
since coffee is really designed for a

00:28:17,050 --> 00:28:22,420
class of ways in the same data center

00:28:18,460 --> 00:28:24,280
weeks we assume the latency you caused

00:28:22,420 --> 00:28:29,290
by this network delay is relatively

00:28:24,280 --> 00:28:31,420
small so we can tolerate this so we just

00:28:29,290 --> 00:28:34,210
described this protocol these pictures

00:28:31,420 --> 00:28:36,880
are just illustrate how it works in a

00:28:34,210 --> 00:28:40,390
graphical way as we can see in this way

00:28:36,880 --> 00:28:43,690
we have a class of with three brokers 1

00:28:40,390 --> 00:28:47,830
2 3 and there's only one partition with

00:28:43,690 --> 00:28:50,920
3 replicas ok and one of the replicas in

00:28:47,830 --> 00:28:53,560
this case the replica broker one is the

00:28:50,920 --> 00:28:57,450
leader the other two replicas are

00:28:53,560 --> 00:29:00,310
basically followers if you have producer

00:28:57,450 --> 00:29:03,100
once the produced producer published

00:29:00,310 --> 00:29:05,410
message the message first flows to the

00:29:03,100 --> 00:29:07,480
leader in this case on broker one and

00:29:05,410 --> 00:29:10,210
the leader takes this message and just a

00:29:07,480 --> 00:29:12,310
pansy to his local lock and the

00:29:10,210 --> 00:29:14,410
followers just keeps pulling data from

00:29:12,310 --> 00:29:16,510
the leader and once the follow received

00:29:14,410 --> 00:29:18,850
those data and it rises to its own local

00:29:16,510 --> 00:29:21,490
lot okay and the followers are putting

00:29:18,850 --> 00:29:23,530
those data in exactly same order as as

00:29:21,490 --> 00:29:25,990
you notice that if there are two

00:29:23,530 --> 00:29:28,300
followers on multiple followers falling

00:29:25,990 --> 00:29:30,400
from the same leader they can do the

00:29:28,300 --> 00:29:33,660
pooling so independently concurrently

00:29:30,400 --> 00:29:37,930
because they don't depend on each other

00:29:33,660 --> 00:29:40,210
once the leader has realized that all

00:29:37,930 --> 00:29:43,300
the followers have received that that

00:29:40,210 --> 00:29:44,320
message now it can committed it the data

00:29:43,300 --> 00:29:48,100
that's in step 3

00:29:44,320 --> 00:29:51,870
ok and produce actually have a choice of

00:29:48,100 --> 00:29:54,040
when to receive the acknowledgement and

00:29:51,870 --> 00:29:57,340
it's actually summarized in the

00:29:54,040 --> 00:30:00,160
following table the producer can choose

00:29:57,340 --> 00:30:02,440
not to receive any acknowledgement at

00:30:00,160 --> 00:30:05,170
all basically you will just do wine

00:30:02,440 --> 00:30:08,140
publishing in this case it's really

00:30:05,170 --> 00:30:11,710
optimizing for latency because in this

00:30:08,140 --> 00:30:14,200
case it doesn't take any network

00:30:11,710 --> 00:30:17,410
overhead to even publish the data but

00:30:14,200 --> 00:30:18,760
trade-off is now if there's any failure

00:30:17,410 --> 00:30:21,400
that could be data loss in particular

00:30:18,760 --> 00:30:23,620
there may be some data that you just

00:30:21,400 --> 00:30:24,140
buffer on the producer side is not even

00:30:23,620 --> 00:30:25,580
standard

00:30:24,140 --> 00:30:29,720
our kid right because you're not waiting

00:30:25,580 --> 00:30:33,410
for an acknowledgment a producer can

00:30:29,720 --> 00:30:36,470
also choose to write to wait until the

00:30:33,410 --> 00:30:39,320
leader has received a message okay in

00:30:36,470 --> 00:30:40,940
this case the latency will be longer is

00:30:39,320 --> 00:30:45,140
now you have to wait for at least one

00:30:40,940 --> 00:30:48,170
round RPC trip right the data loss will

00:30:45,140 --> 00:30:50,360
be less but you can still happen because

00:30:48,170 --> 00:30:51,740
what can happen is maybe a message has

00:30:50,360 --> 00:30:53,090
only be received by the leader but

00:30:51,740 --> 00:30:54,320
hasn't been propagated to the followers

00:30:53,090 --> 00:30:56,630
right in this case it hasn't been

00:30:54,320 --> 00:30:58,780
committed so when there's a failure a

00:30:56,630 --> 00:31:01,910
small number of misses course to be lost

00:30:58,780 --> 00:31:07,400
so the last option if you really want

00:31:01,910 --> 00:31:08,300
your data to be truly assistant and can

00:31:07,400 --> 00:31:11,480
tolerate all those failures

00:31:08,300 --> 00:31:13,580
you can choose to wait until to receive

00:31:11,480 --> 00:31:16,160
an acknowledgement when the message is

00:31:13,580 --> 00:31:19,220
committed which means it has received by

00:31:16,160 --> 00:31:21,130
order replicas that are in sync and in

00:31:19,220 --> 00:31:24,020
this particular case it's all replicas

00:31:21,130 --> 00:31:27,320
in this case the delay will be I'll be

00:31:24,020 --> 00:31:30,350
longer and because you have to take two

00:31:27,320 --> 00:31:35,870
round-trip network overhead so there's a

00:31:30,350 --> 00:31:37,420
question the question is is it possible

00:31:35,870 --> 00:31:39,410
to have a cast data replication

00:31:37,420 --> 00:31:42,230
currently with we haven't thought about

00:31:39,410 --> 00:31:44,450
that but in the future we could think

00:31:42,230 --> 00:31:45,500
about doing something like that now we

00:31:44,450 --> 00:31:49,760
are just wanting to get a basic

00:31:45,500 --> 00:31:50,870
replication working do we want to take a

00:31:49,760 --> 00:31:53,540
question now do we want to take your

00:31:50,870 --> 00:31:55,580
questions towards the end okay let's

00:31:53,540 --> 00:32:00,320
leave it to the end okay but save your

00:31:55,580 --> 00:32:03,350
questions thanks ok so this is just to

00:32:00,320 --> 00:32:05,750
extend this picture to multiple

00:32:03,350 --> 00:32:06,980
partitions multiple topics in reality a

00:32:05,750 --> 00:32:08,390
thing that could be multiple partitions

00:32:06,980 --> 00:32:12,920
and then they each have its own leader

00:32:08,390 --> 00:32:15,680
and the same logic just happens and our

00:32:12,920 --> 00:32:17,840
Gori is to just spread the leaders among

00:32:15,680 --> 00:32:24,130
those partitions more or less evenly

00:32:17,840 --> 00:32:26,210
among our brokers so as we talk about

00:32:24,130 --> 00:32:29,090
normal case when there's no failure in

00:32:26,210 --> 00:32:31,340
the next couple slides or just describe

00:32:29,090 --> 00:32:34,130
what happens when they're failures there

00:32:31,340 --> 00:32:35,960
could be failures in the followers when

00:32:34,130 --> 00:32:37,100
this replication thing is happening that

00:32:35,960 --> 00:32:39,140
could be failures

00:32:37,100 --> 00:32:41,390
on the leaders when it's replicating is

00:32:39,140 --> 00:32:45,920
happening so this slice talks about what

00:32:41,390 --> 00:32:47,840
happens when the follower fares so to

00:32:45,920 --> 00:32:51,560
handle the follow-up failure what we do

00:32:47,840 --> 00:32:54,770
is the leader maintains the offset of

00:32:51,560 --> 00:32:57,620
the last committed message okay this

00:32:54,770 --> 00:32:59,410
last committee offset is propagated from

00:32:57,620 --> 00:33:01,580
the leader to all the followers

00:32:59,410 --> 00:33:05,930
asynchronously and they are also

00:33:01,580 --> 00:33:06,860
checkpoint to disk periodically so if a

00:33:05,930 --> 00:33:09,890
follower

00:33:06,860 --> 00:33:13,190
fails and it comes back the first thing

00:33:09,890 --> 00:33:16,370
it does is to recover from disk its last

00:33:13,190 --> 00:33:19,490
committed offset okay next thing it does

00:33:16,370 --> 00:33:21,380
is truncatus lock to that last committee

00:33:19,490 --> 00:33:23,960
off set because at two point it knows

00:33:21,380 --> 00:33:27,710
everything before that it's safe right

00:33:23,960 --> 00:33:29,510
it's committed then it orjust refat or

00:33:27,710 --> 00:33:31,340
data since that offset from current

00:33:29,510 --> 00:33:33,460
leader and it tries to catch up from the

00:33:31,340 --> 00:33:36,230
current leader once it's fully caught up

00:33:33,460 --> 00:33:38,540
this replica will be added back to this

00:33:36,230 --> 00:33:41,060
is a set which is in sync replica set

00:33:38,540 --> 00:33:43,430
now we are back to the fully replicated

00:33:41,060 --> 00:33:46,580
mode okay that's how would we cover from

00:33:43,430 --> 00:33:49,970
Portal Theatre what happens to leader

00:33:46,580 --> 00:33:51,680
failures so to handle leader failure so

00:33:49,970 --> 00:33:55,040
what we did is we use the approach

00:33:51,680 --> 00:33:58,390
inspired by another Apache project

00:33:55,040 --> 00:34:01,790
called helix which is also sort of

00:33:58,390 --> 00:34:04,250
developed and contributed to Apache at

00:34:01,790 --> 00:34:08,300
Lincoln and there's a helix talk in

00:34:04,250 --> 00:34:10,669
Thursday so the idea there is to use one

00:34:08,300 --> 00:34:14,389
of the Broker that serves as a

00:34:10,669 --> 00:34:16,760
controller for all those detection of

00:34:14,389 --> 00:34:19,250
failures as well as the electing new

00:34:16,760 --> 00:34:20,810
leaders the banning of that having an

00:34:19,250 --> 00:34:22,340
embedded controller is to reduce the

00:34:20,810 --> 00:34:23,899
number of washers they have a set in

00:34:22,340 --> 00:34:27,050
zookeeper so it reduces the zookeeper

00:34:23,899 --> 00:34:30,800
low so we have a 1 or broker service or

00:34:27,050 --> 00:34:33,139
controller and it read register or

00:34:30,800 --> 00:34:37,879
watchers said it will be notified when

00:34:33,139 --> 00:34:40,970
any of the broker goes down so once the

00:34:37,879 --> 00:34:43,550
controller knows some rumor is down it

00:34:40,970 --> 00:34:46,760
will figure out what other leaders are

00:34:43,550 --> 00:34:48,050
on those fire brokers those for those

00:34:46,760 --> 00:34:49,600
leaders you have to elect new ones

00:34:48,050 --> 00:34:53,350
because they are no longer available

00:34:49,600 --> 00:34:54,940
and then the progress pretty simple if

00:34:53,350 --> 00:34:58,360
the controller base secret tries to

00:34:54,940 --> 00:35:03,100
select another replica that's in this in

00:34:58,360 --> 00:35:07,390
sync replica set as the new leader okay

00:35:03,100 --> 00:35:09,880
and by convention because all the

00:35:07,390 --> 00:35:12,250
replicas in this instinct replicas said

00:35:09,880 --> 00:35:14,800
a guaranteed to have received or the

00:35:12,250 --> 00:35:17,080
committed messages so in this fire over

00:35:14,800 --> 00:35:22,300
we guarantee as long as a message is

00:35:17,080 --> 00:35:24,400
committed its it will exist in the new

00:35:22,300 --> 00:35:26,530
leaders log so we are guaranteed we

00:35:24,400 --> 00:35:28,600
won't lose those committed messages so

00:35:26,530 --> 00:35:32,050
that's how we guarantee the strong

00:35:28,600 --> 00:35:34,840
consistency under failure boasters

00:35:32,050 --> 00:35:36,670
leader and is our information stored in

00:35:34,840 --> 00:35:38,800
zookeeper is religious for the failure

00:35:36,670 --> 00:35:40,990
over of controllers and we have to

00:35:38,800 --> 00:35:43,450
recover when a controller tears we had

00:35:40,990 --> 00:35:45,640
read this information and vows this

00:35:43,450 --> 00:35:48,640
leader and is our information we expect

00:35:45,640 --> 00:35:50,080
them to change relatively infrequently

00:35:48,640 --> 00:35:53,740
because only happens when failure occurs

00:35:50,080 --> 00:35:56,260
and the failures in general area okay so

00:35:53,740 --> 00:35:58,120
now I've covered the main protocol in

00:35:56,260 --> 00:36:00,640
the next slide I'm just going to walk

00:35:58,120 --> 00:36:01,930
through with you a specific example so

00:36:00,640 --> 00:36:04,930
that we can hopefully have a better

00:36:01,930 --> 00:36:08,410
understanding of how this varies failure

00:36:04,930 --> 00:36:11,400
cases will work okay so this is the

00:36:08,410 --> 00:36:15,370
example which is focusing on a single

00:36:11,400 --> 00:36:20,070
partition with three replicas a B and C

00:36:15,370 --> 00:36:22,420
okay one of the replicas replicas a is

00:36:20,070 --> 00:36:24,280
initially is the leader and initial

00:36:22,420 --> 00:36:26,080
order replicas are in sync with each

00:36:24,280 --> 00:36:29,500
other they are there or in this is our

00:36:26,080 --> 00:36:31,690
set now the leader can start accepting

00:36:29,500 --> 00:36:34,710
messages in this case you see the leader

00:36:31,690 --> 00:36:39,610
have received three messages m1 m2 m3

00:36:34,710 --> 00:36:43,120
okay at this point only m1 has been

00:36:39,610 --> 00:36:46,330
propagated to order replicas Sony and m1

00:36:43,120 --> 00:36:49,630
can be committed okay so the leader can

00:36:46,330 --> 00:36:51,520
commit m1 but not m2 and m3 you see this

00:36:49,630 --> 00:36:54,190
literal blue arrow

00:36:51,520 --> 00:36:56,950
this points to the last committed offset

00:36:54,190 --> 00:36:59,950
okay so not only am one is committed but

00:36:56,950 --> 00:37:02,290
m2 m3 or not now I imagine at this point

00:36:59,950 --> 00:37:06,110
just right right at this moment

00:37:02,290 --> 00:37:08,300
replicas atheros what happens so when

00:37:06,110 --> 00:37:11,360
this happens what's most important thing

00:37:08,300 --> 00:37:14,030
to do is we we need to preserve M 1

00:37:11,360 --> 00:37:15,830
because M 1 is committed right we can

00:37:14,030 --> 00:37:17,600
lose this data M 2 M 3

00:37:15,830 --> 00:37:20,840
they have been committed so it's

00:37:17,600 --> 00:37:23,630
actually okay to lose that if you if

00:37:20,840 --> 00:37:26,750
need to but you don't have to be

00:37:23,630 --> 00:37:29,420
extremely careful to protect those so

00:37:26,750 --> 00:37:31,520
that's what happens let's say just right

00:37:29,420 --> 00:37:34,850
up at a moment replica 8 fails

00:37:31,520 --> 00:37:36,590
okay so now the first thing we have to

00:37:34,850 --> 00:37:39,560
do is we have to select a new leader in

00:37:36,590 --> 00:37:42,320
this case we can select either B or C as

00:37:39,560 --> 00:37:44,450
new leader because both of now in this

00:37:42,320 --> 00:37:46,670
in syncretic asset let's say in this

00:37:44,450 --> 00:37:47,570
case we picked B but actually we can

00:37:46,670 --> 00:37:49,850
pick C as well

00:37:47,570 --> 00:37:53,960
but what's most important to know notice

00:37:49,850 --> 00:37:56,150
here is both B and C have m1 which is

00:37:53,960 --> 00:37:59,360
this committed message we don't lose

00:37:56,150 --> 00:38:02,050
those a little bit interesting and

00:37:59,360 --> 00:38:05,210
expecting your wrists realized here is

00:38:02,050 --> 00:38:08,450
replica B in this case has to further

00:38:05,210 --> 00:38:10,510
commit message m2 even though it wasn't

00:38:08,450 --> 00:38:13,880
committed by the previous leader

00:38:10,510 --> 00:38:17,390
the reason we're to do that is when a

00:38:13,880 --> 00:38:20,360
new leader is your active he knows it's

00:38:17,390 --> 00:38:21,850
log has all committed messages but it

00:38:20,360 --> 00:38:24,080
doesn't know which ones are uncommitted

00:38:21,850 --> 00:38:26,840
so just to be on the safe side you

00:38:24,080 --> 00:38:28,640
choose to commit everything it has it's

00:38:26,840 --> 00:38:31,760
in this case why that's why it has to

00:38:28,640 --> 00:38:34,340
commit this side message m2 now from the

00:38:31,760 --> 00:38:37,750
clients perspective if a producer is

00:38:34,340 --> 00:38:40,460
waiting for message m2 to be committed

00:38:37,750 --> 00:38:43,640
wire leader wear a replica a was the

00:38:40,460 --> 00:38:45,650
leader whatever happened is that

00:38:43,640 --> 00:38:48,130
producer will get exception so that

00:38:45,650 --> 00:38:51,050
exception basically towers this client

00:38:48,130 --> 00:38:53,690
that particular message may or may not

00:38:51,050 --> 00:38:55,580
be committed so it's up to the

00:38:53,690 --> 00:38:56,720
application to have a deal with in this

00:38:55,580 --> 00:38:59,690
particular eyes we happen to have

00:38:56,720 --> 00:39:02,240
committed but it's up to the producer to

00:38:59,690 --> 00:39:04,490
try the whole system to figure this out

00:39:02,240 --> 00:39:07,000
but most important thing is we haven't

00:39:04,490 --> 00:39:10,280
lost any committed messages which is m1

00:39:07,000 --> 00:39:13,380
now new message can be published to our

00:39:10,280 --> 00:39:15,420
system let's say in SK and 4 and 5 and

00:39:13,380 --> 00:39:18,210
we can continue to convey those messages

00:39:15,420 --> 00:39:21,750
and those committed messages now are

00:39:18,210 --> 00:39:25,260
present in replica BMC but ASD ordinance

00:39:21,750 --> 00:39:29,400
is not in a yet at some point replica a

00:39:25,260 --> 00:39:33,060
comes back okay when a comes back the

00:39:29,400 --> 00:39:35,280
first thing it does is to truncate its

00:39:33,060 --> 00:39:38,280
lock to the last committed offset in

00:39:35,280 --> 00:39:43,800
this case its m1 so basically removed

00:39:38,280 --> 00:39:46,440
both m2m m3 okay and then after they

00:39:43,800 --> 00:39:49,230
basically refetch is data from the

00:39:46,440 --> 00:39:51,750
colonel leader which is replica b and a

00:39:49,230 --> 00:39:55,230
where we get order M two M four and five

00:39:51,750 --> 00:39:57,360
and once the when is fully synchrony

00:39:55,230 --> 00:40:00,090
added to this in sync replica set now

00:39:57,360 --> 00:40:01,320
you can see all the replicas in this

00:40:00,090 --> 00:40:04,080
fully synchronous

00:40:01,320 --> 00:40:05,640
so as you can see in the end picture

00:40:04,080 --> 00:40:09,000
with all those failures happening in the

00:40:05,640 --> 00:40:11,520
middle or committed messages meaning M

00:40:09,000 --> 00:40:13,380
one M two M four and five they are all

00:40:11,520 --> 00:40:15,870
preserved in order epoch as an order

00:40:13,380 --> 00:40:19,260
replicas as they're identical for the

00:40:15,870 --> 00:40:21,900
messages not committed m3 they don't

00:40:19,260 --> 00:40:23,400
need it doesn't exist in all replicas so

00:40:21,900 --> 00:40:25,920
that's how we keep all those replicas

00:40:23,400 --> 00:40:30,990
consistent and how we recover from

00:40:25,920 --> 00:40:35,370
various failures so now covered sort of

00:40:30,990 --> 00:40:38,130
the basically design of this replication

00:40:35,370 --> 00:40:40,710
now in the next few slides I'm going to

00:40:38,130 --> 00:40:44,940
give just to quickly show you some of

00:40:40,710 --> 00:40:46,860
the performance numbers we have so this

00:40:44,940 --> 00:40:49,710
is the setup so we have a very simple

00:40:46,860 --> 00:40:51,960
setup we have three brokers we have one

00:40:49,710 --> 00:40:54,770
topic and one partition

00:40:51,960 --> 00:40:57,630
only this petition has three replicas

00:40:54,770 --> 00:41:00,210
there's one replica on each of the three

00:40:57,630 --> 00:41:02,340
brokers and when we publish message and

00:41:00,210 --> 00:41:06,120
consumers we use a match the size of K

00:41:02,340 --> 00:41:08,280
bytes to these our basic numbers the

00:41:06,120 --> 00:41:10,050
first slide I will show you is the

00:41:08,280 --> 00:41:12,150
trade-off between this latency and

00:41:10,050 --> 00:41:14,670
durability so earlier we had this slide

00:41:12,150 --> 00:41:17,280
talking about when the producer and

00:41:14,670 --> 00:41:19,530
choose to receive acknowledgement right

00:41:17,280 --> 00:41:21,780
there are different choices so this

00:41:19,530 --> 00:41:23,730
slide just counted by this number so if

00:41:21,780 --> 00:41:25,470
the producer doesn't want wait for any

00:41:23,730 --> 00:41:27,660
acknowledgment

00:41:25,470 --> 00:41:30,270
when you publish message that message

00:41:27,660 --> 00:41:33,750
can be published under point three

00:41:30,270 --> 00:41:36,270
milliseconds because we don't want want

00:41:33,750 --> 00:41:39,000
to wait for any RPC call these are

00:41:36,270 --> 00:41:41,760
pretty fast but the trade-off cost there

00:41:39,000 --> 00:41:45,000
could be some amount of data loss if

00:41:41,760 --> 00:41:47,310
there's a failure if you pick to wait

00:41:45,000 --> 00:41:48,330
for just a leader now your latency live

00:41:47,310 --> 00:41:51,570
is longer it's in the order of

00:41:48,330 --> 00:41:54,390
millisecond I still not too bad but it's

00:41:51,570 --> 00:41:57,810
longer and now your data loss will be

00:41:54,390 --> 00:41:59,910
less but it can still happen if you

00:41:57,810 --> 00:42:02,310
choose to wait for a message to be truly

00:41:59,910 --> 00:42:03,900
committed now to publish message you

00:42:02,310 --> 00:42:06,750
have to wait for about two milliseconds

00:42:03,900 --> 00:42:08,400
because now you're doubled to all we

00:42:06,750 --> 00:42:11,010
have in terms Network you not only have

00:42:08,400 --> 00:42:13,290
to wait for the data to be received by

00:42:11,010 --> 00:42:15,660
the leader but they also have to be the

00:42:13,290 --> 00:42:17,820
message it also has to be received by

00:42:15,660 --> 00:42:20,940
the followers as well okay so that's why

00:42:17,820 --> 00:42:22,860
it's a bit longer but the benefit now is

00:42:20,940 --> 00:42:25,800
with all those faders you can guarantee

00:42:22,860 --> 00:42:27,330
your message is no longer lost so this

00:42:25,800 --> 00:42:30,630
is the trade-off between its latency and

00:42:27,330 --> 00:42:31,859
the durability the next couple slides

00:42:30,630 --> 00:42:34,740
I'm going to show you some of the

00:42:31,859 --> 00:42:39,930
throughput numbers so throughput is a

00:42:34,740 --> 00:42:42,000
lipid orthogonal to latency when when

00:42:39,930 --> 00:42:43,500
you consider latency I think there's a

00:42:42,000 --> 00:42:46,260
trade-off between is latency and

00:42:43,500 --> 00:42:49,380
durability but given the same latency

00:42:46,260 --> 00:42:51,800
for all these options relative to the

00:42:49,380 --> 00:42:55,440
producer acknowledgment you can actually

00:42:51,800 --> 00:42:56,040
improve your throughput by doing two

00:42:55,440 --> 00:42:57,720
different things

00:42:56,040 --> 00:43:01,050
one is you can improve your throughput

00:42:57,720 --> 00:43:03,570
by try to sending more send more data

00:43:01,050 --> 00:43:05,490
for RPC request you base in a batch more

00:43:03,570 --> 00:43:08,609
data for RPC Rufus that's one way that

00:43:05,490 --> 00:43:09,780
you can amortize the network latency the

00:43:08,609 --> 00:43:12,210
second thing you can do to improve

00:43:09,780 --> 00:43:15,270
throughput is to have more concurrent

00:43:12,210 --> 00:43:18,000
clients get more clients then jointly

00:43:15,270 --> 00:43:18,869
they can consume or publish more data so

00:43:18,000 --> 00:43:21,240
there's another way you can improve

00:43:18,869 --> 00:43:22,100
throughput so that's what you're see in

00:43:21,240 --> 00:43:25,440
the next slide

00:43:22,100 --> 00:43:28,260
so this slide what we are showing here

00:43:25,440 --> 00:43:33,270
is the throughput in terms megabytes per

00:43:28,260 --> 00:43:35,730
second we get versus the number of 1k by

00:43:33,270 --> 00:43:38,820
messages we are sending per RPC request

00:43:35,730 --> 00:43:42,120
on the x-axis that shows how

00:43:38,820 --> 00:43:43,890
messages you are sending her standard

00:43:42,120 --> 00:43:46,020
request the y-axis shows you the

00:43:43,890 --> 00:43:48,240
megabytes per second as you can see for

00:43:46,020 --> 00:43:49,800
all those options whether you the

00:43:48,240 --> 00:43:51,510
producer doesn't want to wait for any

00:43:49,800 --> 00:43:52,620
acknowledgment or wait for the leader

00:43:51,510 --> 00:43:54,690
I'll wait for the message to be

00:43:52,620 --> 00:43:56,760
committed they can or get a better

00:43:54,690 --> 00:44:01,890
throughput as you're sending more and

00:43:56,760 --> 00:44:03,720
more data per request and even for the

00:44:01,890 --> 00:44:05,070
slowest one which is have to wait for

00:44:03,720 --> 00:44:07,680
the message to be committed they can

00:44:05,070 --> 00:44:10,350
still deliver tens of megabytes of

00:44:07,680 --> 00:44:12,240
throughput on a single partition of

00:44:10,350 --> 00:44:15,360
course even more attentions more brokers

00:44:12,240 --> 00:44:18,300
that throughput will be just growing

00:44:15,360 --> 00:44:21,390
linearly the second thing you can do of

00:44:18,300 --> 00:44:22,920
course is to fix the number of the

00:44:21,390 --> 00:44:25,730
matter the no message do you send a

00:44:22,920 --> 00:44:29,460
request but increased number of

00:44:25,730 --> 00:44:30,990
concurrent producers you have still here

00:44:29,460 --> 00:44:33,600
you will see the same effect by

00:44:30,990 --> 00:44:37,920
increasing the concurrent producers from

00:44:33,600 --> 00:44:39,900
1 to 20 we can gradually drive through

00:44:37,920 --> 00:44:45,990
her from less than a megabyte for almost

00:44:39,900 --> 00:44:47,730
20 or 30 megabytes and of course if you

00:44:45,990 --> 00:44:50,070
don't want 4 wait for acknowledgement

00:44:47,730 --> 00:44:53,100
used to get the highest Luka but even

00:44:50,070 --> 00:44:54,480
for the case where you have to wait for

00:44:53,100 --> 00:44:58,020
the message to be committed you can

00:44:54,480 --> 00:44:59,400
still get some size or throughput so

00:44:58,020 --> 00:45:02,610
this is my last slide in terms of

00:44:59,400 --> 00:45:04,800
performance this is the super on

00:45:02,610 --> 00:45:06,270
consumer side on a consumer side you can

00:45:04,800 --> 00:45:09,270
do the same trick and here I'm just

00:45:06,270 --> 00:45:10,860
showing that what's the throughput in

00:45:09,270 --> 00:45:13,950
terms consumption can get in terms of

00:45:10,860 --> 00:45:16,410
megabytes per second as you are varying

00:45:13,950 --> 00:45:19,230
the size you are catching per RPC

00:45:16,410 --> 00:45:22,200
request as you can see as we scared to

00:45:19,230 --> 00:45:24,240
fantasise to a megabyte we pretty much

00:45:22,200 --> 00:45:26,460
can saturate the one gigabit link

00:45:24,240 --> 00:45:29,820
between the client and server through

00:45:26,460 --> 00:45:32,370
pulleys pretty good okay so this one is

00:45:29,820 --> 00:45:35,250
my last slide so before I open it up for

00:45:32,370 --> 00:45:39,120
Q&A just want to give you a quick update

00:45:35,250 --> 00:45:42,240
of the status of our next cut release

00:45:39,120 --> 00:45:46,110
which is 0.8 release which supports this

00:45:42,240 --> 00:45:48,000
intraclass replication feature we are in

00:45:46,110 --> 00:45:49,180
the final phase of sterilizing and

00:45:48,000 --> 00:45:52,480
debugging it

00:45:49,180 --> 00:45:56,109
and we hope we can release that in March

00:45:52,480 --> 00:45:58,329
sometime and after age there are various

00:45:56,109 --> 00:46:00,520
places where can improve performance and

00:45:58,329 --> 00:46:03,730
through for further which we'll do that

00:46:00,520 --> 00:46:05,740
in a falling point release if you want

00:46:03,730 --> 00:46:08,380
to check out more about cough that is

00:46:05,740 --> 00:46:09,730
the website or you can go to where we

00:46:08,380 --> 00:46:13,480
have the design dog some of the

00:46:09,730 --> 00:46:17,260
QuickStart and we work um people to try

00:46:13,480 --> 00:46:20,380
it out and give us some feedback if

00:46:17,260 --> 00:46:22,660
you're interested there is a clock and

00:46:20,380 --> 00:46:25,089
meet up this evening supposedly to be

00:46:22,660 --> 00:46:26,770
around 8:00 and I don't know the exact

00:46:25,089 --> 00:46:30,029
room here but I think it'd be around

00:46:26,770 --> 00:46:32,559
this area so if you're interested we can

00:46:30,029 --> 00:46:35,049
all you have more detailed questions and

00:46:32,559 --> 00:46:37,329
be happy to talk to some of you there

00:46:35,049 --> 00:46:40,690
and we can have a more detailed sort of

00:46:37,329 --> 00:46:44,760
dive in of caca okay so that's it for me

00:46:40,690 --> 00:46:44,760
any questions yes

00:46:51,130 --> 00:46:57,590
to reach Kafka is that would that be for

00:46:55,250 --> 00:47:00,020
zoo keeper a particular broker or what

00:46:57,590 --> 00:47:02,660
is the producer consumer directly

00:47:00,020 --> 00:47:07,070
communicating with right so in terms of

00:47:02,660 --> 00:47:08,840
configuration I think in terms of on the

00:47:07,070 --> 00:47:11,750
producer side so now there's a limit

00:47:08,840 --> 00:47:13,550
that is kripstly on a producer side for

00:47:11,750 --> 00:47:17,930
a producer to connect to the broker the

00:47:13,550 --> 00:47:20,030
producer needs to know like a broker

00:47:17,930 --> 00:47:22,880
list it doesn't need to know all the

00:47:20,030 --> 00:47:28,130
brokers but it has know at least one lie

00:47:22,880 --> 00:47:31,880
broker and we use that information you

00:47:28,130 --> 00:47:34,010
are air for the row for the producer to

00:47:31,880 --> 00:47:35,750
issue some get metadata request any

00:47:34,010 --> 00:47:37,460
broken can issue cancer of those

00:47:35,750 --> 00:47:39,230
metadata requests basically gives you

00:47:37,460 --> 00:47:41,750
more detailed information for this

00:47:39,230 --> 00:47:43,610
particular topic what eyes partitions

00:47:41,750 --> 00:47:44,420
and where the petitions hosted who is

00:47:43,610 --> 00:47:47,810
the current leader that is should

00:47:44,420 --> 00:47:50,090
connect to yes it's right on the

00:47:47,810 --> 00:47:53,960
consumer side today the consumer has to

00:47:50,090 --> 00:47:56,900
know the zookeeper but we we may evolve

00:47:53,960 --> 00:47:59,870
the API in the future to to reduce that

00:47:56,900 --> 00:48:01,520
dependency okay any other questions can

00:47:59,870 --> 00:48:04,550
go back to the slide when you already

00:48:01,520 --> 00:48:07,400
have the leader failover thing yes you

00:48:04,550 --> 00:48:09,140
want to be go back to that right just

00:48:07,400 --> 00:48:14,990
curious if you do any kind of like smart

00:48:09,140 --> 00:48:16,490
election because so let's say a leader

00:48:14,990 --> 00:48:19,700
fails right and you said you have like a

00:48:16,490 --> 00:48:21,800
replica PNC and in your case you like it

00:48:19,700 --> 00:48:23,570
be how does the election works I have

00:48:21,800 --> 00:48:24,830
like maybe I can just go through this

00:48:23,570 --> 00:48:26,120
right you have some kind of like yet

00:48:24,830 --> 00:48:27,560
time stamp or something so you could do

00:48:26,120 --> 00:48:29,240
like a smarter election which would

00:48:27,560 --> 00:48:31,040
potentially give involved multiple run

00:48:29,240 --> 00:48:32,740
trips but you could find like a

00:48:31,040 --> 00:48:36,200
potentially like a replica which is like

00:48:32,740 --> 00:48:37,820
the furthest down yes yes that's

00:48:36,200 --> 00:48:39,070
definitely possible so that's a good

00:48:37,820 --> 00:48:42,950
observation

00:48:39,070 --> 00:48:44,840
so when the current leader failures in

00:48:42,950 --> 00:48:47,180
our protocol we just we can we just

00:48:44,840 --> 00:48:50,540
randomly select one of the replicas you

00:48:47,180 --> 00:48:53,120
know sr as the new leader which is okay

00:48:50,540 --> 00:48:56,050
because at least every recognizer has to

00:48:53,120 --> 00:48:59,240
commit it data so we don't lose them but

00:48:56,050 --> 00:49:01,580
some of the replicas may be more further

00:48:59,240 --> 00:49:03,890
along than others so if you want to

00:49:01,580 --> 00:49:07,280
preserve more data maybe what it should

00:49:03,890 --> 00:49:10,310
do is to pick the replica that has more

00:49:07,280 --> 00:49:12,290
data in it okay so it is possible we

00:49:10,310 --> 00:49:15,490
haven't thought about how to implement

00:49:12,290 --> 00:49:17,810
that it does add a bit of complication

00:49:15,490 --> 00:49:19,640
when the internal action that happens

00:49:17,810 --> 00:49:21,470
because when it happens you probably

00:49:19,640 --> 00:49:23,360
need another round of coordination to

00:49:21,470 --> 00:49:26,450
figure out who is Leung is now you have

00:49:23,360 --> 00:49:31,030
to worry about things what yeah what

00:49:26,450 --> 00:49:33,140
about do I wait for all the surviving

00:49:31,030 --> 00:49:36,140
replicas and I start to respond to me

00:49:33,140 --> 00:49:38,150
before I make a decision or do I wait

00:49:36,140 --> 00:49:40,340
only up to some time out right what if I

00:49:38,150 --> 00:49:42,050
don't get all of them within a time out

00:49:40,340 --> 00:49:43,970
so there's a little bit the sort of

00:49:42,050 --> 00:49:45,770
subtlety that you have to think about

00:49:43,970 --> 00:49:47,750
but it is possible are just here yeah

00:49:45,770 --> 00:49:49,610
because you already have this offset

00:49:47,750 --> 00:49:50,500
thing and I already imagine that's kind

00:49:49,610 --> 00:49:52,370
of the thing you use for like

00:49:50,500 --> 00:49:54,500
synchronization and stopping you could

00:49:52,370 --> 00:49:55,790
like use this right so it is possible

00:49:54,500 --> 00:49:58,070
but one of the things we try to optimize

00:49:55,790 --> 00:50:00,680
is when there's a leader failure right

00:49:58,070 --> 00:50:02,360
you I this is a pretty severe problem

00:50:00,680 --> 00:50:04,340
because your system is not available

00:50:02,360 --> 00:50:07,100
until you lacked a new leader so when

00:50:04,340 --> 00:50:09,890
this happens our primary goal is to make

00:50:07,100 --> 00:50:12,260
the leader election as fast as possible

00:50:09,890 --> 00:50:14,060
so the fewer coordination have to do the

00:50:12,260 --> 00:50:16,310
better it makes it more highly available

00:50:14,060 --> 00:50:18,020
our secondary goal probably is so you

00:50:16,310 --> 00:50:20,840
know maybe what can proceed or more data

00:50:18,020 --> 00:50:23,650
if possible okay any other questions I

00:50:20,840 --> 00:50:25,750
think that was first yeah the black one

00:50:23,650 --> 00:50:28,030
okay all right have a couple of

00:50:25,750 --> 00:50:29,740
questions the first one is how does that

00:50:28,030 --> 00:50:34,600
replication happen is it serial or

00:50:29,740 --> 00:50:36,250
parallel yeah so the replicator if the

00:50:34,600 --> 00:50:39,190
rapid you are talking about replication

00:50:36,250 --> 00:50:42,430
of different followers that happens in

00:50:39,190 --> 00:50:44,530
parallel okay each follower fetches own

00:50:42,430 --> 00:50:47,830
data independently so it has his own

00:50:44,530 --> 00:50:51,220
socket connection to do the fetch okay

00:50:47,830 --> 00:50:52,840
its independence not chained if that's

00:50:51,220 --> 00:50:56,070
what you asked for

00:50:52,840 --> 00:50:58,480
so we are not yeah so we are not

00:50:56,070 --> 00:51:00,100
forwarding a data from one leader to the

00:50:58,480 --> 00:51:02,950
first follower then to the second

00:51:00,100 --> 00:51:04,840
follower instead the first follower in a

00:51:02,950 --> 00:51:08,770
second follower of a fetch the data from

00:51:04,840 --> 00:51:09,070
the same leader in parallel okay that's

00:51:08,770 --> 00:51:12,670
that

00:51:09,070 --> 00:51:14,260
and the other question is so in the

00:51:12,670 --> 00:51:16,780
terrible case that you lose connection

00:51:14,260 --> 00:51:19,270
to all your Kafka brokers yes what

00:51:16,780 --> 00:51:23,020
happens right it's in the disaster case

00:51:19,270 --> 00:51:25,270
if you lose all those replicas then your

00:51:23,020 --> 00:51:27,910
system is at least for this particular

00:51:25,270 --> 00:51:29,560
partition it's it's the same sort as

00:51:27,910 --> 00:51:31,780
before it's unavailable which means you

00:51:29,560 --> 00:51:34,810
can't read or write to it you have to

00:51:31,780 --> 00:51:38,320
wait until at least some replicas come

00:51:34,810 --> 00:51:40,270
back so our our system is designed for

00:51:38,320 --> 00:51:41,740
supporting the common failures which is

00:51:40,270 --> 00:51:44,460
you know you are doing this rollin

00:51:41,740 --> 00:51:47,290
restart your server or you have this

00:51:44,460 --> 00:51:49,510
unexpected failure but it's isolated

00:51:47,290 --> 00:51:53,290
it's not really designed for having this

00:51:49,510 --> 00:51:55,450
catastrophe failures if you have that I

00:51:53,290 --> 00:51:57,730
think we do have this asynchronous

00:51:55,450 --> 00:51:59,230
replication mechanism across two

00:51:57,730 --> 00:52:01,870
clusters typical across two different

00:51:59,230 --> 00:52:05,520
data centers and you can use that as you

00:52:01,870 --> 00:52:05,520
are so the D our recovery story

00:52:10,350 --> 00:52:16,620
currently we don't write so currently we

00:52:15,060 --> 00:52:19,260
don't have that I think on a producer

00:52:16,620 --> 00:52:22,260
side the producer only buffers things in

00:52:19,260 --> 00:52:24,510
memory if you can't stand for a

00:52:22,260 --> 00:52:26,070
considerable period of time those data

00:52:24,510 --> 00:52:30,060
can not be cute forever

00:52:26,070 --> 00:52:32,400
don't be dropped I think there's some

00:52:30,060 --> 00:52:34,380
people are you interested in having like

00:52:32,400 --> 00:52:36,840
a local data a log to persist though

00:52:34,380 --> 00:52:38,850
those data it's possible we haven't

00:52:36,840 --> 00:52:42,000
fully thought about the trade-off of

00:52:38,850 --> 00:52:43,980
doing this one caveat is if you want to

00:52:42,000 --> 00:52:45,930
do this then there's more storage you

00:52:43,980 --> 00:52:47,700
have to manage because I'm the broker

00:52:45,930 --> 00:52:49,260
side typically there a few of those

00:52:47,700 --> 00:52:50,660
brokers and then you can actually

00:52:49,260 --> 00:52:53,190
configure them and manage them properly

00:52:50,660 --> 00:52:55,620
but typically there are a lot more

00:52:53,190 --> 00:52:57,690
producers than the progress and if you

00:52:55,620 --> 00:52:59,820
have managed or the local storage

00:52:57,690 --> 00:53:02,640
Melrose producers typical is a bigger

00:52:59,820 --> 00:53:07,410
problem okay that's a caveat yes there's

00:53:02,640 --> 00:53:09,330
another question I actually have two

00:53:07,410 --> 00:53:11,790
questions um you mentioned a couple

00:53:09,330 --> 00:53:14,010
times the RPC protocol so I was

00:53:11,790 --> 00:53:15,930
wondering what protocols is like native

00:53:14,010 --> 00:53:18,540
to Kafka what were you guys used to talk

00:53:15,930 --> 00:53:20,430
to it and the second question is how big

00:53:18,540 --> 00:53:22,250
is an average cluster for you guys is

00:53:20,430 --> 00:53:26,130
there tens of nodes or hundreds of nodes

00:53:22,250 --> 00:53:28,500
yeah so the first question is on the RPC

00:53:26,130 --> 00:53:31,200
protocol I think currently we have our

00:53:28,500 --> 00:53:35,370
proprietary RPC protocol so it's not any

00:53:31,200 --> 00:53:40,680
of the standard protocols that are used

00:53:35,370 --> 00:53:42,780
in JMS and one of the primary reason we

00:53:40,680 --> 00:53:45,210
did that is especially on the consumer

00:53:42,780 --> 00:53:48,030
side on the consumers and remember one

00:53:45,210 --> 00:53:49,470
of the Polyphonic covered is in a lowest

00:53:48,030 --> 00:53:52,140
level where we are using the stand of

00:53:49,470 --> 00:53:55,530
our API that allows us to send some

00:53:52,140 --> 00:53:58,410
bytes from local fire to remote socket

00:53:55,530 --> 00:54:03,180
or channel directly so that v has an

00:53:58,410 --> 00:54:05,250
auto oldest application layer and so so

00:54:03,180 --> 00:54:07,170
if we do a proprietor protocol that

00:54:05,250 --> 00:54:08,550
makes it easy if you want to implement

00:54:07,170 --> 00:54:10,590
some existing protocol that makes it

00:54:08,550 --> 00:54:12,000
harder because now you can't do any

00:54:10,590 --> 00:54:13,320
transformation you can you have to take

00:54:12,000 --> 00:54:14,910
some bytes that sort in the fire and

00:54:13,320 --> 00:54:16,790
send it directly to the server or

00:54:14,910 --> 00:54:18,620
another approach is you just have to

00:54:16,790 --> 00:54:20,990
is optimization you have to copy your

00:54:18,620 --> 00:54:22,700
data into the application space and then

00:54:20,990 --> 00:54:24,050
transform it and then send it over and

00:54:22,700 --> 00:54:34,340
then you lose some of the efficiency

00:54:24,050 --> 00:54:37,970
there yes that's right today I think we

00:54:34,340 --> 00:54:41,660
support Java API but various people have

00:54:37,970 --> 00:54:45,290
been building other language bindings at

00:54:41,660 --> 00:54:47,420
least on the producer side and so

00:54:45,290 --> 00:54:49,070
there's even implementation maybe some

00:54:47,420 --> 00:54:51,110
Sun of heights on implementation and

00:54:49,070 --> 00:54:54,260
going forward in a way that we try to

00:54:51,110 --> 00:54:55,400
maintain Lisa C library so you can

00:54:54,260 --> 00:54:58,280
hopefully you can be or different

00:54:55,400 --> 00:55:00,050
bindings on that C 90 of the library on

00:54:58,280 --> 00:55:01,550
the producer side on the consumer side

00:55:00,050 --> 00:55:04,010
currently it'll be tricky because the

00:55:01,550 --> 00:55:06,980
consumer side it has to handle it at the

00:55:04,010 --> 00:55:09,140
rebalance logic and it uses zookeepers

00:55:06,980 --> 00:55:11,780
it needs a zookeeper library but in the

00:55:09,140 --> 00:55:13,640
future we do try to make it in there so

00:55:11,780 --> 00:55:16,670
that we can make it easy to find other

00:55:13,640 --> 00:55:18,080
languages other than Java and for your

00:55:16,670 --> 00:55:20,450
second question in terms classist eyes

00:55:18,080 --> 00:55:23,540
our kernel studying the class is very

00:55:20,450 --> 00:55:25,640
small I think it's probably around 10

00:55:23,540 --> 00:55:28,460
servers but with replication all cause

00:55:25,640 --> 00:55:30,290
you are storing more data and our plan

00:55:28,460 --> 00:55:33,020
is to grow the class to the order of

00:55:30,290 --> 00:55:34,670
maybe 15 nose or something but we do

00:55:33,020 --> 00:55:37,060
have multiple classes for different use

00:55:34,670 --> 00:55:40,060
cases but each closet is in that order

00:55:37,060 --> 00:55:40,060
yes

00:55:40,440 --> 00:55:44,940
okay it's a last question but if you but

00:55:43,770 --> 00:55:47,040
if you have questions yeah you can talk

00:55:44,940 --> 00:55:55,310
to the offline or talk to the total me

00:55:47,040 --> 00:55:59,850
don't meet out yes yes I have it um

00:55:55,310 --> 00:56:01,590
that's making sound so I am a little bit

00:55:59,850 --> 00:56:03,750
confused on the protocol is it

00:56:01,590 --> 00:56:06,840
considered committed when all of the

00:56:03,750 --> 00:56:09,120
followers have responded or have have

00:56:06,840 --> 00:56:12,660
fetched past it or is it considered

00:56:09,120 --> 00:56:14,250
committed when a committed marker has

00:56:12,660 --> 00:56:19,140
been received by all of the followers

00:56:14,250 --> 00:56:22,320
yeah I think the protocol is when you

00:56:19,140 --> 00:56:25,890
have a message that message essentially

00:56:22,320 --> 00:56:27,930
has to be received by Auto replicas race

00:56:25,890 --> 00:56:31,410
using crappy cassette which they all

00:56:27,930 --> 00:56:33,270
have to move past that marker okay

00:56:31,410 --> 00:56:35,790
before you can commit you can consider

00:56:33,270 --> 00:56:40,920
this committed so the marker plan one is

00:56:35,790 --> 00:56:44,250
you are if m1 is considered as committed

00:56:40,920 --> 00:56:46,530
then the marker is actually the next

00:56:44,250 --> 00:56:53,670
offset which marks everything deployed

00:56:46,530 --> 00:56:55,650
has committed but okay someone to

00:56:53,670 --> 00:57:00,350
understand okay something okay all right

00:56:55,650 --> 00:57:00,350

YouTube URL: https://www.youtube.com/watch?v=XcvHmqmh16g


