Title: Apache Tomcat Clustering
Publication date: 2013-10-17
Playlist: Apachecon NA 2013 - day 1
Description: 
	Mark Thomas
ApacheCon NA 2013
Track : A Patchy Web
Captions: 
	00:00:00,030 --> 00:00:07,350
well good afternoon everyone I'm here to

00:00:03,330 --> 00:00:10,670
talk about Apache Tomcat clustering this

00:00:07,350 --> 00:00:13,469
is the agenda for this particular talk

00:00:10,670 --> 00:00:15,990
I'll give a little bit of background

00:00:13,469 --> 00:00:18,990
about myself and then talk about some

00:00:15,990 --> 00:00:21,570
terminology when to cluster perhaps more

00:00:18,990 --> 00:00:23,100
importantly when not to cluster I'll

00:00:21,570 --> 00:00:25,140
talk about some of the components

00:00:23,100 --> 00:00:27,480
there's quite a lot of moving parts in

00:00:25,140 --> 00:00:28,800
clustering it's quite a complex topic I

00:00:27,480 --> 00:00:31,080
could probably talk about it all day and

00:00:28,800 --> 00:00:32,910
I've obviously only got an hour so I'm

00:00:31,080 --> 00:00:35,070
going to focus on what I think are the

00:00:32,910 --> 00:00:36,210
key things you need to think about if

00:00:35,070 --> 00:00:38,129
you're going to if you're going to look

00:00:36,210 --> 00:00:39,270
at clustering so with those components

00:00:38,129 --> 00:00:41,640
we'll also look at some of the

00:00:39,270 --> 00:00:45,180
configuration choices touch a little bit

00:00:41,640 --> 00:00:47,489
on debugging but I'll tell you know that

00:00:45,180 --> 00:00:49,710
there's no magic answer to the some of

00:00:47,489 --> 00:00:51,960
the complexity issues here and then

00:00:49,710 --> 00:00:54,030
we'll wrap up with questions as always

00:00:51,960 --> 00:00:56,539
I'm happy to take questions as we go

00:00:54,030 --> 00:01:00,359
along if you can get my attention

00:00:56,539 --> 00:01:02,340
so introductions for those of you that

00:01:00,359 --> 00:01:04,350
weren't here for the small news talk my

00:01:02,340 --> 00:01:07,020
name is Mark Thomas I'm a committer on

00:01:04,350 --> 00:01:09,360
the Apache Tomcat project I do various

00:01:07,020 --> 00:01:12,360
other things at the ASF as well I'm part

00:01:09,360 --> 00:01:14,280
of the infrastructure team I'm part of

00:01:12,360 --> 00:01:17,040
the security team and I'll be talking

00:01:14,280 --> 00:01:18,420
about security ASF wide and Tomcat

00:01:17,040 --> 00:01:20,729
specifically tomorrow

00:01:18,420 --> 00:01:22,560
I do bits and pieces in the Commons

00:01:20,729 --> 00:01:24,420
project primarily around those things

00:01:22,560 --> 00:01:27,330
that Tomcat itself uses so that's

00:01:24,420 --> 00:01:27,979
Commons pool and Commons dbcp and I'm a

00:01:27,330 --> 00:01:30,720
member

00:01:27,979 --> 00:01:32,880
my day job is as a staff engineer at

00:01:30,720 --> 00:01:35,540
VMware where they pretty much just leave

00:01:32,880 --> 00:01:39,119
me to work on Tomcat which is great I

00:01:35,540 --> 00:01:40,979
also help out on the security side where

00:01:39,119 --> 00:01:42,869
I lead spring sources security team so

00:01:40,979 --> 00:01:44,369
that's much like I do at Apache it's

00:01:42,869 --> 00:01:46,890
managing response to security

00:01:44,369 --> 00:01:49,680
vulnerability reports and I work on

00:01:46,890 --> 00:01:51,570
VMware's TC server product which is

00:01:49,680 --> 00:01:53,490
effectively a application server built

00:01:51,570 --> 00:01:55,619
on apache tomcat and as part of that

00:01:53,490 --> 00:02:00,600
then i provide support to customers both

00:01:55,619 --> 00:02:04,079
using TC server and Tomcats so let's

00:02:00,600 --> 00:02:07,320
talk about some terminology when we talk

00:02:04,079 --> 00:02:09,569
about clustering so in a IT environment

00:02:07,320 --> 00:02:12,720
it means a lot of different things to a

00:02:09,569 --> 00:02:13,200
lot of different people the Tomcat

00:02:12,720 --> 00:02:15,959
community

00:02:13,200 --> 00:02:18,150
has a very very specific definition of

00:02:15,959 --> 00:02:21,300
clustering in mind when we talk about

00:02:18,150 --> 00:02:24,300
Tomcat all we are talking about is

00:02:21,300 --> 00:02:27,690
taking httpsession information that's on

00:02:24,300 --> 00:02:31,500
one node and copying that information to

00:02:27,690 --> 00:02:33,480
another that's it when when we ever we

00:02:31,500 --> 00:02:35,340
talk about tomcat clustering that's the

00:02:33,480 --> 00:02:37,620
functionality that's being discussed and

00:02:35,340 --> 00:02:40,590
that's true throughout the codebase and

00:02:37,620 --> 00:02:41,370
throughout the documentation obviously

00:02:40,590 --> 00:02:43,860
on its own

00:02:41,370 --> 00:02:46,140
that's not a great deal of use so you

00:02:43,860 --> 00:02:48,840
need something like load balancing in

00:02:46,140 --> 00:02:51,959
order to route traffic to these multiple

00:02:48,840 --> 00:02:55,590
Tomcat instances and that's difinitely

00:02:51,959 --> 00:02:57,870
done with a reverse proxy like httpd but

00:02:55,590 --> 00:03:00,630
you could be using iis you could be

00:02:57,870 --> 00:03:02,910
using any anything any web server that

00:03:00,630 --> 00:03:06,599
connect as a reverse proxy and can talk

00:03:02,910 --> 00:03:08,610
HTTP or AJP to tomcat connector can can

00:03:06,599 --> 00:03:09,780
perform that load balancing role and it

00:03:08,610 --> 00:03:11,849
doesn't actually have to be a web server

00:03:09,780 --> 00:03:14,010
it can be a hardware load balancer

00:03:11,849 --> 00:03:17,220
there's lots of different ways of ways

00:03:14,010 --> 00:03:19,609
of doing it and the third bit of

00:03:17,220 --> 00:03:24,200
terminology is what's referred to as

00:03:19,609 --> 00:03:26,760
sticky sessions and this is a

00:03:24,200 --> 00:03:30,600
configuration of the load balancer and

00:03:26,760 --> 00:03:32,670
the cluster nodes to ensure that once a

00:03:30,600 --> 00:03:36,000
client has done something that creates

00:03:32,670 --> 00:03:37,620
an HTTP session then all requests that

00:03:36,000 --> 00:03:40,290
are associated with that session will

00:03:37,620 --> 00:03:43,549
always go to the same tomcat instance

00:03:40,290 --> 00:03:46,500
within the group of tomcat instances so

00:03:43,549 --> 00:03:48,329
if it's if the first few requests are

00:03:46,500 --> 00:03:49,799
stateless then those requests might be

00:03:48,329 --> 00:03:52,470
handled by different nodes within the

00:03:49,799 --> 00:03:54,180
cluster as soon as an HTTP session is

00:03:52,470 --> 00:03:56,819
created and sticky sessions are enabled

00:03:54,180 --> 00:03:59,819
all subsequent requests go to the node

00:03:56,819 --> 00:04:01,590
where that session was created and it

00:03:59,819 --> 00:04:03,569
will the client will continue being

00:04:01,590 --> 00:04:08,280
processed Bolling that node until the

00:04:03,569 --> 00:04:11,069
session expires so when do you want to

00:04:08,280 --> 00:04:13,560
cluster basically you know the an ideal

00:04:11,069 --> 00:04:15,120
world you don't might seem like a

00:04:13,560 --> 00:04:16,200
strange thing to say in a clustering

00:04:15,120 --> 00:04:18,359
talk but I actually think it's worth

00:04:16,200 --> 00:04:19,769
emphasizing if you can avoid clustering

00:04:18,359 --> 00:04:23,340
if at all possible

00:04:19,769 --> 00:04:25,560
avoid it it's not something you want to

00:04:23,340 --> 00:04:28,900
be doing lightly

00:04:25,560 --> 00:04:31,900
reasons why not it adds configuration

00:04:28,900 --> 00:04:33,130
complexity if you can if you don't have

00:04:31,900 --> 00:04:34,600
to configure clustering that's a whole

00:04:33,130 --> 00:04:36,330
bunch of stuff you don't have to worry

00:04:34,600 --> 00:04:38,920
about that's a good thing

00:04:36,330 --> 00:04:41,710
it's very nature means it requires

00:04:38,920 --> 00:04:43,330
additional processing just because

00:04:41,710 --> 00:04:45,190
you've got to take updates to the

00:04:43,330 --> 00:04:47,140
session and replicate them to one or

00:04:45,190 --> 00:04:49,510
more nodes in the cluster that requires

00:04:47,140 --> 00:04:51,810
more work to be done it needs more CPU

00:04:49,510 --> 00:04:53,980
cycles it needs more network bandwidth

00:04:51,810 --> 00:04:59,230
fundamentally your system is doing more

00:04:53,980 --> 00:05:02,470
work and when it goes wrong it can be a

00:04:59,230 --> 00:05:07,690
lot lot harder to figure out what's

00:05:02,470 --> 00:05:10,360
going wrong and why some of the really

00:05:07,690 --> 00:05:12,550
nasty client support engagements I

00:05:10,360 --> 00:05:14,410
really wish I'd managed to avoid has

00:05:12,550 --> 00:05:18,040
involved clustering and trying to figure

00:05:14,410 --> 00:05:20,950
out what's been going wrong and why it's

00:05:18,040 --> 00:05:25,750
not fun it's really slow it's often

00:05:20,950 --> 00:05:28,360
quite difficult it's much simpler to

00:05:25,750 --> 00:05:33,250
work with non clustered environments

00:05:28,360 --> 00:05:35,140
either completely stateless systems or

00:05:33,250 --> 00:05:37,110
systems that just use sticky sessions

00:05:35,140 --> 00:05:40,120
and don't bother with the replication

00:05:37,110 --> 00:05:41,560
and that the rest of the session

00:05:40,120 --> 00:05:43,630
replication is effectively giving you a

00:05:41,560 --> 00:05:45,610
failover protection so if one node dies

00:05:43,630 --> 00:05:47,560
then the session information is

00:05:45,610 --> 00:05:49,780
available on another node and you can

00:05:47,560 --> 00:05:51,190
reroute client requests to that other

00:05:49,780 --> 00:05:53,110
node and the client can carry on

00:05:51,190 --> 00:05:57,280
hopefully without noticing that there's

00:05:53,110 --> 00:06:00,090
a problem so the question is do you

00:05:57,280 --> 00:06:03,340
really need that clustering capability

00:06:00,090 --> 00:06:05,530
is simple though balancing with sticky

00:06:03,340 --> 00:06:07,870
sessions good enough so as I say the

00:06:05,530 --> 00:06:11,170
cost there is if a node dies the users

00:06:07,870 --> 00:06:14,380
data is lost how bad that will be will

00:06:11,170 --> 00:06:18,370
be to depend on your particular system

00:06:14,380 --> 00:06:20,020
your particular environment if you've

00:06:18,370 --> 00:06:22,060
got a completely stateless application

00:06:20,020 --> 00:06:23,920
that you don't care anyway you're not

00:06:22,060 --> 00:06:25,300
using sessions you're not use it you

00:06:23,920 --> 00:06:28,090
don't need to worry about replication

00:06:25,300 --> 00:06:29,170
that's the ideal to be honest if you can

00:06:28,090 --> 00:06:31,780
make your application completely

00:06:29,170 --> 00:06:33,700
stateless but in reality most

00:06:31,780 --> 00:06:36,910
applications do require some form of

00:06:33,700 --> 00:06:38,650
state and they usually store some state

00:06:36,910 --> 00:06:41,919
to a greater or lesser extent

00:06:38,650 --> 00:06:44,590
in the HTTP session something to look at

00:06:41,919 --> 00:06:46,870
is is there somewhere else you can

00:06:44,590 --> 00:06:49,360
persist this state and often it might be

00:06:46,870 --> 00:06:50,800
in a back-end database so the stuff that

00:06:49,360 --> 00:06:52,419
really matters is in the backup date

00:06:50,800 --> 00:06:54,610
back-end database and if you lose the

00:06:52,419 --> 00:06:56,350
HTTP session the application can recover

00:06:54,610 --> 00:07:00,130
and carry on and the user won't notice

00:06:56,350 --> 00:07:02,620
it the sorts of environments we're

00:07:00,130 --> 00:07:06,940
losing the HTTP session is just a

00:07:02,620 --> 00:07:08,860
complete nono tend to be those where

00:07:06,940 --> 00:07:12,400
either there's some sort of financial

00:07:08,860 --> 00:07:15,250
transaction involved or there's a very

00:07:12,400 --> 00:07:17,380
very high availability requirement they

00:07:15,250 --> 00:07:19,330
do happen and there may well be other

00:07:17,380 --> 00:07:22,930
ways to architect your application to

00:07:19,330 --> 00:07:24,370
avoid session replication and if when

00:07:22,930 --> 00:07:27,690
you're designing your application I do

00:07:24,370 --> 00:07:27,690
encourage you to look at those

00:07:28,560 --> 00:07:34,479
clustering really does absolutely have

00:07:31,539 --> 00:07:36,880
to be the last resort there there are

00:07:34,479 --> 00:07:38,530
other solutions that can often work and

00:07:36,880 --> 00:07:41,260
I think they're definitely worth

00:07:38,530 --> 00:07:43,150
considering looks it seems like a

00:07:41,260 --> 00:07:45,940
strange thing to say when I'm talking

00:07:43,150 --> 00:07:47,530
about clustering but it adds sufficient

00:07:45,940 --> 00:07:51,220
complexity both the configuration of

00:07:47,530 --> 00:07:53,320
particularly debugging that I really do

00:07:51,220 --> 00:07:56,020
want to emphasize the point have a look

00:07:53,320 --> 00:07:56,860
at what your other options are but let's

00:07:56,020 --> 00:07:58,780
assume that you've looked at your

00:07:56,860 --> 00:08:00,370
options there's enough of you in here to

00:07:58,780 --> 00:08:02,590
listen to me talk about clustering that

00:08:00,370 --> 00:08:05,070
some of you are interested in it so

00:08:02,590 --> 00:08:08,889
let's look at the clustering components

00:08:05,070 --> 00:08:10,539
this is a diagram of all of the various

00:08:08,889 --> 00:08:13,330
components that are involved in

00:08:10,539 --> 00:08:16,780
clustering and there's quite a few of

00:08:13,330 --> 00:08:19,120
them and all of these appear in server

00:08:16,780 --> 00:08:22,150
dot XML and they're all documented

00:08:19,120 --> 00:08:24,280
within the Tomcat documentation I'm

00:08:22,150 --> 00:08:25,840
going to talk about some of them in a

00:08:24,280 --> 00:08:27,130
little bit more detail than others some

00:08:25,840 --> 00:08:29,289
I'm going to gloss over quite quickly

00:08:27,130 --> 00:08:32,200
but say there is a lot more detail on

00:08:29,289 --> 00:08:34,570
this in in the docs so you've got the

00:08:32,200 --> 00:08:36,430
overall cluster component and it's

00:08:34,570 --> 00:08:38,860
effectively responsible for making sure

00:08:36,430 --> 00:08:41,560
that the HTTP session data gets

00:08:38,860 --> 00:08:44,560
replicated somehow across the cluster

00:08:41,560 --> 00:08:47,650
the first component is the manager and

00:08:44,560 --> 00:08:49,630
really it's the manager that does the

00:08:47,650 --> 00:08:51,420
orchestration of this and it's the

00:08:49,630 --> 00:08:53,730
manager that decides it

00:08:51,420 --> 00:08:55,709
actually how the data is going to be

00:08:53,730 --> 00:08:57,209
replicated we'll talk about this in a

00:08:55,709 --> 00:08:59,130
bit more detail later on but there are

00:08:57,209 --> 00:09:00,810
essentially two implementations that

00:08:59,130 --> 00:09:03,300
have different replication strategies

00:09:00,810 --> 00:09:05,700
and you are free to use either one of

00:09:03,300 --> 00:09:09,480
those or in fact implement your own if

00:09:05,700 --> 00:09:12,420
you so desire moving on from the manager

00:09:09,480 --> 00:09:15,570
we've got the channel and its various

00:09:12,420 --> 00:09:17,459
sub components and it's the channel that

00:09:15,570 --> 00:09:19,560
is the communication mechanism

00:09:17,459 --> 00:09:21,449
it's the channel that takes care of

00:09:19,560 --> 00:09:23,820
getting the replication messages from

00:09:21,449 --> 00:09:26,760
one node to the node or nodes those

00:09:23,820 --> 00:09:29,490
messages need to get to so one part of

00:09:26,760 --> 00:09:33,240
that is the membership and that's how

00:09:29,490 --> 00:09:34,920
that manages knowledge of what are the

00:09:33,240 --> 00:09:36,420
other nodes in this cluster whose

00:09:34,920 --> 00:09:38,220
obviously in order to implement some

00:09:36,420 --> 00:09:39,990
form of replication strategy you need to

00:09:38,220 --> 00:09:41,459
know what the other nodes are so you can

00:09:39,990 --> 00:09:44,100
decide which nodes to replicate your

00:09:41,459 --> 00:09:47,430
data to and that responsibility falls to

00:09:44,100 --> 00:09:48,839
the membership component then you've got

00:09:47,430 --> 00:09:51,449
the receiver in the sender which

00:09:48,839 --> 00:09:54,029
unsurprisingly the sender sends messages

00:09:51,449 --> 00:09:56,579
and the receivers receives messages and

00:09:54,029 --> 00:09:58,860
on those you've also got interceptors

00:09:56,579 --> 00:10:00,690
and they're a bit like servlet filters

00:09:58,860 --> 00:10:02,310
in that they can intercept messages on

00:10:00,690 --> 00:10:04,860
the way out and they can intercept

00:10:02,310 --> 00:10:06,779
messages on the way back they can insert

00:10:04,860 --> 00:10:08,970
messages into the chain and they can

00:10:06,779 --> 00:10:10,230
remove messages so they don't get

00:10:08,970 --> 00:10:12,510
further down the chain or don't get

00:10:10,230 --> 00:10:15,140
further at the stack so for example one

00:10:12,510 --> 00:10:17,430
of the simple interceptors just keeps a

00:10:15,140 --> 00:10:19,050
running total of statistics about how

00:10:17,430 --> 00:10:21,980
many messages have been sent how big

00:10:19,050 --> 00:10:24,720
they were that sort of thing there are

00:10:21,980 --> 00:10:26,910
other interceptors that do things like

00:10:24,720 --> 00:10:30,480
monitor the health of other nodes in the

00:10:26,910 --> 00:10:34,829
cluster the next block of components

00:10:30,480 --> 00:10:36,510
here are the valves now Valve's are also

00:10:34,829 --> 00:10:38,550
as it happens like the bit like servant

00:10:36,510 --> 00:10:41,100
filters but they're Tomcats specific

00:10:38,550 --> 00:10:44,339
version of a syrup filter and a valve

00:10:41,100 --> 00:10:45,720
can sit on any container component

00:10:44,339 --> 00:10:47,519
within the hierarchy so they can sit on

00:10:45,720 --> 00:10:49,500
context they can sit on hosts and they

00:10:47,519 --> 00:10:50,970
can sit on engines because we're talking

00:10:49,500 --> 00:10:52,410
about the clustering which is either

00:10:50,970 --> 00:10:54,870
done at the host level or the engine

00:10:52,410 --> 00:10:56,490
level then whatever valves are

00:10:54,870 --> 00:10:58,800
associated with the cluster they will

00:10:56,490 --> 00:11:01,410
get injected into the processing

00:10:58,800 --> 00:11:03,779
pipeline of The Associated component so

00:11:01,410 --> 00:11:05,710
if you have a cluster attached to a host

00:11:03,779 --> 00:11:07,510
then any valves that can

00:11:05,710 --> 00:11:09,490
cluster will get inserted into the

00:11:07,510 --> 00:11:12,160
pipeline of valves for that particular

00:11:09,490 --> 00:11:17,350
host and the sort of thing that the

00:11:12,160 --> 00:11:19,330
valves do is for example if the session

00:11:17,350 --> 00:11:20,860
ID changes for example because you're

00:11:19,330 --> 00:11:23,170
using sticky sessions you have a fare

00:11:20,860 --> 00:11:25,180
over the jvm route on the end that

00:11:23,170 --> 00:11:27,279
changes then the valve makes sure that

00:11:25,180 --> 00:11:31,149
Tomcats internal data structures are

00:11:27,279 --> 00:11:35,709
updated with that new ID next component

00:11:31,149 --> 00:11:40,510
is the deployer this is something that

00:11:35,709 --> 00:11:42,279
isn't that widely used the reason I know

00:11:40,510 --> 00:11:44,140
this is for a long time there was a

00:11:42,279 --> 00:11:47,350
there was a rather fundamental bug in

00:11:44,140 --> 00:11:49,600
the deployer in that it made the rather

00:11:47,350 --> 00:11:51,970
inaccurate assumption that all messages

00:11:49,600 --> 00:11:54,370
it received were were received in the

00:11:51,970 --> 00:11:56,290
same order it was sent and the job of

00:11:54,370 --> 00:11:58,120
the deployer is to take a war file and

00:11:56,290 --> 00:11:58,540
deploy it across all of the nodes in the

00:11:58,120 --> 00:12:00,190
cluster

00:11:58,540 --> 00:12:01,570
so obviously the war file per ticket if

00:12:00,190 --> 00:12:03,520
there's a big one gets broken down into

00:12:01,570 --> 00:12:05,230
multiple messages that gets sent out

00:12:03,520 --> 00:12:06,760
across the network its reconstructed at

00:12:05,230 --> 00:12:07,360
the other end and build the war file and

00:12:06,760 --> 00:12:10,000
deploy it

00:12:07,360 --> 00:12:11,680
if those messages were reconstructed in

00:12:10,000 --> 00:12:14,500
the wrong order obviously you ended up

00:12:11,680 --> 00:12:17,170
with a corrupted war file and that was

00:12:14,500 --> 00:12:20,020
the case for quite a while and nobody

00:12:17,170 --> 00:12:22,600
actually complained when somebody did

00:12:20,020 --> 00:12:25,300
finally complain we fixed it but it does

00:12:22,600 --> 00:12:27,400
suggest that it's not that widely used

00:12:25,300 --> 00:12:29,740
yeah I think I probably see about maybe

00:12:27,400 --> 00:12:32,589
1/2 messages a year on the users list

00:12:29,740 --> 00:12:34,360
about it so it's a fairly infrequently

00:12:32,589 --> 00:12:35,320
used component most people just when

00:12:34,360 --> 00:12:37,810
they have a cluster they'll just

00:12:35,320 --> 00:12:38,980
manually deploy the war files they want

00:12:37,810 --> 00:12:41,230
to that plus event they're not

00:12:38,980 --> 00:12:44,770
dynamically deploying and under blowing

00:12:41,230 --> 00:12:47,170
war files from a cluster and finally

00:12:44,770 --> 00:12:49,450
there's the listeners components and

00:12:47,170 --> 00:12:51,130
there are various events that are

00:12:49,450 --> 00:12:53,740
generated within the cluster it seems

00:12:51,130 --> 00:12:55,990
like message received message sent and

00:12:53,740 --> 00:12:58,150
node as as disappeared from the cluster

00:12:55,990 --> 00:12:59,589
and you notice arrived lots and lots of

00:12:58,150 --> 00:13:02,529
different messages that you can write a

00:12:59,589 --> 00:13:05,410
listener to respond to and do something

00:13:02,529 --> 00:13:07,630
when that happens and tomcat has some

00:13:05,410 --> 00:13:09,670
internal listeners as well that do some

00:13:07,630 --> 00:13:13,360
of the some of the work of the cluster

00:13:09,670 --> 00:13:16,180
based on other events so let's look at

00:13:13,360 --> 00:13:17,710
configuration choices for some but

00:13:16,180 --> 00:13:20,260
obviously not all of those

00:13:17,710 --> 00:13:22,330
those components first of all the

00:13:20,260 --> 00:13:24,850
manager as I said there are two options

00:13:22,330 --> 00:13:28,750
they referred to as the delta manager

00:13:24,850 --> 00:13:30,820
and the backup manager and the backup

00:13:28,750 --> 00:13:33,940
manager that happens is really the the

00:13:30,820 --> 00:13:35,860
motivating factor for me submitting this

00:13:33,940 --> 00:13:38,260
talk to apache con because there is a

00:13:35,860 --> 00:13:40,810
lot of misunderstanding over what the

00:13:38,260 --> 00:13:41,800
backup manager actually does and one of

00:13:40,810 --> 00:13:43,330
the things I wanted to do in this

00:13:41,800 --> 00:13:45,130
presentation was try and clarify that

00:13:43,330 --> 00:13:47,260
but before we get to the backup manager

00:13:45,130 --> 00:13:50,590
let's talk about the delta manager that

00:13:47,260 --> 00:13:51,790
is the default manager within tomcat

00:13:50,590 --> 00:13:53,080
that's the one that you get for you to

00:13:51,790 --> 00:13:56,800
put a crossed relevant in your server

00:13:53,080 --> 00:13:58,930
but xml and its replication strategy is

00:13:56,800 --> 00:14:01,930
really simple it's going to replicate

00:13:58,930 --> 00:14:04,270
every change in every session to every

00:14:01,930 --> 00:14:06,970
node in the cluster so that gives you

00:14:04,270 --> 00:14:09,400
maximum reliability if you've got four

00:14:06,970 --> 00:14:11,950
four nodes in your cluster three of

00:14:09,400 --> 00:14:13,480
those nodes can fail and in theory you

00:14:11,950 --> 00:14:15,430
will still have all of your sessions on

00:14:13,480 --> 00:14:17,170
your remaining node assuming that that

00:14:15,430 --> 00:14:19,420
node actually has enough memory to

00:14:17,170 --> 00:14:21,220
handle all of them and enough processing

00:14:19,420 --> 00:14:23,170
capability to handle or for all of the

00:14:21,220 --> 00:14:27,430
requests but it does give you maximum

00:14:23,170 --> 00:14:29,890
reliability it does have a teensy tiny

00:14:27,430 --> 00:14:33,880
downside when it comes to scalability

00:14:29,890 --> 00:14:35,410
and that is that the number of messages

00:14:33,880 --> 00:14:37,390
on the network or that the volume of

00:14:35,410 --> 00:14:39,760
network traffic increases with the

00:14:37,390 --> 00:14:42,870
square of the number of nodes in the

00:14:39,760 --> 00:14:46,090
network that is not sustainable

00:14:42,870 --> 00:14:47,860
you rapidly run out of bandwidth if you

00:14:46,090 --> 00:14:48,760
have a fairly busy cluster and a large

00:14:47,860 --> 00:14:53,410
number of nodes

00:14:48,760 --> 00:14:55,810
I reckon on the limit being somewhere

00:14:53,410 --> 00:14:58,390
between four and eight nodes depending

00:14:55,810 --> 00:15:00,550
on how big your sessions are how big the

00:14:58,390 --> 00:15:04,810
objects are that you're replicating how

00:15:00,550 --> 00:15:07,390
busy the site is you can sometimes get

00:15:04,810 --> 00:15:09,520
away with eight for you you're usually

00:15:07,390 --> 00:15:14,770
ok with it it is very application

00:15:09,520 --> 00:15:17,080
specific but it doesn't scale to really

00:15:14,770 --> 00:15:20,830
large numbers of nodes or really large

00:15:17,080 --> 00:15:23,170
sessions the one advantage of it is the

00:15:20,830 --> 00:15:26,200
fair over can be 20 node so if you've

00:15:23,170 --> 00:15:27,970
got 4 nodes and node a fails any of the

00:15:26,200 --> 00:15:30,430
other nodes can immediately pick up the

00:15:27,970 --> 00:15:31,010
request because they've got usually all

00:15:30,430 --> 00:15:32,830
of the

00:15:31,010 --> 00:15:35,000
Dejan we'll come on through it exactly

00:15:32,830 --> 00:15:38,570
when they might not have all of the

00:15:35,000 --> 00:15:42,920
session data a little bit later on the

00:15:38,570 --> 00:15:46,970
backup manager is not and I stress the

00:15:42,920 --> 00:15:49,100
not as some people have described it

00:15:46,970 --> 00:15:51,050
does not have one no that keeps a backup

00:15:49,100 --> 00:15:54,440
copy of every single session that is not

00:15:51,050 --> 00:15:57,830
how it works the way the backup manager

00:15:54,440 --> 00:16:00,260
works is a when a session can be created

00:15:57,830 --> 00:16:02,150
on any node in the cluster the node that

00:16:00,260 --> 00:16:05,900
the session is created on becomes the

00:16:02,150 --> 00:16:09,440
primary node for that session that node

00:16:05,900 --> 00:16:11,420
then picks a backup node it's actually

00:16:09,440 --> 00:16:14,960
round robin basis from one of the other

00:16:11,420 --> 00:16:15,980
nodes in the cluster so node a first of

00:16:14,960 --> 00:16:18,170
all when it needs a backup node it will

00:16:15,980 --> 00:16:24,020
pick node B then C then D then back to

00:16:18,170 --> 00:16:27,620
BCD BCD and so on so in our four node

00:16:24,020 --> 00:16:30,820
cluster if node a is our primary and

00:16:27,620 --> 00:16:35,870
node B is our backup node C and D have

00:16:30,820 --> 00:16:39,800
no session content at all what this

00:16:35,870 --> 00:16:41,960
means is that you have to use sticky

00:16:39,800 --> 00:16:44,570
sessions because you always need the

00:16:41,960 --> 00:16:47,360
requests to go to the primary node in

00:16:44,570 --> 00:16:48,800
the cluster if you don't the cluster

00:16:47,360 --> 00:16:50,240
starts trying to reconfigure itself

00:16:48,800 --> 00:16:51,920
thinking that the primary node has gone

00:16:50,240 --> 00:16:54,800
down so if you're using the backup

00:16:51,920 --> 00:16:59,030
energy you absolutely have to use sticky

00:16:54,800 --> 00:17:00,260
sessions in terms of scalability it's a

00:16:59,030 --> 00:17:01,930
lot more scalable because there's

00:17:00,260 --> 00:17:05,770
effectively only one copy of the data

00:17:01,930 --> 00:17:09,199
then the network traffic scales with

00:17:05,770 --> 00:17:11,839
just the number of nodes so it's linear

00:17:09,199 --> 00:17:15,470
scalability so it you can scale it out

00:17:11,839 --> 00:17:18,079
to much larger numbers failover however

00:17:15,470 --> 00:17:19,910
is a little bit more complicated and

00:17:18,079 --> 00:17:21,949
again particularly if you're trying to

00:17:19,910 --> 00:17:24,410
debug what's going on around failover

00:17:21,949 --> 00:17:25,970
situations you need to have a good

00:17:24,410 --> 00:17:27,290
understanding of how the backup manager

00:17:25,970 --> 00:17:30,100
works and that's what I'm going to talk

00:17:27,290 --> 00:17:33,230
about in a little bit more detail next

00:17:30,100 --> 00:17:37,250
so let's start with our four node

00:17:33,230 --> 00:17:40,060
cluster each node has got 30 sessions on

00:17:37,250 --> 00:17:42,290
it that her primary sessions and

00:17:40,060 --> 00:17:44,150
typically and in a real-world

00:17:42,290 --> 00:17:46,460
application they wouldn't be they would

00:17:44,150 --> 00:17:48,950
distributed this evenly there be a few

00:17:46,460 --> 00:17:51,110
little ups and downs but roughly on

00:17:48,950 --> 00:17:53,090
average for those thirty supreme

00:17:51,110 --> 00:17:55,430
recessions they'll do be distributed ten

00:17:53,090 --> 00:17:57,440
on each of the other three nodes so no

00:17:55,430 --> 00:18:00,020
day has got its thirty primary sessions

00:17:57,440 --> 00:18:03,290
and it's backups are distributed ten on

00:18:00,020 --> 00:18:04,940
B ten on C ten on day and the same for

00:18:03,290 --> 00:18:07,880
the other nodes so when you look at it

00:18:04,940 --> 00:18:10,520
in total each node has 60 sessions it's

00:18:07,880 --> 00:18:12,970
30 primary sessions and 30 backup

00:18:10,520 --> 00:18:16,220
sessions for other nodes in the cluster

00:18:12,970 --> 00:18:18,680
so we're using sticky sessions here so

00:18:16,220 --> 00:18:21,410
for node for a session that's in mode a

00:18:18,680 --> 00:18:23,270
the client will talk to that node any

00:18:21,410 --> 00:18:25,700
updates to those sessions they will get

00:18:23,270 --> 00:18:28,580
replicated just to the backup node

00:18:25,700 --> 00:18:33,620
that's got the copy of that session now

00:18:28,580 --> 00:18:35,270
let's imagine that node D fails a lot of

00:18:33,620 --> 00:18:37,940
things start happening and they start

00:18:35,270 --> 00:18:41,480
happening as soon as the cluster notices

00:18:37,940 --> 00:18:43,820
that that node has failed and that they

00:18:41,480 --> 00:18:45,830
are not dependent on a request coming in

00:18:43,820 --> 00:18:47,090
although a request coming in can

00:18:45,830 --> 00:18:52,850
actually trigger some of this as well

00:18:47,090 --> 00:18:54,290
and to add to the entertainment should

00:18:52,850 --> 00:18:56,060
we say of can't trying to keep track of

00:18:54,290 --> 00:18:59,360
what's going on each of those three

00:18:56,060 --> 00:19:01,130
remaining nodes are independently going

00:18:59,360 --> 00:19:04,220
to start recovering from the fact that

00:19:01,130 --> 00:19:08,330
node D has just failed so what's going

00:19:04,220 --> 00:19:09,980
to happen the short version is that the

00:19:08,330 --> 00:19:13,760
sessions are going to be distributed to

00:19:09,980 --> 00:19:16,400
the other nodes however things are

00:19:13,760 --> 00:19:18,500
complicated by what actually triggers

00:19:16,400 --> 00:19:29,000
that redistribution and what happens

00:19:18,500 --> 00:19:30,740
when so let's know DS failed and the

00:19:29,000 --> 00:19:32,330
other nodes in the cluster notice ok

00:19:30,740 --> 00:19:33,350
we'll assume that there are no requests

00:19:32,330 --> 00:19:35,750
coming in at the minute because that

00:19:33,350 --> 00:19:37,880
just helps keep things simple so the

00:19:35,750 --> 00:19:40,340
other nodes notice that node ears fail

00:19:37,880 --> 00:19:43,340
it says right was I either backup for

00:19:40,340 --> 00:19:45,230
any of node D sessions okay for all of

00:19:43,340 --> 00:19:47,990
node D sessions that I was the backup

00:19:45,230 --> 00:19:50,240
for I will make myself the primary and

00:19:47,990 --> 00:19:53,870
announce that to the other nodes in the

00:19:50,240 --> 00:19:55,460
cluster it will then say right I know I

00:19:53,870 --> 00:19:57,480
was the backup and I'm there the prime

00:19:55,460 --> 00:19:59,400
rate that means there is no backup

00:19:57,480 --> 00:20:01,320
these sessions at the minute so the next

00:19:59,400 --> 00:20:03,840
thing it will do it will again on a

00:20:01,320 --> 00:20:05,220
round-robin basis select a backup node

00:20:03,840 --> 00:20:15,540
for these sessions and then replicate

00:20:05,220 --> 00:20:18,240
that data out to those other nodes okay

00:20:15,540 --> 00:20:20,460
now things are slightly different if we

00:20:18,240 --> 00:20:23,910
look at this in terms of a request comes

00:20:20,460 --> 00:20:27,290
in if a request comes in the load

00:20:23,910 --> 00:20:31,020
balancer will redistribute to that that

00:20:27,290 --> 00:20:34,200
request to another to either a B or C

00:20:31,020 --> 00:20:36,059
but it doesn't know which was the backup

00:20:34,200 --> 00:20:37,380
node for that session so it it's got a

00:20:36,059 --> 00:20:39,750
one in three chance of getting it right

00:20:37,380 --> 00:20:40,980
if it gets it right that's fine

00:20:39,750 --> 00:20:44,100
everything's hunky-dory

00:20:40,980 --> 00:20:45,750
the node can to the node that it selects

00:20:44,100 --> 00:20:47,940
that's the new primary everything

00:20:45,750 --> 00:20:51,570
carries on as it was before if it gets

00:20:47,940 --> 00:20:53,640
it wrong whichever node it selects then

00:20:51,570 --> 00:20:57,270
starts making itself the primary node

00:20:53,640 --> 00:20:59,669
for that session and that then means

00:20:57,270 --> 00:21:05,010
that it needs to select a new backup and

00:20:59,669 --> 00:21:06,090
ants and so on something I should have

00:21:05,010 --> 00:21:09,150
mentioned a little bit earlier and I

00:21:06,090 --> 00:21:11,490
didn't ever although that a node will

00:21:09,150 --> 00:21:14,820
only keep its own sessions and copies of

00:21:11,490 --> 00:21:16,770
its backups it also has a list of every

00:21:14,820 --> 00:21:18,900
single session across the entire cluster

00:21:16,770 --> 00:21:21,750
and which node is primary and which node

00:21:18,900 --> 00:21:23,160
is backup so if a session if a node

00:21:21,750 --> 00:21:24,510
receives a request for a session it

00:21:23,160 --> 00:21:26,190
knows nothing about that it's not the

00:21:24,510 --> 00:21:28,410
primary for and it's not the backup for

00:21:26,190 --> 00:21:30,090
it still knows which nodes within the

00:21:28,410 --> 00:21:31,740
cluster are the primary and are the

00:21:30,090 --> 00:21:33,270
backup and it's able to go to the

00:21:31,740 --> 00:21:35,370
primary and say write excuse me I'm

00:21:33,270 --> 00:21:36,929
there the primary I'm taking over that

00:21:35,370 --> 00:21:40,590
session please send me a copy of the

00:21:36,929 --> 00:21:42,450
data so you've got these two processes

00:21:40,590 --> 00:21:45,900
going on long in peril the load balancer

00:21:42,450 --> 00:21:48,120
is redistributing the requests to the

00:21:45,900 --> 00:21:49,350
nose and that is triggering changes in

00:21:48,120 --> 00:21:51,540
which is the primary and which is the

00:21:49,350 --> 00:21:54,270
backup and the cluster at the same time

00:21:51,540 --> 00:21:55,710
is as notice that the node has gone down

00:21:54,270 --> 00:21:57,990
and it's also redistributing the

00:21:55,710 --> 00:21:59,850
sessions so it can take after a node

00:21:57,990 --> 00:22:02,220
failure a little bit of time for things

00:21:59,850 --> 00:22:04,290
to settle down again you will see spikes

00:22:02,220 --> 00:22:06,240
in CPU usage you will see spikes in

00:22:04,290 --> 00:22:08,880
network traffic and you need to allow

00:22:06,240 --> 00:22:10,409
some Headroom for that to happen again

00:22:08,880 --> 00:22:11,010
how much Headroom you need is going to

00:22:10,409 --> 00:22:13,320
be application

00:22:11,010 --> 00:22:17,340
dependent and that's something that you

00:22:13,320 --> 00:22:19,910
can track and and investigate you in so

00:22:17,340 --> 00:22:23,010
your development in your QA environment

00:22:19,910 --> 00:22:25,560
so where we end up with once things have

00:22:23,010 --> 00:22:26,910
settled down again roughly you know

00:22:25,560 --> 00:22:28,890
there's going to be little statistical

00:22:26,910 --> 00:22:31,770
anomalies here but we've now got 40

00:22:28,890 --> 00:22:34,500
sessions in primary on nodes a B and C

00:22:31,770 --> 00:22:36,930
and node a will have 20 backups of B and

00:22:34,500 --> 00:22:39,690
C B will have 20 backups of ANC and so

00:22:36,930 --> 00:22:43,050
on so we've now got 18 sessions

00:22:39,690 --> 00:22:47,580
effectively on each node in the cluster

00:22:43,050 --> 00:22:50,010
40 primary 40 backup I'll mention at

00:22:47,580 --> 00:22:51,600
this point if you want to know which

00:22:50,010 --> 00:22:53,550
sessions are primary which ones are

00:22:51,600 --> 00:22:56,220
backup on where they are the Tomcat and

00:22:53,550 --> 00:22:59,730
manager application will identify a

00:22:56,220 --> 00:23:02,040
session as a primary a backup or a proxy

00:22:59,730 --> 00:23:03,240
so a primary is obviously the one that's

00:23:02,040 --> 00:23:05,970
currently in use the backup is the

00:23:03,240 --> 00:23:07,830
backup copy a proxy session is

00:23:05,970 --> 00:23:09,360
essentially just the little marker

00:23:07,830 --> 00:23:10,650
within the node saying yeah I'm not the

00:23:09,360 --> 00:23:12,090
primary I'm not the backup but I know

00:23:10,650 --> 00:23:15,630
the primary is over here and the backups

00:23:12,090 --> 00:23:17,130
over there so when you look at sessions

00:23:15,630 --> 00:23:18,390
in a clustered web application in the

00:23:17,130 --> 00:23:19,650
manager if you're using the backup

00:23:18,390 --> 00:23:24,930
manager it will tell you whether it's a

00:23:19,650 --> 00:23:26,790
primary a backup or a proxy so that's

00:23:24,930 --> 00:23:29,700
the manager and the choices you've got

00:23:26,790 --> 00:23:31,800
to make their next one to go on and talk

00:23:29,700 --> 00:23:34,380
about membership and how you track this

00:23:31,800 --> 00:23:36,870
again there are two options you can

00:23:34,380 --> 00:23:40,500
either use multicast or you can use

00:23:36,870 --> 00:23:42,960
static definitions multicast membership

00:23:40,500 --> 00:23:45,060
which again is the default that does

00:23:42,960 --> 00:23:47,910
require multicast to be enabled in the

00:23:45,060 --> 00:23:49,560
network and if it's not then it's

00:23:47,910 --> 00:23:51,750
obviously just just not going to work

00:23:49,560 --> 00:23:54,420
personally it's something I find quite

00:23:51,750 --> 00:23:55,650
difficult to debug when it's going wrong

00:23:54,420 --> 00:23:57,690
I just haven't quite got enough

00:23:55,650 --> 00:23:59,370
experienced and drill down really

00:23:57,690 --> 00:24:02,360
quickly to what the problem areas are so

00:23:59,370 --> 00:24:05,040
it usually takes me a little bit of time

00:24:02,360 --> 00:24:07,740
one of the things that has changed

00:24:05,040 --> 00:24:10,230
recently or I'll say recently in the

00:24:07,740 --> 00:24:11,850
last few years with multicast is as you

00:24:10,230 --> 00:24:14,280
see organizations moving to more

00:24:11,850 --> 00:24:15,390
virtualized environments they often set

00:24:14,280 --> 00:24:17,430
up we're going to set up our own little

00:24:15,390 --> 00:24:19,950
virtual subnet for this particular

00:24:17,430 --> 00:24:21,720
application and because the application

00:24:19,950 --> 00:24:23,450
is on its own virtual subnet there's

00:24:21,720 --> 00:24:25,519
normally

00:24:23,450 --> 00:24:27,409
less of a reluctance to enable multicast

00:24:25,519 --> 00:24:29,149
on that particular subnet than there

00:24:27,409 --> 00:24:30,559
would be if you're trying to if you want

00:24:29,149 --> 00:24:32,630
you're asking it for it to be enabled

00:24:30,559 --> 00:24:37,100
across an entire corporate IT structure

00:24:32,630 --> 00:24:39,470
so I have seen a a greater willingness

00:24:37,100 --> 00:24:42,980
to use multicast in in corporate

00:24:39,470 --> 00:24:44,750
environment so over recent years one of

00:24:42,980 --> 00:24:48,590
the big advantages of multicast is its

00:24:44,750 --> 00:24:51,409
scales more easily if you want to add a

00:24:48,590 --> 00:24:53,840
new new member you just configure the

00:24:51,409 --> 00:24:55,850
node it starts up it announces itself to

00:24:53,840 --> 00:24:58,279
the other nodes they see it's there they

00:24:55,850 --> 00:25:00,019
announce themselves to it and in theory

00:24:58,279 --> 00:25:02,559
and it usually does if multicast is

00:25:00,019 --> 00:25:05,269
enabled it just works

00:25:02,559 --> 00:25:07,429
whereas static membership which doesn't

00:25:05,269 --> 00:25:09,019
require multicast it's it's a lot

00:25:07,429 --> 00:25:10,909
simpler to debuggers it's just a simple

00:25:09,019 --> 00:25:12,980
TCP connection to all of the other nodes

00:25:10,909 --> 00:25:14,750
which you've already you've told your

00:25:12,980 --> 00:25:16,070
current node it exists so it creates

00:25:14,750 --> 00:25:17,000
connections to each of them to make sure

00:25:16,070 --> 00:25:19,940
it can talk to them

00:25:17,000 --> 00:25:21,500
the downside is adding nodes gets

00:25:19,940 --> 00:25:23,690
time-consuming it's ok when you've got

00:25:21,500 --> 00:25:25,279
one or two or three but when you've sort

00:25:23,690 --> 00:25:27,679
of got your 8 node cluster and you want

00:25:25,279 --> 00:25:30,559
to add another two you've now got to go

00:25:27,679 --> 00:25:33,590
to each of those 8 instances and add

00:25:30,559 --> 00:25:37,070
another two static membership entries to

00:25:33,590 --> 00:25:39,559
each of those and then add the nine

00:25:37,070 --> 00:25:40,909
instances and getting all the right way

00:25:39,559 --> 00:25:42,230
around to the two new knows that you've

00:25:40,909 --> 00:25:44,450
just created and there's the number of

00:25:42,230 --> 00:25:45,769
nodes go up it just gets a little bit

00:25:44,450 --> 00:25:50,000
more complicated and a little more

00:25:45,769 --> 00:25:51,679
painful to manage so in the past I've

00:25:50,000 --> 00:25:53,419
had a preference for static membership

00:25:51,679 --> 00:25:55,370
because it's been easier to debug and

00:25:53,419 --> 00:25:57,649
it's met less resistance from sort of

00:25:55,370 --> 00:25:59,929
corporate IT these days my preference is

00:25:57,649 --> 00:26:02,090
for multicast primarily because it's

00:25:59,929 --> 00:26:07,940
just easier to to scale out and add

00:26:02,090 --> 00:26:10,460
additional nodes as you need to the next

00:26:07,940 --> 00:26:14,269
thing I want to talk about are send

00:26:10,460 --> 00:26:16,610
options now this is one of those things

00:26:14,269 --> 00:26:18,110
that are keeping yet we really should

00:26:16,610 --> 00:26:21,320
change the way this is configured it's

00:26:18,110 --> 00:26:22,940
not very consistent and I've just never

00:26:21,320 --> 00:26:24,620
got around to doing it and it's

00:26:22,940 --> 00:26:27,320
something we'd really need to do on a on

00:26:24,620 --> 00:26:29,840
a major release if you're using the

00:26:27,320 --> 00:26:31,610
Delta Manager what you're really talking

00:26:29,840 --> 00:26:33,620
about here is the channel send options

00:26:31,610 --> 00:26:35,510
which is on the cluster element but if

00:26:33,620 --> 00:26:37,040
you're using the backup manager you

00:26:35,510 --> 00:26:38,750
actually need to configure the Maps

00:26:37,040 --> 00:26:41,720
options which is on the manager element

00:26:38,750 --> 00:26:43,610
and timon those really should be on one

00:26:41,720 --> 00:26:45,740
element cluster manager I don't really

00:26:43,610 --> 00:26:48,260
care which one pick one and use it

00:26:45,740 --> 00:26:49,460
consistently between the two but we

00:26:48,260 --> 00:26:52,010
don't so that's just something to be

00:26:49,460 --> 00:26:54,890
aware of and really what the send

00:26:52,010 --> 00:26:58,340
options are controlling whether or not

00:26:54,890 --> 00:26:59,990
you send messages for session updates

00:26:58,340 --> 00:27:04,700
across the cluster in a synchronous

00:26:59,990 --> 00:27:06,980
manner or an asynchronous manner so for

00:27:04,700 --> 00:27:09,610
synchronous what this means is that the

00:27:06,980 --> 00:27:12,650
request processing will not complete

00:27:09,610 --> 00:27:15,340
until the session update has been sent

00:27:12,650 --> 00:27:18,710
to all of the other nodes in the cluster

00:27:15,340 --> 00:27:22,370
now that sort of for a given value of

00:27:18,710 --> 00:27:24,200
sent which can actually mean various

00:27:22,370 --> 00:27:26,870
things depending on how you set set your

00:27:24,200 --> 00:27:28,910
send options it might just be yeah I've

00:27:26,870 --> 00:27:31,460
written it to the TCP stack I've got no

00:27:28,910 --> 00:27:32,960
idea whether it sent it know it but I've

00:27:31,460 --> 00:27:35,570
written it the TCP stack that's good

00:27:32,960 --> 00:27:36,860
enough for me I'm carrying on it might

00:27:35,570 --> 00:27:39,530
mean okay

00:27:36,860 --> 00:27:41,150
I've sent this message to the to the

00:27:39,530 --> 00:27:42,860
other nodes in the cluster and they've

00:27:41,150 --> 00:27:46,340
all acknowledged that they have received

00:27:42,860 --> 00:27:49,190
it all that's obviously yet a level up

00:27:46,340 --> 00:27:51,350
in terms of assurance beyond that

00:27:49,190 --> 00:27:52,700
there's the okay I've sent this message

00:27:51,350 --> 00:27:55,160
to all of the other nodes in the cluster

00:27:52,700 --> 00:27:58,310
they have received it and they have

00:27:55,160 --> 00:28:00,470
processed it so if we're talking about

00:27:58,310 --> 00:28:01,880
updating a session that means okay I've

00:28:00,470 --> 00:28:03,170
sent the updates to all the other nodes

00:28:01,880 --> 00:28:04,640
in the cluster they've all received it

00:28:03,170 --> 00:28:06,440
they've all updated their local copies

00:28:04,640 --> 00:28:08,210
of the session so now all of the

00:28:06,440 --> 00:28:13,030
sessions across the cluster have the

00:28:08,210 --> 00:28:16,520
same view now if you're using

00:28:13,030 --> 00:28:19,400
synchronous send options and you choose

00:28:16,520 --> 00:28:21,050
the option of they must them any all

00:28:19,400 --> 00:28:24,230
messages must have been processed by the

00:28:21,050 --> 00:28:25,730
other node what that means is once one

00:28:24,230 --> 00:28:28,520
node in the cluster has process to

00:28:25,730 --> 00:28:31,070
request then the next request can go to

00:28:28,520 --> 00:28:32,570
any other node in the cluster and it

00:28:31,070 --> 00:28:34,760
will see exactly the same view of the

00:28:32,570 --> 00:28:37,570
HTTP session regardless of which node it

00:28:34,760 --> 00:28:39,950
goes to in the cluster if you're using

00:28:37,570 --> 00:28:41,840
asynchronous messages and that

00:28:39,950 --> 00:28:43,550
essentially means the message gets put

00:28:41,840 --> 00:28:44,870
on a queue and then the response goes

00:28:43,550 --> 00:28:45,980
back to the client you've got no idea

00:28:44,870 --> 00:28:47,070
whether the message you've been sent or

00:28:45,980 --> 00:28:50,460
not

00:28:47,070 --> 00:28:51,840
then the next request that's received if

00:28:50,460 --> 00:28:53,700
it goes to a different node in the

00:28:51,840 --> 00:28:55,590
cluster it might see the session update

00:28:53,700 --> 00:28:57,659
or it might not

00:28:55,590 --> 00:28:59,220
which then comes back to why you'd need

00:28:57,659 --> 00:29:00,539
to use sticky sessions to make sure that

00:28:59,220 --> 00:29:02,880
you see the updated version of the

00:29:00,539 --> 00:29:07,340
session so there's actually some

00:29:02,880 --> 00:29:10,500
interesting decisions to be made here if

00:29:07,340 --> 00:29:11,880
you want if you don't for some reason

00:29:10,500 --> 00:29:16,409
you don't want to use sticky sessions

00:29:11,880 --> 00:29:17,730
and you want to make sure that when a

00:29:16,409 --> 00:29:19,559
request after request has been processed

00:29:17,730 --> 00:29:21,000
the next request sees the updated

00:29:19,559 --> 00:29:23,039
session you have to do things

00:29:21,000 --> 00:29:24,539
synchronously and you have to choose the

00:29:23,039 --> 00:29:28,320
must have been processed by the other

00:29:24,539 --> 00:29:32,899
node option to some extent all of that

00:29:28,320 --> 00:29:34,950
to my mind is somewhat academic because

00:29:32,899 --> 00:29:37,799
browsers have this nasty habit of

00:29:34,950 --> 00:29:40,169
sending parallel requests which kind of

00:29:37,799 --> 00:29:43,470
throws everything I've just said out of

00:29:40,169 --> 00:29:46,860
the window somewhat and generally I find

00:29:43,470 --> 00:29:48,419
that for it to get a consistent view of

00:29:46,860 --> 00:29:50,789
the session you end up having to use two

00:29:48,419 --> 00:29:54,000
key sessions regardless but it does

00:29:50,789 --> 00:29:55,590
depend on exactly which requests need to

00:29:54,000 --> 00:29:57,090
see the updated session and how your

00:29:55,590 --> 00:29:58,649
application is structured and exactly

00:29:57,090 --> 00:30:02,009
what those requests are and what they're

00:29:58,649 --> 00:30:03,299
for if there's only ever one dynamic the

00:30:02,009 --> 00:30:05,789
resource and the other things sorts

00:30:03,299 --> 00:30:08,610
static bits and pieces images JavaScript

00:30:05,789 --> 00:30:13,139
then it's the parallel requests become

00:30:08,610 --> 00:30:15,990
less of an issue one other thing just to

00:30:13,139 --> 00:30:19,379
mention on the asynchronous front one of

00:30:15,990 --> 00:30:21,870
the one of the features of processing

00:30:19,379 --> 00:30:23,639
the messages asynchronously is then no

00:30:21,870 --> 00:30:25,110
there is no guarantee they will be

00:30:23,639 --> 00:30:28,259
processed in the same order they are

00:30:25,110 --> 00:30:30,539
sent so if you send out a message that

00:30:28,259 --> 00:30:32,759
says increment counter to one increment

00:30:30,539 --> 00:30:35,190
counter to two or set counter to one ii

00:30:32,759 --> 00:30:36,870
ii ii ii ii ii 3 which might just be the

00:30:35,190 --> 00:30:39,269
result of something incremental action

00:30:36,870 --> 00:30:40,830
it's possible that the updates might be

00:30:39,269 --> 00:30:43,110
processed on this on the receive side

00:30:40,830 --> 00:30:45,169
has set the value to 3 set value to 1

00:30:43,110 --> 00:30:47,669
set value to 3 set value to 2 and

00:30:45,169 --> 00:30:49,200
therefore you then don't have again have

00:30:47,669 --> 00:30:52,169
that consistent view of the updates

00:30:49,200 --> 00:30:53,610
across the session so something else to

00:30:52,169 --> 00:30:56,010
bear in mind and again people tend to

00:30:53,610 --> 00:30:58,789
fall back to the synchronous approach in

00:30:56,010 --> 00:30:58,789
order to avoid that

00:31:00,080 --> 00:31:05,120
so in summary around the configuration

00:31:03,290 --> 00:31:06,560
choices for the manager you need to

00:31:05,120 --> 00:31:10,460
choose the Delta or the backup manager

00:31:06,560 --> 00:31:12,260
so which which replication strategies

00:31:10,460 --> 00:31:14,960
you want do you want sticky sessions or

00:31:12,260 --> 00:31:16,960
not I would strongly recommend the

00:31:14,960 --> 00:31:20,060
answer to that question is yes

00:31:16,960 --> 00:31:21,290
membership multicast or static that's a

00:31:20,060 --> 00:31:23,450
little bit driven by your environment

00:31:21,290 --> 00:31:26,570
how big your cluster is but I'd lean

00:31:23,450 --> 00:31:28,580
towards multicast faff possible and send

00:31:26,570 --> 00:31:31,040
options synchronous or asynchronous and

00:31:28,580 --> 00:31:32,600
your choice there depends on you know

00:31:31,040 --> 00:31:34,360
how sure do you want to be that messages

00:31:32,600 --> 00:31:37,070
have been replicated across the cluster

00:31:34,360 --> 00:31:38,810
and how important it is is it that you

00:31:37,070 --> 00:31:40,040
have an absolutely consistent view and

00:31:38,810 --> 00:31:43,040
that messages are processed in the right

00:31:40,040 --> 00:31:44,660
order and the answers to those questions

00:31:43,040 --> 00:31:46,880
are really going to be very much

00:31:44,660 --> 00:31:48,860
application dependent there are some

00:31:46,880 --> 00:31:50,150
applications where yeah messages

00:31:48,860 --> 00:31:51,590
processing the wrong order actually we

00:31:50,150 --> 00:31:53,090
don't really mind very much because

00:31:51,590 --> 00:31:55,130
it'll get corrected when it processes

00:31:53,090 --> 00:31:56,480
the following message so if the

00:31:55,130 --> 00:32:00,620
information is slightly out of date in a

00:31:56,480 --> 00:32:04,520
backup yeah not a problem to move on to

00:32:00,620 --> 00:32:09,200
debugging there are really sort of two

00:32:04,520 --> 00:32:11,210
aspects to this so you can view them as

00:32:09,200 --> 00:32:12,980
a relatively easy one and the relatively

00:32:11,210 --> 00:32:14,660
difficult one the first bit is actually

00:32:12,980 --> 00:32:16,280
you've got a cluster for figured and you

00:32:14,660 --> 00:32:17,690
you want to make sure it's configured

00:32:16,280 --> 00:32:18,890
the way you think it's configured and

00:32:17,690 --> 00:32:20,120
you want to make sure failover and

00:32:18,890 --> 00:32:22,100
things are working the way that you

00:32:20,120 --> 00:32:24,050
think they should work so this is sort

00:32:22,100 --> 00:32:25,760
of your new installation you set up your

00:32:24,050 --> 00:32:27,710
four node cluster or your eight no close

00:32:25,760 --> 00:32:30,020
trust or whatever it is and you just

00:32:27,710 --> 00:32:33,650
want to test that it behaves the way you

00:32:30,020 --> 00:32:36,370
think it's meant to behave and really in

00:32:33,650 --> 00:32:38,540
order to do that when you send a request

00:32:36,370 --> 00:32:40,040
through the load balancer to the cluster

00:32:38,540 --> 00:32:43,910
what you really need to know is what's

00:32:40,040 --> 00:32:47,150
the session ID which back-end node

00:32:43,910 --> 00:32:48,650
handled that what's the route which

00:32:47,150 --> 00:32:51,380
should match to the right back end node

00:32:48,650 --> 00:32:53,390
and you might have other information as

00:32:51,380 --> 00:32:55,760
well like is some test value that I put

00:32:53,390 --> 00:32:57,890
in the session still present and I have

00:32:55,760 --> 00:32:59,870
a very simple JSP that I used to test

00:32:57,890 --> 00:33:02,120
this sort of thing I just set up a

00:32:59,870 --> 00:33:03,770
cluster I deploy this JSP across all of

00:33:02,120 --> 00:33:05,540
it and then I make sure that the cluster

00:33:03,770 --> 00:33:08,110
behaves the way that I wanted to and it

00:33:05,540 --> 00:33:10,000
just shows me what's current

00:33:08,110 --> 00:33:13,960
sessionid that gives me the session ID

00:33:10,000 --> 00:33:15,940
and the route what the IP address of

00:33:13,960 --> 00:33:18,430
this the current server that tells me

00:33:15,940 --> 00:33:19,630
which physical node is handling it and

00:33:18,430 --> 00:33:22,300
I've also got some stuff that lets me

00:33:19,630 --> 00:33:23,740
pop attributes into the session if it

00:33:22,300 --> 00:33:25,930
just dumps any attributes that are

00:33:23,740 --> 00:33:27,340
currently in the session and it's a sort

00:33:25,930 --> 00:33:28,720
of JSP page you can throw it together in

00:33:27,340 --> 00:33:30,880
about five minutes but it makes

00:33:28,720 --> 00:33:33,700
debugging cluster configurations a whole

00:33:30,880 --> 00:33:35,350
lot easier so the sort of thing I might

00:33:33,700 --> 00:33:37,030
do is turn off sticky sessions and then

00:33:35,350 --> 00:33:38,860
just check that it round robins or

00:33:37,030 --> 00:33:40,570
whatever my load balancing algorithm is

00:33:38,860 --> 00:33:41,650
around all of the nodes and the cluster

00:33:40,570 --> 00:33:43,270
to make sure that they're they're all

00:33:41,650 --> 00:33:45,790
serving requests and the load balancer

00:33:43,270 --> 00:33:47,290
knows about all of them the next thing I

00:33:45,790 --> 00:33:50,560
normally do is turn on sticky sessions

00:33:47,290 --> 00:33:53,110
and make sure that works and then you

00:33:50,560 --> 00:33:56,340
can start testing failover so I'm I'm

00:33:53,110 --> 00:34:00,190
happily using node a if node a fails and

00:33:56,340 --> 00:34:02,710
do I move over to node B or whatever I

00:34:00,190 --> 00:34:04,420
expect again the the behavior you expect

00:34:02,710 --> 00:34:05,680
will depend on how you've configured

00:34:04,420 --> 00:34:07,480
your load balance but really what you're

00:34:05,680 --> 00:34:10,380
doing here is making sure that the

00:34:07,480 --> 00:34:13,630
behavior is as expected

00:34:10,380 --> 00:34:15,340
you do need to keep in mind how your

00:34:13,630 --> 00:34:18,160
reverse proxy is configured to handle

00:34:15,340 --> 00:34:22,720
failed nodes and there are a couple of

00:34:18,160 --> 00:34:24,910
gotchas here the first and I'm going to

00:34:22,720 --> 00:34:30,460
talk about this in the context of httpd

00:34:24,910 --> 00:34:34,690
if you're using HTTP D and mod proxy if

00:34:30,460 --> 00:34:37,090
a back-end node fails then mod proxy

00:34:34,690 --> 00:34:39,700
will return a 503 error to the client

00:34:37,090 --> 00:34:45,030
end of story I try to contact us now

00:34:39,700 --> 00:34:47,470
that nodes down 503 comes back mod JK

00:34:45,030 --> 00:34:49,740
you can configure it to retry the

00:34:47,470 --> 00:34:52,660
request to another node in the cluster

00:34:49,740 --> 00:34:54,370
now that is potentially quite a nice

00:34:52,660 --> 00:34:56,440
thing because the client doesn't see the

00:34:54,370 --> 00:35:00,940
error message request just gets retried

00:34:56,440 --> 00:35:02,530
and the user sees the response and they

00:35:00,940 --> 00:35:04,380
will see a slightly slower response

00:35:02,530 --> 00:35:07,500
because there are some timeouts involved

00:35:04,380 --> 00:35:11,380
but that's potentially a good thing but

00:35:07,500 --> 00:35:13,000
it does mean that potentially your

00:35:11,380 --> 00:35:15,490
requests got sent to one node in the

00:35:13,000 --> 00:35:17,410
cluster it processed it and something

00:35:15,490 --> 00:35:19,870
went wrong when the response was being

00:35:17,410 --> 00:35:20,510
sent back and now you've just sent the

00:35:19,870 --> 00:35:22,010
note there

00:35:20,510 --> 00:35:24,410
Qwest to another node in the cluster and

00:35:22,010 --> 00:35:26,000
it's processed it again there take an

00:35:24,410 --> 00:35:27,620
extreme example if this is a banking

00:35:26,000 --> 00:35:29,630
application that's processing a deposit

00:35:27,620 --> 00:35:31,730
for a thousand dollars you've just had

00:35:29,630 --> 00:35:33,200
two thousand dollars deposit you didn't

00:35:31,730 --> 00:35:35,120
that particular account rather than a

00:35:33,200 --> 00:35:36,050
thousand now as a user I'd be quite

00:35:35,120 --> 00:35:37,850
happy about that

00:35:36,050 --> 00:35:40,220
but I'd be less happy if we were talking

00:35:37,850 --> 00:35:42,140
about our withdrawal so there are things

00:35:40,220 --> 00:35:45,650
that you need to bear in mind and mod JK

00:35:42,140 --> 00:35:47,600
has more knobs for you to twiddle to

00:35:45,650 --> 00:35:49,580
configure this behavior then you would

00:35:47,600 --> 00:35:51,620
believe imaginable and they're all

00:35:49,580 --> 00:35:53,960
documented on the website and you can

00:35:51,620 --> 00:35:55,520
control how you handle no dude if you

00:35:53,960 --> 00:35:57,530
just get requests to do it for post

00:35:55,520 --> 00:35:59,690
requests dude your own particular error

00:35:57,530 --> 00:36:01,580
codes of what the timeouts are there's

00:35:59,690 --> 00:36:04,400
lots and lots of control and it's one of

00:36:01,580 --> 00:36:06,410
those areas where mod JK has finer

00:36:04,400 --> 00:36:09,950
grained control over the behavior than

00:36:06,410 --> 00:36:12,320
is available through three mod proxy the

00:36:09,950 --> 00:36:14,300
other thing to bear in mind is that the

00:36:12,320 --> 00:36:16,130
reverse proxy once it's tried talking to

00:36:14,300 --> 00:36:19,370
a node and it's decided that oh that

00:36:16,130 --> 00:36:21,470
node has failed it usually sort of marks

00:36:19,370 --> 00:36:22,940
that known mentally has failed for a

00:36:21,470 --> 00:36:25,190
period of time I think by default it's

00:36:22,940 --> 00:36:27,590
60 seconds so if you're doing a quick

00:36:25,190 --> 00:36:29,210
cluster test where you've got say four

00:36:27,590 --> 00:36:31,340
nodes in a cluster you take take node a

00:36:29,210 --> 00:36:33,470
down great about snow B take though B

00:36:31,340 --> 00:36:35,570
down great about snowed C take note seed

00:36:33,470 --> 00:36:37,250
a and great about two node D right let's

00:36:35,570 --> 00:36:40,310
turn turn all the other nodes back on

00:36:37,250 --> 00:36:42,830
take no D down and I get a failed

00:36:40,310 --> 00:36:44,390
request and the reason for that is if

00:36:42,830 --> 00:36:45,800
you've done it quickly enough if you've

00:36:44,390 --> 00:36:48,110
taken all four of those nodes down

00:36:45,800 --> 00:36:49,640
within 60 seconds you reverse proxy

00:36:48,110 --> 00:36:51,800
thinks well all of the nodes are down

00:36:49,640 --> 00:36:53,660
I'm not going to retry Colt talking to

00:36:51,800 --> 00:36:55,340
any of them until 60 seconds after I

00:36:53,660 --> 00:36:57,230
know that they failed the last time and

00:36:55,340 --> 00:37:00,050
at which point all of your requests

00:36:57,230 --> 00:37:01,100
start failing which can be a little bit

00:37:00,050 --> 00:37:03,500
surprising so again it's all about

00:37:01,100 --> 00:37:05,330
remembering how your reverse proxy is

00:37:03,500 --> 00:37:06,860
configured to handle those failures and

00:37:05,330 --> 00:37:09,550
then make sure that your testing takes

00:37:06,860 --> 00:37:09,550
account of that

00:37:11,740 --> 00:37:15,860
so that's sort of debugging your cluster

00:37:14,270 --> 00:37:17,330
configuration to start with to make sure

00:37:15,860 --> 00:37:20,390
that everything is working that the way

00:37:17,330 --> 00:37:24,560
you expect the more complicated problem

00:37:20,390 --> 00:37:26,720
is debugging application problems it's

00:37:24,560 --> 00:37:28,040
in some ways is just like debugging any

00:37:26,720 --> 00:37:30,800
other application problem

00:37:28,040 --> 00:37:32,320
the downside is it's harder because

00:37:30,800 --> 00:37:35,510
you've got a lot more moving parts

00:37:32,320 --> 00:37:37,280
you're less sure about where things are

00:37:35,510 --> 00:37:38,630
which particular node is processing it

00:37:37,280 --> 00:37:40,940
where's you know where do I need to go

00:37:38,630 --> 00:37:43,070
looking for error messages and it gets

00:37:40,940 --> 00:37:45,170
really tricky if you're talking about

00:37:43,070 --> 00:37:47,090
well we only see this problem if one of

00:37:45,170 --> 00:37:49,820
the nodes fails and then not all of the

00:37:47,090 --> 00:37:51,860
time trying to figure out what's going

00:37:49,820 --> 00:37:54,550
on in those situations can be very

00:37:51,860 --> 00:37:56,750
difficult I don't think really there's a

00:37:54,550 --> 00:37:58,150
foolproof oh you just do this and you'll

00:37:56,750 --> 00:38:00,860
always be able to find the problem I

00:37:58,150 --> 00:38:02,390
wish there was if there and if there was

00:38:00,860 --> 00:38:04,070
I'd share it with you but unfortunately

00:38:02,390 --> 00:38:05,390
there isn't and the first thing I

00:38:04,070 --> 00:38:07,430
normally look for is it can i reproduce

00:38:05,390 --> 00:38:09,770
the problem without the cluster if I can

00:38:07,430 --> 00:38:11,960
take that particular moving part out out

00:38:09,770 --> 00:38:14,330
of the out of the problem space great

00:38:11,960 --> 00:38:15,740
and to some extent it's like any other

00:38:14,330 --> 00:38:18,050
problem what you're trying to do is

00:38:15,740 --> 00:38:20,540
reduce the issue to the simplest

00:38:18,050 --> 00:38:23,480
possible most reproducible test case you

00:38:20,540 --> 00:38:24,710
can you can sort of taking looking at it

00:38:23,480 --> 00:38:27,560
a little bit more generally from a

00:38:24,710 --> 00:38:30,110
tomcat point of view I really like bug

00:38:27,560 --> 00:38:32,330
reports that come with war files with

00:38:30,110 --> 00:38:34,910
source code that say stick this on a

00:38:32,330 --> 00:38:36,530
tomcat instance go to this index go to

00:38:34,910 --> 00:38:38,600
the index page click this link and watch

00:38:36,530 --> 00:38:40,970
it fall over because that's great that's

00:38:38,600 --> 00:38:44,930
dead easy to reproduce dead easy for me

00:38:40,970 --> 00:38:50,120
to debug bug reports on lines of Tomcat

00:38:44,930 --> 00:38:51,380
sometimes fails gray in what way oh well

00:38:50,120 --> 00:38:53,240
we've got this snap trace how'd you

00:38:51,380 --> 00:38:56,050
trigger it don't know how many'd you get

00:38:53,240 --> 00:38:59,990
it we get it back once every two months

00:38:56,050 --> 00:39:02,480
right that sort of it is a lot harder to

00:38:59,990 --> 00:39:04,220
deeply you can't reproduce it you can't

00:39:02,480 --> 00:39:06,980
do the sorts of analysis that you'd

00:39:04,220 --> 00:39:08,270
normally do so as always simplest

00:39:06,980 --> 00:39:10,520
possible test cases what you're aiming

00:39:08,270 --> 00:39:13,310
for and getting rid of clustering is the

00:39:10,520 --> 00:39:16,250
first thing I would try and do if you're

00:39:13,310 --> 00:39:18,080
having network or failover issues then

00:39:16,250 --> 00:39:19,490
one of the things you that I find is

00:39:18,080 --> 00:39:21,470
very useful to look at is look at the

00:39:19,490 --> 00:39:23,900
access logs and at this point it's kind

00:39:21,470 --> 00:39:24,500
of critical that the access logs include

00:39:23,900 --> 00:39:26,450
the session

00:39:24,500 --> 00:39:28,940
days because the session IDs would

00:39:26,450 --> 00:39:31,340
include the JVM roots and then you can

00:39:28,940 --> 00:39:32,930
use that to match up well this request

00:39:31,340 --> 00:39:34,460
should have been handled by node a so I

00:39:32,930 --> 00:39:36,620
can go and look at no days access log

00:39:34,460 --> 00:39:40,070
does that actually appear in the right

00:39:36,620 --> 00:39:44,780
place yes it does great one of the

00:39:40,070 --> 00:39:50,030
things to watch out for is when access

00:39:44,780 --> 00:39:51,920
logs are written the very nature of an

00:39:50,030 --> 00:39:54,260
access log entry is it can only be

00:39:51,920 --> 00:39:55,850
written once once the request once the

00:39:54,260 --> 00:39:57,440
responses be generated so it can include

00:39:55,850 --> 00:39:58,520
things like that the length of the

00:39:57,440 --> 00:40:01,550
response how many bytes were written

00:39:58,520 --> 00:40:04,790
that does mean that if a request fails

00:40:01,550 --> 00:40:07,910
and something times out and your timeout

00:40:04,790 --> 00:40:09,320
is say 20 seconds then the entry for

00:40:07,910 --> 00:40:11,030
that request is actually gonna appear 20

00:40:09,320 --> 00:40:13,190
seconds further down the access log than

00:40:11,030 --> 00:40:14,810
when the request started so you do n

00:40:13,190 --> 00:40:17,840
entries in access logs are not

00:40:14,810 --> 00:40:19,940
necessarily in time order of when the

00:40:17,840 --> 00:40:22,580
request started there in time order of

00:40:19,940 --> 00:40:24,050
when the request finished and you need

00:40:22,580 --> 00:40:25,400
to remember the timeouts when you're

00:40:24,050 --> 00:40:27,320
looking in your access logs to go

00:40:25,400 --> 00:40:29,750
looking in the right places that's

00:40:27,320 --> 00:40:31,670
caught me a few times after seeing the

00:40:29,750 --> 00:40:33,110
access logs where is it gone they're not

00:40:31,670 --> 00:40:35,000
roll through it and I eventually find it

00:40:33,110 --> 00:40:37,340
20 seconds later and then I realize what

00:40:35,000 --> 00:40:40,010
I've done wrong and that applies to both

00:40:37,340 --> 00:40:41,960
httpd and Tomcat well if you look at the

00:40:40,010 --> 00:40:44,780
access log look at the error logs there

00:40:41,960 --> 00:40:48,170
might be something useful in there there

00:40:44,780 --> 00:40:49,610
have been cases where I've basically had

00:40:48,170 --> 00:40:52,040
no choice but to look at the network

00:40:49,610 --> 00:40:54,890
traffic which involves them right get

00:40:52,040 --> 00:40:56,660
out TCP dump load up a copy of Wireshark

00:40:54,890 --> 00:41:00,170
and stock trawl start trawling through

00:40:56,660 --> 00:41:03,020
huge amounts of data it's not pretty

00:41:00,170 --> 00:41:05,690
it's a lot easier if you if you've got

00:41:03,020 --> 00:41:07,220
the sort of problem where we don't quite

00:41:05,690 --> 00:41:08,930
know what's going on but we know if we

00:41:07,220 --> 00:41:10,310
let the users hammer at this system for

00:41:08,930 --> 00:41:12,620
15 minutes we're going to get a couple

00:41:10,310 --> 00:41:14,810
of these fail is that's sort of time

00:41:12,620 --> 00:41:16,130
where if there's nothing in nothing in

00:41:14,810 --> 00:41:17,480
the access logs nothing in the error

00:41:16,130 --> 00:41:21,020
logs nothing obviously going wrong

00:41:17,480 --> 00:41:22,970
that's what I might get out TCP dump and

00:41:21,020 --> 00:41:24,740
they say well the user can tell me when

00:41:22,970 --> 00:41:26,240
it fails so they can tell me their IP

00:41:24,740 --> 00:41:28,040
address in the time and then I'll not go

00:41:26,240 --> 00:41:29,930
looking and we'll find the TCP stream

00:41:28,040 --> 00:41:31,100
where it went wrong and then that might

00:41:29,930 --> 00:41:33,200
give me some clues as to what's

00:41:31,100 --> 00:41:35,390
happening and I've I've successfully

00:41:33,200 --> 00:41:37,860
done that a couple of times and it's

00:41:35,390 --> 00:41:40,890
hard work it's it's slow going but you

00:41:37,860 --> 00:41:46,290
get there and the root cause of the

00:41:40,890 --> 00:41:49,640
problem there was a Internet Explorer

00:41:46,290 --> 00:41:52,410
taking advantage of some unofficial

00:41:49,640 --> 00:41:55,560
Microsoft not quite following the HTTP

00:41:52,410 --> 00:41:59,580
specification features and passing them

00:41:55,560 --> 00:42:01,080
on to httpd that was saying no that's

00:41:59,580 --> 00:42:03,600
not spec compliant thank you very much

00:42:01,080 --> 00:42:05,130
go away and just resetting the

00:42:03,600 --> 00:42:07,050
connection before I even got anywhere

00:42:05,130 --> 00:42:09,180
near the access logs it's only by

00:42:07,050 --> 00:42:13,920
looking at the network level don't we

00:42:09,180 --> 00:42:15,540
could actually see what was going on so

00:42:13,920 --> 00:42:18,570
to some extent those those network

00:42:15,540 --> 00:42:20,640
issues are a little bit easier to do to

00:42:18,570 --> 00:42:22,980
debug because it the worst case you can

00:42:20,640 --> 00:42:27,090
look at the raw traffic and figure out

00:42:22,980 --> 00:42:29,310
what's going on and it's sort of a sort

00:42:27,090 --> 00:42:32,310
of heavyweight approach but it is there

00:42:29,310 --> 00:42:34,650
as a root of last resort if you've got

00:42:32,310 --> 00:42:37,620
application issues then it really comes

00:42:34,650 --> 00:42:40,260
down to logging logging and more logging

00:42:37,620 --> 00:42:41,970
the better the logging that's available

00:42:40,260 --> 00:42:43,740
from your application the more likely

00:42:41,970 --> 00:42:47,250
you'll go you are to be able to figure

00:42:43,740 --> 00:42:48,660
out what's happening and you need to be

00:42:47,250 --> 00:42:51,090
able to fine-tune it you need to be able

00:42:48,660 --> 00:42:52,740
to narrow it down to ok this is where

00:42:51,090 --> 00:42:56,600
the this is where the problem is it's

00:42:52,740 --> 00:42:56,600
this particular feature it's this bit

00:42:56,810 --> 00:43:05,250
again if it's repeatable and if you can

00:43:00,920 --> 00:43:06,870
then my most of starting point is can I

00:43:05,250 --> 00:43:08,520
attach a debugger to that tomcat

00:43:06,870 --> 00:43:10,650
instance and put a breakpoint in oh I

00:43:08,520 --> 00:43:13,290
can excellent right a lot of life

00:43:10,650 --> 00:43:14,370
suddenly got a lot easier because now if

00:43:13,290 --> 00:43:15,600
we just sit there waiting for the

00:43:14,370 --> 00:43:16,860
problem to occur and then I can step

00:43:15,600 --> 00:43:19,290
through and see exactly what's happening

00:43:16,860 --> 00:43:21,840
if I can't do that and I'm trying to do

00:43:19,290 --> 00:43:25,380
everything via access via application I

00:43:21,840 --> 00:43:26,520
mean it gets a lot harder it's kind of

00:43:25,380 --> 00:43:27,930
at this point you really need to make

00:43:26,520 --> 00:43:29,130
sure you've got application developers

00:43:27,930 --> 00:43:32,040
that really understand the application

00:43:29,130 --> 00:43:33,840
code working alongside you so they say

00:43:32,040 --> 00:43:35,010
I'll yet you know that they've got the

00:43:33,840 --> 00:43:36,600
understanding of the application

00:43:35,010 --> 00:43:38,790
architecture so they can focus in on the

00:43:36,600 --> 00:43:40,890
right base trying to do this without the

00:43:38,790 --> 00:43:46,830
knowledge of the application is really

00:43:40,890 --> 00:43:47,150
really painful ok that brings me to the

00:43:46,830 --> 00:43:48,260
end

00:43:47,150 --> 00:43:50,480
what I wanted to talk about I'm

00:43:48,260 --> 00:43:51,890
obviously happy to take any questions so

00:43:50,480 --> 00:43:54,770
I think we've got about 15 minutes for

00:43:51,890 --> 00:43:56,210
questions yeah you know want to step up

00:43:54,770 --> 00:44:02,960
to the mic so we can get it for the

00:43:56,210 --> 00:44:05,720
recording please so my question is

00:44:02,960 --> 00:44:09,410
basically when you talk about failure in

00:44:05,720 --> 00:44:10,880
the context of a cluster what's the the

00:44:09,410 --> 00:44:12,730
actual formal definition of a failure

00:44:10,880 --> 00:44:16,430
does it fail a ping test does it fail

00:44:12,730 --> 00:44:19,550
what what are the conditions that this

00:44:16,430 --> 00:44:21,380
would be considered a fail okay so the

00:44:19,550 --> 00:44:23,240
in terms of the conditions the sort of

00:44:21,380 --> 00:44:25,070
failure conditions for a node it's

00:44:23,240 --> 00:44:27,230
actually something that you configure

00:44:25,070 --> 00:44:29,720
within that the load balancer so it

00:44:27,230 --> 00:44:31,310
might it's normally the form i've sent a

00:44:29,720 --> 00:44:34,370
request to that node and I haven't had a

00:44:31,310 --> 00:44:37,880
response back in X seconds it might be

00:44:34,370 --> 00:44:39,470
I've had a 500 error code of some form

00:44:37,880 --> 00:44:41,720
and I'm going to assume that that's

00:44:39,470 --> 00:44:45,080
that's fatal it usually isn't but I'm

00:44:41,720 --> 00:44:47,570
going to assume that it is normally the

00:44:45,080 --> 00:44:51,260
sorts of things that go wrong are things

00:44:47,570 --> 00:44:54,410
like get an out of memory error on the

00:44:51,260 --> 00:44:55,910
JVM that's running Tomcat yeah that's 99

00:44:54,410 --> 00:44:58,550
times out of 100 that's instantly fatal

00:44:55,910 --> 00:45:01,130
or you get a JVM crash and it get

00:44:58,550 --> 00:45:02,300
process just disappears and the load

00:45:01,130 --> 00:45:05,330
balancer is sat there waiting for a

00:45:02,300 --> 00:45:08,030
response so normally it's request has

00:45:05,330 --> 00:45:10,850
taken X seconds to more than X seconds

00:45:08,030 --> 00:45:12,560
to come back and it hasn't that does

00:45:10,850 --> 00:45:15,020
mean that things get really complicated

00:45:12,560 --> 00:45:16,250
if you have an application where most

00:45:15,020 --> 00:45:18,860
requests should come back within a

00:45:16,250 --> 00:45:21,080
second but the odd request might take a

00:45:18,860 --> 00:45:22,760
minute or more to process suddenly your

00:45:21,080 --> 00:45:24,320
timeout has to be a minute or two

00:45:22,760 --> 00:45:26,450
minutes so that means even if one of the

00:45:24,320 --> 00:45:31,210
simple requests fail it ends up there

00:45:26,450 --> 00:45:33,350
spinning for two minutes so you the more

00:45:31,210 --> 00:45:35,150
responsive you can make your application

00:45:33,350 --> 00:45:37,100
in those scenarios then the easier it is

00:45:35,150 --> 00:45:40,070
to set up your sort of your failover

00:45:37,100 --> 00:45:41,630
configuration really you want to avoid

00:45:40,070 --> 00:45:43,640
those long-running requests you want to

00:45:41,630 --> 00:45:45,020
use some other processing basically kick

00:45:43,640 --> 00:45:46,640
them off as a background process and

00:45:45,020 --> 00:45:49,220
check for results periodically that kind

00:45:46,640 --> 00:45:50,750
of thing so really that the definition

00:45:49,220 --> 00:45:53,000
of failure is whatever you define it to

00:45:50,750 --> 00:45:55,640
be in the load balancer and again what

00:45:53,000 --> 00:45:58,400
JK has more fine tuning available than

00:45:55,640 --> 00:46:02,480
mod proxy to do that

00:45:58,400 --> 00:46:05,890
okay are there any disadvantages to

00:46:02,480 --> 00:46:08,780
using deploy or any gotchas that you can

00:46:05,890 --> 00:46:12,680
are there any disadvantages to using the

00:46:08,780 --> 00:46:16,490
deployer within the cluster the

00:46:12,680 --> 00:46:21,770
documentation isn't great it definitely

00:46:16,490 --> 00:46:23,180
works I know what when I fix the message

00:46:21,770 --> 00:46:25,160
order problem then I did a bunch of

00:46:23,180 --> 00:46:27,710
other testing as well and it all works

00:46:25,160 --> 00:46:30,530
the documentation could probably be a

00:46:27,710 --> 00:46:32,750
bit better but other than that if you

00:46:30,530 --> 00:46:37,490
have be using it go for it I think I'd

00:46:32,750 --> 00:46:39,200
be a little bit wary of using in

00:46:37,490 --> 00:46:41,540
production but then again I'm generally

00:46:39,200 --> 00:46:44,360
wary of doing any form of live deport

00:46:41,540 --> 00:46:48,010
deployment in production one thing I

00:46:44,360 --> 00:46:51,410
will add is that it has been tested with

00:46:48,010 --> 00:46:52,760
Tomcats parallel deployment feature for

00:46:51,410 --> 00:46:56,000
those of you that aren't aware that lets

00:46:52,760 --> 00:46:58,760
you deploy multiple versions of a web

00:46:56,000 --> 00:47:00,080
application in parallel and sessions

00:46:58,760 --> 00:47:02,300
that are associated with the older

00:47:00,080 --> 00:47:04,520
version will carry on being processed by

00:47:02,300 --> 00:47:06,020
that older version and new sessions will

00:47:04,520 --> 00:47:08,330
be processed by the new version so it's

00:47:06,020 --> 00:47:10,790
a way of doing application upgrades

00:47:08,330 --> 00:47:12,650
without having to sort of drop users are

00:47:10,790 --> 00:47:14,990
using the old version of the app and a

00:47:12,650 --> 00:47:16,970
more recent addition is you can actually

00:47:14,990 --> 00:47:19,100
get tomcat to automatically and deploy

00:47:16,970 --> 00:47:20,840
the old version of the app once all of

00:47:19,100 --> 00:47:22,490
the sessions have expired and that

00:47:20,840 --> 00:47:23,840
parallel deployment works with the

00:47:22,490 --> 00:47:26,530
cluster deployer as well that's also

00:47:23,840 --> 00:47:26,530
been tested

00:47:29,960 --> 00:47:35,779
I have a question about the memberships

00:47:31,849 --> 00:47:37,700
so what if like the is ER first of all

00:47:35,779 --> 00:47:39,289
is there a leader concept of any sort

00:47:37,700 --> 00:47:41,660
and second is what if let's say that no

00:47:39,289 --> 00:47:43,819
D comes back up but maybe it's in a

00:47:41,660 --> 00:47:45,829
little bit of a network isolation among

00:47:43,819 --> 00:47:47,829
the other nodes but still okay with the

00:47:45,829 --> 00:47:52,930
look from the load balancer perspective

00:47:47,829 --> 00:47:52,930
so how does that get handled okay

00:47:53,019 --> 00:48:00,319
yes there is a concept of leadership

00:47:58,339 --> 00:48:03,410
exactly how it works I would have to

00:48:00,319 --> 00:48:05,539
read the source code but to remind

00:48:03,410 --> 00:48:09,470
myself is not something I've actually

00:48:05,539 --> 00:48:10,970
done a huge amount of work on I remember

00:48:09,470 --> 00:48:12,499
seeing the word leadership in a comment

00:48:10,970 --> 00:48:13,609
somewhere but beyond that I'd have to

00:48:12,499 --> 00:48:18,559
look through the source code to see how

00:48:13,609 --> 00:48:20,059
it's working in terms of difference

00:48:18,559 --> 00:48:21,769
views between what the load balancer

00:48:20,059 --> 00:48:26,019
thinks is working and what the cluster

00:48:21,769 --> 00:48:28,309
thinks is working one of the issues with

00:48:26,019 --> 00:48:30,140
Tomcats clustering implementation is

00:48:28,309 --> 00:48:31,789
that there is a disconnect there they

00:48:30,140 --> 00:48:32,210
have two completely different views of

00:48:31,789 --> 00:48:34,279
the world

00:48:32,210 --> 00:48:36,529
they're usually aligned but they might

00:48:34,279 --> 00:48:40,160
not be if you want them to be aligned

00:48:36,529 --> 00:48:42,289
then one of there is a mod cluster

00:48:40,160 --> 00:48:45,079
written by the JBoss folks at Red Hat

00:48:42,289 --> 00:48:46,789
and that tries to get communication from

00:48:45,079 --> 00:48:48,739
the cluster back to the load balancer to

00:48:46,789 --> 00:48:52,489
inform it when notes go up when those go

00:48:48,739 --> 00:48:54,109
down it's not something that I've

00:48:52,489 --> 00:48:56,359
actually used but I know it's there and

00:48:54,109 --> 00:48:58,609
it's used by others so that tries to

00:48:56,359 --> 00:49:00,410
have a more coherent view of what the

00:48:58,609 --> 00:49:02,269
state of the cluster is there's one

00:49:00,410 --> 00:49:05,720
other thing that things that's been on

00:49:02,269 --> 00:49:07,519
the to-do list for Tomcat and mod proxy

00:49:05,720 --> 00:49:10,970
and what JK for a while but nothing's

00:49:07,519 --> 00:49:13,190
actually happened yet so long the same

00:49:10,970 --> 00:49:15,079
line so you know other Apache projects

00:49:13,190 --> 00:49:16,549
like zookeeper and stuff do things like

00:49:15,079 --> 00:49:19,489
you know memberships management and

00:49:16,549 --> 00:49:22,160
things like that is Tomcat you know

00:49:19,489 --> 00:49:24,230
using the same code base or is a totally

00:49:22,160 --> 00:49:26,630
different approach or I mean how is that

00:49:24,230 --> 00:49:28,130
I hate so it's doing what form of

00:49:26,630 --> 00:49:29,930
management like the membership

00:49:28,130 --> 00:49:31,190
management has cluster management like

00:49:29,930 --> 00:49:33,049
you know their other projects in Apache

00:49:31,190 --> 00:49:35,690
you know probably spending a lot more

00:49:33,049 --> 00:49:38,119
resources on those kind of issues yeah

00:49:35,690 --> 00:49:40,519
Tom the Tomcat faster implementation has

00:49:38,119 --> 00:49:42,410
been around for a while it's actually

00:49:40,519 --> 00:49:47,390
built on something called

00:49:42,410 --> 00:49:50,660
which we have just what we do distribute

00:49:47,390 --> 00:49:53,150
as a separate module through the central

00:49:50,660 --> 00:49:58,250
maven repos some people do use it

00:49:53,150 --> 00:50:00,790
independently but I'm not aware of any

00:49:58,250 --> 00:50:03,530
other Apache project using it and I

00:50:00,790 --> 00:50:05,930
suspect knowing the person that wrote it

00:50:03,530 --> 00:50:07,970
that it was written from scratch rather

00:50:05,930 --> 00:50:13,310
than taking something that already

00:50:07,970 --> 00:50:16,670
existed and there are other folks out

00:50:13,310 --> 00:50:19,940
there that have written their own sort

00:50:16,670 --> 00:50:21,080
of backends to the clustering I happen

00:50:19,940 --> 00:50:23,570
to know this through work with the gem

00:50:21,080 --> 00:50:28,280
fire folks have written to cluster

00:50:23,570 --> 00:50:31,070
managers and oh there's one who I really

00:50:28,280 --> 00:50:32,600
am really should be more to the front of

00:50:31,070 --> 00:50:33,980
my mind than it is well I'll go looking

00:50:32,600 --> 00:50:35,510
the mailing lists if it doesn't come to

00:50:33,980 --> 00:50:38,480
me but there are other third-party

00:50:35,510 --> 00:50:41,120
implementations of either the clustering

00:50:38,480 --> 00:50:44,090
module or Tomcat session manager that

00:50:41,120 --> 00:50:46,010
then let you use third-party replication

00:50:44,090 --> 00:50:50,290
technologies to sort of replicate the

00:50:46,010 --> 00:50:53,210
session - data across a cluster you

00:50:50,290 --> 00:50:55,820
talked previously about parallel

00:50:53,210 --> 00:50:58,210
deployments with the deployer mm-hm

00:50:55,820 --> 00:51:00,950
I'd like to know if you could talk about

00:50:58,210 --> 00:51:03,200
what what's recommended for the

00:51:00,950 --> 00:51:04,850
clustering architecture the requirements

00:51:03,200 --> 00:51:08,090
overall to implement something like that

00:51:04,850 --> 00:51:11,450
and if you knew anybody in the industry

00:51:08,090 --> 00:51:15,410
that uses it on a larger scale okay

00:51:11,450 --> 00:51:17,930
challenges okay in terms of the

00:51:15,410 --> 00:51:19,460
requirements for using that parallel

00:51:17,930 --> 00:51:20,840
deployments with the deployer across a

00:51:19,460 --> 00:51:23,150
cluster

00:51:20,840 --> 00:51:24,800
its standard cluster setup you just add

00:51:23,150 --> 00:51:26,960
the deployer tag into server dot XML

00:51:24,800 --> 00:51:28,880
that's all you need to do and set up a

00:51:26,960 --> 00:51:32,090
few directories on each of the nodes for

00:51:28,880 --> 00:51:35,180
the files to be dropped into and it just

00:51:32,090 --> 00:51:38,810
works in terms of people using it in

00:51:35,180 --> 00:51:40,760
production I can honestly can't think of

00:51:38,810 --> 00:51:42,710
anybody that there are definitely people

00:51:40,760 --> 00:51:44,780
using what I'd have to do is go back

00:51:42,710 --> 00:51:46,100
through the mailing lists and see if the

00:51:44,780 --> 00:51:47,390
people have been asking questions about

00:51:46,100 --> 00:51:49,190
it have done it with their work email

00:51:47,390 --> 00:51:50,960
address or a gmail email address and

00:51:49,190 --> 00:51:52,640
then we could take an educated guess as

00:51:50,960 --> 00:51:56,000
to which organizations might be using it

00:51:52,640 --> 00:51:59,130
but beyond that

00:51:56,000 --> 00:52:02,490
I'm not aware of anybody and again I

00:51:59,130 --> 00:52:04,019
would my suspicion would be the in

00:52:02,490 --> 00:52:08,480
production people would be more likely

00:52:04,019 --> 00:52:10,650
to manually copy the war files across

00:52:08,480 --> 00:52:13,500
manually take nodes out of a cluster

00:52:10,650 --> 00:52:19,289
update them bring them back then use use

00:52:13,500 --> 00:52:22,200
the deployer Chris I get a couple of

00:52:19,289 --> 00:52:23,880
questions as usual if you're using the

00:52:22,200 --> 00:52:27,329
backup manager you may have actually

00:52:23,880 --> 00:52:29,279
mentioned this but if if a backup node

00:52:27,329 --> 00:52:32,730
goes down or if a node goes down that is

00:52:29,279 --> 00:52:35,670
the backup node for any set of sessions

00:52:32,730 --> 00:52:38,549
does the primary detect that and sort of

00:52:35,670 --> 00:52:40,559
read backup yes okay so it's a tree

00:52:38,549 --> 00:52:48,029
balances its yeah so as soon as a note

00:52:40,559 --> 00:52:49,650
goes down if an if a if another node is

00:52:48,029 --> 00:52:51,450
the primary it selects a different

00:52:49,650 --> 00:52:52,799
backup if the node it's gone down was

00:52:51,450 --> 00:52:54,690
its backup its setting you and if it's

00:52:52,799 --> 00:52:58,440
the backup it makes itself the primary

00:52:54,690 --> 00:53:00,329
and selects a new backup so it does

00:52:58,440 --> 00:53:03,660
rebalance itself and you end up with

00:53:00,329 --> 00:53:05,309
again one node failed and a primary and

00:53:03,660 --> 00:53:08,609
a backup of every session across the

00:53:05,309 --> 00:53:10,529
cluster and again every node in the

00:53:08,609 --> 00:53:11,880
cluster knows where the new primary and

00:53:10,529 --> 00:53:15,900
the new backups are they all have that

00:53:11,880 --> 00:53:21,269
coherent picture okay in a pathological

00:53:15,900 --> 00:53:22,950
case if a if a node were to think that

00:53:21,269 --> 00:53:25,559
it went down like instead of actually a

00:53:22,950 --> 00:53:28,349
JVM crash or the power goes out to the

00:53:25,559 --> 00:53:30,599
machine if instead you know somebody

00:53:28,349 --> 00:53:32,369
unplug the network cable and so that

00:53:30,599 --> 00:53:34,470
machine is still running in its own

00:53:32,369 --> 00:53:36,450
little isolated world it actually thinks

00:53:34,470 --> 00:53:39,029
that everyone else is down yes it's

00:53:36,450 --> 00:53:40,319
still up so it's gonna go crazy the rest

00:53:39,029 --> 00:53:42,720
of the network is gonna recover what

00:53:40,319 --> 00:53:44,339
happens when you then rejoin it okay the

00:53:42,720 --> 00:53:46,109
first thing that node is going to do is

00:53:44,339 --> 00:53:50,190
make itself the primary for all the

00:53:46,109 --> 00:53:54,000
sessions it knows about then when it

00:53:50,190 --> 00:53:55,579
rejoins I honestly don't know what's

00:53:54,000 --> 00:53:59,880
going to happen

00:53:55,579 --> 00:54:01,859
well it won't advertise that it is right

00:53:59,880 --> 00:54:05,160
it is either going to get very very

00:54:01,859 --> 00:54:07,240
messy or I suspect what will happen is

00:54:05,160 --> 00:54:08,680
that there is some coding

00:54:07,240 --> 00:54:11,170
for just to elect which one's the

00:54:08,680 --> 00:54:12,520
primary but I'd actually have to look at

00:54:11,170 --> 00:54:17,590
the source code to see how it handles

00:54:12,520 --> 00:54:20,320
that she was working at her than all the

00:54:17,590 --> 00:54:22,270
node handling failover works that was a

00:54:20,320 --> 00:54:23,680
look at the source code it's actually

00:54:22,270 --> 00:54:25,510
pretty well commented and fairly clear

00:54:23,680 --> 00:54:27,670
what happens but I don't remember

00:54:25,510 --> 00:54:28,630
considering that particular case but we

00:54:27,670 --> 00:54:31,920
can have a look at the source code and

00:54:28,630 --> 00:54:36,369
find out okay also you mentioned that

00:54:31,920 --> 00:54:38,170
your send options will configure how the

00:54:36,369 --> 00:54:41,350
replication is done in this session

00:54:38,170 --> 00:54:44,140
whether it's you know synchronous

00:54:41,350 --> 00:54:45,910
asynchronous we're waiting what - look

00:54:44,140 --> 00:54:48,280
what level of commit are you looking for

00:54:45,910 --> 00:54:50,470
from from other members in the cluster

00:54:48,280 --> 00:54:54,030
is there any way to configure that on

00:54:50,470 --> 00:54:55,510
like either a per request or even a perv

00:54:54,030 --> 00:54:58,540
nope

00:54:55,510 --> 00:54:59,560
short answer okay now it's it's done at

00:54:58,540 --> 00:55:02,050
the cluster level that's the only

00:54:59,560 --> 00:55:04,210
control you've got is there is there any

00:55:02,050 --> 00:55:05,470
reason to suspect that that couldn't be

00:55:04,210 --> 00:55:06,820
built in is there something at a

00:55:05,470 --> 00:55:10,420
fundamental level that would make that

00:55:06,820 --> 00:55:16,420
difficult to do without looking at the

00:55:10,420 --> 00:55:19,359
code and seeing how far down sort of

00:55:16,420 --> 00:55:21,130
that level of acknowledgement goes it is

00:55:19,359 --> 00:55:23,290
a synchronous and asynchronous then yeah

00:55:21,130 --> 00:55:25,600
there's a problem in that there's a

00:55:23,290 --> 00:55:27,070
fundamental difference between sending

00:55:25,600 --> 00:55:28,869
and waiting for Frances sticking it on a

00:55:27,070 --> 00:55:30,340
queue so that there's different

00:55:28,869 --> 00:55:31,570
components that get set up underneath

00:55:30,340 --> 00:55:36,720
that but there's no reason that they

00:55:31,570 --> 00:55:39,100
couldn't exist in parallel in terms of

00:55:36,720 --> 00:55:46,530
how that flag is set on a per message

00:55:39,100 --> 00:55:49,660
basis you would what I don't know is

00:55:46,530 --> 00:55:51,940
whether or not that flag is if that flag

00:55:49,660 --> 00:55:53,170
isn't transmitted with the message then

00:55:51,940 --> 00:55:59,140
the other end is always going to treat

00:55:53,170 --> 00:56:00,580
all messages the same way so yeah that's

00:55:59,140 --> 00:56:04,000
a I need to look at the source code

00:56:00,580 --> 00:56:05,980
again but in theory it shouldn't be any

00:56:04,000 --> 00:56:07,990
worse than just adding another parameter

00:56:05,980 --> 00:56:10,420
to what's potentially a very long chain

00:56:07,990 --> 00:56:12,040
chain of messages okay because I can

00:56:10,420 --> 00:56:13,720
imagine application where you're going

00:56:12,040 --> 00:56:15,970
through some workflow maybe a

00:56:13,720 --> 00:56:18,369
twelve-step workflow the first 11 steps

00:56:15,970 --> 00:56:19,490
you can be really sloppy about but that

00:56:18,369 --> 00:56:21,380
last step you need

00:56:19,490 --> 00:56:24,020
sure that it's committed and sent around

00:56:21,380 --> 00:56:26,480
to the cluster yeah I think I'd be

00:56:24,020 --> 00:56:27,950
leaning heavily towards just persisting

00:56:26,480 --> 00:56:30,800
that to a database or something else in

00:56:27,950 --> 00:56:34,100
a transaction rather than relying on the

00:56:30,800 --> 00:56:35,210
session replication but yeah Here I am

00:56:34,100 --> 00:56:36,890
talking about clustering trying to

00:56:35,210 --> 00:56:41,030
persuade as many as view as I can not to

00:56:36,890 --> 00:56:42,980
use it I think we're just about out of

00:56:41,030 --> 00:56:44,690
time so I'll wrap it up there if you've

00:56:42,980 --> 00:56:46,190
got any other questions I'm around for

00:56:44,690 --> 00:56:48,830
the rest of the day I'm around tomorrow

00:56:46,190 --> 00:56:51,940
and some of Thursday so do come and find

00:56:48,830 --> 00:56:51,940

YouTube URL: https://www.youtube.com/watch?v=rX1zm11AXcA


