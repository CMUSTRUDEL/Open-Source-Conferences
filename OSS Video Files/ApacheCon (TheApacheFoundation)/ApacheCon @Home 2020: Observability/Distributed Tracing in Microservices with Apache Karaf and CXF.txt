Title: Distributed Tracing in Microservices with Apache Karaf and CXF
Publication date: 2020-10-16
Playlist: ApacheCon @Home 2020: Observability
Description: 
	Distributed Tracing in Microservices with Apache Karaf and CXF
Andrei Shakirin

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/


Microservice Architectural Pattern suggests splitting of the business domain to several independent bounded contexts exposed as microservices. It brings a lot of benefits for teams working on microservices independently, but, from other side, complicates the problem analysis and monitoring. Sometimes it is very hard to detect which component causes slowdown and failure, to analyse what happens with request spans multiple services. The solutions for this challenge are distributed tracing and monitoring. Talk will introduce and explain basic tracing terminology: span, trace and context. Presenter will show common approaches to distributed tracing using Apache Karaf, CXF and Zipkin, SpringBoot and Sleuth frameworks. Talk contains some real project examples and demos.

The areas of his interest are REST API design, Microservices, Cloud, resilient distributed systems, security and agile development. Andrei is PMC and committer of Apache CXF and committer of Syncope projects. He is member of OASIS S-RAMP Work Group and speaker at Java and Apache conferences. Last speaking experience: • DOAG 2019, Nov 2019, Nurnberg, Design Production-Ready Software • Karlsruhe Entwickertag 2017, Mai 2017, Karlsruhe, Microservices with OSGi • ApacheCon Europe 2016, Nov 2016, Seville, Microservices with Apache Karaf and Apache CXF: Practical Experience • ApacheCon Europe 2015, Oct 2015, Budapest, Create and Secure Your REST API with Apache CXF • ApacheCon Europe 2014, Nov 2014, Budapest, Design REST Services With CXF JAX-RS Implementation: Lessons Learned • WJAX 2011, Nov 2011, Munich, Apache Days, Enabling Services with Apache CXF
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:26,160 --> 00:00:30,480
okay

00:00:26,640 --> 00:00:32,800
i'm agustat hi and welcome to my

00:00:30,480 --> 00:00:34,800
uh presentation so today we'll talk

00:00:32,800 --> 00:00:38,960
about 40 minutes or

00:00:34,800 --> 00:00:41,120
about tracing in microservices

00:00:38,960 --> 00:00:43,040
um and integration in the sex hyphen

00:00:41,120 --> 00:00:45,280
springboard

00:00:43,040 --> 00:00:46,480
my presentation will contains uh some

00:00:45,280 --> 00:00:49,440
parts um

00:00:46,480 --> 00:00:50,079
so the first and just define pricing

00:00:49,440 --> 00:00:52,800
goal

00:00:50,079 --> 00:00:53,199
let's see why tracing is important at

00:00:52,800 --> 00:00:55,600
all

00:00:53,199 --> 00:00:58,079
especially in the distributed system

00:00:55,600 --> 00:01:01,039
microservices landscape

00:00:58,079 --> 00:01:03,680
they make some definitions what is

00:01:01,039 --> 00:01:06,640
expense which is a crisis

00:01:03,680 --> 00:01:07,439
uh then we'll look a bit in the open

00:01:06,640 --> 00:01:10,320
zipkin

00:01:07,439 --> 00:01:12,320
uh tracing solution how it or what is

00:01:10,320 --> 00:01:14,720
the architecture of it

00:01:12,320 --> 00:01:17,040
and we'll uh show two kind of

00:01:14,720 --> 00:01:18,240
integration how 6f apache 6f is

00:01:17,040 --> 00:01:21,439
integrated

00:01:18,240 --> 00:01:24,080
to zipkin and another

00:01:21,439 --> 00:01:24,720
example will be a spring so how spring

00:01:24,080 --> 00:01:26,560
boot

00:01:24,720 --> 00:01:27,840
using slow is also integrated with

00:01:26,560 --> 00:01:30,560
something

00:01:27,840 --> 00:01:32,960
i also make it two short demos for every

00:01:30,560 --> 00:01:36,640
solution

00:01:32,960 --> 00:01:40,799
uh and on the last and the last part

00:01:36,640 --> 00:01:44,399
i will provide uh explain a bit

00:01:40,799 --> 00:01:46,720
real-life solution for tracing what we

00:01:44,399 --> 00:01:49,439
use for one of the cameras customer

00:01:46,720 --> 00:01:52,000
today and also explain how it works this

00:01:49,439 --> 00:01:52,000
is a plane

00:01:52,079 --> 00:01:56,640
you can ask questions every time so you

00:01:54,240 --> 00:01:58,640
can also interrupt me it's not a problem

00:01:56,640 --> 00:02:00,240
during the presentation or you can keep

00:01:58,640 --> 00:02:03,920
your question in the

00:02:00,240 --> 00:02:07,119
chat i will check it on the end

00:02:03,920 --> 00:02:07,119
or during the presentation

00:02:07,439 --> 00:02:12,480
so let's let's go um shortly about me um

00:02:10,879 --> 00:02:14,160
i'm working as a software architect and

00:02:12,480 --> 00:02:16,080
talent team so talent produced some

00:02:14,160 --> 00:02:18,239
products based of the open source

00:02:16,080 --> 00:02:20,000
uh projects also some apache projects

00:02:18,239 --> 00:02:22,160
therefore i also

00:02:20,000 --> 00:02:23,360
uh beam apache community team pmc in

00:02:22,160 --> 00:02:25,920
apache 6f

00:02:23,360 --> 00:02:27,680
is a kind of web service and test

00:02:25,920 --> 00:02:29,280
communication framework

00:02:27,680 --> 00:02:31,200
make also some contributions in the

00:02:29,280 --> 00:02:34,239
apache synchro arias

00:02:31,200 --> 00:02:34,239
and apache coverage

00:02:34,840 --> 00:02:39,120
um the first question why tracing is

00:02:37,440 --> 00:02:40,959
important at all so normally

00:02:39,120 --> 00:02:42,720
if you have a single application that

00:02:40,959 --> 00:02:45,120
run in the in the in the host

00:02:42,720 --> 00:02:46,480
or basically we have all information in

00:02:45,120 --> 00:02:48,959
log and

00:02:46,480 --> 00:02:51,519
the question why we need the tracing as

00:02:48,959 --> 00:02:53,680
the problem is in the

00:02:51,519 --> 00:02:55,120
modern applications normally you deploy

00:02:53,680 --> 00:02:58,480
not a single part

00:02:55,120 --> 00:03:01,760
it's uh contains uh some

00:02:58,480 --> 00:03:03,920
different applications the system that's

00:03:01,760 --> 00:03:04,800
deployed at different hosts especially

00:03:03,920 --> 00:03:06,560
if you use

00:03:04,800 --> 00:03:09,599
microservices for your architectural

00:03:06,560 --> 00:03:13,360
style you'll probably end up with a

00:03:09,599 --> 00:03:16,480
number of application deployed in the

00:03:13,360 --> 00:03:19,200
different instances especially here some

00:03:16,480 --> 00:03:20,560
kind of orchestration solution like uh

00:03:19,200 --> 00:03:22,480
kubernetes

00:03:20,560 --> 00:03:24,080
or deploy in the cloud so application

00:03:22,480 --> 00:03:26,400
will have

00:03:24,080 --> 00:03:28,319
some instances and some different kind

00:03:26,400 --> 00:03:31,280
of of uh application and

00:03:28,319 --> 00:03:32,159
by the call is a client make a call this

00:03:31,280 --> 00:03:35,040
request

00:03:32,159 --> 00:03:37,519
goes through there uh go through the

00:03:35,040 --> 00:03:37,519
different

00:03:38,159 --> 00:03:44,159
microservices and the problem here is

00:03:41,360 --> 00:03:46,000
it's sometimes very difficult to say if

00:03:44,159 --> 00:03:48,720
you have a problem like having error

00:03:46,000 --> 00:03:49,840
in the end it's quite difficult to say

00:03:48,720 --> 00:03:52,799
what exactly

00:03:49,840 --> 00:03:53,680
was the reason of this year so you don't

00:03:52,799 --> 00:03:55,680
see

00:03:53,680 --> 00:03:56,879
your of course every application writes

00:03:55,680 --> 00:04:00,319
the own log file

00:03:56,879 --> 00:04:02,480
on the system but it's quite uh

00:04:00,319 --> 00:04:05,200
difficult to understand what part of the

00:04:02,480 --> 00:04:07,920
system is uh

00:04:05,200 --> 00:04:09,120
has a failure therefore uh it is

00:04:07,920 --> 00:04:11,680
necessary to

00:04:09,120 --> 00:04:13,360
uh track the whole round trip of the

00:04:11,680 --> 00:04:14,799
request so from the beginning from the

00:04:13,360 --> 00:04:18,160
client

00:04:14,799 --> 00:04:21,040
and how it goes through the different uh

00:04:18,160 --> 00:04:22,800
part of your system and exactly see what

00:04:21,040 --> 00:04:26,160
happens so what what what is

00:04:22,800 --> 00:04:28,720
my trip of my request ours is the first

00:04:26,160 --> 00:04:31,759
issue also sometimes

00:04:28,720 --> 00:04:32,880
it's even more uh difficult to to to

00:04:31,759 --> 00:04:36,160
understand why

00:04:32,880 --> 00:04:39,520
my system slows down so why system has

00:04:36,160 --> 00:04:41,680
outage uh it's just don't

00:04:39,520 --> 00:04:43,120
receive some time out on the client side

00:04:41,680 --> 00:04:47,520
but have no idea

00:04:43,120 --> 00:04:50,560
which part of the system has a problem

00:04:47,520 --> 00:04:52,320
tracing also helps such situations with

00:04:50,560 --> 00:04:54,639
tracing you see

00:04:52,320 --> 00:04:56,720
how your request goes and which time

00:04:54,639 --> 00:04:59,520
spent in every

00:04:56,720 --> 00:05:01,120
microservice this is basically the goals

00:04:59,520 --> 00:05:03,680
of the trace

00:05:01,120 --> 00:05:06,000
so if you look at tracing requirements

00:05:03,680 --> 00:05:08,400
and forces

00:05:06,000 --> 00:05:10,320
the first is of course correlate

00:05:08,400 --> 00:05:12,320
distributed local entities you have this

00:05:10,320 --> 00:05:14,160
log entities you have some information

00:05:12,320 --> 00:05:15,360
but it's necessary to correlate it

00:05:14,160 --> 00:05:19,120
somehow

00:05:15,360 --> 00:05:22,840
across or numerous locks numerous host

00:05:19,120 --> 00:05:25,919
number of systems um

00:05:22,840 --> 00:05:27,600
also important to discover based on some

00:05:25,919 --> 00:05:29,039
client information you have some

00:05:27,600 --> 00:05:31,039
some information from the client for

00:05:29,039 --> 00:05:33,360
example session id or

00:05:31,039 --> 00:05:34,479
your information request based on this

00:05:33,360 --> 00:05:37,120
information would be

00:05:34,479 --> 00:05:38,800
nice if you find in lock the whole chain

00:05:37,120 --> 00:05:41,360
of the requests with some additional

00:05:38,800 --> 00:05:41,360
information

00:05:41,759 --> 00:05:46,320
also it makes a lot of sense to add some

00:05:43,840 --> 00:05:48,400
metrics like a time execution time which

00:05:46,320 --> 00:05:49,280
helps to analyze for example slow downs

00:05:48,400 --> 00:05:52,240
of the system

00:05:49,280 --> 00:05:53,680
or outages and also some exactly like

00:05:52,240 --> 00:05:55,199
some information like execution

00:05:53,680 --> 00:05:58,960
operation which operation is

00:05:55,199 --> 00:05:58,960
running for example is a ghetto post

00:05:59,759 --> 00:06:03,280
also very important last last but not

00:06:02,639 --> 00:06:06,479
least

00:06:03,280 --> 00:06:07,520
the solution should produce minimal

00:06:06,479 --> 00:06:10,639
overhead

00:06:07,520 --> 00:06:14,160
at the runtime so it's

00:06:10,639 --> 00:06:17,199
not nice if your system

00:06:14,160 --> 00:06:18,000
has a slowdown because of tracing should

00:06:17,199 --> 00:06:21,039
not happens

00:06:18,000 --> 00:06:23,840
and even worse if your vision has a

00:06:21,039 --> 00:06:25,919
failure only because of trace

00:06:23,840 --> 00:06:27,680
therefore how testing solution normally

00:06:25,919 --> 00:06:30,880
looks like

00:06:27,680 --> 00:06:32,400
a sign for each request and equilibrium

00:06:30,880 --> 00:06:34,639
device request should be

00:06:32,400 --> 00:06:35,759
identified somehow so the tracing

00:06:34,639 --> 00:06:38,639
terminology is

00:06:35,759 --> 00:06:40,400
the name is trace id and it keeps it's

00:06:38,639 --> 00:06:43,440
constant for the whole round trip so

00:06:40,400 --> 00:06:46,319
it's really identifying how you find

00:06:43,440 --> 00:06:49,440
your rounding of the message

00:06:46,319 --> 00:06:51,840
then this request should be propagated

00:06:49,440 --> 00:06:53,919
to all systems that involve it in the

00:06:51,840 --> 00:06:54,960
communications handling of the request

00:06:53,919 --> 00:06:56,479
so this

00:06:54,960 --> 00:06:59,120
doesn't matter as asynchronous

00:06:56,479 --> 00:06:59,919
communication or synchronous we're using

00:06:59,120 --> 00:07:01,280
http

00:06:59,919 --> 00:07:02,960
somehow you should propagate this

00:07:01,280 --> 00:07:07,360
request id and have it

00:07:02,960 --> 00:07:11,120
locked in the whole part of the system

00:07:07,360 --> 00:07:13,199
um then makes a lot of sense to include

00:07:11,120 --> 00:07:14,800
this information this request id in all

00:07:13,199 --> 00:07:17,120
of messages perhaps with also some

00:07:14,800 --> 00:07:20,720
additional information

00:07:17,120 --> 00:07:21,919
then um our yeah this

00:07:20,720 --> 00:07:23,840
also you look at some additional

00:07:21,919 --> 00:07:24,639
information like timing cooperation

00:07:23,840 --> 00:07:27,039
stuff

00:07:24,639 --> 00:07:27,840
they have some lock some exceptions also

00:07:27,039 --> 00:07:29,759
in your

00:07:27,840 --> 00:07:31,199
about your request about your processing

00:07:29,759 --> 00:07:33,280
of the requests

00:07:31,199 --> 00:07:35,759
and aggregate log files and provide

00:07:33,280 --> 00:07:37,440
elastic search solutions normal way to

00:07:35,759 --> 00:07:39,520
for distributed log so you're in single

00:07:37,440 --> 00:07:40,319
place you can find the whole round trip

00:07:39,520 --> 00:07:43,280
of the message

00:07:40,319 --> 00:07:44,800
and see how it looks how much time is

00:07:43,280 --> 00:07:46,639
spent with the different part of the

00:07:44,800 --> 00:07:51,360
system

00:07:46,639 --> 00:07:53,120
um also push trace information into the

00:07:51,360 --> 00:07:56,000
additionally of course you can push

00:07:53,120 --> 00:07:58,240
trace information into central server

00:07:56,000 --> 00:08:01,039
normally it happens asynchronously to

00:07:58,240 --> 00:08:03,199
keep uh

00:08:01,039 --> 00:08:06,319
to make influence on the system

00:08:03,199 --> 00:08:07,840
performance as less as possible

00:08:06,319 --> 00:08:09,840
so you can write it in the locker you

00:08:07,840 --> 00:08:11,759
can directly report it to the

00:08:09,840 --> 00:08:14,639
trace server like open zip king using

00:08:11,759 --> 00:08:14,639
some protocols

00:08:15,440 --> 00:08:23,199
uh terminology of the tracing

00:08:18,720 --> 00:08:25,440
so uh the main uh the most important

00:08:23,199 --> 00:08:27,039
term in the in the in the interesting is

00:08:25,440 --> 00:08:29,360
the trace itself

00:08:27,039 --> 00:08:31,039
identified by trace id and trace is

00:08:29,360 --> 00:08:31,680
basically the whole rounding of the

00:08:31,039 --> 00:08:35,599
message

00:08:31,680 --> 00:08:38,240
so in the picture you see a client

00:08:35,599 --> 00:08:38,880
that sends some http request to the web

00:08:38,240 --> 00:08:41,039
framework

00:08:38,880 --> 00:08:42,159
or on the load balancer and cell

00:08:41,039 --> 00:08:44,159
framework makes

00:08:42,159 --> 00:08:46,480
several actions they make authentication

00:08:44,159 --> 00:08:48,160
call after that they put some billing

00:08:46,480 --> 00:08:50,240
information and after that access the

00:08:48,160 --> 00:08:52,320
resource for the resource api so i have

00:08:50,240 --> 00:08:54,080
different actions distributed for the

00:08:52,320 --> 00:08:55,920
different host and the whole

00:08:54,080 --> 00:08:59,600
communicator the whole steps are trace

00:08:55,920 --> 00:08:59,600
and have unique your trace aid

00:08:59,680 --> 00:09:06,000
the next part of the

00:09:03,519 --> 00:09:08,480
important term is a span so span is a

00:09:06,000 --> 00:09:10,640
building block so basically unit of work

00:09:08,480 --> 00:09:12,480
will produce in the different system for

00:09:10,640 --> 00:09:14,959
example clients and

00:09:12,480 --> 00:09:16,640
requests using http client uh credit

00:09:14,959 --> 00:09:20,160
based client

00:09:16,640 --> 00:09:24,160
and its producer owns pen so this is

00:09:20,160 --> 00:09:26,160
also identified is this id after that

00:09:24,160 --> 00:09:27,760
your web framework communicates with

00:09:26,160 --> 00:09:28,320
authentication there's also additional

00:09:27,760 --> 00:09:31,360
span

00:09:28,320 --> 00:09:34,000
so on spam normally a spam is inside

00:09:31,360 --> 00:09:36,480
a application one application and even

00:09:34,000 --> 00:09:39,519
inside the single thread so as you

00:09:36,480 --> 00:09:41,360
leave the thread it's a additional span

00:09:39,519 --> 00:09:43,120
with the different types of stand would

00:09:41,360 --> 00:09:45,920
be route one

00:09:43,120 --> 00:09:47,519
when they start communication it could

00:09:45,920 --> 00:09:49,920
be parents and then that one span

00:09:47,519 --> 00:09:53,680
produces another span

00:09:49,920 --> 00:09:55,279
the child one and of course a child

00:09:53,680 --> 00:09:58,160
as soon as you should propagate

00:09:55,279 --> 00:09:59,600
information about spam is expand context

00:09:58,160 --> 00:10:01,040
this is a way to propagate this

00:09:59,600 --> 00:10:03,279
information to another system

00:10:01,040 --> 00:10:04,399
so it contains normally it contains at

00:10:03,279 --> 00:10:07,040
least race id

00:10:04,399 --> 00:10:08,079
and parent spending could contain also

00:10:07,040 --> 00:10:11,200
some additional

00:10:08,079 --> 00:10:11,200
attributes as well

00:10:11,600 --> 00:10:15,600
um this pen has a two kind of relations

00:10:14,560 --> 00:10:20,240
so if their

00:10:15,600 --> 00:10:23,040
parents pen a result of the child one

00:10:20,240 --> 00:10:24,079
uh he should wait for that uh the boss

00:10:23,040 --> 00:10:27,120
band has a

00:10:24,079 --> 00:10:28,000
child of relation another kind of is

00:10:27,120 --> 00:10:31,519
that

00:10:28,000 --> 00:10:31,920
spend just a make a fine forget event so

00:10:31,519 --> 00:10:33,680
or

00:10:31,920 --> 00:10:35,200
just send some events and don't interest

00:10:33,680 --> 00:10:38,240
in the response at all

00:10:35,200 --> 00:10:42,079
then the relation is follows from

00:10:38,240 --> 00:10:45,200
you sees on this picture spent a

00:10:42,079 --> 00:10:48,399
start span c but depends on them

00:10:45,200 --> 00:10:51,839
in sense of result so it's the root span

00:10:48,399 --> 00:10:52,720
then c is a chart of a and also produce

00:10:51,839 --> 00:10:56,000
two childs

00:10:52,720 --> 00:10:58,880
e and f but f uh make it just

00:10:56,000 --> 00:11:02,079
a try and forget event and then span g

00:10:58,880 --> 00:11:05,200
span g is the forward sponsor

00:11:02,079 --> 00:11:06,959
it's about relations and of course in

00:11:05,200 --> 00:11:08,640
context we have some attributes the most

00:11:06,959 --> 00:11:10,399
important is of course trace id because

00:11:08,640 --> 00:11:13,040
we should identify the whole

00:11:10,399 --> 00:11:14,160
round tip of the message every span has

00:11:13,040 --> 00:11:16,880
also identifier

00:11:14,160 --> 00:11:17,600
and we also send parent spam id that we

00:11:16,880 --> 00:11:21,600
can see

00:11:17,600 --> 00:11:21,600
the relationship between suspense

00:11:22,079 --> 00:11:29,120
uh also on this picture you can see it

00:11:25,519 --> 00:11:31,760
the attributes quite quite clear

00:11:29,120 --> 00:11:32,560
so we start communication from this

00:11:31,760 --> 00:11:35,279
client

00:11:32,560 --> 00:11:36,959
parent's plan is still null but there's

00:11:35,279 --> 00:11:39,839
already trace id

00:11:36,959 --> 00:11:40,640
in spain a when a produces plan b the

00:11:39,839 --> 00:11:43,360
trace id

00:11:40,640 --> 00:11:44,160
is constant for the whole communication

00:11:43,360 --> 00:11:46,480
and span b

00:11:44,160 --> 00:11:47,760
has a parent span a the same for

00:11:46,480 --> 00:11:51,279
spending c that they have

00:11:47,760 --> 00:11:52,720
as uh parent spend b and uh span els

00:11:51,279 --> 00:11:54,959
which has directly apparent

00:11:52,720 --> 00:11:57,040
spending this is the idea of

00:11:54,959 --> 00:11:59,360
identification of spam and correlation

00:11:57,040 --> 00:11:59,360
of this

00:12:00,560 --> 00:12:05,040
let's look in there zipkin open zipkin

00:12:02,959 --> 00:12:06,480
um what is the functionality and what

00:12:05,040 --> 00:12:08,240
kind of

00:12:06,480 --> 00:12:10,560
advantages especially provide this is

00:12:08,240 --> 00:12:14,959
open source solution that

00:12:10,560 --> 00:12:14,959
helps a distributed system to check

00:12:15,600 --> 00:12:18,639
tracing information the first part is of

00:12:18,240 --> 00:12:20,800
course

00:12:18,639 --> 00:12:22,639
uh your application should be

00:12:20,800 --> 00:12:25,519
instrumented because we need to

00:12:22,639 --> 00:12:27,680
at least insert some headers in your

00:12:25,519 --> 00:12:29,600
clients and servers

00:12:27,680 --> 00:12:31,519
and also additionally you can report

00:12:29,600 --> 00:12:33,760
should report asynchronously and be able

00:12:31,519 --> 00:12:36,880
to report asynchronously information to

00:12:33,760 --> 00:12:39,519
their central

00:12:36,880 --> 00:12:41,120
central server and the first part in

00:12:39,519 --> 00:12:41,760
normally instrumenting of the quote

00:12:41,120 --> 00:12:44,639
libraries

00:12:41,760 --> 00:12:45,600
is available to the numerous different

00:12:44,639 --> 00:12:49,440
languages

00:12:45,600 --> 00:12:49,440
for java is for example brave

00:12:50,560 --> 00:12:54,000
then second task of the elves also

00:12:53,120 --> 00:12:56,959
instrumentation

00:12:54,000 --> 00:12:59,279
uh reporting of spam asynchronously uh

00:12:56,959 --> 00:12:59,519
so there is a kind of reporter says use

00:12:59,279 --> 00:13:00,720
a

00:12:59,519 --> 00:13:02,639
transport it could be different

00:13:00,720 --> 00:13:04,720
transport and let's report

00:13:02,639 --> 00:13:07,279
this information to the central uh

00:13:04,720 --> 00:13:09,040
tracing server

00:13:07,279 --> 00:13:10,800
the next step is a collection of tracing

00:13:09,040 --> 00:13:12,800
data and correlation so there's

00:13:10,800 --> 00:13:14,480
a number of different requests you

00:13:12,800 --> 00:13:15,920
should somehow correlate it based on the

00:13:14,480 --> 00:13:20,160
trace id

00:13:15,920 --> 00:13:21,839
and of course stores addressing data

00:13:20,160 --> 00:13:24,800
that it makes sense of course that the

00:13:21,839 --> 00:13:25,519
customer could uh look up using ipi or

00:13:24,800 --> 00:13:28,959
web gui

00:13:25,519 --> 00:13:32,160
or this facing date and

00:13:28,959 --> 00:13:33,920
zipkin is used as a base for a lot of

00:13:32,160 --> 00:13:35,440
solutions for example apache 65 has

00:13:33,920 --> 00:13:38,560
integration with it in

00:13:35,440 --> 00:13:41,360
spring cloud

00:13:38,560 --> 00:13:43,680
slois is used in spring boot suggests in

00:13:41,360 --> 00:13:45,600
rest easier some integration

00:13:43,680 --> 00:13:48,079
so the architecture of zipkin is shown

00:13:45,600 --> 00:13:50,160
in this picture as i said there is a

00:13:48,079 --> 00:13:51,680
instrument estimated client instrumented

00:13:50,160 --> 00:13:53,440
server so this library

00:13:51,680 --> 00:13:55,920
running on the client side on the server

00:13:53,440 --> 00:13:58,079
side and be able to inject theaters

00:13:55,920 --> 00:14:02,959
focus the communications on the

00:13:58,079 --> 00:14:04,959
trace is propagated here and a reporter

00:14:02,959 --> 00:14:06,240
reported use a different transfer by

00:14:04,959 --> 00:14:10,000
default is http

00:14:06,240 --> 00:14:12,720
yes and could use also kafka and wtmq as

00:14:10,000 --> 00:14:15,279
a synchronous communication

00:14:12,720 --> 00:14:16,800
on the server side of zip can there is a

00:14:15,279 --> 00:14:19,360
collector that's basically

00:14:16,800 --> 00:14:20,240
make a correlation between the different

00:14:19,360 --> 00:14:22,399
single

00:14:20,240 --> 00:14:24,480
entities and stored in the database it

00:14:22,399 --> 00:14:27,279
could be by default is cassandra

00:14:24,480 --> 00:14:30,000
but it could be mysql stackdriver in the

00:14:27,279 --> 00:14:32,000
google cloud platform or elasticsearch

00:14:30,000 --> 00:14:33,839
and also open zip code server provides

00:14:32,000 --> 00:14:35,519
the ipress api to access this

00:14:33,839 --> 00:14:37,440
information programmatically

00:14:35,519 --> 00:14:39,199
or you can also do it using user

00:14:37,440 --> 00:14:42,880
interface

00:14:39,199 --> 00:14:45,279
this is the idea of the zigbee server

00:14:42,880 --> 00:14:46,560
we also look could look could see the

00:14:45,279 --> 00:14:49,519
communication on this

00:14:46,560 --> 00:14:50,160
example there is a user code that makes

00:14:49,519 --> 00:14:54,079
a

00:14:50,160 --> 00:14:57,120
get request it's intercepted by

00:14:54,079 --> 00:15:01,199
uh instrumentation and records the

00:14:57,120 --> 00:15:02,880
text adds headers and this

00:15:01,199 --> 00:15:04,240
request is instrumented at least with

00:15:02,880 --> 00:15:05,680
two attributes the stress id and

00:15:04,240 --> 00:15:09,279
responding

00:15:05,680 --> 00:15:12,079
and are sent to the http uh transport

00:15:09,279 --> 00:15:12,880
and on the server side after that you

00:15:12,079 --> 00:15:16,320
receive

00:15:12,880 --> 00:15:19,519
some response uh the record say duration

00:15:16,320 --> 00:15:20,160
information and uh the spawn will be

00:15:19,519 --> 00:15:21,920
reported

00:15:20,160 --> 00:15:23,839
uh the span will be reported

00:15:21,920 --> 00:15:25,600
asynchronously in gzipkin server

00:15:23,839 --> 00:15:27,680
inclusive duration

00:15:25,600 --> 00:15:31,040
uh name of operation and of course the

00:15:27,680 --> 00:15:31,040
attributes that i said instead

00:15:31,199 --> 00:15:36,720
yeah so zipkin works now

00:15:34,560 --> 00:15:38,160
yeah additionally there is also open

00:15:36,720 --> 00:15:40,399
pricing ipi

00:15:38,160 --> 00:15:42,639
so if you don't like to be dependent on

00:15:40,399 --> 00:15:45,680
single vendor for example

00:15:42,639 --> 00:15:47,600
or brave and would like to be more or

00:15:45,680 --> 00:15:48,240
less independent you can use this open

00:15:47,600 --> 00:15:51,120
bracing

00:15:48,240 --> 00:15:52,880
api that's implemented available in the

00:15:51,120 --> 00:15:55,040
number of languages

00:15:52,880 --> 00:15:57,759
and you see also where known objects

00:15:55,040 --> 00:16:00,079
like tracer span and span context

00:15:57,759 --> 00:16:02,000
and the brave also provided facade for

00:16:00,079 --> 00:16:02,560
open tracing apis you have a choice you

00:16:02,000 --> 00:16:05,360
either

00:16:02,560 --> 00:16:07,360
could use directly brave ipi or can use

00:16:05,360 --> 00:16:11,279
more generic open tracing

00:16:07,360 --> 00:16:11,680
standard apache xlr basically provides a

00:16:11,279 --> 00:16:14,959
boss

00:16:11,680 --> 00:16:17,519
solution so let's look uh

00:16:14,959 --> 00:16:17,519
into the

00:16:19,199 --> 00:16:23,839
integration of tracing into apache 6f

00:16:24,240 --> 00:16:30,880
there is a on the left side is a client

00:16:27,279 --> 00:16:35,120
using the txf jax errors implementation

00:16:30,880 --> 00:16:35,120
and on the right side is service

00:16:35,360 --> 00:16:41,759
what is necessary to do is

00:16:38,480 --> 00:16:44,240
you can inject trace tracer context to

00:16:41,759 --> 00:16:46,320
have access to actual trace information

00:16:44,240 --> 00:16:48,639
to the thread

00:16:46,320 --> 00:16:51,120
also necessary for the client to add a

00:16:48,639 --> 00:16:52,639
jax rs providers as a brave client

00:16:51,120 --> 00:16:54,320
provider or

00:16:52,639 --> 00:16:56,399
one of them or open tracing client

00:16:54,320 --> 00:17:00,000
provider is either you decide to

00:16:56,399 --> 00:17:02,560
use brave ipi directly or you use open

00:17:00,000 --> 00:17:05,360
tracing one

00:17:02,560 --> 00:17:05,919
after that your client is instrumented

00:17:05,360 --> 00:17:08,240
and

00:17:05,919 --> 00:17:09,839
um all calls on success of course will

00:17:08,240 --> 00:17:12,319
include additional headers with the

00:17:09,839 --> 00:17:16,480
trace id span id parent span id

00:17:12,319 --> 00:17:18,079
a sampled flag says either you

00:17:16,480 --> 00:17:20,160
the information should be locked should

00:17:18,079 --> 00:17:21,280
be sent in the zip code server directly

00:17:20,160 --> 00:17:22,559
or not

00:17:21,280 --> 00:17:25,199
and some additional flags could be

00:17:22,559 --> 00:17:27,120
propagated as well on the server side

00:17:25,199 --> 00:17:28,400
is basically very similar you can also

00:17:27,120 --> 00:17:31,440
inject the tracing context

00:17:28,400 --> 00:17:32,160
and you need to provide to add a server

00:17:31,440 --> 00:17:34,240
side

00:17:32,160 --> 00:17:36,480
jax address providers a brave provider

00:17:34,240 --> 00:17:38,880
open tracing pro

00:17:36,480 --> 00:17:39,600
so if it looks in the source code for

00:17:38,880 --> 00:17:42,880
the client

00:17:39,600 --> 00:17:44,880
uh you need to write a bit code here

00:17:42,880 --> 00:17:47,039
so create a transport sender for

00:17:44,880 --> 00:17:50,400
transport is http transfer but

00:17:47,039 --> 00:17:51,760
also active mq and kafka transport

00:17:50,400 --> 00:17:53,520
available

00:17:51,760 --> 00:17:54,960
there create a kind of tracing object

00:17:53,520 --> 00:17:58,880
with some configuration

00:17:54,960 --> 00:18:01,440
at the name of the service

00:17:58,880 --> 00:18:03,520
configure a transport standard transport

00:18:01,440 --> 00:18:04,000
say sampler in this case is always so

00:18:03,520 --> 00:18:06,160
all

00:18:04,000 --> 00:18:07,280
lock entries will be reported to the uh

00:18:06,160 --> 00:18:09,520
zipkin

00:18:07,280 --> 00:18:11,039
and after they create a brave provide

00:18:09,520 --> 00:18:13,679
client provider which i show

00:18:11,039 --> 00:18:14,400
and register with this kind of

00:18:13,679 --> 00:18:16,799
information

00:18:14,400 --> 00:18:18,559
after that you use client just as it is

00:18:16,799 --> 00:18:20,080
with like the docs are standard send

00:18:18,559 --> 00:18:22,559
your request

00:18:20,080 --> 00:18:23,840
receives the responses and the whole

00:18:22,559 --> 00:18:26,880
information will be injected and

00:18:23,840 --> 00:18:26,880
reported automatically

00:18:27,039 --> 00:18:30,799
on the server side is very similar

00:18:28,960 --> 00:18:31,200
configure transport as well create a

00:18:30,799 --> 00:18:35,120
test

00:18:31,200 --> 00:18:37,280
object and use a brave feature to

00:18:35,120 --> 00:18:40,720
add this information into the

00:18:37,280 --> 00:18:40,720
application server application

00:18:41,280 --> 00:18:47,760
uh so let's let's let's see it's now um

00:18:44,400 --> 00:18:50,960
on the demo i just

00:18:47,760 --> 00:18:54,160
should share my another screen

00:18:50,960 --> 00:18:57,200
give me minutes i will try to share the

00:18:54,160 --> 00:18:59,840
whole screen

00:18:57,200 --> 00:18:59,840
to do it

00:19:00,880 --> 00:19:11,840
so here is uh the scenario

00:19:04,400 --> 00:19:11,840
yeah showcase scenario as well

00:19:29,120 --> 00:19:32,160
yeah so scenario is quite simple in this

00:19:31,200 --> 00:19:35,760
case we have

00:19:32,160 --> 00:19:35,760
um sorry so

00:19:36,840 --> 00:19:40,799
yeah scenario is quite quite quite

00:19:39,600 --> 00:19:44,640
simple

00:19:40,799 --> 00:19:53,840
um we have from one side

00:19:44,640 --> 00:19:56,240
just show it in there

00:19:53,840 --> 00:19:57,840
yeah this one so we have a client and

00:19:56,240 --> 00:20:01,120
the server

00:19:57,840 --> 00:20:03,679
duck service client accelerator uh

00:20:01,120 --> 00:20:06,159
what the client didn't just send two

00:20:03,679 --> 00:20:09,200
requests to create a catalogue

00:20:06,159 --> 00:20:10,240
and to get the catalog back and we will

00:20:09,200 --> 00:20:11,679
say it's configured

00:20:10,240 --> 00:20:15,360
it's configured the brave client

00:20:11,679 --> 00:20:18,880
provider with http tracing

00:20:15,360 --> 00:20:20,960
from the server side makes provides the

00:20:18,880 --> 00:20:25,679
two operations

00:20:20,960 --> 00:20:25,679
uh get books and uh ad book

00:20:26,400 --> 00:20:31,760
makes is asynchronously yeah so uh there

00:20:29,679 --> 00:20:33,360
is an executor that said metric fest and

00:20:31,760 --> 00:20:35,200
creates basically another set therefore

00:20:33,360 --> 00:20:37,919
we will see not only single

00:20:35,200 --> 00:20:38,400
spend here we have we will see multiples

00:20:37,919 --> 00:20:41,840
of

00:20:38,400 --> 00:20:42,400
multiple spans uh and basically they

00:20:41,840 --> 00:20:45,919
create

00:20:42,400 --> 00:20:49,840
a catalog book and then provides a

00:20:45,919 --> 00:20:49,840
result of the search

00:20:54,320 --> 00:20:58,000
let's let's make it run

00:20:58,159 --> 00:21:03,840
so i start my server

00:21:04,799 --> 00:21:07,039
and

00:21:08,000 --> 00:21:10,880
start my client

00:21:12,799 --> 00:21:19,520
so just a client will report um

00:21:17,520 --> 00:21:21,120
create a catalog and report that it's

00:21:19,520 --> 00:21:23,280
read it on the test book

00:21:21,120 --> 00:21:26,320
but now interested in what we will see

00:21:23,280 --> 00:21:26,320
in the zip code server

00:21:27,039 --> 00:21:33,120
so to find it

00:21:30,400 --> 00:21:33,120
yeah here's it

00:21:34,400 --> 00:21:37,679
so it's a true communication so the

00:21:36,559 --> 00:21:40,960
first one is

00:21:37,679 --> 00:21:42,480
a post with uh which creates a create a

00:21:40,960 --> 00:21:44,960
book

00:21:42,480 --> 00:21:46,159
so i i as i say there are two spans on

00:21:44,960 --> 00:21:49,120
the server side one

00:21:46,159 --> 00:21:49,919
is calling the post operation uh with a

00:21:49,120 --> 00:21:53,120
span id

00:21:49,919 --> 00:21:53,679
and the second one is uh in setting new

00:21:53,120 --> 00:21:55,679
book

00:21:53,679 --> 00:21:57,679
you see also which how much time it

00:21:55,679 --> 00:21:58,000
takes so post takes a bit more time

00:21:57,679 --> 00:22:00,000
because

00:21:58,000 --> 00:22:01,600
it's uh utilization of cesaro and the

00:22:00,000 --> 00:22:05,600
first trust first request

00:22:01,600 --> 00:22:09,120
and inside book is quite quite quick

00:22:05,600 --> 00:22:11,360
uh and the second is a get operation

00:22:09,120 --> 00:22:12,400
it's also two spans just call the get

00:22:11,360 --> 00:22:14,880
methods and

00:22:12,400 --> 00:22:16,640
looking of the local looking of the book

00:22:14,880 --> 00:22:20,480
as in separate thread

00:22:16,640 --> 00:22:23,520
so we have a client span

00:22:20,480 --> 00:22:25,280
and we have a two service plans here

00:22:23,520 --> 00:22:27,120
of course they have a traceability ids

00:22:25,280 --> 00:22:30,480
constant for the whole communication

00:22:27,120 --> 00:22:32,799
and we will able in this case to see

00:22:30,480 --> 00:22:34,000
which part of the system are involved

00:22:32,799 --> 00:22:38,320
what is the spans

00:22:34,000 --> 00:22:38,320
and how long the operation takes

00:22:39,440 --> 00:22:46,159
let's look into the

00:22:42,720 --> 00:22:46,159
split input integration

00:22:51,520 --> 00:22:55,840
spring boot to integrate the open zip in

00:22:54,080 --> 00:22:57,039
the spring boot is even more easy as a

00:22:55,840 --> 00:22:59,200
budget six seven you

00:22:57,039 --> 00:23:00,240
shouldn't basically write code at all

00:22:59,200 --> 00:23:03,039
because it has

00:23:00,240 --> 00:23:03,919
nice instrumentation uh solutions voice

00:23:03,039 --> 00:23:07,200
is also

00:23:03,919 --> 00:23:08,080
based on the brave uh library basically

00:23:07,200 --> 00:23:09,919
you can also

00:23:08,080 --> 00:23:11,360
insert the tracer using auto right

00:23:09,919 --> 00:23:14,320
annotation to

00:23:11,360 --> 00:23:15,200
get the current trace information and

00:23:14,320 --> 00:23:17,760
after that

00:23:15,200 --> 00:23:18,640
uh basically you need only to configure

00:23:17,760 --> 00:23:22,080
two starters

00:23:18,640 --> 00:23:25,280
um open uh sluice starter

00:23:22,080 --> 00:23:28,559
and zipkin starter in your uh build

00:23:25,280 --> 00:23:29,919
file as dependency and then your

00:23:28,559 --> 00:23:33,280
application will be automatically

00:23:29,919 --> 00:23:33,280
injected only important that

00:23:33,760 --> 00:23:37,919
you need to inject a client your http

00:23:36,880 --> 00:23:40,320
clients

00:23:37,919 --> 00:23:42,080
the rest templates not created within

00:23:40,320 --> 00:23:45,679
you with the new of course

00:23:42,080 --> 00:23:49,360
that the instrumentation will not work

00:23:45,679 --> 00:23:51,919
but either you use springboot

00:23:49,360 --> 00:23:53,360
means injection then it works out of the

00:23:51,919 --> 00:23:55,440
box and you will see

00:23:53,360 --> 00:23:56,799
the communication headers additional

00:23:55,440 --> 00:23:58,720
communication headers

00:23:56,799 --> 00:24:01,200
and the staff will be reported to the

00:23:58,720 --> 00:24:01,200
concern

00:24:02,840 --> 00:24:07,200
um yeah exactly so

00:24:05,360 --> 00:24:08,640
what you need to configure is a two

00:24:07,200 --> 00:24:11,679
starters in your

00:24:08,640 --> 00:24:15,600
form uh it's a slow starter and uh

00:24:11,679 --> 00:24:17,600
zip can start that's all that's

00:24:15,600 --> 00:24:19,760
basically you need to proactively do

00:24:17,600 --> 00:24:21,919
if you have a standard ports and

00:24:19,760 --> 00:24:24,960
standard configuration and inject of

00:24:21,919 --> 00:24:24,960
course inject your clients

00:24:26,559 --> 00:24:33,039
this is a server side

00:24:29,840 --> 00:24:35,200
so you inject client

00:24:33,039 --> 00:24:37,120
you can also not necessarily but you can

00:24:35,200 --> 00:24:39,440
inject the tracer and get some

00:24:37,120 --> 00:24:40,960
information about spawn with a parent id

00:24:39,440 --> 00:24:42,720
trace against vanity or if you can

00:24:40,960 --> 00:24:45,919
properly you like to propagate

00:24:42,720 --> 00:24:47,679
in some system that did not

00:24:45,919 --> 00:24:49,360
automatically instrumented you can also

00:24:47,679 --> 00:24:53,200
use this information as well or you can

00:24:49,360 --> 00:24:53,200
report it into the log file for example

00:24:54,320 --> 00:24:57,600
um the scenario is a bit more

00:24:56,080 --> 00:25:00,880
complicated for

00:24:57,600 --> 00:25:04,080
a spring boot

00:25:00,880 --> 00:25:04,799
it is there we have a two services here

00:25:04,080 --> 00:25:06,559
so two

00:25:04,799 --> 00:25:08,000
uh microservices let's say one is

00:25:06,559 --> 00:25:12,240
composite uh

00:25:08,000 --> 00:25:13,360
have injected rest client and

00:25:12,240 --> 00:25:16,559
[Music]

00:25:13,360 --> 00:25:18,880
they have a basic service that have also

00:25:16,559 --> 00:25:19,840
synchronous communication and the second

00:25:18,880 --> 00:25:21,360
method they have

00:25:19,840 --> 00:25:23,279
additionally it's published stuff in

00:25:21,360 --> 00:25:25,279
just kafka topic at least

00:25:23,279 --> 00:25:28,880
so have synchronous blues some

00:25:25,279 --> 00:25:31,679
asynchronous communication

00:25:28,880 --> 00:25:33,440
and the all informations for the whole

00:25:31,679 --> 00:25:35,279
round chips will be reported to zipkin

00:25:33,440 --> 00:25:38,960
server in this case

00:25:35,279 --> 00:25:44,559
so let's let's look how it works

00:25:38,960 --> 00:25:44,559
so i need to switch to my video now

00:25:46,720 --> 00:25:52,480
so let's look a bit in the court how

00:25:49,520 --> 00:25:52,480
it's implemented

00:25:53,840 --> 00:25:57,679
so from the composite service we have a

00:25:56,080 --> 00:26:01,440
simple composite controller

00:25:57,679 --> 00:26:03,600
with injected uh basic client

00:26:01,440 --> 00:26:05,440
and let us be able to get this

00:26:03,600 --> 00:26:08,159
information here and

00:26:05,440 --> 00:26:09,279
by hello just invoke the basic client

00:26:08,159 --> 00:26:12,799
interval basic

00:26:09,279 --> 00:26:14,240
and by sentence and topic operations so

00:26:12,799 --> 00:26:16,880
two kind of operations

00:26:14,240 --> 00:26:16,880
was a get

00:26:17,600 --> 00:26:22,720
basic client is a

00:26:20,640 --> 00:26:24,640
resting plate has a resting plate

00:26:22,720 --> 00:26:26,480
builder and just invokes information

00:26:24,640 --> 00:26:28,080
interesting that's also protected using

00:26:26,480 --> 00:26:32,320
histix command

00:26:28,080 --> 00:26:36,240
so historic is a kind of resilient

00:26:32,320 --> 00:26:38,320
library to protect your endpoints

00:26:36,240 --> 00:26:39,279
or with circuit breaker so if you

00:26:38,320 --> 00:26:41,919
endpoint not

00:26:39,279 --> 00:26:43,520
response you can either reject it

00:26:41,919 --> 00:26:46,640
immediately with the error or

00:26:43,520 --> 00:26:49,520
so after some uh threshold

00:26:46,640 --> 00:26:50,400
requests if requests increase uh failed

00:26:49,520 --> 00:26:53,120
to question

00:26:50,400 --> 00:26:53,679
increase some threshold you can either

00:26:53,120 --> 00:26:55,679
report

00:26:53,679 --> 00:26:58,240
immediately the year or may use some

00:26:55,679 --> 00:26:59,679
fallback but also create your own thread

00:26:58,240 --> 00:27:02,000
pull for the request to

00:26:59,679 --> 00:27:05,120
don't to pollute the whole system and

00:27:02,000 --> 00:27:05,120
block the whole system

00:27:05,600 --> 00:27:11,200
on the server side and yeah as i said

00:27:08,960 --> 00:27:12,640
in the poem you just need these two

00:27:11,200 --> 00:27:15,919
additional dependencies

00:27:12,640 --> 00:27:20,080
you need a slice and a zipcam to

00:27:15,919 --> 00:27:21,279
make the since done so it's really all

00:27:20,080 --> 00:27:24,559
things are integrated

00:27:21,279 --> 00:27:26,240
out of the box here and the same on the

00:27:24,559 --> 00:27:27,840
basic client you need two additional

00:27:26,240 --> 00:27:31,440
dependencies

00:27:27,840 --> 00:27:31,440
uh slowest and zipkin

00:27:31,679 --> 00:27:35,919
here in the controller there's two

00:27:33,520 --> 00:27:38,720
operation the boss just responds with

00:27:35,919 --> 00:27:40,559
a thing and second make additionally

00:27:38,720 --> 00:27:41,360
communication with the kafka so send

00:27:40,559 --> 00:27:44,240
something into

00:27:41,360 --> 00:27:46,840
kafka topic and there is a listing uh so

00:27:44,240 --> 00:27:48,240
receiver who

00:27:46,840 --> 00:27:50,880
uh

00:27:48,240 --> 00:27:52,720
listen this topic and just publish the

00:27:50,880 --> 00:27:57,919
information

00:27:52,720 --> 00:27:57,919
so let's let's make it run i will start

00:27:59,840 --> 00:28:03,840
composite servers first

00:28:17,039 --> 00:28:19,279
and

00:28:21,520 --> 00:28:25,760
basic service so basically we'll see

00:28:23,919 --> 00:28:28,720
some

00:28:25,760 --> 00:28:30,559
communication with kafka this should

00:28:28,720 --> 00:28:33,919
connect to my kafka server

00:28:30,559 --> 00:28:33,919
the kafka is already started

00:28:42,720 --> 00:28:49,679
it's connected perfect

00:28:46,720 --> 00:28:52,399
so and with the terminal i will invoke

00:28:49,679 --> 00:28:55,520
first hello iteration it's quite similar

00:28:52,399 --> 00:28:57,039
uh simple i just will receive the euro

00:28:55,520 --> 00:28:58,320
from basic service synchronous

00:28:57,039 --> 00:29:02,480
communication

00:28:58,320 --> 00:29:02,480
let's look what happens on the zip code

00:29:10,840 --> 00:29:13,840
site

00:29:16,399 --> 00:29:20,159
i have no communication i don't

00:29:19,520 --> 00:29:23,600
understand

00:29:20,159 --> 00:29:27,120
why ah no it takes sometimes because

00:29:23,600 --> 00:29:30,159
asynchronous communication with

00:29:27,120 --> 00:29:33,440
zipkin service and it applies

00:29:30,159 --> 00:29:35,200
this small delay but therefore

00:29:33,440 --> 00:29:37,520
our influence on the application

00:29:35,200 --> 00:29:40,559
performance is less

00:29:37,520 --> 00:29:42,159
so uh let's see we have one span in the

00:29:40,559 --> 00:29:46,240
basic service here

00:29:42,159 --> 00:29:46,240
and three spans on the composite servers

00:29:48,240 --> 00:29:51,520
so composite service has a get operation

00:29:50,799 --> 00:29:53,440
called

00:29:51,520 --> 00:29:55,840
and after that it's interesting you see

00:29:53,440 --> 00:29:56,480
history so he's takes create a new

00:29:55,840 --> 00:29:59,440
thread

00:29:56,480 --> 00:30:00,720
therefore this is it is a new span here

00:29:59,440 --> 00:30:03,279
it's a parent's pen id

00:30:00,720 --> 00:30:04,559
of get as therefore if he removes the

00:30:03,279 --> 00:30:06,320
hystics annotation and just make

00:30:04,559 --> 00:30:09,440
synchronous communication will

00:30:06,320 --> 00:30:11,120
see only only one span and after that

00:30:09,440 --> 00:30:14,559
the call goes to the

00:30:11,120 --> 00:30:15,360
basic service and invokes hello and to

00:30:14,559 --> 00:30:19,840
just return

00:30:15,360 --> 00:30:19,840
our response to back

00:30:20,080 --> 00:30:24,720
this is a first kind of communication is

00:30:22,720 --> 00:30:27,919
quite simple it's only hystics as

00:30:24,720 --> 00:30:30,640
is interesting here um

00:30:27,919 --> 00:30:31,679
let's try to do a second kind of

00:30:30,640 --> 00:30:34,799
communication

00:30:31,679 --> 00:30:37,039
with ascend so basically result is

00:30:34,799 --> 00:30:38,799
the same we have a string from the basic

00:30:37,039 --> 00:30:39,360
service but additionally in this case

00:30:38,799 --> 00:30:42,000
basic

00:30:39,360 --> 00:30:43,520
areas basic servers make a communication

00:30:42,000 --> 00:30:45,440
uh produce a communication with the

00:30:43,520 --> 00:30:49,200
kafka

00:30:45,440 --> 00:30:51,600
and we'll see it so let's

00:30:49,200 --> 00:30:52,799
sort our communications here we see the

00:30:51,600 --> 00:30:56,559
additional spans

00:30:52,799 --> 00:30:58,240
with the kafka and now uh it's a bit

00:30:56,559 --> 00:30:59,360
more interesting as a composite

00:30:58,240 --> 00:31:02,799
composite service

00:30:59,360 --> 00:31:04,399
stays the same as a make a get a

00:31:02,799 --> 00:31:06,320
correlate operation involves the

00:31:04,399 --> 00:31:08,080
historics but your basic service now

00:31:06,320 --> 00:31:10,880
happens a bit more

00:31:08,080 --> 00:31:12,559
makes a call a get operation and then

00:31:10,880 --> 00:31:15,600
additionally it sends

00:31:12,559 --> 00:31:16,559
asynchronously their information to

00:31:15,600 --> 00:31:19,600
kafka

00:31:16,559 --> 00:31:20,240
then you see a pool threads that's a the

00:31:19,600 --> 00:31:22,480
pulse

00:31:20,240 --> 00:31:23,519
kafka topic and after that the whole

00:31:22,480 --> 00:31:25,519
message message

00:31:23,519 --> 00:31:27,200
will be called so you see the whole

00:31:25,519 --> 00:31:32,720
communication inclusive

00:31:27,200 --> 00:31:36,159
asynchronous one on the server side

00:31:32,720 --> 00:31:39,919
so that's about

00:31:36,159 --> 00:31:42,559
demos let's return back to the

00:31:39,919 --> 00:31:42,559
presentation

00:31:45,200 --> 00:31:51,600
but i would like to additionally um

00:31:48,799 --> 00:31:51,919
show the solution that implemented for

00:31:51,600 --> 00:31:55,039
one

00:31:51,919 --> 00:31:57,679
ecommerce uh provides this real um

00:31:55,039 --> 00:32:00,960
ecommerce scenario architecture how how

00:31:57,679 --> 00:32:00,960
the tracing is organized

00:32:01,600 --> 00:32:07,679
so architecture is a standard one with

00:32:05,360 --> 00:32:10,399
microservices so it's a

00:32:07,679 --> 00:32:10,960
front-end java javascript based

00:32:10,399 --> 00:32:14,880
front-end

00:32:10,960 --> 00:32:17,360
called from the web browser

00:32:14,880 --> 00:32:18,399
um say front-end communicates with

00:32:17,360 --> 00:32:21,679
back-end using

00:32:18,399 --> 00:32:23,120
public api rest public api um it's not

00:32:21,679 --> 00:32:24,480
only front-ends communicators

00:32:23,120 --> 00:32:26,799
external consumers and mobile

00:32:24,480 --> 00:32:27,440
applications and here there is a

00:32:26,799 --> 00:32:30,720
component

00:32:27,440 --> 00:32:32,240
of microservices implementing facade

00:32:30,720 --> 00:32:34,960
it's necessary because there are two

00:32:32,240 --> 00:32:38,559
kind of business here is a core

00:32:34,960 --> 00:32:42,080
services that's responsible to uh

00:32:38,559 --> 00:32:45,600
provide products from

00:32:42,080 --> 00:32:47,840
ecommerce provider itself and sell it

00:32:45,600 --> 00:32:48,720
and there's a market marketplace

00:32:47,840 --> 00:32:51,679
services that's

00:32:48,720 --> 00:32:52,000
responsible to a deal with the products

00:32:51,679 --> 00:32:54,559
from

00:32:52,000 --> 00:32:55,360
other sellers or so just use a platform

00:32:54,559 --> 00:32:58,000
to sell all

00:32:55,360 --> 00:32:59,679
products for the custom for the end

00:32:58,000 --> 00:33:03,039
customer basically it's transparent

00:32:59,679 --> 00:33:05,440
uh it's it doesn't match uh it's a

00:33:03,039 --> 00:33:06,399
let's add so he said in the shopping

00:33:05,440 --> 00:33:09,120
cart uh

00:33:06,399 --> 00:33:10,720
some products from the ecms provider or

00:33:09,120 --> 00:33:14,960
marketplace just

00:33:10,720 --> 00:33:14,960
see together and face together

00:33:16,399 --> 00:33:19,840
the corpus and sales marketplace

00:33:18,159 --> 00:33:25,440
additionally communicates with

00:33:19,840 --> 00:33:28,480
sap er for

00:33:25,440 --> 00:33:30,159
information uh also has a

00:33:28,480 --> 00:33:32,080
document-based mongodb database

00:33:30,159 --> 00:33:34,960
sql-based postgres

00:33:32,080 --> 00:33:36,640
and using activemq as a messaging search

00:33:34,960 --> 00:33:38,559
additional businesses communicators

00:33:36,640 --> 00:33:39,519
online finance system and credit forces

00:33:38,559 --> 00:33:40,720
check system so you see the

00:33:39,519 --> 00:33:42,880
communication is for

00:33:40,720 --> 00:33:45,279
quite complicated and tracing solution

00:33:42,880 --> 00:33:47,919
is absolutely must here

00:33:45,279 --> 00:33:49,120
first we look uh how tracing solution is

00:33:47,919 --> 00:33:51,679
implemented

00:33:49,120 --> 00:33:52,159
there are two kind of identifiers the

00:33:51,679 --> 00:33:55,120
first

00:33:52,159 --> 00:33:56,159
web browser has a kind of session id it

00:33:55,120 --> 00:33:58,000
is sent with

00:33:56,159 --> 00:33:59,440
all requests and we propagate through

00:33:58,000 --> 00:34:02,399
the whole system

00:33:59,440 --> 00:34:02,720
session id is stored about some days in

00:34:02,399 --> 00:34:05,360
the

00:34:02,720 --> 00:34:06,320
browser and we can based on the session

00:34:05,360 --> 00:34:09,119
idea can

00:34:06,320 --> 00:34:10,800
reproduce the whole uh communications of

00:34:09,119 --> 00:34:11,440
each kind of communication including

00:34:10,800 --> 00:34:14,560
front-end

00:34:11,440 --> 00:34:16,720
a customer made additionally for say to

00:34:14,560 --> 00:34:17,839
create a marketplace ids kind of trace

00:34:16,720 --> 00:34:19,440
ideas and propagate

00:34:17,839 --> 00:34:21,520
or now core business services

00:34:19,440 --> 00:34:24,480
marketplace and backend systems

00:34:21,520 --> 00:34:25,520
and you can restore basically a base on

00:34:24,480 --> 00:34:28,639
the marketplace

00:34:25,520 --> 00:34:33,040
um all communication uh for

00:34:28,639 --> 00:34:33,040
all chain call chain from the front

00:34:33,599 --> 00:34:39,679
uh if you look on the deployment

00:34:36,639 --> 00:34:41,440
uh the whole uh microservices are hosted

00:34:39,679 --> 00:34:44,240
in the google cloud platform

00:34:41,440 --> 00:34:45,599
um and for say the marketplace service

00:34:44,240 --> 00:34:47,119
implemented using springboot

00:34:45,599 --> 00:34:49,119
uh in core business services using

00:34:47,119 --> 00:34:49,679
carafe and that's except so deploy it in

00:34:49,119 --> 00:34:51,919
karaf

00:34:49,679 --> 00:34:53,599
it uses except for communication but

00:34:51,919 --> 00:34:55,520
nice in the tracing solution you can use

00:34:53,599 --> 00:34:56,960
as a different technologies

00:34:55,520 --> 00:34:59,520
it doesn't matter which technology you

00:34:56,960 --> 00:35:01,280
use as far as propagated using http

00:34:59,520 --> 00:35:04,400
headers

00:35:01,280 --> 00:35:07,119
the cxf implementation

00:35:04,400 --> 00:35:08,640
or works perfectly with the springboot

00:35:07,119 --> 00:35:10,160
implementation this absolutely doesn't

00:35:08,640 --> 00:35:13,119
matter

00:35:10,160 --> 00:35:15,040
and we just write or write this

00:35:13,119 --> 00:35:16,240
additional basic information in the blog

00:35:15,040 --> 00:35:19,599
files using

00:35:16,240 --> 00:35:20,000
flowing bit to pass the staff and send

00:35:19,599 --> 00:35:22,960
to

00:35:20,000 --> 00:35:23,440
central greylock elasticsearch solution

00:35:22,960 --> 00:35:27,040
and

00:35:23,440 --> 00:35:30,640
this really nice whatever they benefit

00:35:27,040 --> 00:35:32,000
so if every euros are reported or system

00:35:30,640 --> 00:35:34,160
has a slowdown

00:35:32,000 --> 00:35:36,960
i ask just a customer to provide a

00:35:34,160 --> 00:35:39,839
session id and based on the session id i

00:35:36,960 --> 00:35:40,880
see or communication between the browser

00:35:39,839 --> 00:35:44,720
and from 10 system

00:35:40,880 --> 00:35:47,760
and if i get marketplace id i

00:35:44,720 --> 00:35:48,320
can uh see the whole traces inside the

00:35:47,760 --> 00:35:51,760
single

00:35:48,320 --> 00:35:54,800
from a facade in location so just

00:35:51,760 --> 00:35:56,480
demonstrate how it looks like in real

00:35:54,800 --> 00:36:00,400
life

00:35:56,480 --> 00:36:00,400
to start additional browser for that

00:36:05,760 --> 00:36:26,160
it's a pre-production system

00:36:13,119 --> 00:36:28,960
that takes today some time to loading

00:36:26,160 --> 00:36:28,960
just look for

00:36:29,680 --> 00:36:41,839
assad requests

00:36:50,560 --> 00:36:56,960
yeah i have some of them uh here's a

00:36:54,160 --> 00:36:57,839
in the header there's a con session id

00:36:56,960 --> 00:37:00,079
identifier

00:36:57,839 --> 00:37:01,599
it's a from browser and additionally is

00:37:00,079 --> 00:37:04,800
a marketplace

00:37:01,599 --> 00:37:08,480
id that's propagated as a place id

00:37:04,800 --> 00:37:11,760
and if i enter this information

00:37:08,480 --> 00:37:13,280
the grey lock i see the whole

00:37:11,760 --> 00:37:16,480
communication

00:37:13,280 --> 00:37:16,480
a little bit bigger

00:37:18,000 --> 00:37:23,680
so this is a inbound message my request

00:37:20,079 --> 00:37:26,800
to the facade

00:37:23,680 --> 00:37:28,640
after that i have a communication with

00:37:26,800 --> 00:37:29,359
core business services is already take

00:37:28,640 --> 00:37:33,920
sev

00:37:29,359 --> 00:37:37,280
vlogging oh there's a request to come

00:37:33,920 --> 00:37:40,800
i see some processing

00:37:37,280 --> 00:37:43,760
in the core logic and then uh

00:37:40,800 --> 00:37:45,760
result so it takes certain milliseconds

00:37:43,760 --> 00:37:50,079
and produce a result

00:37:45,760 --> 00:37:51,839
after that it asks for

00:37:50,079 --> 00:37:54,880
okay it says additional communication

00:37:51,839 --> 00:37:58,880
with card service because cut should be

00:37:54,880 --> 00:38:01,280
recalculated in this case um

00:37:58,880 --> 00:38:03,200
after that i see also i see the response

00:38:01,280 --> 00:38:06,079
from that

00:38:03,200 --> 00:38:07,520
execution time and here's a

00:38:06,079 --> 00:38:08,720
communication with marketplace service

00:38:07,520 --> 00:38:12,880
implementation book

00:38:08,720 --> 00:38:15,200
this is involved request and response

00:38:12,880 --> 00:38:17,200
and at the end i see the response from

00:38:15,200 --> 00:38:20,240
the first state and execution inside the

00:38:17,200 --> 00:38:23,440
site it's really useful so i see all

00:38:20,240 --> 00:38:27,680
parts of the systems and can analyze

00:38:23,440 --> 00:38:30,240
either slow downs or errors or they see

00:38:27,680 --> 00:38:31,920
which part of my distributed system the

00:38:30,240 --> 00:38:35,200
request has arrived and it's

00:38:31,920 --> 00:38:38,000
quite important so without this kind of

00:38:35,200 --> 00:38:41,119
tracing the maintenance of the system

00:38:38,000 --> 00:38:41,119
was was impossible

00:38:43,200 --> 00:38:49,839
okay conclusions

00:38:47,520 --> 00:38:50,720
the first uh tracing correlation the

00:38:49,839 --> 00:38:52,800
singular

00:38:50,720 --> 00:38:54,480
entities are really essential to this

00:38:52,800 --> 00:38:56,079
beauty system so if you make

00:38:54,480 --> 00:38:58,640
build your distributed system with a

00:38:56,079 --> 00:39:01,599
must so you should really think about

00:38:58,640 --> 00:39:02,720
kind of tracing because without it's a

00:39:01,599 --> 00:39:05,040
if the system is

00:39:02,720 --> 00:39:06,720
uh even not complicated to have some

00:39:05,040 --> 00:39:10,000
only some hosts

00:39:06,720 --> 00:39:12,839
is still a huge issue without tracing

00:39:10,000 --> 00:39:15,680
solution to analyze it

00:39:12,839 --> 00:39:16,240
um zip can open tracing provide quite

00:39:15,680 --> 00:39:18,880
nice

00:39:16,240 --> 00:39:19,359
open source solutions for that and you

00:39:18,880 --> 00:39:23,280
have

00:39:19,359 --> 00:39:27,040
uh you can benefit of uh out of the box

00:39:23,280 --> 00:39:29,040
or integration um with a

00:39:27,040 --> 00:39:31,440
color of six half camera with a spring

00:39:29,040 --> 00:39:32,000
boot so it's already integrated so you

00:39:31,440 --> 00:39:34,640
should

00:39:32,000 --> 00:39:35,040
either write very small amount of code

00:39:34,640 --> 00:39:36,240
or

00:39:35,040 --> 00:39:38,079
for example for spring boot you

00:39:36,240 --> 00:39:39,760
shouldn't write code at all just add

00:39:38,079 --> 00:39:41,920
some dependencies and you here

00:39:39,760 --> 00:39:43,520
you you see a working solution with that

00:39:41,920 --> 00:39:45,119
of course you can cost customize so that

00:39:43,520 --> 00:39:47,440
introduce their own ids

00:39:45,119 --> 00:39:48,240
which is the taste but important that

00:39:47,440 --> 00:39:51,200
you be

00:39:48,240 --> 00:39:51,920
able to trace your round cheeks of the

00:39:51,200 --> 00:39:55,359
messages

00:39:51,920 --> 00:39:57,520
analyze the failure errors and analyze

00:39:55,359 --> 00:39:59,200
the system slowdowns using the tracing

00:39:57,520 --> 00:40:02,880
solution

00:39:59,200 --> 00:40:06,160
yeah that's basically all from my site

00:40:02,880 --> 00:40:06,160
perhaps you have

00:40:06,240 --> 00:40:11,839
some questions

00:40:15,520 --> 00:40:18,400
so i have some

00:40:21,599 --> 00:40:25,359
one of from the modis exception should

00:40:24,079 --> 00:40:29,599
always propagate

00:40:25,359 --> 00:40:31,359
a bubble up to the caller

00:40:29,599 --> 00:40:33,520
so to me the caller process is

00:40:31,359 --> 00:40:36,079
responsible for not to service

00:40:33,520 --> 00:40:37,760
basically all the both sides are

00:40:36,079 --> 00:40:41,040
important because

00:40:37,760 --> 00:40:44,640
if exception or slowdown happens on the

00:40:41,040 --> 00:40:47,760
server side you also need to know

00:40:44,640 --> 00:40:50,079
on the deeper level what really happens

00:40:47,760 --> 00:40:50,880
so of course yeah you lock some

00:40:50,079 --> 00:40:55,119
information

00:40:50,880 --> 00:40:57,040
and exception to the client but

00:40:55,119 --> 00:40:58,720
in some times for example you can

00:40:57,040 --> 00:41:00,240
proceed some exceptional situation on

00:40:58,720 --> 00:41:03,440
the server side

00:41:00,240 --> 00:41:06,720
and therefore a

00:41:03,440 --> 00:41:08,640
client will have some success with infor

00:41:06,720 --> 00:41:10,160
or just normal response but still

00:41:08,640 --> 00:41:10,720
something happens uh something that

00:41:10,160 --> 00:41:14,640
happens

00:41:10,720 --> 00:41:16,960
uh on the uh server side and you would

00:41:14,640 --> 00:41:19,280
like to see this as log as well so

00:41:16,960 --> 00:41:20,000
basically the boss kind of logging is

00:41:19,280 --> 00:41:22,400
important

00:41:20,000 --> 00:41:23,599
on the server side and the client side

00:41:22,400 --> 00:41:26,240
and for me

00:41:23,599 --> 00:41:28,079
it's very really important to understand

00:41:26,240 --> 00:41:30,480
based on the lock how message

00:41:28,079 --> 00:41:31,280
round trips looks like which part of the

00:41:30,480 --> 00:41:34,160
system message

00:41:31,280 --> 00:41:34,880
arrive and what happens in every part of

00:41:34,160 --> 00:41:38,880
the system you

00:41:34,880 --> 00:41:42,400
should have clear build to analyze it

00:41:38,880 --> 00:41:43,599
next question are there books that are

00:41:42,400 --> 00:41:49,440
low together tracing

00:41:43,599 --> 00:41:53,680
in foreign http tracing

00:41:49,440 --> 00:41:57,680
i think so yes but to be honest i cannot

00:41:53,680 --> 00:42:00,960
i have i i see plus plus frameworks um

00:41:57,680 --> 00:42:03,119
15 years ago and to be honest i don't

00:42:00,960 --> 00:42:04,880
know the actual state but i definitely

00:42:03,119 --> 00:42:08,960
there are some integration solutions

00:42:04,880 --> 00:42:08,960
so is c plus plus and dot net

00:42:09,359 --> 00:42:13,119
um next question doesn't not generate

00:42:11,680 --> 00:42:16,240
logical communication that's very really

00:42:13,119 --> 00:42:17,839
good a really good point

00:42:16,240 --> 00:42:19,760
yes it could generate a lot of

00:42:17,839 --> 00:42:23,119
communication to the

00:42:19,760 --> 00:42:25,839
uh your tracing server but

00:42:23,119 --> 00:42:26,880
the first uh points you can decide

00:42:25,839 --> 00:42:29,040
either you need it

00:42:26,880 --> 00:42:30,800
at all or it's enough just to write log

00:42:29,040 --> 00:42:33,200
information you can say

00:42:30,800 --> 00:42:34,640
for example i say i say demonstrate the

00:42:33,200 --> 00:42:36,960
ecommerce solution we don't

00:42:34,640 --> 00:42:38,160
communicate proactively this stuff to

00:42:36,960 --> 00:42:40,160
the uh

00:42:38,160 --> 00:42:41,280
zipkin server or addressing server just

00:42:40,160 --> 00:42:43,520
right in the logs

00:42:41,280 --> 00:42:47,040
and login system elastic elasticsearch

00:42:43,520 --> 00:42:49,680
responsible to correlate this

00:42:47,040 --> 00:42:52,160
the second this communication happens

00:42:49,680 --> 00:42:56,560
asynchronously so basically it's not

00:42:52,160 --> 00:42:58,319
directly slow down your system so

00:42:56,560 --> 00:43:00,079
you have a service communication and

00:42:58,319 --> 00:43:03,599
your system just works or

00:43:00,079 --> 00:43:06,079
cruiser you don't blocked the thread

00:43:03,599 --> 00:43:08,240
the communication with tracing server is

00:43:06,079 --> 00:43:12,079
out of bound

00:43:08,240 --> 00:43:14,240
and you also can control using sample

00:43:12,079 --> 00:43:16,400
flex or which exactly this is right

00:43:14,240 --> 00:43:17,440
perhaps you don't interesting to look or

00:43:16,400 --> 00:43:20,480
all this understand

00:43:17,440 --> 00:43:21,200
all these things uh all the traces but

00:43:20,480 --> 00:43:22,800
only some

00:43:21,200 --> 00:43:24,960
important stuff should be tested you can

00:43:22,800 --> 00:43:27,839
also can fine-grain control this

00:43:24,960 --> 00:43:27,839
communication

00:43:28,400 --> 00:43:34,000
so if microsoft's a down

00:43:31,839 --> 00:43:36,960
or like unavailable you will see traces

00:43:34,000 --> 00:43:40,000
for this request too

00:43:36,960 --> 00:43:43,200
if microservices completely down uh

00:43:40,000 --> 00:43:44,880
you of course don't see the uh traces

00:43:43,200 --> 00:43:46,800
inside this microservice but

00:43:44,880 --> 00:43:48,079
you will see traces of the of the

00:43:46,800 --> 00:43:51,119
clients

00:43:48,079 --> 00:43:55,680
you see either timeout or if my in my

00:43:51,119 --> 00:43:57,839
example is protected using histics

00:43:55,680 --> 00:43:59,520
circuit breaker you will see some errors

00:43:57,839 --> 00:44:02,319
from circuit breaker

00:43:59,520 --> 00:44:04,400
and you also immediately can see okay my

00:44:02,319 --> 00:44:06,160
client or two or three clients for the

00:44:04,400 --> 00:44:08,000
same service reported

00:44:06,160 --> 00:44:09,760
slow downs or timeouts so perhaps

00:44:08,000 --> 00:44:10,480
something happens with my microservices

00:44:09,760 --> 00:44:14,480
also

00:44:10,480 --> 00:44:14,480
quite good to see

00:44:15,040 --> 00:44:17,440
to see

00:44:18,560 --> 00:44:22,480
what part of system is responsible for

00:44:20,839 --> 00:44:24,560
fault

00:44:22,480 --> 00:44:27,359
if there are overload protection the

00:44:24,560 --> 00:44:30,720
many traces might bring down a system

00:44:27,359 --> 00:44:33,359
in a general high load situation uh

00:44:30,720 --> 00:44:34,640
yes so as basically it's a very similar

00:44:33,359 --> 00:44:36,000
to the question uh

00:44:34,640 --> 00:44:38,319
regarding producing a lot of

00:44:36,000 --> 00:44:42,000
communication basically the uh

00:44:38,319 --> 00:44:45,440
protection of uh load uh this request

00:44:42,000 --> 00:44:47,599
is not a task of the tracing system yeah

00:44:45,440 --> 00:44:49,680
you're tracing just reflect so if you

00:44:47,599 --> 00:44:51,440
have overloads of the requests

00:44:49,680 --> 00:44:53,040
your system suffers you should protect

00:44:51,440 --> 00:44:54,400
it using spike arrest for example in

00:44:53,040 --> 00:44:58,240
your gateway

00:44:54,400 --> 00:45:01,359
or a kind of throttling

00:44:58,240 --> 00:45:04,480
solution basically the

00:45:01,359 --> 00:45:07,680
tracing should should produce as less

00:45:04,480 --> 00:45:09,599
overhead as possible and you can control

00:45:07,680 --> 00:45:11,119
or this kind of communication with

00:45:09,599 --> 00:45:13,040
assembling flakes

00:45:11,119 --> 00:45:15,200
or you can refuse completely to a

00:45:13,040 --> 00:45:17,280
communication with a tracing server and

00:45:15,200 --> 00:45:18,400
really good idea to do it asynchronously

00:45:17,280 --> 00:45:20,640
to don't uh

00:45:18,400 --> 00:45:22,000
block your working threads we said but

00:45:20,640 --> 00:45:24,640
still of course if

00:45:22,000 --> 00:45:25,440
you have a high load uh it could be

00:45:24,640 --> 00:45:27,760
possible that

00:45:25,440 --> 00:45:29,520
a lot of communication works through the

00:45:27,760 --> 00:45:30,960
your tracing server in the system and

00:45:29,520 --> 00:45:32,640
then you should tune this perhaps you

00:45:30,960 --> 00:45:35,520
don't need the whole of the

00:45:32,640 --> 00:45:35,920
trace information on the sum of that or

00:45:35,520 --> 00:45:37,599
uh

00:45:35,920 --> 00:45:39,760
it's enough to write information in the

00:45:37,599 --> 00:45:40,640
log and look it in the elasticsearch

00:45:39,760 --> 00:45:43,119
after that

00:45:40,640 --> 00:45:44,960
so it's basically up to you to decide

00:45:43,119 --> 00:45:47,280
how important your tracing information

00:45:44,960 --> 00:45:49,280
is and

00:45:47,280 --> 00:45:52,160
the open zip conserver and integration

00:45:49,280 --> 00:45:56,160
to provide mechanism to control to find

00:45:52,160 --> 00:45:56,160
fine gray fine-grained configures

00:45:57,040 --> 00:46:02,240
okay so i guess already

00:46:03,280 --> 00:46:11,520
out of time ah

00:46:07,359 --> 00:46:14,640
is it possible to read log entries from

00:46:11,520 --> 00:46:17,680
ios cloud watch

00:46:14,640 --> 00:46:21,680
uh you mean rate logs uh

00:46:17,680 --> 00:46:23,200
using uh what what what do you mean with

00:46:21,680 --> 00:46:26,319
red clocks

00:46:23,200 --> 00:46:27,200
so obvious has all mechanism to uh

00:46:26,319 --> 00:46:30,480
search and that so

00:46:27,200 --> 00:46:30,880
what you can do of course you can uh add

00:46:30,480 --> 00:46:33,119
this

00:46:30,880 --> 00:46:34,800
tracing confirmation to the log and

00:46:33,119 --> 00:46:35,839
right into the cloud watches like for

00:46:34,800 --> 00:46:38,079
example we

00:46:35,839 --> 00:46:40,079
i have not so much experience with

00:46:38,079 --> 00:46:42,800
amazon web services but

00:46:40,079 --> 00:46:43,440
we use a google uh stackdriver for that

00:46:42,800 --> 00:46:45,200
or just

00:46:43,440 --> 00:46:47,839
writes information with the trace in

00:46:45,200 --> 00:46:49,920
google stackdriver and we make a

00:46:47,839 --> 00:46:50,960
elastic search through there all logs

00:46:49,920 --> 00:46:52,800
and you see this

00:46:50,960 --> 00:46:55,839
kind of trace information yes of course

00:46:52,800 --> 00:46:57,280
your good idea to integrate if you use

00:46:55,839 --> 00:47:00,640
the cloud provider to

00:46:57,280 --> 00:47:02,730
integrate their own login solution and

00:47:00,640 --> 00:47:04,960
most of client provider also has

00:47:02,730 --> 00:47:07,839
[Music]

00:47:04,960 --> 00:47:08,560
some of the cloud providers user has a

00:47:07,839 --> 00:47:11,440
tracing

00:47:08,560 --> 00:47:13,599
integration so you can just configure

00:47:11,440 --> 00:47:16,640
your

00:47:13,599 --> 00:47:18,079
library with to communicate with a

00:47:16,640 --> 00:47:20,319
placing solution from your cloud

00:47:18,079 --> 00:47:25,119
provider it's not necessary to has

00:47:20,319 --> 00:47:26,880
to host open zipkin server separately

00:47:25,119 --> 00:47:29,280
not sure that i exactly answer your

00:47:26,880 --> 00:47:32,000
question but

00:47:29,280 --> 00:47:32,000
okay good

00:47:34,640 --> 00:47:39,839
so any any more questions

00:47:44,240 --> 00:47:49,839
if not i would say thanks a lot

00:47:47,280 --> 00:47:51,440
i will attend the conference for all

00:47:49,839 --> 00:47:54,480
three days if you have

00:47:51,440 --> 00:47:56,800
some more questions or

00:47:54,480 --> 00:47:58,640
issues or you would like to integrate

00:47:56,800 --> 00:48:02,960
tracing solution your own system

00:47:58,640 --> 00:48:06,640
just being me i will answer that

00:48:02,960 --> 00:48:21,839
thanks a lot and

00:48:06,640 --> 00:48:21,839
have a nice confident day bye

00:48:59,119 --> 00:49:01,200

YouTube URL: https://www.youtube.com/watch?v=H3dJ0hIqD0U


