Title: Why averages lie and how to truely monitor your systems
Publication date: 2020-10-16
Playlist: ApacheCon @Home 2020: Observability
Description: 
	Why averages lie and how to truely monitor your systems
Filipe Costa Oliveira

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

We spend most of our time looking at the reported averages of our monitoring systems, completely disregarding the painful truth that the numbers that we look at and present to our bosses, to our business and make decisions based upon, do not represent our user experience. This simple fact seems to surprise many people. It feels good looking at steady state monitoring charts. In this session, you will be told why is it important to pay to the "higher end" of the percentile spectrum in most application monitoring, benchmarking, and tuning environments and how you can make better usage of the open-source tooling we have at our disposal ( giving examples on both OSS HDR and T-Digest Histograms ).

Performance Engineer, RedisLabs High-performance scientist, low-level C++ grid and distributed computing. Open Source Contributor.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,000 --> 00:00:28,160
let's

00:00:24,480 --> 00:00:30,000
start so my name is philippe

00:00:28,160 --> 00:00:31,920
and i'm here presenting at apache on

00:00:30,000 --> 00:00:35,040
home uh

00:00:31,920 --> 00:00:37,200
on an officer vivid track um on

00:00:35,040 --> 00:00:39,440
my opinion why averages why and how to

00:00:37,200 --> 00:00:43,120
truly monitor your systems

00:00:39,440 --> 00:00:47,280
um giving you a little bit of context

00:00:43,120 --> 00:00:48,879
of omi so i'm portuguese i'm a

00:00:47,280 --> 00:00:50,320
performance engineer at radius labs i

00:00:48,879 --> 00:00:54,719
work

00:00:50,320 --> 00:00:58,000
mainly towards improving and developing

00:00:54,719 --> 00:01:00,559
open source or open source tools and

00:00:58,000 --> 00:01:02,800
also developing as a consequence of

00:01:00,559 --> 00:01:05,920
several dvd tools performance tools

00:01:02,800 --> 00:01:08,080
uh to be able to properly um

00:01:05,920 --> 00:01:09,200
identify what are the bottlenecks and

00:01:08,080 --> 00:01:13,119
act upon

00:01:09,200 --> 00:01:15,040
um etc so the real idea is that

00:01:13,119 --> 00:01:16,960
we use these types of tools and we use

00:01:15,040 --> 00:01:19,600
the um the

00:01:16,960 --> 00:01:21,280
what we have available to be able to

00:01:19,600 --> 00:01:23,119
improve

00:01:21,280 --> 00:01:24,320
the products that you develop that you

00:01:23,119 --> 00:01:28,479
develop

00:01:24,320 --> 00:01:30,159
um going through the agenda so today uh

00:01:28,479 --> 00:01:31,439
i'll start by the takeaway i'll start by

00:01:30,159 --> 00:01:32,240
the single sentence that i wanted you

00:01:31,439 --> 00:01:35,600
guys to

00:01:32,240 --> 00:01:38,400
uh um to have to be opinionated about

00:01:35,600 --> 00:01:39,439
um then we will deploy one on trying to

00:01:38,400 --> 00:01:41,680
understand

00:01:39,439 --> 00:01:42,720
different patterns or different ways to

00:01:41,680 --> 00:01:45,520
see behaviors

00:01:42,720 --> 00:01:46,560
of your application and the problems

00:01:45,520 --> 00:01:49,280
that

00:01:46,560 --> 00:01:51,040
come with it how to solve it and then

00:01:49,280 --> 00:01:53,280
presenting some numbers around

00:01:51,040 --> 00:01:55,280
some different data structures that

00:01:53,280 --> 00:01:56,799
allow us to have a

00:01:55,280 --> 00:02:00,079
full understanding of what the weight of

00:01:56,799 --> 00:02:02,560
c behavior on our systems

00:02:00,079 --> 00:02:03,200
um the the key takeaway is basically

00:02:02,560 --> 00:02:05,520
this sense

00:02:03,200 --> 00:02:06,320
sentence a single number is not enough

00:02:05,520 --> 00:02:09,520
to

00:02:06,320 --> 00:02:11,680
fully describe um your

00:02:09,520 --> 00:02:12,800
the latency of um your system to have a

00:02:11,680 --> 00:02:15,599
full

00:02:12,800 --> 00:02:16,160
uh visibility of how your system is

00:02:15,599 --> 00:02:19,920
behaving

00:02:16,160 --> 00:02:22,560
with regards to latency uh we will um

00:02:19,920 --> 00:02:22,959
i'll justify why is latency so important

00:02:22,560 --> 00:02:24,879
but

00:02:22,959 --> 00:02:26,080
uh prior than that let's take a quick

00:02:24,879 --> 00:02:29,040
step back and

00:02:26,080 --> 00:02:29,760
um try to think about why do we write

00:02:29,040 --> 00:02:33,200
code

00:02:29,760 --> 00:02:36,160
this is not a philosophical question the

00:02:33,200 --> 00:02:37,280
the overall idea of around me making

00:02:36,160 --> 00:02:40,480
this question is to

00:02:37,280 --> 00:02:42,480
to identify that we normally write code

00:02:40,480 --> 00:02:43,760
to respond to someone needs someone's

00:02:42,480 --> 00:02:45,360
needs so basically

00:02:43,760 --> 00:02:47,680
either a business need organizational

00:02:45,360 --> 00:02:51,360
needs society anyone

00:02:47,680 --> 00:02:54,879
who wants to have some work done

00:02:51,360 --> 00:02:58,480
so and uh as soon as we

00:02:54,879 --> 00:03:02,840
we identify the word that we care about

00:02:58,480 --> 00:03:05,599
we need to be able to measure how

00:03:02,840 --> 00:03:07,840
um performance are we

00:03:05,599 --> 00:03:09,280
on the task that we set ourselves to do

00:03:07,840 --> 00:03:11,040
and this is the work around performance

00:03:09,280 --> 00:03:13,200
engineering is to try to measure the

00:03:11,040 --> 00:03:16,239
amount of useful work that i'm doing

00:03:13,200 --> 00:03:18,159
uh out of the overall work that's that's

00:03:16,239 --> 00:03:19,519
required on my application on the system

00:03:18,159 --> 00:03:22,239
that i'm developing

00:03:19,519 --> 00:03:23,599
and to do so we basically need metrics

00:03:22,239 --> 00:03:26,400
uh metrics that are

00:03:23,599 --> 00:03:28,319
deterministic that help us to fully

00:03:26,400 --> 00:03:30,000
characterize the system that we are

00:03:28,319 --> 00:03:32,400
developing upon and developing or

00:03:30,000 --> 00:03:34,560
analyzing

00:03:32,400 --> 00:03:34,560
but

00:03:35,599 --> 00:03:39,040
removing some of the of the abstraction

00:03:37,840 --> 00:03:40,640
and giving some

00:03:39,040 --> 00:03:42,319
some example imagine that you have an

00:03:40,640 --> 00:03:44,959
application and

00:03:42,319 --> 00:03:45,599
you want to understand its behavior you

00:03:44,959 --> 00:03:48,080
can focus

00:03:45,599 --> 00:03:49,200
um the performance engineering around

00:03:48,080 --> 00:03:51,280
several metrics

00:03:49,200 --> 00:03:53,439
some of them like uh operations per unit

00:03:51,280 --> 00:03:55,519
of time like requests per second success

00:03:53,439 --> 00:03:58,840
metrics error metrics etc

00:03:55,519 --> 00:04:02,400
and one very very important one one that

00:03:58,840 --> 00:04:05,439
uh mostly

00:04:02,400 --> 00:04:08,480
all engineers working around it

00:04:05,439 --> 00:04:11,519
focused upon is latency and why

00:04:08,480 --> 00:04:12,400
latency when compared to others or why

00:04:11,519 --> 00:04:14,239
is this

00:04:12,400 --> 00:04:16,239
this method so important when compared

00:04:14,239 --> 00:04:19,440
to the other ones that

00:04:16,239 --> 00:04:21,519
we can use to to understand our system

00:04:19,440 --> 00:04:23,280
and and the the simple answer is that

00:04:21,519 --> 00:04:24,320
latency provides us the user's

00:04:23,280 --> 00:04:26,160
perspective

00:04:24,320 --> 00:04:28,080
uh whereas for example the number of

00:04:26,160 --> 00:04:29,520
requests per second the system vote will

00:04:28,080 --> 00:04:33,280
only give me insights

00:04:29,520 --> 00:04:36,960
of the system that i'm using to serve

00:04:33,280 --> 00:04:38,479
um some client to serve the purpose that

00:04:36,960 --> 00:04:40,560
i set myself to

00:04:38,479 --> 00:04:41,759
um latency will provide you the

00:04:40,560 --> 00:04:43,280
perspective of

00:04:41,759 --> 00:04:45,120
the person that's using it the

00:04:43,280 --> 00:04:46,800
application that's using it etc so we

00:04:45,120 --> 00:04:49,120
will basically have

00:04:46,800 --> 00:04:51,759
the look and feel of the persons or or

00:04:49,120 --> 00:04:54,639
the users that are

00:04:51,759 --> 00:04:55,199
depending upon their application and to

00:04:54,639 --> 00:04:56,800
be able

00:04:55,199 --> 00:04:58,880
first to be able to to understand the

00:04:56,800 --> 00:05:01,120
latency we will simply define it

00:04:58,880 --> 00:05:02,479
wait and see is basically the length of

00:05:01,120 --> 00:05:05,919
the operation uh whether

00:05:02,479 --> 00:05:08,000
whether we are talking about um some

00:05:05,919 --> 00:05:09,039
requests to database uh request to an

00:05:08,000 --> 00:05:10,800
http server

00:05:09,039 --> 00:05:13,120
and the latency is basically the length

00:05:10,800 --> 00:05:14,160
of from the start up until the end of

00:05:13,120 --> 00:05:16,800
the operation

00:05:14,160 --> 00:05:18,000
um and to be able to understand it or to

00:05:16,800 --> 00:05:19,520
be able to start understanding and

00:05:18,000 --> 00:05:22,080
analyzing latency behaviors

00:05:19,520 --> 00:05:23,039
wait and see distributions we need to

00:05:22,080 --> 00:05:24,880
keep track of it

00:05:23,039 --> 00:05:26,560
and to keep track of it the naive

00:05:24,880 --> 00:05:28,800
approach would be basically just store

00:05:26,560 --> 00:05:29,840
all data if i have all data stored i'll

00:05:28,800 --> 00:05:32,960
be able to

00:05:29,840 --> 00:05:34,080
completely uh analyze the latency

00:05:32,960 --> 00:05:36,320
behavior

00:05:34,080 --> 00:05:38,960
but the problem is that it's very

00:05:36,320 --> 00:05:40,479
expensive it's unfeasible we don't have

00:05:38,960 --> 00:05:41,840
infinite amount of compute time we don't

00:05:40,479 --> 00:05:42,560
have infinite amount of storage et

00:05:41,840 --> 00:05:45,600
cetera so

00:05:42,560 --> 00:05:48,160
this is um this would be the the ideal

00:05:45,600 --> 00:05:50,080
approach to get to have all data always

00:05:48,160 --> 00:05:51,440
ready to be creative but this is

00:05:50,080 --> 00:05:53,600
unfeasible

00:05:51,440 --> 00:05:55,759
so then we move to a common case

00:05:53,600 --> 00:05:59,039
approach so we try to solve this

00:05:55,759 --> 00:06:02,400
with a common case number basically

00:05:59,039 --> 00:06:05,280
um by calculating the average of

00:06:02,400 --> 00:06:06,639
all the samples of all the latency

00:06:05,280 --> 00:06:08,639
samples that i've recorded

00:06:06,639 --> 00:06:09,759
but the problem is that basically the

00:06:08,639 --> 00:06:12,479
average is why

00:06:09,759 --> 00:06:13,520
and they try um and they hide the

00:06:12,479 --> 00:06:15,440
problem

00:06:13,520 --> 00:06:17,759
and i'm going to give you some examples

00:06:15,440 --> 00:06:19,759
of some patterns latency patterns

00:06:17,759 --> 00:06:22,800
that you should identify yourself with

00:06:19,759 --> 00:06:24,080
so let's start with the goods above and

00:06:22,800 --> 00:06:26,560
the ugly averages

00:06:24,080 --> 00:06:27,919
um the good average is basically an

00:06:26,560 --> 00:06:30,080
average that

00:06:27,919 --> 00:06:31,039
follows the normal distribution so if

00:06:30,080 --> 00:06:33,600
this

00:06:31,039 --> 00:06:34,800
was the pattern that we would be seeing

00:06:33,600 --> 00:06:36,560
that we would see

00:06:34,800 --> 00:06:37,840
if we uh plotted the wait and see the

00:06:36,560 --> 00:06:40,479
solution

00:06:37,840 --> 00:06:41,680
uh everything would be perfect we would

00:06:40,479 --> 00:06:43,759
be able to

00:06:41,680 --> 00:06:44,720
fully describe our system uh latency

00:06:43,759 --> 00:06:45,840
behavior

00:06:44,720 --> 00:06:48,880
by just having a mean a standard

00:06:45,840 --> 00:06:51,440
deviation given that the mean

00:06:48,880 --> 00:06:53,120
uh is symmetric or or the latency

00:06:51,440 --> 00:06:55,520
distribution is symmetric about

00:06:53,120 --> 00:06:56,720
about the mean the issue is that this is

00:06:55,520 --> 00:06:58,080
not what happens on

00:06:56,720 --> 00:06:59,840
real-life scenarios on listed with the

00:06:58,080 --> 00:07:02,880
systems

00:06:59,840 --> 00:07:04,319
normally when you

00:07:02,880 --> 00:07:05,680
plot the latencies of your system you

00:07:04,319 --> 00:07:06,400
will have something like this a

00:07:05,680 --> 00:07:08,479
long-tailed

00:07:06,400 --> 00:07:09,759
weight and latest distribution and the

00:07:08,479 --> 00:07:12,319
issue around this

00:07:09,759 --> 00:07:13,680
is that basically we have we are

00:07:12,319 --> 00:07:17,520
completely disregarding

00:07:13,680 --> 00:07:19,360
um the higher latency values we are only

00:07:17,520 --> 00:07:21,280
focusing ourselves on the good parts of

00:07:19,360 --> 00:07:23,680
the weight and the latency distribution

00:07:21,280 --> 00:07:25,120
and the the problem is that this can get

00:07:23,680 --> 00:07:28,160
a lot worse

00:07:25,120 --> 00:07:28,720
um as an example you can instead of just

00:07:28,160 --> 00:07:30,240
seeing

00:07:28,720 --> 00:07:32,319
a long-term distribution you can start

00:07:30,240 --> 00:07:34,800
seeing multi-modal distributions

00:07:32,319 --> 00:07:37,199
and this is normally due to every

00:07:34,800 --> 00:07:38,240
multiple workflows multiple patterns on

00:07:37,199 --> 00:07:40,720
the same

00:07:38,240 --> 00:07:42,319
application for example you have a read

00:07:40,720 --> 00:07:44,240
path and the right path

00:07:42,319 --> 00:07:46,400
and the reads are a lot faster than the

00:07:44,240 --> 00:07:47,759
rights and then you basically will start

00:07:46,400 --> 00:07:49,199
having multi-modal three model

00:07:47,759 --> 00:07:52,240
distributions

00:07:49,199 --> 00:07:54,639
and this can even get worser so

00:07:52,240 --> 00:07:55,440
you can you can have an average um

00:07:54,639 --> 00:07:58,479
that's

00:07:55,440 --> 00:08:00,319
it's not representing any uh

00:07:58,479 --> 00:08:01,840
user experience within our application

00:08:00,319 --> 00:08:05,039
for example if i have

00:08:01,840 --> 00:08:08,479
really fast reads and um small writes

00:08:05,039 --> 00:08:10,560
i can have a calculated average

00:08:08,479 --> 00:08:12,400
um that basically sits in the middle of

00:08:10,560 --> 00:08:14,960
both latency distributions

00:08:12,400 --> 00:08:17,360
and i'm i'm basically trying to analyze

00:08:14,960 --> 00:08:18,960
my system based on a metric that no one

00:08:17,360 --> 00:08:20,639
or based on a wait and see that no one

00:08:18,960 --> 00:08:24,000
is experiencing

00:08:20,639 --> 00:08:26,560
and to give you um

00:08:24,000 --> 00:08:27,759
an insight or or to try to justify how

00:08:26,560 --> 00:08:31,039
common this is and

00:08:27,759 --> 00:08:33,919
and how this is not related to uh to

00:08:31,039 --> 00:08:34,640
my application maturity to my company

00:08:33,919 --> 00:08:38,080
maturity

00:08:34,640 --> 00:08:40,719
um this uh two charts basically uh

00:08:38,080 --> 00:08:41,839
um represents or basically put the

00:08:40,719 --> 00:08:44,959
latency distribution

00:08:41,839 --> 00:08:45,920
of 50 random production servers from uh

00:08:44,959 --> 00:08:48,480
netflix

00:08:45,920 --> 00:08:49,839
around an http server and a data store

00:08:48,480 --> 00:08:53,279
and as you can see

00:08:49,839 --> 00:08:56,640
uh a company that we are used to

00:08:53,279 --> 00:08:59,680
relate towards so many good work

00:08:56,640 --> 00:09:02,160
around latency or around

00:08:59,680 --> 00:09:03,839
around performance engineering you will

00:09:02,160 --> 00:09:07,040
also you can see that

00:09:03,839 --> 00:09:08,959
this is not bound to

00:09:07,040 --> 00:09:10,560
the capacity of your engineering is the

00:09:08,959 --> 00:09:13,360
capacity of

00:09:10,560 --> 00:09:14,720
uh of your product et cetera this will

00:09:13,360 --> 00:09:17,839
happen on any application

00:09:14,720 --> 00:09:21,279
so basically we will we need to be ready

00:09:17,839 --> 00:09:22,720
um to be able to analyze these types of

00:09:21,279 --> 00:09:23,360
latency distributions and not base

00:09:22,720 --> 00:09:26,080
ourselves

00:09:23,360 --> 00:09:26,560
um around them an average that basically

00:09:26,080 --> 00:09:29,920
lies

00:09:26,560 --> 00:09:31,680
and is susceptible to outliers

00:09:29,920 --> 00:09:33,279
as we've seen before is completely

00:09:31,680 --> 00:09:35,760
hiding the long tail

00:09:33,279 --> 00:09:36,560
it's underestimating or even not

00:09:35,760 --> 00:09:39,920
estimating

00:09:36,560 --> 00:09:43,120
any of the actual user experience

00:09:39,920 --> 00:09:45,680
um and we know that by basically having

00:09:43,120 --> 00:09:46,880
a better understanding we can take

00:09:45,680 --> 00:09:49,040
better decisions this is

00:09:46,880 --> 00:09:50,720
um this can be applied in anything in

00:09:49,040 --> 00:09:53,360
our life so basically

00:09:50,720 --> 00:09:54,240
to be able to solve it we need a better

00:09:53,360 --> 00:09:57,360
metric

00:09:54,240 --> 00:09:58,640
and we we should not rely just upon the

00:09:57,360 --> 00:10:00,399
average

00:09:58,640 --> 00:10:02,240
uh so we need a metric that's basically

00:10:00,399 --> 00:10:03,839
representative of data

00:10:02,240 --> 00:10:05,680
it's patient time efficient because if

00:10:03,839 --> 00:10:06,800
it um if we didn't have these

00:10:05,680 --> 00:10:08,240
constraints

00:10:06,800 --> 00:10:10,000
basically just storing all data and

00:10:08,240 --> 00:10:12,480
querying all data would be enough

00:10:10,000 --> 00:10:13,760
and it's practical to use and for that

00:10:12,480 --> 00:10:16,240
we have percentiles

00:10:13,760 --> 00:10:17,279
so the percentiles will basically give

00:10:16,240 --> 00:10:20,160
you a value

00:10:17,279 --> 00:10:21,279
of up to a point of the distribution in

00:10:20,160 --> 00:10:23,839
which

00:10:21,279 --> 00:10:24,480
a percentage of those observations lie

00:10:23,839 --> 00:10:25,920
below

00:10:24,480 --> 00:10:27,760
the giving you a quick example the

00:10:25,920 --> 00:10:29,680
percentile value of

00:10:27,760 --> 00:10:31,519
a group of observation of latencies

00:10:29,680 --> 00:10:33,680
would give you

00:10:31,519 --> 00:10:35,040
the number at which 90 percent of our

00:10:33,680 --> 00:10:36,240
users

00:10:35,040 --> 00:10:39,279
at the most we've experienced that

00:10:36,240 --> 00:10:40,399
latency um but to give more concrete

00:10:39,279 --> 00:10:44,720
examples

00:10:40,399 --> 00:10:47,120
um doing a quick movement points around

00:10:44,720 --> 00:10:48,160
questions and and the way to answer them

00:10:47,120 --> 00:10:49,680
what's the weight

00:10:48,160 --> 00:10:51,440
wait and see that ninety percent of our

00:10:49,680 --> 00:10:54,399
user experience as i've told before

00:10:51,440 --> 00:10:54,800
is the percent of 90 what for example is

00:10:54,399 --> 00:10:57,200
uh

00:10:54,800 --> 00:10:59,200
our worst one percent latency interval

00:10:57,200 --> 00:11:01,360
that can be basically answered

00:10:59,200 --> 00:11:03,120
by plotting the percent of the

00:11:01,360 --> 00:11:06,640
distribution between the p99

00:11:03,120 --> 00:11:07,600
and the p100 and can we compare them can

00:11:06,640 --> 00:11:09,360
we compare

00:11:07,600 --> 00:11:11,760
the latency distribution of this week

00:11:09,360 --> 00:11:14,880
with last week we can we can

00:11:11,760 --> 00:11:16,880
completely do it by just plotting both

00:11:14,880 --> 00:11:19,200
intervals and do a differentiation

00:11:16,880 --> 00:11:20,959
and something that comes from for free

00:11:19,200 --> 00:11:22,480
when we start having the data structures

00:11:20,959 --> 00:11:25,040
to be able to compare

00:11:22,480 --> 00:11:26,880
or to be able to to compute percentiles

00:11:25,040 --> 00:11:28,399
is that we are also able to compute the

00:11:26,880 --> 00:11:31,120
inverse function that's

00:11:28,399 --> 00:11:32,560
um basically enables us to to answer to

00:11:31,120 --> 00:11:34,399
questions like for example

00:11:32,560 --> 00:11:36,160
what's the percentage of users that are

00:11:34,399 --> 00:11:37,600
served up to a certain point

00:11:36,160 --> 00:11:39,680
for example one millisecond five

00:11:37,600 --> 00:11:41,279
milliseconds regarding space and time

00:11:39,680 --> 00:11:43,600
efficiency

00:11:41,279 --> 00:11:45,440
we are somewhere in the middle between

00:11:43,600 --> 00:11:48,640
having an average having the full

00:11:45,440 --> 00:11:50,160
uh latency values so maybe we are moving

00:11:48,640 --> 00:11:52,079
from having millions of samples

00:11:50,160 --> 00:11:53,519
to having dozens and hundreds of samples

00:11:52,079 --> 00:11:55,279
depending on

00:11:53,519 --> 00:11:56,560
and the data the data structure that we

00:11:55,279 --> 00:11:58,399
are using to

00:11:56,560 --> 00:12:00,079
calculate the percentiles and the

00:11:58,399 --> 00:12:02,399
cumulative distributed functions

00:12:00,079 --> 00:12:03,279
regarding being practical to use and

00:12:02,399 --> 00:12:07,120
being useful

00:12:03,279 --> 00:12:10,000
across several majorities of

00:12:07,120 --> 00:12:11,680
your company or your projects basically

00:12:10,000 --> 00:12:13,839
you can you can start by

00:12:11,680 --> 00:12:14,800
working if you're a small company if

00:12:13,839 --> 00:12:17,440
you're a startup

00:12:14,800 --> 00:12:18,720
you can start by looking at the initial

00:12:17,440 --> 00:12:20,160
percentiles of the percent of

00:12:18,720 --> 00:12:22,399
distribution so for example

00:12:20,160 --> 00:12:24,800
i created now a startup i i'm not going

00:12:22,399 --> 00:12:28,560
to focus my efforts around improving

00:12:24,800 --> 00:12:31,279
the p99 or the pin 339 because i have

00:12:28,560 --> 00:12:32,399
more important stuff more i have uh more

00:12:31,279 --> 00:12:34,160
vogue hanging fruits

00:12:32,399 --> 00:12:35,760
running around like i want to improve

00:12:34,160 --> 00:12:39,120
first the 50

00:12:35,760 --> 00:12:41,279
of my um user

00:12:39,120 --> 00:12:42,800
latency requests and then when i met

00:12:41,279 --> 00:12:44,560
myself when i have a when i

00:12:42,800 --> 00:12:46,800
i'm in a big company like for example

00:12:44,560 --> 00:12:48,639
twitter you will not be able to focus on

00:12:46,800 --> 00:12:52,560
on the 5th

00:12:48,639 --> 00:12:54,800
percentile 50 you should be focusing on

00:12:52,560 --> 00:12:56,160
the higher end latencies or the higher

00:12:54,800 --> 00:12:58,399
end percentiles because

00:12:56,160 --> 00:12:59,440
those are the ones that matter the most

00:12:58,399 --> 00:13:02,639
one percent

00:12:59,440 --> 00:13:04,880
of um um one percent of bad

00:13:02,639 --> 00:13:06,560
uh latency experiences for twitter is a

00:13:04,880 --> 00:13:07,920
lot worse than one percent of bad weight

00:13:06,560 --> 00:13:10,959
agencies experiences

00:13:07,920 --> 00:13:13,200
for a small company or company that that

00:13:10,959 --> 00:13:15,120
is starting so basically you can adjust

00:13:13,200 --> 00:13:16,720
the interval range that you are

00:13:15,120 --> 00:13:18,160
focusing yourself upon depending on the

00:13:16,720 --> 00:13:22,000
maturity of your project

00:13:18,160 --> 00:13:25,440
company age etc um and to be able to

00:13:22,000 --> 00:13:27,760
uh help us understand uh

00:13:25,440 --> 00:13:28,720
why this is uh so important and so easy

00:13:27,760 --> 00:13:32,320
to use um

00:13:28,720 --> 00:13:34,720
i'll we'll basically plot the

00:13:32,320 --> 00:13:37,200
the latency percentile distribution in

00:13:34,720 --> 00:13:40,079
which basically on the x-axis you have

00:13:37,200 --> 00:13:40,959
the full percentile range on the y-axis

00:13:40,079 --> 00:13:44,320
you have

00:13:40,959 --> 00:13:47,120
um the vacancy values so just to give a

00:13:44,320 --> 00:13:49,600
quick context on on how does this relate

00:13:47,120 --> 00:13:50,959
or how does this compare against the

00:13:49,600 --> 00:13:53,279
average so

00:13:50,959 --> 00:13:54,320
and this is a um these are real number

00:13:53,279 --> 00:13:56,560
scale samples

00:13:54,320 --> 00:13:58,560
collected from my vocal machine if i if

00:13:56,560 --> 00:13:59,600
i based my analysis upon a mean i would

00:13:58,560 --> 00:14:03,040
be basing myself

00:13:59,600 --> 00:14:06,160
or looking at 1.5 milliseconds

00:14:03,040 --> 00:14:09,120
um and i will be completely disregarding

00:14:06,160 --> 00:14:10,000
uh everything else up to the right uh so

00:14:09,120 --> 00:14:14,000
everything of

00:14:10,000 --> 00:14:17,120
everything else up to 70 milliseconds

00:14:14,000 --> 00:14:19,440
or 10 times worse latencies than

00:14:17,120 --> 00:14:21,440
the mean is providing me so i will be

00:14:19,440 --> 00:14:22,800
basically focusing myself on the good

00:14:21,440 --> 00:14:24,320
part of the latest resolutions and

00:14:22,800 --> 00:14:26,480
completely disregarding

00:14:24,320 --> 00:14:27,760
the bad parts and the part that i should

00:14:26,480 --> 00:14:30,320
improve upon

00:14:27,760 --> 00:14:31,760
um regarding answering the exact same

00:14:30,320 --> 00:14:34,240
questions that we've seen before

00:14:31,760 --> 00:14:35,440
what's the wait and see that 90 of our

00:14:34,240 --> 00:14:38,160
of our users experience

00:14:35,440 --> 00:14:39,279
we will basically just look at the plot

00:14:38,160 --> 00:14:42,720
look at the x axis

00:14:39,279 --> 00:14:45,040
on on the 90 percent of 90 and basically

00:14:42,720 --> 00:14:46,720
um retrieve the latency value that was

00:14:45,040 --> 00:14:48,800
observed

00:14:46,720 --> 00:14:50,880
same thing happens when we're talking

00:14:48,800 --> 00:14:54,079
about intervals we focus ourselves

00:14:50,880 --> 00:14:55,680
upon the interval distribution of um

00:14:54,079 --> 00:14:58,079
the percent already that we want to look

00:14:55,680 --> 00:14:59,440
at so the worst one percent way to see

00:14:58,079 --> 00:15:02,720
would be between

00:14:59,440 --> 00:15:03,920
p99 and p100 and to be able to compare

00:15:02,720 --> 00:15:06,000
ourselves

00:15:03,920 --> 00:15:07,440
against uh different observations or

00:15:06,000 --> 00:15:10,240
different periods of time

00:15:07,440 --> 00:15:11,680
we'll basically just plot them together

00:15:10,240 --> 00:15:15,199
and and understand

00:15:11,680 --> 00:15:16,959
uh if we were worse or or not and where

00:15:15,199 --> 00:15:18,399
did we go towards or what the range

00:15:16,959 --> 00:15:22,240
what's the range of percentage

00:15:18,399 --> 00:15:25,199
in which we uh um got worse

00:15:22,240 --> 00:15:26,959
um regarding what's the the inverse of

00:15:25,199 --> 00:15:29,920
the percentile the cumulative

00:15:26,959 --> 00:15:32,160
distributed function instead of um

00:15:29,920 --> 00:15:34,639
looking through the x-axis we would then

00:15:32,160 --> 00:15:36,399
look to the y-axis and trying to find

00:15:34,639 --> 00:15:38,240
the or calculate the cdf of one

00:15:36,399 --> 00:15:40,800
millisecond and five milliseconds

00:15:38,240 --> 00:15:41,519
and that would basically enables us to

00:15:40,800 --> 00:15:44,160
enable us

00:15:41,519 --> 00:15:45,759
to answer questions like what's the

00:15:44,160 --> 00:15:46,720
percentage of users that are served up

00:15:45,759 --> 00:15:48,800
one millisecond

00:15:46,720 --> 00:15:52,000
and in this example it would be five

00:15:48,800 --> 00:15:53,600
percent and the cdf of five or um

00:15:52,000 --> 00:15:55,440
uh what's the percentage of users that

00:15:53,600 --> 00:15:56,000
were served up to five milliseconds

00:15:55,440 --> 00:15:59,600
would be

00:15:56,000 --> 00:16:02,160
99.8 um of users

00:15:59,600 --> 00:16:03,199
with an average you don't have it's

00:16:02,160 --> 00:16:04,079
impossible to have this type of

00:16:03,199 --> 00:16:08,399
calculation so

00:16:04,079 --> 00:16:12,000
you don't have an any any relation with

00:16:08,399 --> 00:16:13,680
the the amount of users or amount of

00:16:12,000 --> 00:16:15,360
data points that are within a certain

00:16:13,680 --> 00:16:18,399
range

00:16:15,360 --> 00:16:19,680
but uh now that we know uh that we can

00:16:18,399 --> 00:16:21,680
calculate percentiles

00:16:19,680 --> 00:16:23,759
or that we need percentages and ncdfs we

00:16:21,680 --> 00:16:25,279
need to have a data structure

00:16:23,759 --> 00:16:27,120
to be able to do so or to be able to

00:16:25,279 --> 00:16:29,759
compute it

00:16:27,120 --> 00:16:31,519
and for that we have issue m's they come

00:16:29,759 --> 00:16:33,839
in different flavors

00:16:31,519 --> 00:16:35,680
you have t digest the i dynamic range

00:16:33,839 --> 00:16:37,680
insulin digest

00:16:35,680 --> 00:16:39,680
we have a lot of different sketches or

00:16:37,680 --> 00:16:40,560
or and data structures that will allow

00:16:39,680 --> 00:16:43,040
you

00:16:40,560 --> 00:16:44,160
to have the the full latency

00:16:43,040 --> 00:16:47,199
distribution

00:16:44,160 --> 00:16:48,720
but you should not focus upon a specific

00:16:47,199 --> 00:16:51,279
implementation you should

00:16:48,720 --> 00:16:52,800
first ask the right questions that is

00:16:51,279 --> 00:16:56,000
that are basically

00:16:52,800 --> 00:16:58,240
um how does this implementation

00:16:56,000 --> 00:16:59,920
uh will provide or what's the precision

00:16:58,240 --> 00:17:02,480
that this implementation will provide me

00:16:59,920 --> 00:17:04,000
how space efficient it is how how speed

00:17:02,480 --> 00:17:06,559
efficient it is

00:17:04,000 --> 00:17:08,319
does it allow grouping of several

00:17:06,559 --> 00:17:10,799
collections several instagrams

00:17:08,319 --> 00:17:11,679
these are the the right questions that

00:17:10,799 --> 00:17:15,199
should be asked

00:17:11,679 --> 00:17:18,559
prior than choosing an implementation

00:17:15,199 --> 00:17:21,760
and for this specifically for this token

00:17:18,559 --> 00:17:22,959
and i think that um even uh uh on a

00:17:21,760 --> 00:17:25,600
keyboard scenario uh

00:17:22,959 --> 00:17:26,160
or on we can see observation i think

00:17:25,600 --> 00:17:29,600
that

00:17:26,160 --> 00:17:30,320
if you issues um either the digest or

00:17:29,600 --> 00:17:32,880
the hdr

00:17:30,320 --> 00:17:33,919
swim i've added the links here for you

00:17:32,880 --> 00:17:34,880
to follow

00:17:33,919 --> 00:17:36,480
because they have different

00:17:34,880 --> 00:17:38,160
implementations different ports on on

00:17:36,480 --> 00:17:39,840
set on on

00:17:38,160 --> 00:17:41,679
several languages so so you should be

00:17:39,840 --> 00:17:44,240
able to be served by using either t

00:17:41,679 --> 00:17:47,360
digest or the hdr list to them

00:17:44,240 --> 00:17:50,400
and i chose them too because they have

00:17:47,360 --> 00:17:51,440
um a very interesting um property in

00:17:50,400 --> 00:17:54,240
which they differ

00:17:51,440 --> 00:17:55,840
so basically on precision the dynamic

00:17:54,240 --> 00:17:57,919
range is to them

00:17:55,840 --> 00:17:59,520
is more precise on the start of the

00:17:57,919 --> 00:18:02,960
interval so

00:17:59,520 --> 00:18:05,360
if you um if you apply

00:18:02,960 --> 00:18:07,760
this towards the latency observations uh

00:18:05,360 --> 00:18:09,760
um i would say that i'm more interested

00:18:07,760 --> 00:18:11,200
upon the latencies that are smaller so

00:18:09,760 --> 00:18:13,440
for example if i have an application

00:18:11,200 --> 00:18:14,640
i'm very very interested on having a

00:18:13,440 --> 00:18:17,120
high precision

00:18:14,640 --> 00:18:18,080
horn on values up to one millisecond up

00:18:17,120 --> 00:18:21,200
to one second

00:18:18,080 --> 00:18:22,880
and then i i i really don't care as soon

00:18:21,200 --> 00:18:24,160
as you you start moving away from the

00:18:22,880 --> 00:18:27,840
interval starts

00:18:24,160 --> 00:18:28,799
i don't care if i have um a percentage

00:18:27,840 --> 00:18:32,160
of error

00:18:28,799 --> 00:18:32,720
um that's uh not as relevant as the

00:18:32,160 --> 00:18:34,480
interval

00:18:32,720 --> 00:18:36,000
as the initial interval starts so i

00:18:34,480 --> 00:18:39,840
don't care if i have

00:18:36,000 --> 00:18:43,280
a one millisecond error um rate

00:18:39,840 --> 00:18:46,720
on uh one minute

00:18:43,280 --> 00:18:48,320
sample that's the the the overall idea

00:18:46,720 --> 00:18:50,000
around the dynamic range interval is

00:18:48,320 --> 00:18:51,039
that you have better accuracy and the

00:18:50,000 --> 00:18:54,320
interval start

00:18:51,039 --> 00:18:56,559
regarding the t digest you you are not

00:18:54,320 --> 00:18:57,520
bound the accuracy is not really bound

00:18:56,559 --> 00:19:00,000
at

00:18:57,520 --> 00:19:02,720
an interval range is more bound how does

00:19:00,000 --> 00:19:06,080
that sample

00:19:02,720 --> 00:19:07,039
matches towards the full um data points

00:19:06,080 --> 00:19:10,160
that i have on

00:19:07,039 --> 00:19:12,799
my islam so basically you

00:19:10,160 --> 00:19:13,200
the this instagram is more accurate on

00:19:12,799 --> 00:19:15,200
uh

00:19:13,200 --> 00:19:17,360
low percentiles and high percentiles and

00:19:15,200 --> 00:19:19,440
it

00:19:17,360 --> 00:19:20,559
reduces accuracy when you are for

00:19:19,440 --> 00:19:24,160
example at the middle

00:19:20,559 --> 00:19:26,400
of the the observations that you have

00:19:24,160 --> 00:19:27,600
so basically you have more precision

00:19:26,400 --> 00:19:30,559
when you need it

00:19:27,600 --> 00:19:32,880
so um low percentiles that's basically

00:19:30,559 --> 00:19:35,440
uh the perception of zero zero one

00:19:32,880 --> 00:19:37,600
or the p49s you have a high precision

00:19:35,440 --> 00:19:42,559
there and you lose position on the p50

00:19:37,600 --> 00:19:46,160
or p55 etc regarding space efficiency

00:19:42,559 --> 00:19:48,000
typically on a typical usage of the

00:19:46,160 --> 00:19:49,760
dynamic range instagram you will need

00:19:48,000 --> 00:19:52,080
around 31k

00:19:49,760 --> 00:19:52,799
to have a full range of observations of

00:19:52,080 --> 00:19:55,679
one day

00:19:52,799 --> 00:19:57,120
with microsecond precision around the t

00:19:55,679 --> 00:20:01,280
digest

00:19:57,120 --> 00:20:04,400
you should also have

00:20:01,280 --> 00:20:07,840
you should you should

00:20:04,400 --> 00:20:11,200
basically expect

00:20:07,840 --> 00:20:14,159
spaces or or specific you should expect

00:20:11,200 --> 00:20:14,960
an overall space uses around 20

00:20:14,159 --> 00:20:17,840
kilobytes

00:20:14,960 --> 00:20:18,320
so but um it's important to to state

00:20:17,840 --> 00:20:22,240
that

00:20:18,320 --> 00:20:24,159
this is more precise more

00:20:22,240 --> 00:20:26,240
specifically on the dynamic range islam

00:20:24,159 --> 00:20:27,520
this is very very bound towards your

00:20:26,240 --> 00:20:28,880
interval range to the towards the

00:20:27,520 --> 00:20:32,720
precision that you want

00:20:28,880 --> 00:20:35,360
and so these values should be

00:20:32,720 --> 00:20:37,120
taken just as a reference you should

00:20:35,360 --> 00:20:38,320
deprive on each of the implementations

00:20:37,120 --> 00:20:40,640
to try to understand

00:20:38,320 --> 00:20:42,480
uh what would be the memory footprints

00:20:40,640 --> 00:20:46,320
that you you would account for

00:20:42,480 --> 00:20:49,600
when using uh either of these rams

00:20:46,320 --> 00:20:52,799
regarding speed so um on the speed

00:20:49,600 --> 00:20:53,919
um question or the speed group i i try

00:20:52,799 --> 00:20:56,400
to

00:20:53,919 --> 00:20:58,080
focus myself on the real time ops so

00:20:56,400 --> 00:21:00,240
basically i try to

00:20:58,080 --> 00:21:01,200
benchmark both rights and reads and

00:21:00,240 --> 00:21:03,360
around writes

00:21:01,200 --> 00:21:05,120
the dynamic range instagram is really

00:21:03,360 --> 00:21:08,799
really fast you can basically

00:21:05,120 --> 00:21:11,650
insert or or ingest data points um

00:21:08,799 --> 00:21:13,039
uh at a nanosecond um

00:21:11,650 --> 00:21:14,880
[Music]

00:21:13,039 --> 00:21:16,960
costing another second five nine seconds

00:21:14,880 --> 00:21:19,360
part at the point uh regarding the

00:21:16,960 --> 00:21:20,240
digest it's more expensive on insurgents

00:21:19,360 --> 00:21:22,400
so it's

00:21:20,240 --> 00:21:24,720
up to ten to twelve times more expensive

00:21:22,400 --> 00:21:28,880
than the ai dynamic rate in instagram

00:21:24,720 --> 00:21:31,440
on rights but it wins on reads basically

00:21:28,880 --> 00:21:33,120
but this is also very dependent upon the

00:21:31,440 --> 00:21:33,760
configuration or upon the settings that

00:21:33,120 --> 00:21:37,600
you've

00:21:33,760 --> 00:21:40,000
um defined your instagrams um

00:21:37,600 --> 00:21:41,679
when creating or when initializing them

00:21:40,000 --> 00:21:43,360
but the overall idea is that

00:21:41,679 --> 00:21:44,960
this should be taken as a reference and

00:21:43,360 --> 00:21:47,919
you should understand that

00:21:44,960 --> 00:21:50,240
these islams are very fast and are

00:21:47,919 --> 00:21:52,960
really suited for

00:21:50,240 --> 00:21:54,799
real-time computation both reads and the

00:21:52,960 --> 00:21:59,039
rights on quantile computation

00:21:54,799 --> 00:22:02,480
and also even merging you can merge

00:21:59,039 --> 00:22:04,799
islam so basically join

00:22:02,480 --> 00:22:06,480
the value observations of one islam with

00:22:04,799 --> 00:22:09,679
another instagram

00:22:06,480 --> 00:22:10,159
at a microsecond level regarding

00:22:09,679 --> 00:22:12,320
grouping

00:22:10,159 --> 00:22:13,520
grouping or merging as i spoken they

00:22:12,320 --> 00:22:15,039
both support it

00:22:13,520 --> 00:22:16,720
and this is very important because you

00:22:15,039 --> 00:22:20,159
can basically have

00:22:16,720 --> 00:22:22,559
um several islams

00:22:20,159 --> 00:22:25,600
with different controversies and merge

00:22:22,559 --> 00:22:28,320
them um to be able to have a full

00:22:25,600 --> 00:22:29,600
um latency distribution of your system

00:22:28,320 --> 00:22:32,000
so you can have for example

00:22:29,600 --> 00:22:34,000
uh several instruments one per machine

00:22:32,000 --> 00:22:36,000
or one per time range et cetera and

00:22:34,000 --> 00:22:37,600
then you can merge it this is very very

00:22:36,000 --> 00:22:39,200
important there are several islands

00:22:37,600 --> 00:22:41,919
implementation that don't support it

00:22:39,200 --> 00:22:43,679
so you should take this into account

00:22:41,919 --> 00:22:46,080
regarding the key of differences

00:22:43,679 --> 00:22:46,880
between these two instagrams as i spoken

00:22:46,080 --> 00:22:48,400
before

00:22:46,880 --> 00:22:50,240
basically the bin boundaries or the

00:22:48,400 --> 00:22:51,039
precision the amount of bins that you

00:22:50,240 --> 00:22:55,520
have

00:22:51,039 --> 00:22:57,840
um or that will accommodate samples

00:22:55,520 --> 00:22:59,280
for the hdr instagram are dependent on

00:22:57,840 --> 00:23:01,360
the interval range

00:22:59,280 --> 00:23:03,039
for the t digest are dependent on the

00:23:01,360 --> 00:23:06,240
data set so depending on the data set

00:23:03,039 --> 00:23:09,600
you will have more or less wins

00:23:06,240 --> 00:23:12,320
regarding what rise ahead so

00:23:09,600 --> 00:23:13,039
when up to this point we know that

00:23:12,320 --> 00:23:16,159
average is

00:23:13,039 --> 00:23:20,400
uh y and we have a better way

00:23:16,159 --> 00:23:22,080
of having um a full latency

00:23:20,400 --> 00:23:23,840
distribution of your system or a full

00:23:22,080 --> 00:23:27,440
latency analysis of your system

00:23:23,840 --> 00:23:30,080
but there's still a lot um um

00:23:27,440 --> 00:23:31,200
there's still a lot of work um to do in

00:23:30,080 --> 00:23:33,360
in the matters of

00:23:31,200 --> 00:23:34,640
applying this knowledge applying these

00:23:33,360 --> 00:23:38,400
data structures to

00:23:34,640 --> 00:23:41,279
our everyday life so even up today

00:23:38,400 --> 00:23:43,039
we have we are basing ourselves on the

00:23:41,279 --> 00:23:43,919
observability tools on the time series

00:23:43,039 --> 00:23:47,360
databases

00:23:43,919 --> 00:23:49,440
uh on a common case uh scenario and we

00:23:47,360 --> 00:23:50,400
should make efforts towards having a

00:23:49,440 --> 00:23:52,480
full visibility of

00:23:50,400 --> 00:23:53,760
over time so basically we should focus

00:23:52,480 --> 00:23:55,679
upon applying

00:23:53,760 --> 00:23:57,840
the data structures that we are that we

00:23:55,679 --> 00:24:00,400
have now that are extremely fast that

00:23:57,840 --> 00:24:01,760
that are purposefully built for

00:24:00,400 --> 00:24:03,600
real-time

00:24:01,760 --> 00:24:05,679
computation so now it's just a matter of

00:24:03,600 --> 00:24:06,320
applying them towards the projects that

00:24:05,679 --> 00:24:09,679
we work

00:24:06,320 --> 00:24:13,039
um on our daily basis so basically

00:24:09,679 --> 00:24:16,400
and as a quick quick side note

00:24:13,039 --> 00:24:16,400
just having a full latency

00:24:16,640 --> 00:24:19,679
behavior and distribution analysis is is

00:24:18,960 --> 00:24:22,240
not enough

00:24:19,679 --> 00:24:23,520
so basically as soon as you start

00:24:22,240 --> 00:24:26,000
unlocking this type of

00:24:23,520 --> 00:24:27,200
of visibility you want more and you will

00:24:26,000 --> 00:24:30,080
need more so basically

00:24:27,200 --> 00:24:31,520
after you have this you need to have i

00:24:30,080 --> 00:24:34,320
need to start cultivating

00:24:31,520 --> 00:24:36,320
um this metric or uh with more metrics

00:24:34,320 --> 00:24:39,279
of your system to have a full picture

00:24:36,320 --> 00:24:40,400
of what's going on so relying on a

00:24:39,279 --> 00:24:42,080
single metric

00:24:40,400 --> 00:24:43,679
is not enough we saw that rewind on a

00:24:42,080 --> 00:24:46,240
single number is not enough

00:24:43,679 --> 00:24:47,840
we now are moving to every having more

00:24:46,240 --> 00:24:48,880
numbers for metric but then we need to

00:24:47,840 --> 00:24:52,240
revive

00:24:48,880 --> 00:24:56,080
on several metrics with visibility um

00:24:52,240 --> 00:24:58,080
regarding uh what's more i'm

00:24:56,080 --> 00:24:59,520
i really wanted to kick off the

00:24:58,080 --> 00:25:02,400
discussion around

00:24:59,520 --> 00:25:02,400
how are you guys

00:25:02,720 --> 00:25:06,080
computing um or doing another latency

00:25:04,960 --> 00:25:09,200
analysis on your system

00:25:06,080 --> 00:25:11,360
and i've prepared the um

00:25:09,200 --> 00:25:12,960
bullets around the references or around

00:25:11,360 --> 00:25:15,120
the things that you should look at

00:25:12,960 --> 00:25:16,320
when investigating investigating this

00:25:15,120 --> 00:25:19,360
matter

00:25:16,320 --> 00:25:19,919
and apart from that i i would now leave

00:25:19,360 --> 00:25:21,760
the

00:25:19,919 --> 00:25:23,600
the remaining time for for the

00:25:21,760 --> 00:25:26,960
discussion and

00:25:23,600 --> 00:25:31,279
hopefully we have more we have um

00:25:26,960 --> 00:25:31,279
questions to um to answer

00:25:33,279 --> 00:25:36,799
um if you guys share on the chat i'll be

00:25:36,080 --> 00:25:39,840
willingly

00:25:36,799 --> 00:25:42,400
um answer all

00:25:39,840 --> 00:25:43,200
your questions or if i don't have the

00:25:42,400 --> 00:25:45,919
straight answer

00:25:43,200 --> 00:25:48,640
at least point you towards reference

00:25:45,919 --> 00:25:48,640
things basically

00:25:54,799 --> 00:26:01,200
if we don't have questions um

00:25:58,320 --> 00:26:02,400
i just um i i i want to thank everybody

00:26:01,200 --> 00:26:05,679
for listening to me

00:26:02,400 --> 00:26:09,039
and uh let's uh shut on

00:26:05,679 --> 00:26:10,080
on twitter let's um let's basically make

00:26:09,039 --> 00:26:12,080
this happen let's

00:26:10,080 --> 00:26:13,919
have more visibility on the applications

00:26:12,080 --> 00:26:15,520
that we work upon because now we have

00:26:13,919 --> 00:26:25,840
all means to do so

00:26:15,520 --> 00:26:25,840
and thank you

00:26:46,480 --> 00:26:48,559

YouTube URL: https://www.youtube.com/watch?v=mjHam20dmW8


