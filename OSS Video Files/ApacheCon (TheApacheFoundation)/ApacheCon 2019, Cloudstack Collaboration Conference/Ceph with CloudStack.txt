Title: Ceph with CloudStack
Publication date: 2019-09-19
Playlist: ApacheCon 2019, Cloudstack Collaboration Conference
Description: 
	In this presentation, Andrija is going to give a brief introduction to Ceph and cover some considerations around it's architecture, both in general and related to CloudStack. He is going to cover Ceph's integration into CloudStack, compare it's feature set versus other Primary Storage solutions for CloudStack and also share some general advises on it's setup.
Captions: 
	00:00:04,550 --> 00:00:12,110
hi everyone so we're going to do a bit

00:00:09,139 --> 00:00:13,879
of talk about Seth with caustic my name

00:00:12,110 --> 00:00:15,619
is Angie punish or panic if you would

00:00:13,879 --> 00:00:18,770
like to read it but I have a trade Mike

00:00:15,619 --> 00:00:20,270
there so give it a night

00:00:18,770 --> 00:00:22,850
before I continue I would just like to

00:00:20,270 --> 00:00:24,140
ask everybody it would be actually

00:00:22,850 --> 00:00:26,810
really nice if you could join us for the

00:00:24,140 --> 00:00:30,890
hackathon tomorrow in the same rooms so

00:00:26,810 --> 00:00:32,180
it's very relaxed you know kind of what

00:00:30,890 --> 00:00:34,160
we're just sitting down and discussing

00:00:32,180 --> 00:00:35,539
different things simply seeing you know

00:00:34,160 --> 00:00:37,579
what what are the problems how to move

00:00:35,539 --> 00:00:41,110
forward and so on and I would also like

00:00:37,579 --> 00:00:43,700
to ask you know whoever is actually

00:00:41,110 --> 00:00:46,730
allowed to or capable to actually

00:00:43,700 --> 00:00:48,440
participate in a very like five to ten

00:00:46,730 --> 00:00:52,220
minute short interviews because we are

00:00:48,440 --> 00:00:55,010
doing clouds tech marketing video to try

00:00:52,220 --> 00:00:57,770
to you know more things even more

00:00:55,010 --> 00:00:59,989
forward than they are actually right now

00:00:57,770 --> 00:01:02,059
so that's also going to be during the

00:00:59,989 --> 00:01:03,230
hackathon probably around lunchtime or

00:01:02,059 --> 00:01:05,780
something like that so it's very short

00:01:03,230 --> 00:01:07,490
you know so whoever is most accuser like

00:01:05,780 --> 00:01:09,350
the guys from the leaderboard or you

00:01:07,490 --> 00:01:13,819
know whoever is willing it will be very

00:01:09,350 --> 00:01:16,880
nice good so yeah my name is Andy punch

00:01:13,819 --> 00:01:20,119
I'm working for Shea blue started

00:01:16,880 --> 00:01:23,209
recently and I'm located in in Serbia

00:01:20,119 --> 00:01:26,270
the moment committer and recent

00:01:23,209 --> 00:01:27,889
appointment to BMC member but I've been

00:01:26,270 --> 00:01:30,200
involved were actually work with our

00:01:27,889 --> 00:01:31,999
taxes since the 4-0 incubating version

00:01:30,200 --> 00:01:34,700
the first let's say kind of official

00:01:31,999 --> 00:01:39,139
version from Apache foundation and it's

00:01:34,700 --> 00:01:40,999
very short that's that's about me I'm

00:01:39,139 --> 00:01:42,979
going to give a very quick self intro

00:01:40,999 --> 00:01:45,020
about architecture and stuff but kind of

00:01:42,979 --> 00:01:47,149
very quick now I do need to kind of

00:01:45,020 --> 00:01:49,490
mention that this will be more let's say

00:01:47,149 --> 00:01:51,469
it's self oriented there will be more

00:01:49,490 --> 00:01:52,909
self oriented content but in relation to

00:01:51,469 --> 00:01:55,249
cloud stack and in some general

00:01:52,909 --> 00:01:57,079
considerations but we were also going to

00:01:55,249 --> 00:02:02,659
cover over sliding its to cloud stack

00:01:57,079 --> 00:02:06,139
and such couple of things there good so

00:02:02,659 --> 00:02:08,360
a bit of fun facts if you like from

00:02:06,139 --> 00:02:09,140
Wikipedia and the reason why I actually

00:02:08,360 --> 00:02:12,020
accept was

00:02:09,140 --> 00:02:13,370
name like that Sager and the guys

00:02:12,020 --> 00:02:15,800
thought it has to do something with a

00:02:13,370 --> 00:02:18,349
multi tactical this is what the

00:02:15,800 --> 00:02:21,370
physiology of the

00:02:18,349 --> 00:02:24,260
cephalopods is the kind of vertebrates

00:02:21,370 --> 00:02:29,030
so that's a bit of the living world

00:02:24,260 --> 00:02:31,010
moving to a more IT world if you like we

00:02:29,030 --> 00:02:33,500
all know it's effectively open source

00:02:31,010 --> 00:02:36,590
distributed software-defined solid

00:02:33,500 --> 00:02:38,659
solution initially the whole idea become

00:02:36,590 --> 00:02:41,000
be kind ser feliz that is meant to be

00:02:38,659 --> 00:02:43,879
scalable up to you know many tens of

00:02:41,000 --> 00:02:45,170
thousands of nodes without any kind of

00:02:43,879 --> 00:02:47,269
single point of failure and two-bit

00:02:45,170 --> 00:02:49,189
self-healing if you like or self managed

00:02:47,269 --> 00:02:51,260
whenever possible which means you know

00:02:49,189 --> 00:02:53,690
when you have dead data these objects

00:02:51,260 --> 00:02:55,310
will be automatically object replicas

00:02:53,690 --> 00:02:58,389
will automatically recreated or not on

00:02:55,310 --> 00:03:01,400
another always these are actually they

00:02:58,389 --> 00:03:03,920
drives if you like of whatever kind you

00:03:01,400 --> 00:03:06,730
are running the whole thing is built

00:03:03,920 --> 00:03:08,739
around the crash algorithm which

00:03:06,730 --> 00:03:12,260
effectively stands for controlled

00:03:08,739 --> 00:03:14,420
replication under other something

00:03:12,260 --> 00:03:16,549
something the thing is that you do not

00:03:14,420 --> 00:03:18,859
have any kind of lookup table for

00:03:16,549 --> 00:03:20,480
metadata and stuff like that the whole

00:03:18,859 --> 00:03:23,079
the whole thing is that actually in

00:03:20,480 --> 00:03:26,629
order to read data from that you like

00:03:23,079 --> 00:03:27,680
all the client and let's say server side

00:03:26,629 --> 00:03:30,199
meaning that safe cluster and the

00:03:27,680 --> 00:03:32,180
clients being self clients like Lily bar

00:03:30,199 --> 00:03:35,660
Billy that's it being used by liberals

00:03:32,180 --> 00:03:38,690
and camo they all know by the hashing

00:03:35,660 --> 00:03:40,280
algorithm specification algorithm by the

00:03:38,690 --> 00:03:42,440
crash algorithm where the data is

00:03:40,280 --> 00:03:44,989
already located so they can speak to a

00:03:42,440 --> 00:03:47,060
specific OSD node which I will explain

00:03:44,989 --> 00:03:48,560
briefly in fetch data or when it needs

00:03:47,060 --> 00:03:50,299
to right there tight actually know was

00:03:48,560 --> 00:03:53,750
the way where to write the initial copy

00:03:50,299 --> 00:03:55,549
of the data so there is no lookup table

00:03:53,750 --> 00:03:57,739
or file system table like with all

00:03:55,549 --> 00:04:00,440
normal like let's say file system so

00:03:57,739 --> 00:04:02,659
that's what makes it pretty powerful it

00:04:00,440 --> 00:04:04,819
does provide a couple of methods so we

00:04:02,659 --> 00:04:09,230
have a file base file and directory

00:04:04,819 --> 00:04:10,730
let's say base which is a file system we

00:04:09,230 --> 00:04:12,199
have a way and what we are we are

00:04:10,730 --> 00:04:14,599
actually consuming through clouds that

00:04:12,199 --> 00:04:16,430
is the block level or the RBT or the

00:04:14,599 --> 00:04:18,489
routers blog device image which is

00:04:16,430 --> 00:04:21,979
effective a virtual volume which can be

00:04:18,489 --> 00:04:24,979
accessed also not only via native at

00:04:21,979 --> 00:04:26,990
self client RBG client in some way but

00:04:24,979 --> 00:04:30,289
it can also be exposed we are ice Christ

00:04:26,990 --> 00:04:31,500
the gateways or there is also for Power

00:04:30,289 --> 00:04:34,100
Team and

00:04:31,500 --> 00:04:36,420
Ganesha if I pronounce it correctly

00:04:34,100 --> 00:04:39,360
softer which is effectively kind of an

00:04:36,420 --> 00:04:42,810
NFS gateway so you can export a file

00:04:39,360 --> 00:04:44,370
system via NFS gateway and connect your

00:04:42,810 --> 00:04:46,710
remember for example because we were

00:04:44,370 --> 00:04:47,930
obviously and then Microsoft solutions

00:04:46,710 --> 00:04:52,290
and stuff like that they don't have

00:04:47,930 --> 00:04:56,460
Native Client forceps or native chef

00:04:52,290 --> 00:04:58,320
client yeah yeah let's that's kind of it

00:04:56,460 --> 00:05:02,790
and obviously it does provide object

00:04:58,320 --> 00:05:05,130
storage so s3 and Swift and Swift being

00:05:02,790 --> 00:05:09,210
a support protocols so how it looks like

00:05:05,130 --> 00:05:11,160
on that retexture side the foundation of

00:05:09,210 --> 00:05:13,860
the class of the let's say the code safe

00:05:11,160 --> 00:05:15,720
thing is the Radice cluster which stands

00:05:13,860 --> 00:05:18,630
or what you can actually read something

00:05:15,720 --> 00:05:20,490
something something object store so

00:05:18,630 --> 00:05:21,990
reliable all tournaments distributed

00:05:20,490 --> 00:05:24,120
objects store the whole thing is that

00:05:21,990 --> 00:05:26,130
it's an object store it does have a

00:05:24,120 --> 00:05:28,080
Libra doze which is effectively an API

00:05:26,130 --> 00:05:31,020
level access its providing API level

00:05:28,080 --> 00:05:32,700
access to the cluster and you can build

00:05:31,020 --> 00:05:35,430
your if it does provide obviously you

00:05:32,700 --> 00:05:37,590
know bindings for C C++ Java Python Ruby

00:05:35,430 --> 00:05:40,190
PHP and stuff like that so you can build

00:05:37,590 --> 00:05:42,600
your own applications which is first

00:05:40,190 --> 00:05:45,180
square over here that will talk to the

00:05:42,600 --> 00:05:46,560
little brother's example of those of

00:05:45,180 --> 00:05:49,830
death it's for example rather Java

00:05:46,560 --> 00:05:51,600
library which was written by we do to

00:05:49,830 --> 00:05:55,110
kind of some natural stuff because

00:05:51,600 --> 00:05:57,419
libvirt via Lib RBD which I'm going to

00:05:55,110 --> 00:05:59,580
explain later doesn't yet support

00:05:57,419 --> 00:06:01,740
managing safe snapshot so we had another

00:05:59,580 --> 00:06:03,930
external utility which is talking

00:06:01,740 --> 00:06:05,340
directly to that safe cluster via leap

00:06:03,930 --> 00:06:06,990
rezo's

00:06:05,340 --> 00:06:10,020
obviously we have others gateway which

00:06:06,990 --> 00:06:13,380
is effectively translates you know ss3

00:06:10,020 --> 00:06:15,270
and swift calls if like a Pecos to a

00:06:13,380 --> 00:06:19,200
Libra other schools and effectively

00:06:15,270 --> 00:06:22,380
storing objects on the object store or

00:06:19,200 --> 00:06:24,360
itself cluster a good example of those

00:06:22,380 --> 00:06:27,030
are obvious less s3 clients of any kind

00:06:24,360 --> 00:06:29,280
or some specifics with Swift clients and

00:06:27,030 --> 00:06:32,340
stuff like that we have our BT which is

00:06:29,280 --> 00:06:34,169
a rather slow device effectively it

00:06:32,340 --> 00:06:37,470
provides virtual volume for a VM which

00:06:34,169 --> 00:06:41,130
you can consume the kmo which uses Lib

00:06:37,470 --> 00:06:43,349
our BT as the chef client library on

00:06:41,130 --> 00:06:45,360
your k vm cost or you can actually map

00:06:43,349 --> 00:06:48,629
the volume district or volume via

00:06:45,360 --> 00:06:50,550
kernel image or RL RBD NBD utility we

00:06:48,629 --> 00:06:53,729
are effectively an between driver to

00:06:50,550 --> 00:06:55,080
your host whether it be mor or physical

00:06:53,729 --> 00:06:56,699
service so you know there are a couple

00:06:55,080 --> 00:07:01,110
ways to actually connect to it's like a

00:06:56,699 --> 00:07:03,870
roadblock device to Linux specifically

00:07:01,110 --> 00:07:06,000
and finally we have at cell five system

00:07:03,870 --> 00:07:07,710
which is kind of in the development for

00:07:06,000 --> 00:07:09,930
a long time and became officially stable

00:07:07,710 --> 00:07:12,210
maybe two or three years ago don't quote

00:07:09,930 --> 00:07:14,520
me about something like that and it can

00:07:12,210 --> 00:07:17,189
be exposed either we are native clients

00:07:14,520 --> 00:07:19,379
or criminal clients and fuse let's say

00:07:17,189 --> 00:07:21,539
client or via easy as support for big

00:07:19,379 --> 00:07:24,419
mountain via fuse or you can actually

00:07:21,539 --> 00:07:26,580
export it via Ganesha and FSA solar

00:07:24,419 --> 00:07:28,860
already mentions to other or you know

00:07:26,580 --> 00:07:31,949
for compatibility reasons to VMware two

00:07:28,860 --> 00:07:35,400
windows to either gnomes N or actually

00:07:31,949 --> 00:07:37,800
whatever you would like to use not the

00:07:35,400 --> 00:07:39,840
best performance when you're using an

00:07:37,800 --> 00:07:42,389
offense gateway so it's not like your

00:07:39,840 --> 00:07:45,090
enterprise met up stuff keep that in

00:07:42,389 --> 00:07:48,150
mind but it is there and and yeah it's

00:07:45,090 --> 00:07:53,339
interesting in order to be able to you

00:07:48,150 --> 00:07:55,860
know access the stuff the NFS very

00:07:53,339 --> 00:07:57,479
briefly uh safe storage cluster which is

00:07:55,860 --> 00:07:59,849
also called routers class stories

00:07:57,479 --> 00:08:01,560
effectively based upon dradis which is

00:07:59,849 --> 00:08:05,849
something something startling objects

00:08:01,560 --> 00:08:08,639
store kind of makes sense it does

00:08:05,849 --> 00:08:10,620
consist of two types of mandatory demons

00:08:08,639 --> 00:08:12,529
which are self also storage I'm going to

00:08:10,620 --> 00:08:14,969
explain this briefly and Steph monitors

00:08:12,529 --> 00:08:17,610
optionally if you're using therefore if

00:08:14,969 --> 00:08:19,740
you want to have safe filesystem then

00:08:17,610 --> 00:08:22,250
you also need to install itself metadata

00:08:19,740 --> 00:08:25,199
servers which are yes as you can suggest

00:08:22,250 --> 00:08:27,210
tracking and keeping metadata of the

00:08:25,199 --> 00:08:28,439
actual data that you put on the files

00:08:27,210 --> 00:08:31,050
and directories that you put on your

00:08:28,439 --> 00:08:32,159
self file system I mean of a possible

00:08:31,050 --> 00:08:33,659
installation obviously well here at

00:08:32,159 --> 00:08:36,120
least twice team

00:08:33,659 --> 00:08:38,219
demons which means kind of to it to sd

00:08:36,120 --> 00:08:40,500
to storage drives or whatever kind

00:08:38,219 --> 00:08:42,539
because of the minimal replicas being

00:08:40,500 --> 00:08:44,880
two which means one original one

00:08:42,539 --> 00:08:46,949
replicas total two replicas and also it

00:08:44,880 --> 00:08:49,620
will hear it needs the one monitor to

00:08:46,949 --> 00:08:51,180
keep all the tracks of everything which

00:08:49,620 --> 00:08:53,520
I'm going to briefly mention in

00:08:51,180 --> 00:08:55,770
production I was for example a couple of

00:08:53,520 --> 00:08:59,120
years ago managing in production and

00:08:55,770 --> 00:09:01,280
self cluster of six nodes and not the

00:08:59,120 --> 00:09:03,170
set up especially if you go with car

00:09:01,280 --> 00:09:06,500
drives with a mixture of carbs rice and

00:09:03,170 --> 00:09:07,910
you know SSDs for for journals not the

00:09:06,500 --> 00:09:12,230
best performance what we are going to

00:09:07,910 --> 00:09:15,440
touch that late there a lot of our

00:09:12,230 --> 00:09:18,530
actual community users would suggest 10

00:09:15,440 --> 00:09:21,230
storage nodes at minimum and which means

00:09:18,530 --> 00:09:23,810
kind of you know ATM + OS DS which

00:09:21,230 --> 00:09:27,140
translates to a single SSD single nvme

00:09:23,810 --> 00:09:31,400
device single hard drive kind of a you

00:09:27,140 --> 00:09:32,990
know eight or to 10 to 12 per note OS

00:09:31,400 --> 00:09:34,790
Dino's are effectively just a physical

00:09:32,990 --> 00:09:36,260
node which we put very clear action many

00:09:34,790 --> 00:09:40,370
ways these that's kind of our

00:09:36,260 --> 00:09:42,020
terminology and OSDs already explained

00:09:40,370 --> 00:09:43,490
this stands for object storage daemon

00:09:42,020 --> 00:09:46,610
which is effectively managing a single

00:09:43,490 --> 00:09:49,580
storage device being a hard drive SSD

00:09:46,610 --> 00:09:51,200
and stuff like that for all these you

00:09:49,580 --> 00:09:53,810
can hear like up to ten thousandths or

00:09:51,200 --> 00:09:55,790
not up to actually tens of thousands of

00:09:53,810 --> 00:09:58,970
them in the cluster it is managing a

00:09:55,790 --> 00:10:00,980
single hard drive SSD an nvme and stuff

00:09:58,970 --> 00:10:03,440
like that they are the ones who actually

00:10:00,980 --> 00:10:05,450
serve the objects from the Radice

00:10:03,440 --> 00:10:07,100
clusters or something-something object

00:10:05,450 --> 00:10:10,100
store they're the ones who actually

00:10:07,100 --> 00:10:13,790
serve the data effectively to the

00:10:10,100 --> 00:10:16,040
clients while monitors actually provided

00:10:13,790 --> 00:10:18,320
just the cluster I mean just not adjust

00:10:16,040 --> 00:10:20,420
it's very kind of by intelligent and in

00:10:18,320 --> 00:10:21,800
complex work they continue the cluster

00:10:20,420 --> 00:10:23,960
maps which are kind of five different

00:10:21,800 --> 00:10:26,750
maps which are sent initially to the

00:10:23,960 --> 00:10:29,480
clients safe clients do connect two

00:10:26,750 --> 00:10:32,300
monitors to obtain the cluster map who

00:10:29,480 --> 00:10:33,800
is alive where the data is what PGS are

00:10:32,300 --> 00:10:36,140
there which we're going to briefly

00:10:33,800 --> 00:10:38,480
mention later so simply to learn the

00:10:36,140 --> 00:10:41,120
topology of the cluster and once the

00:10:38,480 --> 00:10:43,760
client has learned east or got that data

00:10:41,120 --> 00:10:47,540
from the monitors they will actually

00:10:43,760 --> 00:10:49,790
then on dance by simple also being a

00:10:47,540 --> 00:10:52,250
crush algorithm enabled clients they

00:10:49,790 --> 00:10:55,010
will know to which was d to connect to

00:10:52,250 --> 00:10:59,900
read existing data or to write new data

00:10:55,010 --> 00:11:03,160
to the small image we effectively

00:10:59,900 --> 00:11:05,990
represent some many different servers

00:11:03,160 --> 00:11:08,600
for these you will have many many ways

00:11:05,990 --> 00:11:11,690
this per single server physical server

00:11:08,600 --> 00:11:12,769
usually but for monitors and the

00:11:11,690 --> 00:11:14,449
metadata server see

00:11:12,769 --> 00:11:17,480
you have only one of those demons per

00:11:14,449 --> 00:11:18,980
physical host or virtual host and for

00:11:17,480 --> 00:11:22,220
that matter you actually can use and

00:11:18,980 --> 00:11:26,089
probably you would use VMs not wasting

00:11:22,220 --> 00:11:27,769
like extremely powerful server but they

00:11:26,089 --> 00:11:30,139
do need to be of a decent performance

00:11:27,769 --> 00:11:31,910
especially in larger clusters in this

00:11:30,139 --> 00:11:35,299
example here like three monitors which

00:11:31,910 --> 00:11:37,790
is kind of a minimum for redundancy it's

00:11:35,299 --> 00:11:41,809
a minimum recommended the day it has to

00:11:37,790 --> 00:11:44,209
be odd numbers so three five seven most

00:11:41,809 --> 00:11:46,519
of the users on our huge clusters will

00:11:44,209 --> 00:11:50,449
not use more than seven because that's

00:11:46,519 --> 00:11:51,980
already you know seven times you know

00:11:50,449 --> 00:11:54,619
redundant in sense of having seven

00:11:51,980 --> 00:11:56,230
different monitors because the more you

00:11:54,619 --> 00:11:57,980
get the more communication there is

00:11:56,230 --> 00:12:00,319
exponentially and there is a lot of

00:11:57,980 --> 00:12:04,999
noise in that sense so you don't want to

00:12:00,319 --> 00:12:08,660
go like more than seven usually good so

00:12:04,999 --> 00:12:10,790
extremely quick quicker than so far is

00:12:08,660 --> 00:12:12,589
the installation process we're going to

00:12:10,790 --> 00:12:15,290
just mention briefly a preparation

00:12:12,589 --> 00:12:17,209
installation process and then more

00:12:15,290 --> 00:12:19,850
interesting possibly in instance of

00:12:17,209 --> 00:12:23,899
being off about step let's say to create

00:12:19,850 --> 00:12:26,149
a pool and add it to cloud stack so yeah

00:12:23,899 --> 00:12:29,149
let's let's jump forward the preparation

00:12:26,149 --> 00:12:31,579
is obviously a couple of things that you

00:12:29,149 --> 00:12:33,589
need to do safe is extremely sensible to

00:12:31,579 --> 00:12:38,029
time difference between servers so you

00:12:33,589 --> 00:12:40,699
know make sure to to have this really

00:12:38,029 --> 00:12:42,799
tightly sync synced and later I'm going

00:12:40,699 --> 00:12:44,990
to do a very very short demo and I'm

00:12:42,799 --> 00:12:47,480
probably going to hear my server's not

00:12:44,990 --> 00:12:48,189
synchronized just to show you how it

00:12:47,480 --> 00:12:50,240
works

00:12:48,189 --> 00:12:52,129
obviously a fully qualified domain names

00:12:50,240 --> 00:12:53,689
which is that kind of a full host name

00:12:52,129 --> 00:12:55,850
needs to be resolved between all the

00:12:53,689 --> 00:12:57,379
cluster nodes and usually when you're

00:12:55,850 --> 00:12:59,990
building a safe class you will have one

00:12:57,379 --> 00:13:01,910
dedicated small VMware U which we call

00:12:59,990 --> 00:13:04,490
the admin node where we from from which

00:13:01,910 --> 00:13:06,410
we actually deploy whatever tool you are

00:13:04,490 --> 00:13:11,299
using here where I'm going to just show

00:13:06,410 --> 00:13:13,579
you the very let's say old and perfect

00:13:11,299 --> 00:13:15,769
for a demo and PLC's installation at

00:13:13,579 --> 00:13:18,739
self deploy tool but in production you

00:13:15,769 --> 00:13:21,169
would probably use SEF muncipal or route

00:13:18,739 --> 00:13:22,549
rock group or whatever it's actually

00:13:21,169 --> 00:13:25,249
pronounced and stuff like that to be

00:13:22,549 --> 00:13:26,030
able to actually do a much better job of

00:13:25,249 --> 00:13:28,550
automate

00:13:26,030 --> 00:13:30,830
so yeah make sure that the name is

00:13:28,550 --> 00:13:32,990
resolved we'll make sure that the SSH

00:13:30,830 --> 00:13:34,700
key based authentication is working from

00:13:32,990 --> 00:13:37,010
the admin node to all the cluster nodes

00:13:34,700 --> 00:13:40,430
no need between cluster nodes to have

00:13:37,010 --> 00:13:43,370
that it's not consumed and also add the

00:13:40,430 --> 00:13:45,050
proper access repository for a boon -

00:13:43,370 --> 00:13:47,780
for sent to us whatever you are using on

00:13:45,050 --> 00:13:49,940
your abdomen node and finally install

00:13:47,780 --> 00:13:51,980
one secret repository simply - yum

00:13:49,940 --> 00:13:55,040
installer up yet install itself deploy -

00:13:51,980 --> 00:13:56,300
that's on your admin node now the actual

00:13:55,040 --> 00:13:57,620
installation is obviously on that me

00:13:56,300 --> 00:13:59,240
note you would like to create a folder

00:13:57,620 --> 00:14:03,020
where you list all your cluster

00:13:59,240 --> 00:14:05,270
definition files and where you will keep

00:14:03,020 --> 00:14:07,130
yourself configuration files from which

00:14:05,270 --> 00:14:10,100
we actually distribute over to do it

00:14:07,130 --> 00:14:12,640
safe cluster nodes make folder go to

00:14:10,100 --> 00:14:16,010
that folder use the deploy new command

00:14:12,640 --> 00:14:17,840
defining initial nodes no configuration

00:14:16,010 --> 00:14:19,910
or installation is done so far it will

00:14:17,840 --> 00:14:22,010
just make the cluster definition locally

00:14:19,910 --> 00:14:24,890
on the admin node but it will connect

00:14:22,010 --> 00:14:27,080
via SSH just to verify access and gather

00:14:24,890 --> 00:14:30,980
some other information but not changes

00:14:27,080 --> 00:14:33,350
so far safe deploy install command will

00:14:30,980 --> 00:14:35,600
actually go and stall install binaries

00:14:33,350 --> 00:14:38,510
on the hosts which you actually provide

00:14:35,600 --> 00:14:40,910
on that command line now for this latest

00:14:38,510 --> 00:14:42,770
this whole let's say peace your setup I

00:14:40,910 --> 00:14:46,760
was doing with an newest neutralist

00:14:42,770 --> 00:14:48,140
release 14.2 self deploy for now at

00:14:46,760 --> 00:14:50,930
least for in that moment I wasn't

00:14:48,140 --> 00:14:53,720
actually let's say updated it to by

00:14:50,930 --> 00:14:55,520
default use the mutualists which is

00:14:53,720 --> 00:14:57,650
actually what it should do when you do

00:14:55,520 --> 00:15:00,950
not specify the release it installs the

00:14:57,650 --> 00:15:03,740
most recent stable release so I was here

00:15:00,950 --> 00:15:06,830
just explicitly mentioning release new

00:15:03,740 --> 00:15:08,600
tillis and finally once the binaries but

00:15:06,830 --> 00:15:10,310
only the binaries are there nothing

00:15:08,600 --> 00:15:13,160
started no demons running nothing just

00:15:10,310 --> 00:15:15,800
yum yum install was effectively done you

00:15:13,160 --> 00:15:19,010
would self deploy Mon create initial

00:15:15,800 --> 00:15:20,510
which effectively takes all the nodes

00:15:19,010 --> 00:15:22,940
from the classical definition file which

00:15:20,510 --> 00:15:26,270
is great in the first step and create

00:15:22,940 --> 00:15:28,700
monitors and and effectively forms your

00:15:26,270 --> 00:15:30,770
cluster now you do have a cluster

00:15:28,700 --> 00:15:33,830
already but there is no as the daemon so

00:15:30,770 --> 00:15:35,930
kind of not yet useful cluster and

00:15:33,830 --> 00:15:37,520
finally what you would do you would

00:15:35,930 --> 00:15:39,800
actually or not finally actually learn

00:15:37,520 --> 00:15:41,300
to more steps you would deploy

00:15:39,800 --> 00:15:43,790
the need of hearings in itself

00:15:41,300 --> 00:15:45,950
configuration file which was created

00:15:43,790 --> 00:15:48,980
during the first command to all your

00:15:45,950 --> 00:15:50,740
cluster nodes and finally on each of the

00:15:48,980 --> 00:15:53,480
cluster nodes you would effectively

00:15:50,740 --> 00:15:56,150
assuming that the dev s DB is your data

00:15:53,480 --> 00:15:57,650
disc my POC is only one drive of one OSD

00:15:56,150 --> 00:15:59,510
per node which is definitely not

00:15:57,650 --> 00:16:02,810
something you will do but for the POC

00:15:59,510 --> 00:16:06,680
will simply say self deploy or SD create

00:16:02,810 --> 00:16:09,530
and then pass to the actual device and

00:16:06,680 --> 00:16:12,680
at this time effectively you do have

00:16:09,530 --> 00:16:14,990
your cluster running obviously this is

00:16:12,680 --> 00:16:16,820
just a POC but there is a lot of more

00:16:14,990 --> 00:16:18,230
you would like to with additional

00:16:16,820 --> 00:16:22,400
tunings and stuff you would actually do

00:16:18,230 --> 00:16:24,560
in production yeah that being said in

00:16:22,400 --> 00:16:27,290
brew in previous release mimic and now a

00:16:24,560 --> 00:16:29,420
new in nucleus is the dashboard which

00:16:27,290 --> 00:16:31,040
now comes as in neutralist as a separate

00:16:29,420 --> 00:16:33,890
packages which are not still handled but

00:16:31,040 --> 00:16:35,540
it's self deployed you need to locally

00:16:33,890 --> 00:16:38,450
log into the cluster nodes and do the

00:16:35,540 --> 00:16:41,150
yum install and then only one from any

00:16:38,450 --> 00:16:43,400
of the cluster nodes you will say for

00:16:41,150 --> 00:16:45,190
the POC again not something will do

00:16:43,400 --> 00:16:48,320
production you will disable as a cell

00:16:45,190 --> 00:16:51,140
enabled actual - per module and finally

00:16:48,320 --> 00:16:54,290
create your admin / password as the

00:16:51,140 --> 00:16:56,990
administrator role access and he have

00:16:54,290 --> 00:16:58,460
your heavier dashboard running the

00:16:56,990 --> 00:17:00,080
reason I'm recommending or dashboard

00:16:58,460 --> 00:17:02,540
because in new tillis in previous

00:17:00,080 --> 00:17:05,870
releases before new tillis

00:17:02,540 --> 00:17:08,120
it was mostly read-only but now it

00:17:05,870 --> 00:17:09,500
actually became extremely powerful and

00:17:08,120 --> 00:17:13,070
you can do many many different

00:17:09,500 --> 00:17:14,870
modifications I will walk we briefly

00:17:13,070 --> 00:17:17,060
walk you through this in some of the

00:17:14,870 --> 00:17:20,959
next slide so it's a very very useful

00:17:17,060 --> 00:17:22,550
stuff now finally before we touch the

00:17:20,959 --> 00:17:24,290
dashboard the required steps are

00:17:22,550 --> 00:17:26,209
obviously to create a pool for the cloud

00:17:24,290 --> 00:17:30,020
stack set its replica size - in

00:17:26,209 --> 00:17:31,640
production always free and not to new

00:17:30,020 --> 00:17:33,710
things in couple of us releases we need

00:17:31,640 --> 00:17:35,180
to initialize the cloud stack and also a

00:17:33,710 --> 00:17:38,060
new thing in contrast to all the

00:17:35,180 --> 00:17:40,910
releases is little different syntax to

00:17:38,060 --> 00:17:43,400
create effectively a client keyring for

00:17:40,910 --> 00:17:45,440
that specific pool we've got my user on

00:17:43,400 --> 00:17:47,660
the mailing list who was actually using

00:17:45,440 --> 00:17:49,160
an older settings on the newer cluster

00:17:47,660 --> 00:17:52,220
and then some of the features were not

00:17:49,160 --> 00:17:53,570
working because simply he wasn't using

00:17:52,220 --> 00:17:56,600
really the latest documentation

00:17:53,570 --> 00:17:58,279
to create them - or maybe he operated

00:17:56,600 --> 00:18:00,590
something like that so there there was a

00:17:58,279 --> 00:18:02,539
need to actually modify his key rings to

00:18:00,590 --> 00:18:04,279
actually be able to utilize some of the

00:18:02,539 --> 00:18:06,980
newer features like exclusive locks and

00:18:04,279 --> 00:18:09,019
stuff like that example key of the last

00:18:06,980 --> 00:18:10,850
woman will be given just on the standard

00:18:09,019 --> 00:18:13,039
output so write it down oversleep and

00:18:10,850 --> 00:18:14,870
before we proceed we do have a key which

00:18:13,039 --> 00:18:17,090
we can resume but let's configure KVM

00:18:14,870 --> 00:18:19,639
caching which is something you

00:18:17,090 --> 00:18:22,850
absolutely and definitely want to do on

00:18:19,639 --> 00:18:25,659
your KVM nodes usually we will keep

00:18:22,850 --> 00:18:29,779
doing the identical self.com file across

00:18:25,659 --> 00:18:31,909
cluster nodes hypervisor nodes but this

00:18:29,779 --> 00:18:34,730
is for now technically needed only on

00:18:31,909 --> 00:18:38,539
the kayvyun nodes enable cache very

00:18:34,730 --> 00:18:40,879
important for older Linux kernels and in

00:18:38,539 --> 00:18:43,159
general older built io drivers is you

00:18:40,879 --> 00:18:45,950
want to say activate whirl of write

00:18:43,159 --> 00:18:48,049
through cache until I received a very

00:18:45,950 --> 00:18:50,480
first flash command because in older

00:18:48,049 --> 00:18:53,000
Linux kernels like two six from Center

00:18:50,480 --> 00:18:54,620
six numbers of Center six the Ville tayo

00:18:53,000 --> 00:18:56,389
driver was not sending proper flashes

00:18:54,620 --> 00:18:59,570
and then if you have a write back cache

00:18:56,389 --> 00:19:01,580
and no proper flashes you lose

00:18:59,570 --> 00:19:03,019
for whatever reason electricity or

00:19:01,580 --> 00:19:05,029
something like that you end up with a

00:19:03,019 --> 00:19:07,490
corrupted image and you can throw that

00:19:05,029 --> 00:19:08,870
VM away so only after we receive the

00:19:07,490 --> 00:19:10,490
first flash request from the bitter

00:19:08,870 --> 00:19:12,950
driver from the storage driver after

00:19:10,490 --> 00:19:15,590
itself receives that it will say ok this

00:19:12,950 --> 00:19:18,080
is a smart enough clients then I'm

00:19:15,590 --> 00:19:19,309
activating right back caching about

00:19:18,080 --> 00:19:21,230
before that it's only right through

00:19:19,309 --> 00:19:24,500
which effective is not a cache you know

00:19:21,230 --> 00:19:26,990
in a sense and we will once we populate

00:19:24,500 --> 00:19:28,309
this data to the local safe would come

00:19:26,990 --> 00:19:31,549
from the admin node we just want to

00:19:28,309 --> 00:19:34,129
propagate propagate again if this self

00:19:31,549 --> 00:19:36,409
configuration file to the cave VM nodes

00:19:34,129 --> 00:19:38,570
obviously make sure that necessary is

00:19:36,409 --> 00:19:40,899
working the names are resolved all

00:19:38,570 --> 00:19:44,889
things like that similar to the nodes

00:19:40,899 --> 00:19:50,029
cool moving for a very very very short

00:19:44,889 --> 00:19:56,509
dashboard demo give me just a second to

00:19:50,029 --> 00:19:59,240
move my window left or right probably

00:19:56,509 --> 00:20:02,029
right cool

00:19:59,240 --> 00:20:05,389
nuke self dashboard let me refresh that

00:20:02,029 --> 00:20:07,550
but that's much it you will notice I'm

00:20:05,389 --> 00:20:10,160
not green anymore I have a health war

00:20:07,550 --> 00:20:12,280
and if if I click it was said if it were

00:20:10,160 --> 00:20:14,660
say clock skew detected on monitor

00:20:12,280 --> 00:20:18,530
gabion free I'm using carry me notes in

00:20:14,660 --> 00:20:20,690
my PC but this is safe cluster note once

00:20:18,530 --> 00:20:23,510
you do our proper ntp I updates because

00:20:20,690 --> 00:20:25,340
I do have some issues in labs in the lab

00:20:23,510 --> 00:20:27,500
in sense of time synchronization so I

00:20:25,340 --> 00:20:29,720
need to keep it from time to time anyway

00:20:27,500 --> 00:20:34,760
once you do that it will be fine a very

00:20:29,720 --> 00:20:36,470
useful thing is that begone you have a

00:20:34,760 --> 00:20:40,580
couple of uh sections in this obviously

00:20:36,470 --> 00:20:42,410
in this in the first dashboard you care

00:20:40,580 --> 00:20:43,850
about different cluster status for

00:20:42,410 --> 00:20:46,850
monitors Frye's these for the cluster

00:20:43,850 --> 00:20:48,560
itself for different demons for object

00:20:46,850 --> 00:20:50,450
gateways and stuff like that you also

00:20:48,560 --> 00:20:53,390
need your performance section obviously

00:20:50,450 --> 00:20:55,310
my PC cluster is not very busy but you

00:20:53,390 --> 00:20:58,280
will you can you know monitor clients

00:20:55,310 --> 00:21:00,440
through panini ops versus recovery

00:20:58,280 --> 00:21:03,050
throughput which is very important thing

00:21:00,440 --> 00:21:04,370
to kind of throttle properly in

00:21:03,050 --> 00:21:06,740
production I will touch on that later

00:21:04,370 --> 00:21:09,500
also you can see very scrubs which are

00:21:06,740 --> 00:21:13,340
effective effectively a file system

00:21:09,500 --> 00:21:14,870
checks like checks but for the OSD dmoz

00:21:13,340 --> 00:21:17,210
so it's checking the data which means

00:21:14,870 --> 00:21:19,190
scrub effectively is let's say file

00:21:17,210 --> 00:21:21,910
system check in a way although there is

00:21:19,190 --> 00:21:23,780
no file system now with these newer

00:21:21,910 --> 00:21:26,240
self-releases which I'm going to touch

00:21:23,780 --> 00:21:29,030
again a capacity of different kinds

00:21:26,240 --> 00:21:33,260
objects VG's status of P G's which means

00:21:29,030 --> 00:21:35,630
effective health not really so again

00:21:33,260 --> 00:21:38,180
very very useful thing well what you can

00:21:35,630 --> 00:21:43,430
do ok you can observe your monitors

00:21:38,180 --> 00:21:45,380
always these you can scrub your P G's to

00:21:43,430 --> 00:21:48,170
deeps Rob Rubick your ROI is these so do

00:21:45,380 --> 00:21:51,590
many modifications of the OS DS you can

00:21:48,170 --> 00:21:53,050
also set up a plaster while flags if

00:21:51,590 --> 00:21:56,480
needed you can observe performance

00:21:53,050 --> 00:21:58,130
read/write bytes and I obsess d you can

00:21:56,480 --> 00:22:00,260
observe overall performance but notice

00:21:58,130 --> 00:22:05,090
that I didn't enable the primitives

00:22:00,260 --> 00:22:06,680
metrics X exporting if you like so my -

00:22:05,090 --> 00:22:08,450
what cannot consume this because the

00:22:06,680 --> 00:22:10,150
sources are not there for the data but

00:22:08,450 --> 00:22:15,260
once you do that you will hear very nice

00:22:10,150 --> 00:22:18,950
metrics available you can also do a bit

00:22:15,260 --> 00:22:21,170
of cluster wide configuration which for

00:22:18,950 --> 00:22:23,540
some reason is taken time

00:22:21,170 --> 00:22:26,630
a couple of things you can manage pools

00:22:23,540 --> 00:22:28,820
create all delete full edit pool observe

00:22:26,630 --> 00:22:32,030
performance again or upper pool bases

00:22:28,820 --> 00:22:34,520
and stuff like that very useful you can

00:22:32,030 --> 00:22:36,050
manage your blog devices your which are

00:22:34,520 --> 00:22:39,410
effectively are be the images that are

00:22:36,050 --> 00:22:41,330
cloud stack is using do different things

00:22:39,410 --> 00:22:44,900
obviously create edit and stuff like

00:22:41,330 --> 00:22:46,490
that a bunch of you know if mutual sit

00:22:44,900 --> 00:22:47,990
became very very powerful dashboard

00:22:46,490 --> 00:22:49,850
sense that you can actually change and

00:22:47,990 --> 00:22:52,400
manage things in contrast to previous

00:22:49,850 --> 00:22:54,530
and also you can also manage NFS Ganesha

00:22:52,400 --> 00:22:56,150
wants you a do a couple of preparations

00:22:54,530 --> 00:22:59,150
on your staff class or manually object

00:22:56,150 --> 00:23:00,890
gateways and stuff like that so again

00:22:59,150 --> 00:23:03,470
very very powerful dashboard in this new

00:23:00,890 --> 00:23:06,470
releases so that's the reason I'm kind

00:23:03,470 --> 00:23:08,570
of mentioning it but this is a bit more

00:23:06,470 --> 00:23:12,260
a kind of complete list of the changes

00:23:08,570 --> 00:23:16,570
in the dashboard itself so really offer

00:23:12,260 --> 00:23:16,570
a lot of interesting things good stuff

00:23:16,580 --> 00:23:20,510
new new tools not in dashboard but in

00:23:19,040 --> 00:23:21,860
general the result of things which I

00:23:20,510 --> 00:23:23,330
won't be touching because I'm pretty

00:23:21,860 --> 00:23:25,190
sure I won't have time to cover

00:23:23,330 --> 00:23:26,900
everything but you know at least in your

00:23:25,190 --> 00:23:28,160
production you know you will need to

00:23:26,900 --> 00:23:30,440
open you know possibly some additional

00:23:28,160 --> 00:23:32,030
firewall ports because there is version

00:23:30,440 --> 00:23:34,310
two protocol of the monitor

00:23:32,030 --> 00:23:36,770
communication which finally has a

00:23:34,310 --> 00:23:39,080
properly assigned you know official port

00:23:36,770 --> 00:23:39,230
instead of previously used one which was

00:23:39,080 --> 00:23:41,270
not

00:23:39,230 --> 00:23:44,540
I Anna if I pronounce it correctly

00:23:41,270 --> 00:23:46,550
approved or assigned with encryption and

00:23:44,540 --> 00:23:48,490
things like that so a lot of different

00:23:46,550 --> 00:23:50,750
things like smart prediction failure

00:23:48,490 --> 00:23:52,610
optional automatic migration of your

00:23:50,750 --> 00:23:53,360
objects away if it predicts that the

00:23:52,610 --> 00:23:55,430
drive is failing

00:23:53,360 --> 00:23:57,770
so really awful a lot of interesting

00:23:55,430 --> 00:24:00,620
things and also they do a lot of work

00:23:57,770 --> 00:24:02,570
with the rook which is effectively a

00:24:00,620 --> 00:24:04,970
operator in communities to be able to

00:24:02,570 --> 00:24:07,220
extremely easy like with three commands

00:24:04,970 --> 00:24:08,450
or so to spin your demo cluster for

00:24:07,220 --> 00:24:09,650
testing purposes and stuff they are

00:24:08,450 --> 00:24:13,160
still doing with somewhere to make it

00:24:09,650 --> 00:24:15,050
kind of production suitable but so far

00:24:13,160 --> 00:24:17,000
you mainly people were using seven Sybil

00:24:15,050 --> 00:24:23,600
as the proper tool for managing

00:24:17,000 --> 00:24:25,870
production clusters good let me just

00:24:23,600 --> 00:24:28,870
check my slides I believe I did skip

00:24:25,870 --> 00:24:28,870
something

00:24:36,760 --> 00:24:43,970
sorry I did not skip anything good so

00:24:41,480 --> 00:24:46,040
yeah the next thing once we have a pool

00:24:43,970 --> 00:24:50,150
ready and so on you will obviously doll

00:24:46,040 --> 00:24:52,580
yourself cluster not needed to save okay

00:24:50,150 --> 00:24:56,030
then offering a spinner VM but you know

00:24:52,580 --> 00:25:00,110
in sense of nothing really to my mouse

00:24:56,030 --> 00:25:01,670
go instead of adding theft to do a

00:25:00,110 --> 00:25:03,020
plastic as the primary storage you need

00:25:01,670 --> 00:25:04,400
to choose their BT as the protocol

00:25:03,020 --> 00:25:07,430
defined monitor which I'm going to touch

00:25:04,400 --> 00:25:09,500
just now define your rather spool which

00:25:07,430 --> 00:25:11,510
we created by its name user and

00:25:09,500 --> 00:25:13,820
authentication key which was output if

00:25:11,510 --> 00:25:15,770
your standard output optionally do our

00:25:13,820 --> 00:25:17,720
BD tagging sorry storage tagging and

00:25:15,770 --> 00:25:19,340
things like that for the monitors you

00:25:17,720 --> 00:25:20,630
will never ever obviously define if you

00:25:19,340 --> 00:25:22,040
care like three or more monitors you

00:25:20,630 --> 00:25:23,990
will never define a single monitor

00:25:22,040 --> 00:25:26,570
because if that monitors go down your

00:25:23,990 --> 00:25:28,820
KVM hosts cannot more no more speak to

00:25:26,570 --> 00:25:30,830
that set clusters so it's kind of not

00:25:28,820 --> 00:25:32,690
really good setup for demo cluster this

00:25:30,830 --> 00:25:34,220
is what I did but what you will do in

00:25:32,690 --> 00:25:37,790
production will create a round robin DNS

00:25:34,220 --> 00:25:40,760
entries in your local DNS hopefully

00:25:37,790 --> 00:25:42,440
local DNS so it will resolve you know

00:25:40,760 --> 00:25:44,570
randomly across three different IP

00:25:42,440 --> 00:25:46,400
addresses if one of the monitor is dead

00:25:44,570 --> 00:25:49,130
you will rush to your DNS remove the

00:25:46,400 --> 00:25:50,540
entry and you are kind of good to go so

00:25:49,130 --> 00:25:52,100
that's how the things are using

00:25:50,540 --> 00:25:55,220
production unfortunately it cannot be

00:25:52,100 --> 00:25:56,600
you cannot use for example whatever a

00:25:55,220 --> 00:26:00,110
chip rocks or some other approach to

00:25:56,600 --> 00:26:01,700
proxy to monitor them to proxy the money

00:26:00,110 --> 00:26:03,560
safe monitor traffic because the

00:26:01,700 --> 00:26:05,810
messages are signed and you cannot

00:26:03,560 --> 00:26:06,920
actually interfere with a packet so that

00:26:05,810 --> 00:26:08,810
that's the reason why you cannot

00:26:06,920 --> 00:26:10,250
actually do use local bouncers which is

00:26:08,810 --> 00:26:13,760
the first thing in my head that I would

00:26:10,250 --> 00:26:16,700
use if it would work so let's let's see

00:26:13,760 --> 00:26:20,180
let's observe a couple of things you

00:26:16,700 --> 00:26:22,850
know once you spin your VM how to locate

00:26:20,180 --> 00:26:26,690
your images on the disk do some checks

00:26:22,850 --> 00:26:28,850
or simply see how they're organized when

00:26:26,690 --> 00:26:31,370
you creates a new volume in an existing

00:26:28,850 --> 00:26:32,540
safe cluster obviously in the database

00:26:31,370 --> 00:26:33,800
in the volumes table you will have a

00:26:32,540 --> 00:26:36,740
path field

00:26:33,800 --> 00:26:39,350
this path is for the new volumes which

00:26:36,740 --> 00:26:42,680
are created from 0s opposite to migrate

00:26:39,350 --> 00:26:44,330
from somewhere else the IP the path will

00:26:42,680 --> 00:26:45,799
inherited the value of the ID so it's

00:26:44,330 --> 00:26:47,869
very easy just to go to the GUI

00:26:45,799 --> 00:26:49,610
check the ID and then you will go to

00:26:47,869 --> 00:26:53,059
your safe class sorry there list images

00:26:49,610 --> 00:26:55,369
and notice the same ID or with this is

00:26:53,059 --> 00:26:57,950
actually UUID in the database in the GUI

00:26:55,369 --> 00:27:00,379
it said it's mentioned as the ID or you

00:26:57,950 --> 00:27:03,860
can simply set are a bit info and then

00:27:00,379 --> 00:27:06,169
you said say pool / image which is our

00:27:03,860 --> 00:27:07,730
pool is named cloud Stax left image and

00:27:06,169 --> 00:27:10,940
you will get some info bout that

00:27:07,730 --> 00:27:12,889
specific class Tech image different

00:27:10,940 --> 00:27:14,840
things over here but the one I would

00:27:12,889 --> 00:27:16,730
like to kind of innocence of this

00:27:14,840 --> 00:27:20,320
presentation focus on is the parent

00:27:16,730 --> 00:27:22,249
image we've it's you can actually see

00:27:20,320 --> 00:27:25,340
effectively there is a parent image

00:27:22,249 --> 00:27:27,619
which is actually a snapshot of some

00:27:25,340 --> 00:27:29,149
other image so you know what are we

00:27:27,619 --> 00:27:33,259
actually talking about here let's see

00:27:29,149 --> 00:27:35,239
when you create a self brand yourself VM

00:27:33,259 --> 00:27:37,820
or a volume from a bear from very first

00:27:35,239 --> 00:27:40,309
time for a from a template the usual

00:27:37,820 --> 00:27:44,840
thing happens so effectively the image

00:27:40,309 --> 00:27:47,299
will be Q cow - image will be converted

00:27:44,840 --> 00:27:49,220
by the camera's utility that k vm host

00:27:47,299 --> 00:27:51,679
are the one doing the work they will

00:27:49,220 --> 00:27:53,029
convert the camera image from secondary

00:27:51,679 --> 00:27:55,190
storage which is mounted office and

00:27:53,029 --> 00:27:57,440
aquarium host and stream or write that

00:27:55,190 --> 00:28:02,480
as a new image on your r BD cluster

00:27:57,440 --> 00:28:04,149
creating this template image all a seven

00:28:02,480 --> 00:28:07,460
if you go back that is the one always

00:28:04,149 --> 00:28:09,350
seven so once it's done it will create a

00:28:07,460 --> 00:28:11,139
snapshot to give some hard-coded name

00:28:09,350 --> 00:28:13,970
but the name is obviously not important

00:28:11,139 --> 00:28:16,309
in sense if you do it manually it will

00:28:13,970 --> 00:28:19,309
protect it and only then it will create

00:28:16,309 --> 00:28:21,830
actual at the vm for your this is Fe B

00:28:19,309 --> 00:28:23,720
if you see over here that's our volume

00:28:21,830 --> 00:28:26,029
only then it will actually create the

00:28:23,720 --> 00:28:27,559
actual volume the thing that it does

00:28:26,029 --> 00:28:29,749
protect the snapshot is that you cannot

00:28:27,559 --> 00:28:31,639
go and really delete it so that's the

00:28:29,749 --> 00:28:34,999
nice thing if you want to actually see

00:28:31,639 --> 00:28:36,830
all the volumes not be a database button

00:28:34,999 --> 00:28:38,720
in reality you know new staff cluster

00:28:36,830 --> 00:28:40,549
and storage actually you can use the

00:28:38,720 --> 00:28:44,600
arbiter children command and then define

00:28:40,549 --> 00:28:46,940
the pool image and then at snapshot name

00:28:44,600 --> 00:28:50,359
effectively asking what are the children

00:28:46,940 --> 00:28:54,769
images oops what are the children images

00:28:50,359 --> 00:28:56,059
of my snapshot over here which was great

00:28:54,769 --> 00:28:57,470
in a previous step and you will see many

00:28:56,059 --> 00:28:59,190
of those including

00:28:57,470 --> 00:29:02,400
the one that we were

00:28:59,190 --> 00:29:03,780
showing so far just for fun

00:29:02,400 --> 00:29:05,310
how would you manually reproduce that

00:29:03,780 --> 00:29:07,890
behavior actually you would either use

00:29:05,310 --> 00:29:10,050
the camel image to convert cue cow or

00:29:07,890 --> 00:29:12,030
Arbit import to convert to cue cow and

00:29:10,050 --> 00:29:13,950
write it as the new image or in the say

00:29:12,030 --> 00:29:16,320
for the sake of simplicity we'll just

00:29:13,950 --> 00:29:18,030
create an empty image do the snap create

00:29:16,320 --> 00:29:19,950
command which creates a snapshot giving

00:29:18,030 --> 00:29:21,750
it name so you have to give it some name

00:29:19,950 --> 00:29:23,370
doesn't matter if it's really this one

00:29:21,750 --> 00:29:25,470
or the other but then you will protect

00:29:23,370 --> 00:29:27,510
the snapshot and finally you do the RBD

00:29:25,470 --> 00:29:30,990
clone command defining the source

00:29:27,510 --> 00:29:34,380
snapshot and then the destination volume

00:29:30,990 --> 00:29:35,760
obviously in a form pool / image so

00:29:34,380 --> 00:29:37,500
that's what you can do just for exercise

00:29:35,760 --> 00:29:39,390
if you have nothing else to do in your

00:29:37,500 --> 00:29:41,850
life and the deplane app would be

00:29:39,390 --> 00:29:44,100
completely reversed so effectively in

00:29:41,850 --> 00:29:46,800
order to remove the original image you

00:29:44,100 --> 00:29:49,140
cannot remove it as long as it gets and

00:29:46,800 --> 00:29:50,340
snapshots now you cannot remove the

00:29:49,140 --> 00:29:52,920
snapshot because you haven't yet

00:29:50,340 --> 00:29:54,810
unprotect the snapshot and you cannot

00:29:52,920 --> 00:29:57,450
unprotect the snapshot if you still have

00:29:54,810 --> 00:29:59,370
child images so the steps are ready to

00:29:57,450 --> 00:30:01,380
remove all the work volumes all the

00:29:59,370 --> 00:30:03,420
child images unprotect remove the

00:30:01,380 --> 00:30:07,940
snapshot and finally remove your

00:30:03,420 --> 00:30:12,660
template file that was created initially

00:30:07,940 --> 00:30:14,850
good so a bit of and under

00:30:12,660 --> 00:30:17,550
quotes if you would like to actually

00:30:14,850 --> 00:30:19,980
mount your Arbit image on your Linux

00:30:17,550 --> 00:30:22,680
doesn't matter of obviously if it's a vm

00:30:19,980 --> 00:30:24,930
or or a physical host you can either use

00:30:22,680 --> 00:30:28,020
the kernel plug client with the RVD map

00:30:24,930 --> 00:30:31,520
command defining pool slash image this

00:30:28,020 --> 00:30:34,350
short syntax does like in other examples

00:30:31,520 --> 00:30:38,010
for the other tools that does imply that

00:30:34,350 --> 00:30:39,840
you have on your client on this mobile

00:30:38,010 --> 00:30:41,670
Direction mounting these that you do

00:30:39,840 --> 00:30:44,600
have your self configuration file and

00:30:41,670 --> 00:30:46,500
the needed client keyring file which is

00:30:44,600 --> 00:30:48,450
authentication file effectively which

00:30:46,500 --> 00:30:53,700
contains the user in the secret in the

00:30:48,450 --> 00:30:55,440
default slash at UC / set directory you

00:30:53,700 --> 00:30:57,660
can see a different thing later I will I

00:30:55,440 --> 00:31:01,290
will explain it but the problem in the

00:30:57,660 --> 00:31:02,850
criminal plan for some reason which I

00:31:01,290 --> 00:31:05,370
really don't understand is that it's all

00:31:02,850 --> 00:31:07,770
always lagging severely with its

00:31:05,370 --> 00:31:10,230
capabilities behind the current version

00:31:07,770 --> 00:31:11,670
of the cluster so for example I did

00:31:10,230 --> 00:31:14,280
install for

00:31:11,670 --> 00:31:16,830
a poker on the Linux version 5-0 of

00:31:14,280 --> 00:31:19,380
criminal and I still couldn't pay speak

00:31:16,830 --> 00:31:21,480
to my mimic cluster and live alone

00:31:19,380 --> 00:31:23,370
the newer new tools cluster so you know

00:31:21,480 --> 00:31:25,470
that the client image is kind of useless

00:31:23,370 --> 00:31:27,720
in most of the scenarios unfortunately

00:31:25,470 --> 00:31:30,780
it will be nice if the guys would

00:31:27,720 --> 00:31:34,560
actually keep keep their pace with the

00:31:30,780 --> 00:31:36,720
actual cluster think what you can do you

00:31:34,560 --> 00:31:38,760
can do you can use the RVD NBD utility

00:31:36,720 --> 00:31:40,380
which is part of the self repository so

00:31:38,760 --> 00:31:42,270
if you have installed self you already

00:31:40,380 --> 00:31:45,750
are able to actually install this

00:31:42,270 --> 00:31:48,840
utility which actually uses the NBD

00:31:45,750 --> 00:31:51,510
driver which for some reason is not part

00:31:48,840 --> 00:31:52,920
of the stock centers kernel so you need

00:31:51,510 --> 00:31:54,600
to need to compile the kernel and

00:31:52,920 --> 00:31:56,970
extract the image or you can install l

00:31:54,600 --> 00:31:58,890
wrapper kernel for a cent less which

00:31:56,970 --> 00:32:00,930
does contain the Nvidia driver or you

00:31:58,890 --> 00:32:02,880
can use Ubuntu which is not my personal

00:32:00,930 --> 00:32:06,000
favourites operating system but it does

00:32:02,880 --> 00:32:07,920
get all of this stuff once you do that

00:32:06,000 --> 00:32:10,260
you can simply use you can simply map

00:32:07,920 --> 00:32:13,830
your map your image this is using lip

00:32:10,260 --> 00:32:15,600
RBD Khemu is used in lip bar IBG so a

00:32:13,830 --> 00:32:19,020
lip bar beauty is effectively user space

00:32:15,600 --> 00:32:22,500
client for safe for speaking to our bit

00:32:19,020 --> 00:32:25,230
in sense of our bidi images or others

00:32:22,500 --> 00:32:27,780
both device images or volumes if you

00:32:25,230 --> 00:32:29,220
like that's the thing you will need over

00:32:27,780 --> 00:32:31,800
said to install the stuff and modprobe

00:32:29,220 --> 00:32:33,840
the MVD before that and then do they are

00:32:31,800 --> 00:32:36,600
2d map command if you can if you have

00:32:33,840 --> 00:32:39,090
done the loaded them needed module you

00:32:36,600 --> 00:32:40,050
can also use camel BT which is installed

00:32:39,090 --> 00:32:42,930
by default

00:32:40,050 --> 00:32:48,000
usually when you install KVM and simply

00:32:42,930 --> 00:32:49,590
connect your pool / image s NVT device

00:32:48,000 --> 00:32:52,980
and effectively do this becomes just

00:32:49,590 --> 00:32:54,900
your local raw block storage device that

00:32:52,980 --> 00:32:57,360
you can do whatever you want with it you

00:32:54,900 --> 00:32:58,830
can mount existing images like that even

00:32:57,360 --> 00:33:00,750
if that imagery is already attached to a

00:32:58,830 --> 00:33:02,910
VM so you know if you need to do some

00:33:00,750 --> 00:33:04,800
low-level troubleshooting and you know

00:33:02,910 --> 00:33:08,010
you can do that not really good for

00:33:04,800 --> 00:33:10,560
privacy obviously but yeah that's what

00:33:08,010 --> 00:33:12,000
you can do if you don't you have you

00:33:10,560 --> 00:33:13,610
know in the existing the imagery images

00:33:12,000 --> 00:33:15,840
if you don't have encryption and stuff

00:33:13,610 --> 00:33:17,310
with camera image you can do kevin gene

00:33:15,840 --> 00:33:19,020
for camera image convert which is

00:33:17,310 --> 00:33:20,430
effectively all the copy operations

00:33:19,020 --> 00:33:22,500
between the secondary storage and that

00:33:20,430 --> 00:33:24,270
safe club cluster are done by camera

00:33:22,500 --> 00:33:25,230
image utility which the KVM hosts are

00:33:24,270 --> 00:33:28,020
executing

00:33:25,230 --> 00:33:29,669
one randomly actually let me know that

00:33:28,020 --> 00:33:31,470
name or wine that it depends in

00:33:29,669 --> 00:33:33,120
different situations which carrying

00:33:31,470 --> 00:33:35,280
horse is doing the work but anyway the

00:33:33,120 --> 00:33:37,440
KVM host is the one doing the work not

00:33:35,280 --> 00:33:40,890
the secondary storage for example but in

00:33:37,440 --> 00:33:43,410
case over again short syntax implies you

00:33:40,890 --> 00:33:45,750
have your all your data in self.com

00:33:43,410 --> 00:33:48,330
final authentication key or you can use

00:33:45,750 --> 00:33:50,549
this let's say ugly command you need to

00:33:48,330 --> 00:33:53,880
define your monitor host you need to

00:33:50,549 --> 00:33:55,500
define that def X is the application

00:33:53,880 --> 00:33:58,890
method you need to define your ID which

00:33:55,500 --> 00:34:00,929
is a user and your key this only

00:33:58,890 --> 00:34:02,910
requires that you have the Lib RBD is

00:34:00,929 --> 00:34:04,260
that safe client enabled on that

00:34:02,910 --> 00:34:05,549
particular host when you were running

00:34:04,260 --> 00:34:07,049
the camera image command so you don't

00:34:05,549 --> 00:34:09,179
need to have keyring and you don't need

00:34:07,049 --> 00:34:12,780
to have your safe configuration file you

00:34:09,179 --> 00:34:15,720
know and things like that some

00:34:12,780 --> 00:34:18,510
limitations around I can say it's F or

00:34:15,720 --> 00:34:22,649
actually any when you pass any kind of

00:34:18,510 --> 00:34:24,960
roblox storage to the key movie MKB mbm

00:34:22,649 --> 00:34:26,730
it is not possible to do VM snapshot so

00:34:24,960 --> 00:34:29,310
VM snapshots are only in cave in caving

00:34:26,730 --> 00:34:30,780
world possible with kyouko - so you know

00:34:29,310 --> 00:34:33,030
if you're running NFS or local storage

00:34:30,780 --> 00:34:35,280
or some other kind of file system base

00:34:33,030 --> 00:34:38,340
where you can put your cue code - that's

00:34:35,280 --> 00:34:41,129
only the situation setup where you can

00:34:38,340 --> 00:34:45,929
actually create a VM snapshots but you

00:34:41,129 --> 00:34:48,240
can do volume snapshots as nests with

00:34:45,929 --> 00:34:51,600
any other obviously hypervisor nikkei in

00:34:48,240 --> 00:34:52,980
cloud stack there is no yet there is

00:34:51,600 --> 00:34:55,409
some work being done there or will be

00:34:52,980 --> 00:34:56,990
some work done then there there is no

00:34:55,409 --> 00:34:59,010
support for the storage heartbeat file

00:34:56,990 --> 00:35:01,590
you have this cavium heartbeat

00:34:59,010 --> 00:35:04,830
authorization or XenServer RB dosage

00:35:01,590 --> 00:35:06,390
which will reboot the k vm host assuming

00:35:04,830 --> 00:35:08,730
that your storage is the one which is

00:35:06,390 --> 00:35:11,220
highly available if it cannot access the

00:35:08,730 --> 00:35:17,790
storage so there is some there will be

00:35:11,220 --> 00:35:19,619
some some work done done there also it

00:35:17,790 --> 00:35:21,180
is currently not possible really to

00:35:19,619 --> 00:35:23,100
restore a volume from a snapshot so

00:35:21,180 --> 00:35:24,960
prior to 4:11

00:35:23,100 --> 00:35:27,000
for any kind of storage the only way to

00:35:24,960 --> 00:35:28,410
restore volume snapper snapchat was it

00:35:27,000 --> 00:35:30,300
is to convert the snapshot to a temp I

00:35:28,410 --> 00:35:32,580
can spin a new VM if it's a rooted

00:35:30,300 --> 00:35:34,109
snapshot or over snapshot or data disc

00:35:32,580 --> 00:35:35,940
and attach it wherever you like

00:35:34,109 --> 00:35:37,590
so effectively creating new resources

00:35:35,940 --> 00:35:39,000
which is not really a way to restore

00:35:37,590 --> 00:35:41,130
with your data

00:35:39,000 --> 00:35:42,690
but start to me 411 if you're using NFS

00:35:41,130 --> 00:35:44,460
there is actually a very nice restore

00:35:42,690 --> 00:35:47,670
button and API so you can actually

00:35:44,460 --> 00:35:49,109
revert to that snapshot this does imply

00:35:47,670 --> 00:35:51,060
that you keep your snapshots on your

00:35:49,109 --> 00:35:52,790
primary storage and they're coping over

00:35:51,060 --> 00:35:55,020
as well to the secondary storage

00:35:52,790 --> 00:35:58,500
actually this is implemented I believe

00:35:55,020 --> 00:36:00,630
also 447 but it's being buggy for some

00:35:58,500 --> 00:36:02,570
reason so some of the guys are actually

00:36:00,630 --> 00:36:05,400
taking taking care of this as we speak

00:36:02,570 --> 00:36:09,840
also just to be aware that there are two

00:36:05,400 --> 00:36:12,570
external libraries when using seth with

00:36:09,840 --> 00:36:15,660
the KVM let's say partial it's not

00:36:12,570 --> 00:36:17,220
related to cloud stack so you know with

00:36:15,660 --> 00:36:19,680
your Q code to here or storage you have

00:36:17,220 --> 00:36:21,420
your KVM stuff okay most of liberty and

00:36:19,680 --> 00:36:23,310
you don't care here is also here blue

00:36:21,420 --> 00:36:25,619
bar BD which is kind of odd safe clients

00:36:23,310 --> 00:36:28,170
user space client that Khemu is using to

00:36:25,619 --> 00:36:30,210
actually talk to themself cluster and we

00:36:28,170 --> 00:36:32,550
have s already mentioned specific rather

00:36:30,210 --> 00:36:34,740
Java library which is obviously a tool

00:36:32,550 --> 00:36:37,530
library written in Java to be able to

00:36:34,740 --> 00:36:40,770
manage some snapshots functionalities on

00:36:37,530 --> 00:36:43,200
SEF as required by cloud stack so not

00:36:40,770 --> 00:36:45,180
not actually too many of the limitations

00:36:43,200 --> 00:36:47,550
except and that is actually for some

00:36:45,180 --> 00:36:50,160
users or biglan limitations not being

00:36:47,550 --> 00:36:53,960
able to create VM snapshots learning

00:36:50,160 --> 00:36:56,220
curve can be pretty pretty steep

00:36:53,960 --> 00:36:58,980
depending on you know whether you want

00:36:56,220 --> 00:37:00,810
to do that yourself or not what I would

00:36:58,980 --> 00:37:02,490
definitely you know kind of advise if if

00:37:00,810 --> 00:37:05,339
I'm kind of allowed to give advice is

00:37:02,490 --> 00:37:07,410
make sure that you have proper know-how

00:37:05,339 --> 00:37:08,490
on the safe either in-house or support

00:37:07,410 --> 00:37:10,349
with some of the guys

00:37:08,490 --> 00:37:11,910
god forbid right here because it's very

00:37:10,349 --> 00:37:14,609
expensive but you have some other

00:37:11,910 --> 00:37:16,980
companies which also do provide SEF

00:37:14,609 --> 00:37:19,170
consultants in support because sooner or

00:37:16,980 --> 00:37:20,190
later you might need it and also make

00:37:19,170 --> 00:37:21,630
sure that you have really good

00:37:20,190 --> 00:37:23,040
troubleshooting skills because you know

00:37:21,630 --> 00:37:24,660
everybody can spin a new cluster it's

00:37:23,040 --> 00:37:27,599
very kind of a easy you saw a very

00:37:24,660 --> 00:37:28,890
really simple demo you know but when

00:37:27,599 --> 00:37:30,540
it's comfortable shooting you'll need to

00:37:28,890 --> 00:37:33,030
understand very low level of things I

00:37:30,540 --> 00:37:35,130
know in order to be able to save your

00:37:33,030 --> 00:37:37,080
clients from the downtime in certain

00:37:35,130 --> 00:37:38,910
cases I'm not implying that there is

00:37:37,080 --> 00:37:40,859
always like this it's absolutely not but

00:37:38,910 --> 00:37:43,200
there will be sooner or later some of

00:37:40,859 --> 00:37:44,520
the issue simply it's a very complex and

00:37:43,200 --> 00:37:46,920
distributed storage so you you

00:37:44,520 --> 00:37:49,410
definitely want to be prepared you know

00:37:46,920 --> 00:37:52,829
for that moment also a very specific

00:37:49,410 --> 00:37:54,989
thing which I why personal

00:37:52,829 --> 00:37:57,089
where when I started my itself journey

00:37:54,989 --> 00:37:59,039
like four or five years ago when you

00:37:57,089 --> 00:38:01,259
Rosie goes down or you add new eyes D or

00:37:59,039 --> 00:38:03,900
things like that object replicas will be

00:38:01,259 --> 00:38:05,609
either regenerated as a copy of existing

00:38:03,900 --> 00:38:07,289
replicas or moved away it simply

00:38:05,609 --> 00:38:09,209
migrated away to other Rosie's and stuff

00:38:07,289 --> 00:38:11,609
like that there will be a lot of

00:38:09,209 --> 00:38:13,469
recovery and which is kind of that's the

00:38:11,609 --> 00:38:15,209
name which is used as a recovery traffic

00:38:13,469 --> 00:38:17,519
although maybe you just others new

00:38:15,209 --> 00:38:19,349
drives but still based on the crash

00:38:17,519 --> 00:38:21,209
algorithm staff will say okay here on US

00:38:19,349 --> 00:38:23,640
DS now this setup I don't like it let's

00:38:21,209 --> 00:38:25,979
shuffle some objects around and it will

00:38:23,640 --> 00:38:29,579
cause a lot of objects being not a lot

00:38:25,979 --> 00:38:31,380
but you know some portion of the objects

00:38:29,579 --> 00:38:33,180
being moved around and this is kind of

00:38:31,380 --> 00:38:34,680
nameless the recovery traffic so you

00:38:33,180 --> 00:38:37,259
want to make sure that this recovery

00:38:34,680 --> 00:38:38,940
traffic means of a very law there are

00:38:37,259 --> 00:38:40,709
very low priority in contrast to the

00:38:38,940 --> 00:38:42,630
client so I think because if you have a

00:38:40,709 --> 00:38:44,400
lot of throughput which kills the client

00:38:42,630 --> 00:38:47,009
I or your client I will go ahead you

00:38:44,400 --> 00:38:48,719
will get very slow and you will have a

00:38:47,009 --> 00:38:51,900
you know a lot of complaints from

00:38:48,719 --> 00:38:54,299
customer up to the point that if the i/o

00:38:51,900 --> 00:38:56,219
was not served for up to thirty seconds

00:38:54,299 --> 00:38:59,749
or something that in extreme cases some

00:38:56,219 --> 00:39:02,749
file systems will rebound his suite only

00:38:59,749 --> 00:39:05,519
you know just keep that in mind you know

00:39:02,749 --> 00:39:07,140
our performance consultations it is said

00:39:05,519 --> 00:39:08,699
that safe works on commodity hardware

00:39:07,140 --> 00:39:11,579
and it absolutely does you can spin them

00:39:08,699 --> 00:39:15,059
on the arm knows if you like on your

00:39:11,579 --> 00:39:17,190
laptops if you like on your regular x86

00:39:15,059 --> 00:39:19,049
servers but don't expect miracles unless

00:39:17,190 --> 00:39:21,359
you have a lot of CPU a lot of RAM in a

00:39:19,049 --> 00:39:24,420
fast storage simply if you have filter

00:39:21,359 --> 00:39:25,979
replicas when you all when I first write

00:39:24,420 --> 00:39:27,989
is set itself master for some object

00:39:25,979 --> 00:39:29,759
which is modified it goes to what is

00:39:27,989 --> 00:39:32,369
called a primary OSD let's say a prime

00:39:29,759 --> 00:39:33,900
area says SSD for example and then Steph

00:39:32,369 --> 00:39:35,819
will replicate create additional two

00:39:33,900 --> 00:39:38,940
replicas if we have replicas size of

00:39:35,819 --> 00:39:41,640
three and only when all the three OS

00:39:38,940 --> 00:39:43,529
these or all the free storage devices

00:39:41,640 --> 00:39:45,660
can send the acknowledgment

00:39:43,529 --> 00:39:47,160
only then actually through each other or

00:39:45,660 --> 00:39:49,259
let's say only then the right

00:39:47,160 --> 00:39:51,269
acknowledgement is sent to the client so

00:39:49,259 --> 00:39:53,609
I needs to know very wait for the

00:39:51,269 --> 00:39:55,709
network latency between OS DS for the

00:39:53,609 --> 00:39:57,029
data to be sent data to be written to

00:39:55,709 --> 00:39:59,160
and only then it will get the

00:39:57,029 --> 00:40:01,619
acknowledgment so you know for example

00:39:59,160 --> 00:40:03,569
if you're using nvme devices which some

00:40:01,619 --> 00:40:06,300
of our community clouds that users are

00:40:03,569 --> 00:40:07,830
using you can expect very good Layton

00:40:06,300 --> 00:40:10,200
but this is still like you know five to

00:40:07,830 --> 00:40:13,200
ten times penalty if you compare the

00:40:10,200 --> 00:40:16,500
native nvme latency which goes like like

00:40:13,200 --> 00:40:18,330
20 to 100 microseconds we are realistic

00:40:16,500 --> 00:40:20,670
latency what you can achieve our ladies

00:40:18,330 --> 00:40:23,280
but these are actually extremely good I

00:40:20,670 --> 00:40:25,110
would say for a distributed storage now

00:40:23,280 --> 00:40:27,000
if you are mixing aged hard drives and

00:40:25,110 --> 00:40:28,710
SSD drives this is something I was

00:40:27,000 --> 00:40:31,470
getting in a very small safe class to

00:40:28,710 --> 00:40:34,410
which hard drives use primarily and SSDs

00:40:31,470 --> 00:40:35,850
for the journaling so it is awful if I

00:40:34,410 --> 00:40:38,790
may say like this if you're actually

00:40:35,850 --> 00:40:42,570
expecting performance you will not get a

00:40:38,790 --> 00:40:45,720
lot of performance there also never ever

00:40:42,570 --> 00:40:49,020
ever ever ever ever use consumer SSDs I

00:40:45,720 --> 00:40:50,250
have a story to tell somebody from one

00:40:49,020 --> 00:40:52,410
of my companies where I was managing

00:40:50,250 --> 00:40:55,740
staff master thought to just give us

00:40:52,410 --> 00:40:57,720
certain SSDs pro version of certain

00:40:55,740 --> 00:41:00,390
bender but those were consumer SSDs I

00:40:57,720 --> 00:41:02,880
show a long story short after three

00:41:00,390 --> 00:41:06,630
weeks we had less something like seven

00:41:02,880 --> 00:41:08,700
to ten roughly completely dead SSDs they

00:41:06,630 --> 00:41:11,970
were used only for Journal it was not

00:41:08,700 --> 00:41:13,590
really heavy rig right there were no

00:41:11,970 --> 00:41:15,420
heavy write operations in the cluster

00:41:13,590 --> 00:41:18,240
but simply they were not meant to be

00:41:15,420 --> 00:41:21,180
used in that ways because chef uses all

00:41:18,240 --> 00:41:25,950
these sync flags and some other things

00:41:21,180 --> 00:41:28,260
to directly avoid SSD cache and write

00:41:25,950 --> 00:41:29,970
really to the memory without actually

00:41:28,260 --> 00:41:32,820
writing to the Excel or cache of the SSD

00:41:29,970 --> 00:41:34,200
drive and simply when you do have this -

00:41:32,820 --> 00:41:36,120
cell you see like two megabyte partition

00:41:34,200 --> 00:41:38,280
instead of whatever number of gigabytes

00:41:36,120 --> 00:41:41,310
they weren't completely dead and after

00:41:38,280 --> 00:41:42,930
that they actually some guys listen to

00:41:41,310 --> 00:41:44,580
us and then to end to the proper

00:41:42,930 --> 00:41:46,260
interface models

00:41:44,580 --> 00:41:47,640
interesting thing if you have a too many

00:41:46,260 --> 00:41:49,640
parallel streams which is affected with

00:41:47,640 --> 00:41:52,440
something that's eff manages very well

00:41:49,640 --> 00:41:53,910
when you have many sequential streams

00:41:52,440 --> 00:41:55,470
again which are very good in general

00:41:53,910 --> 00:41:57,780
ending any kind of storage because it

00:41:55,470 --> 00:41:59,910
can handle that properly when they inter

00:41:57,780 --> 00:42:02,850
live at the back and storage you have

00:41:59,910 --> 00:42:05,520
effectively a purely random i/o so again

00:42:02,850 --> 00:42:07,710
forget about hard drives erased from the

00:42:05,520 --> 00:42:10,140
history there are awful devices be nice

00:42:07,710 --> 00:42:13,220
so far but now that we have universities

00:42:10,140 --> 00:42:15,630
and nvme is you know simply avoid them

00:42:13,220 --> 00:42:19,070
something I want to also mention suppose

00:42:15,630 --> 00:42:21,650
so far until maybe two

00:42:19,070 --> 00:42:23,360
three years ago it has been again

00:42:21,650 --> 00:42:25,820
unofficially considered unsuitable for

00:42:23,360 --> 00:42:28,100
any a serious random workload if you dig

00:42:25,820 --> 00:42:30,410
up reference architecture guides from

00:42:28,100 --> 00:42:32,600
different vendors with red heads so from

00:42:30,410 --> 00:42:34,520
quanta computer phonology Dell or

00:42:32,600 --> 00:42:37,250
somebody else you will see all of them a

00:42:34,520 --> 00:42:38,660
lackey lacking there is no random i/o

00:42:37,250 --> 00:42:40,790
benchmarks there are only sequential

00:42:38,660 --> 00:42:42,500
benchmarks and actually I believe was it

00:42:40,790 --> 00:42:43,780
I don't know quanta cloud technology or

00:42:42,500 --> 00:42:45,980
some somebody else who actually

00:42:43,780 --> 00:42:49,100
explicitly written that self is not yet

00:42:45,980 --> 00:42:51,080
considered suitable for random i/o

00:42:49,100 --> 00:42:54,550
workload now this was like three years

00:42:51,080 --> 00:43:00,710
ago and more with older pre luminously

00:42:54,550 --> 00:43:02,900
pre blue store releases and I can vouch

00:43:00,710 --> 00:43:06,560
for that I was in that era if you like

00:43:02,900 --> 00:43:09,560
and and why I can say I had a lot of

00:43:06,560 --> 00:43:11,090
pain at that time but you know things he

00:43:09,560 --> 00:43:12,710
had actually change really really

00:43:11,090 --> 00:43:14,630
seriously in the last couple of years

00:43:12,710 --> 00:43:16,520
especially with a new blue store being

00:43:14,630 --> 00:43:18,200
introduced as the back end Ford actually

00:43:16,520 --> 00:43:20,900
like a storage back in the way how you

00:43:18,200 --> 00:43:23,930
held it several store data on on each

00:43:20,900 --> 00:43:26,990
OSD so effectively now with the MU store

00:43:23,930 --> 00:43:28,280
we are writing directly to a raw block

00:43:26,990 --> 00:43:30,500
device which is effectively row

00:43:28,280 --> 00:43:34,070
partition of itself with only one small

00:43:30,500 --> 00:43:36,470
chunk of that being used for rogue DB

00:43:34,070 --> 00:43:39,230
versus previously used level DB and you

00:43:36,470 --> 00:43:41,090
could theoretically you know still your

00:43:39,230 --> 00:43:42,950
let's say the raw data will be on a

00:43:41,090 --> 00:43:45,890
block or partition let's say SSD or hard

00:43:42,950 --> 00:43:47,540
drive god forbid but you could put your

00:43:45,890 --> 00:43:49,640
block TB which is effectively storing

00:43:47,540 --> 00:43:52,550
just a little bit of needed metadata

00:43:49,640 --> 00:43:54,950
about the OSD or the journal for that

00:43:52,550 --> 00:43:57,380
Rob DP on a different devices so for

00:43:54,950 --> 00:43:59,420
example hard drives here you can put the

00:43:57,380 --> 00:44:02,690
metadata database on

00:43:59,420 --> 00:44:04,730
SSDs and in the journal own on envier

00:44:02,690 --> 00:44:07,820
means for example but again avoid earth

00:44:04,730 --> 00:44:11,060
right now with these recent releases and

00:44:07,820 --> 00:44:13,190
now it became much more possible if you

00:44:11,060 --> 00:44:15,170
like to unleash pretty much full

00:44:13,190 --> 00:44:16,760
performance of the SSDs and enemies

00:44:15,170 --> 00:44:18,740
which definitely you could not do

00:44:16,760 --> 00:44:20,330
previously due to simply software

00:44:18,740 --> 00:44:22,220
constraints and architecture literal

00:44:20,330 --> 00:44:23,990
things a throughput has been increased

00:44:22,220 --> 00:44:25,400
from at minimum forty percent

00:44:23,990 --> 00:44:28,430
realistically forty to two hundred

00:44:25,400 --> 00:44:29,660
percent it's not specific rudders we're

00:44:28,430 --> 00:44:31,790
close it went up to three hundred

00:44:29,660 --> 00:44:32,750
percent which is kind of a crazy in the

00:44:31,790 --> 00:44:34,580
latency which is

00:44:32,750 --> 00:44:36,920
one of the main obvious measurements of

00:44:34,580 --> 00:44:38,510
your performance went down by by those

00:44:36,920 --> 00:44:41,690
numbers when compared to the previous

00:44:38,510 --> 00:44:43,190
older file store let's say like a

00:44:41,690 --> 00:44:45,890
storage back-end on a previous release

00:44:43,190 --> 00:44:47,540
how the data was stored with the file

00:44:45,890 --> 00:44:50,570
system and stuff like that so really

00:44:47,540 --> 00:44:52,040
huge huge performance and this is just

00:44:50,570 --> 00:44:53,600
with the blue store you know a lot of

00:44:52,040 --> 00:44:55,310
other other changes have been done so

00:44:53,600 --> 00:44:57,020
for example we also know here at

00:44:55,310 --> 00:44:58,610
specific memory management pro SD

00:44:57,020 --> 00:45:01,100
because you know the blue store runs

00:44:58,610 --> 00:45:03,080
effectively in the user space so you can

00:45:01,100 --> 00:45:05,240
actually set kind of a target memory

00:45:03,080 --> 00:45:07,880
limit for each OSD while previously it

00:45:05,240 --> 00:45:09,500
was effectively a surface page cache if

00:45:07,880 --> 00:45:10,580
you like in some sense which was managed

00:45:09,500 --> 00:45:12,890
by a kernel there is no way to

00:45:10,580 --> 00:45:14,240
explicitly control the memory how much

00:45:12,890 --> 00:45:16,340
memory arrives they can take especially

00:45:14,240 --> 00:45:18,830
in recovery scenarios which they need

00:45:16,340 --> 00:45:20,840
like double the normal space there is

00:45:18,830 --> 00:45:24,020
data metadata checksums which came with

00:45:20,840 --> 00:45:26,390
a blue store compressions metaphor and

00:45:24,020 --> 00:45:28,730
and snappy and some other things the

00:45:26,390 --> 00:45:30,680
thing is which is not yet there is also

00:45:28,730 --> 00:45:33,040
that still all the data is read only

00:45:30,680 --> 00:45:35,630
from the primary ways these there is no

00:45:33,040 --> 00:45:37,100
it's a question of if this will be

00:45:35,630 --> 00:45:38,810
actually change at any later point in

00:45:37,100 --> 00:45:41,030
time still the day data is only read

00:45:38,810 --> 00:45:42,580
from the primary OSD instead of being

00:45:41,030 --> 00:45:47,690
possibly you know read from other

00:45:42,580 --> 00:45:50,390
replicas so yeah mostly that's that's

00:45:47,690 --> 00:45:51,740
pretty much it though those are some of

00:45:50,390 --> 00:45:54,530
the things that some of them from the

00:45:51,740 --> 00:45:56,960
community experiences and and and you

00:45:54,530 --> 00:46:00,320
know advices if you like best practices

00:45:56,960 --> 00:46:02,240
some of that is from my own positive and

00:46:00,320 --> 00:46:03,710
less positive experience from last years

00:46:02,240 --> 00:46:06,500
ago I'm just not speaking about this

00:46:03,710 --> 00:46:08,680
specific slide but in general what I

00:46:06,500 --> 00:46:11,600
have kind of a shared with you so far

00:46:08,680 --> 00:46:12,110
anyone willing to kind of give itself a

00:46:11,600 --> 00:46:14,900
spin

00:46:12,110 --> 00:46:17,270
we actually have three three part

00:46:14,900 --> 00:46:18,380
monster you know article step by step

00:46:17,270 --> 00:46:21,500
guide which you can pretty much just

00:46:18,380 --> 00:46:23,390
copy paste with more than enough of

00:46:21,500 --> 00:46:26,720
explanation of actual what are you doing

00:46:23,390 --> 00:46:29,750
and what are the key bits you can see so

00:46:26,720 --> 00:46:31,930
yeah that's that's pretty much it for

00:46:29,750 --> 00:46:35,680
this time

00:46:31,930 --> 00:46:35,680
any questions

00:46:48,869 --> 00:47:08,819
multiple is sort of what to me I

00:46:50,649 --> 00:47:08,819
dedicated monitors

00:47:09,209 --> 00:47:14,709
for our business you can imagine that's

00:47:11,589 --> 00:47:16,719
not supported by assumed unity it would

00:47:14,709 --> 00:47:19,179
be possible but again you need to see

00:47:16,719 --> 00:47:21,429
how that works with a lip first it's not

00:47:19,179 --> 00:47:23,979
only you know you we can probably easily

00:47:21,429 --> 00:47:26,529
you know support that in cloud state but

00:47:23,979 --> 00:47:29,409
is it really support by delivered to be

00:47:26,529 --> 00:47:32,439
able because you know for every for

00:47:29,409 --> 00:47:33,939
every monitor IP you create there is

00:47:32,439 --> 00:47:36,489
almost a storage pool created and live

00:47:33,939 --> 00:47:38,559
it and there is also a secret created

00:47:36,489 --> 00:47:44,289
there and so on and so forth so you know

00:47:38,559 --> 00:47:46,839
this would kind of imply additional if

00:47:44,289 --> 00:47:48,369
you change IP for example to another IP

00:47:46,839 --> 00:47:49,749
of the monitor in sense of you know

00:47:48,369 --> 00:47:53,169
supporting it there is an additional

00:47:49,749 --> 00:47:55,599
storage will be being created so I guess

00:47:53,169 --> 00:47:57,669
if it might be doable but yeah for now I

00:47:55,599 --> 00:47:59,199
guess we have a couple of big users

00:47:57,669 --> 00:48:01,269
actually which you guys are probably

00:47:59,199 --> 00:48:04,209
aware in in community which are big safe

00:48:01,269 --> 00:48:07,149
users with cloud stack for some reason

00:48:04,209 --> 00:48:08,939
they didn't consider doing that although

00:48:07,149 --> 00:48:11,589
they did a lot of improvements and fixes

00:48:08,939 --> 00:48:12,969
so I'm kind of thinking again that's

00:48:11,589 --> 00:48:14,529
just my personal thing in the end you

00:48:12,969 --> 00:48:16,179
know round robin is kind of good enough

00:48:14,529 --> 00:48:18,249
because you don't really get your

00:48:16,179 --> 00:48:21,579
monitors nodes to really go dead every

00:48:18,249 --> 00:48:26,319
day but yeah I mean that would be a

00:48:21,579 --> 00:48:28,139
valid a valid improvement for sure any

00:48:26,319 --> 00:48:30,609
more questions

00:48:28,139 --> 00:48:34,949
cool thank you for your attention and

00:48:30,609 --> 00:48:37,239
again I'm just kind of reminding you

00:48:34,949 --> 00:48:39,549
just kinda to remind you for the

00:48:37,239 --> 00:48:41,859
hackathon tomorrow and I'm more on deck

00:48:39,549 --> 00:48:44,049
finally asking anybody willing for from

00:48:41,859 --> 00:48:45,819
the classic users to give us some very

00:48:44,049 --> 00:48:48,939
short interviews just for the actual

00:48:45,819 --> 00:48:52,709
cloud state projects move things even a

00:48:48,939 --> 00:48:52,709

YouTube URL: https://www.youtube.com/watch?v=FwYgzICn-Jk


