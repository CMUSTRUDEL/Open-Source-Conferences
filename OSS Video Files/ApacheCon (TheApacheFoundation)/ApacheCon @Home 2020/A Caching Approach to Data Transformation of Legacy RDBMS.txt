Title: A Caching Approach to Data Transformation of Legacy RDBMS
Publication date: 2020-10-15
Playlist: ApacheCon @Home 2020
Description: 
	A Caching Approach to Data Transformation of Legacy RDBMS
Gregory Green

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

This session will a test driven development approach to building data domains where the source system of record is a legacy Relation Database Management System. The initial code will focus on mainframe based DB2 migration. The pattern will be applicable to similar solutions using Oracle, Sybase and other similar traditional relational databases. The talk will highlight the pros and cons of different styles of data pipelines. For example, Day 0 initial loads, Change Data Capture, Event based streams and scheduled pulled based tasks. The following are the highlighted technologies; - Apache Geode - Spring Data Geode - Spring Data/JDBC - Kakfa - Spring Cloud Stream - Spring Task/Spring Batch - Spring Cloud DataFlow

Senior Consultant with over 23 years of software development and architecture experience. Specializing in application transformation from legacy/monolith systems to microservices cloud-native applications with a focus on scalable, highly available and self-healing cloud-native data platforms.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,800 --> 00:00:28,960
all right so

00:00:25,920 --> 00:00:32,239
it's uh 12 15 eastern standard time

00:00:28,960 --> 00:00:35,520
i wanted to say uh hello everyone um

00:00:32,239 --> 00:00:37,760
my name is gregory green uh it is honor

00:00:35,520 --> 00:00:40,239
and privilege to be with you today

00:00:37,760 --> 00:00:41,200
um i'm really excited and i hope

00:00:40,239 --> 00:00:43,600
everyone is

00:00:41,200 --> 00:00:44,879
is safe i pray everyone is safe that's

00:00:43,600 --> 00:00:47,920
all that's going

00:00:44,879 --> 00:00:51,520
on in the world um i work for

00:00:47,920 --> 00:00:52,800
our vmware pivotal labs and uh today

00:00:51,520 --> 00:00:56,320
we're going to be talking about

00:00:52,800 --> 00:00:58,960
it's a pretty long uh title uh

00:00:56,320 --> 00:01:00,320
but we're gonna basically talk about a

00:00:58,960 --> 00:01:03,600
caching approach to

00:01:00,320 --> 00:01:04,799
data transformation of legacy relational

00:01:03,600 --> 00:01:07,280
databases

00:01:04,799 --> 00:01:09,439
my team is called data transformation

00:01:07,280 --> 00:01:10,479
part of vmware pivotal labs and this is

00:01:09,439 --> 00:01:14,560
what we do

00:01:10,479 --> 00:01:16,560
we focus on the data modernization side

00:01:14,560 --> 00:01:17,680
of app modernization and i wanted to

00:01:16,560 --> 00:01:19,600
start out

00:01:17,680 --> 00:01:21,360
this talk we're talking about a

00:01:19,600 --> 00:01:23,360
particular scenario

00:01:21,360 --> 00:01:25,280
so suppose you have a requirement when

00:01:23,360 --> 00:01:28,479
you want to build some apis

00:01:25,280 --> 00:01:28,960
that access an existing data store uh

00:01:28,479 --> 00:01:31,360
your

00:01:28,960 --> 00:01:32,240
requirements may look something like

00:01:31,360 --> 00:01:35,439
this

00:01:32,240 --> 00:01:39,119
where uh this apis is going to be the

00:01:35,439 --> 00:01:42,159
backend for some sort of a ui

00:01:39,119 --> 00:01:43,280
facing applications so the user

00:01:42,159 --> 00:01:46,399
experience

00:01:43,280 --> 00:01:48,000
is uh is critical so it has to be as

00:01:46,399 --> 00:01:50,560
fast as possible

00:01:48,000 --> 00:01:52,880
and the data may be some in some

00:01:50,560 --> 00:01:55,520
existing system or record or some sort

00:01:52,880 --> 00:01:57,439
of legacy relational database

00:01:55,520 --> 00:01:59,360
so you might start out building your

00:01:57,439 --> 00:02:01,840
microservice based apps that you can

00:01:59,360 --> 00:02:05,040
scale out multiple instances of them

00:02:01,840 --> 00:02:06,479
but for the first phase you might leave

00:02:05,040 --> 00:02:09,759
the existing database

00:02:06,479 --> 00:02:13,920
or data source as is and establish a

00:02:09,759 --> 00:02:15,520
direct connection to those data sources

00:02:13,920 --> 00:02:17,520
so i'm going to show you kind of what

00:02:15,520 --> 00:02:20,239
that would look like so let's

00:02:17,520 --> 00:02:22,640
the legacy database that's going to

00:02:20,239 --> 00:02:25,920
represent this particular demo

00:02:22,640 --> 00:02:28,800
is we're going to use ibm's db2

00:02:25,920 --> 00:02:30,160
and whenever you're talking about uh

00:02:28,800 --> 00:02:32,080
modernization

00:02:30,160 --> 00:02:34,000
you start to you try to start to

00:02:32,080 --> 00:02:37,680
identify the domains

00:02:34,000 --> 00:02:40,160
or the entities uh that would uh

00:02:37,680 --> 00:02:41,599
that will start building features around

00:02:40,160 --> 00:02:44,480
but this particular

00:02:41,599 --> 00:02:45,120
domain our uh for this demo we're going

00:02:44,480 --> 00:02:48,879
to use

00:02:45,120 --> 00:02:50,239
the account as our domain contacts

00:02:48,879 --> 00:02:52,800
and we're going to introduce this

00:02:50,239 --> 00:02:55,599
concept of a repository

00:02:52,800 --> 00:02:57,440
uh think about a repository as sort of

00:02:55,599 --> 00:02:58,319
sort of it's an interface at the end of

00:02:57,440 --> 00:03:00,159
the day

00:02:58,319 --> 00:03:01,599
and it's called different things in

00:03:00,159 --> 00:03:04,319
different languages

00:03:01,599 --> 00:03:05,040
but it provides the abstraction to do

00:03:04,319 --> 00:03:07,200
all of the

00:03:05,040 --> 00:03:08,319
write and read operations within the

00:03:07,200 --> 00:03:10,239
data store

00:03:08,319 --> 00:03:11,519
we're going to have an implementation of

00:03:10,239 --> 00:03:16,080
this interface that

00:03:11,519 --> 00:03:16,800
uses the java database connectivity api

00:03:16,080 --> 00:03:19,120
jdbc

00:03:16,800 --> 00:03:20,800
so we're going to build a solution based

00:03:19,120 --> 00:03:22,640
on the java jvm

00:03:20,800 --> 00:03:25,360
so we're going to focus on java this

00:03:22,640 --> 00:03:28,879
could easily be replaced with any

00:03:25,360 --> 00:03:30,560
a jvm based um a language

00:03:28,879 --> 00:03:32,640
we're going to wire in we have a simple

00:03:30,560 --> 00:03:35,680
way to wire in the implementation

00:03:32,640 --> 00:03:39,040
of the repository and uh in

00:03:35,680 --> 00:03:40,400
a cloud native microservice based

00:03:39,040 --> 00:03:44,000
fashion we're going to have

00:03:40,400 --> 00:03:46,959
an embedded web server in this case

00:03:44,000 --> 00:03:48,480
jetty to service our interfaces which is

00:03:46,959 --> 00:03:50,799
going to be hdp

00:03:48,480 --> 00:03:53,519
so typically when we were building this

00:03:50,799 --> 00:03:55,920
i i would typically build this in spring

00:03:53,519 --> 00:03:58,720
but we wanted to show these principles

00:03:55,920 --> 00:04:01,040
um in a non-spring example just so

00:03:58,720 --> 00:04:02,640
just to show you that it's not required

00:04:01,040 --> 00:04:03,840
to use spring

00:04:02,640 --> 00:04:06,319
so we're going to wire that the

00:04:03,840 --> 00:04:09,760
application to be connected to our

00:04:06,319 --> 00:04:12,480
db2 db2 database and

00:04:09,760 --> 00:04:13,519
hook up postman so we can test the

00:04:12,480 --> 00:04:16,160
interface

00:04:13,519 --> 00:04:17,280
and then we're going to look at jmeter

00:04:16,160 --> 00:04:20,000
just to figure out

00:04:17,280 --> 00:04:21,280
how what sort of performance or response

00:04:20,000 --> 00:04:24,560
times we're getting

00:04:21,280 --> 00:04:28,000
from our uh rest app

00:04:24,560 --> 00:04:31,520
also with that being said uh

00:04:28,000 --> 00:04:34,320
let's jump to the uh to the application

00:04:31,520 --> 00:04:35,600
actually what i'm gonna do is uh we're

00:04:34,320 --> 00:04:38,960
going to

00:04:35,600 --> 00:04:41,280
um open up my readme

00:04:38,960 --> 00:04:42,479
so in my readme basically i need to

00:04:41,280 --> 00:04:45,919
provide

00:04:42,479 --> 00:04:49,040
the connection details of of the

00:04:45,919 --> 00:04:53,360
of the app how do i connect uh to

00:04:49,040 --> 00:04:55,600
the database and once that is

00:04:53,360 --> 00:04:57,680
set up i also have an environment

00:04:55,600 --> 00:04:59,520
variable that tells me okay which

00:04:57,680 --> 00:05:01,680
instance of the repository i'm going to

00:04:59,520 --> 00:05:03,840
use so i'm going to use the jdbc

00:05:01,680 --> 00:05:05,919
repository and then i'll start up the

00:05:03,840 --> 00:05:08,160
app and it's a uber jar

00:05:05,919 --> 00:05:09,840
so all of the dependencies or contents

00:05:08,160 --> 00:05:13,280
self

00:05:09,840 --> 00:05:15,280
within the uber jar so now it's up and

00:05:13,280 --> 00:05:18,960
running

00:05:15,280 --> 00:05:21,199
and let's open up postman

00:05:18,960 --> 00:05:23,680
so for postman what we're going to do is

00:05:21,199 --> 00:05:27,440
we're going to

00:05:23,680 --> 00:05:29,360
write a record using a post operation

00:05:27,440 --> 00:05:31,600
so we wrote that record and then we'll

00:05:29,360 --> 00:05:32,800
be able to read that record just by

00:05:31,600 --> 00:05:36,400
specifying

00:05:32,800 --> 00:05:38,639
uh the id that we use to write so

00:05:36,400 --> 00:05:40,400
this account objects very simple we just

00:05:38,639 --> 00:05:41,919
have three fields

00:05:40,400 --> 00:05:43,440
and actually that's actually a good

00:05:41,919 --> 00:05:45,520
principle whenever you're doing

00:05:43,440 --> 00:05:47,280
sort of uh data modernization you start

00:05:45,520 --> 00:05:49,680
out with the minimum number of fields

00:05:47,280 --> 00:05:51,759
and just build on top of that

00:05:49,680 --> 00:05:54,080
i can look at the data that's actually

00:05:51,759 --> 00:05:55,840
in my database so in this case i'm using

00:05:54,080 --> 00:05:58,000
dbeaver

00:05:55,840 --> 00:05:59,680
so it's using jdbc to access the

00:05:58,000 --> 00:06:02,000
database i can execute my

00:05:59,680 --> 00:06:02,880
my queries directly against it so in

00:06:02,000 --> 00:06:06,720
this case i

00:06:02,880 --> 00:06:07,680
see the data that i okay so we did a

00:06:06,720 --> 00:06:10,720
basic

00:06:07,680 --> 00:06:14,960
read write operation so now

00:06:10,720 --> 00:06:18,400
uh let's uh open up our j meter

00:06:14,960 --> 00:06:21,440
and then in jmeter what we'll do is

00:06:18,400 --> 00:06:24,720
uh we're going to basically simulate

00:06:21,440 --> 00:06:27,759
a single user execute in the

00:06:24,720 --> 00:06:30,880
save read update operations

00:06:27,759 --> 00:06:33,039
it's going to do this uh 2000 times

00:06:30,880 --> 00:06:34,160
and again we're just trying to get some

00:06:33,039 --> 00:06:36,080
baseline numbers

00:06:34,160 --> 00:06:37,600
on the performance and it's going to

00:06:36,080 --> 00:06:40,560
write the report of the

00:06:37,600 --> 00:06:42,000
average minimum max response time in

00:06:40,560 --> 00:06:44,400
this tab

00:06:42,000 --> 00:06:46,240
so as i'm starting this up uh you see

00:06:44,400 --> 00:06:51,120
the response times

00:06:46,240 --> 00:06:53,599
um so about 11 seconds four seconds

00:06:51,120 --> 00:06:56,160
um we can see the app running in the

00:06:53,599 --> 00:06:59,199
background so it's doing the

00:06:56,160 --> 00:07:01,360
uh the select and the so the

00:06:59,199 --> 00:07:02,479
the the all the operations at to

00:07:01,360 --> 00:07:05,840
interface or

00:07:02,479 --> 00:07:07,599
access this data within the database

00:07:05,840 --> 00:07:09,039
while this is running let's look at the

00:07:07,599 --> 00:07:12,080
code

00:07:09,039 --> 00:07:15,599
so in the code we have our

00:07:12,080 --> 00:07:17,840
repository uh let me expand this

00:07:15,599 --> 00:07:17,840
so

00:07:22,319 --> 00:07:25,599
all right so here's our repository so in

00:07:25,120 --> 00:07:28,639
this

00:07:25,599 --> 00:07:31,199
repository again we're doing the uh the

00:07:28,639 --> 00:07:33,599
current operations we create read

00:07:31,199 --> 00:07:34,240
update delete operations starting with

00:07:33,599 --> 00:07:36,479
our domain

00:07:34,240 --> 00:07:37,840
object and we are executing we're

00:07:36,479 --> 00:07:41,039
building our queries

00:07:37,840 --> 00:07:44,319
and we're using the jdbc api

00:07:41,039 --> 00:07:47,440
to map our our domain object

00:07:44,319 --> 00:07:49,440
data to the jdbc statements and

00:07:47,440 --> 00:07:52,639
executing them

00:07:49,440 --> 00:07:54,960
of course we are doing a tdd so

00:07:52,639 --> 00:07:57,199
i can look at the tests for this

00:07:54,960 --> 00:08:00,080
repository

00:07:57,199 --> 00:08:01,120
for tests driven development the key to

00:08:00,080 --> 00:08:03,919
building out uh

00:08:01,120 --> 00:08:04,560
these tests these unit tests is to be

00:08:03,919 --> 00:08:07,599
able to

00:08:04,560 --> 00:08:10,560
mock everything that the database needs

00:08:07,599 --> 00:08:12,479
so that i can build my tests of how the

00:08:10,560 --> 00:08:13,599
database is accessed without actually

00:08:12,479 --> 00:08:16,560
having to stand up

00:08:13,599 --> 00:08:18,160
a database so i mark my connections my

00:08:16,560 --> 00:08:21,120
prepared statements

00:08:18,160 --> 00:08:22,400
and then i exit i basically perform my

00:08:21,120 --> 00:08:26,479
operations when i'm

00:08:22,400 --> 00:08:28,800
testing for the create operation i

00:08:26,479 --> 00:08:30,080
i not only check the outputs that i'm

00:08:28,800 --> 00:08:33,279
getting but i've also

00:08:30,080 --> 00:08:36,719
tested them using the jb jdbc

00:08:33,279 --> 00:08:36,719
apis as expected

00:08:37,360 --> 00:08:44,159
okay now let's jump back to

00:08:40,880 --> 00:08:46,720
uh the performance results

00:08:44,159 --> 00:08:48,480
all right so so you see that we get

00:08:46,720 --> 00:08:51,200
about four to five seconds

00:08:48,480 --> 00:08:52,640
our response times for those operations

00:08:51,200 --> 00:08:54,800
uh let's clear this out

00:08:52,640 --> 00:08:57,120
and let's run this again just to see how

00:08:54,800 --> 00:09:02,160
consistent the results are

00:08:57,120 --> 00:09:02,160
and let's continue in the demo

00:09:05,760 --> 00:09:09,839
all right so i want to highlight some of

00:09:08,080 --> 00:09:13,120
the potential pain points with

00:09:09,839 --> 00:09:15,440
uh with this uh scenario right so the

00:09:13,120 --> 00:09:16,720
the potential pain points is that even

00:09:15,440 --> 00:09:19,040
though we're getting at about

00:09:16,720 --> 00:09:19,760
you know five seconds uh five

00:09:19,040 --> 00:09:22,399
milliseconds

00:09:19,760 --> 00:09:24,000
i should say response time uh once you

00:09:22,399 --> 00:09:24,720
start scaling out the number of

00:09:24,000 --> 00:09:27,440
instances

00:09:24,720 --> 00:09:28,480
number of requests or requests uh that

00:09:27,440 --> 00:09:31,440
could easily be

00:09:28,480 --> 00:09:33,680
a slow right so you can easily move from

00:09:31,440 --> 00:09:36,560
milliseconds to a couple of seconds

00:09:33,680 --> 00:09:38,320
easily with traditional uh relational

00:09:36,560 --> 00:09:41,519
databases

00:09:38,320 --> 00:09:44,240
also these legacy databases they're

00:09:41,519 --> 00:09:45,760
generally hard to scale right they just

00:09:44,240 --> 00:09:47,360
want to build the scale there may be

00:09:45,760 --> 00:09:49,440
some replication built in

00:09:47,360 --> 00:09:50,560
for disaster recovery but they are not

00:09:49,440 --> 00:09:54,399
able to scale as

00:09:50,560 --> 00:09:57,680
easily as your micro service based apps

00:09:54,399 --> 00:09:58,800
and also there may be besides our new

00:09:57,680 --> 00:10:01,279
apis

00:09:58,800 --> 00:10:02,800
there may be tons of existing

00:10:01,279 --> 00:10:05,760
applications

00:10:02,800 --> 00:10:06,640
that are using this existing database

00:10:05,760 --> 00:10:09,200
and it might be

00:10:06,640 --> 00:10:10,160
providing some critical business need

00:10:09,200 --> 00:10:12,800
maybe it's the

00:10:10,160 --> 00:10:13,440
backend for multiple commercial products

00:10:12,800 --> 00:10:16,399
so

00:10:13,440 --> 00:10:17,200
for so from a stakeholder point of view

00:10:16,399 --> 00:10:19,519
it may be

00:10:17,200 --> 00:10:20,399
out of the question to introduce any

00:10:19,519 --> 00:10:23,040
sort of

00:10:20,399 --> 00:10:25,360
performance instability into these

00:10:23,040 --> 00:10:28,000
existing databases and also

00:10:25,360 --> 00:10:29,839
because you have all these dependencies

00:10:28,000 --> 00:10:33,120
it may be hard to change

00:10:29,839 --> 00:10:36,399
those databases to meet any uh of new

00:10:33,120 --> 00:10:39,600
requirements that you have for your api

00:10:36,399 --> 00:10:41,680
so how do you address this well one way

00:10:39,600 --> 00:10:43,839
you can address this is to introduce a

00:10:41,680 --> 00:10:47,839
cache so thank you wikipedia

00:10:43,839 --> 00:10:50,079
and the idea of a cache is a data store

00:10:47,839 --> 00:10:51,760
that allows you to store our future

00:10:50,079 --> 00:10:52,720
requests in your data so that data can

00:10:51,760 --> 00:10:55,920
be served

00:10:52,720 --> 00:10:57,760
faster it also acts as a buffer layer

00:10:55,920 --> 00:10:59,760
for some of those risks

00:10:57,760 --> 00:11:01,760
that we talked about right so that we

00:10:59,760 --> 00:11:03,920
can reduce the risk

00:11:01,760 --> 00:11:05,680
from introducing sort of performance

00:11:03,920 --> 00:11:07,440
issues in the database by using the

00:11:05,680 --> 00:11:10,480
cache and also we can

00:11:07,440 --> 00:11:14,320
allow to a change or evolve our cache

00:11:10,480 --> 00:11:17,519
with our uh updated microservice uh

00:11:14,320 --> 00:11:19,440
application api so

00:11:17,519 --> 00:11:21,040
the conceptual architecture would look

00:11:19,440 --> 00:11:22,240
something like this so if you have a

00:11:21,040 --> 00:11:24,959
cloud native based

00:11:22,240 --> 00:11:25,519
uh platform if you want to deploy that

00:11:24,959 --> 00:11:28,480
into

00:11:25,519 --> 00:11:30,320
any one of these public clouds or if you

00:11:28,480 --> 00:11:30,959
want to use some sort of paths or

00:11:30,320 --> 00:11:34,240
platform as

00:11:30,959 --> 00:11:37,519
a service um framework like

00:11:34,240 --> 00:11:39,680
cloud foundry or kubernetes

00:11:37,519 --> 00:11:40,560
of course i work for vmware and we have

00:11:39,680 --> 00:11:43,760
our tanzu

00:11:40,560 --> 00:11:46,480
offering which provides a flavor of both

00:11:43,760 --> 00:11:48,079
cloud foundry and kubernetes

00:11:46,480 --> 00:11:50,320
so that when you're building your

00:11:48,079 --> 00:11:53,680
application api layer

00:11:50,320 --> 00:11:55,920
you can scale up your number of apps and

00:11:53,680 --> 00:11:56,880
instead of having those apps directly

00:11:55,920 --> 00:11:59,200
connect to

00:11:56,880 --> 00:12:01,519
the legacy data store you introduce some

00:11:59,200 --> 00:12:04,639
sort of data cache layer

00:12:01,519 --> 00:12:08,240
and in your data cache layer you can

00:12:04,639 --> 00:12:11,279
scale up the number of data nodes or the

00:12:08,240 --> 00:12:12,079
the processes that handle and manage the

00:12:11,279 --> 00:12:14,639
data

00:12:12,079 --> 00:12:16,000
just as easily as you can the number of

00:12:14,639 --> 00:12:20,079
apps

00:12:16,000 --> 00:12:23,120
and you basically you you design

00:12:20,079 --> 00:12:24,320
a way to synchronize that data from your

00:12:23,120 --> 00:12:27,279
legacy data source

00:12:24,320 --> 00:12:28,800
into your data cache all right and we're

00:12:27,279 --> 00:12:32,320
going to show you how to do that

00:12:28,800 --> 00:12:32,320
using apache geo

00:12:32,480 --> 00:12:37,279
all right so but what is a apache geo

00:12:35,360 --> 00:12:39,600
well i'm glad you asked

00:12:37,279 --> 00:12:40,480
it's basically an in-memory data grid it

00:12:39,600 --> 00:12:43,200
can also

00:12:40,480 --> 00:12:46,160
fall under the category of a node sql

00:12:43,200 --> 00:12:49,200
but it supports sql-like operations

00:12:46,160 --> 00:12:52,399
uh you may of have also heard of the

00:12:49,200 --> 00:12:54,000
product called jim fire so gen fire is

00:12:52,399 --> 00:12:57,680
the commercial version

00:12:54,000 --> 00:13:01,120
of apache geo right um and

00:12:57,680 --> 00:13:03,279
in general geo is very fast well

00:13:01,120 --> 00:13:05,920
the reason why it's so fast is because

00:13:03,279 --> 00:13:07,680
it accesses all of the data and memory

00:13:05,920 --> 00:13:10,720
so we know that uh

00:13:07,680 --> 00:13:11,600
memory access is much faster ram based

00:13:10,720 --> 00:13:14,639
access is much

00:13:11,600 --> 00:13:17,920
faster than disk space access so

00:13:14,639 --> 00:13:21,200
you can highly optimize your read write

00:13:17,920 --> 00:13:22,079
operations and then many times we've

00:13:21,200 --> 00:13:25,120
seen

00:13:22,079 --> 00:13:27,200
a 10 to 100 times greater performance

00:13:25,120 --> 00:13:30,480
improvement just by introducing

00:13:27,200 --> 00:13:32,800
something like apache deal

00:13:30,480 --> 00:13:34,320
it's very scalable in that you can scale

00:13:32,800 --> 00:13:37,600
up the number of instances

00:13:34,320 --> 00:13:37,920
it's horizontally scalable over a local

00:13:37,600 --> 00:13:41,040
or

00:13:37,920 --> 00:13:43,040
a wide area network

00:13:41,040 --> 00:13:44,639
all right some additional fundamentals

00:13:43,040 --> 00:13:47,519
are sort of one-on-one

00:13:44,639 --> 00:13:48,160
things about geo you have these core

00:13:47,519 --> 00:13:50,639
components

00:13:48,160 --> 00:13:52,720
you have your cache server that's the

00:13:50,639 --> 00:13:55,600
where that's where all the that's the

00:13:52,720 --> 00:13:57,279
um the same as the data node and then

00:13:55,600 --> 00:13:59,360
you have these locators

00:13:57,279 --> 00:14:02,079
you can think of them as the controller

00:13:59,360 --> 00:14:02,959
for clients and the cache servers so

00:14:02,079 --> 00:14:04,800
when you're starting

00:14:02,959 --> 00:14:06,480
up a geo you would normally start your

00:14:04,800 --> 00:14:07,760
locator

00:14:06,480 --> 00:14:09,600
then you would start up your cache

00:14:07,760 --> 00:14:12,399
server your cache server would

00:14:09,600 --> 00:14:13,920
register itself with the locator and the

00:14:12,399 --> 00:14:17,120
notion of connected

00:14:13,920 --> 00:14:19,279
locators and cache servers is becomes

00:14:17,120 --> 00:14:21,120
your cluster

00:14:19,279 --> 00:14:23,040
um and then when you're connecting the

00:14:21,120 --> 00:14:25,920
client so clients are generally

00:14:23,040 --> 00:14:27,839
provided with the locator details and

00:14:25,920 --> 00:14:28,639
then the locator provides them with the

00:14:27,839 --> 00:14:31,040
cash

00:14:28,639 --> 00:14:32,560
server so a client can be connected to

00:14:31,040 --> 00:14:35,839
any one of these

00:14:32,560 --> 00:14:38,399
members within the cluster

00:14:35,839 --> 00:14:39,199
and you can add as many cache servers as

00:14:38,399 --> 00:14:41,279
needed

00:14:39,199 --> 00:14:43,040
to as you have more and more data just

00:14:41,279 --> 00:14:45,279
add more and more cash service

00:14:43,040 --> 00:14:46,160
and they would again follow the same

00:14:45,279 --> 00:14:47,839
process

00:14:46,160 --> 00:14:49,839
of registering themselves with the

00:14:47,839 --> 00:14:52,000
locators and discovering

00:14:49,839 --> 00:14:54,240
other cash servers that are part of the

00:14:52,000 --> 00:14:57,440
cluster

00:14:54,240 --> 00:14:59,680
all right so in terms of the clients um

00:14:57,440 --> 00:15:01,600
the libraries or the programming

00:14:59,680 --> 00:15:05,040
languages that are supported

00:15:01,600 --> 00:15:06,639
you have java you have a node.js and

00:15:05,040 --> 00:15:10,240
there's a separate talk on the

00:15:06,639 --> 00:15:13,519
node.js interface or do

00:15:10,240 --> 00:15:14,720
you have c plus and you have a c sharp

00:15:13,519 --> 00:15:18,079
which can use

00:15:14,720 --> 00:15:20,079
the c plus native drivers or you can

00:15:18,079 --> 00:15:22,480
utilize the rest apis

00:15:20,079 --> 00:15:23,760
so any language which is just about

00:15:22,480 --> 00:15:26,000
every language right

00:15:23,760 --> 00:15:27,600
i can support anything that can support

00:15:26,000 --> 00:15:30,880
uh the rest api

00:15:27,600 --> 00:15:31,839
can support uh apache uh geo interface

00:15:30,880 --> 00:15:34,639
sorry about that

00:15:31,839 --> 00:15:35,680
should have muted my phone all right and

00:15:34,639 --> 00:15:38,560
then you have the

00:15:35,680 --> 00:15:40,160
uh the rest apis right so so uh sorry

00:15:38,560 --> 00:15:43,680
and then you have uh

00:15:40,160 --> 00:15:44,240
reddit's so any application any of your

00:15:43,680 --> 00:15:47,680
arrest

00:15:44,240 --> 00:15:51,279
clients uh geo supports the

00:15:47,680 --> 00:15:54,320
the redis apis so your existing redis

00:15:51,279 --> 00:15:57,600
clients can talk to a apache geo

00:15:54,320 --> 00:15:59,920
cluster and then you can support

00:15:57,600 --> 00:16:02,320
a ram replication right so you can have

00:15:59,920 --> 00:16:04,959
data replicated across data centers

00:16:02,320 --> 00:16:07,120
right so if you have one data center on

00:16:04,959 --> 00:16:09,839
the east coast and another data

00:16:07,120 --> 00:16:10,800
center on the west coast you can have

00:16:09,839 --> 00:16:14,320
that data

00:16:10,800 --> 00:16:16,480
a bit replicated in a near real-time

00:16:14,320 --> 00:16:18,240
fashion so that the data becomes

00:16:16,480 --> 00:16:20,720
eventually consistent

00:16:18,240 --> 00:16:23,199
so that supports your active active data

00:16:20,720 --> 00:16:26,959
access or active passive for disaster

00:16:23,199 --> 00:16:26,959
recovery across the incentives

00:16:27,040 --> 00:16:32,079
all right so now you have um

00:16:30,160 --> 00:16:34,320
another thing we need to talk about just

00:16:32,079 --> 00:16:37,199
continuing on the fundamentals

00:16:34,320 --> 00:16:37,839
is the idea of a region so a gem fire

00:16:37,199 --> 00:16:40,320
region

00:16:37,839 --> 00:16:41,279
is similar to a relational database

00:16:40,320 --> 00:16:43,600
table

00:16:41,279 --> 00:16:45,600
where you can access the data by a key

00:16:43,600 --> 00:16:47,440
to get the value so it's a key value

00:16:45,600 --> 00:16:49,360
pair a data structure

00:16:47,440 --> 00:16:50,560
so you can get your data out very fast

00:16:49,360 --> 00:16:53,360
as long as you know the

00:16:50,560 --> 00:16:54,639
the key of your of your entries within

00:16:53,360 --> 00:16:58,240
your region

00:16:54,639 --> 00:16:58,880
but you can also perform um a sql like

00:16:58,240 --> 00:17:01,680
operation

00:16:58,880 --> 00:17:03,040
so for example if i want to do a select

00:17:01,680 --> 00:17:06,400
where i want to see

00:17:03,040 --> 00:17:09,360
all of the states that have the key um

00:17:06,400 --> 00:17:09,839
in new york or la again that's uh it

00:17:09,360 --> 00:17:12,000
looks

00:17:09,839 --> 00:17:12,959
very much like sequel right just a

00:17:12,000 --> 00:17:15,360
little bit different

00:17:12,959 --> 00:17:16,400
right you could do that in apache geo it

00:17:15,360 --> 00:17:18,880
also supports

00:17:16,400 --> 00:17:19,919
apache leasing releasing embedded with

00:17:18,880 --> 00:17:22,240
ngo

00:17:19,919 --> 00:17:24,480
so this allows you to do text based

00:17:22,240 --> 00:17:26,640
searches so think about google like

00:17:24,480 --> 00:17:27,679
searches right so for example if i

00:17:26,640 --> 00:17:30,080
wanted to see

00:17:27,679 --> 00:17:31,360
any state that has i don't know maybe

00:17:30,080 --> 00:17:35,280
the word new in it

00:17:31,360 --> 00:17:37,200
i could do that easily with geo

00:17:35,280 --> 00:17:40,000
you have events so when you think about

00:17:37,200 --> 00:17:41,600
events they are very similar to triggers

00:17:40,000 --> 00:17:43,360
in a relational database

00:17:41,600 --> 00:17:44,720
so that you can have a piece of code

00:17:43,360 --> 00:17:47,200
that is alerted whenever

00:17:44,720 --> 00:17:49,280
changes happen to the data and then you

00:17:47,200 --> 00:17:50,080
also have a special kind of way to

00:17:49,280 --> 00:17:52,400
register

00:17:50,080 --> 00:17:54,080
the uh your listeners which is called a

00:17:52,400 --> 00:17:54,799
continuous query which i think is pretty

00:17:54,080 --> 00:17:58,080
cool

00:17:54,799 --> 00:17:58,960
so for example suppose you have a tweets

00:17:58,080 --> 00:18:01,520
region

00:17:58,960 --> 00:18:04,240
and you want to be alerted anytime

00:18:01,520 --> 00:18:07,440
there's any sort of

00:18:04,240 --> 00:18:09,039
update create or delete of a tweet uh

00:18:07,440 --> 00:18:11,200
you can do that very easily by

00:18:09,039 --> 00:18:14,320
registering this query select star from

00:18:11,200 --> 00:18:17,440
tweets for example uh

00:18:14,320 --> 00:18:19,600
geo supports transactions so if you are

00:18:17,440 --> 00:18:21,600
updating multiple regions suppose you

00:18:19,600 --> 00:18:22,400
want to update the states in the toots

00:18:21,600 --> 00:18:24,640
region

00:18:22,400 --> 00:18:26,080
with a single unit of work and have them

00:18:24,640 --> 00:18:28,960
committed or roll back

00:18:26,080 --> 00:18:30,559
all together uh you can do that with geo

00:18:28,960 --> 00:18:33,360
and in that way

00:18:30,559 --> 00:18:35,200
you can the use case for a geo is not

00:18:33,360 --> 00:18:38,480
just limited to a cache you can actually

00:18:35,200 --> 00:18:41,120
use it for an operational data store

00:18:38,480 --> 00:18:42,000
and then you have data policies that are

00:18:41,120 --> 00:18:44,080
configured with

00:18:42,000 --> 00:18:45,360
the region so you have this idea of a

00:18:44,080 --> 00:18:47,520
replicated region

00:18:45,360 --> 00:18:48,799
uh the replicated region is basically

00:18:47,520 --> 00:18:51,039
each uh

00:18:48,799 --> 00:18:53,760
cache server has an exact copy of the

00:18:51,039 --> 00:18:56,160
data so if i have two of these cases

00:18:53,760 --> 00:18:57,760
with two records each cache server would

00:18:56,160 --> 00:19:00,000
have the copy of the data

00:18:57,760 --> 00:19:01,440
replicated readers are really used for

00:19:00,000 --> 00:19:03,440
smaller data sets

00:19:01,440 --> 00:19:04,880
where you want to be able to i don't

00:19:03,440 --> 00:19:07,600
know for example do

00:19:04,880 --> 00:19:09,840
uh joins across multiple tables and you

00:19:07,600 --> 00:19:13,440
want to make sure all of that data is

00:19:09,840 --> 00:19:15,360
co-located within one cache server

00:19:13,440 --> 00:19:16,960
uh one jvm you can do that with

00:19:15,360 --> 00:19:20,480
replicated regions

00:19:16,960 --> 00:19:21,679
but for larger data sets for something

00:19:20,480 --> 00:19:23,360
like tweets

00:19:21,679 --> 00:19:25,760
you probably want to look at partition

00:19:23,360 --> 00:19:27,440
regions where each cache server only

00:19:25,760 --> 00:19:29,679
stores pieces of the data

00:19:27,440 --> 00:19:30,640
so if you have more and more tweets you

00:19:29,679 --> 00:19:33,760
can add more

00:19:30,640 --> 00:19:37,360
or more cache service to manage and

00:19:33,760 --> 00:19:39,360
store those tweets right

00:19:37,360 --> 00:19:40,640
all right so we're talking more uh

00:19:39,360 --> 00:19:44,320
enough about uh

00:19:40,640 --> 00:19:47,600
the how-to now let's show you a demo so

00:19:44,320 --> 00:19:50,799
in this demo i have a local uh geode

00:19:47,600 --> 00:19:53,280
cluster i only have i have one locator

00:19:50,799 --> 00:19:55,120
and one cache server and we're going to

00:19:53,280 --> 00:19:57,600
use gfish which is basically

00:19:55,120 --> 00:19:58,480
a command line interface to look at and

00:19:57,600 --> 00:20:00,960
manage

00:19:58,480 --> 00:20:03,200
our uh gemfire cluster and then we're

00:20:00,960 --> 00:20:03,840
going to use postman and j meter to look

00:20:03,200 --> 00:20:05,760
at the

00:20:03,840 --> 00:20:08,559
uh how to access it and what's the

00:20:05,760 --> 00:20:11,520
performance look like

00:20:08,559 --> 00:20:12,559
all right so let me jump back uh to my

00:20:11,520 --> 00:20:16,000
app

00:20:12,559 --> 00:20:18,799
and i'm going to clear uh this

00:20:16,000 --> 00:20:18,799
app um

00:20:19,120 --> 00:20:25,760
okay all right

00:20:22,240 --> 00:20:28,799
and now let me look at my readme

00:20:25,760 --> 00:20:31,200
if i go back to my readme i have a way

00:20:28,799 --> 00:20:35,200
to configure my app now

00:20:31,200 --> 00:20:37,120
to instead of uh using the jdbc

00:20:35,200 --> 00:20:40,640
repository we're going to use

00:20:37,120 --> 00:20:40,640
a geo repository

00:20:41,679 --> 00:20:46,840
and then we're going to start that right

00:20:43,120 --> 00:20:50,400
up so now it's connected to

00:20:46,840 --> 00:20:55,360
geo so when i go back to

00:20:50,400 --> 00:20:55,360
uh the postman now let's insert a record

00:20:57,120 --> 00:21:00,400
so we insert and count with account uh

00:20:59,919 --> 00:21:03,520
with a

00:21:00,400 --> 00:21:03,919
number two as the id so i can read that

00:21:03,520 --> 00:21:08,559
out

00:21:03,919 --> 00:21:11,919
from geo so now we get a did a basic

00:21:08,559 --> 00:21:16,559
read write operation within our geo

00:21:11,919 --> 00:21:18,720
so now let's look at uh jmeter

00:21:16,559 --> 00:21:20,080
and let's look at what's the performance

00:21:18,720 --> 00:21:23,120
results from that so now

00:21:20,080 --> 00:21:24,320
uh from geo so now let's remember so now

00:21:23,120 --> 00:21:26,080
we were getting about

00:21:24,320 --> 00:21:28,240
three seconds uh sorry three

00:21:26,080 --> 00:21:29,679
milliseconds as you say for the jdbc

00:21:28,240 --> 00:21:32,480
interface

00:21:29,679 --> 00:21:33,200
we clear this out now these results are

00:21:32,480 --> 00:21:36,960
going to be

00:21:33,200 --> 00:21:38,799
from apache geo so you see now we went

00:21:36,960 --> 00:21:42,400
from and it's already finished

00:21:38,799 --> 00:21:43,280
so we went from about three milliseconds

00:21:42,400 --> 00:21:46,320
to actually

00:21:43,280 --> 00:21:48,320
under a millisecond's response time uh

00:21:46,320 --> 00:21:50,320
we can clear this out to see kind of how

00:21:48,320 --> 00:21:52,640
consistent results are

00:21:50,320 --> 00:21:54,000
and i see the the results are pretty

00:21:52,640 --> 00:21:57,360
consistent and that

00:21:54,000 --> 00:21:58,960
the max times actually uh significantly

00:21:57,360 --> 00:22:02,000
reduce

00:21:58,960 --> 00:22:02,000
let's look at some code

00:22:03,280 --> 00:22:09,840
so uh for the code

00:22:07,360 --> 00:22:10,880
this is what the geo repository looks

00:22:09,840 --> 00:22:13,200
like

00:22:10,880 --> 00:22:14,400
so as long as i have the the region

00:22:13,200 --> 00:22:17,600
object the jim fire

00:22:14,400 --> 00:22:18,640
uh sort of the geode region object i can

00:22:17,600 --> 00:22:20,799
basically just do

00:22:18,640 --> 00:22:22,320
my current operation directly against

00:22:20,799 --> 00:22:24,640
that region object

00:22:22,320 --> 00:22:25,440
so you see the the code is a little bit

00:22:24,640 --> 00:22:28,320
simpler

00:22:25,440 --> 00:22:29,360
so we're doing less uh sort of mapping

00:22:28,320 --> 00:22:32,240
between

00:22:29,360 --> 00:22:33,120
um you know between like the jbc

00:22:32,240 --> 00:22:35,919
statements

00:22:33,120 --> 00:22:36,400
uh from a domain object really geo just

00:22:35,919 --> 00:22:38,799
about

00:22:36,400 --> 00:22:40,960
allows you if you have an object just

00:22:38,799 --> 00:22:42,080
sort of object uh store that object just

00:22:40,960 --> 00:22:44,000
give it a key

00:22:42,080 --> 00:22:45,520
and if you need to get out that object

00:22:44,000 --> 00:22:47,840
as long as you have the key

00:22:45,520 --> 00:22:49,039
it's no real mapping that it's needed so

00:22:47,840 --> 00:22:55,440
the code is much

00:22:49,039 --> 00:22:59,600
simpler okay so that is apache geo

00:22:55,440 --> 00:22:59,600
and now let's continue on our talk

00:23:00,799 --> 00:23:06,880
so you may have lots of questions

00:23:04,240 --> 00:23:07,679
i imagine one of the questions is okay

00:23:06,880 --> 00:23:10,320
that's great

00:23:07,679 --> 00:23:11,039
now how do i get my data from my legacy

00:23:10,320 --> 00:23:13,760
database

00:23:11,039 --> 00:23:15,440
into the cache well uh one of the ways

00:23:13,760 --> 00:23:16,640
or one of the approaches to do that is

00:23:15,440 --> 00:23:19,440
to introduce

00:23:16,640 --> 00:23:20,400
what we call like a lazy or just-in-time

00:23:19,440 --> 00:23:22,880
uh loading

00:23:20,400 --> 00:23:24,080
where the data can be loaded as needed

00:23:22,880 --> 00:23:26,960
uh this approach

00:23:24,080 --> 00:23:27,600
is good when you are typically accessing

00:23:26,960 --> 00:23:30,799
your data

00:23:27,600 --> 00:23:32,000
by a key by an id for example and you

00:23:30,799 --> 00:23:34,159
need to minimize

00:23:32,000 --> 00:23:35,679
the amount of data that is stored in the

00:23:34,159 --> 00:23:38,080
cache so you can imagine

00:23:35,679 --> 00:23:39,840
your legacy databases that have been

00:23:38,080 --> 00:23:41,919
around for a long time

00:23:39,840 --> 00:23:42,960
there might be i don't know petabytes

00:23:41,919 --> 00:23:46,640
worth of data

00:23:42,960 --> 00:23:49,120
and it might be uh not feasible uh to uh

00:23:46,640 --> 00:23:51,279
move all of that data onto the cache so

00:23:49,120 --> 00:23:52,159
you know you can use something one

00:23:51,279 --> 00:23:55,279
implementation

00:23:52,159 --> 00:23:56,640
of this lazy loading is what's called a

00:23:55,279 --> 00:23:59,840
look a soft cache

00:23:56,640 --> 00:24:01,679
so in this case the app would look for

00:23:59,840 --> 00:24:04,480
if you tried to read if you

00:24:01,679 --> 00:24:07,120
submit a read request it would look to

00:24:04,480 --> 00:24:09,200
see if that data is in the cache already

00:24:07,120 --> 00:24:12,640
if it's not in the cache then it'll read

00:24:09,200 --> 00:24:17,039
it from the external data source

00:24:12,640 --> 00:24:17,039
so uh let's show you a demo of that

00:24:17,120 --> 00:24:24,240
okay um so actually let me go back

00:24:20,400 --> 00:24:26,480
oh one second just go back to here

00:24:24,240 --> 00:24:26,480
okay

00:24:27,919 --> 00:24:31,360
all right so let's go back to the trusty

00:24:30,320 --> 00:24:34,640
read state uh

00:24:31,360 --> 00:24:37,120
readme so in this case i'm going to

00:24:34,640 --> 00:24:41,039
configure it to use the look aside

00:24:37,120 --> 00:24:41,039
implementation of the repository

00:24:44,400 --> 00:24:52,000
and we'll go ahead and clear this up

00:24:47,520 --> 00:24:54,559
uh kill this method clear the screen

00:24:52,000 --> 00:24:56,799
and then start that up with the other

00:24:54,559 --> 00:25:00,480
repository

00:24:56,799 --> 00:25:02,080
and let's test this out with postgame so

00:25:00,480 --> 00:25:04,080
and postman we're gonna basically do the

00:25:02,080 --> 00:25:05,440
same thing where in this case we're

00:25:04,080 --> 00:25:08,480
going to

00:25:05,440 --> 00:25:08,480
write a new record

00:25:09,279 --> 00:25:12,240
account number three

00:25:12,880 --> 00:25:17,600
and look at what the code is doing so

00:25:14,400 --> 00:25:19,520
the the code the the basic pattern is

00:25:17,600 --> 00:25:21,520
it it makes sure that that record if

00:25:19,520 --> 00:25:23,039
you're modifying the data

00:25:21,520 --> 00:25:24,720
and make sure that that data is kind of

00:25:23,039 --> 00:25:25,919
removed from the cache or evicted from

00:25:24,720 --> 00:25:28,960
the cache

00:25:25,919 --> 00:25:32,159
and then it updates this data into the

00:25:28,960 --> 00:25:34,960
uh the database so that if i'm

00:25:32,159 --> 00:25:38,720
trying to do a read so now i'll try to

00:25:34,960 --> 00:25:38,720
get that data out by its id

00:25:39,760 --> 00:25:44,000
so what it did was it tried to read from

00:25:42,000 --> 00:25:45,279
the cache it saw that it wasn't in the

00:25:44,000 --> 00:25:48,320
cache and then it

00:25:45,279 --> 00:25:51,520
um uh then it um

00:25:48,320 --> 00:25:53,039
read from db2 and when it read the

00:25:51,520 --> 00:25:55,520
results from db2 then it

00:25:53,039 --> 00:25:58,320
it put it back into the caps so that the

00:25:55,520 --> 00:26:00,799
next time when i try to do a read

00:25:58,320 --> 00:26:02,480
uh next time this will only go from the

00:26:00,799 --> 00:26:06,000
cache right so you see no

00:26:02,480 --> 00:26:07,679
more sql statements right so that's one

00:26:06,000 --> 00:26:11,840
approach that's the look aside

00:26:07,679 --> 00:26:11,840
uh cache uh approach

00:26:12,240 --> 00:26:16,880
uh which is basically this architecture

00:26:14,480 --> 00:26:16,880
right here

00:26:18,080 --> 00:26:23,520
all right so now uh there's it now

00:26:21,360 --> 00:26:25,039
that's that's there's a couple of let me

00:26:23,520 --> 00:26:27,520
actually go back so there's a couple of

00:26:25,039 --> 00:26:29,919
different problems with this uh approach

00:26:27,520 --> 00:26:32,400
uh the problem is down the app there's

00:26:29,919 --> 00:26:34,799
more complexity built into the app

00:26:32,400 --> 00:26:35,840
the app now needs to know how to talk to

00:26:34,799 --> 00:26:39,600
the cache layer

00:26:35,840 --> 00:26:42,799
into the jdbc layer right and also

00:26:39,600 --> 00:26:44,480
uh if data is not in the cache you

00:26:42,799 --> 00:26:47,440
basically you can still get that

00:26:44,480 --> 00:26:48,960
slow response time for the initial loads

00:26:47,440 --> 00:26:52,480
of the data

00:26:48,960 --> 00:26:54,480
so uh uh one way to address this

00:26:52,480 --> 00:26:56,880
is to introduce some sort of data

00:26:54,480 --> 00:27:00,640
pipeline so the data pipeline

00:26:56,880 --> 00:27:02,559
is basically a set of processes

00:27:00,640 --> 00:27:03,679
that would make sure that the cache is

00:27:02,559 --> 00:27:07,440
up to date

00:27:03,679 --> 00:27:09,919
right it can use a a a batch approach

00:27:07,440 --> 00:27:11,679
where you're basically every once in a

00:27:09,919 --> 00:27:13,600
while may be based on a schedule you

00:27:11,679 --> 00:27:14,720
update the data from the legacy database

00:27:13,600 --> 00:27:16,799
into the cache

00:27:14,720 --> 00:27:18,320
or you can use a streaming approach

00:27:16,799 --> 00:27:19,279
which is basically the app is always

00:27:18,320 --> 00:27:20,720
running

00:27:19,279 --> 00:27:23,039
always looking for changes in the

00:27:20,720 --> 00:27:24,240
external database to push to the cache

00:27:23,039 --> 00:27:25,919
and we're going to show you both

00:27:24,240 --> 00:27:29,520
approaches

00:27:25,919 --> 00:27:31,520
see how we're doing on time um

00:27:29,520 --> 00:27:33,440
i think actually we might be running a

00:27:31,520 --> 00:27:37,360
little bit low on time

00:27:33,440 --> 00:27:39,279
um all right so uh the initial approach

00:27:37,360 --> 00:27:41,039
is i'm going to show you the batch based

00:27:39,279 --> 00:27:44,000
approach

00:27:41,039 --> 00:27:44,559
so in the batch base approach let's go

00:27:44,000 --> 00:27:49,840
ahead

00:27:44,559 --> 00:27:49,840
and i have um

00:27:50,480 --> 00:27:55,600
i have my data pipeline all ready to go

00:27:52,799 --> 00:27:57,440
so this is my data pipeline app

00:27:55,600 --> 00:27:59,520
so when i start this up this is

00:27:57,440 --> 00:28:01,840
basically going to let me

00:27:59,520 --> 00:28:04,240
make sure that you can see over the

00:28:01,840 --> 00:28:04,240
screen

00:28:06,240 --> 00:28:10,240
okay so when i started this up it

00:28:08,159 --> 00:28:10,880
basically tried to see if there any

00:28:10,240 --> 00:28:13,440
changes

00:28:10,880 --> 00:28:15,440
in the database it didn't detect any

00:28:13,440 --> 00:28:18,080
changes so it basically immediately

00:28:15,440 --> 00:28:20,000
sent that over basically it it didn't

00:28:18,080 --> 00:28:22,240
find any records

00:28:20,000 --> 00:28:23,279
so we can uh insert some new records

00:28:22,240 --> 00:28:26,480
into the

00:28:23,279 --> 00:28:29,039
database just by using um

00:28:26,480 --> 00:28:30,799
our sql editor so let's go ahead and

00:28:29,039 --> 00:28:34,640
insert a new record

00:28:30,799 --> 00:28:36,720
into the database so now

00:28:34,640 --> 00:28:37,679
when we run the batch again again it

00:28:36,720 --> 00:28:40,799
should pick up

00:28:37,679 --> 00:28:44,320
that new record and move that over

00:28:40,799 --> 00:28:46,399
uh to geo and uh

00:28:44,320 --> 00:28:47,840
actually let's open up which i didn't

00:28:46,399 --> 00:28:51,279
show you this is the

00:28:47,840 --> 00:28:53,200
uh the g the g fish client so in g

00:28:51,279 --> 00:28:55,279
fish i could do things like doing a

00:28:53,200 --> 00:28:58,399
query so i can look at all of the data

00:28:55,279 --> 00:29:00,399
that is in my region which is called

00:28:58,399 --> 00:29:02,559
accounts so i could see the data that we

00:29:00,399 --> 00:29:04,000
just uh inserted right the

00:29:02,559 --> 00:29:06,480
the number four record that we just

00:29:04,000 --> 00:29:06,480
inserted

00:29:06,880 --> 00:29:12,399
all right so the other the other

00:29:09,679 --> 00:29:14,240
approach i want to show you before we

00:29:12,399 --> 00:29:15,840
open this up for questions

00:29:14,240 --> 00:29:17,440
um i don't know if i think i got a

00:29:15,840 --> 00:29:18,799
moderator to be able to open this up

00:29:17,440 --> 00:29:21,039
some questions

00:29:18,799 --> 00:29:22,399
but um now we're going to show you the

00:29:21,039 --> 00:29:24,240
streaming approach

00:29:22,399 --> 00:29:25,600
so in the streaming approach we're going

00:29:24,240 --> 00:29:28,080
to have this

00:29:25,600 --> 00:29:29,120
a source app and this is again always

00:29:28,080 --> 00:29:31,840
running

00:29:29,120 --> 00:29:32,159
looking for trying to detect changes in

00:29:31,840 --> 00:29:35,279
the

00:29:32,159 --> 00:29:35,840
db2 database and in this case it's going

00:29:35,279 --> 00:29:38,880
to push

00:29:35,840 --> 00:29:40,080
all of the changes to cockpit and we

00:29:38,880 --> 00:29:42,880
have a separate app

00:29:40,080 --> 00:29:44,880
a sync app which is looking for uh

00:29:42,880 --> 00:29:46,880
events that's pushed to kafka to store

00:29:44,880 --> 00:29:49,200
that into the apache geo

00:29:46,880 --> 00:29:52,240
uh cache and our apps are just gonna

00:29:49,200 --> 00:29:54,080
configure to just look at apache geo

00:29:52,240 --> 00:29:55,679
you may be asking why would you want to

00:29:54,080 --> 00:29:57,600
do an approach like this

00:29:55,679 --> 00:29:59,520
well one of the reasons is if you want

00:29:57,600 --> 00:30:02,880
to have future apps

00:29:59,520 --> 00:30:04,799
uh to be able to tap into the stream for

00:30:02,880 --> 00:30:06,240
future requirements you can introduce

00:30:04,799 --> 00:30:10,000
something an architecture

00:30:06,240 --> 00:30:13,840
like this all right so

00:30:10,000 --> 00:30:15,760
let's go back uh to our app so what i'm

00:30:13,840 --> 00:30:19,520
going to do is i'm going to

00:30:15,760 --> 00:30:23,440
stop the app so that i can now

00:30:19,520 --> 00:30:28,720
um start it up with the

00:30:23,440 --> 00:30:28,720
um go back to just looking at apache geo

00:30:33,440 --> 00:30:38,640
okay so copy that

00:30:36,480 --> 00:30:40,640
and now we start this up so now it's

00:30:38,640 --> 00:30:44,880
looking at geo

00:30:40,640 --> 00:30:48,559
and uh in our pipelines uh

00:30:44,880 --> 00:30:51,600
actually this for streaming we'll have a

00:30:48,559 --> 00:30:55,360
a streaming source app at the top

00:30:51,600 --> 00:30:57,760
so we'll start that up so this guy is

00:30:55,360 --> 00:30:58,880
always basically just looking for

00:30:57,760 --> 00:31:02,080
changes

00:30:58,880 --> 00:31:02,720
within uh the database so uh it

00:31:02,080 --> 00:31:06,080
basically

00:31:02,720 --> 00:31:09,039
uh it checks for changes uh every

00:31:06,080 --> 00:31:09,039
uh five seconds

00:31:09,360 --> 00:31:13,679
and and here this is the sake so this is

00:31:12,720 --> 00:31:17,200
just looking for

00:31:13,679 --> 00:31:21,039
any data that's been pushed to apache uh

00:31:17,200 --> 00:31:25,039
kafka so that if we go back

00:31:21,039 --> 00:31:28,720
into our database and we insert

00:31:25,039 --> 00:31:28,720
a new record for example

00:31:29,279 --> 00:31:32,320
uh we should see that new record

00:31:31,600 --> 00:31:35,600
basically

00:31:32,320 --> 00:31:39,039
gets detected and gets sent over to

00:31:35,600 --> 00:31:42,799
uh geo so if i go back to gfesh

00:31:39,039 --> 00:31:42,799
i'll see the account number five

00:31:42,960 --> 00:31:46,640
all right so the good thing about this

00:31:45,279 --> 00:31:49,039
is um

00:31:46,640 --> 00:31:50,159
it's actually always it's always looking

00:31:49,039 --> 00:31:53,039
for changes

00:31:50,159 --> 00:31:53,600
so it's kind of keeping everything up up

00:31:53,039 --> 00:31:55,519
to date

00:31:53,600 --> 00:31:56,960
all of the database information are up

00:31:55,519 --> 00:31:59,360
to date and even if i

00:31:56,960 --> 00:32:00,399
update existing records like if i go

00:31:59,360 --> 00:32:03,600
back and

00:32:00,399 --> 00:32:08,080
i don't know change uh um the accounts

00:32:03,600 --> 00:32:11,679
are one through four um if i go back to

00:32:08,080 --> 00:32:14,320
the cash or the stream i should see

00:32:11,679 --> 00:32:15,360
all of those updates got detected so we

00:32:14,320 --> 00:32:18,000
see you've got three

00:32:15,360 --> 00:32:19,360
updates were detected and they all now

00:32:18,000 --> 00:32:22,080
synchronized within

00:32:19,360 --> 00:32:22,080
uh geo

00:32:22,640 --> 00:32:29,760
okay so uh that's pretty much the

00:32:26,399 --> 00:32:31,120
end of the presentation um i don't know

00:32:29,760 --> 00:32:33,039
if we can

00:32:31,120 --> 00:32:35,760
open it up for questions do i have a

00:32:33,039 --> 00:32:35,760
moderator

00:32:38,240 --> 00:32:42,960
oh actually before i do that uh one

00:32:41,200 --> 00:32:44,880
thing i just wanted to sort of to

00:32:42,960 --> 00:32:46,559
to wrap this up and put a put a ball

00:32:44,880 --> 00:32:57,760
around everything

00:32:46,559 --> 00:33:00,399
is uh let me show this

00:32:57,760 --> 00:33:02,640
all right so we showed you that uh for

00:33:00,399 --> 00:33:04,559
apache geo introducing the cache

00:33:02,640 --> 00:33:05,840
we should we have you know we can

00:33:04,559 --> 00:33:08,559
introduce low latency

00:33:05,840 --> 00:33:10,320
for the data access uh we can scale up

00:33:08,559 --> 00:33:11,039
the number of data nodes because it's

00:33:10,320 --> 00:33:14,000
kind of built

00:33:11,039 --> 00:33:16,480
into geode and we can also isolate the

00:33:14,000 --> 00:33:17,360
data access just by moving data into the

00:33:16,480 --> 00:33:18,960
cache

00:33:17,360 --> 00:33:20,720
and again now the cache should be able

00:33:18,960 --> 00:33:24,320
to easily evolve

00:33:20,720 --> 00:33:24,960
just as easily as um as the applications

00:33:24,320 --> 00:33:27,679
are

00:33:24,960 --> 00:33:28,399
and all the code is available on my

00:33:27,679 --> 00:33:31,519
personal

00:33:28,399 --> 00:33:34,559
uh page so if you go to my uh

00:33:31,519 --> 00:33:37,039
my github you can see a caching

00:33:34,559 --> 00:33:38,960
uh on uh relational database

00:33:37,039 --> 00:33:41,440
transformation project

00:33:38,960 --> 00:33:43,919
and you can follow me on twitter and

00:33:41,440 --> 00:33:43,919
link there

00:33:47,519 --> 00:33:53,840
okay great that's pretty much the end of

00:33:51,120 --> 00:33:53,840
our demo

00:34:08,879 --> 00:34:12,560
all right it's basically looking at the

00:34:10,879 --> 00:34:15,839
chat do we see any question

00:34:12,560 --> 00:34:21,839
any questions in the chat

00:34:15,839 --> 00:34:21,839
all right don't see the questions yet

00:34:25,040 --> 00:34:30,399
all right with that being said everybody

00:34:26,879 --> 00:34:30,399
have a great rest of your day

00:34:32,839 --> 00:34:35,839
bye

00:34:44,800 --> 00:34:46,879

YouTube URL: https://www.youtube.com/watch?v=h5UvIJo7eBc


