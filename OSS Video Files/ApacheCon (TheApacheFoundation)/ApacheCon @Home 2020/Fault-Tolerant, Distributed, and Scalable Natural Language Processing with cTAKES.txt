Title: Fault-Tolerant, Distributed, and Scalable Natural Language Processing with cTAKES
Publication date: 2020-10-14
Playlist: ApacheCon @Home 2020
Description: 
	Fault-Tolerant, Distributed, and Scalable Natural Language Processing with cTAKES
Jeritt Thayer, Jeffrey Miller

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Electronic health records contain a substantial amount of clinical information as unstructured free text. This information has the potential to enhance clinical decision making as well as provide insight for secondary health related research. Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES) is a health specific natural language processing (NLP) system that has demonstrated success in the health care industry. However, analyzing large sets of notes with cTAKES can take months or even years to complete. By combining cTAKES with Apache Spark, we developed a fault-tolerant and scalable NLP pipeline that respects the single threaded limitation inherent in cTAKES pipelines. It is capable of processing millions of clinical notes in minutes on a large computing cluster. We have also configured the pipeline to make it easy to adjust common settings like changing negation detection algorithms and toggling whether or not to detect entities over discontinuous spans. At the completion of this session, you will have a practical example of processing large volumes of unstructured text using cTAKES and be able to identify the benefits of using different Apache distributed computing frameworks such as Spark and Beam.

Jeritt Thayer
Jeritt Thayer is a software engineer at Children's Hospital of Philadelphia. His work focuses on designing, developing, and evaluating novel systems to support patient engagement, medical decision making, and care delivery. Prior to his career in software, Jeritt was a professional soccer player. Jeritt is passionate about developing applications that support asynchronous and non-colocated communication to improve provider coordination and patient outcomes.

Jeff Miller:
Jeff Miller leads a team of data scientists at the Children's Hospital of Philadelphia (CHOP). His work focuses on developing tools to help researchers analyze clinical data. Jeff holds a master's degree in applied statistics from Penn State University.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,080 --> 00:00:28,080
so

00:00:24,480 --> 00:00:31,199
i think uh we're on time here to

00:00:28,080 --> 00:00:34,160
go ahead and get started

00:00:31,199 --> 00:00:35,600
great well thank you sean and thank you

00:00:34,160 --> 00:00:37,040
from the apache conference

00:00:35,600 --> 00:00:38,960
for having us and for listening in on

00:00:37,040 --> 00:00:41,280
this session

00:00:38,960 --> 00:00:43,600
we call attention to the presentations

00:00:41,280 --> 00:00:46,480
that have taken place on sea takes even

00:00:43,600 --> 00:00:46,480
the one just before

00:00:46,960 --> 00:00:50,239
and and looking forward to continued

00:00:48,719 --> 00:00:53,440
development on this

00:00:50,239 --> 00:00:56,960
wonderful wonderful piece of software

00:00:53,440 --> 00:00:59,680
but continuing into this presentation

00:00:56,960 --> 00:01:01,280
and i'll be joined by jeffrey miller and

00:00:59,680 --> 00:01:02,399
we both work for the children's hospital

00:01:01,280 --> 00:01:04,000
of philadelphia

00:01:02,399 --> 00:01:05,680
and today we're going to be discussing

00:01:04,000 --> 00:01:07,119
fault tolerant distributed and scalable

00:01:05,680 --> 00:01:10,240
natural language processing

00:01:07,119 --> 00:01:12,000
c takes so

00:01:10,240 --> 00:01:14,000
the general objectives of this

00:01:12,000 --> 00:01:14,880
presentation will hopefully be able to

00:01:14,000 --> 00:01:16,080
give you

00:01:14,880 --> 00:01:17,840
an understanding of the common

00:01:16,080 --> 00:01:19,520
challenges associated with extracting

00:01:17,840 --> 00:01:20,880
information from clinical narratives and

00:01:19,520 --> 00:01:21,920
you might have already you might already

00:01:20,880 --> 00:01:23,119
be familiar with it

00:01:21,920 --> 00:01:25,040
or have seen it from previous

00:01:23,119 --> 00:01:26,400
presentations

00:01:25,040 --> 00:01:27,680
also to describe the benefit of

00:01:26,400 --> 00:01:29,439
utilizing distributed computing

00:01:27,680 --> 00:01:29,759
frameworks to obtain structured data

00:01:29,439 --> 00:01:31,280
from

00:01:29,759 --> 00:01:33,119
natural language text which is often

00:01:31,280 --> 00:01:34,000
what is stored in the ehrs which we'll

00:01:33,119 --> 00:01:35,759
get to

00:01:34,000 --> 00:01:37,680
and also to identify strategies for

00:01:35,759 --> 00:01:39,520
processing clinical narrative data

00:01:37,680 --> 00:01:41,119
in an ongoing matter know it's great to

00:01:39,520 --> 00:01:42,640
have it in batch cycles but we'd love to

00:01:41,119 --> 00:01:44,399
have this continued

00:01:42,640 --> 00:01:45,520
on a daily basis which is i believe

00:01:44,399 --> 00:01:48,079
something we saw in our previous

00:01:45,520 --> 00:01:48,079
presentation

00:01:48,399 --> 00:01:52,079
here's a general overview of the

00:01:50,399 --> 00:01:53,840
presentation i will be giving a

00:01:52,079 --> 00:01:54,720
background about kind of where we are

00:01:53,840 --> 00:01:56,560
how we got there

00:01:54,720 --> 00:01:58,240
framework evaluations related to

00:01:56,560 --> 00:01:59,840
distributed computing

00:01:58,240 --> 00:02:01,680
go over the results of our processing

00:01:59,840 --> 00:02:02,719
pipeline and then jeff miller will be

00:02:01,680 --> 00:02:04,560
able to step in and talk about

00:02:02,719 --> 00:02:06,240
customizations of that pipeline

00:02:04,560 --> 00:02:08,080
lessons learned discussion and we should

00:02:06,240 --> 00:02:10,319
have enough time for questions at the

00:02:08,080 --> 00:02:10,319
end

00:02:10,720 --> 00:02:14,800
so give you a brief brief background

00:02:12,800 --> 00:02:16,319
about computers in healthcare and i

00:02:14,800 --> 00:02:17,680
promise i will keep it brief since i

00:02:16,319 --> 00:02:19,440
believe many of you are likely pretty

00:02:17,680 --> 00:02:21,520
familiar with

00:02:19,440 --> 00:02:23,680
2009 the health information technology

00:02:21,520 --> 00:02:24,239
for economic and clinical health act was

00:02:23,680 --> 00:02:26,160
passed

00:02:24,239 --> 00:02:27,680
also known as high tech and that really

00:02:26,160 --> 00:02:29,040
incentivized the use of electronic

00:02:27,680 --> 00:02:30,959
health records which i'll further refer

00:02:29,040 --> 00:02:34,480
to as ehrs

00:02:30,959 --> 00:02:36,480
and as a result of this information from

00:02:34,480 --> 00:02:38,560
clinical visits and information from

00:02:36,480 --> 00:02:40,959
other aspects of clinical care

00:02:38,560 --> 00:02:42,239
began to be stored within the ehr and

00:02:40,959 --> 00:02:43,519
that was great you know a lot some of it

00:02:42,239 --> 00:02:45,120
was structured but

00:02:43,519 --> 00:02:46,560
the way that physicians communicated

00:02:45,120 --> 00:02:48,480
between other physicians was still

00:02:46,560 --> 00:02:50,160
often through the clinical note and it

00:02:48,480 --> 00:02:54,000
was also required for

00:02:50,160 --> 00:02:56,400
for legal purposes and so most of

00:02:54,000 --> 00:02:57,840
information that was used and that had a

00:02:56,400 --> 00:02:58,400
lot of clinical relevance was still

00:02:57,840 --> 00:03:00,640
stored

00:02:58,400 --> 00:03:02,239
as unstructured free text and that

00:03:00,640 --> 00:03:03,519
unstructured free text not only had the

00:03:02,239 --> 00:03:05,120
potential to enhance

00:03:03,519 --> 00:03:06,640
systems for patient care such as

00:03:05,120 --> 00:03:07,200
clinical distance support or a number of

00:03:06,640 --> 00:03:09,680
other

00:03:07,200 --> 00:03:10,560
applications you had seen at this this

00:03:09,680 --> 00:03:13,519
conference

00:03:10,560 --> 00:03:13,519
secondary research

00:03:13,599 --> 00:03:18,319
and so the next step along this process

00:03:16,640 --> 00:03:19,920
is development of natural language

00:03:18,319 --> 00:03:21,040
processing and commercial enterprises

00:03:19,920 --> 00:03:22,879
have developed natural language

00:03:21,040 --> 00:03:25,200
processing systems it's not new

00:03:22,879 --> 00:03:26,560
you know google has their own amazon has

00:03:25,200 --> 00:03:27,920
their own as well as a number of other

00:03:26,560 --> 00:03:30,000
private companies

00:03:27,920 --> 00:03:31,519
unfortunately a lot of those companies

00:03:30,000 --> 00:03:33,440
at least earlier in their

00:03:31,519 --> 00:03:34,959
their days really just focused on

00:03:33,440 --> 00:03:36,879
natural language text in general and

00:03:34,959 --> 00:03:40,799
we're not necessarily focused on

00:03:36,879 --> 00:03:42,879
clinical text and for

00:03:40,799 --> 00:03:44,159
which is a pediatric institution even

00:03:42,879 --> 00:03:45,120
those that did apply

00:03:44,159 --> 00:03:47,519
you know their natural language

00:03:45,120 --> 00:03:49,519
processing algorithms towards clinical

00:03:47,519 --> 00:03:53,120
text it was very infrequently

00:03:49,519 --> 00:03:55,519
focused on on p um and

00:03:53,120 --> 00:03:57,040
and that's a that's a problem that we're

00:03:55,519 --> 00:03:58,640
we're going to continue to face i think

00:03:57,040 --> 00:03:59,360
um but we're we're continuing to work on

00:03:58,640 --> 00:04:01,599
that

00:03:59,360 --> 00:04:03,200
an apache clinical texture ctakes kind

00:04:01,599 --> 00:04:04,400
of was developed to fill that gap in

00:04:03,200 --> 00:04:06,560
that clinical knowledge base as it

00:04:04,400 --> 00:04:08,400
relates to natural language processing

00:04:06,560 --> 00:04:10,400
now as many of you know see takes runs

00:04:08,400 --> 00:04:12,239
as a single threaded process

00:04:10,400 --> 00:04:13,920
um but unfortunately analyzing large

00:04:12,239 --> 00:04:14,640
sets of notes as a result of that single

00:04:13,920 --> 00:04:16,799
fader process

00:04:14,640 --> 00:04:18,000
can take months or even years to

00:04:16,799 --> 00:04:19,040
complete you know there are a number of

00:04:18,000 --> 00:04:20,320
different ways as you've seen in

00:04:19,040 --> 00:04:21,519
previous presentations of kind of

00:04:20,320 --> 00:04:23,040
spawning off and that's really what

00:04:21,519 --> 00:04:24,720
we're going to talk to you about today

00:04:23,040 --> 00:04:28,880
is how we were able to scale out this

00:04:24,720 --> 00:04:30,800
application across a wide number

00:04:28,880 --> 00:04:32,400
and so the objective of our project was

00:04:30,800 --> 00:04:34,080
to utilize existing distributed

00:04:32,400 --> 00:04:36,080
computing frameworks we wanted to try to

00:04:34,080 --> 00:04:37,919
to build as little as possible to

00:04:36,080 --> 00:04:39,600
develop a fault tolerant and scalable

00:04:37,919 --> 00:04:40,320
cta spine line that was capable of

00:04:39,600 --> 00:04:43,840
processing

00:04:40,320 --> 00:04:43,840
mild of notes in parallel

00:04:44,320 --> 00:04:47,600
and again as promised a brief overview

00:04:47,120 --> 00:04:48,960
of

00:04:47,600 --> 00:04:51,040
distributed computing and kind of

00:04:48,960 --> 00:04:52,800
evaluation of the frameworks will follow

00:04:51,040 --> 00:04:54,560
so distributed computing components of a

00:04:52,800 --> 00:04:56,080
system shared amongst multiple machines

00:04:54,560 --> 00:04:58,240
to improve performance and that follows

00:04:56,080 --> 00:04:59,759
along that map-reduced paradigm

00:04:58,240 --> 00:05:01,440
the great thing about a lot of

00:04:59,759 --> 00:05:02,000
distributed computing frameworks today

00:05:01,440 --> 00:05:03,440
is that they are

00:05:02,000 --> 00:05:04,960
fault tolerant so you don't necessarily

00:05:03,440 --> 00:05:06,960
have to be as concerned about that as

00:05:04,960 --> 00:05:09,039
you would perhaps in previous years

00:05:06,960 --> 00:05:11,039
and a lot of the advanced frameworks

00:05:09,039 --> 00:05:14,240
support that such as

00:05:11,039 --> 00:05:14,240
as well as apache being

00:05:14,400 --> 00:05:17,360
and for those that you are not familiar

00:05:16,160 --> 00:05:19,039
with that traditional map reduce

00:05:17,360 --> 00:05:20,800
paradigm this is a very high level

00:05:19,039 --> 00:05:22,880
conceptual overview of it

00:05:20,800 --> 00:05:24,400
on the left side here you will see that

00:05:22,880 --> 00:05:26,080
text this is kind of the clinical

00:05:24,400 --> 00:05:29,520
narrative text is being input

00:05:26,080 --> 00:05:31,199
into uh the system the computing frame

00:05:29,520 --> 00:05:33,840
which will then be split among a number

00:05:31,199 --> 00:05:36,400
of different machines

00:05:33,840 --> 00:05:37,680
it takes aspects of it and then at which

00:05:36,400 --> 00:05:39,120
point all of those

00:05:37,680 --> 00:05:41,440
notes that have been split across all of

00:05:39,120 --> 00:05:42,880
those machines or worker nodes and

00:05:41,440 --> 00:05:45,680
executors if you're talking in spark

00:05:42,880 --> 00:05:48,880
language will then be reduced

00:05:45,680 --> 00:05:52,160
in that vc takes in this instance

00:05:48,880 --> 00:05:53,520
which will then spit out

00:05:52,160 --> 00:05:55,919
the final results of a number of

00:05:53,520 --> 00:05:58,000
different entities or discrete

00:05:55,919 --> 00:05:58,960
data that can be stored in a database

00:05:58,000 --> 00:06:01,120
and hopefully used

00:05:58,960 --> 00:06:04,319
for further research or to translate

00:06:01,120 --> 00:06:04,319
back into the bedside care

00:06:05,199 --> 00:06:08,960
and so as far as operational tooling

00:06:06,960 --> 00:06:10,000
goes of the of the process of the

00:06:08,960 --> 00:06:12,800
pipeline itself

00:06:10,000 --> 00:06:14,880
we did evaluate frameworks uh we

00:06:12,800 --> 00:06:18,080
developed

00:06:14,880 --> 00:06:21,120
pipelines for both speed and spark

00:06:18,080 --> 00:06:24,240
um excuse me

00:06:21,120 --> 00:06:24,240
any pop-ups here

00:06:30,319 --> 00:06:35,840
sorry this is frustrating

00:06:36,319 --> 00:06:42,880
okay there we go we did evaluate

00:06:39,759 --> 00:06:44,240
umaas but we were really looking for

00:06:42,880 --> 00:06:45,680
something that was really managed again

00:06:44,240 --> 00:06:47,600
we wanted to try to use existing

00:06:45,680 --> 00:06:49,759
technology as much as possible

00:06:47,600 --> 00:06:52,479
and so we uh ultimately provide the

00:06:49,759 --> 00:06:54,160
tools for both spark and

00:06:52,479 --> 00:06:55,759
and beam was really nice in that it

00:06:54,160 --> 00:06:57,440
provided a lot of built-in

00:06:55,759 --> 00:06:59,440
support for the notion of side inputs

00:06:57,440 --> 00:07:01,840
and as jeff will mention a little bit

00:06:59,440 --> 00:07:04,639
late in the presentation kind of

00:07:01,840 --> 00:07:05,360
had the option for free to develop a dag

00:07:04,639 --> 00:07:08,720
and and

00:07:05,360 --> 00:07:11,280
work work from that which

00:07:08,720 --> 00:07:12,880
um the one unfortunate aspect of being

00:07:11,280 --> 00:07:14,400
in the environment that we used it in

00:07:12,880 --> 00:07:15,599
which is not necessarily a limitation of

00:07:14,400 --> 00:07:17,599
being across the board

00:07:15,599 --> 00:07:19,520
was that we didn't have as much control

00:07:17,599 --> 00:07:21,919
over the execution environment

00:07:19,520 --> 00:07:23,039
and this is kind of tying back into that

00:07:21,919 --> 00:07:25,440
single threaded

00:07:23,039 --> 00:07:26,800
issue related to ctakes um we really

00:07:25,440 --> 00:07:28,800
needed to make sure that

00:07:26,800 --> 00:07:30,880
any takes process that we were running

00:07:28,800 --> 00:07:31,440
when operating on on legal cpu we didn't

00:07:30,880 --> 00:07:33,360
have any

00:07:31,440 --> 00:07:36,560
threating issues which would cause

00:07:33,360 --> 00:07:36,560
unfortunate circumstances

00:07:38,880 --> 00:07:42,160
and so the results of our pipeline i

00:07:40,560 --> 00:07:44,240
just jumped straight into it

00:07:42,160 --> 00:07:45,440
was a pipeline that processed 84 million

00:07:44,240 --> 00:07:48,639
00:07:45,440 --> 00:07:51,120
661 notes and it only took 89 minutes it

00:07:48,639 --> 00:07:52,160
provided over 170 billion ontology

00:07:51,120 --> 00:07:54,000
mappings and we really

00:07:52,160 --> 00:07:55,599
first focused on the human phenotype on

00:07:54,000 --> 00:07:57,440
geology and that was the base of our

00:07:55,599 --> 00:07:59,199
dictionary we took out

00:07:57,440 --> 00:08:00,800
rxnorm we took out snowbed we really

00:07:59,199 --> 00:08:03,039
really just focus

00:08:00,800 --> 00:08:04,960
uh hbo and that certainly has

00:08:03,039 --> 00:08:06,000
considerations for uh performance as

00:08:04,960 --> 00:08:08,960
you'll likely uh

00:08:06,000 --> 00:08:10,840
be aware of um the cluster itself scaled

00:08:08,960 --> 00:08:15,120
to 240

00:08:10,840 --> 00:08:18,879
machines 64 cores with 240 gigabytes

00:08:15,120 --> 00:08:20,960
of ram the process

00:08:18,879 --> 00:08:22,160
run on see takes kind of out of the box

00:08:20,960 --> 00:08:23,680
on a single machine

00:08:22,160 --> 00:08:25,919
it's estimated would have taken about

00:08:23,680 --> 00:08:26,240
396 days and i'll show you where i got

00:08:25,919 --> 00:08:30,000
some

00:08:26,240 --> 00:08:31,759
very very rough estimates um and this is

00:08:30,000 --> 00:08:33,919
that that slide so here you have

00:08:31,759 --> 00:08:35,760
standard c takes at the top

00:08:33,919 --> 00:08:36,959
looking at document count you know about

00:08:35,760 --> 00:08:39,200
3 000 notes

00:08:36,959 --> 00:08:40,719
there's about 22 minutes 6 thousand

00:08:39,200 --> 00:08:42,719
notes here took about

00:08:40,719 --> 00:08:44,320
42 minutes again this was probably run

00:08:42,719 --> 00:08:46,160
on my local machine very cheap

00:08:44,320 --> 00:08:47,839
just takes a while then you have a

00:08:46,160 --> 00:08:48,160
patchy beam which we did also evaluate

00:08:47,839 --> 00:08:49,440
i'll

00:08:48,160 --> 00:08:52,160
talk to the bottom of it first and the

00:08:49,440 --> 00:08:55,040
first iteration we did

00:08:52,160 --> 00:08:56,959
a little over 2 million notes uh the

00:08:55,040 --> 00:08:59,279
beam runner that was supported kind of

00:08:56,959 --> 00:08:59,920
automatically scaled it scaled up to 418

00:08:59,279 --> 00:09:01,600
machines

00:08:59,920 --> 00:09:04,399
and the first iteration of this we used

00:09:01,600 --> 00:09:05,600
four cores uh hopefully that you know

00:09:04,399 --> 00:09:07,600
you know we'll just be able to run it in

00:09:05,600 --> 00:09:09,279
it'll be able to kind of identify how

00:09:07,600 --> 00:09:11,279
much

00:09:09,279 --> 00:09:12,800
how much processing it needs and be able

00:09:11,279 --> 00:09:15,519
to limit that cpu

00:09:12,800 --> 00:09:17,279
for the threading issue unfortunately we

00:09:15,519 --> 00:09:19,040
found out that the execution environment

00:09:17,279 --> 00:09:21,360
didn't necessarily allow that so

00:09:19,040 --> 00:09:22,720
we then traded down to a smaller number

00:09:21,360 --> 00:09:24,320
of cores one to kind of handle the

00:09:22,720 --> 00:09:27,360
process and the other to handle

00:09:24,320 --> 00:09:29,839
the actual um just kind of the machine

00:09:27,360 --> 00:09:31,519
over over arching architecture it took

00:09:29,839 --> 00:09:32,000
about 44 minutes to get on two million

00:09:31,519 --> 00:09:34,640
notes

00:09:32,000 --> 00:09:35,760
and then here you have the spark 900 000

00:09:34,640 --> 00:09:38,320
notes

00:09:35,760 --> 00:09:39,680
eight minutes 1.7 million notes 30

00:09:38,320 --> 00:09:43,200
minutes and then

00:09:39,680 --> 00:09:44,880
84 million notes 245 machines 64 cores

00:09:43,200 --> 00:09:46,640
89 minutes and this i think roughly

00:09:44,880 --> 00:09:49,360
equates to about 15

00:09:46,640 --> 00:09:50,480
500 executors but the great thing about

00:09:49,360 --> 00:09:50,880
this was that it was actually pretty

00:09:50,480 --> 00:09:54,160
cheap

00:09:50,880 --> 00:09:56,399
um you know oh it's all relative 1300

00:09:54,160 --> 00:09:56,399
it's not

00:09:57,360 --> 00:10:00,480
a lot of money but uh given the how much

00:09:59,519 --> 00:10:02,720
you would likely spend

00:10:00,480 --> 00:10:04,959
elsewhere it's a pretty pretty

00:10:02,720 --> 00:10:06,560
affordable i think

00:10:04,959 --> 00:10:09,440
i also wanted to give you an overview of

00:10:06,560 --> 00:10:11,680
the actual documents themselves i said

00:10:09,440 --> 00:10:13,200
we processed about 84 million notes

00:10:11,680 --> 00:10:14,800
uh each of those notes could have

00:10:13,200 --> 00:10:16,560
multiple versions depending on whether

00:10:14,800 --> 00:10:18,320
or not

00:10:16,560 --> 00:10:20,640
it updated it or attended the note at a

00:10:18,320 --> 00:10:22,399
later time so each note had about 1.3

00:10:20,640 --> 00:10:24,800
versions the length of this note was

00:10:22,399 --> 00:10:27,680
about 1600 characters obviously a pretty

00:10:24,800 --> 00:10:29,760
wide standard deviation there it was

00:10:27,680 --> 00:10:31,600
that the note population of the

00:10:29,760 --> 00:10:34,160
the corpus itself was represented by

00:10:31,600 --> 00:10:36,079
over 18 000 individual providers

00:10:34,160 --> 00:10:38,480
115 provider types like attending

00:10:36,079 --> 00:10:40,480
physician nurse practitioner

00:10:38,480 --> 00:10:42,640
physical therapist things like that and

00:10:40,480 --> 00:10:45,279
representing over 92 clinical services

00:10:42,640 --> 00:10:46,800
such as pulmonary or allergy

00:10:45,279 --> 00:10:48,720
the characteristics of the actual

00:10:46,800 --> 00:10:52,320
analysis what we got on the

00:10:48,720 --> 00:10:56,160
other side was about 707

00:10:52,320 --> 00:10:58,079
million annotations um and just for

00:10:56,160 --> 00:11:00,079
context the hpo dictionary size like i

00:10:58,079 --> 00:11:02,800
said it's much smaller than snomed and

00:11:00,079 --> 00:11:05,839
rx norm it's about 14

00:11:02,800 --> 00:11:08,160
800 distinct

00:11:05,839 --> 00:11:08,959
items or entities within the dictionary

00:11:08,160 --> 00:11:12,079
and

00:11:08,959 --> 00:11:14,160
the process itself picked up 7

00:11:12,079 --> 00:11:16,320
500 unique terms as a part of the

00:11:14,160 --> 00:11:18,480
analysis

00:11:16,320 --> 00:11:20,399
and at this point i'd love to give it

00:11:18,480 --> 00:11:21,440
over to jeff to to finish up the

00:11:20,399 --> 00:11:23,920
presentation and just

00:11:21,440 --> 00:11:25,040
fyi jeff i i cannot see you on my screen

00:11:23,920 --> 00:11:26,160
so if you just want to give me a shout

00:11:25,040 --> 00:11:29,839
when you need to

00:11:26,160 --> 00:11:29,839
swatch uh switch the slides

00:11:31,839 --> 00:11:35,440
okay hi um my name is jeff miller and i

00:11:34,160 --> 00:11:39,360
work with jarrett

00:11:35,440 --> 00:11:42,560
um so one of the

00:11:39,360 --> 00:11:43,920
secondary goals of our work

00:11:42,560 --> 00:11:46,720
or actually one of the primary goals of

00:11:43,920 --> 00:11:47,200
this work is to enable researchers to

00:11:46,720 --> 00:11:50,480
run

00:11:47,200 --> 00:11:53,360
uh three takes on cohorts of notes um

00:11:50,480 --> 00:11:54,959
that that that may be you know part of a

00:11:53,360 --> 00:11:57,360
particular project they're working on

00:11:54,959 --> 00:11:59,920
uh the work that jared uh showed before

00:11:57,360 --> 00:12:03,200
was sort of a larger project

00:11:59,920 --> 00:12:06,240
um a larger project meant to run

00:12:03,200 --> 00:12:09,440
run on all or a majority of nodes

00:12:06,240 --> 00:12:11,440
uh generated each day and so

00:12:09,440 --> 00:12:13,200
um what i'm going to talk about is how

00:12:11,440 --> 00:12:15,680
we uh

00:12:13,200 --> 00:12:16,560
further customized this deployment to

00:12:15,680 --> 00:12:19,519
support

00:12:16,560 --> 00:12:20,639
setting different options um for the

00:12:19,519 --> 00:12:22,480
pipeline

00:12:20,639 --> 00:12:24,639
to be run on to be run on more of a

00:12:22,480 --> 00:12:30,880
one-time

00:12:24,639 --> 00:12:33,839
analysis um more of a one-time analysis

00:12:30,880 --> 00:12:35,440
for a given quarter notes and these are

00:12:33,839 --> 00:12:39,600
all sort of customizations to the

00:12:35,440 --> 00:12:42,800
default pipeline

00:12:39,600 --> 00:12:45,760
ah we go next slide jared

00:12:42,800 --> 00:12:46,880
so um one basically these are a list of

00:12:45,760 --> 00:12:48,560
settings that when we

00:12:46,880 --> 00:12:50,079
when we actually started talking with

00:12:48,560 --> 00:12:50,399
researchers in the institute that needed

00:12:50,079 --> 00:12:52,639
to

00:12:50,399 --> 00:12:53,839
do to use c takes these are the

00:12:52,639 --> 00:12:55,040
different types of settings that it

00:12:53,839 --> 00:12:57,440
seems like

00:12:55,040 --> 00:12:58,480
um we're going to be convenient for them

00:12:57,440 --> 00:13:01,600
to be able to

00:12:58,480 --> 00:13:03,519
to set for for a given project that

00:13:01,600 --> 00:13:07,040
includes the sea takes dictionary

00:13:03,519 --> 00:13:07,680
um also adding in uh a bar separated

00:13:07,040 --> 00:13:09,360
value

00:13:07,680 --> 00:13:10,720
file containing additional perhaps

00:13:09,360 --> 00:13:12,720
custom terms that are not

00:13:10,720 --> 00:13:14,560
part of the umls that are part of any

00:13:12,720 --> 00:13:16,880
sort of generated dictionary

00:13:14,560 --> 00:13:18,399
um one of the big customization points

00:13:16,880 --> 00:13:20,160
is negation detection

00:13:18,399 --> 00:13:22,079
we actually have three separate

00:13:20,160 --> 00:13:24,320
configurations of various

00:13:22,079 --> 00:13:26,079
um negation engines that we support

00:13:24,320 --> 00:13:28,160
right now depending on what research

00:13:26,079 --> 00:13:33,200
wants to do so we have a default

00:13:28,160 --> 00:13:33,200
ml one um and we have and the

00:13:36,959 --> 00:13:44,399
we wanted users to be able to um

00:13:40,240 --> 00:13:44,399
bring a set of uh

00:13:44,720 --> 00:13:48,800
patterns for negation if if that if

00:13:47,199 --> 00:13:51,519
they're if that was required

00:13:48,800 --> 00:13:53,360
we found that um for particular projects

00:13:51,519 --> 00:13:56,240
the built-in machine learning

00:13:53,360 --> 00:13:57,040
um negation detection was missing some

00:13:56,240 --> 00:13:59,839
scenario that

00:13:57,040 --> 00:14:01,360
researchers were interested in we also

00:13:59,839 --> 00:14:02,959
allow a given project to determine

00:14:01,360 --> 00:14:06,160
whether or not they wanted you to

00:14:02,959 --> 00:14:09,440
overlap jks term annotator or the

00:14:06,160 --> 00:14:10,720
um the standard default jks from

00:14:09,440 --> 00:14:11,360
annotator and that's basically whether

00:14:10,720 --> 00:14:14,880
or not

00:14:11,360 --> 00:14:16,240
you want to um you want to allow for

00:14:14,880 --> 00:14:19,440
discontinuous spans

00:14:16,240 --> 00:14:21,440
in your in your resulting annotations

00:14:19,440 --> 00:14:22,800
and because we are running this project

00:14:21,440 --> 00:14:23,440
and we're not we're not storing the

00:14:22,800 --> 00:14:25,920
entire

00:14:23,440 --> 00:14:27,279
xmi output that c takes might create we

00:14:25,920 --> 00:14:29,199
have an option whether or not you want

00:14:27,279 --> 00:14:31,839
to store all the entity mentions

00:14:29,199 --> 00:14:32,399
that are found in a given um run of sea

00:14:31,839 --> 00:14:34,240
takes

00:14:32,399 --> 00:14:36,560
or only more specific ones so anthony

00:14:34,240 --> 00:14:39,199
mentions is like one is like the base

00:14:36,560 --> 00:14:40,160
um most generic met span that he takes

00:14:39,199 --> 00:14:42,160
might find but

00:14:40,160 --> 00:14:43,760
you know we generally would only keep

00:14:42,160 --> 00:14:44,800
this things like disease disorder

00:14:43,760 --> 00:14:46,880
mentions

00:14:44,800 --> 00:14:48,320
lab mentions that kind of thing but some

00:14:46,880 --> 00:14:50,720
projects may want

00:14:48,320 --> 00:14:51,519
they need to add in that more general

00:14:50,720 --> 00:14:53,199
setting

00:14:51,519 --> 00:14:54,399
uh and another setting that's important

00:14:53,199 --> 00:14:55,120
and i'll talk a little bit more later

00:14:54,399 --> 00:14:57,680
about why

00:14:55,120 --> 00:14:59,279
is the number of notes that are a laugh

00:14:57,680 --> 00:15:05,040
that are sent to each spark at

00:14:59,279 --> 00:15:05,040
here uh look at the next slide please

00:15:08,000 --> 00:15:12,560
so just some details about this the

00:15:10,480 --> 00:15:14,800
these settings are determined

00:15:12,560 --> 00:15:16,240
from uh or can be set via a jenkins

00:15:14,800 --> 00:15:17,839
primatized job so it's a

00:15:16,240 --> 00:15:20,399
number of drop-downs and fill-ins that

00:15:17,839 --> 00:15:21,279
that um either researcher or someone on

00:15:20,399 --> 00:15:24,720
research behalf

00:15:21,279 --> 00:15:27,760
can can set um and this initiates

00:15:24,720 --> 00:15:30,000
um a serverless python function that

00:15:27,760 --> 00:15:31,440
then kicks off the job and the things

00:15:30,000 --> 00:15:32,959
that that function does

00:15:31,440 --> 00:15:34,560
are effectively looking at all the

00:15:32,959 --> 00:15:37,519
settings that they that the person

00:15:34,560 --> 00:15:38,079
configured and writing a piper file to

00:15:37,519 --> 00:15:39,839
to

00:15:38,079 --> 00:15:41,360
accomplish that as well as staging

00:15:39,839 --> 00:15:42,320
dictionary files and that kind of thing

00:15:41,360 --> 00:15:45,120
in shared storage

00:15:42,320 --> 00:15:46,320
whether it be accessible um to this to

00:15:45,120 --> 00:15:48,480
the cluster

00:15:46,320 --> 00:15:50,079
um and one of the things that we're

00:15:48,480 --> 00:15:52,639
actually migrating to right now

00:15:50,079 --> 00:15:55,759
is currently our architecture uses a

00:15:52,639 --> 00:15:58,880
message bus between stages in the um

00:15:55,759 --> 00:16:00,639
in in this pipeline to initiate um

00:15:58,880 --> 00:16:02,959
to initiate the next step and we're

00:16:00,639 --> 00:16:05,519
moving to more of a workflow

00:16:02,959 --> 00:16:06,079
a dag um you know directed a cyclic

00:16:05,519 --> 00:16:09,519
graph

00:16:06,079 --> 00:16:12,880
type of architecture um so that we can

00:16:09,519 --> 00:16:13,759
programmatically set a a linear set of

00:16:12,880 --> 00:16:16,399
steps

00:16:13,759 --> 00:16:17,519
and um have it sort of run and visualize

00:16:16,399 --> 00:16:21,279
it as it's happening

00:16:17,519 --> 00:16:24,000
um this just is much easier to debug

00:16:21,279 --> 00:16:24,880
uh from our perspective then a more

00:16:24,000 --> 00:16:29,920
distributed

00:16:24,880 --> 00:16:29,920
um message bus driven algorithm

00:16:31,440 --> 00:16:37,279
um so um some of the lessons

00:16:34,800 --> 00:16:38,560
learned from this um and if you just

00:16:37,279 --> 00:16:39,759
give me one second here some of the

00:16:38,560 --> 00:16:40,880
lessons that we learned from this

00:16:39,759 --> 00:16:42,399
was that it's important to have a

00:16:40,880 --> 00:16:43,199
failure mechanism for long-running

00:16:42,399 --> 00:16:46,399
documents

00:16:43,199 --> 00:16:49,759
um we've found that depending on um

00:16:46,399 --> 00:16:51,120
see depending on sea takes um set up

00:16:49,759 --> 00:16:52,959
the version i know there are some

00:16:51,120 --> 00:16:55,600
correct there are probably some fixes on

00:16:52,959 --> 00:16:57,120
on trunk for this but um you know by

00:16:55,600 --> 00:16:59,120
default the default clinical pipeline

00:16:57,120 --> 00:17:01,279
can get hung up on longer documents

00:16:59,120 --> 00:17:03,040
and um you don't want that to bring down

00:17:01,279 --> 00:17:05,839
the entire job and so we

00:17:03,040 --> 00:17:07,120
we have a procedure for uh if if a node

00:17:05,839 --> 00:17:09,280
is taking too long

00:17:07,120 --> 00:17:10,720
uh stopping it and just annotate just

00:17:09,280 --> 00:17:12,240
sending setting um

00:17:10,720 --> 00:17:14,959
recording that that there was a failure

00:17:12,240 --> 00:17:19,439
for manual review later on

00:17:14,959 --> 00:17:21,520
um all notes are not the same

00:17:19,439 --> 00:17:22,640
uh so what is what we're trying to say

00:17:21,520 --> 00:17:26,240
here is that

00:17:22,640 --> 00:17:29,200
um the a cluster and

00:17:26,240 --> 00:17:30,240
sort of architecture for a given uh

00:17:29,200 --> 00:17:32,799
spark cluster that

00:17:30,240 --> 00:17:34,240
works for all notes in institution may

00:17:32,799 --> 00:17:35,520
not actually work that well for a

00:17:34,240 --> 00:17:37,919
smaller subset of notes

00:17:35,520 --> 00:17:38,960
if those notes are for example longer

00:17:37,919 --> 00:17:42,000
and the reason

00:17:38,960 --> 00:17:43,760
this came up um through running a set of

00:17:42,000 --> 00:17:46,000
notes on a more complex

00:17:43,760 --> 00:17:47,200
cohort where the notes tend to be longer

00:17:46,000 --> 00:17:49,039
and so the number of

00:17:47,200 --> 00:17:51,760
the number of notes sent to each

00:17:49,039 --> 00:17:53,440
executor um had to be brought down for

00:17:51,760 --> 00:17:54,640
the spark cluster to run successfully or

00:17:53,440 --> 00:17:55,120
to run within a reasonable amount of

00:17:54,640 --> 00:17:57,360
time

00:17:55,120 --> 00:17:59,120
because those notes were longer whereas

00:17:57,360 --> 00:18:00,000
if they were even if the same notes were

00:17:59,120 --> 00:18:01,200
included in a larger

00:18:00,000 --> 00:18:03,039
cohort distribute they would have been

00:18:01,200 --> 00:18:05,200
distributed across um

00:18:03,039 --> 00:18:07,039
a larger cluster and they wouldn't have

00:18:05,200 --> 00:18:09,840
been focused on individual nodes

00:18:07,039 --> 00:18:10,240
or in individual executors uh and so

00:18:09,840 --> 00:18:11,840
they

00:18:10,240 --> 00:18:14,320
would have it would have lessened effect

00:18:11,840 --> 00:18:16,080
on that cluster overall so um

00:18:14,320 --> 00:18:18,160
that's one of the reasons we had to make

00:18:16,080 --> 00:18:21,440
that that setting available

00:18:18,160 --> 00:18:22,400
to exchange given given each job and

00:18:21,440 --> 00:18:22,960
that's something you have to think about

00:18:22,400 --> 00:18:26,160
when you're

00:18:22,960 --> 00:18:29,520
when you're running um ctx

00:18:26,160 --> 00:18:30,880
uh and spark on a number of on different

00:18:29,520 --> 00:18:33,200
types of jobs

00:18:30,880 --> 00:18:35,200
um we also were sort of looking at the

00:18:33,200 --> 00:18:35,600
max spark cluster size i mean and we

00:18:35,200 --> 00:18:38,000
sort of

00:18:35,600 --> 00:18:40,000
look at this application specific um and

00:18:38,000 --> 00:18:41,520
and jared has referred to the fact that

00:18:40,000 --> 00:18:43,760
um c takes his

00:18:41,520 --> 00:18:46,000
single threaded and that you know there

00:18:43,760 --> 00:18:49,280
are ways um and annotators

00:18:46,000 --> 00:18:52,000
that are that allow for

00:18:49,280 --> 00:18:53,280
um potential use of multiple threads

00:18:52,000 --> 00:18:55,360
within a single process

00:18:53,280 --> 00:18:56,400
but one of the um one of the things we

00:18:55,360 --> 00:18:58,000
were trying to do

00:18:56,400 --> 00:19:00,240
is run c takes as close to out of the

00:18:58,000 --> 00:19:03,120
box as we could for researchers

00:19:00,240 --> 00:19:04,799
um and with a specific version of ct so

00:19:03,120 --> 00:19:07,919
we were trying to use the code almost as

00:19:04,799 --> 00:19:10,799
is and with that there are restrictions

00:19:07,919 --> 00:19:11,520
uh the process restrictions and because

00:19:10,799 --> 00:19:13,679
of that

00:19:11,520 --> 00:19:15,360
we had a sort of unusual spark cluster

00:19:13,679 --> 00:19:18,720
in that our

00:19:15,360 --> 00:19:21,360
our um the the the the

00:19:18,720 --> 00:19:22,400
the executors were all basically given

00:19:21,360 --> 00:19:25,039
their own

00:19:22,400 --> 00:19:25,919
uh cpu on each node which is not

00:19:25,039 --> 00:19:29,039
normally the way

00:19:25,919 --> 00:19:30,640
a um a smart cluster is put together

00:19:29,039 --> 00:19:32,559
usually you know you can have multiple

00:19:30,640 --> 00:19:35,360
executors on a single cpu

00:19:32,559 --> 00:19:36,080
and as a result um we were running into

00:19:35,360 --> 00:19:39,840
a bit of a

00:19:36,080 --> 00:19:41,360
bottleneck um whereas we couldn't have

00:19:39,840 --> 00:19:42,320
quite as large a cluster as we were

00:19:41,360 --> 00:19:45,600
expecting

00:19:42,320 --> 00:19:47,360
because the orchestration node

00:19:45,600 --> 00:19:49,200
had to do it seemed like the

00:19:47,360 --> 00:19:51,200
orchestration node had to do much more

00:19:49,200 --> 00:19:52,640
um talking back and forth because it had

00:19:51,200 --> 00:19:54,559
so many more executors

00:19:52,640 --> 00:19:55,840
than would typically be necessary so

00:19:54,559 --> 00:19:59,039
while i believe

00:19:55,840 --> 00:20:00,559
the um the the spark

00:19:59,039 --> 00:20:02,080
frequently asked questions talks about a

00:20:00,559 --> 00:20:06,720
large cluster being

00:20:02,080 --> 00:20:09,760
8 000 nodes we actually ran into um

00:20:06,720 --> 00:20:11,600
you know a um a much smaller

00:20:09,760 --> 00:20:13,679
we android problems in a much smaller uh

00:20:11,600 --> 00:20:14,960
level size cluster but it had many

00:20:13,679 --> 00:20:17,120
executors

00:20:14,960 --> 00:20:18,640
and also as i kind of mentioned i'm

00:20:17,120 --> 00:20:21,679
using a workflow engine

00:20:18,640 --> 00:20:22,080
that allowed you to to sort of to draw

00:20:21,679 --> 00:20:25,600
out

00:20:22,080 --> 00:20:26,240
your uh your pipeline is is very helpful

00:20:25,600 --> 00:20:29,600
because it

00:20:26,240 --> 00:20:32,840
um it makes debugging easier uh rather

00:20:29,600 --> 00:20:34,559
than again just some more distributed um

00:20:32,840 --> 00:20:37,360
workflow

00:20:34,559 --> 00:20:37,360
you go next slide

00:20:38,559 --> 00:20:42,960
so future directions i mean our our main

00:20:41,039 --> 00:20:44,159
goal here as i put in the last bullet is

00:20:42,960 --> 00:20:48,559
to is to make s

00:20:44,159 --> 00:20:50,480
using knee takes um uh

00:20:48,559 --> 00:20:51,840
for for researchers potentially

00:20:50,480 --> 00:20:54,480
non-technical uh

00:20:51,840 --> 00:20:55,440
people as easy as possible and so um

00:20:54,480 --> 00:20:57,520
we're looking at additional

00:20:55,440 --> 00:20:58,480
customizations that users may want for a

00:20:57,520 --> 00:20:59,679
particular run

00:20:58,480 --> 00:21:01,840
and so some of the things we're looking

00:20:59,679 --> 00:21:03,760
at are being able to change the main

00:21:01,840 --> 00:21:05,360
character span that an entity

00:21:03,760 --> 00:21:07,200
that an entity can be the default is

00:21:05,360 --> 00:21:08,880
three but in some runs depending on what

00:21:07,200 --> 00:21:10,159
you're looking for you may want to go

00:21:08,880 --> 00:21:11,919
lower like two depending on if you're

00:21:10,159 --> 00:21:15,520
looking for abbreviations or something

00:21:11,919 --> 00:21:17,679
um we're also looking at uh um

00:21:15,520 --> 00:21:19,280
changing the ability to to set the

00:21:17,679 --> 00:21:22,400
exclusion tags parameter for

00:21:19,280 --> 00:21:23,520
the um jks annotator so that you could

00:21:22,400 --> 00:21:25,120
determine

00:21:23,520 --> 00:21:26,880
you could determine for each run whether

00:21:25,120 --> 00:21:28,640
or not you wanted um

00:21:26,880 --> 00:21:30,240
certain parts of speech to be able to

00:21:28,640 --> 00:21:33,919
contain anti-dimensions

00:21:30,240 --> 00:21:36,960
and um i saw a presentation from sean

00:21:33,919 --> 00:21:38,159
uh earlier this week that there is that

00:21:36,960 --> 00:21:39,840
that setting um

00:21:38,159 --> 00:21:41,360
is sort of being simplified a little bit

00:21:39,840 --> 00:21:42,400
in the future and so it might be

00:21:41,360 --> 00:21:43,039
interested to go back and look at that

00:21:42,400 --> 00:21:45,679
presentation

00:21:43,039 --> 00:21:46,400
so i'm sure that that will be uh helpful

00:21:45,679 --> 00:21:48,000
um we also

00:21:46,400 --> 00:21:49,440
might be looking into adding some custom

00:21:48,000 --> 00:21:51,600
trained annotation engines

00:21:49,440 --> 00:21:52,799
um as we sort of explore the idea of

00:21:51,600 --> 00:21:54,480
creating um

00:21:52,799 --> 00:21:56,799
machine learning models here on our own

00:21:54,480 --> 00:21:56,799
data

00:21:57,679 --> 00:22:00,720
um limitations you know see we have run

00:22:00,400 --> 00:22:02,320
into

00:22:00,720 --> 00:22:03,840
some some issues with c takes on high

00:22:02,320 --> 00:22:04,559
density notes and again i think there

00:22:03,840 --> 00:22:07,360
are some

00:22:04,559 --> 00:22:08,480
um some bug fixes on trunk for that but

00:22:07,360 --> 00:22:09,600
um

00:22:08,480 --> 00:22:10,960
we weren't able we're not able to

00:22:09,600 --> 00:22:12,159
process all nodes it's actually a very

00:22:10,960 --> 00:22:13,840
small number of nodes that run into a

00:22:12,159 --> 00:22:15,280
problem but that was limitation

00:22:13,840 --> 00:22:17,440
um this does require substantial

00:22:15,280 --> 00:22:18,720
resources um you know there's a fairly

00:22:17,440 --> 00:22:19,760
large cluster that we turned up and

00:22:18,720 --> 00:22:23,120
turned down

00:22:19,760 --> 00:22:24,799
um and again as you know caveat here

00:22:23,120 --> 00:22:28,080
this is all based on a single pediatric

00:22:24,799 --> 00:22:28,080
institution where we did this work

00:22:29,039 --> 00:22:34,080
uh so in conclusion you know we were we

00:22:32,400 --> 00:22:36,720
were able to um

00:22:34,080 --> 00:22:38,400
use c takes to deploy i'm sorry use

00:22:36,720 --> 00:22:41,200
spark to deploy c takes

00:22:38,400 --> 00:22:42,960
um basically as is without without

00:22:41,200 --> 00:22:46,240
really changing any of the underlying

00:22:42,960 --> 00:22:47,440
see takes code um and we've

00:22:46,240 --> 00:22:49,280
we've found that it's important to be

00:22:47,440 --> 00:22:52,320
able to tweak settings

00:22:49,280 --> 00:22:54,400
in the uh default pipeline

00:22:52,320 --> 00:22:55,679
um depending on the needs i've given to

00:22:54,400 --> 00:22:56,400
researchers and that's been really

00:22:55,679 --> 00:22:57,679
important

00:22:56,400 --> 00:22:59,440
and and the focus of some of the work

00:22:57,679 --> 00:23:00,720
that i just discussed um

00:22:59,440 --> 00:23:03,360
and it's also helped us sort of

00:23:00,720 --> 00:23:04,880
efficiently iterate um

00:23:03,360 --> 00:23:06,720
it's also important for us to officially

00:23:04,880 --> 00:23:09,280
iterate on different pipelines

00:23:06,720 --> 00:23:11,840
uh within within uh research at an

00:23:09,280 --> 00:23:11,840
institution

00:23:12,240 --> 00:23:14,799
finally just a couple of

00:23:13,039 --> 00:23:16,720
acknowledgements here evanette burroughs

00:23:14,799 --> 00:23:17,440
and bill flynn from our team were very

00:23:16,720 --> 00:23:18,960
helpful

00:23:17,440 --> 00:23:20,320
um in helping us to architect this

00:23:18,960 --> 00:23:20,720
solution and we'd also just like to

00:23:20,320 --> 00:23:22,960
thank

00:23:20,720 --> 00:23:23,919
thank the seat takes community um that

00:23:22,960 --> 00:23:25,840
is always

00:23:23,919 --> 00:23:27,520
very very helpful on message boards and

00:23:25,840 --> 00:23:30,960
we probably would not have been able to

00:23:27,520 --> 00:23:33,039
get this done without them um and

00:23:30,960 --> 00:23:34,000
um i think with that i just also want to

00:23:33,039 --> 00:23:37,440
thank everyone for

00:23:34,000 --> 00:23:39,760
uh their time today and attending this

00:23:37,440 --> 00:23:39,760
session

00:23:42,320 --> 00:23:48,240
thank you that was very good

00:23:45,360 --> 00:23:50,159
there are a series of questions in the

00:23:48,240 --> 00:23:51,520
chat box i don't know if you

00:23:50,159 --> 00:23:53,360
are reading them and just want to take

00:23:51,520 --> 00:23:56,640
them yourself or if you want me to

00:23:53,360 --> 00:23:59,919
uh run through them for you i can

00:23:56,640 --> 00:24:02,960
i can read them um so

00:23:59,919 --> 00:24:04,400
i the first question i'm not we didn't

00:24:02,960 --> 00:24:07,760
split up notes across

00:24:04,400 --> 00:24:08,559
you know across nodes so i'm not sure

00:24:07,760 --> 00:24:10,240
exactly

00:24:08,559 --> 00:24:11,120
peter what you're i'm not exactly sure i

00:24:10,240 --> 00:24:13,039
understand that question but if you

00:24:11,120 --> 00:24:13,760
could if you wanted to clarify it maybe

00:24:13,039 --> 00:24:18,960
we could

00:24:13,760 --> 00:24:19,520
answer that one um dr sevilla yes we

00:24:18,960 --> 00:24:22,799
could

00:24:19,520 --> 00:24:27,279
um definitely share uh

00:24:22,799 --> 00:24:30,159
slides from the presentation um

00:24:27,279 --> 00:24:31,679
so we work with a number of um another

00:24:30,159 --> 00:24:32,159
question about phi requirements on the

00:24:31,679 --> 00:24:34,720
cluster

00:24:32,159 --> 00:24:35,440
we work with a number of cloud providers

00:24:34,720 --> 00:24:39,360
uh but we

00:24:35,440 --> 00:24:46,960
have um the necessary uh

00:24:39,360 --> 00:24:49,760
vas in place to to use them um

00:24:46,960 --> 00:24:51,440
other questions this is about c takes in

00:24:49,760 --> 00:24:52,080
generally um hasn't been updated

00:24:51,440 --> 00:24:55,360
recently

00:24:52,080 --> 00:24:56,320
so ctx 4.00 the last sort of tagged

00:24:55,360 --> 00:25:00,559
version has

00:24:56,320 --> 00:25:03,440
been um it is you know from 2017

00:25:00,559 --> 00:25:04,000
the code on the the trunk and subversion

00:25:03,440 --> 00:25:06,400
is

00:25:04,000 --> 00:25:08,559
consistently updated um and you kind of

00:25:06,400 --> 00:25:09,200
have to watch it to see new developments

00:25:08,559 --> 00:25:11,520
although

00:25:09,200 --> 00:25:13,919
um from sean's earlier presentations

00:25:11,520 --> 00:25:16,640
this week you could get a um

00:25:13,919 --> 00:25:17,520
a a preview of some very cool

00:25:16,640 --> 00:25:19,279
developments

00:25:17,520 --> 00:25:20,880
um and i'm not exactly sure but i

00:25:19,279 --> 00:25:21,440
believe i believe sean if you could

00:25:20,880 --> 00:25:24,080
answer this i

00:25:21,440 --> 00:25:25,760
i think there may be another version at

00:25:24,080 --> 00:25:28,400
least in early stages of

00:25:25,760 --> 00:25:30,240
release um but you you can you know

00:25:28,400 --> 00:25:31,279
compile the code from the from the trunk

00:25:30,240 --> 00:25:35,360
at any time

00:25:31,279 --> 00:25:36,559
um we we tend to use 4.00 because

00:25:35,360 --> 00:25:38,240
we're working in research and we

00:25:36,559 --> 00:25:40,559
actually want researchers to be able to

00:25:38,240 --> 00:25:43,039
cite something specific that they used

00:25:40,559 --> 00:25:43,679
um and so we sort of have stuck with

00:25:43,039 --> 00:25:46,760
that

00:25:43,679 --> 00:25:56,000
uh purposefully that answers that

00:25:46,760 --> 00:25:59,919
question um

00:25:56,000 --> 00:25:59,919
so happy to chat afterwards uh

00:26:00,159 --> 00:26:02,720
about this

00:26:03,440 --> 00:26:07,039
and jeff this is this jared i just

00:26:04,960 --> 00:26:09,679
wanted to jump in for peter's question

00:26:07,039 --> 00:26:11,840
in terms of uh splitting across uh

00:26:09,679 --> 00:26:14,559
additional notes so that was

00:26:11,840 --> 00:26:16,400
that was i guess a poor a poor diagram

00:26:14,559 --> 00:26:18,000
uh in terms of its representation those

00:26:16,400 --> 00:26:18,720
are all supposed to be single notes that

00:26:18,000 --> 00:26:20,880
did get

00:26:18,720 --> 00:26:22,240
split um so like an entire note is going

00:26:20,880 --> 00:26:24,240
to be processed

00:26:22,240 --> 00:26:26,159
by one one c takes engine so we're not

00:26:24,240 --> 00:26:29,360
necessarily splitting the notes

00:26:26,159 --> 00:26:32,480
up though that has been

00:26:29,360 --> 00:26:35,360
something we've been doing in a

00:26:32,480 --> 00:26:36,640
hopefully responsible way for those

00:26:35,360 --> 00:26:39,440
notes that do take

00:26:36,640 --> 00:26:41,279
substantially longer to process don't

00:26:39,440 --> 00:26:42,559
sit and try to process for

00:26:41,279 --> 00:26:45,440
you know hours at a time you know

00:26:42,559 --> 00:26:46,799
overnight we're trying to find if there

00:26:45,440 --> 00:26:48,159
is a marker where we could split on you

00:26:46,799 --> 00:26:49,520
know multiple carriage returns or

00:26:48,159 --> 00:26:51,919
something like that so then

00:26:49,520 --> 00:26:52,720
you may not have the reference problems

00:26:51,919 --> 00:26:55,760
that you're

00:26:52,720 --> 00:26:55,760
you're you're speaking of

00:26:56,320 --> 00:27:00,080
hope that answers your question

00:27:03,200 --> 00:27:11,120
another other questions yeah

00:27:08,400 --> 00:27:13,279
yeah getting that right peter's

00:27:11,120 --> 00:27:15,760
suggesting by section maybe a section or

00:27:13,279 --> 00:27:18,000
you know potentially sentence um but

00:27:15,760 --> 00:27:22,320
yeah

00:27:18,000 --> 00:27:22,320
other any other questions

00:27:25,279 --> 00:27:28,640
and we're also we're also hoping to

00:27:26,880 --> 00:27:29,520
contribute back to the the sea takes

00:27:28,640 --> 00:27:30,880
community the

00:27:29,520 --> 00:27:32,559
you know the code that we we've written

00:27:30,880 --> 00:27:34,720
for this it's just a matter of

00:27:32,559 --> 00:27:36,480
uh taking some time to clean it up a bit

00:27:34,720 --> 00:27:37,760
to remove any chalk related information

00:27:36,480 --> 00:27:39,520
et cetera et cetera

00:27:37,760 --> 00:27:40,960
i guess there's a question how often do

00:27:39,520 --> 00:27:44,320
we run these extractions

00:27:40,960 --> 00:27:45,360
so our initial um our initial plan was

00:27:44,320 --> 00:27:47,600
to run it nightly

00:27:45,360 --> 00:27:48,480
uh we're gonna do one large job to catch

00:27:47,600 --> 00:27:50,720
up um

00:27:48,480 --> 00:27:51,760
in the past and run it nightly um and we

00:27:50,720 --> 00:27:54,320
actually haven't

00:27:51,760 --> 00:27:55,760
we haven't put that into practicing that

00:27:54,320 --> 00:27:57,919
we run every single night

00:27:55,760 --> 00:28:00,000
because what actually became more useful

00:27:57,919 --> 00:28:01,440
was more immediately useful for our work

00:28:00,000 --> 00:28:03,679
was researchers requesting

00:28:01,440 --> 00:28:05,360
the specific specific larger cohorts

00:28:03,679 --> 00:28:07,679
with specific requirements

00:28:05,360 --> 00:28:08,880
and so we will run those you know on

00:28:07,679 --> 00:28:11,200
demand for users

00:28:08,880 --> 00:28:12,880
and i think in the future we will be

00:28:11,200 --> 00:28:13,200
doing a nightly extraction as we build

00:28:12,880 --> 00:28:15,840
up

00:28:13,200 --> 00:28:17,600
tools uh within research within within

00:28:15,840 --> 00:28:18,480
our within our research environment for

00:28:17,600 --> 00:28:22,159
researchers

00:28:18,480 --> 00:28:24,720
to sort of help index notes um but um

00:28:22,159 --> 00:28:25,520
right now it's been more focused on the

00:28:24,720 --> 00:28:30,640
uh

00:28:25,520 --> 00:28:33,440
the individual cohorts um

00:28:30,640 --> 00:28:34,640
um and so we've you know we've run a

00:28:33,440 --> 00:28:37,919
number of those

00:28:34,640 --> 00:28:39,600
sort of one-off cohorts for researchers

00:28:37,919 --> 00:28:40,880
the next question says uh do you see

00:28:39,600 --> 00:28:42,320
researchers using extracts in

00:28:40,880 --> 00:28:43,279
conjunction with the notes themselves or

00:28:42,320 --> 00:28:45,760
structured data

00:28:43,279 --> 00:28:47,360
um we have both we have that happens

00:28:45,760 --> 00:28:47,679
both it depends on the project depends

00:28:47,360 --> 00:28:49,600
on

00:28:47,679 --> 00:28:50,720
it depends on what the research is

00:28:49,600 --> 00:28:53,039
trying to do i mean

00:28:50,720 --> 00:28:54,559
usually they use in conjunction with the

00:28:53,039 --> 00:28:57,279
notes i might i think

00:28:54,559 --> 00:28:58,240
i think the reality in reality you know

00:28:57,279 --> 00:29:00,480
c takes

00:28:58,240 --> 00:29:01,440
is good but it is not 100 accurate and

00:29:00,480 --> 00:29:02,640
so getting a

00:29:01,440 --> 00:29:05,200
it's important for researchers to get a

00:29:02,640 --> 00:29:06,720
sense of how accurate um

00:29:05,200 --> 00:29:08,320
things are and often one of one of the

00:29:06,720 --> 00:29:10,320
things we're looking at is you know how

00:29:08,320 --> 00:29:11,679
can we help improve accuracy custom and

00:29:10,320 --> 00:29:12,480
customize the pipeline for us so when

00:29:11,679 --> 00:29:15,919
researchers

00:29:12,480 --> 00:29:18,399
are looking at both it is helpful um

00:29:15,919 --> 00:29:19,679
uh and also structured data i mean also

00:29:18,399 --> 00:29:21,679
the answer could be both they might also

00:29:19,679 --> 00:29:24,159
use other structured data with the nodes

00:29:21,679 --> 00:29:24,159
themselves

00:29:25,520 --> 00:29:29,440
um so the question is do you have

00:29:27,440 --> 00:29:30,640
customized pipelines by project

00:29:29,440 --> 00:29:32,880
uh well how do you keep track of

00:29:30,640 --> 00:29:34,640
different pipelines so the um

00:29:32,880 --> 00:29:36,080
we the way that the way that it works

00:29:34,640 --> 00:29:39,360
right now is

00:29:36,080 --> 00:29:42,960
we have a jenkins job

00:29:39,360 --> 00:29:43,600
that that you fill in the details of

00:29:42,960 --> 00:29:46,159
your

00:29:43,600 --> 00:29:47,440
um of your pipeline with in within

00:29:46,159 --> 00:29:49,200
reason it has to be

00:29:47,440 --> 00:29:51,520
some of the settings that i mentioned

00:29:49,200 --> 00:29:52,399
and then that jenkins job results in the

00:29:51,520 --> 00:29:54,399
creation

00:29:52,399 --> 00:29:55,919
uh automatically of a piper file that

00:29:54,399 --> 00:29:58,480
meets those needs

00:29:55,919 --> 00:29:59,120
and then runs the and then runs it uh

00:29:58,480 --> 00:30:00,720
and the

00:29:59,120 --> 00:30:02,240
all these things are also recorded in

00:30:00,720 --> 00:30:03,600
shared storage so the pipeline is saved

00:30:02,240 --> 00:30:04,399
somewhere the dictionary is saved

00:30:03,600 --> 00:30:07,279
somewhere

00:30:04,399 --> 00:30:09,360
um and the results are saved somewhere

00:30:07,279 --> 00:30:10,159
and so all that is sort of saved on the

00:30:09,360 --> 00:30:13,919
fly

00:30:10,159 --> 00:30:17,520
uh for reference later on um

00:30:13,919 --> 00:30:17,520
does that answer your question

00:30:21,200 --> 00:30:25,039
so basically we keep track of code that

00:30:22,880 --> 00:30:27,200
then writes these different pipelines

00:30:25,039 --> 00:30:28,480
um and again it's sort of starting from

00:30:27,200 --> 00:30:29,840
the default clinical pipeline and you're

00:30:28,480 --> 00:30:37,200
kind of diffing off of it by answering

00:30:29,840 --> 00:30:38,640
those questions

00:30:37,200 --> 00:30:40,240
that is something we're actively working

00:30:38,640 --> 00:30:41,840
on right now um

00:30:40,240 --> 00:30:44,559
this is part of a larger project at our

00:30:41,840 --> 00:30:48,480
institution to uh enable research

00:30:44,559 --> 00:30:51,919
and we are we are looking at um you know

00:30:48,480 --> 00:30:53,039
providing visualizations um to to show

00:30:51,919 --> 00:30:54,799
how accurate they are

00:30:53,039 --> 00:30:56,240
and we're also looking at actually just

00:30:54,799 --> 00:30:57,360
giving some measurements on how accurate

00:30:56,240 --> 00:30:58,880
c takes is by

00:30:57,360 --> 00:31:01,200
uh running it on some notes for which we

00:30:58,880 --> 00:31:03,360
have had annotators annotate

00:31:01,200 --> 00:31:04,880
but as of right now no uh we are

00:31:03,360 --> 00:31:05,840
actively working on that though

00:31:04,880 --> 00:31:07,440
that question was do you have a suite of

00:31:05,840 --> 00:31:10,320
visualization tools to go with output to

00:31:07,440 --> 00:31:13,519
measure the quantity extraction

00:31:10,320 --> 00:31:14,960
um the question is do you always use

00:31:13,519 --> 00:31:18,000
stripped down dictionaries

00:31:14,960 --> 00:31:21,440
um um

00:31:18,000 --> 00:31:24,720
do we always the

00:31:21,440 --> 00:31:26,480
this this differs um project by project

00:31:24,720 --> 00:31:28,399
a researcher can specify what they're

00:31:26,480 --> 00:31:29,840
interested in uh they can bring their

00:31:28,399 --> 00:31:34,080
own dictionary too

00:31:29,840 --> 00:31:35,360
um we chose hp when when jarrett's

00:31:34,080 --> 00:31:37,840
the discussion that jared was talking

00:31:35,360 --> 00:31:41,440
about we used hbo because

00:31:37,840 --> 00:31:42,320
um because it is often used in research

00:31:41,440 --> 00:31:45,120
and genetics

00:31:42,320 --> 00:31:46,000
and from a standpoint of annotations was

00:31:45,120 --> 00:31:47,679
most useful

00:31:46,000 --> 00:31:50,000
to a wide variety of researchers it made

00:31:47,679 --> 00:31:51,840
a lot of sense and frankly it was

00:31:50,000 --> 00:31:53,279
sort of an easier approach an easier

00:31:51,840 --> 00:31:55,360
thing to accomplish

00:31:53,279 --> 00:31:57,440
on a large set of notes than a very

00:31:55,360 --> 00:32:01,120
large dictionary based on snomed

00:31:57,440 --> 00:32:04,399
um so it does vary we have run

00:32:01,120 --> 00:32:06,000
cohort jobs that do use not that

00:32:04,399 --> 00:32:08,080
that do use effectively a much larger

00:32:06,000 --> 00:32:10,399
dictionary though um

00:32:08,080 --> 00:32:12,559
annotation tools what are any favorites

00:32:10,399 --> 00:32:15,519
yes we did a deep dive on that

00:32:12,559 --> 00:32:17,519
um and we eventually arrived at the

00:32:15,519 --> 00:32:20,159
classic rat annotator because

00:32:17,519 --> 00:32:23,519
it just does it all um a lot of other

00:32:20,159 --> 00:32:25,620
annotators have a more modern interface

00:32:23,519 --> 00:32:26,840
uh more modern sort of um

00:32:25,620 --> 00:32:30,159
[Music]

00:32:26,840 --> 00:32:33,360
uh potentially more recently designed

00:32:30,159 --> 00:32:36,640
but brat um allows for a very nice

00:32:33,360 --> 00:32:38,480
uh customization and allows

00:32:36,640 --> 00:32:40,080
very nice relations to pin annotations

00:32:38,480 --> 00:32:41,760
which we're not necessarily using in c

00:32:40,080 --> 00:32:43,360
takes right now but plan on using for

00:32:41,760 --> 00:32:46,799
other projects and so

00:32:43,360 --> 00:32:52,000
we have um we have uh brat

00:32:46,799 --> 00:32:52,000
um uh england within our within our

00:32:54,840 --> 00:32:57,840
organization

00:32:58,880 --> 00:33:04,240
no problem uh i don't know

00:33:02,000 --> 00:33:05,440
is there a time check it certainly i

00:33:04,240 --> 00:33:06,880
think we have a little bit more time if

00:33:05,440 --> 00:33:08,799
there's any other questions and we're

00:33:06,880 --> 00:33:13,760
happy to chat again afterwards

00:33:08,799 --> 00:33:15,200
um or outside of this discussion i'm not

00:33:13,760 --> 00:33:18,480
i'm not sure how long we could hang out

00:33:15,200 --> 00:33:18,480
in this room but um

00:33:18,799 --> 00:33:22,080
here i don't know with anything i kind

00:33:20,159 --> 00:33:23,200
of took all the answers there

00:33:22,080 --> 00:33:26,399
anything you wanted to add to any of

00:33:23,200 --> 00:33:26,399
those answers please

00:33:26,960 --> 00:33:30,320
no that's great i think i can answer

00:33:28,480 --> 00:33:33,120
them very thoroughly it's perfect

00:33:30,320 --> 00:33:33,919
um i i think the slides for the

00:33:33,120 --> 00:33:36,159
presentation i

00:33:33,919 --> 00:33:38,000
uploaded then yesterday i believe but i

00:33:36,159 --> 00:33:39,279
made some modifications so i'll try to

00:33:38,000 --> 00:33:41,600
go back in and actually

00:33:39,279 --> 00:33:42,559
upload a more more recent version but if

00:33:41,600 --> 00:33:44,640
you do have

00:33:42,559 --> 00:33:46,240
any questions or or would like to

00:33:44,640 --> 00:33:48,320
connect more directly feel free to reach

00:33:46,240 --> 00:33:52,000
out to us

00:33:48,320 --> 00:33:54,080
at any time

00:33:52,000 --> 00:33:55,840
all right well yeah i think our time is

00:33:54,080 --> 00:33:58,480
up thank you both that was

00:33:55,840 --> 00:34:00,399
really interesting uh it was also neat

00:33:58,480 --> 00:34:01,919
to see the two of you uh tag team off of

00:34:00,399 --> 00:34:06,559
each other like that

00:34:01,919 --> 00:34:09,440
um and cover several items all in

00:34:06,559 --> 00:34:10,560
a very short amount of time all right

00:34:09,440 --> 00:34:12,639
thanks john

00:34:10,560 --> 00:34:21,839
appreciate it thank you for moderating

00:34:12,639 --> 00:34:21,839
thank you

00:34:32,800 --> 00:34:34,879

YouTube URL: https://www.youtube.com/watch?v=kZw42pGzyHs


