Title: Improving Search Availability: Striving for more 9s
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Search
Description: 
	Improving Search Availability: Striving for more 9s
Shubhro Roy

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Availability is a critical aspect of any distributed system, especially when your customer's mission critical applications depend on it. But what does availability really mean for Search and how do we measure it? Once measured how do we ensure a multi-cluster deployment of Apache Solr with terabyte scale sharded inverted index hits the holy grail of 4 9s of availability ? How do we automatically detect failures with such systems and what are our options to handle and recover from such failures without human intervention ? In this talk we will discuss various architectural choices and deployment strategies we have adopted at Box to improve availability of search while supporting high-throughput, near real-time indexing, low latency and multi-tenancy. We will share our learnings from various issues we have faced running Solr at scale and how we have address them by building additional scaffolding or tweaking Solr itself. Come take a peek under the hood of Box Search.

Shubhro enjoys working with data at scale, be it indexing, mining or analyzing it. Currently he is part of the Search team at Box, building infrastructure components that enable millions of users to find relevant content. Prior to Box, Shubhro worked on full text database search at Oracle. He has been working on enterprise search and data discovery for the past 8 years after graduating from Carnegie Mellon University with Masters in Information Systems, specializing in Information Retrieval and Machine Learning.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:24,080 --> 00:00:27,599
hello everyone

00:00:25,359 --> 00:00:30,000
welcome to the search track uh at

00:00:27,599 --> 00:00:32,880
apachecon at home 2020.

00:00:30,000 --> 00:00:34,160
uh today we have with us shibro roy from

00:00:32,880 --> 00:00:36,239
box

00:00:34,160 --> 00:00:37,840
he's been working on enterprise search

00:00:36,239 --> 00:00:38,719
and data discovery for the past eight

00:00:37,840 --> 00:00:40,800
years

00:00:38,719 --> 00:00:42,719
and is currently involved in building

00:00:40,800 --> 00:00:45,360
infrastructure components that enable

00:00:42,719 --> 00:00:46,160
millions of users to find relevant

00:00:45,360 --> 00:00:48,480
content

00:00:46,160 --> 00:00:50,800
stored in box shubra will be talking

00:00:48,480 --> 00:00:53,440
about improving search availability

00:00:50,800 --> 00:00:55,120
by striving for more lines i'm looking

00:00:53,440 --> 00:00:58,160
forward to learning more about

00:00:55,120 --> 00:01:00,239
measuring and improving the availability

00:00:58,160 --> 00:01:01,199
and getting a peek under the hood of box

00:01:00,239 --> 00:01:04,160
search uh

00:01:01,199 --> 00:01:04,160
over to you shibro

00:01:04,559 --> 00:01:12,320
hi everyone uh welcome to

00:01:08,000 --> 00:01:16,960
this talk today uh

00:01:12,320 --> 00:01:18,560
sorry let me turn off

00:01:16,960 --> 00:01:20,080
uh my speaker sorry i was getting

00:01:18,560 --> 00:01:23,280
feedback uh

00:01:20,080 --> 00:01:25,119
so welcome to this uh talk today on

00:01:23,280 --> 00:01:28,560
improving search availability

00:01:25,119 --> 00:01:31,119
uh i'm shubro and i work as a staff

00:01:28,560 --> 00:01:34,079
engineer on the box search team

00:01:31,119 --> 00:01:34,400
uh for those of you who are not familiar

00:01:34,079 --> 00:01:37,840
with

00:01:34,400 --> 00:01:38,799
box uh it is a cloud content management

00:01:37,840 --> 00:01:42,560
platform

00:01:38,799 --> 00:01:54,159
that enables users to securely

00:01:42,560 --> 00:01:57,040
share content and collaborate on

00:01:54,159 --> 00:01:58,560
this platforms it's a box search which

00:01:57,040 --> 00:02:01,680
uh

00:01:58,560 --> 00:02:03,759
uh users to find relevant content uh at

00:02:01,680 --> 00:02:05,840
its core we power this using apache

00:02:03,759 --> 00:02:09,280
solar but we also use a lot of other

00:02:05,840 --> 00:02:12,879
apache technologies uh in our stack

00:02:09,280 --> 00:02:15,200
the index is over uh 800

00:02:12,879 --> 00:02:16,319
terabytes and it contains hundreds of

00:02:15,200 --> 00:02:20,400
billions of files

00:02:16,319 --> 00:02:22,640
and these files are the index is growing

00:02:20,400 --> 00:02:26,080
by millions of files each day

00:02:22,640 --> 00:02:28,400
so today's talk is uh going to be

00:02:26,080 --> 00:02:30,000
about failures uh and the kind of

00:02:28,400 --> 00:02:33,200
failures that happen in systems

00:02:30,000 --> 00:02:35,599
but more importantly how we can improve

00:02:33,200 --> 00:02:36,959
uh our availability in the presence of

00:02:35,599 --> 00:02:39,360
these failures

00:02:36,959 --> 00:02:41,200
uh so we are going to be talking about

00:02:39,360 --> 00:02:43,120
how to measure search availability

00:02:41,200 --> 00:02:44,560
what does availability mean even for a

00:02:43,120 --> 00:02:47,519
system like search

00:02:44,560 --> 00:02:48,000
uh and then uh we'll move on to talking

00:02:47,519 --> 00:02:51,920
about

00:02:48,000 --> 00:02:53,680
how to improve fault tolerance how to

00:02:51,920 --> 00:02:56,080
improve our availability using failure

00:02:53,680 --> 00:02:59,200
detection and automated recovery

00:02:56,080 --> 00:03:02,879
and some failure prevention strategies

00:02:59,200 --> 00:03:04,000
so let's move on to uh measuring search

00:03:02,879 --> 00:03:07,200
availability

00:03:04,000 --> 00:03:09,920
um generally uh systems availability

00:03:07,200 --> 00:03:11,760
is measured uh in terms of the

00:03:09,920 --> 00:03:14,640
percentage of downtime it has

00:03:11,760 --> 00:03:15,040
over uh the entire agreed service time

00:03:14,640 --> 00:03:18,400
uh

00:03:15,040 --> 00:03:21,040
for a cloud platform like box the

00:03:18,400 --> 00:03:21,519
greek service time is basically 24 hours

00:03:21,040 --> 00:03:24,879
a day

00:03:21,519 --> 00:03:28,480
uh seven days a week right uh and

00:03:24,879 --> 00:03:32,239
365 days a year so uh it boils down to

00:03:28,480 --> 00:03:35,120
uh measuring uh downtime so

00:03:32,239 --> 00:03:36,319
what do we mean exactly by downtime of a

00:03:35,120 --> 00:03:39,200
system is it

00:03:36,319 --> 00:03:39,840
simply the number of outage minutes that

00:03:39,200 --> 00:03:43,360
you have

00:03:39,840 --> 00:03:47,280
or is it uh also about how

00:03:43,360 --> 00:03:51,200
um we um look at partial failures or

00:03:47,280 --> 00:03:53,760
uh latency or uh the quality of results

00:03:51,200 --> 00:03:55,840
uh and then uh it's not just a single

00:03:53,760 --> 00:03:56,799
solar cluster that we are looking at but

00:03:55,840 --> 00:03:58,879
we are also

00:03:56,799 --> 00:04:00,319
talking about the availability of this

00:03:58,879 --> 00:04:03,280
entire system

00:04:00,319 --> 00:04:03,920
which can contains multiple components

00:04:03,280 --> 00:04:06,480
it has

00:04:03,920 --> 00:04:07,040
multiple services that need to talk to

00:04:06,480 --> 00:04:10,000
each other

00:04:07,040 --> 00:04:11,280
each having their own failure domains

00:04:10,000 --> 00:04:13,920
each fail differently

00:04:11,280 --> 00:04:14,640
and those failures affect other services

00:04:13,920 --> 00:04:16,959
so

00:04:14,640 --> 00:04:18,720
how do we measure the system uh the

00:04:16,959 --> 00:04:20,560
availability of this entire system and

00:04:18,720 --> 00:04:21,680
not just a single component

00:04:20,560 --> 00:04:23,600
and when we are measuring the

00:04:21,680 --> 00:04:26,479
availability of a system like this

00:04:23,600 --> 00:04:28,479
uh what is important is uh to not fall

00:04:26,479 --> 00:04:32,240
into this trap of measuring

00:04:28,479 --> 00:04:35,520
uh something within the system using

00:04:32,240 --> 00:04:37,280
say internal ping to solar to see if the

00:04:35,520 --> 00:04:40,080
solar cluster is available

00:04:37,280 --> 00:04:42,240
because that metric can lie to us maybe

00:04:40,080 --> 00:04:44,000
the solar cluster is doing fine but

00:04:42,240 --> 00:04:46,320
somewhere upstream there is an issue in

00:04:44,000 --> 00:04:48,240
the indexing pipeline because of which

00:04:46,320 --> 00:04:50,240
we are not indexing any new documents or

00:04:48,240 --> 00:04:51,840
maybe there is a bug on the query side

00:04:50,240 --> 00:04:53,360
because of which we are returning empty

00:04:51,840 --> 00:04:55,919
results to the user

00:04:53,360 --> 00:04:56,880
so uh it basically means the users are

00:04:55,919 --> 00:04:59,360
unhappy

00:04:56,880 --> 00:05:00,240
but our metrics are telling us that the

00:04:59,360 --> 00:05:03,360
availability

00:05:00,240 --> 00:05:05,600
is great so it is important to

00:05:03,360 --> 00:05:07,280
measure the availability from the user's

00:05:05,600 --> 00:05:07,680
perspective because at the end of the

00:05:07,280 --> 00:05:09,680
day

00:05:07,680 --> 00:05:10,800
that is what we are trying to measure

00:05:09,680 --> 00:05:14,400
right

00:05:10,800 --> 00:05:16,240
uh so in that aspect uh at box we

00:05:14,400 --> 00:05:18,560
uh measure availability by defining

00:05:16,240 --> 00:05:20,800
something called a bad minute

00:05:18,560 --> 00:05:22,720
uh a bad minute is basically a minute in

00:05:20,800 --> 00:05:23,759
which we return more than one percent of

00:05:22,720 --> 00:05:25,600
errors to

00:05:23,759 --> 00:05:27,360
users out of the total number of

00:05:25,600 --> 00:05:29,759
requests we got in that minute

00:05:27,360 --> 00:05:30,800
and then uh availability is basically

00:05:29,759 --> 00:05:33,120
defined as

00:05:30,800 --> 00:05:34,000
the total number of bad minutes uh that

00:05:33,120 --> 00:05:36,080
we have in

00:05:34,000 --> 00:05:38,000
any measurement period now this could be

00:05:36,080 --> 00:05:40,720
a week a month a quarter

00:05:38,000 --> 00:05:42,240
or a year uh and then uh when we are

00:05:40,720 --> 00:05:43,680
measuring availability it is important

00:05:42,240 --> 00:05:47,199
to measure it continuously

00:05:43,680 --> 00:05:50,000
uh in a stable manner so to do this

00:05:47,199 --> 00:05:51,759
uh we store all the responses that we

00:05:50,000 --> 00:05:55,440
return to users

00:05:51,759 --> 00:05:58,639
and in an analytic system uh based on

00:05:55,440 --> 00:06:01,840
s3 and build hive tables on top of it so

00:05:58,639 --> 00:06:03,600
you can run uh hive queries to generate

00:06:01,840 --> 00:06:04,880
a report at the end of the quarter for

00:06:03,600 --> 00:06:07,039
your availability

00:06:04,880 --> 00:06:08,880
but we also update this information in

00:06:07,039 --> 00:06:10,560
wavefront so we can build dashboards

00:06:08,880 --> 00:06:12,479
like you can see on the right

00:06:10,560 --> 00:06:14,160
which help us continuously monitor

00:06:12,479 --> 00:06:15,840
availability and then

00:06:14,160 --> 00:06:19,199
we can address if there are any

00:06:15,840 --> 00:06:21,680
availability issues that are happening

00:06:19,199 --> 00:06:23,759
uh so now that we understand how to

00:06:21,680 --> 00:06:24,800
measure availability uh let's look at

00:06:23,759 --> 00:06:27,680
what it

00:06:24,800 --> 00:06:28,720
exactly depends on so obviously it

00:06:27,680 --> 00:06:31,840
depends on

00:06:28,720 --> 00:06:33,680
uh the network communications uh or uh

00:06:31,840 --> 00:06:34,319
the network that exists between services

00:06:33,680 --> 00:06:36,560
so

00:06:34,319 --> 00:06:38,319
uh services need to talk to each other

00:06:36,560 --> 00:06:39,759
and if there is a network partition

00:06:38,319 --> 00:06:42,240
then it is going to hamper your

00:06:39,759 --> 00:06:44,240
availability uh it also depends on the

00:06:42,240 --> 00:06:45,840
actual machines that the software is

00:06:44,240 --> 00:06:48,000
running on and the operating system

00:06:45,840 --> 00:06:51,360
which is managing the file system uh

00:06:48,000 --> 00:06:53,039
the internal caches

00:06:51,360 --> 00:06:55,039
the processes that are running on that

00:06:53,039 --> 00:06:56,800
machine and any issues on this layer is

00:06:55,039 --> 00:06:58,800
going to cause issues

00:06:56,800 --> 00:07:00,560
and finally it depends on the

00:06:58,800 --> 00:07:02,560
application software which is

00:07:00,560 --> 00:07:04,000
either like solar or lucine or

00:07:02,560 --> 00:07:05,520
elasticsearch or

00:07:04,000 --> 00:07:07,039
even your own services that you have

00:07:05,520 --> 00:07:09,840
written on top of these

00:07:07,039 --> 00:07:11,039
uh to build a search system uh in

00:07:09,840 --> 00:07:13,680
today's talk uh

00:07:11,039 --> 00:07:15,599
we'll mainly focus on this uh

00:07:13,680 --> 00:07:16,160
application software and things we can

00:07:15,599 --> 00:07:18,720
do

00:07:16,160 --> 00:07:20,720
in this layer to improve our

00:07:18,720 --> 00:07:22,639
availability by tolerating failure in

00:07:20,720 --> 00:07:24,639
the lower three years

00:07:22,639 --> 00:07:26,479
and uh towards the end i'll talk about

00:07:24,639 --> 00:07:27,919
the two pillars that you see which is

00:07:26,479 --> 00:07:30,319
the processes that

00:07:27,919 --> 00:07:32,319
we use to manage uh search and the

00:07:30,319 --> 00:07:33,840
people that are involved in

00:07:32,319 --> 00:07:35,599
managing search and there are things

00:07:33,840 --> 00:07:36,960
that we can do even at this layer

00:07:35,599 --> 00:07:39,680
to significantly improve our

00:07:36,960 --> 00:07:39,680
availability

00:07:40,160 --> 00:07:45,440
so now let's move on to look at

00:07:43,440 --> 00:07:46,560
some suggestions or strategies that we

00:07:45,440 --> 00:07:48,800
can actually use

00:07:46,560 --> 00:07:50,319
uh to improve availability by tolerating

00:07:48,800 --> 00:07:53,520
faults in the system

00:07:50,319 --> 00:07:55,440
uh the first one is uh most commonly

00:07:53,520 --> 00:07:56,319
used by everyone which is adding more

00:07:55,440 --> 00:07:58,960
redundancy

00:07:56,319 --> 00:08:01,120
uh you can have a cluster in a single

00:07:58,960 --> 00:08:03,120
availability zone and if it goes down

00:08:01,120 --> 00:08:06,639
then you're completely unavailable

00:08:03,120 --> 00:08:09,840
uh the idea is to add more replicas

00:08:06,639 --> 00:08:11,599
in different availability zones and by

00:08:09,840 --> 00:08:13,199
availability zone i mean

00:08:11,599 --> 00:08:15,520
maybe data centers which are

00:08:13,199 --> 00:08:17,120
geo-distributed in such a way so that

00:08:15,520 --> 00:08:18,720
power failure and one data center

00:08:17,120 --> 00:08:21,919
doesn't affect the other

00:08:18,720 --> 00:08:24,879
or this could be regions in a

00:08:21,919 --> 00:08:26,639
public cloud like gcp or aws so one

00:08:24,879 --> 00:08:28,319
region failure doesn't impact clusters

00:08:26,639 --> 00:08:31,440
in another region

00:08:28,319 --> 00:08:33,839
and this is a great way to

00:08:31,440 --> 00:08:35,680
handle availability issues uh what is

00:08:33,839 --> 00:08:36,479
important here is to pick a routing

00:08:35,680 --> 00:08:38,399
strategy so

00:08:36,479 --> 00:08:40,240
either you can have a active passive

00:08:38,399 --> 00:08:41,919
routing strategy where you always send

00:08:40,240 --> 00:08:44,880
the queries to an active cluster

00:08:41,919 --> 00:08:45,920
and in case of uh an issue you fail over

00:08:44,880 --> 00:08:48,800
to the passive one

00:08:45,920 --> 00:08:49,440
uh the other one is a more active active

00:08:48,800 --> 00:08:53,040
strategy

00:08:49,440 --> 00:08:54,000
where you can fan out all your queries

00:08:53,040 --> 00:08:56,160
to all the active

00:08:54,000 --> 00:08:57,600
clusters and this works well if you have

00:08:56,160 --> 00:09:00,399
a query heavy

00:08:57,600 --> 00:09:01,920
uh application like search uh so you can

00:09:00,399 --> 00:09:03,839
get better load balancing

00:09:01,920 --> 00:09:06,000
and then if there is a failure on one of

00:09:03,839 --> 00:09:06,640
the clusters you can simply fail over

00:09:06,000 --> 00:09:09,120
the traffic

00:09:06,640 --> 00:09:10,560
to one of the other clusters now what is

00:09:09,120 --> 00:09:12,800
important here is to ensure

00:09:10,560 --> 00:09:14,160
that you have enough compute capacity on

00:09:12,800 --> 00:09:16,399
each of the clusters to take

00:09:14,160 --> 00:09:17,680
additional traffic when a cluster fails

00:09:16,399 --> 00:09:19,040
because otherwise you can have a

00:09:17,680 --> 00:09:21,920
cascading failure where

00:09:19,040 --> 00:09:23,440
you fail over the traffic to uh to

00:09:21,920 --> 00:09:25,120
another active cluster and that gets

00:09:23,440 --> 00:09:26,080
overwhelmed and that phase and so on and

00:09:25,120 --> 00:09:27,519
so forth so

00:09:26,080 --> 00:09:29,360
that is something important to keep in

00:09:27,519 --> 00:09:32,399
mind

00:09:29,360 --> 00:09:32,959
uh next uh let's look at an individual

00:09:32,399 --> 00:09:35,040
cluster

00:09:32,959 --> 00:09:36,800
so far we looked at how we can add

00:09:35,040 --> 00:09:39,360
redundancy at the cluster level

00:09:36,800 --> 00:09:40,080
uh but we can do something at individual

00:09:39,360 --> 00:09:42,480
clusters

00:09:40,080 --> 00:09:44,160
uh to improve our availability or at

00:09:42,480 --> 00:09:45,920
least improve our chances of being

00:09:44,160 --> 00:09:46,720
partially available by sharding our

00:09:45,920 --> 00:09:50,080
index

00:09:46,720 --> 00:09:52,560
so irrespective of uh the size of

00:09:50,080 --> 00:09:53,760
uh the index so we should always try to

00:09:52,560 --> 00:09:57,040
shard our index

00:09:53,760 --> 00:09:58,800
into smaller shards and then allocate uh

00:09:57,040 --> 00:10:00,800
dedicated resources to them either in

00:09:58,800 --> 00:10:03,120
terms of processes or

00:10:00,800 --> 00:10:05,040
assigning them to a specific machine so

00:10:03,120 --> 00:10:06,320
that when a single machine goes down

00:10:05,040 --> 00:10:09,680
your cluster doesn't

00:10:06,320 --> 00:10:11,680
become unavailable so at box we

00:10:09,680 --> 00:10:14,240
assign shards to machines in such a way

00:10:11,680 --> 00:10:16,320
that a single machine going down doesn't

00:10:14,240 --> 00:10:18,079
make more than one percent of our index

00:10:16,320 --> 00:10:21,120
unavailable so we still

00:10:18,079 --> 00:10:21,839
can solve requests from 99 of uh the

00:10:21,120 --> 00:10:23,360
index

00:10:21,839 --> 00:10:25,040
and then later in the presentation we'll

00:10:23,360 --> 00:10:25,839
talk about how to handle that one person

00:10:25,040 --> 00:10:29,120
failure when

00:10:25,839 --> 00:10:32,240
a machine actually fails right uh

00:10:29,120 --> 00:10:34,480
there are a couple of ways to short

00:10:32,240 --> 00:10:36,079
there is a vertical sharding where you

00:10:34,480 --> 00:10:38,800
basically can charge your index

00:10:36,079 --> 00:10:39,360
uh based on some attribute of the index

00:10:38,800 --> 00:10:41,839
uh

00:10:39,360 --> 00:10:42,800
let's say you could split the index into

00:10:41,839 --> 00:10:45,360
a title

00:10:42,800 --> 00:10:46,000
document title index uh content index

00:10:45,360 --> 00:10:48,720
and then

00:10:46,000 --> 00:10:49,839
uh metadata index and then if you have

00:10:48,720 --> 00:10:53,279
any failures

00:10:49,839 --> 00:10:53,279
then you can simply

00:10:53,839 --> 00:10:57,839
return results from your title index if

00:10:56,560 --> 00:11:00,000
your content indexes fail

00:10:57,839 --> 00:11:02,320
so your quality of search goes down a

00:11:00,000 --> 00:11:04,160
little but you're still highly available

00:11:02,320 --> 00:11:05,920
the other one is horizontal partitioning

00:11:04,160 --> 00:11:07,120
where you basically partition your index

00:11:05,920 --> 00:11:08,959
based on a sharding key

00:11:07,120 --> 00:11:10,399
which could be either hash or range

00:11:08,959 --> 00:11:12,399
partitioning

00:11:10,399 --> 00:11:14,240
and it is also possible to use a

00:11:12,399 --> 00:11:16,320
combination of these like you can

00:11:14,240 --> 00:11:17,760
vertically partition your index and then

00:11:16,320 --> 00:11:21,200
horizontally partition uh

00:11:17,760 --> 00:11:22,800
those vertical brush uh so uh it is

00:11:21,200 --> 00:11:23,360
important though to pick a sharding

00:11:22,800 --> 00:11:25,760
strategy

00:11:23,360 --> 00:11:27,519
that really suits your application and

00:11:25,760 --> 00:11:29,360
uh matches your query pattern

00:11:27,519 --> 00:11:31,279
otherwise you can have availability

00:11:29,360 --> 00:11:32,320
issues just because of a bad sharding

00:11:31,279 --> 00:11:34,160
scheme

00:11:32,320 --> 00:11:35,920
and what a great example of this is

00:11:34,160 --> 00:11:37,279
something we actually saw at box couple

00:11:35,920 --> 00:11:39,760
of years back

00:11:37,279 --> 00:11:41,600
when we were starting using hash based

00:11:39,760 --> 00:11:44,800
partitioning of the file id

00:11:41,600 --> 00:11:46,560
and what we saw happened was that

00:11:44,800 --> 00:11:47,920
because we were doing hash based

00:11:46,560 --> 00:11:49,440
partitioning on the file id we were

00:11:47,920 --> 00:11:50,959
randomly distributing the documents

00:11:49,440 --> 00:11:52,959
across the shard

00:11:50,959 --> 00:11:54,959
which meant that at query time we needed

00:11:52,959 --> 00:11:56,000
to find out the query to every single

00:11:54,959 --> 00:11:58,480
chart

00:11:56,000 --> 00:11:59,839
what happens then is uh your bottleneck

00:11:58,480 --> 00:12:01,519
by the slowest chart so

00:11:59,839 --> 00:12:03,760
if you can see the latency graphs on the

00:12:01,519 --> 00:12:06,000
right uh at the bottom right

00:12:03,760 --> 00:12:07,760
you see the short level latency and even

00:12:06,000 --> 00:12:08,320
if a couple of shards have a latency

00:12:07,760 --> 00:12:10,160
spike

00:12:08,320 --> 00:12:12,560
every single query going to the whole

00:12:10,160 --> 00:12:15,279
cluster saw a huge latency spike

00:12:12,560 --> 00:12:17,120
which led to timeouts and returned back

00:12:15,279 --> 00:12:19,600
responses or

00:12:17,120 --> 00:12:20,880
timeout exceptions to the user uh it

00:12:19,600 --> 00:12:22,880
also meant that the cluster was very

00:12:20,880 --> 00:12:24,480
susceptible to

00:12:22,880 --> 00:12:26,000
any user doing high throughput queries

00:12:24,480 --> 00:12:27,200
because now that was getting fanned out

00:12:26,000 --> 00:12:29,440
every single chart

00:12:27,200 --> 00:12:30,880
and single shot failures just made the

00:12:29,440 --> 00:12:33,360
cluster unusable

00:12:30,880 --> 00:12:35,279
so the way we addressed this is we

00:12:33,360 --> 00:12:36,720
changed our partitioning scheme to use a

00:12:35,279 --> 00:12:39,360
range partition

00:12:36,720 --> 00:12:40,160
and used enterprise id to partition so

00:12:39,360 --> 00:12:42,320
that

00:12:40,160 --> 00:12:43,519
we distributed the documents uh by

00:12:42,320 --> 00:12:45,680
enterprise and now

00:12:43,519 --> 00:12:47,360
we can only fan out the query to those

00:12:45,680 --> 00:12:48,240
charts that contain documents from that

00:12:47,360 --> 00:12:51,519
enterprise

00:12:48,240 --> 00:12:54,160
uh this worked well for us and

00:12:51,519 --> 00:12:55,440
we saw that our p95 query latency went

00:12:54,160 --> 00:12:58,560
down significantly

00:12:55,440 --> 00:13:00,639
and now a single shard going down

00:12:58,560 --> 00:13:01,760
only impacts those users or those

00:13:00,639 --> 00:13:04,560
enterprises that

00:13:01,760 --> 00:13:05,519
need the data from that chart and we

00:13:04,560 --> 00:13:07,360
don't have uh

00:13:05,519 --> 00:13:08,880
entire cluster issues so this is one

00:13:07,360 --> 00:13:11,440
example where actually

00:13:08,880 --> 00:13:15,200
modifying your shorting scheme uh can

00:13:11,440 --> 00:13:17,920
help you improve your availability

00:13:15,200 --> 00:13:18,959
other things to think about on uh when

00:13:17,920 --> 00:13:21,839
it comes to shorting

00:13:18,959 --> 00:13:23,200
is hot spotting this is a common thing

00:13:21,839 --> 00:13:24,639
that happens

00:13:23,200 --> 00:13:27,040
in case of sharding especially if you

00:13:24,639 --> 00:13:28,399
have some hotkeys a great example of

00:13:27,040 --> 00:13:31,279
this is say if you're

00:13:28,399 --> 00:13:32,480
building video search and you have music

00:13:31,279 --> 00:13:35,120
videos of

00:13:32,480 --> 00:13:36,720
lady gaga for example and that is

00:13:35,120 --> 00:13:37,519
something which is very popular and if

00:13:36,720 --> 00:13:39,600
you put all

00:13:37,519 --> 00:13:40,639
such documents or all such videos on a

00:13:39,600 --> 00:13:43,199
single chart

00:13:40,639 --> 00:13:44,160
uh then anyone then that particular shot

00:13:43,199 --> 00:13:46,160
is going to get

00:13:44,160 --> 00:13:47,760
disproportionate amount of traffic and

00:13:46,160 --> 00:13:49,519
will be susceptible to going down

00:13:47,760 --> 00:13:51,839
causing your probability issues

00:13:49,519 --> 00:13:54,160
uh so a great way to handle this is to

00:13:51,839 --> 00:13:56,480
do salting which is uh you add

00:13:54,160 --> 00:13:57,680
a few random bits uh to the beginning of

00:13:56,480 --> 00:14:00,480
your shotting key

00:13:57,680 --> 00:14:02,000
which distributes this more evenly

00:14:00,480 --> 00:14:04,639
across multiple charts

00:14:02,000 --> 00:14:06,720
and you can actually control the the fan

00:14:04,639 --> 00:14:08,480
out or the distribution here by picking

00:14:06,720 --> 00:14:12,160
the number of bits that you use

00:14:08,480 --> 00:14:13,120
so you can use a three bit uh salt and

00:14:12,160 --> 00:14:16,320
that will ensure

00:14:13,120 --> 00:14:17,360
that you are spreading uh your shouting

00:14:16,320 --> 00:14:20,399
key across

00:14:17,360 --> 00:14:21,279
eight sharks uh you can still have hot

00:14:20,399 --> 00:14:23,920
spotting issues

00:14:21,279 --> 00:14:24,399
uh even after salting especially if

00:14:23,920 --> 00:14:26,079
you're

00:14:24,399 --> 00:14:27,920
assigning multiple shards to a single

00:14:26,079 --> 00:14:30,000
machine and you end up assigning all the

00:14:27,920 --> 00:14:31,120
lady gaga shots to the same machine or

00:14:30,000 --> 00:14:33,600
let's say you assign

00:14:31,120 --> 00:14:35,199
uh the lady gaga shard and the britney

00:14:33,600 --> 00:14:36,639
spears shark to the same machine so what

00:14:35,199 --> 00:14:39,040
that means is now

00:14:36,639 --> 00:14:40,800
uh you have uh hotspotting at the

00:14:39,040 --> 00:14:42,160
machine level and this can be equally

00:14:40,800 --> 00:14:43,519
bad because the machine is bound by the

00:14:42,160 --> 00:14:47,279
computer resources it has

00:14:43,519 --> 00:14:48,880
right so a good way to handle this is

00:14:47,279 --> 00:14:51,199
using bin packing

00:14:48,880 --> 00:14:52,480
bin packing is a optimization algorithm

00:14:51,199 --> 00:14:55,920
that

00:14:52,480 --> 00:14:57,040
signs elements into buckets which can be

00:14:55,920 --> 00:14:58,880
charged to machines

00:14:57,040 --> 00:15:00,639
in such a way that it can optimize some

00:14:58,880 --> 00:15:02,160
parameter and in this case you can pick

00:15:00,639 --> 00:15:03,920
qps as a parameter

00:15:02,160 --> 00:15:05,440
uh which will enable you to assign

00:15:03,920 --> 00:15:07,279
machines uh

00:15:05,440 --> 00:15:08,560
you know assign charts to machines in

00:15:07,279 --> 00:15:10,959
such a way that you do not have

00:15:08,560 --> 00:15:13,279
hotspotting at the machine level

00:15:10,959 --> 00:15:14,079
another thing to think about is handling

00:15:13,279 --> 00:15:16,320
index growth

00:15:14,079 --> 00:15:18,160
uh this is something we saw at box once

00:15:16,320 --> 00:15:20,639
we moved to enterprise phase starting

00:15:18,160 --> 00:15:22,399
uh which is uh some shards uh started

00:15:20,639 --> 00:15:24,399
growing exponentially compared to others

00:15:22,399 --> 00:15:25,760
because uh those enterprises grow much

00:15:24,399 --> 00:15:28,000
faster and

00:15:25,760 --> 00:15:30,000
upload larger number of documents but

00:15:28,000 --> 00:15:32,240
eventually the shards are bounded by

00:15:30,000 --> 00:15:33,040
the physical disk resource of the

00:15:32,240 --> 00:15:34,880
machine

00:15:33,040 --> 00:15:36,160
so eventually some shards are going to

00:15:34,880 --> 00:15:39,040
run out of disk space

00:15:36,160 --> 00:15:40,800
and you need to handle that so one way

00:15:39,040 --> 00:15:42,480
of doing that is uh to do short

00:15:40,800 --> 00:15:43,040
splitting this is something that hbase

00:15:42,480 --> 00:15:45,199
does

00:15:43,040 --> 00:15:46,720
uh you can split the shard into a

00:15:45,199 --> 00:15:49,199
smaller uh

00:15:46,720 --> 00:15:51,120
range and then you can move one of the

00:15:49,199 --> 00:15:53,519
uh split shots to a different machine

00:15:51,120 --> 00:15:55,680
giving more room to the machine where

00:15:53,519 --> 00:15:56,720
you ran out of disk uh unfortunately

00:15:55,680 --> 00:15:58,720
this does involve

00:15:56,720 --> 00:16:00,320
moving physical bites to a different

00:15:58,720 --> 00:16:02,160
machine which can be expensive

00:16:00,320 --> 00:16:04,000
especially if your shards are large

00:16:02,160 --> 00:16:06,480
uh the alternative is to use something

00:16:04,000 --> 00:16:09,120
called short spinning that we use at box

00:16:06,480 --> 00:16:10,880
which means uh if your shard exceeded a

00:16:09,120 --> 00:16:12,720
particular threshold of size then you

00:16:10,880 --> 00:16:13,519
can spill over the requests uh going to

00:16:12,720 --> 00:16:15,360
that shard

00:16:13,519 --> 00:16:16,800
to another shard on this on a different

00:16:15,360 --> 00:16:17,519
machine which has more head over

00:16:16,800 --> 00:16:19,759
capacity

00:16:17,519 --> 00:16:21,360
this uh complicates your routing logic a

00:16:19,759 --> 00:16:23,440
little but it can save you

00:16:21,360 --> 00:16:24,800
uh the time to actually move the

00:16:23,440 --> 00:16:28,079
physical bites of

00:16:24,800 --> 00:16:29,279
the shot and then finally uh we always

00:16:28,079 --> 00:16:31,440
need to think about

00:16:29,279 --> 00:16:32,800
organic growth so as your index grows

00:16:31,440 --> 00:16:34,399
you're going to add more machines and

00:16:32,800 --> 00:16:36,800
scale your cluster

00:16:34,399 --> 00:16:38,639
but if your sharding strategy involves a

00:16:36,800 --> 00:16:39,120
static mapping of shards to machines

00:16:38,639 --> 00:16:40,959
then

00:16:39,120 --> 00:16:42,880
uh it would mean every time you scale

00:16:40,959 --> 00:16:45,600
your cluster you would need to

00:16:42,880 --> 00:16:46,800
uh rebuild your shard boundaries and

00:16:45,600 --> 00:16:48,560
assign your shards again

00:16:46,800 --> 00:16:51,199
which can involve down timing your

00:16:48,560 --> 00:16:52,720
system uh if your availability cannot

00:16:51,199 --> 00:16:55,040
handle something like that

00:16:52,720 --> 00:16:56,880
then a good option is to use virtual

00:16:55,040 --> 00:16:58,560
partitioning where you

00:16:56,880 --> 00:17:00,320
create a large number of charts and then

00:16:58,560 --> 00:17:01,600
give something like consistent hashing

00:17:00,320 --> 00:17:03,519
or ring hash

00:17:01,600 --> 00:17:05,199
to assign these shards to machines in

00:17:03,519 --> 00:17:08,000
such a way that you don't have to

00:17:05,199 --> 00:17:09,760
modify your chart boundaries or your

00:17:08,000 --> 00:17:11,439
short assignment mapping

00:17:09,760 --> 00:17:12,799
it will be agnostic to that and

00:17:11,439 --> 00:17:16,160
automatically handle uh

00:17:12,799 --> 00:17:19,439
increasing machines in a cluster

00:17:16,160 --> 00:17:21,120
so that brings us uh to uh another

00:17:19,439 --> 00:17:22,640
strategy which is really useful in

00:17:21,120 --> 00:17:25,360
avoiding failures which is

00:17:22,640 --> 00:17:26,400
a retries uh if you have a system like

00:17:25,360 --> 00:17:28,720
this which has

00:17:26,400 --> 00:17:30,160
a large number of services that are

00:17:28,720 --> 00:17:33,679
talking to each other

00:17:30,160 --> 00:17:35,840
then a great way uh to avoid uh network

00:17:33,679 --> 00:17:36,160
packet drops or intermittent failures at

00:17:35,840 --> 00:17:38,400
each

00:17:36,160 --> 00:17:39,360
uh service is to redry the request at

00:17:38,400 --> 00:17:41,520
each layer

00:17:39,360 --> 00:17:43,200
uh one thing to remember though is

00:17:41,520 --> 00:17:45,919
reprice can be overwhelming

00:17:43,200 --> 00:17:47,280
so one thing that we have seen at box is

00:17:45,919 --> 00:17:49,360
if a solar shard

00:17:47,280 --> 00:17:51,280
for example is having intermittent

00:17:49,360 --> 00:17:52,720
issues and failing some requests and the

00:17:51,280 --> 00:17:54,320
upstream service is continuously

00:17:52,720 --> 00:17:56,799
retrying these requests

00:17:54,320 --> 00:17:58,000
then it can actually create a retry

00:17:56,799 --> 00:18:00,799
storm that

00:17:58,000 --> 00:18:01,200
takes down your entire cluster and uh

00:18:00,799 --> 00:18:03,600
that

00:18:01,200 --> 00:18:05,200
that is really bad and uh the way we

00:18:03,600 --> 00:18:07,520
handle it is we always use

00:18:05,200 --> 00:18:08,559
uh exponential back off when retrying so

00:18:07,520 --> 00:18:10,320
that

00:18:08,559 --> 00:18:12,320
we are giving more and more time to the

00:18:10,320 --> 00:18:15,280
downstream system to recover

00:18:12,320 --> 00:18:16,320
when we do the next retry and uh it is

00:18:15,280 --> 00:18:18,640
also important to

00:18:16,320 --> 00:18:19,679
use uh something called a jitter or a

00:18:18,640 --> 00:18:21,679
random weight

00:18:19,679 --> 00:18:23,280
so that if you have multiple instances

00:18:21,679 --> 00:18:25,919
which are retrying um

00:18:23,280 --> 00:18:28,559
you do not somehow end up synchronizing

00:18:25,919 --> 00:18:29,840
the retry so that your uh retrying at

00:18:28,559 --> 00:18:32,160
the same time

00:18:29,840 --> 00:18:34,240
and this can save you again from causing

00:18:32,160 --> 00:18:36,960
uh retry start

00:18:34,240 --> 00:18:38,799
finally we should only try to retry

00:18:36,960 --> 00:18:41,280
failures that you can recover from like

00:18:38,799 --> 00:18:42,080
uh timeout exceptions kp timeouts and

00:18:41,280 --> 00:18:44,400
things like that

00:18:42,080 --> 00:18:46,160
uh we should never try to retry a solar

00:18:44,400 --> 00:18:48,400
exception or

00:18:46,160 --> 00:18:52,000
maybe a 400 because those are things

00:18:48,400 --> 00:18:52,000
that we can't uh recover from

00:18:52,240 --> 00:18:56,080
another strategy that we use at box is

00:18:54,640 --> 00:18:58,160
optimistic retries so

00:18:56,080 --> 00:18:59,440
we talked about having multiple replicas

00:18:58,160 --> 00:19:01,679
in the beginning uh

00:18:59,440 --> 00:19:03,360
and distributing uh them in a more

00:19:01,679 --> 00:19:05,120
active active way so

00:19:03,360 --> 00:19:07,120
if you have multiple clusters that are

00:19:05,120 --> 00:19:10,000
serving traffic in active active

00:19:07,120 --> 00:19:11,760
fashion then you can send your queries

00:19:10,000 --> 00:19:13,440
to more than one cluster

00:19:11,760 --> 00:19:15,120
and then simply return the result from

00:19:13,440 --> 00:19:16,640
the one that returns the fastest and

00:19:15,120 --> 00:19:18,880
cancel the other query

00:19:16,640 --> 00:19:20,400
what this does is it saves you from

00:19:18,880 --> 00:19:22,480
waiting for a failure to happen

00:19:20,400 --> 00:19:24,080
and then retry it to multiple clusters

00:19:22,480 --> 00:19:26,320
by which time maybe your query has

00:19:24,080 --> 00:19:29,360
already timed out at the topmost layer

00:19:26,320 --> 00:19:32,160
and you've returned a 500 to the user

00:19:29,360 --> 00:19:32,799
so uh this is possible if you have

00:19:32,160 --> 00:19:35,280
available

00:19:32,799 --> 00:19:36,080
compute on your clusters and you have

00:19:35,280 --> 00:19:38,400
that

00:19:36,080 --> 00:19:39,440
headroom to absorb the additional query

00:19:38,400 --> 00:19:42,880
that you do

00:19:39,440 --> 00:19:44,880
uh but it is also important uh to uh

00:19:42,880 --> 00:19:45,919
realize that we need to time out

00:19:44,880 --> 00:19:48,000
requests uh

00:19:45,919 --> 00:19:50,080
at the cluster level in such a way that

00:19:48,000 --> 00:19:52,960
we are not wasting computer resources

00:19:50,080 --> 00:19:54,559
uh at the cluster by processing queries

00:19:52,960 --> 00:19:57,440
that have already timed out

00:19:54,559 --> 00:19:58,880
so for example if you have uh if you

00:19:57,440 --> 00:20:00,799
look at this figure on the right if you

00:19:58,880 --> 00:20:01,440
have a web app player where you start a

00:20:00,799 --> 00:20:03,760
query

00:20:01,440 --> 00:20:05,440
with a timeout of 10 seconds then at the

00:20:03,760 --> 00:20:06,080
federator layer maybe you just have nine

00:20:05,440 --> 00:20:08,799
seconds left

00:20:06,080 --> 00:20:10,400
and then the query service you have five

00:20:08,799 --> 00:20:11,840
seconds left because maybe you spend

00:20:10,400 --> 00:20:13,760
some time doing retries and then

00:20:11,840 --> 00:20:15,760
eventually at the solar cluster level

00:20:13,760 --> 00:20:17,520
uh you have only four seconds left but

00:20:15,760 --> 00:20:18,720
if you use a static timeout of 10

00:20:17,520 --> 00:20:20,480
seconds at every layer

00:20:18,720 --> 00:20:22,400
then you have wasted six seconds of

00:20:20,480 --> 00:20:24,159
processing time on the solar cluster

00:20:22,400 --> 00:20:25,760
processing a query that is already timed

00:20:24,159 --> 00:20:27,679
out at the web app layers so

00:20:25,760 --> 00:20:29,520
this is a wasteful consumption of

00:20:27,679 --> 00:20:31,200
compute resources which can be dedicated

00:20:29,520 --> 00:20:32,720
to processing other queries that go to

00:20:31,200 --> 00:20:35,840
that cluster

00:20:32,720 --> 00:20:37,760
so uh in this aspect also a time allowed

00:20:35,840 --> 00:20:39,440
parameter that is provided by solar is a

00:20:37,760 --> 00:20:41,919
great way to push down

00:20:39,440 --> 00:20:43,039
uh the query timeout that you computed

00:20:41,919 --> 00:20:45,520
all the way to solar

00:20:43,039 --> 00:20:46,480
and this parameter is then evaluated at

00:20:45,520 --> 00:20:48,159
the end of

00:20:46,480 --> 00:20:50,159
the query expansion phase and the

00:20:48,159 --> 00:20:51,760
document collection phase so

00:20:50,159 --> 00:20:53,679
it will allow solar to time out the

00:20:51,760 --> 00:20:54,720
query if it has exceeded the time

00:20:53,679 --> 00:20:57,520
allowed parameter

00:20:54,720 --> 00:20:58,880
that you passed and uh at box we have

00:20:57,520 --> 00:21:00,640
seen that this parameter is

00:20:58,880 --> 00:21:02,080
really useful in timing out expensive

00:21:00,640 --> 00:21:04,720
queries at the solar level

00:21:02,080 --> 00:21:06,880
so you can avoid uh processing queries

00:21:04,720 --> 00:21:07,600
which are expensive and that can help

00:21:06,880 --> 00:21:10,320
you

00:21:07,600 --> 00:21:11,760
uh really recover from these kind of

00:21:10,320 --> 00:21:15,280
issues

00:21:11,760 --> 00:21:17,679
uh that brings us to the next section on

00:21:15,280 --> 00:21:19,440
failure detection and recovery so so far

00:21:17,679 --> 00:21:20,400
we've talked about how failures will

00:21:19,440 --> 00:21:22,640
happen

00:21:20,400 --> 00:21:24,320
and how we can tolerate them in this

00:21:22,640 --> 00:21:26,000
section we'll talk about some

00:21:24,320 --> 00:21:27,679
ways we can detect failures in our

00:21:26,000 --> 00:21:30,720
system and try to recover from them

00:21:27,679 --> 00:21:33,440
without human intervention

00:21:30,720 --> 00:21:34,240
so one way to do this is using circuit

00:21:33,440 --> 00:21:36,880
records

00:21:34,240 --> 00:21:38,480
this is a common software uh pattern

00:21:36,880 --> 00:21:41,200
which is used to

00:21:38,480 --> 00:21:42,000
detect uh issues in a downstream system

00:21:41,200 --> 00:21:44,000
and

00:21:42,000 --> 00:21:45,760
trip a circuit breaker which can stop

00:21:44,000 --> 00:21:46,640
requests going to the faulty downstream

00:21:45,760 --> 00:21:50,000
system

00:21:46,640 --> 00:21:51,039
uh in case of search uh or say solar

00:21:50,000 --> 00:21:52,640
shards uh

00:21:51,039 --> 00:21:55,360
the way we can do this is detect

00:21:52,640 --> 00:21:57,679
failures based on uh

00:21:55,360 --> 00:21:59,679
failures return detect issues based on

00:21:57,679 --> 00:22:02,799
failures returned from the solar chart

00:21:59,679 --> 00:22:04,080
or we can monitor the p95 latency of a

00:22:02,799 --> 00:22:06,159
specific chart and

00:22:04,080 --> 00:22:08,480
if it is much higher than uh the other

00:22:06,159 --> 00:22:09,039
shots uh that we are querying then we

00:22:08,480 --> 00:22:10,640
can

00:22:09,039 --> 00:22:12,960
trip the circuit breaker and stop

00:22:10,640 --> 00:22:15,600
sending queries to the faulty chart

00:22:12,960 --> 00:22:16,000
and handle this by rerouting the queries

00:22:15,600 --> 00:22:19,039
to

00:22:16,000 --> 00:22:22,240
alternate replica which is going uh to

00:22:19,039 --> 00:22:23,760
another cluster uh also once the system

00:22:22,240 --> 00:22:26,240
has recovered we can close the circuit

00:22:23,760 --> 00:22:29,360
breaker and that resumes the normal uh

00:22:26,240 --> 00:22:31,360
flow of traffic uh problem here though

00:22:29,360 --> 00:22:33,360
is that we are using failures or

00:22:31,360 --> 00:22:35,840
latencies uh to detect failures

00:22:33,360 --> 00:22:36,880
uh in the downstream system and that

00:22:35,840 --> 00:22:40,320
means we are already

00:22:36,880 --> 00:22:41,520
uh returning 500 or high latency queries

00:22:40,320 --> 00:22:43,200
back to the user

00:22:41,520 --> 00:22:44,720
so basically we are detecting failure at

00:22:43,200 --> 00:22:47,679
the expense of the user

00:22:44,720 --> 00:22:49,760
uh so an alternative here is to use

00:22:47,679 --> 00:22:53,120
health monitoring systems so we can

00:22:49,760 --> 00:22:55,919
uh monitor the health of shards uh using

00:22:53,120 --> 00:22:57,840
health checks uh some health checks that

00:22:55,919 --> 00:22:59,200
you can use for solar is the solar pain

00:22:57,840 --> 00:23:01,520
query that's a great

00:22:59,200 --> 00:23:02,640
uh way to detect if your solar process

00:23:01,520 --> 00:23:06,080
is up and running

00:23:02,640 --> 00:23:08,400
uh you can also use uh uh you can also

00:23:06,080 --> 00:23:11,679
try to index an empty document

00:23:08,400 --> 00:23:14,480
to solar to make sure that the

00:23:11,679 --> 00:23:15,760
indexing is working you can gather stats

00:23:14,480 --> 00:23:18,880
at the machine level like

00:23:15,760 --> 00:23:21,600
cpu and memory

00:23:18,880 --> 00:23:22,880
disk utilization to look at the machine

00:23:21,600 --> 00:23:25,280
and ensure that the machine that is

00:23:22,880 --> 00:23:27,520
running the solar chart is uh

00:23:25,280 --> 00:23:29,120
doing well and uh based on these health

00:23:27,520 --> 00:23:32,720
checks you can determine whether

00:23:29,120 --> 00:23:36,080
your downstream system or shard us is

00:23:32,720 --> 00:23:39,039
in a healthy state which can help uh

00:23:36,080 --> 00:23:39,840
trigger a circuit breaker so at box we

00:23:39,039 --> 00:23:42,240
have built

00:23:39,840 --> 00:23:43,039
uh something called a short health

00:23:42,240 --> 00:23:46,400
monitor

00:23:43,039 --> 00:23:49,520
uh this is a kubernetes services

00:23:46,400 --> 00:23:52,320
uh that runs multiple health checks to

00:23:49,520 --> 00:23:53,360
all solar shards that are serving live

00:23:52,320 --> 00:23:56,159
user traffic

00:23:53,360 --> 00:23:57,440
and whenever there are failures uh for

00:23:56,159 --> 00:24:00,080
multiple health checks on

00:23:57,440 --> 00:24:03,200
a specific chart the short of monitor

00:24:00,080 --> 00:24:04,960
notifies this information to zookeeper

00:24:03,200 --> 00:24:07,360
and then any service that needs to talk

00:24:04,960 --> 00:24:09,919
to these clusters registers watchers

00:24:07,360 --> 00:24:12,080
in zookeeper on that particular z node

00:24:09,919 --> 00:24:14,080
and that makes zookeepers send a

00:24:12,080 --> 00:24:17,440
notification to these services saying

00:24:14,080 --> 00:24:19,760
uh that a particular shard is

00:24:17,440 --> 00:24:21,760
having trouble which then trips the

00:24:19,760 --> 00:24:23,679
circuit breaker in these services

00:24:21,760 --> 00:24:25,200
and stops sending requests to the faulty

00:24:23,679 --> 00:24:26,480
chart and reroutes them to a different

00:24:25,200 --> 00:24:28,480
cluster

00:24:26,480 --> 00:24:30,400
on the flip side once the shot recovers

00:24:28,480 --> 00:24:32,480
short health monitor can detect this

00:24:30,400 --> 00:24:34,159
and update zookeeper which then again

00:24:32,480 --> 00:24:35,279
notifies the services to close their

00:24:34,159 --> 00:24:37,600
circuit records

00:24:35,279 --> 00:24:38,799
and start sending requests back to the

00:24:37,600 --> 00:24:41,360
original chart

00:24:38,799 --> 00:24:42,799
so this is an example of an end to end

00:24:41,360 --> 00:24:45,679
system that you can build

00:24:42,799 --> 00:24:47,039
using the circuit breaker pattern with a

00:24:45,679 --> 00:24:49,919
health monitoring system

00:24:47,039 --> 00:24:50,320
and a notification system like zookeeper

00:24:49,919 --> 00:24:52,559
to

00:24:50,320 --> 00:24:54,480
have end-to-end failure detection and

00:24:52,559 --> 00:24:56,000
automated recovery without requiring

00:24:54,480 --> 00:24:57,120
human intervention and this will take

00:24:56,000 --> 00:25:00,000
you a long way

00:24:57,120 --> 00:25:01,600
in getting close to that three nines or

00:25:00,000 --> 00:25:04,320
four lines of availability

00:25:01,600 --> 00:25:06,640
because uh now your reaction time or

00:25:04,320 --> 00:25:09,679
recovery time is much faster than

00:25:06,640 --> 00:25:12,960
a human being trying to log in and

00:25:09,679 --> 00:25:12,960
pull a shard out of a cluster

00:25:13,039 --> 00:25:16,080
another thing that we haven't talked

00:25:14,720 --> 00:25:19,600
about so far is

00:25:16,080 --> 00:25:21,520
uh the recovery of an index so

00:25:19,600 --> 00:25:22,720
sometimes failures can happen which make

00:25:21,520 --> 00:25:25,039
your index unusable

00:25:22,720 --> 00:25:26,880
uh this could be an index corruption or

00:25:25,039 --> 00:25:28,960
a hard machine failure

00:25:26,880 --> 00:25:30,159
uh in such case the only option is to

00:25:28,960 --> 00:25:31,919
rebuild the index

00:25:30,159 --> 00:25:33,440
which is really expensive to do from

00:25:31,919 --> 00:25:35,120
scratch especially if your shards are

00:25:33,440 --> 00:25:38,080
really large

00:25:35,120 --> 00:25:38,960
so one option here is to maintain an

00:25:38,080 --> 00:25:42,240
offline backup

00:25:38,960 --> 00:25:44,640
of the index in a secondary system

00:25:42,240 --> 00:25:45,760
and uh this can reduce your recovery

00:25:44,640 --> 00:25:47,840
time significantly

00:25:45,760 --> 00:25:50,480
uh by rebuilding from that offline

00:25:47,840 --> 00:25:53,039
backup uh the way we do it at box is

00:25:50,480 --> 00:25:53,600
uh we use edge base for this and we

00:25:53,039 --> 00:25:56,320
store

00:25:53,600 --> 00:25:57,679
solar documents in hbase in a

00:25:56,320 --> 00:26:00,559
denormalized form

00:25:57,679 --> 00:26:02,559
so when a shard goes down uh we can

00:26:00,559 --> 00:26:03,360
simply run a mapreduce job to scan all

00:26:02,559 --> 00:26:05,360
the documents

00:26:03,360 --> 00:26:06,640
uh from edge base that belong to that

00:26:05,360 --> 00:26:09,440
chart and

00:26:06,640 --> 00:26:11,279
re-index them hbase is also great at

00:26:09,440 --> 00:26:13,520
performing time range scans so if we

00:26:11,279 --> 00:26:14,000
have to perform partial failures uh we

00:26:13,520 --> 00:26:16,480
can just

00:26:14,000 --> 00:26:17,039
run a time range scan for a day or a

00:26:16,480 --> 00:26:19,360
week and

00:26:17,039 --> 00:26:20,400
only index those documents from that

00:26:19,360 --> 00:26:23,440
time period

00:26:20,400 --> 00:26:25,279
which can help with the partial recovery

00:26:23,440 --> 00:26:27,360
one problem here is though that we are

00:26:25,279 --> 00:26:29,760
still reindexing the documents to solar

00:26:27,360 --> 00:26:33,039
and indexing is an expensive process so

00:26:29,760 --> 00:26:34,159
if you have a large shorts then uh this

00:26:33,039 --> 00:26:37,120
can

00:26:34,159 --> 00:26:37,679
really have a large recovery time so one

00:26:37,120 --> 00:26:40,720
idea

00:26:37,679 --> 00:26:43,440
here is to maintain an offline index

00:26:40,720 --> 00:26:44,720
uh like edge base and then rebuild uh

00:26:43,440 --> 00:26:46,960
segment files uh

00:26:44,720 --> 00:26:47,840
in an ongoing manner periodically and

00:26:46,960 --> 00:26:51,520
store them

00:26:47,840 --> 00:26:52,880
in an object store like s3 or gcs

00:26:51,520 --> 00:26:54,880
and then when you need to rebuild a

00:26:52,880 --> 00:26:56,320
shard then instead of re-indexing the

00:26:54,880 --> 00:26:58,159
documents you can simply copy these

00:26:56,320 --> 00:26:59,360
segment files from this object store

00:26:58,159 --> 00:27:02,480
into the machine

00:26:59,360 --> 00:27:04,960
and then uh replay all uh the latest

00:27:02,480 --> 00:27:06,480
updates from a system like kafka

00:27:04,960 --> 00:27:09,200
based on the last offset that was

00:27:06,480 --> 00:27:11,440
processed by the segment file generation

00:27:09,200 --> 00:27:12,640
so this is an example of end-to-end

00:27:11,440 --> 00:27:15,919
system that can

00:27:12,640 --> 00:27:18,720
help you recover uh the index in case uh

00:27:15,919 --> 00:27:20,720
the index is unusable uh with very low

00:27:18,720 --> 00:27:23,520
recovery time

00:27:20,720 --> 00:27:24,559
that brings us to our last section today

00:27:23,520 --> 00:27:27,039
on

00:27:24,559 --> 00:27:28,399
failure prevention so far we have talked

00:27:27,039 --> 00:27:30,480
about how

00:27:28,399 --> 00:27:32,480
failures will always happen and how we

00:27:30,480 --> 00:27:34,960
can detect and recover from them

00:27:32,480 --> 00:27:35,760
but there are some kinds of failures uh

00:27:34,960 --> 00:27:39,039
that

00:27:35,760 --> 00:27:40,720
we can actually prevent or some failure

00:27:39,039 --> 00:27:42,320
sources that we can actually prevent

00:27:40,720 --> 00:27:45,279
either by improving our

00:27:42,320 --> 00:27:47,440
uh availability the first one is uh

00:27:45,279 --> 00:27:49,360
abusive users so we cannot predict the

00:27:47,440 --> 00:27:50,799
way users are going to use our system

00:27:49,360 --> 00:27:52,799
and there are going to be some ways that

00:27:50,799 --> 00:27:55,360
they try to use it which is going to be

00:27:52,799 --> 00:27:55,840
abusive uh especially at box we've seen

00:27:55,360 --> 00:27:58,559
uh

00:27:55,840 --> 00:28:00,399
sometimes people can perform api queries

00:27:58,559 --> 00:28:01,360
which are trying to crawl the index uh

00:28:00,399 --> 00:28:03,840
which is like

00:28:01,360 --> 00:28:04,640
uh really expensive and uh if you're

00:28:03,840 --> 00:28:06,559
trying to crawl

00:28:04,640 --> 00:28:08,720
uh our entire index it's going to cause

00:28:06,559 --> 00:28:11,360
issues uh for other users who are

00:28:08,720 --> 00:28:13,279
uh creating the same chart so a great

00:28:11,360 --> 00:28:15,120
way to do uh prevent this is using

00:28:13,279 --> 00:28:17,279
different kinds of rate limiters

00:28:15,120 --> 00:28:19,520
they are a great way to prevent abusive

00:28:17,279 --> 00:28:21,360
queries it is important to use different

00:28:19,520 --> 00:28:23,520
kinds of rate limiters so

00:28:21,360 --> 00:28:24,960
at box we use a global rate limiter on

00:28:23,520 --> 00:28:27,600
the entire throughput

00:28:24,960 --> 00:28:28,080
but we also use rate limiting at the

00:28:27,600 --> 00:28:29,840
user

00:28:28,080 --> 00:28:32,000
and enterprise level so that we can

00:28:29,840 --> 00:28:32,640
prevent specific shards from having

00:28:32,000 --> 00:28:34,320
issues

00:28:32,640 --> 00:28:36,000
when they receive a high throughput of

00:28:34,320 --> 00:28:39,279
queries um

00:28:36,000 --> 00:28:40,799
but still under the global rate limit

00:28:39,279 --> 00:28:42,559
another kind of rate limiting that we

00:28:40,799 --> 00:28:45,039
use is the index rate limiting

00:28:42,559 --> 00:28:47,279
so we have seen that sometimes when we

00:28:45,039 --> 00:28:49,120
are on boarding a large enterprise or

00:28:47,279 --> 00:28:51,520
when a user is indexing a lot of large

00:28:49,120 --> 00:28:53,679
documents this can be expensive because

00:28:51,520 --> 00:28:54,960
solar needs to update a large amount of

00:28:53,679 --> 00:28:57,440
the inverted index

00:28:54,960 --> 00:28:59,360
so a great way to prevent this is to

00:28:57,440 --> 00:28:59,840
limit uh requests based on the bytes

00:28:59,360 --> 00:29:02,559
written

00:28:59,840 --> 00:29:04,640
uh to solar and uh when we fail the

00:29:02,559 --> 00:29:07,039
request uh due to the rate limiting

00:29:04,640 --> 00:29:08,799
instead of failing uh it entirely we

00:29:07,039 --> 00:29:11,520
simply re-keep the request back into

00:29:08,799 --> 00:29:14,480
kafka with a back-off time so that

00:29:11,520 --> 00:29:15,520
uh it can be uh re-indexed later so in

00:29:14,480 --> 00:29:18,960
this way we can

00:29:15,520 --> 00:29:21,279
actually smooth uh indexing spike uh by

00:29:18,960 --> 00:29:23,360
tolerating some high latency for

00:29:21,279 --> 00:29:26,640
indexing that document but we save

00:29:23,360 --> 00:29:28,720
solar from having issues the other

00:29:26,640 --> 00:29:29,760
option is the other rate limiting that

00:29:28,720 --> 00:29:31,840
we use is

00:29:29,760 --> 00:29:33,279
expensive query rate limiter so there

00:29:31,840 --> 00:29:35,120
are uh certain

00:29:33,279 --> 00:29:36,320
queries that can be expensive for solar

00:29:35,120 --> 00:29:39,520
to process uh

00:29:36,320 --> 00:29:41,679
like a deep offset pagination query or

00:29:39,520 --> 00:29:43,279
queries with large number of tokens or

00:29:41,679 --> 00:29:45,600
large number of stock words

00:29:43,279 --> 00:29:47,200
or a very large filter query can be

00:29:45,600 --> 00:29:48,080
really expensive and if you have one too

00:29:47,200 --> 00:29:50,159
many of these

00:29:48,080 --> 00:29:51,679
queries in a small succession of time go

00:29:50,159 --> 00:29:53,840
to the same chart

00:29:51,679 --> 00:29:55,440
it can strain the shard and fail other

00:29:53,840 --> 00:29:57,919
requests that are going to

00:29:55,440 --> 00:29:59,600
that particular shot so we detect these

00:29:57,919 --> 00:30:00,640
queries based on the latency of the

00:29:59,600 --> 00:30:03,360
query and

00:30:00,640 --> 00:30:05,360
try to prevent future queries uh for in

00:30:03,360 --> 00:30:07,360
the next period of time

00:30:05,360 --> 00:30:09,600
to prevent it causing issues in the

00:30:07,360 --> 00:30:11,760
downstream system

00:30:09,600 --> 00:30:14,159
uh another issue that we have seen with

00:30:11,760 --> 00:30:15,360
uh solar is if you suddenly overwhelm it

00:30:14,159 --> 00:30:18,080
with live traffic

00:30:15,360 --> 00:30:19,919
uh then it can have issues and sometimes

00:30:18,080 --> 00:30:22,960
can crash the solar process

00:30:19,919 --> 00:30:24,559
and this happens because uh your caches

00:30:22,960 --> 00:30:26,720
like the filter cache and the document

00:30:24,559 --> 00:30:28,880
cache uh they're all called and uh

00:30:26,720 --> 00:30:30,000
um suddenly being overwhelmed with a

00:30:28,880 --> 00:30:32,559
large amount of traffic

00:30:30,000 --> 00:30:33,279
can cause uh figures so the way we

00:30:32,559 --> 00:30:35,679
address this

00:30:33,279 --> 00:30:37,840
is uh whenever we have a backup cluster

00:30:35,679 --> 00:30:40,080
we warm it up with live queries

00:30:37,840 --> 00:30:41,679
uh we have a service called the shadow

00:30:40,080 --> 00:30:43,360
service which is responsible for

00:30:41,679 --> 00:30:44,159
replaying these queries to the backup

00:30:43,360 --> 00:30:47,200
cluster

00:30:44,159 --> 00:30:48,880
and keeping it warm and it also ramps up

00:30:47,200 --> 00:30:49,919
the traffic slowly so that it doesn't

00:30:48,880 --> 00:30:53,520
overwhelm the

00:30:49,919 --> 00:30:55,200
uh or the cluster entirely and then uh

00:30:53,520 --> 00:30:56,640
we always have a warmed up backup

00:30:55,200 --> 00:30:58,480
cluster so that

00:30:56,640 --> 00:30:59,840
when we have an issue with a live

00:30:58,480 --> 00:31:01,600
cluster we can swap in the backup

00:30:59,840 --> 00:31:04,320
cluster which is already warmed up

00:31:01,600 --> 00:31:05,679
and it does not have issues with latency

00:31:04,320 --> 00:31:08,559
or does not go down when

00:31:05,679 --> 00:31:09,279
it receives the live user traffic uh and

00:31:08,559 --> 00:31:12,399
in that

00:31:09,279 --> 00:31:14,080
respect it's always good to warm up

00:31:12,399 --> 00:31:16,399
to run a few warm-up queries for every

00:31:14,080 --> 00:31:17,360
service even your own services because

00:31:16,399 --> 00:31:19,200
you can have

00:31:17,360 --> 00:31:21,200
internal caches in those services you

00:31:19,200 --> 00:31:22,159
can have thread pools which take time to

00:31:21,200 --> 00:31:24,000
initialize

00:31:22,159 --> 00:31:26,159
or you can have lazy initializations of

00:31:24,000 --> 00:31:28,000
other systems internally

00:31:26,159 --> 00:31:29,600
and then if you don't warm up then the

00:31:28,000 --> 00:31:31,679
first query that goes to that service

00:31:29,600 --> 00:31:33,600
pays the cost of initializing everything

00:31:31,679 --> 00:31:35,279
which can cause failures every time you

00:31:33,600 --> 00:31:38,799
deploy or

00:31:35,279 --> 00:31:38,799
do a rolling restart of a service

00:31:39,200 --> 00:31:44,399
so that lastly even brings us to the

00:31:42,240 --> 00:31:46,640
operational aspects of search

00:31:44,399 --> 00:31:48,000
this is the process and the people

00:31:46,640 --> 00:31:50,240
pillars that i talked about

00:31:48,000 --> 00:31:51,919
uh there are things that we can do uh on

00:31:50,240 --> 00:31:54,159
the operational side to improve

00:31:51,919 --> 00:31:55,679
uh search uh the first one is if you

00:31:54,159 --> 00:31:57,679
have a large monolithic service it's

00:31:55,679 --> 00:32:00,399
always a good idea to split it into

00:31:57,679 --> 00:32:02,000
a single responsibility services this uh

00:32:00,399 --> 00:32:03,760
separates the failure domains and makes

00:32:02,000 --> 00:32:04,559
your service more debuggable in case of

00:32:03,760 --> 00:32:07,440
issues

00:32:04,559 --> 00:32:08,240
uh you should invest in monitoring and

00:32:07,440 --> 00:32:09,679
alerting

00:32:08,240 --> 00:32:11,679
i can't tell you how many issues i've

00:32:09,679 --> 00:32:14,399
seen which happened because

00:32:11,679 --> 00:32:15,919
we lacked a monitoring or visibility

00:32:14,399 --> 00:32:17,600
into the state of the system which could

00:32:15,919 --> 00:32:19,200
have been an early indicator before a

00:32:17,600 --> 00:32:22,000
cascading figure happened

00:32:19,200 --> 00:32:22,559
um in general investing in automation

00:32:22,000 --> 00:32:24,720
and tooling

00:32:22,559 --> 00:32:26,399
around a running search can be really

00:32:24,720 --> 00:32:28,640
helpful in building confidence of

00:32:26,399 --> 00:32:30,399
running the system a good example of

00:32:28,640 --> 00:32:31,919
this is investing in a good cicd

00:32:30,399 --> 00:32:33,679
pipeline which has inbuilt uh

00:32:31,919 --> 00:32:34,559
performance testing so that every time

00:32:33,679 --> 00:32:36,960
you release

00:32:34,559 --> 00:32:38,720
uh something into uh for uh into

00:32:36,960 --> 00:32:40,480
production you run your performance

00:32:38,720 --> 00:32:41,360
tests and integration tests and ensure

00:32:40,480 --> 00:32:44,080
that you're

00:32:41,360 --> 00:32:46,000
not releasing bad code uh finally

00:32:44,080 --> 00:32:47,279
failures will always happen but it is

00:32:46,000 --> 00:32:48,720
important to learn from them

00:32:47,279 --> 00:32:51,120
uh most of the things that i've talked

00:32:48,720 --> 00:32:51,760
about today have been uh lessons learned

00:32:51,120 --> 00:32:53,919
from

00:32:51,760 --> 00:32:55,600
postmoderns that we have done and then

00:32:53,919 --> 00:32:56,080
remediations that have implemented to

00:32:55,600 --> 00:32:59,679
improve

00:32:56,080 --> 00:33:01,760
our uh availability um

00:32:59,679 --> 00:33:03,200
finally we should also document uh

00:33:01,760 --> 00:33:04,480
recovery procedures for standard

00:33:03,200 --> 00:33:06,799
failures like what happens

00:33:04,480 --> 00:33:08,720
uh what to do when a shard goes down or

00:33:06,799 --> 00:33:10,399
what to do when a user is creating us at

00:33:08,720 --> 00:33:11,919
a very high throughput uh

00:33:10,399 --> 00:33:13,519
we should document these procedures and

00:33:11,919 --> 00:33:14,159
run books which are available to the

00:33:13,519 --> 00:33:16,000
on-call

00:33:14,159 --> 00:33:17,360
and then it can save you precious

00:33:16,000 --> 00:33:18,320
minutes when an issue is actually

00:33:17,360 --> 00:33:20,720
happening

00:33:18,320 --> 00:33:22,880
and then lastly uh it's great if you can

00:33:20,720 --> 00:33:26,480
simulate failures in your system before

00:33:22,880 --> 00:33:28,320
they actually happen this can test your

00:33:26,480 --> 00:33:30,240
tools that you have built to handle

00:33:28,320 --> 00:33:32,720
these failures a good example of this is

00:33:30,240 --> 00:33:34,320
chaos monkey that was built by netflix

00:33:32,720 --> 00:33:36,240
uh you can do something like that on

00:33:34,320 --> 00:33:37,760
your end by taking shards offline

00:33:36,240 --> 00:33:39,919
intermittently or causing network

00:33:37,760 --> 00:33:42,399
partitions so this can help you

00:33:39,919 --> 00:33:44,080
test your measures to improve

00:33:42,399 --> 00:33:46,240
availability

00:33:44,080 --> 00:33:48,799
so that brings me to the end of my talk

00:33:46,240 --> 00:33:51,919
today uh hopefully i was able to

00:33:48,799 --> 00:33:53,600
give you some tools and suggestions that

00:33:51,919 --> 00:33:55,600
you can further look into

00:33:53,600 --> 00:33:57,120
when you're trying to improve the

00:33:55,600 --> 00:33:59,760
vulnerability

00:33:57,120 --> 00:34:00,720
posture of your services and especially

00:33:59,760 --> 00:34:03,600
search

00:34:00,720 --> 00:34:05,039
thanks again for your time and i'm open

00:34:03,600 --> 00:34:06,399
to any questions that you have at this

00:34:05,039 --> 00:34:08,800
time

00:34:06,399 --> 00:34:10,800
also a box is always looking for great

00:34:08,800 --> 00:34:11,200
engineers so if these problems excite

00:34:10,800 --> 00:34:13,440
you

00:34:11,200 --> 00:34:14,399
then hit me up and we can talk more

00:34:13,440 --> 00:34:21,839
about

00:34:14,399 --> 00:34:21,839
opportunities at box thank you

00:34:26,159 --> 00:34:32,720
uh so i

00:34:29,359 --> 00:34:35,440
see uh someone posted on the chat uh

00:34:32,720 --> 00:34:37,520
this shard health uh is the shard of

00:34:35,440 --> 00:34:40,560
monitor available to all

00:34:37,520 --> 00:34:49,839
is uh uh

00:34:40,560 --> 00:34:49,839
is this like is it open source or uh

00:34:59,280 --> 00:35:04,000
uh no the shot of monitor monitor is not

00:35:03,280 --> 00:35:07,359
something

00:35:04,000 --> 00:35:11,119
that is open source uh uh we

00:35:07,359 --> 00:35:12,960
we try to contribute uh things back into

00:35:11,119 --> 00:35:14,480
the open source community uh but we

00:35:12,960 --> 00:35:16,800
haven't gotten around to

00:35:14,480 --> 00:35:18,320
uh making charge monitor open source but

00:35:16,800 --> 00:35:21,440
thanks for the suggestion we will

00:35:18,320 --> 00:35:24,880
look into making the shutter monitor and

00:35:21,440 --> 00:35:24,880
open source thing that is available to

00:35:28,839 --> 00:35:31,839
all

00:35:33,200 --> 00:35:37,839
uh so i see another question here uh

00:35:35,920 --> 00:35:40,560
saying if i'm not mistaken solar has

00:35:37,839 --> 00:35:41,839
inbuilt warm-up at least for caches

00:35:40,560 --> 00:35:44,960
would you say it is

00:35:41,839 --> 00:35:48,160
not enough so what we have uh seen

00:35:44,960 --> 00:35:51,760
is um that warming up solar

00:35:48,160 --> 00:35:52,960
using uh static queries or a fixed uh

00:35:51,760 --> 00:35:56,000
set of queries

00:35:52,960 --> 00:35:57,680
uh does not represent the actual user

00:35:56,000 --> 00:36:00,880
traffic that you are getting

00:35:57,680 --> 00:36:02,720
so on top of like any inbuilt setup that

00:36:00,880 --> 00:36:04,800
you have for warming up the caches it's

00:36:02,720 --> 00:36:05,680
always a good idea to warm up a solar

00:36:04,800 --> 00:36:07,680
cluster

00:36:05,680 --> 00:36:09,040
at the amount of traffic that the

00:36:07,680 --> 00:36:11,520
cluster will get uh

00:36:09,040 --> 00:36:13,359
with uh the queries that are you're

00:36:11,520 --> 00:36:14,240
seeing in the live system this ensures

00:36:13,359 --> 00:36:16,160
that you're

00:36:14,240 --> 00:36:17,359
correctly warming up solar and making it

00:36:16,160 --> 00:36:20,079
accustomed to

00:36:17,359 --> 00:36:22,000
uh the throughput and also the kind of

00:36:20,079 --> 00:36:22,480
queries or the kind of filter queries

00:36:22,000 --> 00:36:24,800
that

00:36:22,480 --> 00:36:26,480
that you get from the live system uh and

00:36:24,800 --> 00:36:29,680
that can be helpful in

00:36:26,480 --> 00:36:29,680
preventing failures

00:36:31,119 --> 00:36:37,440
uh so another question that i see is

00:36:34,240 --> 00:36:38,800
uh how time allowed parameter can help

00:36:37,440 --> 00:36:42,480
in case of prevention

00:36:38,800 --> 00:36:43,280
so uh time allowed is a parameter that

00:36:42,480 --> 00:36:46,400
lets you

00:36:43,280 --> 00:36:48,000
uh time out a query so uh

00:36:46,400 --> 00:36:49,520
the prevention that it does is if you

00:36:48,000 --> 00:36:51,040
have an expensive query

00:36:49,520 --> 00:36:52,720
and it's going to and we have seen these

00:36:51,040 --> 00:36:53,680
queries these are like index for all

00:36:52,720 --> 00:36:55,680
inquiries or

00:36:53,680 --> 00:36:57,119
de-pagination queries which can run for

00:36:55,680 --> 00:36:59,440
hours on your index

00:36:57,119 --> 00:37:00,240
and when the query is running on that

00:36:59,440 --> 00:37:03,359
system

00:37:00,240 --> 00:37:04,000
then this will stream the specific chart

00:37:03,359 --> 00:37:06,800
on solar

00:37:04,000 --> 00:37:08,320
and any other query going to that shard

00:37:06,800 --> 00:37:10,880
is going to cause periods

00:37:08,320 --> 00:37:12,320
so if you pass the time allow parameter

00:37:10,880 --> 00:37:13,760
which possibly can time

00:37:12,320 --> 00:37:15,839
uh time out the query at the query

00:37:13,760 --> 00:37:18,079
expansion stage uh if it is

00:37:15,839 --> 00:37:19,920
if it has a large filter query or time

00:37:18,079 --> 00:37:23,200
it out before document collection

00:37:19,920 --> 00:37:23,920
then it can uh basically stop the query

00:37:23,200 --> 00:37:26,160
before it

00:37:23,920 --> 00:37:26,960
runs over that chart and it can stop it

00:37:26,160 --> 00:37:28,560
from failing

00:37:26,960 --> 00:37:37,839
other queries that go to the chart which

00:37:28,560 --> 00:37:37,839
will help improve your availability

00:37:54,000 --> 00:37:57,920
uh so i i think we are almost out of

00:37:57,200 --> 00:38:00,480
time but

00:37:57,920 --> 00:38:01,119
uh if you guys have uh more questions

00:38:00,480 --> 00:38:05,440
you can

00:38:01,119 --> 00:38:19,839
join the slack channel for this session

00:38:05,440 --> 00:38:19,839
and you can ask me more questions

00:38:30,640 --> 00:38:35,359
uh david i see you have a question there

00:38:33,119 --> 00:38:35,359
uh

00:38:35,440 --> 00:38:38,960
uh regarding uh building solar segments

00:38:38,400 --> 00:38:41,440
offline

00:38:38,960 --> 00:38:43,280
uh so it'll take me some time to answer

00:38:41,440 --> 00:38:46,480
this so if you want to connect uh

00:38:43,280 --> 00:38:48,720
one-on-one or we can uh take this up on

00:38:46,480 --> 00:38:49,920
the slack channel this is something we

00:38:48,720 --> 00:39:01,839
are currently working on

00:38:49,920 --> 00:39:01,839
as well so we can discuss this more

00:40:15,520 --> 00:40:21,520
so i see another question from matt uh

00:40:19,119 --> 00:40:23,280
matt appears to there i i can answer it

00:40:21,520 --> 00:40:24,400
here or you can ask me again on the

00:40:23,280 --> 00:40:26,960
slack channel

00:40:24,400 --> 00:40:29,280
uh the question is is the short spelling

00:40:26,960 --> 00:40:29,599
mechanism a part of your offline indexer

00:40:29,280 --> 00:40:32,720
or

00:40:29,599 --> 00:40:35,440
was that just for routing traffic so

00:40:32,720 --> 00:40:35,760
the short spilling mechanism is a part

00:40:35,440 --> 00:40:39,040
of

00:40:35,760 --> 00:40:40,640
our live index process so

00:40:39,040 --> 00:40:42,480
or the live system actually not the

00:40:40,640 --> 00:40:46,000
offline indexer so

00:40:42,480 --> 00:40:47,760
whenever uh our index uh threshold is

00:40:46,000 --> 00:40:49,440
exceeded by a specific shard

00:40:47,760 --> 00:40:51,440
uh we initiate a short spilling

00:40:49,440 --> 00:40:54,480
mechanism which updates uh

00:40:51,440 --> 00:40:56,640
the routing logic for the index as well

00:40:54,480 --> 00:40:58,560
as the query side and then

00:40:56,640 --> 00:40:59,920
we use zookeeper to update this

00:40:58,560 --> 00:41:01,280
information to

00:40:59,920 --> 00:41:03,119
all the services that need this

00:41:01,280 --> 00:41:06,079
information for routing so that

00:41:03,119 --> 00:41:08,400
they can update their logging tables and

00:41:06,079 --> 00:41:11,839
accordingly route requests to the

00:41:08,400 --> 00:41:12,560
spelled shard instead or query both this

00:41:11,839 --> 00:41:21,839
build shard

00:41:12,560 --> 00:41:21,839
and the original

00:41:38,880 --> 00:41:44,160
uh so i see another uh

00:41:42,079 --> 00:41:45,440
question if a query is running for more

00:41:44,160 --> 00:41:47,839
than 30 minutes

00:41:45,440 --> 00:41:49,200
how to find out which component uh is

00:41:47,839 --> 00:41:51,680
consuming more time

00:41:49,200 --> 00:41:52,560
so this is a very good question uh a

00:41:51,680 --> 00:41:55,119
great way

00:41:52,560 --> 00:41:55,760
to address something like this is uh to

00:41:55,119 --> 00:41:59,280
invest in

00:41:55,760 --> 00:42:01,520
tracing and locking so

00:41:59,280 --> 00:42:02,480
the way we do this is whenever a query

00:42:01,520 --> 00:42:05,520
enters our

00:42:02,480 --> 00:42:08,160
system we assign it a a

00:42:05,520 --> 00:42:09,839
trace id and then uh using that we can

00:42:08,160 --> 00:42:11,359
trace uh the queries

00:42:09,839 --> 00:42:13,680
a query all through the lifetime of the

00:42:11,359 --> 00:42:16,400
query so

00:42:13,680 --> 00:42:17,040
we can go and look at the logs at every

00:42:16,400 --> 00:42:19,599
layer

00:42:17,040 --> 00:42:20,560
and we can also look our at our analytic

00:42:19,599 --> 00:42:22,400
system to see

00:42:20,560 --> 00:42:24,400
where the query consumed most of its

00:42:22,400 --> 00:42:27,520
time so

00:42:24,400 --> 00:42:29,599
you can look at the logs at the

00:42:27,520 --> 00:42:31,440
solar level to see how much time solar

00:42:29,599 --> 00:42:33,119
consumed uh solar returns something

00:42:31,440 --> 00:42:34,160
called a queue time which is the amount

00:42:33,119 --> 00:42:37,040
of time it took

00:42:34,160 --> 00:42:39,040
and then uh for at each of the layers we

00:42:37,040 --> 00:42:41,280
also have timers which uh time

00:42:39,040 --> 00:42:42,160
the query and then these timers are

00:42:41,280 --> 00:42:44,160
updated to

00:42:42,160 --> 00:42:45,920
wayfront so we have a dashboard which

00:42:44,160 --> 00:42:48,960
also looks at p95

00:42:45,920 --> 00:42:50,240
latency breakdown of queries at each

00:42:48,960 --> 00:42:52,240
component level so

00:42:50,240 --> 00:42:53,760
whenever you're having a latency spike

00:42:52,240 --> 00:42:55,359
live you can

00:42:53,760 --> 00:42:57,200
detect which particular component is

00:42:55,359 --> 00:42:58,480
having issues or if it is a specific

00:42:57,200 --> 00:43:00,800
query you can trace it

00:42:58,480 --> 00:43:01,680
uh using the trace id in your logs uh to

00:43:00,800 --> 00:43:05,200
look at uh

00:43:01,680 --> 00:43:05,760
what the issue was uh the next question

00:43:05,200 --> 00:43:08,319
is uh

00:43:05,760 --> 00:43:10,160
do you have zookeeper on its own host uh

00:43:08,319 --> 00:43:12,560
yeah we run our own zookeeper

00:43:10,160 --> 00:43:13,760
uh cluster and that's the zookeeper we

00:43:12,560 --> 00:43:26,160
use we don't use this

00:43:13,760 --> 00:43:28,240
zookeeper that's internal to solar

00:43:26,160 --> 00:43:29,280
so uh another question that that was

00:43:28,240 --> 00:43:32,319
asked is uh

00:43:29,280 --> 00:43:35,440
for a solar cloud with real-time updates

00:43:32,319 --> 00:43:38,560
uh how to use uh offline index build

00:43:35,440 --> 00:43:42,079
uh so i i don't know uh about

00:43:38,560 --> 00:43:44,240
uh if it is different with uh solar

00:43:42,079 --> 00:43:45,839
clouds specifically uh but what do you

00:43:44,240 --> 00:43:48,800
mean uh

00:43:45,839 --> 00:43:50,880
what i understand is uh uh how do we

00:43:48,800 --> 00:43:54,000
deal with real-time updates uh

00:43:50,880 --> 00:43:57,440
if if that is your question then the way

00:43:54,000 --> 00:43:59,680
we handle it is you you have your

00:43:57,440 --> 00:44:00,880
offline index and your real-time updates

00:43:59,680 --> 00:44:04,319
are being found out

00:44:00,880 --> 00:44:07,280
uh to the offline index whenever

00:44:04,319 --> 00:44:08,079
the updates happen along with your solar

00:44:07,280 --> 00:44:11,040
clusters

00:44:08,079 --> 00:44:11,920
and then when you have to build an index

00:44:11,040 --> 00:44:13,760
offline

00:44:11,920 --> 00:44:15,119
you have uh the index builder

00:44:13,760 --> 00:44:18,720
continuously generating

00:44:15,119 --> 00:44:21,040
uh offline segments so you can remember

00:44:18,720 --> 00:44:22,160
the last offset that you processed in

00:44:21,040 --> 00:44:25,200
your indexing queue

00:44:22,160 --> 00:44:28,480
which uh possibly is built on kafka

00:44:25,200 --> 00:44:31,200
uh and or some queueing system like that

00:44:28,480 --> 00:44:31,920
uh and then whatever real-time updates

00:44:31,200 --> 00:44:34,560
came in

00:44:31,920 --> 00:44:36,319
uh since the last officer two processed

00:44:34,560 --> 00:44:38,640
uh you can replay those uh

00:44:36,319 --> 00:44:39,680
from the queue and then your index will

00:44:38,640 --> 00:44:42,880
be caught up with

00:44:39,680 --> 00:44:44,960
the uh current state of uh the system

00:44:42,880 --> 00:44:46,640
and uh always like once once you have

00:44:44,960 --> 00:44:47,599
downloaded the segments you have to

00:44:46,640 --> 00:44:49,839
start replaying

00:44:47,599 --> 00:44:52,640
the live indexes otherwise you'll

00:44:49,839 --> 00:44:54,160
continuously play catch-up so

00:44:52,640 --> 00:44:58,160
you start consuming from the queue and

00:44:54,160 --> 00:44:58,160
then replay from a particular offset

00:45:02,079 --> 00:45:06,480
uh yeah so joseph uh asked i meant does

00:45:05,599 --> 00:45:08,800
the zookeeper run

00:45:06,480 --> 00:45:09,599
on the solar post or uh does zk get

00:45:08,800 --> 00:45:12,960
dedicated

00:45:09,599 --> 00:45:14,720
uh uh we uh do not use uh

00:45:12,960 --> 00:45:16,000
or do not run zookeeper on the solar

00:45:14,720 --> 00:45:19,839
hose they

00:45:16,000 --> 00:45:19,839
run on separate hosts

00:45:24,240 --> 00:45:29,040
all right i think we are really quite

00:45:27,760 --> 00:45:31,440
out of time

00:45:29,040 --> 00:45:32,160
thank you again for those of you who

00:45:31,440 --> 00:45:34,560
stayed around

00:45:32,160 --> 00:45:36,480
and asked more questions uh again you

00:45:34,560 --> 00:45:37,440
can ask me more questions on the search

00:45:36,480 --> 00:45:40,720
slack channel

00:45:37,440 --> 00:45:47,839
uh thanks again for your time and have a

00:45:40,720 --> 00:45:47,839
great day

00:45:54,240 --> 00:45:56,319

YouTube URL: https://www.youtube.com/watch?v=fLN0LWM9KSA


