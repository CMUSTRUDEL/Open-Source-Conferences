Title: Monitoring Apache Solr Ecosystem on Kubernetes
Publication date: 2020-10-17
Playlist: ApacheCon @Home 2020: Search
Description: 
	Monitoring Apache Solr Ecosystem on Kubernetes
Amrit Sarkar

A presentation from ApacheCon @Home 2020
https://apachecon.com/acah2020/

Kubernetes is fast becoming the operating system for the Cloud and brings a ubiquity that has the potential for massive benefits for technology organizations. Applications/Microservices are moved to orchestration tools like Kubernetes to leverage features like horizontal autoscaling, fault tolerance, CICD and more. Apache Solr can be deployed on Kubernetes on a large-scale for a plethora of use cases. For such scale, effective metric dashboards, log analytics, monitoring, and alerting system is a requirement to make sure abnormal behaviors are detected, error diagnostics are performed and the ability to fine-tune the entire ecosystem to reach the best possible performance. In this talk, we discuss and compare various monitoring and analytics tools for the Solr ecosystem running on Kubernetes. From inbuilt features to third-party tools which provide powerful yet easy to use dynamic dashboards and OpenTracing support.

Amrit Sarkar is Cloud Search Reliability Engineer at Lucidworks Inc, California-based enterprise search technology company, with 4+ years experience in search domain and big data, e-commerce and product. He is working primarily on running search-based applications on Kubernetes, and developing and improving core components of Apache Solr.
Captions: 
	00:00:06,420 --> 00:00:10,640
[Music]

00:00:14,160 --> 00:00:17,230
[Music]

00:00:25,519 --> 00:00:30,400
well for this session we have

00:00:27,119 --> 00:00:31,359
uh amrit sarkar from lucidworks uh amrit

00:00:30,400 --> 00:00:34,000
is an active

00:00:31,359 --> 00:00:35,520
apache lucy and solar community member

00:00:34,000 --> 00:00:37,040
and currently works on running

00:00:35,520 --> 00:00:40,320
search-based applications

00:00:37,040 --> 00:00:41,760
on kubernetes today amrit will be

00:00:40,320 --> 00:00:43,680
talking about comparing various

00:00:41,760 --> 00:00:45,520
monitoring and analytics tools

00:00:43,680 --> 00:00:46,800
for solar ecosystem running on

00:00:45,520 --> 00:00:49,039
kubernetes

00:00:46,800 --> 00:00:50,800
hope you enjoyed this session um over to

00:00:49,039 --> 00:00:54,239
you amrit

00:00:50,800 --> 00:00:58,640
uh thank you anishin um i am just

00:00:54,239 --> 00:01:01,120
sharing my screen to get started with

00:00:58,640 --> 00:01:01,120
here we go

00:01:06,159 --> 00:01:11,680
all right i hope everyone is able to see

00:01:09,200 --> 00:01:14,640
my screen and my voice is audible

00:01:11,680 --> 00:01:16,240
so welcome to apache con at home it's

00:01:14,640 --> 00:01:18,720
good to be back after your break

00:01:16,240 --> 00:01:20,240
um all the other tracks have one such

00:01:18,720 --> 00:01:22,560
kubernetes discussion

00:01:20,240 --> 00:01:24,560
hence this is the one for this track uh

00:01:22,560 --> 00:01:26,479
we will be covering how to monitor

00:01:24,560 --> 00:01:28,640
apache solar and the auxiliary

00:01:26,479 --> 00:01:31,280
components when hosted on

00:01:28,640 --> 00:01:31,280
kubernetes

00:01:32,479 --> 00:01:36,960
so i work as an engineer in the cloud

00:01:35,280 --> 00:01:39,600
operations team at recede works

00:01:36,960 --> 00:01:41,040
and we worked on putting apache solar

00:01:39,600 --> 00:01:43,040
with devops automation

00:01:41,040 --> 00:01:44,880
smart auto scaling and backup features

00:01:43,040 --> 00:01:47,119
on the cloud in last few months

00:01:44,880 --> 00:01:48,320
apart from that i am fascinated by the

00:01:47,119 --> 00:01:49,840
work done in

00:01:48,320 --> 00:01:51,520
deep learning space with respect to

00:01:49,840 --> 00:01:53,600
information retrieval

00:01:51,520 --> 00:01:55,600
the intention of this talk is to promote

00:01:53,600 --> 00:01:59,360
solar adoption on cloud

00:01:55,600 --> 00:01:59,360
with kubernetes specifically

00:02:01,680 --> 00:02:05,920
so the agenda will be straightforward we

00:02:04,479 --> 00:02:07,600
will discuss how to run solar on

00:02:05,920 --> 00:02:09,200
kubernetes from a higher level

00:02:07,600 --> 00:02:11,120
we won't go into the implementation

00:02:09,200 --> 00:02:13,599
details a glance

00:02:11,120 --> 00:02:14,879
at inbuilt monitoring capabilities of

00:02:13,599 --> 00:02:16,879
solar

00:02:14,879 --> 00:02:18,000
following we move on to discuss some

00:02:16,879 --> 00:02:19,920
open source tools

00:02:18,000 --> 00:02:21,200
and one commercial offering for

00:02:19,920 --> 00:02:22,160
monitoring and locking curricular

00:02:21,200 --> 00:02:25,920
clusters

00:02:22,160 --> 00:02:25,920
when we host them on kubernetes

00:02:27,920 --> 00:02:33,040
so at past years at apache con we have

00:02:30,959 --> 00:02:35,519
discussed deployment of solar cloud

00:02:33,040 --> 00:02:36,959
and complementary services at scale in

00:02:35,519 --> 00:02:38,800
great detail

00:02:36,959 --> 00:02:40,879
the open source community of flucine and

00:02:38,800 --> 00:02:41,680
solar have already built new exciting

00:02:40,879 --> 00:02:44,160
features

00:02:41,680 --> 00:02:46,080
to make the job much easier for us but

00:02:44,160 --> 00:02:47,519
still there is a great deal of micro

00:02:46,080 --> 00:02:50,720
management involved

00:02:47,519 --> 00:02:53,360
like migrating between versions expand

00:02:50,720 --> 00:02:55,040
cluster dynamically or having multiple

00:02:53,360 --> 00:02:57,280
clusters in a different region

00:02:55,040 --> 00:02:58,319
to counter at single point of failure

00:02:57,280 --> 00:03:01,840
developers and

00:02:58,319 --> 00:03:04,400
devops needs to be on point there is

00:03:01,840 --> 00:03:06,159
a need of an r for a potential manager

00:03:04,400 --> 00:03:07,040
we can which can accommodate such

00:03:06,159 --> 00:03:12,800
nuances

00:03:07,040 --> 00:03:12,800
and kubernetes can help us do that

00:03:15,360 --> 00:03:20,400
so kubernetes is an orchestrational tool

00:03:18,560 --> 00:03:22,560
a system used for running and

00:03:20,400 --> 00:03:25,280
coordinating containerized applications

00:03:22,560 --> 00:03:26,879
across cluster of machines kubernetes

00:03:25,280 --> 00:03:28,799
maximizes capacity

00:03:26,879 --> 00:03:30,640
by logically and efficiently

00:03:28,799 --> 00:03:32,799
distributing containers

00:03:30,640 --> 00:03:34,480
based on the demand light traffic more

00:03:32,799 --> 00:03:36,400
data data segregation

00:03:34,480 --> 00:03:37,760
the machines can be scaled up or it can

00:03:36,400 --> 00:03:40,720
be scaled down

00:03:37,760 --> 00:03:42,640
moreover it makes sure the processes we

00:03:40,720 --> 00:03:44,000
configured keeps on running and healthy

00:03:42,640 --> 00:03:46,159
most of the time

00:03:44,000 --> 00:03:48,879
kubernetes does all of this without any

00:03:46,159 --> 00:03:48,879
micro management

00:03:50,159 --> 00:03:53,439
so now we will just jump right into the

00:03:52,720 --> 00:03:57,680
discussion

00:03:53,439 --> 00:04:00,480
of how to host solar cloud on kubernetes

00:03:57,680 --> 00:04:02,239
so first of all solar cloud needs

00:04:00,480 --> 00:04:03,920
zookeeper ensemble for management of

00:04:02,239 --> 00:04:05,040
configurations and cluster state

00:04:03,920 --> 00:04:07,840
information

00:04:05,040 --> 00:04:08,560
zookeeper processes in kubernetes terms

00:04:07,840 --> 00:04:11,280
pods

00:04:08,560 --> 00:04:12,159
are hosted by a stateful set a stateful

00:04:11,280 --> 00:04:15,519
set creates

00:04:12,159 --> 00:04:19,040
pods in a unique order like zk0 zk1

00:04:15,519 --> 00:04:20,000
and so on these zookeeper pods are

00:04:19,040 --> 00:04:23,359
attached with

00:04:20,000 --> 00:04:23,359
persistent volume which is

00:04:24,160 --> 00:04:28,160
which is the i'm like there is a reason

00:04:25,680 --> 00:04:30,160
for it so if one of the zookeeper pod

00:04:28,160 --> 00:04:32,800
crashes due to various reasons

00:04:30,160 --> 00:04:33,919
an exception out of memory or a machine

00:04:32,800 --> 00:04:35,680
got corrupted

00:04:33,919 --> 00:04:37,120
uh kubernetes will spin another

00:04:35,680 --> 00:04:38,720
zookeeper pod

00:04:37,120 --> 00:04:40,400
on the available node and due to its

00:04:38,720 --> 00:04:43,360
unique identification which is

00:04:40,400 --> 00:04:45,040
0 1 and so on same persistent volume

00:04:43,360 --> 00:04:47,520
gets attached to it

00:04:45,040 --> 00:04:49,040
um a zookeeper part is exposed by two

00:04:47,520 --> 00:04:51,759
types of services

00:04:49,040 --> 00:04:53,199
a headless one for each ball for

00:04:51,759 --> 00:04:58,479
internal communication

00:04:53,199 --> 00:05:00,080
within the ensemble and a standard one

00:04:58,479 --> 00:05:02,560
to expose to other services and

00:05:00,080 --> 00:05:02,560
application

00:05:04,720 --> 00:05:09,039
a unique zookeeper and sample string

00:05:07,600 --> 00:05:11,280
generated from this module

00:05:09,039 --> 00:05:12,800
is configured in our solar setup on the

00:05:11,280 --> 00:05:14,639
similar lines with zookeeper

00:05:12,800 --> 00:05:16,720
solar is hosted with stateful set

00:05:14,639 --> 00:05:19,039
template for similar reasons

00:05:16,720 --> 00:05:20,320
solar cluster is exposed by two services

00:05:19,039 --> 00:05:21,360
a headless one for internal

00:05:20,320 --> 00:05:24,080
communication

00:05:21,360 --> 00:05:25,520
and a general one to expose it to other

00:05:24,080 --> 00:05:29,280
services and application

00:05:25,520 --> 00:05:31,039
and finally the solar module

00:05:29,280 --> 00:05:33,600
is exposed to general traffic with

00:05:31,039 --> 00:05:35,840
either a load balancer or an ingress

00:05:33,600 --> 00:05:37,120
the big downside of load balancer which

00:05:35,840 --> 00:05:39,520
is a cloud provider based

00:05:37,120 --> 00:05:40,880
is that each solar service you exposed

00:05:39,520 --> 00:05:44,000
with as load balancer

00:05:40,880 --> 00:05:46,080
will get its own ip address and you have

00:05:44,000 --> 00:05:46,720
to pay for a load balancer per export

00:05:46,080 --> 00:05:48,800
service

00:05:46,720 --> 00:05:50,560
which can get pretty expensive for

00:05:48,800 --> 00:05:52,880
ingress you can specify custom route

00:05:50,560 --> 00:05:54,400
names and host names to setup services

00:05:52,880 --> 00:05:56,479
and if you are running number of solar

00:05:54,400 --> 00:05:57,759
nodes ingress makes it much more

00:05:56,479 --> 00:05:59,280
convenient

00:05:57,759 --> 00:06:00,880
i have abstracted great deal of

00:05:59,280 --> 00:06:02,800
implementation details from

00:06:00,880 --> 00:06:04,479
node discovery mechanism to service

00:06:02,800 --> 00:06:07,280
discovery normal glacier

00:06:04,479 --> 00:06:07,759
so please check out um talks which has

00:06:07,280 --> 00:06:09,600
been done

00:06:07,759 --> 00:06:12,400
in various conferences at the past to

00:06:09,600 --> 00:06:12,400
get the gist of it

00:06:16,160 --> 00:06:19,440
so to get started with kubernetes there

00:06:18,319 --> 00:06:21,039
is no better than

00:06:19,440 --> 00:06:23,199
google cloud platform where you can

00:06:21,039 --> 00:06:24,479
create solar clusters on gke in few

00:06:23,199 --> 00:06:26,960
minutes

00:06:24,479 --> 00:06:28,560
uh lucid works have open sourced the

00:06:26,960 --> 00:06:30,240
helm chart for deploying solar cloud

00:06:28,560 --> 00:06:32,800
clusters on kubernetes

00:06:30,240 --> 00:06:34,080
a helm chart is a baseline module which

00:06:32,800 --> 00:06:37,280
can be configured easily

00:06:34,080 --> 00:06:39,120
to create deployment yaml file

00:06:37,280 --> 00:06:40,880
yeah so after downloading the chart we

00:06:39,120 --> 00:06:43,120
need to execute the following two

00:06:40,880 --> 00:06:44,479
helm commands from command line uh which

00:06:43,120 --> 00:06:46,400
ensures to download all

00:06:44,479 --> 00:06:48,240
dependencies if linked to the current

00:06:46,400 --> 00:06:50,960
chart in this case for solar

00:06:48,240 --> 00:06:52,720
zookeeper chart will be downloaded a

00:06:50,960 --> 00:06:54,560
helm template command will generate the

00:06:52,720 --> 00:06:57,680
respective deployment yaml

00:06:54,560 --> 00:07:00,080
file and following we execute the

00:06:57,680 --> 00:07:02,080
cubectl the kubernetes command line

00:07:00,080 --> 00:07:03,759
tool command which will interact with

00:07:02,080 --> 00:07:04,560
the kubernetes clusters hosted on the

00:07:03,759 --> 00:07:07,280
gcp

00:07:04,560 --> 00:07:10,000
and deploy the respective services and

00:07:07,280 --> 00:07:10,000
statement sets

00:07:10,880 --> 00:07:16,479
so this is a typical solar cluster um

00:07:14,240 --> 00:07:18,400
default running on kubernetes with three

00:07:16,479 --> 00:07:21,280
zookeeper parts creating the ensemble

00:07:18,400 --> 00:07:22,720
two solar pods and as respected services

00:07:21,280 --> 00:07:23,440
a headless service and a standard

00:07:22,720 --> 00:07:26,479
service

00:07:23,440 --> 00:07:27,599
with an ip and the persistent volumes on

00:07:26,479 --> 00:07:30,639
which these

00:07:27,599 --> 00:07:32,720
deployment sets are running

00:07:30,639 --> 00:07:35,039
are hosted on ssds which is solid state

00:07:32,720 --> 00:07:35,039
drives

00:07:38,880 --> 00:07:42,639
so we can deploy multiple solar cloud

00:07:41,840 --> 00:07:45,360
clusters

00:07:42,639 --> 00:07:45,680
within a single kubernetes cluster all

00:07:45,360 --> 00:07:48,240
the

00:07:45,680 --> 00:07:49,919
resources configurations parts services

00:07:48,240 --> 00:07:52,000
persistent volume definition

00:07:49,919 --> 00:07:53,919
can be isolated from one another via

00:07:52,000 --> 00:07:56,000
different namespace

00:07:53,919 --> 00:07:57,360
resources in different namespaces can

00:07:56,000 --> 00:08:00,319
still talk to each other

00:07:57,360 --> 00:08:03,440
by a definite discovery mechanism if

00:08:00,319 --> 00:08:03,440
there is a requirement

00:08:05,039 --> 00:08:10,879
so when we set up our solar cloud

00:08:08,400 --> 00:08:11,520
on kubernetes though communities is

00:08:10,879 --> 00:08:14,800
adding

00:08:11,520 --> 00:08:16,720
ton of exciting management capabilities

00:08:14,800 --> 00:08:18,160
we are adding another layer to our

00:08:16,720 --> 00:08:21,199
search architecture

00:08:18,160 --> 00:08:21,759
it is fairly important to know and

00:08:21,199 --> 00:08:24,479
understand

00:08:21,759 --> 00:08:26,879
the advantages limitations saturation

00:08:24,479 --> 00:08:29,680
level of the services and how one

00:08:26,879 --> 00:08:30,879
is affecting another from a query

00:08:29,680 --> 00:08:33,279
perspective

00:08:30,879 --> 00:08:34,399
we need to reach a baseline metric for

00:08:33,279 --> 00:08:36,399
our response time

00:08:34,399 --> 00:08:40,240
to put a design and effort in place so

00:08:36,399 --> 00:08:42,800
that we can improve in the future

00:08:40,240 --> 00:08:44,080
so let's look at first that what we

00:08:42,800 --> 00:08:46,320
actually need

00:08:44,080 --> 00:08:48,080
while we want to monitor apache solar

00:08:46,320 --> 00:08:50,080
ecosystem on kubernetes

00:08:48,080 --> 00:08:52,880
right so we can grill down to some

00:08:50,080 --> 00:08:52,880
definite points

00:08:53,200 --> 00:08:58,080
do we have inbuilt support for

00:08:55,600 --> 00:08:59,839
retrieving and utilizing solar metrics

00:08:58,080 --> 00:09:01,680
now engineers working on solar knows

00:08:59,839 --> 00:09:03,680
that emits valuable metrics from

00:09:01,680 --> 00:09:06,160
solar cloud specific to solar core

00:09:03,680 --> 00:09:06,160
specific

00:09:06,720 --> 00:09:11,120
further integration with kubernetes

00:09:09,200 --> 00:09:13,120
internals will be a huge advantage

00:09:11,120 --> 00:09:15,040
kubernetes hosts a service called cube

00:09:13,120 --> 00:09:17,120
state metrics which emits internal

00:09:15,040 --> 00:09:19,519
status metadata information about the

00:09:17,120 --> 00:09:19,519
cluster

00:09:20,240 --> 00:09:24,240
one of the core requirements of a

00:09:21,920 --> 00:09:25,760
monitoring is what caused my solar

00:09:24,240 --> 00:09:28,240
applications to be slow

00:09:25,760 --> 00:09:30,240
which component is responsible for it we

00:09:28,240 --> 00:09:31,760
generally use parameter debug equal to

00:09:30,240 --> 00:09:33,600
timing with our query

00:09:31,760 --> 00:09:34,800
to identify the component taking the

00:09:33,600 --> 00:09:36,240
maximum time

00:09:34,800 --> 00:09:38,959
is there a better way to perform

00:09:36,240 --> 00:09:38,959
diagnostics

00:09:39,519 --> 00:09:42,880
so log analytics is extremely important

00:09:41,920 --> 00:09:44,640
aspect to

00:09:42,880 --> 00:09:45,920
diagnose and understand application

00:09:44,640 --> 00:09:47,760
behavior

00:09:45,920 --> 00:09:49,600
ease of reading filter through our

00:09:47,760 --> 00:09:50,000
potential for correlating logs of

00:09:49,600 --> 00:09:52,560
different

00:09:50,000 --> 00:09:55,519
services or of your architecture can be

00:09:52,560 --> 00:09:55,519
also a huge plus

00:09:56,800 --> 00:10:01,200
so does the monitoring applications we

00:09:58,720 --> 00:10:02,959
have chosen uh integrates with the other

00:10:01,200 --> 00:10:06,240
open source or third party monitoring

00:10:02,959 --> 00:10:06,240
logging applications or not

00:10:06,320 --> 00:10:09,760
do we have a rough list of actual

00:10:08,640 --> 00:10:11,519
pressure points of

00:10:09,760 --> 00:10:13,360
solar cluster when we say the

00:10:11,519 --> 00:10:14,240
application is not performing to its

00:10:13,360 --> 00:10:17,279
potential

00:10:14,240 --> 00:10:18,160
or throwing exceptions and errors do we

00:10:17,279 --> 00:10:20,000
have a mechanism

00:10:18,160 --> 00:10:21,279
to notify the concerned engineers or

00:10:20,000 --> 00:10:23,200
devops

00:10:21,279 --> 00:10:26,000
that such an event has happened by

00:10:23,200 --> 00:10:26,000
raising an alert

00:10:26,320 --> 00:10:31,360
and then finally can our monitoring

00:10:29,120 --> 00:10:33,279
application predict a future failure

00:10:31,360 --> 00:10:34,800
based on a pattern it can be an

00:10:33,279 --> 00:10:36,880
artificial intelligence based

00:10:34,800 --> 00:10:40,160
or it can be based on a simple basic

00:10:36,880 --> 00:10:40,160
mathematical analysis

00:10:42,399 --> 00:10:47,839
so let us look at first words available

00:10:45,760 --> 00:10:50,480
in solar out of the box right

00:10:47,839 --> 00:10:51,519
so solar includes a developer api and

00:10:50,480 --> 00:10:52,800
instrumentation

00:10:51,519 --> 00:10:54,399
for the collection of detailed

00:10:52,800 --> 00:10:56,640
performance oriented metrics throughout

00:10:54,399 --> 00:10:59,839
the life cycle of a solar service

00:10:56,640 --> 00:11:02,800
now each group of related metrics

00:10:59,839 --> 00:11:03,839
is managed by a metric registry solar

00:11:02,800 --> 00:11:07,120
supports

00:11:03,839 --> 00:11:07,920
such these registries whether from a

00:11:07,120 --> 00:11:11,360
higher level

00:11:07,920 --> 00:11:14,000
that is jvm jetty node and code

00:11:11,360 --> 00:11:15,680
and open tracing is also being

00:11:14,000 --> 00:11:18,720
introduced in the recent version

00:11:15,680 --> 00:11:20,800
which is an api and infrastructure in

00:11:18,720 --> 00:11:22,880
place for distributed tracing

00:11:20,800 --> 00:11:23,920
we can profile and monitor applications

00:11:22,880 --> 00:11:26,079
especially built within the

00:11:23,920 --> 00:11:27,920
microservices architecture

00:11:26,079 --> 00:11:29,279
you can track a particular request with

00:11:27,920 --> 00:11:31,279
its reading identifier

00:11:29,279 --> 00:11:33,760
the pinpoint causes a failure and what

00:11:31,279 --> 00:11:35,600
causes the poor performance

00:11:33,760 --> 00:11:37,120
in technical terms it allows developer

00:11:35,600 --> 00:11:40,240
to add instrumentation

00:11:37,120 --> 00:11:40,240
to their application code

00:11:41,360 --> 00:11:46,720
so open tracing api was integrated with

00:11:44,160 --> 00:11:49,920
solar in the version 8.2.0

00:11:46,720 --> 00:11:52,639
last year uh thanks to common dot

00:11:49,920 --> 00:11:55,040
user can utilize a jager content module

00:11:52,639 --> 00:11:56,880
called solar yeager tracer configurator

00:11:55,040 --> 00:11:59,120
to set up jager monitoring tool as a

00:11:56,880 --> 00:12:00,800
back-end jager is a very popular

00:11:59,120 --> 00:12:02,399
distributed tracing system open sourced

00:12:00,800 --> 00:12:05,519
by uber

00:12:02,399 --> 00:12:07,519
in version 8. basic tracing for

00:12:05,519 --> 00:12:09,200
outbound communications on querying and

00:12:07,519 --> 00:12:11,279
indexing was introduced

00:12:09,200 --> 00:12:13,200
with potential for adding tracing into

00:12:11,279 --> 00:12:16,079
overseer operations and

00:12:13,200 --> 00:12:17,760
collections api in later versions there

00:12:16,079 --> 00:12:19,519
has not been much work being done

00:12:17,760 --> 00:12:22,480
after the initial release but there is a

00:12:19,519 --> 00:12:22,480
huge potential there

00:12:22,880 --> 00:12:27,279
so here is a typical trace of a post

00:12:25,519 --> 00:12:30,240
request indexing to solar

00:12:27,279 --> 00:12:31,040
uh we have two solar nodes running with

00:12:30,240 --> 00:12:33,120
on ports

00:12:31,040 --> 00:12:34,399
five zero six seven four and five zero

00:12:33,120 --> 00:12:36,639
six seven five

00:12:34,399 --> 00:12:39,200
a post request received by node one

00:12:36,639 --> 00:12:42,000
spends 300 milliseconds simultaneously

00:12:39,200 --> 00:12:43,120
with node 2 while node 2 spends another

00:12:42,000 --> 00:12:45,519
200 millisecond

00:12:43,120 --> 00:12:47,920
potentially for writing it to the desk

00:12:45,519 --> 00:12:50,000
in in a similar manner you can trace a

00:12:47,920 --> 00:12:52,079
query request and witness time spent on

00:12:50,000 --> 00:12:55,040
processing spell checker highlighter

00:12:52,079 --> 00:12:55,040
and other components

00:12:57,279 --> 00:13:02,720
so so we have to start our

00:13:00,560 --> 00:13:03,680
discussion of monetary applications with

00:13:02,720 --> 00:13:06,560
prometheus

00:13:03,680 --> 00:13:08,560
which has a extremely smooth integration

00:13:06,560 --> 00:13:10,480
with kubernetes

00:13:08,560 --> 00:13:11,680
the health chart for generating the

00:13:10,480 --> 00:13:14,320
deployment yaml

00:13:11,680 --> 00:13:15,440
is stated is available on the stated

00:13:14,320 --> 00:13:17,279
link

00:13:15,440 --> 00:13:20,000
to make the integration excellent as i

00:13:17,279 --> 00:13:22,800
stated prometheus operator was added

00:13:20,000 --> 00:13:24,880
in helm charts which provides easy

00:13:22,800 --> 00:13:27,200
monitoring of communities deployment

00:13:24,880 --> 00:13:30,000
and services along with managing

00:13:27,200 --> 00:13:31,920
prometheus alert manager and grafana

00:13:30,000 --> 00:13:33,680
it is based on a service monitor a

00:13:31,920 --> 00:13:36,320
custom resource definition

00:13:33,680 --> 00:13:37,600
to abstract configurations we need for a

00:13:36,320 --> 00:13:40,800
target application

00:13:37,600 --> 00:13:41,839
or service so this is a typical service

00:13:40,800 --> 00:13:44,560
monitor

00:13:41,839 --> 00:13:45,519
which will match label stated here app

00:13:44,560 --> 00:13:47,760
column solar

00:13:45,519 --> 00:13:49,040
and prometheus operator will des will

00:13:47,760 --> 00:13:53,199
does rest of the work

00:13:49,040 --> 00:13:53,199
of retrieving the metrics

00:13:55,120 --> 00:14:01,040
so a kubernetes helm command is

00:13:58,320 --> 00:14:02,000
utilized here to deploy the prometheus

00:14:01,040 --> 00:14:04,240
operator

00:14:02,000 --> 00:14:06,240
which installs the operator cube state

00:14:04,240 --> 00:14:08,160
metrics and complementary reports

00:14:06,240 --> 00:14:09,680
required for effective matrix retrieval

00:14:08,160 --> 00:14:10,880
and alert manager

00:14:09,680 --> 00:14:13,519
in this case you can see on the right

00:14:10,880 --> 00:14:15,279
hand side we have this five or six

00:14:13,519 --> 00:14:17,360
ports running and the respective is

00:14:15,279 --> 00:14:20,240
running which will do the

00:14:17,360 --> 00:14:20,240
division do the work

00:14:23,600 --> 00:14:27,440
here we can see in the targets

00:14:25,680 --> 00:14:29,279
prometheus fetching and displaying

00:14:27,440 --> 00:14:30,800
displaying api endpoints and

00:14:29,279 --> 00:14:32,959
corresponding information

00:14:30,800 --> 00:14:33,839
from all community service specific

00:14:32,959 --> 00:14:37,279
services

00:14:33,839 --> 00:14:39,760
like control manager dns scheduler api

00:14:37,279 --> 00:14:39,760
etc

00:14:41,760 --> 00:14:44,800
the operator also installs an alert

00:14:43,920 --> 00:14:47,600
manager

00:14:44,800 --> 00:14:49,360
which is complemented by prometheus

00:14:47,600 --> 00:14:52,240
stating errors and warnings

00:14:49,360 --> 00:14:54,560
sorted by recency for a user to look at

00:14:52,240 --> 00:14:54,959
you can filter on those alerts and group

00:14:54,560 --> 00:14:58,079
them

00:14:54,959 --> 00:14:59,839
on a criteria for a better visualization

00:14:58,079 --> 00:15:01,279
this is a decent tool to get started

00:14:59,839 --> 00:15:05,199
with alert mechanism

00:15:01,279 --> 00:15:05,199
and it has a potential to deliver more

00:15:08,240 --> 00:15:12,079
so we know prometheus handles solar

00:15:10,639 --> 00:15:15,440
matrix quite well

00:15:12,079 --> 00:15:17,680
uh solar health chart discussed before

00:15:15,440 --> 00:15:18,720
has an option of integration with

00:15:17,680 --> 00:15:21,600
prometheus

00:15:18,720 --> 00:15:23,519
via prometheus exporter which is part of

00:15:21,600 --> 00:15:26,320
the solar docker image

00:15:23,519 --> 00:15:27,360
all we need to do is to create a solar

00:15:26,320 --> 00:15:30,399
exporter yaml

00:15:27,360 --> 00:15:33,040
with parameter exporter enable true

00:15:30,399 --> 00:15:34,720
annotations in the yammer for prometheus

00:15:33,040 --> 00:15:35,120
to know which port it needs to connect

00:15:34,720 --> 00:15:38,240
to

00:15:35,120 --> 00:15:40,160
for scraping the matrix

00:15:38,240 --> 00:15:42,639
it follows with updating the solar

00:15:40,160 --> 00:15:45,040
deployment yaml chart with this file

00:15:42,639 --> 00:15:46,800
reapplying the same via cube ctl command

00:15:45,040 --> 00:15:48,480
to have the changes in effect at the

00:15:46,800 --> 00:15:50,880
kubernetes cluster

00:15:48,480 --> 00:15:51,680
we can see solar exporter service and

00:15:50,880 --> 00:15:55,040
deployment

00:15:51,680 --> 00:15:56,880
is created and we can verify solar

00:15:55,040 --> 00:15:58,639
exporter pod or pods running in the

00:15:56,880 --> 00:16:01,040
humidity cluster with the cube ctl

00:15:58,639 --> 00:16:01,040
command

00:16:03,519 --> 00:16:09,360
so prometheus supports all solar metrics

00:16:06,639 --> 00:16:10,639
including stated num docs in a time

00:16:09,360 --> 00:16:13,199
series format

00:16:10,639 --> 00:16:15,279
with plenty of metrics at disposal we

00:16:13,199 --> 00:16:16,639
need effective dashboard tool which can

00:16:15,279 --> 00:16:20,800
complement prometheus

00:16:16,639 --> 00:16:20,800
and we do know what we need here we need

00:16:20,839 --> 00:16:26,320
grafana

00:16:22,959 --> 00:16:27,680
so if we have not installed prometheus

00:16:26,320 --> 00:16:30,480
operator

00:16:27,680 --> 00:16:33,040
grafana can be installed by a hem chart

00:16:30,480 --> 00:16:36,000
available on the stated link

00:16:33,040 --> 00:16:37,199
once deployed it needs to be pointed to

00:16:36,000 --> 00:16:39,920
the prometheus

00:16:37,199 --> 00:16:40,880
as one of its data sources the

00:16:39,920 --> 00:16:43,680
complementary

00:16:40,880 --> 00:16:44,880
uh j solar dashboard json for a graphana

00:16:43,680 --> 00:16:46,800
is open sourced

00:16:44,880 --> 00:16:48,240
and available on the content folder on

00:16:46,800 --> 00:16:51,839
the solid application

00:16:48,240 --> 00:16:51,839
this is the exact link to the json

00:16:55,120 --> 00:16:59,519
so this is the grafana dashboard for

00:16:57,360 --> 00:17:02,560
kubernetes cluster metrics

00:16:59,519 --> 00:17:05,120
showcasing cpu usage memory utilization

00:17:02,560 --> 00:17:07,199
segregated by request and limits

00:17:05,120 --> 00:17:08,799
the resource usage are available for

00:17:07,199 --> 00:17:10,400
each service and each namespace

00:17:08,799 --> 00:17:13,120
specified

00:17:10,400 --> 00:17:15,360
you can nail down to the exact node

00:17:13,120 --> 00:17:18,079
number of pods running in that node

00:17:15,360 --> 00:17:18,640
resource requested and further can level

00:17:18,079 --> 00:17:21,039
down

00:17:18,640 --> 00:17:22,240
to the exact docker container running in

00:17:21,039 --> 00:17:24,480
your kubernetes cluster

00:17:22,240 --> 00:17:25,760
and the resource management prometheus

00:17:24,480 --> 00:17:28,240
operator integration

00:17:25,760 --> 00:17:29,039
provides five to six inbuilt dashboards

00:17:28,240 --> 00:17:30,840
in grafana

00:17:29,039 --> 00:17:33,840
with respect to kubernetes out of the

00:17:30,840 --> 00:17:33,840
box

00:17:34,080 --> 00:17:37,360
and then we have the grafana dashboard

00:17:36,400 --> 00:17:39,360
for solar

00:17:37,360 --> 00:17:40,640
stating the heap sizes overall memory

00:17:39,360 --> 00:17:43,120
consumption

00:17:40,640 --> 00:17:44,400
resource usage at the threat level

00:17:43,120 --> 00:17:47,440
request and response

00:17:44,400 --> 00:17:50,000
time series dashboard latency error rate

00:17:47,440 --> 00:17:50,000
etc

00:17:52,080 --> 00:17:55,600
now let's look at now we're done with

00:17:54,880 --> 00:17:58,240
discussing

00:17:55,600 --> 00:17:59,600
the matrix part of the monitoring of the

00:17:58,240 --> 00:18:01,760
matrix

00:17:59,600 --> 00:18:03,039
using open source tools now let's look

00:18:01,760 --> 00:18:04,960
at what is available

00:18:03,039 --> 00:18:06,400
when we do log analytics in the

00:18:04,960 --> 00:18:09,280
kubernetes world

00:18:06,400 --> 00:18:10,080
so fluently is an open source data

00:18:09,280 --> 00:18:13,600
collector

00:18:10,080 --> 00:18:15,679
for unified logging layer now deploying

00:18:13,600 --> 00:18:18,320
fluency agents

00:18:15,679 --> 00:18:20,000
is done with help tool the chart

00:18:18,320 --> 00:18:22,559
available on the city link

00:18:20,000 --> 00:18:24,640
the a demon set in kubernetes make sure

00:18:22,559 --> 00:18:25,679
that a pod runs on each node of the

00:18:24,640 --> 00:18:27,760
cluster

00:18:25,679 --> 00:18:30,080
the pods of the fluency are deployed via

00:18:27,760 --> 00:18:30,960
daemon set such that it can scrape

00:18:30,080 --> 00:18:33,120
through

00:18:30,960 --> 00:18:34,559
pod container logs on the respective

00:18:33,120 --> 00:18:36,640
node

00:18:34,559 --> 00:18:38,720
so kubernetes provide out of the box

00:18:36,640 --> 00:18:41,039
support for log management

00:18:38,720 --> 00:18:44,320
and respective end points for strike

00:18:41,039 --> 00:18:46,960
driver which is a deep monitoring tool

00:18:44,320 --> 00:18:49,440
if clusters are hosted on a google cloud

00:18:46,960 --> 00:18:52,240
and elasticsearch

00:18:49,440 --> 00:18:53,520
behind the scenes for the endpoint

00:18:52,240 --> 00:18:55,679
fluently logging

00:18:53,520 --> 00:18:57,919
agent takes care of the log collection

00:18:55,679 --> 00:18:59,760
passing and the distribution

00:18:57,919 --> 00:19:02,080
in the in this talk we are going to

00:18:59,760 --> 00:19:05,200
discuss the efk stack all together

00:19:02,080 --> 00:19:07,840
as it is the truly open source analytics

00:19:05,200 --> 00:19:07,840
tool available

00:19:08,880 --> 00:19:14,880
starting with fluency so this is

00:19:11,919 --> 00:19:16,160
a typical yaml configurations to set to

00:19:14,880 --> 00:19:19,120
deployment full nd

00:19:16,160 --> 00:19:19,520
daemon pos now the source of the logs

00:19:19,120 --> 00:19:22,720
are

00:19:19,520 --> 00:19:25,679
set to var log containers which has

00:19:22,720 --> 00:19:26,799
logs for all containers running in that

00:19:25,679 --> 00:19:30,240
node

00:19:26,799 --> 00:19:33,679
and we tag all those log lines

00:19:30,240 --> 00:19:36,160
log files with raw.kubernetes.star

00:19:33,679 --> 00:19:38,240
there are more elements to be set and

00:19:36,160 --> 00:19:40,160
fluency configurations details them

00:19:38,240 --> 00:19:42,559
properly

00:19:40,160 --> 00:19:43,760
continuing the same xml formatted

00:19:42,559 --> 00:19:47,360
configuration

00:19:43,760 --> 00:19:50,320
uh filter the logs collected here we are

00:19:47,360 --> 00:19:52,480
filtering logs with container name solar

00:19:50,320 --> 00:19:55,600
and cumulative zookeeper

00:19:52,480 --> 00:19:58,640
so logs of these two apps are collected

00:19:55,600 --> 00:19:59,360
and rest are discarded further adding a

00:19:58,640 --> 00:20:02,960
label

00:19:59,360 --> 00:20:06,000
customer extracted form existing ones

00:20:02,960 --> 00:20:07,919
now this is an example and folks using

00:20:06,000 --> 00:20:09,120
fluently logstash are much familiar with

00:20:07,919 --> 00:20:12,240
it

00:20:09,120 --> 00:20:15,120
further we transform our log files

00:20:12,240 --> 00:20:16,880
based on our solar mdcs with labels that

00:20:15,120 --> 00:20:20,640
is we extract core names

00:20:16,880 --> 00:20:23,679
shard number handler queue time hits etc

00:20:20,640 --> 00:20:25,200
and finally as you can see we can set a

00:20:23,679 --> 00:20:28,000
dynamic elastic host

00:20:25,200 --> 00:20:30,960
elasticsearch hostname with labels

00:20:28,000 --> 00:20:33,280
available at the output layer

00:20:30,960 --> 00:20:34,960
the solar mdc part i i have only

00:20:33,280 --> 00:20:37,039
highlighted a single

00:20:34,960 --> 00:20:38,400
format but there are two three or more

00:20:37,039 --> 00:20:40,240
formats

00:20:38,400 --> 00:20:41,440
being available to you you can put it in

00:20:40,240 --> 00:20:43,919
as part of the solar

00:20:41,440 --> 00:20:43,919
mdc

00:20:47,039 --> 00:20:53,679
so following uh tools

00:20:50,240 --> 00:20:55,840
um apart from the fluent me are very

00:20:53,679 --> 00:20:57,600
straightforward to be configured uh we

00:20:55,840 --> 00:20:59,600
all are feminine with elasticsearch

00:20:57,600 --> 00:21:00,720
uh the hem chart is available on the

00:20:59,600 --> 00:21:02,799
link listed

00:21:00,720 --> 00:21:04,000
and pods are deployed by a state food

00:21:02,799 --> 00:21:06,159
set

00:21:04,000 --> 00:21:07,919
similar reasons like solar to have the

00:21:06,159 --> 00:21:08,960
same volumes getting attached to the

00:21:07,919 --> 00:21:12,960
pods

00:21:08,960 --> 00:21:16,000
while it gets restarted once deployed

00:21:12,960 --> 00:21:18,720
the elastic search service is

00:21:16,000 --> 00:21:19,840
available at the elasticsearch master

00:21:18,720 --> 00:21:22,159
dot name space

00:21:19,840 --> 00:21:22,880
and the rest of the nomenclature right

00:21:22,159 --> 00:21:25,039
typically

00:21:22,880 --> 00:21:27,039
we should host more than one single

00:21:25,039 --> 00:21:28,159
elastic part for the replication of log

00:21:27,039 --> 00:21:29,919
indexes

00:21:28,159 --> 00:21:32,240
minimal gamma configuration is required

00:21:29,919 --> 00:21:34,400
in this case just the cluster name as

00:21:32,240 --> 00:21:37,520
elasticsearch nothing much typically

00:21:34,400 --> 00:21:37,520
required apart from that

00:21:40,000 --> 00:21:45,120
there we are executing elasticsearch

00:21:43,120 --> 00:21:46,159
query to get all docs in the logstash

00:21:45,120 --> 00:21:49,039
index

00:21:46,159 --> 00:21:50,080
once fluency is configured properly we

00:21:49,039 --> 00:21:52,880
should start seeing

00:21:50,080 --> 00:21:54,880
log lines captured with definite labels

00:21:52,880 --> 00:21:57,679
recalling the solar mtc logging we

00:21:54,880 --> 00:21:59,120
enabled add fluency we will get core

00:21:57,679 --> 00:22:02,080
status queue time

00:21:59,120 --> 00:22:03,360
etc while on the right hand side what's

00:22:02,080 --> 00:22:05,679
not captured by mdc

00:22:03,360 --> 00:22:06,400
is printed as it is it is still pretty

00:22:05,679 --> 00:22:10,640
valuable

00:22:06,400 --> 00:22:10,640
once you have kibana visualizing all of

00:22:10,840 --> 00:22:16,960
this

00:22:13,360 --> 00:22:18,880
and the final piece we have is kibana

00:22:16,960 --> 00:22:20,159
right kibana is a free and open source

00:22:18,880 --> 00:22:22,960
user interface

00:22:20,159 --> 00:22:24,880
uh that lets you visualize your

00:22:22,960 --> 00:22:25,600
elasticsearch data and navigate the

00:22:24,880 --> 00:22:28,159
elastic

00:22:25,600 --> 00:22:29,120
stack the helm chart is available at the

00:22:28,159 --> 00:22:30,799
city link

00:22:29,120 --> 00:22:32,320
the kibana pods are defined by a

00:22:30,799 --> 00:22:34,000
deployment set

00:22:32,320 --> 00:22:35,919
such that if one parts gets killed

00:22:34,000 --> 00:22:36,559
another one can get up and just start

00:22:35,919 --> 00:22:39,600
serving

00:22:36,559 --> 00:22:42,080
the end customer or end user

00:22:39,600 --> 00:22:43,919
the only configurations needed to

00:22:42,080 --> 00:22:47,039
connect kibana to elastic

00:22:43,919 --> 00:22:48,720
is the elastic service name in this case

00:22:47,039 --> 00:22:52,159
based on what we have discussed

00:22:48,720 --> 00:22:55,039
so far it will be elastic search master

00:22:52,159 --> 00:22:57,840
caller 9200 9200 is the port for the

00:22:55,039 --> 00:22:57,840
elastic service

00:23:00,559 --> 00:23:06,640
and there you have the kibana ui with

00:23:03,919 --> 00:23:07,120
all the label keys from solar mdc and

00:23:06,640 --> 00:23:09,840
the

00:23:07,120 --> 00:23:11,440
raw component is on left navigation and

00:23:09,840 --> 00:23:13,760
the log snippets in a

00:23:11,440 --> 00:23:17,440
series format in the main panel we'll

00:23:13,760 --> 00:23:17,440
see an example of all of this actually

00:23:20,080 --> 00:23:26,400
so now let's look at what we

00:23:23,200 --> 00:23:27,200
need uh what and when we need to be

00:23:26,400 --> 00:23:30,320
alerted

00:23:27,200 --> 00:23:32,080
while running solar on kubernetes we

00:23:30,320 --> 00:23:32,720
will start with the typical solar

00:23:32,080 --> 00:23:36,640
specific

00:23:32,720 --> 00:23:39,600
components now if you if your solar

00:23:36,640 --> 00:23:40,799
jvm nodes are experiencing constant

00:23:39,600 --> 00:23:44,080
garbage collection

00:23:40,799 --> 00:23:45,760
or high heap usage uh multiple factors

00:23:44,080 --> 00:23:47,520
can be responsible for it

00:23:45,760 --> 00:23:49,520
you either may be faceting on a knock

00:23:47,520 --> 00:23:51,440
dock valued field or maybe doing

00:23:49,520 --> 00:23:53,520
irrational wild card searches that is

00:23:51,440 --> 00:23:56,960
without the edge and gram

00:23:53,520 --> 00:24:00,320
filter factory you can see very high uh

00:23:56,960 --> 00:24:02,159
memory consumption uh heat consumption

00:24:00,320 --> 00:24:04,400
also you need to keep an eye on your

00:24:02,159 --> 00:24:06,880
latency and your error rates

00:24:04,400 --> 00:24:08,880
for a solar and zookeeper metrics of

00:24:06,880 --> 00:24:11,039
these are the fundamentals of

00:24:08,880 --> 00:24:12,880
how your application is truly behaving

00:24:11,039 --> 00:24:14,720
if you see a very irrational

00:24:12,880 --> 00:24:17,679
pattern in your latency or error rate

00:24:14,720 --> 00:24:17,679
you need to be informed

00:24:17,760 --> 00:24:22,000
now from kubernetes perspective we need

00:24:20,480 --> 00:24:24,880
to monitor

00:24:22,000 --> 00:24:26,559
uh cpu storage memory utilization at

00:24:24,880 --> 00:24:28,559
part level is must

00:24:26,559 --> 00:24:30,799
both when it is under utilized or even

00:24:28,559 --> 00:24:33,200
when you are overwhelming it

00:24:30,799 --> 00:24:35,120
ah we should have a continuous heartbeat

00:24:33,200 --> 00:24:38,000
to solar and the zookeeper services

00:24:35,120 --> 00:24:39,279
whether they're accessible or not um the

00:24:38,000 --> 00:24:42,400
reason being

00:24:39,279 --> 00:24:42,880
kubernetes have a tendency to restart a

00:24:42,400 --> 00:24:44,960
pod

00:24:42,880 --> 00:24:46,080
or the other process correspondingly

00:24:44,960 --> 00:24:48,480
with that part

00:24:46,080 --> 00:24:50,400
which has recently failed to match the

00:24:48,480 --> 00:24:51,679
configurations provided at the time of

00:24:50,400 --> 00:24:53,679
deployment

00:24:51,679 --> 00:24:55,360
the failure for a solar part can be due

00:24:53,679 --> 00:24:58,960
to configuration mismanagement

00:24:55,360 --> 00:25:01,039
out of memory any other unforced error

00:24:58,960 --> 00:25:03,039
and if such frequent restarts are

00:25:01,039 --> 00:25:04,320
occurring for a solar pod or pods

00:25:03,039 --> 00:25:06,159
including zookeeper

00:25:04,320 --> 00:25:07,760
you need to get an alert that something

00:25:06,159 --> 00:25:10,400
is not right

00:25:07,760 --> 00:25:12,240
now in no shape or form this is an

00:25:10,400 --> 00:25:12,880
exhaustive list of pressure points of

00:25:12,240 --> 00:25:15,520
solar

00:25:12,880 --> 00:25:16,480
so please feel free to add more based on

00:25:15,520 --> 00:25:19,279
your conscious

00:25:16,480 --> 00:25:19,279
and assessment

00:25:20,640 --> 00:25:23,840
now let's move on to discuss one of the

00:25:22,480 --> 00:25:26,320
third party applications

00:25:23,840 --> 00:25:28,080
whose pricing model is based on the

00:25:26,320 --> 00:25:31,360
volume consumed or the traffic

00:25:28,080 --> 00:25:31,919
received now datadock is a sas based

00:25:31,360 --> 00:25:34,400
product

00:25:31,919 --> 00:25:35,360
platform to monitor applications that

00:25:34,400 --> 00:25:39,039
cloud sale

00:25:35,360 --> 00:25:41,120
servers databases and services it offers

00:25:39,039 --> 00:25:42,159
second drop dashboards we can receive

00:25:41,120 --> 00:25:44,400
alerts for

00:25:42,159 --> 00:25:46,080
critical issues which can be configured

00:25:44,400 --> 00:25:48,480
on various factors

00:25:46,080 --> 00:25:50,559
outlier anomaly detection it can

00:25:48,480 --> 00:25:53,200
forecast issues and much more

00:25:50,559 --> 00:25:55,039
uh datadog provides flexible api to

00:25:53,200 --> 00:25:57,679
access unique infrastructures

00:25:55,039 --> 00:25:59,120
multiple sql databases java processes

00:25:57,679 --> 00:26:01,360
user interface

00:25:59,120 --> 00:26:03,840
and much more on in their architecture

00:26:01,360 --> 00:26:06,240
the search capabilities on metrics and

00:26:03,840 --> 00:26:07,279
unique events in the entire cluster can

00:26:06,240 --> 00:26:09,120
be done

00:26:07,279 --> 00:26:11,360
and the best part with the data dog is

00:26:09,120 --> 00:26:14,159
its integration with

00:26:11,360 --> 00:26:14,480
almost all popular open source softwares

00:26:14,159 --> 00:26:16,320
and

00:26:14,480 --> 00:26:18,480
tools available and also with some

00:26:16,320 --> 00:26:20,880
enterprise applications too

00:26:18,480 --> 00:26:22,240
i am in no shape or form promoting a

00:26:20,880 --> 00:26:24,000
commercial application

00:26:22,240 --> 00:26:26,000
but we need to understand the features

00:26:24,000 --> 00:26:29,360
available to make a decision

00:26:26,000 --> 00:26:29,360
at the time of adoption

00:26:30,159 --> 00:26:34,880
so in datadog we can look at each part

00:26:32,880 --> 00:26:36,960
running in the kubernetes cluster

00:26:34,880 --> 00:26:38,320
monitor the cpu utilization memory

00:26:36,960 --> 00:26:39,760
utilizing real time

00:26:38,320 --> 00:26:41,679
in this case we have the respective

00:26:39,760 --> 00:26:42,960
solar pods here we have solar chain one

00:26:41,679 --> 00:26:46,240
solar chain zero

00:26:42,960 --> 00:26:48,720
of which um the respective uh

00:26:46,240 --> 00:26:50,840
cpu and memory consumption is being

00:26:48,720 --> 00:26:53,279
displayed

00:26:50,840 --> 00:26:54,000
so keeping up with the history we will

00:26:53,279 --> 00:26:55,520
have a demo

00:26:54,000 --> 00:26:57,600
it's always good to look at things

00:26:55,520 --> 00:26:59,840
working bear online

00:26:57,600 --> 00:26:59,840
so

00:27:00,640 --> 00:27:06,799
right so

00:27:04,559 --> 00:27:07,600
yeah so let's look at first that what we

00:27:06,799 --> 00:27:11,679
have set up

00:27:07,600 --> 00:27:14,000
so um we i've built this um

00:27:11,679 --> 00:27:15,200
a demo on on the receipt works manage

00:27:14,000 --> 00:27:17,440
search sas offering

00:27:15,200 --> 00:27:19,600
so you will see some urls with some

00:27:17,440 --> 00:27:23,039
typical urls please ignore that

00:27:19,600 --> 00:27:24,000
so we have this apache test and wiki

00:27:23,039 --> 00:27:25,440
collection

00:27:24,000 --> 00:27:27,440
two different collections which is

00:27:25,440 --> 00:27:30,080
hosted on our solar cloud

00:27:27,440 --> 00:27:31,520
um we have apache test with three shards

00:27:30,080 --> 00:27:33,520
two replicas each

00:27:31,520 --> 00:27:35,200
we have wiki collection with three

00:27:33,520 --> 00:27:37,600
shards one replica

00:27:35,200 --> 00:27:38,880
and so on right and if i can quickly go

00:27:37,600 --> 00:27:40,559
to the dashboard

00:27:38,880 --> 00:27:43,279
uh we are hosted with physical memory

00:27:40,559 --> 00:27:45,760
8gb jvm provided around 2.9

00:27:43,279 --> 00:27:47,279
and we have the respective other

00:27:45,760 --> 00:27:50,480
components

00:27:47,279 --> 00:27:52,880
so let's look at first our command line

00:27:50,480 --> 00:27:53,679
of how it looks like so this is my

00:27:52,880 --> 00:27:56,880
deployment

00:27:53,679 --> 00:27:57,919
right so if you can quickly come back to

00:27:56,880 --> 00:28:00,880
this this is our

00:27:57,919 --> 00:28:02,240
lucidbox manager sas offering ui hi here

00:28:00,880 --> 00:28:04,159
i have an apache on

00:28:02,240 --> 00:28:05,520
a cluster name called apachecon hosting

00:28:04,159 --> 00:28:07,360
three inputs with

00:28:05,520 --> 00:28:09,279
storage storage size two and we have two

00:28:07,360 --> 00:28:11,279
collections as discussed

00:28:09,279 --> 00:28:12,480
and if i look at the command line we

00:28:11,279 --> 00:28:15,120
have this

00:28:12,480 --> 00:28:16,799
k get parts i did this names deployed

00:28:15,120 --> 00:28:19,200
this particular name space

00:28:16,799 --> 00:28:21,600
so i have three solar pods running along

00:28:19,200 --> 00:28:24,159
with the solar exporter also running

00:28:21,600 --> 00:28:25,360
and i have the three zookeeper parts

00:28:24,159 --> 00:28:28,880
making those

00:28:25,360 --> 00:28:29,279
ensemble looking at the pvc i have my

00:28:28,880 --> 00:28:32,559
three

00:28:29,279 --> 00:28:34,000
solar uh parts running on ssds

00:28:32,559 --> 00:28:36,000
uh while the zoo cable is running on

00:28:34,000 --> 00:28:37,120
standard these needs to be obviously

00:28:36,000 --> 00:28:39,200
i'm like zookeeper parts should

00:28:37,120 --> 00:28:44,480
obviously run on ssds this is just for

00:28:39,200 --> 00:28:46,399
demo but

00:28:44,480 --> 00:28:47,600
the two services for solar one service

00:28:46,399 --> 00:28:49,520
for exporter

00:28:47,600 --> 00:28:51,679
and two services for zookeeper as

00:28:49,520 --> 00:28:52,159
mentioned so this is our solar setup

00:28:51,679 --> 00:28:55,919
right

00:28:52,159 --> 00:28:59,120
now let's look at the monitoring tools

00:28:55,919 --> 00:29:00,399
so we have deployed the data talk

00:28:59,120 --> 00:29:02,000
this is the commercial part that the

00:29:00,399 --> 00:29:04,240
commercial application part so we have

00:29:02,000 --> 00:29:06,880
deployed the data dog as a demon set

00:29:04,240 --> 00:29:08,080
so at the moment these many data talk

00:29:06,880 --> 00:29:11,279
pods are running

00:29:08,080 --> 00:29:13,120
so um on and each data dog is running on

00:29:11,279 --> 00:29:15,840
one single

00:29:13,120 --> 00:29:18,720
cluster once sorry one single node in

00:29:15,840 --> 00:29:20,720
the kubernetes cluster

00:29:18,720 --> 00:29:22,080
similarly in the similar lines we also

00:29:20,720 --> 00:29:23,840
have deployed fluency

00:29:22,080 --> 00:29:25,919
right this is the open source one so we

00:29:23,840 --> 00:29:26,799
have deployed fluency the elastic starts

00:29:25,919 --> 00:29:28,480
fluently

00:29:26,799 --> 00:29:31,679
again as a daemon set and it is also

00:29:28,480 --> 00:29:31,679
running on each node

00:29:36,240 --> 00:29:42,960
good elastic search cluster with

00:29:40,480 --> 00:29:44,000
and the prometheus server right on our

00:29:42,960 --> 00:29:46,080
multi services

00:29:44,000 --> 00:29:48,000
so these two are truly open source while

00:29:46,080 --> 00:29:50,159
the data dog one is the commercial one

00:29:48,000 --> 00:29:52,240
and then we will just look at get a feel

00:29:50,159 --> 00:29:55,600
of how exactly it looks like when we

00:29:52,240 --> 00:29:57,600
you know set this up so this is our

00:29:55,600 --> 00:30:01,120
prometheus dashboard i had to put

00:29:57,600 --> 00:30:03,279
forward because this is not exposed

00:30:01,120 --> 00:30:05,760
exposed as part of the sas offering so

00:30:03,279 --> 00:30:08,000
here i'm just

00:30:05,760 --> 00:30:09,840
configuring the solar matrix one minute

00:30:08,000 --> 00:30:12,320
rate registered one minute rate

00:30:09,840 --> 00:30:13,279
and i'll get um you know we'll get the

00:30:12,320 --> 00:30:15,520
metrics

00:30:13,279 --> 00:30:17,039
rightfully with values and so on this is

00:30:15,520 --> 00:30:19,440
not really visually

00:30:17,039 --> 00:30:20,320
intuitive so we look at the graphina

00:30:19,440 --> 00:30:21,919
dashboard

00:30:20,320 --> 00:30:23,440
so this is a custom davana dashboard

00:30:21,919 --> 00:30:24,240
this is not a default one which comes

00:30:23,440 --> 00:30:26,480
with solar we

00:30:24,240 --> 00:30:27,600
made some changes we tried to you know

00:30:26,480 --> 00:30:32,159
made it more

00:30:27,600 --> 00:30:35,200
um like user friendly and more user

00:30:32,159 --> 00:30:35,600
using user useful so in this case there

00:30:35,200 --> 00:30:37,039
is a

00:30:35,600 --> 00:30:38,640
intentionally i created two collections

00:30:37,039 --> 00:30:39,600
so we have two collections apache test

00:30:38,640 --> 00:30:41,840
and wiki

00:30:39,600 --> 00:30:43,520
in the wiki i have around 550 000

00:30:41,840 --> 00:30:46,240
documents the page is zero

00:30:43,520 --> 00:30:47,039
and you can see we can see index size

00:30:46,240 --> 00:30:50,559
overall

00:30:47,039 --> 00:30:53,679
update request uh select

00:30:50,559 --> 00:30:54,880
request latencies uh we have some t log

00:30:53,679 --> 00:30:56,720
replay counts to

00:30:54,880 --> 00:30:58,240
this for the replication factor since we

00:30:56,720 --> 00:31:00,000
have three types of nodes

00:30:58,240 --> 00:31:02,080
so we have categorized it like say core

00:31:00,000 --> 00:31:04,080
matrix then we have cache a matrix for

00:31:02,080 --> 00:31:05,200
hit ratios cumulative heat ratios people

00:31:04,080 --> 00:31:06,640
who are working with solar are very

00:31:05,200 --> 00:31:09,279
familiar with this matrix

00:31:06,640 --> 00:31:11,039
we have put them in one single panel

00:31:09,279 --> 00:31:13,039
then we have the jvm specific metrics

00:31:11,039 --> 00:31:16,240
like we mentioned the heap sizes the gc

00:31:13,039 --> 00:31:19,120
usage cpu load on each node

00:31:16,240 --> 00:31:20,720
you know defined and so on so on the top

00:31:19,120 --> 00:31:22,559
side you can see the labels and you can

00:31:20,720 --> 00:31:24,320
you know go for the specific collection

00:31:22,559 --> 00:31:25,039
you can just go for specific node and

00:31:24,320 --> 00:31:29,200
core

00:31:25,039 --> 00:31:30,000
and so on um this is our jbm metrics and

00:31:29,200 --> 00:31:32,240
have opera

00:31:30,000 --> 00:31:34,240
operating system specific ones with

00:31:32,240 --> 00:31:35,840
memory size cpu time

00:31:34,240 --> 00:31:37,679
and we have the error metrics as

00:31:35,840 --> 00:31:39,360
separate so that you can actually look

00:31:37,679 --> 00:31:40,559
at if there is an error spike you know

00:31:39,360 --> 00:31:42,320
we can go and

00:31:40,559 --> 00:31:44,840
just check out okay something's wrong is

00:31:42,320 --> 00:31:46,240
happening so this is with the matrix

00:31:44,840 --> 00:31:49,279
part uh

00:31:46,240 --> 00:31:50,960
in the logs scenario the logs one we

00:31:49,279 --> 00:31:52,000
have the kibana so we have already

00:31:50,960 --> 00:31:53,360
everything set up

00:31:52,000 --> 00:31:54,799
on the left hand side you can see we

00:31:53,360 --> 00:31:56,240
have the current date of the index which

00:31:54,799 --> 00:31:58,960
is 30th september

00:31:56,240 --> 00:32:00,240
right and i am configuring for last 15

00:31:58,960 --> 00:32:01,760
minutes it is getting refreshed

00:32:00,240 --> 00:32:03,919
automatically

00:32:01,760 --> 00:32:05,360
since our solar mbc was enabled by

00:32:03,919 --> 00:32:06,799
default on the left hand side you can

00:32:05,360 --> 00:32:08,559
see

00:32:06,799 --> 00:32:10,399
some very valuable labels are there so

00:32:08,559 --> 00:32:11,039
you can just go and click on a specific

00:32:10,399 --> 00:32:12,640
label

00:32:11,039 --> 00:32:14,559
and can figure out the log line from

00:32:12,640 --> 00:32:16,799
there so in this case in the app

00:32:14,559 --> 00:32:18,320
either i want to see is zookeeper or

00:32:16,799 --> 00:32:22,080
solar let's say i

00:32:18,320 --> 00:32:25,039
um like go for solar and then

00:32:22,080 --> 00:32:26,399
for the code type i have we have

00:32:25,039 --> 00:32:28,399
something called apache test

00:32:26,399 --> 00:32:30,399
core we have wiki shard one apache test

00:32:28,399 --> 00:32:32,799
again different course and so on

00:32:30,399 --> 00:32:34,080
here i just click on the uh on the app

00:32:32,799 --> 00:32:35,120
part so now i have just filtered

00:32:34,080 --> 00:32:37,279
everything by

00:32:35,120 --> 00:32:38,480
all the log lines coming from solar then

00:32:37,279 --> 00:32:41,200
we have node names

00:32:38,480 --> 00:32:41,679
and exit path the request handler right

00:32:41,200 --> 00:32:43,840
exact

00:32:41,679 --> 00:32:44,880
request handler was being called and how

00:32:43,840 --> 00:32:46,640
much time it took

00:32:44,880 --> 00:32:48,159
let us suppose i go for collections so

00:32:46,640 --> 00:32:49,840
admin collections

00:32:48,159 --> 00:32:52,799
and then i can look at the log lines and

00:32:49,840 --> 00:32:54,640
can do the basic diagnosis

00:32:52,799 --> 00:32:56,880
so these are the two things the metrics

00:32:54,640 --> 00:32:59,760
and the the logs part

00:32:56,880 --> 00:33:00,480
um we don't have a very good alert

00:32:59,760 --> 00:33:03,279
manager

00:33:00,480 --> 00:33:04,960
with we were not able to configure an

00:33:03,279 --> 00:33:07,600
edit manager with the for within

00:33:04,960 --> 00:33:09,760
open source tool we will discuss that in

00:33:07,600 --> 00:33:11,679
in the next slide

00:33:09,760 --> 00:33:13,279
further i would like to just go over the

00:33:11,679 --> 00:33:14,240
data dock thing um since this is a

00:33:13,279 --> 00:33:15,840
commercial product

00:33:14,240 --> 00:33:17,840
we won't be really going into what's

00:33:15,840 --> 00:33:19,440
happening but we have just configured

00:33:17,840 --> 00:33:22,080
some dashboards that this is a dashboard

00:33:19,440 --> 00:33:24,480
for the apache con

00:33:22,080 --> 00:33:25,279
for the apache con cluster three solar

00:33:24,480 --> 00:33:27,679
nodes is

00:33:25,279 --> 00:33:28,640
zookeeper nodes running if i look at one

00:33:27,679 --> 00:33:30,159
specific

00:33:28,640 --> 00:33:32,000
you know a search rate which is our

00:33:30,159 --> 00:33:33,679
search rate if i click on past four

00:33:32,000 --> 00:33:35,600
years it will start showing up for my

00:33:33,679 --> 00:33:37,039
past four sorry past four hours and so

00:33:35,600 --> 00:33:38,640
on um

00:33:37,039 --> 00:33:40,559
one interesting thing i just wanted to

00:33:38,640 --> 00:33:43,600
show just over upon this

00:33:40,559 --> 00:33:46,799
this is the host map infrastructure list

00:33:43,600 --> 00:33:48,559
containers processes you can just

00:33:46,799 --> 00:33:50,640
on a ping data dog and they can do the

00:33:48,559 --> 00:33:52,559
demo for you so i won't do the same

00:33:50,640 --> 00:33:54,720
uh one thing which we really find it

00:33:52,559 --> 00:33:56,399
useful was alert manager actually

00:33:54,720 --> 00:33:58,240
so in this case um this is just one

00:33:56,399 --> 00:33:59,039
single example one single alert manager

00:33:58,240 --> 00:34:00,960
example

00:33:59,039 --> 00:34:02,559
where we are configuring if this is

00:34:00,960 --> 00:34:05,679
running low

00:34:02,559 --> 00:34:07,279
on any of my kubernetes cluster on any

00:34:05,679 --> 00:34:10,079
of the solar nodes

00:34:07,279 --> 00:34:12,320
throw me an alert so it was this easy to

00:34:10,079 --> 00:34:14,399
unlike configure just one single monitor

00:34:12,320 --> 00:34:17,200
was enough to configure

00:34:14,399 --> 00:34:18,800
this alert for all kubernetes clusters

00:34:17,200 --> 00:34:20,159
and all the solar clusters running on

00:34:18,800 --> 00:34:23,119
this community cluster

00:34:20,159 --> 00:34:24,800
so that this made it pretty easy for us

00:34:23,119 --> 00:34:26,879
i hope this is useful

00:34:24,800 --> 00:34:28,720
and i'll now get back to our

00:34:26,879 --> 00:34:31,839
presentation i have just

00:34:28,720 --> 00:34:35,679
uh one slide left so

00:34:31,839 --> 00:34:40,320
quickly going to press interview

00:34:35,679 --> 00:34:42,560
right right so

00:34:40,320 --> 00:34:43,520
setting up these monitoring and log

00:34:42,560 --> 00:34:46,000
analytics tools

00:34:43,520 --> 00:34:46,639
are becoming very easy as the tools are

00:34:46,000 --> 00:34:49,200
adopting

00:34:46,639 --> 00:34:51,119
to the cube native space right the

00:34:49,200 --> 00:34:52,399
question and the respective thoughts i

00:34:51,119 --> 00:34:54,480
would like to conclude

00:34:52,399 --> 00:34:57,119
is whether to use open source tools

00:34:54,480 --> 00:34:58,640
available or by commercial services

00:34:57,119 --> 00:35:00,720
this is purely based on personal

00:34:58,640 --> 00:35:01,359
experience and there will surely be more

00:35:00,720 --> 00:35:04,640
factors

00:35:01,359 --> 00:35:07,040
based on yours so i

00:35:04,640 --> 00:35:08,160
particularly was not able to find a good

00:35:07,040 --> 00:35:10,880
alert manager

00:35:08,160 --> 00:35:11,680
open source which can notify any bad

00:35:10,880 --> 00:35:14,000
event

00:35:11,680 --> 00:35:15,599
effectively via commonly used

00:35:14,000 --> 00:35:17,119
notification channels

00:35:15,599 --> 00:35:19,359
so prometheus alert manager is pretty

00:35:17,119 --> 00:35:22,000
primitive one of the reasons

00:35:19,359 --> 00:35:22,880
lucid works adopted datadock for the

00:35:22,000 --> 00:35:25,760
managed

00:35:22,880 --> 00:35:27,040
solar sas offering for alert mechanism

00:35:25,760 --> 00:35:29,440
such that it pushes

00:35:27,040 --> 00:35:31,200
slack notifications it sends an email

00:35:29,440 --> 00:35:33,200
and it has pager duty call

00:35:31,200 --> 00:35:35,040
integration to a concerned operator

00:35:33,200 --> 00:35:39,520
within seconds if a bad

00:35:35,040 --> 00:35:42,000
event has happened and then

00:35:39,520 --> 00:35:42,960
do we have a resource do we have enough

00:35:42,000 --> 00:35:44,960
resources

00:35:42,960 --> 00:35:46,960
to even monitor these monitoring

00:35:44,960 --> 00:35:48,960
applications we just discussed

00:35:46,960 --> 00:35:50,000
now what happens if prometheus breaks

00:35:48,960 --> 00:35:52,560
down right

00:35:50,000 --> 00:35:54,720
something bad happens with elasticsearch

00:35:52,560 --> 00:35:58,240
unintentional security bug

00:35:54,720 --> 00:36:00,720
got introduced in atlanta now the more

00:35:58,240 --> 00:36:01,359
tech stack you add to your kubernetes

00:36:00,720 --> 00:36:04,079
you

00:36:01,359 --> 00:36:04,480
you add on kubernetes the quantitative

00:36:04,079 --> 00:36:06,800
job

00:36:04,480 --> 00:36:08,400
for maintenance also increases so the

00:36:06,800 --> 00:36:09,520
final question i will conclude the

00:36:08,400 --> 00:36:11,920
session will be

00:36:09,520 --> 00:36:14,000
how critical is monitoring solar for you

00:36:11,920 --> 00:36:15,680
for your business or your use case

00:36:14,000 --> 00:36:18,480
based on that i think you can decide

00:36:15,680 --> 00:36:21,520
whether you want to buy or want to build

00:36:18,480 --> 00:36:21,520
the monitoring stack

00:36:21,920 --> 00:36:26,000
so on that note i thank each and

00:36:23,839 --> 00:36:28,000
everyone here to be part of this session

00:36:26,000 --> 00:36:29,680
i hope this was useful and encourages

00:36:28,000 --> 00:36:31,680
you to go docker and communities

00:36:29,680 --> 00:36:33,760
with your solar i personally learned a

00:36:31,680 --> 00:36:34,560
lot these two days and looking forward

00:36:33,760 --> 00:36:36,800
to more

00:36:34,560 --> 00:36:37,760
i'll take questions and comments now if

00:36:36,800 --> 00:36:41,839
you guys have

00:36:37,760 --> 00:36:41,839
thank you

00:36:47,440 --> 00:36:54,079
okay right

00:36:50,560 --> 00:36:56,000
thank you yeah uh so the question from

00:36:54,079 --> 00:36:57,839
robert is will the health chart work

00:36:56,000 --> 00:36:59,839
with azure absolutely so the best thing

00:36:57,839 --> 00:37:04,320
about kubernetes is it works with

00:36:59,839 --> 00:37:06,320
every uh kind of i would say

00:37:04,320 --> 00:37:08,800
uh a cloud offering right you can deploy

00:37:06,320 --> 00:37:11,280
it on gcp kubernetes work with gcp

00:37:08,800 --> 00:37:12,800
you can build it on the aws with the eks

00:37:11,280 --> 00:37:15,040
on an azure with their

00:37:12,800 --> 00:37:16,079
unlike kubernetes of kubernetes platform

00:37:15,040 --> 00:37:18,480
but they have

00:37:16,079 --> 00:37:19,839
or even if you build kubernetes on-prem

00:37:18,480 --> 00:37:21,520
right communities is an open source

00:37:19,839 --> 00:37:23,359
orchestration tool so you can build it

00:37:21,520 --> 00:37:24,560
locally you can even run on mini cube on

00:37:23,359 --> 00:37:27,839
your local system

00:37:24,560 --> 00:37:31,119
and deploy all these tools

00:37:27,839 --> 00:37:31,119
and monitoring

00:37:35,520 --> 00:37:40,320
i'm pretty sure the recording will be

00:37:37,040 --> 00:37:43,760
available after i'm confident about that

00:37:40,320 --> 00:37:44,960
and yeah um

00:37:43,760 --> 00:37:47,760
yeah i'm confident also that the

00:37:44,960 --> 00:37:51,839
recording will be available

00:37:47,760 --> 00:37:51,839
and thank you joffrey thank you rajendra

00:37:52,240 --> 00:37:54,480
right

00:37:56,880 --> 00:38:04,079
i think i have another minute or so so

00:38:00,560 --> 00:38:05,440
i am available at sarkar on all social

00:38:04,079 --> 00:38:07,119
platforms twitter

00:38:05,440 --> 00:38:08,560
and instagram facebook where you will

00:38:07,119 --> 00:38:10,320
want to reach out to me

00:38:08,560 --> 00:38:12,079
um also we would like to talk if you are

00:38:10,320 --> 00:38:14,000
hosting solar on kubernetes

00:38:12,079 --> 00:38:15,680
maybe we can help you out in some manner

00:38:14,000 --> 00:38:19,839
we can collaborate on that

00:38:15,680 --> 00:38:19,839
so do let us know do reach out

00:38:22,560 --> 00:38:27,200
thank you chris thank you sophia and i

00:38:24,960 --> 00:38:28,720
think i have talked my 40 minutes anshum

00:38:27,200 --> 00:38:31,760
if i'm right

00:38:28,720 --> 00:38:43,280
and on that note i'll leave the session

00:38:31,760 --> 00:38:43,280

YouTube URL: https://www.youtube.com/watch?v=JEAhzgP2mAY


