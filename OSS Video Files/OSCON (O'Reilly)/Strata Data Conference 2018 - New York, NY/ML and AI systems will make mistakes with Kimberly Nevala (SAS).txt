Title: ML and AI systems will make mistakes with Kimberly Nevala (SAS)
Publication date: 2018-10-04
Playlist: Strata Data Conference 2018 - New York, NY
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:04,470
I'm Roger McGough is here at strata New

00:00:01,920 --> 00:00:06,210
York 2018 I'm here with Kimberly nevela

00:00:04,470 --> 00:00:10,740
who's the director of business strategy

00:00:06,210 --> 00:00:13,440
at SAS SAS welcome thank you and the

00:00:10,740 --> 00:00:15,870
first question is around risk and trust

00:00:13,440 --> 00:00:18,480
and AI clearly a big topic and I think

00:00:15,870 --> 00:00:21,119
an important topic you talked about the

00:00:18,480 --> 00:00:22,980
importance of businesses rationalizing

00:00:21,119 --> 00:00:25,590
risk and and trying to really make sense

00:00:22,980 --> 00:00:27,090
of what they're doing so that the trust

00:00:25,590 --> 00:00:29,160
is there for the algorithms that are

00:00:27,090 --> 00:00:31,800
developing what does this mean and why

00:00:29,160 --> 00:00:33,450
is it important well one of the things

00:00:31,800 --> 00:00:35,780
are one of my observations has been that

00:00:33,450 --> 00:00:40,950
a lot of the conversation that we have

00:00:35,780 --> 00:00:42,600
today about mitigating risk in AI tends

00:00:40,950 --> 00:00:44,730
to focus on how do we actually mitigate

00:00:42,600 --> 00:00:46,440
risk and eliminate risk within the

00:00:44,730 --> 00:00:49,170
context of the solution or the algorithm

00:00:46,440 --> 00:00:50,700
that we're developing itself so how do

00:00:49,170 --> 00:00:52,820
we do things like making sure that we're

00:00:50,700 --> 00:00:55,469
applying the appropriate level of

00:00:52,820 --> 00:00:57,809
scientific rigor right and discipline

00:00:55,469 --> 00:00:59,520
about how we actually develop these

00:00:57,809 --> 00:01:01,590
solutions and more specifically how do

00:00:59,520 --> 00:01:03,989
we train the algorithm and to grossly

00:01:01,590 --> 00:01:05,850
oversimplify how do we make sure we're

00:01:03,989 --> 00:01:08,520
applying the right techniques to the

00:01:05,850 --> 00:01:09,570
right problem with the right data how do

00:01:08,520 --> 00:01:11,430
we make sure that we're trying to

00:01:09,570 --> 00:01:15,390
account to the extent that we can for

00:01:11,430 --> 00:01:17,070
any implicit bias or deficiencies in the

00:01:15,390 --> 00:01:20,100
datasets how do we make sure that we're

00:01:17,070 --> 00:01:21,900
actually training and testing these

00:01:20,100 --> 00:01:24,060
systems appropriately using different

00:01:21,900 --> 00:01:27,119
datasets and so on and so forth and this

00:01:24,060 --> 00:01:29,040
is all extremely extremely important but

00:01:27,119 --> 00:01:31,110
one of the things I think that we also

00:01:29,040 --> 00:01:32,759
still need to recognize and have a much

00:01:31,110 --> 00:01:35,970
more deliberate discussion with our

00:01:32,759 --> 00:01:38,939
business counterparts is that the best

00:01:35,970 --> 00:01:41,340
data science the best aimo engineering

00:01:38,939 --> 00:01:43,320
team with the best algorithm with the

00:01:41,340 --> 00:01:45,479
best data that we have available to us

00:01:43,320 --> 00:01:49,229
today is still going to deliver a

00:01:45,479 --> 00:01:50,490
solution that makes mistakes it may make

00:01:49,229 --> 00:01:52,259
mistakes that are different than we

00:01:50,490 --> 00:01:56,430
would have made as a human and it may

00:01:52,259 --> 00:01:57,899
very well make them at at a frequencies

00:01:56,430 --> 00:02:00,570
that that's perhaps less than we would

00:01:57,899 --> 00:02:02,850
but it will make mistakes so we really I

00:02:00,570 --> 00:02:04,380
think need to be having the conversation

00:02:02,850 --> 00:02:06,420
with the business stakeholders about

00:02:04,380 --> 00:02:08,670
what does that really mean in terms of

00:02:06,420 --> 00:02:10,590
risk what are the real risks what are

00:02:08,670 --> 00:02:12,610
the real errors that could still occur

00:02:10,590 --> 00:02:15,010
and what's the implication of that

00:02:12,610 --> 00:02:16,660
what are some of the intangible risks

00:02:15,010 --> 00:02:18,490
that are inherent in that as a result of

00:02:16,660 --> 00:02:20,770
that how many people perceive the

00:02:18,490 --> 00:02:24,610
solution what might be those inherent

00:02:20,770 --> 00:02:26,770
biases that are there and what is our

00:02:24,610 --> 00:02:29,260
tolerance for risk as an organization

00:02:26,770 --> 00:02:32,070
what is our ability to react when a

00:02:29,260 --> 00:02:34,300
mistake happens and to actually

00:02:32,070 --> 00:02:36,550
course-correct appropriately when some

00:02:34,300 --> 00:02:40,630
of those material material errors occur

00:02:36,550 --> 00:02:42,610
because they will know that it sounds

00:02:40,630 --> 00:02:44,050
like a very human oriented approach to

00:02:42,610 --> 00:02:46,750
the problem and I think you're

00:02:44,050 --> 00:02:50,290
absolutely right about that becoming an

00:02:46,750 --> 00:02:54,310
important thing for the facilitation of

00:02:50,290 --> 00:02:57,430
AI so so how does you know making the

00:02:54,310 --> 00:03:00,670
risk clear help with that facilitation

00:02:57,430 --> 00:03:04,120
rather than scaring folks away so I

00:03:00,670 --> 00:03:06,040
think that a lot of times uncertainty in

00:03:04,120 --> 00:03:08,320
and of itself doesn't cause the distrust

00:03:06,040 --> 00:03:11,620
I like to say that uncertainty about

00:03:08,320 --> 00:03:13,660
your uncertainty causes distress and in

00:03:11,620 --> 00:03:15,820
that and given that we're currently at a

00:03:13,660 --> 00:03:18,220
data conference it occurs to me that

00:03:15,820 --> 00:03:21,040
this is the you know age-old data

00:03:18,220 --> 00:03:22,810
quality problem sort of writ large we

00:03:21,040 --> 00:03:24,610
used to have this conversation where

00:03:22,810 --> 00:03:26,049
someone would develop a report or an

00:03:24,610 --> 00:03:27,519
analytics model and they would say and I

00:03:26,049 --> 00:03:29,260
delivered it to the business and they

00:03:27,519 --> 00:03:30,940
won't use it even just a simple metric

00:03:29,260 --> 00:03:33,310
because they say the data is not good

00:03:30,940 --> 00:03:34,989
enough it's not perfect I said okay well

00:03:33,310 --> 00:03:36,370
how did how do what did you tell them

00:03:34,989 --> 00:03:38,260
about how good or bad it wasn't I said

00:03:36,370 --> 00:03:41,680
well we told him it was 98% accurate I

00:03:38,260 --> 00:03:44,590
said okay but that's not in a context

00:03:41,680 --> 00:03:46,750
that they can actually absorb so if you

00:03:44,590 --> 00:03:49,420
know for a very very simplified example

00:03:46,750 --> 00:03:50,799
if I look at I'm talking to someone in

00:03:49,420 --> 00:03:53,410
financial services and you've got a

00:03:50,799 --> 00:03:56,200
credit risk exposure of a million

00:03:53,410 --> 00:03:57,580
dollars plus or minus two hundred

00:03:56,200 --> 00:03:59,980
thousand or a billion dollars plus or

00:03:57,580 --> 00:04:01,930
minus two hundred million because the

00:03:59,980 --> 00:04:04,299
data is 98% accurate now I can actually

00:04:01,930 --> 00:04:06,430
make a decision about if and how to use

00:04:04,299 --> 00:04:08,769
that information and I think the same

00:04:06,430 --> 00:04:10,600
principle applies with in artificial

00:04:08,769 --> 00:04:13,030
intelligence and with these solutions if

00:04:10,600 --> 00:04:15,760
I understand what the scope of the risk

00:04:13,030 --> 00:04:18,190
or the errors maybe I can move beyond

00:04:15,760 --> 00:04:20,770
this discussion which is should I write

00:04:18,190 --> 00:04:24,310
if I should implement the solution to

00:04:20,770 --> 00:04:25,660
how can i implement the solution so now

00:04:24,310 --> 00:04:27,880
can have a discussion about what are the

00:04:25,660 --> 00:04:31,090
boundaries within which this solution

00:04:27,880 --> 00:04:33,820
can credibly make a recommendation or

00:04:31,090 --> 00:04:37,240
credibly take independent action what

00:04:33,820 --> 00:04:40,630
are the guardrails and the conditions in

00:04:37,240 --> 00:04:43,810
which we have to you know monitor and

00:04:40,630 --> 00:04:45,610
and and really take take heed and how do

00:04:43,810 --> 00:04:47,380
I actually account for new situations

00:04:45,610 --> 00:04:48,520
and conditions or unexpected errors

00:04:47,380 --> 00:04:51,520
because they're always going to happen

00:04:48,520 --> 00:04:53,110
all right so if we do that right we can

00:04:51,520 --> 00:04:54,760
now have a good conversation about how

00:04:53,110 --> 00:04:56,410
do we actually deploy the solution

00:04:54,760 --> 00:04:59,290
what's the engagement model in the role

00:04:56,410 --> 00:05:01,630
of the human and the solution when we

00:04:59,290 --> 00:05:02,800
actually deploy into production I think

00:05:01,630 --> 00:05:04,479
you're really hitting on some important

00:05:02,800 --> 00:05:07,870
points there but what else is holding

00:05:04,479 --> 00:05:10,510
back the adoption of AI these days well

00:05:07,870 --> 00:05:12,520
I think in a lot of ways I think

00:05:10,510 --> 00:05:15,340
artificial intelligence is strangely

00:05:12,520 --> 00:05:17,440
both overhyped and simultaneously

00:05:15,340 --> 00:05:18,130
undervalued which probably sounds a

00:05:17,440 --> 00:05:21,729
little odd

00:05:18,130 --> 00:05:23,830
except for we've got so much publicity

00:05:21,729 --> 00:05:26,139
about what these solutions might be

00:05:23,830 --> 00:05:27,639
actually able to do and in a lot of

00:05:26,139 --> 00:05:29,140
cases that's just overinflated right

00:05:27,639 --> 00:05:30,850
it's gonna cure all of society's ills

00:05:29,140 --> 00:05:33,700
it's going to allow us to make totally

00:05:30,850 --> 00:05:35,080
unbiased decisions and it's just and so

00:05:33,700 --> 00:05:36,610
that sometimes this sets us unrealistic

00:05:35,080 --> 00:05:38,110
expectation about what can actually be

00:05:36,610 --> 00:05:43,090
delivered and how easily it can be

00:05:38,110 --> 00:05:44,350
delivered on the other hand knowing that

00:05:43,090 --> 00:05:45,789
or people looking at these solutions and

00:05:44,350 --> 00:05:48,940
thinking wow they're really complicated

00:05:45,789 --> 00:05:50,200
and they're really hard are really

00:05:48,940 --> 00:05:52,000
hesitant to then start actually

00:05:50,200 --> 00:05:53,950
deploying and thinking about how to

00:05:52,000 --> 00:05:56,710
deploy the different technologies and

00:05:53,950 --> 00:05:58,960
solutions and ultimately artificial

00:05:56,710 --> 00:06:00,910
intelligence is a it's it's we see it as

00:05:58,960 --> 00:06:02,919
a continuing evolution towards these you

00:06:00,910 --> 00:06:04,570
know evermore advanced analytics and so

00:06:02,919 --> 00:06:06,280
things it's and obviously composed of

00:06:04,570 --> 00:06:07,930
things like machine learning a deep

00:06:06,280 --> 00:06:10,479
learning computer vision and natural

00:06:07,930 --> 00:06:12,130
language processing and becoming AI

00:06:10,479 --> 00:06:14,970
enabled it's it's a marathon and not a

00:06:12,130 --> 00:06:17,940
sprint I like to say so when we're too

00:06:14,970 --> 00:06:20,229
we're too concerned about sort of

00:06:17,940 --> 00:06:21,970
reinventing the business as it is or

00:06:20,229 --> 00:06:23,380
solving problems that are too big we're

00:06:21,970 --> 00:06:24,550
overlooking the opportunity to start

00:06:23,380 --> 00:06:26,860
driving value now

00:06:24,550 --> 00:06:29,259
and to develop these skill sets by

00:06:26,860 --> 00:06:30,699
starting to just apply components of

00:06:29,259 --> 00:06:32,349
artificial intelligence whether it's

00:06:30,699 --> 00:06:34,479
bringing in new types of data right

00:06:32,349 --> 00:06:36,250
language and text in documents or

00:06:34,479 --> 00:06:37,419
applying new techniques maybe it's a

00:06:36,250 --> 00:06:40,180
deep learning you know unsupervised

00:06:37,419 --> 00:06:41,349
learning or computer vision to help us

00:06:40,180 --> 00:06:43,210
do what we're doing today better and

00:06:41,349 --> 00:06:44,680
faster and that's really unfortunate

00:06:43,210 --> 00:06:46,419
because those are the opportunities

00:06:44,680 --> 00:06:48,520
where organizations can see value

00:06:46,419 --> 00:06:50,259
quickly and start to get some confidence

00:06:48,520 --> 00:06:52,780
in their ability to deploy these

00:06:50,259 --> 00:06:54,879
solutions long term so how is SAS

00:06:52,780 --> 00:06:57,159
helping their customers on their AI

00:06:54,879 --> 00:06:58,270
journey so I think in a couple of

00:06:57,159 --> 00:07:01,150
different ways

00:06:58,270 --> 00:07:03,580
first of all we are you know developing

00:07:01,150 --> 00:07:06,610
tools with things like our visual data

00:07:03,580 --> 00:07:10,449
mining and machine learning solutions

00:07:06,610 --> 00:07:12,699
that make very diverse algorithms and

00:07:10,449 --> 00:07:15,250
analytic capabilities available to folks

00:07:12,699 --> 00:07:19,090
to develop their own solutions we also

00:07:15,250 --> 00:07:20,680
recognize that the problem isn't just in

00:07:19,090 --> 00:07:22,300
development of the model but how do we

00:07:20,680 --> 00:07:25,479
actually deploy and maintain those at

00:07:22,300 --> 00:07:26,830
the scale so we've got our platform

00:07:25,479 --> 00:07:29,880
that's looking to support the end-to-end

00:07:26,830 --> 00:07:32,800
analytics cycle so everything from data

00:07:29,880 --> 00:07:34,630
validation and and and integration to

00:07:32,800 --> 00:07:37,060
model development which you can do in

00:07:34,630 --> 00:07:38,650
SAS or using your favorite open source

00:07:37,060 --> 00:07:41,229
tools and other things to being able to

00:07:38,650 --> 00:07:46,029
then deploy those models in production

00:07:41,229 --> 00:07:47,500
at scale and we're also embedding those

00:07:46,029 --> 00:07:49,240
own capabilities whether it's computer

00:07:47,500 --> 00:07:51,690
vision natural language processing into

00:07:49,240 --> 00:07:54,009
our own package solutions as well and

00:07:51,690 --> 00:07:55,479
you know maybe last but not least folks

00:07:54,009 --> 00:07:57,009
like myself are trying to come out and

00:07:55,479 --> 00:07:58,870
talk to folks about what is it really

00:07:57,009 --> 00:08:01,389
required from a business and a people

00:07:58,870 --> 00:08:04,479
and a process perspective to become AI

00:08:01,389 --> 00:08:07,090
enabled an AI capable because ultimately

00:08:04,479 --> 00:08:08,770
the technology is difficult but AI like

00:08:07,090 --> 00:08:10,210
their technologies before it it's not

00:08:08,770 --> 00:08:12,069
the technology that's that's the hard

00:08:10,210 --> 00:08:13,690
part is how do we adopt business

00:08:12,069 --> 00:08:15,699
practice and how do we actually develop

00:08:13,690 --> 00:08:17,259
the rigor and the discipline and the

00:08:15,699 --> 00:08:19,540
skills to be able to deploy and manage

00:08:17,259 --> 00:08:21,100
these solutions on an ongoing base

00:08:19,540 --> 00:08:24,090
that's great well thanks for your time

00:08:21,100 --> 00:08:24,090

YouTube URL: https://www.youtube.com/watch?v=E5Ru9i7JVPU


