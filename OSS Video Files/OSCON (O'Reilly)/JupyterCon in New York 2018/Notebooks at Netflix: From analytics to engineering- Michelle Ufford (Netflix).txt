Title: Notebooks at Netflix: From analytics to engineering- Michelle Ufford (Netflix)
Publication date: 2018-09-20
Playlist: JupyterCon in New York 2018
Description: 
	Notebooks have moved beyond a niche solution at Netflix; they are now the critical path for how everyone runs jobs against the companyâ€™s data platform. From creating original content to delivering bufferless streaming, Netflix relies on notebooks to inform decisions and fuel experiments across the company as well as to power its machine learning infrastructure and run over 150,000 jobs against its 100 PB cloud-based data warehouse every day. The goal is to deliver a compelling notebooks experience that simplifies end-to-end workflows for every type of user. To enable this, Netflix is investing deeply in notebook infrastructure and open source projects such as nteract.

Michelle Ufford shares some interesting ways Netflix uses data and some of the big bets the company is making on notebooks, covering architecture, kernels, UIs, and Netflixâ€™s open source collaborations with projects such as Jupyter, nteract, pandas, and Spark.

This session is sponsored by Netflix.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,990 --> 00:00:05,819
hi everybody I'm Michelle I lead the big

00:00:03,720 --> 00:00:08,340
data tools team at Netflix I work with

00:00:05,819 --> 00:00:11,129
Kyle and we work on notebooks and

00:00:08,340 --> 00:00:12,900
infrastructure I'm gonna start by

00:00:11,129 --> 00:00:15,209
sharing some of the things that we do

00:00:12,900 --> 00:00:20,699
with data and giving you a quick look at

00:00:15,209 --> 00:00:32,520
the company so we have as of q3 2018 130

00:00:20,699 --> 00:00:38,220
million members worldwide we are we are

00:00:32,520 --> 00:00:41,460
currently in 190 countries and we

00:00:38,220 --> 00:00:45,150
support over 4,500 different devices so

00:00:41,460 --> 00:00:49,500
last month we streamed to 450 million

00:00:45,150 --> 00:00:50,880
unique devices around the world so

00:00:49,500 --> 00:00:52,200
there's a reason why I told you those

00:00:50,880 --> 00:00:55,070
numbers we have a hundred and thirty

00:00:52,200 --> 00:00:59,130
million customers in 190 countries

00:00:55,070 --> 00:01:06,840
streaming to 450 million devices we have

00:00:59,130 --> 00:01:08,579
a lot of data we write 1 trillion events

00:01:06,840 --> 00:01:11,939
every single day to are streaming

00:01:08,579 --> 00:01:14,340
ingestion pipeline this data gets parsed

00:01:11,939 --> 00:01:16,289
and processed and put into a data

00:01:14,340 --> 00:01:20,700
warehouse that is currently sitting at

00:01:16,289 --> 00:01:23,909
100 petabytes and every single day we

00:01:20,700 --> 00:01:28,979
run a hundred and fifty thousand data

00:01:23,909 --> 00:01:32,490
jobs against that that data warehouse so

00:01:28,979 --> 00:01:34,590
we have a lot of data what are we doing

00:01:32,490 --> 00:01:36,390
with that data well we are using that

00:01:34,590 --> 00:01:38,880
data to give you guys a better product

00:01:36,390 --> 00:01:40,799
experience one of the things that we're

00:01:38,880 --> 00:01:42,569
trying to do is reduce the amount of

00:01:40,799 --> 00:01:44,880
bandwidth that people have to consume to

00:01:42,569 --> 00:01:47,039
watch Netflix this is helpful here but

00:01:44,880 --> 00:01:49,409
it's also especially helpful around the

00:01:47,039 --> 00:01:51,210
world so we introduced initially

00:01:49,409 --> 00:01:52,740
downloading and then recently we

00:01:51,210 --> 00:01:56,100
launched smart downloads for

00:01:52,740 --> 00:01:59,490
automatically remove the old videos and

00:01:56,100 --> 00:02:01,770
put the new videos out there for you we

00:01:59,490 --> 00:02:04,170
also use it to constantly improve the

00:02:01,770 --> 00:02:06,810
product so 21 years ago this is what

00:02:04,170 --> 00:02:08,459
Netflix looked like we have constantly

00:02:06,810 --> 00:02:11,390
been improving and iterating on the

00:02:08,459 --> 00:02:11,390
product ever since

00:02:16,870 --> 00:02:22,239
and you can't tell but those don't

00:02:20,739 --> 00:02:25,120
happen all in one fell swoop we are

00:02:22,239 --> 00:02:27,550
constantly testing additional iterations

00:02:25,120 --> 00:02:30,310
and additional features to the website

00:02:27,550 --> 00:02:31,900
every single day we are evaluating the

00:02:30,310 --> 00:02:33,989
performance of those and we are rolling

00:02:31,900 --> 00:02:37,390
them out based upon the actual

00:02:33,989 --> 00:02:38,980
experience that customers are having we

00:02:37,390 --> 00:02:43,299
are on track to spend eight billion

00:02:38,980 --> 00:02:45,340
dollars on content this year and we use

00:02:43,299 --> 00:02:47,140
our data to find the best possible

00:02:45,340 --> 00:02:50,799
content we can for people around the

00:02:47,140 --> 00:02:54,760
world so we do this by looking at the

00:02:50,799 --> 00:02:58,810
the estimated audience we have for a

00:02:54,760 --> 00:03:00,220
particular title and we use that to to

00:02:58,810 --> 00:03:01,810
make bids on it so if this is going to

00:03:00,220 --> 00:03:04,660
be new contour if it's content that's

00:03:01,810 --> 00:03:06,940
license we can look and say well this is

00:03:04,660 --> 00:03:09,549
content that serves the entire or like a

00:03:06,940 --> 00:03:10,959
large percentage of our member base so

00:03:09,549 --> 00:03:12,519
we can spend a lot of money on that

00:03:10,959 --> 00:03:16,690
content which is how we get deals like

00:03:12,519 --> 00:03:19,739
the ones with Marvel we can also use in

00:03:16,690 --> 00:03:21,790
that same model serve smaller niche

00:03:19,739 --> 00:03:25,840
customers or customers in other

00:03:21,790 --> 00:03:27,400
countries by backing into the content

00:03:25,840 --> 00:03:28,840
based upon the efficiency of it so we

00:03:27,400 --> 00:03:30,459
can spend less money on that content

00:03:28,840 --> 00:03:32,590
because it's going to serve less people

00:03:30,459 --> 00:03:34,780
but we still purchase that content so

00:03:32,590 --> 00:03:36,069
again we use the data trying to pick the

00:03:34,780 --> 00:03:39,940
right titles that are going to best

00:03:36,069 --> 00:03:43,900
serve you and we are increasingly moving

00:03:39,940 --> 00:03:45,280
into a studio studio engineering and

00:03:43,900 --> 00:03:46,510
seeing how we can innovate there and so

00:03:45,280 --> 00:03:48,010
we're using the data that we have

00:03:46,510 --> 00:03:50,769
because we're uniquely positioned to

00:03:48,010 --> 00:03:52,859
look at how people are viewing data are

00:03:50,769 --> 00:03:55,419
viewing the titles on the service and

00:03:52,859 --> 00:03:58,780
because we are also creating a lot of

00:03:55,419 --> 00:04:00,010
these titles we can we can try to

00:03:58,780 --> 00:04:01,870
innovate in the space and create a

00:04:00,010 --> 00:04:03,489
better viewing experience this is also

00:04:01,870 --> 00:04:05,829
where you see things like the branching

00:04:03,489 --> 00:04:07,329
narratives if you have heard about that

00:04:05,829 --> 00:04:08,380
where it's actually an interactive

00:04:07,329 --> 00:04:10,840
choose-your-own-adventure

00:04:08,380 --> 00:04:12,489
through the title so we have that for

00:04:10,840 --> 00:04:15,549
kids and we're doing more stuff with

00:04:12,489 --> 00:04:18,479
adult content coming and our plan right

00:04:15,549 --> 00:04:20,709
now is to spend 85% of that budget on

00:04:18,479 --> 00:04:23,070
original content so you're going to see

00:04:20,709 --> 00:04:25,229
a lot of more a lot more of this stuff

00:04:23,070 --> 00:04:27,280
so that's what we're doing with data

00:04:25,229 --> 00:04:28,620
that's what we're doing with well it's

00:04:27,280 --> 00:04:33,030
some of what we're doing with data

00:04:28,620 --> 00:04:35,610
how are we doing it though right well we

00:04:33,030 --> 00:04:37,949
have people all across the company that

00:04:35,610 --> 00:04:39,449
are looking at the data we have people

00:04:37,949 --> 00:04:42,300
that are looking at it for

00:04:39,449 --> 00:04:44,220
personalization and search we have about

00:04:42,300 --> 00:04:46,470
60 seconds to capture your attention

00:04:44,220 --> 00:04:47,850
whenever you come to the site and find a

00:04:46,470 --> 00:04:50,490
title that we think you're interested in

00:04:47,850 --> 00:04:53,160
we're also using it to personalize the

00:04:50,490 --> 00:04:55,080
images that we show you continents to do

00:04:53,160 --> 00:04:57,240
we're trying to improve that entire

00:04:55,080 --> 00:04:59,100
process to make it more efficient to

00:04:57,240 --> 00:05:01,590
make it easier to get the content out

00:04:59,100 --> 00:05:03,030
faster we are improving the business

00:05:01,590 --> 00:05:05,550
trying to make sure that we are being

00:05:03,030 --> 00:05:07,320
responsible and that we are being

00:05:05,550 --> 00:05:10,620
efficient with our cloud computing

00:05:07,320 --> 00:05:12,330
resources and growing the company and

00:05:10,620 --> 00:05:13,740
also just constantly improving that

00:05:12,330 --> 00:05:15,930
streaming experience so we want to test

00:05:13,740 --> 00:05:19,139
what is that optimal level of which we

00:05:15,930 --> 00:05:20,370
can deliver content to where we reduced

00:05:19,139 --> 00:05:23,220
the amount of bandwidth that you have to

00:05:20,370 --> 00:05:24,720
consume without compromising the

00:05:23,220 --> 00:05:25,800
experience you have so we have all these

00:05:24,720 --> 00:05:29,910
compression algorithms that were

00:05:25,800 --> 00:05:31,800
constantly testing and iterating on so

00:05:29,910 --> 00:05:33,870
all of those people that are working in

00:05:31,800 --> 00:05:36,270
all of this different areas are using

00:05:33,870 --> 00:05:39,300
the Netflix data platform what happens

00:05:36,270 --> 00:05:42,030
is when you use the service we are

00:05:39,300 --> 00:05:44,280
tracking your activity so as soon as you

00:05:42,030 --> 00:05:46,889
log in we're checking the authentication

00:05:44,280 --> 00:05:49,139
we're tracking what is the content that

00:05:46,889 --> 00:05:51,030
we are showing you because every single

00:05:49,139 --> 00:05:53,400
every single member and every single

00:05:51,030 --> 00:05:56,250
profile is completely 100% personalized

00:05:53,400 --> 00:05:57,330
for that person so we need to know what

00:05:56,250 --> 00:06:00,000
did we show you to be able to evaluate

00:05:57,330 --> 00:06:01,590
was it the right thing to do when you

00:06:00,000 --> 00:06:02,820
click Play when you could pause when you

00:06:01,590 --> 00:06:04,080
go to the next title all of those are

00:06:02,820 --> 00:06:05,520
events that are firing that's what

00:06:04,080 --> 00:06:06,710
that's where we get to that one train

00:06:05,520 --> 00:06:10,320
events a day

00:06:06,710 --> 00:06:11,970
so those trillion events get written

00:06:10,320 --> 00:06:13,620
into Keystone which is a streaming

00:06:11,970 --> 00:06:16,590
ingestion pipeline that is backed by

00:06:13,620 --> 00:06:19,139
kafka it gets written into s3 this

00:06:16,590 --> 00:06:20,849
entire platform actually everything at

00:06:19,139 --> 00:06:24,470
netflix all of our infrastructure is

00:06:20,849 --> 00:06:29,280
entirely 100% cloud-based based on AWS

00:06:24,470 --> 00:06:33,270
so our our our data warehouse is

00:06:29,280 --> 00:06:34,800
basically on s3 and we have a logical

00:06:33,270 --> 00:06:36,630
data warehouse so it follows the

00:06:34,800 --> 00:06:38,219
traditional three-tiered approach where

00:06:36,630 --> 00:06:40,530
we have a raw ingestion layer the data

00:06:38,219 --> 00:06:41,910
gets dumped we do some processing of

00:06:40,530 --> 00:06:43,290
that data we we lead

00:06:41,910 --> 00:06:45,180
into the data warehouse which is where

00:06:43,290 --> 00:06:46,710
we get that 100 petabytes and then we

00:06:45,180 --> 00:06:49,410
have aggregates that we build upon that

00:06:46,710 --> 00:06:52,830
and those aggregates are used to serve

00:06:49,410 --> 00:06:56,100
reports and data science the actual

00:06:52,830 --> 00:06:57,210
process of going from raw to to the data

00:06:56,100 --> 00:06:59,880
warehouse or going from the dinner

00:06:57,210 --> 00:07:01,940
warehouse to those aggregates we we use

00:06:59,880 --> 00:07:04,410
a variety of different tools but

00:07:01,940 --> 00:07:06,150
increasingly spark is the tool that

00:07:04,410 --> 00:07:08,130
we're using so we would use spark and

00:07:06,150 --> 00:07:10,500
presto to do that ETL or those

00:07:08,130 --> 00:07:12,480
transformations and a subset of that

00:07:10,500 --> 00:07:15,260
data gets loaded out to fast access

00:07:12,480 --> 00:07:21,080
storage so this is primarily for us

00:07:15,260 --> 00:07:23,610
druid increasingly snowflake I'm sorry

00:07:21,080 --> 00:07:25,410
redshift primarily increasingly

00:07:23,610 --> 00:07:31,440
snowflake and we also have elasticsearch

00:07:25,410 --> 00:07:35,550
and druid those fast expects fast access

00:07:31,440 --> 00:07:38,520
storage is used to serve a variety of

00:07:35,550 --> 00:07:40,260
data vis options tableau is used heavily

00:07:38,520 --> 00:07:43,470
across the entire company we also

00:07:40,260 --> 00:07:47,160
support d3 of course Jupiter abacus and

00:07:43,470 --> 00:07:49,350
some other internal custom vis and we

00:07:47,160 --> 00:07:51,740
support a variety of tools for

00:07:49,350 --> 00:07:54,030
interactive use or interactive

00:07:51,740 --> 00:07:56,850
exploration of the data in the data

00:07:54,030 --> 00:07:59,580
platform so we have a tool or really

00:07:56,850 --> 00:08:01,140
basically a landing page that's called

00:07:59,580 --> 00:08:02,160
the big data portal from which all of

00:08:01,140 --> 00:08:05,640
the tools and the platform are

00:08:02,160 --> 00:08:07,950
accessible especially notebooks we also

00:08:05,640 --> 00:08:09,990
can run presto queries or spark jobs

00:08:07,950 --> 00:08:11,160
directly from there we have a variety of

00:08:09,990 --> 00:08:15,570
different ways that people can access

00:08:11,160 --> 00:08:17,150
this this entire platform is accessible

00:08:15,570 --> 00:08:20,130
to every single person in the company

00:08:17,150 --> 00:08:22,170
who wants to have access to it so we

00:08:20,130 --> 00:08:23,760
believe that in order for us to do

00:08:22,170 --> 00:08:24,810
really great things with data we need to

00:08:23,760 --> 00:08:25,760
make sure that people can get to the

00:08:24,810 --> 00:08:28,950
data that they need

00:08:25,760 --> 00:08:30,690
obviously PII data is excluded from that

00:08:28,950 --> 00:08:32,190
but all of the non sensitive data is

00:08:30,690 --> 00:08:34,250
completely available for people to work

00:08:32,190 --> 00:08:34,250
with

00:08:35,570 --> 00:08:42,210
so within again kind of going back to

00:08:39,450 --> 00:08:45,540
those different areas or departments we

00:08:42,210 --> 00:08:46,650
had in the company we have different

00:08:45,540 --> 00:08:48,180
people working with the data doing

00:08:46,650 --> 00:08:49,530
different functions we have data vis

00:08:48,180 --> 00:08:52,260
engineers who are primarily focused on

00:08:49,530 --> 00:08:53,940
building tableau and other custom vis we

00:08:52,260 --> 00:08:55,680
have analytics engineers who are working

00:08:53,940 --> 00:08:57,480
on basically the full stack so then

00:08:55,680 --> 00:08:59,010
be doing aggregate sorta normalization

00:08:57,480 --> 00:09:01,200
as well as DataViz

00:08:59,010 --> 00:09:02,580
we have data engineers and data

00:09:01,200 --> 00:09:05,460
engineers for us is basically a software

00:09:02,580 --> 00:09:07,350
engineer who can work with that data

00:09:05,460 --> 00:09:09,779
who's specifically focused on data and

00:09:07,350 --> 00:09:12,209
they understand just distributed

00:09:09,779 --> 00:09:13,709
processing very well and they take care

00:09:12,209 --> 00:09:16,200
of the tree and Road data set to make it

00:09:13,709 --> 00:09:18,660
consumable for everybody else algorithm

00:09:16,200 --> 00:09:21,180
engineers who are constantly iterating

00:09:18,660 --> 00:09:23,550
on our personalization and compression

00:09:21,180 --> 00:09:25,529
algorithms and other algorithms they

00:09:23,550 --> 00:09:27,600
build what's called or what I'm calling

00:09:25,529 --> 00:09:31,110
data products right so the deliverable

00:09:27,600 --> 00:09:32,459
for them is data we also have a whole

00:09:31,110 --> 00:09:34,080
bunch of people that are focused on

00:09:32,459 --> 00:09:35,790
delivering insight from that data so

00:09:34,080 --> 00:09:36,839
their deliverable is an insight and that

00:09:35,790 --> 00:09:39,149
could be a research scientist

00:09:36,839 --> 00:09:41,399
quantitative analyst data scientist

00:09:39,149 --> 00:09:43,920
machine learning scientist this is

00:09:41,399 --> 00:09:45,089
imperfect I realize that people can fall

00:09:43,920 --> 00:09:46,709
into different categories but generally

00:09:45,089 --> 00:09:48,959
speaking this is this is how we divide

00:09:46,709 --> 00:09:50,850
them and then those insights are

00:09:48,959 --> 00:09:52,770
surfaced to other parts of the company

00:09:50,850 --> 00:10:02,070
for making business decisions and

00:09:52,770 --> 00:10:03,240
product improvements so every single

00:10:02,070 --> 00:10:05,130
person in the company has access to the

00:10:03,240 --> 00:10:06,480
data and when we go back to this and we

00:10:05,130 --> 00:10:10,500
look at the people in the data insights

00:10:06,480 --> 00:10:12,420
and the data product layer the the goal

00:10:10,500 --> 00:10:14,640
we have and we're not there yet but the

00:10:12,420 --> 00:10:16,680
goal we have is that any of those people

00:10:14,640 --> 00:10:19,290
should be able to go to our tools and

00:10:16,680 --> 00:10:21,680
answer our question like how many people

00:10:19,290 --> 00:10:23,790
viewed a specific title the previous day

00:10:21,680 --> 00:10:25,080
we want to make that as easy as possible

00:10:23,790 --> 00:10:27,839
for people to get to those types of

00:10:25,080 --> 00:10:30,029
answers and so everything is really

00:10:27,839 --> 00:10:34,430
accessible for everybody to do whatever

00:10:30,029 --> 00:10:37,910
it is that they want to do all right

00:10:34,430 --> 00:10:37,910
turn it over to Kyle

00:10:39,600 --> 00:10:50,670
see see does this one work three ball

00:10:41,790 --> 00:10:53,490
okay there we go okay all right so

00:10:50,670 --> 00:10:55,320
notebooks a Netflix the the way that we

00:10:53,490 --> 00:10:57,030
do the infrastructure right to enable

00:10:55,320 --> 00:10:59,370
all these people so you get access to it

00:10:57,030 --> 00:11:01,860
I mean the base layer of all of it is a

00:10:59,370 --> 00:11:04,100
docker container that has all the

00:11:01,860 --> 00:11:09,060
dependencies that that someone will need

00:11:04,100 --> 00:11:10,920
including all of our glue layer like

00:11:09,060 --> 00:11:14,100
Python libraries to access the data

00:11:10,920 --> 00:11:17,430
platform itself to issue jobs to make

00:11:14,100 --> 00:11:20,160
sequel queries presto Cory's give them

00:11:17,430 --> 00:11:22,350
access to two magics to directly call

00:11:20,160 --> 00:11:24,540
percent percent sequel and then whatever

00:11:22,350 --> 00:11:26,070
query they want to make and as much as

00:11:24,540 --> 00:11:28,110
possible that those tools need to be

00:11:26,070 --> 00:11:29,580
readily available right when they make

00:11:28,110 --> 00:11:30,540
that query they get back a data frame

00:11:29,580 --> 00:11:32,700
and then they should be able to

00:11:30,540 --> 00:11:34,680
visualize that data frame as fast as

00:11:32,700 --> 00:11:38,160
possible so this is this is all about

00:11:34,680 --> 00:11:40,140
enabling these users and so like when

00:11:38,160 --> 00:11:42,240
they run a spark job for instance like

00:11:40,140 --> 00:11:44,490
it's gonna run their their driver is

00:11:42,240 --> 00:11:47,040
running on their their container and

00:11:44,490 --> 00:11:51,330
then it's it's interacting with geni

00:11:47,040 --> 00:11:52,350
which is our escaping me right now but

00:11:51,330 --> 00:11:54,240
it's basically the way that will run

00:11:52,350 --> 00:11:56,310
kind of data jobs that's the easy way to

00:11:54,240 --> 00:11:57,630
put it so it'll get access to to what

00:11:56,310 --> 00:11:59,850
version of spark we're going to run and

00:11:57,630 --> 00:12:01,680
connect you to a cluster so that you can

00:11:59,850 --> 00:12:03,930
actually run the rest of your your spark

00:12:01,680 --> 00:12:06,810
job and it's reading and writing to s3

00:12:03,930 --> 00:12:08,070
like Michelle was saying before and then

00:12:06,810 --> 00:12:10,170
when you're writing your notebook

00:12:08,070 --> 00:12:12,210
currently we're writing it out directly

00:12:10,170 --> 00:12:16,950
to EFS which is a network file system

00:12:12,210 --> 00:12:20,430
from Amazon so this this is all all

00:12:16,950 --> 00:12:22,110
great except for as a data scientist if

00:12:20,430 --> 00:12:23,370
your your production Eliza ghen or you

00:12:22,110 --> 00:12:25,620
heard kind of anybody within that

00:12:23,370 --> 00:12:27,990
category you end up having to pull your

00:12:25,620 --> 00:12:30,000
code out of a notebook and then either

00:12:27,990 --> 00:12:33,240
have a data engineer help you with doing

00:12:30,000 --> 00:12:35,280
ETL or you're figuring out exactly how

00:12:33,240 --> 00:12:37,350
you're gonna get a job scheduled and how

00:12:35,280 --> 00:12:40,710
you're gonna get get that that exact

00:12:37,350 --> 00:12:42,720
notebook out run run as part of an ETL

00:12:40,710 --> 00:12:47,120
pipeline or maybe a model that you're

00:12:42,720 --> 00:12:47,120
building and so it's just like

00:12:48,400 --> 00:12:51,730
I'm gonna pause you right there hey guys

00:12:50,020 --> 00:12:55,990
we've got seats up here if you guys want

00:12:51,730 --> 00:12:59,800
to sit down there one two three seats

00:12:55,990 --> 00:13:04,000
now two four six eight okay yeah we had

00:12:59,800 --> 00:13:07,270
seats so so one thing that we we've

00:13:04,000 --> 00:13:10,420
enabled and we're pushing further on is

00:13:07,270 --> 00:13:13,180
that we make it easy to schedule a

00:13:10,420 --> 00:13:16,360
notebook as a job so we have a scheduler

00:13:13,180 --> 00:13:17,650
that's called mace on and on mace on you

00:13:16,360 --> 00:13:20,260
write a template of what you're gonna

00:13:17,650 --> 00:13:21,490
run and if and and the temp and the

00:13:20,260 --> 00:13:23,410
notebook that you're gonna run along

00:13:21,490 --> 00:13:25,300
with it and that notebook we actually

00:13:23,410 --> 00:13:27,790
allow it to be parameterised you can you

00:13:25,300 --> 00:13:29,680
can set a few cells equals three equals

00:13:27,790 --> 00:13:33,130
B equals two or whatever and then change

00:13:29,680 --> 00:13:35,680
those by by changing the the yamo file

00:13:33,130 --> 00:13:36,760
for your workflow basically so you can

00:13:35,680 --> 00:13:38,740
iterate across a bunch of different

00:13:36,760 --> 00:13:42,490
parameters and then that that comes to

00:13:38,740 --> 00:13:44,170
paper mill and paper mill itself

00:13:42,490 --> 00:13:46,240
actually just runs the notebook it's

00:13:44,170 --> 00:13:48,820
just a notebook runner that that knows

00:13:46,240 --> 00:13:50,110
how to handle parameterization I don't

00:13:48,820 --> 00:13:51,940
have to run the Miss parametrized

00:13:50,110 --> 00:13:53,980
notebooks some people use them to

00:13:51,940 --> 00:13:55,810
basically just generate reports so now

00:13:53,980 --> 00:13:57,940
they have this end-to-end workflow where

00:13:55,810 --> 00:14:01,810
they can where somebody can do ETL and

00:13:57,940 --> 00:14:03,520
then when they look at their job the

00:14:01,810 --> 00:14:05,230
next day effectively have a report right

00:14:03,520 --> 00:14:07,120
they can get the the table output from

00:14:05,230 --> 00:14:08,620
pandas they can do their own plots and

00:14:07,120 --> 00:14:10,420
they can evaluate how that job actually

00:14:08,620 --> 00:14:14,230
performed which is important because

00:14:10,420 --> 00:14:16,420
when the win if and win a job fails you

00:14:14,230 --> 00:14:18,640
can look back at this artifact which is

00:14:16,420 --> 00:14:20,800
this output notebook and know the

00:14:18,640 --> 00:14:22,390
reasons why it failed because in the

00:14:20,800 --> 00:14:24,430
past if people were running a script

00:14:22,390 --> 00:14:25,450
through this this whole engine here you

00:14:24,430 --> 00:14:27,490
basically just have this pile of

00:14:25,450 --> 00:14:29,620
standard error and standard out to look

00:14:27,490 --> 00:14:33,100
at and you're like okay which which

00:14:29,620 --> 00:14:34,630
thing broke and why and then the

00:14:33,100 --> 00:14:37,800
notebook gives you this nice artifact

00:14:34,630 --> 00:14:41,490
that that contains it all together

00:14:37,800 --> 00:14:41,490
yeah so everything's awesome

00:14:42,780 --> 00:14:46,380
I guess I just I just talked through

00:14:44,520 --> 00:14:49,410
this but I can talk so this this will go

00:14:46,380 --> 00:14:52,800
through so for paper mill itself I mean

00:14:49,410 --> 00:14:55,530
we when we get the the source notebook

00:14:52,800 --> 00:14:56,730
for everything that that it's gonna that

00:14:55,530 --> 00:14:58,680
it's gonna you know it's gonna pull from

00:14:56,730 --> 00:15:01,140
databases it's gonna pull from files or

00:14:58,680 --> 00:15:03,060
other services we can set parameters

00:15:01,140 --> 00:15:05,220
directly in the notebook you do note

00:15:03,060 --> 00:15:07,950
that a particular cell is a parameter

00:15:05,220 --> 00:15:10,110
cell and then run it and paper mill

00:15:07,950 --> 00:15:12,060
itself is is just working with the

00:15:10,110 --> 00:15:13,470
Jupiter messages directly to build a

00:15:12,060 --> 00:15:16,470
notebook in memory and it's actually

00:15:13,470 --> 00:15:17,970
using env convert under the hood and and

00:15:16,470 --> 00:15:20,730
while we're doing that we're also

00:15:17,970 --> 00:15:22,230
streaming the input and output that that

00:15:20,730 --> 00:15:24,300
happens during the course of running

00:15:22,230 --> 00:15:26,610
that so while the notebook is being

00:15:24,300 --> 00:15:30,450
written we can still see a live view of

00:15:26,610 --> 00:15:32,010
what's happening with that job just in

00:15:30,450 --> 00:15:33,150
case for some reason we weren't able to

00:15:32,010 --> 00:15:35,640
write the notebook we've still got

00:15:33,150 --> 00:15:38,490
something else and this is this is all

00:15:35,640 --> 00:15:39,960
for for like long term reliability you

00:15:38,490 --> 00:15:42,120
know we know that not everything's gonna

00:15:39,960 --> 00:15:42,870
work out so we always have to make sure

00:15:42,120 --> 00:15:44,820
to stream

00:15:42,870 --> 00:15:48,090
you know these same outputs and logs to

00:15:44,820 --> 00:15:50,520
another place and then when we write we

00:15:48,090 --> 00:15:52,200
write the output notebook out like it's

00:15:50,520 --> 00:15:53,730
not just writing out the notebook right

00:15:52,200 --> 00:15:56,220
you've you've probably written something

00:15:53,730 --> 00:15:57,810
that's that's writing out to hive and so

00:15:56,220 --> 00:15:59,700
you can think of the artifact as being

00:15:57,810 --> 00:16:01,740
you know your presentation layer over

00:15:59,700 --> 00:16:03,870
the real job that you ran but the real

00:16:01,740 --> 00:16:06,660
effect is that you you were up to a

00:16:03,870 --> 00:16:08,430
table and hive you may be stored some

00:16:06,660 --> 00:16:09,480
objects in s3 because you had a model

00:16:08,430 --> 00:16:11,580
that you built you know and you've

00:16:09,480 --> 00:16:16,230
serialized it and you put it up on s3

00:16:11,580 --> 00:16:17,970
for later use right so like in a tie

00:16:16,230 --> 00:16:20,490
rack the way that we do this is that we

00:16:17,970 --> 00:16:22,710
just have one quick thing to to toggle a

00:16:20,490 --> 00:16:24,330
cell as a parameter cell and then

00:16:22,710 --> 00:16:25,710
papermill knows how to pick that up and

00:16:24,330 --> 00:16:28,070
then you can see it visually when you

00:16:25,710 --> 00:16:28,070
get to it

00:16:31,370 --> 00:16:35,930
and then an important part of this after

00:16:34,280 --> 00:16:37,820
so you've gotten this output notebook

00:16:35,930 --> 00:16:39,020
you've written it out people are working

00:16:37,820 --> 00:16:40,460
with it you actually want to be able to

00:16:39,020 --> 00:16:42,530
share it so we own another service that

00:16:40,460 --> 00:16:44,920
looks on top of all the notebooks on s3

00:16:42,530 --> 00:16:47,360
as well as all the notebooks on EFS

00:16:44,920 --> 00:16:49,610
that's called commuter and it's just a

00:16:47,360 --> 00:16:51,220
viewer that uses the same react

00:16:49,610 --> 00:16:54,290
components from the interact front-end

00:16:51,220 --> 00:16:56,240
and then you know we just keep building

00:16:54,290 --> 00:17:00,380
on that to build this ideal interface

00:16:56,240 --> 00:17:02,600
for reporting within a within a recent

00:17:00,380 --> 00:17:05,030
internal report people were actually

00:17:02,600 --> 00:17:06,890
passing links to commuter you know worse

00:17:05,030 --> 00:17:08,540
normally they'd like have offloaded

00:17:06,890 --> 00:17:10,850
stuff to Google Docs and made their memo

00:17:08,540 --> 00:17:13,190
their memo and said was just a link to

00:17:10,850 --> 00:17:14,720
their notebook on the inside and you

00:17:13,190 --> 00:17:16,610
know they like hid the coat they hid the

00:17:14,720 --> 00:17:18,709
source code for the cells because they

00:17:16,610 --> 00:17:20,209
can toggle that easily to and then

00:17:18,709 --> 00:17:23,829
someone was able to just look at it like

00:17:20,209 --> 00:17:23,829
it was a document cuz that's what it is

00:17:24,940 --> 00:17:39,860
right that's what's next yeah ok so

00:17:37,150 --> 00:17:42,440
there is we've done a lot of work on

00:17:39,860 --> 00:17:43,910
notebooks but we're not done in fact I

00:17:42,440 --> 00:17:46,970
would say we're really just getting

00:17:43,910 --> 00:17:48,950
started one of the things that we want

00:17:46,970 --> 00:17:51,680
to improve is the entire scala

00:17:48,950 --> 00:17:54,770
experience for our Scala developers

00:17:51,680 --> 00:17:57,410
inside of a notebook they are using

00:17:54,770 --> 00:17:59,390
notebooks for a whole bunch of different

00:17:57,410 --> 00:18:02,090
things one example is image

00:17:59,390 --> 00:18:05,030
personalization and so we are constantly

00:18:02,090 --> 00:18:08,060
testing variations of images to see what

00:18:05,030 --> 00:18:11,600
is the right image to show we do this

00:18:08,060 --> 00:18:15,110
across the entire population currently

00:18:11,600 --> 00:18:20,720
and so what we saw was around I want to

00:18:15,110 --> 00:18:23,720
say was like a 2/3 23% lift and click

00:18:20,720 --> 00:18:25,190
throughs based upon like the image we

00:18:23,720 --> 00:18:26,780
are now working on personalizing that

00:18:25,190 --> 00:18:28,250
for every single person so when you look

00:18:26,780 --> 00:18:29,930
at something like stranger things it's

00:18:28,250 --> 00:18:32,360
an interesting case because it hits a

00:18:29,930 --> 00:18:34,240
lot of different genres it's hitting the

00:18:32,360 --> 00:18:37,809
80s nostalgia it's hitting

00:18:34,240 --> 00:18:40,780
horror and sci-fi and mystery and when

00:18:37,809 --> 00:18:42,730
we are showing you this image what we

00:18:40,780 --> 00:18:44,350
need to do is express to you why we

00:18:42,730 --> 00:18:46,480
think this abyss is interesting to you

00:18:44,350 --> 00:18:48,520
we want to express what's appealing

00:18:46,480 --> 00:18:50,050
about it for you and so we need to show

00:18:48,520 --> 00:18:52,780
you an image that really denotes that

00:18:50,050 --> 00:18:54,309
and so the algorithm engineering team is

00:18:52,780 --> 00:18:55,929
focused on this type of problem but

00:18:54,309 --> 00:18:59,320
because of the scale we have everything

00:18:55,929 --> 00:19:01,809
they do is pretty much in Scala so we

00:18:59,320 --> 00:19:03,780
want to improve the Scala experience for

00:19:01,809 --> 00:19:08,610
them a lot of them I would say it's

00:19:03,780 --> 00:19:11,050
mixed 50/50 maybe half we're doing

00:19:08,610 --> 00:19:12,880
Jupiter notebooks Heffer doing Zeppelin

00:19:11,050 --> 00:19:16,030
and so our goal right now is to reach a

00:19:12,880 --> 00:19:19,179
workflow parity with Zeppelin and other

00:19:16,030 --> 00:19:21,010
tough offerings so that's one goal how

00:19:19,179 --> 00:19:22,990
we're gonna do that is working on

00:19:21,010 --> 00:19:24,580
improving the Scala kernel to make sure

00:19:22,990 --> 00:19:26,440
that it's more stable and gives them a

00:19:24,580 --> 00:19:28,840
better experience we want to support

00:19:26,440 --> 00:19:31,809
native native DataViz which is where we

00:19:28,840 --> 00:19:33,190
got to data Explorer and again that's an

00:19:31,809 --> 00:19:35,290
area that we have something out there

00:19:33,190 --> 00:19:37,870
but we're going to constantly improve

00:19:35,290 --> 00:19:39,520
and then we want to have better spark

00:19:37,870 --> 00:19:41,740
integration so you have a pretty good

00:19:39,520 --> 00:19:44,590
spark experience right now with Python

00:19:41,740 --> 00:19:47,080
we want to give that same experience to

00:19:44,590 --> 00:19:52,090
all of our all of our developers here

00:19:47,080 --> 00:19:54,880
working with the JVM we want to have

00:19:52,090 --> 00:19:57,510
better integration and so integration

00:19:54,880 --> 00:20:00,250
with the data platform for our purposes

00:19:57,510 --> 00:20:03,130
we want to provide a single cohesive

00:20:00,250 --> 00:20:04,390
platform experience regardless of who

00:20:03,130 --> 00:20:06,280
you are and what you're doing

00:20:04,390 --> 00:20:08,530
so that means Native scheduling

00:20:06,280 --> 00:20:09,700
scheduling becomes just a button that

00:20:08,530 --> 00:20:12,429
you don't have to worry about going to

00:20:09,700 --> 00:20:13,929
find a yeah mole or a job you just you

00:20:12,429 --> 00:20:15,790
specify a schedule so we want to make

00:20:13,929 --> 00:20:18,070
that really easy for people that want to

00:20:15,790 --> 00:20:21,130
specify recurring workloads we want to

00:20:18,070 --> 00:20:23,440
surface all of the information that

00:20:21,130 --> 00:20:25,570
surrounds the actual notebook so we need

00:20:23,440 --> 00:20:28,090
to give our users better context we want

00:20:25,570 --> 00:20:32,280
to give them visibility into logs and

00:20:28,090 --> 00:20:32,280
error messages right there in app and

00:20:33,960 --> 00:20:39,090
what's the last one gonna be it just as

00:20:36,760 --> 00:20:39,090
native

00:20:42,550 --> 00:20:49,910
it's gonna be native whatever it is we

00:20:48,410 --> 00:20:52,640
also want to improve the right

00:20:49,910 --> 00:20:56,240
reliability of the entire experience

00:20:52,640 --> 00:20:58,610
right so increasingly people are using

00:20:56,240 --> 00:21:01,760
notebooks not for ad-hoc exploration or

00:20:58,610 --> 00:21:04,060
not only for ad-hoc exploration and data

00:21:01,760 --> 00:21:06,230
discovery but they are using it for

00:21:04,060 --> 00:21:08,540
mission-critical production work

00:21:06,230 --> 00:21:10,880
workloads and so we need to give them an

00:21:08,540 --> 00:21:13,370
ecosystem that supports that type of

00:21:10,880 --> 00:21:15,560
work so again we need to improve and

00:21:13,370 --> 00:21:17,330
invest in the Scala kernels and other

00:21:15,560 --> 00:21:18,830
kernels as well we want to give

00:21:17,330 --> 00:21:22,340
visibility into the state of those

00:21:18,830 --> 00:21:24,820
kernels and we want to have automated

00:21:22,340 --> 00:21:29,150
source control for our users that way

00:21:24,820 --> 00:21:30,500
they don't have to worry about whether

00:21:29,150 --> 00:21:32,960
people are editing they can always roll

00:21:30,500 --> 00:21:34,640
back it also enables a lot more around

00:21:32,960 --> 00:21:40,550
collaboration and being able to discover

00:21:34,640 --> 00:21:43,540
notebooks so going back to this slide

00:21:40,550 --> 00:21:46,190
and we looked at all of our users

00:21:43,540 --> 00:21:48,080
traditionally we've seen notebooks used

00:21:46,190 --> 00:21:49,940
by our data scientist and so that would

00:21:48,080 --> 00:21:52,730
be data scientist machine learning

00:21:49,940 --> 00:21:55,160
scientists and engineers agram algorithm

00:21:52,730 --> 00:21:57,440
engineers that is the traditional

00:21:55,160 --> 00:22:00,680
notebooks user with the work that we

00:21:57,440 --> 00:22:03,980
have done around scheduling every single

00:22:00,680 --> 00:22:06,740
job at Netflix will be executed through

00:22:03,980 --> 00:22:08,120
a notebook not just the notebooks

00:22:06,740 --> 00:22:10,520
themselves so if you're running a spark

00:22:08,120 --> 00:22:13,280
job if you're running a jar if you've

00:22:10,520 --> 00:22:15,560
got a sequel job you're gonna schedule

00:22:13,280 --> 00:22:16,670
that job in the same way that you

00:22:15,560 --> 00:22:19,070
typically would you're just going to

00:22:16,670 --> 00:22:21,470
specify the code but behind the scenes

00:22:19,070 --> 00:22:24,230
all of that is being executed through a

00:22:21,470 --> 00:22:25,850
notebook and we have 10,000 jobs right

00:22:24,230 --> 00:22:27,680
now that's a hundred and fifty thousand

00:22:25,850 --> 00:22:29,180
jobs a day are going to be executed

00:22:27,680 --> 00:22:32,570
through a notebook so we're gonna see

00:22:29,180 --> 00:22:35,000
increasingly that our data vis engineers

00:22:32,570 --> 00:22:37,250
our analytics engineers our data

00:22:35,000 --> 00:22:38,630
engineers they're going to be living in

00:22:37,250 --> 00:22:41,480
notebooks it gives them a much better

00:22:38,630 --> 00:22:43,250
experience for being able to test do

00:22:41,480 --> 00:22:46,880
data discovery but also to be able to

00:22:43,250 --> 00:22:50,440
debug their production workflows we are

00:22:46,880 --> 00:22:53,539
also going to increasingly work on in

00:22:50,440 --> 00:22:55,820
improving the access for the people and

00:22:53,539 --> 00:22:57,019
this top-level right so this is this is

00:22:55,820 --> 00:22:59,210
kind of directionally where we want to

00:22:57,019 --> 00:23:01,009
go we have data engineers and data vis

00:22:59,210 --> 00:23:02,979
engineers and analytics engineers using

00:23:01,009 --> 00:23:06,799
notebooks today we don't really have

00:23:02,979 --> 00:23:09,259
business analysts or tpms or PMS that

00:23:06,799 --> 00:23:10,789
are using it but as we continue to

00:23:09,259 --> 00:23:12,710
invest in this as we continue to make

00:23:10,789 --> 00:23:14,539
the experience easier and easier for

00:23:12,710 --> 00:23:17,210
people as we do more things with

00:23:14,539 --> 00:23:19,129
notebooks the goal here is that those

00:23:17,210 --> 00:23:22,359
people can feel comfortable using

00:23:19,129 --> 00:23:25,029
notebooks as well this entire group

00:23:22,359 --> 00:23:28,369
should be able to be served by a single

00:23:25,029 --> 00:23:32,479
notebook experience across the entire

00:23:28,369 --> 00:23:35,659
workflow so how are we going to do that

00:23:32,479 --> 00:23:37,399
well one is we really need to lean into

00:23:35,659 --> 00:23:41,330
simplicity we're not trying to create an

00:23:37,399 --> 00:23:43,789
IDE here we want to focus on making data

00:23:41,330 --> 00:23:47,179
easy for the people who work with data

00:23:43,789 --> 00:23:49,249
heavily but are not necessarily focused

00:23:47,179 --> 00:23:51,320
on the technical side like our data

00:23:49,249 --> 00:23:52,999
scientist also making it easier for

00:23:51,320 --> 00:23:56,269
people who work with data heavily but

00:23:52,999 --> 00:23:57,739
who like they just want to do things on

00:23:56,269 --> 00:23:59,509
ad hoc basis they don't want to invest a

00:23:57,739 --> 00:24:01,580
lot of time in coding up database right

00:23:59,509 --> 00:24:05,929
so our data engineers we want to make it

00:24:01,580 --> 00:24:07,700
easy for our non-technical people to be

00:24:05,929 --> 00:24:09,979
able to go visualize data interact with

00:24:07,700 --> 00:24:11,539
this schedule it and a lot of the work

00:24:09,979 --> 00:24:13,909
that we've done around scheduling has

00:24:11,539 --> 00:24:17,599
also enabled templates so template teen

00:24:13,909 --> 00:24:20,330
could be you know we can use it for

00:24:17,599 --> 00:24:22,399
being able to document what the data

00:24:20,330 --> 00:24:25,039
model is we can use it to demonstrate

00:24:22,399 --> 00:24:27,889
how you use a tool data scientists can

00:24:25,039 --> 00:24:29,809
actually launch a template and run

00:24:27,889 --> 00:24:31,279
multiple templates or notebooks in

00:24:29,809 --> 00:24:33,559
parallel with different parameter sets

00:24:31,279 --> 00:24:35,720
and then papermill will aggregate across

00:24:33,559 --> 00:24:36,919
all of those notebooks right so we're

00:24:35,720 --> 00:24:38,539
just trying to make these things that

00:24:36,919 --> 00:24:41,479
are really complicated nature really

00:24:38,539 --> 00:24:43,789
easy for users to do we also want to

00:24:41,479 --> 00:24:45,229
lean into integration across the entire

00:24:43,789 --> 00:24:47,330
platform they don't really need to know

00:24:45,229 --> 00:24:49,669
what Jeany is or why we care about

00:24:47,330 --> 00:24:51,409
federated job execution they don't need

00:24:49,669 --> 00:24:54,859
to know about s3 we just want to create

00:24:51,409 --> 00:24:57,349
a very you know I would say like a very

00:24:54,859 --> 00:24:59,320
user friendly experience and then we

00:24:57,349 --> 00:25:02,119
want to lean into collaboration so

00:24:59,320 --> 00:25:04,039
focusing on that end goal of a single

00:25:02,119 --> 00:25:06,080
notebook transcending all of those

00:25:04,039 --> 00:25:07,399
different user roles right it's

00:25:06,080 --> 00:25:08,779
to be a very collaborative experience

00:25:07,399 --> 00:25:10,610
there's a lot of work we have to do but

00:25:08,779 --> 00:25:21,019
that's something that we are keeping at

00:25:10,610 --> 00:25:22,909
the forefront of our mind so all the

00:25:21,019 --> 00:25:25,519
tools that we're working on and we're

00:25:22,909 --> 00:25:28,039
building in or as much as possible we

00:25:25,519 --> 00:25:29,779
need to lean into open source whether

00:25:28,039 --> 00:25:31,250
that's influencing specs for pandas

00:25:29,779 --> 00:25:33,890
there's rose we didn't actually show the

00:25:31,250 --> 00:25:36,590
data Explorer or did we and I spaced out

00:25:33,890 --> 00:25:38,240
no we did now that's a big demo would

00:25:36,590 --> 00:25:42,559
later come to the Netflix booth I'll

00:25:38,240 --> 00:25:45,470
show it there so like pandas adopted a

00:25:42,559 --> 00:25:47,059
spec for us and us in this case actually

00:25:45,470 --> 00:25:48,679
means part of interact we're like oh hey

00:25:47,059 --> 00:25:50,630
we really want to drive forward making a

00:25:48,679 --> 00:25:52,899
a JSON format that can end up in the

00:25:50,630 --> 00:25:55,429
front end that any of the any of the

00:25:52,899 --> 00:25:58,820
Jupiter front ends could work with to

00:25:55,429 --> 00:26:00,679
display a rich rich way of looking at

00:25:58,820 --> 00:26:02,690
the table whether that means a virtual

00:26:00,679 --> 00:26:05,840
table you could scroll through or even a

00:26:02,690 --> 00:26:07,730
table along with some visualization and

00:26:05,840 --> 00:26:09,710
just the same like pushing on protocols

00:26:07,730 --> 00:26:11,779
and formats within Jupiter we want we

00:26:09,710 --> 00:26:13,279
want all of that to expand and we also

00:26:11,779 --> 00:26:16,070
realized that not everything's going to

00:26:13,279 --> 00:26:18,230
be a best fit like ever and that's why

00:26:16,070 --> 00:26:20,090
there's so many tools even out now today

00:26:18,230 --> 00:26:22,100
like you know we're gonna keep exploring

00:26:20,090 --> 00:26:23,450
these spaces and trying to push in areas

00:26:22,100 --> 00:26:25,190
where you know I think in the past

00:26:23,450 --> 00:26:27,139
people always like you know why isn't

00:26:25,190 --> 00:26:29,210
get good with the notebooks why isn't

00:26:27,139 --> 00:26:30,710
this good with the notebooks and some of

00:26:29,210 --> 00:26:32,720
it's just about the fact that you know

00:26:30,710 --> 00:26:35,659
there's like people have to maintain a

00:26:32,720 --> 00:26:37,159
lot of protocols and formats and encode

00:26:35,659 --> 00:26:38,779
over the years something we need to try

00:26:37,159 --> 00:26:41,690
new areas so that we can you know find

00:26:38,779 --> 00:26:44,000
out what that kind of idealized landal

00:26:41,690 --> 00:26:47,029
landscape will be the same thing goes

00:26:44,000 --> 00:26:48,769
for for spark like just like and I'll

00:26:47,029 --> 00:26:50,840
put this as a message out to you like if

00:26:48,769 --> 00:26:53,210
you ever wonder like why spice Park

00:26:50,840 --> 00:26:54,620
doesn't have exactly what you need like

00:26:53,210 --> 00:26:56,120
you're like why doesn't it present more

00:26:54,620 --> 00:26:58,970
information most of what needs to be

00:26:56,120 --> 00:27:00,620
pushed into it are the the wrappers so

00:26:58,970 --> 00:27:02,480
like when you look at pandas it has its

00:27:00,620 --> 00:27:04,909
own wrapper HTML that's how it gets

00:27:02,480 --> 00:27:07,340
table on the page if you want if you

00:27:04,909 --> 00:27:09,230
want spark to output more information

00:27:07,340 --> 00:27:10,789
for you you you get that in if you've

00:27:09,230 --> 00:27:13,730
used PI spark recently you've probably

00:27:10,789 --> 00:27:16,820
seen that it now has like spark UI links

00:27:13,730 --> 00:27:18,679
those came from me and Ryan blue and

00:27:16,820 --> 00:27:19,620
Holden putting that together to make it

00:27:18,679 --> 00:27:22,140
so that we could we could

00:27:19,620 --> 00:27:23,580
have that in there so you know as much

00:27:22,140 --> 00:27:24,930
as possible when I keep pushing back

00:27:23,580 --> 00:27:26,640
within these like there's Netflix

00:27:24,930 --> 00:27:28,350
open-source projects then there's all

00:27:26,640 --> 00:27:31,200
the community open-source projects so I

00:27:28,350 --> 00:27:33,840
encourage everyone to to come and join

00:27:31,200 --> 00:27:36,410
and work on these like we can we can

00:27:33,840 --> 00:27:38,880
partner together we can build together

00:27:36,410 --> 00:27:40,590
yeah which it's just a good time I guess

00:27:38,880 --> 00:27:42,900
for me to tell everyone that the

00:27:40,590 --> 00:27:44,970
community Sprint's are on Saturday and

00:27:42,900 --> 00:27:48,270
you should definitely come because we'll

00:27:44,970 --> 00:27:59,880
build on things there together so that's

00:27:48,270 --> 00:28:02,309
it thank you so we rushed through that

00:27:59,880 --> 00:28:03,210
we know that was a lot of content and we

00:28:02,309 --> 00:28:04,559
could have taken it in a lot of

00:28:03,210 --> 00:28:07,590
different directions but we wanted to

00:28:04,559 --> 00:28:10,140
leave adequate time for questions and we

00:28:07,590 --> 00:28:11,850
can delve into anything if we don't have

00:28:10,140 --> 00:28:14,790
time to answer your question here please

00:28:11,850 --> 00:28:16,590
come to the booth I'm here for you guys

00:28:14,790 --> 00:28:18,990
right so if you have questions we're

00:28:16,590 --> 00:28:31,679
happy to talk about anything that we're

00:28:18,990 --> 00:28:35,010
working on here and yeah speak to how we

00:28:31,679 --> 00:28:37,830
condone DAGs in your pipelines and

00:28:35,010 --> 00:28:40,290
especially if in they may be you don't

00:28:37,830 --> 00:28:42,090
really need that if some parameters of

00:28:40,290 --> 00:28:44,309
one notebook are based on the parameters

00:28:42,090 --> 00:28:47,970
or outputs of the other notebook and

00:28:44,309 --> 00:28:50,250
maybe if you need or don't need your

00:28:47,970 --> 00:28:54,870
users or analysts to understand this

00:28:50,250 --> 00:28:57,030
whole tag how it works is that the

00:28:54,870 --> 00:28:58,650
notebook is a presentation layer so when

00:28:57,030 --> 00:29:01,800
you run this notebook and you get get

00:28:58,650 --> 00:29:04,380
outputs to it we don't allow it to be

00:29:01,800 --> 00:29:05,309
used like a function so that that

00:29:04,380 --> 00:29:07,080
doesn't happen

00:29:05,309 --> 00:29:09,120
because there's not like a there's not a

00:29:07,080 --> 00:29:11,340
good layer where you can use the up if

00:29:09,120 --> 00:29:13,410
the output is you staged data like you

00:29:11,340 --> 00:29:15,420
put it you write it out like the output

00:29:13,410 --> 00:29:17,220
is literally like you write out to s3 or

00:29:15,420 --> 00:29:19,530
you're writing to a table that's okay

00:29:17,220 --> 00:29:22,140
and I don't know much actually about how

00:29:19,530 --> 00:29:23,970
the workflow like I haven't even worked

00:29:22,140 --> 00:29:26,490
with a more complicated workflow but if

00:29:23,970 --> 00:29:27,570
a stage ends like because you can you

00:29:26,490 --> 00:29:30,179
can chain together a whole bunch of

00:29:27,570 --> 00:29:32,730
things if if one thing fails that's in a

00:29:30,179 --> 00:29:34,800
sequence of those then we don't contain

00:29:32,730 --> 00:29:36,150
one of the workflow yeah so basically a

00:29:34,800 --> 00:29:38,670
notebook should be an atomic unit

00:29:36,150 --> 00:29:39,930
anything that you could think of would

00:29:38,670 --> 00:29:41,790
be committed as part of a transaction

00:29:39,930 --> 00:29:43,950
can be encapsulated within a notebook a

00:29:41,790 --> 00:29:46,230
notebook can be a complete end-to-end

00:29:43,950 --> 00:29:50,670
workflow you can ingest data transform

00:29:46,230 --> 00:29:52,230
it and store it but as you as you can

00:29:50,670 --> 00:29:54,420
imagine a lot of this stuff that we work

00:29:52,230 --> 00:29:56,550
with are pretty complex and so well you

00:29:54,420 --> 00:29:59,910
can do things end to end you probably

00:29:56,550 --> 00:30:01,290
want to do that in your simpler cases if

00:29:59,910 --> 00:30:02,670
you're trying to do an aggregate or a de

00:30:01,290 --> 00:30:04,440
normalization that would make sense to

00:30:02,670 --> 00:30:06,270
put that entire workflow inside of a

00:30:04,440 --> 00:30:07,950
single notebook if you are doing

00:30:06,270 --> 00:30:09,090
something more complicated or even if

00:30:07,950 --> 00:30:10,920
you're doing the aggregation or daar

00:30:09,090 --> 00:30:12,870
normalizing but it's a very large

00:30:10,920 --> 00:30:15,330
complicated data set then you probably

00:30:12,870 --> 00:30:18,330
want to break each one of those each one

00:30:15,330 --> 00:30:19,650
of those steps into its own notebook all

00:30:18,330 --> 00:30:21,750
right and then we would orchestrate

00:30:19,650 --> 00:30:23,790
across all of those notebooks inside of

00:30:21,750 --> 00:30:25,860
Maison so you'd have to specify a

00:30:23,790 --> 00:30:28,740
workflow at that point instead of a

00:30:25,860 --> 00:30:30,810
single you know basically a notebook can

00:30:28,740 --> 00:30:31,770
be its own workflow and when we schedule

00:30:30,810 --> 00:30:33,000
inside of a notebook

00:30:31,770 --> 00:30:34,410
we're just scheduling just a single

00:30:33,000 --> 00:30:35,850
notebook if you only simply more complex

00:30:34,410 --> 00:30:45,390
as a workflow you'd have to go into our

00:30:35,850 --> 00:30:47,940
scheduling thank you so just to follow

00:30:45,390 --> 00:30:52,350
up on this question in your slides so

00:30:47,940 --> 00:30:55,110
EFS called out as the output for

00:30:52,350 --> 00:30:57,510
notebooks and I think that confused me a

00:30:55,110 --> 00:31:00,120
little bit too so EFS is only used for

00:30:57,510 --> 00:31:01,380
the interactive computing so when

00:31:00,120 --> 00:31:03,090
someone's working in their workspace

00:31:01,380 --> 00:31:06,210
when they schedule something is a job

00:31:03,090 --> 00:31:08,100
the ef-s is not mounted on purpose like

00:31:06,210 --> 00:31:10,320
if they need to pull data from somewhere

00:31:08,100 --> 00:31:12,540
they need to stage it locally like their

00:31:10,320 --> 00:31:14,370
notebooks should should run as if it

00:31:12,540 --> 00:31:18,030
doesn't know how the file system is laid

00:31:14,370 --> 00:31:20,040
out actually as part of the scheduling

00:31:18,030 --> 00:31:23,310
process we then copy that notebook over

00:31:20,040 --> 00:31:25,140
to s3 yeah it becomes like one immutable

00:31:23,310 --> 00:31:27,060
notebook so like if you've scheduled one

00:31:25,140 --> 00:31:28,380
you've scheduled that version that way

00:31:27,060 --> 00:31:33,990
you can keep editing your notebook and

00:31:28,380 --> 00:31:36,140
then schedule it again later yeah in the

00:31:33,990 --> 00:31:36,140
back

00:31:37,370 --> 00:31:43,350
hello um for choosing like the different

00:31:40,830 --> 00:31:44,999
interfaces or images representing shows

00:31:43,350 --> 00:31:47,190
that you the slave to display to the

00:31:44,999 --> 00:31:51,090
users how do you gauge or quantify user

00:31:47,190 --> 00:31:52,559
experience using notebooks or data I'm

00:31:51,090 --> 00:31:54,240
sorry it was a little muffled what was

00:31:52,559 --> 00:31:56,850
the last bit Oh like how do you gauge or

00:31:54,240 --> 00:31:59,669
quantify user experience using notebook

00:31:56,850 --> 00:32:02,129
the notebook or data oh okay how would

00:31:59,669 --> 00:32:04,559
we quantify the user experience well we

00:32:02,129 --> 00:32:07,769
do that through a handful of different

00:32:04,559 --> 00:32:09,960
ways I would say one is more subjective

00:32:07,769 --> 00:32:11,700
we survey our users to find out what is

00:32:09,960 --> 00:32:14,519
the overall experience like we're

00:32:11,700 --> 00:32:17,249
constantly working with them and talking

00:32:14,519 --> 00:32:18,929
to them you know we're actually in the

00:32:17,249 --> 00:32:20,730
same org and the data engineering and

00:32:18,929 --> 00:32:22,110
infrastructure works with our data

00:32:20,730 --> 00:32:24,539
engineers and so we have a very close

00:32:22,110 --> 00:32:27,090
relationship there the other way is a

00:32:24,539 --> 00:32:28,830
little bit more discreet which is

00:32:27,090 --> 00:32:30,389
looking at the number of notebooks users

00:32:28,830 --> 00:32:34,919
we have the number of people who are

00:32:30,389 --> 00:32:36,450
using notebooks to schedule jobs how

00:32:34,919 --> 00:32:38,039
many people are still using Zeppelin

00:32:36,450 --> 00:32:39,929
because we're not supporting or we're

00:32:38,039 --> 00:32:41,789
not reliable enough in Jupiter right so

00:32:39,929 --> 00:32:44,720
we would be looking at that type and and

00:32:41,789 --> 00:32:44,720
measuring that way

00:32:54,659 --> 00:32:58,080
this this might be kind of a very

00:32:56,730 --> 00:32:59,669
specific question but you mentioned that

00:32:58,080 --> 00:33:01,499
you have a way to toggle cells in

00:32:59,669 --> 00:33:02,820
Jupiter notebooks to indicate that there

00:33:01,499 --> 00:33:05,129
are metadata that there are parameters

00:33:02,820 --> 00:33:07,549
basically does that mean you've like

00:33:05,129 --> 00:33:10,289
broken the interoperability with other

00:33:07,549 --> 00:33:12,120
Jupiter notebooks you know so actually

00:33:10,289 --> 00:33:15,299
before we showed it in here this

00:33:12,120 --> 00:33:16,649
actually appears as a tag and you can do

00:33:15,299 --> 00:33:18,389
this in the classic you can use paper

00:33:16,649 --> 00:33:21,029
mill and with the classic notebook UI

00:33:18,389 --> 00:33:22,499
and with the Jupiter lab UI you just in

00:33:21,029 --> 00:33:24,749
the classic notebook UI you first have

00:33:22,499 --> 00:33:26,399
to open the tags menu and so that'll

00:33:24,749 --> 00:33:29,249
show tags on every cell and then then

00:33:26,399 --> 00:33:31,139
the tags you type parameters so this

00:33:29,249 --> 00:33:32,759
made it a one-click to add the tag in

00:33:31,139 --> 00:33:34,889
but it's compliant with the spec itself

00:33:32,759 --> 00:33:36,389
so you can you can manually make that

00:33:34,889 --> 00:33:37,860
tag yourself you're saying interesting

00:33:36,389 --> 00:33:40,470
ok there's not yeah there's no magic

00:33:37,860 --> 00:33:44,309
here we can load the same Jupiter

00:33:40,470 --> 00:33:46,950
notebook in Jupiter classic Jupiter labs

00:33:44,309 --> 00:33:48,720
and interact alright so the it's still

00:33:46,950 --> 00:33:58,740
the same notebook it's just a different

00:33:48,720 --> 00:34:02,759
way to render the UI well wait for the

00:33:58,740 --> 00:34:03,869
mic so something like normalization

00:34:02,759 --> 00:34:06,149
that's probably something that a lot of

00:34:03,869 --> 00:34:07,320
different notebooks users are doing at

00:34:06,149 --> 00:34:09,810
the same time and they might be using

00:34:07,320 --> 00:34:11,429
different normalizing in different ways

00:34:09,810 --> 00:34:12,869
how do you think about leverage ability

00:34:11,429 --> 00:34:15,960
and letting people share best practices

00:34:12,869 --> 00:34:16,829
when they're in this environment where a

00:34:15,960 --> 00:34:20,730
lot of different people are using

00:34:16,829 --> 00:34:23,190
notebooks a good question we're tackling

00:34:20,730 --> 00:34:24,869
that in two different ways one is we

00:34:23,190 --> 00:34:28,230
want to improve notebook discovery and

00:34:24,869 --> 00:34:31,169
sharing so we can share it right now but

00:34:28,230 --> 00:34:34,800
that sharing happens with me sending you

00:34:31,169 --> 00:34:36,540
a link to my notebook and Commuter what

00:34:34,800 --> 00:34:38,069
we want to do is surface it in our

00:34:36,540 --> 00:34:40,020
portal in the same way that we surface

00:34:38,069 --> 00:34:42,990
all of our other tables it just becomes

00:34:40,020 --> 00:34:44,760
another object in our catalog so people

00:34:42,990 --> 00:34:46,290
can search for it they you know have the

00:34:44,760 --> 00:34:48,000
annotations around the notebook and we

00:34:46,290 --> 00:34:52,020
can say here is a notebook that does

00:34:48,000 --> 00:34:53,970
this work the second way is a separate

00:34:52,020 --> 00:34:57,930
process altogether that we are looking

00:34:53,970 --> 00:34:59,849
for trying to improve the overall like

00:34:57,930 --> 00:35:03,480
metrics definition and standardizing

00:34:59,849 --> 00:35:04,210
across teams and that's happening in a

00:35:03,480 --> 00:35:05,740
different group

00:35:04,210 --> 00:35:07,930
called the metrics factory and so that's

00:35:05,740 --> 00:35:10,180
something that we haven't really talked

00:35:07,930 --> 00:35:46,599
about publicly at this point but we'll

00:35:10,180 --> 00:35:48,339
talk about in your job will fail yes yes

00:35:46,599 --> 00:35:50,109
oh you can always ask for so there are

00:35:48,339 --> 00:35:53,560
caps on things like memory and stuff and

00:35:50,109 --> 00:35:56,109
there they're definitely like we give

00:35:53,560 --> 00:35:58,089
people enough freedom and flexibility

00:35:56,109 --> 00:36:00,280
and actually I should put this as

00:35:58,089 --> 00:36:01,720
freedom or responsibility for how

00:36:00,280 --> 00:36:03,460
they're gonna run the jobs and if they

00:36:01,720 --> 00:36:05,800
if they need more memory they can get

00:36:03,460 --> 00:36:08,200
more memory if they're maxed out on

00:36:05,800 --> 00:36:11,920
spark jobs for instance like they'll

00:36:08,200 --> 00:36:14,200
know and they'll find out in h2 query

00:36:11,920 --> 00:36:15,670
and who that they're they're already

00:36:14,200 --> 00:36:18,490
like at their allocation and they can

00:36:15,670 --> 00:36:21,250
ask the the team to get more more

00:36:18,490 --> 00:36:22,589
resources if they need it so yeah

00:36:21,250 --> 00:36:25,930
they're there aren't they're definitely

00:36:22,589 --> 00:36:28,450
there definitely feel safes but we give

00:36:25,930 --> 00:36:31,180
a lot of access out so we also have

00:36:28,450 --> 00:36:33,520
workload isolation so Gini is a

00:36:31,180 --> 00:36:35,500
federated job execution engine and when

00:36:33,520 --> 00:36:38,650
you define a genie job you can specify

00:36:35,500 --> 00:36:41,470
is this ad-hoc workload or is this

00:36:38,650 --> 00:36:45,160
production workload and so while you can

00:36:41,470 --> 00:36:46,420
take down the ad hoc cluster and you

00:36:45,160 --> 00:36:47,710
know disrupt a lot of people and they'll

00:36:46,420 --> 00:37:03,940
be unhappy it's not going to affect

00:36:47,710 --> 00:37:06,780
production whenever I have an a/b test

00:37:03,940 --> 00:37:10,060
and actually evaluate the efficacy of a

00:37:06,780 --> 00:37:13,780
certain campaign or a certain certain

00:37:10,060 --> 00:37:15,339
process how in this structure do you

00:37:13,780 --> 00:37:16,859
actually allow for somebody to create

00:37:15,339 --> 00:37:19,589
the test

00:37:16,859 --> 00:37:31,440
and then evaluate between the results of

00:37:19,589 --> 00:37:36,809
two different jobs I would say oh how do

00:37:31,440 --> 00:37:41,789
we how do we enable a/b testing and to

00:37:36,809 --> 00:37:43,920
be able to work across like different

00:37:41,789 --> 00:37:48,140
tests right and be able to surface the

00:37:43,920 --> 00:37:49,440
results across all of those I would say

00:37:48,140 --> 00:37:50,999
two things

00:37:49,440 --> 00:37:53,219
again we have two different solutions

00:37:50,999 --> 00:37:55,559
our part in this would probably be paper

00:37:53,219 --> 00:37:56,999
mill and so paper mill allows us to

00:37:55,559 --> 00:37:58,200
create a template and so you could have

00:37:56,999 --> 00:38:01,380
a template that encapsulates the entire

00:37:58,200 --> 00:38:03,690
experiment and when you run it you can

00:38:01,380 --> 00:38:06,180
run it with different you know different

00:38:03,690 --> 00:38:08,549
variables or different inputs or it

00:38:06,180 --> 00:38:10,289
could be like different algorithms

00:38:08,549 --> 00:38:12,539
whatever it is but something something's

00:38:10,289 --> 00:38:15,269
going to change inside of that and it

00:38:12,539 --> 00:38:16,979
will allow you to surface those across

00:38:15,269 --> 00:38:18,420
all of the notebooks so that's one thing

00:38:16,979 --> 00:38:20,609
that a scientist could do if they were

00:38:18,420 --> 00:38:21,719
done on an ad hoc basis the other thing

00:38:20,609 --> 00:38:24,569
is we've actually built an entire

00:38:21,719 --> 00:38:26,099
experimentation platform which enables a

00:38:24,569 --> 00:38:29,279
B testing so if you go to the Netflix

00:38:26,099 --> 00:38:33,719
tech blog they describe it a lot more

00:38:29,279 --> 00:38:36,269
there when they run those jobs it is

00:38:33,719 --> 00:38:38,519
running through a notebook but they and

00:38:36,269 --> 00:38:42,269
they would use it on an ad hoc or

00:38:38,519 --> 00:38:43,619
exploratory basis but it's not there's a

00:38:42,269 --> 00:38:44,880
lot more infrastructure that goes into

00:38:43,619 --> 00:38:45,630
supporting that and trying to make it as

00:38:44,880 --> 00:38:47,069
easy as possible

00:38:45,630 --> 00:38:48,390
so there's a there's an article you can

00:38:47,069 --> 00:38:49,289
search for about this it's called all

00:38:48,390 --> 00:38:52,799
about testing

00:38:49,289 --> 00:38:54,390
it's all a /b about testing the Netflix

00:38:52,799 --> 00:38:56,719
experimentation platform you can read

00:38:54,390 --> 00:38:56,719
more there

00:39:02,450 --> 00:39:09,170
have you discovered a non-traditional

00:39:05,630 --> 00:39:11,930
use for the notebooks while using it I

00:39:09,170 --> 00:39:15,230
mean something that not what everybody

00:39:11,930 --> 00:39:17,990
else would be using it for say for maybe

00:39:15,230 --> 00:39:22,610
you know discovering a new process or

00:39:17,990 --> 00:39:24,050
something of that sort I think the most

00:39:22,610 --> 00:39:25,850
interesting thing that I've seen out of

00:39:24,050 --> 00:39:29,300
notebooks after seeing it in practice is

00:39:25,850 --> 00:39:33,130
that less internal tools need to be

00:39:29,300 --> 00:39:35,420
built because people can just build

00:39:33,130 --> 00:39:36,710
something that like they can build this

00:39:35,420 --> 00:39:38,090
you know the notebook is kind of

00:39:36,710 --> 00:39:40,070
ephemeral in this sense it's just this

00:39:38,090 --> 00:39:42,530
file you can make it and throw it away

00:39:40,070 --> 00:39:45,440
and the notebook server itself serves as

00:39:42,530 --> 00:39:47,600
this web server so you don't have to

00:39:45,440 --> 00:39:48,680
stand up a whole team just to like put

00:39:47,600 --> 00:39:50,360
up a thing where they're you're gonna

00:39:48,680 --> 00:39:52,310
like analyze some amount of data and be

00:39:50,360 --> 00:39:54,440
able to you know change some some

00:39:52,310 --> 00:39:57,800
parameters and and toggle between stuff

00:39:54,440 --> 00:39:59,690
and so it's it's phenomenal that people

00:39:57,800 --> 00:40:02,690
don't have to go figure out how to

00:39:59,690 --> 00:40:04,700
deploy a specialized server and build up

00:40:02,690 --> 00:40:10,790
all this tooling when they can just do

00:40:04,700 --> 00:40:13,730
it in the notebook actually I think we

00:40:10,790 --> 00:40:15,590
are at time so we are gonna stick around

00:40:13,730 --> 00:40:17,750
out here in the hallway and then we'll

00:40:15,590 --> 00:40:19,580
head over to the booth if you guys have

00:40:17,750 --> 00:40:21,600
any questions please swing by thank you

00:40:19,580 --> 00:40:24,500
very much

00:40:21,600 --> 00:40:24,500

YouTube URL: https://www.youtube.com/watch?v=MaDXqDUG5dk


