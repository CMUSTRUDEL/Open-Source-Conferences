Title: Visualizing high-dimensional biological data with Clustergrammer-Widget in the Jupyter Notebook
Publication date: 2018-09-20
Playlist: JupyterCon in New York 2018
Description: 
	Visualizing high-dimensional biological data with Clustergrammer-Widget in the Jupyter Notebook
Nicolas Fernandez (Icahn School of Medicine at Mount Sinai)

Biological data and other data collected from complex systems can have tens of thousands of variables that interact nonlinearly. Interactive visualizations enable users to develop an intuition about the global structure of their data and immediately identify patterns. While dimensionality reduction techniques are useful for obtaining a birdâ€™s eye view of data, these techniques often obscure important information. Heatmaps, or clustergrams, are powerful alternative but complementary visualization techniques for directly visualizing all variables from high-dimensional data. While there are many software tools that can generate clustergrams, few are web based, fully interactive, or seamlessly integrated into Jupyter notebooks.

Nicolas Fernandez offers an overview of Clustergrammer-Widget, which enables users to easily visualize high-dimensional data (e.g., a pandas DataFrame) within a Jupyter notebook as an interactive hierarchically clustered heatmap. Clustergrammer-Widget generates highly interactive visualizations (e.g., reorderable and zoomable) that can be embedded within notebooks and shared using nbviewer. Clustergrammer-Widget was developed to analyze high-dimensional biological data but can be applied to any high-dimensional data from other fields. Nicolas explains how to use Jupyter notebooks and Clustergrammer-Widget to produce transparent and reproducible analyses for a wide variety of biological datasets and demonstrates how to share your results with collaborators.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,670 --> 00:00:04,730
I'm Nicolas Fernandez I'm a

00:00:03,199 --> 00:00:07,820
computational scientist at the human

00:00:04,730 --> 00:00:09,139
immune monitoring center here in New

00:00:07,820 --> 00:00:10,760
York City at the Icahn School of

00:00:09,139 --> 00:00:13,070
Medicine at Mount Sinai and former

00:00:10,760 --> 00:00:14,150
postdoc into my own lab and I'll be

00:00:13,070 --> 00:00:15,650
discussing visualizing a high

00:00:14,150 --> 00:00:18,010
dimensional biological data with cluster

00:00:15,650 --> 00:00:21,080
grammar widget in the Jupiter notebook

00:00:18,010 --> 00:00:23,260
so just as an overview for the talk I'll

00:00:21,080 --> 00:00:25,550
start with a quick motivating example to

00:00:23,260 --> 00:00:27,560
to kind of get it started and then I'll

00:00:25,550 --> 00:00:28,850
discuss specifically visualizing

00:00:27,560 --> 00:00:31,369
biological data and the problems that

00:00:28,850 --> 00:00:32,930
are inherent with that and then discuss

00:00:31,369 --> 00:00:34,489
building and using a cluster grammar

00:00:32,930 --> 00:00:35,809
widget which is a Jupiter widget which

00:00:34,489 --> 00:00:37,909
can be used within a topic and then

00:00:35,809 --> 00:00:39,289
finally end with a few case studies and

00:00:37,909 --> 00:00:41,149
discuss the future directions of the

00:00:39,289 --> 00:00:44,269
project okay

00:00:41,149 --> 00:00:45,889
so we'll start with limb is Rob so it's

00:00:44,269 --> 00:00:47,239
a novel by Victor Hugo and it's

00:00:45,889 --> 00:00:49,190
considered one of the greatest novels

00:00:47,239 --> 00:00:51,170
ever written and so let's say that our

00:00:49,190 --> 00:00:52,309
goal is to understand the story of look

00:00:51,170 --> 00:00:53,749
miserable right so if we want to

00:00:52,309 --> 00:00:55,010
understand the story the most

00:00:53,749 --> 00:00:56,719
straightforward way is just to read the

00:00:55,010 --> 00:00:58,459
book right but it's also one of the

00:00:56,719 --> 00:01:00,619
longest novels ever written it like 1500

00:00:58,459 --> 00:01:01,850
pages so we if let's just say we don't

00:01:00,619 --> 00:01:03,109
have the time to read the book so what

00:01:01,850 --> 00:01:04,699
we could do is maybe write a script that

00:01:03,109 --> 00:01:06,109
goes through and gather some data and

00:01:04,699 --> 00:01:08,090
then we can visualize data about this

00:01:06,109 --> 00:01:10,579
story and try and get some idea of

00:01:08,090 --> 00:01:14,420
what's happening in this story so one

00:01:10,579 --> 00:01:17,119
thing we can do is construct a network

00:01:14,420 --> 00:01:19,340
of the characters in the story as a to

00:01:17,119 --> 00:01:21,439
get some idea what's happening so we can

00:01:19,340 --> 00:01:22,609
take measurements about these characters

00:01:21,439 --> 00:01:24,200
and we can basically find out the

00:01:22,609 --> 00:01:26,479
chapters that these characters occur in

00:01:24,200 --> 00:01:27,740
and then connect characters based on

00:01:26,479 --> 00:01:29,359
code currents in the same chapter as a

00:01:27,740 --> 00:01:32,899
proxy for like their interaction in the

00:01:29,359 --> 00:01:35,840
story and and then this interactive

00:01:32,899 --> 00:01:37,880
example here shows just that network

00:01:35,840 --> 00:01:40,820
it's actually one of the examples from

00:01:37,880 --> 00:01:42,799
Mike postdocs d3.js if you all are

00:01:40,820 --> 00:01:44,929
familiar and it's visualizing the

00:01:42,799 --> 00:01:46,280
network of characters as a ball and

00:01:44,929 --> 00:01:47,929
stick network so you can like drag it

00:01:46,280 --> 00:01:49,670
around so each node in the network is a

00:01:47,929 --> 00:01:51,439
character and the links represent

00:01:49,670 --> 00:01:53,299
co-occurrence in chapters so if you

00:01:51,439 --> 00:01:56,179
hover over the node you'll see the name

00:01:53,299 --> 00:01:58,210
so this visualization is very

00:01:56,179 --> 00:02:00,170
straightforward and it allows us to see

00:01:58,210 --> 00:02:02,719
clusters of different characters they've

00:02:00,170 --> 00:02:04,249
been colored different colors using a

00:02:02,719 --> 00:02:06,409
clustering algorithm to identify

00:02:04,249 --> 00:02:08,119
clusters and then so we can kind of see

00:02:06,409 --> 00:02:10,069
that there's there's certain structures

00:02:08,119 --> 00:02:11,540
or groups of characters that Co interact

00:02:10,069 --> 00:02:14,490
and then we can see like they're like

00:02:11,540 --> 00:02:15,960
really main characters like Valjean who

00:02:14,490 --> 00:02:17,520
interacts with like a lot of people in

00:02:15,960 --> 00:02:23,640
the story so we know this is really

00:02:17,520 --> 00:02:24,930
important person so like I said we could

00:02:23,640 --> 00:02:27,210
identify the main characters identify

00:02:24,930 --> 00:02:28,860
clusters and it's a straightforward and

00:02:27,210 --> 00:02:30,840
compact visualization but this

00:02:28,860 --> 00:02:33,390
weaknesses of the visualization are that

00:02:30,840 --> 00:02:35,760
there are no labels so you have to hover

00:02:33,390 --> 00:02:38,880
over the the nodes to see the labels and

00:02:35,760 --> 00:02:41,670
also the links frequently overlap so you

00:02:38,880 --> 00:02:42,900
kind of turns into like hard to

00:02:41,670 --> 00:02:44,580
visualize like exactly what is happening

00:02:42,900 --> 00:02:47,010
in these areas because there's like so

00:02:44,580 --> 00:02:49,350
much happening so an alternative way to

00:02:47,010 --> 00:02:51,630
visualize a network is using an

00:02:49,350 --> 00:02:55,830
adjacency matrix so with an adjacency

00:02:51,630 --> 00:02:58,590
matrix you have your your characters as

00:02:55,830 --> 00:03:00,240
columns and rows and then the cells of

00:02:58,590 --> 00:03:01,770
your matrix indicate the their

00:03:00,240 --> 00:03:03,300
interaction so we're visualizing the

00:03:01,770 --> 00:03:04,770
same network as an adjacency matrix and

00:03:03,300 --> 00:03:07,170
then you can also see the clusters that

00:03:04,770 --> 00:03:09,600
you have here so the we have this

00:03:07,170 --> 00:03:12,840
interactive notebook versions or

00:03:09,600 --> 00:03:15,990
interactive d3.js version from a another

00:03:12,840 --> 00:03:18,180
example from my postdoc so now we can

00:03:15,990 --> 00:03:21,650
see the same character clusters and in

00:03:18,180 --> 00:03:24,570
similar to the actual force-directed

00:03:21,650 --> 00:03:26,250
bond stick Network the orientation of

00:03:24,570 --> 00:03:28,410
our components helps us visualize

00:03:26,250 --> 00:03:29,460
structures so here these characters have

00:03:28,410 --> 00:03:31,320
been ordered using hierarchical

00:03:29,460 --> 00:03:33,840
clustering but we can also change the

00:03:31,320 --> 00:03:35,970
order so we can say order them by name

00:03:33,840 --> 00:03:37,950
and now you're going to lose all that

00:03:35,970 --> 00:03:39,330
structure so it's the same data but like

00:03:37,950 --> 00:03:40,890
it's very you're not seeing the same

00:03:39,330 --> 00:03:42,450
structure in the data and then if you

00:03:40,890 --> 00:03:44,880
order by frequency like by the total

00:03:42,450 --> 00:03:46,320
interactions you can start to see the

00:03:44,880 --> 00:03:47,730
same the same main characters again

00:03:46,320 --> 00:03:49,920
right the ranked on number of

00:03:47,730 --> 00:03:50,820
interactions so when you think of a

00:03:49,920 --> 00:03:52,560
network you

00:03:50,820 --> 00:03:54,000
we now kind of like know there's two

00:03:52,560 --> 00:03:55,080
ways to visualize it and when one

00:03:54,000 --> 00:03:56,580
weakness of this is that it's more

00:03:55,080 --> 00:03:58,050
visually spread out and a sort of

00:03:56,580 --> 00:04:00,420
weakness strength is the orientation

00:03:58,050 --> 00:04:03,120
like the the way we arrange the data

00:04:00,420 --> 00:04:05,070
right so so we have two ways of

00:04:03,120 --> 00:04:06,060
visualizing the network but we're

00:04:05,070 --> 00:04:08,340
missing something all right

00:04:06,060 --> 00:04:09,930
so what about the chapters themselves so

00:04:08,340 --> 00:04:11,130
the characters we know they co-occur but

00:04:09,930 --> 00:04:12,480
we don't know the specific chapters they

00:04:11,130 --> 00:04:15,060
co-occur did they go her the beginning

00:04:12,480 --> 00:04:17,790
of the story at the end of the story and

00:04:15,060 --> 00:04:19,260
then also what like what chapters are

00:04:17,790 --> 00:04:20,280
related to each other based on character

00:04:19,260 --> 00:04:22,590
co-occurrence like we could make a

00:04:20,280 --> 00:04:23,880
network of the chapters - if you sort of

00:04:22,590 --> 00:04:25,830
flip your the way of thinking of the

00:04:23,880 --> 00:04:26,590
data so this is where a heat map comes

00:04:25,830 --> 00:04:29,820
in

00:04:26,590 --> 00:04:33,580
and what we're seeing here heat map

00:04:29,820 --> 00:04:35,380
allows you to visualize your data as a

00:04:33,580 --> 00:04:37,630
matrix so here we have our characters as

00:04:35,380 --> 00:04:39,880
columns and then we have our chapters as

00:04:37,630 --> 00:04:41,320
rows and here we're clustering using

00:04:39,880 --> 00:04:43,690
hierarchical hierarchical clustering

00:04:41,320 --> 00:04:45,490
both the characters and the columns and

00:04:43,690 --> 00:04:47,050
the the chapters here so we can now see

00:04:45,490 --> 00:04:48,370
specifically which chapters the

00:04:47,050 --> 00:04:50,350
characters co-current so you can see

00:04:48,370 --> 00:04:52,150
these two characters like Coker

00:04:50,350 --> 00:04:53,889
throughout the story these coker in this

00:04:52,150 --> 00:04:55,360
chapter here and then if you rearrange

00:04:53,889 --> 00:04:57,310
the data again you can see other

00:04:55,360 --> 00:05:00,340
structures so if you if you rearrange

00:04:57,310 --> 00:05:02,050
your rows based on the chapters based on

00:05:00,340 --> 00:05:03,520
their number you can see like the flow

00:05:02,050 --> 00:05:05,290
of the story so it starts off with these

00:05:03,520 --> 00:05:06,460
people and then it's chapters progress

00:05:05,290 --> 00:05:07,479
it moves on to these other people and

00:05:06,460 --> 00:05:10,240
then these people are here at the end

00:05:07,479 --> 00:05:12,580
and then you can you could also reorder

00:05:10,240 --> 00:05:14,620
your chapters or your your characters by

00:05:12,580 --> 00:05:16,389
some to see the main characters and and

00:05:14,620 --> 00:05:19,600
basically interact with your data that

00:05:16,389 --> 00:05:22,180
way so that motivate example shows us

00:05:19,600 --> 00:05:23,560
how we're how a keep map is really like

00:05:22,180 --> 00:05:25,060
you can think of it as like two networks

00:05:23,560 --> 00:05:27,550
that once like an adjacency matrix of

00:05:25,060 --> 00:05:29,229
both networks but I'm important to this

00:05:27,550 --> 00:05:30,370
is the fact that if we really want to

00:05:29,229 --> 00:05:31,720
understand the story is only gonna get

00:05:30,370 --> 00:05:33,669
us so far like this is a deeper interest

00:05:31,720 --> 00:05:35,260
again it's more information than just a

00:05:33,669 --> 00:05:36,729
network but you still like you need to

00:05:35,260 --> 00:05:38,800
read the story it's understand it right

00:05:36,729 --> 00:05:40,180
so now we'll switch over to biology so

00:05:38,800 --> 00:05:43,389
I'll just give a very very quick primer

00:05:40,180 --> 00:05:46,090
to a couple of core biology concepts so

00:05:43,389 --> 00:05:47,560
each the the core component of in

00:05:46,090 --> 00:05:50,470
biology is a cell like that's the

00:05:47,560 --> 00:05:52,510
smallest unit and in a cell you have

00:05:50,470 --> 00:05:55,389
your your information to make the cells

00:05:52,510 --> 00:05:56,889
stored in DNA and by and large the main

00:05:55,389 --> 00:05:58,450
component of that is different genes so

00:05:56,889 --> 00:06:00,039
you have like in humans you have you

00:05:58,450 --> 00:06:03,190
know roughly twenty thirty thousand

00:06:00,039 --> 00:06:05,889
genes the the information is stored in

00:06:03,190 --> 00:06:08,229
the DNA but when you actually a cell is

00:06:05,889 --> 00:06:11,139
really the working components or mainly

00:06:08,229 --> 00:06:12,400
proteins so to get to to from DNA to

00:06:11,139 --> 00:06:14,320
proteins you need to go through RNA

00:06:12,400 --> 00:06:16,630
which is again by and large like a sort

00:06:14,320 --> 00:06:18,190
of temporary working copy of your data

00:06:16,630 --> 00:06:20,169
so you don't want to like mess up the

00:06:18,190 --> 00:06:22,539
DNA so you have an immutable copy of the

00:06:20,169 --> 00:06:24,789
DNA and and this is how information is

00:06:22,539 --> 00:06:26,320
transferred so it's a very complicated

00:06:24,789 --> 00:06:28,950
system because you have like trillions

00:06:26,320 --> 00:06:31,270
of cells in your body 20,000 genes

00:06:28,950 --> 00:06:32,440
20,000 proteins it might be more it's

00:06:31,270 --> 00:06:34,510
very very complicated system and the

00:06:32,440 --> 00:06:36,250
biology biological data will be seen is

00:06:34,510 --> 00:06:39,789
taking snapshots of these different

00:06:36,250 --> 00:06:40,540
molecules in cells so biological data is

00:06:39,789 --> 00:06:42,130
difficult to

00:06:40,540 --> 00:06:43,540
and part of the reason is that it can be

00:06:42,130 --> 00:06:46,120
high dimensional and this is just

00:06:43,540 --> 00:06:48,040
showing a visualization of a huge

00:06:46,120 --> 00:06:49,390
biological network of probably

00:06:48,040 --> 00:06:51,190
interacting proteins and it just kind of

00:06:49,390 --> 00:06:54,220
shows you like the scope of like how

00:06:51,190 --> 00:06:57,550
complicated the interactions are so if

00:06:54,220 --> 00:06:59,980
you if you don't do it well it becomes

00:06:57,550 --> 00:07:01,960
very uninterpretable so for instance RNA

00:06:59,980 --> 00:07:03,700
seek data you can measure like twenty to

00:07:01,960 --> 00:07:05,350
thirty thousand genes in bulk

00:07:03,700 --> 00:07:07,570
flow cytometry you can measure right

00:07:05,350 --> 00:07:08,860
fifty or so proteins but you can measure

00:07:07,570 --> 00:07:10,750
those 50 or so proteins and like

00:07:08,860 --> 00:07:12,070
millions of cells and then single cell

00:07:10,750 --> 00:07:13,570
RNA see some of the newest data where

00:07:12,070 --> 00:07:16,000
you can measure the whole expression of

00:07:13,570 --> 00:07:18,310
every gene which is about again 20,000

00:07:16,000 --> 00:07:19,930
genes in thousands of individual cells

00:07:18,310 --> 00:07:23,440
so you have matrices that are like

00:07:19,930 --> 00:07:26,710
really very large for biology so one

00:07:23,440 --> 00:07:27,580
technique to to deal with high

00:07:26,710 --> 00:07:29,320
dimensional data is to perform

00:07:27,580 --> 00:07:32,890
dimensionality reduction and this is

00:07:29,320 --> 00:07:34,600
just a quick example of T as an e so two

00:07:32,890 --> 00:07:36,340
very popular dimensionality reduction

00:07:34,600 --> 00:07:38,770
algorithms are principal component

00:07:36,340 --> 00:07:40,660
analysis and t-sne to stochastic

00:07:38,770 --> 00:07:42,820
neighbor embedding so this is just

00:07:40,660 --> 00:07:44,380
showing you an embedding of handwritten

00:07:42,820 --> 00:07:46,060
digits which can be thought of as high

00:07:44,380 --> 00:07:47,950
dimensional data points because they

00:07:46,060 --> 00:07:49,450
each pixel can be thought of as a

00:07:47,950 --> 00:07:51,340
dimension and then here it's showing a

00:07:49,450 --> 00:07:53,320
cluster of different digits so we're

00:07:51,340 --> 00:07:54,820
seeing that the digits are clustering

00:07:53,320 --> 00:07:56,550
based on the identity of the digits so

00:07:54,820 --> 00:07:58,900
it does a good job of showing us

00:07:56,550 --> 00:08:01,270
clustering of our data points but it's

00:07:58,900 --> 00:08:03,070
obscuring the underlying data right so

00:08:01,270 --> 00:08:04,360
if our underlying data pixels we may not

00:08:03,070 --> 00:08:05,530
care that much but if it's genes we

00:08:04,360 --> 00:08:07,540
probably care a lot more about those

00:08:05,530 --> 00:08:09,640
underlying pixels or underlying data

00:08:07,540 --> 00:08:13,120
points so again like so mirror what we

00:08:09,640 --> 00:08:14,590
just saw before we can also just

00:08:13,120 --> 00:08:18,190
directly visualize the data using a heat

00:08:14,590 --> 00:08:20,320
map so in this case we have samples as

00:08:18,190 --> 00:08:22,030
rows here just just a toy data set and

00:08:20,320 --> 00:08:24,520
we have some genes as columns and we can

00:08:22,030 --> 00:08:25,990
we're directly able to visualize high

00:08:24,520 --> 00:08:27,850
dimensional data using a heat map where

00:08:25,990 --> 00:08:29,560
we have our data points as columns and

00:08:27,850 --> 00:08:31,450
our dimensions as rows or vice versa and

00:08:29,560 --> 00:08:33,160
then again the arrangement of those is

00:08:31,450 --> 00:08:34,540
usually done through hierarchical

00:08:33,160 --> 00:08:36,669
clustering and you can have a dendrogram

00:08:34,540 --> 00:08:38,860
show you how your clusters are forming

00:08:36,669 --> 00:08:40,840
it like higher levels here however

00:08:38,860 --> 00:08:43,720
static heat maps are limiting because as

00:08:40,840 --> 00:08:44,920
you add more data your components of

00:08:43,720 --> 00:08:46,360
your visualization just become a lot

00:08:44,920 --> 00:08:49,540
smaller and it's a lot harder to gather

00:08:46,360 --> 00:08:51,520
anything useful from it so one of the

00:08:49,540 --> 00:08:52,990
goals for the cluster grammar project

00:08:51,520 --> 00:08:54,700
was to extend the capabilities of a heat

00:08:52,990 --> 00:08:56,800
map and enable complex

00:08:54,700 --> 00:08:58,240
an interactive data exploration make

00:08:56,800 --> 00:08:59,770
something that was user-friendly and

00:08:58,240 --> 00:09:02,050
very easy to share without software

00:08:59,770 --> 00:09:05,650
requirement and also included in by all

00:09:02,050 --> 00:09:08,140
biology specific features so cluster

00:09:05,650 --> 00:09:09,970
grammar as a visualization tool is a

00:09:08,140 --> 00:09:12,160
JavaScript tool and it was built with

00:09:09,970 --> 00:09:13,620
d3.js which is a data-driven documents

00:09:12,160 --> 00:09:15,520
it's a very popular and very powerful

00:09:13,620 --> 00:09:18,820
library for building customized

00:09:15,520 --> 00:09:20,080
visualizations and then the backend of

00:09:18,820 --> 00:09:22,450
cluster grammars where the calculations

00:09:20,080 --> 00:09:25,240
are done we're primarily calculations

00:09:22,450 --> 00:09:26,920
are done was built using Python and we

00:09:25,240 --> 00:09:29,110
chose Python because they have really

00:09:26,920 --> 00:09:31,030
extensive libraries for numerical and

00:09:29,110 --> 00:09:32,290
scientific purposes and then it's also

00:09:31,030 --> 00:09:34,480
general enough so you can bill like a

00:09:32,290 --> 00:09:38,050
web app and obviously real jupiter

00:09:34,480 --> 00:09:39,940
notebooks in this kind of thing so I'll

00:09:38,050 --> 00:09:43,150
just quickly show an example heat map

00:09:39,940 --> 00:09:44,320
here and so this is our interactive heat

00:09:43,150 --> 00:09:46,450
mat we built that we showed a little a

00:09:44,320 --> 00:09:47,920
little bit earlier think about so you

00:09:46,450 --> 00:09:50,710
can like zoom in and out of your data

00:09:47,920 --> 00:09:52,330
and what we have here are lung cancer

00:09:50,710 --> 00:09:54,580
cell lines it's just sort of a small toy

00:09:52,330 --> 00:09:56,050
data set and then we have the expression

00:09:54,580 --> 00:09:57,880
of different kinase 'iz in those cell

00:09:56,050 --> 00:09:59,860
lines so we can hover over them and get

00:09:57,880 --> 00:10:02,320
information we're actually looking up

00:09:59,860 --> 00:10:04,330
the the refseq information about that

00:10:02,320 --> 00:10:05,890
gene which helps biologists to see their

00:10:04,330 --> 00:10:08,140
data and we're actually talking to an

00:10:05,890 --> 00:10:10,630
API here but in terms of the data

00:10:08,140 --> 00:10:12,670
visualization you can reorder your data

00:10:10,630 --> 00:10:14,620
so you can reorder your rows or columns

00:10:12,670 --> 00:10:16,360
based on summer variance to sort of see

00:10:14,620 --> 00:10:17,950
different structuring your data and if

00:10:16,360 --> 00:10:20,850
you have an in cluster view you can use

00:10:17,950 --> 00:10:23,380
an interactive dendrogram did shown as

00:10:20,850 --> 00:10:25,930
slices here so you can see different

00:10:23,380 --> 00:10:29,170
levels of clustering in your data and

00:10:25,930 --> 00:10:30,760
then you can also compare if you've

00:10:29,170 --> 00:10:32,500
brought in your own prior knowledge like

00:10:30,760 --> 00:10:33,550
different categories so in this case we

00:10:32,500 --> 00:10:35,620
have cell lines that have different

00:10:33,550 --> 00:10:36,760
categories so if we see if we want to

00:10:35,620 --> 00:10:38,980
compare our prior knowledge to our

00:10:36,760 --> 00:10:41,800
data-driven clustering we can use the

00:10:38,980 --> 00:10:43,420
interactive dendrogram to show you the

00:10:41,800 --> 00:10:45,190
composition of a cluster based on

00:10:43,420 --> 00:10:46,330
certain categories and the fisher exact

00:10:45,190 --> 00:10:48,130
test will tell you the likelihood that

00:10:46,330 --> 00:10:50,620
that would have happened by chance so a

00:10:48,130 --> 00:10:51,940
lot of features were just built to be

00:10:50,620 --> 00:10:53,640
very generalizable for just data

00:10:51,940 --> 00:10:56,110
analysis

00:10:53,640 --> 00:10:57,640
so that's cluster grammar and we

00:10:56,110 --> 00:10:59,260
actually built a small app for a cluster

00:10:57,640 --> 00:11:02,170
grammar where you can just upload your

00:10:59,260 --> 00:11:03,280
data and again just get a permanent

00:11:02,170 --> 00:11:06,940
visualization that you can share with

00:11:03,280 --> 00:11:08,440
people but now on to Jupiter notebook so

00:11:06,940 --> 00:11:10,060
one of our goals was to get

00:11:08,440 --> 00:11:11,649
cluster grammar to run in a Jupiter

00:11:10,060 --> 00:11:13,690
notebook and as I'm sure you're all

00:11:11,649 --> 00:11:15,160
aware Jupiter notebooks are really ideal

00:11:13,690 --> 00:11:17,319
for generating reproducible and

00:11:15,160 --> 00:11:19,060
shareable workflows and they're kind of

00:11:17,319 --> 00:11:20,529
like hybrid web apps because you can run

00:11:19,060 --> 00:11:22,389
them locally you can share them with

00:11:20,529 --> 00:11:23,740
people on the web and they can be run on

00:11:22,389 --> 00:11:28,689
the web through services like binder or

00:11:23,740 --> 00:11:31,209
another Jupiter hub so we using the

00:11:28,689 --> 00:11:32,589
cookie cutter template the Jupiter

00:11:31,209 --> 00:11:35,410
widget cookiecutter template we built

00:11:32,589 --> 00:11:37,810
cluster grammar widget and we would it

00:11:35,410 --> 00:11:39,639
can be thought of from the Python point

00:11:37,810 --> 00:11:40,990
of view is just a general or pandas

00:11:39,639 --> 00:11:45,329
point of view as a general data frame

00:11:40,990 --> 00:11:48,339
viewer so let me see and then one of the

00:11:45,329 --> 00:11:50,470
about last year they officially added

00:11:48,339 --> 00:11:52,420
support for sharing notebooks with

00:11:50,470 --> 00:11:54,699
interactive widgets so now you that

00:11:52,420 --> 00:11:56,319
means you can have a static notebook a

00:11:54,699 --> 00:11:58,029
static HTML file with an embedded widget

00:11:56,319 --> 00:11:59,920
embedded data and you can put that

00:11:58,029 --> 00:12:01,810
online and share that with people you

00:11:59,920 --> 00:12:03,069
can even email them an HTML file and be

00:12:01,810 --> 00:12:03,569
able to interact with it which I'll show

00:12:03,069 --> 00:12:07,360
later

00:12:03,569 --> 00:12:11,230
so um okay well actually we'll do that

00:12:07,360 --> 00:12:13,750
now okay so here is an example of

00:12:11,230 --> 00:12:15,910
cluster grammar being used on view

00:12:13,750 --> 00:12:17,680
through NB viewer so we have since it's

00:12:15,910 --> 00:12:19,389
a JavaScript library we have all the

00:12:17,680 --> 00:12:21,670
same interaction interactive features we

00:12:19,389 --> 00:12:23,740
showed before it's just that now it's

00:12:21,670 --> 00:12:25,779
rendered in a cell and a jupiter

00:12:23,740 --> 00:12:27,910
notebook but this notebooks actually not

00:12:25,779 --> 00:12:29,380
executable and only a subset of the

00:12:27,910 --> 00:12:32,319
calculations can be done here so here we

00:12:29,380 --> 00:12:34,240
reviewing a a matrix with just some

00:12:32,319 --> 00:12:35,620
random data in it and then here's what

00:12:34,240 --> 00:12:38,110
it looks like if you actually run the

00:12:35,620 --> 00:12:40,540
notebook the same notebook that we saw

00:12:38,110 --> 00:12:43,449
earlier so here we're loading a file

00:12:40,540 --> 00:12:44,230
TSV file with data visualizing it and

00:12:43,449 --> 00:12:51,089
then below

00:12:44,230 --> 00:12:51,089
we're just generating a data frame oops

00:12:51,780 --> 00:12:55,930
so it's just a data frame with just

00:12:53,980 --> 00:12:57,460
random numbers and we're visualizing

00:12:55,930 --> 00:13:08,050
that and then we have certain methods we

00:12:57,460 --> 00:13:10,180
can use like we can try so we just

00:13:08,050 --> 00:13:11,740
z-score the columns just for fun with

00:13:10,180 --> 00:13:13,450
this random data and then it changes the

00:13:11,740 --> 00:13:14,950
way our data looks and we can do that so

00:13:13,450 --> 00:13:16,600
we can basically save this and send it

00:13:14,950 --> 00:13:18,100
to a collaborator and they don't

00:13:16,600 --> 00:13:20,860
actually have to even run the notebook

00:13:18,100 --> 00:13:24,130
but they can interact with the data okay

00:13:20,860 --> 00:13:26,320
so now we're at will go onto some case

00:13:24,130 --> 00:13:27,280
studies to kind of get into a little bit

00:13:26,320 --> 00:13:28,690
more of the weeds of dealing with real

00:13:27,280 --> 00:13:31,210
data and also discuss the future

00:13:28,690 --> 00:13:33,160
directions about the project so we'll

00:13:31,210 --> 00:13:34,930
start with the iris flower dataset then

00:13:33,160 --> 00:13:36,580
move on it's sort of small data that you

00:13:34,930 --> 00:13:38,070
might be familiar with and then move on

00:13:36,580 --> 00:13:41,410
to the cancer cell line encyclopedia

00:13:38,070 --> 00:13:43,120
gene bulk gene expression data which is

00:13:41,410 --> 00:13:44,950
the public data set produced by the

00:13:43,120 --> 00:13:47,050
brownness with you and then finally we

00:13:44,950 --> 00:13:48,610
will show some unpublished single-cell

00:13:47,050 --> 00:13:52,360
protein and gene expression data from an

00:13:48,610 --> 00:13:56,200
assay called site seek okay so the iris

00:13:52,360 --> 00:13:58,060
flower data say it's a small data set in

00:13:56,200 --> 00:14:01,240
a sense because it only has 150 samples

00:13:58,060 --> 00:14:03,220
150 flowers and it's a 4 dimensional so

00:14:01,240 --> 00:14:05,560
it's a multivariate data set and there

00:14:03,220 --> 00:14:07,720
are three flower types it's Atossa

00:14:05,560 --> 00:14:09,400
versicolor and virginica and there's

00:14:07,720 --> 00:14:12,190
four dimensions so you have the sepal

00:14:09,400 --> 00:14:14,260
and petal length and width right so one

00:14:12,190 --> 00:14:15,970
way you can visualize that data is in

00:14:14,260 --> 00:14:18,280
two dimensions at a time is through the

00:14:15,970 --> 00:14:20,110
use of scatter plots so this is just an

00:14:18,280 --> 00:14:21,730
example visualizing that data where the

00:14:20,110 --> 00:14:23,500
flowers have been colored based on their

00:14:21,730 --> 00:14:24,640
category and we're visualizing two

00:14:23,500 --> 00:14:27,130
dimensions at a time so like sepal

00:14:24,640 --> 00:14:29,380
length versus sepal width and etc so you

00:14:27,130 --> 00:14:32,140
have with these four dimensions you need

00:14:29,380 --> 00:14:35,650
six scatter plots to visualize that data

00:14:32,140 --> 00:14:37,540
and and you can see like that here in

00:14:35,650 --> 00:14:39,730
blue like virginica tends to have like

00:14:37,540 --> 00:14:41,250
higher petal length and you know you can

00:14:39,730 --> 00:14:44,020
sort of see the structure in your data

00:14:41,250 --> 00:14:46,270
but you can also visualize the data as a

00:14:44,020 --> 00:14:47,470
heat map in cluster grammar so here

00:14:46,270 --> 00:14:51,370
we're gonna show how we're doing that

00:14:47,470 --> 00:14:53,589
and this is again on NB viewer so so

00:14:51,370 --> 00:14:55,720
here we are loading the iris dataset

00:14:53,589 --> 00:14:57,400
we're just adding some categories to it

00:14:55,720 --> 00:14:59,080
just getting in the format the cluster

00:14:57,400 --> 00:15:00,310
grammar needs a little bit which is in

00:14:59,080 --> 00:15:02,770
the documentation if you're interested

00:15:00,310 --> 00:15:05,170
and then now we are visualizing that

00:15:02,770 --> 00:15:07,630
this data set as flowers is column

00:15:05,170 --> 00:15:09,660
here and then we've added a category for

00:15:07,630 --> 00:15:12,310
the three color types so we gots atossa

00:15:09,660 --> 00:15:16,120
you can also reorder by category so you

00:15:12,310 --> 00:15:18,820
got virginica versicolor and so Tosa and

00:15:16,120 --> 00:15:20,110
then we have the four dimensions here so

00:15:18,820 --> 00:15:22,329
one thing that becomes like immediately

00:15:20,110 --> 00:15:23,889
apparent is is that these three

00:15:22,329 --> 00:15:25,600
dimensions are like very redundant with

00:15:23,889 --> 00:15:26,980
each other also the data is had each

00:15:25,600 --> 00:15:28,720
dimension has been Z squared across all

00:15:26,980 --> 00:15:30,250
the flowers just to kind of make them a

00:15:28,720 --> 00:15:31,990
little bit more comparable so you can

00:15:30,250 --> 00:15:33,730
see suppli with is is doing its own

00:15:31,990 --> 00:15:35,709
thing but sepal length is actually

00:15:33,730 --> 00:15:37,120
behaves very similar to petal length and

00:15:35,709 --> 00:15:40,269
pedal with so that's something that just

00:15:37,120 --> 00:15:42,190
really pops out at you and then what you

00:15:40,269 --> 00:15:43,660
see here is that this cluster here is

00:15:42,190 --> 00:15:46,660
like almost completely composed of two

00:15:43,660 --> 00:15:49,570
Tosa and and then you see this cluster

00:15:46,660 --> 00:15:50,980
here at the at the dendrogram you can

00:15:49,570 --> 00:15:52,149
find like different levels of clustering

00:15:50,980 --> 00:15:54,310
so you can break it down to smaller

00:15:52,149 --> 00:15:56,649
clusters or larger clusters and then if

00:15:54,310 --> 00:15:59,019
you hover over the dendrogram like you

00:15:56,649 --> 00:16:01,180
intuitively it looks like there's

00:15:59,019 --> 00:16:03,550
definitely a significant enrichment for

00:16:01,180 --> 00:16:07,120
that category type in that cluster and

00:16:03,550 --> 00:16:08,440
this will on-the-fly perform the exact

00:16:07,120 --> 00:16:10,329
test to tell you like one of the odds if

00:16:08,440 --> 00:16:11,980
you were to have chosen 49 of them that

00:16:10,329 --> 00:16:13,480
they would have all been so Tosa the

00:16:11,980 --> 00:16:16,390
odds with a p-value don a dislike

00:16:13,480 --> 00:16:18,160
negative 24 so you can find immediately

00:16:16,390 --> 00:16:21,100
compare your prior knowledge to your

00:16:18,160 --> 00:16:22,540
data-driven clustering and then you can

00:16:21,100 --> 00:16:24,310
like reorder on a particular thing and

00:16:22,540 --> 00:16:25,930
say like okay you know if i just ranked

00:16:24,310 --> 00:16:27,790
on sepal with how good of a job do i do

00:16:25,930 --> 00:16:28,779
at breaking down my categories so so it

00:16:27,790 --> 00:16:32,560
just allows you to interactively explore

00:16:28,779 --> 00:16:34,930
your data and it's pretty versatile so

00:16:32,560 --> 00:16:37,750
okay so the next example we'll go over

00:16:34,930 --> 00:16:40,510
is some bulk gene expression data from

00:16:37,750 --> 00:16:42,760
the salsa cell sorry cancer cell line

00:16:40,510 --> 00:16:44,680
encyclopedia from the Broad Institute so

00:16:42,760 --> 00:16:47,740
this is a much bigger dataset so it

00:16:44,680 --> 00:16:49,240
includes 1037 cancer cell lines it's

00:16:47,740 --> 00:16:52,149
this is just showing you a breakdown of

00:16:49,240 --> 00:16:53,260
the number of tissue types in those 1037

00:16:52,149 --> 00:16:54,670
cell lines so you've got 20 different

00:16:53,260 --> 00:16:57,279
tissue types it's mainly composed of

00:16:54,670 --> 00:16:58,420
lung I'm at a poetic lymphoid tissue and

00:16:57,279 --> 00:17:00,190
then you have like central nervous

00:16:58,420 --> 00:17:02,279
system skin large intestine so it's a

00:17:00,190 --> 00:17:03,730
good data set to explore cluster grammar

00:17:02,279 --> 00:17:06,339
let me see

00:17:03,730 --> 00:17:07,780
so we're just going to investigate a

00:17:06,339 --> 00:17:09,490
certain subset of that data so we're

00:17:07,780 --> 00:17:11,919
going to look at some gene expression

00:17:09,490 --> 00:17:13,030
this was gene expression data so we're

00:17:11,919 --> 00:17:15,520
going to look at some gene expression

00:17:13,030 --> 00:17:17,530
data in some bone cancers so I think we

00:17:15,520 --> 00:17:19,070
have it preloaded here yeah so again

00:17:17,530 --> 00:17:21,800
we're we're in MB viewer and we're

00:17:19,070 --> 00:17:24,380
we're basically we're loading the full

00:17:21,800 --> 00:17:27,950
data set so we have it has 18,000 rows

00:17:24,380 --> 00:17:29,480
and one thousand 37 columns and so

00:17:27,950 --> 00:17:31,460
because cluster grammar can't visualize

00:17:29,480 --> 00:17:34,340
that much data once we're just grabbing

00:17:31,460 --> 00:17:36,530
a subset of the data based on bone so

00:17:34,340 --> 00:17:37,850
we're grabbing a category here and then

00:17:36,530 --> 00:17:40,790
here we're filtering to only show the

00:17:37,850 --> 00:17:42,770
top 250 variable genes so now if you

00:17:40,790 --> 00:17:44,390
look at this the top category bone all

00:17:42,770 --> 00:17:46,370
of the all of our cell lines here as

00:17:44,390 --> 00:17:47,930
columns our bone and then we have the

00:17:46,370 --> 00:17:50,420
top 250

00:17:47,930 --> 00:17:51,890
most the genes with the highest variance

00:17:50,420 --> 00:17:54,110
across all these cell lines so this is

00:17:51,890 --> 00:17:56,510
helping us see differences between our

00:17:54,110 --> 00:17:58,280
bone cancer cell lines so we don't just

00:17:56,510 --> 00:18:02,210
have the tissue we're also given like

00:17:58,280 --> 00:18:03,740
the histology sub histology and actually

00:18:02,210 --> 00:18:05,300
the gender and this kind of thing so one

00:18:03,740 --> 00:18:07,070
thing we can see right off the bat is

00:18:05,300 --> 00:18:08,660
that you have this cluster here of cell

00:18:07,070 --> 00:18:10,040
lines that behaves like very different

00:18:08,660 --> 00:18:11,810
than the rest of bone cancer cell lines

00:18:10,040 --> 00:18:13,750
and if you look at the category it's

00:18:11,810 --> 00:18:15,470
mostly composed of Ewing sarcoma

00:18:13,750 --> 00:18:17,780
peripheral primitive neuroectodermal

00:18:15,470 --> 00:18:19,820
tumour but the takeaway is that it's a

00:18:17,780 --> 00:18:22,670
neuroectodermal tumor and if you look at

00:18:19,820 --> 00:18:24,350
the other but it's not completely like

00:18:22,670 --> 00:18:26,440
not all of them fall in that cluster

00:18:24,350 --> 00:18:29,060
there's two other cell lines here that

00:18:26,440 --> 00:18:31,580
cluster with the other histology types

00:18:29,060 --> 00:18:33,380
so the other breakdowns are just

00:18:31,580 --> 00:18:35,660
different different breakdown different

00:18:33,380 --> 00:18:40,070
subtypes of bone cancer and what we've

00:18:35,660 --> 00:18:43,940
done here is overlay prior knowledge

00:18:40,070 --> 00:18:46,220
using a Web API call or a web tool built

00:18:43,940 --> 00:18:48,650
by the Mayan lab call enricher so like

00:18:46,220 --> 00:18:51,350
the column categories we brought in that

00:18:48,650 --> 00:18:52,700
information right but since everybody in

00:18:51,350 --> 00:18:56,090
biology is pretty much dealing with

00:18:52,700 --> 00:18:58,280
genes we have a enormous amount of prior

00:18:56,090 --> 00:18:59,330
knowledge that we can bring in and and

00:18:58,280 --> 00:19:01,640
you can either bring in that information

00:18:59,330 --> 00:19:02,750
or we have an API that allows you to

00:19:01,640 --> 00:19:05,000
bring in that information through a

00:19:02,750 --> 00:19:06,710
process called enrichment analysis so

00:19:05,000 --> 00:19:08,510
what we did here was actually preload

00:19:06,710 --> 00:19:11,540
the heat map with enrichment analysis

00:19:08,510 --> 00:19:12,980
for gene ontology biological process so

00:19:11,540 --> 00:19:14,840
enrichment analysis is a method where

00:19:12,980 --> 00:19:17,120
you have prior knowledge list of genes

00:19:14,840 --> 00:19:19,490
and you basically ask the question I

00:19:17,120 --> 00:19:22,370
have a list of genes over here is this

00:19:19,490 --> 00:19:24,830
list enriched for some some process or

00:19:22,370 --> 00:19:25,790
category some other list like and in one

00:19:24,830 --> 00:19:26,240
of the odds that that would happen by

00:19:25,790 --> 00:19:28,850
chance

00:19:26,240 --> 00:19:30,890
so so what we can see here is the four

00:19:28,850 --> 00:19:34,250
broad biological processes we have

00:19:30,890 --> 00:19:36,340
enrichment for extracellular matrix or

00:19:34,250 --> 00:19:39,140
extra cellular structure organization

00:19:36,340 --> 00:19:40,700
matrix this assembly collagen furball

00:19:39,140 --> 00:19:43,070
organization so these are things from a

00:19:40,700 --> 00:19:44,570
biological perspective that sort of

00:19:43,070 --> 00:19:45,740
makes sense because it's bone cancer

00:19:44,570 --> 00:19:47,720
you're looking at and this is

00:19:45,740 --> 00:19:50,810
variability across bone cancer but

00:19:47,720 --> 00:19:54,440
importantly we can see that here like

00:19:50,810 --> 00:19:57,050
the actual genes that are they fall into

00:19:54,440 --> 00:20:00,050
these categories they're all located in

00:19:57,050 --> 00:20:02,810
this bottom cluster here right so this

00:20:00,050 --> 00:20:04,840
this lower cluster here are all the

00:20:02,810 --> 00:20:06,800
enriched terms are for the most part not

00:20:04,840 --> 00:20:07,730
involved in with these genes here

00:20:06,800 --> 00:20:09,020
they're only involved with the ones on

00:20:07,730 --> 00:20:11,600
the bottom right so you kind of have to

00:20:09,020 --> 00:20:13,100
separate things going on so what I'll

00:20:11,600 --> 00:20:15,410
quickly demonstrate is how we can use

00:20:13,100 --> 00:20:17,660
this crop button here to investigate a

00:20:15,410 --> 00:20:19,670
sub cluster of genes there in this in

00:20:17,660 --> 00:20:22,430
this cluster of interests so now we've

00:20:19,670 --> 00:20:24,890
cropped the matrix on the front end and

00:20:22,430 --> 00:20:27,650
we're only looking at the the genes in

00:20:24,890 --> 00:20:30,020
the cluster that is mostly enriched for

00:20:27,650 --> 00:20:34,420
this neuroectodermal tumor so we can

00:20:30,020 --> 00:20:36,680
rerun the enrichment analysis and for

00:20:34,420 --> 00:20:38,360
biological process and this is actually

00:20:36,680 --> 00:20:41,060
sending this arbitrary list of genes

00:20:38,360 --> 00:20:42,920
that we found here it's talking to

00:20:41,060 --> 00:20:45,770
choosing cross origin request to just

00:20:42,920 --> 00:20:47,450
send that to a web api and then now it's

00:20:45,770 --> 00:20:49,820
getting back the enrichment analysis

00:20:47,450 --> 00:20:52,160
information so so you could basically

00:20:49,820 --> 00:20:54,320
like crop an arbitrary list of genes and

00:20:52,160 --> 00:20:55,970
do enrichment analysis on it so what we

00:20:54,320 --> 00:20:59,330
see here the new terms that are enriched

00:20:55,970 --> 00:21:03,380
our cerebral cortex migration neural

00:20:59,330 --> 00:21:07,100
migration behavior and then like leal

00:21:03,380 --> 00:21:08,690
cell so these are more neuronal related

00:21:07,100 --> 00:21:10,250
terms like central nervous system this

00:21:08,690 --> 00:21:11,810
kind of thing so it matches the

00:21:10,250 --> 00:21:13,640
histology of the primitive

00:21:11,810 --> 00:21:14,270
neuroectodermal tumor so if we didn't

00:21:13,640 --> 00:21:16,580
know these were primitive

00:21:14,270 --> 00:21:18,410
neuroectodermal to nerve tumors we would

00:21:16,580 --> 00:21:20,690
be able to get some like inkling or some

00:21:18,410 --> 00:21:22,850
some hint of the what's actually

00:21:20,690 --> 00:21:26,110
happening in those cell lines by using a

00:21:22,850 --> 00:21:32,000
richer analysis on the gene level data

00:21:26,110 --> 00:21:35,000
okay so yeah the this more or less goes

00:21:32,000 --> 00:21:38,300
over what we discussed okay so now I'll

00:21:35,000 --> 00:21:41,270
move on to the classic example data so

00:21:38,300 --> 00:21:44,360
we have so the I'll be discussing some

00:21:41,270 --> 00:21:46,190
data from a site seek experiment so site

00:21:44,360 --> 00:21:47,929
seek stands for cellular indexing

00:21:46,190 --> 00:21:50,450
transcriptomes and epitopes by

00:21:47,929 --> 00:21:52,460
sequencing and it's an assay there was

00:21:50,450 --> 00:21:54,350
the technique that was developed in New

00:21:52,460 --> 00:21:56,480
York at the New York genome Center and

00:21:54,350 --> 00:22:00,740
recently published in Nature Methods in

00:21:56,480 --> 00:22:01,820
2017 down here so basically what we were

00:22:00,740 --> 00:22:03,440
looking at before was bulk gene

00:22:01,820 --> 00:22:06,169
expression data meaning that you were

00:22:03,440 --> 00:22:08,570
measuring gene expression levels of like

00:22:06,169 --> 00:22:10,220
you know millions of cells so newer

00:22:08,570 --> 00:22:12,230
techniques are coming out where you

00:22:10,220 --> 00:22:15,049
using microfluidics and droplets and

00:22:12,230 --> 00:22:16,370
beads with biochemical enzymes that

00:22:15,049 --> 00:22:18,320
basically allow you to measure gene

00:22:16,370 --> 00:22:20,029
expression in on individual cells so

00:22:18,320 --> 00:22:21,860
here like you're seeing you've got cells

00:22:20,029 --> 00:22:24,230
and beads coming together forming tiny

00:22:21,860 --> 00:22:26,570
droplets of water that are suspended in

00:22:24,230 --> 00:22:29,870
oil and you can perform all the all the

00:22:26,570 --> 00:22:31,429
biochemistry necessary to amplify their

00:22:29,870 --> 00:22:33,409
gene expression but for our purposes

00:22:31,429 --> 00:22:35,389
while we're gonna we're gonna say is

00:22:33,409 --> 00:22:38,799
that this method allows us to measure

00:22:35,389 --> 00:22:41,419
about 30,000 genes about 50 proteins and

00:22:38,799 --> 00:22:42,980
and do that for about 10,000 single

00:22:41,419 --> 00:22:44,629
cells so it's a pretty big data set

00:22:42,980 --> 00:22:46,220
pretty complicated and the reason we're

00:22:44,629 --> 00:22:50,950
measuring proteins and genes separately

00:22:46,220 --> 00:22:53,419
is because this we're actually able to

00:22:50,950 --> 00:22:54,799
relatively well characterize cell types

00:22:53,419 --> 00:22:56,720
based on their surface markers so there

00:22:54,799 --> 00:22:58,429
so we're kind of focusing in you can

00:22:56,720 --> 00:23:00,350
think of the surface marker protein data

00:22:58,429 --> 00:23:02,840
as a means to identify well-known

00:23:00,350 --> 00:23:04,820
subtypes of cells in this case immune

00:23:02,840 --> 00:23:06,139
cells whereas the genes offer a more

00:23:04,820 --> 00:23:07,309
unbiased view of what's happening

00:23:06,139 --> 00:23:09,679
because you're just looking at basically

00:23:07,309 --> 00:23:12,649
everything and then we can compare that

00:23:09,679 --> 00:23:14,779
so for this to analyze this data we need

00:23:12,649 --> 00:23:16,279
a more powerful visualization tool than

00:23:14,779 --> 00:23:18,830
what we currently have and this is where

00:23:16,279 --> 00:23:21,139
wedgy all comes in so what we've been

00:23:18,830 --> 00:23:23,960
working on recently is rebuilding the

00:23:21,139 --> 00:23:25,669
front end of cluster grammar in WebGL

00:23:23,960 --> 00:23:28,340
and we're using a library called regal

00:23:25,669 --> 00:23:30,019
so and the new widget I'm calling is a

00:23:28,340 --> 00:23:33,320
cluster grammar collegiate as in like

00:23:30,019 --> 00:23:36,139
WebGL and widget inclusion so WebGL

00:23:33,320 --> 00:23:38,450
enables GPU accelerated visualization of

00:23:36,139 --> 00:23:41,870
large a large data sets and you'll

00:23:38,450 --> 00:23:44,600
you'll see here is just the Regal

00:23:41,870 --> 00:23:46,909
website so a lot of WebGL stuff is 3d

00:23:44,600 --> 00:23:50,929
but what we're doing is 2d but using the

00:23:46,909 --> 00:23:52,700
same technology okay so now just to give

00:23:50,929 --> 00:23:56,960
a quick example on the old data set we

00:23:52,700 --> 00:23:59,059
had before this was the the the speed at

00:23:56,960 --> 00:23:59,990
which we're dealing with a 250 by 250

00:23:59,059 --> 00:24:02,410
matrix

00:23:59,990 --> 00:24:05,270
but now with the new WebGL

00:24:02,410 --> 00:24:07,429
implementation we scroll down let's see

00:24:05,270 --> 00:24:11,390
so now this is a thousand by a thousand

00:24:07,429 --> 00:24:13,760
and we can zoom in like much faster and

00:24:11,390 --> 00:24:17,990
interact with it much more easily and

00:24:13,760 --> 00:24:19,670
then let's see if this works we can also

00:24:17,990 --> 00:24:23,000
like reorder things and handle like tons

00:24:19,670 --> 00:24:26,150
of data so this is just giving you an

00:24:23,000 --> 00:24:28,340
idea of like how much more powerful

00:24:26,150 --> 00:24:29,570
WebGL is than the SVG implementation so

00:24:28,340 --> 00:24:32,960
allows us to work with much much bigger

00:24:29,570 --> 00:24:35,510
data sets okay so now I will jump back

00:24:32,960 --> 00:24:36,620
to the slide seek data so so dealing

00:24:35,510 --> 00:24:39,140
with this thing I'll sell data really

00:24:36,620 --> 00:24:41,120
has driven us to develop more powerful

00:24:39,140 --> 00:24:42,110
visualization tools and the specific

00:24:41,120 --> 00:24:44,030
data set that I'll be sharing with you

00:24:42,110 --> 00:24:46,010
here is a data set from peripheral blood

00:24:44,030 --> 00:24:49,790
mononuclear cells so these are different

00:24:46,010 --> 00:24:52,040
immunological cells from from a human

00:24:49,790 --> 00:24:55,190
sample so in this particular experiment

00:24:52,040 --> 00:24:58,010
we used site seek to measure 37 service

00:24:55,190 --> 00:25:02,360
markers about 30,000 genes and about

00:24:58,010 --> 00:25:03,950
over 15,000 cells were measured so let

00:25:02,360 --> 00:25:08,090
me see so now I'll show some of the

00:25:03,950 --> 00:25:10,040
results in some static HTML notebooks so

00:25:08,090 --> 00:25:13,250
here we're just loading the data we've

00:25:10,040 --> 00:25:15,740
got 37 markers by around 20,000 cells

00:25:13,250 --> 00:25:18,800
and then here we're visualizing a subset

00:25:15,740 --> 00:25:20,960
of these cells here as columns and we've

00:25:18,800 --> 00:25:23,809
got our 37 dimensions here so if you

00:25:20,960 --> 00:25:25,940
hover over you can it'll tell you the

00:25:23,809 --> 00:25:28,880
the cell type so we can what we've got

00:25:25,940 --> 00:25:30,050
here is unbiased clustering so we're

00:25:28,880 --> 00:25:32,480
doing a hierarchical clustering of our

00:25:30,050 --> 00:25:35,240
cells and our our markers and we could

00:25:32,480 --> 00:25:37,130
see that we the Celt cells tend to

00:25:35,240 --> 00:25:39,220
cluster based on cell type and this cell

00:25:37,130 --> 00:25:41,750
type here was determined using a semi

00:25:39,220 --> 00:25:43,880
manual method so basically we're seeing

00:25:41,750 --> 00:25:45,350
good agreement between our semi manual

00:25:43,880 --> 00:25:47,179
dimensionality reduction stell type

00:25:45,350 --> 00:25:49,190
identification and our just totally

00:25:47,179 --> 00:25:51,470
unbiased cell type identification and

00:25:49,190 --> 00:25:55,460
you can zoom in and see each cell

00:25:51,470 --> 00:25:57,429
actually is encoded by a barcode so if

00:25:55,460 --> 00:26:01,610
you zoom in sufficiently you'll see that

00:25:57,429 --> 00:26:04,220
DNA barcode so one thing we can see here

00:26:01,610 --> 00:26:06,050
like if you right now we're weird z

00:26:04,220 --> 00:26:07,010
scoring the data across the cell so it

00:26:06,050 --> 00:26:08,450
kind of makes it a little bit more

00:26:07,010 --> 00:26:10,490
apparent what's happening and you can

00:26:08,450 --> 00:26:12,080
see here you got like cd8 is high here

00:26:10,490 --> 00:26:13,400
so if you hover over you'll see that

00:26:12,080 --> 00:26:16,150
these are cd8 cells

00:26:13,400 --> 00:26:20,110
cd8 t-cells and they're in Brad here

00:26:16,150 --> 00:26:24,710
yeah and then and then you can like

00:26:20,110 --> 00:26:29,990
we've also labeled doublets using the

00:26:24,710 --> 00:26:31,490
black category here so we can we can see

00:26:29,990 --> 00:26:32,360
that doublets are clustering next to

00:26:31,490 --> 00:26:34,610
each other and they kind of have like

00:26:32,360 --> 00:26:37,640
too many markers that are active and

00:26:34,610 --> 00:26:40,970
then the next thing we can do is to take

00:26:37,640 --> 00:26:42,920
the so basically we've identified the

00:26:40,970 --> 00:26:44,690
cells based on their surface marker data

00:26:42,920 --> 00:26:47,120
but now we can look at the sort of more

00:26:44,690 --> 00:26:48,559
broad unbiased gene expression data so

00:26:47,120 --> 00:26:52,400
here we just took a very general method

00:26:48,559 --> 00:26:54,230
where we took a subsample of five

00:26:52,400 --> 00:26:55,940
thousand genes so this this matrix has

00:26:54,230 --> 00:26:57,920
five thousand columns and we're looking

00:26:55,940 --> 00:27:01,730
at the top 500 variable genes across

00:26:57,920 --> 00:27:03,920
those across these five thousand so now

00:27:01,730 --> 00:27:06,890
we can zoom in and see specifically like

00:27:03,920 --> 00:27:08,990
which genes are being upregulated in

00:27:06,890 --> 00:27:12,740
specific cell types so like if we zoom

00:27:08,990 --> 00:27:15,620
in here we've got cd16 monocytes and

00:27:12,740 --> 00:27:17,030
then we can zoom in and see the specific

00:27:15,620 --> 00:27:20,750
genes that are up and down regulated in

00:27:17,030 --> 00:27:22,880
in that cell type and then we can also

00:27:20,750 --> 00:27:24,590
go a little deeper and we can take a

00:27:22,880 --> 00:27:27,530
particular cell type of interests like

00:27:24,590 --> 00:27:29,510
cd8 cd8 t-cells and then we can cluster

00:27:27,530 --> 00:27:33,050
just those cells specifically and then

00:27:29,510 --> 00:27:35,900
now this this category here is labeling

00:27:33,050 --> 00:27:38,059
this subtypes of cd8 so what this is

00:27:35,900 --> 00:27:40,280
showing us is that there's very good

00:27:38,059 --> 00:27:42,050
agreement between the protein level data

00:27:40,280 --> 00:27:43,220
and the gene expression data even though

00:27:42,050 --> 00:27:46,880
they're pretty much independent data

00:27:43,220 --> 00:27:48,140
sets and it's it's getting all set up a

00:27:46,880 --> 00:27:50,090
lot of confidence in our assay and it's

00:27:48,140 --> 00:27:53,600
also enabling enabling us to identify

00:27:50,090 --> 00:27:54,679
new subtypes of cells and also to

00:27:53,600 --> 00:27:57,020
identify the differentially expressed

00:27:54,679 --> 00:28:00,080
genes within those cell types and then

00:27:57,020 --> 00:28:03,040
the visualization enables us to share to

00:28:00,080 --> 00:28:05,000
share the data with our biology

00:28:03,040 --> 00:28:06,170
collaborators and our researchers who

00:28:05,000 --> 00:28:07,730
aren't necessarily going to run a

00:28:06,170 --> 00:28:10,070
Jupiter notebook but now we can share a

00:28:07,730 --> 00:28:11,510
big dataset with them just as a private

00:28:10,070 --> 00:28:12,920
HTML file and they can interact with it

00:28:11,510 --> 00:28:14,360
and then once they're ready to publish

00:28:12,920 --> 00:28:16,160
we can put that online and we can share

00:28:14,360 --> 00:28:17,450
this through any viewer and then

00:28:16,160 --> 00:28:19,910
obviously this kind of thing could also

00:28:17,450 --> 00:28:25,480
be used through services like my binder

00:28:19,910 --> 00:28:27,290
and in the future so yeah that's about

00:28:25,480 --> 00:28:28,820
it so there's a

00:28:27,290 --> 00:28:30,440
a couple links to some other examples we

00:28:28,820 --> 00:28:32,300
have an example we're using the the

00:28:30,440 --> 00:28:35,240
eminence data set a kind of fun one

00:28:32,300 --> 00:28:36,830
using USDA nutrients data and then we

00:28:35,240 --> 00:28:39,410
already went over the the ccle

00:28:36,830 --> 00:28:42,080
collegiate example so the project itself

00:28:39,410 --> 00:28:43,610
is is open source so that the JavaScript

00:28:42,080 --> 00:28:45,260
libraries or cluster grammar Jas or

00:28:43,610 --> 00:28:47,600
cluster grammar GL for the WebGL and

00:28:45,260 --> 00:28:49,100
then clustering our py is the back end

00:28:47,600 --> 00:28:53,720
and then we have the widget and the new

00:28:49,100 --> 00:28:56,360
WebGL glitchet and documentation is here

00:28:53,720 --> 00:28:58,970
on read the docs so it's pretty well

00:28:56,360 --> 00:29:00,500
documented and has everything about the

00:28:58,970 --> 00:29:02,060
format's and everything and it's PIP

00:29:00,500 --> 00:29:05,660
installable and then it was recently

00:29:02,060 --> 00:29:08,900
published in Nature scientific data last

00:29:05,660 --> 00:29:10,720
year and thank you for attention I would

00:29:08,900 --> 00:29:12,790
just like to acknowledge everyone at the

00:29:10,720 --> 00:29:16,310
human immune monitoring center

00:29:12,790 --> 00:29:18,650
especially Miriam Arad in deeps Angie

00:29:16,310 --> 00:29:22,010
and Sasha and a computational team there

00:29:18,650 --> 00:29:23,960
Melanie and Manuel everybody's very good

00:29:22,010 --> 00:29:25,520
and high-quality there and then the the

00:29:23,960 --> 00:29:27,710
cluster grammar widget was developed

00:29:25,520 --> 00:29:29,660
during the time I was a postdoc in a BMI

00:29:27,710 --> 00:29:31,630
on slab so just acknowledge some of the

00:29:29,660 --> 00:29:33,140
people in obvious lab and our

00:29:31,630 --> 00:29:35,270
collaborated with cell signaling

00:29:33,140 --> 00:29:38,330
technology and then also my wife Don

00:29:35,270 --> 00:29:41,360
Fernandez and her PA she's a postdoc in

00:29:38,330 --> 00:29:43,610
the Johnny Rayleigh lab and and then

00:29:41,360 --> 00:29:44,570
just also I would really like to

00:29:43,610 --> 00:29:45,740
acknowledge all the open source

00:29:44,570 --> 00:29:47,330
developers that made all these tools

00:29:45,740 --> 00:29:48,290
possible because the project would just

00:29:47,330 --> 00:29:49,700
definitely not have been possible

00:29:48,290 --> 00:29:51,740
without all the open source tools that

00:29:49,700 --> 00:29:53,360
exist not to mention the help from

00:29:51,740 --> 00:29:56,210
everybody on message boards then they

00:29:53,360 --> 00:29:59,620
just like extremely helpful and thank

00:29:56,210 --> 00:29:59,620
you did you better comprehend a present

00:30:01,160 --> 00:30:04,319

YouTube URL: https://www.youtube.com/watch?v=82epZkmfkrE


