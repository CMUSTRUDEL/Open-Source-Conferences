Title: Reproducible data dependencies for Jupyter - Jackson Brown, Aneesh Karve
Publication date: 2018-09-21
Playlist: JupyterCon in New York 2018
Description: 
	Reproducible data dependencies for Jupyter: Distributing massive, versioned image datasets from the Allen Institute for Cell Science
Jackson Brown (Allen Institute for Cell Science), Aneesh Karve (Quilt)

Reproducible data is essential for notebooks that work across time, across contributors, and across machines. Jackson Brown and Aneesh Karve demonstrate how to use an open source data registry to create reproducible data dependencies for Jupyter and share a case study in open science over terabyte-size image datasets.

The Allen Institute for Cell Science generates terabytes of microscopy images every week. To improve access to these datasets for data scientists and external collaborators, the institute sought a platform that would enable plain-text search, subsetting of large datasets, version control to support reproducible experiments, and easy accessibility from data science tools like Jupyter, Python, and pandas. The team discovered that software optimized for storing and versioning source code (e.g., GitHub) exhibits slow performance for large files and places hard limits on file size that preclude large data repositories altogether. In response, the team is creating an open repository of image data that is enriched with metadata and encapsulated in “data packages”—versioned, immutable sets of data dependencies.

The concept of package management is well known in software development. To date, however, package management has largely been applied to source code. Jackson and Aneesh propose to extend package management to the unique file size and format challenges of data by building on top of Quilt, an open source data registry. In combination with custom filtering software, Quilt enables efficient search and query of metadata so that data scientists can filter terabyte-sized packages into megabyte-size subsets that fit on a single machine. The package management infrastructure optimizes not only storage and network transfer but also serialization and virtualization. As a result, data scientists can interact with data packages in formats that are native to Jupyter and Python. Jackson and Aneesh also explore the role of data packages in versioning models and detecting model drift using “data unit tests” that check data profiles.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,249 --> 00:00:05,210
we've just been introduced I'm an ish

00:00:03,230 --> 00:00:07,069
this is my colleague Jackson Browne who

00:00:05,210 --> 00:00:09,619
is a senior researcher at the Allen

00:00:07,069 --> 00:00:10,789
Institute for cell science and we're

00:00:09,619 --> 00:00:12,650
here to talk to you about lessons

00:00:10,789 --> 00:00:14,929
learned during a technology partnership

00:00:12,650 --> 00:00:17,510
over the past six months where we have

00:00:14,929 --> 00:00:19,130
distributed to tackle the problem of

00:00:17,510 --> 00:00:21,380
distributing terabytes of open images

00:00:19,130 --> 00:00:23,300
and doing it in a way that's it's

00:00:21,380 --> 00:00:25,580
accessible to any Jupiter notebook

00:00:23,300 --> 00:00:27,140
anywhere in the world and that given the

00:00:25,580 --> 00:00:28,610
same data and the same code that

00:00:27,140 --> 00:00:32,239
notebook will run the same time

00:00:28,610 --> 00:00:34,190
everywhere so you know this you love

00:00:32,239 --> 00:00:36,920
this but it should come with a warning

00:00:34,190 --> 00:00:40,339
sticker and that's notebooks tend to be

00:00:36,920 --> 00:00:42,440
fragile they break over time so when you

00:00:40,339 --> 00:00:44,359
share a notebook oops I forgot I have a

00:00:42,440 --> 00:00:46,309
one gigabyte CSV file on my machine that

00:00:44,359 --> 00:00:48,979
I did put github because doesn't fit on

00:00:46,309 --> 00:00:51,140
github notebooks break over time whoops

00:00:48,979 --> 00:00:53,420
somebody deleted this file on the

00:00:51,140 --> 00:00:54,679
network attacks storage drive and now I

00:00:53,420 --> 00:00:56,929
don't know how to run this notebook and

00:00:54,679 --> 00:00:58,519
they tend to break across machines oh

00:00:56,929 --> 00:01:02,569
you're on Windows you need a different

00:00:58,519 --> 00:01:03,920
deserialize ER than we used on unix the

00:01:02,569 --> 00:01:06,200
reason that this is happening is because

00:01:03,920 --> 00:01:08,330
as a community right now we're only

00:01:06,200 --> 00:01:11,000
standing on one of the two pillars that

00:01:08,330 --> 00:01:13,700
make notebook stick and we've done a lot

00:01:11,000 --> 00:01:15,350
to really make code version and

00:01:13,700 --> 00:01:17,320
immutable and there's more that we can

00:01:15,350 --> 00:01:19,610
do on the data side of the equation and

00:01:17,320 --> 00:01:21,620
what I would encourage everybody in this

00:01:19,610 --> 00:01:24,230
room to start doing is to think about

00:01:21,620 --> 00:01:26,600
your notebooks as pure functions of data

00:01:24,230 --> 00:01:29,210
and code and the importance of a pure

00:01:26,600 --> 00:01:31,010
function is that if the inputs or the

00:01:29,210 --> 00:01:33,260
parameters to that function do not

00:01:31,010 --> 00:01:35,360
change the output will never change

00:01:33,260 --> 00:01:37,340
so if pure function is a function that

00:01:35,360 --> 00:01:38,750
depends wholly and solely on its

00:01:37,340 --> 00:01:41,480
parameters and we'll look at this more

00:01:38,750 --> 00:01:43,580
in detail in just a moment as I

00:01:41,480 --> 00:01:45,530
mentioned earlier up until now code has

00:01:43,580 --> 00:01:48,470
gotten most of the attention you can

00:01:45,530 --> 00:01:51,350
version PIN to a particular module on pi

00:01:48,470 --> 00:01:53,960
PI you can get check out a particular

00:01:51,350 --> 00:01:56,420
state of the repository you can dock or

00:01:53,960 --> 00:01:58,130
run a particular container but on the

00:01:56,420 --> 00:02:00,800
data side of things it's still a little

00:01:58,130 --> 00:02:02,780
bit like the Wild West and one of our

00:02:00,800 --> 00:02:05,420
colleagues Pete worden from Google said

00:02:02,780 --> 00:02:07,460
this feels like it's so bad the state of

00:02:05,420 --> 00:02:08,989
native management is so bad it feels

00:02:07,460 --> 00:02:11,569
like we're coding without source control

00:02:08,989 --> 00:02:14,210
and so the question now is what can we

00:02:11,569 --> 00:02:17,240
do to bring code style management today

00:02:14,210 --> 00:02:19,520
all of you are familiar with docker with

00:02:17,240 --> 00:02:21,080
pip with github and if we can use these

00:02:19,520 --> 00:02:22,160
metaphors to manage data we get some

00:02:21,080 --> 00:02:23,000
interesting results and that's really

00:02:22,160 --> 00:02:24,740
what we're going to share with you

00:02:23,000 --> 00:02:26,660
everything that we're gonna show you

00:02:24,740 --> 00:02:29,480
today is open source you can see our

00:02:26,660 --> 00:02:31,400
github link at the top they're at a very

00:02:29,480 --> 00:02:33,830
high level we think about quilt as

00:02:31,400 --> 00:02:35,540
docker for data and the question we're

00:02:33,830 --> 00:02:37,250
answering is what happens when you bring

00:02:35,540 --> 00:02:40,070
code style management metaphors like

00:02:37,250 --> 00:02:40,760
namespaces like versions like tags like

00:02:40,070 --> 00:02:42,020
copy-on-write

00:02:40,760 --> 00:02:45,770
what happens when you bring those

00:02:42,020 --> 00:02:47,510
metaphors over to data so at a very high

00:02:45,770 --> 00:02:49,160
level the data packages that we're going

00:02:47,510 --> 00:02:52,310
to talk about today have a very simple

00:02:49,160 --> 00:02:55,100
lifecycle packages are built you then

00:02:52,310 --> 00:02:56,870
push them to a registry you pull them

00:02:55,100 --> 00:02:58,460
from a registry and then in order to

00:02:56,870 --> 00:03:03,110
consume or deserialize those packages

00:02:58,460 --> 00:03:05,540
you install them so in one line of code

00:03:03,110 --> 00:03:07,850
this is the change that we want to see

00:03:05,540 --> 00:03:10,010
in the world every notebook that you

00:03:07,850 --> 00:03:12,380
write starts with import pandas s.pd

00:03:10,010 --> 00:03:14,570
import numpy as NP or that at least

00:03:12,380 --> 00:03:16,580
that's really common and we want to add

00:03:14,570 --> 00:03:19,870
this ability for people to import their

00:03:16,580 --> 00:03:22,160
data dependencies and just start working

00:03:19,870 --> 00:03:24,380
so when we talk about data in the

00:03:22,160 --> 00:03:26,180
context of this talk the data management

00:03:24,380 --> 00:03:28,100
system that quilt is built on top of is

00:03:26,180 --> 00:03:30,740
completely schema-less which means it

00:03:28,100 --> 00:03:32,240
can take any type of data of any size so

00:03:30,740 --> 00:03:34,640
you can put structured data in it

00:03:32,240 --> 00:03:36,410
unstructured data like images semi

00:03:34,640 --> 00:03:39,080
structured data like JSON it will all

00:03:36,410 --> 00:03:40,610
work and it scales from sizes of 0

00:03:39,080 --> 00:03:43,510
kilobytes all the way up to about 2

00:03:40,610 --> 00:03:45,650
terabytes so format and size agnostic

00:03:43,510 --> 00:03:50,300
when we turn it over to Jackson for a

00:03:45,650 --> 00:03:53,420
moment so if anyone has ever worked in

00:03:50,300 --> 00:03:55,220
any scientific lab I'm sure this image

00:03:53,420 --> 00:03:56,450
somewhat looks familiar to you this is

00:03:55,220 --> 00:03:58,430
kind of what scientists are used to

00:03:56,450 --> 00:03:59,690
right this is just you start at your lab

00:03:58,430 --> 00:04:01,160
and they say and you say where can I

00:03:59,690 --> 00:04:02,540
find the data and they point you to a

00:04:01,160 --> 00:04:03,920
web page and they say just download it

00:04:02,540 --> 00:04:06,710
just just go ahead and download all the

00:04:03,920 --> 00:04:08,030
files but you don't know what's in these

00:04:06,710 --> 00:04:09,500
you don't know what the size they are

00:04:08,030 --> 00:04:10,760
you don't know how to deserialize it

00:04:09,500 --> 00:04:12,920
like you know if they're tars they're

00:04:10,760 --> 00:04:16,130
zips what's in these I have no idea and

00:04:12,920 --> 00:04:17,600
this is bad for numerous reasons but but

00:04:16,130 --> 00:04:19,970
the but the core one that you know I

00:04:17,600 --> 00:04:22,340
care about is really what is in that

00:04:19,970 --> 00:04:24,500
data so for us like what we're trying to

00:04:22,340 --> 00:04:25,630
move towards is saying here just give me

00:04:24,500 --> 00:04:27,520
the data and

00:04:25,630 --> 00:04:29,380
can view it on a catalogue and will go

00:04:27,520 --> 00:04:31,000
into this catalog later but but the nice

00:04:29,380 --> 00:04:33,580
thing about quilt is that it will pre

00:04:31,000 --> 00:04:35,680
process and populate that metadata for

00:04:33,580 --> 00:04:36,700
us so it will tell us you know has it

00:04:35,680 --> 00:04:39,220
been installed before who's been

00:04:36,700 --> 00:04:42,040
installing it what size is it what files

00:04:39,220 --> 00:04:46,210
are in there and also how can I DCI is

00:04:42,040 --> 00:04:50,170
that how can I use it when I first

00:04:46,210 --> 00:04:51,790
started Alan's suit I you know I'm I got

00:04:50,170 --> 00:04:53,320
in charge of trying to figure out

00:04:51,790 --> 00:04:56,100
methods to actually distribute both

00:04:53,320 --> 00:04:58,360
internally and externally our data and

00:04:56,100 --> 00:04:59,470
the pretty much the first day someone

00:04:58,360 --> 00:05:01,240
came up to me and said you know you have

00:04:59,470 --> 00:05:02,470
scientists that want to share nearly

00:05:01,240 --> 00:05:03,340
everything that comes to their computer

00:05:02,470 --> 00:05:05,710
and they don't how to do that that's

00:05:03,340 --> 00:05:06,970
that's your job now I'm sure many of you

00:05:05,710 --> 00:05:09,940
have also been in that situation where

00:05:06,970 --> 00:05:11,500
someone just says that's your job you

00:05:09,940 --> 00:05:12,940
you know you have to you're the one that

00:05:11,500 --> 00:05:15,910
needs to get our data out there in a

00:05:12,940 --> 00:05:18,340
nice usable way the other problem that

00:05:15,910 --> 00:05:19,720
they come to me often with is where the

00:05:18,340 --> 00:05:21,700
heck are my files and did anyone mess

00:05:19,720 --> 00:05:23,590
with them right like this is a big

00:05:21,700 --> 00:05:25,660
problem for us and specifically the way

00:05:23,590 --> 00:05:28,240
that some of our file formats some of

00:05:25,660 --> 00:05:30,700
our imaging formats if a person even

00:05:28,240 --> 00:05:33,430
opens that file the metadata changes so

00:05:30,700 --> 00:05:35,680
that's in our sense a new file and

00:05:33,430 --> 00:05:38,800
versioning that properly is a little bit

00:05:35,680 --> 00:05:40,750
difficult with standard systems so so

00:05:38,800 --> 00:05:43,180
why do we care about distribute our data

00:05:40,750 --> 00:05:45,310
in the first place well as you can see

00:05:43,180 --> 00:05:47,650
in the background it's it's very complex

00:05:45,310 --> 00:05:49,180
it's very well tagged this is actually a

00:05:47,650 --> 00:05:52,480
40 image so it's the standard three

00:05:49,180 --> 00:05:55,600
dimensions and also time and it's highly

00:05:52,480 --> 00:05:58,900
labeled tagged this is hours and hours

00:05:55,600 --> 00:06:01,390
of gene editors microscopists modelers

00:05:58,900 --> 00:06:02,590
and visual visualization experts going

00:06:01,390 --> 00:06:05,470
through and working together to make

00:06:02,590 --> 00:06:07,270
this data at a high quality so we want

00:06:05,470 --> 00:06:08,650
to share it it's highly useful also we

00:06:07,270 --> 00:06:11,470
want to share it because it's we have a

00:06:08,650 --> 00:06:13,300
ton of it that that single cell I'm

00:06:11,470 --> 00:06:17,320
gonna go back this single cell we have

00:06:13,300 --> 00:06:20,680
probably about 250,000 samples of a cell

00:06:17,320 --> 00:06:22,720
like this so we have 30 terabytes right

00:06:20,680 --> 00:06:24,340
now and it's constantly growing we just

00:06:22,720 --> 00:06:27,130
have a lot of high quality data so we

00:06:24,340 --> 00:06:28,690
need to find a system that will help us

00:06:27,130 --> 00:06:34,060
share this in an intuitive and

00:06:28,690 --> 00:06:35,890
understandable way okay so we're gonna

00:06:34,060 --> 00:06:37,870
organize this talk around code samples

00:06:35,890 --> 00:06:39,400
that you can bring back to the labs and

00:06:37,870 --> 00:06:41,259
teams that you're working on and hope

00:06:39,400 --> 00:06:42,580
we get immediate value out of and let's

00:06:41,259 --> 00:06:44,770
just get right started with that so

00:06:42,580 --> 00:06:46,509
really simple problem I've got data I

00:06:44,770 --> 00:06:47,800
produce the result on my machine and I

00:06:46,509 --> 00:06:49,840
want to share it with other people so

00:06:47,800 --> 00:06:52,060
this is the workflow that we have been

00:06:49,840 --> 00:06:54,009
using the very first line there is super

00:06:52,060 --> 00:06:56,020
simple I just have an umpire ray did

00:06:54,009 --> 00:06:58,090
this happens to be random data imagine

00:06:56,020 --> 00:06:59,740
this is actual structured data and then

00:06:58,090 --> 00:07:02,229
I'm going to do two things I'm gonna

00:06:59,740 --> 00:07:04,750
build that data image locally and you'll

00:07:02,229 --> 00:07:06,940
notice that this URL or this namespace

00:07:04,750 --> 00:07:09,759
is constructed from the package owner

00:07:06,940 --> 00:07:11,800
the package name and then a subfolder

00:07:09,759 --> 00:07:13,870
inside of that package and all I'm doing

00:07:11,800 --> 00:07:16,180
is I'm assigning the value of this numpy

00:07:13,870 --> 00:07:19,240
array to that namespace or that path in

00:07:16,180 --> 00:07:21,639
the package and when I push it it now is

00:07:19,240 --> 00:07:22,600
available to us on a remote registry and

00:07:21,639 --> 00:07:24,810
let me show you at a very high level

00:07:22,600 --> 00:07:28,300
what that looks like I'll blow this up

00:07:24,810 --> 00:07:30,430
so the package is private by default and

00:07:28,300 --> 00:07:32,169
you can start to see a little bit meta

00:07:30,430 --> 00:07:34,599
of metadata I'm going to jump to a more

00:07:32,169 --> 00:07:36,789
mature package this is from a ICS or the

00:07:34,599 --> 00:07:37,930
Allen Institute for cell science and the

00:07:36,789 --> 00:07:39,789
important thing here is that if you

00:07:37,930 --> 00:07:41,949
create a metadata system which forces

00:07:39,789 --> 00:07:43,930
the users to input the metadata they're

00:07:41,949 --> 00:07:45,849
just not going to use it so all this is

00:07:43,930 --> 00:07:47,830
implicit data profiling that's done by

00:07:45,849 --> 00:07:49,960
the system and you'll see here will show

00:07:47,830 --> 00:07:51,190
you package excuse me traffic to the

00:07:49,960 --> 00:07:52,389
packages so how many people are

00:07:51,190 --> 00:07:53,889
installing it how many people are

00:07:52,389 --> 00:07:56,740
viewing it you can see the latest

00:07:53,889 --> 00:07:59,320
revision this is the package top hash

00:07:56,740 --> 00:08:01,120
which uniquely identifies that

00:07:59,320 --> 00:08:03,460
particular version of a given package

00:08:01,120 --> 00:08:05,199
and these top hashes are immutable which

00:08:03,460 --> 00:08:06,880
means once you have a given top patch

00:08:05,199 --> 00:08:08,650
for a package it can never change and

00:08:06,880 --> 00:08:11,199
then you'll see a little bit of metadata

00:08:08,650 --> 00:08:13,389
on the size of the package and what

00:08:11,199 --> 00:08:15,280
types of files it contains and then an

00:08:13,389 --> 00:08:16,990
automatically generated directory

00:08:15,280 --> 00:08:22,030
structure of the package which is here

00:08:16,990 --> 00:08:23,979
under contents alright so that was me

00:08:22,030 --> 00:08:25,599
pushing the data to a remote registry

00:08:23,979 --> 00:08:27,970
and what does the consumption experience

00:08:25,599 --> 00:08:29,740
look like somebody who a colleague of

00:08:27,970 --> 00:08:31,630
mine let's say who wants to grab this

00:08:29,740 --> 00:08:34,510
andy array that i've just created they

00:08:31,630 --> 00:08:36,669
would do an install oops let's go back

00:08:34,510 --> 00:08:38,770
they would do an install they would

00:08:36,669 --> 00:08:41,740
import the data and boom out comes the

00:08:38,770 --> 00:08:43,839
data so notice they're at the very top I

00:08:41,740 --> 00:08:46,870
declare my data dependency on this

00:08:43,839 --> 00:08:49,120
package and then I type ext one with

00:08:46,870 --> 00:08:50,950
parentheses so notice normally when you

00:08:49,120 --> 00:08:52,660
deserialize an umpire array you have to

00:08:50,950 --> 00:08:54,579
do numpy dot read and sometimes

00:08:52,660 --> 00:08:56,440
and more convoluted than that but we

00:08:54,579 --> 00:08:58,209
have a dispatch table inside of quill

00:08:56,440 --> 00:08:59,649
which knows the object type and then

00:08:58,209 --> 00:09:01,959
introspectively finds the right

00:08:59,649 --> 00:09:03,639
deserialize ER for that data type so the

00:09:01,959 --> 00:09:05,980
key point is you don't spend a lot of

00:09:03,639 --> 00:09:07,779
time looking for files and transporting

00:09:05,980 --> 00:09:10,259
data dependencies you just import them

00:09:07,779 --> 00:09:14,769
into your notebook and start working

00:09:10,259 --> 00:09:17,470
data slicing as I said before we have

00:09:14,769 --> 00:09:19,029
about 30 terabytes we are still

00:09:17,470 --> 00:09:21,160
processing a lot of that but on quote

00:09:19,029 --> 00:09:23,110
servers we have three terabytes right

00:09:21,160 --> 00:09:25,750
now and so we need to talk about you

00:09:23,110 --> 00:09:28,000
know not many people have the space just

00:09:25,750 --> 00:09:29,410
readily available to download all three

00:09:28,000 --> 00:09:32,050
terabytes at a single time so how do we

00:09:29,410 --> 00:09:34,000
how can we subset larger datasets into

00:09:32,050 --> 00:09:36,610
smaller datasets that people can use in

00:09:34,000 --> 00:09:39,399
their own work so the first example that

00:09:36,610 --> 00:09:40,930
I'm going to show is just Allen soft

00:09:39,399 --> 00:09:42,970
science we produce a random sample every

00:09:40,930 --> 00:09:44,709
time we generate new larger versions of

00:09:42,970 --> 00:09:46,149
our packages that pretty much exactly

00:09:44,709 --> 00:09:48,759
says it takes a random sample of all the

00:09:46,149 --> 00:09:50,709
data and of that we have a subsection

00:09:48,759 --> 00:09:53,139
called cell segmentations or the actual

00:09:50,709 --> 00:09:55,269
namespaces called cells eggs and we can

00:09:53,139 --> 00:09:56,889
install that specific sub Paget sub

00:09:55,269 --> 00:09:58,689
package so if someone is truly only

00:09:56,889 --> 00:10:00,490
cares about the cell segmentations that

00:09:58,689 --> 00:10:02,170
is a very easy install command to do

00:10:00,490 --> 00:10:04,149
it's just look at the name space and

00:10:02,170 --> 00:10:06,610
then tag on an additional sub folder

00:10:04,149 --> 00:10:08,350
path to it and you get only that portion

00:10:06,610 --> 00:10:11,079
of the data and just like a nice just

00:10:08,350 --> 00:10:12,970
showed it's the same way of using that

00:10:11,079 --> 00:10:14,709
data as before you just say import

00:10:12,970 --> 00:10:16,600
random sample and then go to the

00:10:14,709 --> 00:10:18,130
namespace cells eggs and there's all

00:10:16,600 --> 00:10:20,470
your data for you and this is like a

00:10:18,130 --> 00:10:22,300
text this is text based file file

00:10:20,470 --> 00:10:25,360
management file navigation system at

00:10:22,300 --> 00:10:28,810
this point the other very important

00:10:25,360 --> 00:10:31,389
aspect for us in a scientific sense is

00:10:28,810 --> 00:10:33,160
publishing papers right so again if you

00:10:31,389 --> 00:10:36,399
ever worked in lab you want people to be

00:10:33,160 --> 00:10:38,199
able to reproduce your work so we you

00:10:36,399 --> 00:10:40,389
know as we develop our datasets and as

00:10:38,199 --> 00:10:41,920
we add more to them and maybe we make a

00:10:40,389 --> 00:10:42,459
training set and we say we need more

00:10:41,920 --> 00:10:44,350
negative

00:10:42,459 --> 00:10:47,019
we need more negative training examples

00:10:44,350 --> 00:10:48,939
we know positive training examples that

00:10:47,019 --> 00:10:51,250
data set is going to change but the

00:10:48,939 --> 00:10:52,959
namespace should really be the same so

00:10:51,250 --> 00:10:55,300
when we publish a paper it would be

00:10:52,959 --> 00:10:57,220
ideal to just say okay here's you know

00:10:55,300 --> 00:10:59,379
your lab and here's your paper as the as

00:10:57,220 --> 00:11:01,480
the project namespace but then when you

00:10:59,379 --> 00:11:03,970
publish paper you say here this is the

00:11:01,480 --> 00:11:06,250
hash to reproduce this work and it will

00:11:03,970 --> 00:11:06,460
install those specific files that were

00:11:06,250 --> 00:11:08,350
you

00:11:06,460 --> 00:11:11,140
used to create and train and actually

00:11:08,350 --> 00:11:15,760
test that model or that that result that

00:11:11,140 --> 00:11:18,220
you got from your paper alright so I'm

00:11:15,760 --> 00:11:20,500
going to go through an example of ad hoc

00:11:18,220 --> 00:11:22,120
slicing here and how many people are

00:11:20,500 --> 00:11:25,300
familiar with the Google open images

00:11:22,120 --> 00:11:27,040
dataset okay so open images is about

00:11:25,300 --> 00:11:28,780
nine million different images from the

00:11:27,040 --> 00:11:31,420
open Internet that are all tagged and

00:11:28,780 --> 00:11:33,100
annotated and it's a crying shame that

00:11:31,420 --> 00:11:35,170
nobody uses this data set and I think

00:11:33,100 --> 00:11:37,420
part of the reason is that it's just so

00:11:35,170 --> 00:11:39,010
large enough as to be intractable

00:11:37,420 --> 00:11:41,020
so I mentioned nine million images it's

00:11:39,010 --> 00:11:43,000
about 18 terabytes in size it doesn't

00:11:41,020 --> 00:11:43,420
even live in a single location in Google

00:11:43,000 --> 00:11:45,940
Cloud

00:11:43,420 --> 00:11:48,430
so the problem we'd like to show you how

00:11:45,940 --> 00:11:50,500
we've solved today is let's say I want

00:11:48,430 --> 00:11:52,750
just a handful of images with a specific

00:11:50,500 --> 00:11:54,370
tag those of you who follow Silicon

00:11:52,750 --> 00:11:56,920
Valley are familiar with the hot dog not

00:11:54,370 --> 00:11:58,540
hot dog problem so let's say that we

00:11:56,920 --> 00:12:00,430
want to grab images that are tagged with

00:11:58,540 --> 00:12:02,020
a certain type of food so I'm gonna

00:12:00,430 --> 00:12:04,330
actually grab images that are tagged

00:12:02,020 --> 00:12:07,150
pretzels okay and the key point here

00:12:04,330 --> 00:12:08,950
this is a standard sequel query but this

00:12:07,150 --> 00:12:09,580
sequel query is actually issued to a

00:12:08,950 --> 00:12:11,650
file

00:12:09,580 --> 00:12:14,560
there's no standing database so we use

00:12:11,650 --> 00:12:16,420
Presto's DB prèsto DB from facebook to

00:12:14,560 --> 00:12:18,700
actually query files and treat files as

00:12:16,420 --> 00:12:20,710
tables and it's a very very cost

00:12:18,700 --> 00:12:22,990
effective way of being able to slice

00:12:20,710 --> 00:12:24,280
large data sets that aren't practical

00:12:22,990 --> 00:12:26,380
for people to materialize on their

00:12:24,280 --> 00:12:28,180
machines so key point in this query here

00:12:26,380 --> 00:12:31,060
is that I'm addressing a particular file

00:12:28,180 --> 00:12:32,860
and I'm going to ask for a specific

00:12:31,060 --> 00:12:34,930
keyword in the description that is

00:12:32,860 --> 00:12:36,730
contains pretzel and you'll see that

00:12:34,930 --> 00:12:39,430
when I execute that query what I get

00:12:36,730 --> 00:12:41,080
back is a panda's data frame and if I

00:12:39,430 --> 00:12:43,000
scroll through this data frame I'll see

00:12:41,080 --> 00:12:46,030
different types of annotations so for

00:12:43,000 --> 00:12:48,040
each image I have an image ID I have a

00:12:46,030 --> 00:12:50,800
bounding box for where a particular

00:12:48,040 --> 00:12:52,750
label is in an image and then I have an

00:12:50,800 --> 00:12:55,690
actual pointer to where that image lives

00:12:52,750 --> 00:12:57,550
on the web alright so this is just the

00:12:55,690 --> 00:12:58,780
positive training examples I now want to

00:12:57,550 --> 00:13:00,370
be able to slice out the negative

00:12:58,780 --> 00:13:02,680
training examples so I mentioned earlier

00:13:00,370 --> 00:13:05,020
that there were nine million images in

00:13:02,680 --> 00:13:08,500
Google open images do you think we have

00:13:05,020 --> 00:13:10,540
more or less non pretzels they're way

00:13:08,500 --> 00:13:12,130
more non pretzels right so the key thing

00:13:10,540 --> 00:13:13,660
we're gonna do here is not only are we

00:13:12,130 --> 00:13:15,940
going to invert the logic of our query

00:13:13,660 --> 00:13:17,950
but we're gonna ask for a random sample

00:13:15,940 --> 00:13:19,480
there are sequel gurus out there there

00:13:17,950 --> 00:13:19,950
are much faster ways to do this but it's

00:13:19,480 --> 00:13:21,390
a small

00:13:19,950 --> 00:13:23,430
subset you don't feel it when you run

00:13:21,390 --> 00:13:25,230
the query and we're gonna limit it to

00:13:23,430 --> 00:13:27,150
the number of positive training examples

00:13:25,230 --> 00:13:28,500
that we had so what we're now able to do

00:13:27,150 --> 00:13:30,150
is we're able to create a training set

00:13:28,500 --> 00:13:31,770
with an equal number of positive and

00:13:30,150 --> 00:13:33,150
negative training examples and you'll

00:13:31,770 --> 00:13:36,150
see that everything I've gotten back

00:13:33,150 --> 00:13:37,290
here in this data frame is it contains a

00:13:36,150 --> 00:13:39,180
description which is not pretzeled

00:13:37,290 --> 00:13:41,370
what's exactly what I want for teaching

00:13:39,180 --> 00:13:44,520
my classifier this concept of pretzel

00:13:41,370 --> 00:13:47,010
versus not pretzel all right let's look

00:13:44,520 --> 00:13:49,290
at some examples of deserialization so

00:13:47,010 --> 00:13:50,880
earlier Jackson's showed an example of

00:13:49,290 --> 00:13:53,040
browsing a quilt package and it looked

00:13:50,880 --> 00:13:55,440
like a text-based interface to a file

00:13:53,040 --> 00:13:57,000
system sometimes however we have

00:13:55,440 --> 00:13:58,650
multimedia and we want to be able to

00:13:57,000 --> 00:14:01,410
browse through that in a visual way and

00:13:58,650 --> 00:14:03,210
so I'm gonna use the Berkeley

00:14:01,410 --> 00:14:04,650
segmentation data set which is a

00:14:03,210 --> 00:14:06,390
benchmark data set of 300 different

00:14:04,650 --> 00:14:08,430
images and the first thing you'll know

00:14:06,390 --> 00:14:10,920
there is that when I did the install I

00:14:08,430 --> 00:14:13,500
got a no op and that no op is to my

00:14:10,920 --> 00:14:14,940
advantage because quality duplicates all

00:14:13,500 --> 00:14:17,130
of the fragments that are trafficked

00:14:14,940 --> 00:14:18,240
which means that if you have already the

00:14:17,130 --> 00:14:20,460
dependencies that you need on your

00:14:18,240 --> 00:14:22,110
machine there's no network activity and

00:14:20,460 --> 00:14:23,700
there's no disk activity quotes like oh

00:14:22,110 --> 00:14:26,010
he already has the hashes for these data

00:14:23,700 --> 00:14:28,920
fragments he's got what he's need what

00:14:26,010 --> 00:14:30,780
he needs to start so what I'm gonna do

00:14:28,920 --> 00:14:33,360
here on the D serialization side of

00:14:30,780 --> 00:14:35,430
things is the Berkley segmentation data

00:14:33,360 --> 00:14:37,140
set as I mentioned is 300 images it

00:14:35,430 --> 00:14:41,310
isn't useful to see a bunch of names

00:14:37,140 --> 00:14:43,260
like n 1 0 3 0 7 0 and so inside the

00:14:41,310 --> 00:14:45,300
package namespace which is just BSD

00:14:43,260 --> 00:14:47,700
images tasks right I've imported the

00:14:45,300 --> 00:14:49,710
packages the packages PSD I'm going to

00:14:47,700 --> 00:14:52,170
give it a custom D serializer and in

00:14:49,710 --> 00:14:54,270
this case we provide a library of

00:14:52,170 --> 00:14:55,830
convenience d serializers so that in

00:14:54,270 --> 00:14:57,840
this case for images you literally have

00:14:55,830 --> 00:14:59,580
to do no work and you're like ok I want

00:14:57,840 --> 00:15:01,860
to see this path of the package as a

00:14:59,580 --> 00:15:04,140
plot and boom there it is visually

00:15:01,860 --> 00:15:06,030
browsable let's take another example

00:15:04,140 --> 00:15:08,730
here any any PI torch fans in the

00:15:06,030 --> 00:15:11,070
audience cool we're big fans of pi torch

00:15:08,730 --> 00:15:13,050
and heavy users of pi torch and as you

00:15:11,070 --> 00:15:14,970
know different machine learning cuts

00:15:13,050 --> 00:15:16,530
tool kits like tensorflow and pi torch

00:15:14,970 --> 00:15:18,570
they have different data formats what a

00:15:16,530 --> 00:15:20,490
pain so the question is now how do you

00:15:18,570 --> 00:15:23,010
go from a quote package to a machine

00:15:20,490 --> 00:15:24,720
learning friendly data format and the

00:15:23,010 --> 00:15:27,030
answer is we're gonna use the as a

00:15:24,720 --> 00:15:29,040
deserialize or mechanism again and we

00:15:27,030 --> 00:15:31,110
provide candy serializers for popular

00:15:29,040 --> 00:15:33,089
packages like pi torch and all I'm doing

00:15:31,110 --> 00:15:34,860
here is I'm browsing into

00:15:33,089 --> 00:15:36,660
package name space that should say BST

00:15:34,860 --> 00:15:38,910
that's narrow there I'm browsing into my

00:15:36,660 --> 00:15:41,279
package name space and I'm going to hand

00:15:38,910 --> 00:15:43,230
it a dataset D serializer which will

00:15:41,279 --> 00:15:44,879
then present that as a PI torch dataset

00:15:43,230 --> 00:15:46,949
which is an abstraction that PI torch

00:15:44,879 --> 00:15:48,839
can work with and now I have the data

00:15:46,949 --> 00:15:50,399
living in quilt but I'm able to interact

00:15:48,839 --> 00:15:58,170
with it in a machine learning toolkit in

00:15:50,399 --> 00:16:01,050
a native way very important for imaging

00:15:58,170 --> 00:16:04,350
scientists specifically because imaging

00:16:01,050 --> 00:16:06,959
formats are insane in a certain sense we

00:16:04,350 --> 00:16:09,420
work at lnstitute we work with probably

00:16:06,959 --> 00:16:12,029
40 or so different imaging formats and

00:16:09,420 --> 00:16:14,970
providing a DC réaliser for all those is

00:16:12,029 --> 00:16:16,350
relatively hard to do but with this

00:16:14,970 --> 00:16:17,879
functionality we can say okay you know

00:16:16,350 --> 00:16:20,009
if you want to access our data and view

00:16:17,879 --> 00:16:22,559
it in the same way we do internally this

00:16:20,009 --> 00:16:24,809
as a capability is incredibly powerful

00:16:22,559 --> 00:16:27,839
for us and so you know building a custom

00:16:24,809 --> 00:16:29,670
deserialize er means that we we can

00:16:27,839 --> 00:16:31,800
allow people to focus on using our data

00:16:29,670 --> 00:16:34,290
and not worry with ingesting our data

00:16:31,800 --> 00:16:35,699
you know that is a part of the problem

00:16:34,290 --> 00:16:37,139
with just sharing you you can share all

00:16:35,699 --> 00:16:38,429
the data you want but if people can't

00:16:37,139 --> 00:16:40,620
actually view it or can't actually use

00:16:38,429 --> 00:16:43,980
it that's not usable data it's not open

00:16:40,620 --> 00:16:47,339
science and the last example we want to

00:16:43,980 --> 00:16:48,929
talk about is rehydrating models or you

00:16:47,339 --> 00:16:50,189
know you've already trained your model I

00:16:48,929 --> 00:16:52,470
want other people to use it without

00:16:50,189 --> 00:16:55,170
actually having to you know retrain it

00:16:52,470 --> 00:16:58,110
in in whole this is pretty important

00:16:55,170 --> 00:16:59,670
from a very high level I work on the

00:16:58,110 --> 00:17:02,540
modeling team at Allen's suit and so

00:16:59,670 --> 00:17:04,649
that is producing predictive models and

00:17:02,540 --> 00:17:07,520
this most recent model that we made is

00:17:04,649 --> 00:17:10,350
called the label-free method it is

00:17:07,520 --> 00:17:12,539
without going too much into cell biology

00:17:10,350 --> 00:17:14,039
we we take this bright field image on

00:17:12,539 --> 00:17:15,870
the Left which is exactly if you go back

00:17:14,039 --> 00:17:17,069
to your high school biology class this

00:17:15,870 --> 00:17:18,240
is the bright field image is basically

00:17:17,069 --> 00:17:20,039
what you see when you look through a

00:17:18,240 --> 00:17:22,409
microscope right so it's it's like the

00:17:20,039 --> 00:17:25,620
bare bones just let me see the cells on

00:17:22,409 --> 00:17:27,360
this on this glass plate but with a

00:17:25,620 --> 00:17:29,850
little bit of gene editing and and

00:17:27,360 --> 00:17:31,710
tagging of proteins you can you can

00:17:29,850 --> 00:17:34,470
produce all these channels in the middle

00:17:31,710 --> 00:17:36,720
so at the top one I think that is lamin

00:17:34,470 --> 00:17:38,460
b1 or fibrin which is which is a protein

00:17:36,720 --> 00:17:40,680
in the cell and you can see the nucleus

00:17:38,460 --> 00:17:42,510
is the is the dark blue there

00:17:40,680 --> 00:17:44,490
and some more structures that I don't

00:17:42,510 --> 00:17:46,950
want to go into but you can tag all them

00:17:44,490 --> 00:17:48,960
with all the proteins with fluorescent

00:17:46,950 --> 00:17:51,540
light or with a fluorescent tag that you

00:17:48,960 --> 00:17:54,180
can image and because of that we can

00:17:51,540 --> 00:17:55,980
take a untagged bright field image and a

00:17:54,180 --> 00:17:58,230
tagged structure and we can match them

00:17:55,980 --> 00:17:59,640
up and produce a predictive model but

00:17:58,230 --> 00:18:01,740
this takes a ton of time and a ton

00:17:59,640 --> 00:18:03,150
resources as I just went over it takes a

00:18:01,740 --> 00:18:05,820
gene editor it takes a microscopy team

00:18:03,150 --> 00:18:07,020
it takes a modeling team not many labs

00:18:05,820 --> 00:18:09,150
have all those resources readily

00:18:07,020 --> 00:18:10,410
available so it'd be nice that if we

00:18:09,150 --> 00:18:11,670
produce this model to be able to

00:18:10,410 --> 00:18:14,070
instantly share that with other people

00:18:11,670 --> 00:18:16,350
right it would be really really cool to

00:18:14,070 --> 00:18:17,250
say to a high school student oh it's

00:18:16,350 --> 00:18:18,720
really neat that you have that bright

00:18:17,250 --> 00:18:21,120
field image maybe you want to get an

00:18:18,720 --> 00:18:23,130
image of the nucleus and because of

00:18:21,120 --> 00:18:25,680
because of that we actually can do that

00:18:23,130 --> 00:18:27,750
so we've produced a label free a quilt

00:18:25,680 --> 00:18:29,850
label free package so to actually get

00:18:27,750 --> 00:18:32,610
our pre trained models people can just

00:18:29,850 --> 00:18:34,170
install them and use them normally right

00:18:32,610 --> 00:18:37,200
now we have DNA if you're balerion limit

00:18:34,170 --> 00:18:39,210
be one the membrane etc and actually use

00:18:37,200 --> 00:18:42,960
them you know you just say import our

00:18:39,210 --> 00:18:45,030
fnet package load the model that you

00:18:42,960 --> 00:18:46,920
just installed and then you know process

00:18:45,030 --> 00:18:48,090
your your input image but then to get a

00:18:46,920 --> 00:18:49,770
prediction you don't have to do any

00:18:48,090 --> 00:18:53,400
training it's already there it's already

00:18:49,770 --> 00:18:56,160
installed without any hassle from you so

00:18:53,400 --> 00:18:58,500
to wrap up why is it important that we

00:18:56,160 --> 00:19:00,540
get our data out there well we may have

00:18:58,500 --> 00:19:02,160
an internal Institute workflow where it

00:19:00,540 --> 00:19:05,100
takes a long time for us to you know

00:19:02,160 --> 00:19:06,960
prep and gene edit and tag and and label

00:19:05,100 --> 00:19:08,820
everything that we produce but let's

00:19:06,960 --> 00:19:10,590
package up all those outputs and send

00:19:08,820 --> 00:19:13,200
them out to a world that in a decent way

00:19:10,590 --> 00:19:15,840
in a good way that actually demonstrates

00:19:13,200 --> 00:19:18,660
open science right if if people can't

00:19:15,840 --> 00:19:20,970
use the data that that you're delivering

00:19:18,660 --> 00:19:22,470
in the way that they want to isn't

00:19:20,970 --> 00:19:26,130
really reproducible science is it really

00:19:22,470 --> 00:19:27,240
open science and for us quilt covers the

00:19:26,130 --> 00:19:28,980
four bases we need to actually

00:19:27,240 --> 00:19:30,720
demonstrate open science in my opinion

00:19:28,980 --> 00:19:32,490
which is we have prototype data our

00:19:30,720 --> 00:19:33,870
indie data where you know might it might

00:19:32,490 --> 00:19:35,940
just be a one-off experiment it might

00:19:33,870 --> 00:19:37,920
just be one off research but we also

00:19:35,940 --> 00:19:39,030
have production data and and both those

00:19:37,920 --> 00:19:41,190
need to be covered internally and

00:19:39,030 --> 00:19:43,710
externally and the same system can cover

00:19:41,190 --> 00:19:45,540
all four of those bases for us which

00:19:43,710 --> 00:19:46,950
which means that any notebooks that we

00:19:45,540 --> 00:19:49,260
produce or any research that we write

00:19:46,950 --> 00:19:51,840
the the tagline at the top that says

00:19:49,260 --> 00:19:53,550
import this data set that same data set

00:19:51,840 --> 00:19:55,490
can be imported externally as well as

00:19:53,550 --> 00:19:57,809
internally

00:19:55,490 --> 00:19:59,880
great so we're gonna end with a few

00:19:57,809 --> 00:20:02,700
recommendations so the first is let's

00:19:59,880 --> 00:20:04,860
stop the insanity right as Pete worden

00:20:02,700 --> 00:20:06,600
mentioned you would never code without

00:20:04,860 --> 00:20:08,550
source control and to me it is literally

00:20:06,600 --> 00:20:10,679
insane that we aren't versioning and

00:20:08,550 --> 00:20:12,390
storing our data in immutable blocks so

00:20:10,679 --> 00:20:13,860
really simple place to start version

00:20:12,390 --> 00:20:15,210
your data and your models we can talk

00:20:13,860 --> 00:20:16,890
about different solutions and methods

00:20:15,210 --> 00:20:19,290
for doing that we've shown you a few

00:20:16,890 --> 00:20:21,870
during the course of the talk and it's a

00:20:19,290 --> 00:20:24,030
really obvious point but it's still too

00:20:21,870 --> 00:20:25,710
hard to do open science and so what

00:20:24,030 --> 00:20:27,540
we're really trying to contribute is a

00:20:25,710 --> 00:20:30,090
fundamental piece of data engineering

00:20:27,540 --> 00:20:32,730
that gets solved over and over a hundred

00:20:30,090 --> 00:20:33,990
times all around the world badly and we

00:20:32,730 --> 00:20:35,610
want to solve those set of data

00:20:33,990 --> 00:20:36,990
engineering problems once and give them

00:20:35,610 --> 00:20:38,700
to community as a set of reusable

00:20:36,990 --> 00:20:40,950
building blocks and that's what makes

00:20:38,700 --> 00:20:45,390
for simple you build complex pipelines

00:20:40,950 --> 00:20:47,040
from pieces that you understand well so

00:20:45,390 --> 00:20:49,110
from a design standpoint we've talked a

00:20:47,040 --> 00:20:51,270
lot about immutability here again the

00:20:49,110 --> 00:20:53,190
key point is that once a data set or a

00:20:51,270 --> 00:20:55,470
package is mint minted it will never

00:20:53,190 --> 00:20:57,179
change this you incur a cost for this

00:20:55,470 --> 00:20:59,370
and that means every time you edit that

00:20:57,179 --> 00:21:01,110
data set you copy-on-write that data set

00:20:59,370 --> 00:21:02,970
but the key point here is with an

00:21:01,110 --> 00:21:03,720
immutable history you cannot only audit

00:21:02,970 --> 00:21:05,790
what you've done

00:21:03,720 --> 00:21:07,620
but you have a 100% guarantee that your

00:21:05,790 --> 00:21:09,809
code is executing against the same data

00:21:07,620 --> 00:21:11,220
substrate the architecture is fully

00:21:09,809 --> 00:21:12,900
distributed what does that mean it means

00:21:11,220 --> 00:21:15,120
that when you are streaming data to

00:21:12,900 --> 00:21:16,800
quilt or from quilt quilt is talking

00:21:15,120 --> 00:21:19,140
directly to blob storage is talking

00:21:16,800 --> 00:21:21,450
directly to s3 so there is no server

00:21:19,140 --> 00:21:23,970
side bottleneck the client simply gets

00:21:21,450 --> 00:21:25,590
assigned URL and that URL then allows

00:21:23,970 --> 00:21:27,900
the client to communicate directly with

00:21:25,590 --> 00:21:29,309
s3 and this is critical because you may

00:21:27,900 --> 00:21:31,350
have peak usage times where there are

00:21:29,309 --> 00:21:33,000
hundreds of thousands of people pulling

00:21:31,350 --> 00:21:34,350
data from your servers you don't want

00:21:33,000 --> 00:21:36,120
that to be that load to be on your

00:21:34,350 --> 00:21:37,470
servers you want it to be on Amazon or a

00:21:36,120 --> 00:21:39,750
juror whichever cloud provider you use

00:21:37,470 --> 00:21:41,510
or even on a local Nass you can run

00:21:39,750 --> 00:21:43,320
quote against local storage as well

00:21:41,510 --> 00:21:44,880
deduplication I feel like we talked

00:21:43,320 --> 00:21:47,370
about fairly well this is the beauty of

00:21:44,880 --> 00:21:49,710
Merkle trees and hash trees is that you

00:21:47,370 --> 00:21:51,510
know a given fragment by its content

00:21:49,710 --> 00:21:54,030
addressable identification which is just

00:21:51,510 --> 00:21:56,370
a hash and therefore you know two people

00:21:54,030 --> 00:21:58,620
could call two different pieces of data

00:21:56,370 --> 00:22:00,420
it's two identical pieces of data

00:21:58,620 --> 00:22:03,660
different things but the system will

00:22:00,420 --> 00:22:05,669
understand these are the same bytes all

00:22:03,660 --> 00:22:07,640
right so quickly on the architecture as

00:22:05,669 --> 00:22:08,600
I mentioned all of this is open so

00:22:07,640 --> 00:22:10,160
we would love to have you as

00:22:08,600 --> 00:22:11,540
contributors I'm really showing the

00:22:10,160 --> 00:22:13,820
slide to give you a sense of where we

00:22:11,540 --> 00:22:16,460
could use help so the compiler that we

00:22:13,820 --> 00:22:18,770
saw earlier is implemented in Python we

00:22:16,460 --> 00:22:21,230
use pandas and PI arrow to do the

00:22:18,770 --> 00:22:22,970
parsing and serialization the registry

00:22:21,230 --> 00:22:24,500
is largely built around flask and

00:22:22,970 --> 00:22:26,120
there's a Postgres database instance

00:22:24,500 --> 00:22:28,130
underneath all of this is containerized

00:22:26,120 --> 00:22:29,960
I should mention the registries key job

00:22:28,130 --> 00:22:31,910
is to manage indexing of the data and

00:22:29,960 --> 00:22:34,580
permission management for fragments that

00:22:31,910 --> 00:22:35,960
are in the quilt store the catalog which

00:22:34,580 --> 00:22:37,430
you saw earlier does searching and

00:22:35,960 --> 00:22:42,050
browsing and that's in everybody's

00:22:37,430 --> 00:22:43,490
favorite react and redux all right so I

00:22:42,050 --> 00:22:44,870
think we've talked about a lot we're

00:22:43,490 --> 00:22:46,430
gonna have plenty of time for questions

00:22:44,870 --> 00:22:48,490
if you would like to follow up further

00:22:46,430 --> 00:22:51,320
we'll make these notebooks public and

00:22:48,490 --> 00:22:54,020
there also is an article written by

00:22:51,320 --> 00:22:55,760
Jackson on data notebook exploration

00:22:54,020 --> 00:22:57,380
using quilt and the quilt data pipeline

00:22:55,760 --> 00:22:59,090
so if you're looking for more technical

00:22:57,380 --> 00:23:02,090
details you can find this on the Allen

00:22:59,090 --> 00:23:04,100
sale site you can also hit up the

00:23:02,090 --> 00:23:06,170
Jupiter blog and look for a reproducible

00:23:04,100 --> 00:23:07,880
data and you'll find this post which

00:23:06,170 --> 00:23:08,990
explains in detail how quote works and

00:23:07,880 --> 00:23:12,320
how you can apply it to your daily

00:23:08,990 --> 00:23:13,910
workflow and that is everything we

00:23:12,320 --> 00:23:15,680
wanted to talk about I'd love to spend

00:23:13,910 --> 00:23:19,030
some time hearing what your problems are

00:23:15,680 --> 00:23:27,900
with data flows and how we might help

00:23:19,030 --> 00:23:27,900
[Applause]

00:23:57,730 --> 00:24:01,520
we think about that every day and there

00:24:00,020 --> 00:24:02,060
are three solutions that we have come up

00:24:01,520 --> 00:24:05,060
with

00:24:02,060 --> 00:24:07,460
they aren't ideal but they work and so

00:24:05,060 --> 00:24:10,040
one of them your cloud providers have a

00:24:07,460 --> 00:24:11,660
mode called requester pays and this

00:24:10,040 --> 00:24:13,940
essentially means that the requester if

00:24:11,660 --> 00:24:16,070
they have an identity provider that's

00:24:13,940 --> 00:24:17,480
linked to that cloud provider they then

00:24:16,070 --> 00:24:19,310
pay for the egress from that bucket

00:24:17,480 --> 00:24:20,930
that's one way it is it beautiful but

00:24:19,310 --> 00:24:23,060
it's kind of a directed brutal solution

00:24:20,930 --> 00:24:25,010
the second thing that you can do is

00:24:23,060 --> 00:24:27,730
Amazon will actually allow you to

00:24:25,010 --> 00:24:30,380
designate an s3 bucket as open data and

00:24:27,730 --> 00:24:32,000
if they stamp your bucket or approve

00:24:30,380 --> 00:24:35,060
your bucket egress from that bucket is

00:24:32,000 --> 00:24:38,060
now free the fourth way is really

00:24:35,060 --> 00:24:41,180
server-side slicing and so the the real

00:24:38,060 --> 00:24:42,770
reason that we need presto DB and we

00:24:41,180 --> 00:24:45,350
need the host version of that is called

00:24:42,770 --> 00:24:47,750
Amazon Athena is to reduce the amount of

00:24:45,350 --> 00:24:50,000
egress and a very very typical pattern

00:24:47,750 --> 00:24:52,100
in data science is that yeah I need to

00:24:50,000 --> 00:24:53,780
sift through an enormous data set but

00:24:52,100 --> 00:24:55,280
actually in practice as we saw in the

00:24:53,780 --> 00:24:57,470
machine learning example I only need a

00:24:55,280 --> 00:25:01,190
thousand rows or a hundred rows so the

00:24:57,470 --> 00:25:03,080
cost of running preso DB is much much

00:25:01,190 --> 00:25:26,840
much lower in terms of compute than you

00:25:03,080 --> 00:25:28,610
will pay for people I forgot method 4

00:25:26,840 --> 00:25:30,290
which is put it on quilt so our business

00:25:28,610 --> 00:25:32,540
model actually covers egress from public

00:25:30,290 --> 00:25:33,530
buckets right and the idea there is if

00:25:32,540 --> 00:25:34,400
you're giving your data way you're

00:25:33,530 --> 00:25:42,280
making the world a better place

00:25:34,400 --> 00:25:45,280
both flipped the cost other questions

00:25:42,280 --> 00:25:45,280
yeah

00:26:12,450 --> 00:26:17,140
it is so the question is about accessing

00:26:15,580 --> 00:26:19,090
quote packages from other languages so

00:26:17,140 --> 00:26:21,159
eventually we will have first-class

00:26:19,090 --> 00:26:22,720
clients for those packages however you

00:26:21,159 --> 00:26:25,030
can use them today and there is a

00:26:22,720 --> 00:26:27,039
function called quilt export which will

00:26:25,030 --> 00:26:28,840
take the package out of Python native

00:26:27,039 --> 00:26:31,419
formats and just dump it into a bunch of

00:26:28,840 --> 00:26:33,520
files on disk it isn't ideal because you

00:26:31,419 --> 00:26:35,860
lose some of the deduplication but I see

00:26:33,520 --> 00:26:36,970
it's crazy and it's not an open format

00:26:35,860 --> 00:26:38,860
but CSV is are actually super

00:26:36,970 --> 00:26:40,270
cross-platform and is one of the silly

00:26:38,860 --> 00:26:42,610
problems that we ran into we have all

00:26:40,270 --> 00:26:44,710
these fancy T serializers like we use PI

00:26:42,610 --> 00:26:46,480
Aero to dump tabular data into park' and

00:26:44,710 --> 00:26:47,980
it turns out that CSV

00:26:46,480 --> 00:26:49,900
although park' it's much more performant

00:26:47,980 --> 00:26:51,610
CSV is more cross-platform so the short

00:26:49,900 --> 00:26:53,590
answer your question is yes you can

00:26:51,610 --> 00:26:55,419
export the packages files the second

00:26:53,590 --> 00:26:57,039
thing which the system will do for you

00:26:55,419 --> 00:26:59,289
so I'll just back up to the actual D

00:26:57,039 --> 00:27:01,960
serialization example is you can also

00:26:59,289 --> 00:27:05,530
get a path to that data on disk so I

00:27:01,960 --> 00:27:07,870
have to go back very far okay so II X

00:27:05,530 --> 00:27:09,700
dot DL if you with other types of data

00:27:07,870 --> 00:27:10,780
if we don't have a deserialize therefore

00:27:09,700 --> 00:27:14,049
it will just tell you here's where those

00:27:10,780 --> 00:27:15,340
on disk so it's just a path at that

00:27:14,049 --> 00:27:17,049
point you have to look in the metadata

00:27:15,340 --> 00:27:17,950
and see what the file types are but you

00:27:17,049 --> 00:27:24,270
can do whatever you want with that

00:27:17,950 --> 00:27:24,270
because it's just bytes on disk yeah

00:27:39,990 --> 00:27:45,550
it does not today it could so there's a

00:27:43,570 --> 00:27:47,470
profiling past during the build step the

00:27:45,550 --> 00:27:50,170
data are profiled you can just you could

00:27:47,470 --> 00:27:53,830
extend the build to profile hdf5 and

00:27:50,170 --> 00:27:56,710
other file formats and as an example of

00:27:53,830 --> 00:27:58,540
the kind of agility that abstraction

00:27:56,710 --> 00:28:00,370
buys you the first version of quilt was

00:27:58,540 --> 00:28:02,260
not built using parquet for tabular data

00:28:00,370 --> 00:28:06,490
we use hdf5 we had a lot of problems

00:28:02,260 --> 00:28:07,480
with hdf5 though so we moved come on off

00:28:06,490 --> 00:28:15,940
I feel like you should answer some of

00:28:07,480 --> 00:28:21,760
these questions so part of the reason

00:28:15,940 --> 00:28:24,820
that we wanted to use a system like this

00:28:21,760 --> 00:28:27,280
was specifically for allowing any

00:28:24,820 --> 00:28:29,500
deserialized to be used so that's also

00:28:27,280 --> 00:28:32,380
right why it's you know we are building

00:28:29,500 --> 00:28:34,270
systems to to handle all the different

00:28:32,380 --> 00:28:37,480
types of data we want an export and send

00:28:34,270 --> 00:28:39,070
out but because our data requires custom

00:28:37,480 --> 00:28:41,380
D sterilizers for a lot of cases that

00:28:39,070 --> 00:28:43,420
you know imaging imaging these these

00:28:41,380 --> 00:28:46,000
high density images needs to custom you

00:28:43,420 --> 00:28:48,340
serializer just having the file path

00:28:46,000 --> 00:28:49,630
instead of having like a default load

00:28:48,340 --> 00:28:55,890
data frame or something like that is

00:28:49,630 --> 00:28:55,890
needed so other questions hi

00:28:57,820 --> 00:29:05,700
the largest two are CGI CGI blue and CGI

00:29:01,749 --> 00:29:09,190
black cz I is not Chan Zuckerberg it is

00:29:05,700 --> 00:29:10,809
Carlos ice imaging and the other the

00:29:09,190 --> 00:29:13,299
other major one is called ome TIFF which

00:29:10,809 --> 00:29:15,219
is open microscopy environment and then

00:29:13,299 --> 00:29:18,279
TIFF is this format that was invented

00:29:15,219 --> 00:29:20,710
like 1970 that's ran by Adobe now it's

00:29:18,279 --> 00:29:21,999
yeah those two are the main two and then

00:29:20,710 --> 00:29:24,330
there's a you know whole plethora of

00:29:21,999 --> 00:29:26,590
other ones that we sometimes encounter

00:29:24,330 --> 00:29:28,659
those are the storage formats that Ellen

00:29:26,590 --> 00:29:30,489
still uses but again you quilt is data

00:29:28,659 --> 00:29:31,839
agnostic it's just gonna if it doesn't

00:29:30,489 --> 00:29:33,129
understand the data format it's just

00:29:31,839 --> 00:29:35,409
gonna copy the bytes so you can

00:29:33,129 --> 00:29:37,379
literally put anything in it so we are

00:29:35,409 --> 00:29:39,759
we are also starting to get into

00:29:37,379 --> 00:29:42,039
potentially putting like the RNA

00:29:39,759 --> 00:29:44,529
sequence data in there which is a whole

00:29:42,039 --> 00:29:46,419
different file format as well but yeah

00:29:44,529 --> 00:29:48,969
because you know we generate so much

00:29:46,419 --> 00:29:50,859
data that we want to share it's it's

00:29:48,969 --> 00:29:54,119
nice that we don't have to build a

00:29:50,859 --> 00:29:54,119
schema specifically for it

00:29:58,370 --> 00:30:01,370
hi

00:30:08,570 --> 00:30:12,750
yeah so the question is about what is

00:30:11,010 --> 00:30:16,050
the granularity of change management

00:30:12,750 --> 00:30:17,760
it's currently at the file level we

00:30:16,050 --> 00:30:19,230
could be more granular at the cost of

00:30:17,760 --> 00:30:20,790
performance so this is another thing

00:30:19,230 --> 00:30:22,590
that we could use input from the

00:30:20,790 --> 00:30:25,170
community to work on really common I

00:30:22,590 --> 00:30:28,640
think Dropbox hashes for megabyte chunks

00:30:25,170 --> 00:30:28,640
we just hash the entire file right now

00:30:34,970 --> 00:30:42,350
you could and the way you would do that

00:30:37,740 --> 00:30:42,350
so let me go to the as a example here

00:30:42,380 --> 00:30:47,010
did I miss it okay so the way you would

00:30:45,420 --> 00:30:48,510
do that by the way the as a function

00:30:47,010 --> 00:30:50,580
will take any lambda with the right

00:30:48,510 --> 00:30:52,290
signature so even if it's not part of

00:30:50,580 --> 00:30:55,580
the quote as a library you can just load

00:30:52,290 --> 00:30:55,580
up your own deserialize and in fact

00:30:55,700 --> 00:31:01,050
providing these custom deserialize errs

00:30:58,230 --> 00:31:03,890
as just you know import the inks as a

00:31:01,050 --> 00:31:03,890
library or whatever

00:31:20,240 --> 00:31:26,880
so yes so partially you know the people

00:31:23,820 --> 00:31:28,919
who are really interested in our data

00:31:26,880 --> 00:31:30,890
currently like currently available is is

00:31:28,919 --> 00:31:34,020
people who know how to open these files

00:31:30,890 --> 00:31:35,580
but as I said with our more predictive

00:31:34,020 --> 00:31:37,020
model stuff is like we want to get to a

00:31:35,580 --> 00:31:39,179
point where like educators can say to a

00:31:37,020 --> 00:31:41,460
classroom like here's a model go ahead

00:31:39,179 --> 00:31:43,830
and use it but that in itself is

00:31:41,460 --> 00:31:46,530
somewhat finicky as moschini people

00:31:43,830 --> 00:31:48,150
probably know so we want to provide

00:31:46,530 --> 00:31:50,990
these deserialize errs that make it

00:31:48,150 --> 00:31:55,200
easier in the case of our RNA seek data

00:31:50,990 --> 00:31:57,299
and more complex data types providing a

00:31:55,200 --> 00:31:59,039
package that has all of our Allen cell

00:31:57,299 --> 00:32:00,659
DC er lasers would be nice that they

00:31:59,039 --> 00:32:10,559
don't have to install them one by one or

00:32:00,659 --> 00:32:13,559
something yes

00:32:10,559 --> 00:32:15,450
so it's the file and the deserialize

00:32:13,559 --> 00:32:18,419
there are separate objects right so the

00:32:15,450 --> 00:32:21,030
file can live wherever it wants to it on

00:32:18,419 --> 00:32:23,220
the code servers but it's it's it gets

00:32:21,030 --> 00:32:25,049
deserialized you know in runtime so so

00:32:23,220 --> 00:32:25,620
whenever I can push a change to the D

00:32:25,049 --> 00:32:27,960
serializer

00:32:25,620 --> 00:32:34,020
but the file doesn't change and so it'll

00:32:27,960 --> 00:32:37,470
still interact properly so the example

00:32:34,020 --> 00:32:39,600
being right there are how many imaging

00:32:37,470 --> 00:32:43,440
Python image and image reading libraries

00:32:39,600 --> 00:32:44,909
possible you could have PIL Python

00:32:43,440 --> 00:32:46,590
imaging libraries like main one and then

00:32:44,909 --> 00:32:47,880
like images and other deserialize are

00:32:46,590 --> 00:32:49,559
all for JPEG you could have five

00:32:47,880 --> 00:32:53,340
different imaging light libraries as a

00:32:49,559 --> 00:32:56,789
deserialize er for JPEG files so how

00:32:53,340 --> 00:33:01,380
many people use pickle out there please

00:32:56,789 --> 00:33:03,059
stop so hey we we have bent over

00:33:01,380 --> 00:33:04,830
backwards if this is actually more

00:33:03,059 --> 00:33:07,289
really sharing our battle scars with you

00:33:04,830 --> 00:33:08,880
like pickle as much as you want but

00:33:07,289 --> 00:33:10,940
pickle it has a couple of

00:33:08,880 --> 00:33:13,380
vulnerabilities so the first is if you

00:33:10,940 --> 00:33:14,850
serialized in pickle to you cannot be

00:33:13,380 --> 00:33:16,409
serious use me if you see realized in

00:33:14,850 --> 00:33:18,240
pickle three you cannot deserialize that

00:33:16,409 --> 00:33:19,860
in pickle two and the way to fix that is

00:33:18,240 --> 00:33:21,870
you actually have to monkey patch pickle

00:33:19,860 --> 00:33:23,920
and give it an older version of the

00:33:21,870 --> 00:33:25,930
pickle deserialize err it's ugly you

00:33:23,920 --> 00:33:27,850
think pickle is not secure at all so a

00:33:25,930 --> 00:33:29,770
lot of the work that we need help from

00:33:27,850 --> 00:33:31,330
the open source community is to bring

00:33:29,770 --> 00:33:32,650
this to our dispatch table of like

00:33:31,330 --> 00:33:34,510
because there are just so many exotic

00:33:32,650 --> 00:33:36,370
file formats and we're done trying to

00:33:34,510 --> 00:33:37,600
come up with one true format what we do

00:33:36,370 --> 00:33:39,400
need is for people that contribute to

00:33:37,600 --> 00:33:42,220
serializers and not only is pickle not

00:33:39,400 --> 00:33:44,320
performant it's not secure and so part

00:33:42,220 --> 00:33:45,760
of the serialization process and

00:33:44,320 --> 00:33:47,560
deserialization process has to be also

00:33:45,760 --> 00:33:49,120
recording the version of the serializer

00:33:47,560 --> 00:33:50,470
that you used because version over

00:33:49,120 --> 00:33:51,550
version you know if you serialize the

00:33:50,470 --> 00:33:58,780
version a and then deserialize the

00:33:51,550 --> 00:33:59,860
version b you can have problem alright

00:33:58,780 --> 00:34:02,010
well thanks for your time we'll be

00:33:59,860 --> 00:34:02,010
around

00:34:12,419 --> 00:34:17,250
got it so this is interesting question

00:34:15,419 --> 00:34:19,200
the question is can you Feder eight

00:34:17,250 --> 00:34:21,240
different quilt sources and deduplicate

00:34:19,200 --> 00:34:23,490
across buckets or across registries so

00:34:21,240 --> 00:34:26,309
to speak the infrastructure is there to

00:34:23,490 --> 00:34:27,960
do it the quote client currently assumes

00:34:26,309 --> 00:34:30,089
that you're only talking to one registry

00:34:27,960 --> 00:34:32,099
at a time but because we have

00:34:30,089 --> 00:34:33,720
content-addressable file storage like we

00:34:32,099 --> 00:34:34,740
will know if you have those bytes you

00:34:33,720 --> 00:34:37,139
just have to use exactly the same

00:34:34,740 --> 00:34:38,639
hashing algorithm that we use and where

00:34:37,139 --> 00:34:40,200
we're going for quote 3 which is not

00:34:38,639 --> 00:34:41,789
live yet and then the sequel stuff I

00:34:40,200 --> 00:34:42,990
showed is on top of quote 3 where we're

00:34:41,789 --> 00:34:44,460
going with quote 3 is that you'll be

00:34:42,990 --> 00:34:45,960
able to federate these buckets because

00:34:44,460 --> 00:34:47,460
people don't want to be copying their

00:34:45,960 --> 00:34:48,029
data and moving it around even if it's

00:34:47,460 --> 00:34:49,440
into quill

00:34:48,029 --> 00:34:51,419
they want their data to live where it

00:34:49,440 --> 00:34:53,609
lives and your question about torrenting

00:34:51,419 --> 00:34:55,260
is that is supported so believe it or

00:34:53,609 --> 00:34:56,730
not like amongst the millions of

00:34:55,260 --> 00:34:59,309
features of s3 you can actually turn

00:34:56,730 --> 00:35:01,980
torrenting on from Esther whether or not

00:34:59,309 --> 00:35:03,539
the difference between s3 performance

00:35:01,980 --> 00:35:06,240
and cloud front like CDN performance is

00:35:03,539 --> 00:35:07,890
actually very small so I'm not convinced

00:35:06,240 --> 00:35:09,359
that torrenting from s3 will actually

00:35:07,890 --> 00:35:17,849
help but it's certainly within the realm

00:35:09,359 --> 00:35:21,029
of possibility there is also you can

00:35:17,849 --> 00:35:23,400
point your local quilt store to any

00:35:21,029 --> 00:35:26,339
directory on your drive so you know we

00:35:23,400 --> 00:35:28,410
have a giant shared like 100 terabyte

00:35:26,339 --> 00:35:29,910
drive on our network and I just push

00:35:28,410 --> 00:35:31,140
everything to that drive and that means

00:35:29,910 --> 00:35:32,910
that anyone in the Institute who's

00:35:31,140 --> 00:35:36,809
connected that drive can then access the

00:35:32,910 --> 00:35:39,720
same packages yeah that is all you don't

00:35:36,809 --> 00:35:42,750
have to have 10 versions or 1020 people

00:35:39,720 --> 00:35:49,039
having the same 200 gigabyte package

00:35:42,750 --> 00:35:49,039
it's it's all about shared drives yeah

00:35:50,550 --> 00:35:57,580
all right thanks for time

00:35:53,100 --> 00:35:57,580

YouTube URL: https://www.youtube.com/watch?v=4AS_36-m2X4


