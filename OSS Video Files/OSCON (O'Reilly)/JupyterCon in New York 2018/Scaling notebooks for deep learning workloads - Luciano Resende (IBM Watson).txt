Title: Scaling notebooks for deep learning workloads - Luciano Resende (IBM Watson)
Publication date: 2018-10-09
Playlist: JupyterCon in New York 2018
Description: 
	Luciano Resende outlines a pattern for building deep learning models using the Jupyter Notebook's interactive development in commodity hardware and leveraging platforms and services such as Fabric for Deep Learning (FfDL) for cost-effective full dataset training of deep learning models.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,940 --> 00:00:06,999
my name is Luciano has ng imma open

00:00:03,610 --> 00:00:08,650
source AI platform architect at IBM code

00:00:06,999 --> 00:00:11,530
a code a stands for center of

00:00:08,650 --> 00:00:13,769
open-source data in AI technologies we

00:00:11,530 --> 00:00:16,900
are a group at IBM that we work on

00:00:13,769 --> 00:00:20,650
mostly around open source technologies

00:00:16,900 --> 00:00:22,420
around big data in AI we are kind of

00:00:20,650 --> 00:00:27,939
like located in San Francisco most of

00:00:22,420 --> 00:00:29,859
the team so that's where I'm based if

00:00:27,939 --> 00:00:32,230
you need to reach me if you have like

00:00:29,859 --> 00:00:35,020
questions later on or anything these are

00:00:32,230 --> 00:00:39,309
all the ways you can contact me so like

00:00:35,020 --> 00:00:42,340
my email Twitter github just you like

00:00:39,309 --> 00:00:44,440
what playing wave and also like link it

00:00:42,340 --> 00:00:46,030
in so feel free to reach to me if you

00:00:44,440 --> 00:00:51,879
guys have any further questions or need

00:00:46,030 --> 00:00:53,680
any more details I probably don't need

00:00:51,879 --> 00:00:55,300
to go much around this but I just want

00:00:53,680 --> 00:00:57,910
to make sure everybody is on the same

00:00:55,300 --> 00:01:00,460
page I'm assuming that everybody here

00:00:57,910 --> 00:01:04,900
knows what Jupiter notebooks are we are

00:01:00,460 --> 00:01:07,210
at Jupiter con but what I've seen

00:01:04,900 --> 00:01:09,610
particularly talking with some of the

00:01:07,210 --> 00:01:11,530
folks not everybody knows and

00:01:09,610 --> 00:01:15,750
understands what's going on internally

00:01:11,530 --> 00:01:17,740
when you're using your notebooks so

00:01:15,750 --> 00:01:19,900
familiar with the web interface

00:01:17,740 --> 00:01:22,540
everybody knows we go to the web and we

00:01:19,900 --> 00:01:25,480
kind of like get the notebooks and we

00:01:22,540 --> 00:01:27,820
execute shells when we start executing

00:01:25,480 --> 00:01:32,980
those cells what we have under the

00:01:27,820 --> 00:01:34,780
covers are kernels those kernels they

00:01:32,980 --> 00:01:37,630
abstract the language that you're using

00:01:34,780 --> 00:01:40,390
so we have a Python kernel we have an

00:01:37,630 --> 00:01:41,710
our kernel we have a scholar kernel and

00:01:40,390 --> 00:01:44,740
your name and I think we have more than

00:01:41,710 --> 00:01:49,500
100 different types of kernels that is

00:01:44,740 --> 00:01:53,140
executing your the contents of the cell

00:01:49,500 --> 00:01:54,820
what is interesting to to to understand

00:01:53,140 --> 00:01:56,980
here is that there is a one-on-one

00:01:54,820 --> 00:01:58,930
relationship between when you start a

00:01:56,980 --> 00:02:02,710
notebook you are starting actually a

00:01:58,930 --> 00:02:05,290
kernel that executes those and that

00:02:02,710 --> 00:02:07,840
those kernels they run and consume the

00:02:05,290 --> 00:02:12,160
resource as long as the notebook is

00:02:07,840 --> 00:02:15,070
running so you went to take a cup go for

00:02:12,160 --> 00:02:18,790
or take a coffee break you left those

00:02:15,070 --> 00:02:21,280
running if you are doing interactive

00:02:18,790 --> 00:02:24,040
development on GPUs or any expensive

00:02:21,280 --> 00:02:26,050
resources those resources are locked and

00:02:24,040 --> 00:02:28,690
cannot be reutilized

00:02:26,050 --> 00:02:32,140
by other members this becomes a problem

00:02:28,690 --> 00:02:34,960
when you have kind of like a cluster

00:02:32,140 --> 00:02:36,520
that is being shared among people from

00:02:34,960 --> 00:02:42,910
the department or people from your

00:02:36,520 --> 00:02:44,560
company everybody kind of like on the

00:02:42,910 --> 00:02:50,410
same page let's talk a little bit about

00:02:44,560 --> 00:02:52,240
Jupiter notebooks and deep learning what

00:02:50,410 --> 00:02:55,390
are some of the characteristics of a

00:02:52,240 --> 00:03:00,760
deep learning workload very

00:02:55,390 --> 00:03:04,020
resource-intensive we sometimes can just

00:03:00,760 --> 00:03:07,990
run a quick thing like a simple one

00:03:04,020 --> 00:03:10,600
which like minced which might take an

00:03:07,990 --> 00:03:14,350
hour on my notebook for example so if

00:03:10,600 --> 00:03:17,050
you don't have GPUs but what we do in

00:03:14,350 --> 00:03:18,100
the code day we train some of the models

00:03:17,050 --> 00:03:21,580
that we have there

00:03:18,100 --> 00:03:25,180
we've GPUs and everything powerful

00:03:21,580 --> 00:03:29,490
hardware it sometimes take us one or two

00:03:25,180 --> 00:03:29,490
days just to get the basic model trained

00:03:29,850 --> 00:03:35,650
and yes like as I mentioned and as we

00:03:34,030 --> 00:03:38,980
saw like a Paco mentioning on the

00:03:35,650 --> 00:03:41,020
keynotes today requires expensive

00:03:38,980 --> 00:03:43,540
hardware's and those hardware's we

00:03:41,020 --> 00:03:45,760
usually particularly on big companies we

00:03:43,540 --> 00:03:48,100
want to share those hardware's as it's

00:03:45,760 --> 00:03:51,070
not like very cost effective to have

00:03:48,100 --> 00:03:55,000
those in like every person's desktop and

00:03:51,070 --> 00:03:57,940
things like that here on the right you

00:03:55,000 --> 00:04:03,010
can just see like a very simple one but

00:03:57,940 --> 00:04:06,550
then how very deep and complex those

00:04:03,010 --> 00:04:11,490
models can become and to train those it

00:04:06,550 --> 00:04:11,490
becomes a very intensive computing

00:04:13,200 --> 00:04:19,260
one of the things that we start seeing

00:04:15,360 --> 00:04:21,510
more into the direction is training

00:04:19,260 --> 00:04:24,560
those jobs doing more deep learning with

00:04:21,510 --> 00:04:26,850
the support of like kubernetes

00:04:24,560 --> 00:04:29,669
kubernetes provided to start providing a

00:04:26,850 --> 00:04:31,980
very flexible way to start packaging

00:04:29,669 --> 00:04:33,660
your dependents your dependencies in

00:04:31,980 --> 00:04:35,760
your applications and we talked about

00:04:33,660 --> 00:04:39,840
deep learning we start seeing a lot of

00:04:35,760 --> 00:04:41,490
like various frameworks that are use it

00:04:39,840 --> 00:04:43,710
and with very different characteristics

00:04:41,490 --> 00:04:45,419
and dependencies so like if you're doing

00:04:43,710 --> 00:04:48,090
a model that you're using Chester flow

00:04:45,419 --> 00:04:50,280
or you start doing Cara's and stuff they

00:04:48,090 --> 00:04:53,190
they work differently they depend and

00:04:50,280 --> 00:04:56,730
they require different dependencies and

00:04:53,190 --> 00:04:59,580
also they serve they train and serve

00:04:56,730 --> 00:05:02,760
models differently so kubernetes is

00:04:59,580 --> 00:05:05,520
start being a layer to start abstracting

00:05:02,760 --> 00:05:08,270
some of those from the actual data

00:05:05,520 --> 00:05:12,900
scientist so that it becomes easier for

00:05:08,270 --> 00:05:19,470
building and working on deep learning

00:05:12,900 --> 00:05:21,690
models Kuban it is also helped on kind

00:05:19,470 --> 00:05:25,410
of like sharing some of the expensive

00:05:21,690 --> 00:05:27,360
resources such as GPUs you can start

00:05:25,410 --> 00:05:29,520
defining those resources in kubernetes

00:05:27,360 --> 00:05:32,220
and then you can say that the part that

00:05:29,520 --> 00:05:35,580
you're going to create for doing the

00:05:32,220 --> 00:05:37,080
training is using one of four GPUs so

00:05:35,580 --> 00:05:38,760
that you start sharing those GPUs that

00:05:37,080 --> 00:05:44,700
are available on the server that is

00:05:38,760 --> 00:05:47,790
doing the training so one of the things

00:05:44,700 --> 00:05:51,930
that we did for it we've enterprise

00:05:47,790 --> 00:05:56,190
gateway to help to support cuber Nerys

00:05:51,930 --> 00:05:59,100
is its support in kubernetes as a way

00:05:56,190 --> 00:06:01,050
for you to deploy kernels if we just

00:05:59,100 --> 00:06:03,050
look into you for example what Jupiter

00:06:01,050 --> 00:06:06,690
hub does today or if you just try to

00:06:03,050 --> 00:06:08,610
launch a Jupiter notebook on cuber

00:06:06,690 --> 00:06:13,440
native environment what you're gonna see

00:06:08,610 --> 00:06:14,700
is that you have a pod all the resources

00:06:13,440 --> 00:06:15,930
that you're gonna need for all the

00:06:14,700 --> 00:06:18,780
kernels that you're gonna have to

00:06:15,930 --> 00:06:21,750
instantiate needs to be pre-allocated

00:06:18,780 --> 00:06:23,910
when you create that pod and it will be

00:06:21,750 --> 00:06:24,630
kind of like a locket

00:06:23,910 --> 00:06:27,650
during

00:06:24,630 --> 00:06:29,760
the time that you have the notebook open

00:06:27,650 --> 00:06:32,580
sometimes that is not what we want

00:06:29,760 --> 00:06:35,370
because we might be bringing different

00:06:32,580 --> 00:06:37,020
notebooks up and down maybe we have one

00:06:35,370 --> 00:06:40,200
that is a little bit more long-running

00:06:37,020 --> 00:06:41,550
so we want to have the flexibility and

00:06:40,200 --> 00:06:46,560
we wanted the infrastructure to support

00:06:41,550 --> 00:06:49,850
that as well another problem with that

00:06:46,560 --> 00:06:52,410
is once you bootstrap the the notebook

00:06:49,850 --> 00:06:55,290
expensive resources like GPUs if you had

00:06:52,410 --> 00:06:57,390
like three GPUs assigned to your pod you

00:06:55,290 --> 00:06:59,460
will the first kernel that starts to use

00:06:57,390 --> 00:07:01,320
that will basically start using all of

00:06:59,460 --> 00:07:02,760
the three ones and you cannot start

00:07:01,320 --> 00:07:06,120
sharing between the two different

00:07:02,760 --> 00:07:08,250
notebooks enterprise gateway provides

00:07:06,120 --> 00:07:11,670
you the capability of like remoting the

00:07:08,250 --> 00:07:14,610
kernels in what happens in a kubernetes

00:07:11,670 --> 00:07:19,230
environment those kernels then start

00:07:14,610 --> 00:07:22,250
becoming actually different pods so then

00:07:19,230 --> 00:07:24,450
each part has its own life cycle it only

00:07:22,250 --> 00:07:26,250
locking the resources the moment that

00:07:24,450 --> 00:07:28,920
you need that at the time that you need

00:07:26,250 --> 00:07:31,170
and so for example you if you have three

00:07:28,920 --> 00:07:32,580
GPUs that are available for you you can

00:07:31,170 --> 00:07:34,470
actually have three different kernels

00:07:32,580 --> 00:07:37,080
each one using one GPU and you're going

00:07:34,470 --> 00:07:44,280
to start actually leveraging those

00:07:37,080 --> 00:07:46,230
research much more efficiently with days

00:07:44,280 --> 00:07:47,880
you start yeah that the actual Jupiter

00:07:46,230 --> 00:07:49,590
notebook becomes very lightweight and

00:07:47,880 --> 00:07:51,450
other other resources that you need is

00:07:49,590 --> 00:07:57,360
only allocated at the time that you need

00:07:51,450 --> 00:08:00,120
that we also with the ability to to

00:07:57,360 --> 00:08:04,050
start putting the kernel into its own

00:08:00,120 --> 00:08:06,720
pod we can make those pods be derivated

00:08:04,050 --> 00:08:09,300
from the actual framework that you are

00:08:06,720 --> 00:08:12,870
using so like for example tensorflow has

00:08:09,300 --> 00:08:14,580
a a docker image that can be used it as

00:08:12,870 --> 00:08:15,960
the base image for the car the Python

00:08:14,580 --> 00:08:19,790
car know that you're doing tensorflow

00:08:15,960 --> 00:08:21,750
models and so on for the different

00:08:19,790 --> 00:08:23,930
frameworks that you're using for deep

00:08:21,750 --> 00:08:23,930
learning

00:08:24,410 --> 00:08:30,810
this is kind of like just a quick kind

00:08:27,389 --> 00:08:33,510
of sister diagram we support the ability

00:08:30,810 --> 00:08:35,820
to you bring your own notebook into this

00:08:33,510 --> 00:08:37,710
kind of like environment so you can

00:08:35,820 --> 00:08:39,450
actually have a hosted notebook or you

00:08:37,710 --> 00:08:41,130
can have a notebook that is kind of like

00:08:39,450 --> 00:08:44,870
running the UI the notebook server on

00:08:41,130 --> 00:08:49,140
your laptop it connects to the remote

00:08:44,870 --> 00:08:50,899
platform to be able to actually run the

00:08:49,140 --> 00:08:54,120
kernels which is what needs the

00:08:50,899 --> 00:08:56,190
computing resources and as I mentioned

00:08:54,120 --> 00:08:59,550
the images that are based for those

00:08:56,190 --> 00:09:01,350
kernels are actually become a choice for

00:08:59,550 --> 00:09:03,089
you depending on what model what

00:09:01,350 --> 00:09:08,220
framework you're using for your deep

00:09:03,089 --> 00:09:10,080
learning models and yeah we what we call

00:09:08,220 --> 00:09:13,050
like vanilla kernels which is just like

00:09:10,080 --> 00:09:14,880
a Python or scala or you name the

00:09:13,050 --> 00:09:17,339
current that you want we also support

00:09:14,880 --> 00:09:19,500
for some of the kernels running on spark

00:09:17,339 --> 00:09:21,779
and kubernetes if you're doing some

00:09:19,500 --> 00:09:23,940
analytics that uses a spark or use the

00:09:21,779 --> 00:09:24,959
spark API or some models that are

00:09:23,940 --> 00:09:27,209
available there so you have the

00:09:24,959 --> 00:09:28,740
flexibility you can start mixing and

00:09:27,209 --> 00:09:30,630
matching the framework with the

00:09:28,740 --> 00:09:32,130
environment that you need in that that

00:09:30,630 --> 00:09:38,100
gives you a lot more flexibility in

00:09:32,130 --> 00:09:42,360
terms of like the infrastructure having

00:09:38,100 --> 00:09:43,830
a kind of like at the same we have been

00:09:42,360 --> 00:09:46,740
seeing the kubernetes is becoming very

00:09:43,830 --> 00:09:51,540
kind of like active in this area on top

00:09:46,740 --> 00:09:53,339
of that what we also seeing we we see

00:09:51,540 --> 00:09:57,480
the rise of kind of like the deep

00:09:53,339 --> 00:10:00,209
learning platforms and as we saw some of

00:09:57,480 --> 00:10:02,220
these difference on frameworks and stuff

00:10:00,209 --> 00:10:04,310
a try people have been trying to

00:10:02,220 --> 00:10:07,560
abstract that and that's how the

00:10:04,310 --> 00:10:14,640
platform's have kind of like come into

00:10:07,560 --> 00:10:17,850
play here and another thing is if you're

00:10:14,640 --> 00:10:20,850
doing the kind of like interactive

00:10:17,850 --> 00:10:24,029
development that we usually kind of like

00:10:20,850 --> 00:10:27,450
so use it with notebooks and you may be

00:10:24,029 --> 00:10:28,980
locking a GPU for like a day while

00:10:27,450 --> 00:10:32,400
you're just doing dirt of development

00:10:28,980 --> 00:10:35,910
it's not very cost-effective for the

00:10:32,400 --> 00:10:37,010
platform so you if you want to actually

00:10:35,910 --> 00:10:38,120
start

00:10:37,010 --> 00:10:40,330
supporting more people you have to

00:10:38,120 --> 00:10:44,260
figure out a different way of doing that

00:10:40,330 --> 00:10:47,720
the problem with these deep learning

00:10:44,260 --> 00:10:50,060
platforms it's kind of like go away from

00:10:47,720 --> 00:10:52,970
the interactive development into a more

00:10:50,060 --> 00:10:55,060
batch oriented development which as data

00:10:52,970 --> 00:10:58,160
scientists coming from the notebook

00:10:55,060 --> 00:11:00,440
interactive development we not very fun

00:10:58,160 --> 00:11:02,420
of that we want to get the response we

00:11:00,440 --> 00:11:07,570
want to get kind of like the immediate

00:11:02,420 --> 00:11:10,850
result for the things that we are doing

00:11:07,570 --> 00:11:12,620
or just going back here on the right

00:11:10,850 --> 00:11:14,000
side we see some of the examples of the

00:11:12,620 --> 00:11:17,330
platforms that we are seeing coming up

00:11:14,000 --> 00:11:20,510
so a fiddle is one of the projects that

00:11:17,330 --> 00:11:21,380
we open search at IBM recently a few

00:11:20,510 --> 00:11:23,060
months ago

00:11:21,380 --> 00:11:25,610
it's got like fabric for deep learning

00:11:23,060 --> 00:11:28,040
we just mentioned fiddle what since

00:11:25,610 --> 00:11:30,760
Watson studio have a deep learning as a

00:11:28,040 --> 00:11:35,330
service as well that you can actually

00:11:30,760 --> 00:11:37,250
use hosted at IBM and it provides the

00:11:35,330 --> 00:11:39,950
hardware for you to train with GPUs and

00:11:37,250 --> 00:11:43,640
everything we also see coop flow that

00:11:39,950 --> 00:11:45,290
comes from Google and it's kind of like

00:11:43,640 --> 00:11:48,800
very similar to field all in terms of

00:11:45,290 --> 00:11:54,200
like capabilities and the hardware and

00:11:48,800 --> 00:11:56,390
all the frameworks that it supports so

00:11:54,200 --> 00:11:58,760
what we have started working is this

00:11:56,390 --> 00:12:03,230
notion of like deep learning workspaces

00:11:58,760 --> 00:12:04,610
and we see that there is the difference

00:12:03,230 --> 00:12:07,340
between kind of like a interactive

00:12:04,610 --> 00:12:09,920
development and kind of like the batch

00:12:07,340 --> 00:12:13,220
development they usually have a package

00:12:09,920 --> 00:12:16,250
of requirements so we you can't really

00:12:13,220 --> 00:12:18,920
like just submit a notebook you have to

00:12:16,250 --> 00:12:21,050
have like a Python files in a certain

00:12:18,920 --> 00:12:24,170
format and they diverge from one

00:12:21,050 --> 00:12:26,630
platform to another how do you start

00:12:24,170 --> 00:12:28,250
solving that so if the diplom workspaces

00:12:26,630 --> 00:12:31,130
we started to kind of like try to

00:12:28,250 --> 00:12:32,930
streamline and abstract all of that from

00:12:31,130 --> 00:12:35,300
the data science that really just want

00:12:32,930 --> 00:12:38,510
to make sure that they work on the

00:12:35,300 --> 00:12:40,490
notebook and make all of that kind of

00:12:38,510 --> 00:12:44,860
like the models and and the application

00:12:40,490 --> 00:12:44,860
in general are working ok

00:12:46,360 --> 00:12:54,440
so the idea here is that you start doing

00:12:52,310 --> 00:12:59,270
your interactive development on the

00:12:54,440 --> 00:13:01,910
notebook as you usually do you use a

00:12:59,270 --> 00:13:03,710
certain kind of like sample data and you

00:13:01,910 --> 00:13:10,940
tweak your parameters so that you're not

00:13:03,710 --> 00:13:13,190
really like going to the optimal kind of

00:13:10,940 --> 00:13:15,260
like a results you just want to make

00:13:13,190 --> 00:13:17,300
sure that the logic that you have the

00:13:15,260 --> 00:13:19,850
model that you have kind of works okay

00:13:17,300 --> 00:13:22,279
and when you're ready to kind of like

00:13:19,850 --> 00:13:25,070
start experimenting with the full data

00:13:22,279 --> 00:13:29,600
set you start submitting and schedule

00:13:25,070 --> 00:13:31,700
those jobs as notebooks into kind of

00:13:29,600 --> 00:13:36,140
like a deep learning as a service

00:13:31,700 --> 00:13:38,440
platform the beauty here is also that we

00:13:36,140 --> 00:13:42,830
are not really kind of like a

00:13:38,440 --> 00:13:45,260
decomposing your notebook into like

00:13:42,830 --> 00:13:48,170
maybe Python files or any kind of like

00:13:45,260 --> 00:13:51,560
different infrastructure to submit to

00:13:48,170 --> 00:13:53,230
those we do the packaging we do all the

00:13:51,560 --> 00:13:56,990
requirements that those platforms

00:13:53,230 --> 00:14:00,230
require but when we actually start the

00:13:56,990 --> 00:14:03,790
training we are bootstrapping kind of

00:14:00,230 --> 00:14:07,280
like an internal enterprise gateway

00:14:03,790 --> 00:14:09,350
server that starts the kernel and then

00:14:07,280 --> 00:14:13,339
we start submitting all the cells that

00:14:09,350 --> 00:14:16,610
you have to that kernel to get executed

00:14:13,339 --> 00:14:18,380
the benefit of that is that you are on

00:14:16,610 --> 00:14:22,550
the same environment where you were

00:14:18,380 --> 00:14:23,990
developing your model you can use and

00:14:22,550 --> 00:14:26,839
everything that all the cells and all

00:14:23,990 --> 00:14:29,060
the magics other things that you're used

00:14:26,839 --> 00:14:32,510
to and and works on your notebook it

00:14:29,060 --> 00:14:34,970
will also work on on the batch kind of

00:14:32,510 --> 00:14:37,850
like environment because you basically

00:14:34,970 --> 00:14:41,020
just on the same kind of like notebook

00:14:37,850 --> 00:14:44,420
environment what I've seen in the past

00:14:41,020 --> 00:14:47,089
people trying to kind of like work

00:14:44,420 --> 00:14:48,950
around this these differences is that

00:14:47,089 --> 00:14:51,830
okay they will read a notebook maybe

00:14:48,950 --> 00:14:55,310
generate a Python script and execute

00:14:51,830 --> 00:14:56,560
that Python script into into a different

00:14:55,310 --> 00:14:58,870
platform

00:14:56,560 --> 00:15:01,480
but then like if you not book you have

00:14:58,870 --> 00:15:03,760
any kind of like I mean we we usually

00:15:01,480 --> 00:15:05,110
become creative into the notebooks and

00:15:03,760 --> 00:15:07,450
if you for example you're doing a double

00:15:05,110 --> 00:15:10,210
you get to get some dependency or small

00:15:07,450 --> 00:15:13,630
file or anything out of the ordinary

00:15:10,210 --> 00:15:15,100
then it's not gonna be part of the the

00:15:13,630 --> 00:15:17,710
script because it's not kind of like a

00:15:15,100 --> 00:15:20,230
Python source code then you start

00:15:17,710 --> 00:15:22,200
getting a lot of like problems this kind

00:15:20,230 --> 00:15:27,850
of like by just executing the notebook

00:15:22,200 --> 00:15:30,520
everything will just work for you the

00:15:27,850 --> 00:15:34,210
way we are thinking about this is that

00:15:30,520 --> 00:15:36,430
the schedule becomes a service and what

00:15:34,210 --> 00:15:38,529
happens with that is also you can kind

00:15:36,430 --> 00:15:40,360
of like a secondary problem that we see

00:15:38,529 --> 00:15:42,700
on the industry is that okay I have my

00:15:40,360 --> 00:15:46,060
notebook and I want to start production

00:15:42,700 --> 00:15:48,339
on lies in that notebook meaning I maybe

00:15:46,060 --> 00:15:50,860
I want to run like like every day or

00:15:48,339 --> 00:15:53,440
every morning every hour how do I do

00:15:50,860 --> 00:15:56,080
that today how do we have anything that

00:15:53,440 --> 00:15:57,940
can handle that for us and and the

00:15:56,080 --> 00:16:00,250
scheduler that kind of like takes the

00:15:57,940 --> 00:16:02,680
job from the notebook can also like just

00:16:00,250 --> 00:16:04,900
as a regular REST API start taking like

00:16:02,680 --> 00:16:09,100
just regular notebooks to be scheduled

00:16:04,900 --> 00:16:11,350
and as you can kind of the platform that

00:16:09,100 --> 00:16:14,830
you want to run becomes a choice it's

00:16:11,350 --> 00:16:16,810
it's simple to say oh I just don't need

00:16:14,830 --> 00:16:19,330
any expensive environment with GPUs I

00:16:16,810 --> 00:16:21,520
just want to run in a pane kind of like

00:16:19,330 --> 00:16:23,050
Jupiter environment or I want to run on

00:16:21,520 --> 00:16:29,770
a deep learning as a service so that

00:16:23,050 --> 00:16:35,560
that becomes a choice for you okay so

00:16:29,770 --> 00:16:37,810
that kind of like demos a little or or

00:16:35,560 --> 00:16:41,200
tell us about a little bit I think

00:16:37,810 --> 00:16:44,350
better is to kind of like close with a

00:16:41,200 --> 00:16:47,260
demo where we can actually see how

00:16:44,350 --> 00:16:50,380
things work and if you guys have any

00:16:47,260 --> 00:16:52,990
questions while I switch to the demo or

00:16:50,380 --> 00:16:56,110
after the demo we can take a little bit

00:16:52,990 --> 00:16:58,529
more time to go of over questions so let

00:16:56,110 --> 00:16:58,529
me just

00:16:59,060 --> 00:17:08,750
stop here that didn't make a little

00:17:03,300 --> 00:17:08,750
simpler for me by doing the mirror

00:17:20,560 --> 00:17:30,490
so what I did here I just started a

00:17:23,550 --> 00:17:33,640
notebook so regular just took their

00:17:30,490 --> 00:17:36,850
notebook my write my UI here let's take

00:17:33,640 --> 00:17:40,480
an example of a simple notebook let me

00:17:36,850 --> 00:17:48,010
try to make it a little bit bigger for

00:17:40,480 --> 00:17:50,260
you guys to take a look okay as a data

00:17:48,010 --> 00:17:54,610
scientist usually I'm just gonna kind of

00:17:50,260 --> 00:17:57,100
like do the regular execution I can go

00:17:54,610 --> 00:17:59,260
I'll do run all just to see if

00:17:57,100 --> 00:18:04,600
everything kind of still works or it's

00:17:59,260 --> 00:18:06,330
kind of like the way I wanted to see so

00:18:04,600 --> 00:18:08,590
I start seeing some of the results here

00:18:06,330 --> 00:18:11,860
this is just a simple notebook that I

00:18:08,590 --> 00:18:15,190
have and now I think that kind of like

00:18:11,860 --> 00:18:16,870
I'm ready I want to start at executing

00:18:15,190 --> 00:18:20,260
that in the context of like a very large

00:18:16,870 --> 00:18:24,550
data set and I want to use some

00:18:20,260 --> 00:18:26,830
expensive resources like GPUs so we have

00:18:24,550 --> 00:18:29,290
extended a notebook we have this

00:18:26,830 --> 00:18:34,450
extension where you can submit a

00:18:29,290 --> 00:18:35,890
notebook when you come here we start

00:18:34,450 --> 00:18:37,030
seeing that we mentioned different

00:18:35,890 --> 00:18:39,850
platforms here

00:18:37,030 --> 00:18:42,160
so it started becoming a choice do I

00:18:39,850 --> 00:18:44,950
want to read them do I want to deploy

00:18:42,160 --> 00:18:47,910
and submit this to Phaedo do I want to

00:18:44,950 --> 00:18:50,770
use a deal S which is a hosted platform

00:18:47,910 --> 00:18:52,870
do I just want to run on a simple docker

00:18:50,770 --> 00:18:56,020
where it can parameterize some of the

00:18:52,870 --> 00:18:57,580
stuff or in a Jupiter just a Jupiter a

00:18:56,020 --> 00:19:00,240
regular Jupiter environment that is

00:18:57,580 --> 00:19:00,240
available remotely

00:19:00,370 --> 00:19:05,050
once you define the platform that you

00:19:02,140 --> 00:19:07,480
want you start having a couple of extra

00:19:05,050 --> 00:19:11,070
parameters so in this case what is the

00:19:07,480 --> 00:19:16,950
API endpoint to submit to that fiddle

00:19:11,070 --> 00:19:19,360
platform what what is kind of like the

00:19:16,950 --> 00:19:21,970
framework you want to use so like you

00:19:19,360 --> 00:19:25,210
wanna use tensorflow kapha pi torch

00:19:21,970 --> 00:19:28,330
kapha - what are some of the resources

00:19:25,210 --> 00:19:31,090
that you want I want to give to CPUs or

00:19:28,330 --> 00:19:32,120
or just one CPU but I want to add one

00:19:31,090 --> 00:19:36,860
GPU

00:19:32,120 --> 00:19:38,930
you can play with that incase of fiddle

00:19:36,860 --> 00:19:41,240
you wanted to give a extra information

00:19:38,930 --> 00:19:44,750
which is kind of like where some of

00:19:41,240 --> 00:19:47,900
these data is available they use a cloud

00:19:44,750 --> 00:19:49,670
object storage to support that that

00:19:47,900 --> 00:19:51,770
cloud object storage doesn't need to be

00:19:49,670 --> 00:19:53,840
actually on a cloud it can be like on

00:19:51,770 --> 00:19:56,780
your own premise or where the platform

00:19:53,840 --> 00:19:58,610
is deployed and I think that we have

00:19:56,780 --> 00:20:01,790
seen recently and this is a recent

00:19:58,610 --> 00:20:03,830
addition that I just added before coming

00:20:01,790 --> 00:20:05,030
to the conference is that usually when

00:20:03,830 --> 00:20:10,370
you're doing deep learning you might

00:20:05,030 --> 00:20:13,340
have a couple of notebook models but you

00:20:10,370 --> 00:20:15,530
also have some Python files that you

00:20:13,340 --> 00:20:18,890
just import and use those as kind of

00:20:15,530 --> 00:20:21,710
like a utility functions so you can also

00:20:18,890 --> 00:20:24,020
say okay include some of these

00:20:21,710 --> 00:20:27,440
dependencies you can say all the Python

00:20:24,020 --> 00:20:30,740
files or you can give specific file

00:20:27,440 --> 00:20:32,960
names it's all comma separated and when

00:20:30,740 --> 00:20:35,210
we package the notebook into the zip

00:20:32,960 --> 00:20:37,940
file we're also going to have all these

00:20:35,210 --> 00:20:40,130
dependencies kind of like co-located

00:20:37,940 --> 00:20:44,060
there so when you run your notebook and

00:20:40,130 --> 00:20:46,610
it depends on that extra files those

00:20:44,060 --> 00:20:50,470
files are going to be available into

00:20:46,610 --> 00:20:52,990
into the environment for you as well

00:20:50,470 --> 00:20:56,360
so I'm just gonna head in publish here

00:20:52,990 --> 00:20:58,340
when I'm publishing what is happening

00:20:56,360 --> 00:21:02,870
under the covers is depending on the

00:20:58,340 --> 00:21:05,050
platform we we we understand all the

00:21:02,870 --> 00:21:08,030
requirements and the packaging that is

00:21:05,050 --> 00:21:11,330
needed for that platform we do that

00:21:08,030 --> 00:21:13,970
completely kind of like in a single

00:21:11,330 --> 00:21:16,160
click type of thing without requiring

00:21:13,970 --> 00:21:19,610
the data scientists to understand or

00:21:16,160 --> 00:21:26,690
know any of that and it gets submitted

00:21:19,610 --> 00:21:31,430
to the remote platform we then can see

00:21:26,690 --> 00:21:34,280
kind of like for now we are just

00:21:31,430 --> 00:21:36,620
integrating with the console from the

00:21:34,280 --> 00:21:39,020
platform we are looking to start

00:21:36,620 --> 00:21:41,420
bringing those information more kind of

00:21:39,020 --> 00:21:43,520
like a panel into like a Jupiter lab

00:21:41,420 --> 00:21:45,110
more type of interface to really build a

00:21:43,520 --> 00:21:50,030
workspace for you

00:21:45,110 --> 00:21:51,740
and you can see at the bottom the one

00:21:50,030 --> 00:21:56,890
that is spending that we have just

00:21:51,740 --> 00:22:01,520
submitted you can for example click on

00:21:56,890 --> 00:22:04,990
more information go into the logs and

00:22:01,520 --> 00:22:08,810
what you're seeing here is basically it

00:22:04,990 --> 00:22:10,970
installs enterprise gateway right right

00:22:08,810 --> 00:22:14,330
away for you it will bootstrap that

00:22:10,970 --> 00:22:16,910
environment execute the notebook and

00:22:14,330 --> 00:22:18,770
then make the results available when

00:22:16,910 --> 00:22:23,780
it's executing is just create a kernel

00:22:18,770 --> 00:22:26,660
start sending the the cells in and start

00:22:23,780 --> 00:22:28,540
processing the results here we've wait a

00:22:26,660 --> 00:22:36,380
little bit more we'll see kind of like

00:22:28,540 --> 00:22:42,740
all the information coming back and that

00:22:36,380 --> 00:22:48,460
is about it so like if you guys let me

00:22:42,740 --> 00:22:48,460
go back to to the slide here

00:22:52,710 --> 00:22:59,380
yeah so this is kind of like just some

00:22:55,480 --> 00:23:02,950
screenshots of the exactly demo that I

00:22:59,380 --> 00:23:05,650
just did so I'm mostly kind of like open

00:23:02,950 --> 00:23:08,410
for question I'm pretty sure you guys

00:23:05,650 --> 00:23:12,610
want to have no more details of specific

00:23:08,410 --> 00:23:16,110
areas so we have a question there just

00:23:12,610 --> 00:23:16,110
maybe just wait for the microphone

00:23:31,140 --> 00:23:34,140
yes

00:23:43,090 --> 00:23:53,000
so so what is gonna happen that

00:23:50,360 --> 00:23:55,610
particularly on case of fiddle is gonna

00:23:53,000 --> 00:23:57,460
have a a error during the submission and

00:23:55,610 --> 00:24:00,200
you're gonna have kind of like a

00:23:57,460 --> 00:24:04,430
detailed of like why it didn't get

00:24:00,200 --> 00:24:07,340
accepted by the framework I don't think

00:24:04,430 --> 00:24:09,620
I saw any api's on the platform that we

00:24:07,340 --> 00:24:12,710
are integrating that kind of like give

00:24:09,620 --> 00:24:15,110
us some of that feedback but it gets

00:24:12,710 --> 00:24:18,940
rejected and you have the information

00:24:15,110 --> 00:24:18,940
that why you got to reject it

00:24:20,330 --> 00:24:31,460
question here microphone oh okay sorry

00:24:26,890 --> 00:24:33,020
okay here we oh so I that's the first

00:24:31,460 --> 00:24:34,670
time I've seen Jupiter enterprise

00:24:33,020 --> 00:24:36,590
gateway and I had what I'm sure is a

00:24:34,670 --> 00:24:38,570
very simple question about that can you

00:24:36,590 --> 00:24:42,020
describe a little bit more how it works

00:24:38,570 --> 00:24:44,930
you have a a notebook you I still but

00:24:42,020 --> 00:24:50,210
with kernels deployed let's say in the

00:24:44,930 --> 00:24:52,580
kubernetes option as pods remotely and

00:24:50,210 --> 00:24:54,710
doing remote execution for a UI that

00:24:52,580 --> 00:24:56,140
lives in a different container pod

00:24:54,710 --> 00:25:01,870
somewhere how does that actually work

00:24:56,140 --> 00:25:01,870
correct so uh I think I have

00:25:06,400 --> 00:25:11,830
for those that don't know what took the

00:25:08,860 --> 00:25:14,830
enterprise gateway does in a very

00:25:11,830 --> 00:25:18,730
high-level summary it enables remote

00:25:14,830 --> 00:25:21,910
kernels for Jupiter notebooks and we

00:25:18,730 --> 00:25:23,920
support spark on only yarn so like if

00:25:21,910 --> 00:25:25,750
you guys familiar with spark we start

00:25:23,920 --> 00:25:27,790
running on cluster mode

00:25:25,750 --> 00:25:30,190
it supports like spark and kubernetes

00:25:27,790 --> 00:25:32,230
playing kubernetes spectrum conductor

00:25:30,190 --> 00:25:36,030
and research managers become kind of

00:25:32,230 --> 00:25:38,770
like a choice or very a pluggable layer

00:25:36,030 --> 00:25:41,050
in that case it's almost like what you

00:25:38,770 --> 00:25:43,600
were saying what we we started doing so

00:25:41,050 --> 00:25:46,840
like regular notebooks they run kernels

00:25:43,600 --> 00:25:49,900
as local processes by connecting with

00:25:46,840 --> 00:25:52,630
enterprise gateway those kernels start

00:25:49,900 --> 00:25:55,600
being the lifecycle and the management

00:25:52,630 --> 00:25:58,390
of those remote kernels submitted into a

00:25:55,600 --> 00:26:01,330
cluster starts becoming managed by

00:25:58,390 --> 00:26:02,830
enterprise gateway and then for the

00:26:01,330 --> 00:26:04,929
notebook the kind of the user experience

00:26:02,830 --> 00:26:08,020
it's really kind of like just the same

00:26:04,929 --> 00:26:09,970
but those kernels are distributed the

00:26:08,020 --> 00:26:11,500
benefit with that is instead of like

00:26:09,970 --> 00:26:14,410
just using the resources of like one

00:26:11,500 --> 00:26:16,150
machine and only being able to maybe

00:26:14,410 --> 00:26:19,990
have like two or three data scientists

00:26:16,150 --> 00:26:24,010
doing Big Data you now have the whole

00:26:19,990 --> 00:26:28,980
cluster resources available so you can

00:26:24,010 --> 00:26:32,290
start having like a much more users

00:26:28,980 --> 00:26:34,780
supported and if you need more you start

00:26:32,290 --> 00:26:36,700
adding more nodes in those nodes then

00:26:34,780 --> 00:26:39,480
start becoming resources available for

00:26:36,700 --> 00:26:39,480
running the kernels

00:26:44,300 --> 00:26:51,059
correct what what happens in that case

00:26:48,260 --> 00:26:54,240
when you actually come in and say new

00:26:51,059 --> 00:26:57,059
notebook Python that is a configuration

00:26:54,240 --> 00:26:59,870
that we call kernel specs on the kernel

00:26:57,059 --> 00:27:03,809
specs you switch the kernel manager in

00:26:59,870 --> 00:27:06,929
that will do the life cycle on the

00:27:03,809 --> 00:27:09,990
remote and we have like kernel managers

00:27:06,929 --> 00:27:11,340
for let's say yarn kubernetes and

00:27:09,990 --> 00:27:14,520
depending on the platform and then it

00:27:11,340 --> 00:27:16,770
understands how to submit and connect to

00:27:14,520 --> 00:27:21,500
those remote Carlos it but the user is

00:27:16,770 --> 00:27:21,500
completely transparent thank you

00:27:23,820 --> 00:27:26,840
any other question

00:27:34,580 --> 00:27:40,080
so if you guys want to kind of like

00:27:37,770 --> 00:27:44,250
start playing with this I'll leave it

00:27:40,080 --> 00:27:47,580
here kind of the two repositories that

00:27:44,250 --> 00:27:49,890
we have been working at the moment so

00:27:47,580 --> 00:27:52,970
you can start playing with that if you

00:27:49,890 --> 00:27:56,429
guys have questions just feel free to

00:27:52,970 --> 00:27:58,650
connect to me we also have Kevin Bates

00:27:56,429 --> 00:28:01,020
that is working on the same project we

00:27:58,650 --> 00:28:03,690
with meat as well so just feel free to

00:28:01,020 --> 00:28:06,390
reach us we either around we we're gonna

00:28:03,690 --> 00:28:10,140
be on the IBM booth as well and we have

00:28:06,390 --> 00:28:12,020
a meet-up tonight that we're gonna talk

00:28:10,140 --> 00:28:16,309
a little bit more on these areas as well

00:28:12,020 --> 00:28:19,860
so if you guys have more questions

00:28:16,309 --> 00:28:20,880
otherwise thank you very much thanks for

00:28:19,860 --> 00:28:22,260
coming

00:28:20,880 --> 00:28:23,190
please let me know if you guys have any

00:28:22,260 --> 00:28:26,940
other Thanks

00:28:23,190 --> 00:28:26,940

YouTube URL: https://www.youtube.com/watch?v=8ZOEHATuicY


