Title: Visualizing machine learning models in the Jupyter Notebook- Chakri Cherukuri (Bloomberg LP)
Publication date: 2018-09-20
Playlist: JupyterCon in New York 2018
Description: 
	Chakri Cherukuri offers an overview of the interactive widget ecosystem available in the Jupyter notebook, including ipywidgets and bqplot, and illustrates how Jupyter widgets can be used to build rich visualizations of machine learning models. Along the way, Chakri walks you through algorithms like regression, clustering, and optimization and shares a wizard for building and training deep learning models with diagnostic plots.

This session is sponsored by Bloomberg LP.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,390 --> 00:00:06,700
hello everyone I am Czech lechera Cory I

00:00:04,410 --> 00:00:09,550
work in the quant research group at

00:00:06,700 --> 00:00:11,410
Bloomberg in New York City we

00:00:09,550 --> 00:00:14,830
extensively use Jupiter notebooks for

00:00:11,410 --> 00:00:17,590
all our research projects and we also

00:00:14,830 --> 00:00:20,470
contribute to the Jupiter project mainly

00:00:17,590 --> 00:00:22,900
on the widget libraries we created a 2d

00:00:20,470 --> 00:00:27,550
plotting library called BQ plot which is

00:00:22,900 --> 00:00:29,949
open sourced on github in today's talk

00:00:27,550 --> 00:00:33,900
we'll see how we can visualize machine

00:00:29,949 --> 00:00:33,900
learning models in the Jupiter notebook

00:00:34,590 --> 00:00:39,970
the outline of my talk today is as

00:00:37,150 --> 00:00:42,790
follows I'll first give a brief overview

00:00:39,970 --> 00:00:46,030
of the widget libraries I'll be using in

00:00:42,790 --> 00:00:48,130
my example notebooks and then I'll walk

00:00:46,030 --> 00:00:50,740
you through the code of a simple example

00:00:48,130 --> 00:00:52,360
of an interactive plot so that you

00:00:50,740 --> 00:00:54,820
understand it a high-level how to

00:00:52,360 --> 00:00:58,360
assemble these widgets and how to link

00:00:54,820 --> 00:00:59,860
them to create interactivity and then

00:00:58,360 --> 00:01:03,790
we'll go through a few machine learning

00:00:59,860 --> 00:01:05,650
examples and algorithms I'll start with

00:01:03,790 --> 00:01:07,290
theoretical machine learning examples

00:01:05,650 --> 00:01:10,360
like kernel regression

00:01:07,290 --> 00:01:12,490
we'll see how interactive visualizations

00:01:10,360 --> 00:01:15,040
can help us better understand and

00:01:12,490 --> 00:01:17,140
interpret these models and we'll also

00:01:15,040 --> 00:01:20,650
see the impact of hyper parameters on

00:01:17,140 --> 00:01:24,060
the regression fit and then I will show

00:01:20,650 --> 00:01:27,100
a nice animation of k-means clustering

00:01:24,060 --> 00:01:30,640
both these examples are implemented in

00:01:27,100 --> 00:01:33,100
Nampa right in the notebook and then we

00:01:30,640 --> 00:01:36,220
look at more advanced and applied use

00:01:33,100 --> 00:01:38,320
cases we look at dimensionality

00:01:36,220 --> 00:01:41,680
reduction techniques applied on the Ã®le

00:01:38,320 --> 00:01:44,680
curve data set and finally we look at a

00:01:41,680 --> 00:01:47,320
graphical tool for doing the end-to-end

00:01:44,680 --> 00:01:48,640
pipeline of a deep learning model right

00:01:47,320 --> 00:01:51,909
from selecting the network parameters

00:01:48,640 --> 00:01:53,740
and setting up the network architecture

00:01:51,909 --> 00:01:58,030
and the whole training process and

00:01:53,740 --> 00:02:00,600
diagnostic plots we did a similar talk

00:01:58,030 --> 00:02:04,150
last year where we showed examples of

00:02:00,600 --> 00:02:06,310
advanced applications and dashboards but

00:02:04,150 --> 00:02:08,170
unfortunately all the examples we showed

00:02:06,310 --> 00:02:10,179
at that time for Bloomberg proprietary

00:02:08,170 --> 00:02:12,850
so I could not shave the notebooks so

00:02:10,179 --> 00:02:14,100
for today's talk all my example

00:02:12,850 --> 00:02:16,770
notebooks

00:02:14,100 --> 00:02:19,440
and all the code is available online on

00:02:16,770 --> 00:02:21,540
my github account so please feel free to

00:02:19,440 --> 00:02:23,490
download the examples and you should be

00:02:21,540 --> 00:02:30,480
able to rebuild all the examples which

00:02:23,490 --> 00:02:33,180
I'm going to show today okay the widget

00:02:30,480 --> 00:02:36,330
libraries I'll be using today in my

00:02:33,180 --> 00:02:39,390
example notebooks are I buy widgets so I

00:02:36,330 --> 00:02:42,810
buy widgets our core UI controls like

00:02:39,390 --> 00:02:45,750
text boxes sliders drop-down menus

00:02:42,810 --> 00:02:47,910
etcetera and then Vic you plot

00:02:45,750 --> 00:02:50,430
Vic you plot are again 2d plotting

00:02:47,910 --> 00:02:53,250
widgets which are built on top of the

00:02:50,430 --> 00:02:55,080
same I buy widgets infrastructure so

00:02:53,250 --> 00:02:57,480
they use the same trait lick mechanism

00:02:55,080 --> 00:03:00,330
for binding the attributes of widgets

00:02:57,480 --> 00:03:02,700
so BQ plot and I PI V Jets can be

00:03:00,330 --> 00:03:07,440
seamlessly integrated to build a rich

00:03:02,700 --> 00:03:25,560
interactive visualizations let's now

00:03:07,440 --> 00:03:28,980
look at an example so so here I have an

00:03:25,560 --> 00:03:32,100
example of a simple plot of a Gaussian

00:03:28,980 --> 00:03:34,650
density and we want to see the impact of

00:03:32,100 --> 00:03:36,240
mu and Sigma the location and scale

00:03:34,650 --> 00:03:39,090
parameter on the plot of the Gaussian

00:03:36,240 --> 00:03:41,160
density let me walk you through the code

00:03:39,090 --> 00:03:43,890
I'm doing some imports from my PI

00:03:41,160 --> 00:03:47,550
widgets and you notice that I'm using

00:03:43,890 --> 00:03:49,410
the PI plot API of BQ plot which is very

00:03:47,550 --> 00:03:51,990
similar to the matplotlib spy plot

00:03:49,410 --> 00:03:58,650
interface it's a simple context based

00:03:51,990 --> 00:04:02,010
API so I'm creating two vectors x and y

00:03:58,650 --> 00:04:06,360
is just the norm PDF of X and then I

00:04:02,010 --> 00:04:09,570
create the PLT dot figure which returns

00:04:06,360 --> 00:04:12,960
a figure widget and then I'm calling PLT

00:04:09,570 --> 00:04:16,049
dot plot which renders a line plot I'm

00:04:12,960 --> 00:04:18,480
passing the x and y attributes and this

00:04:16,049 --> 00:04:21,540
returns the line object which I'm

00:04:18,480 --> 00:04:23,880
storing in PDF underscore line right we

00:04:21,540 --> 00:04:26,820
lead will need this line object later to

00:04:23,880 --> 00:04:27,970
update the Y attribute and then I'm

00:04:26,820 --> 00:04:31,600
creating two floats

00:04:27,970 --> 00:04:34,060
idols to represent mu and Sigma and I'm

00:04:31,600 --> 00:04:36,340
going to use an H box which is a layout

00:04:34,060 --> 00:04:40,240
class in I pi widgets to horizontally

00:04:36,340 --> 00:04:42,760
stack these widgets so you see that you

00:04:40,240 --> 00:04:45,760
have two nice sliders here representing

00:04:42,760 --> 00:04:48,160
mu and Sigma and finally I want to put

00:04:45,760 --> 00:04:57,190
all the widgets in a nice layout using a

00:04:48,160 --> 00:05:00,190
V box oops so here we have a nice simple

00:04:57,190 --> 00:05:03,240
dashboard where we have the plot of the

00:05:00,190 --> 00:05:05,440
Gaussian density and we have two sliders

00:05:03,240 --> 00:05:08,410
obviously nothing happens when I move

00:05:05,440 --> 00:05:12,820
these sliders so let's go ahead and link

00:05:08,410 --> 00:05:14,860
them and this is the crucial step to

00:05:12,820 --> 00:05:17,800
link widgets I first need to define a

00:05:14,860 --> 00:05:20,710
callback so I'm creating a callback

00:05:17,800 --> 00:05:23,050
called update density where I'm fetching

00:05:20,710 --> 00:05:27,490
the new values of mu and Sigma from the

00:05:23,050 --> 00:05:30,040
sliders and then I recreate the norm PDF

00:05:27,490 --> 00:05:32,669
using the new values of mu and Sigma and

00:05:30,040 --> 00:05:35,350
then I assign the output of that

00:05:32,669 --> 00:05:39,160
directly to the Y attribute of the PDF

00:05:35,350 --> 00:05:41,590
line and this is a crucial step in I pi

00:05:39,160 --> 00:05:44,979
widgets and all the widget libraries we

00:05:41,590 --> 00:05:47,110
always update the widgets in place we

00:05:44,979 --> 00:05:49,720
don't recreate them we don't really

00:05:47,110 --> 00:05:52,540
under them all we do is just update the

00:05:49,720 --> 00:05:55,650
attributes in line and in place and this

00:05:52,540 --> 00:05:57,790
triggers redraw in the plot and

00:05:55,650 --> 00:06:00,370
similarly I would like to update the

00:05:57,790 --> 00:06:03,760
title to reflect the new values of mu

00:06:00,370 --> 00:06:06,880
and Sigma and finally I need to register

00:06:03,760 --> 00:06:09,790
this callback with sliders so I do that

00:06:06,880 --> 00:06:12,880
using the observed method where what I'm

00:06:09,790 --> 00:06:15,760
doing here is once I update the value

00:06:12,880 --> 00:06:18,160
attribute of the slider I update when I

00:06:15,760 --> 00:06:21,130
move the slider right I would like to

00:06:18,160 --> 00:06:24,760
call this invoke this callback which

00:06:21,130 --> 00:06:29,140
redraws the plot so that's it and now we

00:06:24,760 --> 00:06:34,960
have a nice interactive plot which

00:06:29,140 --> 00:06:36,370
responds to the sliders right so to

00:06:34,960 --> 00:06:39,669
summarize this is a very simple example

00:06:36,370 --> 00:06:41,169
but the concepts are very powerful once

00:06:39,669 --> 00:06:41,650
you understand how to assemble this

00:06:41,169 --> 00:06:43,930
video

00:06:41,650 --> 00:06:45,940
and how to link them you can just scale

00:06:43,930 --> 00:06:53,680
them up to build rich interactive

00:06:45,940 --> 00:06:56,680
visualizations let's now look at some

00:06:53,680 --> 00:07:00,970
machine learning examples the first

00:06:56,680 --> 00:07:03,070
example I have is kernel regression so

00:07:00,970 --> 00:07:05,320
kernel regression goes by different

00:07:03,070 --> 00:07:09,520
names you can call it local regression

00:07:05,320 --> 00:07:11,380
or nonparametric regression the idea

00:07:09,520 --> 00:07:14,620
here is we are trying to compute the

00:07:11,380 --> 00:07:17,590
conditional expectation of Y given X in

00:07:14,620 --> 00:07:19,560
a nonparametric way it's very similar to

00:07:17,590 --> 00:07:22,930
the standard polynomial regression

00:07:19,560 --> 00:07:25,690
except that instead of assigning equal

00:07:22,930 --> 00:07:28,270
weights to all the samples we create a

00:07:25,690 --> 00:07:32,110
localized behavior by assigning weights

00:07:28,270 --> 00:07:34,120
based on a kernel function so ordered

00:07:32,110 --> 00:07:36,280
zero is just a simple local weighted

00:07:34,120 --> 00:07:39,340
average with the weights coming from the

00:07:36,280 --> 00:07:42,220
kernel function and order 1 and above

00:07:39,340 --> 00:07:43,870
are weighted least squares compared to

00:07:42,220 --> 00:07:48,639
the ordinary least squares which we do

00:07:43,870 --> 00:07:50,620
in the polynomial regression now there

00:07:48,639 --> 00:07:52,240
are a lot of choices for the kernel

00:07:50,620 --> 00:07:54,910
functions and the most popular is the

00:07:52,240 --> 00:07:58,240
Gaussian kernel so Gaussian kernel has

00:07:54,910 --> 00:08:00,370
infinite support and the weights decay

00:07:58,240 --> 00:08:03,430
exponentially as we go far away pounds

00:08:00,370 --> 00:08:06,970
and this truly causes the localized

00:08:03,430 --> 00:08:10,090
behavior and we have a hyper parameter

00:08:06,970 --> 00:08:13,000
called bandwidth which controls the

00:08:10,090 --> 00:08:15,370
bias-variance tradeoff so let's now look

00:08:13,000 --> 00:08:20,310
at a visualization which helps us

00:08:15,370 --> 00:08:20,310
understand this local regression

00:08:31,469 --> 00:08:38,050
yeah so here we have a nice

00:08:35,680 --> 00:08:41,140
visualization of kernel regression let

00:08:38,050 --> 00:08:42,880
me explain the components I have a

00:08:41,140 --> 00:08:46,180
slider which controls the kernel

00:08:42,880 --> 00:08:48,190
bandwidth kernel bandwidth is the

00:08:46,180 --> 00:08:51,310
standard deviation of the of the waiting

00:08:48,190 --> 00:08:54,269
function and here we have the waiting

00:08:51,310 --> 00:08:57,220
function which is a Gaussian kernel and

00:08:54,269 --> 00:09:01,660
here I have the scatter plot of x and y

00:08:57,220 --> 00:09:03,930
and we have the regression fit and we

00:09:01,660 --> 00:09:11,110
can also control the polynomial order

00:09:03,930 --> 00:09:13,209
using this integer slider and I can also

00:09:11,110 --> 00:09:16,480
display nonparametric standard deviation

00:09:13,209 --> 00:09:21,339
banks which act as confidence intervals

00:09:16,480 --> 00:09:25,269
right so let's see the impact of this

00:09:21,339 --> 00:09:28,390
bandwidth on the regression fit then it

00:09:25,269 --> 00:09:30,880
reduces the bandwidth what happens is we

00:09:28,390 --> 00:09:32,860
get a truly localized behavior right the

00:09:30,880 --> 00:09:37,180
the Gaussian kernel almost becomes like

00:09:32,860 --> 00:09:39,519
a Dirac function and what happens is we

00:09:37,180 --> 00:09:41,250
totally over fit because this almost

00:09:39,519 --> 00:09:45,220
becomes like a 1 nearest neighbor right

00:09:41,250 --> 00:09:49,779
and as I increase the bandwidth we see

00:09:45,220 --> 00:09:53,050
that you're able to fit better but once

00:09:49,779 --> 00:09:55,060
I make the bandwidth really high like in

00:09:53,050 --> 00:09:56,829
theory when I make it infinity what

00:09:55,060 --> 00:09:59,140
happens is the Gaussian kernel becomes

00:09:56,829 --> 00:10:01,510
like a uniform distribution and you just

00:09:59,140 --> 00:10:03,940
recover the polynomial regression so

00:10:01,510 --> 00:10:07,570
order 0 is just the intercept and order

00:10:03,940 --> 00:10:10,779
1 is the linear model etc so as you can

00:10:07,570 --> 00:10:12,370
see we can clearly understand the impact

00:10:10,779 --> 00:10:15,550
of these hyper parameters on the

00:10:12,370 --> 00:10:19,630
regression fit so let me make this a

00:10:15,550 --> 00:10:22,209
reasonable value here and now let's see

00:10:19,630 --> 00:10:24,100
how we can improve the fit we can see

00:10:22,209 --> 00:10:26,800
that order 0 is doing a flat

00:10:24,100 --> 00:10:29,410
extrapolation and we see some linear

00:10:26,800 --> 00:10:33,670
bias in the data set here so let me make

00:10:29,410 --> 00:10:36,279
it order 1 and we see that this captures

00:10:33,670 --> 00:10:39,160
the linear bias in the data set but we

00:10:36,279 --> 00:10:43,680
also see some convexity bias here so if

00:10:39,160 --> 00:10:45,569
I make it order 2 you see that

00:10:43,680 --> 00:10:54,360
the order to polynomial captures this

00:10:45,569 --> 00:10:56,579
nicely and we can also DQ plot is

00:10:54,360 --> 00:10:58,139
interactive I can see the impact of

00:10:56,579 --> 00:11:02,699
outliers by clicking and adding points

00:10:58,139 --> 00:11:06,300
here you can drag points and see how

00:11:02,699 --> 00:11:08,490
extrapolation works right so it's very

00:11:06,300 --> 00:11:11,069
interactive you can play with this

00:11:08,490 --> 00:11:13,980
visualization to fully understand how

00:11:11,069 --> 00:11:16,860
this regression model works and you can

00:11:13,980 --> 00:11:19,019
also look at the the standard deviation

00:11:16,860 --> 00:11:20,670
bands to get a better picture on the

00:11:19,019 --> 00:11:24,149
confidence intervals surrounding this

00:11:20,670 --> 00:11:28,639
regression fit so that's all I have for

00:11:24,149 --> 00:11:36,360
local regression let's now look at

00:11:28,639 --> 00:11:39,480
k-means clustering here i'm implementing

00:11:36,360 --> 00:11:43,079
a alloys algorithm which is a heuristic

00:11:39,480 --> 00:11:46,589
algorithm for k-means clustering because

00:11:43,079 --> 00:11:48,569
the optimization problem is np-hard the

00:11:46,589 --> 00:11:52,050
algorithm is bound to get stuck in local

00:11:48,569 --> 00:11:54,179
minima and we'll see I created a retry

00:11:52,050 --> 00:11:57,050
button so that we can click and click on

00:11:54,179 --> 00:12:00,120
the button and try out different

00:11:57,050 --> 00:12:03,569
centroids to see if we can get out of

00:12:00,120 --> 00:12:07,129
the local minima the whole algorithm is

00:12:03,569 --> 00:12:07,129
implemented in Nampa right here

00:12:13,639 --> 00:12:17,959
so let me explain these sliders I have a

00:12:16,099 --> 00:12:20,059
slider called points to control the

00:12:17,959 --> 00:12:22,209
number of points and then I have a

00:12:20,059 --> 00:12:26,659
slider to control the number of clusters

00:12:22,209 --> 00:12:28,999
and I also have a slider to control the

00:12:26,659 --> 00:12:31,359
clustered standard deviation to see how

00:12:28,999 --> 00:12:37,399
closely or how far away the clusters are

00:12:31,359 --> 00:12:40,219
let's start the animation we see that in

00:12:37,399 --> 00:12:45,919
five iterations or six iterations it was

00:12:40,219 --> 00:12:47,539
able to okay here we see that the

00:12:45,919 --> 00:12:51,529
algorithm got stuck in the local minimum

00:12:47,539 --> 00:12:55,129
right so let me try to recenter the

00:12:51,529 --> 00:12:58,779
class the centroids and you immediately

00:12:55,129 --> 00:13:05,629
see that it was able to fix the problem

00:12:58,779 --> 00:13:07,549
we can now try four different yep

00:13:05,629 --> 00:13:13,939
again we see that the algorithm got

00:13:07,549 --> 00:13:16,249
stuck in the local minimum and we can

00:13:13,939 --> 00:13:20,629
try to recenter the cluster the

00:13:16,249 --> 00:13:25,449
centroids and hopefully that should fix

00:13:20,629 --> 00:13:28,369
the issue so as you can see this kind of

00:13:25,449 --> 00:13:30,649
interactive visualizations are extremely

00:13:28,369 --> 00:13:33,079
helpful in kind of understanding these

00:13:30,649 --> 00:13:36,379
kind of theoretical models and they can

00:13:33,079 --> 00:13:37,909
be used as training aids in helping

00:13:36,379 --> 00:13:40,329
explaining these kind of algorithms to

00:13:37,909 --> 00:13:40,329
students

00:13:44,860 --> 00:13:54,980
so so let's now look at some more

00:13:50,150 --> 00:13:58,280
applied examples we look at

00:13:54,980 --> 00:14:01,700
dimensionality reduction applied on

00:13:58,280 --> 00:14:06,080
yield curve data set so a quick primer

00:14:01,700 --> 00:14:08,360
on US Treasury yield curves US Treasury

00:14:06,080 --> 00:14:11,240
as you all know and shows bonds with

00:14:08,360 --> 00:14:12,710
different maturities so if you have the

00:14:11,240 --> 00:14:15,080
issue bonds with short-term maturities

00:14:12,710 --> 00:14:18,170
like you know one month three months or

00:14:15,080 --> 00:14:20,600
six months and they also issue bonds

00:14:18,170 --> 00:14:23,840
with long term maturities like 10 years

00:14:20,600 --> 00:14:25,790
20 years and 30 years so what is the new

00:14:23,840 --> 00:14:29,780
curve yield curve is just the plot of

00:14:25,790 --> 00:14:33,740
the interest rates on the y-axis and the

00:14:29,780 --> 00:14:37,760
maturity is on the x-axis right so US

00:14:33,740 --> 00:14:41,870
Treasury has 11 maturity bonds so the

00:14:37,760 --> 00:14:44,270
yield curve has 11 points and the data

00:14:41,870 --> 00:14:47,480
set I'm using here is the time series of

00:14:44,270 --> 00:14:50,120
these 11 interest rates because these

00:14:47,480 --> 00:14:52,880
time series they change every day right

00:14:50,120 --> 00:14:56,330
and you can easily download this data

00:14:52,880 --> 00:14:58,640
set from the US Treasury website if you

00:14:56,330 --> 00:15:01,460
look at this picture we see that the

00:14:58,640 --> 00:15:04,010
shape of the yield curve changes every

00:15:01,460 --> 00:15:07,730
day and especially with different

00:15:04,010 --> 00:15:11,000
regimes the shape totally changes for

00:15:07,730 --> 00:15:13,120
example if you look before the crisis we

00:15:11,000 --> 00:15:16,130
see that all the interest rates were

00:15:13,120 --> 00:15:18,620
pretty much hovering around 5% so the

00:15:16,130 --> 00:15:21,200
yield curve is pretty much flat and

00:15:18,620 --> 00:15:22,940
after the crisis because of quantitative

00:15:21,200 --> 00:15:25,340
easing all the short-term rates were

00:15:22,940 --> 00:15:29,660
pushed down so we have this kind of

00:15:25,340 --> 00:15:32,930
shape here right and recently the rates

00:15:29,660 --> 00:15:35,060
have been globally going up so we have a

00:15:32,930 --> 00:15:36,650
shape in yellow here which is somewhere

00:15:35,060 --> 00:15:41,060
in between the pre and post crisis

00:15:36,650 --> 00:15:44,710
levels so given this highly correlated

00:15:41,060 --> 00:15:47,450
11 dimensional data set the question is

00:15:44,710 --> 00:15:50,540
is there any parsimonious representation

00:15:47,450 --> 00:15:52,460
possible either to model the dynamics of

00:15:50,540 --> 00:15:58,160
the data set or for data visualization

00:15:52,460 --> 00:15:59,990
and we have couple of techniques the

00:15:58,160 --> 00:16:03,130
classical technique is called principle

00:15:59,990 --> 00:16:06,170
component analysis so what PCA does is

00:16:03,130 --> 00:16:09,370
it takes this 11 dimensional data set in

00:16:06,170 --> 00:16:12,020
this example and creates new factors

00:16:09,370 --> 00:16:15,080
which are linear combinations of all

00:16:12,020 --> 00:16:17,330
these 11 dimensional data set so each

00:16:15,080 --> 00:16:21,260
factor is a different linear combination

00:16:17,330 --> 00:16:24,230
of these leavened inputs and we form

00:16:21,260 --> 00:16:27,170
such linear combinations and typically

00:16:24,230 --> 00:16:28,850
we need three to five factors to explain

00:16:27,170 --> 00:16:30,980
almost all the variants in the data set

00:16:28,850 --> 00:16:35,720
and that's how we get the dimensionality

00:16:30,980 --> 00:16:37,370
reduction and you also have newer

00:16:35,720 --> 00:16:39,890
techniques which are based on deep

00:16:37,370 --> 00:16:42,770
learning there is a technique called

00:16:39,890 --> 00:16:45,050
Auto encoder which is based on a neural

00:16:42,770 --> 00:16:47,960
network and what we are trying to do

00:16:45,050 --> 00:16:52,070
here is we'll take the 11 dimensional

00:16:47,960 --> 00:16:55,520
input and try to compress it into two or

00:16:52,070 --> 00:16:57,500
three dimensions and this part of the

00:16:55,520 --> 00:16:59,960
network is called the encoder Network

00:16:57,500 --> 00:17:02,840
encoder where we try to create a

00:16:59,960 --> 00:17:07,310
compressed feature vector by reducing

00:17:02,840 --> 00:17:09,319
the dimensionality from 11 to 2 or 3 and

00:17:07,310 --> 00:17:12,850
in the decoder part we try to

00:17:09,319 --> 00:17:15,410
reconstruct the original yield curve by

00:17:12,850 --> 00:17:18,740
expanding the dimensions so this is

00:17:15,410 --> 00:17:20,949
called the decoder component and because

00:17:18,740 --> 00:17:23,689
it's a neural network I can create

00:17:20,949 --> 00:17:26,449
nonlinear activations after each layer

00:17:23,689 --> 00:17:29,780
to capture nonlinear dynamics in the

00:17:26,449 --> 00:17:32,150
data set for example if all my

00:17:29,780 --> 00:17:35,090
activations are linear I pretty much

00:17:32,150 --> 00:17:39,320
recover PCA because I'm just taking all

00:17:35,090 --> 00:17:41,750
the linear combinations right so PCA is

00:17:39,320 --> 00:17:43,730
a special case of an auto encoder and

00:17:41,750 --> 00:17:47,440
with auto-encoders we can also capture

00:17:43,730 --> 00:17:47,440
the nonlinearities in the data set

00:17:48,130 --> 00:17:54,650
before we try to apply PCA and auto

00:17:52,310 --> 00:17:57,620
encoder on the original 11 dimensional

00:17:54,650 --> 00:18:01,000
data set let's try to do some data

00:17:57,620 --> 00:18:03,440
visualization and try to understand if

00:18:01,000 --> 00:18:05,300
are there any nonlinearities in the data

00:18:03,440 --> 00:18:07,430
set which warrant the use of an auto

00:18:05,300 --> 00:18:09,500
encoder right before we start applying

00:18:07,430 --> 00:18:11,539
these fancy models we need to first

00:18:09,500 --> 00:18:14,149
understand whether it may

00:18:11,539 --> 00:18:16,489
hence why not just PCA so to help us

00:18:14,149 --> 00:18:21,279
answer those kind of questions let's

00:18:16,489 --> 00:18:21,279
look at a nicer data visualization

00:18:39,080 --> 00:18:42,970
let me just go just this one

00:18:47,590 --> 00:18:50,590
okay

00:18:51,440 --> 00:18:58,670
so we have a interactive visualization

00:18:54,800 --> 00:19:02,870
which was built entirely in BQ plot here

00:18:58,670 --> 00:19:07,430
I have the time series of all the eleven

00:19:02,870 --> 00:19:09,110
bond interest rates right you see that

00:19:07,430 --> 00:19:12,980
globally the interest rates were around

00:19:09,110 --> 00:19:15,020
five percent before the crisis and as we

00:19:12,980 --> 00:19:17,810
go into the crisis we see that all the

00:19:15,020 --> 00:19:22,490
short-term interest rates were almost

00:19:17,810 --> 00:19:24,440
around zero and more recently the rates

00:19:22,490 --> 00:19:30,500
have been going up so you have this kind

00:19:24,440 --> 00:19:33,860
of interest rates here and obviously we

00:19:30,500 --> 00:19:36,650
cannot visualize a cloud of points in 11

00:19:33,860 --> 00:19:40,880
dimensional space so what I'm trying to

00:19:36,650 --> 00:19:44,780
do here is take two canonical points on

00:19:40,880 --> 00:19:46,700
this yield curve and try to understand

00:19:44,780 --> 00:19:49,280
the relationship between those two

00:19:46,700 --> 00:19:51,790
points and the points I'm choosing here

00:19:49,280 --> 00:19:55,460
are two-year and ten-year interest rates

00:19:51,790 --> 00:19:57,170
they are so important that the spread

00:19:55,460 --> 00:19:59,450
between the ten-year and the 2-year

00:19:57,170 --> 00:20:03,230
interest rate is actually used as a

00:19:59,450 --> 00:20:05,030
proxy to display the shape of the yield

00:20:03,230 --> 00:20:07,040
curve so these are very important points

00:20:05,030 --> 00:20:09,650
which represent the short term and the

00:20:07,040 --> 00:20:11,420
long term interest rates and what I'm

00:20:09,650 --> 00:20:15,500
trying to do here is to look at a

00:20:11,420 --> 00:20:17,060
scatter plot of these two points so that

00:20:15,500 --> 00:20:20,140
we can visualize the data set in two

00:20:17,060 --> 00:20:22,610
dimensions as opposed to 11 dimensions

00:20:20,140 --> 00:20:25,700
notice that I'm also using the color

00:20:22,610 --> 00:20:29,260
scale to represent chronology here so

00:20:25,700 --> 00:20:31,700
points in red are I think around 2006

00:20:29,260 --> 00:20:34,760
where interest rates were typically

00:20:31,700 --> 00:20:37,730
around 5% and as we go into the crisis

00:20:34,760 --> 00:20:40,280
the rates went down and again you see

00:20:37,730 --> 00:20:42,950
the rates going up and green is the more

00:20:40,280 --> 00:20:45,440
recent period right so using the color

00:20:42,950 --> 00:20:50,720
scale to represent chronology is a very

00:20:45,440 --> 00:20:53,210
nice aspect pick you plot now the

00:20:50,720 --> 00:20:56,720
question is let me look at some slices

00:20:53,210 --> 00:21:01,780
of time periods to understand the

00:20:56,720 --> 00:21:01,780
relationship between these two points so

00:21:02,410 --> 00:21:08,630
okay so this is before the crisis and

00:21:05,900 --> 00:21:10,520
you see the relationship is pretty much

00:21:08,630 --> 00:21:13,520
linear right between the two here and

00:21:10,520 --> 00:21:17,540
the ten-year points and if you go past

00:21:13,520 --> 00:21:20,840
the crisis you again see the

00:21:17,540 --> 00:21:26,390
relationship is linear and if you look

00:21:20,840 --> 00:21:28,160
at more recent period you see the same

00:21:26,390 --> 00:21:31,010
structure there is a linear relationship

00:21:28,160 --> 00:21:32,480
between two year and ten-year yields so

00:21:31,010 --> 00:21:35,420
where is the non-linearity creeping in

00:21:32,480 --> 00:21:38,810
so non-linearity creeps in when we try

00:21:35,420 --> 00:21:42,080
to combine regimes when I combine

00:21:38,810 --> 00:21:44,480
post-crisis and the current periods

00:21:42,080 --> 00:21:47,270
that's when you see the non-linearity so

00:21:44,480 --> 00:21:49,490
here the rates were around 3% and they

00:21:47,270 --> 00:21:52,510
went down and they again started going

00:21:49,490 --> 00:21:56,060
up and we have this v-shaped

00:21:52,510 --> 00:21:58,640
non-linearity which comes into play when

00:21:56,060 --> 00:22:00,290
we combine regimes and these kind of

00:21:58,640 --> 00:22:02,330
data visualizations are extremely

00:22:00,290 --> 00:22:04,070
important in trying to understand the

00:22:02,330 --> 00:22:08,740
relationship between points in lower

00:22:04,070 --> 00:22:12,940
dimensions again when you combine the

00:22:08,740 --> 00:22:12,940
pre-crisis and the post-crisis levels

00:22:13,510 --> 00:22:20,000
you see the non-linearity in the data

00:22:15,770 --> 00:22:22,790
set so what we'll do now is let's try to

00:22:20,000 --> 00:22:25,520
fit a one factor PCA and a one factor

00:22:22,790 --> 00:22:27,950
autoencoder on this reduced

00:22:25,520 --> 00:22:33,560
dimensionality data set and see what

00:22:27,950 --> 00:22:37,100
happens so I'm loading pre-trained

00:22:33,560 --> 00:22:39,740
models here to save time we clearly see

00:22:37,100 --> 00:22:42,500
that the auto encoder in magenta is

00:22:39,740 --> 00:22:45,020
capturing the non-linearity whereas pca

00:22:42,500 --> 00:22:48,980
being a linear model cannot capture this

00:22:45,020 --> 00:22:55,300
non-linearity right and similarly then

00:22:48,980 --> 00:22:57,710
go here we again see that water encoder

00:22:55,300 --> 00:22:59,470
nicely captures this non-linearity in

00:22:57,710 --> 00:23:04,400
the data set the v-shaped structure

00:22:59,470 --> 00:23:06,290
there as PCA just cannot do that so now

00:23:04,400 --> 00:23:07,700
that we have convinced ourselves that

00:23:06,290 --> 00:23:11,900
there is some non-linearity in the data

00:23:07,700 --> 00:23:13,450
set and auto-encoder can potentially do

00:23:11,900 --> 00:23:15,779
much better than pca

00:23:13,450 --> 00:23:19,129
let's now try to compare

00:23:15,779 --> 00:23:19,129
them on the whole data set

00:23:30,270 --> 00:23:35,110
I'm again using pre-trained models I

00:23:33,070 --> 00:23:43,750
didn't want to spend time training the

00:23:35,110 --> 00:23:46,150
models here so here what I did was in

00:23:43,750 --> 00:23:48,640
the last example I took a 2-dimensional

00:23:46,150 --> 00:23:51,520
data set and fitted one factor PCA

00:23:48,640 --> 00:23:54,070
and one factor autoencoder but now I'm

00:23:51,520 --> 00:23:57,790
fitting the PCA and autoencoder on the

00:23:54,070 --> 00:23:59,650
whole 11 dimensional data set and I'm

00:23:57,790 --> 00:24:02,140
doing a two factor pca and a two factor

00:23:59,650 --> 00:24:05,710
auto encoder and we want to compare the

00:24:02,140 --> 00:24:08,860
results of PCA and auto encoder on the

00:24:05,710 --> 00:24:11,440
whole data set so for training I'm using

00:24:08,860 --> 00:24:17,230
this whole data set all the way till

00:24:11,440 --> 00:24:19,660
2017 and for testing I'm using the more

00:24:17,230 --> 00:24:21,820
recent period and this is truly

00:24:19,660 --> 00:24:24,250
out-of-sample the models are not trained

00:24:21,820 --> 00:24:26,560
on this data set and we want to compare

00:24:24,250 --> 00:24:31,660
PCA and an auto encoder on this

00:24:26,560 --> 00:24:35,140
out-of-sample data set so now the

00:24:31,660 --> 00:24:38,320
question is how do we compare PCA and an

00:24:35,140 --> 00:24:40,150
auto encoder so what we are doing here

00:24:38,320 --> 00:24:43,390
is we are compressing the data set from

00:24:40,150 --> 00:24:45,730
11 dimensions to 2 dimensions and then

00:24:43,390 --> 00:24:47,770
reconstructing the whole ill curve back

00:24:45,730 --> 00:24:49,060
to 11 dimensions from the

00:24:47,770 --> 00:24:51,880
two-dimensional compressed feature

00:24:49,060 --> 00:24:54,700
vector so the best way to compare them

00:24:51,880 --> 00:24:58,120
is to see how good the reconstruction is

00:24:54,700 --> 00:25:00,280
of PCA and auto encoder and since we

00:24:58,120 --> 00:25:03,250
already know the actual yield curve we

00:25:00,280 --> 00:25:05,260
can overlay all the three yield curves

00:25:03,250 --> 00:25:08,800
the reconstructed yield curves and the

00:25:05,260 --> 00:25:10,510
original ilka and see how closely PCA or

00:25:08,800 --> 00:25:13,600
auto encoder track these yield curves

00:25:10,510 --> 00:25:15,820
right I'm going to use a nice tool

00:25:13,600 --> 00:25:20,260
called date selector to select different

00:25:15,820 --> 00:25:24,070
dates and see how the reconstruction is

00:25:20,260 --> 00:25:26,260
compared so the true ill curve is in

00:25:24,070 --> 00:25:29,680
yellow and we see that auto encoder

00:25:26,260 --> 00:25:33,280
closely tracks the true wheeled curve

00:25:29,680 --> 00:25:36,340
compared to the green PCA and we can

00:25:33,280 --> 00:25:38,440
look at different dates and see that

00:25:36,340 --> 00:25:41,860
globally the auto encoder as expected is

00:25:38,440 --> 00:25:43,309
doing a much better job but obviously

00:25:41,860 --> 00:25:45,320
this is the

00:25:43,309 --> 00:25:49,570
training samples let's now go to the

00:25:45,320 --> 00:25:51,639
test data set and see how they are doing

00:25:49,570 --> 00:25:53,590
and we again see that

00:25:51,639 --> 00:25:56,139
auto-encoder is doing a much better job

00:25:53,590 --> 00:26:01,490
in tracking the original yield curve

00:25:56,139 --> 00:26:05,870
compared to the Green PCA let's now go

00:26:01,490 --> 00:26:10,490
to the more recent periods and we see a

00:26:05,870 --> 00:26:13,249
problem both auto-encoder and PCA are

00:26:10,490 --> 00:26:16,549
not doing a great job in tracking the in

00:26:13,249 --> 00:26:19,580
tracking the yellow yield curve and the

00:26:16,549 --> 00:26:21,710
reason is obvious we haven't encountered

00:26:19,580 --> 00:26:24,499
this kind of data in our training

00:26:21,710 --> 00:26:26,299
samples right we had we trained the data

00:26:24,499 --> 00:26:28,220
set mostly on the pre-crisis and the

00:26:26,299 --> 00:26:31,460
post-crisis levels and we are in a

00:26:28,220 --> 00:26:33,350
totally different regime now and no

00:26:31,460 --> 00:26:35,840
wonder PCA and auto-encoder will not do

00:26:33,350 --> 00:26:40,759
a great job in tracking the yield curve

00:26:35,840 --> 00:26:42,769
so again this kind of tools where we can

00:26:40,759 --> 00:26:45,440
quickly scan through different time

00:26:42,769 --> 00:26:47,240
periods will help us understand where

00:26:45,440 --> 00:26:49,639
the model is doing well and where the

00:26:47,240 --> 00:26:52,700
model is not doing well and now that we

00:26:49,639 --> 00:26:54,110
understand that we can't just be happy

00:26:52,700 --> 00:26:57,070
training our models on some historical

00:26:54,110 --> 00:26:58,929
data set and hope the models to perform

00:26:57,070 --> 00:27:01,580
consistently well going forward

00:26:58,929 --> 00:27:04,549
especially in time series we encounter

00:27:01,580 --> 00:27:07,039
regimes where we encounter different

00:27:04,549 --> 00:27:09,889
trends or the dynamics of the data set

00:27:07,039 --> 00:27:12,139
changes so we need to keep on doing the

00:27:09,889 --> 00:27:13,970
training process on a rolling basis so

00:27:12,139 --> 00:27:17,690
that we capture all these dynamics and

00:27:13,970 --> 00:27:19,399
this kind of tools can help you clearly

00:27:17,690 --> 00:27:21,169
understand where the models are doing

00:27:19,399 --> 00:27:27,259
well and where the models are not doing

00:27:21,169 --> 00:27:30,100
well so that's all I have for the

00:27:27,259 --> 00:27:30,100
dimensionality reduction

00:27:35,520 --> 00:27:50,410
so let's look at our last example so we

00:27:47,650 --> 00:27:51,880
look at a graphical tool for doing the

00:27:50,410 --> 00:27:58,840
end-to-end pipeline of a deep learning

00:27:51,880 --> 00:28:00,160
model and I built this tool in entirely

00:27:58,840 --> 00:28:02,080
in Python using object-oriented

00:28:00,160 --> 00:28:04,750
techniques with a plug-and-play

00:28:02,080 --> 00:28:08,050
architecture so I just built it as a

00:28:04,750 --> 00:28:09,520
prototype but but the idea is you should

00:28:08,050 --> 00:28:12,070
be able to easily extend the base

00:28:09,520 --> 00:28:14,800
classes to provide your own training

00:28:12,070 --> 00:28:17,710
plots and diagnostic plots I provided

00:28:14,800 --> 00:28:19,510
basic implementations and you could

00:28:17,710 --> 00:28:22,360
easily customize them according to your

00:28:19,510 --> 00:28:24,580
requirements for example I provided

00:28:22,360 --> 00:28:27,040
implementation of regression diagnostic

00:28:24,580 --> 00:28:28,840
plots but you can easily extend the base

00:28:27,040 --> 00:28:31,120
class to create classification

00:28:28,840 --> 00:28:33,640
diagnostic plots where you can add for

00:28:31,120 --> 00:28:35,830
example interactive confusion matrix

00:28:33,640 --> 00:28:37,900
where you can drill down on specific

00:28:35,830 --> 00:28:41,530
cells and further access the

00:28:37,900 --> 00:28:43,720
misclassified points like that and I'm

00:28:41,530 --> 00:28:45,730
also directly introspecting on the

00:28:43,720 --> 00:28:47,770
carers library to build all the UI

00:28:45,730 --> 00:28:51,520
components so they are not hard-coded

00:28:47,770 --> 00:28:53,760
and so let's go ahead and look at this

00:28:51,520 --> 00:28:53,760
tool

00:28:59,559 --> 00:29:04,749
the data set I'm using here is a simple

00:29:02,559 --> 00:29:07,799
toy data set based on the black Scholes

00:29:04,749 --> 00:29:12,099
formula so what is black Scholes formula

00:29:07,799 --> 00:29:14,649
it's a smooth non linear function of for

00:29:12,099 --> 00:29:16,479
numerical inputs and it returns a

00:29:14,649 --> 00:29:19,959
numerical output which is the price of

00:29:16,479 --> 00:29:22,419
an option what we are trying to do here

00:29:19,959 --> 00:29:27,700
is trying to learn the black Scholes

00:29:22,419 --> 00:29:30,159
formula using a deep learning model the

00:29:27,700 --> 00:29:31,869
data set is manually generated using the

00:29:30,159 --> 00:29:39,450
formula so there's nothing special about

00:29:31,869 --> 00:29:44,159
it and okay so we have this tool here

00:29:39,450 --> 00:29:47,649
sorry I'm not a UX expert so the lot of

00:29:44,159 --> 00:29:49,929
enhancements that can be made to this so

00:29:47,649 --> 00:29:52,479
the first tab here shows the network

00:29:49,929 --> 00:29:55,749
parameters we can select the number of

00:29:52,479 --> 00:29:59,109
epochs and we can select the batch size

00:29:55,749 --> 00:30:01,899
for each epoch and you can choose from a

00:29:59,109 --> 00:30:03,820
bunch of loss functions these are all

00:30:01,899 --> 00:30:06,669
directly obtained by introspecting the

00:30:03,820 --> 00:30:09,070
carers library and you can choose from

00:30:06,669 --> 00:30:12,789
different optimizers and for each

00:30:09,070 --> 00:30:14,769
optimizer you can also see that the

00:30:12,789 --> 00:30:18,249
defaults are automatically populated so

00:30:14,769 --> 00:30:20,200
you can pick from them right and since

00:30:18,249 --> 00:30:25,119
this is a regression problem let me pick

00:30:20,200 --> 00:30:28,919
mean squared error and I use atom and

00:30:25,119 --> 00:30:32,889
let me give a learning rate DK to the

00:30:28,919 --> 00:30:36,249
training process and now we can select

00:30:32,889 --> 00:30:38,320
the architecture I'm providing support

00:30:36,249 --> 00:30:41,879
only for fully connected layers here but

00:30:38,320 --> 00:30:46,289
you can easily extend this to support

00:30:41,879 --> 00:30:49,599
convolutional layers or lsdm cells etc

00:30:46,289 --> 00:30:51,629
notice that the tool automatically

00:30:49,599 --> 00:30:54,669
infants the number of inputs and outputs

00:30:51,629 --> 00:30:58,570
by looking at the the inputs the

00:30:54,669 --> 00:31:01,049
training data set so let me add a bunch

00:30:58,570 --> 00:31:01,049
of hidden layers

00:31:06,920 --> 00:31:18,630
and we can choose from different

00:31:08,850 --> 00:31:22,980
activation functions we picked an H so

00:31:18,630 --> 00:31:26,000
tan H is a is a monotonically increasing

00:31:22,980 --> 00:31:30,210
function which goes from minus 1 to 1

00:31:26,000 --> 00:31:36,570
and that creates the non-linearity in

00:31:30,210 --> 00:31:43,170
the data set in the modeling so let's

00:31:36,570 --> 00:31:45,540
now train the model we see that the loss

00:31:43,170 --> 00:31:47,930
and accuracy curves are getting updated

00:31:45,540 --> 00:31:49,230
in real time as the training is going on

00:31:47,930 --> 00:31:52,530
Wow

00:31:49,230 --> 00:31:55,170
we see that training stalled initially

00:31:52,530 --> 00:31:58,080
for 25 bucks and then it starts learning

00:31:55,170 --> 00:32:01,530
the accuracy starts shooting up I'm

00:31:58,080 --> 00:32:06,810
using R square as a metric for this

00:32:01,530 --> 00:32:09,930
problem and see that there is some good

00:32:06,810 --> 00:32:12,270
improvement in the training process so

00:32:09,930 --> 00:32:16,050
we are done with the training and now we

00:32:12,270 --> 00:32:18,840
can look at the distributions of vagues

00:32:16,050 --> 00:32:20,640
biases and activations so here we are

00:32:18,840 --> 00:32:22,650
kind of peeking into the black box of

00:32:20,640 --> 00:32:25,320
the deep learning model we are trying to

00:32:22,650 --> 00:32:27,840
understand how the training loss curves

00:32:25,320 --> 00:32:29,910
are getting updated in real time and

00:32:27,840 --> 00:32:32,940
also we can look at the distributions of

00:32:29,910 --> 00:32:34,440
weights biases and activations we can

00:32:32,940 --> 00:32:38,040
look look at them for different layers

00:32:34,440 --> 00:32:41,010
and for different epochs so if you look

00:32:38,040 --> 00:32:44,640
at the weights that initially they're

00:32:41,010 --> 00:32:46,950
initialized to uniform and as I go into

00:32:44,640 --> 00:32:51,300
the training process you see that the

00:32:46,950 --> 00:32:53,640
converge which is good and you can also

00:32:51,300 --> 00:32:56,880
look at the distributions of activations

00:32:53,640 --> 00:32:59,160
here since tan H goes from minus 1 to 1

00:32:56,880 --> 00:33:03,230
most of the values are either minus 1 or

00:32:59,160 --> 00:33:03,230
1 and you see some values in between

00:33:04,640 --> 00:33:11,070
okay so if you look at the last layer

00:33:07,760 --> 00:33:13,670
you clearly see a problem and the

00:33:11,070 --> 00:33:16,770
problem is the saturation of activations

00:33:13,670 --> 00:33:17,520
what's happening here is the activations

00:33:16,770 --> 00:33:21,090
are

00:33:17,520 --> 00:33:23,640
either minus 1 or 1 so what happens when

00:33:21,090 --> 00:33:25,470
the activations are minus 1 or 1 the

00:33:23,640 --> 00:33:28,080
gradients are 0 right because we're in

00:33:25,470 --> 00:33:31,230
the flat region of the of the tan edge

00:33:28,080 --> 00:33:33,420
so the gradients are 0 and that means

00:33:31,230 --> 00:33:35,910
the gradients are not flowing back in

00:33:33,420 --> 00:33:38,310
the backdrop and that explains this

00:33:35,910 --> 00:33:41,280
stalling in the training process right

00:33:38,310 --> 00:33:44,670
and so if you look at this kind of plots

00:33:41,280 --> 00:33:47,340
you get an intuition behind why exactly

00:33:44,670 --> 00:33:49,140
the training process is stalling or what

00:33:47,340 --> 00:33:53,250
is happening with my activations right

00:33:49,140 --> 00:33:58,110
and if you notice after I go past 25

00:33:53,250 --> 00:34:00,300
ebox right you see the activations are

00:33:58,110 --> 00:34:02,400
no longer saturated that means the

00:34:00,300 --> 00:34:03,870
gradients are nonzero they are flowing

00:34:02,400 --> 00:34:06,390
back and the weights are getting updated

00:34:03,870 --> 00:34:09,210
and the training is going on and that

00:34:06,390 --> 00:34:10,770
explains this behavior here and then you

00:34:09,210 --> 00:34:15,600
immediately start the learning process

00:34:10,770 --> 00:34:17,520
and it's because of this right and

00:34:15,600 --> 00:34:21,380
finally we can look at the diagnostic

00:34:17,520 --> 00:34:24,990
plots now this looks horrible because so

00:34:21,380 --> 00:34:30,710
let's go and change the activation to

00:34:24,990 --> 00:34:30,710
value which has much better convergence

00:34:30,860 --> 00:34:38,760
right so let me go ahead and retrain the

00:34:34,980 --> 00:34:41,610
model so now we see that the accuracy

00:34:38,760 --> 00:34:43,290
immediately improves value is a much

00:34:41,610 --> 00:34:45,270
better choice for an activation function

00:34:43,290 --> 00:34:47,730
and it's obvious when you look at the

00:34:45,270 --> 00:34:53,669
when you look at the loss and accuracy

00:34:47,730 --> 00:34:55,230
curves they're almost done and you

00:34:53,669 --> 00:34:58,310
immediately see that the accuracy is

00:34:55,230 --> 00:34:58,310
approaching hundred percent

00:35:04,040 --> 00:35:10,220
right and the distributions are very

00:35:06,860 --> 00:35:13,790
nice there well there is no saturation

00:35:10,220 --> 00:35:17,750
anywhere right and let me show you one

00:35:13,790 --> 00:35:20,290
final improvement which we can make to

00:35:17,750 --> 00:35:23,540
really improve the tip learning model

00:35:20,290 --> 00:35:26,360
deep learning models typically expect

00:35:23,540 --> 00:35:29,660
the inputs to be small numbers in the

00:35:26,360 --> 00:35:32,660
range from 0 to 1 so transformation and

00:35:29,660 --> 00:35:36,050
scaling of data is crucial and we'll see

00:35:32,660 --> 00:35:41,120
that now let me just use the scale data

00:35:36,050 --> 00:35:43,490
set here so what I'm doing by scaling is

00:35:41,120 --> 00:35:46,070
I'm subtracting the mean and dividing by

00:35:43,490 --> 00:35:49,370
the standard deviation so that all the

00:35:46,070 --> 00:35:53,020
all the numerical values across all the

00:35:49,370 --> 00:35:53,020
features have similar dimensions

00:35:53,830 --> 00:35:58,550
unfortunately I have to rerun this tool

00:35:56,290 --> 00:36:00,770
ideally we can have an extra tab for

00:35:58,550 --> 00:36:03,380
doing data cleaning and data

00:36:00,770 --> 00:36:07,550
transformations but let me just run it

00:36:03,380 --> 00:36:10,180
for 30 box I'll keep the same

00:36:07,550 --> 00:36:10,180
architecture

00:36:17,370 --> 00:36:25,660
right and let's do the training you see

00:36:23,590 --> 00:36:31,600
immediately the accuracy shoots up to

00:36:25,660 --> 00:36:33,280
100% and so you know transforming the

00:36:31,600 --> 00:36:36,060
data is crucial as we can see we can

00:36:33,280 --> 00:36:38,740
only tune the hyper parameter so much

00:36:36,060 --> 00:36:41,710
the first thing is make sure your data

00:36:38,740 --> 00:36:43,000
is transformed well and if you look at

00:36:41,710 --> 00:36:47,710
the diagnostic plots they look much

00:36:43,000 --> 00:36:50,500
better so again to summarize I just

00:36:47,710 --> 00:36:53,410
created a simple prototype to show the

00:36:50,500 --> 00:36:55,120
value this widget libraries what you can

00:36:53,410 --> 00:36:57,640
accomplish with this kind of widget

00:36:55,120 --> 00:37:00,580
libraries and if you spend enough time

00:36:57,640 --> 00:37:02,170
you can make it an industry grade neural

00:37:00,580 --> 00:37:03,460
network builder with all the bells and

00:37:02,170 --> 00:37:06,070
whistles and support for all the

00:37:03,460 --> 00:37:13,660
activations and different layers and all

00:37:06,070 --> 00:37:16,150
these things yep so that's all I have

00:37:13,660 --> 00:37:19,570
for today I hope you found these

00:37:16,150 --> 00:37:21,160
examples instructive and useful I want

00:37:19,570 --> 00:37:23,650
to thank you all for attending my talk

00:37:21,160 --> 00:37:25,320
and if you have any questions I'm happy

00:37:23,650 --> 00:37:35,050
to take them

00:37:25,320 --> 00:37:35,050
[Applause]

00:37:42,390 --> 00:37:51,150
yeah so let me say it's on my github

00:37:48,910 --> 00:37:51,150
account

00:37:57,440 --> 00:38:03,230
all the examples and pretty much

00:37:59,980 --> 00:38:05,030
everything I've shown here is all

00:38:03,230 --> 00:38:13,819
available here so you can just download

00:38:05,030 --> 00:38:15,560
from here yeah yeah actually all the

00:38:13,819 --> 00:38:19,490
examples have shown run in a nice

00:38:15,560 --> 00:38:22,069
beautiful black theme notebook so if you

00:38:19,490 --> 00:38:24,260
try this invite theme if not like this

00:38:22,069 --> 00:38:25,970
good but hopefully with the Jupiter lab

00:38:24,260 --> 00:38:28,130
I think black teaming also will be

00:38:25,970 --> 00:38:30,730
enabled so you should be able to take

00:38:28,130 --> 00:38:30,730
advantage of that

00:38:42,400 --> 00:38:48,890
yeah because two years and ten years are

00:38:46,760 --> 00:38:50,900
like a very important points on the

00:38:48,890 --> 00:38:53,119
yield curve which represent the short

00:38:50,900 --> 00:38:54,650
term and the long term bonds and I was

00:38:53,119 --> 00:38:57,230
mentioning before the spread between the

00:38:54,650 --> 00:38:58,670
ten-year in the two-year is used as a

00:38:57,230 --> 00:39:02,890
proxy for the shape of the yield curve

00:38:58,670 --> 00:39:02,890
so I just pick the most important points

00:39:16,700 --> 00:39:19,590
thank you all right thanks I think is

00:39:19,349 --> 00:39:25,619
much

00:39:19,590 --> 00:39:25,619

YouTube URL: https://www.youtube.com/watch?v=1XTMkcrVyQg


