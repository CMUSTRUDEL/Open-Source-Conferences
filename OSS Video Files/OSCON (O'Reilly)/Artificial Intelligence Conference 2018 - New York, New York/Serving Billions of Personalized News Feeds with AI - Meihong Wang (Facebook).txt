Title: Serving Billions of Personalized News Feeds with AI - Meihong Wang (Facebook)
Publication date: 2018-05-02
Playlist: Artificial Intelligence Conference 2018 - New York, New York
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,079 --> 00:00:05,279
so the first question we have the answer

00:00:02,520 --> 00:00:07,290
is how do we define the relevancy so

00:00:05,279 --> 00:00:09,090
this is a very subjective and every

00:00:07,290 --> 00:00:11,010
people have different opinions and

00:00:09,090 --> 00:00:14,130
ancestor where to come personalization

00:00:11,010 --> 00:00:16,560
comes in so the major approach we take

00:00:14,130 --> 00:00:20,609
is to use the user feedback on a story

00:00:16,560 --> 00:00:22,740
to approximates the relevancy to user so

00:00:20,609 --> 00:00:25,050
for example user can take out actions

00:00:22,740 --> 00:00:26,970
that like and share love and a comment

00:00:25,050 --> 00:00:28,949
on the story so those are really really

00:00:26,970 --> 00:00:31,080
strong signal indicating people want to

00:00:28,949 --> 00:00:33,270
see the content the enjoy the content

00:00:31,080 --> 00:00:35,250
well other actions of people can take on

00:00:33,270 --> 00:00:36,989
stories like you can hide it a story you

00:00:35,250 --> 00:00:39,120
don't want to see the content so this is

00:00:36,989 --> 00:00:40,829
really strong negative signal to

00:00:39,120 --> 00:00:44,160
indicate people don't want to see the

00:00:40,829 --> 00:00:47,670
content with all the historical data we

00:00:44,160 --> 00:00:50,789
have basically we can learn the ML model

00:00:47,670 --> 00:00:52,829
to predict for new story how likely user

00:00:50,789 --> 00:00:54,870
going to take all those action under the

00:00:52,829 --> 00:00:57,899
story a positive ones on it was a

00:00:54,870 --> 00:01:00,719
negative one so here is a one example

00:00:57,899 --> 00:01:03,210
with our prediction model so with this

00:01:00,719 --> 00:01:05,220
particular story and a user pair we

00:01:03,210 --> 00:01:07,110
build a Prada we predict all the

00:01:05,220 --> 00:01:09,689
possibilities for example there's the

00:01:07,110 --> 00:01:12,330
eleven percent chance people will click

00:01:09,689 --> 00:01:15,119
user a will click on a story and in 2.2

00:01:12,330 --> 00:01:18,270
percent chance you say we like the story

00:01:15,119 --> 00:01:20,820
and also 1.1 percent o chance the people

00:01:18,270 --> 00:01:22,650
where you hide at the story so we waited

00:01:20,820 --> 00:01:25,439
a some the score together and again a

00:01:22,650 --> 00:01:27,960
final final ranking score for this

00:01:25,439 --> 00:01:32,009
example the ranking score is pointed to

00:01:27,960 --> 00:01:33,570
277 so for use a when he loves news feed

00:01:32,009 --> 00:01:35,520
and know a base to rank all those

00:01:33,570 --> 00:01:38,640
content and the regular content based on

00:01:35,520 --> 00:01:41,939
the total score here one important note

00:01:38,640 --> 00:01:43,110
here is all the probability column these

00:01:41,939 --> 00:01:45,509
are calculated with machine learning

00:01:43,110 --> 00:01:47,759
models and that it has the grant use and

00:01:45,509 --> 00:01:51,060
are very personalized well the value

00:01:47,759 --> 00:01:53,549
column is more of like subject 7a based

00:01:51,060 --> 00:01:56,850
on the user study and a product decision

00:01:53,549 --> 00:01:58,710
we make so for example on Facebook and

00:01:56,850 --> 00:02:01,680
the value for user is more on the

00:01:58,710 --> 00:02:03,360
interaction with people so the like and

00:02:01,680 --> 00:02:05,040
comments are more meaningful to user

00:02:03,360 --> 00:02:06,990
than just a click on a story

00:02:05,040 --> 00:02:10,410
the weights off or like or commoners is

00:02:06,990 --> 00:02:12,150
much higher here the reason probability

00:02:10,410 --> 00:02:13,510
here is just example to demonstrate the

00:02:12,150 --> 00:02:15,709
points here

00:02:13,510 --> 00:02:17,870
so here are the more detail on the

00:02:15,709 --> 00:02:19,520
perdition model for Mao model the

00:02:17,870 --> 00:02:22,310
feature and the labels are the most

00:02:19,520 --> 00:02:24,440
important two parts so when we have the

00:02:22,310 --> 00:02:26,870
feature joiner system to generate a lot

00:02:24,440 --> 00:02:29,989
of Gini data for us to China model so

00:02:26,870 --> 00:02:32,180
when they use load the news feed and we

00:02:29,989 --> 00:02:34,610
go to the aggregated as a ranking and a

00:02:32,180 --> 00:02:37,730
return story back to the mobile at the

00:02:34,610 --> 00:02:38,750
same time the feature will be log to the

00:02:37,730 --> 00:02:41,720
feature joiner

00:02:38,750 --> 00:02:43,700
and the same with the users feedback

00:02:41,720 --> 00:02:45,920
events like like a comment

00:02:43,700 --> 00:02:48,410
they also be sent to the future joiner

00:02:45,920 --> 00:02:50,209
then the join ability generates a genie

00:02:48,410 --> 00:02:52,670
did how is the feature and also it's a

00:02:50,209 --> 00:02:54,860
label that those data we fit into our

00:02:52,670 --> 00:02:57,170
model genie service and the model will

00:02:54,860 --> 00:02:59,300
pick up the weights and the reach in a

00:02:57,170 --> 00:03:02,120
model and updates the weights to the our

00:02:59,300 --> 00:03:04,250
system and the aggregator we pick up for

00:03:02,120 --> 00:03:06,800
our model we have a huge model and

00:03:04,250 --> 00:03:10,390
typically the training example size

00:03:06,800 --> 00:03:10,390
around the billions of examples

00:03:16,650 --> 00:03:18,709

YouTube URL: https://www.youtube.com/watch?v=wcVJZwO_py0


