Title: The challenges of migrating 150+ microservices to Kubernetes -Sarah Wells (Financial Times)
Publication date: 2018-10-29
Playlist: O'Reilly Software Architecture Conference 2018 - London, United Kingdom
Description: 
	See the full conference compilation at: http://oreilly.com/go/sacon-uk18
If you adopt things at the leading edge, you can gain in the short term, but in the long term, successful technologies get commoditized and become easier to use. You don’t want to be running your own hand-rolled container platform when other people are using managed Kubernetes—the cost is likely too high. But how do you decide whether to adopt a leading-edge technology? And how do you know when it’s time to change? Finally, how can you migrate onto the new architecture while keeping everything up and running?

The Financial Times content platform team put its first containers live in mid-2015 and migrated the rest of its services over by April 2016. At that point, the team was using a largely self-built stack on CoreOS Container Linux with Fleet to do cluster management.

At the end of 2016, the team members decided they wanted to benefit from the work other people were doing and switched over to Kubernetes. But it’s not easy to do that kind of move when you have 150+ microservices and you need to keep the existing platform running in parallel while you do the migration.

Sarah Wells explains why the team decided to do this migration, the challenges they faced in doing it while supporting their live platform, and the lessons learned along the way.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,179 --> 00:00:07,319
so we started on migration early in 2017

00:00:04,670 --> 00:00:08,880
but even having good reasons to do the

00:00:07,319 --> 00:00:10,679
migration it was still a major challenge

00:00:08,880 --> 00:00:11,340
you do not want to change horses

00:00:10,679 --> 00:00:13,519
midstream

00:00:11,340 --> 00:00:16,139
you've got enough other stuff going on

00:00:13,519 --> 00:00:18,510
we had a lot of services live already at

00:00:16,139 --> 00:00:20,039
this point in production and under

00:00:18,510 --> 00:00:22,560
active development when I say under

00:00:20,039 --> 00:00:24,000
active development this is a diagram

00:00:22,560 --> 00:00:25,199
that's attempting to work out all the

00:00:24,000 --> 00:00:26,939
functional stuff we wanted to deliver

00:00:25,199 --> 00:00:28,380
last year we had five teams working on

00:00:26,939 --> 00:00:31,769
it there were a lot of interdependencies

00:00:28,380 --> 00:00:33,059
it was extremely complicated and then we

00:00:31,769 --> 00:00:34,860
had a migration team trying to move

00:00:33,059 --> 00:00:39,899
everything underneath it so you can

00:00:34,860 --> 00:00:41,640
imagine that was quite difficult and we

00:00:39,899 --> 00:00:42,809
had to run things in parallel for quite

00:00:41,640 --> 00:00:44,250
a long time because when you're born

00:00:42,809 --> 00:00:45,660
from scratch you often want to build a

00:00:44,250 --> 00:00:48,030
platform with a few services in it but

00:00:45,660 --> 00:00:50,190
actually we wanted to know whether we

00:00:48,030 --> 00:00:52,140
could run our complicated routing and

00:00:50,190 --> 00:00:53,460
failover logic on kubernetes which meant

00:00:52,140 --> 00:00:56,280
having to migrate quite a lot of things

00:00:53,460 --> 00:00:58,800
this took a while we ran in parallel for

00:00:56,280 --> 00:01:00,390
a long time in fact we did well over

00:00:58,800 --> 00:01:01,969
2000 code releases while running at

00:01:00,390 --> 00:01:05,820
least part of the stack in parallel

00:01:01,969 --> 00:01:08,850
that's about 8 about a year and even if

00:01:05,820 --> 00:01:11,189
it takes 10 minutes extra to deploy to

00:01:08,850 --> 00:01:14,490
your two parallel stacks that's 47

00:01:11,189 --> 00:01:16,560
working days of extra deployment time if

00:01:14,490 --> 00:01:20,280
you've got two thousand at scale these

00:01:16,560 --> 00:01:21,330
things start to add up so when we

00:01:20,280 --> 00:01:22,530
started doing this we needed to make

00:01:21,330 --> 00:01:24,240
some decisions about how we were going

00:01:22,530 --> 00:01:26,310
to run stuff in parallel how to make

00:01:24,240 --> 00:01:27,990
changes for one stack for the other how

00:01:26,310 --> 00:01:31,229
to keep things in sync between the two

00:01:27,990 --> 00:01:33,329
stacks so the first decision was about

00:01:31,229 --> 00:01:34,740
where do we commit code so we started

00:01:33,329 --> 00:01:36,780
off by putting all of our changes for

00:01:34,740 --> 00:01:38,100
kubernetes onto separate branch which

00:01:36,780 --> 00:01:39,750
kept the work the migration team were

00:01:38,100 --> 00:01:41,670
doing separate from the work everyone

00:01:39,750 --> 00:01:43,079
else was doing but that doesn't work

00:01:41,670 --> 00:01:44,759
well if those services are also getting

00:01:43,079 --> 00:01:46,860
active code changes for other reasons

00:01:44,759 --> 00:01:51,090
because eventually you have to merge the

00:01:46,860 --> 00:01:52,979
code and the feature team doesn't want

00:01:51,090 --> 00:01:54,360
to have to merge it because it's it's

00:01:52,979 --> 00:01:56,040
effectively paying a tax on all their

00:01:54,360 --> 00:01:58,200
development and the migration team has

00:01:56,040 --> 00:02:01,710
no idea what this new feature stuff is

00:01:58,200 --> 00:02:04,229
anyway so ultimately all code had to go

00:02:01,710 --> 00:02:05,430
onto the master and any difference in

00:02:04,229 --> 00:02:06,740
behavior between the old and the new

00:02:05,430 --> 00:02:09,090
stack had to be done through

00:02:06,740 --> 00:02:12,150
configuration or code path changes based

00:02:09,090 --> 00:02:13,410
on environment cement we had a single

00:02:12,150 --> 00:02:15,750
docker image for a particular

00:02:13,410 --> 00:02:18,600
the code and we expected people to

00:02:15,750 --> 00:02:19,620
deploy it to both of the stacks so the

00:02:18,600 --> 00:02:21,270
next thing after that was about

00:02:19,620 --> 00:02:22,860
deployment how are we going to do the

00:02:21,270 --> 00:02:24,450
deployment if you have completely

00:02:22,860 --> 00:02:25,830
separate deployment mechanisms you will

00:02:24,450 --> 00:02:27,690
end up with inconsistencies because

00:02:25,830 --> 00:02:29,610
someone forgot to do both deployments

00:02:27,690 --> 00:02:30,660
either accidentally or deliberately cuz

00:02:29,610 --> 00:02:33,360
they're focused on what they want to

00:02:30,660 --> 00:02:36,390
achieve with 150 services you can't see

00:02:33,360 --> 00:02:38,160
it a glance whether that's happening but

00:02:36,390 --> 00:02:40,080
running a single deployment pipeline is

00:02:38,160 --> 00:02:42,000
risky because you really don't want to

00:02:40,080 --> 00:02:44,160
block an emergency deployment for a live

00:02:42,000 --> 00:02:47,670
issue because your new and experimental

00:02:44,160 --> 00:02:49,050
stack had a problem so what we actually

00:02:47,670 --> 00:02:50,370
did was we had two pipelines that were

00:02:49,050 --> 00:02:52,250
kicked off by the same single action

00:02:50,370 --> 00:02:54,780
which was attacking a release in github

00:02:52,250 --> 00:02:56,640
the last step step of both deployments

00:02:54,780 --> 00:02:58,590
was manual someone had to click about

00:02:56,640 --> 00:03:00,390
and say I want this to go live and we

00:02:58,590 --> 00:03:02,160
just wrote a script that we ran manually

00:03:00,390 --> 00:03:04,290
we didn't get around to automating it

00:03:02,160 --> 00:03:06,330
that just said what are the versions of

00:03:04,290 --> 00:03:08,070
the services in both environments and

00:03:06,330 --> 00:03:09,060
then one of our developers around every

00:03:08,070 --> 00:03:10,290
morning they would tell people where

00:03:09,060 --> 00:03:14,880
they hadn't actually finished their

00:03:10,290 --> 00:03:16,410
deployment the approaches we chose meant

00:03:14,880 --> 00:03:18,570
we were deploying functional changes

00:03:16,410 --> 00:03:20,310
into both of our stacks and we were

00:03:18,570 --> 00:03:21,510
deploying things to production when it

00:03:20,310 --> 00:03:23,400
was really only a change for running

00:03:21,510 --> 00:03:25,709
keeping at ease and we didn't want to

00:03:23,400 --> 00:03:27,330
double the amount of testing that we did

00:03:25,709 --> 00:03:28,980
it was testing Mike reservist is quite

00:03:27,330 --> 00:03:33,030
can take up quite a lot of time anyway

00:03:28,980 --> 00:03:36,209
so basically what we did was prioritized

00:03:33,030 --> 00:03:38,400
the live stack that's what we cared

00:03:36,209 --> 00:03:40,980
about and we relied on automation to

00:03:38,400 --> 00:03:44,160
help us reduce the risk so we already

00:03:40,980 --> 00:03:45,480
had a service that would monitor when

00:03:44,160 --> 00:03:46,950
content was published so someone

00:03:45,480 --> 00:03:48,180
publishes an article it would monitor to

00:03:46,950 --> 00:03:50,400
make sure that article made it through

00:03:48,180 --> 00:03:52,470
to all of the necessary data stores and

00:03:50,400 --> 00:03:55,110
api's so we just ran that in both stacks

00:03:52,470 --> 00:03:56,880
and then we added a small service in our

00:03:55,110 --> 00:03:58,920
livestock that just forwarded all the

00:03:56,880 --> 00:04:00,150
real live production publishers to our

00:03:58,920 --> 00:04:02,100
other stack so we'd know if there was

00:04:00,150 --> 00:04:03,420
any difference between the two was it

00:04:02,100 --> 00:04:04,530
failing to publishing one of the two

00:04:03,420 --> 00:04:05,880
stacks well there's probably something

00:04:04,530 --> 00:04:08,270
wrong with our configuration or our

00:04:05,880 --> 00:04:08,270

YouTube URL: https://www.youtube.com/watch?v=fgI3cOdv87I


