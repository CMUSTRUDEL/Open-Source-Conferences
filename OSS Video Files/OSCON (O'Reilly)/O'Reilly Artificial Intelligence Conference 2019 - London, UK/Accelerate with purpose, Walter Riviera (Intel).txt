Title: Accelerate with purpose, Walter Riviera (Intel)
Publication date: 2019-10-17
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	Today’s computing hardware has taken us well into the resurgence of AI, but with the demand for AI compute doubling nearly every three months, new ways of computing are required that can keep up and evolve.
Walter Riviera details three key shifts in the AI landscape—incredibly large models with billions of hyperparameters, massive clusters of compute nodes supporting AI, and the exploding volume of data meeting ever-stricter latency requirements—how to navigate them, and when to explore hardware acceleration.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,919 --> 00:00:05,930
so what are the three key shift

00:00:02,900 --> 00:00:08,390
happening in the eye landscape well

00:00:05,930 --> 00:00:13,310
we're all convinced that computing into

00:00:08,390 --> 00:00:15,560
growth as well as the memory and please

00:00:13,310 --> 00:00:17,480
when I talk about memory is not just

00:00:15,560 --> 00:00:19,570
about being able to host the data set

00:00:17,480 --> 00:00:22,339
when we say you know you go through

00:00:19,570 --> 00:00:24,409
iteration to train a machine it's not

00:00:22,339 --> 00:00:29,359
just about being able to host those

00:00:24,409 --> 00:00:32,780
images per that time to go through the

00:00:29,359 --> 00:00:35,090
model it's also about when we go through

00:00:32,780 --> 00:00:37,280
that model it's a lot of computation we

00:00:35,090 --> 00:00:42,199
have to be able to store that temporary

00:00:37,280 --> 00:00:47,480
results close by the closer we can stay

00:00:42,199 --> 00:00:49,760
the faster we have we can go so when we

00:00:47,480 --> 00:00:52,940
talk about memory is support the

00:00:49,760 --> 00:00:56,570
computation plus the data and the model

00:00:52,940 --> 00:01:01,100
that I need to be able to host so the

00:00:56,570 --> 00:01:04,610
first key is compute the second is of

00:01:01,100 --> 00:01:06,140
course the memory but we don't want to

00:01:04,610 --> 00:01:07,670
introduce a new bottleneck if you don't

00:01:06,140 --> 00:01:09,920
take into account the communication as

00:01:07,670 --> 00:01:16,490
well and by me communication I mean

00:01:09,920 --> 00:01:21,850
within and across the chip this coupled

00:01:16,490 --> 00:01:25,159
up with high programmability flexibility

00:01:21,850 --> 00:01:26,509
given all the variety of frameworks open

00:01:25,159 --> 00:01:28,850
source that are currently available to

00:01:26,509 --> 00:01:31,909
develop these applications and the

00:01:28,850 --> 00:01:33,170
efficiency of the device we truly

00:01:31,909 --> 00:01:35,149
believe that whomever is going to

00:01:33,170 --> 00:01:39,799
provide that technology is going to lead

00:01:35,149 --> 00:01:43,219
in this domain space Intel is going to

00:01:39,799 --> 00:01:46,700
launch this year two devices navona

00:01:43,219 --> 00:01:48,950
neural network processor completely

00:01:46,700 --> 00:01:51,350
dedicated to help our customers and

00:01:48,950 --> 00:01:54,289
partners in this journey in AI this

00:01:51,350 --> 00:01:57,710
growing journey in AI highly scalability

00:01:54,289 --> 00:02:01,430
to help the research go beyond what's

00:01:57,710 --> 00:02:04,159
currently possible we're going to have

00:02:01,430 --> 00:02:06,820
two flavors one for inference and one

00:02:04,159 --> 00:02:06,820

YouTube URL: https://www.youtube.com/watch?v=tTCtK9Kb5Is


