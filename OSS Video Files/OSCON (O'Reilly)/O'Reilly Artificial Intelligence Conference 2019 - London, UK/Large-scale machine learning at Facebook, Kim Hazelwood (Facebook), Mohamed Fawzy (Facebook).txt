Title: Large-scale machine learning at Facebook, Kim Hazelwood (Facebook), Mohamed Fawzy (Facebook)
Publication date: 2019-10-16
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	Large-scale machine learning at Facebook: Implications of platform design on developer productivity

AI plays a key role in achieving Facebookâ€™s mission of connecting people and building communities. Nearly every visible product is powered by machine learning algorithms at its core, from delivering relevant content to making the platform safe. Scaling these products to billions of global users has uncovered many fascinating challenges at every layer in the systems stack, such as uncovering computational and storage bottlenecks, making the ML platform efficient and productive for the ML engineers and tackling critical challenges such as privacy and environmental sustainability.
Kim Hazelwood and Mohamed Fawzy offer an end-to-end look at how applied ML has continued to change the landscape of the platforms and infrastructure at Facebook.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,780 --> 00:00:06,430
so how should we focus on that developer

00:00:04,090 --> 00:00:09,309
productivity should we be focusing on

00:00:06,430 --> 00:00:12,070
software platforms or should be focusing

00:00:09,309 --> 00:00:15,070
on hardware infrastructure before you

00:00:12,070 --> 00:00:17,020
answer I think I'm gonna predict that

00:00:15,070 --> 00:00:19,660
we're gonna disagree on this one so why

00:00:17,020 --> 00:00:21,430
don't we each state our case I'll make

00:00:19,660 --> 00:00:23,020
my case for hardware infrastructure and

00:00:21,430 --> 00:00:28,029
you can make your case for software

00:00:23,020 --> 00:00:30,570
platforms ladies first so I will claim

00:00:28,029 --> 00:00:33,309
that the one of the easiest ways to

00:00:30,570 --> 00:00:36,520
increase developer productivity is to

00:00:33,309 --> 00:00:38,829
speed up the platform ok so if you look

00:00:36,520 --> 00:00:41,170
at the end-to-end ml lifecycle it is

00:00:38,829 --> 00:00:43,870
literally packed with infrastructure

00:00:41,170 --> 00:00:46,149
challenges that we are very optimized to

00:00:43,870 --> 00:00:47,200
help solve this is driving a lot of the

00:00:46,149 --> 00:00:49,420
innovation that we're seeing within

00:00:47,200 --> 00:00:51,640
Facebook we've got storage challenges

00:00:49,420 --> 00:00:53,800
that are making us rethink and redesign

00:00:51,640 --> 00:00:55,570
our storage infrastructure network

00:00:53,800 --> 00:00:58,539
challenges and compute challenges as

00:00:55,570 --> 00:01:00,070
well so let's go through some of the

00:00:58,539 --> 00:01:02,230
compute challenges that we've had and

00:01:00,070 --> 00:01:05,590
how that has driven innovation within

00:01:02,230 --> 00:01:07,840
Facebook many of you may know that our

00:01:05,590 --> 00:01:10,510
hardware infrastructure is open sourced

00:01:07,840 --> 00:01:12,939
through the open compute program so we

00:01:10,510 --> 00:01:13,990
are very transparent about the kinds of

00:01:12,939 --> 00:01:16,420
computer infrastructure that we're

00:01:13,990 --> 00:01:19,180
designing at Facebook so let me walk you

00:01:16,420 --> 00:01:21,450
through the evolution of our machine

00:01:19,180 --> 00:01:24,490
learning training compute infrastructure

00:01:21,450 --> 00:01:27,490
so back in 2013 we were one of the first

00:01:24,490 --> 00:01:29,110
to deploy GPUs into production for

00:01:27,490 --> 00:01:32,979
machine learning training we started

00:01:29,110 --> 00:01:34,900
with a commodity system from using that

00:01:32,979 --> 00:01:37,360
for a couple years we realized that we

00:01:34,900 --> 00:01:39,070
needed to innovate in order to meet some

00:01:37,360 --> 00:01:43,000
of our internal data center efficiency

00:01:39,070 --> 00:01:47,560
and power goals so we designed Big Sur

00:01:43,000 --> 00:01:50,200
and 2015 as our machine learning use

00:01:47,560 --> 00:01:52,659
cases grew from there we realized we

00:01:50,200 --> 00:01:55,180
needed a bit more flexibility and so

00:01:52,659 --> 00:01:57,909
that's when we designed Tioga Pass and

00:01:55,180 --> 00:02:01,540
Big Basin Volta that allowed us to

00:01:57,909 --> 00:02:04,540
disaggregate CPU and GPU to modify the

00:02:01,540 --> 00:02:05,680
ratios of the different types of compute

00:02:04,540 --> 00:02:08,380
that we might need to do machine

00:02:05,680 --> 00:02:10,560
learning training and now as we look

00:02:08,380 --> 00:02:13,660
forward the future is all about

00:02:10,560 --> 00:02:14,370
customization so that's why we recently

00:02:13,660 --> 00:02:17,310
announced

00:02:14,370 --> 00:02:20,610
our Zion hardware training platform that

00:02:17,310 --> 00:02:23,730
leverages up to eight CPUs and up to

00:02:20,610 --> 00:02:25,610
eight custom accelerators that meet the

00:02:23,730 --> 00:02:29,129
standard form factor that we've defined

00:02:25,610 --> 00:02:31,049
and you get all of that for free in

00:02:29,129 --> 00:02:37,170
order to increase developer productivity

00:02:31,049 --> 00:02:38,549
and overall iteration speed in okay this

00:02:37,170 --> 00:02:40,980
is all awesome innovation

00:02:38,549 --> 00:02:42,870
they're certainly speeding up the

00:02:40,980 --> 00:02:47,250
performance of how can how fast we can

00:02:42,870 --> 00:02:49,079
train but how about usability try to the

00:02:47,250 --> 00:02:50,940
worst thing that you could do is expose

00:02:49,079 --> 00:02:54,480
all this hardware complexity to ml

00:02:50,940 --> 00:02:56,160
engineers try to expose this complexity

00:02:54,480 --> 00:02:58,380
without an orchestration and a

00:02:56,160 --> 00:03:00,450
scheduling layer that can abstract away

00:02:58,380 --> 00:03:01,680
this hardware and make it easy to

00:03:00,450 --> 00:03:04,470
schedule on this different type of

00:03:01,680 --> 00:03:06,690
hardware capabilities on top of that

00:03:04,470 --> 00:03:08,519
we're running large-scale distributed

00:03:06,690 --> 00:03:11,220
training jobs when you run in this

00:03:08,519 --> 00:03:12,810
environment failures are norm so how do

00:03:11,220 --> 00:03:14,940
we develop a platform on a runtime

00:03:12,810 --> 00:03:17,610
environment they can provide resilient

00:03:14,940 --> 00:03:21,620
execution through checkpointing and can

00:03:17,610 --> 00:03:23,730
provide reliability for it this ml jobs

00:03:21,620 --> 00:03:25,650
machine learning engineers wanted to

00:03:23,730 --> 00:03:27,090
develop their pipelines as well you need

00:03:25,650 --> 00:03:29,280
a set of development and deployment

00:03:27,090 --> 00:03:31,290
tools so that we can author the

00:03:29,280 --> 00:03:34,530
pipelines and deploy to production with

00:03:31,290 --> 00:03:36,780
ease collectively we refer to these

00:03:34,530 --> 00:03:40,650
layers of platforms as the FBI learner

00:03:36,780 --> 00:03:43,620
flow platform but that's not enough on

00:03:40,650 --> 00:03:45,419
top of this ml engineers want to

00:03:43,620 --> 00:03:47,849
distribute their model use deep learning

00:03:45,419 --> 00:03:49,620
we have model parallelism theater

00:03:47,849 --> 00:03:52,609
parallelism you want to scale these

00:03:49,620 --> 00:03:55,169
models you want a platform to ease that

00:03:52,609 --> 00:03:58,650
scalability so we built the distributed

00:03:55,169 --> 00:04:00,030
training platform we all know ml takes a

00:03:58,650 --> 00:04:01,980
lot of experiments there's a lot of

00:04:00,030 --> 00:04:03,989
hyper parameter tweaking feature

00:04:01,980 --> 00:04:05,940
engineering there's a lot of model

00:04:03,989 --> 00:04:08,389
selection so we built an OT tuning

00:04:05,940 --> 00:04:11,549
platform to all to also help them

00:04:08,389 --> 00:04:13,109
through this process collectively we

00:04:11,549 --> 00:04:15,810
refer to this as the distributed AI

00:04:13,109 --> 00:04:18,299
platform they've been a flow platform

00:04:15,810 --> 00:04:20,280
and distributed AI platform form our

00:04:18,299 --> 00:04:23,310
experimentation management system that

00:04:20,280 --> 00:04:27,289
provide flexibility for ml engineers

00:04:23,310 --> 00:04:27,289
hence increase the productivity

00:04:33,740 --> 00:04:35,800

YouTube URL: https://www.youtube.com/watch?v=HZFsL9L_wEI


