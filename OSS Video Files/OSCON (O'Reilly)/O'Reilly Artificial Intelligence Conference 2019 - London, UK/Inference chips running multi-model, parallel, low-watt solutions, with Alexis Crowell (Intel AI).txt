Title: Inference chips running multi-model, parallel, low-watt solutions, with Alexis Crowell (Intel AI)
Publication date: 2019-10-17
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,149 --> 00:00:05,759
so we had a really fun you know a

00:00:03,179 --> 00:00:06,810
keynote speaker this morning for this

00:00:05,759 --> 00:00:09,809
conference that we were co-chairing

00:00:06,810 --> 00:00:12,120
where Martha had talked about just sort

00:00:09,809 --> 00:00:13,740
of a lot of the you know unknown

00:00:12,120 --> 00:00:15,809
unknowns when it comes to a Ida playing

00:00:13,740 --> 00:00:18,810
ad systems and things that it's just I

00:00:15,809 --> 00:00:20,189
come up around trust and unconscious

00:00:18,810 --> 00:00:22,140
bias and all that so I'm kind of curious

00:00:20,189 --> 00:00:23,670
um you know how do you youth how do you

00:00:22,140 --> 00:00:25,560
think about how we should you know

00:00:23,670 --> 00:00:27,029
confront those issues everything about

00:00:25,560 --> 00:00:29,640
yeah I systems deployed in the real

00:00:27,029 --> 00:00:30,869
world oh god yeah her her conversation

00:00:29,640 --> 00:00:33,930
was fascinating right should we put a

00:00:30,869 --> 00:00:35,190
black box in cars it was really

00:00:33,930 --> 00:00:37,469
interesting some of the research she's

00:00:35,190 --> 00:00:39,989
doing you know so we look at at it

00:00:37,469 --> 00:00:41,820
through a lens of we can put as much

00:00:39,989 --> 00:00:44,219
trust into the system as we can and the

00:00:41,820 --> 00:00:46,079
way we try and attack that is before we

00:00:44,219 --> 00:00:47,430
actually implement AI and I'm talking

00:00:46,079 --> 00:00:49,110
about what we do inside of our own walls

00:00:47,430 --> 00:00:51,570
not just what how we help other

00:00:49,110 --> 00:00:53,670
customers um we look at five questions

00:00:51,570 --> 00:00:56,280
right and those five questions help us

00:00:53,670 --> 00:00:58,859
determine at a minimum are we doing

00:00:56,280 --> 00:01:03,059
everything that we can to make it as

00:00:58,859 --> 00:01:05,250
ethical and as moral and as right in our

00:01:03,059 --> 00:01:07,140
AI decision-making as we possibly can do

00:01:05,250 --> 00:01:09,270
and those five questions are are we even

00:01:07,140 --> 00:01:10,979
asking the right question right does the

00:01:09,270 --> 00:01:13,590
question make any sense should we be

00:01:10,979 --> 00:01:15,780
asking it are we using the right kind of

00:01:13,590 --> 00:01:19,080
data right bias is a huge deal you know

00:01:15,780 --> 00:01:20,970
that um and if there's an inherent bias

00:01:19,080 --> 00:01:22,740
which there always is because there's

00:01:20,970 --> 00:01:25,170
humans involved in making sure that data

00:01:22,740 --> 00:01:27,060
is clean and ready how are we at at

00:01:25,170 --> 00:01:30,150
least mitigating as much as we can is

00:01:27,060 --> 00:01:33,720
the analysis accurate are we looking at

00:01:30,150 --> 00:01:35,460
it objectively what do we plan to do how

00:01:33,720 --> 00:01:38,130
are we interpreting those results and

00:01:35,460 --> 00:01:39,659
then finally based on the decisions what

00:01:38,130 --> 00:01:42,479
are we make like how are we making

00:01:39,659 --> 00:01:44,640
decisions based on the insights and at

00:01:42,479 --> 00:01:46,979
least what we found is that's helping us

00:01:44,640 --> 00:01:48,899
get more and more trust into the system

00:01:46,979 --> 00:01:50,939
because we at least we know all of the

00:01:48,899 --> 00:01:52,890
input was as clean as it could be and as

00:01:50,939 --> 00:01:54,689
solid as it could be right garbage in

00:01:52,890 --> 00:01:56,340
garbage out we're trying to look at it

00:01:54,689 --> 00:01:59,460
from the other method right

00:01:56,340 --> 00:02:00,719
golden hopefully more gold out now it's

00:01:59,460 --> 00:02:03,060
interesting cuz you mentioned how you

00:02:00,719 --> 00:02:04,140
have these internal ways you you kind of

00:02:03,060 --> 00:02:06,540
confront how do you think about these

00:02:04,140 --> 00:02:08,250
pretty tough challenges internally and I

00:02:06,540 --> 00:02:09,569
feel like that's probably something that

00:02:08,250 --> 00:02:12,420
every company that wants to adopt the AI

00:02:09,569 --> 00:02:13,380
has to deal with rights I think how has

00:02:12,420 --> 00:02:16,200
incorporated

00:02:13,380 --> 00:02:19,200
technology adapted how you guys think

00:02:16,200 --> 00:02:20,130
even operationally around how do you put

00:02:19,200 --> 00:02:20,850
a business around the smell of

00:02:20,130 --> 00:02:25,260
Technology

00:02:20,850 --> 00:02:27,390
oh gosh good question so um what we're

00:02:25,260 --> 00:02:29,630
doing internally is we've got a lot of

00:02:27,390 --> 00:02:32,310
different pocs in addition to some of

00:02:29,630 --> 00:02:34,530
some full deployments right so we've

00:02:32,310 --> 00:02:36,390
actually tested AI inside of our

00:02:34,530 --> 00:02:39,000
factories to help do defect detection

00:02:36,390 --> 00:02:40,920
right we're actually a massive

00:02:39,000 --> 00:02:43,740
manufacturing company we build a lot of

00:02:40,920 --> 00:02:45,810
chips right and it takes a lot of human

00:02:43,740 --> 00:02:48,000
ingenuity and understanding to look at

00:02:45,810 --> 00:02:49,410
those and help define defects at a

00:02:48,000 --> 00:02:51,300
computer vision has really helped cut

00:02:49,410 --> 00:02:53,370
down in some of our some of our

00:02:51,300 --> 00:02:56,370
factories which is cool we've

00:02:53,370 --> 00:02:59,760
implemented it in even some of the back

00:02:56,370 --> 00:03:01,950
end processes so for us it tends to be

00:02:59,760 --> 00:03:03,690
more on the operational costs how can we

00:03:01,950 --> 00:03:05,190
help reduce operational costs and put

00:03:03,690 --> 00:03:07,740
more money into the research and

00:03:05,190 --> 00:03:10,110
development and the innovative side and

00:03:07,740 --> 00:03:11,490
less money into the how do we make sure

00:03:10,110 --> 00:03:14,070
our business is running as efficient as

00:03:11,490 --> 00:03:15,240
we can yeah I love the recursiveness

00:03:14,070 --> 00:03:17,160
with that by the way building hardware

00:03:15,240 --> 00:03:19,560
that enables the eye and I'm reapplying

00:03:17,160 --> 00:03:21,150
the eye to build better hardware that's

00:03:19,560 --> 00:03:22,620
amazing

00:03:21,150 --> 00:03:23,700
so how and how do you guys just think

00:03:22,620 --> 00:03:25,440
what kind of use case is right I think

00:03:23,700 --> 00:03:28,260
we're in this really interesting time

00:03:25,440 --> 00:03:29,820
when it comes to I you know AI can do

00:03:28,260 --> 00:03:31,560
everything but you obviously had to pick

00:03:29,820 --> 00:03:33,960
one of our sequences how do you guys

00:03:31,560 --> 00:03:35,910
think about use cases that excite you I

00:03:33,960 --> 00:03:37,200
so it kind of depends on who you talk to

00:03:35,910 --> 00:03:39,270
you right until is a really big company

00:03:37,200 --> 00:03:40,890
if you talk to me the use cases that

00:03:39,270 --> 00:03:42,990
really excite me and I love working on

00:03:40,890 --> 00:03:45,270
or more of the social good ones right so

00:03:42,990 --> 00:03:47,459
I spend time with the Red Cross and with

00:03:45,270 --> 00:03:49,080
companies that are trying to do really

00:03:47,459 --> 00:03:51,060
innovative things in medical to make

00:03:49,080 --> 00:03:52,800
people's lives better we've got folks

00:03:51,060 --> 00:03:55,380
that are really passionate about making

00:03:52,800 --> 00:03:58,530
business better right and they spend a

00:03:55,380 --> 00:04:01,490
lot of time with FSI and with oil and

00:03:58,530 --> 00:04:05,220
gas and looking at how do we automate

00:04:01,490 --> 00:04:07,410
you know preventative maintenance for

00:04:05,220 --> 00:04:09,959
example how do we help oil companies

00:04:07,410 --> 00:04:13,110
scan their lines and look at their rigs

00:04:09,959 --> 00:04:15,300
in the ocean and understand what could

00:04:13,110 --> 00:04:17,130
fail before it'll fail and help prevent

00:04:15,300 --> 00:04:18,480
you know big time issues both from the

00:04:17,130 --> 00:04:20,400
cost perspective and an environmental

00:04:18,480 --> 00:04:22,470
perspective so we kind of look at it

00:04:20,400 --> 00:04:24,660
based on what problems can we solve and

00:04:22,470 --> 00:04:25,830
what are our customers really trying to

00:04:24,660 --> 00:04:28,080
tackle today and the

00:04:25,830 --> 00:04:29,610
get engaged at that level tell me more

00:04:28,080 --> 00:04:31,050
about that Red Cross example I'm kind of

00:04:29,610 --> 00:04:33,240
curious how would they think about

00:04:31,050 --> 00:04:34,409
employing yeah well you'll have to stay

00:04:33,240 --> 00:04:37,289
tuned because we've got some really cool

00:04:34,409 --> 00:04:39,389
news coming out about that one but but I

00:04:37,289 --> 00:04:41,400
mean you can think to like what we're

00:04:39,389 --> 00:04:43,680
doing with trail guard have you heard of

00:04:41,400 --> 00:04:46,020
this one it's kind of a similar idea

00:04:43,680 --> 00:04:48,150
right how do you get AI into an

00:04:46,020 --> 00:04:51,000
environment where your power is probably

00:04:48,150 --> 00:04:53,789
not super accessible there's not a lot

00:04:51,000 --> 00:04:56,159
of connectivity and you want to go in

00:04:53,789 --> 00:04:58,229
and really help make a difference so

00:04:56,159 --> 00:05:00,990
trail guard is this little tiny camera

00:04:58,229 --> 00:05:03,930
that has a couple years of battery life

00:05:00,990 --> 00:05:06,750
and it can distinguish between wildlife

00:05:03,930 --> 00:05:08,789
and poachers right so if you think about

00:05:06,750 --> 00:05:11,190
you know the Sahara Desert and trying to

00:05:08,789 --> 00:05:14,490
help prevent poaching you're gonna have

00:05:11,190 --> 00:05:16,139
lions and tigers and elephants walking

00:05:14,490 --> 00:05:17,789
in front of your cameras and you don't

00:05:16,139 --> 00:05:19,379
want to you don't want to take up

00:05:17,789 --> 00:05:21,029
battery life by having your camera turn

00:05:19,379 --> 00:05:23,009
on send something back through a

00:05:21,029 --> 00:05:24,779
satellite and say yeah it's just an

00:05:23,009 --> 00:05:27,270
animal this this is good we want animals

00:05:24,779 --> 00:05:28,800
there you want it to know and turn on

00:05:27,270 --> 00:05:31,710
only when there's a human where they

00:05:28,800 --> 00:05:33,330
that human shouldn't be so trail guards

00:05:31,710 --> 00:05:35,610
doing some really cool stuff actually

00:05:33,330 --> 00:05:38,279
with the mo videos Vpu in it that's got

00:05:35,610 --> 00:05:40,560
super low power actually allows them to

00:05:38,279 --> 00:05:43,050
extend battery life so that you don't

00:05:40,560 --> 00:05:44,310
have to send people in you know every

00:05:43,050 --> 00:05:46,500
couple of weeks or every couple of

00:05:44,310 --> 00:05:48,750
months to change the battery out yeah

00:05:46,500 --> 00:05:51,120
yeah it's interesting I think when

00:05:48,750 --> 00:05:53,669
people do about hardware new hardware

00:05:51,120 --> 00:05:55,620
solutions for AI you know a lot of

00:05:53,669 --> 00:05:57,120
attention rightfully so on on the

00:05:55,620 --> 00:05:58,379
training side basket inference is just

00:05:57,120 --> 00:06:00,509
as important having your hardware for

00:05:58,379 --> 00:06:01,979
infants because you know in examples you

00:06:00,509 --> 00:06:04,409
highlighted where you have some that's

00:06:01,979 --> 00:06:06,120
not connected to a power outlet yeah

00:06:04,409 --> 00:06:07,830
right I think got energy efficiency and

00:06:06,120 --> 00:06:09,029
things like that matter a lot how do you

00:06:07,830 --> 00:06:11,129
guys think about that market the

00:06:09,029 --> 00:06:12,930
infant's market oh so we actually are

00:06:11,129 --> 00:06:14,819
looking at the inference market as being

00:06:12,930 --> 00:06:17,310
a lot bigger than the training market

00:06:14,819 --> 00:06:19,440
right because once you train models its

00:06:17,310 --> 00:06:21,000
iterative after that and you really want

00:06:19,440 --> 00:06:23,759
to deploy it and when you're deploy it

00:06:21,000 --> 00:06:25,440
that's where you're gonna learn so I we

00:06:23,759 --> 00:06:27,839
see inference at the edges being a huge

00:06:25,440 --> 00:06:30,300
opportunity visions obviously big right

00:06:27,839 --> 00:06:32,310
now right and helping people protect

00:06:30,300 --> 00:06:34,500
their homes right who's coming in and

00:06:32,310 --> 00:06:37,860
out only unlocking the doors for people

00:06:34,500 --> 00:06:39,520
that make sense there's a lot in terms

00:06:37,860 --> 00:06:41,740
of speech recognition right

00:06:39,520 --> 00:06:44,349
if the Alexus of the world in the series

00:06:41,740 --> 00:06:47,889
and the Google Homes a lot of that

00:06:44,349 --> 00:06:49,180
natural language detection but then what

00:06:47,889 --> 00:06:50,860
we it's really cool is when you combine

00:06:49,180 --> 00:06:52,720
all that together right and you get

00:06:50,860 --> 00:06:57,160
inference chips that can run multi model

00:06:52,720 --> 00:06:58,630
all paralyzed in a low watt solution so

00:06:57,160 --> 00:07:00,400
that you can truly make it easier for

00:06:58,630 --> 00:07:02,110
people to live right whether that's in a

00:07:00,400 --> 00:07:03,280
business setting and you're inside of a

00:07:02,110 --> 00:07:04,419
conference room and it's dimming the

00:07:03,280 --> 00:07:06,400
lights because you're not in there so

00:07:04,419 --> 00:07:08,500
it's saving energy or you're in a home

00:07:06,400 --> 00:07:09,910
setting and it's making sure your

00:07:08,500 --> 00:07:12,069
two-year-old isn't running up and

00:07:09,910 --> 00:07:13,360
turning on your stove yeah it's exciting

00:07:12,069 --> 00:07:15,280
I think that the infants market in

00:07:13,360 --> 00:07:16,659
general is exciting I think the edge the

00:07:15,280 --> 00:07:18,580
role that edge devices are gonna plan it

00:07:16,659 --> 00:07:20,169
it's gonna be pretty huge I think

00:07:18,580 --> 00:07:22,419
there's nothing that that it hints as

00:07:20,169 --> 00:07:24,270
well to which is you know privacy issues

00:07:22,419 --> 00:07:26,889
when it comes to to data in your I and

00:07:24,270 --> 00:07:28,509
infants at the edge kind of unlocks new

00:07:26,889 --> 00:07:29,979
capability some privacy front as well to

00:07:28,509 --> 00:07:30,639
use I love to kind of hear how you guys

00:07:29,979 --> 00:07:33,610
think about that

00:07:30,639 --> 00:07:35,349
yeah so we're big believers in moving

00:07:33,610 --> 00:07:37,060
the compute as close to the data ingest

00:07:35,349 --> 00:07:40,300
as possible right because as soon as you

00:07:37,060 --> 00:07:42,159
can run the ingest with where your data

00:07:40,300 --> 00:07:44,800
is being created you don't have to send

00:07:42,159 --> 00:07:47,380
it anywhere and the less distance in

00:07:44,800 --> 00:07:49,539
that data computation the less attack

00:07:47,380 --> 00:07:50,979
surface so we think that the more we can

00:07:49,539 --> 00:07:52,509
move it out towards the edge and the

00:07:50,979 --> 00:07:54,340
closer we can get to where people

00:07:52,509 --> 00:07:56,229
actually want their insights delivered

00:07:54,340 --> 00:07:58,810
the better is going to be for security

00:07:56,229 --> 00:08:01,000
for privacy we don't have to bring

00:07:58,810 --> 00:08:03,039
anyone else into the mix right a lot of

00:08:01,000 --> 00:08:04,719
companies are running services through

00:08:03,039 --> 00:08:07,210
the cloud which is fantastic it's a

00:08:04,719 --> 00:08:10,270
great infrastructure but you want that

00:08:07,210 --> 00:08:13,090
you want that actual solution deployed

00:08:10,270 --> 00:08:16,020
at the edge so that you're not you know

00:08:13,090 --> 00:08:18,819
adding complications and attack layers

00:08:16,020 --> 00:08:21,099
in your data that's that's great I mean

00:08:18,819 --> 00:08:22,840
I guess to close up just imparting

00:08:21,099 --> 00:08:25,000
lessons I guess so as you've worked on

00:08:22,840 --> 00:08:26,919
this as a pioneer in the field of AI

00:08:25,000 --> 00:08:28,419
particularly with this hardware

00:08:26,919 --> 00:08:30,159
component with the business that you

00:08:28,419 --> 00:08:32,200
guys have an Intel and you kind of learn

00:08:30,159 --> 00:08:34,300
learn lessons over recent years as you

00:08:32,200 --> 00:08:36,370
go to market with these new technology

00:08:34,300 --> 00:08:41,229
you know I think the biggest one is that

00:08:36,370 --> 00:08:43,360
I'm we've had solutions and market from

00:08:41,229 --> 00:08:44,890
a hardware perspective that have been

00:08:43,360 --> 00:08:47,110
there for years and they've been good

00:08:44,890 --> 00:08:49,180
enough right it's getting us to where we

00:08:47,110 --> 00:08:52,070
are today which is huge advancements

00:08:49,180 --> 00:08:53,750
huge leaps in both training infra

00:08:52,070 --> 00:08:55,519
and even just moving along classical

00:08:53,750 --> 00:08:57,470
machine learning but I think as we're

00:08:55,519 --> 00:08:59,720
going into kind of this next phase and

00:08:57,470 --> 00:09:01,699
getting bigger models you know billions

00:08:59,720 --> 00:09:03,410
of parameters trillions of data inputs I

00:09:01,699 --> 00:09:05,839
think we need to rethink how we're

00:09:03,410 --> 00:09:07,519
looking at hardware and what sort of

00:09:05,839 --> 00:09:09,649
underlying technology what sort of

00:09:07,519 --> 00:09:12,290
underlying memory access to storage is

00:09:09,649 --> 00:09:13,730
really going to be available so that all

00:09:12,290 --> 00:09:15,589
of the cool stuff that people are

00:09:13,730 --> 00:09:17,420
thinking about all of the innovation can

00:09:15,589 --> 00:09:18,680
actually happen right you've got to have

00:09:17,420 --> 00:09:21,829
the compute you got to have the software

00:09:18,680 --> 00:09:24,350
right those two can't get decoupled but

00:09:21,829 --> 00:09:26,089
you've got to have both to really unlock

00:09:24,350 --> 00:09:29,329
things like GPT to and whatever else

00:09:26,089 --> 00:09:31,430
comes after that it's a holistic problem

00:09:29,329 --> 00:09:33,019
it is a holistic problem without a doubt

00:09:31,430 --> 00:09:35,709
well thank you so much for enjoyed this

00:09:33,019 --> 00:09:35,709
thanks to you

00:09:41,350 --> 00:09:43,410

YouTube URL: https://www.youtube.com/watch?v=u8884bQVmOM


