Title: When to trust AI, Marta Kwiatkowska (Trinity College, University of Oxford)
Publication date: 2019-10-17
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	Machine learning solutions are revolutionizing AI, but their instability against adversarial examples—small perturbations to inputs that can catastrophically affect the output—raises concerns about the readiness of this technology for widespread deployment.
Marta Kwiatkowska uses illustrative examples to give you an overview of techniques being developed to improve the robustness, safety, and trust in AI systems.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,760 --> 00:00:08,590
would you trust a self-driving car and

00:00:05,819 --> 00:00:11,709
specifically how many of you would trust

00:00:08,590 --> 00:00:17,410
a self-driving car to take your child to

00:00:11,709 --> 00:00:18,339
school can you raise your hand okay very

00:00:17,410 --> 00:00:21,130
good very good

00:00:18,339 --> 00:00:24,519
okay still in a minority now of course

00:00:21,130 --> 00:00:27,550
if you live in Arizona you can already

00:00:24,519 --> 00:00:31,210
do that because they are piloting you no

00:00:27,550 --> 00:00:34,030
way no there are trials of five AI and

00:00:31,210 --> 00:00:38,370
ox potica in London so you can see them

00:00:34,030 --> 00:00:42,640
around the corner but perhaps those who

00:00:38,370 --> 00:00:45,059
didn't raise their hand will be aware

00:00:42,640 --> 00:00:48,100
that we've already had some fatal

00:00:45,059 --> 00:00:51,969
accidents and in particular Tesla which

00:00:48,100 --> 00:00:55,379
has an autonomous mode and it is powered

00:00:51,969 --> 00:01:00,550
by a deep learning controller actually

00:00:55,379 --> 00:01:04,440
killed a driver and we also had an uber

00:01:00,550 --> 00:01:07,570
crash uber uses lider

00:01:04,440 --> 00:01:10,660
technology and also uses deep learning

00:01:07,570 --> 00:01:15,250
computer vision in order to identify

00:01:10,660 --> 00:01:18,250
objects ahead of the car now the reasons

00:01:15,250 --> 00:01:21,040
for the crash are a lot more complex

00:01:18,250 --> 00:01:23,980
because there was a test driver in there

00:01:21,040 --> 00:01:27,220
but the biggest question to people like

00:01:23,980 --> 00:01:32,140
me scientists like me is how can this

00:01:27,220 --> 00:01:33,480
happen if we often have 99.9 accuracy

00:01:32,140 --> 00:01:36,790
okay

00:01:33,480 --> 00:01:40,420
99.9 where does it come from you are

00:01:36,790 --> 00:01:45,160
getting this solution a black box and it

00:01:40,420 --> 00:01:48,850
has on the label 99 percent now of

00:01:45,160 --> 00:01:51,190
course as a scientist I also know that

00:01:48,850 --> 00:01:53,500
machine learning researchers have

00:01:51,190 --> 00:01:57,520
already discovered the reasons for this

00:01:53,500 --> 00:02:00,700
and this is because deep learning can be

00:01:57,520 --> 00:02:03,220
fooled and it can be fooled very easily

00:02:00,700 --> 00:02:06,310
so this is the same deep learning that

00:02:03,220 --> 00:02:09,929
can beat human performance in computer

00:02:06,310 --> 00:02:13,810
vision but it turns out that it is very

00:02:09,929 --> 00:02:16,660
unstable and very brittle with respect

00:02:13,810 --> 00:02:20,110
two very small modifications to the

00:02:16,660 --> 00:02:23,290
input so in this particular case you

00:02:20,110 --> 00:02:26,709
take an image that's classified as a

00:02:23,290 --> 00:02:30,370
panda you add some noise to a human it

00:02:26,709 --> 00:02:33,040
looks identical but it will classify it

00:02:30,370 --> 00:02:35,819
as something completely different and of

00:02:33,040 --> 00:02:41,260
course this is a security risk this is a

00:02:35,819 --> 00:02:46,660
safety risk and if I take you to the

00:02:41,260 --> 00:02:51,299
autonomous driving then my question is

00:02:46,660 --> 00:02:55,569
should we worry about autonomous driving

00:02:51,299 --> 00:03:00,849
well what you see there is as some

00:02:55,569 --> 00:03:05,230
images from dashboard cameras the same

00:03:00,849 --> 00:03:08,860
kind of camera that Tesla has and it's

00:03:05,230 --> 00:03:12,959
looking at an urban scene what you can

00:03:08,860 --> 00:03:16,019
also see there is a single pixel

00:03:12,959 --> 00:03:21,250
highlighted and that picture was found

00:03:16,019 --> 00:03:25,870
by my students software now if you look

00:03:21,250 --> 00:03:29,890
at these the deep learning network

00:03:25,870 --> 00:03:33,940
actually classified these images into

00:03:29,890 --> 00:03:38,019
those which have a red or green traffic

00:03:33,940 --> 00:03:40,450
light if you take something with a red

00:03:38,019 --> 00:03:43,389
traffic light imagine you are sitting in

00:03:40,450 --> 00:03:46,600
that car so you are stopped if you can

00:03:43,389 --> 00:03:48,790
attack in something like three seconds

00:03:46,600 --> 00:03:51,069
it is possible to change just this one

00:03:48,790 --> 00:03:53,440
pixel and it will immediately change the

00:03:51,069 --> 00:03:56,440
classification and if it changes to

00:03:53,440 --> 00:04:00,549
green well I wouldn't want to be in this

00:03:56,440 --> 00:04:04,380
car now what you can show is with these

00:04:00,549 --> 00:04:09,269
kinds of modifications you can reduce

00:04:04,380 --> 00:04:13,329
the network's accuracy to zero percent

00:04:09,269 --> 00:04:15,910
okay this is the same black box exactly

00:04:13,329 --> 00:04:20,260
the same network the only thing that

00:04:15,910 --> 00:04:22,560
I've done is I've modified the input

00:04:20,260 --> 00:04:22,560

YouTube URL: https://www.youtube.com/watch?v=skMHsapqVTo


