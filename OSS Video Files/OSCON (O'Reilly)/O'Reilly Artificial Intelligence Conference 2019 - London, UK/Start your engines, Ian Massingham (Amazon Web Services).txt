Title: Start your engines, Ian Massingham (Amazon Web Services)
Publication date: 2019-10-17
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	Start your engines: Making deep reinforcement learning accessible to all developers

Reinforcement learning is an advanced machine learning technique that makes short-term decisions while optimizing for a longer-term goal through trial and error. Ian Massingham dives into state-of-the-art techniques in deep reinforcement learning (DRL) for a variety of use cases. He also explores the efforts that AWS has been making to make DRL accessible to a broader population of software developers.
This keynote is sponsored by AWS.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,560 --> 00:00:05,279
good morning it's great to be back here

00:00:03,840 --> 00:00:07,290
for the second year I was back I was

00:00:05,279 --> 00:00:09,720
here last year at O'Reilly Island and

00:00:07,290 --> 00:00:11,160
talking a little bit about AWS is work

00:00:09,720 --> 00:00:12,750
in their machine learning field and

00:00:11,160 --> 00:00:14,460
today I want to dive into deep

00:00:12,750 --> 00:00:16,320
reinforcement learning a little bit talk

00:00:14,460 --> 00:00:18,359
a little bit about what this field is

00:00:16,320 --> 00:00:20,580
and how it emerged talk about a few

00:00:18,359 --> 00:00:21,630
example use cases but I'm also going to

00:00:20,580 --> 00:00:23,850
talk about some work that we've been

00:00:21,630 --> 00:00:26,070
doing on the developer experience side

00:00:23,850 --> 00:00:29,340
at AWS to try and get more software

00:00:26,070 --> 00:00:32,400
developers using this particular type of

00:00:29,340 --> 00:00:33,719
machine learning technique challenge

00:00:32,400 --> 00:00:36,180
that a lot of us can think about we

00:00:33,719 --> 00:00:37,620
probably have ideas for exploiting ml to

00:00:36,180 --> 00:00:40,020
solve business problems and helping

00:00:37,620 --> 00:00:42,120
advance the agendas of our organizations

00:00:40,020 --> 00:00:43,980
but developing the skills that we need

00:00:42,120 --> 00:00:45,570
in order to make that idea a reality is

00:00:43,980 --> 00:00:46,829
actually quite a big challenge and we've

00:00:45,570 --> 00:00:48,660
been trying to help our customers

00:00:46,829 --> 00:00:51,500
overcome that through something that

00:00:48,660 --> 00:00:53,760
we've created to help developers

00:00:51,500 --> 00:00:54,780
accelerate their ML skills development

00:00:53,760 --> 00:00:57,239
that's going to talk about at the end of

00:00:54,780 --> 00:00:59,070
this talk so what is deep reinforcement

00:00:57,239 --> 00:01:02,399
learning and how did it emerge back in

00:00:59,070 --> 00:01:04,350
the 1950s there were two emergent areas

00:01:02,399 --> 00:01:05,880
of research systems and computer science

00:01:04,350 --> 00:01:07,680
research one was trial and error

00:01:05,880 --> 00:01:09,150
learning as an engineering principle and

00:01:07,680 --> 00:01:11,820
most of the research here was actually

00:01:09,150 --> 00:01:13,380
about trying to first of all quantify

00:01:11,820 --> 00:01:14,850
the amount of time that would be taken

00:01:13,380 --> 00:01:17,159
for trial and and trial and error

00:01:14,850 --> 00:01:19,500
learning to be done and then secondly

00:01:17,159 --> 00:01:21,390
trying to reduce that elapsed time for

00:01:19,500 --> 00:01:23,340
trial and error learning by scoping down

00:01:21,390 --> 00:01:26,009
the problem space and then secondly

00:01:23,340 --> 00:01:27,420
another area of research in optimal

00:01:26,009 --> 00:01:29,369
control through dynamic programming this

00:01:27,420 --> 00:01:32,310
is breaking up complex nonlinear

00:01:29,369 --> 00:01:34,259
problems into subproblems and then

00:01:32,310 --> 00:01:36,090
solving those subproblems recursively

00:01:34,259 --> 00:01:38,250
and that can really really accelerate

00:01:36,090 --> 00:01:41,790
the rate at which you can solve a

00:01:38,250 --> 00:01:43,799
complex systems equation they were

00:01:41,790 --> 00:01:45,450
combined in the 80s to give us a modern

00:01:43,799 --> 00:01:48,950
reinforcement learning and then much

00:01:45,450 --> 00:01:51,990
much more recently this field of deep

00:01:48,950 --> 00:01:52,590
reinforcement learning has emerged where

00:01:51,990 --> 00:01:55,470
we're using

00:01:52,590 --> 00:01:57,329
neural networks in conjunction with

00:01:55,470 --> 00:02:00,299
trial and error techniques to build

00:01:57,329 --> 00:02:02,630
machine learning models that can help us

00:02:00,299 --> 00:02:05,579
navigate complex real-world problems

00:02:02,630 --> 00:02:07,950
this differs from supervised or

00:02:05,579 --> 00:02:09,869
unsupervised learning because it's a

00:02:07,950 --> 00:02:11,610
trial error based technique which those

00:02:09,869 --> 00:02:13,020
are the two areas of machine learning of

00:02:11,610 --> 00:02:14,730
course are not you know we're not using

00:02:13,020 --> 00:02:14,940
trial error in supervised learning what

00:02:14,730 --> 00:02:17,430
you

00:02:14,940 --> 00:02:19,050
labeled datasets and we're not using

00:02:17,430 --> 00:02:21,390
trial and error in unsupervised learning

00:02:19,050 --> 00:02:23,880
we're using algorithms that allow us to

00:02:21,390 --> 00:02:25,500
establish commonality between data and

00:02:23,880 --> 00:02:26,940
then build models to predict based on

00:02:25,500 --> 00:02:29,040
that commonality things like k-means

00:02:26,940 --> 00:02:31,140
clustering or topic modeling would fit

00:02:29,040 --> 00:02:33,600
into that unsupervised category of

00:02:31,140 --> 00:02:36,330
course in contrast to that with deep

00:02:33,600 --> 00:02:39,480
reinforcement learning we are using an

00:02:36,330 --> 00:02:41,640
agent to perform actions in an

00:02:39,480 --> 00:02:44,100
environment and then we're instrumenting

00:02:41,640 --> 00:02:47,070
that environment to observe its state

00:02:44,100 --> 00:02:49,440
and offer rewards back to the agent in

00:02:47,070 --> 00:02:51,570
return for setting that state in a

00:02:49,440 --> 00:02:54,360
certain way so the classic example would

00:02:51,570 --> 00:02:56,520
be a self-driving vehicle where we want

00:02:54,360 --> 00:02:59,220
to keep our vehicle quite obviously on

00:02:56,520 --> 00:03:00,180
the road in the right lane and not hit

00:02:59,220 --> 00:03:02,160
any obstacles

00:03:00,180 --> 00:03:04,890
okay so staying on the road in the right

00:03:02,160 --> 00:03:06,630
lane would yield a reward and over time

00:03:04,890 --> 00:03:08,580
our agent would learn how it needed to

00:03:06,630 --> 00:03:10,110
respond to the environment in order to

00:03:08,580 --> 00:03:12,930
achieve that objective using either

00:03:10,110 --> 00:03:14,550
computer vision lidar or the sensors to

00:03:12,930 --> 00:03:16,620
steer our vehicle and keep it in that

00:03:14,550 --> 00:03:18,480
position where it would be yielded a

00:03:16,620 --> 00:03:20,520
reward by being in the right lane and of

00:03:18,480 --> 00:03:22,050
course not hitting any obstacles in the

00:03:20,520 --> 00:03:23,820
physical environment over time we can

00:03:22,050 --> 00:03:25,620
learn how to do that with a deep

00:03:23,820 --> 00:03:28,470
learning neural network taking inputs

00:03:25,620 --> 00:03:31,110
and operating the control system through

00:03:28,470 --> 00:03:33,510
the outputs within that network how is

00:03:31,110 --> 00:03:35,880
this applied today three different

00:03:33,510 --> 00:03:37,410
examples and I'd like you to grab these

00:03:35,880 --> 00:03:38,910
slides I'm sure they'll be available for

00:03:37,410 --> 00:03:40,590
download from O'Reilly after the event

00:03:38,910 --> 00:03:41,700
there's some citations on the bottom

00:03:40,590 --> 00:03:43,260
here and there's some really excellent

00:03:41,700 --> 00:03:45,150
further reading that you can take a look

00:03:43,260 --> 00:03:46,440
at to learn more about some of the

00:03:45,150 --> 00:03:47,880
emergent resource around these

00:03:46,440 --> 00:03:50,190
particular research around these

00:03:47,880 --> 00:03:54,330
particular use cases for recommender

00:03:50,190 --> 00:03:56,190
systems we are interested in positioning

00:03:54,330 --> 00:03:57,720
products or opportunities in front of

00:03:56,190 --> 00:04:00,000
customers in order to maximize a

00:03:57,720 --> 00:04:02,580
particular goal in the case of Amazon of

00:04:00,000 --> 00:04:04,260
course we are interested in putting

00:04:02,580 --> 00:04:06,570
products in front of customers so that

00:04:04,260 --> 00:04:07,769
they will make a purchase in the case of

00:04:06,570 --> 00:04:09,330
YouTube which is one of the other

00:04:07,769 --> 00:04:10,769
examples here and this is one of the

00:04:09,330 --> 00:04:13,320
best citations actually if you take a

00:04:10,769 --> 00:04:17,190
look at this YouTube video from Chen

00:04:13,320 --> 00:04:19,830
Google you will see a really excellent

00:04:17,190 --> 00:04:21,959
case study for deep reinforcement

00:04:19,830 --> 00:04:24,990
learning for recommending YouTube videos

00:04:21,959 --> 00:04:26,430
for customers users of YouTube and one

00:04:24,990 --> 00:04:28,230
of the most interesting things here is

00:04:26,430 --> 00:04:30,420
we're not using simulation

00:04:28,230 --> 00:04:33,840
techniques we're observing real users as

00:04:30,420 --> 00:04:36,030
they use that site over time and we're

00:04:33,840 --> 00:04:38,250
building a model which optimizes for a

00:04:36,030 --> 00:04:40,950
longer term goal in this case it's a

00:04:38,250 --> 00:04:42,750
longer-term goal of user whole time so

00:04:40,950 --> 00:04:45,180
getting users to watch more videos of

00:04:42,750 --> 00:04:47,280
course and then secondly getting them to

00:04:45,180 --> 00:04:48,960
interact with that video comment content

00:04:47,280 --> 00:04:50,970
through likes and comments and those are

00:04:48,960 --> 00:04:53,730
the rewards that we will take out of

00:04:50,970 --> 00:04:55,530
usage of the system by users so

00:04:53,730 --> 00:04:57,650
recommender systems is a really popular

00:04:55,530 --> 00:04:59,940
use case for this kind of technology

00:04:57,650 --> 00:05:01,830
energy control and we're not just

00:04:59,940 --> 00:05:03,270
talking here about simple thermostatic

00:05:01,830 --> 00:05:05,030
control of the temperature in my

00:05:03,270 --> 00:05:07,710
apartment of course we're talking about

00:05:05,030 --> 00:05:09,450
thermostatic control temperature control

00:05:07,710 --> 00:05:11,400
in much much more complex systems like

00:05:09,450 --> 00:05:13,950
data center environments for example

00:05:11,400 --> 00:05:16,470
where the amount of heat generated over

00:05:13,950 --> 00:05:19,020
time can be very variable there can be a

00:05:16,470 --> 00:05:20,760
very complex airflow environment within

00:05:19,020 --> 00:05:22,740
the data center facility which

00:05:20,760 --> 00:05:25,830
introduces further complexity into the

00:05:22,740 --> 00:05:28,020
system by applying DRL techniques here

00:05:25,830 --> 00:05:30,570
reinforcement learning techniques here

00:05:28,020 --> 00:05:32,610
we can optimize control of the

00:05:30,570 --> 00:05:34,380
temperature cooling and airflow pressure

00:05:32,610 --> 00:05:36,570
within the environment to deliver a more

00:05:34,380 --> 00:05:38,010
efficient cooling environment for data

00:05:36,570 --> 00:05:40,080
centers at scale it's quite a lot of

00:05:38,010 --> 00:05:42,210
examples of that as well and once again

00:05:40,080 --> 00:05:44,370
we're not using simulation here we're

00:05:42,210 --> 00:05:46,530
observing the real environment and we're

00:05:44,370 --> 00:05:48,240
learning from the characteristics the

00:05:46,530 --> 00:05:51,510
observations that are made in the real

00:05:48,240 --> 00:05:54,030
environment over time and then lastly

00:05:51,510 --> 00:05:55,440
robotics and automation this is a use

00:05:54,030 --> 00:05:57,150
case where we can actually use

00:05:55,440 --> 00:05:59,820
simulation technology we can build

00:05:57,150 --> 00:06:02,310
simulated robot environments it's really

00:05:59,820 --> 00:06:04,320
enabled by the the whole use case is

00:06:02,310 --> 00:06:06,510
enabled by low cost and ubiquitous

00:06:04,320 --> 00:06:09,900
availability of computer vision

00:06:06,510 --> 00:06:11,730
technology and low cost edge inference

00:06:09,900 --> 00:06:14,360
devices typically what we'll be doing

00:06:11,730 --> 00:06:16,860
here is integrating a low cost

00:06:14,360 --> 00:06:19,590
floating-point accelerator into our

00:06:16,860 --> 00:06:22,560
robotics hardware system using that to

00:06:19,590 --> 00:06:25,230
interpret RGB video that's coming off a

00:06:22,560 --> 00:06:27,330
camera in the robotic system and then

00:06:25,230 --> 00:06:30,480
using an optimized deep learning model

00:06:27,330 --> 00:06:32,700
to autonomously operate the robotic

00:06:30,480 --> 00:06:34,440
system for a particular goal might be

00:06:32,700 --> 00:06:36,420
self driving it might be materials

00:06:34,440 --> 00:06:38,520
handling it might be welding in a

00:06:36,420 --> 00:06:42,139
manufacturing plant all enabled using

00:06:38,520 --> 00:06:44,699
DRL and often trained in a simulation in

00:06:42,139 --> 00:06:48,930
so what are the some of the some of the

00:06:44,699 --> 00:06:52,589
pitfalls if your YouTube or another very

00:06:48,930 --> 00:06:55,080
large another site with a very large

00:06:52,589 --> 00:06:57,240
user population then you can gather the

00:06:55,080 --> 00:07:00,120
millions to billions of observations

00:06:57,240 --> 00:07:02,669
that are required to build an effective

00:07:00,120 --> 00:07:06,089
trial-and-error learning system using

00:07:02,669 --> 00:07:08,849
real data input but if you're smaller

00:07:06,089 --> 00:07:10,620
scale than that of the process of

00:07:08,849 --> 00:07:13,819
gathering feedback to train can actually

00:07:10,620 --> 00:07:16,680
be quite lengthy second challenge can be

00:07:13,819 --> 00:07:18,779
simulation to reality variance so if I'm

00:07:16,680 --> 00:07:22,729
training a robotics handling system

00:07:18,779 --> 00:07:24,960
inside a simulated robotics environment

00:07:22,729 --> 00:07:26,789
I've got to take into account things

00:07:24,960 --> 00:07:28,379
like lighting variations that might

00:07:26,789 --> 00:07:30,089
occur in the real world or the

00:07:28,379 --> 00:07:32,279
environmental variations that might

00:07:30,089 --> 00:07:34,529
occur in my manufacturing plant what if

00:07:32,279 --> 00:07:36,719
the Sun falls onto my production line a

00:07:34,529 --> 00:07:38,490
part of the day and then does not fall

00:07:36,719 --> 00:07:40,529
onto my production line another part of

00:07:38,490 --> 00:07:42,569
the day how do I build a computer vision

00:07:40,529 --> 00:07:44,430
module model effectively which can cater

00:07:42,569 --> 00:07:46,620
for different lighting conditions it

00:07:44,430 --> 00:07:48,029
sounds trivial but believe me it's

00:07:46,620 --> 00:07:50,039
actually not trivial assisted quite

00:07:48,029 --> 00:07:52,229
difficult to deal with complexity like

00:07:50,039 --> 00:07:55,199
that and then lastly on reward function

00:07:52,229 --> 00:07:57,509
engineering you know let's just take my

00:07:55,199 --> 00:07:59,999
self-driving car example here to prove

00:07:57,509 --> 00:08:02,219
the point so I get one point for staying

00:07:59,999 --> 00:08:05,159
right in the center of the lane and half

00:08:02,219 --> 00:08:07,830
a point if I steer off to the periphery

00:08:05,159 --> 00:08:10,110
of the lane so my reward function is

00:08:07,830 --> 00:08:12,659
going to generate most reward points if

00:08:10,110 --> 00:08:14,249
my car hits every part of the lane by

00:08:12,659 --> 00:08:16,349
zigzagging down it will get one point

00:08:14,249 --> 00:08:18,120
for being in the middle and half a point

00:08:16,349 --> 00:08:19,830
for being on the edge so if that's my

00:08:18,120 --> 00:08:21,449
reward function what is my vehicle going

00:08:19,830 --> 00:08:24,270
to do if I over train and allow it to

00:08:21,449 --> 00:08:25,830
overfit probably gonna zigzag down that

00:08:24,270 --> 00:08:28,050
Lane in order to maximize its point

00:08:25,830 --> 00:08:29,879
generation opportunity it's probably not

00:08:28,050 --> 00:08:31,620
what we want not what we want our system

00:08:29,879 --> 00:08:34,019
to do so to be very careful in

00:08:31,620 --> 00:08:36,810
engineering your reward functions to

00:08:34,019 --> 00:08:40,349
avoid some of those pitfalls so what did

00:08:36,810 --> 00:08:42,389
we what did we do AWS in order to try to

00:08:40,349 --> 00:08:44,190
get some more developers to on-ramp into

00:08:42,389 --> 00:08:47,430
using this technology we created

00:08:44,190 --> 00:08:50,130
something called AWS deep racer which is

00:08:47,430 --> 00:08:52,949
a fully integrated environment for

00:08:50,130 --> 00:08:54,689
building and training reinforcement

00:08:52,949 --> 00:08:55,410
learning models for self-driving

00:08:54,689 --> 00:08:56,730
vehicles

00:08:55,410 --> 00:08:59,569
now they're very small self-driving

00:08:56,730 --> 00:09:01,410
vehicles they're 1/18 scale autonomous

00:08:59,569 --> 00:09:03,209
radio-controlled cars that have a

00:09:01,410 --> 00:09:06,629
computer vision system integrated in

00:09:03,209 --> 00:09:09,300
them and an Intel CPU that enables the

00:09:06,629 --> 00:09:11,699
deep learning model to run on the car

00:09:09,300 --> 00:09:13,560
perform inference and steer the vehicle

00:09:11,699 --> 00:09:16,379
around the track when I say it's fully

00:09:13,560 --> 00:09:18,569
integrated we provide a development

00:09:16,379 --> 00:09:20,399
environment for authoring reward

00:09:18,569 --> 00:09:22,230
functions training those within a

00:09:20,399 --> 00:09:24,180
service we have called Amazon sage maker

00:09:22,230 --> 00:09:25,860
I which you might have heard more about

00:09:24,180 --> 00:09:28,319
over the course of the conference here

00:09:25,860 --> 00:09:31,319
this week and then using simulation with

00:09:28,319 --> 00:09:32,790
another AWS service called AWS Robo

00:09:31,319 --> 00:09:35,370
maker to enable us to do

00:09:32,790 --> 00:09:36,930
simulation-based training trial error

00:09:35,370 --> 00:09:38,699
trial and error simulation based

00:09:36,930 --> 00:09:40,379
training of that deep learning model in

00:09:38,699 --> 00:09:42,360
a virtual race environment it's

00:09:40,379 --> 00:09:44,129
available now and everybody here can

00:09:42,360 --> 00:09:46,529
participate in the final race of the

00:09:44,129 --> 00:09:48,899
league with some credits that you should

00:09:46,529 --> 00:09:51,509
have found on your seats here today and

00:09:48,899 --> 00:09:53,639
of course you can win Fame you can win

00:09:51,509 --> 00:09:57,029
glory and you can win a huge trophy if

00:09:53,639 --> 00:09:58,680
you participate in our competition AWS

00:09:57,029 --> 00:09:59,650
reinvent at the end of this year thank

00:09:58,680 --> 00:10:05,470
you

00:09:59,650 --> 00:10:05,470
[Applause]

00:10:10,370 --> 00:10:12,430

YouTube URL: https://www.youtube.com/watch?v=d9K2Fo8yqh0


