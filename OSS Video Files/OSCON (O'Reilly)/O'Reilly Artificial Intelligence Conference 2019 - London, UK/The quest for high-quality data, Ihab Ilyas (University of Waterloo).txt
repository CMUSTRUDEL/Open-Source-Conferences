Title: The quest for high-quality data, Ihab Ilyas (University of Waterloo)
Publication date: 2019-10-17
Playlist: O'Reilly Artificial Intelligence Conference 2019 - London, UK
Description: 
	“AI starts with good data” is a statement that receives wide agreement from data scientists, analysts, and business owners. There has been a significant increase in our ability to build complex AI models for prediction, classification, and various analytics tasks, and there’s an abundance of (fairly easy to use) tools that allow data scientists and analysts to provision complex models within days. However, the lack of data or data-quality issues remains the main bottleneck holding back further adoption of AI technologies. Even with advances in building robust models, the reality is that noisy data and incomplete data remain the biggest hurdles to effective end-to-end solutions. Multiple studies prove that cleaning data is a much more effective investment than enhancing learning robustness.
Ihab Ilyas highlights this data quality problem and describes the HoloClean framework, a state-of-the-art prediction engine for structured data with direct applications in detecting and repairing data errors, as well as imputing missing labels and values. The framework uses techniques such as data augmentation and self-supervised learning to build models that describe how data is generated and how errors and anomalies are introduced.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,510 --> 00:00:05,920
so today I'm gonna focus on one

00:00:03,940 --> 00:00:07,390
underlying concept why we see data

00:00:05,920 --> 00:00:09,520
claiming as a probabilistic inference

00:00:07,390 --> 00:00:12,160
problem or a probabilistic leaning

00:00:09,520 --> 00:00:14,830
problem so we'd like to think about the

00:00:12,160 --> 00:00:16,570
data as as such there is a generative

00:00:14,830 --> 00:00:19,869
process that generate clean data and

00:00:16,570 --> 00:00:21,759
there is a noisy Channel that kind of

00:00:19,869 --> 00:00:23,769
introduced errors to this data and then

00:00:21,759 --> 00:00:25,689
you observe the work of that noisy

00:00:23,769 --> 00:00:27,160
Channel so this is an example of a table

00:00:25,689 --> 00:00:29,349
in which I have a schema and a bunch of

00:00:27,160 --> 00:00:32,770
rules and then the generative process I

00:00:29,349 --> 00:00:34,449
will generate a clean instance I that

00:00:32,770 --> 00:00:37,899
clean an instance is going to go through

00:00:34,449 --> 00:00:39,789
a bunch of kind of pollution work

00:00:37,899 --> 00:00:41,859
devilish work and then you're gonna see

00:00:39,789 --> 00:00:44,829
J star that polluted version of this I

00:00:41,859 --> 00:00:47,109
if you think about this problem this way

00:00:44,829 --> 00:00:50,199
and you convince yourself that if I

00:00:47,109 --> 00:00:52,960
model this I and are this data

00:00:50,199 --> 00:00:55,210
generation process and noise generation

00:00:52,960 --> 00:00:56,920
process by parameterizing for example

00:00:55,210 --> 00:00:58,570
these two models and then trying to

00:00:56,920 --> 00:01:00,969
estimate these parameters you can do a

00:00:58,570 --> 00:01:03,250
lot of cool stuff for example you can

00:01:00,969 --> 00:01:05,170
repair by looking at the most probable

00:01:03,250 --> 00:01:07,270
instance that maximize the observation

00:01:05,170 --> 00:01:08,800
that you currently see or trying to

00:01:07,270 --> 00:01:11,020
figure out the probability that your

00:01:08,800 --> 00:01:12,900
pollution model touch a specific cell

00:01:11,020 --> 00:01:15,550
and that's error detection for you and

00:01:12,900 --> 00:01:19,540
everything in between so it's pretty

00:01:15,550 --> 00:01:21,430
cool stuff the way we built this in

00:01:19,540 --> 00:01:25,060
order to scale we built it on top of a

00:01:21,430 --> 00:01:26,560
bunch of really nice modern ml concepts

00:01:25,060 --> 00:01:29,140
I'm gonna highlight a bunch of those for

00:01:26,560 --> 00:01:33,160
you today the first one is what we call

00:01:29,140 --> 00:01:35,470
contextual representation of the of the

00:01:33,160 --> 00:01:37,120
of the target attribute and this is an

00:01:35,470 --> 00:01:39,400
attention mechanism that allow us to

00:01:37,120 --> 00:01:40,870
focus on the right or the most important

00:01:39,400 --> 00:01:42,520
representation of the target attribute

00:01:40,870 --> 00:01:45,360
given the rest of the row for example

00:01:42,520 --> 00:01:47,590
and other signals once we build that

00:01:45,360 --> 00:01:49,240
representation we start to augment it

00:01:47,590 --> 00:01:51,970
with a little bit more explicit features

00:01:49,240 --> 00:01:53,500
for example violations of integrity

00:01:51,970 --> 00:01:55,660
constraint domain knowledge and what

00:01:53,500 --> 00:01:58,330
have you and then we have this unified

00:01:55,660 --> 00:02:00,160
representation in which we can then push

00:01:58,330 --> 00:02:01,930
to the upper layers for inference and

00:02:00,160 --> 00:02:05,050
doing a variety of tasks for example

00:02:01,930 --> 00:02:08,530
regression or classification or even

00:02:05,050 --> 00:02:10,060
just error detection we also do a lot of

00:02:08,530 --> 00:02:13,900
self supervision sometimes weak

00:02:10,060 --> 00:02:15,310
supervision and we the task can be very

00:02:13,900 --> 00:02:17,300
different from

00:02:15,310 --> 00:02:19,280
continuous variables for example of the

00:02:17,300 --> 00:02:20,480
target attributes or categorical so we

00:02:19,280 --> 00:02:22,670
use regression for that and

00:02:20,480 --> 00:02:24,140
classification for that and but using

00:02:22,670 --> 00:02:28,450
the same contextual kind of

00:02:24,140 --> 00:02:28,450
representation of the of the data

00:02:34,170 --> 00:02:36,230

YouTube URL: https://www.youtube.com/watch?v=t03lEIrAoBg


