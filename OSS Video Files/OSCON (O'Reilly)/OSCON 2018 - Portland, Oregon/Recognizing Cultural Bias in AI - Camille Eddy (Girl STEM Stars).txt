Title: Recognizing Cultural Bias in AI - Camille Eddy (Girl STEM Stars)
Publication date: 2018-07-18
Playlist: OSCON 2018 - Portland, Oregon
Description: 
	Camille Eddy walks you through how the services we all use everyday have adapted machine learning to become more inclusive. Camille also explains what we can do to create culturally sensitive computer intelligence and why that is important for the future of AI.

Open source is the value that fuels new industries and pushes forward long-standing ones. Join us in person at the O'Reilly Open Source Convention to cover open source projects across all origins and affiliations. Learn more:  https://oreil.ly/2LQLJ8I

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,230 --> 00:00:07,120
so the first description or example I

00:00:04,600 --> 00:00:09,630
could talk about is sexism so we've

00:00:07,120 --> 00:00:11,380
heard of word to back which is a

00:00:09,630 --> 00:00:13,750
algorithm training set

00:00:11,380 --> 00:00:16,330
it was training set that was made by

00:00:13,750 --> 00:00:19,660
Google they trained it on about three

00:00:16,330 --> 00:00:21,310
million words from Google News and they

00:00:19,660 --> 00:00:23,740
found that there was vector

00:00:21,310 --> 00:00:25,720
relationships between word relationships

00:00:23,740 --> 00:00:28,270
that we have and they were also able to

00:00:25,720 --> 00:00:29,800
produce simple algebraic expressions to

00:00:28,270 --> 00:00:31,990
kind of understand these word

00:00:29,800 --> 00:00:34,329
relationships so I would if you would

00:00:31,990 --> 00:00:37,989
let help me I'll have you finished my

00:00:34,329 --> 00:00:42,129
sentence here father is the doctor as

00:00:37,989 --> 00:00:45,579
mother is to nurse man is to computer

00:00:42,129 --> 00:00:47,649
programmer as woman is to homemaker so

00:00:45,579 --> 00:00:50,409
this is a problem this is an implicitly

00:00:47,649 --> 00:00:51,940
sexist bias that is found in our word

00:00:50,409 --> 00:00:54,069
relationships and this answers a

00:00:51,940 --> 00:00:56,379
question I get often which is why can't

00:00:54,069 --> 00:00:58,929
we use the internet to help you know

00:00:56,379 --> 00:01:00,729
devious Rai and make it better and

00:00:58,929 --> 00:01:02,920
that's because we as we talk and

00:01:00,729 --> 00:01:05,590
interact with each other day to day we

00:01:02,920 --> 00:01:08,049
have bias were the ones propagating that

00:01:05,590 --> 00:01:09,130
bias that's how we talk to each other so

00:01:08,049 --> 00:01:11,110
it's really important to think about

00:01:09,130 --> 00:01:13,689
when you're creating datasets or using

00:01:11,110 --> 00:01:16,060
datasets it's not just I brought in this

00:01:13,689 --> 00:01:18,579
data set that's known to work or I have

00:01:16,060 --> 00:01:20,320
this data that you know I brought in we

00:01:18,579 --> 00:01:21,939
do need to look about it in terms of

00:01:20,320 --> 00:01:25,270
these questions and we'll get into that

00:01:21,939 --> 00:01:27,789
a little bit more the next example I

00:01:25,270 --> 00:01:30,310
want to talk about is racism joy blow

00:01:27,789 --> 00:01:32,070
meanie is an amazingly searcher she just

00:01:30,310 --> 00:01:35,140
finished that in my team Media Lab and

00:01:32,070 --> 00:01:37,719
she was dealing with social robots

00:01:35,140 --> 00:01:39,729
so social robots had 3d cameras and she

00:01:37,719 --> 00:01:41,259
was doing with facial recognition she

00:01:39,729 --> 00:01:43,270
found that she couldn't be recognized

00:01:41,259 --> 00:01:45,219
for her own project based on her skin

00:01:43,270 --> 00:01:47,079
tone and she had to wear a white mask in

00:01:45,219 --> 00:01:49,450
order to be detected by her social robot

00:01:47,079 --> 00:01:51,280
she also tried it with her fellow white

00:01:49,450 --> 00:01:53,530
colleagues had them sit in front of the

00:01:51,280 --> 00:01:55,479
camera I detected them she sat in front

00:01:53,530 --> 00:01:57,429
of the same camera and it in to detect

00:01:55,479 --> 00:01:59,670
her you can see this video on her

00:01:57,429 --> 00:02:02,499
website it's algorithmic justice leak

00:01:59,670 --> 00:02:05,740
really important a really cool way to

00:02:02,499 --> 00:02:07,869
demonstrate how where as we're expanding

00:02:05,740 --> 00:02:09,190
our user base and having more people use

00:02:07,869 --> 00:02:11,560
it it's not necessarily serving

00:02:09,190 --> 00:02:14,650
everybody equally another quick example

00:02:11,560 --> 00:02:17,200
is also infamous

00:02:14,650 --> 00:02:19,540
when Google photos accidentally tags to

00:02:17,200 --> 00:02:22,120
black people as guerrillas again social

00:02:19,540 --> 00:02:26,349
faux pas that AI wouldn't necessarily

00:02:22,120 --> 00:02:28,360
know not to do and then again a lack of

00:02:26,349 --> 00:02:30,489
data a lack of datasets to be able to

00:02:28,360 --> 00:02:35,019
recognize these two figures aren't

00:02:30,489 --> 00:02:36,430
definitely not gorillas the last example

00:02:35,019 --> 00:02:40,540
I want to talk about is about influence

00:02:36,430 --> 00:02:43,989
so we've heard of Microsoft and the tabe

00:02:40,540 --> 00:02:46,690
ought from Twitter so this was an

00:02:43,989 --> 00:02:48,640
experiment where Microsoft put out a bot

00:02:46,690 --> 00:02:51,040
and it was supposed to interact with the

00:02:48,640 --> 00:02:53,829
world and Twitter really novel at the

00:02:51,040 --> 00:02:56,109
time and they found that it was easily

00:02:53,829 --> 00:02:57,760
influenced by what people were tweeting

00:02:56,109 --> 00:03:00,400
at it the conversations people would

00:02:57,760 --> 00:03:03,069
have with Tay and they would maybe tree

00:03:00,400 --> 00:03:04,900
at it homophobic xenophobic other things

00:03:03,069 --> 00:03:07,150
I just set up one example but you can

00:03:04,900 --> 00:03:10,109
definitely read the story if you haven't

00:03:07,150 --> 00:03:13,510
already and and it experiences the

00:03:10,109 --> 00:03:15,640
influence that you can present to an AI

00:03:13,510 --> 00:03:18,159
and that also starts making question

00:03:15,640 --> 00:03:19,840
like what doesn't a I believe what does

00:03:18,159 --> 00:03:21,849
machine learning algorithms what kind of

00:03:19,840 --> 00:03:24,569
influence can they have or can be

00:03:21,849 --> 00:03:24,569

YouTube URL: https://www.youtube.com/watch?v=wAsimX3BU_Q


