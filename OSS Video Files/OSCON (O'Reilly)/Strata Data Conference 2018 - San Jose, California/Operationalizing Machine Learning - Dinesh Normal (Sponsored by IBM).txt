Title: Operationalizing Machine Learning - Dinesh Normal (Sponsored by IBM)
Publication date: 2018-03-08
Playlist: Strata Data Conference 2018 - San Jose, California
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,149 --> 00:00:05,400
when I look at machine learning I think

00:00:03,030 --> 00:00:09,330
there is three aspects to it I call it

00:00:05,400 --> 00:00:13,200
the 3ds the data the development of

00:00:09,330 --> 00:00:15,240
models and the deployment of models last

00:00:13,200 --> 00:00:18,449
year here at Strada I talked about

00:00:15,240 --> 00:00:21,750
development of models this year I want

00:00:18,449 --> 00:00:26,310
to focus on deployment of models how do

00:00:21,750 --> 00:00:28,470
you deploy a model into enterprise when

00:00:26,310 --> 00:00:31,800
we look at machine learning many of us

00:00:28,470 --> 00:00:35,850
think machine learning is about pick an

00:00:31,800 --> 00:00:38,640
algorithm fated bunch of data functions

00:00:35,850 --> 00:00:41,100
and labels and then have it learn map it

00:00:38,640 --> 00:00:43,219
to X to Y I know I'm simplifying it but

00:00:41,100 --> 00:00:46,590
that's what we think machine learning is

00:00:43,219 --> 00:00:49,260
but Enterprise machine learning is much

00:00:46,590 --> 00:00:54,539
more complex folks it's much more

00:00:49,260 --> 00:00:57,690
complicated they're unlike the Cagle

00:00:54,539 --> 00:01:00,680
competitions there are no plans data

00:00:57,690 --> 00:01:03,750
sets in an enterprise machine learning

00:01:00,680 --> 00:01:07,020
data is sitting in silos you got to go

00:01:03,750 --> 00:01:08,790
acquire the data cleanse the data there

00:01:07,020 --> 00:01:12,170
might not even be a use case where you

00:01:08,790 --> 00:01:16,259
can infuse ml now you had to go train

00:01:12,170 --> 00:01:18,360
test and tune deploy the model and this

00:01:16,259 --> 00:01:20,850
could go on iteration after iteration

00:01:18,360 --> 00:01:23,850
and now let's assume that you have built

00:01:20,850 --> 00:01:26,790
the model then comes the biggest

00:01:23,850 --> 00:01:30,780
challenge any data scientist would face

00:01:26,790 --> 00:01:33,090
which is deploying that model into the

00:01:30,780 --> 00:01:36,570
current IT infrastructure of an

00:01:33,090 --> 00:01:40,409
enterprise let me think about it every

00:01:36,570 --> 00:01:43,710
Enterprise has legacy applications

00:01:40,409 --> 00:01:46,399
that's 30 40 years old third-party

00:01:43,710 --> 00:01:48,450
software that you cannot touch

00:01:46,399 --> 00:01:50,729
dispersion of skills what do I mean by

00:01:48,450 --> 00:01:52,590
that if you're a data scientist you have

00:01:50,729 --> 00:01:54,479
to work with the CDO you got to work

00:01:52,590 --> 00:01:57,060
with the system admin you got to work

00:01:54,479 --> 00:02:04,229
with the data engineer to get that model

00:01:57,060 --> 00:02:07,250
in data is sitting in silos and now you

00:02:04,229 --> 00:02:09,539
have thousands of business processes and

00:02:07,250 --> 00:02:11,819
to go deploy and now let's say you

00:02:09,539 --> 00:02:13,450
deployed then comes the next step

00:02:11,819 --> 00:02:16,599
challenges

00:02:13,450 --> 00:02:19,090
data preparation bugs now all of a

00:02:16,599 --> 00:02:21,970
sudden at runtime the credit score comes

00:02:19,090 --> 00:02:25,750
in null is your model ready to deal with

00:02:21,970 --> 00:02:27,370
that model drifting score goes low what

00:02:25,750 --> 00:02:29,680
do you do do you retrain do you bring in

00:02:27,370 --> 00:02:32,319
new data echo-chamber

00:02:29,680 --> 00:02:36,459
is the biggest challenge our enterprises

00:02:32,319 --> 00:02:39,760
face what do I mean by that as the model

00:02:36,459 --> 00:02:41,620
gets better and better the training data

00:02:39,760 --> 00:02:44,230
set gets smaller and smaller if your

00:02:41,620 --> 00:02:46,350
model is catching all the fraud there is

00:02:44,230 --> 00:02:48,580
no more fraud data to train the model on

00:02:46,350 --> 00:02:52,420
that is the challenge that we see our

00:02:48,580 --> 00:02:55,630
enterprises face so having worked for

00:02:52,420 --> 00:02:59,069
decades in developing and deploying

00:02:55,630 --> 00:03:02,739
models I came up with the term fluid mo

00:02:59,069 --> 00:03:05,680
fluid ml for an enterprise is where it's

00:03:02,739 --> 00:03:08,500
always on it's always learning that

00:03:05,680 --> 00:03:12,819
model never sleeps and I think there are

00:03:08,500 --> 00:03:16,330
five pillars to fluid mo the first one

00:03:12,819 --> 00:03:18,370
is managed how do you make sure there's

00:03:16,330 --> 00:03:21,100
a versioning and a governance strategy

00:03:18,370 --> 00:03:23,739
in place for your models an enterprise

00:03:21,100 --> 00:03:25,540
could have thousands of models we have

00:03:23,739 --> 00:03:26,859
seen thousands of models at enterprises

00:03:25,540 --> 00:03:30,549
how do you make sure it's managed and

00:03:26,859 --> 00:03:33,160
versioned resilient by which I mean how

00:03:30,549 --> 00:03:37,690
do you make sure you have tested all the

00:03:33,160 --> 00:03:40,930
error conditions performant most

00:03:37,690 --> 00:03:44,890
enterprises need the model to score in

00:03:40,930 --> 00:03:47,260
milliseconds not minutes F fraud needs

00:03:44,890 --> 00:03:49,299
to be caught in milliseconds measurable

00:03:47,260 --> 00:03:51,670
how do you monitor it how do you make

00:03:49,299 --> 00:03:55,000
sure the model can alert when the scores

00:03:51,670 --> 00:03:57,519
goes down continuous how do you add a

00:03:55,000 --> 00:04:01,510
feedback loop to make sure the model is

00:03:57,519 --> 00:04:03,819
constantly learning for a fluid ml for

00:04:01,510 --> 00:04:06,220
enterprise MO these five pillars are key

00:04:03,819 --> 00:04:11,049
the first three are about always-on and

00:04:06,220 --> 00:04:12,970
the last two is all about learning so if

00:04:11,049 --> 00:04:16,269
I could leave you with one thought that

00:04:12,970 --> 00:04:19,419
is enterprise machine learning is not

00:04:16,269 --> 00:04:23,110
just about model development it's about

00:04:19,419 --> 00:04:24,610
model deployment I was in Europe a few

00:04:23,110 --> 00:04:27,210
months ago and I met with the city of a

00:04:24,610 --> 00:04:30,430
major bank and what he said to me rings

00:04:27,210 --> 00:04:33,430
he said Dinesh it only took me three

00:04:30,430 --> 00:04:36,700
weeks to develop the model it has been

00:04:33,430 --> 00:04:41,200
11 months we still haven't deployed it

00:04:36,700 --> 00:04:43,090
and that is the truth about enterprise

00:04:41,200 --> 00:04:44,680
machine learning if you want to learn

00:04:43,090 --> 00:04:46,450
more about enterprise machine learning

00:04:44,680 --> 00:04:49,420
and fluid mo please visit our booth at

00:04:46,450 --> 00:04:51,340
10:19 and also please attend the talk

00:04:49,420 --> 00:04:54,100
set is going to give set this our CDO

00:04:51,340 --> 00:04:56,550
for IBM analogies great speaker thank

00:04:54,100 --> 00:04:56,550
you so much

00:05:03,330 --> 00:05:05,389

YouTube URL: https://www.youtube.com/watch?v=CdE0bVHZxHI


