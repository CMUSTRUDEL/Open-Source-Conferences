Title: Defining Responsible Data Practices: A Community-Driven Approach - Natalie Evans Harris (BrightHive)
Publication date: 2018-03-08
Playlist: Strata Data Conference 2018 - San Jose, California
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,390 --> 00:00:07,390
we must define a common good we must

00:00:05,319 --> 00:00:09,760
define that common good because there's

00:00:07,390 --> 00:00:11,740
no executive order there is no law that

00:00:09,760 --> 00:00:13,750
can require the American people to form

00:00:11,740 --> 00:00:16,869
a national community if you want to see

00:00:13,750 --> 00:00:20,230
it happen you must do it so that's what

00:00:16,869 --> 00:00:22,390
I did I got together with Bloomberg I

00:00:20,230 --> 00:00:24,630
got together with data for democracy and

00:00:22,390 --> 00:00:27,550
some of my favorite friends and I said

00:00:24,630 --> 00:00:29,259
what keeps you up at night as you're

00:00:27,550 --> 00:00:31,200
building tools as you're building

00:00:29,259 --> 00:00:34,240
algorithms and as you're developing

00:00:31,200 --> 00:00:36,700
solutions what keeps you up at night we

00:00:34,240 --> 00:00:40,330
asked about 2,000 data scientist and

00:00:36,700 --> 00:00:42,570
broke it down to these three things we

00:00:40,330 --> 00:00:45,490
want to know that we're actually

00:00:42,570 --> 00:00:47,290
facilitating data sharing and we want to

00:00:45,490 --> 00:00:49,060
know that the data is being used and

00:00:47,290 --> 00:00:51,460
used appropriately we want to ensure

00:00:49,060 --> 00:00:53,610
compliance and we want to understand and

00:00:51,460 --> 00:00:56,290
ascertain the validity of results I

00:00:53,610 --> 00:00:58,210
don't think it's a shock to anyone here

00:00:56,290 --> 00:00:59,800
that that is the things that are

00:00:58,210 --> 00:01:03,070
greatest concern when it comes to

00:00:59,800 --> 00:01:04,690
responsible data use so we create so we

00:01:03,070 --> 00:01:06,640
launched an initiative called the

00:01:04,690 --> 00:01:08,050
community driven principles for ethical

00:01:06,640 --> 00:01:12,250
data sharing and we are not and we

00:01:08,050 --> 00:01:14,080
launched this in September of 2017 last

00:01:12,250 --> 00:01:16,360
year at the new at the data for good

00:01:14,080 --> 00:01:18,790
exchange in New York we're not we

00:01:16,360 --> 00:01:21,460
announced it we held a breakout we came

00:01:18,790 --> 00:01:24,310
together and we said these are the seven

00:01:21,460 --> 00:01:26,290
key areas that need to be addressed in

00:01:24,310 --> 00:01:28,240
order to define the values and

00:01:26,290 --> 00:01:30,850
priorities of ethical behavior for data

00:01:28,240 --> 00:01:32,530
scientist these areas should not be a

00:01:30,850 --> 00:01:34,290
surprise either data ownership and

00:01:32,530 --> 00:01:36,610
provenance private and security

00:01:34,290 --> 00:01:38,860
transparency and openness thought

00:01:36,610 --> 00:01:40,450
diversity and responsible communications

00:01:38,860 --> 00:01:42,760
so that we can actually get to this

00:01:40,450 --> 00:01:45,159
period of productivity versus these

00:01:42,760 --> 00:01:48,940
inflated expectations of what data can

00:01:45,159 --> 00:01:51,280
actually do and we took a community

00:01:48,940 --> 00:01:54,100
first approach about a hundred a little

00:01:51,280 --> 00:01:57,060
bit over a hundred plus data scientist

00:01:54,100 --> 00:02:00,430
across the private sector nonprofits

00:01:57,060 --> 00:02:02,979
academia public sector came together on

00:02:00,430 --> 00:02:05,799
their own time did literature reviews

00:02:02,979 --> 00:02:08,410
and defined a set of principles for each

00:02:05,799 --> 00:02:10,630
of those seven categories and then we

00:02:08,410 --> 00:02:12,760
came together again in February a couple

00:02:10,630 --> 00:02:13,870
weeks ago in San Francisco and held an

00:02:12,760 --> 00:02:15,670
event again

00:02:13,870 --> 00:02:18,370
we did it again and started to end to

00:02:15,670 --> 00:02:20,110
find another set of principles we did

00:02:18,370 --> 00:02:22,120
this by data scientists for data

00:02:20,110 --> 00:02:23,440
scientist and the intention is not that

00:02:22,120 --> 00:02:25,900
this code of ethics becomes something

00:02:23,440 --> 00:02:27,519
that everybody has to be beholden to and

00:02:25,900 --> 00:02:30,190
you should be punished if it never if

00:02:27,519 --> 00:02:31,989
you do not comply it's actually just a

00:02:30,190 --> 00:02:34,510
means of communicating what's important

00:02:31,989 --> 00:02:36,099
and what's a value when using data and

00:02:34,510 --> 00:02:38,620
so that's what we did and it's a living

00:02:36,099 --> 00:02:43,330
document and so what happens next

00:02:38,620 --> 00:02:46,239
oh we voted also so what happens next we

00:02:43,330 --> 00:02:48,370
want signatures we need to know that the

00:02:46,239 --> 00:02:50,620
principles that are laid out data

00:02:48,370 --> 00:02:51,910
practices org we want to know that the

00:02:50,620 --> 00:02:54,459
principles that have been defined by

00:02:51,910 --> 00:02:56,860
this community are actually of meaning

00:02:54,459 --> 00:02:59,410
and value to this community and you do

00:02:56,860 --> 00:03:01,269
that by signing up if you sign and say

00:02:59,410 --> 00:03:03,069
that it's important to you then that

00:03:01,269 --> 00:03:05,140
becomes the collective voice of the

00:03:03,069 --> 00:03:07,780
community and can be used to do things

00:03:05,140 --> 00:03:10,090
like defined curriculum and training for

00:03:07,780 --> 00:03:11,950
a future data scientist can be used to

00:03:10,090 --> 00:03:13,840
find tools to be able to answer

00:03:11,950 --> 00:03:16,750
questions like how do I know that the

00:03:13,840 --> 00:03:19,269
that bias has been mitigated in my

00:03:16,750 --> 00:03:23,019
algorithm how do I know that my dataset

00:03:19,269 --> 00:03:24,670
is not restricted and then we use it to

00:03:23,019 --> 00:03:26,410
build partnerships with people that are

00:03:24,670 --> 00:03:28,510
interested in advancing and building the

00:03:26,410 --> 00:03:31,780
capacity for ethical and responsible

00:03:28,510 --> 00:03:34,510
data use across the community it starts

00:03:31,780 --> 00:03:36,069
a conversation but it starts first with

00:03:34,510 --> 00:03:38,980
data scientists the people that are

00:03:36,069 --> 00:03:41,440
responsible for doing this being able to

00:03:38,980 --> 00:03:43,690
communicate what your values and

00:03:41,440 --> 00:03:47,910
principles are in this space and then

00:03:43,690 --> 00:03:47,910
continuing that conversation to do so

00:03:54,260 --> 00:03:56,319

YouTube URL: https://www.youtube.com/watch?v=GtbavkydxDQ


