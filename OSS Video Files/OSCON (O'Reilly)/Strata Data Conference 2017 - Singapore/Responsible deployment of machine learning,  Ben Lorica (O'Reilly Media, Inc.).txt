Title: Responsible deployment of machine learning,  Ben Lorica (O'Reilly Media, Inc.)
Publication date: 2017-12-07
Playlist: Strata Data Conference 2017 - Singapore
Description: 
	Machine learning models are becoming increasingly widely used and deployed. Ben Lorica explains how to guard against flaws and failures in your machine learning deployments.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:03,740 --> 00:00:08,809
but what I want to leave you with this

00:00:06,710 --> 00:00:11,240
idea that there's more to machine

00:00:08,809 --> 00:00:13,219
learning these days than just optimizing

00:00:11,240 --> 00:00:15,259
your business metric so at least moving

00:00:13,219 --> 00:00:17,180
forward you need to take other

00:00:15,259 --> 00:00:20,330
considerations right so fairness

00:00:17,180 --> 00:00:24,830
transparency and explain ability so in

00:00:20,330 --> 00:00:28,039
particular bias so let's start treating

00:00:24,830 --> 00:00:30,380
biases as important bugs at the same

00:00:28,039 --> 00:00:35,240
level as we would security and privacy

00:00:30,380 --> 00:00:37,910
bugs of course the the challenge is that

00:00:35,240 --> 00:00:43,370
we hear a lot about bias we hear a lot

00:00:37,910 --> 00:00:45,200
about fairness but usually what we hear

00:00:43,370 --> 00:00:48,620
are anecdotes and stories right so

00:00:45,200 --> 00:00:51,730
there's not really a framework for how

00:00:48,620 --> 00:00:54,080
to address in these problems so

00:00:51,730 --> 00:00:55,730
fortunately the machine learning

00:00:54,080 --> 00:00:57,440
research community has actually been

00:00:55,730 --> 00:01:00,230
engaged with this problem over the last

00:00:57,440 --> 00:01:02,030
few years so let me go over a few ideas

00:01:00,230 --> 00:01:02,860
that you may want to take into

00:01:02,030 --> 00:01:07,250
consideration

00:01:02,860 --> 00:01:11,509
so first disparate impact so this refers

00:01:07,250 --> 00:01:13,609
to the case where you have a model that

00:01:11,509 --> 00:01:15,770
appears to be neutral but actually

00:01:13,609 --> 00:01:19,520
behaves very differently in different

00:01:15,770 --> 00:01:21,320
slices of your population so for example

00:01:19,520 --> 00:01:23,869
you may have a predictor or a feature

00:01:21,320 --> 00:01:29,840
let's say it's the distance from a

00:01:23,869 --> 00:01:31,549
specific location and let's say you have

00:01:29,840 --> 00:01:34,210
two income groups right so high-income

00:01:31,549 --> 00:01:36,530
and low-income the the distance

00:01:34,210 --> 00:01:38,869
distribution for the high-income group

00:01:36,530 --> 00:01:42,469
is very different from the distance

00:01:38,869 --> 00:01:44,569
distribution for the low-income group so

00:01:42,469 --> 00:01:46,249
if you plugged it and plug that into

00:01:44,569 --> 00:01:49,069
your model of course your model will

00:01:46,249 --> 00:01:53,450
behave very differently in these two

00:01:49,069 --> 00:01:55,939
different this income groups so what

00:01:53,450 --> 00:01:59,389
some researchers did a few years ago is

00:01:55,939 --> 00:02:02,179
is provide people a frame a systematic

00:01:59,389 --> 00:02:04,869
framework to identify things like

00:02:02,179 --> 00:02:07,369
disparate impact and not just identify

00:02:04,869 --> 00:02:11,030
give you a recipe for how to normalize

00:02:07,369 --> 00:02:15,200
your distributions and remove disparate

00:02:11,030 --> 00:02:17,940
impact error right so usefully you

00:02:15,200 --> 00:02:20,460
optimize some machine learning

00:02:17,940 --> 00:02:22,620
error some business metric and you say

00:02:20,460 --> 00:02:25,500
okay now that I have it as long as

00:02:22,620 --> 00:02:27,990
possible undone but it turns out

00:02:25,500 --> 00:02:32,160
actually you can have a situation like

00:02:27,990 --> 00:02:35,520
this right so imagine you have let's say

00:02:32,160 --> 00:02:38,160
a predictive model for medicine and you

00:02:35,520 --> 00:02:40,680
have most of your draining data which is

00:02:38,160 --> 00:02:43,380
in red are for young people and then you

00:02:40,680 --> 00:02:47,010
have for fewer training or labeled

00:02:43,380 --> 00:02:51,690
datasets for your senior citizens so

00:02:47,010 --> 00:02:56,780
obviously when you plug plug plug this

00:02:51,690 --> 00:02:58,860
into your model error for your for your

00:02:56,780 --> 00:03:00,510
younger people will be much lower than

00:02:58,860 --> 00:03:02,370
for your senior citizens right because

00:03:00,510 --> 00:03:05,730
usually there's a correlation between

00:03:02,370 --> 00:03:10,800
the size of your training data set and

00:03:05,730 --> 00:03:12,800
the accuracy of your model so again the

00:03:10,800 --> 00:03:16,320
good news is that the research community

00:03:12,800 --> 00:03:18,510
has long known about this and so they're

00:03:16,320 --> 00:03:21,660
starting to come out with strategies for

00:03:18,510 --> 00:03:24,540
how you can identify and remove this so

00:03:21,660 --> 00:03:27,180
in particular a group at Google gives

00:03:24,540 --> 00:03:30,480
you a step-by-step strategy for how to

00:03:27,180 --> 00:03:32,940
make sure that when this thing arises

00:03:30,480 --> 00:03:35,459
you can even have a framework so that

00:03:32,940 --> 00:03:37,290
the true positive rate for the different

00:03:35,459 --> 00:03:39,560
groups in your population are roughly

00:03:37,290 --> 00:03:39,560
the same

00:03:44,970 --> 00:03:47,030

YouTube URL: https://www.youtube.com/watch?v=QCYHsh0thFw


