Title: Adam Gibson (Skymind) interview at Strata Data Conference
Publication date: 2017-12-18
Playlist: Strata Data Conference 2017 - Singapore
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,530 --> 00:00:05,850
hi I'm Paco Nathan from O'Reilly Media

00:00:03,120 --> 00:00:08,220
and it's pleasure to have Adam Gibson

00:00:05,850 --> 00:00:11,780
from Sky mind here can I get me here

00:00:08,220 --> 00:00:15,089
Paco well thanks so I'm curious about

00:00:11,780 --> 00:00:18,570
what you're seeing in terms of Industry

00:00:15,089 --> 00:00:20,279
some of the pitfalls maybe of deploying

00:00:18,570 --> 00:00:22,680
machine learning in production right

00:00:20,279 --> 00:00:25,650
well so a lot of it comes down to data

00:00:22,680 --> 00:00:27,330
scientists despite being very smart

00:00:25,650 --> 00:00:29,250
people are at the end of the day are

00:00:27,330 --> 00:00:31,170
very focused on just experimenting

00:00:29,250 --> 00:00:32,669
they're you know there's there's a lot

00:00:31,170 --> 00:00:34,920
of I think there's a lot of enterprises

00:00:32,669 --> 00:00:37,350
still trying to hire for actual line of

00:00:34,920 --> 00:00:38,850
business ok and you know they're they're

00:00:37,350 --> 00:00:40,980
advocating for that more in the

00:00:38,850 --> 00:00:42,510
education but ultimately a lot of data

00:00:40,980 --> 00:00:45,030
scientists are still focused a lot a lot

00:00:42,510 --> 00:00:47,039
of hype so they're jumping on the latest

00:00:45,030 --> 00:00:48,870
thing without understanding can logistic

00:00:47,039 --> 00:00:50,879
regression work can linear regression

00:00:48,870 --> 00:00:52,559
work what's the simplest model I could

00:00:50,879 --> 00:00:53,789
deploy and they're not they're also not

00:00:52,559 --> 00:00:55,469
thinking about maintaining models

00:00:53,789 --> 00:00:57,750
they're just they're just thinking of a

00:00:55,469 --> 00:00:59,609
continuous experimentation and but not

00:00:57,750 --> 00:01:00,570
like the lifecycle yeah exactly they're

00:00:59,609 --> 00:01:02,340
not thinking about the whole model

00:01:00,570 --> 00:01:04,470
lifecycle like what happens after I hand

00:01:02,340 --> 00:01:05,670
this off to production what happens in

00:01:04,470 --> 00:01:07,229
production they're not really asking

00:01:05,670 --> 00:01:09,450
those questions and sometimes it's just

00:01:07,229 --> 00:01:10,680
you know limits of their job sometimes

00:01:09,450 --> 00:01:12,000
they have a big enough teams where they

00:01:10,680 --> 00:01:13,560
don't have to worry about that sure but

00:01:12,000 --> 00:01:14,670
ultimately that they kind of the data

00:01:13,560 --> 00:01:16,350
scientist has a lot more responsibility

00:01:14,670 --> 00:01:18,360
now because they can affect the data

00:01:16,350 --> 00:01:19,920
collection process they might there

00:01:18,360 --> 00:01:21,150
might be a lot of complaints around you

00:01:19,920 --> 00:01:22,470
know features that they might be missing

00:01:21,150 --> 00:01:26,280
but maybe they don't have enough time to

00:01:22,470 --> 00:01:28,710
ask about it and there's not a man like

00:01:26,280 --> 00:01:31,200
why is being a traceroute bias yes by a

00:01:28,710 --> 00:01:33,750
bias it could just be miss you know miss

00:01:31,200 --> 00:01:35,640
specified fields you know could just be

00:01:33,750 --> 00:01:37,170
they they might just amount me signaling

00:01:35,640 --> 00:01:38,520
to management that you know what why

00:01:37,170 --> 00:01:41,159
does this data take three months for me

00:01:38,520 --> 00:01:42,930
to acquire you know so that that's a big

00:01:41,159 --> 00:01:44,880
that's a big bottleneck and so a lot of

00:01:42,930 --> 00:01:47,520
you know and then they also might be

00:01:44,880 --> 00:01:48,840
missing compliance requirements like you

00:01:47,520 --> 00:01:50,729
know are you know are we hitting or you

00:01:48,840 --> 00:01:53,220
know or are we complying with all laws

00:01:50,729 --> 00:01:55,500
around privacy is the right auditing in

00:01:53,220 --> 00:01:57,630
place how do i you know how do I know

00:01:55,500 --> 00:01:59,219
Walt is accessing this data a lot of it

00:01:57,630 --> 00:02:01,079
you know a lot of the times IT will have

00:01:59,219 --> 00:02:02,759
those questions and the data scientists

00:02:01,079 --> 00:02:05,250
will just instead try to work against or

00:02:02,759 --> 00:02:06,719
around those people and it so it's

00:02:05,250 --> 00:02:08,280
really hard for organizations to

00:02:06,719 --> 00:02:10,860
collaborate so it's largely a people

00:02:08,280 --> 00:02:12,750
problem so whoa

00:02:10,860 --> 00:02:14,340
what kind of tools and techniques can

00:02:12,750 --> 00:02:16,740
you bring to bear on this well so a lot

00:02:14,340 --> 00:02:19,050
of so it's kind of a cheesy buzzword but

00:02:16,740 --> 00:02:22,020
data governance you know so really

00:02:19,050 --> 00:02:23,220
having automated auditing so the data

00:02:22,020 --> 00:02:25,920
scientists don't have to think about it

00:02:23,220 --> 00:02:28,050
so tools like not knife I Apache knife I

00:02:25,920 --> 00:02:29,220
and stream sets are doing this where

00:02:28,050 --> 00:02:30,720
you're able to where you're monitoring

00:02:29,220 --> 00:02:31,920
and end data pipelines I think this is

00:02:30,720 --> 00:02:33,630
going to become more of a growing trend

00:02:31,920 --> 00:02:34,980
over time okay so things are just the

00:02:33,630 --> 00:02:36,720
tool the tooling just needs to get

00:02:34,980 --> 00:02:37,980
better around this and so we're at the

00:02:36,720 --> 00:02:39,300
early stages of this but I think

00:02:37,980 --> 00:02:40,860
ultimately if we're gonna build an

00:02:39,300 --> 00:02:42,870
ultimate kind of experiment Haven for

00:02:40,860 --> 00:02:45,150
data scientists to where they're they

00:02:42,870 --> 00:02:46,110
can comply with all requirements more

00:02:45,150 --> 00:02:47,730
you know better collaborate with

00:02:46,110 --> 00:02:49,530
developers like to play models for

00:02:47,730 --> 00:02:51,600
production for example I think we're

00:02:49,530 --> 00:02:52,920
gonna need a kind of a more I think

00:02:51,600 --> 00:02:55,110
we're gonna see a big shift towards a

00:02:52,920 --> 00:02:56,910
portal approach at AI so you're gonna

00:02:55,110 --> 00:02:58,650
see like you know you know Facebook has

00:02:56,910 --> 00:03:00,330
FB learn or Google has like some of

00:02:58,650 --> 00:03:01,560
their internal infrastructure and so you

00:03:00,330 --> 00:03:04,709
there's there's this big there's this

00:03:01,560 --> 00:03:07,080
big focus on hubs being created so for

00:03:04,709 --> 00:03:08,959
example Amazon sage maker yeah you'll

00:03:07,080 --> 00:03:11,070
see tools like that evolve into a hub

00:03:08,959 --> 00:03:14,340
and that's actually what we build a sky

00:03:11,070 --> 00:03:16,380
mine as well interesting you know out of

00:03:14,340 --> 00:03:19,950
enterprise say like you're at a bank or

00:03:16,380 --> 00:03:21,600
telco you know who's really driving a

00:03:19,950 --> 00:03:24,480
lot of that kind of data governments is

00:03:21,600 --> 00:03:27,450
it is it coming solely from IT or is it

00:03:24,480 --> 00:03:28,799
more coming from a higher level well so

00:03:27,450 --> 00:03:30,930
this is well there's all this buzz

00:03:28,799 --> 00:03:32,489
around hiring a chief data officer or

00:03:30,930 --> 00:03:34,290
chief you know chief information officer

00:03:32,489 --> 00:03:36,090
they are the ones really driving a lot

00:03:34,290 --> 00:03:37,320
of those trends I think because they're

00:03:36,090 --> 00:03:38,700
starting to see they're you know they're

00:03:37,320 --> 00:03:40,799
starting you know for example Europe is

00:03:38,700 --> 00:03:42,739
becoming more tightly regulated you have

00:03:40,799 --> 00:03:44,910
Facebook being kind of hit for privacy

00:03:42,739 --> 00:03:46,470
violations things like nature we're

00:03:44,910 --> 00:03:48,570
starting to see that come up more and

00:03:46,470 --> 00:03:50,160
more and I think I think some some

00:03:48,570 --> 00:03:52,140
people are being more proactive about

00:03:50,160 --> 00:03:54,420
making sure that they're that they're

00:03:52,140 --> 00:03:56,130
kind of complying with all laws and and

00:03:54,420 --> 00:03:57,299
and having a lot of infrastructure in

00:03:56,130 --> 00:03:59,730
place so they don't get hit with these

00:03:57,299 --> 00:04:01,980
penalties gotcha yeah it's risk is

00:03:59,730 --> 00:04:04,650
bubbling up to the top exactly yeah

00:04:01,980 --> 00:04:06,299
so are there ways of using I think one

00:04:04,650 --> 00:04:08,430
of the quotes from the keynotes today

00:04:06,299 --> 00:04:09,840
was about using machine learning to

00:04:08,430 --> 00:04:11,519
monitor machine learning I'm right

00:04:09,840 --> 00:04:13,079
there's techniques we can do in machine

00:04:11,519 --> 00:04:14,370
learning to help this right and so

00:04:13,079 --> 00:04:17,250
active learning is one of those areas

00:04:14,370 --> 00:04:18,630
okay so the devil so well so active

00:04:17,250 --> 00:04:20,430
learning is where you ultimately have

00:04:18,630 --> 00:04:21,870
humans in the loop trying to kind of

00:04:20,430 --> 00:04:23,130
auditing what MIT the machine learning

00:04:21,870 --> 00:04:23,800
models are doing because ultimately

00:04:23,130 --> 00:04:25,479
machine

00:04:23,800 --> 00:04:27,520
models come down you know you have the

00:04:25,479 --> 00:04:28,509
softmax distribution in the end right

00:04:27,520 --> 00:04:30,370
you have a zero one you have a

00:04:28,509 --> 00:04:32,319
probability of something happening right

00:04:30,370 --> 00:04:34,479
and so that means there's room for error

00:04:32,319 --> 00:04:36,159
but ultimately you know you have rules

00:04:34,479 --> 00:04:38,800
engines being deployed which are that's

00:04:36,159 --> 00:04:39,039
kind of the the 1950s era machine you

00:04:38,800 --> 00:04:40,569
know

00:04:39,039 --> 00:04:41,770
AI but you know we're still seeing that

00:04:40,569 --> 00:04:44,409
applied today because at least it's

00:04:41,770 --> 00:04:46,659
deterministic and deterministic and

00:04:44,409 --> 00:04:48,400
being you know reproducibility and even

00:04:46,659 --> 00:04:50,770
interpretability is a big that's that's

00:04:48,400 --> 00:04:52,180
a big area of focus now so active

00:04:50,770 --> 00:04:53,560
learning is a kind of a bridge to that

00:04:52,180 --> 00:04:56,050
where you know we can use kind of

00:04:53,560 --> 00:04:57,099
riskier models but you know but we can

00:04:56,050 --> 00:05:02,289
actually monitor it and keep an eye on

00:04:57,099 --> 00:05:04,060
it so the the higher parts of risk

00:05:02,289 --> 00:05:05,919
coming out of a model thing can be

00:05:04,060 --> 00:05:07,900
deferred to the human yes is that yeah

00:05:05,919 --> 00:05:09,669
so we have a lot of things where you

00:05:07,900 --> 00:05:11,289
know the model decisions with a low

00:05:09,669 --> 00:05:13,690
conference interval being kind of kicked

00:05:11,289 --> 00:05:15,550
to humans Oh excellent you know I've

00:05:13,690 --> 00:05:17,229
never heard that described as far as a

00:05:15,550 --> 00:05:19,479
kind of risk mitigation but yes all

00:05:17,229 --> 00:05:21,520
kinds of sense um I mean the general

00:05:19,479 --> 00:05:24,460
category were talking about uncertainty

00:05:21,520 --> 00:05:26,530
systems at work exactly but I think that

00:05:24,460 --> 00:05:27,580
sometimes the the real performance

00:05:26,530 --> 00:05:29,740
metric you're worried about is risk

00:05:27,580 --> 00:05:31,270
right so maybe we can consider right

00:05:29,740 --> 00:05:33,400
yeah a higher value than just

00:05:31,270 --> 00:05:34,810
uncertainty engine exactly well if you

00:05:33,400 --> 00:05:36,400
think about it ROC curves also playing

00:05:34,810 --> 00:05:38,319
those ROC curves and decision boundaries

00:05:36,400 --> 00:05:40,270
right we're you know we're we're tuning

00:05:38,319 --> 00:05:42,610
precision and recall for essentially

00:05:40,270 --> 00:05:43,960
what is risk right and so if you if you

00:05:42,610 --> 00:05:45,849
combine that with humans in the loop

00:05:43,960 --> 00:05:47,800
all of a sudden you have you have a

00:05:45,849 --> 00:05:49,990
pretty tightly controlled system that

00:05:47,800 --> 00:05:51,909
you can tune over time and so and I

00:05:49,990 --> 00:05:53,590
think tooling will evolve around that

00:05:51,909 --> 00:05:55,479
that will allow us to say you know

00:05:53,590 --> 00:05:57,340
monitor the monitor our decision

00:05:55,479 --> 00:05:58,870
boundary over time and watch for you

00:05:57,340 --> 00:06:00,340
know it maybe attach a risk score to

00:05:58,870 --> 00:06:01,539
that yeah and with that kind of

00:06:00,340 --> 00:06:03,250
monitoring and then a human auditor

00:06:01,539 --> 00:06:04,690
built into it yeah you know you can kind

00:06:03,250 --> 00:06:06,819
of you can you can automate things you

00:06:04,690 --> 00:06:08,169
normally couldn't before well also being

00:06:06,819 --> 00:06:10,180
able to say to your you know to your

00:06:08,169 --> 00:06:12,789
your stakeholders that you're deploying

00:06:10,180 --> 00:06:15,159
as safely and here's how that I that

00:06:12,789 --> 00:06:16,930
really underscores that there is a human

00:06:15,159 --> 00:06:18,490
in this interface exactly all the

00:06:16,930 --> 00:06:20,199
questions so when it comes to auditing

00:06:18,490 --> 00:06:22,330
my account interpretability that's

00:06:20,199 --> 00:06:23,620
already built in exactly well and so

00:06:22,330 --> 00:06:24,699
yeah because it's willing if you want to

00:06:23,620 --> 00:06:26,020
apply like a newer tool like deep

00:06:24,699 --> 00:06:27,099
learning for example you know you're

00:06:26,020 --> 00:06:28,900
you're you don't you don't have that

00:06:27,099 --> 00:06:30,279
interpretability I mean there's a lot of

00:06:28,900 --> 00:06:32,020
there you know one of my colleagues Dave

00:06:30,279 --> 00:06:33,130
Cal actually does a machine learning he

00:06:32,020 --> 00:06:35,440
runs a machine learning for healthcare

00:06:33,130 --> 00:06:36,460
conference in LA and so this was this is

00:06:35,440 --> 00:06:36,940
a conference that was hurled earlier

00:06:36,460 --> 00:06:38,800
this year and

00:06:36,940 --> 00:06:40,300
one big thing that came out of that was

00:06:38,800 --> 00:06:41,800
there's a lot of research over only in

00:06:40,300 --> 00:06:43,180
healthcare and deep learning around

00:06:41,800 --> 00:06:45,580
interpretability of the models

00:06:43,180 --> 00:06:47,620
interesting so that so this is an

00:06:45,580 --> 00:06:49,180
emergent this is an emerging area but

00:06:47,620 --> 00:06:50,680
ultimately we're not a lot of that

00:06:49,180 --> 00:06:52,330
research is gonna be applicable until

00:06:50,680 --> 00:06:53,920
five years from now there's just there's

00:06:52,330 --> 00:06:56,260
a lot of there's a lot of room yet for

00:06:53,920 --> 00:06:58,150
for before production izing this so the

00:06:56,260 --> 00:06:59,380
the compromise is kicking it off to it

00:06:58,150 --> 00:07:01,840
kicking off the final decision to a

00:06:59,380 --> 00:07:04,690
human when you're uncertain do you have

00:07:01,840 --> 00:07:06,190
some customer use cases yes yeah so a

00:07:04,690 --> 00:07:08,410
lot of that so one of our biggest

00:07:06,190 --> 00:07:11,680
customers is a gigantic telecom in Asia

00:07:08,410 --> 00:07:13,450
and we're actually helping them with you

00:07:11,680 --> 00:07:16,180
know actually we're augmenting them with

00:07:13,450 --> 00:07:18,430
deep learning to to basically we have

00:07:16,180 --> 00:07:20,290
their their analysts used deep learning

00:07:18,430 --> 00:07:22,360
to figure out with you know with some

00:07:20,290 --> 00:07:24,400
manual observation to both label to

00:07:22,360 --> 00:07:26,320
label data as what well as fine as well

00:07:24,400 --> 00:07:27,610
as fine causation and probably couldn't

00:07:26,320 --> 00:07:30,040
fault on the network among other things

00:07:27,610 --> 00:07:31,540
and so you know so they have you so

00:07:30,040 --> 00:07:32,950
imagine imagine you break up the whole

00:07:31,540 --> 00:07:34,990
world into five hundred meter squares

00:07:32,950 --> 00:07:36,160
and what you're able to do in those five

00:07:34,990 --> 00:07:38,290
hundred meter squares is over fit a

00:07:36,160 --> 00:07:40,050
model you over fit that model then you

00:07:38,290 --> 00:07:42,610
have a human analyst using deep learning

00:07:40,050 --> 00:07:44,200
to surface insights from that and so

00:07:42,610 --> 00:07:48,250
what do I mean by that I mean each 500

00:07:44,200 --> 00:07:51,130
meter square is escorting and ranked for

00:07:48,250 --> 00:07:52,510
four problems by by the Machine and then

00:07:51,130 --> 00:07:54,370
the human can then take it then and take

00:07:52,510 --> 00:07:56,320
a look at it and approve it so by

00:07:54,370 --> 00:07:58,630
overfitting then are you kicking out

00:07:56,320 --> 00:07:59,740
some of anomalies in your game you do

00:07:58,630 --> 00:08:01,540
that you do and you do that on purpose

00:07:59,740 --> 00:08:03,370
yeah because you want to find root

00:08:01,540 --> 00:08:06,190
causes and you want to find you want to

00:08:03,370 --> 00:08:07,960
find you know weird behavior so that's

00:08:06,190 --> 00:08:09,130
actually the goal and that goes directly

00:08:07,960 --> 00:08:11,169
back to what you're saying before you

00:08:09,130 --> 00:08:13,030
want to identify the high risk right and

00:08:11,169 --> 00:08:14,050
those by definition are right and so

00:08:13,030 --> 00:08:15,430
that that's probably one of the few

00:08:14,050 --> 00:08:15,870
areas where overfitting actually makes

00:08:15,430 --> 00:08:19,140
sense

00:08:15,870 --> 00:08:21,790
excellent okay so I I have a question

00:08:19,140 --> 00:08:23,980
you know we talk a lot about supervised

00:08:21,790 --> 00:08:25,330
machine learning kind of the double you

00:08:23,980 --> 00:08:26,890
know that's what most of what people are

00:08:25,330 --> 00:08:28,980
talking about deep learning is typically

00:08:26,890 --> 00:08:31,240
more supervised there's labels

00:08:28,980 --> 00:08:33,940
active learning is a kind of semi

00:08:31,240 --> 00:08:36,700
supervisor but you know there's a whole

00:08:33,940 --> 00:08:38,469
of the realm of unsupervised and I

00:08:36,700 --> 00:08:40,300
haven't seen a lot I know it's still

00:08:38,469 --> 00:08:41,650
kind of in research statements but what

00:08:40,300 --> 00:08:43,270
what are you seeing as far as like

00:08:41,650 --> 00:08:45,130
unsupervised learning I think well well

00:08:43,270 --> 00:08:46,660
for one don't use Gans okay

00:08:45,130 --> 00:08:48,640
don't use Gans out of error they're

00:08:46,660 --> 00:08:50,740
there too so most of the research right

00:08:48,640 --> 00:08:52,600
now is still around stabilizing them

00:08:50,740 --> 00:08:53,740
so using them a production is actually a

00:08:52,600 --> 00:08:55,480
very bad idea

00:08:53,740 --> 00:08:57,579
what you should use instead are

00:08:55,480 --> 00:08:59,199
variational auto-encoders okay because

00:08:57,579 --> 00:09:00,730
they they're they're more stable and

00:08:59,199 --> 00:09:03,100
they're easier to scale and drain and

00:09:00,730 --> 00:09:04,749
they give you priors to mess around so

00:09:03,100 --> 00:09:06,519
they give you a lot so you can actually

00:09:04,749 --> 00:09:08,379
two very similar to restrictive Boulton

00:09:06,519 --> 00:09:09,939
machines they give you a distribution of

00:09:08,379 --> 00:09:11,860
upon which you can constrain the data

00:09:09,939 --> 00:09:13,420
and so you can customize the input and

00:09:11,860 --> 00:09:14,619
the output and there's also a new

00:09:13,420 --> 00:09:17,079
another paper that came out recently

00:09:14,619 --> 00:09:18,069
that called the info VA II the info

00:09:17,079 --> 00:09:19,689
variational auto encoder

00:09:18,069 --> 00:09:20,740
that actually that actually fact there's

00:09:19,689 --> 00:09:23,800
an information gained into the

00:09:20,740 --> 00:09:25,660
reconstruction error oh yeah so now you

00:09:23,800 --> 00:09:27,610
could have like a gradient of what the

00:09:25,660 --> 00:09:28,990
different factors yes exactly and so

00:09:27,610 --> 00:09:30,339
that so there's a lot of areas for

00:09:28,990 --> 00:09:32,290
interpretability I think with a little

00:09:30,339 --> 00:09:33,910
bit of human I think with a little bit

00:09:32,290 --> 00:09:35,379
human to link a human put some tooling

00:09:33,910 --> 00:09:37,540
in there I think you can actually make

00:09:35,379 --> 00:09:39,639
use of that in in in real products being

00:09:37,540 --> 00:09:40,899
deployed and so if you if you have if

00:09:39,639 --> 00:09:42,490
you use that as a way of kind of

00:09:40,899 --> 00:09:43,720
grouping and finding you know

00:09:42,490 --> 00:09:45,759
automatically finding trends and data

00:09:43,720 --> 00:09:47,319
you know maybe variational auto-encoders

00:09:45,759 --> 00:09:49,119
plus plus some sort of you know like in

00:09:47,319 --> 00:09:51,129
this case we use a lot of T stochastic

00:09:49,119 --> 00:09:53,259
neighbor embedding to find find

00:09:51,129 --> 00:09:55,059
anomalies and group you know hidden

00:09:53,259 --> 00:09:57,100
hidden trends in group by automatically

00:09:55,059 --> 00:09:59,319
grouping data for visualization yeah

00:09:57,100 --> 00:10:01,120
yeah so for visualization as well as

00:09:59,319 --> 00:10:03,429
well as ranking by reconstruction error

00:10:01,120 --> 00:10:04,749
to figure out of individual examples to

00:10:03,429 --> 00:10:06,339
figure out what you should be looking at

00:10:04,749 --> 00:10:09,189
so that so there's a whole area where

00:10:06,339 --> 00:10:10,589
you can use a combination of k-means vp

00:10:09,189 --> 00:10:12,279
trees for key nearest neighbors

00:10:10,589 --> 00:10:14,139
reconstruction course scores and tease

00:10:12,279 --> 00:10:15,459
me there's a holes there's a hole slot

00:10:14,139 --> 00:10:17,199
of solutions in a whole new workflow

00:10:15,459 --> 00:10:18,990
where you can kind of bake that into

00:10:17,199 --> 00:10:21,639
your expert torie data analysis process

00:10:18,990 --> 00:10:23,079
interesting and then you can also you

00:10:21,639 --> 00:10:25,149
can use it both in your visuals I

00:10:23,079 --> 00:10:26,550
visualize an exploration process and as

00:10:25,149 --> 00:10:29,379
well as for monitoring and production oh

00:10:26,550 --> 00:10:31,209
really on that yeah yeah well yeah so

00:10:29,379 --> 00:10:32,829
you so what you do is you as you get new

00:10:31,209 --> 00:10:33,999
examples in you can actually track a

00:10:32,829 --> 00:10:35,110
reconstruction and you know moving

00:10:33,999 --> 00:10:37,749
average of the reconstruction errors

00:10:35,110 --> 00:10:39,639
very simply to to just to send alerts

00:10:37,749 --> 00:10:41,889
like when it gets us above a certain

00:10:39,639 --> 00:10:43,059
tunable trends which will then then then

00:10:41,889 --> 00:10:45,339
you you know then you then you can

00:10:43,059 --> 00:10:47,589
proactively find new anomalies so

00:10:45,339 --> 00:10:49,449
another another application where we're

00:10:47,589 --> 00:10:51,670
using this as risk is actually risk

00:10:49,449 --> 00:10:52,660
scoring of of individual loan applicants

00:10:51,670 --> 00:10:55,269
Oh interesting

00:10:52,660 --> 00:10:57,819
mm-hm as opposed easing scorecards yeah

00:10:55,269 --> 00:10:59,589
exactly yeah yes yeah so instead instead

00:10:57,819 --> 00:11:01,389
you can you know you can you can explain

00:10:59,589 --> 00:11:02,870
it and justify it but you use deep

00:11:01,389 --> 00:11:05,330
learning to find the trends

00:11:02,870 --> 00:11:07,640
and then so that tells it basically

00:11:05,330 --> 00:11:09,770
gives that basically gives analysts at

00:11:07,640 --> 00:11:12,440
these big companies a tracking beacon on

00:11:09,770 --> 00:11:14,600
on what the end what it allows them to

00:11:12,440 --> 00:11:16,340
not only increase their hits on on

00:11:14,600 --> 00:11:18,590
things like fraud you know finding fraud

00:11:16,340 --> 00:11:22,310
but also on identifying sources of risk

00:11:18,590 --> 00:11:25,180
as well as you know causation root you

00:11:22,310 --> 00:11:27,140
know what causes in in network failure

00:11:25,180 --> 00:11:28,460
you know I'm curious one of the

00:11:27,140 --> 00:11:30,680
questions that I've heard a few times is

00:11:28,460 --> 00:11:32,710
how could we go from a system working

00:11:30,680 --> 00:11:35,990
with machine blowing uncertainty

00:11:32,710 --> 00:11:37,910
applying deep learning but then perhaps

00:11:35,990 --> 00:11:39,980
at the end of the workflow coming out

00:11:37,910 --> 00:11:42,890
with rules to encapsulate most of it are

00:11:39,980 --> 00:11:45,440
you yeah so another another thing we do

00:11:42,890 --> 00:11:46,850
is we use it to create new rules ok so

00:11:45,440 --> 00:11:49,190
so it's actually deep learning baked

00:11:46,850 --> 00:11:50,630
into a rules engine perfect and then and

00:11:49,190 --> 00:11:53,150
so it's it's a none deterministic flow

00:11:50,630 --> 00:11:55,580
that's watched and then you use that as

00:11:53,150 --> 00:11:57,710
a way of saying you know oh oh so send

00:11:55,580 --> 00:11:59,870
machine learning scored eighty greater

00:11:57,710 --> 00:12:02,180
than 80 percent do something like record

00:11:59,870 --> 00:12:03,350
this or or you know or you can just use

00:12:02,180 --> 00:12:05,420
it to update you know you can have

00:12:03,350 --> 00:12:08,630
something in production that you use it

00:12:05,420 --> 00:12:11,300
to find new new rules to add to an

00:12:08,630 --> 00:12:12,620
existing rules engine CCU so you get the

00:12:11,300 --> 00:12:14,390
the weakness of rules engines is that

00:12:12,620 --> 00:12:16,310
they're not dynamic well this this this

00:12:14,390 --> 00:12:18,380
gives you this gives you yeah this gives

00:12:16,310 --> 00:12:19,940
the people building you know building

00:12:18,380 --> 00:12:22,280
and maintaining rules for things like

00:12:19,940 --> 00:12:24,170
banks the ability to proactively

00:12:22,280 --> 00:12:26,690
identify new new trends to add to the

00:12:24,170 --> 00:12:28,430
rules engine fantastic and also I can

00:12:26,690 --> 00:12:30,230
imagine I'm trying to think as you're

00:12:28,430 --> 00:12:31,670
describing this you know how much code

00:12:30,230 --> 00:12:34,700
would I have to go through in terms of

00:12:31,670 --> 00:12:35,960
the API is to implement this but can you

00:12:34,700 --> 00:12:37,340
give me kind of like a ballpark figure

00:12:35,960 --> 00:12:39,170
are we talking about thousands of lines

00:12:37,340 --> 00:12:42,410
of code or is this something that's is

00:12:39,170 --> 00:12:43,910
relatively well so this is this is

00:12:42,410 --> 00:12:45,400
something I think if you if you have the

00:12:43,910 --> 00:12:47,480
proper unprimed data strip

00:12:45,400 --> 00:12:49,400
infrastructure already in place with

00:12:47,480 --> 00:12:50,420
things like auditing I I think a lot of

00:12:49,400 --> 00:12:51,950
it comes down to more training people

00:12:50,420 --> 00:12:53,090
it's very it's very it's very

00:12:51,950 --> 00:12:55,430
straightforward to implement things like

00:12:53,090 --> 00:12:56,630
a feedback API where you can flag this

00:12:55,430 --> 00:12:59,030
as wrong and then you record that in a

00:12:56,630 --> 00:13:00,350
database okay so it's not it's not

00:12:59,030 --> 00:13:02,420
rocket science to implement it's hard to

00:13:00,350 --> 00:13:03,950
do right yeah but I think with enough

00:13:02,420 --> 00:13:06,500
time and enough effort you can you can

00:13:03,950 --> 00:13:07,730
do you can pull this off fantastic well

00:13:06,500 --> 00:13:09,410
thank you very much Adam yeah this was

00:13:07,730 --> 00:13:11,470
great appreciate thanks Paco

00:13:09,410 --> 00:13:11,470
you

00:13:17,279 --> 00:13:19,339

YouTube URL: https://www.youtube.com/watch?v=hAj5jnt-I10


