Title: Susan Etlinger (Altimeter Group) interviewed at the Artificial Intelligence Conference, London 2018
Publication date: 2018-10-17
Playlist: Artificial Intelligence Conference 2018 - London, United Kingdom
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:04,560
hi I'm Paco Nathan with O'Reilly Media

00:00:02,399 --> 00:00:07,710
and it's pleasure to be here today with

00:00:04,560 --> 00:00:08,880
a good friend susan ettinger also from

00:00:07,710 --> 00:00:11,400
the Bay Area yep

00:00:08,880 --> 00:00:15,929
and you've been speaking here at the AI

00:00:11,400 --> 00:00:18,480
conference about AI and ethics yeah yes

00:00:15,929 --> 00:00:20,490
I've been looking at the impact of

00:00:18,480 --> 00:00:24,960
artificial intelligence or autonomous

00:00:20,490 --> 00:00:27,449
technologies on business and society for

00:00:24,960 --> 00:00:29,160
quite some time now and one of the

00:00:27,449 --> 00:00:31,800
things that's interesting to me is that

00:00:29,160 --> 00:00:33,690
when we think about this notion of

00:00:31,800 --> 00:00:36,989
ethics which is all of a sudden becoming

00:00:33,690 --> 00:00:38,640
kind of more topical for people we don't

00:00:36,989 --> 00:00:40,710
really have a good model for what it is

00:00:38,640 --> 00:00:42,600
and I like to kind of think of ethics in

00:00:40,710 --> 00:00:44,730
a very simple way as norms of behavior

00:00:42,600 --> 00:00:46,770
like norms of acceptable behavior in a

00:00:44,730 --> 00:00:48,719
society or community we don't we and so

00:00:46,770 --> 00:00:52,020
we have that in the real world and the

00:00:48,719 --> 00:00:53,940
physical real the other world the

00:00:52,020 --> 00:00:55,289
physical world we may not always adhere

00:00:53,940 --> 00:00:58,710
to them but we kind of know what they

00:00:55,289 --> 00:00:59,910
are and yet in the digital space like

00:00:58,710 --> 00:01:00,960
there are a lot of places that are

00:00:59,910 --> 00:01:03,120
they're just completely novel

00:01:00,960 --> 00:01:04,470
interactions or novel experiences and we

00:01:03,120 --> 00:01:06,420
don't really know how to navigate those

00:01:04,470 --> 00:01:08,250
interesting it's something alien yeah

00:01:06,420 --> 00:01:11,250
but but just even giving it the

00:01:08,250 --> 00:01:15,090
nomenclature and how can people as you

00:01:11,250 --> 00:01:17,250
know as adults talk in groups that seems

00:01:15,090 --> 00:01:18,390
like the first best step yeah although

00:01:17,250 --> 00:01:20,250
you know it's funny when I started

00:01:18,390 --> 00:01:21,960
writing about this I felt like I needed

00:01:20,250 --> 00:01:23,729
to sort of hide it under the vegetables

00:01:21,960 --> 00:01:26,009
or hide the vegetables under the you

00:01:23,729 --> 00:01:28,380
know main course kind of thing and so I

00:01:26,009 --> 00:01:31,680
started talking about artificial

00:01:28,380 --> 00:01:33,600
intelligence ethics as an element of the

00:01:31,680 --> 00:01:35,040
customer experience because when you're

00:01:33,600 --> 00:01:36,659
talking at people in the enterprise are

00:01:35,040 --> 00:01:38,850
talking business people marketers or

00:01:36,659 --> 00:01:40,140
strategy people or product people they

00:01:38,850 --> 00:01:40,950
really think about the customer

00:01:40,140 --> 00:01:42,630
experience they don't really have a

00:01:40,950 --> 00:01:44,310
model for thinking about ethics it just

00:01:42,630 --> 00:01:46,170
has anything oh well maybe that's

00:01:44,310 --> 00:01:48,420
compliance or that's legal or that's

00:01:46,170 --> 00:01:49,979
like a PR issue that we have to deal

00:01:48,420 --> 00:01:52,799
with they don't necessarily think about

00:01:49,979 --> 00:01:55,439
ethics in the way that it affects the

00:01:52,799 --> 00:01:57,299
customer per se and so I felt like that

00:01:55,439 --> 00:01:59,369
was a useful construct to at least start

00:01:57,299 --> 00:02:01,259
the conversation but I kind of feel like

00:01:59,369 --> 00:02:04,619
you know now I don't know two or so

00:02:01,259 --> 00:02:06,570
years into that it's time to come out of

00:02:04,619 --> 00:02:08,220
the closet and talk about ethics and

00:02:06,570 --> 00:02:09,509
just say okay unapologetically we

00:02:08,220 --> 00:02:11,730
actually need to talk about digital

00:02:09,509 --> 00:02:13,650
ethics serve the raw vegetables as a

00:02:11,730 --> 00:02:18,040
food attack exactly

00:02:13,650 --> 00:02:20,409
exactly it's okay to be a vegan now Li

00:02:18,040 --> 00:02:21,849
you you had a tweet thread recently

00:02:20,409 --> 00:02:23,859
there's one that's pinned on Twitter

00:02:21,849 --> 00:02:25,209
that that goes into excellent detail I

00:02:23,859 --> 00:02:28,780
like there was a lot of interaction on

00:02:25,209 --> 00:02:31,060
that too could you describe more some of

00:02:28,780 --> 00:02:32,140
I guess you know what are the starting

00:02:31,060 --> 00:02:34,420
points or what are the issues that were

00:02:32,140 --> 00:02:36,519
running into yeah so that the thread

00:02:34,420 --> 00:02:38,200
actually have to I have to apologize to

00:02:36,519 --> 00:02:40,209
and also thank Kathy Smith who wrote

00:02:38,200 --> 00:02:42,730
weapons of mass destruction because she

00:02:40,209 --> 00:02:44,260
kind of started that that thread and

00:02:42,730 --> 00:02:45,700
essentially what had happened was she

00:02:44,260 --> 00:02:47,079
wrote this book called weapons of mass

00:02:45,700 --> 00:02:49,590
destruction which is about the ways in

00:02:47,079 --> 00:02:51,220
which algorithms can lead to

00:02:49,590 --> 00:02:52,599
disenfranchisement of people in the

00:02:51,220 --> 00:02:54,879
healthcare and the criminal justice

00:02:52,599 --> 00:02:58,359
system and financial services and things

00:02:54,879 --> 00:03:01,480
like that and one day she made a comment

00:02:58,359 --> 00:03:03,090
that I think was she didn't intend it

00:03:01,480 --> 00:03:05,889
the way it was taken which was that

00:03:03,090 --> 00:03:08,319
academia has been sort of you know on

00:03:05,889 --> 00:03:09,459
its heels about this issue of AI ethics

00:03:08,319 --> 00:03:11,230
and really needs to take a stronger

00:03:09,459 --> 00:03:13,359
stand and so a lot of academics were not

00:03:11,230 --> 00:03:15,609
very pleased with that comment and felt

00:03:13,359 --> 00:03:17,979
that you know and it's sort of piled on

00:03:15,609 --> 00:03:19,690
and and started passing around lists of

00:03:17,979 --> 00:03:22,030
people in academia who are working on

00:03:19,690 --> 00:03:24,310
different ethics issues related to bias

00:03:22,030 --> 00:03:27,129
and discrimination other things and so I

00:03:24,310 --> 00:03:29,829
thought it was a good kind of a useful

00:03:27,129 --> 00:03:31,419
kind of moment to start talking about

00:03:29,829 --> 00:03:34,000
what's happening in happening in

00:03:31,419 --> 00:03:35,440
industry because you know while I work

00:03:34,000 --> 00:03:38,169
in business and I'm essentially a

00:03:35,440 --> 00:03:40,659
business focused person I do look at a

00:03:38,169 --> 00:03:42,280
lot of the academic papers and try to

00:03:40,659 --> 00:03:45,579
understand what's happening in academia

00:03:42,280 --> 00:03:47,199
so I started thinking about kind of what

00:03:45,579 --> 00:03:48,760
are some of the issues we need to be

00:03:47,199 --> 00:03:49,780
thinking and talking about for as

00:03:48,760 --> 00:03:52,569
businesspeople

00:03:49,780 --> 00:03:55,150
given that algorithms have an impact in

00:03:52,569 --> 00:03:57,639
society it's not necessarily an easy

00:03:55,150 --> 00:03:59,290
leap to think about what happens in a

00:03:57,639 --> 00:04:01,120
recommendation engine what happens in a

00:03:59,290 --> 00:04:02,949
personalization algorithm or a

00:04:01,120 --> 00:04:05,680
translation algorithm or natural

00:04:02,949 --> 00:04:08,379
language tool or you know speech

00:04:05,680 --> 00:04:10,419
detection like an Alexa or chatbots

00:04:08,379 --> 00:04:12,459
voice agents and those sorts of things

00:04:10,419 --> 00:04:14,500
and so I wanted to I wanted to say you

00:04:12,459 --> 00:04:16,750
know it's important not only for us to

00:04:14,500 --> 00:04:18,159
deal with this societally but that

00:04:16,750 --> 00:04:20,289
businesses actually need to start

00:04:18,159 --> 00:04:23,789
thinking about the value chain of data

00:04:20,289 --> 00:04:25,060
and also kind of the implications of

00:04:23,789 --> 00:04:27,490
autonomous and

00:04:25,060 --> 00:04:29,200
self-learning technologies on the way

00:04:27,490 --> 00:04:31,750
that they deliver products and services

00:04:29,200 --> 00:04:33,940
and ultimately the impact on individual

00:04:31,750 --> 00:04:36,490
people who receive those products and

00:04:33,940 --> 00:04:38,740
services and and and really also are you

00:04:36,490 --> 00:04:40,840
focusing or pointing that focus toward

00:04:38,740 --> 00:04:43,000
the use cases here are the actual

00:04:40,840 --> 00:04:44,700
business is absolutely customers

00:04:43,000 --> 00:04:46,570
interact with all day every day

00:04:44,700 --> 00:04:49,300
absolutely so if you're doing audio

00:04:46,570 --> 00:04:50,919
audience segmentation and you understand

00:04:49,300 --> 00:04:53,620
for example that like one of the big

00:04:50,919 --> 00:04:56,860
issues around algorithmic bias has to do

00:04:53,620 --> 00:04:59,680
with language in the way that the word

00:04:56,860 --> 00:05:03,370
to back data set which is used to train

00:04:59,680 --> 00:05:05,950
natural languages algorithms is used you

00:05:03,370 --> 00:05:08,260
know it even includes and it contains a

00:05:05,950 --> 00:05:09,700
lot of intrinsic bias and there was a

00:05:08,260 --> 00:05:12,310
paper written by a couple of researchers

00:05:09,700 --> 00:05:13,600
at Microsoft and in Boston University is

00:05:12,310 --> 00:05:15,639
about two years old now that's sort of

00:05:13,600 --> 00:05:18,700
the original you know text in some ways

00:05:15,639 --> 00:05:22,330
of a lot of this discussion and you know

00:05:18,700 --> 00:05:24,010
that's that's important from a societal

00:05:22,330 --> 00:05:26,200
perspective but like when you think

00:05:24,010 --> 00:05:27,669
about it it's feeding and to naturally

00:05:26,200 --> 00:05:30,310
it's feeding into audience segmentation

00:05:27,669 --> 00:05:32,710
it's feeding into any kind of

00:05:30,310 --> 00:05:33,940
conversational business technologies

00:05:32,710 --> 00:05:35,470
that you're using whether they're chat

00:05:33,940 --> 00:05:37,450
or voice it's feeding into off lots of

00:05:35,470 --> 00:05:39,700
other things and so I wanted kind of

00:05:37,450 --> 00:05:42,729
businesspeople to be thinking about gosh

00:05:39,700 --> 00:05:45,430
you know if we're doing this like do we

00:05:42,729 --> 00:05:47,620
don't want obviously to be creating

00:05:45,430 --> 00:05:51,520
racist products and services or sexist

00:05:47,620 --> 00:05:53,979
or you know in some way dehumanizing our

00:05:51,520 --> 00:05:57,190
customers we need to understand the ways

00:05:53,979 --> 00:05:59,080
that these the stuff works and actually

00:05:57,190 --> 00:06:00,639
start to understand some of the ways in

00:05:59,080 --> 00:06:02,350
which people are trying to remediate it

00:06:00,639 --> 00:06:04,270
so that we can ensure that we're not

00:06:02,350 --> 00:06:06,250
making we're not inadvertently doing

00:06:04,270 --> 00:06:08,080
things that end up disenfranchising or

00:06:06,250 --> 00:06:09,820
alienating people and breaking trust

00:06:08,080 --> 00:06:11,680
with us and so there's there's actually

00:06:09,820 --> 00:06:13,690
a long supply chain if you will or a

00:06:11,680 --> 00:06:14,950
long pipeline leading up to the

00:06:13,690 --> 00:06:17,440
decisions that are made automatically

00:06:14,950 --> 00:06:19,330
and organizations could be sort of

00:06:17,440 --> 00:06:20,710
undermining their last-mile efforts

00:06:19,330 --> 00:06:22,330
that's right they're not doing the right

00:06:20,710 --> 00:06:23,620
work that's to prepare their data that's

00:06:22,330 --> 00:06:25,030
right and some of these are known issues

00:06:23,620 --> 00:06:26,979
and some of these are not own issues or

00:06:25,030 --> 00:06:28,810
they're known in some communities and

00:06:26,979 --> 00:06:30,600
not known another so like one simple

00:06:28,810 --> 00:06:32,310
example would be you know

00:06:30,600 --> 00:06:34,170
and actually Cathy Smith I think has

00:06:32,310 --> 00:06:36,870
given this example like if you were to

00:06:34,170 --> 00:06:38,850
create an algorithm to hire people you

00:06:36,870 --> 00:06:40,590
know at a big let's say a big tech

00:06:38,850 --> 00:06:42,510
company like a multibillion-dollar tech

00:06:40,590 --> 00:06:44,570
company and you want it to look for all

00:06:42,510 --> 00:06:47,670
the attributes of the most successful

00:06:44,570 --> 00:06:48,960
employees so you look at tenure and you

00:06:47,670 --> 00:06:50,730
look at level and you look at

00:06:48,960 --> 00:06:52,650
performance evaluations and maybe you

00:06:50,730 --> 00:06:54,630
look at compensation or promotion you

00:06:52,650 --> 00:06:57,290
know the consistent promotion history

00:06:54,630 --> 00:07:02,220
you're gonna find a lot of those people

00:06:57,290 --> 00:07:04,620
shockingly kind of look the same are of

00:07:02,220 --> 00:07:08,040
the same gender are in a pretty narrow

00:07:04,620 --> 00:07:10,680
age range and so there's a pattern

00:07:08,040 --> 00:07:12,540
matching thing that that is done and you

00:07:10,680 --> 00:07:14,820
know algorithms are not like they're not

00:07:12,540 --> 00:07:17,880
intuitive right they if you tell them

00:07:14,820 --> 00:07:20,790
look for success it will define success

00:07:17,880 --> 00:07:24,870
in the ways that it sees it will not

00:07:20,790 --> 00:07:27,060
infer that oh you know women have left

00:07:24,870 --> 00:07:29,970
you know the company for one reason or

00:07:27,060 --> 00:07:32,670
another or weren't hired in or or people

00:07:29,970 --> 00:07:35,250
of color were it right it doesn't

00:07:32,670 --> 00:07:38,130
understand those externalities and so so

00:07:35,250 --> 00:07:40,470
you end up actually you end up actually

00:07:38,130 --> 00:07:42,690
kind of intensifying and amplifying the

00:07:40,470 --> 00:07:44,490
bias that existed in the beginning by

00:07:42,690 --> 00:07:46,890
training that algorithm with the data

00:07:44,490 --> 00:07:48,480
that you already have and I think while

00:07:46,890 --> 00:07:51,630
a data scientist sort of innately

00:07:48,480 --> 00:07:53,370
understands that your average business

00:07:51,630 --> 00:07:55,350
person know that there is an average but

00:07:53,370 --> 00:07:57,480
like you know most business people don't

00:07:55,350 --> 00:08:00,330
necessarily know that so I think it's

00:07:57,480 --> 00:08:02,100
important for us to flip the switch so

00:08:00,330 --> 00:08:03,690
that they walk into a meeting and they

00:08:02,100 --> 00:08:05,010
have that in the back of their mind and

00:08:03,690 --> 00:08:07,290
they start asking those probing

00:08:05,010 --> 00:08:10,290
questions about you know can we ensure

00:08:07,290 --> 00:08:12,630
that we're not inadvertently kind of

00:08:10,290 --> 00:08:14,760
missing groups of our customer base or

00:08:12,630 --> 00:08:17,190
our desired customer base or addressable

00:08:14,760 --> 00:08:18,600
market because we just don't have you

00:08:17,190 --> 00:08:21,330
know rich enough data we don't have the

00:08:18,600 --> 00:08:23,280
right data there was a another Twitter

00:08:21,330 --> 00:08:25,230
thread this morning I think probably

00:08:23,280 --> 00:08:26,700
someone who's a mutual friend of ours if

00:08:25,230 --> 00:08:29,850
not we'll make an introduction right

00:08:26,700 --> 00:08:32,130
away today but James James was talking

00:08:29,850 --> 00:08:35,250
about how supervised machine learning is

00:08:32,130 --> 00:08:37,050
inherently promoting the status quo

00:08:35,250 --> 00:08:38,280
because we're always looking behind yes

00:08:37,050 --> 00:08:41,630
and particularly the future so there's

00:08:38,280 --> 00:08:42,860
sort of an inherent reactionary at

00:08:41,630 --> 00:08:45,860
aspect

00:08:42,860 --> 00:08:49,130
of supervised learning models yes and

00:08:45,860 --> 00:08:50,480
you know that's a bias in itself that's

00:08:49,130 --> 00:08:52,339
right from the get-go that's right

00:08:50,480 --> 00:08:54,560
that's right I mean if you you know I

00:08:52,339 --> 00:08:56,029
mean at the simplest level if you say

00:08:54,560 --> 00:08:58,310
you know there are two categories of

00:08:56,029 --> 00:09:01,730
fruit apples and bananas you show it a

00:08:58,310 --> 00:09:04,490
you know Volkswagen and and it's an

00:09:01,730 --> 00:09:08,149
apple or a banana and so you know so I

00:09:04,490 --> 00:09:10,130
think we you know one of the challenges

00:09:08,149 --> 00:09:11,750
that we're facing in in all of this

00:09:10,130 --> 00:09:14,570
which has absolutely nothing to do with

00:09:11,750 --> 00:09:16,850
technology is that the most of us think

00:09:14,570 --> 00:09:18,410
about artificial intelligence most of us

00:09:16,850 --> 00:09:20,029
who are not data scientists are not

00:09:18,410 --> 00:09:21,950
engineers think of us as I think of

00:09:20,029 --> 00:09:23,570
artificial intelligence in this way

00:09:21,950 --> 00:09:26,690
that's been culturally constructed for

00:09:23,570 --> 00:09:29,120
us by you know the media popular culture

00:09:26,690 --> 00:09:30,829
movies termina Skynet you know I mean

00:09:29,120 --> 00:09:32,540
you know what's the over-under on how

00:09:30,829 --> 00:09:34,220
long it takes for an AI conversation to

00:09:32,540 --> 00:09:37,730
get to Skynet is like a new Godwin's law

00:09:34,220 --> 00:09:39,350
or something right killer robots the

00:09:37,730 --> 00:09:41,000
trolley problem like all of these things

00:09:39,350 --> 00:09:44,329
with the singularity like all of these

00:09:41,000 --> 00:09:46,970
things kind of pop up and yet the issues

00:09:44,329 --> 00:09:50,209
that that artificial intelligence kind

00:09:46,970 --> 00:09:51,350
of raises are not necessarily at least

00:09:50,209 --> 00:09:54,140
not at this point

00:09:51,350 --> 00:09:55,850
those big existential issues about when

00:09:54,140 --> 00:09:57,680
the robots will take us over but they're

00:09:55,850 --> 00:09:58,519
the issues about how do we ensure that

00:09:57,680 --> 00:10:01,220
people of color are not

00:09:58,519 --> 00:10:03,589
disproportionately incarcerated or who

00:10:01,220 --> 00:10:08,420
or have enough medical care or that we

00:10:03,589 --> 00:10:10,070
don't miss that we don't deny mortgage

00:10:08,420 --> 00:10:12,829
loans to young people to young women

00:10:10,070 --> 00:10:15,769
because they lack a certain pattern

00:10:12,829 --> 00:10:19,250
matching capability that older men do

00:10:15,769 --> 00:10:21,529
you know those types of things we were

00:10:19,250 --> 00:10:22,940
talking a little earlier and one of the

00:10:21,529 --> 00:10:27,190
one of the things I was really I was

00:10:22,940 --> 00:10:29,449
learning from you about this is you know

00:10:27,190 --> 00:10:30,980
people making decisions out in business

00:10:29,449 --> 00:10:34,490
understand risk they do that every day

00:10:30,980 --> 00:10:36,230
and we're talking about ethics it's not

00:10:34,490 --> 00:10:37,790
like it's something completely new we

00:10:36,230 --> 00:10:40,040
make these decisions our life and

00:10:37,790 --> 00:10:42,079
there's no there's probably no perfect

00:10:40,040 --> 00:10:44,180
choice we're going to have to make tough

00:10:42,079 --> 00:10:45,769
decisions at some point but it's better

00:10:44,180 --> 00:10:48,140
you know that in advance than it is to

00:10:45,769 --> 00:10:50,570
be surprised by it so could you describe

00:10:48,140 --> 00:10:52,640
how that that kind of revisit some of

00:10:50,570 --> 00:10:55,699
our discussion about you know how do you

00:10:52,640 --> 00:10:56,570
really take a educated approach yeah I

00:10:55,699 --> 00:10:58,490
mean in

00:10:56,570 --> 00:11:00,860
here's the thing part of the challenge

00:10:58,490 --> 00:11:04,210
with the word ethics is that it does

00:11:00,860 --> 00:11:07,670
have this connotation of some you know

00:11:04,210 --> 00:11:09,620
sage bearded person wagging his finger

00:11:07,670 --> 00:11:11,360
at you you know and telling you how to

00:11:09,620 --> 00:11:12,920
live and that that's really not my

00:11:11,360 --> 00:11:14,900
intention like you know if you think

00:11:12,920 --> 00:11:16,490
about something as simple as Amazon

00:11:14,900 --> 00:11:18,440
rolling out its sandy delivery service

00:11:16,490 --> 00:11:20,510
so they roll out their same-day delivery

00:11:18,440 --> 00:11:21,800
service and you know if you you want to

00:11:20,510 --> 00:11:23,780
build an algorithm to say where should

00:11:21,800 --> 00:11:25,610
we put that little warehouse to ensure

00:11:23,780 --> 00:11:26,630
that we can get there right you know

00:11:25,610 --> 00:11:28,970
where should we put them to make sure

00:11:26,630 --> 00:11:30,440
that we can do same-day delivery to the

00:11:28,970 --> 00:11:32,210
greatest number of people with all these

00:11:30,440 --> 00:11:34,820
attributes who spend a certain amount of

00:11:32,210 --> 00:11:37,160
money who have whatever have certain

00:11:34,820 --> 00:11:38,840
shopping behaviors and then you cluster

00:11:37,160 --> 00:11:40,970
them around a city or you cluster them

00:11:38,840 --> 00:11:43,250
around a major metropolitan area you

00:11:40,970 --> 00:11:45,740
know what ended up happening was that in

00:11:43,250 --> 00:11:47,960
this in this construct same-day delivery

00:11:45,740 --> 00:11:50,000
was not available to you know

00:11:47,960 --> 00:11:52,400
lower-income communities and communities

00:11:50,000 --> 00:11:55,460
of color and one of the things you know

00:11:52,400 --> 00:11:59,870
that obviously that race is correlated

00:11:55,460 --> 00:12:01,370
with a zip code and and the reverse as

00:11:59,870 --> 00:12:03,830
well as the code is correlated with race

00:12:01,370 --> 00:12:05,840
right and so if you say well we're gonna

00:12:03,830 --> 00:12:07,490
use these zip codes and that's our proxy

00:12:05,840 --> 00:12:09,650
for how we're going to distribute this

00:12:07,490 --> 00:12:11,060
you end up in this situation where they

00:12:09,650 --> 00:12:12,590
had all these you know news headlines

00:12:11,060 --> 00:12:14,210
that said you know it's aimed at

00:12:12,590 --> 00:12:15,410
Amazon's racists because they're not

00:12:14,210 --> 00:12:17,660
offering same-day delivery to

00:12:15,410 --> 00:12:20,690
communities of people of color

00:12:17,660 --> 00:12:22,670
now was that Amazon's intention I don't

00:12:20,690 --> 00:12:24,020
speak for Amazon I don't you know but

00:12:22,670 --> 00:12:25,880
there was a business decision about

00:12:24,020 --> 00:12:27,620
where we should put these things there

00:12:25,880 --> 00:12:30,560
may or may not have been a discussion

00:12:27,620 --> 00:12:32,030
about what is the secondary implication

00:12:30,560 --> 00:12:33,830
secondary clinic is the wrong word but

00:12:32,030 --> 00:12:36,560
what's what's another implication of

00:12:33,830 --> 00:12:39,110
that and you know another implication of

00:12:36,560 --> 00:12:42,080
that is that now we've actually created

00:12:39,110 --> 00:12:44,150
this sort of what do they call them and

00:12:42,080 --> 00:12:46,790
you have like food desert right yes kind

00:12:44,150 --> 00:12:48,380
of delivery desert and so now we've

00:12:46,790 --> 00:12:50,120
essentially created a situation in which

00:12:48,380 --> 00:12:51,830
a group of people is kind of feeling

00:12:50,120 --> 00:12:53,900
like second-class citizens because

00:12:51,830 --> 00:12:56,180
they're not getting the same services

00:12:53,900 --> 00:12:58,460
that they're you know counterparts and

00:12:56,180 --> 00:13:00,110
other zip codes are getting and so all

00:12:58,460 --> 00:13:02,510
I'm saying here is that yes there's a

00:13:00,110 --> 00:13:04,400
very complex you know series of

00:13:02,510 --> 00:13:05,890
interactions between data science and

00:13:04,400 --> 00:13:08,140
product development

00:13:05,890 --> 00:13:09,490
and ultimately though business decision

00:13:08,140 --> 00:13:12,339
making because some of these things end

00:13:09,490 --> 00:13:13,540
up becoming business decisions and I

00:13:12,339 --> 00:13:16,630
think it's important to make those

00:13:13,540 --> 00:13:18,430
decisions knowing what the issues are as

00:13:16,630 --> 00:13:20,620
opposed to not knowing what the issues

00:13:18,430 --> 00:13:22,149
are and I may agree or disagree I think

00:13:20,620 --> 00:13:24,420
it's right or wrong that's my personal

00:13:22,149 --> 00:13:28,690
thing but you know I don't get to choose

00:13:24,420 --> 00:13:30,790
well you know it does my perspective it

00:13:28,690 --> 00:13:33,279
seems like businesses came into being

00:13:30,790 --> 00:13:36,070
data-driven and adopting digital science

00:13:33,279 --> 00:13:40,240
really looking at just one or two KPIs

00:13:36,070 --> 00:13:43,450
per use case so it's very simple view of

00:13:40,240 --> 00:13:44,709
optimizing and now across the board

00:13:43,450 --> 00:13:46,360
they're finding out that's really not

00:13:44,709 --> 00:13:48,010
good right and it may not be the

00:13:46,360 --> 00:13:49,899
secondary or tertiary or whatever but

00:13:48,010 --> 00:13:51,370
it's not a simple thing to optimize you

00:13:49,899 --> 00:13:53,709
can't just have a product manager

00:13:51,370 --> 00:13:56,410
screaming about one API you have to take

00:13:53,709 --> 00:13:57,910
a look at a lot of trade offs that's but

00:13:56,410 --> 00:13:59,740
that's getting elevated more I mean

00:13:57,910 --> 00:14:02,019
that's becoming more of an executive

00:13:59,740 --> 00:14:03,880
decision it's not a product managers yes

00:14:02,019 --> 00:14:05,050
and I think that this actually makes me

00:14:03,880 --> 00:14:06,730
quite happy because I feel as though

00:14:05,050 --> 00:14:09,329
they're a couple different things that

00:14:06,730 --> 00:14:12,670
are happening at the same time one is

00:14:09,329 --> 00:14:13,990
people like Avinash Kaushik at Google

00:14:12,670 --> 00:14:16,120
have been talking about you know the

00:14:13,990 --> 00:14:18,490
danger of a single KPI or danger of a

00:14:16,120 --> 00:14:20,800
single metric for a long time like if

00:14:18,490 --> 00:14:22,630
you read his his blog he's been talking

00:14:20,800 --> 00:14:25,930
about this for a long time and then the

00:14:22,630 --> 00:14:27,699
Nigerian author Chimamanda Ngozi Adichie

00:14:25,930 --> 00:14:29,170
wrote this book called Americana and a

00:14:27,699 --> 00:14:30,760
couple of other books and she's also

00:14:29,170 --> 00:14:32,410
done a TED talk called the danger of a

00:14:30,760 --> 00:14:35,589
single story in which she talks about

00:14:32,410 --> 00:14:38,350
the dangers that we take on as a society

00:14:35,589 --> 00:14:40,269
when we kind of put people into kind of

00:14:38,350 --> 00:14:42,250
a single story a story of overcoming

00:14:40,269 --> 00:14:44,380
poverty or a story of overcoming

00:14:42,250 --> 00:14:46,269
adversity and not kind of think about

00:14:44,380 --> 00:14:48,640
their full humanity so when we think

00:14:46,269 --> 00:14:51,130
about metrics and optimizing for

00:14:48,640 --> 00:14:53,649
engagement just engagement just revenue

00:14:51,130 --> 00:14:55,180
we have to think about what happens

00:14:53,649 --> 00:14:56,860
what's the what's the trade-off there

00:14:55,180 --> 00:14:58,420
you know there's that old thought

00:14:56,860 --> 00:15:00,100
experiment about what happens if you

00:14:58,420 --> 00:15:01,570
train an algorithm to turn you know to

00:15:00,100 --> 00:15:03,339
create paper clips and then it ends up

00:15:01,570 --> 00:15:04,839
destroying their cars in the whole world

00:15:03,339 --> 00:15:06,459
just you know to make the paper clips

00:15:04,839 --> 00:15:08,740
you know that's sort of the same level

00:15:06,459 --> 00:15:10,870
of thinking I think so the second piece

00:15:08,740 --> 00:15:12,250
is it makes me I think it's very

00:15:10,870 --> 00:15:14,350
important first of all to have these

00:15:12,250 --> 00:15:16,920
conversations to try to educate people

00:15:14,350 --> 00:15:18,660
about what some of those trade-offs are

00:15:16,920 --> 00:15:20,579
then it's natural for the executives to

00:15:18,660 --> 00:15:23,730
walk into a room and have those

00:15:20,579 --> 00:15:26,579
conversations you know in a at least in

00:15:23,730 --> 00:15:28,170
an informed way as opposed to like

00:15:26,579 --> 00:15:30,300
worrying about when the robots are

00:15:28,170 --> 00:15:33,480
coming to kill us which is like by the

00:15:30,300 --> 00:15:41,670
way Thursday at 7:00 just in case you're

00:15:33,480 --> 00:15:43,980
wondering I'll have to adjust me there

00:15:41,670 --> 00:15:46,170
I'm curious about like where's the state

00:15:43,980 --> 00:15:49,740
of things right now I've definitely seen

00:15:46,170 --> 00:15:53,010
you know notable people doing series of

00:15:49,740 --> 00:15:55,050
articles and essays there's a great one

00:15:53,010 --> 00:15:57,000
one of our mutual friends Mike lucky

00:15:55,050 --> 00:15:58,320
he's working with Hillary Mason did you

00:15:57,000 --> 00:16:00,420
until they came out with a great series

00:15:58,320 --> 00:16:02,730
this summer and there's other groups

00:16:00,420 --> 00:16:05,310
that are starting to produce check lists

00:16:02,730 --> 00:16:07,800
and make them get hub friendly so that

00:16:05,310 --> 00:16:10,889
if as you're launching a project include

00:16:07,800 --> 00:16:12,120
this along but you know what are some of

00:16:10,889 --> 00:16:14,220
the next steps and where does this roll

00:16:12,120 --> 00:16:15,870
out what is it baby hose so we're in

00:16:14,220 --> 00:16:17,100
this sort of super nascent place right

00:16:15,870 --> 00:16:18,240
where people are just starting to talk

00:16:17,100 --> 00:16:19,350
about ethics I'm starting to see on

00:16:18,240 --> 00:16:22,890
LinkedIn people are like calling

00:16:19,350 --> 00:16:24,810
themselves like AI emphasis unless

00:16:22,890 --> 00:16:27,570
you're really a philosopher and a data

00:16:24,810 --> 00:16:28,529
scientist like together but so the

00:16:27,570 --> 00:16:30,510
interesting things are happening so

00:16:28,529 --> 00:16:31,949
principles are coming out like a lot of

00:16:30,510 --> 00:16:34,529
businesses are starting to publish

00:16:31,949 --> 00:16:37,380
principles I saw Keith Biglow from GE

00:16:34,529 --> 00:16:40,490
published a piece a blog post not to G

00:16:37,380 --> 00:16:42,810
healthcare publish a piece on principles

00:16:40,490 --> 00:16:46,560
insurance company access published some

00:16:42,810 --> 00:16:48,449
principles different companies DJ and

00:16:46,560 --> 00:16:51,089
Mike and Hillary Mason published some

00:16:48,449 --> 00:16:52,529
checklists so there's different things

00:16:51,089 --> 00:16:53,880
starting to happen I think if the first

00:16:52,529 --> 00:16:55,529
level its principles what are we going

00:16:53,880 --> 00:16:56,519
to adhere to what are we going to you

00:16:55,529 --> 00:16:58,589
know what are gonna be our driving

00:16:56,519 --> 00:17:00,839
values up front is it a value of

00:16:58,589 --> 00:17:04,400
inclusion like and and and should we

00:17:00,839 --> 00:17:06,959
ensure you know that we we prioritize

00:17:04,400 --> 00:17:09,150
our customers dignity or customers

00:17:06,959 --> 00:17:11,100
feeling of self-worth or positive

00:17:09,150 --> 00:17:12,870
experience along with revenue generation

00:17:11,100 --> 00:17:15,150
and how do we handle that trade-off so

00:17:12,870 --> 00:17:16,290
principles first and then practices

00:17:15,150 --> 00:17:17,490
right so what are we going to do in our

00:17:16,290 --> 00:17:19,439
research what are we going to do in our

00:17:17,490 --> 00:17:22,470
engineering what are we going to do in

00:17:19,439 --> 00:17:24,120
our product development processes

00:17:22,470 --> 00:17:28,740
our go-to-market strategies are

00:17:24,120 --> 00:17:30,780
communications to infuse as user

00:17:28,740 --> 00:17:32,070
interface design to infuse all those

00:17:30,780 --> 00:17:33,419
things as much as possible

00:17:32,070 --> 00:17:35,730
and then how are we going to govern

00:17:33,419 --> 00:17:37,559
those so right now there are little

00:17:35,730 --> 00:17:39,059
sparkles of things happening at all

00:17:37,559 --> 00:17:42,270
these different levels their principles

00:17:39,059 --> 00:17:46,169
coming out there are I now has published

00:17:42,270 --> 00:17:47,730
an algorithmic impact assessment for

00:17:46,169 --> 00:17:49,590
public agencies which I think could very

00:17:47,730 --> 00:17:52,440
you know now I wouldn't say easily but

00:17:49,590 --> 00:17:53,970
could be very interestingly transformed

00:17:52,440 --> 00:17:55,559
into something that industry could use

00:17:53,970 --> 00:17:58,140
that businesses could use that's a

00:17:55,559 --> 00:17:59,970
fantastic good nerded yeah and Kay

00:17:58,140 --> 00:18:03,990
Crawford those folks there they're

00:17:59,970 --> 00:18:05,400
brilliant and then iBM has been doing

00:18:03,990 --> 00:18:08,940
some interesting work around trying to

00:18:05,400 --> 00:18:10,830
show like do biased show bias checks and

00:18:08,940 --> 00:18:13,020
a supplier declaration of Conformity to

00:18:10,830 --> 00:18:14,549
basically to say you know toolkits

00:18:13,020 --> 00:18:17,010
essentially to bring partners on board

00:18:14,549 --> 00:18:19,710
and say you know we want to use ethical

00:18:17,010 --> 00:18:20,970
data principles and we would use ethical

00:18:19,710 --> 00:18:22,710
algorithms and we want to ensure that

00:18:20,970 --> 00:18:24,440
you do that as well so that we don't end

00:18:22,710 --> 00:18:27,539
up in a situation in which we've cut

00:18:24,440 --> 00:18:30,480
issues that we don't know about we're

00:18:27,539 --> 00:18:32,039
seeing much more education evangelism

00:18:30,480 --> 00:18:35,010
happen and so stuff like this right

00:18:32,039 --> 00:18:36,809
where we talk about it data and then a

00:18:35,010 --> 00:18:39,630
company called Dion has published a

00:18:36,809 --> 00:18:40,950
checklist for data scientists but one

00:18:39,630 --> 00:18:42,809
thing I do want to stress so there's a

00:18:40,950 --> 00:18:46,049
woman at Salesforce and Kathy Baxter

00:18:42,809 --> 00:18:49,380
who's a kind of an AI ethics and and

00:18:46,049 --> 00:18:50,460
user experience expert and she said

00:18:49,380 --> 00:18:51,990
something at Dreamforce I thought was

00:18:50,460 --> 00:18:55,140
really important she said you know

00:18:51,990 --> 00:18:58,200
ethics or ethics isn't a checklist it's

00:18:55,140 --> 00:18:59,610
a mindset yeah and I think I think you

00:18:58,200 --> 00:19:02,159
know so at the top we have the sort of

00:18:59,610 --> 00:19:04,370
mindset and and slow I wouldn't call it

00:19:02,159 --> 00:19:06,630
culture change but evolution of a

00:19:04,370 --> 00:19:08,190
reasonable discussions about ethics and

00:19:06,630 --> 00:19:10,200
then underneath that we have kind of

00:19:08,190 --> 00:19:12,120
principles practices and then kind of

00:19:10,200 --> 00:19:14,130
governance and there are pieces that are

00:19:12,120 --> 00:19:16,200
happening in all those areas but no real

00:19:14,130 --> 00:19:17,940
cohesive framework as it yet would it be

00:19:16,200 --> 00:19:21,299
fair to say that ethics is a team sport

00:19:17,940 --> 00:19:23,520
oh yeah it has to be right because you

00:19:21,299 --> 00:19:24,539
need the data scientist to understands

00:19:23,520 --> 00:19:26,429
the data you need the business

00:19:24,539 --> 00:19:28,169
stakeholder who understands the

00:19:26,429 --> 00:19:29,429
requirements of the business you need

00:19:28,169 --> 00:19:31,200
engineers who know what can be

00:19:29,429 --> 00:19:32,029
productized operations people who know

00:19:31,200 --> 00:19:33,919
what can

00:19:32,029 --> 00:19:36,350
you need to be able to figure out if the

00:19:33,919 --> 00:19:39,380
you know computation and and you know

00:19:36,350 --> 00:19:42,139
just computation actually makes sense

00:19:39,380 --> 00:19:43,880
you need to have people who can bring

00:19:42,139 --> 00:19:45,590
that to market and explain it you need

00:19:43,880 --> 00:19:48,049
to have compliance and legal you need to

00:19:45,590 --> 00:19:50,059
like all these different employees need

00:19:48,049 --> 00:19:52,760
to be on board I mean this is all over

00:19:50,059 --> 00:19:56,059
the enterprise and I mean Silicon Valley

00:19:52,760 --> 00:19:58,309
people love to debate about full-stack

00:19:56,059 --> 00:20:02,179
and use the word unicorn but this is

00:19:58,309 --> 00:20:04,580
ultimate this is an ultimate in terms of

00:20:02,179 --> 00:20:06,169
full-stack because it goes all the way

00:20:04,580 --> 00:20:08,090
from the base of where you're collecting

00:20:06,169 --> 00:20:10,159
data out to how your customers are

00:20:08,090 --> 00:20:11,269
interacting it's it's encompassing not

00:20:10,159 --> 00:20:13,460
this technology but also all the

00:20:11,269 --> 00:20:15,200
business and realistically if it's not

00:20:13,460 --> 00:20:18,200
done right the question is will you have

00:20:15,200 --> 00:20:20,149
customers and employees left over so you

00:20:18,200 --> 00:20:22,760
know imagine today you know when I used

00:20:20,149 --> 00:20:24,200
to when I when I used to like 18 months

00:20:22,760 --> 00:20:25,909
ago started doing these kinds of talks

00:20:24,200 --> 00:20:27,649
or 2 years ago I'd say raise your hand

00:20:25,909 --> 00:20:28,700
if you've interacted with AI today you

00:20:27,649 --> 00:20:30,380
know when I was talking to business

00:20:28,700 --> 00:20:31,820
audiences like a few hands with god

00:20:30,380 --> 00:20:33,200
they're like okay so raise your hand if

00:20:31,820 --> 00:20:34,700
you use Google razors and if you're on

00:20:33,200 --> 00:20:35,990
Facebook if you're on Twitter raise your

00:20:34,700 --> 00:20:37,730
hand if you texted somebody with

00:20:35,990 --> 00:20:39,110
predictive text raise your hand you know

00:20:37,730 --> 00:20:40,669
and then all of a sudden all the hands

00:20:39,110 --> 00:20:42,350
are pulling everybody is interacting

00:20:40,669 --> 00:20:46,130
with a I don't have to do that anymore

00:20:42,350 --> 00:20:48,980
and so now what's happening is that in

00:20:46,130 --> 00:20:50,510
terms of full stack people have an

00:20:48,980 --> 00:20:52,820
understanding that algorithms are

00:20:50,510 --> 00:20:55,159
influencing our lives but we're in 2018

00:20:52,820 --> 00:20:57,950
you know in five years in ten years

00:20:55,159 --> 00:20:59,990
imagine how many products and services

00:20:57,950 --> 00:21:02,480
are going to be fully did at all right

00:20:59,990 --> 00:21:05,149
now you have companies like Procter &

00:21:02,480 --> 00:21:07,789
Gamble with the oily skin advisor like

00:21:05,149 --> 00:21:10,250
that's a purely digital product for like

00:21:07,789 --> 00:21:11,990
100 plus year old company imagine now

00:21:10,250 --> 00:21:14,210
all these companies with digital product

00:21:11,990 --> 00:21:15,830
products and services and so the more

00:21:14,210 --> 00:21:17,330
that happens the more digital

00:21:15,830 --> 00:21:19,730
interactions we have the more weaning

00:21:17,330 --> 00:21:21,769
norms of behavior that actually support

00:21:19,730 --> 00:21:23,929
digital interactions so that we are

00:21:21,769 --> 00:21:26,570
delivering experiences that we want in

00:21:23,929 --> 00:21:29,120
digital spaces we know how to do that I

00:21:26,570 --> 00:21:31,429
mean say over all right in manufacturing

00:21:29,120 --> 00:21:32,960
and actual physical products that you

00:21:31,429 --> 00:21:34,700
can hold in your hand there you know how

00:21:32,960 --> 00:21:37,490
to publish a book you know how to make a

00:21:34,700 --> 00:21:39,440
car you know how to deal with safety now

00:21:37,490 --> 00:21:40,280
we have to do all those things except in

00:21:39,440 --> 00:21:44,180
digital

00:21:40,280 --> 00:21:46,070
and and one of the one of the stories

00:21:44,180 --> 00:21:48,470
coming out of the conference of a few

00:21:46,070 --> 00:21:51,290
videos we've done earlier today is just

00:21:48,470 --> 00:21:54,010
how much of the machine learning is

00:21:51,290 --> 00:21:57,320
moving out into the field into the edge

00:21:54,010 --> 00:21:58,910
small embedded devices where in some

00:21:57,320 --> 00:22:01,010
cases there's already billions of them

00:21:58,910 --> 00:22:03,890
out there waiting to be programmed and

00:22:01,010 --> 00:22:05,330
now very convenient ways for any

00:22:03,890 --> 00:22:07,880
developer to pop up and write something

00:22:05,330 --> 00:22:10,130
in Python and deploy it but those things

00:22:07,880 --> 00:22:12,710
are really autonomous and they're

00:22:10,130 --> 00:22:15,890
everywhere and it could be like you know

00:22:12,710 --> 00:22:17,570
every coffee machine and every door or

00:22:15,890 --> 00:22:19,160
anything that you interact with has

00:22:17,570 --> 00:22:20,840
something in there with machine learning

00:22:19,160 --> 00:22:24,350
models interacting yeah all four

00:22:20,840 --> 00:22:26,330
potentials for really bad cases of this

00:22:24,350 --> 00:22:27,920
done wrong yeah if we don't think it

00:22:26,330 --> 00:22:29,270
through I mean you know we just bought

00:22:27,920 --> 00:22:32,030
it at home and just bought a new

00:22:29,270 --> 00:22:33,710
dishwasher and my you know I get home

00:22:32,030 --> 00:22:38,360
and I'm like oh my god like there's

00:22:33,710 --> 00:22:39,650
Wi-Fi data are you collecting what are

00:22:38,360 --> 00:22:41,960
you doing with it are you learning

00:22:39,650 --> 00:22:45,290
something like I wouldn't know and so

00:22:41,960 --> 00:22:47,240
and so yes I mean there are tremendous

00:22:45,290 --> 00:22:49,910
opportunities for risk I will say though

00:22:47,240 --> 00:22:51,740
that one of the things I think is the

00:22:49,910 --> 00:22:53,540
most interesting is not necessarily

00:22:51,740 --> 00:22:55,810
approaching everything as miss critical

00:22:53,540 --> 00:22:58,820
risk mitigation oh good okay yeah right

00:22:55,810 --> 00:23:01,640
and I think risk mitigation clearly is

00:22:58,820 --> 00:23:03,320
important but what you know here's a

00:23:01,640 --> 00:23:06,350
thought experiment for you what happens

00:23:03,320 --> 00:23:08,660
if we can remove or mitigate or

00:23:06,350 --> 00:23:10,160
remediate bias what happens if we can

00:23:08,660 --> 00:23:12,560
figure out the interplay between

00:23:10,160 --> 00:23:14,450
precision and recall or you know

00:23:12,560 --> 00:23:16,190
we can optimize models to truly

00:23:14,450 --> 00:23:18,950
understand those trade-offs make

00:23:16,190 --> 00:23:20,930
decisions consider decisions what new

00:23:18,950 --> 00:23:23,420
product services business models

00:23:20,930 --> 00:23:25,910
experiences revenue streams can we

00:23:23,420 --> 00:23:29,390
actually create once we do that one

00:23:25,910 --> 00:23:30,800
great example is around skin cancer and

00:23:29,390 --> 00:23:32,480
there was a wonderful article in the

00:23:30,800 --> 00:23:35,030
Atlantic not too long ago that talked

00:23:32,480 --> 00:23:36,980
about the fact that physicians typically

00:23:35,030 --> 00:23:39,890
are not very good at diagnosing skin

00:23:36,980 --> 00:23:41,000
cancer in people of color there's a

00:23:39,890 --> 00:23:43,010
lower incidence but there's a higher

00:23:41,000 --> 00:23:44,510
mortality and maybe that I don't you

00:23:43,010 --> 00:23:45,620
know I'm not a physician but like I

00:23:44,510 --> 00:23:47,230
think it has to do with like later

00:23:45,620 --> 00:23:48,730
diagnosis and maybe there's some

00:23:47,230 --> 00:23:51,160
four kinds of variations and things like

00:23:48,730 --> 00:23:52,510
that but you know so you imagine that

00:23:51,160 --> 00:23:54,250
that's happening in the physical world

00:23:52,510 --> 00:23:56,500
and then when you translate that to an

00:23:54,250 --> 00:23:58,750
algorithm imagine how thin that data set

00:23:56,500 --> 00:24:01,210
is around skin cancer what a lesion

00:23:58,750 --> 00:24:03,309
looks like in somebody who has you know

00:24:01,210 --> 00:24:05,679
skin darker than a Caucasian person and

00:24:03,309 --> 00:24:07,090
then and so that what was interesting

00:24:05,679 --> 00:24:09,190
about it was the physician who was being

00:24:07,090 --> 00:24:11,860
interviewed said you know I don't think

00:24:09,190 --> 00:24:13,540
AI is bad I actually think you know

00:24:11,860 --> 00:24:16,570
we're out there trying to find to get a

00:24:13,540 --> 00:24:18,929
data set from you know from some other

00:24:16,570 --> 00:24:22,000
countries like in Africa and in South

00:24:18,929 --> 00:24:23,049
you know South India South Asia and

00:24:22,000 --> 00:24:26,080
Southeast Asia

00:24:23,049 --> 00:24:27,940
to try to ensure that we have enough

00:24:26,080 --> 00:24:29,620
data that we can now start to better

00:24:27,940 --> 00:24:32,799
understand the characteristics the

00:24:29,620 --> 00:24:35,350
attributes of skin cancer lesions in a

00:24:32,799 --> 00:24:37,540
rain you know and everybody and once we

00:24:35,350 --> 00:24:40,480
do that we have the ability to deliver

00:24:37,540 --> 00:24:42,340
low-cost digital diagnostic services to

00:24:40,480 --> 00:24:45,160
any community around the world and

00:24:42,340 --> 00:24:50,770
that's amazing right because you go from

00:24:45,160 --> 00:24:52,210
a place of a place of of want and of you

00:24:50,770 --> 00:24:54,040
know kind of discrimination or

00:24:52,210 --> 00:24:56,440
disenfranchisement that literally has

00:24:54,040 --> 00:24:57,880
mortal consequences to a place where now

00:24:56,440 --> 00:25:00,100
you can actually go out and save lives

00:24:57,880 --> 00:25:02,169
because you flip the script by simply

00:25:00,100 --> 00:25:04,390
knowing that you had a data issue by

00:25:02,169 --> 00:25:06,340
addressing that data issue now I'm not

00:25:04,390 --> 00:25:08,230
saying it's all fixed right but but the

00:25:06,340 --> 00:25:10,630
idea is what happens if you flip that

00:25:08,230 --> 00:25:12,700
script and you say if we were to solve

00:25:10,630 --> 00:25:13,990
this what would then be possible that's

00:25:12,700 --> 00:25:15,760
where I think all the innovation

00:25:13,990 --> 00:25:17,320
potential lies that's so that's the

00:25:15,760 --> 00:25:19,090
larger narrative that we really need to

00:25:17,320 --> 00:25:20,559
be using as a patterns than they were

00:25:19,090 --> 00:25:23,049
trying to say let's let's have a

00:25:20,559 --> 00:25:24,910
adoption in enterprise let's not just

00:25:23,049 --> 00:25:26,470
stop the quick fix let's actually look

00:25:24,910 --> 00:25:27,730
at right it's better right and you know

00:25:26,470 --> 00:25:29,950
if you want to look at a model for this

00:25:27,730 --> 00:25:32,380
look at how we dealt with gdpr in May

00:25:29,950 --> 00:25:34,000
yeah right that's great so it's a it's a

00:25:32,380 --> 00:25:37,030
miserable experience right it's a

00:25:34,000 --> 00:25:41,200
miserable expect now I will say that the

00:25:37,030 --> 00:25:43,780
intent behind gdpr which comes out of

00:25:41,200 --> 00:25:45,549
the UN Declaration of Human Rights the

00:25:43,780 --> 00:25:47,950
intent is to give people control of

00:25:45,549 --> 00:25:49,809
their data and to essentially enable

00:25:47,950 --> 00:25:51,549
people to make decisions based on their

00:25:49,809 --> 00:25:52,030
data that affect their rights and their

00:25:51,549 --> 00:25:56,230
safety

00:25:52,030 --> 00:25:59,230
it's a beautiful thing what did it mean

00:25:56,230 --> 00:26:00,519
in practice a whole lot of really really

00:25:59,230 --> 00:26:02,619
annoying emails

00:26:00,519 --> 00:26:04,330
websites we couldn't reach so yes

00:26:02,619 --> 00:26:07,779
there's always that first very clunky

00:26:04,330 --> 00:26:09,729
ugly implementation that everybody hates

00:26:07,779 --> 00:26:14,469
and we're in that we're in that phase

00:26:09,729 --> 00:26:16,089
right now of ugly clunky stuff but you

00:26:14,469 --> 00:26:18,489
know one of the things that I think is

00:26:16,089 --> 00:26:20,440
it's kind of the the bright spot here is

00:26:18,489 --> 00:26:22,269
a comment that Richard so sure made

00:26:20,440 --> 00:26:24,909
actually a dream for us where he talked

00:26:22,269 --> 00:26:26,499
about the beginnings of the you know the

00:26:24,909 --> 00:26:29,469
invention of the internal combustion

00:26:26,499 --> 00:26:31,359
engine and the comment that he made was

00:26:29,469 --> 00:26:32,619
you know if you if you go back to the

00:26:31,359 --> 00:26:34,690
beginning of the internal combustion

00:26:32,619 --> 00:26:36,459
engine if people had had a conversation

00:26:34,690 --> 00:26:40,029
about what are the downstream

00:26:36,459 --> 00:26:41,769
implications of all this coal use of all

00:26:40,029 --> 00:26:43,690
of this you know drilling for oil

00:26:41,769 --> 00:26:46,599
imagine that this technology now

00:26:43,690 --> 00:26:48,399
proliferates around the entire world and

00:26:46,599 --> 00:26:50,889
that the population increases what will

00:26:48,399 --> 00:26:52,959
it look like in 10 20 or 50 years and

00:26:50,889 --> 00:26:54,849
that could have changed where we are

00:26:52,959 --> 00:26:57,159
today in terms of environmental

00:26:54,849 --> 00:26:59,709
sustainability and so his point was that

00:26:57,159 --> 00:27:01,959
this is kind of where we are with AI and

00:26:59,709 --> 00:27:04,690
we're having this conversation now and I

00:27:01,959 --> 00:27:06,940
think that's a really important way of

00:27:04,690 --> 00:27:08,859
looking at this that like number one

00:27:06,940 --> 00:27:11,079
it's not just about risk mitigation it's

00:27:08,859 --> 00:27:12,729
about opportunity creation and number

00:27:11,079 --> 00:27:14,589
two we're at the beginning so we

00:27:12,729 --> 00:27:16,539
actually this is our responsibility the

00:27:14,589 --> 00:27:18,999
onus is on us to have harder

00:27:16,539 --> 00:27:21,219
conversations about awkward things so

00:27:18,999 --> 00:27:23,679
that we can ensure that the next

00:27:21,219 --> 00:27:25,599
generation exactly is it living with you

00:27:23,679 --> 00:27:27,669
know essentially a toxic digital

00:27:25,599 --> 00:27:29,649
environment generational impact that

00:27:27,669 --> 00:27:32,259
that makes so much sense the automobiles

00:27:29,649 --> 00:27:33,669
started what four generations ago and

00:27:32,259 --> 00:27:35,349
now we're now we're having to make

00:27:33,669 --> 00:27:38,829
tourism and it's obvious now right but

00:27:35,349 --> 00:27:40,239
it wasn't obvious then very good Thank

00:27:38,829 --> 00:27:42,539
You Susan thank you so much yeah it was

00:27:40,239 --> 00:27:42,539

YouTube URL: https://www.youtube.com/watch?v=WEEbtQjHOKk


