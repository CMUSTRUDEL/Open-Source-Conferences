Title: Using Amazon SageMaker  to develop and deploy ML models with Ian Massingham (AWS)
Publication date: 2018-10-17
Playlist: Artificial Intelligence Conference 2018 - London, United Kingdom
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:05,100
hi I'm Paco Nathan with O'Reilly Media

00:00:02,490 --> 00:00:08,940
and it's pleasure today to get to speak

00:00:05,100 --> 00:00:14,280
within massing ham from AWS technical

00:00:08,940 --> 00:00:17,120
developer evangelism at AWS really

00:00:14,280 --> 00:00:17,120
appreciated keynote this morning

00:00:19,039 --> 00:00:23,960
amazing amazing performance to be able

00:00:21,359 --> 00:00:26,820
to get that concentration of information

00:00:23,960 --> 00:00:30,330
can you so perhaps we could embellish a

00:00:26,820 --> 00:00:32,520
bit on that here how is it that the

00:00:30,330 --> 00:00:34,260
cloud enables innovation in they are

00:00:32,520 --> 00:00:36,300
yeah there's natural affinity between

00:00:34,260 --> 00:00:39,120
the characteristics of cloud computing

00:00:36,300 --> 00:00:40,379
services like AWS and AI workloads and

00:00:39,120 --> 00:00:42,780
there's a few different dimensions to it

00:00:40,379 --> 00:00:45,390
the first is of course the AI is fueled

00:00:42,780 --> 00:00:47,640
by data and to really run very accurate

00:00:45,390 --> 00:00:49,170
and successful AI model development you

00:00:47,640 --> 00:00:51,719
need large amounts of data which is

00:00:49,170 --> 00:00:53,760
quickly annotated and we find that

00:00:51,719 --> 00:00:56,190
customers tend to create and store lots

00:00:53,760 --> 00:00:57,899
of new data within AWS we have a service

00:00:56,190 --> 00:00:59,550
you probably familiar with called Amazon

00:00:57,899 --> 00:01:01,379
s3 which is a high volume high

00:00:59,550 --> 00:01:03,960
durability globally distributed storage

00:01:01,379 --> 00:01:05,880
service and we also also have a lot of

00:01:03,960 --> 00:01:08,400
ingest mechanisms to help customers

00:01:05,880 --> 00:01:10,830
aggregate and consolidate data on s3 so

00:01:08,400 --> 00:01:13,049
you can grab machine data using our iot

00:01:10,830 --> 00:01:15,090
service or telemetry data from other

00:01:13,049 --> 00:01:17,580
applications using services like Amazon

00:01:15,090 --> 00:01:19,290
Kinesis collect your data within s3 at

00:01:17,580 --> 00:01:20,909
scale and then you can apply actually

00:01:19,290 --> 00:01:23,280
different types of computational

00:01:20,909 --> 00:01:25,350
workload theory it might be analytics it

00:01:23,280 --> 00:01:27,180
might be to put an API in front of your

00:01:25,350 --> 00:01:29,159
data and make it available as a product

00:01:27,180 --> 00:01:31,110
or it might be machine learning for

00:01:29,159 --> 00:01:33,180
artificial intelligence apps so once

00:01:31,110 --> 00:01:35,640
your data is there you can apply mass

00:01:33,180 --> 00:01:37,320
scale compute to it to build models and

00:01:35,640 --> 00:01:39,150
then on the model training side of

00:01:37,320 --> 00:01:41,310
things that is again a very

00:01:39,150 --> 00:01:42,630
computationally intensive task and it's

00:01:41,310 --> 00:01:45,299
something that customers do not do

00:01:42,630 --> 00:01:47,490
continuously you want to apply massive

00:01:45,299 --> 00:01:50,100
amounts of computing power to a data set

00:01:47,490 --> 00:01:51,450
do that repeatedly until you get a model

00:01:50,100 --> 00:01:53,460
which you consider to have a high level

00:01:51,450 --> 00:01:55,710
of efficacy very effective model and

00:01:53,460 --> 00:01:58,049
then deploy that out into production for

00:01:55,710 --> 00:02:00,149
inference and it's that spike of

00:01:58,049 --> 00:02:02,250
activity that's associated with model

00:02:00,149 --> 00:02:04,049
training that is really well served by

00:02:02,250 --> 00:02:05,969
high performance high-volume cloud

00:02:04,049 --> 00:02:08,310
computing resources so there's affinity

00:02:05,969 --> 00:02:10,259
there and then in the inference stage

00:02:08,310 --> 00:02:11,410
when you've got your model you're

00:02:10,259 --> 00:02:13,060
deploying it and you

00:02:11,410 --> 00:02:16,060
to use it for making predictions about

00:02:13,060 --> 00:02:18,970
the world based on new data samples here

00:02:16,060 --> 00:02:20,890
you need durability elasticity and a low

00:02:18,970 --> 00:02:22,630
cost and then characteristics of the

00:02:20,890 --> 00:02:24,940
cloud you'll know very well aligned with

00:02:22,630 --> 00:02:27,250
those particular characteristics so you

00:02:24,940 --> 00:02:29,740
can deploy inference work put workloads

00:02:27,250 --> 00:02:32,200
behind endpoints you can elastically

00:02:29,740 --> 00:02:33,400
scale those endpoints based on the

00:02:32,200 --> 00:02:35,470
amount of traffic that you have at any

00:02:33,400 --> 00:02:37,210
particular point in the day and you can

00:02:35,470 --> 00:02:38,950
do that in a way which is secured as

00:02:37,210 --> 00:02:40,570
well by using appropriate role place

00:02:38,950 --> 00:02:42,310
access control for your inference

00:02:40,570 --> 00:02:44,500
endpoints so the cloud is a really good

00:02:42,310 --> 00:02:47,020
place to run the three phases in the ML

00:02:44,500 --> 00:02:49,080
workload cycle data collection training

00:02:47,020 --> 00:02:51,880
and then longer-term influence workloads

00:02:49,080 --> 00:02:53,860
fantastic some of the examples that I

00:02:51,880 --> 00:02:56,830
believe showing earlier or what we're

00:02:53,860 --> 00:02:58,300
showing earlier with AWS is collecting

00:02:56,830 --> 00:02:59,950
the data it could even be an effective

00:02:58,300 --> 00:03:00,490
floor with robots I mean all the way out

00:02:59,950 --> 00:03:02,770
of the edge

00:03:00,490 --> 00:03:05,680
absolutely we have a service called AWS

00:03:02,770 --> 00:03:08,260
Greengrass which is a IOT operating

00:03:05,680 --> 00:03:10,510
environment you can deploy that out onto

00:03:08,260 --> 00:03:12,760
gateways or edge devices and use that to

00:03:10,510 --> 00:03:13,960
collect data you can also do inference

00:03:12,760 --> 00:03:15,850
there as well so you can actually run

00:03:13,960 --> 00:03:17,680
machine learning out at the edge edge of

00:03:15,850 --> 00:03:20,170
the network on these edge devices as

00:03:17,680 --> 00:03:22,209
well and we have a data ingest service

00:03:20,170 --> 00:03:23,830
called AWS IOT which is a simpler

00:03:22,209 --> 00:03:27,280
mechanism for collecting machine data

00:03:23,830 --> 00:03:28,750
from robots or the MCU or

00:03:27,280 --> 00:03:30,070
microcontroller powered hardware that

00:03:28,750 --> 00:03:31,900
you might have out in a factory floor

00:03:30,070 --> 00:03:35,080
for example so we have services for the

00:03:31,900 --> 00:03:37,030
edge as well how is that the AWS

00:03:35,080 --> 00:03:39,490
differentiates I mean now there's more

00:03:37,030 --> 00:03:41,709
cloud providers so we operate our

00:03:39,490 --> 00:03:43,900
machine learning services three

00:03:41,709 --> 00:03:46,270
distinctive levels within the stack okay

00:03:43,900 --> 00:03:49,180
so the first is in the frameworks layer

00:03:46,270 --> 00:03:52,270
so tools like tensorflow MX net and pi

00:03:49,180 --> 00:03:54,370
torch for example we want to make AWS a

00:03:52,270 --> 00:03:56,950
really effective place for developers

00:03:54,370 --> 00:03:59,110
and research scientists to run whichever

00:03:56,950 --> 00:04:02,500
framework they wish so part of our

00:03:59,110 --> 00:04:04,690
offering is about openness we provide a

00:04:02,500 --> 00:04:06,430
computing platform called the p3

00:04:04,690 --> 00:04:08,590
instance family which includes

00:04:06,430 --> 00:04:11,770
high-performance NVIDIA GPU fbu

00:04:08,590 --> 00:04:13,120
technology and we have something to make

00:04:11,770 --> 00:04:15,340
that more accessible to developers

00:04:13,120 --> 00:04:17,770
called the deep learning army which is a

00:04:15,340 --> 00:04:18,910
prepackaged gold build of an operating

00:04:17,770 --> 00:04:20,769
system environment

00:04:18,910 --> 00:04:23,320
that includes all of the common deep

00:04:20,769 --> 00:04:25,660
learning framework tools we maintain it

00:04:23,320 --> 00:04:27,970
we include things like the cuda drivers

00:04:25,660 --> 00:04:29,650
cuda layer which makes the FPU hardware

00:04:27,970 --> 00:04:31,540
accessible to deep learning developers

00:04:29,650 --> 00:04:34,500
in the framework that they wish to use

00:04:31,540 --> 00:04:36,880
and that is a really simple process of

00:04:34,500 --> 00:04:38,830
access for developers spin up the

00:04:36,880 --> 00:04:39,940
machine select the deep learning army

00:04:38,830 --> 00:04:42,430
and in a few minutes you'll have a

00:04:39,940 --> 00:04:44,050
complete tools environment for you to

00:04:42,430 --> 00:04:45,910
use which is open and includes many

00:04:44,050 --> 00:04:48,250
different framework components so that's

00:04:45,910 --> 00:04:49,930
at the bottom level the next thing is in

00:04:48,250 --> 00:04:52,930
the platform here we have a specific

00:04:49,930 --> 00:04:55,420
service called Amazon sage maker it's a

00:04:52,930 --> 00:04:57,910
tool chain for developing machine

00:04:55,420 --> 00:05:00,580
learning models and for deploying them

00:04:57,910 --> 00:05:03,040
for production or testing use so you can

00:05:00,580 --> 00:05:04,930
prepare your data inside Jupiter

00:05:03,040 --> 00:05:06,790
notebooks the common interactive

00:05:04,930 --> 00:05:09,310
environment that Python developers often

00:05:06,790 --> 00:05:11,050
use in the data science context you can

00:05:09,310 --> 00:05:12,670
train models of scale using these

00:05:11,050 --> 00:05:14,140
Elastic Compute capabilities that i

00:05:12,670 --> 00:05:16,300
talked about and then you can deploy

00:05:14,140 --> 00:05:18,370
them for low-cost manageable inference

00:05:16,300 --> 00:05:21,070
with the elasticity and auto scaling so

00:05:18,370 --> 00:05:23,830
and sage maker includes over 14 built-in

00:05:21,070 --> 00:05:24,400
algorithms for things like factorization

00:05:23,830 --> 00:05:27,010
machines

00:05:24,400 --> 00:05:28,840
x-g boost sequence to sequence all kinds

00:05:27,010 --> 00:05:30,250
of different commonly used ml algorithms

00:05:28,840 --> 00:05:32,440
are built into the service so that's a

00:05:30,250 --> 00:05:34,090
tool that is intended to make it really

00:05:32,440 --> 00:05:36,940
simple for software developers that

00:05:34,090 --> 00:05:39,340
might not be machine learning experts to

00:05:36,940 --> 00:05:42,340
train and productionize ml models at

00:05:39,340 --> 00:05:44,650
scale it's also modular so you can bring

00:05:42,340 --> 00:05:45,820
in a model from outside and run it on

00:05:44,650 --> 00:05:47,800
this edge make an end point for

00:05:45,820 --> 00:05:49,840
inference you can train models with

00:05:47,800 --> 00:05:51,520
Serge maker and take them out and use

00:05:49,840 --> 00:05:53,470
them elsewhere you can even deploy them

00:05:51,520 --> 00:05:54,850
onto those IOT endpoints that I talked

00:05:53,470 --> 00:05:56,800
about directly from within the service

00:05:54,850 --> 00:05:58,750
or you can just use the jupiter notebook

00:05:56,800 --> 00:06:00,820
or you can enter at any stage in the

00:05:58,750 --> 00:06:02,080
lifecycle so it's an open platform for

00:06:00,820 --> 00:06:03,820
machine learning model development

00:06:02,080 --> 00:06:07,300
that's in the second layer of platforms

00:06:03,820 --> 00:06:08,650
and then at the top of the stack there's

00:06:07,300 --> 00:06:10,390
a whole category of software developers

00:06:08,650 --> 00:06:12,400
that don't want to know about everything

00:06:10,390 --> 00:06:14,440
that I've talked about so far just one

00:06:12,400 --> 00:06:16,480
complete simplicity so here we have a

00:06:14,440 --> 00:06:18,430
set of services for things like image

00:06:16,480 --> 00:06:20,860
and video analysis for natural language

00:06:18,430 --> 00:06:23,560
processing for topic extraction topic

00:06:20,860 --> 00:06:24,730
modeling sentiment analysis and also for

00:06:23,560 --> 00:06:26,680
helping developers to build

00:06:24,730 --> 00:06:28,810
conversational interfaces or chat BOTS

00:06:26,680 --> 00:06:30,040
as well so we're also in very

00:06:28,810 --> 00:06:32,290
than in building services where

00:06:30,040 --> 00:06:34,150
simplicity is the priority it really

00:06:32,290 --> 00:06:37,690
makes it easy for front-end or mobile

00:06:34,150 --> 00:06:40,270
app developers to incorporate AI feature

00:06:37,690 --> 00:06:41,860
so human-like capabilities directly into

00:06:40,270 --> 00:06:44,139
applications and what some of the covers

00:06:41,860 --> 00:06:46,510
they of course are pre-trained models of

00:06:44,139 --> 00:06:48,460
various types that we maintain develop

00:06:46,510 --> 00:06:50,530
and enhance and developers plug into

00:06:48,460 --> 00:06:53,680
your viral API so I would say what makes

00:06:50,530 --> 00:06:55,690
AWS different is having really coherent

00:06:53,680 --> 00:06:57,490
offerings every level of the stack that

00:06:55,690 --> 00:07:00,570
serve the different developer personas

00:06:57,490 --> 00:07:03,190
that exist in the AI nml space fantastic

00:07:00,570 --> 00:07:05,200
for example at the the top level the

00:07:03,190 --> 00:07:07,510
stack we see so much in computer vision

00:07:05,200 --> 00:07:08,350
in terms of deep learning so much of

00:07:07,510 --> 00:07:10,210
what will be shown here at the

00:07:08,350 --> 00:07:11,950
conference and there is a service

00:07:10,210 --> 00:07:14,169
specifically for that yes we have Amazon

00:07:11,950 --> 00:07:16,060
recognition so it's a machine learning

00:07:14,169 --> 00:07:18,790
service intended for static image

00:07:16,060 --> 00:07:21,100
analysis so non moving pictures I can

00:07:18,790 --> 00:07:23,620
submit my encoded PNG or JPEG to the

00:07:21,100 --> 00:07:26,110
endpoint specify a particular method

00:07:23,620 --> 00:07:28,450
like detect faces or facial analysis or

00:07:26,110 --> 00:07:30,490
facial comparison or scene or image

00:07:28,450 --> 00:07:33,490
classification and I'll receive back a

00:07:30,490 --> 00:07:35,229
documented format metadata structure

00:07:33,490 --> 00:07:38,380
they includes information about what's

00:07:35,229 --> 00:07:40,360
in the image it's a man of 40 to 55

00:07:38,380 --> 00:07:43,000
years of age he looks happy he is

00:07:40,360 --> 00:07:44,680
smiling for example you'll get back in a

00:07:43,000 --> 00:07:46,419
JSON structure in a metadata structure

00:07:44,680 --> 00:07:48,190
which contains metadata about what's in

00:07:46,419 --> 00:07:51,610
the image that's one thing and then the

00:07:48,190 --> 00:07:53,979
second is recognition video very similar

00:07:51,610 --> 00:07:56,160
but it operates over time series images

00:07:53,979 --> 00:07:58,630
in a video stream so it can not only do

00:07:56,160 --> 00:08:00,430
analysis of each frame to say there's a

00:07:58,630 --> 00:08:02,620
50 year old or 45 year old man in the

00:08:00,430 --> 00:08:04,539
frame or 30 or gone in the frame it can

00:08:02,620 --> 00:08:07,389
also track that person as they move

00:08:04,539 --> 00:08:09,070
around and it can detect activities that

00:08:07,389 --> 00:08:10,660
person is walking that person is running

00:08:09,070 --> 00:08:12,000
that person who's jumping these are

00:08:10,660 --> 00:08:15,370
things that you can obviously only

00:08:12,000 --> 00:08:17,229
capture and analyze with accuracy if you

00:08:15,370 --> 00:08:19,030
consider the time dimension in the data

00:08:17,229 --> 00:08:20,680
stream so that dynamics of the narrative

00:08:19,030 --> 00:08:21,729
yeah exactly right so we have that I

00:08:20,680 --> 00:08:24,310
would say that another incredibly

00:08:21,729 --> 00:08:26,110
popular area for these kind of highly

00:08:24,310 --> 00:08:28,090
packaged simple to access services in

00:08:26,110 --> 00:08:30,460
language though so image recognition is

00:08:28,090 --> 00:08:31,930
substantial but language analysis and

00:08:30,460 --> 00:08:34,300
speech generation are also really

00:08:31,930 --> 00:08:35,979
substantial service areas certainly for

00:08:34,300 --> 00:08:37,360
commercial applications is yes yeah for

00:08:35,979 --> 00:08:39,130
commercial applications for training

00:08:37,360 --> 00:08:41,750
applications even for creative use cases

00:08:39,130 --> 00:08:44,120
where I want to voice an audio book or

00:08:41,750 --> 00:08:46,250
create a video that has a creative

00:08:44,120 --> 00:08:48,350
narrative of spoken English or another

00:08:46,250 --> 00:08:49,699
language on top of it we have a service

00:08:48,350 --> 00:08:51,680
called Amazon poly that can generate

00:08:49,699 --> 00:08:53,980
lifelike speech in over 27 different

00:08:51,680 --> 00:08:56,480
languages so you can voice your video

00:08:53,980 --> 00:08:57,920
with creative voicing characteristics

00:08:56,480 --> 00:08:59,569
without needing to employ a voice actor

00:08:57,920 --> 00:09:02,420
Wow

00:08:59,569 --> 00:09:04,189
when I first started using AWS over a

00:09:02,420 --> 00:09:06,079
decade ago I remember the initial

00:09:04,189 --> 00:09:08,240
release there may have been less than a

00:09:06,079 --> 00:09:10,610
half-dozen different services and I've

00:09:08,240 --> 00:09:13,069
watched this just escalate to the point

00:09:10,610 --> 00:09:15,470
now where it takes pages in very small

00:09:13,069 --> 00:09:18,019
font size to see all the listen we have

00:09:15,470 --> 00:09:19,879
over 125 different services and we have

00:09:18,019 --> 00:09:22,009
search functionality in the console so

00:09:19,879 --> 00:09:23,660
that you can organize them so when you

00:09:22,009 --> 00:09:25,850
go to the AWS console now you can type

00:09:23,660 --> 00:09:27,230
in you know recognition for example

00:09:25,850 --> 00:09:30,319
spelt with a K by the way recognition

00:09:27,230 --> 00:09:31,550
with a K and the recognition console

00:09:30,319 --> 00:09:33,019
will pop up for you and you can then

00:09:31,550 --> 00:09:34,550
jump off into that and start to

00:09:33,019 --> 00:09:36,410
experiment with or configure the service

00:09:34,550 --> 00:09:37,850
so we had to restructure our console

00:09:36,410 --> 00:09:40,699
because we were having a proliferation

00:09:37,850 --> 00:09:43,189
of services in this way well we'll even

00:09:40,699 --> 00:09:46,970
with that many how is Amazon

00:09:43,189 --> 00:09:49,309
accelerating that pace so I would say

00:09:46,970 --> 00:09:51,079
that AWS and Amazon more broadly are

00:09:49,309 --> 00:09:53,269
organized very specifically for

00:09:51,079 --> 00:09:55,399
innovation velocity so when we create

00:09:53,269 --> 00:09:58,309
new services we tend to create brand new

00:09:55,399 --> 00:10:00,410
teams for those new services and we have

00:09:58,309 --> 00:10:02,449
what's best described as a loosely

00:10:00,410 --> 00:10:04,519
coupled architecture for the business so

00:10:02,449 --> 00:10:06,860
these teams operate independently as one

00:10:04,519 --> 00:10:08,500
another they're small in size in fact we

00:10:06,860 --> 00:10:10,519
call them two pizza teams

00:10:08,500 --> 00:10:12,620
yeah two bits of teams we try to

00:10:10,519 --> 00:10:13,819
constrain them to a size such that they

00:10:12,620 --> 00:10:15,319
can be fed with a couple of large

00:10:13,819 --> 00:10:17,209
American pizzas you know it's not a

00:10:15,319 --> 00:10:18,949
strict rule of thumb but the kind of

00:10:17,209 --> 00:10:20,509
guide is if your team needs more than

00:10:18,949 --> 00:10:21,829
that to feed for dinner or lunch then

00:10:20,509 --> 00:10:23,300
it's probably too big and you're

00:10:21,829 --> 00:10:25,670
starting to introduce a communications

00:10:23,300 --> 00:10:27,620
overhead so we tend to spin up lots of

00:10:25,670 --> 00:10:29,389
teams to work on lots of different

00:10:27,620 --> 00:10:31,370
things concurrently which means we can

00:10:29,389 --> 00:10:33,170
innovate on a lot of different fronts at

00:10:31,370 --> 00:10:35,089
the same time and we're also not afraid

00:10:33,170 --> 00:10:37,220
of duplication we can have services that

00:10:35,089 --> 00:10:38,720
perform similar functions as long as

00:10:37,220 --> 00:10:41,059
those services have customers that

00:10:38,720 --> 00:10:43,069
regard those particular functions as

00:10:41,059 --> 00:10:44,540
valuable for them we're quite happy to

00:10:43,069 --> 00:10:47,029
have multiple services that do similar

00:10:44,540 --> 00:10:48,350
things to enable us to make sure that

00:10:47,029 --> 00:10:49,880
we're covering all of the different use

00:10:48,350 --> 00:10:51,630
case requirements that our customers may

00:10:49,880 --> 00:10:55,170
have

00:10:51,630 --> 00:10:57,840
now about Amazon sage maker how is this

00:10:55,170 --> 00:11:01,170
that we were talking a little bit before

00:10:57,840 --> 00:11:03,260
about being able to import or export can

00:11:01,170 --> 00:11:06,390
you go in a bit more detail about that

00:11:03,260 --> 00:11:08,880
so within sage maker when you execute a

00:11:06,390 --> 00:11:11,430
training job what you are running is an

00:11:08,880 --> 00:11:13,680
open source docker container that

00:11:11,430 --> 00:11:15,660
contains the training algorithm okay and

00:11:13,680 --> 00:11:18,180
it's actually a pluggable architecture

00:11:15,660 --> 00:11:19,370
so we ship these built-in models as I've

00:11:18,180 --> 00:11:21,810
talked about already for things like

00:11:19,370 --> 00:11:24,780
gradient boosted trees or for sequence

00:11:21,810 --> 00:11:25,860
the sequence or for factorization

00:11:24,780 --> 00:11:28,170
machines that you might use any

00:11:25,860 --> 00:11:29,340
recommendations engine for example but

00:11:28,170 --> 00:11:31,470
you could also bring in your own

00:11:29,340 --> 00:11:33,300
containers so you can package up your

00:11:31,470 --> 00:11:35,520
own docker containers that might contain

00:11:33,300 --> 00:11:37,560
your own training algorithms and you can

00:11:35,520 --> 00:11:39,390
run those on AWS and this modular

00:11:37,560 --> 00:11:41,550
pluggable architecture allows us to

00:11:39,390 --> 00:11:43,680
bring very quick support for new tools

00:11:41,550 --> 00:11:45,870
that become available we just announced

00:11:43,680 --> 00:11:48,000
a new version of Pi torch for example as

00:11:45,870 --> 00:11:50,040
availability is having availability

00:11:48,000 --> 00:11:51,660
within sage maker by plugging in a new

00:11:50,040 --> 00:11:53,730
docker container that contains that

00:11:51,660 --> 00:11:56,880
particular training algorithm so you'll

00:11:53,730 --> 00:11:59,880
create models that are appropriate for

00:11:56,880 --> 00:12:02,310
whatever algorithm you're using to train

00:11:59,880 --> 00:12:03,750
and you can then take those out ok and

00:12:02,310 --> 00:12:06,390
then we provide another set of docker

00:12:03,750 --> 00:12:08,790
containers which represent our inference

00:12:06,390 --> 00:12:10,680
endpoint work lint containers so you

00:12:08,790 --> 00:12:13,920
could take those docker containers and

00:12:10,680 --> 00:12:16,320
run those independently of AWS on your

00:12:13,920 --> 00:12:17,700
own docker based infrastructure external

00:12:16,320 --> 00:12:20,220
to the cloud so this is how the

00:12:17,700 --> 00:12:21,660
portability works with container based

00:12:20,220 --> 00:12:24,380
training and the container based

00:12:21,660 --> 00:12:27,060
inference and at each point also

00:12:24,380 --> 00:12:28,380
leveraging open source best-of-breed

00:12:27,060 --> 00:12:30,180
coming out of absolutely well have

00:12:28,380 --> 00:12:31,530
provide developers with flexibility to

00:12:30,180 --> 00:12:33,810
use the tools that they're comfortable

00:12:31,530 --> 00:12:36,900
with and also recognize the fact that

00:12:33,810 --> 00:12:38,820
not every deep learning or statistical

00:12:36,900 --> 00:12:40,920
learning use case is necessarily

00:12:38,820 --> 00:12:42,630
serviced most effectively by the same

00:12:40,920 --> 00:12:44,580
framework the idea that you'll have one

00:12:42,630 --> 00:12:46,980
kind of Swiss Army knife for machine

00:12:44,580 --> 00:12:48,960
learning which solves all use cases not

00:12:46,980 --> 00:12:50,550
that accurate we find that developers

00:12:48,960 --> 00:12:52,050
working in different domains have

00:12:50,550 --> 00:12:53,370
different needs and want to use

00:12:52,050 --> 00:12:55,650
different frameworks so we want to

00:12:53,370 --> 00:12:57,200
support everything that developers see

00:12:55,650 --> 00:13:00,770
as valuable and is popular in the

00:12:57,200 --> 00:13:03,440
the community we were talking about

00:13:00,770 --> 00:13:05,240
factory floor and robots and things can

00:13:03,440 --> 00:13:06,890
you describe some other interesting use

00:13:05,240 --> 00:13:08,810
cases for how people are leveraging

00:13:06,890 --> 00:13:10,910
these ml services yeah absolutely so

00:13:08,810 --> 00:13:12,620
within Amazon we have several use cases

00:13:10,910 --> 00:13:14,330
in our own right we have recommender

00:13:12,620 --> 00:13:15,530
systems on the Amazon website which

00:13:14,330 --> 00:13:17,030
you'll be super familiar with pretty

00:13:15,530 --> 00:13:18,950
much advocate every customer that's ever

00:13:17,030 --> 00:13:20,510
used Amazon has seen or used that

00:13:18,950 --> 00:13:21,860
service recommended for you and

00:13:20,510 --> 00:13:24,530
customers who bought this also bought

00:13:21,860 --> 00:13:25,940
two different categories that pop up on

00:13:24,530 --> 00:13:28,510
the Amazon website when you when you

00:13:25,940 --> 00:13:31,340
visit show of hands who's used that yeah

00:13:28,510 --> 00:13:33,950
personally I get a lot of girls

00:13:31,340 --> 00:13:36,020
gymnastics equipment recommend anything

00:13:33,950 --> 00:13:37,340
wait year old is a fan of gym and by her

00:13:36,020 --> 00:13:39,200
quite a lot of replacement gear she

00:13:37,340 --> 00:13:40,340
seems to wear it out so my purchase

00:13:39,200 --> 00:13:41,360
history is seeded with that kind of

00:13:40,340 --> 00:13:44,120
stuff which is quite interesting you can

00:13:41,360 --> 00:13:45,830
see it works and then we have Amazon go

00:13:44,120 --> 00:13:47,510
which is our checkout less retail

00:13:45,830 --> 00:13:49,060
experience opened in Seattle now for

00:13:47,510 --> 00:13:52,310
several months that allows customers to

00:13:49,060 --> 00:13:54,950
purchase products inside a convenience

00:13:52,310 --> 00:13:57,890
store convenience to store

00:13:54,950 --> 00:13:59,660
style format without having to visit a

00:13:57,890 --> 00:14:02,120
cashier or visit a checkout just walk in

00:13:59,660 --> 00:14:03,470
tap your app on the reader walk into the

00:14:02,120 --> 00:14:05,600
store pick up the product and walk out

00:14:03,470 --> 00:14:07,130
and we use machine vision and sensor

00:14:05,600 --> 00:14:08,750
fusion in the built environment to

00:14:07,130 --> 00:14:10,700
figure out accurately what you from me

00:14:08,750 --> 00:14:12,710
removed and charged you for just a few

00:14:10,700 --> 00:14:14,510
seconds after you left the store that's

00:14:12,710 --> 00:14:15,980
a use case that we have and then in

00:14:14,510 --> 00:14:18,440
Amazon Fulfillment in our fulfillment

00:14:15,980 --> 00:14:20,240
centers we do use robotics ourselves for

00:14:18,440 --> 00:14:22,070
automated materials handling and

00:14:20,240 --> 00:14:23,840
inventory management so you'll see we

00:14:22,070 --> 00:14:26,510
have large fleets of robotic systems

00:14:23,840 --> 00:14:29,230
inside our newer fulfillment centers

00:14:26,510 --> 00:14:31,550
that are performing stock management

00:14:29,230 --> 00:14:33,260
tasks and are bringing inventory to our

00:14:31,550 --> 00:14:34,550
pic workers and actually take the

00:14:33,260 --> 00:14:36,590
product that you might have purchased on

00:14:34,550 --> 00:14:38,270
Amazon and play Cena in a box for

00:14:36,590 --> 00:14:40,550
dispatch so those are all internal use

00:14:38,270 --> 00:14:42,830
cases and then looking out into the

00:14:40,550 --> 00:14:46,280
customer community we have customers

00:14:42,830 --> 00:14:47,810
like Intuit digital globe Zillow here in

00:14:46,280 --> 00:14:49,340
the UK where we're speaking today we

00:14:47,810 --> 00:14:51,140
have the Royal National Institute for

00:14:49,340 --> 00:14:54,110
the blind as a customer they are using

00:14:51,140 --> 00:14:56,120
Amazon Pali to voice audio content for

00:14:54,110 --> 00:14:58,760
people who have visual impairments so in

00:14:56,120 --> 00:15:02,090
the pre deep learning speech generation

00:14:58,760 --> 00:15:04,460
days short run periodic publications

00:15:02,090 --> 00:15:06,740
like magazines they may well never have

00:15:04,460 --> 00:15:09,110
been transcribed into audio you know the

00:15:06,740 --> 00:15:10,970
audience was small the lifespan of the

00:15:09,110 --> 00:15:12,680
publication was relatively short

00:15:10,970 --> 00:15:14,959
might not have made economic sense to

00:15:12,680 --> 00:15:17,740
have a voice active voice a piece of

00:15:14,959 --> 00:15:20,029
content like that with Amazon poly

00:15:17,740 --> 00:15:22,220
organizations like our an IB can do

00:15:20,029 --> 00:15:24,829
on-demand audio transcription and it can

00:15:22,220 --> 00:15:26,959
create audio streams in response to the

00:15:24,829 --> 00:15:28,730
needs of their customers and generate

00:15:26,959 --> 00:15:30,500
content in audio format to make it

00:15:28,730 --> 00:15:32,209
accessible for visually impaired or

00:15:30,500 --> 00:15:34,639
blind users that's a customer that we

00:15:32,209 --> 00:15:36,589
have and then in hotels comm have

00:15:34,639 --> 00:15:38,930
something similar on translation so we

00:15:36,589 --> 00:15:40,339
have a new machine translation service

00:15:38,930 --> 00:15:42,889
called Amazon translate that allows

00:15:40,339 --> 00:15:45,410
customers to convert content from one

00:15:42,889 --> 00:15:47,660
language to another and hotels.com use

00:15:45,410 --> 00:15:49,519
that service for on-demand translation

00:15:47,660 --> 00:15:51,259
of hotel reviews so if you visit a hotel

00:15:49,519 --> 00:15:53,810
that's only ever been visited by people

00:15:51,259 --> 00:15:55,339
that write reviews in German you can see

00:15:53,810 --> 00:15:56,720
a review in English at the point at

00:15:55,339 --> 00:15:58,579
which you visit the listing page for

00:15:56,720 --> 00:16:00,259
that property and the translation is

00:15:58,579 --> 00:16:02,180
done on-demand when you first hit the

00:16:00,259 --> 00:16:03,800
reviews they are translated in

00:16:02,180 --> 00:16:05,899
near-real-time from German to English

00:16:03,800 --> 00:16:07,879
and they appear in your browser in just

00:16:05,899 --> 00:16:09,170
a few hundred milliseconds have all

00:16:07,879 --> 00:16:10,819
kinds of customers that are putting

00:16:09,170 --> 00:16:13,250
these technologies to work to improve

00:16:10,819 --> 00:16:15,139
efficiency and also to build new types

00:16:13,250 --> 00:16:18,379
of customer experiences for their

00:16:15,139 --> 00:16:20,000
customers there's something I've been

00:16:18,379 --> 00:16:21,740
critical to you know a lot of the

00:16:20,000 --> 00:16:24,709
discussion around the conference has

00:16:21,740 --> 00:16:26,569
been highlighting how important

00:16:24,709 --> 00:16:28,699
operations that develop side of things

00:16:26,569 --> 00:16:32,240
is to it and I can imagine that at

00:16:28,699 --> 00:16:34,790
Amazon there's this world-class depth of

00:16:32,240 --> 00:16:38,750
deep bench of understanding how to run

00:16:34,790 --> 00:16:40,759
very large-scale systems is that

00:16:38,750 --> 00:16:42,350
expertise coming in to inform the team's

00:16:40,759 --> 00:16:43,790
doing machine learning services as well

00:16:42,350 --> 00:16:45,410
of course when I talked about the

00:16:43,790 --> 00:16:47,389
service development model that we have

00:16:45,410 --> 00:16:50,809
with these two pizza teams there's a lot

00:16:47,389 --> 00:16:52,519
of layering within AWS so when we build

00:16:50,809 --> 00:16:54,470
services we build them one of the

00:16:52,519 --> 00:16:55,939
services that we already have okay so

00:16:54,470 --> 00:16:58,670
when we're talking about things like

00:16:55,939 --> 00:17:01,220
model training or inference with

00:16:58,670 --> 00:17:04,339
existing models those services are

00:17:01,220 --> 00:17:05,659
running on top of their AWS services the

00:17:04,339 --> 00:17:06,829
customers would would already be

00:17:05,659 --> 00:17:09,439
familiar with and would already have

00:17:06,829 --> 00:17:11,569
used it's just that the existence of

00:17:09,439 --> 00:17:13,760
those other AWS services within the

00:17:11,569 --> 00:17:16,579
stack is abstracted within the machine

00:17:13,760 --> 00:17:18,230
learning use case so say I want to train

00:17:16,579 --> 00:17:20,120
my model we've got a service that makes

00:17:18,230 --> 00:17:22,490
it possible to access a large compute

00:17:20,120 --> 00:17:22,760
pull on demand for a short period of

00:17:22,490 --> 00:17:24,380
time

00:17:22,760 --> 00:17:26,780
we make that available to the

00:17:24,380 --> 00:17:28,790
a Jamaica team the sage maker team build

00:17:26,780 --> 00:17:31,550
their special-purpose service on top of

00:17:28,790 --> 00:17:33,250
that and say Jamaica inhabit inherits

00:17:31,550 --> 00:17:35,030
the scalability and reliability

00:17:33,250 --> 00:17:37,460
characteristics of the underlying

00:17:35,030 --> 00:17:39,770
service so it's how the stack works

00:17:37,460 --> 00:17:41,420
within AWS and ultimately of course at

00:17:39,770 --> 00:17:43,280
the bottom we've got things like ec2 and

00:17:41,420 --> 00:17:46,040
Amazon s3 that underpin everything very

00:17:43,280 --> 00:17:47,660
well proven foundational services that

00:17:46,040 --> 00:17:49,490
have been operational for many years and

00:17:47,660 --> 00:17:50,990
have well understood performance

00:17:49,490 --> 00:17:52,730
availability durability and fairely

00:17:50,990 --> 00:17:54,590
characteristics so by building on top of

00:17:52,730 --> 00:17:57,080
pre-existing services we can allow

00:17:54,590 --> 00:17:59,510
higher level services like sage maker to

00:17:57,080 --> 00:18:01,340
inherit their characteristics when we're

00:17:59,510 --> 00:18:04,190
doing things like deploying inference

00:18:01,340 --> 00:18:06,830
endpoints we're placing them behind what

00:18:04,190 --> 00:18:09,260
we call cig v4 API endpoints which is

00:18:06,830 --> 00:18:11,240
the standard security and authorization

00:18:09,260 --> 00:18:13,940
model that is used for all other AWS

00:18:11,240 --> 00:18:16,220
services we have a common authentication

00:18:13,940 --> 00:18:17,600
and authorization layer and we

00:18:16,220 --> 00:18:19,370
understand how to do things like

00:18:17,600 --> 00:18:21,200
instrument those endpoints for

00:18:19,370 --> 00:18:24,080
performance we understand how to low

00:18:21,200 --> 00:18:26,420
balance them we understand how to enable

00:18:24,080 --> 00:18:28,280
and recommend approaches for customers

00:18:26,420 --> 00:18:30,680
that want to do things like a/b testing

00:18:28,280 --> 00:18:32,810
with inference endpoints so everything

00:18:30,680 --> 00:18:35,000
that you know about AWS and that so many

00:18:32,810 --> 00:18:37,190
developers and DevOps pros are familiar

00:18:35,000 --> 00:18:39,290
with is present within this new service

00:18:37,190 --> 00:18:41,300
Amazon Sage maker as well so you can

00:18:39,290 --> 00:18:43,670
just transport over your knowledge about

00:18:41,300 --> 00:18:45,470
best practices for building AWS based

00:18:43,670 --> 00:18:47,030
architecture and we use that knowledge

00:18:45,470 --> 00:18:47,420
in the context today I in machine

00:18:47,030 --> 00:18:49,700
learning

00:18:47,420 --> 00:18:52,480
wonderful thank you very much in

00:18:49,700 --> 00:18:52,480

YouTube URL: https://www.youtube.com/watch?v=5FAS0jowEaI


