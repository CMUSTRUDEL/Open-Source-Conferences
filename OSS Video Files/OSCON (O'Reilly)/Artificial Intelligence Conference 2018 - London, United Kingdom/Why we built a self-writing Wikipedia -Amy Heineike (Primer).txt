Title: Why we built a self-writing Wikipedia -Amy Heineike (Primer)
Publication date: 2018-10-10
Playlist: Artificial Intelligence Conference 2018 - London, United Kingdom
Description: 
	Human-generated knowledge bases like Wikipedia have excellent precision but poor recall. Amy Heineike explains how Primer created a self-updating knowledge base that can track factual claims in unstructured text and describe what it learns in human-readable text.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,050 --> 00:00:07,220
people really struggle with recall at

00:00:03,110 --> 00:00:09,050
scale and so what did we do well we

00:00:07,220 --> 00:00:11,690
realized that machines are pretty good

00:00:09,050 --> 00:00:16,730
at that and so we decided to make it a

00:00:11,690 --> 00:00:19,039
lot easier to make a Wikipedia page so

00:00:16,730 --> 00:00:21,679
we built Quicksilver Quicksilver is a

00:00:19,039 --> 00:00:24,529
system that can draft the first version

00:00:21,679 --> 00:00:27,319
of the Wikipedia page for you we started

00:00:24,529 --> 00:00:28,789
with a focus on scientists because we

00:00:27,319 --> 00:00:30,769
kind of have a soft spot for scientists

00:00:28,789 --> 00:00:32,739
and also because we thought this was a

00:00:30,769 --> 00:00:35,960
useful place to get going

00:00:32,739 --> 00:00:38,600
we put example output outputs for a

00:00:35,960 --> 00:00:40,429
hundred scientists up onto this site and

00:00:38,600 --> 00:00:45,019
you can access it right now if you like

00:00:40,429 --> 00:00:46,850
to and browse some of these pages so we

00:00:45,019 --> 00:00:48,710
made it as easy as we could for you to

00:00:46,850 --> 00:00:50,300
grab one of these pages and go to

00:00:48,710 --> 00:00:52,899
Wikipedia and copy across the work

00:00:50,300 --> 00:00:56,030
references and make that final version

00:00:52,899 --> 00:00:57,890
we also made a lot more data available

00:00:56,030 --> 00:01:00,859
through the link on the blog post that

00:00:57,890 --> 00:01:06,110
you can follow from here so the first

00:01:00,859 --> 00:01:09,440
question is how did we do this well we

00:01:06,110 --> 00:01:13,130
started as any good AI project does with

00:01:09,440 --> 00:01:14,960
a huge pile of data our friends up at

00:01:13,130 --> 00:01:17,270
the Allen Institute for a I gave us a

00:01:14,960 --> 00:01:21,130
massive list of scientists names

00:01:17,270 --> 00:01:23,960
affiliations papers we leveraged our

00:01:21,130 --> 00:01:26,750
huge corpus of english-language news

00:01:23,960 --> 00:01:28,550
articles and we also use Wikipedia and

00:01:26,750 --> 00:01:30,250
its sister project wiki data which has

00:01:28,550 --> 00:01:32,720
structured data that goes alongside it

00:01:30,250 --> 00:01:35,450
our first challenge was to link these

00:01:32,720 --> 00:01:37,550
together based on the scientists names

00:01:35,450 --> 00:01:40,760
now this is tricky because obviously

00:01:37,550 --> 00:01:42,830
people have people don't have unique

00:01:40,760 --> 00:01:46,100
names my favorite example of this

00:01:42,830 --> 00:01:48,320
actually is the Michael Jordan of AI who

00:01:46,100 --> 00:01:50,300
is a computer science professor called

00:01:48,320 --> 00:01:52,520
Michael Jordan he's over at the

00:01:50,300 --> 00:01:54,230
University of California in Berkeley if

00:01:52,520 --> 00:01:56,180
you were to google him and I actually

00:01:54,230 --> 00:01:57,620
recommend you do because he's one of the

00:01:56,180 --> 00:01:59,420
most interesting thinkers on the future

00:01:57,620 --> 00:02:01,970
of AI out there who wrote a great blog

00:01:59,420 --> 00:02:03,080
post this summer if you google him

00:02:01,970 --> 00:02:05,150
you'll realize that you can't get away

00:02:03,080 --> 00:02:06,350
with just googling Michael Jordan you

00:02:05,150 --> 00:02:08,030
need to put in a bunch of other words

00:02:06,350 --> 00:02:09,679
like machine learning or artificial

00:02:08,030 --> 00:02:11,810
intelligence or Berkeley to get down to

00:02:09,679 --> 00:02:13,530
who he is so we had to train the system

00:02:11,810 --> 00:02:15,850
to do a similar thing

00:02:13,530 --> 00:02:18,730
once we link them together we can make a

00:02:15,850 --> 00:02:21,420
list the list of who's missing so the

00:02:18,730 --> 00:02:23,680
people with a lot of content but no page

00:02:21,420 --> 00:02:26,260
some of these gaps are pretty egregious

00:02:23,680 --> 00:02:28,510
people with an enormous amount of press

00:02:26,260 --> 00:02:30,760
coverage a huge amount of interest in

00:02:28,510 --> 00:02:34,900
their scientific research and yet no

00:02:30,760 --> 00:02:36,760
page that's been created we could then

00:02:34,900 --> 00:02:38,830
start to figure out how to build pages

00:02:36,760 --> 00:02:41,320
for them so the first thing we did was

00:02:38,830 --> 00:02:43,300
we went back to Wikipedia we grabbed all

00:02:41,320 --> 00:02:45,000
the scientists already on Wikipedia and

00:02:43,300 --> 00:02:48,340
all the news we could find about them

00:02:45,000 --> 00:02:51,160
that give us a training set of 30,000

00:02:48,340 --> 00:02:54,100
profiles of how news maps to Wikipedia

00:02:51,160 --> 00:02:57,700
pages and then we could train the system

00:02:54,100 --> 00:03:00,010
to draft that first version so here's

00:02:57,700 --> 00:03:04,930
what it looks like this is Karen lips

00:03:00,010 --> 00:03:07,390
who's missing from Wikipedia so she's a

00:03:04,930 --> 00:03:10,239
researcher in amphibians and reptiles

00:03:07,390 --> 00:03:11,560
over at the University of Maryland so

00:03:10,239 --> 00:03:13,600
there's a few things that I want to call

00:03:11,560 --> 00:03:16,150
out that are happening in this in this

00:03:13,600 --> 00:03:18,970
summary the first one is that we would

00:03:16,150 --> 00:03:21,430
leveraged structured content so we went

00:03:18,970 --> 00:03:23,049
into this news and we found out facts

00:03:21,430 --> 00:03:25,239
and claims that people were making about

00:03:23,049 --> 00:03:27,760
her so where does she work what does she

00:03:25,239 --> 00:03:29,920
study where is she based does she have

00:03:27,760 --> 00:03:33,370
any major awards and that we could drop

00:03:29,920 --> 00:03:37,239
in secondly we could make a really rich

00:03:33,370 --> 00:03:39,519
summary trained on the kinds of content

00:03:37,239 --> 00:03:42,640
that make it into Wikipedia paragraphs

00:03:39,519 --> 00:03:46,870
but maybe they're a bit too rich to kind

00:03:42,640 --> 00:03:50,320
of easily structure into into triple so

00:03:46,870 --> 00:03:52,630
things like the idea that she has has

00:03:50,320 --> 00:03:54,489
researched fungus on frogs and

00:03:52,630 --> 00:03:55,989
understood that that was a main reason

00:03:54,489 --> 00:04:00,459
why frog populations would acquit

00:03:55,989 --> 00:04:02,920
decaying and then finally we created an

00:04:00,459 --> 00:04:05,950
event stream so this is really bread and

00:04:02,920 --> 00:04:08,440
butter for primer given a huge number of

00:04:05,950 --> 00:04:11,470
articles pick out the big stories and

00:04:08,440 --> 00:04:12,850
tell the story of how her research is

00:04:11,470 --> 00:04:15,730
impacting the world and how that's

00:04:12,850 --> 00:04:17,440
unfolding over time so this is all

00:04:15,730 --> 00:04:18,540
machine generative here's another

00:04:17,440 --> 00:04:23,830
example

00:04:18,540 --> 00:04:26,200
Andre Carpathia is he's now the director

00:04:23,830 --> 00:04:26,740
of AI over at Tesla so maybe a little

00:04:26,200 --> 00:04:29,430
bit more

00:04:26,740 --> 00:04:29,430

YouTube URL: https://www.youtube.com/watch?v=UHV8JvQenHc


