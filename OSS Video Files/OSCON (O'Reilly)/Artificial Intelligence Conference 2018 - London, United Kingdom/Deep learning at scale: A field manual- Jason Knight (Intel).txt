Title: Deep learning at scale: A field manual- Jason Knight (Intel)
Publication date: 2018-10-11
Playlist: Artificial Intelligence Conference 2018 - London, United Kingdom
Description: 
	There are many resources to help you get started with machine learning and deep learning. From hands-on tutorials highlighting pretrained models to accessible deep learning frameworks, AI practitioners have numerous tools to add to their workflow. However, when scaling up to larger training datasets and deployment scenarios, the path is not always clear.

Jason Knight offers an overview of the state of the field for scaling training and inference across distributed systems from a practitionerâ€™s point of view. Along the way, Jason dives deep into available tools, resources, and venues for getting started without having to go it alone.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:02,850
let's look at the deep learning

00:00:01,260 --> 00:00:05,339
lifecycle and kind of break this down

00:00:02,850 --> 00:00:08,309
how this analogy applies each step of

00:00:05,339 --> 00:00:11,599
the way so the first step is

00:00:08,309 --> 00:00:15,770
accumulating the data accumulating it

00:00:11,599 --> 00:00:18,779
cleaning it etc so how does this apply

00:00:15,770 --> 00:00:22,650
so the bottom line is that your data is

00:00:18,779 --> 00:00:23,850
just a special kind of code and what I

00:00:22,650 --> 00:00:26,519
mean by that is that you want to apply

00:00:23,850 --> 00:00:27,840
the same set of best practices that you

00:00:26,519 --> 00:00:31,619
do to code as you do data

00:00:27,840 --> 00:00:33,510
one of those is versioning how do you

00:00:31,619 --> 00:00:35,640
version data well it's it's a little

00:00:33,510 --> 00:00:37,829
tricky if you're lucky enough to use

00:00:35,640 --> 00:00:40,309
data that's immutable for an immutable

00:00:37,829 --> 00:00:43,020
store which copy-on-write or append only

00:00:40,309 --> 00:00:46,160
kind of semantics then you're

00:00:43,020 --> 00:00:48,870
fortunately much better position than

00:00:46,160 --> 00:00:51,780
versioning more mutable stores like

00:00:48,870 --> 00:00:56,219
traditional databases but and this is

00:00:51,780 --> 00:00:58,890
still somewhat of a black art but it

00:00:56,219 --> 00:01:02,730
it's it's very critical in order to

00:00:58,890 --> 00:01:05,250
track your models in terms of where the

00:01:02,730 --> 00:01:08,330
model came from which data it originated

00:01:05,250 --> 00:01:12,510
from in the training process another

00:01:08,330 --> 00:01:13,890
kind of lens we can use is in the same

00:01:12,510 --> 00:01:17,250
sense that we don't want to have

00:01:13,890 --> 00:01:18,990
multiple different you know libraries

00:01:17,250 --> 00:01:21,150
when you can avoid it or multiple

00:01:18,990 --> 00:01:22,979
different programming languages in the

00:01:21,150 --> 00:01:26,430
same stack you also want to avoid data

00:01:22,979 --> 00:01:29,310
silos now these aren't avoidable in all

00:01:26,430 --> 00:01:31,890
cases because time series databases are

00:01:29,310 --> 00:01:33,110
necessary for certain volumes and types

00:01:31,890 --> 00:01:37,590
of time series data

00:01:33,110 --> 00:01:39,900
likewise for genetic data or other kinds

00:01:37,590 --> 00:01:41,490
but in general though the more you can

00:01:39,900 --> 00:01:44,579
avoid data silos the better you're gonna

00:01:41,490 --> 00:01:46,680
be off and when you can't avoid them

00:01:44,579 --> 00:01:48,270
then you build abstraction layers on top

00:01:46,680 --> 00:01:50,180
of them and those abstraction layers can

00:01:48,270 --> 00:01:54,210
be very valuable as a place for

00:01:50,180 --> 00:01:57,329
inserting the kinds of metadata that's

00:01:54,210 --> 00:01:58,979
useful for the model provenance and

00:01:57,329 --> 00:02:01,259
lineage that was talking about and I'll

00:01:58,979 --> 00:02:03,689
talk about a little bit in addition

00:02:01,259 --> 00:02:06,780
there's the concept of unit data tests

00:02:03,689 --> 00:02:09,989
so in addition to the test data set you

00:02:06,780 --> 00:02:11,730
can construct unit data sets that test

00:02:09,989 --> 00:02:12,880
particular aspects of the model that

00:02:11,730 --> 00:02:15,730
you're training

00:02:12,880 --> 00:02:17,860
and metadata is data too in the same way

00:02:15,730 --> 00:02:20,200
that you wouldn't take a script and

00:02:17,860 --> 00:02:22,030
Python and then say oh this this script

00:02:20,200 --> 00:02:23,200
is special I'm gonna store it on my hard

00:02:22,030 --> 00:02:25,240
drive and not check into version control

00:02:23,200 --> 00:02:28,150
you wouldn't do the same for data either

00:02:25,240 --> 00:02:32,760
all data should be tracked and monitored

00:02:28,150 --> 00:02:32,760

YouTube URL: https://www.youtube.com/watch?v=EjEMYnkteMc


