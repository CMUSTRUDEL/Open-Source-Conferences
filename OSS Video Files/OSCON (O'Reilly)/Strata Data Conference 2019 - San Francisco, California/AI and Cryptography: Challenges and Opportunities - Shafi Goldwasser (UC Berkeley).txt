Title: AI and Cryptography: Challenges and Opportunities - Shafi Goldwasser (UC Berkeley)
Publication date: 2019-04-04
Playlist: Strata Data Conference 2019 - San Francisco, California
Description: 
	To view the full keynote and other talks from Strata SF 2019, visit:
http://oreilly.com/go/stratasf19

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:05,220
so the first observation is since data

00:00:02,610 --> 00:00:07,109
is so important and the power seems to

00:00:05,220 --> 00:00:09,000
come from data right now maybe in the

00:00:07,109 --> 00:00:09,990
future maybe soon we will realize we can

00:00:09,000 --> 00:00:11,700
do all these things we were doing today

00:00:09,990 --> 00:00:14,940
with less data but right now data seems

00:00:11,700 --> 00:00:17,369
to be crucial then in this data is by

00:00:14,940 --> 00:00:20,220
individuals so in order not to give this

00:00:17,369 --> 00:00:22,740
power away so readily we have to ensure

00:00:20,220 --> 00:00:24,779
the privacy of both the data and the

00:00:22,740 --> 00:00:28,260
model that's being generated based on

00:00:24,779 --> 00:00:31,170
its training data during training and

00:00:28,260 --> 00:00:33,000
classifying and this will enable us to

00:00:31,170 --> 00:00:36,930
maintain power and to entertain value of

00:00:33,000 --> 00:00:38,670
our data so privacy is a big issue both

00:00:36,930 --> 00:00:40,860
when you train the model and when you

00:00:38,670 --> 00:00:43,020
use it and another one that people pay

00:00:40,860 --> 00:00:44,579
less attention to is that ok let's say

00:00:43,020 --> 00:00:46,890
that you have trained a fantastic model

00:00:44,579 --> 00:00:48,690
that can answer questions of the sort

00:00:46,890 --> 00:00:51,000
that that black box was set out to

00:00:48,690 --> 00:00:52,260
answer and now it's out there you want

00:00:51,000 --> 00:00:54,239
to make absolutely sure that these

00:00:52,260 --> 00:00:56,370
models are not temporal people cannot

00:00:54,239 --> 00:00:59,309
tamper with them or and there was no

00:00:56,370 --> 00:01:01,379
bias introduced for profit or control by

00:00:59,309 --> 00:01:03,510
whoever supplied you with the data based

00:01:01,379 --> 00:01:05,430
on which you came up with the model so

00:01:03,510 --> 00:01:07,290
again this sounds like a sting operation

00:01:05,430 --> 00:01:10,619
like who is out there really trying to

00:01:07,290 --> 00:01:12,170
modify models but as we know once this

00:01:10,619 --> 00:01:14,100
thing has so much power they'll also be

00:01:12,170 --> 00:01:16,020
adversary's who are trying to take

00:01:14,100 --> 00:01:17,850
control of that power in much more

00:01:16,020 --> 00:01:20,009
sophisticated and devious ways than we

00:01:17,850 --> 00:01:21,900
think of so if you translate that to

00:01:20,009 --> 00:01:24,720
technical challenges we need to develop

00:01:21,900 --> 00:01:27,299
methods that minimize the influence of

00:01:24,720 --> 00:01:29,909
maliciously trained training data we

00:01:27,299 --> 00:01:32,340
need to come up with ways to prove that

00:01:29,909 --> 00:01:35,970
once somebody or a company or individual

00:01:32,340 --> 00:01:38,490
or some you know researcher comes up

00:01:35,970 --> 00:01:40,170
with a fabulous model is to actually

00:01:38,490 --> 00:01:41,880
prove to us that the model is consistent

00:01:40,170 --> 00:01:44,220
with data and we should be able to do

00:01:41,880 --> 00:01:46,560
that you know efficiently maybe with a

00:01:44,220 --> 00:01:48,360
kind of access that a big company has

00:01:46,560 --> 00:01:51,110
the data but access to much smaller

00:01:48,360 --> 00:01:53,460
amounts of data and of less quality and

00:01:51,110 --> 00:01:54,990
obviously we need finally to develop

00:01:53,460 --> 00:01:56,820
secure infrastructures to run these

00:01:54,990 --> 00:01:59,460
models so that in fact people cannot

00:01:56,820 --> 00:02:02,460
tamper with them people cannot sort of

00:01:59,460 --> 00:02:07,799
maul them in ways that will ensure

00:02:02,460 --> 00:02:09,660
profit or risk later on so I guess the

00:02:07,799 --> 00:02:12,120
thesis of the of this talk and in the

00:02:09,660 --> 00:02:13,560
next few slides is that how do we do

00:02:12,120 --> 00:02:15,540
this how do we

00:02:13,560 --> 00:02:17,610
ensure privacy of the data during

00:02:15,540 --> 00:02:19,110
training and classifying how do we

00:02:17,610 --> 00:02:21,870
ensure that the models are not tampered

00:02:19,110 --> 00:02:24,810
with that they're not that they're

00:02:21,870 --> 00:02:25,980
consistent with the data so my thesis is

00:02:24,810 --> 00:02:26,790
that there's really a lot of

00:02:25,980 --> 00:02:28,260
cryptography

00:02:26,790 --> 00:02:30,330
which has nothing to do with machine

00:02:28,260 --> 00:02:33,390
learning when he was developed tools

00:02:30,330 --> 00:02:35,250
models from the last 30 years which can

00:02:33,390 --> 00:02:37,410
go a long way toward achieving these

00:02:35,250 --> 00:02:38,250
goals so people often when they think of

00:02:37,410 --> 00:02:40,470
cryptography and they're not

00:02:38,250 --> 00:02:42,000
cryptographers they think that it really

00:02:40,470 --> 00:02:44,099
is dedicated only to secure

00:02:42,000 --> 00:02:45,569
communication and maybe secure

00:02:44,099 --> 00:02:47,370
transaction so I want to prove Who I am

00:02:45,569 --> 00:02:48,750
when I buy something on Amazon and I

00:02:47,370 --> 00:02:50,430
want to send messages back and forth and

00:02:48,750 --> 00:02:52,410
email and cryptography takes care of

00:02:50,430 --> 00:02:54,480
both of these tasks with encryption and

00:02:52,410 --> 00:02:56,459
digital signatures but the truth is that

00:02:54,480 --> 00:02:57,870
in the last thirty years most of the

00:02:56,459 --> 00:03:00,930
academic research in cryptography has

00:02:57,870 --> 00:03:02,910
been on privacy and correctness of

00:03:00,930 --> 00:03:04,860
computation rather than communication so

00:03:02,910 --> 00:03:07,080
the communication problem was a lot of

00:03:04,860 --> 00:03:10,140
tension to it it's done well maybe we

00:03:07,080 --> 00:03:12,420
need to switch from schemes in order to

00:03:10,140 --> 00:03:15,030
be quantum resilient but a lot of work

00:03:12,420 --> 00:03:16,410
is dedicated a private computation and

00:03:15,030 --> 00:03:18,480
it's a very good question what does it

00:03:16,410 --> 00:03:19,739
mean private computation when we talk

00:03:18,480 --> 00:03:21,750
about computation where does privacy

00:03:19,739 --> 00:03:23,730
comes in what does it mean correctness

00:03:21,750 --> 00:03:26,459
of computation and who's the adversary

00:03:23,730 --> 00:03:29,190
anyway and but these are questions that

00:03:26,459 --> 00:03:31,200
are defined in cryptographic setting if

00:03:29,190 --> 00:03:33,030
you think about correctness if you're

00:03:31,200 --> 00:03:35,010
delegating computation to the cloud and

00:03:33,030 --> 00:03:36,090
you want them to check you want to check

00:03:35,010 --> 00:03:38,040
the correctness of what's been done

00:03:36,090 --> 00:03:39,900
without having to redo the work and if

00:03:38,040 --> 00:03:42,299
you think about privacy of community of

00:03:39,900 --> 00:03:44,970
computation here is the type of things

00:03:42,299 --> 00:03:46,920
that come up in the context of machine

00:03:44,970 --> 00:03:49,139
learning so if we think about machine

00:03:46,920 --> 00:03:52,109
learning as sort of there's two stages

00:03:49,139 --> 00:03:53,579
right there is a training phase where

00:03:52,109 --> 00:03:55,920
you have a whole bunch of training data

00:03:53,579 --> 00:03:57,269
and you come up with a model you would

00:03:55,920 --> 00:03:59,700
like to maintain the privacy of the

00:03:57,269 --> 00:04:01,380
training data and yet come up with a

00:03:59,700 --> 00:04:03,389
model which is as accurate as you could

00:04:01,380 --> 00:04:06,389
if the MA if the privacy was not of

00:04:03,389 --> 00:04:08,940
concern and essentially there's a lot of

00:04:06,389 --> 00:04:10,340
work that is done right now on this

00:04:08,940 --> 00:04:13,200
problem it's a very challenging problem

00:04:10,340 --> 00:04:14,670
and it's done really by picking and

00:04:13,200 --> 00:04:18,060
choosing cryptographic methods of the

00:04:14,670 --> 00:04:20,220
past people probably are very familiar

00:04:18,060 --> 00:04:21,630
with the notion of federated learning so

00:04:20,220 --> 00:04:24,390
it's using things called multi-party

00:04:21,630 --> 00:04:26,220
computation and other methods defined

00:04:24,390 --> 00:04:28,350
within cryptography without any

00:04:26,220 --> 00:04:31,290
- machine learning but seems to be

00:04:28,350 --> 00:04:34,380
useful for the training problem and the

00:04:31,290 --> 00:04:35,910
second is the classification stage where

00:04:34,380 --> 00:04:37,230
the models already developed and now you

00:04:35,910 --> 00:04:39,840
have to party somebody's holding the

00:04:37,230 --> 00:04:42,360
model and somebody who is some data they

00:04:39,840 --> 00:04:45,540
want to use let's say use the model to

00:04:42,360 --> 00:04:48,060
classify and they have opposing security

00:04:45,540 --> 00:04:49,440
concerns the model owner may want to

00:04:48,060 --> 00:04:53,610
protect the parameters of the model

00:04:49,440 --> 00:04:55,950
because they are value may be monetary

00:04:53,610 --> 00:04:57,210
value and the owner of the data wants to

00:04:55,950 --> 00:05:02,490
protect the privacy of their data so

00:04:57,210 --> 00:05:04,650
this is a two party protocol and it is

00:05:02,490 --> 00:05:05,970
an easier problem than you know they're

00:05:04,650 --> 00:05:09,270
establishing the security goals of both

00:05:05,970 --> 00:05:11,580
sides the model and the data owner and a

00:05:09,270 --> 00:05:12,960
lot of work has been done and I feel I

00:05:11,580 --> 00:05:14,280
think people feel a lot more comfortable

00:05:12,960 --> 00:05:18,170
this problem then with the training

00:05:14,280 --> 00:05:18,170
because it obviously is more challenging

00:05:24,420 --> 00:05:26,480

YouTube URL: https://www.youtube.com/watch?v=RIfgWZwtlO4


