Title: Creating a visual interface for machine learning with Martin and Robert (PerceptiLabs)
Publication date: 2019-11-12
Playlist: TensorFlow World 2019
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:06,290
Roger here with Robert Martin CEO and

00:00:02,669 --> 00:00:09,120
CTO of Priscilla's welcome thank you so

00:00:06,290 --> 00:00:10,440
exactly what is perceptive labs and

00:00:09,120 --> 00:00:12,900
maybe we can tell us how you got to

00:00:10,440 --> 00:00:15,350
starting up a company yeah of course

00:00:12,900 --> 00:00:18,420
so me and Martin met the first day at

00:00:15,350 --> 00:00:20,220
university and ever since then we kind

00:00:18,420 --> 00:00:22,800
of throught entire University we did

00:00:20,220 --> 00:00:24,779
projects together a lot of garage

00:00:22,800 --> 00:00:26,490
projects I have a little lab in my

00:00:24,779 --> 00:00:32,730
garage which we used to go to and play

00:00:26,490 --> 00:00:37,320
around with electronics and we during

00:00:32,730 --> 00:00:39,899
the last semester of university we kind

00:00:37,320 --> 00:00:43,770
of figured hey why not start up a

00:00:39,899 --> 00:00:46,200
company we had an idea of kind of how to

00:00:43,770 --> 00:00:50,399
make machine learning easier and more

00:00:46,200 --> 00:00:53,789
efficient so we just developing around

00:00:50,399 --> 00:00:58,739
that and that's kind of how it all

00:00:53,789 --> 00:01:03,329
started and the platform which is now

00:00:58,739 --> 00:01:08,700
recently in beta actually like two days

00:01:03,329 --> 00:01:12,000
ago yeah this is very recent so the

00:01:08,700 --> 00:01:14,670
platform itself is it acts like an

00:01:12,000 --> 00:01:16,979
abstraction to machine learning where

00:01:14,670 --> 00:01:18,450
you can instead of coding everything you

00:01:16,979 --> 00:01:21,900
can drag and drop in different

00:01:18,450 --> 00:01:25,979
components and these components acts as

00:01:21,900 --> 00:01:30,689
kind of templates to different layers in

00:01:25,979 --> 00:01:33,570
a machinery model so what we strive to

00:01:30,689 --> 00:01:36,180
do is we give the abstraction layer with

00:01:33,570 --> 00:01:38,579
just seeing a nice view of how the model

00:01:36,180 --> 00:01:41,040
looks but then you can even and then you

00:01:38,579 --> 00:01:44,310
can also dip dive into every component

00:01:41,040 --> 00:01:46,049
and you can custom code it so in that

00:01:44,310 --> 00:01:48,570
way it's very much like an IDE for

00:01:46,049 --> 00:01:50,240
machine learning and on top of this we

00:01:48,570 --> 00:01:53,939
provide a bunch of boilerplate

00:01:50,240 --> 00:01:56,700
associations so as you're building model

00:01:53,939 --> 00:01:58,979
and as you're training it you get

00:01:56,700 --> 00:02:00,270
instant feedback on how it's going so

00:01:58,979 --> 00:02:01,570
you can match this your debug you can

00:02:00,270 --> 00:02:05,740
see

00:02:01,570 --> 00:02:08,979
where where does it not match up so you

00:02:05,740 --> 00:02:11,260
guys could have a visual tool for AI how

00:02:08,979 --> 00:02:15,220
did you come up with the idea it's a

00:02:11,260 --> 00:02:19,300
pretty fun story or it's a story it

00:02:15,220 --> 00:02:21,400
started in 2016 I was in studying in

00:02:19,300 --> 00:02:24,880
Spain and Robert was still in Stockholm

00:02:21,400 --> 00:02:26,800
and a friend of mine mentioned a hockey

00:02:24,880 --> 00:02:29,710
town called Junction like Europe's large

00:02:26,800 --> 00:02:31,600
hackathon in Helsinki so I asked Robert

00:02:29,710 --> 00:02:34,780
if he wanted to participate since we

00:02:31,600 --> 00:02:38,140
liked doing projects together so we flew

00:02:34,780 --> 00:02:39,880
out to Helsinki and we were gonna do

00:02:38,140 --> 00:02:42,910
like a competition there where we use

00:02:39,880 --> 00:02:46,720
European Space Agency satellite image

00:02:42,910 --> 00:02:48,730
data to track fish swarms and we

00:02:46,720 --> 00:02:50,590
realized pretty quickly that this is

00:02:48,730 --> 00:02:52,840
gonna take longer time than two three

00:02:50,590 --> 00:02:55,630
days to build it out and train it and

00:02:52,840 --> 00:03:00,670
everything so we actually ended up

00:02:55,630 --> 00:03:02,140
building our your game instead and but

00:03:00,670 --> 00:03:04,540
yeah that's how we come up with the idea

00:03:02,140 --> 00:03:08,739
of we want to have a easier way of

00:03:04,540 --> 00:03:10,360
working with machine learning and we

00:03:08,739 --> 00:03:14,110
started thinking about this visual

00:03:10,360 --> 00:03:16,090
graphical user interface and yeah from

00:03:14,110 --> 00:03:17,920
there we started to work in it on

00:03:16,090 --> 00:03:20,680
weekends at Roberts place our first idea

00:03:17,920 --> 00:03:22,930
was to build it in videos you can freely

00:03:20,680 --> 00:03:26,080
say be neurons that's probably our first

00:03:22,930 --> 00:03:28,690
idea yeah it took a turn from there yeah

00:03:26,080 --> 00:03:30,310
but but that sounds interesting that it

00:03:28,690 --> 00:03:33,519
sounds like in a way you're trying to

00:03:30,310 --> 00:03:36,160
abstract away from the nuts and bolts of

00:03:33,519 --> 00:03:38,080
doing it to really thinking about the

00:03:36,160 --> 00:03:42,549
actual model yes that you're trying to

00:03:38,080 --> 00:03:44,950
build so so that it sounds like VR it

00:03:42,549 --> 00:03:47,560
actually does make sense in that from

00:03:44,950 --> 00:03:49,750
some some perspective so where do you

00:03:47,560 --> 00:03:52,000
see things going in machine learning

00:03:49,750 --> 00:03:54,840
space so I think we can look at

00:03:52,000 --> 00:03:59,350
especially now tensorflow and pi torch

00:03:54,840 --> 00:04:03,670
so tensorflow has always been a very

00:03:59,350 --> 00:04:06,780
popular framework especially now in in

00:04:03,670 --> 00:04:09,220
production and everything then Kira's

00:04:06,780 --> 00:04:09,730
storage in cope over 1000 started to

00:04:09,220 --> 00:04:11,500
incorporate

00:04:09,730 --> 00:04:14,019
Kiera's more and more because of the

00:04:11,500 --> 00:04:14,860
high level API it is some people really

00:04:14,019 --> 00:04:17,440
show that they want to use

00:04:14,860 --> 00:04:19,660
is the same thing with pi torch when it

00:04:17,440 --> 00:04:21,489
came out people started using pi torch

00:04:19,660 --> 00:04:25,540
more and more because it's just so much

00:04:21,489 --> 00:04:27,520
simpler to use it's more pythonic so we

00:04:25,540 --> 00:04:30,480
can really see now with tensorflow 2.0

00:04:27,520 --> 00:04:33,730
that they try to bridge the gap between

00:04:30,480 --> 00:04:36,040
like researchers and beginners so you

00:04:33,730 --> 00:04:39,100
can use a high level API but still have

00:04:36,040 --> 00:04:40,840
the flexibility and that's the way we

00:04:39,100 --> 00:04:43,600
see this well that's where it should go

00:04:40,840 --> 00:04:45,730
yeah so so for your framework how does

00:04:43,600 --> 00:04:48,430
it differ from what else is out there

00:04:45,730 --> 00:04:51,870
aside from it being visual well the

00:04:48,430 --> 00:04:54,220
visual is a very big part of it so with

00:04:51,870 --> 00:04:57,760
the other frameworks if we compared to

00:04:54,220 --> 00:05:00,400
like tensorflow and scikit-learn pi

00:04:57,760 --> 00:05:01,390
torch we we don't really complete

00:05:00,400 --> 00:05:03,930
against them in any way

00:05:01,390 --> 00:05:07,150
we rather building on top of them and

00:05:03,930 --> 00:05:08,050
we're wrapping around so every of our

00:05:07,150 --> 00:05:11,020
components

00:05:08,050 --> 00:05:12,970
contains their code what we're doing is

00:05:11,020 --> 00:05:15,210
providing a lot of associations and this

00:05:12,970 --> 00:05:18,010
abstraction layer to make it just

00:05:15,210 --> 00:05:20,860
simpler to get started easier to treat

00:05:18,010 --> 00:05:24,790
models but still allowing it to actually

00:05:20,860 --> 00:05:27,790
dip dive in so that's how we differ from

00:05:24,790 --> 00:05:30,850
or how we compared to tensor flow and so

00:05:27,790 --> 00:05:34,750
on then what we're really striving to do

00:05:30,850 --> 00:05:35,979
is not to lock in one in we don't want

00:05:34,750 --> 00:05:38,860
two components or anything to be

00:05:35,979 --> 00:05:40,990
hard-coded so we want you to be able to

00:05:38,860 --> 00:05:43,840
build anything you would be able to

00:05:40,990 --> 00:05:46,180
build in your stroke code just with some

00:05:43,840 --> 00:05:48,880
extra bonuses do you think you're really

00:05:46,180 --> 00:05:51,130
working on like democratizing access is

00:05:48,880 --> 00:05:52,900
this easier for everyone to do it or is

00:05:51,130 --> 00:05:55,229
it making people already kind of know

00:05:52,900 --> 00:05:57,520
what they're doing more productively so

00:05:55,229 --> 00:05:59,830
democratizing AI was our first slogan

00:05:57,520 --> 00:06:04,600
and it was has been everyone's slogan

00:05:59,830 --> 00:06:06,940
yeah it seems like nowadays so in a way

00:06:04,600 --> 00:06:09,040
it does make the barrier entry smaller

00:06:06,940 --> 00:06:10,780
right you you get visual feedback and

00:06:09,040 --> 00:06:13,600
can very easily understand what's

00:06:10,780 --> 00:06:16,479
happening even though you don't are too

00:06:13,600 --> 00:06:19,510
familiar with the math to start with so

00:06:16,479 --> 00:06:21,430
that's why yeah it makes it easier it is

00:06:19,510 --> 00:06:24,310
a big boom though for those who actually

00:06:21,430 --> 00:06:28,150
already know and we get a log

00:06:24,310 --> 00:06:29,710
from that we're just easily start

00:06:28,150 --> 00:06:31,660
iterating a model and actually seeing

00:06:29,710 --> 00:06:33,220
what's happening inside a model and the

00:06:31,660 --> 00:06:34,840
debugging and everything you don't have

00:06:33,220 --> 00:06:37,780
I mean that's possible in another

00:06:34,840 --> 00:06:39,160
extensive flow framework also but you

00:06:37,780 --> 00:06:42,150
don't really want to sit and code all of

00:06:39,160 --> 00:06:45,010
that every time you'd appeal the model

00:06:42,150 --> 00:06:47,770
yeah yeah so you really think both you

00:06:45,010 --> 00:06:49,240
really trying to cover both yeah we're

00:06:47,770 --> 00:06:53,890
we're definitely not trying to cover

00:06:49,240 --> 00:06:56,230
like what outer ml are doing so because

00:06:53,890 --> 00:06:58,750
that's that's why we really like that's

00:06:56,230 --> 00:07:01,210
really democratizing AI but it's still I

00:06:58,750 --> 00:07:02,680
don't know how good it is to not have a

00:07:01,210 --> 00:07:08,530
clue what's going on behind the scenes

00:07:02,680 --> 00:07:09,790
either so yeah no I there's a pretty I

00:07:08,530 --> 00:07:11,230
think it's a pretty big difference but

00:07:09,790 --> 00:07:12,880
tell me what I don't want mal is trying

00:07:11,230 --> 00:07:15,040
to do yeah I think I don't want mal is

00:07:12,880 --> 00:07:18,010
good when the problem is well understood

00:07:15,040 --> 00:07:19,210
by a lot of people yes and it sounds

00:07:18,010 --> 00:07:21,550
like what you're trying to do is when

00:07:19,210 --> 00:07:25,540
you've got a particular problem yeah

00:07:21,550 --> 00:07:28,630
exactly and to getting coding out of the

00:07:25,540 --> 00:07:31,450
way of iterating is a big boon to

00:07:28,630 --> 00:07:32,860
getting to the end state yes so we

00:07:31,450 --> 00:07:35,290
actually started building it for

00:07:32,860 --> 00:07:36,490
ourselves to do machine learning

00:07:35,290 --> 00:07:38,890
research and come up with novel

00:07:36,490 --> 00:07:41,380
architectures so like the idea was look

00:07:38,890 --> 00:07:43,600
at the archive expect paper see the

00:07:41,380 --> 00:07:45,520
drawing that they made of the network

00:07:43,600 --> 00:07:47,170
and just being able to draw out the

00:07:45,520 --> 00:07:50,290
building blocks and you have it there

00:07:47,170 --> 00:07:52,360
the complex model build in a few seconds

00:07:50,290 --> 00:07:54,010
so one of the things we keep hearing

00:07:52,360 --> 00:07:55,960
about particularly with neural networks

00:07:54,010 --> 00:08:00,040
is that they're hard to explain what's

00:07:55,960 --> 00:08:01,990
going on and we reproducibility is a

00:08:00,040 --> 00:08:03,520
whole different problem but do you think

00:08:01,990 --> 00:08:06,010
with the visual model you're getting

00:08:03,520 --> 00:08:08,580
better explained ability yeah definitely

00:08:06,010 --> 00:08:10,320
I do think that's

00:08:08,580 --> 00:08:11,820
being able to actually look into what

00:08:10,320 --> 00:08:14,670
each component does with a visual

00:08:11,820 --> 00:08:17,700
interface does help a lot to listen some

00:08:14,670 --> 00:08:21,620
level of understanding to Italy what

00:08:17,700 --> 00:08:24,810
kind of what does it do so given that I

00:08:21,620 --> 00:08:27,060
when I when I build a model I have no

00:08:24,810 --> 00:08:29,370
idea what just happened sometimes they

00:08:27,060 --> 00:08:31,500
work sometimes they don't I don't know

00:08:29,370 --> 00:08:33,150
the time to like go into do you have any

00:08:31,500 --> 00:08:35,820
examples where you actually did some

00:08:33,150 --> 00:08:38,130
work and you're like wow I actually know

00:08:35,820 --> 00:08:40,050
why it made this mistake or why it was

00:08:38,130 --> 00:08:43,740
really good at predicting this thing

00:08:40,050 --> 00:08:46,170
yeah whether this something at Red Hat

00:08:43,740 --> 00:08:49,140
summit now and in May was in Boston

00:08:46,170 --> 00:08:51,510
where we with Red Hat built out a

00:08:49,140 --> 00:08:53,340
Twitter bot which allowed you to if you

00:08:51,510 --> 00:08:56,010
upload a picture of Twitter and

00:08:53,340 --> 00:08:58,530
mentioned the bots name it will give you

00:08:56,010 --> 00:09:01,470
a classification if it's a red hat in

00:08:58,530 --> 00:09:05,010
the picture or not so when we build out

00:09:01,470 --> 00:09:06,990
that model we had like five days or

00:09:05,010 --> 00:09:09,600
something on us and we use our platform

00:09:06,990 --> 00:09:12,300
for that and it was very good because we

00:09:09,600 --> 00:09:14,040
could see very quickly like there's some

00:09:12,300 --> 00:09:16,230
ways you can look so if the gradients

00:09:14,040 --> 00:09:18,720
for example is flat in a convolutional

00:09:16,230 --> 00:09:20,490
layer you know that it's not learning so

00:09:18,720 --> 00:09:22,680
we could use that kind of things in the

00:09:20,490 --> 00:09:24,750
platform as you show the gradients yeah

00:09:22,680 --> 00:09:27,510
and you can like see that very quickly

00:09:24,750 --> 00:09:29,460
so we can quick pause or stop the

00:09:27,510 --> 00:09:30,990
training change something it's it's a

00:09:29,460 --> 00:09:32,760
way fast it straight through model

00:09:30,990 --> 00:09:35,360
architectures so in a way you're able to

00:09:32,760 --> 00:09:37,740
do some of your feature engineering yeah

00:09:35,360 --> 00:09:39,800
within the layers of the model

00:09:37,740 --> 00:09:42,210
interesting that sounds pretty cool

00:09:39,800 --> 00:09:44,460
given that we're we're right now with PI

00:09:42,210 --> 00:09:46,650
torch and tensorflow or at the tensor

00:09:44,460 --> 00:09:49,380
flow world conference where do you see

00:09:46,650 --> 00:09:50,790
things evolving I think it's much like

00:09:49,380 --> 00:09:53,100
Martin said we're gonna go more and more

00:09:50,790 --> 00:09:55,320
so watch this abstraction layer no one

00:09:53,100 --> 00:09:56,190
really wants to see it in deep deep deep

00:09:55,320 --> 00:09:59,160
unless they have to

00:09:56,190 --> 00:10:01,380
so if you can abstract that away that's

00:09:59,160 --> 00:10:05,010
a big plus yeah and for the model import

00:10:01,380 --> 00:10:06,810
we can see more and more AP is coming

00:10:05,010 --> 00:10:08,670
out that like make it easier to

00:10:06,810 --> 00:10:10,710
interpret models and all this

00:10:08,670 --> 00:10:15,680
visualization tools so we're gonna work

00:10:10,710 --> 00:10:17,670
a lot on making even making it even more

00:10:15,680 --> 00:10:19,050
visual and

00:10:17,670 --> 00:10:20,550
making it easier to understand what's

00:10:19,050 --> 00:10:22,230
happening yeah there's a lot of things

00:10:20,550 --> 00:10:24,480
cool things you can do with the bugging

00:10:22,230 --> 00:10:26,910
and trying to figure out exactly what's

00:10:24,480 --> 00:10:28,580
happening inside this black box it is it

00:10:26,910 --> 00:10:31,650
does seem like a black box when you're

00:10:28,580 --> 00:10:35,420
when you're using it okay well thank you

00:10:31,650 --> 00:10:35,420

YouTube URL: https://www.youtube.com/watch?v=1Ms3Vuwr_KM


