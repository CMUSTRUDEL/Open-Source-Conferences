Title: Accelerating TensorFlow for research and deployment (sponsored by NVIDIA) - Ujval Kapasi
Publication date: 2019-11-01
Playlist: TensorFlow World 2019
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:02,840 --> 00:00:10,549
so the tensorflow team with tf2 has

00:00:07,970 --> 00:00:14,090
solved a hard problem which is to make

00:00:10,549 --> 00:00:18,260
it easy for you to easily express your

00:00:14,090 --> 00:00:20,689
ideas and debug them in tensorflow this

00:00:18,260 --> 00:00:23,480
is this is a big step but there are

00:00:20,689 --> 00:00:25,669
additional challenges in order to for

00:00:23,480 --> 00:00:28,400
you to obtain the best results for your

00:00:25,669 --> 00:00:31,039
research or your product designs and I'd

00:00:28,400 --> 00:00:33,440
like to talk about how NVIDIA is solving

00:00:31,039 --> 00:00:36,260
three of these challenges the first is

00:00:33,440 --> 00:00:39,230
simple acceleration the second is

00:00:36,260 --> 00:00:41,390
scaling to large clusters and finally

00:00:39,230 --> 00:00:45,650
providing code for every level every

00:00:41,390 --> 00:00:48,530
step of the deep learning workflow one

00:00:45,650 --> 00:00:50,150
of the ingredients of the recent success

00:00:48,530 --> 00:00:54,050
of deep learning has been the use of

00:00:50,150 --> 00:00:57,590
GPUs for providing the necessary raw

00:00:54,050 --> 00:00:59,840
compute horsepower this compute is is

00:00:57,590 --> 00:01:04,129
like oxygen for new ideas and

00:00:59,840 --> 00:01:06,650
applications in the field of AI so we

00:01:04,129 --> 00:01:09,759
designed and shipped tensor cores in our

00:01:06,650 --> 00:01:11,719
Volta and Turing GPUs in order to

00:01:09,759 --> 00:01:13,820
provide an order of magnitude more

00:01:11,719 --> 00:01:15,170
performance capability compute

00:01:13,820 --> 00:01:18,829
capability than was previously available

00:01:15,170 --> 00:01:21,590
and we built libraries such as KU d and

00:01:18,829 --> 00:01:25,700
n to ensure that all the important math

00:01:21,590 --> 00:01:28,159
functions inside of TF can run on top of

00:01:25,700 --> 00:01:31,420
tensor cores and we update these

00:01:28,159 --> 00:01:33,950
regularly as new algorithms are invented

00:01:31,420 --> 00:01:37,609
we worked with Google to provide a

00:01:33,950 --> 00:01:40,639
simple API so you can from your tensor

00:01:37,609 --> 00:01:42,829
flow script easily activate these

00:01:40,639 --> 00:01:44,810
routines in these libraries and train

00:01:42,829 --> 00:01:46,219
with mixed precision on top of tensor

00:01:44,810 --> 00:01:48,979
cores and get speed ups for your

00:01:46,219 --> 00:01:51,920
training with examples here for instance

00:01:48,979 --> 00:01:55,310
2x to 3x faster which helps you iterate

00:01:51,920 --> 00:01:57,679
faster on your research and also maybe

00:01:55,310 --> 00:02:00,109
within a fixed budget of time get better

00:01:57,679 --> 00:02:02,869
results once you have a trained model

00:02:00,109 --> 00:02:06,380
you can we provide a simple API inside

00:02:02,869 --> 00:02:10,250
of tensor flow to activate tensor RT so

00:02:06,380 --> 00:02:12,769
you can get drastically faster latency

00:02:10,250 --> 00:02:14,510
for serving your predictions which lets

00:02:12,769 --> 00:02:16,250
you deploy you know perhaps more

00:02:14,510 --> 00:02:20,080
sophisticated models or pipeline

00:02:16,250 --> 00:02:22,730
then you would be able to otherwise but

00:02:20,080 --> 00:02:26,210
you know optimizing the performance of a

00:02:22,730 --> 00:02:28,370
single GPU is is not enough and let me

00:02:26,210 --> 00:02:30,920
give you an example so Google last year

00:02:28,370 --> 00:02:32,180
released a model called burg as Jeff

00:02:30,920 --> 00:02:35,000
Dean explained yesterday

00:02:32,180 --> 00:02:36,770
this model blew away the accuracy on a

00:02:35,000 --> 00:02:39,200
variety of language tasks compared to

00:02:36,770 --> 00:02:41,930
any approach or model previous to it but

00:02:39,200 --> 00:02:44,480
on a single GPU it takes months to Train

00:02:41,930 --> 00:02:47,150
even on a server with H GPUs it takes

00:02:44,480 --> 00:02:51,200
more than a week but if you can train

00:02:47,150 --> 00:02:53,770
with 32 servers or 256 GPUs training can

00:02:51,200 --> 00:02:56,120
complete with tension flow in mere hours

00:02:53,770 --> 00:02:58,940
however training at these large scales

00:02:56,120 --> 00:03:00,320
introduces and poses several new

00:02:58,940 --> 00:03:04,640
challenges at every level of the system

00:03:00,320 --> 00:03:06,650
if you don't properly co.design the

00:03:04,640 --> 00:03:09,860
hardware and software and precisely tune

00:03:06,650 --> 00:03:11,690
them then as you add more compute you

00:03:09,860 --> 00:03:14,630
will not get a kamesh rate increase in

00:03:11,690 --> 00:03:17,570
performance and you know I think Nvidia

00:03:14,630 --> 00:03:19,790
is actually ideally uniquely suited to

00:03:17,570 --> 00:03:21,830
solve some of these challenges because

00:03:19,790 --> 00:03:25,190
we're building hardware from the level

00:03:21,830 --> 00:03:28,250
of the GPU to servers to supercomputers

00:03:25,190 --> 00:03:30,739
and we're working on challenges at every

00:03:28,250 --> 00:03:33,019
level on Hardware design software design

00:03:30,739 --> 00:03:36,860
system design and at the boundaries of

00:03:33,019 --> 00:03:38,660
these you know the culmination of a

00:03:36,860 --> 00:03:41,450
bunch of our work on this is the dgx

00:03:38,660 --> 00:03:43,730
super pod and to give you you know to

00:03:41,450 --> 00:03:47,090
put its capabilities sort of in visceral

00:03:43,730 --> 00:03:50,090
terms a team at Nvidia recently was able

00:03:47,090 --> 00:03:52,760
to on the dgx super pod as part of

00:03:50,090 --> 00:03:55,250
project Megatron trained the largest

00:03:52,760 --> 00:03:59,200
language model ever more than 8 billion

00:03:55,250 --> 00:04:02,120
parameters 24 times larger than burg

00:03:59,200 --> 00:04:03,760
another contribution that Nvidia is

00:04:02,120 --> 00:04:07,310
making and what we're working on is

00:04:03,760 --> 00:04:09,890
providing reliable code that anyone from

00:04:07,310 --> 00:04:12,760
individuals to enterprises can build on

00:04:09,890 --> 00:04:16,690
top of Nvidia is doing the hard work of

00:04:12,760 --> 00:04:20,060
optimizing documenting qualifying

00:04:16,690 --> 00:04:22,750
packaging publishing maintaining code

00:04:20,060 --> 00:04:25,010
for a variety of models and use cases

00:04:22,750 --> 00:04:27,760
for every step of the deep learning

00:04:25,010 --> 00:04:30,080
workflow from research to production and

00:04:27,760 --> 00:04:33,500
we're curating this code and making

00:04:30,080 --> 00:04:35,960
available to everyone both at NGC and

00:04:33,500 --> 00:04:37,550
video.com but also other places you know

00:04:35,960 --> 00:04:40,520
where developers might frequent such as

00:04:37,550 --> 00:04:45,020
github and TF hub which you just heard

00:04:40,520 --> 00:04:47,240
about as well so I hope that you know in

00:04:45,020 --> 00:04:49,759
this short time I was able to convey

00:04:47,240 --> 00:04:51,289
some of the problems that Nvidia is

00:04:49,759 --> 00:04:53,509
working on the challenges we're working

00:04:51,289 --> 00:04:56,389
on and how we're making available to the

00:04:53,509 --> 00:04:59,090
tensorflow community along with Google's

00:04:56,389 --> 00:05:01,669
simple API s4 acceleration

00:04:59,090 --> 00:05:04,069
solving scaling challenges putting out

00:05:01,669 --> 00:05:07,550
dgx super pods building with GTX super

00:05:04,069 --> 00:05:09,620
pods and curating code that anyone can

00:05:07,550 --> 00:05:11,719
build on top of for the entire deep

00:05:09,620 --> 00:05:12,949
learning workflow thank you for your

00:05:11,719 --> 00:05:15,099
time I hope you enjoy the rest of the

00:05:12,949 --> 00:05:15,099

YouTube URL: https://www.youtube.com/watch?v=lnIXqGG5j28


