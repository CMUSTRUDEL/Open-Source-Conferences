Title: TensorFlow, open source, and IBM (sponsored by IBM) - Frederick Reiss
Publication date: 2019-10-30
Playlist: TensorFlow World 2019
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,989 --> 00:00:06,999
hello everyone I'm Fred Ryce I work for

00:00:04,330 --> 00:00:10,120
IBM I've been working for IBM since 20

00:00:06,999 --> 00:00:12,820
2006 and I've been contributing to

00:00:10,120 --> 00:00:15,040
tensorflow course since 2017 but my

00:00:12,820 --> 00:00:17,529
primary job at IBM is to serve as tech

00:00:15,040 --> 00:00:20,590
lead for code 8 that's the Center for

00:00:17,529 --> 00:00:22,840
open source data and AI technologies we

00:00:20,590 --> 00:00:25,030
are an open source lab located in

00:00:22,840 --> 00:00:26,650
downtown San Francisco and we work on

00:00:25,030 --> 00:00:31,649
open source technologies that are

00:00:26,650 --> 00:00:35,230
foundational to AI and we have on staff

00:00:31,649 --> 00:00:38,890
44 full-time developers who work only on

00:00:35,230 --> 00:00:40,480
open source software now that's that's a

00:00:38,890 --> 00:00:44,890
lot of developers a lot of open source

00:00:40,480 --> 00:00:48,520
developers or is it well if you look

00:00:44,890 --> 00:00:50,739
across IBM at all of the IBM errs who

00:00:48,520 --> 00:00:53,320
are active contributors to open source

00:00:50,739 --> 00:00:55,719
in that they have committed code to

00:00:53,320 --> 00:00:58,239
github in the last 30 days you'll find

00:00:55,719 --> 00:01:01,420
that there are almost 1,200 IBM errs in

00:00:58,239 --> 00:01:04,089
that category so our 44 developers are

00:01:01,420 --> 00:01:08,799
actually a very small slice of a very

00:01:04,089 --> 00:01:11,619
large pie oh and those numbers they

00:01:08,799 --> 00:01:14,140
don't include Red Hat when we close that

00:01:11,619 --> 00:01:15,970
acquisition earlier this year we more

00:01:14,140 --> 00:01:19,390
than doubled our number of active

00:01:15,970 --> 00:01:22,180
contributors to open source so you can

00:01:19,390 --> 00:01:25,180
see that iBM is really big in open

00:01:22,180 --> 00:01:27,070
source and more and more the bulk of our

00:01:25,180 --> 00:01:29,619
contributions in open in the open are

00:01:27,070 --> 00:01:33,909
going towards the foundations of AI and

00:01:29,619 --> 00:01:38,020
when I say AI I mean AI in production I

00:01:33,909 --> 00:01:41,040
mean AI at scale AI at scale is not an

00:01:38,020 --> 00:01:43,750
algorithm it's not a tool it's a process

00:01:41,040 --> 00:01:46,450
it's a process that starts with data and

00:01:43,750 --> 00:01:48,729
then that data turns into features and

00:01:46,450 --> 00:01:50,680
those features train models and those

00:01:48,729 --> 00:01:52,630
models get deployed in applications and

00:01:50,680 --> 00:01:53,890
those applications produce more data and

00:01:52,630 --> 00:01:57,400
the whole thing starts all over again

00:01:53,890 --> 00:02:01,869
and at the core of this process is an

00:01:57,400 --> 00:02:04,770
ecosystem of open source software and at

00:02:01,869 --> 00:02:07,780
the core of this ecosystem is tensorflow

00:02:04,770 --> 00:02:09,879
which is why I'm here on behalf of IBM

00:02:07,780 --> 00:02:12,309
open source to welcome you to tensor for

00:02:09,879 --> 00:02:14,260
the world now throughout this conference

00:02:12,309 --> 00:02:15,790
you're going to see talks that speak to

00:02:14,260 --> 00:02:18,879
all of the difference

00:02:15,790 --> 00:02:20,109
ages of this AI lifecycle but I think

00:02:18,879 --> 00:02:23,920
you're going to see a special emphasis

00:02:20,109 --> 00:02:26,469
on this part moving models into

00:02:23,920 --> 00:02:28,540
production and one of the most important

00:02:26,469 --> 00:02:30,489
aspects of moving models into production

00:02:28,540 --> 00:02:32,439
is that when your model gets deployed in

00:02:30,489 --> 00:02:34,930
a real-world application it's going to

00:02:32,439 --> 00:02:38,170
start having effects on the real world

00:02:34,930 --> 00:02:40,510
and it becomes important to ensure that

00:02:38,170 --> 00:02:42,970
those effects are positive and that

00:02:40,510 --> 00:02:47,019
they're fair to your clients to your

00:02:42,970 --> 00:02:48,819
users now it I B M here's a hypothetical

00:02:47,019 --> 00:02:50,889
example that our researchers put

00:02:48,819 --> 00:02:53,379
together about a little over a year ago

00:02:50,889 --> 00:02:56,049
they've took some real medical records

00:02:53,379 --> 00:02:57,700
data and they produced a model that

00:02:56,049 --> 00:02:59,319
predicts which patients are more likely

00:02:57,700 --> 00:03:03,400
to get sick and therefore should get

00:02:59,319 --> 00:03:05,169
additional as screening and they showed

00:03:03,400 --> 00:03:06,939
that if you naively train this model you

00:03:05,169 --> 00:03:10,030
end up with a model that has significant

00:03:06,939 --> 00:03:11,829
racial bias but that by deploying

00:03:10,030 --> 00:03:13,930
state-of-the-art techniques to adjust

00:03:11,829 --> 00:03:16,030
the data set and the process of making

00:03:13,930 --> 00:03:17,680
the model they could substantially

00:03:16,030 --> 00:03:20,169
reduce this model to produce a model

00:03:17,680 --> 00:03:23,500
reduce this bias to produce a model that

00:03:20,169 --> 00:03:25,209
is much less much more fair you can see

00:03:23,500 --> 00:03:27,099
a jupiter notebook with the entire

00:03:25,209 --> 00:03:30,040
scenario from end to end including code

00:03:27,099 --> 00:03:32,979
and equations and results at the URL

00:03:30,040 --> 00:03:36,159
down here again i need to emphasize this

00:03:32,979 --> 00:03:37,780
was a hypothetical example we we built a

00:03:36,159 --> 00:03:40,840
flawed model deliberately so we could

00:03:37,780 --> 00:03:42,659
show how to make it better but no

00:03:40,840 --> 00:03:46,959
patients were harmed in this exercise

00:03:42,659 --> 00:03:48,669
however last friday i sat down with my

00:03:46,959 --> 00:03:51,159
morning coffee and I opened up the Wall

00:03:48,669 --> 00:03:53,620
Street Journal and I saw this article at

00:03:51,159 --> 00:03:56,199
the bottom of page three describing a

00:03:53,620 --> 00:03:58,840
scenario eerily similar to our

00:03:56,199 --> 00:04:01,629
hypothetical you know when your

00:03:58,840 --> 00:04:04,949
hypothetical starts showing up as new

00:04:01,629 --> 00:04:07,540
papers headlines that's kind of scary

00:04:04,949 --> 00:04:09,939
and I think it is incumbent upon us as

00:04:07,540 --> 00:04:12,430
an industry to move forward the process

00:04:09,939 --> 00:04:15,040
the the technology of trust in AI

00:04:12,430 --> 00:04:17,470
trust and transparency in AI which is

00:04:15,040 --> 00:04:19,329
why IBM and IBM Research have released

00:04:17,470 --> 00:04:22,570
our toolkits of state-of-the-art

00:04:19,329 --> 00:04:25,719
algorithms in this space as open source

00:04:22,570 --> 00:04:28,510
under AI fairness 360 a I explain

00:04:25,719 --> 00:04:29,650
ability 360 an adversarial robustness

00:04:28,510 --> 00:04:31,930
00:04:29,650 --> 00:04:33,760
it is also why IBM is working with other

00:04:31,930 --> 00:04:37,180
members of the Linux Foundation a I

00:04:33,760 --> 00:04:39,130
trusted a I committee to move forward

00:04:37,180 --> 00:04:41,260
open standards in this area so that we

00:04:39,130 --> 00:04:44,050
can all move quickly more quickly to

00:04:41,260 --> 00:04:45,880
trusted AI now if you'd like to hear

00:04:44,050 --> 00:04:47,530
more on this topic my colleague Animesh

00:04:45,880 --> 00:04:50,530
singh will be giving a talk this

00:04:47,530 --> 00:04:53,020
afternoon at 140 on trusted AI for the

00:04:50,530 --> 00:04:54,870
full 40 minute session also I'd like to

00:04:53,020 --> 00:04:56,949
give a quick shout out to my other

00:04:54,870 --> 00:04:59,229
co-workers from code 8 who have come

00:04:56,949 --> 00:05:01,419
down here to show you cool open-source

00:04:59,229 --> 00:05:04,389
demos at the IBM booth that's booth 201

00:05:01,419 --> 00:05:07,389
I also check out our websites developer

00:05:04,389 --> 00:05:09,400
ibm.com and code eight org on behalf of

00:05:07,389 --> 00:05:11,260
IBM I'd like to welcome you all to

00:05:09,400 --> 00:05:14,580
attend sir for the world enjoy the

00:05:11,260 --> 00:05:14,580
conference thank you

00:05:20,569 --> 00:05:22,629

YouTube URL: https://www.youtube.com/watch?v=JAx9IHywDHA


