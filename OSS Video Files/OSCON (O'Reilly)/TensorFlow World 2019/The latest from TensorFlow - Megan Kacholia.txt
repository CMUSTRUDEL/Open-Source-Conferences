Title: The latest from TensorFlow - Megan Kacholia
Publication date: 2019-10-30
Playlist: TensorFlow World 2019
Description: 
	Megan Kacholia outlines the latest TensorFlow product announcements and updates. You'll learn more about how Google's latest innovations provide a comprehensive ecosystem of tools for developers, enterprises, and researchers who want to push state-of-the-art machine learning and build scalable ML-powered applications.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,839 --> 00:00:09,219
hey everyone good morning I just want to

00:00:05,710 --> 00:00:11,049
say first of all welcome today I want to

00:00:09,219 --> 00:00:13,240
talk a little bit about tensorflow 2.0

00:00:11,049 --> 00:00:14,830
and some of the new updates that we have

00:00:13,240 --> 00:00:15,430
they're gonna make your experience with

00:00:14,830 --> 00:00:19,480
tensorflow

00:00:15,430 --> 00:00:21,580
even better but before I dive into a lot

00:00:19,480 --> 00:00:23,800
of those details I want to start off by

00:00:21,580 --> 00:00:25,359
thanking you everyone here everyone on

00:00:23,800 --> 00:00:26,560
the livestream everyone who's been

00:00:25,359 --> 00:00:28,619
contributing to tensorflow

00:00:26,560 --> 00:00:31,210
all of you who make up the community

00:00:28,619 --> 00:00:33,340
tensorflow was open source to help

00:00:31,210 --> 00:00:35,920
accelerate the AI field for everyone

00:00:33,340 --> 00:00:37,540
you've used it in your experiments

00:00:35,920 --> 00:00:39,370
you've deployed it in your businesses

00:00:37,540 --> 00:00:41,440
you've made some amazing different

00:00:39,370 --> 00:00:43,630
applications that were so excited to

00:00:41,440 --> 00:00:45,190
showcase and talk about some that we get

00:00:43,630 --> 00:00:46,810
to see a bit here today which is one of

00:00:45,190 --> 00:00:49,180
my favorite parts about conferences like

00:00:46,810 --> 00:00:51,610
this and you've done so much more and

00:00:49,180 --> 00:00:53,890
all of this has helped make tensorflow

00:00:51,610 --> 00:00:56,590
what it is today it's the most popular

00:00:53,890 --> 00:00:58,540
ml ecosystem in the world and honestly

00:00:56,590 --> 00:01:00,580
that would not happen without the

00:00:58,540 --> 00:01:03,040
community being excited and embracing

00:01:00,580 --> 00:01:04,809
and using this and giving back so on

00:01:03,040 --> 00:01:07,060
behalf of the entire tensorflow team

00:01:04,809 --> 00:01:09,369
I really just first want to say thank

00:01:07,060 --> 00:01:11,950
you because it's so amazing to see how

00:01:09,369 --> 00:01:13,479
tensorflow is used that's one of the

00:01:11,950 --> 00:01:15,130
greatest things I get to see about my

00:01:13,479 --> 00:01:17,150
job is the applications and the way

00:01:15,130 --> 00:01:19,580
folks or using tensorflow

00:01:17,150 --> 00:01:21,110
I want to take a step back and talk a

00:01:19,580 --> 00:01:23,600
little bit about some of the different

00:01:21,110 --> 00:01:26,150
user groups and how we see them making

00:01:23,600 --> 00:01:28,220
use of tensor flow tensor flow is being

00:01:26,150 --> 00:01:30,470
used across a wide range of experiments

00:01:28,220 --> 00:01:32,600
and applications so here calling out

00:01:30,470 --> 00:01:34,400
researchers data scientists and

00:01:32,600 --> 00:01:38,120
developers and there's other groups kind

00:01:34,400 --> 00:01:40,040
of in between as well researchers use it

00:01:38,120 --> 00:01:41,900
because it's flexible it's flexible

00:01:40,040 --> 00:01:43,850
enough to experiment with and push the

00:01:41,900 --> 00:01:45,290
state of the art and deep learning you

00:01:43,850 --> 00:01:46,910
heard this even just a few minutes ago

00:01:45,290 --> 00:01:48,590
with folks from Twitter talking about

00:01:46,910 --> 00:01:50,990
how they're able to use tensor flow and

00:01:48,590 --> 00:01:52,520
expand on top of it in order to do some

00:01:50,990 --> 00:01:55,070
of the amazing things that they want to

00:01:52,520 --> 00:01:56,960
make use of on their own platform and at

00:01:55,070 --> 00:01:58,940
Google we see examples of this when

00:01:56,960 --> 00:02:00,740
researchers are creating advanced models

00:01:58,940 --> 00:02:02,420
like Excel net and some of the other

00:02:00,740 --> 00:02:04,810
things that Jeff referenced in his talk

00:02:02,420 --> 00:02:04,810
earlier

00:02:05,140 --> 00:02:09,259
taking a step forward looking at data

00:02:07,220 --> 00:02:11,150
scientists data scientists and

00:02:09,259 --> 00:02:13,670
enterprise engineers have said they rely

00:02:11,150 --> 00:02:15,980
on tensorflow for performance and scale

00:02:13,670 --> 00:02:17,480
in training and production environments

00:02:15,980 --> 00:02:19,340
that's one of the big things about

00:02:17,480 --> 00:02:21,380
tensorflow that we've always emphasized

00:02:19,340 --> 00:02:22,940
and looked at from the beginning how can

00:02:21,380 --> 00:02:25,600
we make sure this can scale to large

00:02:22,940 --> 00:02:28,550
production use cases for example

00:02:25,600 --> 00:02:31,400
quantify in Blackrock use tensor flow to

00:02:28,550 --> 00:02:33,980
test and deploy Bert in real-world NLP

00:02:31,400 --> 00:02:37,170
instances such as text tokenization as

00:02:33,980 --> 00:02:39,370
well as classification

00:02:37,170 --> 00:02:42,070
hopping one step forward looking a bit

00:02:39,370 --> 00:02:43,690
at application developers application

00:02:42,070 --> 00:02:45,460
developers use tensorflow because it's

00:02:43,690 --> 00:02:47,950
easy to learn ml on the platforms that

00:02:45,460 --> 00:02:50,230
they care about our do we know wants to

00:02:47,950 --> 00:02:52,360
make a metal simple on microcontrollers

00:02:50,230 --> 00:02:54,730
so they rely on tensorflow pre-trained

00:02:52,360 --> 00:02:57,250
models and tensorflow light micro for

00:02:54,730 --> 00:02:58,660
deployment each of these groups is a

00:02:57,250 --> 00:03:01,020
critical part of the tensorflow

00:02:58,660 --> 00:03:03,420
ecosystem

00:03:01,020 --> 00:03:05,610
and this is why we really wanted to make

00:03:03,420 --> 00:03:07,860
sure that tensorflow 2.0 works for

00:03:05,610 --> 00:03:09,990
everyone we announced the Alpha at our

00:03:07,860 --> 00:03:11,790
dev summit earlier this year and over

00:03:09,990 --> 00:03:13,770
the past few months the team has been

00:03:11,790 --> 00:03:15,240
working very hard to incorporate early

00:03:13,770 --> 00:03:16,800
feedback again thank you to the

00:03:15,240 --> 00:03:18,360
community for giving us that early

00:03:16,800 --> 00:03:19,740
feedback so we can make sure we're

00:03:18,360 --> 00:03:21,660
developing something that works well for

00:03:19,740 --> 00:03:23,700
you and we've been working to resolve

00:03:21,660 --> 00:03:26,610
bugs and issues and things like that and

00:03:23,700 --> 00:03:28,590
just last month in September we were

00:03:26,610 --> 00:03:32,690
excited to announce the final general

00:03:28,590 --> 00:03:34,790
release for tensorflow to dot Oh

00:03:32,690 --> 00:03:36,800
you might be familiar with Tenzer flows

00:03:34,790 --> 00:03:38,660
architecture which has always supported

00:03:36,800 --> 00:03:40,370
the ml lifecycle from training through

00:03:38,660 --> 00:03:41,750
deployment again one of the things we've

00:03:40,370 --> 00:03:43,550
emphasized since the beginning when

00:03:41,750 --> 00:03:45,650
tensorflow was initially open sourced a

00:03:43,550 --> 00:03:48,260
few years ago but I want to emphasize

00:03:45,650 --> 00:03:51,820
how tensorflow 2.0 makes this workflow

00:03:48,260 --> 00:03:51,820
even easier and more intuitive

00:03:52,190 --> 00:03:57,350
first we invested in Karis and using an

00:03:55,940 --> 00:04:00,370
easy-to-use package and tensorflow

00:03:57,350 --> 00:04:02,720
making it the default high level API

00:04:00,370 --> 00:04:05,030
many developers love carrots because

00:04:02,720 --> 00:04:06,380
it's easy to use and understand again

00:04:05,030 --> 00:04:07,790
you heard this already mentioned a

00:04:06,380 --> 00:04:09,140
little bit earlier and hopefully we'll

00:04:07,790 --> 00:04:12,020
hear more about it throughout the next

00:04:09,140 --> 00:04:14,570
few days by tightly integrating Kerris

00:04:12,020 --> 00:04:17,060
into 2.0 we can make Karis work even

00:04:14,570 --> 00:04:18,739
better with primitives like TF data we

00:04:17,060 --> 00:04:21,080
can do performance optimizations behind

00:04:18,739 --> 00:04:23,540
the scenes and run distributed training

00:04:21,080 --> 00:04:25,670
again we really wanted to dotto to focus

00:04:23,540 --> 00:04:27,860
on usability how can we make it easier

00:04:25,670 --> 00:04:29,780
for developers how can we make it easier

00:04:27,860 --> 00:04:32,110
for users to get what they need out of

00:04:29,780 --> 00:04:34,659
tensorflow

00:04:32,110 --> 00:04:37,210
for instance lose it a customized weight

00:04:34,659 --> 00:04:39,189
loss app so they use TF Kerris for

00:04:37,210 --> 00:04:40,900
designing their network by leveraging

00:04:39,189 --> 00:04:43,360
mirrored strategy distribution into

00:04:40,900 --> 00:04:46,240
dotto they were able to utilize the full

00:04:43,360 --> 00:04:49,090
power of their GPUs its feed black like

00:04:46,240 --> 00:04:50,770
this that we love to hear and again it's

00:04:49,090 --> 00:04:52,689
very important for us to know how the

00:04:50,770 --> 00:04:54,490
community is making use of things how

00:04:52,689 --> 00:04:56,229
the community is using to ATO the things

00:04:54,490 --> 00:04:57,639
they want to see so that we can make

00:04:56,229 --> 00:04:59,319
sure we're developing the right

00:04:57,639 --> 00:05:01,410
framework and also make sure you can

00:04:59,319 --> 00:05:04,090
contribute back

00:05:01,410 --> 00:05:06,220
when you need a bit more control to

00:05:04,090 --> 00:05:08,410
create advanced algorithms tirado comes

00:05:06,220 --> 00:05:10,750
fully loaded with eager execution making

00:05:08,410 --> 00:05:13,270
it familiar for Python developers this

00:05:10,750 --> 00:05:15,940
is especially useful when you're

00:05:13,270 --> 00:05:17,080
stepping through doing debugging making

00:05:15,940 --> 00:05:19,509
sure you can really understand

00:05:17,080 --> 00:05:21,130
step-by-step what's happening this also

00:05:19,509 --> 00:05:23,050
means there's less coding required when

00:05:21,130 --> 00:05:25,599
training your model all without having

00:05:23,050 --> 00:05:29,440
to use session that run again usability

00:05:25,599 --> 00:05:30,639
is a focus to demonstrate the power of

00:05:29,440 --> 00:05:32,349
training models with to do

00:05:30,639 --> 00:05:34,720
I'll show you how you can train a

00:05:32,349 --> 00:05:37,120
state-of-the-art NLP model in ten lines

00:05:34,720 --> 00:05:39,069
of code using the Transformers NLP

00:05:37,120 --> 00:05:41,409
library by hugging face again a

00:05:39,069 --> 00:05:43,270
community contribution this popular

00:05:41,409 --> 00:05:46,680
package hosts some of the most advanced

00:05:43,270 --> 00:05:49,840
NLP models available today like Bert GPT

00:05:46,680 --> 00:05:52,590
transformer Excel Excel net and now

00:05:49,840 --> 00:05:55,950
supports tensor flow to dotto

00:05:52,590 --> 00:05:57,930
so let's take a look here kind of just

00:05:55,950 --> 00:05:59,760
looking through the code you can see how

00:05:57,930 --> 00:06:01,530
you can use tirado to Train hugging

00:05:59,760 --> 00:06:03,900
faces distill Bert model for text

00:06:01,530 --> 00:06:06,180
classification you can see you just

00:06:03,900 --> 00:06:09,180
simply load the tokenizer model and the

00:06:06,180 --> 00:06:11,790
data set then prepare the data set and

00:06:09,180 --> 00:06:15,180
use TF Kerris compile and fit API s--

00:06:11,790 --> 00:06:17,810
and with a few lines of code I can now

00:06:15,180 --> 00:06:20,210
train my model

00:06:17,810 --> 00:06:21,830
and with just a few more lines we can

00:06:20,210 --> 00:06:24,020
use the train model for tasks such as

00:06:21,830 --> 00:06:24,820
text classification using eager

00:06:24,020 --> 00:06:27,260
execution

00:06:24,820 --> 00:06:28,760
again it's examples like this where we

00:06:27,260 --> 00:06:30,680
can see how the community takes

00:06:28,760 --> 00:06:32,930
something and is able to do something

00:06:30,680 --> 00:06:34,490
very exciting and amazing by making use

00:06:32,930 --> 00:06:37,780
of the platform and the ecosystem that

00:06:34,490 --> 00:06:37,780
tensorflow is providing

00:06:37,950 --> 00:06:42,810
but building and training a model is

00:06:40,200 --> 00:06:44,970
only one part of tensorflow 2.0 you need

00:06:42,810 --> 00:06:46,560
the performance to match that's why we

00:06:44,970 --> 00:06:49,080
worked hard to continue to prove

00:06:46,560 --> 00:06:51,240
performance with tensorflow to dato it

00:06:49,080 --> 00:06:53,010
delivers up to 3x faster training

00:06:51,240 --> 00:06:56,250
performance using mixed precision on

00:06:53,010 --> 00:06:58,110
nvidia Volta and touring GPUs in a few

00:06:56,250 --> 00:07:01,410
lines of code with models like resonant

00:06:58,110 --> 00:07:03,870
50 and Bert as we continue to double

00:07:01,410 --> 00:07:06,180
down on 2.0 in the future performance

00:07:03,870 --> 00:07:08,730
will remain a focus with more models and

00:07:06,180 --> 00:07:11,010
with hardware accelerators for example

00:07:08,730 --> 00:07:14,010
in 2.1 so the next upcoming tensorflow

00:07:11,010 --> 00:07:16,110
release you can expect TPU and TPU pods

00:07:14,010 --> 00:07:18,420
support along with mix precision for

00:07:16,110 --> 00:07:20,340
GPUs so performance is something that

00:07:18,420 --> 00:07:22,530
we're keeping a focus on as well while

00:07:20,340 --> 00:07:25,100
also making sure usability really stands

00:07:22,530 --> 00:07:25,100
to the forefront

00:07:25,250 --> 00:07:29,480
but there's a lot more to the ecosystem

00:07:27,230 --> 00:07:31,310
so beyond model building and performance

00:07:29,480 --> 00:07:32,570
there are many other pieces that help

00:07:31,310 --> 00:07:35,390
round out the tensorflow

00:07:32,570 --> 00:07:37,520
ecosystem add-ons and extensions are a

00:07:35,390 --> 00:07:39,590
very important piece here which is why

00:07:37,520 --> 00:07:42,350
we wanted to make sure that they're also

00:07:39,590 --> 00:07:44,090
compatible with tensorflow - dotto so

00:07:42,350 --> 00:07:45,650
you can use popular libraries like some

00:07:44,090 --> 00:07:48,770
other ones called out here whether it's

00:07:45,650 --> 00:07:51,860
tensorflow probability TF agents or TF

00:07:48,770 --> 00:07:54,110
text we've also introduced a host of new

00:07:51,860 --> 00:07:57,140
libraries to help researchers and email

00:07:54,110 --> 00:07:59,210
practitioners in more useful ways so for

00:07:57,140 --> 00:08:01,340
example neural structure learning helps

00:07:59,210 --> 00:08:03,650
to train neural networks with structured

00:08:01,340 --> 00:08:05,840
signals and the new fairness indicators

00:08:03,650 --> 00:08:08,390
add-on enables regular computation and

00:08:05,840 --> 00:08:10,610
visualization of fairness metrics and

00:08:08,390 --> 00:08:12,380
these are just the types of things that

00:08:10,610 --> 00:08:14,480
you can see kind of as part of the

00:08:12,380 --> 00:08:16,310
tensorflow ecosystem these add-ons that

00:08:14,480 --> 00:08:18,080
again can help you make sure you're able

00:08:16,310 --> 00:08:21,740
to do the things you need to do not with

00:08:18,080 --> 00:08:23,940
your models but kind of beyond just that

00:08:21,740 --> 00:08:26,010
another valuable aspect of the

00:08:23,940 --> 00:08:29,340
tensorflow ecosystem is being able to

00:08:26,010 --> 00:08:31,140
analyze your ml experiments in detail so

00:08:29,340 --> 00:08:32,789
this is showing tensor board tensor

00:08:31,140 --> 00:08:34,140
board as tensor flows visualization

00:08:32,789 --> 00:08:35,039
toolkit which is what helps you

00:08:34,140 --> 00:08:37,229
accomplish this

00:08:35,039 --> 00:08:39,270
it's a popular tool among researchers

00:08:37,229 --> 00:08:41,550
and email practitioners for tracking

00:08:39,270 --> 00:08:44,280
metrics visualizing model graphs and

00:08:41,550 --> 00:08:46,320
parameters and much more it's very

00:08:44,280 --> 00:08:48,390
interesting that we've seen users enjoy

00:08:46,320 --> 00:08:50,070
tensor boards so much it'll even take

00:08:48,390 --> 00:08:51,660
screenshots of their experiments and

00:08:50,070 --> 00:08:53,070
then use those screenshots to be able to

00:08:51,660 --> 00:08:55,120
share with others what they're doing

00:08:53,070 --> 00:08:57,670
with tensor flow

00:08:55,120 --> 00:08:59,319
this type of sharing and collaboration

00:08:57,670 --> 00:09:01,209
in the email community is something we

00:08:59,319 --> 00:09:03,699
really want to encourage with tensorflow

00:09:01,209 --> 00:09:05,649
again there's so much that can happen by

00:09:03,699 --> 00:09:07,990
enabling the community to do good things

00:09:05,649 --> 00:09:10,809
that's why I'm excited to share the

00:09:07,990 --> 00:09:13,449
preview of tents aboard dev a new free

00:09:10,809 --> 00:09:15,160
managed tensor board experience that

00:09:13,449 --> 00:09:17,920
lets you upload and share your ml

00:09:15,160 --> 00:09:20,110
experiment results with anyone you'll

00:09:17,920 --> 00:09:22,629
now be able to host and track your ml

00:09:20,110 --> 00:09:25,329
experiments and share them publicly no

00:09:22,629 --> 00:09:28,120
setup required simply upload your logs

00:09:25,329 --> 00:09:30,279
and then share the URL so that others

00:09:28,120 --> 00:09:31,300
can see the experiments and see the

00:09:30,279 --> 00:09:31,839
things that you are doing with

00:09:31,300 --> 00:09:34,809
tensorflow

00:09:31,839 --> 00:09:37,449
as a preview we're starting off with a

00:09:34,809 --> 00:09:39,670
scalers dashboard but over time we'll be

00:09:37,449 --> 00:09:43,680
adding a lot more functionality to make

00:09:39,670 --> 00:09:43,680
the sharing experience even better

00:09:44,660 --> 00:09:47,900
but if you're not looking to build

00:09:46,190 --> 00:09:50,450
models from scratch and want to reduce

00:09:47,900 --> 00:09:52,340
some computational cost tensorflow has

00:09:50,450 --> 00:09:54,980
always made pre-trained models available

00:09:52,340 --> 00:09:56,870
through tensorflow hub and today we're

00:09:54,980 --> 00:09:59,000
excited to share an improved experience

00:09:56,870 --> 00:10:00,740
of tensorflow hub that's much more

00:09:59,000 --> 00:10:02,840
intuitive where you can find a

00:10:00,740 --> 00:10:06,660
comprehensive repository of pre trained

00:10:02,840 --> 00:10:08,940
models in the tensor flow ecosystem

00:10:06,660 --> 00:10:12,240
this means you can find models like Bert

00:10:08,940 --> 00:10:14,910
and others related to image text video

00:10:12,240 --> 00:10:17,600
and more that are ready to use with

00:10:14,910 --> 00:10:19,800
tensorflow light and tensorflow j/s

00:10:17,600 --> 00:10:21,960
again we wanted to make sure the

00:10:19,800 --> 00:10:23,610
experience here is vastly improved to

00:10:21,960 --> 00:10:25,830
make it easier for you to find what you

00:10:23,610 --> 00:10:28,340
need in order to more quickly get to the

00:10:25,830 --> 00:10:28,340
task at hand

00:10:28,889 --> 00:10:32,970
and since tensorflow is driven by all of

00:10:30,629 --> 00:10:34,679
you tensorflow hub is hosting more treat

00:10:32,970 --> 00:10:37,439
pre train models from the community

00:10:34,679 --> 00:10:40,079
you'll be able to find curated models by

00:10:37,439 --> 00:10:43,170
deep mine Google Microsoft's AI for

00:10:40,079 --> 00:10:43,949
Earth and Nvidia ready to use today with

00:10:43,170 --> 00:10:46,079
many more to come

00:10:43,949 --> 00:10:47,850
we want to make sure that tensorflow hub

00:10:46,079 --> 00:10:49,799
is a great place to find some of these

00:10:47,850 --> 00:10:51,449
excellent pre train models and again

00:10:49,799 --> 00:10:53,100
there's so much the community is doing

00:10:51,449 --> 00:10:55,759
we want to be able to showcase those

00:10:53,100 --> 00:10:55,759
models as well

00:10:56,930 --> 00:11:01,189
tensorflow to tato also highlights

00:10:58,999 --> 00:11:03,170
tensor flows core strengths and areas of

00:11:01,189 --> 00:11:06,050
focus which is being able to go from

00:11:03,170 --> 00:11:07,999
model building experimentation through

00:11:06,050 --> 00:11:10,999
to production no matter what platform

00:11:07,999 --> 00:11:13,129
you work on you can deploy into in email

00:11:10,999 --> 00:11:16,369
pipelines but tensorflow extended or

00:11:13,129 --> 00:11:18,230
tf-x you can use your models on mobile

00:11:16,369 --> 00:11:20,839
and embedded devices with tensor flow

00:11:18,230 --> 00:11:22,550
light for on device inference and you

00:11:20,839 --> 00:11:25,420
can train and run models in the browser

00:11:22,550 --> 00:11:27,649
or nodejs with tensor flow Tijs

00:11:25,420 --> 00:11:29,329
you'll learn more about what's new in

00:11:27,649 --> 00:11:32,319
tensor flow in production during the

00:11:29,329 --> 00:11:32,319
keynote sessions tomorrow

00:11:32,740 --> 00:11:37,150
you can learn more about these updates

00:11:35,080 --> 00:11:38,850
by going to tensorflow org where you'll

00:11:37,150 --> 00:11:41,500
also find the latest documentation

00:11:38,850 --> 00:11:43,780
examples and tutorials for two dotto

00:11:41,500 --> 00:11:45,490
again we want to make sure it's easy for

00:11:43,780 --> 00:11:48,040
the community to see what's happening

00:11:45,490 --> 00:11:49,380
what's new and enable you to just do

00:11:48,040 --> 00:11:51,610
what you need to do with tensorflow

00:11:49,380 --> 00:11:53,860
we've been thrilled to see the positive

00:11:51,610 --> 00:11:55,180
response to do a two to zero and we hope

00:11:53,860 --> 00:11:57,850
you continue to share your feedback

00:11:55,180 --> 00:12:00,480
thank you and I hope you enjoy the rest

00:11:57,850 --> 00:12:00,480

YouTube URL: https://www.youtube.com/watch?v=n56syJSLouA


