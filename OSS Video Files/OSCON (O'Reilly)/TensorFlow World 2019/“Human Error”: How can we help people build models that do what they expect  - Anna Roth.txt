Title: “Human Error”: How can we help people build models that do what they expect  - Anna Roth
Publication date: 2019-11-01
Playlist: TensorFlow World 2019
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:02,409 --> 00:00:06,819
so the world is full of experts right

00:00:05,080 --> 00:00:09,610
like pathologists who can diagnose

00:00:06,819 --> 00:00:12,190
diseases construction workers who know

00:00:09,610 --> 00:00:13,840
that if a certain tube is more than 40%

00:00:12,190 --> 00:00:16,570
obstructed you have to turn that machine

00:00:13,840 --> 00:00:17,679
off like right now people who work in

00:00:16,570 --> 00:00:20,619
support and know how to like kind of

00:00:17,679 --> 00:00:22,509
triage tickets and we do one of the

00:00:20,619 --> 00:00:23,650
exciting things about kind of the past

00:00:22,509 --> 00:00:26,230
few years is that it's become

00:00:23,650 --> 00:00:28,630
increasingly easy for people who want to

00:00:26,230 --> 00:00:30,909
take something they know how to do and

00:00:28,630 --> 00:00:32,470
teach it to a machine I think the big

00:00:30,909 --> 00:00:34,120
dream is real that anybody could be able

00:00:32,470 --> 00:00:35,650
to go and do that it's what I spent my

00:00:34,120 --> 00:00:36,940
time on in the past few years and worked

00:00:35,650 --> 00:00:39,310
on the team that launched cognitive

00:00:36,940 --> 00:00:41,800
services and I spent the past few years

00:00:39,310 --> 00:00:43,780
working on custom vision AI it's a tool

00:00:41,800 --> 00:00:47,410
for building image classifiers and

00:00:43,780 --> 00:00:49,570
object detectors but you know it really

00:00:47,410 --> 00:00:51,070
has never been easier to build machine

00:00:49,570 --> 00:00:52,510
learning models right like the tooling

00:00:51,070 --> 00:00:55,300
is really good real here at tensorflow

00:00:52,510 --> 00:00:57,490
world computational techniques have

00:00:55,300 --> 00:00:59,980
gotten faster you know transfer learning

00:00:57,490 --> 00:01:02,829
easier to use access to compute in the

00:00:59,980 --> 00:01:04,180
cloud and then educational materials

00:01:02,829 --> 00:01:05,829
have like never been better

00:01:04,180 --> 00:01:08,200
right one of my hobbies is to go and

00:01:05,829 --> 00:01:09,850
like browse the fast at AI forums just

00:01:08,200 --> 00:01:12,159
to see what learners are building and

00:01:09,850 --> 00:01:14,350
it's completely inspiring that being

00:01:12,159 --> 00:01:16,569
said it's actually so really hard to

00:01:14,350 --> 00:01:18,719
build a machine learning model in

00:01:16,569 --> 00:01:20,560
particular it's hard to build robust

00:01:18,719 --> 00:01:21,819
production-ready models so I've worked

00:01:20,560 --> 00:01:24,159
with hundreds it's actually at this

00:01:21,819 --> 00:01:26,319
point thousands of customers who are

00:01:24,159 --> 00:01:30,009
trying to automate some particular tasks

00:01:26,319 --> 00:01:32,770
but a lot of these projects fail um you

00:01:30,009 --> 00:01:34,329
know it's it's really easy to build your

00:01:32,770 --> 00:01:35,889
first model and sometimes it's actually

00:01:34,329 --> 00:01:38,229
kind of a trick right like you can get

00:01:35,889 --> 00:01:40,829
something astonishingly good in a couple

00:01:38,229 --> 00:01:43,149
of minutes you get some data off the web

00:01:40,829 --> 00:01:44,590
like a few minutes later I have a model

00:01:43,149 --> 00:01:48,520
that like does something and it's kind

00:01:44,590 --> 00:01:50,499
of uncanny but getting that to be robust

00:01:48,520 --> 00:01:53,490
enough to use kind of in a real

00:01:50,499 --> 00:01:56,409
environment it's actually really tough

00:01:53,490 --> 00:01:58,179
um so the first problem people run into

00:01:56,409 --> 00:02:00,340
is actually hard to transfer your

00:01:58,179 --> 00:02:02,170
knowledge to a machine so like this

00:02:00,340 --> 00:02:04,149
might seem trite right but when people

00:02:02,170 --> 00:02:06,369
first train object detectors actually a

00:02:04,149 --> 00:02:09,209
lot of people don't put bounding boxes

00:02:06,369 --> 00:02:11,740
around every single object doesn't work

00:02:09,209 --> 00:02:13,660
or they get stuck on like how kind of

00:02:11,740 --> 00:02:15,920
the kind of parsimonious this so for

00:02:13,660 --> 00:02:17,930
example and one guy you know it's yeah

00:02:15,920 --> 00:02:20,300
people like the Seahawks wanted to train

00:02:17,930 --> 00:02:21,980
a Seahawks class a detector puts

00:02:20,300 --> 00:02:24,500
bounding boxes about around a bunch of

00:02:21,980 --> 00:02:26,090
football players and discovers that he's

00:02:24,500 --> 00:02:28,459
actually really kind of built a football

00:02:26,090 --> 00:02:29,780
person detector as opposed to a Seahawks

00:02:28,459 --> 00:02:31,540
detector like it's what really upset

00:02:29,780 --> 00:02:33,620
when he kind of uploads another

00:02:31,540 --> 00:02:34,880
information from another team because

00:02:33,620 --> 00:02:38,239
the model didn't have that semantic

00:02:34,880 --> 00:02:40,430
knowledge that the user had and so like

00:02:38,239 --> 00:02:42,050
this is stuff you can document away

00:02:40,430 --> 00:02:44,810
right like you can kind of learn this in

00:02:42,050 --> 00:02:45,680
your first hour or so but it's to the

00:02:44,810 --> 00:02:47,480
unnaturalness

00:02:45,680 --> 00:02:48,800
of the way in which we train models

00:02:47,480 --> 00:02:50,480
today like when you teach something to a

00:02:48,800 --> 00:02:51,950
computer you're having to kind of give

00:02:50,480 --> 00:02:53,870
it data that represents in some way

00:02:51,950 --> 00:02:55,160
distribution that's not how you and I

00:02:53,870 --> 00:02:58,069
would normally teach something and it

00:02:55,160 --> 00:02:59,750
really kind of trips people up a lot but

00:02:58,069 --> 00:03:00,830
sure say you brought that you figure it

00:02:59,750 --> 00:03:03,110
out you figure out all right the problem

00:03:00,830 --> 00:03:05,209
is building a data set that's really

00:03:03,110 --> 00:03:06,590
hard to do too

00:03:05,209 --> 00:03:09,739
and so I want to walk through kind of

00:03:06,590 --> 00:03:11,660
one one kind of hypothetical case so I

00:03:09,739 --> 00:03:14,720
made a customer and what they really

00:03:11,660 --> 00:03:17,360
wanted to do was recognize when people

00:03:14,720 --> 00:03:18,580
had uploaded to their online photo store

00:03:17,360 --> 00:03:21,079
like something that might be like

00:03:18,580 --> 00:03:23,150
personally identifiable information so

00:03:21,079 --> 00:03:25,450
for example if you'd uploaded a photo of

00:03:23,150 --> 00:03:28,130
a credit card or photo of your passport

00:03:25,450 --> 00:03:29,900
so to start this off they scrape some

00:03:28,130 --> 00:03:33,109
web data right you just like go you use

00:03:29,900 --> 00:03:34,519
kind of like a Search API and you get a

00:03:33,109 --> 00:03:37,910
bunch of images or credit cards off the

00:03:34,519 --> 00:03:39,500
web do you value Asians all right looks

00:03:37,910 --> 00:03:41,480
like we're gonna have maybe a 1% false

00:03:39,500 --> 00:03:43,040
positive rate well that's not so good I

00:03:41,480 --> 00:03:44,870
got a million user images I want to kind

00:03:43,040 --> 00:03:46,459
of run this on suddenly I have 10,000

00:03:44,870 --> 00:03:48,260
sort of potential false positives so

00:03:46,459 --> 00:03:50,840
they kind of build them all let's see

00:03:48,260 --> 00:03:52,190
how it goes and when they tried out on a

00:03:50,840 --> 00:03:53,959
real user data it turns out to the

00:03:52,190 --> 00:03:57,799
actual false positive rate as you might

00:03:53,959 --> 00:03:58,970
expect is much much much higher all

00:03:57,799 --> 00:04:00,319
right so now they use dressed take

00:03:58,970 --> 00:04:01,790
another round so now let's add some

00:04:00,319 --> 00:04:04,220
negative classes right we want to be

00:04:01,790 --> 00:04:05,840
able to kind of make examples of other

00:04:04,220 --> 00:04:08,239
kinds of documents sort of non credit

00:04:05,840 --> 00:04:09,799
card things etc etc but it's still okay

00:04:08,239 --> 00:04:11,420
right we're on day one or day two of the

00:04:09,799 --> 00:04:12,290
project like this still feels good you

00:04:11,420 --> 00:04:14,930
know we're able to kind of make progress

00:04:12,290 --> 00:04:16,700
it's a little more tedious second round

00:04:14,930 --> 00:04:19,310
I think you guys kind of know where this

00:04:16,700 --> 00:04:21,019
is going like doesn't work you know

00:04:19,310 --> 00:04:22,610
still you have an unacceptably high

00:04:21,019 --> 00:04:25,340
number of negative examples are coming

00:04:22,610 --> 00:04:27,530
up way too many false positives so yeah

00:04:25,340 --> 00:04:29,240
we kind of go into kind of stage three

00:04:27,530 --> 00:04:29,840
you have the experience of trying to

00:04:29,240 --> 00:04:32,690
build kind of

00:04:29,840 --> 00:04:36,110
usable model which is alright let's

00:04:32,690 --> 00:04:37,880
collect some more data from let's go

00:04:36,110 --> 00:04:39,290
kind of label some more data starts get

00:04:37,880 --> 00:04:40,430
really expensive right now something

00:04:39,290 --> 00:04:42,919
that I thought was gonna take me a day

00:04:40,430 --> 00:04:44,750
in the first round I'm on like you know

00:04:42,919 --> 00:04:46,250
day seven of like getting a bunch of

00:04:44,750 --> 00:04:49,610
label errors trying to get like I'm

00:04:46,250 --> 00:04:52,940
Turek to work and like labeling how to

00:04:49,610 --> 00:04:56,000
very large amounts of data turns out my

00:04:52,940 --> 00:04:57,080
also didn't work so the good news was at

00:04:56,000 --> 00:04:58,130
this point somebody said alright well

00:04:57,080 --> 00:04:59,330
let's try

00:04:58,130 --> 00:05:00,830
what are these kind of like

00:04:59,330 --> 00:05:02,810
interpretability techniques so it's

00:05:00,830 --> 00:05:07,190
alien see visualization and it turns out

00:05:02,810 --> 00:05:09,350
promise thumbs so when you are using

00:05:07,190 --> 00:05:11,540
kind of people take photos like on their

00:05:09,350 --> 00:05:13,669
phone of something like a document

00:05:11,540 --> 00:05:15,169
they're usually holding it which is like

00:05:13,669 --> 00:05:16,190
not what you see in web scraped images

00:05:15,169 --> 00:05:19,370
for example but it's kind of what you

00:05:16,190 --> 00:05:21,110
tend to do they basically built a

00:05:19,370 --> 00:05:22,370
classifier that recognized are you

00:05:21,110 --> 00:05:25,419
holding something and is your thumb in

00:05:22,370 --> 00:05:27,289
the picture I was not the goal but okay

00:05:25,419 --> 00:05:29,270
but this isn't just kind of a one-off

00:05:27,289 --> 00:05:30,500
problem it happens all the time so for

00:05:29,270 --> 00:05:33,229
example is that really famous nature

00:05:30,500 --> 00:05:35,900
paper from 2017 where they were doing

00:05:33,229 --> 00:05:38,479
like dermatology images and they kind of

00:05:35,900 --> 00:05:39,950
discover alright well having a ruler in

00:05:38,479 --> 00:05:41,800
an image of a mole it's actually very

00:05:39,950 --> 00:05:43,700
good signal so that might be cancerous

00:05:41,800 --> 00:05:45,680
so you might think we learn from that

00:05:43,700 --> 00:05:47,169
except just a couple weeks ago I think

00:05:45,680 --> 00:05:50,289
Walker at all published another paper

00:05:47,169 --> 00:05:52,520
where they said having surgical markings

00:05:50,289 --> 00:05:54,680
in an image so having a little kind of

00:05:52,520 --> 00:05:55,970
like marked up things around a mole also

00:05:54,680 --> 00:05:58,520
tended to kind of trip up the classifier

00:05:55,970 --> 00:06:00,560
because not unsurprisingly you know

00:05:58,520 --> 00:06:01,760
people don't tend I kind of like the

00:06:00,560 --> 00:06:02,990
trainee I didn't have any marked up skin

00:06:01,760 --> 00:06:06,110
for people that didn't have kind of

00:06:02,990 --> 00:06:08,060
cancerous moles and a lot of people I

00:06:06,110 --> 00:06:09,860
think particular people huh sometimes on

00:06:08,060 --> 00:06:11,330
our team some look at that and say it's

00:06:09,860 --> 00:06:12,979
user err it's human error right they

00:06:11,330 --> 00:06:16,400
weren't building the right distribution

00:06:12,979 --> 00:06:18,530
of data that's extremely hard to do even

00:06:16,400 --> 00:06:19,700
for experts and even harder to do for

00:06:18,530 --> 00:06:21,650
somebody who's just getting started

00:06:19,700 --> 00:06:24,020
because reality is that like real world

00:06:21,650 --> 00:06:26,389
environments are incredibly complex like

00:06:24,020 --> 00:06:28,190
this is where projects died like out of

00:06:26,389 --> 00:06:29,300
domain problems which most problems

00:06:28,190 --> 00:06:30,169
people want to actually do something

00:06:29,300 --> 00:06:32,330
kind of in a real-world environment

00:06:30,169 --> 00:06:34,220
whether it's a camera microphone a

00:06:32,330 --> 00:06:36,740
website we're kind of user inputs are

00:06:34,220 --> 00:06:39,260
unconstrained or incredibly challenging

00:06:36,740 --> 00:06:42,409
to build good data for what am I kind of

00:06:39,260 --> 00:06:42,960
favorite examples had a customer who had

00:06:42,409 --> 00:06:45,270
pulled

00:06:42,960 --> 00:06:47,789
do you know that camera an IOT camera

00:06:45,270 --> 00:06:49,440
and one day at Hales and it turns out

00:06:47,789 --> 00:06:52,530
that like it just haven't hailed in this

00:06:49,440 --> 00:06:53,940
town before model fails uh I'm like you

00:06:52,530 --> 00:06:56,580
can't expect people who have had data

00:06:53,940 --> 00:06:57,990
for Hale luckily you know that a system

00:06:56,580 --> 00:06:59,639
of multiple sensors and other kinds of

00:06:57,990 --> 00:07:01,860
validation a human in the loop it all

00:06:59,639 --> 00:07:03,600
worked out um but this sort of thing is

00:07:01,860 --> 00:07:05,509
really challenging to do like rare

00:07:03,600 --> 00:07:07,590
events right like if I want to recognize

00:07:05,509 --> 00:07:09,509
explosions like how much data am I gonna

00:07:07,590 --> 00:07:11,820
have from explosions or we had a

00:07:09,509 --> 00:07:13,050
customers doing hand tracking uh turn

00:07:11,820 --> 00:07:14,520
out the model failed the first time

00:07:13,050 --> 00:07:16,320
somebody with a hand tattoo used it

00:07:14,520 --> 00:07:17,610
there aren't that many people with hand

00:07:16,320 --> 00:07:20,039
tattoos but you still want your model to

00:07:17,610 --> 00:07:21,509
work in that case and so look there's a

00:07:20,039 --> 00:07:22,620
lot of techniques for me able to do this

00:07:21,509 --> 00:07:24,660
better um

00:07:22,620 --> 00:07:27,120
what diagnosing that it's actually

00:07:24,660 --> 00:07:28,979
really hard to build a model it's an

00:07:27,120 --> 00:07:30,750
important problem once you build a model

00:07:28,979 --> 00:07:31,800
you got a fair that's gonna work a lot

00:07:30,750 --> 00:07:32,849
of the kind of great work here is

00:07:31,800 --> 00:07:34,229
happening in the fairness and bias

00:07:32,849 --> 00:07:36,240
literature but there's kind of an

00:07:34,229 --> 00:07:37,889
overall impact for any customer or any

00:07:36,240 --> 00:07:39,960
person who's trying to build a high

00:07:37,889 --> 00:07:42,060
quality model one of the big problems is

00:07:39,960 --> 00:07:44,220
that aggregate statistics hide failure

00:07:42,060 --> 00:07:47,039
conditions so you might have you might

00:07:44,220 --> 00:07:48,949
not get this beautiful PR curve even the

00:07:47,039 --> 00:07:51,210
slices that you have look really great

00:07:48,949 --> 00:07:53,729
and then it turns out that you know you

00:07:51,210 --> 00:07:56,070
don't actually have a data set with all

00:07:53,729 --> 00:07:57,389
the features kind of in your model so

00:07:56,070 --> 00:07:58,830
let's say you're doing speech you know

00:07:57,389 --> 00:08:00,930
you may not have actually created a data

00:07:58,830 --> 00:08:02,400
set that says okay well you know this is

00:08:00,930 --> 00:08:03,690
a woman someone with an accent or a

00:08:02,400 --> 00:08:05,970
child as an accent all these kind of

00:08:03,690 --> 00:08:07,110
like sort of subclasses become extremely

00:08:05,970 --> 00:08:11,190
important and it becomes very expensive

00:08:07,110 --> 00:08:12,780
and difficult to actually go and figure

00:08:11,190 --> 00:08:15,030
out kind of where your model is failing

00:08:12,780 --> 00:08:17,130
and look on techniques for this you know

00:08:15,030 --> 00:08:18,539
sampling techniques pairing kind of

00:08:17,130 --> 00:08:20,669
uninsured models interpreter models

00:08:18,539 --> 00:08:22,949
things that you can do but it's super

00:08:20,669 --> 00:08:24,210
challenging for a beginner I had to kind

00:08:22,949 --> 00:08:26,130
of figure out what the problems might be

00:08:24,210 --> 00:08:27,750
and even for experts right like you see

00:08:26,130 --> 00:08:31,349
like these problems come up in kind of

00:08:27,750 --> 00:08:33,900
railroad systems all the time finally

00:08:31,349 --> 00:08:35,820
when you have a model it can be tough to

00:08:33,900 --> 00:08:38,400
actually figure out what to do with it

00:08:35,820 --> 00:08:40,349
right most of the programs that you use

00:08:38,400 --> 00:08:41,339
don't have probabilistic outputs in

00:08:40,349 --> 00:08:43,589
their real what does it mean for

00:08:41,339 --> 00:08:45,060
something to be 70% likely or to have

00:08:43,589 --> 00:08:46,709
seven or eight kind of chain models in a

00:08:45,060 --> 00:08:47,850
row you know might be a more obvious for

00:08:46,709 --> 00:08:49,589
you but for kind of an end user that

00:08:47,850 --> 00:08:54,029
could be hard to figure out what actions

00:08:49,589 --> 00:08:56,700
you should take and so anything for me

00:08:54,029 --> 00:08:57,990
look nothing I've said today I think is

00:08:56,700 --> 00:08:59,130
sigelei novel for the folks in this room

00:08:57,990 --> 00:09:01,110
you've gone through all of these

00:08:59,130 --> 00:09:02,910
challenges before you built a model you

00:09:01,110 --> 00:09:04,980
built the data set you probable to 18

00:09:02,910 --> 00:09:06,650
times finally got it to work

00:09:04,980 --> 00:09:09,060
but I had a boss who used to say that

00:09:06,650 --> 00:09:10,770
problems are inspiring and from either

00:09:09,060 --> 00:09:12,690
isn't a problem that is more inspiring

00:09:10,770 --> 00:09:14,220
than figuring out how can we help

00:09:12,690 --> 00:09:17,280
anybody who wants to automate some

00:09:14,220 --> 00:09:19,500
problem yeah but how do you show and be

00:09:17,280 --> 00:09:22,890
able to train a machine and have like a

00:09:19,500 --> 00:09:24,480
robust production ready model and so I

00:09:22,890 --> 00:09:26,070
can't think of a more fun problem I

00:09:24,480 --> 00:09:27,390
can't think of a more fun problem to

00:09:26,070 --> 00:09:30,200
work on with everybody in this room

00:09:27,390 --> 00:09:30,200

YouTube URL: https://www.youtube.com/watch?v=Asm-PhzVLEs


