Title: Practical performance theory - Kavya Joshi (Samsara)
Publication date: 2018-10-02
Playlist: O'Reilly Velocity Conference 2018 - New York, New York
Description: 
	To view more videos from Velocity NY 2018, please visit:
http://oreilly.com/go/velocityny18

Performance theory offers a rigorous and practical approach to performance tuning and capacity planning. Kavya Joshi dives into elegant results like Littleâ€™s law and the Universal Scalability Law. You'll also discover how performance theory is used in real systems at companies like Facebook and learn how to leverage it to prepare your systems for flux and scale.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:06,690
now performance modeling speaks to this

00:00:03,000 --> 00:00:09,630
idea of representing your system by a

00:00:06,690 --> 00:00:11,910
theoretical model analyzing that model

00:00:09,630 --> 00:00:14,790
to get results and translating those

00:00:11,910 --> 00:00:16,470
results back to your system now the

00:00:14,790 --> 00:00:20,250
crucial things to remember about

00:00:16,470 --> 00:00:23,609
modeling is that it by definition makes

00:00:20,250 --> 00:00:25,199
assumptions about your system and if

00:00:23,609 --> 00:00:27,750
your system doesn't meet these

00:00:25,199 --> 00:00:30,689
underlying assumptions you can't use the

00:00:27,750 --> 00:00:32,969
model alright so we're going to use

00:00:30,689 --> 00:00:34,500
performance modeling to answer questions

00:00:32,969 --> 00:00:37,590
about our system's throughput and

00:00:34,500 --> 00:00:40,530
capacity will first look at single

00:00:37,590 --> 00:00:42,809
server systems we will then look at a

00:00:40,530 --> 00:00:45,239
cluster of servers and then finally

00:00:42,809 --> 00:00:48,629
we'll take a step back and see what we

00:00:45,239 --> 00:00:52,199
can take away for our systems all right

00:00:48,629 --> 00:00:54,870
then let's get started now Sara single

00:00:52,199 --> 00:00:57,750
server system is a very simple web

00:00:54,870 --> 00:01:00,840
server questions we typically have about

00:00:57,750 --> 00:01:02,609
such a system are well what's the

00:01:00,840 --> 00:01:06,000
maximum throughput it can sustain

00:01:02,609 --> 00:01:08,780
without exceeding that response time SLA

00:01:06,000 --> 00:01:11,549
that we have and the second question is

00:01:08,780 --> 00:01:13,680
well how can we improve the average

00:01:11,549 --> 00:01:17,159
response time to give our users a better

00:01:13,680 --> 00:01:20,400
experience we're going to answer these

00:01:17,159 --> 00:01:23,450
questions by modeling our system as a

00:01:20,400 --> 00:01:26,490
queueing system the way this works is

00:01:23,450 --> 00:01:28,740
requests come in at some arrival rate if

00:01:26,490 --> 00:01:31,200
the server is free they're processed

00:01:28,740 --> 00:01:33,659
immediately but if the server is busy

00:01:31,200 --> 00:01:36,509
processing other requests the requests

00:01:33,659 --> 00:01:39,079
will queue now the timer request spends

00:01:36,509 --> 00:01:42,150
in the queue is called the queuing delay

00:01:39,079 --> 00:01:43,649
then when the server is free the server

00:01:42,150 --> 00:01:45,420
is going to pick off a request from the

00:01:43,649 --> 00:01:47,759
queue and processes it and process it

00:01:45,420 --> 00:01:50,070
and the time it spends processing it is

00:01:47,759 --> 00:01:52,170
called the service time and so the

00:01:50,070 --> 00:01:55,890
response time is simply the queuing

00:01:52,170 --> 00:01:58,079
delay plus the service time now first

00:01:55,890 --> 00:02:01,409
things first the assumptions that we

00:01:58,079 --> 00:02:03,420
have about the system well first we're

00:02:01,409 --> 00:02:05,939
going to assume that requests are

00:02:03,420 --> 00:02:07,890
independent in random so they're not

00:02:05,939 --> 00:02:11,910
correlated with each other they're not

00:02:07,890 --> 00:02:13,590
correlated with responses no the second

00:02:11,910 --> 00:02:16,319
assumption we're going to make

00:02:13,590 --> 00:02:19,739
that requests a process in first-in

00:02:16,319 --> 00:02:21,989
first-out order and one at a time so you

00:02:19,739 --> 00:02:25,410
can imagine this is a single core single

00:02:21,989 --> 00:02:27,840
threaded server and the third assumption

00:02:25,410 --> 00:02:30,330
is that we're going to assume that the

00:02:27,840 --> 00:02:33,629
time it takes the process or request the

00:02:30,330 --> 00:02:36,269
service time that's a constant so what

00:02:33,629 --> 00:02:38,760
we're saying is that requests are really

00:02:36,269 --> 00:02:41,310
the same size that that's our assumption

00:02:38,760 --> 00:02:44,280
and we're also assuming that there is no

00:02:41,310 --> 00:02:46,880
downstream saturation so the network the

00:02:44,280 --> 00:02:50,549
database they're not going to bottleneck

00:02:46,880 --> 00:02:53,760
okay so with this let's answer our first

00:02:50,549 --> 00:02:57,360
question what's the maximum throughput

00:02:53,760 --> 00:03:01,079
of the server well using our model let's

00:02:57,360 --> 00:03:03,329
just reason through it intuitively as we

00:03:01,079 --> 00:03:05,670
crank up the arrival rate into the

00:03:03,329 --> 00:03:08,700
system we expect the server's

00:03:05,670 --> 00:03:11,030
utilization its busyness to go up right

00:03:08,700 --> 00:03:13,980
there's nothing revolutionary about this

00:03:11,030 --> 00:03:16,709
but our question today is how is it

00:03:13,980 --> 00:03:19,980
going to increase and the theory gives

00:03:16,709 --> 00:03:22,500
us the answer the utilization law tells

00:03:19,980 --> 00:03:25,170
us that utilization is simply the

00:03:22,500 --> 00:03:28,380
arrival rate times the service time

00:03:25,170 --> 00:03:30,959
which we're assuming is constant so the

00:03:28,380 --> 00:03:36,209
utilization arrival rate graph is linear

00:03:30,959 --> 00:03:39,630
it's a line okay so so what the server's

00:03:36,209 --> 00:03:42,569
gonna get busier well if the server gets

00:03:39,630 --> 00:03:45,540
busier the probability that an incoming

00:03:42,569 --> 00:03:48,209
request will show up and find the server

00:03:45,540 --> 00:03:51,989
busy and half the queue that probability

00:03:48,209 --> 00:03:54,930
is going to go up so the average queue

00:03:51,989 --> 00:03:57,359
length is going to increase so the

00:03:54,930 --> 00:03:59,280
average queuing delay the timer request

00:03:57,359 --> 00:04:01,400
spends in the queue that's going to go

00:03:59,280 --> 00:04:05,069
up makes sense

00:04:01,400 --> 00:04:07,349
but again our question today is how is

00:04:05,069 --> 00:04:10,049
it going to increase and again the

00:04:07,349 --> 00:04:13,079
theory delivers the theory tells us to

00:04:10,049 --> 00:04:16,019
use the PK formula which i think is

00:04:13,079 --> 00:04:20,160
pronounced as palak SEC can shine but

00:04:16,019 --> 00:04:22,409
we're going to call it PK and the PK

00:04:20,160 --> 00:04:25,349
formula tells us that the average

00:04:22,409 --> 00:04:27,060
queuing delay is equal to this term that

00:04:25,349 --> 00:04:30,000
depends on the utilization

00:04:27,060 --> 00:04:32,610
and to other terms that depend on the

00:04:30,000 --> 00:04:35,250
service time now we're not going to

00:04:32,610 --> 00:04:38,210
worry about those last two terms because

00:04:35,250 --> 00:04:42,120
we assumed that service time is constant

00:04:38,210 --> 00:04:44,430
so then looking at that first term the

00:04:42,120 --> 00:04:49,380
average queuing delay is directly

00:04:44,430 --> 00:04:51,090
proportional to u over 1 minus u anybody

00:04:49,380 --> 00:04:55,230
know what that graph look like in

00:04:51,090 --> 00:04:59,640
practice that hockey stick that we all

00:04:55,230 --> 00:05:02,310
love basically what this means is that

00:04:59,640 --> 00:05:05,190
as utilization goes up the queuing delay

00:05:02,310 --> 00:05:08,340
is going to hockey stick and because

00:05:05,190 --> 00:05:11,669
response time is simply a linear

00:05:08,340 --> 00:05:13,410
function off the queuing delay what this

00:05:11,669 --> 00:05:15,780
means is that when the server gets

00:05:13,410 --> 00:05:18,120
busier the response time is going to

00:05:15,780 --> 00:05:22,530
hockey stick this is that infamous graph

00:05:18,120 --> 00:05:25,350
that we're all familiar with so applying

00:05:22,530 --> 00:05:28,380
this to our question well what's the

00:05:25,350 --> 00:05:30,389
maximum throughput of the server well as

00:05:28,380 --> 00:05:33,990
long as we're in the low utilization

00:05:30,389 --> 00:05:37,200
regime increasing the requests per

00:05:33,990 --> 00:05:39,960
second it still keeps the response time

00:05:37,200 --> 00:05:42,390
about the same but once we're in the

00:05:39,960 --> 00:05:45,660
high utilization regime and the server

00:05:42,390 --> 00:05:49,050
is busy it's as busy as it can be it

00:05:45,660 --> 00:05:51,690
can't do any more work increasing the

00:05:49,050 --> 00:05:54,060
arrival rates means that requests are

00:05:51,690 --> 00:05:56,340
going to start to queue right so

00:05:54,060 --> 00:05:59,640
response time is going to hockey stick

00:05:56,340 --> 00:06:02,130
and in fact it's going to hockey stick

00:05:59,640 --> 00:06:05,070
until it crosses that response time

00:06:02,130 --> 00:06:07,919
threshold we have so in this case the

00:06:05,070 --> 00:06:09,780
maximum throughput of the server is the

00:06:07,919 --> 00:06:13,020
point where that curve meets that line

00:06:09,780 --> 00:06:15,440
which is at about 5500 requests per

00:06:13,020 --> 00:06:15,440
second

00:06:21,660 --> 00:06:23,720

YouTube URL: https://www.youtube.com/watch?v=Ivss-VtmbDY


