Title: Wes McKinney interviewed at O'Reilly JupyterCon
Publication date: 2017-08-25
Playlist: JupyterCon
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:04,500
hi I'm Paco Nathan with O'Reilly Media

00:00:02,580 --> 00:00:06,629
and it's pleasure to be able to talk

00:00:04,500 --> 00:00:09,090
here now with West McKinney I was

00:00:06,629 --> 00:00:11,219
currently Software Architect at 2 Sigma

00:00:09,090 --> 00:00:16,199
and also of course very well known as

00:00:11,219 --> 00:00:18,480
the creator of pandas in Python and one

00:00:16,199 --> 00:00:22,020
of our top authors to using Python for

00:00:18,480 --> 00:00:23,310
data analytics was really appreciated

00:00:22,020 --> 00:00:25,289
your keynote this morning that's

00:00:23,310 --> 00:00:28,470
fantastic thank you thank you

00:00:25,289 --> 00:00:30,929
I wanted to to get in a conversation a

00:00:28,470 --> 00:00:33,210
bit more about you know some of the

00:00:30,929 --> 00:00:35,520
motivations behind arrow and and how

00:00:33,210 --> 00:00:39,510
you're you're talking about like a kind

00:00:35,520 --> 00:00:41,070
of a funnel almost effect of what you

00:00:39,510 --> 00:00:44,160
need to build from the data storage out

00:00:41,070 --> 00:00:46,860
to the day analytics right right yeah

00:00:44,160 --> 00:00:49,829
well I would say that arrow is part of a

00:00:46,860 --> 00:00:52,770
much longer journey that I've been on

00:00:49,829 --> 00:00:54,870
over the last you know 10 10 years or so

00:00:52,770 --> 00:00:56,879
when I when I started out I didn't know

00:00:54,870 --> 00:00:58,859
much about about building doing data

00:00:56,879 --> 00:01:02,219
analysis we're building or building data

00:00:58,859 --> 00:01:03,899
systems and I was attracted to the

00:01:02,219 --> 00:01:06,420
Python programming language and found

00:01:03,899 --> 00:01:09,180
this ecosystem of scientific computing

00:01:06,420 --> 00:01:11,210
tools and you know pretty quickly I said

00:01:09,180 --> 00:01:14,640
about building building myself a toolset

00:01:11,210 --> 00:01:16,650
for statistical analysis and data

00:01:14,640 --> 00:01:18,840
manipulation and to have some of the

00:01:16,650 --> 00:01:20,960
kinds of features that are found in our

00:01:18,840 --> 00:01:23,790
and and other statistics focused

00:01:20,960 --> 00:01:27,420
programming environments and I spent

00:01:23,790 --> 00:01:32,460
from you know the middle of 2008 through

00:01:27,420 --> 00:01:34,380
the end of 2012 really focused on really

00:01:32,460 --> 00:01:36,030
focused on pandas and there was a period

00:01:34,380 --> 00:01:38,250
of time where I didn't have a day job

00:01:36,030 --> 00:01:40,979
and I was basically burning up all of my

00:01:38,250 --> 00:01:44,040
savings working on pandas because I felt

00:01:40,979 --> 00:01:46,710
that it was really important that the

00:01:44,040 --> 00:01:48,990
Python ecosystem got over there over

00:01:46,710 --> 00:01:51,990
that hurdle of getting to a place where

00:01:48,990 --> 00:01:54,240
it's a viable language for doing what we

00:01:51,990 --> 00:01:56,729
can now call data science that you can

00:01:54,240 --> 00:01:58,560
read data you can manipulate it you can

00:01:56,729 --> 00:02:00,630
do exploratory analytics you can

00:01:58,560 --> 00:02:03,450
visualize it and then you can hand that

00:02:00,630 --> 00:02:05,549
clean prepare data off to statistical

00:02:03,450 --> 00:02:09,030
models and of course during this time we

00:02:05,549 --> 00:02:11,250
also had to to build a body of

00:02:09,030 --> 00:02:13,220
statistical modeling libraries so things

00:02:11,250 --> 00:02:15,440
like stats models things

00:02:13,220 --> 00:02:17,720
scikit-learn also got started during

00:02:15,440 --> 00:02:19,700
that time and so we needed this kind of

00:02:17,720 --> 00:02:22,820
convergence of technologies to happen to

00:02:19,700 --> 00:02:26,030
make the whole ecosystem successful but

00:02:22,820 --> 00:02:27,980
as as time went on you know when in the

00:02:26,030 --> 00:02:31,580
early days of pandas we built the

00:02:27,980 --> 00:02:33,350
project using the you know the best of

00:02:31,580 --> 00:02:35,960
what was available in the pipe and the

00:02:33,350 --> 00:02:39,500
Python ecosystem and that led to an

00:02:35,960 --> 00:02:41,480
accumulation of technical debt we found

00:02:39,500 --> 00:02:43,070
over time that we you know we had

00:02:41,480 --> 00:02:45,320
developed our own versions of everything

00:02:43,070 --> 00:02:47,090
we had our own CSV readers and our own

00:02:45,320 --> 00:02:49,130
versions of you know data frame

00:02:47,090 --> 00:02:51,680
manipulations and algorithms to process

00:02:49,130 --> 00:02:54,170
those and we've spent countless cycles

00:02:51,680 --> 00:02:56,209
you know optimizing those algorithms to

00:02:54,170 --> 00:02:59,120
make pandas as fast and scalable as

00:02:56,209 --> 00:03:01,370
possible so there's a couple of issues

00:02:59,120 --> 00:03:03,740
the first is that there's a code

00:03:01,370 --> 00:03:05,990
ownership problem in that you know

00:03:03,740 --> 00:03:07,880
everything within you know panthis is

00:03:05,990 --> 00:03:10,370
this full stack system we've implemented

00:03:07,880 --> 00:03:13,130
everything ourselves and that's created

00:03:10,370 --> 00:03:15,560
over time a large burden on a very small

00:03:13,130 --> 00:03:17,390
group of volunteer developers pandas has

00:03:15,560 --> 00:03:20,390
received very little direct funding over

00:03:17,390 --> 00:03:23,510
over its lifetime and we've also found

00:03:20,390 --> 00:03:25,280
that plugging pandas in and really most

00:03:23,510 --> 00:03:27,920
tools in the Pythian ecosystem plugging

00:03:25,280 --> 00:03:30,080
them in to other systems and crossing

00:03:27,920 --> 00:03:32,930
the boundary between between programming

00:03:30,080 --> 00:03:35,390
languages between Python and Big Data

00:03:32,930 --> 00:03:37,550
technologies likes like the spark and

00:03:35,390 --> 00:03:39,230
the Hadoop ecosystem there's a lot of

00:03:37,550 --> 00:03:41,150
pain with that certainly having to

00:03:39,230 --> 00:03:43,790
convert between the pandas dataframe

00:03:41,150 --> 00:03:46,880
representation and other kinds of data

00:03:43,790 --> 00:03:49,459
representations so so the arrow project

00:03:46,880 --> 00:03:51,530
I didn't come up with on my own but as I

00:03:49,459 --> 00:03:53,510
started looking around this is when I

00:03:51,530 --> 00:03:55,100
was at Cloudera and I was looking to

00:03:53,510 --> 00:03:59,180
make Python more of a first-class

00:03:55,100 --> 00:04:02,810
citizen for working on big data so I was

00:03:59,180 --> 00:04:05,480
looking at things like like Impala for

00:04:02,810 --> 00:04:08,269
running Python code inside a sequel

00:04:05,480 --> 00:04:10,130
analytic sequel engine looking at Python

00:04:08,269 --> 00:04:12,019
on spark and how we can make those

00:04:10,130 --> 00:04:14,510
integrations stronger and one of the

00:04:12,019 --> 00:04:16,160
things that jumped out was we need a way

00:04:14,510 --> 00:04:18,890
to exchange data with these systems

00:04:16,160 --> 00:04:22,700
that's efficient and that is reasonably

00:04:18,890 --> 00:04:24,500
reasonably standardized and so I started

00:04:22,700 --> 00:04:25,820
looking around like is there anyone else

00:04:24,500 --> 00:04:28,400
that is thinking about

00:04:25,820 --> 00:04:31,070
this problem of the every system has

00:04:28,400 --> 00:04:33,560
their own they may be open-source but

00:04:31,070 --> 00:04:35,540
they have quote-unquote proprietary data

00:04:33,560 --> 00:04:37,040
formats and so whenever you jump from

00:04:35,540 --> 00:04:39,380
one system to another you pay the

00:04:37,040 --> 00:04:42,680
serialization this crossing this

00:04:39,380 --> 00:04:44,750
crossing penalty and so I spent about

00:04:42,680 --> 00:04:46,730
six months or so kind of looking around

00:04:44,750 --> 00:04:49,490
and trying to get as many people in the

00:04:46,730 --> 00:04:51,560
room as possible to at least talk about

00:04:49,490 --> 00:04:53,900
the problem and again to discuss like is

00:04:51,560 --> 00:04:55,670
this something is this a common enough

00:04:53,900 --> 00:04:59,270
problem where we could develop a

00:04:55,670 --> 00:05:01,790
solution that you know that we could all

00:04:59,270 --> 00:05:05,000
adopt and grow and develop to meet a

00:05:01,790 --> 00:05:06,860
broad set set of use cases so by the end

00:05:05,000 --> 00:05:08,780
of so by the end of 2015 we've got

00:05:06,860 --> 00:05:10,250
enough people in the room to say like

00:05:08,780 --> 00:05:11,660
this is something we really believe in

00:05:10,250 --> 00:05:14,240
and something we want to make a

00:05:11,660 --> 00:05:16,250
community project in standardize and

00:05:14,240 --> 00:05:18,260
start building integration so that the

00:05:16,250 --> 00:05:20,900
next generation of data systems all can

00:05:18,260 --> 00:05:22,220
use this common data format it took us a

00:05:20,900 --> 00:05:24,380
little while to come up with the name

00:05:22,220 --> 00:05:26,990
arrow and we we went through which had

00:05:24,380 --> 00:05:28,700
to jump through a few hoops with you

00:05:26,990 --> 00:05:30,950
know the Apache Software Foundation to

00:05:28,700 --> 00:05:34,220
set up the right governance structure

00:05:30,950 --> 00:05:36,560
and to pull some IP out of other

00:05:34,220 --> 00:05:39,620
projects so we the initial version of

00:05:36,560 --> 00:05:42,320
the Java implementation of arrow came

00:05:39,620 --> 00:05:44,210
from the Apache drill project but we

00:05:42,320 --> 00:05:46,880
felt really strongly about setting up a

00:05:44,210 --> 00:05:48,740
community first project where we could

00:05:46,880 --> 00:05:50,420
develop the format in an open and

00:05:48,740 --> 00:05:52,970
transparent way and to bring as many

00:05:50,420 --> 00:05:53,900
people into the process as possible and

00:05:52,970 --> 00:05:56,270
to make this something that the

00:05:53,900 --> 00:05:58,760
community owns and that could be really

00:05:56,270 --> 00:06:00,380
widely widely adopted I think the last

00:05:58,760 --> 00:06:02,690
thing that we would have wanted to do is

00:06:00,380 --> 00:06:05,150
to have you this be kind of West

00:06:02,690 --> 00:06:07,820
McKinney's special data frame format or

00:06:05,150 --> 00:06:10,940
a format that's created by you know a

00:06:07,820 --> 00:06:12,680
start-up or or a big or a big data

00:06:10,940 --> 00:06:14,480
company because that could create the

00:06:12,680 --> 00:06:16,550
perception of governance conflicts and

00:06:14,480 --> 00:06:18,680
other issues so we wanted this to be the

00:06:16,550 --> 00:06:21,440
community's project and to and to build

00:06:18,680 --> 00:06:23,810
it kind of in full view and so it's been

00:06:21,440 --> 00:06:27,200
really exciting and also scary to start

00:06:23,810 --> 00:06:29,030
from a markdown specification an

00:06:27,200 --> 00:06:31,610
incomplete markdown specification of the

00:06:29,030 --> 00:06:34,430
format and to build libraries from

00:06:31,610 --> 00:06:36,650
scratch and to engage with the community

00:06:34,430 --> 00:06:38,840
to understand how we could best kind of

00:06:36,650 --> 00:06:40,840
build out the ecosystem

00:06:38,840 --> 00:06:43,490
from that kind of chicken-and-egg

00:06:40,840 --> 00:06:47,180
chicken-and-egg stage and so right now

00:06:43,490 --> 00:06:50,600
we're looking at how we can how we can

00:06:47,180 --> 00:06:52,910
use the use the arrow technology to to

00:06:50,600 --> 00:06:54,980
more quickly advance the state of the

00:06:52,910 --> 00:06:57,500
art and what's possible in the python

00:06:54,980 --> 00:06:59,570
ecosystem and and beyond well it's

00:06:57,500 --> 00:07:01,639
fantastic the the the scope and the

00:06:59,570 --> 00:07:04,280
impact of this because it's not just

00:07:01,639 --> 00:07:06,440
working across different multiple

00:07:04,280 --> 00:07:08,510
different popular projects and getting

00:07:06,440 --> 00:07:10,730
them to work with common storage but

00:07:08,510 --> 00:07:13,370
it's again it's across entirely

00:07:10,730 --> 00:07:15,050
different paradigms I mean looking at

00:07:13,370 --> 00:07:17,210
what people are doing in Python versus

00:07:15,050 --> 00:07:22,700
what they're doing in Hadoop or Impala

00:07:17,210 --> 00:07:23,810
looking at R it's it's beyond a scale of

00:07:22,700 --> 00:07:26,810
integration that I would usually

00:07:23,810 --> 00:07:29,840
encounter right I mean one of the one of

00:07:26,810 --> 00:07:32,560
the most excellent yeah kind of one of

00:07:29,840 --> 00:07:35,419
the areas where it's there's the closest

00:07:32,560 --> 00:07:36,800
like semantic compatibility in terms of

00:07:35,419 --> 00:07:39,290
what people are doing with the data it

00:07:36,800 --> 00:07:41,240
really is in the data science world we

00:07:39,290 --> 00:07:44,479
have you know pandas dataframes in

00:07:41,240 --> 00:07:47,690
python are data frames julia has julie

00:07:44,479 --> 00:07:49,250
has data frames apache spark has has

00:07:47,690 --> 00:07:53,410
data frames and if you look at their

00:07:49,250 --> 00:07:56,030
internal data model and the semantics of

00:07:53,410 --> 00:07:57,890
manipulating those objects there's a lot

00:07:56,030 --> 00:08:00,289
of similarity and even the code has come

00:07:57,890 --> 00:08:02,840
out looking very similar when spark

00:08:00,289 --> 00:08:05,570
looked to build an api that looks like

00:08:02,840 --> 00:08:08,060
data frames for for the spark sequel

00:08:05,570 --> 00:08:09,979
runtime they chose to emulate pandas to

00:08:08,060 --> 00:08:12,080
emulate our as closely as possible so

00:08:09,979 --> 00:08:14,180
that when users were working with data

00:08:12,080 --> 00:08:16,220
and spark that they could use the same

00:08:14,180 --> 00:08:18,050
knowledge that they'd accumulated as our

00:08:16,220 --> 00:08:20,960
programmers or as python programmers to

00:08:18,050 --> 00:08:23,150
work with data that's part of the that's

00:08:20,960 --> 00:08:25,340
part of the spark runtime but the hard

00:08:23,150 --> 00:08:27,289
part there is that we have you know so

00:08:25,340 --> 00:08:29,780
many years and you know person years of

00:08:27,289 --> 00:08:32,990
development creating libraries of

00:08:29,780 --> 00:08:36,349
algorithms and modeling tools which are

00:08:32,990 --> 00:08:39,289
very tightly coupled to data frames and

00:08:36,349 --> 00:08:41,360
our data frames in Python and that that

00:08:39,289 --> 00:08:44,209
is one of the the key challenges is that

00:08:41,360 --> 00:08:46,580
if we want to be able to share or code

00:08:44,209 --> 00:08:48,980
in infrastructure between the Python

00:08:46,580 --> 00:08:50,330
world and the our world for example we

00:08:48,980 --> 00:08:52,220
will have to rebuild many of those

00:08:50,330 --> 00:08:54,740
algorithms to be based on it

00:08:52,220 --> 00:08:58,520
in memory format that is not particular

00:08:54,740 --> 00:09:00,380
to are not particularly fond so I think

00:08:58,520 --> 00:09:01,970
I find that you know I found that that

00:09:00,380 --> 00:09:05,600
chicken and egg problems often drive

00:09:01,970 --> 00:09:07,790
away contributors because you know it's

00:09:05,600 --> 00:09:11,690
too daunting it is it is very it is very

00:09:07,790 --> 00:09:15,410
daunting and you know and by it invites

00:09:11,690 --> 00:09:17,060
nature the the when you work on a

00:09:15,410 --> 00:09:19,010
chicken and a problem it is often not

00:09:17,060 --> 00:09:21,650
useful until it reaches a point of

00:09:19,010 --> 00:09:24,080
critical mass but the interesting part

00:09:21,650 --> 00:09:26,090
is that in something like this all of

00:09:24,080 --> 00:09:28,730
the technology is composable so if you

00:09:26,090 --> 00:09:31,700
build two tools which know about know

00:09:28,730 --> 00:09:33,620
about arrow memory those tools compose

00:09:31,700 --> 00:09:35,690
if you build a data connector which

00:09:33,620 --> 00:09:38,090
returns arrow data that can can compose

00:09:35,690 --> 00:09:40,370
with any other tool that uses arrows so

00:09:38,090 --> 00:09:42,620
there's this cumulative effects kind of

00:09:40,370 --> 00:09:44,510
this kind of Gestalt where you know the

00:09:42,620 --> 00:09:47,210
whole is greater than yeah this not the

00:09:44,510 --> 00:09:49,010
sum of its parts and so really what

00:09:47,210 --> 00:09:52,130
we've been doing over the last year and

00:09:49,010 --> 00:09:53,930
a half and not only in in you know

00:09:52,130 --> 00:09:56,480
stabilizing and what I see hardening

00:09:53,930 --> 00:09:59,120
hardening the the arrow format so that

00:09:56,480 --> 00:10:01,160
the format isn't changing and there's

00:09:59,120 --> 00:10:03,650
like lots of fine details dealing with

00:10:01,160 --> 00:10:06,680
time stamps and decimals and you know

00:10:03,650 --> 00:10:08,240
the fringe parts of of memory

00:10:06,680 --> 00:10:10,550
representations we want to make sure all

00:10:08,240 --> 00:10:13,220
that's stable so that we can focus on

00:10:10,550 --> 00:10:16,310
building libraries of analytics code

00:10:13,220 --> 00:10:18,770
which can be used very easily between

00:10:16,310 --> 00:10:20,480
between environments one thing you

00:10:18,770 --> 00:10:21,590
mentioned earlier which I found is

00:10:20,480 --> 00:10:24,860
really interesting is about the

00:10:21,590 --> 00:10:27,260
serialization looking at cluster traces

00:10:24,860 --> 00:10:29,840
for sort of the early years of Hadoop

00:10:27,260 --> 00:10:31,520
and large big data jobs that were

00:10:29,840 --> 00:10:33,680
running you'd spend so much your time

00:10:31,520 --> 00:10:36,440
doing serialization or D serialization

00:10:33,680 --> 00:10:37,610
and and what strikes me about if I

00:10:36,440 --> 00:10:39,950
understand correctly what strikes me

00:10:37,610 --> 00:10:42,500
about arrow is that you know this is

00:10:39,950 --> 00:10:45,110
being addressed directly but even more

00:10:42,500 --> 00:10:47,420
to the point by using columnar formats

00:10:45,110 --> 00:10:49,580
now you can allow for like pushed on

00:10:47,420 --> 00:10:52,040
predicates and even more sophisticated

00:10:49,580 --> 00:10:54,170
means of taking a lot of that tech bad

00:10:52,040 --> 00:10:57,110
off the table right that's right yeah

00:10:54,170 --> 00:10:59,350
and and you know we aren't reinventing

00:10:57,110 --> 00:11:02,900
any wheels as far as you know columnar

00:10:59,350 --> 00:11:05,600
calling their data formats or or even

00:11:02,900 --> 00:11:07,910
columnar storage so people talk about

00:11:05,600 --> 00:11:10,430
you know compare arrow with parquet or

00:11:07,910 --> 00:11:11,959
orc which are columnar storage formats

00:11:10,430 --> 00:11:15,139
or really they're complementary

00:11:11,959 --> 00:11:17,120
technologies where you know we read data

00:11:15,139 --> 00:11:20,930
from those storage formats and then we

00:11:17,120 --> 00:11:23,660
put it in in arrow and memory but you

00:11:20,930 --> 00:11:26,209
can imagine you know a io pipeline where

00:11:23,660 --> 00:11:28,610
you deserialize chunks of data from

00:11:26,209 --> 00:11:31,430
columnar formats into arrow and then you

00:11:28,610 --> 00:11:32,839
apply predicates and filter those those

00:11:31,430 --> 00:11:35,060
chunks of data as they're coming through

00:11:32,839 --> 00:11:37,579
the coming through the pipeline

00:11:35,060 --> 00:11:39,019
you mentioned serialization overhead

00:11:37,579 --> 00:11:41,750
this is one of the the one of the

00:11:39,019 --> 00:11:43,970
biggest motivations for the project is

00:11:41,750 --> 00:11:46,910
the fact that serialization particularly

00:11:43,970 --> 00:11:49,100
when you jump between operators and

00:11:46,910 --> 00:11:51,410
within a single system or if you are

00:11:49,100 --> 00:11:53,600
using two systems as part of solving a

00:11:51,410 --> 00:11:56,000
bigger problem and this is certainly

00:11:53,600 --> 00:11:58,459
true in you know old-school hadoop

00:11:56,000 --> 00:12:00,560
mapreduce where so much of like if you

00:11:58,459 --> 00:12:02,449
profile the runtime of a job and what

00:12:00,560 --> 00:12:04,040
where time is actually being spent so

00:12:02,449 --> 00:12:05,779
much of the time is spent you know could

00:12:04,040 --> 00:12:08,779
be 80 or 90 percent or more of the time

00:12:05,779 --> 00:12:11,660
is you know deserializing parsing the

00:12:08,779 --> 00:12:13,430
format from HDFS or wherever the data

00:12:11,660 --> 00:12:16,069
stored putting that in intermediate data

00:12:13,430 --> 00:12:18,139
structures performing the analytics and

00:12:16,069 --> 00:12:21,050
then risa realizing and writing the

00:12:18,139 --> 00:12:24,290
results back out back out to disk so

00:12:21,050 --> 00:12:26,449
particularly as data processing systems

00:12:24,290 --> 00:12:29,240
are moving to more of an in-memory and

00:12:26,449 --> 00:12:31,069
shared memory model where data in

00:12:29,240 --> 00:12:33,230
between operators is not persisted to

00:12:31,069 --> 00:12:34,759
disk it may be it may move between

00:12:33,230 --> 00:12:36,860
processes but if it moves between

00:12:34,759 --> 00:12:39,319
processes it goes for the true shared

00:12:36,860 --> 00:12:41,870
memory so to be able to move data

00:12:39,319 --> 00:12:44,029
between processes without serialization

00:12:41,870 --> 00:12:46,069
is very very powerful and so we've

00:12:44,029 --> 00:12:48,279
invested a lot of time thinking hard

00:12:46,069 --> 00:12:51,259
about how that quote unquote

00:12:48,279 --> 00:12:54,110
serialization works so that I if I have

00:12:51,259 --> 00:12:56,480
an aero data set in a particular process

00:12:54,110 --> 00:12:58,610
that I can expose that data to another

00:12:56,480 --> 00:13:01,550
process or another threat of execution

00:12:58,610 --> 00:13:03,620
without any without any serialization so

00:13:01,550 --> 00:13:05,839
effectively on the receiver side it's

00:13:03,620 --> 00:13:07,310
looking at some metadata and then moving

00:13:05,839 --> 00:13:10,759
pointers around to be able to access

00:13:07,310 --> 00:13:12,709
each column in the in the data set so

00:13:10,759 --> 00:13:14,930
this evolution of arrow then it sounds

00:13:12,709 --> 00:13:16,939
like it's tracking closely with what's

00:13:14,930 --> 00:13:17,580
going on in the evolution of hardware

00:13:16,939 --> 00:13:20,040
having

00:13:17,580 --> 00:13:22,350
decor having larger memory spaces and

00:13:20,040 --> 00:13:24,000
and taking advantage of those as opposed

00:13:22,350 --> 00:13:28,620
to having a lot of little servers in a

00:13:24,000 --> 00:13:29,850
cluster yeah absolutely and you see you

00:13:28,620 --> 00:13:32,130
know particularly with what with what's

00:13:29,850 --> 00:13:33,960
happening with you know the non-volatile

00:13:32,130 --> 00:13:38,390
memory technology in a 3d crosspoint

00:13:33,960 --> 00:13:41,010
from from Intel so we have you know just

00:13:38,390 --> 00:13:42,660
absolute through you know throughput to

00:13:41,010 --> 00:13:44,820
to disk there's this convergence

00:13:42,660 --> 00:13:48,270
happening in performance and latency

00:13:44,820 --> 00:13:50,010
between solid-state drives and in DRAM

00:13:48,270 --> 00:13:52,830
the other meeting so they're kind of

00:13:50,010 --> 00:13:54,600
meeting and so I mean they may not be a

00:13:52,830 --> 00:13:58,340
full it may not be a full convergence

00:13:54,600 --> 00:14:01,710
but the architecting the systems with

00:13:58,340 --> 00:14:04,650
high speed non-volatile memory in mind

00:14:01,710 --> 00:14:07,680
you know that that is kind of how a lot

00:14:04,650 --> 00:14:11,160
of systems need to evolve in the future

00:14:07,680 --> 00:14:14,340
so that you know used to be that disc is

00:14:11,160 --> 00:14:15,990
disk is slow and so you might be able to

00:14:14,340 --> 00:14:17,760
get better absolute throughput by

00:14:15,990 --> 00:14:19,560
serializing and using column nor

00:14:17,760 --> 00:14:21,450
compression with something like park'

00:14:19,560 --> 00:14:23,160
format you can make the data really

00:14:21,450 --> 00:14:25,650
small and even though your disk is fast

00:14:23,160 --> 00:14:27,690
that the file the data store is so small

00:14:25,650 --> 00:14:30,300
that you can you can deserialize it much

00:14:27,690 --> 00:14:32,730
faster in that kind of highly encoded

00:14:30,300 --> 00:14:34,440
and compressed compressed form but when

00:14:32,730 --> 00:14:37,350
you're dealing with super fast disk and

00:14:34,440 --> 00:14:40,410
you have a zero copy memory format it

00:14:37,350 --> 00:14:42,720
that that concern you know kind of goes

00:14:40,410 --> 00:14:45,360
away in a lot of cases you know it's

00:14:42,720 --> 00:14:46,560
interesting so one thing that when I was

00:14:45,360 --> 00:14:48,960
first looking at this one thing that I

00:14:46,560 --> 00:14:52,950
missed was that as we were saying

00:14:48,960 --> 00:14:55,170
parquet or other kinds of formats for

00:14:52,950 --> 00:14:56,520
storage there's the data at rest there's

00:14:55,170 --> 00:14:58,470
the data in memory there are different

00:14:56,520 --> 00:15:00,030
layers and volunteers so you could look

00:14:58,470 --> 00:15:03,840
at something like parquet for data at

00:15:00,030 --> 00:15:06,270
rest arrow for the data representation

00:15:03,840 --> 00:15:08,010
the data frames are being used across

00:15:06,270 --> 00:15:09,390
different processes it seems like

00:15:08,010 --> 00:15:11,640
there's another layer beyond that in

00:15:09,390 --> 00:15:13,860
terms of the dag of what needs to be

00:15:11,640 --> 00:15:14,990
computed and I saw in your talk you're

00:15:13,860 --> 00:15:17,700
mentioning about that is that

00:15:14,990 --> 00:15:20,160
forward-looking is that an extension of

00:15:17,700 --> 00:15:22,190
the project absolutely so I mean the

00:15:20,160 --> 00:15:25,110
layer I mean the layering of

00:15:22,190 --> 00:15:28,380
technologies so first is the you know

00:15:25,110 --> 00:15:30,149
the memory format then you need a way to

00:15:28,380 --> 00:15:31,980
to manage

00:15:30,149 --> 00:15:34,230
to manage shared memory particularly in

00:15:31,980 --> 00:15:36,720
a multi and a multi-process environment

00:15:34,230 --> 00:15:38,820
so one project where what one sub

00:15:36,720 --> 00:15:40,890
project with an arrow where we've been

00:15:38,820 --> 00:15:44,100
working on this came out of the Rea

00:15:40,890 --> 00:15:46,290
project at UC Berkeley Ryze lab where

00:15:44,100 --> 00:15:48,209
they developed a system called plasma

00:15:46,290 --> 00:15:50,640
which is a shared memory object store

00:15:48,209 --> 00:15:53,940
and the idea is that it provides a third

00:15:50,640 --> 00:15:56,160
party for managing memory lifetime of

00:15:53,940 --> 00:15:58,470
shared memory so if you create an object

00:15:56,160 --> 00:16:01,320
in one process you would materialize

00:15:58,470 --> 00:16:03,060
that in the plasma object store and then

00:16:01,320 --> 00:16:05,670
another process can acquire a reference

00:16:03,060 --> 00:16:07,589
to that data and so if two processes say

00:16:05,670 --> 00:16:10,260
okay I'm done with this object then it

00:16:07,589 --> 00:16:14,010
can be released to the release to the

00:16:10,260 --> 00:16:15,120
memory pool so the next stage of so the

00:16:14,010 --> 00:16:17,370
way that they're using that in the Ray

00:16:15,120 --> 00:16:19,110
project is by evaluating tasks graphs

00:16:17,370 --> 00:16:21,120
for reinforcement learning and deep

00:16:19,110 --> 00:16:23,730
learning models and where you have

00:16:21,120 --> 00:16:25,649
you're evaluating this task graph and

00:16:23,730 --> 00:16:27,450
you have a pool of workers and so the

00:16:25,649 --> 00:16:29,220
scheduler can assign work to a worker

00:16:27,450 --> 00:16:31,560
and that worker can be in any process

00:16:29,220 --> 00:16:34,230
and what it does is the worker will

00:16:31,560 --> 00:16:36,360
acquire pointers to the data that's

00:16:34,230 --> 00:16:38,130
stored in the the general shared you

00:16:36,360 --> 00:16:40,410
know shared memory store and it can

00:16:38,130 --> 00:16:42,180
acquire references to very complex data

00:16:40,410 --> 00:16:43,950
sets you know dictionaries of numpy

00:16:42,180 --> 00:16:45,870
array is another you know it could

00:16:43,950 --> 00:16:48,180
acquire you know a reference to you know

00:16:45,870 --> 00:16:50,940
a gigabyte or four gigabytes of data for

00:16:48,180 --> 00:16:53,040
effectively free and so that that

00:16:50,940 --> 00:16:55,890
programming model is very powerful in a

00:16:53,040 --> 00:16:58,860
you know single machine multi process

00:16:55,890 --> 00:17:01,800
setting and can be used in a you know

00:16:58,860 --> 00:17:04,199
distributed in a distributed setting as

00:17:01,800 --> 00:17:06,660
far as the era project is concerned you

00:17:04,199 --> 00:17:10,770
know I'm very interested in from the

00:17:06,660 --> 00:17:14,160
point of a in memory format the

00:17:10,770 --> 00:17:16,439
mechanics of memory sharing from there

00:17:14,160 --> 00:17:19,020
we need to build start building a

00:17:16,439 --> 00:17:22,199
library of operator kernels which can

00:17:19,020 --> 00:17:24,179
process aero data primitive you know

00:17:22,199 --> 00:17:26,069
aero data and natively so these are

00:17:24,179 --> 00:17:28,439
things like adding two arrays or

00:17:26,069 --> 00:17:30,330
computing the square root or counting

00:17:28,439 --> 00:17:32,429
the distinct values and an array of

00:17:30,330 --> 00:17:34,110
strings things like that stuff that you

00:17:32,429 --> 00:17:35,970
would find but stuff that you would find

00:17:34,110 --> 00:17:38,070
in pandas composable operator is

00:17:35,970 --> 00:17:40,050
composed mostly operators right and so

00:17:38,070 --> 00:17:43,380
then so once you have the

00:17:40,050 --> 00:17:46,590
operator kernels you need the ability to

00:17:43,380 --> 00:17:49,770
create operator graphs that can

00:17:46,590 --> 00:17:51,990
understand dynamic dispatch to the

00:17:49,770 --> 00:17:55,530
appropriate kernel implementation so

00:17:51,990 --> 00:17:59,100
that if you can create a deferred graph

00:17:55,530 --> 00:18:01,530
of effectively data frame operators and

00:17:59,100 --> 00:18:03,660
then that can be evaluated against data

00:18:01,530 --> 00:18:06,660
that is all in shared memory or may be

00:18:03,660 --> 00:18:08,670
partially in memory and when some of is

00:18:06,660 --> 00:18:11,130
in shared memory and when you're

00:18:08,670 --> 00:18:13,260
evaluating that graph you might you know

00:18:11,130 --> 00:18:15,060
choose to materialise the results and

00:18:13,260 --> 00:18:16,980
shared memories so that the results of

00:18:15,060 --> 00:18:19,350
computation can be easily accessible to

00:18:16,980 --> 00:18:21,390
two other processes so if you look at

00:18:19,350 --> 00:18:23,730
the architecture of something like

00:18:21,390 --> 00:18:25,350
tensor flow or you know modern deep

00:18:23,730 --> 00:18:27,120
learning frameworks they're very they're

00:18:25,350 --> 00:18:29,010
very much architected in this form where

00:18:27,120 --> 00:18:32,190
you have deferred operator graphs

00:18:29,010 --> 00:18:35,340
everything works in a graph dataflow

00:18:32,190 --> 00:18:37,190
execution model so I think that this

00:18:35,340 --> 00:18:40,770
model would work very well for

00:18:37,190 --> 00:18:42,120
processing processing aero data except

00:18:40,770 --> 00:18:45,150
that you know they are the data model

00:18:42,120 --> 00:18:47,370
for this is tables and column they're

00:18:45,150 --> 00:18:48,990
data instead of you know if you were

00:18:47,370 --> 00:18:50,580
using tensor flow you'd be manipulating

00:18:48,990 --> 00:18:53,520
tensors which is just a different data

00:18:50,580 --> 00:18:54,990
model or the hardware vendors then

00:18:53,520 --> 00:18:58,290
engaged in this you mentioned about

00:18:54,990 --> 00:19:00,860
Intel that are their primitives in the

00:18:58,290 --> 00:19:04,310
works for supporting these kinds of

00:19:00,860 --> 00:19:07,260
operations in memory yeah yeah there are

00:19:04,310 --> 00:19:08,640
so you know I have not yet I personally

00:19:07,260 --> 00:19:10,770
have not yet started you know building

00:19:08,640 --> 00:19:13,890
an operator kernel library although it's

00:19:10,770 --> 00:19:15,380
something I plan to start on soon but

00:19:13,890 --> 00:19:19,850
one initiative that's going on right now

00:19:15,380 --> 00:19:22,740
that's being spearheaded by a group of

00:19:19,850 --> 00:19:26,520
vendors who work build software for

00:19:22,740 --> 00:19:28,650
graphics cards GPUs so Nvidia continuum

00:19:26,520 --> 00:19:31,740
analytics is involved map DS a GPU

00:19:28,650 --> 00:19:33,900
database graphics tree some there's some

00:19:31,740 --> 00:19:36,770
other players involved so they are

00:19:33,900 --> 00:19:40,650
working together to create open-source

00:19:36,770 --> 00:19:42,810
GPU kernels which process Aero memory so

00:19:40,650 --> 00:19:46,050
they're calling this GPU data frame and

00:19:42,810 --> 00:19:47,220
they have a library of GPU kernels which

00:19:46,050 --> 00:19:49,560
are implemented for the cuda

00:19:47,220 --> 00:19:53,160
architecture and so the idea is that

00:19:49,560 --> 00:19:56,190
they put Aero memory on the GPU and the

00:19:53,160 --> 00:19:59,040
invoke the their library of operator

00:19:56,190 --> 00:20:01,590
kernels on the data in the GPU and so

00:19:59,040 --> 00:20:03,300
this is really exciting because here you

00:20:01,590 --> 00:20:04,560
have you know arrow which you know

00:20:03,300 --> 00:20:06,750
initially was designed for use on the

00:20:04,560 --> 00:20:08,580
CPU but there's no reason why you can't

00:20:06,750 --> 00:20:11,130
put the data on the GPU and then invoke

00:20:08,580 --> 00:20:13,770
and vote kernels on it so I think we'll

00:20:11,130 --> 00:20:16,500
see more and more of this you know with

00:20:13,770 --> 00:20:18,810
from you know kind of specialized use

00:20:16,500 --> 00:20:21,060
cases where you know either for

00:20:18,810 --> 00:20:24,360
specialized hardware for GPUs you know

00:20:21,060 --> 00:20:26,760
for CPUs but I'm really interested in

00:20:24,360 --> 00:20:28,830
you know from a community perspective of

00:20:26,760 --> 00:20:29,340
bringing together as many people as we

00:20:28,830 --> 00:20:32,310
can

00:20:29,340 --> 00:20:34,230
inside the Apache aero project so that

00:20:32,310 --> 00:20:36,210
we can develop this software together

00:20:34,230 --> 00:20:39,600
and kind of solve some of these problems

00:20:36,210 --> 00:20:41,310
in a way that where there's a kind of a

00:20:39,600 --> 00:20:43,230
neutral ground for for collaboration

00:20:41,310 --> 00:20:46,980
than building a body of reusable

00:20:43,230 --> 00:20:48,720
software wonderful something you

00:20:46,980 --> 00:20:50,910
mentioned here in keynote also and you

00:20:48,720 --> 00:20:53,400
mentioned earlier is about confronting

00:20:50,910 --> 00:20:54,900
these chicken-and-egg problems and I

00:20:53,400 --> 00:20:58,590
wanted to close you had some great

00:20:54,900 --> 00:21:00,330
advice about that yeah well yeah what I

00:20:58,590 --> 00:21:02,130
said in my what am I said in my talk and

00:21:00,330 --> 00:21:04,620
I've said this for several years now is

00:21:02,130 --> 00:21:06,570
that when you don't know what to do in a

00:21:04,620 --> 00:21:10,830
chicken-egg problem you should be be the

00:21:06,570 --> 00:21:13,440
chicken and being the chicken is scary

00:21:10,830 --> 00:21:16,950
because you know you don't you don't

00:21:13,440 --> 00:21:19,980
know if there will ever be any eggs and

00:21:16,950 --> 00:21:21,330
so you have to kind of you have to

00:21:19,980 --> 00:21:22,980
believe and you have to believe in the

00:21:21,330 --> 00:21:25,470
vision and be willing to pursue it for

00:21:22,980 --> 00:21:27,450
for a very long time and often the

00:21:25,470 --> 00:21:29,580
feedback cycles are very delayed so what

00:21:27,450 --> 00:21:31,470
I found in building open source software

00:21:29,580 --> 00:21:34,020
for a long time is that you might build

00:21:31,470 --> 00:21:35,820
something and to you it seems like it's

00:21:34,020 --> 00:21:37,440
really great and you release it but then

00:21:35,820 --> 00:21:38,790
you know the crickets chirping and

00:21:37,440 --> 00:21:41,160
you're not getting any feedback and

00:21:38,790 --> 00:21:43,440
really the feedback cycles take in my

00:21:41,160 --> 00:21:47,610
experience six to twelve months and so

00:21:43,440 --> 00:21:49,560
that that you know so especially when

00:21:47,610 --> 00:21:51,900
you're working on a new problem or one

00:21:49,560 --> 00:21:53,580
that requires you know a lot of

00:21:51,900 --> 00:21:56,370
investment from a lot of parties I think

00:21:53,580 --> 00:21:58,020
arrow falls into this category you have

00:21:56,370 --> 00:21:58,950
to be willing to pursue the vision for a

00:21:58,020 --> 00:22:01,080
very long time

00:21:58,950 --> 00:22:03,300
but there's a snowballing effect where

00:22:01,080 --> 00:22:05,040
you know as more people build

00:22:03,300 --> 00:22:06,359
integrations with arrow like we did we

00:22:05,040 --> 00:22:08,399
just

00:22:06,359 --> 00:22:10,349
the first integration spark between

00:22:08,399 --> 00:22:13,019
arrow and spark was merged for

00:22:10,349 --> 00:22:14,999
accelerating Python on spark so we're

00:22:13,019 --> 00:22:16,559
seeing kind of the initial integrations

00:22:14,999 --> 00:22:18,359
get built and I think as more and more

00:22:16,559 --> 00:22:21,899
of them get get built and we start

00:22:18,359 --> 00:22:23,909
seeing components plugging together that

00:22:21,899 --> 00:22:25,709
all that I'll use arrow I think will see

00:22:23,909 --> 00:22:27,869
this kind of you know hockey stick

00:22:25,709 --> 00:22:30,629
growth at some point in the future where

00:22:27,869 --> 00:22:33,719
it will just be the sensible thing to to

00:22:30,629 --> 00:22:35,759
build arrow support either as a primary

00:22:33,719 --> 00:22:38,639
tool that's gets used in a data

00:22:35,759 --> 00:22:41,999
processing system or as a at least as an

00:22:38,639 --> 00:22:44,249
add-on for data ingest and export so

00:22:41,999 --> 00:22:46,649
we'll see I mean I'm I'm very optimistic

00:22:44,249 --> 00:22:49,409
and you know I think one of the best

00:22:46,649 --> 00:22:51,059
parts of you know build of doing this as

00:22:49,409 --> 00:22:54,059
a community project and trying to get as

00:22:51,059 --> 00:22:56,609
many people to you know lend their voice

00:22:54,059 --> 00:22:57,419
and to explain their use cases is that

00:22:56,609 --> 00:22:59,129
we wouldn't we don't want to leave

00:22:57,419 --> 00:23:02,729
people out and we don't want to you know

00:22:59,129 --> 00:23:04,559
build this project in the ivory tower so

00:23:02,729 --> 00:23:06,809
I think you know I'm happy for it to

00:23:04,559 --> 00:23:08,789
take as long as it takes as long as

00:23:06,809 --> 00:23:10,709
we're building something that that

00:23:08,789 --> 00:23:12,419
people feel good about and that you know

00:23:10,709 --> 00:23:15,149
can sustain an ecosystem of

00:23:12,419 --> 00:23:16,559
interoperable technology for hopefully

00:23:15,149 --> 00:23:18,419
many years to come

00:23:16,559 --> 00:23:21,889
wonderful Thank You Wes that we should

00:23:18,419 --> 00:23:21,889
all the best on this yeah thank you

00:23:28,090 --> 00:23:30,150

YouTube URL: https://www.youtube.com/watch?v=Q7y9l-L8yiU


