Title: Deep Learning and Elastic GPUs Using Jupyter - Tim Gasper, Subbu Rama (Bitfusion)
Publication date: 2017-11-08
Playlist: JupyterCon
Description: 
	Combined with GPUs, Jupyter makes for fast development and fast execution, but it is not always easy to switch from a CPU execution context to GPUs and back. Tim Gasper and Subbu Rama share best practices for doing deep learning with Jupyter and explain how to work with CPUs and GPUs more easily by using Elastic GPUs and quick-switching between custom kernels. 

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,569 --> 00:00:05,260
so just to give you a quick background

00:00:03,159 --> 00:00:08,410
about you know our company essentially

00:00:05,260 --> 00:00:10,960
what we provide is tools and platforms

00:00:08,410 --> 00:00:12,519
to enable people to be able to do deep

00:00:10,960 --> 00:00:14,920
learning without any DevOps and

00:00:12,519 --> 00:00:17,110
infrastructure management that's kind of

00:00:14,920 --> 00:00:19,989
what we do and what we gonna be talking

00:00:17,110 --> 00:00:23,770
about today is essentially how we

00:00:19,989 --> 00:00:26,590
generally you know have our users and

00:00:23,770 --> 00:00:28,599
you know how we ourselves use Jupiter in

00:00:26,590 --> 00:00:32,349
a way where we can elastic light add

00:00:28,599 --> 00:00:33,730
GPUs onto a Jupiter thereby minimizing

00:00:32,349 --> 00:00:36,100
the total cost of ownership that you

00:00:33,730 --> 00:00:37,720
have on GPUs because GPUs you know

00:00:36,100 --> 00:00:39,880
sometimes for tests and dev you can

00:00:37,720 --> 00:00:45,190
probably use it in an optimal way to

00:00:39,880 --> 00:00:46,780
keep your cost very low I'm going to go

00:00:45,190 --> 00:00:47,890
through a couple of you know marketing

00:00:46,780 --> 00:00:49,120
slides you know I'm just going to rush

00:00:47,890 --> 00:00:50,190
through them because everybody kinda

00:00:49,120 --> 00:00:52,300
knows these things

00:00:50,190 --> 00:00:54,570
essentially no deep learning you know

00:00:52,300 --> 00:00:57,220
it's kind of like gold rush right now

00:00:54,570 --> 00:00:59,110
you know it's it's almost impacted every

00:00:57,220 --> 00:01:02,560
industry in robotics entertainment

00:00:59,110 --> 00:01:05,530
automotive you know finance healthcare

00:01:02,560 --> 00:01:08,409
education essentially deep learning in

00:01:05,530 --> 00:01:10,780
AI is kind of like the the next comm or

00:01:08,409 --> 00:01:12,400
it's already too common of the.com so

00:01:10,780 --> 00:01:14,800
every company or every business you add

00:01:12,400 --> 00:01:17,020
AI to it it cannot make sense so just

00:01:14,800 --> 00:01:20,920
kind of why this is really really you

00:01:17,020 --> 00:01:23,290
know hot area and I think AI is probably

00:01:20,920 --> 00:01:27,310
the thing that is you know going to be

00:01:23,290 --> 00:01:30,450
much bigger than the dot-com some people

00:01:27,310 --> 00:01:30,450
call it the host industrial evolution

00:01:34,460 --> 00:01:38,690
so why'd you Peter I'm you know you know

00:01:37,340 --> 00:01:40,280
we are Jupiter pond so we should just

00:01:38,690 --> 00:01:42,500
talk a little bit about why Jupiter

00:01:40,280 --> 00:01:44,810
right you know Jupiter you know some

00:01:42,500 --> 00:01:47,210
people call it an IDE some people call

00:01:44,810 --> 00:01:49,040
it develop an environment now some

00:01:47,210 --> 00:01:51,370
people call it you know I can have like

00:01:49,040 --> 00:01:54,740
a tools and people call it you know

00:01:51,370 --> 00:01:56,930
programming environment you know I think

00:01:54,740 --> 00:01:58,250
it's all of the above

00:01:56,930 --> 00:02:00,590
you know it's just not just one thing

00:01:58,250 --> 00:02:01,970
you know the whole the whole idea of

00:02:00,590 --> 00:02:03,710
Jupiter is foreign you're doing

00:02:01,970 --> 00:02:07,220
development it kind of allows us to

00:02:03,710 --> 00:02:09,860
essentially tell a story while we're

00:02:07,220 --> 00:02:12,500
writing code and we can iterative ly

00:02:09,860 --> 00:02:14,510
write code and ability to share

00:02:12,500 --> 00:02:16,670
notebooks it's phenomenal and you know

00:02:14,510 --> 00:02:20,420
when I first you know came across

00:02:16,670 --> 00:02:22,820
Jupiter I used to be a vim and Emacs a

00:02:20,420 --> 00:02:24,620
guy and I now I actually use Jupiter

00:02:22,820 --> 00:02:26,239
pretty much as an IDE for a lot of

00:02:24,620 --> 00:02:29,810
things I actually do so it's you know

00:02:26,239 --> 00:02:32,750
it's really you know one of the best

00:02:29,810 --> 00:02:34,970
tools I've seen

00:02:32,750 --> 00:02:36,200
it's called multi-language support well

00:02:34,970 --> 00:02:37,989
the most important thing that I really

00:02:36,200 --> 00:02:40,940
like the third is the in browser

00:02:37,989 --> 00:02:42,260
developer environment just to give you a

00:02:40,940 --> 00:02:45,530
clear thing you know my laptop didn't

00:02:42,260 --> 00:02:48,380
have D and I didn't have the the cable

00:02:45,530 --> 00:02:49,970
to connect to HDMI - the new Mac so

00:02:48,380 --> 00:02:51,860
Apple keeps changing their interfaces so

00:02:49,970 --> 00:02:53,930
I had no idea and no idea how do I

00:02:51,860 --> 00:02:56,060
actually do a demo so thanks to Jupiter

00:02:53,930 --> 00:02:58,370
I can actually run it on a browser

00:02:56,060 --> 00:03:00,350
I just leverage defense laptop to be

00:02:58,370 --> 00:03:02,480
able to connection or assure a demo so

00:03:00,350 --> 00:03:04,430
that's so I really like the third

00:03:02,480 --> 00:03:05,690
teacher it's collaborative

00:03:04,430 --> 00:03:08,750
you know people can essentially

00:03:05,690 --> 00:03:12,200
collaborate and share we can reproduce

00:03:08,750 --> 00:03:15,140
things extendable like we have actually

00:03:12,200 --> 00:03:17,510
extended Jupiter to use this concept of

00:03:15,140 --> 00:03:18,920
elastic GP which I'll go over so it's

00:03:17,510 --> 00:03:22,130
pretty extensible the community is very

00:03:18,920 --> 00:03:25,000
you know supporting especially for deep

00:03:22,130 --> 00:03:28,400
learning you know and machine learning

00:03:25,000 --> 00:03:30,590
even as an overarching thing we can show

00:03:28,400 --> 00:03:32,180
a lot of visualizations and you know you

00:03:30,590 --> 00:03:33,950
show a lot of an idiot an alert aches

00:03:32,180 --> 00:03:36,590
etcetera right within Jupiter so this

00:03:33,950 --> 00:03:42,140
you know it's which is why you know we

00:03:36,590 --> 00:03:44,090
have this conference right you know so

00:03:42,140 --> 00:03:45,950
some of the things that we basically now

00:03:44,090 --> 00:03:47,390
use you know essentially as the best

00:03:45,950 --> 00:03:49,910
practices for doing

00:03:47,390 --> 00:03:51,410
deep learning with Jupiter before I do

00:03:49,910 --> 00:03:53,450
that before I said how many people in

00:03:51,410 --> 00:03:56,230
this room you know kind of actually do

00:03:53,450 --> 00:04:01,160
convolution neural Nets with Jupiter

00:03:56,230 --> 00:04:02,810
okay I'm assuming rest of the mean how

00:04:01,160 --> 00:04:06,020
many people in the room use Jupiter for

00:04:02,810 --> 00:04:07,670
machine learning okay so I'm assuming

00:04:06,020 --> 00:04:09,260
the rest of folks not gonna use Jupiter

00:04:07,670 --> 00:04:11,720
for various you know other statistical

00:04:09,260 --> 00:04:13,370
things I guess so essentially when we're

00:04:11,720 --> 00:04:14,959
basically doing you know in our deep

00:04:13,370 --> 00:04:17,390
learning with Jupiter so the best

00:04:14,959 --> 00:04:21,079
practices that we have seen is you know

00:04:17,390 --> 00:04:23,300
we use docker you know essentially as a

00:04:21,079 --> 00:04:25,430
package a way to essentially access

00:04:23,300 --> 00:04:27,140
Jupiter and essentially you know we

00:04:25,430 --> 00:04:29,120
basically do essentially a port mapping

00:04:27,140 --> 00:04:30,440
wherever the doctor container is so that

00:04:29,120 --> 00:04:34,190
we can essentially map it to any kind of

00:04:30,440 --> 00:04:35,930
localhost and just run the docker and we

00:04:34,190 --> 00:04:39,980
use essentially a persistent data volume

00:04:35,930 --> 00:04:41,180
to store our data and codes essentially

00:04:39,980 --> 00:04:43,820
we have a data volume which I'll kind of

00:04:41,180 --> 00:04:47,600
show or using you know our dev

00:04:43,820 --> 00:04:49,310
environment we use gate essentially to

00:04:47,600 --> 00:04:51,230
pretty much in a gate and get up to can

00:04:49,310 --> 00:04:53,690
have snow save code and you know kind of

00:04:51,230 --> 00:04:55,220
use it as essentially sometimes even a

00:04:53,690 --> 00:04:57,020
temporary faucet or as well as a more

00:04:55,220 --> 00:05:01,340
persistent repository but versioning

00:04:57,020 --> 00:05:02,600
reproducibility etc and again we

00:05:01,340 --> 00:05:04,430
leverage examples as much as possible

00:05:02,600 --> 00:05:06,530
because it was done of Jupiter notebooks

00:05:04,430 --> 00:05:09,020
will is there in fact sometimes we don't

00:05:06,530 --> 00:05:10,820
need actually powerpoints for explaining

00:05:09,020 --> 00:05:12,700
anything we just need a Jupiter notebook

00:05:10,820 --> 00:05:17,840
and that basically is like the

00:05:12,700 --> 00:05:22,040
PowerPoint or the documentation oh this

00:05:17,840 --> 00:05:24,500
is another no you know slide which kind

00:05:22,040 --> 00:05:26,690
of tells you like hey CPUs versus GPUs

00:05:24,500 --> 00:05:31,940
how many people in the room actually use

00:05:26,690 --> 00:05:34,729
GPUs for your work okay so you know I

00:05:31,940 --> 00:05:39,350
mean as you as you can have no no GPUs

00:05:34,729 --> 00:05:41,390
are an order of magnitude and there's

00:05:39,350 --> 00:05:44,440
two orders of magnitude faster and you

00:05:41,390 --> 00:05:47,060
know CPUs especially for you know

00:05:44,440 --> 00:05:49,400
certain types of workloads which you

00:05:47,060 --> 00:05:51,890
know happens to fall in the machine

00:05:49,400 --> 00:05:55,520
learning deep learning you know space

00:05:51,890 --> 00:05:58,070
and the primary thing with GPUs is with

00:05:55,520 --> 00:06:01,300
each new generation of GPUs it's just

00:05:58,070 --> 00:06:04,090
phenomenal at which you know

00:06:01,300 --> 00:06:06,860
the processing power has increased

00:06:04,090 --> 00:06:08,720
before passed and even Pascal came in it

00:06:06,860 --> 00:06:10,520
was basically you know five to ten times

00:06:08,720 --> 00:06:12,560
faster than the previous one then now

00:06:10,520 --> 00:06:14,390
Volta which is gonna be coming in end of

00:06:12,560 --> 00:06:15,830
the year it's going to essentially we

00:06:14,390 --> 00:06:16,730
gain you know at least four to five

00:06:15,830 --> 00:06:18,530
times faster than the previous

00:06:16,730 --> 00:06:19,610
generation at some point I think you

00:06:18,530 --> 00:06:22,760
know Moore's law is probably gonna hit

00:06:19,610 --> 00:06:24,350
you know GPUs as well but we'll see you

00:06:22,760 --> 00:06:26,900
know and that's probably why people are

00:06:24,350 --> 00:06:28,310
kind of building Asics like you know the

00:06:26,900 --> 00:06:30,230
tensile processing unit and a lot of

00:06:28,310 --> 00:06:33,560
companies building essentially a six to

00:06:30,230 --> 00:06:36,290
do like graph based you know algorithms

00:06:33,560 --> 00:06:38,000
etc you know much better than even

00:06:36,290 --> 00:06:40,190
potentially even doing it in a

00:06:38,000 --> 00:06:41,630
traditional GPU but then when GPUs are

00:06:40,190 --> 00:06:43,820
the new Volta is going to basically have

00:06:41,630 --> 00:06:46,460
a tensile unit which is kind of like

00:06:43,820 --> 00:06:48,800
something people can liberate directly

00:06:46,460 --> 00:06:50,300
for no conversion neuron that's

00:06:48,800 --> 00:06:51,860
essentially some of the essentially

00:06:50,300 --> 00:06:54,650
functions are pretty much baked in

00:06:51,860 --> 00:06:56,720
hardware you know sometimes you know

00:06:54,650 --> 00:06:58,700
when our CPUs are still good for certain

00:06:56,720 --> 00:07:00,830
things you can use it for like in our

00:06:58,700 --> 00:07:04,040
Devon test just learning you should

00:07:00,830 --> 00:07:05,330
prototyping you can essentially use your

00:07:04,040 --> 00:07:07,730
existing infrastructure pretty much

00:07:05,330 --> 00:07:11,390
everybody has a CPU you know machine

00:07:07,730 --> 00:07:13,400
every laptop has a CPU most of the data

00:07:11,390 --> 00:07:17,840
prep and transformations are still done

00:07:13,400 --> 00:07:19,310
on CPUs sometimes the inference you know

00:07:17,840 --> 00:07:21,170
it's kind of done on CPUs today

00:07:19,310 --> 00:07:24,800
especially when things are not at

00:07:21,170 --> 00:07:27,920
massive scale you know it has you know

00:07:24,800 --> 00:07:30,860
CPUs do have much more maturity than a

00:07:27,920 --> 00:07:33,020
GP because CP uses it's kind of like the

00:07:30,860 --> 00:07:35,240
very most important first classes and

00:07:33,020 --> 00:07:37,250
it's got multitasking you know it got

00:07:35,240 --> 00:07:38,840
more memory which is which is kind of

00:07:37,250 --> 00:07:40,340
actually not good

00:07:38,840 --> 00:07:42,440
you can hold position your memory sort

00:07:40,340 --> 00:07:44,480
of the virtual memory concept you can

00:07:42,440 --> 00:07:45,980
basically do state tracking you can do

00:07:44,480 --> 00:07:49,850
Rahzel and all these things you know

00:07:45,980 --> 00:07:51,890
CPUs have so so the reason I'm asking

00:07:49,850 --> 00:07:53,300
I'm saying that is you know as the next

00:07:51,890 --> 00:07:54,440
in the next slide I'm gonna be kind of

00:07:53,300 --> 00:07:56,630
showing you the life cycle of

00:07:54,440 --> 00:07:58,640
essentially deep learning development

00:07:56,630 --> 00:08:00,530
where you have a development and then

00:07:58,640 --> 00:08:02,210
you do some training of your neural nets

00:08:00,530 --> 00:08:04,430
last scale training and then you do

00:08:02,210 --> 00:08:06,020
deployment so mostly in the developments

00:08:04,430 --> 00:08:09,500
when people do development on the CPUs

00:08:06,020 --> 00:08:12,320
or GPUs and when people are using GPUs

00:08:09,500 --> 00:08:13,770
you essentially sometimes log into the

00:08:12,320 --> 00:08:15,930
whole GPU machine

00:08:13,770 --> 00:08:17,280
and use the GP machine you know all the

00:08:15,930 --> 00:08:18,870
time even though most of the time you

00:08:17,280 --> 00:08:21,180
have sitting and writing code or typing

00:08:18,870 --> 00:08:23,669
code pretty much and I'll kind of know

00:08:21,180 --> 00:08:26,729
kind of tie it all together you know in

00:08:23,669 --> 00:08:28,470
the next few minutes on how elastic GPUs

00:08:26,729 --> 00:08:31,680
and what I'm going to be showing kind of

00:08:28,470 --> 00:08:33,510
ties into you know why it's important

00:08:31,680 --> 00:08:35,039
when people are doing it development on

00:08:33,510 --> 00:08:37,950
CPUs most of the elements happening on

00:08:35,039 --> 00:08:39,450
the CPUs today again slide kind of shows

00:08:37,950 --> 00:08:40,860
that development as we finish it

00:08:39,450 --> 00:08:43,680
development you do your training that's

00:08:40,860 --> 00:08:44,940
where you run massive you know GPU forms

00:08:43,680 --> 00:08:46,980
there you kind of want to do your

00:08:44,940 --> 00:08:48,899
training much faster because there's a

00:08:46,980 --> 00:08:50,370
lot of our data that's there and then

00:08:48,899 --> 00:08:51,839
you finish your training and then say

00:08:50,370 --> 00:08:53,339
okay here's and you know like I want to

00:08:51,839 --> 00:08:55,740
deploy it you know create a bunch of

00:08:53,339 --> 00:08:57,209
functions and you can have deployed on

00:08:55,740 --> 00:09:01,709
edge nodes or you know whatever nodes

00:08:57,209 --> 00:09:03,810
that you want to deploy to so however

00:09:01,709 --> 00:09:06,089
when you look at GPUs you know GPUs are

00:09:03,810 --> 00:09:08,550
not not cheap right so if you look at

00:09:06,089 --> 00:09:10,970
the cloud instance pricing you know AWS

00:09:08,550 --> 00:09:13,709
these are these may be a little dated

00:09:10,970 --> 00:09:15,450
you know this is about a few months old

00:09:13,709 --> 00:09:17,279
but I think the price has not changed

00:09:15,450 --> 00:09:19,020
significantly if you really look at

00:09:17,279 --> 00:09:23,220
actually a per year pricing of actually

00:09:19,020 --> 00:09:25,589
using a GPU on AWS or IBM or Azure or in

00:09:23,220 --> 00:09:29,430
Google you know to basically use

00:09:25,589 --> 00:09:30,839
essentially what half a k80 cost about

00:09:29,430 --> 00:09:32,610
close to eight thousand dollars per year

00:09:30,839 --> 00:09:36,930
and if you really want to essentially

00:09:32,610 --> 00:09:38,430
use about 8 k ds 8 GPUs cost $130,000

00:09:36,930 --> 00:09:43,709
per year you just don't see it but it

00:09:38,430 --> 00:09:46,110
just quickly keeps adding up but most of

00:09:43,709 --> 00:09:47,310
the time we're just typing basically we

00:09:46,110 --> 00:09:49,320
basically get a machine and say okay I'm

00:09:47,310 --> 00:09:50,880
just gonna still run type and while the

00:09:49,320 --> 00:09:53,370
GP is sitting idle gp's are not really

00:09:50,880 --> 00:09:55,050
even being used and majority of the you

00:09:53,370 --> 00:09:57,089
know even the application runtime is

00:09:55,050 --> 00:10:00,120
still using CPUs even you're running the

00:09:57,089 --> 00:10:02,070
application so what we have actually

00:10:00,120 --> 00:10:04,470
kind of done is this concept of elastic

00:10:02,070 --> 00:10:06,510
GPUs where you can basically attach GPUs

00:10:04,470 --> 00:10:08,910
to any CPU nodes so very similar to the

00:10:06,510 --> 00:10:10,770
concept of storage how you do network

00:10:08,910 --> 00:10:12,899
attached storage or EFS where you

00:10:10,770 --> 00:10:15,930
basically have a storage node and you

00:10:12,899 --> 00:10:17,700
basically have some CPU nodes just like

00:10:15,930 --> 00:10:20,160
you know how you can attach storage

00:10:17,700 --> 00:10:21,360
through a network onto a CPU node we

00:10:20,160 --> 00:10:24,900
basically have a queue created a way

00:10:21,360 --> 00:10:25,949
where you can attach GPUs on to any CPU

00:10:24,900 --> 00:10:26,819
node and what that really means is for

00:10:25,949 --> 00:10:29,939
example if

00:10:26,819 --> 00:10:31,319
my laptop does not have any GPUs let's

00:10:29,939 --> 00:10:33,899
say if I have another workstation with

00:10:31,319 --> 00:10:35,609
GPUs what I can actually do is I could

00:10:33,899 --> 00:10:38,069
basically remote your attach the GPU

00:10:35,609 --> 00:10:39,660
onto my laptop kind of make it look like

00:10:38,069 --> 00:10:41,369
you know it's like a first class citizen

00:10:39,660 --> 00:10:42,989
just like how you basically you know

00:10:41,369 --> 00:10:45,059
when you want to attach NFS you don't

00:10:42,989 --> 00:10:46,229
really go and keep buying disk to your

00:10:45,059 --> 00:10:49,199
laptop you just attach something in a

00:10:46,229 --> 00:10:50,910
shade for a shared drive and so the

00:10:49,199 --> 00:10:52,319
advantage of that is basically you know

00:10:50,910 --> 00:10:53,759
of course you know running things on a

00:10:52,319 --> 00:10:56,789
native GPU is gonna be much more

00:10:53,759 --> 00:10:58,619
performant but for a lot of cases where

00:10:56,789 --> 00:11:00,539
you're doing test and depth it's pretty

00:10:58,619 --> 00:11:01,799
easy pretty useful because most of the

00:11:00,539 --> 00:11:03,600
times you're basically making sure your

00:11:01,799 --> 00:11:05,489
code works making sure you things

00:11:03,600 --> 00:11:07,979
actually working multi-gpu things kind

00:11:05,489 --> 00:11:09,389
of actually scale in those cases it

00:11:07,979 --> 00:11:11,309
really works very well so essentially

00:11:09,389 --> 00:11:14,549
for example if I let's say if how say

00:11:11,309 --> 00:11:16,919
just to GPU nodes and let's say put 10

00:11:14,549 --> 00:11:19,199
or 20 ignore developers warning to

00:11:16,919 --> 00:11:20,669
actually leverage it what you could do

00:11:19,199 --> 00:11:22,319
is basically install you know

00:11:20,669 --> 00:11:24,600
essentially a software on all the

00:11:22,319 --> 00:11:26,100
workstations or all the laptops and

00:11:24,600 --> 00:11:28,319
basically install the software on GPU

00:11:26,100 --> 00:11:31,589
nodes and pretty much access the GPU

00:11:28,319 --> 00:11:34,289
everywhere essentially you know same you

00:11:31,589 --> 00:11:35,729
know you know even a data center as well

00:11:34,289 --> 00:11:37,619
in the same way where I can basically

00:11:35,729 --> 00:11:39,809
create these different SKUs for example

00:11:37,619 --> 00:11:42,629
I can basically say I can get a user can

00:11:39,809 --> 00:11:45,689
get a point GPU or one GPU or two GPU or

00:11:42,629 --> 00:11:49,769
three GPU for GPU at runtime as opposed

00:11:45,689 --> 00:11:51,029
to hogging on the GPU all the time again

00:11:49,769 --> 00:11:52,499
when it comes to the cloud it becomes

00:11:51,029 --> 00:11:54,509
very interesting where I can basically

00:11:52,499 --> 00:11:56,759
now attach you know GPUs to large memory

00:11:54,509 --> 00:11:59,339
instances for example you know AWS has

00:11:56,759 --> 00:12:00,509
you know you know like large memory

00:11:59,339 --> 00:12:02,519
instance different different types of

00:12:00,509 --> 00:12:05,189
SKUs so now I can basically create like

00:12:02,519 --> 00:12:06,539
you know new SKUs on the fly and I kind

00:12:05,189 --> 00:12:08,609
of show you guys you know what that

00:12:06,539 --> 00:12:10,350
means kind of in a demo as well I can

00:12:08,609 --> 00:12:14,100
basically an attach it to essentially

00:12:10,350 --> 00:12:17,609
the the x1 instances the day w's has the

00:12:14,100 --> 00:12:19,319
you know the you know the which is over

00:12:17,609 --> 00:12:21,299
a terabyte we can basically a GPU so

00:12:19,319 --> 00:12:23,609
that I can attach GPUs to a tee to micro

00:12:21,299 --> 00:12:25,470
or a tee to nano or a do too small and

00:12:23,609 --> 00:12:27,449
now essentially just like how I do

00:12:25,470 --> 00:12:29,159
elastic for no file storage a by the way

00:12:27,449 --> 00:12:34,139
how many people here you know you use

00:12:29,159 --> 00:12:36,900
AWS for their stuff cool I'm will use

00:12:34,139 --> 00:12:42,490
Azure just curious

00:12:36,900 --> 00:12:47,470
Google IBM probably not I guess it's one

00:12:42,490 --> 00:12:49,570
okay good so the way actually we we do

00:12:47,470 --> 00:12:51,070
this thing is basically you know through

00:12:49,570 --> 00:12:52,570
something that we wrote called you know

00:12:51,070 --> 00:12:53,800
application interception so basically

00:12:52,570 --> 00:12:55,900
what it does is you know if you look at

00:12:53,800 --> 00:12:59,020
GPUs and most of the gpo code is all you

00:12:55,900 --> 00:13:02,110
know if you know so GPUs that so many

00:12:59,020 --> 00:13:03,940
types of GPUs but then the real majority

00:13:02,110 --> 00:13:05,260
of GPUs are all in video HD use and

00:13:03,940 --> 00:13:07,540
especially if you look at deep learning

00:13:05,260 --> 00:13:09,310
most of it is pretty much done on NVIDIA

00:13:07,540 --> 00:13:11,830
GPUs so if you look at actually you know

00:13:09,310 --> 00:13:13,210
any NVIDIA GPUs that any GPUs they all

00:13:11,830 --> 00:13:14,920
support you know essentially a

00:13:13,210 --> 00:13:16,870
particular runtime for example a media

00:13:14,920 --> 00:13:18,370
supports CUDA so what we basically have

00:13:16,870 --> 00:13:20,320
is we basically have a software that we

00:13:18,370 --> 00:13:23,980
can have Road it essentially intercepts

00:13:20,320 --> 00:13:26,890
CUDA and remote execute the CUDA calls

00:13:23,980 --> 00:13:28,600
on other machines and which has the GPUs

00:13:26,890 --> 00:13:30,220
for example if I'm basically running it

00:13:28,600 --> 00:13:32,560
prism has run so like an application or

00:13:30,220 --> 00:13:35,020
on the node kind of like a demon or an

00:13:32,560 --> 00:13:36,970
agent so it basically intercepts the

00:13:35,020 --> 00:13:38,890
CUDA calls all the CUDA functions and it

00:13:36,970 --> 00:13:41,110
kind of now remote execute on any you

00:13:38,890 --> 00:13:43,080
know any GP or any sound GPU that you

00:13:41,110 --> 00:13:45,730
have then it all works through tcp/ip

00:13:43,080 --> 00:13:46,990
now in a client silver configuration but

00:13:45,730 --> 00:13:49,210
to the application it can have looks

00:13:46,990 --> 00:13:50,380
like what's shown on the right it all

00:13:49,210 --> 00:13:52,120
looks like a single machine so it

00:13:50,380 --> 00:13:54,820
basically for example you know when you

00:13:52,120 --> 00:13:56,230
do let's say a native NVDA SMI command

00:13:54,820 --> 00:13:57,550
which kind of shows you the list of GPUs

00:13:56,230 --> 00:14:00,340
that you have in your machine will show

00:13:57,550 --> 00:14:02,290
as if you have all the GPU so it for the

00:14:00,340 --> 00:14:03,460
applications or for the people or using

00:14:02,290 --> 00:14:05,140
it they don't even see the difference

00:14:03,460 --> 00:14:06,730
I'll kind of show you guys where I'm

00:14:05,140 --> 00:14:10,360
going to be able to show you hey a bunch

00:14:06,730 --> 00:14:11,680
of GPUs on a single machine and when you

00:14:10,360 --> 00:14:12,700
do that you know essentially you go on

00:14:11,680 --> 00:14:13,960
and this is kind of like the

00:14:12,700 --> 00:14:15,730
architecture of how we actually do it

00:14:13,960 --> 00:14:18,310
essentially we have this thing you know

00:14:15,730 --> 00:14:19,660
we call it like a bit fusion client you

00:14:18,310 --> 00:14:21,820
know which is running essentially on a

00:14:19,660 --> 00:14:23,350
CPU node it could be running on a GPU

00:14:21,820 --> 00:14:24,640
node as well where you could basically

00:14:23,350 --> 00:14:26,620
have two GPU nodes and we can combine

00:14:24,640 --> 00:14:28,390
the GPUs across both the node and create

00:14:26,620 --> 00:14:30,010
a really massive powerful node as well

00:14:28,390 --> 00:14:33,220
so that people don't have to even write

00:14:30,010 --> 00:14:34,360
distributed code so essentially it

00:14:33,220 --> 00:14:36,520
basically works through a client-server

00:14:34,360 --> 00:14:38,410
architecture so essentially we

00:14:36,520 --> 00:14:40,420
essentially have a CPU server we have a

00:14:38,410 --> 00:14:42,760
GPU server essentially

00:14:40,420 --> 00:14:44,170
big fusion client intercepts everything

00:14:42,760 --> 00:14:47,680
and there's a server that's running on

00:14:44,170 --> 00:14:49,329
the GPU in a node and we essentially

00:14:47,680 --> 00:14:51,309
remote execute the calls and you know

00:14:49,329 --> 00:14:52,480
we cannot manage all the you know micro

00:14:51,309 --> 00:14:53,920
scheduling and everything under the hood

00:14:52,480 --> 00:14:57,249
done just to give you some performance

00:14:53,920 --> 00:14:59,410
you know data when we actually we did a

00:14:57,249 --> 00:15:01,239
lot of work and essentially overlapping

00:14:59,410 --> 00:15:02,769
data movement as much as possible and

00:15:01,239 --> 00:15:03,999
hiding inside computer as much as

00:15:02,769 --> 00:15:06,160
possible and a lot of performance

00:15:03,999 --> 00:15:07,689
optimization tricks so we've actually

00:15:06,160 --> 00:15:09,939
seen that when you actually have let's

00:15:07,689 --> 00:15:11,739
say even on 10 gig when you have a CPU

00:15:09,939 --> 00:15:13,569
node and a GPU node connected together

00:15:11,739 --> 00:15:14,860
and let's say an applications running on

00:15:13,569 --> 00:15:16,689
a CPU in order remotely running on the

00:15:14,860 --> 00:15:18,069
GPU node the performance is very close

00:15:16,689 --> 00:15:19,449
to native as a kind of you're like

00:15:18,069 --> 00:15:21,879
running on a local of course as you

00:15:19,449 --> 00:15:24,189
scale your GPUs you know you do get cc's

00:15:21,879 --> 00:15:25,259
and performance hits but again I think

00:15:24,189 --> 00:15:27,970
the world is kind of moving towards

00:15:25,259 --> 00:15:29,920
InfiniBand you know the next few years

00:15:27,970 --> 00:15:31,899
anyway is particularly driven by deep

00:15:29,920 --> 00:15:33,519
learning so when that happens pretty

00:15:31,899 --> 00:15:38,079
much it's going to be you know pretty

00:15:33,519 --> 00:15:38,889
close to PCI speeds here's another thing

00:15:38,079 --> 00:15:40,329
that we can actually do which is really

00:15:38,889 --> 00:15:41,889
cool because because we actually

00:15:40,329 --> 00:15:45,009
intercepting everything we can also

00:15:41,889 --> 00:15:46,600
actually split GPUs for example let's

00:15:45,009 --> 00:15:48,730
say you're let's say you have let's say

00:15:46,600 --> 00:15:50,079
a single GPS a server or a workstation

00:15:48,730 --> 00:15:51,610
with less one GPU let's say I have like

00:15:50,079 --> 00:15:53,199
four developers wanting to basically

00:15:51,610 --> 00:15:55,720
access it right and as you know

00:15:53,199 --> 00:15:57,249
tensorflow is very greedy where let's

00:15:55,720 --> 00:15:58,929
say if tensorflow is basically using a

00:15:57,249 --> 00:16:01,119
GPU it doesn't give it to anybody else

00:15:58,929 --> 00:16:02,619
so what we actually could do is we could

00:16:01,119 --> 00:16:05,860
basically set policies where you say

00:16:02,619 --> 00:16:07,629
split the GPU into four pieces and EGP

00:16:05,860 --> 00:16:10,209
each user or each you know a developer

00:16:07,629 --> 00:16:11,709
gets 1/4 of the GPU again the

00:16:10,209 --> 00:16:13,089
performance is of course going to be 1/4

00:16:11,709 --> 00:16:17,199
of what you would actually done on a

00:16:13,089 --> 00:16:20,319
full GPU but you could actually use it

00:16:17,199 --> 00:16:23,470
and we can basically know allocate a

00:16:20,319 --> 00:16:25,959
particular size of GPU memory to the

00:16:23,470 --> 00:16:28,269
user one of the things we actually also

00:16:25,959 --> 00:16:29,410
doing it as we also you know going to be

00:16:28,269 --> 00:16:31,029
adding a way to essentially

00:16:29,410 --> 00:16:33,459
over-provision the memory so which means

00:16:31,029 --> 00:16:36,069
we could basically if we have it a 12

00:16:33,459 --> 00:16:38,619
gig nor GPU we could basically run

00:16:36,069 --> 00:16:40,419
essentially two applications which

00:16:38,619 --> 00:16:41,769
basically has 12 gig you know which

00:16:40,419 --> 00:16:43,480
requires 12 gives a GPU memory

00:16:41,769 --> 00:16:45,160
so essentially slicing the GB but also

00:16:43,480 --> 00:16:47,019
kind of increasing the memory a little

00:16:45,160 --> 00:16:48,249
bit again all these things is basically

00:16:47,019 --> 00:16:50,529
done through the interception

00:16:48,249 --> 00:16:51,879
you know layer that we've kind of back

00:16:50,529 --> 00:16:54,100
here has it done and I'll kind of show

00:16:51,879 --> 00:16:56,529
you guys in a demo all these things tied

00:16:54,100 --> 00:16:59,410
together using in a workspace a little

00:16:56,529 --> 00:17:00,759
bit again why you know why elastic GPUs

00:16:59,410 --> 00:17:02,090
right again most of the people basically

00:17:00,759 --> 00:17:04,709
start with a small team

00:17:02,090 --> 00:17:06,990
you basically know as your team grows

00:17:04,709 --> 00:17:08,640
you want to be included GPU servers but

00:17:06,990 --> 00:17:09,900
then let's say what if everybody in the

00:17:08,640 --> 00:17:11,400
company wants you basically access the

00:17:09,900 --> 00:17:13,530
GPUs you can't really you know provide I

00:17:11,400 --> 00:17:15,060
know GPUs to everybody in the in the

00:17:13,530 --> 00:17:17,880
company I mean we had this we had this

00:17:15,060 --> 00:17:19,350
customer where there was a development

00:17:17,880 --> 00:17:21,150
team you know AI development team which

00:17:19,350 --> 00:17:23,160
owned all the GPUs then some but some

00:17:21,150 --> 00:17:24,300
people in the application teams or in IT

00:17:23,160 --> 00:17:26,070
teams they actually wanted to use the

00:17:24,300 --> 00:17:28,140
GPUs but these guys did never want to

00:17:26,070 --> 00:17:29,370
give the GPUs to them because and of

00:17:28,140 --> 00:17:30,870
course they get more priority because

00:17:29,370 --> 00:17:32,430
they were and now they were kind of on

00:17:30,870 --> 00:17:33,510
the CTO organization and they would

00:17:32,430 --> 00:17:34,650
actually never want to give that thing

00:17:33,510 --> 00:17:36,390
to anybody else but everybody want to

00:17:34,650 --> 00:17:38,430
use it so here you know you could

00:17:36,390 --> 00:17:40,820
basically install essentially kind of

00:17:38,430 --> 00:17:43,620
the software on all the CP machines on

00:17:40,820 --> 00:17:45,600
limited to my GP machines but everybody

00:17:43,620 --> 00:17:47,880
can get pretty much getting off access

00:17:45,600 --> 00:17:50,400
and we call it sort of like a democratic

00:17:47,880 --> 00:17:51,930
access of GPUs everywhere right because

00:17:50,400 --> 00:17:53,820
once you have access to you know

00:17:51,930 --> 00:17:55,290
essentially amazing hardware amazing

00:17:53,820 --> 00:17:57,480
technology you know people come up with

00:17:55,290 --> 00:17:59,760
new innovative things and it may or may

00:17:57,480 --> 00:18:01,650
not be just the people who have you know

00:17:59,760 --> 00:18:03,630
privileged rights I mean just kind of

00:18:01,650 --> 00:18:05,250
why you have a lot of startups you know

00:18:03,630 --> 00:18:07,110
coming of new ideas and not just Google

00:18:05,250 --> 00:18:12,390
and Amazon coming with the greatest

00:18:07,110 --> 00:18:14,670
ideas again essentially the whole idea

00:18:12,390 --> 00:18:18,810
is to make it enterprise wide you know

00:18:14,670 --> 00:18:20,220
adoption gain a few more in all things

00:18:18,810 --> 00:18:21,570
where now you can basically start your

00:18:20,220 --> 00:18:23,340
development on a CP machine you know

00:18:21,570 --> 00:18:24,720
attach essentially an elastic GPU from

00:18:23,340 --> 00:18:26,850
within a you know a node

00:18:24,720 --> 00:18:28,170
okay you can attach additional GPUs just

00:18:26,850 --> 00:18:29,940
like your attached storage a I just want

00:18:28,170 --> 00:18:32,790
more GPUs just like I attached storage

00:18:29,940 --> 00:18:35,360
today to a you know EFS or I can

00:18:32,790 --> 00:18:39,170
basically even you know kind of you know

00:18:35,360 --> 00:18:42,090
partial you know create partial GPUs X

00:18:39,170 --> 00:18:43,440
again night I went through this you know

00:18:42,090 --> 00:18:44,010
before which is basically the

00:18:43,440 --> 00:18:47,390
client-server

00:18:44,010 --> 00:18:50,100
you know mode again you know split GPUs

00:18:47,390 --> 00:18:51,300
I could also actually run like you know

00:18:50,100 --> 00:18:53,010
essentially you know multiple

00:18:51,300 --> 00:18:54,780
applications can be run multiple users

00:18:53,010 --> 00:18:56,310
can actually be run its multi-tenant

00:18:54,780 --> 00:18:58,140
you can do all sort of things once you

00:18:56,310 --> 00:19:02,010
have access to this you know this kind

00:18:58,140 --> 00:19:03,240
of an infrastructure so one of the

00:19:02,010 --> 00:19:05,300
things we actually did was basically

00:19:03,240 --> 00:19:08,280
with Jupiter what we did essentially is

00:19:05,300 --> 00:19:09,810
we basically essentially add the added a

00:19:08,280 --> 00:19:12,030
capability where when you run Jupiter

00:19:09,810 --> 00:19:15,330
with we call it the bit efficient client

00:19:12,030 --> 00:19:17,070
it basically uses CPU most of the time

00:19:15,330 --> 00:19:18,690
only when it's using the GPU it goes and

00:19:17,070 --> 00:19:20,520
fetches heart as a GPU that you need

00:19:18,690 --> 00:19:22,920
till then it's not even actually you

00:19:20,520 --> 00:19:25,320
know the GPU is not even allocated it's

00:19:22,920 --> 00:19:27,120
it could be used by another user as

00:19:25,320 --> 00:19:29,100
opposed to essentially you know hogging

00:19:27,120 --> 00:19:30,960
the GPU so essentially only when you're

00:19:29,100 --> 00:19:32,880
running your code it uses the GPU so

00:19:30,960 --> 00:19:35,310
when you're typing not even using the

00:19:32,880 --> 00:19:37,530
GPU and we have this ability where you

00:19:35,310 --> 00:19:38,970
can basically have multiple users each

00:19:37,530 --> 00:19:42,450
user can get like half and sheep you

00:19:38,970 --> 00:19:45,210
quarter GPU or even like you know GPU is

00:19:42,450 --> 00:19:47,370
beyond what is available in a single

00:19:45,210 --> 00:19:49,320
node when we started the thing what we

00:19:47,370 --> 00:19:52,020
did was we basically you know modify the

00:19:49,320 --> 00:19:54,420
kernel to essentially add elastic GPUs

00:19:52,020 --> 00:19:56,190
to for example a Python process that's

00:19:54,420 --> 00:20:00,600
basically run but then as we kind of

00:19:56,190 --> 00:20:02,310
know made on progress now what we do is

00:20:00,600 --> 00:20:04,110
we basically whole we wrap the whole

00:20:02,310 --> 00:20:06,000
Jeep Jupiter kind of like this GP

00:20:04,110 --> 00:20:08,300
virtualization so that it kind of

00:20:06,000 --> 00:20:10,470
inherently you know automatically knows

00:20:08,300 --> 00:20:13,140
essentially the resource management

00:20:10,470 --> 00:20:14,730
piece you know in it we don't need to do

00:20:13,140 --> 00:20:17,160
any any change I'm gonna show you guys

00:20:14,730 --> 00:20:19,290
in a demo that predict your piece again

00:20:17,160 --> 00:20:21,390
you know so we actually wrap this all

00:20:19,290 --> 00:20:22,860
around and actually no sell this as a

00:20:21,390 --> 00:20:25,260
product as well and be if you have it

00:20:22,860 --> 00:20:27,510
and has an ami on Amazon as well there

00:20:25,260 --> 00:20:29,760
we call it flex essentially it's

00:20:27,510 --> 00:20:31,320
essentially a DevOps an infrastructure

00:20:29,760 --> 00:20:32,700
management you know a development

00:20:31,320 --> 00:20:35,460
platform for deep learning in AI so

00:20:32,700 --> 00:20:37,650
basically it's like a dev cockpit so

00:20:35,460 --> 00:20:40,230
that data scientists or developers

00:20:37,650 --> 00:20:42,840
without the help of IT can kind of

00:20:40,230 --> 00:20:44,430
manage the GPU infrastructure and also

00:20:42,840 --> 00:20:45,900
manage their deep learning workflows we

00:20:44,430 --> 00:20:48,000
we provide pre-installed you know

00:20:45,900 --> 00:20:50,790
containers which are already you know

00:20:48,000 --> 00:20:52,860
optimized we actually by default we

00:20:50,790 --> 00:20:57,780
basically provide Jupiter you know in

00:20:52,860 --> 00:20:59,310
our you know in our software ok the

00:20:57,780 --> 00:21:00,600
whole lifecycle that I showed before can

00:20:59,310 --> 00:21:02,010
have changes a little bit where you know

00:21:00,600 --> 00:21:03,240
when I'm actually doing with development

00:21:02,010 --> 00:21:04,920
they can basically use this thing to

00:21:03,240 --> 00:21:06,870
essentially use all the preinstalled you

00:21:04,920 --> 00:21:09,120
know containers etc and then start with

00:21:06,870 --> 00:21:10,620
my CPUs and then just attach GPUs as and

00:21:09,120 --> 00:21:12,180
when I need it and then when I'm

00:21:10,620 --> 00:21:14,670
training it I can basically kind of

00:21:12,180 --> 00:21:16,380
scale out beyond my node and combine

00:21:14,670 --> 00:21:17,820
GPUs across multiple nodes and scale out

00:21:16,380 --> 00:21:19,920
my training and then when I'm doing

00:21:17,820 --> 00:21:21,810
deployment I can scale down or scale up

00:21:19,920 --> 00:21:24,420
you know depending on what I want to do

00:21:21,810 --> 00:21:25,800
right and this kind of picture kind of

00:21:24,420 --> 00:21:27,390
shows you know how this is all set up in

00:21:25,800 --> 00:21:28,980
the cloud as well as on Prem for example

00:21:27,390 --> 00:21:30,240
you have a local environment

00:21:28,980 --> 00:21:31,260
where you can basically pretty much you

00:21:30,240 --> 00:21:33,030
know for example if your laptop doesn't

00:21:31,260 --> 00:21:34,500
have a GPU for example you know my

00:21:33,030 --> 00:21:36,830
laptop doesn't have a GPU this is no

00:21:34,500 --> 00:21:39,210
this is only as an integrated you know

00:21:36,830 --> 00:21:40,650
graphics card right so and I can

00:21:39,210 --> 00:21:42,530
basically have the thing where I can

00:21:40,650 --> 00:21:46,169
basically attach a GPU onto this laptop

00:21:42,530 --> 00:21:47,760
or even in a cluster environment I can

00:21:46,169 --> 00:21:50,250
basically just log into my CPU machines

00:21:47,760 --> 00:21:51,990
and do all the things so here it kind of

00:21:50,250 --> 00:21:53,610
shows hey you start with essentially a

00:21:51,990 --> 00:21:55,530
single node a laptop and then you

00:21:53,610 --> 00:21:57,600
quickly move into a cluster on your CPU

00:21:55,530 --> 00:21:59,700
node and then you're at an GPUs as much

00:21:57,600 --> 00:22:01,230
as possible and then you know you can

00:21:59,700 --> 00:22:03,929
essentially do even more batch

00:22:01,230 --> 00:22:05,460
processing and combine GPS across

00:22:03,929 --> 00:22:07,140
multiple machines to do a large-scale

00:22:05,460 --> 00:22:08,520
training and then from there you can

00:22:07,140 --> 00:22:12,960
basically you know pushing through

00:22:08,520 --> 00:22:15,559
production and we provide some you know

00:22:12,960 --> 00:22:18,179
easy ways for people to managers as well

00:22:15,559 --> 00:22:20,280
again the value is essentially you know

00:22:18,179 --> 00:22:21,690
it basically reduces the time to market

00:22:20,280 --> 00:22:23,820
for people right and you know

00:22:21,690 --> 00:22:25,410
essentially today in this world you know

00:22:23,820 --> 00:22:27,570
who wants to be sitting and doing DevOps

00:22:25,410 --> 00:22:29,220
and infrastructure management if it's

00:22:27,570 --> 00:22:31,140
not if it's not my business I don't want

00:22:29,220 --> 00:22:33,120
to be doing it so essentially it reduces

00:22:31,140 --> 00:22:35,460
overall lifecycle all the way from setup

00:22:33,120 --> 00:22:37,590
time to deployment with that I'll kind

00:22:35,460 --> 00:22:39,240
of an on switch to a quick demo you know

00:22:37,590 --> 00:22:42,350
kind of showing you know all these

00:22:39,240 --> 00:22:42,350
pieces so I'm going to

00:22:49,490 --> 00:22:59,240
how much time we have yeah okay

00:23:28,800 --> 00:23:34,860
so this you know I have a couple of day

00:23:31,680 --> 00:23:36,120
a few demo environment that I have you

00:23:34,860 --> 00:23:37,860
know sometimes whenever I actually show

00:23:36,120 --> 00:23:39,540
our demo you know sometimes like she's

00:23:37,860 --> 00:23:41,310
an environment doesn't works I have a

00:23:39,540 --> 00:23:42,840
few backups so this is an environment if

00:23:41,310 --> 00:23:44,280
it basically kind of still works it was

00:23:42,840 --> 00:23:48,360
working just before I started the demo

00:23:44,280 --> 00:23:50,010
this got about eight GPU servers and one

00:23:48,360 --> 00:23:51,660
CPU server okay

00:23:50,010 --> 00:23:53,820
the CP machine doesn't have any GPUs

00:23:51,660 --> 00:23:57,930
it's got eight GPU servers and a GPU

00:23:53,820 --> 00:24:00,420
server has what heat p100 it's about you

00:23:57,930 --> 00:24:01,920
know it's a pretty beefy cluster and

00:24:00,420 --> 00:24:05,330
I'll kind of show you like a few things

00:24:01,920 --> 00:24:05,330
you know in the cluster I can do

00:24:16,340 --> 00:24:23,429
Anglet values now alright so i basically

00:24:21,750 --> 00:24:26,130
not basically we've got about a no age

00:24:23,429 --> 00:24:28,250
you know eight you know i think i think

00:24:26,130 --> 00:24:32,580
i think somebody was robbed with just

00:24:28,250 --> 00:24:36,510
six sexually highlight and then it's got

00:24:32,580 --> 00:24:37,799
a CP machine here right so i'm gonna

00:24:36,510 --> 00:24:39,510
spin up a workplace or i already have

00:24:37,799 --> 00:24:41,159
you know which is a jupiter workspace

00:24:39,510 --> 00:24:42,960
essentially the whole concept of this

00:24:41,159 --> 00:24:44,820
software is just to before actually sure

00:24:42,960 --> 00:24:47,039
here it's essentially brings in like

00:24:44,820 --> 00:24:48,929
like a simple cloud environment you know

00:24:47,039 --> 00:24:51,059
within a simple machine learning

00:24:48,929 --> 00:24:52,559
environment that people can just use on

00:24:51,059 --> 00:24:54,899
top of any infrastructure on top of

00:24:52,559 --> 00:24:57,120
Amazon on top of you know your own on

00:24:54,899 --> 00:24:58,380
Prem or whatever you know you have and

00:24:57,120 --> 00:25:01,230
by the way how many people here actually

00:24:58,380 --> 00:25:03,059
have on Prem you know no clusters so you

00:25:01,230 --> 00:25:05,580
know like their own data center okay

00:25:03,059 --> 00:25:07,110
cool so I can basically create multiple

00:25:05,580 --> 00:25:09,000
users and can basically like you know

00:25:07,110 --> 00:25:11,399
for example I can set limits the user as

00:25:09,000 --> 00:25:14,220
to how many GPUs they have allowed to

00:25:11,399 --> 00:25:16,020
use at the max and things like that

00:25:14,220 --> 00:25:17,909
right so and they have more time I'll

00:25:16,020 --> 00:25:19,470
kinda show you all the features here so

00:25:17,909 --> 00:25:21,419
but for now you know this is basically a

00:25:19,470 --> 00:25:23,220
running on a CPU machine this this

00:25:21,419 --> 00:25:25,169
workplace running on a CP machine and it

00:25:23,220 --> 00:25:26,669
has the ability to attach a remote GP at

00:25:25,169 --> 00:25:28,440
any time when you want to basically know

00:25:26,669 --> 00:25:30,270
when you are using a GPU call it'll

00:25:28,440 --> 00:25:31,230
basically use the remote GPU so I'm

00:25:30,270 --> 00:25:38,299
going to just you know spin up this

00:25:31,230 --> 00:25:41,130
Jupiter workspace that is there and

00:25:38,299 --> 00:25:42,990
non-flex is essentially think of it as

00:25:41,130 --> 00:25:44,820
like a development platform for people

00:25:42,990 --> 00:25:46,409
doing you know deep learning development

00:25:44,820 --> 00:25:48,210
it comes with you know pre-installed

00:25:46,409 --> 00:25:49,529
containers with all the frameworks but

00:25:48,210 --> 00:25:52,380
you can also bring your own frame where

00:25:49,529 --> 00:25:53,909
own containers if you want to you can

00:25:52,380 --> 00:25:55,289
save restore all those things you can do

00:25:53,909 --> 00:25:57,330
here so I'm going to just open the

00:25:55,289 --> 00:25:58,919
terminal here this machine but it

00:25:57,330 --> 00:26:03,539
doesn't have any GPUs locally attached

00:25:58,919 --> 00:26:06,240
okay so I'm going to basically do envy

00:26:03,539 --> 00:26:08,010
DSM I and I just basically like no it

00:26:06,240 --> 00:26:10,080
basically shows half a GPU a car it

00:26:08,010 --> 00:26:11,580
shows one keep you here okay and I'm

00:26:10,080 --> 00:26:15,360
gonna do actually one more thing where

00:26:11,580 --> 00:26:17,899
I'm gonna create a new workspace and I'm

00:26:15,360 --> 00:26:21,480
gonna basically say you know demo -

00:26:17,899 --> 00:26:23,250
alright I can basically I'm gonna pick

00:26:21,480 --> 00:26:25,320
up pre-install environment that I

00:26:23,250 --> 00:26:26,789
already have and I can say okay where I

00:26:25,320 --> 00:26:28,470
want to basically have the workspace

00:26:26,789 --> 00:26:28,980
hosted on it can be on a GPU machine

00:26:28,470 --> 00:26:30,179
where

00:26:28,980 --> 00:26:32,549
let's say I could basically say well I

00:26:30,179 --> 00:26:34,740
could start this workspace on a an AGP

00:26:32,549 --> 00:26:37,139
machine and I want to basically expand

00:26:34,740 --> 00:26:39,659
up to 16 GPUs beyond a node or I could

00:26:37,139 --> 00:26:41,549
say I could start on a CPU machine and

00:26:39,659 --> 00:26:44,630
you know and I say no what I want

00:26:41,549 --> 00:26:44,630
basically has a GPU

00:26:52,930 --> 00:26:57,900
okay so I'm great and just in a bit this

00:26:55,450 --> 00:27:00,040
environment should actually no show up I

00:26:57,900 --> 00:27:01,690
think it should be a half a GPO

00:27:00,040 --> 00:27:03,580
workspace it's not I'll create one more

00:27:01,690 --> 00:27:11,110
so well that's actually our spinning

00:27:03,580 --> 00:27:13,720
I'll create another one I'm going to

00:27:11,110 --> 00:27:17,970
basically say okay you know what some

00:27:13,720 --> 00:27:17,970
some random number of GPUs more than H

00:27:23,300 --> 00:27:27,020
so you look at CEO right it basically

00:27:25,130 --> 00:27:30,620
just attached half a GPU on to this

00:27:27,020 --> 00:27:32,900
workspace right similarly you know here

00:27:30,620 --> 00:27:35,600
it basically attached 14 GPUs on to this

00:27:32,900 --> 00:27:37,280
workspace right so and I can basically

00:27:35,600 --> 00:27:38,870
go into that other workspace that I have

00:27:37,280 --> 00:27:40,309
and I can basically not essentially

00:27:38,870 --> 00:27:42,590
thing already have it here

00:27:40,309 --> 00:27:43,970
alright and I can essentially run like

00:27:42,590 --> 00:27:46,070
you know pretty much any Jupiter code

00:27:43,970 --> 00:27:49,190
that I have actually and it's gonna

00:27:46,070 --> 00:27:50,929
basically used you know the GPUs right

00:27:49,190 --> 00:27:53,150
so essentially I can run any any can

00:27:50,929 --> 00:27:54,710
ever know ipython notebook or whatnot so

00:27:53,150 --> 00:27:56,860
that's kind of you know that's kind of

00:27:54,710 --> 00:27:59,540
what do you think the whole idea here is

00:27:56,860 --> 00:28:01,520
being able to elastically attach without

00:27:59,540 --> 00:28:04,040
changing anything you know and using it

00:28:01,520 --> 00:28:05,360
only during you know during runtime

00:28:04,040 --> 00:28:06,470
I think a few other things that I could

00:28:05,360 --> 00:28:09,140
have showed you where I can basically

00:28:06,470 --> 00:28:11,900
share my volumes you know I do some node

00:28:09,140 --> 00:28:13,280
management had some users where I'm

00:28:11,900 --> 00:28:15,580
going to just create a user let's say

00:28:13,280 --> 00:28:15,580
you know

00:28:25,470 --> 00:28:33,559
similar said user has limit up to grade

00:28:28,590 --> 00:28:33,559
up to one GPU right so

00:28:36,250 --> 00:28:44,550
you know when I log in as a user

00:28:58,330 --> 00:29:03,190
if my internet works you know I log in

00:29:00,730 --> 00:29:05,490
as a user I should be able to see this

00:29:03,190 --> 00:29:05,490
one

00:29:11,130 --> 00:29:14,789
but it kept shows like you know like a

00:29:13,200 --> 00:29:16,919
view of you know what DPS are used for

00:29:14,789 --> 00:29:18,809
Jeep is an art music and I'm doing it

00:29:16,919 --> 00:29:20,730
here I have a limit of only up to 1 GP

00:29:18,809 --> 00:29:25,669
that I can trade right once I create

00:29:20,730 --> 00:29:25,669
let's say I have GPU you know

00:29:29,560 --> 00:29:33,850
and

00:29:31,690 --> 00:29:35,230
and then now when I actually do it it'll

00:29:33,850 --> 00:29:38,980
only allow me to actually do like you

00:29:35,230 --> 00:29:41,290
know the remaining half so that's pretty

00:29:38,980 --> 00:29:42,790
much you know what I had you know I can

00:29:41,290 --> 00:29:45,070
assure though the the you know the

00:29:42,790 --> 00:29:46,810
summary is essentially being able to do

00:29:45,070 --> 00:29:48,670
deep learning development on CPU

00:29:46,810 --> 00:29:50,260
machines and elastically attached GPUs

00:29:48,670 --> 00:29:53,650
just like you do network attached

00:29:50,260 --> 00:30:02,410
storage and what we can have did we

00:29:53,650 --> 00:30:04,180
integrate with Jupiter here so we still

00:30:02,410 --> 00:30:07,710
have about five minutes left if anybody

00:30:04,180 --> 00:30:07,710
had questions for Suba

00:30:17,889 --> 00:30:22,729
absolutely absolutely absolutely in fact

00:30:20,629 --> 00:30:24,949
you know installation is pretty simple I

00:30:22,729 --> 00:30:26,629
know you basically just seems to do that

00:30:24,949 --> 00:30:27,079
in our big vision client on a on any

00:30:26,629 --> 00:30:29,479
nodes

00:30:27,079 --> 00:30:31,219
you know CPU machine on also on the GP

00:30:29,479 --> 00:30:32,989
machines and once you have it you know

00:30:31,219 --> 00:30:34,339
you can pretty much know yeah we could

00:30:32,989 --> 00:30:35,599
mean you could mean you could basically

00:30:34,339 --> 00:30:37,279
run it with Jupiter essentially all you

00:30:35,599 --> 00:30:44,749
do is just invoke Jupiter with the big

00:30:37,279 --> 00:30:47,649
vision time that's it yes yeah you can

00:30:44,749 --> 00:30:47,649
use multiple GPUs yes

00:30:59,240 --> 00:31:03,180
it doesn't matter so for example you

00:31:01,800 --> 00:31:04,770
could have a scenario where I could

00:31:03,180 --> 00:31:07,080
basically have let's say you see my

00:31:04,770 --> 00:31:09,150
client running on my laptop okay and I

00:31:07,080 --> 00:31:11,310
could basically have some GPUs on Amazon

00:31:09,150 --> 00:31:13,380
and remotely attach the GPUs from Amazon

00:31:11,310 --> 00:31:15,630
onto my laptop cane the internet is

00:31:13,380 --> 00:31:17,850
going to be pretty you know slow because

00:31:15,630 --> 00:31:25,320
of one gig but there's nothing stopping

00:31:17,850 --> 00:31:27,800
anybody from doing it yeah so we provide

00:31:25,320 --> 00:31:29,910
a CFN to actually do that on Amazon

00:31:27,800 --> 00:31:41,880
cloud formation deployed to be able to

00:31:29,910 --> 00:31:43,320
do that can access a Judas yeah in fact

00:31:41,880 --> 00:31:44,520
you know the in fact the machines you

00:31:43,320 --> 00:31:45,510
don't need to provide any special

00:31:44,520 --> 00:31:48,120
provisions but actually you know

00:31:45,510 --> 00:31:50,100
accessing the data or anything because

00:31:48,120 --> 00:31:52,680
you know the tunnel happens through the

00:31:50,100 --> 00:31:54,270
bit fusion client right so it's just

00:31:52,680 --> 00:31:56,490
like you know think of it as like it

00:31:54,270 --> 00:31:57,000
basically talks to TCP and it has its

00:31:56,490 --> 00:31:58,890
own port

00:31:57,000 --> 00:32:00,960
you just need expose a port or expose a

00:31:58,890 --> 00:32:02,670
few ports so once you have that you know

00:32:00,960 --> 00:32:04,620
you don't need to do any special things

00:32:02,670 --> 00:32:06,420
to do it you know generally we recommend

00:32:04,620 --> 00:32:12,950
putting in the same VPC if you're using

00:32:06,420 --> 00:32:15,420
Amazon as many machines as you want yep

00:32:12,950 --> 00:32:17,670
from third-party services are if you are

00:32:15,420 --> 00:32:19,080
in your own data center you could are in

00:32:17,670 --> 00:32:20,670
your work web sessions they are four or

00:32:19,080 --> 00:32:26,680
five people you could basically share

00:32:20,670 --> 00:32:30,490
resources yeah

00:32:26,680 --> 00:32:31,540
yeah so yes we we we support Alexis but

00:32:30,490 --> 00:32:32,950
there's nothing actually you know

00:32:31,540 --> 00:32:35,070
stopping from using in within Windows as

00:32:32,950 --> 00:32:35,070
well

00:32:43,760 --> 00:32:47,060
yes you know that is one way of actually

00:32:45,920 --> 00:32:49,040
not doing the implementation the other

00:32:47,060 --> 00:32:50,270
way is we've actually now seen as you

00:32:49,040 --> 00:32:52,280
only spin up the machine when you need

00:32:50,270 --> 00:32:54,260
them so you know there's some plugins

00:32:52,280 --> 00:32:55,940
that yeah we actually actually provide

00:32:54,260 --> 00:32:58,070
some plugins and you know things that

00:32:55,940 --> 00:33:00,530
people integrate in their own you know

00:32:58,070 --> 00:33:01,610
provisioning flow so basically you know

00:33:00,530 --> 00:33:02,000
kind of like what I showed you here

00:33:01,610 --> 00:33:04,400
right

00:33:02,000 --> 00:33:06,200
only when I'm basically know like only

00:33:04,400 --> 00:33:09,220
when I'm really using it you go and spin

00:33:06,200 --> 00:33:09,220
up the Machine and get it back to me

00:33:11,740 --> 00:33:20,330
yeah it could be it's essentially any

00:33:17,930 --> 00:33:23,450
API relay so the way we began or

00:33:20,330 --> 00:33:24,740
architected it is it tomorrow let's say

00:33:23,450 --> 00:33:27,200
if you have functions let's say lambdas

00:33:24,740 --> 00:33:36,470
okay and I want execute lambdas remotely

00:33:27,200 --> 00:33:39,160
I could use it any functions any other

00:33:36,470 --> 00:33:39,160
questions yeah

00:33:42,900 --> 00:33:48,090
specifies how many resources that they

00:33:45,809 --> 00:33:49,350
want yeah I'm only this far situation

00:33:48,090 --> 00:33:51,950
all they've done in the complication was

00:33:49,350 --> 00:33:51,950
just on vacation

00:33:53,710 --> 00:33:58,039
so so that is one way of actually doing

00:33:56,450 --> 00:33:59,960
it the other way is kind of like that

00:33:58,039 --> 00:34:01,610
gentleman asked where for example it say

00:33:59,960 --> 00:34:03,710
I could basically know set up let's say

00:34:01,610 --> 00:34:04,820
say you know what I'm have I have the

00:34:03,710 --> 00:34:07,070
client actually install on all these

00:34:04,820 --> 00:34:09,230
machines and only when I need something

00:34:07,070 --> 00:34:10,820
you basically spin up a machine and you

00:34:09,230 --> 00:34:12,859
know as part of the spin a process

00:34:10,820 --> 00:34:15,050
install the bit fusion you know software

00:34:12,859 --> 00:34:16,280
that server there and you know it just

00:34:15,050 --> 00:34:17,810
starts listening to it and it

00:34:16,280 --> 00:34:19,070
automatically get attached and once

00:34:17,810 --> 00:34:20,570
you're done with it just bring down the

00:34:19,070 --> 00:34:22,280
machine right pretty much using you know

00:34:20,570 --> 00:34:23,480
either ops works or any of the

00:34:22,280 --> 00:34:24,770
provisioning tools that you already have

00:34:23,480 --> 00:34:26,240
now we have seen integrations with

00:34:24,770 --> 00:34:29,240
kubernetes you've done integration in

00:34:26,240 --> 00:34:30,800
spar and a docker and a swarm we've done

00:34:29,240 --> 00:34:31,639
into an assault you know that people

00:34:30,800 --> 00:34:33,619
have actually done it we've done

00:34:31,639 --> 00:34:38,540
interview with CF n that people have and

00:34:33,619 --> 00:34:40,949
sometimes you can shell scripts let's

00:34:38,540 --> 00:34:44,369
thank our speaker

00:34:40,949 --> 00:34:44,369

YouTube URL: https://www.youtube.com/watch?v=3xrTvXE2B34


