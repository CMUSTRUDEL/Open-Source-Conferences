Title: Rachel Thomas interviewed at O'Reilly JupyterCon
Publication date: 2017-08-25
Playlist: JupyterCon
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:04,350
hi I'm Park Unnithan from O'Reilly Media

00:00:01,979 --> 00:00:05,819
and we're here at Jupiter cotton and I'm

00:00:04,350 --> 00:00:08,639
grateful to get to talk with Rachel

00:00:05,819 --> 00:00:11,940
Thomas from fast AI founding researcher

00:00:08,639 --> 00:00:14,700
at Ostia yes and excellent keynote this

00:00:11,940 --> 00:00:16,680
morning thank you it really really

00:00:14,700 --> 00:00:19,740
struck me just that the breadth of

00:00:16,680 --> 00:00:21,539
different applications even I mean like

00:00:19,740 --> 00:00:23,670
looking at the range of what your

00:00:21,539 --> 00:00:24,660
students have been doing of course we

00:00:23,670 --> 00:00:27,599
can talk about the industry in general

00:00:24,660 --> 00:00:29,070
but I wanted to try to talk more about

00:00:27,599 --> 00:00:32,910
that like what what are you seeing in

00:00:29,070 --> 00:00:35,280
terms of range for deep learning sure so

00:00:32,910 --> 00:00:37,200
yeah covered in my keynote this range

00:00:35,280 --> 00:00:39,629
from you know people working on the new

00:00:37,200 --> 00:00:41,340
language in Pakistan to student in India

00:00:39,629 --> 00:00:44,040
who's helping create better crop

00:00:41,340 --> 00:00:45,239
insurance for farmers there but that was

00:00:44,040 --> 00:00:47,100
I mean this was kind of our goal with

00:00:45,239 --> 00:00:48,239
fast day I'd it was to get deep learning

00:00:47,100 --> 00:00:51,210
into the hands the kind of as many

00:00:48,239 --> 00:00:52,289
people as possible and with the idea

00:00:51,210 --> 00:00:53,850
that you know people in different places

00:00:52,289 --> 00:00:55,260
from different backgrounds and with

00:00:53,850 --> 00:00:56,670
different domain expertise they're gonna

00:00:55,260 --> 00:00:58,340
have problems that they're knowledgeable

00:00:56,670 --> 00:01:00,840
and passionate about and want to solve

00:00:58,340 --> 00:01:01,199
what kind of prerequisites come into the

00:01:00,840 --> 00:01:03,510
program

00:01:01,199 --> 00:01:05,280
yeah so the only prerequisite for the

00:01:03,510 --> 00:01:07,650
course is one year of coding experience

00:01:05,280 --> 00:01:11,460
right now so I know advanced math

00:01:07,650 --> 00:01:13,560
prerequisites interesting so so you can

00:01:11,460 --> 00:01:18,720
come in with maybe like a year of

00:01:13,560 --> 00:01:20,280
working in Python or any particular so

00:01:18,720 --> 00:01:22,799
we have this teaching philosophy with

00:01:20,280 --> 00:01:24,840
that kind of ideas it's inspired by

00:01:22,799 --> 00:01:26,909
David Perkins a professor at Harvard of

00:01:24,840 --> 00:01:29,130
them because it the whole game that with

00:01:26,909 --> 00:01:31,079
kids like you don't require a child to

00:01:29,130 --> 00:01:32,670
remember all all the formal rules of

00:01:31,079 --> 00:01:34,439
baseball before they're allowed to play

00:01:32,670 --> 00:01:35,850
they can just get out there and have fun

00:01:34,439 --> 00:01:38,070
and have a general sense of what

00:01:35,850 --> 00:01:39,720
baseball is like and they might not be

00:01:38,070 --> 00:01:41,250
playing a full nine innings or have a

00:01:39,720 --> 00:01:43,259
full team and as they get older they

00:01:41,250 --> 00:01:44,579
learn more formal rules and so we wanted

00:01:43,259 --> 00:01:46,619
to do that with deep learning I've kind

00:01:44,579 --> 00:01:48,840
of get people right away using it

00:01:46,619 --> 00:01:51,000
getting a sense of what it can do and

00:01:48,840 --> 00:01:52,799
then over time digging more into the

00:01:51,000 --> 00:01:53,909
details of how it works and our Courtin

00:01:52,799 --> 00:01:55,829
in our course is always kind of

00:01:53,909 --> 00:01:57,990
motivated by wanting to improve the

00:01:55,829 --> 00:02:01,320
models performance or tackle a more

00:01:57,990 --> 00:02:02,490
challenging problem one of the things at

00:02:01,320 --> 00:02:04,280
or Riley that I've heard from the

00:02:02,490 --> 00:02:06,450
editors like going way back was just

00:02:04,280 --> 00:02:09,090
page one paragraph one in the first

00:02:06,450 --> 00:02:11,400
chapter jump into some code yeah get

00:02:09,090 --> 00:02:12,740
people comfortable and confident it

00:02:11,400 --> 00:02:14,480
sounds like you're taking

00:02:12,740 --> 00:02:16,220
you have a very similar kind of approach

00:02:14,480 --> 00:02:18,800
there yes yeah so kind of get people

00:02:16,220 --> 00:02:23,270
coding right away with working models

00:02:18,800 --> 00:02:26,390
yeah yeah so interesting so what kind of

00:02:23,270 --> 00:02:27,680
conceptual hurdles do people have when

00:02:26,390 --> 00:02:32,410
they're first starting to work out with

00:02:27,680 --> 00:02:34,730
deep learning um that's a good question

00:02:32,410 --> 00:02:35,960
yes I mean they're definitely several

00:02:34,730 --> 00:02:38,150
things I think are are conceptually

00:02:35,960 --> 00:02:39,680
weird we also we do things and this is

00:02:38,150 --> 00:02:41,510
kind of Jeremy's inspiration like well

00:02:39,680 --> 00:02:43,310
we've implemented like we'll use Excel

00:02:41,510 --> 00:02:45,410
to illustrate things cuz excessively

00:02:43,310 --> 00:02:47,660
this is a really neat visual tool yeah

00:02:45,410 --> 00:02:49,100
and so we have likes stochastic gradient

00:02:47,660 --> 00:02:51,290
descent and also all these like fancy

00:02:49,100 --> 00:02:52,790
optimizers implemented in Excel because

00:02:51,290 --> 00:02:54,320
you can you know they just really see

00:02:52,790 --> 00:02:56,000
like okay these are the cells that are

00:02:54,320 --> 00:02:58,130
like updating and we actually have a

00:02:56,000 --> 00:03:00,830
convolution implemented of like this is

00:02:58,130 --> 00:03:02,450
how like what a convolution is doing so

00:03:00,830 --> 00:03:04,940
that's a tool we'll kind of use to try

00:03:02,450 --> 00:03:06,350
to yeah somebody's like stumped on a

00:03:04,940 --> 00:03:07,850
concept of kind of like okay like let's

00:03:06,350 --> 00:03:10,310
think about what it just like looks like

00:03:07,850 --> 00:03:12,380
in terms of like cells updating and then

00:03:10,310 --> 00:03:14,000
we'll let go back to the Python well

00:03:12,380 --> 00:03:15,470
that's excellent because some of these

00:03:14,000 --> 00:03:17,390
algorithms you try to read the

00:03:15,470 --> 00:03:21,740
pseudocode in the paper and that's maybe

00:03:17,390 --> 00:03:23,660
like yeah yeah yeah implementing

00:03:21,740 --> 00:03:25,640
algorithms from papers can be painful

00:03:23,660 --> 00:03:26,920
and we do some of that in part two of

00:03:25,640 --> 00:03:28,460
the course yeah

00:03:26,920 --> 00:03:30,380
excellent so people who are getting

00:03:28,460 --> 00:03:32,450
hands-on then is as far as

00:03:30,380 --> 00:03:34,070
implementation yes yeah yeah and like

00:03:32,450 --> 00:03:35,750
part two the goal is them it's about

00:03:34,070 --> 00:03:37,970
being able to read research papers

00:03:35,750 --> 00:03:38,720
implement them oh that would have been

00:03:37,970 --> 00:03:42,200
so helpful

00:03:38,720 --> 00:03:43,940
yeah I gotta probably know yeah we are

00:03:42,200 --> 00:03:45,380
advising people to unless unless they're

00:03:43,940 --> 00:03:48,410
very interested in that part to just

00:03:45,380 --> 00:03:50,840
skim over the math because it's a kind

00:03:48,410 --> 00:03:52,730
of a set of goals for you know a lot of

00:03:50,840 --> 00:03:54,470
deep learning researchers are coming on

00:03:52,730 --> 00:03:55,820
in this very academic community and you

00:03:54,470 --> 00:03:57,290
know are focused on this publishing

00:03:55,820 --> 00:03:59,090
papers and kind of what the incentives

00:03:57,290 --> 00:04:00,290
are around like what's interesting to

00:03:59,090 --> 00:04:02,720
publish and what's going to get you into

00:04:00,290 --> 00:04:04,100
a top journal which is very different

00:04:02,720 --> 00:04:06,470
from the concerns of a practitioner

00:04:04,100 --> 00:04:08,810
that's like how do I solve this you know

00:04:06,470 --> 00:04:12,230
tangible problem for my particular data

00:04:08,810 --> 00:04:13,910
set what one thing that I mean when I

00:04:12,230 --> 00:04:16,370
first started hearing about deep

00:04:13,910 --> 00:04:18,530
learning I was thinking that okay this

00:04:16,370 --> 00:04:20,500
little hop out in some areas of say

00:04:18,530 --> 00:04:22,910
smart phones making smart phone smarter

00:04:20,500 --> 00:04:24,470
but it didn't really struck me like

00:04:22,910 --> 00:04:27,890
where the

00:04:24,470 --> 00:04:30,800
would be just in in terms of everyday

00:04:27,890 --> 00:04:32,660
needs yeah I think that talking with

00:04:30,800 --> 00:04:35,390
Jeremy at analytic was probably one of

00:04:32,660 --> 00:04:37,910
the first eye openers for me just how

00:04:35,390 --> 00:04:39,770
much this can be applied in medicine a

00:04:37,910 --> 00:04:42,080
lot of areas of healthcare that have

00:04:39,770 --> 00:04:43,940
been very difficult before you were

00:04:42,080 --> 00:04:45,290
talking yes yeah yes I actually just

00:04:43,940 --> 00:04:46,460
wrote a blog post this week about this

00:04:45,290 --> 00:04:47,660
but yeah listening kind of all these

00:04:46,460 --> 00:04:49,280
different areas of Medicine or deep

00:04:47,660 --> 00:04:50,840
learnings being applied and this is

00:04:49,280 --> 00:04:54,890
everything from dermatology to

00:04:50,840 --> 00:04:58,250
ophthalmology and cardiac health and

00:04:54,890 --> 00:05:00,290
pediatric ICU mortality lots of areas

00:04:58,250 --> 00:05:01,160
and I think there's and I still feel

00:05:00,290 --> 00:05:03,530
like we're just at the tip of iceberg

00:05:01,160 --> 00:05:05,660
because a lot you know a lot is still

00:05:03,530 --> 00:05:08,570
using image data but there's so much

00:05:05,660 --> 00:05:11,990
with medical records and other data

00:05:08,570 --> 00:05:14,150
sources to be incorporated so are are we

00:05:11,990 --> 00:05:15,830
looking at deep learning as a kind of

00:05:14,150 --> 00:05:17,570
processing of image data or we're

00:05:15,830 --> 00:05:19,070
looking at deep learning as a way to

00:05:17,570 --> 00:05:21,560
integrate a lot of different data

00:05:19,070 --> 00:05:22,840
sources a way to integrate a lot of

00:05:21,560 --> 00:05:25,520
different data sources yeah and I think

00:05:22,840 --> 00:05:27,500
right now there are use cases that are

00:05:25,520 --> 00:05:29,660
just kind of processing the data but you

00:05:27,500 --> 00:05:32,120
can enter data of you know different

00:05:29,660 --> 00:05:34,220
forms and have and we cover this in part

00:05:32,120 --> 00:05:36,230
two of the course or you can put in an

00:05:34,220 --> 00:05:38,419
image and then also put in you know some

00:05:36,230 --> 00:05:40,340
metadata about that image and have these

00:05:38,419 --> 00:05:43,760
different types of inputs all going into

00:05:40,340 --> 00:05:47,780
a neural network to help you make a

00:05:43,760 --> 00:05:48,400
prediction excellent changing gears a

00:05:47,780 --> 00:05:51,730
little bit

00:05:48,400 --> 00:05:54,110
you had an article months ago about

00:05:51,730 --> 00:05:56,120
diversity in tech but also diversity in

00:05:54,110 --> 00:05:58,250
AI mm-hmm and I want to talk more about

00:05:56,120 --> 00:06:00,500
that there was an excellent article on

00:05:58,250 --> 00:06:02,690
medium I remember you had recently about

00:06:00,500 --> 00:06:05,270
the diversity yeah yes I mean I think

00:06:02,690 --> 00:06:06,350
that so I mean text diversity problems

00:06:05,270 --> 00:06:08,480
are getting a lot of attention in

00:06:06,350 --> 00:06:10,820
general and I think that AI is even less

00:06:08,480 --> 00:06:13,460
diverse than much of the rest of tech

00:06:10,820 --> 00:06:15,590
and this is particularly scary to me

00:06:13,460 --> 00:06:18,410
because we're building such high-impact

00:06:15,590 --> 00:06:19,669
technologies right now and I think that

00:06:18,410 --> 00:06:21,800
you know we're already seeing examples

00:06:19,669 --> 00:06:22,580
of you know unintentional bias in our

00:06:21,800 --> 00:06:24,980
algorithm

00:06:22,580 --> 00:06:26,210
and some you know high-profile examples

00:06:24,980 --> 00:06:27,920
have been you know Google photos

00:06:26,210 --> 00:06:32,480
labeling black people as gorillas in

00:06:27,920 --> 00:06:33,680
2015 or Google Translate still well go

00:06:32,480 --> 00:06:35,690
from languages to have a gender-neutral

00:06:33,680 --> 00:06:37,640
singular pronoun it'll translate a

00:06:35,690 --> 00:06:39,710
sentence into he is a doctor she is a

00:06:37,640 --> 00:06:42,800
nurse even when the gender wasn't

00:06:39,710 --> 00:06:44,540
specified and so I think there's just

00:06:42,800 --> 00:06:46,610
kind of the the potential for more and

00:06:44,540 --> 00:06:49,520
more of that is an AI becomes more

00:06:46,610 --> 00:06:51,230
pervasive and one one thing that needs

00:06:49,520 --> 00:06:54,260
to be done is we need more diverse teams

00:06:51,230 --> 00:06:56,510
building the technology I there was one

00:06:54,260 --> 00:06:58,940
example you're referencing I think it

00:06:56,510 --> 00:07:01,220
was Pro Publica but about you know sins

00:06:58,940 --> 00:07:04,250
and guidelines in it yes yeah and that's

00:07:01,220 --> 00:07:05,870
yes that's really sad and so this is Pro

00:07:04,250 --> 00:07:08,060
Publica I did an investigation on

00:07:05,870 --> 00:07:10,190
recidivism this recidivism algorithm

00:07:08,060 --> 00:07:12,830
that was used I don't know if it still

00:07:10,190 --> 00:07:15,650
is used in the u.s. courtrooms and it

00:07:12,830 --> 00:07:17,480
was found to be biased against black

00:07:15,650 --> 00:07:20,480
people and biased in favor of white

00:07:17,480 --> 00:07:21,920
people yeah and this was used in

00:07:20,480 --> 00:07:24,980
determining people's like whether they

00:07:21,920 --> 00:07:26,630
got parole really concerning Cathy

00:07:24,980 --> 00:07:28,340
O'Neil has an excellent book weapons of

00:07:26,630 --> 00:07:29,480
mass destruction and covers a lot of

00:07:28,340 --> 00:07:31,580
these and you know these aren't

00:07:29,480 --> 00:07:33,050
necessarily machine learning I think in

00:07:31,580 --> 00:07:35,030
some cases because they're even simpler

00:07:33,050 --> 00:07:37,760
algorithms but that algorithms are being

00:07:35,030 --> 00:07:40,610
used more and more for hiring decisions

00:07:37,760 --> 00:07:43,730
firing decisions yeah these ones related

00:07:40,610 --> 00:07:47,840
to people's prison sentences and so a

00:07:43,730 --> 00:07:50,180
lot of area can of concern with bias

00:07:47,840 --> 00:07:53,120
being in those and then also with they

00:07:50,180 --> 00:07:54,500
often are not auditable and in the US

00:07:53,120 --> 00:07:56,090
are kind of getting this weird interface

00:07:54,500 --> 00:07:57,890
of you know private companies produce

00:07:56,090 --> 00:08:00,910
them and then they're not subject to the

00:07:57,890 --> 00:08:02,750
same like state public record laws that

00:08:00,910 --> 00:08:04,970
we usually have into our public

00:08:02,750 --> 00:08:07,550
institutions and so that's scary as well

00:08:04,970 --> 00:08:08,600
like that was frightening about it yeah

00:08:07,550 --> 00:08:12,080
like it's like so the other one that

00:08:08,600 --> 00:08:14,420
really scares me is taser acquired to AI

00:08:12,080 --> 00:08:15,680
companies earlier this year and they've

00:08:14,420 --> 00:08:17,960
been marketing that they're developing

00:08:15,680 --> 00:08:20,390
predictive policing software because

00:08:17,960 --> 00:08:24,320
they own an 80% of the police body cam

00:08:20,390 --> 00:08:26,200
market in the US and so there's there's

00:08:24,320 --> 00:08:29,090
so much that can go wrong there I think

00:08:26,200 --> 00:08:31,730
well what's your perspectives then in

00:08:29,090 --> 00:08:34,850
terms of the balance of this on the one

00:08:31,730 --> 00:08:35,419
hand we're seeing deep learning and

00:08:34,850 --> 00:08:38,690
other

00:08:35,419 --> 00:08:39,829
I being just just augmenting so much of

00:08:38,690 --> 00:08:41,419
what we can do yes

00:08:39,829 --> 00:08:43,700
on the other hand we're seeing federal

00:08:41,419 --> 00:08:45,200
judiciary which should be people who are

00:08:43,700 --> 00:08:46,850
really interested in looking at evidence

00:08:45,200 --> 00:08:49,370
and making it judge yeah basically

00:08:46,850 --> 00:08:50,810
pushing a button it so what what's your

00:08:49,370 --> 00:08:52,579
guidance for being able to strike that

00:08:50,810 --> 00:08:54,110
balance between the automation and the

00:08:52,579 --> 00:08:55,519
inside yeah I mean it's so it's really

00:08:54,110 --> 00:08:57,589
important I mean it's also it's

00:08:55,519 --> 00:08:59,269
important to keep both the like you know

00:08:57,589 --> 00:09:01,699
like all this potential upside of like

00:08:59,269 --> 00:09:03,320
we can save millions of lives and like

00:09:01,699 --> 00:09:05,180
keep that in mind at the same time as

00:09:03,320 --> 00:09:06,350
these potential risk of like oh my gosh

00:09:05,180 --> 00:09:11,510
you know people could be unjustly

00:09:06,350 --> 00:09:12,649
sentenced so I think that it's becoming

00:09:11,510 --> 00:09:14,510
more and more important that I think

00:09:12,649 --> 00:09:16,010
everyone needs to have some working

00:09:14,510 --> 00:09:18,170
knowledge of just like how do you

00:09:16,010 --> 00:09:19,850
evaluate algorithms and think about bias

00:09:18,170 --> 00:09:21,500
because this is something that I think

00:09:19,850 --> 00:09:23,540
lawyers and doctors and you know all

00:09:21,500 --> 00:09:26,750
different fields are gonna need to have

00:09:23,540 --> 00:09:31,010
some level of literacy around even if

00:09:26,750 --> 00:09:33,980
they're not coding themselves but yeah I

00:09:31,010 --> 00:09:35,690
think that the you know I think there's

00:09:33,980 --> 00:09:37,459
potential that algorithms could be less

00:09:35,690 --> 00:09:39,139
biased like we know that human judges

00:09:37,459 --> 00:09:40,970
make really biased decisions in our

00:09:39,139 --> 00:09:43,399
criminal justice system as well like and

00:09:40,970 --> 00:09:45,320
biased hiring and firing decisions and

00:09:43,399 --> 00:09:47,660
so I think there is some potential of

00:09:45,320 --> 00:09:50,180
like well an algorithm could you know be

00:09:47,660 --> 00:09:51,920
less biased but we really need to be

00:09:50,180 --> 00:09:53,870
able to like audit these systems and be

00:09:51,920 --> 00:09:55,190
aware also just yeah the potential risk

00:09:53,870 --> 00:09:57,230
so that we can be looking out for them

00:09:55,190 --> 00:09:59,570
yeah that's a really good analogy there

00:09:57,230 --> 00:10:02,720
in the judiciary we know that there's

00:09:59,570 --> 00:10:04,550
there's always gonna be bias there there

00:10:02,720 --> 00:10:06,949
are audits there's definitely a paper

00:10:04,550 --> 00:10:09,320
trail yeah yeah but there's also appeals

00:10:06,949 --> 00:10:11,000
right and over time yes yeah and I think

00:10:09,320 --> 00:10:12,949
those are important components for yeah

00:10:11,000 --> 00:10:14,420
for this and and these are also like

00:10:12,949 --> 00:10:16,040
important questions that we should be

00:10:14,420 --> 00:10:17,630
asking whenever we hear about these

00:10:16,040 --> 00:10:19,699
algorithms of yeah like what is the

00:10:17,630 --> 00:10:22,850
appeals process of like how are mistakes

00:10:19,699 --> 00:10:24,079
like found and corrected and also just

00:10:22,850 --> 00:10:25,670
really thinking about like you know what

00:10:24,079 --> 00:10:29,029
was the training set and is that

00:10:25,670 --> 00:10:31,639
representative of kind of the variety in

00:10:29,029 --> 00:10:33,139
the world I mean I I think maybe the tip

00:10:31,639 --> 00:10:36,260
of the iceberg would have been the

00:10:33,139 --> 00:10:38,300
scorecards used for credit ratings I

00:10:36,260 --> 00:10:39,740
mean this is going back yeah but it's

00:10:38,300 --> 00:10:41,420
sort of like there are machine learning

00:10:39,740 --> 00:10:44,149
algorithms being used to decide whether

00:10:41,420 --> 00:10:46,370
or not you could buy a new car and now

00:10:44,149 --> 00:10:46,750
it's it's really snowballing because

00:10:46,370 --> 00:10:49,330
there's

00:10:46,750 --> 00:10:51,340
more machinery yes yeah yeah did you

00:10:49,330 --> 00:10:54,520
read there this was and wired this week

00:10:51,340 --> 00:10:57,690
there is like an article how researchers

00:10:54,520 --> 00:10:59,680
found that so they're like training

00:10:57,690 --> 00:11:01,420
training at machine learning algorithm

00:10:59,680 --> 00:11:03,520
and they had pictures of you know like

00:11:01,420 --> 00:11:05,560
women were in kitchens more often and

00:11:03,520 --> 00:11:07,120
men were at computers more often but the

00:11:05,560 --> 00:11:09,580
machine learning algorithm didn't just

00:11:07,120 --> 00:11:11,950
learn that bias it amplified it like it

00:11:09,580 --> 00:11:13,540
was disproportionately like when it saw

00:11:11,950 --> 00:11:15,910
a man in the kitchen saying like oh that

00:11:13,540 --> 00:11:18,550
must be a woman because it's a person in

00:11:15,910 --> 00:11:22,110
the kitchen and like a rate that wasn't

00:11:18,550 --> 00:11:25,780
even there was like magnified from the

00:11:22,110 --> 00:11:28,270
imbalance and the training set which is

00:11:25,780 --> 00:11:29,770
that that's something actually a little

00:11:28,270 --> 00:11:32,470
bit of guidance um one of the things

00:11:29,770 --> 00:11:35,140
we're using deploying for in media is to

00:11:32,470 --> 00:11:36,130
try to disambiguate contexts and so this

00:11:35,140 --> 00:11:37,810
has been something that we kind of

00:11:36,130 --> 00:11:39,630
inherited from big data there's a lot of

00:11:37,810 --> 00:11:42,670
great ways to understand correlations

00:11:39,630 --> 00:11:45,100
and look for things that are generalized

00:11:42,670 --> 00:11:47,530
as yeah but when you have to try to

00:11:45,100 --> 00:11:51,100
separate things out I don't know that

00:11:47,530 --> 00:11:52,390
the tooling is really there yet yeah so

00:11:51,100 --> 00:11:53,890
I mean there are things you can do in

00:11:52,390 --> 00:11:56,290
terms of light you can always kind of

00:11:53,890 --> 00:11:58,030
veer like change your inputs like you

00:11:56,290 --> 00:12:00,490
know change a particular variable to try

00:11:58,030 --> 00:12:02,920
to see how does that have an impact on

00:12:00,490 --> 00:12:04,270
your output and so even because the deep

00:12:02,920 --> 00:12:06,820
learning is often described as a black

00:12:04,270 --> 00:12:09,460
box but it's like there are ways to you

00:12:06,820 --> 00:12:10,870
have to introspect what's going on but

00:12:09,460 --> 00:12:12,610
that's also like an area that should be

00:12:10,870 --> 00:12:14,200
focused on more of you know like looking

00:12:12,610 --> 00:12:16,120
at like how do you look at variable

00:12:14,200 --> 00:12:18,190
importance and kind of find these things

00:12:16,120 --> 00:12:20,020
you're showing heat maps to be able to

00:12:18,190 --> 00:12:22,600
find yes yeah what are the features that

00:12:20,020 --> 00:12:24,850
are being picked on yeah picked up for a

00:12:22,600 --> 00:12:26,560
particular decision sounds a bit like

00:12:24,850 --> 00:12:28,480
kind of a ridge regression approach like

00:12:26,560 --> 00:12:30,640
chain throw a little noise in the input

00:12:28,480 --> 00:12:34,900
and see what shakes out how these

00:12:30,640 --> 00:12:36,339
decisions yeah yeah or like with random

00:12:34,900 --> 00:12:38,410
forests you know you can do this kind of

00:12:36,339 --> 00:12:39,610
like variable importance where you just

00:12:38,410 --> 00:12:42,540
think you know really agreeably

00:12:39,610 --> 00:12:45,130
shuffling got like a particular variable

00:12:42,540 --> 00:12:48,790
um well also to change gears a little

00:12:45,130 --> 00:12:51,640
bit just general advice a big theme in

00:12:48,790 --> 00:12:53,560
Jupiter con here is about use of Jupiter

00:12:51,640 --> 00:12:56,470
in education and you

00:12:53,560 --> 00:12:58,360
we've had fantastic results out of your

00:12:56,470 --> 00:13:00,999
program what kind of advice would you

00:12:58,360 --> 00:13:02,889
have for for teachers for educators who

00:13:00,999 --> 00:13:03,970
are just probably even now trying to

00:13:02,889 --> 00:13:06,899
grapple with how are they going to

00:13:03,970 --> 00:13:10,300
present artificial intelligences yeah I

00:13:06,899 --> 00:13:12,399
definitely recommend Jupiter is just and

00:13:10,300 --> 00:13:15,129
so we've now created three courses

00:13:12,399 --> 00:13:17,139
entirely in Jupiter notebooks that also

00:13:15,129 --> 00:13:18,790
are available online but it's just

00:13:17,139 --> 00:13:21,610
really great to be able to have the text

00:13:18,790 --> 00:13:23,589
and the code and ours first and I'm even

00:13:21,610 --> 00:13:25,839
finding like I'm looking back on stuff I

00:13:23,589 --> 00:13:27,610
did kind of like longer ago a few years

00:13:25,839 --> 00:13:28,839
ago that like it makes it easier to kind

00:13:27,610 --> 00:13:30,579
of break things into these smaller

00:13:28,839 --> 00:13:32,379
pieces you know and it's still

00:13:30,579 --> 00:13:34,660
continuous but to really kind of have

00:13:32,379 --> 00:13:36,759
your text and diagrams interspersed with

00:13:34,660 --> 00:13:38,800
like kind of these like digestible

00:13:36,759 --> 00:13:40,660
blocks of code that people can

00:13:38,800 --> 00:13:43,540
understand it just it really takes like

00:13:40,660 --> 00:13:45,100
commenting to the next level yeah well

00:13:43,540 --> 00:13:47,620
whatever have an explanation and a

00:13:45,100 --> 00:13:49,120
picture alongside yeah sort of the the

00:13:47,620 --> 00:13:50,769
literate programming I think was

00:13:49,120 --> 00:13:53,439
probably an idea this is going back to

00:13:50,769 --> 00:13:56,829
like the 80s the idea of having code

00:13:53,439 --> 00:13:58,059
that conveyed to people oh yeah yeah the

00:13:56,829 --> 00:14:01,209
computation part was sort of a byproduct

00:13:58,059 --> 00:14:02,350
yeah yeah no and I'd totally yeah like I

00:14:01,209 --> 00:14:04,839
feel like that's kind of like the idea

00:14:02,350 --> 00:14:07,389
with Jupiter of like you're you're like

00:14:04,839 --> 00:14:08,589
creating a narrative for a person and

00:14:07,389 --> 00:14:11,019
then it's almost like there's there's

00:14:08,589 --> 00:14:13,029
kind of a cadence or a rhythm when

00:14:11,019 --> 00:14:16,360
you're chunking your code into blocks

00:14:13,029 --> 00:14:18,339
yeah every zhilie understood yeah or the

00:14:16,360 --> 00:14:20,139
text that you're adding to is do you

00:14:18,339 --> 00:14:21,250
feel that there's kind of like a cadence

00:14:20,139 --> 00:14:22,779
that you get into when you're writing

00:14:21,250 --> 00:14:24,309
this yes yeah and I can definitely like

00:14:22,779 --> 00:14:26,050
also tell when things are off of like

00:14:24,309 --> 00:14:28,089
okay I need to like refactor that or

00:14:26,050 --> 00:14:29,529
like split that into two functions or

00:14:28,089 --> 00:14:31,779
something if it's yeah like I think this

00:14:29,529 --> 00:14:34,839
is gonna be like harder to to explain or

00:14:31,779 --> 00:14:37,980
teach I'm ready thank you Rachel really

00:14:34,839 --> 00:14:37,980
appreciate thank you

00:14:44,360 --> 00:14:46,420

YouTube URL: https://www.youtube.com/watch?v=MCY5TsxEW9Q


