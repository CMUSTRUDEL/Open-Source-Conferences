Title: Humans in the Loop: Jupyter Notebooks as a Frontend for AI Pipelines at Scale
Publication date: 2017-11-08
Playlist: JupyterCon
Description: 
	Paco Nathan (O'Reilly Media) reviews use cases where Jupyter provides a frontend to AI as the means for keeping humans in the loop. This process enhances the feedback loop between people and machines, and the end result is that a smaller group of people can handle a wider range of responsibilities for building and maintaining a complex system of automation.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,620 --> 00:00:06,000
it's a pleasure to be here at Jupiter

00:00:03,540 --> 00:00:09,120
con just as a side note it's been really

00:00:06,000 --> 00:00:10,950
fun to see Jupiter Khan develop over the

00:00:09,120 --> 00:00:12,150
past what year and a half we started

00:00:10,950 --> 00:00:14,309
talking about this I guess about a year

00:00:12,150 --> 00:00:15,839
and a half ago and or maybe a little bit

00:00:14,309 --> 00:00:17,670
more but like some of the juber today

00:00:15,839 --> 00:00:19,590
events we were kind of envisioning

00:00:17,670 --> 00:00:21,360
having a conference and now seeing how

00:00:19,590 --> 00:00:23,750
it's turned out and the keynotes and

00:00:21,360 --> 00:00:27,150
everything it's it's just fantastic

00:00:23,750 --> 00:00:28,890
already so humans in the loop this is

00:00:27,150 --> 00:00:32,309
about a kind of design pattern that's

00:00:28,890 --> 00:00:34,829
emerging and in ways of getting people

00:00:32,309 --> 00:00:38,730
and machines working together for AI ops

00:00:34,829 --> 00:00:40,980
we're using this at O'Reilly are some of

00:00:38,730 --> 00:00:42,960
our teams over here and Taylor Martin is

00:00:40,980 --> 00:00:44,550
on our team she just told me this was

00:00:42,960 --> 00:00:47,819
perfect setup to have some kind of a

00:00:44,550 --> 00:00:51,480
human in a Habitrail illo I think I'll

00:00:47,819 --> 00:00:53,879
have to do that for the next slide so a

00:00:51,480 --> 00:00:56,760
little bit of framing if you could

00:00:53,879 --> 00:00:58,859
imagine having an automated mostly

00:00:56,760 --> 00:01:00,809
automated system so people and machines

00:00:58,859 --> 00:01:02,729
collaborating together and even though

00:01:00,809 --> 00:01:04,080
it sounds a bit sci-fi there's a lot of

00:01:02,729 --> 00:01:06,420
this I mean if you have an e-commerce

00:01:04,080 --> 00:01:08,280
site and it's running at scale but then

00:01:06,420 --> 00:01:09,869
you've got customer support to some

00:01:08,280 --> 00:01:13,140
extent you've got people and machines

00:01:09,869 --> 00:01:14,460
working together the challenge is can we

00:01:13,140 --> 00:01:16,829
go beyond that can we have something

00:01:14,460 --> 00:01:18,509
that's more advanced where computers are

00:01:16,829 --> 00:01:20,100
not just running code libraries but

00:01:18,509 --> 00:01:22,500
they're actually doing fairly difficult

00:01:20,100 --> 00:01:24,390
decisions on their exercising some kind

00:01:22,500 --> 00:01:27,810
of judgment they're working with

00:01:24,390 --> 00:01:30,509
complexity and even better beyond that

00:01:27,810 --> 00:01:33,359
can we have systems where the machines

00:01:30,509 --> 00:01:35,039
are being trained by human experts not

00:01:33,359 --> 00:01:37,560
because of the code that's being written

00:01:35,039 --> 00:01:40,259
or the model parameters but really just

00:01:37,560 --> 00:01:43,320
by being shown examples examples pro and

00:01:40,259 --> 00:01:45,210
con over and over so the scope of this

00:01:43,320 --> 00:01:47,850
thing is to try to bring people and

00:01:45,210 --> 00:01:50,509
machines together in workflows working

00:01:47,850 --> 00:01:53,960
on relatively complex problems and

00:01:50,509 --> 00:01:57,079
training by examples for exemplars and

00:01:53,960 --> 00:01:59,729
anybody recognize the illustration there

00:01:57,079 --> 00:02:01,859
it's a little bit of deep learning and

00:01:59,729 --> 00:02:04,200
practice it's deep dream generator but

00:02:01,859 --> 00:02:06,179
it's there's a guy I believe lives here

00:02:04,200 --> 00:02:09,080
in New York City who had an aunt Ella

00:02:06,179 --> 00:02:12,619
installed and has been filing for legal

00:02:09,080 --> 00:02:12,619
classification as a cyborg

00:02:12,650 --> 00:02:18,260
so some of the the research questions

00:02:16,580 --> 00:02:22,909
and it really got into trouble with TSA

00:02:18,260 --> 00:02:25,269
too by the way but um some research

00:02:22,909 --> 00:02:28,310
questions then we're working at O'Reilly

00:02:25,269 --> 00:02:32,420
we do conferences we have a learning

00:02:28,310 --> 00:02:35,090
platform called Safari and our team

00:02:32,420 --> 00:02:36,709
Learning Group here we're really focused

00:02:35,090 --> 00:02:38,860
on personalized learning and how can we

00:02:36,709 --> 00:02:41,239
put together the building blocks for

00:02:38,860 --> 00:02:45,170
personalized learning kind of platform

00:02:41,239 --> 00:02:48,400
and with that or Riley is much about

00:02:45,170 --> 00:02:51,470
peer teaching ever since our founders

00:02:48,400 --> 00:02:54,410
were classics majors at Harvard back in

00:02:51,470 --> 00:02:56,750
the 70s in wound up at a UNIX user group

00:02:54,410 --> 00:02:58,510
meeting in Boston and recognized just

00:02:56,750 --> 00:03:00,590
how horrible the man pages work and

00:02:58,510 --> 00:03:02,299
decided that they actually knew how to

00:03:00,590 --> 00:03:04,670
write you know probably they could

00:03:02,299 --> 00:03:06,319
contribute as writers given that they

00:03:04,670 --> 00:03:08,629
were fuzzy Studies majors and they liked

00:03:06,319 --> 00:03:11,180
to write and so they've been very

00:03:08,629 --> 00:03:13,250
interested in peer teaching across

00:03:11,180 --> 00:03:14,390
different kinds of mediums so you know

00:03:13,250 --> 00:03:16,129
we did books for a while that we did

00:03:14,390 --> 00:03:18,349
conferences and we've done other types

00:03:16,129 --> 00:03:19,940
of media formats and every time we try

00:03:18,349 --> 00:03:22,280
to bring back that sort of pure teaching

00:03:19,940 --> 00:03:24,109
perspective so how do we help the

00:03:22,280 --> 00:03:28,220
experts who by definition are really

00:03:24,109 --> 00:03:31,519
busy be able to engage and share what

00:03:28,220 --> 00:03:32,750
they know in conferences like this we're

00:03:31,519 --> 00:03:34,400
also have a challenge in terms of

00:03:32,750 --> 00:03:37,040
editors and I'll really dig into this

00:03:34,400 --> 00:03:39,349
but the kinds of media that are evolving

00:03:37,040 --> 00:03:41,209
are not as friendly to editors as they

00:03:39,349 --> 00:03:42,650
could be so how do we we try to keep

00:03:41,209 --> 00:03:46,120
that role of being an editor being

00:03:42,650 --> 00:03:49,160
really effective more at a human scale

00:03:46,120 --> 00:03:51,709
and so to illustrate some of this this

00:03:49,160 --> 00:03:54,049
is a prototype for like a next

00:03:51,709 --> 00:03:55,400
generation of search that hopefully will

00:03:54,049 --> 00:03:58,280
be coming out pretty soon for O'Reilly

00:03:55,400 --> 00:03:59,540
and you know you can see you've got a

00:03:58,280 --> 00:04:02,599
little bit of navigation on the side

00:03:59,540 --> 00:04:04,400
there that shows you topics you can

00:04:02,599 --> 00:04:07,489
navigate topics and similar searches and

00:04:04,400 --> 00:04:09,709
all the point being though is that part

00:04:07,489 --> 00:04:11,840
of this content is generated and curated

00:04:09,709 --> 00:04:16,639
by humans and part of it is generated

00:04:11,840 --> 00:04:18,079
and curated by applications from AI so

00:04:16,639 --> 00:04:20,930
here what you're seeing on the page

00:04:18,079 --> 00:04:22,780
there's some extractive summarization

00:04:20,930 --> 00:04:24,800
that's done the machine learning

00:04:22,780 --> 00:04:25,400
certainly there's a lot of work in terms

00:04:24,800 --> 00:04:28,460
of

00:04:25,400 --> 00:04:30,919
ontology and then semantic search in

00:04:28,460 --> 00:04:33,199
terms of the indexing there's various

00:04:30,919 --> 00:04:35,889
cases for deep learning that are applied

00:04:33,199 --> 00:04:40,729
here and we'll go through some of those

00:04:35,889 --> 00:04:44,030
so in terms of machines learning I want

00:04:40,729 --> 00:04:45,380
to kind of cover this a bit you probably

00:04:44,030 --> 00:04:46,970
hear in machine learning I guess

00:04:45,380 --> 00:04:50,360
everybody here heard of supervised

00:04:46,970 --> 00:04:52,729
learning yes okay so you've got a data

00:04:50,360 --> 00:04:54,979
set and every row has a label and you

00:04:52,729 --> 00:04:57,860
can build up learners you can optimize a

00:04:54,979 --> 00:05:01,010
set of learners so that they can predict

00:04:57,860 --> 00:05:02,630
labels given the data typically you're

00:05:01,010 --> 00:05:04,520
doing some kind of holdout to train on a

00:05:02,630 --> 00:05:07,940
portion of a data set and then test on

00:05:04,520 --> 00:05:10,900
the remainder deep learning basically

00:05:07,940 --> 00:05:14,360
structured layers of neural networks

00:05:10,900 --> 00:05:16,580
it's become a very popular way of an

00:05:14,360 --> 00:05:17,990
example of supervised learning but one

00:05:16,580 --> 00:05:20,330
of the problems with deep learning is

00:05:17,990 --> 00:05:23,020
that unless you have a lot of carefully

00:05:20,330 --> 00:05:26,270
labeled data it doesn't really apply I

00:05:23,020 --> 00:05:28,520
teach courses that use deep learning for

00:05:26,270 --> 00:05:31,250
natural language processing and even

00:05:28,520 --> 00:05:33,590
with the toy examples until you have at

00:05:31,250 --> 00:05:36,289
least like 10,000 examples to train on

00:05:33,590 --> 00:05:37,940
there is also horrible it's more like

00:05:36,289 --> 00:05:39,380
until you get into the millions that you

00:05:37,940 --> 00:05:42,560
start to get some convergence and some

00:05:39,380 --> 00:05:44,210
better examples and so that's in

00:05:42,560 --> 00:05:46,520
category for us because when you look at

00:05:44,210 --> 00:05:48,740
chapters and segments of videos we have

00:05:46,520 --> 00:05:50,270
millions and millions of those but

00:05:48,740 --> 00:05:51,680
that's a caveat that deep learning is

00:05:50,270 --> 00:05:53,479
very popular now but it doesn't

00:05:51,680 --> 00:05:55,729
necessarily apply unless you have

00:05:53,479 --> 00:05:59,030
labeled data and as it turned down a lot

00:05:55,729 --> 00:06:01,280
of cases we don't unsupervised learning

00:05:59,030 --> 00:06:03,289
is another popular form people talk

00:06:01,280 --> 00:06:05,780
about and that's where typically you're

00:06:03,289 --> 00:06:07,639
doing some kind of high dimensional data

00:06:05,780 --> 00:06:10,370
and then some sort of low dimensional

00:06:07,639 --> 00:06:12,229
embedded structure things like k-means

00:06:10,370 --> 00:06:14,450
or other clustering algorithms very

00:06:12,229 --> 00:06:17,060
popular examples of this neural networks

00:06:14,450 --> 00:06:21,560
without the deep structure component are

00:06:17,060 --> 00:06:23,120
typically unsupervised learning and you

00:06:21,560 --> 00:06:25,460
don't have the labels you don't need

00:06:23,120 --> 00:06:26,930
them but on the other hand you kind of

00:06:25,460 --> 00:06:28,580
have to dig into the structure that you

00:06:26,930 --> 00:06:31,280
get to figure out what it really means

00:06:28,580 --> 00:06:34,580
so it's still an open question how much

00:06:31,280 --> 00:06:36,140
that contributes to AI but there's

00:06:34,580 --> 00:06:37,340
something else that doesn't get talked

00:06:36,140 --> 00:06:38,930
about as much it's called

00:06:37,340 --> 00:06:43,790
semi-supervised learning

00:06:38,930 --> 00:06:46,520
and we're gonna focus on a special case

00:06:43,790 --> 00:06:48,949
of this called active learning and the

00:06:46,520 --> 00:06:51,020
idea is that if you have a machine

00:06:48,949 --> 00:06:53,630
learning let's say you have an ensemble

00:06:51,020 --> 00:06:57,050
the machine learning models that can

00:06:53,630 --> 00:07:00,860
predict some type of outcome some type

00:06:57,050 --> 00:07:03,139
of label if the ensemble can agree on

00:07:00,860 --> 00:07:05,660
making a prediction great ticket it's

00:07:03,139 --> 00:07:07,639
automated but if the ensemble disagrees

00:07:05,660 --> 00:07:10,130
or has very little confidence in what

00:07:07,639 --> 00:07:12,680
it's trying to produce then take that

00:07:10,130 --> 00:07:15,199
sort of edge case and kick it back to an

00:07:12,680 --> 00:07:18,500
expert and so this way you can have it

00:07:15,199 --> 00:07:19,729
work mostly automated but when you run

00:07:18,500 --> 00:07:21,740
into an edge case where the machines

00:07:19,729 --> 00:07:24,139
really can't agree on it then kick it

00:07:21,740 --> 00:07:26,810
back to an expert and what the judgment

00:07:24,139 --> 00:07:29,600
that the expert makes that iterates

00:07:26,810 --> 00:07:32,180
into training the model so now if you

00:07:29,600 --> 00:07:33,620
can train a model based on examples you

00:07:32,180 --> 00:07:36,260
have a pretty nice workflow pretty nice

00:07:33,620 --> 00:07:39,620
pipeline for putting the other humans

00:07:36,260 --> 00:07:42,620
and machines and a nice thing about this

00:07:39,620 --> 00:07:44,479
is that it works really well if you have

00:07:42,620 --> 00:07:48,349
a lot of data that isn't clot isn't

00:07:44,479 --> 00:07:50,449
labeled and where really the problem is

00:07:48,349 --> 00:07:52,789
getting to the label data set so as a

00:07:50,449 --> 00:07:55,099
case in point there's a company in San

00:07:52,789 --> 00:07:57,169
Francisco called CrowdFlower they've

00:07:55,099 --> 00:08:00,590
done some nice talks and case studies at

00:07:57,169 --> 00:08:02,270
O'Reilly conferences you know Google

00:08:00,590 --> 00:08:04,250
when they started doing self-driving

00:08:02,270 --> 00:08:06,710
cars they had a lot of really complex

00:08:04,250 --> 00:08:08,000
models machine learning models for the

00:08:06,710 --> 00:08:10,760
different activities in a self-driving

00:08:08,000 --> 00:08:12,560
car and they had gotten down the road

00:08:10,760 --> 00:08:14,870
some successful with self-driving cars

00:08:12,560 --> 00:08:16,490
and then they started applying very

00:08:14,870 --> 00:08:18,889
sophisticated deep learning models they

00:08:16,490 --> 00:08:20,270
just wiped out those models the the

00:08:18,889 --> 00:08:21,889
prior work of machine learning was

00:08:20,270 --> 00:08:24,860
subsumed by a lot more work with deep

00:08:21,889 --> 00:08:26,270
learning and the moral the story there

00:08:24,860 --> 00:08:28,130
is that Google has had years of

00:08:26,270 --> 00:08:31,010
collecting carefully level data sets of

00:08:28,130 --> 00:08:32,900
scale about driving and so now the auto

00:08:31,010 --> 00:08:34,520
manufacturers out of Detroit are

00:08:32,900 --> 00:08:36,860
suddenly showing up in San Francisco

00:08:34,520 --> 00:08:38,750
with big piles of unlabeled data saying

00:08:36,860 --> 00:08:40,969
oh yeah we need that too and here's a

00:08:38,750 --> 00:08:43,010
big pile of money do something active

00:08:40,969 --> 00:08:44,180
learning is a way that for instance the

00:08:43,010 --> 00:08:47,510
auto manufacturers are getting those

00:08:44,180 --> 00:08:51,050
labelled data sets but it's it's used in

00:08:47,510 --> 00:08:53,180
a lot of other areas as well so here's a

00:08:51,050 --> 00:08:55,010
book that describes

00:08:53,180 --> 00:08:59,120
actually it's a article but there's also

00:08:55,010 --> 00:09:03,080
a downloadable book for it to take vo

00:08:59,120 --> 00:09:04,250
talking about active learning really

00:09:03,080 --> 00:09:06,490
kind of breaking it down using

00:09:04,250 --> 00:09:09,710
CrowdFlower as one of the case studies

00:09:06,490 --> 00:09:11,300
and down here - one of the parts out of

00:09:09,710 --> 00:09:13,640
the book was what kind of policies

00:09:11,300 --> 00:09:15,650
you're gonna have bias in the system so

00:09:13,640 --> 00:09:17,720
where do you tilt the bias toward to

00:09:15,650 --> 00:09:20,110
make it most optimal and I'll talk about

00:09:17,720 --> 00:09:21,350
how we use that a little bit later

00:09:20,110 --> 00:09:25,310
CrowdFlower

00:09:21,350 --> 00:09:28,280
Luke Bo Wald is a person who created

00:09:25,310 --> 00:09:30,740
that and pretty pretty popular kind of

00:09:28,280 --> 00:09:32,510
overlay on top of him Turk is a way of

00:09:30,740 --> 00:09:35,540
thinking about it I'm so if you're

00:09:32,510 --> 00:09:38,510
familiar with em Turk Mechanical Turk

00:09:35,540 --> 00:09:40,400
service out of AWS then CrowdFlower is

00:09:38,510 --> 00:09:42,380
like another layer that can add some

00:09:40,400 --> 00:09:46,910
quality and statistical measurement of

00:09:42,380 --> 00:09:49,490
results of the EM Turks output and it's

00:09:46,910 --> 00:09:51,620
a nice kind of overlay for integrating

00:09:49,490 --> 00:09:54,770
active learning into that crowdsource

00:09:51,620 --> 00:09:57,320
problem another really great example of

00:09:54,770 --> 00:10:00,110
it is stitch fix any customers of stitch

00:09:57,320 --> 00:10:03,890
fix in the audience or spouses of

00:10:00,110 --> 00:10:06,920
customers yes that - or probably more

00:10:03,890 --> 00:10:08,660
rÃ½fat in it um but they have some great

00:10:06,920 --> 00:10:11,030
talks and case studies about this

00:10:08,660 --> 00:10:12,710
because they do combine a lot of human

00:10:11,030 --> 00:10:15,170
expertise along with the machine

00:10:12,710 --> 00:10:16,700
learning and so you know a case in point

00:10:15,170 --> 00:10:19,940
there Eric Coulson was talking about

00:10:16,700 --> 00:10:22,010
they can do excellent things for fashion

00:10:19,940 --> 00:10:24,320
for fashion selection with machine

00:10:22,010 --> 00:10:26,120
learning but the machine learning models

00:10:24,320 --> 00:10:28,040
have no idea how the people feel about

00:10:26,120 --> 00:10:29,450
it and so you really need to have

00:10:28,040 --> 00:10:30,530
introduced that kind of human component

00:10:29,450 --> 00:10:31,790
to have that kind of emotional

00:10:30,530 --> 00:10:34,550
connection with what the product

00:10:31,790 --> 00:10:38,810
supposed to do so very interesting ways

00:10:34,550 --> 00:10:40,280
of combining people and machines another

00:10:38,810 --> 00:10:43,700
one which i think is increasingly

00:10:40,280 --> 00:10:46,940
sophisticated jason Laska gave a talk

00:10:43,700 --> 00:10:48,470
here at the AI conference actually not

00:10:46,940 --> 00:10:51,910
this room but the next room about two

00:10:48,470 --> 00:10:53,810
months ago and great great case study

00:10:51,910 --> 00:10:56,150
essentially it's like a two sided

00:10:53,810 --> 00:10:58,400
marketplace and so there are jobs that

00:10:56,150 --> 00:11:00,200
need to get done if they can be

00:10:58,400 --> 00:11:03,160
performed entirely through automation

00:11:00,200 --> 00:11:06,399
through some kind of AI process

00:11:03,160 --> 00:11:06,819
then the amount of spend to get the job

00:11:06,399 --> 00:11:09,579
done

00:11:06,819 --> 00:11:12,370
goes down if it's a job that requires a

00:11:09,579 --> 00:11:14,769
person to do then the cost is higher and

00:11:12,370 --> 00:11:17,920
so effectively the people doing these

00:11:14,769 --> 00:11:19,449
tasks if they if they can't outperform

00:11:17,920 --> 00:11:21,279
the machines they'd aren't going to make

00:11:19,449 --> 00:11:23,470
as much money but if they want to like

00:11:21,279 --> 00:11:25,720
level up and earn more money per job

00:11:23,470 --> 00:11:28,360
then they can start to find out areas

00:11:25,720 --> 00:11:30,069
where they can a human cannot perform

00:11:28,360 --> 00:11:31,149
the machine and so it's interesting

00:11:30,069 --> 00:11:32,829
because you end up with a kind of

00:11:31,149 --> 00:11:34,600
spectrum of what is it that people do

00:11:32,829 --> 00:11:36,970
well and what is it the machines do well

00:11:34,600 --> 00:11:38,800
and starting to quantify that and and

00:11:36,970 --> 00:11:40,660
show market dynamics elasticity into

00:11:38,800 --> 00:11:42,189
that so I think it's a really really

00:11:40,660 --> 00:11:44,189
great what Clara labs has been

00:11:42,189 --> 00:11:47,230
publishing about what they're doing and

00:11:44,189 --> 00:11:49,509
and kind of a flip side for that too is

00:11:47,230 --> 00:11:54,430
Adam Marcus from b12 here in New York

00:11:49,509 --> 00:11:57,759
City where they're they're using AI to

00:11:54,430 --> 00:12:00,279
build out business websites and so it

00:11:57,759 --> 00:12:02,350
you know they're able to use automated

00:12:00,279 --> 00:12:04,209
systems to build out most of what a

00:12:02,350 --> 00:12:07,019
customer needs and then kind of at the

00:12:04,209 --> 00:12:09,130
last mile what developers get involved

00:12:07,019 --> 00:12:10,899
they've created an open-source project

00:12:09,130 --> 00:12:14,860
called orchestra to help with some of

00:12:10,899 --> 00:12:17,110
their active learning work along with

00:12:14,860 --> 00:12:19,569
that the person behind that there was

00:12:17,110 --> 00:12:22,360
Adam Marcus and he talks about another

00:12:19,569 --> 00:12:24,970
project here called flash teams anybody

00:12:22,360 --> 00:12:27,189
run across the flash teams yet it's sort

00:12:24,970 --> 00:12:29,279
of it's sort of like uber for

00:12:27,189 --> 00:12:31,839
crowdsourcing that doesn't sound right

00:12:29,279 --> 00:12:34,180
it's sort of like something above

00:12:31,839 --> 00:12:36,550
crowdsourcing above Mechanical Turk and

00:12:34,180 --> 00:12:38,709
as I understand this is out of Stanford

00:12:36,550 --> 00:12:40,180
I've talked with people at Accenture who

00:12:38,709 --> 00:12:43,300
are helping to fund this research at

00:12:40,180 --> 00:12:45,160
Stanford HCI and the idea is you know

00:12:43,300 --> 00:12:47,740
Accenture is a consulting company they

00:12:45,160 --> 00:12:50,230
send teams in to enterprise to do jobs

00:12:47,740 --> 00:12:52,660
they want to understand how to put

00:12:50,230 --> 00:12:54,550
together ad hoc teams on the fly

00:12:52,660 --> 00:12:57,610
basically crowdsourcing their workforce

00:12:54,550 --> 00:12:59,889
to some extent find out who are the

00:12:57,610 --> 00:13:02,050
people who work well on a team in an

00:12:59,889 --> 00:13:05,230
enterprise set up for a given kind of

00:13:02,050 --> 00:13:07,930
problem and so what's going on with

00:13:05,230 --> 00:13:11,290
flash teams then is sort of it dovetails

00:13:07,930 --> 00:13:12,699
very nicely with active learning and b12

00:13:11,290 --> 00:13:14,090
as an example of where they're trying to

00:13:12,699 --> 00:13:16,640
put these two together

00:13:14,090 --> 00:13:18,680
one other thing I'll show this also

00:13:16,640 --> 00:13:21,050
falls under the category of human in the

00:13:18,680 --> 00:13:23,060
loop you know whereas I've been talking

00:13:21,050 --> 00:13:26,330
about active learning as a design

00:13:23,060 --> 00:13:27,560
pattern called human and loop there's

00:13:26,330 --> 00:13:29,030
something out of Stanford

00:13:27,560 --> 00:13:32,090
Alex Ratner here is one of the grad

00:13:29,030 --> 00:13:34,820
students working on the project it was

00:13:32,090 --> 00:13:36,230
originally called deep dive and then a

00:13:34,820 --> 00:13:38,960
subsequent version of it is called

00:13:36,230 --> 00:13:40,130
snorkel and the idea is there they're

00:13:38,960 --> 00:13:42,770
using an approach called week

00:13:40,130 --> 00:13:44,150
supervision so in contrast to the

00:13:42,770 --> 00:13:46,730
semi-supervised that we were talking

00:13:44,150 --> 00:13:50,240
about or unsupervised or supervised

00:13:46,730 --> 00:13:51,740
learning week supervision is applying an

00:13:50,240 --> 00:13:55,220
approach they call data programming

00:13:51,740 --> 00:13:59,230
essentially building up functions to

00:13:55,220 --> 00:14:02,090
feed in data that's trusted more or less

00:13:59,230 --> 00:14:03,980
to develop the features so really

00:14:02,090 --> 00:14:05,570
instead of putting a lot of work toward

00:14:03,980 --> 00:14:08,660
the feature engineering let's back up

00:14:05,570 --> 00:14:11,180
and start engineering the supply of data

00:14:08,660 --> 00:14:12,350
and we know that in certain cases some

00:14:11,180 --> 00:14:14,570
data sources are more trusted than

00:14:12,350 --> 00:14:17,690
others so how can we actually move the

00:14:14,570 --> 00:14:19,220
learning back up to that stage and it's

00:14:17,690 --> 00:14:20,720
another fascinating example of

00:14:19,220 --> 00:14:23,750
human-in-the-loop we're actually looking

00:14:20,720 --> 00:14:25,400
at trying to apply this o'reilly and

00:14:23,750 --> 00:14:27,110
then reinforcement learning we're not

00:14:25,400 --> 00:14:29,000
using this yet but it's another kind of

00:14:27,110 --> 00:14:33,130
contrast to these areas of learning and

00:14:29,000 --> 00:14:35,090
you know i used to work on spark at

00:14:33,130 --> 00:14:39,800
out-of-date bricks very closely

00:14:35,090 --> 00:14:42,890
associated with UC berkeley IES and my

00:14:39,800 --> 00:14:44,780
former boss left data bricks combat he's

00:14:42,890 --> 00:14:49,130
gone back to lead a project called rise

00:14:44,780 --> 00:14:51,290
lab at berkeley and they're just putting

00:14:49,130 --> 00:14:52,820
all the chips in toward reinforcement

00:14:51,290 --> 00:14:56,840
learning they feel this will be the next

00:14:52,820 --> 00:15:00,200
big wave for what's going on in AI okay

00:14:56,840 --> 00:15:02,630
so that gives some context then of

00:15:00,200 --> 00:15:04,430
different ways of getting people and

00:15:02,630 --> 00:15:05,600
machines to work together but I really

00:15:04,430 --> 00:15:08,270
want to focus on the active learning

00:15:05,600 --> 00:15:10,100
part and I want to give a little bit of

00:15:08,270 --> 00:15:12,590
context about what we're trying to solve

00:15:10,100 --> 00:15:16,190
with our team and and how we ran into

00:15:12,590 --> 00:15:20,030
this problem so we're working with AI in

00:15:16,190 --> 00:15:22,340
media and the idea will simply put is

00:15:20,030 --> 00:15:25,400
that if you've got some kind of content

00:15:22,340 --> 00:15:27,750
that can be represented as text then of

00:15:25,400 --> 00:15:29,910
course you can go in and use NLP to part

00:15:27,750 --> 00:15:31,560
the text once you've got it parsed you

00:15:29,910 --> 00:15:34,920
can do a lot of fun things with it

00:15:31,560 --> 00:15:37,890
whether it's summarization or semantic

00:15:34,920 --> 00:15:39,750
similarity or some type of named entity

00:15:37,890 --> 00:15:42,470
extraction there's just a lot of things

00:15:39,750 --> 00:15:45,120
you can do once you've got a parsed text

00:15:42,470 --> 00:15:47,820
of course if you've got audio or video

00:15:45,120 --> 00:15:49,710
now you can get it transcribed and so

00:15:47,820 --> 00:15:52,170
for instance you could do AI services in

00:15:49,710 --> 00:15:54,630
the cloud to get transcriptions out of

00:15:52,170 --> 00:15:56,490
video and now start to apply a lot of

00:15:54,630 --> 00:15:59,670
these techniques even in another media

00:15:56,490 --> 00:16:03,120
format so there's a lot that we can do

00:15:59,670 --> 00:16:05,340
base building off of NLP and if you have

00:16:03,120 --> 00:16:07,770
images you have large sets of labelled

00:16:05,340 --> 00:16:09,060
images such as we have coming out of a

00:16:07,770 --> 00:16:12,450
conference with all the slides in the

00:16:09,060 --> 00:16:15,720
video that gets really interesting too

00:16:12,450 --> 00:16:17,610
and we presuppose that this kind of

00:16:15,720 --> 00:16:20,010
content when we apply machine learning

00:16:17,610 --> 00:16:21,690
to it we're presupposing there's some

00:16:20,010 --> 00:16:23,610
sort of structure there there's some

00:16:21,690 --> 00:16:25,950
kind of low order embedding if you will

00:16:23,610 --> 00:16:27,180
and and there is we hope we hope that

00:16:25,950 --> 00:16:28,020
the learning materials are teaching

00:16:27,180 --> 00:16:30,840
something so there's some kind of

00:16:28,020 --> 00:16:33,930
structure but representing that kind of

00:16:30,840 --> 00:16:35,820
structure is fairly rare in media so

00:16:33,930 --> 00:16:37,470
we're we're we're trying to work on this

00:16:35,820 --> 00:16:39,750
we're trying to find others who are

00:16:37,470 --> 00:16:43,470
working on it as well of really how to

00:16:39,750 --> 00:16:47,040
do the representation part so as an

00:16:43,470 --> 00:16:48,089
example here's a popular instructional

00:16:47,040 --> 00:16:51,900
video that we have

00:16:48,089 --> 00:16:53,460
intro to docker by Andrew Baker and up

00:16:51,900 --> 00:16:56,460
in the top left there's a transcript

00:16:53,460 --> 00:17:00,870
that came out of a cloud service using

00:16:56,460 --> 00:17:04,410
AI to transcribe from an MPEG into you

00:17:00,870 --> 00:17:07,920
know a snippet of transcribed utterance

00:17:04,410 --> 00:17:09,980
if you will and then on the right hand

00:17:07,920 --> 00:17:12,600
side we've got the results of parsing

00:17:09,980 --> 00:17:16,020
running this through Spacey and in

00:17:12,600 --> 00:17:17,730
Python and and also text rank and then

00:17:16,020 --> 00:17:19,620
from there being able to extract out

00:17:17,730 --> 00:17:21,480
what are key phrases that are important

00:17:19,620 --> 00:17:24,329
out of that chapter a video that

00:17:21,480 --> 00:17:26,370
segments a video and you can see Andrew

00:17:24,329 --> 00:17:28,680
Baker was giving some setup for docker

00:17:26,370 --> 00:17:31,980
and he was contrasting it with earlier

00:17:28,680 --> 00:17:34,410
technologies like VMware so chapter

00:17:31,980 --> 00:17:35,960
section one of his video was about the

00:17:34,410 --> 00:17:38,960
previous kind of

00:17:35,960 --> 00:17:40,789
of virtualization strategies and you can

00:17:38,960 --> 00:17:43,909
see what pops out is virtualization tool

00:17:40,789 --> 00:17:48,049
VMware hypervisor VirtualBox the things

00:17:43,909 --> 00:17:50,120
that came before docker and then over on

00:17:48,049 --> 00:17:53,029
the left hand side there's some tie-in

00:17:50,120 --> 00:17:55,360
to part of our ontology and then by

00:17:53,029 --> 00:17:59,149
using vector embedding deep learning

00:17:55,360 --> 00:18:01,340
we're able to go and expand that to take

00:17:59,149 --> 00:18:04,960
a look and see well you know as it turns

00:18:01,340 --> 00:18:08,120
out machine learning tells us that that

00:18:04,960 --> 00:18:10,130
here docker and docker container are

00:18:08,120 --> 00:18:13,429
tend to be associated with things like

00:18:10,130 --> 00:18:16,399
core OS and career Nettie's and Mayo's

00:18:13,429 --> 00:18:19,279
and OpenStack sort of in in that realm

00:18:16,399 --> 00:18:20,480
of DevOps the devil you know so these

00:18:19,279 --> 00:18:23,590
are ways that machine learning can be

00:18:20,480 --> 00:18:27,049
used with with learning content in media

00:18:23,590 --> 00:18:30,770
now getting back to the the humans in

00:18:27,049 --> 00:18:33,770
loop part of the problem is that we've

00:18:30,770 --> 00:18:35,840
got a whole bunch of data products if

00:18:33,770 --> 00:18:38,210
you will that are machine generated that

00:18:35,840 --> 00:18:39,260
come out of machine learning but we want

00:18:38,210 --> 00:18:41,720
to be able to have some kind of control

00:18:39,260 --> 00:18:44,960
points for that basically ways to steer

00:18:41,720 --> 00:18:47,539
the ship so we have an ontology that's

00:18:44,960 --> 00:18:50,929
really more on the order of a few

00:18:47,539 --> 00:18:54,020
hundred rules and then we have the

00:18:50,929 --> 00:18:55,640
machine learning products output which

00:18:54,020 --> 00:18:58,700
tie into it which are on the order of

00:18:55,640 --> 00:19:00,620
billions of data points so you can think

00:18:58,700 --> 00:19:03,380
of having an ontology as a kind of graph

00:19:00,620 --> 00:19:05,480
at a human scale people can work with it

00:19:03,380 --> 00:19:07,640
it's the primary structure of what we're

00:19:05,480 --> 00:19:09,559
working with it's sort of the control

00:19:07,640 --> 00:19:11,510
points for a lot of this automation it's

00:19:09,559 --> 00:19:14,240
also a matter of what can we test and

00:19:11,510 --> 00:19:16,520
verify validate but then you've got this

00:19:14,240 --> 00:19:19,309
small graph that's basically tracking a

00:19:16,520 --> 00:19:20,950
much larger graph below it and so the

00:19:19,309 --> 00:19:24,950
game is to try to keep some structural

00:19:20,950 --> 00:19:26,539
correspondence between those two and and

00:19:24,950 --> 00:19:28,669
then the big win for us is to have a

00:19:26,539 --> 00:19:30,770
nice large annotated graph and do

00:19:28,669 --> 00:19:35,120
inference across it that's where the AI

00:19:30,770 --> 00:19:37,460
part of this really comes in so ontology

00:19:35,120 --> 00:19:39,890
is something that deep learning doesn't

00:19:37,460 --> 00:19:41,840
give you it really helps provide a lot

00:19:39,890 --> 00:19:43,520
of the context and so you've probably

00:19:41,840 --> 00:19:46,490
already hear heard of knowledge graph

00:19:43,520 --> 00:19:48,039
Google project culture knowledge graph a

00:19:46,490 --> 00:19:49,820
very famous project in ontology

00:19:48,039 --> 00:19:52,070
essentially what you're doing is

00:19:49,820 --> 00:19:54,530
building computable for the source and

00:19:52,070 --> 00:19:57,140
it's typically represented as a graph so

00:19:54,530 --> 00:19:58,610
graph algorithms work out really well in

00:19:57,140 --> 00:19:59,960
another way you can think of it as like

00:19:58,610 --> 00:20:02,120
the semantics of business relationships

00:19:59,960 --> 00:20:03,440
and that's that's what we're doing is

00:20:02,120 --> 00:20:04,940
mapping out all the business

00:20:03,440 --> 00:20:07,310
relationships across O'Reilly and our

00:20:04,940 --> 00:20:09,910
partners and and what do they mean in a

00:20:07,310 --> 00:20:12,080
given context

00:20:09,910 --> 00:20:14,510
so I mean just an example here's a

00:20:12,080 --> 00:20:16,700
visualization out of d3 for part of our

00:20:14,510 --> 00:20:18,620
ontology and this was the part that was

00:20:16,700 --> 00:20:21,410
linking Becky and it was referencing

00:20:18,620 --> 00:20:24,200
Library of Congress there you know their

00:20:21,410 --> 00:20:27,800
catalogue for how do we link in through

00:20:24,200 --> 00:20:29,900
OHS OCLC mark records etc and then it's

00:20:27,800 --> 00:20:32,330
also linking into dbpedia which has some

00:20:29,900 --> 00:20:33,530
structured data about deep learning so

00:20:32,330 --> 00:20:35,390
it's great because if you can put that

00:20:33,530 --> 00:20:36,860
kind of structure on your content you

00:20:35,390 --> 00:20:38,780
have those labels and you can link it in

00:20:36,860 --> 00:20:41,690
the right place then you can go and

00:20:38,780 --> 00:20:44,000
access resources out of places like

00:20:41,690 --> 00:20:46,430
Library of Congress or dbpedia you

00:20:44,000 --> 00:20:47,480
really make a lot more use of it but

00:20:46,430 --> 00:20:50,690
there's a problem

00:20:47,480 --> 00:20:54,290
so disambiguating context is hard and

00:20:50,690 --> 00:20:55,760
it's not something that the deep

00:20:54,290 --> 00:20:57,590
learning is going to solve anytime soon

00:20:55,760 --> 00:21:00,170
apparently it's not something that

00:20:57,590 --> 00:21:01,460
ontology really solves it's not really

00:21:00,170 --> 00:21:03,910
something that there's a lot of tooling

00:21:01,460 --> 00:21:06,830
around yet so when you have overlapping

00:21:03,910 --> 00:21:08,420
contexts it really gives you a hard

00:21:06,830 --> 00:21:10,310
problem in kind of natural language

00:21:08,420 --> 00:21:12,680
understanding there's not a lot of

00:21:10,310 --> 00:21:14,630
tooling and it also the whole thing sort

00:21:12,680 --> 00:21:16,640
of runs counter to this big wave of

00:21:14,630 --> 00:21:19,130
correlation that you see in Big Data

00:21:16,640 --> 00:21:20,600
right we're looking for patterns and if

00:21:19,130 --> 00:21:23,270
the patterns all can be generalized

00:21:20,600 --> 00:21:24,740
great and then we find out that that's

00:21:23,270 --> 00:21:27,710
actually the problem that's biting us is

00:21:24,740 --> 00:21:30,170
that having too much correlation is a

00:21:27,710 --> 00:21:32,780
bad thing and just for kicks I did some

00:21:30,170 --> 00:21:34,940
correlation here between Firefly and on

00:21:32,780 --> 00:21:36,680
the expanse so I don't know that's just

00:21:34,940 --> 00:21:41,690
my view of the world but you know we'll

00:21:36,680 --> 00:21:43,310
see a correlation and not causation so

00:21:41,690 --> 00:21:45,350
the problem we have is suppose somebody

00:21:43,310 --> 00:21:47,690
publishes a book and they use the term

00:21:45,350 --> 00:21:50,180
iOS and they're talking about an

00:21:47,690 --> 00:21:51,860
operating system so are they talking and

00:21:50,180 --> 00:21:54,050
based on that operating knowledge are

00:21:51,860 --> 00:21:56,600
they talking about an iPhone or are they

00:21:54,050 --> 00:21:58,250
talking about a Cisco router because it

00:21:56,600 --> 00:21:59,830
could be either and we have a lot of

00:21:58,250 --> 00:22:01,630
content about both and if

00:21:59,830 --> 00:22:02,890
want to do personalized learning you

00:22:01,630 --> 00:22:04,779
have to be able to disambiguate those

00:22:02,890 --> 00:22:06,640
contexts and if you haven't enough

00:22:04,779 --> 00:22:08,590
signal on a person yeah you can use that

00:22:06,640 --> 00:22:10,299
as a prior but when they first start out

00:22:08,590 --> 00:22:13,120
and they first have initial experiences

00:22:10,299 --> 00:22:15,250
you want those to be good UX experiences

00:22:13,120 --> 00:22:16,690
you don't have data on them and so

00:22:15,250 --> 00:22:19,299
there's kind of a chicken and egg

00:22:16,690 --> 00:22:22,120
problem there this would be great to use

00:22:19,299 --> 00:22:24,100
deep learning on if we could except for

00:22:22,120 --> 00:22:26,110
by definition we don't have label data

00:22:24,100 --> 00:22:30,370
so we can't really train we have to

00:22:26,110 --> 00:22:33,190
bootstrap that so just an example if I

00:22:30,370 --> 00:22:35,320
were to search for react on Google my

00:22:33,190 --> 00:22:38,169
first page of results comes back with

00:22:35,320 --> 00:22:40,630
things about charities and surveys and

00:22:38,169 --> 00:22:44,740
video games and oh yeah UI web

00:22:40,630 --> 00:22:47,320
components now with our audience if I

00:22:44,740 --> 00:22:49,000
say react well naturally what you're

00:22:47,320 --> 00:22:50,740
talking about is business leadership

00:22:49,000 --> 00:22:52,690
when you're conducting interviews or

00:22:50,740 --> 00:22:54,519
doing a sales meeting how do people

00:22:52,690 --> 00:22:55,779
react because we have a lot of content

00:22:54,519 --> 00:22:57,700
about that and that's super important

00:22:55,779 --> 00:23:00,730
but obviously if you're talking about

00:22:57,700 --> 00:23:02,559
react what that means is some ions are

00:23:00,730 --> 00:23:04,990
interacting because there's a chemistry

00:23:02,559 --> 00:23:07,120
balance and so you've got things like

00:23:04,990 --> 00:23:08,500
valence and and and whatnot that are

00:23:07,120 --> 00:23:09,789
going on because that's really important

00:23:08,500 --> 00:23:12,159
for the scientific literature that we

00:23:09,789 --> 00:23:13,480
have or if you're talking about react

00:23:12,159 --> 00:23:16,630
obviously you're talking about a

00:23:13,480 --> 00:23:18,519
JavaScript library and we don't really

00:23:16,630 --> 00:23:20,919
know in advance what's inside somebody

00:23:18,519 --> 00:23:24,309
else's head so there's a lot of

00:23:20,919 --> 00:23:27,250
contextual problems so that's where

00:23:24,309 --> 00:23:29,409
we're using Jupiter and rather than

00:23:27,250 --> 00:23:31,360
spending millions of dollars no offense

00:23:29,409 --> 00:23:32,529
to look be evolved rather than spending

00:23:31,360 --> 00:23:34,929
tens of millions of dollars on

00:23:32,529 --> 00:23:37,539
CrowdFlower we didn't really have that

00:23:34,929 --> 00:23:40,870
available so we started use open source

00:23:37,539 --> 00:23:44,679
instead and the idea is that Jupiter

00:23:40,870 --> 00:23:47,830
notebooks are used for managing machine

00:23:44,679 --> 00:23:50,169
learning pipelines that perform the

00:23:47,830 --> 00:23:52,330
disambiguation after we've used AI

00:23:50,169 --> 00:23:54,399
services to get transcripts and whatnot

00:23:52,330 --> 00:23:56,740
after we get text together and we parse

00:23:54,399 --> 00:23:58,570
it and we start doing a lot of magic in

00:23:56,740 --> 00:24:00,370
terms of machine learning for building

00:23:58,570 --> 00:24:03,490
up indexes etc we still have to

00:24:00,370 --> 00:24:06,010
disambiguate and so what we do is to use

00:24:03,490 --> 00:24:07,809
the notebooks as a way of getting the

00:24:06,010 --> 00:24:10,750
people and the machines to collaborate

00:24:07,809 --> 00:24:13,270
together so a notebook serves as almost

00:24:10,750 --> 00:24:16,240
kind of a structured blog it's one part

00:24:13,270 --> 00:24:17,800
figuration file one part data sample one

00:24:16,240 --> 00:24:20,530
part log and one part did a

00:24:17,800 --> 00:24:22,480
visualization tool and the nice thing

00:24:20,530 --> 00:24:27,100
about it is that for the most part this

00:24:22,480 --> 00:24:29,320
is running at scale on you know like I

00:24:27,100 --> 00:24:31,510
say millions of elements in terms of

00:24:29,320 --> 00:24:34,000
chapters it's running at scale mostly

00:24:31,510 --> 00:24:36,880
automated but if there's a problem then

00:24:34,000 --> 00:24:38,260
a person can jump in and review what's

00:24:36,880 --> 00:24:40,900
going on with that pipeline by using

00:24:38,260 --> 00:24:42,460
Jupiter and so sort of the statefulness

00:24:40,900 --> 00:24:45,070
of the pipeline is represented by

00:24:42,460 --> 00:24:46,690
jupiter notebooks the other thing that's

00:24:45,070 --> 00:24:48,430
important is that machine learning here

00:24:46,690 --> 00:24:51,010
is not based on feature engineering or

00:24:48,430 --> 00:24:52,720
model parameter selection the machine

00:24:51,010 --> 00:24:55,810
learning here is based on having people

00:24:52,720 --> 00:24:58,150
working with learning content people on

00:24:55,810 --> 00:25:01,420
my team actually so um taylor and st and

00:24:58,150 --> 00:25:03,520
myself providing examples here's a

00:25:01,420 --> 00:25:06,490
chapter that is an example of javascript

00:25:03,520 --> 00:25:08,650
react here's an example of how to

00:25:06,490 --> 00:25:10,090
conduct an interview react here's an

00:25:08,650 --> 00:25:12,970
example of react that's none of the

00:25:10,090 --> 00:25:14,920
above and so learning by example

00:25:12,970 --> 00:25:17,890
providing exemplars is a really powerful

00:25:14,920 --> 00:25:19,120
tool for us and it's something that

00:25:17,890 --> 00:25:22,480
really enables the active learning to be

00:25:19,120 --> 00:25:25,150
practical so the way that this works is

00:25:22,480 --> 00:25:27,900
that experts use notebooks to provide

00:25:25,150 --> 00:25:30,190
examples of chapters or video segments

00:25:27,900 --> 00:25:33,460
anywhere that there's a key phrase which

00:25:30,190 --> 00:25:35,560
has overlapping contexts and then the

00:25:33,460 --> 00:25:38,050
machines build up ensemble models based

00:25:35,560 --> 00:25:40,990
on those examples the model evaluation

00:25:38,050 --> 00:25:42,460
is all see realized out to the notebooks

00:25:40,990 --> 00:25:45,490
for later review if needed

00:25:42,460 --> 00:25:47,290
the machines attempt to do annotation

00:25:45,490 --> 00:25:49,900
working at scale so millions of pieces

00:25:47,290 --> 00:25:52,030
of content and again the same thing here

00:25:49,900 --> 00:25:52,420
are they talking about : are they

00:25:52,030 --> 00:25:54,430
talking about

00:25:52,420 --> 00:25:57,700
alphago or they using the verb go for

00:25:54,430 --> 00:26:00,490
something mundane and this runs mostly

00:25:57,700 --> 00:26:01,320
automated we're using a spark with

00:26:00,490 --> 00:26:04,810
scikit-learn

00:26:01,320 --> 00:26:07,870
to do a lot of the task management is

00:26:04,810 --> 00:26:10,120
scale so we can run in parallel when the

00:26:07,870 --> 00:26:11,680
ensemble's disagree then the decision

00:26:10,120 --> 00:26:13,030
goes back to the humans to make a

00:26:11,680 --> 00:26:15,310
judgment call and provide further

00:26:13,030 --> 00:26:18,160
examples basically adds more examples to

00:26:15,310 --> 00:26:19,390
the notebooks and and then that goes

00:26:18,160 --> 00:26:23,770
back into training the models to be

00:26:19,390 --> 00:26:26,010
better so it looks a bit like this just

00:26:23,770 --> 00:26:32,370
the way that we're using things we use

00:26:26,010 --> 00:26:34,020
SSH tunnels from our our our our server

00:26:32,370 --> 00:26:35,460
environment where we're running machine

00:26:34,020 --> 00:26:38,820
learning we have some beefy machines to

00:26:35,460 --> 00:26:40,650
do that we use an SSH tunnel so a person

00:26:38,820 --> 00:26:45,750
who's working with it the expert can run

00:26:40,650 --> 00:26:47,790
Jupiter in their browser and I guess

00:26:45,750 --> 00:26:50,160
another way of looking at this is that

00:26:47,790 --> 00:26:53,190
both the machines and the people become

00:26:50,160 --> 00:26:55,080
collaborators on shared documents so in

00:26:53,190 --> 00:26:56,669
a lot of ways this is anticipating what

00:26:55,080 --> 00:26:59,400
will be coming up with Jupiter Jupiter

00:26:56,669 --> 00:27:01,950
lab using Google Drive for shared

00:26:59,400 --> 00:27:05,010
documents essentially machines are

00:27:01,950 --> 00:27:06,120
writing part of the Docs there is an

00:27:05,010 --> 00:27:09,450
open-source project it's called

00:27:06,120 --> 00:27:11,400
env transom it's it's tiny it's not a

00:27:09,450 --> 00:27:14,580
lot of code it's based on top of env

00:27:11,400 --> 00:27:16,530
format and pandas and the idea is being

00:27:14,580 --> 00:27:18,120
able to leverage a notebook as partly as

00:27:16,530 --> 00:27:20,640
a data store partly is like the

00:27:18,120 --> 00:27:22,200
analytics view for this one thing that

00:27:20,640 --> 00:27:24,690
we did do was to come up with a custom

00:27:22,200 --> 00:27:26,760
pretty printer for it just so we could

00:27:24,690 --> 00:27:28,950
kind of optimize its interaction would

00:27:26,760 --> 00:27:30,570
get we don't we don't want to dump out a

00:27:28,950 --> 00:27:33,210
lot of data and then have it all be on

00:27:30,570 --> 00:27:34,830
one line that's kind of a small-knit but

00:27:33,210 --> 00:27:36,870
it's actually helpful to tune your

00:27:34,830 --> 00:27:41,040
pretty printers for what goes into the

00:27:36,870 --> 00:27:42,780
notebooks now in terms of machine

00:27:41,040 --> 00:27:43,860
learning if is ever here familiar with

00:27:42,780 --> 00:27:44,630
something called the no free lunch

00:27:43,860 --> 00:27:46,590
theorem

00:27:44,630 --> 00:27:48,900
you know the idea is there's gonna be

00:27:46,590 --> 00:27:51,450
bias and you're going to have to tilt

00:27:48,900 --> 00:27:53,669
your your operations one way or another

00:27:51,450 --> 00:27:55,440
so for us it's better to err on the side

00:27:53,669 --> 00:27:57,960
of less false positives more false

00:27:55,440 --> 00:27:59,370
negatives because we want to get people

00:27:57,960 --> 00:28:01,140
good search results we don't want to

00:27:59,370 --> 00:28:05,220
give them bad search results we have a

00:28:01,140 --> 00:28:07,049
lot of content if we miss recall if we

00:28:05,220 --> 00:28:09,059
miss showing absolutely everything that

00:28:07,049 --> 00:28:11,130
would be possibly good to show and

00:28:09,059 --> 00:28:12,360
instead show a lot of things that are

00:28:11,130 --> 00:28:14,040
good to show it's still going to

00:28:12,360 --> 00:28:17,309
overwhelm what people can look with in a

00:28:14,040 --> 00:28:19,080
search page now in terms of active

00:28:17,309 --> 00:28:21,000
learning that's a policy that's

00:28:19,080 --> 00:28:24,330
typically called biased toward exemplars

00:28:21,000 --> 00:28:26,429
most likely to influence a classifier so

00:28:24,330 --> 00:28:28,610
we're trying to select for things that

00:28:26,429 --> 00:28:32,520
are going to help the classifier or more

00:28:28,610 --> 00:28:34,830
so examples of this like we've found we

00:28:32,520 --> 00:28:37,530
don't use the introductory chapters in

00:28:34,830 --> 00:28:38,270
our examples those are just they're all

00:28:37,530 --> 00:28:39,700
over the map

00:28:38,270 --> 00:28:41,139
so we

00:28:39,700 --> 00:28:43,360
make sure to sort of start with chapter

00:28:41,139 --> 00:28:45,190
2 you know we know that we shouldn't use

00:28:43,360 --> 00:28:46,870
a bibliography or an appendix or things

00:28:45,190 --> 00:28:48,490
like that to so we we've been trying to

00:28:46,870 --> 00:28:50,080
develop some policy as we go along and

00:28:48,490 --> 00:28:51,669
we're doing this when we we share this a

00:28:50,080 --> 00:28:55,720
lot of what seems to work or what

00:28:51,669 --> 00:28:56,860
doesn't and potentially even though

00:28:55,720 --> 00:28:58,960
right now there's three of us at

00:28:56,860 --> 00:29:02,110
O'Reilly who are being the experts to

00:28:58,960 --> 00:29:04,510
this potentially it wouldn't be hard to

00:29:02,110 --> 00:29:06,370
put into the search UI and allow

00:29:04,510 --> 00:29:08,019
customer service people to do it instead

00:29:06,370 --> 00:29:09,460
they're the ones who are talking to

00:29:08,019 --> 00:29:11,769
customers they hear when there's bad

00:29:09,460 --> 00:29:13,450
content being recommended and they

00:29:11,769 --> 00:29:15,460
should be the ones who can push a button

00:29:13,450 --> 00:29:19,779
and show good examples versus bad

00:29:15,460 --> 00:29:22,059
examples so this is our view of human in

00:29:19,779 --> 00:29:23,769
the loop as a management strategy that

00:29:22,059 --> 00:29:25,360
really this should extend out to

00:29:23,769 --> 00:29:27,309
something like a customer service

00:29:25,360 --> 00:29:33,940
organization possibly to a sales

00:29:27,309 --> 00:29:37,419
organization etc okay and then so

00:29:33,940 --> 00:29:39,700
summary um a little more deep learning

00:29:37,419 --> 00:29:40,630
this is art RCA sort of our mascot but I

00:29:39,700 --> 00:29:43,840
wanted to run it through deep learning

00:29:40,630 --> 00:29:47,950
so um and I know that Edie Freeman's

00:29:43,840 --> 00:29:49,389
gonna kill me for this my personal op-ed

00:29:47,950 --> 00:29:51,610
is that the game that we're working with

00:29:49,389 --> 00:29:54,039
isn't about replacing people by using AI

00:29:51,610 --> 00:29:56,470
rather it's leveraging it's augmenting

00:29:54,039 --> 00:29:59,260
what people do I was talking earlier

00:29:56,470 --> 00:30:03,460
when we used the the illustration we do

00:29:59,260 --> 00:30:04,750
20 these conferences a year and with 20

00:30:03,460 --> 00:30:07,120
conferences a year some of them are much

00:30:04,750 --> 00:30:11,019
larger strata here we'll have you know

00:30:07,120 --> 00:30:13,929
thousands of people that's enough video

00:30:11,019 --> 00:30:16,690
that if we had an editor a development

00:30:13,929 --> 00:30:18,130
editor review all of our video they

00:30:16,690 --> 00:30:19,600
would have to have their finger on the

00:30:18,130 --> 00:30:22,000
fast-forward button for 10 months out of

00:30:19,600 --> 00:30:23,320
the year I mean that's the data rate and

00:30:22,000 --> 00:30:27,039
there's no way the editors are going to

00:30:23,320 --> 00:30:29,769
do that so instead can we use AI tooling

00:30:27,039 --> 00:30:31,389
which is available to produce summaries

00:30:29,769 --> 00:30:33,399
so this is the game that we're getting

00:30:31,389 --> 00:30:36,909
into now is can we take 40 minute talk

00:30:33,399 --> 00:30:38,919
and summarize it on a page and show some

00:30:36,909 --> 00:30:40,450
of the structure what are the main

00:30:38,919 --> 00:30:42,669
things that are being in the main

00:30:40,450 --> 00:30:45,100
phrases the the main key terms etc that

00:30:42,669 --> 00:30:48,580
are being used and then allow an editor

00:30:45,100 --> 00:30:51,129
to go from that text to a minute second

00:30:48,580 --> 00:30:52,480
time code inside the video that's useful

00:30:51,129 --> 00:30:53,070
for editors to make their life more

00:30:52,480 --> 00:30:55,740
livable

00:30:53,070 --> 00:30:57,780
it's useful also for customers too and

00:30:55,740 --> 00:30:59,790
the other thing that's coming up is you

00:30:57,780 --> 00:31:04,350
know YouTube was introduced twelve years

00:30:59,790 --> 00:31:08,850
ago my kids are 13 and 12 and they live

00:31:04,350 --> 00:31:10,260
in YouTube and as a media company we're

00:31:08,850 --> 00:31:12,180
pretty good at this and we're just now

00:31:10,260 --> 00:31:13,920
sort of wrapping our heads around how to

00:31:12,180 --> 00:31:15,600
work with video and we've been doing it

00:31:13,920 --> 00:31:17,160
for a while but not a long while and

00:31:15,600 --> 00:31:19,230
we're still learning like what's

00:31:17,160 --> 00:31:20,640
effective or not and so we're still sort

00:31:19,230 --> 00:31:22,200
of coming up to speed on video even

00:31:20,640 --> 00:31:25,050
though the demand for video is really

00:31:22,200 --> 00:31:27,930
outpacing the demand for books but the

00:31:25,050 --> 00:31:32,670
worry that I have is that I go I went to

00:31:27,930 --> 00:31:34,440
anybody been to a drone race so I went

00:31:32,670 --> 00:31:36,900
to it I got invited to go to the first

00:31:34,440 --> 00:31:38,400
legal drone race in California recently

00:31:36,900 --> 00:31:40,490
because it's available out of the

00:31:38,400 --> 00:31:43,410
illegal ones to you they're pretty cool

00:31:40,490 --> 00:31:45,270
and so we went to weigh in like I don't

00:31:43,410 --> 00:31:47,310
know his Xfinity was sponsoring it they

00:31:45,270 --> 00:31:48,900
dropped way too much money but there's a

00:31:47,310 --> 00:31:50,700
lot of fun but the thing is the drone

00:31:48,900 --> 00:31:53,120
race is on it's inside of a warehouse

00:31:50,700 --> 00:31:56,370
and it's all built on augmented reality

00:31:53,120 --> 00:31:58,680
so the drone racetrack is an AR setup

00:31:56,370 --> 00:32:00,720
and so all the pylons they're flying

00:31:58,680 --> 00:32:02,100
through are all based off of AR and then

00:32:00,720 --> 00:32:06,030
the pilots are working in first-person

00:32:02,100 --> 00:32:08,190
view in VR so both the pilots and the

00:32:06,030 --> 00:32:11,280
and the audience if you want you can put

00:32:08,190 --> 00:32:13,800
on a goggle and watch the race like from

00:32:11,280 --> 00:32:15,030
a given competitors perspective and so

00:32:13,800 --> 00:32:17,700
that's one compelling thing about drone

00:32:15,030 --> 00:32:21,210
races is it's very much this AR VR kind

00:32:17,700 --> 00:32:24,510
of a thing we're seeing we're in

00:32:21,210 --> 00:32:25,980
manufacturing you know we see in

00:32:24,510 --> 00:32:29,580
industrial use cases where people are

00:32:25,980 --> 00:32:32,190
being trained using AR we are certainly

00:32:29,580 --> 00:32:33,270
seeing that in areas of military that's

00:32:32,190 --> 00:32:34,560
been going on for a long time for what

00:32:33,270 --> 00:32:36,540
it's worth I went to West Point way too

00:32:34,560 --> 00:32:38,250
many years ago and my colleagues who are

00:32:36,540 --> 00:32:40,650
still around that kind of space they

00:32:38,250 --> 00:32:45,000
train their troops using you know large

00:32:40,650 --> 00:32:50,100
VR systems a lot gosh we had to crawl

00:32:45,000 --> 00:32:51,690
through mud but so even though we're

00:32:50,100 --> 00:32:54,270
still now just sort of catching up to

00:32:51,690 --> 00:32:56,130
video the the concern I have is that the

00:32:54,270 --> 00:32:57,270
next generation of learning materials

00:32:56,130 --> 00:32:59,970
that's coming up is gonna be based off

00:32:57,270 --> 00:33:03,090
of AR and a lot more embedded kinds of

00:32:59,970 --> 00:33:04,800
applications like this and the data

00:33:03,090 --> 00:33:05,700
rates on that will be much higher so

00:33:04,800 --> 00:33:07,080
that's what we're trying to go for so

00:33:05,700 --> 00:33:09,000
we're not replacing people we're

00:33:07,080 --> 00:33:11,789
to make this this kind of work much more

00:33:09,000 --> 00:33:13,620
livable and now this is just my opinion

00:33:11,789 --> 00:33:16,080
it doesn't reflect the opinions of my

00:33:13,620 --> 00:33:19,110
employer but the opinions of my employer

00:33:16,080 --> 00:33:21,210
are that we'll never run out jobs that's

00:33:19,110 --> 00:33:22,380
a great video of this but the idea is

00:33:21,210 --> 00:33:24,179
that we're not going to run out jobs

00:33:22,380 --> 00:33:25,620
until we run out of problems and so we

00:33:24,179 --> 00:33:27,720
don't really look at AI as replacing

00:33:25,620 --> 00:33:30,600
people and in fact in some ways it may

00:33:27,720 --> 00:33:33,419
actually be causing more jobs for us so

00:33:30,600 --> 00:33:35,970
a lot of shout outs to folks who have

00:33:33,419 --> 00:33:37,470
helped out on this and if you want it

00:33:35,970 --> 00:33:39,750
we've got a lot more material about this

00:33:37,470 --> 00:33:42,149
at strata and AI conference and if you

00:33:39,750 --> 00:33:43,500
want to get a hold of me on Twitter is

00:33:42,149 --> 00:33:45,809
usually a pretty good place to catch me

00:33:43,500 --> 00:33:47,159
and I'll you know I think I'm running

00:33:45,809 --> 00:33:49,310
out of time so I'll catch some questions

00:33:47,159 --> 00:33:56,210
in the back but thank you very much

00:33:49,310 --> 00:33:58,590
[Applause]

00:33:56,210 --> 00:34:02,779
already I we have a few minutes to do

00:33:58,590 --> 00:34:02,779
some questions okay great any questions

00:34:09,220 --> 00:34:20,570
huh ya know it's a great question I mean

00:34:17,350 --> 00:34:23,179
from our perspective were continually

00:34:20,570 --> 00:34:24,679
seeing technologies or other things we

00:34:23,179 --> 00:34:26,210
work in design we work in business we

00:34:24,679 --> 00:34:28,190
work in DevOps we work in machine

00:34:26,210 --> 00:34:31,369
learning and we're continuing seeing

00:34:28,190 --> 00:34:35,480
technology stacks evolve and learning

00:34:31,369 --> 00:34:39,080
curves accelerate and new experts emerge

00:34:35,480 --> 00:34:41,419
and I mean a lot of it is somebody has a

00:34:39,080 --> 00:34:43,429
hard problem and they work very hard to

00:34:41,419 --> 00:34:45,379
articulate a solution and they keep

00:34:43,429 --> 00:34:46,909
going after it but then they also were

00:34:45,379 --> 00:34:48,830
able to show enough leadership to get

00:34:46,909 --> 00:34:51,260
other people involved so I think that

00:34:48,830 --> 00:34:53,000
the answer to that goes back to what Wes

00:34:51,260 --> 00:34:54,889
McKinney gave in the keynote this

00:34:53,000 --> 00:34:56,060
morning and also what Peter Wang was

00:34:54,889 --> 00:34:58,790
talking about in the key their

00:34:56,060 --> 00:35:00,260
respective keynotes about just putting

00:34:58,790 --> 00:35:02,180
the code on github doesn't mean you have

00:35:00,260 --> 00:35:03,820
a successful open source project it's

00:35:02,180 --> 00:35:05,960
more about building a community and

00:35:03,820 --> 00:35:08,360
getting involved in that community and

00:35:05,960 --> 00:35:11,060
so that's a lot of where we see the

00:35:08,360 --> 00:35:12,560
experts emerging is by being engaged in

00:35:11,060 --> 00:35:15,560
these communities to solve problems that

00:35:12,560 --> 00:35:17,420
a lot of other people have it could be

00:35:15,560 --> 00:35:19,070
just that you're in a company and you

00:35:17,420 --> 00:35:20,840
have to solve a problem and it's a hard

00:35:19,070 --> 00:35:22,250
problem and you do it right you get

00:35:20,840 --> 00:35:25,430
lucky and then other people start using

00:35:22,250 --> 00:35:27,109
it but I think it's more often it's more

00:35:25,430 --> 00:35:28,910
about the community aspects the

00:35:27,109 --> 00:35:35,000
leadership the socialization on the

00:35:28,910 --> 00:35:37,070
problem does that help answer or you do

00:35:35,000 --> 00:35:39,890
a PhD on it and nobody else does and

00:35:37,070 --> 00:35:41,270
like look be a Walt you do that and then

00:35:39,890 --> 00:35:46,210
you launch company and then 10 years

00:35:41,270 --> 00:35:46,210
later become successful okay

00:35:52,080 --> 00:35:57,790
excellent question so I definitely lived

00:35:54,760 --> 00:35:59,530
through 2005 and and what happened with

00:35:57,790 --> 00:36:01,390
Semantic Web and there are a lot of

00:35:59,530 --> 00:36:03,280
great things back there and the lesson

00:36:01,390 --> 00:36:05,890
that I learned after watching radar

00:36:03,280 --> 00:36:07,870
networks and others the short answer is

00:36:05,890 --> 00:36:10,840
we don't touch sparkle with a 10-foot

00:36:07,870 --> 00:36:14,950
Pole and we have absolutely no triple

00:36:10,840 --> 00:36:16,960
stores not to disparage them but you

00:36:14,950 --> 00:36:18,910
know what we've learned we do use our

00:36:16,960 --> 00:36:22,240
representation is based off of owl

00:36:18,910 --> 00:36:25,150
because we use scause as the simple

00:36:22,240 --> 00:36:26,470
knowledge ontology we use scause as kind

00:36:25,150 --> 00:36:28,210
of the backbone and that ties in the

00:36:26,470 --> 00:36:29,650
Library of Congress and of course it

00:36:28,210 --> 00:36:33,340
ties into dbpedia and others that we

00:36:29,650 --> 00:36:37,690
need so al and scholars very handy for

00:36:33,340 --> 00:36:40,330
representation we're using n 3 turtle so

00:36:37,690 --> 00:36:44,080
TTL as a representation and then in

00:36:40,330 --> 00:36:46,330
Python we use RDF Lib and so that really

00:36:44,080 --> 00:36:47,620
buys us a lot by using RDF Lib we can do

00:36:46,330 --> 00:36:54,760
a lot of manipulation we can do

00:36:47,620 --> 00:36:57,070
validation etc partum well you know the

00:36:54,760 --> 00:36:59,830
curated part is on the order of hundreds

00:36:57,070 --> 00:37:04,150
of rules not hundreds of triples but

00:36:59,830 --> 00:37:07,960
hundreds of rules or hundreds of subject

00:37:04,150 --> 00:37:09,310
object nouns but then the thing the

00:37:07,960 --> 00:37:13,810
trick that we do is usually using

00:37:09,310 --> 00:37:16,180
Network X so we can take our our main

00:37:13,810 --> 00:37:18,790
ontology and then ratify it add in other

00:37:16,180 --> 00:37:20,500
parts but even then we're still working

00:37:18,790 --> 00:37:22,480
with a graph that we can run in memory

00:37:20,500 --> 00:37:23,770
with Network X so that's what I would

00:37:22,480 --> 00:37:25,510
recommend is like don't go to a triple

00:37:23,770 --> 00:37:28,210
store just use Network X if you can get

00:37:25,510 --> 00:37:32,020
away with it I used to work on spark and

00:37:28,210 --> 00:37:34,480
graphics and I'm a big fan of Titan and

00:37:32,020 --> 00:37:37,600
others so there there's a lot of great

00:37:34,480 --> 00:37:40,720
crap databases these days other

00:37:37,600 --> 00:37:41,650
questions if not I'll be around and I

00:37:40,720 --> 00:37:43,900
should probably make room for the next

00:37:41,650 --> 00:37:44,500
person thank you very much great Thank

00:37:43,900 --> 00:37:48,650
You Paco

00:37:44,500 --> 00:37:48,650

YouTube URL: https://www.youtube.com/watch?v=4ij0jIC_E7U


