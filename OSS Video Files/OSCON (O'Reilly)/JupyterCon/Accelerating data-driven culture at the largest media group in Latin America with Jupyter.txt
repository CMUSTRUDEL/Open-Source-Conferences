Title: Accelerating data-driven culture at the largest media group in Latin America with Jupyter
Publication date: 2017-09-22
Playlist: JupyterCon
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:08,030 --> 00:01:14,689
so basically we will burn down

00:01:11,159 --> 00:01:17,969
transformers Internet by a crafting

00:01:14,689 --> 00:01:20,450
digital bro that sells an operation for

00:01:17,969 --> 00:01:23,159
the what is being able to much damage as

00:01:20,450 --> 00:01:27,960
part of a big group of companies that's

00:01:23,159 --> 00:01:31,740
called a bunch of company and our

00:01:27,960 --> 00:01:34,259
missions to the river there's a lot of

00:01:31,740 --> 00:01:36,420
content for users to agree with

00:01:34,259 --> 00:01:41,189
interview contents and entertainment for

00:01:36,420 --> 00:01:50,609
our audience yes we are a person company

00:01:41,189 --> 00:01:54,329
already Knox is usually some interesting

00:01:50,609 --> 00:01:56,820
numbers to have an idea we have more

00:01:54,329 --> 00:02:01,979
than in seventh medium users that's a

00:01:56,820 --> 00:02:04,940
great big yes we reach 78% of brazuca

00:02:01,979 --> 00:02:10,619
net that means that person that

00:02:04,940 --> 00:02:13,200
we reach 70% of them we are leaders in

00:02:10,619 --> 00:02:16,380
video Sports used an entertainment

00:02:13,200 --> 00:02:18,570
Brazil so brings innovations and

00:02:16,380 --> 00:02:23,460
interesting products for our audience

00:02:18,570 --> 00:02:25,500
our customers and our services yes look

00:02:23,460 --> 00:02:30,990
up called 70 years of remarkable

00:02:25,500 --> 00:02:33,660
achievements the first like a great

00:02:30,990 --> 00:02:35,820
product we have is very young that's a

00:02:33,660 --> 00:02:37,650
Brazilian artist reporters that deliver

00:02:35,820 --> 00:02:40,080
a breaking news

00:02:37,650 --> 00:02:42,060
welcome to use vertical content and TV

00:02:40,080 --> 00:02:48,120
shows

00:02:42,060 --> 00:02:50,790
Jim was launched in 2006 and 2008 resume

00:02:48,120 --> 00:02:55,560
loose women speaking to media TV stars

00:02:50,790 --> 00:02:58,920
and thousands per day and human is to

00:02:55,560 --> 00:03:02,580
timeline and run around the coverage of

00:02:58,920 --> 00:03:05,190
you is impressive we cover our diversity

00:03:02,580 --> 00:03:10,080
States with speed 17 schools and more

00:03:05,190 --> 00:03:14,180
than 5,000 sees another great product

00:03:10,080 --> 00:03:14,180
for that we have this bluish watching

00:03:14,330 --> 00:03:21,239
since 2005 is the big sport water Brazil

00:03:17,959 --> 00:03:25,080
that deliver sport news life Cobra

00:03:21,239 --> 00:03:28,730
bubbles advance scouts statistics and a

00:03:25,080 --> 00:03:28,730
lot of important point is that we are

00:03:32,570 --> 00:03:37,830
another great product we have is the

00:03:35,550 --> 00:03:41,280
capital might see is the biggest fattest

00:03:37,830 --> 00:03:43,880
game in Brazil we have more than 11

00:03:41,280 --> 00:03:50,100
millions of teens and one of more than

00:03:43,880 --> 00:03:52,410
300,000 subscribers what I love the

00:03:50,100 --> 00:03:53,940
great products we have as you show our

00:03:52,410 --> 00:03:56,970
body entertainment you can find there

00:03:53,940 --> 00:03:58,620
like TV shows behind sense fashion under

00:03:56,970 --> 00:04:01,500
design receives and a bigger bird the

00:03:58,620 --> 00:04:03,900
birds you and yes big brother Brazil is

00:04:01,500 --> 00:04:07,290
an internet phenomenon Brazil it's great

00:04:03,900 --> 00:04:10,170
impact with more than 100 4 9 mil

00:04:07,290 --> 00:04:14,130
numbers of votes in the sivappu when the

00:04:10,170 --> 00:04:16,440
Miller pick votes it per minutes but to

00:04:14,130 --> 00:04:18,510
support this great products we have

00:04:16,440 --> 00:04:22,200
there we have

00:04:18,510 --> 00:04:26,910
internal solution to keep in track user

00:04:22,200 --> 00:04:29,370
event the we create our event tracker we

00:04:26,910 --> 00:04:31,530
name it at horizon the best edges to

00:04:29,370 --> 00:04:34,410
care to capturing user data like page

00:04:31,530 --> 00:04:38,070
visitors watch it watching time video

00:04:34,410 --> 00:04:39,840
sharing and use comments on news time

00:04:38,070 --> 00:04:43,320
we're in the news and scope and see

00:04:39,840 --> 00:04:46,290
stage etc the best kgs is to get

00:04:43,320 --> 00:04:48,330
information as fast as possible make

00:04:46,290 --> 00:04:51,900
some simple transformation and push down

00:04:48,330 --> 00:04:55,920
to to do the Kefka cluster behind the

00:04:51,900 --> 00:04:58,860
kafka we have a spark streaming ETL that

00:04:55,920 --> 00:05:01,290
process the whole tech africa talks and

00:04:58,860 --> 00:05:06,300
star in a porky formatting our HFS

00:05:01,290 --> 00:05:11,010
cluster yes lots of data lots of

00:05:06,300 --> 00:05:13,830
challenges and these letters to taking

00:05:11,010 --> 00:05:16,920
hard decisions and hard decisions for us

00:05:13,830 --> 00:05:19,290
is based on data driven our approach is

00:05:16,920 --> 00:05:21,990
data driven development or we make our

00:05:19,290 --> 00:05:24,450
decisions based on world what really

00:05:21,990 --> 00:05:28,200
worked for the real people it's but most

00:05:24,450 --> 00:05:32,880
important for us we are doing many baby

00:05:28,200 --> 00:05:34,890
tasks for each interface changes bhutan

00:05:32,880 --> 00:05:39,480
in a color in a boot on this kind of

00:05:34,890 --> 00:05:43,170
stats as a consequence of a data-driven

00:05:39,480 --> 00:05:46,020
process is we have a scientific bias

00:05:43,170 --> 00:05:48,960
with a fast qualitative cycles that

00:05:46,020 --> 00:05:51,960
giving a hypothesis the trying

00:05:48,960 --> 00:05:54,180
experience and doing analysis learn

00:05:51,960 --> 00:05:56,790
about a cycle and start again see if his

00:05:54,180 --> 00:05:59,880
needs but one thinkers try to do faster

00:05:56,790 --> 00:06:02,520
is to deploying production as possible

00:05:59,880 --> 00:06:05,700
that's the better way to know what it

00:06:02,520 --> 00:06:08,910
really works for our our audience is to

00:06:05,700 --> 00:06:12,150
put in the products our ideas so using

00:06:08,910 --> 00:06:15,570
jupiter with our big data resource and

00:06:12,150 --> 00:06:18,660
lucky for KPIs we can evaluate our user

00:06:15,570 --> 00:06:22,260
experience and a design experience like

00:06:18,660 --> 00:06:25,530
a scientist yes of course

00:06:22,260 --> 00:06:27,960
Jupiter is play an important role in the

00:06:25,530 --> 00:06:29,490
bigger driven decisions in fact new

00:06:27,960 --> 00:06:31,910
jupiter knowledge book are the heart of

00:06:29,490 --> 00:06:33,890
our data analysis process

00:06:31,910 --> 00:06:37,000
that book actually you are do labra

00:06:33,890 --> 00:06:39,800
surfs like performing bit test analysis

00:06:37,000 --> 00:06:44,000
machine learning experience performance

00:06:39,800 --> 00:06:46,570
evaluation of algorithms systems and yes

00:06:44,000 --> 00:06:48,440
we have a team focus on door

00:06:46,570 --> 00:06:50,270
personalization that involves

00:06:48,440 --> 00:06:52,130
implementing machine learning algorithm

00:06:50,270 --> 00:06:53,900
to sister recommendation and all that

00:06:52,130 --> 00:06:56,270
seen as a data scientist that evaluator

00:06:53,900 --> 00:06:58,760
performance of the product growth

00:06:56,270 --> 00:07:02,050
hacking team that is doing as much as

00:06:58,760 --> 00:07:05,900
possible solutions for making production

00:07:02,050 --> 00:07:09,340
but why did you choose Jupiter why did

00:07:05,900 --> 00:07:13,490
you choose this - for our data driven

00:07:09,340 --> 00:07:17,840
support now Munnar ooh you joking are we

00:07:13,490 --> 00:07:22,880
explain a little bit more about oh sorry

00:07:17,840 --> 00:07:25,850
sorry yes before manaro we have some

00:07:22,880 --> 00:07:28,100
leads inside globe.com that led us for

00:07:25,850 --> 00:07:31,610
true Jupiter the first one is the

00:07:28,100 --> 00:07:33,140
scalability we have to as a consequence

00:07:31,610 --> 00:07:35,690
of the data driven decision more more

00:07:33,140 --> 00:07:39,800
users data scientists are interesting in

00:07:35,690 --> 00:07:42,320
making data analysis so we need to scale

00:07:39,800 --> 00:07:45,730
this process for hundreds of data

00:07:42,320 --> 00:07:48,470
science aside our company another

00:07:45,730 --> 00:07:50,900
challenge for us is annals of large and

00:07:48,470 --> 00:07:53,360
complex data sets we have lots of

00:07:50,900 --> 00:07:55,880
different data sets from different fonts

00:07:53,360 --> 00:08:01,190
different sources so it's very complex

00:07:55,880 --> 00:08:05,810
to deal with yet another challenge for

00:08:01,190 --> 00:08:08,960
us that is we have people with different

00:08:05,810 --> 00:08:11,530
backgrounds inside our company different

00:08:08,960 --> 00:08:16,220
tastes different feelings you have

00:08:11,530 --> 00:08:18,800
people that prefer loves are but hate

00:08:16,220 --> 00:08:22,100
Python but on the other hand we have

00:08:18,800 --> 00:08:25,040
people that loves Python but hate our

00:08:22,100 --> 00:08:28,700
and they also have a third group that

00:08:25,040 --> 00:08:31,760
love both so it's a complex scenarios

00:08:28,700 --> 00:08:34,760
that we need to deliver yes he's a great

00:08:31,760 --> 00:08:37,930
challenge for us a great needs another

00:08:34,760 --> 00:08:40,610
great point is the needs of to make a

00:08:37,930 --> 00:08:43,550
collaborative work you're sharing

00:08:40,610 --> 00:08:45,529
knowledge and reproducible studies so

00:08:43,550 --> 00:08:48,110
doing this reason we

00:08:45,529 --> 00:08:49,730
these letters for Jupiter that is in the

00:08:48,110 --> 00:08:52,879
it's naturally is to work

00:08:49,730 --> 00:08:57,019
collaboratively another great point is

00:08:52,879 --> 00:09:00,709
security as we have a bunch of data sets

00:08:57,019 --> 00:09:03,529
different data sources and we have a

00:09:00,709 --> 00:09:07,490
sensitive data like user information

00:09:03,529 --> 00:09:09,319
email purchased so and so on so

00:09:07,490 --> 00:09:13,009
basically we need to take a control of

00:09:09,319 --> 00:09:14,990
the security in our ecosystem so this is

00:09:13,009 --> 00:09:17,329
basically their needs the main reads

00:09:14,990 --> 00:09:21,949
that guide us lead us to the shooter

00:09:17,329 --> 00:09:24,709
solution but how do match this they

00:09:21,949 --> 00:09:27,319
needs with shooter now yes

00:09:24,709 --> 00:09:29,689
Bernardo you explain how we met the

00:09:27,319 --> 00:09:36,980
needs with Jupiter there's light similar

00:09:29,689 --> 00:09:40,879
so sorry how we met these needs so we

00:09:36,980 --> 00:09:44,899
will talk about that point that Philippe

00:09:40,879 --> 00:09:47,449
talk so the first one is call ability

00:09:44,899 --> 00:09:51,139
for hundreds of data scientists I don't

00:09:47,449 --> 00:09:54,230
know no the phones yeah hundreds of data

00:09:51,139 --> 00:09:58,610
science so we have a lot of data science

00:09:54,230 --> 00:10:00,949
in the company and they can't use it dr.

00:09:58,610 --> 00:10:04,639
herb only on a single machine it's

00:10:00,949 --> 00:10:08,000
impossible to scale for or of of the or

00:10:04,639 --> 00:10:13,879
of the the data science so we found that

00:10:08,000 --> 00:10:20,029
Jupiter has spawners there to work with

00:10:13,879 --> 00:10:24,529
doctors farm dr. Swan enabled us to take

00:10:20,029 --> 00:10:28,309
a darker for it user so a user have your

00:10:24,529 --> 00:10:35,870
own machine and can make her analysis

00:10:28,309 --> 00:10:38,420
with your CPU and your memory another

00:10:35,870 --> 00:10:42,199
point is that we have a large amount

00:10:38,420 --> 00:10:48,069
amount of data so when a data sense came

00:10:42,199 --> 00:10:51,529
from came to our company it get lost so

00:10:48,069 --> 00:10:55,600
what about these we have billions of

00:10:51,529 --> 00:10:58,990
daily events from users millions of

00:10:55,600 --> 00:10:58,990
concurrent connections

00:10:59,540 --> 00:11:06,250
millions after unique users per month

00:11:02,410 --> 00:11:09,920
and thousands hundreds of thousands of

00:11:06,250 --> 00:11:12,620
contents created by by month so there

00:11:09,920 --> 00:11:17,500
are a lot of content create hearing how

00:11:12,620 --> 00:11:21,560
how can I analyze all that data and how

00:11:17,500 --> 00:11:26,050
how can I was faced with a fast approach

00:11:21,560 --> 00:11:33,440
or all of the data so we found this Park

00:11:26,050 --> 00:11:38,839
who hears no sparks no okay great great

00:11:33,440 --> 00:11:42,199
it's a it's a solution for better I can

00:11:38,839 --> 00:11:46,550
I can I can use the resources within a

00:11:42,199 --> 00:11:50,720
better way with parallel runs so it's

00:11:46,550 --> 00:11:56,540
great because Sparkle loves vitamin and

00:11:50,720 --> 00:12:01,190
are so Jupiter loves it too in the it's

00:11:56,540 --> 00:12:04,850
it's times today it's it's great for the

00:12:01,190 --> 00:12:07,130
the there are another point that it

00:12:04,850 --> 00:12:13,690
should work with for in Python and and

00:12:07,130 --> 00:12:18,560
both so Jupiter and spark so fits all

00:12:13,690 --> 00:12:25,630
our needs for for both but there is a

00:12:18,560 --> 00:12:31,430
point I have different kernels will need

00:12:25,630 --> 00:12:34,940
fight on two or three with spark to or

00:12:31,430 --> 00:12:38,209
spark wow it's it's very hard to an R

00:12:34,940 --> 00:12:41,680
with spark to or spark well it's very

00:12:38,209 --> 00:12:45,860
hard so the data science understand what

00:12:41,680 --> 00:12:50,720
it's doing to do when it's come from

00:12:45,860 --> 00:12:56,029
from our company so we made some some

00:12:50,720 --> 00:13:00,139
changes would we prefer to take only

00:12:56,029 --> 00:13:03,199
three kernels but how can customize all

00:13:00,139 --> 00:13:05,540
this all trick with only we found three

00:13:03,199 --> 00:13:12,380
careness four different versions of

00:13:05,540 --> 00:13:12,950
spark so we created a internal library

00:13:12,380 --> 00:13:17,210
called

00:13:12,950 --> 00:13:20,990
blob D s and this library is helping a

00:13:17,210 --> 00:13:25,280
lot your users to to make an analysis

00:13:20,990 --> 00:13:28,550
and manage the spark I will show a

00:13:25,280 --> 00:13:32,840
little bit how how users can manage the

00:13:28,550 --> 00:13:35,900
spark with this library and how we it's

00:13:32,840 --> 00:13:39,590
helping our data science analysis at all

00:13:35,900 --> 00:13:42,530
the good point that we did that library

00:13:39,590 --> 00:13:48,740
for R and for Python because we need to

00:13:42,530 --> 00:13:50,690
make all those happiness it's one

00:13:48,740 --> 00:13:54,190
comment on that and at that point that

00:13:50,690 --> 00:13:57,500
the globe ODS has two main functions

00:13:54,190 --> 00:14:01,850
first wants to simplify the data

00:13:57,500 --> 00:14:04,370
analysis process that brings some common

00:14:01,850 --> 00:14:07,040
functions for users another great and

00:14:04,370 --> 00:14:11,450
other functions of the this lip-lip is

00:14:07,040 --> 00:14:14,150
to avoid me mix miss miss configuration

00:14:11,450 --> 00:14:16,220
and Achon as we have different data

00:14:14,150 --> 00:14:18,650
source so the data imagine data

00:14:16,220 --> 00:14:20,990
scientist needs to know what the exactly

00:14:18,650 --> 00:14:23,000
part of a DFS service or the

00:14:20,990 --> 00:14:26,510
neighborhood services or HBase services

00:14:23,000 --> 00:14:29,330
or or or even database so it's very very

00:14:26,510 --> 00:14:31,340
complicated so the our a function of the

00:14:29,330 --> 00:14:33,680
global yes is to simplify this process

00:14:31,340 --> 00:14:40,370
so abstracting this configuration from

00:14:33,680 --> 00:14:43,190
from data sizes so let's talk about this

00:14:40,370 --> 00:14:45,860
library there are two main models one

00:14:43,190 --> 00:14:48,160
model for spark management and another

00:14:45,860 --> 00:14:53,350
model for data sensor chills

00:14:48,160 --> 00:14:57,380
so the first how hard I work the first

00:14:53,350 --> 00:15:01,160
model we we have three functions in this

00:14:57,380 --> 00:15:04,360
model it's basically start spark and you

00:15:01,160 --> 00:15:06,500
can choose if you will start this spark

00:15:04,360 --> 00:15:09,350
loco or

00:15:06,500 --> 00:15:12,560
you can who run the spark inside the

00:15:09,350 --> 00:15:17,450
docker or can start spark on the cluster

00:15:12,560 --> 00:15:20,750
we have a Hadoop cluster and the data

00:15:17,450 --> 00:15:25,550
science can submit jobs for that cluster

00:15:20,750 --> 00:15:30,290
so we when you need a harder

00:15:25,550 --> 00:15:34,399
analysis with a lot of data and so it's

00:15:30,290 --> 00:15:36,800
a bit slow to run locally you will we

00:15:34,399 --> 00:15:40,760
can change choo-choo cluster and with

00:15:36,800 --> 00:15:46,459
this function you can choice what spark

00:15:40,760 --> 00:15:48,860
version you use so you go to the kernel

00:15:46,459 --> 00:15:51,950
when you go to the kernel just a fight

00:15:48,860 --> 00:15:58,310
on or our kernel and when you use start

00:15:51,950 --> 00:16:03,589
this kernel comes to be a spark Sparky

00:15:58,310 --> 00:16:05,740
version or two like so another another

00:16:03,589 --> 00:16:10,040
function stop that stops park station

00:16:05,740 --> 00:16:14,000
stop spark just stops peripheral lock or

00:16:10,040 --> 00:16:16,040
cluster just you start the the session

00:16:14,000 --> 00:16:19,130
in the context of the spark so your

00:16:16,040 --> 00:16:21,589
notebook comes to be a simple fighting

00:16:19,130 --> 00:16:24,079
or our notebook again and you can submit

00:16:21,589 --> 00:16:27,700
jobs like this you can start using

00:16:24,079 --> 00:16:31,010
closer and then stop and start to local

00:16:27,700 --> 00:16:34,190
again one commits on again with this

00:16:31,010 --> 00:16:37,579
point that we have different approaches

00:16:34,190 --> 00:16:39,740
in our data science process we have a

00:16:37,579 --> 00:16:41,480
statistical approach that we are

00:16:39,740 --> 00:16:45,410
sampling data this there's something

00:16:41,480 --> 00:16:47,779
data in sometimes can be heavy and we so

00:16:45,410 --> 00:16:49,910
our device for the idea scientist is who

00:16:47,779 --> 00:16:51,860
please go through the our cluster and

00:16:49,910 --> 00:16:55,910
get the amount of the they need is that

00:16:51,860 --> 00:16:58,550
resource that we you need and play yes

00:16:55,910 --> 00:17:03,620
try yourself it's better that then that

00:16:58,550 --> 00:17:05,270
then run in a in a Dockyard sorry so

00:17:03,620 --> 00:17:08,630
this is our approach for that we

00:17:05,270 --> 00:17:11,209
implemented this function okay and the

00:17:08,630 --> 00:17:14,510
last function was mandatory because we

00:17:11,209 --> 00:17:19,160
don't know how much how much how much

00:17:14,510 --> 00:17:25,220
how many times did is this job you will

00:17:19,160 --> 00:17:28,790
spend so in it should to know when we

00:17:25,220 --> 00:17:32,000
monitoring we we will send you a link

00:17:28,790 --> 00:17:35,630
and you can see sparking you I and see

00:17:32,000 --> 00:17:39,020
the process the number of executors and

00:17:35,630 --> 00:17:42,650
how it's running so you can control your

00:17:39,020 --> 00:17:45,500
your submission of jobs and and even

00:17:42,650 --> 00:17:47,870
local jobs the basically idea is to

00:17:45,500 --> 00:17:52,070
empower the our data science to keep it

00:17:47,870 --> 00:17:57,710
track or they're in their spark jobs so

00:17:52,070 --> 00:18:02,059
they can the they can track they through

00:17:57,710 --> 00:18:05,600
the spark UI we just just made a simple

00:18:02,059 --> 00:18:08,510
function that monitoring we give a spot

00:18:05,600 --> 00:18:14,480
where all UI so the the data size can

00:18:08,510 --> 00:18:19,520
keep track of that job okay and the

00:18:14,480 --> 00:18:24,830
other model was for giving a pattern for

00:18:19,520 --> 00:18:28,970
our data science so when you need so how

00:18:24,830 --> 00:18:32,900
could you with call all the teams to

00:18:28,970 --> 00:18:34,940
make a pattern it's very hard so when we

00:18:32,900 --> 00:18:38,570
do some functions like remove outliers

00:18:34,940 --> 00:18:42,620
that data sunscreen just remove outliers

00:18:38,570 --> 00:18:46,760
or data frame and data frame in and get

00:18:42,620 --> 00:18:49,220
back in that a frame dot outliers so we

00:18:46,760 --> 00:18:53,120
implemented these functions in our in

00:18:49,220 --> 00:19:00,350
Python there are a lot of functions for

00:18:53,120 --> 00:19:03,050
remove outliers do this simple but other

00:19:00,350 --> 00:19:04,840
functions are plotted graphs you have a

00:19:03,050 --> 00:19:07,760
lot of functions for plotting graphs

00:19:04,840 --> 00:19:13,070
simplify data science data format

00:19:07,760 --> 00:19:16,420
timestamp to this time and it's very

00:19:13,070 --> 00:19:19,550
hard to do sometimes and the first type

00:19:16,420 --> 00:19:23,929
device type the branch and functions

00:19:19,550 --> 00:19:27,380
that guess what's the device of the user

00:19:23,929 --> 00:19:30,260
based on user agent because I have a lot

00:19:27,380 --> 00:19:34,760
of different user agents in the need to

00:19:30,260 --> 00:19:39,020
know if it's mobile or desktop so we try

00:19:34,760 --> 00:19:42,470
to simplify that swings safe Perkins CSV

00:19:39,020 --> 00:19:45,830
files in it HDFS because sometimes data

00:19:42,470 --> 00:19:50,360
size need to stop your work on some

00:19:45,830 --> 00:19:52,790
point and can save with that that fire

00:19:50,360 --> 00:19:56,740
was per K or CSV

00:19:52,790 --> 00:20:03,860
in stores that's applicated and the

00:19:56,740 --> 00:20:06,140
never never lost your data so this

00:20:03,860 --> 00:20:10,280
functions helped when data science wants

00:20:06,140 --> 00:20:13,120
to gather up get it get a kami again and

00:20:10,280 --> 00:20:13,120
and work

00:20:13,360 --> 00:20:21,380
ok data sense can have your own

00:20:16,220 --> 00:20:24,200
environments can make your analysis but

00:20:21,380 --> 00:20:27,950
they need to stop package to because the

00:20:24,200 --> 00:20:33,530
globe of the acid can't be a bigger

00:20:27,950 --> 00:20:35,780
package that no we don't want it so we

00:20:33,530 --> 00:20:39,760
can't do that

00:20:35,780 --> 00:20:44,000
ok you can't provide the ability for

00:20:39,760 --> 00:20:47,330
data science to install packages by your

00:20:44,000 --> 00:20:51,200
own because it's they sometimes don't

00:20:47,330 --> 00:20:55,340
know if you defected malicious or have

00:20:51,200 --> 00:21:00,490
any security problem so what we did

00:20:55,340 --> 00:21:03,860
there is a form that the data science

00:21:00,490 --> 00:21:06,950
will put there your your the name of the

00:21:03,860 --> 00:21:11,000
package the version of the package if

00:21:06,950 --> 00:21:15,200
the package don't beats and it should

00:21:11,000 --> 00:21:17,300
install that package inside the user

00:21:15,200 --> 00:21:22,040
docker and only they use the docker and

00:21:17,300 --> 00:21:25,340
you can change that version so I can I'm

00:21:22,040 --> 00:21:27,710
sorry I can't work with a version of

00:21:25,340 --> 00:21:30,520
much what Lee even another user using

00:21:27,710 --> 00:21:33,500
another version without without problems

00:21:30,520 --> 00:21:38,660
this this stock this is taller works

00:21:33,500 --> 00:21:41,030
with Python and our and we I will we are

00:21:38,660 --> 00:21:44,870
using a whitelist for this package of

00:21:41,030 --> 00:21:47,780
course for malicious packages and etc

00:21:44,870 --> 00:21:50,690
and install all this package from our

00:21:47,780 --> 00:21:54,230
internal repository called your axe

00:21:50,690 --> 00:21:58,280
factory as factors up the main product

00:21:54,230 --> 00:22:02,780
that you can buy and use its clones the

00:21:58,280 --> 00:22:05,600
are our hippest our helpers and fighting

00:22:02,780 --> 00:22:06,350
we're helpers and you can use it inside

00:22:05,600 --> 00:22:08,120
your own

00:22:06,350 --> 00:22:12,440
when you doubt to use the external

00:22:08,120 --> 00:22:16,460
network so in the great of this safe

00:22:12,440 --> 00:22:21,020
spec installation it's because all is

00:22:16,460 --> 00:22:25,640
coverage by oath to Authenticator so all

00:22:21,020 --> 00:22:27,740
the other steps are are passing through

00:22:25,640 --> 00:22:33,429
authenticates authentication and the

00:22:27,740 --> 00:22:36,770
user is is always saved

00:22:33,429 --> 00:22:41,059
so yes the user can stop package now I

00:22:36,770 --> 00:22:43,370
can make our analysis but it's country

00:22:41,059 --> 00:22:48,520
it's very hard to share the data

00:22:43,370 --> 00:22:51,909
analysis how could they share with other

00:22:48,520 --> 00:22:56,289
with other users or with stakeholders

00:22:51,909 --> 00:23:00,440
it's very hard Jupiter gives you give up

00:22:56,289 --> 00:23:05,780
three solutions or even more for share

00:23:00,440 --> 00:23:12,770
using PDF using links or using HTML

00:23:05,780 --> 00:23:17,419
exports but it it's not so good we what

00:23:12,770 --> 00:23:20,630
we do is we use the amount points for

00:23:17,419 --> 00:23:25,970
each docker for each user darker with

00:23:20,630 --> 00:23:30,470
all notebooks so every user can see all

00:23:25,970 --> 00:23:34,520
notebooks in the company it's it's a mod

00:23:30,470 --> 00:23:38,919
point from and for some one point of any

00:23:34,520 --> 00:23:42,950
NFS file system a shared file system but

00:23:38,919 --> 00:23:46,429
if all users can see all notebooks it's

00:23:42,950 --> 00:23:51,470
does it make sense because it's not safe

00:23:46,429 --> 00:23:55,940
so let's talk a little about safe the

00:23:51,470 --> 00:23:59,179
last point of of that global con needs a

00:23:55,940 --> 00:24:03,860
security effect accessing data from

00:23:59,179 --> 00:24:09,640
different sources so we have Google

00:24:03,860 --> 00:24:12,919
bigquery solar my Seco it's busy HDFS

00:24:09,640 --> 00:24:16,010
MongoDB have a lot of sources inside

00:24:12,919 --> 00:24:20,799
global account and how user can access

00:24:16,010 --> 00:24:20,799
all the sources saved

00:24:20,840 --> 00:24:32,580
so we have hooks what we did with the

00:24:28,530 --> 00:24:37,920
file system for Jupiter that for each

00:24:32,580 --> 00:24:41,190
action that user makes its its track

00:24:37,920 --> 00:24:44,580
dissection so when the user loads the

00:24:41,190 --> 00:24:48,360
list of notebooks or when the user loads

00:24:44,580 --> 00:24:51,900
a notebook we get all this information

00:24:48,360 --> 00:24:58,380
and send to gray log gray log is our log

00:24:51,900 --> 00:25:00,990
server it's open source and we are

00:24:58,380 --> 00:25:02,940
tracking all this loads information if

00:25:00,990 --> 00:25:06,090
the user wrong as well

00:25:02,940 --> 00:25:10,260
we send it to gray log you know what

00:25:06,090 --> 00:25:13,140
user run what cell if you user delete a

00:25:10,260 --> 00:25:16,799
notebook we send it to gray log so have

00:25:13,140 --> 00:25:20,429
all the other user tracking here logon

00:25:16,799 --> 00:25:24,270
by the way you are thinking okay I'm

00:25:20,429 --> 00:25:27,809
getting this logged but if the the user

00:25:24,270 --> 00:25:33,299
did the user did so I can I can get back

00:25:27,809 --> 00:25:39,390
the deleted it so we have integrations

00:25:33,299 --> 00:25:41,160
with hook using off to - we have a

00:25:39,390 --> 00:25:43,730
backstage accounts that's off to

00:25:41,160 --> 00:25:47,160
provider of choose

00:25:43,730 --> 00:25:51,929
authentication mechanisms so we meet

00:25:47,160 --> 00:25:54,480
authorization for each request so if the

00:25:51,929 --> 00:25:57,000
user try to run the notebook or if there

00:25:54,480 --> 00:25:59,660
is a try to load the notebook I will

00:25:57,000 --> 00:26:02,910
check with backstage accounts that's or

00:25:59,660 --> 00:26:07,020
most to provide if the user has

00:26:02,910 --> 00:26:09,960
permission to access that resource so

00:26:07,020 --> 00:26:13,410
you have a file system shared do you

00:26:09,960 --> 00:26:17,549
know that I did it's not safe but we

00:26:13,410 --> 00:26:22,070
have some meckel's for controlled attack

00:26:17,549 --> 00:26:22,070
that's a detection that access so

00:26:22,760 --> 00:26:29,880
explain a little bit more this flux when

00:26:26,820 --> 00:26:31,260
user try to access the hub ask you to

00:26:29,880 --> 00:26:34,050
backstitch accounts can access

00:26:31,260 --> 00:26:40,130
authorizes a get

00:26:34,050 --> 00:26:43,670
if no it'sit's block it so this is our

00:26:40,130 --> 00:26:49,740
environment with all the stuffs that I

00:26:43,670 --> 00:26:53,910
talked about here and here with dr. herb

00:26:49,740 --> 00:26:57,480
that spawn a lot of Dockers and undock

00:26:53,910 --> 00:27:01,800
for user and the doctor has dr. if spark

00:26:57,480 --> 00:27:04,110
peyten are sends all user tracking Sugar

00:27:01,800 --> 00:27:07,620
Ray log ends authenticated and

00:27:04,110 --> 00:27:09,780
authorized ated with OAuth 2 or

00:27:07,620 --> 00:27:15,600
backstage accounts of Jamaicans

00:27:09,780 --> 00:27:20,510
and now user can access safely all

00:27:15,600 --> 00:27:25,590
doose's resource and can make your your

00:27:20,510 --> 00:27:31,200
analysis within problem so I want to

00:27:25,590 --> 00:27:33,300
thank you because you are here in and

00:27:31,200 --> 00:27:38,070
thank you for your take on for the

00:27:33,300 --> 00:27:42,260
opportunity and our sponsors if you want

00:27:38,070 --> 00:27:47,700
more no moral want to talk about a

00:27:42,260 --> 00:27:51,180
aquifers we can you can we can get a

00:27:47,700 --> 00:27:53,310
link being or email and we will be able

00:27:51,180 --> 00:27:56,790
to ask your questions yes basically by

00:27:53,310 --> 00:27:59,880
the way we are totally open to working

00:27:56,790 --> 00:28:03,510
in new ideas in open source community so

00:27:59,880 --> 00:28:06,540
so stay in touch with us so let us in

00:28:03,510 --> 00:28:08,940
allocating we can talk about but about

00:28:06,540 --> 00:28:14,160
the innovations so more importance is

00:28:08,940 --> 00:28:18,830
our patients me and non-white so thank

00:28:14,160 --> 00:28:18,830
you and yes we are open for questions

00:28:24,930 --> 00:28:51,990
oh okay good point with backstage

00:28:49,080 --> 00:28:54,930
accounts dexter accounts authorizing

00:28:51,990 --> 00:28:58,890
Authenticator are all the company and

00:28:54,930 --> 00:29:02,760
order the global group so we can just

00:28:58,890 --> 00:29:07,560
change the permission for that notebook

00:29:02,760 --> 00:29:10,410
do you know so we can we make groups of

00:29:07,560 --> 00:29:14,520
faxes so can change that that permission

00:29:10,410 --> 00:29:19,230
by a group of faxes and the sacrament or

00:29:14,520 --> 00:29:22,290
the power of our services my sequel HFS

00:29:19,230 --> 00:29:25,530
are secured by the the backstage account

00:29:22,290 --> 00:29:27,150
so the in the backstage accounts we

00:29:25,530 --> 00:29:29,010
deliver there right permission for the

00:29:27,150 --> 00:29:30,540
users own so there is the place the

00:29:29,010 --> 00:29:32,820
right place wind can change their user

00:29:30,540 --> 00:29:35,340
permissions so the users cannot have

00:29:32,820 --> 00:29:37,050
access on this directory on each a path

00:29:35,340 --> 00:29:39,540
yes Kenny okay it's no problem

00:29:37,050 --> 00:29:41,790
so it's a very target environment I have

00:29:39,540 --> 00:29:44,070
any ApS that integrated for all the

00:29:41,790 --> 00:29:47,030
services and and simplify user

00:29:44,070 --> 00:29:47,030
authentication authorization

00:29:56,270 --> 00:29:59,319
[Music]

00:30:15,530 --> 00:30:25,710
how many data centers we have yes is

00:30:21,180 --> 00:30:29,460
that yes we have spread data centers

00:30:25,710 --> 00:30:32,190
because our privacy policies we can

00:30:29,460 --> 00:30:34,200
speak in the independet data for you but

00:30:32,190 --> 00:30:36,930
we have spread data center but in Brazil

00:30:34,200 --> 00:30:37,920
we have our own data centers our Hadoop

00:30:36,930 --> 00:30:42,320
clusters

00:30:37,920 --> 00:30:42,320
oh sorry data sign sorry letsa buddies

00:30:43,760 --> 00:30:52,800
so thatís itís our solution is that that

00:30:49,830 --> 00:30:57,540
each person said global can be a data

00:30:52,800 --> 00:31:07,880
scientist but in practice we have about

00:30:57,540 --> 00:31:12,510
8 18 persons that is a data scientist no

00:31:07,880 --> 00:31:16,220
the job data science 8 but they're all

00:31:12,510 --> 00:31:17,690
of the the product managers or

00:31:16,220 --> 00:31:20,790
[Music]

00:31:17,690 --> 00:31:26,520
developers can can access and access to

00:31:20,790 --> 00:31:28,530
we have it to hundreds of of data

00:31:26,520 --> 00:31:33,050
Sciences accessing the the the

00:31:28,530 --> 00:31:33,050
environment the last time

00:31:52,400 --> 00:32:04,980
when you get the data and wrong yes from

00:32:02,700 --> 00:32:08,730
different sources basically but the me

00:32:04,980 --> 00:32:11,100
approach is the the the our first advice

00:32:08,730 --> 00:32:13,500
for the our data scientists try in the

00:32:11,100 --> 00:32:19,410
small data so we internally we have

00:32:13,500 --> 00:32:22,440
another two that we can smell now but we

00:32:19,410 --> 00:32:24,930
have a big distraction to that is the

00:32:22,440 --> 00:32:28,080
main function is to makes sampling data

00:32:24,930 --> 00:32:30,690
so our advice for our data science I

00:32:28,080 --> 00:32:33,240
start with a small data so it is

00:32:30,690 --> 00:32:35,510
basically interface to interface to

00:32:33,240 --> 00:32:39,000
submit spark job far out closer and

00:32:35,510 --> 00:32:41,490
after the process finish this data are

00:32:39,000 --> 00:32:43,680
available in our HFS so in the joopa

00:32:41,490 --> 00:32:46,230
notebook the data scientist can access

00:32:43,680 --> 00:32:48,900
access directly our Hadoop cluster and

00:32:46,230 --> 00:32:50,790
yes we can of course they can copy the

00:32:48,900 --> 00:32:51,450
data for the local file systems in the

00:32:50,790 --> 00:32:53,430
the docker

00:32:51,450 --> 00:32:56,460
so in that make some machine learning

00:32:53,430 --> 00:32:58,800
models and other approach is to use a ml

00:32:56,460 --> 00:33:01,410
Lib phone from spark under running

00:32:58,800 --> 00:33:03,630
directing the our cluster on off key or

00:33:01,410 --> 00:33:06,930
even in the East Pakistan alone also use

00:33:03,630 --> 00:33:11,120
and basically this approach are are

00:33:06,930 --> 00:33:15,210
using bigquery we have already did

00:33:11,120 --> 00:33:20,130
clonidine bigquery and it's very faster

00:33:15,210 --> 00:33:22,050
than just the cluster basically we are

00:33:20,130 --> 00:33:25,830
in a mission portent mission now that is

00:33:22,050 --> 00:33:28,740
a to brings the data to show to deliver

00:33:25,830 --> 00:33:32,850
the data for the other global group

00:33:28,740 --> 00:33:35,280
companies like Globo Saatchi like info

00:33:32,850 --> 00:33:37,560
global Global TV is a another separate

00:33:35,280 --> 00:33:39,330
company but the same group so the most

00:33:37,560 --> 00:33:44,160
people paid to that we choose it was a

00:33:39,330 --> 00:33:46,950
big query that we faster and very fast

00:33:44,160 --> 00:33:50,850
that many data science knows a sequel

00:33:46,950 --> 00:33:53,460
can just to make a queries Adar curious

00:33:50,850 --> 00:33:56,370
and distract their data so basically we

00:33:53,460 --> 00:33:59,520
are go go through this way at the moment

00:33:56,370 --> 00:34:03,600
to basically the doctor notebook is an

00:33:59,520 --> 00:34:07,490
interface to submit bigquery queries and

00:34:03,600 --> 00:34:07,490
the doing data size through that process

00:34:26,470 --> 00:34:34,539
yes it had many and I'm so sorry - not

00:34:31,299 --> 00:34:39,429
for too short for you but yes I have

00:34:34,539 --> 00:34:43,270
many means a case who have basically we

00:34:39,429 --> 00:34:45,429
have Youngjae as you show our different

00:34:43,270 --> 00:34:48,220
products are different teams so they

00:34:45,429 --> 00:34:51,970
have different clients from basically we

00:34:48,220 --> 00:34:55,720
are we are a platform for the global TV

00:34:51,970 --> 00:34:58,809
so the Globo TV is our our that play our

00:34:55,720 --> 00:35:03,760
salary so they they they sense their

00:34:58,809 --> 00:35:06,520
demons so they say what you are doing

00:35:03,760 --> 00:35:09,490
wrong so we get this feedback and try to

00:35:06,520 --> 00:35:13,839
analyzing the data and yes we have we

00:35:09,490 --> 00:35:17,579
have a internal lab that we make some

00:35:13,839 --> 00:35:20,890
user experience experiments like try to

00:35:17,579 --> 00:35:25,839
collect to select some persons real

00:35:20,890 --> 00:35:29,309
people persons - just to put in there in

00:35:25,839 --> 00:35:32,740
the home and to see how the dis person

00:35:29,309 --> 00:35:35,140
spell Nava gating in our product so are

00:35:32,740 --> 00:35:37,150
they tricking tour the next day feel

00:35:35,140 --> 00:35:40,539
expressions so this kind of stress we

00:35:37,150 --> 00:35:43,539
doing and we collecting also the data

00:35:40,539 --> 00:35:46,420
with this data we can analyze the

00:35:43,539 --> 00:35:49,299
discomfort this behavior and crossing

00:35:46,420 --> 00:35:53,440
the the different the data sources for

00:35:49,299 --> 00:35:55,359
instance we can evaluate the the owl's

00:35:53,440 --> 00:35:57,339
subscriber hit from a product that's

00:35:55,359 --> 00:35:58,119
based on this data so this kind of

00:35:57,339 --> 00:35:59,770
service is over

00:35:58,119 --> 00:36:04,779
there was a many many many cases

00:35:59,770 --> 00:36:07,710
showcase and have I'm so sorry to to not

00:36:04,779 --> 00:36:13,230
to show it for you yeah katal of see

00:36:07,710 --> 00:36:18,240
that's the game that that flip shammgod

00:36:13,230 --> 00:36:24,339
that philippe shows has a high engaged

00:36:18,240 --> 00:36:29,200
people and but some moments yeah it's a

00:36:24,339 --> 00:36:34,000
it's a soccer fantasy games so when when

00:36:29,200 --> 00:36:37,480
starts when did the game open we never

00:36:34,000 --> 00:36:40,150
have a lot of people accessing in what

00:36:37,480 --> 00:36:44,740
to track that that tax

00:36:40,150 --> 00:36:48,849
we need to to build our track as soon as

00:36:44,740 --> 00:36:51,910
possible and it's done it's motivated to

00:36:48,849 --> 00:36:53,920
create a horizon to get user information

00:36:51,910 --> 00:36:57,000
as soon as possible because Google

00:36:53,920 --> 00:37:01,299
Analytics has a lot of information

00:36:57,000 --> 00:37:04,359
groupid and it's not so it's not so real

00:37:01,299 --> 00:37:06,160
time as we can do with with horizon so

00:37:04,359 --> 00:37:09,299
basically this game is based on

00:37:06,160 --> 00:37:12,910
Brazilian football championship so

00:37:09,299 --> 00:37:18,220
before the each matches the the game

00:37:12,910 --> 00:37:20,950
opens and each player can create their

00:37:18,220 --> 00:37:23,200
change but then we've calculated that's

00:37:20,950 --> 00:37:28,059
a cache that today each user has they

00:37:23,200 --> 00:37:30,490
can buy players and and the behavior the

00:37:28,059 --> 00:37:32,289
the performance of the the his team's is

00:37:30,490 --> 00:37:34,390
based on the real play the yellow

00:37:32,289 --> 00:37:37,930
matches so it's very interesting

00:37:34,390 --> 00:37:40,059
tracing game so yes as Munnar said we

00:37:37,930 --> 00:37:42,789
have a great big dating fresh search

00:37:40,059 --> 00:37:44,049
infrastructure to collecting this data

00:37:42,789 --> 00:37:47,680
as far as possible

00:37:44,049 --> 00:37:49,569
to taking place in our our our Hadoop

00:37:47,680 --> 00:37:52,539
cluster and processing this data and

00:37:49,569 --> 00:37:57,250
build recommendations and alleges in a

00:37:52,539 --> 00:38:01,869
real-time some also there probably 10

00:37:57,250 --> 00:38:03,039
milliseconds that data spent through at

00:38:01,869 --> 00:38:05,440
the moment that users make this

00:38:03,039 --> 00:38:07,299
dissection and this data arrives in our

00:38:05,440 --> 00:38:10,599
Hadoop cluster it's very very fast so

00:38:07,299 --> 00:38:17,160
this basically is our our scenarios of

00:38:10,599 --> 00:38:17,160
the big data in structure ok ok

00:38:19,900 --> 00:38:21,960

YouTube URL: https://www.youtube.com/watch?v=oaf4jwHYmHg


