Title: Cloud Datalab: Jupyter with the power of BigQuery and TensorFlow-Kazunori Sato (Google)
Publication date: 2017-09-22
Playlist: JupyterCon
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:02,520 --> 00:00:05,830
thank you for taking a time for my

00:00:05,170 --> 00:00:08,500
session

00:00:05,830 --> 00:00:10,450
it's about Google crowded to love its

00:00:08,500 --> 00:00:12,309
objector running on Google crowd and

00:00:10,450 --> 00:00:15,250
I'll be talking about how you can

00:00:12,309 --> 00:00:18,310
combine bigquery intensive flow by using

00:00:15,250 --> 00:00:21,009
a developer as a hub of those tools and

00:00:18,310 --> 00:00:23,470
cuz i'm a dinner part of it polluted

00:00:21,009 --> 00:00:25,480
crowd especially focusing of fear deter

00:00:23,470 --> 00:00:27,610
and now it exponents like a bigquery

00:00:25,480 --> 00:00:31,689
intensive role I have been working on

00:00:27,610 --> 00:00:33,370
Google over six and half years and for

00:00:31,689 --> 00:00:37,780
us three years I have been working at it

00:00:33,370 --> 00:00:40,360
in a powerful game so what is cloud data

00:00:37,780 --> 00:00:47,470
bob how many people actually try to get

00:00:40,360 --> 00:00:51,100
along it's a shift and not running on

00:00:47,470 --> 00:00:54,100
Google cloud so it's it works as the

00:00:51,100 --> 00:00:56,860
jipner notebooks but it's also provides

00:00:54,100 --> 00:01:00,280
the api's to access fear to the crowd

00:00:56,860 --> 00:01:03,370
services such as the data lab which

00:01:00,280 --> 00:01:05,590
either MapReduce like but processing

00:01:03,370 --> 00:01:08,320
system for us in processing compute

00:01:05,590 --> 00:01:10,920
engine virtual machine but ml ranging is

00:01:08,320 --> 00:01:13,900
a runtime for tensor form crud strange

00:01:10,920 --> 00:01:17,049
bickering bigquery hisses didn't up did

00:01:13,900 --> 00:01:22,030
a warehouse stock drop is modern too so

00:01:17,049 --> 00:01:24,159
you can access those crowd are tools as

00:01:22,030 --> 00:01:26,380
well as the the usual two such as chance

00:01:24,159 --> 00:01:29,680
of warn umpires I cannot not but what

00:01:26,380 --> 00:01:31,630
with all combined into in only the

00:01:29,680 --> 00:01:35,590
single chip tunnel could be environment

00:01:31,630 --> 00:01:38,740
of rocket around and so it's easy to get

00:01:35,590 --> 00:01:41,680
started so you can just go to the cloud

00:01:38,740 --> 00:01:45,189
or Google calm so a state a lab where

00:01:41,680 --> 00:01:48,090
you have all the materials to get

00:01:45,189 --> 00:01:52,530
started first you have to install the

00:01:48,090 --> 00:01:56,049
Google Cloud SDK on your laptop and then

00:01:52,530 --> 00:01:58,150
execute a data lab create common that

00:01:56,049 --> 00:02:00,490
will create an data lab instance or

00:01:58,150 --> 00:02:02,080
crowd actually that is a virtual machine

00:02:00,490 --> 00:02:05,920
Google compute engine virtual machines

00:02:02,080 --> 00:02:07,990
running on cloud and then the cloud data

00:02:05,920 --> 00:02:10,209
lab command will establish a SSH

00:02:07,990 --> 00:02:13,599
connection between the virtual machine

00:02:10,209 --> 00:02:14,920
and your laptop so that you can open

00:02:13,599 --> 00:02:17,830
your browser and

00:02:14,920 --> 00:02:20,860
yesterday localhost:8080 to open the

00:02:17,830 --> 00:02:22,450
knot hooks living here laughter but new

00:02:20,860 --> 00:02:27,370
instance in running a crowd

00:02:22,450 --> 00:02:29,920
it's service it provides here all the

00:02:27,370 --> 00:02:32,350
APS to access the Google cloud services

00:02:29,920 --> 00:02:36,190
write a big query or machine learning

00:02:32,350 --> 00:02:39,310
engine and also it provides dear get

00:02:36,190 --> 00:02:43,330
support so you can when you have a

00:02:39,310 --> 00:02:46,120
belated you're not you can publish push

00:02:43,330 --> 00:02:49,330
it through the kid partially within the

00:02:46,120 --> 00:02:52,239
data lab environment also it is

00:02:49,330 --> 00:02:54,850
integrated with Google chart so not only

00:02:52,239 --> 00:02:56,680
using the popular tools such as Matt

00:02:54,850 --> 00:03:00,910
brought me a block you can also use

00:02:56,680 --> 00:03:02,500
given check ApS and recently we

00:03:00,910 --> 00:03:05,260
announced the latest version of the

00:03:02,500 --> 00:03:09,160
credit Allah it includes you support for

00:03:05,260 --> 00:03:12,519
the Python to be on the photo either

00:03:09,160 --> 00:03:14,890
shut down feature because data lab is

00:03:12,519 --> 00:03:17,290
running on the you do compute engine

00:03:14,890 --> 00:03:20,500
instance or ground if you leave it

00:03:17,290 --> 00:03:24,130
running then you will be charged for the

00:03:20,500 --> 00:03:27,070
virtual machines so now it's supports to

00:03:24,130 --> 00:03:29,769
be shut down automatically so that you

00:03:27,070 --> 00:03:32,709
don't have to worry about the the

00:03:29,769 --> 00:03:35,019
charges and the also it supports the

00:03:32,709 --> 00:03:39,400
test of a wanted to and beam to the pole

00:03:35,019 --> 00:03:41,200
and the latest videos also includes the

00:03:39,400 --> 00:03:45,250
preview version of the so-called and

00:03:41,200 --> 00:03:47,350
then workbench cities are easy tool for

00:03:45,250 --> 00:03:49,810
going through all the machine learning

00:03:47,350 --> 00:03:52,900
like cycles record starting from the

00:03:49,810 --> 00:03:55,930
Year ETL and pre processing all the data

00:03:52,900 --> 00:03:58,260
or the training your model but in

00:03:55,930 --> 00:04:01,420
marinating your model and deployment and

00:03:58,260 --> 00:04:05,079
the we're trying to provider a workbench

00:04:01,420 --> 00:04:08,380
or toolkit that can support and

00:04:05,079 --> 00:04:11,260
integrate only the tools of Arab on

00:04:08,380 --> 00:04:14,079
Google crowd and make it easy to build a

00:04:11,260 --> 00:04:17,650
pipeline for you but it's still preview

00:04:14,079 --> 00:04:20,350
so maybe later in this year I will be

00:04:17,650 --> 00:04:23,770
providing more and more proper documents

00:04:20,350 --> 00:04:26,590
or samples for the network which

00:04:23,770 --> 00:04:30,639
so today I'd like to talk about how you

00:04:26,590 --> 00:04:33,910
can use the credit Allah to integrate

00:04:30,639 --> 00:04:36,400
two important crowd services from us

00:04:33,910 --> 00:04:39,520
that is couldn't bigquery getaway house

00:04:36,400 --> 00:04:42,039
and tensorflow which is the machine

00:04:39,520 --> 00:04:45,340
learning framework let me start with

00:04:42,039 --> 00:04:47,410
bigquery what is big query the query the

00:04:45,340 --> 00:04:50,770
query is a data warehouse service from

00:04:47,410 --> 00:04:54,759
Google graph and that is based on the

00:04:50,770 --> 00:04:55,210
very simple experiment we have done at

00:04:54,759 --> 00:04:58,590
Google

00:04:55,210 --> 00:05:03,699
many years ago we have done experiment

00:04:58,590 --> 00:05:05,470
to learn how many districts you need to

00:05:03,699 --> 00:05:09,580
scan do all the 1 terabyte of data

00:05:05,470 --> 00:05:12,639
stored on hard disk drives scan in one

00:05:09,580 --> 00:05:15,819
second if you store the one terabyte of

00:05:12,639 --> 00:05:17,530
data in a single describe you can

00:05:15,819 --> 00:05:20,560
imagine that you would take record

00:05:17,530 --> 00:05:23,909
several hours to code to be to the

00:05:20,560 --> 00:05:28,780
auditor but if you speak the data into

00:05:23,909 --> 00:05:32,949
100 drives or 1,000 drives it can get

00:05:28,780 --> 00:05:35,440
much much faster so how many describes

00:05:32,949 --> 00:05:36,729
you would need to scan the only 1

00:05:35,440 --> 00:05:40,740
terabyte in one second

00:05:36,729 --> 00:05:44,860
the result was 5000 to 10,000 this works

00:05:40,740 --> 00:05:47,500
so there was a very simple idea why not

00:05:44,860 --> 00:05:51,520
building such a massive power data

00:05:47,500 --> 00:05:54,759
warehouse by using gear he sounds on TV

00:05:51,520 --> 00:05:58,000
describes and GPU processes in Google

00:05:54,759 --> 00:06:01,030
crowd so that's the bigquery because we

00:05:58,000 --> 00:06:03,520
have the data center as a computer that

00:06:01,030 --> 00:06:06,030
means we have the tens of assad under

00:06:03,520 --> 00:06:13,389
these servers in a single data center

00:06:06,030 --> 00:06:16,960
and whole data centers and each service

00:06:13,389 --> 00:06:19,690
in a data center is working as a node in

00:06:16,960 --> 00:06:22,539
muscular power of computing resource and

00:06:19,690 --> 00:06:25,330
to implement that we have been building

00:06:22,539 --> 00:06:26,800
a very unique technologies geotechnics

00:06:25,330 --> 00:06:29,830
one is the continent technologies called

00:06:26,800 --> 00:06:32,349
called called paul bogeys are all

00:06:29,830 --> 00:06:35,860
proprietary continent acknowledges it's

00:06:32,349 --> 00:06:37,060
just like a darker but the largest

00:06:35,860 --> 00:06:40,900
difference is DDS

00:06:37,060 --> 00:06:44,260
scale because with a single cell or

00:06:40,900 --> 00:06:47,410
single cluster or pork we manage 10,000

00:06:44,260 --> 00:06:50,200
machines with Commerce's so any software

00:06:47,410 --> 00:06:55,419
engineers at Google can execute single

00:06:50,200 --> 00:06:58,270
Pokemon children her or his application

00:06:55,419 --> 00:07:00,940
or process on hundreds of machines in a

00:06:58,270 --> 00:07:03,419
few seconds so that's the large scale

00:07:00,940 --> 00:07:07,210
our content ecology we have at Google

00:07:03,419 --> 00:07:09,310
and the second technology is the network

00:07:07,210 --> 00:07:12,340
we have been building our own hardware

00:07:09,310 --> 00:07:14,050
for switch fabric we are not using the

00:07:12,340 --> 00:07:16,180
commodity routers or switches such as

00:07:14,050 --> 00:07:18,130
Cisco and juniper for building our

00:07:16,180 --> 00:07:20,260
datacenter Network instead we are we

00:07:18,130 --> 00:07:23,770
have been hiring the hardware designers

00:07:20,260 --> 00:07:27,210
for switch fabrics and a building the

00:07:23,770 --> 00:07:29,830
network public from scratch second host

00:07:27,210 --> 00:07:32,740
100,000 ports of 10-gig can you know

00:07:29,830 --> 00:07:36,070
what a Gigabit Ethernet that is 1.2 a

00:07:32,740 --> 00:07:37,600
petabyte per second so combining two

00:07:36,070 --> 00:07:41,560
different technologies new content

00:07:37,600 --> 00:07:44,320
immunity and the large-scale network you

00:07:41,560 --> 00:07:46,450
can build the very knowledge must be

00:07:44,320 --> 00:07:50,680
product processing data warehouse that

00:07:46,450 --> 00:07:53,050
is a big query so every single SQL

00:07:50,680 --> 00:07:55,240
you'll be running on big baby will be

00:07:53,050 --> 00:07:57,970
processed by one father to choose 2,000

00:07:55,240 --> 00:08:00,460
sheepy course with hundreds of districts

00:07:57,970 --> 00:08:02,620
running in power so it's totally

00:08:00,460 --> 00:08:05,020
different architecture compared with

00:08:02,620 --> 00:08:08,500
still the other computers data warehouse

00:08:05,020 --> 00:08:10,810
system so let's take a look at the live

00:08:08,500 --> 00:08:13,150
demonstration of how bigquery can work

00:08:10,810 --> 00:08:15,520
this crowd data Wow so this is the

00:08:13,150 --> 00:08:19,150
screaming of the crowd data log it's

00:08:15,520 --> 00:08:22,150
just a Jupiter power book but as you can

00:08:19,150 --> 00:08:25,539
see you can directly write the SQL or

00:08:22,150 --> 00:08:28,990
running of bigquery within Jupiter not

00:08:25,539 --> 00:08:32,560
work so this is a sample query code we

00:08:28,990 --> 00:08:35,440
can be where we are accessing a sample

00:08:32,560 --> 00:08:38,039
table named wiki can be that has the 10

00:08:35,440 --> 00:08:41,349
billion rows of the page view data

00:08:38,039 --> 00:08:47,190
imported from the wikipedia service and

00:08:41,349 --> 00:08:47,190
now i'm trying to counter page views and

00:08:47,310 --> 00:08:54,630
aggregating by

00:08:49,090 --> 00:08:58,020
titles of the wikipedia page and now i'm

00:08:54,630 --> 00:09:01,210
using the regular expression matching

00:08:58,020 --> 00:09:05,980
specifying crowd at the key word or text

00:09:01,210 --> 00:09:08,410
pattern and order by it titles so

00:09:05,980 --> 00:09:09,970
usually if you run this kind of a query

00:09:08,410 --> 00:09:12,520
with regular expression matching there's

00:09:09,970 --> 00:09:15,070
no way to build the database index or

00:09:12,520 --> 00:09:18,160
caching so this query will be processed

00:09:15,070 --> 00:09:21,580
as the full take full scan or table scan

00:09:18,160 --> 00:09:24,850
you have to read all the data from the

00:09:21,580 --> 00:09:26,920
hard disk drive or SSD and process all

00:09:24,850 --> 00:09:30,010
the regular expression matching these

00:09:26,920 --> 00:09:33,010
CPUs so usually if you do this kind of

00:09:30,010 --> 00:09:34,900
inquiry on the other candidate data

00:09:33,010 --> 00:09:36,940
warehouse or database recommends sequel

00:09:34,900 --> 00:09:39,610
or other products your database

00:09:36,940 --> 00:09:42,220
administrator will be so mad at you

00:09:39,610 --> 00:09:45,460
do not allow you to this kind of this

00:09:42,220 --> 00:09:48,850
stupid query but with bigquery you can

00:09:45,460 --> 00:09:51,580
easily execute it and now I have defined

00:09:48,850 --> 00:09:54,700
a query code and we can be and because

00:09:51,580 --> 00:09:57,940
death Allah provides the Python API to

00:09:54,700 --> 00:10:01,540
run G a big query query within a year

00:09:57,940 --> 00:10:04,360
dip the notebook you can easily execute

00:10:01,540 --> 00:10:07,300
the defined query from the Python code

00:10:04,360 --> 00:10:11,160
practice so now it's running the query

00:10:07,300 --> 00:10:14,800
on big query instances on Google Chrome

00:10:11,160 --> 00:10:17,650
so this is not a cache the result so

00:10:14,800 --> 00:10:19,360
this is actually processed all the data

00:10:17,650 --> 00:10:21,220
stored on the hard disk drive hundreds

00:10:19,360 --> 00:10:23,650
of a hard disk drives processed by a

00:10:21,220 --> 00:10:25,780
thousand of a sea peoples are Tecate

00:10:23,650 --> 00:10:27,550
without these real-time result within

00:10:25,780 --> 00:10:29,350
like five seconds of a five or six

00:10:27,550 --> 00:10:32,530
seconds this query

00:10:29,350 --> 00:10:35,920
read about 400 gigabyte of data read

00:10:32,530 --> 00:10:38,980
from disk drives not caching or not

00:10:35,920 --> 00:10:40,589
using any index so this is the

00:10:38,980 --> 00:10:43,000
performance of the Google bigquery and

00:10:40,589 --> 00:10:48,670
everything is accessible through the

00:10:43,000 --> 00:10:51,250
crowd at arap so bigquery is fully

00:10:48,670 --> 00:10:53,560
managed no ops data warehouse and it can

00:10:51,250 --> 00:10:56,280
process the terabytes of data in tens of

00:10:53,560 --> 00:10:58,150
seconds but still the cost is so low

00:10:56,280 --> 00:11:01,270
inexpensive compared with the

00:10:58,150 --> 00:11:02,680
accomplishes products and it's so easy

00:11:01,270 --> 00:11:06,490
to use is just understand

00:11:02,680 --> 00:11:09,699
SQL so and also it's scalable so that

00:11:06,490 --> 00:11:12,430
means any small an inquiry or humble

00:11:09,699 --> 00:11:14,439
crevice or quick and dirty queries do

00:11:12,430 --> 00:11:17,259
not does that affect the performance of

00:11:14,439 --> 00:11:19,329
the production queries so that's reason

00:11:17,259 --> 00:11:21,129
why many bigquery users are letting

00:11:19,329 --> 00:11:23,290
anybody in the enterprise where the

00:11:21,129 --> 00:11:26,379
sales people or marketing people or

00:11:23,290 --> 00:11:29,019
executives learning their own home blue

00:11:26,379 --> 00:11:32,220
copy and pasted SQL directly on

00:11:29,019 --> 00:11:36,129
production instance or bigquery because

00:11:32,220 --> 00:11:40,990
it's scalable nobody would be affected

00:11:36,129 --> 00:11:44,259
by these rolling queries so that was a

00:11:40,990 --> 00:11:46,329
just a very very basic demonstration of

00:11:44,259 --> 00:11:50,379
bigquery and data laughs and what about

00:11:46,329 --> 00:11:54,309
the more smart narratives by using feat

00:11:50,379 --> 00:11:55,959
of feature vectors so I'd like to talk

00:11:54,309 --> 00:11:59,559
about how you can combine the data

00:11:55,959 --> 00:12:01,569
warehouse bigquery this machine learning

00:11:59,559 --> 00:12:04,149
learning with chains of law and grunt

00:12:01,569 --> 00:12:08,980
machine learnings and data lab will be a

00:12:04,149 --> 00:12:12,600
hub for integrating two technologies so

00:12:08,980 --> 00:12:15,339
let me show some interesting example

00:12:12,600 --> 00:12:18,490
demonstration first this is a

00:12:15,339 --> 00:12:20,740
demonstration called query smart where

00:12:18,490 --> 00:12:24,189
you can choose three different kind of

00:12:20,740 --> 00:12:28,170
queries I'll be running a document

00:12:24,189 --> 00:12:32,850
similarity search as a sample this is a

00:12:28,170 --> 00:12:36,569
sample query of query smart that will be

00:12:32,850 --> 00:12:40,290
searching for a similar document

00:12:36,569 --> 00:12:43,569
document similar to an unspecified

00:12:40,290 --> 00:12:45,699
document and on the query we have a

00:12:43,569 --> 00:12:48,910
public data set for many different kinds

00:12:45,699 --> 00:12:51,670
of the data and this sample is using

00:12:48,910 --> 00:12:54,990
Stack Overflow questions for 10 million

00:12:51,670 --> 00:12:57,730
documents 10 million questions so if you

00:12:54,990 --> 00:13:00,369
specify one with these key questions

00:12:57,730 --> 00:13:03,429
like a combating strange into age in

00:13:00,369 --> 00:13:05,470
Java then bigquery is running a query

00:13:03,429 --> 00:13:09,160
against 10 million documents or

00:13:05,470 --> 00:13:13,059
questions to compare the two different

00:13:09,160 --> 00:13:15,220
documents are similar or not this is not

00:13:13,059 --> 00:13:16,060
based on the any cached or indexed

00:13:15,220 --> 00:13:19,430
search or

00:13:16,060 --> 00:13:20,450
tag or label based search instead it is

00:13:19,430 --> 00:13:23,000
actually comparing two different

00:13:20,450 --> 00:13:25,670
documents for ten million documents in

00:13:23,000 --> 00:13:34,880
real time and usually liquid it would

00:13:25,670 --> 00:13:39,560
finish within 30 seconds that's on a

00:13:34,880 --> 00:13:42,080
study so you can see the next question

00:13:39,560 --> 00:13:44,270
is the key document a key question and

00:13:42,080 --> 00:13:46,670
this is the risk of B or the result you

00:13:44,270 --> 00:13:50,150
got these similar questions on stuck

00:13:46,670 --> 00:13:52,820
driver so as you can see these are all

00:13:50,150 --> 00:13:57,140
similar questions and this is not based

00:13:52,820 --> 00:13:59,600
on the label or the tag based search so

00:13:57,140 --> 00:14:02,920
how do you implement this how do you

00:13:59,600 --> 00:14:05,300
find similar posts on Stack Overflow

00:14:02,920 --> 00:14:07,580
this these are the what I have done with

00:14:05,300 --> 00:14:09,500
these demonstrations but first you have

00:14:07,580 --> 00:14:13,070
to speak each sentence in two words

00:14:09,500 --> 00:14:15,380
the segmentation and then you extractive

00:14:13,070 --> 00:14:18,350
feature vectors from each post each

00:14:15,380 --> 00:14:20,540
document and this is the key technology

00:14:18,350 --> 00:14:24,020
in bigquery that is user defined

00:14:20,540 --> 00:14:27,200
function for UDF that allows you to

00:14:24,020 --> 00:14:31,130
define a small JavaScript code running

00:14:27,200 --> 00:14:33,440
inside you SQL so that allows you to do

00:14:31,130 --> 00:14:37,970
very simple pre-processing or small

00:14:33,440 --> 00:14:41,020
calculations within an SQL so in this

00:14:37,970 --> 00:14:43,600
case I've used ugf to do a similarity

00:14:41,020 --> 00:14:46,130
calculation between two feature vectors

00:14:43,600 --> 00:14:49,280
and then you can use the user-defined

00:14:46,130 --> 00:14:52,490
function in any SQL you'll be running on

00:14:49,280 --> 00:14:54,350
bigquery so what is feature vector and a

00:14:52,490 --> 00:14:56,840
fartist benefit of using feature vectors

00:14:54,350 --> 00:14:57,560
if you're using the not view or the

00:14:56,840 --> 00:15:00,920
ordinary

00:14:57,560 --> 00:15:04,070
SQL based relational database you will

00:15:00,920 --> 00:15:06,650
be using simple tag or labels to find

00:15:04,070 --> 00:15:09,230
content like if you are trying to find

00:15:06,650 --> 00:15:10,580
any content with talking about movie or

00:15:09,230 --> 00:15:13,430
music or actor

00:15:10,580 --> 00:15:17,540
you'll be putting tags or labels like a

00:15:13,430 --> 00:15:20,270
movie with music tag but instead of

00:15:17,540 --> 00:15:22,820
using those labels you can use your

00:15:20,270 --> 00:15:26,270
feature vectors we sponsor the numbers

00:15:22,820 --> 00:15:28,330
in a vector each number represents there

00:15:26,270 --> 00:15:34,990
how much

00:15:28,330 --> 00:15:38,800
topic how much did how the we can see

00:15:34,990 --> 00:15:42,250
objects or topics people in a single

00:15:38,800 --> 00:15:46,420
document so this particular document

00:15:42,250 --> 00:15:49,060
with this vector is talking about movie

00:15:46,420 --> 00:15:52,840
a little and does not talking about

00:15:49,060 --> 00:15:57,450
music so much but talking about actors

00:15:52,840 --> 00:16:01,840
so much so you can represent the content

00:15:57,450 --> 00:16:03,430
by using the feature vector and you can

00:16:01,840 --> 00:16:05,260
extract the feature vector from those

00:16:03,430 --> 00:16:07,840
Stack Overflow questions by using a

00:16:05,260 --> 00:16:11,800
simple I will in this case I have used

00:16:07,840 --> 00:16:13,960
here IDF this is a classic very simple

00:16:11,800 --> 00:16:15,670
way of extracting feature vector from

00:16:13,960 --> 00:16:18,880
documents this is not a machine learning

00:16:15,670 --> 00:16:21,130
stuff it's just trying to calculate the

00:16:18,880 --> 00:16:25,690
frequency of the important words suggest

00:16:21,130 --> 00:16:29,590
Java or CSS or text selections and Jeff

00:16:25,690 --> 00:16:33,520
idea allows you to remove all the

00:16:29,590 --> 00:16:36,160
trivial ones such as water or a for you

00:16:33,520 --> 00:16:39,850
and you can extract the most important

00:16:36,160 --> 00:16:42,760
words on these frequencies then you can

00:16:39,850 --> 00:16:44,520
compare the similarities between two

00:16:42,760 --> 00:16:46,720
vectors by using the cosine similarity

00:16:44,520 --> 00:16:52,720
and you can do the same thing in these

00:16:46,720 --> 00:16:54,790
words you can have the vectors like this

00:16:52,720 --> 00:16:57,490
if you have a vector for music world

00:16:54,790 --> 00:17:00,430
music then the vectors for songs or

00:16:57,490 --> 00:17:03,850
dances or bodies will have the high

00:17:00,430 --> 00:17:06,160
similarity to the music reflector for

00:17:03,850 --> 00:17:09,280
music so you can do the same thing with

00:17:06,160 --> 00:17:12,339
your document and to do that usually

00:17:09,280 --> 00:17:14,230
many people are using the bigger thing

00:17:12,339 --> 00:17:17,770
is rhykker apache hadoop mapreduce

00:17:14,230 --> 00:17:19,930
raster or Apache spark waiting for tens

00:17:17,770 --> 00:17:22,839
of minutes processing all the terabytes

00:17:19,930 --> 00:17:25,089
of data but instead by using bigquery

00:17:22,839 --> 00:17:28,060
you can just use a user-defined function

00:17:25,089 --> 00:17:31,870
to do the calculating the achieve idea

00:17:28,060 --> 00:17:34,510
based similarity switch practice because

00:17:31,870 --> 00:17:37,840
as I mentioned because I will be running

00:17:34,510 --> 00:17:39,790
on a 1000 to 2000 cheapy course the

00:17:37,840 --> 00:17:40,420
JavaScript code you'll be running with

00:17:39,790 --> 00:17:42,049
UDL

00:17:40,420 --> 00:17:44,149
will also be running

00:17:42,049 --> 00:17:46,129
year one father no choose a recipe of

00:17:44,149 --> 00:17:50,539
course dynasty one with the largest

00:17:46,129 --> 00:17:53,299
scale information processing system you

00:17:50,539 --> 00:17:56,600
can access their business can be

00:17:53,299 --> 00:18:00,110
finished within tens of seconds not tens

00:17:56,600 --> 00:18:03,289
of minutes so let me actually show the

00:18:00,110 --> 00:18:05,960
demonstration how I have implemented the

00:18:03,289 --> 00:18:10,279
documents Miyagi's search by using crowd

00:18:05,960 --> 00:18:13,220
at a lab so again you can love you can

00:18:10,279 --> 00:18:15,919
directly write down your SQL inside they

00:18:13,220 --> 00:18:19,639
don't love and execute it so this is

00:18:15,919 --> 00:18:22,009
here simple SQL to look at what kind of

00:18:19,639 --> 00:18:23,509
media content you have on the bakery

00:18:22,009 --> 00:18:26,809
other stackdriver

00:18:23,509 --> 00:18:29,809
questions so the table has the title and

00:18:26,809 --> 00:18:34,580
body columns where he has the title and

00:18:29,809 --> 00:18:38,029
the body of each question and then you

00:18:34,580 --> 00:18:40,460
can write a simple user-defined function

00:18:38,029 --> 00:18:44,059
for doing the segmentation speaking the

00:18:40,460 --> 00:18:45,649
ascendance in two words it uses a simple

00:18:44,059 --> 00:18:49,220
JavaScript base as a regular expression

00:18:45,649 --> 00:18:51,590
matching to remove HTML tags converting

00:18:49,220 --> 00:18:55,549
everything into lowercase and extracting

00:18:51,590 --> 00:18:59,899
the words so you don't have to prepare

00:18:55,549 --> 00:19:00,529
your hard crust to do in this kinda

00:18:59,899 --> 00:19:03,830
trivial

00:19:00,529 --> 00:19:08,889
simple conversion or formatting so you

00:19:03,830 --> 00:19:08,889
have the the world's separated us

00:19:09,200 --> 00:19:16,429
the sentence is split into the words and

00:19:13,519 --> 00:19:19,399
then you can extract the feature vectors

00:19:16,429 --> 00:19:22,190
by using key for a year by using user

00:19:19,399 --> 00:19:24,259
defined function achieve idea is very

00:19:22,190 --> 00:19:26,359
simple algorithms it is not a machine

00:19:24,259 --> 00:19:31,009
one is just accounting numbers or we can

00:19:26,359 --> 00:19:33,559
see which was in a document but again in

00:19:31,009 --> 00:19:37,929
pass it's really hard to process this

00:19:33,559 --> 00:19:40,700
kind of the logics within data warehouse

00:19:37,929 --> 00:19:43,789
so many people has been using Hadoop or

00:19:40,700 --> 00:19:45,700
spark but with bigquery you can run your

00:19:43,789 --> 00:19:48,470
own logic on bigquery

00:19:45,700 --> 00:19:51,499
so that you can easily get this kind of

00:19:48,470 --> 00:19:54,690
a picture pictures within tens of

00:19:51,499 --> 00:19:58,360
seconds like a twenty or thirty seconds

00:19:54,690 --> 00:20:00,970
so it's ready to execute the similarity

00:19:58,360 --> 00:20:03,010
search so we define the user-defined

00:20:00,970 --> 00:20:06,510
function for calculating similarity

00:20:03,010 --> 00:20:08,710
between two vectors it's just a simple

00:20:06,510 --> 00:20:11,530
calculations multiplications and

00:20:08,710 --> 00:20:14,350
additions between numbers and then you

00:20:11,530 --> 00:20:23,110
can use your own user-defined function

00:20:14,350 --> 00:20:26,290
in any SQL it will be one it is funny so

00:20:23,110 --> 00:20:28,270
even if you have 10 million posts before

00:20:26,290 --> 00:20:30,880
all the old processing should be done

00:20:28,270 --> 00:20:34,150
with like a 16 or 20 seconds is so fast

00:20:30,880 --> 00:20:34,630
so you're seeing the result from the

00:20:34,150 --> 00:20:36,570
bigquery

00:20:34,630 --> 00:20:39,040
where are you have to all the similar

00:20:36,570 --> 00:20:41,890
questions on Stack Overflow talking

00:20:39,040 --> 00:20:49,720
about deleting or adding a column to

00:20:41,890 --> 00:20:51,220
Microsoft SQL Server so so far I haven't

00:20:49,720 --> 00:20:52,900
talked anything about machine learning I

00:20:51,220 --> 00:20:55,570
have used very simple TF I gave are

00:20:52,900 --> 00:20:59,380
using so let's try using machine

00:20:55,570 --> 00:21:03,160
learning to execute execute to be

00:20:59,380 --> 00:21:05,169
smarter more smarter analytics how to

00:21:03,160 --> 00:21:08,440
analyze data with much complex features

00:21:05,169 --> 00:21:10,330
if you are using the standard SQL simple

00:21:08,440 --> 00:21:13,150
SQL it's very hard to capture those

00:21:10,330 --> 00:21:15,360
complex data structures especially when

00:21:13,150 --> 00:21:19,750
you are handling the unstructured data

00:21:15,360 --> 00:21:22,330
images or audio or or user data with

00:21:19,750 --> 00:21:24,790
year hundreds of the attributes or all

00:21:22,330 --> 00:21:27,429
the dimensions it's really hard to build

00:21:24,790 --> 00:21:30,400
your own SQL to look at tens of

00:21:27,429 --> 00:21:33,190
different or attributes or dimensions in

00:21:30,400 --> 00:21:36,640
a single query so in that case it's

00:21:33,190 --> 00:21:39,370
really easier to use machine learning on

00:21:36,640 --> 00:21:43,780
your network because by using neural

00:21:39,370 --> 00:21:47,410
network we can try to extract the

00:21:43,780 --> 00:21:49,299
features by looking at the path data and

00:21:47,410 --> 00:21:52,179
training data so this is a demonstration

00:21:49,299 --> 00:21:56,559
called tensorflow playground where the

00:21:52,179 --> 00:21:58,450
try is tries to capture the double

00:21:56,559 --> 00:22:02,320
spiral patterns inside it to an integer

00:21:58,450 --> 00:22:03,340
so initially compute computer doesn't

00:22:02,320 --> 00:22:06,700
make any sense

00:22:03,340 --> 00:22:11,230
ordinator but if you keep providing

00:22:06,700 --> 00:22:14,020
the training data and providing the

00:22:11,230 --> 00:22:17,050
computing power you computer tries to

00:22:14,020 --> 00:22:19,930
capture a certain complex patterns or

00:22:17,050 --> 00:22:22,360
pitches formerly data and this is

00:22:19,930 --> 00:22:26,080
something you cannot do with the sequel

00:22:22,360 --> 00:22:29,560
or standard data warehouse so how about

00:22:26,080 --> 00:22:32,380
combining these two technologies and at

00:22:29,560 --> 00:22:34,540
Google we will grant service a platform

00:22:32,380 --> 00:22:36,580
we provide a bunch of different machine

00:22:34,540 --> 00:22:38,500
learning services and there are two

00:22:36,580 --> 00:22:42,160
kinds of the products one is the

00:22:38,500 --> 00:22:45,460
pre-training a male model at right side

00:22:42,160 --> 00:22:48,190
these are all year API

00:22:45,460 --> 00:22:50,290
simple ApS with pre-trained model inside

00:22:48,190 --> 00:22:51,850
him so you don't have to have any

00:22:50,290 --> 00:22:53,740
expertise on machine learning and you

00:22:51,850 --> 00:22:55,510
know how to train your own machine

00:22:53,740 --> 00:22:59,320
learning model instead you can just send

00:22:55,510 --> 00:23:01,150
images audio or natural language text to

00:22:59,320 --> 00:23:05,790
the API and you will be getting a good

00:23:01,150 --> 00:23:09,550
result such as the labels cat or dog or

00:23:05,790 --> 00:23:11,680
the wedding party from api's this is so

00:23:09,550 --> 00:23:13,870
easy to use but in this session a

00:23:11,680 --> 00:23:17,800
directive focus on the customer and

00:23:13,870 --> 00:23:20,890
their models to use custom email models

00:23:17,800 --> 00:23:23,020
we provide the products such as tensor

00:23:20,890 --> 00:23:24,820
properties are open source to for

00:23:23,020 --> 00:23:27,130
building your machine running model and

00:23:24,820 --> 00:23:31,500
machine learning ending or a never

00:23:27,130 --> 00:23:34,060
ending this is a commercial service for

00:23:31,500 --> 00:23:38,520
running a fully managed tensorflow

00:23:34,060 --> 00:23:41,590
runtime which depends of the GPS or GPUs

00:23:38,520 --> 00:23:43,630
so as you may notice a pro is the most

00:23:41,590 --> 00:23:47,200
popular deep learning framework in the

00:23:43,630 --> 00:23:49,330
wall actually inside Google we actually

00:23:47,200 --> 00:23:51,970
use intensive work for building almost

00:23:49,330 --> 00:23:58,660
all new machine learning or AI based

00:23:51,970 --> 00:24:00,550
searches services and it's also getting

00:23:58,660 --> 00:24:03,460
popularity in the community because it's

00:24:00,550 --> 00:24:05,530
so easy to use you can just write ten

00:24:03,460 --> 00:24:08,500
the Python code to define you on your

00:24:05,530 --> 00:24:10,060
network model and let tensorflow

00:24:08,500 --> 00:24:12,700
optimizing you on your neural network

00:24:10,060 --> 00:24:16,110
model with the different algorithms such

00:24:12,700 --> 00:24:16,110
as the gradient descent

00:24:16,280 --> 00:24:19,549
and what is machine learning ng and

00:24:18,289 --> 00:24:22,429
what's the difference between tensorflow

00:24:19,549 --> 00:24:25,130
and ml ng the MND is a commercial

00:24:22,429 --> 00:24:27,200
service that provides the fully managed

00:24:25,130 --> 00:24:29,990
to 10 suppor platform or runtime on

00:24:27,200 --> 00:24:32,240
Google crowd so if you are using open

00:24:29,990 --> 00:24:35,000
source of law you have to take care of

00:24:32,240 --> 00:24:37,730
the runtime platform by yourself Recker

00:24:35,000 --> 00:24:39,980
you know you have to prepare the virtual

00:24:37,730 --> 00:24:42,320
machine instance and if you want to use

00:24:39,980 --> 00:24:45,620
GPU you have to install install crew the

00:24:42,320 --> 00:24:48,320
driver or qu DNN and you have to care

00:24:45,620 --> 00:24:51,409
who about the versions of source library

00:24:48,320 --> 00:24:53,270
and if you want to have the distributed

00:24:51,409 --> 00:24:56,210
training environment you have to build

00:24:53,270 --> 00:24:57,919
your own kubernetes cluster you have to

00:24:56,210 --> 00:25:02,150
think about how to split your work what

00:24:57,919 --> 00:25:03,980
into multiple devices but using by using

00:25:02,150 --> 00:25:05,929
the machine learning engine you can

00:25:03,980 --> 00:25:08,150
forget everything about those things all

00:25:05,929 --> 00:25:10,820
you have to do is build your own test

00:25:08,150 --> 00:25:14,450
for model and prepare training data and

00:25:10,820 --> 00:25:17,630
upload them to the MN o ng so that the

00:25:14,450 --> 00:25:20,320
emerging take care of building and

00:25:17,630 --> 00:25:25,220
operating your distributed training

00:25:20,320 --> 00:25:27,049
systems based on and also it provides

00:25:25,220 --> 00:25:29,390
teacher hyper parameter tuning

00:25:27,049 --> 00:25:31,820
automation called hyper chim it uses the

00:25:29,390 --> 00:25:34,250
gaussian process to minimize the number

00:25:31,820 --> 00:25:38,710
of the trials we still have a parameter

00:25:34,250 --> 00:25:42,169
chip here I directed show two different

00:25:38,710 --> 00:25:45,230
use cases for combining bigquery with

00:25:42,169 --> 00:25:47,150
machine running engine or tensorflow the

00:25:45,230 --> 00:25:48,650
first this case is the most popular one

00:25:47,150 --> 00:25:53,210
using bigquery for

00:25:48,650 --> 00:25:57,440
pre-processing or ETL so extracting the

00:25:53,210 --> 00:25:59,330
data from bigquery and then you can go

00:25:57,440 --> 00:26:03,289
in through all these write write cycle

00:25:59,330 --> 00:26:07,100
of the machine learning so let me show

00:26:03,289 --> 00:26:08,289
some demonstration I have prepared a

00:26:07,100 --> 00:26:10,460
demonstration called

00:26:08,289 --> 00:26:13,159
crash firing Manhattan with bigquery

00:26:10,460 --> 00:26:16,909
this is a very simple sample code

00:26:13,159 --> 00:26:18,860
running on cloud data lab tutoring us

00:26:16,909 --> 00:26:22,340
very simple neural network models that

00:26:18,860 --> 00:26:25,840
can cross by location geolocation

00:26:22,340 --> 00:26:25,840
is in a marathon or now

00:26:25,900 --> 00:26:33,559
so at was let me execute a big query SQL

00:26:30,380 --> 00:26:36,590
that extracts the training data from

00:26:33,559 --> 00:26:38,500
bigquery public data set here I'd like

00:26:36,590 --> 00:26:41,570
to use the public data set called NYPD

00:26:38,500 --> 00:26:43,940
collisions that is a public data set

00:26:41,570 --> 00:26:46,970
which has the already oxygen to recalls

00:26:43,940 --> 00:26:49,250
in New York City so every time you have

00:26:46,970 --> 00:26:50,900
a cop car oxygen in New York City it is

00:26:49,250 --> 00:26:56,000
important in the big query other public

00:26:50,900 --> 00:26:59,809
data set omegas kill so it's running the

00:26:56,000 --> 00:27:02,630
query so this is gtatyr so each accident

00:26:59,809 --> 00:27:06,410
has the timestamp and ratchet and long

00:27:02,630 --> 00:27:08,929
to the pair and borrow so we'll be using

00:27:06,410 --> 00:27:12,740
this data to train the neural network

00:27:08,929 --> 00:27:15,620
model that can crush why whether a

00:27:12,740 --> 00:27:19,220
single geo location is inside Mahajan

00:27:15,620 --> 00:27:21,620
wanna but these are some useless data

00:27:19,220 --> 00:27:24,950
records each data which doesn't have any

00:27:21,620 --> 00:27:28,970
power so we do I have to do some simple

00:27:24,950 --> 00:27:31,730
pre-processing so with bigquery you can

00:27:28,970 --> 00:27:35,320
write a very simple SQL to exclude those

00:27:31,730 --> 00:27:38,270
pages without any rush and longitude and

00:27:35,320 --> 00:27:43,030
also I want to shuffle them all theater

00:27:38,270 --> 00:27:47,080
I want to extract top first 10,000 vows

00:27:43,030 --> 00:27:51,020
to execute so I have just defined his

00:27:47,080 --> 00:27:53,570
SQL code anyways the colleges and again

00:27:51,020 --> 00:27:57,290
you can use the bigquery API inside data

00:27:53,570 --> 00:28:00,559
log to execute the query define a query

00:27:57,290 --> 00:28:03,679
and you can convert it to the pandas

00:28:00,559 --> 00:28:06,169
data player and our ability converting

00:28:03,679 --> 00:28:09,169
DT to come into the numpy array

00:28:06,169 --> 00:28:10,940
directory so and the result is that you

00:28:09,169 --> 00:28:15,700
will be having the result converted into

00:28:10,940 --> 00:28:15,700
an umpire executed

00:28:17,059 --> 00:28:22,529
so by using the bigquery API on data lab

00:28:20,220 --> 00:28:24,149
it's so easy to do the proof of proofs

00:28:22,529 --> 00:28:28,500
of pre-processing with user-defined

00:28:24,149 --> 00:28:30,870
function and standard SQL important in

00:28:28,500 --> 00:28:33,299
wizard will be imported as numpy array

00:28:30,870 --> 00:28:35,760
for pandas dataframe like this

00:28:33,299 --> 00:28:39,409
we have genie appear so we're actually

00:28:35,760 --> 00:28:43,019
ruptured for ten thousand rolls and each

00:28:39,409 --> 00:28:46,919
pair has a label whether it's in my

00:28:43,019 --> 00:28:48,960
heart or not and because it's just

00:28:46,919 --> 00:28:52,049
jupiter notebook you can use access here

00:28:48,960 --> 00:28:55,440
use your tools such as the sake or Mac

00:28:52,049 --> 00:28:57,510
properly for Standardization or parting

00:28:55,440 --> 00:29:00,809
like this so you can see shape of

00:28:57,510 --> 00:29:05,429
mahadji I can even see the Tropic is in

00:29:00,809 --> 00:29:07,830
the middle let's split the data into

00:29:05,429 --> 00:29:10,590
training data on test data and this is

00:29:07,830 --> 00:29:12,480
where you are using the tensorflow for

00:29:10,590 --> 00:29:16,679
defining on deep neural network model

00:29:12,480 --> 00:29:20,029
region importance of role and now tester

00:29:16,679 --> 00:29:23,549
pro has so-called high level API

00:29:20,029 --> 00:29:25,950
compared with the the traditional low

00:29:23,549 --> 00:29:27,960
level ApS so it's getting much much

00:29:25,950 --> 00:29:30,210
easier to define a deep neural network

00:29:27,960 --> 00:29:35,100
model by using tensor form or you have

00:29:30,210 --> 00:29:36,840
to define you have to write to define a

00:29:35,100 --> 00:29:39,779
simple neural network is writing a

00:29:36,840 --> 00:29:42,840
record for lines of code here it's a

00:29:39,779 --> 00:29:45,779
deep neural network crossfire API that

00:29:42,840 --> 00:29:47,970
defines a for hidden layers and each

00:29:45,779 --> 00:29:50,399
hidden layer has a twenty nodes or

00:29:47,970 --> 00:29:54,299
neurons so I have to find in your

00:29:50,399 --> 00:29:55,919
network let's take a look at before

00:29:54,299 --> 00:29:59,240
training let's take a look at let's

00:29:55,919 --> 00:30:03,539
visualize how the network model is

00:29:59,240 --> 00:30:06,059
stupid before training every machine

00:30:03,539 --> 00:30:08,399
learning algorithms is stupid like this

00:30:06,059 --> 00:30:10,740
so because the neural network is

00:30:08,399 --> 00:30:13,769
initialized to determine on numbers it

00:30:10,740 --> 00:30:14,610
thinks this part is Mahuta this is not

00:30:13,769 --> 00:30:17,279
reptilian

00:30:14,610 --> 00:30:21,000
so we have to train the model so by

00:30:17,279 --> 00:30:23,429
calling the feed method of the DNA

00:30:21,000 --> 00:30:26,850
encouraged by your object you can train

00:30:23,429 --> 00:30:28,770
your model by passing year training data

00:30:26,850 --> 00:30:29,909
extract won't repeat great so it just

00:30:28,770 --> 00:30:32,909
rectus I could learn

00:30:29,909 --> 00:30:35,940
the only difference is the scalability

00:30:32,909 --> 00:30:39,369
so as long as I using using this

00:30:35,940 --> 00:30:41,950
high-level API it can be scattered out

00:30:39,369 --> 00:30:47,230
into tens of deep GPUs and terabytes

00:30:41,950 --> 00:30:49,570
worth of data so you're seeing that that

00:30:47,230 --> 00:30:53,889
model is gradually getting smarter and

00:30:49,570 --> 00:30:58,989
smarter by 20 we got to find our result

00:30:53,889 --> 00:31:03,249
so it's pretty smarter than the other

00:30:58,989 --> 00:31:05,679
one the accuracy it's not like that so

00:31:03,249 --> 00:31:07,809
you have seen that is so easy to

00:31:05,679 --> 00:31:10,419
integrate two different things big query

00:31:07,809 --> 00:31:14,129
data warehouse for extracting the

00:31:10,419 --> 00:31:17,169
training data and do pre-processing and

00:31:14,129 --> 00:31:23,200
define your test flow model by writing a

00:31:17,169 --> 00:31:25,629
few lines of Python code so let's take a

00:31:23,200 --> 00:31:28,809
look a look at the use case number two

00:31:25,629 --> 00:31:33,119
which is using machine learning model to

00:31:28,809 --> 00:31:33,119
extract feature vectors such on bigquery

00:31:34,200 --> 00:31:40,629
go back to the query smart demonstration

00:31:37,720 --> 00:31:44,379
so on query it's smart demonstration

00:31:40,629 --> 00:31:46,809
there's another subtle code image

00:31:44,379 --> 00:31:54,129
similarity search where you can choose

00:31:46,809 --> 00:31:59,230
one of the key your key images so now it

00:31:54,129 --> 00:32:02,049
is running a bigquery SQL query against

00:31:59,230 --> 00:32:03,970
one million images actually image itself

00:32:02,049 --> 00:32:07,330
the image files are on Google Cloud

00:32:03,970 --> 00:32:09,789
storage but bigquery has the URL for

00:32:07,330 --> 00:32:12,129
those images and feature vectors of 1

00:32:09,789 --> 00:32:15,549
million images so that Vickery can

00:32:12,129 --> 00:32:18,909
compare whether a single image is come

00:32:15,549 --> 00:32:21,129
is similar to another image and again

00:32:18,909 --> 00:32:23,799
this is not based on attacked or label

00:32:21,129 --> 00:32:25,840
based search it was actually comparing

00:32:23,799 --> 00:32:29,169
the colors and shapes and patterns

00:32:25,840 --> 00:32:32,950
textures of two different images it is

00:32:29,169 --> 00:32:36,450
just not looking for some flower let me

00:32:32,950 --> 00:32:36,450
try another query

00:32:43,100 --> 00:32:50,040
so this quarry is not looking for a

00:32:45,450 --> 00:32:52,500
picture which is looking for images that

00:32:50,040 --> 00:32:54,990
has a similar texture or patterns what

00:32:52,500 --> 00:32:57,090
shakes and usually you have to build

00:32:54,990 --> 00:32:59,340
your own hadoop mapreduce grass or spa

00:32:57,090 --> 00:33:01,860
cluster to do this and you have to wait

00:32:59,340 --> 00:33:04,740
for tens of minutes or many hours to

00:33:01,860 --> 00:33:06,420
finish here the batch processing but

00:33:04,740 --> 00:33:10,410
with bigquery you can get you read that

00:33:06,420 --> 00:33:16,190
within 30 seconds for the 20 is it

00:33:10,410 --> 00:33:16,190
problem I think it's Ross

00:33:19,970 --> 00:33:24,440
maybe my brother is having a problem

00:33:32,820 --> 00:33:35,390
look

00:33:38,620 --> 00:33:46,090
let me try with other properties it's

00:33:43,240 --> 00:33:48,370
not looking for drama five so you'll be

00:33:46,090 --> 00:33:51,309
seeing images with screen background and

00:33:48,370 --> 00:33:52,659
some insect bite patterns in front of

00:33:51,309 --> 00:33:56,950
those green backgrounds

00:33:52,659 --> 00:33:59,710
I hope too that query would finish

00:33:56,950 --> 00:34:02,379
within the right at twenty seconds not

00:33:59,710 --> 00:34:04,690
sure I will cut to bits out so as you

00:34:02,379 --> 00:34:06,610
can see in many of the images has degree

00:34:04,690 --> 00:34:08,560
in background and patterns like a

00:34:06,610 --> 00:34:14,770
dragonfly but it's not looking for drama

00:34:08,560 --> 00:34:17,829
fire okay so what I have down to build

00:34:14,770 --> 00:34:20,829
this demonstration is to at first you

00:34:17,829 --> 00:34:22,480
have to import the all the images on

00:34:20,829 --> 00:34:26,139
Google Cloud storage 1 million images

00:34:22,480 --> 00:34:28,300
and then I have executed a batch process

00:34:26,139 --> 00:34:31,030
of is machine learning engine with test

00:34:28,300 --> 00:34:34,030
for module that applies to shine and

00:34:31,030 --> 00:34:37,510
model to the images to extract feature

00:34:34,030 --> 00:34:40,089
vectors not labels and important feature

00:34:37,510 --> 00:34:42,220
vectors to bigquery and used to using

00:34:40,089 --> 00:34:45,190
UDF to calculate the similarity between

00:34:42,220 --> 00:34:46,960
eg the images so usually if you are

00:34:45,190 --> 00:34:50,530
using shine and model many people are

00:34:46,960 --> 00:34:53,500
using labels such as dog or cat but by

00:34:50,530 --> 00:34:55,450
taking the cut in numbers the vectors in

00:34:53,500 --> 00:34:58,770
your final layers you can extract your

00:34:55,450 --> 00:35:01,750
feature vectors these are about 1000

00:34:58,770 --> 00:35:04,359
numbers in the final layers of DVD g16

00:35:01,750 --> 00:35:06,940
model and it is the feature vectors and

00:35:04,359 --> 00:35:09,880
each elements or numbers in the feature

00:35:06,940 --> 00:35:12,369
vectors and it presents the features

00:35:09,880 --> 00:35:16,210
such as the the patterns or colors or

00:35:12,369 --> 00:35:18,640
context or shakes the images and by

00:35:16,210 --> 00:35:21,040
using tensorflow it's easy to extract

00:35:18,640 --> 00:35:24,760
those feature vectors by specifying gear

00:35:21,040 --> 00:35:27,730
lay position layer and then again you

00:35:24,760 --> 00:35:30,609
can compare the distance between G the

00:35:27,730 --> 00:35:32,980
feature vectors by using user-defined

00:35:30,609 --> 00:35:35,950
function so it's a very simple simple

00:35:32,980 --> 00:35:39,400
mathematic you can define this you year

00:35:35,950 --> 00:35:41,589
to calculate the similarity so that you

00:35:39,400 --> 00:35:44,050
can use the user defined function

00:35:41,589 --> 00:35:46,960
distance at any queries you'll be

00:35:44,050 --> 00:35:48,880
running on bigquery and you can even let

00:35:46,960 --> 00:35:51,490
the sales people or marketing people or

00:35:48,880 --> 00:35:52,240
executives using your user defined

00:35:51,490 --> 00:35:57,700
functions

00:35:52,240 --> 00:35:59,620
to run their own homebrew sql's and we I

00:35:57,700 --> 00:36:02,500
have you used the documents search and

00:35:59,620 --> 00:36:04,470
image search as examples but this

00:36:02,500 --> 00:36:06,850
technique can be applied to any other

00:36:04,470 --> 00:36:09,190
use cases where you can define the

00:36:06,850 --> 00:36:11,050
feature vectors against any Content for

00:36:09,190 --> 00:36:13,480
example if you have a bunch of products

00:36:11,050 --> 00:36:16,510
if you can define a feature vector for

00:36:13,480 --> 00:36:18,790
product you can easily get your product

00:36:16,510 --> 00:36:21,910
recommendation engine just by using

00:36:18,790 --> 00:36:24,400
bigquery and just fro you can find it

00:36:21,910 --> 00:36:28,270
similar users like a premium users or

00:36:24,400 --> 00:36:30,640
cheating users by defining the feature

00:36:28,270 --> 00:36:32,770
vectors while you can find the similar

00:36:30,640 --> 00:36:36,690
IOT devices by looking at the sensor

00:36:32,770 --> 00:36:40,540
data or music or any unstructured data

00:36:36,690 --> 00:36:42,280
so summary so data lab is a one-stop

00:36:40,540 --> 00:36:44,020
shopping for all the cloud-based data

00:36:42,280 --> 00:36:46,030
analysis it combines here

00:36:44,020 --> 00:36:48,880
chipton notebook with the cloud services

00:36:46,030 --> 00:36:50,770
and bigquery as you have seen it's a

00:36:48,880 --> 00:36:53,860
fully managed petabyte scale data

00:36:50,770 --> 00:36:56,620
warehouse single query can learn on the

00:36:53,860 --> 00:36:58,930
1,000 to 2,000 she because with hundreds

00:36:56,620 --> 00:37:01,120
of disk drives and user-defined function

00:36:58,930 --> 00:37:04,270
is the most powerful feature of bigquery

00:37:01,120 --> 00:37:07,390
that can replace a many part of the

00:37:04,270 --> 00:37:10,630
existing apache hadoop mapreduce

00:37:07,390 --> 00:37:12,670
or Apache spark workloads and you can

00:37:10,630 --> 00:37:14,530
combine using the define function with

00:37:12,670 --> 00:37:17,080
tensorflow to extract the intelligence

00:37:14,530 --> 00:37:20,650
and data wrap is the hub for the already

00:37:17,080 --> 00:37:22,300
powerful tools I have explained so

00:37:20,650 --> 00:37:27,510
that's it thank you so much

00:37:22,300 --> 00:37:27,510

YouTube URL: https://www.youtube.com/watch?v=VsKsZPsRo4M


