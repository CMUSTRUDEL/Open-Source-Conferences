Title: Our Skynet Moment - Tim O'Reilly (O'Reilly Media)
Publication date: 2017-09-20
Playlist: O'Reilly Artificial Intelligence Conference 2017 - San Francisco, CA
Description: 
	A world ruled by machines that are hostile to humanity is not a distant possibility. Complex systems evolve from much simpler forebears, and the design of the systems we are building today is already shaping the future of truly intelligent machines. We are in a defining period of the struggle for human freedom.

Tim O’Reilly draws on lessons from networked platforms such as Amazon, Google, Facebook, Airbnb, Uber, and Lyft to show how our economy and financial markets have also become increasingly managed by algorithms, making the case that income inequality, declining upward mobility, and job losses due to technology are not inevitable; they are the result of design choices we have made in the algorithms that manage our markets. Just as Google constantly updates its algorithms in pursuit of relevant search and ad results and as Facebook wrestles with how to rethink its algorithms for user engagement in response to fake news, we must rewrite the algorithms that shape our economy if we wish to create a more human-centered future.


Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:02,040 --> 00:00:08,280
so the title of my talk is our Skynet

00:00:05,790 --> 00:00:10,890
moment now Skynet of course is the

00:00:08,280 --> 00:00:12,960
hostile AI of the Terminator movies how

00:00:10,890 --> 00:00:15,090
many people here have seen a James

00:00:12,960 --> 00:00:17,430
Cameron's original Terminator or any of

00:00:15,090 --> 00:00:20,850
its sequels so fair number of you but

00:00:17,430 --> 00:00:24,000
for those of you who haven't the first

00:00:20,850 --> 00:00:26,580
these movies 1991 the internet was still

00:00:24,000 --> 00:00:28,470
in its infancy so this idea of this

00:00:26,580 --> 00:00:32,279
global network somehow becoming

00:00:28,470 --> 00:00:34,260
self-aware was science fiction but today

00:00:32,279 --> 00:00:36,839
we're living in that science fiction

00:00:34,260 --> 00:00:39,600
we're all connected billions of people

00:00:36,839 --> 00:00:41,850
billions of devices connected by this

00:00:39,600 --> 00:00:45,359
global network which is increasingly

00:00:41,850 --> 00:00:47,909
infused with AI and of course this has

00:00:45,359 --> 00:00:51,329
led a lot of people to worry quite a bit

00:00:47,909 --> 00:00:53,780
Elon Musk is one of the most notable of

00:00:51,329 --> 00:00:57,959
the people who's expressed fears of AI

00:00:53,780 --> 00:01:01,920
Stephen Hawking Bill Gates Nick Bostrom

00:00:57,959 --> 00:01:04,769
and many others Sam Altman but Elon said

00:01:01,920 --> 00:01:07,380
AI is the most serious threat to the

00:01:04,769 --> 00:01:09,540
survival of the human race and I want to

00:01:07,380 --> 00:01:11,880
unpack this a little bit and talk about

00:01:09,540 --> 00:01:15,030
what's true about it and what's not and

00:01:11,880 --> 00:01:17,250
to put it in a broader context so

00:01:15,030 --> 00:01:20,160
because there are people some of whom

00:01:17,250 --> 00:01:22,590
you've heard from on this stage who are

00:01:20,160 --> 00:01:23,970
actual practitioners of AI who think

00:01:22,590 --> 00:01:26,910
that all those fears are fairly

00:01:23,970 --> 00:01:28,680
overblown you know and ruing said those

00:01:26,910 --> 00:01:31,410
of us on the front line shipping code

00:01:28,680 --> 00:01:33,360
we're excited by AI but we don't see a

00:01:31,410 --> 00:01:34,230
realistic path for our software to

00:01:33,360 --> 00:01:37,440
become sentient

00:01:34,230 --> 00:01:40,920
you know Yann Lacan at Facebook's AI

00:01:37,440 --> 00:01:43,920
research makes very similar comments and

00:01:40,920 --> 00:01:44,640
I love Andrews company says you know

00:01:43,920 --> 00:01:46,680
there's a big difference between

00:01:44,640 --> 00:01:48,600
intelligence sent on sentience there

00:01:46,680 --> 00:01:51,240
could be a race of killer robots in the

00:01:48,600 --> 00:01:53,460
far future but at work on not turning AI

00:01:51,240 --> 00:01:54,570
evil today for the same reason I don't

00:01:53,460 --> 00:01:56,280
worry about the problem of

00:01:54,570 --> 00:01:58,909
overpopulation on Mars

00:01:56,280 --> 00:02:02,640
that might be another dig at Elon Musk

00:01:58,909 --> 00:02:04,170
but you know is a I really like worrying

00:02:02,640 --> 00:02:05,820
about the future of a I really like

00:02:04,170 --> 00:02:07,590
worrying about overpopulation on Mars

00:02:05,820 --> 00:02:09,750
let's let's think a little bit more

00:02:07,590 --> 00:02:13,980
about it where's the Elon actually

00:02:09,750 --> 00:02:16,020
Express as a worry what he has expressed

00:02:13,980 --> 00:02:18,390
are a couple of things and

00:02:16,020 --> 00:02:20,970
one of them is the idea of the runaway

00:02:18,390 --> 00:02:23,220
objective function now when Nick Bostrom

00:02:20,970 --> 00:02:24,840
first introduced this as a thought

00:02:23,220 --> 00:02:28,440
experiment many years ago he used the

00:02:24,840 --> 00:02:30,450
idea of an AI that was you know running

00:02:28,440 --> 00:02:33,210
a factory that was designed to create

00:02:30,450 --> 00:02:35,220
paper clips and the paper clip optimizer

00:02:33,210 --> 00:02:36,720
run awry eventually decides that all

00:02:35,220 --> 00:02:38,670
humanity is in the way and it goes on

00:02:36,720 --> 00:02:41,370
making paper clips forever

00:02:38,670 --> 00:02:43,800
and Elon used the same image except his

00:02:41,370 --> 00:02:46,230
idea is a runaway self-improving

00:02:43,800 --> 00:02:47,550
strawberry picking robot this is hey you

00:02:46,230 --> 00:02:50,430
know would be strawberry fields forever

00:02:47,550 --> 00:02:55,650
you know no room for human beings now

00:02:50,430 --> 00:03:00,000
this is a recent quote this actually is

00:02:55,650 --> 00:03:01,740
a great kind of human fear that goes

00:03:00,000 --> 00:03:06,390
back many many years this is a fabulous

00:03:01,740 --> 00:03:08,010
detail actually of a illustration by

00:03:06,390 --> 00:03:10,050
edmond du lac from A Thousand and One

00:03:08,010 --> 00:03:12,720
Nights oh wow commonly known as the

00:03:10,050 --> 00:03:14,850
Arabian Nights you know about jinns you

00:03:12,720 --> 00:03:16,440
know people could summon jinns they had

00:03:14,850 --> 00:03:18,300
the idea that these beings of great

00:03:16,440 --> 00:03:22,230
power would be subject to their wishes

00:03:18,300 --> 00:03:25,050
and the problem was that the jinns just

00:03:22,230 --> 00:03:26,490
kind of get it wrong they don't quite

00:03:25,050 --> 00:03:28,580
understand what you mean or maybe

00:03:26,490 --> 00:03:29,820
sometimes they're slightly malicious or

00:03:28,580 --> 00:03:32,220
troublemaking

00:03:29,820 --> 00:03:33,890
but they misinterpret your wishes in a

00:03:32,220 --> 00:03:38,190
way that turns out to your disadvantage

00:03:33,890 --> 00:03:39,660
now in popular culture you may some of

00:03:38,190 --> 00:03:42,990
you may have seen Walt Disney's Fantasia

00:03:39,660 --> 00:03:46,350
which is a scene with Mickey Mouse and

00:03:42,990 --> 00:03:47,790
he's got his master's you know spell

00:03:46,350 --> 00:03:50,490
book and he's figured out how to get

00:03:47,790 --> 00:03:53,400
this broom to help him carry the pails

00:03:50,490 --> 00:03:55,320
of water except you know it's literally

00:03:53,400 --> 00:03:57,060
the runaway objective function the

00:03:55,320 --> 00:03:58,440
brooms start multiplying and bringing

00:03:57,060 --> 00:04:00,540
more and more buckets of water until

00:03:58,440 --> 00:04:02,340
there's a flood and he doesn't know what

00:04:00,540 --> 00:04:04,950
to do until he's rescued by his master

00:04:02,340 --> 00:04:07,860
so that's really the runaway objective

00:04:04,950 --> 00:04:10,709
in a nutshell yeah we humanity has been

00:04:07,860 --> 00:04:14,700
worrying about that little reef 4,000

00:04:10,709 --> 00:04:16,859
years so I want to actually I really

00:04:14,700 --> 00:04:18,750
want you to hold this thought because

00:04:16,859 --> 00:04:21,120
I'm gonna come back to it in a couple of

00:04:18,750 --> 00:04:24,180
different ways but before I do that I do

00:04:21,120 --> 00:04:27,450
want to pick up on another point that

00:04:24,180 --> 00:04:29,940
Elon has made which i think is very very

00:04:27,450 --> 00:04:32,250
relevant he said look AI might be the

00:04:29,940 --> 00:04:34,470
that starts World War three that was his

00:04:32,250 --> 00:04:36,720
you know big fear and he actually first

00:04:34,470 --> 00:04:38,400
expressed it when he said something

00:04:36,720 --> 00:04:40,590
about there might be a rogue trading

00:04:38,400 --> 00:04:44,280
algorithm that for example decides that

00:04:40,590 --> 00:04:46,560
you know triggering a war is actually

00:04:44,280 --> 00:04:50,580
the best way to optimize for its

00:04:46,560 --> 00:04:52,740
financial outcome maybe a bit

00:04:50,580 --> 00:04:55,860
far-fetched but then you know you

00:04:52,740 --> 00:04:58,290
realize a is already being used in very

00:04:55,860 --> 00:05:00,420
hostile ways and as a up to fekir if you

00:04:58,290 --> 00:05:03,270
don't know her is a wonderful observer

00:05:00,420 --> 00:05:04,620
of the use of technology by repressive

00:05:03,270 --> 00:05:05,990
governments she has a book called

00:05:04,620 --> 00:05:07,950
Twitter and teargas

00:05:05,990 --> 00:05:10,140
wonderful person to follow on Twitter

00:05:07,950 --> 00:05:12,660
but you said let me say this too many

00:05:10,140 --> 00:05:15,840
worry about what AI as if some

00:05:12,660 --> 00:05:18,540
independent entity will do to us too few

00:05:15,840 --> 00:05:21,510
people worry what power will do with AI

00:05:18,540 --> 00:05:23,370
and of course if you think about it face

00:05:21,510 --> 00:05:25,440
recognition you know here's this you

00:05:23,370 --> 00:05:28,710
know narrow AI technology this

00:05:25,440 --> 00:05:30,800
technology has already been used to find

00:05:28,710 --> 00:05:33,630
people who are then killed and tortured

00:05:30,800 --> 00:05:35,100
that's a real worry and and you know

00:05:33,630 --> 00:05:36,390
some of these saying that follows this

00:05:35,100 --> 00:05:38,910
tracks this around the world is

00:05:36,390 --> 00:05:41,220
happening it is a scary world and it's

00:05:38,910 --> 00:05:43,530
only gonna get worse so we do have to

00:05:41,220 --> 00:05:47,280
think about these risks even of narrow

00:05:43,530 --> 00:05:49,530
AI okay that are used by by power you

00:05:47,280 --> 00:05:51,000
know it doesn't have to go as far as you

00:05:49,530 --> 00:05:53,010
know sort of rogue independent

00:05:51,000 --> 00:05:56,010
algorithms you know we're also

00:05:53,010 --> 00:05:58,830
increasingly seeing autonomous weapons

00:05:56,010 --> 00:06:01,380
you know this actually this and rellis

00:05:58,830 --> 00:06:03,210
it's the Samsung Stra one autonomous gun

00:06:01,380 --> 00:06:06,210
has been in use in the Demilitarized

00:06:03,210 --> 00:06:08,280
Zone in South Korea since 2006 it's

00:06:06,210 --> 00:06:11,910
their way of looking at the overwhelming

00:06:08,280 --> 00:06:14,280
you know the number of humans that the

00:06:11,910 --> 00:06:15,600
North Koreans could throw at them and

00:06:14,280 --> 00:06:17,610
they're saying well we'll use autonomous

00:06:15,600 --> 00:06:19,320
weapons now of course the other side of

00:06:17,610 --> 00:06:21,900
this it's not a very smart autonomous

00:06:19,320 --> 00:06:23,460
weapon it basically shoots anything that

00:06:21,900 --> 00:06:26,280
moves in the in the Demilitarized Zone

00:06:23,460 --> 00:06:28,020
you know and and of course Jana Lacan

00:06:26,280 --> 00:06:29,550
pointed out in the meeting I was with

00:06:28,020 --> 00:06:31,260
him recently and he said you know we've

00:06:29,550 --> 00:06:33,630
actually had autonomous weapons for a

00:06:31,260 --> 00:06:35,310
long time they're called land mines and

00:06:33,630 --> 00:06:37,140
they're pretty stupid and so we banned

00:06:35,310 --> 00:06:39,120
them you know and we might actually be

00:06:37,140 --> 00:06:41,550
better off with smarter autonomous

00:06:39,120 --> 00:06:43,000
weapons but you know that's that's sort

00:06:41,550 --> 00:06:44,680
of a thought experiment that we

00:06:43,000 --> 00:06:46,240
we'll really have to struggle with but

00:06:44,680 --> 00:06:49,170
you know we are getting smarter and

00:06:46,240 --> 00:06:52,300
smarter and potentially more dangerous

00:06:49,170 --> 00:06:55,240
autonomous weapons that will be used by

00:06:52,300 --> 00:06:57,700
people in power and so here's one that I

00:06:55,240 --> 00:06:59,920
came across torrontés it's a it's a test

00:06:57,700 --> 00:07:03,430
it's a stealth drone it kind of looks

00:06:59,920 --> 00:07:05,620
like the b1 invisible weaponized and

00:07:03,430 --> 00:07:07,300
that you know while they say it you know

00:07:05,620 --> 00:07:08,770
it does have limitations on what we give

00:07:07,300 --> 00:07:10,870
it in the mission plan it can only fly

00:07:08,770 --> 00:07:12,520
in certain areas but it does think for

00:07:10,870 --> 00:07:14,350
itself it will navigate and it will

00:07:12,520 --> 00:07:16,750
search for targets that's like something

00:07:14,350 --> 00:07:18,730
out of you know Daniel Suarez this

00:07:16,750 --> 00:07:20,980
science fiction novel kill decision so

00:07:18,730 --> 00:07:22,419
yes there are some near-term worries and

00:07:20,980 --> 00:07:24,940
of course

00:07:22,419 --> 00:07:27,310
wladim Putin's recent remarks put us all

00:07:24,940 --> 00:07:29,140
on notice he basically said artificial

00:07:27,310 --> 00:07:31,270
intelligence is the future not only for

00:07:29,140 --> 00:07:33,550
Russia but all of humankind I don't

00:07:31,270 --> 00:07:35,320
think he meant that just as you know

00:07:33,550 --> 00:07:37,030
well leadership in this wonderful new

00:07:35,320 --> 00:07:40,240
technology is going to lead us into a

00:07:37,030 --> 00:07:42,700
new age of prosperity you know what he

00:07:40,240 --> 00:07:45,910
meant was guess what we found out this

00:07:42,700 --> 00:07:48,520
actual great way to use it as a weapon

00:07:45,910 --> 00:07:49,540
and you know cyber warfare did turn out

00:07:48,520 --> 00:07:51,550
to be a little different than we

00:07:49,540 --> 00:07:52,870
expected nobody was sort of thinking

00:07:51,550 --> 00:07:55,479
that this was going to be a tool of

00:07:52,870 --> 00:07:58,950
disinformation that we'd have fake you

00:07:55,479 --> 00:08:01,630
know astroturf BOTS you know that are

00:07:58,950 --> 00:08:03,610
colonizing social media they're

00:08:01,630 --> 00:08:06,160
influencing the turnout of the election

00:08:03,610 --> 00:08:08,350
that these BOTS would be also exploiting

00:08:06,160 --> 00:08:10,330
loopholes in the way that Facebook's AI

00:08:08,350 --> 00:08:12,640
and and other you know algorithmic

00:08:10,330 --> 00:08:14,860
systems were sort of generating content

00:08:12,640 --> 00:08:18,430
what it was showing to people so yes

00:08:14,860 --> 00:08:22,330
here is weaponized AI turned against us

00:08:18,430 --> 00:08:26,979
and that brings me back to this idea of

00:08:22,330 --> 00:08:29,620
the runaway objective function so when

00:08:26,979 --> 00:08:32,789
you know we don't need things like the

00:08:29,620 --> 00:08:35,979
paperclip optimizer or the strawberry

00:08:32,789 --> 00:08:38,530
you know picking optimizer you know

00:08:35,979 --> 00:08:40,390
here's is a runaway objective function

00:08:38,530 --> 00:08:42,760
facebook says we got to create an ad

00:08:40,390 --> 00:08:45,070
inventory against any possible set of

00:08:42,760 --> 00:08:46,870
keywords right and clearly there were

00:08:45,070 --> 00:08:49,330
some keywords again they weren't

00:08:46,870 --> 00:08:51,910
necessarily really used but the fact is

00:08:49,330 --> 00:08:55,690
you can kind of see here that these

00:08:51,910 --> 00:08:56,970
algorithms have implications that we

00:08:55,690 --> 00:09:00,480
don't yet fully on

00:08:56,970 --> 00:09:03,300
stand which is exactly the point of the

00:09:00,480 --> 00:09:05,610
Arabian mythology of the Djinn that

00:09:03,300 --> 00:09:07,290
there are unexpected consequences my

00:09:05,610 --> 00:09:09,329
wife Jen pahlka likes to say in the

00:09:07,290 --> 00:09:12,389
context of government the only law that

00:09:09,329 --> 00:09:16,319
you can always count on being in effect

00:09:12,389 --> 00:09:18,389
is the law of unintended consequences so

00:09:16,319 --> 00:09:21,689
but I don't ask what does this have to

00:09:18,389 --> 00:09:23,490
do with Skynet and here's where I want

00:09:21,689 --> 00:09:25,110
to kind of get into this speculative

00:09:23,490 --> 00:09:27,060
area I want to lay out a line of

00:09:25,110 --> 00:09:28,259
speculations that I cover in my new book

00:09:27,060 --> 00:09:30,779
which is coming out in a couple of weeks

00:09:28,259 --> 00:09:32,790
called WTF what's the future and why

00:09:30,779 --> 00:09:34,980
it's up to us it's really about a lot of

00:09:32,790 --> 00:09:37,290
things about technology platforms and

00:09:34,980 --> 00:09:38,670
the role that algorithmic systems play

00:09:37,290 --> 00:09:41,269
in managing them and the new

00:09:38,670 --> 00:09:43,500
relationship between humans and and

00:09:41,269 --> 00:09:46,529
whether you call AI or simply

00:09:43,500 --> 00:09:48,660
algorithmic systems and that works in

00:09:46,529 --> 00:09:51,329
our society but it has a particular

00:09:48,660 --> 00:09:55,019
element that I want to lay out and it's

00:09:51,329 --> 00:09:56,759
this idea that what if we're thinking

00:09:55,019 --> 00:10:00,089
about AI in the wrong way

00:09:56,759 --> 00:10:01,980
what if strong AI that fabled AI of the

00:10:00,089 --> 00:10:03,660
future that you know Elon is worried

00:10:01,980 --> 00:10:06,899
about bears the same relationship

00:10:03,660 --> 00:10:09,509
today's narrow AI as multicellular life

00:10:06,899 --> 00:10:12,360
does to prokaryotes it's a simple

00:10:09,509 --> 00:10:15,389
single-celled bacteria versus

00:10:12,360 --> 00:10:17,490
multicellular organisms so this is a

00:10:15,389 --> 00:10:21,750
theory that lynn margulis kind of

00:10:17,490 --> 00:10:24,540
articulated called symbiogenesis and she

00:10:21,750 --> 00:10:28,050
basically made the case which first was

00:10:24,540 --> 00:10:30,540
highly resistant but eventually really

00:10:28,050 --> 00:10:32,309
came to be widely accepted that that are

00:10:30,540 --> 00:10:35,579
actually the the multicellular life

00:10:32,309 --> 00:10:37,680
actually has bacteria inside of it so

00:10:35,579 --> 00:10:41,939
for example the chloroplast inside of a

00:10:37,680 --> 00:10:44,009
of a plant inside of a plant cell has if

00:10:41,939 --> 00:10:46,079
you look at its genetic history is

00:10:44,009 --> 00:10:48,079
actually derived from a particular

00:10:46,079 --> 00:10:52,079
species of bacteria if you look at

00:10:48,079 --> 00:10:54,449
mitochondria they also are descended

00:10:52,079 --> 00:10:56,370
from another kind of bacteria and so

00:10:54,449 --> 00:11:00,029
effectively she points makes the point

00:10:56,370 --> 00:11:03,240
that species form new composite entities

00:11:00,029 --> 00:11:05,759
and of course the biological symbiosis

00:11:03,240 --> 00:11:07,800
that is us doesn't stop there you know

00:11:05,759 --> 00:11:09,089
we're now increasingly aware of

00:11:07,800 --> 00:11:10,740
something they call the microbiome

00:11:09,089 --> 00:11:12,870
there's a great book about it called

00:11:10,740 --> 00:11:15,930
contain multitudes by ed young this is

00:11:12,870 --> 00:11:17,730
all zoology Israeli ecology you know we

00:11:15,930 --> 00:11:19,110
can't fully understand the lives of

00:11:17,730 --> 00:11:21,149
animals without understanding our

00:11:19,110 --> 00:11:22,709
microbes and symbiosis with them when we

00:11:21,149 --> 00:11:24,330
look at beetles and elephants sea

00:11:22,709 --> 00:11:28,050
urchins and earthworms parents and

00:11:24,330 --> 00:11:29,459
friends we see individuals right working

00:11:28,050 --> 00:11:31,980
their way through life as a bunch of

00:11:29,459 --> 00:11:33,630
cells in a single body and driven by a

00:11:31,980 --> 00:11:35,700
single brain operating with a single

00:11:33,630 --> 00:11:37,860
genome this is a pleasant fiction in

00:11:35,700 --> 00:11:41,760
fact we are legion each and every one of

00:11:37,860 --> 00:11:45,029
us always a we and never me he'd Walt

00:11:41,760 --> 00:11:47,610
Whitman am large I contain multitudes so

00:11:45,029 --> 00:11:51,300
if that's true of us could that also be

00:11:47,610 --> 00:11:53,279
true of a future AI you know John Rawls

00:11:51,300 --> 00:11:57,800
says each animal is an ecosystem with

00:11:53,279 --> 00:12:02,070
legs well what is the ecosystem of a

00:11:57,800 --> 00:12:04,050
future AI what is its microbiome so I

00:12:02,070 --> 00:12:06,180
want to take this even further the

00:12:04,050 --> 00:12:08,520
microbiome affects our consciousness

00:12:06,180 --> 00:12:11,339
you know this increasing notion that the

00:12:08,520 --> 00:12:15,180
bacteria in our gut affects our mood but

00:12:11,339 --> 00:12:17,730
guess what we're also tied in our blame

00:12:15,180 --> 00:12:20,279
our brain is plastic and we have new

00:12:17,730 --> 00:12:22,589
knowledge pour it into it all right so

00:12:20,279 --> 00:12:24,600
you know we go from genes to memes think

00:12:22,589 --> 00:12:26,820
about language you acquire language from

00:12:24,600 --> 00:12:30,570
outside it doesn't come built-in to a

00:12:26,820 --> 00:12:34,709
child right and knowledge is passed from

00:12:30,570 --> 00:12:36,810
mind of mine you know by language by by

00:12:34,709 --> 00:12:38,550
the printed word and now ever more

00:12:36,810 --> 00:12:40,350
quickly by the internet and we have

00:12:38,550 --> 00:12:42,600
these Mindstorms that go through our

00:12:40,350 --> 00:12:44,370
collective consciousness more and more

00:12:42,600 --> 00:12:48,300
quickly as a result of this new digital

00:12:44,370 --> 00:12:50,370
world and we are part of this new super

00:12:48,300 --> 00:12:53,610
organism that is now connected by the

00:12:50,370 --> 00:12:55,500
internet so we make a lot of the fact

00:12:53,610 --> 00:12:57,420
that humans perform unsupervised

00:12:55,500 --> 00:13:00,510
learning and we're looking to do that in

00:12:57,420 --> 00:13:02,790
AI but we forget how much of what makes

00:13:00,510 --> 00:13:05,190
us human is acquired from other living

00:13:02,790 --> 00:13:08,070
humans but also from knowledge passed on

00:13:05,190 --> 00:13:09,899
we are the product of a great deal of

00:13:08,070 --> 00:13:11,610
supervised learning a great deal of

00:13:09,899 --> 00:13:13,800
reinforcement learning all these terms

00:13:11,610 --> 00:13:16,320
you hear about an AI apply to people as

00:13:13,800 --> 00:13:19,490
well so the idea that it's just going to

00:13:16,320 --> 00:13:22,140
be magically completely unsupervised is

00:13:19,490 --> 00:13:24,360
just wrong I mean we are not completely

00:13:22,140 --> 00:13:26,190
unsupervised we are quite supervised

00:13:24,360 --> 00:13:29,940
and we're being supervised in new ways

00:13:26,190 --> 00:13:32,430
and new collective ways so if we think

00:13:29,940 --> 00:13:34,490
that our emergent proto AIS are also

00:13:32,430 --> 00:13:36,750
compound beings where does that lead us

00:13:34,490 --> 00:13:39,329
you know first of all you can kind of

00:13:36,750 --> 00:13:42,060
see how the rise of specialized chips

00:13:39,329 --> 00:13:45,690
for narrow AI functions like language

00:13:42,060 --> 00:13:46,800
recognition or image recognition is kind

00:13:45,690 --> 00:13:49,500
of interesting because that's also

00:13:46,800 --> 00:13:51,709
specialized subsystems in the brain but

00:13:49,500 --> 00:13:54,269
you also bring in that whole notion of

00:13:51,709 --> 00:13:57,779
the microbiome and you start say oh

00:13:54,269 --> 00:14:00,750
there are people inside these giant AIS

00:13:57,779 --> 00:14:02,279
so here's a human inside a Google Data

00:14:00,750 --> 00:14:05,010
Center who's actually repairing

00:14:02,279 --> 00:14:08,100
something swapping out a part think of

00:14:05,010 --> 00:14:09,810
that as somebody inside the cell

00:14:08,100 --> 00:14:13,560
think about the programmers who are

00:14:09,810 --> 00:14:15,360
building new you know functions new code

00:14:13,560 --> 00:14:18,870
you know implementing new ideas that

00:14:15,360 --> 00:14:20,730
operations engineers all of us are also

00:14:18,870 --> 00:14:23,459
contributing from the outside every time

00:14:20,730 --> 00:14:26,760
we make a link every time we we choose a

00:14:23,459 --> 00:14:29,910
result that's input into this brain that

00:14:26,760 --> 00:14:32,130
Google is a building so we are

00:14:29,910 --> 00:14:32,970
co-creators of the future with the AIS

00:14:32,130 --> 00:14:35,100
in our midst

00:14:32,970 --> 00:14:37,260
which aren't and may not ever be

00:14:35,100 --> 00:14:39,029
independent beings they may be the

00:14:37,260 --> 00:14:40,709
nervous tissue collecting billions of

00:14:39,029 --> 00:14:42,959
human brains augmenting them with

00:14:40,709 --> 00:14:47,130
silicon and making possible things that

00:14:42,959 --> 00:14:49,260
we have not yet imagined so in this

00:14:47,130 --> 00:14:51,180
regard you also have to realize that we

00:14:49,260 --> 00:14:53,279
can put in this context of the

00:14:51,180 --> 00:14:55,740
corporation itself as a new artificial

00:14:53,279 --> 00:14:57,839
life-form with humans and computers as

00:14:55,740 --> 00:15:01,260
its microbiome and this was even in the

00:14:57,839 --> 00:15:03,300
law in 1888 here in the US the Supreme

00:15:01,260 --> 00:15:04,949
Court said under the designation of

00:15:03,300 --> 00:15:06,360
person there is no doubt that a private

00:15:04,949 --> 00:15:08,399
corporation is included in the

00:15:06,360 --> 00:15:10,560
Fourteenth Amendment such corporations

00:15:08,399 --> 00:15:12,300
are merely associations of individuals

00:15:10,560 --> 00:15:14,160
United for a special purpose and

00:15:12,300 --> 00:15:15,959
permitted to do business under a

00:15:14,160 --> 00:15:18,510
particular name and have a succession of

00:15:15,959 --> 00:15:21,390
members without dissolution so here it

00:15:18,510 --> 00:15:23,370
is a collective being made up of us and

00:15:21,390 --> 00:15:25,920
guess what these new world spanning

00:15:23,370 --> 00:15:28,980
platforms are these collective beings

00:15:25,920 --> 00:15:31,500
made up of us but with an independent

00:15:28,980 --> 00:15:33,270
purpose in existence as we see with

00:15:31,500 --> 00:15:34,800
corporations you can take individual

00:15:33,270 --> 00:15:37,430
humans out but they still have their

00:15:34,800 --> 00:15:39,860
goals which are encoded into them

00:15:37,430 --> 00:15:42,110
and so if you think of Google and

00:15:39,860 --> 00:15:43,730
Facebook as the next evolutionary step

00:15:42,110 --> 00:15:46,790
you can ask yourself interesting

00:15:43,730 --> 00:15:50,089
questions because we understand when we

00:15:46,790 --> 00:15:53,360
look at Google and Facebook that these

00:15:50,089 --> 00:15:55,160
are systems that have algorithmic

00:15:53,360 --> 00:15:57,320
objective functions Google is always

00:15:55,160 --> 00:15:59,720
trying to optimize for relevance both in

00:15:57,320 --> 00:16:01,430
ads and in the content that it shows you

00:15:59,720 --> 00:16:03,470
that you know the search results

00:16:01,430 --> 00:16:06,140
Facebook has been trying to optimize for

00:16:03,470 --> 00:16:09,380
engagement and so then you start

00:16:06,140 --> 00:16:11,180
thinking well what's happening as the

00:16:09,380 --> 00:16:13,399
objective function of another one of

00:16:11,180 --> 00:16:16,250
these great proto a is our financial

00:16:13,399 --> 00:16:17,959
markets here's the Equinox New York for

00:16:16,250 --> 00:16:19,880
data center trillions of dollars change

00:16:17,959 --> 00:16:21,290
hands their looks just like you know the

00:16:19,880 --> 00:16:25,160
pictures you'll see of the Google Data

00:16:21,290 --> 00:16:27,589
Centers right what are we telling those

00:16:25,160 --> 00:16:29,270
systems to optimize for what is the

00:16:27,589 --> 00:16:32,209
objective function of our financial

00:16:29,270 --> 00:16:33,830
markets and our corporations and I think

00:16:32,209 --> 00:16:36,440
the objective function was set by

00:16:33,830 --> 00:16:38,180
economist Milton Friedman in 1970 when

00:16:36,440 --> 00:16:40,070
he wrote an op-ed in The New York Times

00:16:38,180 --> 00:16:42,529
called the social responsibility of

00:16:40,070 --> 00:16:46,010
business is to increase its profits that

00:16:42,529 --> 00:16:49,310
idea took hold in society in the global

00:16:46,010 --> 00:16:50,810
mind and that is the point when you see

00:16:49,310 --> 00:16:53,240
this great divergence between the

00:16:50,810 --> 00:16:54,709
continued rise of productivity right on

00:16:53,240 --> 00:16:56,900
that curve that Steve Jurvetson was

00:16:54,709 --> 00:17:00,350
talking about but the real median family

00:16:56,900 --> 00:17:02,120
income of actual ordinary people right

00:17:00,350 --> 00:17:04,790
because we said we stopped saying

00:17:02,120 --> 00:17:07,160
optimize for people the purpose of an

00:17:04,790 --> 00:17:10,160
economy is actually to serve humans we

00:17:07,160 --> 00:17:12,309
said no it's to serve some number of

00:17:10,160 --> 00:17:16,069
small humans who are part of this new

00:17:12,309 --> 00:17:17,689
you know collective life form so this is

00:17:16,069 --> 00:17:19,670
all explored in a couple of really

00:17:17,689 --> 00:17:23,150
interesting books one is called makers

00:17:19,670 --> 00:17:25,490
and takers by Ron Ofuro Harr another the

00:17:23,150 --> 00:17:27,439
golden passport by Duff McDonald Duff's

00:17:25,490 --> 00:17:29,720
book talks about how Harvard Business

00:17:27,439 --> 00:17:32,120
School kind of spread this ideology

00:17:29,720 --> 00:17:33,620
Ronna really talks about the way that

00:17:32,120 --> 00:17:35,450
this is really at the heart of many of

00:17:33,620 --> 00:17:37,610
our current economic problems is the

00:17:35,450 --> 00:17:39,170
single biggest unexplored reason for

00:17:37,610 --> 00:17:41,300
long-term slower growth as well as

00:17:39,170 --> 00:17:42,860
income inequality is the financial

00:17:41,300 --> 00:17:45,559
system to stop serving the real economy

00:17:42,860 --> 00:17:47,840
and this now serves mainly itself so

00:17:45,559 --> 00:17:50,059
think about this way the financial

00:17:47,840 --> 00:17:51,320
market was once a useful hand made into

00:17:50,059 --> 00:17:53,419
human exchange of goods and

00:17:51,320 --> 00:17:55,159
has become the master and it is the

00:17:53,419 --> 00:17:57,259
master of all the other collective

00:17:55,159 --> 00:17:59,529
intelligence it's Google Facebook Amazon

00:17:57,259 --> 00:18:02,929
they're ultimately answerable to

00:17:59,529 --> 00:18:06,440
financial markets so we have told this

00:18:02,929 --> 00:18:08,480
financial market what to optimize for

00:18:06,440 --> 00:18:10,850
we've given it an objective function and

00:18:08,480 --> 00:18:14,450
we have to ask ourselves is it the right

00:18:10,850 --> 00:18:16,789
one just as Facebook is saying well that

00:18:14,450 --> 00:18:23,149
wasn't quite what we had in mind

00:18:16,789 --> 00:18:24,559
you know when they you know wrote their

00:18:23,149 --> 00:18:26,059
function for engagement and it was

00:18:24,559 --> 00:18:27,710
subverted they're trying to fix it we

00:18:26,059 --> 00:18:29,809
have to fix our financial markets in the

00:18:27,710 --> 00:18:32,330
same way I have a friend now

00:18:29,809 --> 00:18:34,009
unfortunately dead for some years who

00:18:32,330 --> 00:18:35,659
told me something in the early years of

00:18:34,009 --> 00:18:37,759
Macintosh programming that it's always

00:18:35,659 --> 00:18:39,529
stuck with me so the art of debugging is

00:18:37,759 --> 00:18:41,360
figuring out what you really told you

00:18:39,529 --> 00:18:43,789
program to do rather than what you

00:18:41,360 --> 00:18:45,769
thought you told it to do and that's

00:18:43,789 --> 00:18:47,990
what we have to do with the rules that

00:18:45,769 --> 00:18:51,070
we've encoded into our broader society

00:18:47,990 --> 00:18:54,200
are they producing the intended outcome

00:18:51,070 --> 00:18:56,509
this engagement that we have with our

00:18:54,200 --> 00:18:58,429
technology is also the engagement that

00:18:56,509 --> 00:19:00,289
we have to have with our entire society

00:18:58,429 --> 00:19:02,929
and the rules of our society going

00:19:00,289 --> 00:19:06,080
forward is it human hostile or is it

00:19:02,929 --> 00:19:08,090
human enhancing so there's a lot of

00:19:06,080 --> 00:19:10,789
reasons why I'm hopeful but the biggest

00:19:08,090 --> 00:19:13,730
one is that you know bias encoded and

00:19:10,789 --> 00:19:16,279
taken to scale becomes visible we're

00:19:13,730 --> 00:19:18,649
starting to see these biases in our

00:19:16,279 --> 00:19:20,720
society because we are encoding them and

00:19:18,649 --> 00:19:22,940
so when we see things like wow you know

00:19:20,720 --> 00:19:24,799
credit scoring this has all this hidden

00:19:22,940 --> 00:19:26,659
bias in the training data has hidden

00:19:24,799 --> 00:19:28,759
bias in the algorithms we've got to fix

00:19:26,659 --> 00:19:30,590
this we are coming to grips with this

00:19:28,759 --> 00:19:32,210
and we can come to grips with it and it

00:19:30,590 --> 00:19:34,909
can make us better people and it can

00:19:32,210 --> 00:19:37,370
make a better society and so we're gonna

00:19:34,909 --> 00:19:39,259
learn from AI you know this is from

00:19:37,370 --> 00:19:41,419
slide from open a I you know it's Bob

00:19:39,259 --> 00:19:43,220
versus Bob learning from boss learning

00:19:41,419 --> 00:19:46,279
from AI about what will make us better

00:19:43,220 --> 00:19:48,799
most hopeful thing you know I've seen

00:19:46,279 --> 00:19:51,139
was this wonderful line from fan Hui on

00:19:48,799 --> 00:19:53,600
the 37th move of the second game between

00:19:51,139 --> 00:19:56,389
alphago and lee sedol where he said it's

00:19:53,600 --> 00:20:00,289
not a human move I've never seen a human

00:19:56,389 --> 00:20:02,419
play this move so beautiful so beautiful

00:20:00,289 --> 00:20:04,230
you know and that's the thing you know I

00:20:02,419 --> 00:20:07,049
would much rather

00:20:04,230 --> 00:20:09,330
search for the AAI that makes beautiful

00:20:07,049 --> 00:20:11,790
moves that makes beautiful insights into

00:20:09,330 --> 00:20:14,340
the way that we play this game of human

00:20:11,790 --> 00:20:17,010
survival that we play this game of human

00:20:14,340 --> 00:20:19,440
flourishing and it helps us build a

00:20:17,010 --> 00:20:22,260
better future and that should be your

00:20:19,440 --> 00:20:25,110
goal as you build not to build you know

00:20:22,260 --> 00:20:29,490
profit-seeking a eyes that disregard

00:20:25,110 --> 00:20:31,919
humans find a way to build AI that is

00:20:29,490 --> 00:20:34,559
beautiful and that helps us to build a

00:20:31,919 --> 00:20:38,070
better world that's why I say what's the

00:20:34,559 --> 00:20:43,760
future it's up to us thank you very much

00:20:38,070 --> 00:20:43,760

YouTube URL: https://www.youtube.com/watch?v=TWyS2z9xv9c


