Title: AI and open source paired to transform and disrupt with Peter Norvig (Google)
Publication date: 2017-10-02
Playlist: O'Reilly Artificial Intelligence Conference 2017 - San Francisco, CA
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,089 --> 00:00:04,170
hi this is Mike Hendrickson from AI

00:00:02,010 --> 00:00:05,549
conference in San Francisco I'm here

00:00:04,170 --> 00:00:07,410
with Peter Norbeck Peter how you doing

00:00:05,549 --> 00:00:09,889
doing great thanks Mike so you were on

00:00:07,410 --> 00:00:13,110
stage today talking with a young

00:00:09,889 --> 00:00:14,519
up-and-coming AI master basically a

00:00:13,110 --> 00:00:18,210
young kid that has done some incredible

00:00:14,519 --> 00:00:20,760
things and you mentioned that AI you're

00:00:18,210 --> 00:00:22,170
just getting started what did you mean

00:00:20,760 --> 00:00:24,449
cuz you've been doing this for quite a

00:00:22,170 --> 00:00:27,990
while and you're just getting started

00:00:24,449 --> 00:00:32,219
yeah yeah I think we're at the point now

00:00:27,990 --> 00:00:35,190
where it's possible for a kid like Abu

00:00:32,219 --> 00:00:37,829
to do a great application where you can

00:00:35,190 --> 00:00:40,469
say let's look at some data let's build

00:00:37,829 --> 00:00:43,410
a model see what comes out of that and

00:00:40,469 --> 00:00:48,300
get performance that solves an important

00:00:43,410 --> 00:00:50,190
task and we can do that now it was much

00:00:48,300 --> 00:00:52,140
harder to do that five years ago you

00:00:50,190 --> 00:00:54,420
know then you thought a bit more as well

00:00:52,140 --> 00:00:59,129
I got to build a big team I got to go

00:00:54,420 --> 00:01:00,930
out I gotta gather all the data I need

00:00:59,129 --> 00:01:03,090
to have a lot of computing

00:01:00,930 --> 00:01:05,729
infrastructure to get it done it was

00:01:03,090 --> 00:01:08,369
like you can only tackle the biggest of

00:01:05,729 --> 00:01:10,350
tasks with the highest of payoffs now

00:01:08,369 --> 00:01:13,619
you can do almost anything and there's

00:01:10,350 --> 00:01:15,530
more freely available data everything is

00:01:13,619 --> 00:01:18,119
online now and data is being collected

00:01:15,530 --> 00:01:20,009
so there's so much more opportunities

00:01:18,119 --> 00:01:22,740
and we're just starting to see what can

00:01:20,009 --> 00:01:25,049
we do with that and some of it is who's

00:01:22,740 --> 00:01:26,850
creative enough to say here's the

00:01:25,049 --> 00:01:29,460
problem and a solution I can match up

00:01:26,850 --> 00:01:32,939
some of it is we need to invent new

00:01:29,460 --> 00:01:35,579
algorithms some of it is we need to have

00:01:32,939 --> 00:01:39,420
the processing power continue to get

00:01:35,579 --> 00:01:42,420
stronger and some of it is we need the

00:01:39,420 --> 00:01:44,610
sort of cultural institutions to say

00:01:42,420 --> 00:01:46,470
we're going to share this and work

00:01:44,610 --> 00:01:49,470
together on so part of that cultural

00:01:46,470 --> 00:01:51,930
institutions um is like the use of

00:01:49,470 --> 00:01:53,430
open-source hmm do you see that growing

00:01:51,930 --> 00:01:55,920
in the next few years because it seems

00:01:53,430 --> 00:01:59,880
like maybe it's not at the same maturity

00:01:55,920 --> 00:02:03,719
level as other areas I definitely see a

00:01:59,880 --> 00:02:05,490
move towards open source within AI when

00:02:03,719 --> 00:02:07,740
within and within the whole computer

00:02:05,490 --> 00:02:09,869
science industry in general I think

00:02:07,740 --> 00:02:10,690
we're seeing more of that so Abu talked

00:02:09,869 --> 00:02:13,390
about using

00:02:10,690 --> 00:02:16,360
open source in his right right so he was

00:02:13,390 --> 00:02:19,600
able to get started with the open source

00:02:16,360 --> 00:02:22,980
data repository and I think that's

00:02:19,600 --> 00:02:25,810
really crucial to be able to get going

00:02:22,980 --> 00:02:28,240
yet Google we built tensorflow to be

00:02:25,810 --> 00:02:30,520
open source from the ground up because

00:02:28,240 --> 00:02:32,290
we had experience in the past where we

00:02:30,520 --> 00:02:34,510
would develop something internally and

00:02:32,290 --> 00:02:35,860
saying now let's open source it and it

00:02:34,510 --> 00:02:38,110
turned out to be difficult if it's

00:02:35,860 --> 00:02:40,900
walked into other yeah it's just messy

00:02:38,110 --> 00:02:42,400
to you know figure out cut the cord and

00:02:40,900 --> 00:02:44,080
if there's one cord you can cut it but

00:02:42,400 --> 00:02:46,060
if there's a million it's hard to know

00:02:44,080 --> 00:02:47,500
where to cut it so from the start we

00:02:46,060 --> 00:02:48,940
said tensorflow is gonna be something

00:02:47,500 --> 00:02:53,020
that we are going to open source and so

00:02:48,940 --> 00:02:55,000
it's much cleaner and and I think other

00:02:53,020 --> 00:02:56,440
people are doing the same seeing it as

00:02:55,000 --> 00:02:59,350
saying you know what's important is

00:02:56,440 --> 00:03:01,420
building a platform that's gonna go

00:02:59,350 --> 00:03:04,180
forward and the platform better be open

00:03:01,420 --> 00:03:06,280
source so in our new economy I look at

00:03:04,180 --> 00:03:08,050
like there's two major phenomenas with

00:03:06,280 --> 00:03:09,640
companies it seems like there's the

00:03:08,050 --> 00:03:12,400
disruptors the small innovative

00:03:09,640 --> 00:03:13,720
companies that are using AI to disrupt

00:03:12,400 --> 00:03:15,910
industries and then there's the

00:03:13,720 --> 00:03:17,980
Transformers mm-hm the large enterprises

00:03:15,910 --> 00:03:19,690
that are trying to use AI to transform

00:03:17,980 --> 00:03:22,540
their business so they don't get

00:03:19,690 --> 00:03:23,110
disrupted yeah who's gonna win that

00:03:22,540 --> 00:03:25,270
battle

00:03:23,110 --> 00:03:28,720
I mean are they gonna fight each other

00:03:25,270 --> 00:03:31,000
or how's that I think everyone's gonna

00:03:28,720 --> 00:03:33,600
win right yeah yeah so to the extent

00:03:31,000 --> 00:03:36,340
that everyone's gonna improve and the

00:03:33,600 --> 00:03:38,290
big companies can have an advantage in

00:03:36,340 --> 00:03:40,510
that they have access to data that

00:03:38,290 --> 00:03:41,860
others might not have right so there's

00:03:40,510 --> 00:03:44,260
plenty of open source data but there's

00:03:41,860 --> 00:03:47,320
also data that by its very nature can't

00:03:44,260 --> 00:03:49,750
be open source so health records health

00:03:47,320 --> 00:03:51,910
records or even just any interaction

00:03:49,750 --> 00:03:54,700
with customers you have to respect their

00:03:51,910 --> 00:03:56,350
privacy and so if you've already got a

00:03:54,700 --> 00:03:59,970
billion customers you have an advantage

00:03:56,350 --> 00:04:04,090
over somebody who doesn't so I think

00:03:59,970 --> 00:04:06,520
those types of forces are in favor of

00:04:04,090 --> 00:04:08,730
the big companies on the other hand a

00:04:06,520 --> 00:04:11,950
big company can only do so many things

00:04:08,730 --> 00:04:14,020
they can do more than one start-up can

00:04:11,950 --> 00:04:15,760
do but they can't do what a million

00:04:14,020 --> 00:04:16,930
startups can do so out of those million

00:04:15,760 --> 00:04:18,760
startups there's going to be a couple

00:04:16,930 --> 00:04:21,430
that are going to be really successful

00:04:18,760 --> 00:04:22,550
and in those companies then can share

00:04:21,430 --> 00:04:23,870
their algorithms

00:04:22,550 --> 00:04:25,520
doesn't have to be the data but the

00:04:23,870 --> 00:04:26,629
algorithm right to help train that

00:04:25,520 --> 00:04:28,129
billion record setter

00:04:26,629 --> 00:04:29,990
yeah whatever and you know sometimes

00:04:28,129 --> 00:04:32,240
they succeed because of the algorithm

00:04:29,990 --> 00:04:34,159
sometimes they succeed because of the

00:04:32,240 --> 00:04:36,650
data sometimes they succeed just because

00:04:34,159 --> 00:04:38,300
they saw a problem that nobody else had

00:04:36,650 --> 00:04:41,030
addressed you so if you could put on

00:04:38,300 --> 00:04:44,030
your ear far-reaching hat where do you

00:04:41,030 --> 00:04:46,370
think AI will be three years from now I

00:04:44,030 --> 00:04:48,259
mean we keep hearing autonomous vehicles

00:04:46,370 --> 00:04:50,599
we keep hearing all this talk about

00:04:48,259 --> 00:04:52,789
what's gonna happen what do you think in

00:04:50,599 --> 00:04:55,520
the short-term which is actually quite

00:04:52,789 --> 00:04:57,050
long if you look at three years no but

00:04:55,520 --> 00:04:58,940
what do you see in that timeframe

00:04:57,050 --> 00:05:03,020
happening that's gonna be significantly

00:04:58,940 --> 00:05:04,879
better for everyone I think that within

00:05:03,020 --> 00:05:09,759
three years we'll see more of this

00:05:04,879 --> 00:05:12,919
transition to a assistant based

00:05:09,759 --> 00:05:14,930
interactions and operating systems right

00:05:12,919 --> 00:05:17,629
where it looks like now we're just

00:05:14,930 --> 00:05:18,889
starting to get into that where the

00:05:17,629 --> 00:05:21,460
industry has gone through various phases

00:05:18,889 --> 00:05:27,800
you start with mainframes you go to pcs

00:05:21,460 --> 00:05:29,180
then you go to phones and now we've a

00:05:27,800 --> 00:05:31,819
lot of people still have their phones

00:05:29,180 --> 00:05:33,919
but they also may have a speaker that

00:05:31,819 --> 00:05:37,880
they talk to or an IOT device or yeah

00:05:33,919 --> 00:05:41,090
and so and that works to some degree I

00:05:37,880 --> 00:05:42,680
think it will get better to the point

00:05:41,090 --> 00:05:45,110
where it really crosses the threshold to

00:05:42,680 --> 00:05:47,419
say this is really convenient to have a

00:05:45,110 --> 00:05:50,150
conversation rather than be poking at

00:05:47,419 --> 00:05:53,120
icons and or we can have to disentangle

00:05:50,150 --> 00:05:56,419
the technology from cultural and

00:05:53,120 --> 00:05:57,919
political like organizational things

00:05:56,419 --> 00:06:00,020
that are gonna happen because I'm

00:05:57,919 --> 00:06:02,779
sharing records of making them better in

00:06:00,020 --> 00:06:04,550
cities is that there's a point where

00:06:02,779 --> 00:06:07,639
you've crossed the continuum from cool

00:06:04,550 --> 00:06:10,610
to creepy and somewhere in there you

00:06:07,639 --> 00:06:13,310
know unfortunate politics happen that's

00:06:10,610 --> 00:06:16,250
right so you know it's important for

00:06:13,310 --> 00:06:19,310
every company to be good Shepherds for

00:06:16,250 --> 00:06:22,789
the data that they have but there's also

00:06:19,310 --> 00:06:25,580
an opportunity to say if we can share

00:06:22,789 --> 00:06:27,249
this data we can learn across multiple

00:06:25,580 --> 00:06:28,689
people and

00:06:27,249 --> 00:06:32,229
can learn from you and that will help me

00:06:28,689 --> 00:06:33,009
and vice versa we gotta figure out ways

00:06:32,229 --> 00:06:34,719
to do that

00:06:33,009 --> 00:06:38,229
in such a way that preserves your

00:06:34,719 --> 00:06:40,599
privacy but it gives a societal benefit

00:06:38,229 --> 00:06:41,889
to everybody and if you get that wrong

00:06:40,599 --> 00:06:45,009
then it is creepy

00:06:41,889 --> 00:06:47,169
yeah like co2 emissions in the air it

00:06:45,009 --> 00:06:49,299
seems like it's good for everyone I mean

00:06:47,169 --> 00:06:51,429
no it's gonna not benefit from that but

00:06:49,299 --> 00:06:53,289
there's going to be wrangling over who

00:06:51,429 --> 00:06:56,789
does that and where yeah yeah yeah

00:06:53,289 --> 00:07:01,209
that's an interesting thing happening so

00:06:56,789 --> 00:07:04,479
AI what do you think the industry that

00:07:01,209 --> 00:07:08,860
is most going to use AI the best and

00:07:04,479 --> 00:07:11,589
quickest is it medical is it consumer is

00:07:08,860 --> 00:07:18,639
it I think there's a lot of headroom for

00:07:11,589 --> 00:07:20,319
consumer of and I think we'll see less

00:07:18,639 --> 00:07:22,179
of a clear boundary between what counts

00:07:20,319 --> 00:07:23,559
as AI and what doesn't right to you

00:07:22,179 --> 00:07:25,599
we're already seeing that in terms of

00:07:23,559 --> 00:07:27,459
marketing of people always want to put

00:07:25,599 --> 00:07:30,369
the brand names of the hot new

00:07:27,459 --> 00:07:32,139
technology on whatever they do no matter

00:07:30,369 --> 00:07:35,169
how seriously they do it

00:07:32,139 --> 00:07:42,189
so we'll see more AI in deep learning

00:07:35,169 --> 00:07:46,599
has a brand name I think sometimes it's

00:07:42,189 --> 00:07:51,269
not so much the technology itself as it

00:07:46,599 --> 00:07:53,589
is more of the mindset of moving from

00:07:51,269 --> 00:07:57,759
you know sort of in traditional

00:07:53,589 --> 00:08:00,279
programming the mindset is an engineer

00:07:57,759 --> 00:08:02,799
sits down and writes out the step step

00:08:00,279 --> 00:08:05,499
by step and then we're kind of stuck

00:08:02,799 --> 00:08:07,360
with that and it's kind of this boolean

00:08:05,499 --> 00:08:10,709
logic it either does it or it doesn't

00:08:07,360 --> 00:08:14,529
and with machine learning it's more like

00:08:10,709 --> 00:08:16,119
well the engineers kind of guide us but

00:08:14,529 --> 00:08:18,009
really it's the data that's going to

00:08:16,119 --> 00:08:19,629
make the decisions and it's going to be

00:08:18,009 --> 00:08:22,149
very fluid it's going to change very

00:08:19,629 --> 00:08:25,179
rapidly and it's more probabilistic

00:08:22,149 --> 00:08:28,239
rather than boolean and so I think those

00:08:25,179 --> 00:08:30,610
mindset change is more important than

00:08:28,239 --> 00:08:32,199
the actual techniques and if you get to

00:08:30,610 --> 00:08:34,750
the point where you say we're doing

00:08:32,199 --> 00:08:37,419
experiments every day and we're trying

00:08:34,750 --> 00:08:37,959
to improve every day then that's the

00:08:37,419 --> 00:08:39,510
important thing

00:08:37,959 --> 00:08:41,010
and whatever technology

00:08:39,510 --> 00:08:43,080
you use to implement that it's less

00:08:41,010 --> 00:08:45,630
important than the mindset so do you

00:08:43,080 --> 00:08:47,520
think the the demo today the efekta I

00:08:45,630 --> 00:08:49,530
think yeah the woman who talked about

00:08:47,520 --> 00:08:52,560
emotions in AI mm-hmm do you think

00:08:49,530 --> 00:08:54,360
that's a area that's needing to get more

00:08:52,560 --> 00:08:56,430
research done in because it seems like

00:08:54,360 --> 00:08:59,730
it's the harder harder nut to crack in

00:08:56,430 --> 00:09:00,240
this this space yeah that's it so I

00:08:59,730 --> 00:09:03,390
think that's great

00:09:00,240 --> 00:09:06,960
and I think that's important and we want

00:09:03,390 --> 00:09:11,340
to have good conversations or dialogues

00:09:06,960 --> 00:09:14,760
with our systems and up to now we've

00:09:11,340 --> 00:09:17,460
really forced the the user to do most of

00:09:14,760 --> 00:09:19,020
the work right train the user you got to

00:09:17,460 --> 00:09:20,520
press this button you got to type this

00:09:19,020 --> 00:09:22,500
thing in and you got to do it exactly

00:09:20,520 --> 00:09:24,990
the way the computer wants and if you

00:09:22,500 --> 00:09:27,570
get one things wrong it's your fault I

00:09:24,990 --> 00:09:30,030
think we want to change that now and say

00:09:27,570 --> 00:09:31,800
you know what if you get it wrong the

00:09:30,030 --> 00:09:34,260
fault is shared and if there's a

00:09:31,800 --> 00:09:36,990
breakdown in communication the system

00:09:34,260 --> 00:09:39,540
should be able to fix itself and so some

00:09:36,990 --> 00:09:42,090
of that is having more fluid

00:09:39,540 --> 00:09:44,840
communication modes maybe I'm talking

00:09:42,090 --> 00:09:48,150
rather than typing or selecting icons

00:09:44,840 --> 00:09:50,960
and some of it is understanding what's

00:09:48,150 --> 00:09:53,340
going on at that deeper level right so

00:09:50,960 --> 00:09:57,510
you should be able to know that I'm

00:09:53,340 --> 00:09:59,940
frustrated and the company you mentioned

00:09:57,510 --> 00:10:02,790
does that by looking at your face and

00:09:59,940 --> 00:10:04,350
understanding your emotions I think even

00:10:02,790 --> 00:10:07,290
the software we have today you know even

00:10:04,350 --> 00:10:09,060
if I didn't have a camera turned on you

00:10:07,290 --> 00:10:11,670
know if I press the same button four

00:10:09,060 --> 00:10:14,340
times and with it I get the same error

00:10:11,670 --> 00:10:15,750
message it should know I'm frustrated

00:10:14,340 --> 00:10:17,370
and it shouldn't give me the same error

00:10:15,750 --> 00:10:20,280
message every single time it should say

00:10:17,370 --> 00:10:21,780
okay I'll try something else yeah and

00:10:20,280 --> 00:10:25,110
the same with the chat BOTS as well yeah

00:10:21,780 --> 00:10:26,670
yeah so if you and I sat down next year

00:10:25,110 --> 00:10:29,580
at this time Peter what would you like

00:10:26,670 --> 00:10:32,130
to say has significantly changed in the

00:10:29,580 --> 00:10:35,910
market and specifically with Google and

00:10:32,130 --> 00:10:40,200
you're AI work it's hard to predict

00:10:35,910 --> 00:10:42,380
especially in the future I think one of

00:10:40,200 --> 00:10:46,590
the things we're really seeing is

00:10:42,380 --> 00:10:49,980
pushing on our cloud platform to say we

00:10:46,590 --> 00:10:52,340
want to help more businesses

00:10:49,980 --> 00:10:55,200
take advantage of these technologies and

00:10:52,340 --> 00:10:57,240
take advantage of these api's I think

00:10:55,200 --> 00:11:00,180
we've done a pretty good job of

00:10:57,240 --> 00:11:03,840
providing tools that experts can use and

00:11:00,180 --> 00:11:06,960
we saw this morning Abu taking good

00:11:03,840 --> 00:11:10,280
advantage of those tools I think we've

00:11:06,960 --> 00:11:12,900
got a ways to go yet to say we want

00:11:10,280 --> 00:11:14,550
everybody in every industry to be able

00:11:12,900 --> 00:11:17,550
to take advantage of this but you know

00:11:14,550 --> 00:11:20,220
without getting themselves to the expert

00:11:17,550 --> 00:11:22,830
level they should be able to say I want

00:11:20,220 --> 00:11:24,870
to plug in what you have into my

00:11:22,830 --> 00:11:26,250
existing workflow and make it better no

00:11:24,870 --> 00:11:28,140
recommendation engine or whatever

00:11:26,250 --> 00:11:29,310
they're looking right right right right

00:11:28,140 --> 00:11:33,050
excellent we will look forward to that

00:11:29,310 --> 00:11:33,050
conversation next year ok can you then

00:11:39,769 --> 00:11:41,829

YouTube URL: https://www.youtube.com/watch?v=OGIEn-orERQ


