Title: How to escape saddle points efficiently, Michael Jordan (UC Berkeley)
Publication date: 2017-09-22
Playlist: O'Reilly Artificial Intelligence Conference 2017 - San Francisco, CA
Description: 
	Many new theoretical challenges have arisen in the area of gradient-based optimization for large-scale data analysis, driven by the needs of applications and the opportunities provided by new hardware and software platforms. Drawing on work undertaken with Chi Jin, Rong Ge, Praneeth Netrapalli, and Sham Kakade, Michael Jordan shares recent research on the avoidance of saddle points in high-dimensional nonconvex optimization.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:03,150 --> 00:00:07,080
so here is the theorem it's due to

00:00:04,740 --> 00:00:09,750
Nesterov on on just first order saddle

00:00:07,080 --> 00:00:12,030
uh first story stationary points okay

00:00:09,750 --> 00:00:14,310
so let's assume that we have a gradient

00:00:12,030 --> 00:00:15,570
Lipschitz function so if you're know

00:00:14,310 --> 00:00:17,220
some mathematics you'll recognize that

00:00:15,570 --> 00:00:20,940
the gradient doesn't change very much

00:00:17,220 --> 00:00:23,010
between points x1 and x2 a stationary

00:00:20,940 --> 00:00:25,080
point is not exactly the gradient equal

00:00:23,010 --> 00:00:26,880
to zero that would be that you would

00:00:25,080 --> 00:00:28,440
only converge to that asymptotically we

00:00:26,880 --> 00:00:30,000
put a little bowl of size epsilon around

00:00:28,440 --> 00:00:31,590
it and we ask how far do you hit how

00:00:30,000 --> 00:00:33,539
time how long does it take to hit that

00:00:31,590 --> 00:00:34,800
ball and then we you know at 4:00 in the

00:00:33,539 --> 00:00:36,989
epsilon and that gives us a rate of

00:00:34,800 --> 00:00:38,610
convergence into the optimum and that

00:00:36,989 --> 00:00:41,000
rate was established by Nesterov who

00:00:38,610 --> 00:00:43,559
showed that it's 1 over epsilon squared

00:00:41,000 --> 00:00:45,630
ok so that's a pretty good rate it's not

00:00:43,559 --> 00:00:47,309
super fast but it's pretty good but the

00:00:45,630 --> 00:00:49,559
key here is there's no dimension in that

00:00:47,309 --> 00:00:51,420
equation right dimension it's

00:00:49,559 --> 00:00:54,540
independent of dimension so that is the

00:00:51,420 --> 00:00:56,700
amazing fact about gradient descent um

00:00:54,540 --> 00:00:58,350
ok so let's move on to the main point of

00:00:56,700 --> 00:01:01,680
this talk oh what about second order

00:00:58,350 --> 00:01:03,360
stationary points ok so we're going to

00:01:01,680 --> 00:01:05,219
be interested in saddle points in

00:01:03,360 --> 00:01:07,560
particular we could be interested in

00:01:05,219 --> 00:01:09,930
local minima so we're gonna now look at

00:01:07,560 --> 00:01:11,729
a little bit more smoothness so we can

00:01:09,930 --> 00:01:13,770
prove a theorem so we're have Hessian

00:01:11,729 --> 00:01:14,940
Lipschitz and the second-order

00:01:13,770 --> 00:01:16,590
stationary point is just an obvious

00:01:14,940 --> 00:01:18,210
extension of the first-order one the

00:01:16,590 --> 00:01:20,340
gradient again is expected to be not

00:01:18,210 --> 00:01:21,659
zero but close to zero and the minimum

00:01:20,340 --> 00:01:24,000
eigen value is not expected to be

00:01:21,659 --> 00:01:25,439
strictly or greater than equal to zero

00:01:24,000 --> 00:01:27,780
but give ourselves a little fudge room

00:01:25,439 --> 00:01:30,299
to again get a rate okay so we can see

00:01:27,780 --> 00:01:31,850
how fast we approach this this value so

00:01:30,299 --> 00:01:34,170
here's the algorithm we're gonna analyze

00:01:31,850 --> 00:01:35,490
it we're just going to call it perturb

00:01:34,170 --> 00:01:38,009
gradient ascent it's just great in a

00:01:35,490 --> 00:01:39,540
sense that's equation four alright but

00:01:38,009 --> 00:01:42,390
occasionally we will add a little bit of

00:01:39,540 --> 00:01:43,680
random noise and we will add it when the

00:01:42,390 --> 00:01:45,600
gradient is small that's at the bottom

00:01:43,680 --> 00:01:48,240
of the page and we will do it

00:01:45,600 --> 00:01:49,290
infrequently once at every T steps for T

00:01:48,240 --> 00:01:51,150
as a parameter we can tell you how to

00:01:49,290 --> 00:01:52,829
set alright so we'll do this in this

00:01:51,150 --> 00:01:54,780
particular theorem uniformly from a ball

00:01:52,829 --> 00:01:56,490
that's not necessary we could analyze

00:01:54,780 --> 00:01:58,649
other things this is done for simplicity

00:01:56,490 --> 00:02:00,420
okay so this is not stochastic reading

00:01:58,649 --> 00:02:02,970
exactly stochastic rate has noise on

00:02:00,420 --> 00:02:05,700
every step this is what we needed appeal

00:02:02,970 --> 00:02:07,619
to get a theorem okay and the theorem is

00:02:05,700 --> 00:02:09,179
amazing so I think you even you know

00:02:07,619 --> 00:02:11,869
this algorithm is worth kind of thinking

00:02:09,179 --> 00:02:11,869

YouTube URL: https://www.youtube.com/watch?v=jMzFcV3xVdg


