Title: Fast-forwarding AI in the data center - Lisa Spelman (Intel)
Publication date: 2017-09-20
Playlist: O'Reilly Artificial Intelligence Conference 2017 - San Francisco, CA
Description: 
	Lisa Spelman explains how businesses are already benefiting from the industryâ€™s most flexible and most optimized solutions for AI and how Intel is fostering the continued growth of the AI ecosystem so that you too can fast-forward AI in the data center.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,110 --> 00:00:04,160
I wanted to share a little bit about the

00:00:02,300 --> 00:00:05,870
scaling and performance that you can

00:00:04,160 --> 00:00:08,389
achieve with big deals so it's not just

00:00:05,870 --> 00:00:11,839
a story about what JD dot-com is doing

00:00:08,389 --> 00:00:13,879
so the first chart over here shows node

00:00:11,839 --> 00:00:16,610
scaling with big deals so if you look

00:00:13,879 --> 00:00:18,560
across image net and ResNet you get near

00:00:16,610 --> 00:00:21,320
linear scaling when you go from 8 nodes

00:00:18,560 --> 00:00:23,300
to 16 nodes again if you think of the

00:00:21,320 --> 00:00:26,000
use case here putting it on top of a

00:00:23,300 --> 00:00:27,980
Hadoop cluster the scaling capability

00:00:26,000 --> 00:00:30,170
and the continued performance delivery

00:00:27,980 --> 00:00:33,050
as you get to larger and larger type

00:00:30,170 --> 00:00:35,060
node sizes is really important and

00:00:33,050 --> 00:00:37,970
allows you to again deliver that same

00:00:35,060 --> 00:00:39,590
performance at greater scale the second

00:00:37,970 --> 00:00:41,570
chart is showing the generational

00:00:39,590 --> 00:00:43,400
performance increase improvements with

00:00:41,570 --> 00:00:45,590
big DL so this is against previous

00:00:43,400 --> 00:00:48,170
generation hardware compared to the

00:00:45,590 --> 00:00:49,850
latest generation and across a couple of

00:00:48,170 --> 00:00:51,649
you know common model training

00:00:49,850 --> 00:00:53,480
frameworks you see that you can get in

00:00:51,649 --> 00:00:56,809
up to 72 percent performance

00:00:53,480 --> 00:00:58,989
improvements jvcom is not the only cloud

00:00:56,809 --> 00:01:02,149
service provider that's moving this into

00:00:58,989 --> 00:01:03,829
production Baidu has also announced just

00:01:02,149 --> 00:01:06,260
last week that they're going to start

00:01:03,829 --> 00:01:08,630
offering it the big-deal toolset to all

00:01:06,260 --> 00:01:10,190
of their customers as well on industry

00:01:08,630 --> 00:01:13,270
standard hardware that they already have

00:01:10,190 --> 00:01:13,270
in the data center today

00:01:19,520 --> 00:01:21,579

YouTube URL: https://www.youtube.com/watch?v=0vYZbfx00D0


