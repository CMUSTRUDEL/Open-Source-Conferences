Title: The inevitable merger of IQ and EQ in technology - Rana el Kaliouby (Affectiva)
Publication date: 2017-09-20
Playlist: O'Reilly Artificial Intelligence Conference 2017 - San Francisco, CA
Description: 
	Rana el Kaliouby lays out a vision for an emotion-enabled world of technology, sharing the inner workings of a multimodal emotion sensing platform that identifies emotions through facial expressions and tone of voice. Along the way, Rana explores the broad applications and ethical implications of this technology. 

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,580 --> 00:00:06,250
we just added speech to the mix this is

00:00:03,340 --> 00:00:08,980
about a couple of days old now where we

00:00:06,250 --> 00:00:11,289
are able to understand the emotions from

00:00:08,980 --> 00:00:13,059
your tone of voice from how you're

00:00:11,289 --> 00:00:15,910
saying the words so it's things like

00:00:13,059 --> 00:00:18,340
tone energy pitch tempo loudness and

00:00:15,910 --> 00:00:19,960
we're able to map that into a number of

00:00:18,340 --> 00:00:27,550
emotional states I'll show you a couple

00:00:19,960 --> 00:00:30,130
of examples this is an example obviously

00:00:27,550 --> 00:00:32,169
of laughter detection if you noticed the

00:00:30,130 --> 00:00:34,150
laughter bar went up but also arousal

00:00:32,169 --> 00:00:37,810
which is a important dimension of

00:00:34,150 --> 00:00:39,640
emotions as well it and it fires with

00:00:37,810 --> 00:00:42,160
the intensity of the emotion regardless

00:00:39,640 --> 00:00:43,899
of how positive or negative it is the

00:00:42,160 --> 00:00:45,820
next one is an example that I think a

00:00:43,899 --> 00:00:47,829
lot of us resonate with you're talking

00:00:45,820 --> 00:00:51,280
to an automated agent it's putting you

00:00:47,829 --> 00:00:53,140
on hold and you're not happy are you

00:00:51,280 --> 00:00:56,260
kidding me I want to speak to a human I

00:00:53,140 --> 00:00:59,110
hate being on hold so you can see that

00:00:56,260 --> 00:01:01,000
the anger bar fired up there now the

00:00:59,110 --> 00:01:03,040
next example is a clear demonstration

00:01:01,000 --> 00:01:05,289
that we're not looking at all at the

00:01:03,040 --> 00:01:08,530
words that are being said but it's in

00:01:05,289 --> 00:01:15,490
how you're saying the words lavender

00:01:08,530 --> 00:01:17,020
body wash makes my poor single we're in

00:01:15,490 --> 00:01:20,259
the process of improving these

00:01:17,020 --> 00:01:23,409
algorithms so we've collected about 6

00:01:20,259 --> 00:01:25,899
million face faces and speech or audio

00:01:23,409 --> 00:01:28,179
sessions from 87 countries around the

00:01:25,899 --> 00:01:29,920
world and this data set serves as our

00:01:28,179 --> 00:01:32,409
training and validation data set and

00:01:29,920 --> 00:01:35,079
it's all spontaneous so these examples

00:01:32,409 --> 00:01:36,579
of the guy we're obviously acted because

00:01:35,079 --> 00:01:39,429
we don't have permission to share a lot

00:01:36,579 --> 00:01:41,799
of our data but the data we have is just

00:01:39,429 --> 00:01:43,630
spontaneous people like emoting on their

00:01:41,799 --> 00:01:46,840
phones or driving around I don't know

00:01:43,630 --> 00:01:48,639
Tokyo or you know video interviews and

00:01:46,840 --> 00:01:50,590
and and it's very important that this

00:01:48,639 --> 00:01:52,950
data is natural and collected in the

00:01:50,590 --> 00:01:52,950

YouTube URL: https://www.youtube.com/watch?v=LPT4r4AnFZA


