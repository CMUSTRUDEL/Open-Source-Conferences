Title: Empowering Quants to Trade Faster: from Excel Files to Data Packages - Aneesh Karve (Quilt Data)
Publication date: 2017-10-19
Playlist: Strata Solutions Showcase Theater 2017
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	                              my name is Aneesh and I am the chief                               technology officer at quilt data and                               quilts mission is to define the standard                               unit of enterprise data so that people                               and machines can learn faster and I'm                               here to talk today about a case study                               that we did with a fortune                                        titled empowering quants to trade faster                               so most modern trading firms modern                                investment banks will tell you that                                they're data-driven but the fact of the                                matter is if you actually look at their                                data systems their data is a mess and                                it's a mess scattered across dozens of                                different data sources dozens of job                                titles and dozens of organizational                                silos and the result of this data                                confusion of this bird's nest that                                you're looking at is that no one in the                                organization has a complete and accurate                                picture of the latest data and in a                                world like quantitative finance where                                people are looking for leading                                indicators and where milliseconds matter                                if you miss the data you miss the trade                                and you miss the profit on that trade                                and so I'm here to talk to you today                                about some really interesting new                                technological paradigms that we're                                pioneering and it's the idea of a data                                registry or a data router and the                                purpose of a data router is to integrate                                all of your enterprise data sources so                                that all the analysts are on the same                                page the surprising insight that I'm                                going to share with you today is this                                radical idea that data is the new code                                people say did is the new oil that's old                                for me the fact of the matter is in                                     we have great source code management                                paradigms but really poor data                                management paradigms and I'm gonna talk                                to you about some of the new paradigms                                we're introducing to make data                                management smoother and faster so our                                client is GFS Bank this is a fortune                                    bank there are global banking and                                financial services firm with over                                      a half trillion dollars in assets under                                management we wanted to get this case                                study out fast so I've proxy the name of                                the bank as GFS bank let me talk to you                                a little bit about their existing data                                pipeline so you have the parent bank                                purchasing data assets for use as                                leading indicators and these are really                                interesting things there like analyses                                of satellite images of Walmart parking                                lots right so looking at Walmart parking                                lots they're looking at oil refineries                                and there                                and to get a sense of what is supplying                                what is demand forthcoming and then they                                take these assets and they distribute                                them to about                                                       subsidiaries inside of the parent bank                                and the goal of these assets through a                                purpose these assets is to feed                                predictive models and here's the life of                                a single excel file a purchased asset                                before they implemented quilts as a                                solution so the thing to note really                                instead of getting caught up in the                                individual steps this is a very manual                                error-prone process so the bank is                                acquiring assets very expensive data                                engineers are then uploading these files                                to blob storage they're then taking                                links to blob storage posting that to                                the wiki the links break the files get                                corrupted every quant needs a different                                API to access the data once they finally                                do access the data then they can                                generate a model and if it isn't too                                late they tend to try to trade on that                                data and so the real thing to get here                                is that if you have a data management                                pipeline which is manually driven and                                labor-intensive you're not helping York                                wants to maximize their return on data                                so we're gonna talk through the                                logistics of how we helps to automate                                their data pipeline and the real thing                                that I want everybody to understand is                                that modern data science the tragedy of                                modern data science is that there's not                                a lot of science involved and the reason                                is                                                                    data prep and so this is a Jupiter                                 notebook which is a workhorse in data                                 science where GFS bank does a lot of                                 their quantitative analysis and if you                                 actually x-ray the code you'll see that                                 most of the code is finding cleaning and                                 organizing files and so the idea here is                                 that if we want to make these precious                                 resource quants data scientists inside                                 of our organization more effectively we                                 need to get them off the firing line of                                 data prep so that instead of their job                                 being just                                                                                                                                      accomplish that transformation so data                                 prep is a drag and what do we do next                                 well the answer is instead of having                                 quants work with files we want them to                                 work with something called a data                                 package which were pioneering I                                 mentioned earlier that we're helping to                                 define the standard unit of enterprise                                 data and the idea behind a data package                                 that you can take any data dependencies                                 that your analysts need to complete an                                 analysis they can be Excel files image                                 database tables and you roll them up                                 into the single tracked versioned                                 deployable asset so a data package at                                 it's very core is virtualized data plus                                 metadata that tracks where everything                                 came from does anybody know why                                 virtualization is important in this                                 context                                 so at GFS bank we have quants working in                                 Python quants working at our quants                                 working in excel and it's really painful                                 if every client needs us has a slightly                                 different method that they need to                                 access the data so the key value of                                 virtualization is this that this data is                                 now consistent across Python across R                                 across bar across sales teams across                                 engineering teams across accounting                                 teams so that everybody has the same                                 complete and accurate picture of the                                 data and here at a code level is the                                 transformation this is the difference                                 for the analyst for the quants right on                                 the left hand side when they're trying                                 to get their data from files again most                                 of their time is spent in data prep on                                 the right hand side they import their                                 data package and they start working so                                 really what we're doing with packages is                                 reducing the time to insight and to give                                 you a sense of head to head of what the                                 bank had before they implemented quilts                                 as a solution so on the Left we have the                                 super manual asset pipeline which is                                 both error-prone and time-consuming and                                 on the right you can see it isn't                                 perfect yet but the key point there is                                 that we get to that orange step on the                                 bottom faster and when you are looking                                 for leading indicators that you want to                                 trade and make money on if you spend                                     of your time and data prep by the time                                 you're done your leading indicator is a                                 lagging indicator right so quilts job                                 really and the value that we provide to                                 GFS is we help their quants get data                                 into code and one of the things I'm                                 going to talk about is that you as an                                 organization should be thinking about                                 how you maximize your return on data                                 don't worry about measuring how much                                 data you can collect worry about how                                 quickly you can leverage the data assets                                 that you have and so the big change and                                 the real numbers are is that once you                                 move away from numerous and confusing                                 files into singular deployable data                                 packages you have to do about                                            amount of data prep                                 your i/o gets                                                         all the data is serialized and the key                                 thing your quants can do their job on                                 better information faster some of the                                 data engineers at GFS bank have                                 described quilt as a seamless way to                                 make Kwan's successful and there were a                                 couple of surprising lessons that we                                 learned in the process of implementing                                 the solution so the first surprise is                                 that don't rely although data scientists                                 and quants are the key beneficiaries of                                 data packages they're not the champions                                 quants are too busy worried about                                 worrying about how to make things work                                 to think about infrastructure so this is                                 a great case study in how multi                                 functional teams help one another solve                                 problems that aren't natural but are                                 hugely productive when you do solve them                                 so you want a data engineering function                                 or an IT function that is only thinking                                 about infrastructure while you're quants                                 are thinking about modeling so again the                                 surprising insight here the people who                                 benefit the most from packages are not                                 the champions it's the people who have                                 the time and the skillset to think about                                 infrastructure another thing which I'll                                 show you when we get to the product demo                                 is that users actually document data                                 when you have a data catalog where                                 people can see what other people are                                 doing shame is the most powerful                                 motivator in human behavior and if                                 people are just dumping things no                                 database or dumping things into a                                 Dropbox folder there's no accountability                                 but when you have what I'm going to show                                 you in just a moment when you have a                                 shared data catalog which is what quote                                 creates for you you can now see on a per                                 package basis you can see who created                                 the package you can see when they                                 created it each package has a distinct                                 version that identifies that data                                 uniquely it's one line of code to grab                                 that data and one line of code to inject                                 that data into code and here's the                                 surprising thing I was just talking                                 about documentation when people know                                 that other people are going to look at                                 their work they actually take the time                                 to document the schemas to document the                                 use of the data and this is again this                                 idea behind return on data it doesn't                                 matter how much data you're collecting                                 it matters how much data you can                                 actually use and documentation is a                                 critical portion of that                                 so I guess the last surprising insight                                 here is that data is the new code right                                 and if we look at the trajectory of                                 compute in the last three years docker                                 containers have become a standard unit                                 of computer and in the next three years                                 we believe that a standard unit of data                                 will emerge that will be an order of                                 magnitude gained in the productivity                                 that analysts have working with data and                                 the thought I want to leave you with is                                 this idea of maximizing return on data                                 return on data means that for every                                 datum that you collect your decisions                                 get better and your organization is able                                 to make more money and the                                 characteristics of that are your data                                 should be discoverable because it's                                 automatically catalog it should be                                 reproducible because it's versioned it                                 should be auditable because there's a                                 full access log again automatically as a                                 result of using data packages it should                                 be compliant and we've built as part of                                 quilt certain compliance modules for                                 gdpr and Omax so that when you package                                 that data the engineers the data                                 engineers can guarantee that it only                                 contains compliant data and of course it                                 should be secure and on the security                                 front                                 the fewer passwords that you have spread                                 around the simpler your security model                                 gets so this data router that we've been                                 talking about becomes the singular                                 source of ground truth for the analyst                                 so that at a very high level is                                 empowering quants to trade faster you                                 can learn more on quilt datacom or find                                 me on twitter i'm a carve and you'll                                 have access to our case study in the                                 form of a PDF on our main site slash                                 Doc's slash quant stat PDF I'll be                                 around for a few minutes if you have any                                 questions thank you for your time
YouTube URL: https://www.youtube.com/watch?v=_IgpBi_qfEM


