Title: How to Do Data Engineering in the Cloud - Jennifer Wu (Cloudera)
Publication date: 2017-10-19
Playlist: Strata Solutions Showcase Theater 2017
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	                              welcome to the showcase for Cloudera                               ultis on AWS and Microsoft Azure                               so Claude Aaron supports four different                               types of application patterns we support                               data engineering for ETL and model                               training type of workflows we support                               data science for ad hoc data discovery                               and model training we do analytics                               database which is a sequel based engine                                an approach for ad-hoc queries of data                                as well as visualization and                                applications to help you visualize we                                also support operational database which                                is real-time streaming and real-time                                analytics all of these four different                                types of applications are supported                                today on-premise and in the cloud                                however in the cloud there are different                                types of mechanisms of using the in                                cloud infrastructure so you can get                                on-demand capacity and have the agility                                for your end users to get cloud era                                capacity when needed the first thing is                                that there is hyper scale cloud storage                                which is highly highly scalable                                      resilient and cheap relative to other                                forms of storage and with hyper scale                                cloud storage like AWS s                                            data Lake you can start scaling your                                compute in your storage independently                                with your data on an external storage                                system you can also spin up compute                                capacity when needed you can spin up                                compute capacity to process your data                                basically grow and shrink your at                                different clusters and then terminate                                the clusters when you don't need them                                anymore and because of this you can                                compartmentalize your different types of                                workloads into different clusters an                                example workflow for using cloud era in                                the cloud on the left-hand side you can                                see data getting ingested into hyper                                scale cloud storage again whether that's                                s                                                                     era Altis to spin up data engineering                                clusters such as spark or mr                                            high one spark that can do the data                                processing from raw data into structured                                clean data that can be used later for                                analytics one common analytics engine                                that people use is Impala which will be                                for ad-hoc queries in and also for bi                                used                                abuse cases impalas not today supported                                on Altis but you can use director to                                spin up Impala clusters and then the end                                user can visualize this data through                                Impala and their bi visualization tool                                of choice so Altis is a platform as a                                service offering that hosts the the                                cloud era family platform as a service                                means that it's a managed service that                                removes cluster operations so you don't                                have a lot of the overhead of managing a                                cluster yourself we manage the                                provisioning plane a cloud air manages                                the provisioning plane as well as the                                clusters that are deployed and                                configured through cloud era Altis you                                don't have to install any kind of                                software to use it unless you want to                                you don't have to worry about cluster                                configurations there are intelligent                                default configurations for all the                                instance types that we support in the                                various clouds                                there's no upgrading or OS patching we                                own the images that we actually deploy                                into the into the cloud infrastructure                                without having to worry about cloud                                operations then users can focus on their                                end-user pipelines so they can focus on                                the applications that they care about so                                for the data engineering workflows we                                have jobs as job submission api so that                                jobs are first-class objects within the                                system you'll see a full list of all of                                your job histories you can determine                                which of the jobs failed and easily                                remediate these jobs and rerun the jobs                                again we provide mechanisms for you to                                troubleshoot these jobs while the                                cluster is running you can use cloud er                                manager which is a basically a Mott can                                be used as a monitoring troubleshooting                                tool we also have workload analytics                                which is native to the cloud sedates                                of cloud era altas has supported AWS as                                 a cloud infrastructure platform we                                 announced at keynote that we will                                 shortly be supporting Azure in beta form                                 next month and in Azure you'll have the                                 same data engineering capabilities as                                 you do on AWS Altis was designed to be                                 multi cloud from the outset so it will                                 support multi cloud workflows in a very                                 native type of way in Azure you would                                 store your data in as                                 so that you would have all of your data                                 growing as you're ingesting into Azure                                 data Lake and spin up compute as                                 necessary in order to process this data                                 they would all be sharing all of this                                 data in your data Lake the clusters                                 themselves that are spun out to process                                 this data run in your Azure subscription                                 and we provide a single pane of glass                                 through Alta so that you can see all of                                 your clusters are either in AWS or eltis                                 so I'm going to show a demo today of                                 cloud era Altis what I'm going to show                                 is basically a correlation between a                                 hospital mortality rates and state GDP                                 and in order to do this we basically                                 ingested data from Medicare gov for the                                 mortality rates and per capita state GDP                                 from Wikipedia into AWS s                                                Microsoft ad LS will use all tests to                                 spin up clusters for data engineering in                                 in AWS and in Azure and with these data                                 engineering clusters we'll run jobs so                                 that will take all of this raw data that                                 has been adjusted into the storage                                 system and create nice well formatted                                 structured data and we'll use Kadare                                 data science workbench to be able to                                 visualize this data so this is cloud era                                 Altis our managed service offering when                                 you first log into cloud era altas                                 you'll see the homepage on the right                                 hand side is everything about what's new                                 so the major features that have come in                                 and say the last two to three months we                                 announced support for clutter navigator                                 lineage so you can see lineage across                                 multiple data engineering clusters in                                 AWS we we have new support for CDH                                      and we support machine machine users in                                 the system so if you have some                                 programmatic uses that you don't want to                                 actually have users that are logging                                 into the system you can have these                                 machine user accounts in the center is                                 basically a summary of the activity that                                 has occurred within this particular                                 account so you can see that there's a                                 summary of the running jobs running                                 clusters and environments that are                                 available here you can see that there                                 were two jobs that were run in the last                                                                                                  clusters that are available in this                                 particular account for processing if I                                 click into environments the environments                                 that map                                 map into AWS and Azure accounts to which                                 you want to deploy your clusters so that                                 so typically people segregate their                                 accounts for isolation purposes for                                 different types of environments like dev                                 test prod in AWS or in Azure and from a                                 single Altis Altos account you can                                 provision into multiple different types                                 of environments including on multiple                                 clouds now once the environments are set                                 up as a one-time setup from a                                 administrative point of view then you                                 can give the environments to your data                                 engineers near data engineers can start                                 spinning up clusters and running jobs so                                 here you can see that we have multiple                                 clusters for AWS as well as at forager                                 to create a cluster and get more                                 capacity in the cloud this is a very                                 simple process you can simply put a                                 custom put a cluster name a service type                                 which CDH version you want to use an                                 environment that you wanted to ploy into                                 and then you specify the node                                 configurations for your cluster so                                 minimum we you need to have three worker                                 nodes you can specify if you want spy                                 instance types you choose a particular                                 private key so that you can SSH into the                                 cluster after the cluster has been                                 provisioned and then you put in username                                 and password for clutter and manager                                 which is in read-only mode when you when                                 you deploy it with the cluster and                                 that's it so that's all it takes in                                 order to create a cluster in a                                 particular CDH cluster in a particular                                 environment so if I want to submit a job                                 I can first submit this job into AWS and                                 if I want to submit a job the fastest                                 way for me to do this is to just clone                                 this particular job                                 and remember I'm going to take some raw                                 data from Medicare Govan from Wikipedia                                 and I'm going to process this data the                                 data is resident in s                                                data from s                                                             write the data into the output directory                                 so here you can see I have a spark you                                 know I have a spark job that's named                                 demo and my it has a class that's that                                 comes in a jar file that is resident on                                 s                                                                     into to be able to read and write from                                 s                                                                        data is coming from and output where I'm                                 going to write directly the output of                                 this of this data transformation and                                 then I can pick a particular cluster                                 that I want to basically use to process                                 this data and then I can submit the job                                 so here you can see that the job demo                                 has been submitted if I look into the                                 AWS console then I can see the area                                 where the data is actually stored so if                                 I look inside the input directory it's                                 picking up this mortality rate data as                                 well as the GDP text which is the raw                                 GDP data if I look into the program                                 directory this is where the actual Scala                                 application resides and then when the                                 output is done if there will be an                                 output directory that has all of this                                 clean structured data so we can check to                                 see if this so you can see that this job                                 is actually running and while the job is                                 running it's very easy to look into                                 cloud era manager cloud era manager is                                 our system management platform that we                                 use on premise and you can also now use                                 in cloud in read-only mode in order to                                 view the status of your jobs so if I                                 look inside Yarn applications you can                                 actually see that this job is executing                                 on this particular cluster and if you                                 want to do some type of troubleshooting                                 with this job you can go into the log                                 files by clicking on to these various                                 links you can see all the components UIs                                 as well through                                 Cloudera manager so this job takes about                                 a minute to run and then after the job                                 is done running you'll see the job                                 complete completed on this on this job                                 form if I look at an old completed job                                 you'll see workload analytics so these                                 are the details of the jobs themselves                                 it was the same job that is typically                                 run in a Rihanna recurring basis and                                 then if I look in job analytics I can                                 troubleshoot this job through execution                                 details you'll also see that we baseline                                 jobs recurring jobs once you've run a                                 job three times and we'll baseline this                                 particular job so here's the health                                 checks execution details will give you                                 all of the log files and then we'll also                                 do baselining and deviations from the                                 baseline with some root cause analysis                                 if things have gone wrong so there's                                 this is the baseline and capability this                                 is baseline and capability and you'll                                 see deviations from the baseline and                                 it'll give you some troubleshooting                                 capability into end and and root cause                                 analysis if the performance isn't as                                 performance as usual anyway we'll go                                 back to the job we can see that it's                                 completed if we look in s                                            refresh this page we should see an                                 output directory with clean data if I                                 want to run this same job it's very easy                                 thing to do if I can run if I want to                                 run the same job on a on adls I can                                 simply clone this particular job and                                 then instead of speccing is specifying s                                                                                                    locations so I have this jar file on                                 adls as well so I can specify an adls                                 location I need to specify an input and                                 an output directory so here I'll specify                                 an input directory                                 that has an idiot ADL path and an output                                 directory and you'll see I put this into                                 the output say well one directory I need                                 to specify and an azure cluster and then                                 this is going to run in the Azure                                 environments and as I look inside the                                 azure inside the azure ad LS URL you can                                 see that I have the same data that's                                 that's inside a sure I have jar files                                 that specify the program itself and then                                 I'm going to have an output directory                                 that will have my initials jwu on this                                 on this directory when it's done                                 so this this application takes about a                                 minute to run and then we should see the                                 output shortly                                 still waiting for it to run I believe                                 so the difference between running jobs                                 inside AWS and on Azure itself will just                                 be simply specifying a different URIs as                                 long as the application is is                                 independent of the particular                                 environment the job says it's completed                                 if I look inside all testimony you'll                                 see my initials here with all of the                                 data here successfully converted so if                                 you are interested in participating in                                 the Microsoft's cloud era altes beta you                                 can request access on the cloud Ericom                                 website or feel free to stop by the                                 cloud era booth thank you
YouTube URL: https://www.youtube.com/watch?v=DylvtWzIc0c


