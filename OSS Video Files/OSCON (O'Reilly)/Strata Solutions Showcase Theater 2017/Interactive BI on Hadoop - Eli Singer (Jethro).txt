Title: Interactive BI on Hadoop - Eli Singer (Jethro)
Publication date: 2017-10-19
Playlist: Strata Solutions Showcase Theater 2017
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	                              hi everyone my name is Ellie singer I'm                               the CEO of jethro data and we'll talk                               about BI on Hadoop and how to make it                               work many of our customers are in the                               process of uploading their bi                               applications from legacy DW to Hadoop                               and what they tries what they worked for                               them for many years you just point your                               bi tools the regulated OOP and it                                doesn't really matter what kind of                                sticker Hadoop truly would be using                                sparks equal impala hive drill and so on                                the bottom line is that those tools are                                too slow they full scan every query that                                you ran and in result is too slow and                                slow bi is no BI now what do you do when                                your BI application is just too slow                                when running on your Hadoop platform                                what people end up doing is what we call                                data engineering acrobatics they end up                                taking their data sets and they pre                                aggregate recalculate they denormalize                                build partitions cubes and so on trying                                to speed up bi access to their Hadoop                                data now very quickly this becomes hard                                to maintain                                you cannot keep up every time user                                changes the dashboard to go and build an                                aggregation for them so the next thing                                we see people are doing is push this                                problem all the way to the end users                                from self-service bi we end up with                                self-service performance we ask our end                                users to create their extracts or build                                their own cubes to speed up their                                queries and of course this is a                                tremendous waste of their time so what's                                happening here is that we are trying to                                solve the performance problem that is                                part of the data platform in the wrong                                place it makes absolutely no sense to                                have our IT waste their resources                                building all of those aggregations or to                                ask the end users to build their own                                extracts if the BI platform requires a                                data platform that is performant the fix                                has to be applied to the data platform                                so this is what JETRO is all about we                                are about speeding up bi on Hadoop jet                                receipts as a transparent layer between                                your bi                                application and your Hadoop platform the                                way we speed up access is by checking                                their Duke data sets and add indexes we                                index every column we add Hueber greg                                asians based on queries sent by the                                application and auto caching all of it                                is self driving no data engineering is                                needed and the application itself adjust                                to data changes and application changes                                the key to Jethro is the combination of                                cubes and indexes if you think of BI                                queries they range from being very                                aggregated some sales grew by state I                                want to see the total sales of all the                                roads that I have in the data set this                                type of query very aggregated benefits                                from Q if I create a cube aggregation                                for this query it will take no time the                                other extreme we have very granular                                queries something like show me all the                                transactions by user one two three in                                this case a Gregg agents do not gonna                                work the only thing that would work is                                indexes and what's unique about general                                solution is that we combine both                                aggregation and indexes we use an                                optimizer to choose for each query which                                way to go with this query be benefited                                from indexes or from aggregations if the                                query will benefit from aggregations we                                will create a cube aggregation for it                                now these two strategies are very                                complementary which means the very                                little overlap between them the types of                                queries that would work great for                                indexes will be terrible with cubes                                think of it if you want to get all the                                rows all transactions for a specific                                customer a cube with a customer ID will                                be gigantic way too big to be effective                                that's where indexes work well and if                                you want to run an aggregation that                                shows you all sales by all states                                without any filtering indexes will be                                useless so those two really complement                                each other very little overlap I need to                                have both if you want to have a full                                support for bi queries okay                                 now what is the process how does this                                 thing all come together the main idea                                 here is that all the heavy lifting the                                 cue building the indexes all done in the                                 background okay so if this is your                                 environment we have data in hive table                                 HDFS table could be sick all sources                                 doesn't really matter what JETRO does is                                 read that data set and only bi data sets                                 not all of your data in Hadoop and                                 creates a highly optimized version of                                 this data set back in Hadoop highly                                 optimized means it's indexed and it's                                 aggregated now at run time we get the                                 queries from the BI tools and we have                                 much less work to do the data is already                                 pre indexed and aggregated all the heavy                                 lifting was done so it's run time the                                 amount of work is much smaller and                                 therefore we can respond much faster and                                 much greater scale okay give you some                                 more details we can incorporate data                                 into J's oh sure                                 the data the question is whether they to                                 leave the data never leaves Hadoop                                 everything JETRO does is within hadoop                                 the queue building is within Hadoop the                                 indexes are within Hadoop the query                                 engine is within Hadoop sorry                                 if you have a dated impeller same thing                                 as you see is high we were just read it                                 for Impala create an index version of                                 the data set                                 store it back in Hadoop and then when we                                 get queries from BI tools we will use                                 the indexed version of the data set okay                                 so the data can come from any place from                                 hive or Impala from CSV Cephas EBS                                 CSV files sequel database everything we                                 index every column because when you give                                 your users self-service bi tools like                                 tableau you want them to be able to                                 filter by any column you don't want to                                 limit them so we index every column we                                 build cubes based on the actual user                                 queries we use Hadoop as our storage and                                 JETRO takes about a third                                               of the original raw data so if your                                 dataset was about a Terada terabyte                                 judge will take about                                                  Hadoop                                 net result we can provide the                                 performance every query you have in your                                 bi tool will be accelerated we can                                 provide a scale thousands of concurrent                                 users billions of rows and we do it                                 without the extra work without the data                                 engineering and without changing your bi                                 applications lastly if you want to put                                 together                                 Jetray in the marketplace and how we                                 compare to other tools the first option                                 you have is just point your bi tools at                                 a'dope any sequel server on hadoop and                                 the challenge with that approach is of                                 course full scan those tools will have                                 two full scan every filter of every                                 query of every dashboard of every user                                 every time the only exception is if you                                 filter by partition key other than that                                 you full scan the data then we have                                 tools that provide cubes on Hadoop they                                 are good for some queries that you can                                 add to a cube they cannot do the queries                                 that are too granular or just simply did                                 not fit within a cube and then there is                                 Jethro's approach which combines both                                 the indexes and the cubes together and                                 the different series that we can                                 accelerate our queries and it's fully                                 automated you don't have to define your                                 cubes you don't define your indexes it's                                 fully automated very easy to deploy                                 JETRO can be installed in minutes you                                 can download it from our site you can                                 index the existing data sets in hours                                 you can apply JETRO gradually it's not                                 an all-or-nothing you choose one                                 application you take its data set and                                 you accelerate it and then you can scale                                 out jetrel servers to support any level                                 of concurrency any number of users that                                 you have so with that I want to thank                                 you and if you have any other questions                                 I'll be happy to answer                                 excellent
YouTube URL: https://www.youtube.com/watch?v=8W3xK_O3bRI


