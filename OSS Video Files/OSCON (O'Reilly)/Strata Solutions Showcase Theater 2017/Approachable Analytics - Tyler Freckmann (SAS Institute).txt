Title: Approachable Analytics - Tyler Freckmann (SAS Institute)
Publication date: 2017-10-19
Playlist: Strata Solutions Showcase Theater 2017
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	                              hi I'm from SAS Institute we are an                               analytics company and basically we do a                               broad range of analytics everything from                               specific point solutions like fraud and                               risk marketing all the way through                               general machine learning and text                               analytics and everything like that so                               basically I am here to speak to you                               about approachable analytics and I'm                                here to show our newest interfaces that                                we're releasing this November and how                                somebody like me who is not necessarily                                a trained data scientist can get access                                to their data and see what the insights                                are and move all along the analytical                                journey of preparation and model                                development to deployment without                                necessarily having to be you know a                                classically trained data scientist                                rather like what we like to call a                                citizen data scientist so the there we                                go                                so this platform that we're using for                                this approachable analytics is SAS via                                and what I like to say is it's an it's a                                platform for everybody because it has                                visual interfaces programming interfaces                                REST API s and so you know I can be a                                super hardcore Python programmer or                                super hardcore SAS programmer but I can                                also be a business user like myself and                                access all of the same capabilities the                                same data and the same scalable                                in-memory engine from the same                                environment regardless of which persona                                I am                                so the moniker that I like to use about                                this interface that we're showing is                                simple powerful and automated and we                                like to say SAS is trying to make the                                jobs of a data scientist like the day at                                a spa so how are we simple like I said                                we have this one environment where you                                can move seamlessly from your data                                preparation to your model deployment                                without having to change between                                different applications different                                interfaces all of that kind of stuff                                it's collaborative                                we have decided this idea of projects                                across all of the different personas and                                of course we have these api's to all of                                the different programming languages like                                Python and R and everything like that so                                powerful because the core of this engine                                is a distributed in memory engine so it                                can scale to you know                                                 of nodes right it can sit on top of your                                Hadoop cluster and so it can really                                solve any sort of big data challenge                                lots of you know we have our structured                                and unstructured analytical capabilities                                as well as all types of machine learning                                capabilities and then finally automated                                so you know we have hyper parameter Auto                                tuning to really scale up the                                performance of models and you know you                                can seamlessly deploy into a database                                environment and run your and score your                                models against the the things that you                                developed and everything like that so a                                day at the spa                                with all of these capabilities so                                without further ado I will move into the                                software all right                                okay so as you can see here's my landing                                page and I can you know choose where to                                start I can start with preparing my data                                or I can jump straight into building                                models off of something that already                                somebody else prepared I can manage my                                models we have the idea of workflows and                                so you know this is would be in a highly                                governed situation where you need to                                check off your model development to meet                                certain criteria before it goes into                                production things like that but I'll                                just start with the basic flow which is                                okay I want to you know I want to start                                a project I'm gonna start preparing some                                data so I have a plan for my data                                preparation and I am working with loans                                data so I am going to be predicting that                                somebody defaulting on their loan based                                off of a variety of different                                characteristics and so you know I can                                get a sneak peek at my data right but I                                actually want to join this against my                                broader customer data set that has all                                 of the geographic characteristics                                 because this has you know it has                                 birthday if I want to incorporate age as                                 well as my geographic characteristics if                                 I want to do analysis on that so you                                 know as simple as that I can you know                                 quickly join my data and then all of                                 this happens in memory so you know if                                 this is                                                               five machines if it's a hundred gigs it                                 can be in ten machines and scaling out                                 so on and so forth and I can do various                                 other sites types of data                                 transformations to prepare but at this                                 point I'm gonna move into the                                 exploration phase so let me actually                                 save this target table so joined loans                                 and here I'm saving my plan if I want to                                 replay that plan against other input                                 input sources and that and I can expose                                 like a REST API and schedule it that way                                 if I want so let me join in low                                 it's okay all right so I'm saving my                                 target table I'm gonna save it to this                                 public library so that anybody else who                                 wants to work on it has access to it as                                 well but let me go ahead and explore and                                 visualize my data all right so this is                                 our exploration interface and so                                 basically you know I can just drag and                                 drop and start to explore my data this                                 is going to be what I'm trying to                                 predict so I'm actually going to make                                 that a categorical column and so I have                                 all of my geographical data but let's                                 say I'm trying to see the relationships                                 between all of my numerical inputs right                                 and so we have this Auto charting                                 concept and let me actually remove there                                 we go okay so I'm gonna change this into                                 a correlation matrix there we go all                                 right so I can start to see the                                 correlations between you know my                                 different inputs I can also start to                                 investigate like if I want to see if                                 there's any skews in my data like this                                 is highly skewed then you know maybe I                                 want to edit that before I move into the                                 modeling phase and like I said I'm not                                 I'm like a citizen data scientist I                                 don't have necessarily classical                                 training so I'm able to just explore and                                 and I have basic understanding of you                                 know what is good practices but this                                 isn't necessarily the heavy doodling                                 heavy-duty modeling modeling interface                                 but I want to like create I want to                                 standardize that column so I'm just                                 going to apply the natural log to it                                 all right                                 and so as you can see you know there's                                 like no coding involved right it's all                                 just point and click and so now I have                                 maybe a more standardized column that I                                 can use for analysis or something like                                 that but straight from this environment                                 you know we have all of our explorations                                 right different ways of slicing and                                 dicing your data but then also we have                                 more advanced analytics that we can do                                 forecasting network analysis and then we                                 get into our more advanced modeling so                                 let's say I wanted you know build a                                 neural network I can just start building                                 a neural network I'm going to choose my                                 response which is whether it's going to                                 be a bad loan or not and then I can                                 choose you know my predictors I don't                                 want my ideas okay and it will just                                 build the neural network on the fly for                                 me and then you know I I have the                                 opportunity to see like an importance                                 plot of the different variables or                                 something like that I can edit the                                 neural network if I want to add a hidden                                 layer or something like that so this is                                 all very intuitive for me as a you know                                 a citizen data scientist but then if I                                 want to go further and like automate my                                 model building process I can actually do                                 what we call create a machine learning                                 pipeline straight from this visual                                 interface so it's taking all of the                                 actions that I did my data preparation                                 my natural log you know calculated                                 column all of that kind of stuff it's                                 putting it into a project and then it's                                 creating a machine learning pipeline                                 that I can build on top of from here so                                 now you know I have my interactive data                                 preparation that I did I have my neural                                 network that I did but I can put in                                 other types of models if I want to like                                 a gradient boosting model                                 and and then I can go in and I can tweak                                 all of the different features for this                                 model if I you know I'm an analytical                                 expert like that we also have the you                                 know the opportunity to do hyper                                 parameter auto tuning so you know I'm                                 choosing I have this patented genetic                                 algorithm that uses advanced                                 optimization techniques to you know find                                 the best values of the parameters for                                 all of these different machine learning                                 models to get the best lift so that's                                 the beginnings of the pipeline but one                                 of the things I love most about this                                 environment is that we have these                                 projects and templates so it's baked                                 with lots of different best practice                                 templates so I can have you know a basic                                 template for my binary target that I can                                 use that will come with you know a                                 machine learning pipeline that's already                                 built out with imputations of missing                                 values transformations I can there are                                 more advanced templates that I can                                 choose from to start out with and then I                                 can save templates that I've created for                                 others to use if I have like the best                                 you know version of the gradient                                 boosting but the best parameters right I                                 can save that for my colleagues to use                                 and so I'm gonna go ahead and kick off                                 these jobs all right                                 and they will be running in parallel on                                 our distributed environment so that                                 afterward then I can go and compare                                 across the different pipelines that I've                                 done so if I have you know one pipeline                                 dedicated to regression based techniques                                 and I'm using all of the best methods                                 for regression and then I have another                                 pipeline dedicated to neural networks                                 another ones educated to tree based                                 techniques right and then I want to                                 compare across all of these different                                 ones I and I don't have a cluttered                                 environment I can do I can do that                                 across all of these different techniques                                 so while it's training one thing that                                 you can do is you can actually get the                                 batch code for this so that you can                                 retrain these pipelines in batch on a                                 scheduled you know basis if you want to                                 do that all right so we the these                                 pipelines have finally finished and so I                                 can move into the comparison across                                 these different pipelines and it chooses                                 my champion model and then what I can do                                 is I can register this model to a model                                 it manager environments where that has                                 versioning it will have model decay                                 plots if we're running on like a regular                                 basis or something like that                                 so that I can have all of these projects                                 that I build in a more Goverdhan                                 environment but then you know I have my                                 fit statistics and everything that I                                 could want from you know this tournament                                 that I created but the last aspect of                                 this environment is okay I want to be                                 able to publish this to either a scoring                                 API or publish it into a database so I                                 can publish the models into a Hadoop                                 database if I want so if my data is in                                 HD F I'm sorry if my data is in any of                                 these databases you know hi for example                                 I can connect to the hive database and                                 then I can publish that model to run in                                 hive for example                                 or you know I can download a scoring API                                 that you know we can be sitt can be                                 submitted and any language right because                                 it's just a REST API but we build it out                                 so that you can submit it from the SAS                                 programming language or from Python to                                 if you want to do that as well so that                                 you know I've moved in ten minutes right                                 from just wrangling my data joining some                                 tables together and then exploring it in                                 that environment building a tournament                                 it with these pipelines and publishing                                 api's all with the click of a button and                                 that's what we mean when we are saying                                 we're trying to make data science like a                                 spa so with that I'll take any questions                                 oh yeah                                 I'm sorry I can't the data size of this                                 so this one was running on just a single                                 machine so it was ten gigs yeah I'm                                 sorry                                 how big was the memory for this machine                                 I think it was maybe                                             something like that but we actually have                                 like you know this is like a group                                 environment right so you can see like                                 all of the different tables that have                                 been loaded for basically my whole team                                 of                                                                   yeah any other questions                                 okay thank you for your time if you want                                 to check up with me I'm at the SAS booth
YouTube URL: https://www.youtube.com/watch?v=EPAbBpj6RHs


