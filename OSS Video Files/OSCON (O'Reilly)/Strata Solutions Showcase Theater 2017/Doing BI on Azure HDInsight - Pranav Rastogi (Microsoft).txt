Title: Doing BI on Azure HDInsight - Pranav Rastogi (Microsoft)
Publication date: 2017-10-19
Playlist: Strata Solutions Showcase Theater 2017
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	                              thank you everyone for coming for this                               session I'm gonna be talking to you                               about how do you do bi on Azure                               hdinsight using HP insight interactive                               query which was generally available this                               week so in this session I'll be talking                               about some basics of how do you do bi                               wanna do some of the challenges that                               customers see as moving from traditional                                warehousing systems to a sort of a big                                data based system for doing the I base                                scenario and we look at a we'll take a                                look at a demo of interactive query on                                how you can do interactive query over                                hadoop using a hive on a real ap so you                                know fundamentally customers require                                these three sort of pivots where they                                want to interact with their data they                                want to interact with the data at scale                                and they want to have a high concurrency                                in terms of number of queries that can                                run in the system it's gonna be it                                should be easy to use it should be                                secure by default so customers should be                                able to use their Active Directory                                credentials the dinner should be secured                                from a Akal perspective on which user                                can have access to what data and it                                should be easy to manage and it should                                ski it should be cheap I like it if you                                should have to pay for high cost for                                querying of the data and if you look at                                how sort of the evolution of the                                warehousing industry has happened                                especially towards Big Data like what                                has happened in historical                                implementation is the entire process                                needs an SLA for for a DW solution to                                exist for an end user and the two                                bottlenecks that were observed was one                                at the ETL front and one of the                                reporting front where as data was volume                                was getting bigger and bigger the                                existing systems were not able to scale                                to to the size of the data and the                                queries that were running in the system                                were too big so the industry can involve                                two sir these MPP offerings which were                                trying to solve this problem or sort of                                doing massive peril                                computing at scale and while they were                                sort of paralyzing it more and more data                                was coming in and the kind of queries                                customers were started running or the                                kind of workloads customers started                                running slightly evolved as well it was                                much more than sort of writing a sequel                                query and it became like I want to do                                analytics on the same data set I want to                                use Park IO use open source libraries so                                the desire for having a big data system                                kind of evolved because of these                                offloading kind of requirements that                                came up from the data and from the crust                                from the requirements of the customers                                my customers wanted to do more                                analytical like workloads along by doing                                bi on on top of the data as well which                                led to the evolution of big day a bi                                going sort of towards a big data                                direction and sort of Hadoop came along                                as a platform which separated compute                                and storage came with HDFS and sort of                                massive computing powers the the problem                                still remains however on how do you get                                a high concurrency system running so you                                can have multiple queries running on the                                system as well and if you look at the                                fundamental building blocks of building                                a petabyte scale DW on Azure here are                                some of the tenants that you have to                                think about it's a daily lake with no                                limits so it's a daily lake that keeps                                on growing it might you might start off                                with a few gigs you might go to a few                                terabytes petabytes well it's it's                                growing on a day to day basis you have                                rich ingestion opposite options to bring                                a variety of data from variety of                                sources you could be bringing in                                streaming data or you could be bringing                                in data from other resources and you can                                do sort of ETL on the data set in the                                data lake itself so you don't have to                                move the data to a different data source                                for ETL once you're done with your ETL                                operation you should be able to sort of                                interactively query the data set so you                                shouldn't have to and within a sub                                 millisecond or a sub second you should                                 get responses for those queries as well                                 and then eventually you should be able                                 to connect these queries or results to a                                 bi solution or a visualization solution                                 like you can connect it to a power bi                                 and if you can broaden it up you can                                 have API is you can connect to tableau                                 and more tools                                 available in the azured ecosystem as                                 well and then the whole system has to be                                 secure in terms of which user has access                                 to the environment and what user                                 privileges are existing for the data                                 that they can access so in terms of                                 tables in terms of columns in terms of                                 rows or in terms of the data set and                                 some of the information that's stored in                                 the data set itself should be secured                                 like PII based data or if you're storing                                 credit card information so that should                                 not be exposed to to the user and what                                 we'll look at in this example is how do                                 you some what are some of the pros and                                 cons of moving data at this scale you                                 know the pros are you can sort of reuse                                 what you have but the cons are that you                                 have to maintain schemas across                                 different systems data offloading takes                                 time specially at a petabyte scale you                                 have challenges on how do you keep both                                 data sources and the data this source                                 and the destination in sync and then                                 this is an operational nightmare in                                 terms of moving you know petabytes of                                 data from one system to the other                                 so that kind of led to this - ll ap                                 solution which allows you to interact                                 query over data which can sit in a de                                 lake store or Azure blob storage so you                                 can do interactive query over that data                                 itself here are some examples of                                 performance benchmark run on a high                                 interactive query cluster so this is                                 high vantes and interactive query and                                 you can see the tremendous improvement                                 that has happened for interactive query                                 over the period of time                                 this light talks about total time it                                 took for a one-terabyte data set to run                                 all the                                                               can see a                                                                on LL ap so I'm going to show you a                                 quick demo on what does a typical query                                 look like in terms of running it on a                                 hive n.l.a.p cluster so what you're                                 seeing over here is this is a cluster                                 that I have it's probably has about                                     terabytes of data                                 and I'm going to execute this query over                                 here and if I switch over to my                                 monitoring view which is the theis view                                 you will see that this query is gonna be                                 submitted to the cluster and and then                                 I'll get the results back over here so                                 that's one example of how you can sort                                 of interactively query you can write the                                 same query in a in visual studio code                                 which is cross-platform editor for                                 developers so I'm on a Mac machine and                                 we're using real studio code and you can                                 submit a cluster or submit a job from                                 this environment itself so by selecting                                 this cluster you can submit a job to the                                 interactive query cluster you can also                                 connect power bi to the same cluster for                                 interactive query so in this example                                 what I'm showing you is I have a power                                 bi which is using direct query which                                 means that it's gonna query directly                                 over the data which is sitting in the in                                 the data Lake itself so there is no data                                 movement involved so as I click around                                 you know the first click is gonna be a                                 query to the through the engine which is                                 high ll ap and you'll get the results                                 back and then the results will be cached                                 in in the query engine itself so the                                 next time when you run the same query                                 you'll get the results from the cache                                 and if you sort of start clicking around                                 you'll see these cache results are                                 coming back fairly quickly which sort of                                 tells you that you know over a terabyte                                 of                                                                     with your data of fairly quickly you can                                 add in more dimensions in terms of the                                 filters you want to apply to your                                 dataset and you can get those results                                 very easily back to your to your sort of                                 visualization tool so this experience is                                 a fairly rich experience in terms of you                                 know how you can do interactive query                                 over over a big data set so here are                                 some results of a end up doing                                 interactive query which is add a sort of                                 a petabyte scale this is an                                 architectural diagram of what does the                                 cluster look like a key point that I                                 wanted to make was what happens if your                                 data changes then how does cache                                 eviction happen in this case you know                                 the cash is evicted and you get sort of                                 the the fresh data always when you                                 aquarii and it supports a variety of                                 file formats                                 so again if you're if you're running                                 multiple workloads in your daily lakes                                 so you might be storing different file                                 formats like or Parque Jason Avro the                                 key thing over here is it supports all                                 these formats so you don't have to do                                 ETL operations to change the data format                                 from one to the other for bi scenarios                                 you can just query over the data in                                 terms of whatever format it has and it                                 could be structured data it could be                                 unstructured data or it should you know                                 so it works on all these form factors so                                 I have these require or slides we'll                                 talk about how the performance looks                                 like but you can run these performance                                 benchmarks on your own so this is a URL                                 where you can download these scripts but                                 and you can even do more concurrent                                 queries over the over the data set as                                 well I think you can do about                                         concurrent requests over the query                                 itself and it's supported by a rich                                 ecosystem so you can use rich bi tools                                 so you can do excel you can do Zeppelin                                 and you can connect it easily to the                                 interactive query cluster so this is one                                 way how you can do bi on top of HD                                 insight on a petabyte scale without                                 having any data movement and you can                                 interactively query over that data set                                 so I hope you liked this presentation                                 thank you for your time
YouTube URL: https://www.youtube.com/watch?v=24OYECw5ki4


