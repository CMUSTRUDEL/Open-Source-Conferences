Title: Tackling ML model degradation with Jean Francois Puget (IBM)
Publication date: 2018-06-18
Playlist: Strata Data Conference 2018 - London, United Kingdom
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:04,740
hi I'm Park hoon athan from O'Reilly

00:00:02,190 --> 00:00:07,770
Media and it's a pleasure here today to

00:00:04,740 --> 00:00:10,620
be with Jean Francois fouchÃ© who's a

00:00:07,770 --> 00:00:12,420
distinguished engineer at IBM in machine

00:00:10,620 --> 00:00:16,470
learning optimization in advanced

00:00:12,420 --> 00:00:18,869
analytics and also a Kaggle Grandmaster

00:00:16,470 --> 00:00:21,689
as I understand it tell us a little bit

00:00:18,869 --> 00:00:24,660
about the type of computation worked it

00:00:21,689 --> 00:00:28,170
then you've done ok a girl is a legal

00:00:24,660 --> 00:00:31,650
drug it's a very addictive it's it

00:00:28,170 --> 00:00:33,510
started as a competition side for

00:00:31,650 --> 00:00:37,380
machine learning so basically you're

00:00:33,510 --> 00:00:39,210
given some data a question to answer

00:00:37,380 --> 00:00:42,450
with machine learning and people are

00:00:39,210 --> 00:00:45,930
ranked and the quality of their

00:00:42,450 --> 00:00:49,260
prediction and it's a great learning

00:00:45,930 --> 00:00:53,129
experience because the best of the world

00:00:49,260 --> 00:00:55,070
out there and it's very addictive so go

00:00:53,129 --> 00:00:57,449
there but be careful

00:00:55,070 --> 00:01:00,180
hopefully I notice also looking at

00:00:57,449 --> 00:01:02,690
github I I saw me repos there you're

00:01:00,180 --> 00:01:06,420
making a lot of use out of open-source

00:01:02,690 --> 00:01:10,939
so far for machine learning and the

00:01:06,420 --> 00:01:13,260
planning open-source as one I would say

00:01:10,939 --> 00:01:17,520
you still have some good proprietary

00:01:13,260 --> 00:01:19,350
software like SPSS but you can find all

00:01:17,520 --> 00:01:23,670
state-of-the-art algorithms in our

00:01:19,350 --> 00:01:26,909
Python basically so this has gender game

00:01:23,670 --> 00:01:29,280
when I started my career the problem was

00:01:26,909 --> 00:01:33,210
to be able to train a model to create a

00:01:29,280 --> 00:01:36,180
model now training model is easy the

00:01:33,210 --> 00:01:39,020
mythic resource but you write few lines

00:01:36,180 --> 00:01:43,049
of code and you can train in good models

00:01:39,020 --> 00:01:46,740
so as as a window in that industry we

00:01:43,049 --> 00:01:48,750
looked at given open source community

00:01:46,740 --> 00:01:51,450
has solved the problem of creating

00:01:48,750 --> 00:01:53,700
models what are the other pain points

00:01:51,450 --> 00:01:55,409
people have if they want to build a

00:01:53,700 --> 00:01:57,509
machine learning or an AI application

00:01:55,409 --> 00:01:59,939
and that's what we are looking we are

00:01:57,509 --> 00:02:03,570
looking at pain points and trying to

00:01:59,939 --> 00:02:04,890
address them by providing tools that

00:02:03,570 --> 00:02:09,209
complement of pencils

00:02:04,890 --> 00:02:12,780
excellent what what kind of tooling then

00:02:09,209 --> 00:02:15,930
so we started like two or three years

00:02:12,780 --> 00:02:18,000
ago nobody was speaking about deploying

00:02:15,930 --> 00:02:23,579
my dad's some were doing it of course

00:02:18,000 --> 00:02:26,939
but so we focus for the last two years

00:02:23,579 --> 00:02:29,159
on well I've met too many times

00:02:26,939 --> 00:02:31,379
prospects telling me okay I've hired

00:02:29,159 --> 00:02:34,650
data scientists it's been two years now

00:02:31,379 --> 00:02:37,530
producing models but how do I move my

00:02:34,650 --> 00:02:40,049
spark a male model into my payment

00:02:37,530 --> 00:02:41,069
system how do I move my cyclotron model

00:02:40,049 --> 00:02:45,629
into something else

00:02:41,069 --> 00:02:48,419
so and the recurrent the approach at

00:02:45,629 --> 00:02:52,079
that time was to have developers record

00:02:48,419 --> 00:02:55,650
in Java or COBOL or C++ something that

00:02:52,079 --> 00:02:57,989
people developed in our Python so we say

00:02:55,650 --> 00:03:00,150
no you cannot record it takes months and

00:02:57,989 --> 00:03:04,439
you do it on new ones so we worked on a

00:03:00,150 --> 00:03:07,680
way to take them out there sterilized it

00:03:04,439 --> 00:03:13,859
and then read it in a runtime that can

00:03:07,680 --> 00:03:17,220
be that is exposed as a REST API so we

00:03:13,859 --> 00:03:20,250
have a one-click story where you can

00:03:17,220 --> 00:03:23,579
take a model and in one click you create

00:03:20,250 --> 00:03:26,760
a web service a red service that's worth

00:03:23,579 --> 00:03:28,439
your money so now what our people are

00:03:26,760 --> 00:03:31,470
doing it but we were really the first

00:03:28,439 --> 00:03:33,989
and that's our first tooling so we have

00:03:31,470 --> 00:03:37,669
a good story on deployment now we are

00:03:33,989 --> 00:03:39,989
looking at that's also something people

00:03:37,669 --> 00:03:42,120
especially coming from determining

00:03:39,989 --> 00:03:44,189
statistics they may not realize she

00:03:42,120 --> 00:03:45,870
learning is a continuous process you

00:03:44,189 --> 00:03:51,049
don't create a model once and you're

00:03:45,870 --> 00:03:53,879
done if you build for detection model

00:03:51,049 --> 00:03:57,500
you need to update it regularly because

00:03:53,879 --> 00:04:00,180
criminals find new ways try new ways so

00:03:57,500 --> 00:04:02,879
you need to be able to manage versioning

00:04:00,180 --> 00:04:05,720
to deploy my dad's but also to detect

00:04:02,879 --> 00:04:10,349
when your model is no longer effective

00:04:05,720 --> 00:04:12,620
so we also when when we deploy motel we

00:04:10,349 --> 00:04:15,389
also deploy a way to capture feedback

00:04:12,620 --> 00:04:18,180
now is that detection of the model

00:04:15,389 --> 00:04:20,430
degradation that's happening in situ as

00:04:18,180 --> 00:04:21,910
the model is deployed or does somebody

00:04:20,430 --> 00:04:25,030
have to pull it back and you can

00:04:21,910 --> 00:04:27,430
it's done in badshah so it can be in

00:04:25,030 --> 00:04:30,190
situate a key commerce recommender

00:04:27,430 --> 00:04:33,940
system which is people may not know but

00:04:30,190 --> 00:04:35,830
it's probably the most useful from a

00:04:33,940 --> 00:04:38,290
business point of view use of machine

00:04:35,830 --> 00:04:40,930
learning you know when you put things in

00:04:38,290 --> 00:04:43,420
your basket and decide the other people

00:04:40,930 --> 00:04:46,990
so by this these recommendations are

00:04:43,420 --> 00:04:48,820
machine learning predictions so here you

00:04:46,990 --> 00:04:50,890
can have feedback because you Rick the

00:04:48,820 --> 00:04:53,890
system your mother recommends which

00:04:50,890 --> 00:04:58,030
product a visitor might want to buy but

00:04:53,890 --> 00:05:00,400
when the visitor actually pays for the

00:04:58,030 --> 00:05:03,580
basket you know exactly what the visitor

00:05:00,400 --> 00:05:06,070
wanted to buy so by comparing what

00:05:03,580 --> 00:05:10,390
people buy versus what was recommended

00:05:06,070 --> 00:05:12,580
to them you can you can see how far your

00:05:10,390 --> 00:05:15,760
predictions are from the reality and as

00:05:12,580 --> 00:05:17,890
a feedback I'm I'm discussing so in this

00:05:15,760 --> 00:05:21,870
case you can get almost real-time

00:05:17,890 --> 00:05:26,020
feedback so if you get this feedback to

00:05:21,870 --> 00:05:28,030
your training system you could train

00:05:26,020 --> 00:05:30,250
your model almost real-time or every day

00:05:28,030 --> 00:05:33,370
or every hour I don't know but you can

00:05:30,250 --> 00:05:35,530
retrain it in other case the feedback

00:05:33,370 --> 00:05:37,980
can take a long time in fraud detection

00:05:35,530 --> 00:05:40,860
credit card transaction fraud detection

00:05:37,980 --> 00:05:43,630
you can have a suspicion of a fraud but

00:05:40,860 --> 00:05:46,390
the confirmation that it is a fraud you

00:05:43,630 --> 00:05:47,260
know you're the cardholder arm has filed

00:05:46,390 --> 00:05:49,419
a complaint

00:05:47,260 --> 00:05:52,060
you must investigate so it may take two

00:05:49,419 --> 00:05:52,860
months so the when the feedback take two

00:05:52,060 --> 00:05:56,880
months

00:05:52,860 --> 00:06:01,270
it's a batch process so but the point is

00:05:56,880 --> 00:06:03,550
you should always monitor how your

00:06:01,270 --> 00:06:05,530
predictions compared to reality because

00:06:03,550 --> 00:06:08,110
that can be a drift and if you don't

00:06:05,530 --> 00:06:09,730
want it all you don't know this is

00:06:08,110 --> 00:06:11,110
interesting because for so long we were

00:06:09,730 --> 00:06:13,060
talking bitter science to make the

00:06:11,110 --> 00:06:15,940
absolute best code the most efficient

00:06:13,060 --> 00:06:18,070
code the most elegant code so somebody

00:06:15,940 --> 00:06:19,540
else could work on it but now the code

00:06:18,070 --> 00:06:21,850
is becoming less relevant it's more

00:06:19,540 --> 00:06:24,220
about the predictions really yeah that's

00:06:21,850 --> 00:06:26,980
also something I like to say you know

00:06:24,220 --> 00:06:29,650
people you can read in social media and

00:06:26,980 --> 00:06:33,070
newspapers oh yeah I will remove this

00:06:29,650 --> 00:06:35,600
work and this work I think a category

00:06:33,070 --> 00:06:38,270
that is at risk is software

00:06:35,600 --> 00:06:38,900
yes I agree if we look at machine

00:06:38,270 --> 00:06:43,060
learning

00:06:38,900 --> 00:06:47,330
it's basically replacing human code by

00:06:43,060 --> 00:06:49,660
trained model so I'm not saying all

00:06:47,330 --> 00:06:52,490
software development would disappear but

00:06:49,660 --> 00:06:55,250
part of it would be replaced by machine

00:06:52,490 --> 00:06:56,990
learning I'm curious are there active

00:06:55,250 --> 00:06:59,060
learning kinds of cases

00:06:56,990 --> 00:07:02,020
human-in-the-loop use cases that you're

00:06:59,060 --> 00:07:04,870
looking at so that's a good segue

00:07:02,020 --> 00:07:08,570
sideway to another bottleneck so I

00:07:04,870 --> 00:07:12,560
deployment was a source bottleneck the

00:07:08,570 --> 00:07:14,240
other bottleneck that people starting

00:07:12,560 --> 00:07:18,979
the machine learning oh hi Johnny

00:07:14,240 --> 00:07:23,800
don't realize machines do not learn from

00:07:18,979 --> 00:07:25,810
raw data they learn from label data from

00:07:23,800 --> 00:07:28,940
input-output pairs

00:07:25,810 --> 00:07:31,160
it's called supervised running people

00:07:28,940 --> 00:07:35,030
speak about and supplies running blah

00:07:31,160 --> 00:07:37,490
blah but in reality what works in

00:07:35,030 --> 00:07:39,680
business today it's supervised learning

00:07:37,490 --> 00:07:41,930
you tell the Machine what it has to

00:07:39,680 --> 00:07:46,280
learn you give it examples of what it

00:07:41,930 --> 00:07:48,260
has to do as I say when you know what to

00:07:46,280 --> 00:07:50,330
learn is difficult but when you don't

00:07:48,260 --> 00:07:53,810
know it's very very difficult

00:07:50,330 --> 00:07:55,669
it's like going at school and being been

00:07:53,810 --> 00:07:58,760
touched in mathematics or discovering

00:07:55,669 --> 00:08:01,639
new theorems is the former is easier

00:07:58,760 --> 00:08:03,289
even if a lot of people find learning

00:08:01,639 --> 00:08:07,370
mathematics difficult discovering

00:08:03,289 --> 00:08:10,820
mathematics is way harder so we need

00:08:07,370 --> 00:08:16,430
label data and here is an example I was

00:08:10,820 --> 00:08:18,200
I was involved in with a back I want you

00:08:16,430 --> 00:08:22,400
to build a fraud detection credit card

00:08:18,200 --> 00:08:24,650
for detection model and I've tried for

00:08:22,400 --> 00:08:26,539
two years and they tried with a lot of

00:08:24,650 --> 00:08:30,490
vendors technologies could not find

00:08:26,539 --> 00:08:33,560
something that would work so and we

00:08:30,490 --> 00:08:36,589
tried with us I said do we have data

00:08:33,560 --> 00:08:40,969
sure sure we have a data set of millions

00:08:36,589 --> 00:08:42,289
of transactions okay do we know do we

00:08:40,969 --> 00:08:45,110
have labels do we know which

00:08:42,289 --> 00:08:47,400
transactions of what yes yes okay can I

00:08:45,110 --> 00:08:50,400
have the data all right so thank you

00:08:47,400 --> 00:08:55,310
so I did what I had to do after a month

00:08:50,400 --> 00:08:58,500
or two I got the data I looked all right

00:08:55,310 --> 00:09:02,010
three million transactions were the

00:08:58,500 --> 00:09:09,050
liberal rights in this other file I look

00:09:02,010 --> 00:09:12,660
at the other file file 1700 rose 8 fraud

00:09:09,050 --> 00:09:14,930
we cannot learn from a Texan per not

00:09:12,660 --> 00:09:17,700
with today's technology at least so

00:09:14,930 --> 00:09:22,920
maybe a human can but not the machine

00:09:17,700 --> 00:09:24,630
so I say can't you assign someone to

00:09:22,920 --> 00:09:29,160
spend a couple of weeks

00:09:24,630 --> 00:09:35,850
labeling more transactions we need 10

00:09:29,160 --> 00:09:38,070
100 times more examples no good luck and

00:09:35,850 --> 00:09:40,380
the guy was fired six months later

00:09:38,070 --> 00:09:44,160
because he could not be limited and all

00:09:40,380 --> 00:09:47,460
it took all it it was missing was too

00:09:44,160 --> 00:09:50,160
weak to manage and that's something

00:09:47,460 --> 00:09:53,270
don't get I even had so you know what

00:09:50,160 --> 00:09:56,430
son is the brand we use for public cloud

00:09:53,270 --> 00:10:00,029
offerings from IBM in in machine

00:09:56,430 --> 00:10:02,670
learning and AI space so I was also

00:10:00,029 --> 00:10:05,610
involved in some pre-sale activity for

00:10:02,670 --> 00:10:10,110
selling some of these service and same

00:10:05,610 --> 00:10:13,670
we need to train them so and the

00:10:10,110 --> 00:10:16,470
prospect was saying count what's on

00:10:13,670 --> 00:10:19,380
compute the labor force before training

00:10:16,470 --> 00:10:21,990
and I try to explain well if we do a

00:10:19,380 --> 00:10:24,630
good job then Watson will be able to

00:10:21,990 --> 00:10:30,450
compute the labor that's what running is

00:10:24,630 --> 00:10:33,600
about but without the labels you really

00:10:30,450 --> 00:10:35,730
don't have we need examples and then it

00:10:33,600 --> 00:10:40,260
will compute labels automatically but we

00:10:35,730 --> 00:10:42,360
cannot reverse oh and to be honest today

00:10:40,260 --> 00:10:45,300
we have not done much so we have

00:10:42,360 --> 00:10:50,160
research researchers and it's an active

00:10:45,300 --> 00:10:53,130
area to build system that requires less

00:10:50,160 --> 00:10:55,890
examples and you mention active learning

00:10:53,130 --> 00:10:58,740
that's exactly what it is so instead of

00:10:55,890 --> 00:11:00,060
labeling millions of the rows you start

00:10:58,740 --> 00:11:03,230
with sub

00:11:00,060 --> 00:11:05,760
set trainer model and then the model

00:11:03,230 --> 00:11:07,470
predicts on the rest and where the

00:11:05,760 --> 00:11:09,540
prediction Nets its binary

00:11:07,470 --> 00:11:12,090
classification so when it is zero it

00:11:09,540 --> 00:11:14,300
knows when it is when it knows but for

00:11:12,090 --> 00:11:18,840
some example no matter will not know

00:11:14,300 --> 00:11:20,970
predicted SEF have probability then if

00:11:18,840 --> 00:11:23,400
you label those examples you have no

00:11:20,970 --> 00:11:26,340
matter a lot so active learning is the

00:11:23,400 --> 00:11:30,090
idea to look at examples where the model

00:11:26,340 --> 00:11:31,830
is not confident and as for Labour's and

00:11:30,090 --> 00:11:35,630
this way the company is adding value to

00:11:31,830 --> 00:11:38,040
the data exactly so we are working on

00:11:35,630 --> 00:11:40,140
tooling to help people label data

00:11:38,040 --> 00:11:42,600
because it is a next bottleneck were

00:11:40,140 --> 00:11:44,550
saying Exim Bullock I want to follow up

00:11:42,600 --> 00:11:46,500
about one at the point we talk about

00:11:44,550 --> 00:11:48,690
supervised learning and deep learning of

00:11:46,500 --> 00:11:51,480
course is very popular for this now but

00:11:48,690 --> 00:11:52,710
it strikes me coming from some of I

00:11:51,480 --> 00:11:55,530
think we have some shared background in

00:11:52,710 --> 00:11:58,800
this AI is a much broader context

00:11:55,530 --> 00:12:02,550
especially if we go back a few yeah I'm

00:11:58,800 --> 00:12:07,740
old enough you have witnessed some AI

00:12:02,550 --> 00:12:10,500
winter in the past so AI goes by hide

00:12:07,740 --> 00:12:14,190
period where people made bold promise

00:12:10,500 --> 00:12:17,310
you know and then the promises are not

00:12:14,190 --> 00:12:21,150
met then nobody wants to speak about

00:12:17,310 --> 00:12:23,340
anymore and it was a and at each cycle

00:12:21,150 --> 00:12:27,660
the scope of AI changes so when I

00:12:23,340 --> 00:12:32,520
started people don't realize so in lot

00:12:27,660 --> 00:12:35,340
of history collection AI it starts with

00:12:32,520 --> 00:12:39,150
a u.s. researcher called John McCarthy

00:12:35,340 --> 00:12:43,800
in a conference in 1956 in that mood

00:12:39,150 --> 00:12:47,720
college 1956 but an Englishman called

00:12:43,800 --> 00:12:51,630
Alan Turing published in 1956 years

00:12:47,720 --> 00:12:53,940
before the paper where he introduced

00:12:51,630 --> 00:12:57,300
what is known as a Turing test so he

00:12:53,940 --> 00:13:00,720
defined the I as so with today's words

00:12:57,300 --> 00:13:03,750
let's say you have two chat windows on

00:13:00,720 --> 00:13:07,230
your smartphone or laptop so you have

00:13:03,750 --> 00:13:09,390
two conversation but one of the

00:13:07,230 --> 00:13:11,490
conversation is with a human the other

00:13:09,390 --> 00:13:13,770
with a machine and you don't know

00:13:11,490 --> 00:13:18,570
and you converse and your goal is to

00:13:13,770 --> 00:13:21,330
find which is a machine and Turing says

00:13:18,570 --> 00:13:23,490
if you cannot find which one is a

00:13:21,330 --> 00:13:25,740
machine then the machine is intelligent

00:13:23,490 --> 00:13:28,470
that's his definition and if you think

00:13:25,740 --> 00:13:30,870
about it it means the machine must be

00:13:28,470 --> 00:13:35,300
able to understand natural language of

00:13:30,870 --> 00:13:38,220
course but also to endure some jokes to

00:13:35,300 --> 00:13:40,170
understand sentiment to reason because

00:13:38,220 --> 00:13:42,450
you can ask the machine to solve

00:13:40,170 --> 00:13:46,350
problems you can as general knowledge

00:13:42,450 --> 00:13:49,589
question you can so disciplines a whole

00:13:46,350 --> 00:13:53,010
lot of possibilities and to really fool

00:13:49,589 --> 00:13:56,370
a human the machine must really be close

00:13:53,010 --> 00:13:58,080
to human capabilities and this way of

00:13:56,370 --> 00:13:59,970
defining era is very broad

00:13:58,080 --> 00:14:03,149
you have natural language you have

00:13:59,970 --> 00:14:05,580
learning you have reasoning planning a

00:14:03,149 --> 00:14:09,600
lot of things and for decades

00:14:05,580 --> 00:14:16,200
AI research was learning natural

00:14:09,600 --> 00:14:19,170
language planning a lot of things but in

00:14:16,200 --> 00:14:23,430
the last five years something strange

00:14:19,170 --> 00:14:25,339
happened very old algorithms called back

00:14:23,430 --> 00:14:29,520
propagation stochastic gradient descent

00:14:25,339 --> 00:14:33,330
became hype because they were used in

00:14:29,520 --> 00:14:38,010
something called deep learning and five

00:14:33,330 --> 00:14:40,680
years ago in 2013 a team of researchers

00:14:38,010 --> 00:14:43,410
showed that these models were able to

00:14:40,680 --> 00:14:45,959
recognize what's in pictures as well as

00:14:43,410 --> 00:14:48,750
humans and much better than what ever

00:14:45,959 --> 00:14:52,560
existed before this was the start of the

00:14:48,750 --> 00:14:56,070
buzz around the planet so today maybe

00:14:52,560 --> 00:14:59,310
90% of the academics and researchers in

00:14:56,070 --> 00:15:03,120
the I work on the planning so today we

00:14:59,310 --> 00:15:06,209
can Mali say AI equals D planning but

00:15:03,120 --> 00:15:07,740
that was not true five years ago and I

00:15:06,209 --> 00:15:12,079
don't think it would be true five years

00:15:07,740 --> 00:15:14,730
from now because reasoning planning

00:15:12,079 --> 00:15:16,470
natural language processing is important

00:15:14,730 --> 00:15:19,560
too it's not just and if we think about

00:15:16,470 --> 00:15:24,209
it now we call AI the ability to

00:15:19,560 --> 00:15:24,900
recognize what's in a picture or to

00:15:24,209 --> 00:15:26,550
recognize

00:15:24,900 --> 00:15:29,370
the meaning of a sentence basically

00:15:26,550 --> 00:15:33,839
that's a true main use case for the

00:15:29,370 --> 00:15:38,760
planning well any children of age five

00:15:33,839 --> 00:15:42,120
can do it or six so now we are ecstatic

00:15:38,760 --> 00:15:46,080
because we have computers able to do

00:15:42,120 --> 00:15:48,060
what an average child can do it is AI I

00:15:46,080 --> 00:15:50,100
remember when I was younger we were

00:15:48,060 --> 00:15:52,680
discussing expert system trying to me

00:15:50,100 --> 00:15:57,300
make the top performance of human in

00:15:52,680 --> 00:16:00,060
reasoning now we just want to mimic what

00:15:57,300 --> 00:16:02,339
children can do well in your keynote

00:16:00,060 --> 00:16:04,890
this morning is a billion example of

00:16:02,339 --> 00:16:07,500
where managers were rejecting the

00:16:04,890 --> 00:16:09,810
automation until they could actually see

00:16:07,500 --> 00:16:12,570
the simulation side by side were the a I

00:16:09,810 --> 00:16:14,100
was optimizing pricing consistently

00:16:12,570 --> 00:16:16,260
better than anything managers could do

00:16:14,100 --> 00:16:18,089
and until people really feel the pain

00:16:16,260 --> 00:16:19,680
they don't understand what the

00:16:18,089 --> 00:16:21,750
competition is there and this brings

00:16:19,680 --> 00:16:25,200
something it's because you say I did say

00:16:21,750 --> 00:16:27,150
I but the technology used there was

00:16:25,200 --> 00:16:30,360
machine learning and something called

00:16:27,150 --> 00:16:32,700
mathematical optimization before it was

00:16:30,360 --> 00:16:35,790
used it was called operations research

00:16:32,700 --> 00:16:39,570
and for some reason I was not born

00:16:35,790 --> 00:16:41,820
probably in the early days people doing

00:16:39,570 --> 00:16:45,709
operations research and people doing I

00:16:41,820 --> 00:16:50,160
hated each other and this divide is

00:16:45,709 --> 00:16:52,620
still alive today to soft form so for

00:16:50,160 --> 00:16:54,630
many researchers mathematical

00:16:52,620 --> 00:16:59,070
optimization is not part of AI I think

00:16:54,630 --> 00:17:01,279
it's if we consume and again look at the

00:16:59,070 --> 00:17:04,679
ability to perform complex tasks

00:17:01,279 --> 00:17:06,720
mathematical transition is really part

00:17:04,679 --> 00:17:09,150
of AI and that's really what I believe

00:17:06,720 --> 00:17:11,130
and I believe every data scientist

00:17:09,150 --> 00:17:12,390
should know about optimization not just

00:17:11,130 --> 00:17:14,550
about machine learning and deep learning

00:17:12,390 --> 00:17:17,000
it's it speaks to the strength of what

00:17:14,550 --> 00:17:20,040
we can do with the waters of automation

00:17:17,000 --> 00:17:23,839
thank you John that's all a sure it was

00:17:20,040 --> 00:17:23,839

YouTube URL: https://www.youtube.com/watch?v=fCxnw09LoY8


