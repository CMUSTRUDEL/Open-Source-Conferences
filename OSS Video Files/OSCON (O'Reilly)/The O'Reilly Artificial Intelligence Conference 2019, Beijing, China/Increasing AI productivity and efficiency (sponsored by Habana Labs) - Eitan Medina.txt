Title: Increasing AI productivity and efficiency (sponsored by Habana Labs) - Eitan Medina
Publication date: 2019-06-28
Playlist: The O'Reilly Artificial Intelligence Conference 2019, Beijing, China
Description: 
	Eitan Medina details transformational advances made possible with AI processors designed from the ground up to address AI-specific computing requirements, chief among them increasing AI throughput speeds while lowering power consumption. This new class of AI processing brings significantly improved productivity and efficiency to the data center to overcome limitations of existing CPU- and GPU-based solutions. Eitan reveals key architectural advances made by Habana Labs in its AI processor.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,089 --> 00:00:05,580
good morning a Habana Labs is an AI

00:00:03,120 --> 00:00:08,280
processor company and we focus on the

00:00:05,580 --> 00:00:10,920
specific needs of data centers and cloud

00:00:08,280 --> 00:00:15,450
and I'm here today to tell you about

00:00:10,920 --> 00:00:18,960
Goudy our new AI training platform back

00:00:15,450 --> 00:00:22,380
in 2018 we introduced the goya inference

00:00:18,960 --> 00:00:25,470
processor Habanos customer are enjoying

00:00:22,380 --> 00:00:27,539
a new level of performance what you see

00:00:25,470 --> 00:00:29,220
here is the micro processor report

00:00:27,539 --> 00:00:32,250
published by the Lindley group in

00:00:29,220 --> 00:00:35,670
December of 2018 which compared

00:00:32,250 --> 00:00:39,809
processors for inference throughput and

00:00:35,670 --> 00:00:42,660
power efficiency Goa outperforms its

00:00:39,809 --> 00:00:47,399
nearest competitor by a factor of over

00:00:42,660 --> 00:00:50,039
3x nine months later Goa is still the

00:00:47,399 --> 00:00:52,350
performance leader in fact Gore's

00:00:50,039 --> 00:00:55,520
performance is so high that the report

00:00:52,350 --> 00:00:58,410
had to use log scale to compare them

00:00:55,520 --> 00:01:01,559
this achievement is rooted in Goya's

00:00:58,410 --> 00:01:04,589
architecture Habanos engineers took the

00:01:01,559 --> 00:01:09,270
time to design a ground-up pure AI

00:01:04,589 --> 00:01:11,580
processor from scratch Goa is shipping

00:01:09,270 --> 00:01:13,200
today with software and development

00:01:11,580 --> 00:01:16,740
tools that allow our customers to

00:01:13,200 --> 00:01:20,100
migrate from less efficient solutions at

00:01:16,740 --> 00:01:22,229
the same time Goa's platform development

00:01:20,100 --> 00:01:26,210
laid the groundwork for the launch of

00:01:22,229 --> 00:01:29,520
gaudi a new AI training platform and

00:01:26,210 --> 00:01:32,670
here it is the gaudi training processor

00:01:29,520 --> 00:01:35,820
and this specific processor card is the

00:01:32,670 --> 00:01:41,340
first one to be offered in the OCP OAM

00:01:35,820 --> 00:01:43,829
form-factor gaudi is the industry's

00:01:41,340 --> 00:01:47,280
highest performance AI training

00:01:43,829 --> 00:01:49,799
processor a single Goudy delivers a

00:01:47,280 --> 00:01:52,380
record throughput of sixteen hundred and

00:01:49,799 --> 00:01:55,590
fifty images per second on the resonant

00:01:52,380 --> 00:01:57,990
50 training benchmark and delivering

00:01:55,590 --> 00:02:03,180
this record throughput while operating

00:01:57,990 --> 00:02:05,340
at a very low batch size of 64 now you

00:02:03,180 --> 00:02:08,970
may ask why should you care about

00:02:05,340 --> 00:02:11,660
throughput at low bed size you would

00:02:08,970 --> 00:02:13,440
care about it as training is often

00:02:11,660 --> 00:02:16,800
accelerated at scale

00:02:13,440 --> 00:02:19,740
using more than one processor and you do

00:02:16,800 --> 00:02:23,280
it by taking the huge training data set

00:02:19,740 --> 00:02:26,040
and dividing it across the processors in

00:02:23,280 --> 00:02:29,850
your system such that every processor

00:02:26,040 --> 00:02:32,280
trains on a smaller batch of data this

00:02:29,850 --> 00:02:35,520
method is called data parallel training

00:02:32,280 --> 00:02:37,860
and it would work well only if the

00:02:35,520 --> 00:02:42,050
processor that you use maintains high

00:02:37,860 --> 00:02:47,250
throughput even at a smaller batch size

00:02:42,050 --> 00:02:49,860
so how does Goudy perform at scale let's

00:02:47,250 --> 00:02:52,770
look again at the resonant 50 benchmark

00:02:49,860 --> 00:02:57,780
but this time it's scale and compare

00:02:52,770 --> 00:03:00,510
Goudy to the V 100 GPU on the x-axis you

00:02:57,780 --> 00:03:02,370
see the number of GPUs or Goudy

00:03:00,510 --> 00:03:05,190
processors that you use in your system

00:03:02,370 --> 00:03:08,810
and the y-axis shows you the throughput

00:03:05,190 --> 00:03:12,810
but this time of your entire system

00:03:08,810 --> 00:03:15,870
gaudí outperforms V 100 on all system

00:03:12,810 --> 00:03:19,950
sizes in both throughput and power

00:03:15,870 --> 00:03:24,060
efficiency we show here results reported

00:03:19,950 --> 00:03:26,160
to ml per F for the V 100 GPU and the

00:03:24,060 --> 00:03:28,950
blue line shows you what you can expect

00:03:26,160 --> 00:03:31,350
by building an equivalent size system

00:03:28,950 --> 00:03:35,310
but this time based on the Goudy

00:03:31,350 --> 00:03:39,690
processor you can see that for a system

00:03:35,310 --> 00:03:43,070
size of 640 processors Goudy outperforms

00:03:39,690 --> 00:03:45,620
the V 100 by a factor of nearly 4 times

00:03:43,070 --> 00:03:48,450
this is where gaudí's architecture

00:03:45,620 --> 00:03:50,940
really shines with its ability to

00:03:48,450 --> 00:03:55,650
maintain high throughput even at a

00:03:50,940 --> 00:03:58,350
smaller bed size so we talked about

00:03:55,650 --> 00:04:00,540
performance at scale now let's talk

00:03:58,350 --> 00:04:05,250
about how you can physically scale a

00:04:00,540 --> 00:04:08,400
system based on gaudí Habana believes

00:04:05,250 --> 00:04:11,190
that scaling AI is fundamentally a

00:04:08,400 --> 00:04:13,530
networking challenge and the best

00:04:11,190 --> 00:04:17,190
networking technology to do that is

00:04:13,530 --> 00:04:20,730
called rocki rocki is an industry

00:04:17,190 --> 00:04:25,039
standard for RDMA over converge Ethernet

00:04:20,730 --> 00:04:26,780
and Goudy is the only AI processors that

00:04:25,039 --> 00:04:30,220
integrates rock

00:04:26,780 --> 00:04:33,110
networking on the processor chip itself

00:04:30,220 --> 00:04:37,190
with massive networking bandwidths of

00:04:33,110 --> 00:04:40,130
ten ports of 100 Gigabit Ethernet data

00:04:37,190 --> 00:04:42,860
centers would be able to use Ethernet

00:04:40,130 --> 00:04:45,380
networking as they're scaling technology

00:04:42,860 --> 00:04:48,620
instead of relying on proprietary

00:04:45,380 --> 00:04:50,990
interfaces on top of the Goudy processor

00:04:48,620 --> 00:04:54,050
cards that are offered in industry

00:04:50,990 --> 00:04:56,630
standard form factors Havana will be

00:04:54,050 --> 00:05:01,790
also offering its customers a system

00:04:56,630 --> 00:05:05,000
called H LS one Habana lab system one h

00:05:01,790 --> 00:05:08,660
LS one includes eight gaudi mezzanine

00:05:05,000 --> 00:05:12,200
cards and offers interfaces of four PCIe

00:05:08,660 --> 00:05:14,330
ports and 24 ports of 100 Gigabit

00:05:12,200 --> 00:05:17,300
Ethernet running the rocker protocol as

00:05:14,330 --> 00:05:20,540
you can see this system does not

00:05:17,300 --> 00:05:22,850
integrate the CPU and the reason is that

00:05:20,540 --> 00:05:25,490
for different AI application you

00:05:22,850 --> 00:05:31,100
actually need a different ratio of host

00:05:25,490 --> 00:05:33,229
CPU vs. AI processors so with HLS 1 we

00:05:31,100 --> 00:05:35,870
let our customer choose the hostile you

00:05:33,229 --> 00:05:40,460
that they need and simply connect it to

00:05:35,870 --> 00:05:43,850
HLS 1 / PCIe cables so let's see how

00:05:40,460 --> 00:05:46,460
scaling actually works every garda

00:05:43,850 --> 00:05:48,740
processor offers ten ports of 100

00:05:46,460 --> 00:05:52,760
gigabit ethernet and a separate PCIe

00:05:48,740 --> 00:05:55,310
interface PCIe is dedicated for host

00:05:52,760 --> 00:05:58,510
connectivity and is not bottleneck with

00:05:55,310 --> 00:06:02,720
scaler traffic like other solutions have

00:05:58,510 --> 00:06:05,870
out of the ten ports three are provided

00:06:02,720 --> 00:06:08,539
for scaling up and the other seven are

00:06:05,870 --> 00:06:12,260
connecting directly to the other seven

00:06:08,539 --> 00:06:15,370
Goudy processors inside HLS one what you

00:06:12,260 --> 00:06:21,950
see here is then all-to-all non-blocking

00:06:15,370 --> 00:06:24,800
rocky networking inside the box so now

00:06:21,950 --> 00:06:28,400
that we have a basic training system

00:06:24,800 --> 00:06:31,250
let's build an actual training server so

00:06:28,400 --> 00:06:34,460
you select your favorite CPU and hook it

00:06:31,250 --> 00:06:37,610
up to the HLS one using PCIe cables

00:06:34,460 --> 00:06:39,540
that's your basic training server if you

00:06:37,610 --> 00:06:41,540
need more than eight gaudi's you

00:06:39,540 --> 00:06:43,470
simply select the Ethernet switch

00:06:41,540 --> 00:06:45,720
standard Ethernet switch from your

00:06:43,470 --> 00:06:48,420
favorite vendor and hook it up to the

00:06:45,720 --> 00:06:51,210
HLS systems using standard copper cables

00:06:48,420 --> 00:06:52,100
each cable can carry 400 Gigabit

00:06:51,210 --> 00:06:55,160
Ethernet

00:06:52,100 --> 00:06:57,810
now all the gaudi's in the rack

00:06:55,160 --> 00:07:00,720
communicate with each other using rocky

00:06:57,810 --> 00:07:02,940
networking / standard Ethernet as if

00:07:00,720 --> 00:07:05,940
they were in the same box together

00:07:02,940 --> 00:07:08,550
and if you need more capacity scale out

00:07:05,940 --> 00:07:13,260
you can interconnect Ethernet switches

00:07:08,550 --> 00:07:15,900
in layers or in parallel to match your

00:07:13,260 --> 00:07:18,510
capacity and the type of models that you

00:07:15,900 --> 00:07:21,450
would be training you can train using

00:07:18,510 --> 00:07:25,680
data parallel model parallel or any

00:07:21,450 --> 00:07:29,670
combination that you need so you get it

00:07:25,680 --> 00:07:32,760
for scaling ai Ethernet is the right

00:07:29,670 --> 00:07:37,380
networking approach and rocky is the

00:07:32,760 --> 00:07:40,860
best protocol so if today you are using

00:07:37,380 --> 00:07:46,950
GPUs for training why should you

00:07:40,860 --> 00:07:49,400
consider Goudy the first reason is raw

00:07:46,950 --> 00:07:52,110
performance gaudí's architecture

00:07:49,400 --> 00:07:55,860
delivers record throughput and power

00:07:52,110 --> 00:07:58,650
efficiency the second reason is that by

00:07:55,860 --> 00:08:02,130
integrating rocky on the processor chip

00:07:58,650 --> 00:08:05,850
itself Habana labs enable scaling AI

00:08:02,130 --> 00:08:08,040
like never before with gaudi's

00:08:05,850 --> 00:08:10,440
performance you can accelerate your

00:08:08,040 --> 00:08:11,760
training boost your productivity and

00:08:10,440 --> 00:08:15,840
save energy

00:08:11,760 --> 00:08:18,120
all at the same time or you can keep the

00:08:15,840 --> 00:08:23,190
extra throughput for future proofing

00:08:18,120 --> 00:08:26,130
your hardware investment with the rocky

00:08:23,190 --> 00:08:28,620
integration on chip Habana enables our

00:08:26,130 --> 00:08:31,920
customers to control their systems

00:08:28,620 --> 00:08:34,650
future you gain an unprecedented level

00:08:31,920 --> 00:08:38,070
of flexibility in designing your system

00:08:34,650 --> 00:08:40,020
and scaling them from a single processor

00:08:38,070 --> 00:08:44,400
to hundreds or even thousands of

00:08:40,020 --> 00:08:47,010
processors while GPUs that are using

00:08:44,400 --> 00:08:49,680
proprietary system interface are hitting

00:08:47,010 --> 00:08:51,920
a bandwidth wall if you try to scale

00:08:49,680 --> 00:08:54,529
them beyond 16

00:08:51,920 --> 00:08:57,470
with Goudy and standard Ethernet

00:08:54,529 --> 00:09:00,170
switching you can build much larger

00:08:57,470 --> 00:09:02,660
systems performing model parallel

00:09:00,170 --> 00:09:04,970
training and therefore be able to handle

00:09:02,660 --> 00:09:08,059
much bigger models so this means that

00:09:04,970 --> 00:09:12,309
with Goudy we are raising the bar on

00:09:08,059 --> 00:09:15,410
what AI training can do lastly and most

00:09:12,309 --> 00:09:18,470
importantly for data centers is avoid

00:09:15,410 --> 00:09:22,100
being locked in to proprietary system

00:09:18,470 --> 00:09:24,379
interfaces by insisting not to use

00:09:22,100 --> 00:09:27,410
processes that come with proprietary

00:09:24,379 --> 00:09:30,470
system interfaces and instead insisting

00:09:27,410 --> 00:09:33,410
on using only standards-based scaling

00:09:30,470 --> 00:09:36,709
data center can avoid being locked into

00:09:33,410 --> 00:09:39,489
any process of vendor by choosing

00:09:36,709 --> 00:09:42,259
Ethernet networking as they're scaling

00:09:39,489 --> 00:09:45,859
infrastructure they can enjoy a healthy

00:09:42,259 --> 00:09:48,499
ecosystem of suppliers this way they can

00:09:45,859 --> 00:09:52,040
replace a process of vendor without

00:09:48,499 --> 00:09:55,839
tearing down the infrastructure so with

00:09:52,040 --> 00:09:58,959
Goudy you can realize all these benefits

00:09:55,839 --> 00:09:58,959
thank you

00:10:04,980 --> 00:10:07,040

YouTube URL: https://www.youtube.com/watch?v=ZJXbQPDFVjo


