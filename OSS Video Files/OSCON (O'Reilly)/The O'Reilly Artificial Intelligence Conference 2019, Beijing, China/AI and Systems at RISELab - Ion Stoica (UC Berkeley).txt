Title: AI and Systems at RISELab - Ion Stoica (UC Berkeley)
Publication date: 2019-06-28
Playlist: The O'Reilly Artificial Intelligence Conference 2019, Beijing, China
Description: 
	Ion Stoica outlines a few projects at the intersection of AI and systems that RISELab, at the University of California, Berkeley, is developing. RISELab is the successor of AMPLab, where several highly successful open source projects, including Apache Spark and Apache Mesos, were developed.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,060 --> 00:00:06,750
so next let me start with the Ray so

00:00:03,870 --> 00:00:09,809
today we are seeing two trends when it

00:00:06,750 --> 00:00:11,700
comes to application workloads one is

00:00:09,809 --> 00:00:14,490
application becoming are becoming more

00:00:11,700 --> 00:00:17,510
and more distributed the second this

00:00:14,490 --> 00:00:20,070
applications are becoming more complex

00:00:17,510 --> 00:00:23,699
there are two reasons the application

00:00:20,070 --> 00:00:28,650
are becoming distributed one as everyone

00:00:23,699 --> 00:00:31,590
knows Moore's law is dead and while you

00:00:28,650 --> 00:00:34,489
are hearing a lot about specialized

00:00:31,590 --> 00:00:37,260
hardware which push the boundaries of

00:00:34,489 --> 00:00:38,879
computation when it comes to memory

00:00:37,260 --> 00:00:41,820
things are little bit different and

00:00:38,879 --> 00:00:44,550
that's because then our scaling which is

00:00:41,820 --> 00:00:47,489
a sister of the Moore's law and it used

00:00:44,550 --> 00:00:51,059
to be that is as every 18 months in the

00:00:47,489 --> 00:00:53,910
same surface you can put twice as many

00:00:51,059 --> 00:00:56,030
transistors so therefore the memory

00:00:53,910 --> 00:00:59,430
increases doubles every 18 months

00:00:56,030 --> 00:01:02,969
however now as you put more transistors

00:00:59,430 --> 00:01:06,270
you are going to draw more power so you

00:01:02,969 --> 00:01:08,159
can no longer do that so fundamentally

00:01:06,270 --> 00:01:12,150
you can order memory is not really

00:01:08,159 --> 00:01:14,970
increasing the second reason is at the

00:01:12,150 --> 00:01:17,939
demands of data processing and machine

00:01:14,970 --> 00:01:21,119
learning are increasing faster than ever

00:01:17,939 --> 00:01:24,240
and this is a plot from open AI which

00:01:21,119 --> 00:01:26,939
shows the amount of computation needed

00:01:24,240 --> 00:01:30,810
to trace the largest model if you know

00:01:26,939 --> 00:01:33,360
at a given time the log the Y scale is a

00:01:30,810 --> 00:01:36,350
log scale and basically what this tells

00:01:33,360 --> 00:01:39,479
you that the amount of computation

00:01:36,350 --> 00:01:43,460
needed to train the largest machine

00:01:39,479 --> 00:01:47,640
learning model doubles every 3.5 months

00:01:43,460 --> 00:01:50,340
if you do the math this is over 30 times

00:01:47,640 --> 00:01:54,000
faster than the Moore's Law and that was

00:01:50,340 --> 00:01:55,829
once the Moore's law used to work so

00:01:54,000 --> 00:01:59,340
really there is no choice but

00:01:55,829 --> 00:02:01,110
distributed applications also the

00:01:59,340 --> 00:02:03,570
applications are becoming more complex

00:02:01,110 --> 00:02:06,060
and many of them are becoming AI centric

00:02:03,570 --> 00:02:08,220
today almost all of the applications are

00:02:06,060 --> 00:02:11,640
including components or plan to include

00:02:08,220 --> 00:02:14,010
components which are doing AI is this

00:02:11,640 --> 00:02:16,620
include you know financial application

00:02:14,010 --> 00:02:18,780
application in manufacturing IOT like

00:02:16,620 --> 00:02:22,860
you heard in the previous talk and many

00:02:18,780 --> 00:02:24,799
more so just to make to take an example

00:02:22,860 --> 00:02:27,599
consider this in app promotion

00:02:24,799 --> 00:02:29,610
application so here we want to provide

00:02:27,599 --> 00:02:31,739
recommendation to the user about a

00:02:29,610 --> 00:02:33,180
particular service a particular product

00:02:31,739 --> 00:02:35,400
the user may be interested in

00:02:33,180 --> 00:02:37,920
so these to implement this application

00:02:35,400 --> 00:02:40,379
is a high-level it's quite simple as a

00:02:37,920 --> 00:02:42,659
pipeline you get a lot from the users

00:02:40,379 --> 00:02:45,359
you do some data processing feature is

00:02:42,659 --> 00:02:48,690
Asian then you train you build a model

00:02:45,359 --> 00:02:51,329
and then you serve that model ok so it's

00:02:48,690 --> 00:02:53,819
what you do of course things get a

00:02:51,329 --> 00:02:57,090
little bit more complex because you also

00:02:53,819 --> 00:02:58,889
want to retrain and to have as fast and

00:02:57,090 --> 00:03:00,930
frequent as possible to have fresh

00:02:58,889 --> 00:03:05,609
models which are based on the latest

00:03:00,930 --> 00:03:08,280
data and if you look how many companies

00:03:05,609 --> 00:03:10,319
implement these pipelines they do so by

00:03:08,280 --> 00:03:12,930
integrating the best-of-breed frameworks

00:03:10,319 --> 00:03:16,319
so for data ingestion virtualization the

00:03:12,930 --> 00:03:18,599
use kafka spark and or fling for

00:03:16,319 --> 00:03:20,910
trainings I may use tensorflow

00:03:18,599 --> 00:03:22,590
distributed tensorflow pythor

00:03:20,910 --> 00:03:23,010
distributed horrible and things like

00:03:22,590 --> 00:03:25,650
that

00:03:23,010 --> 00:03:30,389
and for serving tensorflow serving

00:03:25,650 --> 00:03:32,519
clipper or other systems and again this

00:03:30,389 --> 00:03:35,849
is a natural solution however he has

00:03:32,519 --> 00:03:38,609
some problems it's hard to develop

00:03:35,849 --> 00:03:40,739
because you have to develop against

00:03:38,609 --> 00:03:43,980
different API and eventually using

00:03:40,739 --> 00:03:46,739
different languages it's also hard to

00:03:43,980 --> 00:03:49,019
manage and deploy it in production and

00:03:46,739 --> 00:03:52,799
furthermore if you look at the end to

00:03:49,019 --> 00:03:55,109
end delay that's pretty is not great why

00:03:52,799 --> 00:03:56,790
because between difference is different

00:03:55,109 --> 00:03:57,989
systems you need to move the data and

00:03:56,790 --> 00:03:59,760
you need to use different storage

00:03:57,989 --> 00:04:03,510
systems different formats and things

00:03:59,760 --> 00:04:05,909
like that so it slows you down and again

00:04:03,510 --> 00:04:07,739
this is a simple pipeline in general you

00:04:05,909 --> 00:04:10,470
don't only training you do high

00:04:07,739 --> 00:04:13,799
parameter tuning you don't do only

00:04:10,470 --> 00:04:16,609
serving you do IB testing and as more

00:04:13,799 --> 00:04:18,690
and more companies are starting to

00:04:16,609 --> 00:04:21,030
experiment with reinforcement learning

00:04:18,690 --> 00:04:23,159
you are not only interacting with the

00:04:21,030 --> 00:04:26,940
real world you are also going to

00:04:23,159 --> 00:04:27,780
interact with large-scale simulations so

00:04:26,940 --> 00:04:29,850
now if you think

00:04:27,780 --> 00:04:33,630
all these applications there are a few

00:04:29,850 --> 00:04:35,370
ways you can implement them one is to

00:04:33,630 --> 00:04:38,010
use this domain-specific framework like

00:04:35,370 --> 00:04:39,600
we discussed and these are usually easy

00:04:38,010 --> 00:04:42,780
to use because they provide a high-level

00:04:39,600 --> 00:04:46,830
API but they are obviously not a general

00:04:42,780 --> 00:04:49,340
and if your application uses all single

00:04:46,830 --> 00:04:51,960
framework then you have a easy job

00:04:49,340 --> 00:04:53,700
however if your application use

00:04:51,960 --> 00:04:55,290
different frameworks has to use

00:04:53,700 --> 00:04:57,720
different frameworks like we discussed

00:04:55,290 --> 00:04:59,730
then it's much more difficult much

00:04:57,720 --> 00:05:02,310
harder because you need to develop

00:04:59,730 --> 00:05:05,090
deploy manage different frameworks and

00:05:02,310 --> 00:05:07,380
the entrant performance is pretty low so

00:05:05,090 --> 00:05:09,870
this is one way you can implement these

00:05:07,380 --> 00:05:13,820
applications what is another way well

00:05:09,870 --> 00:05:17,280
you can use a generic framework or

00:05:13,820 --> 00:05:21,419
platform like say kubernetes RPC plus

00:05:17,280 --> 00:05:26,100
some Dockers and on top of that you can

00:05:21,419 --> 00:05:28,770
implement your solution and this will

00:05:26,100 --> 00:05:30,570
provide high performance it's you know a

00:05:28,770 --> 00:05:33,180
specialized solution for your problem

00:05:30,570 --> 00:05:34,919
however it incurs huge cost in terms of

00:05:33,180 --> 00:05:39,270
engineering and time to develop time to

00:05:34,919 --> 00:05:42,030
market recognizes gap many public clouds

00:05:39,270 --> 00:05:46,229
they they they came up with cloud

00:05:42,030 --> 00:05:49,610
functions and these are more general is

00:05:46,229 --> 00:05:53,250
the domain-specific frameworks and

00:05:49,610 --> 00:05:55,740
therefore it makes it easy to implement

00:05:53,250 --> 00:05:58,950
some category of applications however

00:05:55,740 --> 00:06:02,220
they are not as general for instance is

00:05:58,950 --> 00:06:03,990
very hard to use cloud functions to

00:06:02,220 --> 00:06:06,900
implement distributed training or

00:06:03,990 --> 00:06:09,060
serving furthermore there is no standard

00:06:06,900 --> 00:06:12,840
every public cloud provider provides its

00:06:09,060 --> 00:06:13,940
own API so vendors are pretty concerns

00:06:12,840 --> 00:06:16,260
about lock-in

00:06:13,940 --> 00:06:18,960
so there is no good platform for

00:06:16,260 --> 00:06:22,740
distributed applications and here what

00:06:18,960 --> 00:06:26,789
were the ray stands sir ray is aimed to

00:06:22,740 --> 00:06:29,910
fill this gap so ray is general easy to

00:06:26,789 --> 00:06:33,780
use and has high performance in terms of

00:06:29,910 --> 00:06:35,520
generality ray aims to take this word in

00:06:33,780 --> 00:06:40,140
which you have for each of these

00:06:35,520 --> 00:06:41,900
distributed workloads using a particular

00:06:40,140 --> 00:06:44,759
distributed frame or

00:06:41,900 --> 00:06:47,789
to a ward in which you are going to use

00:06:44,759 --> 00:06:51,840
a unified framework that's Ray and

00:06:47,789 --> 00:06:56,759
support these workloads on top of it by

00:06:51,840 --> 00:06:58,860
using powerful libraries and Ray it's

00:06:56,759 --> 00:07:02,009
also easy to use you can actually

00:06:58,860 --> 00:07:04,409
paralyze in many cases your code with

00:07:02,009 --> 00:07:07,379
just a few changes there is no need to

00:07:04,409 --> 00:07:10,499
learn and new API sorry architect for a

00:07:07,379 --> 00:07:13,080
new API and here it's a simple example

00:07:10,499 --> 00:07:15,689
you have a single threaded higher

00:07:13,080 --> 00:07:17,189
parameter search this is around 100

00:07:15,689 --> 00:07:18,990
lines of code you don't need to read

00:07:17,189 --> 00:07:20,520
them actually this the entire

00:07:18,990 --> 00:07:23,729
application is a few hundred lines of

00:07:20,520 --> 00:07:26,939
code and this runs in a single thread on

00:07:23,729 --> 00:07:29,129
a single core so to make it run on

00:07:26,939 --> 00:07:31,169
hundreds or thousands or than a thousand

00:07:29,129 --> 00:07:33,990
tens of thousands of course the only

00:07:31,169 --> 00:07:36,060
thing you need to do is to are these

00:07:33,990 --> 00:07:38,370
lines there are several lines of code

00:07:36,060 --> 00:07:40,800
and it's the important point here is not

00:07:38,370 --> 00:07:42,749
that you need to add only seven lines of

00:07:40,800 --> 00:07:45,029
code which basically identify which

00:07:42,749 --> 00:07:47,189
functions and so forth you want to run

00:07:45,029 --> 00:07:48,960
remotely and in parallel the point here

00:07:47,189 --> 00:07:53,819
is don't need to change the structure of

00:07:48,960 --> 00:07:58,199
the code in terms of performance ray

00:07:53,819 --> 00:08:00,509
it's matching or even exceed the

00:07:58,199 --> 00:08:03,539
performance of specialized systems here

00:08:00,509 --> 00:08:04,889
I am just showing ray against a special

00:08:03,539 --> 00:08:07,020
assistance which provides serving

00:08:04,889 --> 00:08:11,610
training and simulations higher is

00:08:07,020 --> 00:08:15,509
better today ray already provides

00:08:11,610 --> 00:08:17,639
support for some libraries in for

00:08:15,509 --> 00:08:19,620
enforcement learning are a Lib

00:08:17,639 --> 00:08:21,740
based on the number of contributor this

00:08:19,620 --> 00:08:23,849
is a most popular large-scale

00:08:21,740 --> 00:08:26,339
reinforcement learning library high

00:08:23,849 --> 00:08:27,990
parameter search the library called tune

00:08:26,339 --> 00:08:31,579
and data processing mode in which is

00:08:27,990 --> 00:08:33,570
dropping replacement for pandas and many

00:08:31,579 --> 00:08:36,510
companies implement this with many

00:08:33,570 --> 00:08:39,000
disability applications we are seeing a

00:08:36,510 --> 00:08:42,329
growing adoptions with ray we have over

00:08:39,000 --> 00:08:44,940
160 contributors from 40 companies more

00:08:42,329 --> 00:08:47,370
than five tutorials at Oralia I we had

00:08:44,940 --> 00:08:50,910
yesterday a tutorial here is included in

00:08:47,370 --> 00:08:53,490
aw Sh maker in terms of the github stars

00:08:50,910 --> 00:08:55,290
it compares favorably with systems like

00:08:53,490 --> 00:08:58,100
cough cough link and spark

00:08:55,290 --> 00:09:00,780
right here it's a thick blue line and

00:08:58,100 --> 00:09:03,540
it's used by many companies including

00:09:00,780 --> 00:09:07,140
and financial Alibaba and many others in

00:09:03,540 --> 00:09:09,840
production today and you can easily try

00:09:07,140 --> 00:09:12,920
to play with it just using peeping stock

00:09:09,840 --> 00:09:12,920
pip install Dre

00:09:18,410 --> 00:09:20,470

YouTube URL: https://www.youtube.com/watch?v=dIdhT0W0-IQ


