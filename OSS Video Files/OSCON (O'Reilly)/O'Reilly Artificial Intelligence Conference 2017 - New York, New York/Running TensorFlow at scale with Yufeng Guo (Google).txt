Title: Running TensorFlow at scale with Yufeng Guo (Google)
Publication date: 2017-07-05
Playlist: O'Reilly Artificial Intelligence Conference 2017 - New York, New York
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	00:00:00,030 --> 00:00:04,830
hi this is Mike Hendrickson from AI

00:00:02,129 --> 00:00:06,420
conference in New York I'm here with you

00:00:04,830 --> 00:00:06,810
thing from Google you think how you

00:00:06,420 --> 00:00:09,150
doing

00:00:06,810 --> 00:00:11,670
very good thanks Mike so you're with the

00:00:09,150 --> 00:00:13,920
cloud and tensorflow group or can you

00:00:11,670 --> 00:00:14,639
explain kind of where you sit in Google

00:00:13,920 --> 00:00:16,920
short

00:00:14,639 --> 00:00:18,980
so among the cloud developer advocacy

00:00:16,920 --> 00:00:21,810
team and I focus on machine learning

00:00:18,980 --> 00:00:24,869
developer advocates we are kind of the

00:00:21,810 --> 00:00:27,570
bridge between the outside developer

00:00:24,869 --> 00:00:30,420
community who uses Google to develop a

00:00:27,570 --> 00:00:31,980
product and we sit between them and the

00:00:30,420 --> 00:00:34,890
internal product and engineering teams

00:00:31,980 --> 00:00:36,450
so we help kind of bridge that line of

00:00:34,890 --> 00:00:39,329
communication so that means we go both

00:00:36,450 --> 00:00:41,670
ways externally we might presented

00:00:39,329 --> 00:00:43,829
conferences make videos maybe some

00:00:41,670 --> 00:00:45,090
services write blog posts things like

00:00:43,829 --> 00:00:46,920
that interact with the community and

00:00:45,090 --> 00:00:49,440
through that collect the feedback and

00:00:46,920 --> 00:00:51,449
learn about what's working what's

00:00:49,440 --> 00:00:53,520
confusing what's hard and bring that

00:00:51,449 --> 00:00:55,829
feedback back into the developer teams

00:00:53,520 --> 00:00:57,870
and engineering teams making those

00:00:55,829 --> 00:00:59,460
products and helping make that better so

00:00:57,870 --> 00:01:01,559
you're with a particularly interesting

00:00:59,460 --> 00:01:04,500
group though I mean cloud and then

00:01:01,559 --> 00:01:06,780
tensorflow is part of that or related to

00:01:04,500 --> 00:01:08,850
that yeah so tensorflow is Google's open

00:01:06,780 --> 00:01:12,540
source machine learning library who will

00:01:08,850 --> 00:01:15,479
open source that in late 2015 and since

00:01:12,540 --> 00:01:17,189
then it's really grown spectacularly in

00:01:15,479 --> 00:01:18,240
popularity the community community

00:01:17,189 --> 00:01:21,110
around that has been really fantastic

00:01:18,240 --> 00:01:23,790
and tensorflow well being open source

00:01:21,110 --> 00:01:25,740
means that you can run it anywhere we

00:01:23,790 --> 00:01:28,170
wanted to make it easy for you to run at

00:01:25,740 --> 00:01:30,390
scale for folks with a lot of data or

00:01:28,170 --> 00:01:31,650
really complicated training jobs being

00:01:30,390 --> 00:01:33,270
with our one machine learning the cloud

00:01:31,650 --> 00:01:36,030
is definitely something that folks want

00:01:33,270 --> 00:01:38,430
to be able to do and so cloud machine

00:01:36,030 --> 00:01:41,280
learning engine is our packaged product

00:01:38,430 --> 00:01:43,200
that provides managed infrastructure to

00:01:41,280 --> 00:01:45,450
run tensorflow in the cloud and what's

00:01:43,200 --> 00:01:47,549
great about it is that you can take your

00:01:45,450 --> 00:01:49,350
experimental central code that you ran

00:01:47,549 --> 00:01:51,780
on your local machine stay with a subset

00:01:49,350 --> 00:01:53,939
of your data and just push it up to the

00:01:51,780 --> 00:01:55,110
cloud with no real meaningful click

00:01:53,939 --> 00:01:57,689
changes that you have to make you have

00:01:55,110 --> 00:02:01,110
to use any kind of there's no machine

00:01:57,689 --> 00:02:03,509
learning engine sdk or library you need

00:02:01,110 --> 00:02:06,240
to add in it's just tensorflow code and

00:02:03,509 --> 00:02:07,890
you're pulling your data and let it run

00:02:06,240 --> 00:02:09,810
you just pass into some parameters

00:02:07,890 --> 00:02:10,679
specifying how many machines you want

00:02:09,810 --> 00:02:12,540
you

00:02:10,679 --> 00:02:14,549
graphics units like GPUs and things like

00:02:12,540 --> 00:02:16,230
that to accelerate your training but the

00:02:14,549 --> 00:02:17,939
actual machine learning code gets to

00:02:16,230 --> 00:02:18,420
remain the same that same open-source

00:02:17,939 --> 00:02:20,159
code

00:02:18,420 --> 00:02:23,430
sounds like a developer could use this

00:02:20,159 --> 00:02:24,720
to go wide and deep and we talked a

00:02:23,430 --> 00:02:27,359
little bit about that but sounds like

00:02:24,720 --> 00:02:29,129
you're covering great breadth and then

00:02:27,359 --> 00:02:34,079
you can also go in deep on things so

00:02:29,129 --> 00:02:36,569
yeah so in terms of the toilet order so

00:02:34,079 --> 00:02:38,939
with the toilet you can tensorflow is

00:02:36,569 --> 00:02:40,950
for custom models we want to train your

00:02:38,939 --> 00:02:42,720
own models it's great for research and

00:02:40,950 --> 00:02:45,209
it's great for production because you

00:02:42,720 --> 00:02:46,440
can sort of translate and use them for

00:02:45,209 --> 00:02:48,239
both without really needing to change

00:02:46,440 --> 00:02:50,040
your code because everything can be

00:02:48,239 --> 00:02:52,230
written in Python which is very friendly

00:02:50,040 --> 00:02:54,629
to development but then it all gets

00:02:52,230 --> 00:02:57,090
compiled down within execution time to

00:02:54,629 --> 00:03:00,120
C++ so that it's very fast as opposed

00:02:57,090 --> 00:03:02,010
built on a distributed C++ execution

00:03:00,120 --> 00:03:04,950
engine and that allows it to run very

00:03:02,010 --> 00:03:06,900
quickly in a distributed environment on

00:03:04,950 --> 00:03:08,459
the going wide side I mean there's the

00:03:06,900 --> 00:03:10,230
community aspect of it which is really

00:03:08,459 --> 00:03:13,170
great you know you it covers a wide

00:03:10,230 --> 00:03:14,639
breadth of different types of research

00:03:13,170 --> 00:03:16,049
and models and what's really been

00:03:14,639 --> 00:03:17,669
interesting is since it's been

00:03:16,049 --> 00:03:21,750
open-source we've really seen a growth

00:03:17,669 --> 00:03:23,730
in research being published using

00:03:21,750 --> 00:03:25,859
tensorflow and what i mean by that is

00:03:23,730 --> 00:03:28,260
not only is the research being published

00:03:25,859 --> 00:03:30,209
at the PDF with the article explaining

00:03:28,260 --> 00:03:32,489
the methods explaining the results join

00:03:30,209 --> 00:03:34,379
some nice charts but then the code shows

00:03:32,489 --> 00:03:36,750
up on github and it's tensorflow code

00:03:34,379 --> 00:03:38,489
and folks you know from the community

00:03:36,750 --> 00:03:40,470
they can take that code and they can run

00:03:38,489 --> 00:03:42,930
it which means we have reproducible

00:03:40,470 --> 00:03:45,329
science and that's really fantastic so

00:03:42,930 --> 00:03:47,579
is easy way to find that just search for

00:03:45,329 --> 00:03:48,359
tensorflow and github yeah so on github

00:03:47,579 --> 00:03:51,359
we have a country-club

00:03:48,359 --> 00:03:52,739
organization so the actual central code

00:03:51,359 --> 00:03:55,109
itself is open source the entire library

00:03:52,739 --> 00:03:55,769
so that's just github.com slash

00:03:55,109 --> 00:03:57,959
tensorflow

00:03:55,769 --> 00:03:59,699
slashed enter foil again because the

00:03:57,959 --> 00:04:01,709
first one is the organization the second

00:03:59,699 --> 00:04:04,169
one is the repository ok and if you want

00:04:01,709 --> 00:04:05,609
to look at some of the models that these

00:04:04,169 --> 00:04:06,739
are specifically the Google ones it

00:04:05,609 --> 00:04:11,940
would be tensorflow

00:04:06,739 --> 00:04:13,169
slash models ok so this morning we had

00:04:11,940 --> 00:04:16,289
some really interesting keynotes

00:04:13,169 --> 00:04:19,349
talking about machine learning and AI in

00:04:16,289 --> 00:04:21,029
medical and I know John Hopkins is doing

00:04:19,349 --> 00:04:23,950
some really interesting things there is

00:04:21,029 --> 00:04:25,900
is this where we are in

00:04:23,950 --> 00:04:29,320
in the world right now with we're just

00:04:25,900 --> 00:04:31,120
starting our AI journey because it seems

00:04:29,320 --> 00:04:33,670
like we have a long ways to go to

00:04:31,120 --> 00:04:36,010
actually make this work but it seems

00:04:33,670 --> 00:04:38,320
like we're on the journey now it's

00:04:36,010 --> 00:04:39,610
really starting to get going yeah that

00:04:38,320 --> 00:04:42,280
were you see it yeah it really feels

00:04:39,610 --> 00:04:44,770
like things are picking up pace even a

00:04:42,280 --> 00:04:47,110
few short years ago a lot of this

00:04:44,770 --> 00:04:49,330
couldn't have been conceivable as things

00:04:47,110 --> 00:04:50,770
that we could either do or get away with

00:04:49,330 --> 00:04:53,080
doing but you know whether its

00:04:50,770 --> 00:04:55,090
compliance issues and privacy and things

00:04:53,080 --> 00:04:56,350
like that sort of preventing a lot of

00:04:55,090 --> 00:04:58,390
this development happening but we've

00:04:56,350 --> 00:05:00,360
overcome some of those hurdles and now

00:04:58,390 --> 00:05:02,800
we're seeing leaps and bounds in

00:05:00,360 --> 00:05:05,110
advances in both the research and the

00:05:02,800 --> 00:05:08,140
science one of the things I'll call out

00:05:05,110 --> 00:05:09,730
is a lot of the kind of image related

00:05:08,140 --> 00:05:12,120
things you kind of alluded to this

00:05:09,730 --> 00:05:15,520
whether it's skin cancer detection

00:05:12,120 --> 00:05:17,950
there's one for diabetic retinopathy so

00:05:15,520 --> 00:05:19,930
that's looking at the back of the eye

00:05:17,950 --> 00:05:22,990
those images if you look at someone when

00:05:19,930 --> 00:05:24,400
you compare the two one is showing the

00:05:22,990 --> 00:05:27,160
healthy eye Anil one it's one that's

00:05:24,400 --> 00:05:28,750
like starting to degenerate and the

00:05:27,160 --> 00:05:30,250
difference is so subtle even when

00:05:28,750 --> 00:05:32,080
someone marks it's like it's that little

00:05:30,250 --> 00:05:35,230
little area right there that's slightly

00:05:32,080 --> 00:05:37,300
different and to have you know that

00:05:35,230 --> 00:05:39,580
systems that can detect that is really

00:05:37,300 --> 00:05:41,410
fantastic because trained doctors are

00:05:39,580 --> 00:05:43,870
not exactly they're going to miss the

00:05:41,410 --> 00:05:45,880
most plentiful or will they miss it the

00:05:43,870 --> 00:05:47,350
doctor for sure I mean occasionally they

00:05:45,880 --> 00:05:50,200
might miss it but I think the more

00:05:47,350 --> 00:05:53,110
notable point here is that it's a highly

00:05:50,200 --> 00:05:55,480
specialized skill to do to do this task

00:05:53,110 --> 00:05:58,810
and the availability of these doctors

00:05:55,480 --> 00:06:00,700
especially beyond places like the US

00:05:58,810 --> 00:06:02,590
right in say third world countries and

00:06:00,700 --> 00:06:05,230
just in rural places where you don't

00:06:02,590 --> 00:06:09,160
have access to that level of specialized

00:06:05,230 --> 00:06:11,680
care allows this kind of I wouldn't say

00:06:09,160 --> 00:06:14,950
democratization of medicine but more of

00:06:11,680 --> 00:06:16,360
just access to this type of medicine and

00:06:14,950 --> 00:06:18,340
what's really nice about this particular

00:06:16,360 --> 00:06:22,060
application is that it is a highly

00:06:18,340 --> 00:06:25,390
treatable situation but only if detected

00:06:22,060 --> 00:06:28,210
early and if not you go get and you end

00:06:25,390 --> 00:06:31,060
up with permanent blindness so the the

00:06:28,210 --> 00:06:34,720
two are so starkly different so it made

00:06:31,060 --> 00:06:37,150
it for a really worthwhile of tasks to

00:06:34,720 --> 00:06:40,120
get after so you're giving a talk later

00:06:37,150 --> 00:06:42,430
yeah yeah so widen D that's right

00:06:40,120 --> 00:06:44,380
so speaking of you know going wide and

00:06:42,430 --> 00:06:46,120
deep on the topic later today I'll be

00:06:44,380 --> 00:06:47,949
talking about a machine learning model

00:06:46,120 --> 00:06:49,449
it's a piece of research that Google

00:06:47,949 --> 00:06:52,270
brain published a few years ago called

00:06:49,449 --> 00:06:53,800
wide and deep and what it what's

00:06:52,270 --> 00:06:56,199
interesting about it is that since then

00:06:53,800 --> 00:06:58,300
it has been packaged into pencil it's

00:06:56,199 --> 00:07:00,490
one of the library calls in it intense

00:06:58,300 --> 00:07:03,610
flow itself and that makes it really

00:07:00,490 --> 00:07:05,560
easy to use has kind of a first pass on

00:07:03,610 --> 00:07:08,350
structured data so that instead of just

00:07:05,560 --> 00:07:11,139
throwing it in a spreadsheet charting

00:07:08,350 --> 00:07:13,720
out and just drawing a linear line you

00:07:11,139 --> 00:07:16,900
can get a much more sophisticated level

00:07:13,720 --> 00:07:18,699
of prediction for only a little bit more

00:07:16,900 --> 00:07:20,680
effort because the tooling has gotten to

00:07:18,699 --> 00:07:23,110
that stage where you can essentially use

00:07:20,680 --> 00:07:26,320
virtually the same code and just swap

00:07:23,110 --> 00:07:28,180
out the data set so you think you and I

00:07:26,320 --> 00:07:31,479
were to sit down 12 months from now and

00:07:28,180 --> 00:07:33,880
have the same conversation what would

00:07:31,479 --> 00:07:35,919
you like to say has changed with

00:07:33,880 --> 00:07:39,130
tensorflow during that 12 month period

00:07:35,919 --> 00:07:41,260
what would you like to see Google doing

00:07:39,130 --> 00:07:45,010
differently 12 months from now

00:07:41,260 --> 00:07:47,919
yeah so in the next year I think what

00:07:45,010 --> 00:07:49,660
will really enable an even greater

00:07:47,919 --> 00:07:51,639
uptake of machine learning and

00:07:49,660 --> 00:07:53,530
artificial intelligence development will

00:07:51,639 --> 00:07:55,450
be further improvements in the toy as

00:07:53,530 --> 00:07:57,910
good as it is today there are still many

00:07:55,450 --> 00:07:59,020
places for improvements and this is one

00:07:57,910 --> 00:08:00,880
of the areas where I work in right

00:07:59,020 --> 00:08:02,020
bringing that feedback and trying to

00:08:00,880 --> 00:08:04,300
figure out how can we make it even

00:08:02,020 --> 00:08:07,840
easier for developers to get started

00:08:04,300 --> 00:08:10,660
keep going and really spread their wings

00:08:07,840 --> 00:08:13,449
and make either contributions also back

00:08:10,660 --> 00:08:15,610
to the library itself which is fantastic

00:08:13,449 --> 00:08:18,880
as well as developing their own models

00:08:15,610 --> 00:08:21,340
and coming up with creative ways to use

00:08:18,880 --> 00:08:24,810
machine learning I think just like the

00:08:21,340 --> 00:08:27,760
internet enabled the whole new space of

00:08:24,810 --> 00:08:30,099
possibilities of what was possible so to

00:08:27,760 --> 00:08:31,539
a machine learning enabled us to do

00:08:30,099 --> 00:08:33,459
things that we can't even imagine are

00:08:31,539 --> 00:08:35,950
possible today and I don't even mean

00:08:33,459 --> 00:08:37,959
like super crazy far-fetched ones that

00:08:35,950 --> 00:08:40,659
we think of in sci-fi but truly like

00:08:37,959 --> 00:08:42,430
things that we think of but once they

00:08:40,659 --> 00:08:44,350
arrive we become somewhat obvious

00:08:42,430 --> 00:08:46,360
sometimes like oh why didn't we do this

00:08:44,350 --> 00:08:49,000
before and that's that's really exciting

00:08:46,360 --> 00:08:50,410
in terms of the specifics around tooling

00:08:49,000 --> 00:08:52,750
I think we should do a lot better and

00:08:50,410 --> 00:08:55,360
terms are distributed machine money just

00:08:52,750 --> 00:08:57,550
has a field it's still very hard to do a

00:08:55,360 --> 00:08:59,670
distributed learning and what the trivet

00:08:57,550 --> 00:09:02,280
learning allows you to do is access

00:08:59,670 --> 00:09:04,330
faster training large pool of data

00:09:02,280 --> 00:09:06,370
distributing across thoughtful machines

00:09:04,330 --> 00:09:09,100
and are we going to start distributing

00:09:06,370 --> 00:09:12,190
that at the edge like in the field with

00:09:09,100 --> 00:09:13,960
IOT as well so IOT is a really

00:09:12,190 --> 00:09:15,490
interesting de armes they mail piece

00:09:13,960 --> 00:09:17,500
that throw into there there was a piece

00:09:15,490 --> 00:09:20,110
of research a few months ago called a

00:09:17,500 --> 00:09:22,720
federated machine learning and what that

00:09:20,110 --> 00:09:25,600
allowed was it was a method of

00:09:22,720 --> 00:09:27,490
cryptography that allowed devices at the

00:09:25,600 --> 00:09:31,300
edge either mobile devices or IOT

00:09:27,490 --> 00:09:33,610
devices to collect data and do like

00:09:31,300 --> 00:09:35,500
basically special kind of like averaging

00:09:33,610 --> 00:09:36,550
of data so you get anonymized because

00:09:35,500 --> 00:09:39,460
one of the things about collecting

00:09:36,550 --> 00:09:43,860
mobile royalty data is privacy and so by

00:09:39,460 --> 00:09:46,330
having a cryptographically secure way of

00:09:43,860 --> 00:09:48,130
coming combining the data together at

00:09:46,330 --> 00:09:50,380
the edge and then bringing it centrally

00:09:48,130 --> 00:09:52,600
to finish the training had in a BC

00:09:50,380 --> 00:09:55,270
server that was a really interesting

00:09:52,600 --> 00:09:56,800
thing and I think we'll see that sort of

00:09:55,270 --> 00:09:58,780
application become more and more

00:09:56,800 --> 00:10:00,160
prevalent excellent you're saying we

00:09:58,780 --> 00:10:03,630
look for to that conversation to a

00:10:00,160 --> 00:10:03,630
month's often things a lot like

00:10:10,050 --> 00:10:12,110

YouTube URL: https://www.youtube.com/watch?v=LVDefvPIzDA


