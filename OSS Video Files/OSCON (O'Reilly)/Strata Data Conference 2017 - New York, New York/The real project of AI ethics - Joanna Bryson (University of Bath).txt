Title: The real project of AI ethics - Joanna Bryson (University of Bath)
Publication date: 2017-09-28
Playlist: Strata Data Conference 2017 - New York, New York
Description: 
	AI has been with us for hundreds of years; there's no "singularity" step change. Joanna Bryson explains that the main threat of AI is not that it will do anything to us but what we are already doing to each other with itâ€”predicting and manipulating our own and others' behavior.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,580 --> 00:00:06,759
the meaning of a word is how it's used

00:00:04,270 --> 00:00:08,830
and that sounds reductionist but it

00:00:06,759 --> 00:00:12,610
works really well and we were able to

00:00:08,830 --> 00:00:15,369
demonstrate that using artificially

00:00:12,610 --> 00:00:18,430
parsed word embeddings so semantics

00:00:15,369 --> 00:00:20,080
based on how the word is used that we

00:00:18,430 --> 00:00:22,150
could replicate something that was this

00:00:20,080 --> 00:00:24,640
big mystery in psychology which is

00:00:22,150 --> 00:00:27,250
implicit bias that we find it for

00:00:24,640 --> 00:00:30,580
example easier to associate women's

00:00:27,250 --> 00:00:33,010
names with a home compared to males

00:00:30,580 --> 00:00:34,960
names of the home compared to women's

00:00:33,010 --> 00:00:37,750
names with careers a male's names with

00:00:34,960 --> 00:00:39,280
careers does that make sense so it's not

00:00:37,750 --> 00:00:41,500
that you can't say that women have

00:00:39,280 --> 00:00:46,000
careers it's this you're faster at

00:00:41,500 --> 00:00:47,530
saying that a woman has at home and a

00:00:46,000 --> 00:00:50,969
man is that a career than the other way

00:00:47,530 --> 00:00:53,920
around right now these same prejudice

00:00:50,969 --> 00:00:55,540
representations also told us about the

00:00:53,920 --> 00:00:57,640
real world and that's what the graph is

00:00:55,540 --> 00:01:00,489
it's over on that side so that you know

00:00:57,640 --> 00:01:02,530
you remember your school x-axis on the

00:01:00,489 --> 00:01:06,130
bottom that's the proportion of women

00:01:02,530 --> 00:01:07,990
actually in jobs in 2015 so one of those

00:01:06,130 --> 00:01:10,149
blue dots is unfortunately programmer I

00:01:07,990 --> 00:01:12,069
used to be a programmer and then one of

00:01:10,149 --> 00:01:14,740
the red dots up there is nurse right so

00:01:12,069 --> 00:01:17,829
there's a lot of women in that job this

00:01:14,740 --> 00:01:20,380
prejudice representations that we had in

00:01:17,829 --> 00:01:24,399
our embeddings that replicated human

00:01:20,380 --> 00:01:27,189
implicit bias scores also correlated

00:01:24,399 --> 00:01:32,200
ninety percent with the US labor

00:01:27,189 --> 00:01:33,909
statistics so that's not telling us the

00:01:32,200 --> 00:01:36,490
AI is prejudiced and there's some

00:01:33,909 --> 00:01:38,560
horrible thing we have to do it's

00:01:36,490 --> 00:01:41,439
telling us something about the semantics

00:01:38,560 --> 00:01:43,539
that that our words the way we use them

00:01:41,439 --> 00:01:45,729
is capturing stuff that we see in the

00:01:43,539 --> 00:01:48,399
world and what's cool about that is that

00:01:45,729 --> 00:01:50,799
when we say all right it's not okay to

00:01:48,399 --> 00:01:52,479
say that all programmers are male what

00:01:50,799 --> 00:01:54,099
we're saying is that we're developing a

00:01:52,479 --> 00:01:55,359
new world we're going towards something

00:01:54,099 --> 00:01:57,399
all right

00:01:55,359 --> 00:02:01,859
AI and machine learning are picking up

00:01:57,399 --> 00:02:01,859
on the old world I hope that makes sense

00:02:07,830 --> 00:02:09,890

YouTube URL: https://www.youtube.com/watch?v=w-kZRdwlZ9E


