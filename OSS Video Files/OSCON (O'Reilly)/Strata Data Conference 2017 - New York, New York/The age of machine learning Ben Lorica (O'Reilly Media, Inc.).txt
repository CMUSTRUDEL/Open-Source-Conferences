Title: The age of machine learning Ben Lorica (O'Reilly Media, Inc.)
Publication date: 2017-09-27
Playlist: Strata Data Conference 2017 - New York, New York
Description: 
	Ben Lorica explores the age of machine learning.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,089 --> 00:00:06,310
so how do you become a machine learning

00:00:04,450 --> 00:00:08,309
company turns out there's three

00:00:06,310 --> 00:00:14,109
bottlenecks that you need to address

00:00:08,309 --> 00:00:16,029
data engineering and models so let me

00:00:14,109 --> 00:00:20,740
walk through each of these topics

00:00:16,029 --> 00:00:24,029
quickly we're still mostly dealing with

00:00:20,740 --> 00:00:28,359
supervised learning so in an era where

00:00:24,029 --> 00:00:31,720
you need label data training data is

00:00:28,359 --> 00:00:35,860
really of paramount importance so how do

00:00:31,720 --> 00:00:37,300
you get training data so the first thing

00:00:35,860 --> 00:00:39,700
you do is you start with data you

00:00:37,300 --> 00:00:42,340
already have so the basic things like

00:00:39,700 --> 00:00:44,680
data integration data enrichment so I

00:00:42,340 --> 00:00:46,630
say basic but actually a lot of us are

00:00:44,680 --> 00:00:48,430
still grappling with this right so data

00:00:46,630 --> 00:00:54,010
integration in a sense it's a problem

00:00:48,430 --> 00:00:55,630
that it's all ongoing there's also

00:00:54,010 --> 00:00:58,240
techniques in machine learning that

00:00:55,630 --> 00:01:01,060
allow you to get by with less training

00:00:58,240 --> 00:01:03,430
data or in the case of transfer learning

00:01:01,060 --> 00:01:07,439
take something you already know and

00:01:03,430 --> 00:01:12,219
apply it to a slightly similar problem

00:01:07,439 --> 00:01:15,130
looking ahead towards AI there are

00:01:12,219 --> 00:01:18,670
people working in computer vision who

00:01:15,130 --> 00:01:21,009
are doing almost purely unsupervised

00:01:18,670 --> 00:01:22,960
learning right so what they do is they

00:01:21,009 --> 00:01:25,840
learn the underlying structure of data

00:01:22,960 --> 00:01:31,600
and they know enough about the data to

00:01:25,840 --> 00:01:33,759
use it to build products and finally so

00:01:31,600 --> 00:01:37,030
this is fairly new I'm starting to hear

00:01:33,759 --> 00:01:39,009
people collaborate across organizations

00:01:37,030 --> 00:01:41,710
right so how do we set up data exchanges

00:01:39,009 --> 00:01:46,899
so that we can share data across our

00:01:41,710 --> 00:01:50,679
companies or our organizations next

00:01:46,899 --> 00:01:53,079
topic engineering so why do we need a

00:01:50,679 --> 00:01:54,429
new role for machine called machine

00:01:53,079 --> 00:01:57,039
learning engineers it's in data

00:01:54,429 --> 00:02:00,429
scientists enough well it turns out that

00:01:57,039 --> 00:02:03,729
if you go through and create a checklist

00:02:00,429 --> 00:02:06,880
of what you need to do to deploy machine

00:02:03,729 --> 00:02:08,440
learning models in production you end up

00:02:06,880 --> 00:02:09,970
with a long list right so there's

00:02:08,440 --> 00:02:12,879
several thoughts in this conference

00:02:09,970 --> 00:02:14,880
covering various aspects of this so for

00:02:12,879 --> 00:02:17,010
example what happens

00:02:14,880 --> 00:02:19,710
when you deploy a model in production

00:02:17,010 --> 00:02:21,540
how do you monitor whether the model

00:02:19,710 --> 00:02:23,220
still stay fresh

00:02:21,540 --> 00:02:26,520
whether you need to retrain it right so

00:02:23,220 --> 00:02:29,880
there's a talk later by David Talbott on

00:02:26,520 --> 00:02:34,980
this topic privacy and security so is

00:02:29,880 --> 00:02:37,260
there a way for you to in ensure privacy

00:02:34,980 --> 00:02:39,830
while still applying machine learning so

00:02:37,260 --> 00:02:42,930
this is an early research topic so

00:02:39,830 --> 00:02:44,910
academics are looking at machine

00:02:42,930 --> 00:02:49,500
learning methods that can operate over

00:02:44,910 --> 00:02:54,240
encrypted data for example finally

00:02:49,500 --> 00:02:58,020
models machine learning research is

00:02:54,240 --> 00:03:00,900
progressing at a rapid rate but the fact

00:02:58,020 --> 00:03:04,260
is companies can't really absorb that

00:03:00,900 --> 00:03:07,940
much new models or algorithms right so

00:03:04,260 --> 00:03:11,340
let's say for the sake of argument that

00:03:07,940 --> 00:03:12,150
machine learning research stalls for the

00:03:11,340 --> 00:03:15,240
next five years

00:03:12,150 --> 00:03:19,200
god forbid right I still think there's

00:03:15,240 --> 00:03:23,480
there's enough ideas now that we can

00:03:19,200 --> 00:03:23,480
start applying that will be will be fine

00:03:29,940 --> 00:03:32,000

YouTube URL: https://www.youtube.com/watch?v=MSfSx75tQXo


