Title: Hydrograph...Open Source ETL Tool for Hadoop - Shahab Kamal & Ankur Gupta (Bitwise)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              I'm shahab Kemal and from the price and                               I have with me William ma he's from                               capital one we got to talk about                               hydrograph hydrograph is a ETL solution                               available on Hadoop and we are planning                               to open source this in the near future                               today we will talk about the in the next                               Simmons about the quick architecture of                               hydrograph how it works on Hadoop how it                                takes the workloads how it pushes down                                to Hadoop and also about will give try                                to squeeze in a quick demo of hydrograph                                and what it looks and feels like quickly                                about bitwise we are a                                                 integration company based out of Chicago                                offices in pune india and australia                                sydney                                                           focused in the data space we do data                                warehousing been doing for the last                                   years and we have recently collaborated                                with capital one to actually build an                                ETL solution that works on Hadoop                                capital one I think I'll let William                                talk a couple of words about capital one                                and then we'll talk about the journey                                that we had with it we took together to                                actually build this tool Thank You sharp                                so William ah data engineering capital                                one and like shop said we partner                                together build ETL solution that we can                                use on Hadoop and just a little bit of                                capital one those you don't know us so                                we are focused on reimage and banking to                                help folks manage their money and one of                                the things we partner with bitwise to do                                is to enable us to process large amounts                                of data in our Hadoop infrastructure so                                together we decided you know to build                                hydrograph as an open source project and                                I think shahab will take you through how                                we're leveraging this and sort of give                                us a demo how it works                                so that's the architecture of hydrograph                                how it all works when you started a                                journey we were looking to move our data                                loads our workloads on top of Hadoop                                this was almost three years ago the                                initial thought was to write MapReduce                                natively on top of her do not the best                                option so we decided to create or I look                                for some kind of an abstraction on top                                of a rube that would be that would                                automatically generate MapReduce so at                                that point of time we picked the                                cascading framework as that abstraction                                layer cascading auto generates MapReduce                                days and fling so we use that as the                                abstraction layer and inside cascading                                we build our reusable components and                                functions that pretty much look and feel                                like any ETL functions like if you want                                to do a read file sort if you want to do                                aggregate reformat all of this those                                were built as a reusable function inside                                of cascading cascading auto generates                                the MapReduce code to execute these                                functions but to make it look like an                                ETL you write a Java wrapper around                                cascading to call these functions in the                                right sequence like any ETL would do so                                we wanted to move away from even writing                                this Java wrapper which would be a                                couple of lines of pages of Java program                                calling these functions in the right                                sequence so what we did was we mapped                                every one of these functions atl                                functions we built on cascading into an                                xml so there's a corresponding tag in                                the xml for every map every function                                every ETL function we built on top of                                cascading and only now have had to do is                                write an ETA XML which would write an                                XML in the right sequence if you're                                doing a read file sort reformat                                aggregate and right you just have these                                five tags with the right properties                                provider to these tags in the right                                sequence and                                submit it for execution now what we also                                did was wrote execution service that                                sits between cascading and the XML it                                reached the XML and at runtime auto                                generates the Java wrapper around                                cascading so the the task of writing any                                Java code is now taken away from a                                developer all you do is keep your                                business logic in an ETL format inside                                the XML tags and auto generate the Java                                a wrapper around cascading which calls                                 the inbuilt functions in cascading in                                 the right sequence cascading generates a                                 MapReduce execute on Hadoop the last                                 step in the journey was we built a UI on                                 top of the XML and we created this UI                                 icons inside the inside the UI and every                                 time you drag and drop an icon it auto                                 generates the xml tag for that                                 particular function so if you drag and                                 drop a read function it creates the xml                                 tag for the read you drag and drop the                                 sort function iraq it creates the xml                                 tag for sort reformat and so on and so                                 forth so as you string together an etl                                 in the UI it auto generates the xml when                                 you save the etl in the UI and when you                                 run the ETL the execution service takes                                 the xml creates the the java wrapper                                 around cascading which calls the java                                 functions the cascading functions and                                 execute son hadoop so this is how we                                 achieved an actual ETL like environment                                 on top of hydro using all open source                                 components this will be very this will                                 be clarified in the next slide where we                                 have a quick demo as you see over here                                 the look and feel and this is what                                 hydrograph actually looks like in action                                 it looks like any ETL tool on the left                                 hand side you have a component palette                                 so all the components we just talked to                                 you about are actually exposed in this                                 palette you drag and drawing these                                 components on top of the execution                                 pallet of the actual etl palette and it                                 creates the details it also gives you                                 these windows                                 where you can put in the properties for                                 those components so if you're going to                                 do in this case if you're doing a                                 aggregate you want to provide the actual                                 field names the metadata father for the                                 aggregate function all that is actually                                 provided in a in these figure in these                                 windows and the field names that you put                                 in are saved in the xml tag in the back                                 end so as you are as you see this etl is                                 being generated created on the on the UI                                 the xml is being saved in the back end                                 so the xml we talked about where the                                 business logic resides for the whole atl                                 function instead of writing that xml                                 manually now you're just doing it in a                                 graphical format the another good part                                 about this is instead of writing the                                 code in a procedural language or events                                 saving it in an XML line you know saving                                 this code in a graphical format so it's                                 easier for someone to read and to                                 understand the code so the etl is                                 generated this way it looks and feels                                 like any etl the good part is it                                 actually works all the way down to                                 Hadoop so in the interest of time we are                                 just showing you i TL which is                                 completely created and once you click                                 the Run button the etl listener being                                 executed asking you for the actual                                 connectors to the Hadoop environment and                                 to the Hadoop cluster and that you could                                 execute you have the concert which tells                                 you the results of the execution William                                 one add anything on top of that yeah I                                 just want to add one main design                                 decision is to decouple most of these                                 what you see so you might have seen like                                 oh we have this you I do I have to use                                 this in production the answer is no it's                                 very decoupled what cha mention is the                                 UI just generates xml code and XML is a                                 dependency for my code that is compiled                                 execute so go ahead all right so what                                 that what that means for us is you don't                                 have a dependency on cascading over here                                 you can actually swap out cascading with                                 spark and what's what we are that's what                                 we are doing right now so in the next                                 couple of months below the next version                                 of hydrograph we're on the functions we                                 cascading will be now built in spark and                                 the same xml code you generated using                                 the developer you I will now be executed                                 on spark this gives you some kind of                                 obsolescence proofs environment because                                 if beyond spark in the future there is                                 some other environment that is used for                                 the compute your code that the business                                 logic that resides in the developer in                                 the UI in the XML never changes so the                                 etl you write stays the same you don't                                 have to recode your business logic we                                 can easily swap out the compute layer                                 below that by building the same                                 components and pointing the execution                                 service to the next compute fabric and                                 it will just execute on top of that so                                 we are cascading generates MapReduce it                                 generates flank and generate stage we                                 are swapping all that out for spark and                                 it will be compatible to any other                                 nation feature if you're a mug that                                 comes up so in a way it becomes                                 obsolescence proof so I guess that's                                 what hydrograph is all about it will be                                 available as a patchy open source                                 project in the next few months and I                                 guess that's about it yeah all right                                 thank you thank you
YouTube URL: https://www.youtube.com/watch?v=Rusb-wEQ134


