Title: Understanding Why Change is Actually Good for Your Business - Jeff Veis (HPE)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              welcome this will be the best session                               talked this afternoon we're going to                               talk about the big data transformation                               and actually focus on inside and lessons                               from some of our real-world customer                               deployments we working with O'Reilly                               created this little booklet you can                               actually pick this up at our booth and                               that's what we go into the technical                                detail of these five different                                deployments all these deployments had                                Hadoop working with our flagship                                database analytical database which is                                vertica and you're going to learn about                                the lessons that they took away as they                                went through from anywhere from two                                years to three years to transform their                                deployment so let's take a look just to                                kind of set this up this may seem over                                simplistic but it's really really                                critical and we see companies today                                spending cycles with us which is trying                                to decide what is the right tool for the                                right job and most companies are looking                                at these three different tools or                                relational database which is typically                                what they have now most are going after                                building out a day to Lake and then the                                idea of infusing an analytical database                                and we're going to talk about how those                                play together what we're seeing is a                                winning combination increasingly data                                lakes make sense running on Hadoop                                expanding in the cloud or on-premise but                                to be able to deliver that insight from                                them what we're seeing that                                forward-thinking customers do is pairing                                it up with an analytical database that                                works in conjunction with that data lake                                now all of these customers I'm about to                                talk about consistently maybe they use                                different words but they talked about                                the fact that they really had to                                identify their stakeholders what                                analysts business owners IT                                professionals and data center scientists                                wanted and it was different it whether                                they were looking for acid compliance                                because they wanted to ensure that it                                had consistent data or that they wanted                                to be able to use their r and mo models                                and be able to bring a machine learning                                at scale these were different wishes                                that they had to under                                stand and be able to project out in the                                future if you don't do this well you run                                the risk of not having the right tool in                                the right deployment so we're going to                                talk about three different deployments                                number one was with Cerner which is the                                world's leading h is medical systems                                provider they gather huge amounts of                                data specifically are two met dart ems                                is in their millennium health care                                platform and they have over                                             that is driving                                                        come into sirna                                                     every month and this is looking at how                                the system's reforming its looking at                                the telemetry that's coming in and what                                they saw was that they had to think                                outside of the conventional box                                specifically many systems traditionally                                for first query data caching will do                                that with the hope that future queries                                will perform better but because they                                wanted to not sample and look at every                                query in every record they saw that this                                first query data caching was actually                                degrading their performance and so                                that's push their shift to a column or                                database that did not have the need to                                do data caching to get good throughput                                they saw a huge increase in performance                                um being able to look at these kind of                                telemetry coming in issuing two million                                predictive alerts including looking at                                sepsis infection which can often happen                                as a secondary element within hospitals                                that is key to be able to recognize and                                act in the first six hours very                                treatable if you do that but very easy                                to miss if you're in for other ailments                                with this combination of a Hadoop based                                system with verdict in front of it they                                were able to go from down sampling to be                                able to look at every rtms input the                                second one is a french company named                                credo huge big data here we're talking                                 about                                                                    day this is serving up digital ads and                                 conversions and serving three billion                                 banners and tracking a billion different                                 users on a daily basis an enormous                                 amount of data once again they're using                                 Hadoop for the raw data storage tableau                                 for visualization but what they saw here                                 was in addition to Vertica they had to                                 create or what serve them well was a                                 center of excellence and that center of                                 excellence was high-powered analysts                                 that were focused on the big picture and                                 they they're there for two reasons the                                 regional reason they were hired was to                                 resolve critical cases so when their                                 analysts and data users had problems                                 they would kind of come in like Superman                                 to assist them and make sure that they                                 were using and applying their their                                 analytics in a reasonable fashion but                                 what they saw was the same team could                                 practically train these users so they                                 wouldn't make those initial mistakes                                 before this wasn't the original design                                 of the co e and what they saw was if                                 they didn't have that these data                                 analysts would fall into the same old                                 pattern such as creating data silos that                                 were difficult to join together and                                 using directly Hadoop which was                                 unrealistic for the big data use cases                                 that they were using now they have a                                 huge number of users                                                   to                                                                       use against this large data lake with                                 front ended with the vertica analytical                                 calmer database the last one is at                                 saying actually based here in New York                                 it's the world's largest e-commerce site                                 that sells not like Amazon where it's                                 finished goods but these are all                                 handcrafted authentic crafted good so                                 it's the world largest commerce site for                                 that particular item from all around the                                 world and there were the lessons or the                                 takeaway here or really about not what                                 to do but what not to do and once again                                 they had to evolve from a legacy system                                 here it wasn't starting with Hadoop as a                                 legacy system but a post grass system                                 that was absolutely not keeping up with                                                                                                        to be able to analyze in                                 use and bring out in sight um and and so                                 they shifted to a hadoop based data                                 lakefront lend loaded by vertical once                                 again because verdict has the ability to                                 either run directly on those Hadoop                                 clusters or be be able to analyze in                                 place without doing joins against your                                 Hadoop data without any data movement or                                 of course you can ingest in that data                                 the three takeaways here in this                                 deployment was number one if you refresh                                 your data and update it and they used to                                 do this they would throw away the                                 antiquated data but then they realize                                 for example if they updated a website if                                 one of their merchants change their                                 price or the image they would throw away                                 that old data viewing it is obsolete but                                 then an epiphany happened they saw that                                 if they kept that data and now they                                 could and analyze the deltas and changes                                 they could correlate that with                                 performance they could see what price                                 adjustments made over time affected                                 their merchants positively or negatively                                 so they moved away from throwing away                                 quote unquote old data and and actually                                 recording the changes and by analyzing                                 that they were embracing not just the                                 performance of the site but how the                                 change of data that went into the site                                 was affecting the overall performance                                 number one it was really hard for them                                 to shift away from postgres and you'll                                 see that many organizations have legacy                                 systems but what they had to basically                                 embrace was not getting burdened with a                                 technical depth that built up that kind                                 of sunk cost that we have training and                                 money put in these older systems and                                 that held them back when in hindsight                                 they wish they had moved more                                 aggressively um it's important to be                                 able to work with those legacy systems                                 but you have to get that right tool and                                 understand how you move to that and then                                 the last point was with this mast amount                                 of data they actually created their own                                 app called schlep which kind of means                                 move things around it's kind of a                                 Yiddish term and they had to focus on                                 how they could cleanse the data and move                                 it into the system                                 in a very predictable manner they had                                 not thought about that they were doing                                 batch loads before but as they shifted                                 to a real time system they had to be                                 able to move faster for that um the                                 final thing here was that they saw that                                 if they really wanted velocity around                                 their Hadoop dedo lake that they had to                                 accelerate and front end with Vertica                                 being a column or database that does not                                 have to index at all not index on read                                 not index at all and realizing that when                                 you do a sequel query on Hadoop that                                 almost always has to be translated to a                                 MapReduce call and that's why you just                                 don't see that performance so the                                 joining together of the two was once                                 again a winning combination the takeaway                                 and what we'll talk about in what you                                 can find out more about is how vertica                                 which is a commercial offering is                                 designed to run on any cloud and with                                 practically any open source technology                                 with a huge focus on three number one                                 Hadoop we interface and we have                                 optimized readers for orch and parquet                                 in fact we just announced the park a                                 reader that has the highest level                                 concurrency and against impala                                 benchmarked                                                                                                                                query call support unlike most of the                                 sequel on Hadoop offerings that support                                 about two-thirds of that that means any                                 sequel queries that have been created                                 whether it was running a teradata Oracle                                 doesn't matter where it was running you                                 can now bring that run it directly on                                 your Hadoop data stores in addition to                                 that we've optimized connectors that                                 work with both spark and Kafka to be                                 able to bring in in database and                                 external to database advanced analytics                                 as well as an advanced message in queue                                 you can find out a lot more about this                                 and more at our booth and if you take a                                 look at this URL we also invite you to                                 draw and will download the community                                 edition of urtica totally free no                                 catches at all you can operate up to a                                 hundred terabyte of                                 data and be able to test and exercise                                 the vertical column or database to see                                 what it can do for you and it's a great                                 way to get associated with verdict in                                 what it brings to the table and a great                                 way to harness and unleash the data that                                 you have in your Hadoop data stores                                 thank you very much
YouTube URL: https://www.youtube.com/watch?v=FT1Yf60d0pA


