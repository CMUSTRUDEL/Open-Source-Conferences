Title: Analyzing 25 Billion Stock Market Events in Under an Hour - Misha Brukman (Google)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              hello and welcome everyone today I would                               like to talk about processing                                          stock market events in less than one                               hour on Google cloud platform my name is                               mr. Brookman and I'm a product manager                               at Google so first a little bit about me                               but I started at Google as a software                               engineer in                                                          using big table and MapReduce in                                     and the use case is mentioned in the big                                table paper in                                                     transferred to a big table and I'm the                                project manager that is so it's just                                jumping around a little bit                                so a little bit about Google cloud                                platform so over the years google has                                published a number of papers in terms of                                the innovations with big data and file                                systems and databases that we have built                                that power all of our products and we                                have built all of our products that                                scale to millions and billions of users                                on that infrastructure and with Google                                cloud platform we're allowing you to                                build your applications and your systems                                on the same systems that power Google                                applications you can see a number of the                                applications that we've published our                                research papers on from gfs to MapReduce                                to big table that have given rise to                                systems like Hadoop and MapReduce in                                HBase over the years and you can find                                more information about these systems on                                research google com since then we've                                been production izing these systems and                                we've been production izing them and                                making them into products that are                                accessible publicly and you can learn                                more about them on cloud google com and                                so systems like cloud storage and data                                proc and big table now available for you                                use our partner FIS is an international                                financial services technology company                                they're in the fortune                                                 of the S&P                                                       number-one fintech company for several                                years running their bidding on the                                consolidated audit trail project to meet                                the requirements that the exchanges have                                with the Securities and Exchange                                Commission the goal is to provide the                                ability to query and visualize all the                                transactions from the US equities and                                options trade data for a given day by                                noon the following day and so the idea                                is that all these exchanges and                                broker-dealers will provide all the                                information by                                                         day after the trading day and by                                        so that's within four hours the SEC                                would like to be able to query and                                visualize all the data and we're                                budgeting for a                                                        market events which is much larger than                                any largest trade day trading day on                                record so some of the requirements here                                are we need to do an order event linkage                                and I'll just give a quick and simple                                example here when you submit a trade to                                a broker what actually happens is that                                this may go through a bunch of different                                stages where the order may be split up                                or joined with other events before it                                actually makes it to the exchange and                                it's executed what ends up happening is                                that you actually have a tree of events                                that constitute that one single trade                                and so the idea is to find all these                                events                                and coalesce them into trees such that                                you know exactly what happened to any                                particular trade with the SEC would like                                to see is if there's any anomaly if                                there's any insider trading if there's                                anything strange that's happening in the                                markets then we want to do it much                                faster than they can today so now let's                                look at what are the requirements are so                                the the folks at FIS looked at what                                infrastructure they can use to build                                this plat for the the solution that they                                wanted to build and they said that they                                select the Google cloud platform because                                it is the only platform that they found                                in their research they could meet the                                demands at the scale of data                                so for                                                            needed the storage system to ingest all                                the stock market events they need to                                 bulk load the data from all the data                                 providers into one place they needed to                                 wait to load and then refer to all the                                 events individually so you have all                                 these market events that they need to                                 link into the trees and they need to be                                 able to link and order them individually                                 they also needed to be able to process                                 all this data in bulk at very high scale                                 so i needed a parallel processing system                                 that could do the analysis in bulk and                                 finally they need a way to query and                                 analyze the data to visualize the data                                 such that data scientists and not only                                 engineers could access it                                 is an update                                 so as I mentioned over the years we've                                 built a number of big data solutions and                                 so the questions that we've asked                                 ourselves are so for example if we                                 wanted to store a large amount of data                                 you know how would we build a file                                 system that can store terabytes and                                 petabytes and exabytes of data we've                                 been doing this with search for a while                                 and so we end up doing is building gfs                                 which was replaced by Colossus later on                                 and then we've externalized this project                                 as Google Cloud Storage as part of our                                 cloud platform and the next thing that                                 we wanted to do is well once we have                                 this we also need to be able to have                                 random access such that we can update                                 individual data items such as web page                                 as much faster so we can update our                                 search index faster so we ended up                                 building big table and we externalized                                 this as google cloud big table another                                 thing that we wanted to do is we wanted                                 to do data processing at scale we wanted                                 to process petabytes of static and                                 streaming data very very quickly we                                 build several systems to address this                                 problem MapReduce flume and mill wheel                                 and we've combined all of these and                                 we've externalized as a Google Cloud                                 dataflow and we open source the SDK for                                 this as Apache beam finally we also have                                 the same requirement that we wanted to                                 query trillions of rows of data in an ad                                 hoc fashion and run these queries over                                 large amounts of data with very little                                 effort so we build the system called                                 Dremel this is the research paper that                                 we published and we will sterilize this                                 system as Google bigquery so imagine                                 what one can build when one has access                                 to all of these systems so it ends up                                 being that we've built all of our                                 products using the same infrastructure                                 that we're now externalizing as Google                                 cloud platform the same systems that we                                 use to build search Gmail maps at                                 oolitic set cetera are now available for                                 you use as part of Google cloud platform                                 so what you have is you have scalable                                 data storage in terms of bulk as well as                                 random access and no sequel databases                                 management stream processing as well as                                 data warehouse and analytics so if we                                 look at the again the FAS implementation                                 requirements when they needed to use a                                 bulk storage system they chose Google                                 Cloud Storage when I needed to be able                                 to refer to all the market events                                 individually they chose to use big table                                 when I needed a way to do the stitching                                 of all the events together in terms of                                 bulk processing they chose Google Cloud                                 dataflow and when they wanted to                                 visualize and query the data they chose                                 to use bigquery so let's look at the                                 data flow that they followed and how                                 they put the oldc system together                                 part of their architecture so first load                                 they did a mock they put into cloud                                 storage in the second stage they use                                 data flow to move the data from cloud                                 storage to bigtable to give them that                                 random access to the individual events                                 in the third stage they both read from                                 big table and wrote back the big table                                 as they created millions of these graphs                                 novels grass could be going from simple                                 which is just a handful of events to                                 tens to hundreds of possibly thousands                                 of different events so you have millions                                 of these graphs of varying size ability                                 into process and finally in the last                                 stage they export the data from big                                 table using data flow to bigquery and                                 pick where is the data warehouse an                                 analytic solution to be able to query it                                 in addition before he integrates with a                                 number of BI tools such that they could                                 use tableau Google Cloud Data lab or                                 custom written with visualizations that                                 we all use the sequel API with the fully                                 managed service to be able to visualize                                 it any way they need in terms of timing                                 they built the system in six weeks by                                 six developers and that is because they                                 didn't have to manage any of the                                 infrastructure over the infrastructure                                 management was handled by Google and                                 Google infrastructure scales to such an                                 extent that they didn't have to worry                                 about it so the goal as I mentioned                                 before they wanted to target hundred                                 billion events to be processed in four                                 hours what they did was they went one                                 quarter of the size of the benchmark so                                 they wanted to process                                               less than an hour and here's some                                 benchmark results so they did process                                    billion market events in                                                 use                                                               standard                                                                so                                                                    event reads per second and                                            rice per second sustained and                                            event weeds per second and                                            writes per second at peak in terms of                                 bandwidth it was                                                       sustained and thirty four gigabytes and                                                                                                        on a fully managed scalable database                                 that they didn't have to provision                                 manage upgrade or deploy right big table                                 is a fully managed no sequel database                                 that skills very much horizontally so                                 there aren't any bottlenecks so some of                                 the things that we've learned in this                                 process you really need to tune your job                                 to maximize the work or throughput in                                 some cases a small number of very                                 powerful workers where's best and some                                 other use cases a larger number of                                 smaller workers so with fewer CPUs would                                 work better and it really depends                                 the use case in some cases you really                                 don't want to underestimate the power or                                 the necessity of sharding so dataflow                                 does a great job of charting and it also                                 has this feature which are called li-po                                 charting where what happens is with                                 stragglers you may increase the amount                                 of time that you need to take to finish                                 the job much more significantly so it                                 could be                                                              two or three stragglers are taking a                                 long time what data flow does is the                                 dynamically balanced this all the work                                 so it finds the stragglers splits up                                 their work and reassign the such that                                 you can complete your job faster and                                 finally at the scale you really really                                 want to optimize your code every                                 millisecond will be magnified                                 significant it will cost you hours you                                 can learn more details about this use                                 case and other financial use cases on                                 our financial solutions services website                                 this is also applicable in the large                                 number of other industries from IOT to                                 oil and gas retail pharmaceuticals and                                 others particularly any industry that                                 deals with time series related data for                                 more info you can visit us at claddagh                                 google com there's all the products are                                 listed there and we're also boost number                                 for                                                                   questions you may have afterwards thank                                 you very much
YouTube URL: https://www.youtube.com/watch?v=BBfmbcNH-4s


