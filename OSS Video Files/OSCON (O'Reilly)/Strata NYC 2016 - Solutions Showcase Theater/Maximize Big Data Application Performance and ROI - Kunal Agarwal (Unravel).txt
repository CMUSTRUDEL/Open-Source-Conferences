Title: Maximize Big Data Application Performance and ROI - Kunal Agarwal (Unravel)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              hi I'm canal co-founder and CEO of                               unravel data unravel data is the first                               and only full stack performance                               intelligence for data ops I'll explain                               to you what all of these words mean in a                               few minutes a quick overview of our team                               we've been able to assemble an expert                               team of distributed computing people as                               well as enterprise software veterans                                people who have handled big data sacks                                such as Duke spark no sequel etc in an                                industry capacity as well as in a                                research capacity our investors are                                mellow ventures and data elite the                                problem that we started solve is                                everybody wants to do big data and                                everybody is trying to do big data in                                fact ninety-four percent of the                                organization's say that they want to and                                i'm sure you guys do as well but the                                biggest problem is that more than half                                of these projects are not giving you the                                return in value or they're just feeling                                out right and the number one reason for                                that is not that there's no                                determination around it it's just that                                there's not enough expertise to make                                sure that these Big Data projects and                                systems really are up and running and                                that things flow smoothly if you see on                                your left the kind of stack is pretty                                typical that people are stitching                                together today they're putting Kafka for                                data ingestion you may have HDFS as your                                data store maybe running some hoodoo                                spark applications etc but the common                                questions that come by the people who                                work on these tax every day whether they                                are the end users like the data                                scientists or you have the operations                                folks who are making sure that all of                                these things are running well they have                                a few questions every day we just keep                                coming up over and over again the end                                users may ask hey why is my appt slow                                why is it slow today it was working fine                                till yesterday but today is slowed down                                by three acts why is my apps even                                failing and then they will reach out to                                the operations folks in your companies                                and the operations guys were sitting                                down at the center of it all they have                                to make sure that all these applications                                are running well your spark jobs your                                hive queries all of your                                mission-critical SLA jobs and they also                                have to keep an eye on the cluster they                                have                                to know who's using what resources and                                how much is a contention are these                                resources even being used to the maximum                                utilization that they could get and more                                importantly can i track all of these                                usage and access information so that i                                can create a nice chargeback report for                                my business organization as well now                                when you look at any of these problems                                the problem of say is slow application                                the reasons are multiple and the reasons                                more importantly are all over their                                stack so let me walk you through it an                                application could be slow because you                                had bad code you just had a very                                expensive join that's taking up a lot of                                time and resources for running this                                application your application could be                                slow because other other applications                                are running seeing the resources from                                this application so your scheduler                                settings were incorrect for example your                                file formats Hadoop for example does not                                like small files that could be a problem                                of why your app just failed or is very                                slow configuration settings bugs etc the                                product that I'm trying to display over                                here is that you need a full stack                                approach you need to understand what's                                happening at the application level at                                the infrastructure level at the OS level                                to really get to the root cause of all                                of these problems now companies like                                yourself already use a lot of these                                tools to solve challenges like this but                                they're not adequate enough to be able                                to answer these quickly and answer them                                accurately so you may have system                                monitoring tools ganglia etc which give                                you low level visibility which is great                                it'll give you a cpu utilization graph                                 will give you a network io graph but                                 won't tell you a whole lot about                                 applications or for looking at                                 applications you're really digging down                                 into the logs so you're going to your                                 job tracker you burn a wet trash tracker                                 and then as you know each application                                 may have hundred different tasks so                                 you're looking at hundreds of these                                 syrians trying to stitch the story up                                 and figure out hey what happened today                                 with this application right we wanted to                                 change that we wanted to make the                                 ongoing management of these data                                 operations Apple easy what we mean by                                 that is number one let's get data about                                 three things together in one place your                                 application data all your resource                                 utilization data                                 as well as the data stores itself put                                 them together and then start giving me                                 good analytics on top of it to help me                                 resolve some of these problems so what                                 unravel can do for you today is speed up                                 applications from                                                      the type of applications that you're                                 running and guarantee your SaaS make                                 them super reliable we can improve your                                 resource utilization we can tell you if                                 people are abusing resources or you have                                 underutilized containers so that other                                 jobs may be able to run on the same                                 cluster as well we're also able to                                 stitch all of these different pieces up                                 and instead of you having to dig through                                 logs and metrics and ravel answers the                                 question so if you had a slow app                                 unravelled will tell you this is the                                 reason why you have a slow app and this                                 is how you can go and fix it instead of                                 you having to dig through hundreds of                                 different graphs logs and metrics how                                 does it work so on the left you have                                 your big data stack which could be you                                 know running a lot of different                                 applications from analytics to ETL                                 machine learning recommendation engines                                 whatnot but the kind of systems that you                                 using for these are Hadoop spark no                                 sequel maybe some mpb systems maybe                                 Kafka for data ingestion etc then you                                 could be running this on premise or on                                 the cloud or even a hybrid environment                                 now that stack provides unravel all the                                 information that it needs so we gather                                 logs metrics event level information etc                                 in to unravel and the first thing that                                 we do is we analyze this entire workload                                 we stitch up the application information                                 the resource utilization information etc                                 and then run our AI engines on top of it                                 which are then able to do all of these                                 automatic recommendations to usage                                 analytics and very smart alerts which is                                 not just threshold-based but can tell                                 you things like hey your job is about to                                 missus at still a go and now fix a few                                 things so you drop can finish on time                                 etc a lot of companies have already                                 started using unravel in production this                                 is a quick case study from strada last                                 actually at San Jose where autodesk                                 stood up and spoke about how they are                                 using unravel there are an EMR amazon                                 customer and they're using unravel in                                 different ways there are unraveling the                                 monitor resource usage it is helping                                 them identify                                 five bottlenecks with all of their                                 different types of jobs whether that's                                 Hadoop spark Uzi etc it's also helping                                 them forecast a compute requirement so                                 we understand what kind of compute your                                 different applications and workloads and                                 users are needing and we can actually                                 forecast in an elastic environment like                                 Amazon what do you guys should be                                 getting next and unravel is now GE aid                                 there's free trial available at unravel                                 datacom if you go to our website you'll                                 see a short form to fill out tell us                                 about your cluster tell us how many                                 notes you're running whether you're                                 running Hadoop spark no sequel etc so                                 that we may be able to give you the                                 right files so try and Ravel for                                 yourself and see your data ops working                                 much much smoother thank you very much
YouTube URL: https://www.youtube.com/watch?v=GnNVV-kVsoE


