Title: Technical Strategies for Creating a Consumer Profile - Scott Nichols (Novetta)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              thank you guys for coming today I'm                               Scott Nichols I'm the product owner of                               Nevada Alex and today I'm going to talk                               to you about a consumer profile and                               getting to some of the technical parts                               behind building one of those so I like                               to start off with what is a consumer                               profile we recently have started                               building summer profiles for different                                financial services companies and other                                companies out there and really what it's                                about is is building a good view of a                                real world entities so finding all the                                information about a subject whether                                that's a person or an organization or                                some other kind of entity and pulling                                all the data from different data sources                                together whether they're very dirty and                                disparate or their well formed as well                                so it includes a lot of profile data and                                CRM type data but then it also includes                                a lot of the transactional data and so                                not just being able to do that on the                                master data but also being able to do                                that on the full transactions that may                                be a recurring you know in financial                                services that might be wire transfers or                                other kinds of actions or transactions                                more than we buy retail transactions as                                well and then we also discover a lot of                                potential relationships in the data so                                people that are maybe wiring money back                                and forth with one another or other                                kinds of information so building that                                that whole profile that whole consumer                                profile is one of the things that Nevada                                has done with nobody oolitic sand the                                the main use case and I'm going to talk                                to you today about some of the things                                that we also focus on when building this                                consumer profile is trying to make sure                                that it's very flexible when new data                                sources come in right there's many                                systems out there that maybe do master                                master data management but they're not                                as flexible to be able to handle it when                                new data sources come in and we we were                                when we built this consumer profile we                                were told that that is a requirement                                that it needs to be very very flexible                                for new data sets that come in it also                                needs to be able to rebuild his scale                                right in this particular case you'll see                                on the next slide when we talked about                                the case study that we were dealing with                                some very very large data over                                          records in this case and they needed to                                build over night every night and so                                being able to build at that kind of a                                scale and pull this much data together                                on the fly was very critical for for                                this application                                and then the set the third bullet                                they're really being able to manage any                                entity type so in this case we're                                talking mainly about persons and                                organizations but being able to expand                                that out to other things whether it's in                                the supply chain you're talking about                                parts and other objects or that's in                                that the health care and you're talking                                about insurance transactions or other                                kinds of data as well the other critical                                piece especially when in this particular                                use case is being able to handle all the                                data governance requirements and                                security concerns I'm as you can imagine                                this use case of the financial services                                and so they have a lot of requirements                                on managing the data in the cluster and                                ensuring the integrity of the data so                                the use case that I'm going to talk                                about today was in the financial                                services sector and we basically were                                brought in they had they had a system                                where they were doing search with solar                                and they were resolving the search                                results right so they would do a search                                with solar across all their different                                data sources and they would resolve all                                those data all the results and then                                present that back to the analyst right                                that's how their system worked it had                                six billion records in it and they had                                lots of different data sources now the                                problems that they're running into was                                they couldn't meet their performance SLA                                s right they needed to have a certain                                response time in those searches and they                                 weren't able to meet it by doing the                                 resolution on the fly on a consistent                                 basis and they weren't also able to                                 harness the transactional data they were                                 able to do these searches on the profile                                 on the CRM data but when they tried to                                 pull in the transactional data it                                 overwhelmed the the system and so they                                 weren't able to really take advantage                                 the relationships and that were                                 available the transactional data and                                 then they also were very limited to                                 individuals and organizations right they                                 couldn't they couldn't look at other                                 kinds of data they wanted to get into                                 looking at cell phone numbers and other                                 kinds of information that they couldn't                                 do so they brought no vetta in Nevada in                                 the analytics and and they changed their                                 paradigm a little bit to resolve in                                 batch overnight right and so every night                                 they basically would stand up and have                                 it resolve at scale the entire holdings                                 that they had and we're very flexible as                                 far as new data sources and fields go so                                 when new data sources come on they can                                 just simply add it into the mix and let                                 it rerun overnight                                 they also change their rules pretty                                 consistently right so the use case the                                 rules were based very much on their use                                 case today and their business case might                                 change tomorrow or next week and so they                                 needed a system that was very flexible                                 to adapt to that and so the outcome as                                 you can see here was really about                                 resolving their full holdings they had a                                 six to eight hour window they wanted to                                 resolve in so that data was ready the                                 morning when the animals would come in                                 and being able to adapt to new use cases                                 and as well as making sure that                                 governance was was applied right making                                 sure access control so if some of the                                 data sets this is a compliance use case                                 part of it was and so some of the data                                 sets only certain users could see those                                 data sets so ensuring access controls                                 were fully you know monitored and                                 enabled as well I'm going to jump a                                 little bit today into the technical bits                                 of how we kind of accomplished this                                 right and how our software enables this                                 kind of very very fast processing what                                 are what are the way our software                                 typically works right is there's four                                 different steps for our software when                                 the data is already on Hadoop and in                                 hive our software takes that it blocks                                 it is one thing we call it or partitions                                 it and basically distribute it out                                 across the cluster if you're thinking of                                 this in a MapReduce paradigm right                                 that's kind of the map part of the job                                 right is going and blow that data out                                 across the cluster in a very smart way                                 to minimize the amount of matching and                                 scoring that needs to occur after that                                 map is done we basically move into                                 scoring which is basically being able to                                 identify the records that are close to                                 each other right using different things                                 like edit distance Levenstein at a                                 distance or other kinds of comparisons                                 statistical analysis and comparisons on                                 that data we do that across the whole                                 cluster and we can do that very very                                 efficiently and we try very very hard                                 not to over you know over score records                                 and have no chance of coming together                                 right so if you had Scott Nichols and                                 John Smith there's no chance those two                                 records should ever be they won't match                                 so there's no reason you should compare                                 those records with one another and then                                 the last step there is the entity                                 formation so when you're done and you've                                 got all these pairs out being able to go                                 back and pull all that data together to                                 identify the real bien                                 he's and tag them as saying this is in                                                                                                        that point the animals can take that                                 data and deal with all of Scott Nichols                                 data across all their data holdings as                                 one shot our tools built with very                                 flexible rule building and so it's a                                 relatively simple strategy block that                                 you see here and it's just a very simple                                 Python syntax and you basically write a                                 rule in there that says this is how I                                 want you to do the partitioning you                                 notice how I want you to spread the data                                 out across the cluster and the score is                                 where it says this is what has to match                                 in order for these records to be                                 considered a pair and so for example                                 here you can see we're basically in this                                 case we're partitioning on employer                                 right so if the record two records have                                 the same employer they'll go in the same                                 block or the same group and in the score                                 or saying now look at the Levenstein                                 edit distance of those other names or                                 the person field in this case and if                                 it's over                                                                a match right so we have a little over                                 about                                                             functions about                                                       functions and about                                                      functions so there's a lot of different                                 kinds of things as you can do with a                                 matching we use this as well to identify                                 relationships so in this particular use                                 case they didn't want to just resolve                                 the data to find like Scott Nichols                                 across all their data they also wanted                                 to find who else is living at the same                                 address the Scott Nichols or who else                                 could possibly be in the household with                                 Scott Nichols right who's sharing it                                 address and maybe the last name as Scott                                 Nichols so you can use the same language                                 to define a rule that does that as well                                 and just amidst the pairs right so it                                 doesn't actually do the resolution                                 stabbed but it emits the pair's out and                                 you get those relationships and so using                                 these which are very easy to edit                                 there's a there's a console and an API                                 and a command line driven ability for                                 this to the console in this particular                                 case we have                                                             using the console writing their own                                 rules to do this and it's very flexible                                 for being able to build these consumer                                 profiles and                                 and so on the so that in a nutshell is                                 my presentation if you'd like to hear                                 more I know we only get a few minutes                                 here but if you like to hear more we're                                 right here right behind you at booth                                     so come over for a live demo we'd love                                 to hear from you guys and thank you
YouTube URL: https://www.youtube.com/watch?v=C45hxgmmwF0


