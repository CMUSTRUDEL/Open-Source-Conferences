Title: Simplifying Streaming Analytics with GPU-Acceleration - Mike Perez (Kinetica)
Publication date: 2016-10-20
Playlist: Strata NYC 2016 - Solutions Showcase Theater
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	                              hi everyone good afternoon Mike                               preservation services on technical                               support for Connecticut and here I'm                               here to talk about today about how we                               can simplify streaming analytics and                               ingestion and processing using the                               Connecticut database a quick recap of                               what Connecticut is it's a n memory GPU                               power database that's been around since                                                                                                   analytics suggestion and now we're                                helping customers do brute force                                analytics and really simplifying they're                                streaming architectures so I'll walk                                through some of the challenges today                                also how we enable organizations to                                solve multiple use cases with a single                                product if you start looking at where                                customers have been struggling you know                                primarily it's you know complexities                                around streaming capabilities so looking                                at having to deploy and maintain                                multiple components to achieve a single                                platform multiple storage mechanisms in                                terms of having to serve out                                capabilities of ingesting real-time                                pieces batch pieces and then also                                serving it up and then you start looking                                at what are the complexities that you                                would have around compute and memory to                                support these multiple types of                                transformations also looking at                                different ways of how to keep up with                                the different volumes that you would                                have across your infrastructure also                                from an operational perspective you have                                many components to install configure and                                tune and then from a device perspective                                you start looking at how many components                                you have to install manage and keep                                alive then you start looking at                                maintenance and operations for upgrades                                it's many different components that you                                have to go through so if you look at how                                kinetica is able to deliver streaming                                capabilities we look at three core                                paradigms in terms of what we support                                massive parallel processing so giving                                you multiple ways than just the data and                                then recomputing and then most                                importantly HPC style computing which is                                enabled by GPU so Connecticut itself was                                not ported from an existing                                infrastructure it was built from the                                ground up on top of existing GPU                                technology so if you look at it GPUs                                enable people to compute massive amounts                                of data because of the architecture                                itself right so traditional CPUs have                                you know                                                        a GPU at Celta we're going to put into a                                box has                                                               our booth of to start by the hall here                                we have over                                                            server so if you look at the compute                                capabilities that I could enable with                                that that's massive amounts to compute                                that nothing else can touch in terms of                                ingestion processing and storage so it's                                ideal for you know straight scanning of                                data set brute force analytics but then                                also giving people the ability to ingest                                massive amounts of data but then also                                it's a real time access to sequel Java                                Python etc in terms of how they want to                                access it so this enables specific                                capabilities around ingesting at large                                scale so I can scale out infrastructure                                horizontally each machine itself has                                multiple endpoints that could consume                                events coming into the platform that                                gives you a million different ways of                                scaling out and then also against that                                data immediately have to send the                                Connecticut database you can access it                                like I mentioned earlier so this is not                                like you have to do a net new                                infrastructure you could plug this into                                your existing architecture in terms of                                you have a Hadoop topology so we can                                plug into existing spark workloads pork                                streaming Kafka knife I also with the                                stream sets but then at the same time                                you can do queries like I said but then                                also plug in specifically additionally                                we give geo spatial visualization and                                then we also mimic the the the solar                                capabilities that natively produce                                cubana and then also fully support that                                 as well within the stack from a                                 streaming perspective there's many                                 challenges within the lambda Kappa                                 architectures so traditionally like what                                 folks are using now they'll stream                                 events them to Kafka they'll use                                 something like spark streaming or storm                                 and then will persist data into                                 something like HDFS or HBase right and                                 then they'll have to go through                                 different computations in terms of                                 serving data up to the fast layer so                                 that users can do that and interact with                                 between that you now have to do multiple                                 components to manage them and then you                                 also have to start looking at the                                 different ways of maintaining that and                                 making sure that your users have the SLA                                 so they're looking for so you're serving                                 of hot data through HBase and you're                                 also serving up data                                 sing through something like HDFS in                                 order to visualize that keeping those in                                 sync and then also trying to maintain s                                 la's is very challenging and problematic                                 so if you start looking at different                                 options in terms of what we could do                                 with kinetic oh we get parallel Jessel                                 of events so i can bring things in                                 through something like a Kafka Kinesis                                 mq etc and then now I could flatten the                                 architecture out and go straight into                                 the Connecticut I could run all thy                                 programs directly on top of that so                                 that's going to get you direct access                                 like we set up lift kit and scan so you                                 could have your real-time data coming                                 out of that here and then you could you                                 know archive the data off of there or                                 you can have other specific bits                                 available within HDFS for further                                 processing so if you look at this you                                 now have one single package to install                                 manage of monitor which is very                                 different compared to like what you                                 would typically would see within a                                 traditional Hadoop deployment so one                                 hour p.m. want to config file for host                                 from a DevOps I'd that makes me really                                 happy but then also from an application                                 deployment side and engineering I now                                 have one simple API that I could do to                                 ingest the data coming in and then also                                 serve out various capabilities in terms                                 of providing data access across that                                 data set if you look at specifically in                                 terms of how we work with that kinetica                                 itself with the GPUs has what's the                                 concept HTTP head node and each one of                                 those is enabled by the GPU cores that                                 we have so if you look at it we can now                                 have a cluster of Connecticut instances                                 running and I can now support multiple                                 endpoints on one single host to directly                                 ingest that data right so you now have                                 literally scalable that the                                 horizontal level for machines and then                                 you can start storing the data directly                                 in memory and then also supported and                                 backed by data and SSDs so you think                                 about it the datas would be rapidly                                 available one of the demos that we give                                 within our booth is the ability to ski                                 in and query and generate data sets                                 across eight billion records and we're                                 doing that in less than                                                  on eight notes so that's rapid you know                                 access lee we get also while the data                                 streaming in another thing that's really                                 awesome about the platform is that just                                 the different API is and the                                 visualization tools so we talked about                                 the Java side                                 + + Python and rest but she can also                                 interact with Connecticut via JDBC odbc                                 capabilities so we are working with                                 folks like teplo MicroStrategy and some                                 other bugs then also interacting with                                 different spatial capabilities so you                                 have the option to work with data set as                                 in a transactional database or you can                                 work with it within spatial context as                                 well so if you start thinking around                                 solutions around cyber security around                                 IOT network analytics etc you can now                                 bring in data points and events coming                                 in and than that you can now have the                                 ability to query the data sets based on                                 the event sizes etc or you can also                                 start looking at it from different                                 options around spatial queries so you                                 can start it's like I want to look at                                 and slice and dice the data based on                                 time series and then I also want to                                 start looking at it from a spatial                                 context as well so from a cyber IOT                                 Network analytics that's a really common                                 use case that we're seeing customers                                 utilize the kinetic own platform for so                                 if you start looking up where                                 Connecticut fits into an existing data                                 architecture we talked about Connecticut                                 being high throughput and memory GPU                                 power database that enables streaming                                 capabilities so you can start looking at                                 surrogate functions like bull lab no                                 sequel geospatial also providing full                                 text search and data that you work for                                 that you can now have that for real-time                                 analytics accelerating the performance                                 but then also making sure that we do                                 work within your existing Hadoop                                 infrastructure right so you're going to                                 still maintain Hadoop for the large                                 archival data processing deep learning                                 with your spark workloads you can also                                 leverage Connecticut within your spark                                 jobs so it's very common for us to have                                 customers running spark on yarn but then                                 also interacted with in that same                                 workload against the Connecticut                                 database for managing large table joins                                 and putting out data set back into your                                 spark workload also again Connecticut                                 itself can run on premise within a                                 physical infrastructure we also have                                 customers that are working directly with                                 public cloud providers there's also some                                 special cloud providers that have the GP                                 is available itself overall if you start                                 looking at it Connecticut enables you to                                 enabled streaming ingestion real-time                                 processing of your data with a very                                 simple fight                                 texture so you think about it one single                                 product to ingest store and process all                                 your data so it makes your developers                                 happy it makes your DevOps happy and                                 then most importantly it makes a lot of                                 your end users your business users have                                 because now they get real-time access to                                 streaming data and then also brute force                                 analytics against any type of other                                 computations that they have to do great                                 any questions                                 thank you
YouTube URL: https://www.youtube.com/watch?v=2nUeLYEUdM8


