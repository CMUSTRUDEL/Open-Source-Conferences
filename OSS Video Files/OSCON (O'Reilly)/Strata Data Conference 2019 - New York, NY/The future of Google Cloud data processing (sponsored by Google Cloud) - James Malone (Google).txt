Title: The future of Google Cloud data processing (sponsored by Google Cloud) - James Malone (Google)
Publication date: 2019-09-25
Playlist: Strata Data Conference 2019 - New York, NY
Description: 
	Open source has always been a core pillar of Google Cloud’s data and analytics strategy. As the community continues to set industry standards, the company continues to integrate those standards into its services so organizations around the world can unlock the value of data faster. 

Join James Malone as he unveils the future of Google Cloud’s data processing and introduces new capabilities that empower data professionals to build scalable and flexible applications faster.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:03,810
hello and good morning that's a pleasure

00:00:01,829 --> 00:00:05,670
to be here I'm thankful to be here with

00:00:03,810 --> 00:00:07,259
you and I'm thankful to work with teams

00:00:05,670 --> 00:00:09,179
who are passionate about combining the

00:00:07,259 --> 00:00:11,309
best of open-source with the best of

00:00:09,179 --> 00:00:13,259
cloud infrastructure I'm here today

00:00:11,309 --> 00:00:15,509
because I want to talk about what we see

00:00:13,259 --> 00:00:17,460
at Google cloud and on our open source

00:00:15,509 --> 00:00:21,060
data team as the future of open source

00:00:17,460 --> 00:00:22,650
data processing for the past couple of

00:00:21,060 --> 00:00:25,050
years we've been working on cloud data

00:00:22,650 --> 00:00:27,269
proc data proc is our managed spark and

00:00:25,050 --> 00:00:30,119
Hadoop distribution we run it as a

00:00:27,269 --> 00:00:32,040
service to offer the very popular Apache

00:00:30,119 --> 00:00:34,079
and friends things like presto

00:00:32,040 --> 00:00:35,610
components so people can use them

00:00:34,079 --> 00:00:38,100
without having to be an expert on

00:00:35,610 --> 00:00:40,170
infrastructure or know how to tune open

00:00:38,100 --> 00:00:42,360
source we've gotten really positive

00:00:40,170 --> 00:00:44,460
feedback data proc is used by a wide

00:00:42,360 --> 00:00:47,370
variety of people it's used by data

00:00:44,460 --> 00:00:51,000
developers by data scientists business

00:00:47,370 --> 00:00:53,190
analysts so it works quite well but the

00:00:51,000 --> 00:00:55,230
way that these managed SPARC and Hadoop

00:00:53,190 --> 00:00:57,660
offerings are classically constructed

00:00:55,230 --> 00:00:59,850
have a set of problems which are getting

00:00:57,660 --> 00:01:02,399
compounded as people think about hybrid

00:00:59,850 --> 00:01:04,439
cloud and multi cloud so we thought to

00:01:02,399 --> 00:01:06,540
ourselves how do we start solving some

00:01:04,439 --> 00:01:08,520
of these problems and a lot of it boils

00:01:06,540 --> 00:01:10,950
down to yarn and I'll give you a few

00:01:08,520 --> 00:01:13,020
concrete examples saying you have some

00:01:10,950 --> 00:01:14,580
PI spark and you run that PI spark and

00:01:13,020 --> 00:01:16,950
data proc and then you want to go run it

00:01:14,580 --> 00:01:18,840
on Amazon EMR it's probably not going to

00:01:16,950 --> 00:01:20,909
run the same you have operating system

00:01:18,840 --> 00:01:22,710
differences Python library differences

00:01:20,909 --> 00:01:24,659
the environments are tuned differently

00:01:22,710 --> 00:01:26,490
something that should be very portable

00:01:24,659 --> 00:01:28,920
and easy to move is actually really a

00:01:26,490 --> 00:01:30,810
mobile additionally we see customers who

00:01:28,920 --> 00:01:33,119
want to maximize their cloud spend which

00:01:30,810 --> 00:01:35,579
makes complete sense if they're running

00:01:33,119 --> 00:01:37,110
a hundred CPUs in a project they want to

00:01:35,579 --> 00:01:39,540
use those hundred CPUs to do data

00:01:37,110 --> 00:01:40,950
analytics to serve some of their stack

00:01:39,540 --> 00:01:42,869
they don't want to spend more than they

00:01:40,950 --> 00:01:46,710
have to and we want to enable them to

00:01:42,869 --> 00:01:48,240
make the most of their resources so we

00:01:46,710 --> 00:01:49,619
think that kubernetes is a way that we

00:01:48,240 --> 00:01:51,810
can help solve this problem and

00:01:49,619 --> 00:01:53,880
obviously Google if you're unaware is

00:01:51,810 --> 00:01:55,350
very much behind kubernetes we seeded it

00:01:53,880 --> 00:01:57,450
and we're still the largest contributor

00:01:55,350 --> 00:01:59,369
and kubernetes offers a chance for

00:01:57,450 --> 00:02:00,930
open-source to run more cleanly and

00:01:59,369 --> 00:02:02,549
efficiently and what we see is the next

00:02:00,930 --> 00:02:05,610
generation of managed open-source

00:02:02,549 --> 00:02:08,039
services earlier this year we released

00:02:05,610 --> 00:02:10,349
our spark spark operator for kubernetes

00:02:08,039 --> 00:02:12,629
this was so we could get experience on

00:02:10,349 --> 00:02:13,680
how these open source components work on

00:02:12,629 --> 00:02:14,819
kubernetes

00:02:13,680 --> 00:02:16,650
and it was also to make a meaningful

00:02:14,819 --> 00:02:18,030
contribution back to the open source

00:02:16,650 --> 00:02:19,500
community so people could start

00:02:18,030 --> 00:02:22,019
experimenting and what we saw would be

00:02:19,500 --> 00:02:24,209
the future two weeks ago we were really

00:02:22,019 --> 00:02:26,310
excited to announce that cloud data proc

00:02:24,209 --> 00:02:28,980
has now been modified to run both yarn

00:02:26,310 --> 00:02:30,480
and kubernetes clusters we started with

00:02:28,980 --> 00:02:32,159
spark because we see a lot of our new

00:02:30,480 --> 00:02:34,170
development occurring in spark so it

00:02:32,159 --> 00:02:36,030
made sense to start there but our goal

00:02:34,170 --> 00:02:37,470
is to offer the individual open-source

00:02:36,030 --> 00:02:39,720
components running on kubernetes

00:02:37,470 --> 00:02:41,909
clusters managed by our unified control

00:02:39,720 --> 00:02:43,829
plane in this case cloud data proc but

00:02:41,909 --> 00:02:46,079
it doesn't stop there as part of that

00:02:43,829 --> 00:02:48,180
announcement we also released our flink

00:02:46,079 --> 00:02:50,069
operator these are the first two of many

00:02:48,180 --> 00:02:52,260
but we're actively involved in trying to

00:02:50,069 --> 00:02:53,790
make open-source a better place we don't

00:02:52,260 --> 00:02:55,769
want to just let the open-source

00:02:53,790 --> 00:02:57,540
community do things for us or pay

00:02:55,769 --> 00:03:00,090
someone else hoping that things get done

00:02:57,540 --> 00:03:01,709
we need to be actively involved if I

00:03:00,090 --> 00:03:03,569
were you I would be asking myself what

00:03:01,709 --> 00:03:06,239
does this mean for me what's the value

00:03:03,569 --> 00:03:08,849
in it so there's a few key points let's

00:03:06,239 --> 00:03:10,500
go back to that PI spark that you want

00:03:08,849 --> 00:03:12,120
to run in multiple places in a

00:03:10,500 --> 00:03:14,040
kubernetes world it's going to be much

00:03:12,120 --> 00:03:15,480
easier to take that PI spark code and

00:03:14,040 --> 00:03:16,950
all of the things that it requires and

00:03:15,480 --> 00:03:18,959
move it between clouds and that's a good

00:03:16,950 --> 00:03:21,900
thing the goal is not to lock you in and

00:03:18,959 --> 00:03:24,030
make like difficult second you want you

00:03:21,900 --> 00:03:25,500
want better component isolation so if

00:03:24,030 --> 00:03:27,090
you're using something like spark you

00:03:25,500 --> 00:03:28,980
don't necessarily care about pig and

00:03:27,090 --> 00:03:30,359
hive an uzi and all these other things

00:03:28,980 --> 00:03:32,310
you just want to use that one component

00:03:30,359 --> 00:03:35,190
and we want to make that happen for you

00:03:32,310 --> 00:03:37,109
third we want to offer a platform on

00:03:35,190 --> 00:03:38,760
cloud data proc and Google Cloud for the

00:03:37,109 --> 00:03:40,230
open source vendors who specialize in

00:03:38,760 --> 00:03:42,599
components and you see it happening with

00:03:40,230 --> 00:03:44,459
things like presto and druid we want to

00:03:42,599 --> 00:03:46,590
offer a platform where vendors can make

00:03:44,459 --> 00:03:49,560
money and they don't have to focus on

00:03:46,590 --> 00:03:51,329
cluster management and trying to do

00:03:49,560 --> 00:03:53,940
things like security and logging and

00:03:51,329 --> 00:03:55,650
fourth we see that kubernetes is moving

00:03:53,940 --> 00:03:57,659
a lot faster than yarn and the yarn

00:03:55,650 --> 00:03:59,370
based world and we want to expose a lot

00:03:57,659 --> 00:04:01,680
of the primitives like auto scaling and

00:03:59,370 --> 00:04:04,079
cluster healing in kubernetes to the

00:04:01,680 --> 00:04:06,120
open source components what it means for

00:04:04,079 --> 00:04:07,919
Cloud Data proc is a service is that we

00:04:06,120 --> 00:04:10,680
want it to be that unified control plane

00:04:07,919 --> 00:04:13,049
that offers management security logging

00:04:10,680 --> 00:04:15,479
and monitoring across both yarn and

00:04:13,049 --> 00:04:17,729
kubernetes clusters the nice thing is a

00:04:15,479 --> 00:04:19,079
lot of this work is also directly usable

00:04:17,729 --> 00:04:21,209
and beneficial to the open source

00:04:19,079 --> 00:04:23,729
community so what we've done we spark

00:04:21,209 --> 00:04:26,070
and flink if you want to go run on AWS

00:04:23,729 --> 00:04:27,300
Ruhr you can do that and that's how open

00:04:26,070 --> 00:04:29,970
source should work and we're very

00:04:27,300 --> 00:04:31,560
passionate about that so the next thing

00:04:29,970 --> 00:04:34,319
that I want to do is walk you through a

00:04:31,560 --> 00:04:36,210
few demos here specifically what I want

00:04:34,319 --> 00:04:39,930
to start with is how things work on yarn

00:04:36,210 --> 00:04:41,970
today so you can get a sense for how how

00:04:39,930 --> 00:04:44,460
the world exists is it structured today

00:04:41,970 --> 00:04:45,930
what you see here is a cluster it's a

00:04:44,460 --> 00:04:49,169
yarn based cluster in cloud data proc

00:04:45,930 --> 00:04:51,419
we're opening a notebook and in this

00:04:49,169 --> 00:04:54,000
notebook we're going to create a really

00:04:51,419 --> 00:04:55,560
awesome data science model and really

00:04:54,000 --> 00:04:58,680
what we're doing is we're going to

00:04:55,560 --> 00:05:00,030
calculate pi we're going to go ahead and

00:04:58,680 --> 00:05:01,500
run this model and this is a

00:05:00,030 --> 00:05:03,060
pre-recorded demo I will give you

00:05:01,500 --> 00:05:04,680
information how you can see this live at

00:05:03,060 --> 00:05:06,930
our booth I wanted to focus on you and

00:05:04,680 --> 00:05:09,509
not computer here up on stage so this is

00:05:06,930 --> 00:05:12,449
this is pre-recorded here we ran the

00:05:09,509 --> 00:05:14,580
model and we see that pi is 3.14 which

00:05:12,449 --> 00:05:16,770
is great it checks out as a data

00:05:14,580 --> 00:05:18,719
scientist now I think to myself this

00:05:16,770 --> 00:05:20,639
notebook is good I got the results I

00:05:18,719 --> 00:05:22,860
expected I want to go ahead and run it

00:05:20,639 --> 00:05:25,319
in production so the next thing I'm

00:05:22,860 --> 00:05:27,389
going to do is save it into a cloud

00:05:25,319 --> 00:05:29,550
storage bucket and this will allow me to

00:05:27,389 --> 00:05:31,919
reuse this model and submit as the job

00:05:29,550 --> 00:05:33,630
on my clusters time and again so don't

00:05:31,919 --> 00:05:35,789
have to open up my notebook and try

00:05:33,630 --> 00:05:38,520
running at each time I could also do

00:05:35,789 --> 00:05:40,199
things like script out my notebook and

00:05:38,520 --> 00:05:41,699
run it at a particular point in time and

00:05:40,199 --> 00:05:43,860
you can see this is actually a real demo

00:05:41,699 --> 00:05:46,110
that we recorded there was a typo there

00:05:43,860 --> 00:05:48,180
so it's not totally totally fake here

00:05:46,110 --> 00:05:51,029
the next thing I'm going to do in cloud

00:05:48,180 --> 00:05:53,190
data proc is run this notebook as a job

00:05:51,029 --> 00:05:55,680
so in cloud data proc we have an API

00:05:53,190 --> 00:05:57,300
that has three primitives clusters jobs

00:05:55,680 --> 00:05:59,460
and workflows so you can create manage

00:05:57,300 --> 00:06:01,500
clusters you can create manage jobs and

00:05:59,460 --> 00:06:03,330
you can create manage workflows that are

00:06:01,500 --> 00:06:05,219
essentially a cluster definition and a

00:06:03,330 --> 00:06:07,319
set of jobs you don't have to think

00:06:05,219 --> 00:06:09,120
about how long clusters live and

00:06:07,319 --> 00:06:10,469
managing clusters and all of that in

00:06:09,120 --> 00:06:14,219
this case we're going to go ahead and

00:06:10,469 --> 00:06:16,500
run a PI spark job on a cluster and this

00:06:14,219 --> 00:06:17,789
is this pi spark job we fed it the cloud

00:06:16,500 --> 00:06:20,430
storage bucket location

00:06:17,789 --> 00:06:22,440
so basically we're running this PI spark

00:06:20,430 --> 00:06:25,409
I created in my notebook as a job

00:06:22,440 --> 00:06:26,729
against cluster one of the neat things

00:06:25,409 --> 00:06:29,159
that we've done with quad 8 appraoch is

00:06:26,729 --> 00:06:30,960
as we get driver output we spool it

00:06:29,159 --> 00:06:33,629
through our API so in this case we're

00:06:30,960 --> 00:06:35,699
using the web UI and you're gonna see

00:06:33,629 --> 00:06:37,830
the driver output stream directly and

00:06:35,699 --> 00:06:41,040
this is to avoid you having to do things

00:06:37,830 --> 00:06:44,430
like SSH into a cluster you just saw the

00:06:41,040 --> 00:06:46,500
and prompt the g-cloud cloud SDK hint

00:06:44,430 --> 00:06:48,930
come up you could also do this through a

00:06:46,500 --> 00:06:51,570
terminal if you were so inclined and you

00:06:48,930 --> 00:06:54,870
can see here that we have calculated

00:06:51,570 --> 00:06:57,210
that pi is 3.14 well let's think about

00:06:54,870 --> 00:06:59,760
this in the kubernetes world instead of

00:06:57,210 --> 00:07:02,220
starting with a yarn based cluster I'm

00:06:59,760 --> 00:07:04,860
gonna go to my Cooper vision kubernetes

00:07:02,220 --> 00:07:09,420
engine clusters on Google cloud platform

00:07:04,860 --> 00:07:11,700
I have a cluster here called my cool web

00:07:09,420 --> 00:07:13,020
apps and what's neat about this cluster

00:07:11,700 --> 00:07:14,760
is I've gotten all of my networking

00:07:13,020 --> 00:07:16,200
settings I'm running other stuff on it

00:07:14,760 --> 00:07:18,630
so I'm already utilizing this cluster

00:07:16,200 --> 00:07:20,430
you can see here the helm install for

00:07:18,630 --> 00:07:23,520
the components which allow this cluster

00:07:20,430 --> 00:07:26,010
to plug into the cloud data proc API and

00:07:23,520 --> 00:07:27,720
what we're doing now is listing all of

00:07:26,010 --> 00:07:29,040
the clusters that data proc knows about

00:07:27,720 --> 00:07:31,110
and what's really cool is you can

00:07:29,040 --> 00:07:32,970
actually see that we now can show both

00:07:31,110 --> 00:07:34,950
the yarn clusters and the kubernetes

00:07:32,970 --> 00:07:37,470
clusters together so you don't get this

00:07:34,950 --> 00:07:39,060
disjointed view of the world this is

00:07:37,470 --> 00:07:41,580
really important because as people move

00:07:39,060 --> 00:07:43,350
from yarn to kubernetes you don't have

00:07:41,580 --> 00:07:43,910
to think and rationalize about one or

00:07:43,350 --> 00:07:45,900
the other

00:07:43,910 --> 00:07:48,540
additionally the container for our

00:07:45,900 --> 00:07:49,980
kubernetes world matches our yarn based

00:07:48,540 --> 00:07:53,190
image so you don't have to think about

00:07:49,980 --> 00:07:54,630
how you juggle between the two in this

00:07:53,190 --> 00:07:57,030
case we're going to use that same jobs

00:07:54,630 --> 00:07:59,580
API that I mentioned previously and then

00:07:57,030 --> 00:08:01,800
we saw to submit this job to the

00:07:59,580 --> 00:08:04,650
kubernetes cluster you can see that I've

00:08:01,800 --> 00:08:07,200
specified the particular container that

00:08:04,650 --> 00:08:09,690
I want to use and we're going to also

00:08:07,200 --> 00:08:13,470
specify the notebook that I want to run

00:08:09,690 --> 00:08:14,880
on this kubernetes cluster you again see

00:08:13,470 --> 00:08:17,940
the output so you don't have to think

00:08:14,880 --> 00:08:19,470
about opening SSH windows and juggling

00:08:17,940 --> 00:08:21,960
complicated networking settings you're

00:08:19,470 --> 00:08:26,040
just getting the raw output of the job

00:08:21,960 --> 00:08:27,600
as it runs right now in the future in

00:08:26,040 --> 00:08:29,460
the web UI you will actually just see

00:08:27,600 --> 00:08:31,050
something as simple as a toggle box

00:08:29,460 --> 00:08:33,360
between running something on a yarn

00:08:31,050 --> 00:08:35,520
cluster or kubernetes cluster and again

00:08:33,360 --> 00:08:37,620
this is to make transition in migration

00:08:35,520 --> 00:08:40,050
between these two worlds as seamless as

00:08:37,620 --> 00:08:41,310
possible a lot of things that work and

00:08:40,050 --> 00:08:43,229
run today you might not want to just

00:08:41,310 --> 00:08:46,470
instantly move to coober days and that's

00:08:43,229 --> 00:08:48,810
completely completely fine in this

00:08:46,470 --> 00:08:50,610
example you see that there's a job ID

00:08:48,810 --> 00:08:53,010
and that was assigned by the data proc

00:08:50,610 --> 00:08:54,900
service to show you how these are

00:08:53,010 --> 00:08:57,390
integrated and how we've made spar

00:08:54,900 --> 00:08:59,790
play well with kubernetes we can go into

00:08:57,390 --> 00:09:02,580
the kubernetes engine workloads and

00:08:59,790 --> 00:09:05,070
actually look up the logs based on that

00:09:02,580 --> 00:09:07,230
data proc job IDs so again you're not

00:09:05,070 --> 00:09:09,210
having to think and rationalize about I

00:09:07,230 --> 00:09:10,890
have this thing in service a I have this

00:09:09,210 --> 00:09:13,020
thing in service B how do I match them

00:09:10,890 --> 00:09:15,660
it's all seamlessly integrated together

00:09:13,020 --> 00:09:17,970
and in this case we've loaded the spark

00:09:15,660 --> 00:09:21,060
driver output directly from kubernetes

00:09:17,970 --> 00:09:23,400
engine and we can see that pi is 3.14

00:09:21,060 --> 00:09:25,200
and that is an example of the fact that

00:09:23,400 --> 00:09:28,020
we've taken the due diligence and the

00:09:25,200 --> 00:09:31,140
work to cleanly integrate spark and

00:09:28,020 --> 00:09:33,300
kubernetes this is really complicated

00:09:31,140 --> 00:09:34,920
because this is the clean way to do it

00:09:33,300 --> 00:09:36,960
we're making a clean break but we see

00:09:34,920 --> 00:09:39,450
this is the right way to do things going

00:09:36,960 --> 00:09:41,130
into the future it's not just you know

00:09:39,450 --> 00:09:42,960
as simple as putting yarn on kubernetes

00:09:41,130 --> 00:09:46,290
that doesn't set people up for success

00:09:42,960 --> 00:09:48,180
in the future we have a booth we also

00:09:46,290 --> 00:09:50,970
have engineers from the data proc team

00:09:48,180 --> 00:09:52,470
if you want to see this running on GCP

00:09:50,970 --> 00:09:54,750
please stop by our booth we can also

00:09:52,470 --> 00:09:56,970
coincidentally show it you this running

00:09:54,750 --> 00:09:59,279
on other clouds as well which is

00:09:56,970 --> 00:10:01,500
obviously part of our long-term future

00:09:59,279 --> 00:10:04,550
and what most customers want thank you

00:10:01,500 --> 00:10:04,550
very much I appreciate it

00:10:10,510 --> 00:10:12,570

YouTube URL: https://www.youtube.com/watch?v=6qx0Mykbz2E


