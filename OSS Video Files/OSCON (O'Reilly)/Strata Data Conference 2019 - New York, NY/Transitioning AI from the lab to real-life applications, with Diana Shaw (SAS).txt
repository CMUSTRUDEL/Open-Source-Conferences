Title: Transitioning AI from the lab to real-life applications, with Diana Shaw (SAS)
Publication date: 2019-10-03
Playlist: Strata Data Conference 2019 - New York, NY
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,060 --> 00:00:04,830
hi Raja McCool is here with Diana Shaw

00:00:02,310 --> 00:00:07,020
who works for SAS welcome thank you

00:00:04,830 --> 00:00:09,240
so SAS has his long history in

00:00:07,020 --> 00:00:11,010
statistics and it's kind of advanced

00:00:09,240 --> 00:00:14,460
mathematics what is your approach to AI

00:00:11,010 --> 00:00:16,500
well at SAS we define AI as the science

00:00:14,460 --> 00:00:19,439
of training systems to emulate human

00:00:16,500 --> 00:00:21,600
tasks through learning and automation so

00:00:19,439 --> 00:00:23,820
for us we're really actually creating a

00:00:21,600 --> 00:00:25,740
lot of AI within our applications

00:00:23,820 --> 00:00:27,630
themselves the software so we're adding

00:00:25,740 --> 00:00:29,250
a lot of automation and intelligence to

00:00:27,630 --> 00:00:31,980
make it easier for people to get through

00:00:29,250 --> 00:00:33,690
the full analytic lifecycle that really

00:00:31,980 --> 00:00:35,730
helps us because then we can create AI

00:00:33,690 --> 00:00:38,040
applications in a very wide range of

00:00:35,730 --> 00:00:40,200
topics whether they're using text

00:00:38,040 --> 00:00:41,730
analytics or computer vision you can get

00:00:40,200 --> 00:00:43,739
through the entire application very

00:00:41,730 --> 00:00:45,270
quickly there's a lot of automation and

00:00:43,739 --> 00:00:48,149
intelligence to help us along the way

00:00:45,270 --> 00:00:49,680
right so can you share some examples of

00:00:48,149 --> 00:00:51,960
how customers are actually putting this

00:00:49,680 --> 00:00:53,250
in practice yeah we're really excited

00:00:51,960 --> 00:00:55,410
there's some really great work that's

00:00:53,250 --> 00:00:57,600
going on at the Amsterdam University

00:00:55,410 --> 00:00:59,879
Medical Center where they're using our

00:00:57,600 --> 00:01:02,219
computer vision capabilities to actually

00:00:59,879 --> 00:01:05,369
emulate how a radiologist would look at

00:01:02,219 --> 00:01:06,750
CT scans to determine how patients are

00:01:05,369 --> 00:01:08,820
progressing after they've gone through

00:01:06,750 --> 00:01:11,250
chemotherapy and in fact they've

00:01:08,820 --> 00:01:13,680
actually automated the full calculation

00:01:11,250 --> 00:01:15,810
to determine whether or not that

00:01:13,680 --> 00:01:17,400
chemotherapy is working well and from

00:01:15,810 --> 00:01:19,320
that they're able to make decisions as

00:01:17,400 --> 00:01:23,009
to whether or not they have to go in for

00:01:19,320 --> 00:01:25,290
surgical removal or other other paths on

00:01:23,009 --> 00:01:27,330
their chemotherapy or their cancer

00:01:25,290 --> 00:01:29,009
journey so I'm curious with an

00:01:27,330 --> 00:01:31,350
application like that and health is such

00:01:29,009 --> 00:01:35,790
a fraud issue in some ways how does the

00:01:31,350 --> 00:01:38,670
kind of human and AI interface work yeah

00:01:35,790 --> 00:01:41,250
well so you really want to have humans

00:01:38,670 --> 00:01:44,070
being able to use their brain for what

00:01:41,250 --> 00:01:46,680
humans do well which is thinking empathy

00:01:44,070 --> 00:01:48,930
you know creative insights

00:01:46,680 --> 00:01:50,340
whereas machines are incredible of being

00:01:48,930 --> 00:01:52,530
able to deal with a lot of data and

00:01:50,340 --> 00:01:55,229
really complex algorithms and be able to

00:01:52,530 --> 00:01:56,909
deal with all of that at scale so it

00:01:55,229 --> 00:01:58,380
enables us to really take some of the

00:01:56,909 --> 00:02:00,210
tedious work you know the stuff that

00:01:58,380 --> 00:02:01,229
we've done maybe you've done a report

00:02:00,210 --> 00:02:03,630
once or twice and you think to yourself

00:02:01,229 --> 00:02:05,969
why does it take so long to create it

00:02:03,630 --> 00:02:07,040
have the computer do that automate that

00:02:05,969 --> 00:02:08,450
process

00:02:07,040 --> 00:02:09,920
and then free up the time so that you

00:02:08,450 --> 00:02:11,600
can be more creative and you can think

00:02:09,920 --> 00:02:13,490
through how to solve other problems and

00:02:11,600 --> 00:02:15,320
derive insights and have more

00:02:13,490 --> 00:02:18,320
data-driven decisions so that's how we

00:02:15,320 --> 00:02:21,290
approach it right and where do you see

00:02:18,320 --> 00:02:23,630
I'm moving next year and maybe even

00:02:21,290 --> 00:02:25,460
looking out a few years yeah I really

00:02:23,630 --> 00:02:27,950
think AI is gonna move out of the lab

00:02:25,460 --> 00:02:29,390
and more into the general space where

00:02:27,950 --> 00:02:31,910
people are going to be dealing with it

00:02:29,390 --> 00:02:33,530
and the reason why I say that is there

00:02:31,910 --> 00:02:35,900
is a lot being done in terms of

00:02:33,530 --> 00:02:37,790
automating a lot of the steps that a

00:02:35,900 --> 00:02:40,459
data scientist would go through in order

00:02:37,790 --> 00:02:42,130
to create an AI application so being

00:02:40,459 --> 00:02:44,959
able to free at them up to be able to

00:02:42,130 --> 00:02:46,160
accelerate their journey in addition to

00:02:44,959 --> 00:02:48,560
that there's actually a lot of

00:02:46,160 --> 00:02:51,020
democratization that's happening for AI

00:02:48,560 --> 00:02:52,670
to be able to empower people that may

00:02:51,020 --> 00:02:55,730
not have gone and done their masters or

00:02:52,670 --> 00:02:58,250
PhD in computer science or analytics or

00:02:55,730 --> 00:03:00,770
anything regarding AI but we're now

00:02:58,250 --> 00:03:03,440
enabling others to be able to have the

00:03:00,770 --> 00:03:05,930
power of advanced analytics to be able

00:03:03,440 --> 00:03:08,959
to add and create more as part of a team

00:03:05,930 --> 00:03:11,870
sport and then with technology that's

00:03:08,959 --> 00:03:14,300
allowing us to easily embed and deploy

00:03:11,870 --> 00:03:16,310
those models very quickly into a

00:03:14,300 --> 00:03:18,470
production alized environment it's

00:03:16,310 --> 00:03:20,390
really where I think we're gonna take AI

00:03:18,470 --> 00:03:24,170
out of the lab and really move it into

00:03:20,390 --> 00:03:26,450
the commonplace applications so on that

00:03:24,170 --> 00:03:28,760
journey there must be some challenges

00:03:26,450 --> 00:03:31,570
and you guys sit at one end of the kind

00:03:28,760 --> 00:03:33,980
of the model building an analytic side

00:03:31,570 --> 00:03:35,690
the data is absolute big space where do

00:03:33,980 --> 00:03:37,430
you see the challenges there's a couple

00:03:35,690 --> 00:03:39,830
challenges so there's challenges in the

00:03:37,430 --> 00:03:41,989
sense of data itself a perfect case in

00:03:39,830 --> 00:03:45,140
point is we at SAS actually wanted to

00:03:41,989 --> 00:03:46,760
use our own images to be able to use

00:03:45,140 --> 00:03:48,890
computer vision when we walk into a

00:03:46,760 --> 00:03:50,900
building so that the receptionist would

00:03:48,890 --> 00:03:52,940
be able to quickly know without having

00:03:50,900 --> 00:03:55,070
to wait for us to flash our badge for

00:03:52,940 --> 00:03:56,540
whether we were allowed access and one

00:03:55,070 --> 00:03:58,220
would think that we're working for a

00:03:56,540 --> 00:04:01,400
company that would enable this but

00:03:58,220 --> 00:04:03,500
because we never actually opted in from

00:04:01,400 --> 00:04:05,660
a legal standpoint for our images to be

00:04:03,500 --> 00:04:08,120
used in that regard we can't use it

00:04:05,660 --> 00:04:09,769
so I think data is still a little bit of

00:04:08,120 --> 00:04:12,170
a challenge especially for computer

00:04:09,769 --> 00:04:14,090
vision applications like that but then

00:04:12,170 --> 00:04:16,489
also in terms of ethics

00:04:14,090 --> 00:04:18,889
and in terms of explained ability I

00:04:16,489 --> 00:04:20,480
think those are some other challenges at

00:04:18,889 --> 00:04:22,610
sass we're doing a lot in terms of

00:04:20,480 --> 00:04:24,800
generating explain ability in terms of a

00:04:22,610 --> 00:04:26,660
natural language output so that a data

00:04:24,800 --> 00:04:28,010
scientists or a citizen data scientist

00:04:26,660 --> 00:04:30,710
can really understand what that

00:04:28,010 --> 00:04:32,090
mathematical algorithm is doing so as to

00:04:30,710 --> 00:04:33,919
make sure that it's really solving the

00:04:32,090 --> 00:04:36,650
problem that it was intended to solve

00:04:33,919 --> 00:04:39,020
but then when we think about the ethic

00:04:36,650 --> 00:04:41,180
situation or we think about maybe bias

00:04:39,020 --> 00:04:42,800
as well within the data this is where

00:04:41,180 --> 00:04:44,660
having data science be more of a team

00:04:42,800 --> 00:04:48,010
sport where you can have you know the

00:04:44,660 --> 00:04:51,050
trained really classically trained and

00:04:48,010 --> 00:04:52,760
strong capable data scientists work with

00:04:51,050 --> 00:04:54,919
the citizen data scientists work with

00:04:52,760 --> 00:04:56,360
the business practitioners work with you

00:04:54,919 --> 00:04:58,430
know medical stewards and things like

00:04:56,360 --> 00:04:59,990
that having everybody work together we

00:04:58,430 --> 00:05:01,669
can make sure that we really understand

00:04:59,990 --> 00:05:04,250
whether or not we have biased data going

00:05:01,669 --> 00:05:05,750
in or for making biased decision making

00:05:04,250 --> 00:05:08,300
so that we can make sure that that

00:05:05,750 --> 00:05:09,290
application in the end is as bias-free

00:05:08,300 --> 00:05:11,479
as possible

00:05:09,290 --> 00:05:13,160
very very explainable and then something

00:05:11,479 --> 00:05:14,720
that can be utilized by the masses

00:05:13,160 --> 00:05:16,850
because if we create something that's

00:05:14,720 --> 00:05:18,830
really complex what we find is that the

00:05:16,850 --> 00:05:20,690
business community doesn't actually want

00:05:18,830 --> 00:05:22,550
to use it if they don't understand it so

00:05:20,690 --> 00:05:24,740
we need to have them be bought in and be

00:05:22,550 --> 00:05:26,510
part of that team initially early on I

00:05:24,740 --> 00:05:28,280
think that actually drives quite well

00:05:26,510 --> 00:05:33,070
with what we have been finding ourselves

00:05:28,280 --> 00:05:33,070
ok thank you for your time thank you

00:05:38,740 --> 00:05:40,800

YouTube URL: https://www.youtube.com/watch?v=lvp3VeaNp2Q


