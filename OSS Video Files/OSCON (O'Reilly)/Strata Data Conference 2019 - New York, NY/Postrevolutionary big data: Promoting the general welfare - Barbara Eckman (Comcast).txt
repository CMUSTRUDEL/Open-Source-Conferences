Title: Postrevolutionary big data: Promoting the general welfare - Barbara Eckman (Comcast)
Publication date: 2019-09-26
Playlist: Strata Data Conference 2019 - New York, NY
Description: 
	(Sponsored by Io-Tahoe) - Data democratization was a chief goal and major benefit of the big data revolution. Data owners no longer have to wait for the EDW IT Department to write ETL jobs before they can access and query their data. Anyone can store their data in the data lake, in any structure (or no consistent structure).

Ten years later, we’re struggling with the unintended consequences of the big data revolution. Data is often multiply redundantly stored. Data of interest is difficult to locate, and its schemas are often difficult to understand. Precise connections among putatively related datasets are not captured. The great promise of the big data revolution—integration of data across silos to discover otherwise hidden trends and improve customer experience— has largely gone unrealized due to this postrevolutionary chaos.

Barbara Eckman shares lessons learned from early big data mistakes and the progress her team at Comcast is making toward a postrevolutionary big data vision.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,210 --> 00:00:05,250
I'm really here to talk about where Big

00:00:03,060 --> 00:00:06,890
Data went wrong and how to fix it how we

00:00:05,250 --> 00:00:10,650
at Comcast are working to fix it

00:00:06,890 --> 00:00:13,799
required slide from the lawyers very

00:00:10,650 --> 00:00:16,260
important so I'm going to tell you a

00:00:13,799 --> 00:00:19,949
passionate story of oppression and

00:00:16,260 --> 00:00:22,740
liberation back in the day we had

00:00:19,949 --> 00:00:25,949
enterprise data warehouses they were

00:00:22,740 --> 00:00:29,369
owned and tightly controlled by DBAs and

00:00:25,949 --> 00:00:31,380
IT departments there was one schema and

00:00:29,369 --> 00:00:35,340
one schema only that you could put data

00:00:31,380 --> 00:00:37,340
in to a data warehouse and they

00:00:35,340 --> 00:00:41,700
controlled even the ETL jobs sometimes

00:00:37,340 --> 00:00:45,510
this was oppressive they were colonial

00:00:41,700 --> 00:00:48,539
oppressors like King George here so the

00:00:45,510 --> 00:00:48,930
cry went round revolution power to the

00:00:48,539 --> 00:00:52,860
people

00:00:48,930 --> 00:00:57,090
data democratization and that was big

00:00:52,860 --> 00:01:01,800
data's beginning so they overthrew these

00:00:57,090 --> 00:01:05,430
colonial oppressors of the e DWS and now

00:01:01,800 --> 00:01:09,270
are then anyone could write data to the

00:01:05,430 --> 00:01:14,280
data leak in any Sakina or no consistent

00:01:09,270 --> 00:01:16,470
schema and schemas were only discovered

00:01:14,280 --> 00:01:19,259
at the time of reading the data that's

00:01:16,470 --> 00:01:26,220
what we call schema on read previously

00:01:19,259 --> 00:01:28,110
it was called schema on right however as

00:01:26,220 --> 00:01:31,530
with many revolutions there was chaos

00:01:28,110 --> 00:01:35,040
after this one and that's basically

00:01:31,530 --> 00:01:38,189
where we are now so data representation

00:01:35,040 --> 00:01:40,650
and expressiveness of the schemas and of

00:01:38,189 --> 00:01:44,399
the data and self service have really

00:01:40,650 --> 00:01:47,549
blossomed data discovery finding data of

00:01:44,399 --> 00:01:50,579
interest and integration of data have

00:01:47,549 --> 00:01:52,649
really suffered data gives up none of

00:01:50,579 --> 00:01:54,840
its secrets until it's actually read and

00:01:52,649 --> 00:01:56,360
even after you read it unless you made

00:01:54,840 --> 00:01:59,189
it recently

00:01:56,360 --> 00:02:00,810
the attribute names will not tell you

00:01:59,189 --> 00:02:03,270
will not really give you a hint as to

00:02:00,810 --> 00:02:07,350
what the data means at least not all the

00:02:03,270 --> 00:02:10,810
time and data duplication is rampant in

00:02:07,350 --> 00:02:14,890
these lakes so

00:02:10,810 --> 00:02:18,190
at Comcast we want to define a new order

00:02:14,890 --> 00:02:20,560
and we're writing a constitution for the

00:02:18,190 --> 00:02:26,500
new order that will promote the general

00:02:20,560 --> 00:02:29,489
welfare the data can be published to the

00:02:26,500 --> 00:02:33,900
system in any schema we're keeping that

00:02:29,489 --> 00:02:38,319
absolutely but the publisher needs to

00:02:33,900 --> 00:02:42,069
register the schema with documentation

00:02:38,319 --> 00:02:45,120
about what it means before as the data's

00:02:42,069 --> 00:02:51,370
on-boarded so we call this schemas

00:02:45,120 --> 00:02:52,930
plural on right how do we do this I'd

00:02:51,370 --> 00:02:54,310
love to have an hour just to tell you

00:02:52,930 --> 00:03:02,739
how we do it

00:02:54,310 --> 00:03:06,280
I'll cover four methods one is to

00:03:02,739 --> 00:03:09,670
document each attribute of the schema on

00:03:06,280 --> 00:03:13,200
onboarding as I said capture the data

00:03:09,670 --> 00:03:15,940
lineage as it flows through the system

00:03:13,200 --> 00:03:18,370
perhaps being transformed while it's

00:03:15,940 --> 00:03:20,410
doing so capture who did the

00:03:18,370 --> 00:03:23,290
transformation because you might not

00:03:20,410 --> 00:03:25,690
like who do the transformation provide a

00:03:23,290 --> 00:03:28,440
unified and searchable metadata and

00:03:25,690 --> 00:03:31,120
schema repository and lineage repository

00:03:28,440 --> 00:03:36,100
so the people can find data that they're

00:03:31,120 --> 00:03:38,350
interested in and control this rampant

00:03:36,100 --> 00:03:40,390
duplication of data well in order to do

00:03:38,350 --> 00:03:43,019
that first you have to identify the

00:03:40,390 --> 00:03:46,750
duplication of data so to identify

00:03:43,019 --> 00:03:49,329
duplicate data we find that the best way

00:03:46,750 --> 00:03:54,310
to do this is through ml based data

00:03:49,329 --> 00:03:57,370
based ml analysis and that will also

00:03:54,310 --> 00:03:59,829
help you find not lineage quite but

00:03:57,370 --> 00:04:02,980
putative lineage semantic equivalence

00:03:59,829 --> 00:04:07,480
--is between tables tables that let's

00:04:02,980 --> 00:04:10,090
share some attributes Minar all again

00:04:07,480 --> 00:04:13,150
data based ml is really the way to go we

00:04:10,090 --> 00:04:16,630
feel so we're trying to form a more

00:04:13,150 --> 00:04:19,359
perfect union and as in the US

00:04:16,630 --> 00:04:23,560
Constitution I'm quoting from the

00:04:19,359 --> 00:04:25,510
preamble here the founders felt that

00:04:23,560 --> 00:04:29,860
the important thing about forming a more

00:04:25,510 --> 00:04:33,400
perfect union were two polls on the Left

00:04:29,860 --> 00:04:37,530
we have order that's the first Chief

00:04:33,400 --> 00:04:40,710
Justice John Jay on the right we have

00:04:37,530 --> 00:04:44,350
individual creativity and liberty and

00:04:40,710 --> 00:04:48,100
these two together ensure the middle

00:04:44,350 --> 00:04:50,620
which is domestic tranquility general

00:04:48,100 --> 00:04:53,260
welfare and getting to eat or drink hot

00:04:50,620 --> 00:04:57,160
chocolate together which is what these

00:04:53,260 --> 00:05:00,340
folks are doing on the big data level

00:04:57,160 --> 00:05:02,230
what does this mean well we definitely

00:05:00,340 --> 00:05:05,520
are going to keep schemas in the hands

00:05:02,230 --> 00:05:08,980
of the people but we also are going to

00:05:05,520 --> 00:05:11,200
take back some of what we threw away in

00:05:08,980 --> 00:05:14,830
the Revolution which is data governance

00:05:11,200 --> 00:05:17,680
order so cataloging and documenting the

00:05:14,830 --> 00:05:21,130
schemas and lineage and controlling this

00:05:17,680 --> 00:05:25,480
rampant data duplication so ultimately

00:05:21,130 --> 00:05:27,820
what this what this enables is we can

00:05:25,480 --> 00:05:30,340
discover and integrate data across silos

00:05:27,820 --> 00:05:34,000
we can trace the data journey through

00:05:30,340 --> 00:05:36,640
the the enterprise ultimately data

00:05:34,000 --> 00:05:40,870
scientists can find data use the data

00:05:36,640 --> 00:05:42,400
across silos to make insights that will

00:05:40,870 --> 00:05:45,010
be important to the business in to the

00:05:42,400 --> 00:05:49,330
customers and we all get to eat drink

00:05:45,010 --> 00:05:51,450
hot chocolate together so thank you for

00:05:49,330 --> 00:05:51,450
listening

00:05:58,070 --> 00:06:00,130

YouTube URL: https://www.youtube.com/watch?v=-shZkzt4Si8


