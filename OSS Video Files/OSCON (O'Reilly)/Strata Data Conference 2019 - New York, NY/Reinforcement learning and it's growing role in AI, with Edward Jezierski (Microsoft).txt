Title: Reinforcement learning and it's growing role in AI, with Edward Jezierski (Microsoft)
Publication date: 2019-10-03
Playlist: Strata Data Conference 2019 - New York, NY
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:05,160
I Roger McGough is here with Edward

00:00:02,639 --> 00:00:07,649
jiske from Microsoft welcome thank you

00:00:05,160 --> 00:00:09,540
and we're gonna cover a topic I think I

00:00:07,649 --> 00:00:11,160
really looking forward to the

00:00:09,540 --> 00:00:12,990
conversation around reinforcement

00:00:11,160 --> 00:00:14,910
learning so you can explain like like

00:00:12,990 --> 00:00:16,410
what it is and why is it gaining such an

00:00:14,910 --> 00:00:17,789
important role these days in the

00:00:16,410 --> 00:00:20,010
community mm-hmm

00:00:17,789 --> 00:00:22,529
well so reinforcement learning is a type

00:00:20,010 --> 00:00:24,570
of AI techniques you use when you have

00:00:22,529 --> 00:00:26,490
to learn how to make decisions to get to

00:00:24,570 --> 00:00:30,000
an outcome right it's very different

00:00:26,490 --> 00:00:32,399
than trying to learn to detect something

00:00:30,000 --> 00:00:34,380
an image or trying to find something in

00:00:32,399 --> 00:00:36,809
a data set or in text because it is

00:00:34,380 --> 00:00:38,460
about making decisions to go this way or

00:00:36,809 --> 00:00:41,370
this way do you choose this do choose

00:00:38,460 --> 00:00:43,829
that and that entails a whole set of

00:00:41,370 --> 00:00:46,920
concepts that are about exploring the

00:00:43,829 --> 00:00:49,890
unknown you have the notion of exploring

00:00:46,920 --> 00:00:51,750
versus exploiting which is do the tried

00:00:49,890 --> 00:00:54,510
and true versus try something new you

00:00:51,750 --> 00:00:56,969
bring in chi level concepts like the

00:00:54,510 --> 00:00:58,890
notion of curiosity like how much should

00:00:56,969 --> 00:01:00,899
you bias to try new things a notion of

00:00:58,890 --> 00:01:04,939
creativity how crazy are the things that

00:01:00,899 --> 00:01:07,110
you're willing to try out and

00:01:04,939 --> 00:01:09,180
reinforcement learning is a science that

00:01:07,110 --> 00:01:11,880
studies how these things come together

00:01:09,180 --> 00:01:13,670
in a learning system so it's a little

00:01:11,880 --> 00:01:16,979
different than like the mainstream AI

00:01:13,670 --> 00:01:18,780
where can organizations apply this and

00:01:16,979 --> 00:01:20,759
this Microsoft man is like this kind of

00:01:18,780 --> 00:01:22,590
stuff yeah so we've been using it for

00:01:20,759 --> 00:01:24,659
how many years in across the different

00:01:22,590 --> 00:01:25,979
places so if you think of it is a type

00:01:24,659 --> 00:01:28,350
of AI that helps you make the right

00:01:25,979 --> 00:01:29,400
decision that the right time it's the

00:01:28,350 --> 00:01:31,590
sort of thing that can help you

00:01:29,400 --> 00:01:34,470
basically learn or discover the business

00:01:31,590 --> 00:01:37,290
rules that you use in your applications

00:01:34,470 --> 00:01:40,110
to achieve something right so we've been

00:01:37,290 --> 00:01:42,329
using it in many places one example is a

00:01:40,110 --> 00:01:45,090
Xbox homepage every time somebody opens

00:01:42,329 --> 00:01:47,009
their console they get shown with

00:01:45,090 --> 00:01:48,720
different things they can do and it

00:01:47,009 --> 00:01:50,250
turns out there's a lot of good machine

00:01:48,720 --> 00:01:52,140
learning people that can say if you were

00:01:50,250 --> 00:01:53,880
to watch a movie I know exactly which

00:01:52,140 --> 00:01:55,890
movie you'd probably like or which game

00:01:53,880 --> 00:01:57,899
you'll probably play but what's really

00:01:55,890 --> 00:01:59,969
really hard to guess is what are you

00:01:57,899 --> 00:02:01,860
coming to the console for are you coming

00:01:59,969 --> 00:02:04,409
just to watch a stream and sit back to

00:02:01,860 --> 00:02:06,000
watch a movie to join a game casually

00:02:04,409 --> 00:02:08,690
with your friends to get another

00:02:06,000 --> 00:02:12,530
achievement and like try to you know

00:02:08,690 --> 00:02:14,120
complete that sort of result and it's

00:02:12,530 --> 00:02:15,440
very tricky to kind of guess that and

00:02:14,120 --> 00:02:17,570
it's very contextual and it's very

00:02:15,440 --> 00:02:20,440
personal so we've been applying it there

00:02:17,570 --> 00:02:22,420
to essentially based on real-time

00:02:20,440 --> 00:02:25,460
context that we know about that user

00:02:22,420 --> 00:02:27,260
suggests the right type of activity and

00:02:25,460 --> 00:02:29,150
we've done like 40% increase in

00:02:27,260 --> 00:02:29,960
engagement with that on top of all the

00:02:29,150 --> 00:02:31,730
work that the machine learning

00:02:29,960 --> 00:02:33,350
scientists were already doing or we use

00:02:31,730 --> 00:02:35,330
it in MSN News when you're trying to

00:02:33,350 --> 00:02:38,960
stay on top of a changing and simulate

00:02:35,330 --> 00:02:40,880
able a very dynamic world and you know

00:02:38,960 --> 00:02:42,680
your AI algorithms are learning from the

00:02:40,880 --> 00:02:45,050
last 15 minutes and republishing

00:02:42,680 --> 00:02:46,880
themselves automatically every five so

00:02:45,050 --> 00:02:49,250
if you think of content personalization

00:02:46,880 --> 00:02:51,350
or personalization in general it is a

00:02:49,250 --> 00:02:53,420
choice where I think I can make you

00:02:51,350 --> 00:02:57,280
happy or we can have a better

00:02:53,420 --> 00:03:00,200
transaction if I show a or B or C or D

00:02:57,280 --> 00:03:01,940
let's use the I to discover the rule in

00:03:00,200 --> 00:03:04,100
each context that will tell me which one

00:03:01,940 --> 00:03:05,660
to use and how do I tell it whether it

00:03:04,100 --> 00:03:07,850
worked or not well all I need to do is

00:03:05,660 --> 00:03:10,310
transform the sort of result we were

00:03:07,850 --> 00:03:12,830
seeking for into a reward score that

00:03:10,310 --> 00:03:15,709
then is a minimalist way of telling they

00:03:12,830 --> 00:03:18,380
are good or bad and it'll just keep

00:03:15,709 --> 00:03:20,270
learning from that great and so what

00:03:18,380 --> 00:03:21,590
challenges you or Jason's face because

00:03:20,270 --> 00:03:24,440
it's a little different than regular AI

00:03:21,590 --> 00:03:26,090
mm-hmm and implementing RL yeah so I

00:03:24,440 --> 00:03:27,820
think one of the biggest challenges

00:03:26,090 --> 00:03:30,200
first is to understanding where it is

00:03:27,820 --> 00:03:34,040
acceptable we've seen a lot of great

00:03:30,200 --> 00:03:35,810
advancements in AI and driven by RL you

00:03:34,040 --> 00:03:38,480
know the video games are one like

00:03:35,810 --> 00:03:40,250
Starcraft and go and you know we're

00:03:38,480 --> 00:03:43,430
doing the sub driving cars I mean

00:03:40,250 --> 00:03:45,769
there's a lot of industry attention

00:03:43,430 --> 00:03:48,310
going to this space and a lot of these

00:03:45,769 --> 00:03:51,489
problems get solved in simulation so

00:03:48,310 --> 00:03:54,680
what one of the big challenges that

00:03:51,489 --> 00:03:57,260
brings in of itself a whole family of

00:03:54,680 --> 00:03:59,810
things that needs to be resolved to make

00:03:57,260 --> 00:04:02,390
it work in the real world is to have it

00:03:59,810 --> 00:04:04,850
operate not in a simulation but in the

00:04:02,390 --> 00:04:08,390
physical tangible real time world that

00:04:04,850 --> 00:04:10,760
you and I exist in so that means for

00:04:08,390 --> 00:04:12,830
example being able to have algorithms

00:04:10,760 --> 00:04:14,180
that can learn without retries or

00:04:12,830 --> 00:04:17,570
without saying well what if we tried

00:04:14,180 --> 00:04:19,169
this instead of that or designing the

00:04:17,570 --> 00:04:22,109
algorithms so that when

00:04:19,169 --> 00:04:23,819
they are exploring they treat that one

00:04:22,109 --> 00:04:25,680
interaction they had with that use their

00:04:23,819 --> 00:04:26,909
as a very valuable and scarce and

00:04:25,680 --> 00:04:29,639
hard-won data point

00:04:26,909 --> 00:04:31,319
instead of saying Oh put another quarter

00:04:29,639 --> 00:04:33,659
in the clouding in another billion data

00:04:31,319 --> 00:04:35,490
points of simulated data right so the

00:04:33,659 --> 00:04:37,889
algorithms and the systems have to adapt

00:04:35,490 --> 00:04:41,069
themselves to interact with its reality

00:04:37,889 --> 00:04:42,719
and treated as a more scarce source of

00:04:41,069 --> 00:04:44,699
information from which more learning has

00:04:42,719 --> 00:04:47,639
to be harvested and then of course the

00:04:44,699 --> 00:04:49,289
systems have to be fast because lot of

00:04:47,639 --> 00:04:53,279
transactions have to be managed every

00:04:49,289 --> 00:04:54,569
second another area where Microsoft is

00:04:53,279 --> 00:04:56,099
helping with reinforcement learning is

00:04:54,569 --> 00:04:58,620
an area of autonomous systems and there

00:04:56,099 --> 00:05:00,300
we do use simulations to train for

00:04:58,620 --> 00:05:02,039
example robotic controllers that are

00:05:00,300 --> 00:05:04,349
adaptive and things like that and once

00:05:02,039 --> 00:05:07,500
that model is training simulation it

00:05:04,349 --> 00:05:10,020
gets transferred over to physical robots

00:05:07,500 --> 00:05:11,400
that can can use all the learnings of

00:05:10,020 --> 00:05:14,490
all the corner cases that simulation

00:05:11,400 --> 00:05:17,370
expose them to in the real world so

00:05:14,490 --> 00:05:18,930
since you guys have done some RL what

00:05:17,370 --> 00:05:20,789
kind of empirical things have you

00:05:18,930 --> 00:05:23,580
learned about the challenges that RL

00:05:20,789 --> 00:05:26,689
closes that was so I would say that one

00:05:23,580 --> 00:05:29,039
of the biggest challenges is

00:05:26,689 --> 00:05:30,779
surprisingly that implemented in our

00:05:29,039 --> 00:05:33,240
elders days is a lot about user

00:05:30,779 --> 00:05:35,339
experience and design and understanding

00:05:33,240 --> 00:05:37,379
what you're trying to achieve you know

00:05:35,339 --> 00:05:40,589
when you have an algorithm that actually

00:05:37,379 --> 00:05:42,210
asks you I will achieve a goal that you

00:05:40,589 --> 00:05:44,370
have to tell me exactly how what we're

00:05:42,210 --> 00:05:46,080
doing against that goal how many times

00:05:44,370 --> 00:05:47,789
that trips up the business conversation

00:05:46,080 --> 00:05:49,919
right like because are you really

00:05:47,789 --> 00:05:51,509
pursuing the Kliq aren't you pursuing

00:05:49,919 --> 00:05:53,430
something else something a bit deeper

00:05:51,509 --> 00:05:55,020
right and deeper could mean like watch a

00:05:53,430 --> 00:05:57,389
few seconds of the video but is that

00:05:55,020 --> 00:05:59,279
really and how far away from the click

00:05:57,389 --> 00:06:02,089
do you want it can you get before the

00:05:59,279 --> 00:06:04,529
signal gets too noisy so one of the

00:06:02,089 --> 00:06:06,300
minoo the challenges is really to have

00:06:04,529 --> 00:06:08,270
an honest conversation about goals and

00:06:06,300 --> 00:06:12,110
measure ability of them

00:06:08,270 --> 00:06:14,479
then it comes to actually seeing your

00:06:12,110 --> 00:06:16,280
problem through their light over al this

00:06:14,479 --> 00:06:19,219
is why we're really focused first on

00:06:16,280 --> 00:06:20,870
doing applied user services because it

00:06:19,219 --> 00:06:22,370
can be very abstract otherwise it's like

00:06:20,870 --> 00:06:25,430
oh I gotta make decisions that get

00:06:22,370 --> 00:06:27,020
rewards and I'm gonna explore and how do

00:06:25,430 --> 00:06:29,449
I look at my own business problem for

00:06:27,020 --> 00:06:30,740
that light and a lot of people get

00:06:29,449 --> 00:06:32,270
tripped up in that so what you try to

00:06:30,740 --> 00:06:34,539
say look we're gonna draw a smaller box

00:06:32,270 --> 00:06:37,490
we're gonna say we're going to define

00:06:34,539 --> 00:06:39,349
personalization using RL as choose the

00:06:37,490 --> 00:06:41,270
right thing from a menu in a context and

00:06:39,349 --> 00:06:44,030
tell us how well it went and that's not

00:06:41,270 --> 00:06:45,620
the universe of possibility but 90% of

00:06:44,030 --> 00:06:47,930
their people can frame a part of the

00:06:45,620 --> 00:06:50,810
problem that way so if we can design a

00:06:47,930 --> 00:06:52,849
small box where people in it can have

00:06:50,810 --> 00:06:55,250
guaranteed results and we can tell you

00:06:52,849 --> 00:06:57,460
whether you fit in the box or not is a

00:06:55,250 --> 00:07:00,139
great way to get people started with RL

00:06:57,460 --> 00:07:02,719
so how does Microsoft help other

00:07:00,139 --> 00:07:04,190
companies work with RL well there's

00:07:02,719 --> 00:07:06,379
there's many things I mean first of all

00:07:04,190 --> 00:07:07,639
we ship products right and I think one

00:07:06,379 --> 00:07:09,680
of the nice things about the whole

00:07:07,639 --> 00:07:11,150
reinforcement learning ecosystem is that

00:07:09,680 --> 00:07:13,940
we have many groups that Microsoft

00:07:11,150 --> 00:07:17,449
working in RL and all of them are mixing

00:07:13,940 --> 00:07:19,370
advanced research with applied use so

00:07:17,449 --> 00:07:21,830
I'm gonna go into requesting in a second

00:07:19,370 --> 00:07:24,770
but one example is we have a gaming

00:07:21,830 --> 00:07:28,099
division where they're using RL to Train

00:07:24,770 --> 00:07:30,110
agents to play with your against you but

00:07:28,099 --> 00:07:31,849
not to win it's like they're they're

00:07:30,110 --> 00:07:34,879
actually rewarded on how much fun you're

00:07:31,849 --> 00:07:37,009
having right so how can we have like a

00:07:34,879 --> 00:07:38,870
soccer co-player or racecar you're

00:07:37,009 --> 00:07:41,300
racing against race in such a way that

00:07:38,870 --> 00:07:44,089
maximizes your fun right and that is

00:07:41,300 --> 00:07:45,289
like the sort of work that game studios

00:07:44,089 --> 00:07:47,539
can use when they're building they're

00:07:45,289 --> 00:07:49,250
you know playing agents there we also

00:07:47,539 --> 00:07:51,879
ship Enterprise Products you know other

00:07:49,250 --> 00:07:54,409
services that people can just call and

00:07:51,879 --> 00:07:56,360
invoke and you know make it easy for

00:07:54,409 --> 00:07:59,060
every developer to call I mean if you

00:07:56,360 --> 00:08:03,259
want to forget for a second about the AI

00:07:59,060 --> 00:08:04,969
glom or you know this service and we're

00:08:03,259 --> 00:08:08,150
talking about a particulars personalized

00:08:04,969 --> 00:08:08,839
ER it's a glorified sort by results you

00:08:08,150 --> 00:08:11,180
know you could sort a list

00:08:08,839 --> 00:08:13,430
alphabetically you could search and sort

00:08:11,180 --> 00:08:14,990
our list by date you could sort by

00:08:13,430 --> 00:08:16,940
results and as long as you tell it the

00:08:14,990 --> 00:08:18,680
results that it got with that sort it

00:08:16,940 --> 00:08:20,419
can keep learning and so we've made it

00:08:18,680 --> 00:08:20,840
really easy for developers just in an

00:08:20,419 --> 00:08:23,389
array of

00:08:20,840 --> 00:08:24,949
options and some JSON objects and sent

00:08:23,389 --> 00:08:26,830
us a number and train the model and we

00:08:24,949 --> 00:08:30,110
take care of the rest

00:08:26,830 --> 00:08:32,990
and then I think which is an underlying

00:08:30,110 --> 00:08:35,029
how of what we do all this is that we

00:08:32,990 --> 00:08:37,550
know that we are exploring new areas of

00:08:35,029 --> 00:08:39,800
where what does it mean to have an AI

00:08:37,550 --> 00:08:41,089
that is like learning my business rules

00:08:39,800 --> 00:08:43,250
what does it mean in terms of like

00:08:41,089 --> 00:08:44,779
business accountability what does it

00:08:43,250 --> 00:08:47,029
mean in terms of like safety if you're

00:08:44,779 --> 00:08:48,890
talking about physical systems right

00:08:47,029 --> 00:08:50,839
what sort of controls and monitors from

00:08:48,890 --> 00:08:55,160
the business from machine learning side

00:08:50,839 --> 00:08:57,230
doesn't need to have so one way we are

00:08:55,160 --> 00:08:59,420
producing results that everybody will be

00:08:57,230 --> 00:09:01,010
able to benefit from is that we're

00:08:59,420 --> 00:09:02,540
really taking an emphasis on co-creation

00:09:01,010 --> 00:09:04,580
of the product with customers so we're

00:09:02,540 --> 00:09:06,140
basically getting together on the kid in

00:09:04,580 --> 00:09:09,980
the kitchen working on the same side of

00:09:06,140 --> 00:09:12,980
the table and really testing out the

00:09:09,980 --> 00:09:14,839
concepts before putting out a product

00:09:12,980 --> 00:09:17,300
and or before even putting our toolkit

00:09:14,839 --> 00:09:20,240
saying do things we say no like you know

00:09:17,300 --> 00:09:22,400
what we actually walk the talk we

00:09:20,240 --> 00:09:25,010
actually run millions of dollars off

00:09:22,400 --> 00:09:27,740
worth of revenue on this personalization

00:09:25,010 --> 00:09:29,630
engine we are now sharing with you so

00:09:27,740 --> 00:09:31,250
having that proven practice while

00:09:29,630 --> 00:09:33,410
innovating with customers is an

00:09:31,250 --> 00:09:36,170
important how I guess of the process

00:09:33,410 --> 00:09:38,690
great so one of these days I'll bring up

00:09:36,170 --> 00:09:40,610
any new ethical fairness bias issues or

00:09:38,690 --> 00:09:41,089
is it the same ones that we all face no

00:09:40,610 --> 00:09:42,980
it's huge

00:09:41,089 --> 00:09:44,330
and actually the applied reinforcement

00:09:42,980 --> 00:09:45,980
learning product line is the first

00:09:44,330 --> 00:09:48,620
product line at Microsoft that has it as

00:09:45,980 --> 00:09:50,029
a shift criteria to have responsible use

00:09:48,620 --> 00:09:51,620
and ethical guidelines as part of

00:09:50,029 --> 00:09:54,470
product Docs so right next to with the

00:09:51,620 --> 00:09:56,270
API and the open and closing brackets of

00:09:54,470 --> 00:09:58,940
the JSON there's like responsible use

00:09:56,270 --> 00:10:00,709
and responsible use guidelines the way

00:09:58,940 --> 00:10:03,170
we see them are the sort of thing that

00:10:00,709 --> 00:10:05,000
can help those unaware of the

00:10:03,170 --> 00:10:07,130
consequences of what they're doing beam

00:10:05,000 --> 00:10:10,100
come are aware and help those that are

00:10:07,130 --> 00:10:12,860
aware of the consequences and have good

00:10:10,100 --> 00:10:14,750
intention ality have more backing and

00:10:12,860 --> 00:10:16,370
we're giving them materials to become

00:10:14,750 --> 00:10:17,870
stronger leaders we believe like

00:10:16,370 --> 00:10:19,010
individual voices count so we're trying

00:10:17,870 --> 00:10:22,640
to find those individuals that are

00:10:19,010 --> 00:10:24,050
willing to push the status quo and in

00:10:22,640 --> 00:10:26,680
reinforcement learning you get very

00:10:24,050 --> 00:10:29,920
specific questions about

00:10:26,680 --> 00:10:31,839
and of course in personalization to some

00:10:29,920 --> 00:10:33,399
of these things are like where is it

00:10:31,839 --> 00:10:36,579
reasonable to apply reinforcement

00:10:33,399 --> 00:10:40,509
learning is it where is it consequential

00:10:36,579 --> 00:10:42,689
to explore exploit should insurance

00:10:40,509 --> 00:10:46,389
policies be personalized in a webpage

00:10:42,689 --> 00:10:47,800
using reinforcement learning and what

00:10:46,389 --> 00:10:50,410
are the attributes I should drive that

00:10:47,800 --> 00:10:53,529
or is an algorithm trying to find out

00:10:50,410 --> 00:10:54,939
better ways that is not goals towards

00:10:53,529 --> 00:10:57,220
the purpose of insurance which is a

00:10:54,939 --> 00:10:59,949
long-term you know financially pool of

00:10:57,220 --> 00:11:01,660
risk and certain social safety net is it

00:10:59,949 --> 00:11:03,040
even ethical to apply to that sort of

00:11:01,660 --> 00:11:06,129
scenario so we have a guideline that

00:11:03,040 --> 00:11:08,649
says has discussions about when and what

00:11:06,129 --> 00:11:11,589
sort of use cases to apply to it has

00:11:08,649 --> 00:11:13,779
discussions about how reward systems

00:11:11,589 --> 00:11:15,189
good game results for you and you know

00:11:13,779 --> 00:11:16,540
that you will get what you asked for and

00:11:15,189 --> 00:11:18,970
we're not very good at asking for the

00:11:16,540 --> 00:11:20,769
right things in general so we try do

00:11:18,970 --> 00:11:23,110
like nuts folks into asking themselves

00:11:20,769 --> 00:11:25,209
the right questions what features are

00:11:23,110 --> 00:11:26,949
you willing to give to this VI about the

00:11:25,209 --> 00:11:28,929
ambient and about the choices such that

00:11:26,949 --> 00:11:31,029
it can make the good results and then

00:11:28,929 --> 00:11:35,230
things about how to for example in the

00:11:31,029 --> 00:11:37,480
area of personalization how to improve

00:11:35,230 --> 00:11:39,399
the level of dignity by which people's

00:11:37,480 --> 00:11:41,290
data is being used to train the I so

00:11:39,399 --> 00:11:43,839
like hey how can you make your business

00:11:41,290 --> 00:11:45,550
results improvement be something that

00:11:43,839 --> 00:11:46,870
actually goes back to the people that

00:11:45,550 --> 00:11:49,839
contributed the interactions that you

00:11:46,870 --> 00:11:52,059
learned from so and we that guideline is

00:11:49,839 --> 00:11:55,899
there and gets reviewed both by

00:11:52,059 --> 00:11:59,470
customers our field team and our ether

00:11:55,899 --> 00:12:00,699
comedian at Microsoft and and then it

00:11:59,470 --> 00:12:02,740
even starts before with the culture of

00:12:00,699 --> 00:12:04,899
the team we try to make it a non taboo

00:12:02,740 --> 00:12:06,639
topic to discuss these things have a

00:12:04,899 --> 00:12:08,639
diverse team where people with different

00:12:06,639 --> 00:12:11,379
viewpoints can share what they think and

00:12:08,639 --> 00:12:13,329
you know anybody can pull the Anton cord

00:12:11,379 --> 00:12:15,730
you know the good ol thing that stops

00:12:13,329 --> 00:12:19,240
the factory and if you have an ethical

00:12:15,730 --> 00:12:21,370
or responsible use concern you can stop

00:12:19,240 --> 00:12:23,170
the process and it's up to everybody

00:12:21,370 --> 00:12:24,790
else to justify why it should restart

00:12:23,170 --> 00:12:26,499
it's not up to you to justify why you

00:12:24,790 --> 00:12:28,660
stopped it so we take it very seriously

00:12:26,499 --> 00:12:30,279
because in the real world these

00:12:28,660 --> 00:12:32,410
decisions will have consequences

00:12:30,279 --> 00:12:33,250
oh great well thanks for such a complete

00:12:32,410 --> 00:12:35,920
cover

00:12:33,250 --> 00:12:37,600
a clearly a growing topic in the AI

00:12:35,920 --> 00:12:40,110
space and thanks for your time

00:12:37,600 --> 00:12:40,110
no thank you

00:12:46,550 --> 00:12:48,610

YouTube URL: https://www.youtube.com/watch?v=Qahd9poQvLs


