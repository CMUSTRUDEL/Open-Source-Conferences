Title: Computers are easy; people are hard - Bridget Kromhout (Pivotal)
Publication date: 2017-04-04
Playlist: O'Reilly Software Architecture Conference 2017 - New York, New York
Description: 
	Designing distributed systems means considering failure scenarios—both likely and less so. Will the network let you down? (Almost assuredly.) Will some portion of your IaaS misbehave? (Have you met computers?) We build in graceful degradation for much of our automation but often neglect the (just-as-essential) human interactions.

The classic “hard problems” of cache invalidation and naming things revolve around our understanding of what’s correct and true and our agreements with one another on scope and relevance. Communication is essential for making context-dependent decisions.

Whether we’re attempting to determine the current state of reality or distinguish logical boundaries, democratized observability is key to answering our questions. As the fractal complexity of our distributed systems grows, we need to mindfully choose practices that work with our tooling. You can’t buy a silver bullet, but you can forge one from the collaborative efforts of your team.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	00:00:02,200 --> 00:00:06,670
I think there's a much harder problem

00:00:04,150 --> 00:00:08,260
than eventual consistency even though

00:00:06,670 --> 00:00:11,049
eventual consistency is not your friend

00:00:08,260 --> 00:00:12,519
but there is a harder problem and it's I

00:00:11,049 --> 00:00:16,240
think it's fundamental to human nature

00:00:12,519 --> 00:00:18,400
like we're obviously right I mean I

00:00:16,240 --> 00:00:20,590
assume all of you are right at all times

00:00:18,400 --> 00:00:23,320
right but other people just need to

00:00:20,590 --> 00:00:28,810
understand that right yeah that that's

00:00:23,320 --> 00:00:31,690
kind of the problem and to get context

00:00:28,810 --> 00:00:35,130
for a complex system so that you can be

00:00:31,690 --> 00:00:38,050
right you need some kind of mental model

00:00:35,130 --> 00:00:41,170
this this idea of like the mapping the

00:00:38,050 --> 00:00:43,810
territory the whole quote goes the map

00:00:41,170 --> 00:00:46,150
is not the territory it represents but

00:00:43,810 --> 00:00:47,410
if correct and at that point you're

00:00:46,150 --> 00:00:49,630
laughing you're like uh-huh

00:00:47,410 --> 00:00:51,130
correct it has a similar structure to

00:00:49,630 --> 00:00:53,890
the territory which accounts for its

00:00:51,130 --> 00:00:55,570
usefulness and I like to think of this a

00:00:53,890 --> 00:00:57,610
lot when we're constructing our mental

00:00:55,570 --> 00:00:59,980
models for the the castles in the air

00:00:57,610 --> 00:01:03,399
and the when the systems in you know

00:00:59,980 --> 00:01:06,280
silicon that we're constructing like all

00:01:03,399 --> 00:01:10,030
abstractions are somewhat wrong that's

00:01:06,280 --> 00:01:11,889
just a reality and all abstractions are

00:01:10,030 --> 00:01:14,289
in all models all maps are going to have

00:01:11,889 --> 00:01:16,479
some kind of trade-off like I was just

00:01:14,289 --> 00:01:19,780
rewatching the West Wing this past fall

00:01:16,479 --> 00:01:23,229
for reasons and got to the big block of

00:01:19,780 --> 00:01:24,459
cheese episode where CJ was horrified by

00:01:23,229 --> 00:01:26,770
how much we don't know what's going on

00:01:24,459 --> 00:01:29,229
with Matt and it made me think about

00:01:26,770 --> 00:01:32,170
like how all maps are going to

00:01:29,229 --> 00:01:35,289
illustrate some points at the expense of

00:01:32,170 --> 00:01:36,429
others abstractions what you have to

00:01:35,289 --> 00:01:38,850
choose them they're not going to be

00:01:36,429 --> 00:01:38,850
perfect

00:01:44,600 --> 00:01:46,659

YouTube URL: https://www.youtube.com/watch?v=ch4mr2fPBcg


