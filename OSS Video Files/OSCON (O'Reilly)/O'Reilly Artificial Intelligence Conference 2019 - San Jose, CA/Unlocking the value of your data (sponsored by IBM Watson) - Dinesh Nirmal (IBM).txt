Title: Unlocking the value of your data (sponsored by IBM Watson) - Dinesh Nirmal (IBM)
Publication date: 2019-09-12
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Organizations recognize that data is what fuels digital transformation and are looking for new ways to unlock the value of their data and accelerate their journey to AI. Thatâ€™s why 80 percent of business leaders view AI as a strategic opportunity. Yet only 19 percent understand the data required for AI. Further, only 1 in 20 AI models make it to production and deliver any ROI. Simply put: There is no AI without information architecture (IA). Dinesh Nirmal examines how, with a unified, prescriptive information architecture, organizations can successfully unlock the value of their data for AI, as well as trust and control the business impact and risks of AI while coexisting in a multicloud world.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,089 --> 00:00:06,990
there's no doubt that AI is becoming a

00:00:04,290 --> 00:00:09,800
mainstream in our everyday life whether

00:00:06,990 --> 00:00:14,250
it's health care versus insurance or

00:00:09,800 --> 00:00:17,970
whether it's patient care but err models

00:00:14,250 --> 00:00:22,050
are making decisions every day but what

00:00:17,970 --> 00:00:24,930
if these models cannot be trusted what

00:00:22,050 --> 00:00:27,119
if these models are not ethical what if

00:00:24,930 --> 00:00:30,390
these models are not fair what if these

00:00:27,119 --> 00:00:31,920
models are biased so in the next 10

00:00:30,390 --> 00:00:33,630
minutes I want to take you through what

00:00:31,920 --> 00:00:36,480
are the steps involved in making these

00:00:33,630 --> 00:00:40,050
models trusted and I also want to show

00:00:36,480 --> 00:00:46,649
you sometimes bias exists in the most

00:00:40,050 --> 00:00:50,340
unexpected places so look at this 94% of

00:00:46,649 --> 00:00:53,070
the enterprises strongly believe that AI

00:00:50,340 --> 00:00:55,350
is critical for their success I don't

00:00:53,070 --> 00:01:02,100
know who the other 6% is but let me tell

00:00:55,350 --> 00:01:05,420
you they have to embrace AI but 76% also

00:01:02,100 --> 00:01:09,240
believe that lack of transparency is

00:01:05,420 --> 00:01:11,520
impeding that the option of AI think

00:01:09,240 --> 00:01:14,820
about it if you're an enterprise if

00:01:11,520 --> 00:01:17,070
you're a bank you denied a loan how do

00:01:14,820 --> 00:01:20,330
you explain it to the regulators explain

00:01:17,070 --> 00:01:22,830
ability becomes huge in these AI models

00:01:20,330 --> 00:01:25,380
next I want to take you through it's a

00:01:22,830 --> 00:01:28,290
video about a minute video where IBM

00:01:25,380 --> 00:01:32,340
worked with Wimbledon to go through

00:01:28,290 --> 00:01:36,450
hundreds of hours of data to come up

00:01:32,340 --> 00:01:41,520
with 15 seconds of highlights or

00:01:36,450 --> 00:01:44,790
snippets where the players are shown but

00:01:41,520 --> 00:01:48,000
not only that how a I helped you build

00:01:44,790 --> 00:01:50,070
those little snippets in very short

00:01:48,000 --> 00:01:54,360
period of time but I also want to show

00:01:50,070 --> 00:01:56,850
you that bias exists in the most

00:01:54,360 --> 00:01:59,670
unexpected places so take a look at this

00:01:56,850 --> 00:02:00,440
video technology is changing the way we

00:01:59,670 --> 00:02:02,480
views

00:02:00,440 --> 00:02:04,490
Wimbledon through their partnership with

00:02:02,480 --> 00:02:07,190
IBM is at the cutting edge of this

00:02:04,490 --> 00:02:09,950
technology thanks to groundbreaking AI

00:02:07,190 --> 00:02:11,990
systems using IBM Watson the best

00:02:09,950 --> 00:02:15,260
highlights from multiple matches a

00:02:11,990 --> 00:02:17,510
captured simultaneously with over 18

00:02:15,260 --> 00:02:19,760
courts and up to four matches per court

00:02:17,510 --> 00:02:21,950
each day hundreds of hours of footage

00:02:19,760 --> 00:02:24,260
are produced which would ordinarily take

00:02:21,950 --> 00:02:26,900
a team of editors a huge amount of time

00:02:24,260 --> 00:02:29,180
to compile into highlights packages as

00:02:26,900 --> 00:02:31,580
the AI highlight system continuously

00:02:29,180 --> 00:02:33,770
tracks the action with ranks every point

00:02:31,580 --> 00:02:37,100
highlights packages are created within

00:02:33,770 --> 00:02:39,350
two minutes of a match finishing by

00:02:37,100 --> 00:02:41,780
watching player reactions listening to

00:02:39,350 --> 00:02:44,360
crowd excitement levels and analyzing

00:02:41,780 --> 00:02:45,770
the gameplay statistics IBM Watson

00:02:44,360 --> 00:02:55,610
enables we're willing to deliver

00:02:45,770 --> 00:02:58,010
unmissable moments without delay so in

00:02:55,610 --> 00:02:59,780
the past it all used to be done manually

00:02:58,010 --> 00:03:01,820
think about it someone sitting in front

00:02:59,780 --> 00:03:05,239
going through hundreds of hours of

00:03:01,820 --> 00:03:07,900
footage picking 15-second snippets but

00:03:05,239 --> 00:03:11,180
now AI has been able to move much faster

00:03:07,900 --> 00:03:14,120
but the second aspect of it is that I

00:03:11,180 --> 00:03:16,400
want to also show you how bias exists in

00:03:14,120 --> 00:03:17,870
the most unexpected places so for

00:03:16,400 --> 00:03:20,030
example you know there's structured and

00:03:17,870 --> 00:03:22,459
unstructured static and dynamic data

00:03:20,030 --> 00:03:25,070
that we took we created features like

00:03:22,459 --> 00:03:27,590
player rankings crowd size crowd cheer

00:03:25,070 --> 00:03:30,350
venue and we were able to build these

00:03:27,590 --> 00:03:32,989
models but what we realized is that in

00:03:30,350 --> 00:03:35,930
all these models cases they were picking

00:03:32,989 --> 00:03:38,989
the snippets or the highlights of the

00:03:35,930 --> 00:03:41,930
top-ranking players it was always the

00:03:38,989 --> 00:03:44,930
case so we were able to run it through

00:03:41,930 --> 00:03:47,300
what's enough open scale and be able to

00:03:44,930 --> 00:03:50,150
use bias mitigation software to say how

00:03:47,300 --> 00:03:52,190
do we bring lesser ranking players to

00:03:50,150 --> 00:03:54,500
the forefront if they had such a great

00:03:52,190 --> 00:03:56,630
shot or is such a great game to show

00:03:54,500 --> 00:03:58,940
their heart light so here's an example

00:03:56,630 --> 00:04:01,850
where Wimbledon dashboard where we were

00:03:58,940 --> 00:04:03,980
able to you know show Sentaro who's not

00:04:01,850 --> 00:04:05,720
a top-ranked player but he had some

00:04:03,980 --> 00:04:08,420
great shots that day and we were able to

00:04:05,720 --> 00:04:11,900
bring it to the top of the dashboard but

00:04:08,420 --> 00:04:13,690
it also enables us to expose the

00:04:11,900 --> 00:04:17,380
up-and-coming players so

00:04:13,690 --> 00:04:19,209
which is a huge side product of it but

00:04:17,380 --> 00:04:23,170
then the question comes what does it

00:04:19,209 --> 00:04:27,310
take to trust a decision made by an AI

00:04:23,170 --> 00:04:29,290
model how do we trust the decision of an

00:04:27,310 --> 00:04:32,170
AI model I think there's four pillars to

00:04:29,290 --> 00:04:34,630
it it has to be explainable no question

00:04:32,170 --> 00:04:36,820
if a loan was denied we need to explain

00:04:34,630 --> 00:04:40,810
why that Sloan was denied it has to be

00:04:36,820 --> 00:04:43,660
fair meaning there cannot be any bias it

00:04:40,810 --> 00:04:45,730
has to be accurate meaning the model

00:04:43,660 --> 00:04:46,360
drifting happens how do we detect and

00:04:45,730 --> 00:04:48,970
correct it

00:04:46,360 --> 00:04:51,040
and last not but least it has to be open

00:04:48,970 --> 00:04:54,670
and let me take it 30 seconds and

00:04:51,040 --> 00:04:57,910
explain what I mean by open I truly

00:04:54,670 --> 00:05:01,870
believe that AI is the biggest

00:04:57,910 --> 00:05:04,320
undertaking we are going to take in our

00:05:01,870 --> 00:05:07,450
lifetime and for generations to come and

00:05:04,320 --> 00:05:09,040
if a single company or an enterprise is

00:05:07,450 --> 00:05:11,260
going to hold this algorithm close to

00:05:09,040 --> 00:05:14,650
their chest and not make it available

00:05:11,260 --> 00:05:17,740
will miserably fail so it has to be a

00:05:14,650 --> 00:05:20,800
movement it has to be open it has to be

00:05:17,740 --> 00:05:23,250
community driven that's the only way we

00:05:20,800 --> 00:05:25,690
are going to make eliminate bias

00:05:23,250 --> 00:05:29,650
unethical practices from these error

00:05:25,690 --> 00:05:32,410
models so let me explain what explain

00:05:29,650 --> 00:05:35,710
ability means look here it's a real

00:05:32,410 --> 00:05:39,580
customer example where a bank denied the

00:05:35,710 --> 00:05:41,650
loan and we show the three areas why we

00:05:39,580 --> 00:05:43,540
denied the loan but that's not the most

00:05:41,650 --> 00:05:46,900
important thing I mean you know offer

00:05:43,540 --> 00:05:49,090
like lime can do that but what I call it

00:05:46,900 --> 00:05:52,510
the contrastive explanation which means

00:05:49,090 --> 00:05:56,470
not only we can explain why the loan was

00:05:52,510 --> 00:05:59,530
denied but we can also explain how we

00:05:56,470 --> 00:06:01,660
could change the outcome so in this case

00:05:59,530 --> 00:06:03,970
the outcome could have been changed if

00:06:01,660 --> 00:06:06,040
there was more money in the savings I

00:06:03,970 --> 00:06:07,840
mean what a great way to explain to a

00:06:06,040 --> 00:06:11,470
regulator this is why the loan was

00:06:07,840 --> 00:06:13,150
denied but also to the consumer now you

00:06:11,470 --> 00:06:15,040
can take the case and say you had a

00:06:13,150 --> 00:06:16,810
little more money in savings the loan

00:06:15,040 --> 00:06:20,800
could have been approved that's what I

00:06:16,810 --> 00:06:22,150
call contrastive explanations fair son

00:06:20,800 --> 00:06:24,250
another example these are all real

00:06:22,150 --> 00:06:26,200
customer examples that we work with

00:06:24,250 --> 00:06:26,780
customers so in this case you know the

00:06:26,200 --> 00:06:29,060
bank had

00:06:26,780 --> 00:06:31,520
threshold of female and male as you can

00:06:29,060 --> 00:06:34,160
see the female line is running

00:06:31,520 --> 00:06:35,990
consistently under the threshold so you

00:06:34,160 --> 00:06:36,919
want to dig deeper so you want to find

00:06:35,990 --> 00:06:39,830
out what happened

00:06:36,919 --> 00:06:42,800
you can see females were given only 70%

00:06:39,830 --> 00:06:46,880
of the time loans but males were given

00:06:42,800 --> 00:06:49,310
80% of the time now you as an individual

00:06:46,880 --> 00:06:52,460
as the bank officer you want to make

00:06:49,310 --> 00:06:55,520
correct it on the fly but you can also

00:06:52,460 --> 00:06:57,290
you know get an updated model from open

00:06:55,520 --> 00:06:59,330
scale but you want to do it yourself

00:06:57,290 --> 00:07:01,669
so how do you correct it you can correct

00:06:59,330 --> 00:07:04,040
it on the fly by altering the outcome

00:07:01,669 --> 00:07:07,190
based on the features that you have used

00:07:04,040 --> 00:07:09,260
so the fairness or bias detection not

00:07:07,190 --> 00:07:12,800
only can be detected but it can be

00:07:09,260 --> 00:07:15,290
corrected using software three accurate

00:07:12,800 --> 00:07:18,350
I mean we all know model stripped as

00:07:15,290 --> 00:07:22,160
data drifts so in this case as you can

00:07:18,350 --> 00:07:24,650
see there is two lines one is the data

00:07:22,160 --> 00:07:27,530
being interchanging and the model

00:07:24,650 --> 00:07:29,030
drifting so you want to know why why is

00:07:27,530 --> 00:07:32,419
that happening so in this particular

00:07:29,030 --> 00:07:34,250
case the bank had run a marketing

00:07:32,419 --> 00:07:37,760
campaign which was for a particular age

00:07:34,250 --> 00:07:40,160
group but over time a different age

00:07:37,760 --> 00:07:42,740
group started applying for the loan so

00:07:40,160 --> 00:07:44,930
the data changed and the model drifted

00:07:42,740 --> 00:07:47,750
you are able to detect it but now you

00:07:44,930 --> 00:07:49,610
can correct it but you also know why the

00:07:47,750 --> 00:07:51,919
data is changing so you can use a

00:07:49,610 --> 00:07:54,280
different training set now to update the

00:07:51,919 --> 00:07:54,280
model

00:07:54,450 --> 00:08:02,340
open like I said to me it's very

00:07:58,590 --> 00:08:05,850
critical it's very important that we

00:08:02,340 --> 00:08:08,640
keep this movement very open that we

00:08:05,850 --> 00:08:10,800
contribute back and we consume and we

00:08:08,640 --> 00:08:14,930
work all together to make sure that the

00:08:10,800 --> 00:08:19,410
models we build are fair are ethical and

00:08:14,930 --> 00:08:19,830
are accurate so in this case you know we

00:08:19,410 --> 00:08:23,340
built

00:08:19,830 --> 00:08:25,440
what's an open scale but we also

00:08:23,340 --> 00:08:27,360
contribute back into the community so if

00:08:25,440 --> 00:08:30,450
you look at it you know you take a I

00:08:27,360 --> 00:08:32,880
explained ability 360 you take a are

00:08:30,450 --> 00:08:35,660
fairness 360 you take the robustness

00:08:32,880 --> 00:08:37,950
toolkit we have contributed it back

00:08:35,660 --> 00:08:39,300
making sure that we are contributing it

00:08:37,950 --> 00:08:42,150
back into the community we are working

00:08:39,300 --> 00:08:44,100
with many customers many enterprises to

00:08:42,150 --> 00:08:46,470
make sure the models they build are

00:08:44,100 --> 00:08:48,870
using what's an open scale to make sure

00:08:46,470 --> 00:08:52,470
it's fair it's explainable it's not

00:08:48,870 --> 00:08:54,630
unethical so it becomes much more

00:08:52,470 --> 00:08:57,030
important for all of us to be part of

00:08:54,630 --> 00:08:59,700
this moment not just one vendor or one

00:08:57,030 --> 00:09:03,570
enterprise so if you want to learn more

00:08:59,700 --> 00:09:06,950
about open scale and what we are doing

00:09:03,570 --> 00:09:10,380
from making models more trustworthy

00:09:06,950 --> 00:09:13,290
please join us at our booth we also have

00:09:10,380 --> 00:09:15,330
a session at 11 o'clock today where

00:09:13,290 --> 00:09:17,910
rojan is going to give much more deep

00:09:15,330 --> 00:09:19,800
dive into it but thank you for taking

00:09:17,910 --> 00:09:21,090
the time and really appreciate your time

00:09:19,800 --> 00:09:24,169
thank you

00:09:21,090 --> 00:09:24,169
[Applause]

00:09:29,070 --> 00:09:31,130

YouTube URL: https://www.youtube.com/watch?v=JKp3RY0za18


