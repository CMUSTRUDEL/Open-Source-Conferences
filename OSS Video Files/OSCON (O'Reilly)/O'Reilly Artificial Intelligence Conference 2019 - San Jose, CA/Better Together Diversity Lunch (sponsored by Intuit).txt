Title: Better Together Diversity Lunch (sponsored by Intuit)
Publication date: 2019-11-25
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Opening remarks: Tracy Stone, Intuit. Tracy is the global leader for Tech Women @ Intuit initiative.

"AI will change the world. Who will change AI?", Amy Chou is the corporate partnerships manager at AI4ALL.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:01,579 --> 00:00:10,389
hi everyone welcome all right we're

00:00:08,450 --> 00:00:12,650
gonna get started with the program

00:00:10,389 --> 00:00:14,779
sounds like there are a lot of great

00:00:12,650 --> 00:00:16,609
discussions going on which is part of

00:00:14,779 --> 00:00:19,490
the reason why we have these diversity

00:00:16,609 --> 00:00:21,920
lunches so thank you very much my name

00:00:19,490 --> 00:00:23,540
is Suzanne Axtell and I lead many of the

00:00:21,920 --> 00:00:26,539
diversity initiatives at O'Reilly Media

00:00:23,540 --> 00:00:27,830
so excited to have you all here this is

00:00:26,539 --> 00:00:30,170
such an important topic that we're

00:00:27,830 --> 00:00:32,840
discussing and really glad that you're

00:00:30,170 --> 00:00:34,880
here in supporting it I also wanted to

00:00:32,840 --> 00:00:37,640
say how very grateful we are to Intuit

00:00:34,880 --> 00:00:39,670
for sponsoring not just this lunch but

00:00:37,640 --> 00:00:42,110
for their leadership in the area of

00:00:39,670 --> 00:00:43,760
inclusion and diversity in the tech

00:00:42,110 --> 00:00:45,260
community it's one of the critical

00:00:43,760 --> 00:00:47,350
issues that we're facing now so we

00:00:45,260 --> 00:00:49,850
really appreciate their leadership there

00:00:47,350 --> 00:00:51,710
um one of the things that we were very

00:00:49,850 --> 00:00:52,820
passionate about at O'Reilly is

00:00:51,710 --> 00:00:55,329
spreading the knowledge of innovators

00:00:52,820 --> 00:00:58,219
and this includes raising the visibility

00:00:55,329 --> 00:01:00,530
ideas and perspectives of people from

00:00:58,219 --> 00:01:02,930
underrepresented groups so from speaking

00:01:00,530 --> 00:01:04,879
at conferences to creating content for

00:01:02,930 --> 00:01:08,119
our online learning platform we want to

00:01:04,879 --> 00:01:09,679
help everyone all of you share your

00:01:08,119 --> 00:01:11,299
expertise with the world so if you'd

00:01:09,679 --> 00:01:13,159
like more information about that

00:01:11,299 --> 00:01:15,049
including speaking in a future

00:01:13,159 --> 00:01:16,670
conference which I hope that you all are

00:01:15,049 --> 00:01:17,840
thinking about doing if you're not

00:01:16,670 --> 00:01:20,029
already doing it

00:01:17,840 --> 00:01:22,009
please come say hi to me visit the

00:01:20,029 --> 00:01:25,819
O'Reilly booth in the expo hall or visit

00:01:22,009 --> 00:01:27,289
a widely com / Diversity and then before

00:01:25,819 --> 00:01:28,399
I introduce our first speaker I just

00:01:27,289 --> 00:01:30,679
want to point out we do have a

00:01:28,399 --> 00:01:34,159
microphone here in the room and we'll

00:01:30,679 --> 00:01:35,990
have a Q&A or open discussion after the

00:01:34,159 --> 00:01:43,789
program so please be thinking about your

00:01:35,990 --> 00:01:45,409
questions now and so I am really pleased

00:01:43,789 --> 00:01:47,179
to introduce our first speaker as I

00:01:45,409 --> 00:01:48,889
mentioned in - it is a very big leader

00:01:47,179 --> 00:01:50,990
and I'm supporter of diversity and

00:01:48,889 --> 00:01:54,049
inclusion so we're very happy to have

00:01:50,990 --> 00:01:56,240
Tracy stone with us today Tracy is the

00:01:54,049 --> 00:01:59,359
global leader for tech woman at Intuit

00:01:56,240 --> 00:02:01,399
initiative she has over 15 years of

00:01:59,359 --> 00:02:04,520
experience in high-tech companies in

00:02:01,399 --> 00:02:07,729
various roles driving large-scale tragic

00:02:04,520 --> 00:02:09,890
programs projects and teams she is a

00:02:07,729 --> 00:02:12,770
co-founder of girls spark which is a

00:02:09,890 --> 00:02:14,960
girls science plus arithmetic Club which

00:02:12,770 --> 00:02:15,380
is dedicated to increasing the number of

00:02:14,960 --> 00:02:17,420
girls

00:02:15,380 --> 00:02:19,610
supposed to stem and she is passionate

00:02:17,420 --> 00:02:22,280
about boning women in technology and

00:02:19,610 --> 00:02:36,380
inspiring and encouraging young girls in

00:02:22,280 --> 00:02:39,170
stem welcome Tracy Thank You Suzanne

00:02:36,380 --> 00:02:42,140
thank you for having me here today I'm

00:02:39,170 --> 00:02:45,020
thrilled to be here and I just want to

00:02:42,140 --> 00:02:47,900
do a quick introduction for those of you

00:02:45,020 --> 00:02:51,590
who are not familiar with Intuit into it

00:02:47,900 --> 00:02:53,120
is a global financial platform where

00:02:51,590 --> 00:02:56,870
we're focused on our mission of

00:02:53,120 --> 00:02:59,990
prosperity of powering prosperity around

00:02:56,870 --> 00:03:01,550
the globe so I hope you have an

00:02:59,990 --> 00:03:03,440
opportunity as you're here at the

00:03:01,550 --> 00:03:07,040
conference to learn more about Intuit we

00:03:03,440 --> 00:03:08,740
have a number of sessions that are

00:03:07,040 --> 00:03:11,930
happening later today

00:03:08,740 --> 00:03:14,120
and so as Susan mentioned I'm Tracy

00:03:11,930 --> 00:03:17,150
stone why have the wonderful opportunity

00:03:14,120 --> 00:03:19,880
to lead the tech women into an

00:03:17,150 --> 00:03:22,010
initiative at Intuit and what it is is

00:03:19,880 --> 00:03:24,230
it's a global initiative focused on how

00:03:22,010 --> 00:03:28,250
we attract and recruit retain and

00:03:24,230 --> 00:03:29,840
advance women and technical roles and I

00:03:28,250 --> 00:03:32,210
love that I get to do this as Susan

00:03:29,840 --> 00:03:33,770
Susan mentioned my background is I've

00:03:32,210 --> 00:03:35,650
worked in I have a engineering

00:03:33,770 --> 00:03:38,210
background worked in tech for many years

00:03:35,650 --> 00:03:39,560
left the workforce was at home for a

00:03:38,210 --> 00:03:43,760
while and then started a non-profit

00:03:39,560 --> 00:03:46,190
focused on encouraging inspiring young

00:03:43,760 --> 00:03:47,930
girls in STEM and then when I was coming

00:03:46,190 --> 00:03:49,550
back into the corporate world I came

00:03:47,930 --> 00:03:52,760
across this role and I'm just thrilled

00:03:49,550 --> 00:03:55,040
to have this opportunity to be part of

00:03:52,760 --> 00:04:00,550
our efforts to increase diversity in

00:03:55,040 --> 00:04:03,640
tech and for our future generations so

00:04:00,550 --> 00:04:06,650
why I love us so one of the things about

00:04:03,640 --> 00:04:10,250
diversity is that that I love about it

00:04:06,650 --> 00:04:12,800
is that it's not just a moral a moral

00:04:10,250 --> 00:04:14,660
imperative for our companies but I love

00:04:12,800 --> 00:04:17,750
that it gives the companies the

00:04:14,660 --> 00:04:19,970
opportunity to increase their business

00:04:17,750 --> 00:04:22,280
outcomes or have an impulsive impact on

00:04:19,970 --> 00:04:24,890
their business outcomes and so there's

00:04:22,280 --> 00:04:27,440
many studies that show by having a

00:04:24,890 --> 00:04:29,349
diverse workforce in tech it can

00:04:27,440 --> 00:04:32,750
increase the companies over

00:04:29,349 --> 00:04:35,660
innovation productivity and in fact the

00:04:32,750 --> 00:04:37,900
bottom line and it into it one of the

00:04:35,660 --> 00:04:40,880
things that we are that's built into

00:04:37,900 --> 00:04:43,340
Intuit and into our DNA is this idea

00:04:40,880 --> 00:04:45,919
around customer obsession and so we

00:04:43,340 --> 00:04:47,389
innovate based on our customers needs we

00:04:45,919 --> 00:04:50,419
we love to fall in love with our

00:04:47,389 --> 00:04:53,000
customers problems and to do that when

00:04:50,419 --> 00:04:55,340
you think about it to truly get into the

00:04:53,000 --> 00:04:57,110
minds of our customers and and know what

00:04:55,340 --> 00:04:59,569
they need in their products now and in

00:04:57,110 --> 00:05:01,610
the future we have to be able to have

00:04:59,569 --> 00:05:04,610
our workforce represent our diverse

00:05:01,610 --> 00:05:07,130
customers or reflect our mirror ID our

00:05:04,610 --> 00:05:09,380
diverse customers so I think it's really

00:05:07,130 --> 00:05:11,870
neat how our diversity aligns with our

00:05:09,380 --> 00:05:16,970
our passion around customer obsession

00:05:11,870 --> 00:05:20,659
and an innovation so thinking about

00:05:16,970 --> 00:05:23,780
diversity and an AI I think that it's

00:05:20,659 --> 00:05:25,460
it's essential to think about as we

00:05:23,780 --> 00:05:27,470
think about all the advances and the

00:05:25,460 --> 00:05:32,320
innovations happening today and in the

00:05:27,470 --> 00:05:36,620
AI space the opportunity for our

00:05:32,320 --> 00:05:39,320
products to make decisions what without

00:05:36,620 --> 00:05:41,719
human intervention is awesome to think

00:05:39,320 --> 00:05:45,229
about and for us to think about how

00:05:41,719 --> 00:05:47,599
important is for for to have that

00:05:45,229 --> 00:05:49,580
diverse perspective as our we're

00:05:47,599 --> 00:05:53,780
building that those products with the

00:05:49,580 --> 00:05:56,479
with the AI so into its thinking about

00:05:53,780 --> 00:05:58,969
this as we were using a eyes as an our

00:05:56,479 --> 00:06:01,759
vision or mission of powering prosperity

00:05:58,969 --> 00:06:04,430
we're using AI to think about how we can

00:06:01,759 --> 00:06:08,690
help customers save money how we can

00:06:04,430 --> 00:06:10,580
enable access to capital and manage help

00:06:08,690 --> 00:06:13,099
our customers manage their cash cash

00:06:10,580 --> 00:06:16,340
flow so it's really important that we

00:06:13,099 --> 00:06:18,889
have those diverse perspectives in our

00:06:16,340 --> 00:06:21,650
data science scientists and our teams as

00:06:18,889 --> 00:06:25,389
we're developing those future products

00:06:21,650 --> 00:06:27,500
and building AI into our future products

00:06:25,389 --> 00:06:31,759
diversity it into it's a key priority

00:06:27,500 --> 00:06:35,120
for us it's built into our DNA it's just

00:06:31,759 --> 00:06:40,339
how we we think about both our customers

00:06:35,120 --> 00:06:42,770
and our workforce right now into its

00:06:40,339 --> 00:06:43,190
really pleased to know that to have our

00:06:42,770 --> 00:06:46,420
work

00:06:43,190 --> 00:06:48,860
for some tech workforce where we have

00:06:46,420 --> 00:06:52,420
higher than industry average for women

00:06:48,860 --> 00:06:56,830
and technical roles we also have over

00:06:52,420 --> 00:06:59,200
32% of our leaders are female which is

00:06:56,830 --> 00:07:00,800
wonderful to see not just that their

00:06:59,200 --> 00:07:04,310
representations there but they're also

00:07:00,800 --> 00:07:07,820
in rolls of influence and we're looking

00:07:04,310 --> 00:07:10,580
at diversity broader than gender so of

00:07:07,820 --> 00:07:15,050
underrepresented minorities as well as

00:07:10,580 --> 00:07:19,700
other factors and in terms of how we

00:07:15,050 --> 00:07:21,500
think about diversity so through all of

00:07:19,700 --> 00:07:22,970
our initiatives that into it as we're

00:07:21,500 --> 00:07:24,590
looking at how we can increase our

00:07:22,970 --> 00:07:27,200
representation and increase our

00:07:24,590 --> 00:07:31,160
diversity and really build it into how

00:07:27,200 --> 00:07:33,320
we think about our development and how

00:07:31,160 --> 00:07:35,300
we want to think about our customer a

00:07:33,320 --> 00:07:36,800
customer obsession and how we develop

00:07:35,300 --> 00:07:39,860
products that are meaningful for our

00:07:36,800 --> 00:07:43,370
customers I hope that you think I hope

00:07:39,860 --> 00:07:44,960
that you are enjoy this conversation an

00:07:43,370 --> 00:07:46,490
opportunity today to bring us all

00:07:44,960 --> 00:07:48,620
together to really elevate the

00:07:46,490 --> 00:07:51,770
conversation on diversity diversity and

00:07:48,620 --> 00:07:54,470
tech diversity in AI and I'd love for

00:07:51,770 --> 00:07:56,390
you if you want to learn more about our

00:07:54,470 --> 00:07:58,550
initiatives our products our initiatives

00:07:56,390 --> 00:08:00,010
and what we're doing and the diversity

00:07:58,550 --> 00:08:03,710
space feel free to reach out to me

00:08:00,010 --> 00:08:06,890
connect with me on LinkedIn also we have

00:08:03,710 --> 00:08:10,070
I want to call out one of our senior

00:08:06,890 --> 00:08:13,100
vice president's broth Kodama is our SVP

00:08:10,070 --> 00:08:14,570
of tech futures so if you want to learn

00:08:13,100 --> 00:08:17,960
more about our products and what we're

00:08:14,570 --> 00:08:19,790
doing around all of our innovations and

00:08:17,960 --> 00:08:22,280
a I love for you to connect with fur off

00:08:19,790 --> 00:08:23,630
and then we have at the back table back

00:08:22,280 --> 00:08:24,880
there industry harsh a can raise your

00:08:23,630 --> 00:08:27,740
hand if you want to learn more about

00:08:24,880 --> 00:08:29,990
Intuit diversity our products are

00:08:27,740 --> 00:08:32,599
opportunities what life is like it into

00:08:29,990 --> 00:08:35,930
it feel free to connect with us back

00:08:32,599 --> 00:08:38,180
there as well and I hope you have a

00:08:35,930 --> 00:08:41,390
wonderful lunch enjoy the enjoy the talk

00:08:38,180 --> 00:08:44,120
and we look forward to just engaging in

00:08:41,390 --> 00:08:47,150
a conversation with all of you and all

00:08:44,120 --> 00:08:51,080
together our efforts as we work to

00:08:47,150 --> 00:08:57,060
increase diversity and tech so thank you

00:08:51,080 --> 00:08:59,620
[Applause]

00:08:57,060 --> 00:09:00,790
awesome thank you so much Tracy that's

00:08:59,620 --> 00:09:03,220
great

00:09:00,790 --> 00:09:05,440
I also wanted to say thanks to everyone

00:09:03,220 --> 00:09:07,450
who contributed to AI for all when you

00:09:05,440 --> 00:09:09,790
registered for the conference we really

00:09:07,450 --> 00:09:15,580
appreciate your support of this very

00:09:09,790 --> 00:09:17,830
worthwhile organization and so here to

00:09:15,580 --> 00:09:20,130
tell us a bit more about that and the

00:09:17,830 --> 00:09:22,779
great things that they're doing is Amy

00:09:20,130 --> 00:09:25,089
Chu Amy is the corporate partnership

00:09:22,779 --> 00:09:26,680
manager at AI for all they foster a

00:09:25,089 --> 00:09:28,660
diverse pipeline to artificial

00:09:26,680 --> 00:09:29,860
intelligence careers so something that's

00:09:28,660 --> 00:09:32,170
probably important to a lot of us

00:09:29,860 --> 00:09:35,080
previously Amy was the customer

00:09:32,170 --> 00:09:37,180
solutions manager at clever an IDI tech

00:09:35,080 --> 00:09:39,670
startup that aims to make it easier to

00:09:37,180 --> 00:09:41,560
bring technology to k-12 classrooms and

00:09:39,670 --> 00:09:43,600
she led due diligence projects on

00:09:41,560 --> 00:09:46,420
mergers and acquisition deals at a on a

00:09:43,600 --> 00:09:48,670
global financial services firm amy holds

00:09:46,420 --> 00:09:50,800
a BA in sociology a BS in business

00:09:48,670 --> 00:09:52,630
administration and an MBA all from the

00:09:50,800 --> 00:09:54,910
University of California Berkeley

00:09:52,630 --> 00:09:57,490
amy is also a founding board member of

00:09:54,910 --> 00:09:59,260
camp common ground which is a Bay Area

00:09:57,490 --> 00:10:00,970
leadership camp committed to building

00:09:59,260 --> 00:10:02,920
community between racially and

00:10:00,970 --> 00:10:12,190
economically diverse middle school

00:10:02,920 --> 00:10:14,440
students so welcome Amy Thank You

00:10:12,190 --> 00:10:18,030
Suzanne for the warm introduction I'm

00:10:14,440 --> 00:10:20,860
gonna move this down because I am down

00:10:18,030 --> 00:10:22,900
thank you for sharing about all the

00:10:20,860 --> 00:10:25,450
wonderful initiatives that are really

00:10:22,900 --> 00:10:27,130
seeing women in leadership roles which

00:10:25,450 --> 00:10:30,130
is really important into it it's really

00:10:27,130 --> 00:10:32,200
exciting to learn about that and so as I

00:10:30,130 --> 00:10:34,150
meant as Suzanne mentioned I'm at AI for

00:10:32,200 --> 00:10:36,160
all which I'll hopefully all of you I'm

00:10:34,150 --> 00:10:37,930
going to assume all of you donated to in

00:10:36,160 --> 00:10:41,080
addition with your tickets so thank you

00:10:37,930 --> 00:10:43,660
so very much for that your that donation

00:10:41,080 --> 00:10:46,180
really goes a long way into increasing

00:10:43,660 --> 00:10:47,800
diversity inclusion and AI and so I'll

00:10:46,180 --> 00:10:50,610
talk about that today but I'll also talk

00:10:47,800 --> 00:10:53,709
about what you can do in your workplaces

00:10:50,610 --> 00:10:56,920
to really ensure a more ethical and

00:10:53,709 --> 00:10:59,110
inclusive future for AI and because this

00:10:56,920 --> 00:11:02,050
Tracey mentioned we're definitely seeing

00:10:59,110 --> 00:11:03,680
AI touching all industries in all of our

00:11:02,050 --> 00:11:05,839
lives

00:11:03,680 --> 00:11:08,329
so just to start a little bit love a

00:11:05,839 --> 00:11:10,339
show of hands of how many people were

00:11:08,329 --> 00:11:15,740
really excited about the possibilities

00:11:10,339 --> 00:11:17,480
that AI holds okay great how many of you

00:11:15,740 --> 00:11:19,360
have some concerns about some of the

00:11:17,480 --> 00:11:22,009
headlines that we're seeing around

00:11:19,360 --> 00:11:23,689
biases and algorithms and things like

00:11:22,009 --> 00:11:25,399
that these are not mutually exclusive

00:11:23,689 --> 00:11:28,189
questions right you can be really

00:11:25,399 --> 00:11:31,009
excited and also concerned so definitely

00:11:28,189 --> 00:11:33,050
feel free to raise your hand twice great

00:11:31,009 --> 00:11:35,059
that's really exciting to to hear that

00:11:33,050 --> 00:11:36,529
not only are people excited but people

00:11:35,059 --> 00:11:40,550
understand there are some concerns that

00:11:36,529 --> 00:11:43,490
we really want to talk about so just to

00:11:40,550 --> 00:11:45,860
level set on where we are today

00:11:43,490 --> 00:11:49,040
these are recent statistics around the

00:11:45,860 --> 00:11:53,209
black female representation in academia

00:11:49,040 --> 00:11:55,069
in industry in AI specifically so we're

00:11:53,209 --> 00:11:57,290
seeing women dramatically

00:11:55,069 --> 00:11:59,899
underrepresented in the field today and

00:11:57,290 --> 00:12:04,670
of course that those numbers look even

00:11:59,899 --> 00:12:06,559
more just dismal I don't think there's

00:12:04,670 --> 00:12:08,749
another word for it when you layer in

00:12:06,559 --> 00:12:11,889
other element of under-representation

00:12:08,749 --> 00:12:17,839
whether that's race sexual orientation

00:12:11,889 --> 00:12:19,220
or socioeconomic status right so I just

00:12:17,839 --> 00:12:21,290
want to take this time to ground

00:12:19,220 --> 00:12:23,809
ourselves in the reality that we're

00:12:21,290 --> 00:12:26,089
seeing and in addition to this lack of

00:12:23,809 --> 00:12:28,779
diversity we're also seeing this

00:12:26,089 --> 00:12:33,170
tremendous potential that AI has right

00:12:28,779 --> 00:12:36,740
we're seeing some estimates that by 2030

00:12:33,170 --> 00:12:39,050
it'll increase GDP by fifteen point

00:12:36,740 --> 00:12:42,620
seven trillion and impact all of these

00:12:39,050 --> 00:12:46,939
industries so we're seeing AI being

00:12:42,620 --> 00:12:49,249
increasingly used to delegate to to make

00:12:46,939 --> 00:12:53,149
life-changing decisions like who gets

00:12:49,249 --> 00:12:55,189
alone who gets parole who gets a job and

00:12:53,149 --> 00:12:59,139
we're also seeing existing societal

00:12:55,189 --> 00:13:02,829
biases creep into these decision-making

00:12:59,139 --> 00:13:05,089
systems that are often automated and not

00:13:02,829 --> 00:13:09,110
humans are often taken out of the loop

00:13:05,089 --> 00:13:12,399
right and why we think just matters that

00:13:09,110 --> 00:13:15,379
yeah I for all is because fundamentally

00:13:12,399 --> 00:13:18,720
we're seeing tools that are being

00:13:15,379 --> 00:13:21,750
created by a homogenous group of

00:13:18,720 --> 00:13:26,010
and we're seeing the impacts of that in

00:13:21,750 --> 00:13:28,410
our everyday lives so we won't talk a

00:13:26,010 --> 00:13:30,300
little bit more about AI for all and -

00:13:28,410 --> 00:13:31,950
so those of you who are a little bit

00:13:30,300 --> 00:13:33,330
less familiar with our programs can have

00:13:31,950 --> 00:13:34,350
a deeper understanding of the work that

00:13:33,330 --> 00:13:36,930
we do today

00:13:34,350 --> 00:13:39,209
but fundamentally AI for all we really

00:13:36,930 --> 00:13:42,899
believe that by increasing diverse

00:13:39,209 --> 00:13:45,290
perspectives we will solve more more

00:13:42,899 --> 00:13:48,120
pressing problems in more creative ways

00:13:45,290 --> 00:13:50,279
and so we have three programs to do that

00:13:48,120 --> 00:13:52,140
we have our AI for all open learning

00:13:50,279 --> 00:13:53,940
program which we launched in April this

00:13:52,140 --> 00:13:57,000
year we already have hundreds of active

00:13:53,940 --> 00:13:59,130
users it is free open online so you can

00:13:57,000 --> 00:14:00,690
definitely sign up and check it out but

00:13:59,130 --> 00:14:02,820
it is geared specifically for high

00:14:00,690 --> 00:14:05,940
school students so it's to make AI

00:14:02,820 --> 00:14:07,830
approachable for and relevant for high

00:14:05,940 --> 00:14:09,540
school students who may be learning

00:14:07,830 --> 00:14:14,190
about this technology and this material

00:14:09,540 --> 00:14:16,770
for the first time we also run summer

00:14:14,190 --> 00:14:18,540
programs at leading AI universities

00:14:16,770 --> 00:14:20,610
across North America we just wrapped up

00:14:18,540 --> 00:14:24,360
this summer and we were in 11

00:14:20,610 --> 00:14:26,670
universities this summer which was

00:14:24,360 --> 00:14:30,329
incredibly exciting but we work with

00:14:26,670 --> 00:14:33,089
leading researchers to to bring

00:14:30,329 --> 00:14:36,360
underrepresented populations to their

00:14:33,089 --> 00:14:38,520
campuses and learn about the ethical and

00:14:36,360 --> 00:14:41,760
societal implications of AI in addition

00:14:38,520 --> 00:14:43,680
to the technical applications of AI and

00:14:41,760 --> 00:14:46,649
we know that just because you have a

00:14:43,680 --> 00:14:49,020
transformative summer experience in high

00:14:46,649 --> 00:14:51,420
school does not mean you'll go to and

00:14:49,020 --> 00:14:53,520
through careers in college are - with

00:14:51,420 --> 00:14:55,290
their careers in the eye so we do have a

00:14:53,520 --> 00:14:57,209
lifelong alumni program that's

00:14:55,290 --> 00:14:59,279
completely free to the families who

00:14:57,209 --> 00:15:00,930
participate in our summer programs and

00:14:59,279 --> 00:15:02,670
that's really to make sure that we

00:15:00,930 --> 00:15:06,870
continue to support them and engage them

00:15:02,670 --> 00:15:08,760
as they learn - as they learn as they

00:15:06,870 --> 00:15:11,579
develop their own leadership and careers

00:15:08,760 --> 00:15:14,430
in AI two of our students recently

00:15:11,579 --> 00:15:16,589
interviewed bill gates at the human

00:15:14,430 --> 00:15:22,290
centered a I launch for Stanford

00:15:16,589 --> 00:15:24,270
University and I think the most exciting

00:15:22,290 --> 00:15:26,400
thing about my work is that every day I

00:15:24,270 --> 00:15:28,890
get to see the potential of young people

00:15:26,400 --> 00:15:30,570
to make a difference in AI and I'll

00:15:28,890 --> 00:15:33,600
start with four of our young alumni

00:15:30,570 --> 00:15:35,880
today we are first programs for in 2015

00:15:33,600 --> 00:15:37,830
so our alumni are fairly young they're

00:15:35,880 --> 00:15:40,380
in high school and in their early

00:15:37,830 --> 00:15:42,720
college stages but we're already seeing

00:15:40,380 --> 00:15:45,390
them making incredible impacts in AI

00:15:42,720 --> 00:15:48,960
today and I'll start with Ishika who's

00:15:45,390 --> 00:15:51,720
on the top right she is a college

00:15:48,960 --> 00:15:53,460
freshman and she recently won $40,000

00:15:51,720 --> 00:15:56,520
which is more than almost more than I

00:15:53,460 --> 00:15:59,640
made coming out of college for inventing

00:15:56,520 --> 00:16:01,320
a 3d printed device that attaches to

00:15:59,640 --> 00:16:03,780
smart phones to better detect blood

00:16:01,320 --> 00:16:06,030
diseases and she did this because she

00:16:03,780 --> 00:16:08,820
was really inspired by all the promise

00:16:06,030 --> 00:16:11,880
that AI held and learned about different

00:16:08,820 --> 00:16:15,630
applications of AI and really sees this

00:16:11,880 --> 00:16:17,820
as a way to bring access to cutting-edge

00:16:15,630 --> 00:16:21,380
technology in places where medical

00:16:17,820 --> 00:16:23,670
diagnosis tools may not be as advanced

00:16:21,380 --> 00:16:25,710
next if we're going clockwise as

00:16:23,670 --> 00:16:29,250
Stephanie Stephanie is a high school

00:16:25,710 --> 00:16:32,070
senior in Salinas California she went to

00:16:29,250 --> 00:16:34,020
our Stanford program and was really

00:16:32,070 --> 00:16:36,240
inspired to use machine learning

00:16:34,020 --> 00:16:38,850
techniques to research water

00:16:36,240 --> 00:16:41,190
contamination from fertilizer runoff in

00:16:38,850 --> 00:16:44,730
her community and this is something that

00:16:41,190 --> 00:16:48,140
deeply impacts her in her home and her

00:16:44,730 --> 00:16:52,890
families because of where she lives

00:16:48,140 --> 00:16:54,480
Beca on the bottom bottom right she is a

00:16:52,890 --> 00:16:56,610
college freshman and she was really

00:16:54,480 --> 00:16:57,650
inspired by Michelle Alexander's book

00:16:56,610 --> 00:17:00,840
the new Jim Crow

00:16:57,650 --> 00:17:03,360
and what she learned about AI to really

00:17:00,840 --> 00:17:05,730
understand how can machine learning

00:17:03,360 --> 00:17:07,470
techniques be used to positively impact

00:17:05,730 --> 00:17:09,329
the criminal justice face but also

00:17:07,470 --> 00:17:12,240
understanding what's happening today and

00:17:09,329 --> 00:17:15,510
she did research around around those

00:17:12,240 --> 00:17:19,800
effects and last but not least Imogen

00:17:15,510 --> 00:17:22,079
who's on the bottom left not me but

00:17:19,800 --> 00:17:28,050
she's a college sophomore right now and

00:17:22,079 --> 00:17:29,820
she used she wrote a paper on using

00:17:28,050 --> 00:17:32,310
machine learning techniques to give

00:17:29,820 --> 00:17:34,020
surgeons feedback on their technique in

00:17:32,310 --> 00:17:36,840
conjunction with PhD students at

00:17:34,020 --> 00:17:39,690
Stanford and that paper actually won one

00:17:36,840 --> 00:17:42,150
of the best papers at Naropa and she did

00:17:39,690 --> 00:17:44,610
this as a high school student so again

00:17:42,150 --> 00:17:48,450
we're seeing our students

00:17:44,610 --> 00:17:50,280
invent life-changing technology do

00:17:48,450 --> 00:17:52,860
really cutting-edge research winning

00:17:50,280 --> 00:17:56,789
award-winning research and interning at

00:17:52,860 --> 00:17:59,789
top companies we have about 500 alumni

00:17:56,789 --> 00:18:01,559
globally right now and 85% of them are

00:17:59,789 --> 00:18:06,000
currently in high school and this is

00:18:01,559 --> 00:18:08,340
what they're doing today to make make

00:18:06,000 --> 00:18:10,620
the world a better place really for all

00:18:08,340 --> 00:18:13,500
of us and I think we firmly believe by

00:18:10,620 --> 00:18:18,090
increasing the amount or the diversity

00:18:13,500 --> 00:18:20,010
in talent that we have in society we can

00:18:18,090 --> 00:18:22,710
really see better fresher outcomes for

00:18:20,010 --> 00:18:25,230
all of us and really see the potential

00:18:22,710 --> 00:18:31,140
for AI to benefit to reach its potential

00:18:25,230 --> 00:18:33,090
that we're all so excited about so I

00:18:31,140 --> 00:18:35,700
want to also talk about what you can do

00:18:33,090 --> 00:18:37,260
in your work to to make sure that we're

00:18:35,700 --> 00:18:39,960
moving towards a more ethical and

00:18:37,260 --> 00:18:41,940
inclusive future in AI I think we we

00:18:39,960 --> 00:18:44,669
talk a lot about testing monitoring and

00:18:41,940 --> 00:18:47,150
auditing in a really systematic enteral

00:18:44,669 --> 00:18:49,590
way so as you're building your products

00:18:47,150 --> 00:18:51,419
creating processes that are really

00:18:49,590 --> 00:18:52,770
embedded in them to test for the

00:18:51,419 --> 00:18:55,289
fairness of the tools that you're

00:18:52,770 --> 00:18:57,299
building there are a lot of open source

00:18:55,289 --> 00:19:00,630
tools out there including IBM's fairness

00:18:57,299 --> 00:19:03,059
tool and others we also think it's

00:19:00,630 --> 00:19:05,850
really important to have fairness and

00:19:03,059 --> 00:19:08,010
ethics standards at your companies a lot

00:19:05,850 --> 00:19:10,770
of third-party organizations like I

00:19:08,010 --> 00:19:12,210
Triple E partnership on a are taking the

00:19:10,770 --> 00:19:13,770
leadership on creating some of these

00:19:12,210 --> 00:19:16,620
frameworks that you can leverage in your

00:19:13,770 --> 00:19:18,929
company as you're developing tools that

00:19:16,620 --> 00:19:21,270
will be used by large populations I mean

00:19:18,929 --> 00:19:25,440
may or may not be represented in your

00:19:21,270 --> 00:19:27,150
company transparency and explain ability

00:19:25,440 --> 00:19:28,559
of the algorithms and the products that

00:19:27,150 --> 00:19:31,580
you are building as well is really

00:19:28,559 --> 00:19:34,409
important encouraging questions that

00:19:31,580 --> 00:19:35,789
might challenge your methodology might

00:19:34,409 --> 00:19:38,190
challenge the data sets that you're

00:19:35,789 --> 00:19:39,900
using but really making sure that people

00:19:38,190 --> 00:19:42,270
can understand how these systems are

00:19:39,900 --> 00:19:46,110
being built and having more eyes on that

00:19:42,270 --> 00:19:49,559
will create better better outcomes for

00:19:46,110 --> 00:19:50,820
us all and last but not least in because

00:19:49,559 --> 00:19:52,799
it's the topic of the lunch and

00:19:50,820 --> 00:19:54,780
something that you know we all care

00:19:52,799 --> 00:19:57,159
deeply about is increasing diversity

00:19:54,780 --> 00:20:00,070
inclusion at your company

00:19:57,159 --> 00:20:01,840
so I will dive in a little deeper around

00:20:00,070 --> 00:20:04,450
the different ways you can do that I

00:20:01,840 --> 00:20:07,149
think one way is to just open a door for

00:20:04,450 --> 00:20:09,369
a young person in AI I'm giving you a

00:20:07,149 --> 00:20:12,220
lot of examples about our alumni who

00:20:09,369 --> 00:20:13,749
have made incredible strides none of

00:20:12,220 --> 00:20:15,909
them have done that without the help of

00:20:13,749 --> 00:20:20,499
an adult right like none of them none of

00:20:15,909 --> 00:20:22,989
them succeeded because of as smart and

00:20:20,499 --> 00:20:27,309
as driven as they are without the help

00:20:22,989 --> 00:20:28,779
of an adult either you know hiring them

00:20:27,309 --> 00:20:30,849
for an internship even though they were

00:20:28,779 --> 00:20:33,279
in high school and being unclear how did

00:20:30,849 --> 00:20:34,840
you under 18 people into their company

00:20:33,279 --> 00:20:35,649
someone pushed for that to make these

00:20:34,840 --> 00:20:39,279
things happen

00:20:35,649 --> 00:20:43,059
you can amplify under-represented voices

00:20:39,279 --> 00:20:45,429
in the AI excuse me so if you are

00:20:43,059 --> 00:20:48,729
speaking at a carpenter and you notice

00:20:45,429 --> 00:20:50,139
that you are the only or there aren't

00:20:48,729 --> 00:20:52,299
very many women or other

00:20:50,139 --> 00:20:54,399
underrepresented minority speaking you

00:20:52,299 --> 00:20:56,349
can say something to the organizer you

00:20:54,399 --> 00:21:00,849
can open your networks you can really

00:20:56,349 --> 00:21:04,570
encourage people to to open doors in

00:21:00,849 --> 00:21:06,070
spaces that you have access to you can

00:21:04,570 --> 00:21:08,859
share what you're learning today about

00:21:06,070 --> 00:21:11,080
AI for all and use that as a mechanism

00:21:08,859 --> 00:21:14,109
to start conversations about why this is

00:21:11,080 --> 00:21:16,409
important you can also think more

00:21:14,109 --> 00:21:18,849
critically about how to make your

00:21:16,409 --> 00:21:20,590
culture more inclusive from you know

00:21:18,849 --> 00:21:23,970
who's taking notes at meetings all the

00:21:20,590 --> 00:21:26,950
time who's responsible for a lot of the

00:21:23,970 --> 00:21:29,139
unsung hero moments of making sure

00:21:26,950 --> 00:21:32,139
everything works and how can you make

00:21:29,139 --> 00:21:35,049
that more equitable and and looking

00:21:32,139 --> 00:21:37,659
critically at the processes of hiring

00:21:35,049 --> 00:21:40,179
and retention and promotion at your own

00:21:37,659 --> 00:21:42,549
companies and last I would ask you to

00:21:40,179 --> 00:21:44,440
challenge assumptions I think how many

00:21:42,549 --> 00:21:46,869
of you before today thought that high

00:21:44,440 --> 00:21:49,629
school students could invent AI tools

00:21:46,869 --> 00:21:51,519
could do award-winning research could do

00:21:49,629 --> 00:21:54,220
research that will benefit our

00:21:51,519 --> 00:21:56,379
communities as a whole and when you're

00:21:54,220 --> 00:21:58,450
hearing that you know we need a PhD

00:21:56,379 --> 00:22:00,249
student to do this work or we need

00:21:58,450 --> 00:22:01,809
someone with a master's

00:22:00,249 --> 00:22:03,429
maybe challenge that assumption and

00:22:01,809 --> 00:22:05,739
challenge other assumptions of why that

00:22:03,429 --> 00:22:07,510
is what are those skill sets that you're

00:22:05,739 --> 00:22:11,350
actually looking for

00:22:07,510 --> 00:22:17,500
and how can you be creative about how to

00:22:11,350 --> 00:22:19,300
increase access so that was it I just

00:22:17,500 --> 00:22:21,670
want to leave with you know an

00:22:19,300 --> 00:22:23,130
invitation to continue the conversation

00:22:21,670 --> 00:22:25,990
and with this quote from one of our

00:22:23,130 --> 00:22:29,080
alumni at stamp from one of our Stanford

00:22:25,990 --> 00:22:32,500
programs but I think it really captures

00:22:29,080 --> 00:22:35,410
the the potential that we see for AI to

00:22:32,500 --> 00:22:38,380
really be used towards for all of

00:22:35,410 --> 00:22:39,910
humanity's benefit but I think we do

00:22:38,380 --> 00:22:42,280
have to take steps to make sure that

00:22:39,910 --> 00:22:44,770
it's a more inclusive and as ethical

00:22:42,280 --> 00:22:46,780
future and we need everyone in this room

00:22:44,770 --> 00:22:48,490
to do that so thank you very much for

00:22:46,780 --> 00:22:51,090
your time thank you so much for donating

00:22:48,490 --> 00:22:54,190
to AI for all as well with your tickets

00:22:51,090 --> 00:22:56,320
it's just been really touching to see

00:22:54,190 --> 00:22:57,820
how much support we've had from the AI

00:22:56,320 --> 00:23:08,500
community for the work that we've been

00:22:57,820 --> 00:23:09,910
doing so thank you thank you so much

00:23:08,500 --> 00:23:11,710
Andy so as I mentioned we have a

00:23:09,910 --> 00:23:14,230
microphone right here if anyone would

00:23:11,710 --> 00:23:15,850
like to come up and have questions we do

00:23:14,230 --> 00:23:19,870
have some time for that or throw open a

00:23:15,850 --> 00:23:21,580
topic for discussion and I I do have one

00:23:19,870 --> 00:23:24,970
so we've heard a lot of really great

00:23:21,580 --> 00:23:26,260
ideas for supporting inclusion and

00:23:24,970 --> 00:23:29,250
diversity in work but if you had to

00:23:26,260 --> 00:23:32,020
choose just one just a first step or

00:23:29,250 --> 00:23:34,360
maybe what's one thing you could do to

00:23:32,020 --> 00:23:47,830
be a better Ally as an individual what

00:23:34,360 --> 00:23:51,460
what would that what would that be or

00:23:47,830 --> 00:23:53,350
the mics on are the mics on okay so I

00:23:51,460 --> 00:23:56,410
think my biggest ask is obviously that

00:23:53,350 --> 00:23:59,230
you hire our alumni I know that's a big

00:23:56,410 --> 00:24:01,300
ask but I think I just truly believe in

00:23:59,230 --> 00:24:03,670
the talent that they bring to the table

00:24:01,300 --> 00:24:05,740
and would love for everyone here to have

00:24:03,670 --> 00:24:09,940
access to that but in terms of what you

00:24:05,740 --> 00:24:11,770
can do I think it's really just being

00:24:09,940 --> 00:24:15,270
open to breaking down barriers through

00:24:11,770 --> 00:24:18,400
honest dialogue and questions and not

00:24:15,270 --> 00:24:20,590
dismissing people if they don't have a

00:24:18,400 --> 00:24:22,539
technical background I think and you

00:24:20,590 --> 00:24:23,799
there are a lot of technical spaces

00:24:22,539 --> 00:24:25,870
we're like oh you don't understand this

00:24:23,799 --> 00:24:27,659
you cannot possibly understand how I

00:24:25,870 --> 00:24:31,419
built this model it's very complicated

00:24:27,659 --> 00:24:34,419
don't worry about it and I think if I

00:24:31,419 --> 00:24:37,210
could ask one thing is that you you

00:24:34,419 --> 00:24:39,100
break down that norm and really being

00:24:37,210 --> 00:24:40,840
open to sharing like yeah let's have a

00:24:39,100 --> 00:24:42,370
conversation let me figure out how I can

00:24:40,840 --> 00:24:45,669
explain this to you in a way that makes

00:24:42,370 --> 00:24:50,529
sense to you whether you have a PhD in

00:24:45,669 --> 00:24:53,230
CS or not wonderful

00:24:50,529 --> 00:24:57,249
can you hear the mic yeah wonderful so I

00:24:53,230 --> 00:24:59,529
think my biggest single one thing that I

00:24:57,249 --> 00:25:02,409
would say just is I think we all have to

00:24:59,529 --> 00:25:03,850
be intentional about it and not rely on

00:25:02,409 --> 00:25:05,320
good intentions and think that it'll

00:25:03,850 --> 00:25:08,080
just happen I think it has to be

00:25:05,320 --> 00:25:09,700
something we build into what we do and I

00:25:08,080 --> 00:25:11,970
think this is really hard in the

00:25:09,700 --> 00:25:14,919
corporate world because we're driven by

00:25:11,970 --> 00:25:16,600
profits and speed and productivity and

00:25:14,919 --> 00:25:19,509
you may not you may try to think of it

00:25:16,600 --> 00:25:22,119
just gets added on after the fact and I

00:25:19,509 --> 00:25:24,549
think when when that happens it doesn't

00:25:22,119 --> 00:25:26,289
it doesn't occur doesn't that you don't

00:25:24,549 --> 00:25:28,360
see the progress so I think it's really

00:25:26,289 --> 00:25:30,389
important for organizations to really be

00:25:28,360 --> 00:25:33,249
intentional about it and build it into

00:25:30,389 --> 00:25:36,279
how we operate our businesses in terms

00:25:33,249 --> 00:25:48,909
of how we attract talent retain and

00:25:36,279 --> 00:25:51,399
advance our talent all right I'm dr.

00:25:48,909 --> 00:25:53,860
cheeks I have a program called steaming

00:25:51,399 --> 00:25:57,669
global citizenship the nonprofit is

00:25:53,860 --> 00:25:59,710
strong ties but my question if you go

00:25:57,669 --> 00:26:04,389
back not that picture but the picture

00:25:59,710 --> 00:26:09,100
before that showed three different

00:26:04,389 --> 00:26:11,649
pictures of your students go back that

00:26:09,100 --> 00:26:14,379
one okay so the middle picture is

00:26:11,649 --> 00:26:18,820
typical of most programs in this country

00:26:14,379 --> 00:26:22,240
in that you may see one token of black

00:26:18,820 --> 00:26:27,149
and everything else is something other

00:26:22,240 --> 00:26:29,409
than that and so what are you doing to

00:26:27,149 --> 00:26:31,000
aligning with what you said in terms of

00:26:29,409 --> 00:26:34,610
intention what are you doing to do

00:26:31,000 --> 00:26:36,830
intentional reach to particularly groups

00:26:34,610 --> 00:26:38,960
that can change that picture and the

00:26:36,830 --> 00:26:40,730
reason that I'm so passionate about that

00:26:38,960 --> 00:26:43,850
because I work with black and brown

00:26:40,730 --> 00:26:45,980
communities and oftentimes we don't even

00:26:43,850 --> 00:26:48,230
get the news we don't even hear about

00:26:45,980 --> 00:26:51,409
the programs so what kind of alliances

00:26:48,230 --> 00:26:54,169
and partnerships are you creating to

00:26:51,409 --> 00:26:58,370
ensure that programs like myself that

00:26:54,169 --> 00:26:59,840
may be small and medium depending on

00:26:58,370 --> 00:27:01,610
what you think is small and medium

00:26:59,840 --> 00:27:06,049
because I do work around the globe as

00:27:01,610 --> 00:27:08,960
well but making sure that students who

00:27:06,049 --> 00:27:12,470
may never hear the word who may never

00:27:08,960 --> 00:27:17,269
see a person like you in their community

00:27:12,470 --> 00:27:19,309
but there are people passionate and also

00:27:17,269 --> 00:27:21,169
there is industry in their community

00:27:19,309 --> 00:27:23,960
that they can leverage the talented

00:27:21,169 --> 00:27:27,200
talent to make sure that if you give

00:27:23,960 --> 00:27:29,049
them the tools or show them this is

00:27:27,200 --> 00:27:32,120
who's doing this in their community are

00:27:29,049 --> 00:27:35,450
what are you doing to address that big

00:27:32,120 --> 00:27:37,070
gap and I'm gonna sit down well they

00:27:35,450 --> 00:27:41,389
thank you so much for I think being

00:27:37,070 --> 00:27:42,769
really critical I just wanna say I

00:27:41,389 --> 00:27:44,630
really appreciate you being really

00:27:42,769 --> 00:27:46,190
critical about when we are saying we're

00:27:44,630 --> 00:27:49,010
increasing diversity what does that

00:27:46,190 --> 00:27:52,070
really mean and that program that

00:27:49,010 --> 00:27:53,750
picture in the middle is from our Simon

00:27:52,070 --> 00:27:56,210
Fraser University program which is in

00:27:53,750 --> 00:27:58,309
Canada and the picture that you saw at

00:27:56,210 --> 00:28:01,940
the end was at our University of

00:27:58,309 --> 00:28:03,799
Maryland program and I think we we are

00:28:01,940 --> 00:28:05,950
intentionally expanding to universities

00:28:03,799 --> 00:28:08,149
in geographic locations that have larger

00:28:05,950 --> 00:28:12,110
african-american and Latino populations

00:28:08,149 --> 00:28:14,269
so focusing on the south and the and the

00:28:12,110 --> 00:28:17,240
southern states and as well as the

00:28:14,269 --> 00:28:19,309
southern Atlantic states to increase

00:28:17,240 --> 00:28:22,029
that and also we're working with

00:28:19,309 --> 00:28:24,440
partners that really can support

00:28:22,029 --> 00:28:26,029
increasing that representation in all of

00:28:24,440 --> 00:28:29,510
our programs so we partner with teals

00:28:26,029 --> 00:28:32,330
for example Microsoft's program to bring

00:28:29,510 --> 00:28:34,669
more of black and Latino voices and to

00:28:32,330 --> 00:28:35,779
our programs but it's something that we

00:28:34,669 --> 00:28:37,490
are intentionally doing with our

00:28:35,779 --> 00:28:40,279
expansion and we do partner with

00:28:37,490 --> 00:28:42,380
specific schools and other organizations

00:28:40,279 --> 00:28:43,880
to make sure that the outreach is there

00:28:42,380 --> 00:28:46,070
and that we're building relationships

00:28:43,880 --> 00:28:48,200
with people who already have credibility

00:28:46,070 --> 00:28:50,690
in those communities

00:28:48,200 --> 00:28:52,850
to make sure that their students are

00:28:50,690 --> 00:28:55,460
aware of these programs and that they're

00:28:52,850 --> 00:28:57,170
open to them as well and I would love to

00:28:55,460 --> 00:28:58,730
continue the conversation with you

00:28:57,170 --> 00:29:01,309
afterwards and we can talk about how we

00:28:58,730 --> 00:29:03,440
can continue to share the programs more

00:29:01,309 --> 00:29:13,549
broadly as well did I answer your

00:29:03,440 --> 00:29:16,880
question or do you know so the first

00:29:13,549 --> 00:29:21,830
graph you show the first notes was the

00:29:16,880 --> 00:29:24,890
disparity of women in AI yes and the

00:29:21,830 --> 00:29:29,330
numbers look horrible yes when you dice

00:29:24,890 --> 00:29:34,760
them up yes into black Asian white and

00:29:29,330 --> 00:29:36,500
all these other stuff right okay I'm not

00:29:34,760 --> 00:29:39,350
being critical of what you're doing I

00:29:36,500 --> 00:29:41,809
just want us to think about this okay so

00:29:39,350 --> 00:29:43,760
universities predominantly white

00:29:41,809 --> 00:29:46,309
universities all right

00:29:43,760 --> 00:29:48,080
hello friends okay predominantly white

00:29:46,309 --> 00:29:50,030
universities the numbers for

00:29:48,080 --> 00:29:52,490
african-americans for instance I was at

00:29:50,030 --> 00:29:55,840
Arizona State University I was the only

00:29:52,490 --> 00:29:58,730
african-american in the ph.d program

00:29:55,840 --> 00:30:00,440
right now I work with students there as

00:29:58,730 --> 00:30:05,330
undergraduate students I think there are

00:30:00,440 --> 00:30:07,700
like three so if your strategy is to

00:30:05,330 --> 00:30:09,830
align yourself with universities the

00:30:07,700 --> 00:30:12,830
mirror effect would probably be the

00:30:09,830 --> 00:30:15,710
outcome that is exist today so we really

00:30:12,830 --> 00:30:18,380
need to broaden how are we aligning

00:30:15,710 --> 00:30:21,440
ourselves with organizations that can

00:30:18,380 --> 00:30:24,799
scale out and that are more receptive

00:30:21,440 --> 00:30:29,260
unlike traditional approaches that's all

00:30:24,799 --> 00:30:29,260
yeah and you're doing a great job okay

00:30:30,010 --> 00:30:34,700
that is also partly why we want the open

00:30:33,080 --> 00:30:36,500
learning program which is online and

00:30:34,700 --> 00:30:37,640
available for more community groups and

00:30:36,500 --> 00:30:39,860
we're working intentionally with

00:30:37,640 --> 00:30:42,559
different community groups to bring that

00:30:39,860 --> 00:30:44,419
outside of the university programs but

00:30:42,559 --> 00:30:45,500
something that is important for us the

00:30:44,419 --> 00:30:48,679
reason we partner with these

00:30:45,500 --> 00:30:51,620
universities is so that faculty and

00:30:48,679 --> 00:30:54,260
staff at these leading AI universities

00:30:51,620 --> 00:30:56,150
can see a different type of student

00:30:54,260 --> 00:30:58,429
demographics than they're used to seeing

00:30:56,150 --> 00:31:00,830
an envision a different future for what

00:30:58,429 --> 00:31:02,510
is possible in their departments

00:31:00,830 --> 00:31:04,610
so you know as I said this at the

00:31:02,510 --> 00:31:06,320
beginning of my talk it'll take all of

00:31:04,610 --> 00:31:08,090
us in this room and many many more I

00:31:06,320 --> 00:31:10,160
know that we're not going to do it alone

00:31:08,090 --> 00:31:11,990
and thank you so much for encouraging us

00:31:10,160 --> 00:31:16,300
to build deep partnerships with people

00:31:11,990 --> 00:31:16,300
who can support our work thank you

00:31:18,080 --> 00:31:24,650
I have a two-part question and a lot of

00:31:21,260 --> 00:31:26,990
the Abe magnets and community outreach

00:31:24,650 --> 00:31:30,500
programs to see it targeted to high

00:31:26,990 --> 00:31:33,860
school students undergrad students but

00:31:30,500 --> 00:31:35,690
what are you guys doing for stem schools

00:31:33,860 --> 00:31:38,350
at a lower level for example

00:31:35,690 --> 00:31:41,240
I'm mother of a 10 year old who her

00:31:38,350 --> 00:31:43,910
broadest understanding of AI or Compu or

00:31:41,240 --> 00:31:46,040
anything is what she gets off of YouTube

00:31:43,910 --> 00:31:48,560
but she's interested in coding and I

00:31:46,040 --> 00:31:51,890
have no resources even if she's enrolled

00:31:48,560 --> 00:31:54,470
at a stem school which has no compute or

00:31:51,890 --> 00:31:56,600
anything like that so where are programs

00:31:54,470 --> 00:31:58,610
like this or what can I do as a person

00:31:56,600 --> 00:32:00,920
that's very interactive in the Bay Area

00:31:58,610 --> 00:32:04,850
school districts to bring something like

00:32:00,920 --> 00:32:06,830
this to more middle school you know

00:32:04,850 --> 00:32:09,890
where they have so many ideas and have

00:32:06,830 --> 00:32:12,620
nowhere to go with them and then second

00:32:09,890 --> 00:32:15,920
I look at a lot of these summer programs

00:32:12,620 --> 00:32:18,500
and a lot of these particularly ting

00:32:15,920 --> 00:32:21,140
universities in the Bay Area again what

00:32:18,500 --> 00:32:22,670
can I do as maybe somebody that works in

00:32:21,140 --> 00:32:25,280
the tech sector to kind of bridge the

00:32:22,670 --> 00:32:29,300
gap as some of that is not technical I'm

00:32:25,280 --> 00:32:31,850
an EA by trade but I'm interested in

00:32:29,300 --> 00:32:33,830
trig by what AI can do and I'm my

00:32:31,850 --> 00:32:36,350
daughter's looking at me to see how my

00:32:33,830 --> 00:32:38,030
career path and how my education

00:32:36,350 --> 00:32:40,490
brandishes this and you've made a good

00:32:38,030 --> 00:32:42,260
point about breaking down the barrier of

00:32:40,490 --> 00:32:44,810
not being technical and allow somebody

00:32:42,260 --> 00:32:46,910
like me to step into it so what can I do

00:32:44,810 --> 00:32:48,980
to bridge that gap in my community and

00:32:46,910 --> 00:32:55,490
with like my children and people who are

00:32:48,980 --> 00:32:58,010
watching me well I'd love to hear your

00:32:55,490 --> 00:32:59,840
response I guess one thing I we've

00:32:58,010 --> 00:33:00,500
partnered with girls to code I don't

00:32:59,840 --> 00:33:04,310
know if you're familiar that

00:33:00,500 --> 00:33:06,770
organization and they have after-school

00:33:04,310 --> 00:33:08,360
clubs and they're it's been fascinating

00:33:06,770 --> 00:33:10,700
to watch that organization over the past

00:33:08,360 --> 00:33:12,980
few years I think right now they've

00:33:10,700 --> 00:33:14,150
reached 185 thousand girls across the

00:33:12,980 --> 00:33:16,940
u.s. so

00:33:14,150 --> 00:33:20,840
it's expanding like crazy they have we

00:33:16,940 --> 00:33:22,310
we sponsor a summer immersion program on

00:33:20,840 --> 00:33:24,920
the into its campus where we bring in

00:33:22,310 --> 00:33:26,800
high school girls for seven weeks and

00:33:24,920 --> 00:33:29,480
they learn coding and get exposure to

00:33:26,800 --> 00:33:30,950
tech careers and so we bring in some of

00:33:29,480 --> 00:33:33,320
our technologists and share what they're

00:33:30,950 --> 00:33:36,500
doing and kind of show them what a

00:33:33,320 --> 00:33:38,510
career in tech could look like and they

00:33:36,500 --> 00:33:40,490
also have for the younger because I rely

00:33:38,510 --> 00:33:42,620
for I I'm with you i grew i think that

00:33:40,490 --> 00:33:44,390
the middle school years are so critical

00:33:42,620 --> 00:33:46,790
you got a that's when they're deciding

00:33:44,390 --> 00:33:49,940
and they're just saying i'm i'm good at

00:33:46,790 --> 00:33:51,320
this or not and so i'm girls two girls

00:33:49,940 --> 00:33:53,510
who code has an after school club

00:33:51,320 --> 00:33:55,400
program that they're and they're

00:33:53,510 --> 00:33:57,200
increasing the number of clubs around

00:33:55,400 --> 00:33:59,420
the u.s. and now they're going into

00:33:57,200 --> 00:34:02,180
canada as well so that's an option i'm

00:33:59,420 --> 00:34:03,590
not sure what the whether that would be

00:34:02,180 --> 00:34:06,260
something that could happen at your

00:34:03,590 --> 00:34:08,480
daughter's school or not but i do think

00:34:06,260 --> 00:34:10,850
that's a critical time where these kids

00:34:08,480 --> 00:34:13,550
are like they're self-selecting in or

00:34:10,850 --> 00:34:16,480
out and we have to reach everybody and

00:34:13,550 --> 00:34:19,669
give them put them on a shoma path and

00:34:16,480 --> 00:34:21,050
and also so chancer so we don't do that

00:34:19,669 --> 00:34:22,669
much direct work with middle school

00:34:21,050 --> 00:34:25,490
students so we recognize the need is

00:34:22,669 --> 00:34:26,870
there absolutely and but what's

00:34:25,490 --> 00:34:28,399
something that we do see this exciting

00:34:26,870 --> 00:34:30,230
is a lot of our students do work with

00:34:28,399 --> 00:34:31,640
middle school students so Stephanie who

00:34:30,230 --> 00:34:34,280
I mentioned she's in the picture with

00:34:31,640 --> 00:34:36,380
Bill Gates actually she started a middle

00:34:34,280 --> 00:34:38,750
school club in Salinas California

00:34:36,380 --> 00:34:39,980
because she wanted to share what she had

00:34:38,750 --> 00:34:42,290
learned that AI for all with her

00:34:39,980 --> 00:34:43,880
community so we're seeing that for every

00:34:42,290 --> 00:34:45,530
one student who goes through one of our

00:34:43,880 --> 00:34:47,690
programs they on average go on to

00:34:45,530 --> 00:34:49,100
educate about ten more and they do a lot

00:34:47,690 --> 00:34:51,020
of the education work with the younger

00:34:49,100 --> 00:34:53,240
younger grades and we provide small

00:34:51,020 --> 00:34:56,060
grants to help support their outreach

00:34:53,240 --> 00:34:57,710
initiatives so we are seeing our

00:34:56,060 --> 00:34:59,870
students doing a lot of that work which

00:34:57,710 --> 00:35:02,330
is really exciting and we're also seeing

00:34:59,870 --> 00:35:05,000
other organizations specifically focused

00:35:02,330 --> 00:35:06,920
on younger education in AI so

00:35:05,000 --> 00:35:09,140
Technovation is actually a really great

00:35:06,920 --> 00:35:11,360
one and they have a lot of fun family

00:35:09,140 --> 00:35:14,680
challenges around AI and like how can

00:35:11,360 --> 00:35:17,180
you learn about AI with just cups and

00:35:14,680 --> 00:35:19,070
everyday household items to like get a

00:35:17,180 --> 00:35:21,110
basic understanding of what a neural net

00:35:19,070 --> 00:35:22,940
is so definitely encourage you to check

00:35:21,110 --> 00:35:25,400
them out there are wonderful

00:35:22,940 --> 00:35:27,380
organization and I think you just by

00:35:25,400 --> 00:35:29,750
being a role model to your your

00:35:27,380 --> 00:35:32,029
children and your community really

00:35:29,750 --> 00:35:35,950
speaks volumes to to getting more people

00:35:32,029 --> 00:35:35,950
excited about that possibility as well

00:35:36,940 --> 00:35:44,960
hi Tracy and Amy I really enjoy a talk

00:35:42,220 --> 00:35:47,079
and also thanks for the lunch she was

00:35:44,960 --> 00:35:49,940
very delicious I really enjoyed it

00:35:47,079 --> 00:35:53,660
by the way my name is Angela Keene I'm

00:35:49,940 --> 00:35:56,569
from Sydney Australia so I really admire

00:35:53,660 --> 00:35:59,390
what us some is doing a great job for

00:35:56,569 --> 00:36:02,509
the diversity and inclusion especially

00:35:59,390 --> 00:36:08,980
girls in technology so I was so inspired

00:36:02,509 --> 00:36:11,420
by that I joined the woman in AI for the

00:36:08,980 --> 00:36:13,759
nonprofit organization based in Paris

00:36:11,420 --> 00:36:17,420
and from beginning of this year I'm

00:36:13,759 --> 00:36:20,150
working as a woman in a I am ambassador

00:36:17,420 --> 00:36:23,029
for education ambassador for Shelia so

00:36:20,150 --> 00:36:26,509
I've started running the AI camp for the

00:36:23,029 --> 00:36:28,519
girls in lower socioeconomic area so

00:36:26,509 --> 00:36:31,730
like in body near eleven like 30 girls

00:36:28,519 --> 00:36:33,799
who's never exposed to coding before and

00:36:31,730 --> 00:36:36,259
try to really partner with the

00:36:33,799 --> 00:36:39,559
corporates and tried to sponsor them for

00:36:36,259 --> 00:36:43,309
two days on food a workshop I witnessed

00:36:39,559 --> 00:36:45,890
with my own eyes broke them to starting

00:36:43,309 --> 00:36:49,640
from don't have any device at home only

00:36:45,890 --> 00:36:50,960
one communal laptop and 22 students from

00:36:49,640 --> 00:36:52,849
the 30 student they couldn't come

00:36:50,960 --> 00:36:54,920
because their parents asked them to

00:36:52,849 --> 00:36:57,829
actually work with their parents on

00:36:54,920 --> 00:37:00,200
business so that's how drastic two

00:36:57,829 --> 00:37:03,200
extremes we can eat witness very

00:37:00,200 --> 00:37:05,599
well-equipped private school students

00:37:03,200 --> 00:37:07,309
and the other end they don't have any

00:37:05,599 --> 00:37:09,920
capacity they don't have an opportunity

00:37:07,309 --> 00:37:12,650
and I'm working really hard with the

00:37:09,920 --> 00:37:16,039
corporate partners try to help them try

00:37:12,650 --> 00:37:20,299
to run not one of camp but continuous

00:37:16,039 --> 00:37:22,009
learning ecosystem and for us I'd like

00:37:20,299 --> 00:37:24,559
to partner with you know air for all and

00:37:22,009 --> 00:37:26,809
thinking if you can because you guys

00:37:24,559 --> 00:37:29,359
doing great stuff and you know

00:37:26,809 --> 00:37:32,690
capability and connection and learning

00:37:29,359 --> 00:37:34,970
you know ecosystem so once used you know

00:37:32,690 --> 00:37:37,279
you establish yours then you can think

00:37:34,970 --> 00:37:40,880
about the other countries in the world

00:37:37,279 --> 00:37:42,859
and try to see how you can empower them

00:37:40,880 --> 00:37:45,259
you can grow together and they will

00:37:42,859 --> 00:37:47,299
empower girls so we can bring all the

00:37:45,259 --> 00:37:49,940
you know capability and together and

00:37:47,299 --> 00:37:54,859
they really create a better future in

00:37:49,940 --> 00:37:58,400
the you know AI in the future so maybe I

00:37:54,859 --> 00:38:00,650
can contact you separately yeah and also

00:37:58,400 --> 00:38:03,140
I've got one little question

00:38:00,650 --> 00:38:04,910
so students important right because they

00:38:03,140 --> 00:38:08,240
are young generation for the future

00:38:04,910 --> 00:38:10,309
how about single mothers mothers coming

00:38:08,240 --> 00:38:12,440
back from maternity leave and longevity

00:38:10,309 --> 00:38:14,900
all the people who's not very

00:38:12,440 --> 00:38:19,039
comfortable with the technology what can

00:38:14,900 --> 00:38:19,849
we do together to help them because we

00:38:19,039 --> 00:38:23,420
have to think about all different

00:38:19,849 --> 00:38:26,029
pockets it's almost like a scaling up we

00:38:23,420 --> 00:38:29,180
were successfully completed in a POC on

00:38:26,029 --> 00:38:32,509
youngers we can see how they improve and

00:38:29,180 --> 00:38:35,180
they grow and they become leaders right

00:38:32,509 --> 00:38:38,329
how about next people are living longer

00:38:35,180 --> 00:38:39,710
the senior citizens there are so many

00:38:38,329 --> 00:38:42,680
people who need to embrace technology

00:38:39,710 --> 00:38:45,740
right and we can't really work together

00:38:42,680 --> 00:38:48,980
more proactively try to create a better

00:38:45,740 --> 00:38:51,799
future and I admit you know entreat you

00:38:48,980 --> 00:38:53,900
know air for Intel and you guys are

00:38:51,799 --> 00:38:56,630
doing wonderful job internationally

00:38:53,900 --> 00:38:58,309
right but we can work better try to

00:38:56,630 --> 00:38:59,809
connect better and try to use all the

00:38:58,309 --> 00:39:01,759
social you know platforms and try to

00:38:59,809 --> 00:39:04,160
work together you know bring the voice

00:39:01,759 --> 00:39:06,650
together and share the successful

00:39:04,160 --> 00:39:08,450
stories and they share the learnings so

00:39:06,650 --> 00:39:09,920
that people don't have to go same path

00:39:08,450 --> 00:39:12,890
you know making the same mistake and

00:39:09,920 --> 00:39:24,109
take you longer to do better job yes

00:39:12,890 --> 00:39:28,069
thank you yeah I could I love it thank

00:39:24,109 --> 00:39:30,230
you so much I think that's I love all

00:39:28,069 --> 00:39:31,759
those ideas I wanted to just the one

00:39:30,230 --> 00:39:34,700
thing that we can add to that is that

00:39:31,759 --> 00:39:36,589
one of the programs that Intuit has done

00:39:34,700 --> 00:39:40,549
recently is we called in to it again

00:39:36,589 --> 00:39:42,230
it's a return to work program and so we

00:39:40,549 --> 00:39:44,809
were finding people who'd left the

00:39:42,230 --> 00:39:47,420
workforce and it was predominantly women

00:39:44,809 --> 00:39:49,099
and a lot of us were on caregiving but

00:39:47,420 --> 00:39:51,650
doesn't have to be but people left the

00:39:49,099 --> 00:39:53,390
workforce it's so hard to get back in to

00:39:51,650 --> 00:39:54,589
the workforce especially in tech and

00:39:53,390 --> 00:39:57,680
especially with technology

00:39:54,589 --> 00:39:59,719
is evolving so quickly so we created a

00:39:57,680 --> 00:40:01,940
program it's kind of modeled after a

00:39:59,719 --> 00:40:03,979
college intern program but it's called a

00:40:01,940 --> 00:40:05,809
return ship program and so we've been

00:40:03,979 --> 00:40:08,479
successfully doing that and we've done

00:40:05,809 --> 00:40:12,049
it in our at our India location and now

00:40:08,479 --> 00:40:14,569
we're doing it in the u.s. here and it's

00:40:12,049 --> 00:40:18,440
a four-month return ship with the idea

00:40:14,569 --> 00:40:20,749
that then it's an opportunity to hire on

00:40:18,440 --> 00:40:23,450
full-time so we've had a lot of success

00:40:20,749 --> 00:40:26,299
to that and I think that's to your point

00:40:23,450 --> 00:40:27,859
of POC around around that and we could

00:40:26,299 --> 00:40:29,630
another springboard to look at how we

00:40:27,859 --> 00:40:31,279
look at other people who are looking to

00:40:29,630 --> 00:40:32,779
get into these roles and how do we

00:40:31,279 --> 00:40:39,380
support them and create programs and

00:40:32,779 --> 00:40:42,499
opportunities so I love it and so again

00:40:39,380 --> 00:40:44,089
thank you for for again I know we're not

00:40:42,499 --> 00:40:45,589
doing it all we're not doing enough for

00:40:44,089 --> 00:40:49,160
middle school students we're not doing

00:40:45,589 --> 00:40:52,069
enough for older populations but our

00:40:49,160 --> 00:40:55,160
open learning program is our stab at

00:40:52,069 --> 00:40:56,930
trying to make our curriculum and the

00:40:55,160 --> 00:40:58,249
project work and all of the tools that

00:40:56,930 --> 00:41:01,369
we've seen work with high school

00:40:58,249 --> 00:41:04,430
students more widely accessible so that

00:41:01,369 --> 00:41:06,950
is also an opportunity for you to use in

00:41:04,430 --> 00:41:10,249
Australia or wherever you are based

00:41:06,950 --> 00:41:11,960
again it's free and it's online so any

00:41:10,249 --> 00:41:13,849
high school student or community college

00:41:11,960 --> 00:41:15,710
student or anyone who want to learn

00:41:13,849 --> 00:41:22,880
about AI can sign up for free to learn

00:41:15,710 --> 00:41:26,119
about it so thank you hi sorry my name

00:41:22,880 --> 00:41:28,249
is Mary regarding the organization's or

00:41:26,119 --> 00:41:31,789
meals Coker's I wanted to share with

00:41:28,249 --> 00:41:33,259
whoever is interested this organization

00:41:31,789 --> 00:41:35,420
that is called Edgars

00:41:33,259 --> 00:41:37,249
they are based in Philadelphia and what

00:41:35,420 --> 00:41:40,279
they do is like they try to reduce a

00:41:37,249 --> 00:41:42,529
gender gap in the stem field so we focus

00:41:40,279 --> 00:41:44,869
I'm a volunteer instructor for them so

00:41:42,529 --> 00:41:46,999
we focus on workshops for middle school

00:41:44,869 --> 00:41:50,210
girls and then it goes from like

00:41:46,999 --> 00:41:53,359
Photoshop to robotics ai ai programming

00:41:50,210 --> 00:41:55,430
languages we are looking for volunteers

00:41:53,359 --> 00:41:58,549
because we have like we have been based

00:41:55,430 --> 00:42:01,549
in like many more cities like embassy

00:41:58,549 --> 00:42:04,160
Chicago's volunteer there and the

00:42:01,549 --> 00:42:06,349
workshops are like in different

00:42:04,160 --> 00:42:06,930
different dates with days or weekends

00:42:06,349 --> 00:42:09,960
like

00:42:06,930 --> 00:42:11,700
it is their base or middle girls but if

00:42:09,960 --> 00:42:13,859
someone else is interesting they can

00:42:11,700 --> 00:42:15,930
sign up so if they are interested to

00:42:13,859 --> 00:42:18,059
know more about it I will just I post it

00:42:15,930 --> 00:42:25,079
on the whiteboard so people can take a

00:42:18,059 --> 00:42:26,849
look at it any other any other questions

00:42:25,079 --> 00:42:28,230
all right this has really been a

00:42:26,849 --> 00:42:30,359
wonderful lunch a lot of great

00:42:28,230 --> 00:42:32,309
discussion so thanks everyone for coming

00:42:30,359 --> 00:42:34,079
thank you to our speakers Amy and Tracy

00:42:32,309 --> 00:42:36,890
them into it we really appreciate with

00:42:34,079 --> 00:42:36,890

YouTube URL: https://www.youtube.com/watch?v=D6qTgI4b5MI


