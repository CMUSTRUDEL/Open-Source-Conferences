Title: On gradient-based methods for finding game-theoretic equilibria - Michael Jordan (UC Berkeley)
Publication date: 2019-09-13
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Statistical decisions are often given meaning in the context of other decisions, particularly when there are scarce resources to be shared. Michael Jordan details the aim to blend gradient-based methodology with game-theoretic goals as part of a large "microeconomics meets machine learning" program.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:03,000
this is kind of first stage research

00:00:01,680 --> 00:00:03,840
this is kind of get just getting going

00:00:03,000 --> 00:00:05,339
on a topic

00:00:03,840 --> 00:00:07,200
Lydia you and Hardy and Manny are

00:00:05,339 --> 00:00:08,910
students and with me working on this so

00:00:07,200 --> 00:00:10,710
we could talk about matching markets

00:00:08,910 --> 00:00:12,059
it's a particular kind of market it's

00:00:10,710 --> 00:00:14,070
not a mark where there's prices being

00:00:12,059 --> 00:00:15,570
said it's like a kidney exchange as a

00:00:14,070 --> 00:00:17,310
matching market we want to simply match

00:00:15,570 --> 00:00:18,660
things together and we want to do this

00:00:17,310 --> 00:00:20,400
on a learning context

00:00:18,660 --> 00:00:22,350
all right so kind of what's the minimal

00:00:20,400 --> 00:00:24,119
learning problem to me that's the

00:00:22,350 --> 00:00:25,890
multi-armed bandit and not enough people

00:00:24,119 --> 00:00:27,029
talk about this it's not supervised

00:00:25,890 --> 00:00:29,789
learning it's not reinforced with

00:00:27,029 --> 00:00:31,109
learning it's kind of the simplest

00:00:29,789 --> 00:00:32,880
problem which is that there are K

00:00:31,109 --> 00:00:35,010
options here there's three and I'm a

00:00:32,880 --> 00:00:36,840
decision maker and I got to figure out

00:00:35,010 --> 00:00:38,790
which option I prefer and I have no idea

00:00:36,840 --> 00:00:40,920
a priori which of the three is the best

00:00:38,790 --> 00:00:42,270
so I pull the first arm a little bit I

00:00:40,920 --> 00:00:44,040
pull the second arm and the third arm a

00:00:42,270 --> 00:00:45,360
little bit and I start to see what

00:00:44,040 --> 00:00:49,649
reward I'm getting from each of the arms

00:00:45,360 --> 00:00:50,969
and once I start to realize what looks

00:00:49,649 --> 00:00:52,829
best I start to pull that more and more

00:00:50,969 --> 00:00:54,510
often but if I kind of hone in too

00:00:52,829 --> 00:00:56,039
quickly on the wrong arm I could pull it

00:00:54,510 --> 00:00:58,410
forever and really and have a high

00:00:56,039 --> 00:01:00,629
regret I could have pulled arm three

00:00:58,410 --> 00:01:03,149
that would have been better but I too

00:01:00,629 --> 00:01:05,189
early opted for arm one so as you may

00:01:03,149 --> 00:01:07,320
know there's algorithms like UCB upper

00:01:05,189 --> 00:01:08,610
confidence bound where you take not just

00:01:07,320 --> 00:01:10,290
the mean reward you've gotten but you

00:01:08,610 --> 00:01:11,580
take a confidence bound on that and you

00:01:10,290 --> 00:01:14,340
take the upper bound of the confidence

00:01:11,580 --> 00:01:17,009
interval and use that to decide which

00:01:14,340 --> 00:01:18,630
arm to pull and so if you're have a high

00:01:17,009 --> 00:01:20,310
confidence bound because it's really

00:01:18,630 --> 00:01:21,810
clearly a good arm you're gonna pull it

00:01:20,310 --> 00:01:23,220
a lot but you might have a high

00:01:21,810 --> 00:01:25,290
confidence bound because you're very

00:01:23,220 --> 00:01:27,060
unsure about the reward and you should

00:01:25,290 --> 00:01:29,070
also pull the arm for that reason so

00:01:27,060 --> 00:01:31,860
this problem has embedded it in the

00:01:29,070 --> 00:01:33,210
exploration exploitation trade-off which

00:01:31,860 --> 00:01:34,530
is a core part of the learning problem

00:01:33,210 --> 00:01:35,640
which you don't have in supervised

00:01:34,530 --> 00:01:37,229
learning and you don't have in

00:01:35,640 --> 00:01:39,720
reinforcement learning as least as it's

00:01:37,229 --> 00:01:40,890
traditionally done so I think if you

00:01:39,720 --> 00:01:43,110
think about learning that this is a

00:01:40,890 --> 00:01:44,610
great problem to be thinking about okay

00:01:43,110 --> 00:01:46,829
so we pull we get a reward and we go

00:01:44,610 --> 00:01:48,479
back so we're gonna put that together

00:01:46,829 --> 00:01:49,560
with matching markets and you all kind

00:01:48,479 --> 00:01:51,840
of know what they are we've got buyers

00:01:49,560 --> 00:01:53,670
on one side suppliers on the other and

00:01:51,840 --> 00:01:56,100
they have preferences for each other and

00:01:53,670 --> 00:01:57,960
Gale and Shapley introduced an algorithm

00:01:56,100 --> 00:02:00,630
that matches them up that sort of give

00:01:57,960 --> 00:02:02,219
everybody their you know something not

00:02:00,630 --> 00:02:03,899
if not they're exactly preferred item

00:02:02,219 --> 00:02:05,100
gives everybody something they prefer

00:02:03,899 --> 00:02:08,160
enough that they wouldn't change a

00:02:05,100 --> 00:02:09,959
notion of equilibrium now so we're gonna

00:02:08,160 --> 00:02:12,420
do this in a setting it's different from

00:02:09,959 --> 00:02:13,350
Gale Shapley where the buyers don't know

00:02:12,420 --> 00:02:14,340
their preferences

00:02:13,350 --> 00:02:15,450
because they haven't experienced the

00:02:14,340 --> 00:02:17,370
cellar side of the market yet they have

00:02:15,450 --> 00:02:18,720
to try it out and the sellers don't know

00:02:17,370 --> 00:02:20,010
which buyers they prefer they have to

00:02:18,720 --> 00:02:22,770
experience that side of the market so

00:02:20,010 --> 00:02:24,780
we're gonna blend together market

00:02:22,770 --> 00:02:26,520
matching markets with learning okay so

00:02:24,780 --> 00:02:28,800
the participants don't know what they

00:02:26,520 --> 00:02:31,320
prefer so we have an exploration

00:02:28,800 --> 00:02:33,330
exploitation problem but in the context

00:02:31,320 --> 00:02:34,830
of other participants so if I'm playing

00:02:33,330 --> 00:02:37,860
and I'm trying to pick the arm but

00:02:34,830 --> 00:02:40,320
there's a bear playing - we both might

00:02:37,860 --> 00:02:42,420
pick arm - and in our model that we've

00:02:40,320 --> 00:02:44,460
been studying at that Katyn moment what

00:02:42,420 --> 00:02:46,830
happens well only one of us gets that

00:02:44,460 --> 00:02:48,870
arm okay it's like going down a street

00:02:46,830 --> 00:02:50,340
there's a capacity constraint not too

00:02:48,870 --> 00:02:51,540
many people could go down the street so

00:02:50,340 --> 00:02:53,790
we model that by saying there's an

00:02:51,540 --> 00:02:55,350
exclusion competing agents have been

00:02:53,790 --> 00:02:57,240
modeled in economics but it's more of a

00:02:55,350 --> 00:03:00,540
congestion channel if both of us pick

00:02:57,240 --> 00:03:02,460
the same arm neither of us get it okay

00:03:00,540 --> 00:03:05,610
so let's suppose it only one of us get

00:03:02,460 --> 00:03:07,050
it all right and so how do we sort of

00:03:05,610 --> 00:03:08,400
set up this problem and I'm gonna kind

00:03:07,050 --> 00:03:10,890
of go quickly here just to say we've

00:03:08,400 --> 00:03:12,960
done this you have to form a notion of

00:03:10,890 --> 00:03:14,130
regret what's the best decision to make

00:03:12,960 --> 00:03:16,080
well it's relative to the best you could

00:03:14,130 --> 00:03:19,020
have done and so what we do is we define

00:03:16,080 --> 00:03:20,910
it relative to the best Gale Shapley

00:03:19,020 --> 00:03:22,470
matching you could have if you've know

00:03:20,910 --> 00:03:24,780
exactly all your preferences on both

00:03:22,470 --> 00:03:26,640
sides and with that notion of regret

00:03:24,780 --> 00:03:28,440
we're able to prove a theorem that a

00:03:26,640 --> 00:03:30,690
certain algorithm known as upper

00:03:28,440 --> 00:03:33,180
confidence bound Gale Shapley where

00:03:30,690 --> 00:03:34,800
everybody runs upper confidence bound

00:03:33,180 --> 00:03:36,810
and they send those numbers to a gale

00:03:34,800 --> 00:03:39,240
Shapley algorithm to do matching that

00:03:36,810 --> 00:03:41,340
algorithm has a logarithmic regret which

00:03:39,240 --> 00:03:42,990
is sort of the best you can do moreover

00:03:41,340 --> 00:03:44,730
the agents interact with each other a

00:03:42,990 --> 00:03:48,420
particular way capture by this delta

00:03:44,730 --> 00:03:49,590
squared okay so I think this is just an

00:03:48,420 --> 00:03:50,880
indication of lots and lots of research

00:03:49,590 --> 00:03:52,380
to come at least certainly for my group

00:03:50,880 --> 00:03:54,360
but I think a lots of others about how

00:03:52,380 --> 00:03:55,470
you put learning together in the context

00:03:54,360 --> 00:03:57,480
of systems that are trying to make

00:03:55,470 --> 00:03:59,130
multiple agents happy not just a single

00:03:57,480 --> 00:04:02,150
agent happy that's the real world to me

00:03:59,130 --> 00:04:02,150
and that's what we need to be facing

00:04:07,920 --> 00:04:09,980

YouTube URL: https://www.youtube.com/watch?v=fEV-MAo1gc4


