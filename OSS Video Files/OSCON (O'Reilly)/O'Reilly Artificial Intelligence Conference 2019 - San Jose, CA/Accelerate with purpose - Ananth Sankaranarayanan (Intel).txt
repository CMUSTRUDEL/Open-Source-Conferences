Title: Accelerate with purpose - Ananth Sankaranarayanan (Intel)
Publication date: 2019-09-13
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Today’s computing hardware has taken us well into the resurgence of AI, but with the demand for AI compute doubling nearly every three months, new ways of computing are required that can keep up and evolve. Ananth Sankaranarayanan discusses three key shifts in the AI landscape—incredibly large models with billions of hyperparameters, massive clusters of compute nodes supporting AI, and the exploding volume of data meeting ever-stricter latency requirements—how to navigate them, and when to explore hardware acceleration.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,120 --> 00:00:06,509
long gone are the days of detecting cat

00:00:02,730 --> 00:00:09,030
vs. dog in a given image we are now at a

00:00:06,509 --> 00:00:11,849
point of really understanding the

00:00:09,030 --> 00:00:14,639
meaning the context of what the image or

00:00:11,849 --> 00:00:16,949
the data is trying to say okay you see

00:00:14,639 --> 00:00:19,699
the trend shifting from the bottom left

00:00:16,949 --> 00:00:22,560
to the top right which in essence means

00:00:19,699 --> 00:00:25,230
that the amount of data that you need to

00:00:22,560 --> 00:00:29,490
hold significantly changes from few

00:00:25,230 --> 00:00:32,369
megabytes to multiple terabytes in order

00:00:29,490 --> 00:00:34,860
to be able to understand the context or

00:00:32,369 --> 00:00:40,320
the meaning of a given data set one

00:00:34,860 --> 00:00:42,329
needs to build complex models sometimes

00:00:40,320 --> 00:00:47,370
with hundreds of neural network layers

00:00:42,329 --> 00:00:51,120
and billions of parameters in order to

00:00:47,370 --> 00:00:54,500
be able to train those models on large

00:00:51,120 --> 00:00:58,289
datasets you need bigger computer and

00:00:54,500 --> 00:01:03,449
hence you need exponentially increasing

00:00:58,289 --> 00:01:06,330
AI computer which is obviously doubling

00:01:03,449 --> 00:01:11,760
roughly every quarter again these are

00:01:06,330 --> 00:01:20,040
extreme cases may become the future

00:01:11,760 --> 00:01:23,100
status quo tomorrow so this faster

00:01:20,040 --> 00:01:25,200
growth in the a applications has forced

00:01:23,100 --> 00:01:27,330
our Hardware community that start

00:01:25,200 --> 00:01:31,500
thinking differently there's no

00:01:27,330 --> 00:01:33,590
one-size-fits-all meaning there's no one

00:01:31,500 --> 00:01:36,600
architecture that is suited for

00:01:33,590 --> 00:01:40,770
performing every possible applications

00:01:36,600 --> 00:01:42,570
really well so what really happens is as

00:01:40,770 --> 00:01:46,020
you application developers write

00:01:42,570 --> 00:01:48,409
applications some part of it really gets

00:01:46,020 --> 00:01:51,479
translated to a mathematical equation

00:01:48,409 --> 00:01:53,399
some multiplication addition and which

00:01:51,479 --> 00:01:56,820
then gets programmed to underlying

00:01:53,399 --> 00:01:58,159
hardware that you have it could be as

00:01:56,820 --> 00:02:00,750
simple as multiplying two numbers

00:01:58,159 --> 00:02:03,990
integer or floating-point it could be as

00:02:00,750 --> 00:02:06,450
complex as multiplying matrices single

00:02:03,990 --> 00:02:09,479
dimension vectors or two dimension or

00:02:06,450 --> 00:02:12,660
three dimensional tensors specifically

00:02:09,479 --> 00:02:16,170
for AI majority of the application

00:02:12,660 --> 00:02:17,700
it is towards matrix multiplication so

00:02:16,170 --> 00:02:21,180
the accelerators that are being built

00:02:17,700 --> 00:02:21,570
have to have specific focus on three

00:02:21,180 --> 00:02:26,370
things

00:02:21,570 --> 00:02:29,130
compute communication and memory because

00:02:26,370 --> 00:02:31,560
by definition these accelerators are

00:02:29,130 --> 00:02:34,590
purpose-built they have to do one thing

00:02:31,560 --> 00:02:37,020
really really well in this case matrix

00:02:34,590 --> 00:02:40,950
multiplication because we are dealing

00:02:37,020 --> 00:02:43,710
with large amounts of data data movement

00:02:40,950 --> 00:02:47,010
has to happen within the chip and across

00:02:43,710 --> 00:02:50,190
the chips so low latency high bandwidth

00:02:47,010 --> 00:02:53,100
matters because we are dealing with lots

00:02:50,190 --> 00:02:57,710
of data the closer the data gets to the

00:02:53,100 --> 00:02:57,710
compute the faster the compute becomes

00:03:03,640 --> 00:03:05,700

YouTube URL: https://www.youtube.com/watch?v=Iqyl4RwMUVs


