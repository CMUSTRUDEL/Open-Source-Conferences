Title: Developing AI responsibly - Sarah Bird (Microsoft)
Publication date: 2019-09-11
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Researchers and practitioners from different disciplines have highlighted the ethical and legal challenges posed by the use of machine learning in many current and future real-world applications. Sarah Bird outlines her perspective on some of the major challenges in responsible AI development and examines promising new tools and technologies to help enable it in practice.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:06,120
so a Microsoft as a foundation to guide

00:00:03,300 --> 00:00:08,910
our thinking we define six ethical

00:00:06,120 --> 00:00:12,059
principles that we believe all AI

00:00:08,910 --> 00:00:15,540
systems should adhere to the first for

00:00:12,059 --> 00:00:20,189
fairness reliability and safety privacy

00:00:15,540 --> 00:00:21,960
and security and inclusiveness our top

00:00:20,189 --> 00:00:24,029
level properties that these systems

00:00:21,960 --> 00:00:26,970
should achieve the second to

00:00:24,029 --> 00:00:28,920
transparency and accountability underlie

00:00:26,970 --> 00:00:32,250
everything and are about how we build

00:00:28,920 --> 00:00:36,780
our systems and how we allow people to

00:00:32,250 --> 00:00:40,110
interact with them so principles are

00:00:36,780 --> 00:00:42,149
only a first step in a long journey the

00:00:40,110 --> 00:00:44,100
real challenge is making this vision a

00:00:42,149 --> 00:00:46,260
reality and putting our principles into

00:00:44,100 --> 00:00:47,910
action so today I'm going to talk a

00:00:46,260 --> 00:00:51,420
little bit more on some of the first

00:00:47,910 --> 00:00:54,600
steps that we've done to sort of enable

00:00:51,420 --> 00:00:57,930
putting these into action responsible AI

00:00:54,600 --> 00:01:00,300
is one of the most complex and broad

00:00:57,930 --> 00:01:02,489
topics that we have in the field of AI

00:01:00,300 --> 00:01:04,799
it's challenging because it brings

00:01:02,489 --> 00:01:07,560
together both social and technical

00:01:04,799 --> 00:01:09,560
issues and it requires nearly every

00:01:07,560 --> 00:01:12,049
function in product development

00:01:09,560 --> 00:01:14,700
everything from business understanding

00:01:12,049 --> 00:01:19,049
to technical details in data acquisition

00:01:14,700 --> 00:01:22,500
and modeling to details on how we deploy

00:01:19,049 --> 00:01:24,060
and operational logistics and it's at

00:01:22,500 --> 00:01:26,400
this point it's very hard to separate

00:01:24,060 --> 00:01:28,020
all of these and so we really have to

00:01:26,400 --> 00:01:31,680
bring every function together when we

00:01:28,020 --> 00:01:33,560
think about building AI responsibly the

00:01:31,680 --> 00:01:36,810
state of the art in the industry is

00:01:33,560 --> 00:01:40,290
pretty early-stage this is an emerging

00:01:36,810 --> 00:01:43,079
area and in a lot of cases we lack

00:01:40,290 --> 00:01:45,509
robust technical solutions for many of

00:01:43,079 --> 00:01:47,640
the problems so as a result when you

00:01:45,509 --> 00:01:50,970
hear talks like this a lot of the focus

00:01:47,640 --> 00:01:53,549
is going to be about people best

00:01:50,970 --> 00:01:56,790
practices such as testing with diverse

00:01:53,549 --> 00:01:59,579
user groups building diverse teams

00:01:56,790 --> 00:02:03,090
bringing in two main expertise a lot of

00:01:59,579 --> 00:02:05,549
focus on process you know reviewing

00:02:03,090 --> 00:02:07,020
documenting checking things and this is

00:02:05,549 --> 00:02:10,220
because this is the best we have right

00:02:07,020 --> 00:02:11,660
now one of the the

00:02:10,220 --> 00:02:13,340
things we can do technologically though

00:02:11,660 --> 00:02:16,790
one of the most general robust

00:02:13,340 --> 00:02:19,550
approaches we have is focus on measuring

00:02:16,790 --> 00:02:23,270
and understanding so particularly

00:02:19,550 --> 00:02:26,240
emphasizing analysis and testing and the

00:02:23,270 --> 00:02:28,610
good news about this is that measurement

00:02:26,240 --> 00:02:30,620
and measuring and understanding complex

00:02:28,610 --> 00:02:33,760
metrics is something that the field has

00:02:30,620 --> 00:02:33,760

YouTube URL: https://www.youtube.com/watch?v=9yP6FHRmnBg


