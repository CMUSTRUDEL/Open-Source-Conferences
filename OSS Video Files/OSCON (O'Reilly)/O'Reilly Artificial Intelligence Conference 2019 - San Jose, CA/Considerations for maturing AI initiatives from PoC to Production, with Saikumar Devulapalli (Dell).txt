Title: Considerations for maturing AI initiatives from PoC to Production, with Saikumar Devulapalli (Dell)
Publication date: 2019-09-23
Playlist: O'Reilly Artificial Intelligence Conference 2019 - San Jose, CA
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,420 --> 00:00:07,210
hi Roger McGough is here at AI San Jose

00:00:03,610 --> 00:00:09,190
2019 I'm here with psy who does the

00:00:07,210 --> 00:00:11,230
analytics data analytics line of

00:00:09,190 --> 00:00:12,730
business at Dell welcome thank you good

00:00:11,230 --> 00:00:16,030
to see you

00:00:12,730 --> 00:00:18,010
ai is so broad as a discipline and

00:00:16,030 --> 00:00:21,520
there's platforms all over the place

00:00:18,010 --> 00:00:22,870
it's kind of a kind of a mess really do

00:00:21,520 --> 00:00:26,050
you guys have like a kind of a taxonomy

00:00:22,870 --> 00:00:29,350
and how you organize what yeah that's a

00:00:26,050 --> 00:00:30,880
great question so we look at AI into

00:00:29,350 --> 00:00:33,430
primarily into two different buckets

00:00:30,880 --> 00:00:34,810
machine learning and deep learning from

00:00:33,430 --> 00:00:36,880
an infrastructure and a platform

00:00:34,810 --> 00:00:39,400
perspective right that's what Delia's he

00:00:36,880 --> 00:00:42,280
does and so for machine learning we're

00:00:39,400 --> 00:00:45,390
looking at datasets that are anywhere

00:00:42,280 --> 00:00:47,260
from 10 terabytes tens of terabytes to

00:00:45,390 --> 00:00:49,150
hundreds of terabytes to maybe a

00:00:47,260 --> 00:00:51,579
petabyte or maybe slightly higher than

00:00:49,150 --> 00:00:54,220
that and typically you're looking at

00:00:51,579 --> 00:00:55,720
data that is generated from IOT devices

00:00:54,220 --> 00:00:57,160
or servers so this is like

00:00:55,720 --> 00:01:00,160
semi-structured data we're talking about

00:00:57,160 --> 00:01:01,810
right and from a compute perspective

00:01:00,160 --> 00:01:04,210
we're looking at tens of servers to

00:01:01,810 --> 00:01:06,729
hundreds of servers maybe a thousand or

00:01:04,210 --> 00:01:08,409
a couple thousand servers max on the

00:01:06,729 --> 00:01:11,110
deep learning front we're looking at

00:01:08,409 --> 00:01:13,360
much more unstructured data sets so you

00:01:11,110 --> 00:01:18,400
were looking at images really a Content

00:01:13,360 --> 00:01:20,020
free text audio content and the server

00:01:18,400 --> 00:01:21,520
capacities are less much larger so

00:01:20,020 --> 00:01:23,650
you're looking at clusters of you know

00:01:21,520 --> 00:01:27,400
hundreds or thousands of of compute

00:01:23,650 --> 00:01:29,680
clusters the capacity and storage

00:01:27,400 --> 00:01:31,930
requirements are also much larger so it

00:01:29,680 --> 00:01:34,299
could go up to multiple petabytes right

00:01:31,930 --> 00:01:35,590
so the use cases they're also slightly

00:01:34,299 --> 00:01:37,990
different on the machine learning you're

00:01:35,590 --> 00:01:41,170
looking at you skids around you know

00:01:37,990 --> 00:01:43,689
prediction you know cyber security

00:01:41,170 --> 00:01:45,610
analytics recommendation engines things

00:01:43,689 --> 00:01:47,020
like that whereas on the deep learning

00:01:45,610 --> 00:01:49,960
side you're looking at things like gene

00:01:47,020 --> 00:01:53,140
sequencing or autonomous driving or

00:01:49,960 --> 00:01:54,430
natural language processing so slightly

00:01:53,140 --> 00:01:55,390
different set of use cases slightly

00:01:54,430 --> 00:01:57,009
different sort of infrastructure

00:01:55,390 --> 00:02:00,369
requirements so that's kind of how we go

00:01:57,009 --> 00:02:01,540
away mm-hmm so why understanding looking

00:02:00,369 --> 00:02:04,600
at the business the people we talk to

00:02:01,540 --> 00:02:07,090
that there's quite a journey from proof

00:02:04,600 --> 00:02:08,649
of concept into production yeah so what

00:02:07,090 --> 00:02:10,149
kind of challenges are people facing to

00:02:08,649 --> 00:02:11,919
make that leap yeah it's a great

00:02:10,149 --> 00:02:13,719
question I mean we see the industry is

00:02:11,919 --> 00:02:14,160
just as this inflection point writing

00:02:13,719 --> 00:02:16,620
down

00:02:14,160 --> 00:02:19,680
really getting large-scale adoption so a

00:02:16,620 --> 00:02:22,440
lot of our customers a lot of enterprise

00:02:19,680 --> 00:02:24,660
are really in the POC phase led by lines

00:02:22,440 --> 00:02:26,550
of business and now they're starting to

00:02:24,660 --> 00:02:28,470
talk to the IT folks about how do they

00:02:26,550 --> 00:02:29,970
productize what they have been doing in

00:02:28,470 --> 00:02:34,230
the POC phase now that they're proved

00:02:29,970 --> 00:02:36,990
their use cases and so we really look at

00:02:34,230 --> 00:02:38,760
multiple aspects to going to production

00:02:36,990 --> 00:02:40,890
right one of them is how do you

00:02:38,760 --> 00:02:42,840
consolidate the data assets you know for

00:02:40,890 --> 00:02:44,730
a lot of companies data migration

00:02:42,840 --> 00:02:46,530
projects have become like a prerequisite

00:02:44,730 --> 00:02:50,130
before they can do any analytics because

00:02:46,530 --> 00:02:52,920
the data is all over the place and so so

00:02:50,130 --> 00:02:54,870
we really advocate having a strategy on

00:02:52,920 --> 00:02:58,110
how do you consolidate the data assets

00:02:54,870 --> 00:03:00,000
in fewer places it's very difficult to

00:02:58,110 --> 00:03:01,680
consolidate data assets in one place if

00:03:00,000 --> 00:03:03,450
you're a large enterprise but you know

00:03:01,680 --> 00:03:06,660
minimize the number of data stores you

00:03:03,450 --> 00:03:09,300
have and also think about decoupling

00:03:06,660 --> 00:03:10,680
compute from storage so you want to have

00:03:09,300 --> 00:03:13,470
different types of applications

00:03:10,680 --> 00:03:16,140
different types of AI tools all pointing

00:03:13,470 --> 00:03:17,970
to the data as it exists as a customer

00:03:16,140 --> 00:03:20,010
or as a as an enterprise you own the

00:03:17,970 --> 00:03:20,640
data and then the application can come

00:03:20,010 --> 00:03:23,070
and go

00:03:20,640 --> 00:03:24,690
today you have SPARC ml four years from

00:03:23,070 --> 00:03:27,270
now you may even have something else for

00:03:24,690 --> 00:03:29,340
AI but your data is not gonna change or

00:03:27,270 --> 00:03:30,000
you want to have that same data sets to

00:03:29,340 --> 00:03:32,490
be managed

00:03:30,000 --> 00:03:35,820
so really unifying the data sets across

00:03:32,490 --> 00:03:39,630
the board it would be would be one of

00:03:35,820 --> 00:03:41,190
the the key strategies in terms of

00:03:39,630 --> 00:03:43,680
product define the other challenge that

00:03:41,190 --> 00:03:45,900
really we see a lot of enterprises going

00:03:43,680 --> 00:03:48,120
through is how do they scale with

00:03:45,900 --> 00:03:51,209
performance and how do they scale

00:03:48,120 --> 00:03:53,790
without breaking the bank right so you

00:03:51,209 --> 00:03:56,010
know we've heard about linear scaling

00:03:53,790 --> 00:03:57,780
right scaling performance linearly well

00:03:56,010 --> 00:03:59,880
you don't want to scale cost linearly

00:03:57,780 --> 00:04:01,590
because the type of data that you

00:03:59,880 --> 00:04:03,660
accumulate over time may not have the

00:04:01,590 --> 00:04:05,280
same value right so the other that is

00:04:03,660 --> 00:04:07,350
generated recently you know a month ago

00:04:05,280 --> 00:04:08,790
may have a different value then there is

00:04:07,350 --> 00:04:10,890
that's been sitting it on for a couple

00:04:08,790 --> 00:04:13,050
years so how do you optimize your

00:04:10,890 --> 00:04:15,180
infrastructure investment to be able to

00:04:13,050 --> 00:04:16,680
cater to both the performance aspect of

00:04:15,180 --> 00:04:18,090
scaling as well as the capacity aspect

00:04:16,680 --> 00:04:20,010
of scaling so that's another challenge

00:04:18,090 --> 00:04:22,770
that we see and then finally the last

00:04:20,010 --> 00:04:24,750
challenge we see is not many people talk

00:04:22,770 --> 00:04:26,830
about it because it's not mature in the

00:04:24,750 --> 00:04:28,660
sense that the

00:04:26,830 --> 00:04:30,879
aspect of greater governance right data

00:04:28,660 --> 00:04:33,580
governance is a big topic in business

00:04:30,879 --> 00:04:36,490
intelligence but we don't see a lot of

00:04:33,580 --> 00:04:38,919
greater governance discussions in the AI

00:04:36,490 --> 00:04:40,479
space because it's so nice and right but

00:04:38,919 --> 00:04:43,270
once you start product af-- eyeing your

00:04:40,479 --> 00:04:45,280
AI platforms then you know things like

00:04:43,270 --> 00:04:48,639
how do you secure date your data how do

00:04:45,280 --> 00:04:50,669
you make sure the data is robust against

00:04:48,639 --> 00:04:52,030
failures how do you do data quality

00:04:50,669 --> 00:04:54,129
management

00:04:52,030 --> 00:04:56,139
you know lineage metadata tracking all

00:04:54,129 --> 00:04:57,729
those things become very significant so

00:04:56,139 --> 00:04:59,440
those are some of the challenges we see

00:04:57,729 --> 00:05:00,009
as Enterprise is starting to move into

00:04:59,440 --> 00:05:02,740
production

00:05:00,009 --> 00:05:04,389
mm-hm and obviously it's it's a lot of

00:05:02,740 --> 00:05:05,380
different challenges but I think it's

00:05:04,389 --> 00:05:07,120
one step at a time

00:05:05,380 --> 00:05:08,650
mmm-hmm you know one of things you were

00:05:07,120 --> 00:05:10,659
talking about was bringing your data

00:05:08,650 --> 00:05:12,759
together which brings up data like quite

00:05:10,659 --> 00:05:14,440
a buzzword these days how do you define

00:05:12,759 --> 00:05:16,090
a data like from an infrastructure and

00:05:14,440 --> 00:05:18,969
platform perspective yeah it's great

00:05:16,090 --> 00:05:21,669
question so at Dell EMC if you've been

00:05:18,969 --> 00:05:23,800
looking at these challenges that our

00:05:21,669 --> 00:05:26,229
customers have been having in terms of

00:05:23,800 --> 00:05:28,539
going to migrating to large-scale

00:05:26,229 --> 00:05:30,280
production and we have built out this

00:05:28,539 --> 00:05:31,750
concept of data Lake to essentially

00:05:30,280 --> 00:05:34,210
address the pain points we just talked

00:05:31,750 --> 00:05:37,060
about right so one of the requirements

00:05:34,210 --> 00:05:38,949
for a data Lake which we don't want it

00:05:37,060 --> 00:05:40,509
to be a buzzer we want it to be an

00:05:38,949 --> 00:05:41,860
architectural paradigm that we can

00:05:40,509 --> 00:05:43,870
actually place requirements towards

00:05:41,860 --> 00:05:46,569
right so one of the requirements there

00:05:43,870 --> 00:05:48,759
is to be able to consolidate data from

00:05:46,569 --> 00:05:51,580
different places and be able to manage

00:05:48,759 --> 00:05:54,550
data data in a very unified way right

00:05:51,580 --> 00:05:57,610
and to be able to do that you need that

00:05:54,550 --> 00:05:59,110
data Lake to be able to cater to

00:05:57,610 --> 00:06:00,819
different types of applications that

00:05:59,110 --> 00:06:02,770
have different types of signatures when

00:06:00,819 --> 00:06:03,789
it comes to accessing that data some

00:06:02,770 --> 00:06:06,520
applications are real-time

00:06:03,789 --> 00:06:08,050
others are batch mode and some

00:06:06,520 --> 00:06:10,120
applications have certain latency

00:06:08,050 --> 00:06:13,409
requirements so you have to be able to

00:06:10,120 --> 00:06:16,750
honor those at the same time you have to

00:06:13,409 --> 00:06:19,389
look at data not as a single homogenous

00:06:16,750 --> 00:06:20,139
unit but data as having different amount

00:06:19,389 --> 00:06:21,969
of value or

00:06:20,139 --> 00:06:25,000
being accessed differently so you have

00:06:21,969 --> 00:06:27,009
hard data which is usually recently

00:06:25,000 --> 00:06:28,360
generated data you have cold data you

00:06:27,009 --> 00:06:32,740
need to be able to tear the data between

00:06:28,360 --> 00:06:34,180
the two you know transparently and data

00:06:32,740 --> 00:06:36,250
governance right a data layer should be

00:06:34,180 --> 00:06:39,279
able to deliver data governance out of

00:06:36,250 --> 00:06:40,750
the box so that every single AI tool or

00:06:39,279 --> 00:06:43,030
application you build

00:06:40,750 --> 00:06:44,800
and how to rethink and redesign data

00:06:43,030 --> 00:06:49,000
governance so these are some of the

00:06:44,800 --> 00:06:51,160
requirements that that we put on on a

00:06:49,000 --> 00:06:52,870
data Lake and that's our strategy in

00:06:51,160 --> 00:06:55,660
terms of being able to build solutions

00:06:52,870 --> 00:06:57,040
that would address those requirements we

00:06:55,660 --> 00:06:59,050
study a lot about how people are

00:06:57,040 --> 00:07:00,610
deploying their digital assets and we

00:06:59,050 --> 00:07:02,830
know that there's the edge there's

00:07:00,610 --> 00:07:05,200
on-prem there's the cloud what kind of

00:07:02,830 --> 00:07:06,730
trends do you see with AI on those in

00:07:05,200 --> 00:07:09,940
those three areas yeah that's that's a

00:07:06,730 --> 00:07:12,130
good question - so the trend that we are

00:07:09,940 --> 00:07:14,520
looking at these days is typically the

00:07:12,130 --> 00:07:16,390
inferencing happening in the edge

00:07:14,520 --> 00:07:17,919
although there are some innovative

00:07:16,390 --> 00:07:19,600
companies that are doing training at the

00:07:17,919 --> 00:07:21,850
edge what's called as online learning

00:07:19,600 --> 00:07:23,500
but that's few and far between

00:07:21,850 --> 00:07:26,020
most of the influencing happens at the

00:07:23,500 --> 00:07:28,930
edge most of the training happens in the

00:07:26,020 --> 00:07:31,690
data center right that's the kind of the

00:07:28,930 --> 00:07:33,669
common trend that we see and then when

00:07:31,690 --> 00:07:35,950
we talk about the data center then the

00:07:33,669 --> 00:07:37,930
obvious question is on a lot of people's

00:07:35,950 --> 00:07:39,580
minds is whether I go on Prem or whether

00:07:37,930 --> 00:07:41,680
I go to the cloud right that seems to be

00:07:39,580 --> 00:07:44,410
the a lot of questions you know around

00:07:41,680 --> 00:07:46,450
that the answer I think is pretty simple

00:07:44,410 --> 00:07:48,490
right it really depends on where your

00:07:46,450 --> 00:07:50,560
data exists today if your data is

00:07:48,490 --> 00:07:52,390
generated in the cloud most of it is

00:07:50,560 --> 00:07:54,970
generated in the cloud there it doesn't

00:07:52,390 --> 00:07:58,030
make sense to build infrastructure on

00:07:54,970 --> 00:08:00,160
Prem and move the data over if your data

00:07:58,030 --> 00:08:02,770
is generated on Prem it doesn't make

00:08:00,160 --> 00:08:04,240
sense to build analytics infrastructure

00:08:02,770 --> 00:08:06,820
in the cloud and move the data over so

00:08:04,240 --> 00:08:08,530
really the applications and the servers

00:08:06,820 --> 00:08:10,870
the services need to come to where the

00:08:08,530 --> 00:08:12,250
data exists would be the kind of the

00:08:10,870 --> 00:08:13,840
design paradigm you need to follow when

00:08:12,250 --> 00:08:17,380
it comes to on Prem versus cloud

00:08:13,840 --> 00:08:18,820
decision-making mmm-hmm so since it's

00:08:17,380 --> 00:08:20,919
going more production it brings up

00:08:18,820 --> 00:08:23,740
production operational kinds of things

00:08:20,919 --> 00:08:26,950
you know do you see DevOps practices

00:08:23,740 --> 00:08:30,460
being used in AI yeah I mean we are it

00:08:26,950 --> 00:08:33,909
is early days I would say DevOps has

00:08:30,460 --> 00:08:36,039
been primarily a software management or

00:08:33,909 --> 00:08:39,490
software development buzzword so far it

00:08:36,039 --> 00:08:42,130
hasn't really gotten internalized on the

00:08:39,490 --> 00:08:44,640
on the AI side but we do see in some

00:08:42,130 --> 00:08:47,130
industries like for example in the

00:08:44,640 --> 00:08:51,020
algorithmic trading sector right

00:08:47,130 --> 00:08:53,690
companies that build algorithms for for

00:08:51,020 --> 00:08:56,180
automated training the key thing about

00:08:53,690 --> 00:08:59,029
the key aspect of of that particular use

00:08:56,180 --> 00:09:02,360
case is that the assumptions to train

00:08:59,029 --> 00:09:04,720
the AI models change very rapidly so you

00:09:02,360 --> 00:09:08,510
can't afford to have like a training

00:09:04,720 --> 00:09:10,550
project followed by you know influencing

00:09:08,510 --> 00:09:13,220
and you need to you need to have rapid

00:09:10,550 --> 00:09:15,080
development and rapid loop of innovation

00:09:13,220 --> 00:09:16,760
between training and and interaction and

00:09:15,080 --> 00:09:18,950
influencing and production so we see

00:09:16,760 --> 00:09:22,220
those kind of customers following DevOps

00:09:18,950 --> 00:09:25,279
models and our strategy with regards to

00:09:22,220 --> 00:09:27,320
how to rule DevOps is really provide a

00:09:25,279 --> 00:09:31,540
solution that can seamlessly transform

00:09:27,320 --> 00:09:34,399
from the desktop to the data center so

00:09:31,540 --> 00:09:36,740
and there are certain requirements for

00:09:34,399 --> 00:09:38,390
that so one of the requirements is focus

00:09:36,740 --> 00:09:40,100
on containerized platforms because those

00:09:38,390 --> 00:09:42,890
are a lot easier to migrate from the

00:09:40,100 --> 00:09:46,760
desktop to a data center and so you can

00:09:42,890 --> 00:09:49,370
build out a Dell Precision workstation

00:09:46,760 --> 00:09:52,700
written with an eye salon as a data Lake

00:09:49,370 --> 00:09:54,680
right because it provides a lot of those

00:09:52,700 --> 00:09:56,750
features that we talked about and then

00:09:54,680 --> 00:09:58,880
you can expand that to a large scale

00:09:56,750 --> 00:10:00,529
production in your data center using

00:09:58,880 --> 00:10:02,000
containerized frameworks and you can

00:10:00,529 --> 00:10:04,279
build a nice warm cluster and

00:10:02,000 --> 00:10:06,800
automatically tear data across so that

00:10:04,279 --> 00:10:08,870
those are the kind of solutions that

00:10:06,800 --> 00:10:11,029
we're investing in to be able to do that

00:10:08,870 --> 00:10:12,350
DevOps model ok this is all been pretty

00:10:11,029 --> 00:10:15,970
interesting thank you thanks for your

00:10:12,350 --> 00:10:15,970
time today thank you thank you so much

00:10:21,580 --> 00:10:23,640

YouTube URL: https://www.youtube.com/watch?v=go9-es9xh-c


