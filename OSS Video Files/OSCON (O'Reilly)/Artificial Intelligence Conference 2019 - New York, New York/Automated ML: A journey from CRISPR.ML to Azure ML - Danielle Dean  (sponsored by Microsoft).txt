Title: Automated ML: A journey from CRISPR.ML to Azure ML - Danielle Dean  (sponsored by Microsoft)
Publication date: 2019-04-18
Playlist: Artificial Intelligence Conference 2019 - New York, New York
Description: 
	View more keynotes and sessions from AI NY 2019:
https://oreilly.com/go/ainy19

Automated ML is at the forefront of Microsoft’s push to make Azure ML an end-to-end solution for anyone who wants to build and train models that make predictions from data and then deploy them anywhere. Join Danielle Dean for a surprising conversation about a data scientist’s dilemma, a researcher’s ingenuity, and how cloud, data, and AI came together to help build automated ML.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:03,389
so it's been mentioned my name is

00:00:01,680 --> 00:00:05,370
Danielle Dean I'm a data scientist at

00:00:03,389 --> 00:00:08,099
Microsoft and I'm excited to be here

00:00:05,370 --> 00:00:09,809
today to tell you a story about me and

00:00:08,099 --> 00:00:11,519
apparently you all know me because Ben

00:00:09,809 --> 00:00:14,429
just told you about how I'm gonna review

00:00:11,519 --> 00:00:16,170
committee a friend I'm telling you also

00:00:14,429 --> 00:00:18,240
about a story of a friend of mine who

00:00:16,170 --> 00:00:20,640
works in the floor above me and the our

00:00:18,240 --> 00:00:24,060
common dilemma the led to the birth of

00:00:20,640 --> 00:00:26,369
automated machine learning on Azure so I

00:00:24,060 --> 00:00:28,740
always loved learning from data to

00:00:26,369 --> 00:00:30,390
understand how the world works when I

00:00:28,740 --> 00:00:31,289
graduated with my PhD in quantitative

00:00:30,390 --> 00:00:33,570
psychology

00:00:31,289 --> 00:00:35,700
I was hired as a data scientist I was

00:00:33,570 --> 00:00:38,780
excited to tackle huge volumes of data

00:00:35,700 --> 00:00:41,640
to try to understand patterns and trends

00:00:38,780 --> 00:00:44,610
now it was really cool to solve real

00:00:41,640 --> 00:00:46,410
problems I built lots of models did lots

00:00:44,610 --> 00:00:48,539
of different pre-processing pipeline's

00:00:46,410 --> 00:00:51,090
tried to understand do I need to remove

00:00:48,539 --> 00:00:53,489
outliers what type of model do I need do

00:00:51,090 --> 00:00:55,530
I need to reprocess this data there's a

00:00:53,489 --> 00:00:59,070
lot of decisions that go into building

00:00:55,530 --> 00:01:01,500
an m/l system and ultimately these

00:00:59,070 --> 00:01:03,989
decisions that you take they end up

00:01:01,500 --> 00:01:06,450
impacting the accuracy of the model that

00:01:03,989 --> 00:01:08,610
you end up using the combination of data

00:01:06,450 --> 00:01:10,890
pre-processing the model family the

00:01:08,610 --> 00:01:13,650
hyper parameter tuning now people always

00:01:10,890 --> 00:01:16,770
ask for the secret to success and I'm a

00:01:13,650 --> 00:01:20,040
true hardcore data scientist right so I

00:01:16,770 --> 00:01:22,200
know all the answers I know for example

00:01:20,040 --> 00:01:24,420
there's these chi Chi's from psyche

00:01:22,200 --> 00:01:26,400
learned that you can look up and there's

00:01:24,420 --> 00:01:28,799
rules of thumb if you have more than 50

00:01:26,400 --> 00:01:31,439
samples this is what you do oh that's

00:01:28,799 --> 00:01:34,020
not enough data I would have all the

00:01:31,439 --> 00:01:35,280
answers to all these questions but I'm

00:01:34,020 --> 00:01:37,140
going to tell you a little secret today

00:01:35,280 --> 00:01:38,369
because this is not a big room and

00:01:37,140 --> 00:01:41,119
there's not many of you and you're not

00:01:38,369 --> 00:01:41,119
going to tell anyone right

00:01:41,600 --> 00:01:47,520
the real answer to all of those

00:01:44,579 --> 00:01:50,520
questions what do you do how do you

00:01:47,520 --> 00:01:53,880
decide this is the real answer you just

00:01:50,520 --> 00:01:56,100
try trial and error now don't get me

00:01:53,880 --> 00:01:58,500
wrong the rules of thumb are useful the

00:01:56,100 --> 00:02:01,200
guides are useful but at the end of the

00:01:58,500 --> 00:02:03,689
day we just have to try stuff that comes

00:02:01,200 --> 00:02:06,930
down to trial and error and here comes

00:02:03,689 --> 00:02:09,239
the data scientists dilemma how much of

00:02:06,930 --> 00:02:10,830
this do you do do you keep going do you

00:02:09,239 --> 00:02:12,420
keep doing do you keep trying different

00:02:10,830 --> 00:02:13,590
things you keep trying different model

00:02:12,420 --> 00:02:15,209
families to key

00:02:13,590 --> 00:02:17,730
to near hyper parameters do you keep

00:02:15,209 --> 00:02:19,470
reprocessing that data over and over and

00:02:17,730 --> 00:02:23,040
over again this trial and error how much

00:02:19,470 --> 00:02:24,840
do you do and this is a dilemma when do

00:02:23,040 --> 00:02:27,959
you actually stop because let's admit it

00:02:24,840 --> 00:02:29,489
it's kind of boring so I'm going to tell

00:02:27,959 --> 00:02:31,800
you the story now of a friend of mine

00:02:29,489 --> 00:02:35,130
Nicola fousey who's working on a really

00:02:31,800 --> 00:02:37,830
cool problem called CRISPR ml CRISPR is

00:02:35,130 --> 00:02:39,510
a tool that helps in gene editing and

00:02:37,830 --> 00:02:41,580
the goal of gene editing is to empower

00:02:39,510 --> 00:02:44,250
biomedical researchers to make precise

00:02:41,580 --> 00:02:46,799
targeted changes to the genome in order

00:02:44,250 --> 00:02:48,780
to do things like treat diseases and he

00:02:46,799 --> 00:02:50,880
works on the floor above me and he's

00:02:48,780 --> 00:02:53,370
doing lots of cool stuff as you can see

00:02:50,880 --> 00:02:57,090
from these news articles articles from

00:02:53,370 --> 00:02:59,130
nature and The Economist for example now

00:02:57,090 --> 00:03:01,319
of course if you believe Wired this

00:02:59,130 --> 00:03:05,730
could also mean the end of life as we

00:03:01,319 --> 00:03:09,560
know it or hopefully it can also just

00:03:05,730 --> 00:03:13,140
mean some really good tweets and memes

00:03:09,560 --> 00:03:18,570
after all one does not simply edit the

00:03:13,140 --> 00:03:21,060
human germline now of course it's not

00:03:18,570 --> 00:03:22,709
all fun and games the hard part comes in

00:03:21,060 --> 00:03:25,049
how to figure out where to cut the

00:03:22,709 --> 00:03:27,390
genome it turns out that there's 18

00:03:25,049 --> 00:03:29,819
billion combinations of genes

00:03:27,390 --> 00:03:31,769
grr neighs and off target locations

00:03:29,819 --> 00:03:34,590
which are important to understand in

00:03:31,769 --> 00:03:36,900
order to understand where to cut now

00:03:34,590 --> 00:03:39,480
what Niccolo and his fellow researchers

00:03:36,900 --> 00:03:41,430
at Microsoft Research and the Broad

00:03:39,480 --> 00:03:43,170
Institute at MIT and Harvard are doing

00:03:41,430 --> 00:03:45,269
as they're approaching this as a machine

00:03:43,170 --> 00:03:48,269
learning problem looking to predict the

00:03:45,269 --> 00:03:50,790
likelihood of gene edit success and so

00:03:48,269 --> 00:03:52,709
basically they got this data they have a

00:03:50,790 --> 00:03:54,150
machine learning problem they have the

00:03:52,709 --> 00:03:56,910
question the metrics they want to

00:03:54,150 --> 00:04:00,180
optimize and then they spent almost six

00:03:56,910 --> 00:04:02,010
months full-time looking at all of the

00:04:00,180 --> 00:04:04,650
different model families tuning hyper

00:04:02,010 --> 00:04:06,359
parameters slicing and dicing the data

00:04:04,650 --> 00:04:08,459
trying to build the system that would

00:04:06,359 --> 00:04:11,970
predict this well and solve this problem

00:04:08,459 --> 00:04:14,579
that's a lot of time and research spent

00:04:11,970 --> 00:04:18,060
and so again he's coming to that same

00:04:14,579 --> 00:04:20,940
data scientist dilemma developing ml and

00:04:18,060 --> 00:04:24,150
AI solutions involved spending a lot of

00:04:20,940 --> 00:04:26,400
time tuning swapping components in and

00:04:24,150 --> 00:04:27,840
out and figuring out what is the right

00:04:26,400 --> 00:04:29,370
way to approach the

00:04:27,840 --> 00:04:32,820
that trial-and-error thought I was

00:04:29,370 --> 00:04:34,170
mentioning so somebody has got to figure

00:04:32,820 --> 00:04:36,360
this out right there's there's really

00:04:34,170 --> 00:04:38,040
smart people in this community really

00:04:36,360 --> 00:04:39,690
smart people out there how do we solve

00:04:38,040 --> 00:04:41,790
this problem it's not we're not the

00:04:39,690 --> 00:04:43,770
first ones to run into it of course

00:04:41,790 --> 00:04:45,660
there's things like grid search random

00:04:43,770 --> 00:04:48,120
search or even more advanced methods

00:04:45,660 --> 00:04:50,010
such as Bayesian optimization and you

00:04:48,120 --> 00:04:52,440
know what these methods can work pretty

00:04:50,010 --> 00:04:54,150
well for example looking at tuning a

00:04:52,440 --> 00:04:56,610
deeply neural network for something like

00:04:54,150 --> 00:04:58,020
image recognition human tuned en ends

00:04:56,610 --> 00:04:59,940
compared to what you can do with these

00:04:58,020 --> 00:05:01,740
types of methods you can do a lot better

00:04:59,940 --> 00:05:02,850
and so there's some really good methods

00:05:01,740 --> 00:05:05,970
out there for doing this more

00:05:02,850 --> 00:05:07,590
efficiently and of course when anytime

00:05:05,970 --> 00:05:10,710
you say it works there's always a big

00:05:07,590 --> 00:05:13,170
asterisk and that is the cases it works

00:05:10,710 --> 00:05:15,690
if you have just a few hyper parameters

00:05:13,170 --> 00:05:18,480
you're willing to wait or spend a lot of

00:05:15,690 --> 00:05:21,570
money on compute power and most of your

00:05:18,480 --> 00:05:23,220
parameters are continuous well this is

00:05:21,570 --> 00:05:25,530
obviously not always the case in the

00:05:23,220 --> 00:05:27,930
real world and in the case of the CRISPR

00:05:25,530 --> 00:05:30,480
ML problem they had a lot of hyper of

00:05:27,930 --> 00:05:33,090
parameters not just a few they needed to

00:05:30,480 --> 00:05:34,650
solve this quickly and most of their

00:05:33,090 --> 00:05:35,880
parameters were not continuous there's a

00:05:34,650 --> 00:05:38,580
lot of things that they wanted to look

00:05:35,880 --> 00:05:40,620
at that were discrete and so they need a

00:05:38,580 --> 00:05:42,540
completely different approach and so

00:05:40,620 --> 00:05:46,830
what Niccolo did is he built a new

00:05:42,540 --> 00:05:48,990
approach on two key intuitions the first

00:05:46,830 --> 00:05:51,120
intuition is we're going to sample

00:05:48,990 --> 00:05:53,550
instantiation of machine learning

00:05:51,120 --> 00:05:56,010
pipelines which is going to make this

00:05:53,550 --> 00:05:57,660
space discrete so in other words we have

00:05:56,010 --> 00:05:59,490
all of those different pre-processing

00:05:57,660 --> 00:06:01,890
things we could do things like pca

00:05:59,490 --> 00:06:03,750
removing outliers normalizing the data

00:06:01,890 --> 00:06:05,430
we have all these different model

00:06:03,750 --> 00:06:07,350
families we can do things like linear

00:06:05,430 --> 00:06:08,850
regression random for us a deep neural

00:06:07,350 --> 00:06:10,350
network and then we have all those

00:06:08,850 --> 00:06:12,180
different hyper parameters that we can

00:06:10,350 --> 00:06:13,290
go after so we're going to make this

00:06:12,180 --> 00:06:15,660
space discreet

00:06:13,290 --> 00:06:17,190
the second key intuition is we're not

00:06:15,660 --> 00:06:19,020
going to treat each problem as a

00:06:17,190 --> 00:06:20,700
complete as each data set as a

00:06:19,020 --> 00:06:22,920
completely new problem we're gonna

00:06:20,700 --> 00:06:26,010
actually reason across these different

00:06:22,920 --> 00:06:28,890
data sets so those two key intuitions

00:06:26,010 --> 00:06:30,570
again which was looking at each data set

00:06:28,890 --> 00:06:33,390
not as a new problem and turning it into

00:06:30,570 --> 00:06:35,160
discrete search space and what he's

00:06:33,390 --> 00:06:37,140
doing is in this new approach is

00:06:35,160 --> 00:06:39,060
actually building a recommender system

00:06:37,140 --> 00:06:40,560
for machine learning pipelines so you

00:06:39,060 --> 00:06:41,040
know your normal recommender system

00:06:40,560 --> 00:06:43,290
you'll have

00:06:41,040 --> 00:06:45,330
users and you'll have items rather than

00:06:43,290 --> 00:06:47,460
that in this case we actually have data

00:06:45,330 --> 00:06:49,440
sets that we're looking over and then

00:06:47,460 --> 00:06:50,940
machine learning pipelines and then

00:06:49,440 --> 00:06:52,980
additionally he's going to have a

00:06:50,940 --> 00:06:55,110
probabilistic framework for to drive

00:06:52,980 --> 00:06:58,230
exploration and exploitation basically

00:06:55,110 --> 00:07:00,120
how to search that space and so the key

00:06:58,230 --> 00:07:01,560
breakthrough was to take uncertainty

00:07:00,120 --> 00:07:03,420
into account and incorporate a

00:07:01,560 --> 00:07:05,940
probabilistic model to determine which

00:07:03,420 --> 00:07:07,410
machine learning pipeline to try next so

00:07:05,940 --> 00:07:10,020
here I'm showing how the meta learning

00:07:07,410 --> 00:07:12,870
unfolds through animation so the chart

00:07:10,020 --> 00:07:14,400
on the y-axis is the accuracy and it

00:07:12,870 --> 00:07:16,740
you're seeing that it's improving as

00:07:14,400 --> 00:07:18,480
pipelines are evaluated and on the

00:07:16,740 --> 00:07:20,010
bottom all these little dots are

00:07:18,480 --> 00:07:22,890
basically the different machine learning

00:07:20,010 --> 00:07:25,050
pipelines in a latent space and then

00:07:22,890 --> 00:07:27,480
it's converging on things that have

00:07:25,050 --> 00:07:29,760
higher predicted performance so the

00:07:27,480 --> 00:07:32,400
lighter color on the bottom left as well

00:07:29,760 --> 00:07:34,440
as lower uncertainty the darker color on

00:07:32,400 --> 00:07:36,360
the bottom right and so basically what

00:07:34,440 --> 00:07:38,160
this means is the meta learner is going

00:07:36,360 --> 00:07:40,080
to explore the most promising

00:07:38,160 --> 00:07:42,270
possibilities without having to do

00:07:40,080 --> 00:07:44,190
exhaustive search and converge on the

00:07:42,270 --> 00:07:47,600
best pipeline for a given data set much

00:07:44,190 --> 00:07:49,980
faster than brute force approaches and

00:07:47,600 --> 00:07:52,320
so this worked really well for his

00:07:49,980 --> 00:07:54,630
problem using CRISPR ml researchers

00:07:52,320 --> 00:07:58,650
observed a 20% improvement in accuracy

00:07:54,630 --> 00:08:01,650
and a 50% savings and cost in time per

00:07:58,650 --> 00:08:05,520
gene this was amazing progress in gene

00:08:01,650 --> 00:08:06,840
editing now of course Niccolo is not the

00:08:05,520 --> 00:08:08,250
only one with the data scientists ulema

00:08:06,840 --> 00:08:09,740
has you guys all heard of how about my

00:08:08,250 --> 00:08:12,300
problems at the beginning of this talk

00:08:09,740 --> 00:08:14,130
he thought he could help other people

00:08:12,300 --> 00:08:16,050
with this dilemma and so we have this

00:08:14,130 --> 00:08:17,850
hackathon at Microsoft every year called

00:08:16,050 --> 00:08:19,410
one week where everybody comes together

00:08:17,850 --> 00:08:21,480
and does lots of different hackathons

00:08:19,410 --> 00:08:23,280
and there was a ton of people that came

00:08:21,480 --> 00:08:25,170
and said I have this data set and I only

00:08:23,280 --> 00:08:26,670
have a few days how do I get a pretty

00:08:25,170 --> 00:08:30,060
good model out of this that I can then

00:08:26,670 --> 00:08:31,740
go use and so Niccolo and his friends in

00:08:30,060 --> 00:08:33,660
research built a simple system that

00:08:31,740 --> 00:08:35,580
would take an input data set and spit

00:08:33,660 --> 00:08:37,290
back out a Python object to all these

00:08:35,580 --> 00:08:39,479
people in the hackathon that they could

00:08:37,290 --> 00:08:41,310
then use to make predictions and so this

00:08:39,479 --> 00:08:44,760
is what I heard about the system and was

00:08:41,310 --> 00:08:46,890
really amazed by its results it can help

00:08:44,760 --> 00:08:49,500
you transform features identify model

00:08:46,890 --> 00:08:51,120
tune parameters of course it's not magic

00:08:49,500 --> 00:08:52,650
you still need to formulate the problem

00:08:51,120 --> 00:08:54,750
you still need to understand what you're

00:08:52,650 --> 00:08:56,040
trying to optimize you still need

00:08:54,750 --> 00:08:58,230
to understand how you're going to use

00:08:56,040 --> 00:09:00,540
the results but it helps this entire

00:08:58,230 --> 00:09:03,630
inner loop of the data scientist dilemma

00:09:00,540 --> 00:09:06,360
and so what I heard about this started

00:09:03,630 --> 00:09:07,590
using the toolkit how me and lots of

00:09:06,360 --> 00:09:09,210
other people at the company started

00:09:07,590 --> 00:09:11,760
asking questions like we have this

00:09:09,210 --> 00:09:13,650
toolkit that helps to spit out a Python

00:09:11,760 --> 00:09:15,960
object it helps accelerate your

00:09:13,650 --> 00:09:18,030
development we all have this tool that

00:09:15,960 --> 00:09:19,800
leverages and helps you with open source

00:09:18,030 --> 00:09:22,140
Python models called Azure machine

00:09:19,800 --> 00:09:24,090
learning how can we actually bring these

00:09:22,140 --> 00:09:26,250
together to build a product that lets

00:09:24,090 --> 00:09:27,900
you use automated machine learning and

00:09:26,250 --> 00:09:31,230
so when these questions started rising

00:09:27,900 --> 00:09:33,750
yes please let's do this and less than a

00:09:31,230 --> 00:09:36,120
year later this capability is now

00:09:33,750 --> 00:09:38,670
available within modular machine

00:09:36,120 --> 00:09:40,530
learning as well as power bi and these

00:09:38,670 --> 00:09:42,270
links that are at the bottom of the page

00:09:40,530 --> 00:09:43,710
I've tweeted out earlier so that you can

00:09:42,270 --> 00:09:47,040
take advantage of those and read the

00:09:43,710 --> 00:09:48,900
papers and research behind it so then of

00:09:47,040 --> 00:09:51,000
course now that it's available within a

00:09:48,900 --> 00:09:53,460
product we have companies using it such

00:09:51,000 --> 00:09:55,320
as BP to accelerate their deliverables

00:09:53,460 --> 00:09:57,540
there's things built in such as model

00:09:55,320 --> 00:09:59,450
explain ability to actually help with

00:09:57,540 --> 00:10:01,350
some of the enterprise requirements and

00:09:59,450 --> 00:10:02,970
automated machine learning is just one

00:10:01,350 --> 00:10:04,920
example of Microsoft turning research

00:10:02,970 --> 00:10:06,720
into product there's been a lot of

00:10:04,920 --> 00:10:09,570
breakthroughs in areas such as vision

00:10:06,720 --> 00:10:11,640
speech and language that are now

00:10:09,570 --> 00:10:13,980
available through Azure cognitive

00:10:11,640 --> 00:10:16,589
services that are that you can take

00:10:13,980 --> 00:10:18,570
advantage of here's some example

00:10:16,589 --> 00:10:21,270
customers that are leveraging it but

00:10:18,570 --> 00:10:22,740
more importantly how do you do you want

00:10:21,270 --> 00:10:25,080
to avoid the data scientists dilemma

00:10:22,740 --> 00:10:27,150
come visit us at the Microsoft booth or

00:10:25,080 --> 00:10:30,800
a tender session on automated machine

00:10:27,150 --> 00:10:30,800
learning tomorrow thank you all

00:10:37,620 --> 00:10:39,680

YouTube URL: https://www.youtube.com/watch?v=8mVua5bBGps


