Title: Applied machine learning at Facebook - Kim Hazelwood (Facebook)
Publication date: 2019-04-18
Playlist: Artificial Intelligence Conference 2019 - New York, New York
Description: 
	View more keynotes and sessions from AI NY 2019:
https://oreilly.com/go/ainy19

Applied Machine Learning at Facebook

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:04,740
so let me tell you a little bit about

00:00:02,040 --> 00:00:06,810
what we have to do and the hardware and

00:00:04,740 --> 00:00:09,840
software front in order to support all

00:00:06,810 --> 00:00:11,700
of that so I'll talk to you about the

00:00:09,840 --> 00:00:13,559
hardware infrastructure and then the

00:00:11,700 --> 00:00:15,660
software platforms and frameworks that

00:00:13,559 --> 00:00:18,660
we have built in-house in order to

00:00:15,660 --> 00:00:22,410
support all of that so we'll start with

00:00:18,660 --> 00:00:24,390
infrastructure so before delving into

00:00:22,410 --> 00:00:26,310
that infrastructure let me give you a

00:00:24,390 --> 00:00:29,640
quick refresher of the three stages of

00:00:26,310 --> 00:00:32,759
ml that we're supporting so the stage

00:00:29,640 --> 00:00:36,329
one is that we are taking unstructured

00:00:32,759 --> 00:00:39,000
data and manipulating it in order to

00:00:36,329 --> 00:00:40,530
drive our machine learning models then

00:00:39,000 --> 00:00:42,840
we have stage two which is training

00:00:40,530 --> 00:00:44,399
where we're taking all of those this

00:00:42,840 --> 00:00:46,590
data and figuring out which we care the

00:00:44,399 --> 00:00:48,960
most about and then finally we're

00:00:46,590 --> 00:00:52,829
deploying into production for the third

00:00:48,960 --> 00:00:57,210
phase of machine learning so we have

00:00:52,829 --> 00:01:00,059
designed and released Hardware servers

00:00:57,210 --> 00:01:02,100
for each of these stages we are a member

00:01:00,059 --> 00:01:04,170
of Open Compute so all over the hardware

00:01:02,100 --> 00:01:06,890
that we design we release and open

00:01:04,170 --> 00:01:10,409
source to the community so we have two

00:01:06,890 --> 00:01:12,890
storage platforms that we released to

00:01:10,409 --> 00:01:16,170
handle stage one one is a flash based

00:01:12,890 --> 00:01:20,130
server design one is a disk base server

00:01:16,170 --> 00:01:22,409
design we also have two training

00:01:20,130 --> 00:01:26,040
platforms one is CPU based and one is

00:01:22,409 --> 00:01:29,970
GPU based and then finally for inference

00:01:26,040 --> 00:01:32,009
we have two CPU based platforms one is a

00:01:29,970 --> 00:01:33,509
bit of a hit more heavy hitter dual

00:01:32,009 --> 00:01:37,170
socket platform and one is a single

00:01:33,509 --> 00:01:40,290
socket platform so that's the hardware

00:01:37,170 --> 00:01:43,770
let's move up one layer we provide

00:01:40,290 --> 00:01:45,750
internally a a platform for all of our

00:01:43,770 --> 00:01:47,369
ranking engineers and developers to make

00:01:45,750 --> 00:01:49,350
the entire process pretty easy and

00:01:47,369 --> 00:01:52,890
straightforward so we have a system

00:01:49,350 --> 00:01:55,079
called FB learner that has components

00:01:52,890 --> 00:01:58,439
that handle each of the three stages of

00:01:55,079 --> 00:02:00,180
the ml pipeline we have the FB learner

00:01:58,439 --> 00:02:03,420
feature store for handling all of the

00:02:00,180 --> 00:02:05,460
data doing feature engineering and then

00:02:03,420 --> 00:02:07,530
when we move on to training we have FB

00:02:05,460 --> 00:02:09,989
learner flow which hides the complexity

00:02:07,530 --> 00:02:12,780
of doing scheduling allocation of the

00:02:09,989 --> 00:02:13,200
machines and detecting whether or not

00:02:12,780 --> 00:02:15,510
the

00:02:13,200 --> 00:02:17,430
is better than the previous model and

00:02:15,510 --> 00:02:19,050
then finally we have epi learner

00:02:17,430 --> 00:02:22,769
predictor for deploying out into

00:02:19,050 --> 00:02:26,940
production and tracking whether there

00:02:22,769 --> 00:02:30,660
been any regressions so then one layer

00:02:26,940 --> 00:02:32,790
up we have our frameworks historically

00:02:30,660 --> 00:02:35,099
we had two frameworks we had one that

00:02:32,790 --> 00:02:37,349
was customized for research and one that

00:02:35,099 --> 00:02:39,870
was customized for production one was

00:02:37,349 --> 00:02:41,160
easy to use one was fast and we

00:02:39,870 --> 00:02:43,500
announced last year that we've merged

00:02:41,160 --> 00:02:45,030
these together into PI thwarts 1.0 so

00:02:43,500 --> 00:02:47,099
that you have the look and feel that you

00:02:45,030 --> 00:02:49,440
loved from PI torch with the speed and

00:02:47,099 --> 00:02:52,250
performance that we required for our

00:02:49,440 --> 00:02:52,250
production use cases

00:02:58,750 --> 00:03:00,810

YouTube URL: https://www.youtube.com/watch?v=cdAo1a6rYNo


