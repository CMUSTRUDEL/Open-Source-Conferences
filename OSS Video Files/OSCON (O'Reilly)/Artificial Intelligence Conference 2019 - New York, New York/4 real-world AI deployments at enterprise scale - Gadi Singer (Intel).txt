Title: 4 real-world AI deployments at enterprise scale - Gadi Singer (Intel)
Publication date: 2019-04-18
Playlist: Artificial Intelligence Conference 2019 - New York, New York
Description: 
	View more keynotes and sessions from AI NY 2019:
https://oreilly.com/go/ainy19

Gadi Singer explores four real-world AI deployments at enterprise scale.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,060 --> 00:00:04,440
we should start by looking at the last

00:00:01,860 --> 00:00:06,540
three years four years because three

00:00:04,440 --> 00:00:09,920
four years ago a lot of the work around

00:00:06,540 --> 00:00:13,679
deep learning was about figuring out

00:00:09,920 --> 00:00:16,020
what it can do how it can be applied how

00:00:13,679 --> 00:00:20,310
it can provide this tremendous new set

00:00:16,020 --> 00:00:22,680
of functionality but now we are at a

00:00:20,310 --> 00:00:25,590
different phase we are starting the

00:00:22,680 --> 00:00:26,849
implementation the deployment and some

00:00:25,590 --> 00:00:28,949
things that happened over this period

00:00:26,849 --> 00:00:30,990
first was the introduction of the deep

00:00:28,949 --> 00:00:32,279
learning frameworks and those deep

00:00:30,990 --> 00:00:34,980
learning frameworks allow the

00:00:32,279 --> 00:00:37,079
abstraction and allowed the industry to

00:00:34,980 --> 00:00:39,390
develop all those new topologies all

00:00:37,079 --> 00:00:41,820
those new usages what it allowed us to

00:00:39,390 --> 00:00:44,700
do is to build a completely new set of

00:00:41,820 --> 00:00:47,969
software libraries and graph compilers

00:00:44,700 --> 00:00:49,890
and connect from all of those down to

00:00:47,969 --> 00:00:52,800
the hardware the underlying hardware and

00:00:49,890 --> 00:00:55,289
get value out of the hardware and this

00:00:52,800 --> 00:00:57,960
software this optimized software allows

00:00:55,289 --> 00:01:01,649
us to improve our existing hardware by

00:00:57,960 --> 00:01:04,409
two orders of magnitude and in addition

00:01:01,649 --> 00:01:06,450
to that the hardware itself the CPU

00:01:04,409 --> 00:01:10,140
hardware has been enhanced if you look

00:01:06,450 --> 00:01:12,869
for example the second-generation Zeon's

00:01:10,140 --> 00:01:15,960
cable a scalable processors which were

00:01:12,869 --> 00:01:18,930
launched earlier this year are including

00:01:15,960 --> 00:01:22,860
hardware acceleration that gives you 10

00:01:18,930 --> 00:01:25,890
X 14 X depending on the usage compared

00:01:22,860 --> 00:01:28,200
to previous generations and the last big

00:01:25,890 --> 00:01:31,560
change is the transition to inference

00:01:28,200 --> 00:01:33,750
and if the ratio four years ago was

00:01:31,560 --> 00:01:35,880
about one to one training to inference

00:01:33,750 --> 00:01:37,920
because it was primarily about trying

00:01:35,880 --> 00:01:41,490
out things now the ratio I would

00:01:37,920 --> 00:01:44,189
estimate is about one to four training

00:01:41,490 --> 00:01:47,520
to inference going towards one to ten in

00:01:44,189 --> 00:01:52,290
the coming years so the focus goes to

00:01:47,520 --> 00:01:55,590
inference and most of the inference in

00:01:52,290 --> 00:02:01,500
the clouding in the data center runs on

00:01:55,590 --> 00:02:03,810
CPUs and so this shift and the question

00:02:01,500 --> 00:02:07,700
being asked lead us to three basic

00:02:03,810 --> 00:02:12,120
elements one using the CPUs foundation

00:02:07,700 --> 00:02:14,130
using the latest software allows all the

00:02:12,120 --> 00:02:15,990
users all the companies to

00:02:14,130 --> 00:02:18,690
use the existing hardware that they have

00:02:15,990 --> 00:02:21,330
very efficiently and get a lot of value

00:02:18,690 --> 00:02:24,000
out of that in addition to that as they

00:02:21,330 --> 00:02:26,790
scale as they grow they can add hardware

00:02:24,000 --> 00:02:30,270
that is especially tuned and enhanced

00:02:26,790 --> 00:02:33,420
for that in the CPU the second thing is

00:02:30,270 --> 00:02:36,060
about the specific deep learning most

00:02:33,420 --> 00:02:38,780
intensive type of workload which is

00:02:36,060 --> 00:02:40,770
tensor arithmetic so when you have those

00:02:38,780 --> 00:02:43,920
multi-dimensional array and you do this

00:02:40,770 --> 00:02:46,950
arithmetic it is very demanding and this

00:02:43,920 --> 00:02:51,810
does this new class of custom ASIC

00:02:46,950 --> 00:02:54,690
acceleration and we will introduce later

00:02:51,810 --> 00:02:57,600
this year neural network processors both

00:02:54,690 --> 00:02:58,530
for training the NM PL and for inference

00:02:57,600 --> 00:03:01,740
the n NPI

00:02:58,530 --> 00:03:04,320
in order to address this specialized

00:03:01,740 --> 00:03:06,210
task and it's done highly integrated

00:03:04,320 --> 00:03:10,320
with the rest of the platform and

00:03:06,210 --> 00:03:11,640
finally software software is key we

00:03:10,320 --> 00:03:14,040
talked about the importance to

00:03:11,640 --> 00:03:15,990
optimization it's also important to keep

00:03:14,040 --> 00:03:18,030
the overall environment that includes

00:03:15,990 --> 00:03:20,790
multiple components different type of

00:03:18,030 --> 00:03:24,350
compute to keep it streamlined and keep

00:03:20,790 --> 00:03:24,350
it easy to deploy

00:03:30,270 --> 00:03:32,330

YouTube URL: https://www.youtube.com/watch?v=HU0diqjS5KM


