Title: Toward Ethical AI - Kurt Muehmel (Datahiku)
Publication date: 2019-04-18
Playlist: Artificial Intelligence Conference 2019 - New York, New York
Description: 
	View more keynotes and sessions from AI NY 2019:
https://oreilly.com/go/ainy19

AI technologists must consider the ethical implications of what we're building. Kurt Muehmel explores AI within a broader discussion of the ethics of technology, arguing that inclusivity and collaboration is a necessary answer.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:04,920
I know that we all like to think that AI

00:00:02,610 --> 00:00:06,960
and tech in general is neutral or maybe

00:00:04,920 --> 00:00:07,379
even bent towards doing good in the

00:00:06,960 --> 00:00:09,719
world

00:00:07,379 --> 00:00:11,610
but we can be kidding ourselves if we

00:00:09,719 --> 00:00:13,500
don't recognize the way in which biases

00:00:11,610 --> 00:00:15,809
and discrimination can be built into

00:00:13,500 --> 00:00:17,940
these AI systems and we need to

00:00:15,809 --> 00:00:19,289
recognize our own responsibility and not

00:00:17,940 --> 00:00:21,689
doing enough to prevent that from

00:00:19,289 --> 00:00:24,330
happening and I know that this message

00:00:21,689 --> 00:00:26,160
coming from a guy who looks like me can

00:00:24,330 --> 00:00:28,410
just sound like more virtue signaling

00:00:26,160 --> 00:00:30,539
from another woke millennial but it's

00:00:28,410 --> 00:00:32,969
not that I'm up here talking about this

00:00:30,539 --> 00:00:35,070
because 30 years from now I want to be

00:00:32,969 --> 00:00:37,320
able to look back at my career in this

00:00:35,070 --> 00:00:38,969
field and know that all of us that we

00:00:37,320 --> 00:00:42,000
did everything that we could to do the

00:00:38,969 --> 00:00:45,450
right thing now none of us have set out

00:00:42,000 --> 00:00:47,480
to cause harm not you and not me it's

00:00:45,450 --> 00:00:50,579
not intentional and yet it happens

00:00:47,480 --> 00:00:52,890
biased AI is causing harm and it happens

00:00:50,579 --> 00:00:54,300
over and over again maybe some of you

00:00:52,890 --> 00:00:56,789
have worked at companies where this has

00:00:54,300 --> 00:00:58,859
happened maybe you've even been involved

00:00:56,789 --> 00:01:00,329
in some of these projects and I can only

00:00:58,859 --> 00:01:02,070
imagine the feeling of dread that you

00:01:00,329 --> 00:01:04,199
must have felt when you realize the

00:01:02,070 --> 00:01:06,510
unintended consequences of your hard

00:01:04,199 --> 00:01:08,340
work and I can only imagine the feelings

00:01:06,510 --> 00:01:10,860
of the victims of some of these bias

00:01:08,340 --> 00:01:12,659
systems when it had real and lasting

00:01:10,860 --> 00:01:16,650
impact on their lives

00:01:12,659 --> 00:01:20,310
I needed loan denied a medical condition

00:01:16,650 --> 00:01:22,799
ignored or misdiagnosed an innocent

00:01:20,310 --> 00:01:24,630
person arrested the fact that this keeps

00:01:22,799 --> 00:01:26,820
happening at companies of different size

00:01:24,630 --> 00:01:28,350
and scale and different levels of AI

00:01:26,820 --> 00:01:30,360
sophistication makes us think that

00:01:28,350 --> 00:01:32,070
there's something systemic going on in

00:01:30,360 --> 00:01:35,520
the way that we're developing these

00:01:32,070 --> 00:01:36,750
systems so what's the common factor most

00:01:35,520 --> 00:01:38,280
of us in this room are technologists

00:01:36,750 --> 00:01:39,810
right so let's start from the bottom of

00:01:38,280 --> 00:01:42,600
the stack and work our way up let's

00:01:39,810 --> 00:01:44,189
debug so I picked one of delic whose

00:01:42,600 --> 00:01:45,899
standard architecture slides right it's

00:01:44,189 --> 00:01:48,149
not really the point it's nice and

00:01:45,899 --> 00:01:49,979
complex as these things always are but

00:01:48,149 --> 00:01:51,600
where can we point to in that diagram

00:01:49,979 --> 00:01:53,399
the source of the bias wizard in the

00:01:51,600 --> 00:01:56,100
silicon or the processor architecture

00:01:53,399 --> 00:01:57,930
reaching CPU or a GPU so then the OS

00:01:56,100 --> 00:02:00,030
windows versus Linux programming

00:01:57,930 --> 00:02:00,930
language Python versus scallop of course

00:02:00,030 --> 00:02:02,670
not and you know where I'm going with

00:02:00,930 --> 00:02:05,579
this right the source of the bias is not

00:02:02,670 --> 00:02:07,950
in the technology certainly not in what

00:02:05,579 --> 00:02:10,050
anyone components of that technology the

00:02:07,950 --> 00:02:12,239
bias is coming from the combination of

00:02:10,050 --> 00:02:14,069
the code the data and importantly its

00:02:12,239 --> 00:02:15,690
application in the

00:02:14,069 --> 00:02:17,670
and this should really be no surprise to

00:02:15,690 --> 00:02:20,430
us this is the same language that we use

00:02:17,670 --> 00:02:22,290
when we talk about the value of AI but

00:02:20,430 --> 00:02:24,120
it's not just one single component it's

00:02:22,290 --> 00:02:25,920
the way that it all comes together we

00:02:24,120 --> 00:02:27,569
talk about this my colleague will is

00:02:25,920 --> 00:02:29,970
talking about that later today ROI

00:02:27,569 --> 00:02:31,650
positive benefits but we have to

00:02:29,970 --> 00:02:35,160
recognize that benefit can be positive

00:02:31,650 --> 00:02:37,230
or negative so if it's not just the

00:02:35,160 --> 00:02:39,600
technology stack where have we gone

00:02:37,230 --> 00:02:41,040
wrong to find the answer I think that we

00:02:39,600 --> 00:02:41,850
need to consider who's not been brought

00:02:41,040 --> 00:02:43,980
into this room

00:02:41,850 --> 00:02:45,540
the truth is it's difficult to know that

00:02:43,980 --> 00:02:47,130
you're doing well by the world when

00:02:45,540 --> 00:02:49,920
you're working by yourself or maybe in a

00:02:47,130 --> 00:02:50,820
small and often homogenous group we all

00:02:49,920 --> 00:02:53,100
have blind spots

00:02:50,820 --> 00:02:54,810
we all have implicit biases that we may

00:02:53,100 --> 00:02:56,730
not recognize or which we may have

00:02:54,810 --> 00:02:59,550
trouble correcting even when we do

00:02:56,730 --> 00:03:01,350
recognize them I know I do so we can

00:02:59,550 --> 00:03:03,630
distill this into a simple Maxim the

00:03:01,350 --> 00:03:06,840
less inclusive the group building via

00:03:03,630 --> 00:03:10,050
the AI the greater the risk for for bias

00:03:06,840 --> 00:03:12,360
and the greater the risk for bias the

00:03:10,050 --> 00:03:13,920
greater the risk for harm and so if you

00:03:12,360 --> 00:03:15,989
agree with that then the answer to this

00:03:13,920 --> 00:03:18,540
challenge lies in building the human

00:03:15,989 --> 00:03:21,540
systems to ensure that inclusive happens

00:03:18,540 --> 00:03:23,790
pervasively by default and at every step

00:03:21,540 --> 00:03:25,230
in the development process sure this

00:03:23,790 --> 00:03:26,820
includes the actual model development

00:03:25,230 --> 00:03:27,480
the coding but it's so much more than

00:03:26,820 --> 00:03:29,160
that as well

00:03:27,480 --> 00:03:30,620
it's the validation of the application

00:03:29,160 --> 00:03:32,790
the vetting of the training data

00:03:30,620 --> 00:03:34,590
specification of the interface and so on

00:03:32,790 --> 00:03:37,230
and so what's needed is an

00:03:34,590 --> 00:03:39,540
organizational commitment at every step

00:03:37,230 --> 00:03:42,510
to ask who else could be involved in

00:03:39,540 --> 00:03:44,549
this process this means you should draw

00:03:42,510 --> 00:03:45,660
from within your company from people who

00:03:44,549 --> 00:03:47,489
might have a perspective on that

00:03:45,660 --> 00:03:50,430
business application or the way in which

00:03:47,489 --> 00:03:51,930
the training data was obtained set up a

00:03:50,430 --> 00:03:53,820
customer focus group to get outside of

00:03:51,930 --> 00:03:55,950
the walls of your organization to speak

00:03:53,820 --> 00:03:59,370
with the people who might actually be

00:03:55,950 --> 00:04:01,079
impacted by this system and as data

00:03:59,370 --> 00:04:03,329
scientists think a lot about model

00:04:01,079 --> 00:04:05,130
interpreted interpretability tear open

00:04:03,329 --> 00:04:06,959
that black box confront what lurks

00:04:05,130 --> 00:04:08,790
inside and when you do this when you

00:04:06,959 --> 00:04:09,780
bring more people in don't be surprised

00:04:08,790 --> 00:04:11,579
if they come up with some smart

00:04:09,780 --> 00:04:13,829
improvements to your application along

00:04:11,579 --> 00:04:15,570
the way now data echo is a technology

00:04:13,829 --> 00:04:18,570
company we built this platform with this

00:04:15,570 --> 00:04:21,090
inclusive vision but bring you know

00:04:18,570 --> 00:04:23,159
that's not going to be the solution no

00:04:21,090 --> 00:04:24,570
technology not ours not yours is the

00:04:23,159 --> 00:04:27,090
solution it's an organizational

00:04:24,570 --> 00:04:29,460
challenge and it's not going to be easy

00:04:27,090 --> 00:04:32,280
more people in its debt necessarily

00:04:29,460 --> 00:04:34,949
going to slow things down but we have to

00:04:32,280 --> 00:04:37,430
do it we have to do this to get away

00:04:34,949 --> 00:04:40,470
from causing harm and more towards

00:04:37,430 --> 00:04:42,060
causing some benefit in the world and so

00:04:40,470 --> 00:04:43,470
if your organization is not swayed by

00:04:42,060 --> 00:04:44,760
this moral argument that we should do

00:04:43,470 --> 00:04:46,199
this because it's the right thing

00:04:44,760 --> 00:04:48,510
there are hard-nosed business first

00:04:46,199 --> 00:04:50,880
leader who is worried about these things

00:04:48,510 --> 00:04:52,440
then mention the words reputational risk

00:04:50,880 --> 00:04:54,960
and see if they want to confront his

00:04:52,440 --> 00:04:57,690
headlines like this in a PR crisis

00:04:54,960 --> 00:04:59,220
meeting we keep telling ourselves that

00:04:57,690 --> 00:05:00,660
building the future that we're building

00:04:59,220 --> 00:05:02,250
the future and that we're changing the

00:05:00,660 --> 00:05:03,870
worlds and in many ways that's really

00:05:02,250 --> 00:05:05,970
true the future is literally in our

00:05:03,870 --> 00:05:07,800
hands so let's do everything that we can

00:05:05,970 --> 00:05:09,720
to minimize the harm and maximize the

00:05:07,800 --> 00:05:11,580
benefit of our work and a big part of

00:05:09,720 --> 00:05:12,000
that is inviting more people into this

00:05:11,580 --> 00:05:13,530
room

00:05:12,000 --> 00:05:15,890
thanks very much we'll be continuing

00:05:13,530 --> 00:05:20,120
this conversation in June

00:05:15,890 --> 00:05:20,120
[Applause]

00:05:25,200 --> 00:05:27,260

YouTube URL: https://www.youtube.com/watch?v=OpwX_MDxRqU


