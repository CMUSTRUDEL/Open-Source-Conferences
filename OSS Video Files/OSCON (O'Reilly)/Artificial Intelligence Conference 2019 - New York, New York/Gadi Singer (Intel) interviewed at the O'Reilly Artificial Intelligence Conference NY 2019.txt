Title: Gadi Singer (Intel) interviewed at the O'Reilly Artificial Intelligence Conference NY 2019
Publication date: 2019-04-24
Playlist: Artificial Intelligence Conference 2019 - New York, New York
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,030 --> 00:00:04,259
hi my name is Paco Nathan with O'Reilly

00:00:02,340 --> 00:00:05,910
Media and we're here at the O'Reilly

00:00:04,259 --> 00:00:08,220
artificial intelligence conference in

00:00:05,910 --> 00:00:09,540
New York City and today it's a great

00:00:08,220 --> 00:00:11,639
pleasure to get to talk with Gatti

00:00:09,540 --> 00:00:13,830
singer who is VP of artificial

00:00:11,639 --> 00:00:16,049
intelligence product group and also

00:00:13,830 --> 00:00:17,279
general manager of architecture at Intel

00:00:16,049 --> 00:00:18,900
hi good morning

00:00:17,279 --> 00:00:21,210
that is very good good to speak with you

00:00:18,900 --> 00:00:23,250
all right now I have some questions

00:00:21,210 --> 00:00:24,660
following up from your keynote by the

00:00:23,250 --> 00:00:25,430
way excellent keynote this morning thank

00:00:24,660 --> 00:00:29,429
you

00:00:25,430 --> 00:00:31,260
what is deal boost and why does it make

00:00:29,429 --> 00:00:33,899
a difference to your customers can you

00:00:31,260 --> 00:00:36,300
talk a bit about that so deal boost is

00:00:33,899 --> 00:00:39,360
about enhancing the hardware the CPU

00:00:36,300 --> 00:00:43,410
audre to operate better even better on

00:00:39,360 --> 00:00:44,760
deep learning applications the hardware

00:00:43,410 --> 00:00:47,340
had already

00:00:44,760 --> 00:00:49,860
AVX to do vector instructions that are

00:00:47,340 --> 00:00:53,550
very effective for that but we look for

00:00:49,860 --> 00:00:55,110
ways to increase the efficiency of doing

00:00:53,550 --> 00:00:59,039
the acceleration for deep learning

00:00:55,110 --> 00:01:01,219
within the CPU and they'll boost with

00:00:59,039 --> 00:01:04,920
the VN and I the neural network

00:01:01,219 --> 00:01:08,820
instruction Edition allows a significant

00:01:04,920 --> 00:01:13,470
enhancement of the hardware and it gives

00:01:08,820 --> 00:01:15,030
a for some applications a 14x boost as

00:01:13,470 --> 00:01:17,130
in some operations which is very

00:01:15,030 --> 00:01:19,170
significant and the fact that it does

00:01:17,130 --> 00:01:22,049
that boost within the stream of

00:01:19,170 --> 00:01:24,420
instructions that is executed on the CPU

00:01:22,049 --> 00:01:27,240
is very effective for the overall

00:01:24,420 --> 00:01:30,689
application actually is is that more in

00:01:27,240 --> 00:01:34,020
the inference side or is it can be using

00:01:30,689 --> 00:01:37,159
both training and inference there are

00:01:34,020 --> 00:01:39,810
some enhancements for low precision and

00:01:37,159 --> 00:01:41,460
those enhancements probably are more

00:01:39,810 --> 00:01:46,860
applicable on the infant side but

00:01:41,460 --> 00:01:49,710
overall the scale it can be used for in

00:01:46,860 --> 00:01:53,070
the training or inference excellent well

00:01:49,710 --> 00:01:56,850
can we touch about touch on more about

00:01:53,070 --> 00:01:59,939
the Cascade lake's processor and talk

00:01:56,850 --> 00:02:02,790
about the example of implementation of

00:01:59,939 --> 00:02:05,180
that and particular how has deal boost

00:02:02,790 --> 00:02:09,319
helped out with this kinds of use cases

00:02:05,180 --> 00:02:12,939
so the second generation in

00:02:09,319 --> 00:02:18,799
the unscalable processors which is the

00:02:12,939 --> 00:02:22,730
the cascade legs is really upgraded to

00:02:18,799 --> 00:02:26,829
support those workloads and an example

00:02:22,730 --> 00:02:30,069
of that is recommendation systems the

00:02:26,829 --> 00:02:31,849
highest percentage of cycles on

00:02:30,069 --> 00:02:34,090
deployment compute goes to

00:02:31,849 --> 00:02:36,730
recommendation systems interesting and

00:02:34,090 --> 00:02:39,730
those recommendation systems are

00:02:36,730 --> 00:02:43,280
integrating some compute that is

00:02:39,730 --> 00:02:47,019
sequential and can run very well on on

00:02:43,280 --> 00:02:49,299
any cpu but it also has some intensive

00:02:47,019 --> 00:02:53,480
operations on vectors and matrices

00:02:49,299 --> 00:02:55,699
tensors and the ability to increase the

00:02:53,480 --> 00:02:58,370
performance of those vectors and tensors

00:02:55,699 --> 00:03:00,769
activities it's very close within the

00:02:58,370 --> 00:03:02,719
same CPU as the add operation can help

00:03:00,769 --> 00:03:06,889
pro Commendation system achieve

00:03:02,719 --> 00:03:09,919
excellent results fantastic

00:03:06,889 --> 00:03:12,290
also can you explain the shift into more

00:03:09,919 --> 00:03:15,379
inference which you see across different

00:03:12,290 --> 00:03:17,479
industries and how cascade lakes will

00:03:15,379 --> 00:03:21,040
address this and help customers to

00:03:17,479 --> 00:03:24,799
achieve better total cost of ownership

00:03:21,040 --> 00:03:27,319
by extending the the xeon platform I

00:03:24,799 --> 00:03:30,349
know that's it's already being used for

00:03:27,319 --> 00:03:32,120
AI in France but even if models were

00:03:30,349 --> 00:03:34,489
trained on GPUs how is this being used

00:03:32,120 --> 00:03:38,329
now so first on they transition to

00:03:34,489 --> 00:03:41,479
inference about 3-4 years ago and the

00:03:38,329 --> 00:03:43,939
deep learning technology was at a stage

00:03:41,479 --> 00:03:45,919
wood which was more of exploration

00:03:43,939 --> 00:03:49,819
finding out what the technology can do

00:03:45,919 --> 00:03:51,680
proving its results and what happened

00:03:49,819 --> 00:03:54,199
over the last three to four years is

00:03:51,680 --> 00:03:56,180
that it is being readied for major

00:03:54,199 --> 00:03:58,699
deployment there are multiple companies

00:03:56,180 --> 00:04:01,129
that acquired the skill center quartz

00:03:58,699 --> 00:04:03,109
scale scale and integrated those

00:04:01,129 --> 00:04:06,109
technologies into the line of business

00:04:03,109 --> 00:04:11,000
applications and what we're seeing today

00:04:06,109 --> 00:04:12,829
as of 2019 is a ratio of maybe four to

00:04:11,000 --> 00:04:15,670
one between inference cycles and

00:04:12,829 --> 00:04:17,980
training we can see it over the coming

00:04:15,670 --> 00:04:20,350
years are going to ratio of ten to one

00:04:17,980 --> 00:04:22,300
so inference is becoming really the

00:04:20,350 --> 00:04:26,139
focus of a lot of the deep learning

00:04:22,300 --> 00:04:29,020
activities and the regardless of how the

00:04:26,139 --> 00:04:31,360
model was trained the way it is captured

00:04:29,020 --> 00:04:35,440
in one of the deep learning framers can

00:04:31,360 --> 00:04:39,010
be mapped very optimally to zero and

00:04:35,440 --> 00:04:42,780
when it is happening you need to run

00:04:39,010 --> 00:04:46,150
this mix of CPU activities as well as

00:04:42,780 --> 00:04:49,330
tensor or vector intense activities and

00:04:46,150 --> 00:04:51,850
the enhancements being put into cascade

00:04:49,330 --> 00:04:54,669
legs allowed to do that part very well

00:04:51,850 --> 00:04:57,729
and it allows something very important

00:04:54,669 --> 00:05:01,990
to the customers our customers have

00:04:57,729 --> 00:05:05,680
major investment teamed optimized over

00:05:01,990 --> 00:05:08,050
many years to their zone environment and

00:05:05,680 --> 00:05:10,389
when they use AI it's not a new

00:05:08,050 --> 00:05:11,860
application it's enhancing whatever

00:05:10,389 --> 00:05:14,860
they're doing for their line of business

00:05:11,860 --> 00:05:17,470
with AI capabilities and having this

00:05:14,860 --> 00:05:20,050
level of integration allows them to

00:05:17,470 --> 00:05:23,070
protect and extend their all this

00:05:20,050 --> 00:05:25,750
investment and give state-of-the-art

00:05:23,070 --> 00:05:28,930
performance and capabilities all deploy

00:05:25,750 --> 00:05:31,450
mnek fantastic it's really only what

00:05:28,930 --> 00:05:33,370
seven years out now from maybe I guess

00:05:31,450 --> 00:05:35,530
more like six years out since the Alex

00:05:33,370 --> 00:05:37,990
Ned paper came that's right it's it's

00:05:35,530 --> 00:05:40,539
fantastic to see how much tooling at

00:05:37,990 --> 00:05:42,280
scale industrial scale has happened in

00:05:40,539 --> 00:05:45,660
such a brief period of time that's right

00:05:42,280 --> 00:05:48,220
and this is this is the short history of

00:05:45,660 --> 00:05:51,789
deep learning and deep learning in

00:05:48,220 --> 00:05:54,160
action and you start with Alex and you

00:05:51,789 --> 00:05:57,330
start with the cases of doing imagenet

00:05:54,160 --> 00:06:00,340
and showing that a machine can identify

00:05:57,330 --> 00:06:02,700
objects almost at human level and then

00:06:00,340 --> 00:06:06,430
better than the human level in some

00:06:02,700 --> 00:06:10,390
object recognition and since alex net we

00:06:06,430 --> 00:06:13,470
had several generations of improvement

00:06:10,390 --> 00:06:16,169
and half capabilities and the latest

00:06:13,470 --> 00:06:20,280
topology is the latest

00:06:16,169 --> 00:06:23,729
algorithms are creating a mix if alex

00:06:20,280 --> 00:06:25,740
net was more pure doing matrix operation

00:06:23,729 --> 00:06:28,110
and convolutions the new most

00:06:25,740 --> 00:06:31,979
sophisticated models have combinations

00:06:28,110 --> 00:06:35,120
of activities and the Xeon and the

00:06:31,979 --> 00:06:38,460
Cascade lift framework allows this

00:06:35,120 --> 00:06:40,860
hybrid activities to be operating within

00:06:38,460 --> 00:06:43,020
the same system interesting a much more

00:06:40,860 --> 00:06:45,150
heterogeneous much higher genius mix of

00:06:43,020 --> 00:06:48,389
needs and and this is also going if you

00:06:45,150 --> 00:06:49,860
look at Alex net the origins were in

00:06:48,389 --> 00:06:52,560
proving that things can be done

00:06:49,860 --> 00:06:56,029
differently on images yeah but if you

00:06:52,560 --> 00:07:00,090
look at at the use of deployment today

00:06:56,029 --> 00:07:02,430
images is probably the third most common

00:07:00,090 --> 00:07:05,939
usage the most common usage is

00:07:02,430 --> 00:07:08,279
recommendation system followed by speech

00:07:05,939 --> 00:07:10,949
recognition NLP natural language

00:07:08,279 --> 00:07:14,069
processing and then you've got image

00:07:10,949 --> 00:07:17,610
recognition and we need our solutions to

00:07:14,069 --> 00:07:19,409
be very effective in all those families

00:07:17,610 --> 00:07:24,439
of deep learning fitting that general

00:07:19,409 --> 00:07:26,909
case yes now can you also talk about

00:07:24,439 --> 00:07:30,569
specific implementation implementations

00:07:26,909 --> 00:07:33,360
of AI moving in two main lines of

00:07:30,569 --> 00:07:35,569
business for the types of compute that

00:07:33,360 --> 00:07:37,949
are needed in main mainstream business

00:07:35,569 --> 00:07:41,310
so I can imagine that there is much

00:07:37,949 --> 00:07:42,779
lower latency there's needs to scale how

00:07:41,310 --> 00:07:46,289
do these fit into the requirements for

00:07:42,779 --> 00:07:50,810
cascade lakes this is a another change

00:07:46,289 --> 00:07:54,899
that that help CPUs Zeon's really shine

00:07:50,810 --> 00:07:57,830
because in the original usages it was

00:07:54,899 --> 00:08:01,500
primarily about batch so there were

00:07:57,830 --> 00:08:04,830
small pieces of data with large batches

00:08:01,500 --> 00:08:08,759
and to create the efficiency what we see

00:08:04,830 --> 00:08:11,669
today is much more focus on latency we

00:08:08,759 --> 00:08:17,219
see the data sizes growing up

00:08:11,669 --> 00:08:18,810
so the xeon cascade lake with its focus

00:08:17,219 --> 00:08:22,770
on latency with the ability to do it

00:08:18,810 --> 00:08:25,500
very well with the large memory space is

00:08:22,770 --> 00:08:26,310
very well equipped to deal with those

00:08:25,500 --> 00:08:29,040
new usages

00:08:26,310 --> 00:08:31,740
excellent I can imagine in particularly

00:08:29,040 --> 00:08:33,570
the larger memory spaces or quite an

00:08:31,740 --> 00:08:37,500
enhancement for how several customers

00:08:33,570 --> 00:08:41,099
that just could not fit their model in

00:08:37,500 --> 00:08:43,950
the GPU and the large memory space of

00:08:41,099 --> 00:08:46,160
zeon and with the ability to do multi

00:08:43,950 --> 00:08:49,560
node very effective multi node scaling

00:08:46,160 --> 00:08:52,950
was a key factor sometimes and enabler

00:08:49,560 --> 00:08:54,990
for their fantastic can you expand on

00:08:52,950 --> 00:08:58,440
the scenarios where dedicated

00:08:54,990 --> 00:09:00,600
accelerators are needed to to supplement

00:08:58,440 --> 00:09:04,260
this kind of influence I guess because

00:09:00,600 --> 00:09:06,960
of continuance and intensive inference

00:09:04,260 --> 00:09:10,110
processing particularly so one of the

00:09:06,960 --> 00:09:13,730
most demanding tasks and deep learning

00:09:10,110 --> 00:09:16,440
is to do arithmetic on multi-dimensional

00:09:13,730 --> 00:09:19,740
large arrays those are called tensors

00:09:16,440 --> 00:09:22,560
and and part of what is being done is

00:09:19,740 --> 00:09:26,480
just having those tensors multiplied

00:09:22,560 --> 00:09:29,280
some operations on on them and

00:09:26,480 --> 00:09:32,190
convolutions and those very specific

00:09:29,280 --> 00:09:33,930
operations can benefit for from

00:09:32,190 --> 00:09:37,050
acceleration on a machine that's

00:09:33,930 --> 00:09:39,330
designed for that and this is where the

00:09:37,050 --> 00:09:41,400
accelerators come into play the

00:09:39,330 --> 00:09:43,830
accelerators are much more narrow and

00:09:41,400 --> 00:09:46,190
focused and the things that they do but

00:09:43,830 --> 00:09:50,339
those they do those things very well and

00:09:46,190 --> 00:09:52,980
in particular we have the Nirvana neural

00:09:50,339 --> 00:09:56,760
network processors that are coming in

00:09:52,980 --> 00:10:00,630
2019 out of production we have the n NP

00:09:56,760 --> 00:10:04,650
l for learning offer training so the n

00:10:00,630 --> 00:10:08,070
NP l 1000 which is codenamed spring

00:10:04,650 --> 00:10:11,640
crest and we have the n NP I for

00:10:08,070 --> 00:10:16,260
inference and NPI 1000 which is Spring

00:10:11,640 --> 00:10:19,290
Hill and those are targeted dedicated

00:10:16,260 --> 00:10:21,839
acceleration so they do the task of the

00:10:19,290 --> 00:10:25,589
tensor arithmetic very very well

00:10:21,839 --> 00:10:27,030
and we build systems that are really the

00:10:25,589 --> 00:10:29,420
overall system

00:10:27,030 --> 00:10:33,180
is integrating the strength of the host

00:10:29,420 --> 00:10:35,430
with offloading the necessary deep

00:10:33,180 --> 00:10:37,890
learning portions this intensive

00:10:35,430 --> 00:10:39,930
intensive tensor operation to the

00:10:37,890 --> 00:10:41,970
accelerators fantastic

00:10:39,930 --> 00:10:44,010
I imagine that finds use cases far

00:10:41,970 --> 00:10:45,810
beyond deep learning as well and there's

00:10:44,010 --> 00:10:47,730
a lot of general case for this kind of

00:10:45,810 --> 00:10:49,950
math being accelerated that's right

00:10:47,730 --> 00:10:53,210
and the ability to do those activities

00:10:49,950 --> 00:10:56,390
on primarily on tensors on those large

00:10:53,210 --> 00:10:59,550
multi-dimensional arrays in science and

00:10:56,390 --> 00:11:02,430
in some of the big data areas is very

00:10:59,550 --> 00:11:03,870
promising fantastic thank you god is

00:11:02,430 --> 00:11:06,320
very good speaking with you today thank

00:11:03,870 --> 00:11:06,320

YouTube URL: https://www.youtube.com/watch?v=iUsZj8xztnk


