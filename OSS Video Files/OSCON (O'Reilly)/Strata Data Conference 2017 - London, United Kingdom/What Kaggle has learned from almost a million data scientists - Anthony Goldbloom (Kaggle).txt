Title: What Kaggle has learned from almost a million data scientists - Anthony Goldbloom (Kaggle)
Publication date: 2017-05-24
Playlist: Strata Data Conference 2017 - London, United Kingdom
Description: 
	Kaggle is a community of almost a million data scientists, who have built more than two million machine-learning models while participating in Kaggle competitions. Data scientists come to Kaggle to learn, collaborate, and develop the state of the art in machine learning. Anthony Goldbloom shares lessons learned from top performers in the Kaggle community and explores the types of machine-learning techniques typically used, some of the tricks heâ€™s seen, and pitfalls to avoid. Along the way, Anthony discusses work habits and skills that help data scientists succeed.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Google: http://plus.google.com/+oreillymedia
Captions: 
	00:00:01,030 --> 00:00:05,080
we recently passed a pretty large

00:00:02,980 --> 00:00:07,569
milestone which is we have now had more

00:00:05,080 --> 00:00:09,610
than four million models submitted to

00:00:07,569 --> 00:00:12,309
capital competitions that's a huge

00:00:09,610 --> 00:00:14,079
amount of machine learning 1.4 million

00:00:12,309 --> 00:00:15,940
models submitted to competitions last

00:00:14,079 --> 00:00:17,770
year and it looks like we're probably on

00:00:15,940 --> 00:00:21,580
track to pass two million this year

00:00:17,770 --> 00:00:23,200
there's a lot of machine learning so I'm

00:00:21,580 --> 00:00:25,480
a pretty well-positioned to give you a

00:00:23,200 --> 00:00:27,550
sense for what kind of techniques you

00:00:25,480 --> 00:00:30,039
should focus on and for what sort of

00:00:27,550 --> 00:00:31,900
problems we see what we see patterns

00:00:30,039 --> 00:00:35,170
across there the competitions that we

00:00:31,900 --> 00:00:37,629
run as to what wins and what doesn't now

00:00:35,170 --> 00:00:40,179
there are two approaches that typically

00:00:37,629 --> 00:00:42,219
win competitions the first wins

00:00:40,179 --> 00:00:44,620
unstructured data problems so this is

00:00:42,219 --> 00:00:47,109
data that you know maybe sits in an

00:00:44,620 --> 00:00:49,420
Excel spreadsheet or a CSV set file some

00:00:47,109 --> 00:00:51,850
sort of tabular form or possibly in a

00:00:49,420 --> 00:00:55,749
sequel database relational data but a

00:00:51,850 --> 00:00:57,999
structured data now if you want to win a

00:00:55,749 --> 00:00:59,829
competition or do what build a very

00:00:57,999 --> 00:01:02,379
powerful algorithm on structured data

00:00:59,829 --> 00:01:04,930
there's really a three step process that

00:01:02,379 --> 00:01:06,910
you should follow the first is you

00:01:04,930 --> 00:01:09,400
explore the data every which way you can

00:01:06,910 --> 00:01:12,160
you draw a histogram you draw a choral

00:01:09,400 --> 00:01:14,080
Ella Grahams you draw a cross plots you

00:01:12,160 --> 00:01:15,810
really want to understand what is in

00:01:14,080 --> 00:01:19,210
that data what does each variable mean

00:01:15,810 --> 00:01:22,180
what's its distribution ideally how was

00:01:19,210 --> 00:01:24,010
it collected and then once you have a

00:01:22,180 --> 00:01:25,060
really rock solid understanding of

00:01:24,010 --> 00:01:26,590
what's in the data

00:01:25,060 --> 00:01:30,220
the second thing you've got it you want

00:01:26,590 --> 00:01:32,740
to do is start creating features so in

00:01:30,220 --> 00:01:35,140
in machine learning parlance they talk

00:01:32,740 --> 00:01:36,580
we talk about feature engineering if

00:01:35,140 --> 00:01:38,470
you're a statistician you talk about

00:01:36,580 --> 00:01:40,420
creating synthetic variables and it's

00:01:38,470 --> 00:01:43,060
really the same thing and I have a

00:01:40,420 --> 00:01:46,270
picture of different color cars on the

00:01:43,060 --> 00:01:48,400
screen because a nice example of what it

00:01:46,270 --> 00:01:50,740
means to create features comes from one

00:01:48,400 --> 00:01:52,930
of our earlier competitions where what

00:01:50,740 --> 00:01:54,640
we did was we were predicting which cars

00:01:52,930 --> 00:01:56,290
older the second-hand auction would be

00:01:54,640 --> 00:01:59,020
good buyers I should say our community

00:01:56,290 --> 00:02:00,610
with predicting which cars although the

00:01:59,020 --> 00:02:01,870
second option would be good buys and

00:02:00,610 --> 00:02:03,340
which would have ulema which would be

00:02:01,870 --> 00:02:06,580
lemons which would have warranty claims

00:02:03,340 --> 00:02:08,440
against them and it turned out that the

00:02:06,580 --> 00:02:10,179
the the idea that when the competition

00:02:08,440 --> 00:02:13,030
or the feature that separated the winner

00:02:10,179 --> 00:02:14,110
apart from others was Bianca Keller had

00:02:13,030 --> 00:02:16,870
a big effect now

00:02:14,110 --> 00:02:21,580
with car color he didn't just put it in

00:02:16,870 --> 00:02:23,260
as a raw variable as he received it but

00:02:21,580 --> 00:02:25,180
what he did was he grouped car color

00:02:23,260 --> 00:02:27,040
into standard color cars and unusual

00:02:25,180 --> 00:02:30,040
color cars on the assumption that

00:02:27,040 --> 00:02:32,050
standard color car was or is unusual

00:02:30,040 --> 00:02:33,610
color car rather was more likely not to

00:02:32,050 --> 00:02:35,440
be you know to have been looked after

00:02:33,610 --> 00:02:38,110
well because the Bart first buyer was an

00:02:35,440 --> 00:02:39,430
enthusiast so that's what what I mean by

00:02:38,110 --> 00:02:42,610
feature engineering coming up with a

00:02:39,430 --> 00:02:44,230
clever hypothesis that it turns out to

00:02:42,610 --> 00:02:47,470
have a relationship in the data and

00:02:44,230 --> 00:02:50,770
transforming the variable in such a way

00:02:47,470 --> 00:02:53,080
to take advantage of that effect and

00:02:50,770 --> 00:02:56,560
then finally are you'll use a classifier

00:02:53,080 --> 00:02:58,570
so the classifier when Cavill started it

00:02:56,560 --> 00:03:00,940
used to be that people used all sorts of

00:02:58,570 --> 00:03:02,800
different classifiers things bring in

00:03:00,940 --> 00:03:05,020
from self-organizing maps to support

00:03:02,800 --> 00:03:06,970
vector machines and then random forests

00:03:05,020 --> 00:03:08,920
really took off we saw random forests in

00:03:06,970 --> 00:03:10,500
the very early days of cattle random

00:03:08,920 --> 00:03:13,810
forests was by far the most powerful

00:03:10,500 --> 00:03:16,270
algorithm we have now moved on from

00:03:13,810 --> 00:03:17,830
random forests and it's an algorithm

00:03:16,270 --> 00:03:19,930
called gradient boosting machines which

00:03:17,830 --> 00:03:21,880
is dominating as the structured data

00:03:19,930 --> 00:03:24,910
problem and we didn't gradient boosting

00:03:21,880 --> 00:03:26,500
machines they're there the that the

00:03:24,910 --> 00:03:28,780
implementation that is doing best is

00:03:26,500 --> 00:03:30,130
called actually boost and there's

00:03:28,780 --> 00:03:34,290
another one out of Microsoft research

00:03:30,130 --> 00:03:37,049
that's doing very well for by GBM now

00:03:34,290 --> 00:03:40,630
the choice of classifier and how you

00:03:37,049 --> 00:03:42,370
tune the classifier I would just want to

00:03:40,630 --> 00:03:44,890
point out matters much less than the

00:03:42,370 --> 00:03:47,170
choice of features so well you think of

00:03:44,890 --> 00:03:49,360
machine learning as complex math at math

00:03:47,170 --> 00:03:51,430
actually a lot of the matches a lot of

00:03:49,360 --> 00:03:53,200
what separates are the really good data

00:03:51,430 --> 00:03:55,570
scientist is actually step number two

00:03:53,200 --> 00:03:56,950
the complex features and and putting

00:03:55,570 --> 00:03:58,480
you're putting those features into a

00:03:56,950 --> 00:04:00,750
classifier is not actually that

00:03:58,480 --> 00:04:00,750

YouTube URL: https://www.youtube.com/watch?v=jmHbS8z57yI


