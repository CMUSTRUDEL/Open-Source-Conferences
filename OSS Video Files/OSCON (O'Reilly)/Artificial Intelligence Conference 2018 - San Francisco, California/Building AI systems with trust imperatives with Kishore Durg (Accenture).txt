Title: Building AI systems with trust imperatives with Kishore Durg (Accenture)
Publication date: 2018-09-19
Playlist: Artificial Intelligence Conference 2018 - San Francisco, California
Description: 
	Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,020 --> 00:00:06,000
hi I'm Roger Mikolas here at AI San

00:00:04,380 --> 00:00:08,700
Francisco 2018

00:00:06,000 --> 00:00:10,830
I'm here with kosher Dirk who is the

00:00:08,700 --> 00:00:11,550
head of growth and strategy at Accenture

00:00:10,830 --> 00:00:13,710
hi

00:00:11,550 --> 00:00:16,800
nice to meet you so we were to talk

00:00:13,710 --> 00:00:19,289
about AI and like we understand that

00:00:16,800 --> 00:00:20,670
there's an AI imperative you guys have

00:00:19,289 --> 00:00:21,480
been working on you can tell me a little

00:00:20,670 --> 00:00:23,939
about it sure

00:00:21,480 --> 00:00:25,619
you know as you look at a lot of these

00:00:23,939 --> 00:00:27,930
AI systems that are being built and

00:00:25,619 --> 00:00:29,099
there are these systems are being built

00:00:27,930 --> 00:00:31,399
with businesses which are taking

00:00:29,099 --> 00:00:34,829
decisions and it's impacting human lives

00:00:31,399 --> 00:00:36,690
so the clear imperative is that these

00:00:34,829 --> 00:00:40,620
systems that are being built needs to be

00:00:36,690 --> 00:00:43,110
transparent responsive have to align

00:00:40,620 --> 00:00:45,539
with some of the societal values that

00:00:43,110 --> 00:00:47,280
are that are out there and essentially

00:00:45,539 --> 00:00:50,760
ethically they're taking the right

00:00:47,280 --> 00:00:52,500
decisions so the imperative for us is to

00:00:50,760 --> 00:00:55,649
ensure that these systems as they are

00:00:52,500 --> 00:00:58,140
built are in the right track it's just

00:00:55,649 --> 00:01:00,449
like you know when you look at a kid

00:00:58,140 --> 00:01:03,059
which has to be taught the right things

00:01:00,449 --> 00:01:05,549
between right and wrong the right values

00:01:03,059 --> 00:01:08,460
from a societal perspective and you want

00:01:05,549 --> 00:01:11,580
to ensure that they do grow up to be you

00:01:08,460 --> 00:01:13,890
know strong as adults who contribute to

00:01:11,580 --> 00:01:16,590
the society we look at the system

00:01:13,890 --> 00:01:18,479
similarly the imperative for us is how

00:01:16,590 --> 00:01:22,170
do we ensure these systems a responsive

00:01:18,479 --> 00:01:24,090
and and they imbibe the societal values

00:01:22,170 --> 00:01:25,619
so you're making a compelling case for

00:01:24,090 --> 00:01:29,400
that but it's also why should businesses

00:01:25,619 --> 00:01:31,290
care do so you know 92 percent of the

00:01:29,400 --> 00:01:33,990
business executives said oh we you

00:01:31,290 --> 00:01:35,820
really want to get customers trust we we

00:01:33,990 --> 00:01:38,159
want to win their trust lot of the

00:01:35,820 --> 00:01:41,360
growth comes from you know a lot of our

00:01:38,159 --> 00:01:43,740
customers trusting our businesses and

00:01:41,360 --> 00:01:45,479
aligned to that if you piss you know

00:01:43,740 --> 00:01:48,570
customers have to trust you you need to

00:01:45,479 --> 00:01:50,939
ensure that your systems are supporting

00:01:48,570 --> 00:01:52,680
that trust imperative right and that's

00:01:50,939 --> 00:01:54,720
that's exactly why businesses need to

00:01:52,680 --> 00:01:56,040
care because we have seen a lot of

00:01:54,720 --> 00:01:57,479
things that have gone bad you know a lot

00:01:56,040 --> 00:01:59,700
of the conversations started learning

00:01:57,479 --> 00:02:01,200
things they should be learning and there

00:01:59,700 --> 00:02:03,680
have been cases of autonomous vehicles

00:02:01,200 --> 00:02:06,420
going off-track there have been cases

00:02:03,680 --> 00:02:09,140
where you have machine learning

00:02:06,420 --> 00:02:12,840
algorithms picking up the wrong behavior

00:02:09,140 --> 00:02:13,920
so you know businesses if they are going

00:02:12,840 --> 00:02:17,520
to implement Lisi is

00:02:13,920 --> 00:02:20,099
systems we believe that they need to

00:02:17,520 --> 00:02:22,980
care because customers trust businesses

00:02:20,099 --> 00:02:25,620
which have you know verifiable

00:02:22,980 --> 00:02:27,840
explainable trustworthy systems mm-hmm

00:02:25,620 --> 00:02:31,260
that's that's a big imperative for

00:02:27,840 --> 00:02:34,110
business so how does the Accenture teach

00:02:31,260 --> 00:02:36,600
and test framework raise responsible

00:02:34,110 --> 00:02:39,239
systems and raises a good term giving

00:02:36,600 --> 00:02:41,280
yes in the analogy with kids yes so when

00:02:39,239 --> 00:02:43,349
you really raise the CI systems and just

00:02:41,280 --> 00:02:45,269
like kids you need to teach teach it the

00:02:43,349 --> 00:02:47,040
right way so how what are the things

00:02:45,269 --> 00:02:49,260
that you need to be worried about lot of

00:02:47,040 --> 00:02:53,400
the AI systems right now have gender

00:02:49,260 --> 00:02:55,019
bias ratio-wise ethnic biases and a lot

00:02:53,400 --> 00:02:58,140
of the corpus of data that is used to

00:02:55,019 --> 00:03:00,090
train them are done by humans so when

00:02:58,140 --> 00:03:01,980
you actually use the same data to train

00:03:00,090 --> 00:03:04,830
the CI systems you're going to

00:03:01,980 --> 00:03:07,140
perpetuate the biases that you have into

00:03:04,830 --> 00:03:08,459
a system now this could be different in

00:03:07,140 --> 00:03:11,010
different parts of the world but

00:03:08,459 --> 00:03:13,319
essentially what we have as at each

00:03:11,010 --> 00:03:15,420
phase is we try to neutralize a lot of

00:03:13,319 --> 00:03:18,480
these biases so since you have a corpus

00:03:15,420 --> 00:03:20,760
of data which is you know neutral to the

00:03:18,480 --> 00:03:22,980
biases that are out there that that is

00:03:20,760 --> 00:03:24,959
what we call as the teach phase and in

00:03:22,980 --> 00:03:26,940
the test phase just like kids kids make

00:03:24,959 --> 00:03:28,230
mistakes as they learn new things they

00:03:26,940 --> 00:03:29,400
are going out of your house they're

00:03:28,230 --> 00:03:31,950
learning a lot and picking up a lot of

00:03:29,400 --> 00:03:33,930
new things that when kids make mistake

00:03:31,950 --> 00:03:37,350
we teach them how to do it and similarly

00:03:33,930 --> 00:03:41,100
for the the AI systems we have the test

00:03:37,350 --> 00:03:43,709
phase so we monitor for behaviors that

00:03:41,100 --> 00:03:45,780
are not ethically right and we address

00:03:43,709 --> 00:03:47,400
it so it's a very simple concept of

00:03:45,780 --> 00:03:49,320
teacher test it's just like bringing up

00:03:47,400 --> 00:03:51,329
your kids mm-hm it's curious any

00:03:49,320 --> 00:03:53,489
reference to reinforcement learning

00:03:51,329 --> 00:03:56,400
hearing describing it sounds a little

00:03:53,489 --> 00:03:58,319
like it is it is very really aligned

00:03:56,400 --> 00:03:59,970
with that and and yeah and I'm trying to

00:03:58,319 --> 00:04:01,350
simplify it for us so that people can

00:03:59,970 --> 00:04:04,109
understand what exactly mean it is a

00:04:01,350 --> 00:04:06,410
very complicated algorithm in terms of

00:04:04,109 --> 00:04:08,870
how we D by as these systems how we

00:04:06,410 --> 00:04:11,069
address these biases there is also

00:04:08,870 --> 00:04:12,420
metamorphic testing that we use for some

00:04:11,069 --> 00:04:16,200
of the algorithmic issues that are out

00:04:12,420 --> 00:04:17,760
there so in a simplified way we are

00:04:16,200 --> 00:04:19,200
looking at how do you raise kids you

00:04:17,760 --> 00:04:21,299
need to ensure that the systems behave

00:04:19,200 --> 00:04:23,520
similarly mm-hmm that's great you know a

00:04:21,299 --> 00:04:25,620
use case of probably help explain it

00:04:23,520 --> 00:04:27,420
sure I mean just taking autonomous

00:04:25,620 --> 00:04:29,060
vehicles and if you

00:04:27,420 --> 00:04:31,980
you know how you need to ensure that

00:04:29,060 --> 00:04:33,840
these systems know there's a there is a

00:04:31,980 --> 00:04:35,220
stop out there it's not that you'll be

00:04:33,840 --> 00:04:36,540
able to train everything it would take

00:04:35,220 --> 00:04:38,430
one and after two years to actually

00:04:36,540 --> 00:04:39,990
train their systems to get every

00:04:38,430 --> 00:04:41,880
possible conditions that are out there

00:04:39,990 --> 00:04:43,410
and there are cases where you're

00:04:41,880 --> 00:04:46,050
actually putting them out for the human

00:04:43,410 --> 00:04:48,240
to test them and obviously they may not

00:04:46,050 --> 00:04:49,980
end up with the most likely alternative

00:04:48,240 --> 00:04:51,690
in terms of what you would like it to be

00:04:49,980 --> 00:04:53,250
because there are unknown parameters

00:04:51,690 --> 00:04:55,860
that you would have never taken care as

00:04:53,250 --> 00:04:57,270
you validate these systems so one of the

00:04:55,860 --> 00:04:59,190
constructs we have there is around the

00:04:57,270 --> 00:05:01,980
knowledge representation qualitative

00:04:59,190 --> 00:05:03,390
reasoning bringing act together with

00:05:01,980 --> 00:05:04,980
machine learning is a way to go to

00:05:03,390 --> 00:05:07,770
address these systems that are out there

00:05:04,980 --> 00:05:10,500
on the Altima side and essentially that

00:05:07,770 --> 00:05:12,240
will help us understand the knowledge

00:05:10,500 --> 00:05:13,950
graphs and reasoning on why it took a

00:05:12,240 --> 00:05:16,530
decision the way to and Brill's in

00:05:13,950 --> 00:05:18,480
transparency in the decision-making and

00:05:16,530 --> 00:05:22,530
that is something that we have been

00:05:18,480 --> 00:05:25,200
working on similarly on the on the data

00:05:22,530 --> 00:05:26,640
part of the equation we have been

00:05:25,200 --> 00:05:29,310
working with banks to kind of develop

00:05:26,640 --> 00:05:32,100
virtual agents which are neutralized

00:05:29,310 --> 00:05:34,830
from a gender bias racial bias and other

00:05:32,100 --> 00:05:38,070
so that the corpus of data that's used

00:05:34,830 --> 00:05:40,560
to train these agents are neutral in

00:05:38,070 --> 00:05:42,960
nature and unbiased and as they pick up

00:05:40,560 --> 00:05:44,729
and learn we do look at monitoring of

00:05:42,960 --> 00:05:49,320
the activities that are out there so

00:05:44,729 --> 00:05:50,940
even a virtual agent can grow so that's

00:05:49,320 --> 00:05:52,440
that's a that's a very simple way of

00:05:50,940 --> 00:05:54,390
looking at you know these systems that

00:05:52,440 --> 00:05:56,310
are out there you need you need

00:05:54,390 --> 00:05:58,169
parenting you need to raise them

00:05:56,310 --> 00:06:00,540
properly and you need some governance

00:05:58,169 --> 00:06:02,910
and that's that's the construct of

00:06:00,540 --> 00:06:04,169
responsibility that sounds great I

00:06:02,910 --> 00:06:07,010
really appreciate your time

00:06:04,169 --> 00:06:07,010
thank you very much

00:06:12,220 --> 00:06:14,280

YouTube URL: https://www.youtube.com/watch?v=j2jgsQgAvdc


