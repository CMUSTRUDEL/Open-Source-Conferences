Title: Raising AI to Benefit Business and Society (sponsored by Accenture) - Kishore Durg (Accenture)
Publication date: 2018-09-28
Playlist: Artificial Intelligence Conference 2018 - San Francisco, California
Description: 
	Much more than just a technological tool, AI has grown to the point where it often has as much influence as the people putting it to use, both within and outside the company. Kishore Durg explains why deploying AI is no longer just about training it to perform a given task. It’s about “raising” it to act as a responsible representative of the business and a contributing member of society.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,170 --> 00:00:04,730
you know as we are looking at the growth

00:00:02,690 --> 00:00:07,160
of AI systems and the impact its making

00:00:04,730 --> 00:00:09,470
on business one of the elements that we

00:00:07,160 --> 00:00:12,190
need to be focused on is how do we raise

00:00:09,470 --> 00:00:16,160
the game to benefit business and society

00:00:12,190 --> 00:00:18,440
one of the key elements as to why I want

00:00:16,160 --> 00:00:21,320
to discuss this with you all now is you

00:00:18,440 --> 00:00:23,240
all seeing how AI is impacting a lot of

00:00:21,320 --> 00:00:24,770
the systems that we have in place how

00:00:23,240 --> 00:00:26,650
it's making decisions that will help

00:00:24,770 --> 00:00:29,660
drive profitability for businesses and

00:00:26,650 --> 00:00:32,329
as per the tech vision that we publish

00:00:29,660 --> 00:00:36,679
for 2018 we also see that it will be a

00:00:32,329 --> 00:00:38,569
huge engine in 2023 and above in terms

00:00:36,679 --> 00:00:42,589
of making significant discovery in life

00:00:38,569 --> 00:00:45,799
sciences we also believe by 2025 - 2028

00:00:42,589 --> 00:00:48,019
it will you know probably produce a

00:00:45,799 --> 00:00:49,819
global blockbuster film and we also

00:00:48,019 --> 00:00:51,559
believe that there will be a case for

00:00:49,819 --> 00:00:54,499
our national or sub-national government

00:00:51,559 --> 00:00:56,199
to ban autonomous vehicles so why am i

00:00:54,499 --> 00:00:58,819
bringing these points to you now

00:00:56,199 --> 00:01:02,149
so if you really look at it why

00:00:58,819 --> 00:01:04,990
businesses need to acknowledge AI as

00:01:02,149 --> 00:01:07,609
impact and obviously raise the game is

00:01:04,990 --> 00:01:10,219
lot of the elements of VI is about how

00:01:07,609 --> 00:01:11,810
it learns and makes autonomous decisions

00:01:10,219 --> 00:01:13,549
it's all based on the data that you feed

00:01:11,810 --> 00:01:15,439
into it and obviously without human

00:01:13,549 --> 00:01:17,840
intervention somebody will say what's

00:01:15,439 --> 00:01:19,250
the problem with that oh it's a it's a

00:01:17,840 --> 00:01:20,990
it's a good thing you know we don't want

00:01:19,250 --> 00:01:25,340
humans to intervene and it needs to be

00:01:20,990 --> 00:01:28,039
that way what we have seen is biases and

00:01:25,340 --> 00:01:30,560
risks will start to creep into these

00:01:28,039 --> 00:01:32,030
systems so so what's the problem

00:01:30,560 --> 00:01:34,100
somebody will say what's the big deal

00:01:32,030 --> 00:01:35,600
tell me exactly what's the problem so if

00:01:34,100 --> 00:01:37,159
you really look at I'll give you a

00:01:35,600 --> 00:01:39,520
simple example take any translation

00:01:37,159 --> 00:01:43,399
engine if you just say she's a doctor

00:01:39,520 --> 00:01:45,200
he's in the house and you translated to

00:01:43,399 --> 00:01:47,119
Turkish you bring it back from Turkish

00:01:45,200 --> 00:01:49,850
to English it will say she's in the

00:01:47,119 --> 00:01:51,590
house he's a doctor and this is

00:01:49,850 --> 00:01:55,479
happening in our translation systems now

00:01:51,590 --> 00:01:57,920
and if you really look at a lot of the

00:01:55,479 --> 00:02:00,409
you know legal and ethical issues around

00:01:57,920 --> 00:02:03,069
how algorithms are deciding there have

00:02:00,409 --> 00:02:05,299
been issues whether you know you take a

00:02:03,069 --> 00:02:08,119
police software which is identifying

00:02:05,299 --> 00:02:10,280
high risk you know I would say it's

00:02:08,119 --> 00:02:12,019
showing pictures and identifying who of

00:02:10,280 --> 00:02:13,519
these are of high risk you'll be

00:02:12,019 --> 00:02:14,030
surprised in terms of how the existing

00:02:13,519 --> 00:02:18,260
system

00:02:14,030 --> 00:02:20,420
categorize specific sets of people are

00:02:18,260 --> 00:02:23,990
people with bias of high risk of theft

00:02:20,420 --> 00:02:25,490
or other activities which is kind of

00:02:23,990 --> 00:02:28,850
proliferating into a lot of

00:02:25,490 --> 00:02:30,110
decision-making systems say for example

00:02:28,850 --> 00:02:32,690
tomorrow if you want to go for a home

00:02:30,110 --> 00:02:35,140
loan and obviously there's a lot of data

00:02:32,690 --> 00:02:37,670
that is being used to come up with these

00:02:35,140 --> 00:02:40,489
approval systems or advisory systems

00:02:37,670 --> 00:02:43,160
that's out there would you want to be at

00:02:40,489 --> 00:02:44,870
risk of being categorized as a default

00:02:43,160 --> 00:02:47,450
based on obviously the lot of data that

00:02:44,870 --> 00:02:49,580
has been used to develop these systems

00:02:47,450 --> 00:02:51,980
and that is the bias that I'm talking

00:02:49,580 --> 00:02:53,959
about is do we want to perpetuate a lot

00:02:51,980 --> 00:02:55,190
of the human biases that we have into

00:02:53,959 --> 00:02:59,060
the systems that we're going to develop

00:02:55,190 --> 00:03:00,440
for the future and you know with humans

00:02:59,060 --> 00:03:02,360
it's easy to replace you know if

00:03:00,440 --> 00:03:04,340
somebody is making a biased decision we

00:03:02,360 --> 00:03:06,260
have existing systems and norms in place

00:03:04,340 --> 00:03:08,780
to address that so the question that

00:03:06,260 --> 00:03:10,459
becomes what are we going to do for a

00:03:08,780 --> 00:03:12,080
lot of Lycia advisory systems that are

00:03:10,459 --> 00:03:14,120
going to be out there obviously deciding

00:03:12,080 --> 00:03:16,330
today tomorrow and in the future do we

00:03:14,120 --> 00:03:20,269
want those biases do we want those

00:03:16,330 --> 00:03:22,190
elements of issues to perpetuate I think

00:03:20,269 --> 00:03:23,780
Meredydd today talked talked about some

00:03:22,190 --> 00:03:25,820
of the elements of it and that's

00:03:23,780 --> 00:03:27,860
specifically the area that we want to

00:03:25,820 --> 00:03:29,269
focus on that yes we want to build all

00:03:27,860 --> 00:03:32,390
these systems yes they need to make

00:03:29,269 --> 00:03:34,970
autonomous decisions yes they need to

00:03:32,390 --> 00:03:37,400
evolve without human intervention but we

00:03:34,970 --> 00:03:38,930
need to make sure that they wall without

00:03:37,400 --> 00:03:41,540
all the biases we come with and

00:03:38,930 --> 00:03:45,820
obviously with reducing their amount of

00:03:41,540 --> 00:03:48,920
risks that we see with these systems now

00:03:45,820 --> 00:03:51,380
you know the way we look at it is we

00:03:48,920 --> 00:03:54,380
need to raise the CI systems as we raise

00:03:51,380 --> 00:03:55,940
a child why do I say that the machine

00:03:54,380 --> 00:03:57,739
learning elements in terms of how they

00:03:55,940 --> 00:03:59,810
learn in terms of how they communicate

00:03:57,739 --> 00:04:01,459
with the conversational agents in terms

00:03:59,810 --> 00:04:03,560
of how they make unbiased decisions as

00:04:01,459 --> 00:04:06,079
they decide specific elements of

00:04:03,560 --> 00:04:08,060
business decisions or even decisions

00:04:06,079 --> 00:04:09,980
that a government would use to recommend

00:04:08,060 --> 00:04:11,690
or deny services to you it has to

00:04:09,980 --> 00:04:13,970
reflect a lot of our values which comes

00:04:11,690 --> 00:04:17,180
with that and for businesses they have

00:04:13,970 --> 00:04:21,079
to adhere to the norms and regulations

00:04:17,180 --> 00:04:22,370
that come with it the question is you

00:04:21,079 --> 00:04:24,380
know all these are problems that we are

00:04:22,370 --> 00:04:26,270
talking about is there a way to kind of

00:04:24,380 --> 00:04:27,350
break into it and see what we can do to

00:04:26,270 --> 00:04:28,700
address it

00:04:27,350 --> 00:04:31,730
yes there are issues that are out there

00:04:28,700 --> 00:04:33,560
but most of these have been visible to a

00:04:31,730 --> 00:04:35,060
lot of us and there is something that we

00:04:33,560 --> 00:04:39,050
want to make an attempt to address some

00:04:35,060 --> 00:04:41,030
of these issues essentially what we are

00:04:39,050 --> 00:04:42,650
trying to do with this is we have come

00:04:41,030 --> 00:04:45,430
out with a construct of teach and test

00:04:42,650 --> 00:04:48,130
in terms of how do we teach and test

00:04:45,430 --> 00:04:50,870
teach the sia systems to obviously have

00:04:48,130 --> 00:04:52,850
you know neutralized biases to the

00:04:50,870 --> 00:04:56,270
existing data sets that you have how do

00:04:52,850 --> 00:04:57,590
we ensure that the algorithms are being

00:04:56,270 --> 00:04:59,720
addressed by some elements of

00:04:57,590 --> 00:05:02,990
metamorphic you know algorithm that we

00:04:59,720 --> 00:05:05,150
have how do we ensure that these systems

00:05:02,990 --> 00:05:08,150
that you have which are already live or

00:05:05,150 --> 00:05:10,820
which are going to be developed we we

00:05:08,150 --> 00:05:12,530
want to share with you some of our views

00:05:10,820 --> 00:05:15,890
on how we can address some of the biases

00:05:12,530 --> 00:05:19,220
that are out there and and address and

00:05:15,890 --> 00:05:20,540
build I would say a responsibility' mats

00:05:19,220 --> 00:05:23,810
out there and we want to raise the game

00:05:20,540 --> 00:05:26,570
of AI and that's that's the specific

00:05:23,810 --> 00:05:29,810
area of interest that we have today and

00:05:26,570 --> 00:05:31,010
we are at our booth at 1:09 will share

00:05:29,810 --> 00:05:34,510
with you some of our ideas and how we

00:05:31,010 --> 00:05:34,510
are addressing this thank you very much

00:05:41,129 --> 00:05:43,189

YouTube URL: https://www.youtube.com/watch?v=4XdLesESUV8


