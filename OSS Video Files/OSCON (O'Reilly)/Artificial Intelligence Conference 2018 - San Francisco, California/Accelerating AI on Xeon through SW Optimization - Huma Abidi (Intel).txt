Title: Accelerating AI on Xeon through SW Optimization - Huma Abidi (Intel)
Publication date: 2018-09-07
Playlist: Artificial Intelligence Conference 2018 - San Francisco, California
Description: 
	Huma Abidi discusses the importance of optimization to deep learning frameworks and shares Xeon performance results and work that Intel is doing with its framework partners, such TensorFlow.

Subscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi

Follow O'Reilly on: 
Twitter: http://twitter.com/oreillymedia
Facebook: http://facebook.com/OReilly
Instagram: https://www.instagram.com/oreillymedia
LinkedIn: https://www.linkedin.com/company-beta/8459/
Captions: 
	00:00:00,000 --> 00:00:06,540
I want to highlight some of the key

00:00:02,520 --> 00:00:10,170
components in making our software

00:00:06,540 --> 00:00:12,599
optimize and enabling zeon so starting

00:00:10,170 --> 00:00:15,299
with our math kernal library i'm

00:00:12,599 --> 00:00:17,490
Chaldean n which is bad karna library

00:00:15,299 --> 00:00:20,130
for deep neural networks as an open

00:00:17,490 --> 00:00:23,630
source library and it has highly

00:00:20,130 --> 00:00:26,250
optimized kernel implementations for the

00:00:23,630 --> 00:00:31,230
popular deep learning constructs such as

00:00:26,250 --> 00:00:35,430
matrix multiplication bullying etc we

00:00:31,230 --> 00:00:38,010
have what we call in graph it's it's a

00:00:35,430 --> 00:00:42,120
compiler which is which is framework

00:00:38,010 --> 00:00:45,899
neutral and it plugs into the frameworks

00:00:42,120 --> 00:00:47,579
and it supports various architectures so

00:00:45,899 --> 00:00:51,300
it's it's sort of an abstraction layer

00:00:47,579 --> 00:00:53,129
or a glue which supports various

00:00:51,300 --> 00:00:56,070
frameworks and then various hardware

00:00:53,129 --> 00:01:00,000
architectures at the bottom and recently

00:00:56,070 --> 00:01:03,000
we up streamed and graph bridge into

00:01:00,000 --> 00:01:07,260
tensorflow and the top rights you can

00:01:03,000 --> 00:01:10,080
see a list of all open source frameworks

00:01:07,260 --> 00:01:12,810
that we support we are what we call

00:01:10,080 --> 00:01:14,840
direct optimization which means we work

00:01:12,810 --> 00:01:17,909
directly with these framework owners

00:01:14,840 --> 00:01:20,070
developers and what we do is all the

00:01:17,909 --> 00:01:22,860
optimization work that we have done it

00:01:20,070 --> 00:01:25,830
gets merged into the main line so our

00:01:22,860 --> 00:01:28,229
developers and customers can can just

00:01:25,830 --> 00:01:33,840
get all the optimization for CPU

00:01:28,229 --> 00:01:36,329
directly when they use these so all this

00:01:33,840 --> 00:01:38,790
optimization work that we are doing is

00:01:36,329 --> 00:01:42,090
showing great results here's an example

00:01:38,790 --> 00:01:44,399
at the Stanford dawn bench competition

00:01:42,090 --> 00:01:47,939
the best inference results were from

00:01:44,399 --> 00:01:51,570
Intel and this was a combination of the

00:01:47,939 --> 00:01:53,909
optimized framework as well as the the

00:01:51,570 --> 00:01:57,450
hardware the xeon processor so it was

00:01:53,909 --> 00:01:59,700
both low latency and low cost so that

00:01:57,450 --> 00:02:03,270
was inference let me give an example for

00:01:59,700 --> 00:02:04,680
training so our engineers work with some

00:02:03,270 --> 00:02:06,719
of the engineers are here in the

00:02:04,680 --> 00:02:10,229
audience as well so they work closely

00:02:06,719 --> 00:02:12,810
with Novartis our partner and they had a

00:02:10,229 --> 00:02:13,530
very challenging problem they had to

00:02:12,810 --> 00:02:16,530
analyze

00:02:13,530 --> 00:02:18,810
these extremely large images like way

00:02:16,530 --> 00:02:22,709
larger than the datasets that we are

00:02:18,810 --> 00:02:25,560
used to and so for that Jian turned out

00:02:22,709 --> 00:02:28,560
to be a great solution because of its

00:02:25,560 --> 00:02:31,620
large memory capacity so taking

00:02:28,560 --> 00:02:34,920
advantage of Intel optimized tensor flow

00:02:31,620 --> 00:02:36,989
and then scaling it up to eight nodes we

00:02:34,920 --> 00:02:40,880
were able to reduce the training time

00:02:36,989 --> 00:02:40,880
from hours to only minutes

00:02:47,810 --> 00:02:49,870

YouTube URL: https://www.youtube.com/watch?v=Uhaj6AF6z7M


