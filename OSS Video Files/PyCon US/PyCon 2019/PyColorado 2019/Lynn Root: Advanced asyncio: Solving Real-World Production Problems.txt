Title: Lynn Root: Advanced asyncio: Solving Real-World Production Problems
Publication date: 2019-10-22
Playlist: PyColorado 2019
Description: 
	Everyone’s talking about it. Everyone’s using it. But most likely, they’re doing it wrong, just like we did. By building a simplified chaos monkey service, we will walk through how to create a good foundation for an asyncio-based service, including graceful shutdowns, proper exception handling, and testing asynchronous code. We’ll get into the hairier topics as well, covering topics like working with synchronous code, debugging and profiling, and working with threaded code. We’ll learn how to approach asynchronous and concurrent programming with Python’s asyncio library, take away some best practices, and learn what pitfalls to avoid.
Captions: 
	00:00:00,370 --> 00:00:46,489
[Music]

00:00:42,840 --> 00:00:48,480
thank you very much my like

00:00:46,489 --> 00:00:50,670
behind-the-scenes title at Spotify is

00:00:48,480 --> 00:00:51,930
site reliability engineer so we're

00:00:50,670 --> 00:00:53,430
trying we're always trying to like think

00:00:51,930 --> 00:00:55,680
on our feet during like incidents and

00:00:53,430 --> 00:00:57,720
stuff so I figured why not you know

00:00:55,680 --> 00:00:59,309
write up on stage do talk like three

00:00:57,720 --> 00:01:04,589
hours before I'm actually ready for it

00:00:59,309 --> 00:01:06,570
okay so yeah my name is Lynn root as

00:01:04,589 --> 00:01:08,580
Frank said I'm a staff engineer at

00:01:06,570 --> 00:01:11,220
Spotify I've been there for about six

00:01:08,580 --> 00:01:13,130
years and since the beginning of this

00:01:11,220 --> 00:01:15,660
year I've been more focused on building

00:01:13,130 --> 00:01:18,090
machine learning infrastructure for

00:01:15,660 --> 00:01:19,979
engineers and researchers that need to

00:01:18,090 --> 00:01:23,130
productionize their digital signal

00:01:19,979 --> 00:01:25,320
processing models so it's like super

00:01:23,130 --> 00:01:27,930
amazing stuff and if anyone here uses

00:01:25,320 --> 00:01:29,580
Apache beam or works with like streaming

00:01:27,930 --> 00:01:30,869
data pipelines I would love to chat with

00:01:29,580 --> 00:01:34,170
you

00:01:30,869 --> 00:01:36,289
I'm also spotify x' Foss evangelist I

00:01:34,170 --> 00:01:38,550
help a lot of teams like release

00:01:36,289 --> 00:01:40,200
projects and tools under the spotlight

00:01:38,550 --> 00:01:43,679
github as well as contribute back to

00:01:40,200 --> 00:01:44,670
projects and then lastly I've helped

00:01:43,679 --> 00:01:47,280
lead pilotis

00:01:44,670 --> 00:01:49,920
the global mentorship group for women

00:01:47,280 --> 00:01:53,849
and friends to help increase diversity

00:01:49,920 --> 00:01:56,130
in the Python community all right

00:01:53,849 --> 00:01:58,950
getting straight to it this is gonna be

00:01:56,130 --> 00:02:03,660
super quick talk because I have a lot of

00:01:58,950 --> 00:02:06,090
material I try and fit in a half an hour

00:02:03,660 --> 00:02:08,489
so yes async i/o it's like the new hot

00:02:06,090 --> 00:02:11,009
thing right the concurrent Python

00:02:08,489 --> 00:02:14,370
programmers dream answer to everyone's

00:02:11,009 --> 00:02:16,319
asynchronous prayers so with the async

00:02:14,370 --> 00:02:19,020
i/o module it has various layers of

00:02:16,319 --> 00:02:21,060
abstraction among developers as much

00:02:19,020 --> 00:02:25,050
control as I need and are comfortable

00:02:21,060 --> 00:02:29,430
with simple hello world like examples

00:02:25,050 --> 00:02:32,730
show how easy and it is to be to make

00:02:29,430 --> 00:02:34,800
something asing it's super simple but we

00:02:32,730 --> 00:02:37,290
kind of get lulled into this false sense

00:02:34,800 --> 00:02:40,040
of security and this kind of example

00:02:37,290 --> 00:02:40,040
isn't that helpful

00:02:41,129 --> 00:02:45,329
so we're made to believe that we can do

00:02:43,319 --> 00:02:47,849
a lot with the structure at a sink and a

00:02:45,329 --> 00:02:49,709
weight API layer some tutorials well

00:02:47,849 --> 00:02:52,680
great for the developer getting their

00:02:49,709 --> 00:02:56,430
toes but try to illustrate real-world

00:02:52,680 --> 00:02:57,810
real-world examples but there are yeah

00:02:56,430 --> 00:03:00,440
real-world examples but they're just

00:02:57,810 --> 00:03:04,230
beefed up hello world examples

00:03:00,440 --> 00:03:06,840
some even misuse parts of a sink iOS

00:03:04,230 --> 00:03:10,409
interface allowing one to easily fall

00:03:06,840 --> 00:03:12,750
into the depths of callback helm and

00:03:10,409 --> 00:03:14,489
there are some tutorials that get you up

00:03:12,750 --> 00:03:16,859
and running pretty easily with a sink I

00:03:14,489 --> 00:03:19,109
oh but then you realize that it's not

00:03:16,859 --> 00:03:20,819
exactly correct or not exactly what you

00:03:19,109 --> 00:03:23,280
want or only gets you part of the way

00:03:20,819 --> 00:03:25,109
there well some tutorials and

00:03:23,280 --> 00:03:28,440
walkthroughs do a lot to improve upon

00:03:25,109 --> 00:03:30,659
the basic hello world it's still just a

00:03:28,440 --> 00:03:33,030
web crawler and I'm not sure about

00:03:30,659 --> 00:03:36,420
others but I'm not building web crawlers

00:03:33,030 --> 00:03:39,480
at Spotify sure I have to build services

00:03:36,420 --> 00:03:41,069
that make a lot of HTTP requests that

00:03:39,480 --> 00:03:43,440
should be non-blocking but these

00:03:41,069 --> 00:03:47,040
services of mine also need to react from

00:03:43,440 --> 00:03:49,680
events from a pub sub we need to measure

00:03:47,040 --> 00:03:53,120
progress of actions initiated from these

00:03:49,680 --> 00:03:56,489
events handle any incomplete actions or

00:03:53,120 --> 00:03:59,879
external errors deal with pub sub

00:03:56,489 --> 00:04:02,250
messages lease management measure

00:03:59,879 --> 00:04:05,669
service level indicators and then send

00:04:02,250 --> 00:04:08,280
metrics and it needs and I also need to

00:04:05,669 --> 00:04:11,129
use non async IO friendly dependencies

00:04:08,280 --> 00:04:14,879
so for me this problem got difficult

00:04:11,129 --> 00:04:16,139
quickly I'm so allow you allow me to

00:04:14,879 --> 00:04:18,090
provide you with a real little

00:04:16,139 --> 00:04:20,789
real-world example that actually comes

00:04:18,090 --> 00:04:24,479
from the real world has anyone heard of

00:04:20,789 --> 00:04:27,630
Netflix's chaos monkey okay I see some

00:04:24,479 --> 00:04:30,240
hands so that Spotify we built something

00:04:27,630 --> 00:04:32,159
similar to a two chaos monkey and we

00:04:30,240 --> 00:04:34,440
basically create chaos we have a service

00:04:32,159 --> 00:04:37,199
creating chaos among all of our back-end

00:04:34,440 --> 00:04:39,120
services where we periodically do hard

00:04:37,199 --> 00:04:42,180
restarts of our entire fleet of

00:04:39,120 --> 00:04:44,490
instances and so we're gonna do the same

00:04:42,180 --> 00:04:46,680
thing here I mean a bill a service

00:04:44,490 --> 00:04:49,530
called mayhem mandrel where we're gonna

00:04:46,680 --> 00:04:51,570
listen for pub/sub message and restart

00:04:49,530 --> 00:04:53,700
the host based off of that message and

00:04:51,570 --> 00:04:56,220
as we build the service I'll

00:04:53,700 --> 00:04:58,680
some best practices that I may or may

00:04:56,220 --> 00:05:01,110
not have realized my first used a sink I

00:04:58,680 --> 00:05:03,300
own and this will essentially become a

00:05:01,110 --> 00:05:07,560
resource that past Lynn would have

00:05:03,300 --> 00:05:09,990
wanted about like three years ago so all

00:05:07,560 --> 00:05:12,570
the code like the slides the whole like

00:05:09,990 --> 00:05:15,000
a little write-up um is on this link

00:05:12,570 --> 00:05:19,080
right here so you can just like sit back

00:05:15,000 --> 00:05:22,050
and listen alright getting to it we'll

00:05:19,080 --> 00:05:23,550
start with some foundational code we're

00:05:22,050 --> 00:05:27,630
gonna write with a we're gonna write a

00:05:23,550 --> 00:05:30,450
simple publisher and so we have like a

00:05:27,630 --> 00:05:34,170
while true loop with a unique ID and I

00:05:30,450 --> 00:05:35,790
for each message to publish Tara Q and I

00:05:34,170 --> 00:05:38,100
want to highlight that we're not going

00:05:35,790 --> 00:05:41,010
to use a weight on Q dot put for a

00:05:38,100 --> 00:05:42,810
message a stink IO dot create task

00:05:41,010 --> 00:05:44,940
actually will actually schedule the

00:05:42,810 --> 00:05:47,880
co-routine on the loop without blocking

00:05:44,940 --> 00:05:50,670
the rest of this for loop the create

00:05:47,880 --> 00:05:52,080
task method does return a task but we

00:05:50,670 --> 00:05:55,980
can also essentially use it as a

00:05:52,080 --> 00:05:58,560
fire-and-forget mechanism so if we added

00:05:55,980 --> 00:06:01,590
a weight here everything after this

00:05:58,560 --> 00:06:04,200
within this published co-routine will be

00:06:01,590 --> 00:06:07,200
blocked and this isn't really an issue

00:06:04,200 --> 00:06:10,490
for a current setup it could be if we

00:06:07,200 --> 00:06:14,280
limited the size of our Q then the await

00:06:10,490 --> 00:06:18,330
will be waiting on space to free up in

00:06:14,280 --> 00:06:21,690
the queue so we're just gonna stick with

00:06:18,330 --> 00:06:23,460
a single create task and so now that we

00:06:21,690 --> 00:06:24,540
have a publisher co-routine function now

00:06:23,460 --> 00:06:28,020
we're going to make a consumer

00:06:24,540 --> 00:06:29,850
co-routine function here our consumer

00:06:28,020 --> 00:06:31,500
for the messages that we've published

00:06:29,850 --> 00:06:34,440
and it's sort of similar to the

00:06:31,500 --> 00:06:37,080
publisher and we will still have a while

00:06:34,440 --> 00:06:40,530
true loop and we await on the queue for

00:06:37,080 --> 00:06:43,710
a message but we wouldn't want to create

00:06:40,530 --> 00:06:45,540
a task out of queue get it mate it makes

00:06:43,710 --> 00:06:47,610
sense to block the rest of the curve

00:06:45,540 --> 00:06:52,440
routine on this because we can't do much

00:06:47,610 --> 00:06:54,600
if there's no messages to consume I want

00:06:52,440 --> 00:06:57,930
to highlight again that we're only

00:06:54,600 --> 00:07:00,270
blocking within the scope of the consume

00:06:57,930 --> 00:07:05,430
care routine we're not actually blocking

00:07:00,270 --> 00:07:07,389
the event loop so let's replace async IO

00:07:05,430 --> 00:07:09,759
dot sleep with a function that

00:07:07,389 --> 00:07:12,129
restart a host I'm sure it looks like

00:07:09,759 --> 00:07:15,129
I'm just pushing the simulated i/o work

00:07:12,129 --> 00:07:17,110
into a restart host function but in

00:07:15,129 --> 00:07:20,379
doing so I'm actually able to create a

00:07:17,110 --> 00:07:25,659
task out of it and therefore not block

00:07:20,379 --> 00:07:28,539
on consuming more messages may want to

00:07:25,659 --> 00:07:30,759
do more than one thing per message like

00:07:28,539 --> 00:07:33,550
for example in addition to restarting a

00:07:30,759 --> 00:07:35,650
host we may want to store the message in

00:07:33,550 --> 00:07:39,879
a database for a potential replaying

00:07:35,650 --> 00:07:42,759
later on so we'll make we'll make use of

00:07:39,879 --> 00:07:44,199
async await tasks again for the save Co

00:07:42,759 --> 00:07:46,479
routine to be scheduled on a loop

00:07:44,199 --> 00:07:51,400
basically checking it over to the loop

00:07:46,479 --> 00:07:53,500
to be executed when it can next so in

00:07:51,400 --> 00:07:56,409
this example the two tasks of restarting

00:07:53,500 --> 00:07:58,900
and saving don't need to depend on one

00:07:56,409 --> 00:08:00,879
another completely sidestepping the

00:07:58,900 --> 00:08:03,969
potential concern or complexity of

00:08:00,879 --> 00:08:06,039
should we restart a host if we fail to

00:08:03,969 --> 00:08:06,849
save a message into the database and

00:08:06,039 --> 00:08:09,340
vice-versa

00:08:06,849 --> 00:08:12,580
kind of skipping that part but maybe you

00:08:09,340 --> 00:08:14,740
want your work to happen serially you

00:08:12,580 --> 00:08:18,129
may not want to have concurrency for

00:08:14,740 --> 00:08:21,069
some asynchronous tasks tasks so for

00:08:18,129 --> 00:08:23,199
instance maybe you want to restart hosts

00:08:21,069 --> 00:08:26,080
that have an uptime of more than seven

00:08:23,199 --> 00:08:28,029
days so just like in banking you should

00:08:26,080 --> 00:08:30,550
check the balance of the account before

00:08:28,029 --> 00:08:33,360
you debit it so needing code to be

00:08:30,550 --> 00:08:35,199
serial to have steps or dependencies

00:08:33,360 --> 00:08:38,709
doesn't mean that you can't be

00:08:35,199 --> 00:08:40,899
asynchronous the await last restart date

00:08:38,709 --> 00:08:43,149
will yield to the loop but that doesn't

00:08:40,899 --> 00:08:45,850
mean that restart host will be the next

00:08:43,149 --> 00:08:47,980
thing in that loop it just allows other

00:08:45,850 --> 00:08:51,370
things outside of the care routine to

00:08:47,980 --> 00:08:53,829
happen so with that in mind I'll just

00:08:51,370 --> 00:08:56,079
put on all this message related logic

00:08:53,829 --> 00:08:57,820
into a separate code routine and so you

00:08:56,079 --> 00:09:01,360
don't have to block the consumption of

00:08:57,820 --> 00:09:04,329
messages and then saving a message

00:09:01,360 --> 00:09:06,370
message shouldn't block a restart of a

00:09:04,329 --> 00:09:08,529
host if needed so we'll just return that

00:09:06,370 --> 00:09:11,140
to being a task and we're just going to

00:09:08,529 --> 00:09:14,199
remove the uptime check and restart

00:09:11,140 --> 00:09:15,930
hosts and indiscriminately because why

00:09:14,199 --> 00:09:18,850
not

00:09:15,930 --> 00:09:20,540
alright so we've pulled messages from a

00:09:18,850 --> 00:09:23,390
queue and we found fanned out

00:09:20,540 --> 00:09:25,610
work based off of that message now we

00:09:23,390 --> 00:09:29,780
might need to perform any finalization

00:09:25,610 --> 00:09:31,520
work off of that message often and

00:09:29,780 --> 00:09:33,350
pub/sub technologies if you don't

00:09:31,520 --> 00:09:35,540
acknowledge a message and within a

00:09:33,350 --> 00:09:38,330
predefined time frame it will get

00:09:35,540 --> 00:09:40,760
redelivery for a finalization task we

00:09:38,330 --> 00:09:44,210
should acknowledge the message so it

00:09:40,760 --> 00:09:47,570
isn't redeliver to us so we currently

00:09:44,210 --> 00:09:49,370
have two separate tasks save and restart

00:09:47,570 --> 00:09:52,000
and we want to make sure that both are

00:09:49,370 --> 00:09:54,710
done before the message is cleaned up

00:09:52,000 --> 00:09:57,080
and we could go back to those sequential

00:09:54,710 --> 00:10:01,040
awaits since that is a very direct way

00:09:57,080 --> 00:10:03,170
to manipulate ordering but we can also

00:10:01,040 --> 00:10:06,650
use callbacks on the complete on a

00:10:03,170 --> 00:10:08,930
completed task but we therefore want is

00:10:06,650 --> 00:10:10,880
to somehow have a task that wraps around

00:10:08,930 --> 00:10:13,730
the two co-routines the save and the

00:10:10,880 --> 00:10:16,490
restart since we have to wait for both

00:10:13,730 --> 00:10:19,280
to finish before cleaning up we can make

00:10:16,490 --> 00:10:22,280
use of a cinco together which returns a

00:10:19,280 --> 00:10:24,380
future like object which will which we

00:10:22,280 --> 00:10:28,670
can then attach a callback of cleanup

00:10:24,380 --> 00:10:31,070
via the add done callback we can now

00:10:28,670 --> 00:10:33,710
just await that future to kick off the

00:10:31,070 --> 00:10:35,720
save and restart host care routines and

00:10:33,710 --> 00:10:38,960
obviously the callback of cleanup will

00:10:35,720 --> 00:10:41,420
be called once those two are done so you

00:10:38,960 --> 00:10:43,310
see here that when we run we see both

00:10:41,420 --> 00:10:45,020
the save co-routine and the restart care

00:10:43,310 --> 00:10:46,970
routine are complete and then the

00:10:45,020 --> 00:10:50,240
cleanup will be called that signifies a

00:10:46,970 --> 00:10:53,500
message being completely done and we

00:10:50,240 --> 00:10:56,720
still maintain appropriate concurrency

00:10:53,500 --> 00:11:02,720
however I personally have an allergy to

00:10:56,720 --> 00:11:06,350
callbacks and perhaps we need cleanup to

00:11:02,720 --> 00:11:10,280
be non-blocking as well so then here we

00:11:06,350 --> 00:11:12,080
can just await clean after gather since

00:11:10,280 --> 00:11:14,540
the order of operations do matter and

00:11:12,080 --> 00:11:20,540
you can see how much cleaner this

00:11:14,540 --> 00:11:22,160
actually is so quick review basic i/o

00:11:20,540 --> 00:11:24,410
it's kind of pretty easy to use

00:11:22,160 --> 00:11:26,450
initially but it doesn't automatically

00:11:24,410 --> 00:11:28,100
mean that you're doing it correctly you

00:11:26,450 --> 00:11:30,590
can't just throw around async and await

00:11:28,100 --> 00:11:33,030
keywords around blocking code it's sort

00:11:30,590 --> 00:11:35,520
of a shift in the mental paradigm

00:11:33,030 --> 00:11:37,580
both with meaning to think of what work

00:11:35,520 --> 00:11:40,170
can be farmed out and let's do its thing

00:11:37,580 --> 00:11:42,060
as well as what dependencies there are

00:11:40,170 --> 00:11:45,030
and where your code might still need to

00:11:42,060 --> 00:11:47,610
be sequential and so having your code

00:11:45,030 --> 00:11:49,890
like having steps within your code like

00:11:47,610 --> 00:11:52,620
having the first a and then B and then C

00:11:49,890 --> 00:11:54,960
might seem like it's blocking when it's

00:11:52,620 --> 00:11:59,040
actually not sequential code can still

00:11:54,960 --> 00:12:01,770
be asynchronous I might have to call a

00:11:59,040 --> 00:12:04,380
customer service for something and wait

00:12:01,770 --> 00:12:05,940
to be taken off cold to talk to them but

00:12:04,380 --> 00:12:08,400
while I wait I can put my phone on

00:12:05,940 --> 00:12:10,740
speaker and put my super needy cat and

00:12:08,400 --> 00:12:12,510
do whatever else I need to do so I might

00:12:10,740 --> 00:12:17,000
be like single-threaded as a person but

00:12:12,510 --> 00:12:19,860
I can multitask like CPUs alright so

00:12:17,000 --> 00:12:21,900
often you might want your host or your

00:12:19,860 --> 00:12:24,450
service to gracefully shutdown if it

00:12:21,900 --> 00:12:27,450
receives a signal of some sort like

00:12:24,450 --> 00:12:29,790
cleanup open database connections stop

00:12:27,450 --> 00:12:32,160
consuming messages finish responding to

00:12:29,790 --> 00:12:36,270
current requests while not accepting new

00:12:32,160 --> 00:12:38,400
requests so if we happen to restart our

00:12:36,270 --> 00:12:40,410
own instance of our own like mayhem

00:12:38,400 --> 00:12:44,400
mandrill service we should clean

00:12:40,410 --> 00:12:47,220
ourselves up as well so here's some

00:12:44,400 --> 00:12:50,370
typical boilerplate to get a service

00:12:47,220 --> 00:12:52,320
running we have a queue instance and

00:12:50,370 --> 00:12:54,750
setting up a loop and I'm scheduling the

00:12:52,320 --> 00:12:58,740
publish and consume tasks and starting

00:12:54,750 --> 00:13:01,200
and closing the event loop so maybe you

00:12:58,740 --> 00:13:04,530
even catch the commonly known keyboard

00:13:01,200 --> 00:13:08,010
interrupt exception so if we run it as

00:13:04,530 --> 00:13:10,100
is and give it the SIGINT signal we can

00:13:08,010 --> 00:13:14,339
see that the accept and finally block

00:13:10,100 --> 00:13:16,410
within those two log lines but if we

00:13:14,339 --> 00:13:19,950
send our program a signal other than sig

00:13:16,410 --> 00:13:24,540
end like sig term we don't actually

00:13:19,950 --> 00:13:26,760
reach that finally clause so we should

00:13:24,540 --> 00:13:29,430
be seeing like a log message where we're

00:13:26,760 --> 00:13:31,050
cleaning up and closing the loop it

00:13:29,430 --> 00:13:33,270
should also be pointed out that even if

00:13:31,050 --> 00:13:35,970
we're only ever to expect a keyboard

00:13:33,270 --> 00:13:38,280
interrupt or SIGINT signal that it could

00:13:35,970 --> 00:13:40,230
happen outside of catching that

00:13:38,280 --> 00:13:42,060
exception potentially causing the

00:13:40,230 --> 00:13:46,200
service to end up in an incomplete or

00:13:42,060 --> 00:13:46,649
otherwise unknown State so instead of

00:13:46,200 --> 00:13:48,329
cat

00:13:46,649 --> 00:13:51,649
 were to interrupt let's actually

00:13:48,329 --> 00:13:54,540
attach a signal handler to the loop I

00:13:51,649 --> 00:13:56,850
will define a shutdown pair routine that

00:13:54,540 --> 00:14:00,360
will be responsible for doing all of our

00:13:56,850 --> 00:14:02,519
necessary shutdown tasks here I'm just

00:14:00,360 --> 00:14:04,740
closing simulated database connections

00:14:02,519 --> 00:14:06,749
returning pub-like messages to pub/sub

00:14:04,740 --> 00:14:08,279
has not acknowledged so they can be

00:14:06,749 --> 00:14:10,860
redeliver de ventually and not dropped

00:14:08,279 --> 00:14:12,629
and then collecting all of outstanding

00:14:10,860 --> 00:14:15,449
tasks except for the shutdown task

00:14:12,629 --> 00:14:18,329
itself and canceling them we don't

00:14:15,449 --> 00:14:20,759
necessarily need to cancel pending tasks

00:14:18,329 --> 00:14:23,759
we could just collect and allow them to

00:14:20,759 --> 00:14:25,589
finish and we might also want to take

00:14:23,759 --> 00:14:29,490
the opportunity to flush any collected

00:14:25,589 --> 00:14:31,920
metrics so that they are not lost so

00:14:29,490 --> 00:14:35,220
then let's add our shutdown care routine

00:14:31,920 --> 00:14:37,949
to the event loop so the first thing is

00:14:35,220 --> 00:14:41,579
we set up our loop and then add signal

00:14:37,949 --> 00:14:44,009
handler add our signal handler to the

00:14:41,579 --> 00:14:45,839
loop with our desired signals that we

00:14:44,009 --> 00:14:49,439
want to respond to and then we can also

00:14:45,839 --> 00:14:51,689
remove the keyboard interrupt catch so

00:14:49,439 --> 00:14:53,579
then running this again we see that we

00:14:51,689 --> 00:14:57,749
do actually get to that finally clause

00:14:53,579 --> 00:14:59,339
with something other than SIGINT now you

00:14:57,749 --> 00:15:00,809
might be wondering which signals to

00:14:59,339 --> 00:15:02,970
react to what should you actually care

00:15:00,809 --> 00:15:07,050
about and apparently there is no

00:15:02,970 --> 00:15:08,699
standard basically you should be aware

00:15:07,050 --> 00:15:11,670
of how you're running your service and

00:15:08,699 --> 00:15:15,480
handle it accordingly and it seems like

00:15:11,670 --> 00:15:17,670
it can be a quite messy with conflicting

00:15:15,480 --> 00:15:22,740
signals and particularly with adding

00:15:17,670 --> 00:15:26,220
docker into the mix so we don't have

00:15:22,740 --> 00:15:28,290
like nurseries in ASIC i/o core to clean

00:15:26,220 --> 00:15:29,790
ourselves up and it's responsible so

00:15:28,290 --> 00:15:32,459
it's up to us to be responsible and

00:15:29,790 --> 00:15:34,319
close up connections and files respond

00:15:32,459 --> 00:15:38,100
to outstanding requests and basically

00:15:34,319 --> 00:15:39,809
leave things how we found them doing our

00:15:38,100 --> 00:15:41,999
cleanup in a finally Clause isn't enough

00:15:39,809 --> 00:15:44,730
though since the signal could be sent

00:15:41,999 --> 00:15:47,189
outside of a try except clause and so as

00:15:44,730 --> 00:15:49,110
we construct the loop we should tell it

00:15:47,189 --> 00:15:52,619
how it should be deconstructed as soon

00:15:49,110 --> 00:15:53,970
as possible in the program we should we

00:15:52,619 --> 00:15:56,279
need to also be aware of when our

00:15:53,970 --> 00:16:00,029
program shuts down or should shut down

00:15:56,279 --> 00:16:00,490
it should be closely tied to about how

00:16:00,029 --> 00:16:02,709
we run our

00:16:00,490 --> 00:16:04,870
program so if it's a manual script maybe

00:16:02,709 --> 00:16:07,390
SIGINT is fine but if it's demonized

00:16:04,870 --> 00:16:10,839
within a docker container then Sig Tara

00:16:07,390 --> 00:16:12,580
might be more appropriate I didn't I

00:16:10,839 --> 00:16:15,459
don't have enough time to go into it but

00:16:12,580 --> 00:16:16,930
if you need to use async i/o shield to

00:16:15,459 --> 00:16:19,570
kind of shield your tasks from

00:16:16,930 --> 00:16:21,610
cancellation be aware that it doesn't

00:16:19,570 --> 00:16:23,230
necessarily respect that respect to

00:16:21,610 --> 00:16:27,180
shutdown behavior with a signal handler

00:16:23,230 --> 00:16:30,160
so just be aware

00:16:27,180 --> 00:16:31,149
moving on to exception handling you

00:16:30,160 --> 00:16:35,170
might have noticed that we haven't

00:16:31,149 --> 00:16:36,940
really handled an exception so far we're

00:16:35,170 --> 00:16:39,790
gonna revisit our restart host

00:16:36,940 --> 00:16:44,170
co-routine and add a super-realistic

00:16:39,790 --> 00:16:46,450
exception so when running this we do see

00:16:44,170 --> 00:16:50,470
that there is that super serious

00:16:46,450 --> 00:16:52,870
exception raised but we also get a task

00:16:50,470 --> 00:16:55,300
exception was never retrieved and we

00:16:52,870 --> 00:16:58,839
don't properly handle the result of a

00:16:55,300 --> 00:17:01,770
task when it raises what we can do is

00:16:58,839 --> 00:17:05,170
define an exception Handler and then

00:17:01,770 --> 00:17:10,390
attach it to our loop similar to signal

00:17:05,170 --> 00:17:13,390
handling so then when we re running this

00:17:10,390 --> 00:17:18,640
we see that the logging exception

00:17:13,390 --> 00:17:20,530
actually happens so we have set an

00:17:18,640 --> 00:17:21,939
exception handling on a global level but

00:17:20,530 --> 00:17:25,530
perhaps you want to treat some

00:17:21,939 --> 00:17:27,910
exceptions for certain tasks differently

00:17:25,530 --> 00:17:29,800
we're gonna revisit our handle message

00:17:27,910 --> 00:17:32,530
care routine so say for instance you're

00:17:29,800 --> 00:17:35,410
fine with logging when saved the message

00:17:32,530 --> 00:17:37,929
fails but you want to nak or not

00:17:35,410 --> 00:17:40,990
acknowledge the pub/sub message and put

00:17:37,929 --> 00:17:44,400
it back in the queue if if restarting a

00:17:40,990 --> 00:17:49,140
host fails or something like that

00:17:44,400 --> 00:17:52,390
so since async I gather returns results

00:17:49,140 --> 00:17:56,790
we can add a more fine-grain exception

00:17:52,390 --> 00:18:00,340
handler based off of that and handle

00:17:56,790 --> 00:18:02,860
handle results as we wish now I want to

00:18:00,340 --> 00:18:06,520
highlight that setting return exceptions

00:18:02,860 --> 00:18:08,770
to true for a single gather is super

00:18:06,520 --> 00:18:11,020
imperative otherwise exceptions will be

00:18:08,770 --> 00:18:12,730
handled by the default handler if it's

00:18:11,020 --> 00:18:13,309
said and if there's no default handler

00:18:12,730 --> 00:18:14,629
set

00:18:13,309 --> 00:18:17,480
and it just kind of gets swallowed up

00:18:14,629 --> 00:18:19,009
and sort of blocks itself and it's kind

00:18:17,480 --> 00:18:24,049
of hidden because the service would

00:18:19,009 --> 00:18:26,509
otherwise keep running so a quick review

00:18:24,049 --> 00:18:28,070
over exception handling be sure to set

00:18:26,509 --> 00:18:31,370
some sort of exception handling either

00:18:28,070 --> 00:18:33,440
globally individually or a mix otherwise

00:18:31,370 --> 00:18:37,039
exceptions will go and notice and can

00:18:33,440 --> 00:18:39,499
cause weird behavior I personally like

00:18:37,039 --> 00:18:41,120
using async ioad gather because the

00:18:39,499 --> 00:18:43,639
order of returned results are

00:18:41,120 --> 00:18:46,490
deterministic but it's easy to get

00:18:43,639 --> 00:18:48,440
tripped up with it so by default it will

00:18:46,490 --> 00:18:50,990
swallow exceptions but happily continue

00:18:48,440 --> 00:18:54,169
working in other tasks tasks that it was

00:18:50,990 --> 00:18:56,090
given so if an exception was never

00:18:54,169 --> 00:18:59,509
returned then the weird behavior will

00:18:56,090 --> 00:19:02,240
happen all right

00:18:59,509 --> 00:19:06,669
so sometimes you need to work with

00:19:02,240 --> 00:19:09,649
threads like a threaded pub/sub client

00:19:06,669 --> 00:19:12,590
maybe you want to consume a message on

00:19:09,649 --> 00:19:14,509
one thread and handle the message within

00:19:12,590 --> 00:19:18,110
your coding in the main event loop which

00:19:14,509 --> 00:19:20,779
is a separate thread let's first attempt

00:19:18,110 --> 00:19:23,330
to use the async i/o API that we're

00:19:20,779 --> 00:19:26,330
familiar with and update our synchronous

00:19:23,330 --> 00:19:28,999
callback function with creating a task

00:19:26,330 --> 00:19:30,559
via async i/o dot create tasks from the

00:19:28,999 --> 00:19:35,179
handle message care routine we defined

00:19:30,559 --> 00:19:37,639
earlier will then call our threaded

00:19:35,179 --> 00:19:39,740
consume function via a thread pool

00:19:37,639 --> 00:19:43,490
executor much like how we would treat

00:19:39,740 --> 00:19:45,950
synchronous code but we don't actually

00:19:43,490 --> 00:19:48,769
get that far and at this point we're in

00:19:45,950 --> 00:19:50,629
another thread and there's no loop

00:19:48,769 --> 00:19:54,259
running in that thread only in the main

00:19:50,629 --> 00:19:56,330
effect in the main thread okay so if we

00:19:54,259 --> 00:19:59,029
take what we have right now and then

00:19:56,330 --> 00:20:03,350
update our functions to use the main

00:19:59,029 --> 00:20:07,249
event loop it looks like it works right

00:20:03,350 --> 00:20:07,970
and but it's actually deceptive we're

00:20:07,249 --> 00:20:11,179
kind of lucky

00:20:07,970 --> 00:20:12,919
that it works so in an effort to save

00:20:11,179 --> 00:20:16,129
time I'm just going to tell you that

00:20:12,919 --> 00:20:18,950
we're not being thread safe so instead

00:20:16,129 --> 00:20:22,250
of loop dot create task we should be

00:20:18,950 --> 00:20:24,529
using async I use thread safe API the

00:20:22,250 --> 00:20:26,820
run and specifically the run care

00:20:24,529 --> 00:20:29,070
routine thread safe method

00:20:26,820 --> 00:20:30,780
and it can be difficult to tell when

00:20:29,070 --> 00:20:31,830
you're not being thread safe if you are

00:20:30,780 --> 00:20:34,710
if you are in a multi-threaded

00:20:31,830 --> 00:20:36,690
environment particularly when it looks

00:20:34,710 --> 00:20:39,419
like it does work as it did you know

00:20:36,690 --> 00:20:42,929
previously but later on I'll show you

00:20:39,419 --> 00:20:46,159
how it is how easy it is to surface the

00:20:42,929 --> 00:20:48,510
issue of thread safety

00:20:46,159 --> 00:20:51,299
all right so in my opinion it's not too

00:20:48,510 --> 00:20:55,230
difficult to work with threaded code

00:20:51,299 --> 00:20:57,360
with async I am similarly to how and we

00:20:55,230 --> 00:20:59,309
work with non async code in an async

00:20:57,360 --> 00:21:01,950
rolled we make use of a thread pool

00:20:59,309 --> 00:21:04,530
executor which essentially creates an

00:21:01,950 --> 00:21:06,510
avoidable for us however it is difficult

00:21:04,530 --> 00:21:08,309
to work with threads in async IO

00:21:06,510 --> 00:21:10,770
particularly when sharing the state

00:21:08,309 --> 00:21:14,370
between threads in the main event loop

00:21:10,770 --> 00:21:17,580
so if you must use the thread safe API

00:21:14,370 --> 00:21:19,440
is that async code gives you and it did

00:21:17,580 --> 00:21:21,120
take me a bare singly long time to

00:21:19,440 --> 00:21:27,059
figure out figure out that I needed this

00:21:21,120 --> 00:21:28,679
so moving on to testing so for a more

00:21:27,059 --> 00:21:31,049
simplistic starting point I'm going to

00:21:28,679 --> 00:21:33,840
test a single code before we introduced

00:21:31,049 --> 00:21:36,330
a threading so we'll start simple

00:21:33,840 --> 00:21:41,490
testing the safeco routine using pi

00:21:36,330 --> 00:21:44,909
tests since a save is occurring we need

00:21:41,490 --> 00:21:47,130
our tests to run it in an event loop and

00:21:44,909 --> 00:21:51,030
so like so

00:21:47,130 --> 00:21:54,960
so in 3.7 and it makes it easy for us to

00:21:51,030 --> 00:21:57,240
just call a single run but with older

00:21:54,960 --> 00:22:00,530
python versions we'll have to construct

00:21:57,240 --> 00:22:04,020
and deconstruct the event loop ourselves

00:22:00,530 --> 00:22:06,419
but there is a better way there is a PI

00:22:04,020 --> 00:22:08,429
test plug-in called PI test async IO

00:22:06,419 --> 00:22:11,880
that will essentially do the hard work

00:22:08,429 --> 00:22:14,370
for you you'll need to mark particular

00:22:11,880 --> 00:22:16,289
tasks that are testing a sinker

00:22:14,370 --> 00:22:19,770
asynchronous code with a decorator from

00:22:16,289 --> 00:22:21,630
the plug-in as well as make it so that

00:22:19,770 --> 00:22:23,970
the test function itself is actually a

00:22:21,630 --> 00:22:25,650
co-routine function so now when running

00:22:23,970 --> 00:22:26,370
tests the plugin will essentially do the

00:22:25,650 --> 00:22:27,900
work for you

00:22:26,370 --> 00:22:32,610
constructing and be constructing the

00:22:27,900 --> 00:22:35,190
event loop so the PI test async i/o

00:22:32,610 --> 00:22:37,020
plug-in can get you pretty far but it

00:22:35,190 --> 00:22:39,720
doesn't help you when you need to mock

00:22:37,020 --> 00:22:41,909
out co-routines

00:22:39,720 --> 00:22:43,950
so for instance our save care routine

00:22:41,909 --> 00:22:46,649
function calls another care routine

00:22:43,950 --> 00:22:50,580
function with a synced sleep or an

00:22:46,649 --> 00:22:52,620
actual call to a database you don't you

00:22:50,580 --> 00:22:54,149
don't actually want to wait for async IO

00:22:52,620 --> 00:22:56,039
dot sleep to complete while you're

00:22:54,149 --> 00:22:59,519
running tasks nor do you actually want

00:22:56,039 --> 00:23:02,039
to call a database during your tests in

00:22:59,519 --> 00:23:04,100
both the unit tests that mock and PI

00:23:02,039 --> 00:23:06,539
tests mock libraries do not support

00:23:04,100 --> 00:23:08,700
asynchronous mock a secure des mots but

00:23:06,539 --> 00:23:11,299
by nature so we'll have to kind of work

00:23:08,700 --> 00:23:11,299
around this

00:23:12,059 --> 00:23:17,129
so first we'll kind of build upon the PI

00:23:14,820 --> 00:23:19,970
test mock library will first create a PI

00:23:17,129 --> 00:23:22,649
test fixture that will return a function

00:23:19,970 --> 00:23:24,330
and so the outer function itself returns

00:23:22,649 --> 00:23:27,929
the inner function as a fixture that

00:23:24,330 --> 00:23:29,429
we'll end up using in our tests and then

00:23:27,929 --> 00:23:31,889
the inner function is basically creating

00:23:29,429 --> 00:23:34,230
and returning a mock object that we will

00:23:31,889 --> 00:23:37,230
actually use in our test as well as stub

00:23:34,230 --> 00:23:41,220
a stub care routine that that will end

00:23:37,230 --> 00:23:43,289
up calling our mock it will also patch

00:23:41,220 --> 00:23:45,450
if needed the desired care routine with

00:23:43,289 --> 00:23:49,559
the stub to avoid any Network calls or

00:23:45,450 --> 00:23:52,259
sleeps etc so now let's create another

00:23:49,559 --> 00:23:54,179
PI test fixture that will use the create

00:23:52,259 --> 00:23:56,730
Cobra mock mock fixture to mock and

00:23:54,179 --> 00:23:58,409
patched an async i/o dot sleep and we

00:23:56,730 --> 00:24:01,559
don't need the stub care routine that

00:23:58,409 --> 00:24:05,360
create cover mock fixture function

00:24:01,559 --> 00:24:08,220
returns so we can just throw it away and

00:24:05,360 --> 00:24:12,960
now let's use a mock sleep fixture in

00:24:08,220 --> 00:24:14,879
our test save function so what we've

00:24:12,960 --> 00:24:17,639
done here is basic basically patched a

00:24:14,879 --> 00:24:21,029
synced sleep and our mayhem module with

00:24:17,639 --> 00:24:23,250
a stub co-routine function then we

00:24:21,029 --> 00:24:25,980
assert that the mocked and async about

00:24:23,250 --> 00:24:29,039
sleep object is called once mayhem dot

00:24:25,980 --> 00:24:30,809
sleep is called because we now have a

00:24:29,039 --> 00:24:33,179
mock object instead of an actual

00:24:30,809 --> 00:24:35,070
co-routine we can do anything and that's

00:24:33,179 --> 00:24:37,559
supported with standard mock objects

00:24:35,070 --> 00:24:42,629
like assert called once with or setting

00:24:37,559 --> 00:24:44,580
return values side effects etc so it's

00:24:42,629 --> 00:24:48,389
pretty simple like in my point of view I

00:24:44,580 --> 00:24:50,730
guess but you might need to test care

00:24:48,389 --> 00:24:53,070
routine functions that call create tasks

00:24:50,730 --> 00:24:53,540
and we can't simply use the create cover

00:24:53,070 --> 00:24:57,410
mock

00:24:53,540 --> 00:24:59,690
fix your however so for instance let's

00:24:57,410 --> 00:25:02,360
revisit our consume care routine which

00:24:59,690 --> 00:25:06,730
schedules crates and schedules tap a

00:25:02,360 --> 00:25:10,130
task on the loop out of handle message

00:25:06,730 --> 00:25:12,680
well first need a couple of fixtures for

00:25:10,130 --> 00:25:16,700
the cue that gets passed in so we'll

00:25:12,680 --> 00:25:20,690
first mock and patch async i oq class in

00:25:16,700 --> 00:25:22,910
our module well then use that mock u

00:25:20,690 --> 00:25:27,020
fixture in another one a mock and get

00:25:22,910 --> 00:25:28,670
fixture unlike our mock sleep fixture we

00:25:27,020 --> 00:25:30,890
will actually use the stub care routine

00:25:28,670 --> 00:25:35,000
that create code mock returns and set it

00:25:30,890 --> 00:25:37,010
to the mock queues get method and so

00:25:35,000 --> 00:25:39,440
here in our test consume function what

00:25:37,010 --> 00:25:43,310
we're giving our newly created we're

00:25:39,440 --> 00:25:46,070
giving our newly created fixtures so now

00:25:43,310 --> 00:25:49,090
let's try to use create / mock to mock

00:25:46,070 --> 00:25:53,960
and patch the call to handle message

00:25:49,090 --> 00:25:58,220
care routine via create task let's see

00:25:53,960 --> 00:25:59,900
we're not resetting a side-effect to one

00:25:58,220 --> 00:26:01,850
real value in one exception to make sure

00:25:59,900 --> 00:26:04,730
we're not potentially not permantly

00:26:01,850 --> 00:26:06,820
stuck in that while true loop and

00:26:04,730 --> 00:26:09,560
finally we want to assert that our mock

00:26:06,820 --> 00:26:11,270
that our mock for handle message has

00:26:09,560 --> 00:26:14,540
been called after the consume has been

00:26:11,270 --> 00:26:16,970
run when running this we see that mock

00:26:14,540 --> 00:26:19,160
handle message does not actually get

00:26:16,970 --> 00:26:20,870
called like we're expecting this is

00:26:19,160 --> 00:26:23,030
because scheduled tasks are only

00:26:20,870 --> 00:26:26,210
scheduled in pending at this point we

00:26:23,030 --> 00:26:29,570
sort of need to nudge them along we do

00:26:26,210 --> 00:26:31,580
this by collecting all returned tasks

00:26:29,570 --> 00:26:33,830
not just the tests it's not not that

00:26:31,580 --> 00:26:36,650
test itself and then running them

00:26:33,830 --> 00:26:40,130
explicitly this is a little clunky I

00:26:36,650 --> 00:26:41,420
know but if you use particularly used

00:26:40,130 --> 00:26:44,330
unit tests from the standard library

00:26:41,420 --> 00:26:46,490
there's a package called async test that

00:26:44,330 --> 00:26:49,460
handles this better and exhausts the

00:26:46,490 --> 00:26:52,580
scheduled tasks for you I only have like

00:26:49,460 --> 00:26:54,350
a few minutes left and I'm going to skip

00:26:52,580 --> 00:26:57,020
the testing the event loop which is

00:26:54,350 --> 00:27:00,440
covered in the write-up that is linked

00:26:57,020 --> 00:27:05,450
at the bottom and I want to go directly

00:27:00,440 --> 00:27:06,919
to debugging all right so we're decent

00:27:05,450 --> 00:27:09,799
programmers right and we have

00:27:06,919 --> 00:27:11,899
a good test coverage but sometimes

00:27:09,799 --> 00:27:15,139
breaks and we need to figure out what's

00:27:11,899 --> 00:27:16,369
going on so we're gonna use everyone's

00:27:15,139 --> 00:27:20,809
favorite debugger we're gonna print

00:27:16,369 --> 00:27:22,340
everything sort of I guess so if you

00:27:20,809 --> 00:27:25,669
only have like a one small thing to

00:27:22,340 --> 00:27:31,159
debug you can use print stack method on

00:27:25,669 --> 00:27:32,570
a task instance so when running this it

00:27:31,159 --> 00:27:35,149
will print the stack for each running

00:27:32,570 --> 00:27:37,399
task and you can also increase the

00:27:35,149 --> 00:27:40,669
number of frames that are printed as

00:27:37,399 --> 00:27:43,970
well however there's a more proper

00:27:40,669 --> 00:27:45,590
debugging method a single has a debug

00:27:43,970 --> 00:27:49,489
mode already built in the standard

00:27:45,590 --> 00:27:52,190
library itself so along with setting our

00:27:49,489 --> 00:27:54,679
logging level to debug we can easily

00:27:52,190 --> 00:27:58,070
turn on a sink I chose debug mode and

00:27:54,679 --> 00:28:00,440
when we were running our script if we

00:27:58,070 --> 00:28:04,100
didn't have a proper exception handling

00:28:00,440 --> 00:28:07,009
setup we'd get information about which

00:28:04,100 --> 00:28:08,690
task was affected but we also get a

00:28:07,009 --> 00:28:12,019
source trace back to give us more

00:28:08,690 --> 00:28:12,590
context in addition to our normal trace

00:28:12,019 --> 00:28:14,960
back

00:28:12,590 --> 00:28:16,669
so without the debug mode we get told

00:28:14,960 --> 00:28:17,629
that there is an exception that's not

00:28:16,669 --> 00:28:19,460
properly handled

00:28:17,629 --> 00:28:23,539
but the debug mode gives us additional

00:28:19,460 --> 00:28:26,570
clues of where they might be another

00:28:23,539 --> 00:28:28,519
handy thing as the refrig to earlier and

00:28:26,570 --> 00:28:29,929
I wish I knew this a few years ago is

00:28:28,519 --> 00:28:31,879
that if you use if you have to have

00:28:29,929 --> 00:28:32,570
threads and the event loop interacting

00:28:31,879 --> 00:28:35,929
with each other

00:28:32,570 --> 00:28:41,210
debug mode will surface any sort of non

00:28:35,929 --> 00:28:43,639
thread safe operation for you one really

00:28:41,210 --> 00:28:45,259
nice feature of the debug mode in a sync

00:28:43,639 --> 00:28:47,869
IO as it acts like a tiny little

00:28:45,259 --> 00:28:50,139
profiler in that it will log

00:28:47,869 --> 00:28:54,679
asynchronous calls that are slower than

00:28:50,139 --> 00:28:56,299
100 milliseconds so I can show you real

00:28:54,679 --> 00:28:59,029
quick like we're gonna fake a slow curve

00:28:56,299 --> 00:29:01,549
routine and when we run that we can see

00:28:59,029 --> 00:29:03,619
that the debug mode will surface so to

00:29:01,549 --> 00:29:06,580
finish tasks potentially highlighting a

00:29:03,619 --> 00:29:09,139
necessary unnecessarily blocking tasks

00:29:06,580 --> 00:29:11,509
and the default of what's considered

00:29:09,139 --> 00:29:13,730
slow is 100 milliseconds but that is

00:29:11,509 --> 00:29:19,159
config configurable as well you can just

00:29:13,730 --> 00:29:20,750
set slow callback duration in seconds so

00:29:19,159 --> 00:29:22,340
much like some people's testing

00:29:20,750 --> 00:29:25,520
sofie's sometimes we want to debug in

00:29:22,340 --> 00:29:28,250
production you might not want full of

00:29:25,520 --> 00:29:31,540
debug mode on so there's a super

00:29:28,250 --> 00:29:34,940
lightweight package called a IOT bug

00:29:31,540 --> 00:29:36,260
that will log slow callbacks for you and

00:29:34,940 --> 00:29:38,860
that's literally the only thing that it

00:29:36,260 --> 00:29:43,310
does no extra context or thread safety

00:29:38,860 --> 00:29:45,550
well we've got like one minute left it

00:29:43,310 --> 00:29:49,790
does come with the ability to report

00:29:45,550 --> 00:29:51,770
delay callbacks to stats D as well so

00:29:49,790 --> 00:29:54,350
you got print TAS to get debug mode and

00:29:51,770 --> 00:29:57,710
a i/o debug alright I want to get to

00:29:54,350 --> 00:29:59,930
profiling this is last section so

00:29:57,710 --> 00:30:03,770
sometimes you might need to profile you

00:29:59,930 --> 00:30:05,350
can start off by by C profile it doesn't

00:30:03,770 --> 00:30:07,820
you can't really glean that much though

00:30:05,350 --> 00:30:10,670
the top item here is the event loop

00:30:07,820 --> 00:30:12,290
itself if we just look at our own module

00:30:10,670 --> 00:30:15,700
we can kind of get a picture of what's

00:30:12,290 --> 00:30:18,200
going on nothing is immediately surfaced

00:30:15,700 --> 00:30:20,720
so I recently discovered that K cash

00:30:18,200 --> 00:30:22,730
grind can be used with Python and so to

00:30:20,720 --> 00:30:25,640
do so we must save the output of C

00:30:22,730 --> 00:30:27,980
profile and then use a package called PI

00:30:25,640 --> 00:30:29,630
prof to culturing it makes the output of

00:30:27,980 --> 00:30:31,310
C profile and converts the data into

00:30:29,630 --> 00:30:33,860
something that K cash grind can

00:30:31,310 --> 00:30:36,260
understand so when you run that script

00:30:33,860 --> 00:30:37,520
you met with you met with this UI and

00:30:36,260 --> 00:30:39,200
it's okay if you can't really see

00:30:37,520 --> 00:30:42,500
anything but basically on the left hand

00:30:39,200 --> 00:30:43,730
side I got like I'm out of time but in

00:30:42,500 --> 00:30:46,070
left hand side there's profiling data

00:30:43,730 --> 00:30:47,960
that you'd otherwise see from see

00:30:46,070 --> 00:30:50,570
profile and then sections on the right

00:30:47,960 --> 00:30:54,020
are the call graph and information about

00:30:50,570 --> 00:30:57,070
Khali's I want to highlight that there's

00:30:54,020 --> 00:31:00,710
also line profilers so you get a bigger

00:30:57,070 --> 00:31:02,270
visual with the K cash brine then you

00:31:00,710 --> 00:31:08,090
can use line profiler to highlight

00:31:02,270 --> 00:31:09,680
specific lines of code and then you can

00:31:08,090 --> 00:31:12,500
see that like logging is kind of slow

00:31:09,680 --> 00:31:15,320
here so if we use a i/o logger you can

00:31:12,500 --> 00:31:17,600
kind of speed that up a bit and then

00:31:15,320 --> 00:31:20,200
there's also this live profiling package

00:31:17,600 --> 00:31:22,200
that I don't have time to go into and

00:31:20,200 --> 00:31:30,000
thank you sorry

00:31:22,200 --> 00:31:43,299
[Applause]

00:31:30,000 --> 00:31:43,299

YouTube URL: https://www.youtube.com/watch?v=u0JghuNCenY


