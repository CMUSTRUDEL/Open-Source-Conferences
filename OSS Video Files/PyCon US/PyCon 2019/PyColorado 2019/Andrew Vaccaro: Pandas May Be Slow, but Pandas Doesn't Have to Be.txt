Title: Andrew Vaccaro: Pandas May Be Slow, but Pandas Doesn't Have to Be
Publication date: 2019-10-22
Playlist: PyColorado 2019
Description: 
	Python is quickly becoming the de-facto language of data science. Pandas is one of the most popular libraries for managing and transforming data, but many users are unaware of strategies to tune pandas for performance. Simple changes can greatly speed up data processing when applied correctly.
Captions: 
	00:00:00,370 --> 00:00:46,530
[Music]

00:00:43,430 --> 00:00:49,379
um yeah so I guess I've already been

00:00:46,530 --> 00:00:52,079
introduced so I don't need to say my

00:00:49,379 --> 00:00:55,199
name again but I work as a data engineer

00:00:52,079 --> 00:00:58,379
at empiric health and I was actually

00:00:55,199 --> 00:01:00,330
basically brought on to do one job and

00:00:58,379 --> 00:01:02,970
that is take a lot of the code that data

00:01:00,330 --> 00:01:05,009
scientist have written and turn it into

00:01:02,970 --> 00:01:08,009
a more efficient pipeline so that we can

00:01:05,009 --> 00:01:09,479
run client data through it somewhat you

00:01:08,009 --> 00:01:11,880
know turning it into a product as well

00:01:09,479 --> 00:01:12,810
for internal consumption and so that's

00:01:11,880 --> 00:01:16,649
the kind of thing that I'm going to be

00:01:12,810 --> 00:01:19,110
covering today for those of you who may

00:01:16,649 --> 00:01:21,000
not work a lot in data science so for

00:01:19,110 --> 00:01:23,700
example the people who are more involved

00:01:21,000 --> 00:01:26,180
in web development or DevOps I'm going

00:01:23,700 --> 00:01:30,420
to be talking about all of this in the

00:01:26,180 --> 00:01:33,119
context of pandas which is a Python

00:01:30,420 --> 00:01:35,280
library for managing tabular data so you

00:01:33,119 --> 00:01:36,600
can kind of think of it as dealing with

00:01:35,280 --> 00:01:38,789
like an Excel spreadsheet or a

00:01:36,600 --> 00:01:42,810
relational database table but it's all

00:01:38,789 --> 00:01:46,160
in Python you know rows columns pretty

00:01:42,810 --> 00:01:49,009
much kind of standard terminology

00:01:46,160 --> 00:01:53,009
first things first just a quick kind of

00:01:49,009 --> 00:01:54,360
helper function for us and I guess I'll

00:01:53,009 --> 00:01:56,310
take a step back if you've never used

00:01:54,360 --> 00:01:57,660
Jupiter notebooks before or this is

00:01:56,310 --> 00:01:59,369
actually Jupiter lab which kind of gives

00:01:57,660 --> 00:02:02,190
you a little mini IDE in the browser I

00:01:59,369 --> 00:02:03,690
recommend that you google search it look

00:02:02,190 --> 00:02:06,929
it up learn how to use it it's very

00:02:03,690 --> 00:02:08,880
useful for writing interactive code

00:02:06,929 --> 00:02:10,380
something like this I don't have any

00:02:08,880 --> 00:02:12,989
slides I'm basically just gonna be

00:02:10,380 --> 00:02:14,940
stepping through this all my code is

00:02:12,989 --> 00:02:16,680
pre-written and hidden I will be showing

00:02:14,940 --> 00:02:18,810
it as I go along and you'll be able to

00:02:16,680 --> 00:02:20,220
see the output of what I'm writing and

00:02:18,810 --> 00:02:24,180
it'll be there so that we can kind of

00:02:20,220 --> 00:02:26,430
check our results anytime you see a

00:02:24,180 --> 00:02:28,050
little % and then a function there's

00:02:26,430 --> 00:02:30,720
time time it there's a couple others

00:02:28,050 --> 00:02:33,270
this is built into Jupiter and it's

00:02:30,720 --> 00:02:35,760
basically just gonna run a Python line

00:02:33,270 --> 00:02:38,280
of code for us and print out how long it

00:02:35,760 --> 00:02:40,940
takes so in this case it took 226

00:02:38,280 --> 00:02:43,530
microseconds I think it's microseconds

00:02:40,940 --> 00:02:46,020
so I'm gonna be using this a lot be

00:02:43,530 --> 00:02:47,400
prepared to look at this and make sure

00:02:46,020 --> 00:02:48,690
that you're paying attention to the

00:02:47,400 --> 00:02:50,519
units because it will print out

00:02:48,690 --> 00:02:53,519
milliseconds or seconds instead of

00:02:50,519 --> 00:02:54,930
little funny-looking you there's also

00:02:53,519 --> 00:02:56,080
time at which I'm actually not going to

00:02:54,930 --> 00:02:58,150
be using

00:02:56,080 --> 00:02:59,530
because it's gonna run everything seven

00:02:58,150 --> 00:03:01,600
times and I don't want to do that I'm

00:02:59,530 --> 00:03:03,760
just doing individual run so a lot of

00:03:01,600 --> 00:03:05,440
time whenever you look for performance

00:03:03,760 --> 00:03:07,240
testing or whatever and Jupiter notebook

00:03:05,440 --> 00:03:09,340
you'll see time it time just runs at

00:03:07,240 --> 00:03:14,380
once just to kind of clarify why that

00:03:09,340 --> 00:03:16,420
might look a little bit different I will

00:03:14,380 --> 00:03:18,430
jump right in so I have a little data

00:03:16,420 --> 00:03:20,680
set that I downloaded from kaggle it's

00:03:18,430 --> 00:03:22,270
some insurance data basically trying to

00:03:20,680 --> 00:03:24,760
predict how much someone's going to be

00:03:22,270 --> 00:03:27,850
charged for insurance based on their age

00:03:24,760 --> 00:03:30,970
the number of children etc for those of

00:03:27,850 --> 00:03:34,210
you who are maybe not super familiar

00:03:30,970 --> 00:03:35,950
with pandas on the left side you'll see

00:03:34,210 --> 00:03:38,110
this index I don't have an index that's

00:03:35,950 --> 00:03:39,760
just gonna be 0 1 2 3 4 it's basically a

00:03:38,110 --> 00:03:43,000
row number across the top you will see

00:03:39,760 --> 00:03:45,130
the labels of my columns so age is

00:03:43,000 --> 00:03:49,650
obviously a number sex is obviously a

00:03:45,130 --> 00:03:49,650
string BMI is a floating-point number

00:03:50,730 --> 00:03:56,020
now as everyone should be mostly

00:03:53,620 --> 00:03:57,670
familiar Python has different data types

00:03:56,020 --> 00:03:59,140
right it's one of the first things that

00:03:57,670 --> 00:04:00,480
you get introduced to when you are

00:03:59,140 --> 00:04:03,130
learning Python

00:04:00,480 --> 00:04:05,320
well you might know not know is that

00:04:03,130 --> 00:04:07,570
pandas actually uses them slightly

00:04:05,320 --> 00:04:09,340
differently so normally in Python

00:04:07,570 --> 00:04:10,720
there's only one type of integer there's

00:04:09,340 --> 00:04:15,580
an integer there's one type of float

00:04:10,720 --> 00:04:19,480
there's a float however you'll see this

00:04:15,580 --> 00:04:23,080
thing float 64 or in 64 so pandas runs

00:04:19,480 --> 00:04:25,360
on top of numpy which is a library for

00:04:23,080 --> 00:04:26,950
managing numerical data very efficiently

00:04:25,360 --> 00:04:28,750
in Python it's actually doing all of

00:04:26,950 --> 00:04:32,350
this in C and so we're actually getting

00:04:28,750 --> 00:04:34,870
access to the types of C so in this case

00:04:32,350 --> 00:04:37,930
by default if it's a float pandas is

00:04:34,870 --> 00:04:39,340
gonna read it in as a 64-bit float it's

00:04:37,930 --> 00:04:43,000
an integer it's gonna read it in as a

00:04:39,340 --> 00:04:44,200
64-bit integer you'll see this memory

00:04:43,000 --> 00:04:47,680
usage printout that I'm going to be

00:04:44,200 --> 00:04:49,090
using a lot if you've ever used DF info

00:04:47,680 --> 00:04:50,620
it actually has a couple useful keyword

00:04:49,090 --> 00:04:54,780
arguments so we can get a little bit

00:04:50,620 --> 00:04:57,610
more information about this data frame

00:04:54,780 --> 00:04:59,050
so I have to float columns right so

00:04:57,610 --> 00:05:01,000
that's going to be BMI and then the

00:04:59,050 --> 00:05:02,290
charges I have two integer columns and

00:05:01,000 --> 00:05:05,169
then I have three object columns in

00:05:02,290 --> 00:05:07,360
pandas if something is a string it's an

00:05:05,169 --> 00:05:09,400
object so anytime you see object just

00:05:07,360 --> 00:05:11,410
think it's a string and the reads

00:05:09,400 --> 00:05:13,389
for this is that numpy just treats all

00:05:11,410 --> 00:05:15,550
Python objects as essentially the same

00:05:13,389 --> 00:05:21,970
it's not a number it's gonna be a Python

00:05:15,550 --> 00:05:24,550
object so one thing when you are

00:05:21,970 --> 00:05:26,500
handling data and pandas that you will

00:05:24,550 --> 00:05:28,570
probably want to think about is how much

00:05:26,500 --> 00:05:30,340
memory you're gonna use right my laptop

00:05:28,570 --> 00:05:32,650
has a limited amount of memory even if

00:05:30,340 --> 00:05:35,949
I'm running on a cloud server somewhere

00:05:32,650 --> 00:05:37,539
with a lot of RAM it still might be a

00:05:35,949 --> 00:05:40,449
concern depending on how much data

00:05:37,539 --> 00:05:42,280
you're dealing with a lot of the time

00:05:40,449 --> 00:05:44,169
you may be trying to process more data

00:05:42,280 --> 00:05:46,419
than you actually have Ram say if you're

00:05:44,169 --> 00:05:48,460
doing something on this laptop so for

00:05:46,419 --> 00:05:51,820
example maybe I have 10 gigabytes worth

00:05:48,460 --> 00:05:55,419
of data in a file my laptop only has 8

00:05:51,820 --> 00:05:56,740
gigabytes you know it just depends on

00:05:55,419 --> 00:05:58,539
kind of the situation you're in but

00:05:56,740 --> 00:06:00,580
memories is just something that you

00:05:58,539 --> 00:06:03,340
definitely want to be aware of so in

00:06:00,580 --> 00:06:06,070
this case my data frame has almost 300

00:06:03,340 --> 00:06:07,300
kilobytes data most of those are going

00:06:06,070 --> 00:06:11,680
to be string strings or more expensive

00:06:07,300 --> 00:06:12,669
to store than numbers but first things

00:06:11,680 --> 00:06:14,860
first we'll start with kind of the

00:06:12,669 --> 00:06:16,599
smallest stuff first so age if we look

00:06:14,860 --> 00:06:19,599
at age right there's no reason that we

00:06:16,599 --> 00:06:21,849
need 64 whole bits to represent the age

00:06:19,599 --> 00:06:25,270
of someone realistically you're gonna go

00:06:21,849 --> 00:06:27,130
from 0 to maybe 100 something we have no

00:06:25,270 --> 00:06:29,800
negatives right can't be negative age

00:06:27,130 --> 00:06:32,229
and we don't have anything very large so

00:06:29,800 --> 00:06:36,220
we can actually use UN tapes this is an

00:06:32,229 --> 00:06:39,610
8-bit unsigned integer in C to represent

00:06:36,220 --> 00:06:41,949
this so we're gonna use 64 divided by 8

00:06:39,610 --> 00:06:44,650
so we're gonna use an 8 as much memory

00:06:41,949 --> 00:06:46,120
to represent this column and again since

00:06:44,650 --> 00:06:48,400
there's no negative age and since

00:06:46,120 --> 00:06:51,669
there's an upper limit on someone's age

00:06:48,400 --> 00:06:53,490
we don't actually really care about you

00:06:51,669 --> 00:06:57,190
know someone being a million years old

00:06:53,490 --> 00:06:58,960
in pan disease as type function if you

00:06:57,190 --> 00:07:01,659
haven't seen it before to convert from

00:06:58,960 --> 00:07:06,570
one type to another and so we went down

00:07:01,659 --> 00:07:08,949
a little bit we went from 286 to 268

00:07:06,570 --> 00:07:10,120
you'll see that so I did both age and

00:07:08,949 --> 00:07:13,389
children right because you can't have

00:07:10,120 --> 00:07:18,940
negative children and now our data frame

00:07:13,389 --> 00:07:21,010
has to you int 8 typed columns okay they

00:07:18,940 --> 00:07:23,020
didn't really say very much we say you

00:07:21,010 --> 00:07:26,200
know maybe about 10 percent of our

00:07:23,020 --> 00:07:27,160
all usage floats are going to be a

00:07:26,200 --> 00:07:29,980
similar story

00:07:27,160 --> 00:07:32,920
so in numpy and pandas there are two

00:07:29,980 --> 00:07:34,930
float types there's 32 and 64 because

00:07:32,920 --> 00:07:36,550
there's just a minimum number of bits

00:07:34,930 --> 00:07:38,410
required to represent floating point

00:07:36,550 --> 00:07:40,480
numbers and so we get a little bit more

00:07:38,410 --> 00:07:42,250
savings it's actually less than the

00:07:40,480 --> 00:07:45,550
integers because we can only go from 64

00:07:42,250 --> 00:07:50,080
32 instead of 64 8 but we save a little

00:07:45,550 --> 00:07:51,940
bit more now we got to start thinking

00:07:50,080 --> 00:07:55,060
about how we're gonna optimize our

00:07:51,940 --> 00:07:57,850
string columns so you could shorten

00:07:55,060 --> 00:07:59,020
strings for example right depending on

00:07:57,850 --> 00:08:01,660
what kind of data you're dealing with

00:07:59,020 --> 00:08:03,580
maybe you have what's more like

00:08:01,660 --> 00:08:06,100
categorical data so for example right

00:08:03,580 --> 00:08:08,830
region Southwest Northwest there's kind

00:08:06,100 --> 00:08:10,930
of a limited number of values that it's

00:08:08,830 --> 00:08:13,300
gonna have so you could for example keep

00:08:10,930 --> 00:08:15,490
track of a map of strings to integers so

00:08:13,300 --> 00:08:19,360
that you can store a couple strings and

00:08:15,490 --> 00:08:21,070
then convert that whole column to a u +

00:08:19,360 --> 00:08:22,930
8 because again we're not gonna have

00:08:21,070 --> 00:08:24,490
very many categories so we don't need to

00:08:22,930 --> 00:08:27,070
worry about representing negative

00:08:24,490 --> 00:08:28,780
numbers or very large numbers and we're

00:08:27,070 --> 00:08:32,830
gonna save a lot more memory right so

00:08:28,780 --> 00:08:35,860
now we've gone from 257 to 180 so we

00:08:32,830 --> 00:08:39,070
actually saved about a third we no

00:08:35,860 --> 00:08:40,600
longer have this smoker column as a yes

00:08:39,070 --> 00:08:42,700
or no because again we don't really care

00:08:40,600 --> 00:08:44,260
about what the string is if this column

00:08:42,700 --> 00:08:45,670
was something like a name you couldn't

00:08:44,260 --> 00:08:49,150
really do this right because you need to

00:08:45,670 --> 00:08:52,570
preserve that information it's probably

00:08:49,150 --> 00:08:55,180
too unique to really be able to you know

00:08:52,570 --> 00:08:57,010
handle a map effectively it's just not

00:08:55,180 --> 00:08:58,420
something that's gonna be very useful

00:08:57,010 --> 00:09:00,310
for you but anytime you have something

00:08:58,420 --> 00:09:03,640
that's kind of like a category it's

00:09:00,310 --> 00:09:06,880
gonna be useful to do this for and if we

00:09:03,640 --> 00:09:07,990
check so this this is a handy function

00:09:06,880 --> 00:09:09,250
if you didn't know about it I didn't

00:09:07,990 --> 00:09:11,260
actually know about it until a couple

00:09:09,250 --> 00:09:13,690
months ago called select D types it'll

00:09:11,260 --> 00:09:16,150
basically give you all of the columns in

00:09:13,690 --> 00:09:18,250
a data frame that are that type so in

00:09:16,150 --> 00:09:21,160
this case we've converted our smoker

00:09:18,250 --> 00:09:22,660
yes/no to smoker 0 or 1 basically

00:09:21,160 --> 00:09:24,700
indicating short false you could also

00:09:22,660 --> 00:09:26,440
just do true/false as well instead of

00:09:24,700 --> 00:09:30,070
these integers I'm just sticking with

00:09:26,440 --> 00:09:32,020
integers because I like them more so you

00:09:30,070 --> 00:09:34,450
might think okay that's great I can do

00:09:32,020 --> 00:09:36,910
that I can manage this map myself and

00:09:34,450 --> 00:09:38,560
before maybe two

00:09:36,910 --> 00:09:40,540
versions ago in pandas that's how you

00:09:38,560 --> 00:09:43,000
would have to do it but recently they

00:09:40,540 --> 00:09:45,150
introduced something called categoricals

00:09:43,000 --> 00:09:47,860
and I'll actually open up the dock page

00:09:45,150 --> 00:09:49,450
so that if you ever want to go look for

00:09:47,860 --> 00:09:51,580
this you would know where to go look for

00:09:49,450 --> 00:09:53,290
so I'm not actually gonna read any of

00:09:51,580 --> 00:09:56,410
this because I can just explain it in

00:09:53,290 --> 00:09:58,630
fewer words categoricals are a built-in

00:09:56,410 --> 00:10:01,570
way to do exactly what I just said

00:09:58,630 --> 00:10:05,020
pandas will automatically handle mapping

00:10:01,570 --> 00:10:06,970
from these numbers that you're using to

00:10:05,020 --> 00:10:08,920
represent your categories to the strings

00:10:06,970 --> 00:10:11,350
themselves so you no longer have to

00:10:08,920 --> 00:10:14,080
maintain a map of which values

00:10:11,350 --> 00:10:16,300
correspond to which numbers and most

00:10:14,080 --> 00:10:17,740
importantly you're not gonna have to map

00:10:16,300 --> 00:10:21,580
back and forth when you're trying to

00:10:17,740 --> 00:10:24,730
process data so we'll kind of get

00:10:21,580 --> 00:10:25,570
through all of that uniqueness like I

00:10:24,730 --> 00:10:28,210
said is something that you should

00:10:25,570 --> 00:10:29,710
probably take into account when you're

00:10:28,210 --> 00:10:33,070
doing something like this so in this

00:10:29,710 --> 00:10:36,160
case these two columns sex region are

00:10:33,070 --> 00:10:38,470
very good for using this categorical

00:10:36,160 --> 00:10:40,630
type why because they really are

00:10:38,470 --> 00:10:42,460
categories right something like a name

00:10:40,630 --> 00:10:44,070
is gonna be very unique right something

00:10:42,460 --> 00:10:46,450
like an address is gonna be very unique

00:10:44,070 --> 00:10:48,760
probably not that useful to use this for

00:10:46,450 --> 00:10:51,070
but something like a state right in the

00:10:48,760 --> 00:10:53,080
US we have 50 states plus you know DC

00:10:51,070 --> 00:10:57,690
inacol territories so if you have 10

00:10:53,080 --> 00:10:59,680
million rows it's gonna be very very

00:10:57,690 --> 00:11:01,360
duplicate 'iv right you're gonna have a

00:10:59,680 --> 00:11:04,500
lot of the same state that might be

00:11:01,360 --> 00:11:04,500
something that you want to use this for

00:11:05,370 --> 00:11:11,140
you can use as type category to convert

00:11:08,650 --> 00:11:15,520
an object or a string right same thing

00:11:11,140 --> 00:11:16,900
to a category type in pandas the typing

00:11:15,520 --> 00:11:19,450
information will give you a little bit

00:11:16,900 --> 00:11:21,700
of information about what it's actually

00:11:19,450 --> 00:11:24,280
representing so the D type is now

00:11:21,700 --> 00:11:25,630
category instead of object notice that

00:11:24,280 --> 00:11:28,210
they look exactly the same right

00:11:25,630 --> 00:11:30,040
Southwest Northwest southeast they are

00:11:28,210 --> 00:11:33,040
still strings for the purposes of using

00:11:30,040 --> 00:11:35,170
them and it stores the categories that

00:11:33,040 --> 00:11:36,880
are being mapped to so northeast

00:11:35,170 --> 00:11:39,220
northwest southeast southwest right so I

00:11:36,880 --> 00:11:41,290
have just like up here I have four

00:11:39,220 --> 00:11:47,100
unique values for region and I now have

00:11:41,290 --> 00:11:47,100
four strings in my category D type

00:11:47,960 --> 00:11:54,170
let's quickly compare the number of

00:11:50,240 --> 00:11:57,470
bytes from our previous column so this

00:11:54,170 --> 00:12:02,450
is region as a string right

00:11:57,470 --> 00:12:06,890
I didn't reassign it oops and this is as

00:12:02,450 --> 00:12:09,860
a category so I've gone from what ten

00:12:06,890 --> 00:12:13,130
thousand bytes down to about 1300 so I

00:12:09,860 --> 00:12:17,450
saved 80 to 90% of my memory usage with

00:12:13,130 --> 00:12:18,950
one line of code depending on how unique

00:12:17,450 --> 00:12:20,480
your values are you will see a little

00:12:18,950 --> 00:12:24,110
bit more savings or a little bit less

00:12:20,480 --> 00:12:26,390
savings one line eighty percent memory

00:12:24,110 --> 00:12:28,070
savings you just have to make sure that

00:12:26,390 --> 00:12:31,550
you know what your data looks like how

00:12:28,070 --> 00:12:35,540
unique your data is it's gonna sniff ik

00:12:31,550 --> 00:12:37,250
Utley improve your life I can use select

00:12:35,540 --> 00:12:40,100
D types is a very common snippet that

00:12:37,250 --> 00:12:42,020
you probably want to put into your code

00:12:40,100 --> 00:12:43,850
base at some point if you've never tried

00:12:42,020 --> 00:12:45,320
something like this before

00:12:43,850 --> 00:12:48,800
you'll basically just be able to quickly

00:12:45,320 --> 00:12:50,270
iterate all of the types oh sorry all of

00:12:48,800 --> 00:12:52,100
the D types for your columns and

00:12:50,270 --> 00:12:55,640
anything that is an object right so it's

00:12:52,100 --> 00:12:59,900
a string make it a category so that's

00:12:55,640 --> 00:13:03,170
what I'm gonna do I'm printing out the

00:12:59,900 --> 00:13:06,350
memory usage first and then the D types

00:13:03,170 --> 00:13:08,900
for all of this so now rather than two

00:13:06,350 --> 00:13:11,779
object columns I now have two category

00:13:08,900 --> 00:13:14,660
columns I've gone down to 17 kilobytes

00:13:11,779 --> 00:13:20,270
right so we went down from 300 down to

00:13:14,660 --> 00:13:23,330
17 numbers they save a little bit you

00:13:20,270 --> 00:13:25,870
know maybe 10% depending on what you're

00:13:23,330 --> 00:13:28,580
doing most important thing though

00:13:25,870 --> 00:13:31,400
strings are expensive you've gone from

00:13:28,580 --> 00:13:33,560
you know representing strings as eight

00:13:31,400 --> 00:13:35,660
or nine numbers right one per character

00:13:33,560 --> 00:13:42,650
basically all the way down to just one

00:13:35,660 --> 00:13:47,209
number I'll convert back so if you

00:13:42,650 --> 00:13:49,610
remember smoker right my true/false

00:13:47,209 --> 00:13:51,050
column basically was one of these and

00:13:49,610 --> 00:13:53,089
I'm just going to go ahead and convert

00:13:51,050 --> 00:13:54,440
it back to the strings because say for

00:13:53,089 --> 00:13:55,820
some reason you want to keep it as a

00:13:54,440 --> 00:13:58,400
string just to make your life a little

00:13:55,820 --> 00:14:01,050
bit easier when you're programming maybe

00:13:58,400 --> 00:14:02,459
this is you know the answer to

00:14:01,050 --> 00:14:04,310
and you don't want to you want to keep

00:14:02,459 --> 00:14:07,350
what people have typed originally

00:14:04,310 --> 00:14:09,630
instead of you know necessarily

00:14:07,350 --> 00:14:11,430
converting it to just a real true/false

00:14:09,630 --> 00:14:12,839
we now three categories and we've

00:14:11,430 --> 00:14:15,000
actually gone up a little bit in memory

00:14:12,839 --> 00:14:17,190
usage so how the categorical works is

00:14:15,000 --> 00:14:20,040
like I said it's basically maintaining

00:14:17,190 --> 00:14:21,240
this map of you into eight numbers to

00:14:20,040 --> 00:14:23,550
the strings that those numbers represent

00:14:21,240 --> 00:14:24,690
for you but it's still gonna work as a

00:14:23,550 --> 00:14:27,630
string whenever you're doing any

00:14:24,690 --> 00:14:30,120
processing so instead of only

00:14:27,630 --> 00:14:32,670
representing smoker as a number up here

00:14:30,120 --> 00:14:36,029
it's now representing it as a number

00:14:32,670 --> 00:14:38,579
with two extra strings yes no we went up

00:14:36,029 --> 00:14:41,579
just a tad little bit in memory usage

00:14:38,579 --> 00:14:45,350
it's pretty much insignificant when

00:14:41,579 --> 00:14:45,350
there's only two categories like this

00:14:46,700 --> 00:14:53,279
we'll check it out looks exactly the

00:14:50,880 --> 00:14:56,250
same as it did five or ten minutes ago

00:14:53,279 --> 00:14:59,010
right we have strings we have ents we

00:14:56,250 --> 00:15:02,670
have floats except it's now consuming a

00:14:59,010 --> 00:15:04,200
you know what eight percent seven

00:15:02,670 --> 00:15:05,910
percent of the memory we've gone down

00:15:04,200 --> 00:15:08,370
from almost three hundred to less than

00:15:05,910 --> 00:15:09,360
twenty really big savings this is

00:15:08,370 --> 00:15:11,370
something that you should keep in mind

00:15:09,360 --> 00:15:17,220
whenever you're dealing with data like

00:15:11,370 --> 00:15:19,380
this the extra kind of handy feature is

00:15:17,220 --> 00:15:21,570
that you can directly compare the

00:15:19,380 --> 00:15:23,610
results of the categorical D type to

00:15:21,570 --> 00:15:26,790
Python strings right so maybe you wanted

00:15:23,610 --> 00:15:28,470
to look up everywhere where smoker is

00:15:26,790 --> 00:15:30,839
equal to yes right

00:15:28,470 --> 00:15:32,970
that will work because pandas is gonna

00:15:30,839 --> 00:15:34,950
handle that conversion from the

00:15:32,970 --> 00:15:36,630
numerical representation to the string

00:15:34,950 --> 00:15:38,010
that it's actually mapping to you know

00:15:36,630 --> 00:15:39,930
if you were gonna do this yourself

00:15:38,010 --> 00:15:41,579
without the categorical type like you

00:15:39,930 --> 00:15:43,230
would have had to do before they

00:15:41,579 --> 00:15:45,060
introduced this feature you would have

00:15:43,230 --> 00:15:47,190
to map everything right you would have

00:15:45,060 --> 00:15:50,060
to go and maybe look up what number yes

00:15:47,190 --> 00:15:52,170
corresponds to and then do that equality

00:15:50,060 --> 00:15:55,620
this is just gonna be a lot more

00:15:52,170 --> 00:15:58,050
intuitive and really more pythonic right

00:15:55,620 --> 00:16:00,209
so what we don't want to have happen is

00:15:58,050 --> 00:16:01,230
have to write a bunch of like maps back

00:16:00,209 --> 00:16:03,510
and forth and make the code

00:16:01,230 --> 00:16:05,820
unnecessarily complicated Python is

00:16:03,510 --> 00:16:08,130
really really accepting of syntactic

00:16:05,820 --> 00:16:10,680
sugar and that's really all categoricals

00:16:08,130 --> 00:16:11,940
are right it's it's a way to make

00:16:10,680 --> 00:16:14,400
something that you could do yourself

00:16:11,940 --> 00:16:19,500
just a little more elegant a little more

00:16:14,400 --> 00:16:22,440
to use we'll check this out real quick

00:16:19,500 --> 00:16:26,760
just to kind of compare what I was

00:16:22,440 --> 00:16:31,110
saying so right the smoker column by

00:16:26,760 --> 00:16:33,740
itself is a little bit larger than the

00:16:31,110 --> 00:16:36,000
smoker column that we manually mapped

00:16:33,740 --> 00:16:38,400
but again you're still gonna have to

00:16:36,000 --> 00:16:40,110
keep track of that map yourself and I

00:16:38,400 --> 00:16:42,030
guess technically it would consume a

00:16:40,110 --> 00:16:44,150
little bit of memory anyways right if

00:16:42,030 --> 00:16:49,350
you wanted to keep those original values

00:16:44,150 --> 00:16:54,360
one caveat so here just to recap we have

00:16:49,350 --> 00:16:57,720
four values in our D type what happens

00:16:54,360 --> 00:17:00,750
if we try to assign the new one we get a

00:16:57,720 --> 00:17:03,900
value error so you can't just randomly

00:17:00,750 --> 00:17:09,720
add or assign new categories okay

00:17:03,900 --> 00:17:11,520
because you know we we would rather tell

00:17:09,720 --> 00:17:15,060
you that you're you're changing this

00:17:11,520 --> 00:17:17,130
category rather than just you know have

00:17:15,060 --> 00:17:20,580
pandas handle it for us so you do have

00:17:17,130 --> 00:17:22,350
to explicitly call an add categories

00:17:20,580 --> 00:17:26,520
call saying hey I'm gonna start using

00:17:22,350 --> 00:17:27,990
this value in my category D type and

00:17:26,520 --> 00:17:30,930
then you can assign it it's gonna work

00:17:27,990 --> 00:17:34,890
just fine you could you know you could

00:17:30,930 --> 00:17:36,270
overload this to maybe do like a try

00:17:34,890 --> 00:17:37,680
except right where you kept the value

00:17:36,270 --> 00:17:39,300
error and just quickly add the category

00:17:37,680 --> 00:17:40,790
and then you wouldn't even have to think

00:17:39,300 --> 00:17:44,340
about it

00:17:40,790 --> 00:17:47,280
on the other hand if you ever do apply

00:17:44,340 --> 00:17:49,230
functions for example pandas will handle

00:17:47,280 --> 00:17:50,730
that category change for you it already

00:17:49,230 --> 00:17:52,590
does this so like for example if you

00:17:50,730 --> 00:17:55,260
were to multiply like an INT and a float

00:17:52,590 --> 00:17:56,310
column you'll get a float out so you

00:17:55,260 --> 00:17:57,840
don't have to worry about it if you're

00:17:56,310 --> 00:18:00,990
using applies only if you're manually

00:17:57,840 --> 00:18:02,730
signing stuff so the most important

00:18:00,990 --> 00:18:04,740
thing about this particular line though

00:18:02,730 --> 00:18:05,100
is we're using just a string function

00:18:04,740 --> 00:18:08,760
right

00:18:05,100 --> 00:18:10,800
I'm calling upper on the string and it's

00:18:08,760 --> 00:18:13,380
just gonna work for me I get to treat it

00:18:10,800 --> 00:18:15,380
as a perfectly good Python string I

00:18:13,380 --> 00:18:17,760
don't have to worry about anything else

00:18:15,380 --> 00:18:20,100
it's really like a good drop-in

00:18:17,760 --> 00:18:21,720
replacement where at some point in your

00:18:20,100 --> 00:18:24,600
code you can try to convert some of your

00:18:21,720 --> 00:18:26,280
strings into categories you don't have

00:18:24,600 --> 00:18:27,630
to touch anywhere else right if you were

00:18:26,280 --> 00:18:28,269
to try to do this manually by

00:18:27,630 --> 00:18:29,559
maintaining that

00:18:28,269 --> 00:18:31,479
map you're gonna have to add all of

00:18:29,559 --> 00:18:36,909
these mapping calls everywhere but

00:18:31,479 --> 00:18:39,489
pandas does it for you it's great most

00:18:36,909 --> 00:18:41,889
important thing probably for a lot of

00:18:39,489 --> 00:18:43,450
people would be changing when you're

00:18:41,889 --> 00:18:45,849
actually reading your data in right so

00:18:43,450 --> 00:18:48,489
depending on whether or not you know

00:18:45,849 --> 00:18:49,929
what your columns are already you can

00:18:48,489 --> 00:18:52,179
just read them in like this so you can

00:18:49,929 --> 00:18:53,739
pass a map to pandas read CSV read

00:18:52,179 --> 00:18:56,200
sequel whatever and it'll actually

00:18:53,739 --> 00:18:58,029
handle this typing for you you could

00:18:56,200 --> 00:18:59,320
also write a function that would for

00:18:58,029 --> 00:19:00,969
example if you're not sure how unique

00:18:59,320 --> 00:19:04,869
your data is you could pick a threshold

00:19:00,969 --> 00:19:06,789
you know maybe no more than 10% unique

00:19:04,869 --> 00:19:10,089
values right so if you have 1 million

00:19:06,789 --> 00:19:11,529
rows maybe you want to use a categorical

00:19:10,089 --> 00:19:13,749
if you have up to 10,000 different

00:19:11,529 --> 00:19:15,639
categories you could go back to what I

00:19:13,749 --> 00:19:17,919
was showing earlier with DF info and

00:19:15,639 --> 00:19:20,799
then or I think was described I can't

00:19:17,919 --> 00:19:22,959
remember iterate over those columns

00:19:20,799 --> 00:19:23,879
assign them to categories and then

00:19:22,959 --> 00:19:26,049
you're good to go the rest of the time

00:19:23,879 --> 00:19:27,669
the advantage of this though is that

00:19:26,049 --> 00:19:29,200
it's actually going to be more memory

00:19:27,669 --> 00:19:33,399
efficient when you're reading it in as

00:19:29,200 --> 00:19:34,719
well depending on you know like I said

00:19:33,399 --> 00:19:36,579
earlier what you're doing you could

00:19:34,719 --> 00:19:38,709
actually process a text file

00:19:36,579 --> 00:19:42,700
significantly larger than the amount of

00:19:38,709 --> 00:19:46,839
RAM in your on your computer if you read

00:19:42,700 --> 00:19:48,429
in the d-types properly right it's you

00:19:46,839 --> 00:19:50,259
know very nice to be able to just do

00:19:48,429 --> 00:19:51,249
this when you first read it in you don't

00:19:50,259 --> 00:19:52,779
have to change any of the rest of your

00:19:51,249 --> 00:19:56,589
code and you're gonna be a lot more

00:19:52,779 --> 00:19:58,719
memory efficient of course for every

00:19:56,589 --> 00:20:03,459
solution there are situations in which

00:19:58,719 --> 00:20:07,839
the solution does not apply actually I'm

00:20:03,459 --> 00:20:10,389
gonna make this a little bit bigger no

00:20:07,839 --> 00:20:11,679
mm so I basically just wrote a real

00:20:10,389 --> 00:20:14,679
quick function that is gonna give us

00:20:11,679 --> 00:20:16,869
pretend some randomly generated IDs that

00:20:14,679 --> 00:20:24,190
are composed of capital letters and some

00:20:16,869 --> 00:20:27,909
numbers so what happen if I tried to use

00:20:24,190 --> 00:20:32,019
a category type on this essentially

00:20:27,909 --> 00:20:34,839
random column well I actually go up in

00:20:32,019 --> 00:20:38,049
memory usage right because now rather

00:20:34,839 --> 00:20:41,440
than just having one string per row I

00:20:38,049 --> 00:20:45,100
now have an integer and a string

00:20:41,440 --> 00:20:47,110
rrrow I'm not really able to make use of

00:20:45,100 --> 00:20:49,360
the memory savings because I still have

00:20:47,110 --> 00:20:52,450
to keep around one string per unique

00:20:49,360 --> 00:20:55,740
string in my data frame this is

00:20:52,450 --> 00:20:58,269
essentially random it's going to go up

00:20:55,740 --> 00:21:00,940
depending on you know like I said

00:20:58,269 --> 00:21:03,090
earlier you it just takes a little bit

00:21:00,940 --> 00:21:05,950
of trial and error to really figure out

00:21:03,090 --> 00:21:07,779
where and your particular data sets that

00:21:05,950 --> 00:21:09,429
you may encounter where you might want

00:21:07,779 --> 00:21:10,899
to use this depending on how unique your

00:21:09,429 --> 00:21:14,740
values are some things that you might

00:21:10,899 --> 00:21:18,059
not actually expect would be useful for

00:21:14,740 --> 00:21:20,679
this so like for example maybe you have

00:21:18,059 --> 00:21:24,850
IDs that represent objects from

00:21:20,679 --> 00:21:28,179
different locations right and those IDs

00:21:24,850 --> 00:21:30,220
are not necessarily unique globally but

00:21:28,179 --> 00:21:32,409
they are unique per data source that

00:21:30,220 --> 00:21:35,019
you're encountering them in and then

00:21:32,409 --> 00:21:36,730
maybe later on you treat it with an

00:21:35,019 --> 00:21:40,330
identifier for the data source as an

00:21:36,730 --> 00:21:43,919
overall unique ID you could probably use

00:21:40,330 --> 00:21:47,740
categoricals on that type of thing and

00:21:43,919 --> 00:21:49,840
then just as a final note sometimes

00:21:47,740 --> 00:21:52,750
people might think oh I want to use

00:21:49,840 --> 00:21:54,519
categoricals on my dates because there's

00:21:52,750 --> 00:21:56,440
some you know they tend to duplicate

00:21:54,519 --> 00:21:57,789
there's not all that many unique values

00:21:56,440 --> 00:22:00,519
if you're especially if you don't have

00:21:57,789 --> 00:22:04,990
times but really you should just use the

00:22:00,519 --> 00:22:07,450
built-in date/time type because this is

00:22:04,990 --> 00:22:08,500
an example of a Python object you know

00:22:07,450 --> 00:22:10,059
normally you think like a date/time

00:22:08,500 --> 00:22:15,490
object that actually has a corresponding

00:22:10,059 --> 00:22:17,919
type in numpy alright so away from

00:22:15,490 --> 00:22:19,299
memory and on to CPU execution time this

00:22:17,919 --> 00:22:21,730
is actually targeted a little bit more

00:22:19,299 --> 00:22:23,440
at people who are maybe not necessarily

00:22:21,730 --> 00:22:27,220
all that familiar with pandas and is

00:22:23,440 --> 00:22:29,320
more of a recap of what not to do so I

00:22:27,220 --> 00:22:31,929
have some credit card information here I

00:22:29,320 --> 00:22:33,399
only pulled in a thousand rows there's a

00:22:31,929 --> 00:22:36,879
little bit of memory right so I have a

00:22:33,399 --> 00:22:37,779
bunch of floats a couple ents really you

00:22:36,879 --> 00:22:41,590
don't even really have to worry about

00:22:37,779 --> 00:22:48,149
what this is doing if you wanted to very

00:22:41,590 --> 00:22:50,529
quickly look at it it's basically just

00:22:48,149 --> 00:22:52,750
anonymous numerical data we don't really

00:22:50,529 --> 00:22:55,330
care about it but what I'm gonna be

00:22:52,750 --> 00:22:58,870
doing is multiplying some value

00:22:55,330 --> 00:23:01,660
by one hundred one thing that's in the

00:22:58,870 --> 00:23:03,630
zen of python is there should preferably

00:23:01,660 --> 00:23:06,760
only be one way to do things

00:23:03,630 --> 00:23:10,600
pandas breaks that rule and I don't like

00:23:06,760 --> 00:23:13,660
that it does but here are some ways that

00:23:10,600 --> 00:23:15,630
we can multiply a column by 100 the

00:23:13,660 --> 00:23:22,510
absolute worst thing that you can do is

00:23:15,630 --> 00:23:25,330
loop over the index and look it up so

00:23:22,510 --> 00:23:27,040
that took 270 milliseconds which again

00:23:25,330 --> 00:23:28,590
yeah don't take that long what if you're

00:23:27,040 --> 00:23:31,960
doing this on a million records right

00:23:28,590 --> 00:23:35,650
it's gonna get slow very quickly so I'm

00:23:31,960 --> 00:23:37,330
manually looping over this index in this

00:23:35,650 --> 00:23:39,010
case my index is just the number so I

00:23:37,330 --> 00:23:42,370
can kind of get away with logan's that I

00:23:39,010 --> 00:23:43,390
look but it's slow one thing that's

00:23:42,370 --> 00:23:46,060
actually faster that a lot of people

00:23:43,390 --> 00:23:48,070
don't realize is you can actually just

00:23:46,060 --> 00:23:50,250
collect them in a list and then assign

00:23:48,070 --> 00:23:53,140
the list and it basically does less

00:23:50,250 --> 00:23:55,000
shuffling of memory and so it actually

00:23:53,140 --> 00:23:57,160
is gonna be faster even though you're

00:23:55,000 --> 00:23:59,500
still iterating over it manually and

00:23:57,160 --> 00:24:03,210
collecting the results and then saving

00:23:59,500 --> 00:24:05,530
them if you do ever need to loop over

00:24:03,210 --> 00:24:07,600
individual rows of a data frame for some

00:24:05,530 --> 00:24:09,670
reason you should use it arose it's

00:24:07,600 --> 00:24:14,230
faster right so I'm still doing the

00:24:09,670 --> 00:24:16,600
exact same append and then assign the

00:24:14,230 --> 00:24:19,900
column here except I'm using it arose

00:24:16,600 --> 00:24:24,640
instead of manually looking things up so

00:24:19,900 --> 00:24:27,910
it's about four times faster using a ply

00:24:24,640 --> 00:24:29,770
very common right apply is probably the

00:24:27,910 --> 00:24:31,360
most cost function and pandas anytime

00:24:29,770 --> 00:24:33,040
you need to do like specific

00:24:31,360 --> 00:24:35,080
manipulations on rows or columns you're

00:24:33,040 --> 00:24:37,120
gonna be doing this it's even faster so

00:24:35,080 --> 00:24:40,270
this will handle that looping for us and

00:24:37,120 --> 00:24:44,320
then we just assign it over we're down

00:24:40,270 --> 00:24:46,390
to about 20 milliseconds this one is

00:24:44,320 --> 00:24:50,170
very slightly different in that in the

00:24:46,390 --> 00:24:52,150
first one I was having to go row by row

00:24:50,170 --> 00:24:55,270
and in the second one I'm just going by

00:24:52,150 --> 00:24:57,160
the column values 20 times faster so

00:24:55,270 --> 00:25:00,520
make sure you under actually understand

00:24:57,160 --> 00:25:03,670
what data you need in your apply right

00:25:00,520 --> 00:25:04,780
so up here since access is equal to one

00:25:03,670 --> 00:25:06,550
for those of you who aren't familiar

00:25:04,780 --> 00:25:08,440
with pandas this is going to be

00:25:06,550 --> 00:25:10,690
iterating row by row and

00:25:08,440 --> 00:25:13,390
picking out the amount from the row

00:25:10,690 --> 00:25:14,920
multiplying it by a hundred and then it

00:25:13,390 --> 00:25:17,140
basically collects all those values and

00:25:14,920 --> 00:25:19,300
assigns them back over here since I did

00:25:17,140 --> 00:25:22,360
not pass in axis its operating just on

00:25:19,300 --> 00:25:26,380
that column right so DF amount versus DF

00:25:22,360 --> 00:25:28,420
not apply with the axis so again all of

00:25:26,380 --> 00:25:30,460
these things do the exact same thing I'm

00:25:28,420 --> 00:25:32,290
printing out the sum of the result each

00:25:30,460 --> 00:25:34,420
time I haven't actually mentioned that

00:25:32,290 --> 00:25:37,090
but just to prove that it's the same

00:25:34,420 --> 00:25:43,240
exact result there's about 15 different

00:25:37,090 --> 00:25:46,870
ways to do it the one of the more I

00:25:43,240 --> 00:25:48,870
don't know obvious things once you kind

00:25:46,870 --> 00:25:50,770
of see it for the first time is that

00:25:48,870 --> 00:25:53,350
pandas will do what's called

00:25:50,770 --> 00:25:56,800
broadcasting of your operations I guess

00:25:53,350 --> 00:25:58,360
technically called vectorizing but I'm

00:25:56,800 --> 00:26:02,830
just saying hey you know what multiply

00:25:58,360 --> 00:26:06,100
this I think this is a integer column

00:26:02,830 --> 00:26:08,650
but I can't remember multiply by 100

00:26:06,100 --> 00:26:11,950
write the whole column just multiply by

00:26:08,650 --> 00:26:13,810
100 it gives you back a column or every

00:26:11,950 --> 00:26:15,310
value it's been multiplied by 100 and

00:26:13,810 --> 00:26:16,840
then I can sum it up the exact same

00:26:15,310 --> 00:26:18,550
result so anytime you're only doing

00:26:16,840 --> 00:26:22,750
numerical operations this is something

00:26:18,550 --> 00:26:25,180
that you can do and then as I said

00:26:22,750 --> 00:26:26,410
earlier pandas runs on top of numpy one

00:26:25,180 --> 00:26:28,570
thing that a lot of people don't know is

00:26:26,410 --> 00:26:31,360
that you can use dot values to access

00:26:28,570 --> 00:26:33,040
the underlying numpy array and we can

00:26:31,360 --> 00:26:35,200
still do this vectorized operation it's

00:26:33,040 --> 00:26:37,390
like a little bit faster it's really

00:26:35,200 --> 00:26:39,160
about the same depending what you're

00:26:37,390 --> 00:26:40,870
doing you may need access to the numpy

00:26:39,160 --> 00:26:44,380
array for like specific functions and

00:26:40,870 --> 00:26:47,140
stuff but really like if you don't have

00:26:44,380 --> 00:26:49,450
to put dot values yeah just avoid it

00:26:47,140 --> 00:26:51,370
you're kind of just adding a few extra

00:26:49,450 --> 00:26:53,890
characters typing and it's really not

00:26:51,370 --> 00:26:55,510
gonna be that much for you ok but maybe

00:26:53,890 --> 00:26:57,730
all of these strategies still are not

00:26:55,510 --> 00:27:00,400
doing what you need and you need to do

00:26:57,730 --> 00:27:04,450
some parallelization so python is not

00:27:00,400 --> 00:27:06,250
necessarily thought of as a trivially

00:27:04,450 --> 00:27:08,470
parallelizable language because of the

00:27:06,250 --> 00:27:10,900
global interpreter lock but it's

00:27:08,470 --> 00:27:13,270
actually not that hard a lot of the new

00:27:10,900 --> 00:27:16,540
api is and more recent versions make it

00:27:13,270 --> 00:27:17,860
very easy so i'm just going to kind of

00:27:16,540 --> 00:27:21,430
compare a couple things so i'm

00:27:17,860 --> 00:27:22,930
generating a million values and

00:27:21,430 --> 00:27:25,300
I'm just you know checking to see which

00:27:22,930 --> 00:27:26,290
ones can contain the letter C but maybe

00:27:25,300 --> 00:27:28,420
I want to do something a little bit more

00:27:26,290 --> 00:27:31,420
complex so in this case I'm using a

00:27:28,420 --> 00:27:33,490
really ugly negative look behind and

00:27:31,420 --> 00:27:36,330
negative look ahead to make sure that it

00:27:33,490 --> 00:27:44,620
is C surrounded by things that aren't C

00:27:36,330 --> 00:27:48,990
it's pretty slow so it took about five

00:27:44,620 --> 00:27:51,490
seconds right on a million rows which

00:27:48,990 --> 00:27:53,580
yeah it's not that slow but we can get a

00:27:51,490 --> 00:27:55,810
little bit faster

00:27:53,580 --> 00:27:56,920
multi-processing is going to be the

00:27:55,810 --> 00:27:59,800
library that you're generally want to

00:27:56,920 --> 00:28:01,780
use for a lot of your kind of standard

00:27:59,800 --> 00:28:04,030
parallelization there's a bunch of

00:28:01,780 --> 00:28:05,950
different things that you can do I know

00:28:04,030 --> 00:28:07,780
that some other talks one in person is

00:28:05,950 --> 00:28:09,100
gonna cover tasks and one person is

00:28:07,780 --> 00:28:11,440
gonna cover Kudo which are very

00:28:09,100 --> 00:28:13,960
different ways and are used in very

00:28:11,440 --> 00:28:16,000
different situations but this is

00:28:13,960 --> 00:28:18,010
something that is pretty much just

00:28:16,000 --> 00:28:19,390
constrained to the standard library

00:28:18,010 --> 00:28:22,030
right multi-processing privacy native

00:28:19,390 --> 00:28:24,820
library we get ourselves a pool I'm just

00:28:22,030 --> 00:28:29,710
using four processes because I have four

00:28:24,820 --> 00:28:31,450
cores on this laptop and then numpy has

00:28:29,710 --> 00:28:33,520
an array split method so I'm just

00:28:31,450 --> 00:28:38,110
splitting up my data frame into four

00:28:33,520 --> 00:28:40,240
pieces and then I'm summing the summed

00:28:38,110 --> 00:28:44,020
results right so this contains function

00:28:40,240 --> 00:28:45,670
is gonna return the sum of my reg ex you

00:28:44,020 --> 00:28:46,600
know true/false so zeros and ones I'm

00:28:45,670 --> 00:28:48,730
basically just counting up the number of

00:28:46,600 --> 00:28:50,230
rows where it's true for and then I'm

00:28:48,730 --> 00:28:53,680
gonna sum up everything that the pool

00:28:50,230 --> 00:28:56,040
returns and we should be a little bit

00:28:53,680 --> 00:28:58,690
faster so it took about half the time

00:28:56,040 --> 00:28:59,890
notice it's not four times the speed

00:28:58,690 --> 00:29:01,840
even though we're using four cores

00:28:59,890 --> 00:29:04,180
instead of one that's because when

00:29:01,840 --> 00:29:06,490
you're using multi processing Python has

00:29:04,180 --> 00:29:08,440
to copy the memory that it's accessing

00:29:06,490 --> 00:29:10,090
to the different processes that it's

00:29:08,440 --> 00:29:13,480
running on because they can't operate on

00:29:10,090 --> 00:29:15,880
the same memory that's why the global

00:29:13,480 --> 00:29:17,650
interpreter lock is sometimes thought of

00:29:15,880 --> 00:29:21,370
as making it more difficult to write

00:29:17,650 --> 00:29:23,830
parallel parallel programs multi

00:29:21,370 --> 00:29:26,400
processing has a sub module called dummy

00:29:23,830 --> 00:29:30,040
it's basically the same it's just a

00:29:26,400 --> 00:29:33,850
pointer to the multi-threading library I

00:29:30,040 --> 00:29:35,030
prefer doing it cuz it's funny we're

00:29:33,850 --> 00:29:37,880
gonna try thread pool

00:29:35,030 --> 00:29:38,990
and we're gonna be sitting here for a

00:29:37,880 --> 00:29:41,450
while

00:29:38,990 --> 00:29:44,450
any time you're writing something that

00:29:41,450 --> 00:29:46,730
is gonna need to be operating on like

00:29:44,450 --> 00:29:48,440
the same memory you should use processes

00:29:46,730 --> 00:29:50,210
instead of threads because the global

00:29:48,440 --> 00:29:52,460
interpreter lock prevents threads from

00:29:50,210 --> 00:29:54,680
accessing the same objects at the same

00:29:52,460 --> 00:29:57,230
time it took 15 seconds I tried to

00:29:54,680 --> 00:29:59,360
paralyze this on four cores it actually

00:29:57,230 --> 00:30:00,830
made it a lot slower so you just need to

00:29:59,360 --> 00:30:02,120
keep in mind like what kinds of things

00:30:00,830 --> 00:30:04,220
are you doing

00:30:02,120 --> 00:30:07,160
where are threads versus process is

00:30:04,220 --> 00:30:08,330
gonna be more applicable and just kind

00:30:07,160 --> 00:30:10,400
of make sure you know you do a little

00:30:08,330 --> 00:30:12,950
bit of testing to make sure that you're

00:30:10,400 --> 00:30:14,960
not actually slowing anything down you

00:30:12,950 --> 00:30:17,510
might also see concurrent futures

00:30:14,960 --> 00:30:19,730
referenced in certain places it's a

00:30:17,510 --> 00:30:22,670
newer library that essentially works

00:30:19,730 --> 00:30:24,650
exactly the same as the multi processing

00:30:22,670 --> 00:30:27,650
library there are certain differences in

00:30:24,650 --> 00:30:29,480
how the API works but if you look two

00:30:27,650 --> 00:30:33,650
point six two point four four it's

00:30:29,480 --> 00:30:35,240
really about the same in fact I'm pretty

00:30:33,650 --> 00:30:37,730
sure that they essentially operate the

00:30:35,240 --> 00:30:39,380
same in terms of how the pools work the

00:30:37,730 --> 00:30:41,950
api's are just a little bit different

00:30:39,380 --> 00:30:45,980
depending on what exactly you want to do

00:30:41,950 --> 00:30:48,080
so given all of that you know Python has

00:30:45,980 --> 00:30:51,670
a really really great library of

00:30:48,080 --> 00:30:56,720
packages this one is called panda ll

00:30:51,670 --> 00:30:59,570
pandas parallel and basically it is two

00:30:56,720 --> 00:31:01,130
lions and then one code change so you're

00:30:59,570 --> 00:31:03,110
gonna import Pandaria Lyle you're gonna

00:31:01,130 --> 00:31:04,910
initialize it and then anywhere you see

00:31:03,110 --> 00:31:07,070
apply you're gonna change to parallel

00:31:04,910 --> 00:31:10,130
apply and we'll get some nice little

00:31:07,070 --> 00:31:11,900
print out information we're using for

00:31:10,130 --> 00:31:15,500
workers because it identified the number

00:31:11,900 --> 00:31:17,750
of cores that I using and we get about

00:31:15,500 --> 00:31:22,400
the same amount of time right 2.7

00:31:17,750 --> 00:31:23,600
seconds so definitely like use this it's

00:31:22,400 --> 00:31:26,810
kind of well as talking about earlier in

00:31:23,600 --> 00:31:28,010
terms of if you write read in your DS

00:31:26,810 --> 00:31:29,510
types at the beginning you don't have to

00:31:28,010 --> 00:31:32,270
change any other code if you use

00:31:29,510 --> 00:31:34,370
pandeiro l exact same thing you just

00:31:32,270 --> 00:31:34,820
change apply to parallel apply I am out

00:31:34,370 --> 00:31:37,400
of time

00:31:34,820 --> 00:31:39,620
but I promise I will be around for

00:31:37,400 --> 00:31:41,530
questions and answers if anybody has any

00:31:39,620 --> 00:31:48,250
thank you

00:31:41,530 --> 00:32:01,559
[Applause]

00:31:48,250 --> 00:32:01,559

YouTube URL: https://www.youtube.com/watch?v=DxrQ17Ijjf4


