Title: Jordan Hagan: Optimizing SQL + Python Pipelines for Data Science
Publication date: 2019-10-22
Playlist: PyColorado 2019
Description: 
	Poorly written SQL and Python can make data extraction and manipulation tedious and painful. Streamlined processes utilizing SQL best practices will save hours of frustration. My goal is to teach attendees proven SQL methodologies and what python tools to use when.
Captions: 
	00:00:00,370 --> 00:00:46,230
[Music]

00:00:42,980 --> 00:00:49,100
very cool so yeah I'm here to talk about

00:00:46,230 --> 00:00:51,480
some sequel optimization and Python

00:00:49,100 --> 00:00:52,949
optimization that exists within data

00:00:51,480 --> 00:00:54,329
pipelines specifically to data science

00:00:52,949 --> 00:00:56,879
but I think that it's pretty applicable

00:00:54,329 --> 00:00:59,640
across software engineering and data

00:00:56,879 --> 00:01:00,089
science as you said my name is Jordan

00:00:59,640 --> 00:01:01,830
Hagen

00:01:00,089 --> 00:01:04,290
I'm a senior data scientist with Minard

00:01:01,830 --> 00:01:06,360
Kosh we're a full-time consultancy so I

00:01:04,290 --> 00:01:08,640
work with a bunch of other data

00:01:06,360 --> 00:01:11,520
scientists with an array of backgrounds

00:01:08,640 --> 00:01:13,950
from PhDs to people like myself that

00:01:11,520 --> 00:01:16,950
just have a bachelor's but have a near

00:01:13,950 --> 00:01:19,470
decade of data experience I started out

00:01:16,950 --> 00:01:21,560
my career in 2010 as a data analyst

00:01:19,470 --> 00:01:24,330
working with a government subcontractor

00:01:21,560 --> 00:01:26,190
subcontractor to predict Medicare Part D

00:01:24,330 --> 00:01:28,200
fraud and I've loved it ever since I've

00:01:26,190 --> 00:01:30,810
been in various roles different DBA

00:01:28,200 --> 00:01:32,640
roles analyst roles data engineering

00:01:30,810 --> 00:01:33,810
type roles and I've seen a lot of it and

00:01:32,640 --> 00:01:35,940
that's what kind of brought me to bring

00:01:33,810 --> 00:01:38,970
this talk here today additionally

00:01:35,940 --> 00:01:40,259
there's my cute little daughter she's 7

00:01:38,970 --> 00:01:42,330
months old and we have some cats at home

00:01:40,259 --> 00:01:43,130
and and I really enjoy them too so

00:01:42,330 --> 00:01:45,750
that's good

00:01:43,130 --> 00:01:48,000
so why I'm giving this talk when I got

00:01:45,750 --> 00:01:51,119
into data science about three years ago

00:01:48,000 --> 00:01:52,920
now I realized that things like Kegel

00:01:51,119 --> 00:01:54,360
competitions and data science tutorials

00:01:52,920 --> 00:01:56,220
out there provide you with like a really

00:01:54,360 --> 00:01:57,929
clean CSV that you just go ahead and you

00:01:56,220 --> 00:02:02,700
nicely load into your pandas dataframe

00:01:57,929 --> 00:02:05,190
you're like cool did it data it's

00:02:02,700 --> 00:02:06,599
tedious because like as when I got into

00:02:05,190 --> 00:02:08,399
it I was like that's not how data is I

00:02:06,599 --> 00:02:10,440
know because I've been doing this for a

00:02:08,399 --> 00:02:13,410
long time prior to did you do science

00:02:10,440 --> 00:02:15,060
and as I have underlined here data lives

00:02:13,410 --> 00:02:16,500
in data bases like there aren't

00:02:15,060 --> 00:02:18,599
companies out there with massive like

00:02:16,500 --> 00:02:20,640
inventories of CSV files that usually

00:02:18,599 --> 00:02:22,140
harvest through and so I thought it was

00:02:20,640 --> 00:02:24,959
really silly that a lot of these data

00:02:22,140 --> 00:02:26,610
science tutorials and even like software

00:02:24,959 --> 00:02:28,709
engineering type tutorials don't focus

00:02:26,610 --> 00:02:30,780
on sequel when it's such like a critical

00:02:28,709 --> 00:02:33,930
backbone of like literally everything we

00:02:30,780 --> 00:02:35,760
do so sequels been around for 50 years

00:02:33,930 --> 00:02:37,260
and in tech that's a really long time

00:02:35,760 --> 00:02:38,849
when things literally change month to

00:02:37,260 --> 00:02:41,190
month and it's been around so long

00:02:38,849 --> 00:02:43,739
because it's really good at what it was

00:02:41,190 --> 00:02:44,910
built to do and so knowing and how to

00:02:43,739 --> 00:02:47,040
leverage that power I think is

00:02:44,910 --> 00:02:48,599
incredibly valuable Python then provides

00:02:47,040 --> 00:02:51,510
a platform to do more advanced data

00:02:48,599 --> 00:02:52,709
manipulation and parallel processing I'm

00:02:51,510 --> 00:02:55,409
like I said knowing what tool to use

00:02:52,709 --> 00:02:56,150
when leads just an overall increase in

00:02:55,409 --> 00:02:59,540
productivity

00:02:56,150 --> 00:03:00,319
and optimization so my objectives here

00:02:59,540 --> 00:03:02,629
we're gonna go ahead and were going to

00:03:00,319 --> 00:03:04,430
review some sequel best practices we're

00:03:02,629 --> 00:03:05,569
gonna go through some data manipulation

00:03:04,430 --> 00:03:07,489
and feature engineering that you can do

00:03:05,569 --> 00:03:09,319
before you ever bring that data into

00:03:07,489 --> 00:03:11,120
pandas and then I'm gonna also talk

00:03:09,319 --> 00:03:13,549
about probably the biggest pain point of

00:03:11,120 --> 00:03:16,159
dealing with sequel and any software

00:03:13,549 --> 00:03:20,480
programming language is reading and

00:03:16,159 --> 00:03:22,189
writing to and from sequel so this is a

00:03:20,480 --> 00:03:25,040
diagram to kind of give you context as I

00:03:22,189 --> 00:03:26,900
go through this talk everything in my

00:03:25,040 --> 00:03:28,879
current production data pipelines is

00:03:26,900 --> 00:03:32,389
written in Python even the sequel is

00:03:28,879 --> 00:03:33,889
called by a or M library whichever one

00:03:32,389 --> 00:03:36,560
I'm not gonna advocate for one I think I

00:03:33,889 --> 00:03:38,780
tend to use sequel alchemy but I think

00:03:36,560 --> 00:03:41,420
they're all probably fine but everything

00:03:38,780 --> 00:03:43,849
exists within a Python atmosphere and

00:03:41,420 --> 00:03:46,129
then I use various sequel calls to go

00:03:43,849 --> 00:03:48,079
ahead limit my data down get it to where

00:03:46,129 --> 00:03:50,090
I really want it and then go ahead and

00:03:48,079 --> 00:03:51,769
straight load that into pandas for

00:03:50,090 --> 00:03:54,019
either loading into my machine learning

00:03:51,769 --> 00:03:57,680
algorithm or various other data

00:03:54,019 --> 00:03:59,239
manipulation I'd want to do so some

00:03:57,680 --> 00:04:03,019
sequel best practices before we even get

00:03:59,239 --> 00:04:04,669
started I firmly believe that if you

00:04:03,019 --> 00:04:06,620
take nothing else away from this talk

00:04:04,669 --> 00:04:08,949
this slide is probably the most

00:04:06,620 --> 00:04:11,389
important you're from statements

00:04:08,949 --> 00:04:13,699
literally dictates how you write the

00:04:11,389 --> 00:04:14,959
rest of your code it is the first thing

00:04:13,699 --> 00:04:16,789
you should be thinking of before you

00:04:14,959 --> 00:04:19,159
even write your selects before you grab

00:04:16,789 --> 00:04:21,500
your columns just how do you want to

00:04:19,159 --> 00:04:22,909
then join all your other tables how do

00:04:21,500 --> 00:04:25,370
you want to throw your where statements

00:04:22,909 --> 00:04:28,130
in there that from a statement is is

00:04:25,370 --> 00:04:30,530
crux to literally the rest of your

00:04:28,130 --> 00:04:31,970
sequel code you want it to kind of be a

00:04:30,530 --> 00:04:34,159
core table when I say core table I mean

00:04:31,970 --> 00:04:36,380
think of things that are really highly

00:04:34,159 --> 00:04:38,539
indexed and primary keyed within your

00:04:36,380 --> 00:04:39,650
database users as the example I use here

00:04:38,539 --> 00:04:42,199
there's probably a user's table

00:04:39,650 --> 00:04:43,280
literally in every database it has a lot

00:04:42,199 --> 00:04:44,900
of great keys that you're gonna be

00:04:43,280 --> 00:04:47,479
linking to other things it's really

00:04:44,900 --> 00:04:49,940
indexed it's probably concise nice and

00:04:47,479 --> 00:04:52,220
easy to kind of pivot off up there I

00:04:49,940 --> 00:04:54,320
have an example here two queries they

00:04:52,220 --> 00:04:55,550
return the exact same results it is

00:04:54,320 --> 00:04:57,349
grabbing all users that have

00:04:55,550 --> 00:04:59,210
theoretically had an order within the

00:04:57,349 --> 00:05:01,490
last 30 days the only thing different

00:04:59,210 --> 00:05:03,800
about these queries is I flip the tables

00:05:01,490 --> 00:05:05,539
so what was is the from table and one is

00:05:03,800 --> 00:05:07,800
my inner joint in another one and it

00:05:05,539 --> 00:05:09,840
literally makes a six-second difference

00:05:07,800 --> 00:05:11,940
so just stepping back putting a little

00:05:09,840 --> 00:05:13,740
patience into your architecture here and

00:05:11,940 --> 00:05:15,930
understanding of how you want to write

00:05:13,740 --> 00:05:17,789
that rest that query compound it out

00:05:15,930 --> 00:05:21,800
over millions and millions of rows six

00:05:17,789 --> 00:05:24,300
seconds can make a huge difference so

00:05:21,800 --> 00:05:26,849
also advocating for why temp tables rock

00:05:24,300 --> 00:05:28,979
when I was a fledgling data analyst I

00:05:26,849 --> 00:05:31,110
wrote like three hundred line sequel

00:05:28,979 --> 00:05:33,180
queries and I was so proud of them I was

00:05:31,110 --> 00:05:37,259
like super super pumped I thought it was

00:05:33,180 --> 00:05:39,270
hot I was like like nested they're

00:05:37,259 --> 00:05:40,680
impossible to read I did terrible

00:05:39,270 --> 00:05:47,759
aliases and I was like this is perfect

00:05:40,680 --> 00:05:49,020
job security it was very bad when I got

00:05:47,759 --> 00:05:50,280
more into the programming side when I

00:05:49,020 --> 00:05:52,409
went into data science and learned

00:05:50,280 --> 00:05:54,210
Python and learned about single

00:05:52,409 --> 00:05:55,710
responsibilities and how you should

00:05:54,210 --> 00:05:58,440
write things in a really meaningful way

00:05:55,710 --> 00:06:00,300
where they have one goal and one outcome

00:05:58,440 --> 00:06:02,550
that you're really kind of aiming for

00:06:00,300 --> 00:06:04,289
there and so I started taking what I

00:06:02,550 --> 00:06:06,180
used to put into sub selects in my

00:06:04,289 --> 00:06:08,639
sequel query and go ahead and bringing

00:06:06,180 --> 00:06:10,889
them out into temped tables but you then

00:06:08,639 --> 00:06:14,099
just regular inner join when you write

00:06:10,889 --> 00:06:16,409
your like final sequel pull they're one

00:06:14,099 --> 00:06:18,330
it's a way easier to troubleshoot like

00:06:16,409 --> 00:06:20,340
you can see is it happening here and

00:06:18,330 --> 00:06:22,169
this temp table here or here we're

00:06:20,340 --> 00:06:24,990
trying to like troubleshoot nested

00:06:22,169 --> 00:06:27,270
sequel queries is a bit of a nightmare

00:06:24,990 --> 00:06:28,680
also if you don't know about sequel

00:06:27,270 --> 00:06:31,169
there's a thing called a query optimizer

00:06:28,680 --> 00:06:33,360
that runs it's incredibly unpredictable

00:06:31,169 --> 00:06:36,870
trying to figure out how sequel code

00:06:33,360 --> 00:06:38,580
runs is nearly impossible there's some

00:06:36,870 --> 00:06:39,990
code out there like sequel server where

00:06:38,580 --> 00:06:42,419
you can actually like visualize it out

00:06:39,990 --> 00:06:44,279
and see still it's a little bit hard to

00:06:42,419 --> 00:06:45,810
diagnose but knowing that it's there and

00:06:44,279 --> 00:06:47,909
that it's working to find the most

00:06:45,810 --> 00:06:49,440
efficient way to run your code is kind

00:06:47,909 --> 00:06:51,300
of what's important and the query

00:06:49,440 --> 00:06:53,190
optimizer works way better on temp

00:06:51,300 --> 00:06:54,509
tables than it does on sub selects sub

00:06:53,190 --> 00:06:56,310
selects kind of break it and it can't

00:06:54,509 --> 00:06:58,889
quite figure out how to optimize things

00:06:56,310 --> 00:07:00,569
where and temp tables it's like cool I

00:06:58,889 --> 00:07:01,830
optimized over here and optimized over

00:07:00,569 --> 00:07:05,060
here and now I'm bringing them together

00:07:01,830 --> 00:07:05,060
and it's nice and easy

00:07:05,130 --> 00:07:07,680
some just additional things that I'm

00:07:06,690 --> 00:07:10,560
gonna call here that didn't deserve

00:07:07,680 --> 00:07:12,630
their own slide when you're doing

00:07:10,560 --> 00:07:15,479
wildcard searches if you can go ahead

00:07:12,630 --> 00:07:17,820
and just limit it to a back-end wildcard

00:07:15,479 --> 00:07:19,469
search that way it's not searching the

00:07:17,820 --> 00:07:20,599
entire string I think that probably

00:07:19,469 --> 00:07:24,559
actually applies to Python

00:07:20,599 --> 00:07:26,179
to additionally functions on index

00:07:24,559 --> 00:07:28,939
columns in the where clause remove

00:07:26,179 --> 00:07:30,979
indexing that's a bit of a tongue

00:07:28,939 --> 00:07:33,830
twister so I provided an example here so

00:07:30,979 --> 00:07:35,719
substring is a function these two things

00:07:33,830 --> 00:07:37,819
return the again the exact same results

00:07:35,719 --> 00:07:40,069
substring will remove my indexing from

00:07:37,819 --> 00:07:42,889
whatever column I'm calling it on and it

00:07:40,069 --> 00:07:45,979
will make my query run shorter versus a

00:07:42,889 --> 00:07:47,869
like syntax will actually run much

00:07:45,979 --> 00:07:49,009
faster so to again just thinking about

00:07:47,869 --> 00:07:51,669
how you want to go ahead and structure

00:07:49,009 --> 00:07:53,719
it get the same results just faster

00:07:51,669 --> 00:07:55,610
don't pull in columns you don't need

00:07:53,719 --> 00:07:57,830
that applies to all software engineering

00:07:55,610 --> 00:08:00,349
and all data science ever more columns

00:07:57,830 --> 00:08:01,759
as more data more data is slower so if

00:08:00,349 --> 00:08:03,860
you can like really streamline what

00:08:01,759 --> 00:08:07,249
you're pulling in is just going to be

00:08:03,860 --> 00:08:10,309
faster move filters from the where

00:08:07,249 --> 00:08:12,110
statement in the join condition to the

00:08:10,309 --> 00:08:13,490
join condition if using an outer join so

00:08:12,110 --> 00:08:14,779
an outer join you're grabbing everything

00:08:13,490 --> 00:08:16,039
that's in your from table and then

00:08:14,779 --> 00:08:18,229
everything that's in your outer join

00:08:16,039 --> 00:08:20,149
table it will go ahead and grab

00:08:18,229 --> 00:08:22,219
everything and then apply your wear

00:08:20,149 --> 00:08:24,169
condition where is if you go ahead and

00:08:22,219 --> 00:08:26,569
move that to an and statement and your

00:08:24,169 --> 00:08:28,519
join it will apply that filter before

00:08:26,569 --> 00:08:30,409
the wear condition and not pull in

00:08:28,519 --> 00:08:31,550
unnecessary data so just one of those

00:08:30,409 --> 00:08:33,709
things to keep in mind that's a nice

00:08:31,550 --> 00:08:36,110
little easy win

00:08:33,709 --> 00:08:37,399
use your indices as much as possible

00:08:36,110 --> 00:08:39,979
that's what it's there for

00:08:37,399 --> 00:08:42,050
they help the query optimizer like I was

00:08:39,979 --> 00:08:44,420
saying really figure out the fastest way

00:08:42,050 --> 00:08:46,370
to go ahead and do that and last but not

00:08:44,420 --> 00:08:50,120
least if you can use Union all versus

00:08:46,370 --> 00:08:52,040
Union distinct it is faster also an SI

00:08:50,120 --> 00:08:54,410
sequel when you just write Union

00:08:52,040 --> 00:08:55,970
defaults to Union distinct and so you

00:08:54,410 --> 00:08:57,949
might not even be realizing that you're

00:08:55,970 --> 00:09:00,199
purposely causing your queries to take

00:08:57,949 --> 00:09:01,670
longer because you didn't explicitly

00:09:00,199 --> 00:09:02,540
call it out but it's going through and

00:09:01,670 --> 00:09:04,370
it's trying to make sure there's no

00:09:02,540 --> 00:09:06,350
duplicates in it which will take longer

00:09:04,370 --> 00:09:07,490
so if you don't care about tuple kits or

00:09:06,350 --> 00:09:08,509
if you were already deep duped when

00:09:07,490 --> 00:09:10,730
you're going ahead and combining these

00:09:08,509 --> 00:09:12,560
tables just make sure you you add that

00:09:10,730 --> 00:09:17,269
all especially if you're just using

00:09:12,560 --> 00:09:19,779
Union so some data manipulation before

00:09:17,269 --> 00:09:22,490
we ever get into Python and pandas here

00:09:19,779 --> 00:09:24,680
joining multiple data sources this is

00:09:22,490 --> 00:09:27,019
what sequel was made to do this is why

00:09:24,680 --> 00:09:29,510
it's been around for 50 years this is

00:09:27,019 --> 00:09:31,040
what it is good at you're going to have

00:09:29,510 --> 00:09:32,930
a massive

00:09:31,040 --> 00:09:35,300
charge relational database especially if

00:09:32,930 --> 00:09:37,550
you've ever worked in pretty likes

00:09:35,300 --> 00:09:39,710
essentially sized company I used to work

00:09:37,550 --> 00:09:43,490
in healthcare those databases are

00:09:39,710 --> 00:09:47,150
insanity healthcare has so much data and

00:09:43,490 --> 00:09:50,000
so linking across tables like it would

00:09:47,150 --> 00:09:52,160
be very silly to bring in like users and

00:09:50,000 --> 00:09:53,570
orders into two separate pandas

00:09:52,160 --> 00:09:54,890
dataframes and then do your joining

00:09:53,570 --> 00:09:56,510
right like we're gonna want to do our

00:09:54,890 --> 00:09:59,680
joining long before we ever get it into

00:09:56,510 --> 00:10:01,610
pandas I would even go so far as to

00:09:59,680 --> 00:10:03,740
advocate for standing up a quick

00:10:01,610 --> 00:10:05,510
database if it's gonna be CSV is that

00:10:03,740 --> 00:10:06,920
you're accessing multiple times so say

00:10:05,510 --> 00:10:08,780
you do have a Rossy as viewed and you're

00:10:06,920 --> 00:10:10,730
not working with a database but you're

00:10:08,780 --> 00:10:13,760
going to be accessing these CSV is on a

00:10:10,730 --> 00:10:16,190
regular basis go ahead pick a pick a

00:10:13,760 --> 00:10:18,890
sequel that you like I prefer Postgres

00:10:16,190 --> 00:10:20,360
but really any of them are fine I just

00:10:18,890 --> 00:10:21,710
go ahead spin it up a quick database

00:10:20,360 --> 00:10:23,360
really quick and throw your CSV is in

00:10:21,710 --> 00:10:25,310
there that way you're not having to

00:10:23,360 --> 00:10:26,600
reload it in a memory every time you can

00:10:25,310 --> 00:10:29,120
just quickly go ahead and join those

00:10:26,600 --> 00:10:30,650
tables create a final condense table

00:10:29,120 --> 00:10:35,120
that you want and then pull that into

00:10:30,650 --> 00:10:36,320
memory also what you should do before

00:10:35,120 --> 00:10:38,180
you ever loaded in memory like I was

00:10:36,320 --> 00:10:40,760
saying is go ahead and narrow down that

00:10:38,180 --> 00:10:42,440
data set learning five million rows

00:10:40,760 --> 00:10:45,140
straight into memory and then applying

00:10:42,440 --> 00:10:47,300
filters and pandas would be incredibly

00:10:45,140 --> 00:10:49,610
time consuming and inefficient and not

00:10:47,300 --> 00:10:50,660
the best use of any kind of pipeline but

00:10:49,610 --> 00:10:51,860
even if you're not doing a pipeline if

00:10:50,660 --> 00:10:53,870
you're just doing a sport or a data

00:10:51,860 --> 00:10:55,520
analysis or anything like that really

00:10:53,870 --> 00:10:57,020
bringing it down to what you need to

00:10:55,520 --> 00:10:58,610
load in the memory and then applying

00:10:57,020 --> 00:11:00,920
what we learned earlier with Andrews

00:10:58,610 --> 00:11:03,470
stuff as to how to make pandas a little

00:11:00,920 --> 00:11:06,470
more memory efficient will save you a

00:11:03,470 --> 00:11:07,850
lot of headache also and the data

00:11:06,470 --> 00:11:08,630
science side of things time frame

00:11:07,850 --> 00:11:10,670
considerations

00:11:08,630 --> 00:11:13,040
go ahead train your model on one year of

00:11:10,670 --> 00:11:14,690
data and then training on two years and

00:11:13,040 --> 00:11:16,340
see if you see a dramatic improvement if

00:11:14,690 --> 00:11:21,890
you don't don't pull in two years of

00:11:16,340 --> 00:11:23,330
data is unnecessary so again this talk

00:11:21,890 --> 00:11:25,610
is kind of geared towards data science

00:11:23,330 --> 00:11:27,290
so if you don't completely understand by

00:11:25,610 --> 00:11:28,730
a some variants don't worry about it

00:11:27,290 --> 00:11:31,640
these graphs kind of give you a general

00:11:28,730 --> 00:11:33,560
point but in machine learning there is

00:11:31,640 --> 00:11:37,640
something you have to consider this

00:11:33,560 --> 00:11:39,590
variance bias trade-off and what that

00:11:37,640 --> 00:11:41,120
really will help us determine is how

00:11:39,590 --> 00:11:45,080
many records we can go ahead and load

00:11:41,120 --> 00:11:47,390
here so I nicely stole these from course

00:11:45,080 --> 00:11:48,470
intercourse right and renewing but

00:11:47,390 --> 00:11:50,690
they're nice and they get the point

00:11:48,470 --> 00:11:52,250
across so in a high variance case for

00:11:50,690 --> 00:11:55,370
our learning curves here versus our test

00:11:52,250 --> 00:11:56,570
and train set you can kind of see that

00:11:55,370 --> 00:11:59,060
if we were to extrapolate this out

00:11:56,570 --> 00:12:00,770
farther our training set size eventually

00:11:59,060 --> 00:12:03,440
those lines would converge and we would

00:12:00,770 --> 00:12:05,300
agree on an error so more data in this

00:12:03,440 --> 00:12:06,770
case would help us so you might want to

00:12:05,300 --> 00:12:08,810
go back and grab that two years of data

00:12:06,770 --> 00:12:10,400
we were talking about whereas if you

00:12:08,810 --> 00:12:12,080
have high bias we're gonna converge

00:12:10,400 --> 00:12:14,270
those tests and trainsets pretty quickly

00:12:12,080 --> 00:12:16,760
on a high air which has its own problem

00:12:14,270 --> 00:12:19,550
so it's just good for diagnosing that as

00:12:16,760 --> 00:12:21,170
well but more training data will not

00:12:19,550 --> 00:12:22,520
solve your problem here so again it's

00:12:21,170 --> 00:12:24,410
just kind of a nice way to figure out

00:12:22,520 --> 00:12:26,300
all right how much data do I actually

00:12:24,410 --> 00:12:28,970
need to go ahead and make this machine

00:12:26,300 --> 00:12:31,190
learning algorithm work on my next slide

00:12:28,970 --> 00:12:32,720
here I have a what I'm calling a good

00:12:31,190 --> 00:12:35,090
learning curve we don't have bias or

00:12:32,720 --> 00:12:37,790
variance we kind of converge quickly and

00:12:35,090 --> 00:12:38,870
at a low error and here I'm advocating

00:12:37,790 --> 00:12:40,130
so if you're setting up an entire

00:12:38,870 --> 00:12:42,170
pipeline that's going ahead and

00:12:40,130 --> 00:12:45,320
retraining and updating your model on

00:12:42,170 --> 00:12:47,090
new data as it becomes available maybe

00:12:45,320 --> 00:12:48,980
you don't need a massive training set

00:12:47,090 --> 00:12:50,870
like you did that first time go ahead

00:12:48,980 --> 00:12:52,880
plot out a learning curve and see okay

00:12:50,870 --> 00:12:54,710
wherever this line was where we

00:12:52,880 --> 00:12:57,440
converged that's actually all the data I

00:12:54,710 --> 00:12:58,850
need I don't need to pull in everything

00:12:57,440 --> 00:13:00,950
else I'm not getting any benefit from

00:12:58,850 --> 00:13:04,040
that if anything it's it's hurting my

00:13:00,950 --> 00:13:05,780
time to retrain that model so just some

00:13:04,040 --> 00:13:08,900
I think learning curves and data science

00:13:05,780 --> 00:13:12,520
are multifaceted beneficial and so I

00:13:08,900 --> 00:13:15,170
advocate for them pretty pretty heavily

00:13:12,520 --> 00:13:16,640
you can also do a lot of feature

00:13:15,170 --> 00:13:18,440
engineering at sequel I would say

00:13:16,640 --> 00:13:20,870
because it's a language I'm more

00:13:18,440 --> 00:13:22,940
comfortable with over Python I tend to

00:13:20,870 --> 00:13:25,160
do more feature engineering than less in

00:13:22,940 --> 00:13:27,050
sequel but here I called out a few

00:13:25,160 --> 00:13:29,120
instances and where I think it's better

00:13:27,050 --> 00:13:32,120
a lot of it as the last bullet point

00:13:29,120 --> 00:13:33,830
will say is kind of up to you at that

00:13:32,120 --> 00:13:36,980
point in time what language you're most

00:13:33,830 --> 00:13:39,980
comfortable in but specifically with

00:13:36,980 --> 00:13:41,750
things like get dummies you can run into

00:13:39,980 --> 00:13:43,400
some issues so for those who do not know

00:13:41,750 --> 00:13:45,470
what get dummies or one hot encoding

00:13:43,400 --> 00:13:47,570
does and skate learn it goes ahead and

00:13:45,470 --> 00:13:49,269
takes a categorical value improve it

00:13:47,570 --> 00:13:52,749
that out too

00:13:49,269 --> 00:13:54,879
multiple binary columns but if you have

00:13:52,749 --> 00:13:57,160
so when you bring in and you do a

00:13:54,879 --> 00:13:58,299
training thing to train the algorithm

00:13:57,160 --> 00:14:00,579
you have to do that same data

00:13:58,299 --> 00:14:02,499
manipulation to your test set so that it

00:14:00,579 --> 00:14:03,789
goes through those same pipelines but if

00:14:02,499 --> 00:14:05,799
you went ahead and trained and you had

00:14:03,789 --> 00:14:07,809
five categorical values that you one hot

00:14:05,799 --> 00:14:10,569
encoded you're gonna end up with four

00:14:07,809 --> 00:14:12,189
columns and that will be good you'll

00:14:10,569 --> 00:14:14,799
train your model but say you go through

00:14:12,189 --> 00:14:17,169
your test set you one hunting code those

00:14:14,799 --> 00:14:19,779
are get dummy of those and you only have

00:14:17,169 --> 00:14:21,339
four examples of those five categorical

00:14:19,779 --> 00:14:23,139
values it's only going to make three

00:14:21,339 --> 00:14:25,389
columns and so when you go to go and

00:14:23,139 --> 00:14:27,189
train or run your test set through your

00:14:25,389 --> 00:14:29,439
trade model you'll get a size in this

00:14:27,189 --> 00:14:30,879
match it's kind of annoying in that way

00:14:29,439 --> 00:14:33,850
and that it's dynamic it's supposed to

00:14:30,879 --> 00:14:36,249
be helpful but if you run into a

00:14:33,850 --> 00:14:37,660
category value with all categories

00:14:36,249 --> 00:14:40,269
aren't represented equally and one of

00:14:37,660 --> 00:14:41,499
them is kind of a minority you might run

00:14:40,269 --> 00:14:43,419
into this issue where you end up with a

00:14:41,499 --> 00:14:45,220
size mismatch where case statements

00:14:43,419 --> 00:14:47,379
nicely have else statements and um right

00:14:45,220 --> 00:14:50,289
so you write your case in sequel case if

00:14:47,379 --> 00:14:52,329
this then this else zero and then every

00:14:50,289 --> 00:14:54,489
time that data runs through that you're

00:14:52,329 --> 00:14:57,909
always guaranteed the exact same size of

00:14:54,489 --> 00:15:00,189
data for training your data set lead and

00:14:57,909 --> 00:15:01,539
leg and rank functions are available in

00:15:00,189 --> 00:15:06,519
pandas I think it's called shipped and

00:15:01,539 --> 00:15:09,129
rank but kind of circling back lead and

00:15:06,519 --> 00:15:10,809
rank and sequel uses those indices that

00:15:09,129 --> 00:15:13,629
we were talking about to go ahead and

00:15:10,809 --> 00:15:15,639
optimize those where we do not see that

00:15:13,629 --> 00:15:17,409
same performance improvement for the

00:15:15,639 --> 00:15:19,749
similar function in pandas so if you're

00:15:17,409 --> 00:15:21,819
gonna do something likely like lead leg

00:15:19,749 --> 00:15:24,519
and rank I advocate for doing that in

00:15:21,819 --> 00:15:26,199
sequel over pandas and then most other

00:15:24,519 --> 00:15:28,539
feature engineering it's kind of up to

00:15:26,199 --> 00:15:30,009
you as to what you think is better or

00:15:28,539 --> 00:15:34,119
what works for you or what you're most

00:15:30,009 --> 00:15:35,229
comfortable in a shout out to pandas I'm

00:15:34,119 --> 00:15:37,569
not forgetting this is a Python

00:15:35,229 --> 00:15:40,720
convention here and wasn't coming to

00:15:37,569 --> 00:15:42,279
talk all about sequel I promise pandas

00:15:40,720 --> 00:15:43,929
is really really awesome as we've seen

00:15:42,279 --> 00:15:47,169
from a few of the other talks today it's

00:15:43,929 --> 00:15:50,259
super powerful you have an entire like

00:15:47,169 --> 00:15:52,449
Python ecosystem at your hands a lot of

00:15:50,259 --> 00:15:55,749
them because pandas is so ubiquitous is

00:15:52,449 --> 00:15:57,909
made to nicely work with pandas the

00:15:55,749 --> 00:16:00,669
biggest biggest point on this too for

00:15:57,909 --> 00:16:01,540
pandas is its testable sequel is not

00:16:00,669 --> 00:16:03,010
testable

00:16:01,540 --> 00:16:04,870
I think I've seen a few companies try

00:16:03,010 --> 00:16:06,970
you can probably get to like some like

00:16:04,870 --> 00:16:08,500
loose tests but not in the same way you

00:16:06,970 --> 00:16:11,529
would have unit test nicely written in

00:16:08,500 --> 00:16:12,699
Python so there might be a reason that

00:16:11,529 --> 00:16:15,399
you do do a lot of your feature

00:16:12,699 --> 00:16:16,959
engineering in pandas is to go ahead and

00:16:15,399 --> 00:16:18,459
make sure that when you're doing that

00:16:16,959 --> 00:16:20,410
data manipulation you have tests written

00:16:18,459 --> 00:16:22,660
for it and you're validating that as

00:16:20,410 --> 00:16:24,300
you're going through and then easy

00:16:22,660 --> 00:16:26,529
integration with data visualization

00:16:24,300 --> 00:16:29,980
libraries functions like do you have

00:16:26,529 --> 00:16:33,430
describe our pretty cornerstone for any

00:16:29,980 --> 00:16:35,820
EDA you're gonna do so not to not to get

00:16:33,430 --> 00:16:35,820
panda's album

00:16:39,779 --> 00:16:44,320
so optimizing reading and writing

00:16:42,339 --> 00:16:47,980
because this is my biggest pain point

00:16:44,320 --> 00:16:50,250
with advocating for sequel pandas read

00:16:47,980 --> 00:16:53,589
sequel and read Google bigquery are

00:16:50,250 --> 00:16:54,940
notoriously slow especially read Google

00:16:53,589 --> 00:16:58,630
bigquery there's actually I think

00:16:54,940 --> 00:17:00,130
recently while I was writing this Google

00:16:58,630 --> 00:17:01,899
came out with its own library to go

00:17:00,130 --> 00:17:04,000
ahead and read it data into data frames

00:17:01,899 --> 00:17:06,130
from Google bigquery so I would actually

00:17:04,000 --> 00:17:08,650
bypass this completely and never

00:17:06,130 --> 00:17:10,959
recommend you use that function but

00:17:08,650 --> 00:17:13,000
overall the best way I have found seems

00:17:10,959 --> 00:17:15,370
a little odd cuz you add a step in the

00:17:13,000 --> 00:17:16,660
middle but you go ahead and once you

00:17:15,370 --> 00:17:18,100
kind of have your final trimmed down

00:17:16,660 --> 00:17:20,260
data set that we were talking about in

00:17:18,100 --> 00:17:22,329
sequel you save that to a temp table or

00:17:20,260 --> 00:17:24,970
attempt or a table and you go ahead and

00:17:22,329 --> 00:17:26,679
you export that to a compressed CSV now

00:17:24,970 --> 00:17:29,020
it seems silly to throw a step in

00:17:26,679 --> 00:17:31,240
between sequel and pandas when you can

00:17:29,020 --> 00:17:32,980
just read straight in but sequel is

00:17:31,240 --> 00:17:35,350
optimized to write out to see us be

00:17:32,980 --> 00:17:37,870
really easy and pandas is optimized to

00:17:35,350 --> 00:17:39,429
read in CSV is really easy so it seems

00:17:37,870 --> 00:17:42,850
weird to purposely put a step in between

00:17:39,429 --> 00:17:46,240
but it genuinely runs much faster so

00:17:42,850 --> 00:17:47,950
it's kind of annoying but I a similar

00:17:46,240 --> 00:17:49,870
talk to this at a PI data conference and

00:17:47,950 --> 00:17:51,460
a few senior software engineers came up

00:17:49,870 --> 00:17:53,470
to me we're like yes that is the way we

00:17:51,460 --> 00:17:54,940
do it too so until we as a community

00:17:53,470 --> 00:17:57,040
find a slightly better way to do this

00:17:54,940 --> 00:18:00,940
adding that extra step actually makes it

00:17:57,040 --> 00:18:02,350
faster writing there is there is an

00:18:00,940 --> 00:18:05,080
option we don't have to write to CSV is

00:18:02,350 --> 00:18:07,090
here there are batch batch writing

00:18:05,080 --> 00:18:09,610
options and sequel alchemy here I

00:18:07,090 --> 00:18:11,679
provide an example and then also the

00:18:09,610 --> 00:18:14,870
same similar example in SPARC of

00:18:11,679 --> 00:18:17,860
updating our JDBC URL to go ahead

00:18:14,870 --> 00:18:20,300
and enable batch writing I had a

00:18:17,860 --> 00:18:22,880
recommender that was running in spark

00:18:20,300 --> 00:18:25,760
and was writing out to a my sequel

00:18:22,880 --> 00:18:27,050
database and it was taking hours and I

00:18:25,760 --> 00:18:28,760
was like man I don't know what's taking

00:18:27,050 --> 00:18:30,860
so long on this and I was like trying to

00:18:28,760 --> 00:18:32,900
like section it down and it wasn't the

00:18:30,860 --> 00:18:35,180
like actual machine learning computation

00:18:32,900 --> 00:18:36,920
because as we all know from spark that's

00:18:35,180 --> 00:18:39,740
like spread over like 15 worker nodes

00:18:36,920 --> 00:18:41,900
and I made some beefy clusters and that

00:18:39,740 --> 00:18:43,970
was running just fine it was the right

00:18:41,900 --> 00:18:45,350
that was taking so long literally hours

00:18:43,970 --> 00:18:48,500
and then all I did was put in this

00:18:45,350 --> 00:18:50,450
rewrite batch statements equals true and

00:18:48,500 --> 00:18:52,550
it brought it down to like 15 minutes it

00:18:50,450 --> 00:18:54,080
was outrageous so if you are ever

00:18:52,550 --> 00:18:56,270
operating in spark and writing out to

00:18:54,080 --> 00:18:58,400
sequel copy paste this and every single

00:18:56,270 --> 00:19:04,730
code you have ever written and it will

00:18:58,400 --> 00:19:06,230
just be faster so I talk really fast but

00:19:04,730 --> 00:19:08,870
I also wanted to give time for questions

00:19:06,230 --> 00:19:10,250
because I think that's kind of this best

00:19:08,870 --> 00:19:11,900
atmosphere here I wanted to give you

00:19:10,250 --> 00:19:14,390
guys some kind of highlights here of

00:19:11,900 --> 00:19:15,800
like sequel super-powerful pandas is

00:19:14,390 --> 00:19:17,929
very very powerful

00:19:15,800 --> 00:19:19,580
pythons powerful and I just think

00:19:17,929 --> 00:19:22,190
knowing what tool to use when is

00:19:19,580 --> 00:19:23,510
incredibly powerful so I'm actually

00:19:22,190 --> 00:19:26,780
going to go ahead and open up the floor

00:19:23,510 --> 00:19:27,790
to questions right now and you guys are

00:19:26,780 --> 00:19:29,840
free I think they put a microphone

00:19:27,790 --> 00:19:31,610
conveniently located in the middle of

00:19:29,840 --> 00:19:34,900
the floor there if you would like to ask

00:19:31,610 --> 00:19:38,000
questions if not I will not pressure you

00:19:34,900 --> 00:19:41,300
if they're like a maximum data set size

00:19:38,000 --> 00:19:43,970
that you've found beyond which you would

00:19:41,300 --> 00:19:45,830
generally recommend people try and do a

00:19:43,970 --> 00:19:48,590
bunch of working in SQL versus in Pentos

00:19:45,830 --> 00:19:50,530
oh like what you would that's a good

00:19:48,590 --> 00:19:52,910
question

00:19:50,530 --> 00:19:53,710
no I don't think so I think that's very

00:19:52,910 --> 00:19:56,090
subjective

00:19:53,710 --> 00:19:57,470
kind of like what Andrews talk was

00:19:56,090 --> 00:19:59,300
talking about earlier I think it depends

00:19:57,470 --> 00:20:00,350
on the actual types of data you're

00:19:59,300 --> 00:20:02,570
dealing with so if you have a lot of

00:20:00,350 --> 00:20:04,130
like string or objects type stuff going

00:20:02,570 --> 00:20:05,990
ahead and like trying to limit that down

00:20:04,130 --> 00:20:07,670
and the fashion would be beneficial

00:20:05,990 --> 00:20:09,290
before you get into pandas or go ahead

00:20:07,670 --> 00:20:11,330
and doing that data manipulation that he

00:20:09,290 --> 00:20:14,150
was advocating for in pandas go ahead

00:20:11,330 --> 00:20:15,920
and save that memory space so I think it

00:20:14,150 --> 00:20:18,110
depends yeah a lot on how many columns

00:20:15,920 --> 00:20:20,090
versus I can't give a row size because

00:20:18,110 --> 00:20:22,240
like I was saying columns also take up a

00:20:20,090 --> 00:20:26,260
lot of space and so like saying like

00:20:22,240 --> 00:20:27,429
you have 100 columns versus five columns

00:20:26,260 --> 00:20:30,820
it's going to be very different when

00:20:27,429 --> 00:20:32,050
loading it into memory so it's hard to

00:20:30,820 --> 00:20:34,360
give like a row limit on that so I think

00:20:32,050 --> 00:20:36,160
it's very subjective I think there's a

00:20:34,360 --> 00:20:37,480
lot to be said about intuition around

00:20:36,160 --> 00:20:39,400
these kind of things so you as a

00:20:37,480 --> 00:20:41,429
programmer I think could could follow a

00:20:39,400 --> 00:20:43,750
lot of your gut there and see like oh

00:20:41,429 --> 00:20:46,270
I'll just bring in a small sample here

00:20:43,750 --> 00:20:49,059
see how they like MVP pipeline runs with

00:20:46,270 --> 00:20:50,740
just a like 100 sample type thing get

00:20:49,059 --> 00:20:52,510
some terrible tests learning like test

00:20:50,740 --> 00:20:54,429
scores out of that and then go ahead and

00:20:52,510 --> 00:20:56,230
hit er8 from there and just see as I

00:20:54,429 --> 00:20:58,210
start ramping up that data side where my

00:20:56,230 --> 00:21:03,370
pipeline starts to break apart a little

00:20:58,210 --> 00:21:05,080
bit in regards to the data cleaning can

00:21:03,370 --> 00:21:08,260
you speak as to what you prefer to do

00:21:05,080 --> 00:21:10,120
with dirty data with a lot of Nannes do

00:21:08,260 --> 00:21:11,830
you tend to drop them or how would you

00:21:10,120 --> 00:21:13,660
fill them again I think that's it's

00:21:11,830 --> 00:21:16,420
incredibly subjective to but I like it

00:21:13,660 --> 00:21:17,950
I think it depends on the columns the

00:21:16,420 --> 00:21:19,840
Nan's are in so if you think it's gonna

00:21:17,950 --> 00:21:23,230
be like a pretty highly predictive

00:21:19,840 --> 00:21:25,540
column or has shown like if you like

00:21:23,230 --> 00:21:27,550
so say you drop all the rows with Nan's

00:21:25,540 --> 00:21:28,870
in it you don't leave it and then you go

00:21:27,550 --> 00:21:29,760
ahead and you do like a random forest

00:21:28,870 --> 00:21:32,110
type thing and you get a feature

00:21:29,760 --> 00:21:33,790
importance out of that and then that

00:21:32,110 --> 00:21:35,590
column that had a lot of Nan's turned

00:21:33,790 --> 00:21:38,230
out to be really really like predictive

00:21:35,590 --> 00:21:39,580
I would maybe probably advocate for

00:21:38,230 --> 00:21:41,200
going back and trying some machine

00:21:39,580 --> 00:21:44,170
learning ways to go ahead and fill those

00:21:41,200 --> 00:21:45,790
names and more like informed way versus

00:21:44,170 --> 00:21:48,309
like just filling them with an average

00:21:45,790 --> 00:21:49,540
or something like that but it's also

00:21:48,309 --> 00:21:51,100
possible that there's a completely

00:21:49,540 --> 00:21:52,390
useless columns that don't have any

00:21:51,100 --> 00:21:53,590
predictive power and then I would just

00:21:52,390 --> 00:21:56,280
drop them because I wouldn't waste time

00:21:53,590 --> 00:21:59,140
on them and so yeah again subjective

00:21:56,280 --> 00:22:01,450
well your intuition on that one but kind

00:21:59,140 --> 00:22:03,490
of I I guess I as a starting point would

00:22:01,450 --> 00:22:04,750
advocate for either just filling them

00:22:03,490 --> 00:22:07,630
all with zeros or dropping them

00:22:04,750 --> 00:22:09,880
completely and just training a MVP model

00:22:07,630 --> 00:22:12,100
on data you do have and see which

00:22:09,880 --> 00:22:14,380
columns tend to tend to pull out as that

00:22:12,100 --> 00:22:16,210
most important I really like this is

00:22:14,380 --> 00:22:18,550
kind of a side note I tend to put in

00:22:16,210 --> 00:22:20,260
when I'm training a model a random

00:22:18,550 --> 00:22:22,450
column so it's just a column that I just

00:22:20,260 --> 00:22:24,220
fill with random numbers and then when I

00:22:22,450 --> 00:22:25,360
do something like random forest or

00:22:24,220 --> 00:22:28,480
something I can get feature importance

00:22:25,360 --> 00:22:30,130
out of I go ahead and I see everything

00:22:28,480 --> 00:22:31,390
that's above random and I keep it and

00:22:30,130 --> 00:22:33,090
everything that's below random my draft

00:22:31,390 --> 00:22:36,210
because it was no better than random

00:22:33,090 --> 00:22:37,710
and so I I really like that and so I

00:22:36,210 --> 00:22:39,629
think like starting with that throwing

00:22:37,710 --> 00:22:41,370
in a random column dropping anything

00:22:39,629 --> 00:22:43,259
that doesn't have viable data in it and

00:22:41,370 --> 00:22:47,850
then using that as a benchmark for go

00:22:43,259 --> 00:22:50,630
ahead and filling those out more all

00:22:47,850 --> 00:22:55,590
right well thank you guys so much

00:22:50,630 --> 00:23:08,890
[Applause]

00:22:55,590 --> 00:23:08,890

YouTube URL: https://www.youtube.com/watch?v=H5FNFxHgSj8


