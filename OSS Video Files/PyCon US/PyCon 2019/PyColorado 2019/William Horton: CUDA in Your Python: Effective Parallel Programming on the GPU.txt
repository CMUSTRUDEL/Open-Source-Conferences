Title: William Horton: CUDA in Your Python: Effective Parallel Programming on the GPU
Publication date: 2019-10-22
Playlist: PyColorado 2019
Description: 
	It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but GPUs provide a chance for continued hardware performance gains, if you can structure your programs to make good use of them. In this talk you will learn how to speed up your Python programs using Nvidia’s CUDA platform.
Captions: 
	00:00:00,370 --> 00:00:46,230
[Music]

00:00:42,980 --> 00:00:49,320
hi everyone excited to be here at the

00:00:46,230 --> 00:00:51,030
first high colorado conference i'm well

00:00:49,320 --> 00:00:53,460
important and this is kuda and your

00:00:51,030 --> 00:00:55,829
python today we're gonna learn about how

00:00:53,460 --> 00:00:58,970
you can harness the processing power of

00:00:55,829 --> 00:01:02,059
the GPU from within your Python programs

00:00:58,970 --> 00:01:06,140
but to start out I have some bad news

00:01:02,059 --> 00:01:09,060
Moore's law is dead this is unfortunate

00:01:06,140 --> 00:01:11,369
Moore's law basically was the idea that

00:01:09,060 --> 00:01:13,080
the number of transistors that you could

00:01:11,369 --> 00:01:14,970
fit on an integrated circuit would

00:01:13,080 --> 00:01:17,009
double every two years it was originally

00:01:14,970 --> 00:01:20,039
one year he reformulated it but

00:01:17,009 --> 00:01:21,300
basically roughly over two years the

00:01:20,039 --> 00:01:23,490
number of transistors you could fit on

00:01:21,300 --> 00:01:26,640
the chip would keep going up and he

00:01:23,490 --> 00:01:29,880
started with this data and the left side

00:01:26,640 --> 00:01:31,770
is a log linear scale so basically the

00:01:29,880 --> 00:01:35,610
line means that it's it's doubling every

00:01:31,770 --> 00:01:37,679
year and that's continued to hold across

00:01:35,610 --> 00:01:40,110
many decades and in fact Moore's law has

00:01:37,679 --> 00:01:41,789
been the foundation for being able to

00:01:40,110 --> 00:01:43,860
build some of the software that were

00:01:41,789 --> 00:01:47,009
capable of building now because of these

00:01:43,860 --> 00:01:49,020
improvements in hardware but recently

00:01:47,009 --> 00:01:50,700
Moore's law has been running up into

00:01:49,020 --> 00:01:54,060
some difficulties and those difficulties

00:01:50,700 --> 00:01:55,259
are physics this is a representation of

00:01:54,060 --> 00:01:59,520
the highest level of physics that I

00:01:55,259 --> 00:02:02,399
attained so I will not give you an

00:01:59,520 --> 00:02:05,130
in-depth explanation of these exact

00:02:02,399 --> 00:02:06,509
physical limitations but basically as

00:02:05,130 --> 00:02:08,700
things start getting smaller and smaller

00:02:06,509 --> 00:02:11,640
you run into some fundamental problems

00:02:08,700 --> 00:02:14,880
with keeping shrinking it as well as

00:02:11,640 --> 00:02:16,319
some problems with power dissipation so

00:02:14,880 --> 00:02:17,849
they start getting really really hot and

00:02:16,319 --> 00:02:19,950
if you want to know more and maybe you

00:02:17,849 --> 00:02:21,690
know more physics than ap physics see

00:02:19,950 --> 00:02:23,910
you can look up Dennard scaling and

00:02:21,690 --> 00:02:25,020
there's some interesting stuff there but

00:02:23,910 --> 00:02:27,890
we're starting to run into some of these

00:02:25,020 --> 00:02:31,170
fundamental physical problems with

00:02:27,890 --> 00:02:32,819
continuing hardware progress and people

00:02:31,170 --> 00:02:34,980
have been writing about this a lot of

00:02:32,819 --> 00:02:38,280
articles the end of Moore's law death of

00:02:34,980 --> 00:02:39,900
Moore's law all this you might still

00:02:38,280 --> 00:02:42,450
question like this has been true for

00:02:39,900 --> 00:02:45,750
many decades so should we really believe

00:02:42,450 --> 00:02:47,430
that it's going to end here's someone

00:02:45,750 --> 00:02:49,890
who I think is an authority on this he

00:02:47,430 --> 00:02:52,170
said I guess I see Moore's law dying in

00:02:49,890 --> 00:02:53,490
the next decade or so and this is

00:02:52,170 --> 00:02:55,830
someone I really trust on the

00:02:53,490 --> 00:02:59,280
s'matter because this is gordon moore he

00:02:55,830 --> 00:03:01,650
said it in 2015 so i think we can at

00:02:59,280 --> 00:03:03,900
least imagine that things are gonna slow

00:03:01,650 --> 00:03:05,700
down over the next couple years and we

00:03:03,900 --> 00:03:09,120
might want to start looking for an

00:03:05,700 --> 00:03:10,800
alternative to just computing on the cpu

00:03:09,120 --> 00:03:13,590
and that leads us to the question of why

00:03:10,800 --> 00:03:15,000
GPUs why am i giving this talk today I

00:03:13,590 --> 00:03:17,190
think there's a tremendous amount of

00:03:15,000 --> 00:03:18,750
potential in the GPU and trying to

00:03:17,190 --> 00:03:21,060
alleviate some of these problems we're

00:03:18,750 --> 00:03:23,610
facing with the death of Moore's law so

00:03:21,060 --> 00:03:25,170
a little bit of a history of the GPU

00:03:23,610 --> 00:03:27,320
it's a graphics processing unit

00:03:25,170 --> 00:03:29,520
originally developed for gaming purposes

00:03:27,320 --> 00:03:31,830
and was designed to be really good at

00:03:29,520 --> 00:03:33,810
matrix operations and parallel

00:03:31,830 --> 00:03:36,510
processing because you can imagine to

00:03:33,810 --> 00:03:39,210
render a scene or to do shading you're

00:03:36,510 --> 00:03:40,590
basically working on a 3d matrix of

00:03:39,210 --> 00:03:42,600
pixels and then you might want to do

00:03:40,590 --> 00:03:45,180
operations on that and that's basically

00:03:42,600 --> 00:03:48,630
what the GPU was set up to do really

00:03:45,180 --> 00:03:50,760
well to understand a little bit more we

00:03:48,630 --> 00:03:53,580
can compare the specs of some hardware

00:03:50,760 --> 00:03:58,110
so this is a kind of high-end consumer

00:03:53,580 --> 00:03:59,790
GPU from Nvidia versus an Intel CPU so

00:03:58,110 --> 00:04:02,760
you can look at the specs here and you

00:03:59,790 --> 00:04:05,880
can see that the Intel CPU has eight

00:04:02,760 --> 00:04:08,280
cores the GPU has 68 of what are called

00:04:05,880 --> 00:04:11,280
streaming multi processors and those

00:04:08,280 --> 00:04:14,070
roughly match up with core count on a

00:04:11,280 --> 00:04:15,990
CPU but it also has over 4,000 var

00:04:14,070 --> 00:04:17,790
called CUDA cores and those are kind of

00:04:15,990 --> 00:04:20,670
arithmetic units that can perform these

00:04:17,790 --> 00:04:24,090
operations so you can see there's a at

00:04:20,670 --> 00:04:26,520
least sometimes more processing units on

00:04:24,090 --> 00:04:28,890
the GPU on the other hand you can see

00:04:26,520 --> 00:04:30,720
that the base clock and boost clock of

00:04:28,890 --> 00:04:33,210
the CPU are much higher almost three

00:04:30,720 --> 00:04:34,680
times higher and so then you can start

00:04:33,210 --> 00:04:38,310
to understand kind of the trade-off in

00:04:34,680 --> 00:04:40,920
these two processing paradigms where the

00:04:38,310 --> 00:04:43,770
GPU you can do more things but not as

00:04:40,920 --> 00:04:45,480
fast and the CPU maybe you can do not as

00:04:43,770 --> 00:04:48,180
many things but your base clock is

00:04:45,480 --> 00:04:50,580
higher rate and that gets down to a

00:04:48,180 --> 00:04:54,810
fundamental difference in how these are

00:04:50,580 --> 00:04:57,840
architected so basically this is a

00:04:54,810 --> 00:05:00,330
layout of the different chips and the

00:04:57,840 --> 00:05:01,950
green represents arithmetic units and

00:05:00,330 --> 00:05:04,410
this gets the fundamental difference and

00:05:01,950 --> 00:05:06,240
why GPUs can do this data processing

00:05:04,410 --> 00:05:10,280
faster is that the Jeep

00:05:06,240 --> 00:05:13,470
devoting a lot more transistors to

00:05:10,280 --> 00:05:15,690
arithmetic units but the trade-off is it

00:05:13,470 --> 00:05:18,630
devotes a lot less as you can see to

00:05:15,690 --> 00:05:19,620
control flow and to cache and those are

00:05:18,630 --> 00:05:21,569
things that are really important to

00:05:19,620 --> 00:05:23,729
making the CPU a general-purpose

00:05:21,569 --> 00:05:25,919
processor so this is kind of the

00:05:23,729 --> 00:05:28,319
trade-off is the GPU can do a lot more

00:05:25,919 --> 00:05:31,050
of these ads and multiplies but maybe

00:05:28,319 --> 00:05:33,539
your control flow can't be as complex

00:05:31,050 --> 00:05:36,389
and your cache is going to be smaller so

00:05:33,539 --> 00:05:39,720
this is kind of what it looks like CPU

00:05:36,389 --> 00:05:42,539
versus GPU and what the GPU can do well

00:05:39,720 --> 00:05:43,979
on a hard lourve level and in recent

00:05:42,539 --> 00:05:46,190
years we've seen the rise of what's

00:05:43,979 --> 00:05:49,130
called GPGPU it's the idea you can do

00:05:46,190 --> 00:05:51,780
general-purpose processing on the GPU

00:05:49,130 --> 00:05:52,860
and it's just a shift in mindset of

00:05:51,780 --> 00:05:55,440
saying well maybe it's good at these

00:05:52,860 --> 00:05:57,389
workloads for video games but maybe it

00:05:55,440 --> 00:06:00,360
can also do these workflows for data

00:05:57,389 --> 00:06:02,550
processing and machine learning and so a

00:06:00,360 --> 00:06:04,139
lot of the hardware manufacturers came

00:06:02,550 --> 00:06:06,720
up with their own paradigms from

00:06:04,139 --> 00:06:09,150
programming their devices so I'm gonna

00:06:06,720 --> 00:06:10,440
talk about CUDA and NVIDIA today because

00:06:09,150 --> 00:06:13,500
that's kind of the dominant one in the

00:06:10,440 --> 00:06:15,020
space now but AMD has their own and I

00:06:13,500 --> 00:06:17,490
should mention there's also open

00:06:15,020 --> 00:06:20,159
specifications so this is kind of

00:06:17,490 --> 00:06:22,860
starting to grow as a programming

00:06:20,159 --> 00:06:24,210
paradigm and so basically the

00:06:22,860 --> 00:06:27,300
fundamental questions I want to answer

00:06:24,210 --> 00:06:27,960
in this talk are one why GPUs we just

00:06:27,300 --> 00:06:31,770
went through that

00:06:27,960 --> 00:06:34,590
what is CUDA why would you want to have

00:06:31,770 --> 00:06:36,840
it in your Python and then what's next

00:06:34,590 --> 00:06:40,110
what if you learn how to use this in

00:06:36,840 --> 00:06:41,969
your programs what can you do with it

00:06:40,110 --> 00:06:44,009
but before we get too far into that I'll

00:06:41,969 --> 00:06:44,729
talk a little about myself how I got

00:06:44,009 --> 00:06:47,969
here

00:06:44,729 --> 00:06:49,830
so my work I work on data pipelines and

00:06:47,969 --> 00:06:52,500
machine learning at Compass which is a

00:06:49,830 --> 00:06:55,469
real estate technology platform we're

00:06:52,500 --> 00:06:57,870
basically building tools to help agents

00:06:55,469 --> 00:06:59,759
do their work better and we use a bunch

00:06:57,870 --> 00:07:03,569
of open-source Python tools I've listed

00:06:59,759 --> 00:07:04,919
a couple I really like airflow obviously

00:07:03,569 --> 00:07:07,440
we use a lot of jupiter notebooks for

00:07:04,919 --> 00:07:09,090
some of the machine learning stuff and

00:07:07,440 --> 00:07:11,130
one recent project we launched for

00:07:09,090 --> 00:07:12,840
example is recommendations on our

00:07:11,130 --> 00:07:15,630
homepage so if we have some of your

00:07:12,840 --> 00:07:17,110
listings that you've already viewed we

00:07:15,630 --> 00:07:18,490
can serve you some of the similar

00:07:17,110 --> 00:07:21,100
ones from our inventory and say you

00:07:18,490 --> 00:07:22,720
might also be interested in this and I

00:07:21,100 --> 00:07:25,750
will say we haven't actually started

00:07:22,720 --> 00:07:27,970
using GPUs yet but as we start to

00:07:25,750 --> 00:07:31,090
incorporate features like image and text

00:07:27,970 --> 00:07:35,050
that's where people have started using

00:07:31,090 --> 00:07:36,250
GPU processing and so what really got me

00:07:35,050 --> 00:07:38,500
interested in this stuff was some of my

00:07:36,250 --> 00:07:41,140
hobbies I got really into deep learning

00:07:38,500 --> 00:07:43,960
through the FAFSA I course did a couple

00:07:41,140 --> 00:07:45,430
Cavill competitions mostly using PI

00:07:43,960 --> 00:07:48,670
torch and you can see this is actually

00:07:45,430 --> 00:07:51,220
my home server I've got two GPUs at home

00:07:48,670 --> 00:07:53,620
and I'll tell you I built this after one

00:07:51,220 --> 00:07:56,380
weekend when I was in a competition and

00:07:53,620 --> 00:07:59,050
I was running a GPU on AWS and I forgot

00:07:56,380 --> 00:08:01,240
to turn it off and on the people who

00:07:59,050 --> 00:08:03,280
experienced this before right and so on

00:08:01,240 --> 00:08:06,850
Monday I woke up and I'm like oh my god

00:08:03,280 --> 00:08:08,790
what is my AWS bill so at that point I

00:08:06,850 --> 00:08:11,530
was like okay maybe it's better to build

00:08:08,790 --> 00:08:14,020
versus rent and that led me to come up

00:08:11,530 --> 00:08:15,880
with this idea call Hortons law you see

00:08:14,020 --> 00:08:18,580
a repurposed as I grabbed from before

00:08:15,880 --> 00:08:21,070
and this is basically a doubling of your

00:08:18,580 --> 00:08:22,450
AWS bill would they increase in your

00:08:21,070 --> 00:08:25,210
interest in deep learning so just be

00:08:22,450 --> 00:08:27,630
careful out there if you're doing this

00:08:25,210 --> 00:08:27,630
kind of stuff

00:08:29,289 --> 00:08:35,779
so in terms of the use of GPUs right now

00:08:34,190 --> 00:08:38,269
the present which you might have heard

00:08:35,779 --> 00:08:39,440
about some of these tools is GPUs for

00:08:38,269 --> 00:08:42,409
deep learning so there's a lot of

00:08:39,440 --> 00:08:44,180
frameworks pi torch tensorflow a couple

00:08:42,409 --> 00:08:46,070
others and that's part of what got me

00:08:44,180 --> 00:08:48,260
interested in giving this talk is I made

00:08:46,070 --> 00:08:49,880
a P R 2 pi torch last year that was

00:08:48,260 --> 00:08:52,490
basically moving some of the

00:08:49,880 --> 00:08:54,890
functionality of padding tensors from a

00:08:52,490 --> 00:08:57,110
Python implementation into the C++ side

00:08:54,890 --> 00:08:58,550
and seeing the internals of that library

00:08:57,110 --> 00:09:00,440
got me thinking well how did they fit

00:08:58,550 --> 00:09:05,120
this all together they've got a Python

00:09:00,440 --> 00:09:07,700
API C++ and khuda' going on but I think

00:09:05,120 --> 00:09:09,560
CUDA and GPU processing is also leading

00:09:07,700 --> 00:09:12,050
to a lot more than just deep learning

00:09:09,560 --> 00:09:13,430
there's GPU databases which people have

00:09:12,050 --> 00:09:15,620
been I think trying to build for a while

00:09:13,430 --> 00:09:18,200
and this is just one recent example of

00:09:15,620 --> 00:09:21,920
uber built-in analytics database powered

00:09:18,200 --> 00:09:24,680
by GPUs and even the thought of GPUs for

00:09:21,920 --> 00:09:26,750
the whole data pipeline so Rapids is an

00:09:24,680 --> 00:09:28,880
effort an open-source effort from Nvidia

00:09:26,750 --> 00:09:31,760
to basically say we won't just do

00:09:28,880 --> 00:09:34,190
modeling on the GPU but also maybe data

00:09:31,760 --> 00:09:35,779
pre-processing an afterword data

00:09:34,190 --> 00:09:37,579
visualization and understanding your

00:09:35,779 --> 00:09:39,709
results and maybe we can put this all on

00:09:37,579 --> 00:09:42,769
the GPU and just accelerate your whole

00:09:39,709 --> 00:09:44,390
workflow so that's kind of what might be

00:09:42,769 --> 00:09:47,990
the future if we can learn how to use

00:09:44,390 --> 00:09:50,630
these tools in more of our programs so

00:09:47,990 --> 00:09:52,640
to get to the question that you probably

00:09:50,630 --> 00:09:54,350
all wanted to know when I started this

00:09:52,640 --> 00:09:56,810
talk is how can you start programming

00:09:54,350 --> 00:09:58,610
the GPU and so I came up with a little

00:09:56,810 --> 00:10:01,610
bit of an example this is a numpy

00:09:58,610 --> 00:10:04,610
example basically just adding two really

00:10:01,610 --> 00:10:06,920
big vectors and so I'm going to show you

00:10:04,610 --> 00:10:09,290
the example this would obviously run on

00:10:06,920 --> 00:10:12,170
the CPU and numpy and so I'll show you

00:10:09,290 --> 00:10:14,390
the example of GPU that's the example of

00:10:12,170 --> 00:10:17,870
GPU I'll show that to you again okay

00:10:14,390 --> 00:10:18,920
this is CPU GPU I highlighted the

00:10:17,870 --> 00:10:22,610
differences there just a couple

00:10:18,920 --> 00:10:25,130
characters and I benchmarked this

00:10:22,610 --> 00:10:27,290
example I got about a 30 times speed-up

00:10:25,130 --> 00:10:29,750
so just that change is gonna get you 30

00:10:27,290 --> 00:10:32,360
times speed-up and that's that's it

00:10:29,750 --> 00:10:36,279
basically you can go home and program

00:10:32,360 --> 00:10:36,279
your GPUs thank you thank you

00:10:36,370 --> 00:10:39,580
unfortunately it's not quite so simple

00:10:38,170 --> 00:10:41,920
so I'm going to talk about a couple

00:10:39,580 --> 00:10:43,750
different approaches to how you can get

00:10:41,920 --> 00:10:45,510
CUDA in your Python program so what I

00:10:43,750 --> 00:10:48,490
just showed is a drop-in replacement

00:10:45,510 --> 00:10:50,470
coupe high is a great library basically

00:10:48,490 --> 00:10:52,240
a drop-in 4 numpy they try to emulate

00:10:50,470 --> 00:10:55,330
the exact same API that you already are

00:10:52,240 --> 00:10:57,010
familiar with it supports indexing

00:10:55,330 --> 00:10:58,990
broadcasting the kind of things you

00:10:57,010 --> 00:11:00,880
might already like about using them PI

00:10:58,990 --> 00:11:02,500
just to highlight a couple of API

00:11:00,880 --> 00:11:04,840
differences because I made that example

00:11:02,500 --> 00:11:07,390
look a little bit too easy in terms of

00:11:04,840 --> 00:11:09,670
datatypes unfortunately the GPU is not

00:11:07,390 --> 00:11:12,520
good with handling strings and objects

00:11:09,670 --> 00:11:15,790
right now so it's limited to the numpy

00:11:12,520 --> 00:11:19,360
data types that are numerical array

00:11:15,790 --> 00:11:22,930
creation basically you can do an UMP I

00:11:19,360 --> 00:11:24,850
array around a Python list and coop I

00:11:22,930 --> 00:11:26,440
unfortunately doesn't support that so if

00:11:24,850 --> 00:11:28,000
you do that in your programs you might

00:11:26,440 --> 00:11:29,770
need to tweak it a little bit and make a

00:11:28,000 --> 00:11:31,660
couple changes and the last thing is

00:11:29,770 --> 00:11:34,270
just because the GPU wants to always be

00:11:31,660 --> 00:11:36,910
operating on vectors and not scalars if

00:11:34,270 --> 00:11:40,210
you have methods like some that in numpy

00:11:36,910 --> 00:11:41,680
are going to return you a number in coop

00:11:40,210 --> 00:11:44,830
I it's going to return you a zero order

00:11:41,680 --> 00:11:46,270
vector with that in it so those are just

00:11:44,830 --> 00:11:49,240
a few things if you want to actually

00:11:46,270 --> 00:11:51,520
start doing this in your numpy code and

00:11:49,240 --> 00:11:53,650
there's a couple other examples of CUDA

00:11:51,520 --> 00:11:55,840
drop-in so there's qdf now which is

00:11:53,650 --> 00:11:58,420
supposed to be emulating the pandas API

00:11:55,840 --> 00:11:59,290
there's KML which is a CUDA powered

00:11:58,420 --> 00:12:01,600
scikit-learn

00:11:59,290 --> 00:12:04,120
and I think these are pretty cool

00:12:01,600 --> 00:12:06,580
because it it makes it easy if you're

00:12:04,120 --> 00:12:09,070
already familiar with these api's and

00:12:06,580 --> 00:12:12,190
these Python tools to try to start using

00:12:09,070 --> 00:12:13,810
this in your workflows moving beyond

00:12:12,190 --> 00:12:16,570
drop-ins you'd have to start writing

00:12:13,810 --> 00:12:18,190
we're called CUDA kernels and so we'll

00:12:16,570 --> 00:12:19,990
talk a little about the CUDA API so if

00:12:18,190 --> 00:12:22,420
you want to write custom CUDA code

00:12:19,990 --> 00:12:24,370
yourself how can we do that and the

00:12:22,420 --> 00:12:26,650
heart of that is threads blocks and

00:12:24,370 --> 00:12:28,270
grids these are the main structures of

00:12:26,650 --> 00:12:31,090
the kind of the CUDA programming

00:12:28,270 --> 00:12:33,040
paradigm so to start with threads

00:12:31,090 --> 00:12:35,500
basically threads or what are executing

00:12:33,040 --> 00:12:37,600
your CUDA code and the important thing

00:12:35,500 --> 00:12:40,660
is these have a thread index in up to

00:12:37,600 --> 00:12:43,120
three dimensions and basically the

00:12:40,660 --> 00:12:44,499
thread index lets you say what part of

00:12:43,120 --> 00:12:48,099
the data showed this thread were

00:12:44,499 --> 00:12:50,319
gone and why does the index have three

00:12:48,099 --> 00:12:53,529
dimensions it has to do with the kinds

00:12:50,319 --> 00:12:54,999
of workloads that GPU programming is

00:12:53,529 --> 00:12:58,209
going to be good for and so that's

00:12:54,999 --> 00:13:00,669
basically dared data parallel workloads

00:12:58,209 --> 00:13:01,929
and the idea is that you have many of

00:13:00,669 --> 00:13:03,759
these threads and they're all going to

00:13:01,929 --> 00:13:05,889
work on a different piece of the data

00:13:03,759 --> 00:13:07,989
and basically do the same thing so if

00:13:05,889 --> 00:13:10,419
you're adding two really long vectors

00:13:07,989 --> 00:13:13,269
you're gonna have many threads and they

00:13:10,419 --> 00:13:15,849
might just add one slice of that vector

00:13:13,269 --> 00:13:17,259
to another slice of the other vector but

00:13:15,849 --> 00:13:18,909
they're basically doing the same kind of

00:13:17,259 --> 00:13:22,089
logic just on different pieces of data

00:13:18,909 --> 00:13:24,099
and so this is like one example let's

00:13:22,089 --> 00:13:26,199
say you had four threads and you have

00:13:24,099 --> 00:13:28,839
nine elements in your array you might

00:13:26,199 --> 00:13:30,789
say okay the first thread is gonna do

00:13:28,839 --> 00:13:32,079
work on some of these elements the

00:13:30,789 --> 00:13:35,379
second thread is going to do work on

00:13:32,079 --> 00:13:37,569
other elements and the nice thing about

00:13:35,379 --> 00:13:41,709
this 1d example is you can come up with

00:13:37,569 --> 00:13:44,139
a rule for which piece of the array the

00:13:41,709 --> 00:13:45,789
thread is supposed to touch so you can

00:13:44,139 --> 00:13:47,829
see in terms of your thread to your

00:13:45,789 --> 00:13:51,039
index it's basically the thread index

00:13:47,829 --> 00:13:53,529
plus the number of threads times I and

00:13:51,039 --> 00:13:56,289
so you can come up with some kind of

00:13:53,529 --> 00:14:03,069
logic for assigning work to your threads

00:13:56,289 --> 00:14:04,869
and in the 1d case it's lines up because

00:14:03,069 --> 00:14:06,399
you have your thread index and one to

00:14:04,869 --> 00:14:09,249
mention your data index and one to

00:14:06,399 --> 00:14:11,139
mention and the 2d case it gets a little

00:14:09,249 --> 00:14:12,339
bit tricky and what what thread is

00:14:11,139 --> 00:14:15,339
supposed to do work on which part of

00:14:12,339 --> 00:14:17,439
this array you have to use modulo this

00:14:15,339 --> 00:14:19,959
can get a little bit weird the 3d

00:14:17,439 --> 00:14:22,269
example I can't even really like hold in

00:14:19,959 --> 00:14:24,309
my head and so the nice thing about CUDA

00:14:22,269 --> 00:14:26,349
is basically they give you indexes that

00:14:24,309 --> 00:14:28,449
you can use in the same dimension as

00:14:26,349 --> 00:14:31,179
your data and that just allows you to

00:14:28,449 --> 00:14:33,429
more easily do data parallel processing

00:14:31,179 --> 00:14:35,919
because you can basically say your

00:14:33,429 --> 00:14:37,899
threads are basically a two-dimensional

00:14:35,919 --> 00:14:39,399
structure and your data is also a

00:14:37,899 --> 00:14:41,439
two-dimensional structure and that just

00:14:39,399 --> 00:14:43,629
makes it a lot easier to line up what

00:14:41,439 --> 00:14:46,239
each thread is supposed to be processing

00:14:43,629 --> 00:14:48,339
so this is kind of the bottom part of

00:14:46,239 --> 00:14:51,069
the diagram I showed before your threads

00:14:48,339 --> 00:14:53,799
are in grouped in this block and in this

00:14:51,069 --> 00:14:56,529
case they have two indexes so they're

00:14:53,799 --> 00:14:58,760
indexed in in two dimensions in terms of

00:14:56,529 --> 00:15:00,170
blocks and grids blocks organized

00:14:58,760 --> 00:15:02,240
your groups of threads and they provide

00:15:00,170 --> 00:15:04,100
two main important functionalities one

00:15:02,240 --> 00:15:06,170
is shared memory between threads and a

00:15:04,100 --> 00:15:08,150
block the other one is synchronization

00:15:06,170 --> 00:15:09,710
mechanisms if they're sharing the memory

00:15:08,150 --> 00:15:12,920
they also need a way to do some basic

00:15:09,710 --> 00:15:15,200
synchronization the block also can be

00:15:12,920 --> 00:15:16,790
indexed and up to three dimensions and

00:15:15,200 --> 00:15:18,410
then the grid is basically just a group

00:15:16,790 --> 00:15:20,210
of blocks so this is what it ends up

00:15:18,410 --> 00:15:21,740
looking like you've got your grid which

00:15:20,210 --> 00:15:24,080
has made a box and then each of those

00:15:21,740 --> 00:15:27,350
blocks is made up of threads and that's

00:15:24,080 --> 00:15:30,530
kind of the main piece of the CUDA

00:15:27,350 --> 00:15:34,460
paradigm and so as an example of some

00:15:30,530 --> 00:15:37,840
CUDA code this is one piece that

00:15:34,460 --> 00:15:41,090
basically adds to two-dimensional arrays

00:15:37,840 --> 00:15:43,580
and so to understand this a little bit

00:15:41,090 --> 00:15:47,480
the other important piece of the kind of

00:15:43,580 --> 00:15:49,640
CUDA programming is hosted device so to

00:15:47,480 --> 00:15:52,310
execute CUDA code you need both the CPU

00:15:49,640 --> 00:15:54,200
and a GPU so the CPU is referred to as

00:15:52,310 --> 00:15:56,180
host GPU is referred to as device and

00:15:54,200 --> 00:16:00,220
you transfer data between them as you

00:15:56,180 --> 00:16:02,690
need to do computation and the

00:16:00,220 --> 00:16:04,490
underscore ins were global is used to

00:16:02,690 --> 00:16:07,340
mark the kernel which is the piece of

00:16:04,490 --> 00:16:09,530
code that executes on the GPU and the

00:16:07,340 --> 00:16:11,630
other kind of special syntax is these

00:16:09,530 --> 00:16:14,360
angle brackets which you use to specify

00:16:11,630 --> 00:16:16,040
the grid size so basically how many

00:16:14,360 --> 00:16:18,440
threads you want to end up with is a

00:16:16,040 --> 00:16:20,690
function of the size of your grid and

00:16:18,440 --> 00:16:23,660
the size of your blocks so going back to

00:16:20,690 --> 00:16:26,060
this code this is just an example you

00:16:23,660 --> 00:16:29,510
can see you're calling this matrix add

00:16:26,060 --> 00:16:32,360
kernel below and so the bottom part will

00:16:29,510 --> 00:16:33,890
run on your CPU once it calls this

00:16:32,360 --> 00:16:35,210
kernel it'll start executing the code

00:16:33,890 --> 00:16:38,090
above and you can also see that it's

00:16:35,210 --> 00:16:39,740
using these indexing things that I was

00:16:38,090 --> 00:16:44,360
talking about block index and thread

00:16:39,740 --> 00:16:47,750
index so one way that you can start

00:16:44,360 --> 00:16:50,030
putting this to use is numba + CUDA JIT

00:16:47,750 --> 00:16:52,400
just-in-time compilation so if you're

00:16:50,030 --> 00:16:54,650
not familiar with numba it's basically a

00:16:52,400 --> 00:16:58,340
just-in-time compiler it's largely used

00:16:54,650 --> 00:17:01,090
for CPU to try to accelerate Python for

00:16:58,340 --> 00:17:03,740
some things that you might use C++ for

00:17:01,090 --> 00:17:06,350
people start to drop in number to

00:17:03,740 --> 00:17:08,449
accelerate their their Python code but

00:17:06,350 --> 00:17:10,010
it can also be used for GPU which is

00:17:08,449 --> 00:17:12,170
pretty cool and it lets you write

00:17:10,010 --> 00:17:12,620
functions like this so you can see this

00:17:12,170 --> 00:17:15,220
is a PI

00:17:12,620 --> 00:17:18,110
on function you decorated with this CUDA

00:17:15,220 --> 00:17:20,630
decorator and then inside you also are

00:17:18,110 --> 00:17:22,790
writing Python code but basically behind

00:17:20,630 --> 00:17:24,829
the scenes when your program gets to

00:17:22,790 --> 00:17:27,650
this function it's going to Justin time

00:17:24,829 --> 00:17:29,750
compile and cache the basically CUDA

00:17:27,650 --> 00:17:32,450
machine code that you need to execute

00:17:29,750 --> 00:17:34,730
this and then it will call that with

00:17:32,450 --> 00:17:37,670
your data so this is pretty cool because

00:17:34,730 --> 00:17:39,110
it allows you to still write Python but

00:17:37,670 --> 00:17:41,420
behind the scenes this is getting turned

00:17:39,110 --> 00:17:44,200
into the machine code that you need

00:17:41,420 --> 00:17:47,480
there's some limitations because it's

00:17:44,200 --> 00:17:50,960
compiling this function so exception

00:17:47,480 --> 00:17:53,809
handling basically you can't use context

00:17:50,960 --> 00:17:55,460
managers generators those are just a

00:17:53,809 --> 00:17:57,320
little bit too complicated for the

00:17:55,460 --> 00:17:59,300
compiler to be able to understand and

00:17:57,320 --> 00:18:01,880
there's also certain features of numpy

00:17:59,300 --> 00:18:04,730
you can't use within it but it's still I

00:18:01,880 --> 00:18:07,040
think a great way to be able to so write

00:18:04,730 --> 00:18:08,450
Python and and compile it behind the

00:18:07,040 --> 00:18:12,770
scenes and there's a lot of cool

00:18:08,450 --> 00:18:15,110
features so one of them is basically

00:18:12,770 --> 00:18:16,850
being able to access these CUDA features

00:18:15,110 --> 00:18:19,670
but within your Python code so the

00:18:16,850 --> 00:18:21,940
number CUDA module exposes things like

00:18:19,670 --> 00:18:24,740
the thread index thread synchronization

00:18:21,940 --> 00:18:26,330
as well as there's certain CUDA atomic

00:18:24,740 --> 00:18:29,059
operations and so they've also

00:18:26,330 --> 00:18:31,760
implemented those so you can do number

00:18:29,059 --> 00:18:34,520
CUDA atomic add and behind the scenes

00:18:31,760 --> 00:18:36,860
that's going to turn into this kind of

00:18:34,520 --> 00:18:38,630
CUDA basic operation they also have a

00:18:36,860 --> 00:18:41,360
CUDA simulator so this can be really

00:18:38,630 --> 00:18:44,210
cool for testing basically if you're

00:18:41,360 --> 00:18:46,190
writing this code and you don't have a

00:18:44,210 --> 00:18:48,050
GPU for testing it will basically

00:18:46,190 --> 00:18:52,309
simulate that and it's able to do that

00:18:48,050 --> 00:18:55,580
because it understands this and can can

00:18:52,309 --> 00:18:58,280
compile it into kind of their CUDA

00:18:55,580 --> 00:19:00,110
runtime on the Python interpreter for

00:18:58,280 --> 00:19:02,600
testing purposes it also has

00:19:00,110 --> 00:19:04,670
interoperability so this is inspired by

00:19:02,600 --> 00:19:06,980
kind of the numpy interoperability

00:19:04,670 --> 00:19:08,420
mechanism but a lot of frameworks are

00:19:06,980 --> 00:19:13,070
starting to implement this so you can

00:19:08,420 --> 00:19:15,620
basically do a zero copy changing your

00:19:13,070 --> 00:19:18,470
tensors between like KU pi pi torch

00:19:15,620 --> 00:19:20,330
tensorflow is working on it so basically

00:19:18,470 --> 00:19:22,280
without having to copy things back and

00:19:20,330 --> 00:19:24,260
forth you can maybe working in different

00:19:22,280 --> 00:19:27,220
frameworks depending on what you need to

00:19:24,260 --> 00:19:31,370
do so that's also pretty neat

00:19:27,220 --> 00:19:33,890
the next thing is PI CUDA so PI CUDA is

00:19:31,370 --> 00:19:36,650
kind of a scientific library or at least

00:19:33,890 --> 00:19:38,660
its main applications for some of these

00:19:36,650 --> 00:19:40,190
scientific projects it's one of the

00:19:38,660 --> 00:19:42,200
first libraries I found that has a paper

00:19:40,190 --> 00:19:43,790
written about it so if you want to learn

00:19:42,200 --> 00:19:45,980
more about it you can check out that

00:19:43,790 --> 00:19:48,890
paper and it's pretty cool but basically

00:19:45,980 --> 00:19:51,380
what PI CUDA does is you write strings

00:19:48,890 --> 00:19:53,240
of CUDA in your Python code and then

00:19:51,380 --> 00:19:56,300
it's going to compile that and allow you

00:19:53,240 --> 00:19:58,430
to execute it so you can see here the

00:19:56,300 --> 00:20:00,950
code in the string is an example of a

00:19:58,430 --> 00:20:02,900
CUDA kernel but then it's letting you

00:20:00,950 --> 00:20:06,500
compile that within your Python program

00:20:02,900 --> 00:20:09,860
and then run it as a function that you

00:20:06,500 --> 00:20:12,890
can call and some of the benefits that

00:20:09,860 --> 00:20:14,990
PI CUDA gives you automatic memory

00:20:12,890 --> 00:20:17,330
management it has some nice helpers

00:20:14,990 --> 00:20:19,580
around data transfer error checking and

00:20:17,330 --> 00:20:22,820
meta programming so to talk about those

00:20:19,580 --> 00:20:25,940
a little bit more I think this is one of

00:20:22,820 --> 00:20:27,290
the big benefits is basically you know

00:20:25,940 --> 00:20:29,390
memory management in CUDA can be a

00:20:27,290 --> 00:20:31,760
little bit tricky and PI CUDA just ties

00:20:29,390 --> 00:20:33,080
that to the lifetime of Python objects

00:20:31,760 --> 00:20:36,200
so once the object goes out of scope

00:20:33,080 --> 00:20:40,010
it's gonna be cleaned up it also has

00:20:36,200 --> 00:20:42,470
some nice helpers for data transfer and

00:20:40,010 --> 00:20:44,780
basically like I said before you have

00:20:42,470 --> 00:20:47,270
your host CPU you have your device GPU

00:20:44,780 --> 00:20:49,300
you're sending data back and forth but

00:20:47,270 --> 00:20:52,400
if you're using pi CUDA they have these

00:20:49,300 --> 00:20:55,040
nice classes that you basically say this

00:20:52,400 --> 00:20:57,200
data is only going in or it's coming out

00:20:55,040 --> 00:20:59,780
or in out and different combinations of

00:20:57,200 --> 00:21:01,130
that and so it basically saves you a

00:20:59,780 --> 00:21:04,730
bunch of these steps you might have to

00:21:01,130 --> 00:21:07,700
do so creating a CPU array allocating on

00:21:04,730 --> 00:21:09,200
the GPU sending it back and forth this

00:21:07,700 --> 00:21:11,900
provides you a lot of helpers that might

00:21:09,200 --> 00:21:13,460
save you from that manual work this one

00:21:11,900 --> 00:21:15,830
is a little bit weird so this is from

00:21:13,460 --> 00:21:17,330
the CUDA documentation it says if an

00:21:15,830 --> 00:21:19,580
asynchronous error occurs it will be

00:21:17,330 --> 00:21:21,770
reported by some subsequent unrelated

00:21:19,580 --> 00:21:22,550
runtime function call and this can be a

00:21:21,770 --> 00:21:25,040
bit strange

00:21:22,550 --> 00:21:27,230
I will admit but the coudé operations

00:21:25,040 --> 00:21:29,240
are async and so it can't return a

00:21:27,230 --> 00:21:31,280
synchronous error so they decided that

00:21:29,240 --> 00:21:34,070
it's just gonna show up later in your in

00:21:31,280 --> 00:21:36,700
your program so pi cuda is going to

00:21:34,070 --> 00:21:39,420
handle this for you in a nicer way

00:21:36,700 --> 00:21:40,870
returned those as Python exceptions

00:21:39,420 --> 00:21:43,810
metaprogramming

00:21:40,870 --> 00:21:45,730
is another strength of Pi CUDA so

00:21:43,810 --> 00:21:47,590
basically I talked a little bit about

00:21:45,730 --> 00:21:49,390
you can do like the size of your grid

00:21:47,590 --> 00:21:50,860
the size of your block and those are

00:21:49,390 --> 00:21:54,400
kind of parameters that you need to tune

00:21:50,860 --> 00:21:56,920
to get optimal performance and so PI

00:21:54,400 --> 00:21:59,170
CUDA actually says well you don't have

00:21:56,920 --> 00:22:01,210
to try just kind of tweaking those you

00:21:59,170 --> 00:22:03,490
can just try a bunch of them and and

00:22:01,210 --> 00:22:05,440
figure out what's fastest and basically

00:22:03,490 --> 00:22:07,480
how they do that is you can use a

00:22:05,440 --> 00:22:10,030
templating library so this example uses

00:22:07,480 --> 00:22:11,830
Jinja and basically say i want to

00:22:10,030 --> 00:22:14,620
template in some of these parameters and

00:22:11,830 --> 00:22:16,660
then i can just run run it with a bunch

00:22:14,620 --> 00:22:18,640
of these and time it and get the

00:22:16,660 --> 00:22:20,770
performance that way and so that kind of

00:22:18,640 --> 00:22:22,930
saves you from having to like run the

00:22:20,770 --> 00:22:24,940
CUDA kernel then edit the source and and

00:22:22,930 --> 00:22:27,280
so this can be a really big win as well

00:22:24,940 --> 00:22:30,100
and also for the interest of comparison

00:22:27,280 --> 00:22:31,990
I should say coop I also has this kind

00:22:30,100 --> 00:22:33,730
of functionality it's called raw kernel

00:22:31,990 --> 00:22:36,370
and it's the similar kind of thing where

00:22:33,730 --> 00:22:38,290
you can write a raw string as your CUDA

00:22:36,370 --> 00:22:42,040
code and it will compile it in your

00:22:38,290 --> 00:22:44,260
program so the last thing that you might

00:22:42,040 --> 00:22:46,030
want to use is CUDA as a C or C++

00:22:44,260 --> 00:22:49,290
extension so this would give you kind of

00:22:46,030 --> 00:22:52,000
maximum control over your program and

00:22:49,290 --> 00:22:54,100
someone who heard this talk basically

00:22:52,000 --> 00:22:57,790
asked me well at this point why not just

00:22:54,100 --> 00:23:00,640
use C++ and I said well I'm giving it a

00:22:57,790 --> 00:23:02,920
Python conference so I have to talk

00:23:00,640 --> 00:23:04,390
about Python but the real answer is

00:23:02,920 --> 00:23:06,670
there's a lot of libraries that are

00:23:04,390 --> 00:23:08,590
already doing this pi torch tends to

00:23:06,670 --> 00:23:10,930
flow even numpy basically what they're

00:23:08,590 --> 00:23:13,600
doing is putting a Python API on the

00:23:10,930 --> 00:23:14,680
performance of C or C++ or CUDA and I

00:23:13,600 --> 00:23:16,570
think that's one of the strengths of our

00:23:14,680 --> 00:23:18,340
language is that when you need that

00:23:16,570 --> 00:23:20,380
performance you can drop into the lower

00:23:18,340 --> 00:23:23,500
level but still expose this nice

00:23:20,380 --> 00:23:25,690
user-friendly API so there's

00:23:23,500 --> 00:23:27,960
documentation on this extending Python

00:23:25,690 --> 00:23:31,090
with C or C++

00:23:27,960 --> 00:23:34,180
they say it's quite easy to add new

00:23:31,090 --> 00:23:36,750
built-in modules to Python if you know

00:23:34,180 --> 00:23:39,130
how to program and C and I'm like hmm

00:23:36,750 --> 00:23:40,720
going through this I will say it's maybe

00:23:39,130 --> 00:23:43,870
not quite as easy as the documentation

00:23:40,720 --> 00:23:45,550
will make you think but I'm an outline

00:23:43,870 --> 00:23:48,220
kind of the main challenges if you want

00:23:45,550 --> 00:23:49,539
to do this yourself basically the two

00:23:48,220 --> 00:23:52,480
things you have to do is write or

00:23:49,539 --> 00:23:54,669
generate your C++ code and then

00:23:52,480 --> 00:23:57,100
configure your setup top high to be able

00:23:54,669 --> 00:23:59,289
to compile CUDA and C++ and kind of get

00:23:57,100 --> 00:24:01,990
it all together and there's a really

00:23:59,289 --> 00:24:03,429
good example of doing this end to end

00:24:01,990 --> 00:24:05,830
and I'm just gonna pick out a couple

00:24:03,429 --> 00:24:07,150
pieces of code from this but this is

00:24:05,830 --> 00:24:09,220
going to be in my slides later if you're

00:24:07,150 --> 00:24:11,830
interested seeing how it all got to put

00:24:09,220 --> 00:24:14,409
together so in terms of how to write

00:24:11,830 --> 00:24:16,179
your C++ code this is a really crowded

00:24:14,409 --> 00:24:19,299
ecosystem when I started looking into it

00:24:16,179 --> 00:24:23,080
there's so many options but some of the

00:24:19,299 --> 00:24:26,590
main ones are siphons swig you might

00:24:23,080 --> 00:24:28,030
have heard that there's boost Python and

00:24:26,590 --> 00:24:30,340
in terms of what to choose

00:24:28,030 --> 00:24:32,289
I think scythe on is great compared to

00:24:30,340 --> 00:24:33,700
some of the other options some of the

00:24:32,289 --> 00:24:36,580
other options feel like they're written

00:24:33,700 --> 00:24:38,799
for C++ developers who wanted to have

00:24:36,580 --> 00:24:40,659
some Python psyphon feels like it was

00:24:38,799 --> 00:24:42,700
written for Python developers who want

00:24:40,659 --> 00:24:45,429
to add some C++ which is kind of the

00:24:42,700 --> 00:24:47,620
scenario we're in it's pretty actively

00:24:45,429 --> 00:24:50,460
maintained used in a couple open source

00:24:47,620 --> 00:24:53,260
projects if you want more comparisons

00:24:50,460 --> 00:24:55,539
there's this blog post that's kind of up

00:24:53,260 --> 00:24:57,940
to date in terms of what the ecosystem

00:24:55,539 --> 00:25:00,370
looks like right now but psyphon is cool

00:24:57,940 --> 00:25:02,200
and the code looks like this so if you

00:25:00,370 --> 00:25:04,320
kind of look at it it looks mostly like

00:25:02,200 --> 00:25:07,150
Python there's these weird sea depths

00:25:04,320 --> 00:25:09,490
which let you specify some like sea

00:25:07,150 --> 00:25:12,250
types and things but for the most part

00:25:09,490 --> 00:25:16,419
it feels like Python with additional

00:25:12,250 --> 00:25:18,460
syntax so I talked that those would be

00:25:16,419 --> 00:25:21,789
tools on how to connect your C++ code

00:25:18,460 --> 00:25:24,730
into Python but another question is well

00:25:21,789 --> 00:25:26,409
how do you connect your CUDA code Nvidia

00:25:24,730 --> 00:25:29,350
provides you with this compiler called

00:25:26,409 --> 00:25:32,530
NBCC and it basically takes your CUDA

00:25:29,350 --> 00:25:35,260
source code it compiles it to GPU

00:25:32,530 --> 00:25:37,539
assembly it also replaces some of the

00:25:35,260 --> 00:25:41,190
special syntax so that it is actually

00:25:37,539 --> 00:25:43,990
valid C++ and then you can also use

00:25:41,190 --> 00:25:48,490
optionally a C++ compiler to compile the

00:25:43,990 --> 00:25:51,429
host code so the problem becomes then

00:25:48,490 --> 00:25:53,679
like okay I have host code that's C++ I

00:25:51,429 --> 00:25:55,270
have the CUDA kernel I have my Python

00:25:53,679 --> 00:25:57,370
program so you want to get this all

00:25:55,270 --> 00:25:59,890
together and that requires configuring

00:25:57,370 --> 00:26:01,060
your setup dot pi and setting up an

00:25:59,890 --> 00:26:03,940
extension

00:26:01,060 --> 00:26:07,720
so this is the documentation on making

00:26:03,940 --> 00:26:09,340
an extension you use disks utils and at

00:26:07,720 --> 00:26:13,240
least here they admit okay it gets a bit

00:26:09,340 --> 00:26:15,550
more complicated at this point so

00:26:13,240 --> 00:26:18,250
basically an example of a setup PI that

00:26:15,550 --> 00:26:21,910
you can use is to say okay I have this

00:26:18,250 --> 00:26:26,680
extension you specify the sources and so

00:26:21,910 --> 00:26:29,050
dot PI X is the scythe on extension and

00:26:26,680 --> 00:26:31,210
then you're basically providing

00:26:29,050 --> 00:26:34,540
arguments that's gonna tell it okay

00:26:31,210 --> 00:26:37,320
compile my CUDA with MVCC compile my c++

00:26:34,540 --> 00:26:40,180
and achieve CC link in these kind of

00:26:37,320 --> 00:26:42,010
system libraries that you need but like

00:26:40,180 --> 00:26:43,990
I said this can be complicated and you

00:26:42,010 --> 00:26:45,870
might even be wondering why do you want

00:26:43,990 --> 00:26:48,790
to do this so some of the advantages

00:26:45,870 --> 00:26:50,050
just having fewer dependencies so you

00:26:48,790 --> 00:26:53,410
don't have to be worrying about

00:26:50,050 --> 00:26:56,320
versioning of libraries who depend on so

00:26:53,410 --> 00:26:57,730
like if you're using coop I I mean you

00:26:56,320 --> 00:26:59,710
have to figure out how to install that

00:26:57,730 --> 00:27:02,470
and it might also make your library

00:26:59,710 --> 00:27:04,990
easier to ship around if you don't have

00:27:02,470 --> 00:27:06,750
dependencies on these other things the

00:27:04,990 --> 00:27:08,800
other thing is manual memory management

00:27:06,750 --> 00:27:10,570
so this is kind of a blessing and a

00:27:08,800 --> 00:27:15,520
curse you see you have to call like

00:27:10,570 --> 00:27:16,900
Malick's mem copies freeze but there are

00:27:15,520 --> 00:27:19,450
some benefits this isn't even give you

00:27:16,900 --> 00:27:21,310
like the most low-level control over air

00:27:19,450 --> 00:27:22,630
memory and there's some memory features

00:27:21,310 --> 00:27:25,360
of CUDA

00:27:22,630 --> 00:27:27,160
it's basically mapped memory and and

00:27:25,360 --> 00:27:29,350
different ways of locking the memory

00:27:27,160 --> 00:27:31,030
between CPU and GPU some of these you

00:27:29,350 --> 00:27:33,490
can access with higher-level libraries

00:27:31,030 --> 00:27:36,100
but to get the most control you're gonna

00:27:33,490 --> 00:27:39,640
want to drop into like custom CUDA code

00:27:36,100 --> 00:27:41,440
and the last one is a compiler and I

00:27:39,640 --> 00:27:43,540
know that's kind of controversial but I

00:27:41,440 --> 00:27:46,030
will say when you're working on this and

00:27:43,540 --> 00:27:47,680
maybe you're unfamiliar with CUDA having

00:27:46,030 --> 00:27:49,390
actual error messages and telling you

00:27:47,680 --> 00:27:52,840
where you messed up that can be kind of

00:27:49,390 --> 00:27:54,310
a nice benefit so finally I've given you

00:27:52,840 --> 00:27:57,250
a couple of these different methods of

00:27:54,310 --> 00:27:59,500
using CUDA in your programs so you might

00:27:57,250 --> 00:28:02,140
wonder how can you get started the first

00:27:59,500 --> 00:28:04,870
question is accessing a GPU and the good

00:28:02,140 --> 00:28:05,800
news here is that a lot of resources are

00:28:04,870 --> 00:28:07,940
available for free

00:28:05,800 --> 00:28:09,649
so Google collab is kind

00:28:07,940 --> 00:28:13,779
a notebook interface that you can run

00:28:09,649 --> 00:28:17,240
for free on a GPU on Google's servers

00:28:13,779 --> 00:28:19,039
cabrel kernels also is available for

00:28:17,240 --> 00:28:22,220
free and then here's some pricing on

00:28:19,039 --> 00:28:23,840
cloud GPU instances so about a dollar an

00:28:22,220 --> 00:28:27,379
hour google cloud you have to detach a

00:28:23,840 --> 00:28:29,360
VM so it's I think comparable pricing so

00:28:27,379 --> 00:28:31,730
once you get access I wanted to point

00:28:29,360 --> 00:28:34,820
out a couple of good resources so this

00:28:31,730 --> 00:28:36,769
is Udacity had a parallel program in

00:28:34,820 --> 00:28:38,450
CUDA course and even though it's not

00:28:36,769 --> 00:28:40,100
like active the resources are still

00:28:38,450 --> 00:28:41,440
online you just have to watch out

00:28:40,100 --> 00:28:44,000
because some of it hasn't been updated

00:28:41,440 --> 00:28:46,820
and so installing the right dependencies

00:28:44,000 --> 00:28:49,730
I had a lot of trouble installing opencv

00:28:46,820 --> 00:28:50,840
that was a whole debacle but it is a

00:28:49,730 --> 00:28:53,330
really good course and really good

00:28:50,840 --> 00:28:54,769
material and then there's also a rapid

00:28:53,330 --> 00:28:56,919
tutorial that happened at SCI PI this

00:28:54,769 --> 00:29:00,190
year if you're interested in kind of

00:28:56,919 --> 00:29:03,980
CUDA for the whole data science workflow

00:29:00,190 --> 00:29:06,230
and finally where you can go next with

00:29:03,980 --> 00:29:08,450
this the first thing I would say is

00:29:06,230 --> 00:29:10,129
figure out can you apply CUDA to your

00:29:08,450 --> 00:29:12,710
workflow if you're already using numpy

00:29:10,129 --> 00:29:14,419
like I said I think it's relatively

00:29:12,710 --> 00:29:16,940
straightforward to maybe drop in coop hi

00:29:14,419 --> 00:29:18,740
and see how that works for you

00:29:16,940 --> 00:29:20,389
next I think you can start learning

00:29:18,740 --> 00:29:22,789
about different parallel programming

00:29:20,389 --> 00:29:25,789
algorithms and what having CUDA

00:29:22,789 --> 00:29:27,259
available might unlock for you and then

00:29:25,789 --> 00:29:28,730
the last thing is there is a lot of

00:29:27,259 --> 00:29:30,919
interest in other kinds of hardware

00:29:28,730 --> 00:29:33,169
devices so they say like xpu

00:29:30,919 --> 00:29:34,700
architectures so there's now the TPU

00:29:33,169 --> 00:29:36,889
from Google a lot of other companies

00:29:34,700 --> 00:29:40,309
working on their own hardware also like

00:29:36,889 --> 00:29:41,990
FPGA ZAR increasingly popular and

00:29:40,309 --> 00:29:44,210
there's some Python libraries for

00:29:41,990 --> 00:29:45,710
working with that so if you're more in

00:29:44,210 --> 00:29:48,590
the hardware side there's also a lot of

00:29:45,710 --> 00:29:50,269
stuff going on and so the last thing

00:29:48,590 --> 00:29:53,269
I'll say is if you get into this stuff

00:29:50,269 --> 00:29:56,539
remember wardens law turn off your cloud

00:29:53,269 --> 00:29:57,200
instances and go program the GPU thank

00:29:56,539 --> 00:30:02,580
you

00:29:57,200 --> 00:30:15,880
[Applause]

00:30:02,580 --> 00:30:15,880

YouTube URL: https://www.youtube.com/watch?v=h2fVVvU-n50


