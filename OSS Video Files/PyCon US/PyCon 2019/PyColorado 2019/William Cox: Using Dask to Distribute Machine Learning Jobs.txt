Title: William Cox: Using Dask to Distribute Machine Learning Jobs
Publication date: 2019-10-22
Playlist: PyColorado 2019
Description: 
	Dask is a light-weight, Pythonic, library for doing distributed computation. Iâ€™ll talk about how we use it to run machine learning forecasting jobs, and how the library might benefit your machine learning or data science work.
Captions: 
	00:00:00,370 --> 00:00:49,250
[Music]

00:00:42,530 --> 00:00:49,250
thank you let's talk of the day yeah

00:00:49,280 --> 00:00:54,449
we're gonna get you out of here soon

00:00:52,199 --> 00:00:57,120
I'm William I'm from North Carolina at

00:00:54,449 --> 00:00:58,949
1,700 miles that way and 5,000 feet

00:00:57,120 --> 00:01:00,600
straight down

00:00:58,949 --> 00:01:02,790
I build machine learning systems at

00:01:00,600 --> 00:01:04,290
GrubHub part of a delivery team so I

00:01:02,790 --> 00:01:06,900
work to deliver food all around the

00:01:04,290 --> 00:01:09,570
country previously worked in Internet

00:01:06,900 --> 00:01:11,880
security and built sonar systems now I

00:01:09,570 --> 00:01:15,380
work to deliver food and also do amateur

00:01:11,880 --> 00:01:18,600
sleight illustrations so GrubHub

00:01:15,380 --> 00:01:21,119
american company we do online food

00:01:18,600 --> 00:01:24,660
ordering we do delivery we do self

00:01:21,119 --> 00:01:25,680
delivery which is what I work in where

00:01:24,660 --> 00:01:29,700
you can order food and get it delivered

00:01:25,680 --> 00:01:31,520
via bike or car or scooter no drones yet

00:01:29,700 --> 00:01:33,750
so ours I know

00:01:31,520 --> 00:01:36,000
so the problem that we are trying to

00:01:33,750 --> 00:01:37,920
solve is that every week we schedule

00:01:36,000 --> 00:01:39,509
drivers for time slots look at this

00:01:37,920 --> 00:01:42,630
little illustration you can think of it

00:01:39,509 --> 00:01:44,399
as we know food Peaks around lunchtime

00:01:42,630 --> 00:01:47,220
then dies off and you get a big spike

00:01:44,399 --> 00:01:48,899
around dinnertime so we schedule drivers

00:01:47,220 --> 00:01:52,080
to be on the road in order to meet that

00:01:48,899 --> 00:01:54,810
demand if we schedule too few drivers

00:01:52,080 --> 00:01:57,030
then you the diners are unhappy because

00:01:54,810 --> 00:01:59,580
your food doesn't arrive on time if we

00:01:57,030 --> 00:02:02,429
schedule too many drivers GrubHub sound

00:01:59,580 --> 00:02:03,619
happy because we pay out a base rate the

00:02:02,429 --> 00:02:06,360
drivers aren't happy because they're

00:02:03,619 --> 00:02:08,580
sitting there making the base rate

00:02:06,360 --> 00:02:11,550
instead of more for actually delivering

00:02:08,580 --> 00:02:13,590
so to solve that problem we try and

00:02:11,550 --> 00:02:16,410
predict into the future how many orders

00:02:13,590 --> 00:02:19,050
we'll get at any point in time for all

00:02:16,410 --> 00:02:21,599
of our regions around the country and so

00:02:19,050 --> 00:02:26,090
my team owns and runs that product which

00:02:21,599 --> 00:02:26,090
we have called order volume forecasting

00:02:27,709 --> 00:02:33,060
so what that looks like is every day we

00:02:31,319 --> 00:02:35,040
have this large batch job where we pull

00:02:33,060 --> 00:02:37,680
in all of our historic order data for

00:02:35,040 --> 00:02:41,220
all of our region's weather data sports

00:02:37,680 --> 00:02:42,930
data promotions the phase of the Moon

00:02:41,220 --> 00:02:45,870
anything you could think of that might

00:02:42,930 --> 00:02:47,550
influence how people order food then we

00:02:45,870 --> 00:02:49,290
train a series of models for all of our

00:02:47,550 --> 00:02:50,849
regions across the country and we

00:02:49,290 --> 00:02:53,129
predict into the future several weeks

00:02:50,849 --> 00:02:54,849
how many orders we'll get so here's an

00:02:53,129 --> 00:02:57,040
example of

00:02:54,849 --> 00:02:58,989
a predicted time series out for a week

00:02:57,040 --> 00:03:00,970
and you see you got a lunch peak and a

00:02:58,989 --> 00:03:04,799
dinner peak and then nobody really

00:03:00,970 --> 00:03:08,260
orders food between 4 a.m. and 7 a.m.

00:03:04,799 --> 00:03:11,409
and we do this every day and this takes

00:03:08,260 --> 00:03:12,610
quite a while so you can think of for

00:03:11,409 --> 00:03:14,379
all of the models that are daya

00:03:12,610 --> 00:03:16,810
scientists build in certain models work

00:03:14,379 --> 00:03:18,670
better for certain regions and for all

00:03:16,810 --> 00:03:21,760
of the regions we have across the

00:03:18,670 --> 00:03:24,670
country we're training models and do

00:03:21,760 --> 00:03:26,260
things it's predictions the situation

00:03:24,670 --> 00:03:28,000
becomes even more complicated if you as

00:03:26,260 --> 00:03:30,159
a data scientist want to say do a back

00:03:28,000 --> 00:03:33,209
test against the last year of data we

00:03:30,159 --> 00:03:35,200
have and suddenly you're your problem

00:03:33,209 --> 00:03:38,349
increases dramatically in the amount of

00:03:35,200 --> 00:03:40,569
compute you're trying to do so when I

00:03:38,349 --> 00:03:43,030
started here one of my first tasks was

00:03:40,569 --> 00:03:44,799
well how do we parallel why's this to

00:03:43,030 --> 00:03:49,870
take better advantage to the clusters of

00:03:44,799 --> 00:03:52,900
machines that we have so the design

00:03:49,870 --> 00:03:54,879
goals going into this were one we really

00:03:52,900 --> 00:03:56,379
preferred to work in Python that's what

00:03:54,879 --> 00:03:59,019
the data scientists are using to design

00:03:56,379 --> 00:04:00,609
models we wanted to reduce any headaches

00:03:59,019 --> 00:04:03,160
of trying to translate that into another

00:04:00,609 --> 00:04:06,699
system we preferred them to be pythonic

00:04:03,160 --> 00:04:09,129
it's nice to not be fighting with your

00:04:06,699 --> 00:04:11,889
api's and as you're building these

00:04:09,129 --> 00:04:13,810
things - we were preferred simplicity

00:04:11,889 --> 00:04:15,430
the less moving pieces and new sources

00:04:13,810 --> 00:04:19,000
of software that we had to deploy the

00:04:15,430 --> 00:04:21,729
better we thought it was important that

00:04:19,000 --> 00:04:24,099
we were able to do local development say

00:04:21,729 --> 00:04:26,590
on your laptop and then be able to

00:04:24,099 --> 00:04:30,039
deploy that nearly as directly as

00:04:26,590 --> 00:04:32,050
possible to a production cluster and

00:04:30,039 --> 00:04:35,860
then fourthly important for me coming

00:04:32,050 --> 00:04:37,599
into this new code base as not having

00:04:35,860 --> 00:04:39,370
seen any of it before is that I really

00:04:37,599 --> 00:04:41,169
didn't want to have to totally

00:04:39,370 --> 00:04:43,599
disassemble it and rewrite it into

00:04:41,169 --> 00:04:46,330
another framework for distributing this

00:04:43,599 --> 00:04:47,590
work and it's also important to note

00:04:46,330 --> 00:04:49,690
that our particular problem is heavy

00:04:47,590 --> 00:04:51,340
compute but I wouldn't call this a big

00:04:49,690 --> 00:04:54,039
data problem in our case and that our

00:04:51,340 --> 00:04:55,780
data fits mostly on a normal laptops

00:04:54,039 --> 00:04:58,840
memory but really we're trying to do a

00:04:55,780 --> 00:05:02,320
lot of compute in order to run train

00:04:58,840 --> 00:05:05,710
these models for all the regions the

00:05:02,320 --> 00:05:08,520
contenders were doing this work were

00:05:05,710 --> 00:05:14,200
celery which I like his food reference

00:05:08,520 --> 00:05:18,849
apache spark and then tasks ultimately

00:05:14,200 --> 00:05:24,159
we chose tasks to see if it would solve

00:05:18,849 --> 00:05:26,550
our problems and we like - for several

00:05:24,159 --> 00:05:29,889
reasons one it was a very familiar API

00:05:26,550 --> 00:05:32,979
it's often modeling existing pet Python

00:05:29,889 --> 00:05:34,450
API for doing parallel processing very

00:05:32,979 --> 00:05:36,820
easily scales out to clusters of

00:05:34,450 --> 00:05:40,229
machines and also allows you to run

00:05:36,820 --> 00:05:44,289
parallel processes on a single machine

00:05:40,229 --> 00:05:46,719
it also either directly integrates with

00:05:44,289 --> 00:05:49,090
the existing Python ecosystem or

00:05:46,719 --> 00:05:51,039
directly uses it so pandas and

00:05:49,090 --> 00:05:53,289
scikit-learn and numpy are all under the

00:05:51,039 --> 00:05:56,169
hood for doing the heavy lifting in most

00:05:53,289 --> 00:05:57,669
cases it supports complex applications

00:05:56,169 --> 00:06:00,490
if you're trying to create this crazy

00:05:57,669 --> 00:06:02,969
graph of workflow for jobs that feed

00:06:00,490 --> 00:06:06,310
into other jobs it's simple to do that

00:06:02,969 --> 00:06:08,260
and then from not necessarily the

00:06:06,310 --> 00:06:10,330
responsive software but responsive

00:06:08,260 --> 00:06:12,010
maintainer is very easy to get in touch

00:06:10,330 --> 00:06:14,500
with the people writing the software and

00:06:12,010 --> 00:06:17,650
they answer questions on Stack Overflow

00:06:14,500 --> 00:06:20,080
and in the github issues quickly which

00:06:17,650 --> 00:06:22,300
has been nice there's a quote from

00:06:20,080 --> 00:06:25,000
Matthew Rocklin who created the library

00:06:22,300 --> 00:06:27,120
just calling out the different use cases

00:06:25,000 --> 00:06:30,430
here where if you're trying to do

00:06:27,120 --> 00:06:33,070
computational graphs you might be

00:06:30,430 --> 00:06:34,960
considering celery or airflow or maybe

00:06:33,070 --> 00:06:37,750
you're dealing with big data issues like

00:06:34,960 --> 00:06:46,510
using Hadoop or spark all of those tasks

00:06:37,750 --> 00:06:47,919
can essentially work in those cases so

00:06:46,510 --> 00:06:49,719
when you're looking at the use cases for

00:06:47,919 --> 00:06:51,130
the library it's important as you look

00:06:49,719 --> 00:06:52,599
at how people are using it there's two

00:06:51,130 --> 00:06:54,159
main cases one Big Data

00:06:52,599 --> 00:06:56,409
you're working on large arrays where

00:06:54,159 --> 00:06:58,900
maybe you're previously I'd work for the

00:06:56,409 --> 00:07:01,750
database or with spark desk has an array

00:06:58,900 --> 00:07:03,310
in a data frame and a desk bag api's and

00:07:01,750 --> 00:07:05,560
then in our case we're much more of the

00:07:03,310 --> 00:07:07,050
custom task scheduling where previously

00:07:05,560 --> 00:07:13,300
maybe you're using air flow or celery

00:07:07,050 --> 00:07:14,320
that is also a use case for desk so if

00:07:13,300 --> 00:07:16,060
you want to get start with desk right

00:07:14,320 --> 00:07:17,980
now you can do a pip install desk or my

00:07:16,060 --> 00:07:20,350
favorite Conda install it

00:07:17,980 --> 00:07:21,610
Python only it's going to be using a lot

00:07:20,350 --> 00:07:24,160
of libraries that you probably already

00:07:21,610 --> 00:07:28,000
have a pretty small footprint which is

00:07:24,160 --> 00:07:30,040
nice so as an example say you're wanting

00:07:28,000 --> 00:07:32,500
to parallel lies a function run across

00:07:30,040 --> 00:07:34,480
either distributed on one machine or

00:07:32,500 --> 00:07:36,280
many machines you have some arbitrary

00:07:34,480 --> 00:07:37,990
function in our case this would be

00:07:36,280 --> 00:07:41,500
training a model take some inputs

00:07:37,990 --> 00:07:45,280
doesn't work spit some outputs out you

00:07:41,500 --> 00:07:48,400
can take that function and you can use

00:07:45,280 --> 00:07:50,980
the desk distributed components where

00:07:48,400 --> 00:07:52,780
you're grabbing a client the client will

00:07:50,980 --> 00:07:55,150
either connect to a local cluster or

00:07:52,780 --> 00:07:57,550
distributed cluster and we're calling

00:07:55,150 --> 00:08:00,640
client dot submit which mirrors the

00:07:57,550 --> 00:08:02,170
concurrent futures API and you pass it

00:08:00,640 --> 00:08:05,020
the function you want to execute the

00:08:02,170 --> 00:08:06,790
input parameters and in our case the

00:08:05,020 --> 00:08:08,500
functions aren't pure but if your

00:08:06,790 --> 00:08:11,620
functions were a pure and that your

00:08:08,500 --> 00:08:13,960
inputs determine your outputs casual

00:08:11,620 --> 00:08:16,270
tasks will aggressively cache those for

00:08:13,960 --> 00:08:17,830
you if you would like and we are

00:08:16,270 --> 00:08:19,510
appending these into a list of futures

00:08:17,830 --> 00:08:22,030
and then we're using this as completed

00:08:19,510 --> 00:08:23,950
API to wait in a for loop and as the

00:08:22,030 --> 00:08:25,960
work finishes in whatever order it gets

00:08:23,950 --> 00:08:27,610
returned to you you can pull the results

00:08:25,960 --> 00:08:29,260
and if there's any exceptions that

00:08:27,610 --> 00:08:31,330
happened in your function you can access

00:08:29,260 --> 00:08:35,770
those just like you would expect with

00:08:31,330 --> 00:08:37,060
the try accept and so again one of the

00:08:35,770 --> 00:08:38,890
design goals is to make sure that we

00:08:37,060 --> 00:08:40,979
could write software that looked the

00:08:38,890 --> 00:08:44,880
same whether it was local or distributed

00:08:40,979 --> 00:08:47,470
so if you're running locally you can use

00:08:44,880 --> 00:08:49,870
code just like this where you declare a

00:08:47,470 --> 00:08:51,850
local cluster you can choose whether you

00:08:49,870 --> 00:08:54,460
want to run threads or processes tasks

00:08:51,850 --> 00:08:56,140
has an extensive page describing the

00:08:54,460 --> 00:08:58,630
cases where you would choose one or the

00:08:56,140 --> 00:09:00,490
other you can control the number of

00:08:58,630 --> 00:09:02,320
workers dynamically so you have the

00:09:00,490 --> 00:09:04,680
cluster dot scale down here we can add

00:09:02,320 --> 00:09:07,600
or remove workers as needed and then

00:09:04,680 --> 00:09:09,010
desk also does some memory checking so

00:09:07,600 --> 00:09:11,290
if a worker gets out of hand with its

00:09:09,010 --> 00:09:17,410
memory usage it'll kill it and restart

00:09:11,290 --> 00:09:19,900
it if you so desire and it gives you a

00:09:17,410 --> 00:09:21,310
nice UI this is only part of it it shows

00:09:19,900 --> 00:09:23,590
all the workers the amount of memory

00:09:21,310 --> 00:09:26,770
consumed by each worker the active tasks

00:09:23,590 --> 00:09:28,150
if you go to another tab in here you can

00:09:26,770 --> 00:09:30,040
look at the stack traces of all your

00:09:28,150 --> 00:09:31,329
workers to see you what they're doing at

00:09:30,040 --> 00:09:35,499
any given time whether there's

00:09:31,329 --> 00:09:36,970
duck and progress for all the tasks

00:09:35,499 --> 00:09:43,389
whether you had errors and the memory

00:09:36,970 --> 00:09:46,119
usage so then in our particular case we

00:09:43,389 --> 00:09:48,850
have access to a dupe cluster using

00:09:46,119 --> 00:09:51,220
elastic MapReduce this works out of the

00:09:48,850 --> 00:09:53,980
boss box with desk you can log into your

00:09:51,220 --> 00:09:56,259
EMR cluster and pip install tasks and

00:09:53,980 --> 00:09:59,589
create a distributed cluster it will

00:09:56,259 --> 00:10:01,920
take advantage of yarn which is part of

00:09:59,589 --> 00:10:04,019
the Hadoop ecosystem which creates

00:10:01,920 --> 00:10:09,790
containers with a certain amount of

00:10:04,019 --> 00:10:12,399
memory and compute cores and will also

00:10:09,790 --> 00:10:14,920
take advantage of HDFS for distributing

00:10:12,399 --> 00:10:16,779
files to those workers that's also how

00:10:14,920 --> 00:10:18,459
the Python environments get distributed

00:10:16,779 --> 00:10:20,949
to each worker so you don't have to

00:10:18,459 --> 00:10:22,839
worry about configuring each box with a

00:10:20,949 --> 00:10:24,339
specific Python environment you could

00:10:22,839 --> 00:10:27,220
tell desk hey this virtual environment

00:10:24,339 --> 00:10:29,049
that I'm in with the master node zip it

00:10:27,220 --> 00:10:31,559
up send it to all the workers so you

00:10:29,049 --> 00:10:35,139
have identical environments which is

00:10:31,559 --> 00:10:38,769
wonderful in my opinion if you don't

00:10:35,139 --> 00:10:40,569
have a view cluster and that's fine you

00:10:38,769 --> 00:10:43,269
can try kubernetes if you don't have

00:10:40,569 --> 00:10:45,339
kubernetes tasks has a whole deployment

00:10:43,269 --> 00:10:47,589
model using SSH so any box you can

00:10:45,339 --> 00:10:49,660
associate SSH to give it an IP address

00:10:47,589 --> 00:10:51,100
make sure the ports are open it will let

00:10:49,660 --> 00:10:53,199
you instantiate workers on all those

00:10:51,100 --> 00:10:55,660
boxes and they will happily compute and

00:10:53,199 --> 00:10:58,329
return results and there's Google

00:10:55,660 --> 00:11:00,129
compute system and essentially every

00:10:58,329 --> 00:11:07,059
cloud distribution has nice

00:11:00,129 --> 00:11:09,339
out-of-the-box use for exact task okay

00:11:07,059 --> 00:11:11,410
so in our case we're deploying on yarn I

00:11:09,339 --> 00:11:12,850
put all of this on this slide that gives

00:11:11,410 --> 00:11:14,439
us I spent a long time trying to figure

00:11:12,850 --> 00:11:17,230
it out so it may be useful to you

00:11:14,439 --> 00:11:20,350
you can hunt up the slides later but

00:11:17,230 --> 00:11:23,739
we're describing the workers that yarn

00:11:20,350 --> 00:11:25,929
will be creating on our cluster we're

00:11:23,739 --> 00:11:27,179
saying up at the top we have some number

00:11:25,929 --> 00:11:29,610
of workers

00:11:27,179 --> 00:11:32,110
here's to how they restart behavior

00:11:29,610 --> 00:11:34,419
here's how much memory and virtual cores

00:11:32,110 --> 00:11:36,850
we would like for them to have this

00:11:34,419 --> 00:11:38,679
files declaration says hey maybe there's

00:11:36,850 --> 00:11:40,389
a file that I want you to send to each

00:11:38,679 --> 00:11:42,549
of my workers in our case we use that

00:11:40,389 --> 00:11:43,959
for distributing a data cache so we have

00:11:42,549 --> 00:11:44,980
a sequel Lite database where we're

00:11:43,959 --> 00:11:46,899
caching data

00:11:44,980 --> 00:11:48,750
and when a worker starts we say hey desk

00:11:46,899 --> 00:11:51,600
make sure this file is on this worker

00:11:48,750 --> 00:11:53,889
before it starts then you can also

00:11:51,600 --> 00:11:56,440
declare which environment variables you

00:11:53,889 --> 00:11:58,570
want in each of your workers and in the

00:11:56,440 --> 00:12:01,170
bottom here it says hey make sure my

00:11:58,570 --> 00:12:03,550
scheduler starts before all my workers

00:12:01,170 --> 00:12:05,470
then you can just similarly describe a

00:12:03,550 --> 00:12:07,690
scheduler you can pack it together into

00:12:05,470 --> 00:12:10,570
this application spec and that spec

00:12:07,690 --> 00:12:12,190
object is what you send into here create

00:12:10,570 --> 00:12:14,850
cluster command and it will

00:12:12,190 --> 00:12:18,010
automatically distribute those workers

00:12:14,850 --> 00:12:19,959
across yarn and let you do something

00:12:18,010 --> 00:12:22,060
like this so this is pretty much

00:12:19,959 --> 00:12:24,910
verbatim how we're using it whether

00:12:22,060 --> 00:12:26,500
we're distributed on EMR with tens or

00:12:24,910 --> 00:12:28,449
hundreds of nodes or running it straight

00:12:26,500 --> 00:12:30,550
from our laptops where in this case

00:12:28,449 --> 00:12:33,610
we're iterating through some groups of

00:12:30,550 --> 00:12:35,829
regions or calling submit on this

00:12:33,610 --> 00:12:38,529
forecast function passing some variables

00:12:35,829 --> 00:12:40,660
into it then we're waiting till they

00:12:38,529 --> 00:12:42,699
complete their work we're grabbing their

00:12:40,660 --> 00:12:44,350
results and processing it in the

00:12:42,699 --> 00:12:48,820
exceptions the models may have had

00:12:44,350 --> 00:12:52,600
during training and that works locally

00:12:48,820 --> 00:12:55,029
or distributed a few other nice tab

00:12:52,600 --> 00:12:56,740
things it's nice and your log so go

00:12:55,029 --> 00:12:58,990
ahead and plan up printout your cluster

00:12:56,740 --> 00:13:00,880
user interface we do that when the

00:12:58,990 --> 00:13:03,699
logging starts you can pop that open and

00:13:00,880 --> 00:13:05,199
I'll have a shot showing that you can

00:13:03,699 --> 00:13:08,139
see all the workers their current logs

00:13:05,199 --> 00:13:10,240
and statuses also nice to have say you

00:13:08,139 --> 00:13:12,519
want to do some work on your workers

00:13:10,240 --> 00:13:14,589
before they are accepting function calls

00:13:12,519 --> 00:13:17,620
in our case we're using this to set up

00:13:14,589 --> 00:13:19,959
Python logging formats you can give a

00:13:17,620 --> 00:13:22,300
worker callback a function that you want

00:13:19,959 --> 00:13:24,459
run on each worker as it starts

00:13:22,300 --> 00:13:26,649
similarly I believe there's a shot-down

00:13:24,459 --> 00:13:30,459
callback you can use for doing any

00:13:26,649 --> 00:13:33,819
cleanup work you might want in our case

00:13:30,459 --> 00:13:36,370
yarn handles the logging so any standard

00:13:33,819 --> 00:13:38,560
error or standard output from the Python

00:13:36,370 --> 00:13:40,029
functions will be gathered together and

00:13:38,560 --> 00:13:43,209
sent into Hadoop's

00:13:40,029 --> 00:13:44,440
standard logging there are other

00:13:43,209 --> 00:13:48,190
recommendations if you're using

00:13:44,440 --> 00:13:52,569
different deployment methods and ask has

00:13:48,190 --> 00:13:53,890
decent documentation about that there's

00:13:52,569 --> 00:13:55,990
a shot of what it looks like when it's

00:13:53,890 --> 00:13:57,550
deployed on the cluster showing all the

00:13:55,990 --> 00:13:58,160
workers and you can drill into each one

00:13:57,550 --> 00:14:01,790
and look at their

00:13:58,160 --> 00:14:04,550
individual logs finally we found it very

00:14:01,790 --> 00:14:06,290
helpful to write some light wrappers

00:14:04,550 --> 00:14:08,779
i've included those in the appendix here

00:14:06,290 --> 00:14:10,790
so that if you want to completely

00:14:08,779 --> 00:14:12,410
disable tasks you can do so with an

00:14:10,790 --> 00:14:14,899
environment variable and it will run in

00:14:12,410 --> 00:14:17,329
serial mode which makes debugging much

00:14:14,899 --> 00:14:19,819
easier those are just some simple light

00:14:17,329 --> 00:14:25,279
wrappers around the cluster and as

00:14:19,819 --> 00:14:27,379
completed functions for those of you

00:14:25,279 --> 00:14:30,980
that are using more machine learning

00:14:27,379 --> 00:14:35,149
tasks just a couple of notes here there

00:14:30,980 --> 00:14:37,490
is deep integration what scikit-learn so

00:14:35,149 --> 00:14:39,680
that you could wrap any of your

00:14:37,490 --> 00:14:42,379
scikit-learn tasks with this parallel

00:14:39,680 --> 00:14:44,269
back-end using tasks and it will take

00:14:42,379 --> 00:14:45,620
over job Libs functions so any models

00:14:44,269 --> 00:14:48,589
you might use where you have in cores

00:14:45,620 --> 00:14:51,680
equals it will distribute those two task

00:14:48,589 --> 00:14:53,329
workers additionally any functions to

00:14:51,680 --> 00:14:54,949
have a partial fit can also take

00:14:53,329 --> 00:14:58,009
advantage of being distributed to desk

00:14:54,949 --> 00:15:02,149
workers and of using say extra boost or

00:14:58,009 --> 00:15:04,819
tensorflow it will intelligently break

00:15:02,149 --> 00:15:06,589
apart your data and hand it in to XG

00:15:04,819 --> 00:15:10,910
boost and tensors flows existing

00:15:06,589 --> 00:15:12,709
distributed deployment models and then

00:15:10,910 --> 00:15:16,160
if you're working in areas where you

00:15:12,709 --> 00:15:19,759
have large amounts of data desk

00:15:16,160 --> 00:15:24,019
essentially allows you to work just like

00:15:19,759 --> 00:15:27,199
you would with pandas using their data

00:15:24,019 --> 00:15:29,209
frames which is essentially pandas with

00:15:27,199 --> 00:15:31,630
some desk magic on top it breaks apart

00:15:29,209 --> 00:15:33,949
the pandas objects across the workers

00:15:31,630 --> 00:15:35,959
same thing with numpy arrays it will

00:15:33,949 --> 00:15:37,519
break apart numpy arrays and manage

00:15:35,959 --> 00:15:39,170
those intelligently across the workers

00:15:37,519 --> 00:15:42,079
and if you will have unstructured data

00:15:39,170 --> 00:15:44,329
there's a desk bag api which looks

00:15:42,079 --> 00:15:50,750
functionally equivalent to how you might

00:15:44,329 --> 00:15:54,250
work with r DD and pi spark so the

00:15:50,750 --> 00:15:56,689
takeaways it worked well for us

00:15:54,250 --> 00:15:58,610
forecasting is able to scale with our

00:15:56,689 --> 00:16:01,490
number of compute nodes on our cluster

00:15:58,610 --> 00:16:03,800
so we can arbitrarily choose how fast we

00:16:01,490 --> 00:16:06,559
want to run it I can do back tests in

00:16:03,800 --> 00:16:09,649
much shorter time even on a single node

00:16:06,559 --> 00:16:10,579
operation we improve things by about 50%

00:16:09,649 --> 00:16:13,339
in terms of the

00:16:10,579 --> 00:16:17,209
I'm just buying able to distribute some

00:16:13,339 --> 00:16:19,189
of the training if you are doing any

00:16:17,209 --> 00:16:21,259
sort of distributed compute I would

00:16:19,189 --> 00:16:23,089
recommend taking a look at desk at least

00:16:21,259 --> 00:16:24,829
it may not solve your problem but I

00:16:23,089 --> 00:16:28,459
think it's a good first place to try

00:16:24,829 --> 00:16:30,529
rather than some more complex things if

00:16:28,459 --> 00:16:32,720
you're using yarn it does complicate

00:16:30,529 --> 00:16:35,629
matters most of my time was spent

00:16:32,720 --> 00:16:37,279
figuring out darn I don't know the

00:16:35,629 --> 00:16:39,319
kubernetes would make that easier for

00:16:37,279 --> 00:16:42,589
you but something to keep in mind if

00:16:39,319 --> 00:16:45,799
you're estimating effort the desk

00:16:42,589 --> 00:16:48,649
website has great documentation it's a

00:16:45,799 --> 00:16:50,540
fun read just the way they write they

00:16:48,649 --> 00:16:52,129
have a nice page comparing gas other

00:16:50,540 --> 00:16:54,170
solutions that's worth reading through

00:16:52,129 --> 00:16:57,439
to get a sense of how they think about

00:16:54,170 --> 00:16:59,269
problems and as I mentioned you can get

00:16:57,439 --> 00:17:00,860
on Stack Overflow and get an answer from

00:16:59,269 --> 00:17:04,429
any of the maintainer 'he's usually

00:17:00,860 --> 00:17:06,110
within the day and also to know it's a

00:17:04,429 --> 00:17:08,179
complex library there's a lot of

00:17:06,110 --> 00:17:10,039
different ways you can use it and so if

00:17:08,179 --> 00:17:13,760
you look at how people are using a keep

00:17:10,039 --> 00:17:17,720
in mind that the vast ways you can use

00:17:13,760 --> 00:17:20,389
it make it non obvious sometimes whether

00:17:17,720 --> 00:17:23,149
it will suit your use case so something

00:17:20,389 --> 00:17:24,709
you got to get in and play with finally

00:17:23,149 --> 00:17:29,649
grub pubs hiring do you want to come

00:17:24,709 --> 00:17:32,600
work with me or not with me let me know

00:17:29,649 --> 00:17:35,029
so thank you I will take questions but

00:17:32,600 --> 00:17:37,110
I'll do it offline so the rest of you

00:17:35,029 --> 00:17:41,810
can go home

00:17:37,110 --> 00:17:55,109
[Applause]

00:17:41,810 --> 00:17:55,109

YouTube URL: https://www.youtube.com/watch?v=Q7XyGfS84l0


