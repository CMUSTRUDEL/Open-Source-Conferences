Title: Grant Jenks - Python Sorted Collections - PyCon 2016
Publication date: 2016-06-01
Playlist: PyCon 2016
Description: 
	Speaker: Grant Jenks

C++, Java, and .NET provide sorted collections types. Wish Python did too? Look around and you'll find Pandas DataFrame indexes, Sqlite in-memory databases, even redis-py sorted set commands. The SortedContainers module was designed to fill this gap with sorted list, dict and set implementations. It's written in pure-Python but generally faster than C-extension modules. Come see how it works.

Slides can be found at: https://speakerdeck.com/pycon2016 and https://github.com/PyCon/2016-slides
Captions: 
	00:00:00,420 --> 00:00:02,620
(presenter) Okay everybody, welcome.

00:00:02,620 --> 00:00:05,820
If your brains are not already overstuffed,

00:00:05,820 --> 00:00:07,860
we have Grant Jenks here to teach you

00:00:07,860 --> 00:00:10,640
all about Python sorted collections. Take it away.

00:00:10,640 --> 00:00:12,760
(Grant Jenks) Thanks.

00:00:12,760 --> 00:00:18,120
[applause]

00:00:18,600 --> 00:00:21,860
So I'm here today to talk about Python sorted collections.

00:00:21,860 --> 00:00:26,300
I'm excited and a bit nervous to be up here in front of all you smart people,

00:00:26,300 --> 00:00:30,600
but I'm a pretty smart guy myself, and I really admire Python.

00:00:30,600 --> 00:00:32,600
So let's get started.

00:00:32,600 --> 00:00:37,400
Every time I talk about sorted collections, my wife hears "sordid".

00:00:37,400 --> 00:00:41,620
We'll see today how many times I can confuse the on-site captioners.

00:00:41,620 --> 00:00:45,940
You're probably more familiar with sorted collections than you realize.

00:00:45,940 --> 00:00:50,059
Let me make a short argument for sorted collection types.

00:00:50,060 --> 00:00:55,540
In the standard library, we have heapq, bisect and queue.PriorityQueue,

00:00:55,540 --> 00:00:57,860
but none of those quite fill the gap.

00:00:57,860 --> 00:01:02,840
Behind the scenes, PriorityQueue actually uses a heap implementation.

00:01:02,840 --> 00:01:07,219
And another common mistake is to think that collections.ordereddict

00:01:07,219 --> 00:01:12,229
is a dictionary that maintains sort order, but that's not the case.

00:01:12,229 --> 00:01:15,820
I don't always import sorted types, but when I do,

00:01:15,820 --> 00:01:18,720
I expect them in the standard library.

00:01:18,720 --> 00:01:23,480
And here's why: Java, C++, and .NET have them.

00:01:23,480 --> 00:01:27,719
Python has broken into the top five of the TIOBE index

00:01:27,720 --> 00:01:33,360
but feels a bit more like PHP or JavaScript in this regard.

00:01:33,360 --> 00:01:39,140
We also depend on external solutions, so we have SQLite in memory indexes,

00:01:39,140 --> 00:01:43,960
we have pandas.dataframe indexes, and redis sorted sets.

00:01:43,960 --> 00:01:46,980
If you've ever issued a ZADD command to redis,

00:01:46,980 --> 00:01:50,420
then you've used a sorted collection.

00:01:50,420 --> 00:01:55,860
So what should the API of sorted collection types be in Python?

00:01:55,860 --> 00:01:58,880
Well, I think a sorted list should be a mutable sequence.

00:01:58,880 --> 00:02:01,280
Pretty close to the list API.

00:02:01,280 --> 00:02:03,700
There should also be a sort order constraint

00:02:03,700 --> 00:02:08,240
that must be satisfied by the setitem and insert methods.

00:02:08,240 --> 00:02:10,399
I think it should also support a key argument

00:02:10,399 --> 00:02:13,890
like the sorted built-in function, and given sorted order,

00:02:13,890 --> 00:02:18,560
it seems right to add functions like bisect_right and bisect_left.

00:02:18,560 --> 00:02:22,800
You can also imagine an add method and a discard method for elements,

00:02:22,800 --> 00:02:25,640
kind of like a multiset in other languages.

00:02:25,640 --> 00:02:29,680
I'd also expect getitem, contains, count, etc.,

00:02:29,680 --> 00:02:31,620
to be faster than linear time, right?

00:02:31,620 --> 00:02:35,950
We should take advantage of the fact that our data is sorted.

00:02:35,950 --> 00:02:39,260
A sorted dictionary should be a mutablemapping,

00:02:39,260 --> 00:02:41,940
probably pretty close to the dictionary API.

00:02:41,940 --> 00:02:46,300
But when we iterate a sorted dictionary, we should yield items in sorted order.

00:02:46,300 --> 00:02:49,600
It should also support efficient positional indexing.

00:02:49,600 --> 00:02:52,380
So while we have a KeysView and an ItemsView

00:02:52,380 --> 00:02:55,220
and a ValuesView for dictionary,

00:02:55,220 --> 00:02:59,840
I think a sorted dictionary would also add something like a sequence view.

00:03:00,880 --> 00:03:03,560
Again, SortedSet, a lot like a MutableSet.

00:03:03,560 --> 00:03:08,000
This is just building off the collections.abc module,

00:03:08,000 --> 00:03:10,680
so these are abstract base classes.

00:03:10,689 --> 00:03:13,520
If you're not familiar, it's pretty close to the set API

00:03:13,520 --> 00:03:16,820
and a SortedSet should also kind of behave like a sequence.

00:03:16,820 --> 00:03:22,300
So something like the tuple API to support efficient positional indexing.

00:03:22,860 --> 00:03:25,960
The chorus and the refrain from core developers is,

00:03:25,960 --> 00:03:30,300
"Look to the PyPI!" and that's pretty good advice.

00:03:30,300 --> 00:03:32,220
So let's talk about your options

00:03:32,220 --> 00:03:35,880
and do a bit of software archaeology in the process.

00:03:36,360 --> 00:03:39,300
blist is the genesis of our story,

00:03:39,300 --> 00:03:41,780
and it wasn't really designed for sorted collections.

00:03:41,780 --> 00:03:43,600
but that's what we have.

00:03:43,600 --> 00:03:46,660
blist is written in C and the innovation here

00:03:46,660 --> 00:03:50,320
is really something called the blist data type.

00:03:50,320 --> 00:03:55,420
blist is a B-tree implementation for CPython's built-in list.

00:03:55,420 --> 00:03:58,820
The sorted list, sorted dictionary and sorted set

00:03:58,820 --> 00:04:02,500
were built on top of this blist data type,

00:04:02,500 --> 00:04:05,060
and it became the incumbent to beat.

00:04:05,060 --> 00:04:09,860
Also noteworthy is that the API was rather well thought out.

00:04:09,860 --> 00:04:11,740
There were some quirks, however.

00:04:11,740 --> 00:04:15,260
For example, the pop method of a sorted list in blist

00:04:15,260 --> 00:04:17,940
actually returns the first element in the list,

00:04:17,940 --> 00:04:21,400
rather than the last element, as we're used to.

00:04:22,080 --> 00:04:24,560
Next up is SortedCollection.

00:04:24,560 --> 00:04:27,020
This shows up in about 2010.

00:04:27,020 --> 00:04:29,980
This is actually a recipe from Raymond Hettinger,

00:04:29,980 --> 00:04:32,340
one of the core developers of Python.

00:04:32,340 --> 00:04:34,200
And I include it in large part

00:04:34,200 --> 00:04:36,530
because it's linked from the Python docs themselves.

00:04:36,530 --> 00:04:40,220
So, this is, in many ways, the recommended solution.

00:04:40,220 --> 00:04:42,100
There's a couple innovations here.

00:04:42,100 --> 00:04:44,000
The first is: it's simple.

00:04:44,000 --> 00:04:46,060
It's written in pure Python.

00:04:46,060 --> 00:04:47,940
And one of the other things he does

00:04:47,940 --> 00:04:50,660
is he maintains a parallel list of keys.

00:04:50,670 --> 00:04:52,780
So there's a key function support.

00:04:52,780 --> 00:04:54,640
We have a parallel list of keys

00:04:54,640 --> 00:04:58,820
for looking up, say, an item by its key.

00:04:59,560 --> 00:05:01,420
This is bintrees.

00:05:01,420 --> 00:05:03,320
It's still alive and kicking today.

00:05:03,320 --> 00:05:05,310
A few innovations, again, here.

00:05:05,310 --> 00:05:08,700
It's written with Cython support to improve performance,

00:05:08,700 --> 00:05:11,780
and it has a few different tree backends.

00:05:11,780 --> 00:05:16,160
You can create a red-black or AVL tree, depending on your needs.

00:05:16,160 --> 00:05:19,160
There's also some notion of accessing the nodes themselves

00:05:19,170 --> 00:05:21,570
and customizing the tree traversal

00:05:21,570 --> 00:05:25,840
to slice by value rather than by index.

00:05:26,680 --> 00:05:31,240
Banyan had a very short life, but adds another couple innovations.

00:05:31,240 --> 00:05:33,720
It's incredibly fast.

00:05:33,720 --> 00:05:37,580
It achieves that through C++ template metaprogramming,

00:05:37,580 --> 00:05:40,540
so it's maybe not the best candidate for CPython.

00:05:40,540 --> 00:05:44,620
But it has also this feature called "tree augmentation"

00:05:44,620 --> 00:05:47,140
that will let you store metadata at tree nodes,

00:05:47,140 --> 00:05:49,100
so you can actually use banyan

00:05:49,100 --> 00:05:52,200
to implement interval trees, if you need that.

00:05:52,760 --> 00:05:55,900
Finally there is skiplistcollections.

00:05:55,900 --> 00:05:57,900
Couple significant things here.

00:05:57,900 --> 00:06:00,200
It's pure Python, so that's good.

00:06:00,200 --> 00:06:03,420
It's fast, even for large collections.

00:06:03,420 --> 00:06:08,800
So where SortedCollection from Raymond slows down around 10,000 elements,

00:06:08,800 --> 00:06:11,220
skiplistcollection will perform reasonably well

00:06:11,220 --> 00:06:14,340
up to millions of elements. And I think it's also significant

00:06:14,340 --> 00:06:16,540
that this is not a tree-based implementation.

00:06:16,540 --> 00:06:21,660
This is actually a skip list data type rather than a binary tree.

00:06:22,260 --> 00:06:26,660
All together, this is the impression you get from PyPI.

00:06:26,660 --> 00:06:30,700
You try and figure it out and it really -- it kind of looks like a mess.

00:06:31,100 --> 00:06:36,540
PyPI, I think, has got to work better than using Google with the site operator,

00:06:36,540 --> 00:06:39,460
because that's the position I often find myself in.

00:06:39,460 --> 00:06:42,740
There is a couple others worth calling out here.

00:06:42,740 --> 00:06:44,610
Let's see if this works.

00:06:44,610 --> 00:06:46,550
So there's rbtree,

00:06:46,550 --> 00:06:51,020
that's also a very fast sorted dictionary implementation.

00:06:51,020 --> 00:06:54,000
You can see a couple here, treap.

00:06:54,000 --> 00:06:56,600
I don't know if you can read it, but there's a splay,

00:06:56,600 --> 00:06:58,640
there's a scapegoat tree.

00:06:58,640 --> 00:07:01,540
Those are contributions by Dan Stromberg.

00:07:01,540 --> 00:07:03,500
He's done a number of experiments

00:07:03,500 --> 00:07:08,080
to see if you can find, kind of, the best tree implementation.

00:07:08,080 --> 00:07:11,480
Turns out there is no silver bullet when it comes to trees,

00:07:11,480 --> 00:07:15,400
so it's always going to depend a little bit on your use case.

00:07:15,400 --> 00:07:19,360
I love Python because there's one right way to do things.

00:07:19,360 --> 00:07:23,760
If I just wanted sorted types, what's the right answer?

00:07:24,440 --> 00:07:27,640
I couldn't find the right answer, so I built it:

00:07:27,640 --> 00:07:31,840
The missing battery: SortedContainers.

00:07:31,840 --> 00:07:34,760
Here it is. This is the project homepage.

00:07:34,760 --> 00:07:38,020
If you go to Google and type in "SortedContainers" right now,

00:07:38,020 --> 00:07:40,120
you'll probably get a PyPI reference.

00:07:40,120 --> 00:07:43,460
Scroll down, you'll find a link to the project homepage.

00:07:43,460 --> 00:07:48,040
SortedContainers is a Python-sorted collections library with SortedList,

00:07:48,040 --> 00:07:51,240
sorted dictionary, and sorted set implementations.

00:07:51,240 --> 00:07:54,560
It's pure Python, but it's as fast as C extensions.

00:07:54,560 --> 00:07:57,380
It's Python 2- and Python 3-compatible.

00:07:57,380 --> 00:08:00,540
It's fully featured and it's extensively tested

00:08:00,540 --> 00:08:04,400
with 100% coverage and hours of stress.

00:08:05,280 --> 00:08:07,400
Performance is a feature.

00:08:07,400 --> 00:08:10,340
That means graphs. Lots of them.

00:08:10,340 --> 00:08:13,860
There are a 189 performance graphs in total.

00:08:13,860 --> 00:08:16,600
I want to look at a few of them together.

00:08:16,600 --> 00:08:21,340
Here's the performance of adding a random value to a SortedList.

00:08:21,340 --> 00:08:25,980
I'm comparing SortedContainers, here, with other competing implementations,

00:08:25,980 --> 00:08:28,320
so you'll see SortedList at the top.

00:08:28,320 --> 00:08:31,180
That's the SortedContainers implementation.

00:08:31,180 --> 00:08:35,620
Then there's also a SortedListWithKey provided by SortedContainers.

00:08:35,620 --> 00:08:39,540
Then there's blist, so blist has a couple of entries.

00:08:39,540 --> 00:08:42,480
And then there's SortedCollection from Raymond.

00:08:43,280 --> 00:08:46,540
Notice, also, the axes here are log/log.

00:08:46,540 --> 00:08:50,900
So if you have two major tick marks and difference --

00:08:50,900 --> 00:08:54,380
the difference between two of the lines is a major tick mark,

00:08:54,380 --> 00:08:58,820
then one is actually 10 times faster than the other.

00:08:58,820 --> 00:09:03,300
And we see here that SortedContainers is, in fact,

00:09:03,300 --> 00:09:05,960
about 10 times faster than blist

00:09:05,960 --> 00:09:10,100
when it comes to adding random values to a SortedList.

00:09:10,540 --> 00:09:15,380
Notice, also, Raymond's recipe, which is just a list.

00:09:15,380 --> 00:09:18,740
It displays this order n-squared behavior.

00:09:18,740 --> 00:09:22,620
So that's why we get it curving up and to the left.

00:09:23,320 --> 00:09:25,700
Of all the sorted collections libraries,

00:09:25,700 --> 00:09:29,640
SortedContainers is also the fastest at initialization,

00:09:29,640 --> 00:09:32,200
and we'll look at why, pretty soon.

00:09:33,340 --> 00:09:36,420
SortedContainers is not always the fastest,

00:09:36,420 --> 00:09:39,520
but notice here, the performance improves with scale.

00:09:39,520 --> 00:09:41,560
You can see it there in blue. So right here

00:09:41,560 --> 00:09:44,780
in the middle of the pack, we see SortedDict.

00:09:44,780 --> 00:09:48,320
This is deleting a key from a sorted dictionary.

00:09:48,320 --> 00:09:50,140
And with SortedDict,

00:09:50,140 --> 00:09:52,680
there's quite a few other competing implementations,

00:09:52,680 --> 00:09:57,140
so I'm including all of the other fastest ones I can find.

00:09:57,140 --> 00:10:00,540
As we scale upwards -- so we're going from a hundred elements

00:10:00,550 --> 00:10:04,220
up to a million elements, and we're adding a random value --

00:10:04,220 --> 00:10:06,399
SortedContainers actually improves.

00:10:06,399 --> 00:10:11,050
So the slope of our blue line is less than the slope of these --

00:10:11,050 --> 00:10:13,649
all of these are binary tree implementations

00:10:13,649 --> 00:10:16,980
with various balancing algorithms.

00:10:18,900 --> 00:10:24,080
In short, SortedContainers is kind of like a b-tree implementation.

00:10:24,080 --> 00:10:28,440
That means you can configure the fan-out of nodes in the tree.

00:10:28,440 --> 00:10:30,480
So we call that the "load parameter",

00:10:30,480 --> 00:10:32,480
and there are extensive performance graphs

00:10:32,480 --> 00:10:34,880
of three different load parameters.

00:10:34,880 --> 00:10:38,540
Here we see that a load factor of 10,000

00:10:38,540 --> 00:10:41,780
is fastest for indexing into a SortedList,

00:10:41,780 --> 00:10:45,180
so that's about twice as fast, there.

00:10:45,700 --> 00:10:49,200
Notice how the axes now go up to 10 million elements.

00:10:49,200 --> 00:10:54,560
So, many implementations can't handle that many but SortedContainers can.

00:10:54,560 --> 00:10:59,380
I've actually scaled SortedList all the way up to 10 billion elements.

00:10:59,380 --> 00:11:01,740
This was kind of an incredible experiment.

00:11:01,740 --> 00:11:04,819
I had to rent the largest high-memory instance available

00:11:04,820 --> 00:11:07,000
from Google compute engine.

00:11:07,000 --> 00:11:11,320
And that benchmark required about 128 gigabytes of memory,

00:11:11,320 --> 00:11:15,240
ran for 16 hours, and it cost me about $30.

00:11:17,360 --> 00:11:21,240
This is the performance of deleting a key from a dictionary.

00:11:21,249 --> 00:11:24,420
Notice, this time, that the smaller loads --

00:11:24,420 --> 00:11:27,660
a load of 10 or 100 -- is actually faster.

00:11:27,660 --> 00:11:29,540
The default is 1,000.

00:11:29,540 --> 00:11:31,899
You'll often find that's in the middle of the three,

00:11:31,900 --> 00:11:33,840
and it's a very same default.

00:11:33,840 --> 00:11:37,080
So if you're not used to breaking out your profiler

00:11:37,080 --> 00:11:39,600
or doing performance analysis,

00:11:39,600 --> 00:11:44,060
just leave that untouched, and it will work extremely fast.

00:11:45,260 --> 00:11:48,220
In addition to comparisons and load factors,

00:11:48,220 --> 00:11:50,340
I also benchmarked runtimes.

00:11:50,340 --> 00:11:56,040
Here's CPython 2.7, CPython 3.5, and PyPy Version 5.

00:11:56,040 --> 00:11:58,780
You can see where the just-in-time compiler,

00:11:58,780 --> 00:12:01,020
the JIT compiler, kicks in.

00:12:01,020 --> 00:12:03,760
There's some JIT magic right there.

00:12:03,760 --> 00:12:07,340
That will make SortedContainers another ten times faster.

00:12:07,340 --> 00:12:09,280
So because we're pure Python,

00:12:09,280 --> 00:12:12,060
we really scream when we have a Python JIT.

00:12:12,060 --> 00:12:17,280
I was really encouraged seeing Piston, Pyjion, PyPy.

00:12:17,280 --> 00:12:19,640
All of those are great implementations, I think,

00:12:19,640 --> 00:12:23,720
moving in this direction of including a JIT for Python.

00:12:24,400 --> 00:12:28,139
Finally, I made a survey in 2015 on GitHub

00:12:28,140 --> 00:12:31,260
as to how people were using sorted collections.

00:12:31,260 --> 00:12:34,120
I noticed patterns like priority queues,

00:12:34,120 --> 00:12:37,900
multisets, nearest neighbor algorithms, etc.

00:12:37,900 --> 00:12:40,280
This is the priority queue workload,

00:12:40,280 --> 00:12:44,220
which spends about 40% of its time adding elements,

00:12:44,220 --> 00:12:47,459
40% of its time popping elements out of the list,

00:12:47,460 --> 00:12:50,160
10% of its time discarding elements,

00:12:50,160 --> 00:12:53,020
and has a couple of other methods like iteration.

00:12:53,020 --> 00:12:55,180
In all of these workloads, SortedContainers

00:12:55,180 --> 00:12:59,960
is two to ten times faster than other implementations.

00:13:00,800 --> 00:13:02,700
We also have a lot of features.

00:13:02,700 --> 00:13:05,500
The API is nearly a drop-in replacement

00:13:05,500 --> 00:13:09,680
for the blist and rbtree modules which are quite popular.

00:13:09,680 --> 00:13:11,540
But the quirks have been fixed.

00:13:11,540 --> 00:13:14,040
So now the pop() method returns the last element

00:13:14,040 --> 00:13:15,980
as I think you would expect.

00:13:15,980 --> 00:13:18,780
sorted_lists are sorted, so you can bisect them.

00:13:18,780 --> 00:13:20,760
That's what happens in the second line here.

00:13:20,760 --> 00:13:24,060
Looking up the index of an element is very fast.

00:13:24,060 --> 00:13:27,380
bintrees introduced methods for tree traversal,

00:13:27,380 --> 00:13:30,720
and I have boiled those down to a couple of API methods.

00:13:30,720 --> 00:13:33,980
On Line 3, we see an irange method.

00:13:33,980 --> 00:13:39,040
Irange iterates all keys, from bob to eve, in sorted order.

00:13:39,040 --> 00:13:41,410
So this allows us to create an iterator

00:13:41,410 --> 00:13:45,100
to iterate by value rather than by index.

00:13:45,100 --> 00:13:49,060
Sorted_Dicts also have a sequence-like view called iloc.

00:13:49,060 --> 00:13:51,800
If you're coming from pandas, that should look familiar.

00:13:51,800 --> 00:13:53,820
It's "I-L-O-C".

00:13:53,820 --> 00:13:59,460
Line 4 creates a list of the five largest keys in the dictionary.

00:13:59,460 --> 00:14:01,300
So that's my sequence view.

00:14:01,300 --> 00:14:04,840
Similar to irange, there is an islice method.

00:14:04,840 --> 00:14:07,900
islice does positional index slicing.

00:14:07,900 --> 00:14:14,460
In Line 5, we create an iterator over the indexes 10 through 49, inclusive.

00:14:15,060 --> 00:14:17,240
One of the benefits of being pure Python

00:14:17,240 --> 00:14:19,160
is it's easy to hack on.

00:14:19,160 --> 00:14:23,180
Over the years, a few patterns have emerged and become recipes.

00:14:23,180 --> 00:14:25,480
All of these are available from PyPI

00:14:25,480 --> 00:14:29,260
with simply pip install SortedCollections.

00:14:30,420 --> 00:14:33,800
If all that didn't convince you that SortedContainers is great,

00:14:33,800 --> 00:14:36,760
then listen to what other smart people say about it.

00:14:36,760 --> 00:14:40,780
Alex Martelli says, "Good Stuff! ... I like the simple, effective

00:14:40,780 --> 00:14:43,820
"implementation idea of splitting the sorted containers

00:14:43,820 --> 00:14:47,980
"into smaller 'fragments' to avoid the order(N) insertion costs."

00:14:47,980 --> 00:14:51,280
Jeff Knupp writes, "That last part, 'fast as C-extensions,'

00:14:51,280 --> 00:14:56,000
"was difficult to believe. I would need some sort of performance comparison

00:14:56,000 --> 00:15:00,920
"to be convinced this is true. The author includes this in the docs. It is."

00:15:00,920 --> 00:15:04,780
Kevin Samuel says, "I'm quite amazed, not just by the code quality

00:15:04,780 --> 00:15:08,500
"(it's incredibly readable and has more comment than code, wow),

00:15:08,500 --> 00:15:11,620
"but the actual amount of work you put at stuff that is not code:

00:15:11,620 --> 00:15:15,620
"documentation, benchmarking, implementation explanations."

00:15:15,620 --> 00:15:18,620
"Even the git log is clean and the unit tests run

00:15:18,620 --> 00:15:22,060
"out of the box on Python 2 and 3."

00:15:22,680 --> 00:15:26,300
So if you're new to SortedCollections, I hope I've piqued your interest.

00:15:26,300 --> 00:15:28,320
Think about the achievement here.

00:15:28,320 --> 00:15:33,460
SortedContainers is pure Python, but as fast as C implementations.

00:15:33,460 --> 00:15:36,780
So together, I want to look under the hood of SortedContainers

00:15:36,780 --> 00:15:39,360
at what makes it so fast.

00:15:40,380 --> 00:15:43,740
It really comes down to bisect for the heavy lifting.

00:15:43,740 --> 00:15:46,980
So bisect is a module in the standard library

00:15:46,980 --> 00:15:49,780
that implements binary search on lists.

00:15:49,780 --> 00:15:52,880
There's also a handy method called insort

00:15:52,880 --> 00:15:57,460
that does binary search and insertion for us in one call.

00:15:57,460 --> 00:16:00,000
So if we had a value and a SortedList,

00:16:00,000 --> 00:16:03,140
we could simply insert that value into our SortedList

00:16:03,140 --> 00:16:06,100
by calling bisect.insort.

00:16:06,100 --> 00:16:08,020
There's really no magic here.

00:16:08,020 --> 00:16:11,200
It's just implemented in C, and it's part of the standard library.

00:16:11,200 --> 00:16:13,260
So you get bisect for free.

00:16:13,260 --> 00:16:16,820
This is going to do all the heavy lifting for us.

00:16:17,740 --> 00:16:22,680
Here's the basic structure: It's a list of sublists.

00:16:22,680 --> 00:16:28,440
So this is a SortedList, and it has the value 0 through 17 in it.

00:16:28,440 --> 00:16:32,600
There's a number variable, _lists, that points to these sublists,

00:16:32,600 --> 00:16:35,480
and each of those is maintained in sorted order.

00:16:35,480 --> 00:16:38,020
So if you know how B-tree works,

00:16:38,020 --> 00:16:40,500
you should start to see the B-tree pattern.

00:16:40,500 --> 00:16:45,980
You'll sometimes hear me refer to these as the top level list and its sublists.

00:16:45,980 --> 00:16:49,860
So the top level list being _list, and then its sublists

00:16:49,860 --> 00:16:53,040
being those that contain the actual elements.

00:16:53,040 --> 00:16:56,680
There's no need to wrap sublist in its own object.

00:16:56,680 --> 00:17:00,880
They're just lists. Simple is fast and efficient.

00:17:02,100 --> 00:17:07,220
In addition to the list of sublists, there's an index called the maxes index.

00:17:07,220 --> 00:17:11,540
That simply stores the maximum value in each sublist.

00:17:11,540 --> 00:17:15,260
Now, lists in CPython are simply arrays of pointers,

00:17:15,260 --> 00:17:17,840
so we're not adding much overhead with this index.

00:17:17,840 --> 00:17:22,940
So there's one pointer to the largest element in a sublist

00:17:22,940 --> 00:17:25,180
per thousand elements.

00:17:25,440 --> 00:17:27,460
So it's pretty low overhead.

00:17:27,460 --> 00:17:30,500
Let's walk through testing membership with contains,

00:17:30,500 --> 00:17:33,180
given these two elements.

00:17:33,180 --> 00:17:37,740
So if I wanted to know whether the number 14 is in my SortedList,

00:17:37,750 --> 00:17:41,460
I start by bisecting the maxes index.

00:17:41,460 --> 00:17:44,560
So if I bisect maxes, it'll give me the index

00:17:44,560 --> 00:17:48,600
where I should insert that element to maintain a SortedList.

00:17:48,600 --> 00:17:52,180
That will be down here in position 3.

00:17:52,700 --> 00:17:56,000
That position corresponds to this sublist,

00:17:56,000 --> 00:17:59,340
potentially containing the value I'm interested in.

00:17:59,340 --> 00:18:03,140
So then I again call bisect on this sublist,

00:18:03,140 --> 00:18:07,080
and if I bisect for the number 14, I'll get index number 1.

00:18:07,080 --> 00:18:09,380
And then I can simply do a comparison to say,

00:18:09,380 --> 00:18:13,200
"Is that index in fact the element I'm looking for?"

00:18:13,210 --> 00:18:16,820
So it's very quick to do a contains operation.

00:18:16,820 --> 00:18:21,310
We simply call bisect twice, and this has the log(n) behavior

00:18:21,310 --> 00:18:23,880
that we're accustomed to seeing.

00:18:23,880 --> 00:18:26,600
Let's also walk through adding an element.

00:18:26,600 --> 00:18:29,800
Let's add 5 to the SortedList.

00:18:29,800 --> 00:18:32,920
So once again, we'll bisect our maxes index.

00:18:32,920 --> 00:18:35,640
That will give us index 1 in the maxes index

00:18:35,640 --> 00:18:38,300
which corresponds to a sublist.

00:18:38,300 --> 00:18:42,960
We'll bisect that sublist, looking for element 5.

00:18:42,960 --> 00:18:45,640
That will again give us index 1, so it tells us,

00:18:45,640 --> 00:18:47,520
"You should insert here."

00:18:47,520 --> 00:18:50,360
And we'll use that bisect.insort method

00:18:50,360 --> 00:18:54,580
in order to insert the element into the SortedList.

00:18:55,480 --> 00:18:59,320
Now, if you're watching carefully, you should notice

00:18:59,320 --> 00:19:02,780
that these lists could get really large.

00:19:02,780 --> 00:19:05,560
That's where the load factor comes in.

00:19:05,560 --> 00:19:08,540
So, our default load factor is 1,000,

00:19:08,540 --> 00:19:13,640
meaning if we kept inserting elements into these sublists,

00:19:13,640 --> 00:19:16,880
when a sublist becomes twice the load factor in length --

00:19:16,880 --> 00:19:20,160
so if we grew these sublists to twice the load factor,

00:19:20,160 --> 00:19:22,060
like 2,000 elements --

00:19:22,060 --> 00:19:24,960
then we actually split the sublists in half,

00:19:24,960 --> 00:19:28,500
and we insert two sublists in its place.

00:19:29,100 --> 00:19:33,620
Similarly, if a sublist actually shrunk to 500 elements

00:19:33,620 --> 00:19:35,520
(so, half of the load),

00:19:35,520 --> 00:19:38,900
then we combine that sublist with its neighbor.

00:19:38,900 --> 00:19:43,420
So in that way, we're constantly keeping these sublists roughly balanced

00:19:43,420 --> 00:19:47,940
according to the load factor that was given during initialization.

00:19:49,960 --> 00:19:53,720
Now, numeric indexing is a little more complex.

00:19:53,720 --> 00:19:59,540
Numeric indexing uses a tree, packed densely into another list.

00:19:59,550 --> 00:20:02,050
So if you have experience with heaps,

00:20:02,050 --> 00:20:04,740
think about a heap structure for a second.

00:20:04,740 --> 00:20:09,440
I haven't seen this exact structure described in textbooks or research,

00:20:09,440 --> 00:20:12,620
so for the time being, I'd like to call it a Jenks Index.

00:20:12,620 --> 00:20:15,820
That's named after me; I'm a little vain.

00:20:16,820 --> 00:20:20,260
I'll also refer to it as the positional index.

00:20:20,260 --> 00:20:22,740
So let's build this together.

00:20:23,500 --> 00:20:27,540
Starting from our list of sublists,

00:20:27,540 --> 00:20:29,880
notice the lengths of each of these.

00:20:29,880 --> 00:20:32,130
So there's 4 elements in the first one,

00:20:32,130 --> 00:20:36,860
3 elements in the next, then 6 elements here, and 5 here.

00:20:36,860 --> 00:20:40,680
We're simply going to map the len built in (the length built in)

00:20:40,680 --> 00:20:45,300
over all of those sublists in order to create a list of lengths.

00:20:45,300 --> 00:20:48,040
So that's a very fast operation.

00:20:48,040 --> 00:20:52,640
Then we're going to calculate the pair_wise_sums of those lengths.

00:20:52,640 --> 00:20:56,820
So here we're simply adding 4 and 3 to be 7

00:20:56,820 --> 00:20:59,360
and 6 and 5 to be 11.

00:20:59,360 --> 00:21:03,240
We repeat this operation of calculating pair_wise_sums

00:21:03,240 --> 00:21:05,680
until we're left with only one element.

00:21:05,680 --> 00:21:08,700
So here, our original lengths was 4 elements,

00:21:08,700 --> 00:21:13,680
our next pair_wise_sums was 2 elements, then we're getting down to 1 element.

00:21:13,680 --> 00:21:17,020
In order to build the positional _index, we simply zip --

00:21:17,020 --> 00:21:20,240
we simply chain all of these together.

00:21:20,240 --> 00:21:25,440
So our final _index is 18, 7, 11, 4, 3, 6, 5.

00:21:26,000 --> 00:21:28,760
So we've taken something that's kind of like a tree

00:21:28,760 --> 00:21:31,480
and we've shoved it into a list,

00:21:31,480 --> 00:21:35,060
because we forced that tree to be very dense,

00:21:35,060 --> 00:21:38,000
just like with a heap data type.

00:21:38,000 --> 00:21:42,690
The offset is simply the length of all the elements

00:21:42,690 --> 00:21:45,980
up until the lengths list.

00:21:45,980 --> 00:21:49,840
So here, offset 3 counts 1, 2, 3 elements.

00:21:49,840 --> 00:21:53,040
And then here at _index 3 is the start of our lengths list.

00:21:53,040 --> 00:21:58,120
So that lengths list gets copied directly into the Jenks Index.

00:22:00,480 --> 00:22:03,940
Let's try an example with the positional index.

00:22:03,940 --> 00:22:06,000
Remember the positional index is a tree.

00:22:06,000 --> 00:22:09,580
So here, I've actually written it kind of in the shape of a tree.

00:22:09,580 --> 00:22:12,600
There's 18, the node 18 is at the top.

00:22:12,600 --> 00:22:15,360
It has two children 7 and 11,

00:22:15,360 --> 00:22:17,960
and then each of those nodes has two children,

00:22:17,960 --> 00:22:21,940
7 having 4 and 3, 11 having 6 and 5.

00:22:21,940 --> 00:22:26,500
If we want to look up index 8, we can traverse this tree

00:22:26,500 --> 00:22:28,800
to figure out how to do so.

00:22:28,800 --> 00:22:32,380
Starting at the root, which is node 18,

00:22:32,380 --> 00:22:35,700
we compare the index to the left child node.

00:22:35,700 --> 00:22:38,300
So we're going to compare 8 to 7.

00:22:38,300 --> 00:22:43,600
8 is greater than 7, so what we do is, we subtract 7 from 8

00:22:43,600 --> 00:22:46,910
which gives us, now, 1, and we move to the right child node.

00:22:46,910 --> 00:22:50,680
So in step 2 here, we're moving to node 11.

00:22:50,680 --> 00:22:53,840
We decremented the index by the left child node,

00:22:53,840 --> 00:22:56,440
and we're now at position 2 in our index.

00:22:56,440 --> 00:22:59,120
So there's 0, 1, 2.

00:22:59,610 --> 00:23:01,960
We're going to repeat this process.

00:23:01,960 --> 00:23:06,180
So, now with index 1, we again look

00:23:06,180 --> 00:23:08,860
at the left child node. This time it's 6.

00:23:08,860 --> 00:23:13,440
1 is less than 6, so we actually simply move to the left child node.

00:23:13,440 --> 00:23:16,980
And we can also detect, now, that we're at a leaf node.

00:23:17,940 --> 00:23:21,520
It's very easy to calculate these left and right child nodes.

00:23:21,520 --> 00:23:25,120
They're simply at n times two and n times 2 plus 1.

00:23:25,120 --> 00:23:28,360
So it's just like a heap structure, but this is not a heap.

00:23:28,360 --> 00:23:30,940
This is a positional index.

00:23:30,940 --> 00:23:35,400
When we terminate at 6,

00:23:35,400 --> 00:23:38,440
we're left with our index at 1,

00:23:38,440 --> 00:23:41,900
the position in our positional index at 5,

00:23:41,900 --> 00:23:46,820
and we can use that 5 to calculate the top-level list index.

00:23:46,820 --> 00:23:49,840
So we're going to do 5 minus that offset.

00:23:49,850 --> 00:23:52,560
So that's where offset comes back into play.

00:23:52,560 --> 00:23:56,620
If you were able to follow all that, you're left with two values.

00:23:56,620 --> 00:24:00,660
One is the top index which is 2,

00:24:00,660 --> 00:24:03,780
and one is the remaining index, which is 1.

00:24:03,780 --> 00:24:07,560
Remember, we are interested in position 8.

00:24:08,520 --> 00:24:11,000
So let's look. Did we get there?

00:24:12,320 --> 00:24:14,880
So, 2.

00:24:15,240 --> 00:24:18,600
So in the top level, we're at 0, 1, 2.

00:24:18,600 --> 00:24:22,600
And then we're at index 1 within that. So there's eight.

00:24:22,600 --> 00:24:26,360
So the positional index is a way for us to very efficiently

00:24:26,360 --> 00:24:30,280
look up, numerically, an element.

00:24:34,720 --> 00:24:37,100
This leads us to our first lesson.

00:24:37,100 --> 00:24:42,600
The builtin types are fast, like, really fast. They're C.

00:24:42,600 --> 00:24:48,480
The builtin types are C code, and they benefit from years of optimizations.

00:24:48,480 --> 00:24:52,080
So as much as possible, use these C data types.

00:24:52,080 --> 00:24:54,520
Use these builtin types.

00:24:55,160 --> 00:24:59,720
Okay, let's look at the __contains__ method for a SortedList.

00:24:59,720 --> 00:25:04,140
This is the majority of the code, honest. This is how simple it is.

00:25:04,140 --> 00:25:08,720
We bisect the maxes index for a position,

00:25:08,720 --> 00:25:12,620
then we bisect the sublist for another position,

00:25:12,620 --> 00:25:15,100
and then we test: Is the element we found

00:25:15,100 --> 00:25:18,240
equal to the element you gave us?

00:25:18,240 --> 00:25:21,340
How many lines of Python code executed?

00:25:21,920 --> 00:25:23,320
Four.

00:25:23,320 --> 00:25:26,020
How many lines of C code executed?

00:25:26,020 --> 00:25:28,040
Possibly hundreds.

00:25:28,040 --> 00:25:31,040
In this way, I'm not quite programming Python

00:25:31,040 --> 00:25:33,950
so much as I'm programming my interpreter.

00:25:33,950 --> 00:25:37,160
I'm programming against the optimized libraries

00:25:37,160 --> 00:25:41,040
that I've been given. And in that way, I'm writing Python code,

00:25:41,040 --> 00:25:43,800
but it's almost like I'm writing C.

00:25:44,640 --> 00:25:48,880
So here's the lesson: program your interpreter.

00:25:48,880 --> 00:25:51,760
These operations are incredibly fast.

00:25:51,760 --> 00:25:53,620
And again, they're implemented in C.

00:25:53,620 --> 00:25:58,000
This is why SortedContainers is as fast as C implementations.

00:25:58,000 --> 00:26:02,940
If you follow this rule, you will write C code in Python,

00:26:02,940 --> 00:26:06,280
which is much nicer than writing C code itself.

00:26:07,080 --> 00:26:09,060
Now let's talk about memory.

00:26:09,060 --> 00:26:12,180
This is very simplified, so my apologies to those

00:26:12,180 --> 00:26:15,000
who feel it's, maybe, grossly simplified.

00:26:15,000 --> 00:26:17,840
But notice this limited sizes

00:26:17,840 --> 00:26:21,120
in the memory cache hierarchy of your processor.

00:26:21,120 --> 00:26:26,380
You get, like, a dozen registers, you get kilobytes of L1 cache,

00:26:26,390 --> 00:26:29,630
you get megabytes of L3 cache -- maybe.

00:26:29,630 --> 00:26:32,480
Most machines don't even have an L3 cache.

00:26:32,480 --> 00:26:34,900
And so you want to keep the overhead low.

00:26:34,900 --> 00:26:38,350
You need to keep related data packed together.

00:26:38,350 --> 00:26:41,520
Think about SortedContainers for a moment.

00:26:41,520 --> 00:26:46,300
Each sublist will have one pointer to an element that you added.

00:26:46,300 --> 00:26:48,740
That's it. We have a maxes index,

00:26:48,740 --> 00:26:52,240
but that's literally a thousandth of the number of elements.

00:26:52,240 --> 00:26:54,280
That's barely anything.

00:26:54,640 --> 00:26:58,920
So, our sublists add roughly one pointer per element,

00:26:58,920 --> 00:27:02,860
and that's actually 66% less memory

00:27:02,860 --> 00:27:05,320
than traditional binary tree implementations.

00:27:05,320 --> 00:27:09,820
So we're going to take advantage of this memory hierarchy really well.

00:27:10,560 --> 00:27:13,940
Let's also talk about the memory tiers.

00:27:13,940 --> 00:27:16,540
These have very different performance.

00:27:16,540 --> 00:27:19,820
Memory slows down by a factor of 1,000 times

00:27:19,820 --> 00:27:22,200
from registers to main memory.

00:27:22,200 --> 00:27:24,980
So that advertised price of memory lookups

00:27:24,980 --> 00:27:30,740
that intel or AMD show you is actually the average random lookup time,

00:27:30,740 --> 00:27:33,000
but that's only one common pattern.

00:27:33,000 --> 00:27:36,340
Sequential memory access patterns are so fast

00:27:36,340 --> 00:27:38,490
you almost don't pay for them at all.

00:27:38,490 --> 00:27:41,850
The processor literally predicts the memory you'll need next

00:27:41,850 --> 00:27:44,980
and queues it up for you before you ask for it.

00:27:44,980 --> 00:27:48,140
So you almost get sequential accesses for free.

00:27:48,140 --> 00:27:52,280
Then there's also a pattern called data-dependent memory accesses.

00:27:52,280 --> 00:27:55,040
These happen when you follow pointers,

00:27:55,040 --> 00:27:57,420
particularly when you follow a lot of pointers.

00:27:57,420 --> 00:27:59,420
It means that the next memory location

00:27:59,420 --> 00:28:02,380
is dependent on the current memory location,

00:28:02,380 --> 00:28:05,240
and this is typical in binary trees.

00:28:05,240 --> 00:28:07,980
Unfortunately, it's also really slow.

00:28:07,980 --> 00:28:10,330
It's as much as 10 times slower

00:28:10,330 --> 00:28:13,720
than random or sequential memory access.

00:28:14,640 --> 00:28:17,420
Let's think about adding elements again.

00:28:17,420 --> 00:28:21,860
An add calls bisect.insort, and bisect.insort

00:28:21,860 --> 00:28:25,780
does a binary search and then insert on the list.

00:28:25,780 --> 00:28:30,800
Here is the C code for insert in CPython, approximately.

00:28:30,800 --> 00:28:34,380
Notice, it's entirely sequential memory accesses.

00:28:34,380 --> 00:28:37,360
So we're getting a lot of these shifts for free.

00:28:37,360 --> 00:28:39,540
We're just costing cycles to move things over.

00:28:39,550 --> 00:28:42,610
So if we had 1,000 elements in a list

00:28:42,610 --> 00:28:45,180
and we want to insert at position 0,

00:28:45,180 --> 00:28:48,760
then it will cost, say, 1,000 cycles.

00:28:48,760 --> 00:28:51,800
Think also about the binary search process.

00:28:51,800 --> 00:28:55,020
Initially, we're kind of randomly jumping through memory,

00:28:55,020 --> 00:28:59,380
but then, over time, we're narrowing down on one specific region.

00:28:59,380 --> 00:29:02,840
So that also improves our locality.

00:29:02,840 --> 00:29:05,260
By comparison, traditional binary trees

00:29:05,260 --> 00:29:08,100
use this data-dependent memory access

00:29:08,100 --> 00:29:12,500
as they repeatedly dereference pointers for other nodes.

00:29:12,500 --> 00:29:16,360
And so, we can sequentially shift 1,000 elements in memory

00:29:16,360 --> 00:29:21,280
in the time it takes to access a couple of binary nodes from DRAM.

00:29:22,040 --> 00:29:26,660
So memory is tiered, and caches are limited in size.

00:29:26,660 --> 00:29:30,920
This is also why the slope of the performance curve for SortedList

00:29:30,920 --> 00:29:34,200
was less than that for binary tree implementations.

00:29:34,200 --> 00:29:36,540
Remember back at that performance graph?

00:29:36,540 --> 00:29:41,180
At scale, binary trees do more data-dependent DRAM lookups

00:29:41,180 --> 00:29:43,340
than SortedContainers.

00:29:44,400 --> 00:29:47,820
I said that initializing a sorted container is fast.

00:29:47,820 --> 00:29:49,600
Let's look at why.

00:29:49,600 --> 00:29:52,160
Here's the initializer for a SortedList.

00:29:52,160 --> 00:29:54,960
Notice it simply calls the sorted builtin,

00:29:54,960 --> 00:29:57,660
and then chops up the result into sublists

00:29:57,660 --> 00:30:00,360
and initializes the maxes index.

00:30:00,360 --> 00:30:02,520
So you pass in an iterable,

00:30:02,520 --> 00:30:05,720
we pass that off to the sorted builtin,

00:30:05,720 --> 00:30:09,320
we chop up the result, according to the load,

00:30:09,320 --> 00:30:11,780
and then we initialize the maxes index

00:30:11,780 --> 00:30:15,360
as simply the last element in each sublist.

00:30:15,360 --> 00:30:17,960
I think of this kind of as a cheat.

00:30:17,960 --> 00:30:20,840
What I'm doing here is I'm using the power of timsort

00:30:20,840 --> 00:30:22,840
to initialize the container.

00:30:22,840 --> 00:30:25,900
And it turns out, initialization is really common.

00:30:25,900 --> 00:30:30,660
It's quite frequent that we initialize large data types, large collections,

00:30:30,660 --> 00:30:33,040
and then we only make a few edits for them.

00:30:33,040 --> 00:30:37,540
And in those scenarios, SortedContainers will work incredibly fast.

00:30:38,040 --> 00:30:40,480
Think about: how long does it take to initialize

00:30:40,480 --> 00:30:43,260
already-sorted data in this case?

00:30:43,260 --> 00:30:45,200
Well, timsort is heavily optimized,

00:30:45,200 --> 00:30:49,480
so timsort will literally make one linear pass over the iterable,

00:30:49,480 --> 00:30:52,900
copy that, almost like a memcpy into the result,

00:30:52,900 --> 00:30:54,820
and then we're going to iterate that result

00:30:54,820 --> 00:30:57,260
and chop it up into the sublist chunks.

00:30:57,260 --> 00:31:02,180
So, we're doing, maybe, only two memcpy-like operations,

00:31:02,180 --> 00:31:06,200
and those are incredibly fast with today's modern processors.

00:31:06,840 --> 00:31:08,660
Here's another cheat:

00:31:08,660 --> 00:31:11,150
When we add an element to a sorted set,

00:31:11,150 --> 00:31:15,080
I add it to both a set object and a SortedList.

00:31:15,080 --> 00:31:18,740
And this preserves the fast set membership in test.

00:31:18,740 --> 00:31:22,700
Now, some purists will argue that hashing should not be necessary,

00:31:22,700 --> 00:31:24,560
and they are correct.

00:31:24,560 --> 00:31:27,900
But, quite often, if you can define comparisons,

00:31:27,900 --> 00:31:30,360
you can probably define hash.

00:31:30,360 --> 00:31:32,440
And remember, we're solving real problems,

00:31:32,440 --> 00:31:34,260
not theoretical ones.

00:31:34,260 --> 00:31:38,400
If you can reuse the built-in types, then cheat and do it.

00:31:38,880 --> 00:31:40,980
So if you can, cheat.

00:31:40,980 --> 00:31:43,840
The way to make things faster is to do less work.

00:31:43,840 --> 00:31:45,980
There's really no way around that.

00:31:45,980 --> 00:31:49,440
Another cheat I've mentioned regards the positional index.

00:31:49,440 --> 00:31:53,120
If you don't need numerical lookups, don't build the index.

00:31:53,120 --> 00:31:56,060
And that's a very common scenario with sorted dictionaries.

00:31:56,060 --> 00:31:59,220
It's quite rare that people do numerical indexing.

00:31:59,220 --> 00:32:02,380
We use less memory, and we run faster.

00:32:02,900 --> 00:32:07,600
When it comes to runtime complexity, here's the punchline -- ready?

00:32:07,600 --> 00:32:10,540
Adding random elements has an amortized cost

00:32:10,540 --> 00:32:14,420
proportional to the cube root of the container size.

00:32:14,420 --> 00:32:16,840
So we're talking about cube root n.

00:32:16,840 --> 00:32:19,100
That's an unusual runtime complexity,

00:32:19,100 --> 00:32:21,440
but it works quite well in practice.

00:32:21,440 --> 00:32:25,050
The surprising thing is that n stays relatively small.

00:32:25,050 --> 00:32:29,130
For example, if we create a billion integers in CPython,

00:32:29,130 --> 00:32:31,940
it will take more than 30 gigabytes of memory,

00:32:31,940 --> 00:32:35,200
which is already exceeding the limits of most machines.

00:32:35,200 --> 00:32:37,880
We've also seen that memory is expensive.

00:32:37,880 --> 00:32:39,800
Allocations are costly.

00:32:39,800 --> 00:32:43,540
In the common case, SortedContainers allocates no more memory

00:32:43,540 --> 00:32:45,400
when adding elements.

00:32:45,400 --> 00:32:47,800
So we're optimizing that as well.

00:32:47,800 --> 00:32:50,200
If you're still doubtful about performance and scale,

00:32:50,200 --> 00:32:53,140
I encourage you, go read the project docs.

00:32:53,140 --> 00:32:56,760
I created a page very recently called "Performance at Scale".

00:32:56,760 --> 00:32:58,900
You might have to search; there's a lot of links.

00:32:58,900 --> 00:33:02,140
It talks extensively about theory with benchmarks

00:33:02,140 --> 00:33:04,650
all the way up to 10 billion elements.

00:33:04,650 --> 00:33:07,820
So that's going to last us quite a few more years.

00:33:07,820 --> 00:33:10,220
A little PSA before I continue:

00:33:10,220 --> 00:33:15,300
If you claim to be fast, please, please, please do measurements.

00:33:15,300 --> 00:33:17,480
Measure, measure, measure.

00:33:17,480 --> 00:33:19,580
And publish those measurements.

00:33:19,580 --> 00:33:21,780
I understand benchmarks can be misleading,

00:33:21,780 --> 00:33:24,180
but just show me that you measured it.

00:33:24,180 --> 00:33:26,180
Measure, measure, measure.

00:33:26,180 --> 00:33:29,720
Big O notation is not a substitute for benchmarks.

00:33:29,720 --> 00:33:32,600
Quite often, the constants and coefficients

00:33:32,600 --> 00:33:37,280
that are ignored in theory, matter quite a lot in practice.

00:33:37,760 --> 00:33:40,260
So measure, measure, measure.

00:33:40,260 --> 00:33:43,240
This whole project, in fact, started with a measurement.

00:33:43,240 --> 00:33:47,620
I was timing how long it took to add an element to a blist,

00:33:47,620 --> 00:33:52,280
and I noticed that bisect.insort was actually faster

00:33:52,280 --> 00:33:54,900
for a list with 1,000 elements.

00:33:54,900 --> 00:33:56,920
It was so much faster, in fact,

00:33:56,920 --> 00:34:02,120
I thought, "Wow, I could do two inserts in a 1,000-element list

00:34:02,120 --> 00:34:04,600
"and still be faster than blist."

00:34:04,600 --> 00:34:08,579
That thought eventually became the list of sublist implementation

00:34:08,580 --> 00:34:10,800
that we have today.

00:34:10,800 --> 00:34:13,100
So here's the performance lessons.

00:34:13,100 --> 00:34:15,600
The built-in types are fast.

00:34:15,600 --> 00:34:18,000
Program your interpreter.

00:34:18,000 --> 00:34:20,380
Avoid programming in Python.

00:34:20,380 --> 00:34:22,300
Memory is tiered.

00:34:22,300 --> 00:34:24,200
Cheat if you can.

00:34:24,200 --> 00:34:27,020
And measure, measure, measure.

00:34:27,580 --> 00:34:29,500
A couple of closing thoughts.

00:34:29,500 --> 00:34:31,700
Everything related to SortedContainers

00:34:31,700 --> 00:34:34,900
is under an open source Apache 2 license.

00:34:34,900 --> 00:34:37,180
Contributors are very welcome.

00:34:37,180 --> 00:34:39,480
We've started to create a little community, kind of,

00:34:39,480 --> 00:34:41,700
around SortedCollections.

00:34:41,710 --> 00:34:44,129
I think it's interesting to ask, you know,

00:34:44,129 --> 00:34:47,560
"Is this worth a PEP, a Python Enhancement Proposal?"

00:34:47,560 --> 00:34:52,280
I'm personally a little on the fence, but I lean towards being in favor.

00:34:52,280 --> 00:34:56,040
I think SortedCollections would contribute to Python's maturity,

00:34:56,040 --> 00:34:58,720
but I don't know if any proposal could really survive

00:34:58,720 --> 00:35:00,960
the inevitable bikeshedding.

00:35:00,960 --> 00:35:04,380
So far, my contribution is a pure Python implementation

00:35:04,380 --> 00:35:07,880
that's fast enough for most scenarios.

00:35:08,440 --> 00:35:11,320
Let me end with a quote from Mark Summerfield.

00:35:11,320 --> 00:35:15,620
Mark and a couple of other authors have actually deprecated their modules

00:35:15,620 --> 00:35:17,900
in favor of SortedContainers.

00:35:17,900 --> 00:35:19,860
And Mark says this:

00:35:19,860 --> 00:35:22,600
"Python's batteries-included standard library

00:35:22,600 --> 00:35:25,580
"seems to have a battery missing, and the argument

00:35:25,580 --> 00:35:28,760
"that we've never had it before has worn thin.

00:35:28,760 --> 00:35:30,880
"It is time that Python offered

00:35:30,880 --> 00:35:36,320
"a full range of collection classes out of the box, including sorted ones."

00:35:36,320 --> 00:35:38,300
Thanks for letting me share.

00:35:38,300 --> 00:35:45,600
[applause]

00:35:45,600 --> 00:35:48,580
(presenter) Thank you. Questions, anyone?

00:35:48,580 --> 00:35:52,160
Let me come to you so everyone can hear your question.

00:35:56,920 --> 00:35:58,860
(audience member) Hi Grant. Great talk.

00:35:58,860 --> 00:36:00,940
A quick question: have you compared performance

00:36:00,940 --> 00:36:03,540
against the Java or C++ implementations, or .NET?

00:36:03,540 --> 00:36:05,360
Just other languages.

00:36:05,360 --> 00:36:08,040
(Grant Jenks) I have not. That would be an interesting project.

00:36:08,040 --> 00:36:10,020
(audience member) Any expectations?

00:36:13,260 --> 00:36:15,800
(Grant Jenks) I think it would be pretty competitive.

00:36:15,800 --> 00:36:17,840
Python, in general, is fairly competitive.

00:36:17,840 --> 00:36:21,980
Remember, we're ultimately using SortedContainers writing C code.

00:36:21,980 --> 00:36:25,020
So it wouldn't surprise me, even, if we were a little faster than Java,

00:36:25,020 --> 00:36:27,780
until Java was able to kick in its JIT.

00:36:29,380 --> 00:36:31,440
Contributions welcome.

00:36:42,240 --> 00:36:44,900
(audience member) So, I suppose that the, you know,

00:36:44,900 --> 00:36:48,520
SortedContainers, the types, they are all the same.

00:36:48,520 --> 00:36:52,120
What if I build a container, with, for example,

00:36:52,120 --> 00:36:54,120
numerical values like 1, 2, 3,

00:36:54,120 --> 00:36:57,360
and then I add a string later, or like fubar?

00:36:57,360 --> 00:36:59,200
What will happen?

00:36:59,200 --> 00:37:01,060
(Grant Jenks) Well, you'll be constrained

00:37:01,060 --> 00:37:04,080
by your Python implementation, or the Python standard.

00:37:04,080 --> 00:37:07,660
So in Python 2, if you mix types in a list

00:37:07,660 --> 00:37:11,789
and try and sort that list, it will kind of break ties

00:37:11,789 --> 00:37:14,260
when there are different kinds of types itself.

00:37:14,260 --> 00:37:17,059
In Python 3, they made that into an error.

00:37:17,060 --> 00:37:19,640
So my hands are kind of tied when it comes to that.

00:37:19,640 --> 00:37:22,200
SortedContainers will likewise trigger an error

00:37:22,200 --> 00:37:25,320
if you try and compare different types.

00:37:25,320 --> 00:37:27,840
(audience member) Okay, thank you. By the way,

00:37:27,840 --> 00:37:30,220
your -- that Jenks Index?

00:37:30,220 --> 00:37:35,560
It reminds me of the order statistics tree in computer science.

00:37:35,560 --> 00:37:39,960
You know, you build a binary tree and you add a extra field

00:37:39,960 --> 00:37:44,280
which has the number of nodes in this sub tree.

00:37:44,280 --> 00:37:46,180
(Grant Jenks) Right, it's very similar to that.

00:37:46,180 --> 00:37:51,660
I was actually teaching a lesson on heaps to a student when I realized,

00:37:51,660 --> 00:37:54,279
"Gee, these heaps are so fast."

00:37:54,280 --> 00:37:58,720
And the heapq module, in particular, in Python, is quite quick.

00:37:58,720 --> 00:38:02,160
I was like, "Wow, I wish I could leverage that same thing

00:38:02,160 --> 00:38:04,160
"for positional index."

00:38:04,160 --> 00:38:06,560
And with a little whiteboarding later,

00:38:06,560 --> 00:38:11,740
I came up with this kind of sublist-length positional index tree.

00:38:11,740 --> 00:38:13,580
(audience member) Thank you very much.

00:38:19,980 --> 00:38:22,400
(audience member) Have you considered using the array

00:38:22,400 --> 00:38:25,600
storage mechanism for minimizing memory?

00:38:25,600 --> 00:38:30,660
You talked about memory efficiency, but lists are notoriously inefficient.

00:38:30,660 --> 00:38:32,660
Is there another option?

00:38:33,220 --> 00:38:35,240
(Grant Jenks) I started researching that.

00:38:35,240 --> 00:38:37,900
I don't know if you're the one who just opened that GitHub issue,

00:38:37,900 --> 00:38:40,840
but somebody had a very similar thought.

00:38:40,840 --> 00:38:45,280
And I started researching that on the train from PDX to --

00:38:45,280 --> 00:38:48,400
or, from the airport to here.

00:38:48,400 --> 00:38:50,580
If I recall correctly, it would be a little tricky,

00:38:50,580 --> 00:38:52,680
but it wouldn't be too difficult.

00:38:52,680 --> 00:38:55,440
I haven't quite investigated it enough yet.

00:38:55,450 --> 00:38:59,740
My real solution, in that case, is to suggest PyPy.

00:38:59,740 --> 00:39:05,960
So, PyPy, P-Y-P-Y, is an alternative implementation.

00:39:05,960 --> 00:39:09,480
PyPy will actually take advantage of something called tagged pointers.

00:39:09,480 --> 00:39:13,960
So PyPy will stuff the integer inside the pointer itself,

00:39:13,960 --> 00:39:16,580
and this will realize a huge benefit.

00:39:16,580 --> 00:39:19,480
So, a lot of times, too, you know, when I was talking about

00:39:19,480 --> 00:39:23,170
those cache locality problems and memory accesses,

00:39:23,170 --> 00:39:25,560
PyPy really screams on this stuff,

00:39:25,560 --> 00:39:28,320
because it will be able to optimize in those ways.

00:39:28,320 --> 00:39:30,880
So I would try PyPy, but if you can't do that,

00:39:30,880 --> 00:39:33,680
talk to me afterwards. We can research.

00:39:36,080 --> 00:39:37,960
(presenter) Anyone else?

00:39:44,280 --> 00:39:46,299
(audience member) Is there any room for optimization

00:39:46,300 --> 00:39:50,980
on, sort of, frozen or somewhat frozen data types?

00:39:51,380 --> 00:39:53,180
(Grant Jenks) Well, it turned out --

00:39:53,180 --> 00:39:55,020
I've thought about that problem.

00:39:55,020 --> 00:40:00,860
A frozen SortedList is just a tuple.

00:40:00,860 --> 00:40:02,740
(audience member) Right. Sorry, I meant more like

00:40:02,740 --> 00:40:04,840
frozen dict or that sort of thing.

00:40:04,840 --> 00:40:07,150
(Grant Jenks) Well, as I played with that,

00:40:07,150 --> 00:40:09,300
I've done a couple of things.

00:40:09,300 --> 00:40:14,760
There is a way to get, kind of, a dictionary view out of a dictionary,

00:40:14,760 --> 00:40:19,240
but there's an old CPython issue bug that I think kind of squashed that idea

00:40:19,240 --> 00:40:21,620
like, "Nah, we'll never need that."

00:40:21,620 --> 00:40:24,760
We could try and resurrect that to make it work on CPython.

00:40:24,760 --> 00:40:26,740
The other thing you can do, if you're, like,

00:40:26,740 --> 00:40:29,640
really sold on this idea of a frozen dictionary,

00:40:29,640 --> 00:40:33,040
is you can actually just use a tuple,

00:40:33,040 --> 00:40:37,800
and you can copy the C implementation right out of CPython.

00:40:37,800 --> 00:40:41,140
So, I can -- I can give you more details on that later.

00:40:41,140 --> 00:40:43,580
It's a little awkward. It's certainly not as fast.

00:40:43,580 --> 00:40:48,200
But using a big tuple and then indexing into it,

00:40:48,200 --> 00:40:50,620
you only see, maybe, a 2 to 10x slowdown,

00:40:50,620 --> 00:40:53,260
so there are some trade-offs.

00:40:56,140 --> 00:40:58,280
(presenter) Any more questions?

00:41:00,040 --> 00:41:02,040
Well, thanks again, Grant.

00:41:02,040 --> 00:41:05,380

YouTube URL: https://www.youtube.com/watch?v=7z2Ki44Vs4E


