Title: Elizabeth Ramirez - Kalman Filters for non-rocket science - PyCon 2016.mp4
Publication date: 2016-06-17
Playlist: PyCon 2016
Description: 
	Speaker: Elizabeth Ramirez

Kalman Filters have been widely used for scientific applications. No wonder people often think they involve complex math, however you can actually introduce the Kalman Filter in your daily data processing work, without the complex math you would imagine. This talk will show how to implement the discrete Kalman Filter in Python using NumPy and SciPy.

Slides can be found at: https://speakerdeck.com/pycon2016 and https://github.com/PyCon/2016-slides
Captions: 
	00:00:00,399 --> 00:00:07,180
Can you please help me welcome Elizabeth for a talk on Kalman filters.

00:00:07,180 --> 00:00:09,040
She works at the New York Times. Thank you.

00:00:09,200 --> 00:00:17,640
(Applause)

00:00:17,920 --> 00:00:21,439
Ok, so my name is Elizabeth. I work as a software engineer

00:00:21,439 --> 00:00:24,019
at the New York Times, and today we're going to talk

00:00:24,020 --> 00:00:27,800
about Kalman filters for non-rocket science.

00:00:28,640 --> 00:00:33,040
So, a little background. The Kalman filter,

00:00:33,040 --> 00:00:37,120
it's an algorithm named after Rudolf Kalman,

00:00:37,120 --> 00:00:39,950
and it's basically a predictor corrector technique

00:00:39,950 --> 00:00:47,180
by which we calculate recursively a system state at time tk,

00:00:47,180 --> 00:00:53,280
using only the state at previous time step u K minus 1.

00:00:53,280 --> 00:00:58,320
We're going to explain that later. We only use the previous step

00:00:58,330 --> 00:01:01,940
and the new information that comes to the system.

00:01:01,940 --> 00:01:06,970
So if you're interested of the details of this,

00:01:06,970 --> 00:01:14,530
you can read here in this original paper that was published in 1960.

00:01:14,530 --> 00:01:19,760
And nothing really has changed a lot from 1960, so.

00:01:21,990 --> 00:01:25,120
This is a picture of Rudolf Kalman receiving

00:01:25,120 --> 00:01:33,350
the National Medal of Science on October 7 in 2009

00:01:33,350 --> 00:01:36,310
from President Barack Obama at the White House,

00:01:36,310 --> 00:01:42,460
just to remark how important has been the Kalman filter on science development.

00:01:44,660 --> 00:01:50,180
Ok, so first, let's talk about Kalman filter for rocket science.

00:01:50,180 --> 00:01:55,700
So the Kalman filter was first applied to the problem of trajectory estimation

00:01:55,700 --> 00:02:01,470
for the Apollo space program of NASA in the 1960s,

00:02:01,470 --> 00:02:07,150
and then it was incorporated on Apollo space navigation computer.

00:02:07,150 --> 00:02:11,099
It is also being used in the guidance and navigation system

00:02:11,099 --> 00:02:15,099
of the NASA space shuttle and altitude control

00:02:15,099 --> 00:02:19,599
and navigation systems of the International Space Station.

00:02:19,599 --> 00:02:24,360
So, in other words, this is -- the Kalman filter is mostly used

00:02:24,360 --> 00:02:29,540
for positioning and navigation systems, AKA rocket science.

00:02:30,560 --> 00:02:36,159
So, there is a transcription of the original Kalman filter code used

00:02:36,160 --> 00:02:43,120
for Apollo 11 guidance computer, and it is available for public domain.

00:02:43,500 --> 00:02:45,560
Just check this. You are allowed.

00:02:46,820 --> 00:02:54,000
It was implemented in the 1960s using AGC for assembly language.

00:02:54,000 --> 00:02:57,000
AGC: It stands for Apollo Guidance Computer.

00:02:57,700 --> 00:03:02,320
So, it was using like, very low level assembly instructions

00:03:02,320 --> 00:03:10,540
like count, compare, and skip, CCS., transfer to storage, clear and add,

00:03:10,560 --> 00:03:11,400
and so on.

00:03:12,960 --> 00:03:18,500
So this is how the code for the original Kalman filter looks like.

00:03:18,510 --> 00:03:21,470
So it's assembler, if you understand it,

00:03:21,470 --> 00:03:25,140
like, good for you. (Laughter)

00:03:26,080 --> 00:03:29,720
Then, let's talk about Kalman filters for non-rocket science.

00:03:29,730 --> 00:03:35,180
So the Kalman filter can also be used for some time of forecasting problems

00:03:35,180 --> 00:03:38,980
for some specific time series that can be modeled

00:03:38,980 --> 00:03:42,420
as a time-varying mean with additive noise.

00:03:42,980 --> 00:03:47,180
So, this is because the Kalman filter is a generalization

00:03:47,180 --> 00:03:49,500
of the least squares model.

00:03:51,640 --> 00:03:53,640
So let's start with the formulation

00:03:53,940 --> 00:03:58,400
of the least squares model and the normal equation.

00:03:58,409 --> 00:04:03,809
Let's say that we have a linear system represented by Au equals b,

00:04:03,809 --> 00:04:09,620
where A is the matrix of equations, u are representing our unknowns,

00:04:09,620 --> 00:04:13,340
b are a vector representing our measurement.

00:04:13,700 --> 00:04:21,200
So, we need to solve for u. So I think that most of us in college,

00:04:21,200 --> 00:04:24,430
we know the case where A is a square matrix

00:04:24,430 --> 00:04:27,400
in which we have the same number of unknowns

00:04:27,400 --> 00:04:32,960
and the same number of equations. So under other number of conditions

00:04:32,960 --> 00:04:36,080
we know that we can invert A and solve for u.

00:04:36,080 --> 00:04:44,199
So we can have u equals A inverse B. But what if A is not a square matrix?

00:04:44,199 --> 00:04:47,129
What if it's rectangular, because we have

00:04:47,129 --> 00:04:52,639
more equations than unknowns? So the number of rows in this case

00:04:52,640 --> 00:04:57,930
m is bigger than n. So we can't solve the system

00:04:57,930 --> 00:05:01,620
as we normally know, because we can't invert A

00:05:01,621 --> 00:05:06,150
because it's a rectangular matrix. So we say about the system

00:05:06,150 --> 00:05:11,330
that it's overdetermined. We have too many equations

00:05:11,330 --> 00:05:16,909
and too few unknowns. So we're trying to fit m measurement

00:05:16,909 --> 00:05:21,740
by a small number of parameters n. I think this is better illustrated

00:05:21,740 --> 00:05:23,050
by an example.

00:05:23,050 --> 00:05:28,139
Let's say we might have a hundred points that fit a straight line,

00:05:28,139 --> 00:05:33,810
but the system equation for a line is just Cx plus D.

00:05:33,810 --> 00:05:39,710
So we only need to solve this system for two parameters which are C and D,

00:05:39,710 --> 00:05:43,560
but we're solving a hundred equations with two unknowns.

00:05:44,140 --> 00:05:45,740
So, this is a problem.

00:05:46,540 --> 00:05:52,319
So what we can do for this system, for this overdetermined system, is

00:05:52,319 --> 00:05:58,120
that we can find the best estimate for u. We cannot find an exact solution

00:05:58,120 --> 00:06:03,650
but we can find an estimate by minimizing the squared error.

00:06:03,650 --> 00:06:09,070
So each equation introduces an error and the total square error is here,

00:06:09,070 --> 00:06:16,560
E, b minus a dot u estimate represented by the u hat, squared.

00:06:17,440 --> 00:06:21,600
So, let's try to do something here.

00:06:21,600 --> 00:06:26,340
Let's try to multiply both sides of the original equation by a transpose.

00:06:26,340 --> 00:06:34,620
So we have in the left hand side, A transpose A u hat equals A transpose b.

00:06:36,300 --> 00:06:41,020
And this is known as the normal equation. The good thing now is

00:06:41,020 --> 00:06:46,760
that A transpose A is now a square matrix. We can check the dimensions here.

00:06:49,000 --> 00:06:57,800
We know that A transpose is n by m, A is n by n, and A transpose A is n by n.

00:06:58,120 --> 00:07:02,060
So if the original A has independent columns that means

00:07:02,060 --> 00:07:05,900
that columns are not a linear combination of each other,

00:07:06,700 --> 00:07:13,199
now A transpose A is invertible and we can solve for the estimate u hat.

00:07:13,199 --> 00:07:20,159
That minimizes the square error. So our solution here now is going to be

00:07:20,159 --> 00:07:24,939
u hat equals A transpose A inverse A transpose b.

00:07:24,939 --> 00:07:31,879
So this is how least squares work. We need to introduce errors

00:07:31,879 --> 00:07:35,499
because when you take measurements we get errors.

00:07:35,499 --> 00:07:39,410
So let's talk a little bit about covariance matrix.

00:07:39,410 --> 00:07:42,139
So let's run an experiment at once.

00:07:42,139 --> 00:07:49,779
So the error of the -- each measurement might be independent,

00:07:49,779 --> 00:07:52,150
or there might be some correlation between errors

00:07:52,150 --> 00:07:57,120
because simply that the measurements were taken

00:07:57,120 --> 00:08:01,009
by the same device, or something like that.

00:08:01,009 --> 00:08:04,789
So we're going to consider the case when errors are independent

00:08:04,789 --> 00:08:09,220
and the covariance matrix represented by a big sigma here.

00:08:09,220 --> 00:08:14,080
This covariance matrix is going to be a diagonal because the expected value

00:08:14,080 --> 00:08:17,590
of different errors, the product of different errors

00:08:17,590 --> 00:08:22,000
like ei and ej, is going to be zero. So we're going to have

00:08:22,000 --> 00:08:29,080
a diagonal matrix, and ii entries in the diagonal are going to be

00:08:29,080 --> 00:08:34,560
sigma i square which is the expected value of the square of the error.

00:08:35,169 --> 00:08:38,220
So this matrix is going to be always symmetric

00:08:38,220 --> 00:08:41,250
and it's going to be always positive definite

00:08:41,250 --> 00:08:47,590
because the variance of sigma i square is necessarily positive

00:08:47,590 --> 00:08:50,900
and we know that positive definite matrices has

00:08:50,900 --> 00:08:56,350
nice properties. So because of this it can be shown

00:08:56,350 --> 00:09:02,660
that the choice of these C equals big sigma inverse minimizes

00:09:02,660 --> 00:09:05,900
the expected error in estimation of u hat.

00:09:05,900 --> 00:09:12,400
The C is called weighting matrix and the normal equation including now is

00:09:12,400 --> 00:09:16,880
waiting matrix is going to be like the weighted normal equation.

00:09:16,880 --> 00:09:22,440
A transpose C A u hat equals A transpose Cb.

00:09:22,440 --> 00:09:27,960
Now we can solve for the estimate u hat. So when -- in this case

00:09:27,960 --> 00:09:34,890
when sigma square is equal to 1 and sigma ij is equal to 0,

00:09:34,890 --> 00:09:38,830
that means that our error has 0 mean and unit variance.

00:09:38,830 --> 00:09:42,370
The problem becomes just ordinary least squares

00:09:42,370 --> 00:09:47,580
that we saw before. So, well, we need

00:09:47,580 --> 00:09:52,510
a little bit more background for the formulation of the Kalman filter,

00:09:52,510 --> 00:09:56,450
so we need to talk about recursive least squares.

00:09:56,450 --> 00:10:00,390
Let's use an example for illustrating this.

00:10:00,390 --> 00:10:05,220
So let's say we have the average of 99 numbers:

00:10:05,220 --> 00:10:13,180
b1, b2, through b99 and average is going to be u hat 99.

00:10:13,180 --> 00:10:17,800
So let's say that a new number b100 arrives.

00:10:17,800 --> 00:10:24,650
So how to find the new average, u hat 100 without adding all over again.

00:10:24,650 --> 00:10:28,870
I'm pretty sure that this problem came up when I took the GRE test

00:10:28,870 --> 00:10:33,020
and I didn't know how to solve it, but, ok.

00:10:34,940 --> 00:10:42,840
So we want to use only the old average, u hat 99, and the new data, b100.

00:10:42,840 --> 00:10:45,090
So we have two ways to express this:

00:10:45,090 --> 00:10:52,560
The first way is u hat 100 equals 99 over 100,

00:10:53,820 --> 00:10:59,180
the old average, plus 1 over 100, the new measurement.

00:10:59,340 --> 00:11:07,940
Or the second way to express that is how we say here is u99 plus 1 over 100,

00:11:07,940 --> 00:11:12,760
b100 minus u99. So we like better the second form

00:11:12,760 --> 00:11:20,140
because it's presented an update to the previous u hat 99.

00:11:20,850 --> 00:11:29,620
So here in this equation we called this expression b100 minus u hat 99.

00:11:29,620 --> 00:11:33,720
We call these the innovation because it tell us

00:11:33,720 --> 00:11:39,380
how much information is contained in the new measurement b100.

00:11:39,380 --> 00:11:44,990
So you can see that if b100 is equal to u hat 99

00:11:44,990 --> 00:11:48,640
there's no new information in the new data

00:11:48,640 --> 00:11:54,230
so the innovation is 0 and the average doesn't really change.

00:11:54,230 --> 00:11:59,070
The other important part is this fraction 1 over 100

00:11:59,070 --> 00:12:02,420
which is called the gain factor and in part is

00:12:02,420 --> 00:12:07,950
where the filter terminology comes from because it's an input modulated

00:12:07,950 --> 00:12:10,120
but by a gain factor.

00:12:13,640 --> 00:12:15,280
Okay, just a little bit more math.

00:12:17,620 --> 00:12:21,480
Let's generalize the example of the average

00:12:21,480 --> 00:12:26,630
to the same linear system we saw before, Au equal b.

00:12:26,630 --> 00:12:32,410
So let's just start by -- from an old estimate we have

00:12:32,410 --> 00:12:36,990
for the equation A old u equals b old.

00:12:36,990 --> 00:12:42,850
So when new information arrives, let's see, it's called A new or b new,

00:12:42,850 --> 00:12:48,320
we add a new row to A old and a new road to b old.

00:12:48,320 --> 00:12:52,990
And the solution of this new system that you can see,

00:12:52,990 --> 00:13:01,100
like in the second row, is going to lead to a new estimate u new.

00:13:01,920 --> 00:13:06,550
So, we don't want to solve this entire system

00:13:06,550 --> 00:13:10,320
every time a new information comes in.

00:13:10,320 --> 00:13:16,700
So how can we update u old to u new using only A new and b new?

00:13:17,680 --> 00:13:24,140
So, let's back to the normal equation and let's find

00:13:24,140 --> 00:13:28,300
with this new information A transpose A.

00:13:29,350 --> 00:13:32,480
So you can see here that A transpose is this,

00:13:32,480 --> 00:13:34,900
like, horizontal matrix.

00:13:34,900 --> 00:13:39,580
And if we calculate the dot product between these two is going to be

00:13:39,580 --> 00:13:44,110
A transpose old A old plus A transpose new A new.

00:13:44,110 --> 00:13:48,780
So you can see here that is like a known part plus a new part.

00:13:48,780 --> 00:13:51,530
Now if we work on the right hand side of the equation

00:13:51,530 --> 00:13:57,420
A transpose b is going to be this A old transpose b old plus

00:13:57,420 --> 00:14:00,920
A new transpose b new, but we know from the original equation

00:14:00,920 --> 00:14:09,140
that b old is equal to A old u hat old. So we replace that back into this equation.

00:14:09,980 --> 00:14:16,180
So, using those two expressions for the normal equation

00:14:16,190 --> 00:14:19,720
and like, simplifying, we get this expression

00:14:19,720 --> 00:14:23,790
for the recursive least squares in this form

00:14:23,790 --> 00:14:27,360
that we already say that we really like.

00:14:27,360 --> 00:14:32,260
So it's going to be u hat new is equal to u hat old plus

00:14:32,260 --> 00:14:39,920
A transpose A inverse A new transpose b new minus A new b old.

00:14:39,920 --> 00:14:44,370
So this is the expression for recursive least squares.

00:14:44,370 --> 00:14:48,230
So we can see here that the gain matrix is going to be

00:14:48,230 --> 00:14:55,000
A transpose A inverse A new transpose, and is often denoted by K

00:14:55,000 --> 00:15:01,360
for Kalman, obviously. So if we go back to the average problem,

00:15:01,360 --> 00:15:08,220
we have the least square solution for 99 equations and one unknown

00:15:08,220 --> 00:15:10,580
that is going to be this u the average.

00:15:11,200 --> 00:15:16,500
So the A matrix in this case is going to be a column vectors with just ones.

00:15:16,660 --> 00:15:24,360
And when the 100th equation comes in, so it's a new measurement, u equals b100.

00:15:24,360 --> 00:15:28,740
That's b new. So, the only thing we need to do is add

00:15:28,750 --> 00:15:32,950
a new row to A new which is going to be a new one.

00:15:32,950 --> 00:15:37,030
And if we apply the recursive least squares expressions

00:15:37,030 --> 00:15:42,740
for this, we're going to get this expression that is the same

00:15:42,740 --> 00:15:48,400
that we have previously found. So in this case we could have had

00:15:49,760 --> 00:15:54,340
a weighting matrix to measure the reliability of u hat,

00:15:55,400 --> 00:15:58,640
but in this example the hundred equations were

00:15:58,650 --> 00:16:02,260
equally reliable and had the same unit variance.

00:16:02,260 --> 00:16:07,900
So this is why C, the covariance matrix, doesn't show up here.

00:16:12,460 --> 00:16:20,340
So now finally we have enough elements to discuss the Kalman filter formulation.

00:16:20,600 --> 00:16:24,020
So let's see how it works forecasting in time series.

00:16:24,030 --> 00:16:30,000
So the Kalman filter is basically a time-varying least squares problem.

00:16:30,000 --> 00:16:38,200
So in this quick time we produce an estimate u K hat at each time tk.

00:16:38,200 --> 00:16:45,050
So the whole idea of the Kalman filter is updating our best least square estimate

00:16:45,050 --> 00:16:50,520
of the state vector u hat after new observations comes in.

00:16:50,520 --> 00:16:57,040
So, we want to compute like the change to update what we predicted.

00:16:58,340 --> 00:17:03,120
So this will work if we can express the new estimate as a linear combination

00:17:03,680 --> 00:17:08,820
of the old estimate, u old, and the new observation, b new.

00:17:08,820 --> 00:17:13,320
So we can see it like here, like how the linear combination should be.

00:17:13,320 --> 00:17:21,120
U new equals some L dot u old plus some K dot b new.

00:17:22,020 --> 00:17:25,220
So, there are some nice features of the Kalman filter

00:17:25,220 --> 00:17:28,650
that are important to our implementation.

00:17:28,650 --> 00:17:31,730
So first is that the Kalman filter is recursive.

00:17:31,730 --> 00:17:38,430
We didn't store at all observations b old because those measurements are

00:17:38,430 --> 00:17:42,340
already used in the estimate u old hat.

00:17:42,340 --> 00:17:49,980
So normally the state vector u is much shorter than the measurement vector b

00:17:49,980 --> 00:17:52,910
which is growing in length with each measurement.

00:17:52,910 --> 00:18:02,780
So this filter is efficient in this sense. So, we can write the linear combination

00:18:02,780 --> 00:18:07,920
for the Kalman filter as we see in the second point.

00:18:08,700 --> 00:18:13,060
U new is going to be u old plus a gain matrix

00:18:13,060 --> 00:18:19,850
that multiplies our innovation. So again, the innovation

00:18:19,850 --> 00:18:23,540
between all the estimates and new measurements are

00:18:23,540 --> 00:18:28,560
modulated but the gain matrix. So the reliability of u hat

00:18:28,560 --> 00:18:33,590
of our estimate is given but the error covariance matrix P

00:18:33,590 --> 00:18:38,040
that tells the statistical properties of u hat

00:18:38,040 --> 00:18:43,600
based on the statistical properties of the measurements b.

00:18:44,160 --> 00:18:49,000
So we see here that our covariance matrix P is going to be

00:18:49,000 --> 00:18:55,940
A transpose C A inverse. So in the Kalman filter implementation

00:18:55,940 --> 00:19:00,320
we also need to update the covariance matrix

00:19:00,320 --> 00:19:08,520
when a new measurement b new arrives because it, like, includes new information

00:19:08,520 --> 00:19:10,520
for the covariance.

00:19:13,280 --> 00:19:15,400
Let's present the algorithm here.

00:19:19,200 --> 00:19:24,520
I mean there is some other steps before coming to this algorithm

00:19:24,530 --> 00:19:27,740
for prediction correction.

00:19:27,740 --> 00:19:34,590
It's basically in that the property of A transpose C A is

00:19:34,590 --> 00:19:38,800
a tridiagonal matrix. So the forward elimination

00:19:38,800 --> 00:19:41,210
to solve the system is going to be a recursion

00:19:41,210 --> 00:19:44,920
and the back substitution is going to be another recursion.

00:19:44,920 --> 00:19:49,860
The forward recursions find the best estimate of the final state

00:19:49,860 --> 00:19:52,920
and very often that's all we need because we don't really need

00:19:52,920 --> 00:19:55,060
to calculate back substitution.

00:19:55,060 --> 00:19:58,200
And we are not going to do back substitution here

00:19:58,210 --> 00:20:04,270
because back substitution just adjusts earlier estimates to account

00:20:04,270 --> 00:20:08,670
for later measurement. So this process is called smoothing

00:20:08,670 --> 00:20:12,580
and it produces the correct solutions to the normal equation,

00:20:13,380 --> 00:20:20,320
but we're not going to do smoothing today, just the forward recursion.

00:20:20,680 --> 00:20:23,400
So the forward recursion is a two-step process:

00:20:23,400 --> 00:20:28,620
prediction and correction. For a prediction we will use

00:20:28,620 --> 00:20:32,030
all the information we have through time k minus 1

00:20:32,030 --> 00:20:37,640
to generate the prediction, then when a new measurement comes time k,

00:20:38,180 --> 00:20:43,200
we're going to add a correction. So, putting all this together

00:20:43,220 --> 00:20:50,220
the Kalman filter produces the final state u hat K, given K.

00:20:53,320 --> 00:21:01,380
So we can see here for prediction how we predict u K given K minus 1.

00:21:01,380 --> 00:21:05,120
So that's a prediction like taking into account

00:21:05,120 --> 00:21:08,040
all information we have up to time K minus 1.

00:21:08,460 --> 00:21:13,600
We have here that this F matrix is called the state transition matrix.

00:21:13,900 --> 00:21:17,300
It establishes how the state vector changes

00:21:17,300 --> 00:21:21,620
from one time step to another. This -- the F matrix is like

00:21:21,620 --> 00:21:25,480
really particular to the specific system we're trying to solve.

00:21:27,320 --> 00:21:32,600
And then we have the expression for the estimate of the covariance matrix,

00:21:32,600 --> 00:21:38,150
given the old information. We have here a Q,

00:21:38,150 --> 00:21:42,130
which is the covariance matrix of the system error.

00:21:42,130 --> 00:21:46,050
This is going to be because every time we add the new equation

00:21:46,050 --> 00:21:50,510
we add an error that is different from the error of the measurement.

00:21:50,510 --> 00:21:53,340
So the system error is going to be Q.

00:21:55,440 --> 00:21:58,920
So for the correction we need to calculate

00:21:58,920 --> 00:22:03,640
the gain matrix, which is K, using the prediction

00:22:03,640 --> 00:22:07,390
of the -- of the covariance matrix.

00:22:07,390 --> 00:22:14,820
And then we use the gain matrix K to update our predictions of u K

00:22:14,820 --> 00:22:19,330
now given K, because we now have the new measurement at time K

00:22:19,330 --> 00:22:23,060
and the covariance matrix at time K.

00:22:25,200 --> 00:22:29,510
So finally, we're going closer to the code.

00:22:29,510 --> 00:22:34,510
So, as mentioned in the abstract for this talk, NumPy will provide

00:22:34,510 --> 00:22:38,670
all the core linear algebra we need for prediction and updates,

00:22:38,670 --> 00:22:43,700
and also the data structure to hold all the equations of the state.

00:22:43,700 --> 00:22:46,961
So let's implement two functions for the prediction

00:22:46,961 --> 00:22:51,160
and corrections of steps. We want to calculate

00:22:51,160 --> 00:22:55,800
the following variables at each step that predicted

00:22:55,800 --> 00:23:02,290
mean and covariance of the state that's predicted u and predicted P

00:23:02,290 --> 00:23:07,450
before the measurement comes in, the estimated mean and covariance

00:23:07,450 --> 00:23:11,080
of the state after the measurement, the corrected one,

00:23:11,080 --> 00:23:13,760
the innovation and the filter again.

00:23:16,240 --> 00:23:22,300
So, this is the data that we require for making a prediction.

00:23:22,309 --> 00:23:27,500
We require the previous state vector u, the previous covariance matrix P,

00:23:27,500 --> 00:23:34,610
the state transition matrix F, and the process noise covariance matrix Q.

00:23:36,540 --> 00:23:41,560
So, this is like, extremely simple.

00:23:44,880 --> 00:23:47,960
Is it clear? okay.

00:23:48,680 --> 00:23:54,660
So it's going to be a predictor step. It's just going to calculate

00:23:54,670 --> 00:23:58,380
the dot product between the transition matrix

00:23:58,380 --> 00:24:02,700
and the old estimate for the state. And it's going to calculate

00:24:02,780 --> 00:24:08,220
the covariance matrix using the state error

00:24:08,380 --> 00:24:13,760
and the --and also the F matrix, and we return both of these.

00:24:16,340 --> 00:24:22,120
The correction step requires the predicted state and covariance,

00:24:22,120 --> 00:24:27,360
the A matrix which is like our matrix of observation equations.

00:24:27,360 --> 00:24:31,990
Now we account for b which can be on a scalar

00:24:31,990 --> 00:24:39,820
or a vector of observations at time K. The covariance matrix of the system

00:24:39,820 --> 00:24:43,520
and the covariance matrix of error in observations.

00:24:45,350 --> 00:24:50,860
So again, this is really simple. The correct step is going to

00:24:50,870 --> 00:24:58,200
calculate first this C that is given by the Kalman filter update formula,

00:24:58,200 --> 00:25:04,100
and then we compute the Kalman gain matrix which is this K here,

00:25:04,100 --> 00:25:08,300
and finally we add the correction to predict the state

00:25:08,300 --> 00:25:10,690
and to the covariance matrix.

00:25:11,000 --> 00:25:16,380
So this is really simple. So how will we simulate this?

00:25:16,390 --> 00:25:20,110
So we want to initialize all our data structures

00:25:20,110 --> 00:25:24,840
and run the prediction correction for some number of iterations.

00:25:25,280 --> 00:25:29,520
And at the end we want to compare the predicted and corrected state

00:25:29,520 --> 00:25:33,080
with the actual measure that came into the system.

00:25:33,480 --> 00:25:37,920
So here I'm going to just have a time step of point 1.

00:25:38,480 --> 00:25:43,760
I'm going to have this A matrix that is representing some equations.

00:25:44,480 --> 00:25:49,320
I'm going to initialize my state vector with zeroes

00:25:49,330 --> 00:25:53,550
and we're going to use some random measurements

00:25:53,550 --> 00:25:59,120
but center at the state value, like the predicted state value.

00:25:59,120 --> 00:26:03,860
So, we're going to use random numbers that returns a sample

00:26:03,860 --> 00:26:08,280
from the standard distribution in accordance to the unit variance.

00:26:08,280 --> 00:26:14,040
We're using here for Q and R, just for the sake of simplicity.

00:26:16,570 --> 00:26:22,580
So we are going to run this a hundred times.

00:26:22,580 --> 00:26:25,200
We're going to store our prediction corrections

00:26:25,200 --> 00:26:27,740
and measurements on this list.

00:26:28,620 --> 00:26:34,220
We're just going to run predict, correct, and generate a new measurement

00:26:34,220 --> 00:26:38,540
at the end of the look. So we're printing here

00:26:39,040 --> 00:26:43,720
the predicted final estimate, the corrected final estimate

00:26:43,720 --> 00:26:49,070
after the new measurement came in, and the actual measured state.

00:26:49,620 --> 00:26:57,800
So, for the system we predicted minus 23 point 41.

00:26:57,800 --> 00:27:03,340
We corrected and looked at the prediction. It's not that bad,

00:27:03,340 --> 00:27:07,750
so it's kind of useful here.

00:27:07,750 --> 00:27:12,340
So this is really useful for example when you're using your GPS

00:27:12,340 --> 00:27:16,929
and you go through a tunnel or you don't have signal

00:27:16,929 --> 00:27:20,890
but you keep like observing that the GPS is working.

00:27:20,890 --> 00:27:27,120
And it's just because the system is predicting the next state.

00:27:27,120 --> 00:27:35,470
So in this plot we can see that the prediction is on circle,

00:27:35,470 --> 00:27:41,520
the correction is on axis and the actual measurement is on triangles.

00:27:42,010 --> 00:27:47,460
So it's interesting that the correction is always

00:27:47,460 --> 00:27:51,080
between the prediction and the measurement.

00:27:51,220 --> 00:27:59,380
So I don't remember why, but that's the correct way for this.

00:28:00,720 --> 00:28:05,380
So conclusions. We have seen that the Kalman filter is

00:28:05,380 --> 00:28:09,260
a viable forecasting technique for time series.

00:28:10,600 --> 00:28:15,040
For some specific times series that can be modeled in a certain way

00:28:15,640 --> 00:28:23,500
and that is not limited to rocket science. And Kalman filter is like really similar

00:28:23,510 --> 00:28:28,590
to least square method but it has some computational advantages

00:28:28,590 --> 00:28:32,970
in terms of efficiency. So, almost everything

00:28:32,970 --> 00:28:36,090
in this talk has been extracted from the book

00:28:36,090 --> 00:28:39,480
Computational Science and Engineering by Gilbert Strang.

00:28:39,720 --> 00:28:43,440
Gilbert Strang is the guy in the picture.

00:28:43,440 --> 00:28:47,049
So if you feel like getting deeper into Kalman filters,

00:28:47,049 --> 00:28:51,730
I recommend to go to his book or to the videos available

00:28:51,730 --> 00:28:55,900
at MIT OpenCourseWare. So there are also

00:28:55,900 --> 00:28:59,340
some obviously available packages for Kalman filters

00:28:59,340 --> 00:29:05,360
that take into account other information like correlated errors

00:29:05,360 --> 00:29:09,630
and some -- some other informations like pykalman.

00:29:09,630 --> 00:29:14,160
So again, if you want to check this and better understand,

00:29:14,160 --> 00:29:20,080
just download pykalman or go to Gilbert Strang's book,

00:29:20,080 --> 00:29:21,120
Whatever you want.

00:29:24,140 --> 00:29:26,960
Okay. I think that's it. Thank you very much.

00:29:26,960 --> 00:29:32,700
I hope this wasn't like, too heavy of math and too light Python,

00:29:32,710 --> 00:29:37,470
but I hope you enjoyed the talk. Let me know afterwards

00:29:37,470 --> 00:29:41,640
if you have any questions. This is my email address.

00:29:41,720 --> 00:29:47,420
Yes, and my Twitter account, So just feel free to get in touch.

00:29:47,420 --> 00:29:55,740

YouTube URL: https://www.youtube.com/watch?v=k_MpfzMc9PU


