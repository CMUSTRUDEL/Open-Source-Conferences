Title: Irene Chen   A Beginner's Guide to Deep Learning   PyCon 2016
Publication date: 2016-06-17
Playlist: PyCon 2016
Description: 
	"Speaker: Irene Chen

What is deep learning? It has recently exploded in popularity as a complex and incredibly powerful tool. This talk will present the basic concepts underlying deep learning in understandable pieces for complete beginners to machine learning. We will review the math, code up a simple neural network, and provide contextual background on how deep learning is used in production now. 

Slides can be found at: https://speakerdeck.com/pycon2016 and https://github.com/PyCon/2016-slides"
Captions: 
	00:00:05,300 --> 00:00:06,900
(host) I'm excited about hearing your talk.

00:00:06,900 --> 00:00:09,840
(Irene Chen) Oh yeah? It's going to be very beginner,

00:00:09,840 --> 00:00:12,420
which I think... I've been to a lot of these talks and I think

00:00:12,420 --> 00:00:16,680
sometimes you just, like, dive right in and people get very overwhelmed, so...

00:00:16,680 --> 00:00:18,660
(host) That's OK by me. I'm a developer

00:00:18,660 --> 00:00:20,560
that works with a bunch of data scientists,

00:00:20,560 --> 00:00:24,000
so I need the beginner level to understand what they're saying, so that's great.

00:00:24,000 --> 00:00:27,280
All right, I guess it's 4:30. We can begin.

00:00:27,280 --> 00:00:29,000
(Irene Chen) Why don't we wait for that crowd to --

00:00:29,000 --> 00:00:30,360
(host) Oh, yeah.

00:00:30,360 --> 00:00:32,720
(Irene Chen) I just don't want people coming in and out.

00:01:43,560 --> 00:01:45,680
(host) Good afternoon, everyone.

00:01:45,680 --> 00:01:47,680
I'd like to introduce Irene Chen,

00:01:47,680 --> 00:01:50,920
who will be talking about a beginner's guide to deep learning.

00:01:51,880 --> 00:01:53,380
(Irene Chen) All right, hi.

00:01:53,380 --> 00:01:58,860
[applause]

00:01:59,260 --> 00:02:01,920
Good afternoon, everyone. Oh that is loud. OK.

00:02:01,920 --> 00:02:03,940
Hi, my name is Irene.

00:02:03,940 --> 00:02:06,560
I am currently a data scientist at Dropbox,

00:02:06,560 --> 00:02:09,240
and today we're going to be talking about deep learning,

00:02:09,240 --> 00:02:12,640
specifically a beginner's guide to deep learning,

00:02:12,640 --> 00:02:15,080
that is, emphasis on the "beginner,"

00:02:15,080 --> 00:02:18,080
and obviously emphasis on the "deep learning" part.

00:02:18,090 --> 00:02:20,530
So this looks like a pretty bright crowd.

00:02:20,530 --> 00:02:25,730
Raise your hand if you've ever Googled or Binged or DuckDuckGo'd deep learning

00:02:25,730 --> 00:02:27,920
in an effort to teach yourself more about deep learning.

00:02:27,920 --> 00:02:31,780
So, oh wow, that looks like almost everyone in this room.

00:02:31,780 --> 00:02:34,040
Great. Good work, team.

00:02:34,040 --> 00:02:36,900
If you've taken a look on the internet, you may have seen things

00:02:36,900 --> 00:02:39,980
about convolutional nets, back propagation,

00:02:39,980 --> 00:02:42,480
image recognition, restricted Boltzmann machines,

00:02:42,480 --> 00:02:46,600
so, very technical jargon, which may be intimidating.

00:02:46,600 --> 00:02:51,540
Or if you follow tech news you may have come across such headlines as

00:02:51,549 --> 00:02:56,260
DeepMind's AlphaGo beating professional go player Lee Sedol recently,

00:02:56,260 --> 00:02:59,780
NVIDIA and its latest GPU architecture,

00:02:59,780 --> 00:03:03,260
Toyota had a $1 billion AI investment,

00:03:03,260 --> 00:03:08,380
and I recently read an article about how Facebook is building AI that builds AI,

00:03:08,380 --> 00:03:10,340
so that's pretty cool.

00:03:11,180 --> 00:03:13,260
Or if you come from a more academic setting,

00:03:13,260 --> 00:03:17,580
maybe you've read works by these so-called on deep learning pioneers,

00:03:17,580 --> 00:03:21,079
sort of the people who are really setting the stone of what we're doing right now -

00:03:21,079 --> 00:03:26,999
people like Geoff Hinton, Yann LeCun, Andrew Ng, and Yoshua Bengio.

00:03:27,000 --> 00:03:32,720
So there's no doubt that the information on the internet runs very deep,

00:03:32,720 --> 00:03:36,320
but - ha ha! - but at times it can be very overwhelming.

00:03:36,329 --> 00:03:40,169
So a simple search for deep learning can yield over 13 million pages,

00:03:40,169 --> 00:03:44,629
and when I was first starting out, I read a lot of these guides myself.

00:03:44,629 --> 00:03:46,840
And having browsed many of these guides,

00:03:46,840 --> 00:03:50,160
I have found that some of the guides can have too much math

00:03:50,160 --> 00:03:52,420
and some of the guides can have too much code.

00:03:52,430 --> 00:03:56,040
So that's just to set it all in place.

00:03:56,040 --> 00:03:57,900
So what are we going to do today, though?

00:03:57,900 --> 00:04:02,020
Well, today we will have some math and we will have some code.

00:04:02,029 --> 00:04:06,019
The goal, however, is to give you a foundation to better equip you

00:04:06,019 --> 00:04:09,739
so that if you want to dive into the technical side of deep learning on your own,

00:04:09,740 --> 00:04:14,000
if you want to leave this 30-minute talk and figure out more things on your own,

00:04:14,000 --> 00:04:18,500
you'll have -- you will have at least seen the bird's-eye view.

00:04:18,510 --> 00:04:21,680
So we'll start with the basic question of, why now?

00:04:21,680 --> 00:04:27,250
Neural networks have actually been in existence since around the 1970s,

00:04:27,250 --> 00:04:29,430
so why the resurgence now?

00:04:29,430 --> 00:04:32,220
Then we'll hone into what exactly a neural network is,

00:04:32,229 --> 00:04:35,149
how does it relate to deep learning, what is -- how does it work?

00:04:35,149 --> 00:04:37,469
What are these circles and arrows you might have seen?

00:04:37,469 --> 00:04:39,220
What do they represent?

00:04:39,220 --> 00:04:43,220
And the last thing we'll do is we'll get our feet wet a little bit

00:04:43,220 --> 00:04:47,120
with some IPython Notebook coding with Caffe,

00:04:47,120 --> 00:04:50,040
which is a popular computer vision library.

00:04:50,880 --> 00:04:52,680
All right, let's jump right in.

00:04:52,680 --> 00:04:56,280
So this is a cartoon I drew for a Pictionary game.

00:04:56,280 --> 00:04:59,560
The term was "machine learning" and this is what I came up with.

00:04:59,569 --> 00:05:03,319
So, fundamentally, machine learning is all about

00:05:03,319 --> 00:05:06,339
making computers as smart as humans.

00:05:06,340 --> 00:05:11,860
We are marching towards leveraging the efficient computation of a computer

00:05:11,860 --> 00:05:15,900
into something that can actually learn and make decisions about the world.

00:05:15,900 --> 00:05:19,300
Deep learning is a relatively new branch of machine learning

00:05:19,300 --> 00:05:21,900
that has been able to achieve superior results

00:05:21,900 --> 00:05:24,260
using much, much more data.

00:05:25,060 --> 00:05:28,860
Andrew Ng, one of the people that I flashed on the slide earlier,

00:05:28,860 --> 00:05:30,800
has a great analogy that I like,

00:05:30,800 --> 00:05:34,479
where he compares deep learning to a rocket ship.

00:05:34,479 --> 00:05:38,379
So in order to get very far, a rocket ship needs two things.

00:05:38,380 --> 00:05:42,300
One it needs a very powerful, very large engine,

00:05:42,300 --> 00:05:45,760
and two, it needs a massive amount of fuel.

00:05:45,760 --> 00:05:48,370
So in this extended metaphor,

00:05:48,370 --> 00:05:52,530
the more and more sophisticated neural networks that we have today

00:05:52,530 --> 00:05:55,170
could be considered the engine,

00:05:55,170 --> 00:05:58,700
and the massive amount of data we have access to,

00:05:58,710 --> 00:06:02,349
on the order of terabytes and above, are the fuel.

00:06:02,349 --> 00:06:07,869
So if you have a big engine but no fuel, the rocket will not go very far.

00:06:07,869 --> 00:06:13,589
And if you have a large amount of fuel and a very, very small engine or no engine,

00:06:13,589 --> 00:06:15,940
you'll end up grounded as well.

00:06:15,949 --> 00:06:19,709
So today we will actually be focusing on the first one, the engine.

00:06:19,709 --> 00:06:23,680
But it's worth noting that most advances in deep learning

00:06:23,690 --> 00:06:26,600
are equally a result of massive training data sets

00:06:26,600 --> 00:06:28,800
as well as the neural networks.

00:06:28,800 --> 00:06:33,900
As a recent example, AlphaGo recently beat one of the strongest professional go players

00:06:33,900 --> 00:06:37,819
in the history of the game by analyzing and training on a data set

00:06:37,819 --> 00:06:41,900
of about tens of millions of games by expert go players.

00:06:41,900 --> 00:06:45,550
So for those of you that don't know, go is a board game

00:06:45,550 --> 00:06:48,160
involving black and white stones.

00:06:48,160 --> 00:06:51,540
For a long time, people thought it would be the final frontier,

00:06:51,550 --> 00:06:55,920
that no one would -- no computer would ever beat a human at go; it just was not possible.

00:06:55,920 --> 00:06:59,220
And it turns out, by cranking through a lot of data

00:06:59,220 --> 00:07:02,240
and having a lot of GPUs, you can in fact do it.

00:07:02,240 --> 00:07:05,199
So that's -- what an exciting time we live in right now.

00:07:05,199 --> 00:07:08,399
The fact is that we are seeing so many advances

00:07:08,400 --> 00:07:11,340
in deep learning right now because we have this perfect storm.

00:07:11,349 --> 00:07:14,280
We have the ability to capture and store a lot of data,

00:07:14,280 --> 00:07:19,420
and we are making increased advances in the technology of neural networks.

00:07:20,200 --> 00:07:21,939
So let's keep it simple.

00:07:21,939 --> 00:07:26,539
One of the most common types of machine learning algorithms is called a classifier.

00:07:26,540 --> 00:07:30,300
For the purposes of right now, we can think of it as a black box

00:07:30,300 --> 00:07:34,600
or maybe an orange box, because it shows up a little bit better on the screen.

00:07:34,600 --> 00:07:38,380
As the name would suggest, a classifier takes in some input

00:07:38,380 --> 00:07:42,200
and gives scores so that the output would be one of the classes,

00:07:42,200 --> 00:07:47,080
one of the two or more classes it's been asked to label this input with.

00:07:47,080 --> 00:07:49,800
So for example, let's look at an avocado.

00:07:49,800 --> 00:07:51,940
I live in San Francisco.

00:07:51,940 --> 00:07:53,820
Once a week I eat a --

00:07:53,830 --> 00:07:57,060
over the course of a week I eat maybe ten avocados.

00:07:57,060 --> 00:07:59,060
I really like avocados.

00:07:59,060 --> 00:08:02,360
One problem with avocados is that it's actually kind of hard to tell

00:08:02,360 --> 00:08:04,420
a perfectly ripe avocado.

00:08:04,420 --> 00:08:07,380
So, what if we could build a classifier for this?

00:08:07,380 --> 00:08:10,620
So, given an avocado with a certain size,

00:08:10,620 --> 00:08:13,320
maybe a measure of the squishiness of an avocado

00:08:13,330 --> 00:08:17,010
and the RGB value for the color of the skin,

00:08:17,010 --> 00:08:20,390
can we predict if an avocado is in fact ripe?

00:08:21,280 --> 00:08:25,440
Furthermore, if we had data from many, many more avocados,

00:08:25,440 --> 00:08:29,840
could we learn even better? Could we have an even more accurate model?

00:08:29,840 --> 00:08:34,080
Classifiers use all this training data to train a classifier --

00:08:34,080 --> 00:08:37,440
that is, to make themselves better and more accurate.

00:08:37,440 --> 00:08:39,300
So, once they have all this extra data,

00:08:39,300 --> 00:08:41,940
they can make even better predictions about the model's task --

00:08:41,940 --> 00:08:45,460
here, again, deciding if it's a ripe avocado.

00:08:46,180 --> 00:08:49,660
Traditionally, machine learning has a lot of different tools

00:08:49,670 --> 00:08:53,110
that we would use to solve this problem that are not deep learning.

00:08:53,110 --> 00:08:55,900
So you may have heard of things like a logistic regression,

00:08:55,900 --> 00:09:00,180
naive Bayes, support vector machine, k-nearest neighbors, random forests.

00:09:00,180 --> 00:09:04,820
These are all things that are excellent tools that are not deep learning.

00:09:04,820 --> 00:09:07,080
And they all work in a very similar way.

00:09:07,080 --> 00:09:09,060
You take in the input data,

00:09:09,060 --> 00:09:13,380
so maybe the last 1,000 avocados that I've I bought,

00:09:13,380 --> 00:09:15,680
whether or not they were ripe, how much they weighed,

00:09:15,680 --> 00:09:17,780
what color they were, how squishy they were.

00:09:17,780 --> 00:09:21,820
We train the classifier, and then for any new avocado from now on,

00:09:21,820 --> 00:09:24,600
we can predict if it is in fact ripe.

00:09:24,600 --> 00:09:26,640
Compared to some of the other classifiers,

00:09:26,640 --> 00:09:29,740
deep learning takes a very long time to train.

00:09:29,740 --> 00:09:33,040
So for a simple example like my avocado problem,

00:09:33,040 --> 00:09:34,940
it might not be the best tool.

00:09:34,940 --> 00:09:38,440
As the dimensions of the input data grow, though,

00:09:38,440 --> 00:09:41,760
and the complexity of the patterns that we're trying to detect increase,

00:09:41,760 --> 00:09:44,560
deep nets become more and more important.

00:09:44,560 --> 00:09:49,760
So imagine we have a face, and we want to figure out who it is.

00:09:49,760 --> 00:09:53,800
All of a sudden the input data is actually the RGB values

00:09:53,800 --> 00:09:56,480
of each pixel of this image,

00:09:56,480 --> 00:10:01,120
and your training set is probably millions of photos of various people.

00:10:01,120 --> 00:10:06,040
Moreover, the number of classes that you're trying to predict has also increased.

00:10:06,040 --> 00:10:08,880
Instead of predicting if an avocado is ripe or not --

00:10:08,880 --> 00:10:11,460
so that's one, two classes --

00:10:11,460 --> 00:10:15,780
you're trying to predict if this face is actually me, Irene Chen,

00:10:15,780 --> 00:10:18,720
or if it's Ellen Degeneres or if it's Jennifer Lawrence

00:10:18,720 --> 00:10:20,840
or someone else altogether.

00:10:20,840 --> 00:10:22,640
That's a lot of data.

00:10:22,640 --> 00:10:25,620
Deep nets quickly become the only tool

00:10:25,630 --> 00:10:29,270
that can handle such large data sets in a reasonable fashion.

00:10:29,830 --> 00:10:32,350
So that brings us to our first takeaway.

00:10:32,350 --> 00:10:35,660
This is the best time ever for deep learning in the history yet

00:10:35,660 --> 00:10:40,620
because of a mass amount of data, a mass amount of processing power,

00:10:40,620 --> 00:10:42,950
and these robust neural networks.

00:10:45,420 --> 00:10:47,720
All right. But let's take a step back.

00:10:49,280 --> 00:10:52,420
Our goal is to make computers as smart as humans.

00:10:52,420 --> 00:10:56,520
So what makes computers -- what makes humans so smart?

00:10:56,520 --> 00:11:00,620
In humans, the cells that make us think are called neurons.

00:11:00,620 --> 00:11:03,640
They talk to each other using electrical impulses

00:11:03,640 --> 00:11:06,460
through links called synapses.

00:11:06,460 --> 00:11:10,300
This entire nervous system allows us to reason, have consciousness,

00:11:10,300 --> 00:11:14,040
allows you to sit in your seat and listen to me or not listen to me,

00:11:14,040 --> 00:11:16,860
and it allows me to talk at you.

00:11:17,580 --> 00:11:21,800
Computer scientists have been trying to model this complex nervous system

00:11:21,800 --> 00:11:25,820
using a more simplistic model, and we call that a neural network.

00:11:25,830 --> 00:11:27,520
It's a form of a graph.

00:11:27,520 --> 00:11:30,580
Deep learning is all about neural networks.

00:11:30,580 --> 00:11:33,690
A neural network's function, similar to the classifiers we learned about,

00:11:33,690 --> 00:11:35,850
is to process a set of inputs,

00:11:35,850 --> 00:11:40,970
perform a series of increasingly complex and complicated calculations,

00:11:40,970 --> 00:11:44,190
and then use the outputs to solve the desired problem.

00:11:44,190 --> 00:11:46,920
We can use the nodes, similar to the --

00:11:46,920 --> 00:11:50,400
we can use a concept called nodes to represent the neurons

00:11:50,400 --> 00:11:55,220
and we can use edges to represent the synapses in the brain.

00:11:55,220 --> 00:11:57,240
So, here we have a graph.

00:11:58,040 --> 00:12:02,300
If we trigger a node, so we sort of give it some sort of input,

00:12:02,300 --> 00:12:04,700
it then triggers the nodes that it's connected to

00:12:04,710 --> 00:12:08,050
and so forth until it affects the entire graph.

00:12:08,050 --> 00:12:09,830
Because we are computer scientists,

00:12:09,830 --> 00:12:12,040
we want to organize things and give it a little structure,

00:12:12,040 --> 00:12:15,900
so we can create dedicated input and output nodes.

00:12:16,940 --> 00:12:19,420
We throw some extra nodes in the middle for fun,

00:12:19,420 --> 00:12:21,900
and we go ahead and add some directed edges

00:12:21,910 --> 00:12:24,160
to control the flow of information.

00:12:24,160 --> 00:12:27,200
So, each connection can have different values

00:12:27,210 --> 00:12:30,360
in order to decrease or increase the importance

00:12:30,360 --> 00:12:32,720
of the connection between these two nodes.

00:12:32,720 --> 00:12:35,860
If the weight is too low, maybe there is no edge at all.

00:12:35,860 --> 00:12:38,760
So here we represent the weights using thickness of arrows.

00:12:38,760 --> 00:12:40,640
I'm not quite sure how well it rendered

00:12:40,640 --> 00:12:45,280
but some arrows are much thicker and some arrows are very, very thin.

00:12:45,680 --> 00:12:50,380
If we look at node A, which is red, and node B, which is blue,

00:12:50,380 --> 00:12:55,620
we can see that they're both feeding into node C, which is now purple.

00:12:55,620 --> 00:12:59,100
Because the weight of the edge between B and C

00:12:59,110 --> 00:13:02,670
is heavier than the weight of the edge between A and B,

00:13:02,670 --> 00:13:05,570
we could say that maybe it's a little more blue than red

00:13:05,570 --> 00:13:09,990
but it's still purple as a combination of the inputs.

00:13:09,990 --> 00:13:14,180
Mathematically, we represent this by using a decision function

00:13:14,190 --> 00:13:17,970
in order to weigh the inputs and decide which value to output

00:13:17,970 --> 00:13:20,870
using what's called a sigmoid function.

00:13:21,620 --> 00:13:25,220
But back to the graph, once a node computes its own value,

00:13:25,220 --> 00:13:29,860
it feeds information forward in the next layer and so forth

00:13:29,870 --> 00:13:32,360
until the output nodes have their own values.

00:13:32,360 --> 00:13:34,680
So here we have C, and C will influence D,

00:13:34,680 --> 00:13:36,940
and if you notice, D has also been drawn

00:13:36,940 --> 00:13:39,940
from this other node that I did not give a letter.

00:13:40,800 --> 00:13:45,380
The layers, you'll notice, include the input nodes and the output nodes

00:13:45,390 --> 00:13:48,670
and everything in between, which we call the hidden layers,

00:13:48,670 --> 00:13:51,220
which is a shame, because they do most of the work

00:13:51,220 --> 00:13:53,420
and receive none of the glory.

00:13:54,480 --> 00:13:56,820
Neural networks are traditionally used for classification problems.

00:13:56,820 --> 00:14:02,440
So a reminder again, classification problems: inputs feeds forward outputs.

00:14:02,440 --> 00:14:05,880
So let's bring back my handy-dandy avocado example.

00:14:05,880 --> 00:14:09,140
So, given, again, the height, the squishiness,

00:14:09,140 --> 00:14:14,180
and the RGB value of the skin of the avocado, is it ripe?

00:14:14,180 --> 00:14:17,040
So the neural network in this case would read the input data.

00:14:17,040 --> 00:14:19,320
I've made up some fake numbers representing the variables

00:14:19,320 --> 00:14:22,960
I just outlined, measurements from this avocado.

00:14:22,960 --> 00:14:25,620
And it would calculate the value along the wei--

00:14:25,620 --> 00:14:28,560
based on the weights of the directed edges

00:14:28,570 --> 00:14:32,270
until it grows layer by layer, until we get output values.

00:14:32,270 --> 00:14:36,420
This process is known as forward propagation.

00:14:36,420 --> 00:14:39,020
This is the algorithm for feeding input data,

00:14:39,020 --> 00:14:41,460
calculating the values, going along the weights

00:14:41,460 --> 00:14:43,840
until we get to the output values.

00:14:44,680 --> 00:14:46,980
This allows us to calculate the likelihood,

00:14:46,980 --> 00:14:49,080
interpreting the output values

00:14:49,080 --> 00:14:52,940
to determine if the avocado is indeed ripe.

00:14:53,940 --> 00:14:57,940
It's worth noting that no node fires randomly. It's all deterministic.

00:14:57,940 --> 00:15:00,160
That is, if you give the exact same input again,

00:15:00,160 --> 00:15:02,080
you'll get the exact same output.

00:15:02,080 --> 00:15:04,080
So, no randomness.

00:15:04,080 --> 00:15:06,140
But here we have a bigger question.

00:15:06,140 --> 00:15:08,600
How did we get those weights in the first place?

00:15:08,600 --> 00:15:10,860
It sort of -- we just took them for granted.

00:15:10,860 --> 00:15:14,820
So we've got our neural network and we've got these input values.

00:15:14,820 --> 00:15:17,140
We want weights, though,

00:15:17,140 --> 00:15:20,000
the weights of the edges and biases of the whole neural network

00:15:20,010 --> 00:15:22,140
to be such that the accuracy is very high.

00:15:22,140 --> 00:15:24,180
And what do we mean by accuracy?

00:15:24,180 --> 00:15:26,560
So this is where training data comes in.

00:15:26,560 --> 00:15:29,200
Or, should I say, training avocados?

00:15:29,200 --> 00:15:32,720
Training data is simply the existing labeled data

00:15:32,720 --> 00:15:35,260
of previous avocados that we have examined.

00:15:35,260 --> 00:15:38,580
So some are big, some are small, some are ripe, some are not ripe.

00:15:38,580 --> 00:15:42,740
The more data the better. Remember the rocket ship example?

00:15:43,300 --> 00:15:47,000
The weights are then decided using an algorithm called backwards propagation

00:15:47,000 --> 00:15:50,400
or back propagation if you like tongue twisters.

00:15:50,400 --> 00:15:52,820
This selects the weight for the neural network

00:15:52,820 --> 00:15:56,600
that minimizes the error in the network given the existing data.

00:15:56,600 --> 00:15:59,140
So, given all the avocados that we have access to,

00:15:59,140 --> 00:16:01,280
how can we minimize the error?

00:16:01,280 --> 00:16:03,160
So, what do we mean by error?

00:16:03,160 --> 00:16:05,000
So let's take one avocado.

00:16:05,000 --> 00:16:07,020
And we start with some random weights in the network.

00:16:07,020 --> 00:16:10,140
So we said they are all 1 because we're lazy.

00:16:10,140 --> 00:16:14,660
And we add the input avocado measurements for this one avocado.

00:16:14,660 --> 00:16:17,960
Again, we forward propagate using our random selected weights,

00:16:17,960 --> 00:16:21,260
forward forward forward, and we get some output nodes.

00:16:21,260 --> 00:16:23,260
So remember that these are based on a random weight,

00:16:23,260 --> 00:16:25,600
so they're not necessarily correct.

00:16:25,600 --> 00:16:27,600
But we actually know what the output should be

00:16:27,600 --> 00:16:31,240
since this is a known avocado, so we can compare the results.

00:16:31,240 --> 00:16:34,440
And it turns out our output is wrong.

00:16:34,440 --> 00:16:37,440
So we calculated it should be 4 and 20.

00:16:37,440 --> 00:16:40,520
These numbers are made up. Don't read too much into them.

00:16:40,520 --> 00:16:42,940
But it should actually be 5 and 19.

00:16:42,940 --> 00:16:46,540
So it was 4 and 20, now it's 5 and 19.

00:16:46,540 --> 00:16:51,220
Our error values are then how far off our model predicted

00:16:51,220 --> 00:16:55,540
from the examples that we have already observed.

00:16:55,540 --> 00:16:59,960
We use these formulas to backwards propagate the errors.

00:16:59,960 --> 00:17:01,840
So we want to adjust our weights

00:17:01,850 --> 00:17:04,439
based on what we've learned, but only by a small amount.

00:17:04,439 --> 00:17:07,899
So this is a lot of math, and I think it's actually quite small text.

00:17:07,900 --> 00:17:11,800
But the big picture is actually that the formulas depend on

00:17:11,809 --> 00:17:14,899
the values of the nodes, the amount of the error,

00:17:14,899 --> 00:17:18,439
the weights of the edges, and the learning rate.

00:17:18,900 --> 00:17:23,000
The learning rate determines how much we adjust based on each error.

00:17:23,000 --> 00:17:25,240
So this is essentially our step size.

00:17:25,240 --> 00:17:29,700
If we have too big of a step size, we might sail past the right answer,

00:17:29,700 --> 00:17:33,080
the optimal weights, and actually the wrong answer.

00:17:33,080 --> 00:17:37,180
If we have too small of a learning rate, we actually might never get there.

00:17:37,180 --> 00:17:39,980
We will just get stuck and progress very, very slowly

00:17:39,980 --> 00:17:42,700
until we all get bored.

00:17:43,540 --> 00:17:46,640
So we have to decide how much to adjust our weights

00:17:46,640 --> 00:17:49,160
so that we can learn from these errors.

00:17:49,160 --> 00:17:51,820
And we actually push them back from the output nodes

00:17:51,820 --> 00:17:54,120
layer by layer by layer

00:17:54,120 --> 00:17:56,180
until we arrive back at the input layer,

00:17:56,180 --> 00:17:58,659
updating the weights as we go.

00:17:58,659 --> 00:18:01,039
Now that we have new weights, we can begin again

00:18:01,039 --> 00:18:03,859
with a forward propagation, and we continue so

00:18:03,860 --> 00:18:07,639
with all of our known avocados for as many iterations as we can stand

00:18:07,639 --> 00:18:12,129
or until we've determined other criteria for stopping.

00:18:12,129 --> 00:18:15,059
So here is a graph that means very little

00:18:15,059 --> 00:18:16,980
except that it's going down.

00:18:16,980 --> 00:18:20,420
So the x-axis is the number of iterations

00:18:20,420 --> 00:18:23,980
and the y-axis is error, so down is better.

00:18:23,980 --> 00:18:26,860
As you can see, the more iterations we have,

00:18:26,860 --> 00:18:28,740
the more the error goes down.

00:18:28,740 --> 00:18:30,960
But it doesn't necessarily go down smoothly,

00:18:30,970 --> 00:18:32,820
it's not necessarily linear,

00:18:32,820 --> 00:18:36,160
and it's not necessarily, you know, the more iterations the better.

00:18:36,160 --> 00:18:39,280
In fact, at a certain point we could probably say, "That's enough.

00:18:39,280 --> 00:18:41,420
"It seems like that's about as good as we're going to get."

00:18:41,420 --> 00:18:43,200
And we call this convergence.

00:18:43,200 --> 00:18:47,140
So we can define convergence as when it is not --

00:18:47,140 --> 00:18:50,960
it is differing from its previous versions by a certain amount.

00:18:50,960 --> 00:18:54,340
Or we can say that there is a certain number of max iterations,

00:18:54,340 --> 00:18:56,940
because life is only so short and we don't want to sit there

00:18:56,940 --> 00:18:59,920
waiting for our thing to train forever.

00:18:59,920 --> 00:19:01,720
We say the algorithm is complete

00:19:01,720 --> 00:19:04,460
and our neural network has been trained on the data.

00:19:04,460 --> 00:19:07,600
This might take a while, a very long time.

00:19:08,180 --> 00:19:12,720
And one last setup -- we actually can vary some other things.

00:19:12,720 --> 00:19:15,840
So the learning rate, as I mentioned before, can be tuned.

00:19:15,840 --> 00:19:19,140
We can try things with bigger learning rates, smaller learning rates.

00:19:19,150 --> 00:19:21,660
We can also vary the number of nodes we have

00:19:21,660 --> 00:19:24,180
or the number of layers, the number of hidden layers,

00:19:24,180 --> 00:19:27,180
given that our input and output layers are still fixed.

00:19:27,180 --> 00:19:30,800
This is called tuning the parameters of our neural network.

00:19:31,760 --> 00:19:34,120
But ultimately, we will arrive at a neural network

00:19:34,120 --> 00:19:36,470
that is trained with our weights and biases

00:19:36,470 --> 00:19:39,720
that minimizes error in the entire training data set.

00:19:39,720 --> 00:19:42,400
Time to test it out on some real avocados!

00:19:43,000 --> 00:19:46,400
So our big takeaway from here is that neural networks can be trained

00:19:46,400 --> 00:19:50,320
on labeled data and then classify new avocados.

00:19:50,320 --> 00:19:53,580
It can be applied to other things besides avocados as well.

00:19:53,590 --> 00:19:57,690
For example, given a person's height, weight,

00:19:57,690 --> 00:20:00,390
temperature, can we predict if they are sick?

00:20:00,390 --> 00:20:04,010
Or, given the temperature -- the weather outside

00:20:04,010 --> 00:20:06,120
and the current stock price, can we predict

00:20:06,120 --> 00:20:08,360
if the stock price will go up or down tomorrow?

00:20:08,360 --> 00:20:10,760
There's a lot of applications.

00:20:11,520 --> 00:20:14,320
Why don't you figure out what you can try them on?

00:20:14,840 --> 00:20:17,480
All right, so that brings us to the third section.

00:20:17,480 --> 00:20:19,360
I'm going to get some water.

00:20:20,840 --> 00:20:23,899
All right, so now we know what neural networks look like

00:20:23,900 --> 00:20:27,680
as colored circles on a page; let's see what they look like in code.

00:20:27,680 --> 00:20:29,740
This is, after all, PyCon.

00:20:29,749 --> 00:20:31,529
So, we have computers.

00:20:31,529 --> 00:20:34,940
You do not have to hand-compute weights and errors for back propagation.

00:20:34,940 --> 00:20:38,640
Additionally, even though deep learning is a very young field still,

00:20:38,640 --> 00:20:41,340
there are quite a few helpful Python libraries,

00:20:41,350 --> 00:20:44,249
so you don't have to reinvent the neural network.

00:20:44,720 --> 00:20:47,440
Specifically we're going to talk about four Python libraries

00:20:47,450 --> 00:20:49,640
that I have found very helpful.

00:20:49,640 --> 00:20:52,900
Some of you may be familiar with Scikit-learn, Caffe,

00:20:52,910 --> 00:20:55,690
Theano, or IPython Notebook.

00:20:55,690 --> 00:20:59,760
Scikit-learn is a very well-documented machine learning library

00:20:59,760 --> 00:21:02,760
for all of your ML needs. It's a great place to get started.

00:21:02,760 --> 00:21:06,900
There are implementations for almost any ML algorithm you can think of,

00:21:06,900 --> 00:21:08,880
ML meaning machine learning.

00:21:08,880 --> 00:21:11,920
It's very beginner-friendly, there are a lot of examples,

00:21:11,920 --> 00:21:15,740
and there are also some functions for non-specifically machine learning things

00:21:15,749 --> 00:21:20,409
but sort of data cleaning, how to graph things, very good support.

00:21:20,920 --> 00:21:24,280
Caffe is a computer vision deep learning,

00:21:24,280 --> 00:21:26,380
meaning that it's related to images.

00:21:26,380 --> 00:21:28,820
It comes out of the UC Berkeley Vision Group,

00:21:28,820 --> 00:21:31,920
and there are wrappers for Python and C++.

00:21:31,920 --> 00:21:35,480
And the best thing about Caffe is actually a thing called the zoo,

00:21:35,490 --> 00:21:38,630
which is the group of pre-trained models.

00:21:38,630 --> 00:21:41,340
Instead of training them yourself, which could take forever,

00:21:41,350 --> 00:21:44,550
especially for computer vision, deep nets,

00:21:44,550 --> 00:21:47,430
you can have pre-trained models.

00:21:47,940 --> 00:21:52,540
For Theano, this covers efficient GPU-powered math.

00:21:52,540 --> 00:21:54,800
So if you want to implement your own functions,

00:21:54,809 --> 00:21:56,999
if you want to code up your own neural network,

00:21:56,999 --> 00:22:00,199
Theano provides you a way so that you can make your algorithms

00:22:00,200 --> 00:22:02,420
as efficient as possible.

00:22:02,780 --> 00:22:07,280
And lastly, IPython Notebook is great for interactive coding.

00:22:07,280 --> 00:22:10,179
I think there are some other talks about IPython Notebook

00:22:10,179 --> 00:22:13,119
further at this conference, but I'll go ahead and plug the notebook

00:22:13,120 --> 00:22:14,960
as a great way to show your work.

00:22:14,960 --> 00:22:16,980
It's a great way to profile your code

00:22:16,990 --> 00:22:19,160
since a lot of machine learning algorithms

00:22:19,160 --> 00:22:22,740
can take a long time on a few steps so you don't want to rerun everything.

00:22:22,740 --> 00:22:26,259
I think it's known as Jupiter now, and it handles other languages,

00:22:26,259 --> 00:22:31,619
but old habits die hard so I always refer to it as IPython Notebook still.

00:22:32,080 --> 00:22:35,900
All right, so, all four libraries, I would highly encourage you to check them out.

00:22:35,900 --> 00:22:40,139
But we're here to load a pre-trained network into Caffe

00:22:40,139 --> 00:22:42,819
and use it to classify a picture.

00:22:42,820 --> 00:22:45,040
We have only 30 minutes for this talk.

00:22:45,049 --> 00:22:48,720
So we could have watched me train a network for all 30 minutes,

00:22:48,720 --> 00:22:50,900
but that would be less fun.

00:22:50,909 --> 00:22:54,660
Word of warning that Caffe takes a while to install.

00:22:54,660 --> 00:22:56,579
There's a few tricky things.

00:22:56,580 --> 00:23:01,360
When I was younger and more foolish, it took me about a week to install Caffe.

00:23:01,360 --> 00:23:04,440
I was debugging the whole time.

00:23:04,440 --> 00:23:07,900
And so thankfully Caffe has these pre-trained nets.

00:23:07,900 --> 00:23:11,860
So between the week I spent debugging the Caffe installation

00:23:11,860 --> 00:23:14,540
and the time I saved not having to retrain a net,

00:23:14,540 --> 00:23:17,040
I'd probably say it's about a break even.

00:23:18,200 --> 00:23:20,140
So the model we're looking at today

00:23:20,140 --> 00:23:23,220
is trained off a subset of the ImageNet database.

00:23:23,220 --> 00:23:25,960
So one of the first things I learned about machine learning researchers

00:23:25,960 --> 00:23:28,240
is that they love contests.

00:23:28,240 --> 00:23:30,920
Raise your hand if you've heard of Kaggle.

00:23:30,920 --> 00:23:35,600
So, OK, so about maybe half, maybe a third of the audience.

00:23:35,600 --> 00:23:40,680
So Kaggle is a machine learning sort of cooperative casual platform.

00:23:40,680 --> 00:23:42,540
There are some contests with cash prizes.

00:23:42,540 --> 00:23:45,200
There are forums that are very active.

00:23:45,200 --> 00:23:49,539
The computer vision folks have a similar type challenge

00:23:49,539 --> 00:23:54,379
called the, let's see, ILSVRC Challenge,

00:23:54,380 --> 00:23:59,320
which stands for the ImageNet Large Scale Visual Recognition Challenge,

00:23:59,320 --> 00:24:05,100
where they see who can put classifications on images as efficiently.

00:24:05,100 --> 00:24:07,990
The dataset they're going off of is the image net data set

00:24:07,990 --> 00:24:12,280
and it has 10 million images, 10,000 object classes,

00:24:12,280 --> 00:24:15,820
so cats, dogs, foods, anything you can think of.

00:24:15,820 --> 00:24:18,379
And it's already -- this net that we're working with

00:24:18,380 --> 00:24:22,900
has already been pre-trained on 310,000 iterations.

00:24:23,460 --> 00:24:28,560
So first we'll import the essential packages from Caffe.

00:24:28,560 --> 00:24:31,760
I think it's fairly small on the screen.

00:24:31,760 --> 00:24:35,520
Next we'll load our pre-trained network from disk.

00:24:35,520 --> 00:24:37,940
So when I say we load our network,

00:24:37,940 --> 00:24:41,299
I'm meaning that we -- our network consists of the weights

00:24:41,299 --> 00:24:43,739
that we talked about earlier, the number of the nodes,

00:24:43,740 --> 00:24:45,500
the structure of the layers, anything like --

00:24:45,500 --> 00:24:48,160
and other parameters that have been already tuned.

00:24:48,760 --> 00:24:51,000
And we have selected this picture to classify.

00:24:51,000 --> 00:24:54,280
So just to make sure there are no robots in the room,

00:24:54,280 --> 00:24:57,320
raise your hand if you think this is a mailbox.

00:24:57,320 --> 00:24:58,600
[laughter]

00:24:58,600 --> 00:25:02,720
Ooh, I see a few people in the back who think this is a mailbox. Uhh.

00:25:02,720 --> 00:25:05,180
And raise your hand if you think this is a cat.

00:25:05,180 --> 00:25:09,080
So most people think this is a cat. Great.

00:25:09,080 --> 00:25:12,160
So we're wondering if our pre-trained deep net

00:25:12,160 --> 00:25:15,900
can identify this as -- correctly as a cat.

00:25:15,900 --> 00:25:18,560
It's worth pointing out that this image is not in the training set.

00:25:18,570 --> 00:25:20,690
This is a completely new image.

00:25:20,690 --> 00:25:23,780
So we run it and we determine that the label --

00:25:23,780 --> 00:25:27,100
I just sort of increased the size of the text --

00:25:27,100 --> 00:25:29,710
that it comes out with is "tabby cat."

00:25:29,710 --> 00:25:32,509
I am actually not a cat expert so I'm not quite sure

00:25:32,509 --> 00:25:35,729
if it is a tabby cat, but it does seem plausible.

00:25:35,729 --> 00:25:41,560
And so just as a check, we went in and got the top five most likely labels

00:25:41,570 --> 00:25:46,010
in addition to the tabby cat, so we have tiger cat, Egyptian cat,

00:25:46,010 --> 00:25:50,560
red fox and a lynx, which are all some sort of feline, maybe --

00:25:50,560 --> 00:25:53,600
I think the fox is not feline, but it's related things.

00:25:53,600 --> 00:25:55,680
So -- and I think it checks out.

00:25:56,600 --> 00:26:01,320
Unfortunately, now that I know it's so easy for a computer to label a cat,

00:26:01,320 --> 00:26:03,640
I actually have no idea if you all are humans

00:26:03,649 --> 00:26:06,889
because robots can do it so quickly now.

00:26:06,889 --> 00:26:10,209
From here, we can add additional training to this,

00:26:10,209 --> 00:26:13,720
to this pre-trained net, and we can train for more

00:26:13,720 --> 00:26:16,920
depending on what we're looking for, or we can use this ready-made.

00:26:16,920 --> 00:26:19,059
Similar to the analogy I think of

00:26:19,059 --> 00:26:23,129
is when you buy premade pie crust and you use it to make your own pie

00:26:23,129 --> 00:26:26,149
because you didn't want to make the pie crust yourself.

00:26:26,149 --> 00:26:28,309
Yeah, so it's just that simple.

00:26:28,309 --> 00:26:32,689
We can load and use models using Caffe to jumpstart our learning.

00:26:32,689 --> 00:26:35,760
Note that "learning" here can refer to the deep net learning

00:26:35,760 --> 00:26:39,240
or it also can refer to your own personal edification.

00:26:39,240 --> 00:26:42,320
So, three lessons today, just to rehash.

00:26:42,320 --> 00:26:46,320
Deep learning is super hot right now because of the access to the data,

00:26:46,320 --> 00:26:49,060
the processing power, and these robust neural networks.

00:26:49,070 --> 00:26:52,669
Neural networks themselves, if we zoom in, can be trained on data

00:26:52,669 --> 00:26:55,649
to classify avocados and other things.

00:26:55,649 --> 00:26:59,249
And Caffe is one way to load pre-trained models

00:26:59,249 --> 00:27:00,960
to jumpstart the data.

00:27:00,960 --> 00:27:03,080
But that's enough about me.

00:27:03,080 --> 00:27:05,260
Where do YOU go from here?

00:27:05,260 --> 00:27:07,920
So for this presentation we've abstracted away

00:27:07,920 --> 00:27:11,160
a lot of the extra details to get to the heart of deep learning.

00:27:11,160 --> 00:27:14,120
If you want to pick -- if you any of you want to go further,

00:27:14,120 --> 00:27:16,180
you have to pick where you want to dive a little deeper.

00:27:16,190 --> 00:27:18,889
So of the three lessons, raise your hand if you are

00:27:18,889 --> 00:27:22,589
most interested in the things we talked about in the first lesson.

00:27:22,589 --> 00:27:26,769
No one. One person over -- two people over there. Great.

00:27:26,769 --> 00:27:29,820
So it's possible that you're more of a systems person.

00:27:29,820 --> 00:27:34,040
There's some great work going on about how to scale existing databases,

00:27:34,049 --> 00:27:36,399
how to handle not only the massive amount of data

00:27:36,399 --> 00:27:38,219
needed to train these deep nets,

00:27:38,220 --> 00:27:40,920
but also how do you perform the computation efficiently

00:27:40,920 --> 00:27:44,880
and effectively for both processing time and engineer time.

00:27:44,880 --> 00:27:49,200
I would recommend you check out the CUDA implementations for neural networks,

00:27:49,200 --> 00:27:52,200
and also some of the other packages I mentioned, including Theano,

00:27:52,200 --> 00:27:56,400
Google has an open source library called TensorFlow, and similar packages.

00:27:56,409 --> 00:27:58,929
So raise your hand if you're most interested in the second lesson,

00:27:58,929 --> 00:28:03,829
the neural network part. Ooh, much more. Maybe like half, 60%. Great.

00:28:03,829 --> 00:28:06,649
So maybe it seems like you might be a more theoretical person.

00:28:06,649 --> 00:28:09,229
Maybe you even wanted to see more math.

00:28:09,229 --> 00:28:12,100
If you want to learn how deep learning is adapted

00:28:12,100 --> 00:28:15,020
for different kinds of problems, you should check out the different ways

00:28:15,029 --> 00:28:18,700
neural networks can be stacked on top of each other

00:28:18,700 --> 00:28:20,799
or twisted to fit different problems.

00:28:20,799 --> 00:28:24,239
So in addition to the classification problem that we talked about,

00:28:24,240 --> 00:28:26,780
there's unlabeled problems,

00:28:26,780 --> 00:28:29,879
unlabeled sort of deep learning problems as well.

00:28:29,879 --> 00:28:32,860
So for that you should check out what a restricted Boltzmann machine is.

00:28:32,860 --> 00:28:35,940
So that's for unlabeled data and detecting patterns there.

00:28:35,940 --> 00:28:39,380
For text processing you could look at recurrent networks.

00:28:39,380 --> 00:28:41,720
And for image processing, so anything with pictures,

00:28:41,720 --> 00:28:44,000
you could check out convolutional networks.

00:28:44,000 --> 00:28:47,460
And then last but not least, if you liked lesson three the most

00:28:47,460 --> 00:28:49,580
with Caffe, raise your hand.

00:28:49,940 --> 00:28:54,440
Ooh, a fair amount, maybe like 20 hands I saw.

00:28:54,440 --> 00:28:57,200
So if you want to hack something together, or maybe because you like --

00:28:57,210 --> 00:28:59,580
you learn best by just coding it up in Python

00:28:59,580 --> 00:29:03,159
or whatever your language of choice would be, hopefully Python,

00:29:03,159 --> 00:29:07,019
Caffe is a great place to start loading pre-trained nets.

00:29:07,020 --> 00:29:09,919
There's a whole series of IPython Notebooks on there.

00:29:09,919 --> 00:29:13,159
Feel free to grab a net, and maybe also compete in a Kaggle competition.

00:29:13,159 --> 00:29:16,269
They have actually a really good one about how to tell the difference

00:29:16,269 --> 00:29:18,529
between cats and dogs.

00:29:18,529 --> 00:29:21,889
It seems like most of you can detect a cat,

00:29:21,889 --> 00:29:26,020
so maybe we can figure out how to train a similar net to detect a dog.

00:29:26,029 --> 00:29:27,919
The forums are also very supportive

00:29:27,919 --> 00:29:30,959
and some of the competitions even have a cash prize.

00:29:31,460 --> 00:29:34,960
So the bottom line is there's a lot to dig into.

00:29:34,960 --> 00:29:38,980
Beyond the more headline-grabbing news items,

00:29:38,980 --> 00:29:42,000
deep learning has applications

00:29:42,010 --> 00:29:45,860
that could and are already changing the world.

00:29:45,860 --> 00:29:48,980
A big area of impact already is accessibility.

00:29:48,980 --> 00:29:52,320
So for the visually impaired to be able to read a sign,

00:29:52,320 --> 00:29:54,760
go grocery shopping, or complete everyday tasks,

00:29:54,760 --> 00:29:57,260
deep learning has been invaluable.

00:29:57,260 --> 00:29:59,299
Handwriting recognition has allowed us

00:29:59,299 --> 00:30:03,199
to digitize a lot of historical documents to learn from the past.

00:30:03,200 --> 00:30:06,820
And even the, you know, very exciting self-driving cars

00:30:06,820 --> 00:30:09,920
could dramatically reduce the number of fatal accidents,

00:30:09,920 --> 00:30:11,760
traffic accidents at least.

00:30:11,760 --> 00:30:14,940
So deep learning is not new, at least the ideas are not new,

00:30:14,940 --> 00:30:17,540
but it's a very young field as it is now.

00:30:17,540 --> 00:30:19,540
The people I've found are very supportive,

00:30:19,540 --> 00:30:21,500
and it's a growing field with plenty of opportunities,

00:30:21,500 --> 00:30:23,560
so don't be afraid to jump right in.

00:30:23,570 --> 00:30:27,799
Thank you so much. My email address is irenetrampoline@gmail.com,

00:30:27,809 --> 00:30:30,909
so feel free to email me with any questions. Thank you.

00:30:30,909 --> 00:30:38,700
[applause]

00:30:38,700 --> 00:30:40,500
I think we are out of time

00:30:40,500 --> 00:30:43,140
so I'll just be outside if you have any questions.

00:30:43,900 --> 00:30:47,960

YouTube URL: https://www.youtube.com/watch?v=nCPf8zDJ0d0


