Title: Andrew Montalenti - streamparse: real-time streams with Python and Apache Storm - PyCon 2015
Publication date: 2015-04-12
Playlist: PyCon 2015
Description: 
	"Speaker: Andrew Montalenti

Real-time streams are everywhere, but does Python have a good way of processing them? Until recently, there were no good options. A new open source project, streamparse, makes working with real-time data streams easy for Pythonistas. If you have ever wondered how to process 10,000 data tuples per second with Python -- while maintaining high availability and low latency -- this talk is for you.

Slides can be found at: https://speakerdeck.com/pycon2015 and https://github.com/PyCon/2015-slides"
Captions: 
	00:01:18,890 --> 00:01:24,869
good afternoon is this okay good

00:01:22,289 --> 00:01:26,969
afternoon everyone welcome to stream

00:01:24,869 --> 00:01:29,369
parse real-time streams with Python and

00:01:26,969 --> 00:01:29,909
Apache storm with our speaker and Ramon

00:01:29,369 --> 00:01:38,609
to Letty

00:01:29,909 --> 00:01:40,109
I will pass it over to you so yeah

00:01:38,609 --> 00:01:41,759
before I actually get started I'm just

00:01:40,109 --> 00:01:44,579
gonna give a quick background on me I'm

00:01:41,759 --> 00:01:46,829
the CTO and co-founder of parsley which

00:01:44,579 --> 00:01:48,750
is a real-time web analytics company

00:01:46,829 --> 00:01:50,579
I've been hacking on Python for over a

00:01:48,750 --> 00:01:53,159
decade I think this is my fourth or

00:01:50,579 --> 00:01:55,200
fifth PyCon and parsley runs a fully

00:01:53,159 --> 00:01:57,030
distributed team just in case there are

00:01:55,200 --> 00:01:59,039
people here looking for remote jobs in

00:01:57,030 --> 00:02:00,659
the audience um you can follow me on

00:01:59,039 --> 00:02:02,969
Twitter another nice thing I did is I

00:02:00,659 --> 00:02:04,409
actually scheduled some tweets to go out

00:02:02,969 --> 00:02:05,939
for my account during the talk which

00:02:04,409 --> 00:02:08,039
have links to the slides and all the

00:02:05,939 --> 00:02:11,609
materials related to it so you can find

00:02:08,039 --> 00:02:13,349
those online too and so the title that I

00:02:11,609 --> 00:02:17,129
changed this talk to was actually how to

00:02:13,349 --> 00:02:19,079
defeat the Gil with Apache storm and how

00:02:17,129 --> 00:02:21,239
many people in the audience here went to

00:02:19,079 --> 00:02:23,459
Dave Beasley's co-routines talk or

00:02:21,239 --> 00:02:25,680
concurrency talk yeah so almost all you

00:02:23,459 --> 00:02:27,329
which is great it was actually a great I

00:02:25,680 --> 00:02:30,209
didn't know he was giving that talk i

00:02:27,329 --> 00:02:31,530
watch a lot of davies lee stuff and it

00:02:30,209 --> 00:02:33,690
was a great talk because it showed that

00:02:31,530 --> 00:02:35,549
in all of these different concurrency

00:02:33,690 --> 00:02:37,739
models in python the Gil tends to get in

00:02:35,549 --> 00:02:39,989
the way of true concurrent processing

00:02:37,739 --> 00:02:42,510
and these this diagram is actually from

00:02:39,989 --> 00:02:44,220
another beasley talk where he discusses

00:02:42,510 --> 00:02:46,290
the contention that goes on on

00:02:44,220 --> 00:02:48,870
multi-core and single core machines with

00:02:46,290 --> 00:02:51,599
both with multi-threading models right

00:02:48,870 --> 00:02:54,090
and so what do most people do when they

00:02:51,599 --> 00:02:55,980
run into Gil problems they tend to set

00:02:54,090 --> 00:02:58,380
up cue and worker systems right even

00:02:55,980 --> 00:03:00,180
Beasley said that this was a common way

00:02:58,380 --> 00:03:01,950
that people solved this problem how many

00:03:00,180 --> 00:03:04,799
people here have built a large queue and

00:03:01,950 --> 00:03:06,569
worker system before how many people

00:03:04,799 --> 00:03:09,540
here really love the queueing worker

00:03:06,569 --> 00:03:11,819
system they built yeah almost no right

00:03:09,540 --> 00:03:14,489
so queues and workers tend to get pretty

00:03:11,819 --> 00:03:17,280
gnarly operationally whether you're

00:03:14,489 --> 00:03:19,980
using 0 mq or Redis or RabbitMQ and

00:03:17,280 --> 00:03:22,290
whether you're using cron jobs or RQ or

00:03:19,980 --> 00:03:23,880
celery it tends to be that when you

00:03:22,290 --> 00:03:25,829
start going in the direction of queues

00:03:23,880 --> 00:03:27,870
and workers you start drawing drawing

00:03:25,829 --> 00:03:28,770
diagrams that look like this which is a

00:03:27,870 --> 00:03:31,080
diagram I

00:03:28,770 --> 00:03:33,330
of our cue and worker system at parsley

00:03:31,080 --> 00:03:35,670
when we got to a certain scale in 2012

00:03:33,330 --> 00:03:37,980
and basically it starts to look like

00:03:35,670 --> 00:03:40,560
this operationally right where you're

00:03:37,980 --> 00:03:41,910
like monitoring a million dashboards in

00:03:40,560 --> 00:03:45,210
order to get an understanding of what's

00:03:41,910 --> 00:03:47,130
going on and so one thing you might say

00:03:45,210 --> 00:03:49,980
like Raymond Hettinger does is there

00:03:47,130 --> 00:03:52,260
must be a better way right and I think

00:03:49,980 --> 00:03:53,700
there is a better way and it comes with

00:03:52,260 --> 00:03:56,310
a piece of infrastructure called Apache

00:03:53,700 --> 00:03:58,530
storm so when we first discovered storm

00:03:56,310 --> 00:04:01,080
we read that storm was a distributed

00:03:58,530 --> 00:04:03,240
real-time computation system and it

00:04:01,080 --> 00:04:05,100
basically simplifies worker and cue

00:04:03,240 --> 00:04:06,510
systems and we thought that was great

00:04:05,100 --> 00:04:08,010
but we wondered what about Python

00:04:06,510 --> 00:04:10,410
support because storm is actually

00:04:08,010 --> 00:04:12,390
written in Java and a mixture of closure

00:04:10,410 --> 00:04:14,370
as well well that's actually exactly

00:04:12,390 --> 00:04:17,580
what stream parse is about so that's

00:04:14,370 --> 00:04:19,110
what you'll learn about today so just to

00:04:17,580 --> 00:04:20,730
give a little background on parsley we

00:04:19,110 --> 00:04:22,680
work with these big media companies that

00:04:20,730 --> 00:04:24,600
run high traffic websites like some of

00:04:22,680 --> 00:04:26,730
the logos on the screen here and we

00:04:24,600 --> 00:04:29,340
build real-time analytics dashboards for

00:04:26,730 --> 00:04:31,200
those companies and we also build api's

00:04:29,340 --> 00:04:33,600
like ones you might find on websites

00:04:31,200 --> 00:04:35,610
like The New Yorker com that make live

00:04:33,600 --> 00:04:37,770
content recommendations to users so

00:04:35,610 --> 00:04:41,070
we're dealing with billions of pageviews

00:04:37,770 --> 00:04:43,680
and and millions of visitors every day

00:04:41,070 --> 00:04:45,810
and basically when I describe this

00:04:43,680 --> 00:04:48,180
problem to most most people I hear

00:04:45,810 --> 00:04:50,550
responses like this Python can't work on

00:04:48,180 --> 00:04:52,410
this problem it's a toy language it

00:04:50,550 --> 00:04:55,560
can't scale you really should have used

00:04:52,410 --> 00:04:57,270
Scala or something like that and I hate

00:04:55,560 --> 00:04:59,100
hearing this because basically for ten

00:04:57,270 --> 00:05:01,050
years I've heard about how Python can't

00:04:59,100 --> 00:05:03,450
scale and yet I've continued to ship

00:05:01,050 --> 00:05:05,310
system after system that scales with

00:05:03,450 --> 00:05:11,490
Python and I do think that there are

00:05:05,310 --> 00:05:13,800
some yeah and I do think there are some

00:05:11,490 --> 00:05:15,840
problems with the multi-core scaling

00:05:13,800 --> 00:05:17,669
story with Python but I think that storm

00:05:15,840 --> 00:05:19,320
actually provides a gateway to solving a

00:05:17,669 --> 00:05:21,169
lot of them in a really simple way and

00:05:19,320 --> 00:05:24,510
so this is actually a screenshot from

00:05:21,169 --> 00:05:27,360
three of my seven or eight storm worker

00:05:24,510 --> 00:05:29,760
nodes running on my real-time data

00:05:27,360 --> 00:05:32,070
stream and as you can see every core in

00:05:29,760 --> 00:05:34,169
each top is fully lit up as it's per

00:05:32,070 --> 00:05:36,360
turning through billions of tuples of

00:05:34,169 --> 00:05:39,810
data and all of this is running in pure

00:05:36,360 --> 00:05:40,360
sweet C Python right so I think you can

00:05:39,810 --> 00:05:43,090
get around

00:05:40,360 --> 00:05:44,830
um I think that there's a great

00:05:43,090 --> 00:05:47,020
community emerging around storm and so

00:05:44,830 --> 00:05:49,000
let's walk through what storm really is

00:05:47,020 --> 00:05:50,710
all about like why it's such a great way

00:05:49,000 --> 00:05:53,439
to solve this problem and also how we

00:05:50,710 --> 00:05:55,000
get python to work on it so actually

00:05:53,439 --> 00:05:56,560
before I go into that how many people

00:05:55,000 --> 00:05:59,009
here would say they already like know

00:05:56,560 --> 00:06:02,050
how storm works and have used it before

00:05:59,009 --> 00:06:04,000
okay so a handful of you so I'm gonna

00:06:02,050 --> 00:06:06,009
give like a brief overview of storm but

00:06:04,000 --> 00:06:08,319
I try to make it really Python relevant

00:06:06,009 --> 00:06:10,419
and the way I did that is I kind of

00:06:08,319 --> 00:06:12,939
cover storm topology to up concepts

00:06:10,419 --> 00:06:14,830
using Python pseudocode and actually

00:06:12,939 --> 00:06:15,639
using Python so do pseudocode that uses

00:06:14,830 --> 00:06:18,189
co-routines

00:06:15,639 --> 00:06:20,219
which is kind of fun too so that's how

00:06:18,189 --> 00:06:22,810
we'll actually cover how storm works

00:06:20,219 --> 00:06:25,150
before I go into that stream parse is

00:06:22,810 --> 00:06:27,069
really an integration library for storm

00:06:25,150 --> 00:06:29,740
so we're trying to make it like the

00:06:27,069 --> 00:06:31,360
Jango of data processing pipelines so

00:06:29,740 --> 00:06:32,979
you can just start a quick project in

00:06:31,360 --> 00:06:35,080
stream parse it gives you a command line

00:06:32,979 --> 00:06:36,580
interface to work with your project it

00:06:35,080 --> 00:06:39,279
gives you some basic scaffolding

00:06:36,580 --> 00:06:41,020
scaffolding to start with and then there

00:06:39,279 --> 00:06:44,050
are good management tools for working

00:06:41,020 --> 00:06:46,449
with a storm project in pure Python it's

00:06:44,050 --> 00:06:49,810
really good for analytics log sensors

00:06:46,449 --> 00:06:51,219
and other low latency use cases so this

00:06:49,810 --> 00:06:53,589
is the rough agenda I'm gonna cover

00:06:51,219 --> 00:06:55,150
storm topology concepts I'm then going

00:06:53,589 --> 00:06:57,129
to go into storm internals I'm gonna

00:06:55,150 --> 00:06:58,360
show how Python works with storm I'm

00:06:57,129 --> 00:07:00,250
gonna give an overview of the stream

00:06:58,360 --> 00:07:01,479
parse API and then I'm actually gonna

00:07:00,250 --> 00:07:03,009
give you guys a preview of another

00:07:01,479 --> 00:07:05,250
library we're working on called Pye

00:07:03,009 --> 00:07:08,199
Kafka which actually helps with another

00:07:05,250 --> 00:07:09,759
related technology and like I said

00:07:08,199 --> 00:07:11,889
before all these slides can be found on

00:07:09,759 --> 00:07:14,139
Twitter so first let's go into storm

00:07:11,889 --> 00:07:16,029
topology concepts basically storm

00:07:14,139 --> 00:07:18,430
provides an abstraction for computing

00:07:16,029 --> 00:07:20,770
that can operate at cluster scale and

00:07:18,430 --> 00:07:22,569
the abstraction is really simple it's

00:07:20,770 --> 00:07:24,069
these are the four concepts you really

00:07:22,569 --> 00:07:26,979
need to know about there are tuples

00:07:24,069 --> 00:07:29,650
spouts bolts and a and a topology which

00:07:26,979 --> 00:07:31,240
ties it all together a wire topology

00:07:29,650 --> 00:07:32,949
looks something like this where

00:07:31,240 --> 00:07:35,139
essentially there are components that

00:07:32,949 --> 00:07:36,879
are emitting raw data that's the spouts

00:07:35,139 --> 00:07:38,610
on the left-hand side and there are

00:07:36,879 --> 00:07:41,169
components that are consuming that data

00:07:38,610 --> 00:07:43,060
transforming it possibly sending it to

00:07:41,169 --> 00:07:44,770
other components and possibly having

00:07:43,060 --> 00:07:46,689
side effects like writing to databases

00:07:44,770 --> 00:07:49,000
and things like that and that

00:07:46,689 --> 00:07:51,909
abstraction kind of covers pretty much

00:07:49,000 --> 00:07:52,870
every computing use case now as I

00:07:51,909 --> 00:07:54,190
mentioned before I'm going to use a

00:07:52,870 --> 00:07:55,720
little python pseudocode

00:07:54,190 --> 00:07:57,790
here so this isn't like production code

00:07:55,720 --> 00:07:59,710
you should be taking with you anywhere

00:07:57,790 --> 00:08:00,970
it's just illustrating an idea although

00:07:59,710 --> 00:08:02,830
I did actually write a mock

00:08:00,970 --> 00:08:04,990
implementation of storm using Python

00:08:02,830 --> 00:08:06,580
code or teens based on this code so it's

00:08:04,990 --> 00:08:08,950
not totally crazy

00:08:06,580 --> 00:08:10,540
so a tuple is kind of similar to the

00:08:08,950 --> 00:08:12,910
Python notion of a tuple it's a data

00:08:10,540 --> 00:08:15,100
value for example you might have a tuple

00:08:12,910 --> 00:08:17,260
that represents a word and so then it

00:08:15,100 --> 00:08:19,120
has a string value like dog in it and

00:08:17,260 --> 00:08:21,490
the tuple spec for that would just be

00:08:19,120 --> 00:08:23,020
hey the first field is word you could

00:08:21,490 --> 00:08:25,210
also have a tuple that represents a word

00:08:23,020 --> 00:08:27,730
count and then maybe it has two fields a

00:08:25,210 --> 00:08:29,920
dog and the number of times dog appeared

00:08:27,730 --> 00:08:32,050
in some corpus which is four in this

00:08:29,920 --> 00:08:34,870
case and that's pretty much the notion

00:08:32,050 --> 00:08:37,210
of a tuple a spouts on the other hand is

00:08:34,870 --> 00:08:39,550
the is a component that's going to emit

00:08:37,210 --> 00:08:41,950
tuples into your cluster so this is like

00:08:39,550 --> 00:08:44,320
where your raw data comes from and so

00:08:41,950 --> 00:08:46,900
the API for spouts is very simple you

00:08:44,320 --> 00:08:48,850
implement something called next tuple on

00:08:46,900 --> 00:08:51,190
some class and it's called repeatedly

00:08:48,850 --> 00:08:52,840
and when that function is called you

00:08:51,190 --> 00:08:54,880
just get back data values and they get

00:08:52,840 --> 00:08:56,610
emitted into your cluster I wrote a

00:08:54,880 --> 00:08:59,740
little Co routine here which basically

00:08:56,610 --> 00:09:02,800
in a tight while loop that just spins is

00:08:59,740 --> 00:09:04,780
grabbing tuples from this API if a tuple

00:09:02,800 --> 00:09:07,270
exists it's actually sending it down

00:09:04,780 --> 00:09:09,130
stream using generator dot send method

00:09:07,270 --> 00:09:11,560
and if it doesn't exist then it just

00:09:09,130 --> 00:09:13,720
sleeps for 10 milliseconds and then re

00:09:11,560 --> 00:09:15,880
retrieves an to pool again and this is

00:09:13,720 --> 00:09:19,510
kind of modeling what storm does when it

00:09:15,880 --> 00:09:20,470
works with your spouts component my way

00:09:19,510 --> 00:09:24,010
how many people here have written

00:09:20,470 --> 00:09:25,540
co-routines before okay so there's

00:09:24,010 --> 00:09:28,300
nothing really fancy about co-routines

00:09:25,540 --> 00:09:29,500
they're nothing more than a functions

00:09:28,300 --> 00:09:32,770
that are wrapped with a special

00:09:29,500 --> 00:09:34,900
decorator that primes them and basically

00:09:32,770 --> 00:09:36,880
in co-routines you can send values into

00:09:34,900 --> 00:09:38,500
a processing pipeline in the same way

00:09:36,880 --> 00:09:41,410
that generator functions let you return

00:09:38,500 --> 00:09:42,850
values out with the yield statement so

00:09:41,410 --> 00:09:44,050
this is where the KO routine part gets

00:09:42,850 --> 00:09:47,140
kind of interesting and then you could

00:09:44,050 --> 00:09:49,540
go look up Co routines later in a bolt a

00:09:47,140 --> 00:09:52,060
bolt is basically a processing stage so

00:09:49,540 --> 00:09:54,430
it gets tuples process gets recalled

00:09:52,060 --> 00:09:56,470
repeatedly with those tuples and then it

00:09:54,430 --> 00:09:58,990
has to either acknowledge them fail them

00:09:56,470 --> 00:10:01,000
or emit new values into your topology

00:09:58,990 --> 00:10:03,760
and the implementation for this could be

00:10:01,000 --> 00:10:05,830
shown with this KO routine here and with

00:10:03,760 --> 00:10:08,410
the interesting line here to look at is

00:10:05,830 --> 00:10:10,780
actually this yield

00:10:08,410 --> 00:10:13,240
in parentheses here so this is yield

00:10:10,780 --> 00:10:15,420
being used as as as an expression and

00:10:13,240 --> 00:10:17,680
basically what that means is this

00:10:15,420 --> 00:10:19,930
co-routine is gonna wait for a value to

00:10:17,680 --> 00:10:21,610
get sent into it when it receives that

00:10:19,930 --> 00:10:23,320
value it's gonna start processing it so

00:10:21,610 --> 00:10:25,660
this is basically saying wait until I

00:10:23,320 --> 00:10:27,130
get a new tuple if I got a weird tuple

00:10:25,660 --> 00:10:29,620
that was none then sleep for 10

00:10:27,130 --> 00:10:32,140
milliseconds if I didn't call process

00:10:29,620 --> 00:10:33,820
repeatedly on the bolt and then at

00:10:32,140 --> 00:10:36,190
apology is nothing more than a wired

00:10:33,820 --> 00:10:37,570
pipeline right wired pipeline of these

00:10:36,190 --> 00:10:41,770
things it's actually a directed acyclic

00:10:37,570 --> 00:10:43,360
graph describing processing stages so in

00:10:41,770 --> 00:10:45,460
if I were to do it with this little Co

00:10:43,360 --> 00:10:47,920
routine example I would just have some

00:10:45,460 --> 00:10:50,140
classes like word spouts word count ball

00:10:47,920 --> 00:10:52,030
to debug print bolt and I'd wire them

00:10:50,140 --> 00:10:53,530
together and then to start off the

00:10:52,030 --> 00:10:55,660
processing pipeline I would just call

00:10:53,530 --> 00:10:57,640
Python next and that would initiate a

00:10:55,660 --> 00:11:01,060
co-routine pipeline against all of that

00:10:57,640 --> 00:11:03,010
data so that's kind of that's kind of by

00:11:01,060 --> 00:11:04,600
analogy how storm works it lets you set

00:11:03,010 --> 00:11:05,740
up processing pipelines the same way

00:11:04,600 --> 00:11:07,690
that you might set them up using

00:11:05,740 --> 00:11:10,540
co-routines and python but it does this

00:11:07,690 --> 00:11:12,010
at a cluster scale so the other thing

00:11:10,540 --> 00:11:13,480
that's kind of cool is storm has a bunch

00:11:12,010 --> 00:11:15,340
of internals that make it work really

00:11:13,480 --> 00:11:17,560
well for processing large streams of

00:11:15,340 --> 00:11:19,630
data one notion that's in storm is

00:11:17,560 --> 00:11:21,670
called a tuple tree and the tuple tree

00:11:19,630 --> 00:11:23,500
is that if you start with a sentence for

00:11:21,670 --> 00:11:24,010
example like the cow jumped over the

00:11:23,500 --> 00:11:26,350
moon

00:11:24,010 --> 00:11:28,180
and you have a bolt that actually

00:11:26,350 --> 00:11:30,520
tokenize is that sentence and turns it

00:11:28,180 --> 00:11:33,880
into words and emits those it'll keep

00:11:30,520 --> 00:11:36,820
track of the and ancestry of that data

00:11:33,880 --> 00:11:39,250
so it'll know that the cow that happened

00:11:36,820 --> 00:11:41,590
in processing stage 2 actually came from

00:11:39,250 --> 00:11:43,870
the sentence and processing stage 1 and

00:11:41,590 --> 00:11:46,240
then if a later stage does a word count

00:11:43,870 --> 00:11:48,370
word notices that cow only occurred once

00:11:46,240 --> 00:11:50,230
in your data it'll also keep track of

00:11:48,370 --> 00:11:52,390
the fact that there was a tokenized word

00:11:50,230 --> 00:11:54,520
cow and a sentence that originated it

00:11:52,390 --> 00:11:57,820
and that'll be important later when I

00:11:54,520 --> 00:11:59,140
describe how reliability works now the

00:11:57,820 --> 00:12:01,210
other thing that storm does is because

00:11:59,140 --> 00:12:03,240
you can partition your processing across

00:12:01,210 --> 00:12:05,350
your cluster it has to give you a way to

00:12:03,240 --> 00:12:07,480
organize the data that flows between

00:12:05,350 --> 00:12:10,360
parts of your cluster group data

00:12:07,480 --> 00:12:11,560
together on some key and also parallel

00:12:10,360 --> 00:12:13,690
lies different components in your

00:12:11,560 --> 00:12:15,820
cluster and so in this kind of word

00:12:13,690 --> 00:12:17,680
count example again I might have two

00:12:15,820 --> 00:12:20,080
components a word spout and a word count

00:12:17,680 --> 00:12:22,860
bolt and basically the input to the word

00:12:20,080 --> 00:12:25,500
spout is nothing the output is the word

00:12:22,860 --> 00:12:27,540
count bolt the tuple is the word like

00:12:25,500 --> 00:12:29,880
dog and the tuple that comes out of word

00:12:27,540 --> 00:12:33,150
count bolt is the count which is dog and

00:12:29,880 --> 00:12:34,710
for let's say the stream is is word and

00:12:33,150 --> 00:12:36,600
the stream on the word count bolt is

00:12:34,710 --> 00:12:38,460
word and count the grouping is

00:12:36,600 --> 00:12:40,680
interesting so basically what grouping

00:12:38,460 --> 00:12:42,540
lets me do is say when you omit words

00:12:40,680 --> 00:12:44,970
make sure the same words always go to

00:12:42,540 --> 00:12:48,270
the same bolt so for example if I group

00:12:44,970 --> 00:12:50,700
on word it'll say every time you get dog

00:12:48,270 --> 00:12:52,290
make sure dog gets hashed to arrive at

00:12:50,700 --> 00:12:54,930
the same bolt later in the processing

00:12:52,290 --> 00:12:57,360
stage and parallelism is also tunable

00:12:54,930 --> 00:12:59,550
per component so in this example here I

00:12:57,360 --> 00:13:01,200
might have parallelism of two forward

00:12:59,550 --> 00:13:02,910
spouts and parallelism of eight for the

00:13:01,200 --> 00:13:04,740
word count bolt and that means that it's

00:13:02,910 --> 00:13:08,460
using eight Python processes on the

00:13:04,740 --> 00:13:10,410
right and two on the left now storm also

00:13:08,460 --> 00:13:12,810
has a high availability model which is

00:13:10,410 --> 00:13:14,580
pretty great what what's going on there

00:13:12,810 --> 00:13:17,010
is that storm actually split up into

00:13:14,580 --> 00:13:19,170
several cluster components which are

00:13:17,010 --> 00:13:21,510
called the Nimbus the supervisor the

00:13:19,170 --> 00:13:23,520
worker and then there's a zookeeper

00:13:21,510 --> 00:13:25,700
cluster for coordinating cluster state

00:13:23,520 --> 00:13:28,200
so when you actually deploy a storm

00:13:25,700 --> 00:13:30,480
topology you deploy it to this thing

00:13:28,200 --> 00:13:32,670
called the Nimbus it sends your code out

00:13:30,480 --> 00:13:34,080
to the rest of the workers and the

00:13:32,670 --> 00:13:35,940
worker processes are the ones that

00:13:34,080 --> 00:13:37,530
actually operate on your code they also

00:13:35,940 --> 00:13:40,350
give you a little UI that lets you

00:13:37,530 --> 00:13:42,030
monitor what's happening the workers all

00:13:40,350 --> 00:13:44,490
are able to stay online through high

00:13:42,030 --> 00:13:45,600
availability with zookeeper and that's

00:13:44,490 --> 00:13:46,910
kind of just handled for you

00:13:45,600 --> 00:13:49,290
automatically

00:13:46,910 --> 00:13:51,330
now worker nodes are interesting because

00:13:49,290 --> 00:13:53,730
they basically are like processing slots

00:13:51,330 --> 00:13:55,980
in your cluster so when you first deploy

00:13:53,730 --> 00:13:57,660
a storm cluster you're going to have a

00:13:55,980 --> 00:14:00,540
bunch of worker nodes in this example

00:13:57,660 --> 00:14:02,760
for and there's going to be empty slots

00:14:00,540 --> 00:14:05,700
for each worker node and when you deploy

00:14:02,760 --> 00:14:08,370
a topology a certain topology may occupy

00:14:05,700 --> 00:14:09,750
certain slots across your cluster and at

00:14:08,370 --> 00:14:12,660
some point you might end up getting

00:14:09,750 --> 00:14:15,030
unlucky and you're certain parts your

00:14:12,660 --> 00:14:16,860
topology may occupy certain slots on one

00:14:15,030 --> 00:14:18,510
physical node but it doesn't really

00:14:16,860 --> 00:14:21,210
matter because you can rebalance your

00:14:18,510 --> 00:14:23,480
storm topology and and basically get

00:14:21,210 --> 00:14:26,010
balanced computation across your cluster

00:14:23,480 --> 00:14:28,350
so all this stuff I just described about

00:14:26,010 --> 00:14:30,270
storm really briefly at a high level is

00:14:28,350 --> 00:14:31,620
also covered in a great book that just

00:14:30,270 --> 00:14:33,390
came out for Manning called storm

00:14:31,620 --> 00:14:35,370
applied I wrote the foreword for this

00:14:33,390 --> 00:14:36,240
book but the authors work at the ladders

00:14:35,370 --> 00:14:38,820
and they've done a great

00:14:36,240 --> 00:14:41,070
job covering storm internals and some of

00:14:38,820 --> 00:14:43,950
these diagrams came from there too

00:14:41,070 --> 00:14:46,380
yeah so storm is sort of amazing as

00:14:43,950 --> 00:14:48,480
infrastructure it'll actually guarantee

00:14:46,380 --> 00:14:50,820
processing of your data via tuple trees

00:14:48,480 --> 00:14:52,920
it can do tunable parallelism per

00:14:50,820 --> 00:14:55,470
processing stage it implements high

00:14:52,920 --> 00:14:57,839
availability when you use it with Python

00:14:55,470 --> 00:15:00,149
it allocates Python process slots on

00:14:57,839 --> 00:15:01,830
physical nodes for you it can help you

00:15:00,149 --> 00:15:03,720
rebalance computation across your

00:15:01,830 --> 00:15:06,180
cluster and it can handle network

00:15:03,720 --> 00:15:07,800
messaging automatically and when you use

00:15:06,180 --> 00:15:09,510
it with Python you're basically working

00:15:07,800 --> 00:15:11,700
around the Gil because you get process

00:15:09,510 --> 00:15:14,880
level parallelism without having to deal

00:15:11,700 --> 00:15:17,180
with async programming or multi thread

00:15:14,880 --> 00:15:19,350
concurrency or anything like that so

00:15:17,180 --> 00:15:21,540
let's get let's get there let's actually

00:15:19,350 --> 00:15:26,220
get to this loaded up cluster that I

00:15:21,540 --> 00:15:28,380
showed before so so the way that storm

00:15:26,220 --> 00:15:29,970
works with Python is through a protocol

00:15:28,380 --> 00:15:32,370
that's called the multi Lange protocol

00:15:29,970 --> 00:15:34,529
the multi-line protocol is basically a

00:15:32,370 --> 00:15:37,080
simple JSON protocol that works over

00:15:34,529 --> 00:15:38,970
shell based components you communicate

00:15:37,080 --> 00:15:41,339
over standard in and standard out it's

00:15:38,970 --> 00:15:43,950
very clean and UNIX II for people who

00:15:41,339 --> 00:15:45,959
have used like SPARC for example SPARC

00:15:43,950 --> 00:15:48,149
tries to implement stuff like this with

00:15:45,959 --> 00:15:50,100
like weird hacks that go into the JVM

00:15:48,149 --> 00:15:52,470
that's not what storm chose to do

00:15:50,100 --> 00:15:55,170
instead it's just a very clean almost

00:15:52,470 --> 00:15:57,420
unix-like a system where you communicate

00:15:55,170 --> 00:15:59,640
JSON data coming out of your processes

00:15:57,420 --> 00:16:01,380
and you receive JSON data in it's a

00:15:59,640 --> 00:16:04,380
little bit quirky but it was easy enough

00:16:01,380 --> 00:16:06,480
for us to implement each component in a

00:16:04,380 --> 00:16:08,610
Python storm topology is either a shell

00:16:06,480 --> 00:16:10,649
spout or a shell bolt and Java

00:16:08,610 --> 00:16:12,810
implementations speak to Python via JSON

00:16:10,649 --> 00:16:15,390
and the result of this is that there's

00:16:12,810 --> 00:16:17,040
one Python process per storm task so

00:16:15,390 --> 00:16:19,110
when you set your parallelism to eight

00:16:17,040 --> 00:16:22,440
you actually get eight Python processes

00:16:19,110 --> 00:16:23,910
on your machine and there's kind of some

00:16:22,440 --> 00:16:25,950
data interchange that happens between

00:16:23,910 --> 00:16:28,290
Python and JVM this is like a little

00:16:25,950 --> 00:16:30,810
illustration where when a storm topology

00:16:28,290 --> 00:16:32,730
boots up you have a handshake that

00:16:30,810 --> 00:16:35,399
happens between your Python process and

00:16:32,730 --> 00:16:37,620
the JVM and then data either stays

00:16:35,399 --> 00:16:39,300
within your Python process or moves to

00:16:37,620 --> 00:16:41,399
other nodes depending on whether that

00:16:39,300 --> 00:16:43,470
where that data needs to go and there's

00:16:41,399 --> 00:16:45,870
also things like heartbeat heart beating

00:16:43,470 --> 00:16:47,970
mechanisms to keep tabs on whether your

00:16:45,870 --> 00:16:49,800
Python process is still alive or whether

00:16:47,970 --> 00:16:53,640
storm needs to kill it and restart it

00:16:49,800 --> 00:16:56,010
things like that so storm actually had a

00:16:53,640 --> 00:16:58,709
Python implementation bundled with it it

00:16:56,010 --> 00:17:00,149
was called storm drop hi the problem was

00:16:58,709 --> 00:17:02,399
it wasn't pythonic at all it was

00:17:00,149 --> 00:17:04,620
basically the single module written as a

00:17:02,399 --> 00:17:07,650
proof of concept and you were supposed

00:17:04,620 --> 00:17:09,059
to copy paste that module into a jar

00:17:07,650 --> 00:17:11,730
that you would submit to your storm

00:17:09,059 --> 00:17:12,809
cluster like they kind of assumed when

00:17:11,730 --> 00:17:15,300
you were using this that you were

00:17:12,809 --> 00:17:17,250
primarily a Java programmer who maybe

00:17:15,300 --> 00:17:19,110
have like JetBrains or something open

00:17:17,250 --> 00:17:21,540
and you would copy storm dot pi into

00:17:19,110 --> 00:17:24,120
your Java project and thus be able to

00:17:21,540 --> 00:17:25,650
bridge over to C Python so that was like

00:17:24,120 --> 00:17:28,679
the first thing that we thought we

00:17:25,650 --> 00:17:30,179
should fix but in general what we did

00:17:28,679 --> 00:17:32,429
with stream parts is we really turned

00:17:30,179 --> 00:17:34,350
storm into infrastructure the way I look

00:17:32,429 --> 00:17:36,030
at this is that a lot of people in this

00:17:34,350 --> 00:17:38,070
room probably use technologies like

00:17:36,030 --> 00:17:39,990
Cassandra and elasticsearch with Python

00:17:38,070 --> 00:17:41,510
and they do that comfortably even though

00:17:39,990 --> 00:17:44,190
those technologies are written in Java

00:17:41,510 --> 00:17:45,630
but probably most people don't use storm

00:17:44,190 --> 00:17:47,340
because it feels too much like a Java

00:17:45,630 --> 00:17:49,410
tool and what we're trying to do with

00:17:47,340 --> 00:17:51,900
stream parts is make it that Python is a

00:17:49,410 --> 00:17:53,940
first-class citizen in that community

00:17:51,900 --> 00:17:56,880
and we also want to fix what we're

00:17:53,940 --> 00:18:00,210
calling joven onyx which is the fact

00:17:56,880 --> 00:18:02,370
that unlike pythonic code a lot of code

00:18:00,210 --> 00:18:04,530
has like a java nanak feel where like it

00:18:02,370 --> 00:18:06,960
really is built primarily by those

00:18:04,530 --> 00:18:09,150
idiomatic Java programmers and certain

00:18:06,960 --> 00:18:11,340
things like using maven for packaging

00:18:09,150 --> 00:18:13,860
seem very familiar in the Java community

00:18:11,340 --> 00:18:16,860
but seem very foreign in the Python

00:18:13,860 --> 00:18:18,270
community so now I'll dive into stream

00:18:16,860 --> 00:18:20,480
parsing give you an overview of that as

00:18:18,270 --> 00:18:20,480
well

00:18:20,550 --> 00:18:25,110
so the initial release for stream parse

00:18:22,380 --> 00:18:27,059
was April 2014 we've done about a year

00:18:25,110 --> 00:18:30,330
of active development on it it has about

00:18:27,059 --> 00:18:33,630
600 stars on github it's got 90 plus

00:18:30,330 --> 00:18:35,010
mailing list members on Google Groups we

00:18:33,630 --> 00:18:37,230
got five new committers through the

00:18:35,010 --> 00:18:38,520
initial release and there's three

00:18:37,230 --> 00:18:40,380
parsley engineers who are like

00:18:38,520 --> 00:18:42,210
maintaining it as part of their daily

00:18:40,380 --> 00:18:43,950
work and we actually even got funding

00:18:42,210 --> 00:18:45,390
from dark bye to work on it and open

00:18:43,950 --> 00:18:48,000
source all of our work which is pretty

00:18:45,390 --> 00:18:51,690
cool and dark has actually been funding

00:18:48,000 --> 00:18:53,160
a lot of big data stuff lately so stream

00:18:51,690 --> 00:18:55,580
parse when you install it what you get

00:18:53,160 --> 00:18:58,350
is this API this command-line interface

00:18:55,580 --> 00:19:00,510
which is called s parse or sparse and

00:18:58,350 --> 00:19:02,490
what happens is you pip install stream

00:19:00,510 --> 00:19:03,240
parse and you say s parse quicks quick

00:19:02,490 --> 00:19:04,980
start and

00:19:03,240 --> 00:19:07,830
kind of like a Django QuickStart you get

00:19:04,980 --> 00:19:09,600
a storm project ready to go in Python it

00:19:07,830 --> 00:19:13,650
actually generates that word count

00:19:09,600 --> 00:19:15,390
example that I walk through earlier the

00:19:13,650 --> 00:19:17,730
only dependency for stream parse is a

00:19:15,390 --> 00:19:19,470
tool called line again or line which is

00:19:17,730 --> 00:19:22,020
basically a closure tool that knows how

00:19:19,470 --> 00:19:23,730
to resolve Java dependencies so if you

00:19:22,020 --> 00:19:25,470
have that tool installed stream parse

00:19:23,730 --> 00:19:27,240
connects to it and is able to like

00:19:25,470 --> 00:19:28,530
download storm download zookeeper

00:19:27,240 --> 00:19:31,350
download everything that it needs

00:19:28,530 --> 00:19:33,510
if you then run esparza run out of the

00:19:31,350 --> 00:19:36,660
directory created by the QuickStart you

00:19:33,510 --> 00:19:39,360
actually get a locally running storm

00:19:36,660 --> 00:19:41,280
topology in memory that runs your code

00:19:39,360 --> 00:19:43,620
exactly the same way it would run in

00:19:41,280 --> 00:19:45,660
storm remotely so you just run s parse

00:19:43,620 --> 00:19:47,820
run that's it it takes about a second

00:19:45,660 --> 00:19:49,230
and a word count topology is running

00:19:47,820 --> 00:19:50,610
locally on your machine and it's already

00:19:49,230 --> 00:19:53,280
beating the Gil it's already running

00:19:50,610 --> 00:19:55,710
multiple Python processes coordinating

00:19:53,280 --> 00:19:57,690
with JSON and all that stuff all the

00:19:55,710 --> 00:19:59,070
output comes out in your terminal and I

00:19:57,690 --> 00:20:01,620
actually include a link in the slides

00:19:59,070 --> 00:20:03,210
here to a live demo I gave that was

00:20:01,620 --> 00:20:06,210
recorded on YouTube that kind of shows

00:20:03,210 --> 00:20:07,980
all of it working I decided not to risk

00:20:06,210 --> 00:20:11,070
it here because of Wi-Fi issues and

00:20:07,980 --> 00:20:13,350
stuff right so submitting to a remote

00:20:11,070 --> 00:20:15,720
cluster is just as easy you just type s

00:20:13,350 --> 00:20:18,030
parse submit magically it does

00:20:15,720 --> 00:20:20,120
everything necessary to get your cluster

00:20:18,030 --> 00:20:23,010
up and running on a remote storm cluster

00:20:20,120 --> 00:20:24,870
it makes virtual ends on every machine

00:20:23,010 --> 00:20:27,179
in your cluster so that you have

00:20:24,870 --> 00:20:28,590
isolated dependencies there it builds a

00:20:27,179 --> 00:20:29,760
jar out of your source code because

00:20:28,590 --> 00:20:32,580
that's what storm' requires

00:20:29,760 --> 00:20:34,620
unfortunately it opens an ssh reverse

00:20:32,580 --> 00:20:36,870
tunnel to your Nimbus node in your storm

00:20:34,620 --> 00:20:38,580
cluster and it talks to your Nimbus node

00:20:36,870 --> 00:20:40,559
in order to submit your jar there and

00:20:38,580 --> 00:20:42,750
all of that basically gets your topology

00:20:40,559 --> 00:20:45,090
up and running in just a few seconds so

00:20:42,750 --> 00:20:47,460
whether you've got a 5 node storm

00:20:45,090 --> 00:20:49,440
cluster or a 500 node storm cluster you

00:20:47,460 --> 00:20:53,450
can get your production code going from

00:20:49,440 --> 00:20:55,470
local to remote in just a few seconds

00:20:53,450 --> 00:20:57,660
I'm not going to go through this but

00:20:55,470 --> 00:20:59,280
basically stream parts also improved a

00:20:57,660 --> 00:21:02,400
whole lot of other things related to

00:20:59,280 --> 00:21:05,490
storm pie in the IPC layer like we wrote

00:21:02,400 --> 00:21:07,850
tests for example we added logging

00:21:05,490 --> 00:21:10,650
support we have it running in Travis CI

00:21:07,850 --> 00:21:12,390
so you know it's an actually maintained

00:21:10,650 --> 00:21:14,820
open source project instead of like a

00:21:12,390 --> 00:21:17,140
little hack oh and by the way it also

00:21:14,820 --> 00:21:19,180
supports Python 3.4 and Pi

00:21:17,140 --> 00:21:20,350
which is great what the the bundled.one

00:21:19,180 --> 00:21:22,360
doesn't support either of those

00:21:20,350 --> 00:21:25,990
unfortunately so now let's make a

00:21:22,360 --> 00:21:28,330
topology so in order to let you make

00:21:25,990 --> 00:21:30,190
topologies and specify them we take

00:21:28,330 --> 00:21:32,740
advantage of a DSL that's bundled with

00:21:30,190 --> 00:21:34,600
storm I'm not gonna cover the details of

00:21:32,740 --> 00:21:36,310
this DSL but the fraction of the code

00:21:34,600 --> 00:21:39,070
that you need to specify your topology

00:21:36,310 --> 00:21:40,780
is so small relative to the code that's

00:21:39,070 --> 00:21:42,220
written in Python that I don't think

00:21:40,780 --> 00:21:44,410
it's like worth covering it in too much

00:21:42,220 --> 00:21:46,150
detail you can for the most part follow

00:21:44,410 --> 00:21:47,830
the examples in our read the docs

00:21:46,150 --> 00:21:49,960
documentation and always get there

00:21:47,830 --> 00:21:51,520
roughly what's going on here is that I'm

00:21:49,960 --> 00:21:54,160
creating something called a word spout

00:21:51,520 --> 00:21:56,200
it has a certain Python class which is

00:21:54,160 --> 00:21:59,320
implementing that stream par spout

00:21:56,200 --> 00:22:01,060
interface and I specify the stream so

00:21:59,320 --> 00:22:03,970
the fact that this emits a tuple with a

00:22:01,060 --> 00:22:05,830
field called word I then implement the

00:22:03,970 --> 00:22:08,770
spout which is simple Python code like

00:22:05,830 --> 00:22:10,870
this and from stream parse import spouts

00:22:08,770 --> 00:22:12,820
create a class called words which

00:22:10,870 --> 00:22:14,980
subclasses it and then I only have to

00:22:12,820 --> 00:22:17,320
implement two methods initialize in next

00:22:14,980 --> 00:22:19,180
tuple and next tuple like I mentioned

00:22:17,320 --> 00:22:21,070
before is called repeatedly by the storm

00:22:19,180 --> 00:22:23,590
cluster so in this example I'm just

00:22:21,070 --> 00:22:25,600
omitting one word tuples from an endless

00:22:23,590 --> 00:22:29,230
generator that's cycling through these

00:22:25,600 --> 00:22:30,820
four words by the way what example of

00:22:29,230 --> 00:22:32,770
data processing wouldn't be complete

00:22:30,820 --> 00:22:34,660
without extensive focus on word count

00:22:32,770 --> 00:22:38,170
I'm sorry for that but it just actually

00:22:34,660 --> 00:22:39,580
does fit in slides nicely so to specify

00:22:38,170 --> 00:22:41,740
the boat that's going to count these

00:22:39,580 --> 00:22:44,440
words you have to do a little bit more

00:22:41,740 --> 00:22:46,150
work you have to say my input is word

00:22:44,440 --> 00:22:48,370
spout which I SPECT out on the last

00:22:46,150 --> 00:22:51,190
slide and I'm gonna group on the word

00:22:48,370 --> 00:22:53,710
field the class is again the class

00:22:51,190 --> 00:22:56,350
implemented with stream parse and then I

00:22:53,710 --> 00:22:58,390
want to be able to omit the word

00:22:56,350 --> 00:23:00,400
and the count and here I'm actually

00:22:58,390 --> 00:23:02,050
specifying the parallelism I I left it

00:23:00,400 --> 00:23:04,780
out the default is just one when you

00:23:02,050 --> 00:23:06,460
don't specify it on the last slide but

00:23:04,780 --> 00:23:07,660
here I specify parallelism of 2 which

00:23:06,460 --> 00:23:10,240
says it's going to use 2 python

00:23:07,660 --> 00:23:12,850
processes for this and again the bolt

00:23:10,240 --> 00:23:15,280
fits on a slide as well from stream

00:23:12,850 --> 00:23:17,380
parse import bolts initialize method

00:23:15,280 --> 00:23:18,820
then there's a process method and in

00:23:17,380 --> 00:23:21,300
this case the process method actually

00:23:18,820 --> 00:23:23,980
fetches the word out of the tuple it

00:23:21,300 --> 00:23:26,110
increments a in-memory counter with

00:23:23,980 --> 00:23:27,880
collections counter and then it uses

00:23:26,110 --> 00:23:30,040
storm logging to just log the current

00:23:27,880 --> 00:23:31,750
count of the word

00:23:30,040 --> 00:23:34,030
and so this assumes that grouping is

00:23:31,750 --> 00:23:36,910
working because the in-memory counter

00:23:34,030 --> 00:23:40,000
for a dog let's say is going to always

00:23:36,910 --> 00:23:43,630
receive every new dog tuple that comes

00:23:40,000 --> 00:23:45,160
into the topology another cool thing we

00:23:43,630 --> 00:23:47,890
implemented in-stream parse is that we

00:23:45,160 --> 00:23:49,750
actually have a batching bolt for

00:23:47,890 --> 00:23:51,370
performance so the batching bolt

00:23:49,750 --> 00:23:53,620
actually has a background thread that

00:23:51,370 --> 00:23:55,660
will batch your tuples together in

00:23:53,620 --> 00:23:57,370
whatever number of seconds you specify

00:23:55,660 --> 00:23:59,580
and this can dramatically improve

00:23:57,370 --> 00:24:02,710
performance especially for things like

00:23:59,580 --> 00:24:04,690
ETL use cases where for example you're

00:24:02,710 --> 00:24:07,540
writing to cassandra elasticsearch or

00:24:04,690 --> 00:24:11,980
some other database in back in a batch

00:24:07,540 --> 00:24:14,230
duay tends to be faster stream parse

00:24:11,980 --> 00:24:15,880
also has a simple JSON configuration

00:24:14,230 --> 00:24:18,580
format that lets you specify multiple

00:24:15,880 --> 00:24:21,550
storm environments so that's like this

00:24:18,580 --> 00:24:23,290
is specifying here a storm 0.8

00:24:21,550 --> 00:24:25,150
environment this is specifying a vagrant

00:24:23,290 --> 00:24:27,550
environment and you can switch between

00:24:25,150 --> 00:24:30,730
those using command line flags so it

00:24:27,550 --> 00:24:32,260
just works and stream parts has a whole

00:24:30,730 --> 00:24:34,090
lot of other commands too like you can

00:24:32,260 --> 00:24:36,910
list the topologies in your cluster

00:24:34,090 --> 00:24:39,730
killer running topology tail log files

00:24:36,910 --> 00:24:41,230
on your remote servers and we're adding

00:24:39,730 --> 00:24:44,950
more and more every day and actually a

00:24:41,230 --> 00:24:46,360
very open to taking pull requests so to

00:24:44,950 --> 00:24:48,340
close up and then I'll open it up for

00:24:46,360 --> 00:24:50,650
questions I also just wanted to mention

00:24:48,340 --> 00:24:52,570
that a lot of people use Kafka here

00:24:50,650 --> 00:24:55,450
together with storm who here has heard

00:24:52,570 --> 00:24:57,310
of Kafka or uses it yeah so Kafka

00:24:55,450 --> 00:24:59,230
traditionally hasn't had a great Python

00:24:57,310 --> 00:25:00,970
library so we decided to take one we

00:24:59,230 --> 00:25:04,480
wrote internally about a year ago and

00:25:00,970 --> 00:25:07,180
poured it to 0.8 and that's now live at

00:25:04,480 --> 00:25:07,930
PI Kafka at the parsley github

00:25:07,180 --> 00:25:10,330
repository

00:25:07,930 --> 00:25:12,190
it actually supports balanced consuming

00:25:10,330 --> 00:25:13,810
which is a really important feature for

00:25:12,190 --> 00:25:16,570
getting it to work well with storm at

00:25:13,810 --> 00:25:18,220
scale so we're also looking for help

00:25:16,570 --> 00:25:20,500
with that module if you want to find me

00:25:18,220 --> 00:25:22,180
at the sprints so I'm all done I left a

00:25:20,500 --> 00:25:24,130
few minutes open for questions I'm

00:25:22,180 --> 00:25:27,040
probably gonna be sprinting on Monday

00:25:24,130 --> 00:25:28,480
and Tuesday on writing a Python DSL for

00:25:27,040 --> 00:25:29,650
storm topologies which I think will be a

00:25:28,480 --> 00:25:31,240
lot of fun

00:25:29,650 --> 00:25:34,680
but I hope that this talk was helpful

00:25:31,240 --> 00:25:34,680
and hope to hear from you guys

00:25:40,560 --> 00:25:43,630
okay thanks a lot

00:25:42,280 --> 00:25:50,140
Andrew if anyone has any questions

00:25:43,630 --> 00:25:51,940
please feel free to come up I could talk

00:25:50,140 --> 00:25:54,540
Thanks yeah um quick question does this

00:25:51,940 --> 00:25:58,210
thing handle deploying Python

00:25:54,540 --> 00:26:01,330
dependencies with with the bundle as a

00:25:58,210 --> 00:26:02,560
deploys to the storm notes I yeah that's

00:26:01,330 --> 00:26:05,380
a good question so you asked whether it

00:26:02,560 --> 00:26:07,210
handles deploying Python dependencies in

00:26:05,380 --> 00:26:09,640
the jar that goes out to the storm

00:26:07,210 --> 00:26:11,830
cluster it doesn't do that instead the

00:26:09,640 --> 00:26:13,630
approach we chose to take as we SSH into

00:26:11,830 --> 00:26:15,820
the cluster nodes and we maintain

00:26:13,630 --> 00:26:18,100
virtual ends that correspond to the

00:26:15,820 --> 00:26:20,740
topology and we update the dependencies

00:26:18,100 --> 00:26:22,660
live on those servers before actually

00:26:20,740 --> 00:26:23,980
submitting the topology jar so by the

00:26:22,660 --> 00:26:25,180
time the code reaches the server it

00:26:23,980 --> 00:26:27,670
expects that those dependencies exist

00:26:25,180 --> 00:26:28,810
exams started inside those processes are

00:26:27,670 --> 00:26:29,530
started inside of that virtual end

00:26:28,810 --> 00:26:34,030
exactly

00:26:29,530 --> 00:26:35,920
yeah well thank you no problem hi Andrew

00:26:34,030 --> 00:26:37,030
thanks for the great talk um could you

00:26:35,920 --> 00:26:38,640
speak a little bit about the

00:26:37,030 --> 00:26:41,170
similarities and differences between

00:26:38,640 --> 00:26:43,750
this and something like Yelp sparse

00:26:41,170 --> 00:26:47,230
library yeah so yobs Peleus library

00:26:43,750 --> 00:26:48,940
actually came out around a few months

00:26:47,230 --> 00:26:50,530
after we released this and we talked to

00:26:48,940 --> 00:26:54,280
the community there and they have a lot

00:26:50,530 --> 00:26:56,320
of overlap actually so I had a couple of

00:26:54,280 --> 00:26:57,760
I have like a kind of couple slides in

00:26:56,320 --> 00:26:58,860
the appendix that cover that which you

00:26:57,760 --> 00:27:01,540
can find online

00:26:58,860 --> 00:27:03,610
but essentially Pylea also has a

00:27:01,540 --> 00:27:05,530
command-line tool called pi leus which

00:27:03,610 --> 00:27:06,940
is handling the packaging part it

00:27:05,530 --> 00:27:09,310
handles packaging a little bit

00:27:06,940 --> 00:27:12,370
differently than stream parse does and

00:27:09,310 --> 00:27:14,320
the IPC layer is close to identical so

00:27:12,370 --> 00:27:16,030
I'm actually going to probably propose

00:27:14,320 --> 00:27:19,000
to the PI leus community that we like

00:27:16,030 --> 00:27:20,830
try to share code on the IPC part we

00:27:19,000 --> 00:27:23,320
actually registered a project called PI

00:27:20,830 --> 00:27:25,990
storm which we're planning to move the

00:27:23,320 --> 00:27:28,030
IPC part only into that and then we

00:27:25,990 --> 00:27:31,330
could have you know competing CL eyes

00:27:28,030 --> 00:27:32,770
with a unified IPC ideally so we're

00:27:31,330 --> 00:27:34,980
working on that in the community but

00:27:32,770 --> 00:27:42,010
it's just parallel development efforts

00:27:34,980 --> 00:27:43,630
cool things no problem awesome talk so

00:27:42,010 --> 00:27:46,350
I've been experimenting with storm and

00:27:43,630 --> 00:27:49,240
this is like a storm question but sure

00:27:46,350 --> 00:27:51,850
how do you so you mentioned that stream

00:27:49,240 --> 00:27:53,380
forces logging and a lot of times when

00:27:51,850 --> 00:27:54,550
your shell when your bolts die there's

00:27:53,380 --> 00:27:56,760
like kind of hard to figure out what's

00:27:54,550 --> 00:27:59,500
going on like get that introspection so

00:27:56,760 --> 00:28:01,720
what how does steam like how does stream

00:27:59,500 --> 00:28:03,100
parts use like the normal Python logging

00:28:01,720 --> 00:28:05,040
module and like how do you get that

00:28:03,100 --> 00:28:07,660
introspection instead of going through

00:28:05,040 --> 00:28:08,170
storm UI or like through the zookeeper

00:28:07,660 --> 00:28:09,880
nodes

00:28:08,170 --> 00:28:11,350
that's a great question yeah so this is

00:28:09,880 --> 00:28:13,870
something that we improved dramatically

00:28:11,350 --> 00:28:16,030
in the last six months in stream parse

00:28:13,870 --> 00:28:18,790
so we actually make use of Python

00:28:16,030 --> 00:28:22,030
logging in the stream parse IPC module

00:28:18,790 --> 00:28:24,850
and we set up a set of Python log files

00:28:22,030 --> 00:28:27,100
that are in the scratch area specified

00:28:24,850 --> 00:28:29,110
in your stream parse config so on every

00:28:27,100 --> 00:28:31,690
node there are Python log files there

00:28:29,110 --> 00:28:33,940
and that's important because first of

00:28:31,690 --> 00:28:35,890
all we redirect standard out or we

00:28:33,940 --> 00:28:38,380
redirect essentially sis not standard

00:28:35,890 --> 00:28:41,320
out to Python logging because of the

00:28:38,380 --> 00:28:43,540
fact that the multi langdon all itself

00:28:41,320 --> 00:28:45,100
uses standard out the other reason

00:28:43,540 --> 00:28:47,380
that's important is because we have an S

00:28:45,100 --> 00:28:49,720
parse tail command which can go to your

00:28:47,380 --> 00:28:51,670
cluster and get the most recent logs and

00:28:49,720 --> 00:28:53,860
that tail command knows how to find the

00:28:51,670 --> 00:28:56,110
Python log files on your storm worker

00:28:53,860 --> 00:28:59,230
nodes so if you just run s parse tail

00:28:56,110 --> 00:29:01,210
you can tail the logs and if your storm

00:28:59,230 --> 00:29:03,100
worker crashes for some reason you'll

00:29:01,210 --> 00:29:05,440
see the exception right there in the log

00:29:03,100 --> 00:29:07,110
files although a lot of people on my

00:29:05,440 --> 00:29:09,520
team are pretty interested in improving

00:29:07,110 --> 00:29:11,620
logging and debugging broadly and that's

00:29:09,520 --> 00:29:18,760
one of our roadmap items for this year

00:29:11,620 --> 00:29:21,880
pretty much thanks yeah it's more of a

00:29:18,760 --> 00:29:24,310
storm question but sure last time I

00:29:21,880 --> 00:29:27,520
looked at this stuff reliable processing

00:29:24,310 --> 00:29:29,680
was iffy basically if something fails

00:29:27,520 --> 00:29:32,800
can guess rollback there's a good chance

00:29:29,680 --> 00:29:38,110
it's going to fail again and continue

00:29:32,800 --> 00:29:40,990
doing it to your die and in since you

00:29:38,110 --> 00:29:42,760
have a bit more more layers there the

00:29:40,990 --> 00:29:46,330
chances of this chance of this happening

00:29:42,760 --> 00:29:48,790
are higher so is anything happening in

00:29:46,330 --> 00:29:50,500
this regard instrument general and in

00:29:48,790 --> 00:29:52,630
your project in particular yeah it's a

00:29:50,500 --> 00:29:52,990
great question so reliable processing is

00:29:52,630 --> 00:29:55,360
just

00:29:52,990 --> 00:29:57,309
to get right of course I think the

00:29:55,360 --> 00:30:00,040
strategy in the community right now is

00:29:57,309 --> 00:30:02,980
that there's a really high usage of

00:30:00,040 --> 00:30:04,960
Kafka with storm together and at the

00:30:02,980 --> 00:30:06,850
very least you can guarantee with Kafka

00:30:04,960 --> 00:30:09,490
that your raw data stream is like

00:30:06,850 --> 00:30:11,890
maintained for some SLA so that you can

00:30:09,490 --> 00:30:13,590
replay those tuples at the source and if

00:30:11,890 --> 00:30:16,270
you write your spouts the right way

00:30:13,590 --> 00:30:17,710
implementing the reliability semantics

00:30:16,270 --> 00:30:20,650
that storm provides which are pretty

00:30:17,710 --> 00:30:22,450
easy to implement actually then you can

00:30:20,650 --> 00:30:24,309
get some guarantees that you'll replay

00:30:22,450 --> 00:30:26,110
that data you are right that like there

00:30:24,309 --> 00:30:27,880
are many failure scenarios where

00:30:26,110 --> 00:30:30,610
replaying won't buy you anything because

00:30:27,880 --> 00:30:32,260
if your code is broken it's gonna still

00:30:30,610 --> 00:30:34,210
be broken when you replay the data back

00:30:32,260 --> 00:30:36,730
into it after it crashes a hundred times

00:30:34,210 --> 00:30:38,740
right but what I think storm is trying

00:30:36,730 --> 00:30:40,990
to fix and which we have found in

00:30:38,740 --> 00:30:43,330
production is true is it's trying to

00:30:40,990 --> 00:30:45,760
help with the situation where ephemeral

00:30:43,330 --> 00:30:48,070
downtime of specific machines and

00:30:45,760 --> 00:30:50,140
specific services does not cause your

00:30:48,070 --> 00:30:52,270
entire data processing pipeline to fail

00:30:50,140 --> 00:30:54,670
and does not cause you to lose data and

00:30:52,270 --> 00:30:56,530
I think that that's the primary use case

00:30:54,670 --> 00:31:00,340
that the reliability semantics and storm

00:30:56,530 --> 00:31:01,990
tries to solve for you okay thanks

00:31:00,340 --> 00:31:04,120
everyone for your questions one quick

00:31:01,990 --> 00:31:06,370
note there are surveys up on the corner

00:31:04,120 --> 00:31:07,870
of the stage that the organizing

00:31:06,370 --> 00:31:10,120
committee is asking you to fill out

00:31:07,870 --> 00:31:11,770
about the closed-captioning if you could

00:31:10,120 --> 00:31:15,720
do that we would really appreciate it

00:31:11,770 --> 00:31:15,720

YouTube URL: https://www.youtube.com/watch?v=ja4Qj9-l6WQ


