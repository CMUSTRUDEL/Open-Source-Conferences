Title: Geoff Gerrietts - Performance by the Numbers: analyzing the performance of web applications
Publication date: 2015-04-12
Playlist: PyCon 2015
Description: 
	"Speaker: Geoff Gerrietts

Everyone knows poor performance when they see it, and performance concerns affect every application -- web applications more than most. But finding performance problems can be extraordinarily difficult, and requires an analytical approach coupled with good instrumentation. This talk explores approaches to instrumentation and what that instrumentation can tell you.

Slides can be found at: https://speakerdeck.com/pycon2015 and https://github.com/PyCon/2015-slides"
Captions: 
	00:00:06,649 --> 00:00:13,920
right on ooh that's loud hi so like he

00:00:12,059 --> 00:00:17,310
said I'm Jeff Jarrett's I'm a

00:00:13,920 --> 00:00:19,350
development manager at meta I've been a

00:00:17,310 --> 00:00:21,060
Python Easter for 15 years and I've been

00:00:19,350 --> 00:00:26,099
doing web development pretty much as

00:00:21,060 --> 00:00:28,740
long as that was a thing I before I go

00:00:26,099 --> 00:00:31,230
on out of call out that both the Boston

00:00:28,740 --> 00:00:33,239
python meetup and my team at at meta

00:00:31,230 --> 00:00:35,880
really did a great job helping with

00:00:33,239 --> 00:00:40,350
these slides so it's all about the

00:00:35,880 --> 00:00:41,309
delivery now so they tell me that I

00:00:40,350 --> 00:00:42,390
ought to tell you what I'm going to tell

00:00:41,309 --> 00:00:44,370
you and so I'm going to tell you what

00:00:42,390 --> 00:00:47,520
I'm going to tell you and that is first

00:00:44,370 --> 00:00:51,120
that performance matters but it's hard

00:00:47,520 --> 00:00:53,730
to do right and there are a lot of tools

00:00:51,120 --> 00:00:57,600
that will help you with that and you

00:00:53,730 --> 00:00:58,859
should use them all as caveat i will say

00:00:57,600 --> 00:01:00,989
that i'm mostly talking about

00:00:58,859 --> 00:01:04,140
server-side performance web applications

00:01:00,989 --> 00:01:05,460
today user experience is very important

00:01:04,140 --> 00:01:08,130
but it has different tools and

00:01:05,460 --> 00:01:12,420
technologies be a totally separate talk

00:01:08,130 --> 00:01:15,240
okay so performance matters and the

00:01:12,420 --> 00:01:17,070
thing is that it's been hard

00:01:15,240 --> 00:01:19,380
historically to say how much it matters

00:01:17,070 --> 00:01:20,790
I mean features have obvious value

00:01:19,380 --> 00:01:24,990
that's what I've been working on for

00:01:20,790 --> 00:01:27,570
most of my career and even in software

00:01:24,990 --> 00:01:28,829
processes that identify performance

00:01:27,570 --> 00:01:31,079
constraints is something that should be

00:01:28,829 --> 00:01:33,329
called out in a feature description it's

00:01:31,079 --> 00:01:35,790
really very hard for product management

00:01:33,329 --> 00:01:38,130
to specify what those performance

00:01:35,790 --> 00:01:40,200
constraints ought to be or for that

00:01:38,130 --> 00:01:43,770
matter understand what can go wrong with

00:01:40,200 --> 00:01:46,049
the performance and sometimes that's

00:01:43,770 --> 00:01:49,470
because we haven't known the value now

00:01:46,049 --> 00:01:51,030
in the last several years big retail has

00:01:49,470 --> 00:01:55,229
brought a lot of focus on performance

00:01:51,030 --> 00:01:57,840
because they were able to identify that

00:01:55,229 --> 00:02:01,829
low latency correlates with increased

00:01:57,840 --> 00:02:03,600
conversion and revenue amazon and

00:02:01,829 --> 00:02:07,140
walmart in particular released studies

00:02:03,600 --> 00:02:09,209
that showed that as little as a hundred

00:02:07,140 --> 00:02:10,530
milliseconds of response time could have

00:02:09,209 --> 00:02:12,800
a one-percent impact

00:02:10,530 --> 00:02:15,660
on your on your your total revenue

00:02:12,800 --> 00:02:17,880
Google Yahoo and Mozilla also produce

00:02:15,660 --> 00:02:23,280
performance studies that have had

00:02:17,880 --> 00:02:26,580
similar results and the burden of

00:02:23,280 --> 00:02:28,830
performance doesn't all fall on on the

00:02:26,580 --> 00:02:31,920
on the product management team though in

00:02:28,830 --> 00:02:33,840
development we have I think rightly had

00:02:31,920 --> 00:02:36,600
a culture that chooses readable

00:02:33,840 --> 00:02:38,880
maintainable code over the most optimal

00:02:36,600 --> 00:02:43,739
code under the understanding that it is

00:02:38,880 --> 00:02:45,480
really hard to figure out what pieces of

00:02:43,739 --> 00:02:48,209
your code need to be high performance

00:02:45,480 --> 00:02:51,060
before that code is deployed and shown

00:02:48,209 --> 00:02:53,070
to be not performing enough on the other

00:02:51,060 --> 00:02:56,250
hand you also frequently only get one

00:02:53,070 --> 00:03:05,040
chance to do it right which is a pretty

00:02:56,250 --> 00:03:07,410
vicious trade-off so performance does

00:03:05,040 --> 00:03:10,890
matter though because we all know that

00:03:07,410 --> 00:03:14,340
slow websites erode your sanity and and

00:03:10,890 --> 00:03:16,140
that costs you money so the lesson of

00:03:14,340 --> 00:03:18,019
the first part is that the most common

00:03:16,140 --> 00:03:21,420
mistake in performance analysis

00:03:18,019 --> 00:03:23,579
remediation is just ignoring it or maybe

00:03:21,420 --> 00:03:27,600
better put not recognizing the impact

00:03:23,579 --> 00:03:28,860
performance has on your bottom line once

00:03:27,600 --> 00:03:32,360
you start to think about it though

00:03:28,860 --> 00:03:36,540
there's still plenty of mistakes to make

00:03:32,360 --> 00:03:38,820
so bureaucracy is the art of organizing

00:03:36,540 --> 00:03:41,250
an enterprise such that the buck never

00:03:38,820 --> 00:03:43,950
lands on your desk and most

00:03:41,250 --> 00:03:46,140
organizations do some of that right the

00:03:43,950 --> 00:03:47,790
developer points of the database the DBA

00:03:46,140 --> 00:03:50,160
points at the hardware the sysadmin

00:03:47,790 --> 00:03:53,579
points back at the code it's the circle

00:03:50,160 --> 00:03:55,829
of blame and I've been that guy I've

00:03:53,579 --> 00:03:57,570
been the guy that expects the DBA to fix

00:03:55,829 --> 00:04:01,170
all my performance problems by putting

00:03:57,570 --> 00:04:03,600
better indexes on his tables but

00:04:01,170 --> 00:04:05,940
honestly we all have to recognize at

00:04:03,600 --> 00:04:08,040
some point that the indexes can't save

00:04:05,940 --> 00:04:09,959
us we need to write better queries we

00:04:08,040 --> 00:04:11,790
need to have better caches we need to

00:04:09,959 --> 00:04:13,440
ask not what our database can do for our

00:04:11,790 --> 00:04:20,130
app but what our app can do for the

00:04:13,440 --> 00:04:22,830
database so the drunken man approach is

00:04:20,130 --> 00:04:24,060
a little bit different that's where you

00:04:22,830 --> 00:04:26,720
come up with a plow

00:04:24,060 --> 00:04:30,000
double idea to improve your performance

00:04:26,720 --> 00:04:33,330
you go do it and maybe measure the

00:04:30,000 --> 00:04:35,669
results if you can and I don't know

00:04:33,330 --> 00:04:37,860
things might have got better so this

00:04:35,669 --> 00:04:42,889
this model was suggested by Brendan Greg

00:04:37,860 --> 00:04:45,419
who is a security researcher at Netflix

00:04:42,889 --> 00:04:47,340
he's the guy that did screaming at the

00:04:45,419 --> 00:04:51,720
data center if you saw that video it's

00:04:47,340 --> 00:04:53,190
fascinating if you haven't but the thing

00:04:51,720 --> 00:04:55,260
about the drunken man is even when it

00:04:53,190 --> 00:04:56,880
works it might not be addressing the

00:04:55,260 --> 00:05:01,290
most important performance problems you

00:04:56,880 --> 00:05:03,840
have now I get to tell a story I did a

00:05:01,290 --> 00:05:08,639
certain company that I definitely won't

00:05:03,840 --> 00:05:11,250
name for fear litigation that I spent

00:05:08,639 --> 00:05:13,710
two months of maybe four or five

00:05:11,250 --> 00:05:17,639
engineers at a time working on a project

00:05:13,710 --> 00:05:21,270
to consolidate and uglify our JavaScript

00:05:17,639 --> 00:05:23,070
and CSS under the theory a strong theory

00:05:21,270 --> 00:05:25,830
a dominant theory in the web space that

00:05:23,070 --> 00:05:27,810
the number of requests that clients had

00:05:25,830 --> 00:05:32,580
to make in order to retrieve the assets

00:05:27,810 --> 00:05:36,840
were negatively impacting consumers

00:05:32,580 --> 00:05:40,260
ability to use the site and so after it

00:05:36,840 --> 00:05:42,539
was all done we we did some comparison

00:05:40,260 --> 00:05:44,640
measurements and discovered that there

00:05:42,539 --> 00:05:46,169
was actually no measurable improvement

00:05:44,640 --> 00:05:47,610
we could not get a statistical

00:05:46,169 --> 00:05:49,860
indication that we had helped things at

00:05:47,610 --> 00:05:51,450
all but we did manage to spend a couple

00:05:49,860 --> 00:05:57,240
hundred thousand dollars not doing

00:05:51,450 --> 00:05:59,280
anything it was pretty cool so the the

00:05:57,240 --> 00:06:01,050
idea is if you're designing a project

00:05:59,280 --> 00:06:03,139
before you know which parts of the app

00:06:01,050 --> 00:06:06,479
need attention you're the drunken man

00:06:03,139 --> 00:06:10,820
put down the bottle or the flask or the

00:06:06,479 --> 00:06:17,250
pyramid and step away from the backlog

00:06:10,820 --> 00:06:20,330
the last common performance anti-pattern

00:06:17,250 --> 00:06:22,440
that I've seen is what i call the hammer

00:06:20,330 --> 00:06:24,360
passing the bucs bad shooting in the

00:06:22,440 --> 00:06:25,919
dark is bad and so looking for some

00:06:24,360 --> 00:06:29,820
amount of insight is always a good first

00:06:25,919 --> 00:06:32,310
step but when you've only got one tool

00:06:29,820 --> 00:06:35,600
and it's a hammer every problem starts

00:06:32,310 --> 00:06:38,430
to look like a thumb right

00:06:35,600 --> 00:06:41,610
this this problem happens when you lean

00:06:38,430 --> 00:06:43,740
too heavily on one or two tools you

00:06:41,610 --> 00:06:45,660
can't see into their blind spots and if

00:06:43,740 --> 00:06:47,970
there's a theme that I'm going to be

00:06:45,660 --> 00:06:52,830
developing today it's that one tool is

00:06:47,970 --> 00:06:55,919
never enough the story time here I

00:06:52,830 --> 00:06:58,110
worked with assisted man who had Oracle

00:06:55,919 --> 00:07:01,949
writing its tables to a network attached

00:06:58,110 --> 00:07:04,860
storage device over NFS and according to

00:07:01,949 --> 00:07:08,610
his tools which were top and if config

00:07:04,860 --> 00:07:10,740
this system was awesome it had no

00:07:08,610 --> 00:07:12,600
problems at all now if you were a user

00:07:10,740 --> 00:07:15,240
on the website you had a different

00:07:12,600 --> 00:07:17,940
opinion of how successful this build was

00:07:15,240 --> 00:07:19,260
but getting the sysadmin to admit that

00:07:17,940 --> 00:07:21,870
there might be a problem with this

00:07:19,260 --> 00:07:24,720
strategy took took a great deal of time

00:07:21,870 --> 00:07:26,400
and effort that's because he just didn't

00:07:24,720 --> 00:07:28,979
have the right tools he didn't know that

00:07:26,400 --> 00:07:31,860
what he was looking at or that that the

00:07:28,979 --> 00:07:35,940
average query time was significantly

00:07:31,860 --> 00:07:37,800
slower so all three of these

00:07:35,940 --> 00:07:40,770
anti-patterns have sort of a common flaw

00:07:37,800 --> 00:07:44,699
I stopped short of discovering the root

00:07:40,770 --> 00:07:47,250
cause of the latency in your system just

00:07:44,699 --> 00:07:49,710
like fixing bugs because performance

00:07:47,250 --> 00:07:52,169
problems are essentially bugs if you

00:07:49,710 --> 00:07:53,849
don't find the root cause you don't

00:07:52,169 --> 00:07:55,830
understand the problem and if you don't

00:07:53,849 --> 00:07:57,270
understand where the latency is coming

00:07:55,830 --> 00:08:02,490
from you really can't meaningfully

00:07:57,270 --> 00:08:06,449
address it so for most of us well maybe

00:08:02,490 --> 00:08:08,190
for some of us the first place our head

00:08:06,449 --> 00:08:10,110
goes when we start talking about finding

00:08:08,190 --> 00:08:12,180
latency is to profilers right and

00:08:10,110 --> 00:08:13,889
there's a good reason for that that's

00:08:12,180 --> 00:08:15,690
because profilers are extremely powerful

00:08:13,889 --> 00:08:18,599
they've been the go-to tool for

00:08:15,690 --> 00:08:20,070
performance analysis for decades and

00:08:18,599 --> 00:08:22,139
you've got to be careful because the

00:08:20,070 --> 00:08:25,199
profiler can become a hammer pretty

00:08:22,139 --> 00:08:29,659
quickly right a profiler is really best

00:08:25,199 --> 00:08:35,610
used to analyze a specific code path so

00:08:29,659 --> 00:08:38,400
this is an approach that we used again

00:08:35,610 --> 00:08:41,279
at an unmentionable former employer to

00:08:38,400 --> 00:08:43,380
try to use profilers to find performance

00:08:41,279 --> 00:08:46,140
problems the idea was that we would just

00:08:43,380 --> 00:08:46,730
stick a profiler between Django and our

00:08:46,140 --> 00:08:49,100
applique

00:08:46,730 --> 00:08:52,389
code and then just dump the profiling

00:08:49,100 --> 00:08:55,959
stats out and that that's not a bad idea

00:08:52,389 --> 00:08:58,310
but the trouble with profilers

00:08:55,959 --> 00:09:01,300
particularly line profilers the kind

00:08:58,310 --> 00:09:04,070
that the Python has built-in is that

00:09:01,300 --> 00:09:05,899
each time you call a function the

00:09:04,070 --> 00:09:07,459
function has to call out to the profile

00:09:05,899 --> 00:09:08,690
to say hey I'm starting this function

00:09:07,459 --> 00:09:10,100
and each time you come back from the

00:09:08,690 --> 00:09:12,019
function it sends another call out to

00:09:10,100 --> 00:09:13,790
the and the profiler is writing these

00:09:12,019 --> 00:09:16,250
stats out to the file all the time which

00:09:13,790 --> 00:09:18,170
puts a bunch of Io overhead into your

00:09:16,250 --> 00:09:20,480
application that previously didn't exist

00:09:18,170 --> 00:09:23,180
on every function call and it puts a

00:09:20,480 --> 00:09:25,279
bunch of a function call overhead just

00:09:23,180 --> 00:09:30,410
passing those stats into the profiler

00:09:25,279 --> 00:09:31,639
and so profilers you know you can't run

00:09:30,410 --> 00:09:33,740
this in production you can't run a

00:09:31,639 --> 00:09:36,199
traditional line profiler in production

00:09:33,740 --> 00:09:37,850
it will take your application down very

00:09:36,199 --> 00:09:42,290
quickly and that's something I've seen

00:09:37,850 --> 00:09:45,050
before but so the the other thing that's

00:09:42,290 --> 00:09:46,880
interesting is that because of the

00:09:45,050 --> 00:09:49,730
amount of overhead involved in actually

00:09:46,880 --> 00:09:51,589
recording the profiles you can distort

00:09:49,730 --> 00:09:53,990
the picture of where the latency is

00:09:51,589 --> 00:09:56,180
actually coming from right the the

00:09:53,990 --> 00:09:58,880
really tiny little function call that

00:09:56,180 --> 00:10:01,579
only takes a micro second now takes a

00:09:58,880 --> 00:10:04,940
couple of milliseconds instead and the

00:10:01,579 --> 00:10:07,490
the great big honkin guy that's taken a

00:10:04,940 --> 00:10:09,649
second and a half it doesn't have nearly

00:10:07,490 --> 00:10:11,959
as much overhead added to it and so it

00:10:09,649 --> 00:10:14,120
distorts the picture a little bit maybe

00:10:11,959 --> 00:10:17,920
not a big deal who knows all right oh

00:10:14,120 --> 00:10:20,720
look it's time for story time again so

00:10:17,920 --> 00:10:22,639
in this place where we used the the

00:10:20,720 --> 00:10:24,829
where we stuck that that profiler in the

00:10:22,639 --> 00:10:28,370
middle where we were trying to address

00:10:24,829 --> 00:10:31,850
the the fact that the django app was

00:10:28,370 --> 00:10:33,380
pretty slow right and so one of the

00:10:31,850 --> 00:10:35,480
roots of the investigation we used was

00:10:33,380 --> 00:10:37,310
the profiling of course we couldn't

00:10:35,480 --> 00:10:39,319
profile in production so what we did was

00:10:37,310 --> 00:10:41,779
we used the apache logs to try to

00:10:39,319 --> 00:10:43,790
simulate traffic distribution now the

00:10:41,779 --> 00:10:46,940
trouble with that is that get in query

00:10:43,790 --> 00:10:50,269
parts those you can get but but the post

00:10:46,940 --> 00:10:51,440
data isn't there anymore so we would

00:10:50,269 --> 00:10:54,649
have troubles where we couldn't

00:10:51,440 --> 00:10:56,120
distinguish y 1 URL would take maybe 500

00:10:54,649 --> 00:10:57,920
milliseconds on a couple of calls and

00:10:56,120 --> 00:11:00,110
three seconds on the other because that

00:10:57,920 --> 00:11:02,300
was all relative to the

00:11:00,110 --> 00:11:04,040
was the content of the post eventually

00:11:02,300 --> 00:11:07,459
we ended up only testing the assumptions

00:11:04,040 --> 00:11:09,410
it was not a very useful experiment now

00:11:07,459 --> 00:11:10,670
there is a way to use profilers

00:11:09,410 --> 00:11:13,399
responsibly there are actually a couple

00:11:10,670 --> 00:11:16,519
of ways one of them is to use it to it

00:11:13,399 --> 00:11:21,170
actually inspect a code path that you've

00:11:16,519 --> 00:11:23,209
already begun identifying as a problem

00:11:21,170 --> 00:11:25,670
spot there they're fantastic at doing

00:11:23,209 --> 00:11:27,709
that and it can show you that the while

00:11:25,670 --> 00:11:30,170
loop that you thought was completely

00:11:27,709 --> 00:11:33,829
innocuous is actually deeply nested and

00:11:30,170 --> 00:11:36,260
has N squared behavior right there's

00:11:33,829 --> 00:11:38,720
also another way to use profilers in

00:11:36,260 --> 00:11:41,450
production and that is statistical

00:11:38,720 --> 00:11:43,430
profiling and statistical profiling uses

00:11:41,450 --> 00:11:45,920
periodic random sampling to build the

00:11:43,430 --> 00:11:47,930
profiling data it's in exact but it

00:11:45,920 --> 00:11:49,700
proves it produces a statistically

00:11:47,930 --> 00:11:51,529
accurate model of where your code is

00:11:49,700 --> 00:11:54,860
spending time by sort of randomly

00:11:51,529 --> 00:11:56,540
checking every once in a while so this

00:11:54,860 --> 00:11:59,750
is a fairly common architecture right

00:11:56,540 --> 00:12:02,390
you've got a load balancer some app

00:11:59,750 --> 00:12:06,220
nodes may be a my sequel and a memcache

00:12:02,390 --> 00:12:08,600
playing cat's cradle with the app and

00:12:06,220 --> 00:12:11,000
when you statistically profile

00:12:08,600 --> 00:12:12,829
essentially you put a statistical

00:12:11,000 --> 00:12:15,589
profiler on each of those apt nodes and

00:12:12,829 --> 00:12:17,899
they sample everything in there that

00:12:15,589 --> 00:12:21,410
provides some big picture but it lacks

00:12:17,899 --> 00:12:23,990
context you don't know why a particular

00:12:21,410 --> 00:12:27,440
call that's taking a great deal of time

00:12:23,990 --> 00:12:29,930
has takes a great deal of time and you

00:12:27,440 --> 00:12:31,880
especially don't know if you've got a

00:12:29,930 --> 00:12:33,350
call that some take sometimes takes a

00:12:31,880 --> 00:12:35,720
lot of time and some time takes a little

00:12:33,350 --> 00:12:37,220
time why that's the case you just know

00:12:35,720 --> 00:12:41,510
that the average latency is pretty poor

00:12:37,220 --> 00:12:44,480
right still useful but but it does have

00:12:41,510 --> 00:12:46,670
some significant blind spots right so

00:12:44,480 --> 00:12:48,560
maybe when I start talking about

00:12:46,670 --> 00:12:50,600
quantifying latency your mind doesn't go

00:12:48,560 --> 00:12:53,510
to profilers maybe it goes to top or

00:12:50,600 --> 00:12:58,120
estrace maybe secretly here a system

00:12:53,510 --> 00:13:01,279
administrator that's okay we like you

00:12:58,120 --> 00:13:02,570
and there's actually perfectly good

00:13:01,279 --> 00:13:04,459
reason for you to think this right the

00:13:02,570 --> 00:13:06,500
operating system provides a ton of tools

00:13:04,459 --> 00:13:09,170
I mean like look at this graph right

00:13:06,500 --> 00:13:10,430
it's crazy you probably want to look at

00:13:09,170 --> 00:13:13,850
it after the conference though it's

00:13:10,430 --> 00:13:17,240
pretty detailed I stole this from

00:13:13,850 --> 00:13:21,350
one of Brendan Gregg's talks on a Linux

00:13:17,240 --> 00:13:23,420
system optimization it shows various

00:13:21,350 --> 00:13:25,220
parts of the system and the tools that

00:13:23,420 --> 00:13:27,709
can provide insight into that system and

00:13:25,220 --> 00:13:30,889
these tools are great most of them also

00:13:27,709 --> 00:13:35,180
work just fine in production right but

00:13:30,889 --> 00:13:37,300
you remember this architecture so when

00:13:35,180 --> 00:13:40,069
you're using these kinds of tools that

00:13:37,300 --> 00:13:42,980
graph is on every one of those nodes

00:13:40,069 --> 00:13:44,690
right and they don't know anything about

00:13:42,980 --> 00:13:47,569
how they're talking to the other parts

00:13:44,690 --> 00:13:49,490
which is it stuffed I mean it's great

00:13:47,569 --> 00:13:51,740
when one of your nodes as resource

00:13:49,490 --> 00:13:53,540
constrained it's useful to be able to go

00:13:51,740 --> 00:13:54,889
in there and see ah this is the resource

00:13:53,540 --> 00:13:58,310
that's constrained this is why it's

00:13:54,889 --> 00:14:00,139
constrained but when you're trying to

00:13:58,310 --> 00:14:01,579
understand the overall context of where

00:14:00,139 --> 00:14:03,310
your latency is and why you're spending

00:14:01,579 --> 00:14:08,449
all your time it's hard to do that right

00:14:03,310 --> 00:14:09,980
okay so OS tools are great for like I

00:14:08,449 --> 00:14:12,410
said observing resource depletion

00:14:09,980 --> 00:14:15,050
they're also good at diagnosing host or

00:14:12,410 --> 00:14:16,670
operating system failure and that that's

00:14:15,050 --> 00:14:18,439
that's totally legitimate right because

00:14:16,670 --> 00:14:20,810
host or operating system failure often

00:14:18,439 --> 00:14:24,100
presents as a cute performance problems

00:14:20,810 --> 00:14:28,639
sometimes like can't get an answer right

00:14:24,100 --> 00:14:30,459
so let's look closer though because real

00:14:28,639 --> 00:14:33,230
insight into the code requires

00:14:30,459 --> 00:14:35,089
instrumentation and instrumenting code

00:14:33,230 --> 00:14:38,120
meet or instrumenting means inserting

00:14:35,089 --> 00:14:40,519
code to track the the behavior of your

00:14:38,120 --> 00:14:42,050
application the first category of

00:14:40,519 --> 00:14:43,399
instrumentation I'd love to talk to you

00:14:42,050 --> 00:14:46,870
about is what I call ad hoc

00:14:43,399 --> 00:14:50,959
instrumentation and this is where my

00:14:46,870 --> 00:14:54,380
performance career really started in in

00:14:50,959 --> 00:14:56,540
building out ad hoc systems like I was

00:14:54,380 --> 00:14:59,089
working on a corba based service

00:14:56,540 --> 00:15:01,399
oriented architecture and I wanted to

00:14:59,089 --> 00:15:03,050
understand why law where all our latency

00:15:01,399 --> 00:15:04,699
was coming from although maybe you've

00:15:03,050 --> 00:15:08,959
got some clues already based on what

00:15:04,699 --> 00:15:12,350
I've said but I wrote a stat service

00:15:08,959 --> 00:15:16,670
that that would record call timings and

00:15:12,350 --> 00:15:20,089
report mean latency on a medium mean

00:15:16,670 --> 00:15:21,439
deviation on a per endpoint basis now it

00:15:20,089 --> 00:15:23,360
didn't produce quite the insight that I

00:15:21,439 --> 00:15:24,920
was looking for because I didn't have as

00:15:23,360 --> 00:15:26,959
much historical data as i would have

00:15:24,920 --> 00:15:27,560
liked and so it's hard to see the effect

00:15:26,959 --> 00:15:29,630
that the chain

00:15:27,560 --> 00:15:31,790
we're having and what we could have but

00:15:29,630 --> 00:15:33,590
anyway that's a different story but it

00:15:31,790 --> 00:15:34,940
did teach me a lot about how difficult

00:15:33,590 --> 00:15:37,700
it is to write instrumentation and how

00:15:34,940 --> 00:15:40,850
hard it is to get it right a more recent

00:15:37,700 --> 00:15:44,690
example at at my current company we

00:15:40,850 --> 00:15:47,990
wrote a ad hoc piece of instrumentation

00:15:44,690 --> 00:15:49,820
that connects to each of our collectors

00:15:47,990 --> 00:15:51,529
collectors are high throughput nodes

00:15:49,820 --> 00:15:54,830
responsible for receiving in band data

00:15:51,529 --> 00:15:57,500
right and they expose an API that

00:15:54,830 --> 00:15:59,180
reports on connection status and so we

00:15:57,500 --> 00:16:00,440
wrote a little flasks app that reaches

00:15:59,180 --> 00:16:03,020
out to each of the collector nodes

00:16:00,440 --> 00:16:05,330
aggregates the dad data does some

00:16:03,020 --> 00:16:06,980
database lookups to make the data human

00:16:05,330 --> 00:16:10,040
readable and then passes that up to a

00:16:06,980 --> 00:16:12,380
web node and that's a sort of a

00:16:10,040 --> 00:16:15,800
specialized thing and for specialized

00:16:12,380 --> 00:16:18,070
things that's a that's a pretty good

00:16:15,800 --> 00:16:21,860
approach but there there is a better way

00:16:18,070 --> 00:16:25,070
for the more general case right and this

00:16:21,860 --> 00:16:27,710
is a stats key etsy has sort of done a

00:16:25,070 --> 00:16:29,360
good well they definitely done it a good

00:16:27,710 --> 00:16:32,390
turn to the community by releasing stats

00:16:29,360 --> 00:16:34,880
deep stats D is a small server it runs

00:16:32,390 --> 00:16:36,710
on your production nodes and your

00:16:34,880 --> 00:16:39,320
application can shovel all sorts of

00:16:36,710 --> 00:16:41,600
labeled metrics down into stats d then

00:16:39,320 --> 00:16:45,290
stats d uploads those metrics into a

00:16:41,600 --> 00:16:46,339
graphite interface right and graphite if

00:16:45,290 --> 00:16:49,120
you don't know is a general-purpose

00:16:46,339 --> 00:16:51,290
graphing package for time series data

00:16:49,120 --> 00:16:56,900
that will let you trend your metrics

00:16:51,290 --> 00:16:58,700
over time so let's see this is an

00:16:56,900 --> 00:17:00,230
example of how you actually do the

00:16:58,700 --> 00:17:02,210
instrumentation it's ripped straight

00:17:00,230 --> 00:17:05,870
from the the stats d package

00:17:02,210 --> 00:17:09,800
documentation the top one is an example

00:17:05,870 --> 00:17:11,300
of doing a timer you you know you set up

00:17:09,800 --> 00:17:14,839
with a context manager and as soon as

00:17:11,300 --> 00:17:16,699
the the block ends the timing that for

00:17:14,839 --> 00:17:18,860
that block gets sent off to STATS d with

00:17:16,699 --> 00:17:21,160
the label foo in this case which i'm

00:17:18,860 --> 00:17:25,310
sure will be unique in your system and

00:17:21,160 --> 00:17:27,800
then the bottom one is even simpler it's

00:17:25,310 --> 00:17:29,900
just a counter you know some dot event

00:17:27,800 --> 00:17:32,690
which also I'm sure is a very

00:17:29,900 --> 00:17:34,730
descriptive identifier will will

00:17:32,690 --> 00:17:40,460
increment each time that line of code is

00:17:34,730 --> 00:17:41,330
called so this is the kind of graph that

00:17:40,460 --> 00:17:46,570
you can

00:17:41,330 --> 00:17:46,570
back to get out of it so this one is

00:17:47,710 --> 00:17:54,110
what is this oh sorry it's a the

00:17:51,080 --> 00:17:59,870
connections on a one of our my sequel

00:17:54,110 --> 00:18:03,559
databases right and so yeah my speaker

00:17:59,870 --> 00:18:06,200
notes are wrong so but anyway so ad hoc

00:18:03,559 --> 00:18:08,809
instrumentation works great for tracking

00:18:06,200 --> 00:18:10,220
into trending discrete events right if

00:18:08,809 --> 00:18:12,559
there's a specific event that you're

00:18:10,220 --> 00:18:14,779
interested in like our number of

00:18:12,559 --> 00:18:17,389
connections on the database or maybe

00:18:14,779 --> 00:18:19,669
mean time to process a login or even

00:18:17,389 --> 00:18:22,100
just number of logins ad hoc metrics are

00:18:19,669 --> 00:18:24,559
a great way to keep an eye on it right

00:18:22,100 --> 00:18:26,899
this approach to stats can be

00:18:24,559 --> 00:18:29,120
labor-intensive because every point of

00:18:26,899 --> 00:18:30,889
instrumentation is hand-tooled you pick

00:18:29,120 --> 00:18:34,100
out the thing you want to measure you

00:18:30,889 --> 00:18:36,649
put in the instrumentation code and you

00:18:34,100 --> 00:18:38,240
build a graph for it it can also be

00:18:36,649 --> 00:18:39,710
pretty exhausting to interpret because

00:18:38,240 --> 00:18:41,960
once you get above a certain number of

00:18:39,710 --> 00:18:43,429
grafts like one of the guys I work with

00:18:41,960 --> 00:18:45,470
he has a screen that he just keeps

00:18:43,429 --> 00:18:47,690
filled with all these mutant graphs all

00:18:45,470 --> 00:18:49,220
day long and he's like oh yeah I can

00:18:47,690 --> 00:18:51,760
totally see immediately and I'm like

00:18:49,220 --> 00:18:57,049
whoa dude that's too much information

00:18:51,760 --> 00:19:00,320
not in the usual way but so let's see

00:18:57,049 --> 00:19:02,899
and like many of the other tools that

00:19:00,320 --> 00:19:04,159
we've looked at it provides data points

00:19:02,899 --> 00:19:07,909
but it doesn't provide a real good

00:19:04,159 --> 00:19:09,950
context for those data points so this is

00:19:07,909 --> 00:19:11,870
this is where tracing kicks and right

00:19:09,950 --> 00:19:14,480
there's a theme that's been developing

00:19:11,870 --> 00:19:17,450
and the theme is that it's hard to see

00:19:14,480 --> 00:19:20,120
performance in context of the requests

00:19:17,450 --> 00:19:22,840
that are caused are that are taking time

00:19:20,120 --> 00:19:27,549
this is where tracing techniques come in

00:19:22,840 --> 00:19:31,309
so this is a trace in Twitter's Zipkin

00:19:27,549 --> 00:19:34,940
and so a trace is based around the idea

00:19:31,309 --> 00:19:38,500
of or rather it represents it's it's a

00:19:34,940 --> 00:19:41,179
representation of the path of execution

00:19:38,500 --> 00:19:44,419
followed in the fulfillment of a single

00:19:41,179 --> 00:19:47,120
request so this example in Zipkin shows

00:19:44,419 --> 00:19:50,480
113 millisecond trace that makes use of

00:19:47,120 --> 00:19:53,570
a number of strangely named services

00:19:50,480 --> 00:19:55,190
like quickie web server some service and

00:19:53,570 --> 00:19:59,600
memcache d

00:19:55,190 --> 00:20:04,100
oh and began a service nice but anyway

00:19:59,600 --> 00:20:05,900
so you can see at each layer that a

00:20:04,100 --> 00:20:09,410
certain amount of time is being consumed

00:20:05,900 --> 00:20:11,420
by but at that service layer in

00:20:09,410 --> 00:20:18,710
satisfying those requests and ultimately

00:20:11,420 --> 00:20:21,110
the the trace returns to the user when

00:20:18,710 --> 00:20:23,060
you have these traces you can aggregate

00:20:21,110 --> 00:20:25,520
them and trend them in visualizations

00:20:23,060 --> 00:20:29,230
this comes out of trace view which is

00:20:25,520 --> 00:20:34,850
the product i work on and it shows

00:20:29,230 --> 00:20:37,310
application with all of the all of the

00:20:34,850 --> 00:20:39,710
layers in that application this one's I

00:20:37,310 --> 00:20:43,190
think it's a Django application that

00:20:39,710 --> 00:20:48,980
also talks to services implemented in

00:20:43,190 --> 00:20:52,520
Ruby Java oh god in Drupal 7 and assigns

00:20:48,980 --> 00:20:54,800
latency for the requests to to them and

00:20:52,520 --> 00:20:57,740
you can sort of see like as time goes by

00:20:54,800 --> 00:20:59,840
how each of those is a little bit slower

00:20:57,740 --> 00:21:04,460
a little bit faster depending on what's

00:20:59,840 --> 00:21:07,100
going on on the system right on so

00:21:04,460 --> 00:21:09,800
finally this is actually a great place

00:21:07,100 --> 00:21:12,410
to start when you're looking at latency

00:21:09,800 --> 00:21:15,110
data individual traces provide a great

00:21:12,410 --> 00:21:17,540
window into which code paths have

00:21:15,110 --> 00:21:20,090
unacceptable latency and the trace tools

00:21:17,540 --> 00:21:23,030
themselves offer a great way to see the

00:21:20,090 --> 00:21:24,350
overall latency so you may ask why do

00:21:23,030 --> 00:21:25,700
you wait until the end of the talk to

00:21:24,350 --> 00:21:31,340
talk about it if that's where you want

00:21:25,700 --> 00:21:33,440
to start and the answer is this is kind

00:21:31,340 --> 00:21:35,930
of like the architecture diagram we were

00:21:33,440 --> 00:21:39,670
using for ad hoc metrics oh my god my

00:21:35,930 --> 00:21:41,870
microphones falling help help all right

00:21:39,670 --> 00:21:46,280
if you can't hear me just start yelling

00:21:41,870 --> 00:21:49,100
um so it's similar to the architecture

00:21:46,280 --> 00:21:50,840
diagram we used previously but this is

00:21:49,100 --> 00:21:53,660
significantly more articulated right

00:21:50,840 --> 00:21:55,820
you've got your application with some

00:21:53,660 --> 00:21:59,330
kind of metrics aggregator similar to

00:21:55,820 --> 00:22:01,940
the way stats d works and then an

00:21:59,330 --> 00:22:05,450
ingestion that assembles the trace is an

00:22:01,940 --> 00:22:07,280
analysis component that plucks had the

00:22:05,450 --> 00:22:08,480
meaningful parts and inserts it into the

00:22:07,280 --> 00:22:10,610
databases

00:22:08,480 --> 00:22:12,380
large scale data store to handle all of

00:22:10,610 --> 00:22:15,290
this because you're dealing with

00:22:12,380 --> 00:22:17,720
millions of events a day then you've got

00:22:15,290 --> 00:22:19,760
the querying and statistics logic on the

00:22:17,720 --> 00:22:23,240
far side of that plus data visualization

00:22:19,760 --> 00:22:27,559
in UI and what's rough about this is

00:22:23,240 --> 00:22:32,600
that in general that's not super easy to

00:22:27,559 --> 00:22:35,419
do in a very generic way for every

00:22:32,600 --> 00:22:37,429
application you can kind of you can get

00:22:35,419 --> 00:22:39,410
some of it but a lot of it is is pretty

00:22:37,429 --> 00:22:42,320
hand tooled to the application that

00:22:39,410 --> 00:22:43,250
you're building so in short why do I

00:22:42,320 --> 00:22:45,740
wait to the end it's a pretty

00:22:43,250 --> 00:22:48,490
complicated tool to build and set up so

00:22:45,740 --> 00:22:53,510
there are some free versions out there

00:22:48,490 --> 00:22:56,030
of tracing tools and I would encourage

00:22:53,510 --> 00:22:58,010
you all to investigate them and make

00:22:56,030 --> 00:22:59,740
them better because right now they all

00:22:58,010 --> 00:23:02,540
kind of suck from one way or another

00:22:59,740 --> 00:23:04,040
Google's dapper paper is actually just a

00:23:02,540 --> 00:23:08,000
paper you can't really run that very

00:23:04,040 --> 00:23:12,020
well yammer yammer telemetry I

00:23:08,000 --> 00:23:14,210
understand is a functional but a little

00:23:12,020 --> 00:23:16,520
bit hard to deploy and Twitter's zipkin

00:23:14,210 --> 00:23:19,429
as far as I know still doesn't have

00:23:16,520 --> 00:23:21,110
Python support but and it's a little bit

00:23:19,429 --> 00:23:24,650
hard to stand up all the services

00:23:21,110 --> 00:23:28,220
involved but but but it's a completely

00:23:24,650 --> 00:23:31,220
viable tool yeah we saw some screenshots

00:23:28,220 --> 00:23:34,340
earlier right if you don't want to buy

00:23:31,220 --> 00:23:37,370
it there's free tools my company

00:23:34,340 --> 00:23:39,230
produces traceview probably most of you

00:23:37,370 --> 00:23:42,770
are familiar with New Relic AppDynamics

00:23:39,230 --> 00:23:45,049
also makes a tracing tool there are free

00:23:42,770 --> 00:23:49,160
plans available I think from all three

00:23:45,049 --> 00:23:50,990
of those vendors and there are also you

00:23:49,160 --> 00:23:58,070
know paid plans where they'll manage a

00:23:50,990 --> 00:24:01,460
ton of data for you they're nice because

00:23:58,070 --> 00:24:04,970
the instrumentation for these guys tends

00:24:01,460 --> 00:24:08,930
to be sort of a one-stop thing you just

00:24:04,970 --> 00:24:13,370
sort of drop the packages in

00:24:08,930 --> 00:24:14,480
and like this is a Django set up you

00:24:13,370 --> 00:24:15,980
just import it at the top of your

00:24:14,480 --> 00:24:23,510
setting stop by file and it patches

00:24:15,980 --> 00:24:26,720
itself into place but there are some

00:24:23,510 --> 00:24:28,700
limits to tracing right the graphs kind

00:24:26,720 --> 00:24:32,660
of like a limit right well this is

00:24:28,700 --> 00:24:34,610
actually a graph of a memory leak where

00:24:32,660 --> 00:24:38,300
we just restart the server periodically

00:24:34,610 --> 00:24:40,670
to clean the pool tracing is a great

00:24:38,300 --> 00:24:43,010
tool but it has limits the filtering of

00:24:40,670 --> 00:24:46,400
traces gets limited to a specific set of

00:24:43,010 --> 00:24:48,890
metadata that that the tracing tool

00:24:46,400 --> 00:24:51,890
considers relevant and the more metadata

00:24:48,890 --> 00:24:59,210
you the more dimensionality you add to

00:24:51,890 --> 00:25:01,520
your data the more data storage and

00:24:59,210 --> 00:25:07,100
latency you introduce into your into

00:25:01,520 --> 00:25:08,870
your tracing tool also you either rely

00:25:07,100 --> 00:25:11,900
on probabilistic sampling and so you can

00:25:08,870 --> 00:25:13,820
miss certain outliers or you only catch

00:25:11,900 --> 00:25:17,000
the outliers and so you missed the fat

00:25:13,820 --> 00:25:18,140
middle depending on which methodology

00:25:17,000 --> 00:25:23,300
you pick when you're tracing you're

00:25:18,140 --> 00:25:25,970
going to miss something so it's a great

00:25:23,300 --> 00:25:27,470
place to start but it also isn't the

00:25:25,970 --> 00:25:30,500
only way to look at your performance in

00:25:27,470 --> 00:25:32,600
fact it's it's it's it's really just a

00:25:30,500 --> 00:25:34,640
place to start in a lot of place a lot

00:25:32,600 --> 00:25:36,950
of ways so we've looked at a bunch of

00:25:34,640 --> 00:25:38,330
tools for performance management and all

00:25:36,950 --> 00:25:40,940
of them have strengths and all of them

00:25:38,330 --> 00:25:44,270
have limits any of those could be the

00:25:40,940 --> 00:25:46,550
hammer right instead you got to think

00:25:44,270 --> 00:25:48,590
about them as a toolbox you start an

00:25:46,550 --> 00:25:50,180
investigation in a tracing tool to give

00:25:48,590 --> 00:25:52,610
you the high-level insight into wearing

00:25:50,180 --> 00:25:54,290
your application things are slow maybe

00:25:52,610 --> 00:25:55,670
you're getting some outliers and you

00:25:54,290 --> 00:25:57,140
might want to check your hosts with the

00:25:55,670 --> 00:25:59,090
operating system tools to see if you're

00:25:57,140 --> 00:26:00,470
running into resource constraints maybe

00:25:59,090 --> 00:26:02,570
you filter the traces down until you

00:26:00,470 --> 00:26:05,390
find some slow high traffic code paths

00:26:02,570 --> 00:26:07,250
and you switch over to a to a profiler

00:26:05,390 --> 00:26:10,940
at that point to look to see what codes

00:26:07,250 --> 00:26:12,710
actually causing your problems and you

00:26:10,940 --> 00:26:15,110
also always want to be tracking the key

00:26:12,710 --> 00:26:17,690
events and durations to get some insight

00:26:15,110 --> 00:26:21,320
into why things are happening instead of

00:26:17,690 --> 00:26:22,520
just how no one tool has it all so yeah

00:26:21,320 --> 00:26:25,880
build a toolbox

00:26:22,520 --> 00:26:27,950
don't pick a hammer I have some

00:26:25,880 --> 00:26:29,150
references in the slides including these

00:26:27,950 --> 00:26:31,810
slides which is I guess a little

00:26:29,150 --> 00:26:34,810
recursive but maybe that's helpful and

00:26:31,810 --> 00:26:34,810
thanks

00:26:40,550 --> 00:26:44,130
we have time for maybe one or two

00:26:42,690 --> 00:26:46,370
question if you have to stand up in the

00:26:44,130 --> 00:26:46,370
middle

00:26:57,990 --> 00:27:04,020
a quick question about whether you think

00:27:00,750 --> 00:27:08,580
it's better to trace explicitly or

00:27:04,020 --> 00:27:10,980
monkey patch existing instrumentation I

00:27:08,580 --> 00:27:14,010
guess is part one and then part two is

00:27:10,980 --> 00:27:16,710
do you think there's value in building

00:27:14,010 --> 00:27:19,230
the trace retrospectively from other

00:27:16,710 --> 00:27:21,330
instrument instrument data if you have

00:27:19,230 --> 00:27:23,309
the trace ID but it's being collected

00:27:21,330 --> 00:27:28,370
and other metric systems can you kind of

00:27:23,309 --> 00:27:32,340
recreate traces of interest so I think

00:27:28,370 --> 00:27:35,100
let's see if I understand the second

00:27:32,340 --> 00:27:37,710
part it's if you've got trace data that

00:27:35,100 --> 00:27:40,080
is tagged but in multiple monitoring

00:27:37,710 --> 00:27:43,350
databases is there value in reassembling

00:27:40,080 --> 00:27:45,630
that and I think the trace context

00:27:43,350 --> 00:27:48,000
always helps you understand a little bit

00:27:45,630 --> 00:27:50,910
better about the why and the how the

00:27:48,000 --> 00:27:52,350
latency occurs so if there's ways for

00:27:50,910 --> 00:27:54,179
you to assemble that if you can make

00:27:52,350 --> 00:27:56,429
that happen then I think that that extra

00:27:54,179 --> 00:27:58,140
context is usually helpful when you go

00:27:56,429 --> 00:28:00,390
in to try to actually solve the problem

00:27:58,140 --> 00:28:02,670
or address it right I'm not sure I

00:28:00,390 --> 00:28:05,460
understood your first question though

00:28:02,670 --> 00:28:07,500
can you well I mean I looked at OBO

00:28:05,460 --> 00:28:09,600
briefly briefly during the talk and saw

00:28:07,500 --> 00:28:14,550
that it does some monkey patching right

00:28:09,600 --> 00:28:17,190
which is always sure sure I mean so the

00:28:14,550 --> 00:28:20,130
reason we do the monkey patching is

00:28:17,190 --> 00:28:24,690
essentially just to wrap the blocks that

00:28:20,130 --> 00:28:26,130
you would time with with timers and to

00:28:24,690 --> 00:28:33,000
ensure that we're passing along the

00:28:26,130 --> 00:28:34,880
trace metadata and the alternative to

00:28:33,000 --> 00:28:38,250
monkey patching I think in this case

00:28:34,880 --> 00:28:40,640
because many of these interfaces don't

00:28:38,250 --> 00:28:45,540
provide any other extension behavior

00:28:40,640 --> 00:28:48,059
would be to to require the application

00:28:45,540 --> 00:28:50,910
developer to explicitly have tracing in

00:28:48,059 --> 00:28:55,080
mind at at every point along the way and

00:28:50,910 --> 00:28:57,390
so I think that the monkey patching in

00:28:55,080 --> 00:29:02,990
this case probably makes a decent amount

00:28:57,390 --> 00:29:05,420
of sense in other language

00:29:02,990 --> 00:29:08,270
instrumentations we have

00:29:05,420 --> 00:29:11,270
we've rewritten the language or we've

00:29:08,270 --> 00:29:14,270
written bolt-on modules that inject code

00:29:11,270 --> 00:29:17,450
and honestly pythons monkey patching

00:29:14,270 --> 00:29:22,660
makes this almost the least intrusive of

00:29:17,450 --> 00:29:22,660
the of the strategies thank you sure

00:29:23,380 --> 00:29:26,680

YouTube URL: https://www.youtube.com/watch?v=UAztOuO1ANQ


