Title: How to build a brain with Python
Publication date: 2015-04-11
Playlist: PyCon 2015
Description: 
	
Captions: 
	00:00:12,440 --> 00:00:20,490
hi everyone how y'all doing good good

00:00:16,020 --> 00:00:21,869
enjoying your PyCon so far everybody

00:00:20,490 --> 00:00:25,350
gonna go back to the hotel nap or

00:00:21,869 --> 00:00:27,960
something come on uh anyway uh we were

00:00:25,350 --> 00:00:30,630
about to have a fascinating talk Trevor

00:00:27,960 --> 00:00:34,559
Berbick Olli oh I killed it

00:00:30,630 --> 00:00:36,690
that's all right look Olli thank you

00:00:34,559 --> 00:00:37,950
I I promise I practiced that for like

00:00:36,690 --> 00:00:39,840
five minutes and I still messed it up

00:00:37,950 --> 00:00:41,820
anyway he is a theoretical

00:00:39,840 --> 00:00:42,629
neuroscientist that is a real thing and

00:00:41,820 --> 00:00:44,730
it sounds awesome

00:00:42,629 --> 00:00:52,399
and he's going to talk to us about how

00:00:44,730 --> 00:00:54,989
to build a brain with Python Thanks

00:00:52,399 --> 00:00:57,660
today today I want to tell you about

00:00:54,989 --> 00:00:59,930
your own brain so the human brain is

00:00:57,660 --> 00:01:02,879
really an incredible computing device

00:00:59,930 --> 00:01:05,400
weighs about 2 kilograms it's composed

00:01:02,879 --> 00:01:07,140
of 86 billion neurons and about a

00:01:05,400 --> 00:01:10,710
quadrillion connections between those

00:01:07,140 --> 00:01:13,740
neurons it consumes only about 20 watts

00:01:10,710 --> 00:01:15,689
of energy and yet it's responsible for

00:01:13,740 --> 00:01:18,299
everything that humans have created you

00:01:15,689 --> 00:01:21,030
know art language computing Python which

00:01:18,299 --> 00:01:22,290
is why we're all here today so I'm a

00:01:21,030 --> 00:01:24,390
theoretical neuroscientist

00:01:22,290 --> 00:01:26,400
and what I do is I build models of the

00:01:24,390 --> 00:01:28,020
brain build computer simulations of the

00:01:26,400 --> 00:01:29,990
brain and today I want to tell you how

00:01:28,020 --> 00:01:32,640
we use Python to make those simulations

00:01:29,990 --> 00:01:35,280
so it's worth first starting off with

00:01:32,640 --> 00:01:36,840
why we want to simulate the brain the

00:01:35,280 --> 00:01:39,030
first reason is that we want to

00:01:36,840 --> 00:01:41,400
understand it so the late Richard

00:01:39,030 --> 00:01:42,869
Fineman shortly before his passing wrote

00:01:41,400 --> 00:01:44,490
this on the blackboard what I cannot

00:01:42,869 --> 00:01:46,740
create I do not understand

00:01:44,490 --> 00:01:49,020
so our opinion is that if we want to

00:01:46,740 --> 00:01:50,159
understand the brain will we the way

00:01:49,020 --> 00:01:52,500
that we're going to attack it is to try

00:01:50,159 --> 00:01:54,149
to build a model of one to replicate the

00:01:52,500 --> 00:01:56,189
function of the brain under the same

00:01:54,149 --> 00:01:59,700
types of constraints that the biological

00:01:56,189 --> 00:02:01,290
system is under the second reason that

00:01:59,700 --> 00:02:03,960
we want to create a model of the brain

00:02:01,290 --> 00:02:04,860
is to build better artificial

00:02:03,960 --> 00:02:07,910
intelligences

00:02:04,860 --> 00:02:11,220
so there are pessimistic and optimistic

00:02:07,910 --> 00:02:13,620
examples of AIS and science fiction and

00:02:11,220 --> 00:02:13,950
hopefully today I can I can convince you

00:02:13,620 --> 00:02:16,530
that

00:02:13,950 --> 00:02:18,510
use pessimistic eventualities are not

00:02:16,530 --> 00:02:20,220
that likely partly because it's very

00:02:18,510 --> 00:02:22,349
difficult to engineer these types of

00:02:20,220 --> 00:02:24,090
intelligences takes a lot of work and so

00:02:22,349 --> 00:02:27,410
this type of emergency / intelligence

00:02:24,090 --> 00:02:29,459
it's very unlikely to happen in practice

00:02:27,410 --> 00:02:31,290
so the types of software that I'm going

00:02:29,459 --> 00:02:33,390
to tell you about today are called

00:02:31,290 --> 00:02:35,550
neural simulators there's three very

00:02:33,390 --> 00:02:38,250
important features of neural simulators

00:02:35,550 --> 00:02:40,290
that I'm going to talk about the first

00:02:38,250 --> 00:02:42,630
is that they attempt to emulate neural

00:02:40,290 --> 00:02:45,000
biology at some level of abstraction and

00:02:42,630 --> 00:02:47,459
as a consequence of emulating neural

00:02:45,000 --> 00:02:49,680
biology they operate in continuous time

00:02:47,459 --> 00:02:51,510
and they communicate with spikes so I'll

00:02:49,680 --> 00:02:54,120
talk about a bit about what that means

00:02:51,510 --> 00:02:56,400
shortly but I bring up these three kind

00:02:54,120 --> 00:02:58,620
of rules as a way to differentiate

00:02:56,400 --> 00:03:00,840
neural simulators from other types of

00:02:58,620 --> 00:03:02,610
artificial neural network programs so

00:03:00,840 --> 00:03:05,220
you may have heard about these kind of

00:03:02,610 --> 00:03:07,220
massively huge deep learning systems

00:03:05,220 --> 00:03:11,519
that Facebook and Google are using to

00:03:07,220 --> 00:03:12,690
process images and speech that's not

00:03:11,519 --> 00:03:14,849
what I'm going to be talking about today

00:03:12,690 --> 00:03:17,370
these systems don't attempt to emulate

00:03:14,849 --> 00:03:19,049
neural biology in particular they work

00:03:17,370 --> 00:03:20,549
in discrete time rather than continuous

00:03:19,049 --> 00:03:21,109
time and they don't communicate the

00:03:20,549 --> 00:03:23,160
spikes

00:03:21,109 --> 00:03:24,959
so the first neural simulator that I'm

00:03:23,160 --> 00:03:27,000
going to talk about is appropriately one

00:03:24,959 --> 00:03:28,560
of the oldest yet it's still being

00:03:27,000 --> 00:03:31,920
actively developed and in use today

00:03:28,560 --> 00:03:33,930
it's called neuron in its core 2 C

00:03:31,920 --> 00:03:37,950
program with a graphical interface with

00:03:33,930 --> 00:03:39,180
some Python bindings and so let's start

00:03:37,950 --> 00:03:40,889
it up there's a little thing that it

00:03:39,180 --> 00:03:44,280
comes with called the neuron demo so

00:03:40,889 --> 00:03:47,940
let's start up the neuron demo like I

00:03:44,280 --> 00:03:50,310
said old program old GUI and so I'll

00:03:47,940 --> 00:03:52,109
load up this pyramidal example to give

00:03:50,310 --> 00:03:55,590
you a little bit of a sense of what I

00:03:52,109 --> 00:03:58,650
mean by neurobiology so this is

00:03:55,590 --> 00:04:01,470
essentially a simulated neuron all of

00:03:58,650 --> 00:04:03,569
these processes around here except for

00:04:01,470 --> 00:04:05,310
one of them are called dendrites and you

00:04:03,569 --> 00:04:07,530
can think of dendrites as providing

00:04:05,310 --> 00:04:10,230
input to this cell and they provide

00:04:07,530 --> 00:04:12,359
their input as current all these

00:04:10,230 --> 00:04:14,190
dendrites converge upon this center

00:04:12,359 --> 00:04:17,130
point the body of the cell which we call

00:04:14,190 --> 00:04:19,260
the soma and out of the soma there will

00:04:17,130 --> 00:04:22,169
be one projection that provides output

00:04:19,260 --> 00:04:24,210
to other cells which we call the axon so

00:04:22,169 --> 00:04:26,310
essentially it's an oversimplification

00:04:24,210 --> 00:04:27,450
but one way to think about a neuron is

00:04:26,310 --> 00:04:29,640
essentially as a

00:04:27,450 --> 00:04:32,070
that takes in inputs from its dendrites

00:04:29,640 --> 00:04:35,220
and provides a single output through its

00:04:32,070 --> 00:04:37,770
axon so what neuron is doing right now

00:04:35,220 --> 00:04:40,680
is called a current clamp experiment so

00:04:37,770 --> 00:04:43,050
this I clamp here i stands for current

00:04:40,680 --> 00:04:45,360
and essentially at this blue dot there's

00:04:43,050 --> 00:04:46,920
a simulated electrode that's injecting a

00:04:45,360 --> 00:04:50,180
little bit of current into the soma of

00:04:46,920 --> 00:04:53,580
this cell so if I run this model now

00:04:50,180 --> 00:04:56,790
over here this graph populates on the

00:04:53,580 --> 00:05:00,030
x-axis we have time in milliseconds on

00:04:56,790 --> 00:05:02,940
the y-axis we have the voltage recorded

00:05:00,030 --> 00:05:04,290
from this cell and just in this kind of

00:05:02,940 --> 00:05:05,670
screenshot we have the three features

00:05:04,290 --> 00:05:06,090
and drawl simulators that I was talking

00:05:05,670 --> 00:05:08,520
about

00:05:06,090 --> 00:05:10,170
so we're emulating neurobiology we're

00:05:08,520 --> 00:05:12,630
working in continuous time there's no

00:05:10,170 --> 00:05:15,440
discontinuities in this graph and this

00:05:12,630 --> 00:05:20,400
upwards and downwards stereotypes

00:05:15,440 --> 00:05:24,270
voltage trace is called a spike so if we

00:05:20,400 --> 00:05:26,970
move this this current clamp to another

00:05:24,270 --> 00:05:29,670
part of the of the dendritic tree we run

00:05:26,970 --> 00:05:31,020
it again we no longer get a spike so

00:05:29,670 --> 00:05:32,880
it's still some current is being

00:05:31,020 --> 00:05:34,680
injected but it doesn't spike spikes are

00:05:32,880 --> 00:05:36,600
the ways are the way the single way that

00:05:34,680 --> 00:05:39,780
neurons communicates at least in this

00:05:36,600 --> 00:05:41,910
oversimplified picture when a one of

00:05:39,780 --> 00:05:43,650
cells spikes that spike travels down the

00:05:41,910 --> 00:05:46,380
axon and communicates a little bit of

00:05:43,650 --> 00:05:48,060
information to the downstream neurons so

00:05:46,380 --> 00:05:49,620
as you can see there's lots of things

00:05:48,060 --> 00:05:52,020
that we could do in this simulation lots

00:05:49,620 --> 00:05:53,550
of variables we could play around with

00:05:52,020 --> 00:05:56,100
and so I'm going to play around with

00:05:53,550 --> 00:05:58,500
those a little bit in Python using the

00:05:56,100 --> 00:06:00,540
Python bindings of neuron so this is

00:05:58,500 --> 00:06:02,820
going to be a simplified cell it just

00:06:00,540 --> 00:06:06,000
has two parts a soma and a single

00:06:02,820 --> 00:06:07,260
dendrite we set essentially what we're

00:06:06,000 --> 00:06:09,540
doing here is setting some parameters

00:06:07,260 --> 00:06:12,990
the length of our soma is going to be 40

00:06:09,540 --> 00:06:14,610
micro meters and 20 micrometers in

00:06:12,990 --> 00:06:16,890
diameter we're going to insert some ion

00:06:14,610 --> 00:06:19,080
channels into there the dendrite is

00:06:16,890 --> 00:06:21,360
going to be 150 micrometers long and

00:06:19,080 --> 00:06:23,370
three micrometers in diameter has a

00:06:21,360 --> 00:06:25,500
different set of ion channels connect it

00:06:23,370 --> 00:06:28,470
up together and then to simulate our

00:06:25,500 --> 00:06:30,930
current clamp experiments we make this

00:06:28,470 --> 00:06:32,270
eye clamp object we give it the thing

00:06:30,930 --> 00:06:34,830
that we're clamping the currents of

00:06:32,270 --> 00:06:37,110
we're going to inject in this case one

00:06:34,830 --> 00:06:39,990
nano amp of current for 500 milliseconds

00:06:37,110 --> 00:06:40,650
and then what neuron calls vectors are

00:06:39,990 --> 00:06:42,900
how were

00:06:40,650 --> 00:06:46,949
essentially collecting the data from

00:06:42,900 --> 00:06:49,889
that experiment so we can initialize the

00:06:46,949 --> 00:06:52,410
voltage in this cell initial and then

00:06:49,889 --> 00:06:55,050
run it for 500 milliseconds we can take

00:06:52,410 --> 00:06:57,240
the the time and the voltage and put

00:06:55,050 --> 00:06:59,060
them into numpy arrays and then use

00:06:57,240 --> 00:07:02,820
matplotlib to create a plot of that

00:06:59,060 --> 00:07:05,130
membrane voltage over time so in this

00:07:02,820 --> 00:07:06,479
500 millisecond window it spikes quite a

00:07:05,130 --> 00:07:08,370
bit there's a lot of information being

00:07:06,479 --> 00:07:10,919
transmitted from this cell to its

00:07:08,370 --> 00:07:13,050
downstream cells so you might be

00:07:10,919 --> 00:07:14,970
wondering how this simulation actually

00:07:13,050 --> 00:07:16,500
works what's kind of the math behind it

00:07:14,970 --> 00:07:19,169
I'm out going to then very much detail

00:07:16,500 --> 00:07:21,300
but I'll say that neuron has built into

00:07:19,169 --> 00:07:23,729
it a number of mathematical equations

00:07:21,300 --> 00:07:25,560
that essentially emulates electrical

00:07:23,729 --> 00:07:28,080
physics so we think of as a neuron is

00:07:25,560 --> 00:07:30,150
kind of like an electrical device and

00:07:28,080 --> 00:07:32,880
there are more precise and less precise

00:07:30,150 --> 00:07:34,260
ways to emulate those physics if you

00:07:32,880 --> 00:07:35,699
wanted to write your own neuron model

00:07:34,260 --> 00:07:37,860
that is you know either a more or less

00:07:35,699 --> 00:07:40,289
precise version of what neuron has built

00:07:37,860 --> 00:07:42,389
in you can write it in this language

00:07:40,289 --> 00:07:44,639
it's kind of a domain-specific language

00:07:42,389 --> 00:07:46,440
for writing neuron models unfortunately

00:07:44,639 --> 00:07:49,560
it's not Python it's actually you know

00:07:46,440 --> 00:07:50,849
not not the worst language if you're if

00:07:49,560 --> 00:07:55,410
you're a theoretical neuroscientist all

00:07:50,849 --> 00:07:56,789
these things make sense to you so so

00:07:55,410 --> 00:07:58,080
it's not too terrible language but it

00:07:56,789 --> 00:07:59,340
means that there's a two-step process

00:07:58,080 --> 00:08:01,349
for running your model you have to

00:07:59,340 --> 00:08:03,720
compile this then run your Python script

00:08:01,349 --> 00:08:05,820
and that's not great and so there's

00:08:03,720 --> 00:08:08,039
other neural simulators the next one

00:08:05,820 --> 00:08:10,800
I'll talk about is called Brian Brian is

00:08:08,039 --> 00:08:12,810
a neuro simulator implemented entirely

00:08:10,800 --> 00:08:14,490
in Python and it puts the equations

00:08:12,810 --> 00:08:17,610
behind these neurons kind of front and

00:08:14,490 --> 00:08:20,580
center so an example example Brian

00:08:17,610 --> 00:08:23,220
scripts to create a neural model kind of

00:08:20,580 --> 00:08:25,020
has its core this string which defines

00:08:23,220 --> 00:08:27,180
the equations of these cells so a

00:08:25,020 --> 00:08:28,729
complicated complicated equation don't

00:08:27,180 --> 00:08:31,169
expect you actually like understand it

00:08:28,729 --> 00:08:33,419
but it's you know just a string in that

00:08:31,169 --> 00:08:36,360
Python script which gets regenerated

00:08:33,419 --> 00:08:37,979
every time we run it up here we're just

00:08:36,360 --> 00:08:39,690
defining a bunch of parameters and this

00:08:37,979 --> 00:08:42,390
shows off another nice feature of Brian

00:08:39,690 --> 00:08:44,490
which is that it explicitly tracks the

00:08:42,390 --> 00:08:47,160
units associated with these parameters

00:08:44,490 --> 00:08:49,560
so this reset voltage for example is

00:08:47,160 --> 00:08:51,450
negative 70 point 6 millivolts which is

00:08:49,560 --> 00:08:52,890
very important for validation of models

00:08:51,450 --> 00:08:54,390
making sure that you're doing the right

00:08:52,890 --> 00:08:57,459
thing

00:08:54,390 --> 00:08:59,380
so we can create a web Bryan calls an

00:08:57,459 --> 00:09:00,459
Iran group with those equations we're

00:08:59,380 --> 00:09:02,800
just going to make one neuron in this

00:09:00,459 --> 00:09:05,140
case we set its membrane voltage to that

00:09:02,800 --> 00:09:07,839
reset voltage inject one-and-a-half nano

00:09:05,140 --> 00:09:10,269
amps of current and when we yep so we

00:09:07,839 --> 00:09:11,680
can then we monitor the voltage from

00:09:10,269 --> 00:09:14,589
that cell and the spikes coming out of

00:09:11,680 --> 00:09:16,149
it run it for 500 milliseconds and we

00:09:14,589 --> 00:09:19,000
get something that looks very similar to

00:09:16,149 --> 00:09:20,410
what we had in neuron so across this 500

00:09:19,000 --> 00:09:22,149
millisecond time window we have a bunch

00:09:20,410 --> 00:09:24,579
of spikes it kind of seems to slow

00:09:22,149 --> 00:09:27,910
spiking after a while that's interesting

00:09:24,579 --> 00:09:29,709
I suppose but to me it's not you know

00:09:27,910 --> 00:09:31,720
the most interesting thing because like

00:09:29,709 --> 00:09:33,430
I said you know this talk is about how

00:09:31,720 --> 00:09:35,019
to build a brain but so far all I've

00:09:33,430 --> 00:09:37,089
showed you is how to stimulate neurons

00:09:35,019 --> 00:09:38,920
and to me those two things are kind of

00:09:37,089 --> 00:09:40,890
qualitatively different and so the

00:09:38,920 --> 00:09:44,410
question is how do we scale up these

00:09:40,890 --> 00:09:46,690
simulations from neural simulators of

00:09:44,410 --> 00:09:48,370
which there are many as so these are

00:09:46,690 --> 00:09:50,019
eight examples in Python but there are

00:09:48,370 --> 00:09:52,029
actually quite a few more how do we

00:09:50,019 --> 00:09:53,589
scale up these types of simulations to

00:09:52,029 --> 00:09:55,060
something that's brain like and you know

00:09:53,589 --> 00:09:57,430
something that you could say has some

00:09:55,060 --> 00:09:58,200
kind of intelligence so scaling from

00:09:57,430 --> 00:10:00,339
rounds to brains

00:09:58,200 --> 00:10:02,110
people have been trying to tackle this

00:10:00,339 --> 00:10:03,399
problem for quite a while and have come

00:10:02,110 --> 00:10:05,500
up with a bunch of different options

00:10:03,399 --> 00:10:07,060
which I'm going to pull the audience to

00:10:05,500 --> 00:10:10,870
see what you think is is the most

00:10:07,060 --> 00:10:12,820
promising so one option is to so I

00:10:10,870 --> 00:10:14,529
should say you know scaling up we know

00:10:12,820 --> 00:10:16,480
that we need an 86 billion neurons

00:10:14,529 --> 00:10:17,829
Network you know you can just throw a

00:10:16,480 --> 00:10:19,450
bunch of computers at that that'll

00:10:17,829 --> 00:10:21,160
happen eventually that's fine that's no

00:10:19,450 --> 00:10:22,630
problem but it's the one quadrillion

00:10:21,160 --> 00:10:24,850
connections that's really the problem

00:10:22,630 --> 00:10:26,320
because how do you figure out how to

00:10:24,850 --> 00:10:27,760
connect these neurons up together each

00:10:26,320 --> 00:10:29,560
connection has a weight associated with

00:10:27,760 --> 00:10:32,560
it you know how do we figure out what

00:10:29,560 --> 00:10:34,029
are good weights so the first option is

00:10:32,560 --> 00:10:36,550
that you could just randomly generate a

00:10:34,029 --> 00:10:38,050
whole bunch of weights and if you're if

00:10:36,550 --> 00:10:39,399
you really believe that the neuron is

00:10:38,050 --> 00:10:40,630
essential to what's happening

00:10:39,399 --> 00:10:42,100
intelligence it might be the case that

00:10:40,630 --> 00:10:46,209
are connecting them up randomly is

00:10:42,100 --> 00:10:49,420
sufficient the second option is to do

00:10:46,209 --> 00:10:51,279
some recordings from brains to figure

00:10:49,420 --> 00:10:53,019
out the connectivity patterns the

00:10:51,279 --> 00:10:55,839
statistics of connections in them in the

00:10:53,019 --> 00:10:58,480
actual real brain and people do this in

00:10:55,839 --> 00:10:59,529
a field called connect omics and you

00:10:58,480 --> 00:11:01,240
basically get an idea of what's

00:10:59,529 --> 00:11:04,600
connected and the relative strength

00:11:01,240 --> 00:11:06,670
between those areas the third option is

00:11:04,600 --> 00:11:08,860
to connect things up functionally

00:11:06,670 --> 00:11:12,460
so neuroscientists have essentially

00:11:08,860 --> 00:11:14,530
ascribed function functional functions

00:11:12,460 --> 00:11:16,510
to a bunch of brain areas so at the back

00:11:14,530 --> 00:11:18,970
you have visual areas areas of process

00:11:16,510 --> 00:11:21,220
visual information motor areas kind of

00:11:18,970 --> 00:11:23,050
working memories at the front here one

00:11:21,220 --> 00:11:25,630
way we could scale would be to kind of

00:11:23,050 --> 00:11:27,370
take a region of the brain and try to

00:11:25,630 --> 00:11:29,950
replicate its function in a brain like

00:11:27,370 --> 00:11:31,810
way so let's get a little quick show of

00:11:29,950 --> 00:11:34,840
hands even though I've highly biased

00:11:31,810 --> 00:11:36,850
this crowd but who thinks that option

00:11:34,840 --> 00:11:39,880
one connecting these neurons randomly is

00:11:36,850 --> 00:11:43,660
a you know might result in human scale

00:11:39,880 --> 00:11:45,460
intelligence ends up for that good we

00:11:43,660 --> 00:11:47,530
got a few hands nice okay what about

00:11:45,460 --> 00:11:49,120
option to think who thinks that the

00:11:47,530 --> 00:11:50,470
statistics are the most salient points

00:11:49,120 --> 00:11:54,790
and connecting them up that way will

00:11:50,470 --> 00:11:56,440
work also quite a few hands what about

00:11:54,790 --> 00:11:58,180
option three hands up for if you think

00:11:56,440 --> 00:12:01,570
that the function is the most important

00:11:58,180 --> 00:12:04,960
part of things here apparently I did not

00:12:01,570 --> 00:12:05,920
bias you hard enough so seems that

00:12:04,960 --> 00:12:09,400
there's kind of an even split between

00:12:05,920 --> 00:12:11,290
statistics and function in my lab or a

00:12:09,400 --> 00:12:13,480
crystallized miss lab we're really

00:12:11,290 --> 00:12:16,090
dedicated to scaling things on a

00:12:13,480 --> 00:12:18,280
functional level so we're interested in

00:12:16,090 --> 00:12:19,450
how are these neural networks how are

00:12:18,280 --> 00:12:22,960
these networks of spiking neurons

00:12:19,450 --> 00:12:24,280
representing information transforming

00:12:22,960 --> 00:12:26,650
that information and how can we use that

00:12:24,280 --> 00:12:31,330
to build essentially functionally

00:12:26,650 --> 00:12:33,160
relevant brain models so the tool that

00:12:31,330 --> 00:12:33,790
we've created to do this is called

00:12:33,160 --> 00:12:36,820
nangou

00:12:33,790 --> 00:12:38,290
I should say that you know it's possible

00:12:36,820 --> 00:12:39,960
to come up with these functional

00:12:38,290 --> 00:12:42,520
networks in a very kind of precise

00:12:39,960 --> 00:12:43,690
methodological way of varying parameters

00:12:42,520 --> 00:12:46,530
and seeing if you get functional

00:12:43,690 --> 00:12:50,110
networks but we think that tools are

00:12:46,530 --> 00:12:51,550
tools really needed in this space so the

00:12:50,110 --> 00:12:53,500
tool that we create is called nangou and

00:12:51,550 --> 00:12:54,790
it connects neurons together using the

00:12:53,500 --> 00:12:58,330
principles of the neural engineering

00:12:54,790 --> 00:12:59,530
framework and just like the other neural

00:12:58,330 --> 00:13:01,600
simulators I've talked about to this

00:12:59,530 --> 00:13:03,940
point it attempts to emulate neural

00:13:01,600 --> 00:13:08,500
biology it works in continuous time and

00:13:03,940 --> 00:13:09,940
it communicates using spikes so to this

00:13:08,500 --> 00:13:11,170
point I've been talking about you know

00:13:09,940 --> 00:13:13,930
neural spikes and I've been showing you

00:13:11,170 --> 00:13:15,610
voltage traces and I want to you know

00:13:13,930 --> 00:13:17,080
we're talking about scaling up and so

00:13:15,610 --> 00:13:19,240
the way that we scale up those voltage

00:13:17,080 --> 00:13:19,540
traces are to plot what I call our what

00:13:19,240 --> 00:13:21,759
we

00:13:19,540 --> 00:13:23,920
call spy Craster's so each of these

00:13:21,759 --> 00:13:26,500
vertical ticks represents a spike coming

00:13:23,920 --> 00:13:29,350
from a neuron in our case across the

00:13:26,500 --> 00:13:31,389
rows we have individual neurons so this

00:13:29,350 --> 00:13:35,769
row shows you the spikes that this

00:13:31,389 --> 00:13:37,449
neuron emitted over time and there's

00:13:35,769 --> 00:13:39,040
clearly some pattern to this spike

00:13:37,449 --> 00:13:41,259
raster you know there's clearly a lot of

00:13:39,040 --> 00:13:43,389
activity around this part among a

00:13:41,259 --> 00:13:45,130
certain subset of the cells and a lot of

00:13:43,389 --> 00:13:47,259
activity over here for the other subset

00:13:45,130 --> 00:13:49,720
of the cells and so how do we make any

00:13:47,259 --> 00:13:53,170
sense of this how can we use these

00:13:49,720 --> 00:13:55,829
neural the neural firing patterns to

00:13:53,170 --> 00:13:55,829
represent information

00:13:56,019 --> 00:14:01,930
fortunately neurons don't respond in an

00:13:58,329 --> 00:14:03,550
entirely arbitrary way we can kind of

00:14:01,930 --> 00:14:05,800
characterize the way that a particular

00:14:03,550 --> 00:14:07,899
neuron responds with a tuning curve

00:14:05,800 --> 00:14:09,850
which is what this is so imagine that

00:14:07,899 --> 00:14:12,670
you're recording from a neuron that's

00:14:09,850 --> 00:14:15,130
downstream from you know the the sensory

00:14:12,670 --> 00:14:16,839
part of your fingertip your right finger

00:14:15,130 --> 00:14:19,540
tip if you put a little bit of pressure

00:14:16,839 --> 00:14:22,089
the nerve endings and your finger will

00:14:19,540 --> 00:14:23,620
send current down to that neuron and the

00:14:22,089 --> 00:14:25,240
more pressure you put on to it the more

00:14:23,620 --> 00:14:27,670
current the more current gets injected

00:14:25,240 --> 00:14:29,050
and the more this cell will spike so if

00:14:27,670 --> 00:14:30,699
you record this a lot you can kind of

00:14:29,050 --> 00:14:32,740
average this out and you get a curve

00:14:30,699 --> 00:14:34,149
like this so if you're recording from

00:14:32,740 --> 00:14:37,360
this cell and you know that it's spiking

00:14:34,149 --> 00:14:39,670
at around 200 Hertz you can kind of make

00:14:37,360 --> 00:14:41,560
the judgment call that it's you know

00:14:39,670 --> 00:14:45,490
representing a pressure amount that is

00:14:41,560 --> 00:14:46,839
kind of 0.1 in on this scale of course

00:14:45,490 --> 00:14:48,490
this is an idealized version of what

00:14:46,839 --> 00:14:50,470
really happens neurons are extremely

00:14:48,490 --> 00:14:51,579
noisy you can record from the same

00:14:50,470 --> 00:14:53,889
neuron twice and you'll get you know

00:14:51,579 --> 00:14:55,630
slightly varying results and so we

00:14:53,889 --> 00:14:57,550
really need to combine the responses of

00:14:55,630 --> 00:14:59,170
many neurons together to get an idea of

00:14:57,550 --> 00:15:01,269
what's happening and so the way that we

00:14:59,170 --> 00:15:07,810
do that in nangou is actually very

00:15:01,269 --> 00:15:09,819
analogous to binary coding so you know

00:15:07,810 --> 00:15:12,130
in computing binary coding is how we

00:15:09,819 --> 00:15:15,040
represent information if we wanted to

00:15:12,130 --> 00:15:17,079
represent for example the integer 13 we

00:15:15,040 --> 00:15:20,889
know that we can encode that as the

00:15:17,079 --> 00:15:23,259
binary string 1 1 0 1 each of these bits

00:15:20,889 --> 00:15:25,329
each of these binary digits represents a

00:15:23,259 --> 00:15:27,009
different power of 2 so at the low ends

00:15:25,329 --> 00:15:29,620
this one represents two to the zero or

00:15:27,009 --> 00:15:32,350
one at the high end this one represents

00:15:29,620 --> 00:15:33,160
two to the third or eighth and knowing

00:15:32,350 --> 00:15:35,199
you know

00:15:33,160 --> 00:15:39,819
true and false and what power of two

00:15:35,199 --> 00:15:42,310
these bits represents we we can

00:15:39,819 --> 00:15:44,379
essentially do a weighted linear sum so

00:15:42,310 --> 00:15:47,050
we wait the true or false value by the

00:15:44,379 --> 00:15:49,000
power of two add that all up and we get

00:15:47,050 --> 00:15:54,160
back our originally encoded integer

00:15:49,000 --> 00:15:55,990
thirteen as our kind of decoding so

00:15:54,160 --> 00:15:58,329
we're essentially encoding this integer

00:15:55,990 --> 00:16:00,430
into binary and decoding it back out in

00:15:58,329 --> 00:16:02,170
binary code this happens perfectly your

00:16:00,430 --> 00:16:04,230
computer never gets this wrong there

00:16:02,170 --> 00:16:07,629
pretty much always right

00:16:04,230 --> 00:16:11,920
the neural code analogously if we're

00:16:07,629 --> 00:16:13,480
trying to encode a number like 0.5 this

00:16:11,920 --> 00:16:16,329
might represent kind of medium pressure

00:16:13,480 --> 00:16:18,399
on your finger the neurons that are

00:16:16,329 --> 00:16:21,370
sensitive to that quantity are going to

00:16:18,399 --> 00:16:23,319
spike at a certain rate so say we have a

00:16:21,370 --> 00:16:26,769
neuron that's spiking at 23 Hertz 60

00:16:23,319 --> 00:16:29,889
Hertz 3 Hertz nangou we'll figure out a

00:16:26,769 --> 00:16:31,990
set of decoding weights D associated

00:16:29,889 --> 00:16:35,259
with these neurons such that we can do

00:16:31,990 --> 00:16:37,720
the same weighted sum to get back an

00:16:35,259 --> 00:16:41,560
estimate of that originally encoded

00:16:37,720 --> 00:16:44,050
value so 23 times our first decoder 60

00:16:41,560 --> 00:16:45,370
times our second decoder etc etc you add

00:16:44,050 --> 00:16:48,160
all these things up and you get a number

00:16:45,370 --> 00:16:51,399
that's close to 0.5 it's not exact but

00:16:48,160 --> 00:16:52,569
in most cases it's sufficient so what

00:16:51,399 --> 00:16:55,089
essentially this gives us is a way to

00:16:52,569 --> 00:16:56,439
represent information and we can use

00:16:55,089 --> 00:16:59,139
similar methods to transform that

00:16:56,439 --> 00:17:01,839
information and essentially what this

00:16:59,139 --> 00:17:04,539
allows us to do is to work on a higher

00:17:01,839 --> 00:17:06,429
level of abstraction than if we were

00:17:04,539 --> 00:17:09,220
just making neurons and connecting them

00:17:06,429 --> 00:17:12,069
together individually so now a good

00:17:09,220 --> 00:17:15,010
analogy for this is computers sorry

00:17:12,069 --> 00:17:16,240
our compilers in computing so you can

00:17:15,010 --> 00:17:18,880
think about this little snippet of C

00:17:16,240 --> 00:17:20,679
code pretty understandable what it does

00:17:18,880 --> 00:17:22,870
but if you go down to the assembler

00:17:20,679 --> 00:17:24,640
level it's quite difficult to understand

00:17:22,870 --> 00:17:26,829
and even this is just kind of a

00:17:24,640 --> 00:17:28,690
shorthand for binary instructions also

00:17:26,829 --> 00:17:30,549
very hard to understand and I think you

00:17:28,690 --> 00:17:32,980
can agree that if we were programming in

00:17:30,549 --> 00:17:34,000
assembler you know we wouldn't be at the

00:17:32,980 --> 00:17:35,440
state of computing that we're at right

00:17:34,000 --> 00:17:38,220
now we certainly wouldn't be at a Python

00:17:35,440 --> 00:17:41,169
conference because it's another language

00:17:38,220 --> 00:17:44,320
nangou analogously is like a neural

00:17:41,169 --> 00:17:46,220
compiler so we specify a neural model in

00:17:44,320 --> 00:17:48,080
terms of what information or what

00:17:46,220 --> 00:17:51,140
what values are being represented and

00:17:48,080 --> 00:17:52,910
transformed so up here we have a node a

00:17:51,140 --> 00:17:55,010
node is how we represent non neural

00:17:52,910 --> 00:17:56,360
information so kind of like the current

00:17:55,010 --> 00:17:58,220
that we injected in that current clap

00:17:56,360 --> 00:18:00,470
experiment we need a way to somehow

00:17:58,220 --> 00:18:04,970
start off our simulations so we call

00:18:00,470 --> 00:18:06,950
them nodes ensembles or how we represent

00:18:04,970 --> 00:18:11,179
information with populations of neurons

00:18:06,950 --> 00:18:12,620
and so when we connect them in neuron

00:18:11,179 --> 00:18:14,660
when you are sorry in nangou when you

00:18:12,620 --> 00:18:15,950
connect from a node to an ensemble we're

00:18:14,660 --> 00:18:17,539
essentially figuring out how much

00:18:15,950 --> 00:18:20,090
current needs to be injected into that

00:18:17,539 --> 00:18:22,940
population to represent the value that

00:18:20,090 --> 00:18:25,460
your node is encoding similarly when you

00:18:22,940 --> 00:18:28,280
connect two populations together in this

00:18:25,460 --> 00:18:29,659
case X and squared we figure out the

00:18:28,280 --> 00:18:31,730
connection weights between them such

00:18:29,659 --> 00:18:33,140
that they transmit that information you

00:18:31,730 --> 00:18:34,990
can also transform that information

00:18:33,140 --> 00:18:37,870
across that connection so in this case

00:18:34,990 --> 00:18:39,590
we're going to square the value in X and

00:18:37,870 --> 00:18:41,809
transmit that two squared

00:18:39,590 --> 00:18:44,659
hence the names and under-the-hood

00:18:41,809 --> 00:18:46,549
nangou generates both randomly and

00:18:44,659 --> 00:18:48,770
through some optimization processes a

00:18:46,549 --> 00:18:51,740
bunch of numbers that would be a huge

00:18:48,770 --> 00:18:53,600
pain to figure out by hand so the gain

00:18:51,740 --> 00:18:55,340
and bias on these neurons which tells

00:18:53,600 --> 00:18:57,740
you how much current to inject the

00:18:55,340 --> 00:18:59,409
decoding weights on this connection to

00:18:57,740 --> 00:19:01,669
actually implement that squared function

00:18:59,409 --> 00:19:04,510
so let's look at this happening in

00:19:01,669 --> 00:19:08,419
Python code nangou is a Python library

00:19:04,510 --> 00:19:10,840
so we take those nodes and ensembles

00:19:08,419 --> 00:19:13,280
from before we group them together in a

00:19:10,840 --> 00:19:15,110
stretching call the network and then we

00:19:13,280 --> 00:19:18,650
add some probes in order to collect data

00:19:15,110 --> 00:19:20,840
over the course of this simulation we

00:19:18,650 --> 00:19:23,299
create a simulator object so negative

00:19:20,840 --> 00:19:24,980
simulator pass it the the network that

00:19:23,299 --> 00:19:28,460
you want to simulate we run it for a

00:19:24,980 --> 00:19:30,590
second and so let's look at the things

00:19:28,460 --> 00:19:32,450
that we've probed in that simulation so

00:19:30,590 --> 00:19:34,250
the first thing is the voltage coming

00:19:32,450 --> 00:19:36,650
out of just one of the neurons in the X

00:19:34,250 --> 00:19:38,210
population you can see that just like in

00:19:36,650 --> 00:19:40,159
the other simulators we're tracking very

00:19:38,210 --> 00:19:41,360
similar types of information we can look

00:19:40,159 --> 00:19:44,030
at the voltage trace of the cell over

00:19:41,360 --> 00:19:46,010
time we can also look at the spike

00:19:44,030 --> 00:19:49,120
rester so thinking about all of the

00:19:46,010 --> 00:19:51,320
spikes that came out of our X population

00:19:49,120 --> 00:19:52,490
there's a pretty clear pattern here

00:19:51,320 --> 00:19:56,360
because we're just representing a

00:19:52,490 --> 00:19:59,420
constant value 0.5 that all the cells

00:19:56,360 --> 00:20:00,740
are kind of spiking at a regular rate

00:19:59,420 --> 00:20:03,230
but since we have this higher level of

00:20:00,740 --> 00:20:06,620
abstraction and nangou we can also look

00:20:03,230 --> 00:20:08,630
at the value that these these groups of

00:20:06,620 --> 00:20:11,450
neurons are representing over time so

00:20:08,630 --> 00:20:13,940
our X population the blue curve very

00:20:11,450 --> 00:20:15,080
quickly reaches a value of 0.5 which is

00:20:13,940 --> 00:20:18,230
what we are trying to encode in the

00:20:15,080 --> 00:20:20,750
first place and our squared population

00:20:18,230 --> 00:20:23,120
the green curve encodes x squared which

00:20:20,750 --> 00:20:25,340
is about two point point two five we're

00:20:23,120 --> 00:20:27,680
doing a pretty okay job here I just say

00:20:25,340 --> 00:20:29,570
it's not doing the best job because what

00:20:27,680 --> 00:20:31,670
we've asked this network to do represent

00:20:29,570 --> 00:20:35,030
just a 1 value as kind of an odd thing

00:20:31,670 --> 00:20:36,440
to do with these types of models like I

00:20:35,030 --> 00:20:38,810
said they operate in continuous time

00:20:36,440 --> 00:20:41,300
which means that you know they're kind

00:20:38,810 --> 00:20:43,160
of dynamic systems and so it makes they

00:20:41,300 --> 00:20:45,890
work a lot better in dynamic situations

00:20:43,160 --> 00:20:47,900
so what I could do is instead of having

00:20:45,890 --> 00:20:49,880
the output of my value node be a single

00:20:47,900 --> 00:20:52,070
value I can change it to be a function

00:20:49,880 --> 00:20:54,020
of time so in this case I'm just

00:20:52,070 --> 00:20:56,540
changing the output of my value node my

00:20:54,020 --> 00:20:59,000
Val node to be a sine wave I simulate

00:20:56,540 --> 00:21:03,170
that network again if we look at the

00:20:59,000 --> 00:21:05,270
voltage trace in that neuron in X again

00:21:03,170 --> 00:21:07,010
there are some areas here where it's not

00:21:05,270 --> 00:21:08,510
getting any injected current and so

00:21:07,010 --> 00:21:11,540
there's the voltage trace is kind of

00:21:08,510 --> 00:21:13,490
staying at a minimum value if we look at

00:21:11,540 --> 00:21:15,170
the spike raster we see now that there's

00:21:13,490 --> 00:21:16,700
kind of a clear pattern happening here

00:21:15,170 --> 00:21:18,140
that we have some cells that are

00:21:16,700 --> 00:21:19,940
sensitive to the start of the sine wave

00:21:18,140 --> 00:21:22,280
and other cells that are sensitive to

00:21:19,940 --> 00:21:25,820
the end of that sine wave and if we

00:21:22,280 --> 00:21:27,650
decode out that value we get a pretty

00:21:25,820 --> 00:21:30,650
good representation of the sine wave

00:21:27,650 --> 00:21:33,170
that we were trying to encode so we get

00:21:30,650 --> 00:21:36,500
the up-down behavior in our X population

00:21:33,170 --> 00:21:41,870
and we represent the square in this x

00:21:36,500 --> 00:21:44,540
squared population so essentially you

00:21:41,870 --> 00:21:47,120
can take these two kind of ideas that

00:21:44,540 --> 00:21:48,530
nangou is a neural compiler that uses a

00:21:47,120 --> 00:21:50,930
type of coding that's analogous to

00:21:48,530 --> 00:21:54,050
binary coding a couple other ideas and

00:21:50,930 --> 00:21:56,630
like 10 or so years of research and you

00:21:54,050 --> 00:21:58,790
can build models that work very

00:21:56,630 --> 00:22:00,050
functionally like the brain so this is a

00:21:58,790 --> 00:22:03,110
model that we published a couple years

00:22:00,050 --> 00:22:06,130
ago called spawn I'm in it we kind of

00:22:03,110 --> 00:22:10,000
figured out functionally relevant

00:22:06,130 --> 00:22:12,980
networks for vision for motor control

00:22:10,000 --> 00:22:14,180
working memory all these types of things

00:22:12,980 --> 00:22:15,650
and we put all this together and it's

00:22:14,180 --> 00:22:17,360
such a way that it can perform a

00:22:15,650 --> 00:22:19,160
cognitive tasks so what this is it

00:22:17,360 --> 00:22:21,640
performing one of those tasks and you'll

00:22:19,160 --> 00:22:24,320
see it right it's answer at the end here

00:22:21,640 --> 00:22:26,810
five five five that's the right answer

00:22:24,320 --> 00:22:28,280
in case you were curious I don't have

00:22:26,810 --> 00:22:29,900
time to go into the detail about spawn

00:22:28,280 --> 00:22:32,900
but if you're interested I encourage you

00:22:29,900 --> 00:22:34,550
to check out nan Godot CA for some

00:22:32,900 --> 00:22:37,460
videos of spawn happening you can read

00:22:34,550 --> 00:22:40,010
our our paper but what I want to

00:22:37,460 --> 00:22:41,390
highlight here by showing spawn is that

00:22:40,010 --> 00:22:45,950
you know nangou allows us to scale

00:22:41,390 --> 00:22:47,390
things up in an exciting way one of the

00:22:45,950 --> 00:22:50,300
other one of the ways in particular that

00:22:47,390 --> 00:22:51,800
we enable this type of scaling is with

00:22:50,300 --> 00:22:54,830
this new version of mango that we just

00:22:51,800 --> 00:22:57,230
released you know it took a lot of weeks

00:22:54,830 --> 00:23:00,050
of research and kind of tweaking to get

00:22:57,230 --> 00:23:01,580
spawn to work one of the components that

00:23:00,050 --> 00:23:03,320
was quite difficult is called the basal

00:23:01,580 --> 00:23:06,320
ganglia and it essentially does action

00:23:03,320 --> 00:23:09,860
selection in spawn but now with this new

00:23:06,320 --> 00:23:13,220
version of mango we can anyone can you

00:23:09,860 --> 00:23:15,320
know install pit install tango and gets

00:23:13,220 --> 00:23:16,940
the same basal ganglia in their models

00:23:15,320 --> 00:23:19,940
that we have in spawn in just one line

00:23:16,940 --> 00:23:22,610
of code similarly we've been working on

00:23:19,940 --> 00:23:24,680
ways of scaling up that are specific to

00:23:22,610 --> 00:23:28,820
different types of computing platforms

00:23:24,680 --> 00:23:30,590
so you can you can get this nangou CL

00:23:28,820 --> 00:23:33,650
package which includes a simulator that

00:23:30,590 --> 00:23:35,510
will run the exact same nangou model but

00:23:33,650 --> 00:23:37,330
it will run it using an open CL back-end

00:23:35,510 --> 00:23:39,920
which means that you can run it on GPUs

00:23:37,330 --> 00:23:42,350
while we also have a back-end that runs

00:23:39,920 --> 00:23:45,020
on a what's called a spinnaker

00:23:42,350 --> 00:23:46,190
neuromorphic hardware which is an

00:23:45,020 --> 00:23:49,550
exciting project coming out of

00:23:46,190 --> 00:23:50,720
Manchester University of Manchester so

00:23:49,550 --> 00:23:52,850
these are all the ways that nangou is

00:23:50,720 --> 00:23:55,760
helping scale up to the brain to kind of

00:23:52,850 --> 00:23:57,800
the whole brain level and if you're

00:23:55,760 --> 00:23:59,060
interested in these types of things you

00:23:57,800 --> 00:24:01,640
can go look at our github organization

00:23:59,060 --> 00:24:03,710
github calm slash Ning go if you want to

00:24:01,640 --> 00:24:04,970
know lots of details then I encourage

00:24:03,710 --> 00:24:06,710
you to check out my supervisor

00:24:04,970 --> 00:24:09,800
crystallize Smith's book how to build a

00:24:06,710 --> 00:24:11,030
brain but I want to finish off showing

00:24:09,800 --> 00:24:13,160
you one of the things that we're working

00:24:11,030 --> 00:24:14,540
on lately which is a way to kind of

00:24:13,160 --> 00:24:17,630
interactively visualize what's happening

00:24:14,540 --> 00:24:19,250
in these types of networks so what's

00:24:17,630 --> 00:24:20,570
happening this model isn't isn't super

00:24:19,250 --> 00:24:23,120
important but just know that there are

00:24:20,570 --> 00:24:26,120
two ensembles here one with a thousand

00:24:23,120 --> 00:24:26,750
neurons one with 100 neurons this one's

00:24:26,120 --> 00:24:28,160
connected to a

00:24:26,750 --> 00:24:31,450
self that one's connected to that

00:24:28,160 --> 00:24:35,720
ensemble and I'll start up our the

00:24:31,450 --> 00:24:37,670
visualizer that we've been working on so

00:24:35,720 --> 00:24:41,210
in here we get kind of a miniature

00:24:37,670 --> 00:24:43,040
version of our network so our stem node

00:24:41,210 --> 00:24:45,620
the oscillator population the shape

00:24:43,040 --> 00:24:47,540
population if I press play it starts

00:24:45,620 --> 00:24:49,460
simulating you can see that this is

00:24:47,540 --> 00:24:52,310
indeed working in continuous time time

00:24:49,460 --> 00:24:54,110
is just flowing across the x-axis we're

00:24:52,310 --> 00:24:56,420
generating spikes and we're representing

00:24:54,110 --> 00:24:58,550
values I can give this a little kick in

00:24:56,420 --> 00:25:01,520
our stem and now it represents a

00:24:58,550 --> 00:25:03,050
function that we've encoded here kind of

00:25:01,520 --> 00:25:07,340
an exotic function in a two dimensional

00:25:03,050 --> 00:25:10,190
space so you know what I hope to kind of

00:25:07,340 --> 00:25:12,200
prove by this simulation and other types

00:25:10,190 --> 00:25:13,640
of things is that you know our original

00:25:12,200 --> 00:25:16,700
goal goal was to understand how the

00:25:13,640 --> 00:25:18,170
brain works and to build a eyes I hope

00:25:16,700 --> 00:25:19,400
this shows that you know we do actually

00:25:18,170 --> 00:25:21,500
know quite a bit about how the brain

00:25:19,400 --> 00:25:25,130
works and using those ideas we've been

00:25:21,500 --> 00:25:27,710
able to create a Python library that

00:25:25,130 --> 00:25:29,990
allows you to make kind of lousy to use

00:25:27,710 --> 00:25:32,480
these biological parts to create pretty

00:25:29,990 --> 00:25:35,600
exotic functions and you can scale that

00:25:32,480 --> 00:25:37,550
up to things like spawn but what we're

00:25:35,600 --> 00:25:40,420
working on right now is really the you

00:25:37,550 --> 00:25:42,530
know more of the scaling parts to build

00:25:40,420 --> 00:25:44,540
artificial intelligences that will help

00:25:42,530 --> 00:25:47,030
humans in the real world so hopefully

00:25:44,540 --> 00:25:48,620
you know in five or ten years I can come

00:25:47,030 --> 00:25:50,870
back to Pyke on and tell you about those

00:25:48,620 --> 00:25:53,530
projects but for now I'll go all in

00:25:50,870 --> 00:25:53,530
there thanks

00:25:59,000 --> 00:26:02,580
all right we have about five minutes for

00:26:01,350 --> 00:26:04,920
questions if you want to go ahead line

00:26:02,580 --> 00:26:07,170
up at the line up the microphone here

00:26:04,920 --> 00:26:09,690
any question like to ask go right ahead

00:26:07,170 --> 00:26:12,570
a yes I have a question I know that

00:26:09,690 --> 00:26:16,410
there exists a tiny warm today C elegans

00:26:12,570 --> 00:26:18,240
yes which has only nine hundred neurons

00:26:16,410 --> 00:26:20,640
yeah the connector how they are

00:26:18,240 --> 00:26:22,830
connected is already numb yeah have you

00:26:20,640 --> 00:26:26,130
tried to simulate that nervous system

00:26:22,830 --> 00:26:27,660
yeah so so about C elegans you know that

00:26:26,130 --> 00:26:29,220
not only do we know how many neurons

00:26:27,660 --> 00:26:31,470
there are we know kind of how they're

00:26:29,220 --> 00:26:33,000
connected we have a very detailed model

00:26:31,470 --> 00:26:34,530
or asourian as you say we have a very

00:26:33,000 --> 00:26:35,940
detailed idea kind of neuro

00:26:34,530 --> 00:26:38,340
scientifically about what's the elegans

00:26:35,940 --> 00:26:39,840
brain looks like there's a project

00:26:38,340 --> 00:26:41,880
called so I should say that we haven't

00:26:39,840 --> 00:26:43,500
tried to simulate that ourselves mostly

00:26:41,880 --> 00:26:46,380
because the types of neurons that we

00:26:43,500 --> 00:26:48,059
simulate are all kind of similar so

00:26:46,380 --> 00:26:50,160
pyramidal cells in your cortex are all

00:26:48,059 --> 00:26:52,080
kind of similar and so you can kind of

00:26:50,160 --> 00:26:53,490
treat them in a similar way but in kind

00:26:52,080 --> 00:26:55,920
of invertebrates and other lower

00:26:53,490 --> 00:26:57,840
organisms all of their neurons are very

00:26:55,920 --> 00:27:00,240
complicated so you would really need a

00:26:57,840 --> 00:27:03,510
tool like neuron that allows you to have

00:27:00,240 --> 00:27:04,980
a specific function for each neuron to

00:27:03,510 --> 00:27:06,150
make a kind of a C elegans simulation

00:27:04,980 --> 00:27:09,050
but there's a project called open worm

00:27:06,150 --> 00:27:11,820
that's doing that which is very cool I

00:27:09,050 --> 00:27:14,130
thank you for your talk I was wondering

00:27:11,820 --> 00:27:16,140
if I could ask a question about one of

00:27:14,130 --> 00:27:17,850
the basic assumptions of your model you

00:27:16,140 --> 00:27:19,850
said that yeah I use the functional

00:27:17,850 --> 00:27:22,740
model how does that take into account

00:27:19,850 --> 00:27:25,350
people who lost part of their brain and

00:27:22,740 --> 00:27:26,900
have had other parts kind of compensate

00:27:25,350 --> 00:27:29,940
for that does not imply that brain

00:27:26,900 --> 00:27:34,470
location is kind of agnostic to the

00:27:29,940 --> 00:27:36,480
function oh yeah so I should say that so

00:27:34,470 --> 00:27:38,520
remember those models we were making

00:27:36,480 --> 00:27:41,100
kind of discrete ensembles of neurons

00:27:38,520 --> 00:27:42,780
right so you know when we think about

00:27:41,100 --> 00:27:45,240
when we build an eggo model like this it

00:27:42,780 --> 00:27:46,800
contains you know maybe you know 100 or

00:27:45,240 --> 00:27:48,720
200 ensembles that are connected

00:27:46,800 --> 00:27:50,010
together right

00:27:48,720 --> 00:27:52,110
those ensembles you would kind of

00:27:50,010 --> 00:27:54,450
hypothesize to be located in similar

00:27:52,110 --> 00:27:56,309
parts of the brain so in spawn you know

00:27:54,450 --> 00:27:58,020
we have a thousand or so maybe thousands

00:27:56,309 --> 00:28:00,300
of ensembles you can kind of say that

00:27:58,020 --> 00:28:03,780
each one of these ensembles is kind of

00:28:00,300 --> 00:28:05,910
part of a functionally relevant sub

00:28:03,780 --> 00:28:07,590
Network and that sub Network you can

00:28:05,910 --> 00:28:10,050
kind of hypothesize to be in a

00:28:07,590 --> 00:28:10,350
particular region of the brain so the

00:28:10,050 --> 00:28:12,600
part

00:28:10,350 --> 00:28:14,519
to spawn that are dedicated to visual

00:28:12,600 --> 00:28:17,279
processing we would think of those to be

00:28:14,519 --> 00:28:19,080
in like V 1 and V 2 and you can do

00:28:17,279 --> 00:28:20,669
similar things it's not as detailed as a

00:28:19,080 --> 00:28:22,259
neuron where you actually say you know

00:28:20,669 --> 00:28:23,700
this is the spatial properties of the

00:28:22,259 --> 00:28:26,639
cell but you can still kind of make

00:28:23,700 --> 00:28:28,230
analogies to you know make hypotheses

00:28:26,639 --> 00:28:30,240
about what this ensemble where this

00:28:28,230 --> 00:28:32,460
ensemble might be in the brain well

00:28:30,240 --> 00:28:39,330
thank you if we get the back of the room

00:28:32,460 --> 00:28:42,059
for a question oh oh sorry go ahead

00:28:39,330 --> 00:28:44,460
you mentioned sci-fi so in your

00:28:42,059 --> 00:28:48,710
professional opinion what is the best

00:28:44,460 --> 00:28:52,379
most interest in AI in science fiction

00:28:48,710 --> 00:28:54,179
um that's a good question I actually

00:28:52,379 --> 00:28:56,370
watched terminator specifically before

00:28:54,179 --> 00:28:57,539
this talk because you know it's happened

00:28:56,370 --> 00:28:58,980
a few times that people have come up to

00:28:57,539 --> 00:29:00,720
me and be like so you're building Skynet

00:28:58,980 --> 00:29:02,539
and I'm like I don't think so

00:29:00,720 --> 00:29:04,860
but I should watch the movie to find out

00:29:02,539 --> 00:29:05,970
you know the problem with all these iai

00:29:04,860 --> 00:29:08,460
is the science fiction is that they're

00:29:05,970 --> 00:29:11,129
kind of under under explains i just want

00:29:08,460 --> 00:29:14,940
to know more about the details but you

00:29:11,129 --> 00:29:17,519
know and my question yeah great but it

00:29:14,940 --> 00:29:20,600
was for the room does this have general

00:29:17,519 --> 00:29:22,769
application to like machine learning or

00:29:20,600 --> 00:29:24,750
as someone in the audience not trying to

00:29:22,769 --> 00:29:26,759
emulate a brain is there some way I

00:29:24,750 --> 00:29:29,460
could use this in a more general

00:29:26,759 --> 00:29:31,259
application yeah so we're really hoping

00:29:29,460 --> 00:29:34,230
that that will be the case but like I

00:29:31,259 --> 00:29:36,990
said you know we've really been focusing

00:29:34,230 --> 00:29:38,460
on emulating neural biology for now we

00:29:36,990 --> 00:29:41,789
think that there are very important ways

00:29:38,460 --> 00:29:43,679
that this can be applied in industry the

00:29:41,789 --> 00:29:45,419
really the really kind of killer feature

00:29:43,679 --> 00:29:47,250
of spiking neural networks is that

00:29:45,419 --> 00:29:48,870
they're incredibly efficient so I

00:29:47,250 --> 00:29:51,509
mentioned that spinnaker a neural

00:29:48,870 --> 00:29:52,980
morphic hardware it can run on you know

00:29:51,509 --> 00:29:55,080
three orders of magnitude less power

00:29:52,980 --> 00:29:56,850
than typical kind of conventional

00:29:55,080 --> 00:29:58,679
computing the problem is that they don't

00:29:56,850 --> 00:30:00,090
know how to program their chips because

00:29:58,679 --> 00:30:02,220
they're just neural networks they're

00:30:00,090 --> 00:30:04,830
they're you know simulating

00:30:02,220 --> 00:30:05,850
neurobiological systems so we're hoping

00:30:04,830 --> 00:30:07,289
that nego is going to be a way to

00:30:05,850 --> 00:30:09,509
program those types of chips to do

00:30:07,289 --> 00:30:12,059
interesting kind of machine learning

00:30:09,509 --> 00:30:14,639
type functions Thanks

00:30:12,059 --> 00:30:16,440
let's do a question from the back Thanks

00:30:14,639 --> 00:30:19,169
great talk thank you very interesting

00:30:16,440 --> 00:30:20,160
thanks one question I had is it seems

00:30:19,169 --> 00:30:22,260
like you know for

00:30:20,160 --> 00:30:24,600
most part for the small systems you sort

00:30:22,260 --> 00:30:27,830
of get back you put in which validates

00:30:24,600 --> 00:30:32,220
the code but in terms of understanding

00:30:27,830 --> 00:30:34,380
or producing novel things how how

00:30:32,220 --> 00:30:36,360
complicated how big do the systems need

00:30:34,380 --> 00:30:39,210
to get and do you have to modify the

00:30:36,360 --> 00:30:41,880
code in other ways before you start to

00:30:39,210 --> 00:30:43,680
get sort of really meaningful functional

00:30:41,880 --> 00:30:45,630
behavior that you can ascribe to

00:30:43,680 --> 00:30:48,480
something similar that to what a brain

00:30:45,630 --> 00:30:50,190
would do so I can tell you that we don't

00:30:48,480 --> 00:30:52,800
you don't need to modify the code at all

00:30:50,190 --> 00:30:54,630
so that's been really a big push for

00:30:52,800 --> 00:30:55,710
this new version of mango is that in the

00:30:54,630 --> 00:30:58,170
old version you know we had to make a

00:30:55,710 --> 00:31:00,000
lot of modifications to get spawn

00:30:58,170 --> 00:31:02,460
running but now you can you can

00:31:00,000 --> 00:31:06,780
basically take nego too as it is and you

00:31:02,460 --> 00:31:09,690
can run spawn as far as how to build

00:31:06,780 --> 00:31:12,000
these models it is really an art it

00:31:09,690 --> 00:31:15,360
takes a long time to kind of think about

00:31:12,000 --> 00:31:18,060
how you take basically time varying

00:31:15,360 --> 00:31:20,640
vectors and functions on those vectors

00:31:18,060 --> 00:31:25,350
and with that kind of compose it all

00:31:20,640 --> 00:31:27,210
into a brain like system and if you're

00:31:25,350 --> 00:31:32,030
interested you're definitely definitely

00:31:27,210 --> 00:31:35,340
come more to my lab but if you go to our

00:31:32,030 --> 00:31:37,290
github page for nangou in the

00:31:35,340 --> 00:31:39,360
documentation we have a long set of

00:31:37,290 --> 00:31:40,830
examples that kind of go from a simple

00:31:39,360 --> 00:31:42,780
network to a very complicated network

00:31:40,830 --> 00:31:43,980
and if you look through those through

00:31:42,780 --> 00:31:46,950
those examples you should hopefully get

00:31:43,980 --> 00:31:49,590
a sense of kind of what that art is like

00:31:46,950 --> 00:31:50,610
this one in case like a two more

00:31:49,590 --> 00:31:53,970
questions from the front of the room

00:31:50,610 --> 00:31:56,010
alright thanks for your talk your your

00:31:53,970 --> 00:31:58,200
system you did a good job of

00:31:56,010 --> 00:32:00,780
demonstrating modeling at the neural

00:31:58,200 --> 00:32:03,360
level and the functional level does it

00:32:00,780 --> 00:32:05,160
have any provisions for modeling at some

00:32:03,360 --> 00:32:07,920
of the intermediate like the cortical

00:32:05,160 --> 00:32:13,560
column level or something other circuits

00:32:07,920 --> 00:32:15,210
right so uh yeah so I would say that

00:32:13,560 --> 00:32:16,710
actually I would say that we're right

00:32:15,210 --> 00:32:18,900
now kind of at that intermediate stage

00:32:16,710 --> 00:32:20,220
because we've kind of focused on you

00:32:18,900 --> 00:32:23,180
know what are the functionally relevant

00:32:20,220 --> 00:32:25,440
subsystems and then make a very reduced

00:32:23,180 --> 00:32:27,630
model of that and then connect that

00:32:25,440 --> 00:32:29,940
together so spawn only has two and a

00:32:27,630 --> 00:32:31,879
half million simulated neurons which is

00:32:29,940 --> 00:32:34,549
you know less than

00:32:31,879 --> 00:32:36,379
the percentage of the real brain so

00:32:34,549 --> 00:32:38,479
we've just kind of chosen little modules

00:32:36,379 --> 00:32:40,279
and so if you wanted a more

00:32:38,479 --> 00:32:41,539
sophisticated visual system you kind of

00:32:40,279 --> 00:32:43,629
take the visual system that's in there

00:32:41,539 --> 00:32:45,799
and just scale it up kind of in a

00:32:43,629 --> 00:32:47,749
obvious fashion of just increase the

00:32:45,799 --> 00:32:49,940
dimensionality be more sensitive to more

00:32:47,749 --> 00:32:52,309
pixels all that kind of stuff so what we

00:32:49,940 --> 00:32:53,989
have now and this abstraction that we

00:32:52,309 --> 00:32:56,059
call a network is essentially that

00:32:53,989 --> 00:33:00,109
middle ground where it's not an ensemble

00:32:56,059 --> 00:33:02,539
it's a group of ensembles and that group

00:33:00,109 --> 00:33:04,099
of ensembles together compute kind of an

00:33:02,539 --> 00:33:06,039
interesting function more interesting

00:33:04,099 --> 00:33:12,319
than just something that is in one layer

00:33:06,039 --> 00:33:14,149
thank you hi I've been told that the

00:33:12,319 --> 00:33:17,239
power of the brain was the capacity of

00:33:14,149 --> 00:33:21,489
reconnection of neurons and so I'm just

00:33:17,239 --> 00:33:24,349
wondering I've I've I've been lied to or

00:33:21,489 --> 00:33:28,940
and the second question is why don't you

00:33:24,349 --> 00:33:32,869
spawn a random brain and just as for

00:33:28,940 --> 00:33:33,889
connections I'm sorry can you expand a

00:33:32,869 --> 00:33:36,229
little bit about what you mean by the

00:33:33,889 --> 00:33:38,809
power of the brain is so I thought that

00:33:36,229 --> 00:33:40,940
neurons had the power of disconnecting

00:33:38,809 --> 00:33:42,319
and reconnecting with each other oh yeah

00:33:40,940 --> 00:33:43,789
so one thing that I haven't talked about

00:33:42,319 --> 00:33:48,109
at all in this presentation is learning

00:33:43,789 --> 00:33:49,579
so learning is really you know how our

00:33:48,109 --> 00:33:51,349
brain works how it goes from you know

00:33:49,579 --> 00:33:53,869
developments all the way to a fully

00:33:51,349 --> 00:33:56,389
functioning adult so we do have you know

00:33:53,869 --> 00:33:57,829
the capacity to learn in nangou what I

00:33:56,389 --> 00:33:59,929
presented to you today is all based on

00:33:57,829 --> 00:34:01,249
kind of optimization methods and what it

00:33:59,929 --> 00:34:03,289
gives you is essentially kind of a

00:34:01,249 --> 00:34:04,849
picture of the end result of learning so

00:34:03,289 --> 00:34:09,109
there's a lot of work to do in figuring

00:34:04,849 --> 00:34:11,509
out ways to get from kind of just a blob

00:34:09,109 --> 00:34:13,099
of neurons at to figure out error

00:34:11,509 --> 00:34:15,139
signals that's a you can such you can

00:34:13,099 --> 00:34:16,879
kind of as make these really complicated

00:34:15,139 --> 00:34:18,919
networks that's kind of a separate line

00:34:16,879 --> 00:34:21,409
of research I guess my question is more

00:34:18,919 --> 00:34:23,740
why can't you just Pamela random brain

00:34:21,409 --> 00:34:25,609
and make you learn from scratch

00:34:23,740 --> 00:34:27,710
definitely people are trying to do that

00:34:25,609 --> 00:34:29,240
okay but I can tell you that that they

00:34:27,710 --> 00:34:31,669
haven't come up with anything as kind of

00:34:29,240 --> 00:34:33,710
functionally significant as what we've

00:34:31,669 --> 00:34:37,179
done with Ning go all right thank you

00:34:33,710 --> 00:34:37,179
very much everyone and thank you Trevor

00:34:40,140 --> 00:34:45,370
just Romana this is the last talks of

00:34:42,760 --> 00:34:46,870
the day if you're going to one of the

00:34:45,370 --> 00:34:49,720
awesome PyCon dinners you're probably

00:34:46,870 --> 00:34:52,380
have to book it on over there have a

00:34:49,720 --> 00:34:52,380

YouTube URL: https://www.youtube.com/watch?v=7hvpoLKJHOw


