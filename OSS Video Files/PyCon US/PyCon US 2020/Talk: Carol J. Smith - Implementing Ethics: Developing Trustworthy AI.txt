Title: Talk: Carol J. Smith - Implementing Ethics: Developing Trustworthy AI
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Carol J. Smith

Ethics discussions abound, but translating “do no harm” into our work is frustrating at best, and obfuscatory at worst. We can agree that keeping humans safe and in control is important, but implementing ethics is intimidating work.

Learn how to wield your preferred technology ethics code to make an AI system that is accountable, de-risked, respectful, secure, honest and usable. The presenter will introduce the topic of ethics and then step through a user experience (UX) framework to guide AI development teams successfully through this process.
Captions: 
	00:00:04,560 --> 00:00:09,120
I'm Carol Smith and welcome to my

00:00:06,870 --> 00:00:12,959
virtual Python session on implementing

00:00:09,120 --> 00:00:16,110
epics developing trustworthy AI some

00:00:12,959 --> 00:00:17,940
legalese and let's get into it so your

00:00:16,110 --> 00:00:19,500
goal is to create AI systems that are

00:00:17,940 --> 00:00:21,030
safe and trustworthy and today I'm going

00:00:19,500 --> 00:00:23,250
to talk about some tools that can help

00:00:21,030 --> 00:00:25,710
you to get to that goal and to be

00:00:23,250 --> 00:00:27,420
successful in your work this is early

00:00:25,710 --> 00:00:29,010
purposeful work so it's not just about

00:00:27,420 --> 00:00:30,420
the algorithms I'm also going to be

00:00:29,010 --> 00:00:32,309
talking about the conduct the

00:00:30,420 --> 00:00:34,290
interactions the intense the work that

00:00:32,309 --> 00:00:36,059
you want to imbue into your system the

00:00:34,290 --> 00:00:38,760
values that you want to bring into that

00:00:36,059 --> 00:00:41,820
system and how to get to that that

00:00:38,760 --> 00:00:43,980
successful point the first step is to

00:00:41,820 --> 00:00:46,289
have a diverse team and what I mean by

00:00:43,980 --> 00:00:48,960
diversity is diverse with regard to race

00:00:46,289 --> 00:00:51,600
gender culture of course but also with

00:00:48,960 --> 00:00:53,159
regard to education the schools of

00:00:51,600 --> 00:00:55,350
programs that your team has attended

00:00:53,159 --> 00:00:57,329
their thinking processes their

00:00:55,350 --> 00:00:59,159
disability staffs and many many more

00:00:57,329 --> 00:00:59,760
aspects that make them different from

00:00:59,159 --> 00:01:03,030
each other

00:00:59,760 --> 00:01:05,880
these diversity aspects are what can

00:01:03,030 --> 00:01:07,470
make a team stronger and this is not

00:01:05,880 --> 00:01:10,530
about lowering the bar we're talking

00:01:07,470 --> 00:01:12,659
about extending it actually making space

00:01:10,530 --> 00:01:15,420
for this diversity and making an

00:01:12,659 --> 00:01:16,890
inclusive environment and these diverse

00:01:15,420 --> 00:01:19,710
individuals of course need to be

00:01:16,890 --> 00:01:21,689
talented but also multidisciplinary so

00:01:19,710 --> 00:01:23,159
the team needs to include a variety of

00:01:21,689 --> 00:01:25,439
skill sets and problem framing

00:01:23,159 --> 00:01:27,479
approaches to be successful and this is

00:01:25,439 --> 00:01:29,039
context dependent so in some teams you

00:01:27,479 --> 00:01:30,930
may need more data scientists and

00:01:29,039 --> 00:01:32,460
programmers and others projects you may

00:01:30,930 --> 00:01:34,649
need more machine learning experts

00:01:32,460 --> 00:01:37,950
that's going to vary an additional

00:01:34,649 --> 00:01:39,750
aspect is having curiosity experts these

00:01:37,950 --> 00:01:42,060
include people like myself people who

00:01:39,750 --> 00:01:45,000
call themselves UX researchers UX

00:01:42,060 --> 00:01:48,329
designers and our action specialists

00:01:45,000 --> 00:01:50,159
they might be digital anthropologists

00:01:48,329 --> 00:01:51,869
these are the types of people who are

00:01:50,159 --> 00:01:54,240
going to really focus on understanding

00:01:51,869 --> 00:01:56,280
the situation the abilities of the

00:01:54,240 --> 00:01:57,899
people who will use the system and how

00:01:56,280 --> 00:01:59,609
the system is going to be used and

00:01:57,899 --> 00:02:01,590
they'll help you to answer the questions

00:01:59,609 --> 00:02:03,659
that you might have about how and what

00:02:01,590 --> 00:02:05,789
and why people are doing various things

00:02:03,659 --> 00:02:08,520
with the system and they'll help to

00:02:05,789 --> 00:02:11,190
activate curiosity in everyone on the

00:02:08,520 --> 00:02:13,440
team by conducting UX activities these

00:02:11,190 --> 00:02:15,750
also should include social scientists

00:02:13,440 --> 00:02:17,370
ethicists and many many more roles but

00:02:15,750 --> 00:02:18,210
at the same time keeping those teams

00:02:17,370 --> 00:02:20,760
small

00:02:18,210 --> 00:02:22,680
is not an easy problem to solve but it's

00:02:20,760 --> 00:02:24,570
important to figure this out for your

00:02:22,680 --> 00:02:26,940
for the quality of the product that

00:02:24,570 --> 00:02:29,010
you're building there's high value and

00:02:26,940 --> 00:02:30,810
diverse teams as I mentioned and there

00:02:29,010 --> 00:02:32,970
are many reasons for this the ones

00:02:30,810 --> 00:02:34,860
mentioned in Harvard Business Review are

00:02:32,970 --> 00:02:36,870
that they focus more on facts they

00:02:34,860 --> 00:02:39,210
process facts more carefully and that

00:02:36,870 --> 00:02:41,130
they become more innovative because of

00:02:39,210 --> 00:02:42,690
their bridging bringing together of so

00:02:41,130 --> 00:02:46,350
many different ways of thinking and

00:02:42,690 --> 00:02:48,960
because they're all individuals and they

00:02:46,350 --> 00:02:50,610
are minorities on their own they are

00:02:48,960 --> 00:02:53,160
likely to become more aware of their own

00:02:50,610 --> 00:02:55,350
potential biases and actually try to be

00:02:53,160 --> 00:02:58,380
more careful when they're processing

00:02:55,350 --> 00:03:00,360
those facts and so this diverse team

00:02:58,380 --> 00:03:01,890
coming together because they're all so

00:03:00,360 --> 00:03:04,890
different are going to think about

00:03:01,890 --> 00:03:07,410
things in ways that a team that is very

00:03:04,890 --> 00:03:10,830
similar may not they'll also be more

00:03:07,410 --> 00:03:12,840
likely to notice things that a team that

00:03:10,830 --> 00:03:16,710
has all had very similar education or

00:03:12,840 --> 00:03:18,420
similar experiences may not notice so

00:03:16,710 --> 00:03:20,790
great minds think different and this is

00:03:18,420 --> 00:03:22,530
an important value that we want to move

00:03:20,790 --> 00:03:26,490
forward with as we're developing AI

00:03:22,530 --> 00:03:28,260
systems there are some requirements to

00:03:26,490 --> 00:03:30,210
having an inclusive workplace and that

00:03:28,260 --> 00:03:32,430
includes representative diverse

00:03:30,210 --> 00:03:33,960
leadership so without this you're not

00:03:32,430 --> 00:03:36,720
going to have a good retention so you

00:03:33,960 --> 00:03:38,700
may notice that a lot of workplaces you

00:03:36,720 --> 00:03:40,380
may have been and there will be an

00:03:38,700 --> 00:03:42,030
individual hired who's very different

00:03:40,380 --> 00:03:44,100
from everyone else but they don't stay

00:03:42,030 --> 00:03:45,810
and that is because they don't see that

00:03:44,100 --> 00:03:48,000
leadership there that's going to support

00:03:45,810 --> 00:03:50,460
them and who looks like them or who at

00:03:48,000 --> 00:03:51,960
least is different like they are an

00:03:50,460 --> 00:03:54,540
individual differences need to be

00:03:51,960 --> 00:03:56,250
acknowledged and accepted so allowing us

00:03:54,540 --> 00:03:57,660
to bring our whole selves to work now

00:03:56,250 --> 00:04:00,030
that we're all working from home at

00:03:57,660 --> 00:04:01,950
least I hope you are this actually is

00:04:00,030 --> 00:04:05,280
helping to show the diversity in our

00:04:01,950 --> 00:04:07,230
lives people are bringing their pets and

00:04:05,280 --> 00:04:09,720
babies and whoever else is in their

00:04:07,230 --> 00:04:11,310
lives into our screens and this is

00:04:09,720 --> 00:04:13,560
helping to make a more welcoming

00:04:11,310 --> 00:04:15,660
community environment we want people to

00:04:13,560 --> 00:04:17,340
feel valued and connected and that they

00:04:15,660 --> 00:04:20,790
belong and this all will help to make a

00:04:17,340 --> 00:04:22,950
better system the next step is to adopt

00:04:20,790 --> 00:04:24,930
technology ethics and there are many

00:04:22,950 --> 00:04:27,000
many different versions of ethics that

00:04:24,930 --> 00:04:29,340
you can look at some of them are created

00:04:27,000 --> 00:04:31,889
by organizations such as the ACM

00:04:29,340 --> 00:04:33,750
Association for Computing Machinery so

00:04:31,889 --> 00:04:36,379
come from corporate organizations

00:04:33,750 --> 00:04:38,099
Microsoft Google some come from

00:04:36,379 --> 00:04:40,319
organizations such as a Montreal

00:04:38,099 --> 00:04:42,629
declaration for Responsible AI out of

00:04:40,319 --> 00:04:44,819
the University and even the US

00:04:42,629 --> 00:04:47,879
Department of Defense has a set of AI

00:04:44,819 --> 00:04:49,349
ethics and these help you to harmonize

00:04:47,879 --> 00:04:50,909
cultural variation so when you're

00:04:49,349 --> 00:04:53,490
bringing together that very diverse team

00:04:50,909 --> 00:04:55,650
you need something for them all too tied

00:04:53,490 --> 00:04:56,879
together to bridge the changes or the

00:04:55,650 --> 00:05:00,000
differences in their ways of thinking

00:04:56,879 --> 00:05:02,219
together and this helps you to do that

00:05:00,000 --> 00:05:04,229
it also can help you to balance the pace

00:05:02,219 --> 00:05:06,509
of change so the industry pressure may

00:05:04,229 --> 00:05:08,490
push you to move faster and faster but

00:05:06,509 --> 00:05:10,469
technology ethics can help you tie to

00:05:08,490 --> 00:05:12,210
what's really important to help you know

00:05:10,469 --> 00:05:15,360
what your team is trying to do and to

00:05:12,210 --> 00:05:17,180
keep your eye on the goal it also gives

00:05:15,360 --> 00:05:19,529
all of the individuals in your team's

00:05:17,180 --> 00:05:22,319
explicit permission to consider and

00:05:19,529 --> 00:05:24,870
question the breadth of the implications

00:05:22,319 --> 00:05:26,789
of your systems so it really helps them

00:05:24,870 --> 00:05:28,889
to feel that they have a role in

00:05:26,789 --> 00:05:30,479
questioning what is happening and why

00:05:28,889 --> 00:05:32,639
we're making the decisions we're making

00:05:30,479 --> 00:05:34,529
and that's the kind of thing that you

00:05:32,639 --> 00:05:37,289
want to value and you want to encourage

00:05:34,529 --> 00:05:39,810
and your team's is being comfortable

00:05:37,289 --> 00:05:41,789
enough to bring up concerns early so

00:05:39,810 --> 00:05:44,849
that you can address them and mitigate

00:05:41,789 --> 00:05:46,469
and/or prevent them so as this team

00:05:44,849 --> 00:05:48,270
coalescence on the shared said

00:05:46,469 --> 00:05:50,460
technology ethics and the one I actually

00:05:48,270 --> 00:05:54,210
recommend as a Montreal declaration of

00:05:50,460 --> 00:05:58,139
Responsible AI this has a nice long list

00:05:54,210 --> 00:05:59,669
of broad thoughts and ideas and topics

00:05:58,139 --> 00:06:03,270
that are covered and so it should cover

00:05:59,669 --> 00:06:06,330
almost any technical situation that you

00:06:03,270 --> 00:06:07,919
have in your organization and this is

00:06:06,330 --> 00:06:10,319
good because you can start with this set

00:06:07,919 --> 00:06:11,819
and then as you determine how you're

00:06:10,319 --> 00:06:13,529
using it and what's working and what's

00:06:11,819 --> 00:06:15,779
not you can begin to customize

00:06:13,529 --> 00:06:17,430
potentially your set of technology

00:06:15,779 --> 00:06:21,029
ethics source works best with your

00:06:17,430 --> 00:06:22,740
organization and the coalescing around

00:06:21,029 --> 00:06:24,419
those tech ethics is just part of

00:06:22,740 --> 00:06:26,779
building this team so you've got diverse

00:06:24,419 --> 00:06:29,339
inclusive leaders you have a diverse

00:06:26,779 --> 00:06:30,990
multidisciplinary team and you have that

00:06:29,339 --> 00:06:32,819
chaired said technology ethics that

00:06:30,990 --> 00:06:35,099
everyone can come together around and

00:06:32,819 --> 00:06:38,370
use that to help to guide them into

00:06:35,099 --> 00:06:40,500
making these great AI systems so to

00:06:38,370 --> 00:06:43,800
actually do this work you need a

00:06:40,500 --> 00:06:45,270
framework to tie everything together and

00:06:43,800 --> 00:06:45,570
that's what I'm introducing today it's

00:06:45,270 --> 00:06:47,940
you

00:06:45,570 --> 00:06:50,070
framework for design trustworthy AI and

00:06:47,940 --> 00:06:52,590
this helps you get to where you're going

00:06:50,070 --> 00:06:55,590
to making testable ethically I

00:06:52,590 --> 00:06:57,930
so using that set of technology ethics

00:06:55,590 --> 00:06:59,460
and and tying it together with this

00:06:57,930 --> 00:07:03,030
framework we'll get to that trustable

00:06:59,460 --> 00:07:05,310
ethical AI this involves a lot of

00:07:03,030 --> 00:07:07,650
conversations to help people understand

00:07:05,310 --> 00:07:10,440
each other and what the purposes of the

00:07:07,650 --> 00:07:13,620
system so this UX framework can help you

00:07:10,440 --> 00:07:15,570
by phrasing difficult topics framing

00:07:13,620 --> 00:07:16,530
this topics so that you can talk about

00:07:15,570 --> 00:07:19,320
what you value

00:07:16,530 --> 00:07:21,690
who could be hurt by the system what

00:07:19,320 --> 00:07:24,480
lines won't our AI cross how are we

00:07:21,690 --> 00:07:26,040
shifting power and how we track our

00:07:24,480 --> 00:07:27,930
progress these are really important

00:07:26,040 --> 00:07:30,330
questions to think about as early as

00:07:27,930 --> 00:07:33,000
possible and there are many many many

00:07:30,330 --> 00:07:34,770
more and by having a framework to have

00:07:33,000 --> 00:07:36,860
these conversations these conversations

00:07:34,770 --> 00:07:39,570
will happen if you encourage them to

00:07:36,860 --> 00:07:41,670
this is probably very new and

00:07:39,570 --> 00:07:43,950
uncomfortable work for many of you and

00:07:41,670 --> 00:07:46,800
unfortunately this work is uncomfortable

00:07:43,950 --> 00:07:49,470
for almost everyone ethical design is

00:07:46,800 --> 00:07:52,440
not superficial Laura Quebec talks about

00:07:49,470 --> 00:07:55,050
this in her talks and this work can be

00:07:52,440 --> 00:07:56,640
very uncomfortable but it's important

00:07:55,050 --> 00:07:58,710
work and it's important to be able to

00:07:56,640 --> 00:08:01,010
protect the people that were supposed to

00:07:58,710 --> 00:08:03,690
be helping and so we have to do it

00:08:01,010 --> 00:08:05,430
you can prompt those conversations with

00:08:03,690 --> 00:08:07,890
a checklist so this is a checklist I've

00:08:05,430 --> 00:08:10,350
worked on there's a QR code there that

00:08:07,890 --> 00:08:13,650
you can scan to get to the download and

00:08:10,350 --> 00:08:16,320
what you do is you pair a checklist with

00:08:13,650 --> 00:08:18,450
your technical ethics so you have that

00:08:16,320 --> 00:08:20,130
set of ethics and you need to bridge the

00:08:18,450 --> 00:08:22,590
gap between statements that you might

00:08:20,130 --> 00:08:24,030
find in it such as do no harm and what

00:08:22,590 --> 00:08:26,160
you're actually going to be doing with

00:08:24,030 --> 00:08:28,170
your system and this will help you to

00:08:26,160 --> 00:08:30,780
reduce the risk and unwanted bias in the

00:08:28,170 --> 00:08:33,120
system to do mitigation planning and

00:08:30,780 --> 00:08:34,950
also to support the inspection so that

00:08:33,120 --> 00:08:37,590
you know what you're inspecting and why

00:08:34,950 --> 00:08:40,950
and what you need to do yet to make your

00:08:37,590 --> 00:08:43,470
system as ethical as possible so the

00:08:40,950 --> 00:08:45,390
prompts that you'll find in checklist

00:08:43,470 --> 00:08:47,730
will help reveal hidden tasks these are

00:08:45,390 --> 00:08:49,800
some examples in the checklist proposing

00:08:47,730 --> 00:08:52,050
and for example we worked as

00:08:49,800 --> 00:08:54,450
speculatively identify the full range of

00:08:52,050 --> 00:08:57,240
risks and benefits so if you haven't

00:08:54,450 --> 00:08:58,400
done these various items that means that

00:08:57,240 --> 00:09:01,880
there may be some

00:08:58,400 --> 00:09:04,310
tasks that you need to identify and put

00:09:01,880 --> 00:09:06,140
into your backlog and actually be a

00:09:04,310 --> 00:09:08,600
responsibility to someone on your team

00:09:06,140 --> 00:09:10,520
to address it may mean that there are

00:09:08,600 --> 00:09:11,780
many many tasks so you need to do it may

00:09:10,520 --> 00:09:14,540
be that you just need to do one more

00:09:11,780 --> 00:09:16,790
thing to make sure everything is as you

00:09:14,540 --> 00:09:18,530
want it to be and so this will help you

00:09:16,790 --> 00:09:20,210
to determine if you've done the right

00:09:18,530 --> 00:09:24,290
work and if there's still work left to

00:09:20,210 --> 00:09:26,840
do the UX framework itself has four main

00:09:24,290 --> 00:09:29,660
aspects and these four are accountable

00:09:26,840 --> 00:09:32,090
to humans cognizant of speculative risks

00:09:29,660 --> 00:09:34,010
and benefits respectful and secure and

00:09:32,090 --> 00:09:35,870
honest and usable and I'll go through

00:09:34,010 --> 00:09:38,180
each of these and I'm going to use a

00:09:35,870 --> 00:09:40,490
scenario to help you to tie it together

00:09:38,180 --> 00:09:43,100
and this is the right staff scenario

00:09:40,490 --> 00:09:45,410
right staff is an AI ship scheduling

00:09:43,100 --> 00:09:47,570
system and the users are intended to be

00:09:45,410 --> 00:09:49,910
store managers of fast-food restaurants

00:09:47,570 --> 00:09:52,670
so the goals of right staff are to

00:09:49,910 --> 00:09:55,310
improve the decision-making for staffing

00:09:52,670 --> 00:09:57,470
and to improve scheduling and it's also

00:09:55,310 --> 00:10:00,170
to reduce the bias of ship scheduling

00:09:57,470 --> 00:10:02,330
and shift shift selection frequently

00:10:00,170 --> 00:10:04,190
what happens within restaurants is that

00:10:02,330 --> 00:10:06,620
the Friends of the manager get the

00:10:04,190 --> 00:10:08,660
better shifts and more shifts and other

00:10:06,620 --> 00:10:12,110
people don't and so there's a lot of

00:10:08,660 --> 00:10:14,330
bias there and unfairness and so this

00:10:12,110 --> 00:10:17,930
system is being made in order to reduce

00:10:14,330 --> 00:10:19,640
those aspects of unfairness so

00:10:17,930 --> 00:10:22,280
accountable to humans this is the first

00:10:19,640 --> 00:10:24,140
aspect of the of the framework and this

00:10:22,280 --> 00:10:26,150
it talks about ensuring humans have

00:10:24,140 --> 00:10:27,980
ultimate control that we are able to

00:10:26,150 --> 00:10:30,620
monitor and control risks throughout the

00:10:27,980 --> 00:10:32,960
entire system and throughout its life

00:10:30,620 --> 00:10:34,760
its life time and that humans are

00:10:32,960 --> 00:10:36,770
responsible for final decisions

00:10:34,760 --> 00:10:38,000
regarding a person's life their quality

00:10:36,770 --> 00:10:41,960
of life their health and their

00:10:38,000 --> 00:10:43,610
reputation this is really about making

00:10:41,960 --> 00:10:45,650
sure that humans can unplug the machines

00:10:43,610 --> 00:10:48,230
Grady Booch says this in his TED talk

00:10:45,650 --> 00:10:49,370
and this is the core of the majority of

00:10:48,230 --> 00:10:51,200
the work that I'm going to be talking

00:10:49,370 --> 00:10:53,210
about today we want to make sure that

00:10:51,200 --> 00:10:54,920
humans are always in control and that

00:10:53,210 --> 00:10:57,020
humans are in the loop throughout the

00:10:54,920 --> 00:10:58,940
entire lifecycle of the system this is

00:10:57,020 --> 00:10:59,570
never a situation where you set it and

00:10:58,940 --> 00:11:01,640
forget it

00:10:59,570 --> 00:11:04,460
but rather humans are consistently and

00:11:01,640 --> 00:11:08,060
constantly monitoring and managing these

00:11:04,460 --> 00:11:10,970
systems significant decisions that the

00:11:08,060 --> 00:11:12,240
system might make need to be explained

00:11:10,970 --> 00:11:13,830
they need to be able

00:11:12,240 --> 00:11:16,320
be overridden and they need to be

00:11:13,830 --> 00:11:18,540
appealable and reversible by humans and

00:11:16,320 --> 00:11:21,209
so for right staff when we think about

00:11:18,540 --> 00:11:23,040
that scenario the manager should be able

00:11:21,209 --> 00:11:25,320
to reschedule people as needed

00:11:23,040 --> 00:11:28,020
so the AI system may do that initial

00:11:25,320 --> 00:11:29,430
scheduling of the staff but the manager

00:11:28,020 --> 00:11:31,680
needs to be able to come in and make

00:11:29,430 --> 00:11:34,260
changes as needed so that's one example

00:11:31,680 --> 00:11:37,589
of how this framework might affect that

00:11:34,260 --> 00:11:39,690
particular system the responsibilities

00:11:37,589 --> 00:11:42,690
need to be explicitly defined between

00:11:39,690 --> 00:11:45,029
the AI system and the humans and so with

00:11:42,690 --> 00:11:47,640
right staff there's the AI system right

00:11:45,029 --> 00:11:49,230
staff and they're the managers and so we

00:11:47,640 --> 00:11:51,450
need to determine who is going to be

00:11:49,230 --> 00:11:53,520
able to pick employees to schedule how

00:11:51,450 --> 00:11:55,920
do we define shifts and who does that

00:11:53,520 --> 00:11:58,080
work what's the method to integrate new

00:11:55,920 --> 00:12:00,480
information so for example if some right

00:11:58,080 --> 00:12:03,300
staff employees are out sick with kovat

00:12:00,480 --> 00:12:05,880
unfortunately how do we manage that

00:12:03,300 --> 00:12:08,459
system what do we do about that

00:12:05,880 --> 00:12:10,589
situation and is it the AI system of the

00:12:08,459 --> 00:12:12,839
manager that actually handles that and

00:12:10,589 --> 00:12:15,660
how about with resignations how do we

00:12:12,839 --> 00:12:19,079
change the shifts if necessary how do we

00:12:15,660 --> 00:12:21,180
manage that individual's absence and is

00:12:19,079 --> 00:12:24,810
it the system or the manager that's

00:12:21,180 --> 00:12:26,459
doing that work another example is what

00:12:24,810 --> 00:12:28,290
we want right staff to be able to turn

00:12:26,459 --> 00:12:30,420
itself off if it noticed that there was

00:12:28,290 --> 00:12:32,970
something wrong if there was an issue in

00:12:30,420 --> 00:12:35,310
the system and if it does turn itself

00:12:32,970 --> 00:12:37,440
off what are the implications how do we

00:12:35,310 --> 00:12:39,660
communicate that to staff if necessary

00:12:37,440 --> 00:12:41,850
what does that do to their schedules and

00:12:39,660 --> 00:12:43,709
what happens when it's turned back on

00:12:41,850 --> 00:12:45,300
and how does that affect their schedules

00:12:43,709 --> 00:12:47,250
so thinking through these implications

00:12:45,300 --> 00:12:52,800
early will help us to build a system

00:12:47,250 --> 00:12:56,490
that is robust and able to be trusted by

00:12:52,800 --> 00:12:58,770
the humans who are needing to use it the

00:12:56,490 --> 00:13:01,320
second aspect is cognizant of

00:12:58,770 --> 00:13:03,480
speculative risks and benefits so this

00:13:01,320 --> 00:13:05,550
is about identifying the full range of

00:13:03,480 --> 00:13:07,320
harmful and malicious use as well as

00:13:05,550 --> 00:13:09,930
good and beneficial use of the system

00:13:07,320 --> 00:13:12,240
and thinking about the blind spots the

00:13:09,930 --> 00:13:14,670
unwanted and unintended consequences

00:13:12,240 --> 00:13:16,589
there will be a lot of things that you

00:13:14,670 --> 00:13:18,839
don't expect but the more we can be

00:13:16,589 --> 00:13:21,000
speculative about those potential

00:13:18,839 --> 00:13:23,040
outcomes of the system the better

00:13:21,000 --> 00:13:25,520
prepare will be to deal with them

00:13:23,040 --> 00:13:28,040
and so one way to deal with this is

00:13:25,520 --> 00:13:30,710
conducting UX research and using

00:13:28,040 --> 00:13:33,440
activities to activate curiosity within

00:13:30,710 --> 00:13:35,540
the team we can be speculative about all

00:13:33,440 --> 00:13:37,640
the types of misuse and abuse but we

00:13:35,540 --> 00:13:40,220
really only need to think about the

00:13:37,640 --> 00:13:42,010
worst case scenarios and you can do this

00:13:40,220 --> 00:13:44,570
through an activity called Black Mirror

00:13:42,010 --> 00:13:48,350
episodes where if you're familiar with

00:13:44,570 --> 00:13:50,300
the TV show you can imagine what a black

00:13:48,350 --> 00:13:52,430
mirror episode would be like for your

00:13:50,300 --> 00:13:55,520
system and this allows you to identify

00:13:52,430 --> 00:13:57,530
potentially severe abuses or misuses and

00:13:55,520 --> 00:14:00,440
the consequences potentially of those

00:13:57,530 --> 00:14:03,620
and this is an image of a template that

00:14:00,440 --> 00:14:05,930
was created by the ixd a chapter in

00:14:03,620 --> 00:14:07,700
pittsburgh for a workshop and that's

00:14:05,930 --> 00:14:10,250
been conducted multiple times very

00:14:07,700 --> 00:14:13,270
successfully and I highly recommend this

00:14:10,250 --> 00:14:16,070
type of activity for your organization

00:14:13,270 --> 00:14:18,560
the potential abuse ability of right

00:14:16,070 --> 00:14:20,570
staff if you work all the goals of it

00:14:18,560 --> 00:14:22,370
it's for faster staffing decisions and

00:14:20,570 --> 00:14:24,800
scheduling and reducing that bias of

00:14:22,370 --> 00:14:26,540
shift selection so thinking about right

00:14:24,800 --> 00:14:28,880
staff and how it could potentially be

00:14:26,540 --> 00:14:30,740
abuse let's say the system began to

00:14:28,880 --> 00:14:33,500
prioritize people with easier schedules

00:14:30,740 --> 00:14:35,930
so people who had less complex in the

00:14:33,500 --> 00:14:37,490
system for scheduling and if the

00:14:35,930 --> 00:14:40,100
managers go ahead and approve those

00:14:37,490 --> 00:14:42,530
schedules that may reinforce bias that

00:14:40,100 --> 00:14:44,510
was already a problem in the past and

00:14:42,530 --> 00:14:46,340
then the people who were previously

00:14:44,510 --> 00:14:49,040
discriminated against may still be

00:14:46,340 --> 00:14:51,230
discriminated against and so instead of

00:14:49,040 --> 00:14:53,630
reducing the bias of shift selection the

00:14:51,230 --> 00:14:55,400
system may inadvertently begin to

00:14:53,630 --> 00:14:58,940
reinforce that same problem all over

00:14:55,400 --> 00:15:00,920
again and so we need to be speculative

00:14:58,940 --> 00:15:03,650
about that situation and identify it

00:15:00,920 --> 00:15:05,390
early on and then think about how we're

00:15:03,650 --> 00:15:07,040
going to create communication and

00:15:05,390 --> 00:15:09,620
mitigation plans to deal with that

00:15:07,040 --> 00:15:12,110
situation so plan for the em1 and

00:15:09,620 --> 00:15:14,660
consequences think about that as a

00:15:12,110 --> 00:15:16,310
potential problem so how do we deal with

00:15:14,660 --> 00:15:19,550
the system if it's beginning to learn

00:15:16,310 --> 00:15:22,630
the wrong things and then how do we deal

00:15:19,550 --> 00:15:25,370
with that who can report it to whom

00:15:22,630 --> 00:15:27,530
should we turn off the system if we turn

00:15:25,370 --> 00:15:29,450
it off who do we need to notify and then

00:15:27,530 --> 00:15:32,230
what are the consequences of turning off

00:15:29,450 --> 00:15:34,940
that system or changing the system or

00:15:32,230 --> 00:15:36,620
undoing the scheduling and thinking

00:15:34,940 --> 00:15:38,790
through that will help you to be

00:15:36,620 --> 00:15:41,750
prepared if that situation arises

00:15:38,790 --> 00:15:43,890
or to prevent that situation ideally

00:15:41,750 --> 00:15:46,410
respectful and secure is the third

00:15:43,890 --> 00:15:49,500
aspect of this framework and this is

00:15:46,410 --> 00:15:51,930
about valuing humanity ethics equity

00:15:49,500 --> 00:15:53,610
fairness accessibility diversity and

00:15:51,930 --> 00:15:56,790
inclusion if we're going to make an

00:15:53,610 --> 00:15:59,940
ethical system it needs to be inclusive

00:15:56,790 --> 00:16:02,040
it needs to be valuing the things that

00:15:59,940 --> 00:16:04,080
are important in those types of

00:16:02,040 --> 00:16:06,300
environments it includes respecting

00:16:04,080 --> 00:16:08,400
privacy and data rights making the

00:16:06,300 --> 00:16:10,920
system robust valid and reliable and

00:16:08,400 --> 00:16:12,900
providing understandable security all

00:16:10,920 --> 00:16:14,760
these things are aspects that are

00:16:12,900 --> 00:16:18,330
necessary for humans to trust these

00:16:14,760 --> 00:16:20,340
systems so for right staff respectful

00:16:18,330 --> 00:16:22,260
and secure might be about who has

00:16:20,340 --> 00:16:23,820
visibility for reasons for changing

00:16:22,260 --> 00:16:25,590
schedules there may be some very private

00:16:23,820 --> 00:16:28,110
reasons that people share with the

00:16:25,590 --> 00:16:30,030
managers but that they would not want to

00:16:28,110 --> 00:16:32,670
be in the system for other managers to

00:16:30,030 --> 00:16:35,040
see and how is that information used if

00:16:32,670 --> 00:16:37,110
it is put into the system what happens

00:16:35,040 --> 00:16:39,690
with that information and how is

00:16:37,110 --> 00:16:41,670
personally identifiable information of

00:16:39,690 --> 00:16:42,600
employees protected what are we doing to

00:16:41,670 --> 00:16:45,030
keep that PII

00:16:42,600 --> 00:16:48,540
out of other people's hands and safe and

00:16:45,030 --> 00:16:50,280
protected in the system honest and

00:16:48,540 --> 00:16:52,320
usable is the fourth aspect of the

00:16:50,280 --> 00:16:53,790
framework and this is about valuing

00:16:52,320 --> 00:16:56,250
transparency with the goal of

00:16:53,790 --> 00:16:59,340
engendering trust we need to explicitly

00:16:56,250 --> 00:17:01,980
state the identity as an AI system and

00:16:59,340 --> 00:17:04,290
if there is a potential for that to be

00:17:01,980 --> 00:17:06,330
confusing such as in the chat system we

00:17:04,290 --> 00:17:08,490
need to ensure that we're reminding

00:17:06,330 --> 00:17:12,480
humans that they are speaking with an AI

00:17:08,490 --> 00:17:15,240
system in that situation fairness is

00:17:12,480 --> 00:17:16,830
also important part of this and removing

00:17:15,240 --> 00:17:19,650
that unwanted bias and the data

00:17:16,830 --> 00:17:22,110
initially is an ideal situation but

00:17:19,650 --> 00:17:24,000
that's not always possible so we need to

00:17:22,110 --> 00:17:26,370
at least show awareness of known and

00:17:24,000 --> 00:17:28,440
desirable bias so often you'll have a

00:17:26,370 --> 00:17:30,510
system that is biased in a particular

00:17:28,440 --> 00:17:33,360
direction because that is the goal of

00:17:30,510 --> 00:17:35,580
the organization to make sure that

00:17:33,360 --> 00:17:37,470
people are aware of a particular type of

00:17:35,580 --> 00:17:39,690
information and not as aware of another

00:17:37,470 --> 00:17:41,210
for whatever reason but we need to

00:17:39,690 --> 00:17:42,990
acknowledge that issue and

00:17:41,210 --> 00:17:45,150
over-communicate on it so that people

00:17:42,990 --> 00:17:47,970
really understand the system and its

00:17:45,150 --> 00:17:50,010
limitations for right staff the system

00:17:47,970 --> 00:17:51,180
is built to reduce the known bias and

00:17:50,010 --> 00:17:53,580
existing data

00:17:51,180 --> 00:17:56,670
ideally but we also need to make it easy

00:17:53,580 --> 00:17:59,220
to report bias if it is a potential

00:17:56,670 --> 00:18:01,350
problem or prevent it ideally so if we

00:17:59,220 --> 00:18:03,390
can build the system to reduce that bias

00:18:01,350 --> 00:18:06,930
that's wonderful and if we can't we need

00:18:03,390 --> 00:18:09,030
to mitigate for that problem so that is

00:18:06,930 --> 00:18:11,280
the UX framework we need to be

00:18:09,030 --> 00:18:13,410
intentional to keep people safe around

00:18:11,280 --> 00:18:14,940
these four aspects and by building

00:18:13,410 --> 00:18:18,210
around these four aspects we can bring

00:18:14,940 --> 00:18:20,820
an ethical AI system to fruition so

00:18:18,210 --> 00:18:22,700
accountable to humans cognizant of

00:18:20,820 --> 00:18:25,830
speculative risks and benefits

00:18:22,700 --> 00:18:27,900
respectful and secure honest and usable

00:18:25,830 --> 00:18:29,850
and we aren't perfect

00:18:27,900 --> 00:18:32,850
so the AIS that we build will not be

00:18:29,850 --> 00:18:34,800
perfect either we need diverse teams and

00:18:32,850 --> 00:18:36,720
inclusive environments to be able to be

00:18:34,800 --> 00:18:38,280
as creative as possible within the work

00:18:36,720 --> 00:18:40,020
that we're doing we need to adopt

00:18:38,280 --> 00:18:42,120
technical ethics to be able to bring

00:18:40,020 --> 00:18:43,530
each other together to have us coalesce

00:18:42,120 --> 00:18:46,320
around something that we can all share

00:18:43,530 --> 00:18:48,360
we need to encourage deep conversations

00:18:46,320 --> 00:18:50,190
by using a checklist and other types of

00:18:48,360 --> 00:18:52,530
prompts and we need to activate

00:18:50,190 --> 00:18:54,300
curiosity so their teams are speculative

00:18:52,530 --> 00:18:57,620
and imaginative about the ways the

00:18:54,300 --> 00:19:00,240
system can be used and abused and

00:18:57,620 --> 00:19:02,190
another idea is to reward team members

00:19:00,240 --> 00:19:04,830
for finding ethics bugs this comes from

00:19:02,190 --> 00:19:07,140
dr. Ayanna Howard and she spoke on the

00:19:04,830 --> 00:19:09,300
artifice artificial intelligence podcast

00:19:07,140 --> 00:19:11,310
with Lex Friedman recently and this is a

00:19:09,300 --> 00:19:13,800
great way to support your team and to

00:19:11,310 --> 00:19:15,870
entice them to do the work the right

00:19:13,800 --> 00:19:18,720
work to help to build a great trust able

00:19:15,870 --> 00:19:21,630
AI system so I encourage you to invite

00:19:18,720 --> 00:19:24,870
evangelize for human values make ethical

00:19:21,630 --> 00:19:25,980
transparent and fair AI systems and if

00:19:24,870 --> 00:19:27,930
you'd like to continue the conversation

00:19:25,980 --> 00:19:29,700
you can learn more at the software

00:19:27,930 --> 00:19:31,890
engineering Institute website at

00:19:29,700 --> 00:19:34,650
carnegie mellon university and there is

00:19:31,890 --> 00:19:36,120
a QR code for your reference and also

00:19:34,650 --> 00:19:38,550
reach out to me I would be happy to

00:19:36,120 --> 00:19:42,020
consider to continue the conversation

00:19:38,550 --> 00:19:42,020

YouTube URL: https://www.youtube.com/watch?v=vPokIvli8yk


