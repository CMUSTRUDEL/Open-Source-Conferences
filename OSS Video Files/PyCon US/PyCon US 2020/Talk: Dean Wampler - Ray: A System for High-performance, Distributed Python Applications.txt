Title: Talk: Dean Wampler - Ray: A System for High-performance, Distributed Python Applications
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Dean Wampler

Ray is an open-source, distributed framework from U.C. Berkeley’s RISELab that easily scales Python applications from a laptop to a cluster, with an emphasis on the unique performance challenges of ML/AI systems. It is now used in many production deployments.

I’ll explain the problems that Ray solves and useful features it provides, such as rapid distribution, scheduling, and execution of “tasks” and management of distributed stateful “serverless” computing. I’ll illustrate how it’s used in several ML libraries. You’ll learn when to use Ray and how to use it in your projects.




Talk slides: https://speakerdeck.com/wampler/ray-scalability-from-a-laptop-to-a-cluster-582a9702-9c67-4f12-9b27-64846dc52162
Captions: 
	00:00:04,570 --> 00:00:08,620
everyone this is Dean wampler from any

00:00:06,760 --> 00:00:11,170
scale and I'm gonna do a talk about

00:00:08,620 --> 00:00:14,200
scaling Python applications from a

00:00:11,170 --> 00:00:16,029
laptop to a cluster there's a bunch of

00:00:14,200 --> 00:00:17,710
links here you can contact me at Dana

00:00:16,029 --> 00:00:23,500
Denny scale.com

00:00:17,710 --> 00:00:25,390
or on twitter @dnews scale at any scale

00:00:23,500 --> 00:00:27,250
comm we do have some online events

00:00:25,390 --> 00:00:29,739
happening this summer and you can go to

00:00:27,250 --> 00:00:31,720
any scale comm slash events to find out

00:00:29,739 --> 00:00:36,489
more about those and I'll make these

00:00:31,720 --> 00:00:38,500
slides available after the talk the

00:00:36,489 --> 00:00:40,420
motivation for Rae is really reflecting

00:00:38,500 --> 00:00:43,450
a couple of trends in our industry so

00:00:40,420 --> 00:00:45,490
this is a graph on the left of the sizes

00:00:43,450 --> 00:00:47,740
of neural networks as they've evolved

00:00:45,490 --> 00:00:50,170
over the last in five or six years and

00:00:47,740 --> 00:00:52,780
basically they've been evolving you know

00:00:50,170 --> 00:00:54,370
factor of like 35 times every 18 months

00:00:52,780 --> 00:00:56,920
you know by comparison moore's law is

00:00:54,370 --> 00:00:58,900
every two months so you know just

00:00:56,920 --> 00:01:01,690
scaling Hardware alone it's not doing it

00:00:58,900 --> 00:01:03,899
for us we need distributed computing to

00:01:01,690 --> 00:01:06,909
keep up with the demand

00:01:03,899 --> 00:01:09,820
at the same time Python is continuing to

00:01:06,909 --> 00:01:13,750
grow strongly as a language driven in a

00:01:09,820 --> 00:01:16,240
large degree by interest in MLA I and

00:01:13,750 --> 00:01:17,760
other data science workloads and so

00:01:16,240 --> 00:01:20,229
together these two trends are really

00:01:17,760 --> 00:01:22,330
creating a pressing need for us to make

00:01:20,229 --> 00:01:25,030
it easy to distribute Python

00:01:22,330 --> 00:01:27,159
applications over a cluster to meet

00:01:25,030 --> 00:01:29,620
these demands for scalability but also

00:01:27,159 --> 00:01:33,159
to be accessible and easy to use by

00:01:29,620 --> 00:01:35,140
Python developers so if you look at the

00:01:33,159 --> 00:01:37,479
ml landscape today you know there's a

00:01:35,140 --> 00:01:39,340
bunch of sort of tasks that you have to

00:01:37,479 --> 00:01:41,350
do a feature is a ssin is like figuring

00:01:39,340 --> 00:01:43,630
out what aspects of your data are most

00:01:41,350 --> 00:01:46,720
useful for doing predictions or whatever

00:01:43,630 --> 00:01:49,210
we often process data in real time with

00:01:46,720 --> 00:01:51,040
streams I'll talk about hyper parameter

00:01:49,210 --> 00:01:53,860
tuning and why that's important a little

00:01:51,040 --> 00:01:55,119
bit later one type of parameter Trinity

00:01:53,860 --> 00:01:56,710
is about picking the best model

00:01:55,119 --> 00:01:58,810
structure and once we have that then we

00:01:56,710 --> 00:02:00,310
have two trainer models when you're

00:01:58,810 --> 00:02:02,650
doing reinforcement learning you're off

00:02:00,310 --> 00:02:05,170
and running like game simulators or some

00:02:02,650 --> 00:02:06,729
other kinds of simulators and then once

00:02:05,170 --> 00:02:08,379
you've got these models trained after

00:02:06,729 --> 00:02:11,920
all of that work then you have to serve

00:02:08,379 --> 00:02:13,540
them and pretty much all of these things

00:02:11,920 --> 00:02:15,879
really require a distributed

00:02:13,540 --> 00:02:17,709
implementations to scale effectively

00:02:15,879 --> 00:02:19,319
especially as you get in the middle of

00:02:17,709 --> 00:02:22,480
the study

00:02:19,319 --> 00:02:24,159
so the vision of Rey is to create a core

00:02:22,480 --> 00:02:26,140
framework that can meet all the

00:02:24,159 --> 00:02:28,900
requirements for these different kinds

00:02:26,140 --> 00:02:32,409
of compute loads different situations

00:02:28,900 --> 00:02:34,150
different deployment models etc targeted

00:02:32,409 --> 00:02:35,650
originally towards Python but it's

00:02:34,150 --> 00:02:37,269
actually flexible enough to support

00:02:35,650 --> 00:02:39,599
other languages for example there's a

00:02:37,269 --> 00:02:41,980
sort of an alpha quality Java

00:02:39,599 --> 00:02:44,859
implementation that our API that's being

00:02:41,980 --> 00:02:46,780
developed and then on top of that we'd

00:02:44,859 --> 00:02:48,609
like to have domain-specific libraries

00:02:46,780 --> 00:02:50,409
so you may not even ever know you're

00:02:48,609 --> 00:02:52,329
using ray if you're working in hyper

00:02:50,409 --> 00:02:54,370
Bramber tuning or reinforcement learning

00:02:52,329 --> 00:02:56,409
you'll just use these libraries that

00:02:54,370 --> 00:02:58,150
exist for the purpose but if you're

00:02:56,409 --> 00:03:00,459
writing general purpose applications

00:02:58,150 --> 00:03:04,209
then you might really want to know about

00:03:00,459 --> 00:03:06,549
ray so let's see how what it's like to

00:03:04,209 --> 00:03:08,709
actually use ray is designed to be as

00:03:06,549 --> 00:03:11,409
intuitive and concise as possible and to

00:03:08,709 --> 00:03:13,829
leverage familiar ideas so one of them

00:03:11,409 --> 00:03:16,030
of course is writing functions in Python

00:03:13,829 --> 00:03:18,040
this is you know sort of a mocked up

00:03:16,030 --> 00:03:20,019
example where we have some function make

00:03:18,040 --> 00:03:22,180
array that returns an umpire array and

00:03:20,019 --> 00:03:26,470
then some function add arrays that can

00:03:22,180 --> 00:03:28,060
add two arrays together so yeah this is

00:03:26,470 --> 00:03:30,910
very familiar if you know Python this

00:03:28,060 --> 00:03:32,769
should not be hard to figure out well if

00:03:30,910 --> 00:03:35,139
you want to turn these into distributed

00:03:32,769 --> 00:03:36,879
tasks which is the term we use all you

00:03:35,139 --> 00:03:39,359
have to do is annotate these functions

00:03:36,879 --> 00:03:41,769
with rave remote and then they become

00:03:39,359 --> 00:03:45,519
possible to execute across a cluster

00:03:41,769 --> 00:03:47,500
automatically by ray and for

00:03:45,519 --> 00:03:49,419
completeness if this code were to work

00:03:47,500 --> 00:03:53,769
you'd have to do a few imports and then

00:03:49,419 --> 00:03:55,930
initialize ray in your application so

00:03:53,769 --> 00:03:58,720
another difference is you now invoke

00:03:55,930 --> 00:04:01,750
these by appending a dot remote function

00:03:58,720 --> 00:04:04,299
to make array to call them now pythons

00:04:01,750 --> 00:04:06,069
malleable enough that we could have made

00:04:04,299 --> 00:04:08,199
it possible to just say make array with

00:04:06,069 --> 00:04:10,629
an argument list but the reason we keep

00:04:08,199 --> 00:04:12,370
the remote in here is so that it's easy

00:04:10,629 --> 00:04:14,260
reading the code to know exactly what's

00:04:12,370 --> 00:04:16,479
going on so it does require a little

00:04:14,260 --> 00:04:18,400
more code change but it's you know as I

00:04:16,479 --> 00:04:20,320
say you tend to read code more often

00:04:18,400 --> 00:04:22,930
than write it so we felt it was better

00:04:20,320 --> 00:04:25,300
to have dot remote here as an indicator

00:04:22,930 --> 00:04:27,909
about what is actually ray specific

00:04:25,300 --> 00:04:29,500
versus what's general python but what's

00:04:27,909 --> 00:04:32,139
actually happening here is that you're

00:04:29,500 --> 00:04:34,569
sending an asynchronous computation to

00:04:32,139 --> 00:04:37,060
be done this task and it immediately

00:04:34,569 --> 00:04:39,249
returns an ID that actually corresponds

00:04:37,060 --> 00:04:41,080
to a future that we can use later to

00:04:39,249 --> 00:04:44,680
retrieve the the result of this

00:04:41,080 --> 00:04:46,870
computation and that's what ID 1 is you

00:04:44,680 --> 00:04:49,210
can do this again and then we could call

00:04:46,870 --> 00:04:52,719
this remote function to actually add the

00:04:49,210 --> 00:04:54,639
results when we're done and then that we

00:04:52,719 --> 00:04:58,659
use this function rate I get to actually

00:04:54,639 --> 00:05:00,819
retrieve the value from this computation

00:04:58,659 --> 00:05:02,050
in this case we only care about ID 3 we

00:05:00,819 --> 00:05:04,810
don't have to fetch the other two

00:05:02,050 --> 00:05:08,500
although we could this is a blocking

00:05:04,810 --> 00:05:09,969
call it will you know block until ID 3

00:05:08,500 --> 00:05:12,370
is available and then it will return the

00:05:09,969 --> 00:05:14,979
object that ID 3 points to in this case

00:05:12,370 --> 00:05:17,620
a numpy array now one of the cool things

00:05:14,979 --> 00:05:20,199
is that rey is handling the sequencing

00:05:17,620 --> 00:05:23,379
of these dependencies it cannot run add

00:05:20,199 --> 00:05:25,629
arrays until the to make array call us

00:05:23,379 --> 00:05:26,979
have completed and it just does that

00:05:25,629 --> 00:05:28,900
automatically for us we don't have to

00:05:26,979 --> 00:05:30,999
put in logic to wait and you know check

00:05:28,900 --> 00:05:32,979
to see that they're done and then handle

00:05:30,999 --> 00:05:34,719
it that way it just does it for us so it

00:05:32,979 --> 00:05:38,110
knows about this graph on the right and

00:05:34,719 --> 00:05:40,150
it processes it automatically the other

00:05:38,110 --> 00:05:42,339
nice thing is that since add arrays is

00:05:40,150 --> 00:05:44,680
remote we didn't have to extract the

00:05:42,339 --> 00:05:47,139
arrays from these future handles in

00:05:44,680 --> 00:05:48,879
order to pass them to add a race ray

00:05:47,139 --> 00:05:51,699
does this automatically for us behind

00:05:48,879 --> 00:05:53,439
the scenes so it sort of looks like the

00:05:51,699 --> 00:05:55,180
way we would have written regular Python

00:05:53,439 --> 00:05:57,460
code we don't really have to know

00:05:55,180 --> 00:05:59,229
necessarily that these are IDs to some

00:05:57,460 --> 00:06:01,240
future as opposed to actual arrays

00:05:59,229 --> 00:06:02,740
although of course you kind of do need

00:06:01,240 --> 00:06:06,310
to know that at some point especially in

00:06:02,740 --> 00:06:07,560
the last line so what about distributed

00:06:06,310 --> 00:06:10,599
state excuse me

00:06:07,560 --> 00:06:12,819
so it's it's pretty common when you're

00:06:10,599 --> 00:06:14,409
writing a distributed application that

00:06:12,819 --> 00:06:15,909
you run into this problem of how do I

00:06:14,409 --> 00:06:18,580
manage the state that's now getting

00:06:15,909 --> 00:06:19,930
distributed over the cluster and for

00:06:18,580 --> 00:06:22,659
that we're leveraging a familiar concept

00:06:19,930 --> 00:06:24,909
in Python called classes which is

00:06:22,659 --> 00:06:26,800
hopefully familiar to everybody in this

00:06:24,909 --> 00:06:29,110
case I have a simple counter class that

00:06:26,800 --> 00:06:31,180
keeps track of a value and every time I

00:06:29,110 --> 00:06:33,419
call incremented increments the value by

00:06:31,180 --> 00:06:36,009
one and then returns the current value

00:06:33,419 --> 00:06:38,439
well once again you annotate it with ray

00:06:36,009 --> 00:06:41,019
remote now it becomes an actor now I had

00:06:38,439 --> 00:06:44,229
the term actor well this is a kind of an

00:06:41,019 --> 00:06:45,729
old idea that the actor model is over 45

00:06:44,229 --> 00:06:47,499
years old I

00:06:45,729 --> 00:06:49,539
was made famous in commercial

00:06:47,499 --> 00:06:52,059
implementations by the airline language

00:06:49,539 --> 00:06:54,939
in the jvm world there's akka and

00:06:52,059 --> 00:06:56,020
there's other implementations it's the

00:06:54,939 --> 00:06:58,059
idea that you have these autonomous

00:06:56,020 --> 00:07:00,189
agents and you communicate with them by

00:06:58,059 --> 00:07:02,710
sending them messages either to do work

00:07:00,189 --> 00:07:04,409
or to get values or whatever and then

00:07:02,710 --> 00:07:07,059
the environment provides a thread-safe

00:07:04,409 --> 00:07:08,770
execution model one at a time those

00:07:07,059 --> 00:07:11,229
messages are processed so they're kept

00:07:08,770 --> 00:07:13,689
in a queue somewhere and that means that

00:07:11,229 --> 00:07:14,949
the writer of an actor implementation

00:07:13,689 --> 00:07:17,520
doesn't have to worry about thread

00:07:14,949 --> 00:07:20,800
safety code they just write regular code

00:07:17,520 --> 00:07:22,539
and the actor model handles the thread

00:07:20,800 --> 00:07:25,659
safety for you so it's a really powerful

00:07:22,539 --> 00:07:27,159
model for concurrency it abstracts over

00:07:25,659 --> 00:07:29,680
a lot of the complexity of writing

00:07:27,159 --> 00:07:33,189
thread safe code so now we have a remote

00:07:29,680 --> 00:07:35,110
actor now there is one difference you

00:07:33,189 --> 00:07:37,419
have to do here ray currently doesn't

00:07:35,110 --> 00:07:40,360
support just reading the values of

00:07:37,419 --> 00:07:42,819
fields inside the object so you do have

00:07:40,360 --> 00:07:44,589
to write a getter method if it's not

00:07:42,819 --> 00:07:49,419
sufficient to just capture the value of

00:07:44,589 --> 00:07:51,599
increment okay so what actually happens

00:07:49,419 --> 00:07:53,589
once again we use our remote calls

00:07:51,599 --> 00:07:55,539
notice how it's used for both the

00:07:53,589 --> 00:07:58,029
constructor and for these method calls

00:07:55,539 --> 00:08:00,099
and then we can also call raid I get

00:07:58,029 --> 00:08:02,800
with an array of IDs and that's going to

00:08:00,099 --> 00:08:07,599
return an array of or lists of one and

00:08:02,800 --> 00:08:10,569
two this radar remote function actually

00:08:07,599 --> 00:08:12,490
does take a bunch of optional parameters

00:08:10,569 --> 00:08:15,490
that you specify things like how many

00:08:12,490 --> 00:08:17,229
GPUs you want to use and how many times

00:08:15,490 --> 00:08:18,909
you can call this function before not

00:08:17,229 --> 00:08:26,649
allowing any more calls and that kind of

00:08:18,909 --> 00:08:28,709
stuff okay so how does this work so here

00:08:26,649 --> 00:08:31,149
imagine I've got a three node cluster

00:08:28,709 --> 00:08:33,039
we'll talk about what all these boxes

00:08:31,149 --> 00:08:34,779
are doing in a moment but basically

00:08:33,039 --> 00:08:37,510
we're going to see how this graph of

00:08:34,779 --> 00:08:40,630
tasks could be scheduled in a cluster

00:08:37,510 --> 00:08:42,969
like this so let's assume that our

00:08:40,630 --> 00:08:44,889
driver program is running on node one as

00:08:42,969 --> 00:08:47,769
soon as we make these function calls

00:08:44,889 --> 00:08:52,230
it's going to schedule or call the local

00:08:47,769 --> 00:08:52,230
scheduler to do the computation

00:08:53,329 --> 00:08:55,429
it's going to return those IDs

00:08:54,379 --> 00:08:57,980
immediately you know these are

00:08:55,429 --> 00:09:02,540
asynchronous computations so the calling

00:08:57,980 --> 00:09:03,980
them comes back right away and we also

00:09:02,540 --> 00:09:06,019
have this global control store that's

00:09:03,980 --> 00:09:08,439
keeping track of where everything is and

00:09:06,019 --> 00:09:10,759
what's going on

00:09:08,439 --> 00:09:15,220
so the scheduler might pick the local

00:09:10,759 --> 00:09:18,769
worker to do the first make array call

00:09:15,220 --> 00:09:23,959
and it might pick another worker on

00:09:18,769 --> 00:09:26,540
another node to do the other one and

00:09:23,959 --> 00:09:29,989
once each task is finished it will write

00:09:26,540 --> 00:09:33,799
its result its object back to the object

00:09:29,989 --> 00:09:35,720
store and now these tasks can be deleted

00:09:33,799 --> 00:09:36,949
from the worker memory this is one

00:09:35,720 --> 00:09:39,170
difference with that with actors

00:09:36,949 --> 00:09:41,769
actually actors are pinned to the worker

00:09:39,170 --> 00:09:43,939
because they're holding state and until

00:09:41,769 --> 00:09:47,589
references to them disappear on the

00:09:43,939 --> 00:09:47,589
driver code they'll remain in the worker

00:09:50,439 --> 00:09:57,499
now we can schedule our ad arrays it can

00:09:55,819 --> 00:09:59,149
read object 1 from shared memory it

00:09:57,499 --> 00:10:01,009
doesn't need to copy the object out of

00:09:59,149 --> 00:10:02,929
the store into the workers memory space

00:10:01,009 --> 00:10:04,850
so this is rather convenient if if

00:10:02,929 --> 00:10:06,549
object 1 is very big it helps for

00:10:04,850 --> 00:10:09,379
performance

00:10:06,549 --> 00:10:10,999
now object 2 is not on the same node so

00:10:09,379 --> 00:10:13,399
this has to be copied over there's a bit

00:10:10,999 --> 00:10:16,569
of overhead obviously doing this but now

00:10:13,399 --> 00:10:19,850
it can be read from shared memory - and

00:10:16,569 --> 00:10:23,089
when add arrays is done it writes its

00:10:19,850 --> 00:10:24,739
result back to the object store and then

00:10:23,089 --> 00:10:26,749
when we call reh Dogg at the global

00:10:24,739 --> 00:10:29,809
control store tells us where it is and

00:10:26,749 --> 00:10:33,139
our code returns object 3 and that's it

00:10:29,809 --> 00:10:34,459
so it's doing a lot of work behind the

00:10:33,139 --> 00:10:35,869
scenes for you there's obviously a

00:10:34,459 --> 00:10:38,029
little bit of overhead for all this

00:10:35,869 --> 00:10:40,189
stuff so you don't want to use really

00:10:38,029 --> 00:10:42,860
fine grained tasks because you'll incur

00:10:40,189 --> 00:10:44,569
overhead that's you know just not giving

00:10:42,860 --> 00:10:46,699
you that much value but it is fantastic

00:10:44,569 --> 00:10:49,309
for doing all this asynchronous work

00:10:46,699 --> 00:10:54,069
even very large computations with large

00:10:49,309 --> 00:10:54,069
objects over a distributed cluster

00:10:54,769 --> 00:10:57,410
all right suppose you want to get

00:10:55,819 --> 00:11:00,769
involved with ray what are some of the

00:10:57,410 --> 00:11:02,779
things you might want to look into the

00:11:00,769 --> 00:11:05,869
first place to start is radar IO that's

00:11:02,779 --> 00:11:09,170
where you'll find our blog links to the

00:11:05,869 --> 00:11:10,850
documentation and so forth then the next

00:11:09,170 --> 00:11:13,220
link here is actually the documentation

00:11:10,850 --> 00:11:15,230
where you can find details about

00:11:13,220 --> 00:11:19,879
installing Rey getting started and so

00:11:15,230 --> 00:11:21,589
forth we're developing new tutorials and

00:11:19,879 --> 00:11:23,569
our so called any scale Academy those

00:11:21,589 --> 00:11:26,629
will be coming out this summer actually

00:11:23,569 --> 00:11:28,429
this spring in summer if you want to see

00:11:26,629 --> 00:11:30,860
the github code for the rape project

00:11:28,429 --> 00:11:32,329
here's the link for that if you need

00:11:30,860 --> 00:11:34,369
help the best place to go is the race

00:11:32,329 --> 00:11:36,679
slack we monitor it really carefully and

00:11:34,369 --> 00:11:40,629
often help people there there's also a

00:11:36,679 --> 00:11:44,689
rate dev group if you prefer doing a

00:11:40,629 --> 00:11:46,519
conversation over emails all right

00:11:44,689 --> 00:11:48,769
suppose you want to adopt Rae

00:11:46,519 --> 00:11:50,209
you could certainly start programming

00:11:48,769 --> 00:11:53,480
Rae directly but you may have already

00:11:50,209 --> 00:11:55,249
been writing code using other api's well

00:11:53,480 --> 00:11:58,069
it turns out we've done implementations

00:11:55,249 --> 00:12:01,429
of the api's for async i/o table Lib and

00:11:58,069 --> 00:12:04,279
multi processing dot pool so that you

00:12:01,429 --> 00:12:06,040
can just drop in in most cases just drop

00:12:04,279 --> 00:12:08,740
in Ray by changing an import statement

00:12:06,040 --> 00:12:12,949
and then you not only have the same

00:12:08,740 --> 00:12:14,480
local mode local node computation but

00:12:12,949 --> 00:12:16,399
now you've broken that boundary you can

00:12:14,480 --> 00:12:18,470
you can basically scale your apps to a

00:12:16,399 --> 00:12:20,869
cluster just with this substitution

00:12:18,470 --> 00:12:23,600
there's a couple of blog links on our

00:12:20,869 --> 00:12:25,970
rate blog you if you go to radio you can

00:12:23,600 --> 00:12:30,579
get to these blog posts and find out

00:12:25,970 --> 00:12:32,540
more information about how this is done

00:12:30,579 --> 00:12:34,579
alright let's talk about these higher

00:12:32,540 --> 00:12:36,350
level libraries that I mentioned towards

00:12:34,579 --> 00:12:38,019
the beginning for machine learning and

00:12:36,350 --> 00:12:40,339
I'll just talk about two of them today

00:12:38,019 --> 00:12:42,769
you know once again there's several here

00:12:40,339 --> 00:12:44,629
tune is for hyper parameter tuning ray

00:12:42,769 --> 00:12:47,929
SGD is something we just rolled out

00:12:44,629 --> 00:12:50,509
that's it helps distribute training or a

00:12:47,929 --> 00:12:52,459
Lib is perhaps our most popular library

00:12:50,509 --> 00:12:54,439
for reinforcement learning and then

00:12:52,459 --> 00:12:58,249
another new library is server for muddle

00:12:54,439 --> 00:13:00,529
serving so let's talk about aural lip

00:12:58,249 --> 00:13:01,819
for a minute so if you

00:13:00,529 --> 00:13:04,339
know much about reinforcement learning

00:13:01,819 --> 00:13:05,779
the idea is pretty simple although

00:13:04,339 --> 00:13:08,060
obviously there's a lot of complexity

00:13:05,779 --> 00:13:11,060
behind the scenes you have some sort of

00:13:08,060 --> 00:13:14,660
agent acting in an environment and it's

00:13:11,060 --> 00:13:16,879
observing what's going on it's it's also

00:13:14,660 --> 00:13:19,160
making decisions about what actions to

00:13:16,879 --> 00:13:21,110
do next and then it observes what reward

00:13:19,160 --> 00:13:23,930
it receives for taking it those actions

00:13:21,110 --> 00:13:26,209
and the goal is to optimize the rewards

00:13:23,930 --> 00:13:28,279
during the sequence of steps that it

00:13:26,209 --> 00:13:31,129
takes in this environment and some

00:13:28,279 --> 00:13:33,189
famous examples are the alphago system

00:13:31,129 --> 00:13:35,420
that beat the world's best go player

00:13:33,189 --> 00:13:37,639
that was really one of the things that

00:13:35,420 --> 00:13:40,579
put it reinforcement learning on the map

00:13:37,639 --> 00:13:43,430
in a very big way it's been used to play

00:13:40,579 --> 00:13:47,149
the Atari games to train simulated

00:13:43,430 --> 00:13:50,720
robots and even like simulated Walker's

00:13:47,149 --> 00:13:52,459
teaching them to walk you know alpha

00:13:50,720 --> 00:13:53,870
goes a little bit more sophisticated or

00:13:52,459 --> 00:13:55,759
very sophisticated you might imagine

00:13:53,870 --> 00:13:57,639
there's a big neural network behind the

00:13:55,759 --> 00:13:59,959
scenes that's helping make decisions so

00:13:57,639 --> 00:14:01,850
in this case the observations for the

00:13:59,959 --> 00:14:03,980
board state the actions are where to

00:14:01,850 --> 00:14:05,990
place the stones and the neural network

00:14:03,980 --> 00:14:08,240
is trying to give you the best choices

00:14:05,990 --> 00:14:12,769
there and the rewards are pretty simple

00:14:08,240 --> 00:14:15,230
either you win or you don't we're also

00:14:12,769 --> 00:14:16,819
seeing it reinforcement learning being

00:14:15,230 --> 00:14:18,649
used in a bunch of other contexts

00:14:16,819 --> 00:14:20,569
there's some a lot of interesting work

00:14:18,649 --> 00:14:22,730
being done an optimizing industrial

00:14:20,569 --> 00:14:26,600
processes like how factory floors work

00:14:22,730 --> 00:14:28,040
and pipelines and so forth optimizing

00:14:26,600 --> 00:14:30,949
networked computing and that sort of

00:14:28,040 --> 00:14:33,920
thing it's being used as a new way to do

00:14:30,949 --> 00:14:35,689
ad serving and recommendations that get

00:14:33,920 --> 00:14:38,389
get around some of the problems of scale

00:14:35,689 --> 00:14:39,980
with traditional methods and Finance -

00:14:38,389 --> 00:14:43,160
you know obviously the stock market is a

00:14:39,980 --> 00:14:44,449
you know a time bearing system and so in

00:14:43,160 --> 00:14:46,429
theory you should be able to use

00:14:44,449 --> 00:14:49,459
reinforcement learning to optimize your

00:14:46,429 --> 00:14:50,569
performance in the stock market so

00:14:49,459 --> 00:14:52,819
whatever application you're building

00:14:50,569 --> 00:14:54,350
there's a whole bunch of architectural

00:14:52,819 --> 00:14:56,269
decisions you'll make like whether

00:14:54,350 --> 00:14:58,370
there's a single agent or multiple

00:14:56,269 --> 00:15:00,170
cooperating agents or a hierarchy of

00:14:58,370 --> 00:15:01,790
them offline batch is kind of

00:15:00,170 --> 00:15:04,189
interesting it's about the idea that

00:15:01,790 --> 00:15:06,170
well I can't have you just run my

00:15:04,189 --> 00:15:08,209
chemical factory over and over again but

00:15:06,170 --> 00:15:12,649
I do have massive logs of past

00:15:08,209 --> 00:15:14,450
performance can we train against that RL

00:15:12,649 --> 00:15:16,610
lib tries to provide a uniform

00:15:14,450 --> 00:15:19,400
API for all of these choices and then

00:15:16,610 --> 00:15:20,450
give you a wide range of the algorithms

00:15:19,400 --> 00:15:23,030
that have been developed for

00:15:20,450 --> 00:15:28,010
reinforcement learning all executed by

00:15:23,030 --> 00:15:29,540
Ray here's an eye chart of many of the

00:15:28,010 --> 00:15:31,370
algorithms that are available and are a

00:15:29,540 --> 00:15:34,070
Lib these are all links that you can

00:15:31,370 --> 00:15:38,330
click to if when you get the PDF for

00:15:34,070 --> 00:15:41,890
this talk you can even run it in sage

00:15:38,330 --> 00:15:41,890
maker if you're an Amazon customer

00:15:42,430 --> 00:15:46,700
now in fact reinforcement learning was

00:15:44,870 --> 00:15:48,560
one of the big motivators for the

00:15:46,700 --> 00:15:50,480
creation of ray because there's a whole

00:15:48,560 --> 00:15:52,390
lot of different compute and memory

00:15:50,480 --> 00:15:55,130
access patterns involved in

00:15:52,390 --> 00:15:57,560
reinforcement learning you may be

00:15:55,130 --> 00:15:59,330
running a simulator like a game in Qin a

00:15:57,560 --> 00:16:02,710
robot sim factory floor simulator

00:15:59,330 --> 00:16:04,610
whatever and this is more like a typical

00:16:02,710 --> 00:16:07,520
object-oriented application or whatever

00:16:04,610 --> 00:16:09,950
that has very diverse complex graphs and

00:16:07,520 --> 00:16:12,050
memory diverse access patterns for those

00:16:09,950 --> 00:16:15,350
graphs different computations and so

00:16:12,050 --> 00:16:16,970
forth and it's not the traditional data

00:16:15,350 --> 00:16:18,860
processing problem where you just have

00:16:16,970 --> 00:16:22,330
massive data sets that you're flowing

00:16:18,860 --> 00:16:24,200
through a system to do queries or

00:16:22,330 --> 00:16:27,350
transformations or whatever very

00:16:24,200 --> 00:16:28,700
different kind of compute model you're

00:16:27,350 --> 00:16:31,550
also doing all the regular neural

00:16:28,700 --> 00:16:33,560
network stuff that we've been optimizing

00:16:31,550 --> 00:16:36,890
with systems like tensor flow and pi

00:16:33,560 --> 00:16:38,480
torch and you're gonna have to run this

00:16:36,890 --> 00:16:40,580
over and over again because you're gonna

00:16:38,480 --> 00:16:43,880
keep playing that the game if you will

00:16:40,580 --> 00:16:45,920
until you find an optimal configuration

00:16:43,880 --> 00:16:48,050
that maximizes the reward so you need to

00:16:45,920 --> 00:16:50,750
be able to do this efficiently and so

00:16:48,050 --> 00:16:52,580
forth so there's just a diverse a set of

00:16:50,750 --> 00:16:54,890
challenges here that none of the

00:16:52,580 --> 00:16:56,570
existing systems that the researchers at

00:16:54,890 --> 00:17:01,460
Berkeley we're dealing with could really

00:16:56,570 --> 00:17:03,590
address so they invented ray let's talk

00:17:01,460 --> 00:17:07,100
about hyper parameter tuning and the

00:17:03,590 --> 00:17:08,930
tune library for doing this so hyper

00:17:07,100 --> 00:17:11,329
parameter tuning is really asking what's

00:17:08,930 --> 00:17:13,459
the best model type that I should use

00:17:11,329 --> 00:17:14,990
where the hyper parameters tell you that

00:17:13,459 --> 00:17:17,510
so the most trivial example I could come

00:17:14,990 --> 00:17:20,660
up with is the K and k-means clustering

00:17:17,510 --> 00:17:23,540
this is an example of K equals three

00:17:20,660 --> 00:17:25,550
where you know once I picked K then I'll

00:17:23,540 --> 00:17:27,320
iterate through with a relatively

00:17:25,550 --> 00:17:28,370
straightforward algorithm to find the

00:17:27,320 --> 00:17:31,610
clusters in

00:17:28,370 --> 00:17:33,740
data set as you you know look at this

00:17:31,610 --> 00:17:35,540
moving example on the right you can see

00:17:33,740 --> 00:17:37,640
that there's two fairly obvious clusters

00:17:35,540 --> 00:17:39,500
and then one more amorphous cluster so k

00:17:37,640 --> 00:17:41,240
equals three is about right if you're

00:17:39,500 --> 00:17:42,980
working with two-dimensional data you

00:17:41,240 --> 00:17:44,750
can plot it often and just look at it

00:17:42,980 --> 00:17:46,370
and see what the best choice is but it's

00:17:44,750 --> 00:17:48,760
not so easy if you're doing multi

00:17:46,370 --> 00:17:52,270
dimensional data beyond three dimensions

00:17:48,760 --> 00:17:55,030
and maybe very complex structure so

00:17:52,270 --> 00:17:57,380
sometimes you just have to run this

00:17:55,030 --> 00:17:59,630
clustering algorithms many times with

00:17:57,380 --> 00:18:01,220
different values of K to find the best K

00:17:59,630 --> 00:18:03,980
value and that's really what hyper

00:18:01,220 --> 00:18:06,290
parameter tuning is about finding that

00:18:03,980 --> 00:18:08,210
structure and then training the model to

00:18:06,290 --> 00:18:11,000
give you the actual parameters which

00:18:08,210 --> 00:18:13,730
would be the clusters in this case well

00:18:11,000 --> 00:18:16,160
you know finding the K and k-means is

00:18:13,730 --> 00:18:17,660
not a terribly challenging problem it's

00:18:16,160 --> 00:18:19,370
a little bit expensive maybe doing the

00:18:17,660 --> 00:18:20,990
computation but where it really gets

00:18:19,370 --> 00:18:23,300
challenging is when you're looking at

00:18:20,990 --> 00:18:25,070
things like neural networks every single

00:18:23,300 --> 00:18:27,410
number you see here including the number

00:18:25,070 --> 00:18:29,809
of layers what kinds of layers is a

00:18:27,410 --> 00:18:33,140
hyper parameter and there's obviously a

00:18:29,809 --> 00:18:34,460
huge huge space of choices you could

00:18:33,140 --> 00:18:37,460
make when you're trying to pick the best

00:18:34,460 --> 00:18:39,110
neural network for a problem so the idea

00:18:37,460 --> 00:18:41,600
with hyper parameter tuning is to

00:18:39,110 --> 00:18:43,910
optimize that search process so that you

00:18:41,600 --> 00:18:45,620
come up with the best network or a

00:18:43,910 --> 00:18:48,200
reasonably good performing network

00:18:45,620 --> 00:18:51,580
without you know wasting an extreme

00:18:48,200 --> 00:18:53,929
amount of compute trying to find it

00:18:51,580 --> 00:18:56,870
it does matter you know this is an

00:18:53,929 --> 00:18:58,820
example of running a cheetah simulator

00:18:56,870 --> 00:19:01,940
with different sets of hyper parameters

00:18:58,820 --> 00:19:04,040
and the blue does the best job the pink

00:19:01,940 --> 00:19:06,860
does the worst job in this case so it

00:19:04,040 --> 00:19:09,500
doesn't make a difference and tune

00:19:06,860 --> 00:19:12,320
emerged as a tool for helping you find

00:19:09,500 --> 00:19:14,780
the best model when resources are

00:19:12,320 --> 00:19:17,000
expensive like GPUs for example for

00:19:14,780 --> 00:19:18,800
neural network training and it's also

00:19:17,000 --> 00:19:23,179
time consuming to do training and you

00:19:18,800 --> 00:19:25,400
want to optimize all of these things so

00:19:23,179 --> 00:19:26,900
tuned handles the distributed training

00:19:25,400 --> 00:19:30,140
as you try a lot of different model

00:19:26,900 --> 00:19:33,500
structures to see which one is best you

00:19:30,140 --> 00:19:35,630
know it leverages a bunch of algorithms

00:19:33,500 --> 00:19:37,610
behind the scenes and tools like pi

00:19:35,630 --> 00:19:39,650
torch tensorflow scikit-learn and so

00:19:37,610 --> 00:19:41,789
forth and it tries to make it very easy

00:19:39,650 --> 00:19:44,309
to just declare what you want to do

00:19:41,789 --> 00:19:46,830
and then do it and then tune does the

00:19:44,309 --> 00:19:48,450
rest of the work it's integrated with

00:19:46,830 --> 00:19:51,750
tensor boards so you can see what your

00:19:48,450 --> 00:19:53,039
hyper parameters look like and it's

00:19:51,750 --> 00:19:54,690
built with deep learning as a priority

00:19:53,039 --> 00:19:57,390
as we mentioned that's where this is

00:19:54,690 --> 00:19:58,950
really a problem you know so has

00:19:57,390 --> 00:20:03,600
resource aware scheduling you know you

00:19:58,950 --> 00:20:05,309
can tell it where the GPUs are it does

00:20:03,600 --> 00:20:06,690
you're pretty seamless distributed

00:20:05,309 --> 00:20:08,279
execution you don't really have to know

00:20:06,690 --> 00:20:09,539
what's going on behind the scenes

00:20:08,279 --> 00:20:12,659
you just know there's a cluster out

00:20:09,539 --> 00:20:15,630
there doing your work it's relatively

00:20:12,659 --> 00:20:17,760
simple to add new algorithms and new

00:20:15,630 --> 00:20:20,970
tools to the API so it's like an

00:20:17,760 --> 00:20:22,649
umbrella for these things and it's

00:20:20,970 --> 00:20:25,590
framework agnostic it supports a bunch

00:20:22,649 --> 00:20:27,480
of the popular frameworks and here's the

00:20:25,590 --> 00:20:32,070
link for the tune documentation again at

00:20:27,480 --> 00:20:34,049
the ray read the docs that i/o site the

00:20:32,070 --> 00:20:35,850
last thing I want to talk about is let's

00:20:34,049 --> 00:20:38,429
put aside machine learning for a minute

00:20:35,850 --> 00:20:40,500
and talk about ray as a general tool for

00:20:38,429 --> 00:20:41,760
micro services whatever kind of services

00:20:40,500 --> 00:20:43,169
you're building now I know micro

00:20:41,760 --> 00:20:44,970
services are getting a little bit of

00:20:43,169 --> 00:20:47,340
blowback at the moment you may have

00:20:44,970 --> 00:20:50,250
heard Boober famously saying that

00:20:47,340 --> 00:20:51,899
they're going to macro services you know

00:20:50,250 --> 00:20:53,909
aside from the point of what's the right

00:20:51,899 --> 00:20:56,159
sizing for your services the general

00:20:53,909 --> 00:20:59,250
idea is still valid you just have to be

00:20:56,159 --> 00:21:01,250
smart about how you use it there's sort

00:20:59,250 --> 00:21:03,630
of a couple of reasons why you build

00:21:01,250 --> 00:21:05,789
micro services just to stick with the

00:21:03,630 --> 00:21:08,370
term one is that they partition the

00:21:05,789 --> 00:21:10,679
domain and this is often partitioned in

00:21:08,370 --> 00:21:12,299
two ways one is embracing Conway's law

00:21:10,679 --> 00:21:14,480
and I'll explain what I mean by that in

00:21:12,299 --> 00:21:16,799
a second the other is just two separate

00:21:14,480 --> 00:21:19,950
responsibilities like have the network

00:21:16,799 --> 00:21:21,240
team worry about dns resolution the

00:21:19,950 --> 00:21:23,279
security team to worry about

00:21:21,240 --> 00:21:25,740
authentication that kind of stuff and

00:21:23,279 --> 00:21:29,429
also management of these things in

00:21:25,740 --> 00:21:31,409
production is an important driver so

00:21:29,429 --> 00:21:34,350
Conway's law this is an observation by

00:21:31,409 --> 00:21:36,630
Mel Conway back in the 60s that he

00:21:34,350 --> 00:21:39,330
noticed that the architecture of systems

00:21:36,630 --> 00:21:40,770
often reflected the organization of the

00:21:39,330 --> 00:21:42,809
structure that built them in other words

00:21:40,770 --> 00:21:46,289
like the subsystems tended to reflect

00:21:42,809 --> 00:21:48,659
teams or organisations within the larger

00:21:46,289 --> 00:21:50,309
team and it's something we've actually

00:21:48,659 --> 00:21:52,139
decided we should embrace we should

00:21:50,309 --> 00:21:53,970
organize our teams to reflect the

00:21:52,139 --> 00:21:55,850
structure of the system we're building

00:21:53,970 --> 00:21:57,559
and that and the reason for doing that

00:21:55,850 --> 00:21:59,750
so the reason that this happened in the

00:21:57,559 --> 00:22:02,270
beginning was that we want to minimize

00:21:59,750 --> 00:22:03,830
the communication we have to do outside

00:22:02,270 --> 00:22:05,980
of our organization because that's a

00:22:03,830 --> 00:22:09,080
burden for people to have too many

00:22:05,980 --> 00:22:10,940
communication channels to maintain and

00:22:09,080 --> 00:22:13,870
it lets us encapsulate you know within a

00:22:10,940 --> 00:22:18,860
smaller team the amount of high fidelity

00:22:13,870 --> 00:22:20,600
communication that's required obviously

00:22:18,860 --> 00:22:22,429
I just mentioned a few examples of where

00:22:20,600 --> 00:22:24,350
you want your micro service to have you

00:22:22,429 --> 00:22:26,299
minimal like hopefully a single

00:22:24,350 --> 00:22:29,270
responsibility that it does very well

00:22:26,299 --> 00:22:31,010
and it has a natural minimal coupling to

00:22:29,270 --> 00:22:33,760
other micro services and hopefully

00:22:31,010 --> 00:22:36,500
reflects the team organization as well

00:22:33,760 --> 00:22:38,450
but the thing I'm really of heading

00:22:36,500 --> 00:22:41,240
towards here as far as Ray is the the

00:22:38,450 --> 00:22:43,370
management challenge it's common in a

00:22:41,240 --> 00:22:46,000
DevOps world for the development team to

00:22:43,370 --> 00:22:48,020
also be responsible for running and

00:22:46,000 --> 00:22:49,640
operationalizing their micro services

00:22:48,020 --> 00:22:53,299
you know and some framework that was

00:22:49,640 --> 00:22:56,090
built by the organization as a whole and

00:22:53,299 --> 00:22:58,070
what you typically see is that each

00:22:56,090 --> 00:22:59,900
micro service will actually have a

00:22:58,070 --> 00:23:01,460
different number of instances from the

00:22:59,900 --> 00:23:04,510
other micro service and those instances

00:23:01,460 --> 00:23:06,559
will come and go at different rates if

00:23:04,510 --> 00:23:08,690
you know as I'm shown on this diagram

00:23:06,559 --> 00:23:10,460
micro service 3 might not need as many

00:23:08,690 --> 00:23:12,740
instances but suppose it's actually

00:23:10,460 --> 00:23:14,929
evolving much faster has to be updated

00:23:12,740 --> 00:23:16,730
more frequently well you know the other

00:23:14,929 --> 00:23:19,280
two might be more stable but require

00:23:16,730 --> 00:23:20,929
more instances for their load this is a

00:23:19,280 --> 00:23:22,580
bit of a burden and you know why do we

00:23:20,929 --> 00:23:26,030
have so many instances here well there's

00:23:22,580 --> 00:23:28,100
two reasons one is we need we might need

00:23:26,030 --> 00:23:29,929
them for scalability because you know we

00:23:28,100 --> 00:23:31,880
hit the limits of one node so we have to

00:23:29,929 --> 00:23:34,130
go to more nodes and hence more

00:23:31,880 --> 00:23:36,950
instances and also it helps with

00:23:34,130 --> 00:23:38,870
resiliency if we have you know a crucial

00:23:36,950 --> 00:23:40,640
micro service and it's running just on

00:23:38,870 --> 00:23:41,480
one node and that node crashes well we

00:23:40,640 --> 00:23:45,830
could be in trouble

00:23:41,480 --> 00:23:49,039
so resiliency is another reason what ray

00:23:45,830 --> 00:23:51,200
is giving us is the ability you know

00:23:49,039 --> 00:23:53,179
this is sort of a 90/10 kind of solution

00:23:51,200 --> 00:23:55,580
is I don't want to pretend this is magic

00:23:53,179 --> 00:23:57,380
but because ray can transparently

00:23:55,580 --> 00:24:00,110
leverage a whole cluster behind the

00:23:57,380 --> 00:24:02,059
scenes it goes back to the idea that we

00:24:00,110 --> 00:24:03,919
really have one logical instance for

00:24:02,059 --> 00:24:06,580
each micro service we're running and we

00:24:03,919 --> 00:24:09,190
let ray handle the scalability behind us

00:24:06,580 --> 00:24:11,620
and this also

00:24:09,190 --> 00:24:13,630
tails nicely with clustering systems

00:24:11,620 --> 00:24:16,180
like kubernetes you know if you think

00:24:13,630 --> 00:24:18,070
about a pod of containers and kubernetes

00:24:16,180 --> 00:24:20,800
it's really like a small machine and

00:24:18,070 --> 00:24:23,620
just as Ray can use you know physical

00:24:20,800 --> 00:24:25,960
hardware it's a much more fine-grained

00:24:23,620 --> 00:24:27,850
scheduling system that can work inside

00:24:25,960 --> 00:24:33,190
the the constructs of a larger

00:24:27,850 --> 00:24:35,590
scheduling system like kubernetes so to

00:24:33,190 --> 00:24:39,430
conclude Ray is the new state-of-the-art

00:24:35,590 --> 00:24:41,950
system for distributed computing it's

00:24:39,430 --> 00:24:43,360
you know we believe the shortest path to

00:24:41,950 --> 00:24:45,250
go from something that's running on your

00:24:43,360 --> 00:24:47,260
laptop to or something that's running in

00:24:45,250 --> 00:24:49,650
the cloud on a cluster of machines or in

00:24:47,260 --> 00:24:52,120
native Hardware on in your environment

00:24:49,650 --> 00:24:54,250
and its ability it has enough

00:24:52,120 --> 00:24:57,880
flexibility that it can run a wide

00:24:54,250 --> 00:25:02,740
diversity of compute tests and memory

00:24:57,880 --> 00:25:04,890
access patterns any scale was the

00:25:02,740 --> 00:25:07,720
company that's spun out of Berkley to

00:25:04,890 --> 00:25:09,970
develop Ray's an open source system and

00:25:07,720 --> 00:25:11,650
to build products and services around it

00:25:09,970 --> 00:25:14,110
we are actually hiring even in this

00:25:11,650 --> 00:25:17,850
kovat era we live in and you can find

00:25:14,110 --> 00:25:17,850
out more at any scale com

00:25:18,550 --> 00:25:22,000
I can't actually take questions

00:25:19,920 --> 00:25:24,700
unfortunately but here's some more links

00:25:22,000 --> 00:25:27,340
for you again rate at i/o is the place

00:25:24,700 --> 00:25:30,940
to go for information about ray V scale

00:25:27,340 --> 00:25:32,650
comm it find out about our job openings

00:25:30,940 --> 00:25:34,840
find out about the events like our

00:25:32,650 --> 00:25:37,060
summer ray summit connect series these

00:25:34,840 --> 00:25:40,390
are online events we're doing starting

00:25:37,060 --> 00:25:41,950
in May we are going to do ray summit

00:25:40,390 --> 00:25:43,750
that was scheduled for May it's now

00:25:41,950 --> 00:25:46,390
going most likely going to happen in

00:25:43,750 --> 00:25:48,490
November and I do welcome you to reach

00:25:46,390 --> 00:25:50,800
out to me at any scale a Dean at any

00:25:48,490 --> 00:25:52,390
scale comm especially if you want the

00:25:50,800 --> 00:25:54,340
slides I'll try to make them available

00:25:52,390 --> 00:25:56,500
but I'm not sure what they'll be posted

00:25:54,340 --> 00:25:59,020
yet so feel free to reach out to me at

00:25:56,500 --> 00:26:02,730
Dean at any scale comm thank you so much

00:25:59,020 --> 00:26:02,730

YouTube URL: https://www.youtube.com/watch?v=tqUe0gcfqAU


