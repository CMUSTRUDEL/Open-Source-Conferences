Title: Talk: Flávio Juvenal da Silva Junior - 1 + 1 = 1 or Record Deduplication with Python
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Flávio Juvenal da Silva Junior

How to find duplicate records in a dataset without unique identifiers, like the SSN for US citizens? The answer is to use Record Deduplication techniques: look for matches by cleaning and comparing attributes in a fuzzy way. In this talk, you’ll learn with Python examples how to do this without needing any expert Data Science knowledge.

There are several critical applications of Record Deduplication in government and business. For example, by deduping records from Census data, the Australian government was able to find there were 250,000 fewer people in the country than they previously thought. This reduction impacted the estimations of government agencies and even caused the revision of economic projections. Similarly, businesses can use record deduplication techniques to clean up customers’ data. In this talk, you’ll learn with Python examples the main concepts of Record Deduplication: what kinds of problems can be solved, what’s the most common workflow for the process, what algorithms are involved, and which tools and libraries you can use. Although some of the discussed concepts are related to data mining, any intermediate-level Python developer will be able to learn the basics of how to dedupe data using Python.




Talk slides: https://github.com/vintasoftware/deduplication-slides/tree/pycon-2020
Captions: 
	00:00:12,980 --> 00:00:20,010
hello folds I flop you to Reno and I'll

00:00:17,280 --> 00:00:23,100
be talking with you today oh about one

00:00:20,010 --> 00:00:25,410
plus one equals one or record the

00:00:23,100 --> 00:00:29,640
duplication win fight I'm really honored

00:00:25,410 --> 00:00:31,740
to be part of Python u.s. 2020 my

00:00:29,640 --> 00:00:35,010
background is dead I'm a partner at Vita

00:00:31,740 --> 00:00:36,780
we are web consultancy from Brazil that

00:00:35,010 --> 00:00:39,630
works with clients from all over the

00:00:36,780 --> 00:00:42,329
world doing Jago development and we also

00:00:39,630 --> 00:00:45,510
do some record deduplication image

00:00:42,329 --> 00:00:48,330
resolution work here is my twitter

00:00:45,510 --> 00:00:50,970
handle and flabby juvenile in case you

00:00:48,330 --> 00:00:52,410
want to follow me and get news on did

00:00:50,970 --> 00:00:56,460
you placate your projects and my program

00:00:52,410 --> 00:00:59,400
satisfied this is those whose lives are

00:00:56,460 --> 00:01:02,130
just a notebook so you can check them at

00:00:59,400 --> 00:01:04,530
this github repo and you can also

00:01:02,130 --> 00:01:07,830
reproduce the results here on your own

00:01:04,530 --> 00:01:10,590
machine if you mark and what's the

00:01:07,830 --> 00:01:12,869
motivation behind record deduplication

00:01:10,590 --> 00:01:15,150
it's that real work data is a mess

00:01:12,869 --> 00:01:18,510
probably dealt with data like this

00:01:15,150 --> 00:01:21,450
before we have here restaurant names and

00:01:18,510 --> 00:01:24,390
there is clearly duplicates on this data

00:01:21,450 --> 00:01:27,510
as humans we can see that quite easily

00:01:24,390 --> 00:01:28,500
but it's not that simple for computer to

00:01:27,510 --> 00:01:30,720
figure it yourself

00:01:28,500 --> 00:01:35,070
since there are variations on the

00:01:30,720 --> 00:01:37,590
addresses and names the solution is to

00:01:35,070 --> 00:01:40,040
perform deduplication also known as

00:01:37,590 --> 00:01:42,750
record linkage and entity resolution

00:01:40,040 --> 00:01:45,810
deduplication works by joining records

00:01:42,750 --> 00:01:48,510
in a fusee way using data line names

00:01:45,810 --> 00:01:52,260
either says data that doesn't uniquely

00:01:48,510 --> 00:01:54,510
identify people or companies but by

00:01:52,260 --> 00:01:58,470
joining that in a future way we can

00:01:54,510 --> 00:02:01,049
figure out how to duplicate records so

00:01:58,470 --> 00:02:04,770
let's talk a bit about fusee similarity

00:02:01,049 --> 00:02:07,229
and data cleaning you can get to the

00:02:04,770 --> 00:02:09,750
similarity of a pair of streams by using

00:02:07,229 --> 00:02:12,209
edit distance so here we are using a

00:02:09,750 --> 00:02:15,390
library called PI spring matching and

00:02:12,209 --> 00:02:17,730
where is in the lifetime our so we are

00:02:15,390 --> 00:02:18,230
computing the Edit distance between

00:02:17,730 --> 00:02:20,629
those

00:02:18,230 --> 00:02:24,950
- sweets which represent the same

00:02:20,629 --> 00:02:27,410
restaurant but 0.16 is too little

00:02:24,950 --> 00:02:29,599
similarity for very similar names the

00:02:27,410 --> 00:02:32,300
problem here is that the tokens have a

00:02:29,599 --> 00:02:34,489
different order the words of the

00:02:32,300 --> 00:02:37,220
restaurant name is the matter of

00:02:34,489 --> 00:02:40,580
tentative similarity measure we could

00:02:37,220 --> 00:02:41,060
use that ignores token order yes there

00:02:40,580 --> 00:02:43,280
is

00:02:41,060 --> 00:02:46,610
moon check alchemy is the name we have

00:02:43,280 --> 00:02:48,590
that of the same library and it uses an

00:02:46,610 --> 00:02:51,709
internal similarity function and

00:02:48,590 --> 00:02:54,920
computes that against our tokens I will

00:02:51,709 --> 00:02:57,230
talk in pairs and this way we can ignore

00:02:54,920 --> 00:03:00,890
the order of the tokens and we get a

00:02:57,230 --> 00:03:02,900
higher similarity here it worked

00:03:00,890 --> 00:03:06,230
but there's still room for improvement

00:03:02,900 --> 00:03:10,310
here how to get rid of the acute accent

00:03:06,230 --> 00:03:13,459
on cafe because we want one similarity

00:03:10,310 --> 00:03:16,040
of one on those two streets because we

00:03:13,459 --> 00:03:18,470
mean the same thing we can use a library

00:03:16,040 --> 00:03:21,319
called unity code to clean up accents

00:03:18,470 --> 00:03:23,299
and now we can apply mulch out and again

00:03:21,319 --> 00:03:25,340
and we get one the similarity we are

00:03:23,299 --> 00:03:27,650
looking for so brave

00:03:25,340 --> 00:03:29,660
maybe we can consider restaurant

00:03:27,650 --> 00:03:32,510
patterns with highly similarity as

00:03:29,660 --> 00:03:36,079
matches but it's useful to use other

00:03:32,510 --> 00:03:39,200
fields as well for addresses you can get

00:03:36,079 --> 00:03:41,600
a similarity by first geocoding them so

00:03:39,200 --> 00:03:44,329
we use the library of colder it will

00:03:41,600 --> 00:03:47,810
call the Google geo coder to geocode the

00:03:44,329 --> 00:03:50,569
other says with own to get luck to the

00:03:47,810 --> 00:03:53,120
oysters and here we get the latitude for

00:03:50,569 --> 00:03:55,639
those addresses and then we can compute

00:03:53,120 --> 00:03:57,709
the distance between those addresses we

00:03:55,639 --> 00:04:00,260
will use have a scene for that it will

00:03:57,709 --> 00:04:03,260
give us the kilometers distance between

00:04:00,260 --> 00:04:05,030
those two back to the monsters and we

00:04:03,260 --> 00:04:08,060
can consider that close enough other

00:04:05,030 --> 00:04:11,480
cells are matches so let's compare our

00:04:08,060 --> 00:04:13,760
strong pairs now using those functions

00:04:11,480 --> 00:04:17,810
for comparing names and for comparing a

00:04:13,760 --> 00:04:20,450
versus comparing out all this is our

00:04:17,810 --> 00:04:23,150
dataset restaurant dataset we'd like to

00:04:20,450 --> 00:04:25,879
the new stuff so we reached our data set

00:04:23,150 --> 00:04:29,129
have doctors emotions

00:04:25,879 --> 00:04:31,110
and we'll grab the IDS of our records

00:04:29,129 --> 00:04:33,960
we'll have six reference at least zero

00:04:31,110 --> 00:04:37,289
to five then we'll compute out possible

00:04:33,960 --> 00:04:38,789
pairs to inch we will use itertools

00:04:37,289 --> 00:04:42,180
combinations for that

00:04:38,789 --> 00:04:44,580
so in fact ours m2 and past the index

00:04:42,180 --> 00:04:48,509
I'll get all the pairs all possible

00:04:44,580 --> 00:04:50,759
pairs from this data set those are 15

00:04:48,509 --> 00:04:55,050
pairs because I have six records that

00:04:50,759 --> 00:04:55,620
produces 15 pairs checking some of those

00:04:55,050 --> 00:04:58,770
pairs

00:04:55,620 --> 00:05:02,759
yeah they look fine you are will be

00:04:58,770 --> 00:05:05,819
comparing our Gaston now we represent

00:05:02,759 --> 00:05:09,180
theirs as sticked record players there

00:05:05,819 --> 00:05:12,509
will be the reference so we convert that

00:05:09,180 --> 00:05:17,849
from a data frame to list of dicks pairs

00:05:12,509 --> 00:05:20,189
like this and we will do that because we

00:05:17,849 --> 00:05:23,129
will create scoring functions to compare

00:05:20,189 --> 00:05:24,979
the fields from each record so I have a

00:05:23,129 --> 00:05:29,490
function to compare names

00:05:24,979 --> 00:05:32,099
Pepe pair of names x and y and now we

00:05:29,490 --> 00:05:34,409
use modulo for that and I'll compare

00:05:32,099 --> 00:05:36,930
like to the hostess pairs of luck to the

00:05:34,409 --> 00:05:39,240
monsters x and y in a loose Havasu I

00:05:36,930 --> 00:05:43,110
will use those two functions and return

00:05:39,240 --> 00:05:46,919
a single result as a dip and that will

00:05:43,110 --> 00:05:50,279
be my pirate comparison function when I

00:05:46,919 --> 00:05:53,099
run that I get this this example pair

00:05:50,279 --> 00:05:56,270
has low similarity of names and distant

00:05:53,099 --> 00:05:58,889
other system so it is clearly a no match

00:05:56,270 --> 00:06:01,919
now let's call all the pairs with that

00:05:58,889 --> 00:06:06,930
function this is somebody like go to do

00:06:01,919 --> 00:06:11,009
this and that's what we get we get a

00:06:06,930 --> 00:06:13,800
data frame that has the scores for name

00:06:11,009 --> 00:06:16,889
Paris like to merge two terms so for

00:06:13,800 --> 00:06:21,000
third one instead for zero to two is

00:06:16,889 --> 00:06:23,120
that and that goes up let's filter the

00:06:21,000 --> 00:06:29,310
scarce data frame to look for matches

00:06:23,120 --> 00:06:31,229
you filter by scores above 0.5 and not

00:06:29,310 --> 00:06:33,930
two solutions less than one kilometer

00:06:31,229 --> 00:06:36,110
away from each other and we get exactly

00:06:33,930 --> 00:06:38,690
the pairs we are involved imagine fair

00:06:36,110 --> 00:06:41,270
zero one two and three four and five and

00:06:38,690 --> 00:06:44,210
that worked this group based approach

00:06:41,270 --> 00:06:48,920
here worked to figure out matches out of

00:06:44,210 --> 00:06:51,730
scores here are the matching text

00:06:48,920 --> 00:06:54,740
exactly what you're looking for

00:06:51,730 --> 00:06:57,560
first Knicks records is fine to compare

00:06:54,740 --> 00:07:00,080
all against all we get 15 terms but for

00:06:57,560 --> 00:07:03,800
one minute records for example we will

00:07:00,080 --> 00:07:06,710
get almost 500 billion pairs if we

00:07:03,800 --> 00:07:09,350
compared our in-stock that number of

00:07:06,710 --> 00:07:11,660
compares rose to fast as a number of

00:07:09,350 --> 00:07:14,720
record grows it grows quadratically with

00:07:11,660 --> 00:07:17,240
this formula here so to avoid wasting

00:07:14,720 --> 00:07:19,340
too much time comparing a lot of pairs

00:07:17,240 --> 00:07:23,380
with in blocking otherwise we'll take

00:07:19,340 --> 00:07:26,090
years to run some record education

00:07:23,380 --> 00:07:29,060
deduplication job we need to produce

00:07:26,090 --> 00:07:31,430
only pairs that are good candidates of

00:07:29,060 --> 00:07:33,950
being duplicates that's what blocking

00:07:31,430 --> 00:07:37,760
does for this only pairs that are good

00:07:33,950 --> 00:07:40,610
can it's possible duplicates let's learn

00:07:37,760 --> 00:07:42,860
about fingerprinting and blocking what

00:07:40,610 --> 00:07:45,470
would be a good way of blocking return

00:07:42,860 --> 00:07:48,260
things here are the names that we have

00:07:45,470 --> 00:07:50,420
for the single dataset we need a

00:07:48,260 --> 00:07:53,060
fingerprint function that cleans up

00:07:50,420 --> 00:07:55,730
irrelevant name variations so something

00:07:53,060 --> 00:07:59,810
like this I get the name I remove the

00:07:55,730 --> 00:08:03,290
access I lower case it I start the

00:07:59,810 --> 00:08:05,960
tokens and I remove anything that's

00:08:03,290 --> 00:08:09,710
known alphanumeric I think that's not a

00:08:05,960 --> 00:08:15,320
letter or another I remove and I get a

00:08:09,710 --> 00:08:19,040
fingerprint out of this name it here is

00:08:15,320 --> 00:08:22,240
the application of this fingerprint over

00:08:19,040 --> 00:08:26,390
all names so this looks nice because

00:08:22,240 --> 00:08:30,200
this two different restaurant names have

00:08:26,390 --> 00:08:31,190
the same fingerprint and when I grew by

00:08:30,200 --> 00:08:34,970
that fingerprint

00:08:31,190 --> 00:08:37,220
I get the pairs that I need to score and

00:08:34,970 --> 00:08:39,080
to figure out if they match or not in

00:08:37,220 --> 00:08:39,790
this case it wouldn't even need to score

00:08:39,080 --> 00:08:43,240
because those

00:08:39,790 --> 00:08:45,520
exactly the matching verbs we've blocked

00:08:43,240 --> 00:08:47,560
together different resort things with

00:08:45,520 --> 00:08:51,040
our fingerprint structure that's what

00:08:47,560 --> 00:08:53,200
blog eaters here are some examples of

00:08:51,040 --> 00:08:56,410
thing pretty functions you can use for

00:08:53,200 --> 00:08:59,380
blocking you can order the tokens people

00:08:56,410 --> 00:09:01,390
grab a common token nothing prevents you

00:08:59,380 --> 00:09:03,670
for having multiple fingerprints out of

00:09:01,390 --> 00:09:06,490
the same record that's useful as well

00:09:03,670 --> 00:09:09,730
you can get the first end tokens you can

00:09:06,490 --> 00:09:12,490
get a prefix acronym phonetic and coding

00:09:09,730 --> 00:09:14,620
it worked really well on some data sets

00:09:12,490 --> 00:09:17,610
for that tools among students you can

00:09:14,620 --> 00:09:20,230
get agreed you can get a hit geocache

00:09:17,610 --> 00:09:22,960
for numbers you could try the order of

00:09:20,230 --> 00:09:25,720
magnitude you block together numbers

00:09:22,960 --> 00:09:28,990
that have similar or the same order of

00:09:25,720 --> 00:09:30,880
magnitude and other types of data you

00:09:28,990 --> 00:09:35,200
have other kinds of fingerprinting

00:09:30,880 --> 00:09:37,390
functions but type of fingerprinting

00:09:35,200 --> 00:09:40,870
would block together approximate matches

00:09:37,390 --> 00:09:43,750
like harvester fed and hammers with all

00:09:40,870 --> 00:09:46,240
on our coffee this doesn't match

00:09:43,750 --> 00:09:48,760
perfectly and it's quite difficult to

00:09:46,240 --> 00:09:51,640
plot these together we can try

00:09:48,760 --> 00:09:53,650
approximate blocking toe Bruce mataki

00:09:51,640 --> 00:09:55,960
is a challenging area in entity

00:09:53,650 --> 00:09:57,370
resolution a good approximate blocking

00:09:55,960 --> 00:10:00,580
function is able to block together

00:09:57,370 --> 00:10:02,590
similar pairs without resorting to all

00:10:00,580 --> 00:10:05,920
to all comparisons so I don't need to

00:10:02,590 --> 00:10:08,890
compare our installed and I'm not doing

00:10:05,920 --> 00:10:11,410
that I have some other way to group

00:10:08,890 --> 00:10:15,070
together in similar things similar

00:10:11,410 --> 00:10:18,160
records here is an example I have some

00:10:15,070 --> 00:10:21,010
names with some variation between them

00:10:18,160 --> 00:10:23,730
and I'm calling here a blocker function

00:10:21,010 --> 00:10:27,400
noting a print function that is able to

00:10:23,730 --> 00:10:33,580
find exactly the right blocks out of

00:10:27,400 --> 00:10:35,980
those links and I won't get much detail

00:10:33,580 --> 00:10:38,080
here on how this works but you can check

00:10:35,980 --> 00:10:40,000
these resources please state you that

00:10:38,080 --> 00:10:42,070
Fitness blog i'll be posting experiments

00:10:40,000 --> 00:10:43,870
on a prospect blocking with approximate

00:10:42,070 --> 00:10:45,970
nearest neighbors it's what i've done

00:10:43,870 --> 00:10:46,950
with those with that function I just

00:10:45,970 --> 00:10:50,050
show you

00:10:46,950 --> 00:10:51,480
and there is also good resources about

00:10:50,050 --> 00:10:53,819
local assistive

00:10:51,480 --> 00:10:57,019
which is a kind of hashing function that

00:10:53,819 --> 00:11:00,259
optimizes for collisions on similar data

00:10:57,019 --> 00:11:02,759
by thousands of a common ration function

00:11:00,259 --> 00:11:04,740
that you don't want colleges on

00:11:02,759 --> 00:11:06,959
accommodation faction better unluckily

00:11:04,740 --> 00:11:08,790
sensitive hashing it's good that you

00:11:06,959 --> 00:11:11,610
have collisions because then you can

00:11:08,790 --> 00:11:14,220
group similar data without comparing

00:11:11,610 --> 00:11:16,009
alligator please check those other

00:11:14,220 --> 00:11:19,319
resources

00:11:16,009 --> 00:11:22,170
okay now that we can block well can we

00:11:19,319 --> 00:11:24,329
do better on the classification of

00:11:22,170 --> 00:11:27,829
matches versus no matches

00:11:24,329 --> 00:11:30,839
yes let's study a bit of gossip each

00:11:27,829 --> 00:11:33,449
remember we've just used a simple to

00:11:30,839 --> 00:11:38,129
classify our Paris as matches we simply

00:11:33,449 --> 00:11:39,930
got whatever had more than or equal 0.5

00:11:38,129 --> 00:11:41,100
name similarity and less than 1

00:11:39,930 --> 00:11:43,470
kilometer distance

00:11:41,100 --> 00:11:47,240
unlike demolish the preps in regard the

00:11:43,470 --> 00:11:50,160
right pairs out of this simple rule that

00:11:47,240 --> 00:11:52,980
can work well on say points or data sets

00:11:50,160 --> 00:11:54,839
but for complexion larger ones a machine

00:11:52,980 --> 00:11:57,990
learning classifier will probably work

00:11:54,839 --> 00:12:01,410
better so we want something like this we

00:11:57,990 --> 00:12:04,319
want two passes for dinner frame a list

00:12:01,410 --> 00:12:07,529
of tours and figure out matching pairs

00:12:04,319 --> 00:12:10,980
the classifier would figure out all of

00:12:07,529 --> 00:12:13,999
the scores the pairs of scores the

00:12:10,980 --> 00:12:17,850
matching pairs the ones that really are

00:12:13,999 --> 00:12:21,660
representa matches for the duplication

00:12:17,850 --> 00:12:23,610
work the problem is how to train the

00:12:21,660 --> 00:12:26,670
classifier it can be challenging to

00:12:23,610 --> 00:12:29,279
manually find matching pairs on a

00:12:26,670 --> 00:12:31,889
gigantic data set because the number of

00:12:29,279 --> 00:12:35,759
matching pairs tends to be much smaller

00:12:31,889 --> 00:12:38,429
than the non matching pairs we can use

00:12:35,759 --> 00:12:40,649
active learning to solve that to expel

00:12:38,429 --> 00:12:42,269
any active learning we work with a Kusum

00:12:40,649 --> 00:12:45,269
dataset based on the restaurant

00:12:42,269 --> 00:12:47,490
intercept someone dataset it has it

00:12:45,269 --> 00:12:52,019
already to one restaurant records and

00:12:47,490 --> 00:12:55,110
input a is 150 duplicates so here is the

00:12:52,019 --> 00:12:58,559
restaurant dataset harris personal names

00:12:55,110 --> 00:13:00,569
addresses CD phone type of person and a

00:12:58,559 --> 00:13:03,590
clustered column which represents the

00:13:00,569 --> 00:13:07,700
truth about this data data inside

00:13:03,590 --> 00:13:09,170
same cluster is actually actually it's

00:13:07,700 --> 00:13:16,460
the same reference of those are

00:13:09,170 --> 00:13:19,720
duplicates so we can get like the golden

00:13:16,460 --> 00:13:23,210
pairs the parts that represent the

00:13:19,720 --> 00:13:25,010
duplication of this dataset by looking

00:13:23,210 --> 00:13:27,590
at the cluster column so we'll group by

00:13:25,010 --> 00:13:30,890
the clustered column grab the indices

00:13:27,590 --> 00:13:34,010
I grab the combinations and that gives

00:13:30,890 --> 00:13:38,450
us the pair's the pole in pairs party

00:13:34,010 --> 00:13:40,850
stays 150 remove the phone and the type

00:13:38,450 --> 00:13:43,310
fuse to make things a bit more difficult

00:13:40,850 --> 00:13:45,650
that's fun it's an easy match so we'll

00:13:43,310 --> 00:13:48,820
move that type as well so we're left

00:13:45,650 --> 00:13:52,850
with name others see out of this dataset

00:13:48,820 --> 00:13:55,820
where'd you put the addresses so we'll

00:13:52,850 --> 00:13:59,000
get like two semesters for the addresses

00:13:55,820 --> 00:14:02,140
and okay grab also post off from Google

00:13:59,000 --> 00:14:06,020
because postal code is quite useful for

00:14:02,140 --> 00:14:08,420
blocking so it's really nice that we can

00:14:06,020 --> 00:14:10,880
grab it more data from Google not only

00:14:08,420 --> 00:14:14,030
lactose and longitude but we can geocode

00:14:10,880 --> 00:14:16,450
and their post so for this one you can

00:14:14,030 --> 00:14:19,340
try job holders on the Google as well

00:14:16,450 --> 00:14:23,660
there there are even free ones out there

00:14:19,340 --> 00:14:25,430
a python library County two implements

00:14:23,660 --> 00:14:27,290
active learning it has a friendly

00:14:25,430 --> 00:14:32,510
interface so let's see that it practice

00:14:27,290 --> 00:14:35,930
here is the Declaration of the fields

00:14:32,510 --> 00:14:38,840
then I'll pass to d2 so did you work

00:14:35,930 --> 00:14:41,420
over those fields nay it's a straight

00:14:38,840 --> 00:14:43,580
other words it's a straight also it's a

00:14:41,420 --> 00:14:45,920
string and not to understand is a lot to

00:14:43,580 --> 00:14:48,100
constant so I initialized it up with

00:14:45,920 --> 00:14:48,100
that

00:14:49,770 --> 00:14:57,470
and I'll trade it up by labeling

00:14:53,610 --> 00:15:01,320
examples of matches it all matches SSS

00:14:57,470 --> 00:15:03,120
so this code here that is committed

00:15:01,320 --> 00:15:06,360
because I don't want to run that right

00:15:03,120 --> 00:15:09,690
now produces this outfit e so you

00:15:06,360 --> 00:15:12,930
prepare the training and you start the

00:15:09,690 --> 00:15:18,030
consult label from the do what this does

00:15:12,930 --> 00:15:22,590
is a command-line interface where you

00:15:18,030 --> 00:15:25,650
can say if the pairs that it asks you

00:15:22,590 --> 00:15:29,130
for our matches or not so here it asked

00:15:25,650 --> 00:15:31,320
me for this pair is clipping a match of

00:15:29,130 --> 00:15:36,780
everything matches here so I say yes

00:15:31,320 --> 00:15:40,610
this other pair I say yes and then it

00:15:36,780 --> 00:15:44,510
keeps asking us for pairs and we are

00:15:40,610 --> 00:15:49,320
therefore producing the training set

00:15:44,510 --> 00:15:52,680
that's the active learning let's hear

00:15:49,320 --> 00:15:54,630
another example of an homage here up

00:15:52,680 --> 00:15:57,570
until now there were only matches but

00:15:54,630 --> 00:15:59,550
now there is this name this other name

00:15:57,570 --> 00:16:01,860
which is different but the street is the

00:15:59,550 --> 00:16:03,540
same the lab to the motion is the same

00:16:01,860 --> 00:16:07,530
but the name is different so it's not a

00:16:03,540 --> 00:16:10,560
match and then we can go on until we are

00:16:07,530 --> 00:16:13,350
tired or until we think that the pair's

00:16:10,560 --> 00:16:18,270
did you ask us really represent our data

00:16:13,350 --> 00:16:20,430
set 1 after training we can see which

00:16:18,270 --> 00:16:23,280
blocking fingerprints the deferred

00:16:20,430 --> 00:16:25,380
learned from our training input it's

00:16:23,280 --> 00:16:28,980
good to do that to check if we trained

00:16:25,380 --> 00:16:32,100
enough so when I check that I learned

00:16:28,980 --> 00:16:37,440
that did you decided that the first

00:16:32,100 --> 00:16:40,920
token of the name and the same the first

00:16:37,440 --> 00:16:45,420
7 characters of the name represent

00:16:40,920 --> 00:16:47,820
together a block a fingerprint so did

00:16:45,420 --> 00:16:52,410
you poo fingerprint of data by first

00:16:47,820 --> 00:16:54,510
token of name and the same the seven

00:16:52,410 --> 00:16:57,450
characters of the name so this is

00:16:54,510 --> 00:16:57,880
unnamed here when you see that in the

00:16:57,450 --> 00:16:59,820
second

00:16:57,880 --> 00:17:02,920
it's an end so it's a combined

00:16:59,820 --> 00:17:06,010
fingerprint that combines the first

00:17:02,920 --> 00:17:09,089
token at the same the seven characters

00:17:06,010 --> 00:17:12,850
of the name but it also has other

00:17:09,089 --> 00:17:15,329
fingerprints it son Lord that you have

00:17:12,850 --> 00:17:20,520
here so this is an end

00:17:15,329 --> 00:17:24,250
it's a figure printing or it produces

00:17:20,520 --> 00:17:26,940
pears out of this other blocky role as

00:17:24,250 --> 00:17:29,170
well there disorder feeling which is

00:17:26,940 --> 00:17:32,410
alphanumeric predicate for postal

00:17:29,170 --> 00:17:35,470
basically the exact postal in the first

00:17:32,410 --> 00:17:38,020
integral of the name of the others sorry

00:17:35,470 --> 00:17:39,550
so this kind of makes sense because it

00:17:38,020 --> 00:17:42,370
is blocking to be other things that have

00:17:39,550 --> 00:17:44,530
similar names and things that have

00:17:42,370 --> 00:17:46,870
similar addresses it even figured out

00:17:44,530 --> 00:17:49,570
that the first integral of the others is

00:17:46,870 --> 00:17:52,210
really relevant cause it's the number of

00:17:49,570 --> 00:17:53,950
the street the street number of tellers

00:17:52,210 --> 00:17:56,260
so it's quite nice because it lands

00:17:53,950 --> 00:18:00,160
something about our data you learn how

00:17:56,260 --> 00:18:02,920
to block our data and it did you select

00:18:00,160 --> 00:18:06,460
those fingerprints from these extensions

00:18:02,920 --> 00:18:08,470
of possible fingerprints it already

00:18:06,460 --> 00:18:10,240
comes built in with multiple

00:18:08,470 --> 00:18:12,850
fingerprints to try on your data it

00:18:10,240 --> 00:18:16,420
tries them out and based on your

00:18:12,850 --> 00:18:20,770
training set your training in pairs it

00:18:16,420 --> 00:18:23,140
chooses the best the best block you lose

00:18:20,770 --> 00:18:25,570
the best fingerprints that can block

00:18:23,140 --> 00:18:27,730
your data well we thought comparing a

00:18:25,570 --> 00:18:30,940
lot of pairs but covering the possible

00:18:27,730 --> 00:18:33,970
pairs as well to proceed with the

00:18:30,940 --> 00:18:37,570
deduplication we compute the block and

00:18:33,970 --> 00:18:40,360
papers so we can we call the duper dot

00:18:37,570 --> 00:18:43,750
pairs after training we call the two per

00:18:40,360 --> 00:18:45,610
dot pairs on our data and we get the

00:18:43,750 --> 00:18:50,280
block attacks those are the block

00:18:45,610 --> 00:18:50,280
impression one of them is

00:18:50,840 --> 00:18:56,749
then we classify those slotted pipes

00:18:53,950 --> 00:18:59,330
internally it looks like this we have

00:18:56,749 --> 00:19:04,399
the blocker pairs new calculates the

00:18:59,330 --> 00:19:07,070
distances between those pairs

00:19:04,399 --> 00:19:08,870
it's the singularity functions and we

00:19:07,070 --> 00:19:11,919
grab these those distance those

00:19:08,870 --> 00:19:13,970
similarities and then the duper uses a

00:19:11,919 --> 00:19:15,830
classifier like a machine learning

00:19:13,970 --> 00:19:18,919
classifier in order this year we are

00:19:15,830 --> 00:19:22,370
using a random forest classifier you can

00:19:18,919 --> 00:19:25,369
check the code if you want and we call

00:19:22,370 --> 00:19:27,919
this classifier over the scores the

00:19:25,369 --> 00:19:30,950
similarities the distances and grab the

00:19:27,919 --> 00:19:33,619
scores which are not scores for our

00:19:30,950 --> 00:19:38,299
fields but just a single score that

00:19:33,619 --> 00:19:43,580
represents if the if that pair is a

00:19:38,299 --> 00:19:45,830
match or not so to run that we call the

00:19:43,580 --> 00:19:48,440
two products car and this car result is

00:19:45,830 --> 00:19:50,749
a float between 0 and 1 then indicates

00:19:48,440 --> 00:19:55,369
how similar are the records the pair

00:19:50,749 --> 00:20:01,580
winner when I call that I get this I get

00:19:55,369 --> 00:20:05,119
a matrix with the pairs and the float

00:20:01,580 --> 00:20:05,929
that represents if that fair is a match

00:20:05,119 --> 00:20:08,450
or not

00:20:05,929 --> 00:20:13,279
so those in here are probably matches

00:20:08,450 --> 00:20:14,809
this one problem isn't damaged look

00:20:13,279 --> 00:20:17,690
there are records with very low

00:20:14,809 --> 00:20:23,389
similarity in our scartaris result like

00:20:17,690 --> 00:20:25,929
67 similarity 0.04 we needn't threshold

00:20:23,389 --> 00:20:30,259
to filter out the whole similarity pairs

00:20:25,929 --> 00:20:32,419
we set a threshold 0.5 and we filter

00:20:30,259 --> 00:20:35,179
those score pairs with our threshold and

00:20:32,419 --> 00:20:39,080
we get the trash hole pairs the matching

00:20:35,179 --> 00:20:40,940
pairs understand the threshold allows us

00:20:39,080 --> 00:20:44,389
to trade-off between precision and

00:20:40,940 --> 00:20:46,190
recall please if you don't know what

00:20:44,389 --> 00:20:48,409
procedure in common is please come here

00:20:46,190 --> 00:20:52,820
there is a good explanation on Wikipedia

00:20:48,409 --> 00:20:55,580
and that's about trying to be more or

00:20:52,820 --> 00:20:58,909
less sensitive on matching records at

00:20:55,580 --> 00:21:01,250
the risk of introducing false positives

00:20:58,909 --> 00:21:04,690
if you are more sensitive or false

00:21:01,250 --> 00:21:08,750
negatives if you are less

00:21:04,690 --> 00:21:12,400
now we will evaluate how it performed we

00:21:08,750 --> 00:21:16,250
get the threshold pair set and we shall

00:21:12,400 --> 00:21:19,010
check them against our own efforts it's

00:21:16,250 --> 00:21:21,559
quite good result for true positives

00:21:19,010 --> 00:21:26,000
things that are really matches and we

00:21:21,559 --> 00:21:28,880
found that 29 only treat false positives

00:21:26,000 --> 00:21:30,620
things that the duper fix our matches

00:21:28,880 --> 00:21:33,080
parallel not and false negatives fix

00:21:30,620 --> 00:21:36,190
things that did you doesn't think it's a

00:21:33,080 --> 00:21:40,490
match per se but is a match those things

00:21:36,190 --> 00:21:42,740
are 11 in our threshold pair skills in

00:21:40,490 --> 00:21:46,850
terms of procedure you and recall those

00:21:42,740 --> 00:21:48,740
are the results most tutorials there up

00:21:46,850 --> 00:21:50,450
there will come the problem solved at

00:21:48,740 --> 00:21:55,040
this point but there is something

00:21:50,450 --> 00:21:57,650
missing clustering the following usually

00:21:55,040 --> 00:22:00,260
happens after you class fibers we have

00:21:57,650 --> 00:22:04,549
the records a B and C and by the

00:22:00,260 --> 00:22:08,900
duplicating we find a be a sammich BC

00:22:04,549 --> 00:22:10,700
it's a match in a C is a home match and

00:22:08,900 --> 00:22:13,790
that doesn't make sense because that's a

00:22:10,700 --> 00:22:17,330
trial it should close so if it's a match

00:22:13,790 --> 00:22:19,040
vs ESM a match so agency should be a

00:22:17,330 --> 00:22:21,950
match up as well this those things

00:22:19,040 --> 00:22:24,230
should be transitive and the solution

00:22:21,950 --> 00:22:27,710
for that ambiguity here is to compute

00:22:24,230 --> 00:22:32,000
the transitive closure through cluster

00:22:27,710 --> 00:22:34,460
and the do packing cluster or records if

00:22:32,000 --> 00:22:37,040
we call partition instead of calling

00:22:34,460 --> 00:22:40,220
Paris plus car as with D if we call

00:22:37,040 --> 00:22:44,179
directly partition it'll get us the

00:22:40,220 --> 00:22:46,790
clusters from the data so those are the

00:22:44,179 --> 00:22:49,309
clusters they are not ambiguous anymore

00:22:46,790 --> 00:22:52,340
because there are they are closer not

00:22:49,309 --> 00:22:55,880
all pairs and we have the right result

00:22:52,340 --> 00:22:58,220
we want the things that the right parts

00:22:55,880 --> 00:23:00,950
that are represent the same we are world

00:22:58,220 --> 00:23:02,919
entity we duplicate the democracia we

00:23:00,950 --> 00:23:04,070
don't get it Paris but we get the

00:23:02,919 --> 00:23:07,720
clusters

00:23:04,070 --> 00:23:07,720
out of the reference

00:23:08,040 --> 00:23:14,640
and now you get the classifiers so we

00:23:11,730 --> 00:23:19,800
have to get the combinations assign each

00:23:14,640 --> 00:23:23,400
cluster to get that the closet pairs and

00:23:19,800 --> 00:23:26,760
we have populate those cluster files

00:23:23,400 --> 00:23:29,880
against the cluster ones and when we do

00:23:26,760 --> 00:23:34,380
that we see that the true positives on

00:23:29,880 --> 00:23:39,230
the cluster the closer pairs are greater

00:23:34,380 --> 00:23:42,300
then the non cluster pairs there is also

00:23:39,230 --> 00:23:44,760
one less false positive on the cluster

00:23:42,300 --> 00:23:46,890
pairs that's interesting at the

00:23:44,760 --> 00:23:50,280
precision of brow obviously is different

00:23:46,890 --> 00:23:52,290
as well because the employer fires are

00:23:50,280 --> 00:23:54,240
different from the cluster prayers don't

00:23:52,290 --> 00:23:57,090
necessary to solve the transitive

00:23:54,240 --> 00:23:59,820
closure note the consent process can

00:23:57,090 --> 00:24:03,270
create new matches and drop found

00:23:59,820 --> 00:24:05,580
matches as well so if we take the

00:24:03,270 --> 00:24:08,100
symmetric difference between those two

00:24:05,580 --> 00:24:10,530
sets we see there there are different

00:24:08,100 --> 00:24:15,900
pairs of them and let's analyze those

00:24:10,530 --> 00:24:19,200
pairs so here I have some the blue code

00:24:15,900 --> 00:24:22,380
to analyze this data and what I get here

00:24:19,200 --> 00:24:25,860
is those four records they represent the

00:24:22,380 --> 00:24:28,830
same thing the same reward entity so the

00:24:25,860 --> 00:24:33,480
truth is they all should be connected

00:24:28,830 --> 00:24:37,350
like this them cluster fashion gives us

00:24:33,480 --> 00:24:40,020
that there is a missing edge here the d2

00:24:37,350 --> 00:24:43,050
/ couldn't click this record the report

00:24:40,020 --> 00:24:46,680
this record that I said but when I

00:24:43,050 --> 00:24:47,370
cluster that I get the missing edge so

00:24:46,680 --> 00:24:51,150
that's nice

00:24:47,370 --> 00:24:53,100
we created a match that was missing due

00:24:51,150 --> 00:24:55,110
to clustering then we remove an

00:24:53,100 --> 00:24:57,840
ambiguity because that doesn't make

00:24:55,110 --> 00:24:59,970
sense if all those things match but this

00:24:57,840 --> 00:25:02,400
should match as well or you should at

00:24:59,970 --> 00:25:05,760
least remove some of those ideas to

00:25:02,400 --> 00:25:11,100
remove this ability and here is another

00:25:05,760 --> 00:25:14,420
case where the truth is nothing here

00:25:11,100 --> 00:25:14,420
should match everything is different

00:25:14,810 --> 00:25:20,610
at the confusion is because there are

00:25:18,240 --> 00:25:22,530
diseases singing but nothing should

00:25:20,610 --> 00:25:24,540
match the names are quite different so

00:25:22,530 --> 00:25:26,730
the truth is that no links between those

00:25:24,540 --> 00:25:29,490
records when I get them to assert

00:25:26,730 --> 00:25:31,800
patterns I get this so there is there

00:25:29,490 --> 00:25:34,890
are links and there is one missing here

00:25:31,800 --> 00:25:37,110
but the cluster inversion does that it

00:25:34,890 --> 00:25:40,500
removed an edge and that's nice because

00:25:37,110 --> 00:25:43,110
I removed a false positive here I think

00:25:40,500 --> 00:25:46,950
that shouldn't match but I still kept

00:25:43,110 --> 00:25:50,400
their own edge here so closer can't so

00:25:46,950 --> 00:25:51,930
ow a bwiti problem it gets off I will be

00:25:50,400 --> 00:25:55,110
with the problems but you it won't solve

00:25:51,930 --> 00:25:58,620
all false positives problems obviously

00:25:55,110 --> 00:26:02,840
because if it pleasured that but it

00:25:58,620 --> 00:26:06,660
can't be kept that but they remove that

00:26:02,840 --> 00:26:08,460
ok so we have done with the duplication

00:26:06,660 --> 00:26:10,760
job what will be the next steps before

00:26:08,460 --> 00:26:13,130
want to know more about the duplication

00:26:10,760 --> 00:26:15,750
make sure you learn about other

00:26:13,130 --> 00:26:18,300
repossessing methods indexing techniques

00:26:15,750 --> 00:26:21,030
in light functions classifiers check

00:26:18,300 --> 00:26:24,240
this talk and read the book data

00:26:21,030 --> 00:26:27,210
matching it's a really nice book that

00:26:24,240 --> 00:26:30,630
gives an overview on this anti

00:26:27,210 --> 00:26:32,820
resolution regular leadership spectrum

00:26:30,630 --> 00:26:34,440
on the victors plot I'll be posting

00:26:32,820 --> 00:26:35,460
additional contact there on a

00:26:34,440 --> 00:26:39,780
deduplication

00:26:35,460 --> 00:26:41,910
and people usually ask once we get the

00:26:39,780 --> 00:26:45,390
clusters how will we consult the data

00:26:41,910 --> 00:26:48,920
from many records it want you can check

00:26:45,390 --> 00:26:52,440
material on data solution for that and

00:26:48,920 --> 00:26:54,840
people often ask as well what if new

00:26:52,440 --> 00:26:59,070
records arrived should we merge a

00:26:54,840 --> 00:27:01,800
marriage move records from clusters to

00:26:59,070 --> 00:27:05,160
solve that check material on employment

00:27:01,800 --> 00:27:09,600
of record linkage did you support this a

00:27:05,160 --> 00:27:11,270
bit with the gazetteer in class but

00:27:09,600 --> 00:27:15,480
there are other approaches as well

00:27:11,270 --> 00:27:17,760
please chef is forth to discuss the

00:27:15,480 --> 00:27:21,810
privacy implications of record linkage

00:27:17,760 --> 00:27:24,450
there's a chapter on data matching book

00:27:21,810 --> 00:27:28,320
that talks about that there are concepts

00:27:24,450 --> 00:27:31,140
you can use to ensure privacy on record

00:27:28,320 --> 00:27:33,930
linkage differential privacy for example

00:27:31,140 --> 00:27:36,270
you can check rebecca sonic talk on

00:27:33,930 --> 00:27:40,860
privacy person for seven metals on this

00:27:36,270 --> 00:27:43,620
conference to learn more about those but

00:27:40,860 --> 00:27:46,590
even to you have ways to guarantee

00:27:43,620 --> 00:27:48,960
privacy interest Ellucian has ethical

00:27:46,590 --> 00:27:51,000
implications that may not be so cool

00:27:48,960 --> 00:27:53,520
with privacy version preserving our

00:27:51,000 --> 00:27:55,800
image now may not be ready yet

00:27:53,520 --> 00:27:58,140
to protect people's privacy from

00:27:55,800 --> 00:28:00,150
advanced it into resolution record

00:27:58,140 --> 00:28:02,490
linkage techniques so please please

00:28:00,150 --> 00:28:04,920
don't link people that don't want to be

00:28:02,490 --> 00:28:07,170
linked and don't bleep records to dnr

00:28:04,920 --> 00:28:09,630
nice people the reason I use restaurants

00:28:07,170 --> 00:28:12,140
here is because I didn't want to use

00:28:09,630 --> 00:28:14,940
people data isn't always public data

00:28:12,140 --> 00:28:21,000
those people might not want to be linked

00:28:14,940 --> 00:28:23,040
with our techniques here are the

00:28:21,000 --> 00:28:27,330
references for this talk there are the

00:28:23,040 --> 00:28:30,060
talks you can check as well and thank

00:28:27,330 --> 00:28:31,710
you very much that's my email if you

00:28:30,060 --> 00:28:33,900
have any questions about record linkage

00:28:31,710 --> 00:28:35,910
or if you have record linkage projects

00:28:33,900 --> 00:28:39,120
and interesting that to hear more about

00:28:35,910 --> 00:28:42,540
them as well here is my Twitter handle

00:28:39,120 --> 00:28:45,300
my company's website and special thanks

00:28:42,540 --> 00:28:49,350
to those folks have made a lot to

00:28:45,300 --> 00:28:51,510
prepare this presentation and to get get

00:28:49,350 --> 00:28:53,910
it integrated states that I think it is

00:28:51,510 --> 00:28:55,770
right now so please feel free to reach

00:28:53,910 --> 00:28:58,160
me if you have any questions thank you

00:28:55,770 --> 00:28:58,160

YouTube URL: https://www.youtube.com/watch?v=eMI8lwQl3Dc


