Title: Talk: Itamar Turner-Trauring - Small Big Data: using NumPy and Pandas when your data doesn't fit ...
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Itamar Turner-Trauring

Your data is too big to fit in memory—loading it crashes your program—but it’s also too small for a complex Big Data cluster. How to process your data simply and quickly?

In this talk you’ll learn the basic techniques for dealing with Small Big Data: money, compression, batching, and indexing. You’ll specifically learn how to apply these techniques to NumPy and Pandas, but you’ll also learn the key concepts you can apply to other libraries and the specifics of your particular data.
Captions: 
	00:00:05,640 --> 00:00:10,360
hi my name is Itamar trinitramine and

00:00:08,770 --> 00:00:12,160
today i'll be talking about small big

00:00:10,360 --> 00:00:16,090
data what to do when your data doesn't

00:00:12,160 --> 00:00:17,380
fit in memory before we begin I'd like

00:00:16,090 --> 00:00:19,300
to talk about the most important

00:00:17,380 --> 00:00:21,369
question any software project whether or

00:00:19,300 --> 00:00:22,900
not it should be written at all for

00:00:21,369 --> 00:00:25,330
kotas hurting people if your code is

00:00:22,900 --> 00:00:26,859
hurting the environment and the better

00:00:25,330 --> 00:00:28,269
you are the more effective you are the

00:00:26,859 --> 00:00:30,220
faster your code the more efficient it

00:00:28,269 --> 00:00:31,900
is the more damage you're causing this

00:00:30,220 --> 00:00:33,850
before you start your projects before

00:00:31,900 --> 00:00:35,290
you start writing any code make sure

00:00:33,850 --> 00:00:36,969
this is actually code that deserves to

00:00:35,290 --> 00:00:38,590
be written for the rest of my talk

00:00:36,969 --> 00:00:42,550
I'm gonna be assuming that you've

00:00:38,590 --> 00:00:44,140
already made that decision so the

00:00:42,550 --> 00:00:45,699
problem when we talking about the day is

00:00:44,140 --> 00:00:48,550
what to do when you have too much data

00:00:45,699 --> 00:00:51,520
you have let's say a CSV with some data

00:00:48,550 --> 00:00:52,930
maybe no rain and you try it out with

00:00:51,520 --> 00:00:55,059
some small amount of data and everything

00:00:52,930 --> 00:00:57,730
works fine but then when you load the

00:00:55,059 --> 00:01:00,399
data with real data it's 2 gigabytes

00:00:57,730 --> 00:01:02,590
tanga bytes 200 a bytes your program

00:01:00,399 --> 00:01:04,989
crashes and the problem is if you only

00:01:02,590 --> 00:01:08,050
have 60 megabytes of RAM you can't load

00:01:04,989 --> 00:01:11,740
100 gigabytes of data into Ram that's a

00:01:08,050 --> 00:01:14,790
problem that we'd like to solve and so

00:01:11,740 --> 00:01:16,659
one solution is a big data cluster

00:01:14,790 --> 00:01:19,240
you're going to need a cluster of

00:01:16,659 --> 00:01:20,650
computers you're gonna have to set it up

00:01:19,240 --> 00:01:22,360
somehow you're gonna have to quite

00:01:20,650 --> 00:01:23,650
possibly learn a new API and then you're

00:01:22,360 --> 00:01:26,829
gonna have to rewrite all of your code

00:01:23,650 --> 00:01:28,689
and this is an exaggeration I'm being

00:01:26,829 --> 00:01:31,119
unfair you can get clouds of cluster

00:01:28,689 --> 00:01:34,030
clusters of computers in the cloud and

00:01:31,119 --> 00:01:36,430
so on but the transition from your

00:01:34,030 --> 00:01:40,689
normal code running on your computer to

00:01:36,430 --> 00:01:42,399
a large cluster is somewhat jarring its

00:01:40,689 --> 00:01:45,610
requires work at cries effort and

00:01:42,399 --> 00:01:48,369
ideally we would like to avoid it and

00:01:45,610 --> 00:01:50,710
then particularly I'm gonna be focusing

00:01:48,369 --> 00:01:53,320
on some assumptions that you have a

00:01:50,710 --> 00:01:54,759
single computer with minimal setup and

00:01:53,320 --> 00:01:57,100
ideally you're using your existing API

00:01:54,759 --> 00:01:58,570
existing code basically you're just

00:01:57,100 --> 00:02:01,149
using your normal computer to process

00:01:58,570 --> 00:02:03,430
lots of data on the term for this is

00:02:01,149 --> 00:02:06,850
small Big Data which is due to Alex

00:02:03,430 --> 00:02:08,470
Vause Ilya live of and John Lewis and

00:02:06,850 --> 00:02:10,030
they have a small Big Data manifesto

00:02:08,470 --> 00:02:15,580
linked here when you read the slides

00:02:10,030 --> 00:02:17,350
that you can hear about and so before we

00:02:15,580 --> 00:02:18,490
go on to solution to this problem with

00:02:17,350 --> 00:02:20,080
having too much data you might

00:02:18,490 --> 00:02:23,110
be wondering why do we need to actually

00:02:20,080 --> 00:02:24,700
load our data into RAM we have a hard

00:02:23,110 --> 00:02:27,400
drive our data fits on the hard drive

00:02:24,700 --> 00:02:29,710
and you can read and write to the hard

00:02:27,400 --> 00:02:31,690
drive so why not just read and write the

00:02:29,710 --> 00:02:34,720
recce to disk and bypass ram all

00:02:31,690 --> 00:02:36,460
together and the reason is that your

00:02:34,720 --> 00:02:40,030
hard drive even the more modern much

00:02:36,460 --> 00:02:43,870
faster SSDs are still much much slower

00:02:40,030 --> 00:02:47,050
than your RAM so reading from SSD might

00:02:43,870 --> 00:02:49,480
take 16,000 nanoseconds reading from

00:02:47,050 --> 00:02:51,250
memory 100 nanoseconds so if you process

00:02:49,480 --> 00:02:53,890
your data on disk only and you can't do

00:02:51,250 --> 00:02:57,430
that your code will run about 160 times

00:02:53,890 --> 00:02:59,020
slower and if you're ok with that great

00:02:57,430 --> 00:03:01,770
but most of the time running our code

00:02:59,020 --> 00:03:04,720
160 times slower is not what we want so

00:03:01,770 --> 00:03:06,400
if you want your computation be fast you

00:03:04,720 --> 00:03:11,350
have to fit your data Ram you have to

00:03:06,400 --> 00:03:13,720
load some data into memory and so one

00:03:11,350 --> 00:03:16,000
solution is to spend more money you can

00:03:13,720 --> 00:03:18,130
spend more money get more RAM so you can

00:03:16,000 --> 00:03:21,280
go off and bind your computer I just

00:03:18,130 --> 00:03:22,480
randomly did some price checks prices

00:03:21,280 --> 00:03:24,760
might be different you can probably

00:03:22,480 --> 00:03:28,510
spend less money than this this is just

00:03:24,760 --> 00:03:30,640
examples but you can buy workstation 16

00:03:28,510 --> 00:03:33,250
6 cores 64 gigabytes of RAM for a

00:03:30,640 --> 00:03:34,900
thousand dollars you can also get rent a

00:03:33,250 --> 00:03:36,910
computer and get a virtual machine in

00:03:34,900 --> 00:03:39,610
the cloud and so for example you can get

00:03:36,910 --> 00:03:41,860
a machine with 64 cores and 432

00:03:39,610 --> 00:03:43,780
gigabytes of RAM for about three dollars

00:03:41,860 --> 00:03:45,070
sixty cents an hour and again you might

00:03:43,780 --> 00:03:47,380
actually be able to pay less than that

00:03:45,070 --> 00:03:52,510
and so in a lot of circumstances because

00:03:47,380 --> 00:03:54,490
your time cost money you could just

00:03:52,510 --> 00:03:56,410
throw money at the problem by buying a

00:03:54,490 --> 00:03:58,630
computer by renting a computer run your

00:03:56,410 --> 00:03:59,680
code just get more RAM everything's fine

00:03:58,630 --> 00:04:02,380
you don't have to change any of your

00:03:59,680 --> 00:04:06,550
code so it's worth keeping in mind there

00:04:02,380 --> 00:04:08,260
is does exist this simple solution the

00:04:06,550 --> 00:04:10,390
problem though is that this solution of

00:04:08,260 --> 00:04:13,810
spending money doesn't always apply for

00:04:10,390 --> 00:04:16,270
example I once at a certain job where

00:04:13,810 --> 00:04:17,650
running large batch jobs and I

00:04:16,270 --> 00:04:21,400
calculated how much money you were

00:04:17,650 --> 00:04:23,650
spending on computation costs and on

00:04:21,400 --> 00:04:25,210
renting virtual machines and the money

00:04:23,650 --> 00:04:27,370
you were going to spend on compute was

00:04:25,210 --> 00:04:29,470
going to be basically our projected

00:04:27,370 --> 00:04:31,689
revenue so we would have no money left

00:04:29,470 --> 00:04:33,369
over for anything include

00:04:31,689 --> 00:04:37,929
for example my salary which I care very

00:04:33,369 --> 00:04:39,699
much about so I needed to reduce the

00:04:37,929 --> 00:04:41,860
cost of the virtual machines we're using

00:04:39,699 --> 00:04:43,360
which means I needed to reduce the CPU

00:04:41,860 --> 00:04:46,089
said.you but I also had to reduce the

00:04:43,360 --> 00:04:48,789
memory usage by quite a lot so in many

00:04:46,089 --> 00:04:50,259
circumstances spending more money is not

00:04:48,789 --> 00:04:52,169
the solution you want to take you want

00:04:50,259 --> 00:04:54,429
to change your software to use less Ram

00:04:52,169 --> 00:04:56,589
and for the rest of my talk us I'm going

00:04:54,429 --> 00:04:58,779
to focus on and in particular I'm going

00:04:56,589 --> 00:05:01,239
to focus on three techniques compressing

00:04:58,779 --> 00:05:03,069
your data processing your data in junk

00:05:01,239 --> 00:05:04,599
so loading a little bit processing

00:05:03,069 --> 00:05:07,569
loading a bit more processing it more

00:05:04,599 --> 00:05:08,860
and finally indexing your data so you

00:05:07,569 --> 00:05:10,989
only load the data that you actually

00:05:08,860 --> 00:05:16,179
need and they don't need doesn't take up

00:05:10,989 --> 00:05:17,619
any memory so for the roasters taught

00:05:16,179 --> 00:05:19,749
them to focus on these techniques and in

00:05:17,619 --> 00:05:21,519
particular I'm going to talk about them

00:05:19,749 --> 00:05:26,259
in the context of to libraries numpy and

00:05:21,519 --> 00:05:28,569
pandas the implementation of these

00:05:26,259 --> 00:05:30,819
techniques are not exhaustive at the end

00:05:28,569 --> 00:05:32,559
of this talk I'm going to link to my

00:05:30,819 --> 00:05:34,360
website where I have articles expand on

00:05:32,559 --> 00:05:36,039
the stock I talked about some of the

00:05:34,360 --> 00:05:38,800
techniques I don't actually cover and

00:05:36,039 --> 00:05:40,749
stuff just look at a time that's worth

00:05:38,800 --> 00:05:42,759
keeping in mind that the fundamental

00:05:40,749 --> 00:05:44,829
techniques applied across the board

00:05:42,759 --> 00:05:46,119
because they come from the nature of how

00:05:44,829 --> 00:05:49,389
you process data on the nature of the

00:05:46,119 --> 00:05:52,059
hardware and just how algorithms work

00:05:49,389 --> 00:05:55,569
and so other libraries other systems

00:05:52,059 --> 00:05:57,399
will use the same techniques so even if

00:05:55,569 --> 00:06:00,879
you're not using them for pandas these

00:05:57,399 --> 00:06:02,110
techniques will be useful and finally at

00:06:00,879 --> 00:06:04,300
least I'm talking about a fairly generic

00:06:02,110 --> 00:06:06,309
if you have specific data you might be

00:06:04,300 --> 00:06:09,969
able to come up with your own custom

00:06:06,309 --> 00:06:11,769
variation so if you have a certain type

00:06:09,969 --> 00:06:13,869
of data I might compress very well and

00:06:11,769 --> 00:06:15,159
you as the person who understands the

00:06:13,869 --> 00:06:19,089
data best will be able to come up with

00:06:15,159 --> 00:06:21,909
those variations and we're going to

00:06:19,089 --> 00:06:24,959
start with compression and the idea with

00:06:21,909 --> 00:06:27,429
compression is you take some data and

00:06:24,959 --> 00:06:29,559
you represent it in a different way than

00:06:27,429 --> 00:06:33,339
you normally normally would so that it

00:06:29,559 --> 00:06:36,039
uses less memory and you can do this in

00:06:33,339 --> 00:06:37,959
a lossless way where the data that you

00:06:36,039 --> 00:06:39,909
have compressed is identical to the

00:06:37,959 --> 00:06:41,169
original data or you can do it in a

00:06:39,909 --> 00:06:42,550
lossy where where you're losing some

00:06:41,169 --> 00:06:44,709
details and presumably if you're losing

00:06:42,550 --> 00:06:45,159
some details you'll try to only lose

00:06:44,709 --> 00:06:46,869
addy

00:06:45,159 --> 00:06:47,979
they don't really matter too much and

00:06:46,869 --> 00:06:50,199
I'm really talking about lossless

00:06:47,979 --> 00:06:51,550
compression but if you go to my website

00:06:50,199 --> 00:06:54,819
there's an article talks about lossy

00:06:51,550 --> 00:06:56,139
compression and just to clarify when I

00:06:54,819 --> 00:06:58,719
talk about compression you might be

00:06:56,139 --> 00:07:01,839
thinking about like a zip file or gzip

00:06:58,719 --> 00:07:04,389
file that come is compressed on disk and

00:07:01,839 --> 00:07:06,129
the problem with a zip file is that in

00:07:04,389 --> 00:07:08,110
order to do anything with the data you

00:07:06,129 --> 00:07:09,849
have to uncompress the data and then

00:07:08,110 --> 00:07:11,199
once it's in memory it's no longer

00:07:09,849 --> 00:07:12,069
compressed so I'm not talking about

00:07:11,199 --> 00:07:14,019
things like that where you have

00:07:12,069 --> 00:07:19,899
compression M disk I'm talking about

00:07:14,019 --> 00:07:21,669
compression in memory so let's start

00:07:19,899 --> 00:07:24,059
with numpy numpy if you're not familiar

00:07:21,669 --> 00:07:26,110
with it is library lets you store

00:07:24,059 --> 00:07:27,729
multi-dimensional arrays one dimensional

00:07:26,110 --> 00:07:30,939
two dimensional three dimensional and

00:07:27,729 --> 00:07:32,619
each array has a type of data type you

00:07:30,939 --> 00:07:36,939
can start integers you can float our

00:07:32,619 --> 00:07:38,529
floats and you can when you sort integer

00:07:36,939 --> 00:07:40,839
there's different kinds of integer types

00:07:38,529 --> 00:07:43,539
you can store 16-bit unsigned integer

00:07:40,839 --> 00:07:47,559
which lets you store values between 0

00:07:43,539 --> 00:07:49,389
and 655,000 etc or you can have a 64-bit

00:07:47,559 --> 00:07:51,610
unsigned integer and then it can start

00:07:49,389 --> 00:07:56,519
data between 0 and a very large number

00:07:51,610 --> 00:07:58,899
which is 2 to power 64 minus 1 and so

00:07:56,519 --> 00:08:01,149
64-bit integers let you represent much

00:07:58,899 --> 00:08:03,069
larger numbers but as you'd expect they

00:08:01,149 --> 00:08:07,050
also use four times as much memory as a

00:08:03,069 --> 00:08:09,879
16-bit integer let's look at an example

00:08:07,050 --> 00:08:11,319
we have an array we're going to create

00:08:09,879 --> 00:08:13,389
when you create two arrays and dump I

00:08:11,319 --> 00:08:15,249
which is gonna be full of ones that's a

00:08:13,389 --> 00:08:17,979
two-dimensional array a thousand 24 by

00:08:15,249 --> 00:08:20,439
thousand 24 and the first array uses

00:08:17,979 --> 00:08:22,869
64-bit integers and the second array

00:08:20,439 --> 00:08:25,149
uses 16-bit integers and we can look at

00:08:22,869 --> 00:08:27,909
how many bytes of memory each array uses

00:08:25,149 --> 00:08:29,679
and the first one uses around 8

00:08:27,909 --> 00:08:31,719
megabytes of RAM and the second one uses

00:08:29,679 --> 00:08:34,599
2 megabytes of RAM so as you'd expect

00:08:31,719 --> 00:08:38,050
using 64-bit integer is just four times

00:08:34,599 --> 00:08:41,409
as much memory as a 16-bit integer so if

00:08:38,050 --> 00:08:42,519
your data fits in a 16-bit array if all

00:08:41,409 --> 00:08:46,240
your numbers are gonna be less than

00:08:42,519 --> 00:08:49,060
65,000 you should just use a unsigned

00:08:46,240 --> 00:08:50,319
16-bit d-type and you're just going to

00:08:49,060 --> 00:08:54,550
reduce your memory in this case by a

00:08:50,319 --> 00:08:56,740
factor of four another approach to

00:08:54,550 --> 00:08:59,370
compression which only works again on

00:08:56,740 --> 00:09:01,870
certain kinds of data is sparse arrays

00:08:59,370 --> 00:09:07,260
so imagine you have array that's mostly

00:09:01,870 --> 00:09:09,520
zeros if most of your data data is zeros

00:09:07,260 --> 00:09:11,470
storing all those zeros in memory is

00:09:09,520 --> 00:09:14,950
sort of a waste of time you can just

00:09:11,470 --> 00:09:16,570
store those values at R 0 and say if we

00:09:14,950 --> 00:09:21,970
don't have that information about this

00:09:16,570 --> 00:09:23,830
just assume at 0 and this particular

00:09:21,970 --> 00:09:26,770
library the PI data sparse library

00:09:23,830 --> 00:09:28,240
implements sparse arrays and it

00:09:26,770 --> 00:09:29,980
implements in a way that interoperates

00:09:28,240 --> 00:09:31,720
and dump IRAs it supports a subset of

00:09:29,980 --> 00:09:32,950
the same API you can multiply them by

00:09:31,720 --> 00:09:36,570
each other

00:09:32,950 --> 00:09:39,670
and sports different presentations and

00:09:36,570 --> 00:09:42,910
in this case I'm going to show you an

00:09:39,670 --> 00:09:45,520
example it's using coordinate sparse

00:09:42,910 --> 00:09:48,610
coordinates the idea is you have let's

00:09:45,520 --> 00:09:50,500
say a large array and let's say it's

00:09:48,610 --> 00:09:55,540
showing you location of stars in the sky

00:09:50,500 --> 00:09:57,520
and so in much of the sky you'll have

00:09:55,540 --> 00:10:00,550
darkness it's not a very good killer

00:09:57,520 --> 00:10:01,990
scope and in a few pluses here and there

00:10:00,550 --> 00:10:03,550
places here and there you're gonna have

00:10:01,990 --> 00:10:05,830
stars and so really you don't have to

00:10:03,550 --> 00:10:07,480
start you don't have to store the whole

00:10:05,830 --> 00:10:10,630
arrays values you only have to store

00:10:07,480 --> 00:10:13,480
those few locations where you have a

00:10:10,630 --> 00:10:16,000
star and basically you're going to be

00:10:13,480 --> 00:10:19,690
storing at this Y chord XY coordinate

00:10:16,000 --> 00:10:21,100
there is a pixel or array value the

00:10:19,690 --> 00:10:23,740
thing dependent you think of it this

00:10:21,100 --> 00:10:27,550
value let's simulate that we create a

00:10:23,740 --> 00:10:30,370
random array 1024 by 1024 and basically

00:10:27,550 --> 00:10:33,040
the random values of the team between 0

00:10:30,370 --> 00:10:35,380
and 1 and then any value below 0 point 9

00:10:33,040 --> 00:10:36,990
I'm gonna set to 0 so basically 90% of

00:10:35,380 --> 00:10:38,710
the pixels are now black and our

00:10:36,990 --> 00:10:42,339
two-dimensional array two-dimensional

00:10:38,710 --> 00:10:43,660
image then we could take our regular

00:10:42,339 --> 00:10:46,060
numpy array and create a sparse

00:10:43,660 --> 00:10:48,940
coordinate based array from it and if we

00:10:46,060 --> 00:10:50,650
compare the memory usage the sparse

00:10:48,940 --> 00:10:54,010
array uses about a third of the memory

00:10:50,650 --> 00:10:55,390
of the regular array it's not just it

00:10:54,010 --> 00:10:57,850
can't just because it has to store all

00:10:55,390 --> 00:10:59,020
those XY coordinates there is some

00:10:57,850 --> 00:11:00,490
overhead and so that's why it doesn't

00:10:59,020 --> 00:11:03,580
work for a large if you have lots and

00:11:00,490 --> 00:11:04,960
lots and lots of dots in your array as

00:11:03,580 --> 00:11:07,450
long as the number of dots is low enough

00:11:04,960 --> 00:11:09,790
the number the arrays memory usage will

00:11:07,450 --> 00:11:10,840
be low and it can interoperate pretty

00:11:09,790 --> 00:11:15,280
well with a regular Empire

00:11:10,840 --> 00:11:17,440
and have the same operations so max

00:11:15,280 --> 00:11:19,270
let's switch to pandas if you're not

00:11:17,440 --> 00:11:22,210
familiar with pandas pandas is a way to

00:11:19,270 --> 00:11:23,230
still operate and process data similar

00:11:22,210 --> 00:11:25,510
to the kind of data you have in a

00:11:23,230 --> 00:11:28,860
spreadsheet or CSV basically have

00:11:25,510 --> 00:11:31,960
columns of data and each column has a

00:11:28,860 --> 00:11:34,330
name and a call each column has a type

00:11:31,960 --> 00:11:38,140
so you might have a dress that's column

00:11:34,330 --> 00:11:41,440
it's a string and then say a GPS

00:11:38,140 --> 00:11:43,900
coordinate XY and then that caught those

00:11:41,440 --> 00:11:47,970
two columns the array type the column

00:11:43,900 --> 00:11:51,130
type will be an integer or a float and

00:11:47,970 --> 00:11:52,780
just like an umpire you can specify in

00:11:51,130 --> 00:11:56,040
detail for each column you can do the

00:11:52,780 --> 00:12:01,330
same in pandas and as builds and umpire

00:11:56,040 --> 00:12:04,360
so if we just load a CSV from this into

00:12:01,330 --> 00:12:06,250
a pandas dataframe by default it's going

00:12:04,360 --> 00:12:11,440
to load the data and guess of the guess

00:12:06,250 --> 00:12:14,230
what kind of data type it is and so for

00:12:11,440 --> 00:12:16,750
numbers if it can't figure out what kind

00:12:14,230 --> 00:12:20,050
of like if it decides it's a number it's

00:12:16,750 --> 00:12:21,910
going to use 64 bits for every entry but

00:12:20,050 --> 00:12:23,830
if we know that each of all the values

00:12:21,910 --> 00:12:26,440
in the that particular column are

00:12:23,830 --> 00:12:28,980
integers that are smaller than let's say

00:12:26,440 --> 00:12:32,010
16 thousand we can use a 16-bit integer

00:12:28,980 --> 00:12:36,010
and so instead of using a 64-bit integer

00:12:32,010 --> 00:12:38,350
64 bit of memory for each entry in the

00:12:36,010 --> 00:12:40,120
column we're using 16 bits of memory for

00:12:38,350 --> 00:12:43,000
each entry in the column so we've again

00:12:40,120 --> 00:12:45,460
cut our memory usage for this column by

00:12:43,000 --> 00:12:48,580
4 and it's storing the exact same data

00:12:45,460 --> 00:12:50,970
so it's lossless compression we haven't

00:12:48,580 --> 00:12:54,220
lost anything

00:12:50,970 --> 00:12:55,480
so that was compression and next we're

00:12:54,220 --> 00:13:00,310
going to move on to chunking and

00:12:55,480 --> 00:13:02,830
chunking is a way for you to reduce

00:13:00,310 --> 00:13:05,020
memory usage without actually loading

00:13:02,830 --> 00:13:07,300
everything into memory at once and

00:13:05,020 --> 00:13:09,190
compression we did load everything with

00:13:07,300 --> 00:13:14,020
chunking we're saying maybe we don't

00:13:09,190 --> 00:13:16,300
have to load everything and way to think

00:13:14,020 --> 00:13:17,740
about this is consider an array it has a

00:13:16,300 --> 00:13:20,230
bunch of values item you want to find

00:13:17,740 --> 00:13:22,510
the maximum so one thing we can do is

00:13:20,230 --> 00:13:23,350
just load the whole thing into memory

00:13:22,510 --> 00:13:25,060
and

00:13:23,350 --> 00:13:29,650
go through all the values and find the

00:13:25,060 --> 00:13:32,080
maximum but we can also say load the

00:13:29,650 --> 00:13:33,880
first half and get the maximum the first

00:13:32,080 --> 00:13:36,010
half load the second half get the

00:13:33,880 --> 00:13:37,600
maximum the second half and the maximum

00:13:36,010 --> 00:13:39,550
of the whole array is the maximum of

00:13:37,600 --> 00:13:41,320
those two values so we've gotten the

00:13:39,550 --> 00:13:43,810
same answer the maximum for the whole

00:13:41,320 --> 00:13:45,820
array we only had to load half the data

00:13:43,810 --> 00:13:48,370
at once we could do the same thing with

00:13:45,820 --> 00:13:50,980
quarter data each time so basically just

00:13:48,370 --> 00:13:54,690
Welo the trunks operate on the chunks

00:13:50,980 --> 00:13:54,690
and then maybe we combine the results

00:13:55,650 --> 00:14:02,170
when it comes to numpy you want some way

00:14:00,580 --> 00:14:04,750
to store the data on disk so that you

00:14:02,170 --> 00:14:07,090
can load only subsets of the data at a

00:14:04,750 --> 00:14:08,620
time you need to be able to load only

00:14:07,090 --> 00:14:12,000
one chunk of the data and not the whole

00:14:08,620 --> 00:14:15,010
array and one way to do that was as our

00:14:12,000 --> 00:14:17,650
Tsar is a way of storing data on disk

00:14:15,010 --> 00:14:20,890
and lets you set the chunk size and

00:14:17,650 --> 00:14:23,830
stores on disk and then when you load a

00:14:20,890 --> 00:14:26,020
chunk or two chunks together into memory

00:14:23,830 --> 00:14:28,360
or however many gets turned into a numpy

00:14:26,020 --> 00:14:30,550
array and then when you write that array

00:14:28,360 --> 00:14:32,350
back into the Tsar it gets written back

00:14:30,550 --> 00:14:34,210
in the appropriate chunks but you don't

00:14:32,350 --> 00:14:36,970
have to load the whole array just the

00:14:34,210 --> 00:14:41,200
process one part of it so in this

00:14:36,970 --> 00:14:43,240
example we're opening a Tsar and we're

00:14:41,200 --> 00:14:44,740
loading basically from this one

00:14:43,240 --> 00:14:46,210
dimensional array that has a million

00:14:44,740 --> 00:14:49,360
entries we're only loading a thousand

00:14:46,210 --> 00:14:50,800
items at a time and we're running a

00:14:49,360 --> 00:14:52,480
maximum when each of those chunks

00:14:50,800 --> 00:14:54,430
separately and then once we have the

00:14:52,480 --> 00:14:57,160
chunga the the maximum of all those

00:14:54,430 --> 00:14:59,770
chunks we can do the overall maximum of

00:14:57,160 --> 00:15:01,630
all those chunks that's overall maximum

00:14:59,770 --> 00:15:03,370
of the array and so we can calculate the

00:15:01,630 --> 00:15:05,590
max on the whole array without loading

00:15:03,370 --> 00:15:07,420
the whole thing and if you look at as

00:15:05,590 --> 00:15:09,040
our object that we've opened it's type

00:15:07,420 --> 00:15:10,480
is just as our object it doesn't

00:15:09,040 --> 00:15:12,880
actually load anything when you open the

00:15:10,480 --> 00:15:15,190
Tsar it's just a pointer to someplace on

00:15:12,880 --> 00:15:17,230
disk and only when you actually slice it

00:15:15,190 --> 00:15:18,970
only when you get a sub-sub it does it

00:15:17,230 --> 00:15:25,390
get loaded into memory you start using

00:15:18,970 --> 00:15:26,800
memory pandas also supports chunking we

00:15:25,390 --> 00:15:29,950
don't have to use a third party library

00:15:26,800 --> 00:15:32,380
it actually has a feature built in so

00:15:29,950 --> 00:15:34,300
for example if we want load of CSV

00:15:32,380 --> 00:15:37,070
previously we were loading the whole CSV

00:15:34,300 --> 00:15:38,510
into memory at once we can do as we can

00:15:37,070 --> 00:15:41,300
tell pandas instead of loading the whole

00:15:38,510 --> 00:15:42,980
CSV I want to load the CSV in chunks in

00:15:41,300 --> 00:15:45,950
this case we're going to load a hundred

00:15:42,980 --> 00:15:47,360
Lions a hundred records at a time when

00:15:45,950 --> 00:15:50,030
we do that instead of getting a single

00:15:47,360 --> 00:15:53,390
data frame from pandas we get iterator

00:15:50,030 --> 00:15:56,600
of data frames each data frame is a

00:15:53,390 --> 00:15:58,820
hundred lines of the CSV and so again if

00:15:56,600 --> 00:16:01,550
you want to calculate the maximum we

00:15:58,820 --> 00:16:03,460
just calculate the maximum of that chunk

00:16:01,550 --> 00:16:06,200
and then compare it to overall maximum

00:16:03,460 --> 00:16:07,970
and we have calculated the maximum of

00:16:06,200 --> 00:16:10,100
this column of the CSV without actually

00:16:07,970 --> 00:16:12,260
having to load the whole CSV into memory

00:16:10,100 --> 00:16:18,890
only loaded a hundred lines at a time

00:16:12,260 --> 00:16:19,940
with very low memory usage and the final

00:16:18,890 --> 00:16:23,030
technically we're talking about is

00:16:19,940 --> 00:16:24,940
indexing so when you think about an

00:16:23,030 --> 00:16:27,770
index think about the back of a book

00:16:24,940 --> 00:16:32,330
like you have a reference book some sort

00:16:27,770 --> 00:16:34,400
of history book and you want to know

00:16:32,330 --> 00:16:35,900
everything that happened related to

00:16:34,400 --> 00:16:37,450
bought me and everything in the book

00:16:35,900 --> 00:16:39,560
that's related to the Battle of Waterloo

00:16:37,450 --> 00:16:41,900
so you go to the back of the book and

00:16:39,560 --> 00:16:44,180
it's alphabetical so you can easily find

00:16:41,900 --> 00:16:45,890
Battle of Waterloo and it tells you

00:16:44,180 --> 00:16:49,730
batter of water louis mentioned on page

00:16:45,890 --> 00:16:50,960
200 so you flip to page 200 and you read

00:16:49,730 --> 00:16:54,350
that page and now you know everything

00:16:50,960 --> 00:16:57,200
the book has to tell you about the

00:16:54,350 --> 00:17:01,220
Battle of Waterloo and we haven't had to

00:16:57,200 --> 00:17:03,140
read the whole book we just had to read

00:17:01,220 --> 00:17:04,520
if the index which is much smaller than

00:17:03,140 --> 00:17:06,740
the book is just a summary and the

00:17:04,520 --> 00:17:08,110
actual part of the book that talks with

00:17:06,740 --> 00:17:12,949
the data we care about

00:17:08,110 --> 00:17:16,640
that's an index and an index will let us

00:17:12,949 --> 00:17:18,050
get a subset of the data and that is

00:17:16,640 --> 00:17:20,510
different and then what chunking does

00:17:18,050 --> 00:17:21,860
like you they set in some ways chunking

00:17:20,510 --> 00:17:23,810
an index a might seem a little similar

00:17:21,860 --> 00:17:25,760
because in both cases are loading part

00:17:23,810 --> 00:17:30,050
of the data the difference is that

00:17:25,760 --> 00:17:32,420
chunking is a use thing you use when you

00:17:30,050 --> 00:17:34,760
want to you need to read all the data

00:17:32,420 --> 00:17:37,460
you can't avoid this you can't avoid it

00:17:34,760 --> 00:17:38,810
you still want to load all it at once so

00:17:37,460 --> 00:17:42,890
if your question is what's the longest

00:17:38,810 --> 00:17:44,510
word in this book the only way you can

00:17:42,890 --> 00:17:46,460
know that is by reading every single

00:17:44,510 --> 00:17:48,620
page in the book you can't read only

00:17:46,460 --> 00:17:50,570
page 200 and find a longest word in the

00:17:48,620 --> 00:17:52,880
book so just find the longest

00:17:50,570 --> 00:17:55,010
long Spartan page 200 you're going to

00:17:52,880 --> 00:17:57,260
have to read the whole book but you can

00:17:55,010 --> 00:17:58,640
just load it one page at a time finding

00:17:57,260 --> 00:18:01,880
long lists where in that page and then

00:17:58,640 --> 00:18:03,350
forget about that page and that way you

00:18:01,880 --> 00:18:06,290
don't have to load the whole book into

00:18:03,350 --> 00:18:08,450
memory indexing is useful we only have

00:18:06,290 --> 00:18:10,940
to load some of the data not all but

00:18:08,450 --> 00:18:12,170
some so for example you have an

00:18:10,940 --> 00:18:13,640
accounting system and you want to know

00:18:12,170 --> 00:18:15,740
how much money did we spend in July

00:18:13,640 --> 00:18:17,450
don't care about June you don't care

00:18:15,740 --> 00:18:19,460
about May you don't care about September

00:18:17,450 --> 00:18:21,860
you only care about July so you only

00:18:19,460 --> 00:18:24,680
want to load the data specifically for

00:18:21,860 --> 00:18:27,200
July you don't have to look everything

00:18:24,680 --> 00:18:29,840
in practice you might actually end up

00:18:27,200 --> 00:18:31,070
using these two techniques together for

00:18:29,840 --> 00:18:33,650
example if your question is how much

00:18:31,070 --> 00:18:35,720
money did we spend in July it may be

00:18:33,650 --> 00:18:37,790
that the data for July also is quite

00:18:35,720 --> 00:18:39,080
large so yes you can even order all the

00:18:37,790 --> 00:18:41,150
other months but you still have a lot of

00:18:39,080 --> 00:18:42,620
data and so once you've found the data

00:18:41,150 --> 00:18:44,240
for July instead of loading all that

00:18:42,620 --> 00:18:48,740
into memory you can load it in chunks in

00:18:44,240 --> 00:18:49,940
chunks sum up the chunks and get the

00:18:48,740 --> 00:18:53,450
answer so you can actually use these new

00:18:49,940 --> 00:18:55,670
techniques together and the simplest way

00:18:53,450 --> 00:19:00,260
to do chunking sorry the simplest way to

00:18:55,670 --> 00:19:02,390
do indexing is to have a directory and

00:19:00,260 --> 00:19:04,670
have the file name for each of your data

00:19:02,390 --> 00:19:06,650
files tell you what data is in that file

00:19:04,670 --> 00:19:09,080
so in this example we have a different

00:19:06,650 --> 00:19:11,660
file for every day of it for every month

00:19:09,080 --> 00:19:14,060
of the year so we have file for January

00:19:11,660 --> 00:19:15,740
February March and so on it so if you

00:19:14,060 --> 00:19:19,100
want to load the data for July we only

00:19:15,740 --> 00:19:20,540
have to load 2019 - jewel that CSV we

00:19:19,100 --> 00:19:23,240
don't have to load the other files now

00:19:20,540 --> 00:19:28,850
the index is basically just a listing of

00:19:23,240 --> 00:19:30,890
their entries contents and so let's see

00:19:28,850 --> 00:19:32,600
how we can use indexing with pandas and

00:19:30,890 --> 00:19:34,220
we'll start with a simple piece of code

00:19:32,600 --> 00:19:38,180
that does not use indexing and then

00:19:34,220 --> 00:19:40,160
we'll modify it to add indexing it so

00:19:38,180 --> 00:19:43,100
let's say you have a CSV with a voter

00:19:40,160 --> 00:19:45,800
registration records for the people who

00:19:43,100 --> 00:19:47,060
live in your city and you want to find

00:19:45,800 --> 00:19:50,450
the voters who live in a particular

00:19:47,060 --> 00:19:52,820
street and so if you're going to do this

00:19:50,450 --> 00:19:54,440
with chunking because you have a large

00:19:52,820 --> 00:19:56,210
file doesn't fit in memory the way you

00:19:54,440 --> 00:19:58,160
do it is you'd say for every chunk

00:19:56,210 --> 00:20:01,250
although it's say that was in lines at a

00:19:58,160 --> 00:20:03,470
time for every chunk filter by the

00:20:01,250 --> 00:20:03,810
street name making sure that street name

00:20:03,470 --> 00:20:06,210
only

00:20:03,810 --> 00:20:07,890
I chose the one that we want and we'll

00:20:06,210 --> 00:20:09,600
get a bunch of small data frames for the

00:20:07,890 --> 00:20:11,250
voters for particular straights for that

00:20:09,600 --> 00:20:13,860
particular straight and we concatenate

00:20:11,250 --> 00:20:15,420
all those small data frames into one big

00:20:13,860 --> 00:20:19,320
data frame that has all the voters from

00:20:15,420 --> 00:20:23,210
that particular street and if we wanted

00:20:19,320 --> 00:20:25,530
to only do this once this would be fine

00:20:23,210 --> 00:20:26,910
but we you might want to run this

00:20:25,530 --> 00:20:28,350
function over and over again in

00:20:26,910 --> 00:20:31,170
different streets for example you're

00:20:28,350 --> 00:20:32,490
managing a canvassing effort and one day

00:20:31,170 --> 00:20:33,750
you're going to this bunch of streets

00:20:32,490 --> 00:20:35,240
and one day are going to different set

00:20:33,750 --> 00:20:37,560
of streets and so you're calling this

00:20:35,240 --> 00:20:40,530
function over and over and over again if

00:20:37,560 --> 00:20:43,140
this function is slow then that's all

00:20:40,530 --> 00:20:47,040
going to add up and that is where

00:20:43,140 --> 00:20:50,490
indexing is going to be helpful and the

00:20:47,040 --> 00:20:53,100
way we're going to index our data is by

00:20:50,490 --> 00:20:56,430
storing it in sequent sequel Lite is a

00:20:53,100 --> 00:20:59,850
sequel database relational database that

00:20:56,430 --> 00:21:01,440
is built into Python it's a library you

00:20:59,850 --> 00:21:04,950
don't have to run a server like Postgres

00:21:01,440 --> 00:21:06,720
or my sequel you just have a file on

00:21:04,950 --> 00:21:08,460
disk you load it up with this library

00:21:06,720 --> 00:21:10,200
that's built into Python read and write

00:21:08,460 --> 00:21:12,450
to it so it's really convenient to work

00:21:10,200 --> 00:21:14,910
with sequel Lite files no different than

00:21:12,450 --> 00:21:16,110
working the CSV file what we're going to

00:21:14,910 --> 00:21:18,930
do is we're going take all the data from

00:21:16,110 --> 00:21:20,820
the CSV read it in chunks and then dump

00:21:18,930 --> 00:21:23,400
it into a sequel Lite database and

00:21:20,820 --> 00:21:25,050
pandas can do this automatic for us well

00:21:23,400 --> 00:21:26,790
automatically create the sequel table

00:21:25,050 --> 00:21:29,100
don't have to worry about the columns

00:21:26,790 --> 00:21:32,550
and Comm types and as well good for us

00:21:29,100 --> 00:21:35,190
and then once you've loaded all the data

00:21:32,550 --> 00:21:36,960
into that sequel table we're going to

00:21:35,190 --> 00:21:41,130
tell the database to create an index on

00:21:36,960 --> 00:21:42,690
the street column of the voters table in

00:21:41,130 --> 00:21:45,000
sequel databases have a built in

00:21:42,690 --> 00:21:47,340
functionality for creating indexes for

00:21:45,000 --> 00:21:50,880
fast queries and we get to just reuse a

00:21:47,340 --> 00:21:52,920
functionality for our index on the

00:21:50,880 --> 00:21:55,290
street column because it's a sequel

00:21:52,920 --> 00:21:56,730
table we can have as many indexes as we

00:21:55,290 --> 00:21:59,550
want we can have an index and other

00:21:56,730 --> 00:22:01,650
columns we can now indexes that are on

00:21:59,550 --> 00:22:03,810
multiple columns we'll even be able to

00:22:01,650 --> 00:22:07,560
have indexes that span tables although

00:22:03,810 --> 00:22:09,390
that's a more complicated use guys so

00:22:07,560 --> 00:22:11,520
now we have this file a sequel I'd file

00:22:09,390 --> 00:22:13,920
on disk and it has on data and an index

00:22:11,520 --> 00:22:15,750
for fast quartz and now I can rewrite

00:22:13,920 --> 00:22:17,150
our function for querying voters for

00:22:15,750 --> 00:22:18,500
particular street

00:22:17,150 --> 00:22:20,180
but instead of learning firms from a CSV

00:22:18,500 --> 00:22:22,400
we're gonna load from the database so we

00:22:20,180 --> 00:22:24,830
open a connection to the database and we

00:22:22,400 --> 00:22:27,080
can run a query sang select all the rows

00:22:24,830 --> 00:22:29,420
from this table from the voters table

00:22:27,080 --> 00:22:32,540
where the street equals whatever values

00:22:29,420 --> 00:22:34,550
they had we've passed in and then we

00:22:32,540 --> 00:22:37,520
tell pandas to run this sequel query and

00:22:34,550 --> 00:22:39,380
turn the results into a pandas dataframe

00:22:37,520 --> 00:22:41,120
as normal so once you've called this

00:22:39,380 --> 00:22:42,740
function we don't know that the data

00:22:41,120 --> 00:22:44,660
came from a sequel database just let me

00:22:42,740 --> 00:22:48,080
don't know came from CSV we just have a

00:22:44,660 --> 00:22:49,670
normal panda's data frame because sequel

00:22:48,080 --> 00:22:52,580
Lite has an index it will be able to

00:22:49,670 --> 00:22:54,140
find those rows for that match this

00:22:52,580 --> 00:22:56,150
particular street really quickly and

00:22:54,140 --> 00:22:57,800
only load those rows it's not going to

00:22:56,150 --> 00:22:58,820
have to read all the data it's only

00:22:57,800 --> 00:23:03,860
gonna have to read the day although we

00:22:58,820 --> 00:23:05,330
care about so let's see a comparison of

00:23:03,860 --> 00:23:07,190
these two techniques the chunking

00:23:05,330 --> 00:23:08,870
technique we read it from the CSV and

00:23:07,190 --> 00:23:10,460
the indexing technique where we have

00:23:08,870 --> 00:23:11,780
super late running index I'm using the

00:23:10,460 --> 00:23:13,730
voter registration database for

00:23:11,780 --> 00:23:16,130
Cambridge there's about 70 thousand

00:23:13,730 --> 00:23:17,510
voters it's a pretty small number of

00:23:16,130 --> 00:23:20,750
voters New York City would have millions

00:23:17,510 --> 00:23:22,550
and if thing about memory usage in this

00:23:20,750 --> 00:23:24,679
vsp case we were only loading a thousand

00:23:22,550 --> 00:23:25,760
chunks at a time and then we accumulated

00:23:24,679 --> 00:23:27,950
the matching row so it really doesn't

00:23:25,760 --> 00:23:29,510
use as much memory on the sequel egg

00:23:27,950 --> 00:23:31,790
case it has to load the index into

00:23:29,510 --> 00:23:33,679
memory but again it indexes a summary so

00:23:31,790 --> 00:23:34,940
it should be pretty small and again we

00:23:33,679 --> 00:23:36,290
have the memory usage for the matching

00:23:34,940 --> 00:23:38,390
row so in both cases the memory usage

00:23:36,290 --> 00:23:41,440
used to be quite low so you've solved

00:23:38,390 --> 00:23:43,700
our underlying goal of being able to

00:23:41,440 --> 00:23:45,740
process lots of data only a little bit

00:23:43,700 --> 00:23:48,050
in memory but the main difference

00:23:45,740 --> 00:23:51,260
between them is execution time loading

00:23:48,050 --> 00:23:53,840
the CSV took 574 milliseconds because we

00:23:51,260 --> 00:23:57,770
had to load every single row from the

00:23:53,840 --> 00:24:00,440
CSV and parse it and then match it

00:23:57,770 --> 00:24:02,480
against our street filter the sequel

00:24:00,440 --> 00:24:05,000
Lite we only had to load those records

00:24:02,480 --> 00:24:07,730
we need it because we had an index at at

00:24:05,000 --> 00:24:11,000
that table and so it ran 50 times as

00:24:07,730 --> 00:24:16,010
fast again while still having very low

00:24:11,000 --> 00:24:17,150
memory usage so that was our last

00:24:16,010 --> 00:24:18,530
technique indexing

00:24:17,150 --> 00:24:21,590
so what if you're using some other

00:24:18,530 --> 00:24:23,630
library not pandas on Empire you have

00:24:21,590 --> 00:24:28,790
the same issues of too much data and not

00:24:23,630 --> 00:24:30,600
enough memory basically the same root

00:24:28,790 --> 00:24:33,059
causes are the problem memory

00:24:30,600 --> 00:24:34,620
fast and it's also expensive and just

00:24:33,059 --> 00:24:37,679
search if it's slow and so the same

00:24:34,620 --> 00:24:40,919
basic solutions compression chunking and

00:24:37,679 --> 00:24:43,020
innocent indexing well will be available

00:24:40,919 --> 00:24:44,910
to you in fact if you think about sequel

00:24:43,020 --> 00:24:47,160
light Seco light doesn't have to work

00:24:44,910 --> 00:24:49,110
with pandas at all any database doesn't

00:24:47,160 --> 00:24:51,299
have to work with pandas paint databases

00:24:49,110 --> 00:24:53,070
solve this problem all the time and they

00:24:51,299 --> 00:24:56,160
saw that in a similar ways with

00:24:53,070 --> 00:24:57,360
compression chunking and indexing and so

00:24:56,160 --> 00:24:59,490
now you know about these techniques you

00:24:57,360 --> 00:25:01,110
can just read the documentation whatever

00:24:59,490 --> 00:25:03,120
tool you're using and just look for

00:25:01,110 --> 00:25:08,159
those techniques and how to apply them

00:25:03,120 --> 00:25:10,260
in your particular circumstances while

00:25:08,159 --> 00:25:12,030
the bulk of is thought focused on those

00:25:10,260 --> 00:25:13,200
three software techniques don't forget

00:25:12,030 --> 00:25:18,780
that you have the option of spending

00:25:13,200 --> 00:25:20,190
more money to just get more RAM or rent

00:25:18,780 --> 00:25:22,110
a computer with more RAM get a virtual

00:25:20,190 --> 00:25:26,030
machine and sometimes that will be the

00:25:22,110 --> 00:25:29,370
cheapest cheapest easiest thing to do

00:25:26,030 --> 00:25:31,950
finally if you want to get the slides

00:25:29,370 --> 00:25:33,929
for the stock here's the URL and I have

00:25:31,950 --> 00:25:35,909
a prose articles with much of the

00:25:33,929 --> 00:25:39,840
content of this talk and even more

00:25:35,909 --> 00:25:42,320
techniques and strategies on my website

00:25:39,840 --> 00:25:46,250
at Python speed comma slash data science

00:25:42,320 --> 00:25:49,140
here's my email and Twitter handle and

00:25:46,250 --> 00:25:51,510
if you found this talk failable just you

00:25:49,140 --> 00:25:53,549
know I do training these days it's gonna

00:25:51,510 --> 00:25:54,990
be online training so if that's

00:25:53,549 --> 00:25:56,789
something that interests you if you like

00:25:54,990 --> 00:25:59,549
to upgrade your team skills then reach

00:25:56,789 --> 00:26:01,370
up and more broadly if you have any

00:25:59,549 --> 00:26:04,320
questions about the techniques I cover

00:26:01,370 --> 00:26:07,650
about how you process lots of data with

00:26:04,320 --> 00:26:09,360
limited memory please get in touch send

00:26:07,650 --> 00:26:13,070
me an email send me a tweet I'm always

00:26:09,360 --> 00:26:13,070

YouTube URL: https://www.youtube.com/watch?v=8pFnrr0NnwY


