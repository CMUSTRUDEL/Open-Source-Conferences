Title: Talk: Catherine Nelson - Practical privacy-preserving machine learning in Python
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Catherine Nelson

Machine learning is hungry for data, usually collected from users of a product and often including a lot of personal or sensitive information. What if we could build accurate machine learning models while still preserving user privacy? There’s a growing number of tools in Python to help us achieve this, ranging from federated learning, where a user’s data remains on their own device, to algorithms for training models on encrypted data. In this talk, I’ll tour the landscape of these tools and review what works, what doesn’t work, and where they fit in a machine learning pipeline.

Data privacy is a huge concern for everyone in tech these days, thanks to both legislation such as the GDPR, and user opinions driven by scandals in the media. Machine learning is at the forefront of this because it’s hungry for large amounts of training data, but it’s also an area where there’s lots of research on developing solutions that protect user privacy.

When I started learning about privacy-preserving machine learning, I found a bewildering number of research papers, introducing some really cool solutions, but very little practical advice on how to apply them in a real-world situation. This is the talk I wish I could have attended at the start of my learning journey! I’ll review the landscape of Python solutions for privacy-preserving ML and show how they fit into a machine learning pipeline. I’ll explain the tradeoffs of each method and also talk a little about the ethics of using personal data for training ML models. Tools and packages covered will include TensorFlow Privacy, TensorFlow Encrypted and PySyft.




Talk slides: https://github.com/drcat101/pycon-2020-privacy
Captions: 
	00:00:11,800 --> 00:00:16,869
so hi and welcome to my app icons or

00:00:14,770 --> 00:00:19,600
practical privacy-preserving machine

00:00:16,869 --> 00:00:21,550
learning in python i want to say thanks

00:00:19,600 --> 00:00:23,170
to all the PyCon organizers for all

00:00:21,550 --> 00:00:26,020
their hard work and clear communication

00:00:23,170 --> 00:00:27,430
this year and also thanks for the

00:00:26,020 --> 00:00:29,230
opportunity to still would give this

00:00:27,430 --> 00:00:34,480
talk even though we don't get to get

00:00:29,230 --> 00:00:36,310
together in Pittsburgh so I'd like to be

00:00:34,480 --> 00:00:37,870
I'd like to introduce myself because

00:00:36,310 --> 00:00:40,720
it's gonna be a while before we actually

00:00:37,870 --> 00:00:42,970
get to meet in person in my day job I'm

00:00:40,720 --> 00:00:45,940
a senior data scientist at concur labs

00:00:42,970 --> 00:00:48,430
part of a CP finger I work in a small

00:00:45,940 --> 00:00:50,500
team that drives innovation as a beacon

00:00:48,430 --> 00:00:52,660
co which is sa P's business travel in

00:00:50,500 --> 00:00:54,219
the expen store and we evaluate new

00:00:52,660 --> 00:00:56,920
technologies and recommend them to the

00:00:54,219 --> 00:00:59,140
rest of the company in the past we've

00:00:56,920 --> 00:01:01,510
built concur travel and expense and all

00:00:59,140 --> 00:01:04,390
sorts of alternative interfaces from

00:01:01,510 --> 00:01:07,270
slack and outlook to Alexa and even

00:01:04,390 --> 00:01:09,460
expenses from a car and now we have a

00:01:07,270 --> 00:01:11,380
big focus on machine learning and how we

00:01:09,460 --> 00:01:14,140
can recommend machine learning tools and

00:01:11,380 --> 00:01:16,270
technologies to the rest will concur I'm

00:01:14,140 --> 00:01:18,970
also a co-organized over Seattle PI

00:01:16,270 --> 00:01:20,680
ladies and I'd like to give a big shout

00:01:18,970 --> 00:01:22,830
out to this meetup for getting me

00:01:20,680 --> 00:01:26,170
started in Python in the first life and

00:01:22,830 --> 00:01:27,850
I'm also Kor co-authoring and a Riley

00:01:26,170 --> 00:01:29,500
but called building machine learning

00:01:27,850 --> 00:01:34,390
pipe pipes and I'm a Google developer

00:01:29,500 --> 00:01:35,680
expert in machine learning but today I

00:01:34,390 --> 00:01:36,790
want to talk about one thing that I'm

00:01:35,680 --> 00:01:38,760
really interested in there's

00:01:36,790 --> 00:01:41,590
privacy-preserving machine learning I

00:01:38,760 --> 00:01:43,990
first got interested in data privacy in

00:01:41,590 --> 00:01:46,570
early 2018 when everyone was thinking

00:01:43,990 --> 00:01:49,450
and talking a lot about the GDP are the

00:01:46,570 --> 00:01:50,950
general data protection regulation as

00:01:49,450 --> 00:01:53,320
these laws were being introduced

00:01:50,950 --> 00:01:56,290
but privacy issues definitely haven't

00:01:53,320 --> 00:01:57,610
gone away we're still talking about it a

00:01:56,290 --> 00:01:59,590
lot particularly in the current

00:01:57,610 --> 00:02:04,050
situation as you can see by all these

00:01:59,590 --> 00:02:06,430
headlines for the past few weeks and

00:02:04,050 --> 00:02:08,800
machine learning is deeply involved in

00:02:06,430 --> 00:02:11,650
issues around privacy as you can see

00:02:08,800 --> 00:02:14,200
from this tweet from back in 2016 from

00:02:11,650 --> 00:02:16,810
rendering the standard view is that more

00:02:14,200 --> 00:02:21,130
data is better especially when it comes

00:02:16,810 --> 00:02:22,900
to deep neural networks but let's not be

00:02:21,130 --> 00:02:25,210
so abstract about data let's be specific

00:02:22,900 --> 00:02:25,600
what data should we be worrying about

00:02:25,210 --> 00:02:28,540
when

00:02:25,600 --> 00:02:30,430
we talk about data privacy well we can

00:02:28,540 --> 00:02:32,470
divide this into two categories there's

00:02:30,430 --> 00:02:34,360
private data which is personally

00:02:32,470 --> 00:02:37,480
identifiable information such as your

00:02:34,360 --> 00:02:39,580
name address your email and then there's

00:02:37,480 --> 00:02:42,010
quasi identifying data that partially

00:02:39,580 --> 00:02:44,070
identifies someone if you bring enough

00:02:42,010 --> 00:02:47,860
of it together so things like location

00:02:44,070 --> 00:02:50,320
credit card transactions and there's

00:02:47,860 --> 00:02:52,510
also a lot of private data in user

00:02:50,320 --> 00:02:53,920
inputs if there's free text forms you'll

00:02:52,510 --> 00:02:57,270
end up with all sorts of things like

00:02:53,920 --> 00:02:59,740
names email addresses phone numbers and

00:02:57,270 --> 00:03:02,680
then their sensitive data that has some

00:02:59,740 --> 00:03:04,170
kind of consequences if it links out so

00:03:02,680 --> 00:03:07,510
this could be things like health-related

00:03:04,170 --> 00:03:11,800
personal data or company proprietary

00:03:07,510 --> 00:03:14,140
data so when we're dealing with private

00:03:11,800 --> 00:03:15,130
or sensitive data the simplest way to

00:03:14,140 --> 00:03:17,590
keep it private

00:03:15,130 --> 00:03:20,290
is to just not even collected in the

00:03:17,590 --> 00:03:21,460
first place we have a project that helps

00:03:20,290 --> 00:03:24,550
us with this it's called the data

00:03:21,460 --> 00:03:26,680
washing machine it removes personal data

00:03:24,550 --> 00:03:30,160
from natural language such as names

00:03:26,680 --> 00:03:32,620
addresses all that kind of thing we deal

00:03:30,160 --> 00:03:34,840
with a lot of receipt data and concur

00:03:32,620 --> 00:03:36,730
and that contains a lot of personal data

00:03:34,840 --> 00:03:39,370
that we don't actually need to deliver

00:03:36,730 --> 00:03:42,160
the service we provide so we use machine

00:03:39,370 --> 00:03:44,050
learning models to identify the

00:03:42,160 --> 00:03:46,720
different parts of PII that we might

00:03:44,050 --> 00:03:49,510
find in the receipt such as names

00:03:46,720 --> 00:03:54,070
addresses phone numbers and so on and

00:03:49,510 --> 00:03:56,050
then we can remove this data so that

00:03:54,070 --> 00:03:58,660
what works well for us in our situation

00:03:56,050 --> 00:04:01,120
but often there are use cases where

00:03:58,660 --> 00:04:04,150
without collecting the data we can't

00:04:01,120 --> 00:04:05,920
build the model at all and also if we

00:04:04,150 --> 00:04:09,400
don't collect sensitive data such as

00:04:05,920 --> 00:04:11,080
people's genders or their ages we don't

00:04:09,400 --> 00:04:13,540
know if a model is fair to all of our

00:04:11,080 --> 00:04:17,290
users so we do need to collect patent

00:04:13,540 --> 00:04:19,630
sensitive data and it things like

00:04:17,290 --> 00:04:22,990
privacy and machine learning are opposed

00:04:19,630 --> 00:04:25,240
because data privacy is knowing less

00:04:22,990 --> 00:04:28,780
about you but machine learning wants to

00:04:25,240 --> 00:04:30,190
know more about you but actually privacy

00:04:28,780 --> 00:04:32,920
and machine learning can have the same

00:04:30,190 --> 00:04:35,530
girls in many cases the idea is that

00:04:32,920 --> 00:04:38,470
want to learn a pape about a population

00:04:35,530 --> 00:04:39,280
not about single individual both of

00:04:38,470 --> 00:04:42,310
these technology

00:04:39,280 --> 00:04:44,470
want to generalize not personalized and

00:04:42,310 --> 00:04:46,420
my motivation with this talk is to show

00:04:44,470 --> 00:04:48,580
how it's still possible to go back here

00:04:46,420 --> 00:04:52,330
at machine learning models and also give

00:04:48,580 --> 00:04:54,220
you users privacy and there's privacy

00:04:52,330 --> 00:04:57,370
preserving machine learning technologies

00:04:54,220 --> 00:04:59,440
to help us with this but most of these

00:04:57,370 --> 00:05:03,610
appear in research papers how do I do

00:04:59,440 --> 00:05:06,310
this practically in Python to do this

00:05:03,610 --> 00:05:09,690
the most important question to ask is

00:05:06,310 --> 00:05:13,810
who do you trust with your personal data

00:05:09,690 --> 00:05:15,669
and put this into perspective I'll talk

00:05:13,810 --> 00:05:17,470
about a very simplified machine mining

00:05:15,669 --> 00:05:20,470
system with all the players who we might

00:05:17,470 --> 00:05:22,450
trust or not trust and in our simple

00:05:20,470 --> 00:05:25,150
machine learning system the data is

00:05:22,450 --> 00:05:27,990
collected from the either put into some

00:05:25,150 --> 00:05:30,280
central storage system and then it's

00:05:27,990 --> 00:05:31,960
transferred from that storage system to

00:05:30,280 --> 00:05:33,970
a machine learning model and it's used

00:05:31,960 --> 00:05:35,950
to Train that model and then the model

00:05:33,970 --> 00:05:37,600
make some predictions and these are

00:05:35,950 --> 00:05:40,960
returned to the user and shown to them

00:05:37,600 --> 00:05:42,760
and I'll discuss three different ways of

00:05:40,960 --> 00:05:47,440
providing privacy in our machine

00:05:42,760 --> 00:05:49,780
learning system first one of these is

00:05:47,440 --> 00:05:52,570
differential privacy if you want to

00:05:49,780 --> 00:05:54,669
ensure that there's no personal data in

00:05:52,570 --> 00:05:57,430
your model predictions then this is the

00:05:54,669 --> 00:05:59,380
one to use machine learning models and

00:05:57,430 --> 00:06:01,450
in particular deep learning models can

00:05:59,380 --> 00:06:04,030
expose rare examples from their training

00:06:01,450 --> 00:06:08,890
data in their predictions and this gives

00:06:04,030 --> 00:06:11,229
users the loss privacy and the

00:06:08,890 --> 00:06:14,320
definition of differential privacy is a

00:06:11,229 --> 00:06:16,720
formalization of the idea that a query

00:06:14,320 --> 00:06:19,690
should not reveal whether a person is in

00:06:16,720 --> 00:06:21,700
the dataset so the probability that a

00:06:19,690 --> 00:06:23,620
query or transformation gives the

00:06:21,700 --> 00:06:25,810
certain result is nearly the same on

00:06:23,620 --> 00:06:27,700
both these data sets one of them has the

00:06:25,810 --> 00:06:30,550
central person and the other one doesn't

00:06:27,700 --> 00:06:32,020
the idea is that if your personal data

00:06:30,550 --> 00:06:34,000
doesn't affect the result of something

00:06:32,020 --> 00:06:35,860
then you're happy to include it in the

00:06:34,000 --> 00:06:38,710
dataset because there's some kind of

00:06:35,860 --> 00:06:40,750
deniability or to put it another way

00:06:38,710 --> 00:06:42,850
this gives you the ability to say that

00:06:40,750 --> 00:06:45,280
for any possible output of the program

00:06:42,850 --> 00:06:48,820
well that was just as likely without my

00:06:45,280 --> 00:06:51,099
data as with it and the way differential

00:06:48,820 --> 00:06:52,660
privacy is achieved is by some kind of

00:06:51,099 --> 00:06:55,750
randomization that math

00:06:52,660 --> 00:06:57,640
individuals dates values there's many

00:06:55,750 --> 00:06:59,710
different ways to achieve differential

00:06:57,640 --> 00:07:01,570
privacy but I'll briefly talk through

00:06:59,710 --> 00:07:05,230
one of the simplest which is the concept

00:07:01,570 --> 00:07:08,560
of randomized response so this is

00:07:05,230 --> 00:07:10,390
commonly used in surveying questions so

00:07:08,560 --> 00:07:12,970
if you've got a question that has some

00:07:10,390 --> 00:07:15,370
kind of sensitive answer such as did you

00:07:12,970 --> 00:07:17,860
stay out late last night then you can

00:07:15,370 --> 00:07:19,990
flip a coin and if the coin comes up

00:07:17,860 --> 00:07:23,260
heads then the person taking the survey

00:07:19,990 --> 00:07:26,710
answers truthfully if it comes up tails

00:07:23,260 --> 00:07:30,370
then they flip again and select randomly

00:07:26,710 --> 00:07:33,250
from yes or no and because we know the

00:07:30,370 --> 00:07:35,470
probabilities in flipping the coin if we

00:07:33,250 --> 00:07:37,150
ask a lot of people this question we can

00:07:35,470 --> 00:07:39,760
back calculate the proportion of people

00:07:37,150 --> 00:07:42,630
who answered yes or no to this question

00:07:39,760 --> 00:07:45,490
and this gives people some deniability

00:07:42,630 --> 00:07:47,980
they could say oh it was just the I just

00:07:45,490 --> 00:07:51,550
gave the random answer and that way they

00:07:47,980 --> 00:07:53,260
don't lose privacy but what's this have

00:07:51,550 --> 00:07:56,800
to do with machine learning this is just

00:07:53,260 --> 00:07:58,780
an overall statistic well one of the

00:07:56,800 --> 00:08:02,050
ways that we can have differential

00:07:58,780 --> 00:08:03,730
privacy in machine learning is provided

00:08:02,050 --> 00:08:05,590
in the tents for their privacy library

00:08:03,730 --> 00:08:08,080
which lets us train differentially

00:08:05,590 --> 00:08:10,120
private models and it offers us a strong

00:08:08,080 --> 00:08:12,669
mathematical guarantee that prevents the

00:08:10,120 --> 00:08:14,980
users date of being memorized but it

00:08:12,669 --> 00:08:17,980
still gets us the most accurate model

00:08:14,980 --> 00:08:19,720
that we can this technology is

00:08:17,980 --> 00:08:23,820
particularly useful because it fits in

00:08:19,720 --> 00:08:27,280
really easy to our existing workflows so

00:08:23,820 --> 00:08:30,970
if we start with a classic simple tense

00:08:27,280 --> 00:08:33,280
flow Kerris model we've got three layers

00:08:30,970 --> 00:08:37,570
here two fully connected dense layers

00:08:33,280 --> 00:08:39,700
and a sigmoid output layer and we want

00:08:37,570 --> 00:08:44,169
to add privacy to add differential

00:08:39,700 --> 00:08:46,180
privacy to the simple caris model what

00:08:44,169 --> 00:08:50,080
we have to do is provide a new optimizer

00:08:46,180 --> 00:08:52,360
and a new loss so intense flow privacy

00:08:50,080 --> 00:08:56,050
there's this gradient descent Gaussian

00:08:52,360 --> 00:08:58,270
optimizer and this takes stochastic

00:08:56,050 --> 00:09:01,330
gradient descent but adds Gaussian noise

00:08:58,270 --> 00:09:03,760
to it and there's a couple of really

00:09:01,330 --> 00:09:04,980
important new parameters that go in here

00:09:03,760 --> 00:09:08,820
that aren't in the normal

00:09:04,980 --> 00:09:11,880
optimiser there's this l 2-norm clip and

00:09:08,820 --> 00:09:15,270
this noise multiplier so what this means

00:09:11,880 --> 00:09:17,760
is it clips the the gradients and they

00:09:15,270 --> 00:09:20,630
don't get too large and it varies the

00:09:17,760 --> 00:09:24,120
amount of noise that goes into the model

00:09:20,630 --> 00:09:26,400
so what's going on here well instead of

00:09:24,120 --> 00:09:28,560
the normal batches of data that we send

00:09:26,400 --> 00:09:31,680
to the model we split them into mini

00:09:28,560 --> 00:09:35,130
batches smaller batches of data and for

00:09:31,680 --> 00:09:38,280
each one of these we clip the gradients

00:09:35,130 --> 00:09:39,960
we cut off the largest values because we

00:09:38,280 --> 00:09:42,120
don't want the model to be affected by

00:09:39,960 --> 00:09:44,600
outliers because that would break the

00:09:42,120 --> 00:09:46,740
differential privacy promise of the

00:09:44,600 --> 00:09:50,430
transformation not being affected too

00:09:46,740 --> 00:09:52,020
much by one individual we then average

00:09:50,430 --> 00:09:55,320
the gradients after they're clipped and

00:09:52,020 --> 00:09:57,780
add noise so that again the gradients

00:09:55,320 --> 00:10:02,570
from the individual person are masked

00:09:57,780 --> 00:10:04,860
and then we pass these to the model and

00:10:02,570 --> 00:10:07,410
then once we've done that we can pass

00:10:04,860 --> 00:10:10,050
our new your optimizer and our new loss

00:10:07,410 --> 00:10:13,010
to the model and then compile it and fit

00:10:10,050 --> 00:10:16,260
it as normal this is just a standard

00:10:13,010 --> 00:10:20,210
Kerris training setup but we passed in

00:10:16,260 --> 00:10:22,920
that new your optimizer and loss the

00:10:20,210 --> 00:10:25,590
next question we have to ask in

00:10:22,920 --> 00:10:29,270
different privacy is how do we measure

00:10:25,590 --> 00:10:34,050
the amount of privacy that we've

00:10:29,270 --> 00:10:35,940
produced here so we have this concept

00:10:34,050 --> 00:10:38,040
called epsilon epsilon is one of the

00:10:35,940 --> 00:10:42,150
parameters in differential privacy that

00:10:38,040 --> 00:10:44,970
measures how excluding or including an

00:10:42,150 --> 00:10:46,500
individual points is going to change the

00:10:44,970 --> 00:10:49,920
probability of any particular

00:10:46,500 --> 00:10:52,110
transformation occurring so e to the

00:10:49,920 --> 00:10:54,050
power of epsilon is the maximum

00:10:52,110 --> 00:10:57,870
difference between the outcome of two

00:10:54,050 --> 00:11:01,760
transformations or to model predictions

00:10:57,870 --> 00:11:04,620
in this case and if epsilon is small

00:11:01,760 --> 00:11:06,570
we've added more noise we've clipped the

00:11:04,620 --> 00:11:08,730
gradients more then the transformation

00:11:06,570 --> 00:11:11,520
is more private and there's a smaller

00:11:08,730 --> 00:11:14,660
probability of the model changing if one

00:11:11,520 --> 00:11:17,400
person's data is included or excluded

00:11:14,660 --> 00:11:18,130
and we can calculate this using some of

00:11:17,400 --> 00:11:21,280
the built-in there

00:11:18,130 --> 00:11:25,600
it's intense flow privacy in particular

00:11:21,280 --> 00:11:27,910
we use this computes DBE STD privacy STD

00:11:25,600 --> 00:11:30,180
is stochastic gradient descent the

00:11:27,910 --> 00:11:32,830
optimizer that we used in the model and

00:11:30,180 --> 00:11:36,220
we pass in the batch size that we used

00:11:32,830 --> 00:11:38,050
and the noise multiplier we tell it the

00:11:36,220 --> 00:11:40,480
number of epochs that we train for and

00:11:38,050 --> 00:11:42,910
we give it a value Delta which is 1 over

00:11:40,480 --> 00:11:45,250
the approximate size of the data set and

00:11:42,910 --> 00:11:47,950
from this we can get a calculation of

00:11:45,250 --> 00:11:52,900
epsilon and know how private our model

00:11:47,950 --> 00:11:54,760
is differential privacy is particularly

00:11:52,900 --> 00:11:56,890
useful when you don't want to expose

00:11:54,760 --> 00:11:58,420
predictions from the model but to do

00:11:56,890 --> 00:12:00,310
this you still need to collect the data

00:11:58,420 --> 00:12:03,700
in the first place you still need users

00:12:00,310 --> 00:12:05,650
to give you access to their raw data and

00:12:03,700 --> 00:12:07,540
it's particularly useful because it

00:12:05,650 --> 00:12:10,930
provides mathematical definitions and

00:12:07,540 --> 00:12:12,940
guarantees of privacy the second piece

00:12:10,930 --> 00:12:15,010
of privacy preserving technology I'd

00:12:12,940 --> 00:12:17,500
like to talk about today is encrypted

00:12:15,010 --> 00:12:19,390
machine learning there's two ways we can

00:12:17,500 --> 00:12:22,150
use this in our machine learning system

00:12:19,390 --> 00:12:24,190
and the first one is when we encrypt the

00:12:22,150 --> 00:12:26,440
data before it gets moved to the central

00:12:24,190 --> 00:12:28,510
storage system and before the model is

00:12:26,440 --> 00:12:30,220
trained and then the predictions are

00:12:28,510 --> 00:12:33,850
decrypted and then passed back to the

00:12:30,220 --> 00:12:35,470
user and we can do this in Python with

00:12:33,850 --> 00:12:37,060
TF encrypted which handles all the

00:12:35,470 --> 00:12:39,690
encryption part for us

00:12:37,060 --> 00:12:42,910
it keeps the basic Kerris interface and

00:12:39,690 --> 00:12:45,400
it allows us to share previously unused

00:12:42,910 --> 00:12:48,160
data so data that is normally too

00:12:45,400 --> 00:12:50,020
sensitive for us to touch because it can

00:12:48,160 --> 00:12:55,410
be encrypted before it even leaves its

00:12:50,020 --> 00:12:58,090
original location to do this we just

00:12:55,410 --> 00:13:00,790
define a function that provides batches

00:12:58,090 --> 00:13:04,540
of training data and then add the data

00:13:00,790 --> 00:13:07,810
at CFE dot local computation TFE in this

00:13:04,540 --> 00:13:11,470
case is TF encrypted and this handles

00:13:07,810 --> 00:13:15,600
the encryption process for us we can

00:13:11,470 --> 00:13:18,780
then pass that data to Karis model

00:13:15,600 --> 00:13:21,280
import TF encrypted is tier V and

00:13:18,780 --> 00:13:24,670
continue as normal we build our model

00:13:21,280 --> 00:13:27,280
define it using local sequential API and

00:13:24,670 --> 00:13:29,050
our encrypted layers to it and then we

00:13:27,280 --> 00:13:32,730
can go ahead and train and make

00:13:29,050 --> 00:13:32,730
predictions on that encrypted data

00:13:32,920 --> 00:13:37,249
the second way of using encryption in

00:13:35,480 --> 00:13:38,869
our machine learning system is to

00:13:37,249 --> 00:13:41,329
encrypt a model that's already been

00:13:38,869 --> 00:13:43,790
trained so that it serves encrypted

00:13:41,329 --> 00:13:45,800
predictions and in this case we collect

00:13:43,790 --> 00:13:48,259
the data the raw data from the user and

00:13:45,800 --> 00:13:50,600
transfer that to a central storage train

00:13:48,259 --> 00:13:52,490
the model on raw data encrypt the model

00:13:50,600 --> 00:13:55,329
and then the predictions are encrypted

00:13:52,490 --> 00:13:59,329
and we can also do this in TF encrypted

00:13:55,329 --> 00:14:01,850
we can use the CFE doncaster model stock

00:13:59,329 --> 00:14:04,279
cloud model arguments and this will give

00:14:01,850 --> 00:14:07,309
us encrypted predictions there's a few

00:14:04,279 --> 00:14:09,589
more steps to it than this so what's

00:14:07,309 --> 00:14:12,319
going on here is that first of all we

00:14:09,589 --> 00:14:15,050
load and pre-process the data locally on

00:14:12,319 --> 00:14:17,389
the client then the data is encrypted on

00:14:15,050 --> 00:14:19,279
the clients the encrypted data is sent

00:14:17,389 --> 00:14:21,860
to the servers and then we make a

00:14:19,279 --> 00:14:23,839
prediction on the encrypted data send

00:14:21,860 --> 00:14:26,029
the encrypted prediction back to the

00:14:23,839 --> 00:14:27,910
clients and decrypt the prediction on

00:14:26,029 --> 00:14:30,559
the client and show the results the user

00:14:27,910 --> 00:14:31,970
but this gives you a very nice way of

00:14:30,559 --> 00:14:34,459
taking a model that you've already

00:14:31,970 --> 00:14:38,209
trained and turned it in turning it into

00:14:34,459 --> 00:14:39,439
something that's more private so when

00:14:38,209 --> 00:14:41,689
should you use encrypted machine

00:14:39,439 --> 00:14:44,360
learning there's two options encrypting

00:14:41,689 --> 00:14:46,009
the training data or just the model we

00:14:44,360 --> 00:14:48,379
can encrypt the data if the model owner

00:14:46,009 --> 00:14:50,179
is not trusted and we can encrypt the

00:14:48,379 --> 00:14:52,850
model if the training data is public

00:14:50,179 --> 00:14:55,160
that inference is private this gives us

00:14:52,850 --> 00:14:57,049
the possibility of training models on

00:14:55,160 --> 00:14:58,699
data that's too sensitive to share so

00:14:57,049 --> 00:15:02,629
there's lots of medical applications

00:14:58,699 --> 00:15:05,179
here the third technology I want to talk

00:15:02,629 --> 00:15:07,699
about today is federated learning and

00:15:05,179 --> 00:15:09,470
this is suitable when data is spread

00:15:07,699 --> 00:15:11,869
across with lots of different devices

00:15:09,470 --> 00:15:14,149
like mobile phones so this technologies

00:15:11,869 --> 00:15:17,240
used by Google in their deboard keyboard

00:15:14,149 --> 00:15:19,129
to preserve privacy in federated

00:15:17,240 --> 00:15:21,829
learning the raw data never leaves the

00:15:19,129 --> 00:15:23,959
users device and instead the model waits

00:15:21,829 --> 00:15:26,179
are sent to each device the weights are

00:15:23,959 --> 00:15:28,639
updated and then they're returned to

00:15:26,179 --> 00:15:31,009
some secure and aggregation service and

00:15:28,639 --> 00:15:32,749
then they're combined together into the

00:15:31,009 --> 00:15:36,110
model and the model can make predictions

00:15:32,749 --> 00:15:37,910
that are returned to the user and this

00:15:36,110 --> 00:15:41,499
gives users privacy because the model

00:15:37,910 --> 00:15:41,499
owner can never see their raw data

00:15:42,290 --> 00:15:46,879
in Python federated learning is provided

00:15:44,479 --> 00:15:49,609
by their paisa project and this is 4pi

00:15:46,879 --> 00:15:53,149
torture an tense flow and by tense flow

00:15:49,609 --> 00:15:54,679
federated I'm not going to give a code

00:15:53,149 --> 00:15:57,410
example here because it gets really

00:15:54,679 --> 00:15:59,149
complex with all the different bits of

00:15:57,410 --> 00:16:00,619
infrastructure that you need but I'll

00:15:59,149 --> 00:16:04,069
walk through all the steps that make a

00:16:00,619 --> 00:16:06,049
very simple federated lining setup so

00:16:04,069 --> 00:16:07,699
the first thing is to create the virtual

00:16:06,049 --> 00:16:11,899
workers that are going live on each

00:16:07,699 --> 00:16:13,609
device then we make a set of pointers

00:16:11,899 --> 00:16:15,919
that point to the training data that's

00:16:13,609 --> 00:16:17,869
already living on each worker this might

00:16:15,919 --> 00:16:21,829
be what you typed into your keyboard or

00:16:17,869 --> 00:16:23,660
something like that then we send the

00:16:21,829 --> 00:16:27,709
model weights from the model owner to

00:16:23,660 --> 00:16:32,059
each worker the model is trained on each

00:16:27,709 --> 00:16:33,350
worker device and then the workers send

00:16:32,059 --> 00:16:36,229
the weights back to the model owner

00:16:33,350 --> 00:16:37,850
after they've been updated and they also

00:16:36,229 --> 00:16:40,759
send the loss back to the model owner

00:16:37,850 --> 00:16:43,100
and then the model owner combines them

00:16:40,759 --> 00:16:46,059
all into the new model which has been

00:16:43,100 --> 00:16:48,259
improved by the addition of the new data

00:16:46,059 --> 00:16:50,179
what's missing from this very simple

00:16:48,259 --> 00:16:53,179
walkthrough is that we haven't actually

00:16:50,179 --> 00:16:55,279
provided any increase in privacy yet we

00:16:53,179 --> 00:16:58,069
can often infer the original data from

00:16:55,279 --> 00:16:59,629
the model weights so we need to add some

00:16:58,069 --> 00:17:01,429
secure way of averaging the model

00:16:59,629 --> 00:17:03,589
weights before they can reach the model

00:17:01,429 --> 00:17:07,100
owner and this is what you do in a

00:17:03,589 --> 00:17:08,629
full-blown system and also scale you

00:17:07,100 --> 00:17:11,829
need something that's going to increase

00:17:08,629 --> 00:17:13,610
in size or the many many devices

00:17:11,829 --> 00:17:14,839
federated learning is particularly

00:17:13,610 --> 00:17:16,850
useful when we want to build

00:17:14,839 --> 00:17:19,069
personalized models like in the keyboard

00:17:16,850 --> 00:17:21,049
example I mentioned already it's good

00:17:19,069 --> 00:17:24,079
when the data is already decentralized

00:17:21,049 --> 00:17:27,260
so it's been collected on users devices

00:17:24,079 --> 00:17:30,230
or on the browser and the obviously

00:17:27,260 --> 00:17:31,789
sensitive or personal data and we also

00:17:30,230 --> 00:17:33,769
need the labels to have already been

00:17:31,789 --> 00:17:39,129
added because we can't look at the data

00:17:33,769 --> 00:17:41,210
after it's being collected so

00:17:39,129 --> 00:17:42,760
privacy-preserving machine learning

00:17:41,210 --> 00:17:47,210
really comes down to who do you trust

00:17:42,760 --> 00:17:49,340
who gets to see the raw data and if you

00:17:47,210 --> 00:17:50,629
trust the model owner then they can use

00:17:49,340 --> 00:17:52,700
the raw data but off of private

00:17:50,629 --> 00:17:55,340
predictions and we can do this with

00:17:52,700 --> 00:17:57,080
differential privacy or encryption

00:17:55,340 --> 00:17:59,120
and then if you don't trust the model

00:17:57,080 --> 00:18:01,420
only with your data you can encrypt the

00:17:59,120 --> 00:18:05,060
data or keep it on each user's device

00:18:01,420 --> 00:18:06,590
and just a few caveats here right now

00:18:05,060 --> 00:18:08,300
there's always a cost to adding privacy

00:18:06,590 --> 00:18:10,610
to make your machine learning model

00:18:08,300 --> 00:18:12,230
whether that's lower accuracy longer

00:18:10,610 --> 00:18:14,780
training times or more complex

00:18:12,230 --> 00:18:16,250
infrastructure don't assume that just

00:18:14,780 --> 00:18:18,320
because you're using one of these

00:18:16,250 --> 00:18:20,870
technologies you're still providing

00:18:18,320 --> 00:18:22,850
complete privacy for your users and this

00:18:20,870 --> 00:18:25,490
isn't gonna save you from all ethical

00:18:22,850 --> 00:18:27,050
issues if the product that you're

00:18:25,490 --> 00:18:32,570
building isn't ethically sound in the

00:18:27,050 --> 00:18:33,860
first place this isn't gonna help if

00:18:32,570 --> 00:18:36,290
you'd like to learn more about the

00:18:33,860 --> 00:18:37,820
technologies I mentioned today you can

00:18:36,290 --> 00:18:39,410
read about them in the book that I'm

00:18:37,820 --> 00:18:41,360
currently co-authoring with an Apopka

00:18:39,410 --> 00:18:43,760
building machine lighting pipelines

00:18:41,360 --> 00:18:45,590
which that through a Riley I'd also

00:18:43,760 --> 00:18:47,570
encourage you to support the open source

00:18:45,590 --> 00:18:50,210
projects I've mentioned TF encrypted

00:18:47,570 --> 00:18:52,190
five lifts and tents flow of privacy and

00:18:50,210 --> 00:18:55,580
if you've got questions you can find me

00:18:52,190 --> 00:18:57,730
on Twitter thank you very much for

00:18:55,580 --> 00:18:57,730

YouTube URL: https://www.youtube.com/watch?v=NUk6QN02UxQ


