Title: Talk: Manojit Nandi - The Limitations and Danger of Facial Recognition
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
Manojit Nandi

Biometric scanners, such as face recognition technology, have seen widespread adoption in applications, such as identifying suspected criminals, analyzing candidateâ€™s facial expressions during job interviews, and monitoring attendance at schools.

As these technologies have become more pervasive, many organizations have raised potential concerns about the way these technologies schematize faces. Studies have shown commercial face recognition software has noticeably lower accuracy on darker-skinned individuals, and automatic gender recognition systems regularly misgender trans and non-binary individuals.

In addition, many scholars have written about the rise of techno-surveillance and looming threat of constant government tracking of citizens. In this talk, I will discuss these issues, and what we as technologists do to prevent building software that enables harm upon vulnerable populations.




Talk slides: https://speakerdeck.com/lejit/the-limitations-and-dangers-of-facial-recognition
Captions: 
	00:00:11,809 --> 00:00:17,539
hello my name is Manoj inaudi and this

00:00:15,349 --> 00:00:20,689
is my talk the limitations and dangers

00:00:17,539 --> 00:00:23,270
of face recognition technology now since

00:00:20,689 --> 00:00:25,520
I'll be talking about the controversy

00:00:23,270 --> 00:00:26,930
around facial recognition technology I

00:00:25,520 --> 00:00:29,540
need to have this claim in beginning

00:00:26,930 --> 00:00:31,369
that all opinions and views reflected in

00:00:29,540 --> 00:00:36,110
this talk are my own and don't reflect

00:00:31,369 --> 00:00:37,460
those my employers Edition I Kris I

00:00:36,110 --> 00:00:39,530
researched his talk they thought this

00:00:37,460 --> 00:00:41,000
topic and creating this talk out of

00:00:39,530 --> 00:00:42,379
personal interest it has nothing to do

00:00:41,000 --> 00:00:46,820
what I do for my work and it was not

00:00:42,379 --> 00:00:50,739
done on the behalf of my employer so I

00:00:46,820 --> 00:00:54,409
came up with the concept of this talk

00:00:50,739 --> 00:00:56,809
last May shortly after PyCon 2019 ended

00:00:54,409 --> 00:00:58,190
I was interested in face recognition and

00:00:56,809 --> 00:01:00,170
I was thought about giving this talk

00:00:58,190 --> 00:01:01,699
about the benefits and the potential

00:01:00,170 --> 00:01:05,600
dangers of face recognition

00:01:01,699 --> 00:01:08,229
however since May 2019 - now let's see

00:01:05,600 --> 00:01:10,729
how face recognition has been the news

00:01:08,229 --> 00:01:12,920
first we start seeing certain cities

00:01:10,729 --> 00:01:14,150
like San Francisco issuing baths and

00:01:12,920 --> 00:01:15,409
face recognition technology by

00:01:14,150 --> 00:01:19,100
government and law enforcement

00:01:15,409 --> 00:01:23,540
individuals and then other city started

00:01:19,100 --> 00:01:25,549
following suit afterwards we started

00:01:23,540 --> 00:01:27,200
having these congressional hearings

00:01:25,549 --> 00:01:29,659
about face recognition technology as

00:01:27,200 --> 00:01:34,670
potential impacts and on civil liberties

00:01:29,659 --> 00:01:36,130
and then in the international sense we

00:01:34,670 --> 00:01:39,079
started hearing about face recognition

00:01:36,130 --> 00:01:42,829
during the Hong Kong protests that the

00:01:39,079 --> 00:01:45,219
Hong Kong police were using footage

00:01:42,829 --> 00:01:47,659
captured by surveillance towers to

00:01:45,219 --> 00:01:51,829
identify protesters and arrest them with

00:01:47,659 --> 00:01:53,930
our homes and then certain countries

00:01:51,829 --> 00:01:55,909
such as France or India have started

00:01:53,930 --> 00:01:58,909
talking about using facial recognition

00:01:55,909 --> 00:02:01,369
IDs or biometric IDs for all citizens of

00:01:58,909 --> 00:02:03,950
their country and the security and

00:02:01,369 --> 00:02:06,200
privacy concerns around that and most

00:02:03,950 --> 00:02:08,030
recently with Koba 219 in the global

00:02:06,200 --> 00:02:09,649
pandemic some countries have started

00:02:08,030 --> 00:02:12,650
talking about how can we use facial

00:02:09,649 --> 00:02:14,690
recognition in order to identify

00:02:12,650 --> 00:02:18,350
individuals who have been violating

00:02:14,690 --> 00:02:21,080
shelter and home procedures or how do we

00:02:18,350 --> 00:02:23,420
track patients who tested positive for

00:02:21,080 --> 00:02:25,460
coded to make sure that they are self

00:02:23,420 --> 00:02:26,660
quarantine and not going out in public

00:02:25,460 --> 00:02:30,890
and potentially infecting other

00:02:26,660 --> 00:02:33,470
individuals and beyond just surveillance

00:02:30,890 --> 00:02:35,990
we all start seeing this rise of base

00:02:33,470 --> 00:02:39,140
analytics technology the idea that we

00:02:35,990 --> 00:02:40,850
can sort of scan your face and tell

00:02:39,140 --> 00:02:43,400
something about what you are as a person

00:02:40,850 --> 00:02:46,040
we can scan your face and say you are

00:02:43,400 --> 00:02:48,350
high IQ or you would make a good

00:02:46,040 --> 00:02:50,330
employee or on the other thing we can

00:02:48,350 --> 00:02:53,330
scan your face and say you are a

00:02:50,330 --> 00:02:55,730
criminal or a terrorist and this is

00:02:53,330 --> 00:02:58,460
based on this very archaic pseudoscience

00:02:55,730 --> 00:03:00,500
called phrenology the idea that you can

00:02:58,460 --> 00:03:01,640
the structure and shape of your brain

00:03:00,500 --> 00:03:06,650
sort of determines what kind of person

00:03:01,640 --> 00:03:09,560
you are and phrenology was like look it

00:03:06,650 --> 00:03:12,860
promoted in the 1800s and early 1900s as

00:03:09,560 --> 00:03:14,420
a way to like promote racism that the

00:03:12,860 --> 00:03:16,250
brains of African individuals was

00:03:14,420 --> 00:03:18,290
different from brains of these faces of

00:03:16,250 --> 00:03:20,390
Caucasian individuals and that's their

00:03:18,290 --> 00:03:22,430
sort of genetic determinism that if you

00:03:20,390 --> 00:03:23,900
have this type of face you're going to

00:03:22,430 --> 00:03:25,760
be a criminal or terrorist or if you

00:03:23,900 --> 00:03:28,160
have this other type of face structure

00:03:25,760 --> 00:03:31,190
you're going to be a very successful

00:03:28,160 --> 00:03:33,380
politician or businessman and this is

00:03:31,190 --> 00:03:37,130
scary because this was used to promote

00:03:33,380 --> 00:03:40,010
racism and other social forms of social

00:03:37,130 --> 00:03:41,860
impression in the past but now that we

00:03:40,010 --> 00:03:44,870
wrap it around this technological

00:03:41,860 --> 00:03:48,410
mysticism of face analytics startups are

00:03:44,870 --> 00:03:50,600
now able to package this to package this

00:03:48,410 --> 00:03:52,850
and sell it to potential customers and

00:03:50,600 --> 00:03:55,040
so now we are perpetuating racism and

00:03:52,850 --> 00:03:57,530
societal harms through the use of

00:03:55,040 --> 00:03:59,150
technology I think this is important

00:03:57,530 --> 00:04:02,150
because when we talk about the dangers

00:03:59,150 --> 00:04:05,240
of AI we're not talking about terminator

00:04:02,150 --> 00:04:07,640
or Hal 9000 we're not talking about this

00:04:05,240 --> 00:04:09,560
potential danger that AI could represent

00:04:07,640 --> 00:04:12,320
down the road of overthrowing humanity

00:04:09,560 --> 00:04:14,900
we talk we talk about the ways of

00:04:12,320 --> 00:04:16,310
algorithms and AI are harming people

00:04:14,900 --> 00:04:18,650
today and there are plenty of books

00:04:16,310 --> 00:04:21,200
written about this topic this topic of

00:04:18,650 --> 00:04:23,030
the way that algorithms are harming

00:04:21,200 --> 00:04:24,760
vulnerable vulnerable populations the

00:04:23,030 --> 00:04:31,840
way that algorithms are sort of

00:04:24,760 --> 00:04:34,550
reinforcing societal inequalities and

00:04:31,840 --> 00:04:36,230
the key thing about this is there are a

00:04:34,550 --> 00:04:38,120
lot of like here I have displayed a lot

00:04:36,230 --> 00:04:39,200
of books but this is a topic that's been

00:04:38,120 --> 00:04:41,150
studied for a while there

00:04:39,200 --> 00:04:42,590
entire field called science technology

00:04:41,150 --> 00:04:45,500
and society dedicated to the

00:04:42,590 --> 00:04:48,320
understanding the impact of technology

00:04:45,500 --> 00:04:50,810
on society as a whole and then also the

00:04:48,320 --> 00:04:52,220
other side of the loop of what societal

00:04:50,810 --> 00:04:58,730
inputs to make certain technologies

00:04:52,220 --> 00:05:00,440
inevitable and so now we talk about the

00:04:58,730 --> 00:05:02,390
ways that algorithms can protect promote

00:05:00,440 --> 00:05:04,400
bias and harm let's talk about how taste

00:05:02,390 --> 00:05:06,620
recognitions to can sort of promote bias

00:05:04,400 --> 00:05:10,880
and harm and so this is a picture of joy

00:05:06,620 --> 00:05:12,110
bullet I'm weenie joy was visiting grad

00:05:10,880 --> 00:05:13,700
schools and she was interested in

00:05:12,110 --> 00:05:15,590
studying robotics and she goes to this

00:05:13,700 --> 00:05:17,840
foreign college and she's interacting

00:05:15,590 --> 00:05:20,330
with this robot that it should scan the

00:05:17,840 --> 00:05:22,310
Fate issued use face recognition or face

00:05:20,330 --> 00:05:25,280
detection to identify the face on the

00:05:22,310 --> 00:05:27,380
individuals interacting with it and so

00:05:25,280 --> 00:05:29,060
she's trying to go up this robot and the

00:05:27,380 --> 00:05:30,920
robot couldn't pick her face detect her

00:05:29,060 --> 00:05:33,170
face and it was not able to detect her

00:05:30,920 --> 00:05:36,530
face until she put on this white mask

00:05:33,170 --> 00:05:38,300
and so now joy has really dedicated her

00:05:36,530 --> 00:05:40,700
work as a grad student at MIT Media Lab

00:05:38,300 --> 00:05:42,770
in order studying the ways that face

00:05:40,700 --> 00:05:45,470
recognitions or promotes these awkward

00:05:42,770 --> 00:05:49,040
nice types of algorithmic biases she

00:05:45,470 --> 00:05:51,830
also testified before Congress last in

00:05:49,040 --> 00:05:53,180
June I believe about the dangers of face

00:05:51,830 --> 00:05:58,190
recognition and the way it sort of

00:05:53,180 --> 00:05:59,510
enforces societal biases and so based on

00:05:58,190 --> 00:06:01,250
Joe's experience with that robot that

00:05:59,510 --> 00:06:03,590
was unable to detect objects to detect

00:06:01,250 --> 00:06:05,450
her face she set out and created this

00:06:03,590 --> 00:06:08,150
study called gender stays it was she on

00:06:05,450 --> 00:06:09,170
in multiple face analytics systems are

00:06:08,150 --> 00:06:11,240
commercially available such as

00:06:09,170 --> 00:06:13,940
Microsoft's face recognition system or

00:06:11,240 --> 00:06:15,860
IBM's face recognition system and she

00:06:13,940 --> 00:06:18,740
took an intersectional process on it

00:06:15,860 --> 00:06:20,390
she sort of evaluated these face

00:06:18,740 --> 00:06:22,820
recognition software on their

00:06:20,390 --> 00:06:26,240
performance on detecting the faces of

00:06:22,820 --> 00:06:27,890
white males white females black males

00:06:26,240 --> 00:06:29,390
and black females and I think this is a

00:06:27,890 --> 00:06:31,970
may report means the way we usually talk

00:06:29,390 --> 00:06:33,560
about fairness in algorithmically

00:06:31,970 --> 00:06:34,850
awkward experice fairness and iographer

00:06:33,560 --> 00:06:37,160
the clearness literature is served like

00:06:34,850 --> 00:06:38,450
in these different categories in

00:06:37,160 --> 00:06:40,160
isolation we need to talk about race

00:06:38,450 --> 00:06:41,570
what we talk about gender we don't talk

00:06:40,160 --> 00:06:44,330
about the intersection of race and

00:06:41,570 --> 00:06:46,940
gender and interesting what she found is

00:06:44,330 --> 00:06:47,810
that these commercially available

00:06:46,940 --> 00:06:50,570
systems

00:06:47,810 --> 00:06:52,139
miss identify darker skinned women at a

00:06:50,570 --> 00:06:54,780
much higher rate

00:06:52,139 --> 00:06:59,370
um 20 to 35% compared to the other

00:06:54,780 --> 00:07:02,370
groups comfort so 20 to 35% higher like

00:06:59,370 --> 00:07:05,430
error rate on darker skin women compared

00:07:02,370 --> 00:07:08,819
to lighter skin male and that's very

00:07:05,430 --> 00:07:10,439
alarming because if you were a data

00:07:08,819 --> 00:07:13,349
scientist and you trained a model and it

00:07:10,439 --> 00:07:14,819
had 60 70 percent accuracy you probably

00:07:13,349 --> 00:07:16,289
not be allowed on the test data you'd

00:07:14,819 --> 00:07:17,999
probably be not allowed to put in

00:07:16,289 --> 00:07:21,569
production yet these systems are

00:07:17,999 --> 00:07:24,360
commercially available and so joy and

00:07:21,569 --> 00:07:25,889
Tim it gabru polished the results of

00:07:24,360 --> 00:07:29,719
their work they publish the results of

00:07:25,889 --> 00:07:32,939
their audit and had a lot of impact so

00:07:29,719 --> 00:07:34,500
following so IBM saw the results they

00:07:32,939 --> 00:07:36,900
saw how bad that they're doing on darker

00:07:34,500 --> 00:07:39,449
skinned women and so they set out to

00:07:36,900 --> 00:07:41,879
correct this error and they come design

00:07:39,449 --> 00:07:44,729
this diversity and basis data set this

00:07:41,879 --> 00:07:47,159
data set of faces that represents people

00:07:44,729 --> 00:07:48,659
of different ethnic backgrounds and they

00:07:47,159 --> 00:07:51,360
also produces associated white paper

00:07:48,659 --> 00:07:56,810
about their commitment to AI principles

00:07:51,360 --> 00:07:59,159
for safe and fair machine learning

00:07:56,810 --> 00:08:01,110
Google also produce something similar of

00:07:59,159 --> 00:08:02,759
like they're their own set of AI

00:08:01,110 --> 00:08:05,039
principles and they set out to improve

00:08:02,759 --> 00:08:06,449
their face recognition software in order

00:08:05,039 --> 00:08:09,270
to work on darker skinned individuals as

00:08:06,449 --> 00:08:10,770
well in addition multiple federal bills

00:08:09,270 --> 00:08:13,289
such as the algorithmic accountability

00:08:10,770 --> 00:08:14,639
act or no biometrics barrier act -

00:08:13,289 --> 00:08:16,680
Housing Act so you can't use space

00:08:14,639 --> 00:08:19,649
recognitions technology in order in

00:08:16,680 --> 00:08:21,539
public housing also directly set the

00:08:19,649 --> 00:08:23,310
gender state study and some of the

00:08:21,539 --> 00:08:25,259
legislative bans passed by cities and

00:08:23,310 --> 00:08:28,020
states also represents the study

00:08:25,259 --> 00:08:32,399
and last year there was this very big

00:08:28,020 --> 00:08:35,599
legal legal battle going on in Brooklyn

00:08:32,399 --> 00:08:37,890
New York where this Atlantic Plaza tower

00:08:35,599 --> 00:08:40,079
this apartment complex was trying to

00:08:37,890 --> 00:08:42,019
install face recognition blocks on this

00:08:40,079 --> 00:08:46,019
apartment complex that was primarily

00:08:42,019 --> 00:08:47,310
elderly black women and so the tenants

00:08:46,019 --> 00:08:48,449
of this apartment complex did their

00:08:47,310 --> 00:08:51,750
research they came across the gender

00:08:48,449 --> 00:08:54,260
shades study and they realized hey face

00:08:51,750 --> 00:08:55,980
recognition technology is not good for

00:08:54,260 --> 00:08:59,160
darker skinned women

00:08:55,980 --> 00:09:01,470
Despero it won't be good for us and this

00:08:59,160 --> 00:09:02,819
there's a serious risk that these women

00:09:01,470 --> 00:09:04,500
could be locked out of their apartment

00:09:02,819 --> 00:09:07,790
because the face recognition technology

00:09:04,500 --> 00:09:07,790
won't be able to detect their face

00:09:08,270 --> 00:09:12,210
and following gender shades it sort of

00:09:11,400 --> 00:09:15,000
resinize

00:09:12,210 --> 00:09:17,250
of algorithmic audits of machine

00:09:15,000 --> 00:09:20,010
learning systems like how do we test how

00:09:17,250 --> 00:09:21,300
can we identify biases within our

00:09:20,010 --> 00:09:24,390
machine learning systems well we'll

00:09:21,300 --> 00:09:26,790
perform algorithmic audits that's good

00:09:24,390 --> 00:09:29,730
and bad because how do you get into

00:09:26,790 --> 00:09:31,500
verse data set is a very open-ended

00:09:29,730 --> 00:09:32,520
question and what some companies were

00:09:31,500 --> 00:09:33,750
doing in order to build their own

00:09:32,520 --> 00:09:36,440
diverse data set well they're very

00:09:33,750 --> 00:09:38,370
unethical so for example that IBM

00:09:36,440 --> 00:09:41,550
diversity and faceless data set was

00:09:38,370 --> 00:09:44,400
created by crawling the personal

00:09:41,550 --> 00:09:46,350
vacation photos of darker-skinned

00:09:44,400 --> 00:09:49,110
individuals from their vacation albums

00:09:46,350 --> 00:09:50,430
on Flickr Google was doing something

00:09:49,110 --> 00:09:52,260
similar where they're paying darker

00:09:50,430 --> 00:09:54,960
black and brown individuals for their

00:09:52,260 --> 00:09:56,340
selfies or they're using hiring

00:09:54,960 --> 00:09:58,350
specialists to go take pictures of

00:09:56,340 --> 00:09:59,970
homeless individuals in Atlanta which is

00:09:58,350 --> 00:10:03,480
you know super dubious and highly

00:09:59,970 --> 00:10:05,010
unethical and so Deb Raji and some of

00:10:03,480 --> 00:10:07,500
the original authors who worked on

00:10:05,010 --> 00:10:11,090
gender shades did this follow-up study

00:10:07,500 --> 00:10:13,920
of how could we do an ethnically fair

00:10:11,090 --> 00:10:15,030
facial recognition system audit and so

00:10:13,920 --> 00:10:18,210
what they did is they created this data

00:10:15,030 --> 00:10:20,990
sets lab set which is a data set using

00:10:18,210 --> 00:10:24,030
as a name implies using celebrity photos

00:10:20,990 --> 00:10:25,950
do you create a data set of white male

00:10:24,030 --> 00:10:28,260
celebrities white female celebrities

00:10:25,950 --> 00:10:30,300
black male celebrities and black female

00:10:28,260 --> 00:10:32,100
celebrities and so that you don't have

00:10:30,300 --> 00:10:34,230
this like privacy issue that we don't

00:10:32,100 --> 00:10:37,620
have to worry about collecting and

00:10:34,230 --> 00:10:39,660
crawling and over surveilling black and

00:10:37,620 --> 00:10:40,860
brown individuals and key things about

00:10:39,660 --> 00:10:42,210
this paper as I talk about some of the

00:10:40,860 --> 00:10:44,760
tensions between privacy and fairness

00:10:42,210 --> 00:10:47,010
but then they also provide researchers

00:10:44,760 --> 00:10:48,360
who want to do all the audits some

00:10:47,010 --> 00:10:50,100
considerations to think about like how

00:10:48,360 --> 00:10:51,540
do you design a good audit how do you

00:10:50,100 --> 00:10:53,130
sort of design an audit to have a

00:10:51,540 --> 00:10:54,900
meaningful impact how do you design on

00:10:53,130 --> 00:10:56,880
it this sort of encourages any target

00:10:54,900 --> 00:11:02,520
companies to then improve their software

00:10:56,880 --> 00:11:04,530
and next time we talk about effect

00:11:02,520 --> 00:11:06,630
recognition or automatic gender motion

00:11:04,530 --> 00:11:08,340
recognition and so this is the idea of

00:11:06,630 --> 00:11:09,720
like can we sort of estimate your

00:11:08,340 --> 00:11:11,280
emotional state based on pictures of

00:11:09,720 --> 00:11:13,710
your face and this is a topic that's

00:11:11,280 --> 00:11:15,180
very near and dear to my heart because

00:11:13,710 --> 00:11:17,010
when I got started as a psychology

00:11:15,180 --> 00:11:19,059
researcher I was very interested in like

00:11:17,010 --> 00:11:20,919
the integral role the

00:11:19,059 --> 00:11:24,399
you rolled at emotions play and human

00:11:20,919 --> 00:11:25,989
decision-making and this is a very fun

00:11:24,399 --> 00:11:29,499
topic when you work in tech and you have

00:11:25,989 --> 00:11:30,909
to meet individuals who on erotic Lee

00:11:29,499 --> 00:11:33,639
believes that humans would be better off

00:11:30,909 --> 00:11:36,369
without emotions despite the fact that

00:11:33,639 --> 00:11:42,429
that's the premise of multiple dystopian

00:11:36,369 --> 00:11:44,799
science fiction works so let's talk

00:11:42,429 --> 00:11:46,629
about about emotions there's this

00:11:44,799 --> 00:11:49,209
American a psychologist Paul Ekman who

00:11:46,629 --> 00:11:52,359
in the 1960s 1970s he traveled around

00:11:49,209 --> 00:11:54,189
the world and studied the emotions of

00:11:52,359 --> 00:11:55,989
people across different cultures so

00:11:54,189 --> 00:11:57,789
people in different countries but then

00:11:55,989 --> 00:12:00,699
also working with indigenous populations

00:11:57,789 --> 00:12:03,249
and what he found is that there were six

00:12:00,699 --> 00:12:05,469
emotions that were common to all people

00:12:03,249 --> 00:12:08,199
irrespective of their upbringing and

00:12:05,469 --> 00:12:12,999
these six emotions are happiness sadness

00:12:08,199 --> 00:12:15,429
fear anger surprise and disgust now if

00:12:12,999 --> 00:12:18,849
you've seen the movie Disney Pixar movie

00:12:15,429 --> 00:12:20,889
inside out finally 6 emotions sound may

00:12:18,849 --> 00:12:24,699
sound familiar to you the one missing

00:12:20,889 --> 00:12:26,529
was surprise in addition Paul Ekman also

00:12:24,699 --> 00:12:28,899
came up this idea of micro expressions

00:12:26,529 --> 00:12:32,229
that would we experience an emotion we

00:12:28,899 --> 00:12:36,279
serve involuntarily by instinct or some

00:12:32,229 --> 00:12:37,929
other similar concept but we serve just

00:12:36,279 --> 00:12:40,979
involuntary I create this short facial

00:12:37,929 --> 00:12:45,039
expression of the emotion we're feeling

00:12:40,979 --> 00:12:47,439
and so this later a command to create

00:12:45,039 --> 00:12:49,209
this facial action coding system this is

00:12:47,439 --> 00:12:50,679
sort of a schema for defining emotions

00:12:49,209 --> 00:12:53,259
based on the facial expressions and how

00:12:50,679 --> 00:12:55,179
the facial action coding system works is

00:12:53,259 --> 00:12:58,029
or breaks the face into these different

00:12:55,179 --> 00:12:59,979
facial action units and you can serve

00:12:58,029 --> 00:13:02,739
then describe in emotion as a solid the

00:12:59,979 --> 00:13:05,409
action units so for example we say anger

00:13:02,739 --> 00:13:07,449
is equal to the lips narrowing and the

00:13:05,409 --> 00:13:10,749
eyes clearing and the eyebrows furrowing

00:13:07,449 --> 00:13:14,349
and this is very popular in this late

00:13:10,749 --> 00:13:17,439
2000s sign typically inspired crime

00:13:14,349 --> 00:13:21,219
drama lie to me which was about a like

00:13:17,439 --> 00:13:22,899
psychologists to use this facial action

00:13:21,219 --> 00:13:24,819
coding system and like emotional

00:13:22,899 --> 00:13:27,839
analysis in order to identify like when

00:13:24,819 --> 00:13:27,839
criminals were lying or not

00:13:30,630 --> 00:13:33,960
so with this facial action coding system

00:13:32,100 --> 00:13:35,430
the idea behind aspect recognition is

00:13:33,960 --> 00:13:37,110
like well we can take the sex anonyme

00:13:35,430 --> 00:13:40,620
for describing faces based on emotions

00:13:37,110 --> 00:13:42,180
and use computer vision algorithms in

00:13:40,620 --> 00:13:44,730
order to identify the facial action

00:13:42,180 --> 00:13:46,140
units on the face and then output what

00:13:44,730 --> 00:13:48,360
we think is the most like a emotion

00:13:46,140 --> 00:13:50,280
based on the action units detected or

00:13:48,360 --> 00:13:54,030
IED action units identified right that

00:13:50,280 --> 00:13:57,060
should work well there's a problem with

00:13:54,030 --> 00:14:00,300
that last year a group of psychologists

00:13:57,060 --> 00:14:02,130
did a extensive review of over 2000

00:14:00,300 --> 00:14:03,750
papers talking about emotions any

00:14:02,130 --> 00:14:05,880
relationship between facial expressions

00:14:03,750 --> 00:14:07,770
and emotions and they came to this

00:14:05,880 --> 00:14:09,330
conclusion that facial expressions do

00:14:07,770 --> 00:14:13,470
not properly represent emotional states

00:14:09,330 --> 00:14:16,890
for three reasons first is limited

00:14:13,470 --> 00:14:18,390
reliability that's worse that's the way

00:14:16,890 --> 00:14:22,230
that people express the same emotion

00:14:18,390 --> 00:14:23,640
they can do so in different ways so for

00:14:22,230 --> 00:14:25,680
example take a look at the pictures to

00:14:23,640 --> 00:14:28,530
the right here we have two pictures of

00:14:25,680 --> 00:14:30,540
women who we both understand are have

00:14:28,530 --> 00:14:32,370
this emotion of anger but they're

00:14:30,540 --> 00:14:34,740
expressing this emotion of anger through

00:14:32,370 --> 00:14:37,470
to drastically different facial emotions

00:14:34,740 --> 00:14:40,050
the top image is a woman with her eyes

00:14:37,470 --> 00:14:41,880
closed or her mouth open the second

00:14:40,050 --> 00:14:45,240
images of a woman with her mouth closed

00:14:41,880 --> 00:14:47,190
in her eyes open and furrowed and eye

00:14:45,240 --> 00:14:49,080
brows furrowed so it's the same emotion

00:14:47,190 --> 00:14:52,620
expressed with two very different facial

00:14:49,080 --> 00:14:54,720
expressions next is lack of specificity

00:14:52,620 --> 00:14:56,340
so there's no unique mapping between a

00:14:54,720 --> 00:14:58,890
facial configuration and a specific

00:14:56,340 --> 00:15:00,270
emotion one emotion to have be

00:14:58,890 --> 00:15:02,400
represented with many different facial

00:15:00,270 --> 00:15:04,080
expressions and one facial expression

00:15:02,400 --> 00:15:05,880
could be represent many different

00:15:04,080 --> 00:15:07,850
emotions and this ties into this last

00:15:05,880 --> 00:15:10,890
guy you have limited generalizability

00:15:07,850 --> 00:15:12,650
context is missing from this we just see

00:15:10,890 --> 00:15:14,790
a picture of her face we don't know why

00:15:12,650 --> 00:15:16,680
we see it we don't understand the

00:15:14,790 --> 00:15:19,260
context or any of the general context of

00:15:16,680 --> 00:15:20,940
like what that person is experiencing if

00:15:19,260 --> 00:15:23,130
you see a picture of someone trying are

00:15:20,940 --> 00:15:25,050
they crying from sadness that they

00:15:23,130 --> 00:15:27,870
experience death or a loss of a loved

00:15:25,050 --> 00:15:36,480
one or are they crying from happiness so

00:15:27,870 --> 00:15:39,029
they overcome some strenuous ideal next

00:15:36,480 --> 00:15:40,889
how computers think about gender and

00:15:39,029 --> 00:15:43,199
specifically what I'll talk about gender

00:15:40,889 --> 00:15:46,919
gender recommendation systems I'll talk

00:15:43,199 --> 00:15:49,019
about the implication of these systems

00:15:46,919 --> 00:15:51,779
on trans and gender non-binary

00:15:49,019 --> 00:15:53,579
individuals and so here's a non-price

00:15:51,779 --> 00:15:55,499
little picture here of like why do we

00:15:53,579 --> 00:15:57,660
how do we use genetic Ender recognition

00:15:55,499 --> 00:16:00,660
systems well this is a tablet in the

00:15:57,660 --> 00:16:03,059
back of a taxicab that scans in the face

00:16:00,660 --> 00:16:04,379
of the passenger and it tries to

00:16:03,059 --> 00:16:07,139
estimate the gender of the passenger

00:16:04,379 --> 00:16:09,509
based on the estimated gender and then

00:16:07,139 --> 00:16:11,999
tries to display you the most relevant

00:16:09,509 --> 00:16:14,839
ad because after all if you're not

00:16:11,999 --> 00:16:18,209
trying to solve the problem of how to

00:16:14,839 --> 00:16:25,499
optimize ad click through rate are you

00:16:18,209 --> 00:16:29,209
really doing data science so in 2018 ah

00:16:25,499 --> 00:16:31,379
skis conducted a literature review of

00:16:29,209 --> 00:16:33,059
these automatic gender recognition

00:16:31,379 --> 00:16:37,139
systems in the field of human-computer

00:16:33,059 --> 00:16:38,970
interaction and he looked at how gender

00:16:37,139 --> 00:16:41,309
was defined in these systems and what

00:16:38,970 --> 00:16:43,259
they found was that gender was always

00:16:41,309 --> 00:16:46,049
defined as this binary variable gender

00:16:43,259 --> 00:16:48,149
was always defined as male or female and

00:16:46,049 --> 00:16:50,009
that is gender is something that is

00:16:48,149 --> 00:16:52,350
defined by your physical appearance you

00:16:50,009 --> 00:16:55,919
are male if you look male and you are

00:16:52,350 --> 00:16:57,869
female if you look female and what they

00:16:55,919 --> 00:17:01,769
argue is that these systems are

00:16:57,869 --> 00:17:04,679
inherently exclude trans and gender

00:17:01,769 --> 00:17:07,370
non-binary individuals by using a

00:17:04,679 --> 00:17:10,470
definition of gender that is inherently

00:17:07,370 --> 00:17:13,529
at odds with Trant the existence of

00:17:10,470 --> 00:17:16,319
trans and non-binary individuals these

00:17:13,529 --> 00:17:19,399
systems then will then perpetuate harms

00:17:16,319 --> 00:17:22,769
against them and we see that for example

00:17:19,399 --> 00:17:27,360
two years ago uber had this issue where

00:17:22,769 --> 00:17:30,419
their driver verification software the

00:17:27,360 --> 00:17:33,350
job verification system based locked out

00:17:30,419 --> 00:17:35,460
trans and gender non-binary drivers

00:17:33,350 --> 00:17:37,889
basically the drivers would upload a

00:17:35,460 --> 00:17:41,279
selfie of themselves every once in a

00:17:37,889 --> 00:17:43,350
while to burrs database just so that

00:17:41,279 --> 00:17:45,500
mÃ¼ber can verify this the person when

00:17:43,350 --> 00:17:48,179
this driver is who they say they are and

00:17:45,500 --> 00:17:49,630
what happens would block out transgender

00:17:48,179 --> 00:17:52,210
drivers because they would

00:17:49,630 --> 00:17:58,419
facial structures before and after home

00:17:52,210 --> 00:18:01,299
own replacement therapy and so following

00:17:58,419 --> 00:18:04,360
up on Austin's work three researchers at

00:18:01,299 --> 00:18:07,120
the University of Colorado perform the

00:18:04,360 --> 00:18:09,370
study of actually how well do these

00:18:07,120 --> 00:18:13,419
commercially available general

00:18:09,370 --> 00:18:16,750
recognition systems perform on trans and

00:18:13,419 --> 00:18:18,940
gender non-binary individuals and so I

00:18:16,750 --> 00:18:20,679
guess yet the relationship is that Oz's

00:18:18,940 --> 00:18:22,390
work sort of lays its theoretical

00:18:20,679 --> 00:18:24,820
framework that automatic gender

00:18:22,390 --> 00:18:27,640
recognition systems are inherently at

00:18:24,820 --> 00:18:29,770
odds with trans and trans and gender

00:18:27,640 --> 00:18:31,510
non-binary individuals and this work is

00:18:29,770 --> 00:18:33,700
sort of what is the empirical result

00:18:31,510 --> 00:18:36,190
like how well do these systems perform

00:18:33,700 --> 00:18:38,440
and practice on these types of in on

00:18:36,190 --> 00:18:40,750
these individuals and so what they did

00:18:38,440 --> 00:18:43,020
is they called they gathered a data set

00:18:40,750 --> 00:18:45,370
of 2400 images from Instagram

00:18:43,020 --> 00:18:47,260
representing seven gender identities so

00:18:45,370 --> 00:18:49,330
they crawled images with these seven

00:18:47,260 --> 00:18:52,270
different hashtags of a gender gender

00:18:49,330 --> 00:18:55,780
queer man non-binary trans man trans

00:18:52,270 --> 00:18:57,520
woman and women and so for example this

00:18:55,780 --> 00:18:59,740
is an image from the paper and what they

00:18:57,520 --> 00:19:02,530
show is it's a picture of an individual

00:18:59,740 --> 00:19:04,870
with the a gender hashtag yeah when run

00:19:02,530 --> 00:19:06,580
through the gender recognition system it

00:19:04,870 --> 00:19:09,190
sort of says the individual and this

00:19:06,580 --> 00:19:11,049
photo is a female or the individual in

00:19:09,190 --> 00:19:14,100
this photo is a young lady

00:19:11,049 --> 00:19:17,860
the smiths gender and the individual and

00:19:14,100 --> 00:19:19,780
so in their uh audit of these gender

00:19:17,860 --> 00:19:22,690
recognition systems what they found is

00:19:19,780 --> 00:19:24,750
that individuals with the trans man or

00:19:22,690 --> 00:19:27,909
as a woman hashtag where a missed

00:19:24,750 --> 00:19:30,220
misgendered at a much higher right 10%

00:19:27,909 --> 00:19:33,100
to 30% higher error rate compared to

00:19:30,220 --> 00:19:35,580
cisgendered individuals and this is very

00:19:33,100 --> 00:19:37,809
important because these systems work

00:19:35,580 --> 00:19:39,850
without the consent of the individuals

00:19:37,809 --> 00:19:41,650
they serve scan your face based on like

00:19:39,850 --> 00:19:43,659
surveillance photo and then assign a

00:19:41,650 --> 00:19:45,700
general able to you and so these

00:19:43,659 --> 00:19:48,309
individuals could be misgendered against

00:19:45,700 --> 00:19:54,549
their will and thus exposing them to

00:19:48,309 --> 00:19:56,799
emotional and physical harm violently so

00:19:54,549 --> 00:19:58,419
we've talked about the limitations of

00:19:56,799 --> 00:20:00,250
face recognition systems we talked about

00:19:58,419 --> 00:20:02,140
how facial recognition struggles with

00:20:00,250 --> 00:20:03,240
race how struggles with emotions and how

00:20:02,140 --> 00:20:05,350
it struggles with

00:20:03,240 --> 00:20:08,710
definitions of gender beyond the gender

00:20:05,350 --> 00:20:11,080
binary so now let's talk about how these

00:20:08,710 --> 00:20:13,450
systems are used in practice because we

00:20:11,080 --> 00:20:16,269
can't sir and separate the technology

00:20:13,450 --> 00:20:19,210
self bad technology from bad use cases

00:20:16,269 --> 00:20:21,149
and to tie in this idea of face

00:20:19,210 --> 00:20:24,190
recognition and governess there's this a

00:20:21,149 --> 00:20:26,559
very powerful op-ed by Kate Crawford of

00:20:24,190 --> 00:20:28,179
the AI Mountain Institute and we should

00:20:26,559 --> 00:20:30,700
calls for a moratorium on facial

00:20:28,179 --> 00:20:32,500
recognition technology in the US and she

00:20:30,700 --> 00:20:34,990
has a statement that these tools are

00:20:32,500 --> 00:20:41,409
dangerous when they fail and they're

00:20:34,990 --> 00:20:44,019
harmful when they work so there was a

00:20:41,409 --> 00:20:46,090
big scandal last year where it turns out

00:20:44,019 --> 00:20:50,250
that annoy a story probe that NYPD

00:20:46,090 --> 00:20:54,279
officers use this facial identification

00:20:50,250 --> 00:20:56,500
service in order to improperly what they

00:20:54,279 --> 00:20:59,110
would do is that it say a crime occurred

00:20:56,500 --> 00:21:00,850
and I said oh hey the suspect looked

00:20:59,110 --> 00:21:02,679
like the celebrity worry Harrell sent

00:21:00,850 --> 00:21:04,059
there go to Google get a picture of

00:21:02,679 --> 00:21:06,279
Woody Harrelson running through this

00:21:04,059 --> 00:21:08,799
facial mapping system as he generated

00:21:06,279 --> 00:21:10,899
potential leads the system would spit

00:21:08,799 --> 00:21:16,000
out a potential match and they would go

00:21:10,899 --> 00:21:18,070
and arrest that individual and so this

00:21:16,000 --> 00:21:20,019
is like really bad because the system

00:21:18,070 --> 00:21:22,480
wasn't designed to be used like that it

00:21:20,019 --> 00:21:24,370
wasn't designed to be used celebrity

00:21:22,480 --> 00:21:26,049
photo look-alikes it wasn't designed to

00:21:24,370 --> 00:21:27,399
be used police sketches of like the

00:21:26,049 --> 00:21:30,149
hand-drawn police sketch was running

00:21:27,399 --> 00:21:32,559
into his system to generate needs so uh

00:21:30,149 --> 00:21:33,879
so sort of like well we don't have any

00:21:32,559 --> 00:21:36,220
countability of Hallie's technologies

00:21:33,879 --> 00:21:37,240
use so we have no way of stopping misuse

00:21:36,220 --> 00:21:40,899
of these technologies

00:21:37,240 --> 00:21:42,190
but even beyond misuse then there's the

00:21:40,899 --> 00:21:44,559
other issue of like when these

00:21:42,190 --> 00:21:48,519
technologies are used properly to cause

00:21:44,559 --> 00:21:51,250
harm so for example in China face

00:21:48,519 --> 00:21:53,889
recognition is used to track and detain

00:21:51,250 --> 00:21:57,100
members of the order Muslim population

00:21:53,889 --> 00:21:59,740
the orders are a ethically Turkish my

00:21:57,100 --> 00:22:01,720
muscle minority in China and the Chinese

00:21:59,740 --> 00:22:04,269
government is using facial recognition

00:22:01,720 --> 00:22:06,429
to identify these individuals random up

00:22:04,269 --> 00:22:09,340
and detain them in concentration camps

00:22:06,429 --> 00:22:11,919
and this big issue because there's no

00:22:09,340 --> 00:22:13,600
international standards on how we can

00:22:11,919 --> 00:22:16,490
and cannot use facial recognition and

00:22:13,600 --> 00:22:18,770
biometric data so States and

00:22:16,490 --> 00:22:20,660
nation-states and other like government

00:22:18,770 --> 00:22:22,040
institutions are able to abuse a

00:22:20,660 --> 00:22:26,480
technology without any sort of

00:22:22,040 --> 00:22:29,630
accountability and this is important

00:22:26,480 --> 00:22:31,520
because technology is a notion of power

00:22:29,630 --> 00:22:34,580
and we have to think about the ways that

00:22:31,520 --> 00:22:35,840
technology is used we as technically as

00:22:34,580 --> 00:22:37,520
software developers when you think about

00:22:35,840 --> 00:22:39,770
the fact that our algorithms and

00:22:37,520 --> 00:22:42,320
technology build exist in the context of

00:22:39,770 --> 00:22:45,440
human systems and I think it's a partner

00:22:42,320 --> 00:22:47,780
for us that would we design new tools

00:22:45,440 --> 00:22:50,270
that we don't build technology that

00:22:47,780 --> 00:22:54,350
reinforces oppressive socio political

00:22:50,270 --> 00:22:55,850
power inequalities for example there's

00:22:54,350 --> 00:22:57,560
been a lot of work of how can we make

00:22:55,850 --> 00:23:00,440
our facial recognition technology better

00:22:57,560 --> 00:23:03,020
at identifying the faces of black and

00:23:00,440 --> 00:23:05,030
brown individuals well if you D by sir

00:23:03,020 --> 00:23:06,740
technology and make it better at

00:23:05,030 --> 00:23:08,630
identifying black around individuals and

00:23:06,740 --> 00:23:11,900
then you go sell that technology to ice

00:23:08,630 --> 00:23:14,750
an institution that I would argue exists

00:23:11,900 --> 00:23:17,720
primarily to brutalize to ease of colors

00:23:14,750 --> 00:23:21,230
then all you've done is make given them

00:23:17,720 --> 00:23:25,820
a tool to better harass and terrorize

00:23:21,230 --> 00:23:27,470
these populations and one key thing

00:23:25,820 --> 00:23:29,000
about technology power is that the

00:23:27,470 --> 00:23:31,070
technologists who build these tools are

00:23:29,000 --> 00:23:33,080
really the ones who experience the harms

00:23:31,070 --> 00:23:36,650
they're really the ones on the receiving

00:23:33,080 --> 00:23:38,030
ends of the downsides of Technology who

00:23:36,650 --> 00:23:42,320
bills acknowledge they're predominantly

00:23:38,030 --> 00:23:43,880
abled bodied white men and who are the

00:23:42,320 --> 00:23:46,460
people who are hot most harmed by these

00:23:43,880 --> 00:23:49,130
technologies not abled bodied white men

00:23:46,460 --> 00:23:50,840
and I think that's why diversity in tech

00:23:49,130 --> 00:23:53,840
and diversity AI is very important

00:23:50,840 --> 00:23:56,480
because is the people in the room who

00:23:53,840 --> 00:23:57,770
decide how technologies use is the

00:23:56,480 --> 00:23:58,790
people in the room who decide what

00:23:57,770 --> 00:24:01,900
problems get to be solved with

00:23:58,790 --> 00:24:05,330
technology as the people in the room who

00:24:01,900 --> 00:24:08,330
decide that AI face-recognition is the

00:24:05,330 --> 00:24:10,310
solution to the problem and if there

00:24:08,330 --> 00:24:12,440
aren't diverse experiences reflected in

00:24:10,310 --> 00:24:14,180
the room then we won't understand the

00:24:12,440 --> 00:24:19,100
different way is that face recognition

00:24:14,180 --> 00:24:23,060
could be misused or purposely could be

00:24:19,100 --> 00:24:24,920
misused or purposely used in Ord harm or

00:24:23,060 --> 00:24:28,700
target members of with different life

00:24:24,920 --> 00:24:30,410
experiences and one final thing is when

00:24:28,700 --> 00:24:32,390
we talk about billing Technol

00:24:30,410 --> 00:24:34,460
like facial recognition before we talk

00:24:32,390 --> 00:24:36,560
about just how we build a psychology

00:24:34,460 --> 00:24:37,940
than how we devise technology before we

00:24:36,560 --> 00:24:40,460
even build we should ask yourself this a

00:24:37,940 --> 00:24:43,970
very important question should we even

00:24:40,460 --> 00:24:45,290
be building these systems and I think

00:24:43,970 --> 00:24:47,390
that's something very important going

00:24:45,290 --> 00:24:49,160
forward in this ethical AI

00:24:47,390 --> 00:24:50,660
responsibility I space is that we should

00:24:49,160 --> 00:24:52,940
be able to ask yourself is this

00:24:50,660 --> 00:24:55,190
technology worth building do we sit and

00:24:52,940 --> 00:24:57,110
if the answer is no we shouldn't build

00:24:55,190 --> 00:24:59,120
this technology that because we believe

00:24:57,110 --> 00:25:01,520
it could harm certain populations or

00:24:59,120 --> 00:25:03,590
hurt certain individuals then we should

00:25:01,520 --> 00:25:06,470
not build it at all and we're starting

00:25:03,590 --> 00:25:09,650
to see that now we are starting to see

00:25:06,470 --> 00:25:12,500
Google employees and Microsoft employees

00:25:09,650 --> 00:25:16,160
staging walkouts to protest the fact

00:25:12,500 --> 00:25:18,380
that their employers are they have

00:25:16,160 --> 00:25:20,930
accepted military contracts to build

00:25:18,380 --> 00:25:22,430
weapons systems that the software

00:25:20,930 --> 00:25:24,110
engineers don't approve of we're

00:25:22,430 --> 00:25:26,480
starting to see Amazon employees staging

00:25:24,110 --> 00:25:29,000
blackouts to promote their their

00:25:26,480 --> 00:25:30,890
company's impact on climate in order to

00:25:29,000 --> 00:25:33,050
promote for like more climate energy

00:25:30,890 --> 00:25:36,290
efficient data centers and we're

00:25:33,050 --> 00:25:38,270
starting to see students protesting sign

00:25:36,290 --> 00:25:39,530
a pledge to over 2,000 students across

00:25:38,270 --> 00:25:40,700
the u.s. colleges have signed a pledge

00:25:39,530 --> 00:25:43,070
saying that they won't work for the

00:25:40,700 --> 00:25:45,770
company Palantir these Palantir builds

00:25:43,070 --> 00:25:47,330
surveillance software for ice and they

00:25:45,770 --> 00:25:52,400
don't want to build tools that are used

00:25:47,330 --> 00:25:54,410
to harm harm these communities and so

00:25:52,400 --> 00:25:59,300
one key topic is now emerging and

00:25:54,410 --> 00:26:01,010
responsible AI is how do we empower

00:25:59,300 --> 00:26:03,110
developers how do you even power data

00:26:01,010 --> 00:26:06,830
scientists and engineers with this right

00:26:03,110 --> 00:26:08,210
of refusal how do we give allow data

00:26:06,830 --> 00:26:10,010
scientists and engineers to collectively

00:26:08,210 --> 00:26:12,080
organize and say it's like no we will

00:26:10,010 --> 00:26:13,820
not build this technology because we

00:26:12,080 --> 00:26:17,060
believe it could cause serious harm

00:26:13,820 --> 00:26:20,000
and so this draws work draws from

00:26:17,060 --> 00:26:22,160
inspiration on government regulations so

00:26:20,000 --> 00:26:25,160
think about the Kyoto Protocol or the

00:26:22,160 --> 00:26:26,870
Copenhagen protocol or Kyoto cores and

00:26:25,160 --> 00:26:28,670
the Copenhagen Accords but it also

00:26:26,870 --> 00:26:31,670
management strategy so like what

00:26:28,670 --> 00:26:33,980
organizational structures would allow

00:26:31,670 --> 00:26:35,630
employees to feel safe in order to raise

00:26:33,980 --> 00:26:38,060
concerns without the fear of losing

00:26:35,630 --> 00:26:40,700
their jobs and then also DevOps of like

00:26:38,060 --> 00:26:42,830
if you build a system and you realize oh

00:26:40,700 --> 00:26:43,020
wow if you deploy is causing harm how do

00:26:42,830 --> 00:26:47,330
you

00:26:43,020 --> 00:26:49,620
or take down that system very easily and

00:26:47,330 --> 00:26:51,780
in the skull research we're hoping to

00:26:49,620 --> 00:26:53,730
design like common frameworks that

00:26:51,780 --> 00:26:56,370
empower tech employees in order to

00:26:53,730 --> 00:26:59,280
internally protest any products they

00:26:56,370 --> 00:27:05,370
feel could have devastating effects on

00:26:59,280 --> 00:27:06,900
society and so that's my talk and here

00:27:05,370 --> 00:27:09,320
are all the works I cited I hope you

00:27:06,900 --> 00:27:09,320

YouTube URL: https://www.youtube.com/watch?v=OS63Er80KJc


