Title: Talk: James Bennett - A 🐍's guide to Unicode
Publication date: 2021-05-05
Playlist: PyCon US 2020
Description: 
	Presented by:
James Bennett

Unicode can seem like a scary topic, especially since people so often talk about it as a horrendously complex thing programmers should be afraid of 😱. But while Unicode does have some complexity, it doesn’t have to be scary! So this talk will demystify it: you’ll get to wave hello 👋 to Unicode, learn what it really is, how it works, and tips for how you can ❤️ Unicode in 🐍.
Captions: 
	00:00:13,700 --> 00:00:21,330
hi I'm James and I'm here today to talk

00:00:17,520 --> 00:00:23,310
to you about Unicode now I know that

00:00:21,330 --> 00:00:26,369
word can provoke some reactions in

00:00:23,310 --> 00:00:29,970
people so the very first thing I want to

00:00:26,369 --> 00:00:33,260
do is remind you of a wise message from

00:00:29,970 --> 00:00:37,680
a beloved modern philosopher because

00:00:33,260 --> 00:00:40,290
Unicode is complex and I'm sure you've

00:00:37,680 --> 00:00:42,360
heard scary stories about it but it's

00:00:40,290 --> 00:00:44,699
not something that you have to be afraid

00:00:42,360 --> 00:00:47,070
of and by the end of this talk I hope

00:00:44,699 --> 00:00:49,620
you won't be afraid of it because you're

00:00:47,070 --> 00:00:52,609
going to understand where Unicode came

00:00:49,620 --> 00:00:55,350
from what it is and how it really works

00:00:52,609 --> 00:00:57,210
how it gets implemented in computer

00:00:55,350 --> 00:01:00,059
systems and especially programming

00:00:57,210 --> 00:01:02,550
languages like Python and how you can

00:01:00,059 --> 00:01:05,940
work with it and recognize where the

00:01:02,550 --> 00:01:07,979
complexity in it is found so that you

00:01:05,940 --> 00:01:10,350
can manage that complexity and write

00:01:07,979 --> 00:01:12,420
more effective and more confident code

00:01:10,350 --> 00:01:16,560
so that you don't have to be afraid of

00:01:12,420 --> 00:01:18,659
Unicode anymore but of course we have to

00:01:16,560 --> 00:01:21,780
start somewhere which means starting at

00:01:18,659 --> 00:01:24,630
the beginning and the history of Unicode

00:01:21,780 --> 00:01:28,200
really is the history of written text

00:01:24,630 --> 00:01:30,689
which is kind of complicated because

00:01:28,200 --> 00:01:33,600
we're still not sure entirely what that

00:01:30,689 --> 00:01:35,270
history looks like we know writing seems

00:01:33,600 --> 00:01:38,090
to have been invented independently

00:01:35,270 --> 00:01:40,920
multiple times throughout history and

00:01:38,090 --> 00:01:43,609
over the thousands of years since then

00:01:40,920 --> 00:01:46,109
people have come up with almost an

00:01:43,609 --> 00:01:49,399
unbelievable number of different ways of

00:01:46,109 --> 00:01:52,049
writing down their thoughts and words

00:01:49,399 --> 00:01:54,390
anything you can think of as a basis for

00:01:52,049 --> 00:01:57,420
a writing system has probably been used

00:01:54,390 --> 00:01:59,909
at some point there are writing systems

00:01:57,420 --> 00:02:03,539
that are based on individual sounds or

00:01:59,909 --> 00:02:05,250
syllables or whole words or ideas there

00:02:03,539 --> 00:02:08,070
are writing systems that are based on

00:02:05,250 --> 00:02:09,810
abstract representations of an idea

00:02:08,070 --> 00:02:11,910
there are writing systems that are based

00:02:09,810 --> 00:02:15,480
on the position of your mouth as you

00:02:11,910 --> 00:02:18,060
pronounce a sound really anything you

00:02:15,480 --> 00:02:21,269
could imagine probably someone has come

00:02:18,060 --> 00:02:23,519
up with and the history of text

00:02:21,269 --> 00:02:25,830
encompasses all of those things which

00:02:23,519 --> 00:02:28,530
can be pretty complicated

00:02:25,830 --> 00:02:30,390
now fortunately for this talk we really

00:02:28,530 --> 00:02:32,820
only need to talk about the last couple

00:02:30,390 --> 00:02:35,930
of centuries when we've tried to come up

00:02:32,820 --> 00:02:39,270
with systems for representing and

00:02:35,930 --> 00:02:41,070
transmitting text electronically there

00:02:39,270 --> 00:02:43,770
are older systems for long-distance

00:02:41,070 --> 00:02:47,730
transmission there are semaphore systems

00:02:43,770 --> 00:02:49,580
and flag code and signal fires and many

00:02:47,730 --> 00:02:52,610
other systems that were really effective

00:02:49,580 --> 00:02:55,650
but today we're mostly concerned with

00:02:52,610 --> 00:02:59,880
electronic or electromagnetic broadcast

00:02:55,650 --> 00:03:02,910
over a wire or radio waves and we've

00:02:59,880 --> 00:03:05,340
probably all seen some examples of early

00:03:02,910 --> 00:03:07,590
attempts at this this is one a lot of

00:03:05,340 --> 00:03:09,360
people know is Morse code which was

00:03:07,590 --> 00:03:12,540
developed for a telegraph system

00:03:09,360 --> 00:03:15,360
it's a variable-width encoding uses

00:03:12,540 --> 00:03:19,530
binary alphabet of two characters a dot

00:03:15,360 --> 00:03:21,480
and a dash and there was a whole family

00:03:19,530 --> 00:03:23,430
of different telegraph codes with

00:03:21,480 --> 00:03:26,070
different principals and based on

00:03:23,430 --> 00:03:28,920
different ideas one of the more popular

00:03:26,070 --> 00:03:31,410
later on was ITA which is a Baudot code

00:03:28,920 --> 00:03:33,900
named after a meal Bordeaux who also

00:03:31,410 --> 00:03:37,080
gives us the baud as a unit of

00:03:33,900 --> 00:03:38,580
transmission rate and Baudot codes are

00:03:37,080 --> 00:03:40,709
kind of interesting because they

00:03:38,580 --> 00:03:43,590
introduce this concept of control

00:03:40,709 --> 00:03:46,830
characters if you look at this it's a

00:03:43,590 --> 00:03:48,510
five bit binary code and normally these

00:03:46,830 --> 00:03:51,870
messages would be recorded by being

00:03:48,510 --> 00:03:53,190
punched as holes in a paper tape which

00:03:51,870 --> 00:03:55,980
sounds like it should only be able to

00:03:53,190 --> 00:03:58,650
handle 32 characters that's 2 to the

00:03:55,980 --> 00:04:00,900
fifth power but here the capacity is

00:03:58,650 --> 00:04:03,360
over 60 characters because it uses a

00:04:00,900 --> 00:04:06,239
control character to switch between two

00:04:03,360 --> 00:04:08,489
different alphabets of characters these

00:04:06,239 --> 00:04:11,100
sorts of clever innovations let people

00:04:08,489 --> 00:04:13,170
do a lot of cool things with the

00:04:11,100 --> 00:04:15,090
telegraph system as it evolved and

00:04:13,170 --> 00:04:18,540
eventually developed into modern

00:04:15,090 --> 00:04:20,400
computer text encoding systems many of

00:04:18,540 --> 00:04:24,210
which were heavily influenced by these

00:04:20,400 --> 00:04:26,400
Telegraph codes this of course is the

00:04:24,210 --> 00:04:29,640
100 pound gorilla in the room ASCII

00:04:26,400 --> 00:04:33,300
which owes a lot to ITA too and the

00:04:29,640 --> 00:04:36,090
Bordeaux family of Telegraph codes but

00:04:33,300 --> 00:04:38,760
ASCII really took over the world even

00:04:36,090 --> 00:04:39,660
though it shouldn't have the problem

00:04:38,760 --> 00:04:44,610
with ASCII of course

00:04:39,660 --> 00:04:46,650
is it's the American Standard Code which

00:04:44,610 --> 00:04:48,480
is a problem in a world that contains a

00:04:46,650 --> 00:04:50,970
lot more countries than America and a

00:04:48,480 --> 00:04:54,180
lot more languages than English which

00:04:50,970 --> 00:04:56,040
meant that of course even though ASCII

00:04:54,180 --> 00:04:58,320
was built into a lot of systems and

00:04:56,040 --> 00:05:02,250
still is today and a lot of things will

00:04:58,320 --> 00:05:04,680
assume ASCII lots of different people

00:05:02,250 --> 00:05:07,740
developed text encoding x' to represent

00:05:04,680 --> 00:05:10,920
their languages their dialects their

00:05:07,740 --> 00:05:13,230
regions their countries there are a huge

00:05:10,920 --> 00:05:15,300
number of them out there this is just a

00:05:13,230 --> 00:05:19,050
subset that I took from a list that I

00:05:15,300 --> 00:05:21,240
found online and of course that brought

00:05:19,050 --> 00:05:23,640
its own set of problems because how do

00:05:21,240 --> 00:05:25,800
you work together with so many different

00:05:23,640 --> 00:05:28,890
possible in coatings how do you avoid

00:05:25,800 --> 00:05:31,350
the kinds of bugs and translation and

00:05:28,890 --> 00:05:33,090
encoding problems that can come up when

00:05:31,350 --> 00:05:35,040
you have this many different options and

00:05:33,090 --> 00:05:37,980
you may not even know which of them are

00:05:35,040 --> 00:05:41,730
being used wouldn't it be great if we

00:05:37,980 --> 00:05:47,010
just had one Universal agreed-on

00:05:41,730 --> 00:05:50,820
solution well that's what Unicode is

00:05:47,010 --> 00:05:52,920
supposed to be and it's worth pausing

00:05:50,820 --> 00:05:57,480
for a moment to make sure we understand

00:05:52,920 --> 00:06:00,600
really what Unicode is a lot of single

00:05:57,480 --> 00:06:02,760
page guides will really make a point of

00:06:00,600 --> 00:06:06,360
saying Unicode is not a character set

00:06:02,760 --> 00:06:09,680
and Unicode is not an encoding it's much

00:06:06,360 --> 00:06:13,260
more productive to think of Unicode as a

00:06:09,680 --> 00:06:16,680
set of data bases and specifications and

00:06:13,260 --> 00:06:19,380
rules and properties for describing

00:06:16,680 --> 00:06:23,970
different human writing systems that we

00:06:19,380 --> 00:06:26,910
know about and yes some of those include

00:06:23,970 --> 00:06:29,130
ways to encode it into binary text yes

00:06:26,910 --> 00:06:32,580
some of those include sets of characters

00:06:29,130 --> 00:06:34,710
but Unicode itself is so much more than

00:06:32,580 --> 00:06:37,530
any of those individual components and

00:06:34,710 --> 00:06:40,290
of course that means it's also complex

00:06:37,530 --> 00:06:42,060
and it really has to be if you think

00:06:40,290 --> 00:06:44,820
about that long list of different

00:06:42,060 --> 00:06:47,760
encoding 'z Unicode has to try to do the

00:06:44,820 --> 00:06:49,550
job of all of them and handle all of the

00:06:47,760 --> 00:06:52,580
things that they handled

00:06:49,550 --> 00:06:55,099
if any given individual encoding only

00:06:52,580 --> 00:06:57,620
needed to handle perhaps one languages

00:06:55,099 --> 00:07:00,740
or one dialects or one regions

00:06:57,620 --> 00:07:02,810
particular language and rules for

00:07:00,740 --> 00:07:05,120
writing but Unicode has to be able to

00:07:02,810 --> 00:07:07,430
handle them all there's a lot of

00:07:05,120 --> 00:07:10,550
complexity that's just inherent to that

00:07:07,430 --> 00:07:12,770
task and of course that means it's very

00:07:10,550 --> 00:07:14,960
different from those older single

00:07:12,770 --> 00:07:17,050
purpose and codings but different

00:07:14,960 --> 00:07:20,659
doesn't have to be the same thing as

00:07:17,050 --> 00:07:24,620
scary and I hope that's something you'll

00:07:20,659 --> 00:07:26,240
take away from this talk now we need to

00:07:24,620 --> 00:07:28,130
dive a little bit deeper into the

00:07:26,240 --> 00:07:31,580
terminology just to be able to talk

00:07:28,130 --> 00:07:35,479
usefully about Unicode so let's stop and

00:07:31,580 --> 00:07:38,750
do a quick glossary check because often

00:07:35,479 --> 00:07:41,060
we fall into very informal terminology

00:07:38,750 --> 00:07:43,580
like we start talking about characters

00:07:41,060 --> 00:07:46,190
and unicode does have a concept of

00:07:43,580 --> 00:07:48,560
character but it's much more of an

00:07:46,190 --> 00:07:52,069
abstract entity than in those older and

00:07:48,560 --> 00:07:54,590
coatings and character sets the basic

00:07:52,069 --> 00:07:58,729
atoms of Unicode the things that make it

00:07:54,590 --> 00:08:00,349
up are called code points and you might

00:07:58,729 --> 00:08:01,819
try to think of a code point as a

00:08:00,349 --> 00:08:05,539
character but we're going to see

00:08:01,819 --> 00:08:09,469
examples of why that's risky and Unicode

00:08:05,539 --> 00:08:14,509
itself is organized into planes of code

00:08:09,469 --> 00:08:17,000
points 2 to the 16th or 65,536 code

00:08:14,509 --> 00:08:19,870
points per plane originally there was

00:08:17,000 --> 00:08:26,960
just one plane now there are 17 of them

00:08:19,870 --> 00:08:29,270
and code points are much like unicode's

00:08:26,960 --> 00:08:32,649
concept of a character still sort of an

00:08:29,270 --> 00:08:35,959
abstract entity when we start encoding

00:08:32,649 --> 00:08:38,539
unicode into a binary format we need to

00:08:35,959 --> 00:08:41,750
translate it into code units which are

00:08:38,539 --> 00:08:44,750
the atoms of a binary encoding and then

00:08:41,750 --> 00:08:46,520
if we do want to go up a higher level if

00:08:44,750 --> 00:08:49,040
we want something that's analogous to

00:08:46,520 --> 00:08:51,920
what we would call a character unicode

00:08:49,040 --> 00:08:53,630
has the term grapheme sometimes you'll

00:08:51,920 --> 00:08:56,120
also see it described as a graphene

00:08:53,630 --> 00:08:57,920
cluster and there are a couple different

00:08:56,120 --> 00:09:00,050
variations there's legacy graphene

00:08:57,920 --> 00:09:02,930
clusters and extended graphene clusters

00:09:00,050 --> 00:09:05,270
you don't really need to know all

00:09:02,930 --> 00:09:08,140
differences between those to be able to

00:09:05,270 --> 00:09:12,820
work effectively with Unicode in Python

00:09:08,140 --> 00:09:15,380
but a grapheme is in Unicode terms the

00:09:12,820 --> 00:09:18,140
smallest or the minimally distinctive

00:09:15,380 --> 00:09:21,170
unit of writing in a particular system a

00:09:18,140 --> 00:09:23,750
grapheme is the smallest thing such that

00:09:21,170 --> 00:09:26,810
if you change it you change the meaning

00:09:23,750 --> 00:09:28,880
of the text and there is not a

00:09:26,810 --> 00:09:32,570
one-to-one correspondence between these

00:09:28,880 --> 00:09:33,980
and code points as we're about to see so

00:09:32,570 --> 00:09:36,260
let's look at some examples of code

00:09:33,980 --> 00:09:38,300
points here's one that's probably

00:09:36,260 --> 00:09:40,760
familiar to a lot of people it's just a

00:09:38,300 --> 00:09:43,250
Latin capital letter a as you can see

00:09:40,760 --> 00:09:45,890
from the name it's good point is zero

00:09:43,250 --> 00:09:48,500
zero for one code points are always a

00:09:45,890 --> 00:09:51,800
number by tradition they're expressed in

00:09:48,500 --> 00:09:54,020
hexadecimal and we can see that it has a

00:09:51,800 --> 00:09:56,180
block and a category and some other

00:09:54,020 --> 00:09:58,880
information and fact this is just a

00:09:56,180 --> 00:10:02,630
subset of the information Unicode has on

00:09:58,880 --> 00:10:05,750
this code point blocks are a way of

00:10:02,630 --> 00:10:08,750
organizing code points below the level

00:10:05,750 --> 00:10:11,420
of a plain blocks are contiguous sets of

00:10:08,750 --> 00:10:13,790
code points that are all related so for

00:10:11,420 --> 00:10:15,580
example the basic Latin block contains

00:10:13,790 --> 00:10:18,200
the code points for the Latin alphabet

00:10:15,580 --> 00:10:20,660
or at least the most common parts of it

00:10:18,200 --> 00:10:23,390
and has a lot of overlap with asti in

00:10:20,660 --> 00:10:25,640
fact the first 128 code points in

00:10:23,390 --> 00:10:29,930
Unicode match the hundred and

00:10:25,640 --> 00:10:31,660
twenty-eight values in ascii we can see

00:10:29,930 --> 00:10:34,820
it's category it's an uppercase letter

00:10:31,660 --> 00:10:36,950
we can see it has information about by

00:10:34,820 --> 00:10:38,950
directionality English and most other

00:10:36,950 --> 00:10:42,380
Western European languages are written

00:10:38,950 --> 00:10:44,720
left-right there's also this combining

00:10:42,380 --> 00:10:46,730
class property which we'll get to in

00:10:44,720 --> 00:10:48,529
just a second and of course there's a

00:10:46,730 --> 00:10:50,209
lot more that you could look up if you

00:10:48,529 --> 00:10:53,390
went trolling through all of the

00:10:50,209 --> 00:10:54,350
information in a Unicode database so

00:10:53,390 --> 00:10:57,230
let's look at something a little bit

00:10:54,350 --> 00:11:00,560
more complicated this is code point zero

00:10:57,230 --> 00:11:03,380
three zero eight which by itself doesn't

00:11:00,560 --> 00:11:06,260
really do anything it wants to go with

00:11:03,380 --> 00:11:09,770
something else and when it does it shows

00:11:06,260 --> 00:11:11,870
up as an accent mark diuresis or

00:11:09,770 --> 00:11:13,300
sometimes you might call it an umlaut or

00:11:11,870 --> 00:11:15,730
just dots

00:11:13,300 --> 00:11:18,130
and here we can see for example that

00:11:15,730 --> 00:11:21,310
combining class value suddenly shows up

00:11:18,130 --> 00:11:23,110
has a value of two three zero which says

00:11:21,310 --> 00:11:25,269
when we're rendering this and we see

00:11:23,110 --> 00:11:28,390
this in a sequence of code points it

00:11:25,269 --> 00:11:30,250
shows up above whatever came before it

00:11:28,390 --> 00:11:32,890
and there are different values for

00:11:30,250 --> 00:11:34,810
combining class to show positioning and

00:11:32,890 --> 00:11:38,560
how different things combined together

00:11:34,810 --> 00:11:42,910
to form a single visible glyph on your

00:11:38,560 --> 00:11:45,579
screen or when printed but notice that

00:11:42,910 --> 00:11:48,670
this means if we want that you with an

00:11:45,579 --> 00:11:51,339
umlaut above it we're using multiple

00:11:48,670 --> 00:11:53,380
code points to produce what is one

00:11:51,339 --> 00:11:56,260
character from the human readers

00:11:53,380 --> 00:11:58,930
perspective so we've already broken that

00:11:56,260 --> 00:12:00,670
concept of one code point is one

00:11:58,930 --> 00:12:03,100
character because here we have a

00:12:00,670 --> 00:12:06,250
character that uses two code points and

00:12:03,100 --> 00:12:08,790
it actually goes combo the actual

00:12:06,250 --> 00:12:11,680
complexity shows up in both directions

00:12:08,790 --> 00:12:14,800
for example here this character from the

00:12:11,680 --> 00:12:17,440
Arabic sections of Unicode is one code

00:12:14,800 --> 00:12:20,050
point but eighteen characters and

00:12:17,440 --> 00:12:22,360
they're actually several of these in

00:12:20,050 --> 00:12:24,579
Unicode these are used in Arabic

00:12:22,360 --> 00:12:27,040
religious typesetting where there are

00:12:24,579 --> 00:12:29,380
certain phrases that tend to occur quite

00:12:27,040 --> 00:12:31,870
often and there are ligatures for

00:12:29,380 --> 00:12:34,480
representing them just as a single unit

00:12:31,870 --> 00:12:38,740
when typesetting and when printing and

00:12:34,480 --> 00:12:41,230
display but again we see you can't

00:12:38,740 --> 00:12:45,550
assume one code point is one character

00:12:41,230 --> 00:12:50,399
and in fact this is one good point that

00:12:45,550 --> 00:12:54,160
isn't necessarily even one word so

00:12:50,399 --> 00:12:56,199
unicode yes can be complex and yes you

00:12:54,160 --> 00:12:58,390
need to know that there's a difference

00:12:56,199 --> 00:13:01,000
between code points and characters and

00:12:58,390 --> 00:13:03,730
graphemes but when you sit down and

00:13:01,000 --> 00:13:05,829
think about it any system that tries to

00:13:03,730 --> 00:13:08,589
handle all of the complexity of human

00:13:05,829 --> 00:13:11,500
writing sooner or later is going to run

00:13:08,589 --> 00:13:13,779
into something like this and is going to

00:13:11,500 --> 00:13:18,310
present this level of complexity to you

00:13:13,779 --> 00:13:20,829
in some way and of course it keeps going

00:13:18,310 --> 00:13:22,870
because this means that often in Unicode

00:13:20,829 --> 00:13:23,790
there are multiple ways to write the

00:13:22,870 --> 00:13:26,280
same thing

00:13:23,790 --> 00:13:29,280
going back to that you with the umlaut

00:13:26,280 --> 00:13:31,500
above it there are at least two ways you

00:13:29,280 --> 00:13:33,270
can write this in unicode there's the

00:13:31,500 --> 00:13:36,180
one we saw earlier that uses the

00:13:33,270 --> 00:13:38,550
combining accent character there's also

00:13:36,180 --> 00:13:40,290
a pre composed form that does it in one

00:13:38,550 --> 00:13:43,470
code point and this is there for

00:13:40,290 --> 00:13:45,900
historical reasons a lot of earlier in

00:13:43,470 --> 00:13:48,480
codings there were single purpose had

00:13:45,900 --> 00:13:50,700
precomposed characters for different

00:13:48,480 --> 00:13:53,610
combinations of letters and accent marks

00:13:50,700 --> 00:13:55,320
and so for compatibility unicode has to

00:13:53,610 --> 00:13:58,740
have them as well so that you can do a

00:13:55,320 --> 00:14:00,960
lossless conversion to and from unicode

00:13:58,740 --> 00:14:04,500
back to your original encoding if you

00:14:00,960 --> 00:14:06,630
ever want to but this means that in

00:14:04,500 --> 00:14:10,110
unicode we can end up with multiple ways

00:14:06,630 --> 00:14:12,420
to write the same thing and these two

00:14:10,110 --> 00:14:15,240
sequences of code points where the one

00:14:12,420 --> 00:14:17,850
precomposed point and the decomposed

00:14:15,240 --> 00:14:19,860
sequence of two code points are

00:14:17,850 --> 00:14:21,840
considered equivalent and in fact

00:14:19,860 --> 00:14:24,300
unicode calls them canonically

00:14:21,840 --> 00:14:26,910
equivalent because it should always be

00:14:24,300 --> 00:14:28,620
safe to swap one of these for the other

00:14:26,910 --> 00:14:31,320
you won't change the meaning of your

00:14:28,620 --> 00:14:33,750
text by doing so but it also has a

00:14:31,320 --> 00:14:36,990
concept of compatibility equivalence

00:14:33,750 --> 00:14:39,330
which is where it may not always be safe

00:14:36,990 --> 00:14:41,370
to swap between two different ways of

00:14:39,330 --> 00:14:42,990
writing the same thing so here for

00:14:41,370 --> 00:14:46,400
example we have a code point that

00:14:42,990 --> 00:14:49,260
represents a composed fraction 1/2 and

00:14:46,400 --> 00:14:51,810
AD composed sequence that writes it out

00:14:49,260 --> 00:14:54,330
as a 1 at a 2 with a splash between them

00:14:51,810 --> 00:14:56,220
there are times when it's correct to

00:14:54,330 --> 00:15:00,120
swap between these there are also times

00:14:56,220 --> 00:15:03,570
when it's not and this gives rise to the

00:15:00,120 --> 00:15:05,970
concept of normalization which is a way

00:15:03,570 --> 00:15:08,910
that we can take different sequences

00:15:05,970 --> 00:15:11,910
that may represent the same thing and

00:15:08,910 --> 00:15:15,390
find out if they do by making them equal

00:15:11,910 --> 00:15:17,940
after the normalization and because

00:15:15,390 --> 00:15:20,670
unicode has both composed and decomposed

00:15:17,940 --> 00:15:22,890
forms it has two different types of

00:15:20,670 --> 00:15:25,740
equivalence there are four different

00:15:22,890 --> 00:15:27,420
ways to normalize unicode depending on

00:15:25,740 --> 00:15:30,480
what the result should look like and

00:15:27,420 --> 00:15:33,390
what rules you want to apply so you can

00:15:30,480 --> 00:15:35,640
either get a composed or ad composed for

00:15:33,390 --> 00:15:37,800
after the normalization you can use

00:15:35,640 --> 00:15:40,050
either canonical or compatibility

00:15:37,800 --> 00:15:43,560
equivalence rules as you're doing this

00:15:40,050 --> 00:15:46,560
now we'll get to this a little bit later

00:15:43,560 --> 00:15:48,060
on but if you're just feeling

00:15:46,560 --> 00:15:50,340
overwhelmed and want a general

00:15:48,060 --> 00:15:52,830
recommendation if you ever need to do

00:15:50,340 --> 00:15:56,280
Unicode normalization yourself it's

00:15:52,830 --> 00:15:57,690
probably best to pick form NF KC that's

00:15:56,280 --> 00:16:00,240
the one that will make the most

00:15:57,690 --> 00:16:03,030
trade-offs in favor of what you probably

00:16:00,240 --> 00:16:04,950
want but we'll see examples of how

00:16:03,030 --> 00:16:07,850
different normalization forms can be

00:16:04,950 --> 00:16:10,080
good or bad a little bit later on

00:16:07,850 --> 00:16:13,560
speaking of multiple ways of writing the

00:16:10,080 --> 00:16:16,020
same thing though a lot of languages

00:16:13,560 --> 00:16:18,180
have multiple different forms for

00:16:16,020 --> 00:16:21,090
different characters uppercase and

00:16:18,180 --> 00:16:23,520
lowercase and in fact Unicode has three

00:16:21,090 --> 00:16:26,160
different cases lower case upper case

00:16:23,520 --> 00:16:28,650
and title case and multiple different

00:16:26,160 --> 00:16:31,350
case mappings and ways of transforming

00:16:28,650 --> 00:16:34,350
characters according to case as well as

00:16:31,350 --> 00:16:37,350
the concept of completely uncased

00:16:34,350 --> 00:16:40,890
characters and in fact most code points

00:16:37,350 --> 00:16:43,860
in Unicode or most characters abstract

00:16:40,890 --> 00:16:45,960
entities the Unicode handles are encased

00:16:43,860 --> 00:16:47,520
because the case mappings won't change

00:16:45,960 --> 00:16:49,740
them because they're coming from

00:16:47,520 --> 00:16:53,010
languages or systems of symbols that

00:16:49,740 --> 00:16:55,320
just don't have a concept of case now

00:16:53,010 --> 00:16:57,030
you might be wondering well how then do

00:16:55,320 --> 00:16:59,490
I do things like case insensitive

00:16:57,030 --> 00:17:02,040
comparisons especially because unicode

00:16:59,490 --> 00:17:04,440
if you dig into it has at least three

00:17:02,040 --> 00:17:07,230
different concepts of case and ways to

00:17:04,440 --> 00:17:10,170
find out what case a character is or

00:17:07,230 --> 00:17:13,320
whether it even is cased and the answer

00:17:10,170 --> 00:17:15,240
is case folding which Python supports

00:17:13,320 --> 00:17:17,579
and we'll see examples of it in a little

00:17:15,240 --> 00:17:20,459
bit but I do want to call out that

00:17:17,579 --> 00:17:24,690
pythons documentation says something not

00:17:20,459 --> 00:17:27,950
great Python says case folding is like a

00:17:24,690 --> 00:17:31,170
more aggressive form of lower casing and

00:17:27,950 --> 00:17:34,020
while it's true that for a lot of

00:17:31,170 --> 00:17:37,140
Western European languages the result of

00:17:34,020 --> 00:17:39,420
case folding will look lower case this

00:17:37,140 --> 00:17:41,400
is not a guarantee there are languages

00:17:39,420 --> 00:17:43,250
where the result of case folding will

00:17:41,400 --> 00:17:46,830
look uppercase

00:17:43,250 --> 00:17:50,370
so don't think of case folding as being

00:17:46,830 --> 00:17:52,649
upper casing or lower casing it is its

00:17:50,370 --> 00:17:54,990
own thing but the important thing to

00:17:52,649 --> 00:17:57,929
know about case folding is that after

00:17:54,990 --> 00:18:00,269
you've case folded two strings if they

00:17:57,929 --> 00:18:04,049
differed only in case they will be the

00:18:00,269 --> 00:18:08,190
same after the fold and of course case

00:18:04,049 --> 00:18:11,370
can also extend beyond what Unicode

00:18:08,190 --> 00:18:14,760
really handles Unicode handles most of

00:18:11,370 --> 00:18:16,649
these cases for example the Greek Sigma

00:18:14,760 --> 00:18:19,200
which takes different forms depending on

00:18:16,649 --> 00:18:21,480
where it occurs in a word the Turkic

00:18:19,200 --> 00:18:24,419
languages have both dotted and dauntless

00:18:21,480 --> 00:18:26,820
forms of the letter I and it's

00:18:24,419 --> 00:18:28,919
incredibly important to preserve the dot

00:18:26,820 --> 00:18:30,809
or absence of the dot when you're doing

00:18:28,919 --> 00:18:34,019
a case transformation because those

00:18:30,809 --> 00:18:36,289
effect meaning German has this character

00:18:34,019 --> 00:18:38,519
officially it's called the sharp s

00:18:36,289 --> 00:18:42,720
historically it didn't have an uppercase

00:18:38,519 --> 00:18:44,850
form and so in upper cases to SS but

00:18:42,720 --> 00:18:47,549
this also means that case mappings in

00:18:44,850 --> 00:18:49,470
unicode aren't transitive because upper

00:18:47,549 --> 00:18:51,539
casing this and then lower casing again

00:18:49,470 --> 00:18:54,799
won't get you back what you started with

00:18:51,539 --> 00:18:57,899
and there's far more complexity in

00:18:54,799 --> 00:19:00,179
languages that have case the Unicode

00:18:57,899 --> 00:19:02,360
just doesn't handle and tells you you

00:19:00,179 --> 00:19:04,950
may need to have locale aware rules

00:19:02,360 --> 00:19:08,809
there are situations like for example

00:19:04,950 --> 00:19:11,610
Dutch where words that begin with I J

00:19:08,809 --> 00:19:14,779
have two title case that as a single

00:19:11,610 --> 00:19:17,519
unit rather than as two characters and

00:19:14,779 --> 00:19:19,980
Unicode simply tells you you need to get

00:19:17,519 --> 00:19:22,230
locale aware rules for the specific

00:19:19,980 --> 00:19:24,690
language you're going to work with it

00:19:22,230 --> 00:19:27,450
handles some of these but nowhere near

00:19:24,690 --> 00:19:31,590
all the complexity that exists in all

00:19:27,450 --> 00:19:33,360
the languages now finally we need to

00:19:31,590 --> 00:19:36,090
understand since we're going to work

00:19:33,360 --> 00:19:38,309
with computers how we actually get this

00:19:36,090 --> 00:19:40,260
into a computer which means how do we

00:19:38,309 --> 00:19:42,870
get it into a binary form how do we

00:19:40,260 --> 00:19:45,630
encode it and decode it going between

00:19:42,870 --> 00:19:48,330
good points which are numeric values but

00:19:45,630 --> 00:19:51,779
kind of abstract to actual bits and

00:19:48,330 --> 00:19:53,260
bytes and to do that we eat a Unicode

00:19:51,779 --> 00:19:55,780
transformation form

00:19:53,260 --> 00:19:56,740
and there are a lot of those I've listed

00:19:55,780 --> 00:19:58,870
some here

00:19:56,740 --> 00:20:03,610
the two you'll see most often are

00:19:58,870 --> 00:20:06,100
probably utf-8 and utf-16 but it's worth

00:20:03,610 --> 00:20:08,440
being aware that most of these are

00:20:06,100 --> 00:20:11,050
variable width they use different

00:20:08,440 --> 00:20:14,440
numbers of bytes to include different

00:20:11,050 --> 00:20:16,360
code points utf-8 for example for a code

00:20:14,440 --> 00:20:19,240
point from the ASCII range only needs

00:20:16,360 --> 00:20:23,710
one byte but for other code points may

00:20:19,240 --> 00:20:26,560
need up to four utf-16 for anything in

00:20:23,710 --> 00:20:29,410
the lowest numbered plane plane zero or

00:20:26,560 --> 00:20:32,740
the basic multilingual plane BMP as

00:20:29,410 --> 00:20:35,350
you'll sometimes see it written uses two

00:20:32,740 --> 00:20:38,290
bytes for anything from higher numbered

00:20:35,350 --> 00:20:40,480
planes uses four bytes it's because

00:20:38,290 --> 00:20:42,760
originally Unicode had just the one

00:20:40,480 --> 00:20:44,440
plane and it had two to the sixteenth

00:20:42,760 --> 00:20:46,270
code points in it so there was an

00:20:44,440 --> 00:20:50,260
assumption that 16 bits ought to be

00:20:46,270 --> 00:20:53,140
enough for anybody right well eventually

00:20:50,260 --> 00:20:56,410
único had added more planes and utf-16

00:20:53,140 --> 00:21:00,130
was developed with a scheme that lets it

00:20:56,410 --> 00:21:03,460
still handle 16-bit units but sometimes

00:21:00,130 --> 00:21:05,050
use two of them per code point the exact

00:21:03,460 --> 00:21:08,050
mechanics if you want to go look it up

00:21:05,050 --> 00:21:11,110
are called surrogate pairs and basically

00:21:08,050 --> 00:21:12,910
there's a segment of plane 0 of Unicode

00:21:11,110 --> 00:21:17,560
that's set aside that will never be

00:21:12,910 --> 00:21:21,550
assigned and utf-16 transforms a code

00:21:17,560 --> 00:21:24,280
point bigger than 16 bits into two code

00:21:21,550 --> 00:21:26,050
points from that range and then you can

00:21:24,280 --> 00:21:28,630
transform them back again to get back

00:21:26,050 --> 00:21:31,720
the original value and this is how

00:21:28,630 --> 00:21:34,600
utf-16 handles those code points that

00:21:31,720 --> 00:21:37,180
are larger than 16 bits but that also

00:21:34,600 --> 00:21:43,000
means that it - now is a variable-width

00:21:37,180 --> 00:21:44,740
encoding and of course we need to

00:21:43,000 --> 00:21:47,170
consider what kind of abstractions we're

00:21:44,740 --> 00:21:48,580
going to expose to a programmer because

00:21:47,170 --> 00:21:50,650
there are different ways we can handle

00:21:48,580 --> 00:21:53,140
strings in programming languages they

00:21:50,650 --> 00:21:56,050
might be sequences of bytes they might

00:21:53,140 --> 00:21:58,150
be sequences of the encoding code unit

00:21:56,050 --> 00:22:00,340
or they might be sequences of code

00:21:58,150 --> 00:22:02,800
points or sequences of graphemes

00:22:00,340 --> 00:22:04,870
and there are trade-offs involved in all

00:22:02,800 --> 00:22:06,559
of these one of the important things to

00:22:04,870 --> 00:22:08,419
be aware of though is

00:22:06,559 --> 00:22:12,440
depending on the abstraction your

00:22:08,419 --> 00:22:15,950
language chose you may or may not be

00:22:12,440 --> 00:22:18,830
able to cause changes in meaning or even

00:22:15,950 --> 00:22:22,490
completely invalidate a sequence of code

00:22:18,830 --> 00:22:24,980
points by cutting into it so for example

00:22:22,490 --> 00:22:26,870
in a language like C where typically

00:22:24,980 --> 00:22:29,240
strings are exposed as a sequence of

00:22:26,870 --> 00:22:31,340
bytes if you arbitrarily cut in the

00:22:29,240 --> 00:22:34,340
middle of that you might cut in the

00:22:31,340 --> 00:22:36,379
middle of a multi byte code unit or you

00:22:34,340 --> 00:22:39,320
might cut in the middle of a code point

00:22:36,379 --> 00:22:41,389
that requires multiple bytes or you

00:22:39,320 --> 00:22:43,129
might be cutting at a code point

00:22:41,389 --> 00:22:45,049
boundary but cutting in the middle of a

00:22:43,129 --> 00:22:48,049
grapheme that's made up of multiple code

00:22:45,049 --> 00:22:50,600
points all of these operations can be

00:22:48,049 --> 00:22:53,119
unsafe and depending on which

00:22:50,600 --> 00:22:56,659
abstraction your language exposes to you

00:22:53,119 --> 00:23:00,559
you may be at risk of different versions

00:22:56,659 --> 00:23:03,080
of these problems now you might be

00:23:00,559 --> 00:23:05,210
wondering well what does Python do and

00:23:03,080 --> 00:23:06,740
he might also be wondering well we're 20

00:23:05,210 --> 00:23:09,019
minutes into this and you haven't really

00:23:06,740 --> 00:23:13,279
talked about Python I thought this was a

00:23:09,019 --> 00:23:17,860
PyCon talk well okay let's talk about

00:23:13,279 --> 00:23:21,230
Python originally there was Python 2 and

00:23:17,860 --> 00:23:25,129
Python 2 story for Unicode was not that

00:23:21,230 --> 00:23:28,580
great in Python 2 the string type was a

00:23:25,129 --> 00:23:31,549
sequence of bytes there was a separate

00:23:28,580 --> 00:23:33,799
type called Unicode that was a Unicode

00:23:31,549 --> 00:23:35,480
string it offered access to lots of

00:23:33,799 --> 00:23:38,830
features of Unicode it could represent

00:23:35,480 --> 00:23:41,480
any code point in Unicode or at least

00:23:38,830 --> 00:23:45,559
sometimes could we'll get to that in a

00:23:41,480 --> 00:23:47,450
minute and you had to know to convert

00:23:45,559 --> 00:23:49,340
back and forth between them and you had

00:23:47,450 --> 00:23:52,220
to know what encoding this thing's came

00:23:49,340 --> 00:23:54,769
from and were going to and Python

00:23:52,220 --> 00:23:57,200
assumed ascii by default for its byte

00:23:54,769 --> 00:23:59,269
strings and even if you told it

00:23:57,200 --> 00:24:01,879
otherwise still a lot of third-party

00:23:59,269 --> 00:24:03,740
modules and other code didn't behave all

00:24:01,879 --> 00:24:06,559
that well when you presented them with

00:24:03,740 --> 00:24:08,629
non ASCII byte sequences or even

00:24:06,559 --> 00:24:12,769
sometimes with just Unicode instances

00:24:08,629 --> 00:24:14,720
and this was generally a mess so now we

00:24:12,769 --> 00:24:17,629
have Python 3 and there was much

00:24:14,720 --> 00:24:20,450
rejoicing because in Python 3 there's

00:24:17,629 --> 00:24:22,880
only one string type and it is a Unicode

00:24:20,450 --> 00:24:25,640
string type there is still a separate

00:24:22,880 --> 00:24:27,920
type for sequences of bytes and you can

00:24:25,640 --> 00:24:30,470
go back and forth between them you can

00:24:27,920 --> 00:24:33,830
take a byte sequence and if you know the

00:24:30,470 --> 00:24:36,110
encoding you can decode it into a string

00:24:33,830 --> 00:24:39,470
or you can take a string and encode it

00:24:36,110 --> 00:24:41,300
into bytes in a particular encoding but

00:24:39,470 --> 00:24:44,270
a lot of the issues that used to exist

00:24:41,300 --> 00:24:46,790
in Python 2 especially with the sort of

00:24:44,270 --> 00:24:49,190
interchangeability that it had for both

00:24:46,790 --> 00:24:53,150
Unicode and byte strings have been

00:24:49,190 --> 00:24:55,310
cleaned up except for one thing that

00:24:53,150 --> 00:24:59,930
persisted for a few releases into the

00:24:55,310 --> 00:25:02,330
Python 3 series and it's this this is

00:24:59,930 --> 00:25:05,240
everybody's favorite emoji the pile of

00:25:02,330 --> 00:25:07,640
poo and if you fire up a Python

00:25:05,240 --> 00:25:10,250
interpreter from Python three point zero

00:25:07,640 --> 00:25:12,680
point one point two there's a good

00:25:10,250 --> 00:25:14,440
chance you will see this result and you

00:25:12,680 --> 00:25:18,320
might be wondering what's going on here

00:25:14,440 --> 00:25:20,780
well earlier versions of Python when you

00:25:18,320 --> 00:25:23,390
compile to the interpreter you made a

00:25:20,780 --> 00:25:26,600
choice as to how it would store Unicode

00:25:23,390 --> 00:25:30,340
internally and effectively the choice

00:25:26,600 --> 00:25:34,010
was between utf-16 and utf-32 so either

00:25:30,340 --> 00:25:37,280
16-bit or 32-bit storage for Unicode

00:25:34,010 --> 00:25:40,460
these were called narrow and wide built

00:25:37,280 --> 00:25:42,620
of Python most people used a narrow

00:25:40,460 --> 00:25:46,430
build and that's where you would see

00:25:42,620 --> 00:25:49,160
this result because that code point is

00:25:46,430 --> 00:25:52,610
too large to fit in a single 16-bit unit

00:25:49,160 --> 00:25:54,500
so an encoding like utf-16 needs to use

00:25:52,610 --> 00:25:57,730
a surrogate pair for it and split it

00:25:54,500 --> 00:26:00,170
across two replacement code points and

00:25:57,730 --> 00:26:02,600
python would expose this to you directly

00:26:00,170 --> 00:26:04,790
if you iterated over this you would see

00:26:02,600 --> 00:26:07,580
two code points from the surrogate range

00:26:04,790 --> 00:26:09,550
in the basic multilingual plane instead

00:26:07,580 --> 00:26:12,860
of the original code point you put in

00:26:09,550 --> 00:26:16,610
now fortunately that's been fixed python

00:26:12,860 --> 00:26:19,700
3.3 changed this implemented a spec from

00:26:16,610 --> 00:26:22,370
pep 393 which did away with the narrow

00:26:19,700 --> 00:26:24,650
and wide builds of python if you want to

00:26:22,370 --> 00:26:27,680
know the details and how that affected

00:26:24,650 --> 00:26:29,960
in memory storage and how it affects the

00:26:27,680 --> 00:26:33,020
capi of python you can go look up the

00:26:29,960 --> 00:26:34,370
pep one other nice takeaway is that it

00:26:33,020 --> 00:26:36,559
means Python strings

00:26:34,370 --> 00:26:39,680
a lot less memory now on average than

00:26:36,559 --> 00:26:41,990
they used to but the big thing for our

00:26:39,680 --> 00:26:44,750
purposes is it means that a Python

00:26:41,990 --> 00:26:46,910
string now really is a sequence of code

00:26:44,750 --> 00:26:49,910
points where previously it was a

00:26:46,910 --> 00:26:51,770
sequence of code units and this is

00:26:49,910 --> 00:26:53,390
actually a test I like to use with

00:26:51,770 --> 00:26:56,660
different languages when I try them out

00:26:53,390 --> 00:26:59,870
is take a string like the pile of poo

00:26:56,660 --> 00:27:03,080
and ask the language how long is this if

00:26:59,870 --> 00:27:04,820
you get an answer of 1 that means the

00:27:03,080 --> 00:27:07,520
language is probably working with either

00:27:04,820 --> 00:27:10,340
code points or graphemes as its string

00:27:07,520 --> 00:27:13,400
abstraction if you get an answer of more

00:27:10,340 --> 00:27:16,490
than 1 then maybe it's working with code

00:27:13,400 --> 00:27:21,020
units like Python used to with it's 2

00:27:16,490 --> 00:27:23,150
byte 16-bit encoding on narrow builds or

00:27:21,020 --> 00:27:25,910
maybe even at something more complex

00:27:23,150 --> 00:27:28,550
like just exposing sequences of utf-8

00:27:25,910 --> 00:27:32,150
bytes which will give you an even larger

00:27:28,550 --> 00:27:34,220
answer on some of the emoji but it's a

00:27:32,150 --> 00:27:37,340
good way to quickly find out what is a

00:27:34,220 --> 00:27:40,250
language doing and what abstraction is

00:27:37,340 --> 00:27:42,140
it exposing when it says it has Unicode

00:27:40,250 --> 00:27:46,490
support because that's an important

00:27:42,140 --> 00:27:48,440
thing to know now as far as Python we

00:27:46,490 --> 00:27:51,559
now have strings which are sequences of

00:27:48,440 --> 00:27:53,809
code points which means that we can find

00:27:51,559 --> 00:27:56,570
out information about them if we grab

00:27:53,809 --> 00:27:59,179
something out of a string so a string of

00:27:56,570 --> 00:28:01,010
length 1 it's a single code point if we

00:27:59,179 --> 00:28:04,850
iterate a string we're iterating over

00:28:01,010 --> 00:28:07,100
code points if we if we take a slice of

00:28:04,850 --> 00:28:09,860
a string or index into a string we're

00:28:07,100 --> 00:28:12,290
getting code points and we can actually

00:28:09,860 --> 00:28:14,420
find out what's the numeric value of a

00:28:12,290 --> 00:28:16,640
code point and we can transform it into

00:28:14,420 --> 00:28:19,580
hexadecimal as is tradition for

00:28:16,640 --> 00:28:21,470
representing Unicode code points there's

00:28:19,580 --> 00:28:24,230
also a module in the standard library

00:28:21,470 --> 00:28:27,920
that's really useful called Unicode data

00:28:24,230 --> 00:28:29,870
and this gives us a lot of access to the

00:28:27,920 --> 00:28:32,690
Unicode databases and the information

00:28:29,870 --> 00:28:34,850
that Unicode provides about its good

00:28:32,690 --> 00:28:36,710
points and characters so we can ask

00:28:34,850 --> 00:28:38,660
questions like what's the name of this

00:28:36,710 --> 00:28:40,700
good point or what category is it

00:28:38,660 --> 00:28:43,970
assigned to what's it's bi-directional

00:28:40,700 --> 00:28:45,560
or combining rendering behavior all of

00:28:43,970 --> 00:28:48,290
which can be useful information to find

00:28:45,560 --> 00:28:50,510
out we also have access

00:28:48,290 --> 00:28:53,450
from that module to Unicode

00:28:50,510 --> 00:28:56,720
normalization and we can use any Unicode

00:28:53,450 --> 00:28:59,210
normalization form we want and so here

00:28:56,720 --> 00:29:03,260
for example we can take that precomposed

00:28:59,210 --> 00:29:05,600
you with umlaut character and decompose

00:29:03,260 --> 00:29:08,900
it into the two code point sequence or

00:29:05,600 --> 00:29:11,600
we can take that precomposed 1/2

00:29:08,900 --> 00:29:13,220
fraction and decompose it using

00:29:11,600 --> 00:29:17,810
compatibility equivalence into that

00:29:13,220 --> 00:29:20,600
sequence of 1/2 we also have access in

00:29:17,810 --> 00:29:23,000
python to case folding which is useful

00:29:20,600 --> 00:29:26,360
gives us access to case insensitive

00:29:23,000 --> 00:29:28,820
comparison and anytime you need to do a

00:29:26,360 --> 00:29:30,410
case insensitive string comparison in

00:29:28,820 --> 00:29:32,780
Python this is what you should be

00:29:30,410 --> 00:29:35,570
reaching for a lot of us probably

00:29:32,780 --> 00:29:38,750
developed habits from earlier days when

00:29:35,570 --> 00:29:41,810
we weren't working with Unicode of upper

00:29:38,750 --> 00:29:43,700
casing or lower casing and a lot of us

00:29:41,810 --> 00:29:45,800
probably still do that when working with

00:29:43,700 --> 00:29:48,530
databases because we may not have a case

00:29:45,800 --> 00:29:51,770
fold abstraction in our sequel libraries

00:29:48,530 --> 00:29:53,510
or in our database but in Python we have

00:29:51,770 --> 00:29:56,750
that available and that's how we should

00:29:53,510 --> 00:29:59,530
be doing case insensitive comparisons of

00:29:56,750 --> 00:30:07,160
strings so everything is wonderful right

00:29:59,530 --> 00:30:10,160
obviously well yes in a way Python 3 and

00:30:07,160 --> 00:30:13,220
especially since three point three does

00:30:10,160 --> 00:30:15,440
a good job of implementing unicode and

00:30:13,220 --> 00:30:17,470
exposing it in a useful way and making

00:30:15,440 --> 00:30:21,350
it relatively easy for us to work with

00:30:17,470 --> 00:30:25,070
but there are still traps and problems

00:30:21,350 --> 00:30:29,000
that we can fall into one of which I'm

00:30:25,070 --> 00:30:31,130
not normally a fan of absolute

00:30:29,000 --> 00:30:33,290
statements but this is one that I will

00:30:31,130 --> 00:30:36,940
get pretty absolute on I will call this

00:30:33,290 --> 00:30:39,530
the golden rule of working with text in

00:30:36,940 --> 00:30:43,190
any sort of programming language not

00:30:39,530 --> 00:30:46,490
just Python is to be aware of your

00:30:43,190 --> 00:30:50,870
program's boundaries and do encoding and

00:30:46,490 --> 00:30:53,540
decoding there and only there and when I

00:30:50,870 --> 00:30:57,050
say boundaries I mean things like if

00:30:53,540 --> 00:30:59,150
your program reads and rapes files then

00:30:57,050 --> 00:31:01,130
that's a boundary when it opens up and

00:30:59,150 --> 00:31:01,850
reads the contents of a file or writes

00:31:01,130 --> 00:31:03,799
the content

00:31:01,850 --> 00:31:06,169
back out of the file system if your

00:31:03,799 --> 00:31:08,480
program talks over a network that's a

00:31:06,169 --> 00:31:10,280
boundary when it sends information out

00:31:08,480 --> 00:31:13,070
over that connection or receives

00:31:10,280 --> 00:31:14,900
information coming in those are the

00:31:13,070 --> 00:31:16,910
points where you should do your encoding

00:31:14,900 --> 00:31:18,830
and decoding those are the points where

00:31:16,910 --> 00:31:21,679
you should be working with bytes objects

00:31:18,830 --> 00:31:24,140
but once you have them included or

00:31:21,679 --> 00:31:26,720
decoded you should be working entirely

00:31:24,140 --> 00:31:29,750
with strings internally you should not

00:31:26,720 --> 00:31:32,559
be passing around bytes objects or

00:31:29,750 --> 00:31:37,309
dealing with included sequences of bytes

00:31:32,559 --> 00:31:39,890
at almost any cost because most older

00:31:37,309 --> 00:31:42,080
approaches and most older Python code

00:31:39,890 --> 00:31:45,230
that had trouble making the jump two to

00:31:42,080 --> 00:31:48,020
three had trouble because of this

00:31:45,230 --> 00:31:50,570
because of mixing of byte strings and

00:31:48,020 --> 00:31:53,270
unicode strings or even just not using

00:31:50,570 --> 00:31:55,940
unicode strings at all in some cases and

00:31:53,270 --> 00:31:58,669
not thinking about encoding and decoding

00:31:55,940 --> 00:32:00,470
and where they needed to happen so this

00:31:58,669 --> 00:32:03,289
is your golden rule if you take nothing

00:32:00,470 --> 00:32:06,140
else away look for the boundaries of

00:32:03,289 --> 00:32:08,059
your program identify what they are do

00:32:06,140 --> 00:32:12,169
your encoding and decoding there and

00:32:08,059 --> 00:32:13,909
only there everywhere else be working

00:32:12,169 --> 00:32:17,270
with strings be working with real

00:32:13,909 --> 00:32:19,090
unicode now there is still some

00:32:17,270 --> 00:32:21,770
difficulty every once in a while

00:32:19,090 --> 00:32:24,590
especially when it comes to working with

00:32:21,770 --> 00:32:28,490
files and especially on certain types of

00:32:24,590 --> 00:32:30,650
UNIX operating systems and these

00:32:28,490 --> 00:32:33,700
problems fall into a few different

00:32:30,650 --> 00:32:37,039
categories there are some systems where

00:32:33,700 --> 00:32:40,130
there's no reliable way to ask the

00:32:37,039 --> 00:32:42,890
system what encoding it uses for its

00:32:40,130 --> 00:32:44,780
file system so you know that something

00:32:42,890 --> 00:32:46,760
like a file name is a sequence of bytes

00:32:44,780 --> 00:32:49,070
but you might not have any way of

00:32:46,760 --> 00:32:51,159
figuring out how to decode that into a

00:32:49,070 --> 00:32:54,039
sequence of Unicode good points

00:32:51,159 --> 00:32:55,940
there are also file systems where

00:32:54,039 --> 00:32:58,909
technically there's not a requirement

00:32:55,940 --> 00:33:01,549
that they be able to decode where a path

00:32:58,909 --> 00:33:04,909
can simply be any arbitrary sequence of

00:33:01,549 --> 00:33:07,210
bytes you want and never validly decode

00:33:04,909 --> 00:33:10,419
in any known encoding

00:33:07,210 --> 00:33:13,120
and Python has made progress over the

00:33:10,419 --> 00:33:15,610
course of the Python 3 release series

00:33:13,120 --> 00:33:18,940
with getting better at this there are

00:33:15,610 --> 00:33:23,230
some tips and tricks and tools and now

00:33:18,940 --> 00:33:27,039
Python mostly does its best to let you

00:33:23,230 --> 00:33:29,559
treat the file system as utf-8 with some

00:33:27,039 --> 00:33:31,799
tricks to handle potentially invalid or

00:33:29,559 --> 00:33:35,140
just completely undecayed Obul paths

00:33:31,799 --> 00:33:38,049
actually what Python does now is similar

00:33:35,140 --> 00:33:39,909
to what utf-16 does where when it

00:33:38,049 --> 00:33:42,940
encounters a byte that can't possibly

00:33:39,909 --> 00:33:46,000
decode as a sequence of code points it

00:33:42,940 --> 00:33:47,470
preserves it as is by transforming it

00:33:46,000 --> 00:33:49,720
into a code point from the surrogate

00:33:47,470 --> 00:33:52,179
pair range and that lets it transform

00:33:49,720 --> 00:33:53,799
back into the original byte again when

00:33:52,179 --> 00:33:57,940
it's time to write things on the file

00:33:53,799 --> 00:33:59,620
system or do other encoding sometimes

00:33:57,940 --> 00:34:02,950
you can work around this by telling

00:33:59,620 --> 00:34:05,200
Python what encoding your file system is

00:34:02,950 --> 00:34:06,940
using sometimes you just have to hope

00:34:05,200 --> 00:34:09,929
that it works because there are some

00:34:06,940 --> 00:34:12,820
systems that are configured hopelessly

00:34:09,929 --> 00:34:15,669
but it is a thing that has gotten better

00:34:12,820 --> 00:34:18,070
it is a thing that now mostly reliably

00:34:15,669 --> 00:34:21,250
works even on those badly configured

00:34:18,070 --> 00:34:23,320
systems which is a big leap forward from

00:34:21,250 --> 00:34:26,500
where it was in the early days of Python

00:34:23,320 --> 00:34:29,950
3 of course there are other problems you

00:34:26,500 --> 00:34:33,119
can run into we've we've definitely seen

00:34:29,950 --> 00:34:35,800
examples of normalizing different forms

00:34:33,119 --> 00:34:37,869
that can sometimes be a destructive

00:34:35,800 --> 00:34:40,240
operation depending on what language

00:34:37,869 --> 00:34:42,820
you're working with some languages

00:34:40,240 --> 00:34:45,550
really rely on the combining and

00:34:42,820 --> 00:34:47,740
composing features of unicode this

00:34:45,550 --> 00:34:50,679
example is Korean but it's not the only

00:34:47,740 --> 00:34:52,800
language that does this that initial

00:34:50,679 --> 00:34:56,919
string is two code points it's two

00:34:52,800 --> 00:35:00,550
composed characters effectively each one

00:34:56,919 --> 00:35:03,310
represents one syllable of the text and

00:35:00,550 --> 00:35:05,410
each syllable is made up of three

00:35:03,310 --> 00:35:08,260
individual characters or three

00:35:05,410 --> 00:35:10,359
individual letters that represent the

00:35:08,260 --> 00:35:11,900
consonants and vowels that go into that

00:35:10,359 --> 00:35:14,180
syllable

00:35:11,900 --> 00:35:16,970
and performing a decomposing

00:35:14,180 --> 00:35:19,700
normalization can result in that

00:35:16,970 --> 00:35:21,530
sequence of as you see below six

00:35:19,700 --> 00:35:24,470
different code points representing the

00:35:21,530 --> 00:35:26,800
constituent parts of those composed

00:35:24,470 --> 00:35:29,030
characters those single syllable

00:35:26,800 --> 00:35:30,920
representations and depending on what

00:35:29,030 --> 00:35:33,350
system you feed them into they may

00:35:30,920 --> 00:35:35,600
render correctly or they may not the

00:35:33,350 --> 00:35:38,180
terminal application I used to generate

00:35:35,600 --> 00:35:40,070
these examples handled this well and

00:35:38,180 --> 00:35:44,360
rendered both of these strings the same

00:35:40,070 --> 00:35:47,090
way this slide does not so this is

00:35:44,360 --> 00:35:50,060
something to be aware of and in general

00:35:47,090 --> 00:35:53,330
combining and composed forms pop up more

00:35:50,060 --> 00:35:55,190
often than people expect a lot of emoji

00:35:53,330 --> 00:35:58,580
for example are made of combining

00:35:55,190 --> 00:36:00,260
sequences the country flags there is a

00:35:58,580 --> 00:36:03,380
set of code points that are called the

00:36:00,260 --> 00:36:05,660
regional indicator symbols and they

00:36:03,380 --> 00:36:08,750
provide an alphabet that lets you spell

00:36:05,660 --> 00:36:10,940
out to letter country codes and then

00:36:08,750 --> 00:36:13,810
those render has the flags of those

00:36:10,940 --> 00:36:17,570
countries so this is basically the

00:36:13,810 --> 00:36:19,670
sequence US spelled out in regional

00:36:17,570 --> 00:36:21,470
indicator symbol code points if you

00:36:19,670 --> 00:36:24,530
wanted something like the flag of Canada

00:36:21,470 --> 00:36:27,110
you would spelled CA if you wanted the

00:36:24,530 --> 00:36:30,410
flag of France you would spell fr and

00:36:27,110 --> 00:36:32,930
that's how the flag emoji work splitting

00:36:30,410 --> 00:36:34,280
these down the middle could just destroy

00:36:32,930 --> 00:36:36,590
the meaning because you wouldn't know

00:36:34,280 --> 00:36:39,560
how to render it anymore the same thing

00:36:36,590 --> 00:36:42,050
is true of a lot of emoji for people for

00:36:39,560 --> 00:36:44,300
example this is a family of four people

00:36:42,050 --> 00:36:48,230
but it's seven different code points

00:36:44,300 --> 00:36:50,720
under the hood and a lot of the emoji

00:36:48,230 --> 00:36:53,000
for people are multi code point

00:36:50,720 --> 00:36:55,940
sequences either composing groups of

00:36:53,000 --> 00:36:58,250
people or composing on modifiers to

00:36:55,940 --> 00:37:01,340
indicate gender or skin tone or other

00:36:58,250 --> 00:37:03,890
attributes and again splitting in the

00:37:01,340 --> 00:37:06,110
middle of them can be destructive it can

00:37:03,890 --> 00:37:09,500
change the meaning or completely destroy

00:37:06,110 --> 00:37:11,750
the meaning of a sequence so we need to

00:37:09,500 --> 00:37:14,870
turn back to that concept we saw earlier

00:37:11,750 --> 00:37:17,070
of the grapheme the minimally

00:37:14,870 --> 00:37:19,440
distinctive unit of meaning

00:37:17,070 --> 00:37:21,810
and unfortunately Python doesn't

00:37:19,440 --> 00:37:26,460
directly give you a way to work with

00:37:21,810 --> 00:37:29,730
graphemes in strings but you can still

00:37:26,460 --> 00:37:31,770
do it using third-party libraries so for

00:37:29,730 --> 00:37:34,080
example this is a third-party regular

00:37:31,770 --> 00:37:37,500
expression library called regex you can

00:37:34,080 --> 00:37:40,140
pip install it and it offers a lot more

00:37:37,500 --> 00:37:42,650
support for unicode than pythons

00:37:40,140 --> 00:37:45,000
standard library regex module does

00:37:42,650 --> 00:37:48,030
including things like filtering and

00:37:45,000 --> 00:37:50,670
matching on unicode properties and most

00:37:48,030 --> 00:37:52,950
importantly for this case it provides a

00:37:50,670 --> 00:37:55,380
meta character for matching unicode

00:37:52,950 --> 00:37:56,610
grapheme clusters and this is actually

00:37:55,380 --> 00:37:58,620
defined in one of the Unicode

00:37:56,610 --> 00:38:01,350
specifications it's supposed to be this

00:37:58,620 --> 00:38:03,930
capital X character but it means that we

00:38:01,350 --> 00:38:06,540
can do things now like count the number

00:38:03,930 --> 00:38:07,290
of graphemes in a string or split on

00:38:06,540 --> 00:38:10,110
graphemes

00:38:07,290 --> 00:38:12,390
or iterate over graphemes rather than

00:38:10,110 --> 00:38:15,990
risking splitting up a grapheme that's

00:38:12,390 --> 00:38:17,760
made up of multiple good points but even

00:38:15,990 --> 00:38:20,340
in the Python standard library there is

00:38:17,760 --> 00:38:22,980
still a lot of awareness of Unicode if

00:38:20,340 --> 00:38:24,920
we go back to for example the regex

00:38:22,980 --> 00:38:27,420
module in the Python standard library

00:38:24,920 --> 00:38:29,580
most of us have probably written code

00:38:27,420 --> 00:38:31,530
like this where we're saying oh okay I

00:38:29,580 --> 00:38:33,660
need to match something that looks like

00:38:31,530 --> 00:38:36,560
a year so it's a sequence of four digits

00:38:33,660 --> 00:38:39,090
and it's gonna be something like 2020

00:38:36,560 --> 00:38:41,970
well it turns out unicode has a much

00:38:39,090 --> 00:38:44,310
broader concept of digit than what

00:38:41,970 --> 00:38:47,010
speakers of English and Western European

00:38:44,310 --> 00:38:49,980
languages do so that second string for

00:38:47,010 --> 00:38:51,660
example pulls digit characters from four

00:38:49,980 --> 00:38:53,490
different blocks and four different

00:38:51,660 --> 00:38:55,890
languages that are represented in

00:38:53,490 --> 00:38:57,870
Unicode but it still matches because

00:38:55,890 --> 00:39:00,810
according to Unicode properties they are

00:38:57,870 --> 00:39:02,910
all digits and this is an important

00:39:00,810 --> 00:39:06,810
thing to be aware of when you're working

00:39:02,910 --> 00:39:10,470
with Python in Python 3 where everything

00:39:06,810 --> 00:39:15,030
is Unicode and things are mostly Unicode

00:39:10,470 --> 00:39:17,160
aware that you need to be explicit as

00:39:15,030 --> 00:39:19,500
the Zen of Python says explicit is

00:39:17,160 --> 00:39:21,450
better than implicit but you need to

00:39:19,500 --> 00:39:24,000
make sure you understand what some of

00:39:21,450 --> 00:39:25,680
these things mean like this digit meta

00:39:24,000 --> 00:39:28,260
character or the other regex meta

00:39:25,680 --> 00:39:30,720
characters and if what you really wanted

00:39:28,260 --> 00:39:33,359
was only to match digits 0 through 9

00:39:30,720 --> 00:39:36,000
from the Latin character set you can say

00:39:33,359 --> 00:39:39,869
that but you do have to be explicit

00:39:36,000 --> 00:39:41,730
about it there can also be difficulty

00:39:39,869 --> 00:39:45,480
with things like performing string

00:39:41,730 --> 00:39:47,520
comparisons and working with different

00:39:45,480 --> 00:39:51,090
strings that potentially write the same

00:39:47,520 --> 00:39:53,540
thing different ways and here it can be

00:39:51,090 --> 00:39:55,490
difficult to give any single answer

00:39:53,540 --> 00:39:57,540
because the answer is usually

00:39:55,490 --> 00:40:00,720
context-sensitive it depends on what

00:39:57,540 --> 00:40:03,180
you're doing so for example here this is

00:40:00,720 --> 00:40:05,070
a fairly simple algorithm but it comes

00:40:03,180 --> 00:40:08,730
from one of the Unicode technical

00:40:05,070 --> 00:40:11,099
reports on security and this is for

00:40:08,730 --> 00:40:12,930
comparing things that might be used as

00:40:11,099 --> 00:40:14,550
identifiers x' things like maybe

00:40:12,930 --> 00:40:17,700
variable names in a programming language

00:40:14,550 --> 00:40:20,369
or user names in an account system and

00:40:17,700 --> 00:40:23,310
this gives you a way to compare them in

00:40:20,369 --> 00:40:25,080
a case insensitive way and figure out

00:40:23,310 --> 00:40:27,680
which ones should be considered

00:40:25,080 --> 00:40:31,349
equivalent and which ones should not

00:40:27,680 --> 00:40:33,869
there are other ways to normalize and

00:40:31,349 --> 00:40:36,540
prepare strings for comparison and for

00:40:33,869 --> 00:40:38,970
use and it really does depend on your

00:40:36,540 --> 00:40:41,790
use case Python supports quite a few of

00:40:38,970 --> 00:40:43,470
them for example if you are working with

00:40:41,790 --> 00:40:46,680
domain names which can be

00:40:43,470 --> 00:40:48,270
internationalized now there are modules

00:40:46,680 --> 00:40:51,960
in the standard library that support

00:40:48,270 --> 00:40:55,080
this the encoding cDNA module the puny

00:40:51,960 --> 00:40:57,540
code codec let you work with these and

00:40:55,080 --> 00:40:59,760
transform internationalized domain names

00:40:57,540 --> 00:41:01,619
into an ASCII compatible form that's

00:40:59,760 --> 00:41:03,859
safe to transmit through a lot of

00:41:01,619 --> 00:41:08,099
systems that maybe aren't aware of

00:41:03,859 --> 00:41:10,500
internationalized domain names there are

00:41:08,099 --> 00:41:12,740
also even trickier things that you can

00:41:10,500 --> 00:41:16,380
get by digging into third-party modules

00:41:12,740 --> 00:41:18,240
for example this is something that comes

00:41:16,380 --> 00:41:21,300
and goes where people will try to fool

00:41:18,240 --> 00:41:24,300
you by writing out a domain name or

00:41:21,300 --> 00:41:26,970
maybe an email address or a user name or

00:41:24,300 --> 00:41:29,339
some other identifier using a mix of

00:41:26,970 --> 00:41:31,589
scripts where some of the characters

00:41:29,339 --> 00:41:34,320
look like each other but aren't actually

00:41:31,589 --> 00:41:36,180
the same and Unicode actually includes a

00:41:34,320 --> 00:41:39,000
database for this called the visually

00:41:36,180 --> 00:41:40,830
confusing characters file and there's a

00:41:39,000 --> 00:41:42,390
third-party module that you can go

00:41:40,830 --> 00:41:44,100
download that has

00:41:42,390 --> 00:41:47,850
this wonderful function in occult is

00:41:44,100 --> 00:41:50,700
dangerous I love that name that tells

00:41:47,850 --> 00:41:53,010
you when a string contains code points

00:41:50,700 --> 00:41:54,750
from multiple scripts and some of them

00:41:53,010 --> 00:41:56,430
appear in that visually confusing

00:41:54,750 --> 00:41:59,970
characters file some of them are

00:41:56,430 --> 00:42:01,950
confusable characters so you can notice

00:41:59,970 --> 00:42:06,120
when somebody's trying to do something

00:42:01,950 --> 00:42:08,250
dangerous and that module also includes

00:42:06,120 --> 00:42:10,380
a lot of other information you can

00:42:08,250 --> 00:42:12,870
access so you can ask it to show you a

00:42:10,380 --> 00:42:14,910
list of what are the confusable

00:42:12,870 --> 00:42:16,920
characters what was it that set this off

00:42:14,910 --> 00:42:19,110
what were the script properties they

00:42:16,920 --> 00:42:21,870
were being used in this string they were

00:42:19,110 --> 00:42:25,020
being mixed together and there's really

00:42:21,870 --> 00:42:28,380
a whole wide world of things out there

00:42:25,020 --> 00:42:31,740
but hopefully at this point you've got a

00:42:28,380 --> 00:42:35,880
handle on the core ideas of what goes

00:42:31,740 --> 00:42:38,460
into Unicode what it is why it's complex

00:42:35,880 --> 00:42:40,020
and where that complexity comes from so

00:42:38,460 --> 00:42:43,110
you can start thinking about that

00:42:40,020 --> 00:42:45,420
complexity in a productive way start

00:42:43,110 --> 00:42:47,820
anticipating where you may need to do

00:42:45,420 --> 00:42:50,580
extra work where you may need to worry

00:42:47,820 --> 00:42:53,190
about something and how you can write

00:42:50,580 --> 00:42:55,620
better more effective code and feel more

00:42:53,190 --> 00:42:58,880
confident about how you're using Unicode

00:42:55,620 --> 00:43:02,180
and of course if you have any questions

00:42:58,880 --> 00:43:04,680
unfortunately this is an online

00:43:02,180 --> 00:43:07,650
presentation because PyCon had to be

00:43:04,680 --> 00:43:09,240
cancelled this year but I am happy to

00:43:07,650 --> 00:43:11,610
have people reach out and ask me

00:43:09,240 --> 00:43:13,620
questions I also keep a blog where I

00:43:11,610 --> 00:43:15,810
regularly rant about all sorts of things

00:43:13,620 --> 00:43:20,250
including unicode which has its own

00:43:15,810 --> 00:43:22,590
category there and finally I want to

00:43:20,250 --> 00:43:25,200
take a minute to just thank the PyCon

00:43:22,590 --> 00:43:26,850
organizers and the PSF because they were

00:43:25,200 --> 00:43:30,930
really put in an impossible situation

00:43:26,850 --> 00:43:34,170
this year and as sad as it is that the

00:43:30,930 --> 00:43:37,320
in-person version of Pi con 2020 had to

00:43:34,170 --> 00:43:39,780
be canceled it really is incredible the

00:43:37,320 --> 00:43:42,000
way that they reacted and responded and

00:43:39,780 --> 00:43:44,790
were able to put together this online

00:43:42,000 --> 00:43:48,030
tract of talks as quickly as they did

00:43:44,790 --> 00:43:51,720
and as successfully as they did so if

00:43:48,030 --> 00:43:55,260
you're watching this please be thankful

00:43:51,720 --> 00:43:56,420
for the PSF for the PyCon organizers and

00:43:55,260 --> 00:43:59,030
for all the work

00:43:56,420 --> 00:44:02,750
put in to putting this online and

00:43:59,030 --> 00:44:05,300
pulling off a remote pike on 2020 on

00:44:02,750 --> 00:44:09,140
such short notice and under the worst

00:44:05,300 --> 00:44:12,050
possible conditions in the meantime stay

00:44:09,140 --> 00:44:16,300
safe hopefully I will see you at a pike

00:44:12,050 --> 00:44:16,300
on in person sometime in the future

00:44:22,920 --> 00:44:24,980

YouTube URL: https://www.youtube.com/watch?v=olhKTHFYNxA


