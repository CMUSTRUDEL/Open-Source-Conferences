Title: Colin Carroll, Karin C. Knudson - Fighting Gerrymandering with PyMC3 - PyCon 2018
Publication date: 2018-08-06
Playlist: Talks
Description: 
	Speakers: Colin Carroll, Karin C. Knudson

At the end of 2017, there were seven states with ongoing redistricting litigation.  We will discuss a statistical model that the United States Supreme Court declared to be appropriate in cases of racial gerrymandering, and show how it can be implemented and used with the library `PyMC3`.  We will also discuss what the model tells us about racial gerrymandering in North Carolina.


Slides can be found at: https://speakerdeck.com/pycon2018 and https://github.com/PyCon/2018-slides
Captions: 
	00:00:11,030 --> 00:00:16,680
hello everybody welcome back and this is

00:00:14,759 --> 00:00:21,500
gonna be fighting gerrymandering with PI

00:00:16,680 --> 00:00:21,500
mc3 with Colin Carroll and Karen Knudsen

00:00:26,660 --> 00:00:31,830
hello thank you all so much for coming

00:00:29,340 --> 00:00:34,530
I'm Colin I'm super excited to be here

00:00:31,830 --> 00:00:36,030
and to give this talk one piece of

00:00:34,530 --> 00:00:39,629
trivia I want to mention before I start

00:00:36,030 --> 00:00:41,790
the pine c3 logo that you see up here is

00:00:39,629 --> 00:00:43,230
beautiful and transparent and it's

00:00:41,790 --> 00:00:45,450
transparent due to a pull request from

00:00:43,230 --> 00:00:47,640
Jake Vander Plaats last year that he

00:00:45,450 --> 00:00:50,129
made on the same day as his keynote at

00:00:47,640 --> 00:00:52,190
PyCon so we can thank him for the

00:00:50,129 --> 00:00:56,370
beautiful logo that we can put up here

00:00:52,190 --> 00:00:58,530
so this past summer Karen was taking

00:00:56,370 --> 00:01:00,780
part in a gerrymandering summer school

00:00:58,530 --> 00:01:02,879
over at Tufts University and discussing

00:01:00,780 --> 00:01:06,150
one of the statistical models that they

00:01:02,879 --> 00:01:07,770
use in talking about redistricting I'm

00:01:06,150 --> 00:01:10,229
one of the core contributors on the PI

00:01:07,770 --> 00:01:13,530
mc3 project this is a project for doing

00:01:10,229 --> 00:01:14,850
statistical inference in Python and it

00:01:13,530 --> 00:01:16,920
sounds to me like this is something we

00:01:14,850 --> 00:01:19,290
could implement in five or six lines of

00:01:16,920 --> 00:01:21,509
PI and c3 so this this talk is really

00:01:19,290 --> 00:01:24,750
going to be covering sort of the result

00:01:21,509 --> 00:01:27,420
of that observation so we're going to

00:01:24,750 --> 00:01:32,430
talk about is sort of the the history of

00:01:27,420 --> 00:01:34,680
the Voting Rights Act what which is

00:01:32,430 --> 00:01:35,909
where this model came from we're going

00:01:34,680 --> 00:01:36,900
to talk about what ecological inference

00:01:35,909 --> 00:01:38,700
is and then we're gonna do some

00:01:36,900 --> 00:01:41,310
computational experiments with real data

00:01:38,700 --> 00:01:44,700
so we took the 2016 North Carolina

00:01:41,310 --> 00:01:46,770
congressional election and we did some

00:01:44,700 --> 00:01:48,689
some analysis on it using open source

00:01:46,770 --> 00:01:51,240
Python tools that are as far as I know

00:01:48,689 --> 00:01:52,680
unique great so I'm going to turn it

00:01:51,240 --> 00:01:56,610
over to Karen fer for sort of a

00:01:52,680 --> 00:01:59,040
discussion of the history in the math hi

00:01:56,610 --> 00:02:01,590
I'm Ken I'm delighted to be here with

00:01:59,040 --> 00:02:04,979
all of you talking about gerrymandering

00:02:01,590 --> 00:02:07,110
and pie mc3 this is where I'm coming

00:02:04,979 --> 00:02:10,349
from very concretely so this red dot on

00:02:07,110 --> 00:02:12,360
the map is Andover Massachusetts it is

00:02:10,349 --> 00:02:13,500
the belly of the beast in the original

00:02:12,360 --> 00:02:15,780
gerrymander

00:02:13,500 --> 00:02:18,120
to gerrymander means to manipulate

00:02:15,780 --> 00:02:20,520
political district boundaries to achieve

00:02:18,120 --> 00:02:24,930
certain ends and the word itself comes

00:02:20,520 --> 00:02:28,410
from the 1812 district boundaries drawn

00:02:24,930 --> 00:02:30,420
here by then governor of Massachusetts

00:02:28,410 --> 00:02:32,180
Elbridge Gerry so if you believe the

00:02:30,420 --> 00:02:35,010
cartoonist looked a little bit like

00:02:32,180 --> 00:02:37,020
salamander and so from gary's salamander

00:02:35,010 --> 00:02:41,010
we got this wonderful word gerrymander

00:02:37,020 --> 00:02:44,640
that we have today i in and over i am

00:02:41,010 --> 00:02:46,230
the chair of the math and statistics and

00:02:44,640 --> 00:02:48,330
computer science department at Phillips

00:02:46,230 --> 00:02:52,260
Academy where I'm teaching a course on

00:02:48,330 --> 00:02:54,000
gerrymandering this spring my background

00:02:52,260 --> 00:02:56,130
where I'm coming from in that realm is

00:02:54,000 --> 00:02:58,530
in mathematics especially looking at

00:02:56,130 --> 00:03:01,440
Bayesian models applied to questions of

00:02:58,530 --> 00:03:04,709
computational neuroscience so I love

00:03:01,440 --> 00:03:08,010
useful Bayesian models which we're gonna

00:03:04,709 --> 00:03:09,660
look at one of later I should say that

00:03:08,010 --> 00:03:14,190
I'm not a political scientist or a

00:03:09,660 --> 00:03:15,600
historian or a legal scholar at all but

00:03:14,190 --> 00:03:17,250
I really enjoyed getting into the

00:03:15,600 --> 00:03:19,350
history of the content and the context

00:03:17,250 --> 00:03:21,360
of this and I hope you'll bear with me

00:03:19,350 --> 00:03:23,730
when we look at some of the history and

00:03:21,360 --> 00:03:25,650
context in a minute I think that it's

00:03:23,730 --> 00:03:27,959
really interesting it's really important

00:03:25,650 --> 00:03:30,209
and it's also key to why you should care

00:03:27,959 --> 00:03:38,519
about the model and the computation that

00:03:30,209 --> 00:03:40,500
we're gonna show later on alright so we

00:03:38,519 --> 00:03:44,220
will start right here with the Voting

00:03:40,500 --> 00:03:46,590
Rights Act of 1965 I should say that

00:03:44,220 --> 00:03:48,810
gerrymandering is a large issue and what

00:03:46,590 --> 00:03:51,120
we're focusing on today is particular

00:03:48,810 --> 00:03:54,150
issues of racial gerrymandering in the

00:03:51,120 --> 00:03:56,730
context of the Voting Rights Act so this

00:03:54,150 --> 00:03:59,190
is a picture of from the march from

00:03:56,730 --> 00:04:02,489
Selma to Montgomery that took place in

00:03:59,190 --> 00:04:06,019
early 1965 so coming on the heels of the

00:04:02,489 --> 00:04:08,010
1964 Civil Rights Act which prohibited

00:04:06,019 --> 00:04:09,930
discrimination racial discrimination in

00:04:08,010 --> 00:04:11,850
public places racial discrimination in

00:04:09,930 --> 00:04:14,430
the workplace but didn't go nearly as

00:04:11,850 --> 00:04:20,160
far as advocates we're hoping in terms

00:04:14,430 --> 00:04:23,220
of voting rights this is a scene from

00:04:20,160 --> 00:04:25,530
the very first day of the Selma to

00:04:23,220 --> 00:04:26,280
Montgomery March so state troopers a

00:04:25,530 --> 00:04:28,230
chat

00:04:26,280 --> 00:04:32,790
the demonstrators in what became known

00:04:28,230 --> 00:04:35,880
as Bloody Sunday Bloody Sunday the

00:04:32,790 --> 00:04:38,850
events were televised and had a big

00:04:35,880 --> 00:04:41,520
impact on public opinion and it was

00:04:38,850 --> 00:04:43,830
against this backdrop of all of this

00:04:41,520 --> 00:04:45,450
activism that eight days later the

00:04:43,830 --> 00:04:48,780
Voting Rights Act was introduced into

00:04:45,450 --> 00:04:50,340
Congress later on in the summer after it

00:04:48,780 --> 00:04:52,920
had passed in the Senate and the house

00:04:50,340 --> 00:04:55,170
lyndon b johnson signed it into law

00:04:52,920 --> 00:04:57,660
the Voting Rights Act has been called

00:04:55,170 --> 00:04:58,650
the single most effective piece of civil

00:04:57,660 --> 00:05:01,940
rights legislation

00:04:58,650 --> 00:05:04,110
ever to be passed in the United States

00:05:01,940 --> 00:05:06,090
it's gonna be helpful for us to have an

00:05:04,110 --> 00:05:08,850
idea of some of the basic anatomy of the

00:05:06,090 --> 00:05:11,610
Voting Rights Act so here we go section

00:05:08,850 --> 00:05:13,530
two is what you might expect to see in a

00:05:11,610 --> 00:05:15,919
Voting Rights Act it generally prohibits

00:05:13,530 --> 00:05:18,840
racial discrimination in voting

00:05:15,919 --> 00:05:21,390
including vote denial and vote dilution

00:05:18,840 --> 00:05:24,419
so this means that it covers not only

00:05:21,390 --> 00:05:27,360
the blatantly discriminatory early

00:05:24,419 --> 00:05:29,700
applied poll taxes and literacy tests

00:05:27,360 --> 00:05:31,979
that were present in the Jim Crow South

00:05:29,700 --> 00:05:34,410
before its passage but also more subtle

00:05:31,979 --> 00:05:39,870
forms of racial discrimination and vote

00:05:34,410 --> 00:05:42,690
dilution so including vote dilution in

00:05:39,870 --> 00:05:45,660
the form of drawing districts such that

00:05:42,690 --> 00:05:47,729
they would divide up a group of minority

00:05:45,660 --> 00:05:49,650
voters into multiple different districts

00:05:47,729 --> 00:05:51,270
so that a group of minority voters who

00:05:49,650 --> 00:05:53,280
could have been combined into one

00:05:51,270 --> 00:05:55,229
district and elect a candidate of choice

00:05:53,280 --> 00:05:57,060
would not be able to do so

00:05:55,229 --> 00:05:59,790
so in gerrymandering we call this

00:05:57,060 --> 00:06:01,770
cracking dividing a group up into

00:05:59,790 --> 00:06:05,810
multiple districts to dilute their power

00:06:01,770 --> 00:06:09,240
it's complement is packing combining

00:06:05,810 --> 00:06:11,910
wasting votes by combining a sort of

00:06:09,240 --> 00:06:13,200
packing a group into one district and if

00:06:11,910 --> 00:06:18,750
you continue the rhyme there's also

00:06:13,200 --> 00:06:20,430
tacking gerrymandering all right I've

00:06:18,750 --> 00:06:22,940
also got section five here this is the

00:06:20,430 --> 00:06:25,680
preclearance section so this says that

00:06:22,940 --> 00:06:27,990
districts that were determined to have a

00:06:25,680 --> 00:06:30,419
history of discriminatory practices in

00:06:27,990 --> 00:06:32,520
voting were subject to an additional

00:06:30,419 --> 00:06:35,340
layer of scrutiny so these are these

00:06:32,520 --> 00:06:38,220
highlighted jurisdictions on the map

00:06:35,340 --> 00:06:40,520
here and in those jurisdictions

00:06:38,220 --> 00:06:43,590
and one wanted to change voting

00:06:40,520 --> 00:06:46,170
procedures say changing voting locations

00:06:43,590 --> 00:06:48,600
redrawing district lines instituting a

00:06:46,170 --> 00:06:51,600
new ID law before those rules went into

00:06:48,600 --> 00:06:53,190
effect they had to go through approval

00:06:51,600 --> 00:06:55,610
by the Department of Justice in

00:06:53,190 --> 00:06:58,140
Washington or the DC district court and

00:06:55,610 --> 00:07:00,360
Section four gave the formula by which

00:06:58,140 --> 00:07:06,780
the jurisdictions that would be covered

00:07:00,360 --> 00:07:09,630
under Section five were determined I use

00:07:06,780 --> 00:07:11,010
the past tense just then because in 2013

00:07:09,630 --> 00:07:14,360
we have an important development in the

00:07:11,010 --> 00:07:16,890
history of the Voting Rights Act I

00:07:14,360 --> 00:07:18,750
should say I should say what the Voting

00:07:16,890 --> 00:07:20,640
Rights Act to that that along with this

00:07:18,750 --> 00:07:24,270
it's not monolithic it was reauthorized

00:07:20,640 --> 00:07:27,120
after its initial passage in the 70s the

00:07:24,270 --> 00:07:31,320
80s the 90s and again in the 2000 so

00:07:27,120 --> 00:07:35,280
reauthorized extended and clarified in

00:07:31,320 --> 00:07:37,050
2013 we have this Supreme Court case

00:07:35,280 --> 00:07:39,240
shelby v holder which rules the

00:07:37,050 --> 00:07:40,980
preclearance the coverage formula for

00:07:39,240 --> 00:07:44,610
which districts will be covered by

00:07:40,980 --> 00:07:45,900
preclearance unconstitutional so it

00:07:44,610 --> 00:07:48,720
technically leaves the idea of

00:07:45,900 --> 00:07:52,200
preclearance intact but no jurisdictions

00:07:48,720 --> 00:07:55,620
are currently covered by this by this

00:07:52,200 --> 00:07:58,020
preclearance requirement it also leaves

00:07:55,620 --> 00:07:59,610
section 2 intact so this is sort of the

00:07:58,020 --> 00:08:02,160
state of affairs right now you can still

00:07:59,610 --> 00:08:05,760
bring section 2 cases of voter

00:08:02,160 --> 00:08:07,800
discrimination but they come sort of one

00:08:05,760 --> 00:08:10,440
by one case by case after the fact

00:08:07,800 --> 00:08:13,590
rather than the sort of prevention that

00:08:10,440 --> 00:08:19,830
the that preclearance provided in

00:08:13,590 --> 00:08:23,550
certain jurisdictions cool so we're

00:08:19,830 --> 00:08:27,229
gonna look at section 2 and this class

00:08:23,550 --> 00:08:29,940
of vote dilution voter dilution cases

00:08:27,229 --> 00:08:31,169
there was helpful clarification that

00:08:29,940 --> 00:08:33,710
came from both the Senate and the

00:08:31,169 --> 00:08:35,820
Supreme Court in the surrounding the

00:08:33,710 --> 00:08:38,010
1980s reauthorization of the Voting

00:08:35,820 --> 00:08:39,839
Rights Act and let's just focus in on

00:08:38,010 --> 00:08:43,440
the second of those 9 Senate factors

00:08:39,839 --> 00:08:45,930
which is that to successfully argue vote

00:08:43,440 --> 00:08:47,640
dilution you need to make a case about

00:08:45,930 --> 00:08:50,670
the degree to which voting in the

00:08:47,640 --> 00:08:51,600
jurisdiction is racially polarized we

00:08:50,670 --> 00:08:54,420
also have the Supreme

00:08:51,600 --> 00:08:56,490
case Thornburg v jingles and we see

00:08:54,420 --> 00:09:00,260
three factors there so first the degree

00:08:56,490 --> 00:09:02,580
to which the minority group is

00:09:00,260 --> 00:09:03,980
sufficiently numerous and compact such

00:09:02,580 --> 00:09:06,960
that you could make a majority-minority

00:09:03,980 --> 00:09:09,390
district and second and thirdly that the

00:09:06,960 --> 00:09:11,760
minority group is politically cohesive

00:09:09,390 --> 00:09:13,670
and that the majority group votes

00:09:11,760 --> 00:09:16,080
sufficiently as a bloc to usually

00:09:13,670 --> 00:09:17,850
prevent the minority group from electing

00:09:16,080 --> 00:09:21,020
some candidate of choice so again to

00:09:17,850 --> 00:09:25,200
rephrase we have another question of

00:09:21,020 --> 00:09:26,310
polarization that would be that would be

00:09:25,200 --> 00:09:28,560
something you would need to prove in

00:09:26,310 --> 00:09:31,680
order to successfully bring a vote

00:09:28,560 --> 00:09:33,690
dilution case cool so that's what's

00:09:31,680 --> 00:09:36,060
gonna bring us to our question of

00:09:33,690 --> 00:09:39,120
interest here this is called ecological

00:09:36,060 --> 00:09:42,330
inference doesn't really have to do very

00:09:39,120 --> 00:09:45,000
much with ecology but it has everything

00:09:42,330 --> 00:09:47,880
to do with the polarization by by

00:09:45,000 --> 00:09:52,530
different demographic groups so here's

00:09:47,880 --> 00:09:55,950
the idea we have information on the

00:09:52,530 --> 00:09:58,320
margins of this table so we have on the

00:09:55,950 --> 00:10:00,480
right side demographic information we

00:09:58,320 --> 00:10:02,550
know in each precinct something about

00:10:00,480 --> 00:10:04,580
how many people are what proportion of

00:10:02,550 --> 00:10:07,410
people fall in one of several

00:10:04,580 --> 00:10:08,880
demographic groups of interest we also

00:10:07,410 --> 00:10:10,800
have election data we know how many

00:10:08,880 --> 00:10:15,270
people voted for each of the political

00:10:10,800 --> 00:10:17,220
parties of interest and then what we

00:10:15,270 --> 00:10:19,920
want to figure out is the internal cells

00:10:17,220 --> 00:10:22,020
here so we'd like to know something like

00:10:19,920 --> 00:10:24,960
what percent of people in demographic

00:10:22,020 --> 00:10:27,750
group one voted for candidate a what

00:10:24,960 --> 00:10:29,910
percentage of people in demographic

00:10:27,750 --> 00:10:31,350
group two voted for candidate a so that

00:10:29,910 --> 00:10:37,050
we can start to build this picture of

00:10:31,350 --> 00:10:40,290
racial polarization to make this a very

00:10:37,050 --> 00:10:41,970
oversimplified but more concrete example

00:10:40,290 --> 00:10:43,380
we can say that you know we have

00:10:41,970 --> 00:10:44,580
information about say the number of

00:10:43,380 --> 00:10:46,860
black people and the number of white

00:10:44,580 --> 00:10:50,100
people in each precinct from the US

00:10:46,860 --> 00:10:51,720
Census say and then we also have

00:10:50,100 --> 00:10:53,760
election information about how many

00:10:51,720 --> 00:10:56,640
people voted Democrat versus Republican

00:10:53,760 --> 00:10:58,500
and we'd like to know the internals of

00:10:56,640 --> 00:11:00,000
this table again so things like what

00:10:58,500 --> 00:11:01,770
percentage of black voters voted

00:11:00,000 --> 00:11:03,120
Democrat versus Republican what

00:11:01,770 --> 00:11:05,160
percentage of white voters voted

00:11:03,120 --> 00:11:07,889
Democrat versus Republican

00:11:05,160 --> 00:11:10,589
and of course we'd really want to nuance

00:11:07,889 --> 00:11:11,970
this with more demographic groups and

00:11:10,589 --> 00:11:13,560
more political groups and one of the

00:11:11,970 --> 00:11:15,420
advantages of the model that we'll look

00:11:13,560 --> 00:11:17,250
at is that it's easy to extend to this

00:11:15,420 --> 00:11:19,589
case so we're gonna continue looking at

00:11:17,250 --> 00:11:21,350
the oversimplified 2 by 2 case but

00:11:19,589 --> 00:11:24,060
that's just for clarity of presentation

00:11:21,350 --> 00:11:27,750
rest assured that it's it's extensible

00:11:24,060 --> 00:11:30,000
to this other case all right our wish

00:11:27,750 --> 00:11:33,750
list for our inference is we'd like to

00:11:30,000 --> 00:11:35,250
have accurate estimates always something

00:11:33,750 --> 00:11:37,050
that one desires we'd like to have

00:11:35,250 --> 00:11:39,930
possible estimates right we'd like our

00:11:37,050 --> 00:11:42,000
voting rates to be between zero and a

00:11:39,930 --> 00:11:43,769
hundred percent you might think that's

00:11:42,000 --> 00:11:45,509
modest but as we'll see it's uh

00:11:43,769 --> 00:11:47,490
something we shouldn't take for granted

00:11:45,509 --> 00:11:49,889
we'd like to quantify our uncertainty

00:11:47,490 --> 00:11:52,110
and we'd like to make our assumptions

00:11:49,889 --> 00:11:54,509
explicit this is where our Bayesian

00:11:52,110 --> 00:11:56,100
model will be very helpful and then

00:11:54,509 --> 00:11:58,529
lastly we'd like our results to be

00:11:56,100 --> 00:12:00,000
clearly communicable two quarts is

00:11:58,529 --> 00:12:01,529
something that's really I thought was

00:12:00,000 --> 00:12:03,750
really cool it's not a constraint that

00:12:01,529 --> 00:12:08,879
I'm used to working with in modeling

00:12:03,750 --> 00:12:11,910
problems um all right this is a state of

00:12:08,879 --> 00:12:13,920
the art for the second half of the 1900s

00:12:11,910 --> 00:12:16,740
this is Goodman's ecological regression

00:12:13,920 --> 00:12:18,870
was heard as a you know a potential

00:12:16,740 --> 00:12:22,579
solution to the ecological inference

00:12:18,870 --> 00:12:25,199
question in different court cases and

00:12:22,579 --> 00:12:26,850
the key observation here that we're

00:12:25,199 --> 00:12:30,269
going to use in in the model we'll be

00:12:26,850 --> 00:12:33,560
looking at is this equation down below

00:12:30,269 --> 00:12:37,019
the table so let's just notice that the

00:12:33,560 --> 00:12:39,899
total proportion of the population that

00:12:37,019 --> 00:12:43,769
goes Democrats say is a linear

00:12:39,899 --> 00:12:46,290
combination of these coefficients of

00:12:43,769 --> 00:12:49,290
interest times the census data that we

00:12:46,290 --> 00:12:52,730
have so the total percentage of voters

00:12:49,290 --> 00:12:55,319
that go Democratic ah is the sum of

00:12:52,730 --> 00:12:56,850
proportion of black voters who vote

00:12:55,319 --> 00:12:58,259
Democratic times proportion of black

00:12:56,850 --> 00:13:00,689
voters plus proportion of white voters

00:12:58,259 --> 00:13:03,329
who vote Democratic times proportion of

00:13:00,689 --> 00:13:05,309
white voters and so on and this is an

00:13:03,329 --> 00:13:09,120
identity that holds deterministically

00:13:05,309 --> 00:13:11,220
for each precinct Goodman's regression

00:13:09,120 --> 00:13:13,230
basically assumes that the polarization

00:13:11,220 --> 00:13:17,040
that we see is exactly the same in every

00:13:13,230 --> 00:13:18,810
precinct and then performs a linear

00:13:17,040 --> 00:13:20,040
regression

00:13:18,810 --> 00:13:23,520
so a couple of problems with this we

00:13:20,040 --> 00:13:26,220
really want to know the precinct level

00:13:23,520 --> 00:13:29,580
estimates for the degree of polarization

00:13:26,220 --> 00:13:31,950
which Goodman's ecological regression

00:13:29,580 --> 00:13:36,240
erases so that's that's a bit of a

00:13:31,950 --> 00:13:37,920
bummer another another drawback to this

00:13:36,240 --> 00:13:41,940
method is that it's pretty easy for it

00:13:37,920 --> 00:13:43,500
to produce estimates like 105 percent of

00:13:41,940 --> 00:13:44,730
Hispanic voters vote Democratic so

00:13:43,500 --> 00:13:48,900
something that should be theoretically

00:13:44,730 --> 00:13:50,280
impossible a last drawback and I love

00:13:48,900 --> 00:13:52,170
this this is one of that this is an

00:13:50,280 --> 00:13:54,060
expert witness complaining and in some

00:13:52,170 --> 00:13:56,070
in a case in the 80s that Goodman

00:13:54,060 --> 00:13:57,330
psychological regression requires and

00:13:56,070 --> 00:13:59,970
all but the smallest jurisdictions

00:13:57,330 --> 00:14:03,120
reliance on computers to perform the

00:13:59,970 --> 00:14:05,700
calculations so I think this is a few

00:14:03,120 --> 00:14:07,170
years before Python arrived on the scene

00:14:05,700 --> 00:14:08,970
but maybe now if we're not too bummed

00:14:07,170 --> 00:14:14,520
out about about this particular drawback

00:14:08,970 --> 00:14:16,200
we could all right okay I'm gonna walk

00:14:14,520 --> 00:14:20,130
you through the model this is stuff that

00:14:16,200 --> 00:14:23,460
I love Bayesian modeling so if you love

00:14:20,130 --> 00:14:25,710
it too that's awesome and we can and

00:14:23,460 --> 00:14:27,810
please ask questions find me afterwards

00:14:25,710 --> 00:14:29,550
I'll go through it pretty quickly the

00:14:27,810 --> 00:14:32,190
thing that I mostly want to highlight is

00:14:29,550 --> 00:14:35,670
how the equations that we're going to

00:14:32,190 --> 00:14:38,850
use here to specify the model really

00:14:35,670 --> 00:14:40,320
translate very intuitively to the code

00:14:38,850 --> 00:14:45,440
that Colin is going to show you when we

00:14:40,320 --> 00:14:49,560
look at the implementing this in PI MC 3

00:14:45,440 --> 00:14:52,800
all right so here we go at the bottom

00:14:49,560 --> 00:14:56,960
we've got our election data that's that

00:14:52,800 --> 00:15:00,600
shaded circle there so proportion

00:14:56,960 --> 00:15:03,030
actually will switch to number of voters

00:15:00,600 --> 00:15:05,160
who voted for a particular party we're

00:15:03,030 --> 00:15:09,030
going to model that as being drawn from

00:15:05,160 --> 00:15:13,620
a binomial distribution with some base

00:15:09,030 --> 00:15:15,240
rate of success rate of the total

00:15:13,620 --> 00:15:18,600
proportion within a precinct that goes

00:15:15,240 --> 00:15:20,130
for a particular party and again that

00:15:18,600 --> 00:15:21,720
proportion comes from this linear

00:15:20,130 --> 00:15:24,990
combination that involves the census

00:15:21,720 --> 00:15:26,760
data and our parameters of interest we

00:15:24,990 --> 00:15:28,430
assume that we have this for each of P

00:15:26,760 --> 00:15:31,050
precincts that's this little dashed

00:15:28,430 --> 00:15:31,910
rectangle is trying to this is

00:15:31,050 --> 00:15:34,399
indicating

00:15:31,910 --> 00:15:39,199
and then we're gonna add a level of

00:15:34,399 --> 00:15:41,300
hierarchy here so we want to share

00:15:39,199 --> 00:15:43,370
statistical strength across precincts

00:15:41,300 --> 00:15:44,779
that fall in one congressional district

00:15:43,370 --> 00:15:46,579
so we're going to assume that we also

00:15:44,779 --> 00:15:48,740
have congressional district level

00:15:46,579 --> 00:15:50,480
parameters that control the

00:15:48,740 --> 00:15:53,089
distributions of our precinct level

00:15:50,480 --> 00:15:56,269
parameters of interest and then those

00:15:53,089 --> 00:15:58,430
congressional level parameters are

00:15:56,269 --> 00:16:02,750
themselves uncertain so we will also

00:15:58,430 --> 00:16:05,350
model them as random variables drawn

00:16:02,750 --> 00:16:08,209
from their own probability distribution

00:16:05,350 --> 00:16:11,480
so this is a model introduced by King

00:16:08,209 --> 00:16:15,949
Rosen and Tanner in 1999 and refined in

00:16:11,480 --> 00:16:17,779
2001 nice things about this are that it

00:16:15,949 --> 00:16:20,120
produces estimates that are always going

00:16:17,779 --> 00:16:22,430
to be between zero and one between zero

00:16:20,120 --> 00:16:24,589
and a hundred percent it also makes it

00:16:22,430 --> 00:16:27,740
really easy to include covariance so if

00:16:24,589 --> 00:16:30,399
you want for example to include precinct

00:16:27,740 --> 00:16:33,139
wide precinct by precinct information on

00:16:30,399 --> 00:16:35,300
education level or income level or

00:16:33,139 --> 00:16:38,389
geographic information that you think

00:16:35,300 --> 00:16:40,850
would affect the polarization rates you

00:16:38,389 --> 00:16:42,740
can do so pretty easily it also extends

00:16:40,850 --> 00:16:45,139
nicely to having more demographic groups

00:16:42,740 --> 00:16:46,399
and more political groups and of course

00:16:45,139 --> 00:16:48,500
you can do both of those things at the

00:16:46,399 --> 00:16:51,529
same time have more categories and

00:16:48,500 --> 00:16:54,079
covariance and now I'm gonna turn it

00:16:51,529 --> 00:16:56,389
over to Colin so that you can see how

00:16:54,079 --> 00:17:01,130
this all plays in practice and with some

00:16:56,389 --> 00:17:02,959
real data great so thanks so much Karen

00:17:01,130 --> 00:17:03,980
so she is the hard part of this talk

00:17:02,959 --> 00:17:06,110
going through all the math and

00:17:03,980 --> 00:17:09,530
everything I I just get to write a lot

00:17:06,110 --> 00:17:11,209
of code and have some fun with this so I

00:17:09,530 --> 00:17:13,100
want to start with a with a simulation

00:17:11,209 --> 00:17:15,289
study so before we get to the North

00:17:13,100 --> 00:17:17,990
Carolina data we're gonna generate our

00:17:15,289 --> 00:17:19,970
own data which has two or maybe three if

00:17:17,990 --> 00:17:21,980
you like looking at this adorable puppy

00:17:19,970 --> 00:17:25,130
what you really should his name is Pete

00:17:21,980 --> 00:17:26,689
he's really nice so the the two benefits

00:17:25,130 --> 00:17:29,659
we'll have of doing a simulation study

00:17:26,689 --> 00:17:31,880
first one is that we'll be able to see

00:17:29,659 --> 00:17:33,860
all the unknowns right so we're gonna be

00:17:31,880 --> 00:17:35,030
trying to recover some rates that we

00:17:33,860 --> 00:17:37,250
won't know for sure in the North

00:17:35,030 --> 00:17:39,169
Carolina data if we simulate this data

00:17:37,250 --> 00:17:41,270
first we'll be able to double check our

00:17:39,169 --> 00:17:43,130
work the second benefit we're going to

00:17:41,270 --> 00:17:44,400
see is that we'll think about our data

00:17:43,130 --> 00:17:45,930
generating process

00:17:44,400 --> 00:17:48,120
so in any kind of machine learning you

00:17:45,930 --> 00:17:49,530
do this is an important step especially

00:17:48,120 --> 00:17:50,670
in Bayesian modeling this is gonna be an

00:17:49,530 --> 00:17:53,550
important step you want to think about

00:17:50,670 --> 00:17:55,950
how your data is generated so there's a

00:17:53,550 --> 00:17:57,810
lot of notation going on here there's a

00:17:55,950 --> 00:18:00,120
lot of political groups and demographic

00:17:57,810 --> 00:18:02,250
groups so in the following example we're

00:18:00,120 --> 00:18:04,710
gonna imagine we're trying to talk about

00:18:02,250 --> 00:18:07,620
people for president he's an adorable

00:18:04,710 --> 00:18:10,290
dog you should vote for him he's got a

00:18:07,620 --> 00:18:11,610
simple platform and we'll just talk

00:18:10,290 --> 00:18:14,420
about trying to figure out the majority

00:18:11,610 --> 00:18:16,530
versus minority percent of vote for Pete

00:18:14,420 --> 00:18:20,850
who wouldn't vote for those eyes you

00:18:16,530 --> 00:18:24,510
know so simulating data we're gonna use

00:18:20,850 --> 00:18:25,890
numpy so this is numerical Python first

00:18:24,510 --> 00:18:28,050
I'm gonna set up a number of precincts

00:18:25,890 --> 00:18:30,090
so I chose 14 because that's the most

00:18:28,050 --> 00:18:32,820
precincts I could display on a single

00:18:30,090 --> 00:18:34,440
slide for you guys to look at later

00:18:32,820 --> 00:18:36,120
the hidden values these are the values

00:18:34,440 --> 00:18:37,860
that we're gonna want to know later so

00:18:36,120 --> 00:18:39,750
this is the percent of the minority

00:18:37,860 --> 00:18:41,070
group that votes for Pete and the

00:18:39,750 --> 00:18:45,510
percent of the majority group that votes

00:18:41,070 --> 00:18:47,340
repeat numpy the random the numpy random

00:18:45,510 --> 00:18:50,400
module makes it easy to generate vectors

00:18:47,340 --> 00:18:52,710
of length 14 the minority vote for Pete

00:18:50,400 --> 00:18:55,380
will be between 70 and a hundred percent

00:18:52,710 --> 00:18:57,060
the average will be 85% and the minority

00:18:55,380 --> 00:18:59,490
vote for Pete will be between 0 and 30

00:18:57,060 --> 00:19:01,560
percent and the average will be 15% so

00:18:59,490 --> 00:19:02,760
this is a pretty simple setup we're

00:19:01,560 --> 00:19:04,620
assuming that it's going to be polarized

00:19:02,760 --> 00:19:09,120
and we should hope that our model can

00:19:04,620 --> 00:19:11,010
recover recover that fact this is

00:19:09,120 --> 00:19:12,690
observed data so we're gonna be able to

00:19:11,010 --> 00:19:15,090
see this data from either the census

00:19:12,690 --> 00:19:17,340
results or the election results we're

00:19:15,090 --> 00:19:20,400
gonna look at what percent of the total

00:19:17,340 --> 00:19:21,420
population is in the minority group this

00:19:20,400 --> 00:19:24,180
is going to be a random number between

00:19:21,420 --> 00:19:26,490
zero and a hundred percent notice that

00:19:24,180 --> 00:19:27,450
it could be up to above 50 percent so

00:19:26,490 --> 00:19:28,680
just because we're calling it the

00:19:27,450 --> 00:19:30,420
minority group doesn't mean it's the

00:19:28,680 --> 00:19:32,670
minority demographic in that particular

00:19:30,420 --> 00:19:34,740
precinct so this will be another vector

00:19:32,670 --> 00:19:36,960
of length 14 and then finally the

00:19:34,740 --> 00:19:38,520
percent vote for Pete so this is going

00:19:36,960 --> 00:19:39,960
to be this linear combination Karen was

00:19:38,520 --> 00:19:41,640
talking about earlier so somehow we're

00:19:39,960 --> 00:19:44,280
mixing up our our unobserved variables

00:19:41,640 --> 00:19:46,650
and the goal of our model is going to be

00:19:44,280 --> 00:19:48,630
to unmix up this is uh these unobserved

00:19:46,650 --> 00:19:50,640
variables so this will be the percent

00:19:48,630 --> 00:19:53,700
vote for Pete we'll see that you know

00:19:50,640 --> 00:19:55,860
election night and just to make this

00:19:53,700 --> 00:19:57,570
neat we're going to also simulate the

00:19:55,860 --> 00:19:58,300
voting population as some number between

00:19:57,570 --> 00:20:01,090
00:19:58,300 --> 00:20:03,700
10,000 so this be another length 14

00:20:01,090 --> 00:20:05,230
array of integers and then we can

00:20:03,700 --> 00:20:06,640
multiply that by the percent voting for

00:20:05,230 --> 00:20:10,090
Pete to get the actual number of votes

00:20:06,640 --> 00:20:12,820
for Pete in each precinct okay so we've

00:20:10,090 --> 00:20:14,500
got a bunch of vectors of length 14 and

00:20:12,820 --> 00:20:16,390
then we can go that's where it comes

00:20:14,500 --> 00:20:19,120
from then we can go plug this into PI MC

00:20:16,390 --> 00:20:22,720
3 I'm not going to go over much about

00:20:19,120 --> 00:20:24,220
how pi MC 3 works or models but I just

00:20:22,720 --> 00:20:25,780
want to show you put this up here and

00:20:24,220 --> 00:20:27,790
sort of purposely small font

00:20:25,780 --> 00:20:29,020
so maybe you can squint and see some of

00:20:27,790 --> 00:20:32,740
the distributions that Karen was talking

00:20:29,020 --> 00:20:35,560
about earlier the other thing I want to

00:20:32,740 --> 00:20:38,470
emphasize here is that oh you can't see

00:20:35,560 --> 00:20:40,540
but the very last line is p.m. sample

00:20:38,470 --> 00:20:42,460
and that's all you call and there's just

00:20:40,540 --> 00:20:44,380
a lot of pretty cutting edge statistics

00:20:42,460 --> 00:20:47,640
and algorithms going on behind that one

00:20:44,380 --> 00:20:49,540
function call Thomas VQ one of the other

00:20:47,640 --> 00:20:52,780
developers on this project calls that

00:20:49,540 --> 00:20:54,010
the inference button and so it lets it

00:20:52,780 --> 00:20:56,080
lets the developers who are thinking

00:20:54,010 --> 00:20:58,740
about how to do this well come up with

00:20:56,080 --> 00:21:00,940
the right way to sample from your model

00:20:58,740 --> 00:21:02,560
I also want to mention on this slide

00:21:00,940 --> 00:21:04,030
that until about five years ago it would

00:21:02,560 --> 00:21:06,220
have been impossible for a

00:21:04,030 --> 00:21:08,740
general-purpose language to sample a

00:21:06,220 --> 00:21:11,350
model like this so so there are some

00:21:08,740 --> 00:21:13,540
interesting algorithms developed in 2013

00:21:11,350 --> 00:21:14,950
and PI MC 3 is one of the

00:21:13,540 --> 00:21:16,810
implementations of this algorithm in

00:21:14,950 --> 00:21:18,430
Python so if you really want to do

00:21:16,810 --> 00:21:23,170
cutting-edge statistical inference PI MC

00:21:18,430 --> 00:21:26,260
3 is a great choice great so what's

00:21:23,170 --> 00:21:28,570
what's the output of a PI MC 3 analysis

00:21:26,260 --> 00:21:29,470
if you're familiar with scikit-learn if

00:21:28,570 --> 00:21:32,140
you're familiar with deep learning

00:21:29,470 --> 00:21:34,090
libraries you'll often input points and

00:21:32,140 --> 00:21:36,250
you'll get out points as your as your

00:21:34,090 --> 00:21:38,560
estimates in pi MC 3 and probabilistic

00:21:36,250 --> 00:21:40,210
programming you're using probability

00:21:38,560 --> 00:21:41,860
distributions as your input and your

00:21:40,210 --> 00:21:43,930
getting probability distributions as

00:21:41,860 --> 00:21:45,880
your output and so what we're actually

00:21:43,930 --> 00:21:49,150
going to see here is a distribution of

00:21:45,880 --> 00:21:51,850
predictions so I've plotted here in red

00:21:49,150 --> 00:21:54,250
the estimated percent of the majority

00:21:51,850 --> 00:21:56,440
demographic that votes for Pete the

00:21:54,250 --> 00:21:57,760
dotted vertical line is the true value

00:21:56,440 --> 00:21:59,320
that's the value that we won't see in

00:21:57,760 --> 00:22:00,970
the North Carolina data but we can see

00:21:59,320 --> 00:22:03,220
because we simulated the data ourselves

00:22:00,970 --> 00:22:04,990
so you can see it's not exactly at the

00:22:03,220 --> 00:22:07,780
peak it's near the peak and that's

00:22:04,990 --> 00:22:09,370
because the the histogram that you're

00:22:07,780 --> 00:22:10,750
viewing there is essentially likely

00:22:09,370 --> 00:22:13,160
scenarios for

00:22:10,750 --> 00:22:16,070
that would cause the data that we

00:22:13,160 --> 00:22:17,570
observed happen so what time c3 did was

00:22:16,070 --> 00:22:19,310
it went through and it simulated in this

00:22:17,570 --> 00:22:21,080
case five thousand different worlds that

00:22:19,310 --> 00:22:22,370
that might be reasonable given the data

00:22:21,080 --> 00:22:25,550
that we observed and the model that we

00:22:22,370 --> 00:22:27,920
specified I'm gonna plot also the

00:22:25,550 --> 00:22:30,260
minority demographic vote for Pete on

00:22:27,920 --> 00:22:32,450
the same axis so you can see here again

00:22:30,260 --> 00:22:34,190
it was pretty close to the peak of the

00:22:32,450 --> 00:22:36,610
distribution if you're using a library

00:22:34,190 --> 00:22:39,890
like scikit-learn or like some sort of

00:22:36,610 --> 00:22:41,000
deep learning library perhaps you would

00:22:39,890 --> 00:22:42,230
typically get the peak of these

00:22:41,000 --> 00:22:44,360
distributions and that would just be a

00:22:42,230 --> 00:22:46,280
single point as output that would also

00:22:44,360 --> 00:22:47,960
be much faster than using pi mc3 so the

00:22:46,280 --> 00:22:49,610
reason you were using pi mc3 here is

00:22:47,960 --> 00:22:51,680
because we want to reason about our

00:22:49,610 --> 00:22:53,300
uncertainty about these estimates so the

00:22:51,680 --> 00:22:56,810
fact that we get these histograms is is

00:22:53,300 --> 00:22:58,250
a feature and now I said we had 14

00:22:56,810 --> 00:22:59,630
precincts right you can't really

00:22:58,250 --> 00:23:01,610
gerrymander just with one precinct

00:22:59,630 --> 00:23:03,920
because then everyone gets to vote but

00:23:01,610 --> 00:23:06,350
here we've got 14 precincts and I'm

00:23:03,920 --> 00:23:09,140
showing you the the estimates that our

00:23:06,350 --> 00:23:12,080
model gets Kings model gets really

00:23:09,140 --> 00:23:13,910
overall 14 of these these precincts and

00:23:12,080 --> 00:23:15,470
so you can see often it ends up near the

00:23:13,910 --> 00:23:16,610
peak of the distributions sometimes it

00:23:15,470 --> 00:23:17,900
doesn't sometimes we're a little bit

00:23:16,610 --> 00:23:22,720
surprised but usually we're not that

00:23:17,900 --> 00:23:24,770
surprised by what the true value was and

00:23:22,720 --> 00:23:26,540
what I want you to take away from this

00:23:24,770 --> 00:23:28,580
slide is that when we go forward and see

00:23:26,540 --> 00:23:30,770
the North Carolina results we think this

00:23:28,580 --> 00:23:32,060
is a reasonable model sometimes we're

00:23:30,770 --> 00:23:33,500
pretty uncertain about things you know

00:23:32,060 --> 00:23:34,970
in precinct 6 we're pretty uncertain

00:23:33,500 --> 00:23:37,940
about where the true vote might have

00:23:34,970 --> 00:23:41,870
been but but at least it's directionally

00:23:37,940 --> 00:23:43,250
correct great so I want to go and talk

00:23:41,870 --> 00:23:45,470
about the actual data the real data

00:23:43,250 --> 00:23:47,930
we're using two big data sources for

00:23:45,470 --> 00:23:50,570
this open elections which is a wonderful

00:23:47,930 --> 00:23:52,100
project it's it's working in Python and

00:23:50,570 --> 00:23:54,500
they're collecting precinct level

00:23:52,100 --> 00:23:57,050
election results for all across the US

00:23:54,500 --> 00:23:59,180
it's it's really incredible and the

00:23:57,050 --> 00:24:00,980
other source is the US census data

00:23:59,180 --> 00:24:03,050
that's I think that's a more popular

00:24:00,980 --> 00:24:07,010
data source it's it's just a treasure

00:24:03,050 --> 00:24:09,530
trove the two of them don't talk to each

00:24:07,010 --> 00:24:12,230
other very well so the the census blocks

00:24:09,530 --> 00:24:15,770
and the precincts do not match up very

00:24:12,230 --> 00:24:17,960
well so in the in the repository that

00:24:15,770 --> 00:24:20,090
will distribute for this talk I've got a

00:24:17,960 --> 00:24:22,880
manual labeling of a lot of the census

00:24:20,090 --> 00:24:24,240
blocks back over to precincts and the

00:24:22,880 --> 00:24:25,920
precinct level labels

00:24:24,240 --> 00:24:27,120
but this this was very much my

00:24:25,920 --> 00:24:28,920
experience when we were first trying to

00:24:27,120 --> 00:24:30,210
do this I figured it would be you know

00:24:28,920 --> 00:24:33,390
an hour to in the afternoon and we'd be

00:24:30,210 --> 00:24:37,200
all done but geographic joining is quite

00:24:33,390 --> 00:24:38,460
messy I also want to call out open

00:24:37,200 --> 00:24:40,260
elections for the great work they're

00:24:38,460 --> 00:24:42,179
doing so these are some of the pictures

00:24:40,260 --> 00:24:44,610
they've sent out of the precinct level

00:24:42,179 --> 00:24:45,960
results they've gotten on the left is

00:24:44,610 --> 00:24:48,540
Litchfield Connecticut which is where my

00:24:45,960 --> 00:24:50,580
parents live and the handwriting on this

00:24:48,540 --> 00:24:53,790
these election results look suspiciously

00:24:50,580 --> 00:24:54,840
like my mother's sorry I'd like this

00:24:53,790 --> 00:24:56,100
Mother's Day weekend to give her a

00:24:54,840 --> 00:24:58,740
special shout out I don't think it's

00:24:56,100 --> 00:25:01,800
actually her but um you know it's it

00:24:58,740 --> 00:25:03,960
gives you pause and this this on the

00:25:01,800 --> 00:25:05,850
bottom is especially impressive so this

00:25:03,960 --> 00:25:07,500
is some election results from Texas I

00:25:05,850 --> 00:25:09,660
believe and it's a picture of a

00:25:07,500 --> 00:25:14,040
spreadsheet embedded in an Excel

00:25:09,660 --> 00:25:15,809
spreadsheet so that that's just an

00:25:14,040 --> 00:25:20,580
incredible level of dedication to like

00:25:15,809 --> 00:25:23,130
data obscurity so at the end of the day

00:25:20,580 --> 00:25:25,740
after doing all this manual labeling I

00:25:23,130 --> 00:25:28,710
was able to match up about 6.8 million

00:25:25,740 --> 00:25:32,220
out of 7.1 million voting age people in

00:25:28,710 --> 00:25:34,050
North Carolina and about two-thirds find

00:25:32,220 --> 00:25:35,550
about two-thirds of the votes most of

00:25:34,050 --> 00:25:37,050
the votes that are missing were absentee

00:25:35,550 --> 00:25:39,000
or mail-in ballots which don't belong to

00:25:37,050 --> 00:25:40,260
any individual precinct so we would have

00:25:39,000 --> 00:25:42,570
to think more about how to involve those

00:25:40,260 --> 00:25:43,770
in the model what I've got up here is a

00:25:42,570 --> 00:25:45,720
map of North Carolina with the

00:25:43,770 --> 00:25:47,760
congressional districts drawn you'll see

00:25:45,720 --> 00:25:49,140
some little little islands of lines in

00:25:47,760 --> 00:25:51,750
there and that's those are the precincts

00:25:49,140 --> 00:25:53,850
that are missing from my data set so so

00:25:51,750 --> 00:25:57,320
that was just not being able to match up

00:25:53,850 --> 00:25:59,730
the census with the precinct level data

00:25:57,320 --> 00:26:01,500
we can layer on the precincts onto this

00:25:59,730 --> 00:26:04,230
so we found about 2600 we had about

00:26:01,500 --> 00:26:06,360
2,600 precincts in our dataset and we're

00:26:04,230 --> 00:26:08,400
gonna have estimates for each of those

00:26:06,360 --> 00:26:10,500
and I should mention that in King's

00:26:08,400 --> 00:26:12,750
original paper he did this for 260

00:26:10,500 --> 00:26:14,400
voting precincts so we're about an order

00:26:12,750 --> 00:26:17,940
of magnitude bigger with the amount of

00:26:14,400 --> 00:26:19,230
data we're working with here and and

00:26:17,940 --> 00:26:23,520
just to give you another idea this is

00:26:19,230 --> 00:26:25,920
the end result blue is that votes for

00:26:23,520 --> 00:26:27,570
Democratic candidates and red or is is

00:26:25,920 --> 00:26:29,790
voting more for Republican candidates we

00:26:27,570 --> 00:26:32,700
did we did just omit third party voting

00:26:29,790 --> 00:26:33,809
in this in this analysis which I invite

00:26:32,700 --> 00:26:37,200
everyone to give us a hard time for

00:26:33,809 --> 00:26:38,090
later but but so this this data is very

00:26:37,200 --> 00:26:40,760
easy for you to get

00:26:38,090 --> 00:26:42,500
anta see the last thing i want to say

00:26:40,760 --> 00:26:45,110
before i show that the end results of

00:26:42,500 --> 00:26:47,510
running that same pi MC 3 model is exit

00:26:45,110 --> 00:26:48,710
polls so there is no exit polling for

00:26:47,510 --> 00:26:50,690
congressional data this is for the

00:26:48,710 --> 00:26:53,120
governor and the Senate race and there's

00:26:50,690 --> 00:26:54,530
only 5,000 people interviewed so this is

00:26:53,120 --> 00:26:56,960
not some place where you could recover

00:26:54,530 --> 00:26:57,530
the analysis we're doing for all 2,600

00:26:56,960 --> 00:26:59,300
precincts

00:26:57,530 --> 00:27:02,420
however directionally you might expect

00:26:59,300 --> 00:27:04,730
to see 75% of non-white voters vote

00:27:02,420 --> 00:27:07,970
Democrat and about 35% of white voters

00:27:04,730 --> 00:27:09,680
vote Democrat so that further adieu this

00:27:07,970 --> 00:27:13,250
is this is that ecological inference

00:27:09,680 --> 00:27:15,130
model this is on North Carolina 1 and

00:27:13,250 --> 00:27:17,600
I'm just showing a random sampling of 14

00:27:15,130 --> 00:27:20,540
precincts so you can see here it was

00:27:17,600 --> 00:27:23,090
quite divided we had some amount of

00:27:20,540 --> 00:27:24,230
uncertainty over the white vote in in a

00:27:23,090 --> 00:27:26,540
couple of these precincts you can see

00:27:24,230 --> 00:27:28,340
it's really spread out but the non-white

00:27:26,540 --> 00:27:31,730
vote is often very close over to 100%

00:27:28,340 --> 00:27:34,280
for the Democratic candidate this one

00:27:31,730 --> 00:27:36,620
this one went in a big way for the

00:27:34,280 --> 00:27:38,120
Democratic candidate here's another

00:27:36,620 --> 00:27:41,330
precinct that or another congressional

00:27:38,120 --> 00:27:43,580
district that also went Democrat you can

00:27:41,330 --> 00:27:45,200
see here every single non-white precinct

00:27:43,580 --> 00:27:46,970
we found we were estimating was almost

00:27:45,200 --> 00:27:49,220
entirely voting for the Democratic

00:27:46,970 --> 00:27:52,970
candidate whereas the non-white vote was

00:27:49,220 --> 00:27:55,250
more towards the middle this was a

00:27:52,970 --> 00:27:56,600
landslide Republican congressional

00:27:55,250 --> 00:27:58,190
district you can see there's a lot of

00:27:56,600 --> 00:28:00,080
uncertainty over the non-white vote but

00:27:58,190 --> 00:28:02,240
the the white vote was very much

00:28:00,080 --> 00:28:06,020
clustered around 0% further had the

00:28:02,240 --> 00:28:07,340
Democrats or let's say under 20% in the

00:28:06,020 --> 00:28:09,590
last one this was actually the closest

00:28:07,340 --> 00:28:12,530
race in North Carolina in 2016 so this

00:28:09,590 --> 00:28:14,120
was decided by 12 points and you can see

00:28:12,530 --> 00:28:17,510
that the the non-white vote was very

00:28:14,120 --> 00:28:19,010
much clustered up above 90% whereas the

00:28:17,510 --> 00:28:22,730
white vote was sort of spread out a

00:28:19,010 --> 00:28:25,030
little bit more great and I want to

00:28:22,730 --> 00:28:28,100
finish up with just going back to the

00:28:25,030 --> 00:28:30,380
summary of the analysis that was done so

00:28:28,100 --> 00:28:31,850
this was again the election results

00:28:30,380 --> 00:28:34,760
which you could see on CNN the night

00:28:31,850 --> 00:28:37,070
that the election happened this is the

00:28:34,760 --> 00:28:38,570
percentage of vote that went this is the

00:28:37,070 --> 00:28:40,520
election results if only non-white

00:28:38,570 --> 00:28:42,620
people had voted so you can see it's

00:28:40,520 --> 00:28:43,760
just overwhelmingly Democrat you can

00:28:42,620 --> 00:28:45,140
look if you look closely at some of the

00:28:43,760 --> 00:28:47,240
congressional districts you can see the

00:28:45,140 --> 00:28:48,860
artifacts of the hierarchical modeling

00:28:47,240 --> 00:28:50,000
so I think especially out in West North

00:28:48,860 --> 00:28:51,620
Carolina you can see that that's a

00:28:50,000 --> 00:28:52,310
little bit lighter and that's because we

00:28:51,620 --> 00:28:53,600
assumed all the

00:28:52,310 --> 00:28:57,110
syncs were being drawn from the same

00:28:53,600 --> 00:28:58,820
from the same distribution and similarly

00:28:57,110 --> 00:29:00,410
this is the the white vote that that

00:28:58,820 --> 00:29:04,040
we're estimating in North Carolina

00:29:00,410 --> 00:29:07,190
so again very very much not for the

00:29:04,040 --> 00:29:08,750
Democratic candidate and as far as I

00:29:07,190 --> 00:29:10,580
know these are these are novel maps of

00:29:08,750 --> 00:29:15,440
the most sort of granular maps you can

00:29:10,580 --> 00:29:17,870
find of racially polarized voting great

00:29:15,440 --> 00:29:19,370
so just to conclude now's a great time

00:29:17,870 --> 00:29:20,720
to be thinking about redistricting and

00:29:19,370 --> 00:29:22,790
thinking about gerrymandering there's a

00:29:20,720 --> 00:29:26,080
census coming up in 2020 after which

00:29:22,790 --> 00:29:29,630
they'll be redrawing the district lines

00:29:26,080 --> 00:29:31,970
so so yeah thanks so much for coming

00:29:29,630 --> 00:29:32,960
their slides up online and we'll be

00:29:31,970 --> 00:29:35,690
happy to take questions

00:29:32,960 --> 00:29:42,870
out in the hallway afterwards

00:29:35,690 --> 00:29:46,480
[Applause]

00:29:42,870 --> 00:29:48,540
thank you to : and Karen and as he said

00:29:46,480 --> 00:29:51,660
they'll take questions at the back so

00:29:48,540 --> 00:29:51,660
thank you

00:29:51,800 --> 00:29:57,069

YouTube URL: https://www.youtube.com/watch?v=G9I5ZnkWR0A


