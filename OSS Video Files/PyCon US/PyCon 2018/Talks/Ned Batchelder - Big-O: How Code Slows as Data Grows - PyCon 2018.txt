Title: Ned Batchelder - Big-O: How Code Slows as Data Grows - PyCon 2018
Publication date: 2018-08-06
Playlist: Talks
Description: 
	Speaker: Ned Batchelder

Big-O is a computer science technique for analyzing how code performs as data gets larger.  It's a very handy tool for the working programmer, but it's often shrouded in off-putting mathematics.

In this talk, I'll teach you what you need to know about Big-O, and how to use it to keep your programs running well.  Big-O helps you choose the data structures and algorithms that will let your code work efficiently even on large data sets.

You can understand Big-O even if you aren't a theoretical computer science math nerd. Big-O isn't as mystical as it appears. It's wrapped in mathematical trappings, but doesn't have to be more than a common-sense assessment of how your code will behave.


Slides can be found at: https://speakerdeck.com/pycon2018 and https://github.com/PyCon/2018-slides
Captions: 
	00:00:03,200 --> 00:00:12,300
good afternoon everybody and welcome to

00:00:06,210 --> 00:00:15,630
the final talk here of this PyCon I'm

00:00:12,300 --> 00:00:19,529
especially honored here to have a fellow

00:00:15,630 --> 00:00:21,119
Ned here so it's not often I hope you

00:00:19,529 --> 00:00:23,789
appreciate the honor that we have

00:00:21,119 --> 00:00:26,820
arranged to have to Ned's on the stage

00:00:23,789 --> 00:00:30,810
at one time Ned bachelor here is going

00:00:26,820 --> 00:00:33,059
to talk about Big O how code slows as

00:00:30,810 --> 00:00:35,520
data grows there probably will not be

00:00:33,059 --> 00:00:38,010
time for questions afterwards but you

00:00:35,520 --> 00:00:42,140
can you may be able to catch Ned

00:00:38,010 --> 00:00:42,140
elsewhere thanks very much Ned bachelor

00:00:45,829 --> 00:00:50,280
hi everyone thank you for coming to this

00:00:48,539 --> 00:00:52,590
last talk session I've been trying to

00:00:50,280 --> 00:00:54,329
think of this slot not as the exhausted

00:00:52,590 --> 00:00:57,000
speaker and half the people are already

00:00:54,329 --> 00:00:59,250
on planes slot but as the grand finale

00:00:57,000 --> 00:01:06,780
of PyCon so thank you for joining me

00:00:59,250 --> 00:01:08,970
here so as Ned said my name is Ned

00:01:06,780 --> 00:01:10,979
Batchelder I'm Ned batt on most social

00:01:08,970 --> 00:01:12,540
media and there is a bitly short link

00:01:10,979 --> 00:01:15,030
there at the bottom of the slide that

00:01:12,540 --> 00:01:17,159
links to this talk online if you want to

00:01:15,030 --> 00:01:18,960
follow along with the slides and those

00:01:17,159 --> 00:01:21,930
two short things will be at the bottom

00:01:18,960 --> 00:01:23,700
of most of the slides too so I've been

00:01:21,930 --> 00:01:25,290
writing software for a long time and one

00:01:23,700 --> 00:01:26,909
of the things that interests me about

00:01:25,290 --> 00:01:29,460
writing software is that there are two

00:01:26,909 --> 00:01:31,829
mindsets that inform the process of

00:01:29,460 --> 00:01:33,570
writing software the first is computer

00:01:31,829 --> 00:01:35,490
science which is really a branch of

00:01:33,570 --> 00:01:37,950
mathematics and so it's very theoretical

00:01:35,490 --> 00:01:40,740
you do proofs you think about very

00:01:37,950 --> 00:01:42,240
abstract concepts the other mindset that

00:01:40,740 --> 00:01:44,310
informs writing software is software

00:01:42,240 --> 00:01:46,409
engineering which is very pragmatic and

00:01:44,310 --> 00:01:48,240
is basically concerned only with whether

00:01:46,409 --> 00:01:49,619
you are writing software that works how

00:01:48,240 --> 00:01:52,530
can we write software that works and

00:01:49,619 --> 00:01:54,240
there is some crossover lots of computer

00:01:52,530 --> 00:01:55,530
science underpin software engineering

00:01:54,240 --> 00:01:58,500
but we don't think about it every day

00:01:55,530 --> 00:02:00,240
but there's a few topics that do cross

00:01:58,500 --> 00:02:01,979
over into the everyday of software

00:02:00,240 --> 00:02:03,479
engineering and in particular I'm

00:02:01,979 --> 00:02:05,250
interested in people who are working in

00:02:03,479 --> 00:02:07,110
software engineering who don't have

00:02:05,250 --> 00:02:09,149
formal computer science backgrounds and

00:02:07,110 --> 00:02:11,099
maybe feel a little bit insecure about

00:02:09,149 --> 00:02:12,930
that and one of the things that they

00:02:11,099 --> 00:02:13,800
seem to feel keeps them from sitting at

00:02:12,930 --> 00:02:18,560
the grown-ups table

00:02:13,800 --> 00:02:21,240
is this thing called big Oh a big o is

00:02:18,560 --> 00:02:22,860
really a simple thing and I've made a

00:02:21,240 --> 00:02:25,320
rhyme here to help you remember what it

00:02:22,860 --> 00:02:28,710
is it's about how your code slows as

00:02:25,320 --> 00:02:30,000
your data grows and an English major

00:02:28,710 --> 00:02:32,640
friend of mine pointed out that the

00:02:30,000 --> 00:02:33,810
rhyme is on the O sound like Big O and I

00:02:32,640 --> 00:02:38,880
didn't intend that at all but that's

00:02:33,810 --> 00:02:41,040
cool so the question is how does your

00:02:38,880 --> 00:02:43,050
code slow down as the data gets larger

00:02:41,040 --> 00:02:45,180
and larger and this is not the same as

00:02:43,050 --> 00:02:47,160
the running time of any particular run

00:02:45,180 --> 00:02:49,110
of your code we're not trying to measure

00:02:47,160 --> 00:02:50,880
the time in seconds we're not trying to

00:02:49,110 --> 00:02:53,400
figure out exactly how many of them you

00:02:50,880 --> 00:02:55,980
can do in one transaction we're talking

00:02:53,400 --> 00:02:58,950
about the trend over time over many runs

00:02:55,980 --> 00:03:00,390
of your code how does it slow down as

00:02:58,950 --> 00:03:03,000
the data gets larger and larger and

00:03:00,390 --> 00:03:04,830
larger and one very pragmatic way to

00:03:03,000 --> 00:03:06,690
think about this is let's say you have a

00:03:04,830 --> 00:03:08,100
chunk of code you give it a certain

00:03:06,690 --> 00:03:10,950
amount of data it takes a certain amount

00:03:08,100 --> 00:03:12,690
of time how much longer will it take to

00:03:10,950 --> 00:03:14,550
work on ten times as much data if I give

00:03:12,690 --> 00:03:16,230
it ten times as much data how much

00:03:14,550 --> 00:03:18,360
longer does it take and you might think

00:03:16,230 --> 00:03:18,570
intuitively well it'll take ten times as

00:03:18,360 --> 00:03:21,090
long

00:03:18,570 --> 00:03:23,580
obviously but that turns out not to be

00:03:21,090 --> 00:03:25,709
true some code will take ten times as

00:03:23,580 --> 00:03:27,270
long some code will take twice as long

00:03:25,709 --> 00:03:28,620
some code will take a hundred times as

00:03:27,270 --> 00:03:30,870
long and some code won't take any longer

00:03:28,620 --> 00:03:34,260
at all and Big O is all about

00:03:30,870 --> 00:03:36,540
characterizing that growth of the time

00:03:34,260 --> 00:03:38,610
of the code as the code as the data

00:03:36,540 --> 00:03:42,330
grows how the code slows as the data

00:03:38,610 --> 00:03:44,160
grows and computer science people

00:03:42,330 --> 00:03:47,010
approach this topic in a very very

00:03:44,160 --> 00:03:48,780
mathematical way but software engineers

00:03:47,010 --> 00:03:50,340
approach it in a very very pragmatic way

00:03:48,780 --> 00:03:52,590
and I'm trying to I'm going to explain

00:03:50,340 --> 00:03:54,240
the pragmatic approach it doesn't have

00:03:52,590 --> 00:03:57,690
to be done in a Matthew way it can be

00:03:54,240 --> 00:04:00,270
done in a pragmatic way so let's get

00:03:57,690 --> 00:04:01,770
some terminology out of the way this is

00:04:00,270 --> 00:04:03,390
called the Big O notation and the way

00:04:01,770 --> 00:04:05,010
it's written is a capital o and a

00:04:03,390 --> 00:04:07,550
parenthesis and then a bunch of stuff

00:04:05,010 --> 00:04:12,390
with an N in it and a closed parenthesis

00:04:07,550 --> 00:04:15,640
okay I told you it was simple

00:04:12,390 --> 00:04:19,360
the N is meant to stand in for how much

00:04:15,640 --> 00:04:21,130
data you have and the O stands for order

00:04:19,360 --> 00:04:22,600
of and the idea is that we're talking

00:04:21,130 --> 00:04:25,120
about the running time of your code

00:04:22,600 --> 00:04:29,290
grows on the same order of some

00:04:25,120 --> 00:04:30,640
mathematical expression of n the key

00:04:29,290 --> 00:04:32,140
thing here is although it looks like a

00:04:30,640 --> 00:04:33,820
function call there's a name and then

00:04:32,140 --> 00:04:36,040
parentheses with stuff inside it's not a

00:04:33,820 --> 00:04:37,870
function call it's just a notation and

00:04:36,040 --> 00:04:39,340
there's probably a massy reason why it

00:04:37,870 --> 00:04:40,870
looks like a function call but it

00:04:39,340 --> 00:04:42,160
doesn't matter just know that it's not a

00:04:40,870 --> 00:04:44,140
function call it's just a way of

00:04:42,160 --> 00:04:54,100
labeling a piece of code as having a

00:04:44,140 --> 00:04:55,840
certain growth pattern so let's take a

00:04:54,100 --> 00:04:58,180
real world example let's say we have to

00:04:55,840 --> 00:05:00,580
count the number of beans and jars right

00:04:58,180 --> 00:05:02,200
we've got this guy in the left he opens

00:05:00,580 --> 00:05:05,620
up the jars he starts pulling out beans

00:05:02,200 --> 00:05:07,720
one by one right we can see here n is

00:05:05,620 --> 00:05:09,670
the number of beans right if we give

00:05:07,720 --> 00:05:11,710
this guy a jar with ten times as many

00:05:09,670 --> 00:05:11,920
beans it's gonna take him ten times as

00:05:11,710 --> 00:05:15,160
long

00:05:11,920 --> 00:05:17,500
obviously I mean he's sweating this is

00:05:15,160 --> 00:05:20,080
what's known as oh of n meaning that the

00:05:17,500 --> 00:05:22,630
time it takes to complete the task grows

00:05:20,080 --> 00:05:25,030
in the same way that n grows it's on the

00:05:22,630 --> 00:05:27,130
order of n if n doubles the time doubles

00:05:25,030 --> 00:05:29,770
if n is ten times more the time is ten

00:05:27,130 --> 00:05:31,000
times more and you might think well

00:05:29,770 --> 00:05:33,310
there's no other way to approach this

00:05:31,000 --> 00:05:35,620
task but there is of course you get

00:05:33,310 --> 00:05:36,820
beans jars that have labels on them that

00:05:35,620 --> 00:05:38,980
tell you how many beans are in them

00:05:36,820 --> 00:05:40,660
right this guy on the right has a much

00:05:38,980 --> 00:05:43,150
easier job you can see how much happier

00:05:40,660 --> 00:05:44,830
he is about it because it doesn't matter

00:05:43,150 --> 00:05:46,540
how large a jar you give this guy on the

00:05:44,830 --> 00:05:48,160
right it's gonna take the same amount of

00:05:46,540 --> 00:05:49,510
time for him to tell you how many beans

00:05:48,160 --> 00:05:51,430
are in the jar this is what's known as

00:05:49,510 --> 00:05:53,260
oh of one which is kind of a weird

00:05:51,430 --> 00:05:55,270
mathematicians way of saying that n

00:05:53,260 --> 00:05:58,060
isn't involved at all no matter what

00:05:55,270 --> 00:06:02,860
happens to n the running time remains

00:05:58,060 --> 00:06:05,860
the same o event is is slower in the

00:06:02,860 --> 00:06:09,250
long run to know of one and this is a

00:06:05,860 --> 00:06:11,500
silly silly real-world example but for

00:06:09,250 --> 00:06:14,140
instance when you do for X in my list

00:06:11,500 --> 00:06:15,790
you have an O of n operation because you

00:06:14,140 --> 00:06:18,070
have to look at every element in the

00:06:15,790 --> 00:06:19,690
list every bean in the jar when you do

00:06:18,070 --> 00:06:20,979
Len of my list you don't have to look at

00:06:19,690 --> 00:06:22,750
the elements that list at all

00:06:20,979 --> 00:06:24,080
turns out Python lists are kind of like

00:06:22,750 --> 00:06:25,340
those jars on the right

00:06:24,080 --> 00:06:27,590
length of the list is written on the

00:06:25,340 --> 00:06:29,330
label on the outside of the list and so

00:06:27,590 --> 00:06:30,800
no matter how long the list is getting

00:06:29,330 --> 00:06:34,490
the length of the list is a constant

00:06:30,800 --> 00:06:37,969
time operation by the way these drawings

00:06:34,490 --> 00:06:40,009
were drawn by my son who is in art

00:06:37,969 --> 00:06:41,419
school one thing you might not have

00:06:40,009 --> 00:06:45,289
noticed if you look at their eyebrows

00:06:41,419 --> 00:06:54,949
they're shaped like beans that's that's

00:06:45,289 --> 00:06:57,319
art school for you all right another

00:06:54,949 --> 00:06:58,400
real-world example let's say I tell you

00:06:57,319 --> 00:06:59,629
I'm going to give you a book and I want

00:06:58,400 --> 00:07:02,060
you to find a certain word in the book

00:06:59,629 --> 00:07:04,280
like horse right if I hand you a novel

00:07:02,060 --> 00:07:06,560
you're gonna start reading until maybe

00:07:04,280 --> 00:07:09,050
you find the word horse right this is

00:07:06,560 --> 00:07:10,430
sounds like an O of n operation again

00:07:09,050 --> 00:07:11,840
right because if I give you a novel

00:07:10,430 --> 00:07:13,550
that's twice as long it might take you

00:07:11,840 --> 00:07:15,560
twice as long until you encounter the

00:07:13,550 --> 00:07:16,520
word horse well let's say I give you a

00:07:15,560 --> 00:07:18,440
different book I give you an

00:07:16,520 --> 00:07:20,180
encyclopedia now you open the

00:07:18,440 --> 00:07:21,740
encyclopedia to the middle if the word

00:07:20,180 --> 00:07:23,539
you're looking for is earlier than that

00:07:21,740 --> 00:07:26,000
then you do another divide and conquer

00:07:23,539 --> 00:07:27,770
step until you find the word horse if I

00:07:26,000 --> 00:07:29,210
now if I give you an encyclopedia that's

00:07:27,770 --> 00:07:31,279
twice as large it's not going to take

00:07:29,210 --> 00:07:33,110
you twice as long there's just one more

00:07:31,279 --> 00:07:35,360
divide and conquer step to find it

00:07:33,110 --> 00:07:37,279
that's what's called a log of n which is

00:07:35,360 --> 00:07:39,800
a fancy mathematicians way of saying

00:07:37,279 --> 00:07:41,990
that so these are both real-world

00:07:39,800 --> 00:07:43,460
examples of the kinds of tasks that sort

00:07:41,990 --> 00:07:45,469
of sound similar when you first hear

00:07:43,460 --> 00:07:47,089
them but how you organize the data and

00:07:45,469 --> 00:07:49,339
therefore what algorithm you can use on

00:07:47,089 --> 00:07:50,919
the data really affects how that length

00:07:49,339 --> 00:07:53,690
of time it takes you to do the task

00:07:50,919 --> 00:07:55,639
changes as the size of the data changes

00:07:53,690 --> 00:07:59,319
right that's what we're talking about is

00:07:55,639 --> 00:08:01,550
how your code slows as your data grows

00:07:59,319 --> 00:08:02,839
let's get some other terms out of the

00:08:01,550 --> 00:08:04,039
way because I'm going to be speaking

00:08:02,839 --> 00:08:05,240
here and I might throw out some words

00:08:04,039 --> 00:08:07,940
that are a little bit different than

00:08:05,240 --> 00:08:09,589
earlier when we say o of 1 we might call

00:08:07,940 --> 00:08:11,300
it call it constant time I might say

00:08:09,589 --> 00:08:13,969
that the labeled bean jars are a

00:08:11,300 --> 00:08:18,259
constant time algorithm because the time

00:08:13,969 --> 00:08:20,089
remains the same no matter what oh if n

00:08:18,259 --> 00:08:21,830
is often called a linear operation

00:08:20,089 --> 00:08:24,919
because if you look at it mathematically

00:08:21,830 --> 00:08:26,419
there's a linear relationship between

00:08:24,919 --> 00:08:29,150
the size of the data and the running

00:08:26,419 --> 00:08:31,610
time o of N squared is a thing we

00:08:29,150 --> 00:08:32,779
haven't seen yet but we will that's the

00:08:31,610 --> 00:08:34,370
case where you wouldn't you give it 10

00:08:32,779 --> 00:08:35,659
times more data it takes a hundred times

00:08:34,370 --> 00:08:37,159
as long to run and that's called

00:08:35,659 --> 00:08:39,409
quadratic because now you've got

00:08:37,159 --> 00:08:41,089
quadratic equation involved if you don't

00:08:39,409 --> 00:08:42,560
remember what a quadratic equation is it

00:08:41,089 --> 00:08:46,160
doesn't matter it's just a word that

00:08:42,560 --> 00:08:48,079
means N squared and some other words for

00:08:46,160 --> 00:08:49,699
Big O it's sometimes called complexity

00:08:48,079 --> 00:08:51,500
or time complexity or algorithmic

00:08:49,699 --> 00:08:53,420
complexity or if you want to sound

00:08:51,500 --> 00:08:55,819
really fancy you can call it asymptotic

00:08:53,420 --> 00:08:57,589
complexity it's all the same thing right

00:08:55,819 --> 00:09:00,319
one of the underlying themes of this

00:08:57,589 --> 00:09:04,959
talk is that this just this topic of Big

00:09:00,319 --> 00:09:08,269
O notation is littered with mathematical

00:09:04,959 --> 00:09:10,370
detritus that doesn't really matter to

00:09:08,269 --> 00:09:12,290
the key concept and don't let that stuff

00:09:10,370 --> 00:09:15,050
throw you that's just chaff being thrown

00:09:12,290 --> 00:09:20,089
at you by mathematicians you don't have

00:09:15,050 --> 00:09:21,920
to let it throw you off the path so how

00:09:20,089 --> 00:09:24,680
do you actually determine the Big O of a

00:09:21,920 --> 00:09:26,089
piece of code the first step is you

00:09:24,680 --> 00:09:27,889
figure out what code you're talking

00:09:26,089 --> 00:09:30,439
about and that sounds kind of silly but

00:09:27,889 --> 00:09:32,360
in a large system there's you might be

00:09:30,439 --> 00:09:33,649
looking at one function and that might

00:09:32,360 --> 00:09:35,029
be the important thing but it really

00:09:33,649 --> 00:09:36,529
might actually be important to consider

00:09:35,029 --> 00:09:37,910
all the callers of the function or maybe

00:09:36,529 --> 00:09:39,139
you're looking at two larger chunk of

00:09:37,910 --> 00:09:41,149
code you need to think about a small

00:09:39,139 --> 00:09:42,620
piece if you're going to describe a heap

00:09:41,149 --> 00:09:44,500
this piece of code be very clear about

00:09:42,620 --> 00:09:46,790
what piece of code you're talking about

00:09:44,500 --> 00:09:48,350
and then when you look at that code you

00:09:46,790 --> 00:09:49,939
should figure out what n is and I don't

00:09:48,350 --> 00:09:52,459
mean like whether it's a hundred or a

00:09:49,939 --> 00:09:54,230
thousand I mean what is it measuring so

00:09:52,459 --> 00:09:55,759
if you're you've have some code that's

00:09:54,230 --> 00:09:57,800
iterating over all the records in a

00:09:55,759 --> 00:09:59,389
database than n is how many records in

00:09:57,800 --> 00:10:01,250
the database and our being example it

00:09:59,389 --> 00:10:02,810
was how many beans in the jar if you're

00:10:01,250 --> 00:10:06,980
doing a string search it might be the

00:10:02,810 --> 00:10:08,959
length of this string and then here's

00:10:06,980 --> 00:10:10,880
where the real work comes in you're

00:10:08,959 --> 00:10:13,009
going to think about that code running

00:10:10,880 --> 00:10:16,100
and you're going to figure out how many

00:10:13,009 --> 00:10:18,019
steps there are in the code in a typical

00:10:16,100 --> 00:10:20,509
run let me tell you what I mean by

00:10:18,019 --> 00:10:23,449
typical first there's two meanings of

00:10:20,509 --> 00:10:25,069
typical one is what kind of data is it

00:10:23,449 --> 00:10:26,750
going to get in the real world there's

00:10:25,069 --> 00:10:28,819
sort of real world data that kind of is

00:10:26,750 --> 00:10:30,350
what you kind of can expect and then

00:10:28,819 --> 00:10:33,800
there's worst case data right

00:10:30,350 --> 00:10:36,559
a string of 40,000 spaces is not typical

00:10:33,800 --> 00:10:39,110
data typical data is you know last names

00:10:36,559 --> 00:10:41,120
it's mostly ASCII it it's about most to

00:10:39,110 --> 00:10:42,500
15 characters long that kind of thing so

00:10:41,120 --> 00:10:44,629
you can think about what's your typical

00:10:42,500 --> 00:10:47,569
data and then another meaning of a

00:10:44,629 --> 00:10:49,790
typical run is that over many runs of

00:10:47,569 --> 00:10:50,780
your algorithm there's a certain number

00:10:49,790 --> 00:10:53,000
of times the loop

00:10:50,780 --> 00:10:54,440
might run or a certain length of string

00:10:53,000 --> 00:10:57,200
it might get so you kind of think about

00:10:54,440 --> 00:10:58,640
the design center of your code and you

00:10:57,200 --> 00:11:00,200
imagine running that code through that

00:10:58,640 --> 00:11:02,300
design center and you count the steps

00:11:00,200 --> 00:11:05,090
and what I mean by steps is very vague

00:11:02,300 --> 00:11:06,800
and in a way a lot of this topic is very

00:11:05,090 --> 00:11:08,630
vague there are no units and anything

00:11:06,800 --> 00:11:10,880
we're talking about and the number of

00:11:08,630 --> 00:11:13,070
steps it kind of doesn't matter what you

00:11:10,880 --> 00:11:14,630
count as a step and it kind of doesn't

00:11:13,070 --> 00:11:16,310
matter that some steps might actually

00:11:14,630 --> 00:11:18,320
take longer than others because really

00:11:16,310 --> 00:11:20,720
what you're thinking about is if I'm

00:11:18,320 --> 00:11:23,030
doing any equals ten I'll have this many

00:11:20,720 --> 00:11:25,160
steps now how many for N equals 100 and

00:11:23,030 --> 00:11:27,380
so that exactly what steps there are

00:11:25,160 --> 00:11:29,510
doesn't matter as much as how does that

00:11:27,380 --> 00:11:32,150
count grow and I'll show you some

00:11:29,510 --> 00:11:33,710
examples so you'll get a sense of it so

00:11:32,150 --> 00:11:35,900
you count the steps in a typical run and

00:11:33,710 --> 00:11:38,030
since we put in n at the top and not 10

00:11:35,900 --> 00:11:40,130
exactly the number of steps is going to

00:11:38,030 --> 00:11:42,860
be an expression in n you might end up

00:11:40,130 --> 00:11:45,740
with well it's 3 n plus 47 steps

00:11:42,860 --> 00:11:47,210
something like that and then what you do

00:11:45,740 --> 00:11:49,190
is you keep only the most significant

00:11:47,210 --> 00:11:51,530
part of that expression so you keep only

00:11:49,190 --> 00:11:53,090
the highest coefficient piece and then

00:11:51,530 --> 00:11:56,360
you throw away the coefficients so if

00:11:53,090 --> 00:11:59,030
you had 47 n squared plus 53 n plus 101

00:11:56,360 --> 00:12:01,220
that's N squared you throw away all the

00:11:59,030 --> 00:12:03,860
lower order components and the exponent

00:12:01,220 --> 00:12:05,900
coefficients and the reason is that as n

00:12:03,860 --> 00:12:07,880
gets larger and larger and larger the

00:12:05,900 --> 00:12:10,820
lower order components matter less and

00:12:07,880 --> 00:12:12,590
less right 3 n plus 1 the 1 is really

00:12:10,820 --> 00:12:14,960
important when n is 1 but when n is a

00:12:12,590 --> 00:12:16,550
billion who cares about the 1 right

00:12:14,960 --> 00:12:18,320
we're trying to get to that long-term

00:12:16,550 --> 00:12:21,560
trend as the data gets very very large

00:12:18,320 --> 00:12:23,750
and if it's 3 n that doubles when n

00:12:21,560 --> 00:12:26,960
doubles just the way n doubles when n

00:12:23,750 --> 00:12:28,310
doubles so the 3 is irrelevant 2 so you

00:12:26,960 --> 00:12:29,540
get rid of the lower order components

00:12:28,310 --> 00:12:33,050
and get rid of the coefficients and

00:12:29,540 --> 00:12:34,730
what's left is your Big O notation now

00:12:33,050 --> 00:12:36,170
let's look at some examples true fact I

00:12:34,730 --> 00:12:37,520
wrote this code in November and it

00:12:36,170 --> 00:12:38,600
didn't occur to me until I was lying in

00:12:37,520 --> 00:12:40,550
bed this morning

00:12:38,600 --> 00:12:42,880
that this code is about moms and it's

00:12:40,550 --> 00:12:42,880
Mother's Day

00:12:45,620 --> 00:12:49,410
so here's an example of some code what

00:12:48,389 --> 00:12:51,360
we're gonna do is we're going to have a

00:12:49,410 --> 00:12:53,730
data structure called mom's which is a

00:12:51,360 --> 00:12:56,040
list of tuples and the tuples are people

00:12:53,730 --> 00:12:57,779
and their mother's right and then we're

00:12:56,040 --> 00:12:59,339
gonna write a function called find mom

00:12:57,779 --> 00:13:01,019
and find mom is going to take that list

00:12:59,339 --> 00:13:03,420
of moms in the name of a child and it's

00:13:01,019 --> 00:13:05,490
gonna find the child's mom okay

00:13:03,420 --> 00:13:08,339
now if you think about searching through

00:13:05,490 --> 00:13:09,750
this list in a typical run in some runs

00:13:08,339 --> 00:13:11,760
we'll find it in the first entry and

00:13:09,750 --> 00:13:15,149
some runs will find it in the last entry

00:13:11,760 --> 00:13:17,250
so on average in a typical case we find

00:13:15,149 --> 00:13:21,839
it about n over two times we're gonna

00:13:17,250 --> 00:13:23,730
look through half the list so if we come

00:13:21,839 --> 00:13:26,190
down to this line this loop is going to

00:13:23,730 --> 00:13:27,750
run n over two times and I'm gonna say

00:13:26,190 --> 00:13:29,579
that there are three steps in this loop

00:13:27,750 --> 00:13:31,260
we have to get the tuple out of the list

00:13:29,579 --> 00:13:32,519
and then we have to assign the child to

00:13:31,260 --> 00:13:33,990
child name and we have to assign the mom

00:13:32,519 --> 00:13:35,970
to mom named so there's three steps

00:13:33,990 --> 00:13:37,680
which means that this line is going to

00:13:35,970 --> 00:13:42,839
contribute three times and over two

00:13:37,680 --> 00:13:45,690
steps to our count this comparison we're

00:13:42,839 --> 00:13:47,010
going to do n over two times and if

00:13:45,690 --> 00:13:50,130
there's only one step so that gives us

00:13:47,010 --> 00:13:51,750
another n over two and then this line is

00:13:50,130 --> 00:13:53,160
only going to happen once because it's

00:13:51,750 --> 00:13:55,440
the end of the function so that's going

00:13:53,160 --> 00:13:57,570
to give us one more step and so what our

00:13:55,440 --> 00:13:59,699
total is going to be 3n over 2 plus n

00:13:57,570 --> 00:14:02,190
over 2 plus 1 which simplifies down to

00:13:59,699 --> 00:14:03,870
2n plus 1 I said that we get rid of the

00:14:02,190 --> 00:14:05,190
lower order components which is the 1 we

00:14:03,870 --> 00:14:08,970
get rid of the coefficient which is 2

00:14:05,190 --> 00:14:11,070
this is an oo of n function right so

00:14:08,970 --> 00:14:13,740
we've just determined that the

00:14:11,070 --> 00:14:17,940
algorithmic the sorry the asymptotic

00:14:13,740 --> 00:14:19,740
complexity of find mom is o of n and the

00:14:17,940 --> 00:14:22,860
way people say that in the real world in

00:14:19,740 --> 00:14:29,519
a cubicle is find mom is o of n or find

00:14:22,860 --> 00:14:30,690
mom is linear find mom is o of n ok and

00:14:29,519 --> 00:14:33,029
so you saw when we were going through

00:14:30,690 --> 00:14:34,649
the steps we didn't really care which

00:14:33,029 --> 00:14:36,480
steps were expensive which weren't we

00:14:34,649 --> 00:14:37,890
only wanted to know is the relationship

00:14:36,480 --> 00:14:39,810
between the N and the number of steps

00:14:37,890 --> 00:14:40,980
and when n changes the number of steps

00:14:39,810 --> 00:14:42,959
is going to change and that's what we're

00:14:40,980 --> 00:14:44,970
looking for and notice we have no idea

00:14:42,959 --> 00:14:46,139
whether this is fast or slow right we

00:14:44,970 --> 00:14:47,730
don't know whether this function is

00:14:46,139 --> 00:14:48,990
going to take a minute or a millisecond

00:14:47,730 --> 00:14:50,519
all we know is that if we give it 10

00:14:48,990 --> 00:14:54,870
times more data it's probably going to

00:14:50,519 --> 00:14:56,200
take 10 times as long let's look at

00:14:54,870 --> 00:14:57,910
another example

00:14:56,200 --> 00:14:59,500
also about mom's the same same data

00:14:57,910 --> 00:15:01,090
structure the same mom's data structure

00:14:59,500 --> 00:15:03,280
but now what we're gonna do is we're

00:15:01,090 --> 00:15:04,540
going to write a function which tells us

00:15:03,280 --> 00:15:06,100
in that data structure and how many

00:15:04,540 --> 00:15:08,020
grandmother's are there that is how many

00:15:06,100 --> 00:15:10,090
people how many people are mentioned

00:15:08,020 --> 00:15:11,410
both as a mom and as a child in our list

00:15:10,090 --> 00:15:12,670
right and so now we're going to go all

00:15:11,410 --> 00:15:15,430
the way through to the end of the list

00:15:12,670 --> 00:15:17,020
and I mentioned that n over two before

00:15:15,430 --> 00:15:19,270
but remember we're throwing away

00:15:17,020 --> 00:15:21,550
coefficients so in a way the half never

00:15:19,270 --> 00:15:23,140
mattered and as you work through this

00:15:21,550 --> 00:15:25,420
more you'll sort of get a sense of what

00:15:23,140 --> 00:15:26,350
you can not collect in the first place

00:15:25,420 --> 00:15:30,250
because you're gonna throw it away

00:15:26,350 --> 00:15:33,010
anyway so for instance this line is

00:15:30,250 --> 00:15:34,900
gonna run n times right and notice I

00:15:33,010 --> 00:15:36,430
didn't write 3 times n here because like

00:15:34,900 --> 00:15:38,590
I said we're throwing away coefficients

00:15:36,430 --> 00:15:42,850
this is an O if end line we're gonna run

00:15:38,590 --> 00:15:44,050
this line n times now this line is going

00:15:42,850 --> 00:15:46,720
to run n times also

00:15:44,050 --> 00:15:49,350
but it's calling a function find mom

00:15:46,720 --> 00:15:51,550
which we just determined was an O of n

00:15:49,350 --> 00:15:52,990
operation in and of itself when you call

00:15:51,550 --> 00:15:54,910
it once it's so of N and we're gonna

00:15:52,990 --> 00:15:59,620
call it n times that's going to give us

00:15:54,910 --> 00:16:01,420
N squared and we can continue continue

00:15:59,620 --> 00:16:02,500
on and say this is n but remember we're

00:16:01,420 --> 00:16:04,030
gonna throw away the lower order

00:16:02,500 --> 00:16:05,230
components we already found an N squared

00:16:04,030 --> 00:16:08,800
it's kind of uninteresting to keep

00:16:05,230 --> 00:16:11,470
finding the ends right but we're finding

00:16:08,800 --> 00:16:13,570
a bunch of ends we're gonna end up with

00:16:11,470 --> 00:16:16,600
N squared plus some number of n plus 1

00:16:13,570 --> 00:16:18,670
which is o of N squared right so find

00:16:16,600 --> 00:16:27,490
how many grandmother's is a quadratic

00:16:18,670 --> 00:16:27,910
function it's o of N squared now the

00:16:27,490 --> 00:16:30,340
idea

00:16:27,910 --> 00:16:32,620
of course is of1 right constant time you

00:16:30,340 --> 00:16:34,480
can do the same amount you can work on

00:16:32,620 --> 00:16:35,830
any amount of data and not take it how

00:16:34,480 --> 00:16:37,930
to take any longer and it seems kind of

00:16:35,830 --> 00:16:39,820
impossible like how could that be but

00:16:37,930 --> 00:16:42,040
remember we saw a Len of my list is o of

00:16:39,820 --> 00:16:43,690
one because no matter how long the list

00:16:42,040 --> 00:16:45,640
is the length is written on the outside

00:16:43,690 --> 00:16:48,400
we can just pick it up that's kind of

00:16:45,640 --> 00:16:50,230
boring really interesting is the looking

00:16:48,400 --> 00:16:51,790
up a key in a dictionary is o of one no

00:16:50,230 --> 00:16:53,620
matter how many keys are in a dictionary

00:16:51,790 --> 00:16:55,480
it's this takes about it the same amount

00:16:53,620 --> 00:16:57,130
of time to look up a key as in a

00:16:55,480 --> 00:16:58,210
one-element dictionary and in a million

00:16:57,130 --> 00:17:01,240
element dictionary which is why

00:16:58,210 --> 00:17:03,850
dictionaries are heavily optimized and

00:17:01,240 --> 00:17:05,760
engineered and underpin every name

00:17:03,850 --> 00:17:07,960
lookup in Python because they're fast

00:17:05,760 --> 00:17:10,630
and we'll get back to why it is but very

00:17:07,960 --> 00:17:12,130
quickly it's because there's a thing

00:17:10,630 --> 00:17:14,650
called a hash function which turns a key

00:17:12,130 --> 00:17:16,900
into a number and in typical data the

00:17:14,650 --> 00:17:18,550
numbers are all different and so you can

00:17:16,900 --> 00:17:19,900
very quickly use that number to find the

00:17:18,550 --> 00:17:25,900
place in the dictionary where the value

00:17:19,900 --> 00:17:26,890
is now no discussion of of Big O

00:17:25,900 --> 00:17:29,170
notation would be complete without

00:17:26,890 --> 00:17:31,300
showing you the graph along the bottom

00:17:29,170 --> 00:17:34,060
we have that flat green line labeled one

00:17:31,300 --> 00:17:36,430
of the the x-axis is data so data grows

00:17:34,060 --> 00:17:39,430
to the right and then the time grows

00:17:36,430 --> 00:17:41,950
going up so the big flat line at the

00:17:39,430 --> 00:17:43,810
bottom is o of one that looks great log

00:17:41,950 --> 00:17:47,110
n was looking through the encyclopedia

00:17:43,810 --> 00:17:50,140
the linear line going diagonally is o of

00:17:47,110 --> 00:17:53,200
n and the big red one N squared just

00:17:50,140 --> 00:17:54,700
zooms literally off the chart right so

00:17:53,200 --> 00:17:56,380
the onion squared is one of those bad

00:17:54,700 --> 00:17:58,510
things you try to avoid because it

00:17:56,380 --> 00:18:04,960
really grows really fast when things get

00:17:58,510 --> 00:18:07,300
big now when we looked at our code we

00:18:04,960 --> 00:18:09,100
have to understand what functions were

00:18:07,300 --> 00:18:10,720
calling and how what kind of

00:18:09,100 --> 00:18:12,910
complexities they're adding into our

00:18:10,720 --> 00:18:15,130
total function this is a chart of the

00:18:12,910 --> 00:18:18,730
typical operational complexities of

00:18:15,130 --> 00:18:20,590
Lists dicts and sets in Python by the

00:18:18,730 --> 00:18:22,780
way when people say dictionaries are oh

00:18:20,590 --> 00:18:24,940
of one that sentence doesn't make any

00:18:22,780 --> 00:18:26,350
sense and nouns like dictionary can't

00:18:24,940 --> 00:18:27,610
have an algorithmic complexity

00:18:26,350 --> 00:18:29,260
operations have an algorithmic

00:18:27,610 --> 00:18:30,940
complexity so what you're supposed to

00:18:29,260 --> 00:18:34,690
say is that looking up a key in a

00:18:30,940 --> 00:18:35,800
dictionary is of1 now you'll notice that

00:18:34,690 --> 00:18:37,960
a lot of these are kind of the same

00:18:35,800 --> 00:18:40,150
appending to a list is o of 1 adding a

00:18:37,960 --> 00:18:41,779
key in the dictionary is o of 1 adding a

00:18:40,150 --> 00:18:43,759
value in a set is ov 1

00:18:41,779 --> 00:18:46,429
a big difference is looking up a value

00:18:43,759 --> 00:18:47,899
in a list is o of n so if you're going

00:18:46,429 --> 00:18:49,249
to search for a value in a list it's

00:18:47,899 --> 00:18:50,359
gonna have to look at every element in

00:18:49,249 --> 00:18:52,039
the list it's going to be that left

00:18:50,359 --> 00:18:54,739
bean-counting guy with the sweat coming

00:18:52,039 --> 00:18:56,960
off of his forehead but looking up a key

00:18:54,739 --> 00:18:59,200
in a dictionary or a value in a set is o

00:18:56,960 --> 00:19:03,229
of 1 which is why they're really really

00:18:59,200 --> 00:19:04,219
valued so pro tip right off the bat if

00:19:03,229 --> 00:19:06,320
you've got a program that's going to

00:19:04,219 --> 00:19:08,090
slow look to see you if you're looking

00:19:06,320 --> 00:19:14,629
up a value in a list and replace it with

00:19:08,090 --> 00:19:16,940
look up an asset but there are trade

00:19:14,629 --> 00:19:18,349
offs so when I say replace the list look

00:19:16,940 --> 00:19:19,879
up with asset lookup you've got to keep

00:19:18,349 --> 00:19:21,619
your eye on the big picture and this is

00:19:19,879 --> 00:19:24,469
where understanding what piece of code

00:19:21,619 --> 00:19:26,089
you care about matters so I said replace

00:19:24,469 --> 00:19:27,440
a list look up with asset lookup let's

00:19:26,089 --> 00:19:29,269
say we have some code like this where

00:19:27,440 --> 00:19:30,649
we're gonna make a list we make a list

00:19:29,269 --> 00:19:33,379
and then we try to find the thing in the

00:19:30,649 --> 00:19:35,509
list and that line is o of n right we

00:19:33,379 --> 00:19:37,909
just saw that on the table of Python

00:19:35,509 --> 00:19:39,950
complexities so you might think well I

00:19:37,909 --> 00:19:41,479
know what I'll do I'll make a set

00:19:39,950 --> 00:19:42,889
instead and then I can look it up in a

00:19:41,479 --> 00:19:44,539
set that's good if you can do that

00:19:42,889 --> 00:19:48,259
that's good so you don't make the list

00:19:44,539 --> 00:19:49,820
in the first place you make a set bad is

00:19:48,259 --> 00:19:52,039
you go ahead and you make a list anyway

00:19:49,820 --> 00:19:53,929
and then you convert it to a set and

00:19:52,039 --> 00:19:55,969
then you do the lookup in a set right so

00:19:53,929 --> 00:19:57,769
now that last line is great it's o of 1

00:19:55,969 --> 00:19:59,659
but you've added a line before it

00:19:57,769 --> 00:20:02,330
turning the list into a set which itself

00:19:59,659 --> 00:20:04,190
is o of n like literally you've actually

00:20:02,330 --> 00:20:05,839
slowed down your program by a tiny

00:20:04,190 --> 00:20:08,210
amount you still have the O of n

00:20:05,839 --> 00:20:11,389
operation of converting the list into a

00:20:08,210 --> 00:20:13,639
set so it's very easy once you get into

00:20:11,389 --> 00:20:15,649
this algorithmic complexity stuff to get

00:20:13,639 --> 00:20:17,929
sort of focused on the little things and

00:20:15,649 --> 00:20:20,839
lose sight of the big picture right this

00:20:17,929 --> 00:20:22,159
would be a bad trade off a good trade

00:20:20,839 --> 00:20:23,599
off would be even if you're making a

00:20:22,159 --> 00:20:25,399
list if you can convert it into a set

00:20:23,599 --> 00:20:26,749
once and then do many lookups so if

00:20:25,399 --> 00:20:28,399
you're gonna do many lookups in your

00:20:26,749 --> 00:20:30,889
list then it makes sense to turn it into

00:20:28,399 --> 00:20:34,129
a set once and then you have oh you have

00:20:30,889 --> 00:20:37,219
one o of N and then many o of 1 and your

00:20:34,129 --> 00:20:38,839
program will go faster so you always

00:20:37,219 --> 00:20:41,509
have to keep in mind what the real

00:20:38,839 --> 00:20:43,339
usages of your code and where the time

00:20:41,509 --> 00:20:45,200
is being spent about where you're going

00:20:43,339 --> 00:20:46,609
to sort of work on reducing the

00:20:45,200 --> 00:20:50,239
algorithmic complexity and whether it's

00:20:46,609 --> 00:20:51,649
worth it now this is don't read this

00:20:50,239 --> 00:20:53,299
code into code doesn't matter this is a

00:20:51,649 --> 00:20:55,160
real example of code that got me started

00:20:53,299 --> 00:20:57,350
down this path this was from approach

00:20:55,160 --> 00:20:59,000
last summer the code on the left is

00:20:57,350 --> 00:21:00,590
shorter and has fewer data structures

00:20:59,000 --> 00:21:02,780
and fewer functions and in fact fewer

00:21:00,590 --> 00:21:04,940
loops but it's slower than the code on

00:21:02,780 --> 00:21:07,010
the right and the reason is that if we

00:21:04,940 --> 00:21:09,560
label things as o of N and O of 1 the

00:21:07,010 --> 00:21:11,150
code on the left has an O of n operation

00:21:09,560 --> 00:21:14,540
there because it's looking up a value in

00:21:11,150 --> 00:21:16,910
the list the code on the right only has

00:21:14,540 --> 00:21:18,170
oh of 1 operations so the code on the

00:21:16,910 --> 00:21:19,580
right even though it's longer and has

00:21:18,170 --> 00:21:22,610
more functions and more data structures

00:21:19,580 --> 00:21:24,410
and more loops is o of 1 where the one

00:21:22,610 --> 00:21:25,730
on the right is o of n and in fact I was

00:21:24,410 --> 00:21:28,580
using it to draw drawings like this

00:21:25,730 --> 00:21:30,740
these functions work over an entire list

00:21:28,580 --> 00:21:31,940
of points and if you go up a level in

00:21:30,740 --> 00:21:33,680
the code you'll see actually that that

00:21:31,940 --> 00:21:36,440
function is called once for each point

00:21:33,680 --> 00:21:38,270
in the list of points so the O of n on

00:21:36,440 --> 00:21:41,180
our slow side was turning into a bow of

00:21:38,270 --> 00:21:43,250
N squared and so the slow code was

00:21:41,180 --> 00:21:45,710
taking 20 seconds the fast code was

00:21:43,250 --> 00:21:47,180
taking a half a second on only 2,000

00:21:45,710 --> 00:21:49,040
points right I say I've been giving

00:21:47,180 --> 00:21:51,560
examples like o when n gets to a billion

00:21:49,040 --> 00:21:53,090
and got to two thousand here and made a

00:21:51,560 --> 00:21:56,090
huge difference in my running time

00:21:53,090 --> 00:21:59,000
because N squared is really worse than n

00:21:56,090 --> 00:22:00,950
with n of two thousand n squared is four

00:21:59,000 --> 00:22:02,570
million operations I know of n is two

00:22:00,950 --> 00:22:05,060
thousand operations and that's a big

00:22:02,570 --> 00:22:07,580
difference so it really does pay off

00:22:05,060 --> 00:22:09,530
sometimes to reduce the algorithmic

00:22:07,580 --> 00:22:14,570
complexity of your code to reduce the

00:22:09,530 --> 00:22:16,100
running time now we've been talking

00:22:14,570 --> 00:22:18,230
about oh of 1 and O of N and over N

00:22:16,100 --> 00:22:19,700
squared there's more possibilities so

00:22:18,230 --> 00:22:21,290
there's more possibilities of kinds of

00:22:19,700 --> 00:22:24,110
complexities you might encounter in the

00:22:21,290 --> 00:22:25,730
real world of course there's o of n

00:22:24,110 --> 00:22:27,590
cubed and then forth right if we called

00:22:25,730 --> 00:22:30,110
how many grandmother's wants for every

00:22:27,590 --> 00:22:32,150
child for some reason we'd have an N

00:22:30,110 --> 00:22:34,850
cubed operation if I use that point

00:22:32,150 --> 00:22:36,980
algorithm once again for every point I'd

00:22:34,850 --> 00:22:39,040
have gone back up on an end level I'd

00:22:36,980 --> 00:22:41,810
you added know the coefficient to my n

00:22:39,040 --> 00:22:45,470
you can also have worse things like o of

00:22:41,810 --> 00:22:47,150
2 to the N if you have n boolean choices

00:22:45,470 --> 00:22:49,280
and you try all the combinations of them

00:22:47,150 --> 00:22:51,710
you've got 2 to the N if you have n

00:22:49,280 --> 00:22:53,480
things and you try to try all

00:22:51,710 --> 00:22:56,690
permutations of those end things you'll

00:22:53,480 --> 00:22:58,850
have n factorial so as bad as N squared

00:22:56,690 --> 00:23:03,830
is there's sort of no upper limit to how

00:22:58,850 --> 00:23:05,630
horrible your code can get so think

00:23:03,830 --> 00:23:07,550
about how your loops are working where

00:23:05,630 --> 00:23:08,440
how much data you're working on and keep

00:23:07,550 --> 00:23:09,820
an eye on

00:23:08,440 --> 00:23:13,390
where those complexities are getting

00:23:09,820 --> 00:23:14,740
really really big the other kinds of

00:23:13,390 --> 00:23:16,270
possibilities is there could be more

00:23:14,740 --> 00:23:18,010
dimensions so we've been talking about

00:23:16,270 --> 00:23:21,220
doing algorithmic analysis where we have

00:23:18,010 --> 00:23:23,560
one variable n but you could have others

00:23:21,220 --> 00:23:26,590
right if I'm telling you that I've got a

00:23:23,560 --> 00:23:28,450
string search algorithm over some number

00:23:26,590 --> 00:23:29,770
of strings I should maybe also have to

00:23:28,450 --> 00:23:31,600
consider the lengths of those strings

00:23:29,770 --> 00:23:33,490
typically they're fairly short but if

00:23:31,600 --> 00:23:35,110
you're doing you know biopython or

00:23:33,490 --> 00:23:37,120
something you have DNA samples that are

00:23:35,110 --> 00:23:38,230
millions of characters long and then

00:23:37,120 --> 00:23:41,260
suddenly the lengths of the strings

00:23:38,230 --> 00:23:45,580
matter to for example when I was doing

00:23:41,260 --> 00:23:47,440
that point drawing code there was a line

00:23:45,580 --> 00:23:50,440
intersection algorithm that I found

00:23:47,440 --> 00:23:52,360
whose stated complexity was n plus K

00:23:50,440 --> 00:23:53,380
times log of n where n is the number of

00:23:52,360 --> 00:23:55,660
lines and K is the number of

00:23:53,380 --> 00:23:56,920
intersections among those lines and I

00:23:55,660 --> 00:23:58,750
don't know how to figure that out that's

00:23:56,920 --> 00:24:00,460
like a math e thing that you can just

00:23:58,750 --> 00:24:06,280
read about I didn't have to figure out

00:24:00,460 --> 00:24:10,930
what that complexity was now we saw this

00:24:06,280 --> 00:24:14,080
graph before up in the complexity really

00:24:10,930 --> 00:24:15,310
matters as numbers get large but another

00:24:14,080 --> 00:24:17,080
place where you have to be careful not

00:24:15,310 --> 00:24:19,750
to over apply the idea is when numbers

00:24:17,080 --> 00:24:22,630
are small so let's zoom into that lower

00:24:19,750 --> 00:24:24,190
left-hand corner of this of the graph if

00:24:22,630 --> 00:24:25,420
we zoom in there suddenly the lines

00:24:24,190 --> 00:24:27,850
don't look so clear-cut

00:24:25,420 --> 00:24:30,070
the green line is actually above the

00:24:27,850 --> 00:24:31,870
other lines for most of them and the N

00:24:30,070 --> 00:24:33,730
squared line that red line is actually

00:24:31,870 --> 00:24:35,680
below everything for a lot of the time

00:24:33,730 --> 00:24:38,080
and the reason is that when numbers are

00:24:35,680 --> 00:24:39,400
small all those coefficients and lower

00:24:38,080 --> 00:24:42,820
order components that we threw away

00:24:39,400 --> 00:24:45,400
those mattered right 3 n plus 1 when n

00:24:42,820 --> 00:24:47,430
is 1 that one at the end really matters

00:24:45,400 --> 00:24:50,230
and also we haven't taken into account

00:24:47,430 --> 00:24:52,690
what the actual time of the steps is you

00:24:50,230 --> 00:24:54,850
might have an N squared operation where

00:24:52,690 --> 00:24:56,830
you are doing N squared times a

00:24:54,850 --> 00:24:58,360
millisecond and you might be comparing

00:24:56,830 --> 00:25:01,450
that with a constant time algorithm that

00:24:58,360 --> 00:25:03,100
always takes a minute well n has to get

00:25:01,450 --> 00:25:04,210
pretty large before that constant time

00:25:03,100 --> 00:25:06,370
algorithm is worth it

00:25:04,210 --> 00:25:07,870
when n is a billion it's worth it but

00:25:06,370 --> 00:25:10,420
when n is ten you should stick with the

00:25:07,870 --> 00:25:12,310
N squared algorithm so as Rob Pike once

00:25:10,420 --> 00:25:15,910
said fancy algorithms are small when n

00:25:12,310 --> 00:25:19,930
is small and n is usually small so don't

00:25:15,910 --> 00:25:22,060
go overboard with trying to fancy up

00:25:19,930 --> 00:25:24,100
your algorithmic complexity

00:25:22,060 --> 00:25:27,520
it doesn't matter when n is small and

00:25:24,100 --> 00:25:28,810
usually your n is small alright some

00:25:27,520 --> 00:25:31,510
advanced topics there's a thing called

00:25:28,810 --> 00:25:33,730
amortization which is really a long term

00:25:31,510 --> 00:25:35,950
averaging over operations so when I say

00:25:33,730 --> 00:25:37,570
that appending to a list is o of 1 that

00:25:35,950 --> 00:25:39,010
doesn't mean that every single time you

00:25:37,570 --> 00:25:40,690
append to the list it takes a small

00:25:39,010 --> 00:25:42,520
amount of time in fact it usually takes

00:25:40,690 --> 00:25:43,870
a small amount of time but every once in

00:25:42,520 --> 00:25:45,700
a while the whole list has to be copied

00:25:43,870 --> 00:25:47,980
and moved someplace else which isn't

00:25:45,700 --> 00:25:49,630
which gets longer and longer as the list

00:25:47,980 --> 00:25:51,790
gets longer but it also gets less and

00:25:49,630 --> 00:25:53,290
less frequent as the list gets longer so

00:25:51,790 --> 00:25:56,770
that over the long run the average is

00:25:53,290 --> 00:25:58,750
still o of 1 so amortization is a fancy

00:25:56,770 --> 00:26:00,610
word meaning averaging and it means that

00:25:58,750 --> 00:26:02,800
individual operations can take

00:26:00,610 --> 00:26:04,540
difference amount of times algorithmic

00:26:02,800 --> 00:26:08,710
analysis is really about the long term

00:26:04,540 --> 00:26:10,390
trends over many many runs and we

00:26:08,710 --> 00:26:12,310
haven't talked about the worst case so

00:26:10,390 --> 00:26:13,900
earlier we talked about the typical case

00:26:12,310 --> 00:26:15,880
and some people think Big O implies

00:26:13,900 --> 00:26:17,830
typical case or big fellow implies worst

00:26:15,880 --> 00:26:19,180
case no you have to say whether you're

00:26:17,830 --> 00:26:21,550
talking about the complexity of the

00:26:19,180 --> 00:26:24,100
typical case of the worst case here's an

00:26:21,550 --> 00:26:26,470
example where I make a set of 50,000

00:26:24,100 --> 00:26:29,170
numbers which differ by 47 I'm kind of

00:26:26,470 --> 00:26:30,730
walking up the numbers by 47 adding that

00:26:29,170 --> 00:26:32,320
number into the set is an O of 1

00:26:30,730 --> 00:26:34,420
operation so the whole building the

00:26:32,320 --> 00:26:36,790
whole set is o of n and it took about 10

00:26:34,420 --> 00:26:38,620
milliseconds here I'm building another

00:26:36,790 --> 00:26:41,800
set of integers exactly the same size

00:26:38,620 --> 00:26:43,840
50,000 numbers but I happen to choose a

00:26:41,800 --> 00:26:46,000
step that I happen to know was gonna

00:26:43,840 --> 00:26:47,590
make all the hashes exactly the same so

00:26:46,000 --> 00:26:49,120
all the numbers got exactly the same

00:26:47,590 --> 00:26:51,220
hash which turns it into an O of an

00:26:49,120 --> 00:26:56,290
operation which means making this set

00:26:51,220 --> 00:26:58,270
took 34 seconds 3,300 times longer Dix

00:26:56,290 --> 00:27:00,910
also have this problem and people were

00:26:58,270 --> 00:27:02,920
using it to ddos web servers which is

00:27:00,910 --> 00:27:04,630
why Python added hash randomization and

00:27:02,920 --> 00:27:07,030
it's a fascinating topic but it's an

00:27:04,630 --> 00:27:08,620
example where although dicks are o of 1

00:27:07,030 --> 00:27:11,970
in the typical case occasionally you

00:27:08,620 --> 00:27:14,410
have to worry about the worst case and

00:27:11,970 --> 00:27:16,240
there's more math so if you dig into the

00:27:14,410 --> 00:27:17,860
math basically mathematicians have taken

00:27:16,240 --> 00:27:19,240
every letter they either looks like an O

00:27:17,860 --> 00:27:21,520
or sounds like an O and giving it a

00:27:19,240 --> 00:27:23,290
meaning and you don't need it you don't

00:27:21,520 --> 00:27:24,790
need it at all and there might be

00:27:23,290 --> 00:27:26,140
mathematicians in the audience right now

00:27:24,790 --> 00:27:27,700
who are gonna say you know you're not

00:27:26,140 --> 00:27:29,980
really even talking about Big O yeah

00:27:27,700 --> 00:27:32,990
shut up I don't care

00:27:29,980 --> 00:27:37,309
we all we all this is what we which is

00:27:32,990 --> 00:27:39,440
what we mean by Big O and those experts

00:27:37,309 --> 00:27:40,429
by the way so I wrote a blog post when I

00:27:39,440 --> 00:27:42,559
was first starting to think about this

00:27:40,429 --> 00:27:44,299
with the same title they go how code

00:27:42,559 --> 00:27:49,029
slows as data grows and a lot of people

00:27:44,299 --> 00:27:51,320
liked it but one guy wasn't so pleased

00:27:49,029 --> 00:27:52,700
he thought that not only had I gotten

00:27:51,320 --> 00:27:54,080
something wrong but the thing I'd gotten

00:27:52,700 --> 00:27:55,880
wrong was so important that the entire

00:27:54,080 --> 00:27:58,130
blog post was something that I should be

00:27:55,880 --> 00:27:59,330
ashamed of and I actually looked into it

00:27:58,130 --> 00:28:00,500
I gave him the benefit of the doubt I

00:27:59,330 --> 00:28:02,330
learned a little bit more about

00:28:00,500 --> 00:28:04,490
algorithmic analysis I concluded he was

00:28:02,330 --> 00:28:06,740
actually wrong he remains convinced he

00:28:04,490 --> 00:28:16,700
is right the good news is I got another

00:28:06,740 --> 00:28:18,770
blog post out of it so if on your

00:28:16,700 --> 00:28:20,720
journey to explore these things you find

00:28:18,770 --> 00:28:24,049
people like this just walk around them

00:28:20,720 --> 00:28:25,520
and keep going it doesn't matter it let

00:28:24,049 --> 00:28:26,720
me if you're into the math go and do the

00:28:25,520 --> 00:28:28,720
math but if you're just trying to do

00:28:26,720 --> 00:28:31,700
software engineering it doesn't matter

00:28:28,720 --> 00:28:33,529
alright so in conclusion Big O is useful

00:28:31,700 --> 00:28:34,940
it can help you understand how your code

00:28:33,529 --> 00:28:37,309
might perform when the data gets very

00:28:34,940 --> 00:28:39,799
large it doesn't have to be complicated

00:28:37,309 --> 00:28:42,940
it doesn't have to be a fee and you can

00:28:39,799 --> 00:28:52,690
do the thing Thanks

00:28:42,940 --> 00:28:52,690

YouTube URL: https://www.youtube.com/watch?v=duvZ-2UK0fc


