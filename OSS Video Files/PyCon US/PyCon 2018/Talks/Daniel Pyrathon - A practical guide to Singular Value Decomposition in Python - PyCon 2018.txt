Title: Daniel Pyrathon - A practical guide to Singular Value Decomposition in Python - PyCon 2018
Publication date: 2018-08-06
Playlist: Talks
Description: 
	Speaker: Daniel Pyrathon

Recommender systems have become increasingly popular in recent years, and are used by some of the largest websites in the world to predict the likelihood of a user taking an action on an item. In the world of Netflix, this means recommending similar movies to the ones you have seen. In the world of dating, this means suggesting matches similar to people you already showed interest in!

My path to recommenders has been an unusual one: from a Software Engineer to working on matching algorithms at a dating company, with a little background on machine learning. With my knowledge of Python and the use of basic SVD (Singular Value Decomposition) frameworks, I was able to understand SVDs from a practical standpoint of what you can do with them, instead of focusing on the science.

In my talk, you will learn 2 practical ways of generating recommendations using SVDs: matrix factorization and item similarity. We will be learning the high-level components of SVD the "doer way": we will be implementing a simple movie recommendation engine with the help of Jupiter notebooks, the MovieLens database, and the Surprise recommendation package.


Slides can be found at: https://speakerdeck.com/pycon2018 and https://github.com/PyCon/2018-slides
Captions: 
	00:00:03,980 --> 00:00:14,370
okay hey everyone welcome to the talk on

00:00:10,410 --> 00:00:16,890
a practical guide to singular value

00:00:14,370 --> 00:00:19,260
decomposition in Python please welcome

00:00:16,890 --> 00:00:21,300
Daniel who is an organizers for pi bay

00:00:19,260 --> 00:00:30,150
appreciative Python conference in Bay

00:00:21,300 --> 00:00:32,730
Area all right hello thank you for

00:00:30,150 --> 00:00:34,890
coming so today I want to cover two main

00:00:32,730 --> 00:00:37,469
things first of all I want to cover what

00:00:34,890 --> 00:00:39,570
is SVD most important in the context of

00:00:37,469 --> 00:00:41,940
recommendations and then the second

00:00:39,570 --> 00:00:45,180
thing is obviously how can we do SVD

00:00:41,940 --> 00:00:48,030
using Python but before we start let me

00:00:45,180 --> 00:00:51,300
ask you two questions have you ever

00:00:48,030 --> 00:00:55,350
wondered why you binge watch Netflix or

00:00:51,300 --> 00:00:58,890
maybe how did Spotify found out that

00:00:55,350 --> 00:01:00,449
secret love for Taylor Swift in fact

00:00:58,890 --> 00:01:03,510
there is something about these services

00:01:00,449 --> 00:01:06,720
such as Spotify such as Netflix that

00:01:03,510 --> 00:01:09,299
make them so so intimate and so so

00:01:06,720 --> 00:01:14,880
personal why is that

00:01:09,299 --> 00:01:15,810
the answer is recommendations let me

00:01:14,880 --> 00:01:17,250
tell you why I'm here

00:01:15,810 --> 00:01:19,080
so I work for an online dating company

00:01:17,250 --> 00:01:22,229
called coffee meets bagel who knows

00:01:19,080 --> 00:01:23,850
confidence they go hey right brilliant

00:01:22,229 --> 00:01:29,369
okay I hope you haven't had bad

00:01:23,850 --> 00:01:31,290
experiences it's a great dating app so

00:01:29,369 --> 00:01:33,210
and I recently moved from a traditional

00:01:31,290 --> 00:01:35,790
software engineering role to a machine

00:01:33,210 --> 00:01:38,189
learning engineer oh and most

00:01:35,790 --> 00:01:40,320
importantly I did so with very very

00:01:38,189 --> 00:01:41,549
little scientific background and I know

00:01:40,320 --> 00:01:44,490
what you're thinking that's a terrible

00:01:41,549 --> 00:01:47,270
idea but today what I want to tell you

00:01:44,490 --> 00:01:49,590
is what I want to show is how Python

00:01:47,270 --> 00:01:51,750
really helped me learn and get up to

00:01:49,590 --> 00:01:53,939
speed with some of the learning

00:01:51,750 --> 00:01:56,909
algorithms that we run at coffee meets

00:01:53,939 --> 00:01:58,619
bagel most importantly some of these

00:01:56,909 --> 00:02:01,590
algorithms are a lot more easier and

00:01:58,619 --> 00:02:02,850
logical to understand when you're

00:02:01,590 --> 00:02:04,890
looking at Python source code when

00:02:02,850 --> 00:02:06,210
they're implemented in Python and today

00:02:04,890 --> 00:02:08,970
I want to show you an example of one of

00:02:06,210 --> 00:02:10,500
these so before we start I just want to

00:02:08,970 --> 00:02:12,780
make sure we're on the same page what is

00:02:10,500 --> 00:02:13,770
a recommendation engine many of you that

00:02:12,780 --> 00:02:15,090
probably have not heard of

00:02:13,770 --> 00:02:17,550
Commendation engines have probably heard

00:02:15,090 --> 00:02:19,320
of search engines and search engines are

00:02:17,550 --> 00:02:21,540
just think of these as like black boxes

00:02:19,320 --> 00:02:24,600
where you insert some parameters and you

00:02:21,540 --> 00:02:26,910
get some search results recommendation

00:02:24,600 --> 00:02:28,800
engines are similar to search engines

00:02:26,910 --> 00:02:31,740
but they're personÃ­s it's just like

00:02:28,800 --> 00:02:34,020
having one search engine for every user

00:02:31,740 --> 00:02:36,600
in your application what are the

00:02:34,020 --> 00:02:38,100
benefits of search engines well because

00:02:36,600 --> 00:02:40,500
they're personalized for every user in

00:02:38,100 --> 00:02:41,970
your application they they give more

00:02:40,500 --> 00:02:43,260
personal results they give those

00:02:41,970 --> 00:02:45,840
intimate results that we were speaking

00:02:43,260 --> 00:02:48,420
about previously but most importantly

00:02:45,840 --> 00:02:51,420
they learn over time as your as your

00:02:48,420 --> 00:02:55,740
user explores the application as your

00:02:51,420 --> 00:02:56,820
user interacts with with with the items

00:02:55,740 --> 00:02:57,830
the resources that you have in your

00:02:56,820 --> 00:03:00,920
application

00:02:57,830 --> 00:03:04,530
our recommendation engine will learn and

00:03:00,920 --> 00:03:05,940
it will give better results again there

00:03:04,530 --> 00:03:08,430
are many benefits I just want to point

00:03:05,940 --> 00:03:10,410
out two the first one is engagement so

00:03:08,430 --> 00:03:11,940
there's a huge increase in engagement if

00:03:10,410 --> 00:03:13,770
you use a recommendation engine compared

00:03:11,940 --> 00:03:15,270
to a traditional search engine and then

00:03:13,770 --> 00:03:17,760
the second one is also diversity of

00:03:15,270 --> 00:03:19,650
recommendations so if your user is

00:03:17,760 --> 00:03:22,470
treated personally in your application

00:03:19,650 --> 00:03:24,870
there also is a lot more sort of

00:03:22,470 --> 00:03:27,780
engagement and diversity in the results

00:03:24,870 --> 00:03:31,050
that that user gets and if that wasn't

00:03:27,780 --> 00:03:32,670
enough let's look at some stats 35% of

00:03:31,050 --> 00:03:34,560
Amazon's revenue is actually generated

00:03:32,670 --> 00:03:38,730
by some level of recommendation engines

00:03:34,560 --> 00:03:41,190
and Netflix has over 75 percent of

00:03:38,730 --> 00:03:44,480
movies that are actually based on on the

00:03:41,190 --> 00:03:47,400
recommendations this is really huge and

00:03:44,480 --> 00:03:50,459
so today what I want to do is I want to

00:03:47,400 --> 00:03:52,410
cover only a specific subset of of what

00:03:50,459 --> 00:03:54,030
a recommendation engine is effectively

00:03:52,410 --> 00:03:56,820
it's it's a really big talker it's huge

00:03:54,030 --> 00:03:58,290
and what I want to talk about is a

00:03:56,820 --> 00:04:00,180
common strategy to perform

00:03:58,290 --> 00:04:03,420
recommendations which is called

00:04:00,180 --> 00:04:05,940
collaborative filtering in order to

00:04:03,420 --> 00:04:08,820
understand collaborative filtering we

00:04:05,940 --> 00:04:11,820
read to first focus on the three main

00:04:08,820 --> 00:04:17,880
ingredients that compose collaborative

00:04:11,820 --> 00:04:21,090
filtering users ratings and products the

00:04:17,880 --> 00:04:23,610
users are effectively the the actors in

00:04:21,090 --> 00:04:26,520
your in your system they rate products

00:04:23,610 --> 00:04:27,600
and you can think of the users as the

00:04:26,520 --> 00:04:29,790
readers of your blog

00:04:27,600 --> 00:04:33,270
the customers of your online grocery

00:04:29,790 --> 00:04:36,630
store effectively most of us assume some

00:04:33,270 --> 00:04:39,900
some you we assume to be a user in some

00:04:36,630 --> 00:04:41,760
application users rate products which is

00:04:39,900 --> 00:04:43,530
our the far right of this of this slide

00:04:41,760 --> 00:04:46,620
and the products are again very

00:04:43,530 --> 00:04:48,780
domain-specific they could be in case of

00:04:46,620 --> 00:04:52,440
maybe Spotify they could be songs or in

00:04:48,780 --> 00:04:55,200
Netflix maybe they could be movies now

00:04:52,440 --> 00:04:57,780
ratings are the glue that connects the

00:04:55,200 --> 00:04:59,760
users and the products effectively just

00:04:57,780 --> 00:05:03,780
think of the rating as a number to

00:04:59,760 --> 00:05:06,030
somehow quantify how well how good that

00:05:03,780 --> 00:05:08,610
interaction was between a user and a

00:05:06,030 --> 00:05:11,520
product and this number can be as

00:05:08,610 --> 00:05:13,350
granular as saying four out of five or

00:05:11,520 --> 00:05:16,440
it can be as simple as saying thumbs up

00:05:13,350 --> 00:05:19,590
or thumbs down right it's up to you as a

00:05:16,440 --> 00:05:22,050
developer now for the simplicity of this

00:05:19,590 --> 00:05:24,930
of these slides I'm only going to be

00:05:22,050 --> 00:05:26,130
focusing on movies as products so from

00:05:24,930 --> 00:05:27,780
now on we're only going to be speaking

00:05:26,130 --> 00:05:30,900
about movies but remember products can

00:05:27,780 --> 00:05:31,920
be whatever you want and so let's try

00:05:30,900 --> 00:05:35,730
and answer this question what is

00:05:31,920 --> 00:05:38,520
collaborative filtering so effectively

00:05:35,730 --> 00:05:41,400
collaborative filtering is a way to

00:05:38,520 --> 00:05:44,160
provide recommendations by leveraging

00:05:41,400 --> 00:05:46,920
the existing ratings of the users that

00:05:44,160 --> 00:05:49,320
are similar to you and probably the best

00:05:46,920 --> 00:05:52,560
way to understand it is with this matrix

00:05:49,320 --> 00:05:55,860
here on the right this is a user - this

00:05:52,560 --> 00:05:59,340
is a user - movie rate Rick's every row

00:05:55,860 --> 00:06:02,250
here is a unique user and every column

00:05:59,340 --> 00:06:05,130
here is a unique movie and the cell

00:06:02,250 --> 00:06:07,200
between a user and a movie identifies

00:06:05,130 --> 00:06:09,210
the rating that that user gave to that

00:06:07,200 --> 00:06:11,390
movie and so as you can see here they're

00:06:09,210 --> 00:06:13,980
two main ratings thumbs up means

00:06:11,390 --> 00:06:16,410
purchase the movie and thumbs down maybe

00:06:13,980 --> 00:06:19,620
means like you know give the movie back

00:06:16,410 --> 00:06:21,510
or something or dislike the movie this

00:06:19,620 --> 00:06:23,460
is our mini Netflix right we only have

00:06:21,510 --> 00:06:26,220
five users at four movies so it's a very

00:06:23,460 --> 00:06:29,190
limited catalog and so let's say our

00:06:26,220 --> 00:06:33,060
user five here logs into our mini

00:06:29,190 --> 00:06:35,070
Netflix and the mini Netflix asks our

00:06:33,060 --> 00:06:38,340
collaborative filtering engine can you

00:06:35,070 --> 00:06:41,470
provide a recommendation for the blue

00:06:38,340 --> 00:06:43,270
movie well how do we do that

00:06:41,470 --> 00:06:46,300
the first step in collaborative

00:06:43,270 --> 00:06:49,120
filtering is finding users similar to

00:06:46,300 --> 00:06:51,460
our user five here that have also rated

00:06:49,120 --> 00:06:56,160
that blue movie let's take it let's look

00:06:51,460 --> 00:06:58,840
at an example so user two and user five

00:06:56,160 --> 00:07:01,300
both rated it positively the green movie

00:06:58,840 --> 00:07:05,020
and rated negatively the red movie the

00:07:01,300 --> 00:07:08,290
orange movie user free and user five

00:07:05,020 --> 00:07:11,190
both rated positively the red movie and

00:07:08,290 --> 00:07:14,740
also rated positively the green movie

00:07:11,190 --> 00:07:18,460
most importantly both user two and user

00:07:14,740 --> 00:07:20,650
free has also rated the blue movie which

00:07:18,460 --> 00:07:22,690
is what we're trying to predict so let

00:07:20,650 --> 00:07:25,750
me ask any brave soul one to answer this

00:07:22,690 --> 00:07:31,420
question what will user five think about

00:07:25,750 --> 00:07:32,230
the blue movie boo alright fantastic yes

00:07:31,420 --> 00:07:35,560
that's correct

00:07:32,230 --> 00:07:37,540
so as you can see we leveraged existing

00:07:35,560 --> 00:07:40,320
uses information to generate this

00:07:37,540 --> 00:07:45,280
recommendation right we collaboratively

00:07:40,320 --> 00:07:47,169
found the answer and so again I want to

00:07:45,280 --> 00:07:48,220
go one step down the funnel there are

00:07:47,169 --> 00:07:49,990
many different ways to provide

00:07:48,220 --> 00:07:53,500
collaborative filtering today I'm going

00:07:49,990 --> 00:07:58,660
to focus on one specific singular value

00:07:53,500 --> 00:08:00,580
decomposition it's one of the most well

00:07:58,660 --> 00:08:02,710
known algorithms in industry today for

00:08:00,580 --> 00:08:04,150
providing collaborative filtering it's

00:08:02,710 --> 00:08:08,800
highly adopted industry it's actually

00:08:04,150 --> 00:08:10,810
used by by many companies and people

00:08:08,800 --> 00:08:14,020
like companies like Spotify have also

00:08:10,810 --> 00:08:16,270
written about their implementation and

00:08:14,020 --> 00:08:19,480
it's it's very very scalable actually

00:08:16,270 --> 00:08:22,480
and just to show you how widely this

00:08:19,480 --> 00:08:25,090
algorithm is used this is a picture from

00:08:22,480 --> 00:08:29,380
the net fixed price so the Netflix prize

00:08:25,090 --> 00:08:31,930
is this competition that was was created

00:08:29,380 --> 00:08:34,800
by Netflix in 2009 it's an open

00:08:31,930 --> 00:08:36,940
competition that basically called all

00:08:34,800 --> 00:08:39,219
engineers and researchers around the

00:08:36,940 --> 00:08:42,390
world to compete to somehow try and

00:08:39,219 --> 00:08:45,760
increase Netflix's recommendation engine

00:08:42,390 --> 00:08:49,270
well the winner of this competition

00:08:45,760 --> 00:08:53,500
which which are these these people over

00:08:49,270 --> 00:08:55,050
here actually one using variants of the

00:08:53,500 --> 00:08:58,800
SVD algorithm to

00:08:55,050 --> 00:09:03,149
increase Netflix's accuracy by 10%

00:08:58,800 --> 00:09:06,089
that's huge and just to show you the

00:09:03,149 --> 00:09:10,529
price these people got 1 million dollars

00:09:06,089 --> 00:09:14,910
using SVD so listen to this talk listen

00:09:10,529 --> 00:09:17,820
to this talk alright so hopefully by now

00:09:14,910 --> 00:09:20,130
I've tried to convince you that SVD is a

00:09:17,820 --> 00:09:22,440
cool thing but in order to understand

00:09:20,130 --> 00:09:25,770
how SVD actually works right the most

00:09:22,440 --> 00:09:27,959
important things I want to explain to

00:09:25,770 --> 00:09:30,660
you how SVD actually predicts these new

00:09:27,959 --> 00:09:33,750
recommendations effectively SVD is an

00:09:30,660 --> 00:09:36,660
algorithm that creates these things

00:09:33,750 --> 00:09:41,990
called latent features for every user

00:09:36,660 --> 00:09:44,399
and every movie in our ratings database

00:09:41,990 --> 00:09:45,870
but in order to understand what latent

00:09:44,399 --> 00:09:48,200
features are let's first understand what

00:09:45,870 --> 00:09:50,760
features are just to make sure so

00:09:48,200 --> 00:09:53,310
features in the context of machine

00:09:50,760 --> 00:09:56,220
learning are the metadata that we as

00:09:53,310 --> 00:09:59,190
scientists or engineers attribute or

00:09:56,220 --> 00:10:02,370
associate to our users and our products

00:09:59,190 --> 00:10:04,079
so for example in the context of Netflix

00:10:02,370 --> 00:10:06,600
what would be the attributes that we

00:10:04,079 --> 00:10:09,480
would associate to our user may be their

00:10:06,600 --> 00:10:12,450
age the region they're in or their

00:10:09,480 --> 00:10:15,089
gender and what about the movies well

00:10:12,450 --> 00:10:17,339
the release date may be the director

00:10:15,089 --> 00:10:20,970
somehow matters the duration of the

00:10:17,339 --> 00:10:25,110
movie these are all really informative

00:10:20,970 --> 00:10:28,500
features for us as humans in fact we we

00:10:25,110 --> 00:10:31,079
assume that the rating that maybe this

00:10:28,500 --> 00:10:34,560
user gives to this beautiful green movie

00:10:31,079 --> 00:10:37,709
is somehow impacted by these features

00:10:34,560 --> 00:10:39,630
that we define right probably the best

00:10:37,709 --> 00:10:43,529
way to understand this is if you got

00:10:39,630 --> 00:10:46,350
like pass ratings of maybe IMDB or

00:10:43,529 --> 00:10:47,940
something like that and you actually you

00:10:46,350 --> 00:10:50,160
could actually use these features to

00:10:47,940 --> 00:10:55,050
generate some statistics like maybe

00:10:50,160 --> 00:10:57,450
males in their 20s are more rates higher

00:10:55,050 --> 00:10:59,220
Wes Anderson movies than males in their

00:10:57,450 --> 00:11:02,310
40s something like this wait

00:10:59,220 --> 00:11:03,870
statistically and so what we do is we

00:11:02,310 --> 00:11:05,339
generate these features so that then we

00:11:03,870 --> 00:11:08,760
can feed them in some kind of learning

00:11:05,339 --> 00:11:11,040
algorithm that somehow identifies the

00:11:08,760 --> 00:11:14,940
or the weight of every one of these

00:11:11,040 --> 00:11:16,830
features but again there's one fault

00:11:14,940 --> 00:11:19,890
here which is these features are

00:11:16,830 --> 00:11:22,380
generated by us as humans and as humans

00:11:19,890 --> 00:11:24,360
we can only define features that are

00:11:22,380 --> 00:11:28,110
somehow directly observable things that

00:11:24,360 --> 00:11:30,780
we assume are important but most of the

00:11:28,110 --> 00:11:34,350
times there's a set of features that are

00:11:30,780 --> 00:11:36,600
not is not directly observable but it's

00:11:34,350 --> 00:11:39,780
a lot more impactful in predicting a

00:11:36,600 --> 00:11:46,620
rating in fact there's a class of

00:11:39,780 --> 00:11:49,650
algorithms like SVD that learn create

00:11:46,620 --> 00:11:52,140
out of nothing these new new features

00:11:49,650 --> 00:11:56,340
which we call latent features just like

00:11:52,140 --> 00:11:58,590
age region and gender that are somehow

00:11:56,340 --> 00:12:01,010
not directly observable we can't point

00:11:58,590 --> 00:12:04,560
out saying the first latent feature

00:12:01,010 --> 00:12:07,530
identifies I don't know the users users

00:12:04,560 --> 00:12:10,470
that have blonde eyes or something like

00:12:07,530 --> 00:12:12,300
this we cannot do that but all we know

00:12:10,470 --> 00:12:14,430
is that these features can be used and

00:12:12,300 --> 00:12:16,350
are highly informative in the context of

00:12:14,430 --> 00:12:20,040
recommendations in predicting a rating

00:12:16,350 --> 00:12:23,310
between a user and a movie and so SVD as

00:12:20,040 --> 00:12:25,950
an algorithm runs on these data sets and

00:12:23,310 --> 00:12:29,310
generates these features and what do

00:12:25,950 --> 00:12:32,340
these features look like sorry about the

00:12:29,310 --> 00:12:33,870
picture of it there they're very

00:12:32,340 --> 00:12:37,410
ambiguous there's no way to actually

00:12:33,870 --> 00:12:40,010
describe them and this is my creativity

00:12:37,410 --> 00:12:42,480
in trying to describe the features

00:12:40,010 --> 00:12:44,460
they're not really something that you

00:12:42,480 --> 00:12:47,640
can actually point out these are very

00:12:44,460 --> 00:12:51,090
abstract but again we can use these

00:12:47,640 --> 00:12:53,160
features in our data sets so now let's

00:12:51,090 --> 00:12:56,100
try to actually understand how can we

00:12:53,160 --> 00:12:58,110
use these latent features how are these

00:12:56,100 --> 00:13:01,020
features actually generated let's start

00:12:58,110 --> 00:13:03,660
from there so again let's take our

00:13:01,020 --> 00:13:05,550
Netflix meaning Netflix example before

00:13:03,660 --> 00:13:07,860
this is a mini mini Netflix and even

00:13:05,550 --> 00:13:09,870
smaller right we used to have other two

00:13:07,860 --> 00:13:12,680
users while they turned so we now we

00:13:09,870 --> 00:13:14,910
have only three users and four movies

00:13:12,680 --> 00:13:16,410
most important you can see here I'm not

00:13:14,910 --> 00:13:18,900
using thumbs up and thumbs down anymore

00:13:16,410 --> 00:13:21,480
I'm actually using a number remember the

00:13:18,900 --> 00:13:22,710
higher the rating the more relevant so

00:13:21,480 --> 00:13:26,640
for example here

00:13:22,710 --> 00:13:29,130
user to rated very highly the red movie

00:13:26,640 --> 00:13:31,560
here but rated very poorly the blue

00:13:29,130 --> 00:13:36,540
movie because four is obviously bigger

00:13:31,560 --> 00:13:38,399
than what now let's just put this matrix

00:13:36,540 --> 00:13:41,970
over here one second so it turns out

00:13:38,399 --> 00:13:43,920
that there is this this this algorithm

00:13:41,970 --> 00:13:46,170
which is called matrix factorization is

00:13:43,920 --> 00:13:51,510
this technique what matrix factorization

00:13:46,170 --> 00:13:54,270
does is it gets one big matrix and it

00:13:51,510 --> 00:13:57,330
creates two smaller matrices out of this

00:13:54,270 --> 00:13:59,250
big matrix in a way in which we could

00:13:57,330 --> 00:14:01,350
take those two matrices that we create

00:13:59,250 --> 00:14:05,160
and in the second moment we could

00:14:01,350 --> 00:14:06,600
recompose the original matrix it's a bit

00:14:05,160 --> 00:14:13,230
hard to understand but let's try to

00:14:06,600 --> 00:14:15,510
visualize it okay SVD is the and

00:14:13,230 --> 00:14:18,690
implementation an algorithm that uses

00:14:15,510 --> 00:14:22,290
matrix factorization so decomposes these

00:14:18,690 --> 00:14:27,060
two matrices in a way in which we only

00:14:22,290 --> 00:14:28,980
retain the most informative features the

00:14:27,060 --> 00:14:30,750
most important factors from these two

00:14:28,980 --> 00:14:32,610
matrices the ones that have most

00:14:30,750 --> 00:14:34,380
important big esteem portance and the

00:14:32,610 --> 00:14:36,290
number of features that we actually

00:14:34,380 --> 00:14:39,810
extract is something that us as

00:14:36,290 --> 00:14:41,970
developers or scientists define we tell

00:14:39,810 --> 00:14:44,190
SVD how many features we want from this

00:14:41,970 --> 00:14:47,640
data set so let's say we wanted to run

00:14:44,190 --> 00:14:49,260
SVD on this matrix here and we only

00:14:47,640 --> 00:14:53,220
wanted to output two features what would

00:14:49,260 --> 00:14:55,770
it look like as you can see here we have

00:14:53,220 --> 00:14:58,350
every user has these two latent features

00:14:55,770 --> 00:15:01,350
here and every movie has two latent

00:14:58,350 --> 00:15:03,000
features and if you somehow believe me

00:15:01,350 --> 00:15:05,640
you could get these two matrices and we

00:15:03,000 --> 00:15:07,740
multiply them together and you would get

00:15:05,640 --> 00:15:11,209
an approximation that most the nearest

00:15:07,740 --> 00:15:14,730
approximation to that Center matrix

00:15:11,209 --> 00:15:16,140
minus the question marks which basically

00:15:14,730 --> 00:15:19,440
means that you can actually create and

00:15:16,140 --> 00:15:21,360
generate new predictions now what about

00:15:19,440 --> 00:15:23,310
these features why are they I only use

00:15:21,360 --> 00:15:25,649
two latent features here simply for

00:15:23,310 --> 00:15:27,149
illustration purpose but in your real

00:15:25,649 --> 00:15:29,459
example you'll probably want to use more

00:15:27,149 --> 00:15:31,709
than those the more latent features you

00:15:29,459 --> 00:15:33,720
generate the more information you're

00:15:31,709 --> 00:15:36,259
actually extracting from the original

00:15:33,720 --> 00:15:38,059
matrix the more latent features

00:15:36,259 --> 00:15:41,209
you extract the more computation

00:15:38,059 --> 00:15:42,829
expensive your SVD is going to take so

00:15:41,209 --> 00:15:46,339
that is a number that you are going to

00:15:42,829 --> 00:15:48,229
describe so now we have these little

00:15:46,339 --> 00:15:50,239
features what can we do with them well

00:15:48,229 --> 00:15:52,160
today I want to show two use cases of

00:15:50,239 --> 00:15:55,249
how we can use these these latent

00:15:52,160 --> 00:15:58,220
features the first one is to predict new

00:15:55,249 --> 00:16:00,919
scores effectively once we generate

00:15:58,220 --> 00:16:03,019
these latent features we can we can

00:16:00,919 --> 00:16:06,619
predict the rating between any

00:16:03,019 --> 00:16:08,569
combination of user and movie so let's

00:16:06,619 --> 00:16:11,779
take an example here let's say we had

00:16:08,569 --> 00:16:13,549
our user 1 and we wanted to predict what

00:16:11,779 --> 00:16:16,039
user 1 would think about the blue movie

00:16:13,549 --> 00:16:19,069
so we just perform a dot product between

00:16:16,039 --> 00:16:22,549
the latent features of user 1 and the

00:16:19,069 --> 00:16:25,519
latent features of the blue movie so we

00:16:22,549 --> 00:16:28,009
multiply latent feature 1 of user 1 with

00:16:25,519 --> 00:16:30,589
latent feature 1 of blue movie and then

00:16:28,009 --> 00:16:33,439
we sum latent feature 2 of user 1 with

00:16:30,589 --> 00:16:37,100
latent feature to a blue movie result

00:16:33,439 --> 00:16:39,739
here is 3 point 5 2 so effectively user

00:16:37,100 --> 00:16:43,429
1 would pretty much enjoy the blue movie

00:16:39,739 --> 00:16:44,869
based on this example and let's look at

00:16:43,429 --> 00:16:47,419
another way in which we can actually use

00:16:44,869 --> 00:16:51,160
these these vectors and this is the way

00:16:47,419 --> 00:16:53,269
I actually actually like most which is

00:16:51,160 --> 00:16:55,939
effectively once we generate these

00:16:53,269 --> 00:16:57,919
latent features we could compare users

00:16:55,939 --> 00:17:00,769
with other users and movies with other

00:16:57,919 --> 00:17:03,049
movies we could actually find similarity

00:17:00,769 --> 00:17:06,679
the similarity between two users or two

00:17:03,049 --> 00:17:08,779
movies by identifying the similarity of

00:17:06,679 --> 00:17:10,760
their vectors and there are many

00:17:08,779 --> 00:17:13,250
different ways to measure similarity

00:17:10,760 --> 00:17:15,769
today I'm going to be using cosine

00:17:13,250 --> 00:17:17,750
cosine similarity which effectively

00:17:15,769 --> 00:17:20,659
measures the angle the cosine of the

00:17:17,750 --> 00:17:23,059
angle between these two vectors and so

00:17:20,659 --> 00:17:27,079
in this example you could probably say

00:17:23,059 --> 00:17:30,379
that if a user user free here is more

00:17:27,079 --> 00:17:33,799
closer in terms of tastes two user one

00:17:30,379 --> 00:17:36,649
is more similar to user 1 than user free

00:17:33,799 --> 00:17:39,740
two user two what that effectively means

00:17:36,649 --> 00:17:42,409
is that user free is more likely to

00:17:39,740 --> 00:17:47,889
agree with the with the tastes the

00:17:42,409 --> 00:17:49,970
ratings of user 1 then user two and so

00:17:47,889 --> 00:17:52,669
hopefully this'll be fun but

00:17:49,970 --> 00:17:55,130
what I want to show you now is a small

00:17:52,669 --> 00:17:58,850
demo effectively what we're going to do

00:17:55,130 --> 00:18:01,940
is we are going to tada

00:17:58,850 --> 00:18:06,169
we are going to be training an SVD using

00:18:01,940 --> 00:18:07,880
a library called surprise SVD it's a

00:18:06,169 --> 00:18:10,159
really great library it's very very

00:18:07,880 --> 00:18:11,390
simple to use and what we are going to

00:18:10,159 --> 00:18:14,000
be doing is we're going to be

00:18:11,390 --> 00:18:17,630
downloading and exploring this dataset

00:18:14,000 --> 00:18:19,010
which is called movie lens and once

00:18:17,630 --> 00:18:22,370
we've explored this data set we are

00:18:19,010 --> 00:18:24,980
going to train an SVD using very very

00:18:22,370 --> 00:18:26,840
few steps you're going to go back home

00:18:24,980 --> 00:18:30,590
today and you can do this on your own

00:18:26,840 --> 00:18:33,860
data set and then finally those two ways

00:18:30,590 --> 00:18:37,100
that I was showing you before generating

00:18:33,860 --> 00:18:38,480
recommendations by reconstructing the

00:18:37,100 --> 00:18:41,929
result by performing the dot product

00:18:38,480 --> 00:18:45,470
between a user in a movie and similarity

00:18:41,929 --> 00:18:50,590
between movies we're actually going to

00:18:45,470 --> 00:18:52,909
be able to do that using this example so

00:18:50,590 --> 00:18:54,860
let's start by speaking about movie lens

00:18:52,909 --> 00:18:56,990
so movie lens is a great open source

00:18:54,860 --> 00:19:00,799
data set it's pretty famous it's using

00:18:56,990 --> 00:19:02,539
all our competitions and it has a lot of

00:19:00,799 --> 00:19:05,059
different ratings and a lot of unique

00:19:02,539 --> 00:19:06,799
movies and users I built a small

00:19:05,059 --> 00:19:10,940
function here called load movie lens

00:19:06,799 --> 00:19:13,700
that simply converts the CSV file to a

00:19:10,940 --> 00:19:15,559
data frame a panda's data frame and as

00:19:13,700 --> 00:19:18,590
you can see here every row is a is a

00:19:15,559 --> 00:19:21,830
pass rating right and so for example if

00:19:18,590 --> 00:19:23,780
you take the first row this user 742

00:19:21,830 --> 00:19:26,929
rated Jeremy McGuire with a rating of 4

00:19:23,780 --> 00:19:30,530
and again the ratings go in scale from 1

00:19:26,929 --> 00:19:32,659
to 5 inclusive as you can see here there

00:19:30,530 --> 00:19:35,960
are free columns to this to this data

00:19:32,659 --> 00:19:37,610
frame the first column is a user ID the

00:19:35,960 --> 00:19:40,130
second column is the movie title and

00:19:37,610 --> 00:19:44,000
these are strings they can be whatever

00:19:40,130 --> 00:19:46,280
you want your users and your your items

00:19:44,000 --> 00:19:48,919
and your your items on your part can be

00:19:46,280 --> 00:19:50,690
whatever data structure you want the

00:19:48,919 --> 00:19:53,419
important thing is that the rating is a

00:19:50,690 --> 00:19:58,399
number right remember needs to be

00:19:53,419 --> 00:20:03,470
quantifiable so how can we train an SPD

00:19:58,399 --> 00:20:06,950
using 4 simple steps the first step

00:20:03,470 --> 00:20:09,169
is we obviously import surprise SVD and

00:20:06,950 --> 00:20:12,530
we define something called a reader and

00:20:09,169 --> 00:20:14,990
a reader is effectively a component

00:20:12,530 --> 00:20:17,990
inside of is a class inside of surprise

00:20:14,990 --> 00:20:21,860
that defines the lower and upper bounds

00:20:17,990 --> 00:20:23,450
of your ratings remember you can choose

00:20:21,860 --> 00:20:24,799
whatever lower and upper bound you want

00:20:23,450 --> 00:20:27,740
the important thing is that you define

00:20:24,799 --> 00:20:29,600
it upfront and you tell surprise so in

00:20:27,740 --> 00:20:31,789
this case movie Lance has a great open

00:20:29,600 --> 00:20:33,890
source documentation which tells us that

00:20:31,789 --> 00:20:39,309
the ratings go from 1 to 5 so this is

00:20:33,890 --> 00:20:42,700
what I'm going to define second step is

00:20:39,309 --> 00:20:45,320
we initialize a data set instance and

00:20:42,700 --> 00:20:47,450
just think of the data set as a as a

00:20:45,320 --> 00:20:51,520
loader write effectively what it does is

00:20:47,450 --> 00:20:53,780
it prepares the data for performing SVD

00:20:51,520 --> 00:20:56,000
data set accepts two parameters the

00:20:53,780 --> 00:20:58,010
first one is the movie lens data set the

00:20:56,000 --> 00:21:00,890
movie lens data frame that we we defined

00:20:58,010 --> 00:21:02,960
previously and remember this needs to

00:21:00,890 --> 00:21:04,700
have this needs to be a panda's data

00:21:02,960 --> 00:21:07,520
frame which has free columns in this

00:21:04,700 --> 00:21:09,919
specific order user ID product ID and

00:21:07,520 --> 00:21:14,750
rating it's very important and then the

00:21:09,919 --> 00:21:17,600
second parameter is that reader now as

00:21:14,750 --> 00:21:20,840
the first step what we want to do is we

00:21:17,600 --> 00:21:24,140
want to retain some amount of the data

00:21:20,840 --> 00:21:25,820
set for testing purposes later today I'm

00:21:24,140 --> 00:21:28,280
actually not going to show you how to

00:21:25,820 --> 00:21:30,020
perform testing but if you download my

00:21:28,280 --> 00:21:33,980
notebook you'll actually find some

00:21:30,020 --> 00:21:36,669
hidden slides that perform that finally

00:21:33,980 --> 00:21:41,510
the moment we've all been waiting for is

00:21:36,669 --> 00:21:44,000
we initialize a new SVD instance and we

00:21:41,510 --> 00:21:45,710
fit our data set and here as you can see

00:21:44,000 --> 00:21:47,210
the SVD accepts one argument in its

00:21:45,710 --> 00:21:49,400
constructor which is number of factors

00:21:47,210 --> 00:21:52,400
this is the number of latent features we

00:21:49,400 --> 00:21:54,530
want to use remember this is here I

00:21:52,400 --> 00:21:56,480
chose 100 as an arbitrary number it just

00:21:54,530 --> 00:21:57,830
worked well with my examples but you can

00:21:56,480 --> 00:22:00,740
choose whatever number you want whatever

00:21:57,830 --> 00:22:03,559
number you feel confident with the data

00:22:00,740 --> 00:22:05,090
set that I've been using I with the

00:22:03,559 --> 00:22:08,539
movie lens data set if you just take a

00:22:05,090 --> 00:22:10,159
good subset for example in this example

00:22:08,539 --> 00:22:11,679
you will be using it will take very very

00:22:10,159 --> 00:22:14,450
little to train it's not a

00:22:11,679 --> 00:22:18,340
computationally it's not a very

00:22:14,450 --> 00:22:18,340
expensive in this example

00:22:18,770 --> 00:22:25,380
so once we've trained once we've trained

00:22:22,590 --> 00:22:28,200
that SVD instance remember what happens

00:22:25,380 --> 00:22:30,300
the SVD creates those two matrices the

00:22:28,200 --> 00:22:33,090
user matrix and the the movie matrix

00:22:30,300 --> 00:22:34,980
right and every one of these matrices

00:22:33,090 --> 00:22:36,540
has all these latent features so where

00:22:34,980 --> 00:22:38,040
are these where are these matrices now

00:22:36,540 --> 00:22:43,230
well it turns out that there's this

00:22:38,040 --> 00:22:45,510
attribute called a qi on top of the

00:22:43,230 --> 00:22:50,520
model that gets created once the SVD is

00:22:45,510 --> 00:22:54,600
run and as you can see this this model

00:22:50,520 --> 00:22:54,990
has 596 rows and 100 columns so why is

00:22:54,600 --> 00:22:58,260
that

00:22:54,990 --> 00:23:00,600
well they're 596 unique movies in our

00:22:58,260 --> 00:23:04,110
ratings and every movie will now have

00:23:00,600 --> 00:23:09,290
these 100 latent features so now you may

00:23:04,110 --> 00:23:13,020
be asking you maybe won't ask well sorry

00:23:09,290 --> 00:23:15,360
okay so we have all these latent

00:23:13,020 --> 00:23:19,950
features but how do we map every vector

00:23:15,360 --> 00:23:22,620
back to its movie well it turns out

00:23:19,950 --> 00:23:24,990
there's just like hidden attribute which

00:23:22,620 --> 00:23:26,600
is called raw to inner ID items which is

00:23:24,990 --> 00:23:29,970
a dictionary and this dictionary

00:23:26,600 --> 00:23:32,040
effectively maps every every item every

00:23:29,970 --> 00:23:35,700
movie and the way we defined it in this

00:23:32,040 --> 00:23:37,290
case is a string to be row index that

00:23:35,700 --> 00:23:39,090
corresponds to the row index of the

00:23:37,290 --> 00:23:41,580
latent features so let's take an example

00:23:39,090 --> 00:23:45,000
say we wanted to identify Toy Story the

00:23:41,580 --> 00:23:48,150
first thing that we do is we index we

00:23:45,000 --> 00:23:50,430
find the the row index of Toy Story by

00:23:48,150 --> 00:23:53,400
using the name and then once we have the

00:23:50,430 --> 00:23:55,290
row index we can actually find all the

00:23:53,400 --> 00:23:58,290
latent features by indexing the QI

00:23:55,290 --> 00:24:00,210
matrix at that particular row you're

00:23:58,290 --> 00:24:02,280
gonna get the entire row back and that's

00:24:00,210 --> 00:24:07,680
going to be 100 latent features in this

00:24:02,280 --> 00:24:12,200
example so now we've learned how to

00:24:07,680 --> 00:24:14,460
train an SVD and we've learned how to

00:24:12,200 --> 00:24:17,130
identify where these latent features

00:24:14,460 --> 00:24:19,880
actually are hidden so it's time to show

00:24:17,130 --> 00:24:22,170
you two great examples the first one is

00:24:19,880 --> 00:24:26,280
predicting on your rating between any

00:24:22,170 --> 00:24:28,800
combination of user and movie this is

00:24:26,280 --> 00:24:30,120
very very simple as a refresher this is

00:24:28,800 --> 00:24:30,840
what our movie lens data frame looks

00:24:30,120 --> 00:24:32,340
like our

00:24:30,840 --> 00:24:35,910
user IDs are defined as strings here

00:24:32,340 --> 00:24:40,050
it's user for free seven and movie title

00:24:35,910 --> 00:24:41,640
here also define of strings so surprises

00:24:40,050 --> 00:24:45,150
VD makes it really simple there is a

00:24:41,640 --> 00:24:48,540
predict API and this predict API accepts

00:24:45,150 --> 00:24:50,760
two parameters the user and the movie

00:24:48,540 --> 00:24:52,890
and the output here is going to be a

00:24:50,760 --> 00:24:55,080
prediction object which will give you

00:24:52,890 --> 00:24:57,600
this inside the prediction object you'll

00:24:55,080 --> 00:24:59,790
have this EST attribute which will

00:24:57,600 --> 00:25:03,330
effectively tell you the predicted

00:24:59,790 --> 00:25:06,120
rating in this case yes cs4 which means

00:25:03,330 --> 00:25:07,980
pretty high right so I mean everyone

00:25:06,120 --> 00:25:13,950
loves toy story but this user in

00:25:07,980 --> 00:25:15,140
particular now the moment we've all been

00:25:13,950 --> 00:25:18,810
waiting for

00:25:15,140 --> 00:25:21,260
recommendations by comparing items this

00:25:18,810 --> 00:25:25,770
in my opinion is the most fun part so

00:25:21,260 --> 00:25:28,410
remember as a refresher two movies two

00:25:25,770 --> 00:25:31,040
products are similar when the cosine

00:25:28,410 --> 00:25:35,100
distance is as near to zero as possible

00:25:31,040 --> 00:25:36,810
so what we're going to do here is we're

00:25:35,100 --> 00:25:39,140
going to fetch vectors for free movies

00:25:36,810 --> 00:25:41,310
the first one is the original Star Wars

00:25:39,140 --> 00:25:47,190
second one is return of the Jedi and the

00:25:41,310 --> 00:25:49,080
third one is Aladdin first I'm going to

00:25:47,190 --> 00:25:51,120
measure the distance between the Star

00:25:49,080 --> 00:25:53,900
Wars vector and the Return of the Jedi

00:25:51,120 --> 00:26:01,050
vector as you can see the distance is

00:25:53,900 --> 00:26:02,310
0.26 oh sorry 129 and let's say I wanted

00:26:01,050 --> 00:26:04,980
to perform the same distance between

00:26:02,310 --> 00:26:08,700
Star Wars and Aladdin

00:26:04,980 --> 00:26:11,670
well our distance here is 0.85 so

00:26:08,700 --> 00:26:13,410
effectively what that means is that Star

00:26:11,670 --> 00:26:15,690
Wars the original Star Wars is a lot

00:26:13,410 --> 00:26:20,220
more similar to Return of the Jedi

00:26:15,690 --> 00:26:21,690
than Star Wars to Aladdin and we didn't

00:26:20,220 --> 00:26:24,150
and remember these were generated

00:26:21,690 --> 00:26:25,830
without any information any metadata of

00:26:24,150 --> 00:26:27,840
the original features there was no

00:26:25,830 --> 00:26:31,320
concept of a director there was no

00:26:27,840 --> 00:26:35,190
concept of a genre nothing just through

00:26:31,320 --> 00:26:37,200
the ratings so now what if we wanted to

00:26:35,190 --> 00:26:39,120
find similar movies by performing

00:26:37,200 --> 00:26:40,800
ranking right in fact that what we could

00:26:39,120 --> 00:26:43,950
do is we could build one small function

00:26:40,800 --> 00:26:44,880
which would accept a movie title and it

00:26:43,950 --> 00:26:47,070
would Jen

00:26:44,880 --> 00:26:49,230
the similarity between that movie title

00:26:47,070 --> 00:26:51,300
and all the other movies in our system

00:26:49,230 --> 00:26:55,500
and then rank those movies by similarity

00:26:51,300 --> 00:26:57,950
what would this look like if they wanted

00:26:55,500 --> 00:27:00,540
to generate similarities for Star Wars

00:26:57,950 --> 00:27:03,540
first result obviously Star Wars because

00:27:00,540 --> 00:27:06,140
it's itself Empire Strikes Back Return

00:27:03,540 --> 00:27:08,460
of the Jedi Raiders of the Lost Ark

00:27:06,140 --> 00:27:11,070
these are shockingly these are

00:27:08,460 --> 00:27:13,280
shockingly accurate and remember there

00:27:11,070 --> 00:27:15,720
was no information at all about

00:27:13,280 --> 00:27:18,240
potentially the same director or the

00:27:15,720 --> 00:27:22,290
same saga or any of that let's look at

00:27:18,240 --> 00:27:25,230
pulp fiction okay Edward Trainspotting

00:27:22,290 --> 00:27:26,880
from dusk till dawn and so what

00:27:25,230 --> 00:27:30,630
effectively this also turns out in a

00:27:26,880 --> 00:27:33,600
concept of rating is maybe you have

00:27:30,630 --> 00:27:35,520
users that enjoyed pulp fiction and if

00:27:33,600 --> 00:27:37,680
they enjoy pulp fiction well why don't

00:27:35,520 --> 00:27:39,330
you see one of these movies because they

00:27:37,680 --> 00:27:41,250
are shockingly similar and people rate

00:27:39,330 --> 00:27:42,570
them in the same way so this is also

00:27:41,250 --> 00:27:48,690
another way of providing recommendations

00:27:42,570 --> 00:27:50,700
to your users so in conclusion SPT is a

00:27:48,690 --> 00:27:52,770
really powerful technique to provide

00:27:50,700 --> 00:27:54,150
those recommendations once you generate

00:27:52,770 --> 00:27:56,250
these latent features you can use them

00:27:54,150 --> 00:27:59,370
in so many ways you can even use them as

00:27:56,250 --> 00:28:02,670
as features for your classification

00:27:59,370 --> 00:28:05,580
algorithms after and most importantly if

00:28:02,670 --> 00:28:08,160
you want to get into SVD but you do not

00:28:05,580 --> 00:28:10,050
have a scientific background really try

00:28:08,160 --> 00:28:12,300
some of these libraries because Python

00:28:10,050 --> 00:28:15,720
makes the barrier of entry solo for you

00:28:12,300 --> 00:28:20,490
and you can really learn as an engineer

00:28:15,720 --> 00:28:22,870
how these things actually work that's

00:28:20,490 --> 00:28:32,570
all thank you very much

00:28:22,870 --> 00:28:32,570
[Applause]

00:28:33,049 --> 00:28:42,000
if you guys have questions there's a mic

00:28:35,520 --> 00:28:44,250
up front we can probably take three hi I

00:28:42,000 --> 00:28:45,900
was wondering how resilient SPD is

00:28:44,250 --> 00:28:50,340
against the curse curse of

00:28:45,900 --> 00:28:52,920
dimensionality yes so so yeah I did I

00:28:50,340 --> 00:28:55,590
did try I wanted to cover Pearson in

00:28:52,920 --> 00:28:57,540
this example but I found cosine

00:28:55,590 --> 00:28:59,010
similarity at just a very very simple

00:28:57,540 --> 00:29:00,720
and more visual way to actually show

00:28:59,010 --> 00:29:05,850
this difference but yeah you could use n

00:29:00,720 --> 00:29:08,460
you could also use Pearson so I was

00:29:05,850 --> 00:29:12,030
wondering if you're launching a new

00:29:08,460 --> 00:29:15,330
product and you basically have a very

00:29:12,030 --> 00:29:18,240
poorly populated sparse matrix like at

00:29:15,330 --> 00:29:21,270
what point do you get where the training

00:29:18,240 --> 00:29:23,490
of the SVD becomes actually feasible

00:29:21,270 --> 00:29:25,710
right like if if in your example of the

00:29:23,490 --> 00:29:27,600
three by four you only have two ratings

00:29:25,710 --> 00:29:32,450
yeah then it's not gonna be that helpful

00:29:27,600 --> 00:29:32,450
yeah that's that's a great question so

00:29:32,960 --> 00:29:37,950
SVD is luckily a very parallelizable

00:29:36,840 --> 00:29:40,559
algorithm and in fact there are

00:29:37,950 --> 00:29:42,120
different implementations of this

00:29:40,559 --> 00:29:44,429
concept of matrix factorization one of

00:29:42,120 --> 00:29:47,010
them is called ALS alternate least

00:29:44,429 --> 00:29:48,540
squares which has really great parallel

00:29:47,010 --> 00:29:50,640
implementations one of them is actually

00:29:48,540 --> 00:29:51,809
present in PI spark and it's also one of

00:29:50,640 --> 00:29:55,710
them that we actually use in production

00:29:51,809 --> 00:29:58,710
at coffee meets bagel and although there

00:29:55,710 --> 00:30:03,390
we have found that SVD is accurate under

00:29:58,710 --> 00:30:06,830
some aspects a OS is a very paralyzed of

00:30:03,390 --> 00:30:12,630
algorithm that will also work at scale

00:30:06,830 --> 00:30:16,620
did I answer your question so are you

00:30:12,630 --> 00:30:19,020
basically saying that if if in the three

00:30:16,620 --> 00:30:21,559
by four you had only two of those

00:30:19,020 --> 00:30:25,380
squares populated ALS will be a better

00:30:21,559 --> 00:30:28,080
algorithm than that's PD oh no so I just

00:30:25,380 --> 00:30:29,880
said that as your data as your so sorry

00:30:28,080 --> 00:30:31,169
is there a question like the more latent

00:30:29,880 --> 00:30:33,120
features the more latent features you

00:30:31,169 --> 00:30:35,010
generate the more competition I'm saying

00:30:33,120 --> 00:30:36,570
if like what happens if you want to

00:30:35,010 --> 00:30:38,940
train your SVD in

00:30:36,570 --> 00:30:41,730
3x4 and you only have two things that

00:30:38,940 --> 00:30:43,470
are popular oh yeah of course good

00:30:41,730 --> 00:30:44,789
question so it's still gonna work but

00:30:43,470 --> 00:30:45,899
the results are gonna be really bad so

00:30:44,789 --> 00:30:47,279
what you want to do is you want to

00:30:45,899 --> 00:30:49,590
measure the reconstruction of that

00:30:47,279 --> 00:30:51,210
matrix as your error right so you can

00:30:49,590 --> 00:30:54,630
use like mean squared error or some

00:30:51,210 --> 00:30:56,130
other metric to actually find the error

00:30:54,630 --> 00:30:58,320
between the reconstruction of the

00:30:56,130 --> 00:31:00,059
original matrix with your so the more

00:30:58,320 --> 00:31:03,210
ratings you have the more accurate it's

00:31:00,059 --> 00:31:06,210
going to be actually we are running out

00:31:03,210 --> 00:31:08,929
of time now let's just take the

00:31:06,210 --> 00:31:08,929
quotients outside

00:31:10,100 --> 00:31:15,320

YouTube URL: https://www.youtube.com/watch?v=d7iIb_XVkZs


