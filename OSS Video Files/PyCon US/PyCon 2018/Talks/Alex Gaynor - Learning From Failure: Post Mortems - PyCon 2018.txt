Title: Alex Gaynor - Learning From Failure: Post Mortems - PyCon 2018
Publication date: 2018-08-06
Playlist: Talks
Description: 
	Speaker: Alex Gaynor

Unless you work on pacemakers or at NASA, you've probably accepted the fact that you will make mistakes in your code, and those mistakes will creep into production. This talk will introduce you to post-mortems, and how to use them as a vehicle for improving your code and your process.

Slides can be found at: https://speakerdeck.com/pycon2018 and https://github.com/PyCon/2018-slides
Captions: 
	00:00:02,179 --> 00:00:10,170
good afternoon everyone welcome back to

00:00:04,470 --> 00:00:12,090
our second to last session here our next

00:00:10,170 --> 00:00:14,790
speaker does not need any introduction

00:00:12,090 --> 00:00:17,550
so I won't spend any time on introducing

00:00:14,790 --> 00:00:19,080
Alex here I think most of you know him

00:00:17,550 --> 00:00:21,510
and he's going to be speaking to us

00:00:19,080 --> 00:00:30,210
about learning from failure post-mortems

00:00:21,510 --> 00:00:31,710
Alex gainer Thank You Ned and thank you

00:00:30,210 --> 00:00:34,140
to all of you for coming act listen this

00:00:31,710 --> 00:00:35,700
talk like that said I'm Alex and this

00:00:34,140 --> 00:00:37,829
talk is gonna be about learning from

00:00:35,700 --> 00:00:38,760
failure with post-mortems we're gonna be

00:00:37,829 --> 00:00:40,370
talking about how to introduce

00:00:38,760 --> 00:00:42,510
post-mortems to your development process

00:00:40,370 --> 00:00:45,300
in order to get the most from the

00:00:42,510 --> 00:00:46,800
failures that inevitably happen bugs are

00:00:45,300 --> 00:00:49,079
a part of life for software developers

00:00:46,800 --> 00:00:51,149
and unless you make space shuttles or

00:00:49,079 --> 00:00:53,370
pacemakers you're probably ok with that

00:00:51,149 --> 00:00:54,750
you've accepted into your heart that

00:00:53,370 --> 00:00:57,600
there's something that will happen from

00:00:54,750 --> 00:00:59,160
time to time in my experience customers

00:00:57,600 --> 00:01:00,539
are never super happy about bugs

00:00:59,160 --> 00:01:03,949
but today we're gonna focus on a

00:01:00,539 --> 00:01:07,170
specific subset of them operational bugs

00:01:03,949 --> 00:01:10,260
what's an operational failure downtime

00:01:07,170 --> 00:01:11,400
for your website a data breach something

00:01:10,260 --> 00:01:14,850
went wrong in the production environment

00:01:11,400 --> 00:01:16,619
a quick test is that if it's something

00:01:14,850 --> 00:01:19,140
you'd measure in an SLA it's an

00:01:16,619 --> 00:01:20,729
operational failure we're gonna focus on

00:01:19,140 --> 00:01:23,040
how you can learn from your operational

00:01:20,729 --> 00:01:24,720
failures some of these practices are

00:01:23,040 --> 00:01:26,310
probably applicable to other situations

00:01:24,720 --> 00:01:28,770
you want to learn from like an agile

00:01:26,310 --> 00:01:30,600
post-mortem or agile sprint

00:01:28,770 --> 00:01:33,299
retrospective but those won't be my

00:01:30,600 --> 00:01:35,100
focus I'll be using the website went

00:01:33,299 --> 00:01:36,780
down for most of the examples here but

00:01:35,100 --> 00:01:37,890
it's a Scituate because that's a

00:01:36,780 --> 00:01:40,049
situation that all I think will be

00:01:37,890 --> 00:01:41,460
accessible to many folks but don't take

00:01:40,049 --> 00:01:44,820
that to mean this is the only type of

00:01:41,460 --> 00:01:46,290
operational failure before we dive too

00:01:44,820 --> 00:01:47,759
far in you should probably know the

00:01:46,290 --> 00:01:50,130
perspective I'm bringing to that I'm

00:01:47,759 --> 00:01:52,950
currently a security engineer protecting

00:01:50,130 --> 00:01:54,509
Firefox at Mozilla in previous lifetimes

00:01:52,950 --> 00:01:56,250
I've worked for startups and for the US

00:01:54,509 --> 00:01:58,290
government I've developed web

00:01:56,250 --> 00:02:00,990
applications and compilers and nowadays

00:01:58,290 --> 00:02:03,119
a web browser I've spent a lot of time

00:02:00,990 --> 00:02:05,189
in the Python ecosystem from developing

00:02:03,119 --> 00:02:06,270
popular open-source projects to serving

00:02:05,189 --> 00:02:10,470
on the board of the Python Software

00:02:06,270 --> 00:02:11,780
Foundation so something is broken what

00:02:10,470 --> 00:02:15,020
do you do next

00:02:11,780 --> 00:02:16,760
first stop the bleeding resolving the

00:02:15,020 --> 00:02:18,200
immediate presentation of the breakage

00:02:16,760 --> 00:02:20,569
is beyond the scope of this talk

00:02:18,200 --> 00:02:22,760
hopefully somebody else at PyCon gave a

00:02:20,569 --> 00:02:25,790
talk on fixing bugs if not you're on

00:02:22,760 --> 00:02:27,530
your own once the situation is

00:02:25,790 --> 00:02:30,500
stabilized what do you do next

00:02:27,530 --> 00:02:33,380
in my experience there are two possible

00:02:30,500 --> 00:02:35,410
answers to this question one is how do

00:02:33,380 --> 00:02:38,810
we make sure this never happens again

00:02:35,410 --> 00:02:42,200
two is how do we figure out who needs to

00:02:38,810 --> 00:02:44,239
be fired the team in the meeting are for

00:02:42,200 --> 00:02:45,590
solving problems there are plenty of

00:02:44,239 --> 00:02:48,080
other venues where people devote their

00:02:45,590 --> 00:02:50,150
creative energies to shifting blame we

00:02:48,080 --> 00:02:50,870
do not need another one we don't shoot

00:02:50,150 --> 00:02:53,300
the messenger

00:02:50,870 --> 00:02:56,030
especially when the message is I screwed

00:02:53,300 --> 00:02:58,459
something up I think this quote from my

00:02:56,030 --> 00:02:59,420
former boss Mikey Dickerson captures the

00:02:58,459 --> 00:03:01,850
difference between these two

00:02:59,420 --> 00:03:03,170
perspectives mikey was one of the first

00:03:01,850 --> 00:03:06,260
engineers brought in to rescue

00:03:03,170 --> 00:03:07,850
healthcare.gov when it failed in 2013 if

00:03:06,260 --> 00:03:09,560
you haven't heard that story you should

00:03:07,850 --> 00:03:12,319
find someone to tell it to you it's a

00:03:09,560 --> 00:03:14,690
great one see they had a problem the

00:03:12,319 --> 00:03:16,070
website was down all the time and while

00:03:14,690 --> 00:03:18,049
some folks were trying to make it work

00:03:16,070 --> 00:03:19,640
Congress was holding hearings and

00:03:18,049 --> 00:03:21,260
whether or not the site was up or down

00:03:19,640 --> 00:03:24,290
at any given moment was the lead story

00:03:21,260 --> 00:03:25,880
on CNN no one wanted to share anything

00:03:24,290 --> 00:03:27,590
that would point the blame at themselves

00:03:25,880 --> 00:03:30,320
or their company even if it would be

00:03:27,590 --> 00:03:31,459
helpful in fixing the website if you

00:03:30,320 --> 00:03:34,010
leave here having learned absolutely

00:03:31,459 --> 00:03:35,660
nothing else I want you to walk away

00:03:34,010 --> 00:03:36,230
believing that holding people

00:03:35,660 --> 00:03:39,470
accountable

00:03:36,230 --> 00:03:41,870
aka firing them and making systemic

00:03:39,470 --> 00:03:44,660
improvements are unrelated problems and

00:03:41,870 --> 00:03:46,880
maybe even mutually exclusive if you

00:03:44,660 --> 00:03:48,950
want to learn how to choose who to fire

00:03:46,880 --> 00:03:50,900
when something goes wrong you'll need to

00:03:48,950 --> 00:03:54,320
find a different talk this one is about

00:03:50,900 --> 00:03:55,970
how we learn from our mistakes you may

00:03:54,320 --> 00:03:58,130
have heard the phrase blameless post

00:03:55,970 --> 00:03:59,590
mortem before this phrase refers to

00:03:58,130 --> 00:04:02,030
exactly what I was just talking about

00:03:59,590 --> 00:04:04,160
post mortems for finding systemic

00:04:02,030 --> 00:04:06,859
improvements not for finding out who to

00:04:04,160 --> 00:04:08,930
blame it's very frequent that in the

00:04:06,859 --> 00:04:10,489
event something goes wrong we can look

00:04:08,930 --> 00:04:12,739
for somebody who we could say is

00:04:10,489 --> 00:04:14,870
responsible and to blame the person who

00:04:12,739 --> 00:04:16,370
pushed the code to production the person

00:04:14,870 --> 00:04:18,229
who ran the administrative script that

00:04:16,370 --> 00:04:20,120
went wrong the person who developed the

00:04:18,229 --> 00:04:23,000
pull request that introduced the code

00:04:20,120 --> 00:04:24,320
that went wrong I want to take just a

00:04:23,000 --> 00:04:25,700
minute and give an example of what

00:04:24,320 --> 00:04:28,910
blamelessness looks like be

00:04:25,700 --> 00:04:31,820
is so important to this process root

00:04:28,910 --> 00:04:34,400
cause a script was run which deleted

00:04:31,820 --> 00:04:36,710
half the production cluster at first

00:04:34,400 --> 00:04:39,080
glance this sounds ok we're not blaming

00:04:36,710 --> 00:04:41,480
anybody the script was run it's not

00:04:39,080 --> 00:04:43,670
important by who if you've ever used

00:04:41,480 --> 00:04:45,830
Microsoft Word and had it complain to

00:04:43,670 --> 00:04:48,500
you about the passive voice this is it

00:04:45,830 --> 00:04:50,390
this is a blameless sentence it doesn't

00:04:48,500 --> 00:04:52,130
blame anyone but it doesn't produce a

00:04:50,390 --> 00:04:57,290
blameless culture which is what we're

00:04:52,130 --> 00:04:59,480
really after root cause I Alex ran a

00:04:57,290 --> 00:05:02,630
script which deleted half the production

00:04:59,480 --> 00:05:04,010
cluster this seems worse we're

00:05:02,630 --> 00:05:06,140
attributing deleting the production

00:05:04,010 --> 00:05:09,110
cluster to one person surely that's

00:05:06,140 --> 00:05:11,630
blaming Lana a culture of blamelessness

00:05:09,110 --> 00:05:13,880
is not one where we deny that specific

00:05:11,630 --> 00:05:16,340
humans did things but rather one where I

00:05:13,880 --> 00:05:18,380
can say I did this and the whole team

00:05:16,340 --> 00:05:20,290
pushes right past that to look for

00:05:18,380 --> 00:05:22,910
systemic opportunities for improvement

00:05:20,290 --> 00:05:25,100
it's critical that your team see and

00:05:22,910 --> 00:05:27,140
understand the blamelessness comes not

00:05:25,100 --> 00:05:28,880
from omitting someone's name but from a

00:05:27,140 --> 00:05:30,860
deeply held belief that almost all

00:05:28,880 --> 00:05:33,160
problems have systemic solutions and

00:05:30,860 --> 00:05:35,420
pursuing those is far more important

00:05:33,160 --> 00:05:37,550
using the passive voice subtly

00:05:35,420 --> 00:05:38,090
communicates we blame someone if we knew

00:05:37,550 --> 00:05:40,040
who it was

00:05:38,090 --> 00:05:42,080
we're keeping this information secret

00:05:40,040 --> 00:05:44,420
because deep down we really want someone

00:05:42,080 --> 00:05:46,730
to blame having a culture where you can

00:05:44,420 --> 00:05:48,230
say I did this gives much more

00:05:46,730 --> 00:05:50,450
opportunity for people to speak freely

00:05:48,230 --> 00:05:53,000
to communicate fully about what happened

00:05:50,450 --> 00:05:54,590
and to understand root causes rather

00:05:53,000 --> 00:05:57,350
than being distracted by the nagging

00:05:54,590 --> 00:05:59,240
question Oh was it was it mark again did

00:05:57,350 --> 00:06:02,630
he run the script wrong because that's

00:05:59,240 --> 00:06:04,940
not what we're after I've used the

00:06:02,630 --> 00:06:07,370
phrase systemic problem or systemic

00:06:04,940 --> 00:06:09,080
solution several times now if you all

00:06:07,370 --> 00:06:11,560
permeate a minute or two of philosophy I

00:06:09,080 --> 00:06:14,240
want to dive into what those mean an

00:06:11,560 --> 00:06:16,630
operational failure happened the website

00:06:14,240 --> 00:06:19,570
went down and then someone fixed the bug

00:06:16,630 --> 00:06:21,860
redeployed and now the website is up

00:06:19,570 --> 00:06:24,860
there was a failure and now it's

00:06:21,860 --> 00:06:26,840
resolved what more needs doing the

00:06:24,860 --> 00:06:28,880
ironclad belief I bring to this process

00:06:26,840 --> 00:06:30,770
is that op is that the operational

00:06:28,880 --> 00:06:33,260
failure that happened gave us one

00:06:30,770 --> 00:06:35,300
example presentation of an issue but

00:06:33,260 --> 00:06:38,150
it's one of a dozen ways some underlying

00:06:35,300 --> 00:06:39,289
failure could have presented the job of

00:06:38,150 --> 00:06:41,749
the post-mortem is defy

00:06:39,289 --> 00:06:43,909
the other eleven see how we can fix them

00:06:41,749 --> 00:06:45,919
all at once and to discover what other

00:06:43,909 --> 00:06:49,129
challenges exacerbated the severity of

00:06:45,919 --> 00:06:51,319
these bugs you've probably heard the

00:06:49,129 --> 00:06:53,689
phrase root cause analysis before

00:06:51,319 --> 00:06:55,009
in general it's philosophically well

00:06:53,689 --> 00:06:56,960
aligned with what I'm talking about

00:06:55,009 --> 00:07:00,949
there's one important correction though

00:06:56,960 --> 00:07:02,930
root causes plural I wish I had the time

00:07:00,949 --> 00:07:05,270
to fully explore this material but I

00:07:02,930 --> 00:07:08,029
want everyone to jot down the URL how

00:07:05,270 --> 00:07:09,589
dot complex systems not fail or you can

00:07:08,029 --> 00:07:12,349
find it in the slides online after the

00:07:09,589 --> 00:07:15,020
talk this page summarizes some research

00:07:12,349 --> 00:07:16,939
done by dr. Richard cook he's a medical

00:07:15,020 --> 00:07:18,680
doctor who researches failure modes of

00:07:16,939 --> 00:07:21,199
complex systems whether they're the

00:07:18,680 --> 00:07:23,449
electrical grid hospitals or production

00:07:21,199 --> 00:07:25,939
distributed system if you've got a

00:07:23,449 --> 00:07:28,490
handful of web servers a load balancer a

00:07:25,939 --> 00:07:30,379
network file system a database and a

00:07:28,490 --> 00:07:33,709
monitoring tool you've got a complex

00:07:30,379 --> 00:07:35,930
system one of the major observations of

00:07:33,709 --> 00:07:38,300
dr. cooks research is that for complex

00:07:35,930 --> 00:07:41,689
systems they already do a lot of work to

00:07:38,300 --> 00:07:43,610
handle failures every except try or

00:07:41,689 --> 00:07:45,589
finally or with block you put in your

00:07:43,610 --> 00:07:48,009
Python code is trying to handle some

00:07:45,589 --> 00:07:50,149
sort of failure as a result any

00:07:48,009 --> 00:07:53,089
operational failure will have multiple

00:07:50,149 --> 00:07:55,370
contributing factors because of this

00:07:53,089 --> 00:07:57,649
it's likely that whatever changed to

00:07:55,370 --> 00:07:59,839
initiate the operational failure merely

00:07:57,649 --> 00:08:02,319
triggered a set of pre-existing latent

00:07:59,839 --> 00:08:04,999
bugs that just weren't apparent before

00:08:02,319 --> 00:08:07,339
it's critical that we regard the latent

00:08:04,999 --> 00:08:09,379
bugs as being as much a part of the

00:08:07,339 --> 00:08:11,959
cause of the incident as the proximate

00:08:09,379 --> 00:08:13,729
changes which made them active it's also

00:08:11,959 --> 00:08:15,680
we can important that we consider things

00:08:13,729 --> 00:08:17,719
that may not be bugs but which

00:08:15,680 --> 00:08:19,969
nevertheless contribute are contributing

00:08:17,719 --> 00:08:22,580
factors to our incident or its severity

00:08:19,969 --> 00:08:23,029
now go forth and read the rest of this

00:08:22,580 --> 00:08:25,279
website

00:08:23,029 --> 00:08:27,879
ideally after I'm done talking but it's

00:08:25,279 --> 00:08:29,749
pretty good stuff I'll be understanding

00:08:27,879 --> 00:08:31,909
so I've given an underlying

00:08:29,749 --> 00:08:34,130
philosophical rationale for post-mortems

00:08:31,909 --> 00:08:37,639
I told you I have to have you have to

00:08:34,130 --> 00:08:39,860
have one thack is a post mortem a post

00:08:37,639 --> 00:08:40,939
mortem is a process usually in the form

00:08:39,860 --> 00:08:43,069
of a meeting and a written document

00:08:40,939 --> 00:08:45,439
whose goal is to take our single

00:08:43,069 --> 00:08:47,449
specific operational failure and turn it

00:08:45,439 --> 00:08:49,610
into learning learning in the form of

00:08:47,449 --> 00:08:51,560
improvements to our code improvements to

00:08:49,610 --> 00:08:52,649
our documentation improvements to our

00:08:51,560 --> 00:08:54,180
processes

00:08:52,649 --> 00:08:55,410
improvements to the weird script on

00:08:54,180 --> 00:08:56,939
David and Sarah's laptops that

00:08:55,410 --> 00:08:59,579
accidentally became critical production

00:08:56,939 --> 00:09:00,839
infrastructure learning in the form of a

00:08:59,579 --> 00:09:02,220
document the people who joined the

00:09:00,839 --> 00:09:04,199
company in six months can read to

00:09:02,220 --> 00:09:05,970
understand what happened and most

00:09:04,199 --> 00:09:08,430
importantly learning in the form of

00:09:05,970 --> 00:09:09,869
turning this specific incident into

00:09:08,430 --> 00:09:12,869
observations about a more general

00:09:09,869 --> 00:09:14,819
pattern post-mortems are put together by

00:09:12,869 --> 00:09:16,740
the team responsible for operations and

00:09:14,819 --> 00:09:18,959
for implementing the lessons learned if

00:09:16,740 --> 00:09:21,119
your SSH into servers to fix things

00:09:18,959 --> 00:09:22,860
you're in the postmortem process if

00:09:21,119 --> 00:09:24,449
you're gonna be responsible for redoing

00:09:22,860 --> 00:09:26,639
the chef cookbooks you're in the

00:09:24,449 --> 00:09:28,410
post-mortem process if you write code

00:09:26,639 --> 00:09:29,759
for the application that was part of

00:09:28,410 --> 00:09:32,040
what was affected you're in the

00:09:29,759 --> 00:09:33,899
post-mortem process post-mortems aren't

00:09:32,040 --> 00:09:36,389
something somebody puts together about

00:09:33,899 --> 00:09:39,059
someone else they're done by and for the

00:09:36,389 --> 00:09:41,369
people doing the work the format i've

00:09:39,059 --> 00:09:43,110
seen work well is a few days after the

00:09:41,369 --> 00:09:45,179
incident everyone involved meets the

00:09:43,110 --> 00:09:48,300
other shares their notes and produces

00:09:45,179 --> 00:09:50,629
the document together we've got our team

00:09:48,300 --> 00:09:53,040
in a room we're sitting down to write

00:09:50,629 --> 00:09:55,110
the post-mortem for an instant that just

00:09:53,040 --> 00:09:57,269
happened what do we need to make sure is

00:09:55,110 --> 00:09:58,559
in this document I'm gonna run through

00:09:57,269 --> 00:10:01,110
the elements of a post-mortem with

00:09:58,559 --> 00:10:02,879
examples this isn't a hard-and-fast set

00:10:01,110 --> 00:10:04,290
of rules but these the elements I've

00:10:02,879 --> 00:10:06,029
seen as being necessary to get the

00:10:04,290 --> 00:10:07,290
important lessons and which seemed to

00:10:06,029 --> 00:10:09,929
match up with what friends at other

00:10:07,290 --> 00:10:11,309
companies use I personally find it

00:10:09,929 --> 00:10:13,259
helpful to create a template with each

00:10:11,309 --> 00:10:15,029
one of these fields and when I say a

00:10:13,259 --> 00:10:17,699
template I mean a markdown file with a

00:10:15,029 --> 00:10:19,740
few prefilled and headers nothing fancy

00:10:17,699 --> 00:10:21,540
it's useful for your post-mortems to all

00:10:19,740 --> 00:10:23,189
have the same format as each other this

00:10:21,540 --> 00:10:26,160
makes it easy to notice trends between

00:10:23,189 --> 00:10:27,449
them oh I've seen that root cause three

00:10:26,160 --> 00:10:30,959
or four times across multiple

00:10:27,449 --> 00:10:33,179
post-mortems that said practicality

00:10:30,959 --> 00:10:34,439
beats purity if one of your incidents is

00:10:33,179 --> 00:10:36,720
significantly different from another

00:10:34,439 --> 00:10:39,990
don't shoehorn it into a template that

00:10:36,720 --> 00:10:42,660
doesn't make sense a summary of what

00:10:39,990 --> 00:10:44,549
happened and user visible impact the

00:10:42,660 --> 00:10:46,230
website was down for 17 minutes and

00:10:44,549 --> 00:10:48,869
there were elevated exception rates for

00:10:46,230 --> 00:10:50,309
another 24 minutes right from the top

00:10:48,869 --> 00:10:52,230
you want to have a crisp description of

00:10:50,309 --> 00:10:53,670
what the failure was what was the

00:10:52,230 --> 00:10:56,639
visible impact of everything that was

00:10:53,670 --> 00:10:58,889
going on what user visible impact mean

00:10:56,639 --> 00:11:00,899
varies by what your system is if you

00:10:58,889 --> 00:11:02,910
have a fancy micro-services architecture

00:11:00,899 --> 00:11:05,670
and your services consumers or people's

00:11:02,910 --> 00:11:06,450
ETL jobs they are your users and you

00:11:05,670 --> 00:11:08,220
care about

00:11:06,450 --> 00:11:10,350
time or other impact as they perceive it

00:11:08,220 --> 00:11:13,890
not necessarily how your end customers

00:11:10,350 --> 00:11:17,280
perceive it how was the incident

00:11:13,890 --> 00:11:19,560
resolved I added a timeout to the HTTP

00:11:17,280 --> 00:11:21,960
requests our app makes to Facebook and

00:11:19,560 --> 00:11:23,520
redeployed the application you did

00:11:21,960 --> 00:11:27,360
something in the moment to make the

00:11:23,520 --> 00:11:31,590
bleeding stop what was it a complete

00:11:27,360 --> 00:11:34,970
timeline of what happened 909 monitoring

00:11:31,590 --> 00:11:40,380
indicates the website is 503 Alex paged

00:11:34,970 --> 00:11:44,850
912 Alex axe the page 914 Alex restarts

00:11:40,380 --> 00:11:46,470
nginx incident is not resolved 921 Julie

00:11:44,850 --> 00:11:48,720
comments on slack that before the

00:11:46,470 --> 00:11:50,250
incident started the API endpoints which

00:11:48,720 --> 00:11:54,150
communicate with Facebook were showing

00:11:50,250 --> 00:11:56,400
increased latency etc etc this is one of

00:11:54,150 --> 00:11:57,780
the most important pieces document

00:11:56,400 --> 00:12:00,300
everything that was going on and

00:11:57,780 --> 00:12:02,310
everything each person was doing this is

00:12:00,300 --> 00:12:04,440
essential for evaluating how well your

00:12:02,310 --> 00:12:06,270
instance response process worked how

00:12:04,440 --> 00:12:07,950
well your monitoring worked and for

00:12:06,270 --> 00:12:10,350
understanding deeply how the incident

00:12:07,950 --> 00:12:12,030
played out write as much as you can

00:12:10,350 --> 00:12:15,390
there's no such thing as too much

00:12:12,030 --> 00:12:18,180
information at this point we have the

00:12:15,390 --> 00:12:20,100
facts of the incident what happened what

00:12:18,180 --> 00:12:22,950
everyone was doing how it was ultimately

00:12:20,100 --> 00:12:24,660
resolved thus far everything we've

00:12:22,950 --> 00:12:26,850
written down is more or less objective

00:12:24,660 --> 00:12:29,550
information about what was going on in

00:12:26,850 --> 00:12:31,980
the world next we have to focus on the

00:12:29,550 --> 00:12:35,640
analysis which brings into subjective

00:12:31,980 --> 00:12:38,250
elements into this first what were the

00:12:35,640 --> 00:12:40,140
causes of the incident in this case

00:12:38,250 --> 00:12:42,930
Facebook were started responding to

00:12:40,140 --> 00:12:44,850
requests slowly we had no timeouts on

00:12:42,930 --> 00:12:47,130
requests to Facebook and we make

00:12:44,850 --> 00:12:50,310
blocking requests to Facebook during the

00:12:47,130 --> 00:12:52,440
HTTP requests to our API our process per

00:12:50,310 --> 00:12:54,120
request web server made it easy for a

00:12:52,440 --> 00:12:56,550
small number of slow request to

00:12:54,120 --> 00:12:58,530
denial-of-service us one thing you might

00:12:56,550 --> 00:13:00,570
notice is without any one of these

00:12:58,530 --> 00:13:02,790
components this incident wouldn't have

00:13:00,570 --> 00:13:04,980
happened so each one of these is a

00:13:02,790 --> 00:13:08,730
potential contributing cause to our

00:13:04,980 --> 00:13:11,100
incident this is the section where we

00:13:08,730 --> 00:13:12,570
really analyze the causes this is where

00:13:11,100 --> 00:13:15,090
all the stuff I talked about at the top

00:13:12,570 --> 00:13:17,790
about multiple causes comes into play in

00:13:15,090 --> 00:13:19,770
this particular incident the proximate

00:13:17,790 --> 00:13:20,340
cause Facebook started responding to

00:13:19,770 --> 00:13:22,080
requests low

00:13:20,340 --> 00:13:24,330
was completely outside of our control

00:13:22,080 --> 00:13:26,340
but there's still a lot of work we can

00:13:24,330 --> 00:13:28,200
do on our side to make ourselves more

00:13:26,340 --> 00:13:31,529
resilient to when Facebook gets slow

00:13:28,200 --> 00:13:33,930
next time dig deep here list every

00:13:31,529 --> 00:13:35,520
possible cause you can decide it's not

00:13:33,930 --> 00:13:37,650
something you're gonna pursue fixing

00:13:35,520 --> 00:13:39,630
later on take a lesson from my

00:13:37,650 --> 00:13:42,270
experience though if you dig all the way

00:13:39,630 --> 00:13:46,980
down to computers were a mistake you may

00:13:42,270 --> 00:13:48,930
have gone too far what went well and

00:13:46,980 --> 00:13:51,990
what went poorly during the handling of

00:13:48,930 --> 00:13:53,670
the incident itself unfortunately our

00:13:51,990 --> 00:13:55,860
monitoring and graphing didn't do a good

00:13:53,670 --> 00:13:58,529
job of highlighting which API endpoints

00:13:55,860 --> 00:14:00,450
on our systems were slow we didn't have

00:13:58,529 --> 00:14:02,460
any direct monitoring of the latency and

00:14:00,450 --> 00:14:04,529
error rates of our request to Facebook

00:14:02,460 --> 00:14:06,450
so we had to know that a particular URL

00:14:04,529 --> 00:14:09,510
on our site meetings being slow

00:14:06,450 --> 00:14:11,970
potentially implicated Facebook on the

00:14:09,510 --> 00:14:13,589
good side once we had a fix developed we

00:14:11,970 --> 00:14:15,770
were able to deploy to production very

00:14:13,589 --> 00:14:18,210
quickly and the instant was resolved

00:14:15,770 --> 00:14:20,250
being good at responding to instance

00:14:18,210 --> 00:14:22,529
when they do occur is a critical element

00:14:20,250 --> 00:14:24,029
of resiliency so we need to review how

00:14:22,529 --> 00:14:27,240
things went during the incident itself

00:14:24,029 --> 00:14:28,710
beyond just the causes that led to it in

00:14:27,240 --> 00:14:30,060
this case there are opportunities to

00:14:28,710 --> 00:14:32,400
make improvements to our monitoring

00:14:30,060 --> 00:14:33,900
tools on the plus side our deployment

00:14:32,400 --> 00:14:36,210
system worked flawlessly during the

00:14:33,900 --> 00:14:38,070
middle of an incident which is important

00:14:36,210 --> 00:14:40,920
if our mot if our deployment systems had

00:14:38,070 --> 00:14:42,710
gone sideways while we were attempting

00:14:40,920 --> 00:14:44,910
to resolve an incident that would

00:14:42,710 --> 00:14:46,160
exacerbate our problem far more

00:14:44,910 --> 00:14:49,770
seriously

00:14:46,160 --> 00:14:52,560
finally follow-up actions add metrics

00:14:49,770 --> 00:14:54,180
around all requests to Facebook make the

00:14:52,560 --> 00:14:57,450
dashboards do a better job highlighting

00:14:54,180 --> 00:14:59,610
what API endpoints are slow review other

00:14:57,450 --> 00:15:01,339
third party API usage for the same

00:14:59,610 --> 00:15:04,080
issues that affected us with Facebook

00:15:01,339 --> 00:15:05,640
how much third party API usage can we

00:15:04,080 --> 00:15:08,550
move out of request handling entirely

00:15:05,640 --> 00:15:10,260
and into Sellery tasks we should

00:15:08,550 --> 00:15:12,060
investigate if auto scaling on our web

00:15:10,260 --> 00:15:14,940
nodes would have given us enough spare

00:15:12,060 --> 00:15:16,470
capacity to weather the timeouts this is

00:15:14,940 --> 00:15:19,230
where you synthesize the cause of the

00:15:16,470 --> 00:15:20,910
incidents and the factors from handling

00:15:19,230 --> 00:15:23,670
that exacerbated the severity into

00:15:20,910 --> 00:15:26,490
follow-up items expect to file lots of

00:15:23,670 --> 00:15:27,779
tickets into your bug tracker then you

00:15:26,490 --> 00:15:30,030
can use your normal prioritization

00:15:27,779 --> 00:15:32,459
process to make sure the most important

00:15:30,030 --> 00:15:33,960
ones get done quickly you should expect

00:15:32,459 --> 00:15:35,550
to include both Shore

00:15:33,960 --> 00:15:37,890
term follow-up items things that can be

00:15:35,550 --> 00:15:39,600
done immediately as well as longer term

00:15:37,890 --> 00:15:42,690
follow-up items that might involve

00:15:39,600 --> 00:15:45,630
serious Corrections to your code or much

00:15:42,690 --> 00:15:47,910
longer timescales to implement in this

00:15:45,630 --> 00:15:50,130
case we generate action items to improve

00:15:47,910 --> 00:15:51,840
our metrics review our code to see which

00:15:50,130 --> 00:15:53,520
other third-party integrations have the

00:15:51,840 --> 00:15:56,100
possibility to denial-of-service us

00:15:53,520 --> 00:15:57,510
consider a longer-term improvement to

00:15:56,100 --> 00:16:00,360
move some of these integrations into

00:15:57,510 --> 00:16:02,580
Sellery tasks so that so that they can't

00:16:00,360 --> 00:16:04,020
affect web serving availability and to

00:16:02,580 --> 00:16:06,330
investigate what would have happened if

00:16:04,020 --> 00:16:10,050
we'd had auto scaling based on slow

00:16:06,330 --> 00:16:12,660
response time auto scaling based on slow

00:16:10,050 --> 00:16:14,610
request latency as you can see each of

00:16:12,660 --> 00:16:16,770
these is inspired by what we saw as a

00:16:14,610 --> 00:16:19,590
route call as one of the root causes of

00:16:16,770 --> 00:16:20,850
our incident now that we've walked

00:16:19,590 --> 00:16:22,500
through the elements of a post-mortem

00:16:20,850 --> 00:16:25,530
I'm gonna give all of you some homework

00:16:22,500 --> 00:16:27,120
I'm gonna describe an incident and seed

00:16:25,530 --> 00:16:29,790
your thoughts with a few follow-up items

00:16:27,120 --> 00:16:31,620
I see I want everyone here to take this

00:16:29,790 --> 00:16:33,660
home and think about what else you saw

00:16:31,620 --> 00:16:35,460
in the situation I described what other

00:16:33,660 --> 00:16:39,570
systemic opportunities are there for us

00:16:35,460 --> 00:16:40,920
to improve Alex ran a script intended to

00:16:39,570 --> 00:16:43,830
increase the size of the kubernetes

00:16:40,920 --> 00:16:46,920
cluster by 20% instead he accidentally

00:16:43,830 --> 00:16:48,840
shrunk it down to 20 nodes the cluster

00:16:46,920 --> 00:16:51,390
now had less capacity than was needed to

00:16:48,840 --> 00:16:52,860
serve all incoming requests the auto

00:16:51,390 --> 00:16:55,350
scaling group attempted to recover

00:16:52,860 --> 00:16:57,060
capacity but so many servers spawn up at

00:16:55,350 --> 00:17:00,150
the same time denial of service to

00:16:57,060 --> 00:17:01,980
docker registry ultimately auto scaling

00:17:00,150 --> 00:17:03,540
was disabled and new servers were

00:17:01,980 --> 00:17:06,080
manually brought off five at a time

00:17:03,540 --> 00:17:08,400
until we were back to full capacity in

00:17:06,080 --> 00:17:10,170
total things were almost totally

00:17:08,400 --> 00:17:12,240
unavailable for just over an hour and

00:17:10,170 --> 00:17:14,000
fully resolving the incident from start

00:17:12,240 --> 00:17:17,400
to failure took more than three hours

00:17:14,000 --> 00:17:19,350
it'd be very easy for us to say Alex not

00:17:17,400 --> 00:17:22,260
understanding how to use scale dot SH

00:17:19,350 --> 00:17:24,209
was the root cause hopefully by now I've

00:17:22,260 --> 00:17:25,860
convinced you that doing so would leave

00:17:24,209 --> 00:17:28,170
opportunities to make the whole system

00:17:25,860 --> 00:17:30,630
better on the table to say nothing of

00:17:28,170 --> 00:17:33,600
blaming one person for was ultimately a

00:17:30,630 --> 00:17:35,790
team effort some things that jump out to

00:17:33,600 --> 00:17:39,270
me there was clearly a user experience

00:17:35,790 --> 00:17:42,300
issue in scale dot sh both the fact that

00:17:39,270 --> 00:17:43,920
it's API that I used was confusing and

00:17:42,300 --> 00:17:46,110
led to misunderstanding what the

00:17:43,920 --> 00:17:47,820
arguments it took did and also the fact

00:17:46,110 --> 00:17:50,160
that it would happily delete almost all

00:17:47,820 --> 00:17:52,320
of the cluster what taking us well below

00:17:50,160 --> 00:17:55,380
the capacity we needed without any

00:17:52,320 --> 00:17:57,480
confirmation auto-scaling was supposed

00:17:55,380 --> 00:17:58,830
to make us more resilient by increasing

00:17:57,480 --> 00:18:01,140
capacity when we needed it

00:17:58,830 --> 00:18:03,480
instead it made this incident worse by

00:18:01,140 --> 00:18:05,940
putting us into a situation where we had

00:18:03,480 --> 00:18:09,000
nodes spinning but unusable that

00:18:05,940 --> 00:18:09,900
deserves serious review what do you see

00:18:09,000 --> 00:18:12,090
in this incident

00:18:09,900 --> 00:18:13,740
what systemic improvements could we make

00:18:12,090 --> 00:18:15,210
to handle these six other ways this

00:18:13,740 --> 00:18:18,240
situation could manifest

00:18:15,210 --> 00:18:20,010
besides Alex ran the script wrong maybe

00:18:18,240 --> 00:18:21,720
Amazon would just delete some of our

00:18:20,010 --> 00:18:23,580
servers from some of this from time to

00:18:21,720 --> 00:18:26,130
time producing the exact same symptoms

00:18:23,580 --> 00:18:28,140
to the conclusions we see about how to

00:18:26,130 --> 00:18:31,140
prevent this incident also work for that

00:18:28,140 --> 00:18:32,940
situation if you are gonna be handling

00:18:31,140 --> 00:18:35,010
this incident what tools would you want

00:18:32,940 --> 00:18:37,800
to have what metrics would you want to

00:18:35,010 --> 00:18:39,480
have it's not you want to add

00:18:37,800 --> 00:18:43,020
post-mortems to your team's process

00:18:39,480 --> 00:18:46,350
where do you start well you simply

00:18:43,020 --> 00:18:48,300
decide to however until the next time

00:18:46,350 --> 00:18:49,620
things break for real here are few

00:18:48,300 --> 00:18:53,370
approaches you can use to get some

00:18:49,620 --> 00:18:55,740
practice first is simulate an incident

00:18:53,370 --> 00:18:58,410
in security we often call this a

00:18:55,740 --> 00:19:01,020
tabletop exercise come up with an

00:18:58,410 --> 00:19:03,780
example scenario sit around a table and

00:19:01,020 --> 00:19:05,280
talk through how you'd handle it this is

00:19:03,780 --> 00:19:08,160
an excellent tool for improving your

00:19:05,280 --> 00:19:09,840
instance response capability but it's

00:19:08,160 --> 00:19:12,390
not quite as good as finding underlying

00:19:09,840 --> 00:19:14,880
bugs although sometimes just thinking

00:19:12,390 --> 00:19:17,160
how might this situation fail can

00:19:14,880 --> 00:19:19,530
trigger its own set of improvements the

00:19:17,160 --> 00:19:21,510
Twitter account bad things daily tweets

00:19:19,530 --> 00:19:22,620
practice examples of security instance

00:19:21,510 --> 00:19:25,920
for you to use for one of these

00:19:22,620 --> 00:19:27,540
exercises at first you might think some

00:19:25,920 --> 00:19:30,420
of these are awfully strange scenarios

00:19:27,540 --> 00:19:32,190
but pretty quickly you'll realize oh if

00:19:30,420 --> 00:19:33,090
we get good at handling something that

00:19:32,190 --> 00:19:35,070
looks like this

00:19:33,090 --> 00:19:36,390
that will also make it us good at

00:19:35,070 --> 00:19:40,890
handling a wide variety of other

00:19:36,390 --> 00:19:43,470
scenarios choice number two caused an

00:19:40,890 --> 00:19:45,090
incident Netflix runs something called

00:19:43,470 --> 00:19:47,580
chaos monkey in their production

00:19:45,090 --> 00:19:49,860
environment basically from time to time

00:19:47,580 --> 00:19:52,530
it randomly deletes an ec2 instance in

00:19:49,860 --> 00:19:54,240
production this ensures that their

00:19:52,530 --> 00:19:56,520
systems can handle a loss instance

00:19:54,240 --> 00:19:59,730
without problem and if they can't it

00:19:56,520 --> 00:20:00,810
gives them a practice opportunity to run

00:19:59,730 --> 00:20:01,680
through their instance response

00:20:00,810 --> 00:20:04,410
procedures

00:20:01,680 --> 00:20:05,690
live in production although perhaps you

00:20:04,410 --> 00:20:08,520
could start in a development environment

00:20:05,690 --> 00:20:10,050
if you killed one of your servers would

00:20:08,520 --> 00:20:14,430
everything respond flawlessly

00:20:10,050 --> 00:20:16,830
have you ever tested that theory finally

00:20:14,430 --> 00:20:19,680
you can redefine what an instant is if

00:20:16,830 --> 00:20:21,360
you're already hitting your SLA of 99

00:20:19,680 --> 00:20:25,290
percent of requests without an error

00:20:21,360 --> 00:20:27,090
raise your SLA to 99.5% with a 500

00:20:25,290 --> 00:20:29,610
millisecond 99th percentile response

00:20:27,090 --> 00:20:31,140
latency target we run a post-mortem

00:20:29,610 --> 00:20:33,270
about the slow request that made you

00:20:31,140 --> 00:20:36,870
miss the SLA even though the website was

00:20:33,270 --> 00:20:38,520
available the whole time I've talked

00:20:36,870 --> 00:20:40,740
about the application of post-mortems to

00:20:38,520 --> 00:20:43,290
computer problems but software engineers

00:20:40,740 --> 00:20:44,970
didn't invent this idea post-mortems

00:20:43,290 --> 00:20:47,220
have a long history in other industries

00:20:44,970 --> 00:20:50,000
in the military they call them hot

00:20:47,220 --> 00:20:52,020
washes or actor after-action reviews

00:20:50,000 --> 00:20:54,420
doctors call them morbidity and

00:20:52,020 --> 00:20:55,710
mortality conferences and they're a core

00:20:54,420 --> 00:20:59,310
part of what the National Transportation

00:20:55,710 --> 00:21:01,230
Safety Board does the NTSB is an

00:20:59,310 --> 00:21:03,540
independent federal agency with a few

00:21:01,230 --> 00:21:05,040
hundred employees they're responsible

00:21:03,540 --> 00:21:07,170
for investigating transportation

00:21:05,040 --> 00:21:10,200
accidents be they planes trains or

00:21:07,170 --> 00:21:12,720
automobiles the mission of the NTSB is

00:21:10,200 --> 00:21:14,580
quote to determine the probable cause of

00:21:12,720 --> 00:21:16,950
transportation accidents and incidents

00:21:14,580 --> 00:21:19,740
and to formulate safety recommendations

00:21:16,950 --> 00:21:23,010
to improve transportation safety tell me

00:21:19,740 --> 00:21:24,630
that doesn't sound familiar the NTSB

00:21:23,010 --> 00:21:27,420
performs investigations by what they

00:21:24,630 --> 00:21:28,950
call the party system basically in

00:21:27,420 --> 00:21:31,350
addition to their own team members who

00:21:28,950 --> 00:21:33,570
work for the NTSB the party will also

00:21:31,350 --> 00:21:35,340
include people from industry including

00:21:33,570 --> 00:21:37,950
people directly from the organizations

00:21:35,340 --> 00:21:39,930
involved in the accident if the accident

00:21:37,950 --> 00:21:42,210
was an engine malfunction as an airplane

00:21:39,930 --> 00:21:43,950
was on final descent they'll probably be

00:21:42,210 --> 00:21:47,460
somebody from the engine manufacturer

00:21:43,950 --> 00:21:49,680
air traffic control and the airline who

00:21:47,460 --> 00:21:51,330
are a part of their party you cannot

00:21:49,680 --> 00:21:52,860
investigate the causes of an accident

00:21:51,330 --> 00:21:55,530
without having experts from all the

00:21:52,860 --> 00:21:56,910
perspectives in the room however you are

00:21:55,530 --> 00:21:58,650
not allowed to be a member of an

00:21:56,910 --> 00:22:00,870
insurance company and be a member of the

00:21:58,650 --> 00:22:04,050
party insurance companies are all about

00:22:00,870 --> 00:22:05,910
allocating blame probably the most

00:22:04,050 --> 00:22:07,170
important thing to know about the NTSB

00:22:05,910 --> 00:22:09,630
is that they are not a law enforcement

00:22:07,170 --> 00:22:11,190
agency they conduct accident

00:22:09,630 --> 00:22:13,830
investigations not criminal

00:22:11,190 --> 00:22:15,330
investigations this extends beyond

00:22:13,830 --> 00:22:18,179
merely words in their mission

00:22:15,330 --> 00:22:20,850
the result of an NTSB investigation and

00:22:18,179 --> 00:22:23,220
testimony they receive cannot be used as

00:22:20,850 --> 00:22:26,039
evidence in a court of law their public

00:22:23,220 --> 00:22:28,019
safety mission requires this the NTSB

00:22:26,039 --> 00:22:29,850
will turn over the objective technical

00:22:28,019 --> 00:22:31,919
and scientific data they collect and

00:22:29,850 --> 00:22:34,379
support law enforcement with analysis of

00:22:31,919 --> 00:22:36,299
that data but the reports on underlying

00:22:34,379 --> 00:22:38,999
causes are all privileged from use in

00:22:36,299 --> 00:22:40,919
court this is an extraordinary status

00:22:38,999 --> 00:22:42,749
sort of like attorney-client privilege

00:22:40,919 --> 00:22:44,279
or spousal privilege and I don't think

00:22:42,749 --> 00:22:46,649
there can be any more clear evidence

00:22:44,279 --> 00:22:48,600
that making safety recommendations is a

00:22:46,649 --> 00:22:52,590
different line of work from finding out

00:22:48,600 --> 00:22:53,879
who or what is to blame before we wrap

00:22:52,590 --> 00:22:55,799
up there are a few more pieces of

00:22:53,879 --> 00:22:57,359
information I want everyone to have that

00:22:55,799 --> 00:23:01,169
didn't fit cleanly anywhere else in the

00:22:57,359 --> 00:23:03,480
slides first sometimes things break and

00:23:01,169 --> 00:23:05,909
we don't know why that does not mean we

00:23:03,480 --> 00:23:07,350
can't conduct a post-mortem in fact it

00:23:05,909 --> 00:23:10,379
makes the post-mortem all the more

00:23:07,350 --> 00:23:12,029
important at a previous job we once said

00:23:10,379 --> 00:23:13,950
an incident where the website was

00:23:12,029 --> 00:23:15,929
brought down because every single one of

00:23:13,950 --> 00:23:18,840
our mobile apps started phoning home at

00:23:15,929 --> 00:23:20,580
the exact same time we fix bugs in the

00:23:18,840 --> 00:23:22,830
mobile apps exponential back-off

00:23:20,580 --> 00:23:24,840
we made the API endpoints the mobile

00:23:22,830 --> 00:23:27,720
phones used more efficient we made the

00:23:24,840 --> 00:23:29,580
API handle too much load better we never

00:23:27,720 --> 00:23:31,379
once found out why all of the phones

00:23:29,580 --> 00:23:33,090
started making requests at the same time

00:23:31,379 --> 00:23:34,470
but that didn't stop us from making

00:23:33,090 --> 00:23:36,119
improvements that would allow us to

00:23:34,470 --> 00:23:39,809
handle the situation better if it ever

00:23:36,119 --> 00:23:42,299
occurred if you ever hear someone say

00:23:39,809 --> 00:23:45,330
the root cause was human error that

00:23:42,299 --> 00:23:47,489
should be a giant red flag I once saw an

00:23:45,330 --> 00:23:49,529
incident where a human copied a value

00:23:47,489 --> 00:23:51,840
incorrectly from a field in one system

00:23:49,529 --> 00:23:55,289
to another and it was described as root

00:23:51,840 --> 00:23:57,269
cause human error no discussion of why

00:23:55,289 --> 00:23:59,909
the system didn't do any data validation

00:23:57,269 --> 00:24:01,499
no discussion of why humans were

00:23:59,909 --> 00:24:04,619
involved in manually copying things

00:24:01,499 --> 00:24:05,879
between two software systems human error

00:24:04,619 --> 00:24:08,489
means you could not come up with a

00:24:05,879 --> 00:24:10,739
single underlying opportunity for a

00:24:08,489 --> 00:24:11,309
problem to fix and if you're making that

00:24:10,739 --> 00:24:13,830
claim

00:24:11,309 --> 00:24:15,779
you better back it up I've never once

00:24:13,830 --> 00:24:18,989
seen a post mortem where that was true I

00:24:15,779 --> 00:24:20,909
once saw a situation where fixing the

00:24:18,989 --> 00:24:22,649
underlying problems were believed to be

00:24:20,909 --> 00:24:24,749
too rare and too expensive to be

00:24:22,649 --> 00:24:26,940
cost-effective but that is a different

00:24:24,749 --> 00:24:29,420
claim and we damn sure knew what the

00:24:26,940 --> 00:24:32,040
causes were

00:24:29,420 --> 00:24:33,720
I've spent the last 20 minutes making a

00:24:32,040 --> 00:24:34,710
fairly intensive addition here

00:24:33,720 --> 00:24:37,560
development process

00:24:34,710 --> 00:24:39,060
why bother this is a lot of effort it's

00:24:37,560 --> 00:24:40,620
a lot of time to hold these meetings

00:24:39,060 --> 00:24:42,300
every time something goes wrong it's

00:24:40,620 --> 00:24:43,920
another set of skills to develop and

00:24:42,300 --> 00:24:45,720
software engineers already have many

00:24:43,920 --> 00:24:49,770
skills they need why bother

00:24:45,720 --> 00:24:51,330
is it worth it if you don't fix classes

00:24:49,770 --> 00:24:53,520
of bugs at the root you'll end up

00:24:51,330 --> 00:24:55,590
generating more classes of bugs faster

00:24:53,520 --> 00:24:58,470
than you can fix individual instances of

00:24:55,590 --> 00:25:00,390
them as time advances your product will

00:24:58,470 --> 00:25:03,990
get less and less reliable as more

00:25:00,390 --> 00:25:05,670
systemic errors are added software

00:25:03,990 --> 00:25:07,530
engineering is a discipline that

00:25:05,670 --> 00:25:10,020
requires practice to improve at like any

00:25:07,530 --> 00:25:11,610
other if you don't take the opportunity

00:25:10,020 --> 00:25:13,200
to learn from your mistakes you are

00:25:11,610 --> 00:25:16,140
missing out on an awful lot of learning

00:25:13,200 --> 00:25:17,880
opportunities particularly once you

00:25:16,140 --> 00:25:19,740
learn to recognize certain classes of

00:25:17,880 --> 00:25:21,330
underlying errors you can avoid making

00:25:19,740 --> 00:25:23,730
them right from the start on your next

00:25:21,330 --> 00:25:25,650
project there are an awful lot of types

00:25:23,730 --> 00:25:27,090
of bugs that are very cheap to avoid if

00:25:25,650 --> 00:25:29,310
you know about them at the start and

00:25:27,090 --> 00:25:31,800
very expensive to fix after the fact

00:25:29,310 --> 00:25:34,910
once your project is large an ounce of

00:25:31,800 --> 00:25:37,680
prevention is worth a pound of cure

00:25:34,910 --> 00:25:39,060
finally fixing bugs systemically is

00:25:37,680 --> 00:25:44,820
cheaper than fixing them one at a time

00:25:39,060 --> 00:25:47,040
in the long run key takeaways I'd like

00:25:44,820 --> 00:25:48,510
folks to walk out of here with if even

00:25:47,040 --> 00:25:50,040
if you ignore all of the advice about

00:25:48,510 --> 00:25:51,360
how to run the meeting what elements

00:25:50,040 --> 00:25:53,010
need to be in your post mortems

00:25:51,360 --> 00:25:55,680
I want you to walk away having learned

00:25:53,010 --> 00:25:58,340
these things post mortems are for

00:25:55,680 --> 00:26:00,660
learning things not blaming people

00:25:58,340 --> 00:26:02,850
blamelessness is about psychological

00:26:00,660 --> 00:26:05,010
safety not about using the passive voice

00:26:02,850 --> 00:26:07,860
when people see you using the passive

00:26:05,010 --> 00:26:10,820
voice they assume it means I'd like to

00:26:07,860 --> 00:26:12,900
blame them if I just knew who they were

00:26:10,820 --> 00:26:15,180
analyzing specific failures in your

00:26:12,900 --> 00:26:16,860
system is an opportunity to extract more

00:26:15,180 --> 00:26:20,550
general observations about how it

00:26:16,860 --> 00:26:22,500
responds to failure the trend over the

00:26:20,550 --> 00:26:24,960
course of many post-mortems should be

00:26:22,500 --> 00:26:26,520
increasingly that your system was able

00:26:24,960 --> 00:26:30,390
to handle more situations without

00:26:26,520 --> 00:26:32,640
catastrophic failures both your system

00:26:30,390 --> 00:26:36,600
and the post-mortem process will improve

00:26:32,640 --> 00:26:39,000
as you practice them the operational the

00:26:36,600 --> 00:26:41,220
operations of complex systems has many

00:26:39,000 --> 00:26:43,140
characteristics that defy expectations

00:26:41,220 --> 00:26:45,390
and the how comply

00:26:43,140 --> 00:26:47,340
systems that fail website details many

00:26:45,390 --> 00:26:48,570
of them in a way that will hopefully be

00:26:47,340 --> 00:26:52,140
helpful to you and operating your

00:26:48,570 --> 00:26:54,240
systems the incident already happened

00:26:52,140 --> 00:26:56,910
don't waste the opportunity to learn

00:26:54,240 --> 00:26:59,130
something from it specific failures are

00:26:56,910 --> 00:27:00,780
almost always more educational than

00:26:59,130 --> 00:27:02,030
trying to think hard about what could

00:27:00,780 --> 00:27:06,620
have happened

00:27:02,030 --> 00:27:08,670
finally human error is not a root cause

00:27:06,620 --> 00:27:10,590
thank you very much for spending your

00:27:08,670 --> 00:27:12,060
Sunday afternoon with me that's my

00:27:10,590 --> 00:27:13,920
website and the URL the slides will be

00:27:12,060 --> 00:27:16,320
up on I believe we have about three

00:27:13,920 --> 00:27:18,240
minutes for questions now if anyone has

00:27:16,320 --> 00:27:19,440
anything they'd like to ask hopefully

00:27:18,240 --> 00:27:20,970
everybody knows this by now

00:27:19,440 --> 00:27:23,310
but a question is something you don't

00:27:20,970 --> 00:27:26,990
already know the answer to mics are

00:27:23,310 --> 00:27:26,990
there and maybe there

00:27:27,710 --> 00:27:38,180
[Applause]

00:27:45,560 --> 00:27:50,640
hey thank you so much for the lesson on

00:27:48,180 --> 00:27:53,250
post-mortems I'm sure a lot of us have

00:27:50,640 --> 00:27:55,770
worked in different dev shops that are

00:27:53,250 --> 00:27:57,360
very blamed full and post mortems do you

00:27:55,770 --> 00:27:59,910
have any advice on how to help

00:27:57,360 --> 00:28:03,000
transition a culture from a blame full

00:27:59,910 --> 00:28:04,650
mindset to a blameless mindset yeah it's

00:28:03,000 --> 00:28:06,300
definitely hard once people learn the

00:28:04,650 --> 00:28:08,190
lesson that if I say I did something

00:28:06,300 --> 00:28:09,960
wrong I'll get blamed for it

00:28:08,190 --> 00:28:12,090
it takes an awful lot of work to undo

00:28:09,960 --> 00:28:14,370
that lesson I think it's very difficult

00:28:12,090 --> 00:28:16,260
to undo that lesson without having your

00:28:14,370 --> 00:28:18,540
team leads or management involved in

00:28:16,260 --> 00:28:20,370
sort of leading by example being willing

00:28:18,540 --> 00:28:22,260
to say I did this and it led to this

00:28:20,370 --> 00:28:24,510
situation and here's how we're gonna

00:28:22,260 --> 00:28:25,920
focus on the systemic improvement so I

00:28:24,510 --> 00:28:27,170
believe that's one of those things that

00:28:25,920 --> 00:28:34,080
really has to start with leadership

00:28:27,170 --> 00:28:35,370
demonstrating it for everyone you may

00:28:34,080 --> 00:28:36,810
have mentioned this earlier in your

00:28:35,370 --> 00:28:38,730
presentation but do you have

00:28:36,810 --> 00:28:40,890
recommendations from your experience on

00:28:38,730 --> 00:28:44,310
whether or not it's helpful or not

00:28:40,890 --> 00:28:45,990
helpful to have like executive

00:28:44,310 --> 00:28:47,640
representation in a post-mortem or

00:28:45,990 --> 00:28:51,330
explicitly keep it to people who were

00:28:47,640 --> 00:28:52,980
involved in an incident I think it can

00:28:51,330 --> 00:28:54,960
be helpful to have executives in the

00:28:52,980 --> 00:28:57,150
room but it's critical that you have the

00:28:54,960 --> 00:28:59,850
team members who are on the sharp end in

00:28:57,150 --> 00:29:01,470
the room generally you want executives

00:28:59,850 --> 00:29:03,900
to see the value of putting the effort

00:29:01,470 --> 00:29:06,060
into these and to understand why you're

00:29:03,900 --> 00:29:07,860
asking for resources to prioritize these

00:29:06,060 --> 00:29:09,960
fixes so if having them in the room

00:29:07,860 --> 00:29:11,850
helps them understand why you want to

00:29:09,960 --> 00:29:18,330
put time into these remedial efforts

00:29:11,850 --> 00:29:19,500
that definitely is valuable cool thank

00:29:18,330 --> 00:29:20,230
you all very much hope you all have an

00:29:19,500 --> 00:29:25,079
excellent PyCon

00:29:20,230 --> 00:29:25,079

YouTube URL: https://www.youtube.com/watch?v=L9Y2ap6vIMg


