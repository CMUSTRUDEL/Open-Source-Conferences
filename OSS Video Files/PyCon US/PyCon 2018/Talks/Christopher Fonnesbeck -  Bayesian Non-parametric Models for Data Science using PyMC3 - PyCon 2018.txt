Title: Christopher Fonnesbeck -  Bayesian Non-parametric Models for Data Science using PyMC3 - PyCon 2018
Publication date: 2018-08-06
Playlist: Talks
Description: 
	Speaker: Christopher Fonnesbeck

Nowadays, there are many ways of building data science models using Python, including statistical and machine learning methods. I will introduce probabilistic models, which use Bayesian statistical methods to quantify all aspects of uncertainty relevant to your problem, and provide inferences in simple, interpretable terms using probabilities.  A particularly flexible form of probabilistic models uses Bayesian *non-parametric* methods, which allow models to vary in complexity depending on how much data are available. In doing so, they avoid the over-fitting that is common in machine learning and statistical modeling. I will demonstrate the basics of Bayesian non-parametric modeling in Python, using the PyMC3 package. Specifically, I will introduce two common types, Gaussian processes and Dirichlet processes, and show how they can be applied easily to real-world problems using two examples.

Slides can be found at: https://speakerdeck.com/pycon2018 and https://github.com/PyCon/2018-slides
Captions: 
	00:00:45,899 --> 00:00:50,949
right good afternoon everybody welcome

00:00:48,879 --> 00:00:52,840
to this session this afternoon

00:00:50,949 --> 00:00:55,870
today we're thrilled to have Chris

00:00:52,840 --> 00:00:58,120
Vaughn's Beck who is from the Vanderbilt

00:00:55,870 --> 00:01:00,579
University Department of biostats he's

00:00:58,120 --> 00:01:03,730
the benevolent dictator for Life of Pi

00:01:00,579 --> 00:01:05,950
mc3 a package I use a lot and it's a

00:01:03,730 --> 00:01:08,020
it's it's for Bayesian statistical

00:01:05,950 --> 00:01:09,880
modeling and the likes and so he's here

00:01:08,020 --> 00:01:12,369
to talk about Bayesian nonparametric

00:01:09,880 --> 00:01:15,070
models which actually ironically means

00:01:12,369 --> 00:01:17,320
infinite number of parameters and we're

00:01:15,070 --> 00:01:18,759
gonna hear him tell tell us about that

00:01:17,320 --> 00:01:24,910
stuff thank you Chris

00:01:18,759 --> 00:01:29,050
please come on you spoiled everything

00:01:24,910 --> 00:01:30,880
Eric that was my punchline okay thanks

00:01:29,050 --> 00:01:35,440
everybody for coming they're competing

00:01:30,880 --> 00:01:39,009
PI MC talks in this time slot so Colin

00:01:35,440 --> 00:01:41,409
Carol is talking about some applications

00:01:39,009 --> 00:01:44,009
I'm glad you joined me here I heard his

00:01:41,409 --> 00:01:49,810
have nice pictures of puppies and such

00:01:44,009 --> 00:01:53,920
but hopefully I will give you something

00:01:49,810 --> 00:01:58,469
to think about here as well so let me

00:01:53,920 --> 00:02:01,680
just adjust my display a little bit

00:01:58,469 --> 00:02:01,680
sorry about that

00:02:11,180 --> 00:02:17,269
there we go all right so yeah i MC 3 is

00:02:14,780 --> 00:02:19,189
a library programming probabilistic

00:02:17,269 --> 00:02:24,439
programming library for python that I

00:02:19,189 --> 00:02:29,590
helped to develop Eric does too and it

00:02:24,439 --> 00:02:32,359
is a very specific application for doing

00:02:29,590 --> 00:02:35,180
statistical and probabilistic machine

00:02:32,359 --> 00:02:39,409
learning I'm gonna today talk about one

00:02:35,180 --> 00:02:42,469
very specific subset of that so you know

00:02:39,409 --> 00:02:45,409
PyCon is one of the useful things about

00:02:42,469 --> 00:02:47,829
PyCon talks is that it can be used for

00:02:45,409 --> 00:02:50,269
kind of stocking or your toolbox and

00:02:47,829 --> 00:02:52,250
exposing sort of maybe less known

00:02:50,269 --> 00:02:54,439
capabilities of packages and so that's

00:02:52,250 --> 00:02:56,510
kind of what I'm intending to do here

00:02:54,439 --> 00:02:58,250
and what I'm going to focus on is my

00:02:56,510 --> 00:03:02,540
name that the name of the talk implies

00:02:58,250 --> 00:03:04,069
is one particular nonparametric Bayesian

00:03:02,540 --> 00:03:05,900
model and explain what nonparametric

00:03:04,069 --> 00:03:08,750
Bayesian models are in a second I

00:03:05,900 --> 00:03:13,189
realize in the abstract it claims to be

00:03:08,750 --> 00:03:14,900
presenting two of these methods but as I

00:03:13,189 --> 00:03:16,849
finished the Gaussian process part of

00:03:14,900 --> 00:03:18,769
things I was already up to 60 slides so

00:03:16,849 --> 00:03:21,769
I think we're gonna have more than

00:03:18,769 --> 00:03:28,010
enough to to chew on here but just to

00:03:21,769 --> 00:03:30,109
motivate this application this should

00:03:28,010 --> 00:03:33,260
sort of be motivated by the fact that

00:03:30,109 --> 00:03:34,549
data are messy as we all know and and

00:03:33,260 --> 00:03:36,590
there's different ways of being messy

00:03:34,549 --> 00:03:38,900
right I'm not really talking about kind

00:03:36,590 --> 00:03:43,370
of missing values or unstructured miss

00:03:38,900 --> 00:03:47,419
or incorrect values I'm talking more

00:03:43,370 --> 00:03:51,019
about the the underlying generating

00:03:47,419 --> 00:03:53,209
models that create the data that we

00:03:51,019 --> 00:03:58,000
observe and so the data that we often

00:03:53,209 --> 00:04:01,159
see in documentation and examples and

00:03:58,000 --> 00:04:03,620
textbooks are usually nice and neat

00:04:01,159 --> 00:04:05,989
right so this actually comes from a text

00:04:03,620 --> 00:04:08,090
book it's you know a linear regression

00:04:05,989 --> 00:04:10,250
type of a model and and of course the

00:04:08,090 --> 00:04:13,939
the points all live more or less along

00:04:10,250 --> 00:04:15,949
that straight line but of course you

00:04:13,939 --> 00:04:19,159
know nature is different than that right

00:04:15,949 --> 00:04:22,159
the data that we often get look more

00:04:19,159 --> 00:04:24,800
like more like this so these are just

00:04:22,159 --> 00:04:27,020
sort of arbitrarily

00:04:24,800 --> 00:04:29,750
drawn examples from stuff that I work on

00:04:27,020 --> 00:04:31,520
so three of these are baseball datasets

00:04:29,750 --> 00:04:36,229
the one on the top the purple one in the

00:04:31,520 --> 00:04:38,360
top left side is player value as a

00:04:36,229 --> 00:04:40,520
function of signing age the big

00:04:38,360 --> 00:04:44,319
distribution on the other top corner are

00:04:40,520 --> 00:04:46,789
actually boston marathon finishing times

00:04:44,319 --> 00:04:49,389
and so on and and of course you know

00:04:46,789 --> 00:04:53,000
straight lines don't really fit through

00:04:49,389 --> 00:04:55,539
these guys very well you know they're

00:04:53,000 --> 00:04:59,750
sort of mixtures of different processes

00:04:55,539 --> 00:05:02,479
and applying some of these sort of

00:04:59,750 --> 00:05:04,840
default behaviors as we often do when we

00:05:02,479 --> 00:05:09,229
build models won't work all that well

00:05:04,840 --> 00:05:11,630
and the default statistical assumptions

00:05:09,229 --> 00:05:14,180
that were usually relying on are ones

00:05:11,630 --> 00:05:16,190
like this so for those of you unfamiliar

00:05:14,180 --> 00:05:17,860
with statistical modeling this is just a

00:05:16,190 --> 00:05:20,569
simple linear regression right so

00:05:17,860 --> 00:05:22,789
predicting Y as a function of X and

00:05:20,569 --> 00:05:24,319
there's a slope beta one and an

00:05:22,789 --> 00:05:26,840
intercept bit or not and there's some

00:05:24,319 --> 00:05:28,099
error epsilon and so this this implies a

00:05:26,840 --> 00:05:31,759
straight line a straight line

00:05:28,099 --> 00:05:33,469
relationship and moreover the the the

00:05:31,759 --> 00:05:37,880
error is normally distributed it's a

00:05:33,469 --> 00:05:39,919
nice bell curve and so these assumptions

00:05:37,880 --> 00:05:41,930
of parametric distributions for data and

00:05:39,919 --> 00:05:44,569
what I'm going to focus on more here

00:05:41,930 --> 00:05:47,719
today is the linear relationships among

00:05:44,569 --> 00:05:49,490
variables is often inappropriate so

00:05:47,719 --> 00:05:52,120
here's a you know an example that I'll

00:05:49,490 --> 00:05:55,430
talk a little bit about later this is a

00:05:52,120 --> 00:05:58,270
small data set of salmon spawning so

00:05:55,430 --> 00:06:00,680
along the x-axis our spawners or female

00:05:58,270 --> 00:06:02,900
salmon and then the recruits are the the

00:06:00,680 --> 00:06:05,150
fry the young salmon are on the x-axis

00:06:02,900 --> 00:06:08,210
and that's on the log scale and you know

00:06:05,150 --> 00:06:10,430
you could fit these this is just using a

00:06:08,210 --> 00:06:12,889
linear regression and and you might get

00:06:10,430 --> 00:06:14,509
a decent you know correlation with this

00:06:12,889 --> 00:06:16,279
but obviously it's a wrong model you'll

00:06:14,509 --> 00:06:18,169
make some there'll be some problems if

00:06:16,279 --> 00:06:19,909
you try to use this for anything it over

00:06:18,169 --> 00:06:21,889
predicts in some other areas and under

00:06:19,909 --> 00:06:23,180
predicts and others and of course we

00:06:21,889 --> 00:06:25,219
could you know fit these using

00:06:23,180 --> 00:06:27,440
traditional nonlinear methods like

00:06:25,219 --> 00:06:28,639
polynomials and things like that but

00:06:27,440 --> 00:06:29,690
it's a little bit more cumbersome so

00:06:28,639 --> 00:06:30,800
what I'm gonna talk about today is a

00:06:29,690 --> 00:06:33,110
little bit something that's a little

00:06:30,800 --> 00:06:35,630
more automated a little more flexible

00:06:33,110 --> 00:06:37,790
and the approach that I'm going to

00:06:35,630 --> 00:06:39,770
discuss uses Bayesian methods

00:06:37,790 --> 00:06:42,800
which is you know a fancy word really

00:06:39,770 --> 00:06:45,740
for probabilistic programming and what

00:06:42,800 --> 00:06:48,140
Bayesian methods do for those unfamiliar

00:06:45,740 --> 00:06:49,880
is that it allows us to make inferences

00:06:48,140 --> 00:06:51,980
and predictions about things that we

00:06:49,880 --> 00:06:55,010
care about using probabilities

00:06:51,980 --> 00:06:57,230
essentially so my sort of two-minute

00:06:55,010 --> 00:06:58,670
Bayes intro this is Bayes formula I'm

00:06:57,230 --> 00:07:00,110
not gonna have a lot of equations in

00:06:58,670 --> 00:07:02,720
this talk the ones that I do have are

00:07:00,110 --> 00:07:04,370
simply illustrative so this is this is

00:07:02,720 --> 00:07:07,100
base formula and what it does is it

00:07:04,370 --> 00:07:08,510
tells us something about unknown things

00:07:07,100 --> 00:07:11,480
that we care about we're gonna call

00:07:08,510 --> 00:07:14,480
these theta based on things we observe

00:07:11,480 --> 00:07:17,930
ie data and this is why so the thing on

00:07:14,480 --> 00:07:20,330
the far left is something we call a

00:07:17,930 --> 00:07:24,340
posterior distribution it's what we know

00:07:20,330 --> 00:07:27,800
about theta after having observed Y and

00:07:24,340 --> 00:07:30,020
we get that the way that Bayes formula

00:07:27,800 --> 00:07:32,660
is magical is it gives that gives us

00:07:30,020 --> 00:07:36,140
that using quantitative quantities that

00:07:32,660 --> 00:07:37,910
we have in hand ie information about our

00:07:36,140 --> 00:07:40,250
unknowns before we look at our data and

00:07:37,910 --> 00:07:42,950
we call the this the prior and then our

00:07:40,250 --> 00:07:44,870
we integrate our data using a particular

00:07:42,950 --> 00:07:49,280
type of probability distribution called

00:07:44,870 --> 00:07:51,020
a likelihood function and that symbol

00:07:49,280 --> 00:07:53,780
that's linking these two components is a

00:07:51,020 --> 00:07:56,300
proportional to sines so there are equal

00:07:53,780 --> 00:08:00,080
to up to a constant okay that's all you

00:07:56,300 --> 00:08:02,540
really need to know about Bayes here so

00:08:00,080 --> 00:08:05,140
what I'm gonna do is the method that I'm

00:08:02,540 --> 00:08:07,930
gonna show off is is a method for

00:08:05,140 --> 00:08:10,640
modeling complex nonlinear functions

00:08:07,930 --> 00:08:13,340
using Gaussian distributions normal

00:08:10,640 --> 00:08:15,320
distributions so most of us probably

00:08:13,340 --> 00:08:17,270
remember from our undergraduate or

00:08:15,320 --> 00:08:20,410
graduate statistics class normal

00:08:17,270 --> 00:08:23,570
distributions a very particular

00:08:20,410 --> 00:08:26,390
statistical distribution it's it it's

00:08:23,570 --> 00:08:28,370
symmetric it has most of its

00:08:26,390 --> 00:08:31,070
observations within two standard

00:08:28,370 --> 00:08:33,440
deviations of the mean it seems like an

00:08:31,070 --> 00:08:36,700
unlikely candidate for modeling messy

00:08:33,440 --> 00:08:39,860
data seems like a very very structured

00:08:36,700 --> 00:08:41,270
limited probability distribution but

00:08:39,860 --> 00:08:42,740
there are a couple of properties of

00:08:41,270 --> 00:08:45,320
normal distributions that are very

00:08:42,740 --> 00:08:48,080
helpful in this context and there are

00:08:45,320 --> 00:08:50,660
two in particular one is something known

00:08:48,080 --> 00:08:51,700
as the conditioning property the idea

00:08:50,660 --> 00:08:54,140
here is that

00:08:51,700 --> 00:08:55,760
that can if you have a multivariate

00:08:54,140 --> 00:08:58,190
normal distribution the conditional

00:08:55,760 --> 00:09:01,040
distribution of some elements of that

00:08:58,190 --> 00:09:02,960
data conditional on the rest of them is

00:09:01,040 --> 00:09:05,510
also normal so the only reason I'm

00:09:02,960 --> 00:09:07,430
showing this equation here is that this

00:09:05,510 --> 00:09:09,529
is the conditional normal so the things

00:09:07,430 --> 00:09:12,260
the mean and the and the covariance

00:09:09,529 --> 00:09:14,150
there are things that can be calculated

00:09:12,260 --> 00:09:15,860
in closed form right you don't have to

00:09:14,150 --> 00:09:17,810
do any numerical approximation you can

00:09:15,860 --> 00:09:20,390
write it down on a piece of paper and

00:09:17,810 --> 00:09:22,490
that's very handy relatedly the other

00:09:20,390 --> 00:09:24,470
useful property is the marginalization

00:09:22,490 --> 00:09:26,480
properly and this all this says that the

00:09:24,470 --> 00:09:28,520
marginal distribution of some of the

00:09:26,480 --> 00:09:31,880
elements of your multivariate normal is

00:09:28,520 --> 00:09:34,160
also normally distributed so if we have

00:09:31,880 --> 00:09:35,930
a big multivariate normal we call one

00:09:34,160 --> 00:09:39,650
subset of them X and the other subset

00:09:35,930 --> 00:09:41,779
why the if you integrate out y you don't

00:09:39,650 --> 00:09:44,029
actually have to do that integration

00:09:41,779 --> 00:09:46,490
nobody likes to do integration you just

00:09:44,029 --> 00:09:50,060
have to pop in the the marginal that the

00:09:46,490 --> 00:09:51,920
MU X for the mean and Sigma X for the

00:09:50,060 --> 00:09:53,089
covariance and you you're done you can

00:09:51,920 --> 00:09:57,020
walk away don't have to worry about

00:09:53,089 --> 00:09:58,310
anything else so well what this allows

00:09:57,020 --> 00:09:59,690
us to do is build a model called a

00:09:58,310 --> 00:10:01,460
Gaussian process and that's what I'm

00:09:59,690 --> 00:10:02,660
going to talk about today the even thing

00:10:01,460 --> 00:10:04,970
of a Gaussian process is a

00:10:02,660 --> 00:10:07,910
generalization of a normal distribution

00:10:04,970 --> 00:10:10,339
so rather than being a distribution over

00:10:07,910 --> 00:10:12,650
values so here why are some values that

00:10:10,339 --> 00:10:14,990
we observe we're talking about here a

00:10:12,650 --> 00:10:16,970
distribution over functions so a

00:10:14,990 --> 00:10:18,800
realization of a Gaussian processes I

00:10:16,970 --> 00:10:20,270
function and that's a little bit

00:10:18,800 --> 00:10:23,300
different you can think of a function as

00:10:20,270 --> 00:10:25,610
being a generalization of like a lookup

00:10:23,300 --> 00:10:27,230
table right so rather than indexing

00:10:25,610 --> 00:10:30,530
individual elements out with index

00:10:27,230 --> 00:10:34,280
values you can give arbitrary arguments

00:10:30,530 --> 00:10:35,810
and get some value back so what is a

00:10:34,280 --> 00:10:37,940
Gaussian process well the formal

00:10:35,810 --> 00:10:40,270
definition is this an infinite connect

00:10:37,940 --> 00:10:42,980
collection of random variables any

00:10:40,270 --> 00:10:45,260
finite subset of which has a Gaussian

00:10:42,980 --> 00:10:48,380
distribution well that doesn't sound

00:10:45,260 --> 00:10:50,150
like a very helpful or useful definition

00:10:48,380 --> 00:10:53,390
but it does illustrate how we can work

00:10:50,150 --> 00:10:57,730
with them so as Eric pointed out in the

00:10:53,390 --> 00:11:00,890
introduction we call this nonparametric

00:10:57,730 --> 00:11:03,260
methods but what's all this business

00:11:00,890 --> 00:11:05,360
then of an infinite number of parameters

00:11:03,260 --> 00:11:07,370
well nonparametric is

00:11:05,360 --> 00:11:08,990
bit of a misnomer we actually do mean an

00:11:07,370 --> 00:11:10,519
infinite number of parameters or a

00:11:08,990 --> 00:11:12,079
better way of thinking of it is that the

00:11:10,519 --> 00:11:13,850
number of parameters the end of using

00:11:12,079 --> 00:11:15,290
scales with the size of the data and

00:11:13,850 --> 00:11:16,700
that's what we want if you don't have

00:11:15,290 --> 00:11:18,470
much data you shouldn't be using very

00:11:16,700 --> 00:11:20,570
many parameters in your model if you

00:11:18,470 --> 00:11:21,860
have lots of data you should be be using

00:11:20,570 --> 00:11:25,149
more parameters and this is what

00:11:21,860 --> 00:11:27,920
Gaussian processes allows us to do so

00:11:25,149 --> 00:11:29,360
and what we're doing here is we're

00:11:27,920 --> 00:11:31,160
actually modeling the underlying

00:11:29,360 --> 00:11:33,800
function directly and that's how it's

00:11:31,160 --> 00:11:36,079
useful for complex nonlinear stuff right

00:11:33,800 --> 00:11:38,690
so if you can you think about what a

00:11:36,079 --> 00:11:41,000
regular normal distribution does or or

00:11:38,690 --> 00:11:42,709
say a regular linear regression does it

00:11:41,000 --> 00:11:44,690
models that function indirectly via

00:11:42,709 --> 00:11:47,510
those parameters beta and beta naught

00:11:44,690 --> 00:11:49,100
beta 1 and beta naught here we're

00:11:47,510 --> 00:11:53,930
essentially modeling the functions

00:11:49,100 --> 00:11:55,790
directly and the the meet of the

00:11:53,930 --> 00:11:58,760
Gaussian process the thing that makes it

00:11:55,790 --> 00:12:00,829
go the engine is a better analogy is the

00:11:58,760 --> 00:12:02,510
covariance function so rather than

00:12:00,829 --> 00:12:04,519
having a covariance matrix like you'd

00:12:02,510 --> 00:12:06,230
have in a normal distribution we have a

00:12:04,519 --> 00:12:08,600
covariance function that generates

00:12:06,230 --> 00:12:11,839
covariance matrices as we give it

00:12:08,600 --> 00:12:13,459
arguments and what this does is that the

00:12:11,839 --> 00:12:15,410
the covariance matrix essentially

00:12:13,459 --> 00:12:17,750
ensures that values close together in

00:12:15,410 --> 00:12:19,519
the input space will produce output

00:12:17,750 --> 00:12:21,709
values that are also close together and

00:12:19,519 --> 00:12:23,660
so the art in doing Gaussian processes

00:12:21,709 --> 00:12:26,440
is choosing the appropriate covariance

00:12:23,660 --> 00:12:29,180
function so if you have sort of smoothly

00:12:26,440 --> 00:12:30,589
varying functions that you're trying to

00:12:29,180 --> 00:12:33,890
model you might pick something like a

00:12:30,589 --> 00:12:35,990
quadratic and so the sample the plot at

00:12:33,890 --> 00:12:38,269
the bottom are simply realizations from

00:12:35,990 --> 00:12:40,790
up a Gaussian process prior with a

00:12:38,269 --> 00:12:43,089
quadratic covariance function if you

00:12:40,790 --> 00:12:46,570
need something a little bit more jagged

00:12:43,089 --> 00:12:48,920
changing a little faster with distance

00:12:46,570 --> 00:12:50,720
amatuer n-- is a little bit more

00:12:48,920 --> 00:12:52,519
flexible and there's a whole suite of

00:12:50,720 --> 00:12:53,810
them I'm only going to show you three

00:12:52,519 --> 00:12:56,480
here there's a cosine so you have

00:12:53,810 --> 00:12:58,040
periodic stuff perhaps going on inside

00:12:56,480 --> 00:13:01,820
of your Gaussian process you can use a

00:12:58,040 --> 00:13:03,410
cosine covariance function etc there is

00:13:01,820 --> 00:13:05,390
also a mean function so just like a

00:13:03,410 --> 00:13:08,180
normal distribution is parametrized by a

00:13:05,390 --> 00:13:10,269
mean and a covariance matrix the

00:13:08,180 --> 00:13:12,890
Gaussian process is fully specified by a

00:13:10,269 --> 00:13:15,079
covariance function and a mean function

00:13:12,890 --> 00:13:17,810
it turns out the mean function is not

00:13:15,079 --> 00:13:19,279
very interesting it's essentially used

00:13:17,810 --> 00:13:21,890
to

00:13:19,279 --> 00:13:23,600
provide kind of a prior guess at what

00:13:21,890 --> 00:13:25,610
the underlying function might look like

00:13:23,600 --> 00:13:27,740
if you have that kind of information but

00:13:25,610 --> 00:13:29,360
it and it and turns out that it doesn't

00:13:27,740 --> 00:13:31,970
have a lot of influence on the posterior

00:13:29,360 --> 00:13:34,670
once you add some data to it so you can

00:13:31,970 --> 00:13:37,040
usually specify it as a zero or a

00:13:34,670 --> 00:13:38,450
constant or maybe a linear function I'll

00:13:37,040 --> 00:13:40,490
show you one with a linear function

00:13:38,450 --> 00:13:42,740
later on so to kind of give you a visual

00:13:40,490 --> 00:13:44,839
sense of what GPS look like I'm gonna

00:13:42,740 --> 00:13:47,600
what I'm gonna do here is sort of piece

00:13:44,839 --> 00:13:49,730
by piece draw sample from a prior

00:13:47,600 --> 00:13:54,310
Gaussian process so this particular

00:13:49,730 --> 00:13:57,529
prior GP has a mean of 0 and a quadratic

00:13:54,310 --> 00:13:59,209
quadratic exponential covariance

00:13:57,529 --> 00:14:00,290
function with parameter 1 and so it kind

00:13:59,209 --> 00:14:03,140
of looks like this so we're really just

00:14:00,290 --> 00:14:05,690
specifying the range of values that that

00:14:03,140 --> 00:14:07,880
the function could take and so it should

00:14:05,690 --> 00:14:10,220
bounce around inside of that interval

00:14:07,880 --> 00:14:12,800
with a mean on that red line and so we

00:14:10,220 --> 00:14:14,420
can draw let's say one point from one of

00:14:12,800 --> 00:14:16,160
these functions and so all we have to do

00:14:14,420 --> 00:14:17,600
here is draw on normal if there's no

00:14:16,160 --> 00:14:19,399
covariance now cuz this is the first

00:14:17,600 --> 00:14:21,740
point that's being drawn so I can do

00:14:19,399 --> 00:14:23,959
that in Python and then conditional on

00:14:21,740 --> 00:14:25,910
that point I can draw others so here's

00:14:23,959 --> 00:14:28,339
my second point and you see what happens

00:14:25,910 --> 00:14:30,260
here we're making kind of link sausages

00:14:28,339 --> 00:14:32,060
right so as you get farther away from

00:14:30,260 --> 00:14:33,800
the point it starts returning to its

00:14:32,060 --> 00:14:35,959
prior but we have a lot of information

00:14:33,800 --> 00:14:38,959
near the point and because they have

00:14:35,959 --> 00:14:40,790
covary it becomes closer to that value

00:14:38,959 --> 00:14:42,050
we don't have to do this one at a time

00:14:40,790 --> 00:14:43,399
the only reason I'm sampling them

00:14:42,050 --> 00:14:45,709
separately at all is to kind of again

00:14:43,399 --> 00:14:48,260
give you an idea so let's take four or

00:14:45,709 --> 00:14:50,300
five more points and and what we're

00:14:48,260 --> 00:14:52,940
starting to see here is a function right

00:14:50,300 --> 00:14:56,029
if I kept taking points I would get more

00:14:52,940 --> 00:14:58,250
or less continuous function and if I

00:14:56,029 --> 00:15:00,800
reset my or picked a different random

00:14:58,250 --> 00:15:02,600
number seed at the beginning and I did a

00:15:00,800 --> 00:15:04,160
few of these I would get a whole bunch

00:15:02,600 --> 00:15:06,290
of different samples so these are draws

00:15:04,160 --> 00:15:08,209
from the prior and notice that these are

00:15:06,290 --> 00:15:10,430
all kind of bounce around with mean zero

00:15:08,209 --> 00:15:13,550
and within sort of a standard deviation

00:15:10,430 --> 00:15:15,949
or two from that from that mean there's

00:15:13,550 --> 00:15:17,990
no data here all right these are prior

00:15:15,949 --> 00:15:20,240
processes and we'll convert these into

00:15:17,990 --> 00:15:23,750
posteriors functions once we've seen

00:15:20,240 --> 00:15:26,149
some data doing that is a little tricky

00:15:23,750 --> 00:15:28,790
and it's hard to do manually so this is

00:15:26,149 --> 00:15:31,730
where you want to look for a third party

00:15:28,790 --> 00:15:32,660
package happily there are many many ways

00:15:31,730 --> 00:15:35,660
of fitting

00:15:32,660 --> 00:15:38,540
in Python lots of packages allow for

00:15:35,660 --> 00:15:40,160
this now GPI and GP flow from the folks

00:15:38,540 --> 00:15:41,779
at the University of Sheffield is one of

00:15:40,160 --> 00:15:42,980
the best ones out there

00:15:41,779 --> 00:15:44,810
one of the ones has been around the

00:15:42,980 --> 00:15:47,540
longest you can do them in scikit-learn

00:15:44,810 --> 00:15:50,029
they're not quite as Bayesian as in some

00:15:47,540 --> 00:15:52,699
other implementations but they're very

00:15:50,029 --> 00:15:54,290
good particularly the new ones there's

00:15:52,699 --> 00:15:57,350
the three old men stand Edward and

00:15:54,290 --> 00:16:01,040
George they all do them with varying

00:15:57,350 --> 00:16:03,350
degrees of automation and then PIME c3

00:16:01,040 --> 00:16:05,930
which I'm going to talk about here so PI

00:16:03,350 --> 00:16:07,850
mc3 just a quick intro I I started this

00:16:05,930 --> 00:16:11,300
way back in 2003 when I was a postdoc

00:16:07,850 --> 00:16:14,300
and it's up it's a probabilistic

00:16:11,300 --> 00:16:17,290
programming framework for again fitting

00:16:14,300 --> 00:16:19,279
a variety of probability models using

00:16:17,290 --> 00:16:21,350
what I would call next-generation

00:16:19,279 --> 00:16:23,990
Bayesian inference methods so we're

00:16:21,350 --> 00:16:26,180
talking gradient based Markov chain

00:16:23,990 --> 00:16:30,110
Monte Carlo and variational inference

00:16:26,180 --> 00:16:32,800
primarily it's currently based on Theano

00:16:30,110 --> 00:16:36,019
which some of you may have heard has

00:16:32,800 --> 00:16:39,649
essentially shut its doors and so we'll

00:16:36,019 --> 00:16:42,889
be looking to shift PI MC over to a new

00:16:39,649 --> 00:16:45,589
back-end in the coming months and years

00:16:42,889 --> 00:16:49,009
most likely tensorflow so stay tuned for

00:16:45,589 --> 00:16:52,100
information on that so I want to

00:16:49,009 --> 00:16:53,810
motivate the the PI MC Gaussian

00:16:52,100 --> 00:16:55,910
processes with some real-world examples

00:16:53,810 --> 00:16:58,370
I already showed you this salmon

00:16:55,910 --> 00:17:01,279
recruitment data set that looks like

00:16:58,370 --> 00:17:04,250
this and so how do we fit a GP to this

00:17:01,279 --> 00:17:07,730
well if we go back to a base formula

00:17:04,250 --> 00:17:10,250
remember posterior likelihood prior our

00:17:07,730 --> 00:17:12,289
prior is gonna be a GP prior like the

00:17:10,250 --> 00:17:14,299
one that I was constructing using the

00:17:12,289 --> 00:17:16,189
covariance function that I drew values

00:17:14,299 --> 00:17:19,730
from our data in this case is going to

00:17:16,189 --> 00:17:21,470
be Gaussian because it's a log transform

00:17:19,730 --> 00:17:23,510
the data and then what I'm gonna get out

00:17:21,470 --> 00:17:25,339
the other end is a closed form Gaussian

00:17:23,510 --> 00:17:27,350
process closed form like we don't have

00:17:25,339 --> 00:17:29,480
to really do any numerical computation

00:17:27,350 --> 00:17:31,669
you start with a GP you give it normal

00:17:29,480 --> 00:17:35,059
data you get a Gaussian process back

00:17:31,669 --> 00:17:36,380
normal normal normal so here's what it

00:17:35,059 --> 00:17:40,309
looks like this is what prime C code

00:17:36,380 --> 00:17:42,860
looks like so it uses a context manager

00:17:40,309 --> 00:17:45,530
to construct the model so you declare

00:17:42,860 --> 00:17:46,130
you instantiate a model object give it a

00:17:45,530 --> 00:17:47,960
name

00:17:46,130 --> 00:17:51,220
and what I'm doing here is I'm

00:17:47,960 --> 00:17:55,220
specifying the hyper parameters for a

00:17:51,220 --> 00:17:56,900
quadratic exponential so ro and ADA here

00:17:55,220 --> 00:17:58,700
are actually very interpretable hyper

00:17:56,900 --> 00:18:01,700
parameters ADA's the signal variance

00:17:58,700 --> 00:18:04,100
along the Y and Rho is the length scale

00:18:01,700 --> 00:18:07,580
that kind of stretches things to see how

00:18:04,100 --> 00:18:09,140
far observations have to go before they

00:18:07,580 --> 00:18:11,390
become very different and and this is

00:18:09,140 --> 00:18:15,140
the again the exponential quadratic

00:18:11,390 --> 00:18:17,750
exponential covariance function so

00:18:15,140 --> 00:18:18,950
here's the covariance function so I'm

00:18:17,750 --> 00:18:20,600
going to cope there's a mean function

00:18:18,950 --> 00:18:24,200
and a covariance function here so M is

00:18:20,600 --> 00:18:26,380
one of those sort of throwaway functions

00:18:24,200 --> 00:18:28,850
this time I'm going to use a linear

00:18:26,380 --> 00:18:30,890
model here because you know you might

00:18:28,850 --> 00:18:32,570
have some prior knowledge about salmon

00:18:30,890 --> 00:18:34,430
growth you you think it goes up like

00:18:32,570 --> 00:18:36,650
this and so I'm just taking the rise

00:18:34,430 --> 00:18:38,300
over the run of the data and supplying

00:18:36,650 --> 00:18:40,100
that as the coefficient again kind of a

00:18:38,300 --> 00:18:41,900
prior guess we don't really expect it to

00:18:40,100 --> 00:18:43,970
look like that when we're done but it's

00:18:41,900 --> 00:18:46,850
a good place to start and then the

00:18:43,970 --> 00:18:48,740
second line is the is the quadratic

00:18:46,850 --> 00:18:51,530
exponential covariance function that's

00:18:48,740 --> 00:18:54,470
parameterize by row and then I construct

00:18:51,530 --> 00:18:59,450
a marginal Gaussian process so that's

00:18:54,470 --> 00:19:00,740
the marginal GP and and then the data so

00:18:59,450 --> 00:19:02,750
then we bring in the data so that was

00:19:00,740 --> 00:19:03,800
the prior here's the data so we use

00:19:02,750 --> 00:19:07,820
what's called the marginal likelihood

00:19:03,800 --> 00:19:09,590
and we pass it x and y the spawners and

00:19:07,820 --> 00:19:11,720
the recruits and then we allow for a

00:19:09,590 --> 00:19:13,520
little bit of observation noise right

00:19:11,720 --> 00:19:17,090
differences between the expected value

00:19:13,520 --> 00:19:18,710
and what you actually observe and all we

00:19:17,090 --> 00:19:20,360
have to do here is optimize we actually

00:19:18,710 --> 00:19:21,650
don't have to do any again fancy

00:19:20,360 --> 00:19:24,190
numerical stuff I'm just going to

00:19:21,650 --> 00:19:27,740
optimize the values of those parameters

00:19:24,190 --> 00:19:29,990
and one of the nice things about using

00:19:27,740 --> 00:19:31,760
Bayesian methods is that it's a really

00:19:29,990 --> 00:19:33,200
elegant way of making predictions once

00:19:31,760 --> 00:19:36,500
you've got your model you kind of get

00:19:33,200 --> 00:19:37,610
your predictions for free and what we

00:19:36,500 --> 00:19:39,710
use for that is something called the

00:19:37,610 --> 00:19:41,300
posterior predictive distribution which

00:19:39,710 --> 00:19:44,390
kind of looks ugly anything with an

00:19:41,300 --> 00:19:44,900
integral looks you know unfriendly from

00:19:44,390 --> 00:19:46,820
the start

00:19:44,900 --> 00:19:49,160
but what we're doing here is essentially

00:19:46,820 --> 00:19:51,290
we're incorporating the information that

00:19:49,160 --> 00:19:53,330
we have about our process using the

00:19:51,290 --> 00:19:55,850
posterior distribution about theta and

00:19:53,330 --> 00:19:57,560
then also the randomness the uncertainty

00:19:55,850 --> 00:19:58,550
having to do with just kind of random

00:19:57,560 --> 00:20:00,620
sampling of our date

00:19:58,550 --> 00:20:03,920
and so we're able to make predictions

00:20:00,620 --> 00:20:06,429
about new things why new given stuff

00:20:03,920 --> 00:20:09,559
that we've seen and used to fit our data

00:20:06,429 --> 00:20:10,820
so and in this case with a GP we're

00:20:09,559 --> 00:20:12,200
actually just going to be drawing from a

00:20:10,820 --> 00:20:15,980
normal distribution again we're gonna

00:20:12,200 --> 00:20:18,440
have a new mu and Sigma based on what we

00:20:15,980 --> 00:20:20,720
use to fit our data and in time see this

00:20:18,440 --> 00:20:22,610
is just a couple of lines I specify a

00:20:20,720 --> 00:20:25,520
grid of values or in this case a range

00:20:22,610 --> 00:20:29,059
of values over X I create a conditional

00:20:25,520 --> 00:20:31,370
Gaussian process to sample from then I

00:20:29,059 --> 00:20:33,860
just draw samples so sample PPC is just

00:20:31,370 --> 00:20:36,460
drawing samples from that value and I'm

00:20:33,860 --> 00:20:39,410
just gonna take three so here are three

00:20:36,460 --> 00:20:42,440
candidate values for the underlying

00:20:39,410 --> 00:20:44,720
function again realizations of GPS are

00:20:42,440 --> 00:20:46,700
functions so it's not a single value

00:20:44,720 --> 00:20:48,440
it's a continuous function and so that's

00:20:46,700 --> 00:20:50,990
three of them and you can see that they

00:20:48,440 --> 00:20:57,050
kind of vary a little bit particularly

00:20:50,990 --> 00:21:00,470
towards the end of the time series and

00:20:57,050 --> 00:21:02,270
if those particular samples were drawn

00:21:00,470 --> 00:21:03,800
without observation noise there's a

00:21:02,270 --> 00:21:06,500
little switch there so the pred nose

00:21:03,800 --> 00:21:08,240
argument in the first line can turn on

00:21:06,500 --> 00:21:09,890
and off prediction noise using that

00:21:08,240 --> 00:21:11,570
Sigma and so here I'm going to draw a

00:21:09,890 --> 00:21:13,940
thousand samples to give a better idea

00:21:11,570 --> 00:21:16,160
of kind of the range of values and we

00:21:13,940 --> 00:21:19,580
can plot those and so now you can see

00:21:16,160 --> 00:21:22,250
kind of a reasonable estimate of that

00:21:19,580 --> 00:21:23,270
function with kind of uncertainty bounds

00:21:22,250 --> 00:21:25,040
and that's one of the nice things about

00:21:23,270 --> 00:21:26,900
a Bayesian inference is that everything

00:21:25,040 --> 00:21:31,910
is in terms of probabilities and so you

00:21:26,900 --> 00:21:33,380
always have a pretty honest evaluation

00:21:31,910 --> 00:21:35,450
of the uncertainty associated with your

00:21:33,380 --> 00:21:36,950
estimates when you're done and again we

00:21:35,450 --> 00:21:39,830
could have used a polynomial to fit this

00:21:36,950 --> 00:21:41,120
maybe even a quadratic but there would

00:21:39,830 --> 00:21:43,340
have to do a lot of work thinking about

00:21:41,120 --> 00:21:45,500
what you know what what how exactly to

00:21:43,340 --> 00:21:47,030
parameterize this with the GP the truth

00:21:45,500 --> 00:21:51,020
is in there somewhere you just have to

00:21:47,030 --> 00:21:52,970
pull it out using your data another

00:21:51,020 --> 00:21:55,790
couple of examples so what about when we

00:21:52,970 --> 00:21:58,340
don't have normal data right it's easy

00:21:55,790 --> 00:22:00,770
if you've got normal normal normal well

00:21:58,340 --> 00:22:03,470
we often get data that are not normally

00:22:00,770 --> 00:22:07,370
distributed this is an example that we

00:22:03,470 --> 00:22:09,380
use in one of our example scripts in the

00:22:07,370 --> 00:22:11,480
PI MC code base here I'm gonna use it a

00:22:09,380 --> 00:22:12,559
little bit differently this is a time

00:22:11,480 --> 00:22:14,509
series of historical

00:22:12,559 --> 00:22:19,850
coal mining disasters in Britain from

00:22:14,509 --> 00:22:21,649
what 1851 to 1961 and typically we treat

00:22:19,850 --> 00:22:23,210
this with light at least in our example

00:22:21,649 --> 00:22:24,889
is kind of a switch point analysis we

00:22:23,210 --> 00:22:26,779
say that there's a constant mean at the

00:22:24,889 --> 00:22:28,279
beginning and then something changes and

00:22:26,779 --> 00:22:31,490
then it becomes lower some sort of

00:22:28,279 --> 00:22:33,499
safety provisions kicked in perhaps in

00:22:31,490 --> 00:22:35,269
the middle of the time series now I'm

00:22:33,499 --> 00:22:37,399
gonna use a GP for this to do it a

00:22:35,269 --> 00:22:38,899
little bit more flexibly so now this is

00:22:37,399 --> 00:22:40,610
what Bayes formula looks like for this

00:22:38,899 --> 00:22:42,499
GP I've got a Gaussian process prior

00:22:40,610 --> 00:22:44,690
like before but now my data is gonna be

00:22:42,499 --> 00:22:46,820
Poisson distributed and poised on is

00:22:44,690 --> 00:22:49,159
another distribution for discrete values

00:22:46,820 --> 00:22:50,659
that are typically used to model counts

00:22:49,159 --> 00:22:52,429
that's the reason I'm choosing it here

00:22:50,659 --> 00:22:54,740
and then out the other end I have a

00:22:52,429 --> 00:22:57,049
transformed Gaussian process I have to

00:22:54,740 --> 00:22:59,690
transform it now because it's not normal

00:22:57,049 --> 00:23:02,960
anymore and so the model looks more or

00:22:59,690 --> 00:23:04,279
less the same so we set it up exactly

00:23:02,960 --> 00:23:06,499
the same I'm using a quadratic

00:23:04,279 --> 00:23:08,779
exponential and I'm just gonna highlight

00:23:06,499 --> 00:23:10,429
what's different and it's just these two

00:23:08,779 --> 00:23:11,869
lines here now what I'm using is

00:23:10,429 --> 00:23:14,269
something called a latent Gaussian

00:23:11,869 --> 00:23:16,700
process because I'm modeling that latent

00:23:14,269 --> 00:23:18,080
mean the mean that you never observe you

00:23:16,700 --> 00:23:20,029
can't observe a mean right I can't

00:23:18,080 --> 00:23:22,519
observe a rate it's something that you

00:23:20,029 --> 00:23:24,740
infer and and then I'm gonna create a

00:23:22,519 --> 00:23:29,090
prior for that based on the data that

00:23:24,740 --> 00:23:31,700
I've observed and then I can add a

00:23:29,090 --> 00:23:33,320
Poisson likelihood and this is done in

00:23:31,700 --> 00:23:36,200
exactly the same way it would be done

00:23:33,320 --> 00:23:39,499
for any model in PI MC using the

00:23:36,200 --> 00:23:42,049
observed flag or the observed argument

00:23:39,499 --> 00:23:43,820
to pass the data in and then now I'm

00:23:42,049 --> 00:23:45,830
gonna use MCMC alright I haven't got a

00:23:43,820 --> 00:23:48,129
closed-form GP anymore because I have

00:23:45,830 --> 00:23:51,379
I'm not fully Gaussian but I'm gonna

00:23:48,129 --> 00:23:54,619
sample a thousand iterations tuning for

00:23:51,379 --> 00:23:56,629
2000 using an MC MC sampler called nuts

00:23:54,619 --> 00:23:58,909
called a No u-turn sampler and this is a

00:23:56,629 --> 00:24:00,889
very efficient sort of the state current

00:23:58,909 --> 00:24:03,190
state of the art and MCMC sampling and

00:24:00,889 --> 00:24:06,919
it's it's the default for continuous

00:24:03,190 --> 00:24:09,919
parameters in pi MC and here's what we

00:24:06,919 --> 00:24:13,669
get again a nice continuous function of

00:24:09,919 --> 00:24:15,169
the underlying mean width and you can

00:24:13,669 --> 00:24:17,600
see these are a thousand draws so you

00:24:15,169 --> 00:24:19,369
can see kind of how they vary you know

00:24:17,600 --> 00:24:20,809
it could be there's sort of less it's

00:24:19,369 --> 00:24:23,690
sort of less likely to be highly

00:24:20,809 --> 00:24:24,630
variable most of them are tend to be a

00:24:23,690 --> 00:24:26,970
kind of

00:24:24,630 --> 00:24:30,270
functions okay we're not overfitting

00:24:26,970 --> 00:24:33,770
here it's taking that the marginal

00:24:30,270 --> 00:24:36,840
likelihood takes into account the

00:24:33,770 --> 00:24:38,330
overfitting aspect of the model so you

00:24:36,840 --> 00:24:41,010
kind of get that regularization

00:24:38,330 --> 00:24:43,590
automatically here the other nice thing

00:24:41,010 --> 00:24:45,810
that you get with doing MCMC sampling is

00:24:43,590 --> 00:24:48,450
you get distributions for those hyper

00:24:45,810 --> 00:24:51,630
parameters so this is the uncertainty in

00:24:48,450 --> 00:24:53,190
the true underlying value of those of

00:24:51,630 --> 00:24:57,360
those two parameters for the quadratic

00:24:53,190 --> 00:24:59,910
exponential which is really nice a

00:24:57,360 --> 00:25:02,490
second example I mean this comes from

00:24:59,910 --> 00:25:04,920
some some of my own work I do some

00:25:02,490 --> 00:25:07,260
epidemiological research at Vanderbilt

00:25:04,920 --> 00:25:09,060
and this is from a reconstruction of a

00:25:07,260 --> 00:25:14,030
measles outbreak that occurred in Sao

00:25:09,060 --> 00:25:18,000
Paulo Brazil in 1997 and this is a

00:25:14,030 --> 00:25:20,450
different sort of messy data it's what

00:25:18,000 --> 00:25:22,680
happens here is that the the the the

00:25:20,450 --> 00:25:25,110
outbreak the measles outbreak gets

00:25:22,680 --> 00:25:26,370
monitored by recording cases as they

00:25:25,110 --> 00:25:30,480
come into clinics during the outbreak

00:25:26,370 --> 00:25:33,360
and measles is kind of a febrile illness

00:25:30,480 --> 00:25:36,390
they results in fever and rash and a lot

00:25:33,360 --> 00:25:38,160
of illnesses for young people

00:25:36,390 --> 00:25:40,200
particularly have fever and rash like

00:25:38,160 --> 00:25:42,260
rubella dengue and they can be

00:25:40,200 --> 00:25:44,940
misdiagnosed as measles so there's a

00:25:42,260 --> 00:25:46,800
misidentification of folks coming into

00:25:44,940 --> 00:25:51,180
the clinic kids coming into the clinic

00:25:46,800 --> 00:25:53,400
and and this can result in a biased

00:25:51,180 --> 00:25:56,160
assessment of the age classes at risk

00:25:53,400 --> 00:25:57,900
here and ultimately of the vaccination

00:25:56,160 --> 00:26:01,650
coverage necessary to limit the outbreak

00:25:57,900 --> 00:26:03,210
and so we see here are values of cases

00:26:01,650 --> 00:26:05,040
that were confirmed versus those that

00:26:03,210 --> 00:26:07,590
were unconfirmed and we're going to want

00:26:05,040 --> 00:26:09,660
to use this to correct for the true

00:26:07,590 --> 00:26:11,820
underlying number of cases in the

00:26:09,660 --> 00:26:14,160
population so here what we have is kind

00:26:11,820 --> 00:26:15,720
of a binomial situation right what we're

00:26:14,160 --> 00:26:20,400
trying to estimate the bias in

00:26:15,720 --> 00:26:22,740
confirmation so again I'm age something

00:26:20,400 --> 00:26:25,560
that varies by age is very commonly

00:26:22,740 --> 00:26:28,280
nonlinear and in this case it certainly

00:26:25,560 --> 00:26:31,530
is so here's a confirmation model then

00:26:28,280 --> 00:26:34,440
as a GP again exactly the same as before

00:26:31,530 --> 00:26:38,490
but now rather than Poisson or Gaussian

00:26:34,440 --> 00:26:40,500
data we have binomial data so I'm taking

00:26:38,490 --> 00:26:42,330
i gaussian process here and I'm

00:26:40,500 --> 00:26:44,460
transforming it using an inverse logic

00:26:42,330 --> 00:26:46,110
transformation which changes it from the

00:26:44,460 --> 00:26:48,450
real line where the where the normal

00:26:46,110 --> 00:26:49,440
distribution is defined to a zero one

00:26:48,450 --> 00:26:51,270
interval because we're modeling

00:26:49,440 --> 00:26:55,830
probabilities here and now we have a

00:26:51,270 --> 00:26:57,780
binomial likelihood okay and there's all

00:26:55,830 --> 00:26:59,790
here it's quite nice because it's an

00:26:57,780 --> 00:27:02,400
it's a difficult problem because there

00:26:59,790 --> 00:27:03,840
are lots and lots of cases at the really

00:27:02,400 --> 00:27:06,090
young age groups you tend to be young

00:27:03,840 --> 00:27:07,590
when you get measles so the the dots for

00:27:06,090 --> 00:27:10,680
the data here are proportional to the

00:27:07,590 --> 00:27:13,290
log of the number of cases and so that

00:27:10,680 --> 00:27:15,750
that estimates the intervals around the

00:27:13,290 --> 00:27:20,130
estimates are very tight that's younger

00:27:15,750 --> 00:27:21,870
ages but then as you get into you know

00:27:20,130 --> 00:27:23,610
if age 50 and Beyond where very few

00:27:21,870 --> 00:27:25,380
people get measles the uncertainty

00:27:23,610 --> 00:27:31,230
increases but we're still able to make

00:27:25,380 --> 00:27:34,290
predictions in that in that domain so

00:27:31,230 --> 00:27:35,940
it's not all happiness in sunshine with

00:27:34,290 --> 00:27:38,160
Gaussian processes they are very

00:27:35,940 --> 00:27:41,100
powerful useful tools I use them a lot

00:27:38,160 --> 00:27:42,990
but they are a little bit limited in

00:27:41,100 --> 00:27:46,170
that they don't tend to scale very well

00:27:42,990 --> 00:27:49,530
out of the box with large datasets this

00:27:46,170 --> 00:27:51,420
is not a big data analysis tool and the

00:27:49,530 --> 00:27:54,780
problem is and sorry this may be your

00:27:51,420 --> 00:27:58,470
last equation the problem has to do with

00:27:54,780 --> 00:28:01,470
this this is the formula that shows the

00:27:58,470 --> 00:28:03,950
posterior covariance function and in

00:28:01,470 --> 00:28:07,320
particular over here what we have is a

00:28:03,950 --> 00:28:12,090
essentially an X by X and a matrix of

00:28:07,320 --> 00:28:13,230
the data that we're inverting and that

00:28:12,090 --> 00:28:16,410
doesn't that's not a very fast

00:28:13,230 --> 00:28:19,950
operations so so it's in fact it's cubic

00:28:16,410 --> 00:28:24,360
in compute time and and quadratic in

00:28:19,950 --> 00:28:26,070
memory and so what we we're not at a

00:28:24,360 --> 00:28:29,850
complete loss of what to do here what we

00:28:26,070 --> 00:28:31,470
can do is approximate so what we're

00:28:29,850 --> 00:28:34,350
going to do here is do what's called a

00:28:31,470 --> 00:28:35,970
sparse approximation which involves

00:28:34,350 --> 00:28:39,390
selecting a subset of the training

00:28:35,970 --> 00:28:41,460
points size Big M if you like that's

00:28:39,390 --> 00:28:44,640
that's much smaller than big n the size

00:28:41,460 --> 00:28:46,110
of the data and this we based our

00:28:44,640 --> 00:28:47,820
computation on that and of course this

00:28:46,110 --> 00:28:49,230
reduces the expressiveness of the

00:28:47,820 --> 00:28:51,900
Gaussian process we're not it's not

00:28:49,230 --> 00:28:52,800
going to be as good as before but some

00:28:51,900 --> 00:28:54,960
problems simply

00:28:52,800 --> 00:28:59,400
even be approached without without doing

00:28:54,960 --> 00:29:01,380
this so a quick example for this this is

00:28:59,400 --> 00:29:04,620
some data that's freely available online

00:29:01,380 --> 00:29:06,540
is the finishers of various runnings of

00:29:04,620 --> 00:29:08,840
the Boston Marathon I'm using the 2015

00:29:06,540 --> 00:29:11,760
data set here so this is 20 almost

00:29:08,840 --> 00:29:16,470
27,000 finishers so that matrix would be

00:29:11,760 --> 00:29:18,960
a 27,000 by 27,000 matrix it's it's not

00:29:16,470 --> 00:29:20,220
you know you can't use a standard GP for

00:29:18,960 --> 00:29:22,710
that so this is what the data looks like

00:29:20,220 --> 00:29:25,200
and we want to ask to me you know I mean

00:29:22,710 --> 00:29:26,340
this and and again it's not exactly a

00:29:25,200 --> 00:29:27,810
straight line it's kind of hard to

00:29:26,340 --> 00:29:31,470
interpret here because of the density of

00:29:27,810 --> 00:29:33,420
the points but the the GP looks again

00:29:31,470 --> 00:29:36,000
the implementation looks the same the

00:29:33,420 --> 00:29:39,120
API is exactly the same except now I'm

00:29:36,000 --> 00:29:40,470
using here you'll notice the GP is a

00:29:39,120 --> 00:29:42,330
marginal sparse

00:29:40,470 --> 00:29:44,070
so now we have to give it some

00:29:42,330 --> 00:29:47,460
approximations and you can you can

00:29:44,070 --> 00:29:49,380
specify the approximation type this

00:29:47,460 --> 00:29:51,480
one's called a fitzy a fully independent

00:29:49,380 --> 00:29:53,490
training conditional approximation there

00:29:51,480 --> 00:29:57,030
are two others they're just different

00:29:53,490 --> 00:29:58,880
ways of doing it fit the fitzy can tend

00:29:57,030 --> 00:30:00,960
to underestimate the noise variance

00:29:58,880 --> 00:30:02,790
which I'm not too concerned about in

00:30:00,960 --> 00:30:05,580
this particular example some of the

00:30:02,790 --> 00:30:07,560
others will will overestimate the noise

00:30:05,580 --> 00:30:08,700
variance again there's no free lunch and

00:30:07,560 --> 00:30:10,050
machine learning

00:30:08,700 --> 00:30:11,850
if we're gonna approximate we're gonna

00:30:10,050 --> 00:30:15,960
make a sacrifice somewhere and that's

00:30:11,850 --> 00:30:19,230
where it is but it can very quickly be

00:30:15,960 --> 00:30:21,750
fit in in PMC this take took about a

00:30:19,230 --> 00:30:24,360
minute and a half to run and again very

00:30:21,750 --> 00:30:26,010
tight bounds here on the uncertainty

00:30:24,360 --> 00:30:28,080
because there are so many runners but as

00:30:26,010 --> 00:30:29,070
you get you know beyond 80 years old

00:30:28,080 --> 00:30:32,340
there aren't as many people running

00:30:29,070 --> 00:30:33,660
marathons and so the you know they're

00:30:32,340 --> 00:30:35,670
probably not gonna drop back down to

00:30:33,660 --> 00:30:38,160
three hour marathons but you know that's

00:30:35,670 --> 00:30:40,140
how he that's where prior specification

00:30:38,160 --> 00:30:41,970
will help us and then the red points

00:30:40,140 --> 00:30:44,370
here are where those approximations go

00:30:41,970 --> 00:30:47,100
and by default it pi MC will use k-means

00:30:44,370 --> 00:30:53,490
to determine the optimal location of

00:30:47,100 --> 00:30:54,900
those of those points my last example so

00:30:53,490 --> 00:30:56,480
far I've just talked about kind of time

00:30:54,900 --> 00:30:59,460
series

00:30:56,480 --> 00:31:02,370
applications but of course we often have

00:30:59,460 --> 00:31:04,910
multi-dimensional data and in particular

00:31:02,370 --> 00:31:06,450
you know spatial analyses are quite

00:31:04,910 --> 00:31:08,670
approachable using

00:31:06,450 --> 00:31:09,840
some processes so the example I'm going

00:31:08,670 --> 00:31:12,270
to use here actually comes from a

00:31:09,840 --> 00:31:15,180
textbook but it's it's still pretty

00:31:12,270 --> 00:31:17,340
messy data set this is from Isaac

00:31:15,180 --> 00:31:20,310
Isaac's and sugar stavos applied

00:31:17,340 --> 00:31:23,370
geostatistics from 1989 one of our

00:31:20,310 --> 00:31:26,940
lapsed PI MC developers from PI at the

00:31:23,370 --> 00:31:29,490
time see two days used one of these in

00:31:26,940 --> 00:31:33,060
in the examples from an earlier version

00:31:29,490 --> 00:31:35,190
I'm going to use it here this is XY

00:31:33,060 --> 00:31:39,210
location geospatial locations of

00:31:35,190 --> 00:31:40,410
essentially geological samples of three

00:31:39,210 --> 00:31:43,680
different things and they're not even

00:31:40,410 --> 00:31:46,080
identified in the in the textbook

00:31:43,680 --> 00:31:49,200
they're just substance V substance use

00:31:46,080 --> 00:31:51,180
substance T two of them are our floating

00:31:49,200 --> 00:31:54,540
point values one of them is integer

00:31:51,180 --> 00:31:57,660
values so I'm gonna use substance V here

00:31:54,540 --> 00:31:59,070
whatever that is and these are spatial

00:31:57,660 --> 00:32:02,130
samples so what what looks like is

00:31:59,070 --> 00:32:04,230
happening here is that they took samples

00:32:02,130 --> 00:32:06,420
on a more or less regular grid and then

00:32:04,230 --> 00:32:08,610
when they found sort of positive values

00:32:06,420 --> 00:32:10,620
they did a little bit more intensive

00:32:08,610 --> 00:32:13,740
local sampling so it's a very uneven

00:32:10,620 --> 00:32:16,650
grid so the lighter the yellower the

00:32:13,740 --> 00:32:19,950
color the higher the values of V are the

00:32:16,650 --> 00:32:22,800
purple values are essentially zeros so

00:32:19,950 --> 00:32:26,760
how do we do this in PI MC 3 with the GP

00:32:22,800 --> 00:32:30,410
module it's very very easy the only

00:32:26,760 --> 00:32:32,160
thing I'm changing this is examine

00:32:30,410 --> 00:32:35,070
covariance function remember the one

00:32:32,160 --> 00:32:36,600
that was slightly more jagged just to

00:32:35,070 --> 00:32:38,900
allow a little bit more flexibility it

00:32:36,600 --> 00:32:41,850
won't smooth things out as much and then

00:32:38,900 --> 00:32:44,220
notice now I have a two for the first

00:32:41,850 --> 00:32:45,660
argument rather than a one I didn't

00:32:44,220 --> 00:32:47,670
explain what that was before the one

00:32:45,660 --> 00:32:52,020
just said one dimension essentially now

00:32:47,670 --> 00:32:54,960
I've got a two dimensional GP etc and

00:32:52,020 --> 00:32:57,750
that runs as usual I can I can optimize

00:32:54,960 --> 00:33:00,170
that it's a sparse marginal again

00:32:57,750 --> 00:33:02,640
because it's a fairly large dataset and

00:33:00,170 --> 00:33:04,290
then the conditional what we're

00:33:02,640 --> 00:33:06,210
interested in now is like a surface

00:33:04,290 --> 00:33:08,400
essentially right we're a kind of

00:33:06,210 --> 00:33:10,080
approximate a circuit a surface in fact

00:33:08,400 --> 00:33:12,210
in geostatistics this is a well known

00:33:10,080 --> 00:33:13,830
procedure called Creegan you can show

00:33:12,210 --> 00:33:15,930
that creaking is essentially a special

00:33:13,830 --> 00:33:18,330
case of Gaussian processes and so what

00:33:15,930 --> 00:33:19,580
I've done here is I've created a using

00:33:18,330 --> 00:33:22,820
numpy a

00:33:19,580 --> 00:33:24,470
grid regular grid over the whole area

00:33:22,820 --> 00:33:26,900
and then I'm just running that through

00:33:24,470 --> 00:33:29,990
the conditional Gaussian process and you

00:33:26,900 --> 00:33:31,670
get a nice surface which is great and

00:33:29,990 --> 00:33:33,530
you could get uncertainties associated

00:33:31,670 --> 00:33:38,360
with this as well of course because it's

00:33:33,530 --> 00:33:40,340
fully probabilistic so so there's more

00:33:38,360 --> 00:33:41,270
it could have kept going but we're

00:33:40,340 --> 00:33:42,950
running out of time

00:33:41,270 --> 00:33:45,620
you can use Gaussian processes for

00:33:42,950 --> 00:33:47,540
classification something called ard

00:33:45,620 --> 00:33:50,000
which is essentially feature selection

00:33:47,540 --> 00:33:51,680
automatic relevance determination you

00:33:50,000 --> 00:33:53,390
can show that there that neural networks

00:33:51,680 --> 00:33:55,130
are a special case of GP deep neural

00:33:53,390 --> 00:33:58,540
networks you can use them for

00:33:55,130 --> 00:34:00,950
reinforcement learning lots of stuff

00:33:58,540 --> 00:34:02,810
there are advantages to using them the

00:34:00,950 --> 00:34:05,690
reason I use them is that they're kind

00:34:02,810 --> 00:34:08,180
of a mindless way of doing mindless and

00:34:05,690 --> 00:34:09,860
a good way of doing nonlinear regression

00:34:08,180 --> 00:34:12,320
you don't have to really think about the

00:34:09,860 --> 00:34:15,380
underlying process but you can recover

00:34:12,320 --> 00:34:16,850
almost any arbitrary continuous

00:34:15,380 --> 00:34:19,220
functions using that way it's a fully

00:34:16,850 --> 00:34:20,930
probabilistic setup and so your

00:34:19,220 --> 00:34:23,030
predictions will have uncertainties

00:34:20,930 --> 00:34:25,220
associated with them the hyper

00:34:23,030 --> 00:34:26,050
parameters are very interpretable link

00:34:25,220 --> 00:34:28,910
scales

00:34:26,050 --> 00:34:31,370
variances and so on and it's very easy

00:34:28,910 --> 00:34:33,230
to automate you can combine I didn't

00:34:31,370 --> 00:34:35,630
show that this in our in our any of the

00:34:33,230 --> 00:34:37,730
examples but you can have a system that

00:34:35,630 --> 00:34:40,340
has multiple covariance functions that

00:34:37,730 --> 00:34:42,860
model different parts of the time series

00:34:40,340 --> 00:34:45,320
process or the spatial process and of

00:34:42,860 --> 00:34:46,700
course the limitations are outlined here

00:34:45,320 --> 00:34:49,190
mostly have to do with the fact that

00:34:46,700 --> 00:34:52,670
they don't scale well without a little

00:34:49,190 --> 00:34:55,460
bit of help most of what you saw today

00:34:52,670 --> 00:34:57,290
is due to the work of one person bill

00:34:55,460 --> 00:34:59,510
angles who last year was one of our

00:34:57,290 --> 00:35:01,280
Google Summer of Code students and he

00:34:59,510 --> 00:35:02,960
cranked this out more or less in one

00:35:01,280 --> 00:35:04,700
summer extremely impressive and he's

00:35:02,960 --> 00:35:06,830
gonna do it again this summer he's going

00:35:04,700 --> 00:35:08,780
to improve things even more and

00:35:06,830 --> 00:35:10,730
particularly on the Big Data side of

00:35:08,780 --> 00:35:13,010
things so we're looking forward to more

00:35:10,730 --> 00:35:15,890
there and I just want to say thanks to

00:35:13,010 --> 00:35:18,920
the entire PI MC team which includes

00:35:15,890 --> 00:35:21,290
Bill and includes Eric here and for

00:35:18,920 --> 00:35:24,590
making it a tremendous community-based

00:35:21,290 --> 00:35:26,120
package now so it's really worth

00:35:24,590 --> 00:35:28,070
spending a lot of time to produce good

00:35:26,120 --> 00:35:29,810
software and support it and and the team

00:35:28,070 --> 00:35:34,260
works very hard to do that so I hope you

00:35:29,810 --> 00:35:36,299
will check out Pomp's III and the GPS

00:35:34,260 --> 00:35:44,910
when you get the chance I'm happy to

00:35:36,299 --> 00:35:47,760
take any questions alright so we're

00:35:44,910 --> 00:35:50,099
going to enter the Q&A period just a few

00:35:47,760 --> 00:35:51,839
rules of thumbs first off if you've got

00:35:50,099 --> 00:35:54,000
a question please keep it short please

00:35:51,839 --> 00:35:58,289
keep it concise ask one question don't

00:35:54,000 --> 00:36:00,480
ask to part one question and don't

00:35:58,289 --> 00:36:01,859
phrase your question as a comment all

00:36:00,480 --> 00:36:03,660
right so with that I'd like to take the

00:36:01,859 --> 00:36:06,029
first question from the front and then

00:36:03,660 --> 00:36:07,529
we'll go to the back their rules okay

00:36:06,029 --> 00:36:09,930
thank you very much for the interesting

00:36:07,529 --> 00:36:12,809
introduction but actually I'm not quite

00:36:09,930 --> 00:36:14,940
uh I don't have a clear idea of that

00:36:12,809 --> 00:36:17,670
about the Gaussian processes you

00:36:14,940 --> 00:36:20,279
explained earlier I wonder is it like

00:36:17,670 --> 00:36:23,279
you try to do some kernel regression

00:36:20,279 --> 00:36:26,849
using Gaussian kernel or E or I

00:36:23,279 --> 00:36:30,119
misunderstood it you can show that

00:36:26,849 --> 00:36:31,619
Gaussian well you can show that kernel

00:36:30,119 --> 00:36:33,420
regressions could you can derive

00:36:31,619 --> 00:36:36,569
Gaussian processes from kernel

00:36:33,420 --> 00:36:38,400
regression but kind of like splines as

00:36:36,569 --> 00:36:40,140
well but you don't have to pre specify

00:36:38,400 --> 00:36:42,180
the points so you're essentially

00:36:40,140 --> 00:36:44,609
modeling the whole process as a infinite

00:36:42,180 --> 00:36:47,910
dimensional problem and because of that

00:36:44,609 --> 00:36:50,579
conditioning property we're able to

00:36:47,910 --> 00:36:51,900
focus our attention on just the points

00:36:50,579 --> 00:36:53,190
that we've observed or the ones that we

00:36:51,900 --> 00:36:56,130
want to predict and we can forget about

00:36:53,190 --> 00:36:59,130
the rest of the real line okay so it's

00:36:56,130 --> 00:37:00,569
over generalization or marginalization

00:36:59,130 --> 00:37:02,220
or conditioning depending on the

00:37:00,569 --> 00:37:04,140
operation that you're doing I see or

00:37:02,220 --> 00:37:06,240
it's like in terms of the infinite space

00:37:04,140 --> 00:37:08,700
what what space are you referring to is

00:37:06,240 --> 00:37:10,470
it like functional space or do referring

00:37:08,700 --> 00:37:13,589
to the original space yeah okay

00:37:10,470 --> 00:37:15,029
functional space okay we're gonna stop

00:37:13,589 --> 00:37:16,049
that question there if you have further

00:37:15,029 --> 00:37:18,660
questions please talk to Chris

00:37:16,049 --> 00:37:21,900
afterwards back there how does this

00:37:18,660 --> 00:37:25,950
apply to design of experiments for

00:37:21,900 --> 00:37:27,089
looking at the results modeling them I'm

00:37:25,950 --> 00:37:28,650
not sure you'd use it for the design

00:37:27,089 --> 00:37:31,440
this would be more from model-based

00:37:28,650 --> 00:37:33,390
inference so you could design

00:37:31,440 --> 00:37:35,970
experiments that result in say a surface

00:37:33,390 --> 00:37:38,010
or something that is varying with some

00:37:35,970 --> 00:37:40,829
variable like age and so this is a

00:37:38,010 --> 00:37:43,650
flexible way of building that model so

00:37:40,829 --> 00:37:46,410
you could have a clinical trial or

00:37:43,650 --> 00:37:48,660
something where you have candidates of

00:37:46,410 --> 00:37:50,420
Jex of different age and you may want to

00:37:48,660 --> 00:37:53,220
model or you may want to express how

00:37:50,420 --> 00:37:54,960
some particular outcome varies as a

00:37:53,220 --> 00:37:59,490
function of age and I'm just using that

00:37:54,960 --> 00:38:01,020
because that's commonly nonlinear that's

00:37:59,490 --> 00:38:03,930
so what would what would you think would

00:38:01,020 --> 00:38:06,569
happen if we took some state-of-the-art

00:38:03,930 --> 00:38:08,190
deep learning models these these you

00:38:06,569 --> 00:38:10,680
know deep convolutional neural networks

00:38:08,190 --> 00:38:13,859
that can can sometimes have millions of

00:38:10,680 --> 00:38:17,069
parameters and reduce the parametric

00:38:13,859 --> 00:38:20,130
complexity and instead replace it by

00:38:17,069 --> 00:38:22,200
modeling some of those parameters in a

00:38:20,130 --> 00:38:24,900
Bayesian fashion what what do you think

00:38:22,200 --> 00:38:27,930
the trade-off is is there in terms of

00:38:24,900 --> 00:38:29,339
the the usefulness of the model well

00:38:27,930 --> 00:38:31,170
anytime you're going into a bayesian

00:38:29,339 --> 00:38:33,930
context you're worried about uncertainty

00:38:31,170 --> 00:38:38,730
and so there are some good examples in

00:38:33,930 --> 00:38:41,940
in on the privacy docs about Bayesian

00:38:38,730 --> 00:38:43,710
deep neural networks where you assign

00:38:41,940 --> 00:38:45,690
priors to the weights in the neural

00:38:43,710 --> 00:38:47,400
network and and so you can get

00:38:45,690 --> 00:38:48,780
uncertainty about your predictions and

00:38:47,400 --> 00:38:50,579
that's where I would cross over there

00:38:48,780 --> 00:38:53,069
are deep Gaussian processes that are

00:38:50,579 --> 00:38:55,470
used particularly in the reinforcement

00:38:53,069 --> 00:38:58,079
learning space and so they those worlds

00:38:55,470 --> 00:39:01,380
do intersect a little bit you're never

00:38:58,079 --> 00:39:02,789
quite leaving the parametric space but

00:39:01,380 --> 00:39:05,640
sometimes you are interested in

00:39:02,789 --> 00:39:08,250
interpreting some of those and and so we

00:39:05,640 --> 00:39:12,180
hold onto them all right any other

00:39:08,250 --> 00:39:14,400
questions back there the pi MC dot MC

00:39:12,180 --> 00:39:17,279
stands for Monte Carlo or a Markov chain

00:39:14,400 --> 00:39:20,099
and the second question is any change to

00:39:17,279 --> 00:39:22,380
update the model based on the new data

00:39:20,099 --> 00:39:27,119
point not observe based on the new what

00:39:22,380 --> 00:39:30,599
sorry the new data are observed how to

00:39:27,119 --> 00:39:31,910
update the model oh oh se so after you

00:39:30,599 --> 00:39:35,279
fit the model updated with new

00:39:31,910 --> 00:39:37,319
observations yeah so first part of the

00:39:35,279 --> 00:39:39,960
question what is the MC stand for I get

00:39:37,319 --> 00:39:42,210
it could be it either when I thought

00:39:39,960 --> 00:39:44,579
about calling up PI MC MC but it sounded

00:39:42,210 --> 00:39:47,760
a little awkward so pi MC so it can be

00:39:44,579 --> 00:39:50,400
MC or MC updating a bayesian model

00:39:47,760 --> 00:39:51,779
that's a tricky question you know with a

00:39:50,400 --> 00:39:53,520
Gaussian process it's great because you

00:39:51,779 --> 00:39:55,319
can you can take that posterior and make

00:39:53,520 --> 00:39:56,609
it the prior the next time and add more

00:39:55,319 --> 00:39:59,609
data to it I think that's what you're

00:39:56,609 --> 00:40:01,289
referring to so you could do that you

00:39:59,609 --> 00:40:02,849
there isn't an automated way of doing

00:40:01,289 --> 00:40:05,640
that in privacy right now because you

00:40:02,849 --> 00:40:07,380
build the static graph ends up being a

00:40:05,640 --> 00:40:09,269
theano graph and and so you would have

00:40:07,380 --> 00:40:11,969
to manually take your posterior from one

00:40:09,269 --> 00:40:14,670
model and add it and and add it as a

00:40:11,969 --> 00:40:15,900
prior for your next model but in

00:40:14,670 --> 00:40:17,369
principle you can do it and that's

00:40:15,900 --> 00:40:19,319
really the power one of the powerful

00:40:17,369 --> 00:40:20,969
things about Bayes is that you can keep

00:40:19,319 --> 00:40:22,950
turning the Bayesian crank and learning

00:40:20,969 --> 00:40:24,479
learn more and more about it but you

00:40:22,950 --> 00:40:26,549
would have to take those posteriors and

00:40:24,479 --> 00:40:28,799
turn them back into priors again so in

00:40:26,549 --> 00:40:30,509
these case I'm sorry we're gonna leave

00:40:28,799 --> 00:40:33,209
it as for fairness for other people

00:40:30,509 --> 00:40:36,599
would like to ask questions next please

00:40:33,209 --> 00:40:39,089
I missed this but is it possible to use

00:40:36,599 --> 00:40:44,660
this method to model multiple outputs

00:40:39,089 --> 00:40:47,579
like multiple dimensions right yeah yeah

00:40:44,660 --> 00:40:49,440
in principle yes but you cannot I'm not

00:40:47,579 --> 00:40:51,450
in PMC right now you don't get about a

00:40:49,440 --> 00:40:54,959
multivariate up so where why is

00:40:51,450 --> 00:40:56,729
multivariate yeah yeah yeah in principle

00:40:54,959 --> 00:40:59,459
you can but it's you'd have to do it by

00:40:56,729 --> 00:41:02,309
hand in PI MC okay and it wouldn't be

00:40:59,459 --> 00:41:04,709
easy thank you yeah next question back

00:41:02,309 --> 00:41:06,509
there for poems see when you're

00:41:04,709 --> 00:41:07,979
computing the inverse of the matrix are

00:41:06,509 --> 00:41:10,259
there other numerical methods

00:41:07,979 --> 00:41:13,369
implemented that you would typically see

00:41:10,259 --> 00:41:16,589
in like partial differential equations

00:41:13,369 --> 00:41:19,949
comps yeah we have different decomps we

00:41:16,589 --> 00:41:21,539
have chill Eskie decompositions and i

00:41:19,949 --> 00:41:22,469
can't remember the name of the other one

00:41:21,539 --> 00:41:25,759
do you remember the name of the other

00:41:22,469 --> 00:41:29,219
decomposition that it uses ljk

00:41:25,759 --> 00:41:30,449
decomposition so that's where I think a

00:41:29,219 --> 00:41:34,079
lot of the research this summer is going

00:41:30,449 --> 00:41:36,299
to go to making those faster and and and

00:41:34,079 --> 00:41:39,329
possibly leveraging stuff in in the

00:41:36,299 --> 00:41:41,190
future tensor flow for doing some of

00:41:39,329 --> 00:41:44,180
that more efficiently and perhaps using

00:41:41,190 --> 00:41:49,380
the GPU thank you

00:41:44,180 --> 00:41:51,329
next question kanpai MTV use can the

00:41:49,380 --> 00:41:54,509
Gaussian process be used for hydros to

00:41:51,329 --> 00:41:56,459
get a Scholastic errors yeah you just

00:41:54,509 --> 00:41:58,979
have to so and the noise that I was

00:41:56,459 --> 00:42:02,160
passing in there was a sigma a scalar

00:41:58,979 --> 00:42:05,279
Sigma so diagonal covariance matrix and

00:42:02,160 --> 00:42:07,589
you can certainly make it more

00:42:05,279 --> 00:42:11,489
structured than that yes by passing in a

00:42:07,589 --> 00:42:12,570
matrix rather than a scalar value all

00:42:11,489 --> 00:42:17,240
right do we have any other

00:42:12,570 --> 00:42:19,650
questions going once going twice sold

00:42:17,240 --> 00:42:20,750
alright thank you Chris for a lot of

00:42:19,650 --> 00:42:24,369
great talk

00:42:20,750 --> 00:42:24,369

YouTube URL: https://www.youtube.com/watch?v=-sIOMs4MSuA


