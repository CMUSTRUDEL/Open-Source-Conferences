Title: Titus Brown: Data intensive biology in the cloud: instrumenting ALL the things - PyCon 2014
Publication date: 2014-04-24
Playlist: PyCon 2014
Description: 
	Speaker: Titus Brown

"Cloud computing offers some great opportunities for science, but most cloud computing platforms are I/O and memory limited, and hence are poor matches for data-intensive computing. After 4 years of research software development we are now instrumenting and benchmarking our analysis pipelines; numbers, lessons learned, and future plans will be discussed. Everything is open source."

Slides can be found at: https://speakerdeck.com/pycon2014 and https://github.com/PyCon/2014-slides
Captions: 
	00:00:06,259 --> 00:00:14,009
please take a seat we're about to make a

00:00:08,610 --> 00:00:17,010
start and thank you everyone for coming

00:00:14,009 --> 00:00:20,150
along to this second session after lunch

00:00:17,010 --> 00:00:23,220
here at PyCon 2014 here in Montreal a

00:00:20,150 --> 00:00:26,000
next presenter has a PhD in

00:00:23,220 --> 00:00:29,369
developmental biology from Caltech and

00:00:26,000 --> 00:00:30,420
he says he likes Python a lot he

00:00:29,369 --> 00:00:32,790
currently works at Michigan State

00:00:30,420 --> 00:00:35,550
University and today is going to talk

00:00:32,790 --> 00:00:37,530
about data intensive biology in the

00:00:35,550 --> 00:00:47,039
cloud instrumenting all the things

00:00:37,530 --> 00:00:50,340
please welcome Titus Brown right all

00:00:47,039 --> 00:00:51,629
right thank you all for coming so I

00:00:50,340 --> 00:00:54,960
wanted to start with a few upfront

00:00:51,629 --> 00:00:56,489
definitions first of all there's a lot

00:00:54,960 --> 00:00:58,289
of confusion about what big data means

00:00:56,489 --> 00:00:59,699
and I'd like to point out as far as I'm

00:00:58,289 --> 00:01:01,859
concerned it means whatever is still

00:00:59,699 --> 00:01:03,420
inconvenient to compute upon if you can

00:01:01,859 --> 00:01:06,119
do the computation easily it's no longer

00:01:03,420 --> 00:01:07,650
big data data scientist you may have

00:01:06,119 --> 00:01:11,270
seen this before it's a statistician who

00:01:07,650 --> 00:01:14,490
lives in San Francisco and a professor

00:01:11,270 --> 00:01:16,170
was able to find earlier earlier today

00:01:14,490 --> 00:01:17,790
by fernando pÃ©rez someone who writes

00:01:16,170 --> 00:01:23,729
grants to fund the people who do the

00:01:17,790 --> 00:01:24,960
work and so i am a professor and not a

00:01:23,729 --> 00:01:27,869
data scientist because i live in

00:01:24,960 --> 00:01:31,229
michigan and i write grants so that

00:01:27,869 --> 00:01:32,850
others can do data intensive biology i'd

00:01:31,229 --> 00:01:35,490
also like to dedicate this talk to Terry

00:01:32,850 --> 00:01:37,710
peppers so Terry is a friend who helps

00:01:35,490 --> 00:01:38,880
run the testing and Python birds of a

00:01:37,710 --> 00:01:41,070
feather he couldn't be here this year

00:01:38,880 --> 00:01:43,439
i'm in for the last five years as I've

00:01:41,070 --> 00:01:46,200
taken on my faculty position he's um

00:01:43,439 --> 00:01:48,000
he's progressively grown less

00:01:46,200 --> 00:01:50,280
understanding of what it is that I do um

00:01:48,000 --> 00:01:52,290
and so every year it's like what I

00:01:50,280 --> 00:01:54,420
didn't understood even less of your talk

00:01:52,290 --> 00:01:57,060
this year than I did last year so it

00:01:54,420 --> 00:01:58,380
struck me that this up this winter we

00:01:57,060 --> 00:02:02,130
were stuck I don't know how many of you

00:01:58,380 --> 00:02:03,750
live in the North the frozen wastes this

00:02:02,130 --> 00:02:05,130
year we had a lot of snow in Michigan

00:02:03,750 --> 00:02:07,530
and so I spent a lot of time indoors

00:02:05,130 --> 00:02:09,509
with my six-year-old doing puzzles and

00:02:07,530 --> 00:02:11,190
and it struck me that she was asking the

00:02:09,509 --> 00:02:13,050
same question that Terry was asking me

00:02:11,190 --> 00:02:13,830
and I figured that if I could explain

00:02:13,050 --> 00:02:15,660
what I do it

00:02:13,830 --> 00:02:19,740
work to my six-year-old probably Terry

00:02:15,660 --> 00:02:21,570
might understand it also so what I do is

00:02:19,740 --> 00:02:24,180
I actually assemble puzzles for a living

00:02:21,570 --> 00:02:25,470
and and I told this to my six-year-old

00:02:24,180 --> 00:02:27,150
we're actually working on a puzzle and

00:02:25,470 --> 00:02:29,760
she looked at me was sort of wide-eyed

00:02:27,150 --> 00:02:32,130
wonder and said you actually get paid to

00:02:29,760 --> 00:02:33,990
do that I said well it's a little more

00:02:32,130 --> 00:02:35,610
complicated I strategize about solving

00:02:33,990 --> 00:02:37,290
multi-dimensional puzzles with billions

00:02:35,610 --> 00:02:41,280
of pieces and no picture on the box but

00:02:37,290 --> 00:02:43,410
it's still solving puzzles so if you if

00:02:41,280 --> 00:02:46,560
I give you many many many many many

00:02:43,410 --> 00:02:48,060
pieces of a puzzle and ask you to come

00:02:46,560 --> 00:02:49,530
up with a strategy there's sort of three

00:02:48,060 --> 00:02:51,510
basic strategies that you could use

00:02:49,530 --> 00:02:52,680
these are all strategy used in genome

00:02:51,510 --> 00:02:54,990
assembly which is what I actually work

00:02:52,680 --> 00:02:56,610
on and the three strategies are a greedy

00:02:54,990 --> 00:02:58,080
strategy where you say hey this piece

00:02:56,610 --> 00:03:00,810
sort of fits this piece let's mash them

00:02:58,080 --> 00:03:02,880
together which has some obvious flaws as

00:03:00,810 --> 00:03:05,070
my six-year-old has found out um N

00:03:02,880 --> 00:03:06,330
squared do these two pieces match how

00:03:05,070 --> 00:03:08,700
about these two pieces how about these

00:03:06,330 --> 00:03:10,530
two pieces and then the Dutch approach

00:03:08,700 --> 00:03:11,850
and I figure for the for Pike on the

00:03:10,530 --> 00:03:13,830
Dutch approach is obviously going to be

00:03:11,850 --> 00:03:15,300
the right one and so I thought I'd try

00:03:13,830 --> 00:03:16,650
and explain it this way so the Dutch

00:03:15,300 --> 00:03:18,989
approach is also known as de Brian

00:03:16,650 --> 00:03:20,850
assembly and the idea behind it is that

00:03:18,989 --> 00:03:23,010
what you do is you you decompose each

00:03:20,850 --> 00:03:24,989
puzzle piece down into small patches and

00:03:23,010 --> 00:03:26,700
then you look for similarities among

00:03:24,989 --> 00:03:28,320
those patches and essentially with these

00:03:26,700 --> 00:03:30,209
patches that you decompose things into

00:03:28,320 --> 00:03:32,190
can be easily hashed and compared in a

00:03:30,209 --> 00:03:33,930
hash table which actually ends up making

00:03:32,190 --> 00:03:36,180
turning everything into a linear problem

00:03:33,930 --> 00:03:37,470
and so you're finding these similarities

00:03:36,180 --> 00:03:39,269
within your puzzle pieces and

00:03:37,470 --> 00:03:40,739
algorithmically it's pretty awesome it's

00:03:39,269 --> 00:03:43,130
linear in time with the number of pieces

00:03:40,739 --> 00:03:45,450
which is way better than N squared right

00:03:43,130 --> 00:03:46,620
however it's also linear in memory with

00:03:45,450 --> 00:03:47,700
the volume of the data and this is

00:03:46,620 --> 00:03:49,890
largely due to errors in the

00:03:47,700 --> 00:03:51,390
digitization process if you have small

00:03:49,890 --> 00:03:53,010
errors when you're reading the pieces

00:03:51,390 --> 00:03:54,959
into the computer memory those errors

00:03:53,010 --> 00:03:57,269
will cause problems with the hashing and

00:03:54,959 --> 00:03:58,800
you end up having to keep track of all

00:03:57,269 --> 00:04:00,630
of the different tight little patterns

00:03:58,800 --> 00:04:02,850
that you have and so this is basically

00:04:00,630 --> 00:04:05,430
the problem that we've spent five or six

00:04:02,850 --> 00:04:06,660
years in my lab trying to solve just to

00:04:05,430 --> 00:04:08,030
show you the practical effects of this

00:04:06,660 --> 00:04:11,100
for about five hundred dollars of

00:04:08,030 --> 00:04:12,600
sequencing data today we would require

00:04:11,100 --> 00:04:14,430
about a hundred gigabytes of RAM in

00:04:12,600 --> 00:04:16,830
order to to put that puzzle together to

00:04:14,430 --> 00:04:18,479
put these sequences together and so this

00:04:16,830 --> 00:04:20,820
was a real problem a couple years back

00:04:18,479 --> 00:04:22,200
it's still a real problem within the

00:04:20,820 --> 00:04:25,130
field although we've prevent we've

00:04:22,200 --> 00:04:27,290
provided some some

00:04:25,130 --> 00:04:30,050
approaches that help deal with it so our

00:04:27,290 --> 00:04:31,790
research challenges the research

00:04:30,050 --> 00:04:33,170
challenges that we talked on my lab but

00:04:31,790 --> 00:04:34,580
right now it only costs about ten

00:04:33,170 --> 00:04:36,650
thousand dollars and takes about a week

00:04:34,580 --> 00:04:38,330
to generate enough sequence that really

00:04:36,650 --> 00:04:40,310
no commodity computer can handle it and

00:04:38,330 --> 00:04:41,750
even really very few supercomputers can

00:04:40,310 --> 00:04:43,160
handle that amount of sequence in terms

00:04:41,750 --> 00:04:45,110
of us doing this sort of puzzle piece

00:04:43,160 --> 00:04:47,630
putting the sequences back together um

00:04:45,110 --> 00:04:49,160
and the other problem is that hundreds

00:04:47,630 --> 00:04:50,720
to thousands of such data sets are

00:04:49,160 --> 00:04:53,720
actually being generated by biologists

00:04:50,720 --> 00:04:55,820
on a weekly to monthly basis and so this

00:04:53,720 --> 00:04:57,350
is really a really vast data analysis

00:04:55,820 --> 00:04:58,610
problem sort of the inverse of the

00:04:57,350 --> 00:05:00,350
particle accelerator problem where you

00:04:58,610 --> 00:05:02,210
have one big particle accelerator

00:05:00,350 --> 00:05:03,590
generating massive volumes of data and

00:05:02,210 --> 00:05:05,420
then thousands of people looking at the

00:05:03,590 --> 00:05:06,770
data here you have many thousands of

00:05:05,420 --> 00:05:10,700
people generating data that only a few

00:05:06,770 --> 00:05:11,990
people can analyze so the nice thing is

00:05:10,700 --> 00:05:13,460
that over the last five or six years

00:05:11,990 --> 00:05:16,250
we've basically solved or at least

00:05:13,460 --> 00:05:17,210
address to this top approach and what

00:05:16,250 --> 00:05:18,860
I'm going to tell you about today is

00:05:17,210 --> 00:05:20,780
some of the outcomes of our attempts to

00:05:18,860 --> 00:05:22,420
address this bottom issue which is that

00:05:20,780 --> 00:05:25,790
we're generating lots of these datasets

00:05:22,420 --> 00:05:27,350
so our research the computer science

00:05:25,790 --> 00:05:29,000
side of our research is we've built a

00:05:27,350 --> 00:05:30,530
streaming lossy compression approach and

00:05:29,000 --> 00:05:32,270
basically what we can do is we can read

00:05:30,530 --> 00:05:33,710
in each piece at one at a time and say

00:05:32,270 --> 00:05:35,360
have we seen this piece before or not

00:05:33,710 --> 00:05:36,920
and if we have seen the piece before we

00:05:35,360 --> 00:05:39,890
can discard it and decrease the total

00:05:36,920 --> 00:05:41,420
number of pieces we're looking at and it

00:05:39,890 --> 00:05:42,590
turns out to be a single pass algorithm

00:05:41,420 --> 00:05:44,630
and it's really nice in low memory and

00:05:42,590 --> 00:05:46,010
all of that we've also invested heavily

00:05:44,630 --> 00:05:47,690
and a lot of probabilistic data

00:05:46,010 --> 00:05:48,830
structures low memory probabilistic data

00:05:47,690 --> 00:05:50,900
structures which I talked about last

00:05:48,830 --> 00:05:52,400
year and we're now to reach the point

00:05:50,900 --> 00:05:53,930
with our computer science research where

00:05:52,400 --> 00:05:54,950
our memory now scales considerably

00:05:53,930 --> 00:05:56,180
better it scales with the amount of

00:05:54,950 --> 00:05:57,680
information in the puzzle which is

00:05:56,180 --> 00:05:59,000
basically the size of the picture which

00:05:57,680 --> 00:06:01,490
is always much smaller than the number

00:05:59,000 --> 00:06:03,500
of pieces we have and this I is sample

00:06:01,490 --> 00:06:05,960
dependent but typically its 120th or

00:06:03,500 --> 00:06:08,600
less than the number of pieces in the in

00:06:05,960 --> 00:06:10,130
the that we have in front of us the

00:06:08,600 --> 00:06:13,490
other research approach and the other

00:06:10,130 --> 00:06:14,990
component of our research is really was

00:06:13,490 --> 00:06:16,970
addressed very nicely by Fernando this

00:06:14,990 --> 00:06:19,130
morning so um we've invested heavily

00:06:16,970 --> 00:06:20,660
coming for the Python community I've

00:06:19,130 --> 00:06:21,980
invested heavily in things like open

00:06:20,660 --> 00:06:23,660
source open science open access

00:06:21,980 --> 00:06:25,400
reproducible computational research

00:06:23,660 --> 00:06:27,290
using tools that are pretty familiar

00:06:25,400 --> 00:06:29,060
here and are really never heard of in

00:06:27,290 --> 00:06:30,860
scientific conferences so we use github

00:06:29,060 --> 00:06:32,060
we use automated testing we use

00:06:30,860 --> 00:06:33,380
continuous integration we have a

00:06:32,060 --> 00:06:35,840
literate testing framework that we've

00:06:33,380 --> 00:06:37,220
just put in place we blog about our

00:06:35,840 --> 00:06:38,660
stuff we tweet about our stuff and we

00:06:37,220 --> 00:06:40,850
use ipython notebook to do all

00:06:38,660 --> 00:06:42,890
data analysis essentially our papers do

00:06:40,850 --> 00:06:44,960
come as Fernando says because here's the

00:06:42,890 --> 00:06:47,060
data here's the github repository here's

00:06:44,960 --> 00:06:49,940
a make file type make and here's your

00:06:47,060 --> 00:06:52,340
here's all of your data analyzed just as

00:06:49,940 --> 00:06:54,410
we did it um and as part of this we've

00:06:52,340 --> 00:06:56,750
been extending our efforts into more

00:06:54,410 --> 00:06:59,060
general protocols so our papers are on

00:06:56,750 --> 00:07:00,650
our software but then we also want to

00:06:59,060 --> 00:07:02,630
integrate our software into a larger

00:07:00,650 --> 00:07:04,700
ecosystem of stuff that people actually

00:07:02,630 --> 00:07:06,890
use to do biological sequence analysis

00:07:04,700 --> 00:07:10,100
and for that purpose we've actually been

00:07:06,890 --> 00:07:12,470
developing these protocols and applying

00:07:10,100 --> 00:07:13,940
them to tackle squishy biology problems

00:07:12,470 --> 00:07:16,490
so I thought I should put a real biology

00:07:13,940 --> 00:07:17,780
slide in these are two organisms that we

00:07:16,490 --> 00:07:19,310
work on that you find off the coast of

00:07:17,780 --> 00:07:21,620
France they look basically like rocks

00:07:19,310 --> 00:07:23,210
for disguise their sea squirts and this

00:07:21,620 --> 00:07:25,040
is what they look like underneath their

00:07:23,210 --> 00:07:26,800
tunic is actually the gonad which we

00:07:25,040 --> 00:07:28,910
then harvest eggs and sperm from and

00:07:26,800 --> 00:07:33,380
generate embryos which we then do

00:07:28,910 --> 00:07:35,300
horrible things to um so these protocols

00:07:33,380 --> 00:07:36,740
are actually directly useful for real

00:07:35,300 --> 00:07:38,030
biological purposes and what I mean to

00:07:36,740 --> 00:07:40,130
talk about for the rest of the day for

00:07:38,030 --> 00:07:41,720
the rest of my talk is is trying to

00:07:40,130 --> 00:07:45,320
understand how these protocols perform

00:07:41,720 --> 00:07:47,120
computationally so the trick is rather

00:07:45,320 --> 00:07:48,710
than writing a black box pipeline a set

00:07:47,120 --> 00:07:51,050
of scripts that you feed data into and

00:07:48,710 --> 00:07:53,900
then outputs something we've actually

00:07:51,050 --> 00:07:55,730
written a set of tutorials for running

00:07:53,900 --> 00:07:57,680
through a data analysis from start to

00:07:55,730 --> 00:07:59,480
finish in the cloud so the great thing

00:07:57,680 --> 00:08:00,530
about the cloud is you you know what

00:07:59,480 --> 00:08:01,669
machine you're going to get you know

00:08:00,530 --> 00:08:03,200
what resources you have you can

00:08:01,669 --> 00:08:04,580
configure it exactly the way you want it

00:08:03,200 --> 00:08:06,590
can be a hundred percent reproducible

00:08:04,580 --> 00:08:08,690
and so we've written these tutorials

00:08:06,590 --> 00:08:11,750
which you can go find online very easily

00:08:08,690 --> 00:08:13,010
associated with my blog post and we've

00:08:11,750 --> 00:08:14,330
written them in English with a bunch of

00:08:13,010 --> 00:08:16,910
shell commands so you have to basically

00:08:14,330 --> 00:08:18,620
know how to use SSH and and start up a

00:08:16,910 --> 00:08:20,540
cloud machine in order to run them um

00:08:18,620 --> 00:08:22,340
and then so we've been using them for

00:08:20,540 --> 00:08:24,530
education and actually on Monday I'll be

00:08:22,340 --> 00:08:26,390
giving a software carpentry a tutorial

00:08:24,530 --> 00:08:28,880
that's running through some of these um

00:08:26,390 --> 00:08:30,800
but then we've also said well why stop

00:08:28,880 --> 00:08:35,180
there why not do literate testing so

00:08:30,800 --> 00:08:36,560
what we've done is we've written a very

00:08:35,180 --> 00:08:38,060
simple shell script that actually runs

00:08:36,560 --> 00:08:40,280
through all of our protocols pulls out

00:08:38,060 --> 00:08:41,690
the shell commands and then turns them

00:08:40,280 --> 00:08:43,340
into shell scripts that can be run for

00:08:41,690 --> 00:08:45,020
several different purposes one purpose

00:08:43,340 --> 00:08:46,520
is for a tool competition so if people

00:08:45,020 --> 00:08:48,800
want to swap in different tools and run

00:08:46,520 --> 00:08:50,600
the same workflow with one thing changed

00:08:48,800 --> 00:08:52,030
they can do that very easily we've

00:08:50,600 --> 00:08:54,010
actually implemented them as except

00:08:52,030 --> 00:08:55,510
tests on our actual core software which

00:08:54,010 --> 00:08:57,340
is integrated into these protocols and

00:08:55,510 --> 00:08:58,390
then we thought we're also using them

00:08:57,340 --> 00:08:59,980
for benchmarking and that's what I'll be

00:08:58,390 --> 00:09:01,690
talking about for the rest and I think

00:08:59,980 --> 00:09:03,130
one thing that that we've been seeing is

00:09:01,690 --> 00:09:04,780
that when you actually do all of this

00:09:03,130 --> 00:09:06,070
when you do these things correctly you

00:09:04,780 --> 00:09:08,500
have everything at github you have

00:09:06,070 --> 00:09:11,980
everything automatically tested it sort

00:09:08,500 --> 00:09:14,200
of becomes unstoppable force for forward

00:09:11,980 --> 00:09:16,270
momentum and science because um we don't

00:09:14,200 --> 00:09:18,100
have to worry about bit rot our our

00:09:16,270 --> 00:09:20,890
backs are protected because everything

00:09:18,100 --> 00:09:22,930
that we do is sort of automatically run

00:09:20,890 --> 00:09:26,440
on a pretty regular basis with a few

00:09:22,930 --> 00:09:27,880
exceptions okay so this talk was going

00:09:26,440 --> 00:09:29,680
to be about benchmarking so let me get

00:09:27,880 --> 00:09:31,150
to some benchmarking so our benchmarking

00:09:29,680 --> 00:09:32,710
strategy was basically to go out and

00:09:31,150 --> 00:09:35,050
find how our protocols ran on real

00:09:32,710 --> 00:09:36,640
computers and by real computers I mean

00:09:35,050 --> 00:09:39,190
fake computers that you rent from amazon

00:09:36,640 --> 00:09:41,260
or Rackspace so we went and rented a

00:09:39,190 --> 00:09:42,670
bunch of cloud VMS we extracted the

00:09:41,260 --> 00:09:43,780
commands from the tutorials using our

00:09:42,670 --> 00:09:45,190
literate resting framework and their

00:09:43,780 --> 00:09:47,170
instructions for running all of this and

00:09:45,190 --> 00:09:48,790
then we use this neat package that we'd

00:09:47,170 --> 00:09:50,080
never I'd never heard of before but that

00:09:48,790 --> 00:09:52,390
several different people found via

00:09:50,080 --> 00:09:55,480
google and sent to me um called SAR

00:09:52,390 --> 00:09:57,600
which is a system activity report I

00:09:55,480 --> 00:09:59,920
think part of the cysts at package to

00:09:57,600 --> 00:10:01,300
simultaneously sample the CPU RAM and

00:09:59,920 --> 00:10:03,670
disk i/o that was currently happening

00:10:01,300 --> 00:10:05,440
and from that we get output that looks

00:10:03,670 --> 00:10:06,790
like this which is this is really quite

00:10:05,440 --> 00:10:09,580
cool this is from an eye Python notebook

00:10:06,790 --> 00:10:12,220
and matplotlib figure and what you can

00:10:09,580 --> 00:10:14,800
see here is for a data subset that takes

00:10:12,220 --> 00:10:18,580
about an hour to run we have the cpu

00:10:14,800 --> 00:10:22,030
load in blue we have the ram a load in

00:10:18,580 --> 00:10:23,950
gigabytes in red and the disk thousands

00:10:22,030 --> 00:10:25,330
of transactions per second in green you

00:10:23,950 --> 00:10:26,650
can see for the entire run of our

00:10:25,330 --> 00:10:28,990
protocol which goes through quality

00:10:26,650 --> 00:10:30,940
control and some trimming and filtering

00:10:28,990 --> 00:10:32,110
and some assembly and some differential

00:10:30,940 --> 00:10:33,850
expression there's a lot of different

00:10:32,110 --> 00:10:35,680
things going on that all have various

00:10:33,850 --> 00:10:38,500
different CPU requirements disk

00:10:35,680 --> 00:10:40,690
requirements and start disk requirements

00:10:38,500 --> 00:10:42,550
and memory requirements and just keep an

00:10:40,690 --> 00:10:43,870
eye on the RAM the whole purpose of our

00:10:42,550 --> 00:10:46,810
research program for the last six years

00:10:43,870 --> 00:10:47,740
has been to decrease this red line to

00:10:46,810 --> 00:10:51,310
the point where we can run it on a

00:10:47,740 --> 00:10:52,690
larger variety of computers so each

00:10:51,310 --> 00:10:54,430
protocol has many steps here I've

00:10:52,690 --> 00:10:55,810
labeled them the quality control digital

00:10:54,430 --> 00:10:58,030
normalization which is our software

00:10:55,810 --> 00:11:00,970
assembly annotation and differential

00:10:58,030 --> 00:11:02,770
expression analysis and we're really

00:11:00,970 --> 00:11:04,390
most interested in the RAM intensive

00:11:02,770 --> 00:11:05,740
bits which is the digital normalization

00:11:04,390 --> 00:11:07,300
and the assembly

00:11:05,740 --> 00:11:08,860
and again this takes about an hour and

00:11:07,300 --> 00:11:10,690
when you run it on the entire protocol

00:11:08,860 --> 00:11:12,610
it's about 40 hours so this is about

00:11:10,690 --> 00:11:15,040
this is about how long it takes to run

00:11:12,610 --> 00:11:17,650
through a protocol for a set of data

00:11:15,040 --> 00:11:18,670
that costs about a about eight thousand

00:11:17,650 --> 00:11:20,830
dollars and took about a week to

00:11:18,670 --> 00:11:23,650
generate on a machine what you can see

00:11:20,830 --> 00:11:26,980
is things change when we're using the

00:11:23,650 --> 00:11:28,600
whole data set the digi norm phrase

00:11:26,980 --> 00:11:30,550
which is our software takes less time

00:11:28,600 --> 00:11:34,570
but then the assembly actually takes a

00:11:30,550 --> 00:11:36,820
lot of time and is very fairly intensive

00:11:34,570 --> 00:11:38,530
CPU eyes and uses a lot of memory and

00:11:36,820 --> 00:11:40,480
somewhat unpredictably and then actually

00:11:38,530 --> 00:11:44,320
uses very little disk has very little

00:11:40,480 --> 00:11:46,900
disk access so this is this is our

00:11:44,320 --> 00:11:49,120
complete protocol run on sorry these are

00:11:46,900 --> 00:11:52,390
our protocol on run on the entire data

00:11:49,120 --> 00:11:54,010
set okay so we have all sorts of numbers

00:11:52,390 --> 00:11:55,060
like this in our repository I encourage

00:11:54,010 --> 00:11:56,770
you to go dig through them if you're

00:11:55,060 --> 00:11:59,350
interested what kind of conclusions did

00:11:56,770 --> 00:12:01,390
we reach so one observation is that rack

00:11:59,350 --> 00:12:03,640
space offers by default faster machines

00:12:01,390 --> 00:12:06,370
so for the 15 gigabyte machine that we

00:12:03,640 --> 00:12:08,260
were using it took only 34 hours to run

00:12:06,370 --> 00:12:10,960
through our entire protocol for cost

00:12:08,260 --> 00:12:12,970
about 23 bucks and then the various

00:12:10,960 --> 00:12:15,130
amazon machines were considerably slower

00:12:12,970 --> 00:12:16,180
although given the different costs you

00:12:15,130 --> 00:12:17,950
can depend whether not you want to

00:12:16,180 --> 00:12:20,350
optimize for latency or for cost right

00:12:17,950 --> 00:12:23,380
doesn't either you can you can pick your

00:12:20,350 --> 00:12:24,580
battle the second observation and this

00:12:23,380 --> 00:12:27,310
one is something that I would love to

00:12:24,580 --> 00:12:29,350
hear theories on is that the Amazon

00:12:27,310 --> 00:12:31,600
ephemeral storage is faster than the EBS

00:12:29,350 --> 00:12:34,000
block devices even if you turn the I ops

00:12:31,600 --> 00:12:36,400
performance all the way up so what we

00:12:34,000 --> 00:12:39,250
did was we have three m1x large machines

00:12:36,400 --> 00:12:40,630
here these are 15 gigabytes of RAM and

00:12:39,250 --> 00:12:43,270
that's basically and a couple hundred

00:12:40,630 --> 00:12:46,030
gigabytes of local ephemeral storage and

00:12:43,270 --> 00:12:48,790
when we added either for the data disk

00:12:46,030 --> 00:12:52,180
or for our working disk the maximum I

00:12:48,790 --> 00:12:53,890
ops io operations per second on to the

00:12:52,180 --> 00:12:56,530
data disk or the working disk we just

00:12:53,890 --> 00:12:59,500
slowed things down steadily and i should

00:12:56,530 --> 00:13:01,390
say this all costs money so we're paying

00:12:59,500 --> 00:13:03,190
more money to get worse performance and

00:13:01,390 --> 00:13:04,690
i didn't factor in the disk costs here

00:13:03,190 --> 00:13:06,610
this is just the per hour cost the disk

00:13:04,690 --> 00:13:08,830
costs on a 24-hour basis are essentially

00:13:06,610 --> 00:13:10,390
negligible and the only thing i can

00:13:08,830 --> 00:13:12,040
think of here and this is my current

00:13:10,390 --> 00:13:13,510
working hypothesis is that what you're

00:13:12,040 --> 00:13:17,020
paying for with the I ops is better

00:13:13,510 --> 00:13:18,520
average behavior whereas the ephemeral

00:13:17,020 --> 00:13:19,570
storage frequently can be better but

00:13:18,520 --> 00:13:21,460
they don't guarantee that

00:13:19,570 --> 00:13:23,170
they don't guarantee the worst-case

00:13:21,460 --> 00:13:25,390
scenario can be considerably worse and I

00:13:23,170 --> 00:13:26,940
see a few people nodding I'd love to

00:13:25,390 --> 00:13:30,000
hear from you afterwards if you actually

00:13:26,940 --> 00:13:34,270
if this isn't the guess on your part

00:13:30,000 --> 00:13:38,350
okay observation number three so the

00:13:34,270 --> 00:13:40,000
numa architecture is an important aspect

00:13:38,350 --> 00:13:41,530
of big machine so i should let me let me

00:13:40,000 --> 00:13:44,200
take a step back so all of our software

00:13:41,530 --> 00:13:46,360
rights all of our software the cage

00:13:44,200 --> 00:13:48,640
where software um works with really

00:13:46,360 --> 00:13:50,560
large hash tables what we do is we go

00:13:48,640 --> 00:13:52,420
and we allocate you say I want to use

00:13:50,560 --> 00:13:54,220
100 megabytes 100 gigabytes of memory

00:13:52,420 --> 00:13:56,020
and I want it split among four hash

00:13:54,220 --> 00:13:57,580
tables and these are probabilistic data

00:13:56,020 --> 00:13:59,110
structures and we use them in this way

00:13:57,580 --> 00:14:00,730
they implement a count min sketch which

00:13:59,110 --> 00:14:02,560
I talked about last year and we use them

00:14:00,730 --> 00:14:04,470
this way because um it's considerably

00:14:02,560 --> 00:14:06,880
more memory efficient than any possible

00:14:04,470 --> 00:14:07,990
exact data structure but you do have to

00:14:06,880 --> 00:14:10,090
specify how much remember you're gonna

00:14:07,990 --> 00:14:11,680
be using up front when you have these

00:14:10,090 --> 00:14:13,390
big hash tables of your indexing into

00:14:11,680 --> 00:14:14,500
randomly you run into something called

00:14:13,390 --> 00:14:17,560
the non-uniform memory access

00:14:14,500 --> 00:14:19,570
architecture on multi-core on multi CPU

00:14:17,560 --> 00:14:20,770
machines there are different parts of

00:14:19,570 --> 00:14:24,280
the memory that have preferential access

00:14:20,770 --> 00:14:25,930
to each for each of the CPUs and so if

00:14:24,280 --> 00:14:27,190
you're on this CP if you're computing on

00:14:25,930 --> 00:14:28,180
this cpu and reaching over to this

00:14:27,190 --> 00:14:29,920
memory it will be a lot slower

00:14:28,180 --> 00:14:32,560
potentially than if you're reaching to

00:14:29,920 --> 00:14:36,070
sort of memory that's local to that CPU

00:14:32,560 --> 00:14:38,140
and so here we're showing the effects of

00:14:36,070 --> 00:14:39,790
the slowdown what I did was the slow

00:14:38,140 --> 00:14:41,230
down due to this sort of long-range

00:14:39,790 --> 00:14:44,020
memory access and so what we did was we

00:14:41,230 --> 00:14:47,530
took a constant task a very small tasks

00:14:44,020 --> 00:14:48,940
that took about a minute on with small

00:14:47,530 --> 00:14:50,500
amounts of memory and we did the same

00:14:48,940 --> 00:14:52,510
task over and over and over again with

00:14:50,500 --> 00:14:53,650
increasing amounts of total memory used

00:14:52,510 --> 00:14:55,360
all the way up to a terabyte of memory

00:14:53,650 --> 00:14:57,040
we do have data sets where we need to

00:14:55,360 --> 00:14:58,660
use a terabyte of memory so this is all

00:14:57,040 --> 00:14:59,920
run on our HPC because right now I don't

00:14:58,660 --> 00:15:02,200
know of any cloud computing machines

00:14:59,920 --> 00:15:06,400
that offer a full terabyte of memory nor

00:15:02,200 --> 00:15:09,130
did I want to pay for them so the the

00:15:06,400 --> 00:15:10,570
total time here is in solid blue and it

00:15:09,130 --> 00:15:11,830
basically you can see that you go from

00:15:10,570 --> 00:15:13,330
something that takes a minute or two to

00:15:11,830 --> 00:15:14,770
something that takes almost an hour when

00:15:13,330 --> 00:15:17,020
you go up to a terabyte of memory now

00:15:14,770 --> 00:15:18,010
the this blue line could be caused by

00:15:17,020 --> 00:15:19,480
the fact that you're allocating a

00:15:18,010 --> 00:15:21,160
terabyte of memory and you need to zero

00:15:19,480 --> 00:15:23,770
it out maybe the allocation phase is

00:15:21,160 --> 00:15:25,360
really slow so I separately benchmark to

00:15:23,770 --> 00:15:28,090
the post allocation phase and what you

00:15:25,360 --> 00:15:31,660
can see is computing on the same data on

00:15:28,090 --> 00:15:33,270
the same machine but in slightly in

00:15:31,660 --> 00:15:34,970
considerably larger memory

00:15:33,270 --> 00:15:36,840
you get a really significant slowdown

00:15:34,970 --> 00:15:39,390
just from the fact that you're accessing

00:15:36,840 --> 00:15:41,400
so much memory and if you compute the

00:15:39,390 --> 00:15:43,680
ratio this is the lost time ratio due

00:15:41,400 --> 00:15:48,020
only to the RAM access you can see that

00:15:43,680 --> 00:15:50,040
you get a 20 to 25 fold slow down for

00:15:48,020 --> 00:15:53,070
increasing the amount of memory using by

00:15:50,040 --> 00:15:55,560
by by such a great great amount so that

00:15:53,070 --> 00:15:57,240
was a that was quite a surprise actually

00:15:55,560 --> 00:15:58,680
the effect we knew from running it that

00:15:57,240 --> 00:15:59,970
that this was a problem but from

00:15:58,680 --> 00:16:01,230
watching it while I ran this was around

00:15:59,970 --> 00:16:04,350
but we'd never really benchmarked it to

00:16:01,230 --> 00:16:05,490
this extent so the next question is why

00:16:04,350 --> 00:16:07,620
can't we just use a faster computer

00:16:05,490 --> 00:16:09,530
while we just rent a faster computer so

00:16:07,620 --> 00:16:12,300
it turns out amazon now offers an m3

00:16:09,530 --> 00:16:14,790
extra large and m3 extra-large is a

00:16:12,300 --> 00:16:16,740
computer that has 240 gigabyte SSD

00:16:14,790 --> 00:16:18,870
drives and it's about forty percent

00:16:16,740 --> 00:16:20,220
faster cores and what we see when we run

00:16:18,870 --> 00:16:22,980
our demo data is that we get about a

00:16:20,220 --> 00:16:24,210
thirty percent speed boost goes from

00:16:22,980 --> 00:16:26,130
about three thousand seconds to about

00:16:24,210 --> 00:16:28,560
2000 seconds so we thought we'd try it

00:16:26,130 --> 00:16:31,070
out on the whole data set and here's the

00:16:28,560 --> 00:16:33,210
fun bit about data intensive computing

00:16:31,070 --> 00:16:35,760
turns out that we ran out of disk space

00:16:33,210 --> 00:16:36,960
because 40 gigabyte hard drives 80

00:16:35,760 --> 00:16:39,180
gigabyte hard drives in this case

00:16:36,960 --> 00:16:40,440
actually we ran it on a 2 x-large 80

00:16:39,180 --> 00:16:42,270
gigabyte hard drives weren't big enough

00:16:40,440 --> 00:16:44,340
for our full data set so we'd have had

00:16:42,270 --> 00:16:46,620
to complicate our engineering in order

00:16:44,340 --> 00:16:48,210
to spread the data over multiple hard

00:16:46,620 --> 00:16:50,160
drives in fact i'm not sure 160 gigs

00:16:48,210 --> 00:16:51,630
would have been enough either so at the

00:16:50,160 --> 00:16:53,370
moment with a lot of the cloud computing

00:16:51,630 --> 00:16:54,810
platforms you can have fast disk or lots

00:16:53,370 --> 00:16:57,420
of disk but you can't have both on the

00:16:54,810 --> 00:16:59,070
same computer or you end up becoming

00:16:57,420 --> 00:17:00,480
really expensive what those are your

00:16:59,070 --> 00:17:02,850
options and so it's an interesting

00:17:00,480 --> 00:17:03,810
lesson that you know you really have to

00:17:02,850 --> 00:17:05,430
pick what you're going to optimize for

00:17:03,810 --> 00:17:07,680
you can optimize for latency or do you

00:17:05,430 --> 00:17:08,820
care about cost or do you what else what

00:17:07,680 --> 00:17:12,720
other what are the other things you care

00:17:08,820 --> 00:17:14,070
about um okay so so for the data

00:17:12,720 --> 00:17:15,900
oriented portion of talk to future

00:17:14,070 --> 00:17:17,850
directions i think i'm going to

00:17:15,900 --> 00:17:19,440
reprioritize investing in cash local

00:17:17,850 --> 00:17:20,880
data structures and algorithms rather

00:17:19,440 --> 00:17:22,350
than these massive hash tables that are

00:17:20,880 --> 00:17:24,060
spread all over memory we're going to

00:17:22,350 --> 00:17:26,870
try and feel I think I think we should

00:17:24,060 --> 00:17:29,520
figure out how to work more locally um

00:17:26,870 --> 00:17:31,200
some of our pain is caused by writing

00:17:29,520 --> 00:17:32,460
the same data out to disk and reading it

00:17:31,200 --> 00:17:34,740
back in because we can't keep it all in

00:17:32,460 --> 00:17:37,140
memory and I think that we have some

00:17:34,740 --> 00:17:38,460
algorithm algorithm Xin mind for turning

00:17:37,140 --> 00:17:41,520
everything into a purely streaming

00:17:38,460 --> 00:17:42,600
approach which may cost more memory but

00:17:41,520 --> 00:17:44,400
might actually be considerably faster

00:17:42,600 --> 00:17:46,290
and then this

00:17:44,400 --> 00:17:47,400
what's surprising conclusion to me I

00:17:46,290 --> 00:17:48,420
mean I guess I was already leaning in

00:17:47,400 --> 00:17:50,040
this direction but it's nice to have

00:17:48,420 --> 00:17:51,540
some numbers I'm worried that I don't

00:17:50,040 --> 00:17:52,950
know that straight code optimization or

00:17:51,540 --> 00:17:55,110
infrastructure engineering in a

00:17:52,950 --> 00:17:56,460
worthwhile investment for us that would

00:17:55,110 --> 00:17:57,960
complicate the protocols it would

00:17:56,460 --> 00:17:59,100
complicate what we're actually doing and

00:17:57,960 --> 00:18:01,650
it's not clear that it's going to give

00:17:59,100 --> 00:18:03,960
us more than a 50-percent speed increase

00:18:01,650 --> 00:18:04,920
compared to what like cash up to my cash

00:18:03,960 --> 00:18:06,870
local data structures and algorithms

00:18:04,920 --> 00:18:10,320
would give us just like a 30 or 40 fold

00:18:06,870 --> 00:18:13,020
increase in speed okay so I give a lot

00:18:10,320 --> 00:18:14,250
of talks apparently when i speak i'm

00:18:13,020 --> 00:18:18,180
somewhat approachable which is something

00:18:14,250 --> 00:18:20,880
I'm working on and so people come up and

00:18:18,180 --> 00:18:23,430
offer solutions so here a couple

00:18:20,880 --> 00:18:26,160
frequently offered solutions so you

00:18:23,430 --> 00:18:31,590
should like totally multithread that and

00:18:26,160 --> 00:18:32,490
so we did this we we wrote a chapter for

00:18:31,590 --> 00:18:34,080
in the performance of open source

00:18:32,490 --> 00:18:36,390
applications and we can actually get a

00:18:34,080 --> 00:18:38,820
two or four fold speed up before we come

00:18:36,390 --> 00:18:40,200
really solidly i/o bound and it turns

00:18:38,820 --> 00:18:42,150
out that the code the results from this

00:18:40,200 --> 00:18:43,740
is kind of a mess and as a current

00:18:42,150 --> 00:18:45,090
maintenance headache and so it's not

00:18:43,740 --> 00:18:47,610
clear it was a great investment actually

00:18:45,090 --> 00:18:50,400
um Hadoop will just crush that workload

00:18:47,610 --> 00:18:52,320
dude I hear this a lot it's unlikely to

00:18:50,400 --> 00:18:53,790
be cost effective we have I'll talk

00:18:52,320 --> 00:18:55,860
about this in a little bit but we have a

00:18:53,790 --> 00:18:58,860
lot of data we have no locality in the

00:18:55,860 --> 00:19:01,110
data we can't shard the data easily and

00:18:58,860 --> 00:19:04,080
our individual compute per unit of data

00:19:01,110 --> 00:19:05,880
is essentially trivial and so the cost

00:19:04,080 --> 00:19:07,470
of distributing it of fanning it out and

00:19:05,880 --> 00:19:11,910
fanning it back in and gathering it back

00:19:07,470 --> 00:19:15,000
in is likely to really over dominate the

00:19:11,910 --> 00:19:16,620
cost of everything the time and then I

00:19:15,000 --> 00:19:19,320
think Fernando talked about this also

00:19:16,620 --> 00:19:21,330
have you tried quote my proprietary Big

00:19:19,320 --> 00:19:22,500
Data technology stack you got to Silicon

00:19:21,330 --> 00:19:24,150
Valley and a bunch of people are like

00:19:22,500 --> 00:19:25,770
hey I have a company that they can solve

00:19:24,150 --> 00:19:27,600
all your problems I can't give you the

00:19:25,770 --> 00:19:29,280
source code but it's going to be awesome

00:19:27,600 --> 00:19:30,420
when you run it and the problem is that

00:19:29,280 --> 00:19:32,250
if you're actually trying to do science

00:19:30,420 --> 00:19:34,320
having hidden methods that may have

00:19:32,250 --> 00:19:37,110
unknown effects doesn't that can't

00:19:34,320 --> 00:19:38,100
possibly work or well it does for a lot

00:19:37,110 --> 00:19:42,030
of people but I don't think it's good

00:19:38,100 --> 00:19:44,070
science so okay so here's the ranti

00:19:42,030 --> 00:19:46,410
portion of my talk optimization versus

00:19:44,070 --> 00:19:48,900
scaling so everybody wants to suggest

00:19:46,410 --> 00:19:50,580
linear time memory improvements and we

00:19:48,900 --> 00:19:52,050
actually spent two years eking out a

00:19:50,580 --> 00:19:53,550
20-fold improvement during which we

00:19:52,050 --> 00:19:58,060
forgot a hundred fold increase in data

00:19:53,550 --> 00:20:00,010
generation per per unit dollar

00:19:58,060 --> 00:20:01,990
which meant that we were falling

00:20:00,010 --> 00:20:04,720
slightly slower behind than everybody

00:20:01,990 --> 00:20:06,400
else which is not a winning strategy the

00:20:04,720 --> 00:20:08,170
puzzle problem is the graph problem it's

00:20:06,400 --> 00:20:09,850
got big data it's got no locality and

00:20:08,170 --> 00:20:11,380
it's got small compute it's simply not a

00:20:09,850 --> 00:20:13,510
friendly computational problem for

00:20:11,380 --> 00:20:15,070
today's architectures and what we really

00:20:13,510 --> 00:20:16,300
needed to do with scale our algorithms

00:20:15,070 --> 00:20:18,040
or rather the algorithms that were being

00:20:16,300 --> 00:20:19,240
used in the entire field and we've

00:20:18,040 --> 00:20:21,370
gotten now down to the point where

00:20:19,240 --> 00:20:25,570
machines things that previously needed

00:20:21,370 --> 00:20:27,760
to run on really expensive you know 16

00:20:25,570 --> 00:20:29,440
terabyte memory machines can now run on

00:20:27,760 --> 00:20:33,120
single chassis computers in about 15

00:20:29,440 --> 00:20:36,490
gigabytes of memory because we chose to

00:20:33,120 --> 00:20:38,770
stop doing that doing the linear time

00:20:36,490 --> 00:20:41,170
memory improvements and go 44 algorithm

00:20:38,770 --> 00:20:44,460
scaling it to take five years but you

00:20:41,170 --> 00:20:46,630
know I'm an academic that's okay so um

00:20:44,460 --> 00:20:47,830
optimization forces scaling I think many

00:20:46,630 --> 00:20:49,330
people forget when they see something

00:20:47,830 --> 00:20:50,650
like this and they say well I'm going to

00:20:49,330 --> 00:20:52,270
choose the black line because the red

00:20:50,650 --> 00:20:54,370
line is clearly inferior in terms of

00:20:52,270 --> 00:20:55,690
computer TSA's needed they forget that

00:20:54,370 --> 00:20:56,890
if you zoom out and you tackle a big

00:20:55,690 --> 00:20:58,630
enough problem that scaling is

00:20:56,890 --> 00:21:00,400
eventually going to dominate and this

00:20:58,630 --> 00:21:02,950
can be a better investment of your time

00:21:00,400 --> 00:21:03,970
if your data size is growing is totally

00:21:02,950 --> 00:21:07,420
made up graph just in case you're

00:21:03,970 --> 00:21:09,820
wondering um and I actually see it

00:21:07,420 --> 00:21:11,650
Pikkon and online and blogging and so on

00:21:09,820 --> 00:21:13,270
that a lot of people are focusing on

00:21:11,650 --> 00:21:14,770
pleasantly parallel problems anything

00:21:13,270 --> 00:21:17,230
that you can shove into Hadoop basically

00:21:14,770 --> 00:21:18,490
and I worry about a couple things one is

00:21:17,230 --> 00:21:21,280
that Hadoop's fundamentally not that

00:21:18,490 --> 00:21:22,600
interesting another is that if you're

00:21:21,280 --> 00:21:23,950
actually doing research on this stuff

00:21:22,600 --> 00:21:26,080
it's about a hundred fold improvement

00:21:23,950 --> 00:21:27,340
not about a can we buy more machines and

00:21:26,080 --> 00:21:29,230
throw throw things with the machines

00:21:27,340 --> 00:21:30,670
ultimately depending on how your data

00:21:29,230 --> 00:21:32,500
input scaling you're going to run into

00:21:30,670 --> 00:21:34,540
problems with that approach I mean I

00:21:32,500 --> 00:21:36,700
think research can really there's a lot

00:21:34,540 --> 00:21:38,800
of cs research gets a lot of bad press

00:21:36,700 --> 00:21:40,840
because we look like we're not doing

00:21:38,800 --> 00:21:42,280
anything and maybe sometimes that's true

00:21:40,840 --> 00:21:43,930
but we're also working on things like

00:21:42,280 --> 00:21:44,950
scaling new problems and evaluating

00:21:43,930 --> 00:21:46,960
creating new data structures and

00:21:44,950 --> 00:21:49,420
algorithms and I think we perform a

00:21:46,960 --> 00:21:51,700
valuable the useful service sometimes um

00:21:49,420 --> 00:21:54,610
this is a talk for my Pike on 20 less

00:21:51,700 --> 00:21:56,080
slide for my PI con 2011 talk so I'd

00:21:54,610 --> 00:21:57,760
like to make a pitch to all of you which

00:21:56,080 --> 00:21:59,710
is life's too short to tackle the easy

00:21:57,760 --> 00:22:03,400
problems you should come work for me for

00:21:59,710 --> 00:22:05,320
a lot less money instead or academia

00:22:03,400 --> 00:22:07,150
more generally here we have problems

00:22:05,320 --> 00:22:08,980
that are really hard to parallelize and

00:22:07,150 --> 00:22:11,800
for which we have no money whatsoever

00:22:08,980 --> 00:22:12,700
it's way better than going to google and

00:22:11,800 --> 00:22:16,120
hang on google search which is

00:22:12,700 --> 00:22:18,220
relatively easy so you know come talk to

00:22:16,120 --> 00:22:19,480
me I was going to say you know Alex

00:22:18,220 --> 00:22:20,740
gainer at some points going to realize

00:22:19,480 --> 00:22:22,150
that his life's not that challenging and

00:22:20,740 --> 00:22:23,460
then he's going to come work for me for

00:22:22,150 --> 00:22:27,000
pennies and it's going to be awesome

00:22:23,460 --> 00:22:29,560
okay so i think i'll conclude there a

00:22:27,000 --> 00:22:32,650
leash enamines graduate student who

00:22:29,560 --> 00:22:33,970
started the benchmarking project I and

00:22:32,650 --> 00:22:36,100
then a bunch of lab he's contributed to

00:22:33,970 --> 00:22:39,310
this Mike Russo Louise uber lick it

00:22:36,100 --> 00:22:41,710
Camille and QP all of our stuff is

00:22:39,310 --> 00:22:44,170
freely available under BSD licenses I

00:22:41,710 --> 00:22:45,940
have a blog post with resources and

00:22:44,170 --> 00:22:48,820
pointers to everything I should point

00:22:45,940 --> 00:22:51,250
out Michael lick it Camille and QPR here

00:22:48,820 --> 00:22:54,340
at PyCon you can probably afford to buy

00:22:51,250 --> 00:22:55,300
them from me I didn't put their pictures

00:22:54,340 --> 00:22:56,500
up because I want to make it a little

00:22:55,300 --> 00:22:59,290
bit more challenging for you to find

00:22:56,500 --> 00:23:02,800
them the reason I say this is actually

00:22:59,290 --> 00:23:05,800
that Camille's right over there um I the

00:23:02,800 --> 00:23:07,300
reason I say this is I've now had one

00:23:05,800 --> 00:23:09,100
graduate student get his ph.d and go to

00:23:07,300 --> 00:23:10,990
Amazon six months before he defended and

00:23:09,100 --> 00:23:12,490
another graduate student got bought by

00:23:10,990 --> 00:23:16,390
google i sorry got offered a job by

00:23:12,490 --> 00:23:17,920
google about 32 years before his defense

00:23:16,390 --> 00:23:19,390
and he was like hey they just offered me

00:23:17,920 --> 00:23:23,830
triple your salary what do you say I'm

00:23:19,390 --> 00:23:25,060
like well should go so uh but it is a

00:23:23,830 --> 00:23:26,590
little frustrating to keep on losing

00:23:25,060 --> 00:23:29,110
people but they're awesome people and

00:23:26,590 --> 00:23:32,800
you should hire them so please go ahead

00:23:29,110 --> 00:23:34,450
I'll cheer for them while crying and

00:23:32,800 --> 00:23:36,510
that's all i have to say thank you for

00:23:34,450 --> 00:23:36,510
listening

00:23:43,400 --> 00:23:49,230
so if you have anything to say cannon

00:23:47,070 --> 00:23:50,790
lineup this microphone here in the

00:23:49,230 --> 00:23:53,040
middle of the room I were taking

00:23:50,790 --> 00:24:00,210
questions for Titus and job offers for

00:23:53,040 --> 00:24:02,400
Titus's students yes I Titus as always

00:24:00,210 --> 00:24:04,830
in wonderful stuff you're working on um

00:24:02,400 --> 00:24:08,730
I think the answer to the question

00:24:04,830 --> 00:24:10,320
advances you don't know but I I know

00:24:08,730 --> 00:24:12,900
I've talked to you but I work in a

00:24:10,320 --> 00:24:15,000
mother in a science lab in a different

00:24:12,900 --> 00:24:19,320
domain that's not really related in

00:24:15,000 --> 00:24:22,830
molecular dynamics and this lab has

00:24:19,320 --> 00:24:25,230
built very custom hardware and as a

00:24:22,830 --> 00:24:27,510
consequence got two to three orders of

00:24:25,230 --> 00:24:29,550
magnitude improvement over the next best

00:24:27,510 --> 00:24:31,980
super computers in the world for this

00:24:29,550 --> 00:24:36,929
problem set and no good for anything

00:24:31,980 --> 00:24:38,520
else we do have like things like memory

00:24:36,929 --> 00:24:41,340
locality in the partitioning of the

00:24:38,520 --> 00:24:43,110
space although you know I'm latency

00:24:41,340 --> 00:24:45,920
requirements in her note is very crucial

00:24:43,110 --> 00:24:49,530
so it's a very different sort of

00:24:45,920 --> 00:24:52,520
parallel ISM issues than you have but if

00:24:49,530 --> 00:24:54,690
you had a lot more money than you have

00:24:52,520 --> 00:24:57,840
millions and millions to actually design

00:24:54,690 --> 00:25:00,330
hardware is this something that custom

00:24:57,840 --> 00:25:01,920
hardware could make you know Jenna three

00:25:00,330 --> 00:25:04,740
orders of magnitude faster not twenty

00:25:01,920 --> 00:25:07,530
percent faster sure so custom hardware

00:25:04,740 --> 00:25:10,910
so I'm worth yeah so there is a company

00:25:07,530 --> 00:25:14,400
called I think conve that is doing this

00:25:10,910 --> 00:25:15,270
and the fundamental problem here I mean

00:25:14,400 --> 00:25:17,340
apart from the fact that if somebody

00:25:15,270 --> 00:25:19,170
gave me billions of dollars i invested

00:25:17,340 --> 00:25:22,290
in people rather than hardware but or a

00:25:19,170 --> 00:25:24,150
vacation but the the fundamental problem

00:25:22,290 --> 00:25:26,040
here is that the data generation

00:25:24,150 --> 00:25:28,290
capacity has been so democratized that

00:25:26,040 --> 00:25:29,760
there are thousands if not tens of

00:25:28,290 --> 00:25:32,760
thousands of labs generating this data

00:25:29,760 --> 00:25:34,170
and so that custom hardware would have

00:25:32,760 --> 00:25:36,470
to be bought by all of them and

00:25:34,170 --> 00:25:39,929
typically they generate the data or

00:25:36,470 --> 00:25:41,370
provided centrally and and the field of

00:25:39,929 --> 00:25:42,900
there's a cultural problem the field of

00:25:41,370 --> 00:25:45,059
biology just doesn't like to play nice

00:25:42,900 --> 00:25:47,550
that way and there's another problem

00:25:45,059 --> 00:25:49,679
which is that the downstream

00:25:47,550 --> 00:25:51,840
applications change the applications

00:25:49,679 --> 00:25:53,550
change fast enough that i worry about

00:25:51,840 --> 00:25:55,080
the lack of flexibility so i'd be

00:25:53,550 --> 00:25:56,130
interested in looking into like fpga

00:25:55,080 --> 00:25:56,610
style solutions where you can

00:25:56,130 --> 00:25:58,650
reconfigure

00:25:56,610 --> 00:26:02,220
you're exactly how you're doing it but I

00:25:58,650 --> 00:26:05,940
worry that the time cost of investing in

00:26:02,220 --> 00:26:07,799
Hardware generation would be too long on

00:26:05,940 --> 00:26:09,780
the scale the the speed with which the

00:26:07,799 --> 00:26:11,820
field is moving but I don't know just an

00:26:09,780 --> 00:26:14,850
intuition but you know what give me a

00:26:11,820 --> 00:26:18,809
million as a pilot project and and we'll

00:26:14,850 --> 00:26:21,270
see and I can keep some of my grad

00:26:18,809 --> 00:26:23,790
students I Titus great talk thank you

00:26:21,270 --> 00:26:26,880
maybe it's different biology but in my

00:26:23,790 --> 00:26:28,010
own academic experience in computer

00:26:26,880 --> 00:26:30,150
science software engineering

00:26:28,010 --> 00:26:31,679
unfortunately the funding agencies in

00:26:30,150 --> 00:26:34,049
the journal is tended not to care about

00:26:31,679 --> 00:26:36,000
this incredibly interesting awesome very

00:26:34,049 --> 00:26:38,250
difficult work necessary to build the

00:26:36,000 --> 00:26:40,260
tools and techniques to do the melon

00:26:38,250 --> 00:26:42,600
quote real research same field same

00:26:40,260 --> 00:26:45,299
field are you seeing any changes in in

00:26:42,600 --> 00:26:46,559
how I've yeah I never liked edenia for a

00:26:45,299 --> 00:26:49,679
couple years now but are you seeing any

00:26:46,559 --> 00:26:51,750
changes and how cochin season journals

00:26:49,679 --> 00:26:55,350
look at this kind of work versus the

00:26:51,750 --> 00:26:57,450
yeah there's a great post by Jake van

00:26:55,350 --> 00:26:59,850
der plas on the Big Data brain drain and

00:26:57,450 --> 00:27:01,650
the problem is that anybody that knows

00:26:59,850 --> 00:27:04,140
how to effectively develop software and

00:27:01,650 --> 00:27:05,820
has any intelligence whatsoever flees

00:27:04,140 --> 00:27:07,799
academia for industry as quickly as

00:27:05,820 --> 00:27:09,210
possible so getting paid a lot more and

00:27:07,799 --> 00:27:12,510
they're doing work that's appreciated

00:27:09,210 --> 00:27:13,799
and academia especially in biology we

00:27:12,510 --> 00:27:15,390
have this problem that we don't have any

00:27:13,799 --> 00:27:16,710
senior we don't have many senior people

00:27:15,390 --> 00:27:18,690
that are really focused on software

00:27:16,710 --> 00:27:19,679
engineering and tool development and so

00:27:18,690 --> 00:27:21,059
they're not represented in the grant

00:27:19,679 --> 00:27:22,740
panels and not resident representative

00:27:21,059 --> 00:27:25,740
the people that decide how the funding

00:27:22,740 --> 00:27:27,059
is allocated and it's just it's a big

00:27:25,740 --> 00:27:29,370
problem because there's nobody to speak

00:27:27,059 --> 00:27:30,720
for that um it's slowly changing in part

00:27:29,370 --> 00:27:34,110
because myself and some other people are

00:27:30,720 --> 00:27:35,730
finally getting to the point where we're

00:27:34,110 --> 00:27:37,470
senior enough to talk to the program

00:27:35,730 --> 00:27:39,450
managers and the program managers are

00:27:37,470 --> 00:27:40,980
also seeing that there's a slowdown in

00:27:39,450 --> 00:27:42,990
biology because all the biologist

00:27:40,980 --> 00:27:45,750
biologists are generating data that they

00:27:42,990 --> 00:27:46,950
cannot analyze at all so it's slowly

00:27:45,750 --> 00:27:48,299
changing but it's going to be a

00:27:46,950 --> 00:27:49,890
generational thing it's always more

00:27:48,299 --> 00:27:51,450
exciting to dinner a new data even if

00:27:49,890 --> 00:27:53,370
you can't analyze it than it is to

00:27:51,450 --> 00:27:54,570
analyze than to do the boring job of

00:27:53,370 --> 00:27:58,890
figuring out how to analyze the data you

00:27:54,570 --> 00:28:01,260
already have hi I'm wondering your

00:27:58,890 --> 00:28:02,960
specific problem or genetic problems

00:28:01,260 --> 00:28:07,400
biology are they mean able to

00:28:02,960 --> 00:28:07,400
crowdsourcing or crowdsourcing

00:28:08,010 --> 00:28:13,620
I have to think about it this one

00:28:11,610 --> 00:28:15,300
probably not but I think we're trying to

00:28:13,620 --> 00:28:16,950
get to the point where the data we

00:28:15,300 --> 00:28:19,340
generate the information we generate

00:28:16,950 --> 00:28:21,600
from the data will be amenable to human

00:28:19,340 --> 00:28:24,450
analysis right now this is all fairly

00:28:21,600 --> 00:28:26,640
boring I'm a I'm a human time scale we

00:28:24,450 --> 00:28:28,800
just want to compute but it means

00:28:26,640 --> 00:28:30,120
something down the road and and that's

00:28:28,800 --> 00:28:31,440
where we should bring in humans I don't

00:28:30,120 --> 00:28:34,640
think it's ready for crowdsourcing yet

00:28:31,440 --> 00:28:37,650
but I have to think about that thank you

00:28:34,640 --> 00:28:40,560
not here I saw that Amazon is like a new

00:28:37,650 --> 00:28:41,850
hpc kind of network setup and I haven't

00:28:40,560 --> 00:28:43,410
able take it for a spin but it's like

00:28:41,850 --> 00:28:44,490
halfway between commodity and super

00:28:43,410 --> 00:28:46,140
competitor and I was wondering if you

00:28:44,490 --> 00:28:49,410
run your workload on that and what

00:28:46,140 --> 00:28:51,420
that's like so so I get this I get the

00:28:49,410 --> 00:28:53,670
kind of question a fair bit what if you

00:28:51,420 --> 00:28:56,430
had this kind of hpc and I guess what

00:28:53,670 --> 00:28:58,320
I'd say is we're almost entirely ram

00:28:56,430 --> 00:29:00,240
limited so as soon as we have to run

00:28:58,320 --> 00:29:04,380
across multiple chassis zits no longer

00:29:00,240 --> 00:29:06,150
effective so the HPC offerings from from

00:29:04,380 --> 00:29:08,700
amazon and our local hpc at Michigan

00:29:06,150 --> 00:29:10,080
State I'll have fast interconnects but

00:29:08,700 --> 00:29:12,090
they're chaste the individual chasity's

00:29:10,080 --> 00:29:13,170
aren't necessarily that powerful and so

00:29:12,090 --> 00:29:15,030
we're trying to do everything on a

00:29:13,170 --> 00:29:16,650
single chassis with no use of in Turkish

00:29:15,030 --> 00:29:19,470
so even like InfiniBand or one of these

00:29:16,650 --> 00:29:20,760
high memory still too slow I I think

00:29:19,470 --> 00:29:25,560
it's the latency that's the problem not

00:29:20,760 --> 00:29:27,090
the speed if it related to the academic

00:29:25,560 --> 00:29:28,800
brain drain I think the reason a lot of

00:29:27,090 --> 00:29:30,330
people leave the reason I left is that

00:29:28,800 --> 00:29:31,590
the job market is porn on the sense that

00:29:30,330 --> 00:29:34,200
like you get paid less but that it's

00:29:31,590 --> 00:29:35,910
hard to find a job but all often or what

00:29:34,200 --> 00:29:39,980
do you think like solutions for that you

00:29:35,910 --> 00:29:42,420
have any thoughts on the matter ok so

00:29:39,980 --> 00:29:43,500
like we'll just more grant money solve

00:29:42,420 --> 00:29:49,770
that Leigh doesn't seem sustainable to

00:29:43,500 --> 00:29:52,020
me yeah I maybe it's a systemic it's

00:29:49,770 --> 00:29:54,570
just a systemic issue and I mean more

00:29:52,020 --> 00:29:56,100
grant money is that is the easy is the

00:29:54,570 --> 00:29:59,910
easy answer I think the better solution

00:29:56,100 --> 00:30:01,020
is to become I don't know maybe might be

00:29:59,910 --> 00:30:02,490
a little more efficient at how we're

00:30:01,020 --> 00:30:03,840
using your grant money allocated to

00:30:02,490 --> 00:30:05,490
people who are doing data analysis as

00:30:03,840 --> 00:30:07,980
well as data generation and then there

00:30:05,490 --> 00:30:09,360
would be a need right now yeah I don't

00:30:07,980 --> 00:30:12,300
know it's a very good let's get some

00:30:09,360 --> 00:30:14,990
beer better than everybody please thank

00:30:12,300 --> 00:30:14,990
Titus Brown

00:30:21,080 --> 00:30:23,140

YouTube URL: https://www.youtube.com/watch?v=RCRbfKK57X8


