Title: Kevin Ballard: Pushing Python: Building a High Throughput, Low Latency System - PyCon 2014
Publication date: 2014-04-24
Playlist: PyCon 2014
Description: 
	Speaker: Kevin Ballard

Taba is a distributed metrics aggregator, similar in concept to statsd. Built with Python using Redis, gevent, and Cython, it currently handles over 6M events/sec with strong consistency guarantees. This talk will present an overview of its design, and discuss the challenges and solutions encountered in the process of building a high throughput, low latency distributed service.

Slides can be found at: https://speakerdeck.com/pycon2014 and https://github.com/PyCon/2014-slides
Captions: 
	00:00:05,629 --> 00:00:08,880
good afternoon everybody we're going to

00:00:07,589 --> 00:00:09,929
get started with the next talk I'd like

00:00:08,880 --> 00:00:11,099
to deduce Kevin who's going to be

00:00:09,929 --> 00:00:20,279
talking about building a high-throughput

00:00:11,099 --> 00:00:22,590
systems with Python all right so yeah

00:00:20,279 --> 00:00:24,930
again my name is Kevin I worked for a

00:00:22,590 --> 00:00:28,140
company called tell apart we build a

00:00:24,930 --> 00:00:31,529
data platform for retailers and we deal

00:00:28,140 --> 00:00:34,440
with some fairly high high traffic rates

00:00:31,529 --> 00:00:37,350
low latencies and we use Python to do

00:00:34,440 --> 00:00:38,969
pretty much all of it so today I'm going

00:00:37,350 --> 00:00:41,879
to talk about a project called taba

00:00:38,969 --> 00:00:43,800
which is a distributed event aggregation

00:00:41,879 --> 00:00:45,300
service basically what it does is it

00:00:43,800 --> 00:00:48,120
helps you instrument your code by

00:00:45,300 --> 00:00:49,469
putting calls to record events and then

00:00:48,120 --> 00:00:51,719
those get aggregated into a central

00:00:49,469 --> 00:00:54,989
location where you can then query them

00:00:51,719 --> 00:00:56,699
so for example we can record the the

00:00:54,989 --> 00:00:59,699
particular wind price of a type of bid

00:00:56,699 --> 00:01:00,960
and then query it for for example the 10

00:00:59,699 --> 00:01:05,400
minute sliding window and some

00:01:00,960 --> 00:01:08,430
percentiles so top is built using Python

00:01:05,400 --> 00:01:10,830
it uses Redis as a back-end store we

00:01:08,430 --> 00:01:12,810
have a cluster of those instances it

00:01:10,830 --> 00:01:15,420
makes heavy use of G event and does some

00:01:12,810 --> 00:01:17,570
optimizations using scythes on and it

00:01:15,420 --> 00:01:19,830
does a fairly high ingest rate uses

00:01:17,570 --> 00:01:21,990
multiple over a hundred different

00:01:19,830 --> 00:01:26,159
processors so it's a relatively large

00:01:21,990 --> 00:01:28,890
scale deployment so instead of talking

00:01:26,159 --> 00:01:30,000
today about the the architecture and how

00:01:28,890 --> 00:01:32,100
its implemented I thought it'd be more

00:01:30,000 --> 00:01:34,950
useful to talk about some of the lessons

00:01:32,100 --> 00:01:39,060
learned when trying to build large-scale

00:01:34,950 --> 00:01:41,729
distributed systems using Python so

00:01:39,060 --> 00:01:44,790
lesson number one is get your data model

00:01:41,729 --> 00:01:46,020
right so any given problem has a number

00:01:44,790 --> 00:01:47,100
of different ways you can solve it

00:01:46,020 --> 00:01:50,930
number of different ways that you can

00:01:47,100 --> 00:01:53,729
model it and the one you go with really

00:01:50,930 --> 00:01:56,670
limits or informs what your application

00:01:53,729 --> 00:01:58,140
is capable of doing and some very subtle

00:01:56,670 --> 00:02:01,320
differences in the way that you model a

00:01:58,140 --> 00:02:04,229
program can have massive impacts on your

00:02:01,320 --> 00:02:06,360
ability to perform later and once you've

00:02:04,229 --> 00:02:09,270
committed to a model it is extremely

00:02:06,360 --> 00:02:10,679
important it's extremely difficult to go

00:02:09,270 --> 00:02:11,730
back and change it especially in a

00:02:10,679 --> 00:02:13,170
Jewess tributed system where you have

00:02:11,730 --> 00:02:13,440
things deployed all over the place and

00:02:13,170 --> 00:02:15,600
you have

00:02:13,440 --> 00:02:19,560
protocols between different components

00:02:15,600 --> 00:02:20,970
it's very hard to change so at a high

00:02:19,560 --> 00:02:23,310
level this is kind of what the data

00:02:20,970 --> 00:02:24,810
model for tada looks like you have a

00:02:23,310 --> 00:02:26,760
large stream of events which are

00:02:24,810 --> 00:02:30,060
generated by clients those get

00:02:26,760 --> 00:02:32,790
aggregated into these state objects one

00:02:30,060 --> 00:02:35,370
per tag which is essentially the metric

00:02:32,790 --> 00:02:37,170
and the source and those get stored in

00:02:35,370 --> 00:02:39,360
the database and then when you perform a

00:02:37,170 --> 00:02:42,810
query the states that match that query

00:02:39,360 --> 00:02:44,880
get combined into an aggregate those

00:02:42,810 --> 00:02:47,550
different objects look like this so an

00:02:44,880 --> 00:02:49,740
event is basically a tuple of the the

00:02:47,550 --> 00:02:52,290
name the type the time and some payload

00:02:49,740 --> 00:02:55,710
and this in this example of a moving

00:02:52,290 --> 00:02:57,900
interval counter the state is basically

00:02:55,710 --> 00:03:00,210
some bucket eyes counts and totals over

00:02:57,900 --> 00:03:02,790
to certain time range and those states

00:03:00,210 --> 00:03:04,380
are stored as compact efficient binary

00:03:02,790 --> 00:03:07,110
objects so that because there they go

00:03:04,380 --> 00:03:08,970
over the wire a lot they get edited very

00:03:07,110 --> 00:03:12,480
often so they're extremely efficient in

00:03:08,970 --> 00:03:15,720
terms of space and mutability the

00:03:12,480 --> 00:03:18,450
aggregates are responses to queries so

00:03:15,720 --> 00:03:22,950
they're the friendly json text that you

00:03:18,450 --> 00:03:24,989
want to consume by another service so

00:03:22,950 --> 00:03:26,850
one way we can go we can respond to

00:03:24,989 --> 00:03:29,610
queries go between states and aggregates

00:03:26,850 --> 00:03:31,200
is to convert each state into an

00:03:29,610 --> 00:03:33,060
aggregate to the simpler representation

00:03:31,200 --> 00:03:35,340
and then combine those together so

00:03:33,060 --> 00:03:37,080
require that every counter type or every

00:03:35,340 --> 00:03:39,330
metric type has a way to combine those

00:03:37,080 --> 00:03:41,310
aggregates and this seems like a

00:03:39,330 --> 00:03:43,350
reasonable thing to do these aggregate

00:03:41,310 --> 00:03:45,450
objects are much simpler to their much

00:03:43,350 --> 00:03:48,060
simpler schema so there be easier to

00:03:45,450 --> 00:03:50,880
implement a way to combine them however

00:03:48,060 --> 00:03:51,989
there's a problem with this approach and

00:03:50,880 --> 00:03:56,820
if you're sharp you may have picked up

00:03:51,989 --> 00:04:00,510
on it that's you can't cash this right

00:03:56,820 --> 00:04:02,610
if you were to cash the the projection

00:04:00,510 --> 00:04:04,320
and aggregation ahead of time you would

00:04:02,610 --> 00:04:06,560
be losing some of the data like you lose

00:04:04,320 --> 00:04:10,019
the temporal data in that in that

00:04:06,560 --> 00:04:11,700
efficient state representation and so

00:04:10,019 --> 00:04:14,390
you can't actually respond to queries

00:04:11,700 --> 00:04:17,160
correctly if you cash that ahead of time

00:04:14,390 --> 00:04:20,160
so the solution is to change the model

00:04:17,160 --> 00:04:21,299
so that you provide a way or you require

00:04:20,160 --> 00:04:23,610
that there's a way to combine these

00:04:21,299 --> 00:04:25,700
states together and by doing that you

00:04:23,610 --> 00:04:27,410
can actually cash those

00:04:25,700 --> 00:04:28,910
those combinations ahead of time and you

00:04:27,410 --> 00:04:32,240
can respond to queries almost instantly

00:04:28,910 --> 00:04:34,760
and changing the model in this way has

00:04:32,240 --> 00:04:37,790
another benefit in that you can actually

00:04:34,760 --> 00:04:40,340
do pipelining so instead of having the

00:04:37,790 --> 00:04:43,010
requirement that all of the incoming

00:04:40,340 --> 00:04:44,720
events have to be combined directly into

00:04:43,010 --> 00:04:45,950
the database when it's available you can

00:04:44,720 --> 00:04:47,180
actually convert them into partial

00:04:45,950 --> 00:04:48,350
States and then combine the stays

00:04:47,180 --> 00:04:52,190
together because you already have that

00:04:48,350 --> 00:04:54,320
primitive so these are examples are very

00:04:52,190 --> 00:04:58,880
specific to Taba but the the main

00:04:54,320 --> 00:05:00,620
takeaway is that the model the way that

00:04:58,880 --> 00:05:02,690
you treat data the way you flow data

00:05:00,620 --> 00:05:04,100
through a system could mean big

00:05:02,690 --> 00:05:05,690
differences in performance even if it's

00:05:04,100 --> 00:05:08,080
small differences in the way you handle

00:05:05,690 --> 00:05:10,430
it so the small difference between

00:05:08,080 --> 00:05:12,980
projecting and then combining versus

00:05:10,430 --> 00:05:14,840
combining and then projecting means that

00:05:12,980 --> 00:05:16,310
queries can be responded to in a matter

00:05:14,840 --> 00:05:23,300
of milliseconds as opposed to a matter

00:05:16,310 --> 00:05:26,570
of minutes so lesson number two state is

00:05:23,300 --> 00:05:28,880
hard maintaining consistent and durable

00:05:26,570 --> 00:05:30,650
state is a very difficult problem it's

00:05:28,880 --> 00:05:32,810
one of the most fundamental problems of

00:05:30,650 --> 00:05:34,730
computer science and there are a lot of

00:05:32,810 --> 00:05:37,970
tools out there that do this very well

00:05:34,730 --> 00:05:41,420
and whenever possible you want to not

00:05:37,970 --> 00:05:43,460
reinvent that wheel you want to offload

00:05:41,420 --> 00:05:45,230
the state into sub systems that are

00:05:43,460 --> 00:05:48,830
designed to handle it which are

00:05:45,230 --> 00:05:53,810
basically database systems or offload it

00:05:48,830 --> 00:05:55,790
to your clients this is the basic

00:05:53,810 --> 00:05:58,100
architecture of the the top of back-end

00:05:55,790 --> 00:06:00,860
so in the center there we have the

00:05:58,100 --> 00:06:03,290
cluster of Redis instances that store

00:06:00,860 --> 00:06:05,930
all of the state in the system it's a

00:06:03,290 --> 00:06:07,990
hardon sub service it uses master slave

00:06:05,930 --> 00:06:10,910
replication uses sentinels for failover

00:06:07,990 --> 00:06:13,550
does periodic snapshotting on off-site

00:06:10,910 --> 00:06:16,250
backups that small component of the

00:06:13,550 --> 00:06:18,350
entire system is very well protected and

00:06:16,250 --> 00:06:22,160
it uses a lot of the features of Redis

00:06:18,350 --> 00:06:23,960
to maintain that consistent State the

00:06:22,160 --> 00:06:25,580
rest of the system which composes of

00:06:23,960 --> 00:06:27,740
these front ends and back ends which run

00:06:25,580 --> 00:06:29,930
a number of instances of the tava server

00:06:27,740 --> 00:06:30,920
process are completely stateless and

00:06:29,930 --> 00:06:32,720
actually the front ends and back in to

00:06:30,920 --> 00:06:34,760
run the exact same code its exact same

00:06:32,720 --> 00:06:38,390
image the only difference is the traffic

00:06:34,760 --> 00:06:39,560
that they get sent and because those are

00:06:38,390 --> 00:06:41,720
totally stateless the own

00:06:39,560 --> 00:06:43,550
the actual state they have is whatever's

00:06:41,720 --> 00:06:45,380
in there working memory whatever request

00:06:43,550 --> 00:06:47,389
they're working on now and those

00:06:45,380 --> 00:06:49,060
requests are always stored somewhere

00:06:47,389 --> 00:06:52,490
else while they're being processed

00:06:49,060 --> 00:06:55,220
because of that this is very resistant

00:06:52,490 --> 00:06:56,750
to failure so if any of the front ends

00:06:55,220 --> 00:06:58,160
go down and the back ends go down it

00:06:56,750 --> 00:07:00,650
doesn't matter their traffic will just

00:06:58,160 --> 00:07:03,650
get simply rerouted somewhere else it

00:07:00,650 --> 00:07:07,040
also means we can very easily scale up

00:07:03,650 --> 00:07:09,919
and down based on the the current load

00:07:07,040 --> 00:07:12,200
so if we need to handle spike we can

00:07:09,919 --> 00:07:14,660
double the number of servers doesn't

00:07:12,200 --> 00:07:15,860
matter or if at the middle in the middle

00:07:14,660 --> 00:07:17,720
of the night you know when there's not a

00:07:15,860 --> 00:07:20,330
lot of traffic we can shut down half the

00:07:17,720 --> 00:07:22,010
cluster and we actually do code pushes

00:07:20,330 --> 00:07:23,120
in this way so we have a little script

00:07:22,010 --> 00:07:24,350
that when a machine boots up it

00:07:23,120 --> 00:07:26,389
downloads the latest code and starts

00:07:24,350 --> 00:07:27,860
running it to do a push just upload the

00:07:26,389 --> 00:07:30,020
latest bundle shut down the entire

00:07:27,860 --> 00:07:36,770
cluster start it back up and it starts

00:07:30,020 --> 00:07:40,400
up with the new code so lesson number

00:07:36,770 --> 00:07:43,280
three this is an interesting pattern

00:07:40,400 --> 00:07:46,640
that that comes about when when using

00:07:43,280 --> 00:07:48,919
green 'let's and generators they work

00:07:46,640 --> 00:07:57,950
extremely well together especially in

00:07:48,919 --> 00:07:59,419
processing pipelines of data so this

00:07:57,950 --> 00:08:01,580
pattern that occurs a lot in the tava

00:07:59,419 --> 00:08:04,520
code is the concept of an asynchronous

00:08:01,580 --> 00:08:06,440
iterator so basically what it is is you

00:08:04,520 --> 00:08:08,450
have this object which acts like an

00:08:06,440 --> 00:08:12,919
iterator technically it's a generator it

00:08:08,450 --> 00:08:15,470
has an input queue of objects that it's

00:08:12,919 --> 00:08:17,870
processing from a source iterator and

00:08:15,470 --> 00:08:20,150
then as a number of green lights that

00:08:17,870 --> 00:08:23,210
are acting as workers and an output Q

00:08:20,150 --> 00:08:26,270
that is exposed as the as the output of

00:08:23,210 --> 00:08:29,150
the iterator and the trick is that the

00:08:26,270 --> 00:08:30,530
in and out queues are of a fixed size so

00:08:29,150 --> 00:08:32,750
that when you try to put something into

00:08:30,530 --> 00:08:36,589
a full queue or remove from an empty one

00:08:32,750 --> 00:08:38,060
that current greenlit will block and

00:08:36,589 --> 00:08:40,550
then when green let's block what I do is

00:08:38,060 --> 00:08:42,800
I switch to another one immediately so

00:08:40,550 --> 00:08:47,180
you can actually chain these into a long

00:08:42,800 --> 00:08:49,520
series of processors that will basically

00:08:47,180 --> 00:08:51,740
jump around to wherever in the chain

00:08:49,520 --> 00:08:54,260
currently requires processing

00:08:51,740 --> 00:08:55,520
and you can because of the the way G

00:08:54,260 --> 00:08:58,970
event works you can actually hide some

00:08:55,520 --> 00:09:01,100
fairly complicated network work inside

00:08:58,970 --> 00:09:02,990
these these work or green lights and

00:09:01,100 --> 00:09:04,790
when you hit the network it'll jump

00:09:02,990 --> 00:09:07,550
somewhere else so you can do real-time

00:09:04,790 --> 00:09:10,310
processing on a stream of data without

00:09:07,550 --> 00:09:12,770
having to explicitly either do full

00:09:10,310 --> 00:09:14,209
batches or do synchronization between

00:09:12,770 --> 00:09:21,320
different points in the chain it kind of

00:09:14,209 --> 00:09:25,100
happens automatically so let last thing

00:09:21,320 --> 00:09:27,800
I want to talk about is so this is

00:09:25,100 --> 00:09:30,380
specific to C Python and I'm assuming a

00:09:27,800 --> 00:09:33,020
Linux system it does suffer from memory

00:09:30,380 --> 00:09:36,140
fragmentation and extra there a couple

00:09:33,020 --> 00:09:38,959
of really great talks earlier today

00:09:36,140 --> 00:09:40,459
about memory management in Python and I

00:09:38,959 --> 00:09:42,320
won't be able to go into nearly as much

00:09:40,459 --> 00:09:47,209
detail but I think it bears repeating

00:09:42,320 --> 00:09:50,170
here so fragmentation is when a process

00:09:47,209 --> 00:09:54,260
heap is inefficiently used that is the

00:09:50,170 --> 00:09:56,870
operating system will report a much

00:09:54,260 --> 00:09:59,329
larger and number in terms of the amount

00:09:56,870 --> 00:10:02,410
of memory used then the garbage

00:09:59,329 --> 00:10:04,610
collector will and they'll both be right

00:10:02,410 --> 00:10:06,709
and the reason for this is that you end

00:10:04,610 --> 00:10:10,790
up with lots of chunks of memory which

00:10:06,709 --> 00:10:12,589
are not not optimally usable so

00:10:10,790 --> 00:10:14,149
fragmentation happens by so say you

00:10:12,589 --> 00:10:16,910
start with a heap that looks like this

00:10:14,149 --> 00:10:21,010
it's perfectly allocated your

00:10:16,910 --> 00:10:23,420
application continues to execute and as

00:10:21,010 --> 00:10:26,690
probably execution you'll you'll free up

00:10:23,420 --> 00:10:29,029
some blocks of memory some blocks will

00:10:26,690 --> 00:10:31,970
be allocated and at some point you'll

00:10:29,029 --> 00:10:35,839
want to allocate an object of this

00:10:31,970 --> 00:10:38,149
yellow size here and that object is

00:10:35,839 --> 00:10:40,820
smaller than the total amount of free

00:10:38,149 --> 00:10:42,770
memory but because objects have to be

00:10:40,820 --> 00:10:45,560
contiguous in memory there's no one

00:10:42,770 --> 00:10:47,149
place to put it so you have to what ends

00:10:45,560 --> 00:10:48,380
up happening is that the interpreter

00:10:47,149 --> 00:10:51,560
will have to go back to the operating

00:10:48,380 --> 00:10:53,800
system request more heap and then put it

00:10:51,560 --> 00:10:53,800
at the end

00:10:56,500 --> 00:11:02,600
so that ways to fight fragmentation you

00:11:00,590 --> 00:11:05,420
want to avoid large numbers of small

00:11:02,600 --> 00:11:07,280
objects because small objects tend to

00:11:05,420 --> 00:11:11,180
get placed wherever there's space for

00:11:07,280 --> 00:11:12,560
them in the heap and that's mostly true

00:11:11,180 --> 00:11:14,300
there's some optimizations that the

00:11:12,560 --> 00:11:17,090
interpreter will actually do for four

00:11:14,300 --> 00:11:19,490
very common objects like tuples and

00:11:17,090 --> 00:11:21,020
lists but generally small objects will

00:11:19,490 --> 00:11:22,610
get placed arbitrarily throughout the

00:11:21,020 --> 00:11:26,020
heap and then when your request that

00:11:22,610 --> 00:11:29,150
generated them completes they'll get

00:11:26,020 --> 00:11:30,200
collected from arbitrary places in the

00:11:29,150 --> 00:11:32,840
heap you end up with lots of little

00:11:30,200 --> 00:11:34,400
spaces and this gets much worse if

00:11:32,840 --> 00:11:37,160
you're dealing with a combination of a

00:11:34,400 --> 00:11:38,900
few large objects in many small objects

00:11:37,160 --> 00:11:40,820
because then there's never a good place

00:11:38,900 --> 00:11:43,270
to put those few large objects we end up

00:11:40,820 --> 00:11:45,950
always having to keep growing the heap

00:11:43,270 --> 00:11:48,980
so another way to fight fragmentation is

00:11:45,950 --> 00:11:50,870
to minimize in flight data so less

00:11:48,980 --> 00:11:52,910
memory used means less memory fragmented

00:11:50,870 --> 00:11:57,350
and generators like I mentioned earlier

00:11:52,910 --> 00:11:59,330
are a really great use for this and also

00:11:57,350 --> 00:12:04,089
a good piece of advice in general is

00:11:59,330 --> 00:12:04,089
whenever possible reference don't copy

00:12:04,120 --> 00:12:09,650
so we use another technique in Taba

00:12:06,550 --> 00:12:11,480
where will actually use scythe on which

00:12:09,650 --> 00:12:13,370
it for those not familiar scythe on is a

00:12:11,480 --> 00:12:15,280
kind of a hybrid see Python language

00:12:13,370 --> 00:12:18,860
where you can switch between the two

00:12:15,280 --> 00:12:20,810
basically line to line so we use psyphon

00:12:18,860 --> 00:12:22,970
to go down to the sea level and do

00:12:20,810 --> 00:12:27,950
manual memory allocation on a particular

00:12:22,970 --> 00:12:29,690
type of object and we then hand those

00:12:27,950 --> 00:12:32,750
objects off to the GC and let it handle

00:12:29,690 --> 00:12:35,210
it normally so say you have this request

00:12:32,750 --> 00:12:37,310
coming in this this is a an HP a quest

00:12:35,210 --> 00:12:40,250
with some JSON data what you would

00:12:37,310 --> 00:12:42,920
normally do is decode that and it would

00:12:40,250 --> 00:12:44,270
generate a lots of little strings and

00:12:42,920 --> 00:12:45,980
little reference objects and they go

00:12:44,270 --> 00:12:48,380
wherever it can find them in some place

00:12:45,980 --> 00:12:52,370
for them in the heap what we do instead

00:12:48,380 --> 00:12:55,000
is that we decode it in saipan and then

00:12:52,370 --> 00:12:59,660
we allocate page size blocks of memory

00:12:55,000 --> 00:13:02,990
that point back to ranges of text in

00:12:59,660 --> 00:13:04,310
that original blob of data and at first

00:13:02,990 --> 00:13:06,650
pass that seems a little efficient

00:13:04,310 --> 00:13:08,810
because you could be allocating an

00:13:06,650 --> 00:13:09,800
entire 4k page just to hold a couple

00:13:08,810 --> 00:13:13,240
pointers

00:13:09,800 --> 00:13:16,610
but the the main takeaway is that by

00:13:13,240 --> 00:13:19,250
putting pieces of memory that are

00:13:16,610 --> 00:13:20,839
related next to each other you end up

00:13:19,250 --> 00:13:22,670
when you end up finishing that request

00:13:20,839 --> 00:13:24,980
in freeing the memory you get large

00:13:22,670 --> 00:13:27,649
continue to contiguous blocks of memory

00:13:24,980 --> 00:13:29,839
that are freed all at once and large

00:13:27,649 --> 00:13:31,339
blocks are more useful for storing more

00:13:29,839 --> 00:13:32,810
incoming data and there it's also a much

00:13:31,339 --> 00:13:37,910
higher chance that look at released back

00:13:32,810 --> 00:13:39,769
to the operating system more thing I

00:13:37,910 --> 00:13:42,260
want to talk about is the concept of

00:13:39,769 --> 00:13:43,970
ratcheting and that's kind of a

00:13:42,260 --> 00:13:46,130
pathological case of fragmentation

00:13:43,970 --> 00:13:48,140
caused by the fact that the heap has to

00:13:46,130 --> 00:13:49,490
be contiguous and the little asterisk

00:13:48,140 --> 00:13:52,760
beside contiguous because that's not a

00:13:49,490 --> 00:13:54,440
hundred percent true it depends on the

00:13:52,760 --> 00:13:55,610
platform and the allocator you're using

00:13:54,440 --> 00:13:59,600
and some alligators are worse than

00:13:55,610 --> 00:14:01,990
others but for Python on Linux using the

00:13:59,600 --> 00:14:04,730
standard allocator this is mostly true

00:14:01,990 --> 00:14:07,010
so say you have this heap it's fairly

00:14:04,730 --> 00:14:09,440
efficiently allocated and you get a

00:14:07,010 --> 00:14:10,880
request that requires you to allocate a

00:14:09,440 --> 00:14:12,290
really big chunk of memory so you have

00:14:10,880 --> 00:14:14,649
some really big piece of text in eat the

00:14:12,290 --> 00:14:16,940
store so it goes to the operating system

00:14:14,649 --> 00:14:19,970
requests a whole bunch of heap and puts

00:14:16,940 --> 00:14:21,470
it in there and everything's fine so you

00:14:19,970 --> 00:14:23,720
prosecute continue processing your

00:14:21,470 --> 00:14:27,110
request and at some point some

00:14:23,720 --> 00:14:29,480
background process runs and allocates

00:14:27,110 --> 00:14:33,170
some small object and decides to place

00:14:29,480 --> 00:14:35,300
it right at the end of the heap now your

00:14:33,170 --> 00:14:37,399
request completes it collects that big

00:14:35,300 --> 00:14:38,690
blob of data but that little object that

00:14:37,399 --> 00:14:41,300
was allocated in the background sticks

00:14:38,690 --> 00:14:42,680
around and because there's that one

00:14:41,300 --> 00:14:46,010
little object right at the end of the

00:14:42,680 --> 00:14:47,089
heap it can't shrink so you end up in a

00:14:46,010 --> 00:14:49,160
situation where you have this really

00:14:47,089 --> 00:14:50,329
terrible memory usage and you can't free

00:14:49,160 --> 00:14:54,410
it to the operating system because of

00:14:50,329 --> 00:14:56,180
one tiny object and this is just a basic

00:14:54,410 --> 00:14:59,329
limitation of the way C Python is

00:14:56,180 --> 00:15:04,750
implemented it can't move objects around

00:14:59,329 --> 00:15:08,149
in memory once they're allocated so yeah

00:15:04,750 --> 00:15:11,180
basically there's the there's no way to

00:15:08,149 --> 00:15:17,120
move it so you can't shrink the heat so

00:15:11,180 --> 00:15:19,670
some ways to fight this so avoid

00:15:17,120 --> 00:15:23,939
persistent objects sockets are really

00:15:19,670 --> 00:15:26,279
common offenders here they especially in

00:15:23,939 --> 00:15:28,499
action pools so when your applications

00:15:26,279 --> 00:15:30,509
under its highest load that's typically

00:15:28,499 --> 00:15:31,949
when it's using the most memory and when

00:15:30,509 --> 00:15:33,839
there's most the most pressure on the

00:15:31,949 --> 00:15:35,489
connection pool if the pool is

00:15:33,839 --> 00:15:38,429
implemented poorly it'll actually

00:15:35,489 --> 00:15:40,229
allocate a new socket at that point it

00:15:38,429 --> 00:15:42,389
could stick it near the end of the heap

00:15:40,229 --> 00:15:43,709
and if it does that that object will

00:15:42,389 --> 00:15:46,529
never die and you'll never be able to

00:15:43,709 --> 00:15:48,449
shrink your heap down so you want to

00:15:46,529 --> 00:15:49,589
avoid persistent objects as much as

00:15:48,449 --> 00:15:51,479
possible everything should have a

00:15:49,589 --> 00:15:54,209
lifetime caches are another good example

00:15:51,479 --> 00:15:55,949
here everything in a cache should have a

00:15:54,209 --> 00:15:59,909
finite lifetime you should always try to

00:15:55,949 --> 00:16:01,739
clear out your anything cached anything

00:15:59,909 --> 00:16:04,559
that has to be persistent should be

00:16:01,739 --> 00:16:06,209
created when the application starts up

00:16:04,559 --> 00:16:08,429
so as soon as possible before you start

00:16:06,209 --> 00:16:10,589
processing any data and by doing that

00:16:08,429 --> 00:16:12,689
you'll get pretty much all of your

00:16:10,589 --> 00:16:14,609
objects fairly compactly down near the

00:16:12,689 --> 00:16:16,439
bottom of the heap and so you don't get

00:16:14,609 --> 00:16:19,919
this the issue where you have long lived

00:16:16,439 --> 00:16:22,169
objects near the top and obviously let

00:16:19,919 --> 00:16:25,979
the avoid letting the heap grow in the

00:16:22,169 --> 00:16:28,019
first place again less memory means less

00:16:25,979 --> 00:16:33,989
fragmented memory means less ratcheted

00:16:28,019 --> 00:16:36,509
memory so yeah that's all the lessons I

00:16:33,989 --> 00:16:38,879
had for today the code for Taba is up on

00:16:36,509 --> 00:16:46,349
github if anybody's interested and I'd

00:16:38,879 --> 00:16:47,970
be happy to answering questions oh yeah

00:16:46,349 --> 00:16:50,299
for any questions theres the mic in the

00:16:47,970 --> 00:16:50,299
middle here

00:17:01,870 --> 00:17:05,829
so I wasn't able to make it to the

00:17:03,970 --> 00:17:07,870
earlier talks on garbage collection but

00:17:05,829 --> 00:17:10,540
you were talking about this

00:17:07,870 --> 00:17:13,620
fragmentation problem C Python is the

00:17:10,540 --> 00:17:18,490
same true of pi PI are you aware of that

00:17:13,620 --> 00:17:20,170
it's a good question it's not it well

00:17:18,490 --> 00:17:24,430
it's fragmentations always an issue in

00:17:20,170 --> 00:17:26,920
dynamic languages but pi pi is able to

00:17:24,430 --> 00:17:29,500
move things around in memory so it can

00:17:26,920 --> 00:17:31,750
do periodic compaction and you don't get

00:17:29,500 --> 00:17:32,980
nearly as bad performance or you don't

00:17:31,750 --> 00:17:38,370
get nearly as bad fragmentation or

00:17:32,980 --> 00:17:41,320
ratcheting you hi thanks for the talk

00:17:38,370 --> 00:17:43,840
we've done a little bit of benchmarking

00:17:41,320 --> 00:17:48,370
with a G event in terms of concurrency

00:17:43,840 --> 00:17:51,309
and in a very artificial kind of sense

00:17:48,370 --> 00:17:54,880
like most benchmarks we ran into like a

00:17:51,309 --> 00:17:57,429
CPU like I could just use so much cpu

00:17:54,880 --> 00:17:59,350
that was the first thing it ran into did

00:17:57,429 --> 00:18:01,720
you seem like you're doing lots of

00:17:59,350 --> 00:18:03,730
concurrent requests with G then what was

00:18:01,720 --> 00:18:06,100
the I guess the first thing that you ran

00:18:03,730 --> 00:18:08,230
into that stopped you going more

00:18:06,100 --> 00:18:11,260
concurrently and how did you get around

00:18:08,230 --> 00:18:13,809
it so our bottleneck was actually

00:18:11,260 --> 00:18:16,210
network I oh and for those kind of use

00:18:13,809 --> 00:18:17,550
cases the event is great because it I

00:18:16,210 --> 00:18:21,010
mean it's designed specifically for

00:18:17,550 --> 00:18:22,870
switching context on Io personally I

00:18:21,010 --> 00:18:26,200
found that the event is pretty good for

00:18:22,870 --> 00:18:28,150
any worker that's not cpu-bound if you

00:18:26,200 --> 00:18:31,780
are cpu-bound though it's actually it

00:18:28,150 --> 00:18:34,480
has a lot of problems because it it uses

00:18:31,780 --> 00:18:35,860
a non-preemptive model so you end up you

00:18:34,480 --> 00:18:37,720
can actually end up in a in a state

00:18:35,860 --> 00:18:38,860
where one green light will hog the CPU

00:18:37,720 --> 00:18:40,330
for a long time and that actually

00:18:38,860 --> 00:18:42,010
prevents anything else from happening

00:18:40,330 --> 00:18:45,550
including responding to other requests

00:18:42,010 --> 00:18:47,700
so G event is you know great for network

00:18:45,550 --> 00:18:50,700
bound not great for CPU bound loads

00:18:47,700 --> 00:18:50,700
thanks

00:18:54,800 --> 00:19:00,290
yeah well there are no other questions

00:18:57,500 --> 00:19:02,740
that wanted to just thank you for free

00:19:00,290 --> 00:19:02,740

YouTube URL: https://www.youtube.com/watch?v=k8YtaSMv-Nc


