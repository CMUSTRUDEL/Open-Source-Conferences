Title: How the PyPy JIT works
Publication date: 2012-04-29
Playlist: PyCon 2012
Description: 
	Benjamin Peterson
The Python community is abuzz about the major speed gains PyPy can offer pure Python code. But how does PyPy JIT actually work? This talk will discuss how the PyPy JIT is implemented. It will include descriptions of the tracing, opt
Captions: 
	00:00:01,399 --> 00:00:08,130
so yeah I have to say David Beasley made

00:00:05,400 --> 00:00:10,610
me both feel proud that I kind of

00:00:08,130 --> 00:00:14,990
understand what's happening in pi PI and

00:00:10,610 --> 00:00:21,029
bad because it's not actually that hard

00:00:14,990 --> 00:00:23,820
so this is both an advanced listed as

00:00:21,029 --> 00:00:25,619
advanced and on the expert track so if

00:00:23,820 --> 00:00:27,740
you don't know what pi PI is this may

00:00:25,619 --> 00:00:31,349
not be the best talk for you

00:00:27,740 --> 00:00:33,989
however David's keynote which I wasn't

00:00:31,349 --> 00:00:38,850
expecting probably prepared most of

00:00:33,989 --> 00:00:41,430
PyCon for this I'd like to start with a

00:00:38,850 --> 00:00:45,000
joke which I stole from a presentation I

00:00:41,430 --> 00:00:48,809
saw by Kenneth writes and he he quotes

00:00:45,000 --> 00:00:50,489
our favorite line from the Zen of Python

00:00:48,809 --> 00:00:52,980
if the implementation is hard to explain

00:00:50,489 --> 00:00:56,059
it's a bad idea

00:00:52,980 --> 00:00:56,059
except for pi PI

00:00:58,550 --> 00:01:06,090
ok so I'll start with the overall design

00:01:02,460 --> 00:01:07,650
principles of the pipe I did one of the

00:01:06,090 --> 00:01:09,900
most important ones is that it's

00:01:07,650 --> 00:01:13,229
interpreter agnostic we do not actually

00:01:09,900 --> 00:01:16,259
have a Python JIT we have a JIT

00:01:13,229 --> 00:01:18,509
generator which generates jits for

00:01:16,259 --> 00:01:23,220
interpreters written in our Python for

00:01:18,509 --> 00:01:27,509
the pi PI translation tool chain so this

00:01:23,220 --> 00:01:28,890
works pretty well but the generator is

00:01:27,509 --> 00:01:33,270
not perfect and it cannot generate a

00:01:28,890 --> 00:01:34,590
facet immediately or the fastest yet so

00:01:33,270 --> 00:01:36,869
we have to allow the language

00:01:34,590 --> 00:01:40,829
implementation the interpreter to be

00:01:36,869 --> 00:01:44,909
able to help the JIT with type inference

00:01:40,829 --> 00:01:48,899
for example and our JIT and pi PI is a

00:01:44,909 --> 00:01:52,740
tracing JIT which means that we take to

00:01:48,899 --> 00:01:55,920
heart the idea that 90% of your time is

00:01:52,740 --> 00:01:58,049
spent at 10% of your code and we compile

00:01:55,920 --> 00:02:01,110
only the code which it seems like is

00:01:58,049 --> 00:02:03,540
tape it is being run repeatedly through

00:02:01,110 --> 00:02:09,270
the interpreter which means hot

00:02:03,540 --> 00:02:11,160
functions or loops and optimizing

00:02:09,270 --> 00:02:12,840
dynamic languages for assembly

00:02:11,160 --> 00:02:14,810
generation is

00:02:12,840 --> 00:02:18,560
a bit different than static languages

00:02:14,810 --> 00:02:21,450
like C I'll talk about about automation

00:02:18,560 --> 00:02:23,430
optimizations later but essentially what

00:02:21,450 --> 00:02:24,780
we do is we have to remove all the

00:02:23,430 --> 00:02:28,680
indirection which is in dynamic

00:02:24,780 --> 00:02:33,810
languages because in Python you get lots

00:02:28,680 --> 00:02:36,680
of power from being able to dynamically

00:02:33,810 --> 00:02:40,080
sign attributes to instances for example

00:02:36,680 --> 00:02:43,410
but it also adds a lot of interpretation

00:02:40,080 --> 00:02:46,440
overhead and last thing we try to do is

00:02:43,410 --> 00:02:51,170
make our JIT fastest on code which is

00:02:46,440 --> 00:02:53,790
simple an idiomatic in Python and so

00:02:51,170 --> 00:02:56,790
well you can put a million different

00:02:53,790 --> 00:02:58,349
types in a list that code is not

00:02:56,790 --> 00:03:03,720
necessarily going to be the fastest on

00:02:58,349 --> 00:03:07,610
pi PI JIT also things like introspection

00:03:03,720 --> 00:03:10,980
like intersecting the stack frames and

00:03:07,610 --> 00:03:14,879
debugging information is not the fastest

00:03:10,980 --> 00:03:16,620
on the pi PI chip so the several

00:03:14,879 --> 00:03:19,440
components of the pipet which I'm going

00:03:16,620 --> 00:03:21,900
to be talking about is translation where

00:03:19,440 --> 00:03:24,480
they actually generate the JIT from the

00:03:21,900 --> 00:03:25,950
description of the interpreter and then

00:03:24,480 --> 00:03:29,010
I'm going to talk about how the JIT

00:03:25,950 --> 00:03:32,010
works at runtime specifically tracing

00:03:29,010 --> 00:03:37,670
how we optimized the result of tracing

00:03:32,010 --> 00:03:40,170
and assembly generation so to add a a

00:03:37,670 --> 00:03:43,760
JIT to your interpreter and our Python

00:03:40,170 --> 00:03:46,340
is actually very simple you just need to

00:03:43,760 --> 00:03:49,260
modify your interpreter source to add

00:03:46,340 --> 00:03:50,790
two special functions which the our

00:03:49,260 --> 00:03:54,269
Python translator will write later

00:03:50,790 --> 00:03:58,709
recognize and I'll show you later how

00:03:54,269 --> 00:04:00,630
this works in the Python interpreter the

00:03:58,709 --> 00:04:05,579
two hits are called can interject and

00:04:00,630 --> 00:04:08,849
JIT merge point merge point tells the

00:04:05,579 --> 00:04:10,859
JIT when it's safe to return to the

00:04:08,849 --> 00:04:13,430
interpreter and usually that's at the

00:04:10,859 --> 00:04:16,919
head of the bytecode dispatch loop and

00:04:13,430 --> 00:04:20,130
can enter jet surprisingly tells the JIT

00:04:16,919 --> 00:04:22,770
when it should enter and this is at the

00:04:20,130 --> 00:04:24,659
beginning of loops once a loop has been

00:04:22,770 --> 00:04:26,250
closed where they could enter and the

00:04:24,659 --> 00:04:28,230
jet also needs some

00:04:26,250 --> 00:04:30,120
arguments which is basically the data

00:04:28,230 --> 00:04:31,980
which the interpreter is working with

00:04:30,120 --> 00:04:34,470
and these are classified into two

00:04:31,980 --> 00:04:37,860
categories green variables and red

00:04:34,470 --> 00:04:41,210
variables and green green variables are

00:04:37,860 --> 00:04:45,180
constants they never changed during the

00:04:41,210 --> 00:04:48,140
the loop and so they're let me show you

00:04:45,180 --> 00:04:51,390
what's in there the Python interpreter

00:04:48,140 --> 00:04:53,280
so on the Python interpreter the green

00:04:51,390 --> 00:04:54,770
the constant arguments are the code

00:04:53,280 --> 00:04:57,720
object and the instruction pointer

00:04:54,770 --> 00:04:59,730
because when we enter the JIT those are

00:04:57,720 --> 00:05:02,340
always going to become constant for a

00:04:59,730 --> 00:05:04,620
given loop and then the Reds are the

00:05:02,340 --> 00:05:06,780
frame object and the execution time text

00:05:04,620 --> 00:05:09,960
in the Python interpreter the frame

00:05:06,780 --> 00:05:12,419
object is an object which stores the

00:05:09,960 --> 00:05:15,630
local variables and the value stack for

00:05:12,419 --> 00:05:18,270
a Python frame and the execution context

00:05:15,630 --> 00:05:21,870
stores thread local daily data basically

00:05:18,270 --> 00:05:23,940
like exception state so this is a

00:05:21,870 --> 00:05:26,490
simplified view of how Ian's have

00:05:23,940 --> 00:05:27,090
inserted the JIT hooks into the Python

00:05:26,490 --> 00:05:30,840
interpreter

00:05:27,090 --> 00:05:33,450
that while true loop is the bytecode

00:05:30,840 --> 00:05:35,580
dispatch loop so I have a JIT merge

00:05:33,450 --> 00:05:39,450
point every time we go to dispatch of

00:05:35,580 --> 00:05:42,210
byte byte code and the jumpring absolute

00:05:39,450 --> 00:05:45,330
by code is a Python bytecode which tells

00:05:42,210 --> 00:05:47,400
the interpreter to jump to some other by

00:05:45,330 --> 00:05:49,590
code and it's at the end of loops so

00:05:47,400 --> 00:05:58,440
that's how we know that we've ended a

00:05:49,590 --> 00:06:00,300
loop so to get a JIT we we have these

00:05:58,440 --> 00:06:04,560
hints in the source and then we go to

00:06:00,300 --> 00:06:07,320
translate our interpreter and as Steve's

00:06:04,560 --> 00:06:09,930
Davy Lee said what happens is we have

00:06:07,320 --> 00:06:13,110
the interpreter halfway through

00:06:09,930 --> 00:06:15,479
translation in a bunch of low-level flow

00:06:13,110 --> 00:06:19,410
graphs there in a modified single

00:06:15,479 --> 00:06:21,510
assignment form compiler intermediate

00:06:19,410 --> 00:06:27,570
representation and they have low-level

00:06:21,510 --> 00:06:29,820
type annotations on them so like it says

00:06:27,570 --> 00:06:31,860
add these two integers or add these two

00:06:29,820 --> 00:06:36,000
floats or access some value of a

00:06:31,860 --> 00:06:37,620
structure and so what we have this flow

00:06:36,000 --> 00:06:39,449
graph description of the interpreter and

00:06:37,620 --> 00:06:43,919
what we do

00:06:39,449 --> 00:06:47,479
is the generator takes these and it

00:06:43,919 --> 00:06:49,949
basically serializes them into a

00:06:47,479 --> 00:06:53,219
representation which we can write into

00:06:49,949 --> 00:06:54,719
the binary so what happens that run time

00:06:53,219 --> 00:06:57,110
is we'll have a description of the

00:06:54,719 --> 00:06:59,520
binary in terms of the serialized

00:06:57,110 --> 00:07:02,309
representations of the flow graphs which

00:06:59,520 --> 00:07:05,370
we call JIT codes and we'll see how

00:07:02,309 --> 00:07:07,319
those are used and so now I'd like to

00:07:05,370 --> 00:07:11,460
introduce the example I will be using to

00:07:07,319 --> 00:07:17,240
show how the piped JIT works it's a very

00:07:11,460 --> 00:07:17,240
dumb prime allottee test trial division

00:07:17,569 --> 00:07:20,939
but it works well with a pipe idea

00:07:19,949 --> 00:07:25,499
because we're really good at it

00:07:20,939 --> 00:07:26,009
optimizing integers so this is what the

00:07:25,499 --> 00:07:29,999
bytecode

00:07:26,009 --> 00:07:34,909
looks like I just wanted to point out

00:07:29,999 --> 00:07:34,909
that there's there's a lot of

00:07:35,839 --> 00:07:40,499
manipulation of the stack like these

00:07:38,219 --> 00:07:42,749
this load fast dock code these load fast

00:07:40,499 --> 00:07:45,210
off codes they they load these variables

00:07:42,749 --> 00:07:47,219
and they push it onto the stack and then

00:07:45,210 --> 00:07:53,460
this operation will pop them off the

00:07:47,219 --> 00:07:55,999
stack and and compare them and so we're

00:07:53,460 --> 00:07:58,800
gonna see later how the pipe idea

00:07:55,999 --> 00:08:03,599
optimizes out all this miscellaneous

00:07:58,800 --> 00:08:06,149
stack manipulation so now we have a PI

00:08:03,599 --> 00:08:09,889
pi binary and we're running it on some

00:08:06,149 --> 00:08:13,919
Python code say my perm ality test and

00:08:09,889 --> 00:08:17,430
the intern the runtime part of the JIT

00:08:13,919 --> 00:08:19,919
keeps a track of every every loop and

00:08:17,430 --> 00:08:26,520
every function call which happens in the

00:08:19,919 --> 00:08:29,279
interpreter and once this a threshold is

00:08:26,520 --> 00:08:30,749
reached with the amount of the count the

00:08:29,279 --> 00:08:32,880
number of times the loop of the function

00:08:30,749 --> 00:08:35,969
has been entered then we entered the

00:08:32,880 --> 00:08:37,589
tracing portion legit it's slightly more

00:08:35,969 --> 00:08:39,690
complicated than that because we want to

00:08:37,589 --> 00:08:41,940
avoid having massive amounts of

00:08:39,690 --> 00:08:47,430
compilation at one time so we slightly

00:08:41,940 --> 00:08:49,529
fudge the thresholds so now we're at the

00:08:47,430 --> 00:08:51,720
tracing partion and we use something

00:08:49,529 --> 00:08:53,220
called the meta interpreter and the

00:08:51,720 --> 00:08:55,709
reason it's called the meta interpreter

00:08:53,220 --> 00:08:57,240
as we take the save JIT codes the

00:08:55,709 --> 00:08:59,009
serialized representation of the

00:08:57,240 --> 00:09:01,560
interpreter which we generated a

00:08:59,009 --> 00:09:04,680
translation and we actually actually

00:09:01,560 --> 00:09:07,290
executed at runtime so what happens is

00:09:04,680 --> 00:09:09,240
we have an interpreter for the JIT codes

00:09:07,290 --> 00:09:12,420
which is interpreting the interpreter

00:09:09,240 --> 00:09:15,300
that's the meta interpreter and what the

00:09:12,420 --> 00:09:19,500
meta interpreter does is it records the

00:09:15,300 --> 00:09:24,449
all the operations that execute tracing

00:09:19,500 --> 00:09:27,240
the loop onto a giant list and the

00:09:24,449 --> 00:09:29,939
values of the program are are stored as

00:09:27,240 --> 00:09:31,920
boxes they represent low low type level

00:09:29,939 --> 00:09:35,129
types like pointers integers and floats

00:09:31,920 --> 00:09:37,199
and so the meta interpreter is tracing

00:09:35,129 --> 00:09:39,089
and eventually it'll reach the jump

00:09:37,199 --> 00:09:41,790
absolute byte code and we'll know the

00:09:39,089 --> 00:09:49,139
loop has been closed and so what we have

00:09:41,790 --> 00:09:51,060
is a big linear history of the of the

00:09:49,139 --> 00:09:53,670
the loop or function which we've traced

00:09:51,060 --> 00:09:55,259
and it's important to note that the

00:09:53,670 --> 00:09:57,209
trace is almost completely linear

00:09:55,259 --> 00:10:01,680
because as the meta interpreter is

00:09:57,209 --> 00:10:04,019
tracing it enters all the calls which

00:10:01,680 --> 00:10:07,829
the interpreter would bake and it

00:10:04,019 --> 00:10:11,879
flattens those out into a giant list so

00:10:07,829 --> 00:10:15,389
there's almost no calls out of the out

00:10:11,879 --> 00:10:17,819
of the trace so I've tried to depict

00:10:15,389 --> 00:10:19,620
this with a diagram so the basic idea is

00:10:17,819 --> 00:10:22,129
you have a the Python interpreter which

00:10:19,620 --> 00:10:24,839
is running mariely along it decides to

00:10:22,129 --> 00:10:27,149
start the JIT is invoked and then the

00:10:24,839 --> 00:10:29,759
meta interpreter uses the JIT codes to

00:10:27,149 --> 00:10:31,740
run the Python interpreter and it

00:10:29,759 --> 00:10:36,089
generates the intermediate

00:10:31,740 --> 00:10:39,449
representation which is language which

00:10:36,089 --> 00:10:42,329
we use which is which traces are written

00:10:39,449 --> 00:10:43,949
in in the Python interpreter one other

00:10:42,329 --> 00:10:45,180
thing which is important to mention is

00:10:43,949 --> 00:10:48,389
something called guards

00:10:45,180 --> 00:10:50,519
so as the meta interpreter is tracing

00:10:48,389 --> 00:10:51,839
it's based on runtime information we're

00:10:50,519 --> 00:10:54,269
running one iteration of the loop

00:10:51,839 --> 00:10:55,769
basically so we're going to be making

00:10:54,269 --> 00:10:57,809
some assumptions we're not going to be

00:10:55,769 --> 00:11:00,629
entering every conditional which is in

00:10:57,809 --> 00:11:02,429
the loop for example in the prime ality

00:11:00,629 --> 00:11:06,170
test there's the condition which says if

00:11:02,429 --> 00:11:06,170
this integer divides the

00:11:06,310 --> 00:11:13,420
and then we're gonna leave the loop and

00:11:09,699 --> 00:11:15,040
so all that the trace after we've taken

00:11:13,420 --> 00:11:16,600
this conditions would be invalid if that

00:11:15,040 --> 00:11:18,610
condition was not true anymore

00:11:16,600 --> 00:11:22,410
so what we do is we insert something

00:11:18,610 --> 00:11:25,480
called guards into the trace and guards

00:11:22,410 --> 00:11:30,970
make sure that the trace continues to be

00:11:25,480 --> 00:11:33,339
valid as or it tells us what to trace

00:11:30,970 --> 00:11:35,980
well under what conditions the trace

00:11:33,339 --> 00:11:38,350
will be invalid and so I've listed some

00:11:35,980 --> 00:11:40,540
of the things which trait

00:11:38,350 --> 00:11:42,819
you'll get guards for like adding

00:11:40,540 --> 00:11:46,629
integers you'll get a guard after it

00:11:42,819 --> 00:11:48,310
that the operation didn't overflow so

00:11:46,629 --> 00:11:53,740
this is what the prime allottee test

00:11:48,310 --> 00:11:56,319
looks in the Gir so the this list thing

00:11:53,740 --> 00:11:59,649
up on the top is the arguments to the

00:11:56,319 --> 00:12:01,540
trace and it's it's slightly obscure

00:11:59,649 --> 00:12:04,509
what these mean basically they're all

00:12:01,540 --> 00:12:09,939
values which are in the the Python value

00:12:04,509 --> 00:12:11,620
stack and the operations of the Python

00:12:09,939 --> 00:12:15,149
value stack have been removed in this

00:12:11,620 --> 00:12:18,490
trace I'll talk about that a little so

00:12:15,149 --> 00:12:21,069
here here's these this these guards here

00:12:18,490 --> 00:12:23,610
are basically ensuring that none of the

00:12:21,069 --> 00:12:26,350
that the variables are still defined

00:12:23,610 --> 00:12:32,019
they're not equal to none and we don't

00:12:26,350 --> 00:12:36,029
get an undefined local exception so this

00:12:32,019 --> 00:12:38,290
is completely unauthorized I have to add

00:12:36,029 --> 00:12:41,829
so this is the part where I actually

00:12:38,290 --> 00:12:46,689
compute the Modelo you can see that

00:12:41,829 --> 00:12:48,430
already there's no there's no we didn't

00:12:46,689 --> 00:12:51,610
generate operations for this load trace

00:12:48,430 --> 00:12:55,059
oper this load fast operation because

00:12:51,610 --> 00:12:58,980
we've removed value stack operations and

00:12:55,059 --> 00:13:05,319
so you'll notice that the binary Modelo

00:12:58,980 --> 00:13:07,059
modulo operation is rather long and

00:13:05,319 --> 00:13:11,500
complicated and the reason for that is a

00:13:07,059 --> 00:13:14,319
bit technical because the the semantics

00:13:11,500 --> 00:13:18,010
of modulo rounding are different in

00:13:14,319 --> 00:13:19,900
Python and C so we have to

00:13:18,010 --> 00:13:24,450
have some extra operations to correct

00:13:19,900 --> 00:13:30,310
that and here's the end of the loop

00:13:24,450 --> 00:13:36,190
basically we check if we leave we add to

00:13:30,310 --> 00:13:42,460
the loop counter oh sorry we this is the

00:13:36,190 --> 00:13:44,950
end one thing to notice is that this is

00:13:42,460 --> 00:13:46,570
creating a new integer object and so

00:13:44,950 --> 00:13:47,800
it's actually allocating memory and

00:13:46,570 --> 00:13:52,060
we're gonna see we're gonna be able to

00:13:47,800 --> 00:13:55,930
remove that so now we hand the JIT trace

00:13:52,060 --> 00:13:58,180
off to the optimizer and JIT

00:13:55,930 --> 00:14:00,460
optimizations are different as I alluded

00:13:58,180 --> 00:14:03,490
to earlier than classical compiler

00:14:00,460 --> 00:14:05,860
optimizations generally one reason for

00:14:03,490 --> 00:14:07,480
this is that we are so we are limited by

00:14:05,860 --> 00:14:10,510
speed we don't want to spend three

00:14:07,480 --> 00:14:14,350
seconds optimizing it as fast as GCC

00:14:10,510 --> 00:14:18,000
possibly could because no one wants to

00:14:14,350 --> 00:14:20,500
program to stop for three seconds

00:14:18,000 --> 00:14:22,720
however we do some basic compiler

00:14:20,500 --> 00:14:25,840
optimizations strengthed Junction we

00:14:22,720 --> 00:14:29,200
check integer bonds but the most

00:14:25,840 --> 00:14:31,060
important optimizations we do are

00:14:29,200 --> 00:14:32,680
primarily responsible for removing the

00:14:31,060 --> 00:14:34,510
indirection and the interpreter and

00:14:32,680 --> 00:14:37,360
those are virtuals and virtualize

00:14:34,510 --> 00:14:38,020
opposed so the banks the basic idea of

00:14:37,360 --> 00:14:40,570
virtuals

00:14:38,020 --> 00:14:42,850
is that if we have an object which is

00:14:40,570 --> 00:14:44,920
allocated in the trace that we don't

00:14:42,850 --> 00:14:47,620
have to l and it is not passed to

00:14:44,920 --> 00:14:49,290
anything else outside of the trace that

00:14:47,620 --> 00:14:52,360
we don't have to allocate it and we can

00:14:49,290 --> 00:14:55,290
flatten out its fields and perhaps store

00:14:52,360 --> 00:14:59,440
them in registers or on the stack

00:14:55,290 --> 00:15:02,920
instead of in the heap and so what this

00:14:59,440 --> 00:15:05,800
does is it not only avoids allocating

00:15:02,920 --> 00:15:08,740
stuff but it also removes boxing for

00:15:05,800 --> 00:15:11,140
example all the you saw the integer

00:15:08,740 --> 00:15:13,930
types in pi pi are wrapped in this

00:15:11,140 --> 00:15:16,060
object called w integer int object

00:15:13,930 --> 00:15:17,830
that's actually an object which has to

00:15:16,060 --> 00:15:21,160
be allocated if you're just running on

00:15:17,830 --> 00:15:24,240
the interpreter but we can remove the

00:15:21,160 --> 00:15:24,240
boxing of that and

00:15:24,940 --> 00:15:30,830
and thus significantly speeded up

00:15:28,820 --> 00:15:32,540
there's other thing as virtualize bulls

00:15:30,830 --> 00:15:34,370
and virtue eliza Buhl's are the thing

00:15:32,540 --> 00:15:38,120
which were responsible for removing the

00:15:34,370 --> 00:15:39,740
stack access and they're almost like

00:15:38,120 --> 00:15:42,110
virtuals but they're allowed to escape

00:15:39,740 --> 00:15:46,670
the trace meaning they might possibly be

00:15:42,110 --> 00:15:49,070
passed out of the out of the trace and

00:15:46,670 --> 00:15:54,280
so we need a lot of extra bookkeeping to

00:15:49,070 --> 00:15:56,740
keep track of those and the final thing

00:15:54,280 --> 00:15:59,500
optimization we do is unrolling

00:15:56,740 --> 00:16:01,910
basically in the simple case we have

00:15:59,500 --> 00:16:04,820
well happiness will have will generate

00:16:01,910 --> 00:16:07,670
two iterations of the loop the first one

00:16:04,820 --> 00:16:09,080
will compute all the loop invariants the

00:16:07,670 --> 00:16:11,030
things which don't change throughout the

00:16:09,080 --> 00:16:14,150
course of the loop and it might read

00:16:11,030 --> 00:16:17,240
some fields out of structures or the

00:16:14,150 --> 00:16:18,950
frame which we we which won't change

00:16:17,240 --> 00:16:21,860
over the course of the loop and then the

00:16:18,950 --> 00:16:23,570
second iteration it will at the end of

00:16:21,860 --> 00:16:26,720
it will jump to the second iteration and

00:16:23,570 --> 00:16:29,960
the second iteration will be completely

00:16:26,720 --> 00:16:35,990
optimized and it won't have any of these

00:16:29,960 --> 00:16:37,460
extraneous field accesses and so now I'm

00:16:35,990 --> 00:16:41,270
going to show you what optimizations

00:16:37,460 --> 00:16:44,450
have done to the prime allottee example

00:16:41,270 --> 00:16:47,620
with something called the JIT beer which

00:16:44,450 --> 00:16:52,490
is a fun toy which we have for showing

00:16:47,620 --> 00:16:55,790
JIT ir so i've made the font really big

00:16:52,490 --> 00:17:04,459
so it's usually less ugly but so this is

00:16:55,790 --> 00:17:05,450
the oh sorry forgot to start it there we

00:17:04,459 --> 00:17:09,390
go

00:17:05,450 --> 00:17:13,380
so this is the legit viewer it shows you

00:17:09,390 --> 00:17:15,480
the different layers of the JIT so we

00:17:13,380 --> 00:17:19,830
have the source code the Python bytecode

00:17:15,480 --> 00:17:21,480
and finally the JIT I are here so you

00:17:19,830 --> 00:17:23,550
can see that the many numerous

00:17:21,480 --> 00:17:25,260
operations which I saw when you showed

00:17:23,550 --> 00:17:29,100
you the unoptimized version have been

00:17:25,260 --> 00:17:35,580
roofed no more integer objects are being

00:17:29,100 --> 00:17:37,740
allocated and the so everything can now

00:17:35,580 --> 00:17:39,510
when we generate assembly can be stored

00:17:37,740 --> 00:17:47,130
immediately in registers and on the

00:17:39,510 --> 00:17:49,590
stack so now it's finally time to

00:17:47,130 --> 00:17:52,580
generate assembly and generating

00:17:49,590 --> 00:17:56,460
assembly is surprisingly easy

00:17:52,580 --> 00:17:58,350
considering it's Python and basically

00:17:56,460 --> 00:18:00,090
we're the operations which we've

00:17:58,350 --> 00:18:03,090
recorded already so level that we

00:18:00,090 --> 00:18:06,570
essentially just have to write out the

00:18:03,090 --> 00:18:07,800
x86 instructions our our register

00:18:06,570 --> 00:18:10,290
allocator is not particularly

00:18:07,800 --> 00:18:14,340
sophisticated this is mostly because it

00:18:10,290 --> 00:18:16,050
would be slow and we're not we're not

00:18:14,340 --> 00:18:20,040
really at the point that having a fancy

00:18:16,050 --> 00:18:21,930
algorithm would give giant speed ups the

00:18:20,040 --> 00:18:24,030
other tricky part is the garbage

00:18:21,930 --> 00:18:27,150
collector we have to dynamically inform

00:18:24,030 --> 00:18:32,930
the garbage collector about things which

00:18:27,150 --> 00:18:36,390
we've allocated in the the JIT race and

00:18:32,930 --> 00:18:45,680
so I can show you the assembly on the

00:18:36,390 --> 00:18:45,680
JIT viewer too so it's pretty tight

00:18:47,090 --> 00:18:55,370
but we're doing the the division

00:18:51,740 --> 00:19:00,140
operations directly in assembly and so

00:18:55,370 --> 00:19:02,570
that's how it got fast there's one other

00:19:00,140 --> 00:19:03,950
important thing so remember guards we

00:19:02,570 --> 00:19:08,360
have all these guards which are

00:19:03,950 --> 00:19:09,340
protecting the trace and the assembly

00:19:08,360 --> 00:19:12,470
from

00:19:09,340 --> 00:19:14,960
valid our runtime conditions which it

00:19:12,470 --> 00:19:18,110
was not prepared for and so these are

00:19:14,960 --> 00:19:19,340
also inserted the assembly and so if one

00:19:18,110 --> 00:19:20,780
of these fails we don't have any

00:19:19,340 --> 00:19:22,310
assembly which could run for it so we

00:19:20,780 --> 00:19:24,350
have to go back to the interpreter and

00:19:22,310 --> 00:19:26,360
this is kind of an intricate process

00:19:24,350 --> 00:19:29,990
because a lot of stuff has been

00:19:26,360 --> 00:19:31,490
optimized out and there's values sitting

00:19:29,990 --> 00:19:34,490
on the stack which need to be wrapped

00:19:31,490 --> 00:19:36,860
into Python objects and and stuck back

00:19:34,490 --> 00:19:39,710
into the interpreter so we have a

00:19:36,860 --> 00:19:41,180
special place in the which when were

00:19:39,710 --> 00:19:44,000
generating assembly we generate a

00:19:41,180 --> 00:19:49,430
special place where we jump to if a

00:19:44,000 --> 00:19:52,100
guard fails and this there's a compact

00:19:49,430 --> 00:19:53,780
description there of where everything is

00:19:52,100 --> 00:19:55,490
on the stack and in the registers and

00:19:53,780 --> 00:19:58,880
then we're able to rebuild the

00:19:55,490 --> 00:20:02,060
interpreter state from that and it's

00:19:58,880 --> 00:20:03,650
almost that easy except we can't jump

00:20:02,060 --> 00:20:06,440
right back into the interpreter because

00:20:03,650 --> 00:20:07,610
the assembly might have been correspond

00:20:06,440 --> 00:20:10,130
to someplace in the middle of a

00:20:07,610 --> 00:20:13,760
complicated opcode so what we do is we

00:20:10,130 --> 00:20:18,410
have we use the meta interpreter to

00:20:13,760 --> 00:20:20,150
interpret back to the back to the JIT

00:20:18,410 --> 00:20:22,760
merge point which is ahead of bytecode

00:20:20,150 --> 00:20:24,740
dispatch and then that when we're there

00:20:22,760 --> 00:20:27,680
we can safely jump back to the

00:20:24,740 --> 00:20:30,200
interpreter and the meta interpreter

00:20:27,680 --> 00:20:32,450
when it's doing this function during

00:20:30,200 --> 00:20:35,540
guard recovery is called black hole

00:20:32,450 --> 00:20:40,480
interpreter because it eats everything

00:20:35,540 --> 00:20:43,430
it doesn't record any tracing of course

00:20:40,480 --> 00:20:46,100
this is kind of inflexible if we were

00:20:43,430 --> 00:20:48,530
only ever to able to JIT stuff which

00:20:46,100 --> 00:20:52,130
didn't have any conditions in it so if a

00:20:48,530 --> 00:20:54,230
guard repeatedly fails instead of using

00:20:52,130 --> 00:20:57,380
the black hole interpreter we trace to

00:20:54,230 --> 00:20:58,850
the end of the loop again and then we've

00:20:57,380 --> 00:21:00,380
compiled something called a bridge and

00:20:58,850 --> 00:21:00,920
the next time the guard fails we can

00:21:00,380 --> 00:21:05,330
jump to

00:21:00,920 --> 00:21:09,710
the bridge which goes fit which allows

00:21:05,330 --> 00:21:12,200
us to remain in assembly so as I said

00:21:09,710 --> 00:21:17,180
before we have to tell the JIT a little

00:21:12,200 --> 00:21:18,710
bit in the Python interpreter and so

00:21:17,180 --> 00:21:21,170
this is these are some of the ways we do

00:21:18,710 --> 00:21:22,820
it one of the ways is that we can we can

00:21:21,170 --> 00:21:24,740
tell the JIT this is probably going to

00:21:22,820 --> 00:21:27,140
be constant so you should constant fold

00:21:24,740 --> 00:21:29,450
a staunch ech to make sure it's constant

00:21:27,140 --> 00:21:32,420
and that's called constant promotion and

00:21:29,450 --> 00:21:35,510
we've also engineered a lot of data

00:21:32,420 --> 00:21:39,560
structures too which are help which are

00:21:35,510 --> 00:21:41,570
optimized better by the JIT a lot of

00:21:39,560 --> 00:21:43,310
them are dictionaries probably because

00:21:41,570 --> 00:21:46,100
the dictionary type is so important in

00:21:43,310 --> 00:21:48,200
Python and I'd like to briefly talk

00:21:46,100 --> 00:21:51,620
about one of them it's called math Dix

00:21:48,200 --> 00:21:54,500
and what map sticks are based on the

00:21:51,620 --> 00:21:58,340
observation that most instances of a

00:21:54,500 --> 00:22:02,060
class look exactly the same so what we

00:21:58,340 --> 00:22:03,980
try to do is we share the layout the

00:22:02,060 --> 00:22:07,640
instant layout where the attributes are

00:22:03,980 --> 00:22:12,140
of the type we share that we put it on

00:22:07,640 --> 00:22:13,940
the class and then we just and then we

00:22:12,140 --> 00:22:17,330
can just store the values of the

00:22:13,940 --> 00:22:19,730
instance in a linear array on the class

00:22:17,330 --> 00:22:23,980
and so this is this is similar to

00:22:19,730 --> 00:22:27,830
feature in v8 called hidden classes and

00:22:23,980 --> 00:22:30,410
so basically what this allows us to do

00:22:27,830 --> 00:22:32,990
in the JIT is that the class is almost

00:22:30,410 --> 00:22:34,940
how is going to be constant so what it

00:22:32,990 --> 00:22:38,050
ends up having is attribute accesses

00:22:34,940 --> 00:22:42,650
access can just be compiled down to a

00:22:38,050 --> 00:22:47,210
lookup on an array which is much better

00:22:42,650 --> 00:22:52,550
than a dict lookup and so this is a

00:22:47,210 --> 00:22:54,980
diagram of that and so there's a the

00:22:52,550 --> 00:23:00,110
storage is stored independently of the

00:22:54,980 --> 00:23:04,430
instance and so to conclude we're able

00:23:00,110 --> 00:23:08,780
to generate a fast ship from the our

00:23:04,430 --> 00:23:12,050
Python interpreters and we trace we do

00:23:08,780 --> 00:23:14,090
this by arch is a tracing JIT and our

00:23:12,050 --> 00:23:14,510
optimizations seek primarily to remove

00:23:14,090 --> 00:23:16,429
the

00:23:14,510 --> 00:23:19,580
in efficiencies in indirection founded

00:23:16,429 --> 00:23:21,890
dynamic languages and we're able to

00:23:19,580 --> 00:23:26,330
adapt to change your unkind conditions

00:23:21,890 --> 00:23:28,010
with bridges on the loop and finally our

00:23:26,330 --> 00:23:31,580
interpreter is optimized for the JIT

00:23:28,010 --> 00:23:33,590
with special data structures thank you

00:23:31,580 --> 00:23:35,770
very much I'll be happy to take

00:23:33,590 --> 00:23:35,770
questions

00:23:44,220 --> 00:23:52,540
so what is your what's the next step or

00:23:48,960 --> 00:23:56,080
attack for performance that you you work

00:23:52,540 --> 00:23:59,200
on or you know the next goal that you're

00:23:56,080 --> 00:24:04,810
trying to reach with with getting the

00:23:59,200 --> 00:24:09,070
code okay so the question is what are we

00:24:04,810 --> 00:24:11,010
looking at next there's a lot of things

00:24:09,070 --> 00:24:13,270
to be done one of the things is

00:24:11,010 --> 00:24:17,170
integrating it with other pi PI

00:24:13,270 --> 00:24:20,110
technology so recently stackless support

00:24:17,170 --> 00:24:24,640
has been added to the JIT another thing

00:24:20,110 --> 00:24:27,730
is STM I think they talked about that at

00:24:24,640 --> 00:24:32,940
the last talk Oh we'd like the the JIT

00:24:27,730 --> 00:24:32,940
to be able to help optimize that and the

00:24:33,600 --> 00:24:39,160
the other part of the other prongs of

00:24:36,490 --> 00:24:44,110
attack are we're doing some more work

00:24:39,160 --> 00:24:45,820
with unrolling and bridges it's which

00:24:44,110 --> 00:24:47,890
it's actually a bit more complicated

00:24:45,820 --> 00:24:52,810
that we have something called targets

00:24:47,890 --> 00:24:55,570
which allow us to unto unroll in

00:24:52,810 --> 00:24:59,860
different ways so there's lots of work

00:24:55,570 --> 00:25:01,780
to do and we could also do some work on

00:24:59,860 --> 00:25:05,800
the back end just making our assembly

00:25:01,780 --> 00:25:07,900
even tighter early on in the talk you

00:25:05,800 --> 00:25:09,340
said and when we hit the jump absolute

00:25:07,900 --> 00:25:11,710
we know that the loop is over and I

00:25:09,340 --> 00:25:13,480
thought oh I wonder like is that an

00:25:11,710 --> 00:25:15,610
assumption that whenever I see a

00:25:13,480 --> 00:25:18,190
backwards jump in the generator bytecode

00:25:15,610 --> 00:25:20,110
it's because we've set the scene low end

00:25:18,190 --> 00:25:21,880
of a loop and like how many built-in

00:25:20,110 --> 00:25:23,560
assumptions are there in the JIT that

00:25:21,880 --> 00:25:26,500
the bytecodes came out of the C

00:25:23,560 --> 00:25:28,120
pythoness compiler and could I confuse

00:25:26,500 --> 00:25:30,820
it by hand writing bytecode that did

00:25:28,120 --> 00:25:33,370
something funny yeah you could confuse

00:25:30,820 --> 00:25:35,110
it up basically yes basically the jump

00:25:33,370 --> 00:25:40,390
absolute by code is only ever used in

00:25:35,110 --> 00:25:41,830
while in for loops so if that changed it

00:25:40,390 --> 00:25:44,289
would probably be a five-minute change

00:25:41,830 --> 00:25:46,940
in the JIT though

00:25:44,289 --> 00:25:48,769
I'm working on optimizing some

00:25:46,940 --> 00:25:51,649
scientific simulations that are already

00:25:48,769 --> 00:25:53,389
running under pi PI to optimize further

00:25:51,649 --> 00:25:55,759
would you recommend looking at something

00:25:53,389 --> 00:25:58,190
like the JIT viewer or how much do

00:25:55,759 --> 00:26:05,179
traditional profiling tools still apply

00:25:58,190 --> 00:26:08,570
with the jittering okay so yeah the the

00:26:05,179 --> 00:26:10,909
JIT viewer is often helpful it can take

00:26:08,570 --> 00:26:13,359
a bit to kind of understand what it's

00:26:10,909 --> 00:26:13,359
telling you

00:26:13,899 --> 00:26:20,989
profilers we do have profiler support in

00:26:16,909 --> 00:26:23,179
the JIT but sometimes basically what I

00:26:20,989 --> 00:26:25,429
usually do is I look at the the JIT

00:26:23,179 --> 00:26:28,809
viewer trace and I try to see okay what

00:26:25,429 --> 00:26:32,570
does the JIT getting confused about and

00:26:28,809 --> 00:26:39,289
how can I help it so I guess that's

00:26:32,570 --> 00:26:42,619
where that's where I'd start so if I

00:26:39,289 --> 00:26:46,849
understand right most of this cheating

00:26:42,619 --> 00:26:48,919
is completely independent of Python like

00:26:46,849 --> 00:26:50,779
you could use this to JIT other

00:26:48,919 --> 00:26:52,580
languages that aren't Python yes in fact

00:26:50,779 --> 00:26:54,619
we have several interpreter up other

00:26:52,580 --> 00:26:56,749
jits in the Python interpreter like our

00:26:54,619 --> 00:27:01,429
regular expression interpreter is jaded

00:26:56,749 --> 00:27:02,090
as well as several built-ins like filter

00:27:01,429 --> 00:27:05,899
and map

00:27:02,090 --> 00:27:07,549
we've also added jits to I I thought I

00:27:05,899 --> 00:27:10,249
had seen that there was like a little

00:27:07,549 --> 00:27:14,389
toy directory that had it looked like a

00:27:10,249 --> 00:27:18,309
scheme interpreter uh yes also yeah we

00:27:14,389 --> 00:27:21,289
have some other toys a prologue scheme

00:27:18,309 --> 00:27:24,549
so yeah you can that's definitely one we

00:27:21,289 --> 00:27:29,139
want to create a dynamic language

00:27:24,549 --> 00:27:32,149
implementation framework cool thanks

00:27:29,139 --> 00:27:33,950
thanks for the talk which CPU

00:27:32,149 --> 00:27:36,529
architectures are an indeed of

00:27:33,950 --> 00:27:39,019
subfamilies of CPU architectures are

00:27:36,529 --> 00:27:41,479
supported and is it kind of dynamic

00:27:39,019 --> 00:27:42,429
based on which cpuid is detected at

00:27:41,479 --> 00:27:47,539
runtime

00:27:42,429 --> 00:27:50,749
okay so we're basically the only thing

00:27:47,539 --> 00:27:53,799
we detect is SSE because we need to know

00:27:50,749 --> 00:27:53,799
if we're going to support floats

00:27:54,670 --> 00:27:58,990
besides that I think I think we try to

00:27:56,590 --> 00:28:03,490
use the square root operations and on

00:27:58,990 --> 00:28:05,980
some CPUs so it's not particularly we

00:28:03,490 --> 00:28:08,980
don't do much with that yeah besides and

00:28:05,980 --> 00:28:11,410
it's x86 but also arm as well what

00:28:08,980 --> 00:28:12,940
anything else I mean if I need to

00:28:11,410 --> 00:28:15,340
support other architectures am I going

00:28:12,940 --> 00:28:18,760
to need to attach this I think the

00:28:15,340 --> 00:28:19,090
answer is yes if you need to support

00:28:18,760 --> 00:28:25,980
other

00:28:19,090 --> 00:28:25,980
you mean like armors or PPC s/390

00:28:28,320 --> 00:28:39,910
is operable yes the answer to this

00:28:36,100 --> 00:28:45,900
question is that we support x86 x86 64

00:28:39,910 --> 00:28:50,799
arm arm v7 bomb precisely and power pcs

00:28:45,900 --> 00:28:56,500
64 is in progress right now thank you

00:28:50,799 --> 00:28:58,980
thank you thank you he said all right

00:28:56,500 --> 00:28:58,980

YouTube URL: https://www.youtube.com/watch?v=NIcijUt-HlE


