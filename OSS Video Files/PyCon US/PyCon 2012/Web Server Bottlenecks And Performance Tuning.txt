Title: Web Server Bottlenecks And Performance Tuning
Publication date: 2012-04-29
Playlist: PyCon 2012
Description: 
	Graham Dumpleton
New Python web developers seem to love running benchmarks on WSGI servers. Reality is that they often have no idea what they are doing or what to look at. This talk will look at a range of factors which can influence the performance
Captions: 
	00:00:00,030 --> 00:00:06,750
ooh the session afternoon for track 5

00:00:03,830 --> 00:00:10,650
we're going to start with Graham

00:00:06,750 --> 00:00:13,559
Dumbleton the author of Maude

00:00:10,650 --> 00:00:15,900
daliyah's GI and the title of his dog

00:00:13,559 --> 00:00:26,670
its web servers web server bottlenecks

00:00:15,900 --> 00:00:28,109
and performance tuning if you do have

00:00:26,670 --> 00:00:30,119
your laptop's open you want to follow

00:00:28,109 --> 00:00:32,850
along the slides are already on

00:00:30,119 --> 00:00:36,960
SlideShare go to that dress just search

00:00:32,850 --> 00:00:39,180
my name before we talk about web server

00:00:36,960 --> 00:00:40,680
performance tuning is important to step

00:00:39,180 --> 00:00:42,840
back and look at the bigger picture

00:00:40,680 --> 00:00:44,160
although newbies especially have an

00:00:42,840 --> 00:00:46,500
obsession with trying to find the

00:00:44,160 --> 00:00:48,050
fastest web server reality is that

00:00:46,500 --> 00:00:50,550
things are more complicated than that

00:00:48,050 --> 00:00:52,980
systems have many moving parts the

00:00:50,550 --> 00:00:55,440
actual per request latency introduced by

00:00:52,980 --> 00:00:58,800
a web server is very small and relation

00:00:55,440 --> 00:01:01,230
to other parts of the system as far as a

00:00:58,800 --> 00:01:02,940
user is concerned the main delays they

00:01:01,230 --> 00:01:04,979
will notice are those resulting from how

00:01:02,940 --> 00:01:06,570
long it takes to their web browser to

00:01:04,979 --> 00:01:08,549
render the page returned by the web

00:01:06,570 --> 00:01:10,320
application this will be followed up by

00:01:08,549 --> 00:01:12,210
Network delays when talking to the web

00:01:10,320 --> 00:01:14,100
application and when grabbing down

00:01:12,210 --> 00:01:18,000
static assets from media servers or

00:01:14,100 --> 00:01:19,619
content data networks the time spent in

00:01:18,000 --> 00:01:21,810
the web application is therefore a small

00:01:19,619 --> 00:01:23,610
percentage of time as perceived by the

00:01:21,810 --> 00:01:26,100
user for rendering a web page being

00:01:23,610 --> 00:01:27,960
served any time delay is introduced by

00:01:26,100 --> 00:01:32,009
the web server will be a much smaller

00:01:27,960 --> 00:01:34,860
percentage again Steve Saunders

00:01:32,009 --> 00:01:36,540
summarizes this disparity between front

00:01:34,860 --> 00:01:38,130
end time and web application time in

00:01:36,540 --> 00:01:41,040
what he calls the performance golden

00:01:38,130 --> 00:01:43,409
rule this is that 80 to 90 percent of

00:01:41,040 --> 00:01:46,079
the end-user response time is spent on

00:01:43,409 --> 00:01:48,360
the front end so if you're after an easy

00:01:46,079 --> 00:01:50,579
win for improving end-user satisfaction

00:01:48,360 --> 00:01:53,149
with your website the front end is where

00:01:50,579 --> 00:01:55,470
you should start

00:01:53,149 --> 00:01:57,570
although the big immediate gains may be

00:01:55,470 --> 00:01:59,880
one enough in the front end the web

00:01:57,570 --> 00:02:01,469
application still represents a lot

00:01:59,880 --> 00:02:03,420
represents a lot of opportunity to

00:02:01,469 --> 00:02:04,950
further reduce response times through

00:02:03,420 --> 00:02:07,350
means other than fiddling with the web

00:02:04,950 --> 00:02:10,020
server improvements can be had in the

00:02:07,350 --> 00:02:11,610
application code but also databases or

00:02:10,020 --> 00:02:13,480
back-end services used by the web

00:02:11,610 --> 00:02:16,940
application

00:02:13,480 --> 00:02:19,459
is running benchmarks on web servers

00:02:16,940 --> 00:02:21,800
complete waste of time then the answer

00:02:19,459 --> 00:02:23,510
is yes and no the sort of benchmarks

00:02:21,800 --> 00:02:25,670
that are usually published on websites

00:02:23,510 --> 00:02:28,040
to compare web servers usually serve

00:02:25,670 --> 00:02:29,840
little value they generally only serve

00:02:28,040 --> 00:02:32,330
to give newbies a false sense of

00:02:29,840 --> 00:02:34,790
security over any decision they make as

00:02:32,330 --> 00:02:36,920
to which server web server to use worse

00:02:34,790 --> 00:02:38,900
is that people forever reference them

00:02:36,920 --> 00:02:41,019
it's the gospel truth when they could be

00:02:38,900 --> 00:02:43,670
far from it

00:02:41,019 --> 00:02:46,010
the main reason that typical web server

00:02:43,670 --> 00:02:49,190
benchmarks are useless is that they test

00:02:46,010 --> 00:02:51,380
only a single narrow idealized use case

00:02:49,190 --> 00:02:53,209
web servers are implemented using

00:02:51,380 --> 00:02:55,519
different architectures and different

00:02:53,209 --> 00:02:57,260
code you're better off choosing a web

00:02:55,519 --> 00:02:59,810
server that you believe is the features

00:02:57,260 --> 00:03:03,730
you require and then use benchmarks to

00:02:59,810 --> 00:03:06,319
help explore the behavior of that system

00:03:03,730 --> 00:03:08,450
often the documented benchmarks you find

00:03:06,319 --> 00:03:10,940
and nothing more than a hello world

00:03:08,450 --> 00:03:13,370
program the testing consists of running

00:03:10,940 --> 00:03:15,350
at at maximum request freeport with some

00:03:13,370 --> 00:03:18,049
arbitrary number of concurrent users

00:03:15,350 --> 00:03:20,600
this does not mirror what a real trick

00:03:18,049 --> 00:03:23,000
what real traffic a public-facing web

00:03:20,600 --> 00:03:25,010
server would receive it certainly

00:03:23,000 --> 00:03:27,890
doesn't show what causes the server to

00:03:25,010 --> 00:03:33,260
fail as load increases just that it will

00:03:27,890 --> 00:03:34,880
fail what should you test them there are

00:03:33,260 --> 00:03:36,859
many different use cases one could test

00:03:34,880 --> 00:03:39,109
and how any one performs can be dictated

00:03:36,859 --> 00:03:41,000
by the architecture of the system how

00:03:39,109 --> 00:03:43,160
the code was written and how the system

00:03:41,000 --> 00:03:44,750
is configured when the test is run the

00:03:43,160 --> 00:03:46,760
more interesting tests of those which

00:03:44,750 --> 00:03:48,829
deliberately go out to trigger specific

00:03:46,760 --> 00:03:51,290
problems this is because it is the

00:03:48,829 --> 00:03:53,570
corner cases that are usually going to

00:03:51,290 --> 00:03:57,530
cause an issue rather than the typical

00:03:53,570 --> 00:03:58,940
use case what sort of factors can come

00:03:57,530 --> 00:04:01,850
into play and affect performance

00:03:58,940 --> 00:04:03,709
these are varied and can arise from the

00:04:01,850 --> 00:04:05,239
hardware or virtualized system being

00:04:03,709 --> 00:04:07,130
used they can derive from the

00:04:05,239 --> 00:04:09,440
configuration you use for the specific

00:04:07,130 --> 00:04:11,030
web server it also can be influenced by

00:04:09,440 --> 00:04:14,090
how the Python language interpreter

00:04:11,030 --> 00:04:15,680
itself works to make it harder these can

00:04:14,090 --> 00:04:20,209
all interplay with each other in

00:04:15,680 --> 00:04:21,950
unexpected ways some things can be out

00:04:20,209 --> 00:04:23,750
of your control together such as the

00:04:21,950 --> 00:04:25,700
type of web browser and what type of

00:04:23,750 --> 00:04:26,060
network the traffic between you and the

00:04:25,700 --> 00:04:28,490
user

00:04:26,060 --> 00:04:30,770
has to traverse very few published

00:04:28,490 --> 00:04:34,730
benchmarks try to count for these issues

00:04:30,770 --> 00:04:36,680
in a realistic way requirements is

00:04:34,730 --> 00:04:38,570
dictated by your own web application or

00:04:36,680 --> 00:04:41,240
how they you decided to architect your

00:04:38,570 --> 00:04:42,380
overall systems can also contribute such

00:04:41,240 --> 00:04:47,870
as whether you try and use the same

00:04:42,380 --> 00:04:49,280
server to serve up static assets to

00:04:47,870 --> 00:04:51,530
illustrate how some of these different

00:04:49,280 --> 00:04:53,630
factors can come into play I will go

00:04:51,530 --> 00:04:55,610
through a few specific use cases for %

00:04:53,630 --> 00:04:57,800
issues in practice and where possible

00:04:55,610 --> 00:05:00,229
relate them to those factors these

00:04:57,800 --> 00:05:02,330
include memory usage use of processes

00:05:00,229 --> 00:05:05,030
vs. Fred's impacts of long-running

00:05:02,330 --> 00:05:09,650
requests restarting of server processes

00:05:05,030 --> 00:05:12,139
and startup costs a simple place to get

00:05:09,650 --> 00:05:14,240
started is memory usage this is always a

00:05:12,139 --> 00:05:16,700
hot topic of contention with benchmarks

00:05:14,240 --> 00:05:18,950
it isn't hard to find people claiming

00:05:16,700 --> 00:05:21,110
that Apache is a big bloated memory hog

00:05:18,950 --> 00:05:23,630
this benchmark in particular is

00:05:21,110 --> 00:05:26,210
representative of poorly chosen config

00:05:23,630 --> 00:05:27,290
Apache configuration of course it's

00:05:26,210 --> 00:05:29,650
going to use more memory if you

00:05:27,290 --> 00:05:32,390
configure it to have 1,000 Fred's if

00:05:29,650 --> 00:05:34,910
service tested aren't set up in a

00:05:32,390 --> 00:05:38,840
comparable way you can hardly expect it

00:05:34,910 --> 00:05:40,430
to be a fair comparison actually

00:05:38,840 --> 00:05:42,860
estimating the overall amount of memory

00:05:40,430 --> 00:05:44,600
used is not a difficult exercise it is

00:05:42,860 --> 00:05:45,919
after all a simple formula which takes

00:05:44,600 --> 00:05:48,320
into consideration the number of

00:05:45,919 --> 00:05:50,479
processes the base memory used by your

00:05:48,320 --> 00:05:52,729
webs by the web server memory for each

00:05:50,479 --> 00:05:55,160
additional Fred and the application

00:05:52,729 --> 00:05:57,080
itself things get more complicated when

00:05:55,160 --> 00:05:59,780
one considers per request transient

00:05:57,080 --> 00:06:03,919
memory but ignoring that one can easily

00:05:59,780 --> 00:06:06,710
visualize what you're dealing with in

00:06:03,919 --> 00:06:08,690
short adding more processes going to see

00:06:06,710 --> 00:06:10,280
your memory usage grow quicker than

00:06:08,690 --> 00:06:12,620
adding more threads to existing

00:06:10,280 --> 00:06:15,229
processes although some of that per

00:06:12,620 --> 00:06:17,360
process based memory usage is the web

00:06:15,229 --> 00:06:19,400
server the majority of it will in the

00:06:17,360 --> 00:06:21,740
end be your fat pipe and web application

00:06:19,400 --> 00:06:24,560
to blame a web server for using too much

00:06:21,740 --> 00:06:26,300
memory is a plain silly when your web

00:06:24,560 --> 00:06:29,360
application could be using up to 50

00:06:26,300 --> 00:06:32,270
times as much memory the issue really is

00:06:29,360 --> 00:06:36,830
about what configuration you choose to

00:06:32,270 --> 00:06:38,900
set the web server up with what usually

00:06:36,830 --> 00:06:39,889
happens is that people blindly use

00:06:38,900 --> 00:06:42,349
whatever the default

00:06:39,889 --> 00:06:44,419
so for a server for fat python web

00:06:42,349 --> 00:06:46,789
applications which are a lot use a lot

00:06:44,419 --> 00:06:49,599
of memory this can be disastrous apache

00:06:46,789 --> 00:06:53,240
root spree for npm can be for example

00:06:49,599 --> 00:06:56,060
dynamically create up to 150 processes

00:06:53,240 --> 00:06:58,219
that is potentially 150 copies of your

00:06:56,060 --> 00:07:02,060
fat pipe and web application so of

00:06:58,219 --> 00:07:03,499
course it will use a lot of memory those

00:07:02,060 --> 00:07:05,810
service which are generally seen as

00:07:03,499 --> 00:07:07,999
faring best as far as memory usage as

00:07:05,810 --> 00:07:10,610
those as default configurations use only

00:07:07,999 --> 00:07:12,439
a single process and a single fred guess

00:07:10,610 --> 00:07:14,719
what if you configure apache that way

00:07:12,439 --> 00:07:15,620
then the amount of memory users will not

00:07:14,719 --> 00:07:18,110
be much different

00:07:15,620 --> 00:07:20,360
granted it does help to also strip

00:07:18,110 --> 00:07:22,129
unneeded modules out of apache that you

00:07:20,360 --> 00:07:25,879
don't use to really get the best from it

00:07:22,129 --> 00:07:27,680
there so don't start things off by using

00:07:25,879 --> 00:07:30,050
whatever the default processes friends

00:07:27,680 --> 00:07:32,389
configurations are especially looking at

00:07:30,050 --> 00:07:33,409
memory usage do so and you can easily

00:07:32,389 --> 00:07:35,990
get the wrong impression

00:07:33,409 --> 00:07:37,400
also don't pick arbitrary values when

00:07:35,990 --> 00:07:40,939
you have no idea whether it is

00:07:37,400 --> 00:07:43,039
reasonable a disk scale a configuration

00:07:40,939 --> 00:07:47,599
1,000 friends will not even fit on this

00:07:43,039 --> 00:07:49,339
chart it's somewhere out there how many

00:07:47,599 --> 00:07:51,650
processes the fridge should you use then

00:07:49,339 --> 00:07:53,629
the total number of threads across all

00:07:51,650 --> 00:07:56,449
processes is dictated by the number of

00:07:53,629 --> 00:07:58,339
overlapping concurrent requests how much

00:07:56,449 --> 00:08:00,649
overlap there is depends on response

00:07:58,339 --> 00:08:02,629
times and fruit put processors are

00:08:00,649 --> 00:08:05,569
preferred over Fred's be constrained by

00:08:02,629 --> 00:08:07,430
memory the optimal number can also be

00:08:05,569 --> 00:08:10,159
dictated by how many processes are

00:08:07,430 --> 00:08:13,789
available unicorn recommends for example

00:08:10,159 --> 00:08:15,770
using 2 to 4 times the number 2 to 4 the

00:08:13,789 --> 00:08:19,939
number of processes as you have CPU

00:08:15,770 --> 00:08:21,379
cores one can get a feel for how many

00:08:19,939 --> 00:08:23,899
friends you will need by looking at Fred

00:08:21,379 --> 00:08:25,610
utilization that is how much do the

00:08:23,899 --> 00:08:28,339
requests take up of the potential

00:08:25,610 --> 00:08:30,379
capacity in this example by adding up

00:08:28,339 --> 00:08:32,329
the green areas representing the

00:08:30,379 --> 00:08:35,149
requests coming in over time we have

00:08:32,329 --> 00:08:36,620
here a Fred utilization of about 2 this

00:08:35,149 --> 00:08:37,490
means that if all requests were

00:08:36,620 --> 00:08:40,969
serialized

00:08:37,490 --> 00:08:42,829
we would need only to Fred's requests

00:08:40,969 --> 00:08:45,140
don't arrive in such an orderly fashion

00:08:42,829 --> 00:08:47,860
note so we need more friends to ensure

00:08:45,140 --> 00:08:47,860
they aren't delayed

00:08:49,410 --> 00:08:52,890
because response times are generally

00:08:50,970 --> 00:08:54,810
quite short is actually surprising how

00:08:52,890 --> 00:08:56,250
few fridge you can get away with if the

00:08:54,810 --> 00:08:58,080
number of friends is too low in the

00:08:56,250 --> 00:09:00,200
response times or Freeport grows though

00:08:58,080 --> 00:09:02,250
then Fred utilization will increase

00:09:00,200 --> 00:09:03,990
eventually what happens then is that

00:09:02,250 --> 00:09:05,910
requests will start to back up as they

00:09:03,990 --> 00:09:08,040
wait for available Fred's and queuing

00:09:05,910 --> 00:09:09,690
time will increase this will add to the

00:09:08,040 --> 00:09:13,500
delays that the end user sees in their

00:09:09,690 --> 00:09:16,050
total page load time if we add processes

00:09:13,500 --> 00:09:18,270
rather than Fred's we can delay the

00:09:16,050 --> 00:09:20,250
onset of such problems the reason that

00:09:18,270 --> 00:09:22,260
processes work better is that the Python

00:09:20,250 --> 00:09:24,510
global interpret a lot effectively

00:09:22,260 --> 00:09:27,120
serializes execution with indistinct

00:09:24,510 --> 00:09:29,940
threads of a single process adding more

00:09:27,120 --> 00:09:32,130
processes so obviously means more memory

00:09:29,940 --> 00:09:35,220
this has got nothing to do with the

00:09:32,130 --> 00:09:37,260
server now and a choice you make as to

00:09:35,220 --> 00:09:40,710
which is going to be bound by how much

00:09:37,260 --> 00:09:43,020
memory you have available if you are

00:09:40,710 --> 00:09:44,730
memory constrained finding the right

00:09:43,020 --> 00:09:46,800
balance and what you can get away with

00:09:44,730 --> 00:09:49,680
in order to reduce memory usage is a

00:09:46,800 --> 00:09:51,450
tricky problem it is all is all made

00:09:49,680 --> 00:09:53,250
harder when you have no idea what is

00:09:51,450 --> 00:09:55,590
going on inside of your web application

00:09:53,250 --> 00:09:57,600
if a web application has a heavy bias

00:09:55,590 --> 00:09:59,730
towards CPU bound activity within the

00:09:57,600 --> 00:10:02,300
process then you are forced towards the

00:09:59,730 --> 00:10:04,920
direction of needing more processes if

00:10:02,300 --> 00:10:07,260
your web application is making lots of

00:10:04,920 --> 00:10:09,600
call-outs to back-end services and so

00:10:07,260 --> 00:10:11,700
Fred's are blocked waiting on i/o more

00:10:09,600 --> 00:10:13,350
of the time than not you can get away or

00:10:11,700 --> 00:10:14,970
using more Fred's because the friends

00:10:13,350 --> 00:10:17,220
aren't competing as much of each other

00:10:14,970 --> 00:10:19,860
for use of the CPU of in the same

00:10:17,220 --> 00:10:21,630
process if you have no ideas of what

00:10:19,860 --> 00:10:23,100
your web application is doing this

00:10:21,630 --> 00:10:25,080
judgment is going to be a hit-and-miss

00:10:23,100 --> 00:10:29,430
affair as far as tuning the processes

00:10:25,080 --> 00:10:30,270
Fred's balance to make such judgment

00:10:29,430 --> 00:10:32,280
even harder

00:10:30,270 --> 00:10:34,350
you also have long-running requests to

00:10:32,280 --> 00:10:35,840
contend with these going to ride you to

00:10:34,350 --> 00:10:38,490
issues in your own code or back-end

00:10:35,840 --> 00:10:40,560
services but also due to how much data

00:10:38,490 --> 00:10:43,350
you are moving around and how slow the

00:10:40,560 --> 00:10:45,840
HTTP clients are the basic problem here

00:10:43,350 --> 00:10:48,150
is that a long-running request because

00:10:45,840 --> 00:10:49,620
it ties up a Fred will reduce the

00:10:48,150 --> 00:10:53,340
maximum throughput you could achieve

00:10:49,620 --> 00:10:55,440
during that period of time the

00:10:53,340 --> 00:10:57,540
unpredictability of request times means

00:10:55,440 --> 00:10:59,310
you need to also ensure you have a good

00:10:57,540 --> 00:11:01,589
amount of extra capacity in the number

00:10:59,310 --> 00:11:02,819
of processes Fred's allocated don't

00:11:01,589 --> 00:11:04,499
provide sufficient head

00:11:02,819 --> 00:11:07,229
rune and when a number of long-running

00:11:04,499 --> 00:11:09,720
quests can coincide you'll suddenly find

00:11:07,229 --> 00:11:11,939
fred availability drops requests can

00:11:09,720 --> 00:11:14,309
start back logging and overall response

00:11:11,939 --> 00:11:18,959
time seen as seen by the user will

00:11:14,309 --> 00:11:20,999
increase where your application code or

00:11:18,959 --> 00:11:23,549
back-end service is slow you also need

00:11:20,999 --> 00:11:25,259
to work out why sometimes issues can

00:11:23,549 --> 00:11:27,929
come from places you least expect them

00:11:25,259 --> 00:11:30,299
for example and especially with Django

00:11:27,929 --> 00:11:32,970
watch out for how long Postgres database

00:11:30,299 --> 00:11:34,649
connections take one thing you can

00:11:32,970 --> 00:11:36,660
consider in this case is a local

00:11:34,649 --> 00:11:45,689
external connection Pooler such as PG

00:11:36,660 --> 00:11:48,269
bouncer if you're using Apache mod WSGI

00:11:45,689 --> 00:11:50,249
or guna corn stick nginx in front of it

00:11:48,269 --> 00:11:52,679
in proxy requests through the WSGI

00:11:50,249 --> 00:11:55,079
server this will make your WSGI server

00:11:52,679 --> 00:11:57,089
perform better as you'll be isolated

00:11:55,079 --> 00:11:58,799
from slow clients the friends in the

00:11:57,089 --> 00:12:01,379
backend will be tied up for less time

00:11:58,799 --> 00:12:03,209
meaning lower Fred utilization thus

00:12:01,379 --> 00:12:05,999
allowing you to handle higher throughput

00:12:03,209 --> 00:12:07,799
with less resources you can also offload

00:12:05,999 --> 00:12:09,659
tasks such as static file serving to

00:12:07,799 --> 00:12:14,249
engineers which is going to do a better

00:12:09,659 --> 00:12:16,439
job of it anyway when introducing a

00:12:14,249 --> 00:12:18,269
front-end do be careful though the

00:12:16,439 --> 00:12:19,739
funneling effect especially if the

00:12:18,269 --> 00:12:22,350
number of concurrent requests that can

00:12:19,739 --> 00:12:25,139
be handle reduces at each step if your

00:12:22,350 --> 00:12:26,999
web application backlogs users can may

00:12:25,139 --> 00:12:28,829
give up but requests are still queued

00:12:26,999 --> 00:12:30,869
and have to be handled your web

00:12:28,829 --> 00:12:32,669
application wastes time and may have

00:12:30,869 --> 00:12:35,339
trouble catching up with a backlog

00:12:32,669 --> 00:12:37,709
it is perhaps better to set up service

00:12:35,339 --> 00:12:39,899
so that requests time out of a 503

00:12:37,709 --> 00:12:44,429
before getting to your web application

00:12:39,899 --> 00:12:46,499
if you can worst case scenario here is a

00:12:44,429 --> 00:12:48,059
complete overload where the server never

00:12:46,499 --> 00:12:50,279
really recovers for an extended period

00:12:48,059 --> 00:12:53,459
or until you you can shut down the

00:12:50,279 --> 00:12:55,109
server requests timeouts within the web

00:12:53,459 --> 00:12:57,119
application where supporter can help a

00:12:55,109 --> 00:12:59,819
bit but only to throw out long-running

00:12:57,119 --> 00:13:01,679
requests as already mentioned you really

00:12:59,819 --> 00:13:03,600
need to stop the request getting to the

00:13:01,679 --> 00:13:06,329
web application if there is no longer a

00:13:03,600 --> 00:13:08,579
point handling and options here vary and

00:13:06,329 --> 00:13:12,839
solutions are available to avoid it are

00:13:08,579 --> 00:13:14,850
always great you might actually think

00:13:12,839 --> 00:13:16,740
that doing a restart will solve a

00:13:14,850 --> 00:13:18,060
problem with backlog requests

00:13:16,740 --> 00:13:20,190
you have to be careful here as well

00:13:18,060 --> 00:13:22,830
though for some service the listener

00:13:20,190 --> 00:13:25,529
socket can be reserved so any backlog

00:13:22,830 --> 00:13:28,260
there isn't actually cleared further

00:13:25,529 --> 00:13:30,420
when performing a restart new processes

00:13:28,260 --> 00:13:33,000
have to be created and application code

00:13:30,420 --> 00:13:35,520
loaded again this can take time and

00:13:33,000 --> 00:13:37,890
cause more requests to backlog so choose

00:13:35,520 --> 00:13:39,029
carefully when you restart if you've got

00:13:37,890 --> 00:13:41,250
a problem with overloading

00:13:39,029 --> 00:13:43,320
- totally reset it is better to do a

00:13:41,250 --> 00:13:48,000
full shutdown and clear the backlog in

00:13:43,320 --> 00:13:49,680
the whole pipeline for that Python web

00:13:48,000 --> 00:13:51,510
applications of a large startup cost

00:13:49,680 --> 00:13:53,820
server configurations which allow for

00:13:51,510 --> 00:13:56,640
auto scaling can also compound problems

00:13:53,820 --> 00:13:58,830
when under load and you get a free foot

00:13:56,640 --> 00:14:00,870
further throughput spike the server can

00:13:58,830 --> 00:14:03,390
decide to start more processes this

00:14:00,870 --> 00:14:05,730
slows the system down temporarily causes

00:14:03,390 --> 00:14:07,950
backlog and if it takes a long time to

00:14:05,730 --> 00:14:10,260
start processes the server could decide

00:14:07,950 --> 00:14:12,660
to start even more process again

00:14:10,260 --> 00:14:14,760
increasing system load again blowing out

00:14:12,660 --> 00:14:19,380
memory and ello overloading your whole

00:14:14,760 --> 00:14:20,700
system to avoid unexpected surprises

00:14:19,380 --> 00:14:22,740
you're better off starting up the

00:14:20,700 --> 00:14:25,230
maximum number of processes you expect

00:14:22,740 --> 00:14:27,450
to require or can fit in available

00:14:25,230 --> 00:14:29,970
memory with your web application loaded

00:14:27,450 --> 00:14:32,640
ensure you preload your web application

00:14:29,970 --> 00:14:35,100
when processes start and not lazily when

00:14:32,640 --> 00:14:36,990
you first request arise do everything

00:14:35,100 --> 00:14:39,990
possible to keep the processes in memory

00:14:36,990 --> 00:14:42,329
at all time avoiding restarts especially

00:14:39,990 --> 00:14:47,100
don't use options to restart when some

00:14:42,329 --> 00:14:48,600
maximum request count is reached because

00:14:47,100 --> 00:14:50,100
the suggestion is that you should pre

00:14:48,600 --> 00:14:52,589
configure the server to its maximum

00:14:50,100 --> 00:14:54,870
capacity at the outset it does limit the

00:14:52,589 --> 00:14:56,220
vertical scaling you can do it lease

00:14:54,870 --> 00:14:58,320
with in the confines of the same

00:14:56,220 --> 00:15:00,899
hardware so next step therefore is

00:14:58,320 --> 00:15:03,000
horizontal scaling keep in mind the same

00:15:00,899 --> 00:15:04,680
issues about pre loading you don't want

00:15:03,000 --> 00:15:07,350
to bring on new hosts and direct traffic

00:15:04,680 --> 00:15:09,300
to them only for the first requests sent

00:15:07,350 --> 00:15:12,990
to it to it to be delayed while the

00:15:09,300 --> 00:15:15,870
application lives no matter how you set

00:15:12,990 --> 00:15:17,430
your system up if problems do arise the

00:15:15,870 --> 00:15:19,050
only way you're going to start to be

00:15:17,430 --> 00:15:20,910
able to understand what went wrong when

00:15:19,050 --> 00:15:23,250
it does all crash in a heap is through

00:15:20,910 --> 00:15:25,709
monitoring if you treat your system as a

00:15:23,250 --> 00:15:28,890
black box how you know what is going on

00:15:25,709 --> 00:15:30,400
inside one thing for sure is all those

00:15:28,890 --> 00:15:33,010
benchmarks you may have run

00:15:30,400 --> 00:15:36,330
what was the fastest web server was and

00:15:33,010 --> 00:15:36,330
not going to help you one bit

00:15:36,450 --> 00:15:41,560
server monitoring tools although useful

00:15:38,880 --> 00:15:43,540
only show the effect of the problem the

00:15:41,560 --> 00:15:45,640
overall system they don't necessarily

00:15:43,540 --> 00:15:47,530
provide you that insight of what is

00:15:45,640 --> 00:15:50,170
going on inside of your web application

00:15:47,530 --> 00:15:52,270
as they still largely treat your web web

00:15:50,170 --> 00:15:54,970
application and web server like a black

00:15:52,270 --> 00:15:57,880
box a deeper level of introspection is

00:15:54,970 --> 00:15:59,530
required when we talk about finding out

00:15:57,880 --> 00:16:01,420
what is happening inside of your Python

00:15:59,530 --> 00:16:03,730
web application the options have been

00:16:01,420 --> 00:16:06,400
limited tools such as Django debug

00:16:03,730 --> 00:16:08,190
toolbar or using the Python profiler are

00:16:06,400 --> 00:16:10,360
only suited to a development environment

00:16:08,190 --> 00:16:12,430
century can be used in production to

00:16:10,360 --> 00:16:14,410
capture errors but performance problems

00:16:12,430 --> 00:16:18,580
aren't going to generate nice exceptions

00:16:14,410 --> 00:16:19,990
for you historical lack of good tools

00:16:18,580 --> 00:16:21,550
for knowing what is going on inside of

00:16:19,990 --> 00:16:24,850
your Python web applications why I'm

00:16:21,550 --> 00:16:26,440
loving my current D job if you hadn't

00:16:24,850 --> 00:16:28,570
managed to miss it I am now working at

00:16:26,440 --> 00:16:30,610
New Relic New Relic performance

00:16:28,570 --> 00:16:32,290
monitoring provides the ability to

00:16:30,610 --> 00:16:34,240
monitor the front-end your web

00:16:32,290 --> 00:16:36,220
application and the underlying server

00:16:34,240 --> 00:16:37,840
I'm bringing all that goodness now to

00:16:36,220 --> 00:16:39,250
the world of Python web and that's why

00:16:37,840 --> 00:16:41,290
I'm excited about being there

00:16:39,250 --> 00:16:43,480
Sony really gives you that deep

00:16:41,290 --> 00:16:46,750
introspection required to know what is

00:16:43,480 --> 00:16:49,540
going on I'm of course also the author

00:16:46,750 --> 00:16:51,250
of mod WSGI being able to get new relic

00:16:49,540 --> 00:16:53,110
working with Python means I've been able

00:16:51,250 --> 00:16:54,790
to use the reporting it provides to

00:16:53,110 --> 00:16:57,520
delve quite deeply into the behavior of

00:16:54,790 --> 00:16:59,740
mod WSGI and at different situations the

00:16:57,520 --> 00:17:00,940
results have been quite revealing one of

00:16:59,740 --> 00:17:03,400
the areas it has helped in understanding

00:17:00,940 --> 00:17:05,770
is a funneling effects when using demon

00:17:03,400 --> 00:17:08,650
mode and I'll admit there is room for

00:17:05,770 --> 00:17:09,700
improvement with mod wsj and I'll be

00:17:08,650 --> 00:17:14,080
trying to address some of the issues

00:17:09,700 --> 00:17:16,000
I've uncovered in mod W so for summing

00:17:14,080 --> 00:17:17,890
things up pick a web server and

00:17:16,000 --> 00:17:20,260
architecture which seems to meet your

00:17:17,890 --> 00:17:22,780
requirements then use benchmarks to

00:17:20,260 --> 00:17:24,790
evaluate its behavior don't use

00:17:22,780 --> 00:17:27,010
benchmarks simply to try and compare

00:17:24,790 --> 00:17:29,380
different systems don't trust server

00:17:27,010 --> 00:17:31,360
defaults configure and tune your whole

00:17:29,380 --> 00:17:34,330
stack based on the results you get from

00:17:31,360 --> 00:17:36,640
live production monitoring Chinese

00:17:34,330 --> 00:17:38,050
really really deep introspection to what

00:17:36,640 --> 00:17:39,060
is going on in all the parts of your

00:17:38,050 --> 00:17:41,230
system

00:17:39,060 --> 00:17:42,880
so if you are doing Python web

00:17:41,230 --> 00:17:43,890
application develop do consider giving

00:17:42,880 --> 00:17:46,770
new relic a try

00:17:43,890 --> 00:17:48,570
if you're not sure you really does

00:17:46,770 --> 00:17:51,540
provide a free trial period when you can

00:17:48,570 --> 00:17:53,040
try all the features it has and at the

00:17:51,540 --> 00:17:56,190
moment we're running a special

00:17:53,040 --> 00:17:57,900
conference trial extended trial period

00:17:56,190 --> 00:18:01,530
of 30 days where we've normally used at

00:17:57,900 --> 00:18:03,360
any two weeks so give that a go even

00:18:01,530 --> 00:18:04,860
when the trial ends a free light

00:18:03,360 --> 00:18:08,250
subscription is available which still

00:18:04,860 --> 00:18:10,020
provides a lot of useful information and

00:18:08,250 --> 00:18:12,179
finally if you if you want to work at

00:18:10,020 --> 00:18:13,650
New Relic to come talk to us right now

00:18:12,179 --> 00:18:16,290
we are looking for a Python developer

00:18:13,650 --> 00:18:19,730
important so while you think about how

00:18:16,290 --> 00:18:23,910
cool that might be we're working with me

00:18:19,730 --> 00:18:26,870
I'll work you hard we'll see with we've

00:18:23,910 --> 00:18:26,870
got plenty of time for questions

00:18:40,380 --> 00:18:44,590
so do we have any questions and if you

00:18:43,270 --> 00:18:46,090
have a question there's a microphone up

00:18:44,590 --> 00:18:48,720
microphone here up front of you can

00:18:46,090 --> 00:18:48,720
please come down

00:18:53,720 --> 00:19:00,919
I hope you're just not on the way at the

00:18:55,730 --> 00:19:04,039
door no I do have a question up since

00:19:00,919 --> 00:19:06,970
it's a theme today have you had any

00:19:04,039 --> 00:19:10,549
chance to observe the effect of pi PI on

00:19:06,970 --> 00:19:12,230
performance in webservers no I haven't

00:19:10,549 --> 00:19:13,909
it's something I really want to do

00:19:12,230 --> 00:19:15,139
because I'll see without a new relic

00:19:13,909 --> 00:19:18,440
stuffing what actually make sure it

00:19:15,139 --> 00:19:19,820
works on pi PI it's pure Python so

00:19:18,440 --> 00:19:21,110
fairly clear it should but I haven't

00:19:19,820 --> 00:19:23,509
tried it's one of those things I really

00:19:21,110 --> 00:19:25,730
want to get to but from all I've heard

00:19:23,509 --> 00:19:27,590
about it it should be wonderful when PI

00:19:25,730 --> 00:19:29,779
PI's to that level of quality where we

00:19:27,590 --> 00:19:32,629
can trust it for production web apps ok

00:19:29,779 --> 00:19:35,330
I I asked because I know some people

00:19:32,629 --> 00:19:38,149
already are doing production whiskey

00:19:35,330 --> 00:19:39,529
with pi PI and I was just curious if

00:19:38,149 --> 00:19:41,389
it's something that you guys have

00:19:39,529 --> 00:19:47,570
already started looking into no I

00:19:41,389 --> 00:19:49,879
haven't had a chance ok thank you hi can

00:19:47,570 --> 00:19:52,789
you talk about um any other techniques

00:19:49,879 --> 00:19:55,519
for instrumenting your web server and

00:19:52,789 --> 00:19:57,049
application other than new relics but

00:19:55,519 --> 00:19:58,639
say I want to get data into the like

00:19:57,049 --> 00:20:00,919
graphite or something like that

00:19:58,639 --> 00:20:03,230
oh my understanding of graphite is that

00:20:00,919 --> 00:20:05,509
it is going to graph the date it's not

00:20:03,230 --> 00:20:07,220
necessary it's not necessarily providing

00:20:05,509 --> 00:20:09,169
you a I need to get that data somehow

00:20:07,220 --> 00:20:10,759
yes you need to get the data somehow and

00:20:09,169 --> 00:20:13,009
that's that's where at the moment

00:20:10,759 --> 00:20:14,809
there's been that whole in terms of

00:20:13,009 --> 00:20:16,879
getting data out of web applications and

00:20:14,809 --> 00:20:21,259
that's where new relic we doing stuff

00:20:16,879 --> 00:20:23,809
obviously in terms of some general data

00:20:21,259 --> 00:20:26,600
collection I don't know if you saw atom

00:20:23,809 --> 00:20:29,419
Lowery's talk yesterday he's doing work

00:20:26,600 --> 00:20:32,440
on trying to get metrics out of

00:20:29,419 --> 00:20:34,399
applications as well one of the things I

00:20:32,440 --> 00:20:36,019
discussed with him is one of the ideas

00:20:34,399 --> 00:20:41,450
I've had which has come up with a pep

00:20:36,019 --> 00:20:44,570
for a standard for a data sampler API so

00:20:41,450 --> 00:20:46,490
that we define this thing and people

00:20:44,570 --> 00:20:47,720
with any sort of application which we

00:20:46,490 --> 00:20:50,299
will want to generate some sort of

00:20:47,720 --> 00:20:53,600
metrics can write stuff to produce data

00:20:50,299 --> 00:20:56,090
that can be produce try this sampler API

00:20:53,600 --> 00:20:57,679
and the idea then is that sampling API

00:20:56,090 --> 00:20:59,570
could be registered with new rally or

00:20:57,679 --> 00:21:02,480
someone could provide an adapter to push

00:20:59,570 --> 00:21:04,399
that data into graphite or anything else

00:21:02,480 --> 00:21:06,470
that people want to do so I'm really

00:21:04,399 --> 00:21:06,870
keen on sort of trying to come up with

00:21:06,470 --> 00:21:09,030
that

00:21:06,870 --> 00:21:11,940
sort of push this whole area of metrics

00:21:09,030 --> 00:21:14,070
collection for reps and there's a open

00:21:11,940 --> 00:21:18,420
space that Adams set up for later this

00:21:14,070 --> 00:21:20,670
afternoon are covering what time which

00:21:18,420 --> 00:21:21,960
you may well be interested in coming

00:21:20,670 --> 00:21:23,090
along to if you're interested in that

00:21:21,960 --> 00:21:26,090
sort of stuff

00:21:23,090 --> 00:21:26,090
Thanks

00:21:30,179 --> 00:21:33,019
no one else

00:21:34,120 --> 00:21:38,120
you you mentioned that pre-loading your

00:21:36,679 --> 00:21:39,919
application is pretty critical and

00:21:38,120 --> 00:21:41,150
front-loading some of that work I was

00:21:39,919 --> 00:21:43,160
wondering if you could comment on some

00:21:41,150 --> 00:21:44,770
techniques at least with mod whiskey to

00:21:43,160 --> 00:21:48,860
make sure that your application is

00:21:44,770 --> 00:21:53,960
pre-loaded correctly and completely in

00:21:48,860 --> 00:21:55,940
mod WSGI there is well there's a

00:21:53,960 --> 00:21:59,030
directive called WSGI import script

00:21:55,940 --> 00:22:02,210
which you can use to say i want to

00:21:59,030 --> 00:22:05,650
preload this WSGI scripts could be the

00:22:02,210 --> 00:22:09,190
WCA file itself using for you out and

00:22:05,650 --> 00:22:11,570
say I want to preload it into this

00:22:09,190 --> 00:22:14,090
embedded mode or daemon process group

00:22:11,570 --> 00:22:17,090
into this interpreter and that'll be

00:22:14,090 --> 00:22:19,250
done when the process starts up now one

00:22:17,090 --> 00:22:22,400
issue with pre loading though is you

00:22:19,250 --> 00:22:25,240
look at something like Django if you

00:22:22,400 --> 00:22:28,070
follow the traditional recipe for

00:22:25,240 --> 00:22:31,790
setting up the wsgi application it

00:22:28,070 --> 00:22:33,679
imports various django files but it

00:22:31,790 --> 00:22:36,620
doesn't import your junk your actual

00:22:33,679 --> 00:22:40,669
real app code and so even if you can

00:22:36,620 --> 00:22:42,890
sort of do fiddles to try and make it

00:22:40,669 --> 00:22:44,870
force import more and i've got a blog

00:22:42,890 --> 00:22:45,440
post which i sort of explain one way of

00:22:44,870 --> 00:22:47,030
doing that

00:22:45,440 --> 00:22:49,250
it still doesn't import all the

00:22:47,030 --> 00:22:52,610
application code anyway so it's delays

00:22:49,250 --> 00:22:54,710
Lilo's a lot so you can try but it's not

00:22:52,610 --> 00:23:00,679
always effective depending on what the

00:22:54,710 --> 00:23:03,070
particular web framework does that

00:23:00,679 --> 00:23:03,070
answer the question

00:23:04,980 --> 00:23:10,539
yes and no it depends on the framework

00:23:08,279 --> 00:23:13,450
but it would be it would be nice if

00:23:10,539 --> 00:23:17,110
there were better ways provided for

00:23:13,450 --> 00:23:20,019
applications to somehow allow you

00:23:17,110 --> 00:23:21,789
control over pre-loading so you can sort

00:23:20,019 --> 00:23:23,919
of get your application hot into memory

00:23:21,789 --> 00:23:25,419
first before you start whacking requests

00:23:23,919 --> 00:23:30,789
that other way to get that little bit a

00:23:25,419 --> 00:23:33,639
delays on your startup cost in a patch

00:23:30,789 --> 00:23:36,779
e24 they introduced the lucky event back

00:23:33,639 --> 00:23:38,860
in you have like will mod whiskey work

00:23:36,779 --> 00:23:41,440
potentially better with that or do you

00:23:38,860 --> 00:23:44,590
have some it inside that works with it

00:23:41,440 --> 00:23:46,480
provided you use the mod whi source code

00:23:44,590 --> 00:23:48,730
from the repository because I haven't

00:23:46,480 --> 00:23:51,309
released back port I haven't back ported

00:23:48,730 --> 00:23:54,029
any changes for some changes they made

00:23:51,309 --> 00:23:57,100
at the last minute I went from like

00:23:54,029 --> 00:23:59,320
2.3.6 Deane to 2.4 and they made a

00:23:57,100 --> 00:24:01,450
change to dot 3.16 which broke the code

00:23:59,320 --> 00:24:04,470
and I only found out about it last week

00:24:01,450 --> 00:24:07,360
so I was rushing to fix it

00:24:04,470 --> 00:24:09,190
yeah the performance is interesting I I

00:24:07,360 --> 00:24:13,509
only looked at the performance first

00:24:09,190 --> 00:24:15,279
time last week as well and I my box was

00:24:13,509 --> 00:24:18,970
in a really weird state so I don't know

00:24:15,279 --> 00:24:21,009
whether my just the measurements I was

00:24:18,970 --> 00:24:23,350
getting from my apache to - installation

00:24:21,009 --> 00:24:26,440
which was the default Apple one was

00:24:23,350 --> 00:24:29,049
right or not but for hello will which I

00:24:26,440 --> 00:24:32,259
detest as a benchmark it was actually

00:24:29,049 --> 00:24:36,639
showing twice as much fruit put on event

00:24:32,259 --> 00:24:41,200
npm 2.4 then would have been pre for

00:24:36,639 --> 00:24:43,179
chem p.m. on 2.2 that's got me a little

00:24:41,200 --> 00:24:47,129
excited but i need to go back and look

00:24:43,179 --> 00:24:47,129
properly so it looks promising

00:24:57,880 --> 00:25:01,590

YouTube URL: https://www.youtube.com/watch?v=Bt2HStzaBzE


