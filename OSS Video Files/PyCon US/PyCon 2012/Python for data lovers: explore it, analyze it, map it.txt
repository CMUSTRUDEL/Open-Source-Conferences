Title: Python for data lovers: explore it, analyze it, map it
Publication date: 2012-04-29
Playlist: PyCon 2012
Description: 
	Jacqueline Kazil, Dana Bauer
Exploring and analyzing data can be daunting and time-consuming, even for data lovers. Python can make the process fun and exciting. We will present techniques of data analysis, along with python tools that help you explo
Captions: 
	00:00:07,790 --> 00:00:14,820
good afternoon so first we have a slight

00:00:11,580 --> 00:00:18,210
revision to our talk title it's going to

00:00:14,820 --> 00:00:22,320
be python for open data lovers because

00:00:18,210 --> 00:00:24,660
we love open data and that's going to be

00:00:22,320 --> 00:00:28,650
a significant emphasis in our talk so

00:00:24,660 --> 00:00:31,980
our big goal today is to use data to

00:00:28,650 --> 00:00:34,280
build a story about how and where DC

00:00:31,980 --> 00:00:36,840
city agencies are spending their money

00:00:34,280 --> 00:00:38,700
and as part of that process we're going

00:00:36,840 --> 00:00:41,820
to show you several different Python

00:00:38,700 --> 00:00:44,070
libraries and python based tools and

00:00:41,820 --> 00:00:46,110
other open source tools for analysis but

00:00:44,070 --> 00:00:50,239
first we want to tell you a little bit

00:00:46,110 --> 00:00:53,760
about us so Jackie

00:00:50,239 --> 00:00:56,460
so my name is Jackie during the day I

00:00:53,760 --> 00:01:04,199
work for CACI the Library of Congress

00:00:56,460 --> 00:01:06,180
and at night that was me having fun at

00:01:04,199 --> 00:01:10,920
PyCon with Zane

00:01:06,180 --> 00:01:14,220
and at night I I'm doing some graduate

00:01:10,920 --> 00:01:16,140
studies at George Mason and I helped run

00:01:14,220 --> 00:01:19,979
Django district this was our last Meetup

00:01:16,140 --> 00:01:22,380
it's my little plug to come to Django

00:01:19,979 --> 00:01:36,180
con DC because that's how we do Django

00:01:22,380 --> 00:01:38,579
in DC so uh so yeah that's me so I'm a

00:01:36,180 --> 00:01:41,369
geographer based in Philadelphia I make

00:01:38,579 --> 00:01:46,259
maps and analyze data for a living

00:01:41,369 --> 00:01:47,970
mostly for non-profit clients this is

00:01:46,259 --> 00:01:50,610
one particular map where I looked at

00:01:47,970 --> 00:01:52,770
poverty in Philadelphia and locations of

00:01:50,610 --> 00:01:55,680
child care facilities I ran got open

00:01:52,770 --> 00:01:57,119
data all the time in its many forms and

00:01:55,680 --> 00:02:00,869
I use lots of different tools to do this

00:01:57,119 --> 00:02:03,509
and kind of agnostic about that I use

00:02:00,869 --> 00:02:08,700
mostly open source sometimes Python but

00:02:03,509 --> 00:02:11,280
not always I also organize intro to

00:02:08,700 --> 00:02:13,210
Python workshops through the

00:02:11,280 --> 00:02:16,360
Philadelphia Python users group

00:02:13,210 --> 00:02:19,180
and we really relied a lot on the Boston

00:02:16,360 --> 00:02:22,180
Python workshop folks and the pie ladies

00:02:19,180 --> 00:02:23,920
to help us get started and it's really

00:02:22,180 --> 00:02:26,140
nice that several of our workshop

00:02:23,920 --> 00:02:30,370
instructors and students are here at

00:02:26,140 --> 00:02:31,930
PyCon thanks to pi ladies and the PSF so

00:02:30,370 --> 00:02:37,680
that's something that I'm really proud

00:02:31,930 --> 00:02:40,270
of so where are we going today first

00:02:37,680 --> 00:02:43,690
we're gonna give you a little bit of an

00:02:40,270 --> 00:02:46,000
overview of the open data movement these

00:02:43,690 --> 00:02:48,400
enormous amounts of data that have been

00:02:46,000 --> 00:02:51,250
released by cities counties States the

00:02:48,400 --> 00:02:54,850
federal government in formats that are

00:02:51,250 --> 00:02:57,460
for the most part consumable by ordinary

00:02:54,850 --> 00:02:59,380
people like us and then we're gonna

00:02:57,460 --> 00:03:01,810
describe one particular data set from

00:02:59,380 --> 00:03:03,910
the DC catalog that Jackie and I found

00:03:01,810 --> 00:03:05,770
interesting and this data set will be

00:03:03,910 --> 00:03:09,010
the thread that pulls us through our

00:03:05,770 --> 00:03:10,570
talk we're going to talk specifically

00:03:09,010 --> 00:03:12,850
about some tools that can help you

00:03:10,570 --> 00:03:16,300
explore open data and we're gonna start

00:03:12,850 --> 00:03:18,430
with a tool that is really simple and

00:03:16,300 --> 00:03:21,730
really powerful we like to call it the

00:03:18,430 --> 00:03:24,370
data Swiss Army knife and I believe

00:03:21,730 --> 00:03:26,110
others have called it that before and

00:03:24,370 --> 00:03:28,690
then we'll talk a little bit about more

00:03:26,110 --> 00:03:31,540
complex Python based tools that will

00:03:28,690 --> 00:03:34,060
help us look at patterns in the data and

00:03:31,540 --> 00:03:36,250
try to make meaning and at the end of

00:03:34,060 --> 00:03:38,560
the talk we're going to discuss some

00:03:36,250 --> 00:03:41,860
steps for how you might go from a data

00:03:38,560 --> 00:03:43,510
analysis to actually building a story

00:03:41,860 --> 00:03:49,410
that you could share with users and

00:03:43,510 --> 00:03:52,510
readers in many different forms okay so

00:03:49,410 --> 00:03:56,740
I'm not going to provide a full overview

00:03:52,510 --> 00:03:58,360
of all Open Data initiatives but let's

00:03:56,740 --> 00:04:00,220
suffice it to say that there's been an

00:03:58,360 --> 00:04:04,540
explosion of data in the last few years

00:04:00,220 --> 00:04:06,940
in this Civic sphere and it's sort of a

00:04:04,540 --> 00:04:09,700
party out there of liberating data

00:04:06,940 --> 00:04:11,320
connecting data gold Starring data

00:04:09,700 --> 00:04:12,760
tweeting about data blogging about data

00:04:11,320 --> 00:04:14,920
hacking on data if you read the language

00:04:12,760 --> 00:04:17,650
on some of these sites and these are

00:04:14,920 --> 00:04:21,340
just some examples of different City

00:04:17,650 --> 00:04:21,910
data portals some cities do it better

00:04:21,340 --> 00:04:24,340
than other

00:04:21,910 --> 00:04:27,100
but you know there's been a lot of

00:04:24,340 --> 00:04:35,050
exciting development in just the past

00:04:27,100 --> 00:04:38,440
couple of years and so a lot of the

00:04:35,050 --> 00:04:40,630
sites organize their offerings in many

00:04:38,440 --> 00:04:42,370
different ways by file type but a

00:04:40,630 --> 00:04:46,480
standard way of presenting it to

00:04:42,370 --> 00:04:49,540
laypeople is data from A to Z and so

00:04:46,480 --> 00:04:52,120
I've pulled a smattering of selections

00:04:49,540 --> 00:04:56,740
from all these different City data sites

00:04:52,120 --> 00:04:58,960
and when I see this list I see potential

00:04:56,740 --> 00:05:01,200
stories and because I'm a geographer I

00:04:58,960 --> 00:05:05,380
look at this list and I think about maps

00:05:01,200 --> 00:05:08,200
and in fact many of the data sets on

00:05:05,380 --> 00:05:10,750
this list have been in the news for

00:05:08,200 --> 00:05:14,100
example Assemblymember expenses from the

00:05:10,750 --> 00:05:16,120
London data store that was in the news

00:05:14,100 --> 00:05:17,530
people were spending money on things

00:05:16,120 --> 00:05:19,450
they shouldn't have and I think some of

00:05:17,530 --> 00:05:21,550
them actually got fired when the

00:05:19,450 --> 00:05:26,680
Guardian started writing about it in

00:05:21,550 --> 00:05:28,360
Philly the transit data has become quite

00:05:26,680 --> 00:05:30,160
a quite a thing and there's a lot of

00:05:28,360 --> 00:05:34,330
momentum about around building mobile

00:05:30,160 --> 00:05:38,470
apps or septa regional transit authority

00:05:34,330 --> 00:05:40,240
I spend a lot of time in my past job

00:05:38,470 --> 00:05:42,340
looking at political districts and

00:05:40,240 --> 00:05:44,169
measuring compactness and trying to come

00:05:42,340 --> 00:05:47,680
up with measures for gerrymandering and

00:05:44,169 --> 00:05:49,450
all of that is open data I have some

00:05:47,680 --> 00:05:52,990
colleagues who look at violent crime

00:05:49,450 --> 00:05:56,169
incidents on any number of cities so

00:05:52,990 --> 00:05:59,430
that last data set I couldn't come up

00:05:56,169 --> 00:06:02,620
with an X so I gave you an extra are

00:05:59,430 --> 00:06:06,120
real-time parking availability in

00:06:02,620 --> 00:06:06,120
pricing it's actually an API

00:06:06,320 --> 00:06:10,850
the San Francisco Data and that is I

00:06:08,840 --> 00:06:13,250
think one of the most amazing creations

00:06:10,850 --> 00:06:19,160
from open data and every city should

00:06:13,250 --> 00:06:23,270
have something like that so our focus

00:06:19,160 --> 00:06:26,330
today is on the DC data catalog and

00:06:23,270 --> 00:06:29,480
Jackie and I decided to pull datasets

00:06:26,330 --> 00:06:32,780
from this site to be the focus of our

00:06:29,480 --> 00:06:35,180
presentation today this particular site

00:06:32,780 --> 00:06:37,580
was launched during the Adrian Fenty

00:06:35,180 --> 00:06:40,880
administration and the city got a lot of

00:06:37,580 --> 00:06:43,220
kudos for doing it right and one open

00:06:40,880 --> 00:06:45,710
data advocate said it wasn't just the

00:06:43,220 --> 00:06:47,600
breadth of data on the site it was that

00:06:45,710 --> 00:06:50,630
DC seemed to have integrated the

00:06:47,600 --> 00:06:55,180
principles of open data into its very

00:06:50,630 --> 00:06:58,220
DNA so the offerings here are really

00:06:55,180 --> 00:06:59,780
extensive and pretty impressive it might

00:06:58,220 --> 00:07:02,120
not be the easiest site to navigate but

00:06:59,780 --> 00:07:07,790
but there's a tremendous amount of data

00:07:02,120 --> 00:07:12,770
here just as a side note when Vince gray

00:07:07,790 --> 00:07:16,970
came into office he closed off a portion

00:07:12,770 --> 00:07:19,640
of the city data catalog that was at one

00:07:16,970 --> 00:07:22,190
time readily available and there's a

00:07:19,640 --> 00:07:23,540
great blog post about this from the Open

00:07:22,190 --> 00:07:26,030
Knowledge Foundation if you're

00:07:23,540 --> 00:07:29,180
interested in reading it it basically

00:07:26,030 --> 00:07:36,890
had to do with date on registered

00:07:29,180 --> 00:07:39,470
businesses in the city so let's take a

00:07:36,890 --> 00:07:43,010
little bit of a look at our study area

00:07:39,470 --> 00:07:46,190
here just to orient you most of you

00:07:43,010 --> 00:07:48,980
probably know that DC is really nestled

00:07:46,190 --> 00:07:53,300
in between Maryland and Virginia so any

00:07:48,980 --> 00:07:55,010
look at the business landscape in DC is

00:07:53,300 --> 00:08:00,260
going to have to take these two other

00:07:55,010 --> 00:08:02,710
states into account you know and so

00:08:00,260 --> 00:08:06,069
they're obviously there there are some

00:08:02,710 --> 00:08:08,360
Geographic constraints here on

00:08:06,069 --> 00:08:11,060
the pattern of businesses within the

00:08:08,360 --> 00:08:12,949
city there are also some tax constraints

00:08:11,060 --> 00:08:15,319
and permit constraints that affect how

00:08:12,949 --> 00:08:21,740
how businesses spread in the city as

00:08:15,319 --> 00:08:24,590
well so a few of the questions that we

00:08:21,740 --> 00:08:27,470
were really interested in were what our

00:08:24,590 --> 00:08:29,210
DC agencies spending their money on how

00:08:27,470 --> 00:08:31,069
much are they spending what are the

00:08:29,210 --> 00:08:33,080
relationships between these businesses

00:08:31,069 --> 00:08:36,800
and city agencies and where these

00:08:33,080 --> 00:08:39,589
businesses located and so to answer some

00:08:36,800 --> 00:08:42,380
of these questions we pulled down seven

00:08:39,589 --> 00:08:44,899
years worth of purchase order data from

00:08:42,380 --> 00:08:48,800
the city which sounds kind of crazy it

00:08:44,899 --> 00:08:51,380
was all in CSV format but we wanted to

00:08:48,800 --> 00:08:53,570
get a look at some of the spending over

00:08:51,380 --> 00:08:57,230
a long period of time and then open data

00:08:53,570 --> 00:09:00,950
site made that possible for us and so of

00:08:57,230 --> 00:09:04,279
course when you pull data from a site in

00:09:00,950 --> 00:09:07,579
CVS format you get this if you're

00:09:04,279 --> 00:09:10,940
looking at it in a text file which is

00:09:07,579 --> 00:09:15,410
kind of daunting for some people who are

00:09:10,940 --> 00:09:17,570
perhaps new to open data or Python but

00:09:15,410 --> 00:09:18,320
lucky for us this is our data Swiss Army

00:09:17,570 --> 00:09:22,040
knife

00:09:18,320 --> 00:09:24,290
it's called CSV kit it's a amazing set

00:09:22,040 --> 00:09:27,529
of Python utilities for working with

00:09:24,290 --> 00:09:29,690
comma delimited files and it was

00:09:27,529 --> 00:09:32,690
developed by some journalists at the

00:09:29,690 --> 00:09:34,760
Chicago Tribune the documentation is

00:09:32,690 --> 00:09:36,560
excellent and there are a number of

00:09:34,760 --> 00:09:39,410
tutorials out there and for people who

00:09:36,560 --> 00:09:42,140
are fairly new to coding or command line

00:09:39,410 --> 00:09:44,450
it's a really fantastic introduction to

00:09:42,140 --> 00:09:51,170
working at the command line and it's

00:09:44,450 --> 00:09:53,990
super super easy to install so one of

00:09:51,170 --> 00:09:55,550
the first things we did with CSV kit

00:09:53,990 --> 00:09:57,380
when we pulled down these comma

00:09:55,550 --> 00:09:59,329
delimited files of purchase or Gaede

00:09:57,380 --> 00:10:01,910
order data was just to take a quick look

00:09:59,329 --> 00:10:04,070
at what we had and this is one way to

00:10:01,910 --> 00:10:05,740
examine some of the headings so we have

00:10:04,070 --> 00:10:09,589
purchase order numbers agency names

00:10:05,740 --> 00:10:12,279
descriptions the money of course date

00:10:09,589 --> 00:10:12,279
and suppliers

00:10:13,950 --> 00:10:19,890
you can also cut the some of the data

00:10:17,670 --> 00:10:24,960
and get some basic summary statistics

00:10:19,890 --> 00:10:28,890
so here were summarizing a number of

00:10:24,960 --> 00:10:31,560
agencies number of suppliers and the

00:10:28,890 --> 00:10:33,570
groups within both of those camps that

00:10:31,560 --> 00:10:39,690
have had the most transactions and this

00:10:33,570 --> 00:10:41,670
is just 2011 so you can also if you have

00:10:39,690 --> 00:10:45,000
want to look for something in the data

00:10:41,670 --> 00:10:47,280
you can use a little bit of regular

00:10:45,000 --> 00:10:49,170
expressions in with CSV Ken in this

00:10:47,280 --> 00:10:50,910
particular case we were focusing on

00:10:49,170 --> 00:10:52,830
how many different transactions that

00:10:50,910 --> 00:10:59,850
were with the maya angelou charter

00:10:52,830 --> 00:11:05,220
school and so here was just a quick look

00:10:59,850 --> 00:11:06,960
at the top transactions so where a lot

00:11:05,220 --> 00:11:09,510
of the money was going transaction by

00:11:06,960 --> 00:11:11,480
transaction and this alone could be the

00:11:09,510 --> 00:11:15,000
basis for it for any number of stories

00:11:11,480 --> 00:11:18,270
so a huge amount of money to

00:11:15,000 --> 00:11:21,330
construction companies a lot of money

00:11:18,270 --> 00:11:25,140
spent on healthcare in prisons money

00:11:21,330 --> 00:11:33,300
spent in schools so we found this alone

00:11:25,140 --> 00:11:35,130
very interesting so moving beyond that

00:11:33,300 --> 00:11:37,800
first look at the data we decided to

00:11:35,130 --> 00:11:43,500
start with some more complex pattern

00:11:37,800 --> 00:11:46,230
analysis so the international network

00:11:43,500 --> 00:11:47,850
for social network analysis describes

00:11:46,230 --> 00:11:51,150
social network analysis on their website

00:11:47,850 --> 00:11:52,920
as social network analysis is focused on

00:11:51,150 --> 00:11:57,150
uncovering the pattern of people's

00:11:52,920 --> 00:12:00,270
interaction you know it's it's really

00:11:57,150 --> 00:12:03,810
hard to describe social network analysis

00:12:00,270 --> 00:12:10,920
in a few minutes so I want to try to do

00:12:03,810 --> 00:12:12,590
this by example so this was a social

00:12:10,920 --> 00:12:17,270
network analysis project on

00:12:12,590 --> 00:12:21,570
congressional votes that I had done a

00:12:17,270 --> 00:12:23,280
year ago or something so this is looking

00:12:21,570 --> 00:12:26,200
at how

00:12:23,280 --> 00:12:28,930
individuals in Congress vote together in

00:12:26,200 --> 00:12:31,240
the same way so they both approve they

00:12:28,930 --> 00:12:33,400
get a link if they both disapprove they

00:12:31,240 --> 00:12:38,050
can link they're absent they're not

00:12:33,400 --> 00:12:41,230
counted so this was the 99th house there

00:12:38,050 --> 00:12:44,260
was a little cross-pollination but we're

00:12:41,230 --> 00:12:47,140
gonna go ahead and fast-forward to the

00:12:44,260 --> 00:12:48,910
hundred and seven where there is none

00:12:47,140 --> 00:12:56,580
and so what I want you to notice in

00:12:48,910 --> 00:13:00,340
these is that the whoever's in majority

00:12:56,580 --> 00:13:04,150
tends to have a closer clustering than

00:13:00,340 --> 00:13:05,110
the party that is not in majority and

00:13:04,150 --> 00:13:08,770
we're gonna go through a couple more

00:13:05,110 --> 00:13:13,920
years so again the Republicans have

00:13:08,770 --> 00:13:19,710
majority Republicans have the majority

00:13:13,920 --> 00:13:21,850
Democrats have the majority and

00:13:19,710 --> 00:13:23,590
Democrats have the majority but there's

00:13:21,850 --> 00:13:25,600
still there's still a lot of

00:13:23,590 --> 00:13:28,180
back-and-forth so the Republicans still

00:13:25,600 --> 00:13:30,190
have a pretty tight clustering there so

00:13:28,180 --> 00:13:34,210
this was just to try to show you guys an

00:13:30,190 --> 00:13:36,010
example outside of the context of social

00:13:34,210 --> 00:13:37,900
network analysis how a lot of people

00:13:36,010 --> 00:13:42,100
might think about it like with respect

00:13:37,900 --> 00:13:43,930
to Twitter or Facebook and those kinds

00:13:42,100 --> 00:13:53,200
of contexts directly relating to social

00:13:43,930 --> 00:13:55,030
networks okay so we took our CSV and

00:13:53,200 --> 00:13:57,850
turn it into a network this is really

00:13:55,030 --> 00:14:00,250
stripped-down code there are other steps

00:13:57,850 --> 00:14:04,150
to this but I just wanted to give you an

00:14:00,250 --> 00:14:06,970
idea of what this involves it's it's

00:14:04,150 --> 00:14:10,180
it's really simple you import Network X

00:14:06,970 --> 00:14:15,940
create a graph create a node edge list

00:14:10,180 --> 00:14:19,470
loop over your CSV file for your to add

00:14:15,940 --> 00:14:22,990
your nodes and possible edges and then

00:14:19,470 --> 00:14:25,300
loop over your possible edge node an

00:14:22,990 --> 00:14:27,760
edge list twice to compare it against

00:14:25,300 --> 00:14:30,090
itself and then add your edges to the

00:14:27,760 --> 00:14:30,090
graft

00:14:31,639 --> 00:14:40,079
so before we get to more fun stuff this

00:14:37,139 --> 00:14:41,790
is describing social network analysis I

00:14:40,079 --> 00:14:44,970
can't just pass over it without saying

00:14:41,790 --> 00:14:46,670
something about centrality so there's

00:14:44,970 --> 00:14:49,860
different ways to measure measure

00:14:46,670 --> 00:14:54,360
centrality there's a degree closeness

00:14:49,860 --> 00:14:55,499
between this and PageRank but I because

00:14:54,360 --> 00:14:58,740
I'm not going to go into great detail

00:14:55,499 --> 00:15:00,809
about this what I suggest is there's

00:14:58,740 --> 00:15:08,040
resources at the end of this talk that

00:15:00,809 --> 00:15:10,290
you get a book and and read more because

00:15:08,040 --> 00:15:14,089
what IMT what I'm showing today is just

00:15:10,290 --> 00:15:17,759
gonna make you dangerous very dangerous

00:15:14,089 --> 00:15:21,089
so running as centrality metrics against

00:15:17,759 --> 00:15:27,629
the network we have a couple of obvious

00:15:21,089 --> 00:15:30,809
winners up top and looking at these

00:15:27,629 --> 00:15:32,910
people didja Docs document managers

00:15:30,809 --> 00:15:35,929
offers software that generates loan

00:15:32,910 --> 00:15:39,120
documents for electronic delivery now

00:15:35,929 --> 00:15:43,139
before I continue let me explain what

00:15:39,120 --> 00:15:44,970
this means so we had some of the raw

00:15:43,139 --> 00:15:46,920
pools of data which you just query a

00:15:44,970 --> 00:15:49,230
data set and you can count transactions

00:15:46,920 --> 00:15:51,569
or count money or count sort of

00:15:49,230 --> 00:15:56,220
aggregated results this is showing

00:15:51,569 --> 00:15:58,889
overlap between agencies so there are a

00:15:56,220 --> 00:16:00,809
lot of agencies in DC who use digital

00:15:58,889 --> 00:16:03,779
sync

00:16:00,809 --> 00:16:05,309
Iron Mountain provides information

00:16:03,779 --> 00:16:07,439
management services that help

00:16:05,309 --> 00:16:10,079
organizations lower cost risks and

00:16:07,439 --> 00:16:14,939
inefficiencies of managing their

00:16:10,079 --> 00:16:16,860
physical and digital data so a lot of

00:16:14,939 --> 00:16:26,869
people a lot of different agencies pay

00:16:16,860 --> 00:16:33,230
money to to lower in efficiencies so MSB

00:16:26,869 --> 00:16:36,720
MVS Inc is a consulting technology yes

00:16:33,230 --> 00:16:38,370
MDM office office supplies there are

00:16:36,720 --> 00:16:42,660
quite a few office supplies on the list

00:16:38,370 --> 00:16:44,579
I just want to say I think this is not

00:16:42,660 --> 00:16:48,440
the only one of the ones that I saw that

00:16:44,579 --> 00:16:53,300
also office offers coffee services

00:16:48,440 --> 00:16:57,930
capital services and supplies just

00:16:53,300 --> 00:17:00,029
general general office solutions and the

00:16:57,930 --> 00:17:02,399
bottom two are both I just want to point

00:17:00,029 --> 00:17:07,980
out located in or the bottom three are

00:17:02,399 --> 00:17:09,540
all located in Washington DC I didn't

00:17:07,980 --> 00:17:11,280
include descriptions of the US Postal

00:17:09,540 --> 00:17:13,650
Service and Dell Computer Corporation

00:17:11,280 --> 00:17:19,500
because I figured most you guys would

00:17:13,650 --> 00:17:23,669
know what those where so visually

00:17:19,500 --> 00:17:25,439
visualizing the network I have to take a

00:17:23,669 --> 00:17:28,250
look at centrality what we want to do is

00:17:25,439 --> 00:17:31,230
try to see what this network looks like

00:17:28,250 --> 00:17:36,120
so you run it through you run the graph

00:17:31,230 --> 00:17:39,710
through a spring layout which sort of

00:17:36,120 --> 00:17:44,070
pulls nodes that are like alike together

00:17:39,710 --> 00:17:47,490
and then plotting the figure you can

00:17:44,070 --> 00:17:50,790
change the figure size I did 15 by 15

00:17:47,490 --> 00:17:54,570
for the purpose of this presentation and

00:17:50,790 --> 00:17:57,270
then you're drawing the nodes and we'll

00:17:54,570 --> 00:17:59,910
show you what the graph looks like so it

00:17:57,270 --> 00:18:02,250
looks like a hairball which a lot of

00:17:59,910 --> 00:18:04,020
these graphs do and it doesn't bring a

00:18:02,250 --> 00:18:07,410
lot of meaning so what we're going to do

00:18:04,020 --> 00:18:10,919
is trim some nodes and what this is

00:18:07,410 --> 00:18:13,110
doing is basically looking at the degree

00:18:10,919 --> 00:18:15,929
which was one of the centrality metric

00:18:13,110 --> 00:18:18,650
metrics so degree is how many nodes are

00:18:15,929 --> 00:18:22,280
connected to an individual node and

00:18:18,650 --> 00:18:25,860
basically removing the late week links

00:18:22,280 --> 00:18:31,890
and so this is of the original degree

00:18:25,860 --> 00:18:35,790
distribution it's kind of kind of power

00:18:31,890 --> 00:18:39,809
Lodge well definitely power loss except

00:18:35,790 --> 00:18:42,450
for that this random spike which I have

00:18:39,809 --> 00:18:44,670
not been able to explain so that's

00:18:42,450 --> 00:18:47,400
that's part of the more research

00:18:44,670 --> 00:18:53,100
so this is after trimming down some

00:18:47,400 --> 00:18:54,480
nodes trimming a little more and then we

00:18:53,100 --> 00:18:57,900
have the center of the network which is

00:18:54,480 --> 00:19:00,080
a little bit easier to see what's going

00:18:57,900 --> 00:19:00,080
on

00:19:00,920 --> 00:19:06,630
so then adding labels to the network can

00:19:04,350 --> 00:19:08,730
help you identify who is located where

00:19:06,630 --> 00:19:14,700
in the network this is really hard to

00:19:08,730 --> 00:19:20,940
show without without having a larger

00:19:14,700 --> 00:19:24,990
screen but I will show you a bit there

00:19:20,940 --> 00:19:27,570
we go so then you can see where your

00:19:24,990 --> 00:19:31,680
nodes are located for the things you

00:19:27,570 --> 00:19:34,530
could do is also and I haven't I didn't

00:19:31,680 --> 00:19:36,410
include this but you could size nodes by

00:19:34,530 --> 00:19:40,620
money spent and sort of see

00:19:36,410 --> 00:19:42,210
juxtaposition of where the nodes are but

00:19:40,620 --> 00:19:44,780
now I'm going to turn it over to Dana

00:19:42,210 --> 00:19:44,780
for mapping

00:19:51,649 --> 00:20:00,179
so what Jackie actually just showed you

00:19:54,960 --> 00:20:01,649
was a special kind of map and they

00:20:00,179 --> 00:20:03,679
weren't Maps in the traditional sense

00:20:01,649 --> 00:20:05,570
but basically they were maps of

00:20:03,679 --> 00:20:08,999
relationships that are not inherently

00:20:05,570 --> 00:20:11,820
spatial and the jargon here is the

00:20:08,999 --> 00:20:14,820
spatialization of a spatial data but I

00:20:11,820 --> 00:20:16,830
promise I won't say that again so now

00:20:14,820 --> 00:20:19,379
we're gonna talk a little bit about how

00:20:16,830 --> 00:20:21,830
we might examine some of these patterns

00:20:19,379 --> 00:20:24,690
in space

00:20:21,830 --> 00:20:28,109
so first anyone here who's worked with

00:20:24,690 --> 00:20:30,659
spatial data knows that it's special and

00:20:28,109 --> 00:20:32,489
not always in a magical wonderful kind

00:20:30,659 --> 00:20:35,519
of way it can be your real pain to work

00:20:32,489 --> 00:20:40,049
with but the cool part of course is that

00:20:35,519 --> 00:20:41,940
it is mapable one of the challenge is on

00:20:40,049 --> 00:20:44,309
the other hand is that you have to deal

00:20:41,940 --> 00:20:47,399
with things like spatial references and

00:20:44,309 --> 00:20:49,379
projections and that can be really

00:20:47,399 --> 00:20:52,259
difficult and if you screw that up not

00:20:49,379 --> 00:20:54,960
only can your data look really funny but

00:20:52,259 --> 00:20:58,970
any kind of underlying analysis distance

00:20:54,960 --> 00:21:01,379
or density can be really really far off

00:20:58,970 --> 00:21:05,210
just an important thing to keep in mind

00:21:01,379 --> 00:21:07,649
with spatial data is this concept of

00:21:05,210 --> 00:21:10,279
things that are closer together and more

00:21:07,649 --> 00:21:13,019
alike than things that are farther apart

00:21:10,279 --> 00:21:15,330
and that's an oversimplification of

00:21:13,019 --> 00:21:18,239
Tobler's first law of geography but it's

00:21:15,330 --> 00:21:21,840
really a good thing to keep in mind when

00:21:18,239 --> 00:21:24,869
you're trying to understand how points

00:21:21,840 --> 00:21:28,049
or polygons in space interact and how

00:21:24,869 --> 00:21:33,239
you might describe in a mathematical way

00:21:28,049 --> 00:21:34,919
that particular interaction so there are

00:21:33,239 --> 00:21:38,159
lots of different types of spatial

00:21:34,919 --> 00:21:40,409
analysis like a lot of other types of

00:21:38,159 --> 00:21:43,710
data analysis you're basically trying to

00:21:40,409 --> 00:21:45,899
derive meaning from large data sets one

00:21:43,710 --> 00:21:49,649
of my favorite ways of looking at

00:21:45,899 --> 00:21:54,599
spatial analysis is yes data ESDA

00:21:49,649 --> 00:21:56,309
exploratory spatial analysis where

00:21:54,599 --> 00:21:58,780
you're running a lot of tests to explore

00:21:56,309 --> 00:22:02,200
local and global patterns and the data

00:21:58,780 --> 00:22:05,860
it's a great way to get a sense of what

00:22:02,200 --> 00:22:08,440
you have and what you don't know and it

00:22:05,860 --> 00:22:11,050
can be a fantastic way to help you ask

00:22:08,440 --> 00:22:13,540
better questions of your data spatial

00:22:11,050 --> 00:22:17,050
statistics is a type of spatial analysis

00:22:13,540 --> 00:22:19,750
and then more complex mathematical

00:22:17,050 --> 00:22:23,620
predictive modeling we won't get into

00:22:19,750 --> 00:22:25,060
that today there are a lot of different

00:22:23,620 --> 00:22:27,720
techniques that you can use we're

00:22:25,060 --> 00:22:31,600
looking at point data here so there are

00:22:27,720 --> 00:22:35,140
many ways that you can look at those

00:22:31,600 --> 00:22:36,700
spatial relationships between points and

00:22:35,140 --> 00:22:38,500
also there techniques that are

00:22:36,700 --> 00:22:40,120
particular to spatial statistics

00:22:38,500 --> 00:22:48,460
different types of spatial regression

00:22:40,120 --> 00:22:50,350
for example so there are many Python

00:22:48,460 --> 00:22:54,520
based tools for working with spatial

00:22:50,350 --> 00:22:59,920
data I come from a GIS mapping world so

00:22:54,520 --> 00:23:01,570
I like using a GIS to explore data and

00:22:59,920 --> 00:23:05,830
there are many different kinds all of

00:23:01,570 --> 00:23:07,750
them play really nicely with Python for

00:23:05,830 --> 00:23:12,010
this particular example I used a package

00:23:07,750 --> 00:23:15,730
called QGIS so let's back up a little

00:23:12,010 --> 00:23:23,950
bit and come back to our CSV so how do

00:23:15,730 --> 00:23:27,640
we get from this to a map it's not magic

00:23:23,950 --> 00:23:30,340
not even close and one of the most

00:23:27,640 --> 00:23:32,290
difficult things about preparing data

00:23:30,340 --> 00:23:33,190
for a map is the cleaning of it

00:23:32,290 --> 00:23:37,360
especially if you're going to be doing

00:23:33,190 --> 00:23:39,430
any geocoding and so here this is not at

00:23:37,360 --> 00:23:41,560
all a Python tool but I rely heavily on

00:23:39,430 --> 00:23:45,700
Google refine for this kind of work and

00:23:41,560 --> 00:23:48,000
it's an amazing way to find clusters in

00:23:45,700 --> 00:23:51,910
your data and correct things and mass

00:23:48,000 --> 00:23:54,940
you can also geo code within the Google

00:23:51,910 --> 00:23:59,560
refine environment and that's what I did

00:23:54,940 --> 00:24:04,990
here so what do we end up with points

00:23:59,560 --> 00:24:07,270
floating in space with no real context

00:24:04,990 --> 00:24:10,659
but you can probably see very roughly

00:24:07,270 --> 00:24:13,989
the outline of the United States there

00:24:10,659 --> 00:24:18,099
the eastern seaboard so let's add a

00:24:13,989 --> 00:24:22,330
little contacts here and project it so

00:24:18,099 --> 00:24:26,220
these are all of the businesses that DC

00:24:22,330 --> 00:24:30,070
agencies purchased from in 2011 and

00:24:26,220 --> 00:24:33,700
there you can see clustered mostly

00:24:30,070 --> 00:24:35,799
around major urban areas most of the

00:24:33,700 --> 00:24:39,159
points are in the eastern seaboard but

00:24:35,799 --> 00:24:41,859
you can really get just a visual sense

00:24:39,159 --> 00:24:46,389
of where what businesses were being

00:24:41,859 --> 00:24:51,129
solicited so it will zoom in a little

00:24:46,389 --> 00:24:57,249
bit to the DC area where our largest

00:24:51,129 --> 00:25:04,090
cluster is so you mean again to the city

00:24:57,249 --> 00:25:07,299
proper and so here I'm I'm showing you

00:25:04,090 --> 00:25:10,950
in large dark grey points where the

00:25:07,299 --> 00:25:14,559
actual agencies are and then the

00:25:10,950 --> 00:25:17,999
polylines our business revitalization

00:25:14,559 --> 00:25:22,330
corridors so one question we might ask

00:25:17,999 --> 00:25:24,779
is the city really trying to buy from

00:25:22,330 --> 00:25:27,070
businesses on along these corridors and

00:25:24,779 --> 00:25:36,519
that's something that we'd like to look

00:25:27,070 --> 00:25:39,759
at at some point so there's a really

00:25:36,519 --> 00:25:42,190
fantastic spatial analysis library in

00:25:39,759 --> 00:25:45,039
Python called pycelle and it was

00:25:42,190 --> 00:25:47,830
developed by researchers who are now at

00:25:45,039 --> 00:25:49,659
Arizona State University and the Giotto

00:25:47,830 --> 00:25:52,479
Center has a number of different

00:25:49,659 --> 00:25:55,989
software tools some of them built on C++

00:25:52,479 --> 00:25:59,470
a lot of them built on Python and there

00:25:55,989 --> 00:26:01,059
are modules in pycelle for a lot of the

00:25:59,470 --> 00:26:02,409
different spatial analysis techniques

00:26:01,059 --> 00:26:05,289
that I talked about earlier in the

00:26:02,409 --> 00:26:08,619
presentation it can be a little

00:26:05,289 --> 00:26:13,509
complicated to install but it's it's

00:26:08,619 --> 00:26:16,779
very powerful it's great in particular

00:26:13,509 --> 00:26:18,820
for developers who are looking to

00:26:16,779 --> 00:26:22,649
incorporate some of those spatial

00:26:18,820 --> 00:26:22,649
analysis methods in an application

00:26:22,780 --> 00:26:26,680
it's pretty good for GIS analysts who

00:26:25,210 --> 00:26:29,290
want to do some custom stripping

00:26:26,680 --> 00:26:32,160
scripting but for the most part if

00:26:29,290 --> 00:26:35,110
you're looking for a user-friendly GIS

00:26:32,160 --> 00:26:36,820
or a user-friendly GUI for this

00:26:35,110 --> 00:26:39,940
particular set of tools you might try

00:26:36,820 --> 00:26:43,930
one of the wrappers that the Giotto team

00:26:39,940 --> 00:26:45,910
has created for them and on the horizon

00:26:43,930 --> 00:26:47,770
there's going to be quite a bit of high

00:26:45,910 --> 00:26:54,340
style integration in these other GIS

00:26:47,770 --> 00:26:58,120
packages ArcGIS in QGIS in particular so

00:26:54,340 --> 00:26:59,470
this is a schematic of pycelle here with

00:26:58,120 --> 00:27:02,830
all of the different modules and

00:26:59,470 --> 00:27:05,980
components and the exploratory spatial

00:27:02,830 --> 00:27:12,010
data analysis packages is perhaps the

00:27:05,980 --> 00:27:15,060
most well documented and developed so if

00:27:12,010 --> 00:27:19,960
we were to continue using this data set

00:27:15,060 --> 00:27:21,460
to build a possible story next steps

00:27:19,960 --> 00:27:24,460
would be to use some of the pycelle

00:27:21,460 --> 00:27:26,650
tools to quantify some of these clusters

00:27:24,460 --> 00:27:28,960
and the city in the region and the

00:27:26,650 --> 00:27:31,960
nation and in particular to examine

00:27:28,960 --> 00:27:35,350
clustering along networks and those

00:27:31,960 --> 00:27:38,680
business quarters in particular and then

00:27:35,350 --> 00:27:41,410
a step beyond that would be to create

00:27:38,680 --> 00:27:44,110
really beautiful interactive maps and

00:27:41,410 --> 00:27:47,910
charts so that users could go in and

00:27:44,110 --> 00:27:47,910
explore the data on their own

00:27:54,330 --> 00:27:59,790
so I mean part of this is about telling

00:27:58,600 --> 00:28:02,320
stories

00:27:59,790 --> 00:28:05,070
that's something both Dana and I like to

00:28:02,320 --> 00:28:08,410
do my background is actually in

00:28:05,070 --> 00:28:10,950
journalism and I used to work for the

00:28:08,410 --> 00:28:10,950
Washington Post

00:28:11,820 --> 00:28:18,550
so which stories would we go after

00:28:16,260 --> 00:28:20,440
construction contracts funding to

00:28:18,550 --> 00:28:24,730
charter schools healthcare costs and

00:28:20,440 --> 00:28:28,380
prisons was actually one that I believe

00:28:24,730 --> 00:28:32,110
Dana found while looking at the data

00:28:28,380 --> 00:28:36,610
local versus regional versus national

00:28:32,110 --> 00:28:39,000
purchases DC has a very odd layout in

00:28:36,610 --> 00:28:43,030
that originally it was a 10 by 10 square

00:28:39,000 --> 00:28:45,970
and then they hacked off what is now

00:28:43,030 --> 00:28:47,800
basically Arlington because they felt

00:28:45,970 --> 00:28:52,990
like it wasn't worth anything

00:28:47,800 --> 00:28:56,680
and so it's 68 square miles there's not

00:28:52,990 --> 00:28:58,510
a lot of room in DC so local versus

00:28:56,680 --> 00:29:02,200
regional regional being Maryland

00:28:58,510 --> 00:29:04,540
Virginia and then national and then

00:29:02,200 --> 00:29:06,610
Technology Services one of the things

00:29:04,540 --> 00:29:09,190
while looking at this what I noticed was

00:29:06,610 --> 00:29:12,880
that there was um it seemed to me that

00:29:09,190 --> 00:29:16,570
there was a lot of overlap with hiring

00:29:12,880 --> 00:29:23,980
consultants and technology services by

00:29:16,570 --> 00:29:25,300
various agencies in relation to sort of

00:29:23,980 --> 00:29:27,520
different needs that each of them had

00:29:25,300 --> 00:29:31,360
like a couple of them might have hired a

00:29:27,520 --> 00:29:33,790
couple different DBAs or DBA consultants

00:29:31,360 --> 00:29:37,450
so looking for overlaps for that where

00:29:33,790 --> 00:29:42,400
there could be possibly consolidation of

00:29:37,450 --> 00:29:44,650
services and lastly we provided some

00:29:42,400 --> 00:29:48,550
links to learn more the sage handbook

00:29:44,650 --> 00:29:51,130
for spatial analysis interactive spatial

00:29:48,550 --> 00:29:55,570
data analysis geo graphic information

00:29:51,130 --> 00:29:59,970
analysis and pycelle and there's Dana's

00:29:55,570 --> 00:29:59,970
daughter who's a geographer and training

00:30:01,370 --> 00:30:08,060
and more the network next tutorial is

00:30:05,600 --> 00:30:12,970
really great and network X they have a

00:30:08,060 --> 00:30:15,440
pretty responsive mailing list I found

00:30:12,970 --> 00:30:19,160
UCD Dublin summer course that had a lot

00:30:15,440 --> 00:30:21,860
of great materials and then I want to

00:30:19,160 --> 00:30:24,830
recommend a book I want to say I do have

00:30:21,860 --> 00:30:27,620
a personal conflict of interest because

00:30:24,830 --> 00:30:29,240
my professor wrote it social network

00:30:27,620 --> 00:30:34,340
analysis for startups by O'Reilly Media

00:30:29,240 --> 00:30:35,990
and I think if you find him he's sitting

00:30:34,340 --> 00:30:43,960
in the front row he'll give you 40% off

00:30:35,990 --> 00:30:43,960
and questions

00:31:00,930 --> 00:31:07,720
hi sue talked about how the DC data set

00:31:05,710 --> 00:31:09,730
got shut down I actually lived in DC

00:31:07,720 --> 00:31:11,620
when that political change happened and

00:31:09,730 --> 00:31:13,060
it was a big deal because people kind of

00:31:11,620 --> 00:31:18,220
suspected that kind of thing would

00:31:13,060 --> 00:31:20,830
exactly happen so what this is you know

00:31:18,220 --> 00:31:23,350
I didn't you know outside mapping as

00:31:20,830 --> 00:31:29,260
sort of digital citizens what can we do

00:31:23,350 --> 00:31:36,910
to help keep open data open or or who's

00:31:29,260 --> 00:31:39,160
working on that so the open knowledge

00:31:36,910 --> 00:31:43,180
foundation is really a fantastic

00:31:39,160 --> 00:31:45,850
resource here they're having a huge

00:31:43,180 --> 00:31:48,430
summit somewhere in Europe early this

00:31:45,850 --> 00:31:49,930
summer where a lot of different open

00:31:48,430 --> 00:31:51,910
data advocates from all over the world

00:31:49,930 --> 00:31:57,600
are meeting to discuss this just this

00:31:51,910 --> 00:32:02,200
sort of thing you know I think that the

00:31:57,600 --> 00:32:05,710
the sites are you know they're they're

00:32:02,200 --> 00:32:07,030
there for people to use and I think most

00:32:05,710 --> 00:32:10,270
cities that go to the trouble to create

00:32:07,030 --> 00:32:12,520
those sites want people to make things

00:32:10,270 --> 00:32:14,350
with the data and so I guess what I

00:32:12,520 --> 00:32:17,050
could really just encourage you to do is

00:32:14,350 --> 00:32:20,500
to make stuff with the data and keep

00:32:17,050 --> 00:32:26,770
using it keep asking for more data to be

00:32:20,500 --> 00:32:28,860
opened up you know then create a need

00:32:26,770 --> 00:32:32,590
for that an initiative in your own City

00:32:28,860 --> 00:32:35,170
around that thank you I also want to add

00:32:32,590 --> 00:32:38,350
to what Dana said and say that just

00:32:35,170 --> 00:32:40,870
because a data set is not available

00:32:38,350 --> 00:32:43,300
online does not mean you can't gain

00:32:40,870 --> 00:32:46,320
access to it sometimes if you're nice

00:32:43,300 --> 00:32:48,940
and you ask they'll give it to you and

00:32:46,320 --> 00:32:51,790
sometimes they'll give it to you in the

00:32:48,940 --> 00:32:54,340
format you want and lots of times they

00:32:51,790 --> 00:32:56,330
won't and lots of times they'll say

00:32:54,340 --> 00:32:59,480
that'll be $1,000

00:32:56,330 --> 00:33:05,500
or PDF yeah PDF yeah they'll give you a

00:32:59,480 --> 00:33:08,870
PDF so you know it doesn't hurt to ask

00:33:05,500 --> 00:33:11,210
lots of times too I've heard of people

00:33:08,870 --> 00:33:15,950
getting data from the government via web

00:33:11,210 --> 00:33:19,790
scraping be polite be nice identify

00:33:15,950 --> 00:33:23,900
yourself and it's nice to just ask first

00:33:19,790 --> 00:33:27,470
and if that doesn't work then you can

00:33:23,900 --> 00:33:28,850
FOIA and then they will give you Word

00:33:27,470 --> 00:33:31,760
documents which they've done to me in

00:33:28,850 --> 00:33:34,130
the past so sort of following up on

00:33:31,760 --> 00:33:35,720
getting in touch with people one thing

00:33:34,130 --> 00:33:40,010
that I used to run into when I did this

00:33:35,720 --> 00:33:41,030
a lot was resistance not to like it

00:33:40,010 --> 00:33:42,620
would be sets of data that were

00:33:41,030 --> 00:33:44,180
available but we would still want to

00:33:42,620 --> 00:33:46,790
talk to the people who were providing

00:33:44,180 --> 00:33:48,440
that data and knew at best and they

00:33:46,790 --> 00:33:50,270
would be really hesitant to talk to us

00:33:48,440 --> 00:33:52,070
like we did one a couple of years ago

00:33:50,270 --> 00:33:54,290
when everybody was worried about swine

00:33:52,070 --> 00:33:56,240
flu we talked to our state health

00:33:54,290 --> 00:33:59,240
department we got their weekly reports

00:33:56,240 --> 00:34:01,400
we got the CDC's weekly reports and they

00:33:59,240 --> 00:34:03,500
came in sort of with this dismissive

00:34:01,400 --> 00:34:05,300
attitude toward us of well you're not

00:34:03,500 --> 00:34:07,670
epidemiologist what would you know about

00:34:05,300 --> 00:34:09,169
presenting this kind of data you know

00:34:07,670 --> 00:34:11,510
what would you know about analyzing this

00:34:09,169 --> 00:34:13,340
have you ever run into that and and had

00:34:11,510 --> 00:34:15,770
any sort of breakthroughs or ways to

00:34:13,340 --> 00:34:17,660
convince people yes we really do know

00:34:15,770 --> 00:34:19,880
what we're doing and we can you know

00:34:17,660 --> 00:34:28,280
tell compelling stories that won't make

00:34:19,880 --> 00:34:34,120
you want to tear your hair out so I mean

00:34:28,280 --> 00:34:37,910
I've had experiences with a city-related

00:34:34,120 --> 00:34:42,470
agency being really reluctant to release

00:34:37,910 --> 00:34:44,240
data on green space and the turnaround

00:34:42,470 --> 00:34:45,560
of vacant land because they knew that

00:34:44,240 --> 00:34:47,150
particular researchers were going to

00:34:45,560 --> 00:34:52,040
look at the relationship between that

00:34:47,150 --> 00:34:53,570
green space and crime you know and they

00:34:52,040 --> 00:34:55,960
were a little nervous about how that

00:34:53,570 --> 00:35:01,490
study might turn out

00:34:55,960 --> 00:35:04,670
so that's been my experience with it it

00:35:01,490 --> 00:35:06,530
wasn't a concern over the group's

00:35:04,670 --> 00:35:08,900
ability to analyze the data it was

00:35:06,530 --> 00:35:16,000
concerned over what the study might

00:35:08,900 --> 00:35:20,780
reveal so I I haven't had any

00:35:16,000 --> 00:35:24,080
experiences where somebody outright sort

00:35:20,780 --> 00:35:25,940
of denied me data for the sake that they

00:35:24,080 --> 00:35:27,710
didn't believe I could analyze it or

00:35:25,940 --> 00:35:29,240
look at it correctly well they didn't

00:35:27,710 --> 00:35:30,020
deny us the data the data was published

00:35:29,240 --> 00:35:32,120
free to the public

00:35:30,020 --> 00:35:33,590
they just didn't really discuss it with

00:35:32,120 --> 00:35:35,180
you yeah they didn't want to talk to us

00:35:33,590 --> 00:35:41,060
they were afraid that we would screw it

00:35:35,180 --> 00:35:41,630
up okay sure sure I mean hello okay

00:35:41,060 --> 00:35:44,120
sorry

00:35:41,630 --> 00:35:48,170
you're gonna you're you're gonna run

00:35:44,120 --> 00:35:51,800
into it I ran into it I think we ran

00:35:48,170 --> 00:35:53,630
into it when when when I was at the

00:35:51,800 --> 00:35:57,020
Washington Post and we worked on top

00:35:53,630 --> 00:35:59,030
secret America no one's gonna talk to

00:35:57,020 --> 00:36:02,120
you or very little people are gonna talk

00:35:59,030 --> 00:36:05,960
to you and sometimes you know you're

00:36:02,120 --> 00:36:10,940
trying to add context to sort of the

00:36:05,960 --> 00:36:14,050
what you've found it's hard another way

00:36:10,940 --> 00:36:17,300
to get around it too is to try to find

00:36:14,050 --> 00:36:18,470
in professionals or academics in the

00:36:17,300 --> 00:36:20,120
field that are familiar with the data

00:36:18,470 --> 00:36:22,160
maybe not necessarily the authority

00:36:20,120 --> 00:36:24,320
where the data came from but someone who

00:36:22,160 --> 00:36:28,340
could speak to the data and offer some

00:36:24,320 --> 00:36:31,550
insight so my my experience trying to

00:36:28,340 --> 00:36:33,320
track down political data is that most

00:36:31,550 --> 00:36:36,380
data people like to talk to other data

00:36:33,320 --> 00:36:38,980
people and so I would find myself on the

00:36:36,380 --> 00:36:41,630
phone with these other joyous analysts

00:36:38,980 --> 00:36:43,820
often trapped in little offices and city

00:36:41,630 --> 00:36:47,330
agencies who are dying to talk to

00:36:43,820 --> 00:36:50,030
another GIS person it was like 45

00:36:47,330 --> 00:36:52,400
minutes of therapy on the phone no no

00:36:50,030 --> 00:36:55,340
one understood how hard it was for them

00:36:52,400 --> 00:36:56,420
deal with spatial data but so maybe it's

00:36:55,340 --> 00:36:59,930
just a matter of finding the right

00:36:56,420 --> 00:37:02,950
person to talk to you you guys called

00:36:59,930 --> 00:37:05,290
out ArcGIS as one of the tools to

00:37:02,950 --> 00:37:09,030
with have you done anything with tile

00:37:05,290 --> 00:37:13,329
mill I have any experience with that uh

00:37:09,030 --> 00:37:14,980
so I I'm fairly new to it I played with

00:37:13,329 --> 00:37:16,720
it some and preparation for this

00:37:14,980 --> 00:37:19,150
presentation it didn't make it past the

00:37:16,720 --> 00:37:22,089
presentation because I'm still new with

00:37:19,150 --> 00:37:25,450
it but but yeah I was really amazed with

00:37:22,089 --> 00:37:27,579
how quickly I could just start getting

00:37:25,450 --> 00:37:30,880
Maps up there and doing more complicated

00:37:27,579 --> 00:37:32,849
things with them so I think that that

00:37:30,880 --> 00:37:36,160
that if we were to take this to the next

00:37:32,849 --> 00:37:38,440
step and turn the analysis into a more

00:37:36,160 --> 00:37:41,349
developed story that we might share that

00:37:38,440 --> 00:37:48,160
would be an amazing tool to use to have

00:37:41,349 --> 00:37:51,070
that interactivity and tile mill and I'm

00:37:48,160 --> 00:37:53,500
serving as a Don's voice he wanted me to

00:37:51,070 --> 00:37:56,770
ask if you have any experience with d3

00:37:53,500 --> 00:38:02,760
and visualizing geo data and anything

00:37:56,770 --> 00:38:05,589
that you can share on that actually

00:38:02,760 --> 00:38:13,569
actually yes I have some recent

00:38:05,589 --> 00:38:16,690
experience using d3 there's not d3 yeah

00:38:13,569 --> 00:38:19,750
d3 is pretty awesome

00:38:16,690 --> 00:38:24,520
New York Times started using it there's

00:38:19,750 --> 00:38:27,160
an there's nothing oh I'll say that you

00:38:24,520 --> 00:38:30,250
can kick out Maps fast and you can

00:38:27,160 --> 00:38:31,990
present stuff fast but if you're

00:38:30,250 --> 00:38:33,369
presenting if you're putting stuff on

00:38:31,990 --> 00:38:35,380
the web

00:38:33,369 --> 00:38:38,470
you're gonna lose some of the

00:38:35,380 --> 00:38:40,780
capabilities that you get with like post

00:38:38,470 --> 00:38:44,589
GIS and Geoje Ango you know being able

00:38:40,780 --> 00:38:47,710
to do spatial analysis because you're

00:38:44,589 --> 00:38:49,990
gonna write a lot more code and it's

00:38:47,710 --> 00:38:51,910
already you're you're basically going to

00:38:49,990 --> 00:38:54,970
rewrite what another service already

00:38:51,910 --> 00:38:57,970
offers so I mean I would say with d3 in

00:38:54,970 --> 00:38:59,710
Maps it's fast I actually I'm in the

00:38:57,970 --> 00:39:05,190
middle of a project right now using d3

00:38:59,710 --> 00:39:05,190
maps called election gauge which is a

00:39:05,390 --> 00:39:12,590
comparison of candidates corpus and

00:39:08,900 --> 00:39:16,470
tweets and looking how different regions

00:39:12,590 --> 00:39:17,870
the language people use compares to the

00:39:16,470 --> 00:39:21,240
language of a Canon in different regions

00:39:17,870 --> 00:39:30,330
it's a development new features every

00:39:21,240 --> 00:39:31,920
week or every other week thanks one of

00:39:30,330 --> 00:39:37,110
the really amazing things to do with

00:39:31,920 --> 00:39:38,580
your social mapping data for the DC

00:39:37,110 --> 00:39:40,680
contractors would be to combine it with

00:39:38,580 --> 00:39:42,870
other publicly available social data

00:39:40,680 --> 00:39:44,640
such as who's on whose board of

00:39:42,870 --> 00:39:47,910
directors and who's married to whom and

00:39:44,640 --> 00:39:51,270
who used to work for whom have you done

00:39:47,910 --> 00:39:54,810
any of that um no no but that would

00:39:51,270 --> 00:39:56,910
definitely could be a definite future

00:39:54,810 --> 00:40:00,510
story I mean also looking at job

00:39:56,910 --> 00:40:03,630
movements and of individuals who work at

00:40:00,510 --> 00:40:07,710
agencies LinkedIn there's a lot of great

00:40:03,630 --> 00:40:10,220
information yeah that you can gather so

00:40:07,710 --> 00:40:16,350
yeah that's that's definitely a future

00:40:10,220 --> 00:40:18,000
future possibility thank you so you had

00:40:16,350 --> 00:40:20,220
one data set that seemed to have two

00:40:18,000 --> 00:40:23,510
different sets of networks you had both

00:40:20,220 --> 00:40:26,400
spatial relationships as well as the

00:40:23,510 --> 00:40:27,960
network relationships I was wondering if

00:40:26,400 --> 00:40:33,200
there is any way that you knew of to

00:40:27,960 --> 00:40:39,330
integrate a topology or a network to

00:40:33,200 --> 00:40:42,990
physical spatial map yeah so that geo

00:40:39,330 --> 00:40:45,840
des Center is a great resource for

00:40:42,990 --> 00:40:48,060
learning more about how that works and

00:40:45,840 --> 00:40:51,720
they build tools that would allow you to

00:40:48,060 --> 00:40:53,490
do that kind of spatial analysis along

00:40:51,720 --> 00:40:56,940
networks so I would encourage you to

00:40:53,490 --> 00:40:58,560
check out their their website and the

00:40:56,940 --> 00:41:00,090
documentation of their various tools

00:40:58,560 --> 00:41:02,310
that are out there now and that are

00:41:00,090 --> 00:41:06,500
coming out soon so there are people

00:41:02,310 --> 00:41:06,500
doing that kind of work Thanks

00:41:12,950 --> 00:41:15,790

YouTube URL: https://www.youtube.com/watch?v=GxyfYEe8MiQ


