Title: Advanced Celery
Publication date: 2012-04-29
Playlist: PyCon 2012
Description: 
	Ask Solem Hoel
This talk will delve deep into advanced aspects of the Celery task queue and
ecosystem.  Previous experience with task queues and message oriented
middleware is beneficial.
Captions: 
	00:00:00,060 --> 00:00:05,040
thank you everybody for your patience

00:00:02,070 --> 00:00:18,980
and without further ado I have a solemn

00:00:05,040 --> 00:00:23,730
but a bansuri so Suren late yeah

00:00:18,980 --> 00:00:26,939
my name is asked okay that always

00:00:23,730 --> 00:00:29,099
happens when I say that and I know

00:00:26,939 --> 00:00:31,050
that's a very rare name and in even in

00:00:29,099 --> 00:00:34,020
Norway is the very rare actually I'm

00:00:31,050 --> 00:00:37,800
currently employed by VMware we're

00:00:34,020 --> 00:00:40,110
working the rabbitmq team and we do lots

00:00:37,800 --> 00:00:42,120
of exciting things like helping the

00:00:40,110 --> 00:00:45,719
Cloud Foundry team define what the next

00:00:42,120 --> 00:00:47,879
web stack will be I'm also father we're

00:00:45,719 --> 00:00:50,730
the first son two years ago I'm

00:00:47,879 --> 00:00:52,969
Norwegian born and raised in Trondheim

00:00:50,730 --> 00:00:57,930
while living in Oslo later years and

00:00:52,969 --> 00:01:01,530
just recently we moved to London and I

00:00:57,930 --> 00:01:04,170
started writing salary while working at

00:01:01,530 --> 00:01:07,200
opera software the first commit was an

00:01:04,170 --> 00:01:11,729
April 24th almost three years ago so we

00:01:07,200 --> 00:01:13,290
actually have a birthday coming up as

00:01:11,729 --> 00:01:15,900
I'm sure most of you already know what

00:01:13,290 --> 00:01:19,979
salary is but in any case I'll read our

00:01:15,900 --> 00:01:21,930
mission statement salary aims to be a

00:01:19,979 --> 00:01:24,509
flexible and reliable best-of-breed

00:01:21,930 --> 00:01:27,060
solution to process vast amount of

00:01:24,509 --> 00:01:29,130
messages in a distributed fashion of

00:01:27,060 --> 00:01:33,450
providing operations with the tools to

00:01:29,130 --> 00:01:35,130
maintain such a system flexible that

00:01:33,450 --> 00:01:36,600
means it's very extensible almost every

00:01:35,130 --> 00:01:38,729
part and corner of salaries either

00:01:36,600 --> 00:01:41,390
configurable or really easy to subclass

00:01:38,729 --> 00:01:44,100
or can be instantiated separately and

00:01:41,390 --> 00:01:46,799
reliability this is a very hard thing to

00:01:44,100 --> 00:01:48,390
do in Python actually and we spent

00:01:46,799 --> 00:01:50,520
countless iterations getting the current

00:01:48,390 --> 00:01:53,869
cases right though you may still find

00:01:50,520 --> 00:01:55,140
ways to break it but as getting rarer

00:01:53,869 --> 00:01:57,780
sorry

00:01:55,140 --> 00:02:00,840
communicating communicates exclusively

00:01:57,780 --> 00:02:04,680
by message passing everything from

00:02:00,840 --> 00:02:07,590
sending the tasks to controlling the

00:02:04,680 --> 00:02:09,840
work process remotely to monitoring and

00:02:07,590 --> 00:02:12,020
sometimes publishing and receiving

00:02:09,840 --> 00:02:12,020
results

00:02:13,880 --> 00:02:20,400
you may notice how it doesn't say

00:02:16,050 --> 00:02:23,580
processing tasks explicitly but rather

00:02:20,400 --> 00:02:25,770
processing messages this is because

00:02:23,580 --> 00:02:26,730
service isn't necessarily a job given in

00:02:25,770 --> 00:02:28,920
traditional sense

00:02:26,730 --> 00:02:32,040
rather it's about reacting to messages

00:02:28,920 --> 00:02:33,570
as they arrive and alternate messaging

00:02:32,040 --> 00:02:38,400
scenarios are also supported like

00:02:33,570 --> 00:02:39,750
publish to subscribe and broadcast and

00:02:38,400 --> 00:02:41,370
last but not least there's operations

00:02:39,750 --> 00:02:44,070
the system needs to be maintained and

00:02:41,370 --> 00:02:49,190
salary provides the tools and framework

00:02:44,070 --> 00:02:51,720
necessary for them to do their job okay

00:02:49,190 --> 00:02:54,090
actually where justices held me myself a

00:02:51,720 --> 00:02:57,870
long long time ago but angry calls left

00:02:54,090 --> 00:02:59,820
me terribly depressed writing your own

00:02:57,870 --> 00:03:01,680
in-house tasks you may not be so much

00:02:59,820 --> 00:03:05,400
work initially it's likely that we'll be

00:03:01,680 --> 00:03:07,320
bumping into things down the road that's

00:03:05,400 --> 00:03:09,180
already been sold by salary there's also

00:03:07,320 --> 00:03:11,630
value in having a generic system but the

00:03:09,180 --> 00:03:14,130
community big community base around it

00:03:11,630 --> 00:03:16,470
as he may be four to five new job

00:03:14,130 --> 00:03:18,180
postings mentioning solving the service

00:03:16,470 --> 00:03:20,910
description every week and sometimes

00:03:18,180 --> 00:03:23,100
even that many a day and having a

00:03:20,910 --> 00:03:26,900
framework uses terminology the basis of

00:03:23,100 --> 00:03:28,950
acceptations around these concepts and

00:03:26,900 --> 00:03:34,680
contributing something back to salary

00:03:28,950 --> 00:03:37,710
means that everyone benefits to it so I

00:03:34,680 --> 00:03:40,530
realized that the proposal for his talk

00:03:37,710 --> 00:03:42,720
was a little enthusiastic so for such a

00:03:40,530 --> 00:03:46,350
short time so the contents will be

00:03:42,720 --> 00:03:49,770
different phenomena can be different

00:03:46,350 --> 00:03:51,120
from what's in the program I'm kind of

00:03:49,770 --> 00:03:53,520
hoping that you'll have some questions

00:03:51,120 --> 00:03:59,970
later so you can answer I'm guessing

00:03:53,520 --> 00:04:03,600
that is the case so onto the tasks the

00:03:59,970 --> 00:04:05,580
task granularity the amount of

00:04:03,600 --> 00:04:07,800
computation and task is called the

00:04:05,580 --> 00:04:10,230
granularity and we said that the task is

00:04:07,800 --> 00:04:13,320
coarsely grained it has lots of

00:04:10,230 --> 00:04:16,290
computation and finely grained if does

00:04:13,320 --> 00:04:18,270
less every single message must be

00:04:16,290 --> 00:04:21,049
transferred or network and may be

00:04:18,270 --> 00:04:23,400
written to disk so the task granularity

00:04:21,049 --> 00:04:25,970
must be large enough to offset all read

00:04:23,400 --> 00:04:25,970
the messaging

00:04:26,720 --> 00:04:32,400
chunking there's a simple technique to

00:04:29,520 --> 00:04:34,290
make finely grained tasks more coarse

00:04:32,400 --> 00:04:38,040
and it does impact the concurrency of

00:04:34,290 --> 00:04:40,110
the task if your task is to send a

00:04:38,040 --> 00:04:42,240
hundred thousand emails for example you

00:04:40,110 --> 00:04:45,510
can slice them up say into chunks of

00:04:42,240 --> 00:04:48,500
hundred emails per piece so the task is

00:04:45,510 --> 00:04:51,750
composed of a thousand tasks instead

00:04:48,500 --> 00:04:54,780
this reduces the overhead and it can

00:04:51,750 --> 00:04:56,850
also yield other benefits like having

00:04:54,780 --> 00:05:02,310
access to warm CPU caches and reusing

00:04:56,850 --> 00:05:05,280
connections at opera we have this feed

00:05:02,310 --> 00:05:07,650
processing system processing lots of

00:05:05,280 --> 00:05:09,660
feeds every hour in the database well

00:05:07,650 --> 00:05:11,280
one entry for every feed and to start

00:05:09,660 --> 00:05:14,300
refreshing those feeds where our tossed

00:05:11,280 --> 00:05:18,140
salad all of them from the database and

00:05:14,300 --> 00:05:20,790
send a message for them one by one and

00:05:18,140 --> 00:05:23,000
this performs very badly of course and

00:05:20,790 --> 00:05:26,010
eventually not all not at all

00:05:23,000 --> 00:05:27,960
so history so he's chunking to divide

00:05:26,010 --> 00:05:29,640
the messages in the parts of thousands

00:05:27,960 --> 00:05:31,500
and then account down to each of the

00:05:29,640 --> 00:05:33,470
parts so that the law was spread

00:05:31,500 --> 00:05:38,760
throughout the available time we know

00:05:33,470 --> 00:05:42,150
and this was a first take without

00:05:38,760 --> 00:05:44,340
chunking and if you have something like

00:05:42,150 --> 00:05:50,210
this operating with lots of data that

00:05:44,340 --> 00:05:52,800
should have alarm bells ringing simply

00:05:50,210 --> 00:05:55,430
for every object and database do

00:05:52,800 --> 00:05:55,430
something with it

00:05:55,790 --> 00:06:03,270
and you probably can't see this first

00:06:01,140 --> 00:06:05,910
case just download the slides later but

00:06:03,270 --> 00:06:09,330
this basically this is similar to what

00:06:05,910 --> 00:06:12,030
we ended up with we divide each each

00:06:09,330 --> 00:06:14,550
payload in 2000 chonks and separated

00:06:12,030 --> 00:06:21,210
chunks in time using the count on our

00:06:14,550 --> 00:06:24,930
count an argument to apply a sync an

00:06:21,210 --> 00:06:26,850
addition we try to limit the time to 80%

00:06:24,930 --> 00:06:28,230
of the time window well available so it

00:06:26,850 --> 00:06:31,440
doesn't overlap with the next refresh

00:06:28,230 --> 00:06:33,000
cycle but this is only theoretical limit

00:06:31,440 --> 00:06:35,160
as it depends on the time the test takes

00:06:33,000 --> 00:06:37,380
around so may not work in practice

00:06:35,160 --> 00:06:38,639
actually but with good monitoring

00:06:37,380 --> 00:06:41,699
Nagus meaning

00:06:38,639 --> 00:06:44,099
whatever your favorite is and you should

00:06:41,699 --> 00:06:47,900
have one we were able to tweak the

00:06:44,099 --> 00:06:47,900
values as soon as it poses a problem

00:06:52,640 --> 00:07:00,000
so this is the task that process East

00:06:55,290 --> 00:07:02,370
chunk of feeds instead of passing the

00:07:00,000 --> 00:07:04,770
credit set to the task we send start and

00:07:02,370 --> 00:07:07,470
stop parameters so that the task can

00:07:04,770 --> 00:07:08,730
fish it data from the database itself if

00:07:07,470 --> 00:07:11,070
didn't the parent else could have

00:07:08,730 --> 00:07:14,100
fetched all of the data and away the

00:07:11,070 --> 00:07:17,880
messages will be large containing lots

00:07:14,100 --> 00:07:20,370
and lots of data it may be

00:07:17,880 --> 00:07:22,950
counterintuitive but this is a much

00:07:20,370 --> 00:07:24,930
better solution this ensures that any

00:07:22,950 --> 00:07:27,060
dad added since the parent task is

00:07:24,930 --> 00:07:30,450
included though it that is not so

00:07:27,060 --> 00:07:32,070
relevant for this application and later

00:07:30,450 --> 00:07:34,710
we actually refer him to optimizations

00:07:32,070 --> 00:07:36,330
that was even more effective like

00:07:34,710 --> 00:07:38,550
detecting how often the feeds were

00:07:36,330 --> 00:07:40,620
updated and refreshing frequently

00:07:38,550 --> 00:07:43,110
updated feeds more often and rarely

00:07:40,620 --> 00:07:49,320
updated fees very efficiently on user

00:07:43,110 --> 00:07:51,450
demand then quartz quartz is a

00:07:49,320 --> 00:07:53,820
synchronization primitive it's also

00:07:51,450 --> 00:07:59,760
known as a barrier and it consists of a

00:07:53,820 --> 00:08:02,310
header and a body it enables you to play

00:07:59,760 --> 00:08:04,740
a callback of the tasks it completes and

00:08:02,310 --> 00:08:07,520
in this context the task set is called

00:08:04,740 --> 00:08:10,260
the header and the callback is the body

00:08:07,520 --> 00:08:12,300
you may notice in examples there is

00:08:10,260 --> 00:08:14,430
group instead of task set this because

00:08:12,300 --> 00:08:16,440
when the process of renaming task sets

00:08:14,430 --> 00:08:18,690
to groups because an insight that's a

00:08:16,440 --> 00:08:22,130
much better name has used also a

00:08:18,690 --> 00:08:22,130
computer to break

00:08:26,230 --> 00:08:31,460
and of course the support in the table

00:08:29,780 --> 00:08:33,860
in the red is a memcache trestle packets

00:08:31,460 --> 00:08:36,740
where I use atomic counters to

00:08:33,860 --> 00:08:39,710
immediately apply the callback after the

00:08:36,740 --> 00:08:41,720
last task completes there's also

00:08:39,710 --> 00:08:43,880
fallback implementation available for

00:08:41,720 --> 00:08:46,280
other result backends but it launches

00:08:43,880 --> 00:08:49,730
the task every second polling the result

00:08:46,280 --> 00:08:53,120
until completion and this is not all in

00:08:49,730 --> 00:08:59,990
deal but it works good in practice most

00:08:53,120 --> 00:09:02,120
of the time you see this no that's sad

00:08:59,990 --> 00:09:04,640
and this is the native radius

00:09:02,120 --> 00:09:09,020
implementation every task part of the

00:09:04,640 --> 00:09:11,780
header is provided with the body it's

00:09:09,020 --> 00:09:15,550
provided with the callback so that any

00:09:11,780 --> 00:09:18,080
of the tasks in header can apply it and

00:09:15,550 --> 00:09:20,180
this is available in the tasks request

00:09:18,080 --> 00:09:22,100
along with the toasted South said ID

00:09:20,180 --> 00:09:24,800
which is also used at the ID of the

00:09:22,100 --> 00:09:26,720
Court and the river

00:09:24,800 --> 00:09:28,730
I had whenever I had the task completes

00:09:26,720 --> 00:09:30,260
he simply increments the counter and if

00:09:28,730 --> 00:09:32,390
the counter exceeds the total number of

00:09:30,260 --> 00:09:34,580
the tasks in the header it applies to

00:09:32,390 --> 00:09:37,430
call back and this is the stuff where

00:09:34,580 --> 00:09:38,690
this is greater and it enables you to

00:09:37,430 --> 00:09:40,880
easily implement the payment is

00:09:38,690 --> 00:09:48,980
necessary for solving tasks that are not

00:09:40,880 --> 00:09:51,670
embarrassing parallel and the fallback

00:09:48,980 --> 00:09:54,800
implementation which is charmingly naive

00:09:51,670 --> 00:09:56,660
by simply launching a retread charge via

00:09:54,800 --> 00:09:59,000
a chain of tasks that applies the

00:09:56,660 --> 00:10:01,820
callback and terminates when the header

00:09:59,000 --> 00:10:03,370
is complete this is also useful pattern

00:10:01,820 --> 00:10:05,900
that you can use for other things like

00:10:03,370 --> 00:10:13,700
adding retry and timeouts Devon black

00:10:05,900 --> 00:10:16,250
workers using a textbook example here to

00:10:13,700 --> 00:10:21,410
demonstrate quartz which is the parallel

00:10:16,250 --> 00:10:23,240
Samar algorithm of course that example

00:10:21,410 --> 00:10:27,040
is rather silly because the overhead

00:10:23,240 --> 00:10:27,040
makes it terribly inefficient

00:10:29,950 --> 00:10:36,140
as in the feeds we divide the work into

00:10:33,110 --> 00:10:37,970
chunks then we sum each chunk and by

00:10:36,140 --> 00:10:40,430
using the core div a callback that adds

00:10:37,970 --> 00:10:44,870
up all the seams together giving the

00:10:40,430 --> 00:10:46,820
total so that's what caught that that's

00:10:44,870 --> 00:10:49,160
sort of what course can be used for but

00:10:46,820 --> 00:10:53,630
this is just a theoretical example that

00:10:49,160 --> 00:10:57,200
isn't really based in reality also

00:10:53,630 --> 00:10:59,450
MapReduce as you may know MapReduce is a

00:10:57,200 --> 00:11:02,660
technique where the work is divided into

00:10:59,450 --> 00:11:06,080
two stages a mapping stage and a

00:11:02,660 --> 00:11:08,750
reducing stage the mapping stage gathers

00:11:06,080 --> 00:11:12,920
data into key value pairs which is then

00:11:08,750 --> 00:11:15,620
sent to the reducers and this slide

00:11:12,920 --> 00:11:18,710
shows to reduce implementation a reduce

00:11:15,620 --> 00:11:22,250
implementation in soldering and it takes

00:11:18,710 --> 00:11:26,390
two parameters a list of results and

00:11:22,250 --> 00:11:33,410
reducer subtask the result is then

00:11:26,390 --> 00:11:35,390
converted to a dictionary and the

00:11:33,410 --> 00:11:39,890
reducer is supplied for every key and

00:11:35,390 --> 00:11:42,560
value this is a group or a tossed set if

00:11:39,890 --> 00:11:46,820
you may so that the task and resolve can

00:11:42,560 --> 00:11:50,780
be tracked as a single entity and we

00:11:46,820 --> 00:11:55,700
also use sub tasks clone which is a new

00:11:50,780 --> 00:11:58,250
thing with that you can clone the sub

00:11:55,700 --> 00:12:00,200
task instance and augment the arguments

00:11:58,250 --> 00:12:04,160
so that any new push positional

00:12:00,200 --> 00:12:06,830
arguments added to the clone call will

00:12:04,160 --> 00:12:09,380
be appended to the original arguments

00:12:06,830 --> 00:12:13,810
with the sub tasks so it's like it's

00:12:09,380 --> 00:12:13,810
kind of like a reverse partial I guess

00:12:15,100 --> 00:12:20,180
and also keyword of our arguments and

00:12:18,290 --> 00:12:22,600
task options are also merged with

00:12:20,180 --> 00:12:22,600
original

00:12:24,430 --> 00:12:34,570
and yeah some talks top subtasks are

00:12:29,110 --> 00:12:37,840
actually just dictionaries so and they

00:12:34,570 --> 00:12:40,570
can be pickled but the best practice is

00:12:37,840 --> 00:12:42,640
the cold soup task on it again once you

00:12:40,570 --> 00:12:48,270
receive it because then you can also

00:12:42,640 --> 00:12:48,270
support Jason and yeah whatever you want

00:12:54,680 --> 00:13:01,250
and this is the main task that with a

00:12:57,440 --> 00:13:03,970
mapper and reducer and some data you can

00:13:01,250 --> 00:13:06,470
perform a MapReduce operation with and

00:13:03,970 --> 00:13:08,960
it creates a court where the header

00:13:06,470 --> 00:13:12,529
calls the mapper every part of the data

00:13:08,960 --> 00:13:19,940
I mean all the mappers have finished the

00:13:12,529 --> 00:13:22,160
reduced task is called the reduced

00:13:19,940 --> 00:13:24,800
implementation is actually kind of lame

00:13:22,160 --> 00:13:28,940
because it doesn't support intermediate

00:13:24,800 --> 00:13:31,580
data which really send it for every

00:13:28,940 --> 00:13:33,500
value you should start to reduce

00:13:31,580 --> 00:13:39,560
operation at once instead of merging

00:13:33,500 --> 00:13:44,120
them together does use of both and

00:13:39,560 --> 00:13:48,459
that's an example using an action this

00:13:44,120 --> 00:13:50,839
is the canonical example used when

00:13:48,459 --> 00:13:56,410
presenting MapReduce and it's counting

00:13:50,839 --> 00:13:56,410
words in the data but this does it by

00:13:56,890 --> 00:14:05,209
making a year old so the mapper takes

00:14:03,200 --> 00:14:07,520
the document URL and returns list words

00:14:05,209 --> 00:14:10,300
and initial counts for each word found

00:14:07,520 --> 00:14:10,300
in the documents

00:14:25,890 --> 00:14:28,519
right

00:14:28,680 --> 00:14:34,619
the reduce phase then collects all of

00:14:31,259 --> 00:14:36,269
the keys the values and reducer and a

00:14:34,619 --> 00:14:38,490
reducer is called with a word as the

00:14:36,269 --> 00:14:43,649
first argument and a list of counts in a

00:14:38,490 --> 00:14:45,600
second and it's simply returning the

00:14:43,649 --> 00:14:49,069
word and assume of the counts which then

00:14:45,600 --> 00:14:52,199
becomes the follow result for Edward and

00:14:49,069 --> 00:14:53,639
we have a convenience method count words

00:14:52,199 --> 00:14:57,319
that is used instead of calling

00:14:53,639 --> 00:14:57,319
MapReduce directly

00:15:10,570 --> 00:15:18,280
so yeah blocking is bad it's like

00:15:14,950 --> 00:15:19,960
kryptonite to a synchronous i/o but it's

00:15:18,280 --> 00:15:23,650
also bad when using the multi processing

00:15:19,960 --> 00:15:25,510
pool since every worker process has a

00:15:23,650 --> 00:15:28,150
finite amount of prospects every worker

00:15:25,510 --> 00:15:31,540
instance Amin has a finite amount of

00:15:28,150 --> 00:15:33,400
processes having a task blocking also

00:15:31,540 --> 00:15:35,350
means blocking networking from being

00:15:33,400 --> 00:15:38,380
able being able to have a lot of tasks

00:15:35,350 --> 00:15:39,580
and even worse is a cluster of workers

00:15:38,380 --> 00:15:43,750
that is all blocked

00:15:39,580 --> 00:15:47,560
of course because then no waiting tasks

00:15:43,750 --> 00:15:49,420
will be processed anymore so this is

00:15:47,560 --> 00:15:56,620
something you need to avoid as much as

00:15:49,420 --> 00:16:03,520
possible and one such trick is to set a

00:15:56,620 --> 00:16:05,680
timeout and retry but if this task CPU

00:16:03,520 --> 00:16:09,190
bomb you could try to decompose the task

00:16:05,680 --> 00:16:10,750
into smaller parts or but a most

00:16:09,190 --> 00:16:15,190
effective solution is to be smart about

00:16:10,750 --> 00:16:16,630
how your Hound routing and with routing

00:16:15,190 --> 00:16:18,880
you can direct us to different workers

00:16:16,630 --> 00:16:20,590
and you should route and you should

00:16:18,880 --> 00:16:23,560
resume running thoughts to dedicated

00:16:20,590 --> 00:16:25,270
workers as this makes way for short

00:16:23,560 --> 00:16:29,950
tasks that could be or could be of

00:16:25,270 --> 00:16:31,390
higher importance and one idea and one

00:16:29,950 --> 00:16:34,090
that I also would like to have built

00:16:31,390 --> 00:16:36,640
into salary it's the ability to reroute

00:16:34,090 --> 00:16:42,190
tasks to workers that are able to handle

00:16:36,640 --> 00:16:44,950
them by using metrics such as available

00:16:42,190 --> 00:16:47,080
CPU or memory and you can definitely do

00:16:44,950 --> 00:16:49,360
this yourself right now but it's going

00:16:47,080 --> 00:16:51,250
to be some work and if anyone wants to

00:16:49,360 --> 00:16:54,970
do work on this you should come up to me

00:16:51,250 --> 00:17:01,210
and say hi that would be great I work in

00:16:54,970 --> 00:17:07,180
anything really I just say hi then it's

00:17:01,210 --> 00:17:09,690
time Sam is new so took the name from a

00:17:07,180 --> 00:17:15,150
particular flower pattern in nature

00:17:09,690 --> 00:17:18,430
which fits well into our botanical team

00:17:15,150 --> 00:17:21,040
but Simon is a disability service where

00:17:18,430 --> 00:17:23,820
each node manages the seller instances

00:17:21,040 --> 00:17:23,820
of that machine

00:17:23,930 --> 00:17:32,820
so and it's distributed and a sign node

00:17:29,670 --> 00:17:36,210
is called the branch there's no master

00:17:32,820 --> 00:17:37,800
so it's decentralized and every branch

00:17:36,210 --> 00:17:42,240
knows about every other branch in the

00:17:37,800 --> 00:17:45,150
same neighborhood and each commit on

00:17:42,240 --> 00:17:48,150
embedded embedded web server exposes an

00:17:45,150 --> 00:17:50,400
API and internally every API operation

00:17:48,150 --> 00:17:52,950
may or may not reroute to requests to

00:17:50,400 --> 00:17:55,410
another branch for example adding a new

00:17:52,950 --> 00:17:57,300
instance will send that request will

00:17:55,410 --> 00:18:01,170
forward that request to random branch

00:17:57,300 --> 00:18:04,620
and very soon it will be sent to the

00:18:01,170 --> 00:18:08,310
branch with available capacity so this

00:18:04,620 --> 00:18:13,080
is perfect for what platform as a

00:18:08,310 --> 00:18:17,220
service operators I guess and really for

00:18:13,080 --> 00:18:20,730
for everyone even as the local mode that

00:18:17,220 --> 00:18:23,010
can replace supervisor that doesn't

00:18:20,730 --> 00:18:28,620
actually run honey run the distributed

00:18:23,010 --> 00:18:32,280
parts and it's got an AGP up API where

00:18:28,620 --> 00:18:37,280
you can create a managed you can create

00:18:32,280 --> 00:18:37,280
worker instances and queues and

00:18:39,530 --> 00:18:46,520
everything belongs to an application

00:18:41,490 --> 00:18:46,520
which provides defaults for all of the

00:18:47,000 --> 00:18:54,210
revolved instances and you can configure

00:18:51,090 --> 00:18:57,390
workers at runtime configure to scale

00:18:54,210 --> 00:19:00,720
that a concurrency our consumed what

00:18:57,390 --> 00:19:05,490
cues they consume from and much and much

00:19:00,720 --> 00:19:09,660
more I'm sure of soon and just have to

00:19:05,490 --> 00:19:13,470
install it by pip obviously start a

00:19:09,660 --> 00:19:16,350
branch you can provide the directory

00:19:13,470 --> 00:19:19,700
where all of the log files are stored

00:19:16,350 --> 00:19:19,700
and the pitfall is stored

00:19:21,810 --> 00:19:28,720
and yeah this looks much better here my

00:19:26,950 --> 00:19:33,970
fellow honest better it's much better

00:19:28,720 --> 00:19:38,590
resolution and every branch has an

00:19:33,970 --> 00:19:41,380
idealized automatically generated of

00:19:38,590 --> 00:19:44,290
course there is a common line clients

00:19:41,380 --> 00:19:50,140
thoughts I'm fighting client in aruba

00:19:44,290 --> 00:19:53,220
client and it's just it's just an h h hv

00:19:50,140 --> 00:19:57,820
api so it should be easy turn neurons

00:19:53,220 --> 00:19:59,920
and i was an admin UI i recall if you

00:19:57,820 --> 00:20:08,230
could out managing advertising the

00:19:59,920 --> 00:20:17,200
Django admin as well that I like so this

00:20:08,230 --> 00:20:19,380
is the client so I'm branches lists the

00:20:17,200 --> 00:20:23,490
branches in that neighborhood or

00:20:19,380 --> 00:20:23,490
actually the AMQP be hosts

00:20:29,380 --> 00:20:34,810
them can create an app which is provides

00:20:32,860 --> 00:20:38,230
defaults for every instance created

00:20:34,810 --> 00:20:43,740
within that app and I'm sure can also

00:20:38,230 --> 00:20:50,380
provide some sort of scaling or sharding

00:20:43,740 --> 00:20:53,260
eventually and it supports arbitrary

00:20:50,380 --> 00:20:58,660
arguments to salary D and everything

00:20:53,260 --> 00:21:03,930
like that so it works with Django today

00:20:58,660 --> 00:21:07,840
as well creating instance is simple just

00:21:03,930 --> 00:21:10,210
know there's a there's an URL at the

00:21:07,840 --> 00:21:13,270
bottom that does the same and that

00:21:10,210 --> 00:21:16,360
returns the instance name of the new

00:21:13,270 --> 00:21:22,530
instance or you can provided your own

00:21:16,360 --> 00:21:25,300
name if you want the and as I said that

00:21:22,530 --> 00:21:29,380
that request is actually routed to a

00:21:25,300 --> 00:21:31,510
random branch so if you have a cluster

00:21:29,380 --> 00:21:33,250
of four hundred machines then that

00:21:31,510 --> 00:21:36,610
request will be routed to one of them

00:21:33,250 --> 00:21:44,020
and in the future the machine with the

00:21:36,610 --> 00:21:48,000
most capacity and the branch log

00:21:44,020 --> 00:21:48,000
variable in the instance directory

00:21:48,690 --> 00:21:58,540
there's log files big files and you can

00:21:55,980 --> 00:22:03,010
get information like what instance there

00:21:58,540 --> 00:22:06,150
are and get statistics from all of by

00:22:03,010 --> 00:22:06,150
the HTTP API

00:22:18,330 --> 00:22:26,480
and you can configure currency when

00:22:23,550 --> 00:22:26,480
instance is running

00:22:36,830 --> 00:22:45,710
and that's that's using all to scale by

00:22:39,860 --> 00:22:48,110
default actually so it the minimum the

00:22:45,710 --> 00:22:51,590
minimum is the amount of processes it

00:22:48,110 --> 00:22:54,470
should have all the time and but when

00:22:51,590 --> 00:22:58,460
there's a lot less load it can grow to

00:22:54,470 --> 00:23:01,960
maximum number of processes and it will

00:22:58,460 --> 00:23:05,240
decrease when the load decreases as well

00:23:01,960 --> 00:23:08,600
which I don't think it's that useful

00:23:05,240 --> 00:23:13,180
actually unless you have memory limits

00:23:08,600 --> 00:23:13,180
or you're on the shared hosting provider

00:23:14,830 --> 00:23:23,510
and the queue is referenced by name but

00:23:20,030 --> 00:23:26,960
you can also use all of the mqp features

00:23:23,510 --> 00:23:31,120
there so you can specify exchange the

00:23:26,960 --> 00:23:34,120
routing keys and excellent types and

00:23:31,120 --> 00:23:35,300
this just creates the queue definition

00:23:34,120 --> 00:23:38,090
business

00:23:35,300 --> 00:23:42,520
if this isn't actual queue but this is

00:23:38,090 --> 00:23:48,260
what what work is used to get the

00:23:42,520 --> 00:23:49,940
congregation of the queue and then we

00:23:48,260 --> 00:23:56,780
can have our instance instance consume

00:23:49,940 --> 00:23:57,820
from it and the logs show it to

00:23:56,780 --> 00:24:02,110
consuming from it

00:23:57,820 --> 00:24:02,110
and this is all at runtime

00:24:11,610 --> 00:24:30,990
that's what I have a really or any

00:24:13,650 --> 00:24:32,910
questions so one thing I didn't

00:24:30,990 --> 00:24:35,220
understand about the above the same

00:24:32,910 --> 00:24:38,850
server I guess is what the exact

00:24:35,220 --> 00:24:41,030
semantics are of a sine branch so how

00:24:38,850 --> 00:24:46,980
would they behave I'm sitting there

00:24:41,030 --> 00:24:48,600
hello again about the sine branches what

00:24:46,980 --> 00:24:50,190
exactly there are these semantics of the

00:24:48,600 --> 00:24:57,240
branches and how do they behave oh yeah

00:24:50,190 --> 00:25:01,140
okay so the bass simply every machine in

00:24:57,240 --> 00:25:03,920
a cluster runs a sine branch which is

00:25:01,140 --> 00:25:06,330
then responsible for watching and

00:25:03,920 --> 00:25:08,640
supervising the work processes that

00:25:06,330 --> 00:25:12,240
would work instances then that it's

00:25:08,640 --> 00:25:13,890
responsible for okay so a branch is

00:25:12,240 --> 00:25:16,950
basically a physical a machine basically

00:25:13,890 --> 00:25:18,330
yeah well I don't think there's any

00:25:16,950 --> 00:25:20,820
point to run more of them on one machine

00:25:18,330 --> 00:25:23,550
but it manages the instances or look

00:25:20,820 --> 00:25:27,000
every machine on that machine so if your

00:25:23,550 --> 00:25:30,800
cluster of machines that should be able

00:25:27,000 --> 00:25:33,330
to provision new celerity instances then

00:25:30,800 --> 00:25:37,140
the Bryant the same branch is

00:25:33,330 --> 00:25:39,690
responsible for creating and and if it

00:25:37,140 --> 00:25:42,780
stops working or whatever it will

00:25:39,690 --> 00:25:44,970
restart it and manage the decelerate

00:25:42,780 --> 00:25:46,740
instances on that particular machine so

00:25:44,970 --> 00:25:51,660
it's like a supervisor to supervise

00:25:46,740 --> 00:25:54,210
Addie but wait but this is the one where

00:25:51,660 --> 00:25:57,590
you can create new ones and it's like

00:25:54,210 --> 00:26:02,850
burgers does a service second I guess

00:25:57,590 --> 00:26:04,440
okay thanks so maybe I should have just

00:26:02,850 --> 00:26:09,240
put this in a feature request but I

00:26:04,440 --> 00:26:10,950
figured I'd ask well here the so for

00:26:09,240 --> 00:26:15,600
project that I work on called media

00:26:10,950 --> 00:26:17,910
gobble and we do image and video hosting

00:26:15,600 --> 00:26:19,770
and one of the issues we have is once

00:26:17,910 --> 00:26:22,200
you move from video to video hosting you

00:26:19,770 --> 00:26:24,870
can't the always eager option obviously

00:26:22,200 --> 00:26:28,500
fails because you want to move to trail

00:26:24,870 --> 00:26:30,480
coding so I was wondering how open do

00:26:28,500 --> 00:26:36,330
you guys think you'd be to a combo

00:26:30,480 --> 00:26:38,460
option for like for like Nate UNIX named

00:26:36,330 --> 00:26:40,800
pipes or do you think that's a terrible

00:26:38,460 --> 00:26:41,160
idea I don't think that's a terrible

00:26:40,800 --> 00:26:45,630
idea

00:26:41,160 --> 00:26:47,550
we also want to see rmq and that's great

00:26:45,630 --> 00:26:49,500
for things like notification as well if

00:26:47,550 --> 00:26:51,720
you want to avoid polling you can

00:26:49,500 --> 00:26:54,030
instead send a notification to say that

00:26:51,720 --> 00:26:56,940
there's a message here and now you can

00:26:54,030 --> 00:26:59,040
come get it oh I don't think that's a

00:26:56,940 --> 00:27:03,960
bad idea at all okay cool

00:26:59,040 --> 00:27:07,679
I first just want to say thanks for

00:27:03,960 --> 00:27:10,340
celery we we use it a lot and its work

00:27:07,679 --> 00:27:13,170
really well for what we have to do a

00:27:10,340 --> 00:27:17,100
couple of questions when you're going

00:27:13,170 --> 00:27:18,840
through the sign slides it looks like

00:27:17,100 --> 00:27:24,120
that's true is that replacing like

00:27:18,840 --> 00:27:27,179
celery config routing stuff in every

00:27:24,120 --> 00:27:29,790
maybe eventually okay which actually

00:27:27,179 --> 00:27:32,400
brings me my next thing so we we

00:27:29,790 --> 00:27:35,670
developed on 2.2 and then we actually

00:27:32,400 --> 00:27:38,010
upgraded 2.4 just when we rolled out

00:27:35,670 --> 00:27:40,440
some new servers and we get all kinds of

00:27:38,010 --> 00:27:43,010
deprecation warnings because of future

00:27:40,440 --> 00:27:46,920
you're removing a bunch of stuff from

00:27:43,010 --> 00:27:49,100
celery config yeah is is there a reason

00:27:46,920 --> 00:27:52,080
why those are all moving to command-line

00:27:49,100 --> 00:27:55,410
artists keeping like command-line are

00:27:52,080 --> 00:28:00,030
just to like override the celery config

00:27:55,410 --> 00:28:01,800
well they write this little config well

00:28:00,030 --> 00:28:04,679
yeah so like in celery config like we

00:28:01,800 --> 00:28:06,780
have log level set and we have the you

00:28:04,679 --> 00:28:09,330
know the log dirt or you know the log

00:28:06,780 --> 00:28:12,120
file oh yeah all set and you know so we

00:28:09,330 --> 00:28:13,590
get all kinds of like just warning

00:28:12,120 --> 00:28:17,160
messages now and I mean we're gonna

00:28:13,590 --> 00:28:19,350
change all that but yeah well that's

00:28:17,160 --> 00:28:22,050
usually just due to some stupid

00:28:19,350 --> 00:28:24,900
decisions I made in the beginning which

00:28:22,050 --> 00:28:26,940
I don't like now which may could be just

00:28:24,900 --> 00:28:29,040
in my head so I'm removing log files

00:28:26,940 --> 00:28:32,490
even the configuration gotcha

00:28:29,040 --> 00:28:35,490
and because all is not a flexible anyway

00:28:32,490 --> 00:28:37,950
after all and it just work it's

00:28:35,490 --> 00:28:38,400
inconsistent with how everything else

00:28:37,950 --> 00:28:39,840
works

00:28:38,400 --> 00:28:41,940
just trying to make things more

00:28:39,840 --> 00:28:45,600
consistent okay but I guess it shouldn't

00:28:41,940 --> 00:28:47,850
really have to raise the deprecation

00:28:45,600 --> 00:28:49,980
morning yeah I just didn't realize why

00:28:47,850 --> 00:28:51,840
you wouldn't keep you know let the

00:28:49,980 --> 00:28:53,670
command line argh override something

00:28:51,840 --> 00:28:55,170
that's said in the config yeah but if

00:28:53,670 --> 00:28:56,250
the configs are still there I mean we

00:28:55,170 --> 00:28:57,750
only found out about it because we

00:28:56,250 --> 00:29:00,900
upgraded and without it I wouldn't

00:28:57,750 --> 00:29:02,550
realize if it didn't I made deprecations

00:29:00,900 --> 00:29:11,400
warning then then how would you find out

00:29:02,550 --> 00:29:14,910
about it yeah okay all right thanks hey

00:29:11,400 --> 00:29:16,110
thanks for celery as well I was

00:29:14,910 --> 00:29:19,440
wondering if you talked a little bit

00:29:16,110 --> 00:29:24,720
about high availability RabbitMQ oh yeah

00:29:19,440 --> 00:29:26,640
I was I wanted to do that yeah yeah sure

00:29:24,720 --> 00:29:33,390
I'm kind of researching that myself

00:29:26,640 --> 00:29:36,210
right now but so we just recently got

00:29:33,390 --> 00:29:38,430
support for high availability most in

00:29:36,210 --> 00:29:44,790
the monster master replication setting

00:29:38,430 --> 00:29:46,530
and it works really all you have to do

00:29:44,790 --> 00:29:49,700
is set up something like keep a live D

00:29:46,530 --> 00:29:52,260
or chorus saying key and pacemaker

00:29:49,700 --> 00:29:56,010
pacemaker is really hard to set up but

00:29:52,260 --> 00:29:58,140
keep alive it is very simple I'm

00:29:56,010 --> 00:30:04,070
planning on writing a tutorial on that

00:29:58,140 --> 00:30:07,860
soon actually for the OpenStack guys so

00:30:04,070 --> 00:30:12,510
but it's already working it's just a

00:30:07,860 --> 00:30:15,930
matter of installing the D proxy which

00:30:12,510 --> 00:30:18,270
is keep alive the and maybe we'll also

00:30:15,930 --> 00:30:20,130
have support for that in the clients but

00:30:18,270 --> 00:30:24,390
then you need to maintain a static list

00:30:20,130 --> 00:30:25,830
list of brokers in each client and if

00:30:24,390 --> 00:30:27,450
you want to add a new broker then you

00:30:25,830 --> 00:30:30,120
have to restart all the clients and all

00:30:27,450 --> 00:30:33,410
the consumers just to add a new broker

00:30:30,120 --> 00:30:33,410
so that's kind of

00:30:38,760 --> 00:30:42,540
did that answer your question

00:30:45,420 --> 00:30:49,309
yeah well it is yeah

00:30:53,360 --> 00:30:59,050
so if we don't have any more questions I

00:30:56,060 --> 00:30:59,050

YouTube URL: https://www.youtube.com/watch?v=gpKMwPoldak


