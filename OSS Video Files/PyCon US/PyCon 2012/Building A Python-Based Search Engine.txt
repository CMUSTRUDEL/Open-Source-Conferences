Title: Building A Python-Based Search Engine
Publication date: 2012-04-29
Playlist: PyCon 2012
Description: 
	Daniel Lindsley
Search is an increasingly common request in all types of applications as the amount of data all of us deal with continues to grow. The technology/architecture behind search engines is wildly different from what
many developers expect
Captions: 
	00:00:02,689 --> 00:00:09,480
good afternoon everyone our speaker this

00:00:06,569 --> 00:00:11,160
afternoon for the first half of this

00:00:09,480 --> 00:00:12,599
track is going to be Daniel Inslee he's

00:00:11,160 --> 00:00:22,859
going to be speaking on building a

00:00:12,599 --> 00:00:26,189
Python based search engine all right

00:00:22,859 --> 00:00:29,880
y'all Jeff good lunch all right good

00:00:26,189 --> 00:00:33,329
so my name is Daniel Inslee I am from

00:00:29,880 --> 00:00:39,120
Lawrence Kansas not this Kansas but this

00:00:33,329 --> 00:00:39,629
Kansas but really I'm from the

00:00:39,120 --> 00:00:42,239
Internet's

00:00:39,629 --> 00:00:46,260
so I wrote a little web shop called

00:00:42,239 --> 00:00:50,070
toast driven might you see this and I do

00:00:46,260 --> 00:00:51,539
consulting an open source and I'm the

00:00:50,070 --> 00:00:53,670
primary author of a package called

00:00:51,539 --> 00:00:54,930
haystack which is a django app for

00:00:53,670 --> 00:00:57,719
pluggable search but we're not here to

00:00:54,930 --> 00:00:59,039
talk about haystack today so what I'd

00:00:57,719 --> 00:01:01,050
like the goal of what we're going to

00:00:59,039 --> 00:01:03,539
talk about today is to give you a broad

00:01:01,050 --> 00:01:04,680
overview of how search technologies work

00:01:03,539 --> 00:01:07,140
because if you're familiar with other

00:01:04,680 --> 00:01:09,119
technologies like Artemyev s's they're

00:01:07,140 --> 00:01:10,799
not the same beast they work very

00:01:09,119 --> 00:01:12,270
differently and understanding how a

00:01:10,799 --> 00:01:14,939
search engine works can give you

00:01:12,270 --> 00:01:17,280
insights as to how to work with other

00:01:14,939 --> 00:01:19,680
engines what we're going to try not to

00:01:17,280 --> 00:01:21,450
do is have all of you right yet another

00:01:19,680 --> 00:01:25,979
search library because we really

00:01:21,450 --> 00:01:28,350
probably don't need that so there'll be

00:01:25,979 --> 00:01:30,720
code and a link to the slides at the end

00:01:28,350 --> 00:01:33,720
of the talk so bear with me so why

00:01:30,720 --> 00:01:36,810
should you care about search doesn't the

00:01:33,720 --> 00:01:37,770
googles handle that well turns out

00:01:36,810 --> 00:01:39,840
there's a lot of good reasons to have

00:01:37,770 --> 00:01:43,470
in-house search one of those is that

00:01:39,840 --> 00:01:45,030
things like Google or ask or Microsoft's

00:01:43,470 --> 00:01:47,130
and Yahoo's technologies they have to

00:01:45,030 --> 00:01:50,189
scrape web content which means they get

00:01:47,130 --> 00:01:51,869
HTML soup and they have to parse through

00:01:50,189 --> 00:01:54,840
it and try and pull out any kind of

00:01:51,869 --> 00:01:56,909
meaning getting reasonable proper

00:01:54,840 --> 00:02:01,530
content out of that can be a big

00:01:56,909 --> 00:02:03,360
nightmare another good reason is you

00:02:01,530 --> 00:02:06,750
know your data model better than any

00:02:03,360 --> 00:02:08,580
scraper does they just have HTML they

00:02:06,750 --> 00:02:10,770
might have add content in there they

00:02:08,580 --> 00:02:12,420
might have our SS feeds all kinds of

00:02:10,770 --> 00:02:13,790
things that aren't pertinent to the

00:02:12,420 --> 00:02:16,969
content that's really being too

00:02:13,790 --> 00:02:21,200
lady or maybe you're weird and you don't

00:02:16,969 --> 00:02:22,579
do web apps like I do mmm so good

00:02:21,200 --> 00:02:25,069
reasons to have let's talk about some

00:02:22,579 --> 00:02:27,680
core concepts search engines are all

00:02:25,069 --> 00:02:30,560
document based these are not Road V's

00:02:27,680 --> 00:02:33,709
and you know your average are DBMS

00:02:30,560 --> 00:02:35,840
you're never ever just grepping through

00:02:33,709 --> 00:02:38,209
a string that is like the worst thing

00:02:35,840 --> 00:02:39,980
you can possibly be doing we don't want

00:02:38,209 --> 00:02:43,219
to be looking at text big text blobs

00:02:39,980 --> 00:02:44,569
every time we go to run a query you'll

00:02:43,219 --> 00:02:46,010
hear me talk a lot about inverted

00:02:44,569 --> 00:02:47,569
indexes we're going to get into this in

00:02:46,010 --> 00:02:51,950
a few minutes as well as stemming

00:02:47,569 --> 00:02:53,510
engrams and relevance so as with all

00:02:51,950 --> 00:02:54,319
fields there's a ton of terminology so

00:02:53,510 --> 00:02:55,939
we're going to go through some stuff

00:02:54,319 --> 00:02:57,109
really quick so that what I say

00:02:55,939 --> 00:02:58,459
throughout the rest of the talk has some

00:02:57,109 --> 00:03:01,269
meaning so we're going to talk about

00:02:58,459 --> 00:03:04,939
these terms and we'll start with engine

00:03:01,269 --> 00:03:06,709
the engine is the black box you hand a

00:03:04,939 --> 00:03:09,260
query to and you get the results from

00:03:06,709 --> 00:03:11,090
the good examples of open source engines

00:03:09,260 --> 00:03:14,810
are things like solar elasticsearch

00:03:11,090 --> 00:03:20,180
Sapien whoosh and many others sphinx as

00:03:14,810 --> 00:03:22,519
an example documents documents are the

00:03:20,180 --> 00:03:23,959
text blob the text that you sent to a

00:03:22,519 --> 00:03:25,430
search engine is one of the most

00:03:23,959 --> 00:03:27,319
important things that you can pay

00:03:25,430 --> 00:03:28,519
attention to and additionally you

00:03:27,319 --> 00:03:31,909
probably want some additional metadata

00:03:28,519 --> 00:03:33,229
to describe what that text blob consists

00:03:31,909 --> 00:03:36,409
of where it came from

00:03:33,229 --> 00:03:38,840
titles other attributes and stuff corpus

00:03:36,409 --> 00:03:43,159
corpus is just the collection of all the

00:03:38,840 --> 00:03:45,319
documents in the index stop words stop

00:03:43,159 --> 00:03:48,620
words are words that are basically

00:03:45,319 --> 00:03:51,260
filler these are things like and uh the

00:03:48,620 --> 00:03:52,970
but and whatnot words that don't

00:03:51,260 --> 00:03:57,729
contribute a lot of meaning but appear

00:03:52,970 --> 00:04:00,590
very frequently in documents stemming

00:03:57,729 --> 00:04:02,090
stemming is finding root words and we're

00:04:00,590 --> 00:04:06,829
going to looking at this a little bit

00:04:02,090 --> 00:04:09,650
later segments segments are the data

00:04:06,829 --> 00:04:11,180
that make up the overall index because

00:04:09,650 --> 00:04:13,790
you may be dealing with gigabytes or

00:04:11,180 --> 00:04:15,739
terabytes of data shoving everything in

00:04:13,790 --> 00:04:18,380
one big file really doesn't scale well

00:04:15,739 --> 00:04:20,030
so a common technique is to shard things

00:04:18,380 --> 00:04:23,050
down into what are called segment files

00:04:20,030 --> 00:04:25,150
which as a whole make up the index

00:04:23,050 --> 00:04:28,189
relevance

00:04:25,150 --> 00:04:31,400
relevance are the algorithm or

00:04:28,189 --> 00:04:32,780
algorithms you may apply to the results

00:04:31,400 --> 00:04:35,419
you get back out of the search engine to

00:04:32,780 --> 00:04:38,500
determine how well a certain document or

00:04:35,419 --> 00:04:42,229
result may fit a individual query

00:04:38,500 --> 00:04:44,000
faceting faceting is drill down if

00:04:42,229 --> 00:04:45,409
you've ever been on amazon.com that

00:04:44,000 --> 00:04:47,509
left-hand bar that lets you say hey I

00:04:45,409 --> 00:04:48,860
want a camera that's a Canon in two

00:04:47,509 --> 00:04:51,199
hundred to four hundred dollar range

00:04:48,860 --> 00:04:56,050
that's and lets you drill down

00:04:51,199 --> 00:04:59,000
that's faceting boost boost is a way to

00:04:56,050 --> 00:05:00,979
push up certain kinds of results in the

00:04:59,000 --> 00:05:07,190
result set so that they bubble up to the

00:05:00,979 --> 00:05:10,490
top so let's start at the beginning

00:05:07,190 --> 00:05:13,759
let's talk about indexing indexing is

00:05:10,490 --> 00:05:15,050
the is taking a document that the blob

00:05:13,759 --> 00:05:16,879
that we were talking about with metadata

00:05:15,050 --> 00:05:18,949
and pulling it into the engine and

00:05:16,879 --> 00:05:20,509
performing transformations and storage

00:05:18,949 --> 00:05:23,029
so that we can query on it later on

00:05:20,509 --> 00:05:25,029
there's four major breakdowns receiving

00:05:23,029 --> 00:05:27,710
in storing documents tokenization

00:05:25,029 --> 00:05:29,889
generating terms and indexing those

00:05:27,710 --> 00:05:32,330
terms so let's start with documents

00:05:29,889 --> 00:05:35,210
documents are the simplest thing we will

00:05:32,330 --> 00:05:39,379
talk about all day they are not ever

00:05:35,210 --> 00:05:43,750
ever ever not not not not they're not a

00:05:39,379 --> 00:05:46,129
row in the DB really no really really

00:05:43,750 --> 00:05:47,990
you really want to be thinking like I've

00:05:46,129 --> 00:05:49,940
said a blob of text and some metadata

00:05:47,990 --> 00:05:52,219
that goes with it

00:05:49,940 --> 00:05:54,080
like I've said quality is the most

00:05:52,219 --> 00:05:57,710
important thing what your users search

00:05:54,080 --> 00:05:58,940
on is what I'm sorry what you index is

00:05:57,710 --> 00:06:01,069
what your users will be able to search

00:05:58,940 --> 00:06:05,690
on if you put a bunch of junk in they're

00:06:01,069 --> 00:06:09,289
going to get garbage results out search

00:06:05,690 --> 00:06:11,180
our documents are a very flat concept a

00:06:09,289 --> 00:06:13,490
lot of people try to work relations into

00:06:11,180 --> 00:06:15,639
search engines and it never ends well

00:06:13,490 --> 00:06:17,719
you want to be thinking things like I

00:06:15,639 --> 00:06:19,580
like to think of search engines as kind

00:06:17,719 --> 00:06:22,849
of the original no sequels after

00:06:19,580 --> 00:06:24,469
Berkeley DB so don't try and put

00:06:22,849 --> 00:06:28,310
relations in it doesn't really work well

00:06:24,469 --> 00:06:29,659
and the the the one thing I really want

00:06:28,310 --> 00:06:31,370
you to take away from any of this

00:06:29,659 --> 00:06:34,039
because it applies everywhere is you

00:06:31,370 --> 00:06:37,069
want to denormalize the living crap out

00:06:34,039 --> 00:06:38,569
of anything your indexing because the

00:06:37,069 --> 00:06:41,179
more you can shove on

00:06:38,569 --> 00:06:42,860
that has meaning again text quality is

00:06:41,179 --> 00:06:44,479
important the more you can chef on that

00:06:42,860 --> 00:06:46,520
has meaning on to the individual

00:06:44,479 --> 00:06:48,439
documents the better they will bubble up

00:06:46,520 --> 00:06:51,080
in results and the better results your

00:06:48,439 --> 00:06:52,669
users will get when they search so a

00:06:51,080 --> 00:06:55,490
document looks really familiar if we

00:06:52,669 --> 00:06:58,580
represent it as say JSON or Python it's

00:06:55,490 --> 00:07:02,149
just a dict there's really not a whole

00:06:58,580 --> 00:07:07,159
lot to it it's a text blob with optional

00:07:02,149 --> 00:07:09,339
metadata from there and you can imagine

00:07:07,159 --> 00:07:12,919
all kinds of ways to store that data

00:07:09,339 --> 00:07:14,959
tokenization is the process of taking

00:07:12,919 --> 00:07:18,430
that big text blob and breaking it down

00:07:14,959 --> 00:07:20,509
into little individual word size chunks

00:07:18,430 --> 00:07:21,860
typically what you'll find most engines

00:07:20,509 --> 00:07:24,680
doing is splitting on things like

00:07:21,860 --> 00:07:26,899
whitespace the lowercase every single

00:07:24,680 --> 00:07:28,909
token that they get out of that filter

00:07:26,899 --> 00:07:31,430
out all those meaningless words like and

00:07:28,909 --> 00:07:35,089
and though and but and um which are

00:07:31,430 --> 00:07:37,490
going to hear a lot of um and stripping

00:07:35,089 --> 00:07:39,830
punctuation etc etc many times is

00:07:37,490 --> 00:07:43,009
configurable so that you can tweak it to

00:07:39,830 --> 00:07:45,680
match your document set the point of

00:07:43,009 --> 00:07:47,779
that is to get some nice neat normalized

00:07:45,680 --> 00:07:52,459
little tokens out that you can work with

00:07:47,779 --> 00:07:54,259
when querying so we mentioned stemming

00:07:52,459 --> 00:07:56,119
and so that's where we'll go to next

00:07:54,259 --> 00:07:57,889
because now we've got documents we've

00:07:56,119 --> 00:08:00,289
got a list of tokens in those documents

00:07:57,889 --> 00:08:02,990
the next thing we want to do is be able

00:08:00,289 --> 00:08:04,430
to take those mostly normalized tokens

00:08:02,990 --> 00:08:11,449
and get something even better out of it

00:08:04,430 --> 00:08:13,039
so by doing stemming I'm sorry so if you

00:08:11,449 --> 00:08:14,689
do something like grep you're processing

00:08:13,039 --> 00:08:16,369
through a file stream and it's literally

00:08:14,689 --> 00:08:17,419
just checking hey is it here is it here

00:08:16,369 --> 00:08:18,949
is it here is it here

00:08:17,419 --> 00:08:20,749
that's horrible when you've got

00:08:18,949 --> 00:08:23,839
gigabytes and gigabytes and gigabytes of

00:08:20,749 --> 00:08:24,949
stored occupant content so when we do

00:08:23,839 --> 00:08:27,289
this tokenization and the

00:08:24,949 --> 00:08:30,649
post-processing we're really close to

00:08:27,289 --> 00:08:31,969
what we want but not quite so stemming

00:08:30,649 --> 00:08:33,889
is the process of finding the root word

00:08:31,969 --> 00:08:36,289
examples of this would be like test

00:08:33,889 --> 00:08:39,009
becoming testing because the ing ending

00:08:36,289 --> 00:08:41,509
is not particularly meaningful or

00:08:39,009 --> 00:08:43,099
searchers becoming searched or becoming

00:08:41,509 --> 00:08:45,350
searched because we want to find root

00:08:43,099 --> 00:08:48,529
words so that if someone misspelled

00:08:45,350 --> 00:08:50,689
something or if if there's a pluralized

00:08:48,529 --> 00:08:51,440
form we can get back down to that root

00:08:50,689 --> 00:08:54,860
word and

00:08:51,440 --> 00:08:56,600
or one variant rather than twenty these

00:08:54,860 --> 00:08:58,970
then become the terms in the inverted

00:08:56,600 --> 00:09:01,040
index that we'll be querying on and if

00:08:58,970 --> 00:09:03,530
you apply these same the same

00:09:01,040 --> 00:09:05,900
tokenization and stemming to the query

00:09:03,530 --> 00:09:07,310
what you get out of a user's query is

00:09:05,900 --> 00:09:10,400
the exact same thing as the terms you're

00:09:07,310 --> 00:09:13,220
shoving in the index the downsides of

00:09:10,400 --> 00:09:16,610
stemming are that it really only works

00:09:13,220 --> 00:09:18,290
for the language it was built for most

00:09:16,610 --> 00:09:20,390
summers that are out there are pretty

00:09:18,290 --> 00:09:23,540
much English only there are other

00:09:20,390 --> 00:09:25,370
languages but they don't work

00:09:23,540 --> 00:09:27,080
particularly well and they're poorly

00:09:25,370 --> 00:09:29,510
supported it's really really painful

00:09:27,080 --> 00:09:31,910
even in high quality search engines and

00:09:29,510 --> 00:09:33,890
they're really hard to make work cross

00:09:31,910 --> 00:09:37,640
language because the structure of German

00:09:33,890 --> 00:09:40,070
or French is not the same as English so

00:09:37,640 --> 00:09:43,010
this is a really bad shortcoming so

00:09:40,070 --> 00:09:44,830
there are many ways to solve this the

00:09:43,010 --> 00:09:49,310
approach we're going to look at today is

00:09:44,830 --> 00:09:51,230
engrams as I said it takes care of some

00:09:49,310 --> 00:09:52,610
of the shortcomings of stemming we're

00:09:51,230 --> 00:09:55,280
going to introduce some new problems but

00:09:52,610 --> 00:09:58,400
overall it's typically worth it what

00:09:55,280 --> 00:10:00,800
engrams consist of is taking a window

00:09:58,400 --> 00:10:05,510
and passing it over your toe cannot over

00:10:00,800 --> 00:10:07,580
your tokens and that these new little

00:10:05,510 --> 00:10:10,100
terms that come out of this moving

00:10:07,580 --> 00:10:12,140
window are what become your terms in the

00:10:10,100 --> 00:10:13,670
index as opposed to a stemmed version of

00:10:12,140 --> 00:10:17,240
the word and we'll see an example of

00:10:13,670 --> 00:10:20,570
this in a few moments so for instance if

00:10:17,240 --> 00:10:22,040
we do a typical Engram and we assume a

00:10:20,570 --> 00:10:27,050
gram size of three we've got three

00:10:22,040 --> 00:10:28,910
characters a qi l ll e ll ll o and then

00:10:27,050 --> 00:10:33,310
we jump to the next term or the next

00:10:28,910 --> 00:10:35,770
token and generate a complete list of

00:10:33,310 --> 00:10:39,589
terms based on the tokens that we have

00:10:35,770 --> 00:10:41,900
now that's great except that typically

00:10:39,589 --> 00:10:45,920
you're probably not typing in ll o as a

00:10:41,900 --> 00:10:47,930
query so what we want to do is find

00:10:45,920 --> 00:10:50,270
another way to approach this this

00:10:47,930 --> 00:10:52,580
shortcoming ed Jen Graham's typically

00:10:50,270 --> 00:10:54,980
solve this so what an end edge Engram

00:10:52,580 --> 00:10:59,300
will do is it will pin to a side of a

00:10:54,980 --> 00:11:01,580
token in this case if with a gram size

00:10:59,300 --> 00:11:03,620
of three we still have that HDL but

00:11:01,580 --> 00:11:05,070
instead of moving the window along we're

00:11:03,620 --> 00:11:07,020
going to generate different

00:11:05,070 --> 00:11:09,150
Grahame lengths so that we have a

00:11:07,020 --> 00:11:11,850
gradually more and more complete word

00:11:09,150 --> 00:11:14,010
that becomes our terms and then again

00:11:11,850 --> 00:11:15,510
once we switch to the next token we

00:11:14,010 --> 00:11:18,570
start with this gram size of three again

00:11:15,510 --> 00:11:21,570
move to four and five and as you can see

00:11:18,570 --> 00:11:26,340
these terms are much more likely to be

00:11:21,570 --> 00:11:31,320
in a user's query than llo so this is

00:11:26,340 --> 00:11:32,850
great because it gives us a first of all

00:11:31,320 --> 00:11:34,740
it works great for autocomplete because

00:11:32,850 --> 00:11:36,810
user can type start typing something

00:11:34,740 --> 00:11:40,500
very short and you can start immediately

00:11:36,810 --> 00:11:41,880
feeding results back to them it's also

00:11:40,500 --> 00:11:44,040
good because it works across other

00:11:41,880 --> 00:11:45,720
languages because it no longer relies on

00:11:44,040 --> 00:11:47,760
the grammatical structure you're just

00:11:45,720 --> 00:11:50,700
simply looking at tokens and if the word

00:11:47,760 --> 00:11:53,640
starts the same way in it in a different

00:11:50,700 --> 00:11:55,790
language it'll match well too however

00:11:53,640 --> 00:11:58,620
whatever the term is you generate it

00:11:55,790 --> 00:12:00,660
cons are that when you use engrams or

00:11:58,620 --> 00:12:02,400
edge engrams you generate a lot more

00:12:00,660 --> 00:12:05,580
terms

00:12:02,400 --> 00:12:08,910
and as a result storing all those terms

00:12:05,580 --> 00:12:11,880
can be kind of a problem or at least it

00:12:08,910 --> 00:12:13,860
bloats up on space and the initial

00:12:11,880 --> 00:12:15,390
quality of search can suffer a little

00:12:13,860 --> 00:12:17,220
bit because now you're dealing with

00:12:15,390 --> 00:12:21,330
fragments and something that you may not

00:12:17,220 --> 00:12:23,220
have meant to have match now matches so

00:12:21,330 --> 00:12:25,080
an example in Python might look

00:12:23,220 --> 00:12:26,550
something like this we set a minimum and

00:12:25,080 --> 00:12:28,950
Max gram size like we did on the

00:12:26,550 --> 00:12:30,690
previous slide 3 to 6 we have a

00:12:28,950 --> 00:12:33,510
dictionary of terms that we're going to

00:12:30,690 --> 00:12:35,700
be storing out of this we pass in a list

00:12:33,510 --> 00:12:38,160
of tokens run through it make sure we

00:12:35,700 --> 00:12:40,470
grab position information as well as the

00:12:38,160 --> 00:12:42,840
individual token and then we just for

00:12:40,470 --> 00:12:45,360
the range of our graham sized pass a

00:12:42,840 --> 00:12:49,800
window over it and show the newly

00:12:45,360 --> 00:12:52,440
generated term into our terms so now

00:12:49,800 --> 00:12:55,800
we've got documents and we've got Engram

00:12:52,440 --> 00:12:58,410
terms the next step is storing this in a

00:12:55,800 --> 00:13:01,020
way that we can actually query on that

00:12:58,410 --> 00:13:03,690
comes in the inverted index the inverted

00:13:01,020 --> 00:13:06,570
index is the heart and soul of the

00:13:03,690 --> 00:13:08,460
search engine it is how things work you

00:13:06,570 --> 00:13:11,070
can think of it exactly like a Python

00:13:08,460 --> 00:13:14,880
dictionary because it's largely key

00:13:11,070 --> 00:13:16,710
value those keys are the terms that we

00:13:14,880 --> 00:13:19,649
just got finished generating with Engram

00:13:16,710 --> 00:13:22,080
and it's all the terms from all the

00:13:19,649 --> 00:13:24,870
documents so you can imagine taking a

00:13:22,080 --> 00:13:27,240
large tip body of text generating tons

00:13:24,870 --> 00:13:29,460
and tons and tons of edgy engrams each

00:13:27,240 --> 00:13:32,209
one of those that you generated becomes

00:13:29,460 --> 00:13:35,310
a new key in that big inverted index and

00:13:32,209 --> 00:13:37,709
of course this is across all documents

00:13:35,310 --> 00:13:38,850
not just one document at a time the

00:13:37,709 --> 00:13:40,500
position information that we were

00:13:38,850 --> 00:13:44,220
grabbing out of the previous example of

00:13:40,500 --> 00:13:46,740
of edge engrams is important because we

00:13:44,220 --> 00:13:49,200
can say how far into the document did

00:13:46,740 --> 00:13:51,270
this word appear we can also say hey how

00:13:49,200 --> 00:13:53,610
many times in this document did this

00:13:51,270 --> 00:13:56,010
word appear and of course we want to

00:13:53,610 --> 00:13:57,839
sort document IDs so when we search we

00:13:56,010 --> 00:14:01,380
can map things back to the data that

00:13:57,839 --> 00:14:03,060
we've stored so an index as I said might

00:14:01,380 --> 00:14:05,610
just look just like a plain old Python

00:14:03,060 --> 00:14:08,820
dictionary and you will be storing the

00:14:05,610 --> 00:14:10,800
individual terms like blob and text if

00:14:08,820 --> 00:14:14,760
you had a graham size of 4 for instance

00:14:10,800 --> 00:14:17,850
and document IDs such as document 1 5 2

00:14:14,760 --> 00:14:21,230
4 and blob appeared at position 3 of

00:14:17,850 --> 00:14:26,010
that example we saw in a previous slide

00:14:21,230 --> 00:14:27,870
once you have these this index generated

00:14:26,010 --> 00:14:30,390
the problem then becomes querying over

00:14:27,870 --> 00:14:33,120
it efficiently like I said if you have

00:14:30,390 --> 00:14:35,970
this huge massive data structure hanging

00:14:33,120 --> 00:14:38,730
out you can't put it all in one file and

00:14:35,970 --> 00:14:40,470
search over it effectively so there's

00:14:38,730 --> 00:14:42,270
lots and lots of ways to do segments to

00:14:40,470 --> 00:14:46,130
break that index down for efficient

00:14:42,270 --> 00:14:49,020
querying many many many projects follow

00:14:46,130 --> 00:14:50,580
this is this famous Java project called

00:14:49,020 --> 00:14:54,570
loose end which is really really good at

00:14:50,580 --> 00:14:58,380
search but we're going to cheat because

00:14:54,570 --> 00:14:59,610
it's easier so in the example code that

00:14:58,380 --> 00:15:02,220
I've got we're going to just use flat

00:14:59,610 --> 00:15:04,079
files we're going to take those Ngram

00:15:02,220 --> 00:15:06,029
terms that we generated when we rip

00:15:04,079 --> 00:15:07,589
through the big blob of data and we're

00:15:06,029 --> 00:15:09,390
going to hash them and we're going to

00:15:07,589 --> 00:15:12,180
take part of that hash the leading part

00:15:09,390 --> 00:15:12,839
and use it to map it on to a file in the

00:15:12,180 --> 00:15:14,790
file system

00:15:12,839 --> 00:15:16,170
this give this limits the number of

00:15:14,790 --> 00:15:17,850
files that we've got down to something

00:15:16,170 --> 00:15:20,820
that like a unix-like system can handle

00:15:17,850 --> 00:15:23,040
as well as ensures that if you've got to

00:15:20,820 --> 00:15:24,810
give it an Engram you can easily map

00:15:23,040 --> 00:15:27,750
right down into the proper segment file

00:15:24,810 --> 00:15:29,820
that it's associated with we're going to

00:15:27,750 --> 00:15:32,130
keep terms in always sorted order

00:15:29,820 --> 00:15:33,480
and we're going to use JSON to store the

00:15:32,130 --> 00:15:35,280
document and position information not

00:15:33,480 --> 00:15:37,850
because it's particularly performant but

00:15:35,280 --> 00:15:41,730
because it'll work well across other

00:15:37,850 --> 00:15:42,930
other languages or other tools so an

00:15:41,730 --> 00:15:44,460
example implementation might look

00:15:42,930 --> 00:15:46,680
something like this I mentioned we're

00:15:44,460 --> 00:15:48,690
going to hash so we just take the term

00:15:46,680 --> 00:15:50,370
pass it in we're going to take a length

00:15:48,690 --> 00:15:52,740
of six on the hash just so that it's

00:15:50,370 --> 00:15:55,830
short and keeps a reasonable number of

00:15:52,740 --> 00:15:58,110
files in the file system and we just

00:15:55,830 --> 00:16:01,380
simply do a simple md5 on it get a hex

00:15:58,110 --> 00:16:04,650
digest and return the first six as far

00:16:01,380 --> 00:16:05,910
as saving a segment this isn't an always

00:16:04,650 --> 00:16:07,710
sorted order the code I'm going to

00:16:05,910 --> 00:16:09,990
present to you at the end is always in

00:16:07,710 --> 00:16:12,420
sorted order but it's simply just

00:16:09,990 --> 00:16:14,790
appending on to the file the information

00:16:12,420 --> 00:16:15,990
and we're going to tab delimit all of

00:16:14,790 --> 00:16:18,270
our tokens already been whitespace

00:16:15,990 --> 00:16:23,820
separated so there's no chance of tabs

00:16:18,270 --> 00:16:26,570
showing up so by getting to this point

00:16:23,820 --> 00:16:29,100
we've already covered creating the terms

00:16:26,570 --> 00:16:30,360
storing them storing the documents we've

00:16:29,100 --> 00:16:33,420
got enough information that now we can

00:16:30,360 --> 00:16:37,170
start query on querying breaks down into

00:16:33,420 --> 00:16:39,360
a query parser index reader and scoring

00:16:37,170 --> 00:16:41,300
at a high level so let's dive into the

00:16:39,360 --> 00:16:43,680
query parser

00:16:41,300 --> 00:16:47,370
Tikku typically the purpose of a query

00:16:43,680 --> 00:16:48,840
parser is taking a user's hand written

00:16:47,370 --> 00:16:50,280
queried maybe they have advanced

00:16:48,840 --> 00:16:51,660
knowledge of it maybe not you can think

00:16:50,280 --> 00:16:54,000
of this as analogous to sequel and

00:16:51,660 --> 00:16:56,280
parsing it out into something that the

00:16:54,000 --> 00:16:58,770
engine can start tackling you're going

00:16:56,280 --> 00:17:00,930
to process all the elements of that

00:16:58,770 --> 00:17:04,110
users query the same way you did when

00:17:00,930 --> 00:17:06,510
preparing the document for indexing so

00:17:04,110 --> 00:17:07,800
an example trivial Python implementation

00:17:06,510 --> 00:17:09,900
might look like something like this

00:17:07,800 --> 00:17:12,959
we've got a list of common English stop

00:17:09,900 --> 00:17:14,550
words and we're going to we're going to

00:17:12,959 --> 00:17:16,339
really really cheat in this case and

00:17:14,550 --> 00:17:18,660
we're just going to split it up

00:17:16,339 --> 00:17:20,280
check that the tokens not in stop words

00:17:18,660 --> 00:17:21,570
if it's not there we're going to toss in

00:17:20,280 --> 00:17:23,070
our list of tokens and we're going to

00:17:21,570 --> 00:17:24,780
make engrams it's the same kind of stuff

00:17:23,070 --> 00:17:29,970
we just did when we're preparing the

00:17:24,780 --> 00:17:31,710
index index reading becomes easy now

00:17:29,970 --> 00:17:34,610
because we have this list of edged

00:17:31,710 --> 00:17:37,290
Engram terms we can simply go through

00:17:34,610 --> 00:17:39,510
hash them and we can find whatever file

00:17:37,290 --> 00:17:40,850
we just stowed them in in a very quick

00:17:39,510 --> 00:17:42,590
efficient lookup

00:17:40,850 --> 00:17:45,529
and we just ripped through all the

00:17:42,590 --> 00:17:46,759
tokens that we I'm sorry all the terms

00:17:45,529 --> 00:17:49,370
that we generate from a user's query

00:17:46,759 --> 00:17:51,649
grab all the hashes go into the files

00:17:49,370 --> 00:17:54,380
pull out the results and collect the

00:17:51,649 --> 00:17:55,970
position and document information this

00:17:54,380 --> 00:17:59,000
is a little big I apologize if it's

00:17:55,970 --> 00:18:00,740
small but it's basically the same kind

00:17:59,000 --> 00:18:02,419
of stuff we make a segment name so that

00:18:00,740 --> 00:18:04,909
we know what the what the file name of

00:18:02,419 --> 00:18:07,159
the term is check if it's there if it's

00:18:04,909 --> 00:18:08,419
not we just return nothing if it's there

00:18:07,159 --> 00:18:10,519
we go through and we start ripping

00:18:08,419 --> 00:18:11,690
through the file and there are more

00:18:10,519 --> 00:18:13,190
efficient ways to do this but we're

00:18:11,690 --> 00:18:15,019
splitting on the tab for now so we've

00:18:13,190 --> 00:18:17,419
got the term in the info if we found

00:18:15,019 --> 00:18:21,200
that term say we entered hello as our

00:18:17,419 --> 00:18:22,820
user query and we found HCl in the

00:18:21,200 --> 00:18:25,570
segment we're going to pull out those

00:18:22,820 --> 00:18:27,440
results because we we found a match and

00:18:25,570 --> 00:18:29,389
collecting the results is simply a

00:18:27,440 --> 00:18:30,860
matter of just going through all the

00:18:29,389 --> 00:18:33,409
terms that were generated from the user

00:18:30,860 --> 00:18:37,850
query and applying that load segment to

00:18:33,409 --> 00:18:39,740
those terms finally so where we're at

00:18:37,850 --> 00:18:41,899
now is we've got a user's query

00:18:39,740 --> 00:18:44,450
we've tokenized it we've made terms

00:18:41,899 --> 00:18:46,669
we've gotten all the results together

00:18:44,450 --> 00:18:48,019
but they're out of order they have no

00:18:46,669 --> 00:18:49,879
meaningful order for the query that we

00:18:48,019 --> 00:18:52,159
generated so what we're going to do with

00:18:49,879 --> 00:18:54,019
scoring is reorder that collection so

00:18:52,159 --> 00:18:56,779
that it means something based on the

00:18:54,019 --> 00:18:59,029
user's query there are tons and tons of

00:18:56,779 --> 00:19:01,220
choices of scoring algorithms we're not

00:18:59,029 --> 00:19:03,830
going to go through all of them a major

00:19:01,220 --> 00:19:05,419
one is BM 25 it's very famous you can

00:19:03,830 --> 00:19:07,730
look it up on Wikipedia and it's it's

00:19:05,419 --> 00:19:11,019
pretty interesting there's also what's

00:19:07,730 --> 00:19:13,309
called the phased scoring query and

00:19:11,019 --> 00:19:15,710
Google's PageRank is a great example of

00:19:13,309 --> 00:19:18,440
this this can be anything you may decide

00:19:15,710 --> 00:19:20,539
that words that include Bob are really

00:19:18,440 --> 00:19:23,409
really important and you're just like

00:19:20,539 --> 00:19:27,049
Bob you're always up at the top buddy so

00:19:23,409 --> 00:19:32,269
an implementation of BM 25 looks

00:19:27,049 --> 00:19:37,809
something like this I can't explain it

00:19:32,269 --> 00:19:42,649
I'm sorry so as a quick demo of this I

00:19:37,809 --> 00:19:44,029
have indexed the first hundred fifty Erb

00:19:42,649 --> 00:19:49,000
I'm sorry thousand five hundred

00:19:44,029 --> 00:19:49,000
documents of the Enron email data set so

00:19:50,260 --> 00:19:56,900
if anyone has any at correct for polite

00:19:55,010 --> 00:20:06,610
company terms they'd like to search on

00:19:56,900 --> 00:20:06,610
fraud okay let's do fraud Oh demo fail

00:20:09,220 --> 00:20:16,820
I'm mad okay I'm sad because this

00:20:13,790 --> 00:20:18,530
literally just worked before I uh before

00:20:16,820 --> 00:20:22,190
I came down why you don't do demos and

00:20:18,530 --> 00:20:24,400
talks thank you so let's talk about some

00:20:22,190 --> 00:20:26,810
advanced topics shall we

00:20:24,400 --> 00:20:29,450
I'm not going to cover any code on this

00:20:26,810 --> 00:20:33,140
but it's diving into this would be

00:20:29,450 --> 00:20:35,300
probably a great exercise for you god I

00:20:33,140 --> 00:20:37,640
sound like a college professor horrible

00:20:35,300 --> 00:20:39,320
so faceting fascinating is made out to

00:20:37,640 --> 00:20:42,860
be a lot of things but what drill down

00:20:39,320 --> 00:20:45,950
really consists of is for all the terms

00:20:42,860 --> 00:20:47,480
that are provided giving accurate counts

00:20:45,950 --> 00:20:49,040
on the number of documents that contain

00:20:47,480 --> 00:20:50,060
those things so when you're on Amazon

00:20:49,040 --> 00:20:53,150
and you're digging through digital

00:20:50,060 --> 00:20:55,040
cameras and you say Canon what you're

00:20:53,150 --> 00:20:57,110
really and they give you a count

00:20:55,040 --> 00:20:59,360
that's really all faceting is it's just

00:20:57,110 --> 00:21:01,370
that count but that count is important

00:20:59,360 --> 00:21:03,350
because it lets the user know both what

00:21:01,370 --> 00:21:05,630
things are matching within the document

00:21:03,350 --> 00:21:09,530
set as well as how many are there say

00:21:05,630 --> 00:21:11,270
there's only two Canon cameras on Amazon

00:21:09,530 --> 00:21:12,740
which will never ever happen you might

00:21:11,270 --> 00:21:14,120
be like well wow they don't really have

00:21:12,740 --> 00:21:16,880
a really great selection maybe I need to

00:21:14,120 --> 00:21:18,470
go elsewhere so an implementation would

00:21:16,880 --> 00:21:21,200
probably look like collecting all the

00:21:18,470 --> 00:21:23,480
terms counting the length of the unique

00:21:21,200 --> 00:21:25,820
document IDs for each of them and then

00:21:23,480 --> 00:21:27,620
just order by descending because once

00:21:25,820 --> 00:21:29,480
you have those terms and you know you

00:21:27,620 --> 00:21:31,910
know how many things matched and how

00:21:29,480 --> 00:21:33,530
many documents were there you just order

00:21:31,910 --> 00:21:35,390
by the count and you can provide back

00:21:33,530 --> 00:21:38,710
something that's useful to the people

00:21:35,390 --> 00:21:41,020
who are using your software boost

00:21:38,710 --> 00:21:43,910
happens during the scoring process

00:21:41,020 --> 00:21:46,250
literally it's just saying hey we've got

00:21:43,910 --> 00:21:48,350
a score that we generated but we want to

00:21:46,250 --> 00:21:51,380
artificially bump things up so Bob gets

00:21:48,350 --> 00:21:54,200
a bump up if it's matched you just tweak

00:21:51,380 --> 00:21:55,700
the score there's literally it'd be

00:21:54,200 --> 00:21:58,910
literally nothing more than modifying

00:21:55,700 --> 00:22:00,560
the be m25 relevant score another common

00:21:58,910 --> 00:22:02,210
one is something like more like this

00:22:00,560 --> 00:22:05,210
what more like this provide

00:22:02,210 --> 00:22:07,370
is basically contextually similar

00:22:05,210 --> 00:22:09,290
documents so you've got this huge list

00:22:07,370 --> 00:22:12,770
of terms that you generated when you

00:22:09,290 --> 00:22:14,750
indexed you know all the terms that were

00:22:12,770 --> 00:22:17,180
in that document and you can also find

00:22:14,750 --> 00:22:20,150
other documents that have a very similar

00:22:17,180 --> 00:22:23,450
set of terms M implementation would look

00:22:20,150 --> 00:22:24,920
like collecting all the terms based on

00:22:23,450 --> 00:22:28,040
documents so you probably need an

00:22:24,920 --> 00:22:30,470
alternative data structure to store a

00:22:28,040 --> 00:22:33,230
document two term mapping and then just

00:22:30,470 --> 00:22:36,080
sorting simply based on how many times a

00:22:33,230 --> 00:22:37,910
given document was seen in the set this

00:22:36,080 --> 00:22:39,980
is really really simplistic as is most

00:22:37,910 --> 00:22:43,130
of this presentation just because it's

00:22:39,980 --> 00:22:45,110
to get a high-level engines like solar

00:22:43,130 --> 00:22:47,510
and whatnot use much more complete

00:22:45,110 --> 00:22:50,030
complex solutions like natural language

00:22:47,510 --> 00:22:54,560
processing as well as other kinds of

00:22:50,030 --> 00:22:57,110
types of connect all other types of

00:22:54,560 --> 00:23:01,160
contextual analysis to get a much better

00:22:57,110 --> 00:23:03,260
quality result so I told you not to go

00:23:01,160 --> 00:23:07,430
invent a search engine and I went and

00:23:03,260 --> 00:23:09,440
invented a search engines mmm its code

00:23:07,430 --> 00:23:11,540
is up on github BSD license feel free to

00:23:09,440 --> 00:23:12,770
poke through I've annotated the whole

00:23:11,540 --> 00:23:15,740
source so it should be easy to dive

00:23:12,770 --> 00:23:19,700
through here's a list of some additional

00:23:15,740 --> 00:23:22,340
resources lots of good stuff here the ir

00:23:19,700 --> 00:23:24,350
book is fantastic and largely free on

00:23:22,340 --> 00:23:26,780
the internet it's worth it for any kind

00:23:24,350 --> 00:23:29,090
of textual processing and just learning

00:23:26,780 --> 00:23:31,220
about how information retrieval kinds of

00:23:29,090 --> 00:23:35,410
topics work as well as a bunch of other

00:23:31,220 --> 00:23:35,410
things and thank you very much

00:23:43,149 --> 00:23:51,740
as I mentioned the the slides are up on

00:23:47,899 --> 00:23:55,309
speaker deck at this URL once I make it

00:23:51,740 --> 00:24:01,179
public they'll be up just after this

00:23:55,309 --> 00:24:01,179
talk there

00:24:02,409 --> 00:24:07,700
we have about loops we have about six

00:24:05,899 --> 00:24:09,320
minutes for questions if anyone would

00:24:07,700 --> 00:24:12,919
like to ask a question please line up

00:24:09,320 --> 00:24:14,720
behind that microphone quick note on

00:24:12,919 --> 00:24:17,059
process ask your question I will repeat

00:24:14,720 --> 00:24:21,529
your question and then you will answer

00:24:17,059 --> 00:24:22,850
it so just to kind of review when where

00:24:21,529 --> 00:24:24,230
do you think the cutoff is between

00:24:22,850 --> 00:24:25,760
rolling your own search engine and just

00:24:24,230 --> 00:24:27,529
basically trying to figure out how solar

00:24:25,760 --> 00:24:31,789
works and implementing it yourself and

00:24:27,529 --> 00:24:33,230
your um to me the code I put up I would

00:24:31,789 --> 00:24:36,470
say no one should ever deploy in

00:24:33,230 --> 00:24:37,909
production it is horrific on IO because

00:24:36,470 --> 00:24:40,070
you're literally writing with every

00:24:37,909 --> 00:24:42,919
single term you probably want something

00:24:40,070 --> 00:24:44,929
that does something more like indexes a

00:24:42,919 --> 00:24:46,880
bunch of documents in memory and then

00:24:44,929 --> 00:24:49,460
does a commit and flushes them out to

00:24:46,880 --> 00:24:51,799
disk so to me it's a learning exercise

00:24:49,460 --> 00:24:54,409
it was great to go through and like

00:24:51,799 --> 00:24:58,130
solidify things for myself at a high

00:24:54,409 --> 00:25:00,909
level I'd encourage you to go ahead and

00:24:58,130 --> 00:25:04,190
do that if it's of interest to you but

00:25:00,909 --> 00:25:06,350
it rapidly becomes one of those things

00:25:04,190 --> 00:25:11,840
where you'll probably start hating life

00:25:06,350 --> 00:25:14,690
if you have to support it hey can you

00:25:11,840 --> 00:25:16,610
speak to the relative importance of

00:25:14,690 --> 00:25:19,279
things like stop words when you're using

00:25:16,610 --> 00:25:20,750
something like BM 25 or tf-idf that

00:25:19,279 --> 00:25:25,130
takes into account the frequency of

00:25:20,750 --> 00:25:29,000
words or better refer to any resources

00:25:25,130 --> 00:25:34,700
that show those kinds of differences

00:25:29,000 --> 00:25:39,500
shares so when I showed this I kind of

00:25:34,700 --> 00:25:41,990
glossed over some things the B so what

00:25:39,500 --> 00:25:43,909
BM like you mentioned BM 25 does a great

00:25:41,990 --> 00:25:50,299
job of filtering out words that are

00:25:43,909 --> 00:25:52,830
extremely frequent I can't again I can't

00:25:50,299 --> 00:25:54,260
explain the math on this but essentially

00:25:52,830 --> 00:25:57,659
if something has I believe it's like a

00:25:54,260 --> 00:26:01,350
70 like a 75 percent chance of appearing

00:25:57,659 --> 00:26:04,490
or something or a certain it's like 75

00:26:01,350 --> 00:26:07,110
percent repeats it'll just be ignored so

00:26:04,490 --> 00:26:09,000
the the the way the relevance score is

00:26:07,110 --> 00:26:14,730
calculated is it's kind of weird it's

00:26:09,000 --> 00:26:19,260
it's a trip to read on Wikipedia um you

00:26:14,730 --> 00:26:21,230
can tune it yeah write that the B zero

00:26:19,260 --> 00:26:24,240
that's up there that's passed in as a

00:26:21,230 --> 00:26:26,519
quark that actually is document length

00:26:24,240 --> 00:26:29,820
if you set that it'll just score

00:26:26,519 --> 00:26:31,950
slightly as well as the K modifier is

00:26:29,820 --> 00:26:34,320
just used to like kind of normalize the

00:26:31,950 --> 00:26:37,350
range so like when you do scoring you

00:26:34,320 --> 00:26:38,730
don't get a zero to 100% match like you

00:26:37,350 --> 00:26:40,830
might have seen in older search engines

00:26:38,730 --> 00:26:43,260
you get a weird floating-point number it

00:26:40,830 --> 00:26:45,389
doesn't really mean a whole lot because

00:26:43,260 --> 00:26:47,880
it's just based on the document set as

00:26:45,389 --> 00:26:49,169
well as the query but like the next at

00:26:47,880 --> 00:26:51,360
the moment someone changes the query

00:26:49,169 --> 00:26:53,669
even slightly you can get very different

00:26:51,360 --> 00:26:55,559
scores so that's why you rarely see

00:26:53,669 --> 00:26:56,760
people exposing scores on the front end

00:26:55,559 --> 00:27:02,220
of things because they don't mean a lot

00:26:56,760 --> 00:27:05,490
now be I'm 25 and I believe um I think

00:27:02,220 --> 00:27:08,909
the phased approach also handles this

00:27:05,490 --> 00:27:10,409
they kind of already deal with excluding

00:27:08,909 --> 00:27:13,710
things like stop words and really

00:27:10,409 --> 00:27:16,320
frequent frequently repeated words the

00:27:13,710 --> 00:27:19,649
problem is that you don't if you lean on

00:27:16,320 --> 00:27:21,240
just that you don't get as in my opinion

00:27:19,649 --> 00:27:23,820
don't get as good of quality results

00:27:21,240 --> 00:27:25,889
because other things might be repeated

00:27:23,820 --> 00:27:27,809
very frequently say you're in the Python

00:27:25,889 --> 00:27:31,409
documentation the word Python is

00:27:27,809 --> 00:27:34,980
everywhere and it can be it can be

00:27:31,409 --> 00:27:36,929
ranked lower by a score I score out of B

00:27:34,980 --> 00:27:38,549
of 25 just because of how frequently it

00:27:36,929 --> 00:27:42,750
appears even though it's not really a

00:27:38,549 --> 00:27:45,029
stop word also if you filter out stop

00:27:42,750 --> 00:27:46,320
words ahead of time you save on lots and

00:27:45,029 --> 00:27:48,980
lots and lots and lots and lots of disk

00:27:46,320 --> 00:27:54,179
space it's really more of like an i/o

00:27:48,980 --> 00:27:55,590
saving crutch how you doing

00:27:54,179 --> 00:27:59,370
what are some techniques and algorithms

00:27:55,590 --> 00:28:02,100
for doing a full phrase matching for do

00:27:59,370 --> 00:28:03,120
so the question was what are some

00:28:02,100 --> 00:28:06,810
techniques for doing full phrase

00:28:03,120 --> 00:28:14,100
matching it really depends

00:28:06,810 --> 00:28:16,950
on the it depends on how the query is

00:28:14,100 --> 00:28:18,510
parsed and how the index is read what

00:28:16,950 --> 00:28:20,520
what will typically happen is you'll

00:28:18,510 --> 00:28:24,200
find that a query gets broken down into

00:28:20,520 --> 00:28:27,900
something that's more like a tree with

00:28:24,200 --> 00:28:30,300
operators in between them and so like if

00:28:27,900 --> 00:28:32,340
you double quote something in a query

00:28:30,300 --> 00:28:34,740
chances are that'll get pulled out into

00:28:32,340 --> 00:28:36,570
like if you imagine a query tree it'll

00:28:34,740 --> 00:28:40,860
get pulled out into an exact match node

00:28:36,570 --> 00:28:42,510
and so it'll generate terms so it like

00:28:40,860 --> 00:28:44,550
in the case of engrams not everything

00:28:42,510 --> 00:28:47,070
uses engrams in fact most things default

00:28:44,550 --> 00:28:52,560
to stemming but engrams work better in a

00:28:47,070 --> 00:28:56,100
lot of circumstances so in that exact

00:28:52,560 --> 00:28:58,140
match node of this query tree it will

00:28:56,100 --> 00:29:00,960
still generate those edge in Graham

00:28:58,140 --> 00:29:03,300
terms and look them up but probably

00:29:00,960 --> 00:29:05,430
generate them for the max length of the

00:29:03,300 --> 00:29:07,770
word so like in the case of hello world

00:29:05,430 --> 00:29:09,960
both terms are 5 so generate an edge

00:29:07,770 --> 00:29:12,630
Engram of 5 for both of them look them

00:29:09,960 --> 00:29:14,310
up find documents where it's has both of

00:29:12,630 --> 00:29:16,620
those words and then look at the

00:29:14,310 --> 00:29:17,760
position information and if the

00:29:16,620 --> 00:29:19,200
positions aren't right next to each

00:29:17,760 --> 00:29:20,820
other you discard the result so you're

00:29:19,200 --> 00:29:22,950
just you're just manually filtering the

00:29:20,820 --> 00:29:24,120
results set then to find the documents

00:29:22,950 --> 00:29:26,880
that actually have that phrase

00:29:24,120 --> 00:29:28,320
essentially yes now some people take a

00:29:26,880 --> 00:29:30,210
different some engines take a different

00:29:28,320 --> 00:29:32,690
approach and they will store full

00:29:30,210 --> 00:29:35,670
matches you have to you have to realize

00:29:32,690 --> 00:29:37,920
most search engines are like nearly

00:29:35,670 --> 00:29:39,630
infinitely configurable so you can make

00:29:37,920 --> 00:29:42,030
it do whatever you want and this is a

00:29:39,630 --> 00:29:44,220
really really simple view of things but

00:29:42,030 --> 00:29:46,020
um some engines will store you know full

00:29:44,220 --> 00:29:47,880
exact matches the thing you don't want

00:29:46,020 --> 00:29:49,740
to do is you know try and go through an

00:29:47,880 --> 00:29:52,800
entire blob because as soon as you do

00:29:49,740 --> 00:29:58,260
that performance just drops thanks a lot

00:29:52,800 --> 00:29:59,850
mm-hmm any other questions I actually

00:29:58,260 --> 00:30:02,640
have one

00:29:59,850 --> 00:30:03,990
how does Engram searching work with

00:30:02,640 --> 00:30:08,190
languages that make heavy use of

00:30:03,990 --> 00:30:13,800
prefixes like Greek for instance so I

00:30:08,190 --> 00:30:15,000
can't speak to Greek but if you have it

00:30:13,800 --> 00:30:17,040
depends how it's prefixed if it's

00:30:15,000 --> 00:30:18,480
prefixed with punctuation like you say

00:30:17,040 --> 00:30:19,950
like it's hyphenated or something

00:30:18,480 --> 00:30:20,390
typically you'll be splitting on

00:30:19,950 --> 00:30:23,630
puncture

00:30:20,390 --> 00:30:25,040
anyway if you're talking about I'm

00:30:23,630 --> 00:30:27,080
trying to think of examples like in

00:30:25,040 --> 00:30:32,799
English of common we have a center

00:30:27,080 --> 00:30:34,880
pre-post epicenter at the center so

00:30:32,799 --> 00:30:36,679
another thing that I've kind of hidden

00:30:34,880 --> 00:30:38,929
away in this talk is that say for

00:30:36,679 --> 00:30:41,630
instance solar or loose ends their

00:30:38,929 --> 00:30:47,770
default implementation generates edge

00:30:41,630 --> 00:30:51,260
Ngram sizes from 2 to 15 so even though

00:30:47,770 --> 00:30:53,840
it doesn't help in the case of prefixing

00:30:51,260 --> 00:30:55,460
it doesn't post fixing and the way you

00:30:53,840 --> 00:30:57,169
get around it for prefixed things is

00:30:55,460 --> 00:31:00,770
typically generate engrams

00:30:57,169 --> 00:31:02,600
instead of edge engrams got it ladies

00:31:00,770 --> 00:31:04,960
and gentlemen Daniel Inslee thank you

00:31:02,600 --> 00:31:04,960

YouTube URL: https://www.youtube.com/watch?v=cY7pE7vX6MU


