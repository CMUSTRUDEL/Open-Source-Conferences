Title: Joseph Leingang   Temporal Data Structures with SQLAlchemy and Postgres   PyCon 2017
Publication date: 2017-05-20
Playlist: PyCon 2017
Description: 
	"Speaker: Joseph Leingang

SQLAlchemy ([http://www.sqlalchemy.org](http://www.sqlalchemy.org/)) and Postgres ([https://www.postgresql.org](https://www.postgresql.org/)) provide several useful tools that allow us to build and query records through time: _temporal models_. Combining a need to have robust auditing, as well feature development on per-property history, we can turn “regulatory overhead” into an exciting technical challenge. At Clover Health we have built a small library to automate the task of decorating a model and making it “temporal.” This talk aims to demonstrate the underlying data model and interface for building this system.

Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:00,000 --> 00:00:04,920
data age now data become more and more

00:00:02,820 --> 00:00:06,210
important in our daily life some of you

00:00:04,920 --> 00:00:08,189
probably have the same question as me

00:00:06,210 --> 00:00:09,929
like what is the temporal data structure

00:00:08,189 --> 00:00:12,750
and I think you will find out the answer

00:00:09,929 --> 00:00:24,680
in the next next talk let's welcome our

00:00:12,750 --> 00:00:28,080
second lecture Josie Megan good morning

00:00:24,680 --> 00:00:30,660
thank you for joining me for my first

00:00:28,080 --> 00:00:34,290
conference talk temporal data structures

00:00:30,660 --> 00:00:35,390
with sequel alchemy and Postgres a real

00:00:34,290 --> 00:00:39,180
mouthful

00:00:35,390 --> 00:00:42,030
my name is Joey Lynn gang and I am an

00:00:39,180 --> 00:00:45,899
engineering manager at Clover health we

00:00:42,030 --> 00:00:46,530
are a San Francisco New Jersey based two

00:00:45,899 --> 00:00:49,350
different places

00:00:46,530 --> 00:00:51,920
based health insurance start-up and we

00:00:49,350 --> 00:00:54,079
use Python and Postgres to build

00:00:51,920 --> 00:00:56,039
operational and clinical software

00:00:54,079 --> 00:00:57,780
excellent for me because I've been a

00:00:56,039 --> 00:01:00,840
Python programmer for quite some time

00:00:57,780 --> 00:01:04,049
now I'm not a particularly interesting

00:01:00,840 --> 00:01:05,700
person on Twitter so I encourage you to

00:01:04,049 --> 00:01:07,020
check out our technology blog rather

00:01:05,700 --> 00:01:10,790
than trying to follow me on Twitter you

00:01:07,020 --> 00:01:14,189
won't actually get any good content so

00:01:10,790 --> 00:01:16,950
briefly our agenda today we're going to

00:01:14,189 --> 00:01:19,619
take a look at what temporal means and

00:01:16,950 --> 00:01:22,080
why you might want to use temporal data

00:01:19,619 --> 00:01:24,270
structures I'm going to briefly go over

00:01:22,080 --> 00:01:26,909
some of the things we explored at Clover

00:01:24,270 --> 00:01:30,060
health and then get into the fun stuff

00:01:26,909 --> 00:01:32,340
with Postgres and sequel alchemy this is

00:01:30,060 --> 00:01:35,549
a fairly technical talk with a good bit

00:01:32,340 --> 00:01:37,500
of code and I will hopefully have some

00:01:35,549 --> 00:01:39,210
time at the end for some questions and

00:01:37,500 --> 00:01:44,130
hopefully you will have learned

00:01:39,210 --> 00:01:47,420
something new so time it's like a big

00:01:44,130 --> 00:01:50,460
ball of wibbly-wobbly timey-wimey stuff

00:01:47,420 --> 00:01:52,310
that's great but if you tell that to

00:01:50,460 --> 00:01:55,829
someone who is using your software or

00:01:52,310 --> 00:01:58,740
gasp in auditor they're probably not

00:01:55,829 --> 00:02:00,990
going to care for your sense of humor at

00:01:58,740 --> 00:02:05,700
Clover health our applications need to

00:02:00,990 --> 00:02:06,719
have a rich and consistent history we

00:02:05,700 --> 00:02:09,629
need to be able to answer questions

00:02:06,719 --> 00:02:11,760
about when things changed or why they

00:02:09,629 --> 00:02:13,290
changed and compliance and auditing

00:02:11,760 --> 00:02:17,040
certainly matter

00:02:13,290 --> 00:02:19,379
but you want your software to be the joy

00:02:17,040 --> 00:02:23,250
for your users to use you want it to be

00:02:19,379 --> 00:02:25,409
forgiving but not forgetful and if you

00:02:23,250 --> 00:02:28,079
are in the talk prior to this you've

00:02:25,409 --> 00:02:32,400
maybe experienced a lot of date times

00:02:28,079 --> 00:02:34,109
and a presentation about applying date

00:02:32,400 --> 00:02:35,790
times to absolutely everything in your

00:02:34,109 --> 00:02:40,310
application might be a little bit scary

00:02:35,790 --> 00:02:44,549
don't worry it absolutely scares me so

00:02:40,310 --> 00:02:47,909
this is the dream every time you've

00:02:44,549 --> 00:02:49,650
added a date created or date modified to

00:02:47,909 --> 00:02:52,650
one of your models you might ask

00:02:49,650 --> 00:02:54,989
yourself wouldn't it be nice if you

00:02:52,650 --> 00:02:58,109
could actually know all of the times

00:02:54,989 --> 00:03:00,840
that it was modified or what changed

00:02:58,109 --> 00:03:02,639
when those modifications happened we're

00:03:00,840 --> 00:03:05,939
going to be looking at a simple example

00:03:02,639 --> 00:03:09,150
today with friends and where they live

00:03:05,939 --> 00:03:10,980
some real Big Brother stuff I want to

00:03:09,150 --> 00:03:14,189
know where all of my friends live now

00:03:10,980 --> 00:03:15,689
but everybody seems to move around so I

00:03:14,189 --> 00:03:18,989
don't want to forget where they've been

00:03:15,689 --> 00:03:21,120
in this example Rachael someone I met in

00:03:18,989 --> 00:03:23,819
New York and then at some point in the

00:03:21,120 --> 00:03:26,989
future she moved to San Francisco let's

00:03:23,819 --> 00:03:30,060
see how we can actually pull this off

00:03:26,989 --> 00:03:32,849
first thing we looked at at Clover was

00:03:30,060 --> 00:03:34,709
just using a log file it's a

00:03:32,849 --> 00:03:38,299
time-honored activity and let's be

00:03:34,709 --> 00:03:40,379
honest we all log everything right so

00:03:38,299 --> 00:03:43,019
everything that occurs generates an

00:03:40,379 --> 00:03:45,750
event you get a nice timestamp goes into

00:03:43,019 --> 00:03:47,819
a file or a database somewhere you can

00:03:45,750 --> 00:03:49,979
even beef that up if you want to add

00:03:47,819 --> 00:03:51,780
structured logging maybe you're using

00:03:49,979 --> 00:03:54,449
JSON to give you a little bit more

00:03:51,780 --> 00:03:57,030
information about what happened and this

00:03:54,449 --> 00:03:59,250
definitely works for auditing purposes

00:03:57,030 --> 00:04:02,189
it's a little bit harder to build

00:03:59,250 --> 00:04:04,650
application features with so we decided

00:04:02,189 --> 00:04:07,169
we'll keep logging things because

00:04:04,650 --> 00:04:08,340
logging is the right thing to do but

00:04:07,169 --> 00:04:12,540
we're going to need something a little

00:04:08,340 --> 00:04:15,359
bit different the next thing that we

00:04:12,540 --> 00:04:17,609
looked at was version tables this is

00:04:15,359 --> 00:04:21,570
another very common way to keep history

00:04:17,609 --> 00:04:23,580
we start with two tables one is the

00:04:21,570 --> 00:04:26,730
current state of the world so here we've

00:04:23,580 --> 00:04:29,700
got that friend's table on some entries

00:04:26,730 --> 00:04:31,860
and where they currently live and then

00:04:29,700 --> 00:04:34,710
you have a history table that history

00:04:31,860 --> 00:04:37,230
table has basically the exact same shape

00:04:34,710 --> 00:04:40,790
but it has some sort of history

00:04:37,230 --> 00:04:43,770
bookkeeping shown here as a timestamp

00:04:40,790 --> 00:04:47,280
you could collapse this into a single

00:04:43,770 --> 00:04:51,270
table where the primary key is the ID of

00:04:47,280 --> 00:04:53,970
your friend and the version of it or the

00:04:51,270 --> 00:04:56,280
timestamp of it and this definitely

00:04:53,970 --> 00:04:59,010
works for auditing makes it a little bit

00:04:56,280 --> 00:05:01,230
easier to build features on top of but

00:04:59,010 --> 00:05:04,350
it's still kind of hard to ask the

00:05:01,230 --> 00:05:06,690
question what specifically changed and

00:05:04,350 --> 00:05:11,040
you know how long was a particular

00:05:06,690 --> 00:05:13,920
person in one location so the thing that

00:05:11,040 --> 00:05:16,830
we actually settled on is per property

00:05:13,920 --> 00:05:19,380
tracking here's the last thing we tried

00:05:16,830 --> 00:05:22,200
and we found it to be very powerful

00:05:19,380 --> 00:05:24,620
so you still have that current state

00:05:22,200 --> 00:05:27,990
table all of my friends are listed there

00:05:24,620 --> 00:05:30,210
and their current location and that

00:05:27,990 --> 00:05:33,480
drives the vast majority of our read

00:05:30,210 --> 00:05:37,230
operations but now we split out history

00:05:33,480 --> 00:05:39,330
into two streams to keep track of and

00:05:37,230 --> 00:05:43,170
the first thing is that friends o'clock

00:05:39,330 --> 00:05:46,590
table it has a foreign key to the friend

00:05:43,170 --> 00:05:48,990
a tick which is an integer that tells us

00:05:46,590 --> 00:05:50,730
each time and edit happened in a

00:05:48,990 --> 00:05:54,360
timestamp for when that occurred and

00:05:50,730 --> 00:05:56,190
then we pull that property out into a

00:05:54,360 --> 00:05:59,550
separate table the location history

00:05:56,190 --> 00:06:04,230
table and this lets us know where a

00:05:59,550 --> 00:06:06,600
person was at any point in time all

00:06:04,230 --> 00:06:08,490
right so let's actually make this happen

00:06:06,600 --> 00:06:11,460
we're going to create some tables in

00:06:08,490 --> 00:06:13,110
sequel first and then the first table

00:06:11,460 --> 00:06:15,630
we're going to make is that current

00:06:13,110 --> 00:06:17,820
state the friends table we give it a

00:06:15,630 --> 00:06:20,880
primary key because we always give

00:06:17,820 --> 00:06:23,220
everything a primary key and we add a

00:06:20,880 --> 00:06:26,940
name and a location as simple text

00:06:23,220 --> 00:06:29,190
fields we're going to add that column v

00:06:26,940 --> 00:06:31,410
o'clock or version o'clock which is an

00:06:29,190 --> 00:06:34,800
integer that's going to keep track of

00:06:31,410 --> 00:06:37,020
every generation of edit in the second

00:06:34,800 --> 00:06:39,420
table that we create is that o'clock

00:06:37,020 --> 00:06:40,529
table it has a foreign key to our

00:06:39,420 --> 00:06:43,589
friends

00:06:40,529 --> 00:06:45,929
a tick which is an integer once again

00:06:43,589 --> 00:06:49,139
and the timestamp

00:06:45,929 --> 00:06:51,209
don't forget your time zones every time

00:06:49,139 --> 00:06:53,399
we change a friend's location we're

00:06:51,209 --> 00:06:56,069
going to increment the version clock on

00:06:53,399 --> 00:07:02,609
the friend table and create an entry in

00:06:56,069 --> 00:07:04,859
our clock so slight diversion slight

00:07:02,609 --> 00:07:08,309
diversion we got to talk about ranges

00:07:04,859 --> 00:07:11,519
ranges are an awesome thing in Postgres

00:07:08,309 --> 00:07:13,379
and there are two types of ranges that

00:07:11,519 --> 00:07:16,079
you can consider there are ones that

00:07:13,379 --> 00:07:19,349
have discrete values and continuous

00:07:16,079 --> 00:07:21,649
values all ranges have an upper and a

00:07:19,349 --> 00:07:24,929
lower bound regardless of the type

00:07:21,649 --> 00:07:28,889
integers and date ranges are discrete

00:07:24,929 --> 00:07:33,499
think of one two three four and so on or

00:07:28,889 --> 00:07:37,529
May 20th 21st 22nd so on

00:07:33,499 --> 00:07:41,459
numeric or floating-point and date time

00:07:37,529 --> 00:07:43,919
are continuous date time actually has a

00:07:41,459 --> 00:07:46,319
fixed precision because that's how time

00:07:43,919 --> 00:07:48,599
is kept but for your applications you

00:07:46,319 --> 00:07:53,279
can consider date/time ranges to be

00:07:48,599 --> 00:07:55,949
continuous ranges not only store the

00:07:53,279 --> 00:07:58,739
upper and lower bound in a single column

00:07:55,949 --> 00:08:01,229
they give you some special operators so

00:07:58,739 --> 00:08:03,539
what we have here is some examples for

00:08:01,229 --> 00:08:05,759
creating ranges and doing operators on

00:08:03,539 --> 00:08:08,159
them the first one which is most

00:08:05,759 --> 00:08:11,939
important for this talk is containment

00:08:08,159 --> 00:08:15,059
we can ask the question is the number 3

00:08:11,939 --> 00:08:18,179
in this range of 10 to 20 that answer is

00:08:15,059 --> 00:08:21,359
no it's not you can also do things like

00:08:18,179 --> 00:08:24,359
check if the range is overlap in this

00:08:21,359 --> 00:08:27,419
situation those ranges do overlap we can

00:08:24,359 --> 00:08:29,009
ask what's the upper or lower bound in a

00:08:27,419 --> 00:08:32,939
range if we need to pull that value out

00:08:29,009 --> 00:08:35,129
distinctly there's also intersections

00:08:32,939 --> 00:08:38,189
you can ask if ranges are empty there's

00:08:35,129 --> 00:08:40,169
a whole set of operators on ranges and

00:08:38,189 --> 00:08:44,370
the Postgres documentation is really

00:08:40,169 --> 00:08:46,949
excellent that's where I would start so

00:08:44,370 --> 00:08:49,110
the next thing we need to set up some

00:08:46,949 --> 00:08:50,970
criteria so we're going to add a little

00:08:49,110 --> 00:08:52,590
bit of exclusion and it's the only good

00:08:50,970 --> 00:08:56,040
kind of exclusion

00:08:52,590 --> 00:08:59,620
we need a special Postgres extension

00:08:56,040 --> 00:09:01,950
b-tree Geist just stands for generalized

00:08:59,620 --> 00:09:05,110
search trees they let you make arbitrary

00:09:01,950 --> 00:09:07,690
indices over multiple columns with a

00:09:05,110 --> 00:09:10,300
bunch of different operator classes and

00:09:07,690 --> 00:09:12,370
so our location history table is going

00:09:10,300 --> 00:09:14,380
to take advantage of this we're going to

00:09:12,370 --> 00:09:16,320
give it an ID that primary key because

00:09:14,380 --> 00:09:19,240
once again everything gets a primary key

00:09:16,320 --> 00:09:21,040
and we're going to have a foreign key

00:09:19,240 --> 00:09:23,680
back to the friend that we're keeping

00:09:21,040 --> 00:09:26,410
the location history on and the version

00:09:23,680 --> 00:09:30,400
clock is now arranged rather than just

00:09:26,410 --> 00:09:33,580
being a single entry the actual location

00:09:30,400 --> 00:09:35,650
value at that point in time and this new

00:09:33,580 --> 00:09:38,980
thing which is an exclusion constraint

00:09:35,650 --> 00:09:42,520
this exclusion constraint says for any

00:09:38,980 --> 00:09:44,800
friend that we have they can't have an

00:09:42,520 --> 00:09:47,650
overlapping version clock what this

00:09:44,800 --> 00:09:49,930
means is a person can't live in more

00:09:47,650 --> 00:09:51,820
than one location at the same time so

00:09:49,930 --> 00:09:54,760
Postgres is going to keep track of that

00:09:51,820 --> 00:09:56,410
for us what does this look like if we

00:09:54,760 --> 00:10:00,280
actually try to create some history

00:09:56,410 --> 00:10:03,340
entries so we're going to create a

00:10:00,280 --> 00:10:05,680
history entry for my first friend dave

00:10:03,340 --> 00:10:09,610
who lived in tucson when i met him or

00:10:05,680 --> 00:10:13,990
version one tell postgres to insert it

00:10:09,610 --> 00:10:15,690
no problem time passes dave moves to San

00:10:13,990 --> 00:10:18,790
Francisco a good choice in my opinion

00:10:15,690 --> 00:10:22,390
we're going to create the next entry so

00:10:18,790 --> 00:10:24,790
same friend ID now in San Francisco and

00:10:22,390 --> 00:10:27,910
we're going to say version two and up

00:10:24,790 --> 00:10:28,810
Postgres says slow down there buddy you

00:10:27,910 --> 00:10:32,260
can't do that

00:10:28,810 --> 00:10:34,570
there's overlapping the clocks so to

00:10:32,260 --> 00:10:36,940
actually make this work we need to

00:10:34,570 --> 00:10:39,670
update that previous entry that we had

00:10:36,940 --> 00:10:42,520
before we insert the new one we would do

00:10:39,670 --> 00:10:46,530
this all in one transaction so that we

00:10:42,520 --> 00:10:49,900
could keep a transaction time on it

00:10:46,530 --> 00:10:52,840
creating a lot of tables writing inserts

00:10:49,900 --> 00:10:54,760
and updates is very tedious and this is

00:10:52,840 --> 00:10:57,580
PyCon after all so I had better show you

00:10:54,760 --> 00:10:59,830
some Python the first thing we're going

00:10:57,580 --> 00:11:01,510
to need is a whole bunch of inserts I

00:10:59,830 --> 00:11:05,620
don't really want to talk about them

00:11:01,510 --> 00:11:07,779
I like sequel alchemy as si but that's a

00:11:05,620 --> 00:11:12,279
tested things so we'll skip right over

00:11:07,779 --> 00:11:16,270
that and sequel alchemy declarative this

00:11:12,279 --> 00:11:18,190
is a tool in sequel alchemy that allows

00:11:16,270 --> 00:11:20,910
us to think about our software in terms

00:11:18,190 --> 00:11:23,080
of classes and we can apply

00:11:20,910 --> 00:11:23,740
object-oriented programming to our

00:11:23,080 --> 00:11:28,180
problems

00:11:23,740 --> 00:11:30,580
so my friends they have names they have

00:11:28,180 --> 00:11:32,890
locations and now both of them can

00:11:30,580 --> 00:11:35,710
change and we're going to keep all of

00:11:32,890 --> 00:11:37,060
that in a separate schema in Postgres so

00:11:35,710 --> 00:11:42,490
that we don't get confused when we're

00:11:37,060 --> 00:11:44,800
looking at our tables that model had a

00:11:42,490 --> 00:11:47,529
mixin on it a special mixin called

00:11:44,800 --> 00:11:51,520
temporal model let's take a take a look

00:11:47,529 --> 00:11:54,850
at what that does so this is a

00:11:51,520 --> 00:11:58,420
declarative interface to defining your

00:11:54,850 --> 00:12:01,690
history it's got that column on it V

00:11:58,420 --> 00:12:03,910
clock which is an integer and every

00:12:01,690 --> 00:12:06,339
model that we mix this in on gets it

00:12:03,910 --> 00:12:09,940
very typical do not repeat yourself

00:12:06,339 --> 00:12:12,010
stuff and then we have four methods that

00:12:09,940 --> 00:12:15,040
call into core parts of the library

00:12:12,010 --> 00:12:18,279
we've got this clock tick we have a

00:12:15,040 --> 00:12:20,350
temporal map we have an initializing of

00:12:18,279 --> 00:12:22,360
the clock and we have a mapper class

00:12:20,350 --> 00:12:25,140
let's take a look at each one of these

00:12:22,360 --> 00:12:28,959
methods individually

00:12:25,140 --> 00:12:32,470
so first up sequel alchemy declared

00:12:28,959 --> 00:12:34,240
attribute this is a scalar like property

00:12:32,470 --> 00:12:37,420
that can be called from the class

00:12:34,240 --> 00:12:40,330
directly sequel alchemy treats declared

00:12:37,420 --> 00:12:45,040
attributes as returning a special

00:12:40,330 --> 00:12:47,800
construct used when mapping a class and

00:12:45,040 --> 00:12:50,620
we typically use this to mix in custom

00:12:47,800 --> 00:12:53,950
logic for foreign keys and relationships

00:12:50,620 --> 00:12:57,100
but in this case we need to override a

00:12:53,950 --> 00:13:02,920
course equal alchemy directive which is

00:12:57,100 --> 00:13:06,370
the macro class itself the M in ORM we

00:13:02,920 --> 00:13:08,470
need this class to be mapped first then

00:13:06,370 --> 00:13:11,890
we can set up our history tracking on it

00:13:08,470 --> 00:13:14,260
so we build a function in inline and we

00:13:11,890 --> 00:13:18,420
return that sequel alchemy we'll use

00:13:14,260 --> 00:13:18,420
that function to map our class

00:13:18,620 --> 00:13:24,410
next up is that temporal map function

00:13:21,500 --> 00:13:27,170
and here I'm going to do a bit of hand

00:13:24,410 --> 00:13:29,750
waving the work is not actually done as

00:13:27,170 --> 00:13:31,160
comments but it's a it's a very long

00:13:29,750 --> 00:13:34,400
function so we can't show the whole

00:13:31,160 --> 00:13:37,430
thing first thing is this is a Python

00:13:34,400 --> 00:13:39,860
static method means there's no implicit

00:13:37,430 --> 00:13:43,760
arguments to it whatsoever and in fact

00:13:39,860 --> 00:13:45,620
the second argument to it is class CLS

00:13:43,760 --> 00:13:49,070
which you normally see the first

00:13:45,620 --> 00:13:51,380
argument but because it's passed in when

00:13:49,070 --> 00:13:54,260
we're mapping this is actually the

00:13:51,380 --> 00:13:56,840
mapped model that we want to generate

00:13:54,260 --> 00:13:58,940
some history for so what it does is

00:13:56,840 --> 00:14:00,800
actually looks at all of those

00:13:58,940 --> 00:14:04,700
properties that we said we wanted to

00:14:00,800 --> 00:14:07,330
keep track of it builds the history

00:14:04,700 --> 00:14:10,310
tables and the history models for it

00:14:07,330 --> 00:14:12,890
builds that clock table and the clock

00:14:10,310 --> 00:14:15,140
model and then it configures a whole

00:14:12,890 --> 00:14:19,310
bunch of relationships that go back to

00:14:15,140 --> 00:14:21,710
our friend model you could probably swap

00:14:19,310 --> 00:14:25,160
out the underlying storage mechanism

00:14:21,710 --> 00:14:27,740
here so if you wanted to do a log file

00:14:25,160 --> 00:14:31,850
or you wanted to do version tables you

00:14:27,740 --> 00:14:33,980
could replace the logic here the last

00:14:31,850 --> 00:14:36,800
thing that we do is we have to set up an

00:14:33,980 --> 00:14:38,930
event listener so whenever you create a

00:14:36,800 --> 00:14:40,690
new friend or whenever you meet a new

00:14:38,930 --> 00:14:44,000
person and you want to make an entry you

00:14:40,690 --> 00:14:45,680
have to start history somewhere this is

00:14:44,000 --> 00:14:49,490
another sequel alchemy feature that

00:14:45,680 --> 00:14:51,950
allows you to listen to events when a

00:14:49,490 --> 00:14:56,300
model is created so when an instance is

00:14:51,950 --> 00:14:58,690
created from that model so what happens

00:14:56,300 --> 00:15:02,180
when we actually start keeping history

00:14:58,690 --> 00:15:05,840
this is once again another static method

00:15:02,180 --> 00:15:10,010
the init clock and this is the thing

00:15:05,840 --> 00:15:11,750
that we just listened to so we want to

00:15:10,010 --> 00:15:13,790
make sure that our versions start at

00:15:11,750 --> 00:15:16,160
somewhere sane we'll start it at 1:00

00:15:13,790 --> 00:15:18,260
unless you tell it to start at a

00:15:16,160 --> 00:15:20,750
different number we're going to generate

00:15:18,260 --> 00:15:24,200
that clock entry with the current

00:15:20,750 --> 00:15:26,720
timestamp the new thing here that I

00:15:24,200 --> 00:15:29,540
haven't talked about yet so activity

00:15:26,720 --> 00:15:31,910
activity is a special feature in the

00:15:29,540 --> 00:15:32,449
temporal library that allows you to

00:15:31,910 --> 00:15:36,439
define

00:15:32,449 --> 00:15:37,999
a custom model to represent how or why

00:15:36,439 --> 00:15:40,249
something changed

00:15:37,999 --> 00:15:43,549
so you basically give your history any

00:15:40,249 --> 00:15:45,739
context that you want I'll let your

00:15:43,549 --> 00:15:49,209
imagination run wild on that it's a very

00:15:45,739 --> 00:15:53,149
important feature at Clover health

00:15:49,209 --> 00:15:55,879
finally we have the most gnarly method

00:15:53,149 --> 00:15:57,980
in the library this is the clock tick

00:15:55,879 --> 00:16:01,879
this is what happens when we change

00:15:57,980 --> 00:16:04,129
something we've made it a Python context

00:16:01,879 --> 00:16:07,040
manager so you can logically think of

00:16:04,129 --> 00:16:10,639
all of your changes corresponding to one

00:16:07,040 --> 00:16:13,779
edit we come in so you're going to say

00:16:10,639 --> 00:16:16,489
with some instance dot clock tick and

00:16:13,779 --> 00:16:19,459
that instance had to be selected from

00:16:16,489 --> 00:16:22,730
the database so it already exists in a

00:16:19,459 --> 00:16:25,999
sequel alchemy session we're going to

00:16:22,730 --> 00:16:28,249
disable auto flush auto flush is usually

00:16:25,999 --> 00:16:30,679
a very good thing in sequel alchemy that

00:16:28,249 --> 00:16:32,720
as it needs to read things from the

00:16:30,679 --> 00:16:34,999
database it will automatically write

00:16:32,720 --> 00:16:38,269
your changes it allows the session to

00:16:34,999 --> 00:16:40,549
stay in sync but we got to be very

00:16:38,269 --> 00:16:42,259
careful about writing all of our things

00:16:40,549 --> 00:16:46,489
at the same time so we're going to

00:16:42,259 --> 00:16:48,769
disable that then we yield to the

00:16:46,489 --> 00:16:51,679
business logic of our application where

00:16:48,769 --> 00:16:54,319
we make those changes we come back in

00:16:51,679 --> 00:16:57,949
and we check did anything actually

00:16:54,319 --> 00:17:02,029
change if so we will go ahead and

00:16:57,949 --> 00:17:05,419
increase that version clock by one and

00:17:02,029 --> 00:17:07,939
create a new clock tick entry and we

00:17:05,419 --> 00:17:10,760
will associate the activity instance

00:17:07,939 --> 00:17:14,449
with it if necessary and we add all of

00:17:10,760 --> 00:17:16,339
that to the session and at some point

00:17:14,449 --> 00:17:18,350
later you would do a flush or you would

00:17:16,339 --> 00:17:24,079
do a commit and it would all get written

00:17:18,350 --> 00:17:27,649
to the database so what I should say is

00:17:24,079 --> 00:17:30,380
that the model mixin is not the only way

00:17:27,649 --> 00:17:34,010
to use the library it looks an awful lot

00:17:30,380 --> 00:17:36,529
like a Django made a class which is

00:17:34,010 --> 00:17:38,510
intentional but there's another way that

00:17:36,529 --> 00:17:41,240
you could use it we have a decorator in

00:17:38,510 --> 00:17:43,370
there that you can just add a clock to

00:17:41,240 --> 00:17:45,559
anything you give it a list of the

00:17:43,370 --> 00:17:46,350
properties that you want to track once

00:17:45,559 --> 00:17:48,529
again at

00:17:46,350 --> 00:17:51,179
tivity class is totally optional and you

00:17:48,529 --> 00:17:56,029
can specify the schema that you want to

00:17:51,179 --> 00:17:56,029
write all of this stuff into excuse me

00:17:56,809 --> 00:18:02,130
so if we wanted to use the decorator on

00:18:00,240 --> 00:18:05,100
our friend model it would look a bit

00:18:02,130 --> 00:18:07,110
like this just add the clock we say once

00:18:05,100 --> 00:18:10,500
again let's keep track of a person's

00:18:07,110 --> 00:18:14,210
name and their location and once again

00:18:10,500 --> 00:18:14,210
let's store that in the history schema

00:18:14,480 --> 00:18:20,279
so this was the dream that we had when

00:18:18,269 --> 00:18:22,259
we started to talk let's come back to it

00:18:20,279 --> 00:18:25,429
we can kind of see how it all works

00:18:22,259 --> 00:18:28,399
together now I met Rachel in New York

00:18:25,429 --> 00:18:31,679
sometime past she moved to San Francisco

00:18:28,399 --> 00:18:35,009
so I update her record with a clock tick

00:18:31,679 --> 00:18:37,740
I save it to the database and now I can

00:18:35,009 --> 00:18:39,960
actually inspect the history of the

00:18:37,740 --> 00:18:42,570
location the first entry was New York

00:18:39,960 --> 00:18:46,200
the second and current value is San

00:18:42,570 --> 00:18:51,240
Francisco it's very cool each property

00:18:46,200 --> 00:18:54,629
is actually tracked independently so

00:18:51,240 --> 00:18:57,509
what are the takeaways keeping history

00:18:54,629 --> 00:19:00,480
on your models if you want temporal data

00:18:57,509 --> 00:19:02,970
structures you are going to add a lot of

00:19:00,480 --> 00:19:06,840
complexity to your application this

00:19:02,970 --> 00:19:09,600
method creates a ton of tables right and

00:19:06,840 --> 00:19:12,090
that's that sometimes very scary so I

00:19:09,600 --> 00:19:15,419
probably wouldn't use it for absolutely

00:19:12,090 --> 00:19:17,429
everything you have to decide what

00:19:15,419 --> 00:19:21,299
method you need and what are the

00:19:17,429 --> 00:19:24,330
trade-offs there bulk operations with

00:19:21,299 --> 00:19:27,360
temporal with log files with version

00:19:24,330 --> 00:19:29,870
tables all of them very challenging you

00:19:27,360 --> 00:19:33,269
have to do a number of inserts

00:19:29,870 --> 00:19:35,190
potentially a number of updates so doing

00:19:33,269 --> 00:19:37,440
any any history stuff in bulk is very

00:19:35,190 --> 00:19:38,940
confusing you also have to decide what

00:19:37,440 --> 00:19:41,490
is the timeline that we want for this

00:19:38,940 --> 00:19:43,259
bulk operation are we doing things back

00:19:41,490 --> 00:19:43,710
in time do we need to forward data

00:19:43,259 --> 00:19:48,000
anything

00:19:43,710 --> 00:19:50,759
it's very complicated Postgres and

00:19:48,000 --> 00:19:54,450
sequel alchemy however are very powerful

00:19:50,759 --> 00:19:57,539
and together with the ranges and the

00:19:54,450 --> 00:19:59,490
exclusion constraints and that ORM all

00:19:57,539 --> 00:20:02,760
of those features in there

00:19:59,490 --> 00:20:06,770
you can kind of come up with a mix of

00:20:02,760 --> 00:20:11,100
history tracking that works for you

00:20:06,770 --> 00:20:13,110
so this library actually is open source

00:20:11,100 --> 00:20:16,590
you can check it out on github

00:20:13,110 --> 00:20:18,500
we don't have any documentation yet the

00:20:16,590 --> 00:20:23,160
test coverage however is very good so

00:20:18,500 --> 00:20:25,500
it's very good you can use it download

00:20:23,160 --> 00:20:27,000
it try it out add it to your models

00:20:25,500 --> 00:20:29,940
there's the decorator there's the

00:20:27,000 --> 00:20:32,460
temporal model mixin it will get

00:20:29,940 --> 00:20:36,390
published to the Python package index as

00:20:32,460 --> 00:20:38,040
soon as we get some documentation you

00:20:36,390 --> 00:20:40,200
should know that it does require some

00:20:38,040 --> 00:20:44,130
more up-to-date things

00:20:40,200 --> 00:20:47,370
Python 3 3 or better sequel alchemy 1 0

00:20:44,130 --> 00:20:50,300
15 or better it only works with Postgres

00:20:47,370 --> 00:20:54,240
and only Postgres 9 3 or better and

00:20:50,300 --> 00:20:57,179
psycho PG 262 which gives you those sort

00:20:54,240 --> 00:20:59,400
of underlying range type primitives now

00:20:57,179 --> 00:21:03,480
I went way faster than I actually

00:20:59,400 --> 00:21:05,929
intended to I'm out of content ready for

00:21:03,480 --> 00:21:05,929
questions

00:21:12,260 --> 00:21:16,350
I'm supposed to ask you to use the

00:21:14,820 --> 00:21:19,410
microphones as well if you have some

00:21:16,350 --> 00:21:21,960
questions I was wondering what about if

00:21:19,410 --> 00:21:24,210
you wanted to track rather the history

00:21:21,960 --> 00:21:26,580
of kind of sets of objects or rather

00:21:24,210 --> 00:21:28,800
than the relationship between objects so

00:21:26,580 --> 00:21:30,930
instead of tracking scaler properties on

00:21:28,800 --> 00:21:35,190
an individual object so maybe you wanted

00:21:30,930 --> 00:21:37,170
to know like the history over time of

00:21:35,190 --> 00:21:40,620
like which of your friends are friends

00:21:37,170 --> 00:21:42,630
with other friends so like is there have

00:21:40,620 --> 00:21:44,450
you looked at that or tracking the

00:21:42,630 --> 00:21:48,030
history of the relationships between

00:21:44,450 --> 00:21:52,470
objects yes so you actually can track

00:21:48,030 --> 00:21:53,880
relationships I did not show that it's a

00:21:52,470 --> 00:21:56,580
little bit more complicated of an

00:21:53,880 --> 00:21:58,590
exercise materializing those

00:21:56,580 --> 00:22:01,080
relationships is actually the hard part

00:21:58,590 --> 00:22:03,570
keeping track of them as you insert them

00:22:01,080 --> 00:22:05,520
into the database where you use sequel

00:22:03,570 --> 00:22:08,220
Alchemy's ORM to keep track of that

00:22:05,520 --> 00:22:10,350
stuff makes it a breeze when you need to

00:22:08,220 --> 00:22:12,990
pull it out and you have to sort of

00:22:10,350 --> 00:22:14,460
decide how do i display this to someone

00:22:12,990 --> 00:22:16,110
isn't going to go into an audit log

00:22:14,460 --> 00:22:18,150
where's it going to be in the

00:22:16,110 --> 00:22:21,000
application itself you kind of have to

00:22:18,150 --> 00:22:22,200
make gametime decisions so that's why I

00:22:21,000 --> 00:22:24,320
didn't show it but yeah you can

00:22:22,200 --> 00:22:30,000
absolutely do that thank you thank you

00:22:24,320 --> 00:22:32,490
all right so you showed I think very

00:22:30,000 --> 00:22:35,130
Bruce on the screen would look like a

00:22:32,490 --> 00:22:39,420
fancy Postgres feature the gist

00:22:35,130 --> 00:22:42,060
oh yeah and let's go back part of the

00:22:39,420 --> 00:22:46,140
the what this dream would be I think for

00:22:42,060 --> 00:22:47,580
me is after you know wide swath of

00:22:46,140 --> 00:22:49,680
people have had many updates over time

00:22:47,580 --> 00:22:53,310
to and ask well what was the situation

00:22:49,680 --> 00:22:57,030
as of certain day two years ago certain

00:22:53,310 --> 00:22:59,610
moment does that query end up being well

00:22:57,030 --> 00:23:02,790
indexed here how does that work yeah so

00:22:59,610 --> 00:23:06,150
with the with everything being indexed

00:23:02,790 --> 00:23:08,820
and with the exclusion constraint there

00:23:06,150 --> 00:23:11,850
there is an easy way to sort of say give

00:23:08,820 --> 00:23:14,280
me the state of this model at a point in

00:23:11,850 --> 00:23:16,590
time the state of sort of this one

00:23:14,280 --> 00:23:18,840
instance or the of yeah of one in front

00:23:16,590 --> 00:23:22,410
one at a point in time that's what we

00:23:18,840 --> 00:23:23,640
built now it's once again it kind of

00:23:22,410 --> 00:23:25,020
gets back to how do you want to

00:23:23,640 --> 00:23:26,520
materialize that data

00:23:25,020 --> 00:23:28,800
and how do you want to interact with it

00:23:26,520 --> 00:23:32,040
in your application so we haven't quite

00:23:28,800 --> 00:23:33,870
decided do you get the full class or you

00:23:32,040 --> 00:23:35,460
just get like a dictionary of the

00:23:33,870 --> 00:23:37,620
information at that point in time

00:23:35,460 --> 00:23:40,590
can you move forward and backwards from

00:23:37,620 --> 00:23:45,150
it we haven't built any of that yet cool

00:23:40,590 --> 00:23:46,890
thing I'm curious did you have to deal

00:23:45,150 --> 00:23:49,800
with by temporality so in your example

00:23:46,890 --> 00:23:50,910
for example you know not only where

00:23:49,800 --> 00:23:52,860
somebody lives but what their job was

00:23:50,910 --> 00:23:54,929
and you want to know the full Cartesian

00:23:52,860 --> 00:23:57,450
product of where they lived in which

00:23:54,929 --> 00:24:00,210
aisle they had over time yes so the

00:23:57,450 --> 00:24:03,510
question is about by temporality so can

00:24:00,210 --> 00:24:06,840
plural is just time dimensions and by

00:24:03,510 --> 00:24:09,780
temporality is - time dimensions this is

00:24:06,840 --> 00:24:13,190
very cool if you want to say I knew

00:24:09,780 --> 00:24:17,760
something about this at this time and

00:24:13,190 --> 00:24:22,230
the world knew it at a different time so

00:24:17,760 --> 00:24:24,960
if you were to add or keep track of a

00:24:22,230 --> 00:24:27,390
time stamp property you are actually

00:24:24,960 --> 00:24:30,450
making it by temporal just by doing that

00:24:27,390 --> 00:24:33,980
the other thing is under the hood the

00:24:30,450 --> 00:24:37,170
history tracking actually uses Postgres

00:24:33,980 --> 00:24:39,570
current transaction time stamp so it

00:24:37,170 --> 00:24:42,059
does keep track of when did the system

00:24:39,570 --> 00:24:44,309
know about this stuff and so if you need

00:24:42,059 --> 00:24:46,080
to differentiate when did the system

00:24:44,309 --> 00:24:51,660
know about it and when did I know about

00:24:46,080 --> 00:24:53,460
it you can do that I post consuls a lot

00:24:51,660 --> 00:24:55,679
of really interesting advanced features

00:24:53,460 --> 00:24:58,559
that I don't know a great deal about the

00:24:55,679 --> 00:24:59,280
range of feature is a new one to me

00:24:58,559 --> 00:25:01,320
mm-hmm

00:24:59,280 --> 00:25:03,120
it looks like a nifty use case that

00:25:01,320 --> 00:25:04,470
you're using it for but it also looks to

00:25:03,120 --> 00:25:05,970
me like this is basically just a

00:25:04,470 --> 00:25:08,250
sequence of integers one after another

00:25:05,970 --> 00:25:10,830
yes can you talk about the trade offs of

00:25:08,250 --> 00:25:14,340
using the more complicated range versus

00:25:10,830 --> 00:25:17,490
just integers so the reason we use

00:25:14,340 --> 00:25:20,160
integers is it makes thinking about your

00:25:17,490 --> 00:25:22,590
data a bit easier if you're looking at

00:25:20,160 --> 00:25:24,720
the current state table you have an

00:25:22,590 --> 00:25:27,780
integer right there that tells you how

00:25:24,720 --> 00:25:30,720
many times did this change so if I

00:25:27,780 --> 00:25:32,970
needed to manually construct queries to

00:25:30,720 --> 00:25:35,910
read the history entries I know how many

00:25:32,970 --> 00:25:37,350
queries I'm going to have to do you

00:25:35,910 --> 00:25:38,680
would usually collapse that into one if

00:25:37,350 --> 00:25:43,180
you if you really wanted to get

00:25:38,680 --> 00:25:44,560
and see with it but the the fun thing

00:25:43,180 --> 00:25:46,600
with the ranges is when you stop

00:25:44,560 --> 00:25:47,890
thinking about just the discrete ones

00:25:46,600 --> 00:25:51,250
and you start thinking about the

00:25:47,890 --> 00:25:54,880
continuous ranges so you could use the

00:25:51,250 --> 00:25:57,220
date/time with time zone range and that

00:25:54,880 --> 00:25:59,440
would keep track of a full continuous

00:25:57,220 --> 00:26:02,440
history and you could actually build a

00:25:59,440 --> 00:26:04,090
thing where you slid the time forward

00:26:02,440 --> 00:26:07,930
and backwards and it would know exactly

00:26:04,090 --> 00:26:09,520
where you are you're absolutely right

00:26:07,930 --> 00:26:12,460
though the integers are just super easy

00:26:09,520 --> 00:26:14,560
to reason about super easy to store and

00:26:12,460 --> 00:26:17,740
select it doesn't use a ton of disk

00:26:14,560 --> 00:26:19,810
space and I think you should always

00:26:17,740 --> 00:26:21,280
start with the easiest to think first

00:26:19,810 --> 00:26:26,380
that's why we started with the integers

00:26:21,280 --> 00:26:28,930
thank you so on the subject of the the

00:26:26,380 --> 00:26:30,730
easiest thing first a very very naive

00:26:28,930 --> 00:26:32,500
approach to this problem would be that

00:26:30,730 --> 00:26:34,240
you have a single table and you have a

00:26:32,500 --> 00:26:36,550
start time and an end time for each

00:26:34,240 --> 00:26:38,110
entry and that has the advantage that

00:26:36,550 --> 00:26:39,790
when you're doing historical lookups you

00:26:38,110 --> 00:26:43,120
can use between and you can get the

00:26:39,790 --> 00:26:45,460
entire state very cleanly what

00:26:43,120 --> 00:26:48,460
advantages do two doesn't deserve up

00:26:45,460 --> 00:26:51,910
here have over such a nice solution so

00:26:48,460 --> 00:26:54,730
we looked at that and the reason why we

00:26:51,910 --> 00:26:57,580
ultimately decided on this was we had

00:26:54,730 --> 00:26:59,290
there are some properties of our models

00:26:57,580 --> 00:27:01,360
at Clover health that change very

00:26:59,290 --> 00:27:03,730
frequently they're like high velocity

00:27:01,360 --> 00:27:05,650
changes other properties do not change

00:27:03,730 --> 00:27:07,810
that frequently but we still need to

00:27:05,650 --> 00:27:10,500
keep track of it for the ones that

00:27:07,810 --> 00:27:13,270
change very frequently there's also

00:27:10,500 --> 00:27:16,360
auditing requirements related with that

00:27:13,270 --> 00:27:19,450
so we needed to build features for those

00:27:16,360 --> 00:27:21,970
properties specifically this allowed us

00:27:19,450 --> 00:27:24,790
to sort of think about everything as the

00:27:21,970 --> 00:27:27,220
history is always being kept but here is

00:27:24,790 --> 00:27:30,010
a nice easy interface for those high

00:27:27,220 --> 00:27:31,990
velocity changes and you can use it for

00:27:30,010 --> 00:27:39,250
the low velocity stuff but you probably

00:27:31,990 --> 00:27:42,700
don't even need to worry about it hi so

00:27:39,250 --> 00:27:45,730
one you did give the warning about the

00:27:42,700 --> 00:27:48,310
bulk inserts as such so what's the

00:27:45,730 --> 00:27:49,390
question is if you have say five

00:27:48,310 --> 00:27:51,010
properties that you're going to track

00:27:49,390 --> 00:27:51,770
you're going to be inserting into five

00:27:51,010 --> 00:27:55,040
different

00:27:51,770 --> 00:27:58,820
oh yeah so now you're wrapping that

00:27:55,040 --> 00:28:01,940
under one transaction now how do you see

00:27:58,820 --> 00:28:03,650
that going and then isn't that a bit

00:28:01,940 --> 00:28:05,840
dangerous as an I aren't I gonna get a

00:28:03,650 --> 00:28:09,140
huge performance problem even for a

00:28:05,840 --> 00:28:11,090
single insert forget bulk and have you

00:28:09,140 --> 00:28:15,320
tried looking at both dresses listen and

00:28:11,090 --> 00:28:17,450
notify do what is the abstract away from

00:28:15,320 --> 00:28:21,380
and then put when you put in putting the

00:28:17,450 --> 00:28:23,060
data it into the history tables away

00:28:21,380 --> 00:28:25,640
from the transaction that is after the

00:28:23,060 --> 00:28:30,550
transaction happens we looked at doing

00:28:25,640 --> 00:28:32,450
that with triggers and stored procedures

00:28:30,550 --> 00:28:34,370
I'm going to be completely honest with

00:28:32,450 --> 00:28:36,980
everybody yeah it is very dangerous to

00:28:34,370 --> 00:28:38,650
have a whole racket tables and have to

00:28:36,980 --> 00:28:41,390
do a whole bunch of inserts and updates

00:28:38,650 --> 00:28:44,510
but living dangerously is very fun so I

00:28:41,390 --> 00:28:46,130
encourage you to do it what we've

00:28:44,510 --> 00:28:49,310
actually discovered we've been using

00:28:46,130 --> 00:28:53,140
this library in production on sort of

00:28:49,310 --> 00:28:59,030
audited software for many months now and

00:28:53,140 --> 00:29:00,320
the performance though less than some

00:28:59,030 --> 00:29:02,300
other methods or if you didn't have to

00:29:00,320 --> 00:29:03,650
keep track of history at all it's it's

00:29:02,300 --> 00:29:07,520
definitely a little bit slower

00:29:03,650 --> 00:29:10,160
it is not slow enough to actually cause

00:29:07,520 --> 00:29:12,260
a concern and we actually do a

00:29:10,160 --> 00:29:14,060
tremendous amount of bulk operations

00:29:12,260 --> 00:29:18,590
with it and the performance is totally

00:29:14,060 --> 00:29:20,600
fine Postgres is awesome plenty fast you

00:29:18,590 --> 00:29:21,890
can beef it up don't worry about a lot

00:29:20,600 --> 00:29:24,140
of tables don't worry about a lot of

00:29:21,890 --> 00:29:28,190
inserts Postgres has totally got your

00:29:24,140 --> 00:29:29,000
back hey there just wondering your

00:29:28,190 --> 00:29:31,310
thoughts and if there were any

00:29:29,000 --> 00:29:33,620
considerations to having a single source

00:29:31,310 --> 00:29:36,140
of truth being that history table as

00:29:33,620 --> 00:29:37,310
opposed to having the history table and

00:29:36,140 --> 00:29:39,160
then also having to update the user

00:29:37,310 --> 00:29:41,450
object so you'd like get the last

00:29:39,160 --> 00:29:44,240
location in that history table for the

00:29:41,450 --> 00:29:46,580
user yeah that's a great question

00:29:44,240 --> 00:29:48,710
so when we first started building this

00:29:46,580 --> 00:29:50,690
we did have a dream that we wouldn't

00:29:48,710 --> 00:29:53,120
need that current state table at all

00:29:50,690 --> 00:29:56,360
except for properties that are immutable

00:29:53,120 --> 00:30:00,410
we're just about at time so you could do

00:29:56,360 --> 00:30:02,330
that but it does make the work on data

00:30:00,410 --> 00:30:04,179
science significantly harder because now

00:30:02,330 --> 00:30:08,159
they got to join everything even

00:30:04,179 --> 00:30:10,899
the current state yeah what's up

00:30:08,159 --> 00:30:12,820
last question maybe I can come up after

00:30:10,899 --> 00:30:14,679
but how do historical Corrections work

00:30:12,820 --> 00:30:16,059
with the schema of someone so they lived

00:30:14,679 --> 00:30:20,019
in a different city but they actually

00:30:16,059 --> 00:30:22,179
didn't so that would be where you get

00:30:20,019 --> 00:30:24,340
into the realm of by temporality so that

00:30:22,179 --> 00:30:26,860
you can rearrange history but you're

00:30:24,340 --> 00:30:29,590
only ever moving forward with your

00:30:26,860 --> 00:30:31,749
inserts the way we think about it at

00:30:29,590 --> 00:30:33,789
Clover is if you've made a mistake

00:30:31,749 --> 00:30:36,100
remember the software is supposed to be

00:30:33,789 --> 00:30:38,080
forgiving but not forgetful you just

00:30:36,100 --> 00:30:39,730
update the record and you create a new

00:30:38,080 --> 00:30:42,789
history entry and you go forward in time

00:30:39,730 --> 00:30:45,070
if you did need to have that different

00:30:42,789 --> 00:30:49,090
timeline that would be where you would

00:30:45,070 --> 00:30:51,090
add the by temporality to it all right

00:30:49,090 --> 00:30:59,650
we're out of time thanks everybody

00:30:51,090 --> 00:30:59,650

YouTube URL: https://www.youtube.com/watch?v=2Za9kca3Tu0


