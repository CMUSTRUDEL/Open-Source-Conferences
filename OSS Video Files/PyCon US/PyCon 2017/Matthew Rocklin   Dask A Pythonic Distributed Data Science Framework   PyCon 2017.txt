Title: Matthew Rocklin   Dask A Pythonic Distributed Data Science Framework   PyCon 2017
Publication date: 2017-05-20
Playlist: PyCon 2017
Description: 
	"Speaker: Matthew Rocklin

Dask is a general purpose parallel computing system capable of Celery-like task
scheduling, Spark-like big data computing, and Numpy/Pandas/Scikit-learn level
complex algorithms, written in Pure Python.  Dask has been adopted by the
PyData community as a Big Data solution.

This talk focuses on the distributed task scheduler that powers Dask when
running on a cluster.  We'll focus on how we built a Big Data computing system
using the Python networking stack (Tornado/AsyncIO) in service of its data     science stack (NumPy/Pandas/Scikit Learn).  Additionally we'll talk about the  challenges of effective task scheduling in a data science context (data        locality, resilience, load balancing) and how we manage this dynamically with  aggressive measurement and dynamic scheduling heuristics.


Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:00,030 --> 00:00:21,750
there will be about five minutes for

00:00:01,979 --> 00:00:23,760
questions at the end so hold off uh so

00:00:21,750 --> 00:00:25,769
hi everyone my name is Matthew Rocklin I

00:00:23,760 --> 00:00:27,060
work for continuum analytics continuing

00:00:25,769 --> 00:00:29,010
with a company that's a for-profit

00:00:27,060 --> 00:00:31,500
company inside the open-source data

00:00:29,010 --> 00:00:32,520
science side of the Python ecosystem so

00:00:31,500 --> 00:00:34,590
they do you know consulting and training

00:00:32,520 --> 00:00:36,469
and such wheels apply for grants in

00:00:34,590 --> 00:00:39,059
those grants for open source software

00:00:36,469 --> 00:00:41,280
tasks is one such example that so I work

00:00:39,059 --> 00:00:43,140
in a project called task we live it for

00:00:41,280 --> 00:00:45,390
parallel and distributing computing in

00:00:43,140 --> 00:00:46,820
Python talk a little about that also

00:00:45,390 --> 00:00:49,950
generally about peril computing a Python

00:00:46,820 --> 00:00:52,530
we're doing so it's gonna be a

00:00:49,950 --> 00:00:53,699
combination of slides and of sort of

00:00:52,530 --> 00:00:55,829
live demonstrations if the wireless

00:00:53,699 --> 00:00:57,930
plays nicely with us the slides are

00:00:55,829 --> 00:01:01,859
available at my webpage at Matthew

00:00:57,930 --> 00:01:04,290
rockland comm slash slides slash PyCon -

00:01:01,859 --> 00:01:10,409
2017 if you want if I'm sort of boring I

00:01:04,290 --> 00:01:13,200
want to sort of jump ahead so I come

00:01:10,409 --> 00:01:14,850
from like the the numpy side of the

00:01:13,200 --> 00:01:16,200
pilot the PyCon stack super sort of

00:01:14,850 --> 00:01:19,470
raise your hand if you're more familiar

00:01:16,200 --> 00:01:22,259
with numpy than django if we're familiar

00:01:19,470 --> 00:01:23,759
with django then numpy yeah that's great

00:01:22,259 --> 00:01:26,250
it's like 5050 this is a crazy

00:01:23,759 --> 00:01:27,780
conference like so so Jake mentioned

00:01:26,250 --> 00:01:30,540
this in beginning of the keynote today

00:01:27,780 --> 00:01:33,299
which I loved that we are a mixed group

00:01:30,540 --> 00:01:35,100
I think would be a talk about mixing a

00:01:33,299 --> 00:01:36,420
couple of those sides together we're

00:01:35,100 --> 00:01:39,420
gonna take the sort of the networking

00:01:36,420 --> 00:01:41,220
side of Python and use it alongside the

00:01:39,420 --> 00:01:44,520
data science at Python to make a

00:01:41,220 --> 00:01:46,140
distributed computing system so we have

00:01:44,520 --> 00:01:47,939
this really strong sort of analytic

00:01:46,140 --> 00:01:49,310
system in Python so I was like numpy and

00:01:47,939 --> 00:01:52,460
pandas and scikit-learn

00:01:49,310 --> 00:01:54,479
they're very fast they're very intuitive

00:01:52,460 --> 00:01:56,369
scientists like them and let's like them

00:01:54,479 --> 00:01:58,829
they have a flaw which that they were

00:01:56,369 --> 00:02:01,320
designed to run on a single core mostly

00:01:58,829 --> 00:02:03,149
andand out of the Fitz and RAM as that

00:02:01,320 --> 00:02:04,439
is becoming less and as we're sitting at

00:02:03,149 --> 00:02:06,990
large in large data sets margin large

00:02:04,439 --> 00:02:08,479
clusters we saw we now want to expand

00:02:06,990 --> 00:02:11,920
those libraries the whole ecosystem

00:02:08,479 --> 00:02:16,390
beyond just single core processing

00:02:11,920 --> 00:02:17,830
that and that's hard to do so I'm just

00:02:16,390 --> 00:02:20,170
gonna paralyze a single library we want

00:02:17,830 --> 00:02:21,550
to paralyze an ecosystem of thousands

00:02:20,170 --> 00:02:25,270
and thousands of packages so I'm going

00:02:21,550 --> 00:02:26,770
to steal some slides from Jake so these

00:02:25,270 --> 00:02:28,810
are slides Jake but up in the keynote

00:02:26,770 --> 00:02:31,150
this morning the Python scientific stack

00:02:28,810 --> 00:02:32,380
and there's Python at the core there's

00:02:31,150 --> 00:02:34,960
some core libraries on top of that like

00:02:32,380 --> 00:02:36,970
numpy on top of that we things like like

00:02:34,960 --> 00:02:38,920
pandas and Syfy and all these other sort

00:02:36,970 --> 00:02:40,660
of slightly more peripheral packages as

00:02:38,920 --> 00:02:42,910
you go out there's actually that was in

00:02:40,660 --> 00:02:45,580
the little small packages packages for

00:02:42,910 --> 00:02:47,560
genomics and for people doing you know

00:02:45,580 --> 00:02:50,230
sequencing and people doing studying the

00:02:47,560 --> 00:02:51,790
Sun there's do thousands a little small

00:02:50,230 --> 00:02:54,040
research groups that are all producing

00:02:51,790 --> 00:02:56,470
software using their expertise that they

00:02:54,040 --> 00:02:58,150
have alone to support lots of scientists

00:02:56,470 --> 00:02:59,860
these are people doing real work helping

00:02:58,150 --> 00:03:01,600
the world and we want to sort paralyse

00:02:59,860 --> 00:03:03,220
not just numb pyar pandas but everything

00:03:01,600 --> 00:03:04,360
that's that's that's a big challenge

00:03:03,220 --> 00:03:07,270
that's what of the challenge we're sort

00:03:04,360 --> 00:03:09,700
of faced with and are foolishly trying

00:03:07,270 --> 00:03:11,230
to try to solve these packages are also

00:03:09,700 --> 00:03:12,460
they have custom algorithms it's not

00:03:11,230 --> 00:03:13,810
just one kind of thing they'll have

00:03:12,460 --> 00:03:17,680
their own secret sauce they're made by

00:03:13,810 --> 00:03:20,710
smart people so we knew parallel

00:03:17,680 --> 00:03:22,870
computing library that can be flexible

00:03:20,710 --> 00:03:25,240
enough to handle both the you know solar

00:03:22,870 --> 00:03:28,030
astronomers work and the Janome assists

00:03:25,240 --> 00:03:29,430
work it is familiar enough that it can

00:03:28,030 --> 00:03:35,500
be adopted by a large number of people

00:03:29,430 --> 00:03:36,580
and this is again a challenge so in

00:03:35,500 --> 00:03:37,960
order to try to solve this problem I'm

00:03:36,580 --> 00:03:39,130
talk about tasks something that I and

00:03:37,960 --> 00:03:42,280
others have worked on for the last few

00:03:39,130 --> 00:03:44,020
years and I hope you enjoy so this talk

00:03:42,280 --> 00:03:46,090
will be sort of where to start from sort

00:03:44,020 --> 00:03:47,650
of the more numpy pandas east side so go

00:03:46,090 --> 00:03:49,300
over more numpy pandas we sort of

00:03:47,650 --> 00:03:50,860
happier more comfortable beginning I'm

00:03:49,300 --> 00:03:52,900
going to end up more towards the sort of

00:03:50,860 --> 00:03:54,820
tornado networking concurrency side

00:03:52,900 --> 00:03:56,920
towards the end so hopefully is

00:03:54,820 --> 00:03:58,240
something for everyone we're talking

00:03:56,920 --> 00:04:00,070
about first paralyzing numpy and pandas

00:03:58,240 --> 00:04:02,830
which is sort of the first intent of

00:04:00,070 --> 00:04:03,940
tasks never talk about paralyzing more

00:04:02,830 --> 00:04:05,500
general code which ends up being

00:04:03,940 --> 00:04:08,140
necessary want to paralyze lots of

00:04:05,500 --> 00:04:10,120
packages will talk about task scheduling

00:04:08,140 --> 00:04:11,560
and tasks graphs and sort of compared

00:04:10,120 --> 00:04:13,030
with other computing systems that can do

00:04:11,560 --> 00:04:14,080
this sort of thing things like spark or

00:04:13,030 --> 00:04:15,580
airflow

00:04:14,080 --> 00:04:18,010
they'll talk more about how task solves

00:04:15,580 --> 00:04:20,890
this problem we'll finish up with some

00:04:18,010 --> 00:04:22,780
sort of general talking about Python API

00:04:20,890 --> 00:04:25,420
is in protocol that is useful to to

00:04:22,780 --> 00:04:27,070
adhere to or to gain better adoption

00:04:25,420 --> 00:04:29,410
and I'm will sort of finish with final

00:04:27,070 --> 00:04:31,780
thoughts so first we're going to

00:04:29,410 --> 00:04:35,410
paralyze numpy and pandas who has used

00:04:31,780 --> 00:04:38,740
numpy or pandas who used numpy are

00:04:35,410 --> 00:04:40,330
pandas and last three days okay so

00:04:38,740 --> 00:04:45,220
there's there some familiarity I'm glad

00:04:40,330 --> 00:04:47,650
to see them so talk about two sub

00:04:45,220 --> 00:04:50,130
modules of desk so there's a desk array

00:04:47,650 --> 00:04:52,720
which is a multi-dimensional array

00:04:50,130 --> 00:04:54,760
composed of many small numpy rays let's

00:04:52,720 --> 00:04:56,770
say I say I'm looking at climate data I

00:04:54,760 --> 00:04:58,660
have the temperature of the earth every

00:04:56,770 --> 00:05:00,580
you know square mile for the entire

00:04:58,660 --> 00:05:03,010
earth that might be a very large array

00:05:00,580 --> 00:05:05,140
I'm going to be able to put that array

00:05:03,010 --> 00:05:06,430
inside of one computer it's not going to

00:05:05,140 --> 00:05:07,450
block that array up into many different

00:05:06,430 --> 00:05:11,380
pieces and put them on different

00:05:07,450 --> 00:05:13,540
computers or on on my hard drive and ask

00:05:11,380 --> 00:05:16,300
array is going to logically coordinate

00:05:13,540 --> 00:05:17,860
all those no pirates so - gray is a

00:05:16,300 --> 00:05:19,900
coordinate is a collection of any numpy

00:05:17,860 --> 00:05:21,910
rays and a screa is going to when you

00:05:19,900 --> 00:05:22,990
type in you know - grey dots um it's

00:05:21,910 --> 00:05:24,910
going to figure out how to compute that

00:05:22,990 --> 00:05:26,100
sum by doing all this coordination on

00:05:24,910 --> 00:05:28,750
the Pyrates

00:05:26,100 --> 00:05:30,760
similarly others projects like task data

00:05:28,750 --> 00:05:33,070
frame which is a logical connection of

00:05:30,760 --> 00:05:34,390
many pandas data frames so for example

00:05:33,070 --> 00:05:36,190
we might have you know a lot of you

00:05:34,390 --> 00:05:37,720
might have a time series and the data

00:05:36,190 --> 00:05:39,550
for every month is quite large that can

00:05:37,720 --> 00:05:41,650
fit maybe on one computer we need to use

00:05:39,550 --> 00:05:43,630
many computers or our entire data set we

00:05:41,650 --> 00:05:46,810
need to use our hard drive but it's for

00:05:43,630 --> 00:05:49,420
many many many months so what data from

00:05:46,810 --> 00:05:51,550
dasker I provided they provide an

00:05:49,420 --> 00:05:53,650
interface that is very very similar to

00:05:51,550 --> 00:05:55,810
numpy and pandas but operates in

00:05:53,650 --> 00:05:58,150
parallel either on your laptop with many

00:05:55,810 --> 00:05:59,550
cores scaling out on your disk or across

00:05:58,150 --> 00:06:01,330
the cluster using many computers

00:05:59,550 --> 00:06:02,950
additionally because they're using numpy

00:06:01,330 --> 00:06:05,170
and pandas under the hood a lot of

00:06:02,950 --> 00:06:06,640
things work very easily with them so

00:06:05,170 --> 00:06:09,640
it's a very sort of lightweight change

00:06:06,640 --> 00:06:10,360
to your code news so I'm just switched a

00:06:09,640 --> 00:06:12,100
couple of examples

00:06:10,360 --> 00:06:16,150
hopefully the wireless is working a bit

00:06:12,100 --> 00:06:18,100
better today than yesterday so I have

00:06:16,150 --> 00:06:21,840
here a cluster running on on Google

00:06:18,100 --> 00:06:24,990
Golden Gate in them and this cluster has

00:06:21,840 --> 00:06:27,820
32 machines each machine has two cores I

00:06:24,990 --> 00:06:30,480
went to create an array let's make that

00:06:27,820 --> 00:06:30,480
a little bit larger

00:06:31,919 --> 00:06:35,189
I'm going to create array which is a

00:06:33,060 --> 00:06:37,259
thousand by thousand or 10,000 by 10,000

00:06:35,189 --> 00:06:38,849
but composed of numpy arrays or two

00:06:37,259 --> 00:06:41,819
thousand five thousand so sort of a 10

00:06:38,849 --> 00:06:43,080
by 10 grid of numpy raids each element

00:06:41,819 --> 00:06:46,349
of that greater than umpire a thousand

00:06:43,080 --> 00:06:49,199
by thousand and now when I when I

00:06:46,349 --> 00:06:50,250
compute that on my own my cluster gas is

00:06:49,199 --> 00:06:51,539
gone ahead and has computed all those

00:06:50,250 --> 00:06:53,340
numpy rights for me we've seen that here

00:06:51,539 --> 00:06:54,870
on the left so we're gonna see these

00:06:53,340 --> 00:06:57,360
plots a fair amount throughout the talk

00:06:54,870 --> 00:06:59,669
so I have 64 cores and they're on this

00:06:57,360 --> 00:07:01,680
axis here so every line here is what a

00:06:59,669 --> 00:07:03,689
cores been doing over time and we see

00:07:01,680 --> 00:07:05,879
that you know this core called the numpy

00:07:03,689 --> 00:07:08,789
random function a couple of times and

00:07:05,879 --> 00:07:10,620
created a couple numpy race so on my 32

00:07:08,789 --> 00:07:12,710
machines I have a hundred numpy raised

00:07:10,620 --> 00:07:15,180
in memory on them spread around them I

00:07:12,710 --> 00:07:17,400
do something let's say I'll compute you

00:07:15,180 --> 00:07:19,819
know the sum of this array it's really

00:07:17,400 --> 00:07:21,569
simple for the simple computation

00:07:19,819 --> 00:07:23,370
dousing Oh has compute the sum of all

00:07:21,569 --> 00:07:25,139
the intermediate arrays is then going to

00:07:23,370 --> 00:07:26,039
transfer some of the intermediate values

00:07:25,139 --> 00:07:28,409
to other machines

00:07:26,039 --> 00:07:29,939
that's the read read as data transfer

00:07:28,409 --> 00:07:31,469
and it's going to compute you know some

00:07:29,939 --> 00:07:34,039
final result and it gives us back a nice

00:07:31,469 --> 00:07:36,650
answer you know in a sort of fast time

00:07:34,039 --> 00:07:38,789
something more complex

00:07:36,650 --> 00:07:43,650
try to scroll this down to Devon the

00:07:38,789 --> 00:07:45,089
back gives me more complex you know it's

00:07:43,650 --> 00:07:49,190
if you sort of are familiar with numpy

00:07:45,089 --> 00:07:49,190
this this syntax should look familiar to

00:07:49,219 --> 00:07:53,339
and so we can we can do a lot of these

00:07:51,719 --> 00:07:54,719
note by computations what look like

00:07:53,339 --> 00:07:57,089
numpy computations we're actually spread

00:07:54,719 --> 00:07:58,589
across a cluster so this is sort of sort

00:07:57,089 --> 00:08:00,389
of the highest level use of tasks

00:07:58,589 --> 00:08:02,389
it looks like numpy but actually runs on

00:08:00,389 --> 00:08:04,319
you know terabytes of data if you wish

00:08:02,389 --> 00:08:06,719
certainly will consider you know a

00:08:04,319 --> 00:08:12,029
panda's data frame example so here i

00:08:06,719 --> 00:08:14,189
have a bunch of csv files on Google

00:08:12,029 --> 00:08:17,219
storage it's going to go on s3 or on

00:08:14,189 --> 00:08:19,199
Amazon or as you like and is the New

00:08:17,219 --> 00:08:21,120
York City Taxi Cab data set it's around

00:08:19,199 --> 00:08:22,680
20 gigs on disk around 60 gigs in RAM so

00:08:21,120 --> 00:08:24,509
it's too big for my laptop but I can

00:08:22,680 --> 00:08:26,430
read a little bit of it with pandas and

00:08:24,509 --> 00:08:28,979
this shows all of the cab rides in city

00:08:26,430 --> 00:08:32,190
of New York the year 2015 so how many

00:08:28,979 --> 00:08:33,599
passengers are in the cab when did they

00:08:32,190 --> 00:08:35,490
take off when do they when they just

00:08:33,599 --> 00:08:37,829
swap as well as you know a breakdown of

00:08:35,490 --> 00:08:39,539
the fare so it's a large time series

00:08:37,829 --> 00:08:41,069
data set maybe like you sort of seen

00:08:39,539 --> 00:08:42,899
before it sort of convenient because

00:08:41,069 --> 00:08:45,810
it's it's easy to understand and it's

00:08:42,899 --> 00:08:48,990
infinitely large so get me - all in one

00:08:45,810 --> 00:08:51,360
Sheen but I can I can read on many

00:08:48,990 --> 00:08:53,400
machines so here about and use the the

00:08:51,360 --> 00:08:55,260
pandas read PSP function I've used the

00:08:53,400 --> 00:08:58,110
data frame you see function has the same

00:08:55,260 --> 00:09:00,000
API but breaks down my computation takes

00:08:58,110 --> 00:09:02,070
those twelve CSV files and breaks it

00:09:00,000 --> 00:09:03,630
into hundreds of blocks of bytes then

00:09:02,070 --> 00:09:06,029
calls the pandas read CSV function all

00:09:03,630 --> 00:09:08,160
those blocks of bytes and so you're

00:09:06,029 --> 00:09:10,589
seeing here on the left is all the

00:09:08,160 --> 00:09:13,350
machines in my cluster are busy creating

00:09:10,589 --> 00:09:17,010
pandas data frames in there in the local

00:09:13,350 --> 00:09:18,480
memory the algebra get back isn't a

00:09:17,010 --> 00:09:20,760
panda's data frame at the desk data

00:09:18,480 --> 00:09:22,620
frame which looks and feels the same as

00:09:20,760 --> 00:09:23,760
we operate on that data frame the ask

00:09:22,620 --> 00:09:24,870
will do the coordination to make sure

00:09:23,760 --> 00:09:25,500
all the pandas data frames are affected

00:09:24,870 --> 00:09:27,000
accordingly

00:09:25,500 --> 00:09:28,950
so again task is sort of like a very

00:09:27,000 --> 00:09:31,020
good secretary actually doing any

00:09:28,950 --> 00:09:32,130
computation it's coordinating many

00:09:31,020 --> 00:09:36,240
pandas data frames to do the right

00:09:32,130 --> 00:09:37,560
computations so you know this because

00:09:36,240 --> 00:09:39,060
it's all built on pandas it looks

00:09:37,560 --> 00:09:41,370
familiar it did all the d-type sniffing

00:09:39,060 --> 00:09:43,860
we like from pandas we TSV and at you

00:09:41,370 --> 00:09:45,930
know it it runs the same way it sort of

00:09:43,860 --> 00:09:48,330
renders the same way I should feel

00:09:45,930 --> 00:09:49,529
relatively familiar and this is really

00:09:48,330 --> 00:09:51,030
important if you want to get all of the

00:09:49,529 --> 00:09:54,570
people in the Python ecosystem to use it

00:09:51,030 --> 00:09:55,770
they tend to like pandas it turns out so

00:09:54,570 --> 00:09:58,550
you know a simple computation we might

00:09:55,770 --> 00:10:00,240
repeat the length of that data frame and

00:09:58,550 --> 00:10:02,550
that's you know that's done really

00:10:00,240 --> 00:10:06,330
simply we could be the lengths all the

00:10:02,550 --> 00:10:08,930
intermediate values so if you sort of

00:10:06,330 --> 00:10:08,930
zoom in over here

00:10:11,560 --> 00:10:15,519
we compute the length of this particular

00:10:13,029 --> 00:10:17,680
panda's data frame and it took you know

00:10:15,519 --> 00:10:20,199
a few microseconds and we did that a few

00:10:17,680 --> 00:10:21,310
hundred times and we communicated some

00:10:20,199 --> 00:10:23,740
data over to one machine and the

00:10:21,310 --> 00:10:25,180
computer calls me to this um this is a

00:10:23,740 --> 00:10:27,370
very simple computation but we could do

00:10:25,180 --> 00:10:29,829
the more complex here we'll see how well

00:10:27,370 --> 00:10:31,540
New Yorkers tip so we're gonna remove

00:10:29,829 --> 00:10:33,059
some bad rows there were like some free

00:10:31,540 --> 00:10:35,589
rides in New York City it turns out

00:10:33,059 --> 00:10:37,149
we're going to create a new column into

00:10:35,589 --> 00:10:39,459
the tip fraction so you know the

00:10:37,149 --> 00:10:42,550
division of the tip versus affair with a

00:10:39,459 --> 00:10:43,809
10% tip or 20% tip we're in a group by

00:10:42,550 --> 00:10:45,309
the hour of the day and by the day of

00:10:43,809 --> 00:10:47,790
the week I receive the average the tip

00:10:45,309 --> 00:10:51,009
fraction so how well did New Yorkers tip

00:10:47,790 --> 00:10:53,170
grouped by hour of day and what this

00:10:51,009 --> 00:10:54,519
does this produces the thousands of

00:10:53,170 --> 00:10:58,120
Python functions that then ran on our

00:10:54,519 --> 00:11:01,509
cluster and it gives us about a nice

00:10:58,120 --> 00:11:03,910
result so you know tasks data frame

00:11:01,509 --> 00:11:05,170
turned this pandas like computation into

00:11:03,910 --> 00:11:06,999
thousands of Python function that had to

00:11:05,170 --> 00:11:09,220
run all of our planets outer frames and

00:11:06,999 --> 00:11:11,410
then the daffy task scheduler ran all

00:11:09,220 --> 00:11:14,230
those functions for us in about like

00:11:11,410 --> 00:11:18,069
three or four seconds about the nice

00:11:14,230 --> 00:11:20,949
results that New Yorkers tip relatively

00:11:18,069 --> 00:11:23,589
generously as a recent migrants in New

00:11:20,949 --> 00:11:26,079
York I feel now proud of this it's about

00:11:23,589 --> 00:11:29,709
twenty two percent on average with a

00:11:26,079 --> 00:11:32,350
startling spike at 4:00 a.m. if it's

00:11:29,709 --> 00:11:35,709
thirty eight percent on average tips at

00:11:32,350 --> 00:11:37,959
4:00 a.m. I understand this is last call

00:11:35,709 --> 00:11:40,089
the virus but so you know we did some

00:11:37,959 --> 00:11:42,970
data science on a large ink media data

00:11:40,089 --> 00:11:44,439
set with api's that we already knew on a

00:11:42,970 --> 00:11:46,509
cluster that was you know relatively

00:11:44,439 --> 00:11:48,730
easy to set up so this is sort of like

00:11:46,509 --> 00:11:51,490
the first big hurrah of desk staff gives

00:11:48,730 --> 00:11:53,170
you a paralyzed or distributed numpy

00:11:51,490 --> 00:11:56,730
rare Pettus data frame and for some

00:11:53,170 --> 00:12:00,490
people that is that is that is useful

00:11:56,730 --> 00:12:02,019
however it turns out so we thought this

00:12:00,490 --> 00:12:03,699
was useful too and then we'd like took

00:12:02,019 --> 00:12:04,899
it to academic groups and to companies

00:12:03,699 --> 00:12:07,149
and we said hey isn't this review useful

00:12:04,899 --> 00:12:09,249
I said yeah but our problems are still

00:12:07,149 --> 00:12:11,559
more complex sort of but not all

00:12:09,249 --> 00:12:14,679
problems are a big data frame or a big

00:12:11,559 --> 00:12:18,279
array or they list instead people often

00:12:14,679 --> 00:12:21,100
had code look like this it's are just

00:12:18,279 --> 00:12:24,170
normal Python code with four loops this

00:12:21,100 --> 00:12:26,480
is very clearly paralyzed it'll

00:12:24,170 --> 00:12:28,700
but it's not clearly a data frame and

00:12:26,480 --> 00:12:30,139
rate computation and so then our first

00:12:28,700 --> 00:12:31,760
approach was to sort of try to force

00:12:30,139 --> 00:12:33,470
this into being a data frame competition

00:12:31,760 --> 00:12:34,730
maybe a sort of two data frames and

00:12:33,470 --> 00:12:37,310
we're joining them and doing sort of the

00:12:34,730 --> 00:12:39,709
filter that was an awkward process so

00:12:37,310 --> 00:12:42,500
instead we developed other libraries or

00:12:39,709 --> 00:12:44,870
other modules of tasks that can work on

00:12:42,500 --> 00:12:47,449
more generic code so the system

00:12:44,870 --> 00:12:49,430
underneath tasks array or data frame

00:12:47,449 --> 00:12:51,050
that's doing all the coordination the

00:12:49,430 --> 00:12:52,940
secretary part that is actually fairly

00:12:51,050 --> 00:12:56,060
flexible and as long as we can expose

00:12:52,940 --> 00:12:58,639
that with is more flexible more

00:12:56,060 --> 00:13:01,490
fine-grained ap is we can paralyze far

00:12:58,639 --> 00:13:03,320
more clever and far more custom code so

00:13:01,490 --> 00:13:05,000
this actually ends up being far more

00:13:03,320 --> 00:13:06,470
used in practice in the big data grams

00:13:05,000 --> 00:13:11,810
of big arrays at least by sort of larger

00:13:06,470 --> 00:13:12,829
institutions so let's see an example I'm

00:13:11,810 --> 00:13:14,420
actually going to switch not to the

00:13:12,829 --> 00:13:15,800
cluster but it's my local machine this

00:13:14,420 --> 00:13:17,389
runs on a cluster just fine but I want

00:13:15,800 --> 00:13:19,670
to show you that it is easy to use tasks

00:13:17,389 --> 00:13:22,010
on your laptop my goal at the end of the

00:13:19,670 --> 00:13:24,699
this talk is rule to use tasks on their

00:13:22,010 --> 00:13:28,459
laptop answer to start playing with it

00:13:24,699 --> 00:13:29,750
so I'm going to instead of connecting

00:13:28,459 --> 00:13:31,820
out to a particular cluster

00:13:29,750 --> 00:13:33,199
I'm just connect so normally when we

00:13:31,820 --> 00:13:35,769
create a client we'll talk about in a

00:13:33,199 --> 00:13:37,790
little bit we put in some some address

00:13:35,769 --> 00:13:39,199
of sort of the head node of our

00:13:37,790 --> 00:13:41,240
scheduler in this case we're not going

00:13:39,199 --> 00:13:42,110
to add anything that's a signal to task

00:13:41,240 --> 00:13:44,839
that we should it should create

00:13:42,110 --> 00:13:47,329
something locally so that's going to

00:13:44,839 --> 00:13:50,600
create a sort of little mask cluster on

00:13:47,329 --> 00:13:54,130
our laptop next time sorry I don't want

00:13:50,600 --> 00:13:54,130
us notebook at all I want the other one

00:13:54,579 --> 00:14:01,519
so pretty but I'm just saying so I have

00:13:58,850 --> 00:14:03,019
some functions here that simulate work

00:14:01,519 --> 00:14:04,640
so they're going to do a small but it

00:14:03,019 --> 00:14:05,990
worked learn asleep for a while this is

00:14:04,640 --> 00:14:08,750
maybe a some code you're writing on your

00:14:05,990 --> 00:14:11,420
own ink as a number as one to a number

00:14:08,750 --> 00:14:12,440
decrement removes one from a number and

00:14:11,420 --> 00:14:16,959
add adds two numbers together

00:14:12,440 --> 00:14:16,959
I can call these locally

00:14:22,330 --> 00:14:25,390
I can call these locally and they take

00:14:23,920 --> 00:14:29,140
you know a couple of seconds probably

00:14:25,390 --> 00:14:32,500
sort of randomly or I can import desk

00:14:29,140 --> 00:14:34,570
and I can annotate those functions to be

00:14:32,500 --> 00:14:35,529
delayed that's what I mean here is that

00:14:34,570 --> 00:14:36,850
we're not going to when we call that

00:14:35,529 --> 00:14:38,980
function now we're not going to actually

00:14:36,850 --> 00:14:40,540
execute the code so we're going to put

00:14:38,980 --> 00:14:42,010
that function and its arguments inside

00:14:40,540 --> 00:14:45,399
of a task graph that we can execute

00:14:42,010 --> 00:14:48,040
later so now when I when I run that code

00:14:45,399 --> 00:14:49,930
same code from before it computes

00:14:48,040 --> 00:14:51,940
immediately but hasn't really done any

00:14:49,930 --> 00:14:56,230
work yet instead it produced this object

00:14:51,940 --> 00:14:57,399
Z which holds on to a recipe of how it

00:14:56,230 --> 00:15:01,180
should be computed whenever we want it

00:14:57,399 --> 00:15:03,970
to be done and so when I call compute on

00:15:01,180 --> 00:15:05,500
this object Z it's now going to run in

00:15:03,970 --> 00:15:07,420
parallel just on my laptop not on the

00:15:05,500 --> 00:15:08,950
cluster not only set up I haven't set

00:15:07,420 --> 00:15:11,050
anything up here I imported tasks

00:15:08,950 --> 00:15:12,670
I called compute by default this ran in

00:15:11,050 --> 00:15:15,279
a thread pool like running a process

00:15:12,670 --> 00:15:17,110
pool or other things as well but this is

00:15:15,279 --> 00:15:18,339
an interface that we can use to build up

00:15:17,110 --> 00:15:21,700
parallel computations in a sort of more

00:15:18,339 --> 00:15:23,470
pythonic way now I'm going to set up I'm

00:15:21,700 --> 00:15:28,839
start up a cluster I got on my local

00:15:23,470 --> 00:15:31,480
laptop and when I think that gives me

00:15:28,839 --> 00:15:33,700
this it gives me this nice dashboard

00:15:31,480 --> 00:15:36,450
page this is a bouquet dashboard okay

00:15:33,700 --> 00:15:41,500
web application people interested and

00:15:36,450 --> 00:15:44,860
what's been Sara bird there your thank

00:15:41,500 --> 00:15:47,520
you sir I never can run this on this

00:15:44,860 --> 00:15:50,500
dashboard we can see we can see it run

00:15:47,520 --> 00:15:52,390
so we call decrement on one of our

00:15:50,500 --> 00:15:54,850
threads we call increment on another

00:15:52,390 --> 00:15:56,290
thread we then this little red bit is

00:15:54,850 --> 00:15:56,730
communication between two different

00:15:56,290 --> 00:15:59,110
threads

00:15:56,730 --> 00:16:01,630
positives and we called add on to the

00:15:59,110 --> 00:16:03,250
results so TAS again is figuring out

00:16:01,630 --> 00:16:04,930
when to call things moving data around

00:16:03,250 --> 00:16:06,399
doing all the parts peril computing you

00:16:04,930 --> 00:16:07,630
don't want to think about all still

00:16:06,399 --> 00:16:11,140
letting you giving you the freedom to do

00:16:07,630 --> 00:16:12,490
the things that you do so this is sort

00:16:11,140 --> 00:16:13,600
of this neat it becomes lot better when

00:16:12,490 --> 00:16:15,910
you have four loops and you can make

00:16:13,600 --> 00:16:17,589
sort of arbitrary complex things so here

00:16:15,910 --> 00:16:19,029
I've got you know doing lots more

00:16:17,589 --> 00:16:20,920
computations and we're filling up these

00:16:19,029 --> 00:16:22,029
prophecies and doing lots of work I'm

00:16:20,920 --> 00:16:24,010
noticing from these progress bars is

00:16:22,029 --> 00:16:29,800
going to take a while so I'm going to

00:16:24,010 --> 00:16:30,860
start up some more workers pointing it

00:16:29,800 --> 00:16:33,790
to this

00:16:30,860 --> 00:16:33,790
scheduler

00:16:37,740 --> 00:16:41,010
this is you know making ten processes

00:16:39,600 --> 00:16:42,480
each will have four quart four four

00:16:41,010 --> 00:16:44,100
threads running those are connecting up

00:16:42,480 --> 00:16:45,870
to the scheduler and now we're seeing

00:16:44,100 --> 00:16:49,290
the tasks are responding to that is

00:16:45,870 --> 00:16:51,420
paralyzing nicely I could remove them -

00:16:49,290 --> 00:16:52,500
Gazala is resilient - can scale it can

00:16:51,420 --> 00:16:59,190
do all all the nice things you want a

00:16:52,500 --> 00:17:00,810
cluster to do so now that we have this

00:16:59,190 --> 00:17:04,800
ability let's do something a little more

00:17:00,810 --> 00:17:05,940
complex let's say all these numbers

00:17:04,800 --> 00:17:08,640
across all these firm prophecies on my

00:17:05,940 --> 00:17:10,290
laptop now when I add them together one

00:17:08,640 --> 00:17:11,700
way to do this is just to call some and

00:17:10,290 --> 00:17:13,080
all of them and they would be I have to

00:17:11,700 --> 00:17:15,210
migrate to one machine and add them

00:17:13,080 --> 00:17:16,740
together in one machine but it's do

00:17:15,210 --> 00:17:19,590
something more fancy let's add them up

00:17:16,740 --> 00:17:20,850
pair by pair so the first to add those

00:17:19,590 --> 00:17:23,490
together take a second to add those

00:17:20,850 --> 00:17:26,250
together and so here's some Python code

00:17:23,490 --> 00:17:28,380
that does that and this Python code

00:17:26,250 --> 00:17:30,720
isn't you know task code

00:17:28,380 --> 00:17:32,370
it's just Python code if you look at

00:17:30,720 --> 00:17:33,990
this for about a minute I think you

00:17:32,370 --> 00:17:35,820
probably understand what it's doing has

00:17:33,990 --> 00:17:38,450
a list of values and it goes through

00:17:35,820 --> 00:17:42,900
that list and it adds neighboring pairs

00:17:38,450 --> 00:17:45,330
in that list together and it keeps doing

00:17:42,900 --> 00:17:48,990
that while while the list is is longer

00:17:45,330 --> 00:17:51,750
than one and we actually visualize that

00:17:48,990 --> 00:17:54,630
results just expand a little bit for a

00:17:51,750 --> 00:17:55,680
moment maybe you can see that but the

00:17:54,630 --> 00:17:57,180
graph that we get this actually is

00:17:55,680 --> 00:18:00,120
showing us the competition that we want

00:17:57,180 --> 00:18:01,740
to compute you can see that it is indeed

00:18:00,120 --> 00:18:02,880
you know producing the kind of copy of

00:18:01,740 --> 00:18:07,560
you wanted to do actually kinda looks

00:18:02,880 --> 00:18:09,180
like this image up here and nothing has

00:18:07,560 --> 00:18:11,820
actually run yet we have just created a

00:18:09,180 --> 00:18:13,320
recipe for how to do these things if we

00:18:11,820 --> 00:18:15,960
go ahead and be we asked asked to

00:18:13,320 --> 00:18:17,700
compute that result for us it's going to

00:18:15,960 --> 00:18:19,470
use our cluster use all those processes

00:18:17,700 --> 00:18:21,060
to do that you see it's communicating

00:18:19,470 --> 00:18:23,040
between croissants upon season necessary

00:18:21,060 --> 00:18:24,210
in the beginning there's lots of work to

00:18:23,040 --> 00:18:25,890
do because right sort of the base of

00:18:24,210 --> 00:18:27,210
this tree and towards the end is the

00:18:25,890 --> 00:18:33,540
answer to fewer and fewer ads to do

00:18:27,210 --> 00:18:34,710
there's only one I see so if you like

00:18:33,540 --> 00:18:36,960
come to the continuum booth I'll give

00:18:34,710 --> 00:18:39,540
you this demo this demo is demonstrating

00:18:36,960 --> 00:18:41,220
two things one big data frames but two

00:18:39,540 --> 00:18:43,020
the ability to write your own Python

00:18:41,220 --> 00:18:47,060
code and paralyze it and that's being a

00:18:43,020 --> 00:18:47,060
lot more attractive a lot more pragmatic

00:18:47,180 --> 00:18:51,140
so let's go back to slides

00:18:52,520 --> 00:18:56,160
okay so let's let's dive a little bit

00:18:54,810 --> 00:18:57,720
deeper into what task was doing let's

00:18:56,160 --> 00:19:00,390
explain tasks graphs that explain

00:18:57,720 --> 00:19:04,320
explain task scheduling a little bit so

00:19:00,390 --> 00:19:06,210
that does two things for you one it

00:19:04,320 --> 00:19:08,370
produces tasks graphs so you write some

00:19:06,210 --> 00:19:09,750
data frame code and it figures out a

00:19:08,370 --> 00:19:14,580
recipe for how to actually execute that

00:19:09,750 --> 00:19:16,350
code to given such a graph it and a bug

00:19:14,580 --> 00:19:18,270
cluster or laptop it figures out how to

00:19:16,350 --> 00:19:20,040
execute that graph in parallel those are

00:19:18,270 --> 00:19:21,690
two very different polymers generally

00:19:20,040 --> 00:19:23,220
numpy pandas kind of people like the

00:19:21,690 --> 00:19:25,110
first kind of problem and sort of

00:19:23,220 --> 00:19:27,450
tornado async concurrency people like

00:19:25,110 --> 00:19:28,740
the second kind of problem but there's

00:19:27,450 --> 00:19:29,610
sort of two things happening ones we're

00:19:28,740 --> 00:19:33,300
talk about the first part and on the

00:19:29,610 --> 00:19:38,220
second part so tasks graphs is this font

00:19:33,300 --> 00:19:40,110
visible in the back yeah no someone's

00:19:38,220 --> 00:19:42,390
saying this does this mean it yes it is

00:19:40,110 --> 00:19:42,900
visible or make it bigger it's good

00:19:42,390 --> 00:19:45,300
excellent

00:19:42,900 --> 00:19:47,790
oh no I'm sitting some bad okay let's

00:19:45,300 --> 00:19:49,560
see what we can do reveal if not there

00:19:47,790 --> 00:19:51,330
we go a little bit better hopefully

00:19:49,560 --> 00:19:56,340
okay I think it's probably as good as I

00:19:51,330 --> 00:19:57,660
can go so I'm gonna make another very

00:19:56,340 --> 00:20:00,330
simple array computation and we're going

00:19:57,660 --> 00:20:02,430
to see how DAF is producing that way

00:20:00,330 --> 00:20:04,740
copying it into a task graph so I'm

00:20:02,430 --> 00:20:06,540
creating with numpy I might create an

00:20:04,740 --> 00:20:09,830
array of 15 ones with the numpy ones

00:20:06,540 --> 00:20:12,270
function it produces a result like that

00:20:09,830 --> 00:20:13,710
let's say that I only have like 20 bytes

00:20:12,270 --> 00:20:16,080
on my computer any this looks up between

00:20:13,710 --> 00:20:18,630
many computers then I might use the desk

00:20:16,080 --> 00:20:20,130
or a ones function so we're still

00:20:18,630 --> 00:20:21,870
creating arrays 15 ones where we're

00:20:20,130 --> 00:20:26,580
chopping up into three arrays each

00:20:21,870 --> 00:20:28,290
arrays of chunk size five so we're

00:20:26,580 --> 00:20:31,160
calling the lump i1 function three times

00:20:28,290 --> 00:20:33,210
and producing three and I'm Pyrates

00:20:31,160 --> 00:20:36,300
now if I were to call something like

00:20:33,210 --> 00:20:39,060
some on that array task we'll know I've

00:20:36,300 --> 00:20:41,430
got to call some of the numpy ones

00:20:39,060 --> 00:20:43,710
function arrays all those sums together

00:20:41,430 --> 00:20:45,390
and compute the sum of all of them sort

00:20:43,710 --> 00:20:48,900
of a very simple sort of Map Reduce II

00:20:45,390 --> 00:20:51,080
kind of computation so we're seeing

00:20:48,900 --> 00:20:54,840
numpy numpy like code produced a scraps

00:20:51,080 --> 00:20:57,360
now this one's more interesting if we go

00:20:54,840 --> 00:21:00,990
to two dimensions term creating a 15 by

00:20:57,360 --> 00:21:03,660
15 array that's composed of numpy rays

00:21:00,990 --> 00:21:05,640
of size 5 by 5 so I've got 9 blocks

00:21:03,660 --> 00:21:08,970
by some along one axis I guess or the

00:21:05,640 --> 00:21:09,870
same summing behavior visiting on

00:21:08,970 --> 00:21:11,460
something you can do with not produce

00:21:09,870 --> 00:21:13,620
with sparkles a sort of an easy

00:21:11,460 --> 00:21:15,210
competition to do numpy array

00:21:13,620 --> 00:21:16,470
computations actually come significally

00:21:15,210 --> 00:21:18,120
more complex pretty quickly

00:21:16,470 --> 00:21:20,400
Cermak taking that array I'm adding to

00:21:18,120 --> 00:21:21,990
it to its transpose and so you know

00:21:20,400 --> 00:21:23,700
numpy people will be interested to see

00:21:21,990 --> 00:21:25,650
that there's you know on the on diagonal

00:21:23,700 --> 00:21:27,570
nodes only topped in cells the off

00:21:25,650 --> 00:21:30,480
diagonal they try to talk to their their

00:21:27,570 --> 00:21:31,830
transpose neighbor and this this

00:21:30,480 --> 00:21:34,320
difference is actually where I'll get to

00:21:31,830 --> 00:21:36,900
this in a bit but is where old systems

00:21:34,320 --> 00:21:38,790
like or more databases systems like

00:21:36,900 --> 00:21:40,170
spark or storm I started to fall down

00:21:38,790 --> 00:21:42,300
when they sort of lose for losing this

00:21:40,170 --> 00:21:44,010
these symmetries that's when two being

00:21:42,300 --> 00:21:46,590
very good when you break down symmetries

00:21:44,010 --> 00:21:49,320
it handles generic cases single more

00:21:46,590 --> 00:21:51,210
complex sorted majors multiplied let's

00:21:49,320 --> 00:21:54,000
subtract off the mean let's take the

00:21:51,210 --> 00:21:55,530
standard deviation and we can keep going

00:21:54,000 --> 00:21:56,700
is actually a very very simple thing to

00:21:55,530 --> 00:21:58,800
do if you look at say the climate

00:21:56,700 --> 00:22:00,480
scientists they do things that are ten

00:21:58,800 --> 00:22:03,840
hundred times more complex in this they

00:22:00,480 --> 00:22:07,050
produce graphs with millions of nodes so

00:22:03,840 --> 00:22:16,850
we've made this graph and now let's go

00:22:07,050 --> 00:22:16,850
ahead and compute it so again

00:22:22,510 --> 00:22:25,950
that's not copy at all

00:22:30,780 --> 00:22:37,220
nope okay import da

00:22:54,580 --> 00:22:59,080
so this this object Y is still lazy

00:22:56,830 --> 00:23:02,860
you've never done any work yet and when

00:22:59,080 --> 00:23:05,139
I call why not compute of time that the

00:23:02,860 --> 00:23:06,669
answer is zero at a concentrated a bunch

00:23:05,139 --> 00:23:07,899
of stuff to it extended deviation not

00:23:06,669 --> 00:23:09,610
surprising that the answer is zero

00:23:07,899 --> 00:23:11,169
what's interesting is that it returned

00:23:09,610 --> 00:23:14,320
the answer in about 47 milliseconds and

00:23:11,169 --> 00:23:16,720
in that time four threads because I have

00:23:14,320 --> 00:23:19,179
four cords went through every circle in

00:23:16,720 --> 00:23:20,559
this task graph ran that function hold

00:23:19,179 --> 00:23:21,940
on to that result pass two other

00:23:20,559 --> 00:23:23,769
functions around those functions deleted

00:23:21,940 --> 00:23:25,149
the rule didn't have to do and proceeded

00:23:23,769 --> 00:23:26,350
to through that graph and so that's

00:23:25,149 --> 00:23:29,019
roughly what does have scheduler does

00:23:26,350 --> 00:23:30,580
that's to execute this graph and give us

00:23:29,019 --> 00:23:32,470
a number back I deal in a short amount

00:23:30,580 --> 00:23:34,029
of time you know as of supporting

00:23:32,470 --> 00:23:35,740
interactive people who are sitting at

00:23:34,029 --> 00:23:38,549
the computer typing at it waiting for

00:23:35,740 --> 00:23:44,460
result acts you want to be very fast

00:23:38,549 --> 00:23:44,460
so let's go

00:23:52,980 --> 00:24:00,660
great so systems like task array or data

00:23:57,330 --> 00:24:01,350
frame produce tasks graphs like this one

00:24:00,660 --> 00:24:03,360
there's actually from a secular

00:24:01,350 --> 00:24:05,160
computation and in a scheduler executes

00:24:03,360 --> 00:24:06,690
that graph in parallel so here's a trace

00:24:05,160 --> 00:24:07,830
of a single machine scheduler walking

00:24:06,690 --> 00:24:09,990
through that graph with four four

00:24:07,830 --> 00:24:12,480
threads it is really sort of switch from

00:24:09,990 --> 00:24:14,640
the numpy people to the sort of more you

00:24:12,480 --> 00:24:16,350
know tornado concurrency networking

00:24:14,640 --> 00:24:20,100
people how do we run these graphs

00:24:16,350 --> 00:24:21,690
efficiently so before we talk about how

00:24:20,100 --> 00:24:24,000
- does that only serve a brief summary

00:24:21,690 --> 00:24:26,550
of other options inside the sort of

00:24:24,000 --> 00:24:29,340
Python space sort of to motivate why we

00:24:26,550 --> 00:24:32,640
had sort of build our own thing so we're

00:24:29,340 --> 00:24:33,990
talking about a few few systems simple

00:24:32,640 --> 00:24:36,240
systems like multi processing or

00:24:33,990 --> 00:24:38,100
concurrent futures big data collections

00:24:36,240 --> 00:24:39,270
like spark or storm or flink or

00:24:38,100 --> 00:24:41,190
databases or tensorflow

00:24:39,270 --> 00:24:43,280
and then task schedulers like the weed

00:24:41,190 --> 00:24:49,290
or airflow

00:24:43,280 --> 00:24:51,990
so let's consider map who has used

00:24:49,290 --> 00:24:54,030
multi-processing before lots of people

00:24:51,990 --> 00:24:55,770
yeah almost everybody I'm going to claim

00:24:54,030 --> 00:24:59,340
who has not used multi-processing before

00:24:55,770 --> 00:25:02,490
sorry to pull you out okay five brave

00:24:59,340 --> 00:25:05,430
people and maybe some less free people

00:25:02,490 --> 00:25:06,900
that's okay so I take a function I have

00:25:05,430 --> 00:25:09,060
a bunch of data and apply the functional

00:25:06,900 --> 00:25:11,190
all that data moldy frosting is a very

00:25:09,060 --> 00:25:12,540
easy way to do this in parallel I think

00:25:11,190 --> 00:25:15,120
about the pros and cons multi-processing

00:25:12,540 --> 00:25:17,790
it's very easy to install it's in the

00:25:15,120 --> 00:25:19,530
send of library and use its rate API

00:25:17,790 --> 00:25:21,090
very simple it's also very lightweight

00:25:19,530 --> 00:25:23,010
dependency because in the standard

00:25:21,090 --> 00:25:24,060
library libraries don't mind depending

00:25:23,010 --> 00:25:26,220
on it remember that's one of our goals

00:25:24,060 --> 00:25:28,890
we want to build a library that many

00:25:26,220 --> 00:25:30,390
libraries in the ecosystem in all of

00:25:28,890 --> 00:25:32,190
these and all the thousands more are

00:25:30,390 --> 00:25:33,210
actually willing to depend upon are so

00:25:32,190 --> 00:25:40,230
willing to put it in the requirements

00:25:33,210 --> 00:25:41,670
that AFP file some cons there's some

00:25:40,230 --> 00:25:43,440
costs to move data across the process

00:25:41,670 --> 00:25:45,600
that is unfortunate

00:25:43,440 --> 00:25:47,160
and it's also not able to handle more

00:25:45,600 --> 00:25:49,020
complex computations so you know all

00:25:47,160 --> 00:25:52,230
those graphs that we had map can't

00:25:49,020 --> 00:25:54,420
easily do those so let's go to something

00:25:52,230 --> 00:25:56,070
more complex let's look at a big data

00:25:54,420 --> 00:25:59,760
collection things like spark or database

00:25:56,070 --> 00:26:01,710
so these systems are a big step up right

00:25:59,760 --> 00:26:04,040
they give you a fixed API things like

00:26:01,710 --> 00:26:06,750
map and filter and group by and join

00:26:04,040 --> 00:26:08,280
really use all of those operations

00:26:06,750 --> 00:26:10,500
just map and they will handle the

00:26:08,280 --> 00:26:12,150
parallelism for you as this allows you

00:26:10,500 --> 00:26:13,980
to paralyze a much broader set of

00:26:12,150 --> 00:26:15,930
applications particularly many of these

00:26:13,980 --> 00:26:18,810
were implemented force or ETL or sort of

00:26:15,930 --> 00:26:21,480
data extraction and cleaning or database

00:26:18,810 --> 00:26:23,430
comp computations or some sort of

00:26:21,480 --> 00:26:25,230
lightweight machine learning and so

00:26:23,430 --> 00:26:27,450
using the systems you can do all of

00:26:25,230 --> 00:26:29,010
those things so you're given a broader

00:26:27,450 --> 00:26:32,850
set of API to work with like the sequel

00:26:29,010 --> 00:26:35,250
language it scales nicely and it's

00:26:32,850 --> 00:26:36,630
widely trusted by enterprise so I'm

00:26:35,250 --> 00:26:38,130
going to sort of take off my open source

00:26:36,630 --> 00:26:41,910
community hat and put on my like

00:26:38,130 --> 00:26:43,320
for-profit evil hat I also like you know

00:26:41,910 --> 00:26:44,850
dass's open source and free and

00:26:43,320 --> 00:26:46,020
everything but it's nice to pay

00:26:44,850 --> 00:26:48,420
developers to do this and the more

00:26:46,020 --> 00:26:50,610
people we get using tasks and and we can

00:26:48,420 --> 00:26:51,900
like convince people to help us pay

00:26:50,610 --> 00:26:52,140
developers to work on it to make it

00:26:51,900 --> 00:26:53,850
better

00:26:52,140 --> 00:26:55,410
so I actually care about this a little

00:26:53,850 --> 00:27:00,870
bit I'm a little bit evil I apologize

00:26:55,410 --> 00:27:03,660
for that ok so some cons about these

00:27:00,870 --> 00:27:04,800
systems they're somewhat heavyweight so

00:27:03,660 --> 00:27:06,540
it'll be it'll be sort of hard to

00:27:04,800 --> 00:27:08,430
convince libraries to depend on SPARC

00:27:06,540 --> 00:27:10,740
there's sort of this big sort of

00:27:08,430 --> 00:27:12,120
transfer of you sort of have to step

00:27:10,740 --> 00:27:13,950
into the SPARC world for a bit using

00:27:12,120 --> 00:27:16,560
computations in this step out it is it

00:27:13,950 --> 00:27:18,360
is unpleasant for a few reasons it's

00:27:16,560 --> 00:27:20,190
focused on the JVM Python sort of always

00:27:18,360 --> 00:27:21,360
a second-class citizen this is true with

00:27:20,190 --> 00:27:23,820
actually most of the parallel computing

00:27:21,360 --> 00:27:25,350
libraries out there storm flank

00:27:23,820 --> 00:27:26,580
they were often they were built out of

00:27:25,350 --> 00:27:29,450
the JVM stack and we're sort of

00:27:26,580 --> 00:27:31,530
piggybacking on the success

00:27:29,450 --> 00:27:34,230
also he says we're not able to handle

00:27:31,530 --> 00:27:36,030
very complex computations now what do I

00:27:34,230 --> 00:27:38,690
mean by that I mean the graph that we

00:27:36,030 --> 00:27:43,020
saw before right these sorts of very

00:27:38,690 --> 00:27:45,180
these graphs without much symmetry right

00:27:43,020 --> 00:27:47,430
sort of arbitrary dynamic arbitrary

00:27:45,180 --> 00:27:49,260
tasks graphs where every every circling

00:27:47,430 --> 00:27:52,440
here is one Python function to run on

00:27:49,260 --> 00:27:54,360
one piece of data so other not able to

00:27:52,440 --> 00:27:55,860
handle these systems like sparkle

00:27:54,360 --> 00:27:57,690
databases tend be good at mapping a

00:27:55,860 --> 00:28:00,240
function across many things these are

00:27:57,690 --> 00:28:02,220
all tall shuffle communications reducing

00:28:00,240 --> 00:28:03,930
things but always sort of in lockstep

00:28:02,220 --> 00:28:05,130
it's not as sort of fine-grained or

00:28:03,930 --> 00:28:07,110
granules we need to do some these more

00:28:05,130 --> 00:28:08,490
complex computations they're going to

00:28:07,110 --> 00:28:12,510
need to handle inorder support all of

00:28:08,490 --> 00:28:15,690
these libraries however there are some

00:28:12,510 --> 00:28:16,950
libraries that do handle more messy

00:28:15,690 --> 00:28:18,030
computations these are systems actually

00:28:16,950 --> 00:28:19,540
commonly use this or the data

00:28:18,030 --> 00:28:21,700
engineering space

00:28:19,540 --> 00:28:23,320
libraries like airflo or Luigi or celery

00:28:21,700 --> 00:28:24,390
 room has used one of those

00:28:23,320 --> 00:28:26,530
libraries

00:28:24,390 --> 00:28:27,850
okay that's attached awesome this is

00:28:26,530 --> 00:28:30,090
that that is not the case in like the

00:28:27,850 --> 00:28:32,679
SyFy our PI data talks are conferences

00:28:30,090 --> 00:28:35,200
so these libraries are able to handle

00:28:32,679 --> 00:28:36,940
much more complex much more arbitrary

00:28:35,200 --> 00:28:38,290
tasks graphs they sort of fit the model

00:28:36,940 --> 00:28:40,809
that we need

00:28:38,290 --> 00:28:42,309
they're also Python native often so

00:28:40,809 --> 00:28:43,630
these are the libraries that we can hack

00:28:42,309 --> 00:28:46,620
on that our communities are willing to

00:28:43,630 --> 00:28:49,360
depend upon they're nicer in that sense

00:28:46,620 --> 00:28:51,570
however those are some comms so there's

00:28:49,360 --> 00:28:54,309
no inner worker storage or communication

00:28:51,570 --> 00:28:55,929
latency is relatively high so like 100

00:28:54,309 --> 00:28:57,610
milliseconds between tasks is an okay

00:28:55,929 --> 00:28:58,570
thing for these libraries it's not okay

00:28:57,610 --> 00:29:00,690
for us we work on sort of the

00:28:58,570 --> 00:29:02,890
millisecond to Hunter microsecond level

00:29:00,690 --> 00:29:05,110
they're just whenever whenever we're not

00:29:02,890 --> 00:29:06,220
optimized for computation I think says

00:29:05,110 --> 00:29:07,929
optimize for lots of things that task

00:29:06,220 --> 00:29:09,490
doesn't do I don't mean to sort of diss

00:29:07,929 --> 00:29:11,620
much either of these libraries both

00:29:09,490 --> 00:29:13,660
spark and Luigi and airflow do lots of

00:29:11,620 --> 00:29:15,670
things that task won't do but task is

00:29:13,660 --> 00:29:18,820
sort of good at this sort of mixing

00:29:15,670 --> 00:29:20,890
between them so we want a task scheduler

00:29:18,820 --> 00:29:22,630
like air flow and Luigi but it is more

00:29:20,890 --> 00:29:24,090
computationally focused like something

00:29:22,630 --> 00:29:26,740
like spark or flame core tensorflow

00:29:24,090 --> 00:29:29,070
and that ends up so our sort of attempts

00:29:26,740 --> 00:29:31,900
to build that system is called task

00:29:29,070 --> 00:29:34,150
which so we've seen that data frame and

00:29:31,900 --> 00:29:36,400
ask array and those are things built

00:29:34,150 --> 00:29:39,490
with tasks that is not desk task is a

00:29:36,400 --> 00:29:44,110
dynamic task scheduler at its core so it

00:29:39,490 --> 00:29:47,530
gets a graph of tasks such as you would

00:29:44,110 --> 00:29:49,450
give to Luigi in a complex case and it

00:29:47,530 --> 00:29:51,220
executes them on parallel hardware or

00:29:49,450 --> 00:29:54,550
where might be your laptop or might be a

00:29:51,220 --> 00:29:57,370
cluster we're relatively fast in various

00:29:54,550 --> 00:30:00,490
ways we're not as satisfied and most

00:29:57,370 --> 00:30:03,280
other things as also lightweight we'll

00:30:00,490 --> 00:30:05,350
see in a bit and it's well supported so

00:30:03,280 --> 00:30:07,120
a little bit about task schedulers there

00:30:05,350 --> 00:30:10,030
are two main tasks others within the

00:30:07,120 --> 00:30:11,710
task sort of world others one that was

00:30:10,030 --> 00:30:13,540
built many years ago for single machines

00:30:11,710 --> 00:30:16,540
this typically runs on top of a thread

00:30:13,540 --> 00:30:18,220
pool or local multiple testing pool and

00:30:16,540 --> 00:30:19,510
this runs with with very low overhead it

00:30:18,220 --> 00:30:22,000
actually only depends so it's not you

00:30:19,510 --> 00:30:25,150
know 50 microseconds per task overhead

00:30:22,000 --> 00:30:27,160
it's fast I can handle arbitrary graphs

00:30:25,150 --> 00:30:29,260
it's relatively concise about sort of a

00:30:27,160 --> 00:30:30,760
thousand lines of code it actually

00:30:29,260 --> 00:30:32,890
depends on nothing except for the

00:30:30,760 --> 00:30:34,150
standard library so very

00:30:32,890 --> 00:30:36,220
even very conservative libraries are

00:30:34,150 --> 00:30:38,320
willing to depend on this code it is

00:30:36,220 --> 00:30:39,640
very it's been stable I've been changed

00:30:38,320 --> 00:30:40,420
much in people I'm not when I'm change

00:30:39,640 --> 00:30:43,060
it's cut for a long time

00:30:40,420 --> 00:30:45,130
this is a stable code it is lightweight

00:30:43,060 --> 00:30:46,570
it is easy to use and it's used by many

00:30:45,130 --> 00:30:48,400
groups on many different applications

00:30:46,570 --> 00:30:50,260
the diversity of applications on these

00:30:48,400 --> 00:30:51,370
schedulers is very broad so it's likely

00:30:50,260 --> 00:30:52,780
that if you have different application

00:30:51,370 --> 00:30:54,970
it will also likely work they're not

00:30:52,780 --> 00:30:56,380
optimized for like array computing or

00:30:54,970 --> 00:31:01,120
for diagram computations they're

00:30:56,380 --> 00:31:02,050
optimized for general computing maybe

00:31:01,120 --> 00:31:03,430
like a year and a half ago we started

00:31:02,050 --> 00:31:04,780
loading attributed cluster scheduler

00:31:03,430 --> 00:31:08,230
which is more sophisticated but also the

00:31:04,780 --> 00:31:13,240
more heavyweight this is a tornado TCP

00:31:08,230 --> 00:31:15,190
application it's a left concise it's

00:31:13,240 --> 00:31:17,260
fully asynchronous so you can you can

00:31:15,190 --> 00:31:18,400
submit graphs to the thing in those even

00:31:17,260 --> 00:31:20,230
others running you can get sort of a

00:31:18,400 --> 00:31:22,840
constant conversation back and forth it

00:31:20,230 --> 00:31:24,280
supports sort of the HDFS space the

00:31:22,840 --> 00:31:28,360
Hadoop space and also supports the more

00:31:24,280 --> 00:31:29,560
sort of traditional cluster HPC space so

00:31:28,360 --> 00:31:30,520
we were seeing things run in a cluster

00:31:29,560 --> 00:31:34,330
we're running this to ship to the

00:31:30,520 --> 00:31:35,260
classes this district is scheduler how

00:31:34,330 --> 00:31:37,390
it is organized

00:31:35,260 --> 00:31:39,610
there is a single process running

00:31:37,390 --> 00:31:40,960
somewhere on your on your cluster which

00:31:39,610 --> 00:31:42,940
is the scheduler so we'll my call is

00:31:40,960 --> 00:31:45,100
like the master or the head node this

00:31:42,940 --> 00:31:46,650
this process is going to coordinate all

00:31:45,100 --> 00:31:49,570
of the other processes in your cluster

00:31:46,650 --> 00:31:51,730
there are then many workers that will

00:31:49,570 --> 00:31:53,080
take instructions from that scheduler so

00:31:51,730 --> 00:31:54,430
the schedule might say hey worker one I

00:31:53,080 --> 00:31:56,260
want you to compute this task

00:31:54,430 --> 00:31:57,790
the work will compute that task and hold

00:31:56,260 --> 00:32:00,190
on to the results and inform the

00:31:57,790 --> 00:32:01,690
schedule it is finished the workers will

00:32:00,190 --> 00:32:02,950
communicate peer to peer to share data

00:32:01,690 --> 00:32:05,440
around so they're not communicating

00:32:02,950 --> 00:32:07,810
through one central bottleneck and then

00:32:05,440 --> 00:32:09,310
we as a client down here from our you

00:32:07,810 --> 00:32:11,170
know maybe from our jupiter notebooks or

00:32:09,310 --> 00:32:13,420
from some script we're running you know

00:32:11,170 --> 00:32:16,180
nightly or whatever we're then going to

00:32:13,420 --> 00:32:18,160
submit graphs up to that scheduler so

00:32:16,180 --> 00:32:20,260
the workers and the schedulers are all

00:32:18,160 --> 00:32:22,810
tcp servers they're communicating over

00:32:20,260 --> 00:32:24,550
various interesting protocols and the

00:32:22,810 --> 00:32:29,560
client is just just a client up to the

00:32:24,550 --> 00:32:32,680
scheduler so the you can setup this up

00:32:29,560 --> 00:32:34,330
relatively easily you can salt on your

00:32:32,680 --> 00:32:36,070
laptop like I did before you can run on

00:32:34,330 --> 00:32:38,830
a cluster as well to be lightweight

00:32:36,070 --> 00:32:40,600
also so the bottom I'm actually just

00:32:38,830 --> 00:32:43,690
running it inside of one process and I

00:32:40,600 --> 00:32:44,920
can start this up including the fancy

00:32:43,690 --> 00:32:45,850
dashboard and everything around 43

00:32:44,920 --> 00:32:48,280
milliseconds

00:32:45,850 --> 00:32:49,510
so you can you can import this you can

00:32:48,280 --> 00:32:51,910
run ups tear it down

00:32:49,510 --> 00:32:53,320
and it's not a big thing it is cheap you

00:32:51,910 --> 00:32:55,780
should not think of distributing as

00:32:53,320 --> 00:32:58,720
being like a far away hard thing to do

00:32:55,780 --> 00:33:01,270
it is like like this is faster than

00:32:58,720 --> 00:33:03,070
importing pandas and importing pandas

00:33:01,270 --> 00:33:04,800
takes like 200 o seconds so you should

00:33:03,070 --> 00:33:07,030
think this is being a cheap thing to do

00:33:04,800 --> 00:33:08,200
you can also if you don't like Conda you

00:33:07,030 --> 00:33:17,040
can pin the stall bask everything is

00:33:08,200 --> 00:33:17,040
pure Python so I'm gonna go through

00:33:18,030 --> 00:33:22,690
inches of time I'm not going to go

00:33:19,870 --> 00:33:24,790
through this but this shows a sort of an

00:33:22,690 --> 00:33:27,760
example of how the scheduler might work

00:33:24,790 --> 00:33:29,680
in this example so that's been easy to

00:33:27,760 --> 00:33:31,240
use and adopt you actually already know

00:33:29,680 --> 00:33:34,420
a lot of the API is that - presents

00:33:31,240 --> 00:33:35,770
there's no single - API we largely steal

00:33:34,420 --> 00:33:37,420
the api's of other languages or other

00:33:35,770 --> 00:33:39,280
projects and you probably already have

00:33:37,420 --> 00:33:40,450
the dependencies involved installed you

00:33:39,280 --> 00:33:42,670
probably have tornado installs you

00:33:40,450 --> 00:33:44,620
probably have message pack installed if

00:33:42,670 --> 00:33:46,030
you want you know better frames you need

00:33:44,620 --> 00:33:49,860
pandas but you probably have installed

00:33:46,030 --> 00:33:52,660
if you want to anyway so in order to

00:33:49,860 --> 00:33:54,730
improve adoption in order to sort of

00:33:52,660 --> 00:33:58,270
reduce the amount of like creativity we

00:33:54,730 --> 00:34:01,030
had to have tasks largely uses existing

00:33:58,270 --> 00:34:03,340
pythonic api's so we support a lot of

00:34:01,030 --> 00:34:04,600
the numpy and pandas api's and protocols

00:34:03,340 --> 00:34:07,840
where they exist

00:34:04,600 --> 00:34:09,820
we support pep 3148 which is concurrent

00:34:07,840 --> 00:34:12,400
futures if you're there that library

00:34:09,820 --> 00:34:15,060
task supports that perfectly we also do

00:34:12,400 --> 00:34:17,740
async await - like concurrent async work

00:34:15,060 --> 00:34:19,570
it'll so use Java applet so usually take

00:34:17,740 --> 00:34:20,950
existing scikit-learn code and there's a

00:34:19,570 --> 00:34:23,020
way in job lid which is Sai kepner's

00:34:20,950 --> 00:34:24,520
parallelism library to hijack how it

00:34:23,020 --> 00:34:26,200
does parallelism as you can run your

00:34:24,520 --> 00:34:29,380
scikit-learn code on a cluster it may

00:34:26,200 --> 00:34:31,750
not work very well in all cases but but

00:34:29,380 --> 00:34:33,970
we where there is an existing protocol

00:34:31,750 --> 00:34:36,160
for parallel computing in Python we have

00:34:33,970 --> 00:34:38,740
usually implemented that so you probably

00:34:36,160 --> 00:34:41,280
already know how to use desk you

00:34:38,740 --> 00:34:43,660
probably also it's also very lightweight

00:34:41,280 --> 00:34:50,830
and sort of just you know using existing

00:34:43,660 --> 00:34:52,080
libraries let's look at yeah a little

00:34:50,830 --> 00:34:58,730
bit time

00:34:52,080 --> 00:34:58,730
so let's look at somebody's API

00:35:03,390 --> 00:35:07,080
so again we've seen you know if you know

00:35:05,490 --> 00:35:10,200
numpy you probably know how to use

00:35:07,080 --> 00:35:11,820
Tasker ray here's a example of

00:35:10,200 --> 00:35:12,870
scikit-learn this came from the

00:35:11,820 --> 00:35:16,920
disposition example with no scikit-learn

00:35:12,870 --> 00:35:18,750
docks and someone so we're making a

00:35:16,920 --> 00:35:20,850
pipeline we're going to do a cross valid

00:35:18,750 --> 00:35:30,000
search across that pipeline just normal

00:35:20,850 --> 00:35:32,190
scikit-learn code it takes around 8

00:35:30,000 --> 00:35:34,320
seconds or 9 seconds and now someone

00:35:32,190 --> 00:35:36,330
actually the name is Jim Crist a word

00:35:34,320 --> 00:35:38,390
elaborate ask search CV which is a

00:35:36,330 --> 00:35:41,730
drop-in replacement for a grid search

00:35:38,390 --> 00:35:43,680
that's now when you run that you can run

00:35:41,730 --> 00:35:44,700
that on our on our cluster and looks

00:35:43,680 --> 00:35:47,130
just the same we're using all the

00:35:44,700 --> 00:35:48,810
secular an API so it looks it fits into

00:35:47,130 --> 00:35:50,670
existing workflow as well

00:35:48,810 --> 00:35:51,660
it ran faster we also notice like

00:35:50,670 --> 00:35:53,640
there's a lot of white space here a lot

00:35:51,660 --> 00:35:55,020
of our workers weren't doing anything so

00:35:53,640 --> 00:35:58,470
we have a lot more computing power let's

00:35:55,020 --> 00:36:00,870
go ahead and increase this a little bit

00:35:58,470 --> 00:36:02,910
well increase our parameter search space

00:36:00,870 --> 00:36:04,290
if you don't know secular that's ok you

00:36:02,910 --> 00:36:06,240
should just understand I'm increasing

00:36:04,290 --> 00:36:09,270
inputs and we'll get more work and I

00:36:06,240 --> 00:36:11,250
won't be slow so now that we have more

00:36:09,270 --> 00:36:14,970
work more competing power or to search

00:36:11,250 --> 00:36:17,190
this space a little bit better and again

00:36:14,970 --> 00:36:18,660
you sort of notice I could learn this

00:36:17,190 --> 00:36:21,090
should have been pretty familiar to you

00:36:18,660 --> 00:36:22,500
so while the tasks work is just helping

00:36:21,090 --> 00:36:25,980
other libraries paralyze themselves

00:36:22,500 --> 00:36:29,160
using the same interfaces staff supports

00:36:25,980 --> 00:36:33,110
concurrent futures interface so here we

00:36:29,160 --> 00:36:35,520
can do do the executor not submit model

00:36:33,110 --> 00:36:37,320
it's actually fully asynchronous so like

00:36:35,520 --> 00:36:39,300
as work comes in you can submit more

00:36:37,320 --> 00:36:44,850
work in a fully real-time way which is

00:36:39,300 --> 00:36:46,290
fun we do things like async await so

00:36:44,850 --> 00:36:47,970
here's you know fully asynchronous thing

00:36:46,290 --> 00:36:50,160
you know all the sort of Python code

00:36:47,970 --> 00:36:51,630
that you've seen none of except for like

00:36:50,160 --> 00:36:53,700
the import that statement none of the

00:36:51,630 --> 00:36:56,670
code you've seen on this notebook you

00:36:53,700 --> 00:36:58,860
would you would qualify his task code it

00:36:56,670 --> 00:37:00,840
was all the kind of code you've seen

00:36:58,860 --> 00:37:02,130
before but it was all paralyzed in a

00:37:00,840 --> 00:37:04,850
nice way that's again sort of one

00:37:02,130 --> 00:37:04,850
objective some tasks

00:37:06,020 --> 00:37:13,700
okay three and a half gives you parallel

00:37:10,700 --> 00:37:15,740
api's parallel pandas 4l numpy Perla

00:37:13,700 --> 00:37:17,990
second learn subset of all those not

00:37:15,740 --> 00:37:21,440
complete so we're got a paralyzing

00:37:17,990 --> 00:37:23,810
existing custom systems at a low level

00:37:21,440 --> 00:37:25,760
tasks with a task scheduler which means

00:37:23,810 --> 00:37:28,220
that it runs Python functions on Python

00:37:25,760 --> 00:37:29,300
objects on parallel hardware will apply

00:37:28,220 --> 00:37:30,800
some functions are well the Python

00:37:29,300 --> 00:37:32,240
objects are is up to you it can be your

00:37:30,800 --> 00:37:34,010
own special object or on special

00:37:32,240 --> 00:37:36,980
functions definitely need to know what

00:37:34,010 --> 00:37:38,180
it is it can just run it and it will

00:37:36,980 --> 00:37:39,980
figure out where and when to run those

00:37:38,180 --> 00:37:41,510
functions the machine goes down or bring

00:37:39,980 --> 00:37:45,140
it back up all those sorts of nice

00:37:41,510 --> 00:37:47,180
problems and good things okay so I want

00:37:45,140 --> 00:37:52,130
to take a few minutes about sort of just

00:37:47,180 --> 00:37:54,710
general ecosystem comments so I'm

00:37:52,130 --> 00:37:56,540
normally a fairly critical or sort of

00:37:54,710 --> 00:37:58,430
like pessimistic person I apologize I'm

00:37:56,540 --> 00:38:00,890
gonna gush a little bit so I think that

00:37:58,430 --> 00:38:04,880
I think this is really the best place to

00:38:00,890 --> 00:38:06,830
build this kind of project we built ask

00:38:04,880 --> 00:38:08,330
way more easily than you would expect us

00:38:06,830 --> 00:38:09,320
to be able to that's because of all the

00:38:08,330 --> 00:38:13,280
work that's already happened in the

00:38:09,320 --> 00:38:14,780
ecosystem so let's look at the Python

00:38:13,280 --> 00:38:16,100
strengths and weaknesses from the sort

00:38:14,780 --> 00:38:18,980
of a parallel data analysis point of

00:38:16,100 --> 00:38:21,230
view so python is a very strong

00:38:18,980 --> 00:38:23,690
algorithmic tradition all the community

00:38:21,230 --> 00:38:26,750
who is like writing code with punch

00:38:23,690 --> 00:38:28,640
cards or in Fortran has moved with the

00:38:26,750 --> 00:38:29,870
Python right and they know how to do

00:38:28,640 --> 00:38:31,850
those things very very well they know

00:38:29,870 --> 00:38:34,820
algorithms very well they know they have

00:38:31,850 --> 00:38:36,110
PhDs in math or something a lot of

00:38:34,820 --> 00:38:38,510
battle-hardened Fortran code which ones

00:38:36,110 --> 00:38:41,870
very efficiently it uses the best you

00:38:38,510 --> 00:38:43,280
know sse2 instruction on your cpu we

00:38:41,870 --> 00:38:45,710
also have alongside of that a very

00:38:43,280 --> 00:38:47,480
strong networking currency stack and

00:38:45,710 --> 00:38:50,030
it's very sort of very rare to have both

00:38:47,480 --> 00:38:51,590
of those in the same language we also

00:38:50,030 --> 00:38:54,260
we're sort of very standard in teaching

00:38:51,590 --> 00:38:55,520
so the bicycle using Python weaknesses I

00:38:54,260 --> 00:38:57,890
think are not actually that true a

00:38:55,520 --> 00:39:00,260
people thing that Python is slow there's

00:38:57,890 --> 00:39:02,380
the Gil those are questions about

00:39:00,260 --> 00:39:10,490
packaging so a few comments about this

00:39:02,380 --> 00:39:12,350
so the numpy and pipe the see sort of

00:39:10,490 --> 00:39:15,230
numeric stack on Python is very very

00:39:12,350 --> 00:39:16,520
fast is running bare metal speeds so

00:39:15,230 --> 00:39:18,250
here I have a thousand five thousand

00:39:16,520 --> 00:39:20,210
array and I'm doing a matrix multiply

00:39:18,250 --> 00:39:21,830
about a billion

00:39:20,210 --> 00:39:23,450
multiplies and adds in that computation

00:39:21,830 --> 00:39:26,330
and we ran that and around 70

00:39:23,450 --> 00:39:29,030
milliseconds it's about 10 billion

00:39:26,330 --> 00:39:31,070
operations per second I think about my

00:39:29,030 --> 00:39:33,200
laptop and laptop only like has like a 2

00:39:31,070 --> 00:39:36,170
or 3 gigahertz processor so actually

00:39:33,200 --> 00:39:38,090
doing more ads than I have cycles in my

00:39:36,170 --> 00:39:39,530
in my system if you know the pod CPUs

00:39:38,090 --> 00:39:43,610
that doesn't make those as fries you but

00:39:39,530 --> 00:39:44,810
we're running a bare-metal speeds the

00:39:43,610 --> 00:39:46,580
Gil people often concern about

00:39:44,810 --> 00:39:47,750
parallelism and the Gil again it's a

00:39:46,580 --> 00:39:49,850
numeric stack this actually doesn't

00:39:47,750 --> 00:39:52,700
matter the Gil is not a problem if

00:39:49,850 --> 00:39:54,020
you're mostly running numeric code with

00:39:52,700 --> 00:39:57,320
libraries like numpy and pandas and

00:39:54,020 --> 00:39:59,410
scikit-learn so the Gil stops to Python

00:39:57,320 --> 00:40:01,580
threads from operating at the same time

00:39:59,410 --> 00:40:03,050
but or to python fries or a products

00:40:01,580 --> 00:40:05,000
using python code at the same time but

00:40:03,050 --> 00:40:06,710
our threads are just calling out to some

00:40:05,000 --> 00:40:09,380
c function and then they sort of wait

00:40:06,710 --> 00:40:10,880
until a function returns so we're not

00:40:09,380 --> 00:40:14,110
actually calling python code we're

00:40:10,880 --> 00:40:16,310
calling C or Fortran code so you can run

00:40:14,110 --> 00:40:17,720
Python with many threads and saturate

00:40:16,310 --> 00:40:21,290
your hardware very easily if using the

00:40:17,720 --> 00:40:24,380
umpire pandas or that sort of stack you

00:40:21,290 --> 00:40:26,210
should use threads freely unless you're

00:40:24,380 --> 00:40:29,510
doing you know pythonic string

00:40:26,210 --> 00:40:33,260
manipulation but use threads and it just

00:40:29,510 --> 00:40:34,670
general comment alongside that there's

00:40:33,260 --> 00:40:36,050
also this this is totally different

00:40:34,670 --> 00:40:37,700
community at the same time

00:40:36,050 --> 00:40:39,440
simultaneously building out a very

00:40:37,700 --> 00:40:41,390
strong and very intuitive concurrency

00:40:39,440 --> 00:40:43,190
networking stack so here's some code

00:40:41,390 --> 00:40:45,230
that you know such as you might see

00:40:43,190 --> 00:40:47,660
inside a side of desk that does a lot of

00:40:45,230 --> 00:40:50,960
concurrent things you might now write

00:40:47,660 --> 00:40:53,690
that what I think await oh and ask we

00:40:50,960 --> 00:40:54,800
use tornado this code looks simple it

00:40:53,690 --> 00:40:56,930
looks easy to understand and that is

00:40:54,800 --> 00:40:58,880
actually remarkable this would not have

00:40:56,930 --> 00:41:01,000
been the case ten to ten years ago at

00:40:58,880 --> 00:41:04,370
the same time it's also quite quite fast

00:41:01,000 --> 00:41:06,730
so this is from a blog post about UV

00:41:04,370 --> 00:41:08,840
loop see if it gets that link in there

00:41:06,730 --> 00:41:11,360
talking about you know various UV loops

00:41:08,840 --> 00:41:12,650
various event loops and that's Dexter

00:41:11,360 --> 00:41:15,050
running on time tornado which is way

00:41:12,650 --> 00:41:18,170
down here even that is running at 20,000

00:41:15,050 --> 00:41:21,290
TCP requests per second which is like

00:41:18,170 --> 00:41:23,750
totally fine for us so it's this needles

00:41:21,290 --> 00:41:25,040
are very focused on performance not at a

00:41:23,750 --> 00:41:28,610
numeric competing wit sort of way but in

00:41:25,040 --> 00:41:30,500
a concurrency sort of way so python is

00:41:28,610 --> 00:41:32,840
with an excellent numeric computing

00:41:30,500 --> 00:41:33,630
stack and an excellent concurrency a

00:41:32,840 --> 00:41:35,250
networking stack

00:41:33,630 --> 00:41:37,319
actually quite rare it's really a

00:41:35,250 --> 00:41:38,910
blessing that we had sort of two groups

00:41:37,319 --> 00:41:40,559
of people in the beginning and they're

00:41:38,910 --> 00:41:42,720
both in the same room together and it's

00:41:40,559 --> 00:41:46,230
because of that that desk was actually

00:41:42,720 --> 00:41:47,819
really easy to build so most of the work

00:41:46,230 --> 00:41:49,950
of building tasks was was built you know

00:41:47,819 --> 00:41:53,450
5 or 10 years ago by all of you

00:41:49,950 --> 00:41:56,220
so I appreciate appreciate your efforts

00:41:53,450 --> 00:41:59,220
and interest of time I'm going to stop

00:41:56,220 --> 00:42:00,720
but many people work on desk it's not

00:41:59,220 --> 00:42:01,710
just me

00:42:00,720 --> 00:42:04,259
there are various government

00:42:01,710 --> 00:42:04,740
organizations and nonprofits that fund

00:42:04,259 --> 00:42:08,279
tasks

00:42:04,740 --> 00:42:09,809
we like to thank them as well oh yeah

00:42:08,279 --> 00:42:12,269
just show engagement in the last 48

00:42:09,809 --> 00:42:14,160
hours these projects have mentioned or

00:42:12,269 --> 00:42:16,440
committed code using tasks it's not on

00:42:14,160 --> 00:42:27,569
github and search for desk so it sort of

00:42:16,440 --> 00:42:31,079
nice to know ok thank you we've got

00:42:27,569 --> 00:42:33,869
about five minutes for questions so if

00:42:31,079 --> 00:42:36,779
you could line up by the microphone sure

00:42:33,869 --> 00:42:38,910
there's the API there's not a few yet

00:42:36,779 --> 00:42:41,940
you guys on the Python device task is a

00:42:38,910 --> 00:42:45,029
Python project okay so let's say I have

00:42:41,940 --> 00:42:48,690
some codes that takes it's a wrap with

00:42:45,029 --> 00:42:51,240
swig and it takes in an umpire or a the

00:42:48,690 --> 00:42:52,859
numpy array could it also potentially

00:42:51,240 --> 00:42:55,349
operate on a desk array and still take

00:42:52,859 --> 00:42:58,440
advantage of the functionality or is it

00:42:55,349 --> 00:43:00,420
a completed ask array does not implement

00:42:58,440 --> 00:43:01,259
sort of the numpy low-level api

00:43:00,420 --> 00:43:03,240
interface okay

00:43:01,259 --> 00:43:05,160
however give a function because now

00:43:03,240 --> 00:43:06,180
squeeze compose many numpy arrays can be

00:43:05,160 --> 00:43:08,609
devised people to just apply your

00:43:06,180 --> 00:43:09,869
function on many on all of them it's

00:43:08,609 --> 00:43:14,690
depending on what your function does

00:43:09,869 --> 00:43:17,190
okay thank you thank you um in your

00:43:14,690 --> 00:43:21,119
demos you were acting as a single client

00:43:17,190 --> 00:43:23,849
to your cluster how does it behave if

00:43:21,119 --> 00:43:26,130
you have say multiple clients competing

00:43:23,849 --> 00:43:29,130
for the clusters resources and give a

00:43:26,130 --> 00:43:30,660
time yeah so one - cause we have

00:43:29,130 --> 00:43:33,809
multiple clients is working on the same

00:43:30,660 --> 00:43:35,900
data that - graphs or a merkel dag well

00:43:33,809 --> 00:43:38,819
you know what that means you'll be great

00:43:35,900 --> 00:43:40,470
so we can it can share nicely that being

00:43:38,819 --> 00:43:42,599
said most people who do multi client

00:43:40,470 --> 00:43:44,700
workloads end up a locating separate

00:43:42,599 --> 00:43:48,390
schedulers and workers

00:43:44,700 --> 00:43:50,880
you doesn't ask you any optimization of

00:43:48,390 --> 00:43:52,920
the task graph is that a thing that is a

00:43:50,880 --> 00:43:54,420
priority for the project right now yes

00:43:52,920 --> 00:43:56,310
so we do the kind of optimizations that

00:43:54,420 --> 00:43:57,210
are all linear in time so the kind of

00:43:56,310 --> 00:44:00,300
things you would see in a normal

00:43:57,210 --> 00:44:01,290
compiler but none of the complex ones so

00:44:00,300 --> 00:44:03,300
quickly we don't do any sort of like

00:44:01,290 --> 00:44:05,040
data frame optimizations like like

00:44:03,300 --> 00:44:06,390
ordering reordering because we don't

00:44:05,040 --> 00:44:08,339
have the tab scrap level we we sort of

00:44:06,390 --> 00:44:10,260
already lost information but loop fusion

00:44:08,339 --> 00:44:14,880
you know those sorts of operations we do

00:44:10,260 --> 00:44:17,670
nicely calling the ground have you

00:44:14,880 --> 00:44:20,220
looked into or do you do any sort of use

00:44:17,670 --> 00:44:24,810
of OpenCL or CUDA for even higher

00:44:20,220 --> 00:44:26,970
parallelization on the GPU no sput you

00:44:24,810 --> 00:44:28,470
can in your python function and then

00:44:26,970 --> 00:44:30,750
tasks can run that python function on

00:44:28,470 --> 00:44:32,400
various machines as GPUs so gasps run

00:44:30,750 --> 00:44:33,750
Python functions on Python objects

00:44:32,400 --> 00:44:36,060
well those Python functions to do is

00:44:33,750 --> 00:44:37,650
entirely up to the user you can annotate

00:44:36,060 --> 00:44:39,450
that certain workers in your graph have

00:44:37,650 --> 00:44:42,030
GPUs and ask will respect that and

00:44:39,450 --> 00:44:44,400
allocate accordingly but we don't

00:44:42,030 --> 00:44:44,760
actually like use CUDA to execute our

00:44:44,400 --> 00:44:48,089
parallelism

00:44:44,760 --> 00:44:50,490
okay thank you can the schedule take

00:44:48,089 --> 00:44:57,990
advantage of any data locality say this

00:44:50,490 --> 00:45:00,240
node has this set of files so yes in

00:44:57,990 --> 00:45:02,730
many in many ways yes is there any

00:45:00,240 --> 00:45:04,230
limitation to the the work that you can

00:45:02,730 --> 00:45:06,750
do inside of the functions that you're

00:45:04,230 --> 00:45:07,859
passing and if it's a custom function so

00:45:06,750 --> 00:45:10,079
I think I'm pleased is there any

00:45:07,859 --> 00:45:11,280
limitations in to what you can do inside

00:45:10,079 --> 00:45:13,740
of the function that gets passed into

00:45:11,280 --> 00:45:15,119
the distributed workload system your

00:45:13,740 --> 00:45:17,069
function must be serializable with Cloud

00:45:15,119 --> 00:45:19,410
pickle so you can have like locks or

00:45:17,069 --> 00:45:21,869
open files or passing into it your

00:45:19,410 --> 00:45:23,819
function should not mutate state that's

00:45:21,869 --> 00:45:27,569
given to it so we generally to be

00:45:23,819 --> 00:45:29,130
resilient we're sort of assume that you

00:45:27,569 --> 00:45:31,800
shouldn't call this exit but other than

00:45:29,130 --> 00:45:39,960
that you know defined like to ask we

00:45:31,800 --> 00:45:41,760
recover from matress fine how big can it

00:45:39,960 --> 00:45:43,160
scale up like what's the largest cluster

00:45:41,760 --> 00:45:45,030
you've run it on where it looks some

00:45:43,160 --> 00:45:47,490
massive workloads you've tried at least

00:45:45,030 --> 00:45:51,530
the largest cluster I've had my hands on

00:45:47,490 --> 00:45:51,530
as a thousand oddly windows machines

00:45:53,700 --> 00:45:56,610
the number having your head is that

00:45:55,020 --> 00:45:58,770
every task is at a 200 microseconds

00:45:56,610 --> 00:46:01,140
overhead inside the scheduler secret

00:45:58,770 --> 00:46:03,870
tasks take about one second each you can

00:46:01,140 --> 00:46:05,460
saturate around 5,000 cores sort of the

00:46:03,870 --> 00:46:07,200
background of loped number to have in

00:46:05,460 --> 00:46:08,940
your head but a thousand is

00:46:07,200 --> 00:46:10,500
pragmatically what I've seen I've

00:46:08,940 --> 00:46:12,510
actually seen someone try bigger and

00:46:10,500 --> 00:46:16,590
fail but I assume you get to 100,000

00:46:12,510 --> 00:46:17,790
we'd run into something hey get off your

00:46:16,590 --> 00:46:25,909
time

00:46:17,790 --> 00:46:25,909

YouTube URL: https://www.youtube.com/watch?v=RA_2qdipVng


