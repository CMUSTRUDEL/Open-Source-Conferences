Title: Amit Ramesh, Qui Nguyen   Building Stream Processing Applications   PyCon 2017
Publication date: 2017-05-21
Playlist: PyCon 2017
Description: 
	"Speakers: Amit Ramesh, Qui Nguyen

Do you have a stream of data that you would like to process in real time? There are many components with Python APIs that you can put together to build a stream processing application. We will go through some common design patterns, tradeoffs and available components / frameworks for designing such systems. We will solve an example problem during the presentation to make these points concrete. Much of what will be presented is based on experience gained from building production pipelines for the real-time processing of ad streams at Yelp. This talk will cover topics such as consistency, availability, idempotency, scalability, etc.

Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:33,289 --> 00:00:36,289
so

00:00:37,180 --> 00:00:44,920
I think we might kick off this final

00:00:39,460 --> 00:00:47,650
session of pike on 2017 our presenters

00:00:44,920 --> 00:00:50,739
today are from Yelp they are unknit

00:00:47,650 --> 00:00:52,810
ramesh and we UN and they'll be telling

00:00:50,739 --> 00:00:57,030
us about building stream processing

00:00:52,810 --> 00:00:57,030
applications please let them welcome

00:01:01,530 --> 00:01:08,260
hello everyone welcome to our talk I'm

00:01:04,600 --> 00:01:10,300
Amit and we and we're from of your

00:01:08,260 --> 00:01:13,540
engineers that you all work on the ads

00:01:10,300 --> 00:01:15,399
platform and here we are talking about

00:01:13,540 --> 00:01:17,850
stream processing and more specifically

00:01:15,399 --> 00:01:22,360
about building your own applications

00:01:17,850 --> 00:01:23,830
using stream processing so depend on

00:01:22,360 --> 00:01:25,990
where you're from we may or may not be

00:01:23,830 --> 00:01:27,280
familiar with Yelp Yelp mission is to

00:01:25,990 --> 00:01:29,649
connect people with great local

00:01:27,280 --> 00:01:32,050
businesses and via the website and

00:01:29,649 --> 00:01:34,660
mobile apps we build the tools that make

00:01:32,050 --> 00:01:36,280
this possible and with that I will turn

00:01:34,660 --> 00:01:39,009
off my PR mode and go back to my

00:01:36,280 --> 00:01:41,409
engineering mode so here's what we plan

00:01:39,009 --> 00:01:42,880
to curve during this presentation so

00:01:41,409 --> 00:01:46,030
we'll start off with some motivation for

00:01:42,880 --> 00:01:47,740
student passing and then we will come up

00:01:46,030 --> 00:01:50,650
with a example problem that you want to

00:01:47,740 --> 00:01:52,090
solve and walk through the solution and

00:01:50,650 --> 00:01:54,610
then we will cover some more general

00:01:52,090 --> 00:01:58,420
principles that's going to be useful

00:01:54,610 --> 00:02:00,790
when you build your own applications so

00:01:58,420 --> 00:02:03,939
why stream processing well clearly

00:02:00,790 --> 00:02:05,500
that's data coming all the time hitting

00:02:03,939 --> 00:02:07,960
us bombarding us whether we like it or

00:02:05,500 --> 00:02:09,790
not and it could be in various forms

00:02:07,960 --> 00:02:11,410
it's continually coming in maybe it's a

00:02:09,790 --> 00:02:13,450
measurement of a sensor or something

00:02:11,410 --> 00:02:16,239
like a Fitbit device producing these

00:02:13,450 --> 00:02:18,549
measurements or maybe it's just like

00:02:16,239 --> 00:02:21,610
people clicking on ads generating events

00:02:18,549 --> 00:02:23,230
and you probably want to make some kind

00:02:21,610 --> 00:02:24,489
of meaningful you want to attract

00:02:23,230 --> 00:02:26,709
something meaningful out of these may be

00:02:24,489 --> 00:02:29,019
the average value over the last minute

00:02:26,709 --> 00:02:31,320
for the measurements make sense in order

00:02:29,019 --> 00:02:34,660
to take a decision or maybe total clicks

00:02:31,320 --> 00:02:36,910
for ads on a day make sense to find out

00:02:34,660 --> 00:02:37,780
how much your campaign is doing how well

00:02:36,910 --> 00:02:41,500
your campaign is doing

00:02:37,780 --> 00:02:44,590
so traditionally although the data

00:02:41,500 --> 00:02:46,389
stream coming in continuously invariably

00:02:44,590 --> 00:02:49,430
they ended up in files log files more

00:02:46,389 --> 00:02:51,349
specifically and which meant like

00:02:49,430 --> 00:02:54,409
most natural way to handle the data was

00:02:51,349 --> 00:02:55,760
batch processing we're in it's kind of

00:02:54,409 --> 00:02:58,370
like a food processor at home I mean you

00:02:55,760 --> 00:03:00,140
have a finite amount of data that you

00:02:58,370 --> 00:03:02,390
want to process you just push it through

00:03:00,140 --> 00:03:04,099
the processing pipeline all in one go

00:03:02,390 --> 00:03:07,700
and then you get the results on your end

00:03:04,099 --> 00:03:10,609
but more recently frameworks such as

00:03:07,700 --> 00:03:13,159
Kafka are enabling more streaming kind

00:03:10,609 --> 00:03:16,250
of semantics and so there are definitely

00:03:13,159 --> 00:03:18,920
a shift towards mainstream shift towards

00:03:16,250 --> 00:03:21,680
actually doing stream processing and so

00:03:18,920 --> 00:03:23,030
in contrast to a batch process stream

00:03:21,680 --> 00:03:25,970
processing in situation would be where

00:03:23,030 --> 00:03:28,359
you have data coming in all the time and

00:03:25,970 --> 00:03:31,879
you're currently processing that data

00:03:28,359 --> 00:03:34,129
now while stream processing or batch

00:03:31,879 --> 00:03:36,109
processing well clearly you're reacting

00:03:34,129 --> 00:03:38,810
faster which means you're getting a

00:03:36,109 --> 00:03:40,909
results faster but there's also some

00:03:38,810 --> 00:03:43,189
quirks with batch processing that

00:03:40,909 --> 00:03:44,569
actually don't quite exist when you

00:03:43,189 --> 00:03:46,909
start doing stream processing directly

00:03:44,569 --> 00:03:50,349
so to give you an example let's say you

00:03:46,909 --> 00:03:53,329
have data for day 12 and a 13 and you're

00:03:50,349 --> 00:03:54,470
doing some kind of operation on on a day

00:03:53,329 --> 00:03:56,299
to day basis

00:03:54,470 --> 00:03:58,310
maybe you're processing in the West

00:03:56,299 --> 00:03:59,959
Coast but maybe the results are meant

00:03:58,310 --> 00:04:01,909
for the East Coast and so clearly

00:03:59,959 --> 00:04:04,849
there's this day boundary that doesn't

00:04:01,909 --> 00:04:05,689
quite match up on other two ends so some

00:04:04,849 --> 00:04:08,840
things like that you will have to

00:04:05,689 --> 00:04:10,310
basically work on a kind of resolve and

00:04:08,840 --> 00:04:13,340
especially if you want to support both

00:04:10,310 --> 00:04:14,629
the east and west coasts and such with

00:04:13,340 --> 00:04:16,940
stream processing things are a little

00:04:14,629 --> 00:04:18,919
more naturally set where you can have a

00:04:16,940 --> 00:04:21,680
window and that kind of moved those

00:04:18,919 --> 00:04:23,539
slides over and that way things are more

00:04:21,680 --> 00:04:27,800
naturally solved within a stream

00:04:23,539 --> 00:04:29,060
crossing context so at Yelp and

00:04:27,800 --> 00:04:31,009
specifically in the ad scheme our own

00:04:29,060 --> 00:04:33,110
evolution has kind of been similar like

00:04:31,009 --> 00:04:36,530
over many years we've been doing batch

00:04:33,110 --> 00:04:40,060
processing of events of log files and

00:04:36,530 --> 00:04:42,590
then more recently we have been

00:04:40,060 --> 00:04:45,620
investing time and resources towards

00:04:42,590 --> 00:04:47,900
stream processing so to set some context

00:04:45,620 --> 00:04:50,570
let's say you there is a business that

00:04:47,900 --> 00:04:54,110
has a presence on Yelp and now it's

00:04:50,570 --> 00:04:55,820
trying to advertise so that they've get

00:04:54,110 --> 00:05:00,349
more aware I use this are more aware of

00:04:55,820 --> 00:05:03,110
that so if a user using an app let's say

00:05:00,349 --> 00:05:05,060
it search for Thai food

00:05:03,110 --> 00:05:06,920
what might end up happening are a few

00:05:05,060 --> 00:05:09,440
ads may be shown about the search

00:05:06,920 --> 00:05:13,040
results pertaining to the business that

00:05:09,440 --> 00:05:16,460
wanted to advertise so now the user may

00:05:13,040 --> 00:05:18,080
choose to click on the ad or maybe they

00:05:16,460 --> 00:05:20,990
call the business so these are some

00:05:18,080 --> 00:05:24,740
events which can be tracked over time

00:05:20,990 --> 00:05:27,110
and aggregated and we eventually produce

00:05:24,740 --> 00:05:29,570
reports for their devices so that they

00:05:27,110 --> 00:05:33,620
get to know what kind of impact their

00:05:29,570 --> 00:05:35,300
advertising has made out there and the

00:05:33,620 --> 00:05:38,420
way we've been traditionally doing it is

00:05:35,300 --> 00:05:41,540
through nightly jobs so essentially we

00:05:38,420 --> 00:05:44,270
go the logs we have Hadoop jobs running

00:05:41,540 --> 00:05:45,440
overnight and they're producing these or

00:05:44,270 --> 00:05:47,450
crunching through and producing the

00:05:45,440 --> 00:05:50,960
reports so that we can share it with the

00:05:47,450 --> 00:05:53,870
advertisers on a daily basis but in the

00:05:50,960 --> 00:05:56,630
last couple of years we've always also

00:05:53,870 --> 00:05:59,390
introduced like a phone app for business

00:05:56,630 --> 00:06:01,010
users where they can actually see how

00:05:59,390 --> 00:06:03,800
their campaigns are doing they get their

00:06:01,010 --> 00:06:07,880
metrics on their phones which means it

00:06:03,800 --> 00:06:11,750
naturally makes sense for us to present

00:06:07,880 --> 00:06:14,360
this data on a more granular level like

00:06:11,750 --> 00:06:15,800
as in not at the daily or a nightly

00:06:14,360 --> 00:06:17,630
basis but you know like a more

00:06:15,800 --> 00:06:19,130
continuous basis and so we're kind of

00:06:17,630 --> 00:06:22,460
headed in that direction where we are

00:06:19,130 --> 00:06:23,870
working on internally having more

00:06:22,460 --> 00:06:25,760
systems work in the stream processing

00:06:23,870 --> 00:06:29,620
realm and then eventually surface it at

00:06:25,760 --> 00:06:32,420
some point to the external wall as well

00:06:29,620 --> 00:06:34,220
alright so now that we know what stream

00:06:32,420 --> 00:06:37,250
processing is the next section is going

00:06:34,220 --> 00:06:38,480
to be about putting one together and the

00:06:37,250 --> 00:06:40,400
rest of this section is going to be in

00:06:38,480 --> 00:06:41,900
the context of an example problem

00:06:40,400 --> 00:06:43,460
similar to something that we're working

00:06:41,900 --> 00:06:45,380
on at Yelp but a little bit simplified

00:06:43,460 --> 00:06:47,630
and that problem is going to be ad

00:06:45,380 --> 00:06:49,040
campaign metrics so the metrics are

00:06:47,630 --> 00:06:50,690
going to be numbers like the number of

00:06:49,040 --> 00:06:52,910
views and clicks that our campaigns are

00:06:50,690 --> 00:06:54,530
getting in a campaign is just a package

00:06:52,910 --> 00:06:56,270
that a business owner buys from us so

00:06:54,530 --> 00:07:00,140
basically we want to get these metrics

00:06:56,270 --> 00:07:02,540
for business so you can imagine we have

00:07:00,140 --> 00:07:04,310
events associated with ads in many

00:07:02,540 --> 00:07:07,220
different parts of our system for

00:07:04,310 --> 00:07:08,990
example we might have one log where we

00:07:07,220 --> 00:07:11,509
log when

00:07:08,990 --> 00:07:14,150
the ad is actually served from our

00:07:11,509 --> 00:07:15,770
back-end another one when the user views

00:07:14,150 --> 00:07:17,960
the ad and another one when they click

00:07:15,770 --> 00:07:19,819
on it so they might look something like

00:07:17,960 --> 00:07:21,380
this where the ad log has the most

00:07:19,819 --> 00:07:23,569
information about the advertisement

00:07:21,380 --> 00:07:25,250
itself and then the view and click logs

00:07:23,569 --> 00:07:28,039
just say what ads are associated with

00:07:25,250 --> 00:07:29,840
and the time they occurred and what we

00:07:28,039 --> 00:07:31,220
want to do is can find all these events

00:07:29,840 --> 00:07:35,919
together from the different parts of our

00:07:31,220 --> 00:07:38,900
system and get these metrics over time

00:07:35,919 --> 00:07:40,789
but before we actually try solving this

00:07:38,900 --> 00:07:42,410
problem let's figure out what are the

00:07:40,789 --> 00:07:46,550
tools available what's the arsenal that

00:07:42,410 --> 00:07:48,650
we have to build such a system so first

00:07:46,550 --> 00:07:50,300
off at a very high level a stream

00:07:48,650 --> 00:07:52,070
processing pipeline looks something like

00:07:50,300 --> 00:07:54,500
this I mean you have a source at the one

00:07:52,070 --> 00:07:57,169
end a streaming source and then you have

00:07:54,500 --> 00:07:58,310
a scene processing engine which is where

00:07:57,169 --> 00:08:01,490
you will have most of your business

00:07:58,310 --> 00:08:03,349
logic and then the data sink where it

00:08:01,490 --> 00:08:05,090
could be another streaming source so if

00:08:03,349 --> 00:08:07,039
you want to change stream processing

00:08:05,090 --> 00:08:09,319
pipelines together or it could be maybe

00:08:07,039 --> 00:08:11,690
a traditional database so that it can

00:08:09,319 --> 00:08:15,740
downstream systems can just query on off

00:08:11,690 --> 00:08:18,349
of them and in our case specifically we

00:08:15,740 --> 00:08:19,729
use Kafka for the streaming source view

00:08:18,349 --> 00:08:22,610
sports streaming for the actual

00:08:19,729 --> 00:08:24,080
processing and use both Kafka and

00:08:22,610 --> 00:08:29,139
Cassandra depending on the use case for

00:08:24,080 --> 00:08:31,610
the positive sink so in terms of the

00:08:29,139 --> 00:08:34,579
processing side of things itself I mean

00:08:31,610 --> 00:08:37,729
like so there are these basic categories

00:08:34,579 --> 00:08:39,380
of operations that you can do and we

00:08:37,729 --> 00:08:41,990
will go through each of these in a

00:08:39,380 --> 00:08:43,969
little bit of detail now so first of all

00:08:41,990 --> 00:08:46,370
ingestion like clearly you want

00:08:43,969 --> 00:08:48,380
something that pulls data from your

00:08:46,370 --> 00:08:51,110
streaming source into your pipeline for

00:08:48,380 --> 00:08:53,860
processing and that of course going to

00:08:51,110 --> 00:08:56,660
be your first stage of your pipeline so

00:08:53,860 --> 00:08:58,339
if you for example considered Kafka as

00:08:56,660 --> 00:09:00,260
your source then you need something

00:08:58,339 --> 00:09:02,720
which is Kafka specific in terms of like

00:09:00,260 --> 00:09:04,670
talking the right protocol in terms of

00:09:02,720 --> 00:09:06,589
right talking about semantics and

00:09:04,670 --> 00:09:12,500
translating the data from Kafka into the

00:09:06,589 --> 00:09:16,220
pipeline and so as an example so this is

00:09:12,500 --> 00:09:18,440
a park streaming example where you

00:09:16,220 --> 00:09:21,530
import a specific utility which is Kafka

00:09:18,440 --> 00:09:23,330
specific and then you create your stream

00:09:21,530 --> 00:09:25,280
and you provide the kafka parameters it

00:09:23,330 --> 00:09:28,130
needs so that it can talk to Kafka and

00:09:25,280 --> 00:09:31,850
create a stream that's then handed down

00:09:28,130 --> 00:09:34,040
to the operation that is coming next so

00:09:31,850 --> 00:09:34,970
once you have the data the simplest

00:09:34,040 --> 00:09:38,060
thing you can do are stateless

00:09:34,970 --> 00:09:39,920
transforms wherein you're really

00:09:38,060 --> 00:09:41,390
focusing on a single event at a time so

00:09:39,920 --> 00:09:44,180
all you're doing is within the context

00:09:41,390 --> 00:09:45,980
of the single event so an example could

00:09:44,180 --> 00:09:47,600
be like a filtering operation so you

00:09:45,980 --> 00:09:49,460
have data coming in events coming in and

00:09:47,600 --> 00:09:51,800
maybe you have some criteria based on

00:09:49,460 --> 00:09:55,310
which you either drop events or let them

00:09:51,800 --> 00:09:56,900
pass through so an example could be you

00:09:55,310 --> 00:09:59,620
have these add events coming in and

00:09:56,900 --> 00:10:01,850
maybe some of these IPS are from BOTS

00:09:59,620 --> 00:10:04,070
and if you have some kind of a blacklist

00:10:01,850 --> 00:10:05,960
then you can use that to just filter out

00:10:04,070 --> 00:10:07,600
the ones that are from BOTS

00:10:05,960 --> 00:10:10,670
and only keep the monster oh gentleman

00:10:07,600 --> 00:10:14,240
another example of shapeless transforms

00:10:10,670 --> 00:10:15,710
could be projection where your events

00:10:14,240 --> 00:10:18,140
may have fields that you don't really

00:10:15,710 --> 00:10:20,660
care about and so you're just trimming

00:10:18,140 --> 00:10:23,390
them down and only keeping the fields

00:10:20,660 --> 00:10:25,100
that you recap rewards so again an

00:10:23,390 --> 00:10:27,170
example could be like yeah we have add

00:10:25,100 --> 00:10:29,480
IDs in campaign IDs which you care about

00:10:27,170 --> 00:10:31,790
maybe for subsequent processing and so

00:10:29,480 --> 00:10:33,920
you just keep those fields and their

00:10:31,790 --> 00:10:37,850
values while dropping all the other keys

00:10:33,920 --> 00:10:41,620
away and then the next level of

00:10:37,850 --> 00:10:44,150
interesting is stateful transforms where

00:10:41,620 --> 00:10:46,120
you're now concerned but not just a

00:10:44,150 --> 00:10:49,900
single event at a time but actually a

00:10:46,120 --> 00:10:52,430
window of events at a time and you could

00:10:49,900 --> 00:10:53,600
basically do like sliding windows where

00:10:52,430 --> 00:10:55,010
you have overlaps or you could do

00:10:53,600 --> 00:10:57,170
tumbling windows I mean most frameworks

00:10:55,010 --> 00:11:00,620
support like multiple kinds of windowing

00:10:57,170 --> 00:11:02,960
operations but the central idea is that

00:11:00,620 --> 00:11:06,230
you're going to apply operations over

00:11:02,960 --> 00:11:08,060
multiple events and let's take an

00:11:06,230 --> 00:11:09,830
example actually to make it more

00:11:08,060 --> 00:11:12,050
explicit why we call these stateful

00:11:09,830 --> 00:11:14,180
transforms so let's say this we're

00:11:12,050 --> 00:11:16,640
trying to aggregate numbers just simple

00:11:14,180 --> 00:11:19,490
numbers of each event contains a single

00:11:16,640 --> 00:11:22,160
number we're summing them up based on

00:11:19,490 --> 00:11:26,780
the window and producing the results in

00:11:22,160 --> 00:11:28,460
this case now in theory you're basically

00:11:26,780 --> 00:11:30,410
going to go through every event or

00:11:28,460 --> 00:11:32,900
you're going to get one even at a time

00:11:30,410 --> 00:11:35,460
which means that you have to keep the

00:11:32,900 --> 00:11:36,529
intermediate results of the

00:11:35,460 --> 00:11:39,089
that's something that you're doing

00:11:36,529 --> 00:11:41,580
before you can get to the end of the

00:11:39,089 --> 00:11:44,120
window and then emit that some so there

00:11:41,580 --> 00:11:46,800
is a state there's this intermediate a

00:11:44,120 --> 00:11:48,839
count or the accumulation that you're

00:11:46,800 --> 00:11:50,430
doing and that's kind of that's the

00:11:48,839 --> 00:11:52,440
reason why it's it's stateful transform

00:11:50,430 --> 00:11:53,850
and of course you can imagine that this

00:11:52,440 --> 00:11:55,140
is a very simple case where you had a

00:11:53,850 --> 00:11:56,970
single number to hold on to but

00:11:55,140 --> 00:11:59,250
depending on what kind of operation

00:11:56,970 --> 00:12:02,670
you're doing it may be a more complex

00:11:59,250 --> 00:12:04,980
state and here again the example here

00:12:02,670 --> 00:12:07,260
it's a pretty straightforward API in

00:12:04,980 --> 00:12:10,440
SPARC and so you just do a reduced by

00:12:07,260 --> 00:12:12,120
window you provide the standard Python

00:12:10,440 --> 00:12:13,470
add operator there and then you can

00:12:12,120 --> 00:12:15,540
specify the kind of windowing you want

00:12:13,470 --> 00:12:19,440
to do and then you have the results on

00:12:15,540 --> 00:12:21,060
the other end and then the next step

00:12:19,440 --> 00:12:24,089
would be doing something like a keyed

00:12:21,060 --> 00:12:26,580
stateful transform so here we are still

00:12:24,089 --> 00:12:28,680
applying operations within the scope of

00:12:26,580 --> 00:12:32,220
Windows but then we're also trying to

00:12:28,680 --> 00:12:34,529
group by keys so colors are used here to

00:12:32,220 --> 00:12:37,770
kind of distinguish keys so you have the

00:12:34,529 --> 00:12:40,410
orange and the green and what you see

00:12:37,770 --> 00:12:43,740
here are there's a bucketing by color or

00:12:40,410 --> 00:12:45,540
by key and there's an implicit stage in

00:12:43,740 --> 00:12:48,240
these cases called shuffle which is

00:12:45,540 --> 00:12:50,910
where the shuffling or the grouping of

00:12:48,240 --> 00:12:52,950
keys takes place and then subsequently

00:12:50,910 --> 00:12:54,420
you apply whatever operation or

00:12:52,950 --> 00:12:55,529
transform you want to apply on the data

00:12:54,420 --> 00:12:57,810
that's been collected or grouped

00:12:55,529 --> 00:12:59,279
together which is represented by darker

00:12:57,810 --> 00:13:01,410
shades of color here just to say okay

00:12:59,279 --> 00:13:04,520
there's some processed version of that

00:13:01,410 --> 00:13:07,440
data so an example for this would be

00:13:04,520 --> 00:13:09,300
lets say you have the add events we were

00:13:07,440 --> 00:13:12,660
talking about earlier and people viewing

00:13:09,300 --> 00:13:14,970
these ads and so you have a color-coded

00:13:12,660 --> 00:13:16,950
campaign IDs right now so you have two

00:13:14,970 --> 00:13:19,260
campaigns green and orange or one and

00:13:16,950 --> 00:13:21,630
two and here again you have a field

00:13:19,260 --> 00:13:24,180
called views which is just counting a

00:13:21,630 --> 00:13:27,000
count of number of views and then on the

00:13:24,180 --> 00:13:30,480
other end you have the things of summed

00:13:27,000 --> 00:13:32,010
up by the corresponding keys and again

00:13:30,480 --> 00:13:33,990
the code is pretty straightforward

00:13:32,010 --> 00:13:36,120
because there's a direct API again where

00:13:33,990 --> 00:13:38,100
you just say reduce by key and window

00:13:36,120 --> 00:13:40,170
and then you just have the same operator

00:13:38,100 --> 00:13:44,820
and whatever been doing you want to

00:13:40,170 --> 00:13:46,200
choose and there's another way or this

00:13:44,820 --> 00:13:48,180
is something else you can do also with

00:13:46,200 --> 00:13:49,089
in the case of key state for transforms

00:13:48,180 --> 00:13:50,769
where you're not just stealing

00:13:49,089 --> 00:13:51,970
the single stream but actually dealing

00:13:50,769 --> 00:13:54,459
with more than one stream so it's more

00:13:51,970 --> 00:13:58,209
like a joint operation in this case and

00:13:54,459 --> 00:13:59,470
here again it's pretty similar in the

00:13:58,209 --> 00:14:02,040
sense that you still have the shuffle

00:13:59,470 --> 00:14:04,689
and things going on grouping by key

00:14:02,040 --> 00:14:07,720
except that your original source had two

00:14:04,689 --> 00:14:09,069
streams and then the final join is some

00:14:07,720 --> 00:14:12,189
kind of a combination of the records

00:14:09,069 --> 00:14:14,199
that were brought together so here's an

00:14:12,189 --> 00:14:15,879
example where you have two different

00:14:14,199 --> 00:14:18,399
streams one is the add stream and one is

00:14:15,879 --> 00:14:20,800
the the view stream and we want a

00:14:18,399 --> 00:14:22,839
consolidated view of the entire thing

00:14:20,800 --> 00:14:24,610
that's happening and so we bring those

00:14:22,839 --> 00:14:26,620
corresponding records together into a

00:14:24,610 --> 00:14:29,470
single record and that's basically our

00:14:26,620 --> 00:14:33,100
output so that's example of how you do

00:14:29,470 --> 00:14:34,660
this and in terms of code again it's

00:14:33,100 --> 00:14:37,899
pretty straightforward you essentially

00:14:34,660 --> 00:14:40,029
just apply windows or define windows for

00:14:37,899 --> 00:14:41,949
the two streams that you have and then

00:14:40,029 --> 00:14:43,600
when you actually apply a join the join

00:14:41,949 --> 00:14:46,860
is always in the context of the windows

00:14:43,600 --> 00:14:49,870
that you were defining earlier and

00:14:46,860 --> 00:14:52,540
finally you have the publishing phase

00:14:49,870 --> 00:14:54,339
which is kind of the the other end of

00:14:52,540 --> 00:14:57,160
the pipeline where you have to write to

00:14:54,339 --> 00:14:59,079
some kind of sink and here again it has

00:14:57,160 --> 00:15:01,300
to be specific to the thing that you're

00:14:59,079 --> 00:15:02,860
writing to a simple example could just

00:15:01,300 --> 00:15:05,470
be like yeah maybe you're saving it as

00:15:02,860 --> 00:15:10,089
text files on s3 and there's a direct

00:15:05,470 --> 00:15:11,439
interface for that so in summary we need

00:15:10,089 --> 00:15:13,569
ingestion to get the data into the

00:15:11,439 --> 00:15:15,160
pipeline we need we can use stateless

00:15:13,569 --> 00:15:17,740
transforms whenever we just acting on

00:15:15,160 --> 00:15:19,749
single events stateful transforms if we

00:15:17,740 --> 00:15:22,449
are actually acting on Windows of events

00:15:19,749 --> 00:15:23,860
and keyed stateful transforms if you're

00:15:22,449 --> 00:15:25,929
not only operating our windows but also

00:15:23,860 --> 00:15:28,389
like wanting to group by a certain

00:15:25,929 --> 00:15:29,889
attribute or key and finally publishing

00:15:28,389 --> 00:15:34,179
to actually make your results available

00:15:29,889 --> 00:15:36,100
to external systems okay so we have all

00:15:34,179 --> 00:15:38,860
the pieces now and so let's go back to

00:15:36,100 --> 00:15:41,259
our example and put them together to get

00:15:38,860 --> 00:15:43,689
the campaign metrics so remember we have

00:15:41,259 --> 00:15:46,569
the add view and click logs or event

00:15:43,689 --> 00:15:48,370
streams coming in so the first stage is

00:15:46,569 --> 00:15:51,970
reading them so we have a couple events

00:15:48,370 --> 00:15:54,850
in our add stream we filter out the ones

00:15:51,970 --> 00:15:57,759
that maybe come from su-mei piece and

00:15:54,850 --> 00:15:59,350
then trim down the field so here's a

00:15:57,759 --> 00:16:01,389
scoring field in the add log that says

00:15:59,350 --> 00:16:02,649
how this ad was chosen we don't need

00:16:01,389 --> 00:16:05,499
that for a final camp

00:16:02,649 --> 00:16:06,879
Metrix so we'll take that out and you

00:16:05,499 --> 00:16:08,379
can imagine that the viewing clique laws

00:16:06,879 --> 00:16:10,240
go through similar stages where we

00:16:08,379 --> 00:16:12,610
trimmed them down to exactly what we

00:16:10,240 --> 00:16:14,439
want so that we don't have to send as

00:16:12,610 --> 00:16:17,259
much data around when we get to the

00:16:14,439 --> 00:16:18,879
joining stage so in the joining stage we

00:16:17,259 --> 00:16:22,389
want all the data for a single ad

00:16:18,879 --> 00:16:23,800
together so here we have the same ad the

00:16:22,389 --> 00:16:26,410
ad and view data for it

00:16:23,800 --> 00:16:30,009
so the join puts them together in a

00:16:26,410 --> 00:16:31,600
single event and then we can further do

00:16:30,009 --> 00:16:33,670
a transformation pulling out the data we

00:16:31,600 --> 00:16:36,519
want the campaign ID whether or not

00:16:33,670 --> 00:16:38,529
there was a view finally we put together

00:16:36,519 --> 00:16:40,720
events from different ads for the same

00:16:38,529 --> 00:16:42,699
campaign add together the number of

00:16:40,720 --> 00:16:44,649
views so here we have two views in one

00:16:42,699 --> 00:16:46,869
click and that's the information that we

00:16:44,649 --> 00:16:49,569
wanted so we can write that out to the

00:16:46,869 --> 00:16:51,429
database so here's the full picture

00:16:49,569 --> 00:16:52,809
again hopefully that gives you an idea

00:16:51,429 --> 00:16:57,759
of how you can put these operations

00:16:52,809 --> 00:16:59,589
together so now that we spoke about a

00:16:57,759 --> 00:17:02,619
specific example some operations that

00:16:59,589 --> 00:17:04,870
you can do and very specific solution to

00:17:02,619 --> 00:17:07,029
a problem let's kind of step back and

00:17:04,870 --> 00:17:10,270
talk about some more general principles

00:17:07,029 --> 00:17:12,220
that are applicable to design upstream

00:17:10,270 --> 00:17:14,500
processing applications so the first one

00:17:12,220 --> 00:17:15,220
we'll talk about is horizontal

00:17:14,500 --> 00:17:17,679
scalability

00:17:15,220 --> 00:17:20,589
so let's say you build an application

00:17:17,679 --> 00:17:22,390
you have a solution out there and maybe

00:17:20,589 --> 00:17:24,640
looks like this you have some data you

00:17:22,390 --> 00:17:27,760
have some processing going on and it's

00:17:24,640 --> 00:17:30,520
working fine for a while and then maybe

00:17:27,760 --> 00:17:32,409
someday you get hit by a load of data

00:17:30,520 --> 00:17:35,380
and then it just squashes your

00:17:32,409 --> 00:17:37,630
application and that's basically not the

00:17:35,380 --> 00:17:39,940
kind of design you want so the central

00:17:37,630 --> 00:17:41,919
idea of horizontal scalability is to

00:17:39,940 --> 00:17:44,080
kind of design your solution in such a

00:17:41,919 --> 00:17:46,809
way that the data can always be broken

00:17:44,080 --> 00:17:49,740
down into reasonable chunks and be

00:17:46,809 --> 00:17:53,860
distributed across your processing

00:17:49,740 --> 00:17:55,570
resources and the only thing you will

00:17:53,860 --> 00:17:57,429
have to do is probably go in more

00:17:55,570 --> 00:17:59,230
machines or throw in more resources to

00:17:57,429 --> 00:18:04,360
solve the problem but the design should

00:17:59,230 --> 00:18:06,880
kind of stay unaffected by it and why

00:18:04,360 --> 00:18:09,549
would you care about this well for the

00:18:06,880 --> 00:18:11,110
most part I mean there's always growth

00:18:09,549 --> 00:18:12,639
in data that's expected like if you have

00:18:11,110 --> 00:18:14,770
a website or whatever other source of

00:18:12,639 --> 00:18:16,039
data it's very likely that data will

00:18:14,770 --> 00:18:17,840
grow over time and so

00:18:16,039 --> 00:18:20,379
good to have a design that's able to

00:18:17,840 --> 00:18:23,559
scale very easily with the growing data

00:18:20,379 --> 00:18:25,850
or even if you don't have such a

00:18:23,559 --> 00:18:28,029
situation you could always end up with a

00:18:25,850 --> 00:18:30,259
situation where you have some kind of

00:18:28,029 --> 00:18:32,629
differences in terms of how much data

00:18:30,259 --> 00:18:34,789
you get maybe during different times of

00:18:32,629 --> 00:18:36,049
the day maybe on a weekly basis and so

00:18:34,789 --> 00:18:37,820
if there's like a radical difference

00:18:36,049 --> 00:18:40,129
between the peaks and the troughs then

00:18:37,820 --> 00:18:42,289
again it makes sense to have design

00:18:40,129 --> 00:18:45,200
that's horizontally scalable so you can

00:18:42,289 --> 00:18:47,359
actually save some cost by like having

00:18:45,200 --> 00:18:50,179
fewer machines say doing the trucks and

00:18:47,359 --> 00:18:51,950
then adding the machines when there are

00:18:50,179 --> 00:18:53,419
peaks and they're also like automated

00:18:51,950 --> 00:18:55,789
solutions for this like auto scaling

00:18:53,419 --> 00:18:59,450
solutions where you can essentially have

00:18:55,789 --> 00:19:01,489
this automated alright so how do you go

00:18:59,450 --> 00:19:03,889
about actually doing it for a stream

00:19:01,489 --> 00:19:06,200
processing application well it turns out

00:19:03,889 --> 00:19:08,019
it's pretty straightforward because the

00:19:06,200 --> 00:19:11,029
events are already discrete enough that

00:19:08,019 --> 00:19:13,399
this is coming in sizable chunks that

00:19:11,029 --> 00:19:15,109
that can be handled and all you need to

00:19:13,399 --> 00:19:17,869
do is just give you up into multiple

00:19:15,109 --> 00:19:19,749
partitions and so you can just paralyze

00:19:17,869 --> 00:19:22,220
it as much as you like depending on the

00:19:19,749 --> 00:19:26,149
resources you have and that way you're

00:19:22,220 --> 00:19:27,580
just going to spread this data and it's

00:19:26,149 --> 00:19:29,739
going back to our example from earlier

00:19:27,580 --> 00:19:31,729
like clearly the first three stages

00:19:29,739 --> 00:19:33,080
where they're all like stateless

00:19:31,729 --> 00:19:34,759
transforms this is like readily

00:19:33,080 --> 00:19:36,679
applicable because each one is just

00:19:34,759 --> 00:19:37,970
working on a single event and so it

00:19:36,679 --> 00:19:40,159
really doesn't matter where that event

00:19:37,970 --> 00:19:42,950
ends up and so they can basically

00:19:40,159 --> 00:19:46,599
distribute the word across themselves

00:19:42,950 --> 00:19:49,489
and you can pretty much scale that way

00:19:46,599 --> 00:19:51,169
but if you look at the second stage in

00:19:49,489 --> 00:19:53,149
that processing pipeline where you are

00:19:51,169 --> 00:19:55,340
joining the add ID no clearly you don't

00:19:53,149 --> 00:19:57,739
get to just randomly throw things around

00:19:55,340 --> 00:19:59,570
so this would be the case where you use

00:19:57,739 --> 00:20:01,849
keyed partitioning because you're

00:19:59,570 --> 00:20:03,889
actually using the key to determine the

00:20:01,849 --> 00:20:07,129
buckets or how you want to distribute

00:20:03,889 --> 00:20:08,599
your data but it's so it's something

00:20:07,129 --> 00:20:10,099
that's pretty straightforward all you

00:20:08,599 --> 00:20:13,249
have to do is just use some kind of

00:20:10,099 --> 00:20:14,779
hashing to the bucket things the one

00:20:13,249 --> 00:20:17,899
thing to watch out for especially with

00:20:14,779 --> 00:20:21,200
key transformation key the partitioning

00:20:17,899 --> 00:20:23,599
is the possibility of hotspots or data

00:20:21,200 --> 00:20:25,249
skew so what do I mean by that well

00:20:23,599 --> 00:20:26,690
let's take the next stage in the

00:20:25,249 --> 00:20:29,450
pipeline where we were trying to sum my

00:20:26,690 --> 00:20:31,370
campaign now it's quite possible

00:20:29,450 --> 00:20:33,080
that you have some two different ad

00:20:31,370 --> 00:20:35,059
campaigns which behave very very

00:20:33,080 --> 00:20:36,590
differently so for example maybe there's

00:20:35,059 --> 00:20:37,880
an restaurant that's advertising and

00:20:36,590 --> 00:20:39,980
then there's probably a bike shop that's

00:20:37,880 --> 00:20:42,649
advertising and clearly the restaurant

00:20:39,980 --> 00:20:44,419
one possibly can reach most people while

00:20:42,649 --> 00:20:47,090
the bike shop one is probably a more

00:20:44,419 --> 00:20:50,210
narrow restricted subset that it wants

00:20:47,090 --> 00:20:51,649
to target so the data might end up

00:20:50,210 --> 00:20:53,269
looking like this so the red ones

00:20:51,649 --> 00:20:55,549
representing the right a restaurant

00:20:53,269 --> 00:20:56,960
where like there's too many ads and then

00:20:55,549 --> 00:21:01,220
you have the blue one which is the bike

00:20:56,960 --> 00:21:03,350
shop which just happens to so it's quite

00:21:01,220 --> 00:21:05,659
possible so this is like a situation

00:21:03,350 --> 00:21:08,120
that can pretty much occur for any kind

00:21:05,659 --> 00:21:10,340
of problem and there are mitigation

00:21:08,120 --> 00:21:12,080
solutions mitigation strategies to kind

00:21:10,340 --> 00:21:13,669
of overcome this kind of skew and and

00:21:12,080 --> 00:21:16,250
resolve it in a way that things are

00:21:13,669 --> 00:21:17,570
balanced but we don't have the time and

00:21:16,250 --> 00:21:19,429
so it says out of the scope for this

00:21:17,570 --> 00:21:21,620
particular talk but I do encourage you

00:21:19,429 --> 00:21:25,720
to look it up online or talk to us after

00:21:21,620 --> 00:21:28,460
and we can go over that so in summary

00:21:25,720 --> 00:21:30,230
use random partitioning whenever you

00:21:28,460 --> 00:21:31,880
have stateless transforms you can use

00:21:30,230 --> 00:21:34,010
key partitioning for key transforms and

00:21:31,880 --> 00:21:36,860
definitely watch out for hotspots and

00:21:34,010 --> 00:21:39,919
use mitigation strategies to eradicate

00:21:36,860 --> 00:21:41,899
those okay

00:21:39,919 --> 00:21:43,429
the next two principles that we're going

00:21:41,899 --> 00:21:45,590
to talk about are related to handling

00:21:43,429 --> 00:21:47,809
failures so once you've distributed out

00:21:45,590 --> 00:21:49,490
the work in your application any of the

00:21:47,809 --> 00:21:52,100
machines that you're doing work on could

00:21:49,490 --> 00:21:54,200
fail so idempotency and the trade-off

00:21:52,100 --> 00:21:57,110
between consistency and availability are

00:21:54,200 --> 00:22:00,320
things that you need to think about so

00:21:57,110 --> 00:22:03,230
idempotency just in case we're going to

00:22:00,320 --> 00:22:04,760
review an item plot an idempotent

00:22:03,230 --> 00:22:07,059
operation is one that can be applied

00:22:04,760 --> 00:22:10,610
more than once and have the same effect

00:22:07,059 --> 00:22:12,710
so we want this because if any operation

00:22:10,610 --> 00:22:14,750
in our pipeline fails for example the

00:22:12,710 --> 00:22:16,789
projection or the right we want to be

00:22:14,750 --> 00:22:19,070
able to retry that an item potency

00:22:16,789 --> 00:22:20,630
allows you to reason really easily about

00:22:19,070 --> 00:22:22,429
what the effective outreach I will be

00:22:20,630 --> 00:22:25,909
without worrying about its effects

00:22:22,429 --> 00:22:28,070
so transforms like filters or

00:22:25,909 --> 00:22:30,200
projections where you're not updating

00:22:28,070 --> 00:22:33,139
any state are item potent whereas

00:22:30,200 --> 00:22:35,029
stateful operations like things with

00:22:33,139 --> 00:22:37,429
local state or where you're writing out

00:22:35,029 --> 00:22:40,159
to a data sink you need to think more

00:22:37,429 --> 00:22:42,320
carefully about so here it's that right

00:22:40,159 --> 00:22:44,929
stage that we need to design for

00:22:42,320 --> 00:22:46,659
and some rights can be idempotent for

00:22:44,929 --> 00:22:49,159
example if you're writing to unique key

00:22:46,659 --> 00:22:50,960
one campaign one minute we're saying

00:22:49,159 --> 00:22:52,610
that there are two views there obviously

00:22:50,960 --> 00:22:54,799
no matter how many times you write there

00:22:52,610 --> 00:22:58,610
are two views you'll still be two so

00:22:54,799 --> 00:23:00,350
that's item potent in contrast if you're

00:22:58,610 --> 00:23:01,970
doing something like an increment where

00:23:00,350 --> 00:23:03,620
you're saying you want to add to the

00:23:01,970 --> 00:23:05,509
number of views of course if you do that

00:23:03,620 --> 00:23:08,269
more than once the result is different

00:23:05,509 --> 00:23:11,809
so increments are pretty common

00:23:08,269 --> 00:23:14,450
operation so some databases do try to

00:23:11,809 --> 00:23:15,769
add support for item potency there are a

00:23:14,450 --> 00:23:17,929
lot of different solutions but one of

00:23:15,769 --> 00:23:19,490
them is adding some kind of version to

00:23:17,929 --> 00:23:22,460
the operation so you're saying I'm

00:23:19,490 --> 00:23:24,379
updating this specific version and then

00:23:22,460 --> 00:23:26,330
if the database is keeping track of

00:23:24,379 --> 00:23:28,179
those versions then it'll know that it's

00:23:26,330 --> 00:23:30,830
seen an update for that version before

00:23:28,179 --> 00:23:32,179
so this becomes either button because no

00:23:30,830 --> 00:23:34,190
matter how many times you send that

00:23:32,179 --> 00:23:38,240
update the database will only apply it

00:23:34,190 --> 00:23:40,340
once so back to stripping pipelines in

00:23:38,240 --> 00:23:42,950
general we just talked about an example

00:23:40,340 --> 00:23:44,840
related to the data sink but it's also

00:23:42,950 --> 00:23:45,980
important in the local state when you're

00:23:44,840 --> 00:23:49,250
doing something like a stateful

00:23:45,980 --> 00:23:52,129
transform like joining or some operation

00:23:49,250 --> 00:23:54,769
over the window because any event could

00:23:52,129 --> 00:23:56,330
be reprocessed and that will affect the

00:23:54,769 --> 00:23:58,669
local state that you're maintaining and

00:23:56,330 --> 00:24:01,700
some frameworks try to provide exactly

00:23:58,669 --> 00:24:02,990
once guarantees on the reprocessing but

00:24:01,700 --> 00:24:05,539
what they're doing under the hood is

00:24:02,990 --> 00:24:07,250
analogous to what the database is doing

00:24:05,539 --> 00:24:10,549
they're adding extra information so they

00:24:07,250 --> 00:24:12,470
can keep track of what their they've

00:24:10,549 --> 00:24:13,820
already processed so it's important to

00:24:12,470 --> 00:24:17,419
understand the mechanism that they're

00:24:13,820 --> 00:24:18,830
using to guarantee that okay and then

00:24:17,419 --> 00:24:21,019
the final concept we're going to talk

00:24:18,830 --> 00:24:23,899
about is consistency versus availability

00:24:21,019 --> 00:24:25,340
and so basically there's always a

00:24:23,899 --> 00:24:28,000
trade-off between these two things when

00:24:25,340 --> 00:24:30,409
you're handling failures and so

00:24:28,000 --> 00:24:33,049
consistency we're going to say is every

00:24:30,409 --> 00:24:35,210
reading every read seeing a current view

00:24:33,049 --> 00:24:37,490
of the data and then availability is

00:24:35,210 --> 00:24:39,919
your system's capacity to serve requests

00:24:37,490 --> 00:24:41,629
and this is a fundamental trade-off when

00:24:39,919 --> 00:24:43,159
you're doing distributed systems and

00:24:41,629 --> 00:24:45,169
trying to handle failures between them

00:24:43,159 --> 00:24:47,990
so to see why we're just going to do a

00:24:45,169 --> 00:24:50,120
quick example with a replicated data

00:24:47,990 --> 00:24:52,760
store where two machines are trying to

00:24:50,120 --> 00:24:54,710
keep track of the same state in the

00:24:52,760 --> 00:24:56,120
happy case when someone updates the

00:24:54,710 --> 00:24:57,679
value of a

00:24:56,120 --> 00:24:59,840
one machine will tell the other machine

00:24:57,679 --> 00:25:02,419
and they still all agree but in the

00:24:59,840 --> 00:25:03,799
situation where they're not able to

00:25:02,419 --> 00:25:06,980
communicate with each other then we need

00:25:03,799 --> 00:25:09,049
to make this trade-off for example if

00:25:06,980 --> 00:25:11,240
someone tries to write a equals three

00:25:09,049 --> 00:25:12,620
and that machine isn't able to

00:25:11,240 --> 00:25:15,140
communicate to the other one

00:25:12,620 --> 00:25:17,630
then a consistent solution would be to

00:25:15,140 --> 00:25:20,690
reject that right but that makes rights

00:25:17,630 --> 00:25:23,270
less available and if we wanted to

00:25:20,690 --> 00:25:25,460
accept that right then you know during

00:25:23,270 --> 00:25:27,049
the time when the machines are unable to

00:25:25,460 --> 00:25:28,789
communicate with each other then you

00:25:27,049 --> 00:25:30,830
have this inconsistency where one

00:25:28,789 --> 00:25:35,029
machine doesn't see the most current

00:25:30,830 --> 00:25:37,070
view so again this is pretty fundamental

00:25:35,029 --> 00:25:38,510
to all distributed systems so it's

00:25:37,070 --> 00:25:42,320
important to think about in your data

00:25:38,510 --> 00:25:44,539
source and data sync some systems make a

00:25:42,320 --> 00:25:45,799
specific choice along the spectrums or

00:25:44,539 --> 00:25:48,169
should you should be aware of what

00:25:45,799 --> 00:25:50,779
they're choosing and make sure it fits

00:25:48,169 --> 00:25:52,640
your application others do let you be

00:25:50,779 --> 00:25:54,679
more flexible for example cassandra

00:25:52,640 --> 00:25:56,630
allows you to choose how many replicas

00:25:54,679 --> 00:25:58,520
have to respond to a right before it's

00:25:56,630 --> 00:26:00,350
successful so the more replicas that

00:25:58,520 --> 00:26:03,380
need to respond to the more consistency

00:26:00,350 --> 00:26:05,240
the less availability and one thing to

00:26:03,380 --> 00:26:07,899
keep in mind is streaming applications

00:26:05,240 --> 00:26:09,770
are designed to run continuously so

00:26:07,899 --> 00:26:12,679
usually you want to prioritize

00:26:09,770 --> 00:26:15,230
availability because you don't want your

00:26:12,679 --> 00:26:17,149
application to shut down in case your

00:26:15,230 --> 00:26:20,600
sink or source become slightly

00:26:17,149 --> 00:26:22,399
unavailable but in some cases for

00:26:20,600 --> 00:26:24,470
example if you're using the data and

00:26:22,399 --> 00:26:27,020
your application are using clicks for

00:26:24,470 --> 00:26:30,200
billing advertisers in that case we

00:26:27,020 --> 00:26:33,049
might actually care that the data is the

00:26:30,200 --> 00:26:34,909
most current and accurate so if you want

00:26:33,049 --> 00:26:36,880
to prioritize consistencies and you need

00:26:34,909 --> 00:26:39,590
to add some extra retries to your

00:26:36,880 --> 00:26:42,789
application to make sure that you can

00:26:39,590 --> 00:26:45,289
handle unavailable of use sometimes

00:26:42,789 --> 00:26:46,880
right and then in other cases where it's

00:26:45,289 --> 00:26:51,169
less critical than you can prioritize

00:26:46,880 --> 00:26:53,720
availability all right so that it is the

00:26:51,169 --> 00:26:55,399
end of our talk and basically student

00:26:53,720 --> 00:26:58,279
processing is just processing on events

00:26:55,399 --> 00:27:00,320
and as they come in you can have

00:26:58,279 --> 00:27:03,260
operations on single events or over

00:27:00,320 --> 00:27:05,450
windows of events something to keep in

00:27:03,260 --> 00:27:08,400
mind is trying to keep your design

00:27:05,450 --> 00:27:10,410
horizontally scalable because data often

00:27:08,400 --> 00:27:12,090
grows and changes over times you want to

00:27:10,410 --> 00:27:15,630
make sure that your partitions are even

00:27:12,090 --> 00:27:19,170
as possible and then because streaming

00:27:15,630 --> 00:27:21,900
processing any event could cause a

00:27:19,170 --> 00:27:24,300
failure in your system handling failures

00:27:21,900 --> 00:27:25,920
appropriately is really important you

00:27:24,300 --> 00:27:28,140
want to try to keep your operations item

00:27:25,920 --> 00:27:29,700
put in and make sure you're making the

00:27:28,140 --> 00:27:32,640
right trade-off between availability and

00:27:29,700 --> 00:27:34,650
consistency right so we hope that we've

00:27:32,640 --> 00:27:37,140
given you a good foundation for trying

00:27:34,650 --> 00:27:39,120
to build your own application and before

00:27:37,140 --> 00:27:40,590
we end you do want to say that we're

00:27:39,120 --> 00:27:42,780
hiring at Yelp across the board

00:27:40,590 --> 00:27:44,040
including for data processing and if

00:27:42,780 --> 00:27:46,020
you're interested in reading about the

00:27:44,040 --> 00:27:47,880
work that we do check out our

00:27:46,020 --> 00:27:49,620
engineering blog and we also have a lot

00:27:47,880 --> 00:27:57,690
of open source libraries related to

00:27:49,620 --> 00:28:00,150
Feist on on our github thank you so if

00:27:57,690 --> 00:28:01,950
you have any questions for Amit and

00:28:00,150 --> 00:28:04,740
we there is a microphone here in this

00:28:01,950 --> 00:28:11,730
aisle I will just see if anybody wants

00:28:04,740 --> 00:28:15,300
to line up that looks like a yes go

00:28:11,730 --> 00:28:17,940
ahead I will try to make this quick at

00:28:15,300 --> 00:28:22,790
my organization we haven't gone from a

00:28:17,940 --> 00:28:26,730
batch style processing because of

00:28:22,790 --> 00:28:30,059
possibility actually the recurrence of

00:28:26,730 --> 00:28:33,720
delayed messages coming in and what that

00:28:30,059 --> 00:28:38,130
does for consistency and much of our

00:28:33,720 --> 00:28:43,500
data coming in is used for billing so is

00:28:38,130 --> 00:28:45,420
there a way of gradually going to a

00:28:43,500 --> 00:28:48,960
streaming approach but dealing with

00:28:45,420 --> 00:28:51,780
potentially 12-hour delayed messages

00:28:48,960 --> 00:28:53,550
that you need to retry or something to

00:28:51,780 --> 00:28:56,220
that effect like how how can you still

00:28:53,550 --> 00:28:58,860
do a streaming approach but I deal with

00:28:56,220 --> 00:29:00,170
this eventual consistency of nodes that

00:28:58,860 --> 00:29:05,790
might be offline for 12 hours

00:29:00,170 --> 00:29:07,830
sure so definitely many of these stream

00:29:05,790 --> 00:29:09,450
processing frameworks that exist today

00:29:07,830 --> 00:29:12,150
are moving in the direction where they

00:29:09,450 --> 00:29:14,190
do support like delays and what

00:29:12,150 --> 00:29:16,110
typically means is that Yahoo yeah

00:29:14,190 --> 00:29:17,370
basically provided with more resources I

00:29:16,110 --> 00:29:19,520
mean clearly if you want like a 12 hour

00:29:17,370 --> 00:29:21,630
delay then you want to have enough

00:29:19,520 --> 00:29:23,760
memory or buffer whatever

00:29:21,630 --> 00:29:25,590
to actually support that much state

00:29:23,760 --> 00:29:27,510
basically escape full operation at that

00:29:25,590 --> 00:29:29,550
point and so it's really comes down to

00:29:27,510 --> 00:29:31,440
the resource you throw in but definitely

00:29:29,550 --> 00:29:33,360
frameworks are beginning to abstract out

00:29:31,440 --> 00:29:35,340
things enough that you can just say okay

00:29:33,360 --> 00:29:36,690
this is my delay or this is how much

00:29:35,340 --> 00:29:40,260
delay I'm going to tolerate and then and

00:29:36,690 --> 00:29:43,140
work towards it I mean it's still kind

00:29:40,260 --> 00:29:44,520
of early on in the sense that they're

00:29:43,140 --> 00:29:46,170
not going to they're not at the stage

00:29:44,520 --> 00:29:47,970
where they're like robustly handling

00:29:46,170 --> 00:29:49,260
this but definitely things are moving in

00:29:47,970 --> 00:29:51,180
the direction where you have less to

00:29:49,260 --> 00:29:54,300
worry about and the frameworks actually

00:29:51,180 --> 00:29:55,380
take care of it okay that's all the time

00:29:54,300 --> 00:29:57,330
we have

00:29:55,380 --> 00:30:03,660
everybody please thank our we and our

00:29:57,330 --> 00:30:06,600
net for a wonderful talk okay now

00:30:03,660 --> 00:30:10,050
there's a keynote downstairs starting in

00:30:06,600 --> 00:30:13,020
about ten minutes time in expo hall c so

00:30:10,050 --> 00:30:15,480
go and get to that and PyCon we'll wrap

00:30:13,020 --> 00:30:17,370
up at that point so thank you all for

00:30:15,480 --> 00:30:19,430
being a great audience and see you next

00:30:17,370 --> 00:30:19,430

YouTube URL: https://www.youtube.com/watch?v=CHCC2ITcMfk


