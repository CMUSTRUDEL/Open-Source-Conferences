Title: Eric J  Ma   Bayesian Statistical Analysis with Python   PyCon 2017
Publication date: 2017-05-21
Playlist: PyCon 2017
Description: 
	"Speaker: Eric J. Ma

You've got some data, and now you want to analyze it with Python. You're on your way to greatness! Now the problem comes: do I do the t-test? Chi-squared test? How do I decide? In this talk, inspired by many Pythonista Bayesians (@jakevdp, @allendowney, @twiecki, @fonnesbeck) before, I will show you how you can take common statistical decision problems, formulate them as a Bayesian analysis problem, and use PyMC3 as your workhorse tool for gaining insights. This talk will be math-light and code-heavy, and if you download the slides, you'll have a simple template for more complex Bayesian analysis down the road!

Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:00,000 --> 00:00:05,759
okay let's go ahead and get started

00:00:02,220 --> 00:00:10,500
perhaps a minute early today we were

00:00:05,759 --> 00:00:14,429
privileged to have dr. Eric ma new new

00:00:10,500 --> 00:00:17,130
doctor at that barely even out of the

00:00:14,429 --> 00:00:20,300
gate he's going to talk about Bayesian

00:00:17,130 --> 00:00:22,529
statistics for us and maybe a

00:00:20,300 --> 00:00:26,269
congratulations to start this off as

00:00:22,529 --> 00:00:26,269
he's a new doctor

00:00:31,279 --> 00:00:36,809
okay thanks everybody for coming the

00:00:35,040 --> 00:00:39,300
last thing that I tweeted on Twitter

00:00:36,809 --> 00:00:42,450
my handle is up there is the slides to

00:00:39,300 --> 00:00:44,670
this note to this notebook and the sorry

00:00:42,450 --> 00:00:46,710
the URL to the HTML versions of the

00:00:44,670 --> 00:00:48,300
notebook slides and so if you go onto

00:00:46,710 --> 00:00:50,700
Twitter find my username you'll be able

00:00:48,300 --> 00:00:53,520
to find a link there alternatively the

00:00:50,700 --> 00:00:55,590
links to the slides is also online and

00:00:53,520 --> 00:00:57,180
so it's up here as well and so if you

00:00:55,590 --> 00:01:01,260
have your laptop open feel free to open

00:00:57,180 --> 00:01:04,019
the slides and follow along all right

00:01:01,260 --> 00:01:04,589
give people one ten more seconds to get

00:01:04,019 --> 00:01:07,619
that done

00:01:04,589 --> 00:01:10,020
all right I think we're good all right

00:01:07,619 --> 00:01:13,049
so my talk today is on Bayesian

00:01:10,020 --> 00:01:14,460
statistical analysis with PI MC 3 and to

00:01:13,049 --> 00:01:16,560
the best of my knowledge this room has

00:01:14,460 --> 00:01:20,880
been basil and for the past hour and so

00:01:16,560 --> 00:01:23,549
I stand on a great amount of I stand

00:01:20,880 --> 00:01:26,130
here following realizing that I have a

00:01:23,549 --> 00:01:27,960
hard act to follow and so this is my

00:01:26,130 --> 00:01:29,970
attempt at telling people about what

00:01:27,960 --> 00:01:32,100
Bayesian stats can be enabled with PI MC

00:01:29,970 --> 00:01:35,340
3 all right

00:01:32,100 --> 00:01:38,400
this talk is not going to be very math

00:01:35,340 --> 00:01:41,549
heavy and so rest assured that you won't

00:01:38,400 --> 00:01:43,560
be dizzied fun and overwhelmed with math

00:01:41,549 --> 00:01:45,600
symbols and likes this talk basically

00:01:43,560 --> 00:01:48,180
will have a lot of minimal will have

00:01:45,600 --> 00:01:50,399
very minimal field jargon there's a lot

00:01:48,180 --> 00:01:53,009
of terminology that's emerging in the

00:01:50,399 --> 00:01:55,049
field of Bayes land and so rather than

00:01:53,009 --> 00:01:56,939
focus on the terminology and what they

00:01:55,049 --> 00:01:58,860
all mean like AV testing spikes lab

00:01:56,939 --> 00:02:00,409
regression and conjugate distributions

00:01:58,860 --> 00:02:02,430
I'm not going to talk about that stuff

00:02:00,409 --> 00:02:04,259
basically I'm going to focus on the

00:02:02,430 --> 00:02:07,649
mechanics of how you do Bayesian

00:02:04,259 --> 00:02:09,239
statistical analysis with PI MC 3 I also

00:02:07,649 --> 00:02:11,129
believe in the period of principle so

00:02:09,239 --> 00:02:13,260
what's going to be covered in this talk

00:02:11,129 --> 00:02:13,800
is not necessarily going to be the most

00:02:13,260 --> 00:02:15,990
complicated

00:02:13,800 --> 00:02:17,940
fancy newest advanced thing that's out

00:02:15,990 --> 00:02:19,470
there I'm going to cover the basics and

00:02:17,940 --> 00:02:21,330
I believe that the basics will get you

00:02:19,470 --> 00:02:22,830
to 80 percent of the kinds of problems

00:02:21,330 --> 00:02:26,190
that you'll ever encounter with

00:02:22,830 --> 00:02:28,020
statistical data analysis finally there

00:02:26,190 --> 00:02:30,660
is going to be code and as I've dried

00:02:28,020 --> 00:02:33,240
run this dried run this this talk before

00:02:30,660 --> 00:02:34,920
I realize people get hung up on the code

00:02:33,240 --> 00:02:36,420
and I want put this out there don't get

00:02:34,920 --> 00:02:38,400
hung up on the code if you're following

00:02:36,420 --> 00:02:40,440
along you'll have the code in your

00:02:38,400 --> 00:02:42,810
browsers on your computers

00:02:40,440 --> 00:02:47,220
so enjoy the talk focus on Bayes and you

00:02:42,810 --> 00:02:50,220
can get the code later I'm assuming some

00:02:47,220 --> 00:02:52,890
knowledge amongst the crowd here and so

00:02:50,220 --> 00:02:55,020
first off the biggest one is that you

00:02:52,890 --> 00:02:57,600
are familiar with Python and that you

00:02:55,020 --> 00:02:59,850
know things like objects and methods and

00:02:57,600 --> 00:03:02,580
their methods and you know the context

00:02:59,850 --> 00:03:04,230
manager syntax the second thing that I'm

00:03:02,580 --> 00:03:06,450
also assuming is that you know some

00:03:04,230 --> 00:03:08,370
basic stats terminology so for example

00:03:06,450 --> 00:03:10,350
if I were to ask you for this

00:03:08,370 --> 00:03:12,540
distribution what's the mean what's the

00:03:10,350 --> 00:03:14,190
variance and what's a particular

00:03:12,540 --> 00:03:17,670
interval there you'd be able to specify

00:03:14,190 --> 00:03:19,410
that for me I'm also assuming you're

00:03:17,670 --> 00:03:22,560
familiar looking at statistical

00:03:19,410 --> 00:03:24,570
distributions and so you know what a

00:03:22,560 --> 00:03:26,790
continuous distribution is you know what

00:03:24,570 --> 00:03:28,709
a discrete distribution is you know the

00:03:26,790 --> 00:03:30,239
names of these distributions and roughly

00:03:28,709 --> 00:03:32,820
what their characteristic shapes are and

00:03:30,239 --> 00:03:34,560
you know what the support is which is

00:03:32,820 --> 00:03:37,140
the values for which they those

00:03:34,560 --> 00:03:38,970
distributions are valid in addition to

00:03:37,140 --> 00:03:39,840
that if I ask you to quantify your

00:03:38,970 --> 00:03:43,410
belief using the statistical

00:03:39,840 --> 00:03:45,600
distribution you might be inclined to

00:03:43,410 --> 00:03:47,820
say a strong strong belief has a narrow

00:03:45,600 --> 00:03:49,860
distribution and a weak belief has a

00:03:47,820 --> 00:03:51,330
very wide and flat distribution and

00:03:49,860 --> 00:03:53,220
you'll be able to sort of specify the

00:03:51,330 --> 00:03:56,160
shapes or roughly sketch out what the

00:03:53,220 --> 00:03:58,250
shapes look like for both the continuous

00:03:56,160 --> 00:04:01,320
and the discrete distribution cases

00:03:58,250 --> 00:04:03,630
alright this is a talk about Bayes

00:04:01,320 --> 00:04:05,489
Bayesian statistical analysis and all

00:04:03,630 --> 00:04:07,830
Bayesian statistical analysis talks must

00:04:05,489 --> 00:04:10,260
have the obligatory Bayes rule slide and

00:04:07,830 --> 00:04:12,180
so Bayes rule exists as a formula that

00:04:10,260 --> 00:04:15,360
has hanging up on someone's office as a

00:04:12,180 --> 00:04:17,010
neon sign and probably more advanced

00:04:15,360 --> 00:04:17,940
mathematicians and statisticians would

00:04:17,010 --> 00:04:19,470
be able to give you the mathematical

00:04:17,940 --> 00:04:22,560
definition of what's going on inside

00:04:19,470 --> 00:04:24,990
there I don't plan to do that I want to

00:04:22,560 --> 00:04:26,430
give you the intuition Bayes land is all

00:04:24,990 --> 00:04:27,480
about updating your beliefs

00:04:26,430 --> 00:04:29,280
having seen the

00:04:27,480 --> 00:04:31,530
our beliefs are modeled using

00:04:29,280 --> 00:04:34,050
statistical distributions evidence are

00:04:31,530 --> 00:04:36,000
the data and we update our beliefs

00:04:34,050 --> 00:04:39,780
update the shape of that distribution

00:04:36,000 --> 00:04:42,420
having seen the evidence because this is

00:04:39,780 --> 00:04:44,040
to talk about PI MC 3 I just wanted to

00:04:42,420 --> 00:04:46,200
give you a very very quick introduction

00:04:44,040 --> 00:04:48,480
to what PI MC 3 is if you at Crispin's

00:04:46,200 --> 00:04:50,640
Beck's talk two sessions ago you would

00:04:48,480 --> 00:04:52,140
have a very very in-depth introduction

00:04:50,640 --> 00:04:54,570
on to what primacy theory is all about

00:04:52,140 --> 00:04:56,910
basically all i want to say is that x

00:04:54,570 --> 00:04:59,100
III is this really awesome package that

00:04:56,910 --> 00:05:01,470
provides a library of statistical

00:04:59,100 --> 00:05:04,170
distributions sampling algorithms and

00:05:01,470 --> 00:05:06,420
syntax for specifying statistical models

00:05:04,170 --> 00:05:08,400
and one of its greatest features is that

00:05:06,420 --> 00:05:11,010
everything is in Python you don't have

00:05:08,400 --> 00:05:12,540
to learn a separate language in order to

00:05:11,010 --> 00:05:15,830
start specifying statistical models

00:05:12,540 --> 00:05:20,550
everything can be done with Python

00:05:15,830 --> 00:05:23,450
alright what are the what are the 80% or

00:05:20,550 --> 00:05:25,380
what are the majority class of

00:05:23,450 --> 00:05:27,240
statistical analysis problems that

00:05:25,380 --> 00:05:29,820
you'll ever encounter in my opinion they

00:05:27,240 --> 00:05:31,860
fall under two major classes the first

00:05:29,820 --> 00:05:34,710
one is called parameter estimation which

00:05:31,860 --> 00:05:36,060
is is the value of something that we're

00:05:34,710 --> 00:05:39,330
measuring some parameter that we're

00:05:36,060 --> 00:05:41,330
measuring equal to the value X the

00:05:39,330 --> 00:05:43,470
second one is a comparison between

00:05:41,330 --> 00:05:45,420
experimental groups so in other words

00:05:43,470 --> 00:05:48,690
are the treatments basically different

00:05:45,420 --> 00:05:51,330
from the controls so let's take a look

00:05:48,690 --> 00:05:53,250
at a few examples for problem type 1

00:05:51,330 --> 00:05:54,750
which is about parameter estimation as I

00:05:53,250 --> 00:05:56,430
mentioned just now it's basically

00:05:54,750 --> 00:05:59,070
answering the question is the true value

00:05:56,430 --> 00:06:01,380
equal to X given the data or given the

00:05:59,070 --> 00:06:04,260
data for the parameter of interest what

00:06:01,380 --> 00:06:06,270
is the probability distribution over the

00:06:04,260 --> 00:06:09,000
possible values of this parameter and

00:06:06,270 --> 00:06:10,890
let's look at the obligatory coin toss

00:06:09,000 --> 00:06:13,710
problem which has to happen in any

00:06:10,890 --> 00:06:16,440
statistics talk so it's basically format

00:06:13,710 --> 00:06:19,680
formulated as such I tossed my coin n

00:06:16,440 --> 00:06:23,610
times and it came up as heads h times is

00:06:19,680 --> 00:06:25,950
this coin bias alright if we

00:06:23,610 --> 00:06:28,770
parameterize the problem it basically

00:06:25,950 --> 00:06:31,830
looks like this I want to know how P the

00:06:28,770 --> 00:06:33,330
probability of tossing heads I want to

00:06:31,830 --> 00:06:36,000
know the value of P the probability of

00:06:33,330 --> 00:06:38,130
tossing heads now given n tosses an H

00:06:36,000 --> 00:06:41,310
observe heads is it probable that the

00:06:38,130 --> 00:06:43,710
value of P is close to 0.5 say

00:06:41,310 --> 00:06:45,720
for example in this region of practical

00:06:43,710 --> 00:06:47,190
equivalents save zero point four eight

00:06:45,720 --> 00:06:49,050
to zero point five to which for our

00:06:47,190 --> 00:06:52,350
purposes is basically equal to zero

00:06:49,050 --> 00:06:54,720
point five I can specify and

00:06:52,350 --> 00:06:56,400
parameterize the model using two major

00:06:54,720 --> 00:06:57,990
components one is the likelihood

00:06:56,400 --> 00:06:59,970
function the other is the prior

00:06:57,990 --> 00:07:01,440
distribution and in the diagrams that

00:06:59,970 --> 00:07:03,330
you'll see in the slides coming forward

00:07:01,440 --> 00:07:05,070
the prior distributions will always be

00:07:03,330 --> 00:07:07,140
colored in red and the likelihood

00:07:05,070 --> 00:07:10,170
functions will always have no red color

00:07:07,140 --> 00:07:13,050
so with coin tosses we can model this

00:07:10,170 --> 00:07:14,610
using a Bernoulli distribution and this

00:07:13,050 --> 00:07:17,070
Bernoulli distribution has a single

00:07:14,610 --> 00:07:20,340
parameter P that parameter P is what

00:07:17,070 --> 00:07:22,710
we're interested in right and so P we

00:07:20,340 --> 00:07:25,470
can say for example it's we don't know

00:07:22,710 --> 00:07:27,990
much about what pete be except for the

00:07:25,470 --> 00:07:30,330
fact that it should be between 0 & 1 all

00:07:27,990 --> 00:07:32,880
right since we don't know what it should

00:07:30,330 --> 00:07:34,890
be then since we don't believe a

00:07:32,880 --> 00:07:37,620
priority that it could take any

00:07:34,890 --> 00:07:39,780
particular set of values then except for

00:07:37,620 --> 00:07:41,580
the range 0 to 1 and that that it should

00:07:39,780 --> 00:07:43,350
be then it should be equal probability

00:07:41,580 --> 00:07:45,840
across 0 & 1 and therefore we use a week

00:07:43,350 --> 00:07:49,230
prior say the uniform distribution over

00:07:45,840 --> 00:07:51,450
0 1 but some people might disagree with

00:07:49,230 --> 00:07:54,050
that some people might say oh I actually

00:07:51,450 --> 00:07:56,820
believe you know most coins are kind of

00:07:54,050 --> 00:07:58,770
most coins are kind of built to be fair

00:07:56,820 --> 00:08:02,310
coins and the vast majority of coins

00:07:58,770 --> 00:08:04,860
that we see are are not biased coins and

00:08:02,310 --> 00:08:07,830
so we might say a priori without having

00:08:04,860 --> 00:08:11,580
seen the data I believe that this this

00:08:07,830 --> 00:08:13,590
value of P should be centered on point 5

00:08:11,580 --> 00:08:15,780
and have less probability mass on the

00:08:13,590 --> 00:08:17,280
edges alright so we'll use a we can use

00:08:15,780 --> 00:08:19,500
a non-uniform distribution for that

00:08:17,280 --> 00:08:22,890
basically modeling choices we have to

00:08:19,500 --> 00:08:26,520
talk about the data looks something like

00:08:22,890 --> 00:08:28,950
this so I toss the coin 30 times and it

00:08:26,520 --> 00:08:31,430
comes up 11 how many people here think

00:08:28,950 --> 00:08:35,450
it's a biased coin raise your hand oh

00:08:31,430 --> 00:08:39,270
this is good very good

00:08:35,450 --> 00:08:41,610
okay here's what the code looks like in

00:08:39,270 --> 00:08:44,460
PI MC 3 for investigating this question

00:08:41,610 --> 00:08:46,590
is e 3 like I mentioned earlier uses the

00:08:44,460 --> 00:08:47,940
so as I mentioned earlier the context

00:08:46,590 --> 00:08:50,370
manager syntax is going to feature

00:08:47,940 --> 00:08:53,550
heavily and PI MC 3 uses the context

00:08:50,370 --> 00:08:54,779
manager syntax at the top one of the

00:08:53,550 --> 00:08:56,579
cool things about PI M

00:08:54,779 --> 00:08:57,749
all of the statistical distributions

00:08:56,579 --> 00:09:01,199
that you might think about they're all

00:08:57,749 --> 00:09:03,209
implemented as Python objects that's why

00:09:01,199 --> 00:09:05,670
you can use PI and z3 without having to

00:09:03,209 --> 00:09:07,769
learn a new syntax or a new language to

00:09:05,670 --> 00:09:09,600
do statistical modeling basically you're

00:09:07,769 --> 00:09:12,959
just playing with Python objects all

00:09:09,600 --> 00:09:15,779
right so I can specify specify my prior

00:09:12,959 --> 00:09:17,790
on the parameter P using the uniform

00:09:15,779 --> 00:09:20,430
distribution and it's got the parameter

00:09:17,790 --> 00:09:22,259
0 and 1 and I named it P and my

00:09:20,430 --> 00:09:24,540
likelihood function is the Bernoulli

00:09:22,259 --> 00:09:26,670
distribution I pass in the data because

00:09:24,540 --> 00:09:28,410
that's what I've observed and I pass in

00:09:26,670 --> 00:09:32,189
the prior distribution which is

00:09:28,410 --> 00:09:34,410
specified above what happens after that

00:09:32,189 --> 00:09:36,209
is we hit the MCMC entrance button and

00:09:34,410 --> 00:09:39,059
you don't have to worry what this is all

00:09:36,209 --> 00:09:41,100
about because the way I see it the right

00:09:39,059 --> 00:09:42,930
abstraction for programmers is it does

00:09:41,100 --> 00:09:44,970
the fancy math that lazy programmers

00:09:42,930 --> 00:09:47,850
don't have to worry about and so you hit

00:09:44,970 --> 00:09:51,480
the inference button and results pop out

00:09:47,850 --> 00:09:53,550
and so the result of this observing the

00:09:51,480 --> 00:09:56,100
data and updating our beliefs

00:09:53,550 --> 00:09:59,220
having seen the data is as such

00:09:56,100 --> 00:10:01,529
basically the value of P is centered

00:09:59,220 --> 00:10:04,589
around say 0.4 and has some range

00:10:01,529 --> 00:10:06,209
between say 0.2 and point 6 we can go a

00:10:04,589 --> 00:10:07,980
bit more quantitative by specifying

00:10:06,209 --> 00:10:12,059
particular intervals and particular

00:10:07,980 --> 00:10:15,360
statistics so for example we can call we

00:10:12,059 --> 00:10:17,339
can call we can observe that so we can

00:10:15,360 --> 00:10:20,459
compute the mean of the posterior

00:10:17,339 --> 00:10:23,519
distribution and it's 0.37 7 in addition

00:10:20,459 --> 00:10:25,740
we can also ask what's the 95% highest

00:10:23,519 --> 00:10:28,139
posterior density on that distribution

00:10:25,740 --> 00:10:30,449
which basically corresponds to what you

00:10:28,139 --> 00:10:33,029
might think it's the it's that given the

00:10:30,449 --> 00:10:35,129
data there's a 95% probability that the

00:10:33,029 --> 00:10:39,569
true parameter falls inside the range of

00:10:35,129 --> 00:10:41,309
that of that 95 HP d now here's the best

00:10:39,569 --> 00:10:44,100
part remember earlier on I had specified

00:10:41,309 --> 00:10:48,120
a priori for my purposes practically

00:10:44,100 --> 00:10:50,819
speaking 0.48 2.5 2 is equal to 0.5 all

00:10:48,120 --> 00:10:53,069
right so what's the relationship of this

00:10:50,819 --> 00:10:56,550
region of practical equivalence in red

00:10:53,069 --> 00:10:58,019
to the 95 percent HPD which is the black

00:10:56,550 --> 00:11:01,350
stick bar on the x-axis

00:10:58,019 --> 00:11:04,290
well basically encompasses the entire

00:11:01,350 --> 00:11:06,120
rope and the region of practical

00:11:04,290 --> 00:11:07,070
equivalence only occupies a small

00:11:06,120 --> 00:11:10,850
fraction

00:11:07,070 --> 00:11:12,680
of the 95% HPD we're not really sure 30

00:11:10,850 --> 00:11:14,090
coin tosses some of you might have

00:11:12,680 --> 00:11:16,280
thought that was enough to conclude that

00:11:14,090 --> 00:11:19,790
it was a bias or an unbiased coin

00:11:16,280 --> 00:11:22,370
actually no it's not we need to get more

00:11:19,790 --> 00:11:24,710
data and so Bayesian and analysis helps

00:11:22,370 --> 00:11:26,600
us quantify the uncertainty around the

00:11:24,710 --> 00:11:28,580
parameter and helps us with the right

00:11:26,600 --> 00:11:29,960
frame of mind helps us to decide whether

00:11:28,580 --> 00:11:32,990
we need to collect more data or if we

00:11:29,960 --> 00:11:34,910
should stop collecting data all right so

00:11:32,990 --> 00:11:37,310
the pattern basically looks like this we

00:11:34,910 --> 00:11:39,380
take our problem and we think about it

00:11:37,310 --> 00:11:40,970
completely in terms of statistical

00:11:39,380 --> 00:11:42,950
distributions interacting with one

00:11:40,970 --> 00:11:45,200
another through a particular model

00:11:42,950 --> 00:11:46,880
structure and that's where we actually

00:11:45,200 --> 00:11:48,650
have to spend the most amount of time

00:11:46,880 --> 00:11:50,360
you don't spend time writing time to see

00:11:48,650 --> 00:11:52,310
three code you actually spend more time

00:11:50,360 --> 00:11:53,840
specifying and justifying the model

00:11:52,310 --> 00:11:54,950
structure why did you choose these

00:11:53,840 --> 00:11:56,630
priors why did you choose this

00:11:54,950 --> 00:11:57,830
likelihood function why is this prior

00:11:56,630 --> 00:11:59,510
being passed into that likelihood

00:11:57,830 --> 00:12:02,210
function not the other one you have to

00:11:59,510 --> 00:12:04,250
justify your model structure once that's

00:12:02,210 --> 00:12:06,140
done and that's down pat you can

00:12:04,250 --> 00:12:08,090
actually very quickly write the model in

00:12:06,140 --> 00:12:10,130
PI MC 3 hit the inference button and

00:12:08,090 --> 00:12:12,740
interpret the data based on the

00:12:10,130 --> 00:12:13,960
posterior distributions suppose for

00:12:12,740 --> 00:12:16,360
example you had new information

00:12:13,960 --> 00:12:18,470
completely unrelated to the data that

00:12:16,360 --> 00:12:19,820
told you that you might need to change

00:12:18,470 --> 00:12:21,380
the model structure that's where you can

00:12:19,820 --> 00:12:23,980
iterate and loop on the model

00:12:21,380 --> 00:12:27,860
specification on the model structure

00:12:23,980 --> 00:12:29,090
specification and interpretation ok with

00:12:27,860 --> 00:12:30,980
that I'm going to go to the second

00:12:29,090 --> 00:12:32,900
example which is called the Kemp which I

00:12:30,980 --> 00:12:34,280
call the chemical activity problem this

00:12:32,900 --> 00:12:36,320
is a problem that's close to my heart

00:12:34,280 --> 00:12:37,340
because I worked in the flu world even

00:12:36,320 --> 00:12:39,140
even though I wasn't in the chem

00:12:37,340 --> 00:12:41,480
informatics world it's still a problem

00:12:39,140 --> 00:12:43,790
that we kept our eyes on so it worked

00:12:41,480 --> 00:12:46,390
the problem is specified as such I have

00:12:43,790 --> 00:12:50,030
a newly developed molecule called X and

00:12:46,390 --> 00:12:53,390
I want to know how good X is in stopping

00:12:50,030 --> 00:12:54,920
the replication of flu all right so how

00:12:53,390 --> 00:12:57,320
do would you do the measurement

00:12:54,920 --> 00:13:00,110
experiment to measure how good this

00:12:57,320 --> 00:13:02,060
molecule is it essentially follows this

00:13:00,110 --> 00:13:04,340
logic I test the range of concentrations

00:13:02,060 --> 00:13:07,730
of X and I measure the flu activity of

00:13:04,340 --> 00:13:09,410
that drug measure flu activity upon

00:13:07,730 --> 00:13:11,450
treatment with that drug at that

00:13:09,410 --> 00:13:13,370
concentration and what we're looking for

00:13:11,450 --> 00:13:15,770
is the ic50

00:13:13,370 --> 00:13:17,390
which is the concentration of X that

00:13:15,770 --> 00:13:20,240
causes the replication rate of the

00:13:17,390 --> 00:13:22,620
viruses to be cut in half

00:13:20,240 --> 00:13:27,270
the data essentially looks something

00:13:22,620 --> 00:13:28,740
like this we might have so what we're

00:13:27,270 --> 00:13:31,260
seeing on is on the x-axis the

00:13:28,740 --> 00:13:33,210
concentration of the molecule X and on

00:13:31,260 --> 00:13:35,880
the y-axis it's the activity of the flu

00:13:33,210 --> 00:13:38,340
virus and as we apply more drug we

00:13:35,880 --> 00:13:39,990
expect the activity to go down a good

00:13:38,340 --> 00:13:41,790
drug will follow the curve that looks

00:13:39,990 --> 00:13:42,810
like the red line and a bad drug will

00:13:41,790 --> 00:13:44,640
follow the curve that looks like the

00:13:42,810 --> 00:13:47,610
blue line because the good drug only

00:13:44,640 --> 00:13:49,830
needs a small amount of only needs a

00:13:47,610 --> 00:13:51,810
small amount of activity a small

00:13:49,830 --> 00:13:53,280
concentration in order to cut its cut

00:13:51,810 --> 00:13:55,200
the replication activity of flu in half

00:13:53,280 --> 00:13:57,210
whereas a bad drug needs a large amount

00:13:55,200 --> 00:14:02,220
so low ic50

00:13:57,210 --> 00:14:03,990
good drug hi I see 50 bad drug and so

00:14:02,220 --> 00:14:06,330
the parametrized problem essentially

00:14:03,990 --> 00:14:09,150
looks like this given my curve dose

00:14:06,330 --> 00:14:10,770
response curve what is the ic50 value of

00:14:09,150 --> 00:14:11,490
the chemical and the uncertainties

00:14:10,770 --> 00:14:15,090
surrounding it

00:14:11,490 --> 00:14:17,280
I might specify the model to be as such

00:14:15,090 --> 00:14:19,230
so we know that the shape of the curve

00:14:17,280 --> 00:14:20,940
from our background knowledge we know

00:14:19,230 --> 00:14:23,310
that the shape of the curve is specified

00:14:20,940 --> 00:14:25,440
by what's in this dotted box here which

00:14:23,310 --> 00:14:27,210
we call the link function and there's

00:14:25,440 --> 00:14:28,830
this parameter called beta which I don't

00:14:27,210 --> 00:14:31,410
really care about so we call it a

00:14:28,830 --> 00:14:33,690
nuisance parameter in this case I'm more

00:14:31,410 --> 00:14:37,200
about the parameter ic50 which is on the

00:14:33,690 --> 00:14:38,310
denominator and this is the thing that

00:14:37,200 --> 00:14:41,220
we're actually this

00:14:38,310 --> 00:14:42,750
mahtim this thing inside the link

00:14:41,220 --> 00:14:44,390
function box is what we're measuring

00:14:42,750 --> 00:14:48,690
right we're measuring the concentration

00:14:44,390 --> 00:14:52,350
and its so we can model this with a

00:14:48,690 --> 00:14:53,370
normal distribution at the end now what

00:14:52,350 --> 00:14:55,290
about these priors

00:14:53,370 --> 00:14:57,510
we know beta has to be sort of

00:14:55,290 --> 00:14:59,310
positively valued things so we might say

00:14:57,510 --> 00:15:01,380
let's use a half normal distribution or

00:14:59,310 --> 00:15:03,030
let's use a half Koshi distribution that

00:15:01,380 --> 00:15:05,130
only has positives that only takes on

00:15:03,030 --> 00:15:06,750
positive values and for ic50

00:15:05,130 --> 00:15:09,510
say for example like in the previous

00:15:06,750 --> 00:15:11,580
example we're in log ten land it could

00:15:09,510 --> 00:15:13,770
then take any value from negative

00:15:11,580 --> 00:15:17,820
whatever to positive whatever we'll call

00:15:13,770 --> 00:15:19,560
it an improper flat distribution the

00:15:17,820 --> 00:15:22,590
data would look something like this in

00:15:19,560 --> 00:15:24,330
log ten land I have my molecules and it

00:15:22,590 --> 00:15:25,230
doesn't follow the exact shape shape of

00:15:24,330 --> 00:15:26,640
the curve because there's a bit of

00:15:25,230 --> 00:15:28,560
uncertainty surrounding that so that

00:15:26,640 --> 00:15:29,760
that little bit of uncertainty causes a

00:15:28,560 --> 00:15:33,010
little bit of uncertainty in our

00:15:29,760 --> 00:15:36,029
measured ic50 value the

00:15:33,010 --> 00:15:39,970
looks something like this I can specify

00:15:36,029 --> 00:15:42,370
my beta to be to have to follow the half

00:15:39,970 --> 00:15:45,160
normal distribution the ic50 and log10

00:15:42,370 --> 00:15:47,199
land to have a flat distribution and

00:15:45,160 --> 00:15:49,269
here's the best part we've got a link

00:15:47,199 --> 00:15:51,399
function and we can do math with these

00:15:49,269 --> 00:15:53,769
distribution objects and so that makes

00:15:51,399 --> 00:15:57,310
this specification of link functions

00:15:53,769 --> 00:15:58,870
really easy we have the likelihood we

00:15:57,310 --> 00:16:00,279
model it with a normal distribution and

00:15:58,870 --> 00:16:02,560
here's another really awesome thing with

00:16:00,279 --> 00:16:05,019
x III you can do deterministic

00:16:02,560 --> 00:16:08,230
transformations of your variables so I

00:16:05,019 --> 00:16:10,029
had log10 land for the ic50 values and

00:16:08,230 --> 00:16:12,130
so I want to transform it back to non

00:16:10,029 --> 00:16:16,930
log10 land and so I do a deterministic

00:16:12,130 --> 00:16:19,320
log 10 10 to the power transform after

00:16:16,930 --> 00:16:21,370
that hit the MCMC inference button

00:16:19,320 --> 00:16:22,930
complicated math happens for lazy

00:16:21,370 --> 00:16:25,360
computer scientists and software

00:16:22,930 --> 00:16:28,000
engineers and we get this trace plot

00:16:25,360 --> 00:16:31,000
which is the result of our MCMC sampling

00:16:28,000 --> 00:16:32,980
having seen the data in log 10 land it

00:16:31,000 --> 00:16:35,980
follows this particular distribution and

00:16:32,980 --> 00:16:37,569
in ic50 land non log 10 land it has this

00:16:35,980 --> 00:16:39,760
particular distribution and because it's

00:16:37,569 --> 00:16:41,980
a deterministic transform they

00:16:39,760 --> 00:16:45,310
essentially all have the same rough same

00:16:41,980 --> 00:16:47,790
shape all right so let's do some

00:16:45,310 --> 00:16:52,540
interpretation of the results given the

00:16:47,790 --> 00:16:54,130
given the posterior distributions so if

00:16:52,540 --> 00:16:54,940
we plot the posterior distributions it

00:16:54,130 --> 00:16:56,680
looks something like that

00:16:54,940 --> 00:16:59,949
and I'll read off the values for you

00:16:56,680 --> 00:17:02,290
it's mean 2.2 to millimolar x' with the

00:16:59,949 --> 00:17:04,329
range 0 to point zero one six to two

00:17:02,290 --> 00:17:06,939
point four to nine for the ic50 values

00:17:04,329 --> 00:17:09,370
what does this really mean if it's in

00:17:06,939 --> 00:17:11,829
the millimolar order of magnitude range

00:17:09,370 --> 00:17:13,689
it's kind of a bad chemical we got the

00:17:11,829 --> 00:17:15,939
uncertainty but because the uncertainty

00:17:13,689 --> 00:17:17,980
already encompasses this millimolar

00:17:15,939 --> 00:17:19,900
range and we know millimolar ic50 s are

00:17:17,980 --> 00:17:22,419
kind of a bad idea for drugs

00:17:19,900 --> 00:17:24,160
well that uncertainty may not

00:17:22,419 --> 00:17:25,959
necessarily matter here particularly

00:17:24,160 --> 00:17:29,980
because of the scale of the badness of

00:17:25,959 --> 00:17:31,630
this molecule nonetheless we had the

00:17:29,980 --> 00:17:33,549
uncertainty and that uncertainty can be

00:17:31,630 --> 00:17:35,500
propagated down to other things we can

00:17:33,549 --> 00:17:37,390
also likewise compute uncertainty for

00:17:35,500 --> 00:17:41,950
other interesting things as I'll show in

00:17:37,390 --> 00:17:43,990
the coming examples ok so that was

00:17:41,950 --> 00:17:46,120
problem type 1 which is the estimation

00:17:43,990 --> 00:17:46,910
of parameters now I'm going to move on

00:17:46,120 --> 00:17:49,130
to problems

00:17:46,910 --> 00:17:52,210
- which is how do we do comparison

00:17:49,130 --> 00:17:56,000
between treatment groups using pi MC 3

00:17:52,210 --> 00:17:57,950
the first example is a drug IQ problem

00:17:56,000 --> 00:18:01,370
it's famous in Basel and it was

00:17:57,950 --> 00:18:03,650
published in by John kreski in 2013 PI M

00:18:01,370 --> 00:18:05,420
C 3 s documentation has this example up

00:18:03,650 --> 00:18:07,550
there and for the purposes of this talk

00:18:05,420 --> 00:18:09,970
I actually did a few modifications to

00:18:07,550 --> 00:18:13,010
highlight some points about Bayes land

00:18:09,970 --> 00:18:16,400
alright how would this experiment be

00:18:13,010 --> 00:18:18,110
conducted as you might imagine we might

00:18:16,400 --> 00:18:19,970
have a pool of participants and will

00:18:18,110 --> 00:18:23,270
randomly partition them into two groups

00:18:19,970 --> 00:18:25,400
one receiving the drug and one receiving

00:18:23,270 --> 00:18:27,260
the placebo and then what we do

00:18:25,400 --> 00:18:30,460
afterwards as we measure the IQ scores

00:18:27,260 --> 00:18:34,430
of each of those participants after the

00:18:30,460 --> 00:18:36,560
application of the drug because we have

00:18:34,430 --> 00:18:38,900
two treatment groups we'll probably want

00:18:36,560 --> 00:18:41,390
to have two likelihood functions one

00:18:38,900 --> 00:18:43,040
likelihood function for the drug treated

00:18:41,390 --> 00:18:46,100
group and one likelihood function for

00:18:43,040 --> 00:18:47,780
the control group they both are modeled

00:18:46,100 --> 00:18:49,550
with T distributions and the T

00:18:47,780 --> 00:18:51,800
distribution takes three parameters it

00:18:49,550 --> 00:18:54,140
takes the MU which is the mean of that

00:18:51,800 --> 00:18:56,390
distribution and it has the Sigma which

00:18:54,140 --> 00:18:57,980
is the variance of that distribution and

00:18:56,390 --> 00:19:00,020
has this nuisance parameter which is

00:18:57,980 --> 00:19:02,030
shared across the two models called nu

00:19:00,020 --> 00:19:04,220
I'm not really interested in it I'm more

00:19:02,030 --> 00:19:07,310
interested in mu but we need to model it

00:19:04,220 --> 00:19:09,530
anyways and so modeling choices I can

00:19:07,310 --> 00:19:12,320
say let's model the Mew with a wide

00:19:09,530 --> 00:19:13,760
normal distribution the Sigma always has

00:19:12,320 --> 00:19:16,010
to be positive so we'll use a half's

00:19:13,760 --> 00:19:20,330
Koshi distribution which has this really

00:19:16,010 --> 00:19:22,070
high tails and nu commonly use the

00:19:20,330 --> 00:19:24,200
distribution for modeling this as the

00:19:22,070 --> 00:19:26,960
modeling the prior is the exponential

00:19:24,200 --> 00:19:28,520
distribution all right what does the

00:19:26,960 --> 00:19:31,910
data look like the data looks something

00:19:28,520 --> 00:19:34,220
like that and so we have in blue the

00:19:31,910 --> 00:19:35,810
cumulative distribution curve of all of

00:19:34,220 --> 00:19:37,370
the participants who took the drug and

00:19:35,810 --> 00:19:39,170
in green we have the cumulative

00:19:37,370 --> 00:19:41,480
distribution curve of all the

00:19:39,170 --> 00:19:43,970
participants who took the placebo there

00:19:41,480 --> 00:19:47,000
are 47 and 42 respectively and if you

00:19:43,970 --> 00:19:50,330
look at it ha looks like hmm the drug

00:19:47,000 --> 00:19:52,670
the median IQ of drug treated

00:19:50,330 --> 00:19:54,500
participants was kind of shifted to the

00:19:52,670 --> 00:19:57,560
right which means it's higher than the

00:19:54,500 --> 00:19:58,630
placebo treated participants this could

00:19:57,560 --> 00:20:02,050
be a cool

00:19:58,630 --> 00:20:04,780
all right let's look at the code with

00:20:02,050 --> 00:20:06,670
the code again we're using normal we're

00:20:04,780 --> 00:20:08,500
using Python objects to model the

00:20:06,670 --> 00:20:10,570
distributions and we're doing a few

00:20:08,500 --> 00:20:14,080
things as I'm highlighting down here

00:20:10,570 --> 00:20:16,180
with deterministic transforms of some of

00:20:14,080 --> 00:20:17,530
the variables so for example we can

00:20:16,180 --> 00:20:19,780
compute the difference in the means

00:20:17,530 --> 00:20:21,310
right and use that as a way of deciding

00:20:19,780 --> 00:20:24,010
is one treatment better than the other

00:20:21,310 --> 00:20:26,740
in addition we can do a computation of

00:20:24,010 --> 00:20:28,540
the effect size and again use that as a

00:20:26,740 --> 00:20:33,040
way of determining whether one treatment

00:20:28,540 --> 00:20:36,070
is better than the other we then hit the

00:20:33,040 --> 00:20:37,840
MCMC inference button and again

00:20:36,070 --> 00:20:39,850
complicated math happens for lazy

00:20:37,840 --> 00:20:41,970
programmers and the results look

00:20:39,850 --> 00:20:44,740
something like this there's this is the

00:20:41,970 --> 00:20:46,720
difference in means for the drug treated

00:20:44,740 --> 00:20:48,970
patients oh sorry this is the mean IQ

00:20:46,720 --> 00:20:50,530
for drug treatment participants and this

00:20:48,970 --> 00:20:53,230
is the mean for the placebo treated

00:20:50,530 --> 00:20:55,660
participants and if we plot the

00:20:53,230 --> 00:20:57,940
posterior distributions focusing on that

00:20:55,660 --> 00:20:59,530
bottom plot over here we can compute the

00:20:57,940 --> 00:21:00,700
difference of means not just a

00:20:59,530 --> 00:21:03,100
difference of means but also the

00:21:00,700 --> 00:21:04,780
uncertainty surrounding the difference

00:21:03,100 --> 00:21:06,220
of means and this is a real feature

00:21:04,780 --> 00:21:09,310
because the difference of means it's

00:21:06,220 --> 00:21:11,470
about two IQ points but it could also be

00:21:09,310 --> 00:21:13,780
anywhere in the 95% confidence

00:21:11,470 --> 00:21:17,680
incredible interval or HVD from zero

00:21:13,780 --> 00:21:20,260
point five to four point six so I did

00:21:17,680 --> 00:21:22,270
the thing I'm in I'm a die-hard vision

00:21:20,260 --> 00:21:24,370
but I still went ahead and computed the

00:21:22,270 --> 00:21:27,460
p-value for this thing and it's point O

00:21:24,370 --> 00:21:29,890
two and oh my gosh this could be like

00:21:27,460 --> 00:21:31,150
big right it'd be a science paper it

00:21:29,890 --> 00:21:35,020
could be a multi-million dollar drug

00:21:31,150 --> 00:21:36,670
right hold on let's take a look at the

00:21:35,020 --> 00:21:38,770
posterior distributions of the Meuse and

00:21:36,670 --> 00:21:41,350
actually look at the puss and not only

00:21:38,770 --> 00:21:44,200
that let's also take a look at the

00:21:41,350 --> 00:21:48,220
effect size and the actual magnitude of

00:21:44,200 --> 00:21:50,050
the effect the average so as you can see

00:21:48,220 --> 00:21:52,270
the oh there's a bit of this is an

00:21:50,050 --> 00:21:55,240
alternate view of the two distributions

00:21:52,270 --> 00:21:56,980
in which we summarized them into what we

00:21:55,240 --> 00:21:59,530
call a forest plot and the forest plot

00:21:56,980 --> 00:22:02,200
is actually done as such for the

00:21:59,530 --> 00:22:03,970
posterior distribution the 95 percent

00:22:02,200 --> 00:22:06,190
highest posterior density is the thin

00:22:03,970 --> 00:22:08,380
line the interquartile range is the

00:22:06,190 --> 00:22:10,600
thicker line and the dot is the median

00:22:08,380 --> 00:22:12,580
of that distribution and what this

00:22:10,600 --> 00:22:14,440
allows what the forest plot allows is

00:22:12,580 --> 00:22:16,330
allows for us to be able to directly

00:22:14,440 --> 00:22:19,720
compare these two distributions on the

00:22:16,330 --> 00:22:23,470
same plot so does this look like a good

00:22:19,720 --> 00:22:25,750
drug hmm let's think about that let's

00:22:23,470 --> 00:22:27,309
look at the effect size to the effect

00:22:25,750 --> 00:22:29,200
size is an important metric because

00:22:27,309 --> 00:22:30,760
there are ways of computing it I'm not

00:22:29,200 --> 00:22:32,529
going to belabor it over here but

00:22:30,760 --> 00:22:36,669
basically I computed the Cohen's D

00:22:32,529 --> 00:22:38,529
effect size score and computed also the

00:22:36,669 --> 00:22:40,149
uncertainty surrounding it so if you

00:22:38,529 --> 00:22:43,539
look at the mean effect size its point

00:22:40,149 --> 00:22:45,639
four it would be in traditional land I'm

00:22:43,539 --> 00:22:48,539
not going to say that word it will be in

00:22:45,639 --> 00:22:50,889
traditional land a medium effect size

00:22:48,539 --> 00:22:53,289
but if you look at the uncertainty it's

00:22:50,889 --> 00:22:55,450
actually anywhere from nothing to really

00:22:53,289 --> 00:22:58,029
large and we're not really sure at all

00:22:55,450 --> 00:22:59,950
given the data and if you think about it

00:22:58,029 --> 00:23:02,529
even more the improvement in IQ is

00:22:59,950 --> 00:23:05,820
anywhere from basically zero IQ points

00:23:02,529 --> 00:23:09,190
to four IQ points and this is like not

00:23:05,820 --> 00:23:11,110
biologically relevant at all when we

00:23:09,190 --> 00:23:13,210
talk about improvements in IQ the thing

00:23:11,110 --> 00:23:15,159
that really matters is having like 10 to

00:23:13,210 --> 00:23:17,320
20 points of IQ jump and here we're

00:23:15,159 --> 00:23:20,289
talking about statistically significant

00:23:17,320 --> 00:23:23,110
point oh two two points to four point

00:23:20,289 --> 00:23:25,149
zero to four points and IQ improvement I

00:23:23,110 --> 00:23:27,309
don't think this is a good drug I

00:23:25,149 --> 00:23:29,820
actually think it's snake oil so we

00:23:27,309 --> 00:23:32,620
shouldn't go out and sell this thing

00:23:29,820 --> 00:23:34,899
okay that was interpretation for example

00:23:32,620 --> 00:23:37,870
one let's talk about example two which

00:23:34,899 --> 00:23:39,639
is the phone sterilization problem this

00:23:37,870 --> 00:23:42,159
is based on actual research that's being

00:23:39,639 --> 00:23:43,620
conducted at MIT one of my friends who

00:23:42,159 --> 00:23:46,120
works on the eighth floor of my building

00:23:43,620 --> 00:23:47,470
she works in an animal lab and so she

00:23:46,120 --> 00:23:49,690
has to go in and out of the animal

00:23:47,470 --> 00:23:52,510
facility a lot and she has to sterilize

00:23:49,690 --> 00:23:54,429
her phone before going in and out so the

00:23:52,510 --> 00:23:56,260
experimental design basically would look

00:23:54,429 --> 00:23:58,149
probably as you might think as well you

00:23:56,260 --> 00:24:01,480
gather a lot of large pool of volunteers

00:23:58,149 --> 00:24:04,899
phones mine included and you randomly

00:24:01,480 --> 00:24:07,029
assign the fold phones to four to one of

00:24:04,899 --> 00:24:09,340
six different groups where we have four

00:24:07,029 --> 00:24:11,919
fancy methods and two control methods

00:24:09,340 --> 00:24:14,409
the experiment essentially is I swab the

00:24:11,919 --> 00:24:16,539
phone before the treatment with the

00:24:14,409 --> 00:24:19,120
fancy method or the control method and I

00:24:16,539 --> 00:24:21,639
swab it after the treatment I ask and I

00:24:19,120 --> 00:24:23,880
try to grow bacteria and I ask then how

00:24:21,639 --> 00:24:26,620
many bacterial colonies formed before

00:24:23,880 --> 00:24:28,510
treatment with the sterilization device

00:24:26,620 --> 00:24:30,970
and how many colonies formed after and

00:24:28,510 --> 00:24:34,210
we compared those counts so the data

00:24:30,970 --> 00:24:37,510
looks something like this prior to

00:24:34,210 --> 00:24:39,340
treatment I have a bunch of colonies I

00:24:37,510 --> 00:24:41,140
have a bunch of phones and each phone I

00:24:39,340 --> 00:24:43,540
count the number of bacterial colonies

00:24:41,140 --> 00:24:44,950
that formed and so there's a bunch of

00:24:43,540 --> 00:24:46,840
people who have really really dirty

00:24:44,950 --> 00:24:48,970
phones and a bunch of people who for

00:24:46,840 --> 00:24:52,390
some reason have really pristine phones

00:24:48,970 --> 00:24:54,520
and they form this distribution and post

00:24:52,390 --> 00:24:57,160
treatment after treatment we can observe

00:24:54,520 --> 00:25:01,000
whether the amount of bacterial colonies

00:24:57,160 --> 00:25:02,530
that are observed goes down or not how

00:25:01,000 --> 00:25:04,990
do we model this with a statistical

00:25:02,530 --> 00:25:07,150
distribution a simple way to do it is to

00:25:04,990 --> 00:25:09,880
say that the treatment before and the

00:25:07,150 --> 00:25:11,890
treatment after the since they are since

00:25:09,880 --> 00:25:13,690
we are measuring counts will use the

00:25:11,890 --> 00:25:16,420
Poisson distribution to model that data

00:25:13,690 --> 00:25:17,530
and the Poisson distribution for the

00:25:16,420 --> 00:25:19,240
Poisson distribution were most

00:25:17,530 --> 00:25:22,030
interested in then the parameter lambda

00:25:19,240 --> 00:25:23,920
which is which specifies sort of the the

00:25:22,030 --> 00:25:26,050
mean position of that distribution and

00:25:23,920 --> 00:25:28,840
we don't know what to believe about it

00:25:26,050 --> 00:25:31,840
and therefore we use a really really

00:25:28,840 --> 00:25:34,150
wide discrete uniform distribution from

00:25:31,840 --> 00:25:37,510
that we can then compute the percentage

00:25:34,150 --> 00:25:40,920
change of colony counts by using this

00:25:37,510 --> 00:25:44,290
simple formula pre- post divided by pre

00:25:40,920 --> 00:25:46,510
the code looks something like this and I

00:25:44,290 --> 00:25:48,309
want to emphasize here that this example

00:25:46,510 --> 00:25:50,350
is a way that we can highlight how we

00:25:48,309 --> 00:25:53,260
can do multiple treatments and multiple

00:25:50,350 --> 00:25:55,240
controls in a really concise way using

00:25:53,260 --> 00:25:57,550
fancy indexing syntax in PI and z3

00:25:55,240 --> 00:26:01,150
because basically Pi mc3 is working with

00:25:57,550 --> 00:26:02,830
matrices and tensors of data and so I'm

00:26:01,150 --> 00:26:03,970
not going to belabor it over here you

00:26:02,830 --> 00:26:07,780
can go back and take a look at the

00:26:03,970 --> 00:26:11,770
example online and let's move on with

00:26:07,780 --> 00:26:13,570
what's with us with the science so note

00:26:11,770 --> 00:26:16,690
here I've also got the deterministic

00:26:13,570 --> 00:26:19,450
computation of my percentage change and

00:26:16,690 --> 00:26:21,220
it's at the bottom and so I can compute

00:26:19,450 --> 00:26:23,020
then not only the percentage change as a

00:26:21,220 --> 00:26:26,470
number but also the uncertainty

00:26:23,020 --> 00:26:29,110
surrounding it all right we hit the

00:26:26,470 --> 00:26:31,510
inference button fancy math happens for

00:26:29,110 --> 00:26:34,780
lazy programmers and this is what the

00:26:31,510 --> 00:26:37,000
result looks like I have on the top the

00:26:34,780 --> 00:26:38,770
four fancy methods and then the

00:26:37,000 --> 00:26:39,950
percentage reduction in bacterial

00:26:38,770 --> 00:26:41,870
colonies for the four five

00:26:39,950 --> 00:26:44,029
see methods and the two control methods

00:26:41,870 --> 00:26:46,880
at the bottom the four fancy methods at

00:26:44,029 --> 00:26:49,760
the top given the data fancy method one

00:26:46,880 --> 00:26:52,190
had a hundred percent reduction in the

00:26:49,760 --> 00:26:54,230
number of bacterial colonies formed with

00:26:52,190 --> 00:26:57,080
zero uncertainty and that's perfectly

00:26:54,230 --> 00:26:59,809
allowable and so yeah given the data it

00:26:57,080 --> 00:27:02,480
is a hundred percent effective at

00:26:59,809 --> 00:27:04,130
killing off bacteria on our phones on

00:27:02,480 --> 00:27:06,380
the other hand there are some other

00:27:04,130 --> 00:27:08,059
fancy methods that sort of don't even

00:27:06,380 --> 00:27:09,740
really compete well with the control

00:27:08,059 --> 00:27:12,110
methods and they're probably trying to

00:27:09,740 --> 00:27:15,500
say sell you snake oil as well so watch

00:27:12,110 --> 00:27:17,090
out all right so throughout this talk

00:27:15,500 --> 00:27:19,760
what have you seen you've seen two

00:27:17,090 --> 00:27:21,380
things two major problems major classes

00:27:19,760 --> 00:27:23,120
of problems which are parameter

00:27:21,380 --> 00:27:25,490
estimation and control versus treatment

00:27:23,120 --> 00:27:28,039
comparison in the parameter estimation

00:27:25,490 --> 00:27:30,139
portion I tried to illustrate to you how

00:27:28,039 --> 00:27:33,320
you can use PI mc3 to specify your

00:27:30,139 --> 00:27:34,970
priors as statistical distributions your

00:27:33,320 --> 00:27:36,620
likelihood functions as statistical

00:27:34,970 --> 00:27:38,899
distributions and how they join together

00:27:36,620 --> 00:27:40,840
with the data to form the posterior

00:27:38,899 --> 00:27:43,880
distributions on which you do your

00:27:40,840 --> 00:27:45,409
interpretation with the ic50 example it

00:27:43,880 --> 00:27:47,120
got a little bit more complicated in

00:27:45,409 --> 00:27:50,389
which there's a link function you can do

00:27:47,120 --> 00:27:52,669
math with distribution objects in the

00:27:50,389 --> 00:27:54,289
control versus treatment case I showed

00:27:52,669 --> 00:27:56,360
you that you can do one treatment versus

00:27:54,289 --> 00:27:57,740
one control but you can also do multiple

00:27:56,360 --> 00:27:59,570
treatment and multiple controls and

00:27:57,740 --> 00:28:01,100
that's perfectly fine and here's the

00:27:59,570 --> 00:28:04,159
best part you actually don't have to do

00:28:01,100 --> 00:28:07,159
some fancy multiple hypothesis testing

00:28:04,159 --> 00:28:09,440
Corrections because what you're getting

00:28:07,159 --> 00:28:11,330
from Bayes land is a complete summary of

00:28:09,440 --> 00:28:13,880
the data a complete summary of our

00:28:11,330 --> 00:28:15,769
beliefs given the underlying data so

00:28:13,880 --> 00:28:17,809
multiply pollicis correction we can

00:28:15,769 --> 00:28:19,880
throw that out of the window alright

00:28:17,809 --> 00:28:22,549
just to reiterate what the pattern is

00:28:19,880 --> 00:28:24,409
sort of like again we parameterize our

00:28:22,549 --> 00:28:26,659
problem using statistical distributions

00:28:24,409 --> 00:28:28,190
if it helps go draw out those

00:28:26,659 --> 00:28:31,880
distributions draw out how they're

00:28:28,190 --> 00:28:33,860
linked on paper and use that to discuss

00:28:31,880 --> 00:28:36,470
with other people to justify why you're

00:28:33,860 --> 00:28:37,909
using that as your model structure once

00:28:36,470 --> 00:28:40,070
you've spent the bulk of your time on

00:28:37,909 --> 00:28:42,710
that thing the rest of it is fairly easy

00:28:40,070 --> 00:28:44,059
right your bottle in pi MC 3 hit the

00:28:42,710 --> 00:28:45,799
inference button and do your

00:28:44,059 --> 00:28:49,039
interpretation based on the posterior

00:28:45,799 --> 00:28:52,100
distributions with that I want to leave

00:28:49,039 --> 00:28:53,870
you with the notebooks are online I have

00:28:52,100 --> 00:28:55,640
the last thing that I tweeted was

00:28:53,870 --> 00:28:57,980
also that link to the notebooks online

00:28:55,640 --> 00:29:00,669
and finally I just want to say go bays

00:28:57,980 --> 00:29:00,669
thank you

00:29:05,400 --> 00:29:10,650
so we have time for one really quick

00:29:08,260 --> 00:29:10,650
question

00:29:24,540 --> 00:29:27,550
sup Eric thanks for a great tool hey

00:29:26,770 --> 00:29:29,980
Hugo

00:29:27,550 --> 00:29:31,840
so my question is you convinced me that

00:29:29,980 --> 00:29:33,820
looking at these distributions that an

00:29:31,840 --> 00:29:35,650
effect size was something or that the

00:29:33,820 --> 00:29:39,100
control definitely was worse than

00:29:35,650 --> 00:29:40,630
something else how do you quantify it

00:29:39,100 --> 00:29:42,550
though how do you give it a number to

00:29:40,630 --> 00:29:44,560
convince a manager or someone who is

00:29:42,550 --> 00:29:46,840
technical you can use the summary

00:29:44,560 --> 00:29:48,520
statistics of the distribution as well

00:29:46,840 --> 00:29:52,990
as the width the width is I think the

00:29:48,520 --> 00:29:55,210
most important part so for example the

00:29:52,990 --> 00:29:57,160
effect size in the example in the talk

00:29:55,210 --> 00:30:00,930
the effect size was 0.4 but it could be

00:29:57,160 --> 00:30:03,520
anywhere from 0 to 0.7 and in

00:30:00,930 --> 00:30:05,560
translation to manager land from Bayes

00:30:03,520 --> 00:30:08,590
land managers basically you want to say

00:30:05,560 --> 00:30:12,130
alright it could have a probability of

00:30:08,590 --> 00:30:14,710
no effect maybe we shouldn't go ahead

00:30:12,130 --> 00:30:16,480
with this thing or there might or the

00:30:14,710 --> 00:30:18,070
manager might also be thinking hey there

00:30:16,480 --> 00:30:20,560
might be a probability of a large effect

00:30:18,070 --> 00:30:22,840
maybe we should do another trial right

00:30:20,560 --> 00:30:25,140
and so they can take that qualitative

00:30:22,840 --> 00:30:31,480
description from our summarized

00:30:25,140 --> 00:30:32,960
quantitative description great thanks ok

00:30:31,480 --> 00:30:36,039
thank you very much Thanks

00:30:32,960 --> 00:30:36,039
[Applause]

00:30:36,650 --> 00:30:38,710

YouTube URL: https://www.youtube.com/watch?v=p1IB4zWq9C8


