Title: Lynn Root   Tracing, Fast and Slow Digging into and improving your web service’s performance
Publication date: 2017-05-21
Playlist: PyCon 2017
Description: 
	"Speaker: Lynn Root

Do you maintain a [Rube Goldberg](https://s-media-cache-ak0.pinimg.com/564x/92/27/a6/9227a66f6028bd19d418c4fb3a55b379.jpg)-like service? Perhaps it’s highly distributed? Or you recently walked onto a team with an unfamiliar codebase? Have you noticed your service responds slower than molasses? This talk will walk you through how to pinpoint bottlenecks, approaches and tools to make improvements, and make you seem like the hero! All in a day’s work.

The talk will describe various types of tracing a web service, including black & white box tracing, tracing distributed systems, as well as various tools and external services available to measure performance. I’ll also present a few different rabbit holes to dive into when trying to improve your service’s performance.

Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:55,940 --> 00:01:04,739
good afternoon and welcome to the last

00:00:59,010 --> 00:01:06,420
talk of today's hike on 2017 before we

00:01:04,739 --> 00:01:08,369
start I would very much like everyone

00:01:06,420 --> 00:01:10,530
who has a device capable of making noise

00:01:08,369 --> 00:01:12,090
to convince it not to do so so that no

00:01:10,530 --> 00:01:15,680
one stares at you during that thing or

00:01:12,090 --> 00:01:20,060
mocks you on Twitter the fire alarm is

00:01:15,680 --> 00:01:20,060
everyone blame Sam's the fire alarms

00:01:20,479 --> 00:01:24,630
that out of the way I would like to

00:01:22,560 --> 00:01:26,220
introduce our speaker Lynne root is

00:01:24,630 --> 00:01:27,270
going to be discussing tracing and

00:01:26,220 --> 00:01:30,920
digging into your web services

00:01:27,270 --> 00:01:30,920
performance please make a feel welcome

00:01:34,670 --> 00:01:43,710
hello how are you folks good hyped up on

00:01:39,990 --> 00:01:47,760
sugar from all those doughnuts right at

00:01:43,710 --> 00:01:49,800
least are you so my name is Lin root I

00:01:47,760 --> 00:01:53,070
am a site reliability engineer at

00:01:49,800 --> 00:01:56,130
Spotify I also do a lot of open source

00:01:53,070 --> 00:01:58,800
evangelism internally and you might also

00:01:56,130 --> 00:02:00,119
know me from PI ladies and if you

00:01:58,800 --> 00:02:02,100
haven't visited our booth I think you

00:02:00,119 --> 00:02:03,509
are too late unfortunately but there's

00:02:02,100 --> 00:02:07,800
an option tonight that you should all go

00:02:03,509 --> 00:02:10,350
do right after lightning talks um so I

00:02:07,800 --> 00:02:12,959
want to quickly mention that um I do

00:02:10,350 --> 00:02:16,200
have to run right after this talk tool I

00:02:12,959 --> 00:02:17,489
can talk since I am the chair so if you

00:02:16,200 --> 00:02:21,840
have questions you can walk with me

00:02:17,489 --> 00:02:24,630
really quickly all right to start off

00:02:21,840 --> 00:02:27,030
quick question who has read the site

00:02:24,630 --> 00:02:29,610
reliability engineering book from

00:02:27,030 --> 00:02:32,489
O'Reilly also known as the Google s or

00:02:29,610 --> 00:02:33,840
ebook yeah I see a couple hands maybe

00:02:32,489 --> 00:02:36,090
you can agree with me that like

00:02:33,840 --> 00:02:39,690
basically the TLDR of every chapter is

00:02:36,090 --> 00:02:42,000
like use distributed tracing so with a

00:02:39,690 --> 00:02:43,890
with a prevalence of micro-services

00:02:42,000 --> 00:02:46,709
where you may or may not own all the

00:02:43,890 --> 00:02:49,140
services it's pretty much imperative to

00:02:46,709 --> 00:02:51,329
understand where your code fits into the

00:02:49,140 --> 00:02:55,290
grand scheme of things and how it all

00:02:51,329 --> 00:02:59,070
operates so there are three main needs

00:02:55,290 --> 00:03:01,650
two for tracing a system performance

00:02:59,070 --> 00:03:04,610
debugging capacity planning and problem

00:03:01,650 --> 00:03:08,209
diagnosis among some other needs as well

00:03:04,610 --> 00:03:10,560
so while this talk will focus on

00:03:08,209 --> 00:03:13,700
performance debugging these techniques

00:03:10,560 --> 00:03:16,260
can certainly be applied to other needs

00:03:13,700 --> 00:03:18,480
jam-packed day today so I'll start off

00:03:16,260 --> 00:03:21,420
with an overview of what tracing is in

00:03:18,480 --> 00:03:24,200
the problem space I'll also talk about

00:03:21,420 --> 00:03:26,610
general types of tracing that we can use

00:03:24,200 --> 00:03:30,079
what key things to think about when

00:03:26,610 --> 00:03:32,160
scaling up to larger distributed systems

00:03:30,079 --> 00:03:34,230
the inspiration for this talk actually

00:03:32,160 --> 00:03:36,329
stems from me trying to improve the

00:03:34,230 --> 00:03:39,150
performance of one of my team's systems

00:03:36,329 --> 00:03:42,720
which implies that we don't really trace

00:03:39,150 --> 00:03:44,580
at Spotify so I'll I'll run through some

00:03:42,720 --> 00:03:46,410
questions to ask and approaches

00:03:44,580 --> 00:03:49,200
to take when diagnosing and fixing a

00:03:46,410 --> 00:03:51,660
services bottlenecks and then finally

00:03:49,200 --> 00:03:55,590
I'll wrap up with some tracing solutions

00:03:51,660 --> 00:03:57,210
for profiling performance and again I'm

00:03:55,590 --> 00:03:58,650
sorry I won't have much time at the end

00:03:57,210 --> 00:04:01,560
for questions but again you can like

00:03:58,650 --> 00:04:04,410
walk with me so in the simplest of terms

00:04:01,560 --> 00:04:06,300
a trace and follows a complete workflow

00:04:04,410 --> 00:04:09,180
from the start of a transaction our

00:04:06,300 --> 00:04:11,970
request to its end including the

00:04:09,180 --> 00:04:14,520
components that it flows through so for

00:04:11,970 --> 00:04:16,290
a very simple web application to M it's

00:04:14,520 --> 00:04:18,690
pretty easy to understand a workflow of

00:04:16,290 --> 00:04:21,750
a request but when you add some

00:04:18,690 --> 00:04:22,920
databases databases separate the front

00:04:21,750 --> 00:04:25,710
in from the back end

00:04:22,920 --> 00:04:27,810
maybe throw on some caching maybe have

00:04:25,710 --> 00:04:29,280
an external API call and then all behind

00:04:27,810 --> 00:04:30,990
a load balancer it gets kind of

00:04:29,280 --> 00:04:34,410
difficult to put together the workflows

00:04:30,990 --> 00:04:37,910
of requests and so historically we've

00:04:34,410 --> 00:04:41,040
been focused on machine centric metrics

00:04:37,910 --> 00:04:43,440
including system level metrics like CPU

00:04:41,040 --> 00:04:45,270
a disk space and memory as well as app

00:04:43,440 --> 00:04:49,710
level metrics like requests per second

00:04:45,270 --> 00:04:50,850
response latency destroy its etc and so

00:04:49,710 --> 00:04:52,950
following and understanding these

00:04:50,850 --> 00:04:54,990
metrics are definitely important but

00:04:52,950 --> 00:04:58,080
there is no view into a services

00:04:54,990 --> 00:05:00,419
dependencies or its dependence it's also

00:04:58,080 --> 00:05:03,030
not possible to get a view of a complete

00:05:00,419 --> 00:05:05,340
flow of a request nor develop an

00:05:03,030 --> 00:05:09,660
understanding of how one service

00:05:05,340 --> 00:05:11,760
performs at scale so a workflow centric

00:05:09,660 --> 00:05:15,360
approach understands the relationships

00:05:11,760 --> 00:05:17,490
of components with an entire system and

00:05:15,360 --> 00:05:18,930
we can follow a request from beginning

00:05:17,490 --> 00:05:22,800
to end to understand its bottlenecks

00:05:18,930 --> 00:05:24,630
Honan on anomalistic have and figure out

00:05:22,800 --> 00:05:28,200
where we need we might need to add more

00:05:24,630 --> 00:05:30,600
resources so in look at a super

00:05:28,200 --> 00:05:32,370
simplified distributed system where we

00:05:30,600 --> 00:05:34,950
have like a load balancer a front-end

00:05:32,370 --> 00:05:38,280
back-end database maybe an external

00:05:34,950 --> 00:05:40,080
dependency to the third party API so we

00:05:38,280 --> 00:05:42,030
have when we have redundant systems they

00:05:40,080 --> 00:05:44,880
can also get particularly confusing to

00:05:42,030 --> 00:05:48,300
follow a request how do we debug a

00:05:44,880 --> 00:05:50,490
problem of a very rare workflow how do

00:05:48,300 --> 00:05:53,520
we know which component of this system

00:05:50,490 --> 00:05:55,740
is the bottleneck which function call is

00:05:53,520 --> 00:05:57,120
taking the longest or is there another

00:05:55,740 --> 00:05:59,669
app on my

00:05:57,120 --> 00:06:02,250
causing distortion for machine centric

00:05:59,669 --> 00:06:04,230
performance metrics which is like the

00:06:02,250 --> 00:06:05,970
noisy neighbor problem which is a

00:06:04,230 --> 00:06:09,330
growing concern as we all like move to

00:06:05,970 --> 00:06:12,060
the cloud so with many potential paths

00:06:09,330 --> 00:06:14,970
that a request can take with potential

00:06:12,060 --> 00:06:18,000
issues at every node in every edge

00:06:14,970 --> 00:06:21,389
this can be mind-numbing ly difficult if

00:06:18,000 --> 00:06:22,979
we continue to be machine centric so and

00:06:21,389 --> 00:06:24,770
then tracing allows us to get a better

00:06:22,979 --> 00:06:27,720
picture of these concerns

00:06:24,770 --> 00:06:30,240
all right so real quickly there are many

00:06:27,720 --> 00:06:31,740
reasons to trace a system the one that

00:06:30,240 --> 00:06:33,150
inspired this talk is a basically

00:06:31,740 --> 00:06:35,160
performance analysis trying to

00:06:33,150 --> 00:06:37,620
understand what happens at the 50th or

00:06:35,160 --> 00:06:40,650
the 75th percentile are the steady state

00:06:37,620 --> 00:06:44,040
problems will help identify Layton sees

00:06:40,650 --> 00:06:46,919
resource usages other performance issues

00:06:44,040 --> 00:06:49,530
we're also able to understand questions

00:06:46,919 --> 00:06:51,419
or ask questions like did this

00:06:49,530 --> 00:06:53,550
particular deploy of this service have

00:06:51,419 --> 00:06:57,650
an effect on latency for the whole

00:06:53,550 --> 00:07:02,030
system tracing can also clue us into

00:06:57,650 --> 00:07:05,190
anomalistic request flows the 99.9% file

00:07:02,030 --> 00:07:07,260
these issues can be can still be related

00:07:05,190 --> 00:07:09,960
to performance but also help identify

00:07:07,260 --> 00:07:14,460
problems with correctness like component

00:07:09,960 --> 00:07:16,349
failures or timeouts profiling very

00:07:14,460 --> 00:07:18,270
similar to the first one but here we're

00:07:16,349 --> 00:07:20,520
just interested in particular components

00:07:18,270 --> 00:07:24,020
or aspects of a system and we don't

00:07:20,520 --> 00:07:27,180
necessarily care about the full workflow

00:07:24,020 --> 00:07:29,490
you can also answer questions about what

00:07:27,180 --> 00:07:31,979
a particular component depends on and

00:07:29,490 --> 00:07:34,889
what depends on it particularly useful

00:07:31,979 --> 00:07:36,960
for complex systems and so when

00:07:34,889 --> 00:07:39,180
dependents are identified we can

00:07:36,960 --> 00:07:42,180
actually attribute particularly

00:07:39,180 --> 00:07:44,639
expensive work like if component a adds

00:07:42,180 --> 00:07:47,630
significant workload to with this writes

00:07:44,639 --> 00:07:50,340
to component beam which is helpful for

00:07:47,630 --> 00:07:53,910
attributed costs or just attributed

00:07:50,340 --> 00:07:56,639
blame and then finally we were able to

00:07:53,910 --> 00:07:59,070
create models of our systems and ask

00:07:56,639 --> 00:08:01,560
what if questions like and what would

00:07:59,070 --> 00:08:04,169
happen to component a if we did disaster

00:08:01,560 --> 00:08:08,400
e disaster recovery testing on component

00:08:04,169 --> 00:08:10,689
B all right now onto the tracing

00:08:08,400 --> 00:08:12,549
approaches there are simple

00:08:10,689 --> 00:08:15,159
we can add to our web services

00:08:12,549 --> 00:08:17,589
especially when you don't have any

00:08:15,159 --> 00:08:19,619
dependent or depending on components

00:08:17,589 --> 00:08:21,759
that you might not have access to

00:08:19,619 --> 00:08:24,279
granted you don't get any pretty

00:08:21,759 --> 00:08:26,529
visualizations or help with centralized

00:08:24,279 --> 00:08:29,559
collection beyond how you typically

00:08:26,529 --> 00:08:32,379
handle your logs but you can you can

00:08:29,559 --> 00:08:35,829
still get a lot of insight so this is an

00:08:32,379 --> 00:08:38,050
example flask route and a decorator here

00:08:35,829 --> 00:08:40,449
you can simply add a UUID to each

00:08:38,050 --> 00:08:43,029
request received as they as they come in

00:08:40,449 --> 00:08:44,980
and Adam adds a header and then log

00:08:43,029 --> 00:08:46,420
particular points of interest like at

00:08:44,980 --> 00:08:47,920
the beginning and end of a request and

00:08:46,420 --> 00:08:50,110
any other in-between component or

00:08:47,920 --> 00:08:53,379
function calls and then propagate those

00:08:50,110 --> 00:08:55,089
headers as you go on and this is exactly

00:08:53,379 --> 00:08:56,889
what I ended up doing for my service

00:08:55,089 --> 00:08:59,290
which made me scream for like a better

00:08:56,889 --> 00:09:04,420
way and hence this talk I like to do a

00:08:59,290 --> 00:09:06,579
lot of conference driven development if

00:09:04,420 --> 00:09:08,019
your app is behind them engine decks

00:09:06,579 --> 00:09:10,120
installation and that you're able to

00:09:08,019 --> 00:09:12,579
manipulate you can turn on instability

00:09:10,120 --> 00:09:15,699
to stamp each request with a request or

00:09:12,579 --> 00:09:17,589
X request ID header as you can see with

00:09:15,699 --> 00:09:21,189
the add header and that proxy set header

00:09:17,589 --> 00:09:27,120
and you can also add these to your nginx

00:09:21,189 --> 00:09:29,620
logs as well next of another approach

00:09:27,120 --> 00:09:31,269
blackbox tracing is essentially tracing

00:09:29,620 --> 00:09:34,480
with no instrumentation across

00:09:31,269 --> 00:09:36,610
components it tries to infer workflows

00:09:34,480 --> 00:09:39,370
and relationships by correlating

00:09:36,610 --> 00:09:42,610
variables and timing with already

00:09:39,370 --> 00:09:44,529
defined log messages so from here a

00:09:42,610 --> 00:09:46,809
relationship inference is actually done

00:09:44,529 --> 00:09:49,899
via statistical or regression analysis

00:09:46,809 --> 00:09:51,939
and the easiest way is is with

00:09:49,899 --> 00:09:53,800
centralized logging and if there is

00:09:51,939 --> 00:09:56,379
somewhat standardized schema to your log

00:09:53,800 --> 00:09:59,050
messages so you can pull out a some sort

00:09:56,379 --> 00:10:02,559
of an ID and a timestamp and this is

00:09:59,050 --> 00:10:04,660
particularly useful if instrumenting an

00:10:02,559 --> 00:10:06,670
entire system is just too cumbersome

00:10:04,660 --> 00:10:08,860
maybe too much coordination with

00:10:06,670 --> 00:10:11,009
engineers or you can't otherwise

00:10:08,860 --> 00:10:15,370
instrument components that you don't own

00:10:11,009 --> 00:10:17,889
and as such it's quite portable and very

00:10:15,370 --> 00:10:19,809
little overhead but it does require a

00:10:17,889 --> 00:10:23,960
lot of data points to infer

00:10:19,809 --> 00:10:26,270
relationships it also lacks accuracy and

00:10:23,960 --> 00:10:28,160
the absence of instrumenting and

00:10:26,270 --> 00:10:30,530
components themselves as well as the

00:10:28,160 --> 00:10:33,430
ability to attribute causality with

00:10:30,530 --> 00:10:38,120
asynchronous behavior and concurrency

00:10:33,430 --> 00:10:39,920
and Facebook and University of Michigan

00:10:38,120 --> 00:10:42,350
and they wrote a very readable paper

00:10:39,920 --> 00:10:44,210
about how they assessed end-to-end

00:10:42,350 --> 00:10:46,490
performance by employing essentially

00:10:44,210 --> 00:10:49,610
this method and actually have a link to

00:10:46,490 --> 00:10:51,650
it at the end I suggest reading finally

00:10:49,610 --> 00:10:54,620
another approach to a black box tracing

00:10:51,650 --> 00:10:57,470
you can also network tap with the use of

00:10:54,620 --> 00:11:00,620
s flow or NF damp or like IP table

00:10:57,470 --> 00:11:04,610
packet data which I'm sure the NSA is

00:11:00,620 --> 00:11:06,020
very familiar with themselves one final

00:11:04,610 --> 00:11:09,230
common type of tracing is through

00:11:06,020 --> 00:11:12,860
metadata propagation and this approach

00:11:09,230 --> 00:11:15,110
is is was made famous by Google's

00:11:12,860 --> 00:11:17,570
research paper on dapper which I also

00:11:15,110 --> 00:11:19,580
linked at the end so components are

00:11:17,570 --> 00:11:22,190
instrumented at particular trace points

00:11:19,580 --> 00:11:24,890
to follow causality between functions

00:11:22,190 --> 00:11:27,770
and components and services or even with

00:11:24,890 --> 00:11:30,320
just common RPC libraries like G RPC and

00:11:27,770 --> 00:11:33,830
that will automatically add metadata to

00:11:30,320 --> 00:11:37,190
each call and metadata that it's track

00:11:33,830 --> 00:11:40,310
includes a unique trace ID which

00:11:37,190 --> 00:11:42,770
represents a single flow a single trace

00:11:40,310 --> 00:11:46,280
or single workflow and a span ID for

00:11:42,770 --> 00:11:48,710
every point in a particular trace like

00:11:46,280 --> 00:11:51,320
request sent from client requests

00:11:48,710 --> 00:11:54,590
received by server our server responds

00:11:51,320 --> 00:11:58,520
and then the spans start and end time

00:11:54,590 --> 00:12:00,650
and this approach is best when the

00:11:58,520 --> 00:12:03,770
system itself is designed with tracing

00:12:00,650 --> 00:12:05,900
in mind but who actually does that and

00:12:03,770 --> 00:12:09,020
it also avoids the guesswork with

00:12:05,900 --> 00:12:11,330
inferring causal relationships however I

00:12:09,020 --> 00:12:13,790
can add a bit of overhead to response

00:12:11,330 --> 00:12:16,550
time into throughput so the use of

00:12:13,790 --> 00:12:19,580
sampling traces will allow you to limit

00:12:16,550 --> 00:12:24,290
that burden on a system and data point

00:12:19,580 --> 00:12:27,650
storage so sampling between like 0.01

00:12:24,290 --> 00:12:29,780
percent to 10 percent of requests as can

00:12:27,650 --> 00:12:34,130
be plenty for understanding of systems

00:12:29,780 --> 00:12:35,960
performance so I'm starting to have many

00:12:34,130 --> 00:12:37,300
microservices and scaling out with more

00:12:35,960 --> 00:12:38,589
resources there

00:12:37,300 --> 00:12:39,790
few things to keep in mind when

00:12:38,589 --> 00:12:42,149
instrumenting your system and

00:12:39,790 --> 00:12:46,600
particularly with the metadata

00:12:42,149 --> 00:12:48,790
propagation approach and so things that

00:12:46,600 --> 00:12:51,910
keep in mind I'll go into detail about

00:12:48,790 --> 00:12:53,980
each we want to know what relationships

00:12:51,910 --> 00:12:56,290
to track essentially how to follow a

00:12:53,980 --> 00:13:00,459
trace and what is considered part of a

00:12:56,290 --> 00:13:02,890
workflow and then how how to track them

00:13:00,459 --> 00:13:04,959
constructing metadata to track causal

00:13:02,890 --> 00:13:07,600
relationships it can be particularly

00:13:04,959 --> 00:13:10,200
difficult there are a few approaches and

00:13:07,600 --> 00:13:13,630
each with around Forte's and drawbacks

00:13:10,200 --> 00:13:16,750
and then how to reduce overhead of

00:13:13,630 --> 00:13:19,630
tracking the approach one and chooses

00:13:16,750 --> 00:13:20,890
for sampling is largely defined by what

00:13:19,630 --> 00:13:23,230
questions that you're trying to answer

00:13:20,890 --> 00:13:25,630
with tracing and there may be a clear

00:13:23,230 --> 00:13:28,420
answer to sampling but not without their

00:13:25,630 --> 00:13:31,300
own penalties and finally how to

00:13:28,420 --> 00:13:34,570
visualize the visualizations needed to

00:13:31,300 --> 00:13:38,589
is also to be informed by what you're

00:13:34,570 --> 00:13:40,930
trying to answer with tracing so what to

00:13:38,589 --> 00:13:43,750
track when looking I was in a request

00:13:40,930 --> 00:13:46,209
you can take two points of view either

00:13:43,750 --> 00:13:49,450
the submitter point of view or the

00:13:46,209 --> 00:13:52,810
trigger so the submitter point of view

00:13:49,450 --> 00:13:54,670
focuses on one complete request and it

00:13:52,810 --> 00:13:56,890
does not take into account any part of

00:13:54,670 --> 00:13:59,829
the request that is caused by another

00:13:56,890 --> 00:14:02,529
request or action so for instance

00:13:59,829 --> 00:14:05,110
ejecting cache was actually triggered by

00:14:02,529 --> 00:14:07,270
request two in this example but it's

00:14:05,110 --> 00:14:12,190
still attributed to request one since

00:14:07,270 --> 00:14:13,990
its data comes from request one and then

00:14:12,190 --> 00:14:16,870
the trigger point of view focuses on the

00:14:13,990 --> 00:14:19,600
trigger that initiates the action so

00:14:16,870 --> 00:14:22,660
with the same example a request to evict

00:14:19,600 --> 00:14:25,270
cache from a request one and therefore

00:14:22,660 --> 00:14:29,020
the eviction is included in request twos

00:14:25,270 --> 00:14:30,579
trace so choosing which flow to follow

00:14:29,020 --> 00:14:31,750
depends upon the answers that you're

00:14:30,579 --> 00:14:33,730
trying to find we're trying to figure

00:14:31,750 --> 00:14:35,829
out so for instance it doesn't really

00:14:33,730 --> 00:14:38,740
matter which approach is choosing each

00:14:35,829 --> 00:14:40,990
use for performance profiling but

00:14:38,740 --> 00:14:43,390
following trigger causality will help

00:14:40,990 --> 00:14:47,890
detect anomalies and by showing critical

00:14:43,390 --> 00:14:50,140
paths all right how to check or

00:14:47,890 --> 00:14:52,149
essentially what is needed in your data

00:14:50,140 --> 00:14:53,800
in your metadata and what this

00:14:52,149 --> 00:14:55,510
essentially boils down to is it can be

00:14:53,800 --> 00:14:57,610
very difficult to track causal

00:14:55,510 --> 00:15:00,160
relationships within a distributed

00:14:57,610 --> 00:15:02,020
system the sheer nature of a distributed

00:15:00,160 --> 00:15:03,730
system implies issues with ordering

00:15:02,020 --> 00:15:07,000
events and traces that happen across

00:15:03,730 --> 00:15:08,529
many hosts there's not a global there's

00:15:07,000 --> 00:15:11,800
often not a global synchronous clock

00:15:08,529 --> 00:15:14,170
available so care must be taken when

00:15:11,800 --> 00:15:18,970
deciding how to craft metadata that is

00:15:14,170 --> 00:15:21,790
fed through and end-to-end trees so you

00:15:18,970 --> 00:15:23,410
can use a random ID like you UID a

00:15:21,790 --> 00:15:26,230
request ID header that I showed earlier

00:15:23,410 --> 00:15:28,329
and it will help identify and some calls

00:15:26,230 --> 00:15:30,610
will related activity but tracing

00:15:28,329 --> 00:15:33,130
implementations must then use an

00:15:30,610 --> 00:15:36,760
external clock to collate those traces

00:15:33,130 --> 00:15:40,060
and then in the absence of a global sync

00:15:36,760 --> 00:15:43,390
clock or to avoid issues like clock skew

00:15:40,060 --> 00:15:45,760
and looking at network send and receive

00:15:43,390 --> 00:15:48,069
messages can then be used to construct

00:15:45,760 --> 00:15:49,990
causal relationships because you can't

00:15:48,069 --> 00:15:50,940
exactly receive a message before it's

00:15:49,990 --> 00:15:53,980
sent

00:15:50,940 --> 00:15:56,170
however this approach lacks resiliency

00:15:53,980 --> 00:15:59,050
as there is a potential for data loss

00:15:56,170 --> 00:16:01,720
and across external systems or from

00:15:59,050 --> 00:16:04,660
external systems or just the inability

00:16:01,720 --> 00:16:07,690
to trace to add trace points to

00:16:04,660 --> 00:16:09,990
components in two components owned by

00:16:07,690 --> 00:16:09,990
others

00:16:10,170 --> 00:16:16,720
tracing systems can also add a timestamp

00:16:13,839 --> 00:16:20,410
a tent-like timestamp from a local

00:16:16,720 --> 00:16:22,510
logical clock to the workflow ID where

00:16:20,410 --> 00:16:23,230
it isn't exactly the local systems

00:16:22,510 --> 00:16:25,480
timestamp

00:16:23,230 --> 00:16:27,310
but rather like a counter or some sort

00:16:25,480 --> 00:16:29,440
of randomized timestamp that is paired

00:16:27,310 --> 00:16:32,020
with a trace message as it flows through

00:16:29,440 --> 00:16:34,120
components so with this approach we

00:16:32,020 --> 00:16:36,579
don't need the tracing system to spend

00:16:34,120 --> 00:16:38,610
time ordering the traces as it collects

00:16:36,579 --> 00:16:42,339
since it's explicitly in the clock data

00:16:38,610 --> 00:16:44,170
but parallelization and concurrency can

00:16:42,339 --> 00:16:48,370
complicate understanding relationships

00:16:44,170 --> 00:16:50,319
here and filing one can also add the

00:16:48,370 --> 00:16:52,720
previous trace points and I have been

00:16:50,319 --> 00:16:54,400
executed within the metadata to

00:16:52,720 --> 00:16:57,250
understand all those Forks and joins

00:16:54,400 --> 00:17:00,040
paralyzation brain concurrency it also

00:16:57,250 --> 00:17:00,700
allows immediate availability of tracing

00:17:00,040 --> 00:17:02,860
data as

00:17:00,700 --> 00:17:04,690
as work flow ends as there is no need to

00:17:02,860 --> 00:17:08,260
spend time on trying to establish order

00:17:04,690 --> 00:17:10,030
ordering of causal relationships but as

00:17:08,260 --> 00:17:12,940
you can imagine the metadata will only

00:17:10,030 --> 00:17:16,930
grow in size as the work flow progresses

00:17:12,940 --> 00:17:19,540
adding to the payload so basically it

00:17:16,930 --> 00:17:21,880
boils down to this if you really care

00:17:19,540 --> 00:17:25,600
about payload of requests then a simple

00:17:21,880 --> 00:17:26,949
unique ID is your go-to but at the

00:17:25,600 --> 00:17:29,830
expense of needing to infer

00:17:26,949 --> 00:17:31,870
relationships then you can add a

00:17:29,830 --> 00:17:34,810
timestamp of sorts to help establish

00:17:31,870 --> 00:17:36,970
explicit causal relationships but you're

00:17:34,810 --> 00:17:41,230
still susceptible to potential ordering

00:17:36,970 --> 00:17:43,720
issues of traces if data is lost then

00:17:41,230 --> 00:17:45,550
you may add previously executed trace

00:17:43,720 --> 00:17:47,080
points to avoid data loss and to

00:17:45,550 --> 00:17:50,050
understand Forks and joins with the

00:17:47,080 --> 00:17:51,790
currency of a trace while gaining

00:17:50,050 --> 00:17:52,750
immediate availability since causal

00:17:51,790 --> 00:17:55,390
relationships are already established

00:17:52,750 --> 00:17:57,760
but then you suffer in payload size and

00:17:55,390 --> 00:18:00,190
there's also the fact that no tracing

00:17:57,760 --> 00:18:04,540
implementation actually and implements

00:18:00,190 --> 00:18:06,700
this this last one so and in tracing

00:18:04,540 --> 00:18:08,230
will definitely have an effect on your

00:18:06,700 --> 00:18:10,810
runtime and storage overhead no matter

00:18:08,230 --> 00:18:12,850
what you choose for instance if Google

00:18:10,810 --> 00:18:15,250
were to trace all web searches and

00:18:12,850 --> 00:18:17,650
despite its intelligent tracing

00:18:15,250 --> 00:18:22,240
alimentation with stab dapper it would

00:18:17,650 --> 00:18:25,440
still it would impose a 1.5% throughput

00:18:22,240 --> 00:18:28,600
penalty and add 16% to response time so

00:18:25,440 --> 00:18:30,010
I won't go into that much detail but

00:18:28,600 --> 00:18:33,250
there are essentially three basic

00:18:30,010 --> 00:18:35,590
approaches to sampling there's a head

00:18:33,250 --> 00:18:37,750
base which will make a random sampling

00:18:35,590 --> 00:18:39,820
decision at the start of a workflow and

00:18:37,750 --> 00:18:43,600
then follow all the way through to

00:18:39,820 --> 00:18:45,130
completion there's an tail base which

00:18:43,600 --> 00:18:46,870
will make the sampling decision at the

00:18:45,130 --> 00:18:49,930
end of the workflow implying some

00:18:46,870 --> 00:18:52,240
caching going on here and tale based

00:18:49,930 --> 00:18:54,190
sampling needs to be a little bit more

00:18:52,240 --> 00:18:56,680
intelligent but it's particular

00:18:54,190 --> 00:19:01,930
particularly useful for tracing

00:18:56,680 --> 00:19:04,630
anomalistic behavior and science finally

00:19:01,930 --> 00:19:07,270
unitary sampling where sampling decision

00:19:04,630 --> 00:19:08,830
is made at the trace point itself and

00:19:07,270 --> 00:19:12,010
therefore sort of prevents the

00:19:08,830 --> 00:19:14,650
construction of a workflow so head base

00:19:12,010 --> 00:19:17,110
is a simplest and ideal for before

00:19:14,650 --> 00:19:18,790
it's profiling and both head base and

00:19:17,110 --> 00:19:20,800
unit area are most often seen and

00:19:18,790 --> 00:19:23,530
current tracing system implementations

00:19:20,800 --> 00:19:25,060
and I'm actually not sure if if there

00:19:23,530 --> 00:19:28,870
are any of that implements tablebases

00:19:25,060 --> 00:19:31,840
sampling unfortunately alright finally

00:19:28,870 --> 00:19:35,130
what visualization you choose depends

00:19:31,840 --> 00:19:37,630
upon what you're trying to figure out so

00:19:35,130 --> 00:19:41,200
Gantt charts quite popular definitely

00:19:37,630 --> 00:19:43,840
appealing but they only show like one

00:19:41,200 --> 00:19:45,580
single trace and I'm sure you've

00:19:43,840 --> 00:19:50,820
definitely seen this type in your

00:19:45,580 --> 00:19:52,630
network tab of your browser Det tool but

00:19:50,820 --> 00:19:54,880
we're trying to get a sense of

00:19:52,630 --> 00:19:57,580
assistance bottleneck a request flow

00:19:54,880 --> 00:20:01,420
graph shows will show workflow that are

00:19:57,580 --> 00:20:03,550
executed unlike Gantt charts and can

00:20:01,420 --> 00:20:08,290
aggregate information of multiple

00:20:03,550 --> 00:20:10,060
requests of the same workflow another

00:20:08,290 --> 00:20:12,340
useful implement another useful

00:20:10,060 --> 00:20:14,710
representation is a calling context

00:20:12,340 --> 00:20:17,500
stream in order to visualize multiple

00:20:14,710 --> 00:20:20,590
requests of different workflows and this

00:20:17,500 --> 00:20:22,990
reveals valid in invalid paths and a

00:20:20,590 --> 00:20:25,000
request can take and it's best for

00:20:22,990 --> 00:20:29,980
creating a general understanding of

00:20:25,000 --> 00:20:32,470
system behavior so the takeaway here is

00:20:29,980 --> 00:20:34,840
there there's a few things when I'm

00:20:32,470 --> 00:20:37,600
considering what we trace in a system or

00:20:34,840 --> 00:20:38,890
when we trace a system you should have

00:20:37,600 --> 00:20:41,350
an understanding of what you're trying

00:20:38,890 --> 00:20:42,670
to do and what you want to answer what

00:20:41,350 --> 00:20:46,590
questions are you trying to address with

00:20:42,670 --> 00:20:48,490
tracing and there's certainly other

00:20:46,590 --> 00:20:51,280
realizations and questions that might

00:20:48,490 --> 00:20:54,310
come about after tracing for instance

00:20:51,280 --> 00:20:57,100
with dapper Google is able to audit

00:20:54,310 --> 00:20:58,720
systems for security asserting only

00:20:57,100 --> 00:21:01,680
authorized components are able to talk

00:20:58,720 --> 00:21:03,340
to a sensitive services but not without

00:21:01,680 --> 00:21:05,200
understanding what you're trying to

00:21:03,340 --> 00:21:08,770
figure out you might end up approaching

00:21:05,200 --> 00:21:10,630
your instrumentation incorrectly so the

00:21:08,770 --> 00:21:13,240
answer to this question will help

00:21:10,630 --> 00:21:15,280
identify what approach to causality that

00:21:13,240 --> 00:21:19,690
you take whether it be trigger point or

00:21:15,280 --> 00:21:21,370
a submitter point of view another

00:21:19,690 --> 00:21:24,010
important question how much time can you

00:21:21,370 --> 00:21:26,710
put into instrumenting your system or

00:21:24,010 --> 00:21:28,300
can you even instrument all parts and in

00:21:26,710 --> 00:21:28,780
this little forum inform the approach

00:21:28,300 --> 00:21:30,790
that

00:21:28,780 --> 00:21:34,240
you can use to tracing be it black box

00:21:30,790 --> 00:21:36,310
or not if you can instrument all the

00:21:34,240 --> 00:21:38,160
things then it becomes a question of

00:21:36,310 --> 00:21:42,670
what data you should propagate through

00:21:38,160 --> 00:21:44,860
and finally how much of the flows do you

00:21:42,670 --> 00:21:47,500
want to understand do you want to

00:21:44,860 --> 00:21:49,090
understand all the requests then you

00:21:47,500 --> 00:21:51,490
should be prepared to take a performance

00:21:49,090 --> 00:21:55,960
penalty on the service itself and you

00:21:51,490 --> 00:21:57,910
can have fun storing all that data is

00:21:55,960 --> 00:21:59,140
and then as a percentage of the workflow

00:21:57,910 --> 00:22:01,900
okay

00:21:59,140 --> 00:22:04,420
if so then how do you approach sampling

00:22:01,900 --> 00:22:06,340
and that is answered and essentially

00:22:04,420 --> 00:22:08,350
what you're trying to know so for

00:22:06,340 --> 00:22:11,410
understanding performance head based

00:22:08,350 --> 00:22:13,210
sampling is just fine you also need to

00:22:11,410 --> 00:22:14,800
know whether you want whether or not you

00:22:13,210 --> 00:22:16,780
want to capture the full flow of a

00:22:14,800 --> 00:22:19,090
request or if you only want to focus on

00:22:16,780 --> 00:22:22,200
a subset of the system and this will

00:22:19,090 --> 00:22:24,010
also affect your sampling approach and

00:22:22,200 --> 00:22:27,340
I'm just going to go ahead and answer

00:22:24,010 --> 00:22:29,920
this for you because I'm up here so with

00:22:27,340 --> 00:22:31,720
performance or steady state problems and

00:22:29,920 --> 00:22:34,000
you want to try and preserve trigger

00:22:31,720 --> 00:22:36,130
causality rather than a submitter ko

00:22:34,000 --> 00:22:39,370
value as it shows a critical path to

00:22:36,130 --> 00:22:41,350
that bottleneck and as I mentioned

00:22:39,370 --> 00:22:43,720
before ahead base sampling as fun as we

00:22:41,350 --> 00:22:46,360
don't need intelligent sampling and even

00:22:43,720 --> 00:22:48,490
with very low sampling rates we can get

00:22:46,360 --> 00:22:51,550
a pretty good idea of where our problem

00:22:48,490 --> 00:22:53,830
lies and then finally a request flow

00:22:51,550 --> 00:22:56,980
graph here is ideal and since we don't

00:22:53,830 --> 00:22:58,440
care about anomalistic behavior and we

00:22:56,980 --> 00:23:00,790
want information about the big picture

00:22:58,440 --> 00:23:03,030
rather than looking at particular

00:23:00,790 --> 00:23:05,350
individual workflows

00:23:03,030 --> 00:23:07,330
alright so most often when you're

00:23:05,350 --> 00:23:10,990
tracing a system the problem will reveal

00:23:07,330 --> 00:23:13,450
itself as will the solution but not

00:23:10,990 --> 00:23:14,950
always so I have a few questions for you

00:23:13,450 --> 00:23:19,090
to keep in mind when figuring out how to

00:23:14,950 --> 00:23:20,560
improve your services performance are

00:23:19,090 --> 00:23:23,530
you making multiple requests to the same

00:23:20,560 --> 00:23:25,120
service round trip call network calls

00:23:23,530 --> 00:23:26,140
are certainly expensive so perhaps

00:23:25,120 --> 00:23:29,380
there's a way that you can batch

00:23:26,140 --> 00:23:31,170
requests and perhaps your service

00:23:29,380 --> 00:23:34,150
doesn't need to be synchronous or

00:23:31,170 --> 00:23:36,310
unnecessarily blocks for example if

00:23:34,150 --> 00:23:38,310
you're some big social network site can

00:23:36,310 --> 00:23:42,070
you grab like a user's a profile photo

00:23:38,310 --> 00:23:42,429
as well as their timeline data at the

00:23:42,070 --> 00:23:46,360
same

00:23:42,429 --> 00:23:48,909
that you get their messages and then is

00:23:46,360 --> 00:23:51,460
the data the same data being repeatedly

00:23:48,909 --> 00:23:53,710
requested but not cached or perhaps your

00:23:51,460 --> 00:23:55,690
caching too much or maybe not the right

00:23:53,710 --> 00:24:00,340
data or maybe you've set the expiration

00:23:55,690 --> 00:24:02,860
too high or too low and then what about

00:24:00,340 --> 00:24:05,019
your site's assets could they be ordered

00:24:02,860 --> 00:24:06,700
better to improve loading time can you

00:24:05,019 --> 00:24:09,730
minimize the amount of inline scripts

00:24:06,700 --> 00:24:11,830
maybe make them asynchronous are there a

00:24:09,730 --> 00:24:15,730
lot of distinct domain lookups that add

00:24:11,830 --> 00:24:17,769
time from DNS responses perhaps you can

00:24:15,730 --> 00:24:20,499
actually decrease the amount of actual

00:24:17,769 --> 00:24:22,659
files referenced or minify and compress

00:24:20,499 --> 00:24:23,440
them and there's certainly a bunch of

00:24:22,659 --> 00:24:26,679
things that you can do with the

00:24:23,440 --> 00:24:28,480
front-end and finally perhaps you can

00:24:26,679 --> 00:24:30,940
use chunked encoding when returning a

00:24:28,480 --> 00:24:32,679
large amounts of data or otherwise be

00:24:30,940 --> 00:24:34,539
able to have your servers produce

00:24:32,679 --> 00:24:36,190
elements of response as they are needed

00:24:34,539 --> 00:24:42,820
rather than trying to produce all

00:24:36,190 --> 00:24:44,649
elements as fast as possible alright so

00:24:42,820 --> 00:24:46,869
some systems and services that are

00:24:44,649 --> 00:24:48,639
currently out there so impressively

00:24:46,869 --> 00:24:51,039
there is actually an open standard for

00:24:48,639 --> 00:24:53,440
distributed tracing allowing developers

00:24:51,039 --> 00:24:57,279
and applications or open source packages

00:24:53,440 --> 00:24:58,840
or services like engine X or m2 to

00:24:57,279 --> 00:25:00,970
instrument their code without vendor

00:24:58,840 --> 00:25:05,169
lock-in and they do this by

00:25:00,970 --> 00:25:06,789
standardizing trace span ap is the one

00:25:05,169 --> 00:25:09,789
criticism I have is that they don't

00:25:06,789 --> 00:25:11,710
really prescribe a way to implement more

00:25:09,789 --> 00:25:13,769
intelligent sampling rather than just

00:25:11,710 --> 00:25:15,639
simple percentage and setting priority

00:25:13,769 --> 00:25:18,129
there's also a lack of standardization

00:25:15,639 --> 00:25:21,369
for how to track relationships whether

00:25:18,129 --> 00:25:24,369
it be a submitter or trigger based it's

00:25:21,369 --> 00:25:26,409
pretty much all submitter so this is

00:25:24,369 --> 00:25:29,289
basically a standardization for managing

00:25:26,409 --> 00:25:31,360
the span itself but mind you it's still

00:25:29,289 --> 00:25:34,470
very young and ever evolving and

00:25:31,360 --> 00:25:37,119
developing and there are a few

00:25:34,470 --> 00:25:39,480
self-hosted popular solutions that do

00:25:37,119 --> 00:25:43,059
support this open tracing specification

00:25:39,480 --> 00:25:45,309
the one widely known and probably most

00:25:43,059 --> 00:25:47,529
use is the Zipkin which is from twitter

00:25:45,309 --> 00:25:51,190
has implementations in Java and go

00:25:47,529 --> 00:25:53,169
JavaScript Ruby and Scala and the

00:25:51,190 --> 00:25:55,820
architecture setup is basically the

00:25:53,169 --> 00:25:59,240
instrumented app sends data out of

00:25:55,820 --> 00:26:01,700
and to remote collector that accepts a

00:25:59,240 --> 00:26:05,990
few different transport mechanisms like

00:26:01,700 --> 00:26:07,940
HTTP Kafka and scribe so it's

00:26:05,990 --> 00:26:09,800
propagating data around all of the

00:26:07,940 --> 00:26:13,010
current Python libraries out there only

00:26:09,800 --> 00:26:17,750
support and HTTP there's like no RPC and

00:26:13,010 --> 00:26:19,430
support and then finally with UI is it

00:26:17,750 --> 00:26:22,820
can does provide like a very nice Gantt

00:26:19,430 --> 00:26:25,370
chart or waterfall type for individual

00:26:22,820 --> 00:26:27,740
traces and then you can also view a tree

00:26:25,370 --> 00:26:30,050
of dependencies but it's essentially a

00:26:27,740 --> 00:26:32,480
tree with like no information as to like

00:26:30,050 --> 00:26:34,370
Layton sees or response codes or

00:26:32,480 --> 00:26:36,530
anything else so you can see

00:26:34,370 --> 00:26:38,650
dependencies and dependence but nothing

00:26:36,530 --> 00:26:38,650
else

00:26:38,950 --> 00:26:44,570
so using pies of caen on which a lot of

00:26:42,620 --> 00:26:47,120
other libraries are based and you need

00:26:44,570 --> 00:26:49,400
to define a transport mechanism which

00:26:47,120 --> 00:26:52,310
can be just a simple post post request

00:26:49,400 --> 00:26:55,370
with content you can know that you can

00:26:52,310 --> 00:26:57,830
otherwise define a Kafka or ascribe

00:26:55,370 --> 00:27:00,440
transport but then it's just a simple

00:26:57,830 --> 00:27:06,350
context manager and placed wherever you

00:27:00,440 --> 00:27:08,390
want the trace Jaeger is another

00:27:06,350 --> 00:27:10,490
self-hosted tracing system that supports

00:27:08,390 --> 00:27:14,420
the open tracing specification and it

00:27:10,490 --> 00:27:16,250
comes from uber so rather than the

00:27:14,420 --> 00:27:18,410
application of the client library

00:27:16,250 --> 00:27:21,200
reporting to a remote collector it

00:27:18,410 --> 00:27:23,960
reports to a local agent via UDP who

00:27:21,200 --> 00:27:28,490
then sends traces to a collector

00:27:23,960 --> 00:27:31,730
remotely unlike Zipkin and it can only

00:27:28,490 --> 00:27:33,860
supports Cassandra or so can support

00:27:31,730 --> 00:27:36,410
Cassandra elasticsearch and my sequel

00:27:33,860 --> 00:27:40,100
Jager only supports Cassandra for its

00:27:36,410 --> 00:27:42,440
storage on the UI is very similar to the

00:27:40,100 --> 00:27:44,450
kin with pretty waterfall graph indepen

00:27:42,440 --> 00:27:47,300
see tree but again though our gated

00:27:44,450 --> 00:27:48,890
performance information and their

00:27:47,300 --> 00:27:50,240
documentation is also very horribly

00:27:48,890 --> 00:27:53,480
lacking but they do have a decent

00:27:50,240 --> 00:27:54,620
tutorial and to walk through this

00:27:53,480 --> 00:27:59,240
tutorials are always better than

00:27:54,620 --> 00:28:01,340
documentation and then the client

00:27:59,240 --> 00:28:04,370
library is pretty cringe-worthy as well

00:28:01,340 --> 00:28:06,800
and I took this from their Docs

00:28:04,370 --> 00:28:08,960
this is a trend example but you can get

00:28:06,800 --> 00:28:09,710
the gist here basically you initialize a

00:28:08,960 --> 00:28:12,140
tracer

00:28:09,710 --> 00:28:14,120
that the open tracing library will use

00:28:12,140 --> 00:28:17,899
and create a span and child span with

00:28:14,120 --> 00:28:20,899
context managers but their usage of time

00:28:17,899 --> 00:28:24,140
dot sleep for yielding to IO Lib a bit

00:28:20,899 --> 00:28:26,539
of a head-scratcher its docks also make

00:28:24,140 --> 00:28:29,000
mentioned that they support monkey

00:28:26,539 --> 00:28:32,600
patching for libraries like requests and

00:28:29,000 --> 00:28:35,720
Redis and your load too so all I can say

00:28:32,600 --> 00:28:37,399
is you that your own risk and they're a

00:28:35,720 --> 00:28:40,100
couple others I'm I didn't get a chance

00:28:37,399 --> 00:28:43,250
to play with them including app - and

00:28:40,100 --> 00:28:45,080
light step that both support Python or

00:28:43,250 --> 00:28:46,610
both have Python client libraries and

00:28:45,080 --> 00:28:50,799
then there are a few more that don't

00:28:46,610 --> 00:28:52,880
have a Python client libraries quite yet

00:28:50,799 --> 00:28:54,320
so in case you don't want to host your

00:28:52,880 --> 00:28:56,270
own system there are a few services out

00:28:54,320 --> 00:28:58,039
there that to help

00:28:56,270 --> 00:28:59,840
there is stack driver trays from Google

00:28:58,039 --> 00:29:03,169
not to be confused as deck driver

00:28:59,840 --> 00:29:05,090
logging and unfortunately google has no

00:29:03,169 --> 00:29:07,669
Python or G or PC client libraries to

00:29:05,090 --> 00:29:10,429
instrument your app with they do have a

00:29:07,669 --> 00:29:13,580
rest and an RPC interface if you feel so

00:29:10,429 --> 00:29:16,039
inclined to use that but they do have

00:29:13,580 --> 00:29:17,480
support for Zipkin traces so you can set

00:29:16,039 --> 00:29:19,820
up their own google flavored supreme

00:29:17,480 --> 00:29:22,520
server either on their infrastructure or

00:29:19,820 --> 00:29:25,279
yours and then have that forward traces

00:29:22,520 --> 00:29:27,080
to stack driver and they actually make

00:29:25,279 --> 00:29:28,490
it pretty easy I was able to spin up a

00:29:27,080 --> 00:29:32,179
docker image and start viewing traces

00:29:28,490 --> 00:29:33,860
within a couple of minutes and one last

00:29:32,179 --> 00:29:36,830
thing it's kind of annoying a storage

00:29:33,860 --> 00:29:39,260
limitation is is a set up 30 days and

00:29:36,830 --> 00:29:41,830
similar to their stack driver logging

00:29:39,260 --> 00:29:41,830
service

00:29:42,350 --> 00:29:48,409
next up Amazon it also has a tracing

00:29:45,020 --> 00:29:50,570
service available called x-ray and I was

00:29:48,409 --> 00:29:52,789
only able to set up their node app but

00:29:50,570 --> 00:29:55,730
it looks like they they don't have

00:29:52,789 --> 00:29:58,010
explicit Python support but there is a

00:29:55,730 --> 00:30:01,640
Python SDK bottom and I have support for

00:29:58,010 --> 00:30:03,770
traces sending to local daemon and I'm

00:30:01,640 --> 00:30:06,710
running out time so what's nice about X

00:30:03,770 --> 00:30:09,260
ray is a despite of being proprietary

00:30:06,710 --> 00:30:11,330
and no open tracing compliant and you're

00:30:09,260 --> 00:30:13,669
able to configure sampling rates for

00:30:11,330 --> 00:30:16,340
different URL routes but you can't with

00:30:13,669 --> 00:30:18,500
photo unfortunately but they're

00:30:16,340 --> 00:30:20,690
visualizations are awesome and there's a

00:30:18,500 --> 00:30:22,610
typical waterfall chart as well as

00:30:20,690 --> 00:30:23,350
requests flow graphs to see average

00:30:22,610 --> 00:30:25,860
latency

00:30:23,350 --> 00:30:28,900
captured traces permanent and requests

00:30:25,860 --> 00:30:30,940
broken down by response status so

00:30:28,900 --> 00:30:32,860
essentially an eighth of a s seems

00:30:30,940 --> 00:30:33,789
pretty cool and could be useful but you

00:30:32,860 --> 00:30:35,890
would have to spend a lot of time

00:30:33,789 --> 00:30:38,799
instrumenting your Python app and it

00:30:35,890 --> 00:30:40,990
includes vendor lock-in and a couple of

00:30:38,799 --> 00:30:42,850
honorable mentions and then finally

00:30:40,990 --> 00:30:45,070
quick write opinionated wrap-up

00:30:42,850 --> 00:30:47,230
if you run micro services you

00:30:45,070 --> 00:30:48,460
essentially need to trace them otherwise

00:30:47,230 --> 00:30:52,150
it's very difficult to understand

00:30:48,460 --> 00:30:55,990
everything but good luck there's a very

00:30:52,150 --> 00:30:58,900
there's lots of lack of Docs it's very

00:30:55,990 --> 00:31:01,570
young space and the standard is still

00:30:58,900 --> 00:31:04,470
developing and as I mentioned there

00:31:01,570 --> 00:31:07,120
isn't a 100% support for Python language

00:31:04,470 --> 00:31:09,190
and there's a lack of configuration for

00:31:07,120 --> 00:31:11,559
relationship tracking telogen sampling

00:31:09,190 --> 00:31:14,350
and available visualizations but open

00:31:11,559 --> 00:31:16,000
spec that you can influence or you can

00:31:14,350 --> 00:31:18,789
implement to your own tracing system if

00:31:16,000 --> 00:31:21,220
you're so inclined alright that is all

00:31:18,789 --> 00:31:23,530
so thank you all sources and links

00:31:21,220 --> 00:31:27,610
around that link and you can run with me

00:31:23,530 --> 00:31:30,330
to the letting talks to talk thank you

00:31:27,610 --> 00:31:33,460
[Applause]

00:31:30,330 --> 00:31:34,929
thank you again Lyn and as has been

00:31:33,460 --> 00:31:38,610
mentioned longing talks are happening in

00:31:34,929 --> 00:31:38,610

YouTube URL: https://www.youtube.com/watch?v=lu0F-psmBzc


