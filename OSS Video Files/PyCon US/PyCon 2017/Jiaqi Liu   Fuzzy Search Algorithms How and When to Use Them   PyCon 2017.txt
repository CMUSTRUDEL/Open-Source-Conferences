Title: Jiaqi Liu   Fuzzy Search Algorithms How and When to Use Them   PyCon 2017
Publication date: 2017-05-21
Playlist: PyCon 2017
Description: 
	"Speaker: Jiaqi Liu

Fuzzy Searching or approximate string matching is powerful because often text data is messy. For example, shorthand and abbreviated text are common in various data sets. In addition, outputs from OCR or voice to text conversions tend to be messy or imperfect. Thus, we want to be able to make the most of our data by extrapolating as much information as possible. 

In this talk, we will explore the various approaches used in fuzzy string matching and demonstrate how they can be used as a feature in a model or a component in your python code. We will dive deep into the approaches of different algorithms such as Soundex, Trigram/n-gram search, and Levenshtein distances and what the best use cases are. We will also discuss situations where it’s important to take into account the meaning or intent of a word and demonstrate approaches for measuring semantic similarity using nltk and word2vec. Furthermore, we will demonstrate via live coding how to implement some of these fuzzy search algorithms using python and/or built-in fuzzy search functions within PostgreSQL.



Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:28,100 --> 00:00:33,829
so welcome walls so our next speaker is

00:00:31,490 --> 00:00:36,860
Josh Alou who would be speaking on fuzzy

00:00:33,829 --> 00:00:47,480
search algorithm and how and when to use

00:00:36,860 --> 00:00:49,100
them okay thanks for the intro so last

00:00:47,480 --> 00:00:51,650
session of the day so hopefully I won't

00:00:49,100 --> 00:00:53,900
keep you guys too long I'm going to be

00:00:51,650 --> 00:00:55,879
going over fuzzy search algorithms today

00:00:53,900 --> 00:00:59,540
and I'm going through a couple of

00:00:55,879 --> 00:01:01,100
examples all the content for the

00:00:59,540 --> 00:01:03,590
examples I'll go through is all my

00:01:01,100 --> 00:01:05,720
github you can check it out there and

00:01:03,590 --> 00:01:09,680
that's my Twitter handle although I

00:01:05,720 --> 00:01:12,410
recently rediscovered Twitter a little

00:01:09,680 --> 00:01:15,729
bit about myself I am a software

00:01:12,410 --> 00:01:20,590
engineer at button-button is in a

00:01:15,729 --> 00:01:23,720
start-up in New York City in the mobile

00:01:20,590 --> 00:01:26,149
affiliation link a space and we are

00:01:23,720 --> 00:01:29,360
connecting apps together to bring a

00:01:26,149 --> 00:01:32,330
whole new on-demand experience so check

00:01:29,360 --> 00:01:34,159
out our tech blog over there at blog

00:01:32,330 --> 00:01:36,290
youth button com

00:01:34,159 --> 00:01:38,210
I also have stickers so feel free to

00:01:36,290 --> 00:01:42,290
come up at the end of the session for

00:01:38,210 --> 00:01:45,770
dos the overall agenda for today I am

00:01:42,290 --> 00:01:48,640
going to go over a couple of comments as

00:01:45,770 --> 00:01:51,140
you search algorithms and go over the

00:01:48,640 --> 00:01:53,420
implementation in Python and then also

00:01:51,140 --> 00:01:56,560
the search functionality that's built in

00:01:53,420 --> 00:01:59,479
to post graphs and I'm going to go over

00:01:56,560 --> 00:02:04,119
difference between syntax algorithms

00:01:59,479 --> 00:02:13,100
versus semantic analysis so going into

00:02:04,119 --> 00:02:15,350
why why this matters so it prior to to

00:02:13,100 --> 00:02:18,709
my row as an engineer a button I was a

00:02:15,350 --> 00:02:20,630
data scientist and when nowadays when

00:02:18,709 --> 00:02:22,549
you work as an engineer or as a data

00:02:20,630 --> 00:02:25,489
scientist you you kind of can't avoid

00:02:22,549 --> 00:02:29,450
data anyway and when working with data

00:02:25,489 --> 00:02:31,459
context is really important it's not

00:02:29,450 --> 00:02:32,989
just enough to know a snippet of

00:02:31,459 --> 00:02:35,000
information but you also have to sort of

00:02:32,989 --> 00:02:36,770
know where that fits in

00:02:35,000 --> 00:02:39,860
and that's where a lot of the

00:02:36,770 --> 00:02:43,130
qualitative data comes in and what

00:02:39,860 --> 00:02:46,720
qualitative data it gets very messy very

00:02:43,130 --> 00:02:50,000
quickly so you can have basically

00:02:46,720 --> 00:02:52,070
various forms of how to say one thing

00:02:50,000 --> 00:02:54,860
and then suddenly you have this entity

00:02:52,070 --> 00:02:58,280
resolution issue where you have

00:02:54,860 --> 00:03:01,030
different versions of the same thing for

00:02:58,280 --> 00:03:07,000
instance maybe in terms of abbreviations

00:03:01,030 --> 00:03:11,090
you can abbreviate the US as US or USA

00:03:07,000 --> 00:03:12,500
and granted there are standards of for

00:03:11,090 --> 00:03:14,300
the very reason that entity resolution

00:03:12,500 --> 00:03:17,510
is hard but what do you do when your

00:03:14,300 --> 00:03:18,860
data is messy and is very tricky so

00:03:17,510 --> 00:03:21,710
that's where fuzzy search algorithms

00:03:18,860 --> 00:03:24,110
come in they're a tool to help you with

00:03:21,710 --> 00:03:26,600
these kind of problems but not the end

00:03:24,110 --> 00:03:29,150
to demean themselves so I want to start

00:03:26,600 --> 00:03:32,300
off with talking about phonetic

00:03:29,150 --> 00:03:36,830
algorithms this is very straight forward

00:03:32,300 --> 00:03:39,560
in terms of identifying strings as how

00:03:36,830 --> 00:03:43,010
they sound in a particular language and

00:03:39,560 --> 00:03:47,180
in this case English language so the

00:03:43,010 --> 00:03:50,780
American sound exa coding is very very

00:03:47,180 --> 00:03:54,800
straightforward it's actually this chart

00:03:50,780 --> 00:03:57,350
right here and it's a very deterministic

00:03:54,800 --> 00:03:59,989
algorithm and you can implement it

00:03:57,350 --> 00:04:02,600
yourself or you can use a library here

00:03:59,989 --> 00:04:05,650
I'm going to demo a library that I use

00:04:02,600 --> 00:04:05,650
called jellyfish

00:04:16,810 --> 00:04:29,370
oh excuse me once again okay

00:04:30,120 --> 00:04:41,820
I forgot that my screen is elsewhere

00:04:34,550 --> 00:04:45,479
let's go okay I make this slightly

00:04:41,820 --> 00:04:47,580
bigger so in the soundex engines go into

00:04:45,479 --> 00:04:50,880
the jellyfish library so we try

00:04:47,580 --> 00:04:53,220
something like Portland was fully

00:04:50,880 --> 00:04:55,949
spelled out at Portland spelled without

00:04:53,220 --> 00:04:58,500
the oh you'll get these in coatings

00:04:55,949 --> 00:05:00,870
associated with them and every sound X

00:04:58,500 --> 00:05:02,580
encoding is a four character encoding

00:05:00,870 --> 00:05:05,060
starting with a letter and then three

00:05:02,580 --> 00:05:07,800
numbers so in this case you see that the

00:05:05,060 --> 00:05:11,340
two strings here are actually identical

00:05:07,800 --> 00:05:13,350
in terms of how they sell if you want to

00:05:11,340 --> 00:05:15,720
look at an example where there's a word

00:05:13,350 --> 00:05:18,750
and then maybe trailing characters

00:05:15,720 --> 00:05:21,060
following it you'll see that it's still

00:05:18,750 --> 00:05:25,470
the same encoding because if you

00:05:21,060 --> 00:05:29,430
remember the soundex chart it doesn't

00:05:25,470 --> 00:05:32,190
take into account letter numbers and

00:05:29,430 --> 00:05:34,410
characters that are not letters so

00:05:32,190 --> 00:05:38,789
that's one of the issues that you might

00:05:34,410 --> 00:05:42,330
run into with sound x so you look at

00:05:38,789 --> 00:05:45,210
except vs. except you'll see that they

00:05:42,330 --> 00:05:47,310
clearly have very similar sounds so they

00:05:45,210 --> 00:05:49,500
actually have to say miracle value but

00:05:47,310 --> 00:05:50,789
they have different starting letters so

00:05:49,500 --> 00:05:53,460
that's something that's taken into

00:05:50,789 --> 00:05:58,289
consideration by this one and for

00:05:53,460 --> 00:06:01,229
homophones they are again identical so

00:05:58,289 --> 00:06:06,330
here's another example for versus letter

00:06:01,229 --> 00:06:08,820
the number numerical for and Postgres

00:06:06,330 --> 00:06:12,990
very nicely has this built-in fuzzy

00:06:08,820 --> 00:06:15,900
search algorithm tonality and the fuzzy

00:06:12,990 --> 00:06:19,020
string match actually uses the soundex

00:06:15,900 --> 00:06:22,320
algorithm so i set up a local database

00:06:19,020 --> 00:06:26,340
how python let me connect to it using

00:06:22,320 --> 00:06:29,539
sequel alchemy and here's like a very

00:06:26,340 --> 00:06:34,500
basic query that's based very much

00:06:29,539 --> 00:06:36,330
saying for these two strains calculate

00:06:34,500 --> 00:06:39,180
the difference between them and this

00:06:36,330 --> 00:06:42,330
isn't actually querying a database of

00:06:39,180 --> 00:06:43,620
any sort it's just running this one

00:06:42,330 --> 00:06:45,660
simple query

00:06:43,620 --> 00:06:49,350
and you'll see that it calculates the

00:06:45,660 --> 00:06:51,389
soundex encoding and it also calculates

00:06:49,350 --> 00:06:54,000
the difference and the way that Postgres

00:06:51,389 --> 00:06:58,520
handles that difference is it's

00:06:54,000 --> 00:07:01,650
considered which how many how many

00:06:58,520 --> 00:07:03,750
characters matched exactly for the

00:07:01,650 --> 00:07:09,180
encoding in this case four means that

00:07:03,750 --> 00:07:10,650
it's a perfect match okay so that's not

00:07:09,180 --> 00:07:15,500
very interesting to play with without

00:07:10,650 --> 00:07:17,910
any real data so I downloaded all the

00:07:15,500 --> 00:07:20,100
packages on the pi PI server their

00:07:17,910 --> 00:07:23,699
package name and their description and

00:07:20,100 --> 00:07:29,550
loaded it into my little local Postgres

00:07:23,699 --> 00:07:32,699
database so now I can query this table

00:07:29,550 --> 00:07:36,060
and search for every package that sounds

00:07:32,699 --> 00:07:39,060
like fuzzy search and this is what I end

00:07:36,060 --> 00:07:42,240
up getting and you'll see that I

00:07:39,060 --> 00:07:44,280
actually get some of the actually get

00:07:42,240 --> 00:07:49,950
some fuzzy sorting but be searching

00:07:44,280 --> 00:07:52,470
facehugger is kind of weird okay so let

00:07:49,950 --> 00:07:53,729
me search the descriptions because those

00:07:52,470 --> 00:07:56,039
you know have a lot of really

00:07:53,729 --> 00:07:59,039
interesting data and you'll see that in

00:07:56,039 --> 00:08:01,139
the descriptions okay I guess somebody

00:07:59,039 --> 00:08:03,360
prayed a lot of fake servers for testing

00:08:01,139 --> 00:08:05,849
but there's also like the fuzzy search

00:08:03,360 --> 00:08:09,800
with the Postgres diagram extension

00:08:05,849 --> 00:08:09,800
which is exactly what I'm doing

00:08:11,599 --> 00:08:16,979
yeah so the other nice thing about the

00:08:14,820 --> 00:08:20,460
search functionality is you can set the

00:08:16,979 --> 00:08:23,340
threshold for what your tolerance is so

00:08:20,460 --> 00:08:25,800
that I think the default is on obviously

00:08:23,340 --> 00:08:28,530
for a perfect match is great but here if

00:08:25,800 --> 00:08:32,010
I set the threshold to be I only care if

00:08:28,530 --> 00:08:35,659
two of the characters in the encoding

00:08:32,010 --> 00:08:38,690
match then I get a lot of a lot more

00:08:35,659 --> 00:08:43,080
results that might not match as well

00:08:38,690 --> 00:08:45,959
versus let's see if I set this to three

00:08:43,080 --> 00:08:47,010
I got a lot like up a dirt result so

00:08:45,959 --> 00:08:48,330
that's something that you can play

00:08:47,010 --> 00:08:52,020
around with depending on what your

00:08:48,330 --> 00:08:53,209
particular use cases like also try I

00:08:52,020 --> 00:08:55,589
think

00:08:53,209 --> 00:08:56,360
debugging is like a really interesting

00:08:55,589 --> 00:08:59,390
thing so

00:08:56,360 --> 00:09:02,029
let's try it see what kind of debugging

00:08:59,390 --> 00:09:03,470
libraries it could be oh and it's very

00:09:02,029 --> 00:09:07,040
interesting to see that debugging is

00:09:03,470 --> 00:09:11,620
also sort of similar to get device and

00:09:07,040 --> 00:09:17,480
and things like that all right

00:09:11,620 --> 00:09:19,430
moving back to slides so the pros I

00:09:17,480 --> 00:09:21,829
found out that is very easy to implement

00:09:19,430 --> 00:09:23,930
it's deterministic so you always know

00:09:21,829 --> 00:09:26,300
what you're getting back some of the

00:09:23,930 --> 00:09:28,760
cons are it's very limited to some

00:09:26,300 --> 00:09:31,399
language to the dialect it's

00:09:28,760 --> 00:09:36,529
computationally fast which I'll cover

00:09:31,399 --> 00:09:39,140
more later and the con there is that it

00:09:36,529 --> 00:09:40,760
only takes into consideration letters so

00:09:39,140 --> 00:09:43,550
you know all those version numbers

00:09:40,760 --> 00:09:48,890
associated in the in the pi PI database

00:09:43,550 --> 00:09:50,390
like what happens to those etc and it's

00:09:48,890 --> 00:09:52,700
very limited in terms of how it

00:09:50,390 --> 00:09:56,600
calculates distance because there's only

00:09:52,700 --> 00:09:58,459
for coding values so what happens when

00:09:56,600 --> 00:10:00,320
you care much more about the distance

00:09:58,459 --> 00:10:03,380
between words and what does that mean

00:10:00,320 --> 00:10:05,930
anyway so one way to value this sense is

00:10:03,380 --> 00:10:09,230
the de levenshtein distance also known

00:10:05,930 --> 00:10:11,269
as the Edit distance which means how

00:10:09,230 --> 00:10:15,110
many you know changes you have to make

00:10:11,269 --> 00:10:17,529
to a string to get to 8-under string and

00:10:15,110 --> 00:10:21,589
those specific changes are defined as

00:10:17,529 --> 00:10:24,279
you can have a deletion deleting a

00:10:21,589 --> 00:10:26,959
character you can have an insertion

00:10:24,279 --> 00:10:29,149
inserting a new character into a string

00:10:26,959 --> 00:10:31,640
or you can have a substitution so

00:10:29,149 --> 00:10:35,750
swapping out a character for a different

00:10:31,640 --> 00:10:39,079
character there's a different types of a

00:10:35,750 --> 00:10:41,600
Xiuying algorithm called the damaru love

00:10:39,079 --> 00:10:45,199
and sonia algorithm which includes also

00:10:41,600 --> 00:10:46,850
a trends position between two two

00:10:45,199 --> 00:10:49,250
strength two characters are next to each

00:10:46,850 --> 00:10:56,360
other so swapping two characters let's

00:10:49,250 --> 00:10:58,579
move to the ipython demo all right so

00:10:56,360 --> 00:11:00,769
here again I downloaded a library and

00:10:58,579 --> 00:11:02,690
this is also a deterministic algorithm

00:11:00,769 --> 00:11:05,060
and here you can see that for Smith but

00:11:02,690 --> 00:11:07,610
the Y versus Smith with the I you only

00:11:05,060 --> 00:11:08,490
need to change two characters to get to

00:11:07,610 --> 00:11:12,630
get

00:11:08,490 --> 00:11:16,290
to be the same for pi PI it's one for

00:11:12,630 --> 00:11:19,080
changing one little character some of

00:11:16,290 --> 00:11:20,580
the pitfalls here are that it gets

00:11:19,080 --> 00:11:23,790
really tricky you say you're comparing

00:11:20,580 --> 00:11:26,850
two addresses and you know that 99

00:11:23,790 --> 00:11:29,339
Broadway is probably really close to a

00:11:26,850 --> 00:11:31,589
hundred Broadway so in terms of your

00:11:29,339 --> 00:11:35,670
concept of distance or addresses that's

00:11:31,589 --> 00:11:39,690
kind of what you care about but in terms

00:11:35,670 --> 00:11:46,110
of you look at this 99 and 100 have an

00:11:39,690 --> 00:11:48,270
edit distance of 3 but 99 and 999 as an

00:11:46,110 --> 00:11:50,760
editor since a 1 they're a lot closer in

00:11:48,270 --> 00:11:52,649
terms of strengths and molarity but the

00:11:50,760 --> 00:11:54,240
notion would because we understand the

00:11:52,649 --> 00:11:55,709
notion of addresses they're actually

00:11:54,240 --> 00:11:57,930
further away

00:11:55,709 --> 00:12:00,330
so in this scenario you actually want to

00:11:57,930 --> 00:12:02,760
customize your your edit distance

00:12:00,330 --> 00:12:08,130
calculation maybe way numbers

00:12:02,760 --> 00:12:12,270
differently than letters yes here 99 and

00:12:08,130 --> 00:12:14,160
100 is the same distance is 99 100

00:12:12,270 --> 00:12:17,550
another issue is in terms of when you

00:12:14,160 --> 00:12:20,070
have longer strings here you have

00:12:17,550 --> 00:12:22,770
micrometers Elian toast and a New Yorker

00:12:20,070 --> 00:12:25,350
Yoko and the Edit distance here is

00:12:22,770 --> 00:12:28,320
actually the same as if this if this

00:12:25,350 --> 00:12:29,640
were a shorter string and depending on

00:12:28,320 --> 00:12:33,329
you know what kind of analysis you're

00:12:29,640 --> 00:12:34,829
doing maybe that that is the use case

00:12:33,329 --> 00:12:36,209
that you want to go what that is correct

00:12:34,829 --> 00:12:38,850
in terms of how you want to analyze this

00:12:36,209 --> 00:12:42,120
data but it also mean that you actually

00:12:38,850 --> 00:12:43,320
care more about when it's a smaller

00:12:42,120 --> 00:12:45,510
string and there's a bigger difference

00:12:43,320 --> 00:12:47,730
so you might want to weigh your added

00:12:45,510 --> 00:12:50,520
distances based on the length of the

00:12:47,730 --> 00:12:52,649
overall string in addition to just the

00:12:50,520 --> 00:12:56,610
Edit distances that you will have to

00:12:52,649 --> 00:13:00,930
make yeah so converting raw edits

00:12:56,610 --> 00:13:05,040
penalize long strings and being able to

00:13:00,930 --> 00:13:06,720
weigh numbers differently or fun things

00:13:05,040 --> 00:13:10,980
you can do a Levenstein to customize

00:13:06,720 --> 00:13:13,680
your algorithm so let's move to the next

00:13:10,980 --> 00:13:15,720
topic oh and this is the comparison

00:13:13,680 --> 00:13:18,870
between you guess those two where you

00:13:15,720 --> 00:13:20,720
swap letters

00:13:18,870 --> 00:13:22,380
yeah pretty easy to implement

00:13:20,720 --> 00:13:24,720
computationally fast

00:13:22,380 --> 00:13:27,540
that's always a pairwise comparison

00:13:24,720 --> 00:13:29,520
which means it which pretty much

00:13:27,540 --> 00:13:31,740
constrains you to you know making

00:13:29,520 --> 00:13:33,630
comparisons between two strains in the

00:13:31,740 --> 00:13:35,100
making more comparisons etc and you

00:13:33,630 --> 00:13:41,280
might need to customize depending on

00:13:35,100 --> 00:13:44,970
your use case and grams so this is my

00:13:41,280 --> 00:13:47,640
personal favorite and trigrams it's it's

00:13:44,970 --> 00:13:49,650
slightly different from the phonetic

00:13:47,640 --> 00:13:52,260
search and edit distance because it take

00:13:49,650 --> 00:13:54,000
into consideration a lot more components

00:13:52,260 --> 00:13:56,280
of the words instead of one letter or

00:13:54,000 --> 00:13:59,760
one character at a time it sort of takes

00:13:56,280 --> 00:14:02,640
into consideration the characters

00:13:59,760 --> 00:14:04,860
preceding the other characters and you

00:14:02,640 --> 00:14:08,850
can customize it to see if you want you

00:14:04,860 --> 00:14:10,800
know Looney gram or bigram trigram is

00:14:08,850 --> 00:14:14,790
the most common one and Postgres is a

00:14:10,800 --> 00:14:20,670
want to Fogo in trigram search and take

00:14:14,790 --> 00:14:23,340
a look at the demo okay so I wrote my

00:14:20,670 --> 00:14:25,790
own tokenizer but a lot of the open

00:14:23,340 --> 00:14:28,500
source libraries again have a lot of

00:14:25,790 --> 00:14:31,020
tokenizer czar easy to come by and

00:14:28,500 --> 00:14:33,810
here's what it's oaken ization for a

00:14:31,020 --> 00:14:38,880
sentence will look like in terms of the

00:14:33,810 --> 00:14:42,450
grams being words and see here's what

00:14:38,880 --> 00:14:46,260
tokenization for a a string would look

00:14:42,450 --> 00:14:47,340
like in terms of calculating similarity

00:14:46,260 --> 00:14:51,300
there's a lot of different ways that

00:14:47,340 --> 00:14:53,070
this can go but I typically seen the

00:14:51,300 --> 00:14:57,540
Jaccard similarity being used which is

00:14:53,070 --> 00:14:59,970
the intersection of over the Union so in

00:14:57,540 --> 00:15:02,610
this particular case if you find an

00:14:59,970 --> 00:15:06,900
intersection of two two graham arrays

00:15:02,610 --> 00:15:10,770
for here yeah this is how I would assign

00:15:06,900 --> 00:15:14,400
function and if you use the same array

00:15:10,770 --> 00:15:17,550
you would get a perfect score of 1.0

00:15:14,400 --> 00:15:21,210
because you're comparing two exactly

00:15:17,550 --> 00:15:23,040
same strings and here has two slightly

00:15:21,210 --> 00:15:25,890
different strings and you'll find that I

00:15:23,040 --> 00:15:28,590
get a slightly less force so this is use

00:15:25,890 --> 00:15:31,670
a very really useful one score for doing

00:15:28,590 --> 00:15:31,670
further analysis

00:15:31,810 --> 00:15:37,750
try Granville Postgres so again Postgres

00:15:35,330 --> 00:15:39,950
is a built-in trigram functionality and

00:15:37,750 --> 00:15:42,110
I'm going to use the same database that

00:15:39,950 --> 00:15:46,070
I was using earlier with the pi PI

00:15:42,110 --> 00:15:51,200
packages into it and here I'm going to

00:15:46,070 --> 00:15:58,000
search the description for the

00:15:51,200 --> 00:15:58,000
similarity between two different strings

00:16:00,040 --> 00:16:04,100
okay

00:16:01,160 --> 00:16:07,250
so the weight of this query is

00:16:04,100 --> 00:16:09,740
structured it scoring the pi PI table on

00:16:07,250 --> 00:16:14,950
the description column and it's going to

00:16:09,740 --> 00:16:23,750
take in as input this script string and

00:16:14,950 --> 00:16:27,290
friendís okay see just looks like okay

00:16:23,750 --> 00:16:32,180
cool and I here are the results that I

00:16:27,290 --> 00:16:33,710
get and I also get a trigram score

00:16:32,180 --> 00:16:35,570
associated with it

00:16:33,710 --> 00:16:38,360
so post-development Postgres the

00:16:35,570 --> 00:16:39,860
functionality cuts off the score at 0.3

00:16:38,360 --> 00:16:42,710
which is why you don't see any values

00:16:39,860 --> 00:16:44,990
below 0.3 otherwise you know you could

00:16:42,710 --> 00:16:50,300
basically have anything that to score at

00:16:44,990 --> 00:16:53,720
0 all right so we are now going to run

00:16:50,300 --> 00:17:01,310
the same query over the package column

00:16:53,720 --> 00:17:03,560
and see and here I am going to so I see

00:17:01,310 --> 00:17:05,300
that I have some really high scoring

00:17:03,560 --> 00:17:08,150
lines and something I can do to

00:17:05,300 --> 00:17:10,280
basically customize how I want to

00:17:08,150 --> 00:17:13,010
interpret this data and this limit can

00:17:10,280 --> 00:17:16,040
be added to the query string itself or

00:17:13,010 --> 00:17:18,350
in Python but I can cut off the query to

00:17:16,040 --> 00:17:22,190
be as you know I'm only willing to

00:17:18,350 --> 00:17:23,960
tolerate a score or 0.5 or above and

00:17:22,190 --> 00:17:27,980
then these are the results I felt it

00:17:23,960 --> 00:17:31,280
down to so what this sort of brings us

00:17:27,980 --> 00:17:34,670
towards is that you can use the score

00:17:31,280 --> 00:17:37,220
from the trigram search and you can use

00:17:34,670 --> 00:17:39,530
the score from buzzy search to do other

00:17:37,220 --> 00:17:41,390
things to build other algorithms but

00:17:39,530 --> 00:17:43,430
here is a very simple heuristic that you

00:17:41,390 --> 00:17:46,370
can actually use as a feature and this

00:17:43,430 --> 00:17:50,870
model which in itself is an entirely

00:17:46,370 --> 00:17:52,820
different talk but this is very much a

00:17:50,870 --> 00:17:56,440
stepping stone towards getting you to

00:17:52,820 --> 00:18:00,440
Arango as much as it is about also

00:17:56,440 --> 00:18:03,890
figuring out how to do analyze

00:18:00,440 --> 00:18:11,000
qualitative data ok let's see that

00:18:03,890 --> 00:18:11,960
cancer debug and there's a lot more more

00:18:11,000 --> 00:18:20,620
results here

00:18:11,960 --> 00:18:23,900
very interesting alright so that is um

00:18:20,620 --> 00:18:26,860
more so on the yeah leveraging trigram

00:18:23,900 --> 00:18:29,900
as sort of as a stepping stone for

00:18:26,860 --> 00:18:31,940
bigger analyses and then you can also

00:18:29,900 --> 00:18:35,030
build the difficult models on top of the

00:18:31,940 --> 00:18:40,790
score as a feature something to know is

00:18:35,030 --> 00:18:44,170
I only have about 120 mm records in my

00:18:40,790 --> 00:18:47,300
table and so my dataset isn't very large

00:18:44,170 --> 00:18:51,400
but even so the the query

00:18:47,300 --> 00:18:54,380
that was the an on an enquiry that was

00:18:51,400 --> 00:18:57,260
0.485 milliseconds and then the soundex

00:18:54,380 --> 00:19:01,100
query on 100k records was actually 44

00:18:57,260 --> 00:19:03,080
milliseconds and even scarier is the

00:19:01,100 --> 00:19:05,600
trigram query so if you think about a

00:19:03,080 --> 00:19:09,290
four trigram it has to calculate the 10

00:19:05,600 --> 00:19:13,780
grams for all of the strings in the

00:19:09,290 --> 00:19:18,800
table and evaluate the similarity there

00:19:13,780 --> 00:19:21,260
so let me when I ran a analysis on the

00:19:18,800 --> 00:19:24,680
query and within postgres the execution

00:19:21,260 --> 00:19:27,710
time is actually 867 milliseconds which

00:19:24,680 --> 00:19:33,230
is kind of scary in terms of how long

00:19:27,710 --> 00:19:37,820
that takes to to run some things to to

00:19:33,230 --> 00:19:40,820
tacko there are Postgres has these just

00:19:37,820 --> 00:19:44,060
engine indices and you can pray chess

00:19:40,820 --> 00:19:47,720
engine indices to pre calculate your

00:19:44,060 --> 00:19:50,170
your engrams for larger tables and this

00:19:47,720 --> 00:19:53,570
actually helps with performance a lot

00:19:50,170 --> 00:19:55,550
for my like really small table it

00:19:53,570 --> 00:19:57,730
doesn't really improve performance that

00:19:55,550 --> 00:19:57,730
much

00:19:58,849 --> 00:20:04,129
any more contacts for for trigram

00:20:01,969 --> 00:20:05,509
surgery instead of paneling one

00:20:04,129 --> 00:20:07,699
character at a time you're actually

00:20:05,509 --> 00:20:10,519
taking to consideration combination of

00:20:07,699 --> 00:20:12,319
characters and also their neighbors you

00:20:10,519 --> 00:20:15,409
can determine what your proper unit of

00:20:12,319 --> 00:20:18,069
analysis is one character at a time or

00:20:15,409 --> 00:20:22,279
two or three but it is slower to

00:20:18,069 --> 00:20:26,449
calculate especially on the fly and it

00:20:22,279 --> 00:20:29,319
is slower to to search in general okay

00:20:26,449 --> 00:20:32,059
so the screen goes to semantic search

00:20:29,319 --> 00:20:33,649
yeah all this kinetic stuff is super

00:20:32,059 --> 00:20:36,919
helpful and it gets you one step further

00:20:33,649 --> 00:20:38,989
but what happens you know if you want to

00:20:36,919 --> 00:20:43,729
take into consideration the meaning of

00:20:38,989 --> 00:20:46,549
the words and this is pretty interesting

00:20:43,729 --> 00:20:49,190
in terms of this is like the context for

00:20:46,549 --> 00:20:51,709
semantic stuff matter is I'm sorry and

00:20:49,190 --> 00:20:54,549
my bad mean the same thing but not

00:20:51,709 --> 00:20:59,629
really illness you're at a funeral sad

00:20:54,549 --> 00:21:03,859
and I'll show you two examples today NLP

00:20:59,629 --> 00:21:07,579
K and we're two BEC NLP K has word net

00:21:03,859 --> 00:21:10,519
and as it's built in corpus and the very

00:21:07,579 --> 00:21:14,089
simple code snippet die up here is if

00:21:10,519 --> 00:21:15,949
you you know look for two words with an

00:21:14,089 --> 00:21:18,409
NLT K you can actually sort of calculate

00:21:15,949 --> 00:21:20,929
the similarity between those two words

00:21:18,409 --> 00:21:25,519
and blue and green of colors and they're

00:21:20,929 --> 00:21:28,459
pretty pretty similar there where 2x is

00:21:25,519 --> 00:21:30,709
a it's from Google and actually now it's

00:21:28,459 --> 00:21:35,269
part of the tensor flow library it

00:21:30,709 --> 00:21:38,089
allows you to basically train a corpus

00:21:35,269 --> 00:21:41,919
in a neural network and it's maximize

00:21:38,089 --> 00:21:45,859
the conditional probability to take into

00:21:41,919 --> 00:21:48,949
context of the meaning of a word and I

00:21:45,859 --> 00:21:53,509
use the Train model to generate the

00:21:48,949 --> 00:21:56,779
vector form of each word and then once

00:21:53,509 --> 00:21:59,989
you have the vector form of a word you

00:21:56,779 --> 00:22:02,839
can actually use that to perform all

00:21:59,989 --> 00:22:05,549
sorts of calculations and you can use

00:22:02,839 --> 00:22:09,059
that to find similarities

00:22:05,549 --> 00:22:12,899
and a very common example is when you

00:22:09,059 --> 00:22:15,809
see the the the king vector the man

00:22:12,899 --> 00:22:20,759
vector cleaning vector you often see the

00:22:15,809 --> 00:22:23,369
example where King - man + Kringle limit

00:22:20,759 --> 00:22:30,989
or actually King - man plus women equals

00:22:23,369 --> 00:22:34,080
clean and ya word analogy is a very fun

00:22:30,989 --> 00:22:38,690
way to interact with word vectors so an

00:22:34,080 --> 00:22:42,869
example that I have is breakfast -

00:22:38,690 --> 00:22:45,210
sunshine and a + like time equals

00:22:42,869 --> 00:22:51,029
dinners I know you guys are thinking

00:22:45,210 --> 00:22:58,679
about dinner already and yeah let's see

00:22:51,029 --> 00:23:00,809
the example I have here ok so I kind of

00:22:58,679 --> 00:23:02,879
jute it a little bit and I pre train the

00:23:00,809 --> 00:23:09,980
model otherwise that takes a little bit

00:23:02,879 --> 00:23:14,009
of time and I used a very generic corpus

00:23:09,980 --> 00:23:16,139
something to to note is depending on

00:23:14,009 --> 00:23:20,940
what your use cases what your scenario

00:23:16,139 --> 00:23:22,529
is your corporis matters a lot and here

00:23:20,940 --> 00:23:25,499
I just use a very generic one and this

00:23:22,529 --> 00:23:32,580
is the vector representation of the word

00:23:25,499 --> 00:23:35,460
coffee okay and then I can actually get

00:23:32,580 --> 00:23:41,009
words that are consider similar to

00:23:35,460 --> 00:23:42,450
coffee and the distances between them so

00:23:41,009 --> 00:23:44,009
here is good distance is actually

00:23:42,450 --> 00:23:46,799
calculated as the cosine distance

00:23:44,009 --> 00:23:49,440
between two vectors and so coffee is

00:23:46,799 --> 00:23:52,739
very close to wheat and cotton and beef

00:23:49,440 --> 00:23:59,460
and such let's see what I get when I

00:23:52,739 --> 00:24:01,889
type in Python oh my god this is I had

00:23:59,460 --> 00:24:04,230
not expected that because so you never

00:24:01,889 --> 00:24:05,999
know I put corporates is whether they

00:24:04,230 --> 00:24:08,340
interpret Python is like distinct and

00:24:05,999 --> 00:24:11,279
animal or as the language so Python is

00:24:08,340 --> 00:24:18,990
close to PHP I don't have that event or

00:24:11,279 --> 00:24:22,820
makes people feel good ok alright and

00:24:18,990 --> 00:24:29,519
this is the example for the word analogy

00:24:22,820 --> 00:24:31,980
so coffee + night - day I get tea and

00:24:29,519 --> 00:24:39,210
and all sorts of quirky things let's see

00:24:31,980 --> 00:24:41,269
I have the Sun + Co - warm gets moving

00:24:39,210 --> 00:24:47,700
and things like that

00:24:41,269 --> 00:24:49,830
alright let me go back to the sides so

00:24:47,700 --> 00:24:52,669
fuzzy search is very powerful for adding

00:24:49,830 --> 00:24:54,929
context to qualitative data and also for

00:24:52,669 --> 00:24:56,820
turning a lot of your qualitative data

00:24:54,929 --> 00:24:59,429
into quantitative data so you can do

00:24:56,820 --> 00:25:02,879
more with it like build models or you

00:24:59,429 --> 00:25:05,309
know rank things it can't be used as a

00:25:02,879 --> 00:25:08,059
stepping stone to broader analysis or

00:25:05,309 --> 00:25:10,379
you know if your purpose is to just find

00:25:08,059 --> 00:25:13,769
search for things that is a very

00:25:10,379 --> 00:25:17,519
powerful search mechanism and you can

00:25:13,769 --> 00:25:21,929
customize a lot of the syntax use cases

00:25:17,519 --> 00:25:23,609
for for things that for whatever your

00:25:21,929 --> 00:25:26,609
use case our scenario is you can also

00:25:23,609 --> 00:25:28,230
customize the semantic search ones by

00:25:26,609 --> 00:25:31,440
building out your own corporate and

00:25:28,230 --> 00:25:33,769
training on that and yeah that is it for

00:25:31,440 --> 00:25:37,980
my presentation again you can find

00:25:33,769 --> 00:25:41,009
content on github and feel free to ask

00:25:37,980 --> 00:25:41,240
questions or all of afterwards thank you

00:25:41,009 --> 00:25:42,600
guys

00:25:41,240 --> 00:25:43,930
[Applause]

00:25:42,600 --> 00:25:49,210
[Music]

00:25:43,930 --> 00:25:51,890
[Applause]

00:25:49,210 --> 00:25:53,810
so we still have time for questions so

00:25:51,890 --> 00:26:02,990
if anybody has questions please queue up

00:25:53,810 --> 00:26:06,440
here to the mic so first of all thank

00:26:02,990 --> 00:26:08,480
you very much I had a question regarding

00:26:06,440 --> 00:26:10,730
the sound EXO already had experience

00:26:08,480 --> 00:26:14,470
with like three grams and engrams for

00:26:10,730 --> 00:26:16,280
building native based classifiers

00:26:14,470 --> 00:26:17,870
probably the simple example would be

00:26:16,280 --> 00:26:19,520
like spam detection and stuff like that

00:26:17,870 --> 00:26:21,830
but I wanted to know if you or anyone

00:26:19,520 --> 00:26:25,010
else here had experience using sound x

00:26:21,830 --> 00:26:28,520
to generate features for an a based

00:26:25,010 --> 00:26:31,370
classifier or something similar yeah I

00:26:28,520 --> 00:26:34,370
think um in my experience I work a lot

00:26:31,370 --> 00:26:36,170
with abbreviated data so that's where I

00:26:34,370 --> 00:26:39,650
sound X and there's there's another

00:26:36,170 --> 00:26:42,320
library thing called meta phonics comes

00:26:39,650 --> 00:26:44,930
in and it's it's helpful because with a

00:26:42,320 --> 00:26:47,660
preview data what happens is that a lot

00:26:44,930 --> 00:26:50,360
of the vowels are taken out so sound

00:26:47,660 --> 00:26:51,650
acts actually becomes just better in

00:26:50,360 --> 00:26:54,650
terms of metric than something like

00:26:51,650 --> 00:26:56,330
Lebanon or trigram because all the

00:26:54,650 --> 00:26:58,610
vowels are taking also still like sound

00:26:56,330 --> 00:27:01,550
the same but it's a very very different

00:26:58,610 --> 00:27:04,010
string so that's that's in terms of my

00:27:01,550 --> 00:27:06,260
experience I don't have anyone in the

00:27:04,010 --> 00:27:12,440
audience has other experiences a sound

00:27:06,260 --> 00:27:14,600
accent thank you cool thanks of the talk

00:27:12,440 --> 00:27:16,790
yeah so kind of knowing that there's no

00:27:14,600 --> 00:27:19,040
great answers question but just curious

00:27:16,790 --> 00:27:20,390
to your approach like threshold setting

00:27:19,040 --> 00:27:21,940
right when we're talking about these

00:27:20,390 --> 00:27:24,860
things if you're doing an ordinal

00:27:21,940 --> 00:27:27,440
comparison ranking it's fine but if you

00:27:24,860 --> 00:27:28,910
really need inclusion exclusion how do

00:27:27,440 --> 00:27:34,330
you approach the problem of what do we

00:27:28,910 --> 00:27:36,770
set that at yeah I think when I saw L

00:27:34,330 --> 00:27:38,720
something that was really helpful to to

00:27:36,770 --> 00:27:41,510
me was we we actually like throughout

00:27:38,720 --> 00:27:43,310
this ROC curve of thresholds that we

00:27:41,510 --> 00:27:45,320
were willing to tolerate and how that

00:27:43,310 --> 00:27:47,650
affected our performance in terms of

00:27:45,320 --> 00:27:50,060
model performance and things like that

00:27:47,650 --> 00:27:51,590
so yeah it is a very much a guessing

00:27:50,060 --> 00:27:54,140
game and how you're how much you're

00:27:51,590 --> 00:27:55,340
willing to tolerate and being able to to

00:27:54,140 --> 00:27:58,700
tweak that on-the-fly

00:27:55,340 --> 00:28:01,340
writing reading code that is easy to

00:27:58,700 --> 00:28:02,720
edit later on where you

00:28:01,340 --> 00:28:05,200
your mind about how you want to approach

00:28:02,720 --> 00:28:09,470
that threshold is always helpful as well

00:28:05,200 --> 00:28:13,820
yeah first off thank you this is a

00:28:09,470 --> 00:28:16,940
really excellent overview um I was just

00:28:13,820 --> 00:28:20,630
wondering if anybody had taken the step

00:28:16,940 --> 00:28:23,290
of putting the output of sound X or

00:28:20,630 --> 00:28:25,460
anything like that into word devack and

00:28:23,290 --> 00:28:27,200
trying to train with that it seems like

00:28:25,460 --> 00:28:30,380
it might be pretty powerful are you

00:28:27,200 --> 00:28:33,470
aware of anything like that I am not

00:28:30,380 --> 00:28:36,170
aware of how that would would play

00:28:33,470 --> 00:28:38,990
I think it could be really interesting I

00:28:36,170 --> 00:28:42,080
didn't play around with taking the

00:28:38,990 --> 00:28:44,150
output of sound X and putting that

00:28:42,080 --> 00:28:47,090
through love inch dying as like a kind

00:28:44,150 --> 00:28:50,930
of like Oh calculating the metric the

00:28:47,090 --> 00:28:54,050
distance metric a different way I think

00:28:50,930 --> 00:28:56,480
it's a little bit hard because the code

00:28:54,050 --> 00:28:58,910
is just limited to those four characters

00:28:56,480 --> 00:29:01,190
and how much information that that can

00:28:58,910 --> 00:29:06,800
possibly provide but that's yeah that

00:29:01,190 --> 00:29:09,740
could be a fun weekend project do you

00:29:06,800 --> 00:29:12,070
know if any of these approaches can be

00:29:09,740 --> 00:29:14,420
vectorized and if they're not currently

00:29:12,070 --> 00:29:16,160
or at least the ones that you put up if

00:29:14,420 --> 00:29:20,450
there are any packages that can do

00:29:16,160 --> 00:29:23,930
vectorized fuzzy searching so as in

00:29:20,450 --> 00:29:27,530
terms of like other than the words avec

00:29:23,930 --> 00:29:31,370
vectorization i guess i'm not really

00:29:27,530 --> 00:29:33,500
sure how it kind of ties in to what your

00:29:31,370 --> 00:29:37,870
question was is can you bet their eyes I

00:29:33,500 --> 00:29:41,410
guess the do me like vectorizing as in

00:29:37,870 --> 00:29:44,240
turning it into an operation or

00:29:41,410 --> 00:29:47,020
vectorizing like the results of like

00:29:44,240 --> 00:29:51,680
sound axel WebAssign mainly the former

00:29:47,020 --> 00:29:54,880
okay former yeah I'm not sure or not but

00:29:51,680 --> 00:29:56,960
I imagine that somewhere on the internet

00:29:54,880 --> 00:29:59,420
someone has tried to vectorize these

00:29:56,960 --> 00:30:04,150
those operations that it would be

00:29:59,420 --> 00:30:04,150
intuitive thing to do thank you

00:30:09,130 --> 00:30:13,480
so thank you for this wonderful talk

00:30:14,050 --> 00:30:21,900

YouTube URL: https://www.youtube.com/watch?v=kTS2b6pGElE


