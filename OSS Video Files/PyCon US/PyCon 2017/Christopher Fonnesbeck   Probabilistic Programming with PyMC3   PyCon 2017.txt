Title: Christopher Fonnesbeck   Probabilistic Programming with PyMC3   PyCon 2017
Publication date: 2017-05-21
Playlist: PyCon 2017
Description: 
	"Speaker: Christopher Fonnesbeck

Bayesian statistics offers robust and flexible methods for data analysis that, because they are based on probability models, have the added benefit of being readily interpretable by non-statisticians. Until recently, however, the implementation of Bayesian models has been prohibitively complex for use by most analysts. But, the advent of probabilistic programming has served to abstract the complexity of Bayesian statistics, making such methods more broadly available. PyMC3 is a open-source Python module for probabilistic programming that implements several modern, computationally-intensive statistical algorithms for fitting Bayesian models, including Hamiltonian Monte Carlo (HMC) and variational inference. PyMC3â€™s intuitive syntax is helpful for new users, and the reliance on Theano for much of the computational work has allowed developers to keep the code base simple, making it easy to extend the software to meet analytic needs. PyMC3 itself extends Python's powerful ""scientific stack"" of development tools, which provide fast and efficient data structures, parallel processing, and interfaces for describing statistical models.  

Slides can be found at: https://speakerdeck.com/pycon2017 and https://github.com/PyCon/2017-slides"
Captions: 
	00:00:00,000 --> 00:00:05,069
right good afternoon so we have Chris

00:00:03,060 --> 00:00:07,440
Vaughn back here from Vanderbilt

00:00:05,069 --> 00:00:09,719
University I think and he's going to

00:00:07,440 --> 00:00:12,150
talk to you about probability which

00:00:09,719 --> 00:00:16,549
provides you lots of room for an app or

00:00:12,150 --> 00:00:16,549
something exciting will happen who knows

00:00:16,789 --> 00:00:27,960
all right okay I welcome everyone as I

00:00:25,800 --> 00:00:30,300
was introduced I come from Nashville

00:00:27,960 --> 00:00:32,430
Tennessee or at least I live there in

00:00:30,300 --> 00:00:34,170
the Department of biostatistics at the

00:00:32,430 --> 00:00:36,899
Vandy Medical Center so I'm going to

00:00:34,170 --> 00:00:39,750
talk about probabilistic programming

00:00:36,899 --> 00:00:43,440
I'll begin by defining it probabilistic

00:00:39,750 --> 00:00:45,210
programming is not new that the term is

00:00:43,440 --> 00:00:47,010
new it's one of those things that we've

00:00:45,210 --> 00:00:48,030
been doing for quite a while kind of

00:00:47,010 --> 00:00:51,480
like data science

00:00:48,030 --> 00:00:54,059
it just has a new name now probabilistic

00:00:51,480 --> 00:00:57,239
programming is any language that's used

00:00:54,059 --> 00:00:59,070
to describe and fit probability models

00:00:57,239 --> 00:01:00,649
has nothing to do with code being

00:00:59,070 --> 00:01:02,850
randomly generated or anything like that

00:01:00,649 --> 00:01:06,210
it's the probability modeling so to

00:01:02,850 --> 00:01:08,460
illustrate one of the characteristics of

00:01:06,210 --> 00:01:11,820
a probabilistic programming language is

00:01:08,460 --> 00:01:13,860
that it has language primitives that are

00:01:11,820 --> 00:01:16,259
stochastic so they have random

00:01:13,860 --> 00:01:19,380
quantities qualities to it so for

00:01:16,259 --> 00:01:23,460
example we may want to define a

00:01:19,380 --> 00:01:25,710
distribution over some various values of

00:01:23,460 --> 00:01:28,680
a variable X and instead of assigning a

00:01:25,710 --> 00:01:31,439
value to it we we assign a probability

00:01:28,680 --> 00:01:33,479
distribution so here it's a normal

00:01:31,439 --> 00:01:35,700
distribution normal distribution with

00:01:33,479 --> 00:01:39,060
some mean mu and standard deviation

00:01:35,700 --> 00:01:42,090
Sigma and it's got a method for example

00:01:39,060 --> 00:01:44,579
that will draw random values in this

00:01:42,090 --> 00:01:47,490
case 100 of them and find that to a

00:01:44,579 --> 00:01:48,780
variable you can get more complicated

00:01:47,490 --> 00:01:51,420
than that you can have distributions

00:01:48,780 --> 00:01:53,159
over functions where each realization

00:01:51,420 --> 00:01:58,049
each random sample is an entire function

00:01:53,159 --> 00:02:01,350
not just a value and importantly what we

00:01:58,049 --> 00:02:03,360
were able to do is condition variables

00:02:01,350 --> 00:02:06,420
based on others so we might say that

00:02:03,360 --> 00:02:08,340
some variable P a probability is beta

00:02:06,420 --> 00:02:09,660
distributed don't worry about if you

00:02:08,340 --> 00:02:11,700
don't know what that is it's just a

00:02:09,660 --> 00:02:13,770
different probability distribution and

00:02:11,700 --> 00:02:16,320
then we're going to use

00:02:13,770 --> 00:02:19,110
that to condition the value of another

00:02:16,320 --> 00:02:22,010
random variable in this case a Bernoulli

00:02:19,110 --> 00:02:25,830
and what this does it allows us to

00:02:22,010 --> 00:02:31,680
specify probability models at a very

00:02:25,830 --> 00:02:33,840
high level what we use PP for is it

00:02:31,680 --> 00:02:35,700
allows us to well it facilitates the

00:02:33,840 --> 00:02:38,820
application of Bayesian methods and

00:02:35,700 --> 00:02:41,490
that's a area of high interest amongst

00:02:38,820 --> 00:02:43,290
statisticians and others and the thing

00:02:41,490 --> 00:02:44,940
about Bayesian inference is that it

00:02:43,290 --> 00:02:48,120
interprets probabilities a little bit

00:02:44,940 --> 00:02:51,180
differently than classical statistics

00:02:48,120 --> 00:02:54,570
and in particular it uses the definition

00:02:51,180 --> 00:02:56,940
of probability it's known as inverse

00:02:54,570 --> 00:03:00,600
probability and it's inverse because it

00:02:56,940 --> 00:03:05,040
reasons backwards from effects to causes

00:03:00,600 --> 00:03:06,480
so we may have observations why data

00:03:05,040 --> 00:03:09,270
things that we observe those are the

00:03:06,480 --> 00:03:10,860
effects and we can use those quantities

00:03:09,270 --> 00:03:12,930
in conditioning statements to help

00:03:10,860 --> 00:03:15,720
determine what the causes might be so

00:03:12,930 --> 00:03:18,480
the bar is a condition a conditioning

00:03:15,720 --> 00:03:20,640
operator and the theta can be some

00:03:18,480 --> 00:03:24,330
vector of parameters that we might be

00:03:20,640 --> 00:03:27,060
interested in so why do we need a whole

00:03:24,330 --> 00:03:29,400
other paradigm of statistics well there

00:03:27,060 --> 00:03:32,010
are a lot of very good reasons for this

00:03:29,400 --> 00:03:35,100
the one that I like the best is is that

00:03:32,010 --> 00:03:38,400
it's a pragmatic justification it allows

00:03:35,100 --> 00:03:40,230
me to build really complex well more

00:03:38,400 --> 00:03:42,120
importantly very realistic models of

00:03:40,230 --> 00:03:45,150
things so I'm an epidemiologist I build

00:03:42,120 --> 00:03:47,720
disease models and Bayes offers me a lot

00:03:45,150 --> 00:03:50,640
of flexibility and that flexibility is

00:03:47,720 --> 00:03:52,650
partly because of this simplicity and

00:03:50,640 --> 00:03:54,810
this ability to break break down big

00:03:52,650 --> 00:03:58,800
models into small conditional pieces

00:03:54,810 --> 00:04:00,209
so here's Bayes formula this is the only

00:03:58,800 --> 00:04:03,420
estimator that you need if you're a

00:04:00,209 --> 00:04:05,490
Bayesian and the thing about it is that

00:04:03,420 --> 00:04:07,020
it uses probability distributions to

00:04:05,490 --> 00:04:09,180
characterize what we know or don't know

00:04:07,020 --> 00:04:13,200
about some unknown quantity or

00:04:09,180 --> 00:04:15,000
quantities that we care about and so at

00:04:13,200 --> 00:04:17,160
the far end we have a prior probability

00:04:15,000 --> 00:04:19,200
that that essentially encapsulate Sall

00:04:17,160 --> 00:04:23,040
the information that we know about our

00:04:19,200 --> 00:04:25,440
unknowns before we observe the world or

00:04:23,040 --> 00:04:27,540
do an experiment and then we take data

00:04:25,440 --> 00:04:30,060
that we observe and we update

00:04:27,540 --> 00:04:31,620
that prior probability to a what we call

00:04:30,060 --> 00:04:33,870
a posterior it's called posterior

00:04:31,620 --> 00:04:35,220
because it's after you've observed the

00:04:33,870 --> 00:04:38,190
data and that's why we condition on Y

00:04:35,220 --> 00:04:40,590
and the nice thing is is that the

00:04:38,190 --> 00:04:42,600
outputs from probabilistic programs are

00:04:40,590 --> 00:04:46,080
always going to be in probabilistic

00:04:42,600 --> 00:04:47,940
terms and so you don't just get a single

00:04:46,080 --> 00:04:49,700
value a single point estimate or a

00:04:47,940 --> 00:04:51,960
single prediction you get an entire

00:04:49,700 --> 00:04:54,180
distribution of them and what that it

00:04:51,960 --> 00:04:55,590
allows us to do is provide measures of

00:04:54,180 --> 00:04:57,750
uncertainty associated with those

00:04:55,590 --> 00:05:00,210
estimates it allows us to ask questions

00:04:57,750 --> 00:05:01,890
like what's the probability that the

00:05:00,210 --> 00:05:06,060
true value is bigger than zero or bigger

00:05:01,890 --> 00:05:07,140
than some other value of interest so

00:05:06,060 --> 00:05:09,990
that's what it is

00:05:07,140 --> 00:05:12,720
how do we do it there's three easy steps

00:05:09,990 --> 00:05:16,530
to probabilistic programming the first

00:05:12,720 --> 00:05:19,200
is encode a probability model specify a

00:05:16,530 --> 00:05:22,530
probability model in any language you

00:05:19,200 --> 00:05:22,920
like as long as it's Python how do we do

00:05:22,530 --> 00:05:27,420
that

00:05:22,920 --> 00:05:28,530
well first we specify our priors so

00:05:27,420 --> 00:05:30,960
again this is quantifying the

00:05:28,530 --> 00:05:33,120
uncertainty in our latent variables and

00:05:30,960 --> 00:05:34,560
so we might pick say if we have

00:05:33,120 --> 00:05:36,360
something that we know the values are

00:05:34,560 --> 00:05:38,430
somewhere on either side of zero we

00:05:36,360 --> 00:05:41,480
might pick a is a standard normal normal

00:05:38,430 --> 00:05:43,260
it means zero and a variance of of one

00:05:41,480 --> 00:05:44,640
you've probably seen normal

00:05:43,260 --> 00:05:46,110
distributions before you might not have

00:05:44,640 --> 00:05:48,240
seen this one though this is a normal

00:05:46,110 --> 00:05:50,130
distribution to that line across the

00:05:48,240 --> 00:05:53,250
bottom this is a normal with mean 0 and

00:05:50,130 --> 00:05:55,560
standard deviation 100 so this is

00:05:53,250 --> 00:05:58,170
encoding lack of information we don't

00:05:55,560 --> 00:06:00,870
have really any preference almost on

00:05:58,170 --> 00:06:05,190
whole real line in terms of what value

00:06:00,870 --> 00:06:06,930
it's supposed to be if I'm studying a so

00:06:05,190 --> 00:06:09,960
I'm trying to estimate say the

00:06:06,930 --> 00:06:11,220
prevalence of a rare disease I might

00:06:09,960 --> 00:06:13,890
pick something like this where the

00:06:11,220 --> 00:06:18,420
probabilities piled up close to zero

00:06:13,890 --> 00:06:20,930
somewhere less than 0.1 if you're

00:06:18,420 --> 00:06:23,460
geneticists how many people have done

00:06:20,930 --> 00:06:25,110
fruit fly breeding experiments as an

00:06:23,460 --> 00:06:27,990
undergraduate in biology anybody a

00:06:25,110 --> 00:06:30,300
couple people I close my eyes I can

00:06:27,990 --> 00:06:31,260
still smell the ether from that and so

00:06:30,300 --> 00:06:33,120
let's say we're doing breeding

00:06:31,260 --> 00:06:37,050
experiments we want to measure let's say

00:06:33,120 --> 00:06:39,180
the variation in fly wing widths well we

00:06:37,050 --> 00:06:41,370
wouldn't put a big normal prior on that

00:06:39,180 --> 00:06:44,100
we know that wing fly widths can

00:06:41,370 --> 00:06:46,580
very so much and so we might pick a

00:06:44,100 --> 00:06:49,050
little log normal distribution like that

00:06:46,580 --> 00:06:50,570
let's say we're say berm attrition or

00:06:49,050 --> 00:06:52,800
we're just a baseball fan and our

00:06:50,570 --> 00:06:54,900
favorite player has just started the

00:06:52,800 --> 00:06:56,190
season by going for for ten four hits

00:06:54,900 --> 00:06:58,020
and kind of bats and we want to know the

00:06:56,190 --> 00:07:00,720
probability that he's going to maintain

00:06:58,020 --> 00:07:01,350
that 400 batting average for the rest of

00:07:00,720 --> 00:07:04,530
the season

00:07:01,350 --> 00:07:08,370
well the distribution I've shown here is

00:07:04,530 --> 00:07:09,930
based on all of the batters in Major

00:07:08,370 --> 00:07:11,550
League Baseball going back as far as you

00:07:09,930 --> 00:07:14,460
like in history and it turns out to be a

00:07:11,550 --> 00:07:16,260
mean of about 0.26 with a standard

00:07:14,460 --> 00:07:18,120
deviation of about point zero three

00:07:16,260 --> 00:07:20,220
that's what we should be using as our

00:07:18,120 --> 00:07:23,550
prior because we we know how baseball

00:07:20,220 --> 00:07:25,830
players perform next is the likelihood

00:07:23,550 --> 00:07:27,990
function the likelihood function is

00:07:25,830 --> 00:07:30,449
basically our data generating mechanism

00:07:27,990 --> 00:07:31,860
and it conditions on our model on the

00:07:30,449 --> 00:07:35,460
observed data is how we update that

00:07:31,860 --> 00:07:36,570
prior probability so these would might

00:07:35,460 --> 00:07:39,120
be things like a normal distribution

00:07:36,570 --> 00:07:41,270
let's say we're studying outcomes in

00:07:39,120 --> 00:07:44,010
terms of height and weight of patients

00:07:41,270 --> 00:07:45,690
those tend to be normally distributed if

00:07:44,010 --> 00:07:47,639
we're doing our baseball example a

00:07:45,690 --> 00:07:50,789
binomial distribution describes the

00:07:47,639 --> 00:07:53,550
number of hits in n at-bats based on

00:07:50,789 --> 00:07:55,080
some probability of getting a hit if

00:07:53,550 --> 00:07:56,370
you're doing web analytics and you want

00:07:55,080 --> 00:07:57,840
to know how many people you want to

00:07:56,370 --> 00:08:01,560
model how many people are visiting your

00:07:57,840 --> 00:08:03,870
website every month year day you might

00:08:01,560 --> 00:08:06,900
use a Poisson distribution for modeling

00:08:03,870 --> 00:08:09,870
counts that sort of thing next step is

00:08:06,900 --> 00:08:11,010
we want to infer the values for our

00:08:09,870 --> 00:08:13,260
latent variables the posterior

00:08:11,010 --> 00:08:16,349
distributions and it seems pretty

00:08:13,260 --> 00:08:18,000
straightforward we update our prior

00:08:16,349 --> 00:08:20,550
distribution with our likelihood to get

00:08:18,000 --> 00:08:22,349
a posterior but this is a none

00:08:20,550 --> 00:08:26,039
normalized form here we're forgetting

00:08:22,349 --> 00:08:28,229
one little piece which is this marginal

00:08:26,039 --> 00:08:29,370
likelihood or it's called the model

00:08:28,229 --> 00:08:32,550
evidence it's just really just

00:08:29,370 --> 00:08:34,650
normalizing factor but when you figure

00:08:32,550 --> 00:08:37,440
out what it is turns out that it's the

00:08:34,650 --> 00:08:39,330
numerator integrated over all of theta

00:08:37,440 --> 00:08:40,919
we're going to integrate out theta and

00:08:39,330 --> 00:08:43,640
that's fine if we only have say one or

00:08:40,919 --> 00:08:46,200
two parameters some of us can do

00:08:43,640 --> 00:08:48,089
integration pretty easily with a couple

00:08:46,200 --> 00:08:50,790
of variables but if you've got ten

00:08:48,089 --> 00:08:52,290
hundred thousand million parameters it's

00:08:50,790 --> 00:08:54,660
not possible to do that and so we

00:08:52,290 --> 00:08:56,069
require numerical methods and one

00:08:54,660 --> 00:08:57,899
things that probabilistic programming

00:08:56,069 --> 00:08:59,490
does it allows us to abstract that

00:08:57,899 --> 00:09:00,449
inference procedure and that's all I'm

00:08:59,490 --> 00:09:02,190
going to say about it right now I'm

00:09:00,449 --> 00:09:03,449
going to circle back to it later but

00:09:02,190 --> 00:09:05,610
we're going to consider it a black box

00:09:03,449 --> 00:09:07,980
from a probabilistic programming

00:09:05,610 --> 00:09:09,600
perspective and then the last step that

00:09:07,980 --> 00:09:12,300
people often overlook is you have to

00:09:09,600 --> 00:09:14,579
check your model the model outputs the

00:09:12,300 --> 00:09:17,040
validity of them are conditional on the

00:09:14,579 --> 00:09:18,720
specification of your model and you

00:09:17,040 --> 00:09:20,129
specify your model based on a whole

00:09:18,720 --> 00:09:22,860
bunch of assumptions that are largely

00:09:20,129 --> 00:09:24,569
unverifiable so you've got to go back in

00:09:22,860 --> 00:09:27,389
and check them all are the results

00:09:24,569 --> 00:09:29,370
reasonable one nice way of doing that is

00:09:27,389 --> 00:09:31,740
to actually simulate data from your

00:09:29,370 --> 00:09:33,360
model and compare that simulated data to

00:09:31,740 --> 00:09:35,490
the model or the data that was used to

00:09:33,360 --> 00:09:37,319
fit the model and you can see in this

00:09:35,490 --> 00:09:39,629
case they line up quite nicely these

00:09:37,319 --> 00:09:42,990
could have been drawn from the model

00:09:39,629 --> 00:09:45,110
that we we've used so like I said

00:09:42,990 --> 00:09:46,740
probabilistic program is not new

00:09:45,110 --> 00:09:48,750
statisticians have been doing

00:09:46,740 --> 00:09:51,120
probabilistic programming since the

00:09:48,750 --> 00:09:54,480
grunge era so in the kind of the early

00:09:51,120 --> 00:09:56,100
90s package called wind bugs came along

00:09:54,480 --> 00:09:59,670
from the folks at the University College

00:09:56,100 --> 00:10:03,380
of London and it was great it exposed

00:09:59,670 --> 00:10:06,630
Bayesian methods to thousands of people

00:10:03,380 --> 00:10:08,579
statisticians applied scientists who

00:10:06,630 --> 00:10:12,569
never would have been able to do Bayes

00:10:08,579 --> 00:10:14,910
if they had to coded by hand and and it

00:10:12,569 --> 00:10:16,410
featured a probabilistic programming

00:10:14,910 --> 00:10:21,170
language the bugs language that allows

00:10:16,410 --> 00:10:21,170
you to specify variables distributions

00:10:22,189 --> 00:10:26,910
functions of distributions and nowhere

00:10:25,500 --> 00:10:28,740
in here do you have to say how you fit

00:10:26,910 --> 00:10:32,850
the model bugs would do that for you

00:10:28,740 --> 00:10:35,850
uses MCMC but you know after using that

00:10:32,850 --> 00:10:37,800
myself for quite some time sort sort of

00:10:35,850 --> 00:10:39,899
come frustrated with it for one it's you

00:10:37,800 --> 00:10:42,199
know is closed source and even when they

00:10:39,899 --> 00:10:44,850
did open up the source code it's

00:10:42,199 --> 00:10:46,559
implemented in object pascal so the word

00:10:44,850 --> 00:10:49,470
we have many opportunities for me to

00:10:46,559 --> 00:10:52,500
contribute and really i don't want to

00:10:49,470 --> 00:10:54,899
specify my models using some crappy

00:10:52,500 --> 00:10:58,139
domain-specific language I want to use

00:10:54,899 --> 00:11:00,660
Python so when I was graduate student I

00:10:58,139 --> 00:11:02,370
started hacking away at some Python code

00:11:00,660 --> 00:11:04,500
for doing probabilistic programming or

00:11:02,370 --> 00:11:06,540
Bayesian inference and at the time I

00:11:04,500 --> 00:11:08,160
didn't expect anybody to use it other

00:11:06,540 --> 00:11:11,100
than me which

00:11:08,160 --> 00:11:13,350
Planes it's atrocious lack of quality

00:11:11,100 --> 00:11:15,810
but over the years as I put this on

00:11:13,350 --> 00:11:17,850
initially SourceForge and now github

00:11:15,810 --> 00:11:19,709
it's attracted some really talented

00:11:17,850 --> 00:11:21,750
developers people who are much better at

00:11:19,709 --> 00:11:25,050
this than I am and the current version

00:11:21,750 --> 00:11:26,910
of prime c3 is now based on us siano

00:11:25,050 --> 00:11:29,100
which I'll describe in a minute and this

00:11:26,910 --> 00:11:32,209
allows us to implement what we might

00:11:29,100 --> 00:11:35,730
call next-generation Bayesian

00:11:32,209 --> 00:11:38,550
approximation procedures and and

00:11:35,730 --> 00:11:40,589
actually as of late last year we're now

00:11:38,550 --> 00:11:41,970
under the num focus umbrella so we're

00:11:40,589 --> 00:11:46,850
really happy about that it's already

00:11:41,970 --> 00:11:49,860
reaping benefits for us piano is a a

00:11:46,850 --> 00:11:52,889
Python library or at least it's got a

00:11:49,860 --> 00:11:55,170
Python API for specifying and evaluating

00:11:52,889 --> 00:11:57,470
mathematical expressions using tensors

00:11:55,170 --> 00:12:01,319
which are generalizations of

00:11:57,470 --> 00:12:04,470
multi-dimensional arrays and it comes

00:12:01,319 --> 00:12:05,910
out of Joshua Ben Gio's Lisa lab it's

00:12:04,470 --> 00:12:08,639
now called the Montreal Institute for

00:12:05,910 --> 00:12:10,769
learning algorithms and what it does it

00:12:08,639 --> 00:12:12,240
performs dynamic C code generation so it

00:12:10,769 --> 00:12:14,880
makes all this stuff very fast and

00:12:12,240 --> 00:12:16,470
importantly for us what it does is that

00:12:14,880 --> 00:12:18,509
it does very efficient automatic

00:12:16,470 --> 00:12:19,649
symbolic differentiation so for any

00:12:18,509 --> 00:12:22,740
model you throw at it

00:12:19,649 --> 00:12:25,259
you can get automatic gradients and this

00:12:22,740 --> 00:12:28,230
is a boon because we'll need them so to

00:12:25,259 --> 00:12:29,579
motivate show you how you implement a

00:12:28,230 --> 00:12:32,370
model in prime c3 I'm gonna motivate

00:12:29,579 --> 00:12:35,370
that using a real example this is a

00:12:32,370 --> 00:12:39,060
example using radon gas contamination

00:12:35,370 --> 00:12:42,120
radon gas is a radioactive odorless

00:12:39,060 --> 00:12:44,819
tasteless gas that is actually the

00:12:42,120 --> 00:12:46,800
primary non smoking cause of lung cancer

00:12:44,819 --> 00:12:50,639
and it gets into people's houses through

00:12:46,800 --> 00:12:53,600
the groundwater and and through cracks

00:12:50,639 --> 00:12:56,970
in the in the soil and the bedrock and

00:12:53,600 --> 00:12:58,980
and it prolonged exposure increases your

00:12:56,970 --> 00:13:01,589
risk of cancer and so the data we've got

00:12:58,980 --> 00:13:03,180
here is some monitoring data from

00:13:01,589 --> 00:13:05,040
households in the state of Minnesota

00:13:03,180 --> 00:13:07,709
from different counties across the state

00:13:05,040 --> 00:13:08,819
we're going to try to estimate the

00:13:07,709 --> 00:13:10,410
probability of a house being

00:13:08,819 --> 00:13:13,110
contaminated or the levels of

00:13:10,410 --> 00:13:17,009
contamination in various places around

00:13:13,110 --> 00:13:18,149
that state and so specify our model here

00:13:17,009 --> 00:13:21,089
what we're going to do is just use a

00:13:18,149 --> 00:13:22,170
very simple one it's going to have mean

00:13:21,089 --> 00:13:25,760
values for each

00:13:22,170 --> 00:13:28,080
County individually and then there's a

00:13:25,760 --> 00:13:29,790
effect of having a basement in your

00:13:28,080 --> 00:13:31,500
house if you have a basement you have a

00:13:29,790 --> 00:13:34,020
higher probability generally or it's

00:13:31,500 --> 00:13:35,130
expected of having radon contamination

00:13:34,020 --> 00:13:37,290
because it's easier for it to get up

00:13:35,130 --> 00:13:39,000
into the house and then this is the

00:13:37,290 --> 00:13:40,410
error term there is essentially the

00:13:39,000 --> 00:13:42,390
distribution of our data that represents

00:13:40,410 --> 00:13:44,610
measurement error or temporal within

00:13:42,390 --> 00:13:47,850
house variation or between house

00:13:44,610 --> 00:13:49,680
variation that sort of thing so what we

00:13:47,850 --> 00:13:53,130
do in Plan C is we instantiate this

00:13:49,680 --> 00:13:56,220
model object and this model object can

00:13:53,130 --> 00:13:57,690
be used in conjunction with a context

00:13:56,220 --> 00:14:01,140
manager and what that will do is that

00:13:57,690 --> 00:14:03,450
when we start specifying variables for

00:14:01,140 --> 00:14:05,010
our priors and our likelihood it will

00:14:03,450 --> 00:14:07,800
automatically add them to model so we

00:14:05,010 --> 00:14:09,750
have to add have to have add statements

00:14:07,800 --> 00:14:11,400
and so on and what we see art here are

00:14:09,750 --> 00:14:13,080
some what we call some stochastic nodes

00:14:11,400 --> 00:14:16,490
these are the variables in the model we

00:14:13,080 --> 00:14:20,550
don't know the true values of so the

00:14:16,490 --> 00:14:22,410
county specific means and the effect of

00:14:20,550 --> 00:14:24,810
basements and then there's the error

00:14:22,410 --> 00:14:27,390
term there the error variation and

00:14:24,810 --> 00:14:29,700
because we're using Python 3 we can use

00:14:27,390 --> 00:14:32,580
fancy Greek Unicode characters for our

00:14:29,700 --> 00:14:34,590
variables so if we introspect some of

00:14:32,580 --> 00:14:37,260
these guys so we look at beta and its

00:14:34,590 --> 00:14:41,280
type is it this free random variable in

00:14:37,260 --> 00:14:42,750
pi MC 3 and it's got very useful

00:14:41,280 --> 00:14:44,490
attributes it can calculate the log

00:14:42,750 --> 00:14:46,920
probability of any value that you pass

00:14:44,490 --> 00:14:50,310
to it and because this is a tensor a

00:14:46,920 --> 00:14:53,460
siano tensor it's evaluated lazily here

00:14:50,310 --> 00:14:54,720
and then we can sample from it we can

00:14:53,460 --> 00:14:58,010
draw random samples from that

00:14:54,720 --> 00:15:00,120
distribution as well next step is we

00:14:58,010 --> 00:15:01,500
specify essentially our regression

00:15:00,120 --> 00:15:03,480
function here right this is a linear

00:15:01,500 --> 00:15:06,000
combination of the data then we get an

00:15:03,480 --> 00:15:08,400
expected value for every county with or

00:15:06,000 --> 00:15:10,290
without a basement and then our

00:15:08,400 --> 00:15:13,050
likelihood is just another distribution

00:15:10,290 --> 00:15:15,330
a normal distribution in this case but

00:15:13,050 --> 00:15:16,830
here we add this additional arguments as

00:15:15,330 --> 00:15:20,670
observed and that's where we pass our

00:15:16,830 --> 00:15:22,290
data in okay so we've developed this

00:15:20,670 --> 00:15:25,380
graph essentially a directed acyclic

00:15:22,290 --> 00:15:28,470
graph and it's in C on O and we're good

00:15:25,380 --> 00:15:32,070
to go so now you know I kind of glazed

00:15:28,470 --> 00:15:33,840
over the whole posterior calculation

00:15:32,070 --> 00:15:35,970
just told you is hard

00:15:33,840 --> 00:15:38,220
it's analytically impossible it's

00:15:35,970 --> 00:15:40,110
numerically challenging so how do we you

00:15:38,220 --> 00:15:41,639
know how do we do it we're supposed to

00:15:40,110 --> 00:15:43,110
you know not worry our pretty little

00:15:41,639 --> 00:15:45,180
heads about it just let probabilistic

00:15:43,110 --> 00:15:47,490
programming take care of it but it sort

00:15:45,180 --> 00:15:49,319
of reminds you of the South Park

00:15:47,490 --> 00:15:51,509
underwear gnomes right where they have

00:15:49,319 --> 00:15:52,860
business plan to collect underwear and

00:15:51,509 --> 00:15:55,170
profit from it but they're missing the

00:15:52,860 --> 00:15:57,209
key step in there well similarly here

00:15:55,170 --> 00:15:59,550
I've kind of set up the same sort of

00:15:57,209 --> 00:16:02,029
thing how do we get this inference if we

00:15:59,550 --> 00:16:04,019
have to do impossible calculations and

00:16:02,029 --> 00:16:06,269
what we have to rely on are

00:16:04,019 --> 00:16:09,449
approximations of one sort or another

00:16:06,269 --> 00:16:12,870
and over the years different approaches

00:16:09,449 --> 00:16:16,129
have been established for that and they

00:16:12,870 --> 00:16:18,000
range from at the very top brute-force

00:16:16,129 --> 00:16:21,149
maximization of the unnormalized

00:16:18,000 --> 00:16:23,069
posterior so we're you will get a you

00:16:21,149 --> 00:16:24,779
know a modal value but you're not going

00:16:23,069 --> 00:16:27,209
to get any sort of uncertainty around

00:16:24,779 --> 00:16:28,649
it's not a fully Bayesian approach the

00:16:27,209 --> 00:16:30,810
second one you might do a normal

00:16:28,649 --> 00:16:32,730
approximation so assume the posteriors

00:16:30,810 --> 00:16:34,769
are normal which can be fine unless

00:16:32,730 --> 00:16:36,990
you've got you know some skewness or

00:16:34,769 --> 00:16:39,889
some covariance between them and then it

00:16:36,990 --> 00:16:42,839
presents it becomes a less realistic a

00:16:39,889 --> 00:16:45,379
less realistic approach and then a

00:16:42,839 --> 00:16:48,240
variety of sampling based approaches

00:16:45,379 --> 00:16:50,430
that folks have used and their adequacy

00:16:48,240 --> 00:16:53,730
kind of depends on your model and on the

00:16:50,430 --> 00:16:55,559
objectives of what you're doing the de

00:16:53,730 --> 00:16:57,540
facto standard for doing Bayesian

00:16:55,559 --> 00:16:59,730
inference is something called Markov

00:16:57,540 --> 00:17:01,470
chain Monte Carlo now while we're not

00:16:59,730 --> 00:17:02,850
able to sample independently from our

00:17:01,470 --> 00:17:06,209
posterior because we don't know exactly

00:17:02,850 --> 00:17:08,250
what it is we can sample in a dependent

00:17:06,209 --> 00:17:10,829
fashion that's what MCMC does it creates

00:17:08,250 --> 00:17:13,559
a Markov chain so Markov chain is just a

00:17:10,829 --> 00:17:15,780
sequence of variables whose next value

00:17:13,559 --> 00:17:20,339
depends on the value partly of the

00:17:15,780 --> 00:17:22,919
current element in the chain and if it

00:17:20,339 --> 00:17:28,159
has a certain satisfy certain criteria

00:17:22,919 --> 00:17:31,650
its its distribution will be

00:17:28,159 --> 00:17:33,900
indistinguishable from the posterior

00:17:31,650 --> 00:17:35,340
distribution of interest in particular

00:17:33,900 --> 00:17:39,120
it's going to satisfy this thing called

00:17:35,340 --> 00:17:41,610
the detailed balance equation and if you

00:17:39,120 --> 00:17:43,500
do that you can do MCMC and you can get

00:17:41,610 --> 00:17:47,010
a reasonably good approximation of your

00:17:43,500 --> 00:17:49,559
posterior and the workhorse of this

00:17:47,010 --> 00:17:51,149
class is something called the metropolis

00:17:49,559 --> 00:17:52,950
them actually our next speaker is going

00:17:51,149 --> 00:17:54,809
to talk about metropolis a fair bit so

00:17:52,950 --> 00:17:56,370
I'm going to punt on explaining that in

00:17:54,809 --> 00:17:59,490
great detail but you're essentially

00:17:56,370 --> 00:18:01,049
proposing values and you have some sort

00:17:59,490 --> 00:18:02,909
of function to either accept or reject

00:18:01,049 --> 00:18:06,149
it if you accept it you add that

00:18:02,909 --> 00:18:10,139
proposed value to your sample otherwise

00:18:06,149 --> 00:18:13,249
you retain the current value and you

00:18:10,139 --> 00:18:17,549
know it works fine in a lot of cases but

00:18:13,249 --> 00:18:18,960
where you have sort of larger models it

00:18:17,549 --> 00:18:20,639
can perform poorly it can have trouble

00:18:18,960 --> 00:18:23,159
converging so we're trying to get

00:18:20,639 --> 00:18:24,600
samples from this oval here and you're

00:18:23,159 --> 00:18:26,940
seeing you know the samples are kind of

00:18:24,600 --> 00:18:28,619
uneven it kind of stops from time to

00:18:26,940 --> 00:18:31,019
time and that means that it's getting

00:18:28,619 --> 00:18:34,139
its proposals rejected and so it's

00:18:31,019 --> 00:18:36,779
retaining that current value and it you

00:18:34,139 --> 00:18:38,999
often require tens or hundreds or even

00:18:36,779 --> 00:18:42,139
millions of thousands of samples even

00:18:38,999 --> 00:18:44,850
more to get a converging algorithm and

00:18:42,139 --> 00:18:46,169
and it's very inefficient its optimal

00:18:44,850 --> 00:18:49,559
acceptance rate is about twenty four

00:18:46,169 --> 00:18:50,730
percent so a newer Verte what that was

00:18:49,559 --> 00:18:51,649
would be what I call kind of

00:18:50,730 --> 00:18:54,840
first-generation

00:18:51,649 --> 00:18:57,059
MCMC the next generation you is uses a

00:18:54,840 --> 00:18:59,419
much better approach in the sense that

00:18:57,059 --> 00:19:02,159
we abandon this random walk behavior and

00:18:59,419 --> 00:19:03,690
we allow ourselves to make proposals

00:19:02,159 --> 00:19:05,639
that are more likely to be accepted and

00:19:03,690 --> 00:19:07,740
we do this by using gradient information

00:19:05,639 --> 00:19:09,539
we talked about siano providing us

00:19:07,740 --> 00:19:11,039
gradients it's required to do this sort

00:19:09,539 --> 00:19:13,200
of thing this is called Hamiltonian

00:19:11,039 --> 00:19:15,450
Monte Carlo we're essentially simulating

00:19:13,200 --> 00:19:17,309
a physical system Hamiltonian set of

00:19:15,450 --> 00:19:19,440
equations we have a frictionless

00:19:17,309 --> 00:19:21,950
particle that are like a marble rolling

00:19:19,440 --> 00:19:24,720
around on the manifold that is our

00:19:21,950 --> 00:19:27,029
posterior distribution so no more random

00:19:24,720 --> 00:19:29,389
walk so we we essentially flick our

00:19:27,029 --> 00:19:31,200
marble across the landscape and

00:19:29,389 --> 00:19:33,269
calculate a whole bunch of what we call

00:19:31,200 --> 00:19:36,360
leapfrog steps that discretize is this

00:19:33,269 --> 00:19:38,220
continuous system and then you accept a

00:19:36,360 --> 00:19:40,619
value at the far end so it allows you to

00:19:38,220 --> 00:19:42,629
go clear across the model doesn't get as

00:19:40,619 --> 00:19:45,059
many of those points rejected and so you

00:19:42,629 --> 00:19:47,220
get something like this that makes long

00:19:45,059 --> 00:19:48,659
confident jumps that are usually

00:19:47,220 --> 00:19:50,309
accepted because again it knows

00:19:48,659 --> 00:19:52,230
something about the gradient of the

00:19:50,309 --> 00:19:53,940
model and your sample ends up being

00:19:52,230 --> 00:19:57,509
pretty close to what you'd get if you

00:19:53,940 --> 00:19:59,369
were able to independently sample and

00:19:57,509 --> 00:20:01,590
the one that Prime C uses is an

00:19:59,369 --> 00:20:03,389
extension of nuts that sort of Auditors

00:20:01,590 --> 00:20:05,809
our extension of hm c call

00:20:03,389 --> 00:20:08,070
of nuts the no u-turn sampler that

00:20:05,809 --> 00:20:10,399
essentially Auto Tunes a lot of the

00:20:08,070 --> 00:20:12,839
hyper parameters and so on pie emcee

00:20:10,399 --> 00:20:14,849
icon looks like this you call sample on

00:20:12,839 --> 00:20:17,159
your model inside your context manager

00:20:14,849 --> 00:20:19,440
and it will draw a thousand or so is all

00:20:17,159 --> 00:20:21,809
you need for this one and initially

00:20:19,440 --> 00:20:23,519
initially it will try to initialize that

00:20:21,809 --> 00:20:25,049
algorithm using something called a BB I

00:20:23,519 --> 00:20:28,709
then I will talk about in a second and

00:20:25,049 --> 00:20:31,169
and it it runs pretty quickly so you get

00:20:28,709 --> 00:20:33,679
some nice posterior samples here that

00:20:31,169 --> 00:20:37,289
you can you can summarize and so for our

00:20:33,679 --> 00:20:39,299
our radon example you get you know

00:20:37,289 --> 00:20:42,719
estimates like this the dots are the are

00:20:39,299 --> 00:20:44,609
the data the blue lines are the county

00:20:42,719 --> 00:20:47,249
specific estimates and the red is the

00:20:44,609 --> 00:20:48,809
the overall mean it actually this action

00:20:47,249 --> 00:20:50,459
so we would do model checking on this we

00:20:48,809 --> 00:20:52,979
would find actually this is not a great

00:20:50,459 --> 00:20:56,219
model it actually over fits it's over

00:20:52,979 --> 00:20:58,859
fitting the ones with a relatively

00:20:56,219 --> 00:21:00,509
little data so we will go back and fit

00:20:58,859 --> 00:21:04,139
something better like a hierarchical

00:21:00,509 --> 00:21:05,879
model another approach that you can use

00:21:04,139 --> 00:21:07,139
the other sort of next-generation tool

00:21:05,879 --> 00:21:08,690
that we have at our disposal is

00:21:07,139 --> 00:21:11,219
something called variational inference

00:21:08,690 --> 00:21:13,739
so even with those more sophisticated

00:21:11,219 --> 00:21:15,599
algorithms MCMC can can be quite slow

00:21:13,739 --> 00:21:17,219
when you have Big Data and we're in the

00:21:15,599 --> 00:21:19,349
era of big data now right everybody

00:21:17,219 --> 00:21:20,940
wants everybody has bigger data set so

00:21:19,349 --> 00:21:24,239
they want to fit you really can't use

00:21:20,940 --> 00:21:27,209
MCMC of any kind with even moderately

00:21:24,239 --> 00:21:30,690
large data sets so another approach is

00:21:27,209 --> 00:21:32,459
to take some approximate unknown or some

00:21:30,690 --> 00:21:34,139
approximate known distribution and use

00:21:32,459 --> 00:21:37,619
that to approximate our unknown

00:21:34,139 --> 00:21:40,440
posterior and what we do is we transform

00:21:37,619 --> 00:21:43,679
that known form and parameterize it such

00:21:40,440 --> 00:21:47,369
that it it's as close as possible to the

00:21:43,679 --> 00:21:49,559
true distribution and we what this does

00:21:47,369 --> 00:21:51,659
is that changes our approach from that

00:21:49,559 --> 00:21:53,369
of a sampling based approach to an

00:21:51,659 --> 00:21:55,229
optimization approach we can just do

00:21:53,369 --> 00:21:56,489
straightforward optimization Eldar me

00:21:55,229 --> 00:21:58,469
that kind of thing that we know how to

00:21:56,489 --> 00:22:00,329
do that's fast the way that we measure

00:21:58,469 --> 00:22:02,969
closeness is based on something called

00:22:00,329 --> 00:22:05,329
kullbackleibler divergence it's an

00:22:02,969 --> 00:22:08,700
information distance essentially between

00:22:05,329 --> 00:22:10,799
distributions and this is the form here

00:22:08,700 --> 00:22:13,169
and but you can see that you know our

00:22:10,799 --> 00:22:14,399
posterior is in this function and so we

00:22:13,169 --> 00:22:16,529
don't know what that is so we can't

00:22:14,399 --> 00:22:17,309
optimize it directly but it turns out

00:22:16,529 --> 00:22:18,840
that minimize

00:22:17,309 --> 00:22:22,259
callback lobular divergence is

00:22:18,840 --> 00:22:23,639
equivalent to maximizing this piece in

00:22:22,259 --> 00:22:25,860
the middle here called the evidence

00:22:23,639 --> 00:22:28,860
lower bound so if we maximize that we

00:22:25,860 --> 00:22:31,110
minimize call back globular distance and

00:22:28,860 --> 00:22:33,570
that's great but again this has been

00:22:31,110 --> 00:22:35,549
around for a while but it's sort of very

00:22:33,570 --> 00:22:37,799
manual you got to go in and specify what

00:22:35,549 --> 00:22:40,200
you're approximating distribution is you

00:22:37,799 --> 00:22:42,090
got to make sure that it supports all

00:22:40,200 --> 00:22:44,759
the potential values of the variable

00:22:42,090 --> 00:22:49,009
that you're trying to approximate but

00:22:44,759 --> 00:22:51,899
very recently there's a great paper by

00:22:49,009 --> 00:22:54,179
Canseco beer and and colleagues that

00:22:51,899 --> 00:22:55,019
essentially automated this using

00:22:54,179 --> 00:22:56,700
something called automatic

00:22:55,019 --> 00:22:59,879
differentiation variational inference

00:22:56,700 --> 00:23:01,919
and what this does is it chooses the

00:22:59,879 --> 00:23:03,499
approximation for you and then there's a

00:23:01,919 --> 00:23:05,940
transformation so that all of the

00:23:03,499 --> 00:23:07,999
requirements are satisfied and then all

00:23:05,940 --> 00:23:10,590
you have to do is is hit the button and

00:23:07,999 --> 00:23:13,409
optimize and so what we get is something

00:23:10,590 --> 00:23:15,659
like this these are not samples this is

00:23:13,409 --> 00:23:17,490
the mean evidence lower bound you can

00:23:15,659 --> 00:23:19,740
see that it hits this asymptote so it

00:23:17,490 --> 00:23:22,110
converges on something and that

00:23:19,740 --> 00:23:24,389
something depends on what it is so

00:23:22,110 --> 00:23:29,039
here's a contrived example using a known

00:23:24,389 --> 00:23:31,409
true posterior a beta 147 255 that's the

00:23:29,039 --> 00:23:34,649
dotted one and you can see for various

00:23:31,409 --> 00:23:38,789
numbers of optimization iterations you

00:23:34,649 --> 00:23:40,860
get quite a good posterior approximation

00:23:38,789 --> 00:23:43,590
after a while and so in PI MC we've got

00:23:40,860 --> 00:23:45,419
this implemented as a fit function so we

00:23:43,590 --> 00:23:47,039
had sampled before now we have fit and

00:23:45,419 --> 00:23:51,360
it will go in and run the optimization

00:23:47,039 --> 00:23:53,580
and we get this approximation object and

00:23:51,360 --> 00:23:55,619
since this is a known distribution we

00:23:53,580 --> 00:23:57,980
can sample from this independently and

00:23:55,619 --> 00:24:00,539
get a sample just like we would with a

00:23:57,980 --> 00:24:03,720
MCMC approach and you know your mileage

00:24:00,539 --> 00:24:05,999
may vary these are approximations the

00:24:03,720 --> 00:24:07,919
trade-off is you can get an answer for

00:24:05,999 --> 00:24:10,200
large data sets in relatively short

00:24:07,919 --> 00:24:12,059
period of time the good news is is that

00:24:10,200 --> 00:24:13,110
the approximations are getting better

00:24:12,059 --> 00:24:14,759
and better we're getting better and

00:24:13,110 --> 00:24:16,830
better versions of these algorithms and

00:24:14,759 --> 00:24:19,710
we're starting to implement these in in

00:24:16,830 --> 00:24:21,929
pi MC 3 there's a whole bunch of new

00:24:19,710 --> 00:24:25,409
stuff where you know this used to be

00:24:21,929 --> 00:24:28,110
sort of a three-man show Plan C but

00:24:25,409 --> 00:24:29,909
we've taken on lots of new developers in

00:24:28,110 --> 00:24:31,080
the past few years we've got some really

00:24:29,909 --> 00:24:35,070
nice stuff

00:24:31,080 --> 00:24:36,570
Jim processes due largely to Bill who's

00:24:35,070 --> 00:24:38,970
sitting here in the audience

00:24:36,570 --> 00:24:40,950
elliptical slice sampling another MCMC

00:24:38,970 --> 00:24:43,740
algorithm sequential Monte Carlo time

00:24:40,950 --> 00:24:45,720
series models we can do some really neat

00:24:43,740 --> 00:24:47,700
things for example we can interact with

00:24:45,720 --> 00:24:50,549
packages like care asks for doing

00:24:47,700 --> 00:24:52,620
machine learning care s allows us

00:24:50,549 --> 00:24:55,559
actually uses a Sano back end as well

00:24:52,620 --> 00:24:58,200
optionally and in this case we're

00:24:55,559 --> 00:25:01,350
implementing a convolutional variational

00:24:58,200 --> 00:25:04,470
auto encoder as part of a prime c model

00:25:01,350 --> 00:25:07,080
so we can define this class here and pop

00:25:04,470 --> 00:25:09,919
it right in to our model and what we're

00:25:07,080 --> 00:25:12,929
doing in Prime's C then is adding a

00:25:09,919 --> 00:25:14,220
basically priors on the unknowns alright

00:25:12,929 --> 00:25:17,130
so we have these latent variables

00:25:14,220 --> 00:25:19,649
disease and we have the observations X

00:25:17,130 --> 00:25:23,309
we put that's the likelihood and then we

00:25:19,649 --> 00:25:24,750
drop this convolutional neural net

00:25:23,309 --> 00:25:28,769
essentially in the middle as the decoder

00:25:24,750 --> 00:25:31,769
and it runs Thomas one of our other key

00:25:28,769 --> 00:25:33,840
developers implemented Bayesian deep

00:25:31,769 --> 00:25:36,029
learning in pi MC 3 and again all this

00:25:33,840 --> 00:25:37,850
involves is if you're familiar with deep

00:25:36,029 --> 00:25:40,860
learning and neural networks as we put

00:25:37,850 --> 00:25:43,500
priors on the weights essentially of the

00:25:40,860 --> 00:25:47,600
neural network and and we can fit that

00:25:43,500 --> 00:25:50,100
using MCMC or variational inference and

00:25:47,600 --> 00:25:52,559
what we have here that actually is

00:25:50,100 --> 00:25:55,409
picture not of classification

00:25:52,559 --> 00:25:57,330
probabilities but the standard deviation

00:25:55,409 --> 00:25:59,340
of them so we can get uncertainty

00:25:57,330 --> 00:26:03,059
associated with our predictions from a

00:25:59,340 --> 00:26:05,100
neural network which is great the future

00:26:03,059 --> 00:26:07,320
is bright we're getting lots of new

00:26:05,100 --> 00:26:08,730
stuff pull requests for really

00:26:07,320 --> 00:26:10,260
interesting new stuff operator

00:26:08,730 --> 00:26:13,049
variational inference it's one of these

00:26:10,260 --> 00:26:14,309
newer approaches for doing variational

00:26:13,049 --> 00:26:17,010
inference that has sort of better

00:26:14,309 --> 00:26:19,049
approximation performance

00:26:17,010 --> 00:26:22,019
similarly things like normalizing flows

00:26:19,049 --> 00:26:24,750
romani and manifold HMC it's a souped up

00:26:22,019 --> 00:26:26,580
version of HMC that uses not only

00:26:24,750 --> 00:26:28,549
gradient information but second order

00:26:26,580 --> 00:26:31,200
derivatives to get even better proposals

00:26:28,549 --> 00:26:34,019
stein variational gradient descent OTE

00:26:31,200 --> 00:26:36,419
solvers and this year happy to announce

00:26:34,019 --> 00:26:38,750
with that we have three google Summer of

00:26:36,419 --> 00:26:40,679
Code students that are going to be

00:26:38,750 --> 00:26:41,399
contributing as well so we're excited

00:26:40,679 --> 00:26:43,049
about that

00:26:41,399 --> 00:26:44,520
if you're interested in exploring some

00:26:43,049 --> 00:26:45,059
of this stuff one of the best places to

00:26:44,520 --> 00:26:48,720
go

00:26:45,059 --> 00:26:51,120
actually in on github we have a folder

00:26:48,720 --> 00:26:53,340
of notebooks and it's we've had amazing

00:26:51,120 --> 00:26:56,970
contributions from both our own

00:26:53,340 --> 00:26:59,039
developers and others who aren't part of

00:26:56,970 --> 00:27:01,740
the development team and they've

00:26:59,039 --> 00:27:04,679
contributed a whole slew of amazing

00:27:01,740 --> 00:27:07,740
models well documented well explained in

00:27:04,679 --> 00:27:09,029
the form of Jupiter notebooks if you

00:27:07,740 --> 00:27:12,299
want to know more about probabilistic

00:27:09,029 --> 00:27:14,970
programming in the context of prime C

00:27:12,299 --> 00:27:17,029
and Python count Davidson peel on put

00:27:14,970 --> 00:27:20,279
together a really good book years ago

00:27:17,029 --> 00:27:21,749
that uses a computational approach to

00:27:20,279 --> 00:27:23,490
give folks an intuition about

00:27:21,749 --> 00:27:25,320
probabilistic programming it's based on

00:27:23,490 --> 00:27:28,649
PI and C and it was just recently

00:27:25,320 --> 00:27:33,330
updated by the aforementioned Thomas

00:27:28,649 --> 00:27:36,049
Viki and Mac max margin Oh from cannot

00:27:33,330 --> 00:27:38,970
continuum quanto pian their employer and

00:27:36,049 --> 00:27:42,090
they ported all this to plan C 3 so this

00:27:38,970 --> 00:27:43,950
has all been updated for Plan C 3 so

00:27:42,090 --> 00:27:47,429
finally I'd just like to acknowledge the

00:27:43,950 --> 00:27:49,980
rest of the PI mc3 team most of these

00:27:47,429 --> 00:27:52,889
folks have only jumped on board in the

00:27:49,980 --> 00:27:54,539
past year to 18 months so it's been

00:27:52,889 --> 00:27:56,639
really fantastic it's really hard to

00:27:54,539 --> 00:27:59,580
keep up with the progress of the package

00:27:56,639 --> 00:28:00,869
and I encourage you to not only try out

00:27:59,580 --> 00:28:02,999
some of these things but if you're

00:28:00,869 --> 00:28:05,340
interested in capable please do

00:28:02,999 --> 00:28:08,159
contribute we would love to diversify

00:28:05,340 --> 00:28:10,320
even further and and really hit it out

00:28:08,159 --> 00:28:13,799
of the park doing probabilistic

00:28:10,320 --> 00:28:16,440
programming in Python and I think I'm

00:28:13,799 --> 00:28:19,529
out of time so you can't ask me any hard

00:28:16,440 --> 00:28:22,129
questions but feel free to approach me

00:28:19,529 --> 00:28:22,129
after the talks

00:28:26,570 --> 00:28:33,310
we have time for one shouldn't deleted

00:28:30,890 --> 00:28:33,310
those slides

00:28:40,440 --> 00:28:47,220
just a general question and doing

00:28:43,260 --> 00:28:48,930
Bayesian analysis how do you how would

00:28:47,220 --> 00:28:50,160
you suggest going about selecting your

00:28:48,930 --> 00:28:52,200
prior I know that in a lot of

00:28:50,160 --> 00:28:54,210
applications there's a really a lot of

00:28:52,200 --> 00:28:56,640
different ways you can reasonably set

00:28:54,210 --> 00:28:59,520
priors is that something that you can

00:28:56,640 --> 00:29:01,560
that you think about tuning or what like

00:28:59,520 --> 00:29:03,810
what is like the optimal how do you go

00:29:01,560 --> 00:29:05,520
about defending your choice of what the

00:29:03,810 --> 00:29:06,990
priors because you're you're in my

00:29:05,520 --> 00:29:08,310
experience like your end results are

00:29:06,990 --> 00:29:10,410
somewhat sensitive to how you set your

00:29:08,310 --> 00:29:12,900
priors could be that's a good question

00:29:10,410 --> 00:29:14,550
and that my tutorial spent a lot of time

00:29:12,900 --> 00:29:16,530
that did a tutorial earlier in the week

00:29:14,550 --> 00:29:19,380
on this and we spend a great deal of

00:29:16,530 --> 00:29:21,270
time going over that like every other

00:29:19,380 --> 00:29:23,490
part of your model it priors are our

00:29:21,270 --> 00:29:27,000
choice that you make just like what data

00:29:23,490 --> 00:29:28,440
to collect which likelihoods to use and

00:29:27,000 --> 00:29:30,330
but you have to be explicit about it you

00:29:28,440 --> 00:29:32,370
can't hide your priors and the best

00:29:30,330 --> 00:29:35,400
thing you want to do is do a sensitivity

00:29:32,370 --> 00:29:37,560
analysis so it's not tuning so much it's

00:29:35,400 --> 00:29:40,710
just making sure or quantifying to what

00:29:37,560 --> 00:29:44,250
degree your inferences are sensitive to

00:29:40,710 --> 00:29:46,080
your priors and if so you can you know

00:29:44,250 --> 00:29:48,210
either change the functional form you

00:29:46,080 --> 00:29:50,910
know use a gamma instead of a normal or

00:29:48,210 --> 00:29:52,650
log normal stuff in normal or you know

00:29:50,910 --> 00:29:54,840
change the amount of variance you give

00:29:52,650 --> 00:29:56,850
it the amount of information and if it's

00:29:54,840 --> 00:29:58,620
relatively insensitive you're good to go

00:29:56,850 --> 00:30:01,140
if they're sensitive then you you know

00:29:58,620 --> 00:30:02,490
you've got some explaining to do but so

00:30:01,140 --> 00:30:05,600
just like any other part of the model

00:30:02,490 --> 00:30:05,600
you've got to kind of validate it

00:30:17,610 --> 00:30:23,530
hi I hear you don't like crappy dsls

00:30:20,680 --> 00:30:28,059
what do you think of Stan oh no Stan is

00:30:23,530 --> 00:30:29,950
a turing-complete language now Stan is

00:30:28,059 --> 00:30:34,240
great and we interact well with the Stan

00:30:29,950 --> 00:30:35,860
team and Stan for those that don't know

00:30:34,240 --> 00:30:37,600
it's kind of the successor to win bugs

00:30:35,860 --> 00:30:41,050
so it's from Andrew gell-mann's group at

00:30:37,600 --> 00:30:43,720
Columbia and it's an expansion of the

00:30:41,050 --> 00:30:46,360
bugs language and again uses these sort

00:30:43,720 --> 00:30:48,429
of newer MCMC samplers and variational

00:30:46,360 --> 00:30:50,380
inference and it's excellent it's

00:30:48,429 --> 00:30:52,420
actually a C++ library now which is

00:30:50,380 --> 00:30:53,980
better too so anything you can build in

00:30:52,420 --> 00:30:57,870
PMC you can also build and stand and

00:30:53,980 --> 00:30:57,870
vice versa thank you very much

00:30:58,960 --> 00:31:04,349

YouTube URL: https://www.youtube.com/watch?v=5TyvJ6jXHYE


