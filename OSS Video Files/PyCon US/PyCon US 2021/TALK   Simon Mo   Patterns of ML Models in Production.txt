Title: TALK   Simon Mo   Patterns of ML Models in Production
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	You trained a ML model, now what? The model needs to be deployed for online serving and offline processing. This talk walks through the journey of deploying your ML models in production. I will cover common deployment patterns backed by concrete use cases which are drawn from 100+ user interviews for Ray and Ray Serve. Lastly, I will cover how we built Ray Serve, a scalable model serving framework, from these learnings.

Slides: https://drive.google.com/file/d/1iLY5Tw7Sq3Hik4Fy0LXRnclyfXBbsJUj/view?usp=sharing
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:13,920 --> 00:00:17,119
hi pycon 2021

00:00:15,440 --> 00:00:18,560
welcome to my talk patterns of machine

00:00:17,119 --> 00:00:20,640
learning modeling production

00:00:18,560 --> 00:00:21,840
my name is simon mo and let's get into

00:00:20,640 --> 00:00:23,840
it

00:00:21,840 --> 00:00:25,199
so what does this talk about by talking

00:00:23,840 --> 00:00:25,760
about machine learning models in

00:00:25,199 --> 00:00:27,920
production

00:00:25,760 --> 00:00:30,800
i mean the steps that typically happen

00:00:27,920 --> 00:00:32,480
after you train the model well a typical

00:00:30,800 --> 00:00:34,559
answer is to put it in production

00:00:32,480 --> 00:00:36,320
but what does that even mean as well as

00:00:34,559 --> 00:00:37,040
soon see there isn't a very simple

00:00:36,320 --> 00:00:39,280
answer

00:00:37,040 --> 00:00:41,200
one does not simply put ml models in

00:00:39,280 --> 00:00:43,120
production

00:00:41,200 --> 00:00:45,120
there are so many tools for training the

00:00:43,120 --> 00:00:47,039
models and serving just one model

00:00:45,120 --> 00:00:49,760
you may write a recognized few of these

00:00:47,039 --> 00:00:51,920
two on the collection logo on the left

00:00:49,760 --> 00:00:53,360
these tools help you run and deploy one

00:00:51,920 --> 00:00:56,079
model very well

00:00:53,360 --> 00:00:57,680
but when you actually want to do that in

00:00:56,079 --> 00:01:00,320
real life and put it in production

00:00:57,680 --> 00:01:01,840
is not that simple sometimes scaling

00:01:00,320 --> 00:01:04,239
beyond one copy of the model

00:01:01,840 --> 00:01:06,479
just isn't supported sometimes you as a

00:01:04,239 --> 00:01:08,080
data scientist or ml engineers have to

00:01:06,479 --> 00:01:09,920
work through complex yaml

00:01:08,080 --> 00:01:12,720
configuration file and learn custom

00:01:09,920 --> 00:01:16,159
toolings and become kubernetes experts

00:01:12,720 --> 00:01:18,960
and sometimes you hit the scalability or

00:01:16,159 --> 00:01:20,240
performance issue very early on even if

00:01:18,960 --> 00:01:22,479
all these worked well

00:01:20,240 --> 00:01:24,840
many of the tools are very costly and

00:01:22,479 --> 00:01:26,880
often lead to under utilizing the

00:01:24,840 --> 00:01:28,880
resource we need to recognize the

00:01:26,880 --> 00:01:30,479
reality that there is always going to be

00:01:28,880 --> 00:01:31,840
new machine learning model trained and

00:01:30,479 --> 00:01:33,439
deployed over time

00:01:31,840 --> 00:01:35,920
and there is always going to be a need

00:01:33,439 --> 00:01:37,840
for production use cases to scale beyond

00:01:35,920 --> 00:01:39,680
one copy of the single model

00:01:37,840 --> 00:01:40,960
and additionally for many complex

00:01:39,680 --> 00:01:43,119
real-world use cases

00:01:40,960 --> 00:01:44,000
you need to compose multiple models

00:01:43,119 --> 00:01:46,240
together

00:01:44,000 --> 00:01:47,600
so that's what this talk is about how

00:01:46,240 --> 00:01:50,320
does mod

00:01:47,600 --> 00:01:50,799
many many models run in production and

00:01:50,320 --> 00:01:53,439
what are

00:01:50,799 --> 00:01:54,880
some of the patterns used today for that

00:01:53,439 --> 00:01:56,799
well who am i

00:01:54,880 --> 00:01:58,320
currently i'm building a python library

00:01:56,799 --> 00:02:00,079
called reserve at any scale

00:01:58,320 --> 00:02:02,960
and previously i work on production

00:02:00,079 --> 00:02:05,119
serving system at uc berkeley rice lab

00:02:02,960 --> 00:02:06,799
in both places i'm constantly talking to

00:02:05,119 --> 00:02:08,479
machine learning practitioners to learn

00:02:06,799 --> 00:02:10,000
and improve the tooling around mlm

00:02:08,479 --> 00:02:11,760
production

00:02:10,000 --> 00:02:13,680
if you are tuning in you're probably a

00:02:11,760 --> 00:02:16,080
ml engineer or data scientist

00:02:13,680 --> 00:02:17,440
or even just enthusiast you are excited

00:02:16,080 --> 00:02:20,160
about deploying

00:02:17,440 --> 00:02:21,840
deployment and ml in production and

00:02:20,160 --> 00:02:23,520
hopefully you can take away some useful

00:02:21,840 --> 00:02:25,280
tips and tricks from the talk today

00:02:23,520 --> 00:02:27,520
this topic will be broken down into two

00:02:25,280 --> 00:02:29,840
parts first we're going to cover

00:02:27,520 --> 00:02:31,920
some patterns that summarize from 100

00:02:29,840 --> 00:02:34,319
more user interviews conducted by me

00:02:31,920 --> 00:02:36,239
and the teams at any scale and then i'll

00:02:34,319 --> 00:02:37,440
talk about reserve a framework we built

00:02:36,239 --> 00:02:40,080
from the lesson nerd

00:02:37,440 --> 00:02:41,519
it has a simple to use pysonic api and

00:02:40,080 --> 00:02:44,640
it is especially built

00:02:41,519 --> 00:02:45,920
for ml models in production we'll start

00:02:44,640 --> 00:02:48,080
from the very high level

00:02:45,920 --> 00:02:49,680
architecturally where do you put ml

00:02:48,080 --> 00:02:52,480
models in production

00:02:49,680 --> 00:02:54,000
there are three categories batch stream

00:02:52,480 --> 00:02:56,560
and serve

00:02:54,000 --> 00:02:57,440
these are in their order of latency

00:02:56,560 --> 00:02:58,959
guarantee

00:02:57,440 --> 00:03:00,640
batch workload on the one end of the

00:02:58,959 --> 00:03:02,400
spectrum has high latency

00:03:00,640 --> 00:03:04,879
but also perform influence on high

00:03:02,400 --> 00:03:07,599
throughput on the other end

00:03:04,879 --> 00:03:09,280
is serving where the use case for low

00:03:07,599 --> 00:03:11,599
latency is optimized

00:03:09,280 --> 00:03:12,640
let's quickly go through them one by one

00:03:11,599 --> 00:03:14,959
in the batch mode

00:03:12,640 --> 00:03:17,360
the model is ran in regular cadence by

00:03:14,959 --> 00:03:19,680
either a scheduler or workflow manager

00:03:17,360 --> 00:03:20,400
it's kicked off to score new batches of

00:03:19,680 --> 00:03:22,400
data

00:03:20,400 --> 00:03:24,799
and use neural models to score a batch

00:03:22,400 --> 00:03:26,400
of data the operation is essentially

00:03:24,799 --> 00:03:28,480
replicating the training and scoring

00:03:26,400 --> 00:03:30,400
phase of the training pipeline

00:03:28,480 --> 00:03:32,000
in the streaming mode incoming events

00:03:30,400 --> 00:03:33,920
are coming in constantly

00:03:32,000 --> 00:03:36,480
into the stream processing engine like

00:03:33,920 --> 00:03:38,400
kafka flink or spark streaming

00:03:36,480 --> 00:03:39,920
they'll compute features real time and

00:03:38,400 --> 00:03:43,599
run small batches

00:03:39,920 --> 00:03:46,080
through the ml model and then lastly

00:03:43,599 --> 00:03:48,159
there is the serving architecture where

00:03:46,080 --> 00:03:49,599
the models are served in real time for

00:03:48,159 --> 00:03:51,760
interactive use cases

00:03:49,599 --> 00:03:53,920
we'll focus on serving because it is not

00:03:51,760 --> 00:03:55,680
only the super set of batch and stream

00:03:53,920 --> 00:03:57,120
after all if you can serve in low

00:03:55,680 --> 00:03:58,879
latency high throughput

00:03:57,120 --> 00:04:00,959
you are well positioned to run it in

00:03:58,879 --> 00:04:03,439
stream and batch scenario

00:04:00,959 --> 00:04:04,480
but also serving covers quite a majority

00:04:03,439 --> 00:04:07,599
of use cases

00:04:04,480 --> 00:04:09,280
that we have observed so that's a

00:04:07,599 --> 00:04:11,120
high-level architectural pattern

00:04:09,280 --> 00:04:12,720
now let's take a look at four concrete

00:04:11,120 --> 00:04:14,879
patterns we'll cover today

00:04:12,720 --> 00:04:16,720
there are pipeline ensemble business

00:04:14,879 --> 00:04:19,120
logic and online learning

00:04:16,720 --> 00:04:20,720
these are core pieces of common machine

00:04:19,120 --> 00:04:22,479
learning app in production today

00:04:20,720 --> 00:04:24,479
and we'll take a close look at each one

00:04:22,479 --> 00:04:27,360
of them showing their pros and cons

00:04:24,479 --> 00:04:28,720
and use cases so let's start with

00:04:27,360 --> 00:04:30,080
pipeline

00:04:28,720 --> 00:04:32,160
let's take a look at the typical

00:04:30,080 --> 00:04:34,160
pipeline showing here is a computer

00:04:32,160 --> 00:04:36,240
vision pipeline where we will use

00:04:34,160 --> 00:04:38,160
multiple diplomatic model to caption the

00:04:36,240 --> 00:04:40,400
object in the texture

00:04:38,160 --> 00:04:42,080
starting with a raw image you'll need to

00:04:40,400 --> 00:04:43,040
go through some pre-processing like

00:04:42,080 --> 00:04:45,600
image decoding

00:04:43,040 --> 00:04:48,000
augmentation and clipping and then is

00:04:45,600 --> 00:04:49,199
passed into deep learning detector or

00:04:48,000 --> 00:04:50,960
classifier

00:04:49,199 --> 00:04:52,479
in here the model identifies the

00:04:50,960 --> 00:04:55,120
bounding box and the category

00:04:52,479 --> 00:04:55,759
well it's got once we detect the sun

00:04:55,120 --> 00:04:57,840
object

00:04:55,759 --> 00:05:00,160
the image is passed into a keypoint

00:04:57,840 --> 00:05:01,199
detection model to identify the posture

00:05:00,160 --> 00:05:03,280
of the object

00:05:01,199 --> 00:05:05,440
so the model identifies key points like

00:05:03,280 --> 00:05:07,440
head neck and the head

00:05:05,440 --> 00:05:10,160
lastly we run through visualization with

00:05:07,440 --> 00:05:12,800
all these inputs and generate a category

00:05:10,160 --> 00:05:14,720
of what this picture show well in this

00:05:12,800 --> 00:05:16,400
case a standing cat

00:05:14,720 --> 00:05:18,880
while this example might seem a bit

00:05:16,400 --> 00:05:19,840
contrived a typical pipeline rarely

00:05:18,880 --> 00:05:22,320
consists of just

00:05:19,840 --> 00:05:24,000
one single model to tackle real life

00:05:22,320 --> 00:05:26,000
issue a machine learning application

00:05:24,000 --> 00:05:27,600
used many many different models to

00:05:26,000 --> 00:05:30,639
perform a very

00:05:27,600 --> 00:05:33,199
even a very simple task

00:05:30,639 --> 00:05:33,759
pipeline in general breaks a specific

00:05:33,199 --> 00:05:36,639
task

00:05:33,759 --> 00:05:38,160
into many steps each step is conquered

00:05:36,639 --> 00:05:40,880
by a machine learning algorithm

00:05:38,160 --> 00:05:41,199
or some procedure you might be familiar

00:05:40,880 --> 00:05:43,600
with

00:05:41,199 --> 00:05:45,440
cycling pipeline construct where you can

00:05:43,600 --> 00:05:47,280
combine multiple models or processing

00:05:45,440 --> 00:05:48,880
objects together and call the fit

00:05:47,280 --> 00:05:50,800
function as a whole

00:05:48,880 --> 00:05:52,160
there is also common pipeline setup in

00:05:50,800 --> 00:05:54,240
recommendation system

00:05:52,160 --> 00:05:55,759
where a typical recommendation like item

00:05:54,240 --> 00:05:58,960
recommendation in amazon

00:05:55,759 --> 00:06:00,319
or a movie recognition on netflix goes

00:05:58,960 --> 00:06:02,479
through multiple stages like

00:06:00,319 --> 00:06:05,600
embedding lookup feature interaction

00:06:02,479 --> 00:06:08,560
nearest neighbor model and ranking model

00:06:05,600 --> 00:06:10,400
additionally there are very common use

00:06:08,560 --> 00:06:12,240
cases where some mega machine learning

00:06:10,400 --> 00:06:14,240
model takes care of the common

00:06:12,240 --> 00:06:16,639
processing for for example text or

00:06:14,240 --> 00:06:17,199
images and this mega model can be the

00:06:16,639 --> 00:06:19,600
studio

00:06:17,199 --> 00:06:22,000
featurizer trend using millions and

00:06:19,600 --> 00:06:24,560
millions of dollars or compute time

00:06:22,000 --> 00:06:25,440
for example gpd3 or even giant image

00:06:24,560 --> 00:06:28,080
models

00:06:25,440 --> 00:06:29,919
and then different teams or pipeline

00:06:28,080 --> 00:06:30,880
will utilize the same model to

00:06:29,919 --> 00:06:32,800
downstream tasks

00:06:30,880 --> 00:06:34,639
with lighter and cheaper models like

00:06:32,800 --> 00:06:36,000
decision tree or some sort of boosting

00:06:34,639 --> 00:06:38,639
model for specific

00:06:36,000 --> 00:06:40,240
business specific tasks thinking about

00:06:38,639 --> 00:06:40,800
the pros and cons of the pipeline

00:06:40,240 --> 00:06:43,600
pattern

00:06:40,800 --> 00:06:45,440
that is very common and widely used it

00:06:43,600 --> 00:06:48,000
encourages separation of concerns and

00:06:45,440 --> 00:06:48,960
each model can be updated separately in

00:06:48,000 --> 00:06:51,520
this pattern

00:06:48,960 --> 00:06:52,160
model each models tackle one part of the

00:06:51,520 --> 00:06:54,720
task

00:06:52,160 --> 00:06:55,680
and there are modular components however

00:06:54,720 --> 00:06:57,759
as a downside

00:06:55,680 --> 00:06:59,759
the dependency of the downstream models

00:06:57,759 --> 00:07:01,039
on the upstream models make it tricky to

00:06:59,759 --> 00:07:04,560
reasoning about

00:07:01,039 --> 00:07:05,759
what and when to retrain if the upstream

00:07:04,560 --> 00:07:08,000
model was

00:07:05,759 --> 00:07:09,440
retrained on the new data then the

00:07:08,000 --> 00:07:12,720
output feature space

00:07:09,440 --> 00:07:14,800
of the option model would change the old

00:07:12,720 --> 00:07:16,560
downstream models will no longer be

00:07:14,800 --> 00:07:18,000
accurate in this case you have to

00:07:16,560 --> 00:07:20,560
restrain the downstream model

00:07:18,000 --> 00:07:22,160
as well however the reverse is not the

00:07:20,560 --> 00:07:24,160
case you can continuously improve the

00:07:22,160 --> 00:07:26,639
downstream model like decision tree

00:07:24,160 --> 00:07:28,400
without retraining the option mega

00:07:26,639 --> 00:07:30,240
featurizer

00:07:28,400 --> 00:07:32,560
additionally they are tricky to paralyze

00:07:30,240 --> 00:07:34,880
and scale what do i mean by that

00:07:32,560 --> 00:07:36,560
one constant thing to of today's talk

00:07:34,880 --> 00:07:37,919
will be thinking about how can this be

00:07:36,560 --> 00:07:40,160
implemented

00:07:37,919 --> 00:07:41,680
in general there are two approaches you

00:07:40,160 --> 00:07:43,440
can either wrap your model and

00:07:41,680 --> 00:07:44,080
orchestration in the web server in a web

00:07:43,440 --> 00:07:46,879
server

00:07:44,080 --> 00:07:48,479
for example in the left code block here

00:07:46,879 --> 00:07:50,720
models are loaded and ran

00:07:48,479 --> 00:07:51,599
in in a for loop during the web handling

00:07:50,720 --> 00:07:53,520
pass

00:07:51,599 --> 00:07:55,919
whenever requests come in we load the

00:07:53,520 --> 00:07:57,919
model well they can be cached of course

00:07:55,919 --> 00:07:59,919
and run through the pipeline this is

00:07:57,919 --> 00:08:02,960
simple and easy to implement

00:07:59,919 --> 00:08:04,240
however a major flaw is that this is

00:08:02,960 --> 00:08:06,000
hard to scale

00:08:04,240 --> 00:08:08,319
and is not going to be performant

00:08:06,000 --> 00:08:10,000
because each request will handle

00:08:08,319 --> 00:08:11,919
sequentially

00:08:10,000 --> 00:08:13,680
on the other hand you can use

00:08:11,919 --> 00:08:15,919
specialized microservices

00:08:13,680 --> 00:08:18,560
especially essentially build and deploy

00:08:15,919 --> 00:08:20,639
one microservice per model replica

00:08:18,560 --> 00:08:22,000
this microservice can be native

00:08:20,639 --> 00:08:25,039
machinery platform

00:08:22,000 --> 00:08:27,680
qflow or even hosted services like aws

00:08:25,039 --> 00:08:28,080
maker however as the number of models

00:08:27,680 --> 00:08:30,639
grow

00:08:28,080 --> 00:08:33,680
the complexity will drastically increase

00:08:30,639 --> 00:08:36,159
the operation cost will also skyrocket

00:08:33,680 --> 00:08:37,440
before race surf which is the framework

00:08:36,159 --> 00:08:39,519
we will talk about later

00:08:37,440 --> 00:08:42,719
you have to choose either the left or

00:08:39,519 --> 00:08:42,719
the right solution

00:08:42,959 --> 00:08:47,279
now let's switch gears a little bit in

00:08:44,800 --> 00:08:47,760
many cases pipeline is a good pattern to

00:08:47,279 --> 00:08:49,760
have

00:08:47,760 --> 00:08:52,000
however one limitation is that often

00:08:49,760 --> 00:08:54,160
times there can be many upstream model

00:08:52,000 --> 00:08:56,959
for a given downstream model this is

00:08:54,160 --> 00:08:59,600
where ensembl pattern comes in

00:08:56,959 --> 00:09:00,240
one is for model updates as we mentioned

00:08:59,600 --> 00:09:02,240
before

00:09:00,240 --> 00:09:03,680
new models are developed and trend over

00:09:02,240 --> 00:09:05,279
time this means

00:09:03,680 --> 00:09:06,800
there will always be new version of the

00:09:05,279 --> 00:09:09,120
model in production

00:09:06,800 --> 00:09:10,800
how do we make sure that the new models

00:09:09,120 --> 00:09:14,800
are valid and performant

00:09:10,800 --> 00:09:16,320
in the live online traffic scenario

00:09:14,800 --> 00:09:20,160
we can put some portion of the traffic

00:09:16,320 --> 00:09:22,720
through it this is one possible use case

00:09:20,160 --> 00:09:24,720
of model and sampling where you select

00:09:22,720 --> 00:09:26,560
the output from known good model

00:09:24,720 --> 00:09:27,920
whilst it runs through and collect live

00:09:26,560 --> 00:09:30,320
output from

00:09:27,920 --> 00:09:32,240
newer version of the model and then the

00:09:30,320 --> 00:09:33,040
second use case i'll talk about here is

00:09:32,240 --> 00:09:34,800
aggregation

00:09:33,040 --> 00:09:36,720
this is one of the most common

00:09:34,800 --> 00:09:38,959
definition on sampling

00:09:36,720 --> 00:09:41,279
if it's a regression model the output

00:09:38,959 --> 00:09:43,360
from the model are typically averaged

00:09:41,279 --> 00:09:44,959
and for classification model the output

00:09:43,360 --> 00:09:47,120
are typically a voted version of the

00:09:44,959 --> 00:09:50,480
output from the upstream models

00:09:47,120 --> 00:09:52,720
for example two models says uh the

00:09:50,480 --> 00:09:54,000
image container cat and one model says

00:09:52,720 --> 00:09:56,160
the image contains a dog

00:09:54,000 --> 00:09:57,760
this is an output would just be a cat

00:09:56,160 --> 00:10:00,880
aggregation help to combat

00:09:57,760 --> 00:10:02,800
inaccuracy in each individual model and

00:10:00,880 --> 00:10:05,680
generally make the output

00:10:02,800 --> 00:10:07,360
more accurate and safer one more use

00:10:05,680 --> 00:10:09,519
case for example is to

00:10:07,360 --> 00:10:10,399
dynamically perform model selection

00:10:09,519 --> 00:10:13,040
given

00:10:10,399 --> 00:10:14,800
input attributes for example if the

00:10:13,040 --> 00:10:18,000
input contains a cat

00:10:14,800 --> 00:10:20,880
then it will use a model a maybe

00:10:18,000 --> 00:10:22,079
trans specialized for cats if the model

00:10:20,880 --> 00:10:24,399
contains a dog

00:10:22,079 --> 00:10:25,360
then we'll use model b specialized for

00:10:24,399 --> 00:10:27,519
dog

00:10:25,360 --> 00:10:29,600
note note that this dynamic selection

00:10:27,519 --> 00:10:31,120
doesn't necessarily mean the pipeline

00:10:29,600 --> 00:10:33,279
itself has to be static

00:10:31,120 --> 00:10:35,360
and choose given attribute you could

00:10:33,279 --> 00:10:37,839
also be selecting models giving user

00:10:35,360 --> 00:10:39,600
feedback

00:10:37,839 --> 00:10:41,360
implementation wise we still have the

00:10:39,600 --> 00:10:44,000
same sort of issue

00:10:41,360 --> 00:10:46,079
on one on the one hand you can wrap the

00:10:44,000 --> 00:10:46,880
models in the same web handler and run

00:10:46,079 --> 00:10:48,480
the models

00:10:46,880 --> 00:10:50,720
through the critical passive web

00:10:48,480 --> 00:10:52,320
handling on the other hand

00:10:50,720 --> 00:10:55,040
you end up having a lot of micro

00:10:52,320 --> 00:10:57,760
services to manage the number of micro

00:10:55,040 --> 00:11:00,000
services scale by the number of models

00:10:57,760 --> 00:11:02,160
and we have the same issue as pipeline

00:11:00,000 --> 00:11:04,480
choosing web server means simple but not

00:11:02,160 --> 00:11:05,839
performant choosing specialized services

00:11:04,480 --> 00:11:08,399
means complexity and

00:11:05,839 --> 00:11:09,279
operational overhead so that's pipeline

00:11:08,399 --> 00:11:11,440
ensemble

00:11:09,279 --> 00:11:12,800
let's take a look at two other common

00:11:11,440 --> 00:11:16,640
patterns we observed

00:11:12,800 --> 00:11:18,959
one of them is business logic

00:11:16,640 --> 00:11:20,800
by business logic we mean everything

00:11:18,959 --> 00:11:21,600
that's involved in a common and machine

00:11:20,800 --> 00:11:23,360
learning app

00:11:21,600 --> 00:11:25,120
that are not the machine learning model

00:11:23,360 --> 00:11:27,120
inference itself

00:11:25,120 --> 00:11:29,920
there are common operations like

00:11:27,120 --> 00:11:31,920
database lookup for traditional record

00:11:29,920 --> 00:11:33,200
feature store lookout for pre-computed

00:11:31,920 --> 00:11:35,760
feature vectors

00:11:33,200 --> 00:11:37,839
web api call for external services or

00:11:35,760 --> 00:11:39,519
even just inline feature transformation

00:11:37,839 --> 00:11:42,399
like data validation

00:11:39,519 --> 00:11:44,160
encoding and decoding productionizing

00:11:42,399 --> 00:11:45,519
machine learning will always involve

00:11:44,160 --> 00:11:47,920
business logic

00:11:45,519 --> 00:11:49,279
no models can be a standalone and server

00:11:47,920 --> 00:11:51,519
requests by themselves

00:11:49,279 --> 00:11:54,079
as the meme says this logic is

00:11:51,519 --> 00:11:54,079
everywhere

00:11:54,560 --> 00:12:00,639
however there is one critical issue

00:11:57,600 --> 00:12:01,920
around the business logic ml in ml-based

00:12:00,639 --> 00:12:03,680
application

00:12:01,920 --> 00:12:05,440
well let's take a look at the pseudo

00:12:03,680 --> 00:12:06,160
code on the left here for the web

00:12:05,440 --> 00:12:09,040
handler

00:12:06,160 --> 00:12:10,639
we first load the model say from s3 and

00:12:09,040 --> 00:12:12,959
then value the input

00:12:10,639 --> 00:12:14,240
using a database and then look up some

00:12:12,959 --> 00:12:17,519
precomputed feature

00:12:14,240 --> 00:12:18,480
from an external feature store once we

00:12:17,519 --> 00:12:20,399
have

00:12:18,480 --> 00:12:22,160
completed all this business logic the

00:12:20,399 --> 00:12:23,440
input will be passed through the ml

00:12:22,160 --> 00:12:26,639
model

00:12:23,440 --> 00:12:26,639
can you follow the issue here

00:12:27,040 --> 00:12:30,720
while the model while the model loading

00:12:29,680 --> 00:12:32,880
step

00:12:30,720 --> 00:12:34,000
database lookout step and feature

00:12:32,880 --> 00:12:37,279
storage custom

00:12:34,000 --> 00:12:40,399
are network bounded i o heavy

00:12:37,279 --> 00:12:42,399
the model inference step is compute

00:12:40,399 --> 00:12:45,200
bounded and memory hungry

00:12:42,399 --> 00:12:46,399
this requirement for model inference and

00:12:45,200 --> 00:12:49,040
business logic

00:12:46,399 --> 00:12:49,920
lead to the server being both network

00:12:49,040 --> 00:12:52,560
bounded

00:12:49,920 --> 00:12:54,240
and compute bounded this is bad because

00:12:52,560 --> 00:12:55,120
we cannot efficiently utilize all the

00:12:54,240 --> 00:12:58,320
resource

00:12:55,120 --> 00:13:00,880
and scaling the the kind of app will be

00:12:58,320 --> 00:13:03,120
very expensive

00:13:00,880 --> 00:13:05,040
one common alternative is to split the

00:13:03,120 --> 00:13:07,760
model out into a model server

00:13:05,040 --> 00:13:08,399
or microservice this approach shown on

00:13:07,760 --> 00:13:10,160
the right

00:13:08,399 --> 00:13:12,160
is often a solution to increase

00:13:10,160 --> 00:13:13,279
utilization as compared to a web server

00:13:12,160 --> 00:13:16,320
deployment model

00:13:13,279 --> 00:13:18,639
on the left with this splitting

00:13:16,320 --> 00:13:19,440
the web app is now purely network

00:13:18,639 --> 00:13:21,200
bounded

00:13:19,440 --> 00:13:22,880
while the model servers are compute

00:13:21,200 --> 00:13:25,680
bonded each

00:13:22,880 --> 00:13:26,480
server can scale separately however a

00:13:25,680 --> 00:13:28,560
common problem

00:13:26,480 --> 00:13:30,720
is about the interface between the two

00:13:28,560 --> 00:13:31,600
if we put too much business logic into

00:13:30,720 --> 00:13:33,519
the model server

00:13:31,600 --> 00:13:35,839
since the model server again will become

00:13:33,519 --> 00:13:37,120
a mix of network bounded and compute

00:13:35,839 --> 00:13:39,040
bounded calls

00:13:37,120 --> 00:13:40,639
but if we don't do that there's a model

00:13:39,040 --> 00:13:43,360
server be pure

00:13:40,639 --> 00:13:45,199
ml model servers then you have the

00:13:43,360 --> 00:13:46,800
tensor intensity out problem

00:13:45,199 --> 00:13:48,720
this is how i define to describe this

00:13:46,800 --> 00:13:50,800
kind of interface problem

00:13:48,720 --> 00:13:53,199
the input type for modal server as in

00:13:50,800 --> 00:13:56,079
typically very constrained to just

00:13:53,199 --> 00:13:57,440
numeric tensor or some alternative form

00:13:56,079 --> 00:13:59,199
this will make it hard to keep

00:13:57,440 --> 00:14:01,279
pre-processing

00:13:59,199 --> 00:14:03,120
post-processing and business logic in

00:14:01,279 --> 00:14:04,800
sync with the model itself

00:14:03,120 --> 00:14:06,399
because they are located in two

00:14:04,800 --> 00:14:09,519
different services

00:14:06,399 --> 00:14:11,440
and because that it becomes hard to

00:14:09,519 --> 00:14:14,000
reason about the interaction between the

00:14:11,440 --> 00:14:15,199
processing logic and the model itself

00:14:14,000 --> 00:14:17,360
because during the training the

00:14:15,199 --> 00:14:17,839
processing logic and models are are very

00:14:17,360 --> 00:14:20,240
much

00:14:17,839 --> 00:14:21,920
very tightly coupled but in serving time

00:14:20,240 --> 00:14:24,639
they are now split across

00:14:21,920 --> 00:14:26,399
two servers and two implementation again

00:14:24,639 --> 00:14:28,000
we'll see the current industry either

00:14:26,399 --> 00:14:31,760
have to choose left or right

00:14:28,000 --> 00:14:33,360
but none of them are satisfactory

00:14:31,760 --> 00:14:35,680
so that's the business logic in

00:14:33,360 --> 00:14:37,920
production ml app

00:14:35,680 --> 00:14:39,279
i'll now present a final pattern here

00:14:37,920 --> 00:14:41,440
about online learning

00:14:39,279 --> 00:14:43,519
which is an emerging pattern that become

00:14:41,440 --> 00:14:45,360
more and more widely used

00:14:43,519 --> 00:14:46,880
by saying online learning i mean that

00:14:45,360 --> 00:14:49,360
the model running production

00:14:46,880 --> 00:14:50,560
is constantly being updated trained

00:14:49,360 --> 00:14:53,279
validated and even

00:14:50,560 --> 00:14:54,160
undeployed there are different paradigms

00:14:53,279 --> 00:14:56,399
here i would say

00:14:54,160 --> 00:14:58,240
uh online learning is by far the most

00:14:56,399 --> 00:15:00,079
cutting edge modes of deploying machine

00:14:58,240 --> 00:15:02,079
learning modeling production today

00:15:00,079 --> 00:15:03,760
there are use cases for dynamically

00:15:02,079 --> 00:15:06,959
learning model weights online

00:15:03,760 --> 00:15:08,880
as you users interact with your services

00:15:06,959 --> 00:15:10,800
these updated model weights can

00:15:08,880 --> 00:15:12,320
contribute to a personalized model for

00:15:10,800 --> 00:15:14,160
each user or group

00:15:12,320 --> 00:15:16,160
there are also use cases for learning

00:15:14,160 --> 00:15:16,720
parameters to orchestrate or compose a

00:15:16,160 --> 00:15:18,639
model

00:15:16,720 --> 00:15:20,000
for example learning which model does a

00:15:18,639 --> 00:15:22,320
user prefer

00:15:20,000 --> 00:15:23,279
this manifests often a model selection

00:15:22,320 --> 00:15:25,440
scenario

00:15:23,279 --> 00:15:27,120
as users interact with the application

00:15:25,440 --> 00:15:28,079
the preference to where which model will

00:15:27,120 --> 00:15:29,759
be updated

00:15:28,079 --> 00:15:32,560
and a lot of times through the

00:15:29,759 --> 00:15:34,480
contextual bandits algorithm

00:15:32,560 --> 00:15:35,839
and lastly there is paradigm from

00:15:34,480 --> 00:15:38,399
reinforcement learning

00:15:35,839 --> 00:15:40,079
rl is a branch of machine learning that

00:15:38,399 --> 00:15:41,120
trains agents to interact with the

00:15:40,079 --> 00:15:44,000
environment

00:15:41,120 --> 00:15:45,199
let's uh environment b is a physical

00:15:44,000 --> 00:15:47,839
world similarly

00:15:45,199 --> 00:15:50,160
physical world or even artificial

00:15:47,839 --> 00:15:52,639
environment for business scenario

00:15:50,160 --> 00:15:53,519
one popular example of rl is alpha gold

00:15:52,639 --> 00:15:57,120
and aging that be

00:15:53,519 --> 00:15:59,199
human in get go rl is getting more and

00:15:57,120 --> 00:16:00,399
more popular for recommendation system

00:15:59,199 --> 00:16:02,160
as well

00:16:00,399 --> 00:16:03,920
because it can train the agent to

00:16:02,160 --> 00:16:07,199
interact with live environments

00:16:03,920 --> 00:16:08,880
in an end-to-end fashion online learning

00:16:07,199 --> 00:16:10,639
is very much an emerging trend

00:16:08,880 --> 00:16:12,240
it is very powerful but we still don't

00:16:10,639 --> 00:16:12,880
have the standard practice on how to get

00:16:12,240 --> 00:16:15,360
it right

00:16:12,880 --> 00:16:16,959
here i list a few potential questions

00:16:15,360 --> 00:16:19,120
when do we know the model is

00:16:16,959 --> 00:16:21,120
performing well when the safety update

00:16:19,120 --> 00:16:23,120
model and can we ensure personalized

00:16:21,120 --> 00:16:24,639
models can never go wrong and deliver

00:16:23,120 --> 00:16:25,839
biased predictions

00:16:24,639 --> 00:16:27,680
so that's all the patterns we're

00:16:25,839 --> 00:16:29,279
covering there are pipeline and sample

00:16:27,680 --> 00:16:31,360
based logic and online learning

00:16:29,279 --> 00:16:32,880
as we mentioned before we discover the

00:16:31,360 --> 00:16:33,920
gap in the current implementation for

00:16:32,880 --> 00:16:36,160
these patterns

00:16:33,920 --> 00:16:37,279
so we build reserve a framework that

00:16:36,160 --> 00:16:39,120
encodes and

00:16:37,279 --> 00:16:40,639
enables this pattern let me tell you

00:16:39,120 --> 00:16:42,160
more about it

00:16:40,639 --> 00:16:44,320
between wrapping the model using the

00:16:42,160 --> 00:16:46,560
same web server and splitting everything

00:16:44,320 --> 00:16:48,320
up into specialized microservices

00:16:46,560 --> 00:16:50,000
reserve offers the best of the best

00:16:48,320 --> 00:16:51,839
world has the simplicity

00:16:50,000 --> 00:16:53,440
of the web server but it gives extra

00:16:51,839 --> 00:16:54,880
power to scale out and parallelizing

00:16:53,440 --> 00:16:57,279
individual components

00:16:54,880 --> 00:16:59,199
it is especially built for multiple

00:16:57,279 --> 00:17:01,199
models in production scenario

00:16:59,199 --> 00:17:02,720
how does it do that well reserve is

00:17:01,199 --> 00:17:04,720
built on top of ray

00:17:02,720 --> 00:17:06,319
rays a framework that provides a simple

00:17:04,720 --> 00:17:08,640
universal api for distribu

00:17:06,319 --> 00:17:10,480
building distributed application and

00:17:08,640 --> 00:17:13,280
reserve is simply a framework

00:17:10,480 --> 00:17:14,480
on top of ray that's specially built for

00:17:13,280 --> 00:17:16,959
deploy and scale

00:17:14,480 --> 00:17:18,400
ml services let's dive a little bit

00:17:16,959 --> 00:17:20,319
deeper into rey

00:17:18,400 --> 00:17:21,919
and this should help understand what

00:17:20,319 --> 00:17:23,199
makes reserve and astro fit for

00:17:21,919 --> 00:17:24,959
deploying models

00:17:23,199 --> 00:17:27,839
in production and implementing the

00:17:24,959 --> 00:17:29,840
patterns we laid out before

00:17:27,839 --> 00:17:31,440
ray ice core is an open source library

00:17:29,840 --> 00:17:34,640
for parallel distributed python

00:17:31,440 --> 00:17:36,960
it takes arbitrary python function or

00:17:34,640 --> 00:17:38,000
classes and translates them into

00:17:36,960 --> 00:17:40,320
distributed settings

00:17:38,000 --> 00:17:41,039
for example here we have many functions

00:17:40,320 --> 00:17:43,039
on the left

00:17:41,039 --> 00:17:44,320
and invocation of this function and

00:17:43,039 --> 00:17:47,520
right on the right

00:17:44,320 --> 00:17:48,160
we have stable classes to run the

00:17:47,520 --> 00:17:49,919
application

00:17:48,160 --> 00:17:52,559
on ray all we need to do is just

00:17:49,919 --> 00:17:55,280
decorate the function with radar remote

00:17:52,559 --> 00:17:56,160
and then it will turn the function into

00:17:55,280 --> 00:17:59,360
a task and

00:17:56,160 --> 00:18:00,400
a class into a reactor with a remote

00:17:59,360 --> 00:18:02,480
task on the left

00:18:00,400 --> 00:18:03,919
you can invoke the task and the task

00:18:02,480 --> 00:18:05,760
will be right in your remote worker

00:18:03,919 --> 00:18:08,160
without you changing anything

00:18:05,760 --> 00:18:10,080
the input argument is still the python

00:18:08,160 --> 00:18:12,160
object but the output becomes reference

00:18:10,080 --> 00:18:14,320
to the eventual upwards of the task

00:18:12,160 --> 00:18:16,640
which is something you can be uh you can

00:18:14,320 --> 00:18:18,559
retrieve later using read.get

00:18:16,640 --> 00:18:21,039
with remote actor on the right you can

00:18:18,559 --> 00:18:23,200
specify arbitrary resource like num gpus

00:18:21,039 --> 00:18:25,520
to aid rescheduler to spawn

00:18:23,200 --> 00:18:27,600
the process containing this class in to

00:18:25,520 --> 00:18:29,039
your machine with one gpu on them

00:18:27,600 --> 00:18:31,200
now you can call the actor just like

00:18:29,039 --> 00:18:32,880
normal python class in your process

00:18:31,200 --> 00:18:35,840
the only difference is that class will

00:18:32,880 --> 00:18:39,679
be hosted on a remote process

00:18:35,840 --> 00:18:41,360
and then again while ray provides uh

00:18:39,679 --> 00:18:43,200
the perimeter for distributed

00:18:41,360 --> 00:18:45,520
applications reserve

00:18:43,200 --> 00:18:48,160
is a api and framework purposely built

00:18:45,520 --> 00:18:48,160
on top of it

00:18:48,559 --> 00:18:52,640
reserve has three simple concepts that

00:18:50,880 --> 00:18:55,679
brings the best of the both road

00:18:52,640 --> 00:18:57,679
there are deployment ingress and handle

00:18:55,679 --> 00:18:59,440
let's first talk about deployment as a

00:18:57,679 --> 00:19:00,160
core of reserve is a concept called

00:18:59,440 --> 00:19:02,080
deployment

00:19:00,160 --> 00:19:03,520
it is a single unit of code that can be

00:19:02,080 --> 00:19:07,360
deployed into a remote

00:19:03,520 --> 00:19:10,000
reactor it has some http prefix so http

00:19:07,360 --> 00:19:11,840
proxy can route traffic to it to declare

00:19:10,000 --> 00:19:13,520
a deployment you just need to annotate

00:19:11,840 --> 00:19:15,039
your class with the decorator

00:19:13,520 --> 00:19:16,960
note that regular internet and other

00:19:15,039 --> 00:19:19,039
messages just works

00:19:16,960 --> 00:19:20,960
once we call up my deployment at deploy

00:19:19,039 --> 00:19:21,520
surf deploys your code to the raid

00:19:20,960 --> 00:19:24,960
cluster

00:19:21,520 --> 00:19:26,320
and the classes is now curable for http

00:19:24,960 --> 00:19:28,240
let's take a closer look at the

00:19:26,320 --> 00:19:30,160
architecture diagram on the right

00:19:28,240 --> 00:19:32,000
reserve can scale from your laptop to

00:19:30,160 --> 00:19:34,720
many many machines

00:19:32,000 --> 00:19:36,559
here we just generalism as rain nodes

00:19:34,720 --> 00:19:38,480
reserve has a controller actor that

00:19:36,559 --> 00:19:40,799
performs ghost-driven control loops that

00:19:38,480 --> 00:19:41,280
tries to reconcile the disaster's desire

00:19:40,799 --> 00:19:43,919
state

00:19:41,280 --> 00:19:45,919
and current state it is responsible for

00:19:43,919 --> 00:19:46,880
managing creating and updating replica

00:19:45,919 --> 00:19:48,960
actors

00:19:46,880 --> 00:19:51,600
reserve also provide a high performance

00:19:48,960 --> 00:19:53,760
http proxy that's built on top of the

00:19:51,600 --> 00:19:57,039
fastest python asynchronous server uv

00:19:53,760 --> 00:19:58,559
coin when web requests come in reserve

00:19:57,039 --> 00:20:00,160
proxies http requests

00:19:58,559 --> 00:20:02,640
and route traffic to the deployment

00:20:00,160 --> 00:20:05,280
actor so what happens

00:20:02,640 --> 00:20:07,200
what happens if we want to scale out the

00:20:05,280 --> 00:20:09,280
deployment can easily scale to many

00:20:07,200 --> 00:20:12,640
replicas across many machines

00:20:09,280 --> 00:20:14,080
all you need to do is to change the

00:20:12,640 --> 00:20:16,480
argument in the decorator

00:20:14,080 --> 00:20:18,559
when non-replicated specified reserve

00:20:16,480 --> 00:20:22,000
starts 10 copies of replicas

00:20:18,559 --> 00:20:23,280
each machine use one gpu each replica

00:20:22,000 --> 00:20:25,120
use one gpu

00:20:23,280 --> 00:20:26,559
and the http proxy will automatically

00:20:25,120 --> 00:20:28,960
load balance across

00:20:26,559 --> 00:20:31,039
replicas of course going up and down

00:20:28,960 --> 00:20:33,280
down can also be performed at runtime

00:20:31,039 --> 00:20:35,440
without any application downtime

00:20:33,280 --> 00:20:36,960
so that's the research deployment but

00:20:35,440 --> 00:20:39,919
what about more complex

00:20:36,960 --> 00:20:41,120
web feature reserve natively integrate

00:20:39,919 --> 00:20:43,440
with fast api

00:20:41,120 --> 00:20:44,240
which is a type safe and ergonomic web

00:20:43,440 --> 00:20:46,320
framework

00:20:44,240 --> 00:20:48,240
fast api has featured including

00:20:46,320 --> 00:20:50,960
automatic dependency injection

00:20:48,240 --> 00:20:52,159
type checking and validation open api

00:20:50,960 --> 00:20:55,120
dock generation

00:20:52,159 --> 00:20:55,679
as well we can directly pass the fast

00:20:55,120 --> 00:20:58,720
api

00:20:55,679 --> 00:21:00,640
app object into a reserve we serve the

00:20:58,720 --> 00:21:03,120
ingress decorator

00:21:00,640 --> 00:21:05,039
we can make sure that i uh make sure

00:21:03,120 --> 00:21:07,840
that all existing uh

00:21:05,039 --> 00:21:08,400
uh fast api routes would just work and

00:21:07,840 --> 00:21:10,159
you can

00:21:08,400 --> 00:21:12,320
attach new routes with the deployment

00:21:10,159 --> 00:21:14,480
class so states like loaded model

00:21:12,320 --> 00:21:15,600
network database connection can easily

00:21:14,480 --> 00:21:18,320
be managed

00:21:15,600 --> 00:21:20,799
architecturally we just make sure that

00:21:18,320 --> 00:21:22,400
your fast api app is correctly embedded

00:21:20,799 --> 00:21:26,320
into the replica actor and

00:21:22,400 --> 00:21:28,559
scale out across many read nodes

00:21:26,320 --> 00:21:30,960
finally in reserve you can directly call

00:21:28,559 --> 00:21:33,679
other deployments within your deployment

00:21:30,960 --> 00:21:34,480
and this can be our nested arbitrary

00:21:33,679 --> 00:21:36,320
deep

00:21:34,480 --> 00:21:37,520
uh what does this mean let's take a look

00:21:36,320 --> 00:21:39,600
at the code snippet uh

00:21:37,520 --> 00:21:41,280
that implements a simple two-stage

00:21:39,600 --> 00:21:44,480
pipeline um

00:21:41,280 --> 00:21:45,840
in this file um so like in the file we

00:21:44,480 --> 00:21:48,559
have three deployments

00:21:45,840 --> 00:21:49,280
featurizer predictor are just regular

00:21:48,559 --> 00:21:51,760
deployment

00:21:49,280 --> 00:21:53,360
and then the orchestrator receives input

00:21:51,760 --> 00:21:56,240
pass it to the featurizer

00:21:53,360 --> 00:21:57,039
via the featurizer handle and then pass

00:21:56,240 --> 00:21:59,600
the feature

00:21:57,039 --> 00:22:00,240
to the predictor the interface is just

00:21:59,600 --> 00:22:02,080
python

00:22:00,240 --> 00:22:04,159
you don't need to learn a new framework

00:22:02,080 --> 00:22:07,200
and near a new

00:22:04,159 --> 00:22:07,679
dsl as one of my colleagues will call it

00:22:07,200 --> 00:22:09,520
you can

00:22:07,679 --> 00:22:11,840
just write your for loop however you

00:22:09,520 --> 00:22:12,960
want surf handle gives you the

00:22:11,840 --> 00:22:14,799
flexibility

00:22:12,960 --> 00:22:17,200
similar to embedding everything in the

00:22:14,799 --> 00:22:19,280
web server but without the downside

00:22:17,200 --> 00:22:21,200
serve handle allow you to directly call

00:22:19,280 --> 00:22:23,440
other deployment

00:22:21,200 --> 00:22:24,400
that lives in other processes on the

00:22:23,440 --> 00:22:26,159
other nodes

00:22:24,400 --> 00:22:27,760
this allows you to scale out each

00:22:26,159 --> 00:22:30,880
deployment individually

00:22:27,760 --> 00:22:33,679
and the handle cost will be low balance

00:22:30,880 --> 00:22:36,080
between the replicas

00:22:33,679 --> 00:22:37,679
with reserve you can arbitrarily compose

00:22:36,080 --> 00:22:40,159
your deployment together

00:22:37,679 --> 00:22:42,000
to implement all four patterns we have

00:22:40,159 --> 00:22:44,159
shown in the pipeline code sample

00:22:42,000 --> 00:22:45,679
and samples should be straightforward

00:22:44,159 --> 00:22:47,280
and similar

00:22:45,679 --> 00:22:49,440
we'll now give you an example of

00:22:47,280 --> 00:22:51,600
business logic in research

00:22:49,440 --> 00:22:53,520
here are the simple change to the old

00:22:51,600 --> 00:22:54,240
web server code that implements business

00:22:53,520 --> 00:22:55,919
logic

00:22:54,240 --> 00:22:57,760
instead of loading the model directly we

00:22:55,919 --> 00:22:59,760
can retrieve a serve handle that wraps

00:22:57,760 --> 00:23:00,400
the model and offload the computation to

00:22:59,760 --> 00:23:02,880
another

00:23:00,400 --> 00:23:03,679
deployment all the data types are

00:23:02,880 --> 00:23:05,280
preserved

00:23:03,679 --> 00:23:07,200
and there is no need to write tensory

00:23:05,280 --> 00:23:10,080
intestinal api calls

00:23:07,200 --> 00:23:11,280
you can just pass regular python types

00:23:10,080 --> 00:23:13,840
and additionally

00:23:11,280 --> 00:23:14,960
the model deployment class can stay even

00:23:13,840 --> 00:23:16,880
in the same file

00:23:14,960 --> 00:23:18,559
and deploy together with the production

00:23:16,880 --> 00:23:21,039
prediction handler

00:23:18,559 --> 00:23:22,880
this makes it easy to reason about the

00:23:21,039 --> 00:23:25,600
code end to end

00:23:22,880 --> 00:23:27,919
model.remote looks like just a function

00:23:25,600 --> 00:23:30,799
and you can easily trace it to model

00:23:27,919 --> 00:23:32,720
deployment class in this way reserve

00:23:30,799 --> 00:23:35,200
helps you split up business logic and

00:23:32,720 --> 00:23:37,440
inference into two separate components

00:23:35,200 --> 00:23:38,240
one i o heavy and the other compute

00:23:37,440 --> 00:23:39,679
heavy

00:23:38,240 --> 00:23:41,919
you can now scale each piece

00:23:39,679 --> 00:23:42,640
individually without losing the ease of

00:23:41,919 --> 00:23:44,960
deployment

00:23:42,640 --> 00:23:45,919
and ability to reason about the code end

00:23:44,960 --> 00:23:47,840
to end

00:23:45,919 --> 00:23:49,520
additionally because monitor remote is

00:23:47,840 --> 00:23:52,159
just regular function call

00:23:49,520 --> 00:23:54,640
it's a lot more testable than separate

00:23:52,159 --> 00:23:56,880
external services

00:23:54,640 --> 00:23:58,799
so that's the power of reserve a

00:23:56,880 --> 00:24:00,000
framework we built for scaling out ml

00:23:58,799 --> 00:24:02,640
models in production

00:24:00,000 --> 00:24:04,400
it has three core concepts and apis

00:24:02,640 --> 00:24:06,880
deployment helps you to create

00:24:04,400 --> 00:24:08,080
a scalable deployment and update them

00:24:06,880 --> 00:24:09,919
over time

00:24:08,080 --> 00:24:12,320
ingress help you integrate with fully

00:24:09,919 --> 00:24:14,480
featured http application

00:24:12,320 --> 00:24:16,640
handle enables arbitrary nesting and

00:24:14,480 --> 00:24:19,120
composition of the deployment

00:24:16,640 --> 00:24:20,720
with these three concepts research give

00:24:19,120 --> 00:24:22,400
you the best of those world

00:24:20,720 --> 00:24:24,799
between wrapping your model in web

00:24:22,400 --> 00:24:26,000
server and manage many many different

00:24:24,799 --> 00:24:28,640
micro services

00:24:26,000 --> 00:24:30,400
in conclusion machine learning models in

00:24:28,640 --> 00:24:31,200
production means many models in

00:24:30,400 --> 00:24:33,760
production

00:24:31,200 --> 00:24:35,520
over the lifetime model are returned

00:24:33,760 --> 00:24:38,080
updated and redeployed

00:24:35,520 --> 00:24:38,799
models are added and scaled out we

00:24:38,080 --> 00:24:40,960
covered

00:24:38,799 --> 00:24:43,279
in this talk we cover four different

00:24:40,960 --> 00:24:44,480
patterns and show that existing tooling

00:24:43,279 --> 00:24:47,279
doesn't work well

00:24:44,480 --> 00:24:48,799
with the multiple model reality reserve

00:24:47,279 --> 00:24:50,400
is built for sexual reality

00:24:48,799 --> 00:24:52,720
and it is the best of the best world

00:24:50,400 --> 00:24:54,320
between wrapping the model in web server

00:24:52,720 --> 00:24:56,799
and working with many many different

00:24:54,320 --> 00:24:57,679
micro services to learn more about rate

00:24:56,799 --> 00:25:00,559
and reserve

00:24:57,679 --> 00:25:02,320
check out radario and reserve.org while

00:25:00,559 --> 00:25:04,559
also holding the second resume

00:25:02,320 --> 00:25:06,799
in late june is free and you can check

00:25:04,559 --> 00:25:09,200
out more production use cases of rey

00:25:06,799 --> 00:25:10,400
and research and lastly the company

00:25:09,200 --> 00:25:13,200
behind ray and reserve

00:25:10,400 --> 00:25:15,840
any skill is hiring check it out thank

00:25:13,200 --> 00:25:15,840
you

00:26:18,559 --> 00:26:20,640

YouTube URL: https://www.youtube.com/watch?v=GJt89jCiPUQ


