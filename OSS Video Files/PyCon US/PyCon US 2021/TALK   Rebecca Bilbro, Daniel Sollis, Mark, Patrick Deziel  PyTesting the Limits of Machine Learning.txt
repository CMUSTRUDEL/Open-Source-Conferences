Title: TALK   Rebecca Bilbro, Daniel Sollis, Mark, Patrick Deziel  PyTesting the Limits of Machine Learning
Publication date: 2021-05-30
Playlist: PyCon US 2021
Description: 
	Despite the hype cycle, each day machine learning becomes a little less magic and a little more real. Predictions increasingly drive our everyday lives, embedded into more of our everyday applications. To support this creative surge, development teams are evolving, integrating novel open source software and state-of-the-art GPU hardware, and bringing on essential new teammates like data ethicists and machine learning engineers. Software teams are also now challenged to build and maintain codebases that are intentionally not fully deterministic.

This nondeterminism can manifest in a number of surprising and oftentimes very stressful ways! Successive runs of model training may produce slight but meaningful variations. Data wrangling pipelines turn out to be extremely sensitive to the order in which transformations are applied, and require thoughtful orchestration to avoid leakage. Model hyperparameters that can be tuned independently may have mutually exclusive conditions. Models can also degrade over time, producing increasingly unreliable predictions. Moreover, open source libraries are living, dynamic things; the latest release of your team's favorite library might cause your code to suddenly behave in unexpected ways.

Put simply, as ML becomes more of an expectation than an exception in our industry, testing has never been more important! Fortunately, we are lucky to have a rich open source ecosystem to support us in our journey to build the next generation of apps in a safe, stable way. In this talk we'll share some hard-won lessons, favorite open source packages, and reusable techniques for testing ML software components.

Slides: https://docs.google.com/presentation/d/1Qrg0C5L6-5uQCtkUdqgw5UZPyFoCNJ07LxHWXVAzx2g
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:15,040 --> 00:00:18,960
hello

00:00:15,679 --> 00:00:20,480
and welcome to pycon 2021 my name is

00:00:18,960 --> 00:00:22,880
rebecca bilbrow

00:00:20,480 --> 00:00:24,000
and together with my co-presenters

00:00:22,880 --> 00:00:25,840
daniel salas

00:00:24,000 --> 00:00:27,439
and patrick desill we are going to be

00:00:25,840 --> 00:00:29,439
presenting a talk on

00:00:27,439 --> 00:00:31,840
pie testing the limits of machine

00:00:29,439 --> 00:00:34,079
learning which is informed by our

00:00:31,840 --> 00:00:35,760
experience working together as a team

00:00:34,079 --> 00:00:37,840
building machine learning products for

00:00:35,760 --> 00:00:40,239
industry and writing tests

00:00:37,840 --> 00:00:42,800
against those products to give you a

00:00:40,239 --> 00:00:44,640
sense of how the talk is going to unfold

00:00:42,800 --> 00:00:47,039
i'm going to start by giving a short

00:00:44,640 --> 00:00:49,680
introduction to motivate this question

00:00:47,039 --> 00:00:52,079
of why we test machine learning and then

00:00:49,680 --> 00:00:53,280
daniel is going to talk about building a

00:00:52,079 --> 00:00:55,399
testing api

00:00:53,280 --> 00:00:56,800
patrick will talk about dealing with

00:00:55,399 --> 00:00:58,719
non-determinism

00:00:56,800 --> 00:01:00,719
and i'll talk a little bit about how to

00:00:58,719 --> 00:01:03,199
do experimentation

00:01:00,719 --> 00:01:05,040
while still doing testing and then we'll

00:01:03,199 --> 00:01:06,560
leave you with a few tips

00:01:05,040 --> 00:01:08,640
that you can take with you after the

00:01:06,560 --> 00:01:11,760
talk so

00:01:08,640 --> 00:01:12,080
this question why test ml comes to me a

00:01:11,760 --> 00:01:14,080
lot

00:01:12,080 --> 00:01:15,600
and to be fair it's a question that i

00:01:14,080 --> 00:01:18,000
used to ask myself

00:01:15,600 --> 00:01:18,960
at the beginning of my career as a data

00:01:18,000 --> 00:01:20,560
scientist

00:01:18,960 --> 00:01:22,320
and i think it's an important question

00:01:20,560 --> 00:01:24,479
for us to really tackle head-on at the

00:01:22,320 --> 00:01:26,560
beginning of this talk

00:01:24,479 --> 00:01:27,680
you know the first family of questions

00:01:26,560 --> 00:01:29,680
that i get

00:01:27,680 --> 00:01:30,799
kind of in this vein are do we really

00:01:29,680 --> 00:01:33,520
need to test

00:01:30,799 --> 00:01:34,640
um our machine learning code usually

00:01:33,520 --> 00:01:37,040
these questions

00:01:34,640 --> 00:01:38,079
kind of follow these similar patterns

00:01:37,040 --> 00:01:40,720
you know people say

00:01:38,079 --> 00:01:41,759
testing is for software it's not for

00:01:40,720 --> 00:01:43,439
science

00:01:41,759 --> 00:01:45,840
it's not for data scientists we're

00:01:43,439 --> 00:01:46,799
scientists we do science we don't do

00:01:45,840 --> 00:01:48,720
software

00:01:46,799 --> 00:01:50,320
um you know i heard people say that

00:01:48,720 --> 00:01:52,560
they're what their teams do

00:01:50,320 --> 00:01:54,320
is hypothesis driven development they're

00:01:52,560 --> 00:01:55,920
testing out hypotheses they're not

00:01:54,320 --> 00:01:57,920
testing their code

00:01:55,920 --> 00:01:59,680
which is a different kind of best

00:01:57,920 --> 00:02:02,320
practice altogether

00:01:59,680 --> 00:02:03,040
and i've also heard people complain that

00:02:02,320 --> 00:02:05,360
you know

00:02:03,040 --> 00:02:07,119
it hinders the experimentation process

00:02:05,360 --> 00:02:08,879
if you feel like you have to write tests

00:02:07,119 --> 00:02:09,840
against all of your experimental

00:02:08,879 --> 00:02:11,760
research code

00:02:09,840 --> 00:02:13,120
i think these are important um kind of

00:02:11,760 --> 00:02:15,840
things to listen to i've

00:02:13,120 --> 00:02:17,680
i've also heard kind of a family of

00:02:15,840 --> 00:02:20,480
concerns that really fall into this

00:02:17,680 --> 00:02:22,319
category of is it even possible to test

00:02:20,480 --> 00:02:23,040
machine learning code you know i've

00:02:22,319 --> 00:02:24,640
heard

00:02:23,040 --> 00:02:26,640
engineers say that you know these

00:02:24,640 --> 00:02:28,160
algorithms are non-deterministic and so

00:02:26,640 --> 00:02:30,080
there's no way to test them

00:02:28,160 --> 00:02:31,280
you know i've heard people raise

00:02:30,080 --> 00:02:33,760
concerns about the sheer

00:02:31,280 --> 00:02:35,200
number of parameters that you would have

00:02:33,760 --> 00:02:36,720
to test and the combinations of

00:02:35,200 --> 00:02:37,599
parameters that you would have to test

00:02:36,720 --> 00:02:39,440
in order to

00:02:37,599 --> 00:02:41,840
kind of cover machine learning

00:02:39,440 --> 00:02:44,720
algorithms and then other teams complain

00:02:41,840 --> 00:02:46,319
that the tools that they're using for

00:02:44,720 --> 00:02:49,440
example jupyter notebooks

00:02:46,319 --> 00:02:51,360
you know don't support uh robust testing

00:02:49,440 --> 00:02:53,200
you know you won't be able to run a test

00:02:51,360 --> 00:02:56,400
runner against your jupiter notebook

00:02:53,200 --> 00:02:57,040
um at least not yet and so these are

00:02:56,400 --> 00:02:58,400
kind of the

00:02:57,040 --> 00:03:00,319
you know the concerns that i'm hearing

00:02:58,400 --> 00:03:01,840
from the community um and really what

00:03:00,319 --> 00:03:02,720
i'd like to say is i think these are all

00:03:01,840 --> 00:03:05,280
valid

00:03:02,720 --> 00:03:06,720
um and you know i think that when we

00:03:05,280 --> 00:03:09,920
build machine learning

00:03:06,720 --> 00:03:11,760
uh projects the dream is that uh the

00:03:09,920 --> 00:03:13,680
models that you're building are gonna go

00:03:11,760 --> 00:03:16,000
and be useful for someone right it's not

00:03:13,680 --> 00:03:18,319
just about winning a competition

00:03:16,000 --> 00:03:19,680
or kind of having a flashy presentation

00:03:18,319 --> 00:03:20,720
you know you want to build things that

00:03:19,680 --> 00:03:23,120
make the world better

00:03:20,720 --> 00:03:24,879
um you want your models to do good um

00:03:23,120 --> 00:03:27,440
and if you are going to be building

00:03:24,879 --> 00:03:27,920
models that have the potential of going

00:03:27,440 --> 00:03:30,959
into

00:03:27,920 --> 00:03:33,120
a user-facing product even you know

00:03:30,959 --> 00:03:34,400
a few iterations down the line you need

00:03:33,120 --> 00:03:35,840
to think about testing

00:03:34,400 --> 00:03:38,000
from the beginning right we need to make

00:03:35,840 --> 00:03:40,239
sure that we're protecting the consumers

00:03:38,000 --> 00:03:42,400
the eventual consumers of our models

00:03:40,239 --> 00:03:44,400
um and so that's why we need to test ml

00:03:42,400 --> 00:03:46,480
and so if there's any potential

00:03:44,400 --> 00:03:48,080
um that this could become you know part

00:03:46,480 --> 00:03:48,720
of a product or be in the hands of the

00:03:48,080 --> 00:03:51,519
user

00:03:48,720 --> 00:03:52,720
um we do need to write tests so next i'm

00:03:51,519 --> 00:03:55,599
going to pass it off

00:03:52,720 --> 00:03:57,360
to daniel who's going to talk a little

00:03:55,599 --> 00:03:58,480
bit about what the building blocks of

00:03:57,360 --> 00:04:01,840
something like that

00:03:58,480 --> 00:04:01,840
might look like

00:04:03,200 --> 00:04:07,760
thanks rebecca my name is daniel solis

00:04:06,000 --> 00:04:09,760
this is a bit of an introduction

00:04:07,760 --> 00:04:11,680
i've been working at unisys for coming

00:04:09,760 --> 00:04:13,040
up on two years now as a systems

00:04:11,680 --> 00:04:15,120
engineer

00:04:13,040 --> 00:04:16,880
my job really focuses on ai ops and

00:04:15,120 --> 00:04:18,400
building proof of concepts for machine

00:04:16,880 --> 00:04:20,000
learning projects

00:04:18,400 --> 00:04:21,759
and i'd like to go over a few things

00:04:20,000 --> 00:04:22,639
that i found really helpful in getting

00:04:21,759 --> 00:04:26,080
that job done

00:04:22,639 --> 00:04:27,600
particularly in regards to testing so

00:04:26,080 --> 00:04:29,040
one of the most important aspects of a

00:04:27,600 --> 00:04:30,639
good test suite is the structure

00:04:29,040 --> 00:04:32,400
surrounding the tests that ensure the

00:04:30,639 --> 00:04:33,759
tests are easy to write maintainable and

00:04:32,400 --> 00:04:35,199
enforced

00:04:33,759 --> 00:04:36,800
i'll go over some features in the sk

00:04:35,199 --> 00:04:38,400
learn library which make testing machine

00:04:36,800 --> 00:04:39,199
learning code a whole lot more

00:04:38,400 --> 00:04:40,320
manageable

00:04:39,199 --> 00:04:43,840
as well as some other ways to

00:04:40,320 --> 00:04:43,840
standardize your testing process

00:04:46,479 --> 00:04:50,000
the sklearn estimator and transformer

00:04:48,320 --> 00:04:52,240
classes collectively contain the fit

00:04:50,000 --> 00:04:54,400
predict and transform methods

00:04:52,240 --> 00:04:55,600
overriding these three methods are

00:04:54,400 --> 00:04:57,680
really helpful

00:04:55,600 --> 00:04:59,919
they allow you to generalize sqlearn's

00:04:57,680 --> 00:05:01,840
various models and transformations

00:04:59,919 --> 00:05:03,280
for consistent use across preprocessing

00:05:01,840 --> 00:05:04,960
as well as modeling

00:05:03,280 --> 00:05:06,320
this really reduces the number of tests

00:05:04,960 --> 00:05:11,840
required as well as simplifying the

00:05:06,320 --> 00:05:11,840
tests that do need to be written

00:05:12,639 --> 00:05:16,080
so here we see the basic idea of

00:05:14,560 --> 00:05:17,520
creating a wrapper class

00:05:16,080 --> 00:05:20,160
basically just inherit from the

00:05:17,520 --> 00:05:23,520
estimator and transformer

00:05:20,160 --> 00:05:25,680
classes and then overload the predict

00:05:23,520 --> 00:05:27,280
fit and transform methods

00:05:25,680 --> 00:05:29,120
this creates a catch-all for sql in

00:05:27,280 --> 00:05:29,919
various classes basically allowing you

00:05:29,120 --> 00:05:32,320
to

00:05:29,919 --> 00:05:33,919
wrap both transformers and estimators

00:05:32,320 --> 00:05:35,840
and get some additional metadata that

00:05:33,919 --> 00:05:37,600
sqlearn doesn't surface

00:05:35,840 --> 00:05:39,759
for instance in the fit method you could

00:05:37,600 --> 00:05:41,039
actually just get the fit time

00:05:39,759 --> 00:05:43,520
with very little work which can be

00:05:41,039 --> 00:05:44,800
helpful for testing later

00:05:43,520 --> 00:05:48,160
similar with the predicted transform

00:05:44,800 --> 00:05:48,160
methods it really allows you to do

00:05:50,840 --> 00:05:53,840
anything

00:05:52,240 --> 00:05:55,759
uh the next few uh the next feature i'd

00:05:53,840 --> 00:05:57,120
like to highlight are pipelines and

00:05:55,759 --> 00:05:58,240
future unions

00:05:57,120 --> 00:06:00,240
these let you organize the

00:05:58,240 --> 00:06:00,960
pre-processing and modeling stages so

00:06:00,240 --> 00:06:02,400
you can really

00:06:00,960 --> 00:06:04,800
just iterate very quickly through

00:06:02,400 --> 00:06:06,400
experiments not only does this reduce

00:06:04,800 --> 00:06:08,639
time spent coding but it also makes

00:06:06,400 --> 00:06:10,319
writing tests a lot easier

00:06:08,639 --> 00:06:11,680
pipelines are great for simple uses

00:06:10,319 --> 00:06:13,440
while featuring unions

00:06:11,680 --> 00:06:15,120
are really designed for more complicated

00:06:13,440 --> 00:06:16,240
purposes and they specialize in

00:06:15,120 --> 00:06:19,360
parallelizing

00:06:16,240 --> 00:06:20,560
steps by using a wrapper class like we

00:06:19,360 --> 00:06:22,160
mentioned previously

00:06:20,560 --> 00:06:24,800
using these features becomes a lot more

00:06:22,160 --> 00:06:24,800
streamlined

00:06:29,120 --> 00:06:33,120
so here's an example of a pipeline as

00:06:31,919 --> 00:06:35,280
you can see

00:06:33,120 --> 00:06:36,240
this pipeline basically just extracts

00:06:35,280 --> 00:06:39,440
some data from

00:06:36,240 --> 00:06:41,680
a series of essays translates those into

00:06:39,440 --> 00:06:44,319
a count matrix and then takes that count

00:06:41,680 --> 00:06:46,720
matrix and turns it into a tf idf

00:06:44,319 --> 00:06:48,720
representation and then finally we train

00:06:46,720 --> 00:06:51,120
a multinomial knife based classifier

00:06:48,720 --> 00:06:51,120
with that

00:06:51,840 --> 00:06:54,960
as you can see here like a pipeline

00:06:53,520 --> 00:06:56,720
really reduces the amount of code

00:06:54,960 --> 00:06:58,000
required to perform pre-processing and

00:06:56,720 --> 00:06:59,440
training of a model

00:06:58,000 --> 00:07:03,840
and obviously this is going to apply to

00:06:59,440 --> 00:07:03,840
your tests as well

00:07:05,599 --> 00:07:09,360
so here's an example of a feature union

00:07:08,080 --> 00:07:09,919
very similar to the pipeline from

00:07:09,360 --> 00:07:12,319
earlier

00:07:09,919 --> 00:07:14,720
uh you can see through the code that the

00:07:12,319 --> 00:07:15,680
um it's basically really just a nested

00:07:14,720 --> 00:07:17,039
pipeline

00:07:15,680 --> 00:07:19,280
which is why it's really good for

00:07:17,039 --> 00:07:21,680
parallelizing different

00:07:19,280 --> 00:07:21,680
steps

00:07:24,560 --> 00:07:28,080
so in addition to using the different

00:07:26,479 --> 00:07:29,680
features from sklearn that i mentioned

00:07:28,080 --> 00:07:31,919
previously another big part of

00:07:29,680 --> 00:07:34,000
maintaining a healthy testing suite

00:07:31,919 --> 00:07:36,000
is making sure your repository maintains

00:07:34,000 --> 00:07:38,000
high coding standards

00:07:36,000 --> 00:07:40,639
so what we decided to use to help with

00:07:38,000 --> 00:07:42,720
this were the black formatter

00:07:40,639 --> 00:07:44,000
and a pre-commit pre-commit just to

00:07:42,720 --> 00:07:47,759
ensure that black is run

00:07:44,000 --> 00:07:49,520
every time anyone does a push um

00:07:47,759 --> 00:07:51,360
yeah it really helps ensure that the

00:07:49,520 --> 00:07:53,599
repo maintains

00:07:51,360 --> 00:07:54,960
like high coding standards and is clean

00:07:53,599 --> 00:07:57,280
across different commits from different

00:07:54,960 --> 00:07:57,280
people

00:07:57,599 --> 00:08:02,160
uh now while black is really great at

00:08:00,879 --> 00:08:04,560
maintaining a clean repo

00:08:02,160 --> 00:08:06,080
uh it does have its downsides it's

00:08:04,560 --> 00:08:07,199
pretty strict and might not be for

00:08:06,080 --> 00:08:08,560
everyone

00:08:07,199 --> 00:08:10,479
there are some alternatives to black

00:08:08,560 --> 00:08:13,280
such as why apf which is maintained by

00:08:10,479 --> 00:08:14,560
google as well as auto pep8

00:08:13,280 --> 00:08:15,759
these would be good for people looking

00:08:14,560 --> 00:08:19,039
for something a little bit less

00:08:15,759 --> 00:08:19,039
uncompromising than black

00:08:22,000 --> 00:08:26,160
one final thing i'd like to highlight is

00:08:23,680 --> 00:08:28,240
really the importance of ci cd

00:08:26,160 --> 00:08:29,680
for this we ended up using jenkins you

00:08:28,240 --> 00:08:32,880
can see on the right are

00:08:29,680 --> 00:08:34,080
cice flow for each push pre-commit would

00:08:32,880 --> 00:08:35,599
launch black

00:08:34,080 --> 00:08:38,240
to format the code and ensure code

00:08:35,599 --> 00:08:40,959
cleanliness and then jenkins would

00:08:38,240 --> 00:08:43,039
create a build and run our tests

00:08:40,959 --> 00:08:44,159
uh this is this is really an incredibly

00:08:43,039 --> 00:08:46,959
important

00:08:44,159 --> 00:08:48,720
part of the process um jenkins is an

00:08:46,959 --> 00:08:50,560
extremely useful way to ensure that

00:08:48,720 --> 00:08:51,920
testing is enforced

00:08:50,560 --> 00:08:53,600
we actually managed to find a couple of

00:08:51,920 --> 00:08:54,080
issues with some libraries we're using

00:08:53,600 --> 00:08:55,839
the

00:08:54,080 --> 00:08:58,320
versions we were using that we wouldn't

00:08:55,839 --> 00:09:00,160
have found otherwise

00:08:58,320 --> 00:09:01,760
plus automatic builds really helps

00:09:00,160 --> 00:09:02,320
ensure that code works in different

00:09:01,760 --> 00:09:05,519
environments

00:09:02,320 --> 00:09:07,279
on different machines now all passed off

00:09:05,519 --> 00:09:10,320
to patrick so he can go over dealing

00:09:07,279 --> 00:09:10,320
with non-determinism

00:09:11,920 --> 00:09:15,920
thanks daniel uh hey everybody my name

00:09:14,480 --> 00:09:18,399
is patrick de zeal

00:09:15,920 --> 00:09:19,440
uh this is my first python so pretty

00:09:18,399 --> 00:09:21,760
excited

00:09:19,440 --> 00:09:22,560
i've been working at unisys for a number

00:09:21,760 --> 00:09:26,160
of years

00:09:22,560 --> 00:09:29,279
uh most recently we've been working on

00:09:26,160 --> 00:09:33,600
uh proof of concepts in python for

00:09:29,279 --> 00:09:36,959
machine learning applications and

00:09:33,600 --> 00:09:38,720
so like rebecca mentioned earlier uh

00:09:36,959 --> 00:09:41,200
testing machine learning code can seem

00:09:38,720 --> 00:09:43,839
like it's pretty intimidating

00:09:41,200 --> 00:09:46,000
uh but it's important not to get

00:09:43,839 --> 00:09:46,480
discouraged by it so i'm gonna show you

00:09:46,000 --> 00:09:48,240
some

00:09:46,480 --> 00:09:51,440
techniques you can use to deal with some

00:09:48,240 --> 00:09:53,680
of the weird stuff like non-determinism

00:09:51,440 --> 00:09:56,240
when you're looking at testing machine

00:09:53,680 --> 00:09:56,240
learning code

00:09:56,959 --> 00:10:03,440
so let's say we have the example where

00:10:00,240 --> 00:10:05,200
we have a bunch of images of of

00:10:03,440 --> 00:10:05,600
different things and we want to classify

00:10:05,200 --> 00:10:08,720
them

00:10:05,600 --> 00:10:12,160
as either muffins or

00:10:08,720 --> 00:10:12,160
chihuahuas or dogs

00:10:12,240 --> 00:10:15,279
so we can write a good cycle and

00:10:14,160 --> 00:10:19,040
pipeline for this

00:10:15,279 --> 00:10:21,360
uh but the real question is um

00:10:19,040 --> 00:10:22,720
how do we handle non-determinism when

00:10:21,360 --> 00:10:24,800
we're testing

00:10:22,720 --> 00:10:25,839
how do we handle the multiple parameters

00:10:24,800 --> 00:10:28,720
that our

00:10:25,839 --> 00:10:30,480
cycle learn apis require and how do we

00:10:28,720 --> 00:10:32,480
handle the small variations that

00:10:30,480 --> 00:10:35,839
could occur when we're running our

00:10:32,480 --> 00:10:37,440
pipeline over and over again

00:10:35,839 --> 00:10:39,120
so the first step in non-determinism

00:10:37,440 --> 00:10:42,240
that you might encounter is

00:10:39,120 --> 00:10:43,839
if you have a data set that

00:10:42,240 --> 00:10:45,839
you're splitting into train and test but

00:10:43,839 --> 00:10:48,640
you're doing it

00:10:45,839 --> 00:10:50,000
randomly each time you execute your uh

00:10:48,640 --> 00:10:52,959
for your pipeline

00:10:50,000 --> 00:10:54,480
so if you're feeding in different data

00:10:52,959 --> 00:10:56,160
to your pipeline obviously

00:10:54,480 --> 00:10:59,040
you're probably going to get different

00:10:56,160 --> 00:11:02,079
results out of that which is

00:10:59,040 --> 00:11:04,320
not conducive to testing

00:11:02,079 --> 00:11:05,360
the second type of non-determinism is if

00:11:04,320 --> 00:11:06,720
you have

00:11:05,360 --> 00:11:08,959
the same data that you're feeding into

00:11:06,720 --> 00:11:11,600
the pipeline but

00:11:08,959 --> 00:11:13,440
your your underlying machine learning

00:11:11,600 --> 00:11:14,000
algorithms have some type of random

00:11:13,440 --> 00:11:17,040
process to

00:11:14,000 --> 00:11:20,560
them like for example uh stochastic

00:11:17,040 --> 00:11:21,839
gradient descent uh so that each time

00:11:20,560 --> 00:11:25,040
you run your pipeline

00:11:21,839 --> 00:11:26,720
you're getting different results uh

00:11:25,040 --> 00:11:28,880
which is difficult to test because you

00:11:26,720 --> 00:11:30,000
don't really know what to expect

00:11:28,880 --> 00:11:31,920
so you don't know what to put in your

00:11:30,000 --> 00:11:34,800
test

00:11:31,920 --> 00:11:35,680
so one thing they can do with this is

00:11:34,800 --> 00:11:38,079
you can

00:11:35,680 --> 00:11:39,839
fix the random seed and that will ensure

00:11:38,079 --> 00:11:40,959
that in all your non-deterministic

00:11:39,839 --> 00:11:44,560
functions

00:11:40,959 --> 00:11:47,440
you're going to execute code

00:11:44,560 --> 00:11:48,959
the same way because you're fixing all

00:11:47,440 --> 00:11:50,240
the points where you're making a random

00:11:48,959 --> 00:11:54,079
decision

00:11:50,240 --> 00:11:56,800
so one way you can do this

00:11:54,079 --> 00:11:57,120
if you're using psyclearn you can it has

00:11:56,800 --> 00:11:58,800
a

00:11:57,120 --> 00:12:00,800
random state parameter which they

00:11:58,800 --> 00:12:02,880
provide and for each of their

00:12:00,800 --> 00:12:05,680
non-deterministic functions

00:12:02,880 --> 00:12:06,560
and so all you have to do is look to see

00:12:05,680 --> 00:12:09,760
if it's

00:12:06,560 --> 00:12:12,320
supported in the the giant list of

00:12:09,760 --> 00:12:14,560
parameters and if it is that's good then

00:12:12,320 --> 00:12:14,560
you can

00:12:14,720 --> 00:12:17,760
set the random state that way very

00:12:16,240 --> 00:12:20,320
easily if you set it to

00:12:17,760 --> 00:12:22,079
some constant then each time you run

00:12:20,320 --> 00:12:25,120
your

00:12:22,079 --> 00:12:26,240
pipeline for your tests it's going to

00:12:25,120 --> 00:12:28,720
produce the same

00:12:26,240 --> 00:12:28,720
results

00:12:29,360 --> 00:12:33,680
so let's say we take our muffins and

00:12:32,320 --> 00:12:36,399
dogs examples from before

00:12:33,680 --> 00:12:37,440
and we can write a very simple function

00:12:36,399 --> 00:12:41,920
which takes in

00:12:37,440 --> 00:12:44,800
a data set of all those images and

00:12:41,920 --> 00:12:45,680
we track we extract features for them we

00:12:44,800 --> 00:12:49,279
create our train

00:12:45,680 --> 00:12:52,639
and test splits and then we run an

00:12:49,279 --> 00:12:55,760
mlp classifier on it

00:12:52,639 --> 00:12:57,440
and then we return our predictions based

00:12:55,760 --> 00:13:01,200
on a test set

00:12:57,440 --> 00:13:03,600
so um

00:13:01,200 --> 00:13:06,079
we can here we can add the random state

00:13:03,600 --> 00:13:08,560
to both our train test blade and our mlp

00:13:06,079 --> 00:13:09,839
classifier and what that does is fixes

00:13:08,560 --> 00:13:11,839
both those functions

00:13:09,839 --> 00:13:13,600
so that we're producing the same results

00:13:11,839 --> 00:13:17,680
each time and we'd expect

00:13:13,600 --> 00:13:20,720
this function to behave the same way

00:13:17,680 --> 00:13:24,160
on multiple executions

00:13:20,720 --> 00:13:25,200
so then that allows us to slap a test

00:13:24,160 --> 00:13:29,440
onto this

00:13:25,200 --> 00:13:31,360
function and

00:13:29,440 --> 00:13:32,959
this is pretty simple if we're using pi

00:13:31,360 --> 00:13:35,680
test because

00:13:32,959 --> 00:13:37,200
we just have to define what our expected

00:13:35,680 --> 00:13:40,160
um

00:13:37,200 --> 00:13:40,959
results are and we've uh we have a

00:13:40,160 --> 00:13:43,199
situation where

00:13:40,959 --> 00:13:44,480
a dog is zero and muffin is one so we

00:13:43,199 --> 00:13:46,959
just send in our

00:13:44,480 --> 00:13:49,839
instead our expected list and then

00:13:46,959 --> 00:13:51,519
compare to that once we've

00:13:49,839 --> 00:13:53,199
executed our muffins and dogs function

00:13:51,519 --> 00:13:58,320
which we wrote earlier

00:13:53,199 --> 00:14:01,519
and this works pretty well but

00:13:58,320 --> 00:14:02,000
this in reality is is going to be a very

00:14:01,519 --> 00:14:05,279
simple

00:14:02,000 --> 00:14:08,000
case um you're much more likely to have

00:14:05,279 --> 00:14:08,560
a very a more complicated situation

00:14:08,000 --> 00:14:11,279
where you're

00:14:08,560 --> 00:14:13,519
passing in multiple parameters to your

00:14:11,279 --> 00:14:16,560
your mlp classifier

00:14:13,519 --> 00:14:17,440
um or other things so the real question

00:14:16,560 --> 00:14:19,680
is how do we

00:14:17,440 --> 00:14:20,800
handle these multiple parameters without

00:14:19,680 --> 00:14:24,880
turning our test code

00:14:20,800 --> 00:14:28,480
into just complete unreadable nonsense

00:14:24,880 --> 00:14:32,240
so one thing we can do here is

00:14:28,480 --> 00:14:35,040
utilize the parametrize marker within

00:14:32,240 --> 00:14:37,360
pi test and this is a one of those most

00:14:35,040 --> 00:14:40,079
powerful things that i've

00:14:37,360 --> 00:14:41,120
discovered pi test because what allows

00:14:40,079 --> 00:14:44,320
you to do is

00:14:41,120 --> 00:14:45,839
set um identify sets of parameters

00:14:44,320 --> 00:14:48,560
which are passing into the same test

00:14:45,839 --> 00:14:50,880
function so

00:14:48,560 --> 00:14:51,680
you don't have to write a test function

00:14:50,880 --> 00:14:54,320
for

00:14:51,680 --> 00:14:56,800
every single set of things you want to

00:14:54,320 --> 00:14:59,440
pass into your muffins or dogs function

00:14:56,800 --> 00:15:00,720
we just have to write one test function

00:14:59,440 --> 00:15:04,480
which is taking in

00:15:00,720 --> 00:15:06,880
whatever parameters we're defining and

00:15:04,480 --> 00:15:07,519
then define each set of parameter sets

00:15:06,880 --> 00:15:10,399
that you're

00:15:07,519 --> 00:15:12,240
passing in so for example i might use

00:15:10,399 --> 00:15:15,040
this with

00:15:12,240 --> 00:15:16,639
different data sets or i might want to

00:15:15,040 --> 00:15:17,519
try out a different classifier like a

00:15:16,639 --> 00:15:20,800
random forest

00:15:17,519 --> 00:15:23,600
classifier or i

00:15:20,800 --> 00:15:25,600
um one cool thing that we can do here is

00:15:23,600 --> 00:15:26,480
we can redefine our muffins and dogs

00:15:25,600 --> 00:15:29,120
function

00:15:26,480 --> 00:15:31,600
to take in like a generic cycle or an

00:15:29,120 --> 00:15:35,120
estimator

00:15:31,600 --> 00:15:37,519
and also generic parameters and

00:15:35,120 --> 00:15:38,800
since we define our parameters as more

00:15:37,519 --> 00:15:40,959
of like a dictionary

00:15:38,800 --> 00:15:42,639
we can it's very easy for us to define

00:15:40,959 --> 00:15:46,959
it up in the

00:15:42,639 --> 00:15:48,800
parametrize code and we can use this to

00:15:46,959 --> 00:15:51,360
set whatever parameters we need for the

00:15:48,800 --> 00:15:53,519
estimator we can also use it to

00:15:51,360 --> 00:15:55,120
set that random state which we talked

00:15:53,519 --> 00:15:58,639
about earlier

00:15:55,120 --> 00:16:00,959
and this gives us a very concise way of

00:15:58,639 --> 00:16:02,079
writing tests and makes them very easy

00:16:00,959 --> 00:16:05,440
to maintain

00:16:02,079 --> 00:16:05,440
modify etc

00:16:05,600 --> 00:16:09,519
so one thing that you also might

00:16:08,480 --> 00:16:11,199
encounter with

00:16:09,519 --> 00:16:12,639
machine learning is since there's a lot

00:16:11,199 --> 00:16:15,519
of floating point arithmetic

00:16:12,639 --> 00:16:17,839
under the hood things can get a little

00:16:15,519 --> 00:16:20,079
strange

00:16:17,839 --> 00:16:22,079
things might not work exactly as you

00:16:20,079 --> 00:16:23,440
would expect based on your math class

00:16:22,079 --> 00:16:25,360
because

00:16:23,440 --> 00:16:26,959
um of the ways floating points are

00:16:25,360 --> 00:16:29,279
handled

00:16:26,959 --> 00:16:30,639
under the covers uh and depending on

00:16:29,279 --> 00:16:33,920
what precision you have for

00:16:30,639 --> 00:16:37,120
floating point in your python

00:16:33,920 --> 00:16:38,720
so in order to correctly test

00:16:37,120 --> 00:16:40,480
machine learning code you're going to

00:16:38,720 --> 00:16:42,000
need a better way of comparing flowing

00:16:40,480 --> 00:16:44,880
point results

00:16:42,000 --> 00:16:47,519
and what that boils down to is that we

00:16:44,880 --> 00:16:49,600
need a way of handling results that are

00:16:47,519 --> 00:16:50,800
close enough to what we expect them to

00:16:49,600 --> 00:16:54,720
be

00:16:50,800 --> 00:16:57,920
um another way we might use this

00:16:54,720 --> 00:17:01,199
we might need this is that if we're uh

00:16:57,920 --> 00:17:03,199
let's say we're trying to time how fast

00:17:01,199 --> 00:17:06,079
our algorithm is going to train

00:17:03,199 --> 00:17:07,280
um since if we're basing it off of wall

00:17:06,079 --> 00:17:08,799
clock time

00:17:07,280 --> 00:17:10,640
it's going to be a bit different every

00:17:08,799 --> 00:17:13,039
time but we need it to be

00:17:10,640 --> 00:17:15,360
close enough maybe within like .01

00:17:13,039 --> 00:17:19,120
seconds or something

00:17:15,360 --> 00:17:23,120
so another thing that pi test provides

00:17:19,120 --> 00:17:26,559
is the prox function and

00:17:23,120 --> 00:17:27,600
what this allows you to do is uh write a

00:17:26,559 --> 00:17:30,080
bit of shorthand

00:17:27,600 --> 00:17:32,000
for making these uh weird like

00:17:30,080 --> 00:17:35,520
approximate comparisons

00:17:32,000 --> 00:17:36,960
before we'd have to write maybe like an

00:17:35,520 --> 00:17:39,919
absolute value notation

00:17:36,960 --> 00:17:42,160
for this and then compare whether or not

00:17:39,919 --> 00:17:45,840
that result is less than

00:17:42,160 --> 00:17:46,799
0.01 or whatever our expected uh

00:17:45,840 --> 00:17:50,799
relative

00:17:46,799 --> 00:17:52,559
tolerances um so the problem with that

00:17:50,799 --> 00:17:54,320
is that when you're looking at this code

00:17:52,559 --> 00:17:54,720
it's hard to understand what it's doing

00:17:54,320 --> 00:17:59,440
and it's

00:17:54,720 --> 00:18:02,799
also hard to understand how to change it

00:17:59,440 --> 00:18:06,000
so if we use the approx it's a bit more

00:18:02,799 --> 00:18:08,320
readable and um allows us to do

00:18:06,000 --> 00:18:10,160
what we wanted to do which is just

00:18:08,320 --> 00:18:14,960
compare two values within

00:18:10,160 --> 00:18:16,799
some relative range that we pass in

00:18:14,960 --> 00:18:18,160
so hopefully you'll find these

00:18:16,799 --> 00:18:20,240
techniques useful

00:18:18,160 --> 00:18:21,520
when you're dealing with testing machine

00:18:20,240 --> 00:18:24,160
learning code

00:18:21,520 --> 00:18:25,200
and makes you a bit more confident and

00:18:24,160 --> 00:18:29,039
actually being

00:18:25,200 --> 00:18:32,640
able to test all the aspects of your

00:18:29,039 --> 00:18:36,400
ml code without

00:18:32,640 --> 00:18:39,840
having to resort to not testing it which

00:18:36,400 --> 00:18:42,559
would be probably not advised if you're

00:18:39,840 --> 00:18:43,039
trying to put something into production

00:18:42,559 --> 00:18:46,000
so

00:18:43,039 --> 00:18:46,640
now i'll pass it over to rebecca to talk

00:18:46,000 --> 00:18:50,480
about

00:18:46,640 --> 00:18:50,480
diagnostics for machine learning

00:18:50,720 --> 00:18:54,640
thanks patrick at the beginning of this

00:18:53,679 --> 00:18:57,280
talk

00:18:54,640 --> 00:18:58,160
i said that if you are going to be

00:18:57,280 --> 00:19:00,799
building

00:18:58,160 --> 00:19:01,840
um models that have the potential to go

00:19:00,799 --> 00:19:04,400
into a

00:19:01,840 --> 00:19:05,440
consumer facing product you need to

00:19:04,400 --> 00:19:08,480
write tests

00:19:05,440 --> 00:19:11,760
um but i do worry that you know there is

00:19:08,480 --> 00:19:12,240
a possibility that you might take away

00:19:11,760 --> 00:19:14,880
from this

00:19:12,240 --> 00:19:16,720
that i think that there is sort of a

00:19:14,880 --> 00:19:17,440
trade-off that you can either do good

00:19:16,720 --> 00:19:20,160
engineering

00:19:17,440 --> 00:19:21,520
you know that comes with testing and um

00:19:20,160 --> 00:19:23,039
you know code formatting and all of

00:19:21,520 --> 00:19:25,440
these things or you can do good

00:19:23,039 --> 00:19:27,760
experimentation and be a good scientist

00:19:25,440 --> 00:19:30,080
um what i would like to suggest is that

00:19:27,760 --> 00:19:30,880
that might actually be a false dichotomy

00:19:30,080 --> 00:19:33,280
and that there are

00:19:30,880 --> 00:19:34,400
tools out there open source tools um

00:19:33,280 --> 00:19:36,200
that can help you

00:19:34,400 --> 00:19:37,600
kind of do these two things

00:19:36,200 --> 00:19:40,640
simultaneously

00:19:37,600 --> 00:19:42,080
one of the tools that i'm particularly

00:19:40,640 --> 00:19:44,320
excited to talk about

00:19:42,080 --> 00:19:45,760
is the yellow brick project yellow brick

00:19:44,320 --> 00:19:48,400
is a pure

00:19:45,760 --> 00:19:49,360
python open source project that sort of

00:19:48,400 --> 00:19:52,559
takes the best

00:19:49,360 --> 00:19:55,520
of scikit-learn together with matplotlib

00:19:52,559 --> 00:19:57,360
and allows you to visualize your machine

00:19:55,520 --> 00:19:58,080
learning models and understand what's

00:19:57,360 --> 00:20:01,440
going on

00:19:58,080 --> 00:20:04,159
under the hood it helps with diagnostics

00:20:01,440 --> 00:20:05,200
it helps with algorithm selection and

00:20:04,159 --> 00:20:08,799
hyper

00:20:05,200 --> 00:20:11,679
parameter tuning this is a

00:20:08,799 --> 00:20:13,200
package that i co-wrote with a colleague

00:20:11,679 --> 00:20:15,600
benjamin bengfort

00:20:13,200 --> 00:20:16,240
about five years ago now and actually

00:20:15,600 --> 00:20:19,440
announced

00:20:16,240 --> 00:20:20,559
at pycon many years ago and since then

00:20:19,440 --> 00:20:22,240
we've amassed

00:20:20,559 --> 00:20:23,679
many contributors from all over the

00:20:22,240 --> 00:20:25,440
world and

00:20:23,679 --> 00:20:26,880
really the main thing that we've tried

00:20:25,440 --> 00:20:29,600
to do in the

00:20:26,880 --> 00:20:30,159
yellow brick api is abstract out some of

00:20:29,600 --> 00:20:32,400
the very

00:20:30,159 --> 00:20:33,200
common machine learning workflow

00:20:32,400 --> 00:20:35,280
patterns

00:20:33,200 --> 00:20:37,039
one of those is trying to understand

00:20:35,280 --> 00:20:38,640
what's in your data right so

00:20:37,039 --> 00:20:40,320
taking your data set performing

00:20:38,640 --> 00:20:42,480
transformations and seeing

00:20:40,320 --> 00:20:44,240
if the transformation was successful at

00:20:42,480 --> 00:20:47,120
removing outliers or

00:20:44,240 --> 00:20:48,640
removing noise at making the clusters

00:20:47,120 --> 00:20:51,679
cohere more

00:20:48,640 --> 00:20:54,080
at you know reducing multi-collinearity

00:20:51,679 --> 00:20:55,360
so that's one of the patterns um that

00:20:54,080 --> 00:20:57,200
workflow patterns that we

00:20:55,360 --> 00:20:58,640
support with the yellow brick api the

00:20:57,200 --> 00:21:01,039
second one is

00:20:58,640 --> 00:21:02,480
allowing you to compare different

00:21:01,039 --> 00:21:04,320
pipelines right to

00:21:02,480 --> 00:21:06,159
kind of use some combination of

00:21:04,320 --> 00:21:08,320
transformers and

00:21:06,159 --> 00:21:10,400
couple that with an estimator and then

00:21:08,320 --> 00:21:11,200
evaluate how well that model has

00:21:10,400 --> 00:21:14,159
performed

00:21:11,200 --> 00:21:16,400
you know and that could mean comparing

00:21:14,159 --> 00:21:18,480
completely different algorithms together

00:21:16,400 --> 00:21:21,039
or comparing one algorithm with

00:21:18,480 --> 00:21:22,480
variations on different hyper parameters

00:21:21,039 --> 00:21:24,400
and so what does that look like in

00:21:22,480 --> 00:21:27,440
practice

00:21:24,400 --> 00:21:30,880
here's an example that takes patrick's

00:21:27,440 --> 00:21:31,679
dog and muffin classifier and allows you

00:21:30,880 --> 00:21:34,720
to

00:21:31,679 --> 00:21:37,200
compare two different algorithms a

00:21:34,720 --> 00:21:39,919
random forest classifier classifier with

00:21:37,200 --> 00:21:42,880
a stochastic gradient descent classifier

00:21:39,919 --> 00:21:44,000
and see how the two different algorithms

00:21:42,880 --> 00:21:46,000
performed on the same

00:21:44,000 --> 00:21:47,840
data set and i hope that you see right

00:21:46,000 --> 00:21:50,000
away that you know there's a lot of

00:21:47,840 --> 00:21:51,280
nuance that goes into this visualization

00:21:50,000 --> 00:21:52,559
that you can't get if you're just

00:21:51,280 --> 00:21:55,200
looking at a high level

00:21:52,559 --> 00:21:57,120
f1 score right so already we can see

00:21:55,200 --> 00:21:58,880
that there's nuance related to the

00:21:57,120 --> 00:22:00,640
precision and recall

00:21:58,880 --> 00:22:02,000
across our different classes between

00:22:00,640 --> 00:22:04,320
these two models these are probably

00:22:02,000 --> 00:22:07,280
things that you want to be aware of

00:22:04,320 --> 00:22:07,679
in terms of experimentation so how do we

00:22:07,280 --> 00:22:11,520
wrap

00:22:07,679 --> 00:22:12,400
in that experimentation into a testable

00:22:11,520 --> 00:22:16,320
uh

00:22:12,400 --> 00:22:19,440
flow so let's look again at patrick's

00:22:16,320 --> 00:22:21,679
function that differentiates um muffins

00:22:19,440 --> 00:22:23,520
and dogs and does model training

00:22:21,679 --> 00:22:25,200
under the hood you know so what we're

00:22:23,520 --> 00:22:26,799
going to do here is in addition to

00:22:25,200 --> 00:22:29,840
importing our scikit-learn

00:22:26,799 --> 00:22:31,520
classifiers we're going to import a

00:22:29,840 --> 00:22:33,200
yellow brick visualizer called the

00:22:31,520 --> 00:22:35,840
classification report

00:22:33,200 --> 00:22:37,840
and so in our function the first step is

00:22:35,840 --> 00:22:40,559
we're going to create a figure

00:22:37,840 --> 00:22:42,480
and inside that figure we're going to be

00:22:40,559 --> 00:22:44,080
able to plot this data and then we'll

00:22:42,480 --> 00:22:47,200
store that image to disk

00:22:44,080 --> 00:22:51,039
and so that will run in parallel with

00:22:47,200 --> 00:22:52,320
our training code so we split our data

00:22:51,039 --> 00:22:54,640
into a train and test

00:22:52,320 --> 00:22:57,039
we instantiate a visualizer in the same

00:22:54,640 --> 00:23:00,799
way that you're used to instantiating

00:22:57,039 --> 00:23:02,720
classifiers you pass in our model so

00:23:00,799 --> 00:23:05,600
whatever the model is going to be

00:23:02,720 --> 00:23:07,919
that we use in the parameter here the

00:23:05,600 --> 00:23:08,799
classes we can pass in a color map

00:23:07,919 --> 00:23:10,720
optionally

00:23:08,799 --> 00:23:12,080
we can pass in the axes for the figure

00:23:10,720 --> 00:23:14,880
that we've just created

00:23:12,080 --> 00:23:15,760
we can um turn support on which will

00:23:14,880 --> 00:23:18,240
tell us how many

00:23:15,760 --> 00:23:19,440
data points went into each training um

00:23:18,240 --> 00:23:22,240
and we can select show

00:23:19,440 --> 00:23:22,880
equals false which will allow us to

00:23:22,240 --> 00:23:24,799
suppress

00:23:22,880 --> 00:23:26,880
the show function of yellow brick and

00:23:24,799 --> 00:23:27,679
allow us instead to store the image to

00:23:26,880 --> 00:23:29,919
disk

00:23:27,679 --> 00:23:30,720
we'll call fit on the visualizer passing

00:23:29,919 --> 00:23:32,559
in the train

00:23:30,720 --> 00:23:33,840
data and then we'll score it using the

00:23:32,559 --> 00:23:37,520
test data and

00:23:33,840 --> 00:23:41,120
output the results of that visualization

00:23:37,520 --> 00:23:42,400
to disk in our path and then we will

00:23:41,120 --> 00:23:45,440
also return

00:23:42,400 --> 00:23:47,200
the predictions um from y test which is

00:23:45,440 --> 00:23:48,799
what the original function did

00:23:47,200 --> 00:23:51,039
and so we haven't changed anything in

00:23:48,799 --> 00:23:52,559
the original function except that now we

00:23:51,039 --> 00:23:55,440
also will have

00:23:52,559 --> 00:23:58,240
this image to compare after the fact

00:23:55,440 --> 00:24:01,039
which will support experimentation

00:23:58,240 --> 00:24:01,840
so just a few final tips before we leave

00:24:01,039 --> 00:24:03,600
you

00:24:01,840 --> 00:24:05,679
the first is to leverage a machine

00:24:03,600 --> 00:24:06,880
learning api scikit-learn is one option

00:24:05,679 --> 00:24:07,360
that's the one that we've looked at

00:24:06,880 --> 00:24:09,600
today

00:24:07,360 --> 00:24:11,279
in this talk but there are other options

00:24:09,600 --> 00:24:12,559
so using some kind of open source

00:24:11,279 --> 00:24:14,960
machine learning framework

00:24:12,559 --> 00:24:16,080
will give you the building blocks for

00:24:14,960 --> 00:24:17,520
doing uh

00:24:16,080 --> 00:24:19,919
systematic testing of that machine

00:24:17,520 --> 00:24:21,840
learning code pipelines um serve in a

00:24:19,919 --> 00:24:23,600
similar way they allow you to

00:24:21,840 --> 00:24:26,320
chain the steps together and allow you

00:24:23,600 --> 00:24:28,159
to detect things like the possibility

00:24:26,320 --> 00:24:30,080
that there's leakage happening

00:24:28,159 --> 00:24:31,600
and prevent those from bleeding into

00:24:30,080 --> 00:24:34,320
your code

00:24:31,600 --> 00:24:35,919
you can also drill into fuzziness using

00:24:34,320 --> 00:24:38,080
the excellent tools that come

00:24:35,919 --> 00:24:39,919
in for example the pi test library which

00:24:38,080 --> 00:24:41,520
allows you to run

00:24:39,919 --> 00:24:44,240
tests against multiple different

00:24:41,520 --> 00:24:45,600
parameter options and also allows you to

00:24:44,240 --> 00:24:48,559
tolerate fuzziness

00:24:45,600 --> 00:24:49,679
and approximate results which is really

00:24:48,559 --> 00:24:52,559
useful

00:24:49,679 --> 00:24:54,320
we embrace consistency right so um all

00:24:52,559 --> 00:24:56,400
of the things that aren't about machine

00:24:54,320 --> 00:24:58,480
learning right code formatting

00:24:56,400 --> 00:25:01,039
um you know all those preferences you

00:24:58,480 --> 00:25:03,120
know we come up with a agreed-upon

00:25:01,039 --> 00:25:04,640
uh system in the beginning whether

00:25:03,120 --> 00:25:06,159
that's you know pie flakes whether

00:25:04,640 --> 00:25:06,559
that's black whatever it is you like to

00:25:06,159 --> 00:25:08,000
use

00:25:06,559 --> 00:25:10,080
and we embrace that from the start so

00:25:08,000 --> 00:25:12,720
that we can really focus as a team

00:25:10,080 --> 00:25:14,000
um on looking and assessing the state of

00:25:12,720 --> 00:25:17,840
the machine learning

00:25:14,000 --> 00:25:20,240
code we use ci cd to help us

00:25:17,840 --> 00:25:22,159
identify dependency changes and test

00:25:20,240 --> 00:25:24,799
regression so that we don't have to be

00:25:22,159 --> 00:25:26,240
manually monitoring those and finally we

00:25:24,799 --> 00:25:29,039
use tools like yellow brick

00:25:26,240 --> 00:25:29,440
to help us marry the best of both worlds

00:25:29,039 --> 00:25:32,559
with

00:25:29,440 --> 00:25:34,400
engineering and experimentation

00:25:32,559 --> 00:25:35,600
and that is all we have for you today

00:25:34,400 --> 00:25:38,080
thank you very much

00:25:35,600 --> 00:25:43,840
for coming to our talk and we hope that

00:25:38,080 --> 00:25:43,840
you enjoy the rest of pycon 2021

00:26:41,200 --> 00:26:43,279

YouTube URL: https://www.youtube.com/watch?v=GycRK_K0x2s


