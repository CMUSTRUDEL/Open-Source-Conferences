Title: TALK   Emery Berger   Scalene: A high-performance, high-precision CPU+GPU+memory profiler for Python
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Scalene is a high-performance CPU and memory profiler for Python that does a number of things that other Python profilers do not and cannot do. It runs orders of magnitude faster than other profilers while delivering far more detailed information. This talk will present case studies of using Scalene, and describe some of the technical advances that make it work.

Slides: https://www.cs.umass.edu/~emery/scalene-pycon2021.pdf
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,559 --> 00:00:16,640
hi

00:00:14,960 --> 00:00:17,520
i'm emery berger a professor of computer

00:00:16,640 --> 00:00:18,560
science at the university of

00:00:17,520 --> 00:00:19,920
massachusetts

00:00:18,560 --> 00:00:23,279
today i'll be talking about a new

00:00:19,920 --> 00:00:24,960
profiler for python called scalene

00:00:23,279 --> 00:00:26,480
as you know when you have a performance

00:00:24,960 --> 00:00:27,760
problem or you just want your code to

00:00:26,480 --> 00:00:29,599
run faster

00:00:27,760 --> 00:00:32,559
you reach for a profiler to help you

00:00:29,599 --> 00:00:34,480
identify problems and hopefully fix them

00:00:32,559 --> 00:00:36,559
listed here is a selection of some of

00:00:34,480 --> 00:00:38,160
the most popular profilers

00:00:36,559 --> 00:00:40,160
including the ones that come with python

00:00:38,160 --> 00:00:42,160
by default in fact

00:00:40,160 --> 00:00:43,520
there's such a proliferation of

00:00:42,160 --> 00:00:46,079
profilers

00:00:43,520 --> 00:00:47,360
that one of the profilers is actually

00:00:46,079 --> 00:00:49,440
called yappy

00:00:47,360 --> 00:00:50,719
which stands for yet another python

00:00:49,440 --> 00:00:53,520
profiler

00:00:50,719 --> 00:00:54,239
so today i'll be talking about scalian

00:00:53,520 --> 00:00:56,559
which is

00:00:54,239 --> 00:00:57,920
another profiler but one that's quite

00:00:56,559 --> 00:00:58,640
different from the others so it's not

00:00:57,920 --> 00:01:02,800
really yet

00:00:58,640 --> 00:01:04,479
another profiler as i hope you'll see

00:01:02,800 --> 00:01:05,840
so one thing that you want out of a

00:01:04,479 --> 00:01:08,080
profiler is not

00:01:05,840 --> 00:01:09,760
too much runtime overhead if you're

00:01:08,080 --> 00:01:10,960
profiling a program it's because it's

00:01:09,760 --> 00:01:12,880
already too slow

00:01:10,960 --> 00:01:14,320
so you don't want to get that much

00:01:12,880 --> 00:01:16,240
slower

00:01:14,320 --> 00:01:19,040
so let's see how all of these profilers

00:01:16,240 --> 00:01:20,880
stack up in terms of performance

00:01:19,040 --> 00:01:22,560
to measure this we're going to run them

00:01:20,880 --> 00:01:23,520
on a benchmark from the pi performance

00:01:22,560 --> 00:01:26,080
suite

00:01:23,520 --> 00:01:27,920
on the y-axis of the graph we have

00:01:26,080 --> 00:01:29,600
normalized execution time

00:01:27,920 --> 00:01:31,680
which means we took the time running the

00:01:29,600 --> 00:01:33,119
profiler and divided it by how long it

00:01:31,680 --> 00:01:34,240
took to run the program without

00:01:33,119 --> 00:01:35,960
profiling

00:01:34,240 --> 00:01:40,000
that means that the best case would be

00:01:35,960 --> 00:01:42,560
1.0x meaning no overhead

00:01:40,000 --> 00:01:44,399
so some of the profilers do quite well

00:01:42,560 --> 00:01:47,200
with minimal overhead

00:01:44,399 --> 00:01:48,560
1.0 being as low as you can get and

00:01:47,200 --> 00:01:51,520
these three profilers go

00:01:48,560 --> 00:01:51,920
up to no more than 1.5 x which is pretty

00:01:51,520 --> 00:01:55,119
good

00:01:51,920 --> 00:01:56,560
which is why they're marked in green

00:01:55,119 --> 00:01:59,119
unfortunately some of the other

00:01:56,560 --> 00:02:00,880
profilers are considerably slower

00:01:59,119 --> 00:02:03,200
and these are shown in yellow for

00:02:00,880 --> 00:02:04,799
caution this includes the built-in c

00:02:03,200 --> 00:02:07,040
profile

00:02:04,799 --> 00:02:08,640
and here the slowdowns range from 2x to

00:02:07,040 --> 00:02:11,360
almost 7x

00:02:08,640 --> 00:02:12,080
this isn't great obviously you certainly

00:02:11,360 --> 00:02:14,400
wouldn't want to run

00:02:12,080 --> 00:02:16,239
in production with profiling on and it's

00:02:14,400 --> 00:02:17,120
inconvenient that things are slowed down

00:02:16,239 --> 00:02:20,560
this much

00:02:17,120 --> 00:02:23,040
but maybe it's not a deal breaker

00:02:20,560 --> 00:02:25,360
on the other hand some other profilers

00:02:23,040 --> 00:02:28,400
impose pretty drastic overheads

00:02:25,360 --> 00:02:30,480
up to almost 40x as you can see

00:02:28,400 --> 00:02:32,000
to make this concrete and really feel

00:02:30,480 --> 00:02:34,160
the pain imagine

00:02:32,000 --> 00:02:35,200
profiling a program that takes just one

00:02:34,160 --> 00:02:37,920
minute to run

00:02:35,200 --> 00:02:38,640
but now it takes 40 minutes for most

00:02:37,920 --> 00:02:40,640
people

00:02:38,640 --> 00:02:42,319
this would be an unacceptably high

00:02:40,640 --> 00:02:43,920
overhead which is why

00:02:42,319 --> 00:02:45,519
in the graph they're colored red for

00:02:43,920 --> 00:02:48,879
stop

00:02:45,519 --> 00:02:50,640
but it gets even worse so there is a

00:02:48,879 --> 00:02:52,319
profiler for memory

00:02:50,640 --> 00:02:55,360
called memory profiler and memory

00:02:52,319 --> 00:02:59,360
profiler imposes overheads of almost

00:02:55,360 --> 00:03:02,400
300 x which is pretty extreme really

00:02:59,360 --> 00:03:04,400
an extraordinary amount of slowdown now

00:03:02,400 --> 00:03:05,519
obviously we should put kind of a big

00:03:04,400 --> 00:03:08,720
asterisk here

00:03:05,519 --> 00:03:10,560
by memory profiler because after all

00:03:08,720 --> 00:03:14,159
all these other profilers that i've

00:03:10,560 --> 00:03:17,120
shown before only profile cpu time

00:03:14,159 --> 00:03:18,800
while memory profiler profiles memory so

00:03:17,120 --> 00:03:19,920
maybe it makes sense for it to be more

00:03:18,800 --> 00:03:22,319
costly

00:03:19,920 --> 00:03:22,959
and really a lot more costly but as

00:03:22,319 --> 00:03:25,599
you'll see

00:03:22,959 --> 00:03:27,040
this is not necessarily the case it is

00:03:25,599 --> 00:03:30,080
possible to profile memory

00:03:27,040 --> 00:03:31,519
vastly more efficiently so

00:03:30,080 --> 00:03:33,519
to make the rest of this discussion

00:03:31,519 --> 00:03:36,560
easier we're going to move from a graph

00:03:33,519 --> 00:03:38,480
to a table and then we'll rotate it so

00:03:36,560 --> 00:03:40,799
it's a little bit easier to read

00:03:38,480 --> 00:03:43,680
and so now you may be asking yourself

00:03:40,799 --> 00:03:45,840
where scalene fits in on this graph

00:03:43,680 --> 00:03:46,720
and the answer is scalene is pretty

00:03:45,840 --> 00:03:48,640
efficient

00:03:46,720 --> 00:03:51,280
for this benchmark it's just 20 percent

00:03:48,640 --> 00:03:52,799
slower than the original program

00:03:51,280 --> 00:03:54,400
there are options that let scalene

00:03:52,799 --> 00:03:56,640
impose even less overhead

00:03:54,400 --> 00:03:58,799
but this talk is mostly about the full

00:03:56,640 --> 00:04:01,920
scalene with its default options

00:03:58,799 --> 00:04:04,640
and the full scalene profiles cpu

00:04:01,920 --> 00:04:05,599
profiles memory and it profiles gpu and

00:04:04,640 --> 00:04:09,040
we'll talk about that

00:04:05,599 --> 00:04:11,280
more about that in detail soon

00:04:09,040 --> 00:04:12,400
now profilers typically fall into one of

00:04:11,280 --> 00:04:15,280
two categories

00:04:12,400 --> 00:04:16,720
they profile either by functions or by

00:04:15,280 --> 00:04:18,880
lines

00:04:16,720 --> 00:04:21,359
so what this means is if you have a

00:04:18,880 --> 00:04:23,840
function level profile

00:04:21,359 --> 00:04:25,520
yes it still shows line numbers but only

00:04:23,840 --> 00:04:27,360
the line numbers that correspond to the

00:04:25,520 --> 00:04:29,120
start of the function

00:04:27,360 --> 00:04:31,680
otherwise the rest of the information is

00:04:29,120 --> 00:04:33,600
aggregated over the entire function

00:04:31,680 --> 00:04:35,360
which is fine if you have lots of little

00:04:33,600 --> 00:04:36,960
functions that each don't do very much

00:04:35,360 --> 00:04:39,280
and then the profiler says hey this

00:04:36,960 --> 00:04:41,120
function is taking a lot of time maybe

00:04:39,280 --> 00:04:43,440
you should make it run faster

00:04:41,120 --> 00:04:46,160
however it's really not so great when

00:04:43,440 --> 00:04:47,759
you have long functions

00:04:46,160 --> 00:04:50,000
really once you get over a certain

00:04:47,759 --> 00:04:50,800
length a function level profiler is not

00:04:50,000 --> 00:04:52,720
super helpful

00:04:50,800 --> 00:04:55,199
because it says hey this function is

00:04:52,720 --> 00:04:56,720
slow but you might want more granular

00:04:55,199 --> 00:04:58,560
assistance

00:04:56,720 --> 00:05:00,880
which is where line level profilers come

00:04:58,560 --> 00:05:03,120
in so a line level profiler

00:05:00,880 --> 00:05:04,560
of course reports information for every

00:05:03,120 --> 00:05:06,479
line of code

00:05:04,560 --> 00:05:09,039
which is great when you're profiling

00:05:06,479 --> 00:05:11,039
large functions like i mentioned earlier

00:05:09,039 --> 00:05:13,280
or code that interacts with libraries

00:05:11,039 --> 00:05:15,039
like numpy where each line

00:05:13,280 --> 00:05:17,520
is potentially doing really a lot of

00:05:15,039 --> 00:05:19,520
work or in cases where function calls

00:05:17,520 --> 00:05:22,320
are just obscured because of python

00:05:19,520 --> 00:05:24,320
overloading on the other hand when

00:05:22,320 --> 00:05:27,039
you're profiling a large program

00:05:24,320 --> 00:05:28,080
this might be far too finer granularity

00:05:27,039 --> 00:05:29,919
right profiling

00:05:28,080 --> 00:05:31,360
every single line in a large program

00:05:29,919 --> 00:05:33,199
could really miss the forest for the

00:05:31,360 --> 00:05:35,759
trees

00:05:33,199 --> 00:05:37,039
so you might be asking yourself why not

00:05:35,759 --> 00:05:38,880
both

00:05:37,039 --> 00:05:40,080
and indeed that's the approach that

00:05:38,880 --> 00:05:42,240
scanline takes

00:05:40,080 --> 00:05:45,919
scaling simultaneously performs function

00:05:42,240 --> 00:05:45,919
level and line level profiling

00:05:46,240 --> 00:05:50,880
so that's great there's another

00:05:48,880 --> 00:05:53,600
characteristic of some profilers

00:05:50,880 --> 00:05:54,720
which is you have to change your code to

00:05:53,600 --> 00:05:57,199
make the profiler

00:05:54,720 --> 00:05:59,759
be able to actually profile it now it's

00:05:57,199 --> 00:06:02,560
not an extraordinarily huge change

00:05:59,759 --> 00:06:04,880
you just put at profile decorators on

00:06:02,560 --> 00:06:06,560
functions that you want to profile

00:06:04,880 --> 00:06:08,560
but this is kind of putting the cart

00:06:06,560 --> 00:06:10,720
before the horse it assumes that you

00:06:08,560 --> 00:06:12,639
already know where the problems are

00:06:10,720 --> 00:06:14,880
which is not always the case

00:06:12,639 --> 00:06:17,919
also it's a pain to go in and change the

00:06:14,880 --> 00:06:20,160
program before running the profiler

00:06:17,919 --> 00:06:22,639
so scalene like many of the other

00:06:20,160 --> 00:06:24,160
profilers works on unmodified code

00:06:22,639 --> 00:06:26,800
reporting performance information for

00:06:24,160 --> 00:06:28,800
the whole program but it also supports

00:06:26,800 --> 00:06:30,960
the at profile decorators

00:06:28,800 --> 00:06:32,000
so when you use these in scalene it

00:06:30,960 --> 00:06:34,400
tells scalene

00:06:32,000 --> 00:06:36,240
to focus just on specific functions

00:06:34,400 --> 00:06:37,680
which works best once you already know

00:06:36,240 --> 00:06:39,680
where the problem spots are

00:06:37,680 --> 00:06:42,240
which you can find out without modifying

00:06:39,680 --> 00:06:42,240
any code

00:06:42,560 --> 00:06:47,280
now a number of profilers have

00:06:44,960 --> 00:06:50,000
surprisingly limited support for

00:06:47,280 --> 00:06:51,680
python threads so you can see that

00:06:50,000 --> 00:06:52,720
there's a number of profilers nearly

00:06:51,680 --> 00:06:54,880
half of them

00:06:52,720 --> 00:06:56,400
that actually don't work when you're

00:06:54,880 --> 00:06:58,160
running multi-threaded code

00:06:56,400 --> 00:07:00,319
and what i mean by that is either they

00:06:58,160 --> 00:07:02,400
literally don't work that is they just

00:07:00,319 --> 00:07:06,319
fail to operate entirely

00:07:02,400 --> 00:07:08,400
or they misreport the performance so if

00:07:06,319 --> 00:07:11,120
you have code that's running in a thread

00:07:08,400 --> 00:07:12,319
its runtime might be completely ignored

00:07:11,120 --> 00:07:14,479
to our surprise

00:07:12,319 --> 00:07:16,160
it turns out that no existing profiler

00:07:14,479 --> 00:07:18,319
correctly profiles code

00:07:16,160 --> 00:07:20,400
that uses the multi-processing library

00:07:18,319 --> 00:07:20,880
so scalene turns out to be the only

00:07:20,400 --> 00:07:22,160
option

00:07:20,880 --> 00:07:24,560
if you want to profile such an

00:07:22,160 --> 00:07:26,639
application but these features are not

00:07:24,560 --> 00:07:28,800
the biggest advantages of scaling over

00:07:26,639 --> 00:07:30,960
past profilers

00:07:28,800 --> 00:07:32,960
no pass profiler can do any of the

00:07:30,960 --> 00:07:34,639
things listed here on this slide

00:07:32,960 --> 00:07:36,800
except for one which i'll mention in a

00:07:34,639 --> 00:07:39,680
second uh just to read

00:07:36,800 --> 00:07:42,080
the information across the top scaling

00:07:39,680 --> 00:07:44,560
can separate out python time versus c

00:07:42,080 --> 00:07:45,360
or native time it can identify how much

00:07:44,560 --> 00:07:48,639
time is spent

00:07:45,360 --> 00:07:51,759
in system system calls meaning i o

00:07:48,639 --> 00:07:53,120
it profiles memory uh it profiles a gpu

00:07:51,759 --> 00:07:56,080
if you have one

00:07:53,120 --> 00:07:57,840
it illustrates memory trends and reports

00:07:56,080 --> 00:07:58,560
copy volume which i'll explain in a

00:07:57,840 --> 00:08:00,479
minute

00:07:58,560 --> 00:08:02,720
and it automatically detects memory

00:08:00,479 --> 00:08:05,039
leaks now like i said there is

00:08:02,720 --> 00:08:06,400
one profiler that does one of these

00:08:05,039 --> 00:08:08,400
things

00:08:06,400 --> 00:08:10,080
naturally memory profiler does memory

00:08:08,400 --> 00:08:12,160
profiling but remember

00:08:10,080 --> 00:08:14,720
for this benchmark memory profiler was

00:08:12,160 --> 00:08:17,199
almost 300 times slower

00:08:14,720 --> 00:08:18,720
while scalene ran the same benchmark and

00:08:17,199 --> 00:08:22,160
produced a memory profile

00:08:18,720 --> 00:08:24,400
with only 20 overhead

00:08:22,160 --> 00:08:25,680
all right so in the rest of this talk

00:08:24,400 --> 00:08:27,599
i'm going to talk about how

00:08:25,680 --> 00:08:29,759
scalene is able to profile all of these

00:08:27,599 --> 00:08:30,560
other things which gives you vastly more

00:08:29,759 --> 00:08:32,640
information

00:08:30,560 --> 00:08:33,839
about your python programs and makes

00:08:32,640 --> 00:08:35,440
scalene way better

00:08:33,839 --> 00:08:38,159
at helping you identify and fix

00:08:35,440 --> 00:08:40,399
performance problems

00:08:38,159 --> 00:08:42,000
so scalene is straightforward to use

00:08:40,399 --> 00:08:43,360
you've just replaced python3 with

00:08:42,000 --> 00:08:45,360
scalene

00:08:43,360 --> 00:08:47,920
there are lots of options that let you

00:08:45,360 --> 00:08:51,200
tailor how scalene profiles your code

00:08:47,920 --> 00:08:53,600
dash-help provides the list as is normal

00:08:51,200 --> 00:08:55,279
i will walk through a few really useful

00:08:53,600 --> 00:08:57,920
options that you would want to know when

00:08:55,279 --> 00:09:00,320
you're using scaling

00:08:57,920 --> 00:09:00,959
so one particularly useful option is

00:09:00,320 --> 00:09:03,839
reduced

00:09:00,959 --> 00:09:06,080
profile which only produces profiles for

00:09:03,839 --> 00:09:08,320
lines of code that run for at least one

00:09:06,080 --> 00:09:10,240
percent of overall execution time

00:09:08,320 --> 00:09:11,360
or which allocates some reasonable

00:09:10,240 --> 00:09:13,120
amount of memory

00:09:11,360 --> 00:09:14,560
so i'm not going to show it but we're

00:09:13,120 --> 00:09:17,440
going to be using this option for all

00:09:14,560 --> 00:09:17,440
the next examples

00:09:17,760 --> 00:09:21,519
by default scaling prints its output to

00:09:20,320 --> 00:09:23,279
the console

00:09:21,519 --> 00:09:25,120
but you can also tell scaling to output

00:09:23,279 --> 00:09:27,600
to a file and

00:09:25,120 --> 00:09:28,800
if you want you can tell it to use dash

00:09:27,600 --> 00:09:30,880
html

00:09:28,800 --> 00:09:32,880
which will lead it to produce a web page

00:09:30,880 --> 00:09:36,080
containing its results

00:09:32,880 --> 00:09:37,760
and here is an example profile

00:09:36,080 --> 00:09:40,640
we'll dig into this in more detail in a

00:09:37,760 --> 00:09:43,600
minute but the top part of the profile

00:09:40,640 --> 00:09:44,720
is the line level information with lines

00:09:43,600 --> 00:09:47,120
emitted when they

00:09:44,720 --> 00:09:48,880
have a little contribution to runtime or

00:09:47,120 --> 00:09:51,200
memory consumption

00:09:48,880 --> 00:09:52,480
and at the bottom you get the function

00:09:51,200 --> 00:09:55,120
level profile

00:09:52,480 --> 00:09:57,680
which scalene reports for every single

00:09:55,120 --> 00:09:59,920
profile all the aggregated information

00:09:57,680 --> 00:10:01,200
it also provides a breakdown in

00:09:59,920 --> 00:10:03,040
descending order

00:10:01,200 --> 00:10:04,880
of the lines responsible for the most

00:10:03,040 --> 00:10:06,320
memory consumption so you can look for

00:10:04,880 --> 00:10:07,839
things that might be consuming an

00:10:06,320 --> 00:10:11,120
inordinate amount of memory

00:10:07,839 --> 00:10:13,680
at a glance finally briefly

00:10:11,120 --> 00:10:14,720
scalene also allows you to disable

00:10:13,680 --> 00:10:17,120
memory profiling

00:10:14,720 --> 00:10:18,160
and some other aspects of profiling that

00:10:17,120 --> 00:10:20,880
i'll talk about later

00:10:18,160 --> 00:10:22,880
by saying cpu only this is a bit of a

00:10:20,880 --> 00:10:25,440
misnomer because if you have a gpu

00:10:22,880 --> 00:10:27,839
scaling will still do gpu profiling but

00:10:25,440 --> 00:10:31,680
both of these are extremely efficient

00:10:27,839 --> 00:10:34,480
and effectively impose zero overhead

00:10:31,680 --> 00:10:36,320
so when you go ahead and you run scaling

00:10:34,480 --> 00:10:39,760
with cpu only you can see that

00:10:36,320 --> 00:10:41,360
there's far fewer columns of information

00:10:39,760 --> 00:10:45,040
that are reported

00:10:41,360 --> 00:10:47,600
and and then you just focus on cpu time

00:10:45,040 --> 00:10:50,079
or potentially gpu time

00:10:47,600 --> 00:10:50,720
okay so let's walk through a simple

00:10:50,079 --> 00:10:52,480
example

00:10:50,720 --> 00:10:54,640
of how scalene's holistic approach to

00:10:52,480 --> 00:10:55,600
profiling can root out performance

00:10:54,640 --> 00:11:00,079
problems

00:10:55,600 --> 00:11:02,480
so here is some code that uses numpy

00:11:00,079 --> 00:11:03,920
and we'll focus in on the on this so

00:11:02,480 --> 00:11:05,920
don't worry about it right now

00:11:03,920 --> 00:11:07,200
but here's what a c profile profile

00:11:05,920 --> 00:11:10,160
looks like so

00:11:07,200 --> 00:11:12,000
this is a function level profile and it

00:11:10,160 --> 00:11:12,880
may be hard to see but what it tells us

00:11:12,000 --> 00:11:15,360
is that main

00:11:12,880 --> 00:11:17,760
consumes all the runtime which is not

00:11:15,360 --> 00:11:19,920
super helpful

00:11:17,760 --> 00:11:20,959
by contrast here's a profile from line

00:11:19,920 --> 00:11:22,720
profiler

00:11:20,959 --> 00:11:24,800
and this of course tells us how much l

00:11:22,720 --> 00:11:25,760
time is being spent on each of the lines

00:11:24,800 --> 00:11:27,680
of main

00:11:25,760 --> 00:11:29,120
but it's not particularly actionable it

00:11:27,680 --> 00:11:30,160
doesn't really give us much of a clue as

00:11:29,120 --> 00:11:32,640
to what's going on

00:11:30,160 --> 00:11:35,040
or why there would be a slowdown or a

00:11:32,640 --> 00:11:37,680
particular line of code

00:11:35,040 --> 00:11:39,440
so here is scalene's profile i'm

00:11:37,680 --> 00:11:42,959
omitting the function level profile and

00:11:39,440 --> 00:11:45,680
memory consumption summary for now

00:11:42,959 --> 00:11:47,519
so rather than just saying how much time

00:11:45,680 --> 00:11:50,320
was spent in each line of code

00:11:47,519 --> 00:11:52,639
scaling breaks down execution time into

00:11:50,320 --> 00:11:54,720
time spent running python code

00:11:52,639 --> 00:11:56,240
time spent running native code which

00:11:54,720 --> 00:11:58,959
means c libraries

00:11:56,240 --> 00:12:01,760
like numpy and system time which

00:11:58,959 --> 00:12:04,959
includes time spent doing i o

00:12:01,760 --> 00:12:06,480
usually as a python programmer if you're

00:12:04,959 --> 00:12:08,800
trying to make a program

00:12:06,480 --> 00:12:10,240
much more efficient your goal is to move

00:12:08,800 --> 00:12:12,560
execution time

00:12:10,240 --> 00:12:14,240
out of the python interpreter and into

00:12:12,560 --> 00:12:17,120
native libraries

00:12:14,240 --> 00:12:19,040
right now it's clear that the code is

00:12:17,120 --> 00:12:21,040
actually doing just that that is

00:12:19,040 --> 00:12:22,720
most of the time is being spent running

00:12:21,040 --> 00:12:24,480
native code

00:12:22,720 --> 00:12:26,959
so if that was all the information you

00:12:24,480 --> 00:12:27,920
had you might reasonably conclude that

00:12:26,959 --> 00:12:30,480
there is really no

00:12:27,920 --> 00:12:31,519
optimization opportunity here it's all

00:12:30,480 --> 00:12:35,040
running in native

00:12:31,519 --> 00:12:35,040
this is as good as you could hope for

00:12:35,600 --> 00:12:40,320
but scalene provides more information so

00:12:38,560 --> 00:12:42,160
in addition to cpu time as

00:12:40,320 --> 00:12:44,399
has been mentioned it produces memory

00:12:42,160 --> 00:12:46,000
profiles like memory profiler

00:12:44,399 --> 00:12:48,399
but it breaks down how much of the

00:12:46,000 --> 00:12:50,480
memory consumption was from python

00:12:48,399 --> 00:12:52,240
and how much was native and where it's

00:12:50,480 --> 00:12:54,560
blank it means that all of the memory

00:12:52,240 --> 00:12:56,959
consumed was in native code

00:12:54,560 --> 00:12:58,320
and so here you can see there's a lot of

00:12:56,959 --> 00:13:03,519
memory consumption happening

00:12:58,320 --> 00:13:05,680
in native code in line six in particular

00:13:03,519 --> 00:13:07,600
in addition to profiling overall memory

00:13:05,680 --> 00:13:08,800
consumption scalene profiles memory

00:13:07,600 --> 00:13:11,920
usage trends

00:13:08,800 --> 00:13:13,920
over time so it illustrates these with

00:13:11,920 --> 00:13:15,519
these things called sparklines and a

00:13:13,920 --> 00:13:18,880
sparkline you can think of

00:13:15,519 --> 00:13:21,120
as an inline graph so the x-axis is time

00:13:18,880 --> 00:13:23,360
and the y-axis is memory consumption and

00:13:21,120 --> 00:13:26,560
each line has its own memory trend

00:13:23,360 --> 00:13:28,800
sparkline finally

00:13:26,560 --> 00:13:30,160
scalene reports a novel metric that we

00:13:28,800 --> 00:13:31,600
call copy volume

00:13:30,160 --> 00:13:34,079
and this is meant to capture several

00:13:31,600 --> 00:13:34,959
performance problems so copying can be

00:13:34,079 --> 00:13:37,200
quite expensive

00:13:34,959 --> 00:13:39,040
and it's often indicative of a problem

00:13:37,200 --> 00:13:41,760
like accidentally converting between

00:13:39,040 --> 00:13:43,839
native and python data structures

00:13:41,760 --> 00:13:45,199
and as i mentioned before scalene also

00:13:43,839 --> 00:13:47,519
reports gpu time

00:13:45,199 --> 00:13:48,480
but we didn't run this on a gpu so that

00:13:47,519 --> 00:13:50,959
would all be

00:13:48,480 --> 00:13:52,320
uh there's there would be no reports and

00:13:50,959 --> 00:13:55,600
in this case

00:13:52,320 --> 00:13:58,959
regardless numpy doesn't use the gpu

00:13:55,600 --> 00:13:59,839
so it's immaterial all right so let's

00:13:58,959 --> 00:14:01,680
dig in

00:13:59,839 --> 00:14:04,880
let's look at this line six which seems

00:14:01,680 --> 00:14:07,199
to be where all the action is happening

00:14:04,880 --> 00:14:09,120
this one line is responsible for 34

00:14:07,199 --> 00:14:10,959
percent of execution time

00:14:09,120 --> 00:14:13,120
it's all in native code which like i

00:14:10,959 --> 00:14:15,360
said does not necessarily make it a good

00:14:13,120 --> 00:14:17,760
candidate for optimization

00:14:15,360 --> 00:14:18,800
but the native code is also allocating a

00:14:17,760 --> 00:14:22,720
lot of memory

00:14:18,800 --> 00:14:25,199
again that maybe seems reasonable

00:14:22,720 --> 00:14:26,639
it's really a lot it's 87 percent of all

00:14:25,199 --> 00:14:29,440
memory activity

00:14:26,639 --> 00:14:31,120
but the memory trend is interesting it

00:14:29,440 --> 00:14:33,279
exhibits this sawtooth pattern

00:14:31,120 --> 00:14:35,120
it goes up and it goes down and this

00:14:33,279 --> 00:14:36,240
indicates that it allocated a big chunk

00:14:35,120 --> 00:14:38,560
of memory

00:14:36,240 --> 00:14:40,800
allocated what looks at a glance to be

00:14:38,560 --> 00:14:43,519
the same amount of memory

00:14:40,800 --> 00:14:44,720
which it is and then freed it so this is

00:14:43,519 --> 00:14:47,040
a big clue

00:14:44,720 --> 00:14:47,760
and this is confirmed by the copy volume

00:14:47,040 --> 00:14:49,760
number

00:14:47,760 --> 00:14:51,920
the copy volume indicates that there's a

00:14:49,760 --> 00:14:54,800
ton of copying happening

00:14:51,920 --> 00:14:55,800
so what's going on so if we look at this

00:14:54,800 --> 00:14:59,279
code

00:14:55,800 --> 00:15:00,480
np.array is a numpy library call that

00:14:59,279 --> 00:15:02,720
converts its argument

00:15:00,480 --> 00:15:03,519
into a numpy array which really means a

00:15:02,720 --> 00:15:07,199
native

00:15:03,519 --> 00:15:09,920
c or fortran array but in this case

00:15:07,199 --> 00:15:12,480
the argument which is the return value

00:15:09,920 --> 00:15:15,360
of np.random.uniform

00:15:12,480 --> 00:15:16,160
is already a numpy array and

00:15:15,360 --> 00:15:19,440
unfortunately

00:15:16,160 --> 00:15:20,399
by default np.array makes a copy of its

00:15:19,440 --> 00:15:22,240
input

00:15:20,399 --> 00:15:24,560
and in this case that copy is entirely

00:15:22,240 --> 00:15:26,639
unnecessary so we can easily speed up

00:15:24,560 --> 00:15:28,959
this program quite drastically

00:15:26,639 --> 00:15:31,839
so what we're going to do is we're going

00:15:28,959 --> 00:15:33,360
to change this one line of code slightly

00:15:31,839 --> 00:15:35,120
on the bottom we have the optimized

00:15:33,360 --> 00:15:37,199
version and if you look

00:15:35,120 --> 00:15:38,720
you can see that the only difference

00:15:37,199 --> 00:15:40,800
between these two versions

00:15:38,720 --> 00:15:43,040
is that we got rid of the redundant np

00:15:40,800 --> 00:15:44,720
dot array call

00:15:43,040 --> 00:15:46,639
and we can immediately see that this has

00:15:44,720 --> 00:15:48,560
the desired result

00:15:46,639 --> 00:15:50,800
cpu time has dropped because we're no

00:15:48,560 --> 00:15:52,639
longer copying as much

00:15:50,800 --> 00:15:54,000
but more importantly the sawtooth

00:15:52,639 --> 00:15:56,399
pattern in the memory trend

00:15:54,000 --> 00:15:57,600
is now gone and the copy volume has gone

00:15:56,399 --> 00:15:59,759
to zero

00:15:57,600 --> 00:16:01,759
so our modification worked we can of

00:15:59,759 --> 00:16:03,920
course verify by testing that we get the

00:16:01,759 --> 00:16:07,120
same results

00:16:03,920 --> 00:16:08,800
this one change that scalene effectively

00:16:07,120 --> 00:16:11,440
directly led us to

00:16:08,800 --> 00:16:12,800
dropped peak memory usage from 1.6

00:16:11,440 --> 00:16:15,839
gigabytes

00:16:12,800 --> 00:16:17,759
to about 900 megs

00:16:15,839 --> 00:16:19,680
and not only did it reduce maximum

00:16:17,759 --> 00:16:22,399
memory consumption almost by half

00:16:19,680 --> 00:16:23,120
but it also cut 15 off total execution

00:16:22,399 --> 00:16:26,240
time

00:16:23,120 --> 00:16:27,680
so not bad for a day's work so

00:16:26,240 --> 00:16:29,680
in the rest of the talk i'm just going

00:16:27,680 --> 00:16:32,240
to talk about some technical challenges

00:16:29,680 --> 00:16:34,160
on how scalene achieves its precision

00:16:32,240 --> 00:16:36,720
and unprecedented level of detail

00:16:34,160 --> 00:16:38,560
while maintaining low overhead and one

00:16:36,720 --> 00:16:40,720
of the most pernicious challenges is how

00:16:38,560 --> 00:16:43,839
python handles signals

00:16:40,720 --> 00:16:46,800
scaling relies on signals to track

00:16:43,839 --> 00:16:48,079
many aspects of performance whenever a

00:16:46,800 --> 00:16:49,600
timer goes off

00:16:48,079 --> 00:16:51,600
scalene looks to see what line of code

00:16:49,600 --> 00:16:53,199
is currently executing and it also

00:16:51,600 --> 00:16:55,199
measures the load on the gpu

00:16:53,199 --> 00:16:56,480
and over time this very accurately

00:16:55,199 --> 00:16:59,040
predicts which lines of code are

00:16:56,480 --> 00:17:01,600
consuming the most time

00:16:59,040 --> 00:17:02,480
now the way that delivery works in

00:17:01,600 --> 00:17:05,039
python

00:17:02,480 --> 00:17:05,520
is that if you're running python code

00:17:05,039 --> 00:17:07,839
the

00:17:05,520 --> 00:17:09,199
the delivery of signals happens promptly

00:17:07,839 --> 00:17:11,280
so if you say

00:17:09,199 --> 00:17:12,559
i want a timer interrupt to go off every

00:17:11,280 --> 00:17:14,720
0.01 seconds

00:17:12,559 --> 00:17:16,559
it pretty much will happen every 0.01

00:17:14,720 --> 00:17:17,199
seconds when you're running python byte

00:17:16,559 --> 00:17:19,760
codes

00:17:17,199 --> 00:17:20,559
most of the time because python delivers

00:17:19,760 --> 00:17:23,039
signals

00:17:20,559 --> 00:17:24,720
right after it executes one byte code in

00:17:23,039 --> 00:17:27,919
its interpreter

00:17:24,720 --> 00:17:30,240
however if you are running native code

00:17:27,919 --> 00:17:31,679
so if one of these bytecodes actually is

00:17:30,240 --> 00:17:34,960
a call to a native function

00:17:31,679 --> 00:17:37,679
like a numpy call for example the whole

00:17:34,960 --> 00:17:38,640
execution time will have no signals

00:17:37,679 --> 00:17:42,080
delivered

00:17:38,640 --> 00:17:44,400
until python regains control

00:17:42,080 --> 00:17:46,480
so in effect it's as if from the

00:17:44,400 --> 00:17:49,120
perspective of a sampling profiler

00:17:46,480 --> 00:17:49,919
that no time elapsed whatsoever if

00:17:49,120 --> 00:17:52,400
scalene

00:17:49,919 --> 00:17:52,960
just relied on timer signals alone it

00:17:52,400 --> 00:17:55,280
would

00:17:52,960 --> 00:17:56,400
misreport where programs are spending

00:17:55,280 --> 00:17:59,440
their time

00:17:56,400 --> 00:18:00,240
this would really degrade the quality of

00:17:59,440 --> 00:18:04,080
information

00:18:00,240 --> 00:18:07,200
from the perspective of a scalene user

00:18:04,080 --> 00:18:09,280
to combat this scalene uses an algorithm

00:18:07,200 --> 00:18:10,480
that infers how much time was spent in

00:18:09,280 --> 00:18:12,000
native code

00:18:10,480 --> 00:18:13,840
it does this by checking the current

00:18:12,000 --> 00:18:15,440
time and here we're looking at the

00:18:13,840 --> 00:18:18,640
so-called virtual clock

00:18:15,440 --> 00:18:20,160
that is to say the time spent when the

00:18:18,640 --> 00:18:21,760
process is actually scheduled for

00:18:20,160 --> 00:18:23,360
execution

00:18:21,760 --> 00:18:25,520
and it checks the current time according

00:18:23,360 --> 00:18:28,160
to that clock

00:18:25,520 --> 00:18:28,960
then when it gets another signal it

00:18:28,160 --> 00:18:31,760
checks again

00:18:28,960 --> 00:18:32,400
to see what the time was and it turns

00:18:31,760 --> 00:18:35,360
out

00:18:32,400 --> 00:18:36,160
that you can then actually accurately

00:18:35,360 --> 00:18:38,000
say

00:18:36,160 --> 00:18:40,160
how much time was spent in python and

00:18:38,000 --> 00:18:43,200
how much time was spent in native code

00:18:40,160 --> 00:18:43,760
intuitively this makes sense if the

00:18:43,200 --> 00:18:46,880
program

00:18:43,760 --> 00:18:49,440
was delayed beyond the expected interval

00:18:46,880 --> 00:18:49,919
so the interval was say 0.01 seconds and

00:18:49,440 --> 00:18:53,360
it took

00:18:49,919 --> 00:18:55,919
10 seconds we kind of know where those

00:18:53,360 --> 00:18:56,480
10 seconds minus 0.01 seconds went they

00:18:55,919 --> 00:18:58,799
went to c

00:18:56,480 --> 00:19:00,000
code and so there's a little formula

00:18:58,799 --> 00:19:02,080
here that

00:19:00,000 --> 00:19:04,320
we can show leads to an accurate

00:19:02,080 --> 00:19:06,000
prediction of the amount of time spent

00:19:04,320 --> 00:19:08,400
in python code and its c code

00:19:06,000 --> 00:19:09,280
and i'll show you this in just a second

00:19:08,400 --> 00:19:11,360
in addition

00:19:09,280 --> 00:19:13,280
we can rely on the same insight to

00:19:11,360 --> 00:19:14,559
separate out system time from total

00:19:13,280 --> 00:19:17,440
execution time

00:19:14,559 --> 00:19:18,400
so here scalene records the actual wall

00:19:17,440 --> 00:19:21,039
clock time

00:19:18,400 --> 00:19:23,039
which includes any time spent on the cpu

00:19:21,039 --> 00:19:24,640
or sleeping because of i o

00:19:23,039 --> 00:19:26,559
and this lets scalene compute how much

00:19:24,640 --> 00:19:29,039
time was spent in system time for each

00:19:26,559 --> 00:19:30,799
line of code

00:19:29,039 --> 00:19:32,720
so here i have an example that

00:19:30,799 --> 00:19:35,120
demonstrates scanline's ability

00:19:32,720 --> 00:19:37,280
to tease apart native code execution

00:19:35,120 --> 00:19:39,440
time and python time

00:19:37,280 --> 00:19:42,559
this particular program is designed to

00:19:39,440 --> 00:19:44,480
spend 0.7 seconds in a native extension

00:19:42,559 --> 00:19:46,720
that we wrote specifically for this

00:19:44,480 --> 00:19:49,919
purpose and 0.3 seconds

00:19:46,720 --> 00:19:51,679
inside of python and scalene

00:19:49,919 --> 00:19:53,919
accurately determines that the python

00:19:51,679 --> 00:19:55,200
code is consuming roughly thirty percent

00:19:53,919 --> 00:19:56,960
of the execution time

00:19:55,200 --> 00:19:58,240
and the c code roughly seventy percent

00:19:56,960 --> 00:20:01,280
of the time um

00:19:58,240 --> 00:20:03,280
this despite the fact that the sampling

00:20:01,280 --> 00:20:04,880
is not being delivered right the timer

00:20:03,280 --> 00:20:06,000
signals are not being delivered during

00:20:04,880 --> 00:20:08,559
the entire time the c

00:20:06,000 --> 00:20:12,159
code is running it's quite accurate the

00:20:08,559 --> 00:20:12,159
error here is just one percent

00:20:12,400 --> 00:20:15,840
so we're going to look at two more

00:20:13,760 --> 00:20:18,960
features of scaling quickly

00:20:15,840 --> 00:20:20,799
one is gpu profiling here is

00:20:18,960 --> 00:20:22,480
uh an image of scalene running in a

00:20:20,799 --> 00:20:25,600
jupiter notebook on a system

00:20:22,480 --> 00:20:27,600
that has gpus so scalene does not

00:20:25,600 --> 00:20:29,440
yet do full profiling on jupiter

00:20:27,600 --> 00:20:31,840
notebooks but we're working on it

00:20:29,440 --> 00:20:34,240
um it does cpu profiling and gpu

00:20:31,840 --> 00:20:37,280
profiling but not the memory profiling

00:20:34,240 --> 00:20:38,880
so to profile a program with scalene as

00:20:37,280 --> 00:20:40,799
usual you pip install it

00:20:38,880 --> 00:20:43,039
and then in a jupyter notebook you load

00:20:40,799 --> 00:20:45,120
it with percent load xd

00:20:43,039 --> 00:20:46,799
and then you finally run it with percent

00:20:45,120 --> 00:20:49,600
sc run

00:20:46,799 --> 00:20:51,919
and here's what a profile looks like if

00:20:49,600 --> 00:20:52,559
scalene detects that there is an nvidia

00:20:51,919 --> 00:20:54,720
gpu

00:20:52,559 --> 00:20:56,080
on the system it will automatically do

00:20:54,720 --> 00:20:57,919
gpu profiling

00:20:56,080 --> 00:20:59,280
and you can see the results here this is

00:20:57,919 --> 00:21:00,799
on a program that uses pi

00:20:59,280 --> 00:21:03,120
torch which does take advantage of

00:21:00,799 --> 00:21:05,440
available gpus if you tell it to

00:21:03,120 --> 00:21:07,520
and you can see in addition to profiling

00:21:05,440 --> 00:21:09,600
python native and system time

00:21:07,520 --> 00:21:11,520
scaling is now showing in the final

00:21:09,600 --> 00:21:14,640
column right before the source code

00:21:11,520 --> 00:21:17,520
gpu time also is a percent and scalene

00:21:14,640 --> 00:21:18,400
implements gpu profiling using the timer

00:21:17,520 --> 00:21:20,720
driven sampling

00:21:18,400 --> 00:21:21,919
just as with the cpu time it measures

00:21:20,720 --> 00:21:23,280
the load on the gpu

00:21:21,919 --> 00:21:25,840
and attributes it to the currently

00:21:23,280 --> 00:21:28,880
running line of code and over time again

00:21:25,840 --> 00:21:32,400
this delivers a precise and accurate uh

00:21:28,880 --> 00:21:32,400
reporting of the time spent

00:21:32,720 --> 00:21:36,720
so now let's talk finally about how

00:21:34,880 --> 00:21:38,320
scalene performs automatic memory leak

00:21:36,720 --> 00:21:40,960
detection

00:21:38,320 --> 00:21:42,080
and at a high level imagine that for

00:21:40,960 --> 00:21:44,320
every line

00:21:42,080 --> 00:21:45,760
every time a line of python code

00:21:44,320 --> 00:21:49,200
allocates some memory

00:21:45,760 --> 00:21:51,679
we say that it got heads on a coin flip

00:21:49,200 --> 00:21:53,679
and every time that that memory is freed

00:21:51,679 --> 00:21:57,840
we call it tails

00:21:53,679 --> 00:22:00,240
so if that line of code is not leaking

00:21:57,840 --> 00:22:01,120
every single time it allocates memory

00:22:00,240 --> 00:22:03,039
eventually

00:22:01,120 --> 00:22:04,720
there's going to be a matching free when

00:22:03,039 --> 00:22:07,679
it reclaims that memory

00:22:04,720 --> 00:22:08,400
and so over time you'll get a bunch of

00:22:07,679 --> 00:22:10,880
heads

00:22:08,400 --> 00:22:12,320
and a bunch of tails and if they balance

00:22:10,880 --> 00:22:14,159
out then we can

00:22:12,320 --> 00:22:16,480
conclude quite reasonably that there is

00:22:14,159 --> 00:22:19,280
no memory leak

00:22:16,480 --> 00:22:20,240
if however these allocations don't get

00:22:19,280 --> 00:22:22,799
paired up

00:22:20,240 --> 00:22:25,200
with any freeze and we just get

00:22:22,799 --> 00:22:27,360
allocations allocations allocations

00:22:25,200 --> 00:22:30,799
you can conclude quite safely that you

00:22:27,360 --> 00:22:33,200
do have a memory leak on your hands

00:22:30,799 --> 00:22:35,200
so tracking this for every single

00:22:33,200 --> 00:22:37,760
allocation performed by python

00:22:35,200 --> 00:22:39,200
or a native library and then associating

00:22:37,760 --> 00:22:41,440
it with every line of code

00:22:39,200 --> 00:22:43,200
and then checking every free to see

00:22:41,440 --> 00:22:44,799
which things were allocated would be

00:22:43,200 --> 00:22:47,520
very very costly

00:22:44,799 --> 00:22:48,960
so instead as usual scalene does this

00:22:47,520 --> 00:22:51,280
via sampling

00:22:48,960 --> 00:22:52,640
so scalene randomly chooses allocations

00:22:51,280 --> 00:22:54,559
to track

00:22:52,640 --> 00:22:56,080
it records the address of the allocated

00:22:54,559 --> 00:22:58,559
object and then

00:22:56,080 --> 00:23:00,480
on every free it does a quick comparison

00:22:58,559 --> 00:23:02,159
to see if that same object was just

00:23:00,480 --> 00:23:04,960
freed

00:23:02,159 --> 00:23:06,080
if over and over again allocated objects

00:23:04,960 --> 00:23:07,760
are not freed

00:23:06,080 --> 00:23:09,440
then we can conclude that the line is

00:23:07,760 --> 00:23:12,159
leaking memory

00:23:09,440 --> 00:23:12,880
so conceptually scalene assumes that all

00:23:12,159 --> 00:23:14,480
lines of code

00:23:12,880 --> 00:23:16,720
start with an equal number of

00:23:14,480 --> 00:23:19,840
allocations and freeze right a one

00:23:16,720 --> 00:23:22,000
heads and one tails meaning no leaks it

00:23:19,840 --> 00:23:23,440
then gradually builds up information as

00:23:22,000 --> 00:23:25,520
the program is running

00:23:23,440 --> 00:23:27,440
sampling allocations and recording

00:23:25,520 --> 00:23:29,840
whether they were freed

00:23:27,440 --> 00:23:31,200
and over time we will see a pattern that

00:23:29,840 --> 00:23:34,240
will start to emerge

00:23:31,200 --> 00:23:36,480
and after we have enough data

00:23:34,240 --> 00:23:37,440
we can conclude when the allocations are

00:23:36,480 --> 00:23:39,360
roughly balanced

00:23:37,440 --> 00:23:42,240
there's no leak and when they're quite

00:23:39,360 --> 00:23:44,159
unbalanced it indicates a leak

00:23:42,240 --> 00:23:45,840
and when scalene concludes with high

00:23:44,159 --> 00:23:47,919
probability that there is a leak

00:23:45,840 --> 00:23:50,080
it reports the probability and the

00:23:47,919 --> 00:23:51,840
volume in megabytes per second which

00:23:50,080 --> 00:23:54,159
indicates how serious the leak is

00:23:51,840 --> 00:23:55,679
which lets you as the programmer focus

00:23:54,159 --> 00:23:57,360
your effort on the leaks that really

00:23:55,679 --> 00:23:59,360
matter

00:23:57,360 --> 00:24:01,039
so in conclusion i hope i've convinced

00:23:59,360 --> 00:24:01,840
you that scalene is not yet another

00:24:01,039 --> 00:24:04,000
profiler

00:24:01,840 --> 00:24:04,880
it's a very different profiler it's

00:24:04,000 --> 00:24:07,440
precise

00:24:04,880 --> 00:24:08,080
providing unprecedented levels of detail

00:24:07,440 --> 00:24:10,960
for

00:24:08,080 --> 00:24:11,760
cpu time while simultaneously profiling

00:24:10,960 --> 00:24:14,320
memory

00:24:11,760 --> 00:24:15,919
gpu utilization copy volume and

00:24:14,320 --> 00:24:18,720
identifying leaks

00:24:15,919 --> 00:24:20,799
it's reasonably fast it's accurate

00:24:18,720 --> 00:24:23,279
correctly attributing time for code

00:24:20,799 --> 00:24:25,200
that uses threads for multi-processing

00:24:23,279 --> 00:24:27,279
and finally it's easy to install

00:24:25,200 --> 00:24:28,559
it works on linux mac and jupyter

00:24:27,279 --> 00:24:30,720
notebooks

00:24:28,559 --> 00:24:32,000
we look forward to feedback and success

00:24:30,720 --> 00:24:34,240
stories of course

00:24:32,000 --> 00:24:35,919
on using scalene to identify and fix

00:24:34,240 --> 00:24:47,840
performance problems in your code

00:24:35,919 --> 00:24:47,840
and thank you for your attention

00:25:39,840 --> 00:25:41,919

YouTube URL: https://www.youtube.com/watch?v=nrQPqy3YY5A


