Title: TALK   Adam Breindel   Dask-SQL: Empowering Pythonistas for Scalable End-to-End Data Engineering
Publication date: 2021-05-30
Playlist: PyCon US 2021
Description: 
	Few things are more frustrating -- or inefficient -- than having a team of brilliant Python folks get stuck at the initial "get the data" stage of a project, because that data is "trapped" in a Hive/Spark-based datalake or requires complex SQL queries to assemble. Let's get unstuck, with dask-sql!

PyData tooling and Dask are immensely popular in data pipelines, but the beginning stages of those pipelines -- often involving SQL data extraction from enterprise datalakes -- have traditionally required Java/JVM-based tools, such as Apache Spark.

That changed in the past year, with the release of dask-sql. Dask-sql empowers Pythonistas with little or no knowledge of the JVM/Hadoop world to create end-to-end data projects.

In this talk, we'll explore how we can use Python and dask-sql to perform SQL data/feature extraction from datalakes and Hive tables. We'll see how we can immediately refine and use that data for machine learning, analytics, or transformation workloads with our favorite PyData tools.

We'll also discuss the design of dask-sql: an innovative project that combines battle-tested SQL optimization from Apache Calcite, scalable dataframe operations via Dask, and integration to the enterprise-standard Hive metastore data catalog.

Slides: https://github.com/adbreind/pycon2021-dask-sql
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,880 --> 00:00:18,240
welcome to pycon

00:00:16,080 --> 00:00:20,800
and thank you for coming to my talk on

00:00:18,240 --> 00:00:21,520
das sql empowering pythonistas for

00:00:20,800 --> 00:00:23,840
scalable

00:00:21,520 --> 00:00:25,680
end-to-end data engineering and data

00:00:23,840 --> 00:00:28,080
science

00:00:25,680 --> 00:00:29,439
my name is adam brindell i look a little

00:00:28,080 --> 00:00:31,840
bit like this

00:00:29,439 --> 00:00:32,880
at least before kovid gave me an even

00:00:31,840 --> 00:00:35,840
worse haircut

00:00:32,880 --> 00:00:36,960
i looked like that i have some contact

00:00:35,840 --> 00:00:39,040
information

00:00:36,960 --> 00:00:40,559
in the presentation here you're welcome

00:00:39,040 --> 00:00:44,399
to connect to me

00:00:40,559 --> 00:00:45,120
and i work as training lead at coiled

00:00:44,399 --> 00:00:48,399
computing

00:00:45,120 --> 00:00:51,840
a desk in the cloud startup

00:00:48,399 --> 00:00:54,079
focusing on making it easy to scale desk

00:00:51,840 --> 00:00:56,399
i also do some work on data engineering

00:00:54,079 --> 00:00:58,399
and machine learning as a consultant

00:00:56,399 --> 00:01:00,960
do some software development and a few

00:00:58,399 --> 00:01:04,239
other things i've been doing this for a

00:01:00,960 --> 00:01:05,840
ridiculously long time uh over 20 years

00:01:04,239 --> 00:01:07,680
and i have a little bit of other

00:01:05,840 --> 00:01:09,600
information about some of the things

00:01:07,680 --> 00:01:11,600
i've worked on before but i know we're

00:01:09,600 --> 00:01:14,560
all here to learn about desk and sql and

00:01:11,600 --> 00:01:16,960
not so much about me so i want to get

00:01:14,560 --> 00:01:17,759
right into the really exciting topic

00:01:16,960 --> 00:01:21,680
here

00:01:17,759 --> 00:01:25,119
so um first just to kind of

00:01:21,680 --> 00:01:26,720
uh orient everyone in case you're coming

00:01:25,119 --> 00:01:29,200
from one of the many other

00:01:26,720 --> 00:01:30,320
corners of the python world and not the

00:01:29,200 --> 00:01:33,119
large-scale

00:01:30,320 --> 00:01:34,000
data engineering corner uh what i've got

00:01:33,119 --> 00:01:37,520
on this

00:01:34,000 --> 00:01:39,759
picture here is a sort of simplified

00:01:37,520 --> 00:01:40,799
model of a typical large-scale

00:01:39,759 --> 00:01:44,399
enterprise

00:01:40,799 --> 00:01:46,240
data processing pipeline so this might

00:01:44,399 --> 00:01:47,759
involve lots of different tools so it's

00:01:46,240 --> 00:01:49,119
not a single program

00:01:47,759 --> 00:01:51,360
this is sort of a high level

00:01:49,119 --> 00:01:53,360
architecture and the idea is that over

00:01:51,360 --> 00:01:55,840
on the left hand side

00:01:53,360 --> 00:01:56,399
we have a bunch of data typically

00:01:55,840 --> 00:01:58,960
sitting

00:01:56,399 --> 00:02:00,719
in some kind of a data lake although

00:01:58,960 --> 00:02:02,479
there's a few other options

00:02:00,719 --> 00:02:04,079
and we want to do things with that data

00:02:02,479 --> 00:02:05,759
we need to pull it out so we need to

00:02:04,079 --> 00:02:07,280
kind of locate it

00:02:05,759 --> 00:02:09,360
pull out that data figure out how to

00:02:07,280 --> 00:02:11,840
process it usually it's not

00:02:09,360 --> 00:02:13,680
exactly in the form that we want even if

00:02:11,840 --> 00:02:14,560
it's in the format we want we usually

00:02:13,680 --> 00:02:16,000
need to

00:02:14,560 --> 00:02:18,400
mess around with that a little bit so

00:02:16,000 --> 00:02:21,360
there's this transform step

00:02:18,400 --> 00:02:23,200
so if you're thinking about producing a

00:02:21,360 --> 00:02:24,959
report that might be kind of

00:02:23,200 --> 00:02:27,200
transforming and aggregating and

00:02:24,959 --> 00:02:28,640
rearranging the data to get closer to

00:02:27,200 --> 00:02:29,920
the business information you're trying

00:02:28,640 --> 00:02:32,160
to report on

00:02:29,920 --> 00:02:34,000
if you're on the machine learning side

00:02:32,160 --> 00:02:37,040
of things this might be

00:02:34,000 --> 00:02:40,480
creating customized features or

00:02:37,040 --> 00:02:41,599
standardizing and cleaning up features

00:02:40,480 --> 00:02:43,120
because there are a lot of things that

00:02:41,599 --> 00:02:44,879
we might be doing in there that get us

00:02:43,120 --> 00:02:46,959
from raw data to some data that's a

00:02:44,879 --> 00:02:49,200
little bit more usable

00:02:46,959 --> 00:02:51,519
for whatever the next step is in our

00:02:49,200 --> 00:02:52,959
project so sometimes after we rearrange

00:02:51,519 --> 00:02:55,200
that data and

00:02:52,959 --> 00:02:57,040
transform it and clean it up and merge

00:02:55,200 --> 00:02:58,959
different files and all of that we

00:02:57,040 --> 00:03:00,319
write it back out so that's kind of this

00:02:58,959 --> 00:03:02,319
path

00:03:00,319 --> 00:03:03,840
on the upper right here so sometimes we

00:03:02,319 --> 00:03:04,319
do that and we're finished and sometimes

00:03:03,840 --> 00:03:06,000
it's

00:03:04,319 --> 00:03:08,720
automated we set this up and this just

00:03:06,000 --> 00:03:10,400
goes other times we're going to analyze

00:03:08,720 --> 00:03:12,239
that data and we might produce some

00:03:10,400 --> 00:03:13,920
reports so there might be some human

00:03:12,239 --> 00:03:15,200
generator reports some automated

00:03:13,920 --> 00:03:16,319
dashboards

00:03:15,200 --> 00:03:18,400
there might be some routine that

00:03:16,319 --> 00:03:20,080
produces reports on an

00:03:18,400 --> 00:03:22,000
ongoing basis and ships them off to our

00:03:20,080 --> 00:03:23,840
partners and then there's also the

00:03:22,000 --> 00:03:24,560
machine learning modeling where we might

00:03:23,840 --> 00:03:26,239
take

00:03:24,560 --> 00:03:27,599
some of that data that we've analyzed

00:03:26,239 --> 00:03:30,000
and

00:03:27,599 --> 00:03:31,920
apply some machine learning to it to try

00:03:30,000 --> 00:03:33,519
and build some model that we can later

00:03:31,920 --> 00:03:34,080
deploy into production to help our

00:03:33,519 --> 00:03:36,640
business in

00:03:34,080 --> 00:03:37,760
in some way now there's a few more steps

00:03:36,640 --> 00:03:39,840
on here that

00:03:37,760 --> 00:03:41,120
are not or not on here that are not in

00:03:39,840 --> 00:03:43,200
the picture um

00:03:41,120 --> 00:03:44,720
so before we even get the data into the

00:03:43,200 --> 00:03:46,000
data lake there's a lot of you know

00:03:44,720 --> 00:03:48,159
challenging upstream work

00:03:46,000 --> 00:03:49,920
in terms of acquiring data and ingesting

00:03:48,159 --> 00:03:50,319
it and we're not going to get into that

00:03:49,920 --> 00:03:52,640
part

00:03:50,319 --> 00:03:54,560
uh today and also there's very important

00:03:52,640 --> 00:03:56,319
things downstream so when we have some

00:03:54,560 --> 00:03:57,519
reports or some models

00:03:56,319 --> 00:03:59,040
you know those things need to be

00:03:57,519 --> 00:04:00,959
deployed somewhere and they need to be

00:03:59,040 --> 00:04:01,519
monitored and there's a lot more over

00:04:00,959 --> 00:04:03,360
there

00:04:01,519 --> 00:04:05,200
but today we're kind of zooming in on

00:04:03,360 --> 00:04:06,799
the core of this

00:04:05,200 --> 00:04:08,319
process and specifically some of the

00:04:06,799 --> 00:04:10,560
steps i'm going to look at

00:04:08,319 --> 00:04:12,080
on the left hand side of this picture

00:04:10,560 --> 00:04:15,120
because that's really where

00:04:12,080 --> 00:04:16,479
sql is most important and where it's

00:04:15,120 --> 00:04:19,759
been difficult

00:04:16,479 --> 00:04:21,120
for python folks in the past so about a

00:04:19,759 --> 00:04:24,080
year ago

00:04:21,120 --> 00:04:25,680
i was talking a lot about dasc and using

00:04:24,080 --> 00:04:29,199
das for these enterprise

00:04:25,680 --> 00:04:31,680
large scale data processing pipelines

00:04:29,199 --> 00:04:32,400
and over on the left hand side here we

00:04:31,680 --> 00:04:35,919
have

00:04:32,400 --> 00:04:38,320
desk apis that can do a lot of these

00:04:35,919 --> 00:04:40,240
steps so they can retrieve data in

00:04:38,320 --> 00:04:43,680
various formats

00:04:40,240 --> 00:04:45,040
they can use friendly apis to rearrange

00:04:43,680 --> 00:04:46,560
that data

00:04:45,040 --> 00:04:49,360
and then we can feed that into

00:04:46,560 --> 00:04:52,160
subsequent steps um however

00:04:49,360 --> 00:04:52,960
that required at least last year

00:04:52,160 --> 00:04:55,520
required

00:04:52,960 --> 00:04:57,360
uh using dask's regular apis now i'm a

00:04:55,520 --> 00:04:58,240
big fan of tasks apis i think they're

00:04:57,360 --> 00:05:00,000
pretty

00:04:58,240 --> 00:05:01,280
friendly pretty easy to learn and pretty

00:05:00,000 --> 00:05:03,680
easy to use

00:05:01,280 --> 00:05:04,320
but not everybody in the world is a

00:05:03,680 --> 00:05:06,880
python

00:05:04,320 --> 00:05:08,720
programmer and sometimes even if we are

00:05:06,880 --> 00:05:12,000
a python programmer we may be

00:05:08,720 --> 00:05:13,759
asked by our boss let's say to

00:05:12,000 --> 00:05:15,120
retrieve data where we already have

00:05:13,759 --> 00:05:17,039
existing sql

00:05:15,120 --> 00:05:19,280
right so sql is kind of the lingua

00:05:17,039 --> 00:05:21,680
franca of the data world

00:05:19,280 --> 00:05:22,400
probably more people know sql than any

00:05:21,680 --> 00:05:25,520
other

00:05:22,400 --> 00:05:26,639
individual data relevant language i

00:05:25,520 --> 00:05:28,160
can't prove that but i

00:05:26,639 --> 00:05:30,400
have a pretty strong feeling that it's

00:05:28,160 --> 00:05:33,440
the case and

00:05:30,400 --> 00:05:35,120
sql allows us to span

00:05:33,440 --> 00:05:37,120
different eras of technology a lot of

00:05:35,120 --> 00:05:38,320
times we have queries or reports that we

00:05:37,120 --> 00:05:40,160
need to uh

00:05:38,320 --> 00:05:41,680
implement where that sequel was written

00:05:40,160 --> 00:05:44,320
5 10 20

00:05:41,680 --> 00:05:44,800
30 even 40 years ago and that stuff is

00:05:44,320 --> 00:05:48,240
still

00:05:44,800 --> 00:05:50,400
important so sql kind of bridges across

00:05:48,240 --> 00:05:51,520
technological eras it also bridges

00:05:50,400 --> 00:05:53,440
across

00:05:51,520 --> 00:05:55,440
business functions right we have

00:05:53,440 --> 00:05:56,880
business analysts that know sql but

00:05:55,440 --> 00:05:59,360
they're not programmers

00:05:56,880 --> 00:06:00,080
we might have programmers from other

00:05:59,360 --> 00:06:02,000
language

00:06:00,080 --> 00:06:04,319
kind of ecosystems and they don't really

00:06:02,000 --> 00:06:07,680
know python but they do know sql

00:06:04,319 --> 00:06:10,080
so there's lots and lots of uses uh for

00:06:07,680 --> 00:06:11,199
for sql and lots of places where in

00:06:10,080 --> 00:06:14,240
order to implement

00:06:11,199 --> 00:06:15,039
the existing business process uh we need

00:06:14,240 --> 00:06:17,039
to use

00:06:15,039 --> 00:06:18,639
sql so it's not you know in theory we

00:06:17,039 --> 00:06:21,120
could rewrite some of that but if

00:06:18,639 --> 00:06:23,360
a company has thousands of scripts that

00:06:21,120 --> 00:06:24,000
are hundreds of lines of complicated sql

00:06:23,360 --> 00:06:26,319
each

00:06:24,000 --> 00:06:28,000
uh it's it's unlikely that they're going

00:06:26,319 --> 00:06:29,680
to want to rewrite all of that and then

00:06:28,000 --> 00:06:31,520
you know test it for correctness and all

00:06:29,680 --> 00:06:32,880
of those things so sql is really really

00:06:31,520 --> 00:06:34,800
important

00:06:32,880 --> 00:06:36,240
and a year ago we had this whole kind of

00:06:34,800 --> 00:06:37,360
nice picture and over here on the left

00:06:36,240 --> 00:06:40,160
we had das

00:06:37,360 --> 00:06:42,319
but we did not have sql so if we needed

00:06:40,160 --> 00:06:43,120
sql we needed to use other tools at the

00:06:42,319 --> 00:06:45,280
time so

00:06:43,120 --> 00:06:47,759
we talked about at that time were

00:06:45,280 --> 00:06:50,960
integrating jvm based tools

00:06:47,759 --> 00:06:54,160
so these are things like spark

00:06:50,960 --> 00:06:56,560
presto trino these are tools that are

00:06:54,160 --> 00:06:57,680
kind of come from the java based big

00:06:56,560 --> 00:06:59,680
data world

00:06:57,680 --> 00:07:01,680
and they know how to apply a sql over

00:06:59,680 --> 00:07:03,599
this collections of files

00:07:01,680 --> 00:07:05,120
but that's not always very easy to work

00:07:03,599 --> 00:07:06,160
with if we're coming from the python

00:07:05,120 --> 00:07:10,000
point of view

00:07:06,160 --> 00:07:12,000
a lot of times it's the python apis for

00:07:10,000 --> 00:07:14,160
finding the pieces of data that we need

00:07:12,000 --> 00:07:17,039
can be really really tricky

00:07:14,160 --> 00:07:17,440
and it's it can kind of push somebody

00:07:17,039 --> 00:07:20,080
maybe

00:07:17,440 --> 00:07:21,840
out of their uh specialization uh kind

00:07:20,080 --> 00:07:23,759
of into a data engineering role that

00:07:21,840 --> 00:07:25,520
they're just not really used to and it's

00:07:23,759 --> 00:07:27,280
not their main job and then it's very

00:07:25,520 --> 00:07:30,160
difficult to succeed so

00:07:27,280 --> 00:07:31,280
uh luckily uh it is not 2020 anymore

00:07:30,160 --> 00:07:33,680
we're all happy

00:07:31,280 --> 00:07:35,280
now i think that it's 2021 so not

00:07:33,680 --> 00:07:36,160
everything is perfect but things are

00:07:35,280 --> 00:07:37,840
improving

00:07:36,160 --> 00:07:39,840
and one of those things that's improving

00:07:37,840 --> 00:07:43,120
is we now have a thing called

00:07:39,840 --> 00:07:45,120
das sql so what is task sql well it's in

00:07:43,120 --> 00:07:47,280
in one line it's desk plus sql but we're

00:07:45,120 --> 00:07:50,400
going to get into a little more detail

00:07:47,280 --> 00:07:51,919
in this talk so what we can do now is do

00:07:50,400 --> 00:07:53,120
all of this work on the left hand side

00:07:51,919 --> 00:07:56,479
of the picture

00:07:53,120 --> 00:07:57,599
using uh sql we can even do some of the

00:07:56,479 --> 00:07:59,759
stuff on the right hand side of the

00:07:57,599 --> 00:08:01,440
picture using sql but we're not going to

00:07:59,759 --> 00:08:03,120
dash sql but we're not going to get into

00:08:01,440 --> 00:08:06,639
all of those details

00:08:03,120 --> 00:08:07,440
here today so okay so what is das sql

00:08:06,639 --> 00:08:11,039
exactly

00:08:07,440 --> 00:08:12,960
so the one fundamental headline here is

00:08:11,039 --> 00:08:16,240
adding sql execution

00:08:12,960 --> 00:08:17,840
and hive access to python

00:08:16,240 --> 00:08:20,479
the picture here is a gentleman named

00:08:17,840 --> 00:08:22,560
niels brown and he works for the bosch

00:08:20,479 --> 00:08:24,400
center for artificial intelligence in

00:08:22,560 --> 00:08:26,720
germany he is the

00:08:24,400 --> 00:08:28,879
lead on this project and and created

00:08:26,720 --> 00:08:31,919
this project i've got a link to his

00:08:28,879 --> 00:08:33,440
uh linkedin and the github his github

00:08:31,919 --> 00:08:37,599
down here

00:08:33,440 --> 00:08:40,320
and what does das sql do

00:08:37,599 --> 00:08:41,200
so it does quite a lot even though the

00:08:40,320 --> 00:08:43,839
project is

00:08:41,200 --> 00:08:45,440
less than a year old so first we'll look

00:08:43,839 --> 00:08:48,720
at these core features

00:08:45,440 --> 00:08:51,839
sql parsing optimization planning and

00:08:48,720 --> 00:08:54,080
translation for desk basically take sql

00:08:51,839 --> 00:08:55,760
make it run on desk and we'll talk more

00:08:54,080 --> 00:08:57,839
about how that works in a bit

00:08:55,760 --> 00:08:59,519
uh it lets us start with data from lots

00:08:57,839 --> 00:09:00,000
of places there could be files in the

00:08:59,519 --> 00:09:01,760
cloud

00:09:00,000 --> 00:09:03,920
right so pretty much any cloud for

00:09:01,760 --> 00:09:06,080
example s3 but you know lots of other

00:09:03,920 --> 00:09:07,680
places that data could be

00:09:06,080 --> 00:09:09,760
date any data you already have in python

00:09:07,680 --> 00:09:11,200
so you have das data frames well now you

00:09:09,760 --> 00:09:12,880
can do sql over those

00:09:11,200 --> 00:09:14,800
even if you just have a bunch of pandas

00:09:12,880 --> 00:09:17,760
data frames we can

00:09:14,800 --> 00:09:18,399
work that into this picture as well

00:09:17,760 --> 00:09:20,880
there's

00:09:18,399 --> 00:09:22,399
some new support for modern catalog and

00:09:20,880 --> 00:09:24,880
aggregation systems like

00:09:22,399 --> 00:09:26,399
intake intake's a really neat project if

00:09:24,880 --> 00:09:27,279
you haven't had a chance to play with

00:09:26,399 --> 00:09:28,959
that yet

00:09:27,279 --> 00:09:31,040
one of the most important pieces here

00:09:28,959 --> 00:09:33,200
especially in terms of

00:09:31,040 --> 00:09:35,440
empowering pythonistas in the

00:09:33,200 --> 00:09:37,120
traditional enterprise data world

00:09:35,440 --> 00:09:39,600
we can read data directly from

00:09:37,120 --> 00:09:41,839
enterprise data lakes or warehouses

00:09:39,600 --> 00:09:43,920
where the catalog is typically stored in

00:09:41,839 --> 00:09:46,320
something like the hive meta store

00:09:43,920 --> 00:09:48,000
data bricks catalog and so on that's a

00:09:46,320 --> 00:09:50,080
really big deal because if you're

00:09:48,000 --> 00:09:52,160
coming from the python world and you

00:09:50,080 --> 00:09:53,519
show up at your new job at some you know

00:09:52,160 --> 00:09:55,519
giant company and they say oh

00:09:53,519 --> 00:09:57,600
we're so happy to have you here uh your

00:09:55,519 --> 00:09:59,200
first project is this really simple

00:09:57,600 --> 00:10:00,640
uh i don't know a linear regression and

00:09:59,200 --> 00:10:01,920
you're like oh great i can knock this

00:10:00,640 --> 00:10:03,279
out of the park and they go

00:10:01,920 --> 00:10:06,079
the thing is you're gonna need to join

00:10:03,279 --> 00:10:08,800
these 17 tables and they're

00:10:06,079 --> 00:10:10,320
built out of views over all of this data

00:10:08,800 --> 00:10:11,839
in our data lake

00:10:10,320 --> 00:10:13,600
and then things start getting a little

00:10:11,839 --> 00:10:14,399
murkier and then you kind of dig into

00:10:13,600 --> 00:10:16,160
that and they say oh

00:10:14,399 --> 00:10:17,600
no it's no problem all of these tables

00:10:16,160 --> 00:10:19,279
are defined in the hive meta store just

00:10:17,600 --> 00:10:20,399
go grab the underlying data and you're

00:10:19,279 --> 00:10:22,240
good to go

00:10:20,399 --> 00:10:23,440
it's not quite as straightforward as

00:10:22,240 --> 00:10:26,079
that

00:10:23,440 --> 00:10:26,959
but luckily desk sql is going to make

00:10:26,079 --> 00:10:29,519
that

00:10:26,959 --> 00:10:31,440
really really easy there's a few other

00:10:29,519 --> 00:10:33,519
great features we've got here too so we

00:10:31,440 --> 00:10:35,040
can just like we can normally do a desk

00:10:33,519 --> 00:10:37,360
we can cache data sets

00:10:35,040 --> 00:10:38,399
so if we have a big pool of servers a

00:10:37,360 --> 00:10:40,959
big cluster

00:10:38,399 --> 00:10:41,600
we can cache those uh cache the data in

00:10:40,959 --> 00:10:43,920
there

00:10:41,600 --> 00:10:45,680
and use that for our work and there's a

00:10:43,920 --> 00:10:46,640
bunch of kind of what i've called bonus

00:10:45,680 --> 00:10:48,160
features

00:10:46,640 --> 00:10:51,040
which will maybe eventually be core

00:10:48,160 --> 00:10:52,880
features but um

00:10:51,040 --> 00:10:54,320
they're maybe not the headline today and

00:10:52,880 --> 00:10:55,120
those are things like user defined

00:10:54,320 --> 00:10:57,600
functions

00:10:55,120 --> 00:10:59,519
uh a sql server that'll allows folks

00:10:57,600 --> 00:11:00,640
using reporting front ends like say

00:10:59,519 --> 00:11:02,640
tableau

00:11:00,640 --> 00:11:04,000
to be able to pull directly from your

00:11:02,640 --> 00:11:06,240
desk system

00:11:04,000 --> 00:11:08,079
machine learning and sql so that's

00:11:06,240 --> 00:11:09,040
something that's slowly getting more

00:11:08,079 --> 00:11:11,920
popular

00:11:09,040 --> 00:11:14,399
and there are options for that already

00:11:11,920 --> 00:11:16,480
supported in desk sql a command line

00:11:14,399 --> 00:11:18,800
client so you can run sql

00:11:16,480 --> 00:11:19,839
directly from the command line uh and

00:11:18,800 --> 00:11:22,720
even more

00:11:19,839 --> 00:11:24,000
that's currently in the works so i've

00:11:22,720 --> 00:11:28,160
got a few links here

00:11:24,000 --> 00:11:31,839
the home page for the das sql project

00:11:28,160 --> 00:11:35,360
so there's a brand new really shiny nice

00:11:31,839 --> 00:11:36,480
quick start home page for the desk sql

00:11:35,360 --> 00:11:38,640
project

00:11:36,480 --> 00:11:39,920
there's also full documentation linked

00:11:38,640 --> 00:11:42,959
down here and

00:11:39,920 --> 00:11:45,519
the github repo of course

00:11:42,959 --> 00:11:47,519
so we're going to do some coding but

00:11:45,519 --> 00:11:50,480
before we do our code demos

00:11:47,519 --> 00:11:51,120
i want to make a little clarification

00:11:50,480 --> 00:11:52,720
and if you're

00:11:51,120 --> 00:11:54,800
if you're a veteran of the big data

00:11:52,720 --> 00:11:56,399
world this is kind of you can you can

00:11:54,800 --> 00:11:58,240
grab an extra cup of coffee and come

00:11:56,399 --> 00:11:58,639
back in a minute but for the folks that

00:11:58,240 --> 00:12:00,800
are

00:11:58,639 --> 00:12:02,399
new or just haven't spent a lot of time

00:12:00,800 --> 00:12:03,360
in this large scale data engineering

00:12:02,399 --> 00:12:05,519
world

00:12:03,360 --> 00:12:08,000
this can be a little bit confusing a lot

00:12:05,519 --> 00:12:10,000
of questions that i get around ask sql

00:12:08,000 --> 00:12:11,920
folks say wait a second but doesn't das

00:12:10,000 --> 00:12:14,000
have a read sql table already

00:12:11,920 --> 00:12:16,000
uh doesn't pandas have a read sql and a

00:12:14,000 --> 00:12:18,079
read sql query and then i've got

00:12:16,000 --> 00:12:20,079
you know i've got sql alchemy i've been

00:12:18,079 --> 00:12:20,720
using python with sql alchemy for a long

00:12:20,079 --> 00:12:22,240
time

00:12:20,720 --> 00:12:24,079
how is this different why do we need

00:12:22,240 --> 00:12:26,800
something new and so it's worth

00:12:24,079 --> 00:12:27,440
clarifying this so that those systems

00:12:26,800 --> 00:12:29,519
are all

00:12:27,440 --> 00:12:31,360
very very different from what we're

00:12:29,519 --> 00:12:33,519
trying to do here with desk sql

00:12:31,360 --> 00:12:34,880
and the difference is this those other

00:12:33,519 --> 00:12:38,000
approaches

00:12:34,880 --> 00:12:40,240
allow you to formulate a query and they

00:12:38,000 --> 00:12:42,000
pass it to a database system which

00:12:40,240 --> 00:12:44,240
already understands sql

00:12:42,000 --> 00:12:45,040
or a similar language and can execute

00:12:44,240 --> 00:12:46,959
that query

00:12:45,040 --> 00:12:48,560
and that other system has control over

00:12:46,959 --> 00:12:49,920
your data in other words they pass a

00:12:48,560 --> 00:12:51,279
query to another system that already

00:12:49,920 --> 00:12:54,639
knows where all the data is

00:12:51,279 --> 00:12:55,040
and can do all of the work so there's a

00:12:54,639 --> 00:12:58,000
big

00:12:55,040 --> 00:12:58,399
challenge when we want to apply sql to

00:12:58,000 --> 00:13:00,880
these

00:12:58,399 --> 00:13:02,880
enterprise data lakes now there are

00:13:00,880 --> 00:13:04,639
other tools

00:13:02,880 --> 00:13:06,399
that have supported this use case and

00:13:04,639 --> 00:13:07,760
have been very successful in the past so

00:13:06,399 --> 00:13:08,800
you've probably heard of systems like

00:13:07,760 --> 00:13:11,680
hive

00:13:08,800 --> 00:13:12,480
and then apache spark and those are

00:13:11,680 --> 00:13:14,800
great tools

00:13:12,480 --> 00:13:16,320
but they're based on a kind of a hadoop

00:13:14,800 --> 00:13:17,519
jvm system

00:13:16,320 --> 00:13:19,440
they're kind of becoming a little bit

00:13:17,519 --> 00:13:21,839
legacy tools and in any case

00:13:19,440 --> 00:13:22,720
they're not necessarily the easiest

00:13:21,839 --> 00:13:26,000
things to

00:13:22,720 --> 00:13:27,519
understand optimize use

00:13:26,000 --> 00:13:29,760
all before you can start your work in

00:13:27,519 --> 00:13:30,160
python right so those things can be an

00:13:29,760 --> 00:13:33,040
optic

00:13:30,160 --> 00:13:34,959
an obstacle the goal of task sql is to

00:13:33,040 --> 00:13:37,440
allow you to formulate a sql query

00:13:34,959 --> 00:13:40,480
against arbitrary files and formats

00:13:37,440 --> 00:13:43,600
and then execute that squarey query at

00:13:40,480 --> 00:13:44,639
large scale using your desk cluster and

00:13:43,600 --> 00:13:47,519
this is going to help us

00:13:44,639 --> 00:13:48,000
put all of that together okay so it is

00:13:47,519 --> 00:13:51,279
coding

00:13:48,000 --> 00:13:53,440
time let's go and demo a few different

00:13:51,279 --> 00:13:55,040
uh fun things here uh we're going to

00:13:53,440 --> 00:13:57,279
demo three approaches

00:13:55,040 --> 00:13:58,480
to using the ask sql and the little bit

00:13:57,279 --> 00:14:00,480
of time that we've got

00:13:58,480 --> 00:14:03,680
uh first we're going to show creating a

00:14:00,480 --> 00:14:06,560
task data frame over a set of files

00:14:03,680 --> 00:14:08,240
and then using dask sql to query that

00:14:06,560 --> 00:14:10,880
then we're going to create a desk sql

00:14:08,240 --> 00:14:13,040
table completely within sql

00:14:10,880 --> 00:14:14,959
which is something that might be really

00:14:13,040 --> 00:14:16,000
popular with your sql analyst friends or

00:14:14,959 --> 00:14:18,240
folks that are

00:14:16,000 --> 00:14:19,600
maybe used to a more uh kind of hive

00:14:18,240 --> 00:14:21,760
type approach

00:14:19,600 --> 00:14:23,040
and then the third example is we're

00:14:21,760 --> 00:14:25,279
going to show using das

00:14:23,040 --> 00:14:26,720
sql to access tables already defined in

00:14:25,279 --> 00:14:28,480
the hive catalog

00:14:26,720 --> 00:14:30,160
but we're going to do the actual query

00:14:28,480 --> 00:14:32,959
execution using task

00:14:30,160 --> 00:14:34,079
so that's really uh kind of the golden

00:14:32,959 --> 00:14:35,920
ring here

00:14:34,079 --> 00:14:38,079
because in that story when you show up

00:14:35,920 --> 00:14:40,240
to your new job this is what you really

00:14:38,079 --> 00:14:41,839
want you want to be able to say okay no

00:14:40,240 --> 00:14:43,279
problem i'm going to write my sequel

00:14:41,839 --> 00:14:44,959
i'm going to reference these tables

00:14:43,279 --> 00:14:48,240
those tables are defined in

00:14:44,959 --> 00:14:50,000
a hive catalog but i'm going to find all

00:14:48,240 --> 00:14:51,040
the underlying data without having to do

00:14:50,000 --> 00:14:53,360
the work myself

00:14:51,040 --> 00:14:54,079
and i'm going to execute my queries

00:14:53,360 --> 00:14:57,120
against that

00:14:54,079 --> 00:14:58,160
using my desk cluster in python so these

00:14:57,120 --> 00:14:59,199
are the three things we're going to look

00:14:58,160 --> 00:15:01,760
at

00:14:59,199 --> 00:15:02,639
so first i'm going to spin up a little

00:15:01,760 --> 00:15:04,320
desk

00:15:02,639 --> 00:15:06,800
cluster with the distributed scheduler

00:15:04,320 --> 00:15:10,000
here and let's just

00:15:06,800 --> 00:15:13,440
make sure that that is alive

00:15:10,000 --> 00:15:14,959
over here so this is the das dashboard

00:15:13,440 --> 00:15:15,600
we'll look at this once or twice just to

00:15:14,959 --> 00:15:17,760
make things

00:15:15,600 --> 00:15:20,000
make sure things are actually running uh

00:15:17,760 --> 00:15:22,480
in desk

00:15:20,000 --> 00:15:23,199
now we're going to spin up a dash sql

00:15:22,480 --> 00:15:26,880
context

00:15:23,199 --> 00:15:28,720
object and in this first example

00:15:26,880 --> 00:15:31,440
remember we're going to start with a das

00:15:28,720 --> 00:15:32,720
data frame so this is a data frame built

00:15:31,440 --> 00:15:36,079
over the uc irvine

00:15:32,720 --> 00:15:37,040
power plant data set so it's a a simple

00:15:36,079 --> 00:15:38,639
data set

00:15:37,040 --> 00:15:40,160
around power plant measurements you can

00:15:38,639 --> 00:15:41,519
see it's got five columns and they're

00:15:40,160 --> 00:15:44,160
all floats

00:15:41,519 --> 00:15:45,519
and what i'm going to do is i'm going to

00:15:44,160 --> 00:15:48,959
register that

00:15:45,519 --> 00:15:50,800
uh data source with dask sql and i'm

00:15:48,959 --> 00:15:52,560
going to call it powerplant so when i

00:15:50,800 --> 00:15:56,079
say call it i'm going to create

00:15:52,560 --> 00:15:58,720
a virtual table name in the dash sql

00:15:56,079 --> 00:15:59,839
table space called powerplant and once

00:15:58,720 --> 00:16:03,040
i've done that i can

00:15:59,839 --> 00:16:04,480
run sql against that table so here's a

00:16:03,040 --> 00:16:08,320
pretty simple

00:16:04,480 --> 00:16:09,120
query a select star and we get basically

00:16:08,320 --> 00:16:12,320
the same

00:16:09,120 --> 00:16:14,160
sort of result that we got from

00:16:12,320 --> 00:16:16,639
just looking directly at the underlying

00:16:14,160 --> 00:16:18,720
data so this is pretty much the same

00:16:16,639 --> 00:16:20,720
view you can see there's a little bit of

00:16:18,720 --> 00:16:21,120
a different task graph involved i'm not

00:16:20,720 --> 00:16:22,639
going to

00:16:21,120 --> 00:16:25,199
get into the details there today but

00:16:22,639 --> 00:16:28,720
we're looking at the same data

00:16:25,199 --> 00:16:31,680
the results in the default results

00:16:28,720 --> 00:16:33,680
from a dash sql query are a das data

00:16:31,680 --> 00:16:35,839
frame you can see that here

00:16:33,680 --> 00:16:37,519
like any other desk data frame if it's

00:16:35,839 --> 00:16:40,800
sufficiently small

00:16:37,519 --> 00:16:43,279
we can compute it and get back a pandas

00:16:40,800 --> 00:16:45,600
data frame

00:16:43,279 --> 00:16:46,320
now if we don't want to go through those

00:16:45,600 --> 00:16:48,079
two steps

00:16:46,320 --> 00:16:50,160
and we know that the query we're doing

00:16:48,079 --> 00:16:52,639
is not going to return too much data

00:16:50,160 --> 00:16:53,920
we can pass this parameter return to

00:16:52,639 --> 00:16:56,959
future is false

00:16:53,920 --> 00:16:57,680
so i can say das sql context run this

00:16:56,959 --> 00:16:59,839
query

00:16:57,680 --> 00:17:00,800
don't give me futures that is don't give

00:16:59,839 --> 00:17:04,000
me a delayed

00:17:00,800 --> 00:17:05,839
execution just give me the actual data

00:17:04,000 --> 00:17:07,360
and you can see here that comes back

00:17:05,839 --> 00:17:10,079
just as a regular old

00:17:07,360 --> 00:17:10,640
pandas data frame so there's a lot we

00:17:10,079 --> 00:17:12,959
can

00:17:10,640 --> 00:17:14,799
do with this we can write maybe a little

00:17:12,959 --> 00:17:16,160
bit fancier query maybe if we're

00:17:14,799 --> 00:17:18,160
interested in looking at the

00:17:16,160 --> 00:17:20,079
relationship between the temperature

00:17:18,160 --> 00:17:21,280
and the power output in these power

00:17:20,079 --> 00:17:23,679
plants um

00:17:21,280 --> 00:17:27,039
maybe we kind of discretize the

00:17:23,679 --> 00:17:27,039
temperature a little bit here

00:17:27,120 --> 00:17:33,280
and we can get a table of temperature

00:17:30,080 --> 00:17:35,120
and output values and if we compute that

00:17:33,280 --> 00:17:39,039
we can then plot this using

00:17:35,120 --> 00:17:39,039
you know regular pandas plotting

00:17:39,200 --> 00:17:42,320
now we can do even more with this data

00:17:41,760 --> 00:17:44,080
maybe

00:17:42,320 --> 00:17:45,360
you look at this picture and you say we

00:17:44,080 --> 00:17:47,360
could do an interesting

00:17:45,360 --> 00:17:48,880
regression model with this uh you

00:17:47,360 --> 00:17:49,600
actually can do a lot with machine

00:17:48,880 --> 00:17:52,960
learning and

00:17:49,600 --> 00:17:55,440
ask sql uh and there's a documentation

00:17:52,960 --> 00:17:56,720
page on here if that sounds interesting

00:17:55,440 --> 00:17:59,440
we're not going to get into details of

00:17:56,720 --> 00:18:01,520
that today but it's a pretty interesting

00:17:59,440 --> 00:18:03,360
set of capabilities but i want to move

00:18:01,520 --> 00:18:06,240
on to my next example what about

00:18:03,360 --> 00:18:07,200
creating the table completely in sql

00:18:06,240 --> 00:18:08,559
right so

00:18:07,200 --> 00:18:10,720
we're not going to write any python in

00:18:08,559 --> 00:18:13,280
here but we're going to be using

00:18:10,720 --> 00:18:15,039
python systems to do this execution so

00:18:13,280 --> 00:18:17,919
there's python behind the scenes

00:18:15,039 --> 00:18:20,000
so the desk sql context has a method

00:18:17,919 --> 00:18:22,720
that installs an ipython magic

00:18:20,000 --> 00:18:24,080
and now we can go percent percent of sql

00:18:22,720 --> 00:18:27,280
and we can just put our sql

00:18:24,080 --> 00:18:29,679
right here in the notebook and

00:18:27,280 --> 00:18:31,679
if i run this i'm creating a table this

00:18:29,679 --> 00:18:32,960
one's called all sql since we're doing

00:18:31,679 --> 00:18:35,520
all sql today

00:18:32,960 --> 00:18:36,400
uh and i have a couple of keyword values

00:18:35,520 --> 00:18:39,440
in here uh

00:18:36,400 --> 00:18:42,160
format csv location points to a

00:18:39,440 --> 00:18:43,520
location of any desk accessible source

00:18:42,160 --> 00:18:46,000
or format

00:18:43,520 --> 00:18:49,679
and now if i run that query this time

00:18:46,000 --> 00:18:53,120
against my all sql table

00:18:49,679 --> 00:18:53,440
we get the same output here and we're

00:18:53,120 --> 00:18:55,600
now

00:18:53,440 --> 00:18:56,559
working completely in sql but again

00:18:55,600 --> 00:18:58,720
we're using

00:18:56,559 --> 00:19:00,640
uh das behind the scenes so all of that

00:18:58,720 --> 00:19:04,240
execution is happening

00:19:00,640 --> 00:19:06,880
in task now our third example

00:19:04,240 --> 00:19:08,960
is hive catalog integration but what

00:19:06,880 --> 00:19:12,160
we're going to do here is we're going to

00:19:08,960 --> 00:19:16,640
create a cursor

00:19:12,160 --> 00:19:18,880
that points to the hive service

00:19:16,640 --> 00:19:21,200
and then we're going to create a table

00:19:18,880 --> 00:19:23,840
on our task sequel context

00:19:21,200 --> 00:19:26,080
we're going to call it my diamonds and

00:19:23,840 --> 00:19:27,280
it's actually going to point to a table

00:19:26,080 --> 00:19:32,320
in the hive catalog

00:19:27,280 --> 00:19:35,120
called diamonds now when i run this

00:19:32,320 --> 00:19:35,679
a query is going to run against the hive

00:19:35,120 --> 00:19:37,919
server

00:19:35,679 --> 00:19:39,200
just to describe the table and you can

00:19:37,919 --> 00:19:41,280
see that over here

00:19:39,200 --> 00:19:42,880
so if i refresh this this is a hive

00:19:41,280 --> 00:19:44,880
server page

00:19:42,880 --> 00:19:46,160
if i refresh this you can see that we

00:19:44,880 --> 00:19:49,600
just did a describe

00:19:46,160 --> 00:19:49,600
formatted on diamonds

00:19:50,559 --> 00:19:54,640
but what i'm uh claiming here is that if

00:19:52,720 --> 00:19:55,840
we run lots of queries over this

00:19:54,640 --> 00:19:57,760
diamonds table in

00:19:55,840 --> 00:19:59,360
das equal now you won't see any more

00:19:57,760 --> 00:20:00,000
queries in hot we're not using hive

00:19:59,360 --> 00:20:02,320
anymore

00:20:00,000 --> 00:20:03,200
is just telling us where the data lives

00:20:02,320 --> 00:20:06,720
but we're gonna do

00:20:03,200 --> 00:20:08,240
all of this stuff uh in ask and ask sql

00:20:06,720 --> 00:20:10,159
so

00:20:08,240 --> 00:20:12,400
here we've got a query to pull up the

00:20:10,159 --> 00:20:14,240
first few rows

00:20:12,400 --> 00:20:16,880
uh and here's a little kind of more

00:20:14,240 --> 00:20:20,000
interesting query where we're going to

00:20:16,880 --> 00:20:22,480
kind of look at our diamonds

00:20:20,000 --> 00:20:23,280
in tenths of a carat and we're going to

00:20:22,480 --> 00:20:25,600
calculate the

00:20:23,280 --> 00:20:26,480
average price for each of those tenth of

00:20:25,600 --> 00:20:29,039
a carat

00:20:26,480 --> 00:20:32,480
integrals and we'll also see how many

00:20:29,039 --> 00:20:34,400
diamonds are in those intervals

00:20:32,480 --> 00:20:36,520
so we define a query we're going to go

00:20:34,400 --> 00:20:38,720
ahead and run it just like we did before

00:20:36,520 --> 00:20:40,799
context.sql query we're going to do a

00:20:38,720 --> 00:20:42,159
compute to get a data frame that we can

00:20:40,799 --> 00:20:45,280
plot locally

00:20:42,159 --> 00:20:47,280
and we get this uh nice set of diagrams

00:20:45,280 --> 00:20:49,159
here so one thing you can see is that

00:20:47,280 --> 00:20:51,520
in the diamond data set this is the

00:20:49,159 --> 00:20:52,480
ggplot2r diamonds dataset you may have

00:20:51,520 --> 00:20:54,640
seen it before

00:20:52,480 --> 00:20:56,720
uh there's lots of small diamonds that

00:20:54,640 --> 00:20:58,400
show a really nice correlation between

00:20:56,720 --> 00:21:00,320
carat weight and price

00:20:58,400 --> 00:21:02,640
until you get to about two and a half

00:21:00,320 --> 00:21:03,600
carats and then that relationship starts

00:21:02,640 --> 00:21:05,360
to come apart

00:21:03,600 --> 00:21:06,880
for a variety of reasons and one of them

00:21:05,360 --> 00:21:07,600
is that there's just not a whole lot of

00:21:06,880 --> 00:21:09,200
data

00:21:07,600 --> 00:21:10,960
up there if you look at the right hand

00:21:09,200 --> 00:21:12,159
side of this bottom graph

00:21:10,960 --> 00:21:15,039
so these are just the kind of things

00:21:12,159 --> 00:21:17,360
that we can now really easily do

00:21:15,039 --> 00:21:19,520
and we can share with our analysts and

00:21:17,360 --> 00:21:21,280
see equal friends

00:21:19,520 --> 00:21:23,440
so let's spend a minute or two talking

00:21:21,280 --> 00:21:26,240
about how dash sql works

00:21:23,440 --> 00:21:26,880
so these are the steps when we're doing

00:21:26,240 --> 00:21:29,120
something with

00:21:26,880 --> 00:21:30,799
desk sql and as you've seen most of this

00:21:29,120 --> 00:21:32,000
happens automatically but this is what's

00:21:30,799 --> 00:21:34,400
going on under the hood

00:21:32,000 --> 00:21:35,360
first we need to locate the data source

00:21:34,400 --> 00:21:37,280
right so this could be

00:21:35,360 --> 00:21:39,120
hive like we just demonstrated or some

00:21:37,280 --> 00:21:41,280
other catalog integrations

00:21:39,120 --> 00:21:42,799
or it could be files or python data that

00:21:41,280 --> 00:21:45,360
comes from the user

00:21:42,799 --> 00:21:47,440
uh we then need to prepare the query and

00:21:45,360 --> 00:21:49,120
that's done using an open source project

00:21:47,440 --> 00:21:52,240
called apache calcite

00:21:49,120 --> 00:21:55,840
so the actual query parser

00:21:52,240 --> 00:21:56,400
optimizer and um sort of a relational

00:21:55,840 --> 00:21:59,039
algebra

00:21:56,400 --> 00:22:00,880
engine is actually done in apache cal

00:21:59,039 --> 00:22:03,039
site which is a wonderful open source

00:22:00,880 --> 00:22:04,320
project uh that's actually implemented

00:22:03,039 --> 00:22:06,480
in java but it's kind of

00:22:04,320 --> 00:22:08,720
uh hidden away so it just uh so you

00:22:06,480 --> 00:22:11,360
don't need to directly deal with the

00:22:08,720 --> 00:22:12,159
java bits of this and that goes through

00:22:11,360 --> 00:22:15,440
some standard

00:22:12,159 --> 00:22:16,960
phases of uh query processing so

00:22:15,440 --> 00:22:19,600
kind of parsing since we're starting

00:22:16,960 --> 00:22:22,000
with strings uh analyzing to make sure

00:22:19,600 --> 00:22:25,200
that the right tables and columns exist

00:22:22,000 --> 00:22:27,360
uh optimizing using a set of

00:22:25,200 --> 00:22:28,720
different mechanisms to potentially

00:22:27,360 --> 00:22:31,039
improve the query

00:22:28,720 --> 00:22:32,559
uh and then we need to create an

00:22:31,039 --> 00:22:35,120
execution plan

00:22:32,559 --> 00:22:36,960
and this is where das sql takes over

00:22:35,120 --> 00:22:40,080
from apache calcite

00:22:36,960 --> 00:22:43,280
uh and it takes a structure over logical

00:22:40,080 --> 00:22:46,159
relational operators

00:22:43,280 --> 00:22:47,440
things like projection which corresponds

00:22:46,159 --> 00:22:50,240
to the sql select or

00:22:47,440 --> 00:22:51,120
filters they correspond to the sql where

00:22:50,240 --> 00:22:53,440
and so on

00:22:51,120 --> 00:22:55,520
and it converts those into das data

00:22:53,440 --> 00:22:57,280
frame api calls right so maybe they

00:22:55,520 --> 00:22:58,320
become a dot query maybe they become a

00:22:57,280 --> 00:23:00,480
dot merge

00:22:58,320 --> 00:23:01,679
and that is a big part of the magic of

00:23:00,480 --> 00:23:03,760
das sql

00:23:01,679 --> 00:23:05,600
uh desk sql can then do one of two

00:23:03,760 --> 00:23:08,480
things it can return a handle to the

00:23:05,600 --> 00:23:10,080
desk data frame of results

00:23:08,480 --> 00:23:11,600
so that's what we saw in the very

00:23:10,080 --> 00:23:13,360
beginning you get it back into

00:23:11,600 --> 00:23:15,360
data frames so this is our virtual data

00:23:13,360 --> 00:23:17,039
frame right we've got metadata

00:23:15,360 --> 00:23:18,720
uh but we don't actually have the data

00:23:17,039 --> 00:23:21,919
it's kind of delayed execution

00:23:18,720 --> 00:23:23,520
uh or we can say hey we just want to run

00:23:21,919 --> 00:23:24,159
this right away so that's where we said

00:23:23,520 --> 00:23:27,120
things like

00:23:24,159 --> 00:23:28,159
return futures is false uh or where we

00:23:27,120 --> 00:23:31,039
use the

00:23:28,159 --> 00:23:33,200
percent percent of sql magic and we

00:23:31,039 --> 00:23:35,200
execute the resulting data frame and

00:23:33,200 --> 00:23:37,440
return the results right away so either

00:23:35,200 --> 00:23:40,159
there's a panda's data frame

00:23:37,440 --> 00:23:41,520
or just directly out to the screen with

00:23:40,159 --> 00:23:44,159
the notebook magic

00:23:41,520 --> 00:23:44,960
a little bit more detail on all this

00:23:44,159 --> 00:23:46,720
under the hood

00:23:44,960 --> 00:23:48,400
processing including how you can see

00:23:46,720 --> 00:23:49,840
some of the query planning in action if

00:23:48,400 --> 00:23:52,480
you're interested in that

00:23:49,840 --> 00:23:54,960
over at this doc page so we have just a

00:23:52,480 --> 00:23:55,440
minute or so left and i want to offer a

00:23:54,960 --> 00:23:58,960
couple of

00:23:55,440 --> 00:24:01,840
practical details as we finish up here

00:23:58,960 --> 00:24:03,440
so how do you install this thing uh well

00:24:01,840 --> 00:24:03,760
there's a dock page that explains that

00:24:03,440 --> 00:24:05,520
but

00:24:03,760 --> 00:24:08,559
basically if you're used to using

00:24:05,520 --> 00:24:11,120
anaconda or a variant like mini conda

00:24:08,559 --> 00:24:12,640
it's just a conda install uh so it's on

00:24:11,120 --> 00:24:13,360
commander forge and it brings in all the

00:24:12,640 --> 00:24:15,840
dependencies

00:24:13,360 --> 00:24:16,880
including the jvm and that all just

00:24:15,840 --> 00:24:19,520
works it's really

00:24:16,880 --> 00:24:20,480
nicely put together um what kind of sql

00:24:19,520 --> 00:24:21,760
can i use

00:24:20,480 --> 00:24:23,919
right so you're probably thinking well

00:24:21,760 --> 00:24:26,240
sql is an enormous language these days

00:24:23,919 --> 00:24:27,360
and ask sql is a young project so there

00:24:26,240 --> 00:24:28,720
must be something missing

00:24:27,360 --> 00:24:30,880
and of course there is right not

00:24:28,720 --> 00:24:32,080
everything is there yet um the details

00:24:30,880 --> 00:24:35,120
on that are on these pages

00:24:32,080 --> 00:24:38,960
so things like uh query support uh

00:24:35,120 --> 00:24:41,520
table creation so uh loading data

00:24:38,960 --> 00:24:42,240
and that machine learning via sql down

00:24:41,520 --> 00:24:45,279
here

00:24:42,240 --> 00:24:47,440
uh how can i contribute um there is

00:24:45,279 --> 00:24:49,600
source code and info on a development

00:24:47,440 --> 00:24:52,480
install on the github page

00:24:49,600 --> 00:24:54,559
uh of course there's a desk sql github

00:24:52,480 --> 00:24:56,480
issues list so you can see what's broken

00:24:54,559 --> 00:24:57,120
or if you break stuff you can file new

00:24:56,480 --> 00:24:58,880
issues

00:24:57,120 --> 00:25:01,440
uh and there's even a good first issue

00:24:58,880 --> 00:25:04,080
list if you're not sure where to start

00:25:01,440 --> 00:25:05,440
so thank you very much for checking out

00:25:04,080 --> 00:25:07,919
dask sql

00:25:05,440 --> 00:25:09,120
i hope this is useful and allows you to

00:25:07,919 --> 00:25:15,840
do even more

00:25:09,120 --> 00:25:15,840
with python thanks

00:26:15,600 --> 00:26:17,679

YouTube URL: https://www.youtube.com/watch?v=z7xKikaScxg


