Title: TALK   Niels Bantilan   Statistical Typing: A Runtime TypingSystem for Data Science&Machine Learning
Publication date: 2021-05-30
Playlist: PyCon US 2021
Description: 
	Data science and machine learning rely on high quality datasets for visualization, statistical inference, and modeling. However, the barriers to testing data processing, analysis, or model-training code are high, even with the extensive tooling that the python ecosystem offers, such as
pandas, pytest, and hypothesis.

To address this problem, in this talk I define statistical typing as a general concept describing a runtime typing system, which extends primitive data types like bool, str, and float into the class of statistical data types. By providing additional semantics about the properties held by a collection of data points, statistical typing enables us to naturally express types as multivariate schemas. It also enables us to implement schemas as generative data contracts, which serve to both validate data at runtime and generate valid samples for testing purposes.

I'll use pandera, a pandas data testing library, to illustrate how statistical typing makes data testing easier by enabling you to validate real-world data with reusable schemas and isolate units of processing, analysis, and model-training code.

Slides: https://pandera-dev.github.io/pandera-presentations/slides/20210515_pycon_statistical_typing.slides.html
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,880 --> 00:00:19,119
hi everyone i'm niels bentilan

00:00:17,520 --> 00:00:21,119
and i'm excited to present to you at

00:00:19,119 --> 00:00:22,480
pycon this year

00:00:21,119 --> 00:00:24,480
just to give you a little background

00:00:22,480 --> 00:00:25,599
about myself i'm one of the core

00:00:24,480 --> 00:00:27,359
maintainers of flight

00:00:25,599 --> 00:00:28,960
which is an open source machine learning

00:00:27,359 --> 00:00:30,960
orchestration tool

00:00:28,960 --> 00:00:32,239
and i'm also the author of a data frame

00:00:30,960 --> 00:00:34,399
validation tool called

00:00:32,239 --> 00:00:36,320
pandara which is something i'll return

00:00:34,399 --> 00:00:37,920
to a little later

00:00:36,320 --> 00:00:39,760
i want to start my presentation by

00:00:37,920 --> 00:00:41,840
making the claim that

00:00:39,760 --> 00:00:45,280
type systems help programmers reason

00:00:41,840 --> 00:00:47,120
about and write more robust code

00:00:45,280 --> 00:00:49,039
since python takes a gradual typing

00:00:47,120 --> 00:00:50,960
approach you can opt in

00:00:49,039 --> 00:00:52,480
to using type hints and i found that

00:00:50,960 --> 00:00:54,160
they improve readability

00:00:52,480 --> 00:00:57,280
and help me build a better mental model

00:00:54,160 --> 00:00:57,280
of what my code is doing

00:00:57,360 --> 00:01:01,760
to see the benefits of type hints if you

00:01:00,239 --> 00:01:03,280
take a look at this code snippet you can

00:01:01,760 --> 00:01:03,680
see that we're defining a function

00:01:03,280 --> 00:01:08,080
called

00:01:03,680 --> 00:01:11,439
add and double which takes two numbers

00:01:08,080 --> 00:01:14,400
either an int or a float

00:01:11,439 --> 00:01:15,280
and then outputs another number which is

00:01:14,400 --> 00:01:17,360
presumably

00:01:15,280 --> 00:01:19,600
the sum of the two numbers multiplied by

00:01:17,360 --> 00:01:19,600
two

00:01:19,840 --> 00:01:24,000
now i want you to take a few seconds to

00:01:22,479 --> 00:01:26,159
ask yourself

00:01:24,000 --> 00:01:28,479
just with type hints and no actual

00:01:26,159 --> 00:01:30,079
implementation in the function body

00:01:28,479 --> 00:01:32,799
can you predict the outcome of these

00:01:30,079 --> 00:01:35,520
function calls

00:01:32,799 --> 00:01:37,200
we'll review this again later but now i

00:01:35,520 --> 00:01:38,840
think we have enough context

00:01:37,200 --> 00:01:41,759
to get to the central question of my

00:01:38,840 --> 00:01:43,600
presentation which is

00:01:41,759 --> 00:01:45,520
what would a type system geared toward

00:01:43,600 --> 00:01:47,759
data science and machine learning look

00:01:45,520 --> 00:01:47,759
like

00:01:48,720 --> 00:01:53,439
to address this question i'm going to

00:01:50,560 --> 00:01:55,759
introduce you to some of my problems

00:01:53,439 --> 00:01:58,560
define a specification for data types in

00:01:55,759 --> 00:02:00,719
the statistical domain

00:01:58,560 --> 00:02:03,200
demonstrate one way it might be put into

00:02:00,719 --> 00:02:08,160
practice using pandara

00:02:03,200 --> 00:02:09,599
and discuss where this idea can go next

00:02:08,160 --> 00:02:13,120
so first let me introduce you to some of

00:02:09,599 --> 00:02:15,200
my problems there are three

00:02:13,120 --> 00:02:16,959
the first one is that the worst thugs

00:02:15,200 --> 00:02:18,720
are the silent ones

00:02:16,959 --> 00:02:20,840
especially if they're in machine

00:02:18,720 --> 00:02:23,680
learning models that took a long time to

00:02:20,840 --> 00:02:25,599
train

00:02:23,680 --> 00:02:27,440
to see why consider the notion that

00:02:25,599 --> 00:02:30,000
statistical models

00:02:27,440 --> 00:02:31,360
are some kind of compression

00:02:30,000 --> 00:02:35,120
representation

00:02:31,360 --> 00:02:35,120
or approximation of the data

00:02:35,519 --> 00:02:41,280
this implies that if the data changes

00:02:38,239 --> 00:02:43,840
the model changes as well

00:02:41,280 --> 00:02:46,720
so really when i'm using a model for

00:02:43,840 --> 00:02:48,480
explanatory or predictive purposes

00:02:46,720 --> 00:02:50,080
at a high level we're really using the

00:02:48,480 --> 00:02:51,040
model as a function that takes some

00:02:50,080 --> 00:02:54,560
input x

00:02:51,040 --> 00:02:54,560
and produces a result y

00:02:54,879 --> 00:03:00,319
now the question is how do i know if f

00:02:57,840 --> 00:03:02,640
is working as intended

00:03:00,319 --> 00:03:04,080
there are many answers to this question

00:03:02,640 --> 00:03:05,920
but here i'd like to highlight

00:03:04,080 --> 00:03:08,000
two approaches which are well

00:03:05,920 --> 00:03:10,159
established in the software development

00:03:08,000 --> 00:03:12,720
world

00:03:10,159 --> 00:03:14,239
the first approach is to catch certain

00:03:12,720 --> 00:03:18,000
type errors statically

00:03:14,239 --> 00:03:19,519
even before running any code so

00:03:18,000 --> 00:03:21,680
let's return to this example i showed

00:03:19,519 --> 00:03:24,799
you in the beginning

00:03:21,680 --> 00:03:27,519
since python 3.6 we've had type hints

00:03:24,799 --> 00:03:28,879
and tools like my pi which can identify

00:03:27,519 --> 00:03:31,760
errors

00:03:28,879 --> 00:03:33,680
like the middle example here which

00:03:31,760 --> 00:03:34,080
invokes add and double with an invalid

00:03:33,680 --> 00:03:37,519
type

00:03:34,080 --> 00:03:37,519
namely a string hello

00:03:38,879 --> 00:03:43,440
but what if the underlying

00:03:40,080 --> 00:03:44,959
implementation is wrong

00:03:43,440 --> 00:03:47,360
type hints can only get us so far

00:03:44,959 --> 00:03:50,640
because the first and third call to add

00:03:47,360 --> 00:03:53,040
and double are valid invocations

00:03:50,640 --> 00:03:53,680
but the actual output is incorrect

00:03:53,040 --> 00:03:56,000
because

00:03:53,680 --> 00:03:58,400
the function body implements the wrong

00:03:56,000 --> 00:03:58,400
logic

00:03:59,120 --> 00:04:04,000
this is where unit tests come in

00:04:02,239 --> 00:04:06,400
unit tests verify the behavior of

00:04:04,000 --> 00:04:08,319
isolated pieces of functionality

00:04:06,400 --> 00:04:11,840
and let you know when changes cause

00:04:08,319 --> 00:04:11,840
breakages or regressions

00:04:12,159 --> 00:04:15,680
as you can see i've divided up my tests

00:04:14,799 --> 00:04:18,720
in terms of

00:04:15,680 --> 00:04:21,359
happy and sad path tests

00:04:18,720 --> 00:04:23,840
in the happy path tests i've manually

00:04:21,359 --> 00:04:25,440
written examples of valid inputs

00:04:23,840 --> 00:04:27,360
and then test whether the output is

00:04:25,440 --> 00:04:30,160
correct

00:04:27,360 --> 00:04:31,840
and then into sad path tests define

00:04:30,160 --> 00:04:33,520
cases that should raise some sort of

00:04:31,840 --> 00:04:36,000
exception

00:04:33,520 --> 00:04:36,800
note that actually these sad path tests

00:04:36,000 --> 00:04:39,120
right here

00:04:36,800 --> 00:04:40,960
are actually redundant since the type

00:04:39,120 --> 00:04:43,280
lender should be able to catch type

00:04:40,960 --> 00:04:44,400
errors

00:04:43,280 --> 00:04:47,360
you might have noticed that it's

00:04:44,400 --> 00:04:50,000
actually quite burdensome to write

00:04:47,360 --> 00:04:52,000
test cases explicitly so another thing

00:04:50,000 --> 00:04:54,479
we can do to be confident that

00:04:52,000 --> 00:04:56,960
my function works is to define

00:04:54,479 --> 00:05:00,479
property-based tests

00:04:56,960 --> 00:05:02,320
here i'm using hypothesis to define the

00:05:00,479 --> 00:05:03,360
interface of my function as type

00:05:02,320 --> 00:05:05,680
strategies

00:05:03,360 --> 00:05:07,199
there's actually a tutorial here at

00:05:05,680 --> 00:05:09,520
pycon this year

00:05:07,199 --> 00:05:11,280
that focuses exclusively on the

00:05:09,520 --> 00:05:14,240
hypothesis library and

00:05:11,280 --> 00:05:16,240
property based testing so i'd recommend

00:05:14,240 --> 00:05:18,880
checking that out

00:05:16,240 --> 00:05:19,680
but here i'll just give you adjust when

00:05:18,880 --> 00:05:22,960
i run my

00:05:19,680 --> 00:05:25,199
test suite under the hood

00:05:22,960 --> 00:05:26,720
hypothesis generates a bunch of data

00:05:25,199 --> 00:05:28,240
according to those types

00:05:26,720 --> 00:05:29,919
and then attempts to falsify the

00:05:28,240 --> 00:05:33,199
assumptions made in the test

00:05:29,919 --> 00:05:35,039
function body and if it does so it then

00:05:33,199 --> 00:05:36,160
tries to find the smallest human

00:05:35,039 --> 00:05:39,840
readable example

00:05:36,160 --> 00:05:39,840
that falsifies the test case

00:05:41,199 --> 00:05:44,880
all this talk of testing brings me to my

00:05:43,280 --> 00:05:46,960
next problem which is that

00:05:44,880 --> 00:05:48,479
testing code is hard but testing

00:05:46,960 --> 00:05:52,160
statistical code

00:05:48,479 --> 00:05:54,800
is harder

00:05:52,160 --> 00:05:56,479
to see why let's look at a toy example

00:05:54,800 --> 00:05:58,080
where we have a system that ingests

00:05:56,479 --> 00:06:00,560
survey data

00:05:58,080 --> 00:06:02,479
stores responses in a database and

00:06:00,560 --> 00:06:03,120
creates a data set and trains a model to

00:06:02,479 --> 00:06:06,560
predict

00:06:03,120 --> 00:06:08,319
a target of interest

00:06:06,560 --> 00:06:10,080
now here's what your pipeline might look

00:06:08,319 --> 00:06:13,120
like and to help with readability

00:06:10,080 --> 00:06:17,280
i've defined two types one

00:06:13,120 --> 00:06:20,000
that represents the process response

00:06:17,280 --> 00:06:20,639
and another that represents a training

00:06:20,000 --> 00:06:22,240
example

00:06:20,639 --> 00:06:24,880
which consists of a list of floats for

00:06:22,240 --> 00:06:27,280
the features and a boolean value for the

00:06:24,880 --> 00:06:27,280
target

00:06:28,080 --> 00:06:31,680
if we just focus on the store data and

00:06:29,840 --> 00:06:33,759
create dataset functions

00:06:31,680 --> 00:06:35,360
you may notice that storedata's scope of

00:06:33,759 --> 00:06:37,280
concern is atomic

00:06:35,360 --> 00:06:39,280
in that it only operates on a single

00:06:37,280 --> 00:06:41,680
data point

00:06:39,280 --> 00:06:43,520
on the other hand create data set needs

00:06:41,680 --> 00:06:44,400
to worry about the overall statistical

00:06:43,520 --> 00:06:46,639
distribution

00:06:44,400 --> 00:06:49,840
of a data sample when it creates a data

00:06:46,639 --> 00:06:49,840
set for modeling

00:06:50,240 --> 00:06:53,520
now going back to the idea of unit

00:06:51,840 --> 00:06:55,520
testing to be confident that our model

00:06:53,520 --> 00:06:58,400
works as intended

00:06:55,520 --> 00:07:00,160
i'd ideally want to test with some data

00:06:58,400 --> 00:07:02,880
that looks reasonably close

00:07:00,160 --> 00:07:05,039
to what i'd see in the real world so

00:07:02,880 --> 00:07:06,479
what if i want to test create data set

00:07:05,039 --> 00:07:10,400
on a plausible example

00:07:06,479 --> 00:07:13,199
data set unfortunately

00:07:10,400 --> 00:07:14,720
the main answer here is you need to hand

00:07:13,199 --> 00:07:16,400
craft example data

00:07:14,720 --> 00:07:18,000
which often takes the form of panda's

00:07:16,400 --> 00:07:20,319
data frames

00:07:18,000 --> 00:07:22,800
and the most i'll say on this topic is

00:07:20,319 --> 00:07:25,840
it's not fun

00:07:22,800 --> 00:07:27,759
ideally what i'd like to be able to do

00:07:25,840 --> 00:07:29,360
is specify the data types of my

00:07:27,759 --> 00:07:32,479
variables as a schema

00:07:29,360 --> 00:07:34,080
along with additional constraints

00:07:32,479 --> 00:07:36,720
here you can see that i'm importing

00:07:34,080 --> 00:07:39,360
pandera and i'm using it to define

00:07:36,720 --> 00:07:40,800
a schema for the survey data in our toy

00:07:39,360 --> 00:07:43,199
example

00:07:40,800 --> 00:07:44,560
i'm making sure that questions 1 and 2

00:07:43,199 --> 00:07:48,080
have values

00:07:44,560 --> 00:07:49,039
of 1 through 5 and i'm making sure that

00:07:48,080 --> 00:07:52,639
question 3

00:07:49,039 --> 00:07:52,639
matches some regular expression

00:07:57,520 --> 00:08:02,800
i can then use a schema to both validate

00:07:59,919 --> 00:08:04,879
the properties of some data at runtime

00:08:02,800 --> 00:08:08,400
and also sample data under the schema's

00:08:04,879 --> 00:08:08,400
constraints for testing purposes

00:08:09,199 --> 00:08:13,120
now the code you saw in the previous

00:08:10,960 --> 00:08:14,000
slide is all valid pandara syntax but

00:08:13,120 --> 00:08:16,400
actually

00:08:14,000 --> 00:08:18,800
before diving further into pandara as a

00:08:16,400 --> 00:08:20,720
specific implementation

00:08:18,800 --> 00:08:22,400
i want to first define statistical

00:08:20,720 --> 00:08:24,840
typing more generally

00:08:22,400 --> 00:08:27,840
with a few examples to illustrate what i

00:08:24,840 --> 00:08:27,840
mean

00:08:28,000 --> 00:08:33,279
i define statistical typing as a type

00:08:30,639 --> 00:08:36,399
system that extends primitive data types

00:08:33,279 --> 00:08:38,159
like booleans strings and floats

00:08:36,399 --> 00:08:40,159
with additional semantics about the

00:08:38,159 --> 00:08:42,560
properties held by a collection of data

00:08:40,159 --> 00:08:42,560
points

00:08:42,959 --> 00:08:48,160
so for instance the boolean data type

00:08:45,920 --> 00:08:52,320
consists of two possible values

00:08:48,160 --> 00:08:54,080
true and false in statistics speak we

00:08:52,320 --> 00:08:57,279
would call this the support of a

00:08:54,080 --> 00:09:01,040
particular data distribution

00:08:57,279 --> 00:09:01,839
now we can extend booleans to bernoulli

00:09:01,040 --> 00:09:03,680
types

00:09:01,839 --> 00:09:05,519
and to do that we need to supply just

00:09:03,680 --> 00:09:07,360
one more piece of metadata

00:09:05,519 --> 00:09:10,000
which is a probability mass function

00:09:07,360 --> 00:09:13,360
that maps values to probabilities

00:09:10,000 --> 00:09:13,360
all of which must sum to one

00:09:13,519 --> 00:09:17,920
this is sufficient to specify a

00:09:15,760 --> 00:09:18,720
bernoulli distribution that we can name

00:09:17,920 --> 00:09:21,279
for example

00:09:18,720 --> 00:09:22,640
fair coin type and you can imagine

00:09:21,279 --> 00:09:25,200
assigning a variable

00:09:22,640 --> 00:09:27,680
of this type to some data and then

00:09:25,200 --> 00:09:29,920
performing statistical operations on it

00:09:27,680 --> 00:09:32,399
for example getting the mean or mode of

00:09:29,920 --> 00:09:32,399
the data

00:09:33,600 --> 00:09:36,880
we can generalize booleans to

00:09:35,360 --> 00:09:38,640
enumerations

00:09:36,880 --> 00:09:40,720
which gives us a way of expressing a

00:09:38,640 --> 00:09:42,880
type with a finite set of values

00:09:40,720 --> 00:09:44,959
for example we can define a set of

00:09:42,880 --> 00:09:48,080
animals consisting of cats

00:09:44,959 --> 00:09:50,720
dogs and cows

00:09:48,080 --> 00:09:51,680
enums can be extended to categorical

00:09:50,720 --> 00:09:53,600
types

00:09:51,680 --> 00:09:54,880
by again providing a probability mass

00:09:53,600 --> 00:09:56,080
function as you saw in the previous

00:09:54,880 --> 00:09:59,680
slide

00:09:56,080 --> 00:10:03,440
in addition to a flag that indicates

00:09:59,680 --> 00:10:03,440
whether the values are ordered or not

00:10:03,600 --> 00:10:07,440
here you can see we're defining a farm

00:10:05,760 --> 00:10:09,040
animals type with a particular

00:10:07,440 --> 00:10:11,760
distribution of animals that you might

00:10:09,040 --> 00:10:11,760
find at a farm

00:10:13,120 --> 00:10:16,640
you can imagine doing runtime type

00:10:14,959 --> 00:10:18,079
checks on data associated with the

00:10:16,640 --> 00:10:19,760
animal farms type

00:10:18,079 --> 00:10:22,399
to make sure it follows the specified

00:10:19,760 --> 00:10:22,399
distribution

00:10:23,680 --> 00:10:27,600
the last example i wanted to show you is

00:10:26,399 --> 00:10:31,040
extending floats

00:10:27,600 --> 00:10:33,200
into gaussian types this is conceptually

00:10:31,040 --> 00:10:34,800
simple because all you need to specify a

00:10:33,200 --> 00:10:36,000
normal distribution or a gaussian

00:10:34,800 --> 00:10:38,560
distribution

00:10:36,000 --> 00:10:40,880
is the mean and standard deviation and

00:10:38,560 --> 00:10:42,800
here we've defined a tree height type

00:10:40,880 --> 00:10:46,000
with a mean of 10 and a standard

00:10:42,800 --> 00:10:48,160
deviation of one

00:10:46,000 --> 00:10:49,839
cool thing about these types is just

00:10:48,160 --> 00:10:51,279
like the property-based testing example

00:10:49,839 --> 00:10:53,279
that i showed you earlier

00:10:51,279 --> 00:10:55,760
i could potentially use them to sample

00:10:53,279 --> 00:10:58,839
data for the purpose of unit testing

00:10:55,760 --> 00:11:01,200
so if i have a function that processes

00:10:58,839 --> 00:11:03,360
data drawn from the tree height

00:11:01,200 --> 00:11:05,200
distribution

00:11:03,360 --> 00:11:07,680
i should be able to draw samples from

00:11:05,200 --> 00:11:07,680
this type

00:11:08,160 --> 00:11:15,519
pass it into process data and then make

00:11:11,440 --> 00:11:17,200
assertions about the result

00:11:15,519 --> 00:11:18,640
now i'd like to make the point that

00:11:17,200 --> 00:11:21,680
statistical typing

00:11:18,640 --> 00:11:22,640
isn't really new when i first thought of

00:11:21,680 --> 00:11:24,640
the term

00:11:22,640 --> 00:11:25,680
i googled around for a bit and to my

00:11:24,640 --> 00:11:28,000
surprise

00:11:25,680 --> 00:11:29,760
i could only find one or two blog posts

00:11:28,000 --> 00:11:32,640
that specifically used the term in the

00:11:29,760 --> 00:11:34,880
same way that i was thinking about it

00:11:32,640 --> 00:11:37,200
to prove this point i want you to

00:11:34,880 --> 00:11:39,200
consider the following code snippet

00:11:37,200 --> 00:11:40,399
and don't worry too much about the what

00:11:39,200 --> 00:11:43,120
the function actually does

00:11:40,399 --> 00:11:44,800
just look toward the end if you've ever

00:11:43,120 --> 00:11:47,519
written a search statements

00:11:44,800 --> 00:11:48,560
that do runtime validations to check

00:11:47,519 --> 00:11:50,639
whether

00:11:48,560 --> 00:11:52,079
the output fulfills certain assumptions

00:11:50,639 --> 00:11:54,839
like value ranges

00:11:52,079 --> 00:11:56,000
or other such properties then

00:11:54,839 --> 00:11:58,079
congratulations

00:11:56,000 --> 00:12:00,320
you've been doing statistical typing all

00:11:58,079 --> 00:12:02,639
along

00:12:00,320 --> 00:12:03,680
so we can try and codify a few of these

00:12:02,639 --> 00:12:06,320
ideas

00:12:03,680 --> 00:12:08,880
and we can think of statistical types as

00:12:06,320 --> 00:12:11,440
potentially multivariate schemas

00:12:08,880 --> 00:12:13,600
where for each variable i can define a

00:12:11,440 --> 00:12:16,720
few things

00:12:13,600 --> 00:12:19,040
the primitive data type

00:12:16,720 --> 00:12:21,760
a set of deterministic properties that

00:12:19,040 --> 00:12:23,839
the type must adhere to

00:12:21,760 --> 00:12:26,240
and a set of probabilistic properties

00:12:23,839 --> 00:12:26,880
such as the distributions that apply to

00:12:26,240 --> 00:12:30,720
the variable

00:12:26,880 --> 00:12:30,720
along with their sufficient statistics

00:12:31,440 --> 00:12:36,160
the implications of a fully implemented

00:12:33,760 --> 00:12:38,160
statistical type system

00:12:36,160 --> 00:12:39,440
are that some statistical properties can

00:12:38,160 --> 00:12:42,560
be checked statically

00:12:39,440 --> 00:12:46,240
for example you can't really apply

00:12:42,560 --> 00:12:48,240
the mean operation to categorical data

00:12:46,240 --> 00:12:50,000
and yet other properties can only be

00:12:48,240 --> 00:12:52,560
checked at runtime

00:12:50,000 --> 00:12:53,200
for example whether a specific sample of

00:12:52,560 --> 00:12:57,040
data

00:12:53,200 --> 00:12:57,040
was drawn from a gaussian distribution

00:12:57,200 --> 00:13:00,560
and finally the schemas can actually be

00:12:59,360 --> 00:13:02,959
implemented as

00:13:00,560 --> 00:13:04,720
generative data contracts that can be

00:13:02,959 --> 00:13:10,000
used for both type checking

00:13:04,720 --> 00:13:10,000
and data validation and also sampling

00:13:12,160 --> 00:13:15,920
to illustrate this last implication it's

00:13:14,079 --> 00:13:16,720
time to look at a concrete example using

00:13:15,920 --> 00:13:19,680
pandera

00:13:16,720 --> 00:13:21,120
which i'd say is a rough and somewhat

00:13:19,680 --> 00:13:23,600
incomplete implementation

00:13:21,120 --> 00:13:25,839
of statistical typing but many of the

00:13:23,600 --> 00:13:27,600
ideas are there

00:13:25,839 --> 00:13:29,440
so suppose we're building a predictive

00:13:27,600 --> 00:13:31,600
model of house prices given features

00:13:29,440 --> 00:13:33,920
about different houses

00:13:31,600 --> 00:13:36,720
you can see from the raw data that we're

00:13:33,920 --> 00:13:40,880
working with four variables

00:13:36,720 --> 00:13:43,920
square footage number of bedrooms

00:13:40,880 --> 00:13:47,120
property type and price

00:13:43,920 --> 00:13:47,120
which is our target variable

00:13:47,279 --> 00:13:50,880
we can think of our pipeline in two

00:13:48,720 --> 00:13:52,079
steps the first one being to process the

00:13:50,880 --> 00:13:54,079
raw data

00:13:52,079 --> 00:13:57,360
and the second one being to train a

00:13:54,079 --> 00:13:57,360
model on the process data

00:13:58,160 --> 00:14:02,399
and now we can define schemas in pandara

00:14:00,639 --> 00:14:04,240
which in practice actually requires a

00:14:02,399 --> 00:14:06,720
little bit of data exploration to get a

00:14:04,240 --> 00:14:10,399
sense of what the data look like

00:14:06,720 --> 00:14:13,440
but here we are defining a base schema

00:14:10,399 --> 00:14:16,639
which we'll use as the foundational type

00:14:13,440 --> 00:14:21,040
and we'll have our raw and

00:14:16,639 --> 00:14:22,720
process data schemas inherit from it

00:14:21,040 --> 00:14:24,320
from these schema definitions you can

00:14:22,720 --> 00:14:26,240
immediately see

00:14:24,320 --> 00:14:27,680
which variables the raw and process data

00:14:26,240 --> 00:14:30,959
have in common

00:14:27,680 --> 00:14:33,040
which is these three but

00:14:30,959 --> 00:14:34,720
you can also see what the differences

00:14:33,040 --> 00:14:37,760
are namely that

00:14:34,720 --> 00:14:39,360
the raw data schema has a property type

00:14:37,760 --> 00:14:41,600
variable containing strings that need to

00:14:39,360 --> 00:14:45,839
be converted into a set of

00:14:41,600 --> 00:14:45,839
binary indicator variables

00:14:47,199 --> 00:14:51,600
we can then add type annotations to our

00:14:49,680 --> 00:14:52,639
process data and train model functions

00:14:51,600 --> 00:14:54,639
to make sure that

00:14:52,639 --> 00:14:56,959
the inputs and outputs conform with the

00:14:54,639 --> 00:14:59,040
schema definitions

00:14:56,959 --> 00:15:01,199
and we can fill our functions in with

00:14:59,040 --> 00:15:03,440
actual implementations

00:15:01,199 --> 00:15:05,519
um and here i'm using pandas and sklearn

00:15:03,440 --> 00:15:07,839
to process and train

00:15:05,519 --> 00:15:07,839
model

00:15:08,639 --> 00:15:12,480
now the cool thing about this is that

00:15:10,720 --> 00:15:14,079
our processing and training functions

00:15:12,480 --> 00:15:16,720
now inherently validate

00:15:14,079 --> 00:15:18,560
our raw and process data every time we

00:15:16,720 --> 00:15:20,160
invoke those functions

00:15:18,560 --> 00:15:22,639
for example when we run it in our

00:15:20,160 --> 00:15:22,639
pipeline

00:15:23,440 --> 00:15:27,680
and if we happen to ingest invalid data

00:15:25,920 --> 00:15:29,440
the pipeline fails early and were

00:15:27,680 --> 00:15:30,959
provided useful information about what

00:15:29,440 --> 00:15:34,240
exactly went wrong

00:15:30,959 --> 00:15:37,279
in this case an unknown value in the

00:15:34,240 --> 00:15:37,279
property type column

00:15:37,680 --> 00:15:41,920
and to circle back to property-based

00:15:39,199 --> 00:15:43,279
testing pandera exposes a strategy

00:15:41,920 --> 00:15:46,079
method

00:15:43,279 --> 00:15:48,320
that compiles the schema metadata into a

00:15:46,079 --> 00:15:51,120
hypothesis strategy that you can use in

00:15:48,320 --> 00:15:51,120
your unit tests

00:15:53,120 --> 00:15:57,759
this becomes useful because in the case

00:15:55,759 --> 00:16:00,480
that we've implemented something wrong

00:15:57,759 --> 00:16:02,000
we'll get a useful error message too

00:16:00,480 --> 00:16:04,240
here pandara is complaining that the

00:16:02,000 --> 00:16:07,279
output of process data doesn't contain

00:16:04,240 --> 00:16:08,959
the property type condo variable which

00:16:07,279 --> 00:16:11,199
is the case because

00:16:08,959 --> 00:16:13,120
we've wrongly implemented our function

00:16:11,199 --> 00:16:13,759
to just return the raw data instead of

00:16:13,120 --> 00:16:16,720
actually

00:16:13,759 --> 00:16:16,720
processing the data

00:16:18,160 --> 00:16:22,639
and finally you can even bootstrap a

00:16:20,079 --> 00:16:24,399
schema from a sample of data because it

00:16:22,639 --> 00:16:26,000
can be tedious to write a schema from

00:16:24,399 --> 00:16:28,240
scratch

00:16:26,000 --> 00:16:29,360
all you have to do is call the in first

00:16:28,240 --> 00:16:31,920
game a function

00:16:29,360 --> 00:16:32,399
which you can then write out to a yaml

00:16:31,920 --> 00:16:34,839
file

00:16:32,399 --> 00:16:37,839
or a python script to further edit and

00:16:34,839 --> 00:16:37,839
refine

00:16:38,160 --> 00:16:41,440
to sum up the practical use cases of

00:16:40,160 --> 00:16:44,160
statistical typing

00:16:41,440 --> 00:16:46,079
and pandera in particular you can use it

00:16:44,160 --> 00:16:47,440
in the context of continuous integration

00:16:46,079 --> 00:16:49,600
tests for your etl

00:16:47,440 --> 00:16:50,880
or modeling pipelines but you can also

00:16:49,600 --> 00:16:53,360
use it for alerting

00:16:50,880 --> 00:16:54,959
when detecting data set shift or

00:16:53,360 --> 00:16:58,000
monitoring the quality of your model's

00:16:54,959 --> 00:16:58,000
predictions in production

00:16:58,079 --> 00:17:02,399
but to go from practical use cases into

00:17:00,160 --> 00:17:04,319
theoretical applications

00:17:02,399 --> 00:17:05,919
i want to end with a few ideas that

00:17:04,319 --> 00:17:07,679
might be interesting to consider when

00:17:05,919 --> 00:17:09,199
thinking about the question of

00:17:07,679 --> 00:17:11,919
type systems for data science and

00:17:09,199 --> 00:17:11,919
machine learning

00:17:12,400 --> 00:17:15,520
the first idea is that it would be

00:17:14,079 --> 00:17:18,880
really nice to be able to

00:17:15,520 --> 00:17:21,039
statically analyze code in order to call

00:17:18,880 --> 00:17:22,000
out cases where statistical operations

00:17:21,039 --> 00:17:25,520
don't make sense

00:17:22,000 --> 00:17:28,160
given some type so returning to the

00:17:25,520 --> 00:17:29,280
farm animal example from earlier this

00:17:28,160 --> 00:17:31,600
would mean that even

00:17:29,280 --> 00:17:33,280
before running code we could tell that

00:17:31,600 --> 00:17:34,000
computing the mean of a categorical

00:17:33,280 --> 00:17:36,559
distribution

00:17:34,000 --> 00:17:39,679
doesn't make sense and our type lender

00:17:36,559 --> 00:17:39,679
should be able to tell us that

00:17:40,080 --> 00:17:44,320
the second idea is that it would be

00:17:42,240 --> 00:17:45,120
possible to infer the model architecture

00:17:44,320 --> 00:17:46,960
space

00:17:45,120 --> 00:17:48,799
based on function signatures with

00:17:46,960 --> 00:17:50,640
statistical types

00:17:48,799 --> 00:17:53,679
so in theory if i have a function that

00:17:50,640 --> 00:17:55,679
takes a gaussian distribution as input

00:17:53,679 --> 00:17:56,799
and then outputs a bernoulli

00:17:55,679 --> 00:17:58,960
distribution

00:17:56,799 --> 00:18:00,640
we should be able to infer the space of

00:17:58,960 --> 00:18:03,760
valid model architectures that could

00:18:00,640 --> 00:18:03,760
approximate this function

00:18:04,240 --> 00:18:12,080
and finally maybe the wackiest idea

00:18:07,600 --> 00:18:14,240
is to infer statistical types from data

00:18:12,080 --> 00:18:16,080
and the rationale behind this is that

00:18:14,240 --> 00:18:18,320
it's not always the case

00:18:16,080 --> 00:18:20,160
that my data can be neatly categorized

00:18:18,320 --> 00:18:23,120
by the theoretical statistical

00:18:20,160 --> 00:18:26,240
distributions we know about today

00:18:23,120 --> 00:18:28,559
for example image data or natural

00:18:26,240 --> 00:18:30,559
language data might be drawn from a very

00:18:28,559 --> 00:18:34,240
complex manifold that would be

00:18:30,559 --> 00:18:36,799
impossible to write down manually

00:18:34,240 --> 00:18:38,320
in this case we would want a schema

00:18:36,799 --> 00:18:40,400
inference routine that could be

00:18:38,320 --> 00:18:42,640
arbitrarily complex

00:18:40,400 --> 00:18:45,360
to describe statistical types that can

00:18:42,640 --> 00:18:47,840
also be arbitrarily complex

00:18:45,360 --> 00:18:49,280
using data that can be encoded as a

00:18:47,840 --> 00:18:51,280
statistical model

00:18:49,280 --> 00:18:54,640
where the artifacts can lend themselves

00:18:51,280 --> 00:18:54,640
as components in a schema

00:18:55,679 --> 00:19:00,559
so to illustrate this final point let's

00:18:57,919 --> 00:19:02,720
consider generative adversarial networks

00:19:00,559 --> 00:19:05,919
or gans for short

00:19:02,720 --> 00:19:07,600
in siri a gan can be used as a schema to

00:19:05,919 --> 00:19:11,440
validate real-world data

00:19:07,600 --> 00:19:13,039
but also generate synthetic data

00:19:11,440 --> 00:19:15,280
the way it works is during training you

00:19:13,039 --> 00:19:18,240
draw real samples from the real world

00:19:15,280 --> 00:19:19,600
and then fake samples from a generator

00:19:18,240 --> 00:19:21,440
whose objective is to fool the

00:19:19,600 --> 00:19:24,880
discriminator into thinking that its

00:19:21,440 --> 00:19:26,400
synthetic data is real conversely

00:19:24,880 --> 00:19:28,480
you have the discriminator whose

00:19:26,400 --> 00:19:32,480
objective is to accurately tell

00:19:28,480 --> 00:19:32,480
when a data point is real or fake

00:19:32,720 --> 00:19:37,200
typically after training again people

00:19:35,600 --> 00:19:40,480
only care about the generator

00:19:37,200 --> 00:19:41,440
and discard the discriminator however in

00:19:40,480 --> 00:19:43,600
our case

00:19:41,440 --> 00:19:45,440
we can actually use the discriminator to

00:19:43,600 --> 00:19:46,799
validate data by telling us whether a

00:19:45,440 --> 00:19:50,000
particular data point

00:19:46,799 --> 00:19:51,760
is real or fake we could also use the

00:19:50,000 --> 00:19:53,919
generator for its primary purpose of

00:19:51,760 --> 00:19:56,640
synthesizing data for testing our model

00:19:53,919 --> 00:19:56,640
training functions

00:19:57,840 --> 00:20:03,039
in effect this would imply that we can

00:20:01,039 --> 00:20:04,640
implement validation and data synthesis

00:20:03,039 --> 00:20:07,679
modules for complex

00:20:04,640 --> 00:20:09,840
statistical types for example

00:20:07,679 --> 00:20:13,360
an image data set with a particular set

00:20:09,840 --> 00:20:15,840
of categories describing those images

00:20:13,360 --> 00:20:17,039
in siri it would be possible to define a

00:20:15,840 --> 00:20:19,039
schema like this

00:20:17,039 --> 00:20:20,960
where we have an image type that points

00:20:19,039 --> 00:20:22,880
to a jpeg file

00:20:20,960 --> 00:20:25,120
and at validation time that file is read

00:20:22,880 --> 00:20:27,120
into memory and passed into again to

00:20:25,120 --> 00:20:28,400
verify that the image is drawn from a

00:20:27,120 --> 00:20:31,440
similar distribution

00:20:28,400 --> 00:20:33,200
as what we saw during training time

00:20:31,440 --> 00:20:34,960
this might be useful in production to at

00:20:33,200 --> 00:20:37,520
least warn us when an image is out of

00:20:34,960 --> 00:20:37,520
distribution

00:20:38,400 --> 00:20:42,880
so i'm hoping that in this last section

00:20:40,960 --> 00:20:44,880
i've given you some food for thought

00:20:42,880 --> 00:20:46,559
and really the main takeaway i want to

00:20:44,880 --> 00:20:49,200
leave you with is that

00:20:46,559 --> 00:20:50,400
statistical typing extends primitive

00:20:49,200 --> 00:20:52,400
data types

00:20:50,400 --> 00:20:53,840
by enforcing a set of deterministic and

00:20:52,400 --> 00:20:56,320
probabilistic properties

00:20:53,840 --> 00:20:58,480
that must be held by a collection of

00:20:56,320 --> 00:21:00,240
data points

00:20:58,480 --> 00:21:02,000
this opens up a bunch of testing

00:21:00,240 --> 00:21:03,760
capabilities that make the code that we

00:21:02,000 --> 00:21:05,039
write as data scientists and machine

00:21:03,760 --> 00:21:09,520
learning practitioners

00:21:05,039 --> 00:21:09,520
more robust and easier to reason about

00:21:10,480 --> 00:21:14,480
and with that i'd like to thank you for

00:21:12,799 --> 00:21:16,559
your time and attention

00:21:14,480 --> 00:21:17,840
please feel free to reach out to me via

00:21:16,559 --> 00:21:20,400
email twitter or

00:21:17,840 --> 00:21:21,679
github and i hope you got something out

00:21:20,400 --> 00:21:31,840
of this talk

00:21:21,679 --> 00:21:31,840
enjoy the rest of your conference

00:22:24,000 --> 00:22:26,080

YouTube URL: https://www.youtube.com/watch?v=PI5UmKi14cM


