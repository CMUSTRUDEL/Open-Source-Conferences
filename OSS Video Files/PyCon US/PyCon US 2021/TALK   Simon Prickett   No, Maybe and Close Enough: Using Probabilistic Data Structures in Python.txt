Title: TALK   Simon Prickett   No, Maybe and Close Enough: Using Probabilistic Data Structures in Python
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Being right all the time isn't necessarily the best idea. This talk examines how to count distinct items from a firehose of data, how to determine if we've seen a given item before, and why absolute accuracy may be impractical when doing so.

Probabilistic data structures trade accuracy for approximate results, speed and economy of resources. They provide fast, scalable solutions to problems such as counting likes on social media posts, or determining which articles on a website a user has previously read.

I'll introduce the Hyperloglog and Bloom Filter, explain how they work at a high level, and demonstrate different ways in which each can be leveraged in Python.

A GitHub repo to accompany this talk can be found at https://github.com/simonprickett/python-probabilistic-data-structures

Slides: https://simonprickett.dev/no_maybe_and_close_enough_slides.pdf
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,400 --> 00:00:17,279
hello

00:00:14,960 --> 00:00:18,880
my name is simon prickett and this is no

00:00:17,279 --> 00:00:19,760
maybe and close enough

00:00:18,880 --> 00:00:21,600
where we're going to look at some

00:00:19,760 --> 00:00:24,000
probabilistic data structures with

00:00:21,600 --> 00:00:25,439
python

00:00:24,000 --> 00:00:27,439
so the problem we're going to look at

00:00:25,439 --> 00:00:30,560
today is related to

00:00:27,439 --> 00:00:32,320
counting things so counting things seems

00:00:30,560 --> 00:00:34,239
quite easy on the face of it we

00:00:32,320 --> 00:00:36,320
just maintain a count of the things that

00:00:34,239 --> 00:00:37,840
we want to count and every time we see a

00:00:36,320 --> 00:00:40,399
new and different thing we

00:00:37,840 --> 00:00:41,920
add one to that count so how hard can

00:00:40,399 --> 00:00:44,800
this be

00:00:41,920 --> 00:00:45,200
so let's assume we want to count cheap

00:00:44,800 --> 00:00:47,039
so

00:00:45,200 --> 00:00:48,480
i want to count sheep and i'm doing that

00:00:47,039 --> 00:00:50,399
in python

00:00:48,480 --> 00:00:51,600
so i have here a very simple python

00:00:50,399 --> 00:00:54,239
program that does

00:00:51,600 --> 00:00:55,920
exactly that so python has a set data

00:00:54,239 --> 00:00:57,760
structure built into it

00:00:55,920 --> 00:01:00,320
these are great for this sort of problem

00:00:57,760 --> 00:01:02,239
because we can add things to a set

00:01:00,320 --> 00:01:03,760
and if we add them multiple times it

00:01:02,239 --> 00:01:05,360
will deduplicate them

00:01:03,760 --> 00:01:07,840
and then we can ask it how many things

00:01:05,360 --> 00:01:08,799
are in the set so on the face of it we

00:01:07,840 --> 00:01:11,840
can answer the question

00:01:08,799 --> 00:01:14,080
how many sheep have i seen with a set so

00:01:11,840 --> 00:01:15,439
here i'm declaring a set and then adding

00:01:14,080 --> 00:01:19,040
some sheep uh ear

00:01:15,439 --> 00:01:22,560
id tags to it so 1934 1201

00:01:19,040 --> 00:01:24,479
1199 etc then further down you'll see i

00:01:22,560 --> 00:01:25,439
have 1934 again

00:01:24,479 --> 00:01:27,520
that's actually going to get

00:01:25,439 --> 00:01:29,200
deduplicated so when we ask this how

00:01:27,520 --> 00:01:31,119
many sheep are in the

00:01:29,200 --> 00:01:33,200
in the set of sheep that we've seen by

00:01:31,119 --> 00:01:35,520
using the len function

00:01:33,200 --> 00:01:37,439
it's not going to count that one twice

00:01:35,520 --> 00:01:39,360
so that's perfect we've got

00:01:37,439 --> 00:01:40,880
an exact number of how many sheep we've

00:01:39,360 --> 00:01:43,439
seen

00:01:40,880 --> 00:01:44,880
so another question that i might want to

00:01:43,439 --> 00:01:47,840
ask when i'm counting things

00:01:44,880 --> 00:01:48,159
is not just how many sheep have i seen

00:01:47,840 --> 00:01:52,000
but

00:01:48,159 --> 00:01:54,640
have i seen this particular shape so

00:01:52,000 --> 00:01:55,280
in this case i need to be able to

00:01:54,640 --> 00:01:57,600
retrieve

00:01:55,280 --> 00:01:58,880
data from my set or data structure that

00:01:57,600 --> 00:02:00,479
i'm using

00:01:58,880 --> 00:02:02,799
to determine if we've seen this before

00:02:00,479 --> 00:02:05,759
so it's a sort of set membership query

00:02:02,799 --> 00:02:08,080
is sheet 1934 in the set of sheet that

00:02:05,759 --> 00:02:11,280
we've seen for example

00:02:08,080 --> 00:02:12,959
so here again i'm using a python set and

00:02:11,280 --> 00:02:14,080
this seems to be a great fit for this

00:02:12,959 --> 00:02:16,160
problem so

00:02:14,080 --> 00:02:17,599
i declare the set with some sheet tags

00:02:16,160 --> 00:02:20,720
that we've seen already so

00:02:17,599 --> 00:02:22,720
1934 1201 etc

00:02:20,720 --> 00:02:23,760
then i have a simple function that just

00:02:22,720 --> 00:02:26,400
basically says

00:02:23,760 --> 00:02:28,319
is the sheep id passed to it in the set

00:02:26,400 --> 00:02:30,800
of sheep that we've seen

00:02:28,319 --> 00:02:32,319
and it either is or it isn't and that's

00:02:30,800 --> 00:02:33,200
going to work perfectly and it's going

00:02:32,319 --> 00:02:36,400
to be reliable

00:02:33,200 --> 00:02:38,400
100 of the time so when we call have i

00:02:36,400 --> 00:02:40,400
seen 1934

00:02:38,400 --> 00:02:41,840
it's going to say yes we have seen sheep

00:02:40,400 --> 00:02:44,000
00:02:41,840 --> 00:02:45,760
have we seen 1283 it's going to say no

00:02:44,000 --> 00:02:46,239
we haven't seen that one before or at

00:02:45,760 --> 00:02:49,760
least

00:02:46,239 --> 00:02:50,879
not yet so when we're counting things

00:02:49,760 --> 00:02:51,760
and we want to answer these two

00:02:50,879 --> 00:02:54,800
questions

00:02:51,760 --> 00:02:55,840
how many things have i seen or distinct

00:02:54,800 --> 00:02:57,920
things have i seen

00:02:55,840 --> 00:02:59,440
and have i seen this particularly

00:02:57,920 --> 00:03:01,680
distinct thing

00:02:59,440 --> 00:03:02,840
then a set got us covered it's done

00:03:01,680 --> 00:03:06,640
everything we need

00:03:02,840 --> 00:03:10,159
so that's it we're done um

00:03:06,640 --> 00:03:13,519
problem solved but well

00:03:10,159 --> 00:03:15,200
are we so the set works great and it's

00:03:13,519 --> 00:03:18,720
100 accurate

00:03:15,200 --> 00:03:21,280
but we had a relatively small data set

00:03:18,720 --> 00:03:23,200
so we had a few sheep and we're

00:03:21,280 --> 00:03:26,239
remembering the ids of all the sheep

00:03:23,200 --> 00:03:27,920
and the ids were fairly short so

00:03:26,239 --> 00:03:29,680
remembering all of those in a set and

00:03:27,920 --> 00:03:33,360
storing all of that data is not

00:03:29,680 --> 00:03:35,440
a huge problem but if we need to count

00:03:33,360 --> 00:03:36,159
huge numbers of distinct things so we're

00:03:35,440 --> 00:03:38,000
operating at

00:03:36,159 --> 00:03:40,239
internet scale or we're operating at

00:03:38,000 --> 00:03:41,519
sort of australia new zealand sheep farm

00:03:40,239 --> 00:03:43,760
scale

00:03:41,519 --> 00:03:45,680
then we might need to think about this

00:03:43,760 --> 00:03:47,760
again because we might have some issues

00:03:45,680 --> 00:03:50,319
here

00:03:47,760 --> 00:03:51,760
so at scale when things get big with

00:03:50,319 --> 00:03:53,519
counting things

00:03:51,760 --> 00:03:55,200
we start to hit problems with for

00:03:53,519 --> 00:03:57,200
example memory usage

00:03:55,200 --> 00:03:59,360
so remembering all of those things in a

00:03:57,200 --> 00:04:01,040
set starts to get expensive in terms of

00:03:59,360 --> 00:04:03,040
the amount of memory that that set

00:04:01,040 --> 00:04:06,080
requires

00:04:03,040 --> 00:04:07,840
to be stored it also gives us a problem

00:04:06,080 --> 00:04:09,360
of horizontal scaling so if we're

00:04:07,840 --> 00:04:10,239
counting lots and lots and lots of

00:04:09,360 --> 00:04:11,519
things

00:04:10,239 --> 00:04:13,439
the chances are it's not just going to

00:04:11,519 --> 00:04:15,280
be one person or one process

00:04:13,439 --> 00:04:17,840
out there counting things and using a

00:04:15,280 --> 00:04:18,479
local in-memory process variable to do

00:04:17,840 --> 00:04:20,079
it

00:04:18,479 --> 00:04:21,680
we're going to have several processes

00:04:20,079 --> 00:04:22,960
counting things and they're going to

00:04:21,680 --> 00:04:25,759
want to count together

00:04:22,960 --> 00:04:28,160
and maintain a common counter so we need

00:04:25,759 --> 00:04:29,680
a way of sharing our counters

00:04:28,160 --> 00:04:31,360
and making sure that updates to them are

00:04:29,680 --> 00:04:34,720
atomic so that

00:04:31,360 --> 00:04:35,520
we do not get false counts or we don't

00:04:34,720 --> 00:04:37,759
have a problem of

00:04:35,520 --> 00:04:38,560
if one counter goes down we've lost some

00:04:37,759 --> 00:04:40,800
of the data

00:04:38,560 --> 00:04:42,720
or we can't fit all the data in a single

00:04:40,800 --> 00:04:46,560
process is memory

00:04:42,720 --> 00:04:49,440
so once we get to scale counting things

00:04:46,560 --> 00:04:50,160
exactly starts to get very expensive in

00:04:49,440 --> 00:04:53,040
terms of

00:04:50,160 --> 00:04:54,080
memory usage potentially time

00:04:53,040 --> 00:04:57,120
performance

00:04:54,080 --> 00:05:00,000
and concurrency um

00:04:57,120 --> 00:05:01,840
now one way that we might want to

00:05:00,000 --> 00:05:04,880
resolve that would be to

00:05:01,840 --> 00:05:08,639
move the counting problem out of memory

00:05:04,880 --> 00:05:11,199
and into say a database so

00:05:08,639 --> 00:05:13,360
here i'm using a database i'm using the

00:05:11,199 --> 00:05:13,759
redis database for the reason that it

00:05:13,360 --> 00:05:16,000
has

00:05:13,759 --> 00:05:18,320
a set data structure so we can take the

00:05:16,000 --> 00:05:20,720
set that we were using in python

00:05:18,320 --> 00:05:22,080
and we can move that out of the python

00:05:20,720 --> 00:05:25,280
and into redis

00:05:22,080 --> 00:05:26,320
so this is a fairly simple code change

00:05:25,280 --> 00:05:28,000
we now just

00:05:26,320 --> 00:05:29,759
create a redis connection using the

00:05:28,000 --> 00:05:32,639
redis module

00:05:29,759 --> 00:05:33,120
and we basically tell it what the key

00:05:32,639 --> 00:05:34,960
name

00:05:33,120 --> 00:05:37,199
of the redis set is that we want to

00:05:34,960 --> 00:05:40,160
store our sheet counts in

00:05:37,199 --> 00:05:41,919
and we just s add things to it so in

00:05:40,160 --> 00:05:42,960
python where we were doing dot ads to

00:05:41,919 --> 00:05:46,479
add things to a set

00:05:42,960 --> 00:05:48,479
redis is dot s add for set add

00:05:46,479 --> 00:05:49,840
same sort of thing we say which set we

00:05:48,479 --> 00:05:51,520
want to put it in because we're now a

00:05:49,840 --> 00:05:52,080
database so we can store multiple of

00:05:51,520 --> 00:05:55,120
these

00:05:52,080 --> 00:05:56,160
in a key value store way and we give it

00:05:55,120 --> 00:05:58,319
the tags

00:05:56,160 --> 00:06:00,880
the same behavior happens so when i have

00:05:58,319 --> 00:06:04,000
1934 a second time

00:06:00,880 --> 00:06:06,080
1934 will be deduplicated

00:06:04,000 --> 00:06:07,600
and now because we're using redis for

00:06:06,080 --> 00:06:10,000
this and it's out of the

00:06:07,600 --> 00:06:10,720
python process and accessible across the

00:06:10,000 --> 00:06:13,759
network

00:06:10,720 --> 00:06:15,199
we can connect multiple counters to it

00:06:13,759 --> 00:06:17,360
so we can solve a couple of problems

00:06:15,199 --> 00:06:18,720
here we can solve the problem of

00:06:17,360 --> 00:06:20,400
what if i have a load of people out

00:06:18,720 --> 00:06:21,600
there counting the sheep and we want to

00:06:20,400 --> 00:06:24,560
maintain a

00:06:21,600 --> 00:06:25,759
centralized overall count and we've

00:06:24,560 --> 00:06:29,039
solved the problem

00:06:25,759 --> 00:06:32,240
of the memory limitations

00:06:29,039 --> 00:06:34,000
in a given process so the process is no

00:06:32,240 --> 00:06:34,479
longer becoming a memory hog with all of

00:06:34,000 --> 00:06:37,039
these

00:06:34,479 --> 00:06:38,960
sheep ids in the set we've moved that

00:06:37,039 --> 00:06:42,400
out to a database so in this case

00:06:38,960 --> 00:06:46,000
redis but we've still

00:06:42,400 --> 00:06:49,120
got the problem here of

00:06:46,000 --> 00:06:52,160
overall size so as we add more and more

00:06:49,120 --> 00:06:54,000
and more sheep the data set

00:06:52,160 --> 00:06:55,599
is still going to take up a reasonable

00:06:54,000 --> 00:06:56,479
amount of space and that's going to grow

00:06:55,599 --> 00:06:59,120
according to

00:06:56,479 --> 00:07:00,800
how we add sheep and if we were using

00:06:59,120 --> 00:07:02,080
longer tags it would grow more every

00:07:00,800 --> 00:07:04,800
time we added a new

00:07:02,080 --> 00:07:07,280
item because we're having to store the

00:07:04,800 --> 00:07:07,280
items

00:07:07,680 --> 00:07:12,400
so let's have a look at how we can

00:07:10,880 --> 00:07:15,039
determine if we've seen this sheet

00:07:12,400 --> 00:07:17,680
before when using a database as well

00:07:15,039 --> 00:07:19,360
so here we are again using redis so

00:07:17,680 --> 00:07:22,240
imagine we put all of our data

00:07:19,360 --> 00:07:23,520
into that set and we've now got shared

00:07:22,240 --> 00:07:24,960
counters and

00:07:23,520 --> 00:07:27,360
lots of people can go out and count the

00:07:24,960 --> 00:07:28,960
sheep and to know if we've seen this

00:07:27,360 --> 00:07:31,199
sheet before

00:07:28,960 --> 00:07:33,199
we then basically have a new havoi scene

00:07:31,199 --> 00:07:35,280
function and some preamble before it

00:07:33,199 --> 00:07:37,599
that clears out any old set and redis

00:07:35,280 --> 00:07:39,520
and sets some sample data

00:07:37,599 --> 00:07:40,639
what we're going to do now is instead of

00:07:39,520 --> 00:07:43,680
using an if

00:07:40,639 --> 00:07:46,000
a sheet tag is in the set

00:07:43,680 --> 00:07:47,919
like we did with the python set we're

00:07:46,000 --> 00:07:48,800
going to use a redis command called s's

00:07:47,919 --> 00:07:51,440
member so

00:07:48,800 --> 00:07:51,919
set is member and we're going to say if

00:07:51,440 --> 00:07:54,879
this

00:07:51,919 --> 00:07:55,199
cheap id is in the set then we've seen

00:07:54,879 --> 00:07:57,919
it

00:07:55,199 --> 00:07:59,599
otherwise we haven't as we'd expect that

00:07:57,919 --> 00:08:01,360
works exactly the same as it does in

00:07:59,599 --> 00:08:02,400
python with sets but we've solved these

00:08:01,360 --> 00:08:04,800
two problems we've solved the

00:08:02,400 --> 00:08:07,680
concurrency problem we've solved the uh

00:08:04,800 --> 00:08:09,520
individual process memory limit problem

00:08:07,680 --> 00:08:10,000
but we've really just moved that memory

00:08:09,520 --> 00:08:14,160
problem

00:08:10,000 --> 00:08:16,080
into the database itself so

00:08:14,160 --> 00:08:18,160
to solve that and to enable counting it

00:08:16,080 --> 00:08:19,599
like really large scale without chewing

00:08:18,160 --> 00:08:21,520
through a lot of memory we're gonna need

00:08:19,599 --> 00:08:24,240
to make some trade-offs

00:08:21,520 --> 00:08:25,120
so trade-offs basically involve giving

00:08:24,240 --> 00:08:27,120
up one thing

00:08:25,120 --> 00:08:28,960
in exchange for another so our sheep on

00:08:27,120 --> 00:08:30,720
the left there has its fleece our sheep

00:08:28,960 --> 00:08:32,159
on the right has given up its fleece

00:08:30,720 --> 00:08:34,959
in exchange for being a little bit

00:08:32,159 --> 00:08:35,680
cooler but we can determine that both

00:08:34,959 --> 00:08:38,000
are sheep

00:08:35,680 --> 00:08:39,120
so this is kind of a key thing here is

00:08:38,000 --> 00:08:42,479
we've been storing

00:08:39,120 --> 00:08:44,399
the whole data set and all of the data

00:08:42,479 --> 00:08:46,000
to determine which sheet we've seen but

00:08:44,399 --> 00:08:48,000
can we get away with storing

00:08:46,000 --> 00:08:50,240
something about the data or bits of the

00:08:48,000 --> 00:08:50,560
data and still know that it's that sheep

00:08:50,240 --> 00:08:51,839
so

00:08:50,560 --> 00:08:53,519
the sheep on the left and sheep on the

00:08:51,839 --> 00:08:57,040
right we still sell their sheep even

00:08:53,519 --> 00:08:58,959
though one has lost its fleece

00:08:57,040 --> 00:09:01,120
so this is where something called

00:08:58,959 --> 00:09:04,240
probabilistic data structures come in

00:09:01,120 --> 00:09:07,120
these are a family of data structures

00:09:04,240 --> 00:09:08,000
that make some trade-offs so rather than

00:09:07,120 --> 00:09:10,240
being completely

00:09:08,000 --> 00:09:11,920
accurate they will trade off accuracy

00:09:10,240 --> 00:09:14,000
for some storage efficiency

00:09:11,920 --> 00:09:16,320
so we'll see we can save a lot of memory

00:09:14,000 --> 00:09:17,839
by giving up a bit of accuracy

00:09:16,320 --> 00:09:20,160
we might also trade off some

00:09:17,839 --> 00:09:22,160
functionality so as we'll see we can

00:09:20,160 --> 00:09:24,000
save a lot of memory by not actually

00:09:22,160 --> 00:09:25,680
storing the data which means we can no

00:09:24,000 --> 00:09:26,880
longer get a list back of what sheet

00:09:25,680 --> 00:09:28,640
we've seen

00:09:26,880 --> 00:09:31,440
but we can still determine whether we've

00:09:28,640 --> 00:09:32,800
seen a sheet with reasonable accuracy

00:09:31,440 --> 00:09:34,160
the other trade-off that's often

00:09:32,800 --> 00:09:34,800
involved with probabilistic data

00:09:34,160 --> 00:09:37,040
structures

00:09:34,800 --> 00:09:40,080
is performance but we'll mostly be

00:09:37,040 --> 00:09:40,080
looking at these three

00:09:40,560 --> 00:09:43,920
so we have two questions that we wanted

00:09:42,800 --> 00:09:46,959
to ask here

00:09:43,920 --> 00:09:48,320
so how many sheep have i seen is the

00:09:46,959 --> 00:09:50,560
first one

00:09:48,320 --> 00:09:52,320
and a data structure or an algorithm

00:09:50,560 --> 00:09:54,080
that we can use for that that comes from

00:09:52,320 --> 00:09:54,959
these probabilistic data structures

00:09:54,080 --> 00:09:57,600
family

00:09:54,959 --> 00:09:58,000
is called the hyperlog log so what that

00:09:57,600 --> 00:10:00,959
does

00:09:58,000 --> 00:10:02,399
is it approximates distinct items so it

00:10:00,959 --> 00:10:05,360
basically

00:10:02,399 --> 00:10:05,920
guesses at the cardinality of a set

00:10:05,360 --> 00:10:09,360
based

00:10:05,920 --> 00:10:11,519
on um not actually storing the data

00:10:09,360 --> 00:10:13,600
but hashing the data and storing

00:10:11,519 --> 00:10:16,480
information about it

00:10:13,600 --> 00:10:18,000
so there's some pros and cons to this so

00:10:16,480 --> 00:10:19,440
the way the hyperlog log works

00:10:18,000 --> 00:10:21,519
is it's going to run all the data

00:10:19,440 --> 00:10:24,640
through some hash functions

00:10:21,519 --> 00:10:27,040
and it's going to look at the longest

00:10:24,640 --> 00:10:28,240
number of leading zeros in what results

00:10:27,040 --> 00:10:30,720
from that it's going to hash everything

00:10:28,240 --> 00:10:31,839
to a 0 1 binary sequence

00:10:30,720 --> 00:10:33,680
i'm going we'll look at the longest

00:10:31,839 --> 00:10:35,519
sequence to start with and there's a

00:10:33,680 --> 00:10:36,880
formula that we'll look at but we don't

00:10:35,519 --> 00:10:40,160
need to understand

00:10:36,880 --> 00:10:44,160
which will enable us to determine

00:10:40,160 --> 00:10:44,480
if we've seen so many items before so

00:10:44,160 --> 00:10:46,399
it'll

00:10:44,480 --> 00:10:49,600
allow us to guess the cardinality of the

00:10:46,399 --> 00:10:51,839
set with reasonable accuracy

00:10:49,600 --> 00:10:53,839
so the benefits here is the hyperlog log

00:10:51,839 --> 00:10:55,760
has a similar interface to a set we can

00:10:53,839 --> 00:10:58,560
add things to it and we can add it for

00:10:55,760 --> 00:11:00,079
ask it for how many things are in there

00:10:58,560 --> 00:11:01,760
it's going to save a lot of space

00:11:00,079 --> 00:11:03,600
because we're using a hashing function

00:11:01,760 --> 00:11:04,399
so it'll come down to like a fixed size

00:11:03,600 --> 00:11:07,680
data structure

00:11:04,399 --> 00:11:09,600
no matter how much data we put in there

00:11:07,680 --> 00:11:11,760
but we can't retrieve the items back

00:11:09,600 --> 00:11:13,120
again unlike with a set

00:11:11,760 --> 00:11:15,360
and that's both a benefit and a

00:11:13,120 --> 00:11:17,120
trade-off because we can't retrieve them

00:11:15,360 --> 00:11:18,720
that's great in some cases where we just

00:11:17,120 --> 00:11:20,480
want to count and we don't want the

00:11:18,720 --> 00:11:22,399
overhead of storing the information

00:11:20,480 --> 00:11:24,240
for example if it's personally

00:11:22,399 --> 00:11:26,160
identifiable information

00:11:24,240 --> 00:11:27,680
but it's also a bad thing if we did want

00:11:26,160 --> 00:11:30,800
to get the information back

00:11:27,680 --> 00:11:32,480
like in the set so

00:11:30,800 --> 00:11:34,160
we can use hyperloglog when we want to

00:11:32,480 --> 00:11:34,880
count but we don't necessarily need that

00:11:34,160 --> 00:11:37,920
information

00:11:34,880 --> 00:11:39,360
the actual information back again

00:11:37,920 --> 00:11:41,279
the other trade-off involved here is

00:11:39,360 --> 00:11:42,720
it's not built into the python language

00:11:41,279 --> 00:11:44,399
so we'll need to use some sort of

00:11:42,720 --> 00:11:46,480
library implementation

00:11:44,399 --> 00:11:48,399
and we'll need to use something else to

00:11:46,480 --> 00:11:51,519
store it into a data store which we'll

00:11:48,399 --> 00:11:54,240
look at so

00:11:51,519 --> 00:11:56,160
here's the algorithm for hyperloglog

00:11:54,240 --> 00:11:58,320
this is on wikipedia you can read about

00:11:56,160 --> 00:12:00,880
how it works if you're interested

00:11:58,320 --> 00:12:02,000
but basically it's a lot of math to do

00:12:00,880 --> 00:12:03,920
with um

00:12:02,000 --> 00:12:05,839
hashing things down to zeros and ones

00:12:03,920 --> 00:12:06,240
looking at how many leading zeros there

00:12:05,839 --> 00:12:08,320
are

00:12:06,240 --> 00:12:10,079
keeping a count of the greatest number

00:12:08,320 --> 00:12:12,240
of leading zeros we've seen

00:12:10,079 --> 00:12:14,320
then you can actually approximate the

00:12:12,240 --> 00:12:18,320
size of the data set based on

00:12:14,320 --> 00:12:19,519
on that so the takeaway here is we don't

00:12:18,320 --> 00:12:21,519
need to do that we're going to use the

00:12:19,519 --> 00:12:22,880
library or another implementation that's

00:12:21,519 --> 00:12:26,399
built into a data store

00:12:22,880 --> 00:12:26,399
and we'll look at both of those

00:12:26,560 --> 00:12:31,360
so the hyperlog log doesn't

00:12:29,839 --> 00:12:33,120
actually answer the question how many

00:12:31,360 --> 00:12:34,560
sheep have i seen for us

00:12:33,120 --> 00:12:36,320
it's going to answer the question

00:12:34,560 --> 00:12:39,120
approximately how many sheep

00:12:36,320 --> 00:12:40,800
have i seen which may well be good

00:12:39,120 --> 00:12:42,000
enough for our data set

00:12:40,800 --> 00:12:44,800
and it's going to save us a lot of

00:12:42,000 --> 00:12:48,000
memory so here in the python program

00:12:44,800 --> 00:12:48,959
i'm using the hyperlog log module and i

00:12:48,000 --> 00:12:51,519
am

00:12:48,959 --> 00:12:53,200
declaring a set as well for comparison

00:12:51,519 --> 00:12:54,880
so we're going to see how a set compares

00:12:53,200 --> 00:12:56,720
with a hyperlog log

00:12:54,880 --> 00:12:58,720
i'm declaring my hyperlog log and i'm

00:12:56,720 --> 00:13:00,560
giving it an accuracy factor which is

00:12:58,720 --> 00:13:01,839
something you can tune in the algorithm

00:13:00,560 --> 00:13:04,240
so you can trade off

00:13:01,839 --> 00:13:06,000
the amount of data bits it's going to

00:13:04,240 --> 00:13:08,639
take for the

00:13:06,000 --> 00:13:10,079
relative accuracy of the count and when

00:13:08,639 --> 00:13:10,800
we come to look at that with a data

00:13:10,079 --> 00:13:15,360
store we'll

00:13:10,800 --> 00:13:17,360
actually see how the the sizes compare

00:13:15,360 --> 00:13:18,800
so we've then got a loop we're going to

00:13:17,360 --> 00:13:21,200
add 100 000

00:13:18,800 --> 00:13:22,720
sheep to both the hyperlog log in the

00:13:21,200 --> 00:13:23,200
set and then we're going to ask both of

00:13:22,720 --> 00:13:26,240
them

00:13:23,200 --> 00:13:26,240
how many do you have

00:13:26,399 --> 00:13:30,000
and when we do that what we'll see there

00:13:28,480 --> 00:13:32,800
as we expect the set

00:13:30,000 --> 00:13:34,800
is absolutely 100 correct we've got 100

00:13:32,800 --> 00:13:36,800
000 sheep in our set

00:13:34,800 --> 00:13:37,839
and the hyperlog log has slightly over

00:13:36,800 --> 00:13:41,760
counted so

00:13:37,839 --> 00:13:44,160
100 075 so it's within a good

00:13:41,760 --> 00:13:46,079
uh margin of error and the trade-off

00:13:44,160 --> 00:13:48,639
here is that the set has taken up way

00:13:46,079 --> 00:13:50,240
more memory than the hyperlog blog has

00:13:48,639 --> 00:13:53,440
i will put some numbers on that when we

00:13:50,240 --> 00:13:53,440
look at it in a database

00:13:53,760 --> 00:13:58,399
so one of the reasons i pick redis is

00:13:56,800 --> 00:14:00,639
the data store for this is because it

00:13:58,399 --> 00:14:01,760
has sets and it also has hyperlog logs

00:14:00,639 --> 00:14:04,639
as data types

00:14:01,760 --> 00:14:06,399
so here i have a small python program

00:14:04,639 --> 00:14:07,519
it's going to do the same thing it's

00:14:06,399 --> 00:14:09,760
going to store

00:14:07,519 --> 00:14:10,639
cheap in a reddish set and in a redis

00:14:09,760 --> 00:14:13,199
hyperlog log

00:14:10,639 --> 00:14:15,040
so we begin by deleting those and loop

00:14:13,199 --> 00:14:18,320
over a hundred thousand sheep

00:14:15,040 --> 00:14:20,720
and add ids to redis for those

00:14:18,320 --> 00:14:21,360
and we put them into a set and we use

00:14:20,720 --> 00:14:23,680
the pf

00:14:21,360 --> 00:14:25,440
add commands down there to add them to

00:14:23,680 --> 00:14:29,680
the hyperlog log

00:14:25,440 --> 00:14:31,519
pf is uh philippe flagella the french

00:14:29,680 --> 00:14:33,279
mathematician who partly came up with

00:14:31,519 --> 00:14:36,079
the hyperlog log algorithm so redis

00:14:33,279 --> 00:14:37,279
commands for that are named after him

00:14:36,079 --> 00:14:39,519
and then when we've done that we'll

00:14:37,279 --> 00:14:41,199
again ask redis what's the cardinality

00:14:39,519 --> 00:14:42,800
of the set how many sheets did you count

00:14:41,199 --> 00:14:44,720
it'll tell us a hundred thousand because

00:14:42,800 --> 00:14:46,800
it's accurate and it'll tell us the

00:14:44,720 --> 00:14:49,440
approximation with the hyperlog log so

00:14:46,800 --> 00:14:50,800
we can compare

00:14:49,440 --> 00:14:52,720
so here we can see in the redis

00:14:50,800 --> 00:14:55,199
implementation um

00:14:52,720 --> 00:14:56,560
we got 100 000 sheep as we'd expect and

00:14:55,199 --> 00:14:58,880
it took about

00:14:56,560 --> 00:15:00,079
four and a half 4.6 megabyte of memory

00:14:58,880 --> 00:15:03,920
to store that

00:15:00,079 --> 00:15:04,320
with the hyperlog log we got 99 565

00:15:03,920 --> 00:15:06,240
sheep

00:15:04,320 --> 00:15:07,600
so we were pretty close to the hundred

00:15:06,240 --> 00:15:10,720
thousand

00:15:07,600 --> 00:15:12,000
but it only took 12k of memory and we

00:15:10,720 --> 00:15:13,920
could keep adding sheep to

00:15:12,000 --> 00:15:16,320
that all day and it's only going to take

00:15:13,920 --> 00:15:18,079
12k of memory whereas the set would have

00:15:16,320 --> 00:15:20,480
to keep growing

00:15:18,079 --> 00:15:21,680
so you can start to see some of the

00:15:20,480 --> 00:15:23,760
trade-offs here we're getting an

00:15:21,680 --> 00:15:27,519
approximate account we're saving a lot

00:15:23,760 --> 00:15:30,320
of memory

00:15:27,519 --> 00:15:31,920
so the second probabilistic data

00:15:30,320 --> 00:15:35,759
structure i wanted to look at

00:15:31,920 --> 00:15:36,720
is the bloom filter so the bloom filter

00:15:35,759 --> 00:15:38,800
is used for

00:15:36,720 --> 00:15:40,880
our other question that we wanted to ask

00:15:38,800 --> 00:15:42,880
which is have i seen this sheep

00:15:40,880 --> 00:15:45,680
so that's a set membership type of

00:15:42,880 --> 00:15:48,160
question is the sheep one two three four

00:15:45,680 --> 00:15:49,759
in the set of sheep that we've seen and

00:15:48,160 --> 00:15:51,680
when we're using a set we'll get an

00:15:49,759 --> 00:15:54,720
absolute answer we'll get yes

00:15:51,680 --> 00:15:57,040
or no when we're using a bloom filter

00:15:54,720 --> 00:15:58,720
we'll get an approximated answer so

00:15:57,040 --> 00:16:02,000
we'll get absolutely no

00:15:58,720 --> 00:16:04,639
it's not in the uh set or we'll get

00:16:02,000 --> 00:16:06,639
maybe it is there's a high likelihood

00:16:04,639 --> 00:16:09,680
that it is in the set

00:16:06,639 --> 00:16:11,519
and again that uncertainty

00:16:09,680 --> 00:16:13,360
comes from us not storing the data in

00:16:11,519 --> 00:16:14,399
the bloom filter so we're going to hash

00:16:13,360 --> 00:16:15,920
the data

00:16:14,399 --> 00:16:19,120
and we're going to trade that memory

00:16:15,920 --> 00:16:21,519
savings off for a little bit of accuracy

00:16:19,120 --> 00:16:23,519
so the way the bloom filter works and i

00:16:21,519 --> 00:16:27,120
have one laid out here

00:16:23,519 --> 00:16:29,040
is that you have a bit array and that is

00:16:27,120 --> 00:16:30,880
how many bits you want to make it wide

00:16:29,040 --> 00:16:33,040
so one of the things we can configure is

00:16:30,880 --> 00:16:34,800
the the width of the bit array

00:16:33,040 --> 00:16:37,199
so how much memory is it going to take

00:16:34,800 --> 00:16:39,680
here i've got 15 bits as a simple

00:16:37,199 --> 00:16:42,160
example that fits on the screen

00:16:39,680 --> 00:16:43,600
and then we configure a number of hash

00:16:42,160 --> 00:16:46,959
functions

00:16:43,600 --> 00:16:50,639
so every time we put a new sheet id

00:16:46,959 --> 00:16:52,000
or a new data item into the bloom filter

00:16:50,639 --> 00:16:54,000
we're going to run it through those hash

00:16:52,000 --> 00:16:55,600
functions and they all have to return a

00:16:54,000 --> 00:16:57,759
result that

00:16:55,600 --> 00:16:59,600
varies between zero and the length of

00:16:57,759 --> 00:17:01,839
the bit array

00:16:59,600 --> 00:17:03,680
so essentially they're going to identify

00:17:01,839 --> 00:17:06,720
positions in the bit array that

00:17:03,680 --> 00:17:08,640
that sheep id hashes to and we're going

00:17:06,720 --> 00:17:10,240
to use three in our example so each

00:17:08,640 --> 00:17:11,600
sheep id we're going to hash to three

00:17:10,240 --> 00:17:13,679
different bits

00:17:11,600 --> 00:17:15,439
and we'll see how that enables us to

00:17:13,679 --> 00:17:16,480
answer whether we've seen that sheet

00:17:15,439 --> 00:17:19,919
before

00:17:16,480 --> 00:17:22,959
in a no or maybe style so

00:17:19,919 --> 00:17:23,360
if we start out with adding the id one

00:17:22,959 --> 00:17:25,679
zero

00:17:23,360 --> 00:17:27,199
zero nine let's say that we have three

00:17:25,679 --> 00:17:29,440
hash functions

00:17:27,199 --> 00:17:30,240
and the first one hashes it to position

00:17:29,440 --> 00:17:33,600
one there

00:17:30,240 --> 00:17:36,880
and the second one hashes it to position

00:17:33,600 --> 00:17:39,280
six and the third one to position

00:17:36,880 --> 00:17:39,280
eight

00:17:40,400 --> 00:17:43,919
then what is going to happen here is

00:17:42,720 --> 00:17:48,160
that

00:17:43,919 --> 00:17:50,320
each bit in that filter is then set or

00:17:48,160 --> 00:17:53,039
the bit array is set to one

00:17:50,320 --> 00:17:54,720
so we know that a hash function has

00:17:53,039 --> 00:17:58,160
landed on that

00:17:54,720 --> 00:18:00,640
so similarly when we add more

00:17:58,160 --> 00:18:02,480
sheep so we add sheep 9107 here the

00:18:00,640 --> 00:18:03,840
three hash functions result in these

00:18:02,480 --> 00:18:06,400
positions

00:18:03,840 --> 00:18:07,360
and we can see in this case that 9107

00:18:06,400 --> 00:18:09,840
generated two

00:18:07,360 --> 00:18:11,520
new positions that were previously unset

00:18:09,840 --> 00:18:14,799
in our bit array

00:18:11,520 --> 00:18:16,880
and one existing one so there's

00:18:14,799 --> 00:18:18,960
potential here for as with a lot of

00:18:16,880 --> 00:18:20,960
hashing clashes so

00:18:18,960 --> 00:18:22,080
the more hash filters we use the wider

00:18:20,960 --> 00:18:24,080
the uh

00:18:22,080 --> 00:18:26,000
the bits array we can kind of dial some

00:18:24,080 --> 00:18:29,360
of that out but in this simple example

00:18:26,000 --> 00:18:32,880
we're gonna get some clashes

00:18:29,360 --> 00:18:35,679
adding more we get 1458 that hashes

00:18:32,880 --> 00:18:36,880
to three things that were already taken

00:18:35,679 --> 00:18:39,360
so

00:18:36,880 --> 00:18:43,120
we've don't set any new items but we

00:18:39,360 --> 00:18:45,200
don't set any new bits to one hit

00:18:43,120 --> 00:18:47,520
now when we want to look something up

00:18:45,200 --> 00:18:48,960
what we do is the same thing but we look

00:18:47,520 --> 00:18:51,760
at the value of what's

00:18:48,960 --> 00:18:52,160
in the bit array so here when i look up

00:18:51,760 --> 00:18:56,000
sheep

00:18:52,160 --> 00:18:58,000
2045 have we seen sheep 2045

00:18:56,000 --> 00:19:00,559
the first hash function hashes to a

00:18:58,000 --> 00:19:02,160
position where we've got a one so

00:19:00,559 --> 00:19:03,679
it's possible we have the second one

00:19:02,160 --> 00:19:04,640
hashes to a position where we've got a

00:19:03,679 --> 00:19:06,240
zero

00:19:04,640 --> 00:19:08,559
so that means we haven't seen this sheet

00:19:06,240 --> 00:19:10,960
before we could actually stop and not

00:19:08,559 --> 00:19:13,039
continue with the third hashing function

00:19:10,960 --> 00:19:14,720
but for completeness i've shown it

00:19:13,039 --> 00:19:16,720
so as soon as we get one that returns a

00:19:14,720 --> 00:19:17,200
zero we know that we haven't seen that

00:19:16,720 --> 00:19:20,640
before

00:19:17,200 --> 00:19:23,280
absolutely definitely not seen it

00:19:20,640 --> 00:19:24,640
9107 here is a sheet that we have seen

00:19:23,280 --> 00:19:27,440
before

00:19:24,640 --> 00:19:29,600
all of the hash functions land on a

00:19:27,440 --> 00:19:31,039
position that already has a 1 in it

00:19:29,600 --> 00:19:32,880
so we can say there is a strong

00:19:31,039 --> 00:19:34,480
likelihood that we've seen this sheet

00:19:32,880 --> 00:19:37,760
before

00:19:34,480 --> 00:19:40,240
and the reason why we can't say that

00:19:37,760 --> 00:19:42,160
with absolute certainty is if we look at

00:19:40,240 --> 00:19:44,400
sheep 29.89 here

00:19:42,160 --> 00:19:45,600
that's not in the set of sheep that we

00:19:44,400 --> 00:19:48,000
added at the

00:19:45,600 --> 00:19:48,640
top there this is not one we've seen

00:19:48,000 --> 00:19:51,760
before

00:19:48,640 --> 00:19:53,600
but it's number hash to

00:19:51,760 --> 00:19:54,960
positions that are all set to one so the

00:19:53,600 --> 00:19:58,320
bloom filter in this case

00:19:54,960 --> 00:19:59,840
is gonna lie to us it's gonna say 29.89

00:19:58,320 --> 00:20:03,280
there's a strong likelihood that that

00:19:59,840 --> 00:20:06,400
sheep exists but actually it doesn't

00:20:03,280 --> 00:20:07,440
so we are trading off here a lot of

00:20:06,400 --> 00:20:09,360
memory use

00:20:07,440 --> 00:20:11,600
um because we're getting down to just

00:20:09,360 --> 00:20:14,080
this bit array

00:20:11,600 --> 00:20:14,960
and some computational time because

00:20:14,080 --> 00:20:17,760
we're doing

00:20:14,960 --> 00:20:20,159
hashing across a number of functions but

00:20:17,760 --> 00:20:22,799
we are going to save a lot of memory

00:20:20,159 --> 00:20:24,480
and if we want to know absolutely

00:20:22,799 --> 00:20:28,000
whether we've seen this sheep

00:20:24,480 --> 00:20:29,280
and no or strong possibility is an okay

00:20:28,000 --> 00:20:31,440
answer

00:20:29,280 --> 00:20:34,480
then we can use this and we can save

00:20:31,440 --> 00:20:37,600
ourselves a lot of memory

00:20:34,480 --> 00:20:38,000
so here's some python code that uses

00:20:37,600 --> 00:20:40,559
this

00:20:38,000 --> 00:20:41,360
so we're going to use again some library

00:20:40,559 --> 00:20:45,120
code for

00:20:41,360 --> 00:20:47,760
bloom filter using pi probables

00:20:45,120 --> 00:20:49,679
and we just set up a bloom filter so we

00:20:47,760 --> 00:20:51,440
can configure it

00:20:49,679 --> 00:20:53,440
and this will work out how many hashes

00:20:51,440 --> 00:20:55,360
and the bit array size etc so we're

00:20:53,440 --> 00:20:58,080
saying we want to store that many items

00:20:55,360 --> 00:20:59,919
we want to store 200 000 items

00:20:58,080 --> 00:21:01,440
and we can dial in a false positive rate

00:20:59,919 --> 00:21:03,039
that's acceptable to us

00:21:01,440 --> 00:21:04,799
and then that will figure out the memory

00:21:03,039 --> 00:21:06,720
size used and that's

00:21:04,799 --> 00:21:08,400
part of our trade-offs so the more

00:21:06,720 --> 00:21:09,200
accurate we get the more memory the less

00:21:08,400 --> 00:21:12,240
accurate the

00:21:09,200 --> 00:21:14,320
less memory then we basically just add

00:21:12,240 --> 00:21:16,640
100 000 sheets to the bloom filter

00:21:14,320 --> 00:21:17,679
in much the same way as we did with the

00:21:16,640 --> 00:21:20,080
set

00:21:17,679 --> 00:21:21,600
and a havoc seem function is pretty much

00:21:20,080 --> 00:21:23,360
the same again we have a check

00:21:21,600 --> 00:21:24,960
function that says have i seen the sheep

00:21:23,360 --> 00:21:26,559
and it'll say

00:21:24,960 --> 00:21:29,679
i might have seen it because we can't be

00:21:26,559 --> 00:21:31,520
100 sure or no i definitely haven't

00:21:29,679 --> 00:21:33,440
so this is a good drop in for a set the

00:21:31,520 --> 00:21:36,559
interface is very very similar but we're

00:21:33,440 --> 00:21:36,559
saving a lot of memory

00:21:37,039 --> 00:21:40,159
and when we run this you get the answers

00:21:39,600 --> 00:21:43,679
that we

00:21:40,159 --> 00:21:46,000
kind of expect so it might have seen

00:21:43,679 --> 00:21:46,280
and it hasn't seen four five four nine

00:21:46,000 --> 00:21:48,480
one

00:21:46,280 --> 00:21:51,679
[Music]

00:21:48,480 --> 00:21:53,600
so we can also do this in data stores so

00:21:51,679 --> 00:21:54,720
again i pick redis as a data store for

00:21:53,600 --> 00:21:57,520
this talk because

00:21:54,720 --> 00:22:00,080
it has via a installable module an

00:21:57,520 --> 00:22:03,600
implementation of a bloom filter

00:22:00,080 --> 00:22:04,480
so similarly i can uh create a redis

00:22:03,600 --> 00:22:06,559
bloom filter

00:22:04,480 --> 00:22:08,240
the bf reserve command there is doing

00:22:06,559 --> 00:22:10,799
pretty much the same thing it's saying i

00:22:08,240 --> 00:22:12,880
want to store about 200 000 items with

00:22:10,799 --> 00:22:14,559
about that accuracy

00:22:12,880 --> 00:22:17,039
and then i can add sheep into the

00:22:14,559 --> 00:22:18,480
balloon filter and i can ask it

00:22:17,039 --> 00:22:21,520
does this sheep exist in the balloon

00:22:18,480 --> 00:22:22,159
filter and we will get the same sort of

00:22:21,520 --> 00:22:26,240
results

00:22:22,159 --> 00:22:28,480
as we did before so i might have seen it

00:22:26,240 --> 00:22:30,320
or i've not seen it but we are saving a

00:22:28,480 --> 00:22:32,080
lot of memory because now instead of

00:22:30,320 --> 00:22:34,080
using a set that's just going to grow as

00:22:32,080 --> 00:22:36,480
we add things to it we've got this fixed

00:22:34,080 --> 00:22:38,240
bit array that isn't going to grow but

00:22:36,480 --> 00:22:38,960
it is going to fill up as we add more

00:22:38,240 --> 00:22:40,880
sheep to it

00:22:38,960 --> 00:22:43,520
and there are strategies for stacking

00:22:40,880 --> 00:22:45,600
bloom filters so as one bit array fills

00:22:43,520 --> 00:22:47,919
putting another one on top of it

00:22:45,600 --> 00:22:51,520
that's beyond the scope of this talk but

00:22:47,919 --> 00:22:51,520
it is a problem that can be solved

00:22:51,840 --> 00:22:56,159
so when should you use probabilistic

00:22:54,799 --> 00:22:59,840
data structures

00:22:56,159 --> 00:23:00,960
well trade-offs so if an approximate

00:22:59,840 --> 00:23:03,679
account is good enough

00:23:00,960 --> 00:23:05,120
a hyperlog log's great so for example it

00:23:03,679 --> 00:23:07,200
doesn't really matter that we know

00:23:05,120 --> 00:23:07,840
exactly how many people read the article

00:23:07,200 --> 00:23:10,960
on medium

00:23:07,840 --> 00:23:12,640
as long as we're in the right ballpark

00:23:10,960 --> 00:23:15,120
you could use a bloom filter when it's

00:23:12,640 --> 00:23:18,240
okay to have some false positives

00:23:15,120 --> 00:23:20,799
so for example have i

00:23:18,240 --> 00:23:22,400
recommended this article on medium to

00:23:20,799 --> 00:23:24,080
this user before

00:23:22,400 --> 00:23:26,880
it doesn't really matter if we

00:23:24,080 --> 00:23:28,640
occasionally get that wrong

00:23:26,880 --> 00:23:30,400
and we're saving a lot of memory again

00:23:28,640 --> 00:23:32,240
especially in those cases where we need

00:23:30,400 --> 00:23:34,960
to maintain one of these data structures

00:23:32,240 --> 00:23:36,880
per user

00:23:34,960 --> 00:23:38,480
it might be advantageous to use these

00:23:36,880 --> 00:23:40,320
where you don't need to store or

00:23:38,480 --> 00:23:41,200
retrieve the original data so if the

00:23:40,320 --> 00:23:43,520
original data

00:23:41,200 --> 00:23:44,960
is either personal stuff that you don't

00:23:43,520 --> 00:23:46,559
want to store

00:23:44,960 --> 00:23:48,320
or it's just never ending so it's a

00:23:46,559 --> 00:23:50,559
continuous stream of say temperature

00:23:48,320 --> 00:23:51,919
humidity values

00:23:50,559 --> 00:23:54,400
which leads me on to the last point

00:23:51,919 --> 00:23:55,200
which is when you're working with huge

00:23:54,400 --> 00:23:57,279
data sets

00:23:55,200 --> 00:23:59,279
where exact strategies just aren't going

00:23:57,279 --> 00:24:01,520
to work out for you

00:23:59,279 --> 00:24:02,559
then um you're going to have to make

00:24:01,520 --> 00:24:05,600
some trade-offs

00:24:02,559 --> 00:24:09,200
and these this family of data structures

00:24:05,600 --> 00:24:12,799
offers a good set of trade-offs for

00:24:09,200 --> 00:24:12,799
between memory and accuracy

00:24:13,440 --> 00:24:18,080
so that was everything i had um the code

00:24:16,559 --> 00:24:20,320
that you've seen in this talk

00:24:18,080 --> 00:24:22,400
i have put in a small github repo which

00:24:20,320 --> 00:24:24,640
also has a docker compose file so you

00:24:22,400 --> 00:24:25,600
can both play with it in python just in

00:24:24,640 --> 00:24:27,840
memory

00:24:25,600 --> 00:24:29,840
and you can play with it in redis and

00:24:27,840 --> 00:24:31,760
have it inside a data store

00:24:29,840 --> 00:24:47,840
i hope you enjoyed this and have a great

00:24:31,760 --> 00:24:47,840
time at the conference

00:25:34,559 --> 00:25:36,640

YouTube URL: https://www.youtube.com/watch?v=VjFS-_H10bw


