Title: TALK   Kevin Kho   Large Scale Data Validation (with Spark and Dask)
Publication date: 2021-05-30
Playlist: PyCon US 2021
Description: 
	Data validation is checking if data follows certain requirements needed for data pipelines to run reliably. It is used by data scientists and data engineers to preserve the integrity of existing workflows, especially as they get modified. As an example, extreme machine learning predictions can be stopped from being displayed to application users if a new model is bad. Missing data can be flagged if it has the potential to break downstream operations.

As data volume continues to increase, we will examine how data validation differs between a single-machine setting and a distributed computing setting. We will show what validations become more computationally expensive in Spark and Dask. For large scale data, there is sometimes also a need to apply different validations on different partitions of data. This is currently not feasible with any single library. In this talk, we will show how we can achieve this by combining the strengths of different frameworks.

To demonstrate the data validation journey, we'll go over a fictitious case study. The data will start small, and we'll apply Pandas-based validations with Pandera and Great Expectations while discussing the pros and cons of each. As data size increases, we'll go over in detail the pain points of transitioning to a distributed setting. We'll show one way to reuse the same Pandas-based validations on Spark and Dask by wrapping them with Fugue.

Slides: https://drive.google.com/file/d/1x3w4pfk8PVw1dcy1717Qi-_kt67xQj-S/view?usp=sharing
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:15,599 --> 00:00:18,960
hi everyone

00:00:16,480 --> 00:00:21,279
my name is kevin co and i'm an open

00:00:18,960 --> 00:00:22,960
source community engineer at prefect

00:00:21,279 --> 00:00:25,279
where i work on data workflow

00:00:22,960 --> 00:00:26,800
orchestration

00:00:25,279 --> 00:00:28,640
today i'm going to be talking about

00:00:26,800 --> 00:00:31,840
large-scale data validation

00:00:28,640 --> 00:00:33,760
with spark and ask

00:00:31,840 --> 00:00:35,120
we'll start by going over what data

00:00:33,760 --> 00:00:37,680
validation is

00:00:35,120 --> 00:00:39,840
and when it's used and then we'll cover

00:00:37,680 --> 00:00:43,040
two data validation frameworks

00:00:39,840 --> 00:00:45,280
in great expectations and pandera great

00:00:43,040 --> 00:00:48,079
expectations is available for both

00:00:45,280 --> 00:00:50,079
pandas and spark where pandera is only

00:00:48,079 --> 00:00:52,719
available on pandas

00:00:50,079 --> 00:00:55,199
for our specific use case we'll explain

00:00:52,719 --> 00:00:57,760
why we want to use pandera

00:00:55,199 --> 00:00:58,320
on top of spark and in order to do so

00:00:57,760 --> 00:01:01,120
we'll

00:00:58,320 --> 00:01:02,399
we'll use fugue to do it fugue is an

00:01:01,120 --> 00:01:04,879
open source framework

00:01:02,399 --> 00:01:06,080
that serves as an abstraction layer and

00:01:04,879 --> 00:01:09,040
lets users write

00:01:06,080 --> 00:01:12,000
python or pandas code and then apply it

00:01:09,040 --> 00:01:14,560
on top of spark and ask

00:01:12,000 --> 00:01:16,400
interestingly using fugue and pandera

00:01:14,560 --> 00:01:18,479
will show how easy it is to have

00:01:16,400 --> 00:01:21,439
different validation schemes

00:01:18,479 --> 00:01:21,840
for each logical partition of data this

00:01:21,439 --> 00:01:23,520
is an

00:01:21,840 --> 00:01:26,000
operation that the current data

00:01:23,520 --> 00:01:29,759
validation frameworks don't

00:01:26,000 --> 00:01:29,759
don't have support for at the moment

00:01:29,840 --> 00:01:34,880
so for our talk today we'll we'll we'll

00:01:32,799 --> 00:01:36,479
have a case study with this fictitious

00:01:34,880 --> 00:01:38,840
company called food sloth

00:01:36,479 --> 00:01:40,720
and food sloth is a food delivery

00:01:38,840 --> 00:01:43,840
service

00:01:40,720 --> 00:01:44,240
food sloth is interested in using demand

00:01:43,840 --> 00:01:46,799
price

00:01:44,240 --> 00:01:50,079
and having demand pricing per location

00:01:46,799 --> 00:01:52,960
that updates every 10 minutes

00:01:50,079 --> 00:01:54,320
but we're also interested in making sure

00:01:52,960 --> 00:01:56,640
that the prices that are

00:01:54,320 --> 00:01:57,920
that are published to our users are not

00:01:56,640 --> 00:01:59,920
too ridiculous

00:01:57,920 --> 00:02:01,600
that they're not too cheap or too

00:01:59,920 --> 00:02:03,600
expensive

00:02:01,600 --> 00:02:04,799
and we'll use data validation in order

00:02:03,600 --> 00:02:07,920
to make sure

00:02:04,799 --> 00:02:10,080
that uh ridiculous values are not pushed

00:02:07,920 --> 00:02:10,080
out

00:02:10,399 --> 00:02:15,599
so first we'll cover what data

00:02:12,959 --> 00:02:17,440
validation is and data validation

00:02:15,599 --> 00:02:19,520
is just making sure that the data

00:02:17,440 --> 00:02:22,560
follows some certain assumptions

00:02:19,520 --> 00:02:25,360
that you may have as

00:02:22,560 --> 00:02:26,400
data pipelines get interconnected data

00:02:25,360 --> 00:02:28,400
validations

00:02:26,400 --> 00:02:31,440
are there to catch errors before they

00:02:28,400 --> 00:02:34,160
happen and ruin downstream processes

00:02:31,440 --> 00:02:37,200
so for our specific use case we'll

00:02:34,160 --> 00:02:39,440
likely have a model training pipeline

00:02:37,200 --> 00:02:41,440
where we load data in from the source

00:02:39,440 --> 00:02:42,000
perform validation on it to make sure

00:02:41,440 --> 00:02:43,680
that

00:02:42,000 --> 00:02:45,040
it meets the requirements needed for

00:02:43,680 --> 00:02:48,319
model training

00:02:45,040 --> 00:02:50,640
and then and then once we uh

00:02:48,319 --> 00:02:52,480
once we perform the model training we're

00:02:50,640 --> 00:02:53,120
sure that everything will go smoothly

00:02:52,480 --> 00:02:56,640
because

00:02:53,120 --> 00:02:59,440
the data has already been validated

00:02:56,640 --> 00:03:01,360
in this specific case our model training

00:02:59,440 --> 00:03:03,040
will be weekly or monthly

00:03:01,360 --> 00:03:05,360
the point is that it's not a time

00:03:03,040 --> 00:03:06,080
sensitive pipeline and data validation

00:03:05,360 --> 00:03:09,840
can be

00:03:06,080 --> 00:03:11,040
very uh very extensive and take a lot of

00:03:09,840 --> 00:03:14,239
resources and time

00:03:11,040 --> 00:03:16,840
because we're not in a rush to push that

00:03:14,239 --> 00:03:18,000
we're not in a rush to execute the model

00:03:16,840 --> 00:03:19,920
training

00:03:18,000 --> 00:03:21,280
on the other hand we'll have a inference

00:03:19,920 --> 00:03:23,280
pipeline

00:03:21,280 --> 00:03:25,120
and this pipeline as we said is running

00:03:23,280 --> 00:03:27,040
every 10 minutes

00:03:25,120 --> 00:03:29,599
so we'll load the data from the source

00:03:27,040 --> 00:03:31,599
run the model and get the new prices

00:03:29,599 --> 00:03:34,400
and then we'll perform data validation

00:03:31,599 --> 00:03:37,599
to check if these prices are reasonable

00:03:34,400 --> 00:03:39,440
before we update them on the app for our

00:03:37,599 --> 00:03:40,239
inference pipeline that runs every 10

00:03:39,440 --> 00:03:42,400
minutes

00:03:40,239 --> 00:03:43,519
we're concerned about the time that it

00:03:42,400 --> 00:03:46,640
takes for

00:03:43,519 --> 00:03:47,519
data validation to work because we have

00:03:46,640 --> 00:03:51,120
we cannot

00:03:47,519 --> 00:03:53,360
have a process that takes too much time

00:03:51,120 --> 00:03:55,439
so for our 10 minute pipeline we'll need

00:03:53,360 --> 00:03:57,920
a more lightweight data validation

00:03:55,439 --> 00:03:57,920
framework

00:03:58,000 --> 00:04:01,200
some common validations that exist so

00:04:00,799 --> 00:04:04,319
first

00:04:01,200 --> 00:04:06,159
we we can check it for the existence of

00:04:04,319 --> 00:04:09,920
null values and columns that are

00:04:06,159 --> 00:04:12,640
important to have second we can check

00:04:09,920 --> 00:04:14,319
uh the schema of our data set do we have

00:04:12,640 --> 00:04:16,639
the columns that are necessary

00:04:14,319 --> 00:04:18,239
to make predictions are they in the

00:04:16,639 --> 00:04:20,720
correct type

00:04:18,239 --> 00:04:22,880
we can check the data frame shape this

00:04:20,720 --> 00:04:23,919
is useful for making sure that rows are

00:04:22,880 --> 00:04:26,800
not dropped

00:04:23,919 --> 00:04:30,080
during data transformations that you

00:04:26,800 --> 00:04:31,919
still have the expected number of rows

00:04:30,080 --> 00:04:33,520
and for our specific use case we're

00:04:31,919 --> 00:04:35,919
interested in seeing

00:04:33,520 --> 00:04:38,320
if the price is within a reasonable

00:04:35,919 --> 00:04:38,320
range

00:04:39,440 --> 00:04:43,680
so we'll begin by talking about

00:04:41,759 --> 00:04:44,800
validation in spark with great

00:04:43,680 --> 00:04:46,960
expectations

00:04:44,800 --> 00:04:49,840
so create expectations is available for

00:04:46,960 --> 00:04:52,240
both pandas and spark

00:04:49,840 --> 00:04:54,240
for for people who are not familiar as

00:04:52,240 --> 00:04:56,400
familiar with spark

00:04:54,240 --> 00:04:57,440
spark is a distributed computing

00:04:56,400 --> 00:05:00,320
framework

00:04:57,440 --> 00:05:01,039
that that has data frames that people

00:05:00,320 --> 00:05:03,840
use

00:05:01,039 --> 00:05:06,080
when the data size is too big to

00:05:03,840 --> 00:05:10,160
efficiently handle in pandas

00:05:06,080 --> 00:05:12,800
so uh so a normal workflow would be

00:05:10,160 --> 00:05:15,440
that people start using pandas and as

00:05:12,800 --> 00:05:17,280
the data size becomes too big

00:05:15,440 --> 00:05:20,880
then you would scale to a distributed

00:05:17,280 --> 00:05:23,280
compute platform like spark or desk

00:05:20,880 --> 00:05:24,080
great expectations luckily has the same

00:05:23,280 --> 00:05:26,639
as an api

00:05:24,080 --> 00:05:28,960
for both pandas and spark making it very

00:05:26,639 --> 00:05:31,919
easy to transition

00:05:28,960 --> 00:05:32,880
so here we have a code snippet to to

00:05:31,919 --> 00:05:34,800
demonstrate what

00:05:32,880 --> 00:05:36,160
using great expectations looks like

00:05:34,800 --> 00:05:37,919
looks like and

00:05:36,160 --> 00:05:39,199
this is an example of how you would use

00:05:37,919 --> 00:05:41,520
it in a notebook

00:05:39,199 --> 00:05:42,320
so first you import the spark df data

00:05:41,520 --> 00:05:44,479
set

00:05:42,320 --> 00:05:46,320
and you pass the spark data frame to the

00:05:44,479 --> 00:05:48,560
spark df data set

00:05:46,320 --> 00:05:50,560
the spark df data set inherits from the

00:05:48,560 --> 00:05:52,560
spark data frame meaning that you can

00:05:50,560 --> 00:05:53,919
perform all spark operations on it as

00:05:52,560 --> 00:05:56,960
you normally would

00:05:53,919 --> 00:05:57,440
but added to those operations we also

00:05:56,960 --> 00:06:00,160
have

00:05:57,440 --> 00:06:01,919
these expectation methods that come with

00:06:00,160 --> 00:06:04,160
the spark df data set

00:06:01,919 --> 00:06:05,520
and here we're using the expect columns

00:06:04,160 --> 00:06:08,240
to be between

00:06:05,520 --> 00:06:10,160
call one and we're applying it on call

00:06:08,240 --> 00:06:13,759
one and we're giving a minimum value of

00:06:10,160 --> 00:06:15,840
zero and a max value of ninety five

00:06:13,759 --> 00:06:17,039
an interesting thing about the great

00:06:15,840 --> 00:06:19,280
expectations

00:06:17,039 --> 00:06:21,120
uh built in expectations is that they

00:06:19,280 --> 00:06:21,759
come with the mostly parameter most of

00:06:21,120 --> 00:06:24,080
the time

00:06:21,759 --> 00:06:25,680
the mostly parameter tells us what

00:06:24,080 --> 00:06:29,600
percent of records

00:06:25,680 --> 00:06:31,120
uh can need to succeed in order to make

00:06:29,600 --> 00:06:32,960
the expectation a success

00:06:31,120 --> 00:06:34,160
so in this specific case we're saying

00:06:32,960 --> 00:06:38,319
that 95

00:06:34,160 --> 00:06:40,800
of records must be between 0 and 95.

00:06:38,319 --> 00:06:42,560
if more than 5 of the records are not in

00:06:40,800 --> 00:06:44,000
this range then that's considered to be

00:06:42,560 --> 00:06:46,560
a failure

00:06:44,000 --> 00:06:48,319
and then there are different result

00:06:46,560 --> 00:06:51,759
formats that we can get

00:06:48,319 --> 00:06:54,080
with varying degrees of detail

00:06:51,759 --> 00:06:56,160
and here's an example of what the

00:06:54,080 --> 00:06:58,240
results show so on the first line you'll

00:06:56,160 --> 00:07:00,960
see that the success is false meaning we

00:06:58,240 --> 00:07:02,960
failed this specific validation

00:07:00,960 --> 00:07:04,000
and then a bit below you'll see the

00:07:02,960 --> 00:07:06,319
result

00:07:04,000 --> 00:07:07,039
the result contains a lot of information

00:07:06,319 --> 00:07:09,520
about

00:07:07,039 --> 00:07:10,880
uh how many values were missing what was

00:07:09,520 --> 00:07:13,759
the missing percentage

00:07:10,880 --> 00:07:14,800
how many uh records did not pass the

00:07:13,759 --> 00:07:17,520
expectation

00:07:14,800 --> 00:07:19,440
what percent of that what percent of the

00:07:17,520 --> 00:07:22,639
total was that

00:07:19,440 --> 00:07:26,960
and then it also provides example values

00:07:22,639 --> 00:07:29,360
of uh of of values that did not match

00:07:26,960 --> 00:07:30,400
uh that did not pass the expectation so

00:07:29,360 --> 00:07:33,360
in our case

00:07:30,400 --> 00:07:34,720
we were expecting a maximum value of 95

00:07:33,360 --> 00:07:38,240
and we have 96

00:07:34,720 --> 00:07:40,319
98 96

00:07:38,240 --> 00:07:43,120
a nice thing about great expectations is

00:07:40,319 --> 00:07:46,000
that as you perform your expectations

00:07:43,120 --> 00:07:46,879
under the hood it it's creating a data

00:07:46,000 --> 00:07:50,160
con

00:07:46,879 --> 00:07:53,360
an expectation config and the config

00:07:50,160 --> 00:07:53,360
is basically uh

00:07:53,759 --> 00:07:57,599
a set of expectations that you can then

00:07:56,479 --> 00:07:59,759
apply

00:07:57,599 --> 00:08:02,560
to other data sets once you save it or

00:07:59,759 --> 00:08:05,520
to the same data set

00:08:02,560 --> 00:08:08,000
data the expectation config can also be

00:08:05,520 --> 00:08:10,000
used to generate data documentation

00:08:08,000 --> 00:08:11,759
which is a very interesting feature that

00:08:10,000 --> 00:08:14,720
great expectations have

00:08:11,759 --> 00:08:16,639
so you can use the data config and

00:08:14,720 --> 00:08:19,039
create the data documentation to show

00:08:16,639 --> 00:08:19,919
what validations were run across a data

00:08:19,039 --> 00:08:21,440
set

00:08:19,919 --> 00:08:24,639
and then you can share this with your

00:08:21,440 --> 00:08:27,280
team in in these html documents

00:08:24,639 --> 00:08:28,479
great expectations also has a data

00:08:27,280 --> 00:08:30,720
profiler

00:08:28,479 --> 00:08:32,800
which you can use to explore data that

00:08:30,720 --> 00:08:35,200
you haven't worked with yet before

00:08:32,800 --> 00:08:36,000
and data the data profiler will go in

00:08:35,200 --> 00:08:38,479
and

00:08:36,000 --> 00:08:39,680
and per and get some summary statistics

00:08:38,479 --> 00:08:41,680
and get some

00:08:39,680 --> 00:08:44,159
information about your data that you can

00:08:41,680 --> 00:08:46,800
then use as a starting point for your

00:08:44,159 --> 00:08:49,519
for for your expectations in creating a

00:08:46,800 --> 00:08:52,560
new expectation suite

00:08:49,519 --> 00:08:55,920
you all the ui also shows a lot of

00:08:52,560 --> 00:08:57,519
uh the previous uh the previous

00:08:55,920 --> 00:08:59,200
validations that were performed and

00:08:57,519 --> 00:09:00,720
whether or not they were a success or

00:08:59,200 --> 00:09:02,959
failure

00:09:00,720 --> 00:09:04,560
so in general great expectations has a

00:09:02,959 --> 00:09:07,200
really robust

00:09:04,560 --> 00:09:08,080
set of features uh that definitely

00:09:07,200 --> 00:09:11,360
extend

00:09:08,080 --> 00:09:12,640
uh beyond code uh with with this data

00:09:11,360 --> 00:09:14,959
documentation

00:09:12,640 --> 00:09:16,320
and definitely i would encourage you uh

00:09:14,959 --> 00:09:20,240
people to check it out

00:09:16,320 --> 00:09:24,800
uh because they um they have a very

00:09:20,240 --> 00:09:27,680
uh set of tools of to perform really

00:09:24,800 --> 00:09:29,440
extensive data validations for our

00:09:27,680 --> 00:09:31,440
specific use case

00:09:29,440 --> 00:09:33,120
attaching great expectations to model

00:09:31,440 --> 00:09:36,399
training makes a lot of sense

00:09:33,120 --> 00:09:38,800
because we can really explore uh

00:09:36,399 --> 00:09:40,320
extensively but for our inference

00:09:38,800 --> 00:09:41,760
pipeline we may need something

00:09:40,320 --> 00:09:43,279
lightweight

00:09:41,760 --> 00:09:45,760
and that's why we have to search for a

00:09:43,279 --> 00:09:46,959
more lightweight framework

00:09:45,760 --> 00:09:50,480
and that's where we come in with

00:09:46,959 --> 00:09:52,560
pandaras pandera so pandera is a panda's

00:09:50,480 --> 00:09:55,279
only data validation framework

00:09:52,560 --> 00:09:58,240
similar to great expectations it has a

00:09:55,279 --> 00:10:01,200
lot of built-in validations

00:09:58,240 --> 00:10:02,720
and it also has statistical validations

00:10:01,200 --> 00:10:04,640
for example you can see

00:10:02,720 --> 00:10:06,560
whether two columns follow the same

00:10:04,640 --> 00:10:09,120
distribution you can run t

00:10:06,560 --> 00:10:11,040
tests a nice thing about pandara is

00:10:09,120 --> 00:10:13,040
because it's lightweight and simple

00:10:11,040 --> 00:10:15,839
it's very easy to extend with custom

00:10:13,040 --> 00:10:15,839
validators

00:10:16,000 --> 00:10:19,519
here's an example code snippet of

00:10:17,760 --> 00:10:23,200
pandera

00:10:19,519 --> 00:10:26,399
so first we have uh we import pandera

00:10:23,200 --> 00:10:29,360
and we import the data frame schema

00:10:26,399 --> 00:10:30,800
in the middle section we we make a new

00:10:29,360 --> 00:10:33,200
data frame schema

00:10:30,800 --> 00:10:34,079
that and here we're going we specified

00:10:33,200 --> 00:10:37,360
that the column

00:10:34,079 --> 00:10:39,760
price should be of type float

00:10:37,360 --> 00:10:40,480
and then we run a check that it is in

00:10:39,760 --> 00:10:43,120
the range

00:10:40,480 --> 00:10:44,880
of min value 5 and max value 10. so we

00:10:43,120 --> 00:10:48,000
are checking that price

00:10:44,880 --> 00:10:50,320
is within the values 5 and 10.

00:10:48,000 --> 00:10:51,920
once we define the data frame schema

00:10:50,320 --> 00:10:54,320
it's very easy to apply it

00:10:51,920 --> 00:10:55,040
all we have to do is call the validate

00:10:54,320 --> 00:10:58,160
method

00:10:55,040 --> 00:11:00,000
and pass in the data frame so here

00:10:58,160 --> 00:11:01,600
the data frame that we pass in has a

00:11:00,000 --> 00:11:05,120
column price and the

00:11:01,600 --> 00:11:08,480
check will be applied on that column

00:11:05,120 --> 00:11:09,040
the interface is very easy uh very easy

00:11:08,480 --> 00:11:12,320
to

00:11:09,040 --> 00:11:14,000
use and extend uh panera also has

00:11:12,320 --> 00:11:16,399
decorators so that these data

00:11:14,000 --> 00:11:18,399
frame schemas can be applied on data

00:11:16,399 --> 00:11:20,880
frames through decorators

00:11:18,399 --> 00:11:23,040
meaning that it won't invade the code at

00:11:20,880 --> 00:11:25,120
all

00:11:23,040 --> 00:11:26,399
so in order to so to compare the two

00:11:25,120 --> 00:11:29,519
frameworks that were

00:11:26,399 --> 00:11:30,480
that we explored great expectations on

00:11:29,519 --> 00:11:32,959
one side

00:11:30,480 --> 00:11:34,079
has spark support and panda support well

00:11:32,959 --> 00:11:37,360
pandera only has

00:11:34,079 --> 00:11:40,000
panda support great expectations

00:11:37,360 --> 00:11:41,519
has flexible success criteria with most

00:11:40,000 --> 00:11:44,640
of their expectations

00:11:41,519 --> 00:11:46,480
they give very detailed outputs the data

00:11:44,640 --> 00:11:48,480
documentation is certainly

00:11:46,480 --> 00:11:50,160
an interesting feature so that teams can

00:11:48,480 --> 00:11:52,880
be on the same page on what

00:11:50,160 --> 00:11:53,360
on what uh that what expectations the

00:11:52,880 --> 00:11:55,839
data

00:11:53,360 --> 00:11:57,200
failed failed in and then there's also

00:11:55,839 --> 00:11:58,720
notification

00:11:57,200 --> 00:12:00,639
slack notifications and other

00:11:58,720 --> 00:12:04,639
notifications that you can get if

00:12:00,639 --> 00:12:06,639
pipelines fail their data validation

00:12:04,639 --> 00:12:08,560
and then you can use the cli as well to

00:12:06,639 --> 00:12:10,560
perform your data validations

00:12:08,560 --> 00:12:12,720
on the other hand we have pandera which

00:12:10,560 --> 00:12:16,000
is very lightweight

00:12:12,720 --> 00:12:18,160
and uh but but easy to extend for our

00:12:16,000 --> 00:12:20,000
specific use case with foodslot

00:12:18,160 --> 00:12:21,839
it makes sense to use great expectations

00:12:20,000 --> 00:12:24,639
in the training side when we need

00:12:21,839 --> 00:12:26,240
a lot more information with the data

00:12:24,639 --> 00:12:28,720
documentation

00:12:26,240 --> 00:12:30,480
and then for our inference side maybe we

00:12:28,720 --> 00:12:32,639
want to use something more lightweight

00:12:30,480 --> 00:12:34,560
like pandera something that will run

00:12:32,639 --> 00:12:36,240
fast and just easily check whether or

00:12:34,560 --> 00:12:39,839
not our data passes to

00:12:36,240 --> 00:12:39,839
the standards required

00:12:41,279 --> 00:12:45,600
but the problem is that for very large

00:12:43,440 --> 00:12:48,639
data sets where we're using spark

00:12:45,600 --> 00:12:50,000
there uh pandera will not run since it

00:12:48,639 --> 00:12:52,480
only runs on pandas

00:12:50,000 --> 00:12:53,120
so we have to find a way for to run

00:12:52,480 --> 00:12:55,920
pandera

00:12:53,120 --> 00:12:57,680
on top of the spark execution engine and

00:12:55,920 --> 00:12:59,920
that's where fugue comes in

00:12:57,680 --> 00:13:01,440
so fugue is an open source abstraction

00:12:59,920 --> 00:13:04,720
layer that lets

00:13:01,440 --> 00:13:07,279
users write code in python or pandas

00:13:04,720 --> 00:13:09,120
and then apply to spark and desk for

00:13:07,279 --> 00:13:10,800
those familiar with the spark user

00:13:09,120 --> 00:13:13,120
defined functions

00:13:10,800 --> 00:13:15,360
fugue is a friendlier interface which

00:13:13,120 --> 00:13:17,120
i'll be sharing later

00:13:15,360 --> 00:13:19,839
fugue's goal is to allow users to

00:13:17,120 --> 00:13:22,320
decouple logic and execution

00:13:19,839 --> 00:13:24,320
users only need to write code once in a

00:13:22,320 --> 00:13:26,560
scale agnostic way

00:13:24,320 --> 00:13:28,160
and then during execution decide whether

00:13:26,560 --> 00:13:31,360
or not it will run on pandas

00:13:28,160 --> 00:13:33,360
spark or desk now users

00:13:31,360 --> 00:13:34,639
will be able to just focus on defining

00:13:33,360 --> 00:13:37,839
their logic

00:13:34,639 --> 00:13:40,160
and then scaling seamlessly when needed

00:13:37,839 --> 00:13:41,440
you can test locally on smaller data

00:13:40,160 --> 00:13:43,279
sets with pandas

00:13:41,440 --> 00:13:48,160
and then have the same code work on

00:13:43,279 --> 00:13:50,800
spark in a production environment

00:13:48,160 --> 00:13:52,720
has two interfaces the python interface

00:13:50,800 --> 00:13:56,160
and the sql interface

00:13:52,720 --> 00:13:59,279
the sql interface um is all

00:13:56,160 --> 00:14:02,240
will will run on top of spark sql

00:13:59,279 --> 00:14:03,240
recently released also was a blazing

00:14:02,240 --> 00:14:04,320
sequel

00:14:03,240 --> 00:14:08,240
[Music]

00:14:04,320 --> 00:14:11,279
was a blazing sequel back end so

00:14:08,240 --> 00:14:13,440
fugue sql can use uh sql on top of the

00:14:11,279 --> 00:14:16,720
blazing sql execution engine

00:14:13,440 --> 00:14:18,720
also on the python side

00:14:16,720 --> 00:14:20,639
we can write code in native python and

00:14:18,720 --> 00:14:24,399
then have it applied to pandas

00:14:20,639 --> 00:14:28,560
spark or desk and i'll share

00:14:24,399 --> 00:14:32,160
a sample code snippet so here

00:14:28,560 --> 00:14:35,040
where we in in this if we make a fill

00:14:32,160 --> 00:14:36,880
n a function it's it's going to be it's

00:14:35,040 --> 00:14:38,639
probably going to be written in pandas

00:14:36,880 --> 00:14:41,600
because pandas is faster

00:14:38,639 --> 00:14:43,760
but here the goal is to demonstrate that

00:14:41,600 --> 00:14:44,720
our code can be written in pure native

00:14:43,760 --> 00:14:47,839
python

00:14:44,720 --> 00:14:49,680
and then applied on panda spark or desk

00:14:47,839 --> 00:14:50,959
so here on the first line we have a

00:14:49,680 --> 00:14:53,600
schema hint

00:14:50,959 --> 00:14:55,600
and the schema says that we're keeping

00:14:53,600 --> 00:14:58,000
all the columns but adding a new column

00:14:55,600 --> 00:15:01,199
called filled which is of type double

00:14:58,000 --> 00:15:04,399
schema is enforced in fugue so

00:15:01,199 --> 00:15:07,120
few as field performs the opera a

00:15:04,399 --> 00:15:08,800
few builds a dag on before the

00:15:07,120 --> 00:15:11,199
operations are performed

00:15:08,800 --> 00:15:12,880
and it's lazily evaluated when using the

00:15:11,199 --> 00:15:17,440
spark execution engine

00:15:12,880 --> 00:15:20,480
similar to spark now

00:15:17,440 --> 00:15:22,480
as before the operations are performed

00:15:20,480 --> 00:15:24,839
there are validations that the dac does

00:15:22,480 --> 00:15:27,199
to make sure that the schemas are

00:15:24,839 --> 00:15:29,680
matching

00:15:27,199 --> 00:15:30,639
and then here we define our fill and a

00:15:29,680 --> 00:15:33,759
function

00:15:30,639 --> 00:15:36,160
which takes type iterable dict

00:15:33,759 --> 00:15:39,120
an iterable of dictionaries and then

00:15:36,160 --> 00:15:42,240
outputs an iterable of dictionaries also

00:15:39,120 --> 00:15:43,199
now we can loop through it similar to an

00:15:42,240 --> 00:15:45,920
iterable

00:15:43,199 --> 00:15:48,000
uh similar to just any standard iterable

00:15:45,920 --> 00:15:50,720
where we say four row in df

00:15:48,000 --> 00:15:51,600
and we create a new uh this is creating

00:15:50,720 --> 00:15:54,880
the new column

00:15:51,600 --> 00:15:58,639
where we say row filled equals row value

00:15:54,880 --> 00:16:02,000
or that value that to fill the n a value

00:15:58,639 --> 00:16:03,920
and then we yield this row notice that

00:16:02,000 --> 00:16:06,079
this code is written in pure native

00:16:03,920 --> 00:16:08,720
python

00:16:06,079 --> 00:16:09,360
and it has no dependencies on pandas or

00:16:08,720 --> 00:16:12,399
spark

00:16:09,360 --> 00:16:13,680
there is no framework dependency a few

00:16:12,399 --> 00:16:16,240
games

00:16:13,680 --> 00:16:16,959
the fugue believes that code should not

00:16:16,240 --> 00:16:20,000
even be

00:16:16,959 --> 00:16:21,360
dependent on fugue itself and then we

00:16:20,000 --> 00:16:23,759
create the fugue workflow

00:16:21,360 --> 00:16:25,199
context manager and we pass the spark

00:16:23,759 --> 00:16:27,279
execution engine

00:16:25,199 --> 00:16:28,639
this is where we specify the execution

00:16:27,279 --> 00:16:31,279
engine that it will run on

00:16:28,639 --> 00:16:32,720
we can also use the das execution engine

00:16:31,279 --> 00:16:34,639
or we can use pandas

00:16:32,720 --> 00:16:37,120
by using the native execution engine

00:16:34,639 --> 00:16:40,079
which is the default

00:16:37,120 --> 00:16:41,360
and then under the fugue workflow

00:16:40,079 --> 00:16:43,279
context manager

00:16:41,360 --> 00:16:46,320
we just load a file and perform this

00:16:43,279 --> 00:16:48,959
fill in a operation

00:16:46,320 --> 00:16:50,000
underneath the context manager there is

00:16:48,959 --> 00:16:53,199
no dependency

00:16:50,000 --> 00:16:53,199
on any framework either

00:16:55,600 --> 00:16:59,120
now we can show how to combine fugue and

00:16:57,759 --> 00:17:01,279
pandera

00:16:59,120 --> 00:17:02,959
so for the first three lines of this

00:17:01,279 --> 00:17:06,079
code should be very

00:17:02,959 --> 00:17:08,720
familiar this is how this was the basic

00:17:06,079 --> 00:17:09,520
pandera code snippet where we checked if

00:17:08,720 --> 00:17:13,280
a value

00:17:09,520 --> 00:17:15,679
if our price value was between 5 and 10.

00:17:13,280 --> 00:17:18,480
the middle section of this code is where

00:17:15,679 --> 00:17:22,160
we create another field transformer

00:17:18,480 --> 00:17:24,559
where we uh where we applied the data

00:17:22,160 --> 00:17:26,959
validation

00:17:24,559 --> 00:17:27,600
as mentioned earlier the schema is

00:17:26,959 --> 00:17:30,000
enforced

00:17:27,600 --> 00:17:31,120
but data validation operations tend to

00:17:30,000 --> 00:17:33,200
be read only

00:17:31,120 --> 00:17:36,320
so our schema here is just everything in

00:17:33,200 --> 00:17:38,640
is the same as everything out

00:17:36,320 --> 00:17:40,880
then for our data validation all we're

00:17:38,640 --> 00:17:41,919
doing is getting that data frame schema

00:17:40,880 --> 00:17:43,840
from earlier

00:17:41,919 --> 00:17:45,679
and calling the validate method on the

00:17:43,840 --> 00:17:46,160
date on the data frame that was passed

00:17:45,679 --> 00:17:49,280
in

00:17:46,160 --> 00:17:52,559
and then we returned the data frame

00:17:49,280 --> 00:17:55,280
now now we can

00:17:52,559 --> 00:17:56,240
uh write our field workflow context

00:17:55,280 --> 00:17:59,520
manager

00:17:56,240 --> 00:18:02,720
and pass in the spark execution engine

00:17:59,520 --> 00:18:06,080
and then now this makes the pandera

00:18:02,720 --> 00:18:07,840
run on top of spark so when we do df

00:18:06,080 --> 00:18:11,280
equals df.transform

00:18:07,840 --> 00:18:13,120
price validation we are running the

00:18:11,280 --> 00:18:15,840
pandera data validation

00:18:13,120 --> 00:18:15,840
on top of spark

00:18:16,880 --> 00:18:21,280
and this leads us to the next thing

00:18:18,799 --> 00:18:23,440
which is using this setup it's actually

00:18:21,280 --> 00:18:26,080
very easy to extend our validation and

00:18:23,440 --> 00:18:29,039
have custom validations for each logical

00:18:26,080 --> 00:18:30,960
partition of data when peop when people

00:18:29,039 --> 00:18:33,039
are using spark or das

00:18:30,960 --> 00:18:34,880
chances are it's because the data is

00:18:33,039 --> 00:18:36,480
very is very large

00:18:34,880 --> 00:18:39,120
and if it's very large it means that

00:18:36,480 --> 00:18:41,039
there is some logical way we can split

00:18:39,120 --> 00:18:42,640
the data into smaller sizes which are

00:18:41,039 --> 00:18:45,679
our partitions

00:18:42,640 --> 00:18:47,679
and then we can perform

00:18:45,679 --> 00:18:50,400
logic specific to each of those

00:18:47,679 --> 00:18:50,400
partitions

00:18:50,960 --> 00:18:55,039
so our motivation here for food slot is

00:18:53,919 --> 00:18:58,480
that food slot

00:18:55,039 --> 00:19:01,360
is growing rapidly and we're

00:18:58,480 --> 00:19:02,160
instead of only being in one state as

00:19:01,360 --> 00:19:05,200
before

00:19:02,160 --> 00:19:06,559
it's now available in three states and

00:19:05,200 --> 00:19:07,919
along with that there's different

00:19:06,559 --> 00:19:10,400
business logics and different

00:19:07,919 --> 00:19:12,880
validations that have to be performed

00:19:10,400 --> 00:19:14,080
for each different location we want to

00:19:12,880 --> 00:19:17,840
make sure that

00:19:14,080 --> 00:19:20,799
uh the price ranges for each of these lo

00:19:17,840 --> 00:19:22,880
locations can be defined independently

00:19:20,799 --> 00:19:25,679
and this is something that's hard to do

00:19:22,880 --> 00:19:27,039
in with the data validation frameworks

00:19:25,679 --> 00:19:28,480
today

00:19:27,039 --> 00:19:30,080
with the with the current data

00:19:28,480 --> 00:19:32,480
validation frameworks

00:19:30,080 --> 00:19:34,000
you have to define a validation on a

00:19:32,480 --> 00:19:37,120
column

00:19:34,000 --> 00:19:40,160
level and we can't we can in our

00:19:37,120 --> 00:19:43,120
we we need to do some extra uh work

00:19:40,160 --> 00:19:44,320
in order to or perform multiple

00:19:43,120 --> 00:19:47,440
validations

00:19:44,320 --> 00:19:51,600
in order to get it uh to work in this

00:19:47,440 --> 00:19:51,600
uh on a partition level

00:19:51,679 --> 00:19:56,559
so here we have example data and

00:19:54,880 --> 00:19:59,200
there is a clear split between the

00:19:56,559 --> 00:20:01,440
pricing of the of the florida

00:19:59,200 --> 00:20:03,280
uh locations versus the california

00:20:01,440 --> 00:20:05,280
locations

00:20:03,280 --> 00:20:06,640
the florida locations on average are

00:20:05,280 --> 00:20:08,720
cheaper

00:20:06,640 --> 00:20:10,320
and the california locations on average

00:20:08,720 --> 00:20:11,760
are more expensive

00:20:10,320 --> 00:20:13,520
so what we want to do is we want to

00:20:11,760 --> 00:20:16,400
apply custom ranges

00:20:13,520 --> 00:20:19,840
for each of the california locations for

00:20:16,400 --> 00:20:22,320
the california locations and

00:20:19,840 --> 00:20:24,000
price range for the florida locations we

00:20:22,320 --> 00:20:26,880
want to split this data frame

00:20:24,000 --> 00:20:28,000
into two based on uh floored based on

00:20:26,880 --> 00:20:31,679
the location

00:20:28,000 --> 00:20:31,679
and then apply that validation

00:20:32,720 --> 00:20:37,039
so here's an example of how we can

00:20:34,880 --> 00:20:40,400
easily extend our previous example

00:20:37,039 --> 00:20:43,280
and perform validation by partition

00:20:40,400 --> 00:20:45,120
first the fir the the two price checks

00:20:43,280 --> 00:20:47,200
should be very familiar again

00:20:45,120 --> 00:20:49,760
they are very similar to the pandera

00:20:47,200 --> 00:20:51,600
code that we showed earlier

00:20:49,760 --> 00:20:54,400
on the first we have the price check

00:20:51,600 --> 00:20:56,400
florida and it has a min value of 5 and

00:20:54,400 --> 00:20:58,320
a max value of 10.

00:20:56,400 --> 00:21:00,240
the second one is the price check of

00:20:58,320 --> 00:21:04,000
california where we have a min

00:21:00,240 --> 00:21:07,520
value of 10 and a max value of 15.

00:21:04,000 --> 00:21:08,240
and then we can add add these price

00:21:07,520 --> 00:21:11,679
check

00:21:08,240 --> 00:21:14,320
objects into the price checks dictionary

00:21:11,679 --> 00:21:14,960
where the key will be the location ca

00:21:14,320 --> 00:21:17,840
and fl

00:21:14,960 --> 00:21:20,320
and the value will be the data frame

00:21:17,840 --> 00:21:20,320
schema

00:21:20,640 --> 00:21:25,200
now we can define we can extend our

00:21:23,280 --> 00:21:28,840
price validation transformer

00:21:25,200 --> 00:21:31,520
as before the schema was read only

00:21:28,840 --> 00:21:32,720
so everything in will be the same as

00:21:31,520 --> 00:21:34,320
everything out

00:21:32,720 --> 00:21:36,159
the first thing we'll do when the data

00:21:34,320 --> 00:21:39,039
frame comes in

00:21:36,159 --> 00:21:40,880
is that it will already be partitioned

00:21:39,039 --> 00:21:43,200
by location

00:21:40,880 --> 00:21:45,200
meaning that the florida data frames

00:21:43,200 --> 00:21:47,760
will will be separated

00:21:45,200 --> 00:21:49,039
a data frame with the florida entries

00:21:47,760 --> 00:21:50,720
will be separated from

00:21:49,039 --> 00:21:52,159
the data frame with the california

00:21:50,720 --> 00:21:54,640
entries and the

00:21:52,159 --> 00:21:56,799
function will be applied for each of

00:21:54,640 --> 00:21:59,520
those separate data frames

00:21:56,799 --> 00:22:00,880
so once the data frame comes in and we

00:21:59,520 --> 00:22:04,080
pull out the location

00:22:00,880 --> 00:22:07,360
that location will be uh the same

00:22:04,080 --> 00:22:09,919
throughout the whole data frame already

00:22:07,360 --> 00:22:12,799
so first the first operation we do is we

00:22:09,919 --> 00:22:15,120
pull the location and then using that

00:22:12,799 --> 00:22:16,320
location we pull the appropriate data

00:22:15,120 --> 00:22:19,360
frame schema check

00:22:16,320 --> 00:22:22,000
from our dictionary price checks and

00:22:19,360 --> 00:22:24,480
then once we have that data frame schema

00:22:22,000 --> 00:22:26,000
all we have to do as before is called

00:22:24,480 --> 00:22:29,120
the validate method on

00:22:26,000 --> 00:22:29,120
top of the data frame

00:22:29,440 --> 00:22:33,440
now with a few uh workflow context

00:22:32,400 --> 00:22:35,360
manager

00:22:33,440 --> 00:22:37,440
we can pass in the spark execution

00:22:35,360 --> 00:22:39,360
engine so that we can take advantage of

00:22:37,440 --> 00:22:43,120
distributed computing

00:22:39,360 --> 00:22:45,200
and have each partition uh

00:22:43,120 --> 00:22:47,600
and run this uh parallel on on the

00:22:45,200 --> 00:22:50,240
different partitions

00:22:47,600 --> 00:22:52,880
and on the last line of code which we

00:22:50,240 --> 00:22:55,200
changed it a bit by adding a partition

00:22:52,880 --> 00:22:56,880
so first we're partitioning the data

00:22:55,200 --> 00:22:59,120
frame by location

00:22:56,880 --> 00:23:01,280
meaning that the florida entries will be

00:22:59,120 --> 00:23:03,200
separate from the california entries

00:23:01,280 --> 00:23:04,480
and then we transform it with the price

00:23:03,200 --> 00:23:07,440
validation

00:23:04,480 --> 00:23:08,000
and now we achieved validation by each

00:23:07,440 --> 00:23:10,159
logic

00:23:08,000 --> 00:23:12,799
a custom validation on each logical

00:23:10,159 --> 00:23:12,799
partition

00:23:13,679 --> 00:23:17,760
so to wrap up the talk we went over data

00:23:16,240 --> 00:23:20,400
validation what it is

00:23:17,760 --> 00:23:22,559
when you would use it we talked about

00:23:20,400 --> 00:23:24,400
two very different data validation

00:23:22,559 --> 00:23:25,440
frameworks and great expectations in

00:23:24,400 --> 00:23:27,280
pandara

00:23:25,440 --> 00:23:29,360
great expectations being a very

00:23:27,280 --> 00:23:32,400
heavyweight uh

00:23:29,360 --> 00:23:33,919
very robust ecosystem but in

00:23:32,400 --> 00:23:36,799
in exchange for that there is some

00:23:33,919 --> 00:23:38,799
overhead of course to using it

00:23:36,799 --> 00:23:39,919
for everything that it provides and in

00:23:38,799 --> 00:23:42,000
cases where we

00:23:39,919 --> 00:23:43,039
we may want a lighter weight validation

00:23:42,000 --> 00:23:46,320
framework

00:23:43,039 --> 00:23:48,480
we can use a pandera for for a quick

00:23:46,320 --> 00:23:52,640
validation

00:23:48,480 --> 00:23:53,840
we showed how uh we can use fugue in

00:23:52,640 --> 00:23:56,320
order to use

00:23:53,840 --> 00:23:57,600
python or pandas-based libraries on top

00:23:56,320 --> 00:24:00,720
of spark

00:23:57,600 --> 00:24:01,440
fuel can also use great expectations uh

00:24:00,720 --> 00:24:04,559
on the

00:24:01,440 --> 00:24:07,279
to up to apply partition

00:24:04,559 --> 00:24:07,679
validation on each partition uh we just

00:24:07,279 --> 00:24:11,840
chose

00:24:07,679 --> 00:24:15,039
pandera because to to to illustrate like

00:24:11,840 --> 00:24:16,640
to illustrate why you would want uh

00:24:15,039 --> 00:24:19,520
to illustrate using a lighter weight

00:24:16,640 --> 00:24:21,440
framework for data validation

00:24:19,520 --> 00:24:22,960
and we showed how using fugue you can

00:24:21,440 --> 00:24:25,279
actually achieve

00:24:22,960 --> 00:24:27,919
validation on each partition which is

00:24:25,279 --> 00:24:31,840
something that none of the existing

00:24:27,919 --> 00:24:34,159
data validation frameworks offer today

00:24:31,840 --> 00:24:34,960
and with that that's the end of my talk

00:24:34,159 --> 00:24:37,520
uh

00:24:34,960 --> 00:24:39,679
i hope that uh you all check out these

00:24:37,520 --> 00:24:40,960
projects great expectations and pandera

00:24:39,679 --> 00:24:43,520
for data validation

00:24:40,960 --> 00:24:45,200
and fugue also as an abstraction layer

00:24:43,520 --> 00:24:47,840
and with that thank you for coming to my

00:24:45,200 --> 00:24:47,840
talk

00:25:51,760 --> 00:25:53,840

YouTube URL: https://www.youtube.com/watch?v=2AdvBgjO_3Q


