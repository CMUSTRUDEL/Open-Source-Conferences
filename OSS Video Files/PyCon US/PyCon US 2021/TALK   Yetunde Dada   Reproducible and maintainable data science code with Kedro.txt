Title: TALK   Yetunde Dada   Reproducible and maintainable data science code with Kedro
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Code produced by data scientists is under attack! There are a growing series of conference talks, Medium blog posts and business stakeholders telling a story of how changing business objectives are driving interest in production-level code. Production-level code is considered time-consuming to produce and limiting for the experimentation process needed to create amazing models. You're going to follow a workflow that deconstructs your experimentation workflow in a Jupyter notebook and helps you create production-ready ML pipelines. The talk is focused on an open source Python framework, called kedro that emphasises creating reproducible, maintainable and modular data science code.

Documentation: https://kedro.readthedocs.io/en/stable/
GitHub Repository: https://github.com/quantumblacklabs/kedro
Slides: https://speakerdeck.com/yetudada/reproducible-and-maintainable-data-science-code-with-kedro?slide=18
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:15,759 --> 00:00:20,640
hello everyone

00:00:16,800 --> 00:00:22,960
i'm super excited to be at pycon us 2021

00:00:20,640 --> 00:00:25,279
um thank you so much for joining in uh

00:00:22,960 --> 00:00:27,039
i'm gonna be talking to you about

00:00:25,279 --> 00:00:30,000
reproducible and maintainable data

00:00:27,039 --> 00:00:31,359
science code with kedro so stay tuned um

00:00:30,000 --> 00:00:33,840
i guess it's always good to

00:00:31,359 --> 00:00:35,760
get to know who who's the stranger on

00:00:33,840 --> 00:00:38,800
the screen that you're talking to

00:00:35,760 --> 00:00:40,559
my name is yitunde i am

00:00:38,800 --> 00:00:42,079
a principal product manager at a company

00:00:40,559 --> 00:00:43,920
called quantum black which is part of

00:00:42,079 --> 00:00:45,360
mckinsey and company i'll get into a bit

00:00:43,920 --> 00:00:48,559
more detail about that

00:00:45,360 --> 00:00:50,879
i'm one of the maintainers on kedro it's

00:00:48,559 --> 00:00:53,680
an open source project

00:00:50,879 --> 00:00:54,559
and part of an amazing team i really

00:00:53,680 --> 00:00:56,239
think

00:00:54,559 --> 00:00:58,239
there's lots of ways to reach out if you

00:00:56,239 --> 00:00:59,520
want to get into contact with me i talk

00:00:58,239 --> 00:01:01,039
about kidro all the time

00:00:59,520 --> 00:01:02,800
um because i'm really passionate about

00:01:01,039 --> 00:01:04,559
it in and out well

00:01:02,800 --> 00:01:06,720
in my old life i actually would have

00:01:04,559 --> 00:01:08,240
been a user of keto when i used to build

00:01:06,720 --> 00:01:10,320
data pipelines and worried about the

00:01:08,240 --> 00:01:12,000
reproducibility of my work

00:01:10,320 --> 00:01:13,200
but now on the other side it's it's

00:01:12,000 --> 00:01:15,840
great to be able to build products that

00:01:13,200 --> 00:01:18,080
help people that do so

00:01:15,840 --> 00:01:19,920
um when we talk about what quantum black

00:01:18,080 --> 00:01:21,360
is it's an advanced analytics company

00:01:19,920 --> 00:01:22,640
that was acquired by mckinsey a few

00:01:21,360 --> 00:01:24,720
years ago

00:01:22,640 --> 00:01:26,400
our official wording is that we're the

00:01:24,720 --> 00:01:28,640
machine learning and ai

00:01:26,400 --> 00:01:30,000
center of excellence for mckinsey but i

00:01:28,640 --> 00:01:31,920
really like to think of us as the black

00:01:30,000 --> 00:01:33,280
ops team you know dirty data disparate

00:01:31,920 --> 00:01:35,040
data sources

00:01:33,280 --> 00:01:36,079
large-scale machine learning products

00:01:35,040 --> 00:01:37,280
that need to be built with many

00:01:36,079 --> 00:01:39,200
collaborators

00:01:37,280 --> 00:01:40,960
and our origins are actually in formula

00:01:39,200 --> 00:01:42,399
one um where our co-founders met each

00:01:40,960 --> 00:01:43,920
other and realized that they could take

00:01:42,399 --> 00:01:46,079
some of their learnings of applying

00:01:43,920 --> 00:01:49,280
advanced analytics to formula one

00:01:46,079 --> 00:01:50,720
into the business world but today i'm

00:01:49,280 --> 00:01:52,640
actually going to talk to you about

00:01:50,720 --> 00:01:55,200
what is this problem that you know

00:01:52,640 --> 00:01:56,560
kitters trying to solve what is kedro

00:01:55,200 --> 00:01:57,920
and we're going to talk about that whole

00:01:56,560 --> 00:01:58,880
workflow of converting a jupiter

00:01:57,920 --> 00:02:01,840
notebook

00:01:58,880 --> 00:02:03,119
into a ketter project so in terms of

00:02:01,840 --> 00:02:06,159
framing

00:02:03,119 --> 00:02:09,280
it's always handy to understand really

00:02:06,159 --> 00:02:11,200
what is your end goal are you writing

00:02:09,280 --> 00:02:12,480
code all the time that you're sure that

00:02:11,200 --> 00:02:13,840
no one is going to use after your

00:02:12,480 --> 00:02:15,680
project is complete

00:02:13,840 --> 00:02:18,080
it's kind of used for some sort of like

00:02:15,680 --> 00:02:19,920
business decision or report

00:02:18,080 --> 00:02:21,599
unless you you can run it on your

00:02:19,920 --> 00:02:24,480
machine kind of if you

00:02:21,599 --> 00:02:26,000
make it work a little bit and jiggle it

00:02:24,480 --> 00:02:27,520
but it's kind of like

00:02:26,000 --> 00:02:29,520
no one is expected to use this code

00:02:27,520 --> 00:02:32,160
because in that in those cases it's

00:02:29,520 --> 00:02:34,560
perfectly fine to optimize for speed

00:02:32,160 --> 00:02:36,239
um you're optimizing for speed in the

00:02:34,560 --> 00:02:36,560
case of this building over here you can

00:02:36,239 --> 00:02:39,840
see

00:02:36,560 --> 00:02:42,400
that it works this is not very robust

00:02:39,840 --> 00:02:44,160
it does what it's supposed to but

00:02:42,400 --> 00:02:46,080
sometimes what happens is your business

00:02:44,160 --> 00:02:47,760
stakeholders or team members get really

00:02:46,080 --> 00:02:50,239
excited about the insights and they ask

00:02:47,760 --> 00:02:51,280
you to take it to production and this is

00:02:50,239 --> 00:02:52,000
where you should have been thinking

00:02:51,280 --> 00:02:53,840
about

00:02:52,000 --> 00:02:55,519
creating something that looks like this

00:02:53,840 --> 00:02:57,120
a machine learning product

00:02:55,519 --> 00:02:58,720
which we call data science code that

00:02:57,120 --> 00:02:59,280
needs to be rerun and maintained in

00:02:58,720 --> 00:03:03,120
future

00:02:59,280 --> 00:03:05,200
by someone or a system and

00:03:03,120 --> 00:03:07,440
in the these two examples you actually

00:03:05,200 --> 00:03:10,400
see that we need to completely flatten

00:03:07,440 --> 00:03:11,920
that shed and rebuild this building

00:03:10,400 --> 00:03:13,040
because we can't use any of that code

00:03:11,920 --> 00:03:15,440
base because of the way that it was

00:03:13,040 --> 00:03:16,800
rewritten and structured

00:03:15,440 --> 00:03:18,640
so we start to go into some of the

00:03:16,800 --> 00:03:19,920
reasons why it's so hard to write code

00:03:18,640 --> 00:03:21,280
like this all the time

00:03:19,920 --> 00:03:23,280
because i mean we love experimenting

00:03:21,280 --> 00:03:24,959
with our code bases right

00:03:23,280 --> 00:03:26,799
and one of the issues lies with our

00:03:24,959 --> 00:03:28,879
reliance in jupiter notebooks

00:03:26,799 --> 00:03:30,480
so we believe that jupiter notebooks are

00:03:28,879 --> 00:03:31,360
really good for exploratory data

00:03:30,480 --> 00:03:34,000
analysis

00:03:31,360 --> 00:03:35,040
initial pipeline development and

00:03:34,000 --> 00:03:36,480
reporting

00:03:35,040 --> 00:03:38,640
but in terms of building your whole

00:03:36,480 --> 00:03:40,239
pipeline or workflow in it

00:03:38,640 --> 00:03:42,640
you run into a few challenges which

00:03:40,239 --> 00:03:44,480
we've called the five c's of challenges

00:03:42,640 --> 00:03:46,159
so the first one is collaboration how do

00:03:44,480 --> 00:03:47,200
multiple people work on the same jupiter

00:03:46,159 --> 00:03:48,959
notebook together

00:03:47,200 --> 00:03:51,440
there are a few ways to get around it

00:03:48,959 --> 00:03:53,439
but it's generally not built for it

00:03:51,440 --> 00:03:54,560
how do you do efficient code reviews how

00:03:53,439 --> 00:03:55,920
do we make sure that we're improving

00:03:54,560 --> 00:03:59,200
each other's code

00:03:55,920 --> 00:04:01,360
as we're using a jupyter notebook

00:03:59,200 --> 00:04:02,879
we talk about code quality as well how

00:04:01,360 --> 00:04:05,519
do you think about writing unit tests

00:04:02,879 --> 00:04:07,200
documentation or doing code linting in

00:04:05,519 --> 00:04:10,080
those cases

00:04:07,200 --> 00:04:11,840
um what happens when you've heavily

00:04:10,080 --> 00:04:13,200
relied on jupyter notebook's ability to

00:04:11,840 --> 00:04:16,079
help you with cached

00:04:13,200 --> 00:04:17,440
state um and now i have to run your

00:04:16,079 --> 00:04:19,600
notebook from start to finish and

00:04:17,440 --> 00:04:20,639
realize that actually it's not working

00:04:19,600 --> 00:04:22,079
as it should

00:04:20,639 --> 00:04:23,840
um because you've got some hidden state

00:04:22,079 --> 00:04:26,160
in your jupyter notebook

00:04:23,840 --> 00:04:28,240
and really the final c is i guess a

00:04:26,160 --> 00:04:31,840
representation of all of these things

00:04:28,240 --> 00:04:34,000
is consistency so nyu in 2019 conducted

00:04:31,840 --> 00:04:36,720
a study where they took 860 000

00:04:34,000 --> 00:04:38,240
notebooks found in over 260 000 github

00:04:36,720 --> 00:04:41,440
repositories

00:04:38,240 --> 00:04:42,880
24 of those notebooks ran and

00:04:41,440 --> 00:04:44,800
only four percent produce the same

00:04:42,880 --> 00:04:47,199
results so we start to see

00:04:44,800 --> 00:04:47,919
some issues around reproducibility when

00:04:47,199 --> 00:04:50,160
we rely

00:04:47,919 --> 00:04:52,000
solely on jupiter notebooks for our

00:04:50,160 --> 00:04:53,840
workflows

00:04:52,000 --> 00:04:55,759
but let's say you've actually moved on

00:04:53,840 --> 00:04:57,360
you're relying on python script you're

00:04:55,759 --> 00:04:58,479
using linting you're using all these

00:04:57,360 --> 00:04:59,840
amazing things

00:04:58,479 --> 00:05:02,000
and now you start to find a different

00:04:59,840 --> 00:05:04,000
series of challenges primarily that

00:05:02,000 --> 00:05:05,840
everyone works in different ways

00:05:04,000 --> 00:05:07,680
you can't go across your organization or

00:05:05,840 --> 00:05:08,960
your unit and work on someone else's

00:05:07,680 --> 00:05:10,800
project because they've just set it up

00:05:08,960 --> 00:05:12,400
in very different ways

00:05:10,800 --> 00:05:14,320
you have to learn so many tools to do

00:05:12,400 --> 00:05:16,720
this as well and it's constantly growing

00:05:14,320 --> 00:05:19,199
in this space

00:05:16,720 --> 00:05:20,960
you all have different levels of

00:05:19,199 --> 00:05:23,360
software engineering expertise in your

00:05:20,960 --> 00:05:26,880
organization as well or in your

00:05:23,360 --> 00:05:29,280
academic field and you need kind of need

00:05:26,880 --> 00:05:31,440
higher levels to produce high quality

00:05:29,280 --> 00:05:32,880
code

00:05:31,440 --> 00:05:35,039
and this is where we understand that

00:05:32,880 --> 00:05:35,759
machine learning is definitely not the

00:05:35,039 --> 00:05:37,919
hard part

00:05:35,759 --> 00:05:39,440
you know this building and maintaining

00:05:37,919 --> 00:05:40,960
the data and machine learning pipeline

00:05:39,440 --> 00:05:44,000
is

00:05:40,960 --> 00:05:46,800
so what is kedro kedro

00:05:44,000 --> 00:05:47,919
is our an open source python framework

00:05:46,800 --> 00:05:49,440
that was developed

00:05:47,919 --> 00:05:50,960
it's developed and maintained by quantum

00:05:49,440 --> 00:05:52,240
black it's also mckinsey's first open

00:05:50,960 --> 00:05:54,160
source product

00:05:52,240 --> 00:05:55,759
and it helps us produce reproducible

00:05:54,160 --> 00:05:56,639
maintainable and modular data science

00:05:55,759 --> 00:05:58,720
code

00:05:56,639 --> 00:06:00,160
it does this by borrowing concepts from

00:05:58,720 --> 00:06:02,800
software engineering so

00:06:00,160 --> 00:06:04,639
things like modularity separation of

00:06:02,800 --> 00:06:05,919
concerns and versioning

00:06:04,639 --> 00:06:07,680
and we use it to address the

00:06:05,919 --> 00:06:08,800
shortcomings of jupiter notebooks and

00:06:07,680 --> 00:06:11,039
glue code

00:06:08,800 --> 00:06:13,280
um we also use it because it increases

00:06:11,039 --> 00:06:15,759
the efficiency of an analytics team

00:06:13,280 --> 00:06:17,600
and the way that we're using kedro now

00:06:15,759 --> 00:06:18,240
is actually to build reusable code

00:06:17,600 --> 00:06:20,639
stores

00:06:18,240 --> 00:06:21,280
kind of like how you use react to build

00:06:20,639 --> 00:06:22,960
um

00:06:21,280 --> 00:06:24,319
you know react components to build

00:06:22,960 --> 00:06:26,080
design systems

00:06:24,319 --> 00:06:27,759
um for your front end we're not doing

00:06:26,080 --> 00:06:29,600
that with machine learning code with

00:06:27,759 --> 00:06:31,280
kedro

00:06:29,600 --> 00:06:32,639
um it's kind of making headway in the

00:06:31,280 --> 00:06:34,319
envelope space

00:06:32,639 --> 00:06:36,160
winning different awards and it's of

00:06:34,319 --> 00:06:38,960
course being used all over the world

00:06:36,160 --> 00:06:40,560
um at startups and major enterprises in

00:06:38,960 --> 00:06:42,000
academia you can check out the growing

00:06:40,560 --> 00:06:43,840
list on our readme

00:06:42,000 --> 00:06:45,840
on github and see which companies

00:06:43,840 --> 00:06:47,120
feature there

00:06:45,840 --> 00:06:48,960
i'm going to lightly talk about some of

00:06:47,120 --> 00:06:50,560
the concepts in kedrow but we'll

00:06:48,960 --> 00:06:53,680
actually be covering them um

00:06:50,560 --> 00:06:54,960
in the preview or demo that i'm going to

00:06:53,680 --> 00:06:57,039
be running

00:06:54,960 --> 00:06:58,400
so we talk about our project template

00:06:57,039 --> 00:06:59,759
series of files and folders if you're

00:06:58,400 --> 00:07:01,280
familiar with a tool called the cookie

00:06:59,759 --> 00:07:02,319
cutter data science it falls within that

00:07:01,280 --> 00:07:03,919
remit

00:07:02,319 --> 00:07:05,599
we talk about removing hard-coded

00:07:03,919 --> 00:07:07,280
variables from machine learning code

00:07:05,599 --> 00:07:08,720
so that it can run locally or in the

00:07:07,280 --> 00:07:09,840
cloud or production without major

00:07:08,720 --> 00:07:12,000
changes

00:07:09,840 --> 00:07:13,199
um we talk about our data catalog which

00:07:12,000 --> 00:07:15,360
is make you know

00:07:13,199 --> 00:07:16,639
extensible collection of data model or

00:07:15,360 --> 00:07:18,479
image connectors that allow you to

00:07:16,639 --> 00:07:20,240
connect any cloud storage

00:07:18,479 --> 00:07:22,400
or production system and borrow

00:07:20,240 --> 00:07:23,599
arguments from things like pandas spark

00:07:22,400 --> 00:07:25,599
matplotlib

00:07:23,599 --> 00:07:28,400
you name it and then i'll also talk

00:07:25,599 --> 00:07:30,880
about our pipeline extraction

00:07:28,400 --> 00:07:32,479
so if we think of kedra kind of in the

00:07:30,880 --> 00:07:35,120
example of those buildings as a

00:07:32,479 --> 00:07:38,160
scaffolding for a project

00:07:35,120 --> 00:07:40,000
um then we can look at

00:07:38,160 --> 00:07:42,080
obviously that supports how you write

00:07:40,000 --> 00:07:44,240
code that you're proud to deploy

00:07:42,080 --> 00:07:45,759
so caterer office obviously has many

00:07:44,240 --> 00:07:46,160
different deployment modes so it fits

00:07:45,759 --> 00:07:48,560
into

00:07:46,160 --> 00:07:50,319
many different ways that people work um

00:07:48,560 --> 00:07:52,639
so obviously supporting a range of tools

00:07:50,319 --> 00:07:55,680
including data bricks or

00:07:52,639 --> 00:07:56,560
kubeflow aws batch or aws stage maker or

00:07:55,680 --> 00:07:58,800
docker

00:07:56,560 --> 00:08:00,479
whatever you need to use um and we

00:07:58,800 --> 00:08:03,759
really do think of like

00:08:00,479 --> 00:08:05,520
is kedra an orchestrator no because yes

00:08:03,759 --> 00:08:06,720
we do have a pipeline abstraction but we

00:08:05,520 --> 00:08:09,840
can convert it

00:08:06,720 --> 00:08:11,039
um into um a pipeline

00:08:09,840 --> 00:08:12,879
abstraction that belongs to an

00:08:11,039 --> 00:08:13,759
orchestrator and that's what we prefer

00:08:12,879 --> 00:08:15,520
to do

00:08:13,759 --> 00:08:17,199
because things like what time will my

00:08:15,520 --> 00:08:18,240
pipeline run how will i know if it

00:08:17,199 --> 00:08:20,479
failed what will it

00:08:18,240 --> 00:08:21,440
retry if it failed we leave to things

00:08:20,479 --> 00:08:24,240
that are really good

00:08:21,440 --> 00:08:26,319
at that like apache airflow argo cube

00:08:24,240 --> 00:08:28,160
flow prefect and many more

00:08:26,319 --> 00:08:29,840
and really pedro focuses on things that

00:08:28,160 --> 00:08:30,400
they're not good at which is how do i

00:08:29,840 --> 00:08:31,919
write

00:08:30,400 --> 00:08:33,519
standardized modular maintainable and

00:08:31,919 --> 00:08:36,880
reproducible data science code

00:08:33,519 --> 00:08:39,360
all the time um and that's what we do

00:08:36,880 --> 00:08:40,479
you'll find this online as well um the

00:08:39,360 --> 00:08:42,880
caterpillar community

00:08:40,479 --> 00:08:45,200
is growing we have kendricks all around

00:08:42,880 --> 00:08:46,959
the world building amazing plugins

00:08:45,200 --> 00:08:48,320
and showing us how to do things you can

00:08:46,959 --> 00:08:50,480
find us on github you can find us

00:08:48,320 --> 00:08:52,480
underscore stack overflow read the docs

00:08:50,480 --> 00:08:53,760
and soon discord

00:08:52,480 --> 00:08:56,160
so right now i'm going to actually jump

00:08:53,760 --> 00:08:57,200
into converting a jupiter notebook into

00:08:56,160 --> 00:08:58,720
a project

00:08:57,200 --> 00:09:00,399
and actually briefly take you through an

00:08:58,720 --> 00:09:03,680
example of how you could possibly

00:09:00,399 --> 00:09:04,240
um think to attempt this um so what i'm

00:09:03,680 --> 00:09:08,080
going to do

00:09:04,240 --> 00:09:11,440
is share this and

00:09:08,080 --> 00:09:13,360
uh and show and here we go

00:09:11,440 --> 00:09:15,839
so here's an example of what one of

00:09:13,360 --> 00:09:17,120
those reposts could look like

00:09:15,839 --> 00:09:19,200
um i'm going to take you through the

00:09:17,120 --> 00:09:21,040
issues that are wrong with it um so

00:09:19,200 --> 00:09:23,040
we see over here we've got some readme

00:09:21,040 --> 00:09:24,959
someone tried to explain to me

00:09:23,040 --> 00:09:26,560
this example like the context of this

00:09:24,959 --> 00:09:28,720
example so the context of this example

00:09:26,560 --> 00:09:30,080
is that it's the year 2160

00:09:28,720 --> 00:09:31,360
um there are many companies flying

00:09:30,080 --> 00:09:32,880
people to the moon and back space

00:09:31,360 --> 00:09:35,920
tourism is booming

00:09:32,880 --> 00:09:37,839
and um you are trying to predict the

00:09:35,920 --> 00:09:39,519
price of a space flight

00:09:37,839 --> 00:09:41,839
and you have access to three data

00:09:39,519 --> 00:09:43,200
sources companies information

00:09:41,839 --> 00:09:44,800
shuttles that are owned by those

00:09:43,200 --> 00:09:46,080
companies and reviews of people that

00:09:44,800 --> 00:09:48,080
have ridden on those shuttles

00:09:46,080 --> 00:09:50,240
and you now have to predict the the

00:09:48,080 --> 00:09:53,680
price of a

00:09:50,240 --> 00:09:55,839
of a space ticket essentially

00:09:53,680 --> 00:09:57,279
you want to split your workflow into two

00:09:55,839 --> 00:09:58,959
parts data

00:09:57,279 --> 00:10:02,320
pre-processing and then some sort of

00:09:58,959 --> 00:10:04,320
data science or modeling workflow

00:10:02,320 --> 00:10:05,680
in this case we already see we have our

00:10:04,320 --> 00:10:07,920
data sources here so

00:10:05,680 --> 00:10:09,200
and this is a very unstandard um

00:10:07,920 --> 00:10:11,600
workflow in the sense that

00:10:09,200 --> 00:10:13,120
enterprise data science has never should

00:10:11,600 --> 00:10:16,480
not be done like this

00:10:13,120 --> 00:10:19,519
um where you have for get full

00:10:16,480 --> 00:10:21,279
data committed to a git repository

00:10:19,519 --> 00:10:22,959
and also normally your data sets are so

00:10:21,279 --> 00:10:23,680
big that this could never happen anyways

00:10:22,959 --> 00:10:26,079
right

00:10:23,680 --> 00:10:28,480
so in this case working with local

00:10:26,079 --> 00:10:30,800
folders so this is why my data is here

00:10:28,480 --> 00:10:32,399
um we also see someone tried to explain

00:10:30,800 --> 00:10:34,320
to me that i need a scalar and version

00:10:32,399 --> 00:10:36,959
0.2

00:10:34,320 --> 00:10:38,079
full or above installed to try and get

00:10:36,959 --> 00:10:39,760
the same results

00:10:38,079 --> 00:10:41,279
and i really wish they told me the exact

00:10:39,760 --> 00:10:43,839
version to install um

00:10:41,279 --> 00:10:45,360
in case like i wanted to reproduce the

00:10:43,839 --> 00:10:48,800
same thing

00:10:45,360 --> 00:10:50,000
um if i open up the the jupiter

00:10:48,800 --> 00:10:52,480
notebooks here

00:10:50,000 --> 00:10:54,320
we see my import statements here yeah

00:10:52,480 --> 00:10:57,200
that's great you tell me to read csv

00:10:54,320 --> 00:10:57,839
i see this company's table total fleet

00:10:57,200 --> 00:11:01,120
count

00:10:57,839 --> 00:11:03,839
this is the company location company id

00:11:01,120 --> 00:11:05,680
great oh someone deleted something in my

00:11:03,839 --> 00:11:08,720
junior notebook oh no

00:11:05,680 --> 00:11:12,320
um i see that is true is not defined

00:11:08,720 --> 00:11:12,320
because i didn't run state in order

00:11:13,360 --> 00:11:16,480
once again i have another import

00:11:15,040 --> 00:11:17,440
statement here i'm not sure why i did

00:11:16,480 --> 00:11:19,440
this

00:11:17,440 --> 00:11:21,760
um and eventually i would put something

00:11:19,440 --> 00:11:25,040
to a csv

00:11:21,760 --> 00:11:26,480
i do the same um with my data science

00:11:25,040 --> 00:11:28,880
workflow this one's a little bit better

00:11:26,480 --> 00:11:30,640
so my import statements at that top

00:11:28,880 --> 00:11:32,399
um i've tried to write in python

00:11:30,640 --> 00:11:35,440
functions this is great

00:11:32,399 --> 00:11:36,399
um let me see another import statement

00:11:35,440 --> 00:11:38,160
here where i could have done a little

00:11:36,399 --> 00:11:38,800
bit better but the data science workflow

00:11:38,160 --> 00:11:40,240
is not good

00:11:38,800 --> 00:11:42,240
but you also see i have some interesting

00:11:40,240 --> 00:11:43,760
naming for my jupyter notebooks

00:11:42,240 --> 00:11:45,680
because when we talk about how could we

00:11:43,760 --> 00:11:47,839
improve this workflow we're going to be

00:11:45,680 --> 00:11:50,240
refactoring in stages um

00:11:47,839 --> 00:11:51,839
in this exercise let's go to have a look

00:11:50,240 --> 00:11:54,320
at what's changed

00:11:51,839 --> 00:11:55,200
so all my data has been moved into a

00:11:54,320 --> 00:11:58,320
data

00:11:55,200 --> 00:11:59,600
directory i've moved it there all my

00:11:58,320 --> 00:12:00,959
notebooks have been moved into a

00:11:59,600 --> 00:12:02,959
notebooks directory

00:12:00,959 --> 00:12:04,639
my notebooks have been renamed they can

00:12:02,959 --> 00:12:06,880
now run start to finish

00:12:04,639 --> 00:12:08,959
all workflow uses python functions and

00:12:06,880 --> 00:12:11,200
all functions have been grouped

00:12:08,959 --> 00:12:12,399
and logic for loading and saving has

00:12:11,200 --> 00:12:13,120
loading and saving data has been

00:12:12,399 --> 00:12:16,399
separated

00:12:13,120 --> 00:12:18,480
so let's have a look we see over here my

00:12:16,399 --> 00:12:20,399
import statements at the top it's good

00:12:18,480 --> 00:12:21,600
um i clearly demarcate where i'm loading

00:12:20,399 --> 00:12:23,440
data

00:12:21,600 --> 00:12:25,680
i have some helper functions that i'm

00:12:23,440 --> 00:12:27,519
using so i specify them here

00:12:25,680 --> 00:12:29,519
i have my main functions to run

00:12:27,519 --> 00:12:30,320
pre-process companies pre-purchase

00:12:29,519 --> 00:12:33,360
shuttles

00:12:30,320 --> 00:12:35,519
and create a model input table and then

00:12:33,360 --> 00:12:38,160
i run everything

00:12:35,519 --> 00:12:39,360
and then i save my data i do the same

00:12:38,160 --> 00:12:42,240
with data science

00:12:39,360 --> 00:12:43,760
in the sense that we have my import

00:12:42,240 --> 00:12:46,800
statements at the top

00:12:43,760 --> 00:12:48,560
i load my data i have some data science

00:12:46,800 --> 00:12:49,839
parameters including things like

00:12:48,560 --> 00:12:53,440
the names or the columns that i'm

00:12:49,839 --> 00:12:56,079
interested in the model input table

00:12:53,440 --> 00:12:58,399
i import things like my parameters into

00:12:56,079 --> 00:13:00,800
the different functions that i'm using

00:12:58,399 --> 00:13:02,480
um and then i run my workflow and then i

00:13:00,800 --> 00:13:05,680
save my model at the end

00:13:02,480 --> 00:13:06,000
so looking much better right it could

00:13:05,680 --> 00:13:07,839
get

00:13:06,000 --> 00:13:09,519
even better and here's where we leverage

00:13:07,839 --> 00:13:11,360
some of kendra's functionality so now i

00:13:09,519 --> 00:13:13,440
introduce configuration to you

00:13:11,360 --> 00:13:15,200
we have this new thing called the

00:13:13,440 --> 00:13:16,800
content directory so over here we see

00:13:15,200 --> 00:13:18,639
we've introduced conf

00:13:16,800 --> 00:13:20,560
we've removed loading and saving paths

00:13:18,639 --> 00:13:21,519
for data and we have data science

00:13:20,560 --> 00:13:24,480
parameters now

00:13:21,519 --> 00:13:26,000
um also out of our workflow um so what

00:13:24,480 --> 00:13:28,959
we see over here is obviously

00:13:26,000 --> 00:13:30,399
configuration um we see we've introduced

00:13:28,959 --> 00:13:31,839
this conf directory

00:13:30,399 --> 00:13:33,600
and let's have a look at what's inside

00:13:31,839 --> 00:13:34,000
it so there's a there's a file called

00:13:33,600 --> 00:13:37,040
catalog

00:13:34,000 --> 00:13:38,720
yaml this is kedro's data catalog um

00:13:37,040 --> 00:13:41,360
it manages both the loading and the

00:13:38,720 --> 00:13:42,639
saving of data and you see it uses

00:13:41,360 --> 00:13:44,880
common integrations

00:13:42,639 --> 00:13:47,360
like pandas you can use spark you can

00:13:44,880 --> 00:13:49,680
use das you can use math.lib

00:13:47,360 --> 00:13:51,360
plotly or anything else hitter really

00:13:49,680 --> 00:13:52,720
considers these as connectors kind of

00:13:51,360 --> 00:13:53,920
like to whatever data source that you're

00:13:52,720 --> 00:13:55,680
looking for

00:13:53,920 --> 00:13:57,120
you have to specify a type in this case

00:13:55,680 --> 00:13:57,839
i know i'm going to be using the pandas

00:13:57,120 --> 00:14:00,959
api

00:13:57,839 --> 00:14:02,959
to read a csv data set

00:14:00,959 --> 00:14:04,639
and then also i'll tell kedrow exactly

00:14:02,959 --> 00:14:06,800
where this file is stored

00:14:04,639 --> 00:14:08,160
um so i can interact with it in that way

00:14:06,800 --> 00:14:08,800
you can do the same with excel for

00:14:08,160 --> 00:14:11,920
instance

00:14:08,800 --> 00:14:13,519
read um reading excel spreadsheets

00:14:11,920 --> 00:14:15,440
um the shadows table was an excel

00:14:13,519 --> 00:14:17,040
spreadsheet so i have things accordingly

00:14:15,440 --> 00:14:19,680
but i can even interact with things like

00:14:17,040 --> 00:14:21,279
pickle um which is quite cool

00:14:19,680 --> 00:14:23,040
if you're using the data catalog within

00:14:21,279 --> 00:14:24,560
the framework the keter framework it's

00:14:23,040 --> 00:14:26,720
not just as a library as we are

00:14:24,560 --> 00:14:28,399
using it now then you actually get

00:14:26,720 --> 00:14:29,920
things like versioning

00:14:28,399 --> 00:14:31,519
on top of your workflow as well which is

00:14:29,920 --> 00:14:33,440
quite cool

00:14:31,519 --> 00:14:35,920
let's have a look at parameters yaml

00:14:33,440 --> 00:14:39,199
which in this case over here we've got

00:14:35,920 --> 00:14:40,399
um essentially that block of parameters

00:14:39,199 --> 00:14:43,600
that we're in

00:14:40,399 --> 00:14:46,720
um our second example in our refactoring

00:14:43,600 --> 00:14:48,320
have now become um just parameters here

00:14:46,720 --> 00:14:49,839
that we can use

00:14:48,320 --> 00:14:53,920
so let's actually see what happens what

00:14:49,839 --> 00:14:53,920
changes happen in our jupyter notebooks

00:14:54,000 --> 00:14:58,160
so we see over here we now import a

00:14:56,639 --> 00:15:01,040
config loader from kedrow

00:14:58,160 --> 00:15:02,800
as well as io to specifically interact

00:15:01,040 --> 00:15:04,320
with our data catalog

00:15:02,800 --> 00:15:05,920
we tell ketter where to find our

00:15:04,320 --> 00:15:08,800
configuration in this place

00:15:05,920 --> 00:15:10,160
it's con we tell caterer we're

00:15:08,800 --> 00:15:11,120
specifically looking for the catalog

00:15:10,160 --> 00:15:13,279
yaml file

00:15:11,120 --> 00:15:14,720
so that we can create our data catalog

00:15:13,279 --> 00:15:16,720
and this is how you interact with

00:15:14,720 --> 00:15:18,800
loading and saving data in kedrow

00:15:16,720 --> 00:15:20,560
it becomes as simple as a catalog.load

00:15:18,800 --> 00:15:22,480
companies and kedron knows

00:15:20,560 --> 00:15:25,279
to use the pandas api to load the

00:15:22,480 --> 00:15:28,320
company's table from configuration

00:15:25,279 --> 00:15:30,639
if you are using keto natively in

00:15:28,320 --> 00:15:32,480
um within the framework and using a

00:15:30,639 --> 00:15:33,839
command called kendrick jupiter notebook

00:15:32,480 --> 00:15:36,399
to open up your

00:15:33,839 --> 00:15:37,759
notebook your ketter version of the

00:15:36,399 --> 00:15:40,079
jupyter notebook

00:15:37,759 --> 00:15:41,360
all you'd need to do is specify this you

00:15:40,079 --> 00:15:43,120
wouldn't have to specify any of this

00:15:41,360 --> 00:15:43,839
because we take care about knowledge for

00:15:43,120 --> 00:15:46,000
you

00:15:43,839 --> 00:15:47,519
so your workflow simply becomes catalog

00:15:46,000 --> 00:15:50,639
dot load

00:15:47,519 --> 00:15:52,320
everything else remains the same and

00:15:50,639 --> 00:15:54,880
when you need to save stuff

00:15:52,320 --> 00:15:56,880
you just use catalog.save um and

00:15:54,880 --> 00:15:58,480
quedro's data catalog will take care of

00:15:56,880 --> 00:15:59,839
the loading and the saving of data for

00:15:58,480 --> 00:16:01,440
you

00:15:59,839 --> 00:16:03,120
if we look at how the data science

00:16:01,440 --> 00:16:04,399
workflow changed we'll see something

00:16:03,120 --> 00:16:07,440
similar happened

00:16:04,399 --> 00:16:11,680
um so tell kendra where to find

00:16:07,440 --> 00:16:12,000
configuration um how to load our data

00:16:11,680 --> 00:16:14,639
set

00:16:12,000 --> 00:16:16,240
as well that we're interested in and

00:16:14,639 --> 00:16:19,360
then in this case

00:16:16,240 --> 00:16:22,639
we're interested in parameters

00:16:19,360 --> 00:16:24,240
so we get our parameters um using a

00:16:22,639 --> 00:16:28,320
dictionary format

00:16:24,240 --> 00:16:30,480
um from our confloader

00:16:28,320 --> 00:16:31,839
and then nothing else everything else

00:16:30,480 --> 00:16:35,519
stays the same

00:16:31,839 --> 00:16:35,519
so this is the second stage refactoring

00:16:35,600 --> 00:16:38,880
um so i'm actually just going to show

00:16:37,040 --> 00:16:39,360
you how easy it is is to create a new

00:16:38,880 --> 00:16:41,040
catcher

00:16:39,360 --> 00:16:42,480
project template and start like this

00:16:41,040 --> 00:16:45,199
from the beginning

00:16:42,480 --> 00:16:50,079
we're just going to go headroom new let

00:16:45,199 --> 00:16:52,480
me run that

00:16:50,079 --> 00:16:55,120
and it'll ask me for some name i'll just

00:16:52,480 --> 00:16:58,320
say this one is pycon

00:16:55,120 --> 00:17:00,240
uh demo i'll accept the default naming

00:16:58,320 --> 00:17:03,120
by pressing enter

00:17:00,240 --> 00:17:04,400
and when we have a look at our new

00:17:03,120 --> 00:17:05,839
folder structure you'll see that we have

00:17:04,400 --> 00:17:08,079
a new folder structure

00:17:05,839 --> 00:17:09,919
this one has the conf directory data

00:17:08,079 --> 00:17:12,799
docs logs notebooks and source

00:17:09,919 --> 00:17:14,720
i'll go into what all those things do

00:17:12,799 --> 00:17:16,480
but essentially yeah this is the start

00:17:14,720 --> 00:17:18,319
project template i mentioned series of

00:17:16,480 --> 00:17:19,760
files and folders it's modifiable

00:17:18,319 --> 00:17:21,360
it's got built-in support for python

00:17:19,760 --> 00:17:23,120
logging for pi test vienna tests and

00:17:21,360 --> 00:17:25,679
strengths for documentation

00:17:23,120 --> 00:17:27,600
it means that you can spend time on

00:17:25,679 --> 00:17:29,280
documenting your ml approach and not how

00:17:27,600 --> 00:17:30,720
you structured your project

00:17:29,280 --> 00:17:32,480
and then you spend less time digging

00:17:30,720 --> 00:17:34,080
around in previous projects for useful

00:17:32,480 --> 00:17:35,679
code and it's also easier for other

00:17:34,080 --> 00:17:37,919
collaborators to work with you

00:17:35,679 --> 00:17:39,840
so that's why it's useful to have that

00:17:37,919 --> 00:17:41,840
but let's actually go into

00:17:39,840 --> 00:17:43,360
overkiller space flights and i'm just

00:17:41,840 --> 00:17:47,200
going to copy the path

00:17:43,360 --> 00:17:50,799
for this one

00:17:47,200 --> 00:17:52,880
cool i'm in it um so let's actually have

00:17:50,799 --> 00:17:53,360
a look at how configuration is changed

00:17:52,880 --> 00:17:57,039
from

00:17:53,360 --> 00:17:59,600
o3 which was single directory

00:17:57,039 --> 00:18:00,080
with catalog and parameters in this case

00:17:59,600 --> 00:18:03,440
we now

00:18:00,080 --> 00:18:05,200
have um a conf directory that has um in

00:18:03,440 --> 00:18:06,160
a typical kedra project just base and

00:18:05,200 --> 00:18:08,240
local

00:18:06,160 --> 00:18:10,240
base is project sharable configuration

00:18:08,240 --> 00:18:12,000
which you can use across your team

00:18:10,240 --> 00:18:14,000
um and then local would be things like

00:18:12,000 --> 00:18:15,919
your vs code like

00:18:14,000 --> 00:18:17,600
um config you don't want to really share

00:18:15,919 --> 00:18:19,919
that with anyone else so you'll pop that

00:18:17,600 --> 00:18:22,160
in this folder because it's get ignored

00:18:19,919 --> 00:18:24,480
um so if we're going to base you'll see

00:18:22,160 --> 00:18:25,760
the same catalog.yaml is right here it's

00:18:24,480 --> 00:18:28,880
the same one

00:18:25,760 --> 00:18:30,559
um parameters.yaml is also the same um

00:18:28,880 --> 00:18:32,080
and we also have a configuration for

00:18:30,559 --> 00:18:34,080
logging as well this is

00:18:32,080 --> 00:18:35,600
uh generated for you automatically it's

00:18:34,080 --> 00:18:37,440
not something that you have to do

00:18:35,600 --> 00:18:39,360
and it gives you python logging control

00:18:37,440 --> 00:18:40,880
for free you can modify this file if you

00:18:39,360 --> 00:18:43,440
want

00:18:40,880 --> 00:18:45,679
it's really useful to have configuration

00:18:43,440 --> 00:18:48,480
because i mentioned

00:18:45,679 --> 00:18:50,160
um with that same code base even with

00:18:48,480 --> 00:18:51,679
our notebook example that we had in the

00:18:50,160 --> 00:18:54,080
previous example

00:18:51,679 --> 00:18:55,280
i could change the path to the

00:18:54,080 --> 00:18:58,880
configuration that i'm

00:18:55,280 --> 00:19:00,720
like i'm interested in and run it with

00:18:58,880 --> 00:19:03,600
maybe my data wasn't on my local

00:19:00,720 --> 00:19:07,520
computer it was on s3

00:19:03,600 --> 00:19:09,440
maybe it was a sql table or sql database

00:19:07,520 --> 00:19:12,320
that i was interested in

00:19:09,440 --> 00:19:13,360
maybe it was on google cloud platform

00:19:12,320 --> 00:19:15,280
having

00:19:13,360 --> 00:19:16,880
this flexibility means that i could

00:19:15,280 --> 00:19:20,000
easily do something like

00:19:16,880 --> 00:19:22,400
header run um with my base configuration

00:19:20,000 --> 00:19:25,360
with files on my local computer

00:19:22,400 --> 00:19:27,280
but if i add i'm really interested in

00:19:25,360 --> 00:19:30,000
doing a header run

00:19:27,280 --> 00:19:31,760
with my production version of my code

00:19:30,000 --> 00:19:33,039
base it's very easy to transition to

00:19:31,760 --> 00:19:36,080
that right now

00:19:33,039 --> 00:19:39,200
same code different config

00:19:36,080 --> 00:19:39,679
um i'll talk next about data so in most

00:19:39,200 --> 00:19:41,360
cases

00:19:39,679 --> 00:19:43,360
enterprise data science i mentioned does

00:19:41,360 --> 00:19:44,080
not have data stored on the local

00:19:43,360 --> 00:19:45,679
computer

00:19:44,080 --> 00:19:47,679
but you will note that we have a data

00:19:45,679 --> 00:19:50,240
data engineering convention here which

00:19:47,679 --> 00:19:52,400
specifies things like like your raw data

00:19:50,240 --> 00:19:54,080
is immutable data and in this case

00:19:52,400 --> 00:19:56,000
our company's reviews and shuttle's data

00:19:54,080 --> 00:19:56,799
will sit here um but generally we

00:19:56,000 --> 00:19:58,000
recommend that

00:19:56,799 --> 00:20:00,240
i mean like even if you're working from

00:19:58,000 --> 00:20:01,679
s3 or zero blob storage or something

00:20:00,240 --> 00:20:03,280
that you structure your folders the same

00:20:01,679 --> 00:20:05,120
way because it keeps the convention on

00:20:03,280 --> 00:20:07,360
how what you would expect each

00:20:05,120 --> 00:20:08,320
different type of data processing to

00:20:07,360 --> 00:20:09,520
have happened

00:20:08,320 --> 00:20:12,480
during the different levels that you

00:20:09,520 --> 00:20:13,919
move we also have a dots folder

00:20:12,480 --> 00:20:15,520
if you're using docs strings in your

00:20:13,919 --> 00:20:16,880
python function schedule

00:20:15,520 --> 00:20:18,880
we use things to generate your

00:20:16,880 --> 00:20:20,320
documentation for you

00:20:18,880 --> 00:20:22,000
we have a folder for logs this is where

00:20:20,320 --> 00:20:23,360
we support python logging so your logs

00:20:22,000 --> 00:20:24,400
will go there

00:20:23,360 --> 00:20:26,000
there's obviously place for your

00:20:24,400 --> 00:20:26,640
notebook still in this project template

00:20:26,000 --> 00:20:29,200
too

00:20:26,640 --> 00:20:30,080
um and then the really important folder

00:20:29,200 --> 00:20:32,799
is source

00:20:30,080 --> 00:20:33,520
so source actually becomes the python

00:20:32,799 --> 00:20:35,280
code

00:20:33,520 --> 00:20:37,120
that you wrote in those jupiter

00:20:35,280 --> 00:20:38,080
notebooks that's what we're interested

00:20:37,120 --> 00:20:40,960
in here

00:20:38,080 --> 00:20:42,880
and remember in that case we had two

00:20:40,960 --> 00:20:45,200
notebooks that we're interested in

00:20:42,880 --> 00:20:46,720
data processing um one and a data

00:20:45,200 --> 00:20:48,000
science one so let's go into data

00:20:46,720 --> 00:20:51,039
processing and have a look

00:20:48,000 --> 00:20:54,159
at what is happening here

00:20:51,039 --> 00:20:56,480
so we have our header nodes in this case

00:20:54,159 --> 00:20:57,919
they are the utility functions or helper

00:20:56,480 --> 00:21:00,159
functions that we had

00:20:57,919 --> 00:21:01,919
and then we had three functions below it

00:21:00,159 --> 00:21:03,840
pre-process companies pre-processed

00:21:01,919 --> 00:21:05,120
shuttles and create model input

00:21:03,840 --> 00:21:08,320
i'm going to show you what it looks like

00:21:05,120 --> 00:21:10,960
now in kendra um

00:21:08,320 --> 00:21:12,559
when we actually go through the

00:21:10,960 --> 00:21:14,799
pipeline.pi so i'll

00:21:12,559 --> 00:21:17,039
show you that now so if i want to

00:21:14,799 --> 00:21:18,880
combine these ones into a workflow

00:21:17,039 --> 00:21:21,120
i'll take the function repost this

00:21:18,880 --> 00:21:24,320
companies i'll use its input the

00:21:21,120 --> 00:21:26,880
companies table cold from config

00:21:24,320 --> 00:21:26,880
right here

00:21:28,840 --> 00:21:33,120
here okay

00:21:31,440 --> 00:21:35,360
my output will be pre-processed

00:21:33,120 --> 00:21:37,760
companies and i'll just call the node

00:21:35,360 --> 00:21:39,679
preprocess companies node in this case

00:21:37,760 --> 00:21:41,760
i'll do the same with the shadows table

00:21:39,679 --> 00:21:43,840
and then i'll create my mobile input

00:21:41,760 --> 00:21:45,360
here's what it looks like

00:21:43,840 --> 00:21:47,039
because now you can actually visualize

00:21:45,360 --> 00:21:50,400
your workflow

00:21:47,039 --> 00:21:58,400
i'm just going to run kids revis

00:21:50,400 --> 00:22:01,360
and show you what this looks like

00:21:58,400 --> 00:22:01,360
let's go from here

00:22:02,000 --> 00:22:05,760
so here are those three tables companies

00:22:04,720 --> 00:22:08,640
shuttles

00:22:05,760 --> 00:22:09,679
and reviews there's the pre-processed

00:22:08,640 --> 00:22:11,760
companies node

00:22:09,679 --> 00:22:13,679
the pre-processed shuttles node and the

00:22:11,760 --> 00:22:15,360
create model input node that created our

00:22:13,679 --> 00:22:17,039
model input table

00:22:15,360 --> 00:22:18,559
we knew we had some sort of data science

00:22:17,039 --> 00:22:21,360
parameters we mentioned

00:22:18,559 --> 00:22:22,960
those column names that we're interested

00:22:21,360 --> 00:22:24,400
in from our table

00:22:22,960 --> 00:22:26,400
the fact that we're interested in we had

00:22:24,400 --> 00:22:28,320
random state and a test size as well

00:22:26,400 --> 00:22:29,520
and then we split our data and we were

00:22:28,320 --> 00:22:31,280
able to do things

00:22:29,520 --> 00:22:33,360
and you get this view for free um for

00:22:31,280 --> 00:22:36,080
the first time with kedron

00:22:33,360 --> 00:22:37,440
so what i'll do is i'll stop here um

00:22:36,080 --> 00:22:40,080
maybe i'll do a

00:22:37,440 --> 00:22:41,120
caterer run as i talk through the rest

00:22:40,080 --> 00:22:44,159
of the stuff

00:22:41,120 --> 00:22:46,400
you might be interested in um so yeah um

00:22:44,159 --> 00:22:47,919
essentially caterer now helps with this

00:22:46,400 --> 00:22:49,360
reproducible workflow as

00:22:47,919 --> 00:22:51,039
you were able to take your jupiter

00:22:49,360 --> 00:22:52,400
notebooks and refactor them in different

00:22:51,039 --> 00:22:53,919
stages

00:22:52,400 --> 00:22:56,400
you eventually get down to something

00:22:53,919 --> 00:22:58,320
that you can use um indicator project

00:22:56,400 --> 00:23:00,240
but we do always recommend actually

00:22:58,320 --> 00:23:01,600
starting off in your caterer project

00:23:00,240 --> 00:23:02,880
i mean there is support for jupiter

00:23:01,600 --> 00:23:04,640
notebook if you were going to be using

00:23:02,880 --> 00:23:07,919
that and it's much nicer

00:23:04,640 --> 00:23:09,280
um a much nicer experience within um

00:23:07,919 --> 00:23:10,080
because you can interact with conflict

00:23:09,280 --> 00:23:11,330
directly

00:23:10,080 --> 00:23:13,200
for instance um

00:23:11,330 --> 00:23:15,520
[Music]

00:23:13,200 --> 00:23:16,799
so the proper way to actually interact

00:23:15,520 --> 00:23:19,919
with a jupiter notebook

00:23:16,799 --> 00:23:20,960
in kendra if you're interested is maybe

00:23:19,919 --> 00:23:22,640
actually best shown through our

00:23:20,960 --> 00:23:23,520
documentation so this is using caterer

00:23:22,640 --> 00:23:25,520
with ipython

00:23:23,520 --> 00:23:27,360
and jupyter notebooks or lab um

00:23:25,520 --> 00:23:28,960
whichever you prefer

00:23:27,360 --> 00:23:30,480
and what you find is that we have our

00:23:28,960 --> 00:23:31,760
own cli commands for

00:23:30,480 --> 00:23:33,760
interacting with opening up those

00:23:31,760 --> 00:23:37,280
sessions so you'd run kidry python

00:23:33,760 --> 00:23:39,200
or you'd run um kedro jupiter notebook

00:23:37,280 --> 00:23:41,520
down here

00:23:39,200 --> 00:23:43,279
and you'd see it's a simple matter of

00:23:41,520 --> 00:23:45,279
just running catalog.load

00:23:43,279 --> 00:23:46,799
um because kedro knows where your

00:23:45,279 --> 00:23:48,400
configuration should be

00:23:46,799 --> 00:23:50,400
um and you get to interact with your

00:23:48,400 --> 00:23:52,320
data front like this

00:23:50,400 --> 00:23:53,440
um once it's specified in configuration

00:23:52,320 --> 00:23:57,039
for you

00:23:53,440 --> 00:23:59,679
uh so yeah that's the ease of

00:23:57,039 --> 00:24:01,360
starting firstly starting with kedrow in

00:23:59,679 --> 00:24:03,039
your jupyter notebook workflow

00:24:01,360 --> 00:24:04,720
you also will see that example the

00:24:03,039 --> 00:24:07,120
tutorials taking you through

00:24:04,720 --> 00:24:08,640
um caterer space flights one is right

00:24:07,120 --> 00:24:10,480
here it will get you acquainted with

00:24:08,640 --> 00:24:13,360
beginner to intermediate functionality

00:24:10,480 --> 00:24:15,279
quite quickly um in kendra and show you

00:24:13,360 --> 00:24:17,120
what a typical development workflow will

00:24:15,279 --> 00:24:18,480
look like set up the project template

00:24:17,120 --> 00:24:20,720
set up where you're getting your data

00:24:18,480 --> 00:24:22,080
from create your pipeline and package

00:24:20,720 --> 00:24:23,120
your project and then visualize it at

00:24:22,080 --> 00:24:25,200
the end

00:24:23,120 --> 00:24:29,360
uh so yeah shout out if you have any

00:24:25,200 --> 00:24:29,360
questions and i'll talk to you

00:24:30,840 --> 00:24:33,840
soon

00:25:36,640 --> 00:25:38,720

YouTube URL: https://www.youtube.com/watch?v=JLTYNPoK7nw


