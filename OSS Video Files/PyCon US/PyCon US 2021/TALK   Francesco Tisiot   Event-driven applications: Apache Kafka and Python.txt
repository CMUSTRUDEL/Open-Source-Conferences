Title: TALK   Francesco Tisiot   Event-driven applications: Apache Kafka and Python
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Code and data go together like tomato and basil; not many applications work without moving data in some way. As our applications modernise and evolve to become more event-driven, the requirements for data are changing. In this session we will explore Apache Kafka, a data streaming platform, to enable reliable real-time data integration for your applications.
We will look at the types of problems that Kafka is best at solving, and show how to use it in your own applications. Whether you have a new application or are looking to upgrade an existing one, this session includes advice on adding Kafka using the Python libraries and includes code examples (with bonus discussion of pizza toppings) to use.
With Kafka in place, many things are possible so this session also introduces Kafka Connect, a selection of pre-built connectors that you can use to route events between systems and integrate with other tools. This session is recommended for engineers and architects whose applications are ready for next-level data abilities.

Notebook link: https://github.com/aiven/kafka-python-notebooks
Slides: https://speakerdeck.com/ftisiot/event-driven-applications-apache-kafka-and-python
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,320 --> 00:00:17,119
hi i'm francisco tizot developer

00:00:15,839 --> 00:00:19,039
advocato type

00:00:17,119 --> 00:00:20,800
in this session we will check out how

00:00:19,039 --> 00:00:23,119
you can create event driven application

00:00:20,800 --> 00:00:25,920
using apache kafka and python

00:00:23,119 --> 00:00:27,519
first of all if you are here at pycon i

00:00:25,920 --> 00:00:29,039
believe you will be somehow familiar

00:00:27,519 --> 00:00:32,000
with python

00:00:29,039 --> 00:00:33,760
but you may start guessing why should i

00:00:32,000 --> 00:00:35,680
care about apache kafka

00:00:33,760 --> 00:00:37,280
well i believe you will be somehow

00:00:35,680 --> 00:00:38,399
programmer so you will be creating

00:00:37,280 --> 00:00:40,399
application

00:00:38,399 --> 00:00:41,600
and most likely event driven application

00:00:40,399 --> 00:00:43,600
what are those

00:00:41,600 --> 00:00:44,879
are applications that as soon as an

00:00:43,600 --> 00:00:47,520
event happens in real

00:00:44,879 --> 00:00:48,879
life they will need to know about it

00:00:47,520 --> 00:00:50,559
they will start parsing it

00:00:48,879 --> 00:00:52,079
and probably they will take the output

00:00:50,559 --> 00:00:53,680
of their computation and pass it to

00:00:52,079 --> 00:00:54,480
another application probably even

00:00:53,680 --> 00:00:58,000
revenue

00:00:54,480 --> 00:01:00,079
to create a chain of them so why

00:00:58,000 --> 00:01:01,280
should you hit event driven application

00:01:00,079 --> 00:01:03,520
well because

00:01:01,280 --> 00:01:04,640
we live in a fast world and we are using

00:01:03,520 --> 00:01:07,840
we are used to have

00:01:04,640 --> 00:01:09,760
events happening every second

00:01:07,840 --> 00:01:11,520
and we are also used to receive

00:01:09,760 --> 00:01:13,760
notification about those events

00:01:11,520 --> 00:01:15,600
for example when we receive a message or

00:01:13,760 --> 00:01:17,200
when we make a payment with our credit

00:01:15,600 --> 00:01:18,400
card and we receive the related

00:01:17,200 --> 00:01:21,200
notification

00:01:18,400 --> 00:01:22,960
or when someone else stores stalls our

00:01:21,200 --> 00:01:25,119
credit card and makes a payment

00:01:22,960 --> 00:01:26,960
we receive a weird notification about

00:01:25,119 --> 00:01:27,840
purchasing something that we didn't want

00:01:26,960 --> 00:01:30,159
to

00:01:27,840 --> 00:01:31,280
and at the time that we received the

00:01:30,159 --> 00:01:33,280
notification

00:01:31,280 --> 00:01:34,320
we want to receive it immediately we

00:01:33,280 --> 00:01:36,560
cannot wait like

00:01:34,320 --> 00:01:38,079
six hours or three hours or 20 minutes

00:01:36,560 --> 00:01:40,320
or five minutes of batch time

00:01:38,079 --> 00:01:42,399
we want to receive a notification that

00:01:40,320 --> 00:01:45,680
our credit card was stolen immediately

00:01:42,399 --> 00:01:47,520
because we want to react

00:01:45,680 --> 00:01:49,759
as an event-driven application and

00:01:47,520 --> 00:01:51,040
probably we want to change to call our

00:01:49,759 --> 00:01:53,840
bank in order to stop

00:01:51,040 --> 00:01:54,960
our credit card from being used the same

00:01:53,840 --> 00:01:57,280
goes well with

00:01:54,960 --> 00:01:59,200
food you know i'm italian so i will at a

00:01:57,280 --> 00:02:01,040
certain point of my talk go into food

00:01:59,200 --> 00:02:03,759
and it's better to do it now

00:02:01,040 --> 00:02:05,360
when you take your phone you open your

00:02:03,759 --> 00:02:07,040
food delivery very app you select the

00:02:05,360 --> 00:02:08,959
restaurant you select the pizzas that

00:02:07,040 --> 00:02:09,920
you want and you submit the order that

00:02:08,959 --> 00:02:11,680
is generating

00:02:09,920 --> 00:02:13,120
a chain of event driven application

00:02:11,680 --> 00:02:14,879
first of all your order will be

00:02:13,120 --> 00:02:16,319
submitted to the restaurant that will

00:02:14,879 --> 00:02:18,239
act as an application

00:02:16,319 --> 00:02:19,760
and start creating the pizzas for you

00:02:18,239 --> 00:02:21,200
once the pizza will be ready

00:02:19,760 --> 00:02:22,879
it will be another event for the

00:02:21,200 --> 00:02:26,000
delivery people

00:02:22,879 --> 00:02:27,440
that will take to your home

00:02:26,000 --> 00:02:29,120
both the cases of credit card and

00:02:27,440 --> 00:02:32,480
delivery have something in common

00:02:29,120 --> 00:02:33,920
you can understand that the value of the

00:02:32,480 --> 00:02:35,680
information is strictly

00:02:33,920 --> 00:02:36,959
related to the time that it takes to be

00:02:35,680 --> 00:02:40,400
delivered

00:02:36,959 --> 00:02:40,800
with a credit card i need to know now

00:02:40,400 --> 00:02:43,680
that

00:02:40,800 --> 00:02:44,959
my credit card has been used with the

00:02:43,680 --> 00:02:48,080
delivery

00:02:44,959 --> 00:02:48,959
position of the delivery person i need

00:02:48,080 --> 00:02:51,760
to know

00:02:48,959 --> 00:02:52,480
now what the position is i need to know

00:02:51,760 --> 00:02:54,560
that in

00:02:52,480 --> 00:02:56,560
for example 10 seconds delay i couldn't

00:02:54,560 --> 00:02:57,599
care less what the position was 5 or 10

00:02:56,560 --> 00:03:00,959
minutes ago

00:02:57,599 --> 00:03:04,720
the information needs to

00:03:00,959 --> 00:03:07,920
find his its way in real time

00:03:04,720 --> 00:03:09,519
so we need a component that makes this

00:03:07,920 --> 00:03:12,319
transmission of data

00:03:09,519 --> 00:03:13,200
this communication easy and reliable and

00:03:12,319 --> 00:03:17,040
this is what

00:03:13,200 --> 00:03:20,319
kafka is for so what is apache kafka

00:03:17,040 --> 00:03:23,920
well the concept of kafka is really easy

00:03:20,319 --> 00:03:25,280
it's concept of a log file a log file

00:03:23,920 --> 00:03:27,120
where event happens

00:03:25,280 --> 00:03:29,120
and they get stored in kafka so event

00:03:27,120 --> 00:03:32,319
zero happens and gets stored

00:03:29,120 --> 00:03:35,280
event one happens get store two 3 and 4.

00:03:32,319 --> 00:03:38,080
even more it's up and only and immutable

00:03:35,280 --> 00:03:39,680
this means that once event 0 is stored

00:03:38,080 --> 00:03:41,920
it's not like a record in the database i

00:03:39,680 --> 00:03:43,920
cannot go there and change it

00:03:41,920 --> 00:03:45,680
if something happens that changes the

00:03:43,920 --> 00:03:48,080
reality of event 0

00:03:45,680 --> 00:03:50,080
i will have to add that to my log as a

00:03:48,080 --> 00:03:52,239
new event

00:03:50,080 --> 00:03:55,040
even more kafka allows us to manage

00:03:52,239 --> 00:03:56,959
multiple event types in multiple logs

00:03:55,040 --> 00:03:59,680
for example i have my pizza order log

00:03:56,959 --> 00:04:04,080
and my delivery position log

00:03:59,680 --> 00:04:07,360
in kafka terms stores are called topics

00:04:04,080 --> 00:04:11,439
kafka doesn't come with a huge

00:04:07,360 --> 00:04:13,360
server kafka is a distributed platform

00:04:11,439 --> 00:04:15,040
so this means that when you create a

00:04:13,360 --> 00:04:16,160
kafka instance you are actually most

00:04:15,040 --> 00:04:19,199
likely creating

00:04:16,160 --> 00:04:20,079
a set of nodes of kafka which in kafka

00:04:19,199 --> 00:04:23,360
terms are called

00:04:20,079 --> 00:04:24,080
brokers and now your log information is

00:04:23,360 --> 00:04:26,560
stored

00:04:24,080 --> 00:04:28,880
in the brokers and as you can see in the

00:04:26,560 --> 00:04:30,880
image is not stored once but is stored

00:04:28,880 --> 00:04:33,280
in multiple copies

00:04:30,880 --> 00:04:34,800
the number of copies of each topic that

00:04:33,280 --> 00:04:36,479
are stored across brokers

00:04:34,800 --> 00:04:38,560
is defined by a parameter called

00:04:36,479 --> 00:04:40,560
replication factor so we have three

00:04:38,560 --> 00:04:43,120
copies for our sharp edges log and

00:04:40,560 --> 00:04:44,800
two copies for the other one why do we

00:04:43,120 --> 00:04:47,120
do copis because we know that the

00:04:44,800 --> 00:04:48,800
computers are not completely reliable

00:04:47,120 --> 00:04:50,560
so we can know we know that we could

00:04:48,800 --> 00:04:52,960
lose a node but still we are not going

00:04:50,560 --> 00:04:56,080
to lose information

00:04:52,960 --> 00:04:59,199
so we know that kafka is a good platform

00:04:56,080 --> 00:05:01,280
to manage logs but what is

00:04:59,199 --> 00:05:02,560
sorry to manage events but what is an

00:05:01,280 --> 00:05:04,800
event for kafka

00:05:02,560 --> 00:05:06,880
well for all what matters to kafka an

00:05:04,800 --> 00:05:08,560
event is just a key value pair

00:05:06,880 --> 00:05:10,960
and the beauty of kafka is that it

00:05:08,560 --> 00:05:11,440
doesn't care what you put in the key of

00:05:10,960 --> 00:05:14,000
the or

00:05:11,440 --> 00:05:15,520
in the value it's just a series of bytes

00:05:14,000 --> 00:05:19,120
so you could go from really

00:05:15,520 --> 00:05:20,639
simple messages like maximum temperature

00:05:19,120 --> 00:05:22,880
385 to 3

00:05:20,639 --> 00:05:24,800
or you could go pretty wild for example

00:05:22,880 --> 00:05:26,479
you want to include

00:05:24,800 --> 00:05:28,080
in both the key and the value of json

00:05:26,479 --> 00:05:31,280
for the order that you

00:05:28,080 --> 00:05:33,120
are ordering and specifying that

00:05:31,280 --> 00:05:34,479
on the key the restaurant receiving the

00:05:33,120 --> 00:05:35,039
order and the phone line used to make

00:05:34,479 --> 00:05:37,280
the call

00:05:35,039 --> 00:05:38,479
and in the value the order id the name

00:05:37,280 --> 00:05:41,680
of the person

00:05:38,479 --> 00:05:44,479
making the order and the list of pizzas

00:05:41,680 --> 00:05:45,199
as you can see you can specify that in

00:05:44,479 --> 00:05:46,960
any format

00:05:45,199 --> 00:05:49,120
in this case it's json which is

00:05:46,960 --> 00:05:51,840
beautiful because i can read

00:05:49,120 --> 00:05:53,840
the world data set with my eyes but it's

00:05:51,840 --> 00:05:56,000
pretty heavy when transmitted on wire

00:05:53,840 --> 00:05:57,759
because it contains both the fill name

00:05:56,000 --> 00:05:59,520
and the fill value for every field in

00:05:57,759 --> 00:06:01,199
your data set if you want

00:05:59,520 --> 00:06:02,880
a more compacted version of the same

00:06:01,199 --> 00:06:05,680
information you could use

00:06:02,880 --> 00:06:06,560
avro or protovat which basically detach

00:06:05,680 --> 00:06:08,479
the schema

00:06:06,560 --> 00:06:10,319
from the payload and send the schema to

00:06:08,479 --> 00:06:12,720
schema registry which is used

00:06:10,319 --> 00:06:15,759
in order to compress the data and also

00:06:12,720 --> 00:06:19,440
when you read the data to uncompress it

00:06:15,759 --> 00:06:21,680
so now how can we write to kafka

00:06:19,440 --> 00:06:23,520
well we know that we are we love python

00:06:21,680 --> 00:06:25,120
so we have our iphone application that

00:06:23,520 --> 00:06:27,840
wants to browse to kafka

00:06:25,120 --> 00:06:30,000
and in this case it's called a producer

00:06:27,840 --> 00:06:31,840
it writes to a topic

00:06:30,000 --> 00:06:34,160
and in order to do that it only needs to

00:06:31,840 --> 00:06:36,080
know three information the first one is

00:06:34,160 --> 00:06:38,319
where to find kafka list of broker host

00:06:36,080 --> 00:06:40,080
name and ports the second one is how to

00:06:38,319 --> 00:06:43,039
authenticate do we need ssl

00:06:40,080 --> 00:06:44,319
sasl something else third one how to

00:06:43,039 --> 00:06:46,240
encode the data from

00:06:44,319 --> 00:06:49,360
for example json to the row series of

00:06:46,240 --> 00:06:51,360
bytes that kafka understands

00:06:49,360 --> 00:06:52,639
on the other side i can read from kafka

00:06:51,360 --> 00:06:55,840
and if i have an application

00:06:52,639 --> 00:06:57,759
python it's called the consumer and now

00:06:55,840 --> 00:06:58,560
the consumer works is that it reads

00:06:57,759 --> 00:07:00,720
event number

00:06:58,560 --> 00:07:02,639
zero and then it communicates kafka zero

00:07:00,720 --> 00:07:04,639
down let's move to one

00:07:02,639 --> 00:07:05,919
read events number one communicate back

00:07:04,639 --> 00:07:10,240
to kafka one done

00:07:05,919 --> 00:07:12,479
let's move to two etc etc

00:07:10,240 --> 00:07:14,400
why communicating back is so important

00:07:12,479 --> 00:07:16,160
well because

00:07:14,400 --> 00:07:17,919
we know computers are not reliable so

00:07:16,160 --> 00:07:19,919
the consumer could go down

00:07:17,919 --> 00:07:21,919
still kafka will know until what point a

00:07:19,919 --> 00:07:22,319
certain consumer was reading a certain

00:07:21,919 --> 00:07:24,080
log

00:07:22,319 --> 00:07:25,759
so the next time the same consumer pops

00:07:24,080 --> 00:07:27,680
up it will probably submit

00:07:25,759 --> 00:07:29,759
the message number three because it was

00:07:27,680 --> 00:07:31,599
the last one not being read

00:07:29,759 --> 00:07:33,120
in order to read data from kafka the

00:07:31,599 --> 00:07:36,319
consumer needs to know where to find

00:07:33,120 --> 00:07:38,080
kafka how to authenticate as before

00:07:36,319 --> 00:07:40,160
before we were encoding that and now we

00:07:38,080 --> 00:07:42,880
need to decode the same data

00:07:40,160 --> 00:07:44,560
and we also need to know which topics we

00:07:42,880 --> 00:07:47,520
want to read from

00:07:44,560 --> 00:07:48,160
all simple now don't trust me when i

00:07:47,520 --> 00:07:50,879
tell you

00:07:48,160 --> 00:07:53,599
trust me when i show you let's check

00:07:50,879 --> 00:07:55,840
with a nice

00:07:53,599 --> 00:07:57,919
jupiter notebook that i created for you

00:07:55,840 --> 00:07:59,360
what this notebook does it's a series of

00:07:57,919 --> 00:08:01,440
notebook pages

00:07:59,360 --> 00:08:03,599
there are more more than one which goes

00:08:01,440 --> 00:08:06,560
through several steps of kafka

00:08:03,599 --> 00:08:07,599
and the basic the first one it creates a

00:08:06,560 --> 00:08:10,400
kafka instance

00:08:07,599 --> 00:08:11,280
under the ivan managed services if you

00:08:10,400 --> 00:08:12,879
want to show

00:08:11,280 --> 00:08:15,840
to see i haven't managed services i

00:08:12,879 --> 00:08:18,720
created this kakka python

00:08:15,840 --> 00:08:20,720
instance for us which is in google cloud

00:08:18,720 --> 00:08:20,960
europe germany but you can create your

00:08:20,720 --> 00:08:23,280
own

00:08:20,960 --> 00:08:25,199
instance pretty easily you can select

00:08:23,280 --> 00:08:26,960
not only kafka but a set of open source

00:08:25,199 --> 00:08:28,639
data platforms you can select

00:08:26,960 --> 00:08:30,160
one of the cloud providers available

00:08:28,639 --> 00:08:30,720
within the cloud provider you can select

00:08:30,160 --> 00:08:32,560
the region

00:08:30,720 --> 00:08:34,959
and you can select the plan other

00:08:32,560 --> 00:08:37,200
goodies of ivan is that you can upscale

00:08:34,959 --> 00:08:39,200
or downscale you can move your kafka

00:08:37,200 --> 00:08:41,200
cluster from one provider to the other

00:08:39,200 --> 00:08:43,279
you can upgrade your kafka cluster

00:08:41,200 --> 00:08:44,240
and all those action will happen while

00:08:43,279 --> 00:08:46,240
the class

00:08:44,240 --> 00:08:48,240
while the cluster is still online so you

00:08:46,240 --> 00:08:50,959
don't have any downtime

00:08:48,240 --> 00:08:52,000
apart from that let's go back to our

00:08:50,959 --> 00:08:53,680
notebook

00:08:52,000 --> 00:08:55,040
and we want to know how to push data to

00:08:53,680 --> 00:08:59,040
kafka so let's start

00:08:55,040 --> 00:09:00,640
a producer first step we installed kafka

00:08:59,040 --> 00:09:02,160
python which is the library

00:09:00,640 --> 00:09:05,120
the basic library that allows us to

00:09:02,160 --> 00:09:07,040
communicate with kafka

00:09:05,120 --> 00:09:08,800
once this is done we can create the

00:09:07,040 --> 00:09:11,680
producer all we need to know

00:09:08,800 --> 00:09:13,360
is where to find kafka hostname and port

00:09:11,680 --> 00:09:16,080
i'm using parameters but

00:09:13,360 --> 00:09:16,959
you can put strings here it's the same

00:09:16,080 --> 00:09:19,200
how to connect

00:09:16,959 --> 00:09:20,880
ssl using three certificates and how to

00:09:19,200 --> 00:09:24,720
serialize how to encode the data

00:09:20,880 --> 00:09:26,800
using from json to a row series of byte

00:09:24,720 --> 00:09:29,600
for both the value and the key

00:09:26,800 --> 00:09:31,839
let's execute this okay now we are ready

00:09:29,600 --> 00:09:35,200
to push our first record to kafka

00:09:31,839 --> 00:09:38,240
let's put to a topic name

00:09:35,200 --> 00:09:40,160
with key ed1 and the value it's

00:09:38,240 --> 00:09:41,680
francesco myself ordering a nice pizza

00:09:40,160 --> 00:09:43,120
margarita i'm a simple guy i want a

00:09:41,680 --> 00:09:46,080
moderator

00:09:43,120 --> 00:09:47,360
okay now the record is in kafka how can

00:09:46,080 --> 00:09:50,480
i check that

00:09:47,360 --> 00:09:53,279
let me let me create a consumer

00:09:50,480 --> 00:09:54,640
let's create a consumer over here and

00:09:53,279 --> 00:09:57,680
close

00:09:54,640 --> 00:09:59,360
okay i want to create a consumer so

00:09:57,680 --> 00:10:01,040
let's leave out the group id for now we

00:09:59,360 --> 00:10:03,839
will come back to this later

00:10:01,040 --> 00:10:05,519
i create a consumer with named clyde1 it

00:10:03,839 --> 00:10:08,880
can be anything

00:10:05,519 --> 00:10:09,839
and the server security certificates are

00:10:08,880 --> 00:10:11,760
the same as before

00:10:09,839 --> 00:10:13,040
before i was serializing now i'm

00:10:11,760 --> 00:10:16,320
deserializing from

00:10:13,040 --> 00:10:17,360
from row series of black to json all

00:10:16,320 --> 00:10:20,640
pretty

00:10:17,360 --> 00:10:22,160
clear now i can check which topics are

00:10:20,640 --> 00:10:24,000
available in kafka

00:10:22,160 --> 00:10:26,000
and i can see some internal topics

00:10:24,000 --> 00:10:26,640
together with a nice francesco pizza

00:10:26,000 --> 00:10:28,160
topic

00:10:26,640 --> 00:10:30,160
that is the one that i created for the

00:10:28,160 --> 00:10:33,680
purpose of this demo

00:10:30,160 --> 00:10:35,680
i can subscribe to it and now i can

00:10:33,680 --> 00:10:37,519
start reading

00:10:35,680 --> 00:10:39,360
we can immediately notice a couple of

00:10:37,519 --> 00:10:40,720
things the first one being that the

00:10:39,360 --> 00:10:44,320
consumer

00:10:40,720 --> 00:10:46,079
never stops well because we are

00:10:44,320 --> 00:10:47,680
we are generating an event-driven

00:10:46,079 --> 00:10:49,519
application so we want to be ready as

00:10:47,680 --> 00:10:50,800
soon as kafka has a message for us to

00:10:49,519 --> 00:10:52,800
read from it

00:10:50,800 --> 00:10:53,839
and in streaming world there is never an

00:10:52,800 --> 00:10:55,279
end

00:10:53,839 --> 00:10:57,519
the other bit which is interesting is

00:10:55,279 --> 00:10:59,360
that we push the order from francesco

00:10:57,519 --> 00:11:00,000
ordering pizza margarita but we don't

00:10:59,360 --> 00:11:01,920
see it

00:11:00,000 --> 00:11:04,240
on the consumer side well this is

00:11:01,920 --> 00:11:06,720
because by default

00:11:04,240 --> 00:11:08,480
the consumer starts reading from kafka

00:11:06,720 --> 00:11:10,560
from the point in time that it attached

00:11:08,480 --> 00:11:13,360
to kafka

00:11:10,560 --> 00:11:14,240
and we attach later then we push our

00:11:13,360 --> 00:11:15,760
first record

00:11:14,240 --> 00:11:17,519
this is the default behavior we will

00:11:15,760 --> 00:11:19,440
later see how to change this

00:11:17,519 --> 00:11:21,360
but just bear in mind that this is the

00:11:19,440 --> 00:11:22,640
default in order to prove that the wall

00:11:21,360 --> 00:11:25,519
pipeline is working

00:11:22,640 --> 00:11:26,800
let me produce other two events a delay

00:11:25,519 --> 00:11:28,959
ordering a pizza y

00:11:26,800 --> 00:11:31,200
and mark all dream pizza with chocolate

00:11:28,959 --> 00:11:32,880
now you know i'm italian so

00:11:31,200 --> 00:11:34,320
if you come to italy and you order a

00:11:32,880 --> 00:11:37,360
pizza why you probably

00:11:34,320 --> 00:11:38,079
will end up in jail so i'm just telling

00:11:37,360 --> 00:11:40,240
you i'm

00:11:38,079 --> 00:11:41,120
just suggesting you not doing that in

00:11:40,240 --> 00:11:44,160
italy but

00:11:41,120 --> 00:11:46,800
it's your choice if you want a pizza y

00:11:44,160 --> 00:11:48,560
you can deal with it so let's try to

00:11:46,800 --> 00:11:50,000
send those two orders hopefully they

00:11:48,560 --> 00:11:53,120
will come across

00:11:50,000 --> 00:11:54,000
let's run and we see that immediately we

00:11:53,120 --> 00:11:56,240
receive on

00:11:54,000 --> 00:11:58,240
them on the consumer side and we have

00:11:56,240 --> 00:12:00,240
the order both from mandela and from mac

00:11:58,240 --> 00:12:02,399
so the world pipeline is actually

00:12:00,240 --> 00:12:05,279
working now let's go back to

00:12:02,399 --> 00:12:05,279
some more slides

00:12:05,600 --> 00:12:12,240
so let's talk about the log size we said

00:12:09,120 --> 00:12:14,800
that the log is stored in a broker

00:12:12,240 --> 00:12:16,160
so this means that the wall log has to

00:12:14,800 --> 00:12:18,639
fit in a broker

00:12:16,160 --> 00:12:20,880
this means that i have to have enough

00:12:18,639 --> 00:12:23,760
disk space in the broker to fit

00:12:20,880 --> 00:12:25,200
the lock on the other side i need to

00:12:23,760 --> 00:12:26,959
reduce the amount of data they want to

00:12:25,200 --> 00:12:28,160
store in the log because as to fit on

00:12:26,959 --> 00:12:30,160
the disk space

00:12:28,160 --> 00:12:31,920
this is a bad trade-off for a platform

00:12:30,160 --> 00:12:32,480
that wants to manage a huge amount of

00:12:31,920 --> 00:12:34,639
logs

00:12:32,480 --> 00:12:36,079
a huge amount of topics a huge amount of

00:12:34,639 --> 00:12:38,320
events

00:12:36,079 --> 00:12:39,600
well luckily for us kafka doesn't

00:12:38,320 --> 00:12:43,120
enforce on us

00:12:39,600 --> 00:12:47,200
this trade-off kafka has knowledge of

00:12:43,120 --> 00:12:49,839
topic partitions which is a way to

00:12:47,200 --> 00:12:51,279
logically divide events of the same time

00:12:49,839 --> 00:12:54,000
belonging to the same topic

00:12:51,279 --> 00:12:54,720
into partitions so for example our pizza

00:12:54,000 --> 00:12:57,279
orders

00:12:54,720 --> 00:12:58,399
i could store all the pizza orders

00:12:57,279 --> 00:13:00,959
events

00:12:58,399 --> 00:13:02,959
of the same topic into separate

00:13:00,959 --> 00:13:04,079
partitions depending on the restaurant

00:13:02,959 --> 00:13:06,880
receiving the order

00:13:04,079 --> 00:13:08,000
for example mario's pizza on the blue

00:13:06,880 --> 00:13:09,760
one

00:13:08,000 --> 00:13:12,000
luigi spits on the yellow one francesco

00:13:09,760 --> 00:13:14,480
pizza on the red one

00:13:12,000 --> 00:13:15,600
what is really stored in the broker is

00:13:14,480 --> 00:13:18,480
the partitions

00:13:15,600 --> 00:13:20,079
not the wall lock so this means that the

00:13:18,480 --> 00:13:21,200
trade-off between disk size and

00:13:20,079 --> 00:13:24,720
partition

00:13:21,200 --> 00:13:27,519
is less worrying because if i have

00:13:24,720 --> 00:13:29,360
a huge topic with a lot of events and a

00:13:27,519 --> 00:13:32,320
smaller disk space i just need more

00:13:29,360 --> 00:13:34,000
partitions to fit in

00:13:32,320 --> 00:13:36,240
how do you select a partition usually

00:13:34,000 --> 00:13:38,560
that is done with the key

00:13:36,240 --> 00:13:39,920
kafka by default hashes the key and

00:13:38,560 --> 00:13:42,000
ensures that

00:13:39,920 --> 00:13:43,519
messages with the same key end up in the

00:13:42,000 --> 00:13:45,440
same partition

00:13:43,519 --> 00:13:47,360
while select why selecting a partition

00:13:45,440 --> 00:13:49,360
is important it's important because of

00:13:47,360 --> 00:13:51,760
ordering let me show you this little

00:13:49,360 --> 00:13:53,440
example where i have my producer

00:13:51,760 --> 00:13:56,240
producing data to a topic with two

00:13:53,440 --> 00:13:57,680
partitions and then i have my consumer

00:13:56,240 --> 00:13:59,360
in this example we have only three

00:13:57,680 --> 00:14:01,519
events blue

00:13:59,360 --> 00:14:03,360
yellow and red and let's assume that

00:14:01,519 --> 00:14:06,000
while pushing data to kafka

00:14:03,360 --> 00:14:07,440
the events will land the blue one will

00:14:06,000 --> 00:14:08,399
land in partition zero the yellow and

00:14:07,440 --> 00:14:10,720
partition one

00:14:08,399 --> 00:14:11,760
and the red in partition zero again now

00:14:10,720 --> 00:14:14,399
when reading

00:14:11,760 --> 00:14:16,399
from the topic it could happen that i

00:14:14,399 --> 00:14:20,079
will end up with the following sequence

00:14:16,399 --> 00:14:22,720
blue red and yellow why is this

00:14:20,079 --> 00:14:24,800
this is because once we start using

00:14:22,720 --> 00:14:25,839
partitioning we have to give up on the

00:14:24,800 --> 00:14:28,079
global ordering

00:14:25,839 --> 00:14:31,440
kafka ensures the correct ordering only

00:14:28,079 --> 00:14:34,560
per partition

00:14:31,440 --> 00:14:35,760
so when we start using partitioning we

00:14:34,560 --> 00:14:38,959
need to make sure

00:14:35,760 --> 00:14:40,639
that all the events for which we care

00:14:38,959 --> 00:14:42,839
about the related ordering

00:14:40,639 --> 00:14:44,160
that they will land in the same

00:14:42,839 --> 00:14:46,320
partition

00:14:44,160 --> 00:14:47,199
so we understood that partitioning is

00:14:46,320 --> 00:14:49,760
good because of

00:14:47,199 --> 00:14:51,360
the disk trade-off and is bad because of

00:14:49,760 --> 00:14:53,839
the global ordering

00:14:51,360 --> 00:14:55,600
but if we think about a topic with a

00:14:53,839 --> 00:14:58,639
single partition

00:14:55,600 --> 00:15:01,040
it's just a unique thread opening event

00:14:58,639 --> 00:15:03,920
appending events one after the other

00:15:01,040 --> 00:15:06,000
so you can guess that the throughput is

00:15:03,920 --> 00:15:08,480
also due to this unique event

00:15:06,000 --> 00:15:10,079
happening the unique thread happening

00:15:08,480 --> 00:15:13,360
events one of the other

00:15:10,079 --> 00:15:16,399
if we now have partitions those are now

00:15:13,360 --> 00:15:18,240
in this case three independent threads

00:15:16,399 --> 00:15:20,399
so you can start guessing that the

00:15:18,240 --> 00:15:24,639
throughput of those three independent

00:15:20,399 --> 00:15:26,480
writing events will be likely something

00:15:24,639 --> 00:15:27,920
around three times the original

00:15:26,480 --> 00:15:29,920
throughput so this means that we can

00:15:27,920 --> 00:15:32,399
have a lot more producers start pushing

00:15:29,920 --> 00:15:34,240
there to kafka

00:15:32,399 --> 00:15:35,839
and not only pushing that to kafka we

00:15:34,240 --> 00:15:38,160
can also have consumers

00:15:35,839 --> 00:15:39,040
consuming data from kafka but still we

00:15:38,160 --> 00:15:41,600
want to consume

00:15:39,040 --> 00:15:43,040
all the events that are in kafka but we

00:15:41,600 --> 00:15:45,839
don't want to consume

00:15:43,040 --> 00:15:46,800
the same event twice how can we do that

00:15:45,839 --> 00:15:49,920
how kafka

00:15:46,800 --> 00:15:51,680
enables us to do that well kafka will

00:15:49,920 --> 00:15:53,440
assign a non-overlapping set of

00:15:51,680 --> 00:15:55,600
partitions to consumer

00:15:53,440 --> 00:15:57,680
this is a difficult word let me show you

00:15:55,600 --> 00:15:59,519
in this example we have three partitions

00:15:57,680 --> 00:16:01,440
and two consumer kafka will

00:15:59,519 --> 00:16:03,440
for example assign the blue partition to

00:16:01,440 --> 00:16:05,759
the consumer one and the yellow and red

00:16:03,440 --> 00:16:08,399
partition to the consumer two

00:16:05,759 --> 00:16:09,040
in this case basically kafka ensures

00:16:08,399 --> 00:16:10,800
that

00:16:09,040 --> 00:16:12,720
all the messages have been read and none

00:16:10,800 --> 00:16:14,720
of the messages is read twice

00:16:12,720 --> 00:16:17,199
so let me show you a little partition

00:16:14,720 --> 00:16:18,639
example let's go back to our nice

00:16:17,199 --> 00:16:22,560
notebook

00:16:18,639 --> 00:16:25,600
so now what i will do i will create

00:16:22,560 --> 00:16:25,600
a new producer

00:16:26,160 --> 00:16:32,399
and let me show you i create a new

00:16:29,120 --> 00:16:34,160
producer like the one before

00:16:32,399 --> 00:16:36,320
and now i'm creating with kafka admin

00:16:34,160 --> 00:16:37,839
client am i touching to kafka

00:16:36,320 --> 00:16:39,759
from an admin point of view with the

00:16:37,839 --> 00:16:42,079
same parameters as before

00:16:39,759 --> 00:16:43,759
and i'm creating a new topic called

00:16:42,079 --> 00:16:46,800
francesco pizza partition

00:16:43,759 --> 00:16:49,279
with two partitions in this case so

00:16:46,800 --> 00:16:52,320
nothing really complex

00:16:49,279 --> 00:16:53,839
before now pushing data to kafka let me

00:16:52,320 --> 00:16:56,240
create

00:16:53,839 --> 00:16:58,480
two consumers so let me create a first

00:16:56,240 --> 00:17:01,680
consumer here

00:16:58,480 --> 00:17:03,759
and let me start the consumer

00:17:01,680 --> 00:17:05,039
and let me create another consumer at

00:17:03,759 --> 00:17:08,160
the bottom

00:17:05,039 --> 00:17:09,839
and let me start it let me start this

00:17:08,160 --> 00:17:12,880
consumer too

00:17:09,839 --> 00:17:16,079
okay i started both consumers

00:17:12,880 --> 00:17:18,240
let me double check yes both are started

00:17:16,079 --> 00:17:20,160
so now what happens i have a topic with

00:17:18,240 --> 00:17:20,880
chicken with two partitions and two

00:17:20,160 --> 00:17:23,280
consumers

00:17:20,880 --> 00:17:24,720
if everything i was telling you so far

00:17:23,280 --> 00:17:27,919
was true

00:17:24,720 --> 00:17:30,559
since i'm now sending two messages

00:17:27,919 --> 00:17:31,760
with slightly different keys ed0 in the

00:17:30,559 --> 00:17:34,000
d1

00:17:31,760 --> 00:17:35,679
i would guess that those two messages

00:17:34,000 --> 00:17:36,799
will end up into two different

00:17:35,679 --> 00:17:40,320
partitions

00:17:36,799 --> 00:17:41,360
so i should have my consumer each of my

00:17:40,320 --> 00:17:43,600
consumers reading

00:17:41,360 --> 00:17:44,559
only from one partition so reading only

00:17:43,600 --> 00:17:47,200
one record

00:17:44,559 --> 00:17:47,600
let's check this out so let's write

00:17:47,200 --> 00:17:52,240
those

00:17:47,600 --> 00:17:54,880
two orders and we can correctly see that

00:17:52,240 --> 00:17:56,320
only the top consumer is receiving only

00:17:54,880 --> 00:17:58,160
one and the

00:17:56,320 --> 00:18:00,080
bottom consumer is receiving only the

00:17:58,160 --> 00:18:02,640
other if you wonder what

00:18:00,080 --> 00:18:04,480
those little flags means this means that

00:18:02,640 --> 00:18:06,240
i'm reading from the partition zero the

00:18:04,480 --> 00:18:07,200
offset zero so the first record from

00:18:06,240 --> 00:18:09,520
partition zero

00:18:07,200 --> 00:18:10,880
and i'm reading from partition one the

00:18:09,520 --> 00:18:14,400
offset zero so

00:18:10,880 --> 00:18:15,120
the first record of partition one now if

00:18:14,400 --> 00:18:18,400
i send

00:18:15,120 --> 00:18:21,360
other two records this time

00:18:18,400 --> 00:18:21,760
reusing the same key so i would expect

00:18:21,360 --> 00:18:24,320
that

00:18:21,760 --> 00:18:25,919
mac order since it's reusing the same

00:18:24,320 --> 00:18:28,240
key id equal to zero

00:18:25,919 --> 00:18:30,000
will land in the same partition as frank

00:18:28,240 --> 00:18:32,320
and the same for yan

00:18:30,000 --> 00:18:35,280
landing in the same partition as adela

00:18:32,320 --> 00:18:35,280
let's try this out

00:18:35,520 --> 00:18:38,880
and it's exactly like that mac landing

00:18:38,559 --> 00:18:40,480
in

00:18:38,880 --> 00:18:42,320
the same partition as frank with

00:18:40,480 --> 00:18:44,320
partition zero offset one

00:18:42,320 --> 00:18:46,320
and yan landing in the same partition as

00:18:44,320 --> 00:18:48,799
the delay partition one offset one

00:18:46,320 --> 00:18:51,760
everything working as expected now let's

00:18:48,799 --> 00:18:51,760
go back to slides

00:18:52,080 --> 00:18:59,039
so as of now we saw a pretty linear

00:18:56,400 --> 00:19:00,559
way of having one or more threads of a

00:18:59,039 --> 00:19:02,960
producer

00:19:00,559 --> 00:19:04,160
kafka and one of more threads of a

00:19:02,960 --> 00:19:05,919
consumer

00:19:04,160 --> 00:19:07,600
this one of more threats of a consumer

00:19:05,919 --> 00:19:08,240
were working one against the other to

00:19:07,600 --> 00:19:10,480
consume

00:19:08,240 --> 00:19:12,080
all the messages from a kafka topic but

00:19:10,480 --> 00:19:14,000
still they didn't want to consume the

00:19:12,080 --> 00:19:17,120
same message twice

00:19:14,000 --> 00:19:17,760
however when we consume a message from

00:19:17,120 --> 00:19:20,640
kafka

00:19:17,760 --> 00:19:21,280
that is not deleted making it available

00:19:20,640 --> 00:19:24,480
from

00:19:21,280 --> 00:19:26,640
for other applications to read so for

00:19:24,480 --> 00:19:28,320
example i could have my pizza makers

00:19:26,640 --> 00:19:30,160
that are working one against the other

00:19:28,320 --> 00:19:30,720
to read all the orders and not to make

00:19:30,160 --> 00:19:32,000
them

00:19:30,720 --> 00:19:33,919
but still they don't want to make the

00:19:32,000 --> 00:19:35,280
same pizza twice

00:19:33,919 --> 00:19:37,679
on the other side i could have my

00:19:35,280 --> 00:19:40,000
billing person willing to receive a copy

00:19:37,679 --> 00:19:41,840
of the orders in order to make the bills

00:19:40,000 --> 00:19:44,640
how can i enable that well with kafka

00:19:41,840 --> 00:19:47,440
there is a concept of consumer groups

00:19:44,640 --> 00:19:48,559
so i can define my two pizza makers a

00:19:47,440 --> 00:19:50,480
part of the

00:19:48,559 --> 00:19:52,320
same application by saying they are part

00:19:50,480 --> 00:19:53,360
of the same consumer group and then i

00:19:52,320 --> 00:19:55,280
define my

00:19:53,360 --> 00:19:57,600
billing person as part of a new consumer

00:19:55,280 --> 00:19:58,559
group when saying that to kafka kafka

00:19:57,600 --> 00:20:00,880
will basically take

00:19:58,559 --> 00:20:02,400
and send another copy of the data to

00:20:00,880 --> 00:20:04,559
this new application

00:20:02,400 --> 00:20:06,240
and what this new application will read

00:20:04,559 --> 00:20:07,760
the data at this home page which has

00:20:06,240 --> 00:20:10,320
nothing to do with the

00:20:07,760 --> 00:20:12,559
pizza makers one let's check this out as

00:20:10,320 --> 00:20:16,480
well

00:20:12,559 --> 00:20:18,960
so if now we go back to the notebook

00:20:16,480 --> 00:20:21,440
and we go back to the original consumer

00:20:18,960 --> 00:20:25,840
and the original producer

00:20:21,440 --> 00:20:29,440
and we take now the new consumer group

00:20:25,840 --> 00:20:32,840
down here let's close everything

00:20:29,440 --> 00:20:34,159
okay so if we go at the top of the

00:20:32,840 --> 00:20:36,400
consumer

00:20:34,159 --> 00:20:37,600
we had our group id called pizza makers

00:20:36,400 --> 00:20:41,360
which should sound

00:20:37,600 --> 00:20:44,159
familiar to you as of now if we see

00:20:41,360 --> 00:20:45,120
the new consumer group it's called bill

00:20:44,159 --> 00:20:48,240
in person

00:20:45,120 --> 00:20:50,559
so this is the way that we tell kafka

00:20:48,240 --> 00:20:52,320
look that the bottom consumer is part of

00:20:50,559 --> 00:20:55,360
a new application that has nothing

00:20:52,320 --> 00:20:56,080
in common with the top one the other bit

00:20:55,360 --> 00:20:58,559
that we

00:20:56,080 --> 00:21:01,039
can see here is that if you remember we

00:20:58,559 --> 00:21:03,039
push three records francesco marquez in

00:21:01,039 --> 00:21:05,919
the daily

00:21:03,039 --> 00:21:07,120
but we only saw two of them because we

00:21:05,919 --> 00:21:11,039
started our consumer

00:21:07,120 --> 00:21:14,640
later than when we produced the events

00:21:11,039 --> 00:21:18,320
now when we create this new consumer

00:21:14,640 --> 00:21:20,320
we are also telling kafka look that

00:21:18,320 --> 00:21:22,240
i want to start reading from the log

00:21:20,320 --> 00:21:22,880
from the beginning with auto offset

00:21:22,240 --> 00:21:26,159
reset

00:21:22,880 --> 00:21:27,919
equal to earliest this will allow

00:21:26,159 --> 00:21:29,840
us to connect to kafka and to start

00:21:27,919 --> 00:21:31,440
reading the topic from the beginning

00:21:29,840 --> 00:21:33,440
let's try this out let's start our

00:21:31,440 --> 00:21:35,520
consumer

00:21:33,440 --> 00:21:37,840
and now we should receive not only the

00:21:35,520 --> 00:21:40,559
original order but also the other two

00:21:37,840 --> 00:21:41,200
there we are we receive not only adeles

00:21:40,559 --> 00:21:43,120
and mark

00:21:41,200 --> 00:21:45,200
but also francesco order from the

00:21:43,120 --> 00:21:48,960
beginning now if we

00:21:45,200 --> 00:21:50,880
also send a new record a new event

00:21:48,960 --> 00:21:53,120
done ordering pizza with price we should

00:21:50,880 --> 00:21:54,400
receive that in both applications since

00:21:53,120 --> 00:21:56,559
there are those two distinct

00:21:54,400 --> 00:21:58,799
applications let's try this out

00:21:56,559 --> 00:22:00,080
as we can see done is present both at

00:21:58,799 --> 00:22:02,720
the top and at the bottom

00:22:00,080 --> 00:22:06,240
so multiple consumer groups now let's

00:22:02,720 --> 00:22:06,240
finish off with some slides

00:22:07,760 --> 00:22:11,280
so far we saw a lot of things we saw how

00:22:09,600 --> 00:22:14,159
to produce data

00:22:11,280 --> 00:22:15,200
how to consume data how to use multiple

00:22:14,159 --> 00:22:18,240
partitions

00:22:15,200 --> 00:22:20,960
how to define multiple applications

00:22:18,240 --> 00:22:21,440
that consume data from kafka however we

00:22:20,960 --> 00:22:27,200
had

00:22:21,440 --> 00:22:29,120
always to write our own code

00:22:27,200 --> 00:22:31,200
to produce and to consume that the

00:22:29,120 --> 00:22:34,000
reality is that most of the times

00:22:31,200 --> 00:22:35,039
we will include kafka not in a green

00:22:34,000 --> 00:22:37,600
cell project but

00:22:35,039 --> 00:22:38,400
in a data project with a lot of data

00:22:37,600 --> 00:22:41,280
systems

00:22:38,400 --> 00:22:42,000
already available and we want to quickly

00:22:41,280 --> 00:22:45,039
integrate

00:22:42,000 --> 00:22:47,840
kafka in them possibly without having to

00:22:45,039 --> 00:22:50,880
reinvent the world writing our own code

00:22:47,840 --> 00:22:51,679
this is where kafka connect plays a huge

00:22:50,880 --> 00:22:53,919
role

00:22:51,679 --> 00:22:54,799
kafka connect is a pre-built framework

00:22:53,919 --> 00:22:57,360
that allows us

00:22:54,799 --> 00:22:59,200
an easy integration of kafka with

00:22:57,360 --> 00:23:00,880
existing ecosystem

00:22:59,200 --> 00:23:03,840
so for example i have my application

00:23:00,880 --> 00:23:03,840
writing data to

00:23:04,400 --> 00:23:08,640
postgres database to cassandra to google

00:23:07,360 --> 00:23:10,400
pub sub

00:23:08,640 --> 00:23:12,000
well with kafka connect and just a

00:23:10,400 --> 00:23:14,640
config file

00:23:12,000 --> 00:23:16,320
i can take any changes happening in one

00:23:14,640 --> 00:23:16,799
of those source system and propagate

00:23:16,320 --> 00:23:20,320
them

00:23:16,799 --> 00:23:23,679
as events as messages in a kafka topic

00:23:20,320 --> 00:23:26,880
i have data in a kafka topic and i

00:23:23,679 --> 00:23:29,919
my users are using bigquery

00:23:26,880 --> 00:23:32,000
elasticsearch postgres

00:23:29,919 --> 00:23:34,240
i can take the data from a kafka topic

00:23:32,000 --> 00:23:36,480
and with kafka connect

00:23:34,240 --> 00:23:38,720
push the data push several copies of the

00:23:36,480 --> 00:23:39,440
same of the topic data to a lot of data

00:23:38,720 --> 00:23:42,640
sets

00:23:39,440 --> 00:23:45,840
do i want another copy for long

00:23:42,640 --> 00:23:47,120
term storage in s3 it's just available

00:23:45,840 --> 00:23:49,039
with kafka connect

00:23:47,120 --> 00:23:50,159
the beauty of kafka connect is that it's

00:23:49,039 --> 00:23:52,000
just a thread

00:23:50,159 --> 00:23:54,000
which in case of hyphen is also managed

00:23:52,000 --> 00:23:56,480
for you and you just have to

00:23:54,000 --> 00:23:58,159
write a config file telling which is

00:23:56,480 --> 00:23:59,760
your source and which is your target

00:23:58,159 --> 00:24:01,840
for example if you want to sync data

00:23:59,760 --> 00:24:02,880
from kafka which is the topic that you

00:24:01,840 --> 00:24:05,520
want to sync from

00:24:02,880 --> 00:24:08,640
and which is your target for example the

00:24:05,520 --> 00:24:10,159
details of your bigquery

00:24:08,640 --> 00:24:12,159
some more resources that i believe you

00:24:10,159 --> 00:24:14,320
will find useful the first one is my

00:24:12,159 --> 00:24:16,320
twitter handle

00:24:14,320 --> 00:24:18,000
message me my messages are open for any

00:24:16,320 --> 00:24:20,080
question regarding kafka

00:24:18,000 --> 00:24:21,600
and python or any other open source

00:24:20,080 --> 00:24:25,120
technology that

00:24:21,600 --> 00:24:27,440
ivan provides second thing is the url to

00:24:25,120 --> 00:24:30,480
the notebooks that i've been showing you

00:24:27,440 --> 00:24:32,159
you can find them in getup public and

00:24:30,480 --> 00:24:32,799
you can do the same stuff that i've been

00:24:32,159 --> 00:24:35,200
doing

00:24:32,799 --> 00:24:37,120
even more the third thing is if you want

00:24:35,200 --> 00:24:38,320
to try kafka but you don't have any any

00:24:37,120 --> 00:24:40,960
data

00:24:38,320 --> 00:24:43,039
well check out this github url because

00:24:40,960 --> 00:24:45,520
it contains a python

00:24:43,039 --> 00:24:47,200
fake pizza provider so it starts

00:24:45,520 --> 00:24:48,799
creating pizza orders which are

00:24:47,200 --> 00:24:50,559
way more complex than the ones that i've

00:24:48,799 --> 00:24:53,279
been showing you in this demo

00:24:50,559 --> 00:24:54,960
the last bit is that if you want to try

00:24:53,279 --> 00:24:57,440
kafka but you don't have kafka

00:24:54,960 --> 00:24:59,039
well go to ivan.io because we offered

00:24:57,440 --> 00:25:00,480
that as a managed service and you can

00:24:59,039 --> 00:25:02,480
redeem the 300

00:25:00,480 --> 00:25:04,080
of free credit that lets you start your

00:25:02,480 --> 00:25:06,640
journey in kafka

00:25:04,080 --> 00:25:07,200
i hope you find this session useful and

00:25:06,640 --> 00:25:10,080
you will

00:25:07,200 --> 00:25:15,840
start your journey in kafka soon thanks

00:25:10,080 --> 00:25:15,840
a lot and ciao from francesco

00:26:12,240 --> 00:26:14,320

YouTube URL: https://www.youtube.com/watch?v=Ltgt0ekso4c


