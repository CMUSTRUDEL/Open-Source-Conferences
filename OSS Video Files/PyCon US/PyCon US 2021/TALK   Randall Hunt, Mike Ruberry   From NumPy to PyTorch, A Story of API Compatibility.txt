Title: TALK   Randall Hunt, Mike Ruberry   From NumPy to PyTorch, A Story of API Compatibility
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	NumPy has grown to be a vital part of the data science workflow for everyone from astrophysicists to zoologists. This talk is about how PyTorch approaches being “NumPy-compatible,” and why the PyTorch community thinks that’s important, why it can be challenging, and why sometimes it’s necessary to be divergent from NumPy’s behavior.

Slides: https://www.slideshare.net/MikeRuberry1/from-numpy-to-pytorch
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:13,759 --> 00:00:17,359
hey pycon 2021

00:00:15,599 --> 00:00:19,279
i'm mike rubery a technical lead on

00:00:17,359 --> 00:00:22,080
facebook's pytorch team

00:00:19,279 --> 00:00:24,560
and this talk from numpy to pi torch is

00:00:22,080 --> 00:00:26,080
all about how pytorch has translated and

00:00:24,560 --> 00:00:29,359
continues to translate

00:00:26,080 --> 00:00:30,560
operators from numpy here's the talk

00:00:29,359 --> 00:00:33,680
outline

00:00:30,560 --> 00:00:35,760
it's got five parts in the first part

00:00:33,680 --> 00:00:37,760
we'll have a couple minutes on numpy and

00:00:35,760 --> 00:00:40,079
operating on tensors

00:00:37,760 --> 00:00:40,960
then we'll talk about pi torch hardware

00:00:40,079 --> 00:00:44,000
accelerators

00:00:40,960 --> 00:00:46,399
autograd and computational graphs

00:00:44,000 --> 00:00:48,320
using that context we'll look at how

00:00:46,399 --> 00:00:51,120
numpy operators have been

00:00:48,320 --> 00:00:53,199
and continue to be added to pi torch

00:00:51,120 --> 00:00:53,440
then we'll talk about a few places where

00:00:53,199 --> 00:00:55,600
pi

00:00:53,440 --> 00:00:57,199
torch is different than numpy and why we

00:00:55,600 --> 00:00:58,879
think that's okay

00:00:57,199 --> 00:01:00,640
and we'll finish with lessons learned

00:00:58,879 --> 00:01:03,039
from adding these operators

00:01:00,640 --> 00:01:04,000
and plan future work so let's dive in

00:01:03,039 --> 00:01:06,240
with numpy

00:01:04,000 --> 00:01:08,799
and use that to very briefly introduce

00:01:06,240 --> 00:01:11,520
operations on multi-dimensional arrays

00:01:08,799 --> 00:01:14,000
which numpy simply calls arrays but

00:01:11,520 --> 00:01:15,759
since pytorch refers them as tensors

00:01:14,000 --> 00:01:18,000
i'll refer to them as tensors in this

00:01:15,759 --> 00:01:18,000
talk

00:01:19,600 --> 00:01:26,799
a simple sample numpy program shown here

00:01:23,119 --> 00:01:29,200
can help us get started

00:01:26,799 --> 00:01:31,040
this program starts by creating a matrix

00:01:29,200 --> 00:01:32,720
a two-dimensional tensor

00:01:31,040 --> 00:01:34,560
and tensors are interesting because they

00:01:32,720 --> 00:01:35,280
can represent a tremendous amount of

00:01:34,560 --> 00:01:37,840
data

00:01:35,280 --> 00:01:39,920
including images and text and they can

00:01:37,840 --> 00:01:41,200
be efficiently manipulated by modern

00:01:39,920 --> 00:01:43,119
hardware

00:01:41,200 --> 00:01:45,600
they're critical for modern scientific

00:01:43,119 --> 00:01:48,000
computing and the basis for modern deep

00:01:45,600 --> 00:01:48,000
learning

00:01:48,320 --> 00:01:52,479
in the middle snippet another matrix is

00:01:50,399 --> 00:01:55,840
created and the matrices are added

00:01:52,479 --> 00:01:57,520
element wise together

00:01:55,840 --> 00:02:01,600
and in the final snippet the two

00:01:57,520 --> 00:02:01,600
matrices are matrix multiplied

00:02:02,479 --> 00:02:06,000
of course numpy can do much more than

00:02:04,320 --> 00:02:07,680
what we just saw

00:02:06,000 --> 00:02:09,599
here for example are some more

00:02:07,680 --> 00:02:12,800
complicated domain specific

00:02:09,599 --> 00:02:14,800
snippets in the first snippet a fast

00:02:12,800 --> 00:02:16,720
fourier transform is performed

00:02:14,800 --> 00:02:19,200
and in the second snippet the scholeski

00:02:16,720 --> 00:02:20,959
decomposition of a complex matrix is

00:02:19,200 --> 00:02:23,040
computed

00:02:20,959 --> 00:02:26,319
obviously this talk won't be going over

00:02:23,040 --> 00:02:28,160
all of numpy's thousand plus functions

00:02:26,319 --> 00:02:29,920
but it supports a tremendous amount of

00:02:28,160 --> 00:02:32,879
functionality

00:02:29,920 --> 00:02:34,239
behind the scenes numpy operators are

00:02:32,879 --> 00:02:36,560
conceptually

00:02:34,239 --> 00:02:39,200
implemented either as composites which

00:02:36,560 --> 00:02:41,519
are composed of other numpy operations

00:02:39,200 --> 00:02:42,800
or primitives which have their own

00:02:41,519 --> 00:02:44,720
kernels

00:02:42,800 --> 00:02:46,560
a kernel is just a special name for a

00:02:44,720 --> 00:02:49,519
function that operates directly on a

00:02:46,560 --> 00:02:49,519
tensor's values

00:02:49,760 --> 00:02:53,280
the distinction between composites and

00:02:51,440 --> 00:02:56,160
primitives is important from an

00:02:53,280 --> 00:02:57,680
implementation perspective a composite

00:02:56,160 --> 00:03:00,800
numpy operation

00:02:57,680 --> 00:03:01,360
like the sync operation shown at left is

00:03:00,800 --> 00:03:04,400
typically

00:03:01,360 --> 00:03:06,159
implemented in numpy using python the

00:03:04,400 --> 00:03:07,040
mathematical definition of the sync

00:03:06,159 --> 00:03:09,440
operation

00:03:07,040 --> 00:03:11,120
appears at top and the python

00:03:09,440 --> 00:03:15,280
implementation in numpy

00:03:11,120 --> 00:03:18,400
at bottom primitive operations however

00:03:15,280 --> 00:03:20,319
are implemented in c plus plus here's

00:03:18,400 --> 00:03:21,040
part of the implementation of numpy's

00:03:20,319 --> 00:03:24,480
copy sign

00:03:21,040 --> 00:03:24,480
operator as an example

00:03:24,640 --> 00:03:28,720
when operations are implemented in c

00:03:26,480 --> 00:03:31,360
plus plus there's generated glue

00:03:28,720 --> 00:03:32,959
or binding code that connects the python

00:03:31,360 --> 00:03:36,080
function copy sign

00:03:32,959 --> 00:03:37,680
to its c plus implementation

00:03:36,080 --> 00:03:39,280
unfortunately there's some overhead

00:03:37,680 --> 00:03:41,040
associated with this binding

00:03:39,280 --> 00:03:43,360
and we'll briefly revisit that topic

00:03:41,040 --> 00:03:43,360
later

00:03:43,920 --> 00:03:48,080
now that we've had an extremely brief

00:03:45,760 --> 00:03:49,360
tour of numpy as a python package with

00:03:48,080 --> 00:03:51,360
tensor operations

00:03:49,360 --> 00:03:52,560
that's written in both python and c plus

00:03:51,360 --> 00:03:54,720
plus

00:03:52,560 --> 00:03:56,400
let's switch over to pytorch and expand

00:03:54,720 --> 00:03:57,280
our discussion to include hardware

00:03:56,400 --> 00:04:01,840
accelerators

00:03:57,280 --> 00:04:01,840
autograd and computational graphs

00:04:01,920 --> 00:04:07,439
like numpy pytorch is also a popular

00:04:04,879 --> 00:04:09,439
python package for operating on tensors

00:04:07,439 --> 00:04:11,519
and its user interface is extremely

00:04:09,439 --> 00:04:13,439
similar to numpy's

00:04:11,519 --> 00:04:17,120
let's see that by translating our simple

00:04:13,439 --> 00:04:17,120
numpy program to pi torch

00:04:17,199 --> 00:04:20,400
in the first snippet we just need to

00:04:19,120 --> 00:04:24,320
swap the word array

00:04:20,400 --> 00:04:26,479
for tensor in the second snippet

00:04:24,320 --> 00:04:28,960
nothing changes except we're now using

00:04:26,479 --> 00:04:30,400
the torch namespace

00:04:28,960 --> 00:04:32,880
and the same is true in the third

00:04:30,400 --> 00:04:32,880
snippet

00:04:32,960 --> 00:04:37,440
so here's our simple pytorch program a

00:04:35,520 --> 00:04:39,759
drop in replacement for the original

00:04:37,440 --> 00:04:41,680
numpy program

00:04:39,759 --> 00:04:43,440
that suggests that pi torch and numpy

00:04:41,680 --> 00:04:44,800
are basically equivalent for simple

00:04:43,440 --> 00:04:48,960
programs

00:04:44,800 --> 00:04:48,960
but what about more complicated examples

00:04:49,040 --> 00:04:53,120
well here's our complicated numpy

00:04:50,720 --> 00:04:54,840
snippets

00:04:53,120 --> 00:04:57,120
and here's the same snippets and pie

00:04:54,840 --> 00:04:59,680
torch we can actually see that

00:04:57,120 --> 00:05:01,199
printing prints a few less digits in

00:04:59,680 --> 00:05:03,680
pytorch by default

00:05:01,199 --> 00:05:07,680
but that's actually customizable

00:05:03,680 --> 00:05:07,680
otherwise we have the same operations

00:05:08,960 --> 00:05:13,280
pytorch doesn't just implement many of

00:05:10,960 --> 00:05:15,199
numpy's operators either

00:05:13,280 --> 00:05:17,520
pi torch and numpy tensors can actually

00:05:15,199 --> 00:05:20,400
be converted between the two frameworks

00:05:17,520 --> 00:05:21,919
as shown here where a pi torch sensor is

00:05:20,400 --> 00:05:24,240
passed to numpy

00:05:21,919 --> 00:05:28,560
added with a numpy tensor and then the

00:05:24,240 --> 00:05:28,560
result is translated back to pi torch

00:05:28,720 --> 00:05:33,919
our discussion so far might suggest that

00:05:30,880 --> 00:05:37,199
pytorch implements every numpy operator

00:05:33,919 --> 00:05:38,720
but that's not the case numpy is more

00:05:37,199 --> 00:05:41,120
than a thousand operators

00:05:38,720 --> 00:05:42,800
but many of them are rarely used have

00:05:41,120 --> 00:05:44,720
only niche application

00:05:42,800 --> 00:05:46,320
are deprecated or are in need of

00:05:44,720 --> 00:05:48,320
deprecation

00:05:46,320 --> 00:05:50,479
in fact numpy's maintainers are probably

00:05:48,320 --> 00:05:50,800
the first people to tell you that numpy

00:05:50,479 --> 00:05:54,080
has

00:05:50,800 --> 00:05:55,199
too many operations so pytorch doesn't

00:05:54,080 --> 00:05:57,199
implement them all

00:05:55,199 --> 00:05:59,440
and i don't know of any framework other

00:05:57,199 --> 00:06:01,680
than numpy which does

00:05:59,440 --> 00:06:02,720
the pytorch does have hundreds of numpy

00:06:01,680 --> 00:06:04,720
operators

00:06:02,720 --> 00:06:06,560
and we've focused on implementing those

00:06:04,720 --> 00:06:08,319
our community cares about

00:06:06,560 --> 00:06:11,360
i'll elaborate more on the community

00:06:08,319 --> 00:06:11,360
aspect of this later

00:06:11,840 --> 00:06:15,360
now let's start to talk about how pie

00:06:13,280 --> 00:06:17,280
torch is different from numpy

00:06:15,360 --> 00:06:18,800
because it's being identical to numpy

00:06:17,280 --> 00:06:20,560
would be pointless

00:06:18,800 --> 00:06:22,319
we already have numpy to be identical to

00:06:20,560 --> 00:06:24,479
numpy

00:06:22,319 --> 00:06:26,400
so pytorch has several features like

00:06:24,479 --> 00:06:28,639
hardware accelerator support

00:06:26,400 --> 00:06:30,479
that numpy doesn't have here's our

00:06:28,639 --> 00:06:34,080
simple snippets with the tensors and

00:06:30,479 --> 00:06:36,080
computations performed on a cuda device

00:06:34,080 --> 00:06:38,400
close observers may have noticed one

00:06:36,080 --> 00:06:40,560
small hitch with this program

00:06:38,400 --> 00:06:42,560
in line five the tensors are now

00:06:40,560 --> 00:06:45,440
converted to the float d-type

00:06:42,560 --> 00:06:47,199
before the matrix multiplication we

00:06:45,440 --> 00:06:48,639
haven't talked much about tensor data

00:06:47,199 --> 00:06:50,800
types so far

00:06:48,639 --> 00:06:51,759
but operations on different devices

00:06:50,800 --> 00:06:54,720
sometimes support

00:06:51,759 --> 00:06:56,400
different data types in this case the

00:06:54,720 --> 00:06:58,800
cuda matrix multiplication

00:06:56,400 --> 00:07:00,960
uses a math library and that math

00:06:58,800 --> 00:07:03,440
library doesn't accept integer tensors

00:07:00,960 --> 00:07:05,919
as inputs

00:07:03,440 --> 00:07:07,840
importantly though even though data type

00:07:05,919 --> 00:07:09,280
support might vary slightly between

00:07:07,840 --> 00:07:11,680
device types

00:07:09,280 --> 00:07:12,479
the semantics of pytorch programs remain

00:07:11,680 --> 00:07:14,800
the same

00:07:12,479 --> 00:07:15,680
whether they're run on a cpu device a

00:07:14,800 --> 00:07:20,720
cuda device

00:07:15,680 --> 00:07:20,720
a tpu or another hardware accelerator

00:07:22,080 --> 00:07:26,160
in addition to support for hardware

00:07:23,599 --> 00:07:29,199
accelerators pytorch also has

00:07:26,160 --> 00:07:29,759
built-in autograd support this short

00:07:29,199 --> 00:07:31,680
snippet

00:07:29,759 --> 00:07:33,840
shows the computation of a gradient for

00:07:31,680 --> 00:07:36,000
a tensor after it's been pointwise

00:07:33,840 --> 00:07:38,319
multiplied with another

00:07:36,000 --> 00:07:40,319
although not the subject of this talk

00:07:38,319 --> 00:07:44,319
autograd is especially useful

00:07:40,319 --> 00:07:46,560
when training neural networks

00:07:44,319 --> 00:07:49,919
pytorch also supports the construction

00:07:46,560 --> 00:07:52,080
and optimization of computational graphs

00:07:49,919 --> 00:07:53,759
if we go back to the sync function we

00:07:52,080 --> 00:07:56,319
can pretend for a moment that pi

00:07:53,759 --> 00:07:59,199
torch didn't have its own implementation

00:07:56,319 --> 00:08:00,960
and we were writing one in python

00:07:59,199 --> 00:08:02,960
we could tell pytorch's torscript

00:08:00,960 --> 00:08:04,800
compiler to read the python

00:08:02,960 --> 00:08:07,840
implementation of the function

00:08:04,800 --> 00:08:09,919
and create a computational graph for it

00:08:07,840 --> 00:08:11,680
the computational graph on this slide

00:08:09,919 --> 00:08:14,560
actually hints at how

00:08:11,680 --> 00:08:16,240
operators and pi torch are implemented

00:08:14,560 --> 00:08:18,960
but the important takeaway for us

00:08:16,240 --> 00:08:19,599
now is that these graphs allow for cross

00:08:18,960 --> 00:08:22,000
operator

00:08:19,599 --> 00:08:23,440
optimizations and they can reduce the

00:08:22,000 --> 00:08:26,800
overhead of moving between

00:08:23,440 --> 00:08:29,199
python and c plus plus by invoking the

00:08:26,800 --> 00:08:30,720
series of operations from python one

00:08:29,199 --> 00:08:32,800
time

00:08:30,720 --> 00:08:33,919
in addition to torch script as shown

00:08:32,800 --> 00:08:35,919
here

00:08:33,919 --> 00:08:38,320
pytorch also supports creating and

00:08:35,919 --> 00:08:40,159
optimizing computational graphs with its

00:08:38,320 --> 00:08:42,959
torch.fx package

00:08:40,159 --> 00:08:45,440
and the xla deep learning compiler which

00:08:42,959 --> 00:08:48,000
is also used by the tensorflow and jaxx

00:08:45,440 --> 00:08:48,000
frameworks

00:08:49,200 --> 00:08:53,360
support for hardware accelerators

00:08:50,880 --> 00:08:55,680
autograd and computational graphs

00:08:53,360 --> 00:08:57,680
make pytorch great for deep learning and

00:08:55,680 --> 00:08:59,680
pytorch has its own state-of-the-art

00:08:57,680 --> 00:09:01,760
neural network library

00:08:59,680 --> 00:09:03,920
in this very brief snippet we just see

00:09:01,760 --> 00:09:05,040
the construction and use of a single

00:09:03,920 --> 00:09:06,800
linear layer

00:09:05,040 --> 00:09:09,839
which is a common component of many

00:09:06,800 --> 00:09:09,839
neural networks

00:09:10,800 --> 00:09:14,480
so while pytorch doesn't have every

00:09:12,480 --> 00:09:16,480
numpy operator

00:09:14,480 --> 00:09:17,600
those it does implement we can think of

00:09:16,480 --> 00:09:19,519
as numpy

00:09:17,600 --> 00:09:22,640
plus additional features like

00:09:19,519 --> 00:09:24,959
accelerator support and autograd

00:09:22,640 --> 00:09:26,800
and to be clear pytorch also has

00:09:24,959 --> 00:09:29,120
additional operators that numpy

00:09:26,800 --> 00:09:30,560
doesn't particularly operators from

00:09:29,120 --> 00:09:32,399
scipy

00:09:30,560 --> 00:09:33,920
so that's a bit on what pytorch looks

00:09:32,399 --> 00:09:36,240
like to users

00:09:33,920 --> 00:09:37,360
but behind the scenes pi torch operators

00:09:36,240 --> 00:09:40,640
are almost always

00:09:37,360 --> 00:09:43,680
implemented in c plus plus or a mix of c

00:09:40,640 --> 00:09:44,320
plus plus and device specific code this

00:09:43,680 --> 00:09:47,920
would include

00:09:44,320 --> 00:09:50,160
cpu intrinsics and cuda for example

00:09:47,920 --> 00:09:51,920
it also supports executing operators as

00:09:50,160 --> 00:09:54,160
computational graphs

00:09:51,920 --> 00:09:57,360
and differentiable operators get

00:09:54,160 --> 00:09:59,440
autograd formulas

00:09:57,360 --> 00:10:02,480
let's look at pytorch's implementation

00:09:59,440 --> 00:10:04,800
of sync to make this more concrete

00:10:02,480 --> 00:10:06,240
as a reminder here's the definition of

00:10:04,800 --> 00:10:10,000
the sync operation

00:10:06,240 --> 00:10:10,000
and its implementation in numpy

00:10:10,079 --> 00:10:15,279
now here's the implementation of that

00:10:11,920 --> 00:10:17,200
operator in pytorch for the cpu

00:10:15,279 --> 00:10:19,600
there's a lot of supporting architecture

00:10:17,200 --> 00:10:21,920
that we don't have time to review here

00:10:19,600 --> 00:10:26,079
but lines 6 through 10 are the heart of

00:10:21,920 --> 00:10:26,079
the kernel which computes this operation

00:10:29,120 --> 00:10:32,800
since the cuda kernel looks very similar

00:10:31,120 --> 00:10:34,399
to the cpu kernel

00:10:32,800 --> 00:10:36,480
let's skip ahead to the auto grad

00:10:34,399 --> 00:10:36,959
formula for sync which is actually

00:10:36,480 --> 00:10:40,079
written

00:10:36,959 --> 00:10:42,000
in pythonic yaml more complicated

00:10:40,079 --> 00:10:42,959
autograd formulas can be directly

00:10:42,000 --> 00:10:47,760
implemented in c

00:10:42,959 --> 00:10:49,360
plus plus

00:10:47,760 --> 00:10:51,760
with that background on numpy and

00:10:49,360 --> 00:10:54,160
pytorch let's look at what's involved in

00:10:51,760 --> 00:10:56,000
adding a numpy operator to pi torch

00:10:54,160 --> 00:11:00,320
and why we think it's important to add

00:10:56,000 --> 00:11:02,959
numpy operators

00:11:00,320 --> 00:11:05,360
to port a numpy operator to pytorch we

00:11:02,959 --> 00:11:07,120
typically need three things

00:11:05,360 --> 00:11:09,600
we need to write a c-plus plus

00:11:07,120 --> 00:11:10,959
implementation and if the operation is a

00:11:09,600 --> 00:11:14,079
primitive op

00:11:10,959 --> 00:11:16,240
a cpu and a cuda kernel

00:11:14,079 --> 00:11:18,560
we need to write an autograd formula if

00:11:16,240 --> 00:11:20,560
the op is differentiable

00:11:18,560 --> 00:11:22,000
and we need to write comprehensive tests

00:11:20,560 --> 00:11:25,360
that validate the operator

00:11:22,000 --> 00:11:27,839
works correctly since that's no

00:11:25,360 --> 00:11:29,760
small amount of work it's reasonable to

00:11:27,839 --> 00:11:31,680
ask why we bother at all

00:11:29,760 --> 00:11:35,440
why does pytorch think implementing

00:11:31,680 --> 00:11:35,440
numpy operators is important

00:11:36,079 --> 00:11:39,680
one way we know it's important to

00:11:37,760 --> 00:11:42,720
implement numpy operators

00:11:39,680 --> 00:11:44,320
is that our community often asks us to

00:11:42,720 --> 00:11:46,480
on this slide i've included some

00:11:44,320 --> 00:11:49,200
requests for numpy and scipy

00:11:46,480 --> 00:11:49,200
functionality

00:11:49,760 --> 00:11:53,279
even better than asking for numpy

00:11:51,680 --> 00:11:55,200
operators however

00:11:53,279 --> 00:11:57,279
the pytorch community has helped

00:11:55,200 --> 00:11:59,200
implement dozens of them

00:11:57,279 --> 00:12:00,800
here is a portion of one of our github

00:11:59,200 --> 00:12:02,560
tracking issues

00:12:00,800 --> 00:12:05,519
most of the github names here are

00:12:02,560 --> 00:12:05,519
community members

00:12:06,720 --> 00:12:10,880
one surprise when we engaged the

00:12:08,560 --> 00:12:11,200
community was not just how eager they

00:12:10,880 --> 00:12:13,839
were

00:12:11,200 --> 00:12:15,839
for more numpy functionality but also

00:12:13,839 --> 00:12:17,920
how frustrated they were with the few

00:12:15,839 --> 00:12:20,160
cases where pie torch's behavior was

00:12:17,920 --> 00:12:22,000
divergent from numpy's

00:12:20,160 --> 00:12:24,720
here's a comment from one community

00:12:22,000 --> 00:12:27,200
member on the same issue we just saw

00:12:24,720 --> 00:12:28,000
they were so irked by pytorch diverging

00:12:27,200 --> 00:12:30,320
from numpy

00:12:28,000 --> 00:12:32,720
in a few places that they were thinking

00:12:30,320 --> 00:12:38,240
of switching to a different framework

00:12:32,720 --> 00:12:39,760
i'll come back to this point again later

00:12:38,240 --> 00:12:41,839
so our community has made it really

00:12:39,760 --> 00:12:44,880
clear that faithful implementations of

00:12:41,839 --> 00:12:47,600
numpy operators are important

00:12:44,880 --> 00:12:51,600
given that how can we best facilitate

00:12:47,600 --> 00:12:53,200
porting operators from numpy to pytorch

00:12:51,600 --> 00:12:55,200
we already briefly saw that there was

00:12:53,200 --> 00:12:56,079
some supporting architecture for writing

00:12:55,200 --> 00:12:58,959
a kernel in pi

00:12:56,079 --> 00:13:02,320
torch and we saw that auto grad formulas

00:12:58,959 --> 00:13:04,240
can simply be expressed as pythonic yaml

00:13:02,320 --> 00:13:06,720
but that still leaves us with tests to

00:13:04,240 --> 00:13:09,440
write and pie torch has a huge

00:13:06,720 --> 00:13:10,480
test matrix if every test had to be

00:13:09,440 --> 00:13:12,079
written by hand

00:13:10,480 --> 00:13:15,440
then it would be harder to write our

00:13:12,079 --> 00:13:17,440
tests than our operators

00:13:15,440 --> 00:13:19,760
to simplify testing we've developed our

00:13:17,440 --> 00:13:22,959
own test generation framework that works

00:13:19,760 --> 00:13:24,800
with both pi test and unit test

00:13:22,959 --> 00:13:27,600
and since i don't have the time today to

00:13:24,800 --> 00:13:29,279
elaborate on each of these three systems

00:13:27,600 --> 00:13:31,040
i'll just describe more of our test

00:13:29,279 --> 00:13:32,880
framework since it's implemented in

00:13:31,040 --> 00:13:34,959
python

00:13:32,880 --> 00:13:36,959
so a little more detail on pytorch's

00:13:34,959 --> 00:13:39,040
test matrix

00:13:36,959 --> 00:13:40,880
when testing pi torch operators we need

00:13:39,040 --> 00:13:43,519
to account for different tensors

00:13:40,880 --> 00:13:44,160
and a variety of pi torch systems and

00:13:43,519 --> 00:13:47,839
doing so

00:13:44,160 --> 00:13:50,079
is challenging for two reasons

00:13:47,839 --> 00:13:52,480
hand writing all of these test variants

00:13:50,079 --> 00:13:54,399
would be hundreds of lines of code

00:13:52,480 --> 00:13:56,399
and two developers would need to be

00:13:54,399 --> 00:13:59,920
familiar with these different systems

00:13:56,399 --> 00:14:01,760
and device types requiring developers

00:13:59,920 --> 00:14:03,680
write all that code and that they be

00:14:01,760 --> 00:14:06,560
familiar with so many systems

00:14:03,680 --> 00:14:08,800
is not a real option so that means we

00:14:06,560 --> 00:14:10,480
can either give up on test coverage

00:14:08,800 --> 00:14:12,639
or we can start generating tests

00:14:10,480 --> 00:14:16,000
automatically and pytorch shows the

00:14:12,639 --> 00:14:18,079
latter approach

00:14:16,000 --> 00:14:19,920
heart of pi torch's new test framework

00:14:18,079 --> 00:14:21,920
are op infos

00:14:19,920 --> 00:14:23,040
these are python classes that describe

00:14:21,920 --> 00:14:25,760
an operator

00:14:23,040 --> 00:14:26,720
its test directives and include a sample

00:14:25,760 --> 00:14:28,839
inputs function

00:14:26,720 --> 00:14:30,639
that can return valid inputs to the

00:14:28,839 --> 00:14:33,680
operation

00:14:30,639 --> 00:14:35,120
here is the op info for mul or multiply

00:14:33,680 --> 00:14:37,360
and numpy

00:14:35,120 --> 00:14:41,279
in fact we can see that numpy name is

00:14:37,360 --> 00:14:43,519
alias to the original pi torch operation

00:14:41,279 --> 00:14:44,959
this app info describes which data types

00:14:43,519 --> 00:14:48,160
the operation supports

00:14:44,959 --> 00:14:49,680
on lines 3 and then points to its sample

00:14:48,160 --> 00:14:52,880
inputs function on line

00:14:49,680 --> 00:14:54,079
4. here the sample inputs for mul are

00:14:52,880 --> 00:14:57,440
the same as for other

00:14:54,079 --> 00:14:59,680
binary element-wise operations that is

00:14:57,440 --> 00:15:02,959
functions that operate on two tensors in

00:14:59,680 --> 00:15:02,959
an element-wise fashion

00:15:06,399 --> 00:15:10,880
in addition to the baseop info class we

00:15:08,880 --> 00:15:12,240
also have subclasses with additional

00:15:10,880 --> 00:15:15,040
metadata

00:15:12,240 --> 00:15:17,440
here's an op info for the sign operator

00:15:15,040 --> 00:15:19,519
which is actually a unary ufunc

00:15:17,440 --> 00:15:23,120
that is a function that accepts a single

00:15:19,519 --> 00:15:26,399
tensor and operates on it element wise

00:15:23,120 --> 00:15:28,880
instead of being the op info base class

00:15:26,399 --> 00:15:31,360
it's actually the unary ufunc info

00:15:28,880 --> 00:15:33,600
subclass

00:15:31,360 --> 00:15:35,279
note the reference numpy function on

00:15:33,600 --> 00:15:37,839
line two

00:15:35,279 --> 00:15:40,480
that reference pointer and the structure

00:15:37,839 --> 00:15:42,720
of unary element wise operations

00:15:40,480 --> 00:15:45,759
actually lets us automatically generate

00:15:42,720 --> 00:15:47,759
every test we need for torch.sign

00:15:45,759 --> 00:15:51,440
and we can validate that it works just

00:15:47,759 --> 00:15:51,440
like it's numpy reference to

00:15:52,880 --> 00:15:56,639
op infos alone don't generate tests of

00:15:55,120 --> 00:15:58,720
course because they're just simple

00:15:56,639 --> 00:16:01,199
python classes

00:15:58,720 --> 00:16:03,199
so for that we use the ops decorator

00:16:01,199 --> 00:16:05,040
which takes a sequence of operations to

00:16:03,199 --> 00:16:06,639
review when instantiating a test

00:16:05,040 --> 00:16:08,639
template

00:16:06,639 --> 00:16:11,120
here's one test template that supports

00:16:08,639 --> 00:16:13,839
unary u funcs like sine

00:16:11,120 --> 00:16:15,440
it accepts the device data type and the

00:16:13,839 --> 00:16:17,279
app being tested

00:16:15,440 --> 00:16:19,199
and then it validates that the result of

00:16:17,279 --> 00:16:22,000
the operation is the same whether its

00:16:19,199 --> 00:16:24,800
inputs are contiguous or not

00:16:22,000 --> 00:16:26,480
the actual test isn't so important here

00:16:24,800 --> 00:16:26,959
but what is important is to understand

00:16:26,480 --> 00:16:29,440
that this

00:16:26,959 --> 00:16:30,240
test is designed to run on every unary

00:16:29,440 --> 00:16:32,959
u-func

00:16:30,240 --> 00:16:32,959
not just sign

00:16:33,920 --> 00:16:38,079
before the tests are actually run by pi

00:16:36,320 --> 00:16:39,759
test or unit test

00:16:38,079 --> 00:16:41,519
these test templates need to be

00:16:39,759 --> 00:16:43,600
instantiated

00:16:41,519 --> 00:16:45,040
the instantiated tests for this template

00:16:43,600 --> 00:16:47,920
and the sign operation

00:16:45,040 --> 00:16:49,440
are shown here the test names include

00:16:47,920 --> 00:16:52,160
the name of the operation

00:16:49,440 --> 00:16:55,199
the device type and the data type that

00:16:52,160 --> 00:16:55,199
are passed to the test

00:16:57,279 --> 00:17:01,759
some of pytorch's automated op info

00:16:59,519 --> 00:17:02,639
tests validate the operator's auto grad

00:17:01,759 --> 00:17:05,679
formula

00:17:02,639 --> 00:17:06,880
support for torscript and torch.fx and

00:17:05,679 --> 00:17:09,199
the consistency

00:17:06,880 --> 00:17:10,959
of its function method and in place

00:17:09,199 --> 00:17:13,120
variance

00:17:10,959 --> 00:17:14,079
the one thing the basic opinfo class

00:17:13,120 --> 00:17:15,839
can't do

00:17:14,079 --> 00:17:18,000
is test that the operator actually

00:17:15,839 --> 00:17:20,880
computes what it should

00:17:18,000 --> 00:17:22,959
although as i mentioned in special cases

00:17:20,880 --> 00:17:24,640
like the unary ufuncs

00:17:22,959 --> 00:17:26,400
we've written tests that exploit the

00:17:24,640 --> 00:17:26,959
operator structure and a reference

00:17:26,400 --> 00:17:29,919
function

00:17:26,959 --> 00:17:31,760
to validate it in most cases we don't

00:17:29,919 --> 00:17:34,400
understand the operator enough to know

00:17:31,760 --> 00:17:34,400
what it should do

00:17:37,360 --> 00:17:43,440
some features of our test generator are

00:17:40,400 --> 00:17:45,520
it works with both pi test and unit test

00:17:43,440 --> 00:17:48,080
it dynamically identifies available

00:17:45,520 --> 00:17:49,760
device types when running the test suite

00:17:48,080 --> 00:17:52,000
so if you have a machine that doesn't

00:17:49,760 --> 00:17:54,480
have a cuda device for example

00:17:52,000 --> 00:17:56,559
cuda variants of test templates won't be

00:17:54,480 --> 00:17:59,280
instantiated

00:17:56,559 --> 00:18:00,320
it allows for device type setup and tear

00:17:59,280 --> 00:18:01,840
down

00:18:00,320 --> 00:18:03,919
so we can do different things when

00:18:01,840 --> 00:18:05,440
setting up a cuda device like checking

00:18:03,919 --> 00:18:09,039
for memory leaks

00:18:05,440 --> 00:18:10,799
than we do with the cpu it's extensible

00:18:09,039 --> 00:18:13,760
by other packages

00:18:10,799 --> 00:18:15,919
the pytorch xla package for example adds

00:18:13,760 --> 00:18:18,320
xla devices to the test framework

00:18:15,919 --> 00:18:21,120
programmatically

00:18:18,320 --> 00:18:22,240
the op info repository acts as a single

00:18:21,120 --> 00:18:25,600
source of truth

00:18:22,240 --> 00:18:27,679
for all our operators functionality

00:18:25,600 --> 00:18:30,080
and it's easy to write new tests that

00:18:27,679 --> 00:18:32,480
run on every pi torch operator

00:18:30,080 --> 00:18:34,640
or on a more structured subset of those

00:18:32,480 --> 00:18:37,360
operators

00:18:34,640 --> 00:18:40,400
systematic testing of new features like

00:18:37,360 --> 00:18:42,400
pytorch's support for complex tensors

00:18:40,400 --> 00:18:44,400
has been made tremendously easier with

00:18:42,400 --> 00:18:46,320
our op info pattern

00:18:44,400 --> 00:18:49,120
new data types can just be added to the

00:18:46,320 --> 00:18:52,880
list of data types and operator supports

00:18:49,120 --> 00:18:52,880
and they'll automatically be tested

00:18:53,280 --> 00:18:57,840
so far we've talked about how pi torches

00:18:55,200 --> 00:18:59,840
like numpy but with extra stuff

00:18:57,840 --> 00:19:01,760
but now let's look at a few cases where

00:18:59,840 --> 00:19:03,039
pytorch has chosen to be different than

00:19:01,760 --> 00:19:05,120
numpy

00:19:03,039 --> 00:19:06,240
these discrepancies are rare and most of

00:19:05,120 --> 00:19:08,480
them are small

00:19:06,240 --> 00:19:11,600
but there are a few places where pytorch

00:19:08,480 --> 00:19:13,360
is loudly divergent

00:19:11,600 --> 00:19:16,000
so here's a simple difference to get us

00:19:13,360 --> 00:19:18,080
started this is a change that we wanted

00:19:16,000 --> 00:19:19,919
to make for consistency with the rest of

00:19:18,080 --> 00:19:21,840
pytorch's ux

00:19:19,919 --> 00:19:24,320
and it's with the return type of the

00:19:21,840 --> 00:19:26,720
reciprocal function

00:19:24,320 --> 00:19:28,559
reciprocal is an element-wise operation

00:19:26,720 --> 00:19:30,559
that computes the reciprocal of each

00:19:28,559 --> 00:19:34,080
value in a tensor

00:19:30,559 --> 00:19:34,400
in numpy as seen at left the reciprocal

00:19:34,080 --> 00:19:36,559
of

00:19:34,400 --> 00:19:39,200
integers is computed using integer

00:19:36,559 --> 00:19:41,360
division or truncation division

00:19:39,200 --> 00:19:43,520
so the result is zero for every number

00:19:41,360 --> 00:19:44,880
other than one

00:19:43,520 --> 00:19:47,280
we thought that behavior was

00:19:44,880 --> 00:19:50,720
inconsistent with division in python

00:19:47,280 --> 00:19:53,440
pi torch and actually numpy itself

00:19:50,720 --> 00:19:55,360
because in those systems 1 divided by 2

00:19:53,440 --> 00:19:57,440
is typically a half

00:19:55,360 --> 00:19:58,720
so we change the behavior in pi torch

00:19:57,440 --> 00:20:02,000
for consistency

00:19:58,720 --> 00:20:02,000
and you can see that at right

00:20:02,640 --> 00:20:06,400
another example of a small difference

00:20:04,720 --> 00:20:08,640
between numpy and pi torch

00:20:06,400 --> 00:20:11,840
is the ig function which computes the

00:20:08,640 --> 00:20:13,600
eigenvalues and eigenvectors of a matrix

00:20:11,840 --> 00:20:16,080
at left we can see that numpy has

00:20:13,600 --> 00:20:18,320
returned two real valued tensors

00:20:16,080 --> 00:20:20,159
well at right pi torch returned two

00:20:18,320 --> 00:20:23,520
complex tensors for the same

00:20:20,159 --> 00:20:26,000
input it turns out that the eigenvalues

00:20:23,520 --> 00:20:28,960
and eigenvectors of a real valued matrix

00:20:26,000 --> 00:20:31,520
can be either real valued or complex

00:20:28,960 --> 00:20:33,440
and numpy returns them as complex only

00:20:31,520 --> 00:20:35,440
when necessary

00:20:33,440 --> 00:20:37,840
pie torch on the other hand always

00:20:35,440 --> 00:20:39,760
returns complex tensors

00:20:37,840 --> 00:20:41,840
this change was actually motivated by

00:20:39,760 --> 00:20:42,799
pytorch's support for computational

00:20:41,840 --> 00:20:44,960
graphs

00:20:42,799 --> 00:20:47,200
who want to infer the shape and data

00:20:44,960 --> 00:20:49,200
type of an operation's outputs

00:20:47,200 --> 00:20:51,520
from the shapes and data types of an

00:20:49,200 --> 00:20:53,360
operations inputs

00:20:51,520 --> 00:20:54,960
the numpy version of this operation

00:20:53,360 --> 00:20:57,600
wouldn't allow this

00:20:54,960 --> 00:20:58,559
because two matrices with the same shape

00:20:57,600 --> 00:21:00,480
and data type

00:20:58,559 --> 00:21:04,640
can end up producing either floating

00:21:00,480 --> 00:21:04,640
point tensors or complex tensors

00:21:05,919 --> 00:21:09,520
now let's look at a larger discrepancy

00:21:08,720 --> 00:21:11,919
in numpy

00:21:09,520 --> 00:21:13,200
complex tensors have maximums and can be

00:21:11,919 --> 00:21:14,960
sorted

00:21:13,200 --> 00:21:16,880
but in pine torch we throw a runtime

00:21:14,960 --> 00:21:19,360
error when a user attempts either of

00:21:16,880 --> 00:21:21,200
these operations

00:21:19,360 --> 00:21:23,760
this is because complex numbers don't

00:21:21,200 --> 00:21:25,760
belong to any totally ordered field

00:21:23,760 --> 00:21:29,760
so there's no mathematically intuitive

00:21:25,760 --> 00:21:32,640
way to say one is greater than another

00:21:29,760 --> 00:21:34,080
we didn't think sorting like numpy does

00:21:32,640 --> 00:21:36,320
based on the real value

00:21:34,080 --> 00:21:38,480
and then the complex value would be

00:21:36,320 --> 00:21:40,000
intuitive for users who didn't come from

00:21:38,480 --> 00:21:41,760
numpy

00:21:40,000 --> 00:21:44,080
but we also knew that we couldn't

00:21:41,760 --> 00:21:46,240
implement another comparator

00:21:44,080 --> 00:21:48,159
without being too different from numpy

00:21:46,240 --> 00:21:49,919
that people would notice

00:21:48,159 --> 00:21:51,440
so in this case we decided to simply

00:21:49,919 --> 00:21:53,200
throw a runtime error

00:21:51,440 --> 00:21:58,400
and inform the user that these

00:21:53,200 --> 00:22:00,480
operations aren't supported

00:21:58,400 --> 00:22:01,840
it's important that in all three of

00:22:00,480 --> 00:22:04,799
these cases

00:22:01,840 --> 00:22:07,120
the change from numpy was principled it

00:22:04,799 --> 00:22:09,840
was motivated by a desire to be more

00:22:07,120 --> 00:22:09,840
consistent

00:22:10,240 --> 00:22:14,400
well we saw earlier that users want

00:22:12,480 --> 00:22:15,520
faithful implementations of numpy

00:22:14,400 --> 00:22:17,200
operators

00:22:15,520 --> 00:22:19,840
it turns out that these principal

00:22:17,200 --> 00:22:21,840
discrepancies are okay

00:22:19,840 --> 00:22:23,679
in fact pytorch has systemic

00:22:21,840 --> 00:22:25,840
discrepancies with numpy that

00:22:23,679 --> 00:22:27,440
pass with little or no comment from our

00:22:25,840 --> 00:22:30,480
community

00:22:27,440 --> 00:22:33,360
our type promotion behavior for example

00:22:30,480 --> 00:22:34,960
is different because pytorch prefers the

00:22:33,360 --> 00:22:37,039
floating point data type

00:22:34,960 --> 00:22:40,320
well numpy prefers the default of the

00:22:37,039 --> 00:22:42,640
more precise double data type

00:22:40,320 --> 00:22:45,120
also in pytorch functions and method

00:22:42,640 --> 00:22:46,799
always compute the same operation

00:22:45,120 --> 00:22:48,559
but in numpy they can be slightly

00:22:46,799 --> 00:22:50,880
different

00:22:48,559 --> 00:22:52,640
and as the sharp eyed may have spotted

00:22:50,880 --> 00:22:55,440
an earlier snippet

00:22:52,640 --> 00:22:56,080
numpy sometimes returns scalars while

00:22:55,440 --> 00:22:59,919
pytorch

00:22:56,080 --> 00:22:59,919
consistently returns tensors

00:23:02,320 --> 00:23:06,480
let's take a minute to recap then talk

00:23:04,960 --> 00:23:08,960
about what we've learned from porting

00:23:06,480 --> 00:23:12,240
numpy operators to pytorch

00:23:08,960 --> 00:23:12,240
and discuss future work

00:23:12,960 --> 00:23:17,760
so here's the whole talk so far in case

00:23:15,120 --> 00:23:20,320
you just walked into the room

00:23:17,760 --> 00:23:21,600
numpy and python are popular python

00:23:20,320 --> 00:23:24,720
packages

00:23:21,600 --> 00:23:26,960
they operate with tensors

00:23:24,720 --> 00:23:27,919
pytorch implements many of numpy's

00:23:26,960 --> 00:23:30,720
operators

00:23:27,919 --> 00:23:32,799
but not all and it extends them with

00:23:30,720 --> 00:23:36,000
support for hardware accelerators and

00:23:32,799 --> 00:23:37,919
systems like auto grab

00:23:36,000 --> 00:23:40,799
pi torch community wants faithful

00:23:37,919 --> 00:23:42,799
implementations of numpy operators

00:23:40,799 --> 00:23:44,000
but it's also okay with principal

00:23:42,799 --> 00:23:46,240
differences

00:23:44,000 --> 00:23:48,159
as long as they keep pytorch consistent

00:23:46,240 --> 00:23:50,159
with itself

00:23:48,159 --> 00:23:52,320
to make implementing numpy operators

00:23:50,159 --> 00:23:56,400
tractable pytorch has developed a

00:23:52,320 --> 00:23:56,400
variety of supporting architectures

00:23:56,720 --> 00:24:00,559
given that recap what should we learn

00:23:58,960 --> 00:24:02,720
from this talk

00:24:00,559 --> 00:24:04,799
first do the work to engage your

00:24:02,720 --> 00:24:06,080
community and listen to what they tell

00:24:04,799 --> 00:24:08,720
you

00:24:06,080 --> 00:24:10,880
when pytorch started refocusing on numpy

00:24:08,720 --> 00:24:12,799
operators over a year ago

00:24:10,880 --> 00:24:15,360
it wasn't clear whether the community

00:24:12,799 --> 00:24:18,159
just wanted the functionality from numpy

00:24:15,360 --> 00:24:19,440
or fidelity to numpy and they've now

00:24:18,159 --> 00:24:21,440
made it clear that they want

00:24:19,440 --> 00:24:23,200
faithful implementations of numpy

00:24:21,440 --> 00:24:25,279
operators but they want those

00:24:23,200 --> 00:24:27,679
implementations to be consistent with

00:24:25,279 --> 00:24:31,200
pytorch's ux

00:24:27,679 --> 00:24:33,440
second focus on developer efficiency

00:24:31,200 --> 00:24:34,320
supporting architecture like pytorch's

00:24:33,440 --> 00:24:37,360
test framework

00:24:34,320 --> 00:24:40,720
has saved years of developer time

00:24:37,360 --> 00:24:42,720
third be clear about your own principles

00:24:40,720 --> 00:24:45,200
especially when implementing a user

00:24:42,720 --> 00:24:47,520
experience from another project

00:24:45,200 --> 00:24:49,760
blindly implementing numpy operators

00:24:47,520 --> 00:24:53,200
would have led to a really inconsistent

00:24:49,760 --> 00:24:55,279
pi torch experience in the future

00:24:53,200 --> 00:24:57,039
we plan to focus more on deprecating

00:24:55,279 --> 00:24:59,679
world war divergent with numpy

00:24:57,039 --> 00:25:01,360
instead of adding new numpy operators

00:24:59,679 --> 00:25:04,799
we're also interested in adding more

00:25:01,360 --> 00:25:04,799
operators from scipy

00:25:05,919 --> 00:25:09,279
thank you for coming and listening to

00:25:07,279 --> 00:25:15,840
this talk and i hope you enjoy the rest

00:25:09,279 --> 00:25:15,840
of pycon 2021

00:26:15,840 --> 00:26:17,919

YouTube URL: https://www.youtube.com/watch?v=5wk13yle5GA


