Title: TALK   Dino Viehland   Python Performance at Scale - Making Python Faster at Instagram
Publication date: 2021-05-30
Playlist: PyCon US 2021
Description: 
	Python is used in a large number of web sites where the performance of the web tier is a significant cost. There are multiple ways to improve the performance of these applications: improving the Python code itself, moving code out of Python using tools like Cython, and extreme options like directly improving the performance of the Python interpreter.

In this talk we’ll explore some of the changes we’ve made to the CPython runtime to improve the performance of our workload. We’ll start with a high level overview of our architecture which isn’t atypical for a Python web application and see opportunities and challenges that has provided for optimization. Then we’ll go deep down the rabbit hole and look at common hot spots in the Python runtime and the results we’ve had in reducing the overhead of them.

Along the way we’ll look at both targeted optimization opportunities and classic techniques such as inline caching, a JIT compiler, and leveraging type annotations for performance. We’ll cover techniques that we’ve proven successful, and ones that are still experimental. We’ll see how these can be applied to the Python runtime and what are the performance results of doing so: overall we’ve seen a 20-30% improvement in our production workload and up to 7x improvement on benchmarks.

Slides: https://www.viehland.com/PyCon_2021.pdf
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:14,719 --> 00:00:16,480
hi

00:00:15,040 --> 00:00:17,920
my name is dean o'veilland and today

00:00:16,480 --> 00:00:19,119
i'll be talking to you about python

00:00:17,920 --> 00:00:21,920
performance at scale

00:00:19,119 --> 00:00:23,680
and how we make python faster instagram

00:00:21,920 --> 00:00:25,039
we'll look at a few different things

00:00:23,680 --> 00:00:26,560
first we're going to look at some of the

00:00:25,039 --> 00:00:28,400
successful improvements that we've made

00:00:26,560 --> 00:00:30,000
to see python and we have our own fork

00:00:28,400 --> 00:00:32,160
of it that we call cinder

00:00:30,000 --> 00:00:34,800
we'll look at some more experimental

00:00:32,160 --> 00:00:37,040
work both in cinder and beyond

00:00:34,800 --> 00:00:40,079
and finally we'll look at the results of

00:00:37,040 --> 00:00:41,760
our work and what's next for us

00:00:40,079 --> 00:00:43,280
but before i get into that let me first

00:00:41,760 --> 00:00:44,719
talk about python and instagram this is

00:00:43,280 --> 00:00:46,000
going to be a super quick review for

00:00:44,719 --> 00:00:48,239
those of you who have seen other talks

00:00:46,000 --> 00:00:51,120
about instagram

00:00:48,239 --> 00:00:53,360
what is instagram it's a monolithic web

00:00:51,120 --> 00:00:56,840
application that runs on django

00:00:53,360 --> 00:00:58,719
python 3.8 and uses uwisky as the web

00:00:56,840 --> 00:01:00,399
server

00:00:58,719 --> 00:01:02,320
for those of you who aren't familiar

00:01:00,399 --> 00:01:03,680
with uwisky the way it works is that

00:01:02,320 --> 00:01:05,439
there's a parent process

00:01:03,680 --> 00:01:07,520
and then several child processes get

00:01:05,439 --> 00:01:09,439
created and these child processes are

00:01:07,520 --> 00:01:12,000
what serve the actual workload

00:01:09,439 --> 00:01:13,439
these will spawn and respawn over the

00:01:12,000 --> 00:01:16,640
lifetime of the application

00:01:13,439 --> 00:01:16,640
and serve all the requests

00:01:16,799 --> 00:01:20,560
we do a lot of profiling on our workload

00:01:19,119 --> 00:01:21,200
to understand what we need to make

00:01:20,560 --> 00:01:22,880
faster

00:01:21,200 --> 00:01:24,560
and the way we do this is with the linux

00:01:22,880 --> 00:01:26,479
perf sampling profiler

00:01:24,560 --> 00:01:28,159
we've done a few tweaks the runtime to

00:01:26,479 --> 00:01:30,479
get a little bit of better data

00:01:28,159 --> 00:01:32,240
especially around async call stacks

00:01:30,479 --> 00:01:34,079
and we get really good insights both at

00:01:32,240 --> 00:01:34,880
the python and the c level that we can

00:01:34,079 --> 00:01:36,880
drill into

00:01:34,880 --> 00:01:40,079
and kind of mix and match those views

00:01:36,880 --> 00:01:42,399
and understand what's going on

00:01:40,079 --> 00:01:43,200
we look at one main metric to improve

00:01:42,399 --> 00:01:45,680
our performance

00:01:43,200 --> 00:01:47,680
which is request per second under load

00:01:45,680 --> 00:01:49,600
we drive our servers up to a 90

00:01:47,680 --> 00:01:51,200
level of load and we see how many

00:01:49,600 --> 00:01:51,759
requests they're able to serve at that

00:01:51,200 --> 00:01:53,680
level

00:01:51,759 --> 00:01:55,840
it's not really a metric that's stable

00:01:53,680 --> 00:01:57,520
over time but it's good for just

00:01:55,840 --> 00:01:58,880
testing out the win and loss of an

00:01:57,520 --> 00:02:00,960
individual change

00:01:58,880 --> 00:02:02,479
as the application changes different

00:02:00,960 --> 00:02:05,200
requests need to

00:02:02,479 --> 00:02:07,040
use more or less cpu and so that's the

00:02:05,200 --> 00:02:10,319
reason why it's not good over a

00:02:07,040 --> 00:02:12,160
long period of time so

00:02:10,319 --> 00:02:13,840
let's dive in and look at the successful

00:02:12,160 --> 00:02:15,920
improvements that we've made to our fork

00:02:13,840 --> 00:02:18,000
of see python which is cinder one big

00:02:15,920 --> 00:02:20,000
change that we've made is to leverage

00:02:18,000 --> 00:02:21,120
that relationship between the parent and

00:02:20,000 --> 00:02:23,599
child processes

00:02:21,120 --> 00:02:24,239
that happens in the uwisky server these

00:02:23,599 --> 00:02:26,480
processes

00:02:24,239 --> 00:02:28,160
end up sharing a bunch of memory and the

00:02:26,480 --> 00:02:30,080
more we can maximize that

00:02:28,160 --> 00:02:33,120
the more memory that we have for the

00:02:30,080 --> 00:02:35,519
children to actually serve the request

00:02:33,120 --> 00:02:37,680
the way shared memory works is when a

00:02:35,519 --> 00:02:39,519
child writes to one of these pages

00:02:37,680 --> 00:02:41,360
it ends up no longer being shared and

00:02:39,519 --> 00:02:43,200
the child process gets its own copy

00:02:41,360 --> 00:02:44,800
so we want to avoid that as much as we

00:02:43,200 --> 00:02:46,640
can

00:02:44,800 --> 00:02:48,160
a large source of these rights is just

00:02:46,640 --> 00:02:50,400
from the reference counts to the objects

00:02:48,160 --> 00:02:52,640
that live in the heap before we fork

00:02:50,400 --> 00:02:54,640
so what we've done is we've modified the

00:02:52,640 --> 00:02:57,280
reference count to use a high bit in it

00:02:54,640 --> 00:02:58,560
that marks objects as being immortal

00:02:57,280 --> 00:03:00,159
and this just means that they're never

00:02:58,560 --> 00:03:01,760
actually going to die inside of the

00:03:00,159 --> 00:03:03,440
child processes

00:03:01,760 --> 00:03:05,599
this means that we have to update pi

00:03:03,440 --> 00:03:06,080
increment and pi decrep the macros which

00:03:05,599 --> 00:03:08,080
alter

00:03:06,080 --> 00:03:09,360
the reference count and every single

00:03:08,080 --> 00:03:11,280
time we do one of those

00:03:09,360 --> 00:03:13,280
we need to avoid updating the reference

00:03:11,280 --> 00:03:14,319
count this actually adds a significant

00:03:13,280 --> 00:03:16,319
amount of overhead

00:03:14,319 --> 00:03:18,720
in the normal case of just running

00:03:16,319 --> 00:03:20,480
normal code not in this forked model

00:03:18,720 --> 00:03:23,440
but the memory savings actually make it

00:03:20,480 --> 00:03:26,080
more than worth it in our workload

00:03:23,440 --> 00:03:27,280
pre-fork the heap is actually collected

00:03:26,080 --> 00:03:28,959
and then it's traversed

00:03:27,280 --> 00:03:31,360
and all those objects that are in the

00:03:28,959 --> 00:03:33,200
heap are marked as being immortal

00:03:31,360 --> 00:03:35,680
and this actually gives us a five

00:03:33,200 --> 00:03:37,280
percent win in requests per second in

00:03:35,680 --> 00:03:39,200
our production workloads which is pretty

00:03:37,280 --> 00:03:42,400
amazing

00:03:39,200 --> 00:03:42,879
another area that we've focused a lot of

00:03:42,400 --> 00:03:46,480
effort

00:03:42,879 --> 00:03:46,879
is on async i o so one big change we've

00:03:46,480 --> 00:03:49,760
made

00:03:46,879 --> 00:03:52,000
is just avoiding using stop iteration to

00:03:49,760 --> 00:03:54,400
send and receive values into async

00:03:52,000 --> 00:03:56,000
functions this is creating an exception

00:03:54,400 --> 00:03:56,799
object which is a major source of

00:03:56,000 --> 00:03:58,560
overhead

00:03:56,799 --> 00:04:00,720
and on simple benchmarks we see

00:03:58,560 --> 00:04:02,400
improvements of it being 1.6 times

00:04:00,720 --> 00:04:04,000
faster

00:04:02,400 --> 00:04:07,360
we actually upstream this work into

00:04:04,000 --> 00:04:08,799
python so it's available in python 3.10

00:04:07,360 --> 00:04:10,799
through a couple different patches that

00:04:08,799 --> 00:04:11,280
have improved it and this was yet

00:04:10,799 --> 00:04:12,879
another

00:04:11,280 --> 00:04:15,360
five percent one in production which is

00:04:12,879 --> 00:04:17,040
really really huge for us

00:04:15,360 --> 00:04:18,560
another way that we've improved async i

00:04:17,040 --> 00:04:20,239
o is something that we call eager

00:04:18,560 --> 00:04:21,440
evaluation

00:04:20,239 --> 00:04:24,080
frequently you have something that will

00:04:21,440 --> 00:04:24,560
be like a wait some call and right now

00:04:24,080 --> 00:04:26,479
that

00:04:24,560 --> 00:04:28,960
needs to go off and schedule that

00:04:26,479 --> 00:04:30,560
function to run onto the event loop

00:04:28,960 --> 00:04:32,000
but a lot of functions will just

00:04:30,560 --> 00:04:33,680
complete synchronously and you don't

00:04:32,000 --> 00:04:35,680
have to wait for them

00:04:33,680 --> 00:04:37,600
so if it completes without blocking we

00:04:35,680 --> 00:04:39,120
avoid the creation of the code routine

00:04:37,600 --> 00:04:40,960
and instead what we do is we have a

00:04:39,120 --> 00:04:42,080
singleton weight handle that gets

00:04:40,960 --> 00:04:44,320
returned and this

00:04:42,080 --> 00:04:46,080
actually has the value and it gets

00:04:44,320 --> 00:04:46,960
immediately consumed at the call site

00:04:46,080 --> 00:04:50,960
there's only one

00:04:46,960 --> 00:04:54,000
in the entire process we also use this

00:04:50,960 --> 00:04:55,520
uh with async io.gather and the way this

00:04:54,000 --> 00:04:57,199
is implemented is that there's a new

00:04:55,520 --> 00:04:58,320
vector call flag to indicate that

00:04:57,199 --> 00:05:02,080
something's awaited

00:04:58,320 --> 00:05:05,199
and so functions recognize this flag

00:05:02,080 --> 00:05:06,160
as do does async io.gather and this is a

00:05:05,199 --> 00:05:09,280
three percent win

00:05:06,160 --> 00:05:10,400
in production another big optimization

00:05:09,280 --> 00:05:12,639
we've done is adding

00:05:10,400 --> 00:05:14,720
inline caching support into the bytecode

00:05:12,639 --> 00:05:16,400
the way this works is hot methods get a

00:05:14,720 --> 00:05:18,160
hidden copy of the bytecode and you can

00:05:16,400 --> 00:05:19,039
see the data structure for that over on

00:05:18,160 --> 00:05:21,280
the right

00:05:19,039 --> 00:05:25,280
this includes a copy of the bytecode as

00:05:21,280 --> 00:05:27,039
well as a bunch of inline caches as well

00:05:25,280 --> 00:05:29,520
when we encounter an opcode that we can

00:05:27,039 --> 00:05:31,120
optimize we replace the existing opcode

00:05:29,520 --> 00:05:33,360
with a more specific one

00:05:31,120 --> 00:05:34,800
which can simply do a type check and do

00:05:33,360 --> 00:05:36,639
a fast dispatch

00:05:34,800 --> 00:05:38,639
that doesn't have to go through the full

00:05:36,639 --> 00:05:40,240
normal dynamic lookup

00:05:38,639 --> 00:05:42,000
overall this has been a five percent win

00:05:40,240 --> 00:05:45,440
in production for us which is again

00:05:42,000 --> 00:05:46,479
another really big one so let's look at

00:05:45,440 --> 00:05:47,199
some of the bytecodes that we end up

00:05:46,479 --> 00:05:50,000
replacing

00:05:47,199 --> 00:05:51,199
and what we replace them with load adder

00:05:50,000 --> 00:05:52,960
is one of the most common

00:05:51,199 --> 00:05:54,960
op codes that we use because we look up

00:05:52,960 --> 00:05:55,759
a lot of attributes every python program

00:05:54,960 --> 00:05:57,039
does

00:05:55,759 --> 00:05:58,880
and there's a lot of different

00:05:57,039 --> 00:06:01,759
variations of things that we can look up

00:05:58,880 --> 00:06:02,800
attributes against a lot of time

00:06:01,759 --> 00:06:05,199
attributes

00:06:02,800 --> 00:06:06,880
are methods which are descriptors a lot

00:06:05,199 --> 00:06:09,199
of time attributes are

00:06:06,880 --> 00:06:11,120
instance members stored in a dictionary

00:06:09,199 --> 00:06:12,240
python also has something called split

00:06:11,120 --> 00:06:15,280
dictionaries

00:06:12,240 --> 00:06:18,240
where the dictionary layout is shared

00:06:15,280 --> 00:06:20,720
between multiple instances and of course

00:06:18,240 --> 00:06:22,880
we have things like types and modules

00:06:20,720 --> 00:06:24,319
and we have polymorphic call sites as

00:06:22,880 --> 00:06:27,440
well where the attributes are being

00:06:24,319 --> 00:06:30,080
looked up against multiple objects

00:06:27,440 --> 00:06:31,919
and the opposite of loading attributes

00:06:30,080 --> 00:06:34,479
is of course drawing attributes

00:06:31,919 --> 00:06:36,240
this doesn't happen quite as much and it

00:06:34,479 --> 00:06:38,160
doesn't happen quite with as many

00:06:36,240 --> 00:06:40,160
sort of variations for example you don't

00:06:38,160 --> 00:06:42,319
do a lot of stores against types

00:06:40,160 --> 00:06:44,319
so this one ends up being a lot simpler

00:06:42,319 --> 00:06:45,440
and we have a lot fewer replacements for

00:06:44,319 --> 00:06:47,919
it

00:06:45,440 --> 00:06:49,599
load global is of course a big big thing

00:06:47,919 --> 00:06:51,039
people look up the names of types and

00:06:49,599 --> 00:06:51,759
other things they've imported all the

00:06:51,039 --> 00:06:53,919
time

00:06:51,759 --> 00:06:55,840
and so we have a simple replacement for

00:06:53,919 --> 00:06:57,440
that which is load global cache

00:06:55,840 --> 00:07:00,560
and we'll see how that works in a little

00:06:57,440 --> 00:07:01,759
bit more detail on the next slide

00:07:00,560 --> 00:07:03,599
and then we have some other things that

00:07:01,759 --> 00:07:05,120
we've optimized as well like binary

00:07:03,599 --> 00:07:07,039
subscription

00:07:05,120 --> 00:07:08,639
and this isn't all the options that

00:07:07,039 --> 00:07:12,400
we've replaced but it gives you a pretty

00:07:08,639 --> 00:07:12,400
good feel of the surface area

00:07:12,639 --> 00:07:16,319
so the way we've optimized our global

00:07:14,479 --> 00:07:17,360
lookups is with something that's called

00:07:16,319 --> 00:07:19,599
dictionary work

00:07:17,360 --> 00:07:21,360
watchers dictionary watchers provide

00:07:19,599 --> 00:07:22,880
updates to globals and built-ins when

00:07:21,360 --> 00:07:24,479
they're modified

00:07:22,880 --> 00:07:26,400
so we start off with the built-ins

00:07:24,479 --> 00:07:27,759
dictionary which has all the built-ins

00:07:26,400 --> 00:07:28,240
that you're familiar with things like

00:07:27,759 --> 00:07:31,360
min

00:07:28,240 --> 00:07:33,120
max type and so on

00:07:31,360 --> 00:07:34,960
and then a module has its own dictionary

00:07:33,120 --> 00:07:36,479
where you've defined the things and

00:07:34,960 --> 00:07:38,880
some of these things might just be top

00:07:36,479 --> 00:07:40,720
level things like x equals one

00:07:38,880 --> 00:07:42,160
other things might be shadowing things

00:07:40,720 --> 00:07:44,720
that are built-ins like

00:07:42,160 --> 00:07:45,759
max here has been shadowed to return men

00:07:44,720 --> 00:07:47,759
i don't know why

00:07:45,759 --> 00:07:49,360
you'd want to do that let's say you did

00:07:47,759 --> 00:07:50,319
and then finally we have a function here

00:07:49,360 --> 00:07:52,800
which is actually

00:07:50,319 --> 00:07:54,160
using the globals that are defined so

00:07:52,800 --> 00:07:57,360
it's calling max

00:07:54,160 --> 00:07:59,520
and passing xn and 42. so we'll end up

00:07:57,360 --> 00:08:01,120
with three different caches for these

00:07:59,520 --> 00:08:03,680
all the globals that are inside this

00:08:01,120 --> 00:08:04,639
function max will end up storing the

00:08:03,680 --> 00:08:07,039
value for the

00:08:04,639 --> 00:08:08,879
max function that's in the module x will

00:08:07,039 --> 00:08:09,759
end up storing the value for x that's in

00:08:08,879 --> 00:08:11,840
the module

00:08:09,759 --> 00:08:13,520
and min will end up storing the value

00:08:11,840 --> 00:08:15,440
that's men in the built-ins

00:08:13,520 --> 00:08:17,680
and if we were to go and mutate either

00:08:15,440 --> 00:08:19,440
the module or the built-ins

00:08:17,680 --> 00:08:21,120
these little holders would get updated

00:08:19,440 --> 00:08:22,960
when that mutation happens so that all

00:08:21,120 --> 00:08:24,319
we have to do is retrieve a value from

00:08:22,960 --> 00:08:26,720
one address in memory

00:08:24,319 --> 00:08:29,680
and we know exactly what the built-in is

00:08:26,720 --> 00:08:31,440
with no extra checks

00:08:29,680 --> 00:08:34,080
the way we've implemented this is to

00:08:31,440 --> 00:08:36,800
reuse the existing version tag to mark

00:08:34,080 --> 00:08:38,719
watch dictionaries and the way that

00:08:36,800 --> 00:08:40,880
works is dictionaries are marked with a

00:08:38,719 --> 00:08:42,320
low bit in the dictionary version tag

00:08:40,880 --> 00:08:44,240
so now whenever we're updating a

00:08:42,320 --> 00:08:44,800
dictionary version we actually bump it

00:08:44,240 --> 00:08:46,800
by two

00:08:44,800 --> 00:08:48,000
instead of bumping it by one and we can

00:08:46,800 --> 00:08:49,360
just check that little bit to know

00:08:48,000 --> 00:08:51,760
whether or not a dictionary

00:08:49,360 --> 00:08:52,800
is being watched whenever a dictionary

00:08:51,760 --> 00:08:55,680
gets mutated

00:08:52,800 --> 00:08:58,399
so this gives us a really low overhead

00:08:55,680 --> 00:09:00,160
way to implement this feature

00:08:58,399 --> 00:09:01,600
and when we integrate this in with

00:09:00,160 --> 00:09:02,640
shadow bytecode

00:09:01,600 --> 00:09:04,480
because it was originally an

00:09:02,640 --> 00:09:06,320
optimization that we used in legit which

00:09:04,480 --> 00:09:08,480
i'll talk about soon

00:09:06,320 --> 00:09:10,480
it ended up being an extra five percent

00:09:08,480 --> 00:09:12,320
win on top of the existing shadow by

00:09:10,480 --> 00:09:14,080
code

00:09:12,320 --> 00:09:16,560
we've done a bunch of targeted

00:09:14,080 --> 00:09:19,920
optimizations as well

00:09:16,560 --> 00:09:22,160
one of those is fixed under built-ins

00:09:19,920 --> 00:09:24,240
so dunder built-ins is an attribute that

00:09:22,160 --> 00:09:26,880
exists at the module level

00:09:24,240 --> 00:09:27,760
and if you were to redefine it in c

00:09:26,880 --> 00:09:29,680
python

00:09:27,760 --> 00:09:30,880
you'll actually get a different set of

00:09:29,680 --> 00:09:32,399
built-ins

00:09:30,880 --> 00:09:34,480
there's a caveat there you only

00:09:32,399 --> 00:09:35,600
sometimes get those additional sets of

00:09:34,480 --> 00:09:37,360
built-ins

00:09:35,600 --> 00:09:39,040
there's some weird see python

00:09:37,360 --> 00:09:40,240
implementation details that kind of

00:09:39,040 --> 00:09:42,320
thwart that

00:09:40,240 --> 00:09:44,240
but this is actually documented as a c

00:09:42,320 --> 00:09:45,519
python implementation detail

00:09:44,240 --> 00:09:47,760
and we just got rid of that

00:09:45,519 --> 00:09:50,000
implementation detail so that ended up

00:09:47,760 --> 00:09:53,680
giving us about a one percent when

00:09:50,000 --> 00:09:55,760
on requests per second we've done some

00:09:53,680 --> 00:09:56,959
micro optimization throughout the entire

00:09:55,760 --> 00:09:58,240
code base

00:09:56,959 --> 00:10:01,040
one that ended up being kind of

00:09:58,240 --> 00:10:01,519
significant on benchmarks was pi type

00:10:01,040 --> 00:10:03,120
lookup

00:10:01,519 --> 00:10:04,880
and we've upstreamed this so it'll be in

00:10:03,120 --> 00:10:08,800
python 3.10

00:10:04,880 --> 00:10:11,360
and on some benchmarks this is 1.19

00:10:08,800 --> 00:10:13,200
times faster so that's on inbody but

00:10:11,360 --> 00:10:14,480
there are a couple of dozen benchmarks

00:10:13,200 --> 00:10:17,200
that showed improvement

00:10:14,480 --> 00:10:19,360
with a minimum of 1.03 x improvement so

00:10:17,200 --> 00:10:21,120
that was really kind of meaningful

00:10:19,360 --> 00:10:23,760
at least as far as the micro benchmarks

00:10:21,120 --> 00:10:24,320
go shockingly this was really hard for

00:10:23,760 --> 00:10:27,760
us to

00:10:24,320 --> 00:10:29,920
measure in production because it is

00:10:27,760 --> 00:10:32,000
just a really micro tweak to what's

00:10:29,920 --> 00:10:33,839
going on we're doing a cache lookup

00:10:32,000 --> 00:10:36,079
so we don't actually attribute to

00:10:33,839 --> 00:10:38,160
anything there

00:10:36,079 --> 00:10:40,560
we've done a lot of work to avoid thread

00:10:38,160 --> 00:10:42,320
state lookup throughout the runtime

00:10:40,560 --> 00:10:44,240
this is work that has been happening

00:10:42,320 --> 00:10:47,360
upstream as well

00:10:44,240 --> 00:10:49,279
so one nice thing is as we upgrade

00:10:47,360 --> 00:10:50,480
we'll have less patches for these sorts

00:10:49,279 --> 00:10:51,680
of things

00:10:50,480 --> 00:10:53,680
we've also done some work around

00:10:51,680 --> 00:10:54,399
pre-fetching where loading certain

00:10:53,680 --> 00:10:56,720
attributes

00:10:54,399 --> 00:10:58,640
especially around frame creation ends up

00:10:56,720 --> 00:11:00,640
going off in reading memory we can just

00:10:58,640 --> 00:11:03,040
pre-fetch that earlier and avoid some

00:11:00,640 --> 00:11:03,040
stalls

00:11:03,680 --> 00:11:07,040
a huge amount of improvements have

00:11:05,279 --> 00:11:08,399
actually come from just build system

00:11:07,040 --> 00:11:11,440
improvements

00:11:08,399 --> 00:11:13,519
so normally c python is optimized using

00:11:11,440 --> 00:11:17,360
profile guided optimizations

00:11:13,519 --> 00:11:19,680
and those optimizations go through

00:11:17,360 --> 00:11:21,680
and look at what code's running and

00:11:19,680 --> 00:11:24,160
optimize those specific code paths

00:11:21,680 --> 00:11:24,959
bring branches that are frequent closer

00:11:24,160 --> 00:11:27,839
together

00:11:24,959 --> 00:11:29,120
inline things things like that and by

00:11:27,839 --> 00:11:31,760
default see python

00:11:29,120 --> 00:11:33,200
just runs pgo against a set of tests and

00:11:31,760 --> 00:11:34,560
that's what we're doing for a long time

00:11:33,200 --> 00:11:36,800
as well

00:11:34,560 --> 00:11:38,560
we also use something called bolt which

00:11:36,800 --> 00:11:40,640
is an additional

00:11:38,560 --> 00:11:43,040
binary optimizer that improves the

00:11:40,640 --> 00:11:45,040
layout of the binaries even more

00:11:43,040 --> 00:11:47,839
and for both of these we switched to

00:11:45,040 --> 00:11:48,640
actually using data from live production

00:11:47,839 --> 00:11:50,399
host

00:11:48,640 --> 00:11:51,920
rather than using the test cases which

00:11:50,399 --> 00:11:52,560
include many paths that we're not going

00:11:51,920 --> 00:11:54,639
to take

00:11:52,560 --> 00:11:57,200
and don't necessarily include all the

00:11:54,639 --> 00:11:59,760
paths that we will take

00:11:57,200 --> 00:12:02,160
another big source of wins has been

00:11:59,760 --> 00:12:03,760
moving our you whiskey binary onto huge

00:12:02,160 --> 00:12:05,519
pages

00:12:03,760 --> 00:12:07,519
this helps reduce the instruction

00:12:05,519 --> 00:12:09,920
translation look aside buffer

00:12:07,519 --> 00:12:12,480
cache misses and that was about a three

00:12:09,920 --> 00:12:14,320
percent win so that's pretty awesome

00:12:12,480 --> 00:12:15,600
we've also done a whole bunch of other

00:12:14,320 --> 00:12:16,800
experimental changes

00:12:15,600 --> 00:12:19,440
these are still things that we're

00:12:16,800 --> 00:12:20,959
working on and hope will pan out but

00:12:19,440 --> 00:12:21,680
it's still a little bit too early to

00:12:20,959 --> 00:12:23,519
tell

00:12:21,680 --> 00:12:26,079
so one of the biggest changes to cinder

00:12:23,519 --> 00:12:28,160
has been the development of a custom jet

00:12:26,079 --> 00:12:30,160
this is a method at a time jet and we

00:12:28,160 --> 00:12:31,920
have nearly full coverage of all of the

00:12:30,160 --> 00:12:33,680
opcodes now

00:12:31,920 --> 00:12:35,200
most of the unsupported opcodes are

00:12:33,680 --> 00:12:36,560
things that are super rare or that we

00:12:35,200 --> 00:12:38,560
don't really care about things like

00:12:36,560 --> 00:12:41,120
import star which are only going to

00:12:38,560 --> 00:12:43,600
occur at the top level of the module

00:12:41,120 --> 00:12:46,079
which we're never going to get

00:12:43,600 --> 00:12:46,959
the jit works uh with both a front end

00:12:46,079 --> 00:12:50,079
and a back end

00:12:46,959 --> 00:12:52,560
the front end lowers to our hir

00:12:50,079 --> 00:12:53,600
intermediate representation this does

00:12:52,560 --> 00:12:56,160
things like

00:12:53,600 --> 00:12:58,320
single static assignment we have a ref

00:12:56,160 --> 00:13:00,880
count insertion pass that runs over

00:12:58,320 --> 00:13:03,120
the final code to actually insert our

00:13:00,880 --> 00:13:06,000
ink roughs and decreps so that

00:13:03,120 --> 00:13:07,920
they can be as optimal as possible and

00:13:06,000 --> 00:13:09,760
we do some other optimization passes in

00:13:07,920 --> 00:13:12,480
here as well

00:13:09,760 --> 00:13:12,880
so if we take a simple function this is

00:13:12,480 --> 00:13:15,519
just

00:13:12,880 --> 00:13:17,519
assigning an attribute to a class it

00:13:15,519 --> 00:13:19,200
ends up turning into this hir

00:13:17,519 --> 00:13:21,279
initially and so this is pretty

00:13:19,200 --> 00:13:23,600
straightforward we're loading self

00:13:21,279 --> 00:13:24,639
we're loading the value one we check to

00:13:23,600 --> 00:13:26,720
see if self is

00:13:24,639 --> 00:13:28,399
defined this is something that happens

00:13:26,720 --> 00:13:29,600
in the interpreter loop for every single

00:13:28,399 --> 00:13:32,839
load of a variable

00:13:29,600 --> 00:13:34,160
we store the value one and we return

00:13:32,839 --> 00:13:35,920
none

00:13:34,160 --> 00:13:37,760
when we take this through all the passes

00:13:35,920 --> 00:13:38,480
we end up with this slightly different

00:13:37,760 --> 00:13:40,800
tree

00:13:38,480 --> 00:13:42,880
and so you can see here that we've

00:13:40,800 --> 00:13:43,519
managed to get rid of the variable check

00:13:42,880 --> 00:13:45,519
on self

00:13:43,519 --> 00:13:47,920
it's a parameter it's never been deleted

00:13:45,519 --> 00:13:50,160
we know it's always going to be signed

00:13:47,920 --> 00:13:51,040
and you can also see that we've inserted

00:13:50,160 --> 00:13:52,880
some ink refs

00:13:51,040 --> 00:13:54,160
on the none and the return that wasn't

00:13:52,880 --> 00:13:55,600
there before

00:13:54,160 --> 00:13:57,519
but the other interesting thing is there

00:13:55,600 --> 00:13:59,040
aren't actually any ink graphs on other

00:13:57,519 --> 00:13:59,920
things like we don't have to incorrect

00:13:59,040 --> 00:14:03,120
self

00:13:59,920 --> 00:14:04,480
or incorrect the value of one we know

00:14:03,120 --> 00:14:07,440
those are going to be alive for the

00:14:04,480 --> 00:14:07,440
lifetime of the function

00:14:07,839 --> 00:14:13,680
after going through hir we lower things

00:14:11,120 --> 00:14:15,760
to our lower intermediate representation

00:14:13,680 --> 00:14:17,360
here things like register allocation

00:14:15,760 --> 00:14:19,199
happen and we do some targeted

00:14:17,360 --> 00:14:20,880
optimizations while lowering as well

00:14:19,199 --> 00:14:23,839
things like direct dispatch to known

00:14:20,880 --> 00:14:27,279
functions and this uses azim jet for its

00:14:23,839 --> 00:14:29,839
final x64 code generation

00:14:27,279 --> 00:14:30,880
so if we take that same function this is

00:14:29,839 --> 00:14:32,880
what the lir

00:14:30,880 --> 00:14:35,600
ends up looking like so you can see we

00:14:32,880 --> 00:14:38,800
start seeing actual registers in here

00:14:35,600 --> 00:14:41,440
where we're binding the argument to rdi

00:14:38,800 --> 00:14:42,320
you can see that we load the constant

00:14:41,440 --> 00:14:46,160
the store is

00:14:42,320 --> 00:14:49,120
a complicated call to a helper but um

00:14:46,160 --> 00:14:49,920
it's a call and then we load the cons

00:14:49,120 --> 00:14:52,639
for none

00:14:49,920 --> 00:14:54,079
and we do an in-graph on it and this bit

00:14:52,639 --> 00:14:55,519
test that you see in here is actually

00:14:54,079 --> 00:14:57,199
part of

00:14:55,519 --> 00:14:59,440
the immortalization that we do that

00:14:57,199 --> 00:15:02,639
makes this a little bit more expensive

00:14:59,440 --> 00:15:04,079
and finally we return the value if not

00:15:02,639 --> 00:15:05,680
another experiment we've been working on

00:15:04,079 --> 00:15:07,279
is something that we call static python

00:15:05,680 --> 00:15:08,720
this provides you with the performance

00:15:07,279 --> 00:15:10,399
gains that you get from something like

00:15:08,720 --> 00:15:12,560
mypi c or cython

00:15:10,399 --> 00:15:14,720
but you just use normal python files and

00:15:12,560 --> 00:15:16,399
load them at runtime like normal

00:15:14,720 --> 00:15:18,399
this all starts with a source loader

00:15:16,399 --> 00:15:20,320
which recognizes files that

00:15:18,399 --> 00:15:22,959
have been marked with a import under

00:15:20,320 --> 00:15:24,720
static it supports cross module

00:15:22,959 --> 00:15:26,480
compilation and so we'll go off and

00:15:24,720 --> 00:15:28,959
analyze other modules that the static

00:15:26,480 --> 00:15:31,199
module is importing as well

00:15:28,959 --> 00:15:32,480
there's a new set of byte codes for

00:15:31,199 --> 00:15:34,160
static features

00:15:32,480 --> 00:15:35,920
things like invoke function and load

00:15:34,160 --> 00:15:38,639
field and rather than taking

00:15:35,920 --> 00:15:39,199
simple parameters like what attribute to

00:15:38,639 --> 00:15:40,800
load

00:15:39,199 --> 00:15:42,399
it takes a descriptor which indicates

00:15:40,800 --> 00:15:43,519
the class and the field that we're

00:15:42,399 --> 00:15:44,959
loading it from

00:15:43,519 --> 00:15:47,839
and that can be bound more efficiently

00:15:44,959 --> 00:15:48,800
at room time it just uses pet 44

00:15:47,839 --> 00:15:51,759
annotations

00:15:48,800 --> 00:15:53,120
like my pisces does but we also added

00:15:51,759 --> 00:15:55,600
several new types

00:15:53,120 --> 00:15:56,720
things like n64 which live in the dunder

00:15:55,600 --> 00:15:58,320
static module

00:15:56,720 --> 00:16:00,079
and these allow you to use primitive

00:15:58,320 --> 00:16:02,320
image or arithmetic in a very efficient

00:16:00,079 --> 00:16:02,320
way

00:16:02,880 --> 00:16:06,240
static python code has to interrupt with

00:16:05,040 --> 00:16:08,079
normal python code

00:16:06,240 --> 00:16:10,320
in a safe manner and therefore we

00:16:08,079 --> 00:16:12,959
enforce type tracks at boundaries

00:16:10,320 --> 00:16:14,800
so when you call in to statically type

00:16:12,959 --> 00:16:17,120
python code from normal python code

00:16:14,800 --> 00:16:19,759
we'll validate the arguments are correct

00:16:17,120 --> 00:16:21,360
on the function call and we're using a

00:16:19,759 --> 00:16:24,800
new static compiler for this

00:16:21,360 --> 00:16:26,880
this is based upon the python 2

00:16:24,800 --> 00:16:28,320
compiler package which has been updated

00:16:26,880 --> 00:16:30,480
to python 3

00:16:28,320 --> 00:16:32,480
and we've added support for emitting the

00:16:30,480 --> 00:16:35,120
static op codes as well so the compiler

00:16:32,480 --> 00:16:36,959
is entirely written in python

00:16:35,120 --> 00:16:38,399
so let's take a look at what some static

00:16:36,959 --> 00:16:41,120
python looks like

00:16:38,399 --> 00:16:42,399
um so it all starts with the import

00:16:41,120 --> 00:16:45,360
under static and this

00:16:42,399 --> 00:16:45,839
marks that the static loader should be

00:16:45,360 --> 00:16:47,279
used

00:16:45,839 --> 00:16:50,000
and that this should be statically

00:16:47,279 --> 00:16:53,199
compiled we've got some things like from

00:16:50,000 --> 00:16:54,800
static import in 64. and so this is type

00:16:53,199 --> 00:16:56,800
annotation that can be used to indicate

00:16:54,800 --> 00:16:59,680
that something's a primitive type

00:16:56,800 --> 00:17:01,279
and it will avoid a boxed imager and

00:16:59,680 --> 00:17:02,240
significantly increase the performance

00:17:01,279 --> 00:17:05,439
if you're doing

00:17:02,240 --> 00:17:07,839
simple image or arithmetic we can do

00:17:05,439 --> 00:17:11,520
things like use final constants so final

00:17:07,839 --> 00:17:13,439
int is a normal um type annotation

00:17:11,520 --> 00:17:15,120
and this will allow us to treat this

00:17:13,439 --> 00:17:16,720
global as a constant

00:17:15,120 --> 00:17:19,280
and actually inline it into the

00:17:16,720 --> 00:17:21,600
generated code

00:17:19,280 --> 00:17:23,280
so i mentioned that we do type checks at

00:17:21,600 --> 00:17:25,439
the boundaries and so that's actually

00:17:23,280 --> 00:17:28,160
done by emitting a check args

00:17:25,439 --> 00:17:30,000
op code when we call into functions

00:17:28,160 --> 00:17:31,600
we'll actually skip this op code

00:17:30,000 --> 00:17:33,520
when we're calling from stack python to

00:17:31,600 --> 00:17:34,720
static python but here's what it looks

00:17:33,520 --> 00:17:36,240
like for this function

00:17:34,720 --> 00:17:38,320
we need to make sure that self is an

00:17:36,240 --> 00:17:41,919
instance of c and that next

00:17:38,320 --> 00:17:41,919
is an instance of c or none

00:17:42,240 --> 00:17:46,080
we transform the initialized slots in

00:17:44,720 --> 00:17:48,160
dunder in it

00:17:46,080 --> 00:17:49,360
the initialized fields in dunder in it

00:17:48,160 --> 00:17:52,160
into slots

00:17:49,360 --> 00:17:54,480
and so that ends up with a dunder slots

00:17:52,160 --> 00:17:55,280
assignment in the generated code for the

00:17:54,480 --> 00:17:57,200
class

00:17:55,280 --> 00:17:59,760
and we've extended this to support slot

00:17:57,200 --> 00:18:02,720
types and so here we actually say that

00:17:59,760 --> 00:18:03,919
length is a in 64. and that's just a

00:18:02,720 --> 00:18:06,960
normal struct member

00:18:03,919 --> 00:18:10,160
in c python parlance and next

00:18:06,960 --> 00:18:11,600
ends up being typed to this optional c

00:18:10,160 --> 00:18:13,280
and we've added a new type descriptor

00:18:11,600 --> 00:18:15,840
that also does type enforcement when you

00:18:13,280 --> 00:18:15,840
assign to it

00:18:16,320 --> 00:18:20,000
for our field stores we end up

00:18:18,480 --> 00:18:21,760
generating something like this

00:18:20,000 --> 00:18:24,320
where we have a store field op code and

00:18:21,760 --> 00:18:25,600
we say what instance and what name we

00:18:24,320 --> 00:18:28,000
are assigning to

00:18:25,600 --> 00:18:29,440
and when this turns into generated code

00:18:28,000 --> 00:18:30,320
it's literally just a single move

00:18:29,440 --> 00:18:32,400
instruction

00:18:30,320 --> 00:18:34,160
that's moving to the correct offset

00:18:32,400 --> 00:18:34,960
which is much more efficient than having

00:18:34,160 --> 00:18:38,080
to go through

00:18:34,960 --> 00:18:39,280
a whole set of system to

00:18:38,080 --> 00:18:42,720
figure out what attribute we're

00:18:39,280 --> 00:18:45,200
assigning to and where that lives

00:18:42,720 --> 00:18:46,000
and then things like this primitive

00:18:45,200 --> 00:18:48,240
integer

00:18:46,000 --> 00:18:49,520
arithmetic actually turn into special op

00:18:48,240 --> 00:18:52,559
codes as well

00:18:49,520 --> 00:18:55,760
and when we jit those those turn into

00:18:52,559 --> 00:18:56,559
simple x64 instructions to load a simple

00:18:55,760 --> 00:18:59,200
integer

00:18:56,559 --> 00:19:00,480
and to do the multiply another big

00:18:59,200 --> 00:19:01,840
experiment we're working on is something

00:19:00,480 --> 00:19:04,080
that we call pyro which is an

00:19:01,840 --> 00:19:04,880
experimental from scratch runtime that

00:19:04,080 --> 00:19:07,679
reuses the c

00:19:04,880 --> 00:19:09,200
python standard library there's several

00:19:07,679 --> 00:19:12,720
significant differences from c

00:19:09,200 --> 00:19:13,440
python it has compacting gc it uses tag

00:19:12,720 --> 00:19:15,600
pointers

00:19:13,440 --> 00:19:17,039
that allows us to treat integers or

00:19:15,600 --> 00:19:19,440
floating point types

00:19:17,039 --> 00:19:21,039
as primitives and it uses hidden classes

00:19:19,440 --> 00:19:24,000
which give it a really efficient way to

00:19:21,039 --> 00:19:27,679
provide inline caching

00:19:24,000 --> 00:19:28,480
we used uh the c api pep 384 subset for

00:19:27,679 --> 00:19:31,440
supporting c

00:19:28,480 --> 00:19:33,679
extensions and there are several open

00:19:31,440 --> 00:19:36,799
questions about pyro

00:19:33,679 --> 00:19:38,480
there's the difficulty in adapting a pet

00:19:36,799 --> 00:19:40,960
384 at scale

00:19:38,480 --> 00:19:42,880
most modules are not written to pep384

00:19:40,960 --> 00:19:44,880
and there's the performance of that api

00:19:42,880 --> 00:19:47,919
emulation

00:19:44,880 --> 00:19:48,720
so what's next one thing we'd like to

00:19:47,919 --> 00:19:50,960
work on more

00:19:48,720 --> 00:19:52,880
is upstreaming some of our changes

00:19:50,960 --> 00:19:53,919
you've seen from the previous slides

00:19:52,880 --> 00:19:55,840
there have been various things that

00:19:53,919 --> 00:19:57,120
we've already upstreamed and there's a

00:19:55,840 --> 00:19:58,880
lot of things in here that are probably

00:19:57,120 --> 00:20:01,440
too big and experimental for us to

00:19:58,880 --> 00:20:02,960
upstream like the jet or static python

00:20:01,440 --> 00:20:04,480
but there's a lot that the community

00:20:02,960 --> 00:20:06,240
could benefit from as well

00:20:04,480 --> 00:20:07,919
and it helped us out too because it'd be

00:20:06,240 --> 00:20:09,760
less for us to upgrade from version to

00:20:07,919 --> 00:20:11,039
version

00:20:09,760 --> 00:20:14,000
now let's go ahead and jump in and look

00:20:11,039 --> 00:20:15,919
at some performance results

00:20:14,000 --> 00:20:17,760
measuring our production improvements is

00:20:15,919 --> 00:20:18,640
kind of a little bit difficult we've

00:20:17,760 --> 00:20:20,240
seen lots of

00:20:18,640 --> 00:20:21,840
additive things you've seen all the

00:20:20,240 --> 00:20:22,640
percentages as we've gone through the

00:20:21,840 --> 00:20:24,320
slides

00:20:22,640 --> 00:20:26,320
but you can't just add them up so we

00:20:24,320 --> 00:20:27,600
estimate that's probably around a 20 to

00:20:26,320 --> 00:20:29,919
30 percent when

00:20:27,600 --> 00:20:32,880
maybe close to the 30 percent side in

00:20:29,919 --> 00:20:32,880
requests per second

00:20:32,960 --> 00:20:37,200
we can look at micro benchmarks or the

00:20:35,280 --> 00:20:39,840
pi performance benchmark suite

00:20:37,200 --> 00:20:40,960
and see kind of where our wins are

00:20:39,840 --> 00:20:42,799
overall

00:20:40,960 --> 00:20:44,880
um this isn't something that we usually

00:20:42,799 --> 00:20:47,760
do we're really a lot more focused

00:20:44,880 --> 00:20:48,640
on looking at the performance in

00:20:47,760 --> 00:20:50,400
production

00:20:48,640 --> 00:20:52,480
and so this is actually the first time

00:20:50,400 --> 00:20:55,200
that we've run the entire

00:20:52,480 --> 00:20:57,440
pi performance benchmark against sender

00:20:55,200 --> 00:20:59,280
and see python to see how they compare

00:20:57,440 --> 00:21:00,799
we do focus on a couple of random

00:20:59,280 --> 00:21:02,960
benchmarks

00:21:00,799 --> 00:21:05,360
and run those regularly those include

00:21:02,960 --> 00:21:07,360
richards and delta blue and shockingly

00:21:05,360 --> 00:21:08,159
those are the two that we do the best on

00:21:07,360 --> 00:21:12,320
here

00:21:08,159 --> 00:21:15,679
um but these numbers are uh

00:21:12,320 --> 00:21:18,880
baseline to see python so c python is

00:21:15,679 --> 00:21:19,360
1x over here on the right and a smaller

00:21:18,880 --> 00:21:22,400
bar

00:21:19,360 --> 00:21:25,200
represents a less runtime

00:21:22,400 --> 00:21:26,159
for that benchmark so you can see with

00:21:25,200 --> 00:21:29,440
richards

00:21:26,159 --> 00:21:31,840
it's close to a 4x win and

00:21:29,440 --> 00:21:32,960
kind of as we go through the benchmarks

00:21:31,840 --> 00:21:35,440
these are sorted

00:21:32,960 --> 00:21:36,400
by order of performance and something

00:21:35,440 --> 00:21:39,039
that we call

00:21:36,400 --> 00:21:39,440
the jet no frame mode and this is a mode

00:21:39,039 --> 00:21:41,840
where

00:21:39,440 --> 00:21:42,720
the jit runs but it doesn't use c python

00:21:41,840 --> 00:21:45,520
frames at all

00:21:42,720 --> 00:21:45,840
doesn't create them we're not yet using

00:21:45,520 --> 00:21:48,799
this

00:21:45,840 --> 00:21:49,440
in production there's still a few issues

00:21:48,799 --> 00:21:51,280
with it

00:21:49,440 --> 00:21:54,320
but this is ultimately the mode that we

00:21:51,280 --> 00:21:56,320
intend to run on long term

00:21:54,320 --> 00:21:58,080
the green bars here are just the normal

00:21:56,320 --> 00:21:58,799
cinder jit and so those actually have

00:21:58,080 --> 00:22:02,159
frames

00:21:58,799 --> 00:22:04,559
and the blue bars are just cinder so

00:22:02,159 --> 00:22:07,039
all the various optimizations excluding

00:22:04,559 --> 00:22:09,520
the jet

00:22:07,039 --> 00:22:10,320
these winds kind of continue through

00:22:09,520 --> 00:22:13,919
these

00:22:10,320 --> 00:22:16,000
other benchmarks and you can see that

00:22:13,919 --> 00:22:19,280
generally speaking the jit is a win

00:22:16,000 --> 00:22:19,760
sometimes cinder is a loss we haven't

00:22:19,280 --> 00:22:22,960
really

00:22:19,760 --> 00:22:22,960
dug into these yet

00:22:23,120 --> 00:22:26,240
and then there's a set of things that in

00:22:24,880 --> 00:22:29,679
general cinder

00:22:26,240 --> 00:22:30,960
is kind of losing out on startup time is

00:22:29,679 --> 00:22:32,799
a big one

00:22:30,960 --> 00:22:36,000
two to three we end up spending a whole

00:22:32,799 --> 00:22:38,159
bunch of time in legit for some reason

00:22:36,000 --> 00:22:39,679
and part of that may be that right now

00:22:38,159 --> 00:22:41,919
the jit is really

00:22:39,679 --> 00:22:43,440
the way we jit functions is tuned for

00:22:41,919 --> 00:22:45,200
our workload heavily

00:22:43,440 --> 00:22:47,120
and that in production we have a jit

00:22:45,200 --> 00:22:48,799
list that enables the functions that we

00:22:47,120 --> 00:22:49,440
want to compile and those get compiled

00:22:48,799 --> 00:22:52,640
in that

00:22:49,440 --> 00:22:54,880
master process but

00:22:52,640 --> 00:22:56,480
if we're not running in production we

00:22:54,880 --> 00:22:59,039
just jit every single function

00:22:56,480 --> 00:23:00,799
the first time it gets invoked and that

00:22:59,039 --> 00:23:02,159
isn't how you really wanted it to work

00:23:00,799 --> 00:23:04,640
in the real world

00:23:02,159 --> 00:23:06,080
um if you're only running a function

00:23:04,640 --> 00:23:08,400
once there's no reason to spend the

00:23:06,080 --> 00:23:09,919
overhead of jitting it so that would be

00:23:08,400 --> 00:23:10,640
an easy thing that would probably

00:23:09,919 --> 00:23:13,760
improve

00:23:10,640 --> 00:23:16,559
these numbers a lot

00:23:13,760 --> 00:23:18,640
another thing to dive into is just

00:23:16,559 --> 00:23:21,520
looking at a specific benchmark

00:23:18,640 --> 00:23:23,120
and in this case it's richards so what

00:23:21,520 --> 00:23:26,400
we have here is the run time

00:23:23,120 --> 00:23:29,600
on baseline see python 3.8

00:23:26,400 --> 00:23:31,360
the run time on normal sender the run

00:23:29,600 --> 00:23:33,200
time with the jit

00:23:31,360 --> 00:23:35,360
the runtime with the jit in new frame

00:23:33,200 --> 00:23:37,280
mode and finally we have a version of

00:23:35,360 --> 00:23:40,320
returns that has been

00:23:37,280 --> 00:23:41,039
annotated to run in static python and so

00:23:40,320 --> 00:23:43,600
you can see

00:23:41,039 --> 00:23:44,240
you know we end up with something like a

00:23:43,600 --> 00:23:47,760
8x

00:23:44,240 --> 00:23:50,240
win from the baseline so

00:23:47,760 --> 00:23:51,279
that is kind of quite a significant one

00:23:50,240 --> 00:23:53,279
overall

00:23:51,279 --> 00:23:55,520
so i mentioned another large source of

00:23:53,279 --> 00:23:58,480
wins for us was just our build changes

00:23:55,520 --> 00:24:00,320
and this slide kind of looks at that so

00:23:58,480 --> 00:24:01,679
initially we're just running the normal

00:24:00,320 --> 00:24:03,520
python pgo

00:24:01,679 --> 00:24:05,440
and a few things were missing from those

00:24:03,520 --> 00:24:06,960
runs one of those was running with our

00:24:05,440 --> 00:24:08,799
inline caching enabled

00:24:06,960 --> 00:24:10,720
and another was running with async io

00:24:08,799 --> 00:24:13,520
and co-routines so adding those

00:24:10,720 --> 00:24:14,400
in brought us up to a three percent win

00:24:13,520 --> 00:24:16,559
and then

00:24:14,400 --> 00:24:18,080
we were also missing out uh the same

00:24:16,559 --> 00:24:18,799
sort of things we would run the same

00:24:18,080 --> 00:24:22,240
training

00:24:18,799 --> 00:24:23,760
on the uski process and their see python

00:24:22,240 --> 00:24:26,480
gets statically embedded

00:24:23,760 --> 00:24:28,799
and so when we added inline caching and

00:24:26,480 --> 00:24:29,919
async io and core routines into the runs

00:24:28,799 --> 00:24:31,679
inside of there

00:24:29,919 --> 00:24:33,440
that brought us up to almost a six

00:24:31,679 --> 00:24:34,799
percent win so that was really kind of

00:24:33,440 --> 00:24:36,400
awesome as you can see this is a

00:24:34,799 --> 00:24:38,240
significant effort and it takes

00:24:36,400 --> 00:24:40,159
a lot of people to make it all happen

00:24:38,240 --> 00:24:41,039
and come to life it's great working with

00:24:40,159 --> 00:24:42,559
all these people

00:24:41,039 --> 00:24:47,600
and it's great to be able to talk about

00:24:42,559 --> 00:24:49,360
all their hard work

00:24:47,600 --> 00:24:51,440
if you want to check out sender it's now

00:24:49,360 --> 00:25:01,840
available in open source on github

00:24:51,440 --> 00:25:01,840
and of course we're hiring

00:26:01,360 --> 00:26:03,440

YouTube URL: https://www.youtube.com/watch?v=xGY45EmhwrE


