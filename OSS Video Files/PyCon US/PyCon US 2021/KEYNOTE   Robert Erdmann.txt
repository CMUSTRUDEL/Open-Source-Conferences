Title: KEYNOTE   Robert Erdmann
Publication date: 2021-05-28
Playlist: PyCon US 2021
Description: 
	Prior to earning his Ph.D. from the University of Arizona in 2006, Robert Erdmann started a science and engineering software company and worked extensively on solidification and multiscale transport modeling at Sandia National Laboratories. Upon graduation, he joined the faculty at the University of Arizona in the Department of Materials Science and Engineering and the Program in Applied Mathematics, where he worked on multiscale material process modeling and image processing for cultural heritage. In 2014 he moved permanently to Amsterdam to focus full-time on combining materials science, computer science, and imaging science to help the world access, preserve, and understand its cultural heritage. He is Senior Scientist at the Rijksmuseum, and is also Full Professor of Conservation Science in the Faculties of Science and of Humanities at the University of Amsterdam. He has been using Python since 2001 and teaching Python since 2006.
Captions: 
	00:00:04,400 --> 00:00:07,120
hi

00:00:04,720 --> 00:00:07,759
my name is rob erdmann i'm senior

00:00:07,120 --> 00:00:10,639
scientist

00:00:07,759 --> 00:00:12,080
at the reichsmuseum and a professor at

00:00:10,639 --> 00:00:14,480
the university of amsterdam

00:00:12,080 --> 00:00:16,320
and today i'm going to be giving you a

00:00:14,480 --> 00:00:19,439
behind the scenes tour

00:00:16,320 --> 00:00:21,039
of an effort to capture rembrandt's most

00:00:19,439 --> 00:00:23,519
famous painting the night watch

00:00:21,039 --> 00:00:24,960
at a resolution of 5 micrometers that

00:00:23,519 --> 00:00:28,640
will then result in

00:00:24,960 --> 00:00:30,080
an image that's 717 gigapixels and i'm

00:00:28,640 --> 00:00:32,559
going to be talking about

00:00:30,080 --> 00:00:34,239
how much python and the open source

00:00:32,559 --> 00:00:36,079
environment surrounding python has

00:00:34,239 --> 00:00:39,120
helped me to achieve this goal

00:00:36,079 --> 00:00:42,640
so here is the night watch

00:00:39,120 --> 00:00:45,120
it was painted by rembrandt in 1642 it's

00:00:42,640 --> 00:00:47,440
probably his most famous painting

00:00:45,120 --> 00:00:49,760
it shows a militia company that was

00:00:47,440 --> 00:00:52,239
charged with guarding amsterdam

00:00:49,760 --> 00:00:54,079
and the thing to understand about this

00:00:52,239 --> 00:00:57,520
is that it's really big

00:00:54,079 --> 00:01:01,359
it's 379

00:00:57,520 --> 00:01:05,840
centimeters tall 453 centimeters wide

00:01:01,359 --> 00:01:05,840
and it weighs 337 kilograms so

00:01:06,000 --> 00:01:10,479
if you prefer imperial units that's 12

00:01:09,119 --> 00:01:13,680
and a half feet tall

00:01:10,479 --> 00:01:15,680
almost 15 feet wide and 743 pounds so

00:01:13,680 --> 00:01:18,640
we're going to try to image this at

00:01:15,680 --> 00:01:19,200
incredible detail because this um is

00:01:18,640 --> 00:01:21,119
part of

00:01:19,200 --> 00:01:22,240
what's called operation night watch

00:01:21,119 --> 00:01:25,360
which is a multi-year

00:01:22,240 --> 00:01:27,680
project to perform very

00:01:25,360 --> 00:01:30,400
in-depth comprehensive research about

00:01:27,680 --> 00:01:32,000
the painting and its current condition

00:01:30,400 --> 00:01:34,799
and then to design a conservation

00:01:32,000 --> 00:01:38,320
treatment to ensure that going forward

00:01:34,799 --> 00:01:40,960
it looks its very best and that it stays

00:01:38,320 --> 00:01:42,720
beautiful for as long as possible

00:01:40,960 --> 00:01:46,320
indefinitely into the future

00:01:42,720 --> 00:01:48,079
um so this project um requires an

00:01:46,320 --> 00:01:50,640
incredibly large number of people

00:01:48,079 --> 00:01:51,680
and a huge amount of resources and so

00:01:50,640 --> 00:01:54,560
i'd like to give

00:01:51,680 --> 00:01:55,360
a special acknowledgement uh for all of

00:01:54,560 --> 00:01:58,479
our various

00:01:55,360 --> 00:01:59,280
uh generous partners and sponsors some

00:01:58,479 --> 00:02:01,840
of who

00:01:59,280 --> 00:02:03,840
which wish to remain anonymous without

00:02:01,840 --> 00:02:07,119
whom this project could not happen

00:02:03,840 --> 00:02:10,560
um i would also like to give thanks to

00:02:07,119 --> 00:02:11,840
uh wukash langa and the python community

00:02:10,560 --> 00:02:14,400
i was working in front of the night

00:02:11,840 --> 00:02:17,680
watch um using a jupiter notebook

00:02:14,400 --> 00:02:19,520
um wukash is one of the core developers

00:02:17,680 --> 00:02:21,200
for python was taking a tour

00:02:19,520 --> 00:02:23,680
through the reichson cm and he spotted

00:02:21,200 --> 00:02:26,080
that i was using jupiter and python

00:02:23,680 --> 00:02:27,040
and so he tweeted it and then this got

00:02:26,080 --> 00:02:28,480
picked up by

00:02:27,040 --> 00:02:30,160
many other members of the python

00:02:28,480 --> 00:02:33,599
community

00:02:30,160 --> 00:02:36,720
and um ultimately um

00:02:33,599 --> 00:02:40,080
it was spotted by fuido fun harassm

00:02:36,720 --> 00:02:40,879
who is dutch of course so it's kind of

00:02:40,080 --> 00:02:43,200
nice that

00:02:40,879 --> 00:02:43,920
um the city where python was invented is

00:02:43,200 --> 00:02:47,040
also

00:02:43,920 --> 00:02:50,560
on the subject of this talk so

00:02:47,040 --> 00:02:52,400
um if you're going to capture um

00:02:50,560 --> 00:02:54,959
an image of a painting to photograph the

00:02:52,400 --> 00:02:56,640
painting at this incredible resolution

00:02:54,959 --> 00:02:58,159
this is a really hard thing to do and

00:02:56,640 --> 00:02:59,200
the reason that it's so hard is that

00:02:58,159 --> 00:03:02,800
when you have

00:02:59,200 --> 00:03:04,000
a macro photography camera so you're

00:03:02,800 --> 00:03:06,959
going to get very very close

00:03:04,000 --> 00:03:09,280
to the painting and capture the whole

00:03:06,959 --> 00:03:10,879
painting in a grid of photos

00:03:09,280 --> 00:03:13,040
there's a very very narrow depth of

00:03:10,879 --> 00:03:14,480
field and so what this means is that

00:03:13,040 --> 00:03:16,159
if the painting is a little bit too far

00:03:14,480 --> 00:03:17,519
away your photo will be blurry and if

00:03:16,159 --> 00:03:19,120
it's a little bit too close

00:03:17,519 --> 00:03:20,720
it will be blurry so we have to have a

00:03:19,120 --> 00:03:23,840
very elaborate system

00:03:20,720 --> 00:03:25,599
that allows us to sense how the painting

00:03:23,840 --> 00:03:27,280
is situated relative to the camera and

00:03:25,599 --> 00:03:28,879
then to move the camera

00:03:27,280 --> 00:03:31,840
perfectly into place so that we can

00:03:28,879 --> 00:03:35,519
capture a photo that's

00:03:31,840 --> 00:03:37,840
perfectly sharp and color accurate

00:03:35,519 --> 00:03:39,280
so one thing to notice about this

00:03:37,840 --> 00:03:42,000
operation nightwatch

00:03:39,280 --> 00:03:43,040
campaign is that the entire thing both

00:03:42,000 --> 00:03:45,920
the research phase

00:03:43,040 --> 00:03:46,400
and the subsequent conservation

00:03:45,920 --> 00:03:48,480
treatment

00:03:46,400 --> 00:03:50,239
are carried out entirely in front of the

00:03:48,480 --> 00:03:52,319
public so

00:03:50,239 --> 00:03:53,760
um it's this painting is basically the

00:03:52,319 --> 00:03:56,640
centerpiece of the museum

00:03:53,760 --> 00:03:58,080
so it has to be visible at all times so

00:03:56,640 --> 00:04:01,040
you can see here is a

00:03:58,080 --> 00:04:02,720
glass house that's a sort of glass box

00:04:01,040 --> 00:04:04,560
that's built around the painting so that

00:04:02,720 --> 00:04:05,599
the researchers can carry out their work

00:04:04,560 --> 00:04:07,760
inside

00:04:05,599 --> 00:04:09,200
while the public can see them and

00:04:07,760 --> 00:04:10,400
although the museum is closed at the

00:04:09,200 --> 00:04:12,080
moment

00:04:10,400 --> 00:04:13,599
we're hoping that it will reopen shortly

00:04:12,080 --> 00:04:14,319
and then we can welcome the public back

00:04:13,599 --> 00:04:16,239
to watch

00:04:14,319 --> 00:04:17,919
all of our research um you'll notice

00:04:16,239 --> 00:04:18,799
here to the left and right of the

00:04:17,919 --> 00:04:22,000
painting are

00:04:18,799 --> 00:04:24,960
very large pair of vertical bars

00:04:22,000 --> 00:04:25,360
and then an eight meter long or 26 foot

00:04:24,960 --> 00:04:27,600
long

00:04:25,360 --> 00:04:29,360
beam and this beam can be moved up and

00:04:27,600 --> 00:04:31,520
down

00:04:29,360 --> 00:04:33,120
here is a view of that as a cad

00:04:31,520 --> 00:04:35,759
rendering

00:04:33,120 --> 00:04:37,440
and here's a person for a scale so on

00:04:35,759 --> 00:04:37,919
this beam which you see now below the

00:04:37,440 --> 00:04:40,720
painting

00:04:37,919 --> 00:04:41,120
there is a kind of sled that we can move

00:04:40,720 --> 00:04:44,639
left

00:04:41,120 --> 00:04:46,639
and right and here you see the back view

00:04:44,639 --> 00:04:48,240
of that so this bit here can move left

00:04:46,639 --> 00:04:49,680
and right and again the beam can move up

00:04:48,240 --> 00:04:51,520
and down

00:04:49,680 --> 00:04:53,280
and then if you zoom into that sled you

00:04:51,520 --> 00:04:55,440
can see that

00:04:53,280 --> 00:04:58,080
it is also quite elaborate we have

00:04:55,440 --> 00:05:00,720
places where we can attach the lights

00:04:58,080 --> 00:05:02,240
we can put an imaging laptop in this

00:05:00,720 --> 00:05:04,400
little cubby hole here we can put a

00:05:02,240 --> 00:05:07,600
power supply for our lights

00:05:04,400 --> 00:05:10,320
over here and um this

00:05:07,600 --> 00:05:12,639
bit is called the orbital frame this

00:05:10,320 --> 00:05:14,479
gives us three more degrees of freedom

00:05:12,639 --> 00:05:16,000
the first is we can move in the z

00:05:14,479 --> 00:05:17,199
direction which is closer and farther

00:05:16,000 --> 00:05:19,039
away from the painting

00:05:17,199 --> 00:05:20,639
we also have two rotational degrees of

00:05:19,039 --> 00:05:22,800
freedom so we can

00:05:20,639 --> 00:05:24,000
basically tilt our head left and right

00:05:22,800 --> 00:05:26,960
so that's rotation

00:05:24,000 --> 00:05:28,400
around the y axis or yaw we can also

00:05:26,960 --> 00:05:30,639
tilt the head up and down

00:05:28,400 --> 00:05:31,440
and that is rotation around the x-axis

00:05:30,639 --> 00:05:33,199
or pitch

00:05:31,440 --> 00:05:35,120
but the interesting thing about this is

00:05:33,199 --> 00:05:37,440
that we can rotate

00:05:35,120 --> 00:05:39,600
around the painting so we can change the

00:05:37,440 --> 00:05:41,520
direction that the camera is facing

00:05:39,600 --> 00:05:43,199
without looking at a different location

00:05:41,520 --> 00:05:46,240
on the painting

00:05:43,199 --> 00:05:47,840
so then on that orbital head we mount

00:05:46,240 --> 00:05:51,199
the world's highest resolution digital

00:05:47,840 --> 00:05:54,400
camera this is a hasselblad h6d

00:05:51,199 --> 00:05:58,319
camera and surrounding that

00:05:54,400 --> 00:06:01,360
is an array of distance sensors

00:05:58,319 --> 00:06:04,319
and and then ultimately

00:06:01,360 --> 00:06:04,319
we also have

00:06:04,560 --> 00:06:08,080
the rendering of the imaging location

00:06:07,600 --> 00:06:09,360
here

00:06:08,080 --> 00:06:11,120
which when viewed from the painting

00:06:09,360 --> 00:06:12,639
outward at the camera shows

00:06:11,120 --> 00:06:15,440
um the region that we're going to be

00:06:12,639 --> 00:06:17,120
imaging so this one photo which is 100

00:06:15,440 --> 00:06:19,280
megapixels is going to capture this

00:06:17,120 --> 00:06:21,039
rectangle here which is

00:06:19,280 --> 00:06:24,160
only five and a half centimeters wide so

00:06:21,039 --> 00:06:24,160
that's about two inches wide

00:06:25,039 --> 00:06:31,600
so here is a picture for scale

00:06:28,560 --> 00:06:34,560
where i'm reaching up and then

00:06:31,600 --> 00:06:36,960
here is the view of the um the power

00:06:34,560 --> 00:06:40,960
supply the pair of lights the camera

00:06:36,960 --> 00:06:44,160
the imaging laptop a webcam

00:06:40,960 --> 00:06:46,880
that's over here and then um

00:06:44,160 --> 00:06:48,560
we also have a laser sensor that's down

00:06:46,880 --> 00:06:50,080
here

00:06:48,560 --> 00:06:51,759
so this is the hardware that we have to

00:06:50,080 --> 00:06:54,400
work with

00:06:51,759 --> 00:06:55,919
so next i'm going to describe the

00:06:54,400 --> 00:06:57,440
architecture of the system that

00:06:55,919 --> 00:06:59,680
coordinates all of the motion of the

00:06:57,440 --> 00:07:02,080
camera and the captures and so on

00:06:59,680 --> 00:07:03,440
so here you see it illustrated with a

00:07:02,080 --> 00:07:06,960
big block diagram here

00:07:03,440 --> 00:07:08,880
the system has five main parts

00:07:06,960 --> 00:07:10,400
central to all this is the control

00:07:08,880 --> 00:07:14,479
subsystem and that's what

00:07:10,400 --> 00:07:15,759
is coordinating the motion of the camera

00:07:14,479 --> 00:07:17,840
and all the sensors

00:07:15,759 --> 00:07:19,120
in the imaging frame subsystem it

00:07:17,840 --> 00:07:22,479
coordinates

00:07:19,120 --> 00:07:24,000
the command to take photos

00:07:22,479 --> 00:07:26,479
which is done over here by the image

00:07:24,000 --> 00:07:29,520
capture laptop

00:07:26,479 --> 00:07:31,980
it writes its log files in

00:07:29,520 --> 00:07:33,759
real-time telemetry

00:07:31,980 --> 00:07:35,680
[Music]

00:07:33,759 --> 00:07:38,240
which is receiving from the imaging fram

00:07:35,680 --> 00:07:42,240
subsystem into a high capacity nas

00:07:38,240 --> 00:07:44,000
which is the fifth fourth part rather

00:07:42,240 --> 00:07:45,680
this is also where we receive all the

00:07:44,000 --> 00:07:47,440
high-resolution images

00:07:45,680 --> 00:07:48,720
um and then there's the computer system

00:07:47,440 --> 00:07:50,639
that i'm sitting in front of

00:07:48,720 --> 00:07:51,919
um and that's illustrated in blue down

00:07:50,639 --> 00:07:56,479
here um

00:07:51,919 --> 00:07:56,479
this is uh mostly um

00:07:56,560 --> 00:08:00,879
the the the connection that i'm using is

00:07:59,520 --> 00:08:04,000
is a jupiter notebook

00:08:00,879 --> 00:08:06,879
that gives me a real-time control

00:08:04,000 --> 00:08:08,960
with status and control widgets that are

00:08:06,879 --> 00:08:10,240
all running asynchronously talking up

00:08:08,960 --> 00:08:13,840
here

00:08:10,240 --> 00:08:16,240
um so this is the appearance of the

00:08:13,840 --> 00:08:19,039
control system this is all done with ipi

00:08:16,240 --> 00:08:20,160
widgets um and the nice thing is that

00:08:19,039 --> 00:08:23,840
the jupyter notebook

00:08:20,160 --> 00:08:26,639
is is all running inside an async io

00:08:23,840 --> 00:08:28,560
event loops so i can have other cells in

00:08:26,639 --> 00:08:31,599
which i'm doing my work

00:08:28,560 --> 00:08:32,320
while seeing real-time status updates

00:08:31,599 --> 00:08:34,560
about the

00:08:32,320 --> 00:08:36,560
velocities and positions of all of the

00:08:34,560 --> 00:08:38,000
motors and everything else

00:08:36,560 --> 00:08:39,680
so here's a picture of what that looks

00:08:38,000 --> 00:08:43,279
like in use

00:08:39,680 --> 00:08:46,560
so here you see side by side my use of

00:08:43,279 --> 00:08:47,360
jupiter with this ipi widgets control

00:08:46,560 --> 00:08:49,279
system

00:08:47,360 --> 00:08:51,040
here's the view from the webcam that's

00:08:49,279 --> 00:08:52,720
riding along behind the camera

00:08:51,040 --> 00:08:55,680
and then here is a view of the latest

00:08:52,720 --> 00:08:58,880
high resolution image that i've captured

00:08:55,680 --> 00:09:00,080
so to describe all of the working parts

00:08:58,880 --> 00:09:02,320
here i'll just

00:09:00,080 --> 00:09:03,760
describe the process by which a single

00:09:02,320 --> 00:09:06,160
photo was taken

00:09:03,760 --> 00:09:08,880
so to begin with i'm sitting in front of

00:09:06,160 --> 00:09:11,760
the laptop down here and i will command

00:09:08,880 --> 00:09:12,640
to take a photo at a particular location

00:09:11,760 --> 00:09:16,880
so

00:09:12,640 --> 00:09:20,480
that location is then translated into

00:09:16,880 --> 00:09:22,240
a region on a previously made 3d

00:09:20,480 --> 00:09:23,519
capture laser capture that i'll describe

00:09:22,240 --> 00:09:27,200
later

00:09:23,519 --> 00:09:28,880
so that the control system then

00:09:27,200 --> 00:09:31,040
does a robust estimation with

00:09:28,880 --> 00:09:32,959
scikit-learn of the approximate

00:09:31,040 --> 00:09:34,080
distance and orientation of that patch

00:09:32,959 --> 00:09:36,240
of the painting

00:09:34,080 --> 00:09:39,360
and then it communicates to the imaging

00:09:36,240 --> 00:09:43,120
frame subsystem

00:09:39,360 --> 00:09:45,519
uh via an async io connection which has

00:09:43,120 --> 00:09:47,680
web sockets and then over web sockets

00:09:45,519 --> 00:09:50,720
for using protocol buffers

00:09:47,680 --> 00:09:55,200
in order to tell the imaging frame

00:09:50,720 --> 00:09:59,200
to pre-position the angle and

00:09:55,200 --> 00:10:01,360
distance of the camera to the painting

00:09:59,200 --> 00:10:02,800
and then when it arrives in place it

00:10:01,360 --> 00:10:04,959
performs

00:10:02,800 --> 00:10:05,839
a set of sensing with this high

00:10:04,959 --> 00:10:08,640
precision

00:10:05,839 --> 00:10:10,160
hybrid image laser rangefinder so it

00:10:08,640 --> 00:10:11,120
gets to the approximate position and

00:10:10,160 --> 00:10:14,240
then you see here

00:10:11,120 --> 00:10:15,760
laser scan this then allows me to record

00:10:14,240 --> 00:10:18,800
the positions

00:10:15,760 --> 00:10:20,800
um the distance to the painting at 15

00:10:18,800 --> 00:10:22,880
positions which have been very carefully

00:10:20,800 --> 00:10:26,320
placed to cover the field of view

00:10:22,880 --> 00:10:27,839
of the main hasselblad camera that then

00:10:26,320 --> 00:10:31,360
gives me a set of 15

00:10:27,839 --> 00:10:35,360
distance readings which get aggregated

00:10:31,360 --> 00:10:37,920
here into the telemetry bundle

00:10:35,360 --> 00:10:38,959
and and sent back to the control

00:10:37,920 --> 00:10:40,720
subsystem

00:10:38,959 --> 00:10:42,640
which again uses this set of 15

00:10:40,720 --> 00:10:44,880
measurements to estimate the shape of

00:10:42,640 --> 00:10:47,440
the surface very precisely

00:10:44,880 --> 00:10:49,279
and then knowing the region of space

00:10:47,440 --> 00:10:50,399
that will be perfectly sharp in front of

00:10:49,279 --> 00:10:52,320
the camera

00:10:50,399 --> 00:10:53,519
we make a final adjustment to position

00:10:52,320 --> 00:10:55,360
the camera

00:10:53,519 --> 00:10:56,880
so that the painting is in that

00:10:55,360 --> 00:10:58,959
so-called depth of field

00:10:56,880 --> 00:11:00,000
so now we're positioned to take what we

00:10:58,959 --> 00:11:03,519
think will be a very sharp

00:11:00,000 --> 00:11:06,560
photo and then the next step is

00:11:03,519 --> 00:11:10,000
via pi zmq a

00:11:06,560 --> 00:11:12,959
connection to the image capture laptop

00:11:10,000 --> 00:11:15,920
that's riding along with the frame

00:11:12,959 --> 00:11:17,680
that is um mainly controlled by this um

00:11:15,920 --> 00:11:19,839
python process here

00:11:17,680 --> 00:11:20,800
we're talking via pi zmq and there's an

00:11:19,839 --> 00:11:24,000
async io

00:11:20,800 --> 00:11:26,240
event loop here so what happens

00:11:24,000 --> 00:11:28,000
over here is that now we've received a

00:11:26,240 --> 00:11:31,120
command to take a picture

00:11:28,000 --> 00:11:34,399
but the problem is that the software

00:11:31,120 --> 00:11:35,920
that connects to the camera sort of the

00:11:34,399 --> 00:11:38,720
tether software which is called

00:11:35,920 --> 00:11:40,399
focus by hasselblad doesn't have an api

00:11:38,720 --> 00:11:40,959
so we can't just tell it to take a

00:11:40,399 --> 00:11:45,839
picture

00:11:40,959 --> 00:11:48,959
instead we have to use a jython based

00:11:45,839 --> 00:11:52,320
open source software called seculi x

00:11:48,959 --> 00:11:56,880
to basically do screen reading

00:11:52,320 --> 00:11:59,839
via opencv into the um

00:11:56,880 --> 00:12:00,639
uh focus software so it can see where

00:11:59,839 --> 00:12:02,320
the buttons are

00:12:00,639 --> 00:12:04,000
and i can see where the dialogs are and

00:12:02,320 --> 00:12:07,680
so on and so

00:12:04,000 --> 00:12:08,480
um this will then tell the camera to put

00:12:07,680 --> 00:12:13,200
the mirror up

00:12:08,480 --> 00:12:16,000
tell the camera to um to capture a photo

00:12:13,200 --> 00:12:18,079
and um interact with dialog boxes for

00:12:16,000 --> 00:12:21,519
file naming and these kinds of things

00:12:18,079 --> 00:12:24,959
um and then meanwhile

00:12:21,519 --> 00:12:27,040
this system um is waiting for

00:12:24,959 --> 00:12:28,959
a raw image to show up on the local

00:12:27,040 --> 00:12:32,720
storage of the capture laptop

00:12:28,959 --> 00:12:36,800
so then um the next step is that we use

00:12:32,720 --> 00:12:39,440
pyvips and pytorch to load the raw image

00:12:36,800 --> 00:12:41,519
and there's a neural network that's

00:12:39,440 --> 00:12:42,880
basically tasked with assessing the

00:12:41,519 --> 00:12:45,519
focus quality

00:12:42,880 --> 00:12:46,000
of the image and to to to ensure that

00:12:45,519 --> 00:12:48,959
the

00:12:46,000 --> 00:12:49,600
flashes have both gone off um and then

00:12:48,959 --> 00:12:52,880
if the

00:12:49,600 --> 00:12:55,519
image is good quality then

00:12:52,880 --> 00:12:57,200
focus is told to develop the image which

00:12:55,519 --> 00:12:58,880
means that it has to apply a color

00:12:57,200 --> 00:13:00,560
profile which will ensure that we have

00:12:58,880 --> 00:13:01,200
perfect color management for the whole

00:13:00,560 --> 00:13:03,760
thing

00:13:01,200 --> 00:13:04,720
and when the image is developed this

00:13:03,760 --> 00:13:07,600
process

00:13:04,720 --> 00:13:08,720
will shovel it down to the high capacity

00:13:07,600 --> 00:13:12,800
nas

00:13:08,720 --> 00:13:14,720
and report back to the central

00:13:12,800 --> 00:13:15,920
control subsystem that the capture was

00:13:14,720 --> 00:13:18,839
successful

00:13:15,920 --> 00:13:20,320
and then we can move on to the next

00:13:18,839 --> 00:13:22,000
location

00:13:20,320 --> 00:13:23,440
if the capture was not successful then

00:13:22,000 --> 00:13:25,440
we take it again

00:13:23,440 --> 00:13:27,120
after repositioning the camera because

00:13:25,440 --> 00:13:29,120
um there's sometimes wind on the

00:13:27,120 --> 00:13:32,000
painting just air currents in the room

00:13:29,120 --> 00:13:33,920
and it's a 17.2 square meter sail in a

00:13:32,000 --> 00:13:36,240
way so it can move away and closer to

00:13:33,920 --> 00:13:38,639
the camera

00:13:36,240 --> 00:13:41,120
so the first bit of that system working

00:13:38,639 --> 00:13:44,399
was the idea that i had a

00:13:41,120 --> 00:13:45,600
low resolution 3d map of the shape of

00:13:44,399 --> 00:13:48,000
the painting

00:13:45,600 --> 00:13:49,440
so for this again sci pi is an integral

00:13:48,000 --> 00:13:52,079
part

00:13:49,440 --> 00:13:54,320
both scipy.spatial and all of its

00:13:52,079 --> 00:13:56,959
algorithms kd trees and so on

00:13:54,320 --> 00:13:59,199
as well as data shader to help make nice

00:13:56,959 --> 00:14:01,600
visualizations

00:13:59,199 --> 00:14:03,360
so here you see the system without a

00:14:01,600 --> 00:14:05,839
camera and you see these five

00:14:03,360 --> 00:14:07,279
laser rangefinders that are positioned

00:14:05,839 --> 00:14:09,040
around one two three four and then

00:14:07,279 --> 00:14:11,920
there's another one here in the corner

00:14:09,040 --> 00:14:13,279
so what i can do one time only is i

00:14:11,920 --> 00:14:14,800
drive that

00:14:13,279 --> 00:14:16,399
up and down and left and right across

00:14:14,800 --> 00:14:19,120
the painting and gather around

00:14:16,399 --> 00:14:19,600
20 million points it's very very noisy

00:14:19,120 --> 00:14:21,279
these

00:14:19,600 --> 00:14:22,720
these points but i now have a point

00:14:21,279 --> 00:14:25,920
cloud

00:14:22,720 --> 00:14:28,959
which i can then aggregate using

00:14:25,920 --> 00:14:31,440
pandas and scipy.spatial

00:14:28,959 --> 00:14:33,360
and visualize using data shader and

00:14:31,440 --> 00:14:36,480
ultimately get a nice

00:14:33,360 --> 00:14:39,519
a nice 3d map

00:14:36,480 --> 00:14:40,639
that then shows us something like this

00:14:39,519 --> 00:14:42,959
so this has been

00:14:40,639 --> 00:14:44,800
vertically exaggerated by a factor of 10

00:14:42,959 --> 00:14:46,399
but as you can see there's ripples along

00:14:44,800 --> 00:14:48,240
the left and right sides of the painting

00:14:46,399 --> 00:14:50,240
and the painting bulges out as you might

00:14:48,240 --> 00:14:52,560
expect for something that's so heavy and

00:14:50,240 --> 00:14:55,120
of course it's it's hanging vertically

00:14:52,560 --> 00:14:56,880
so with this i have a sense of the

00:14:55,120 --> 00:14:59,040
approximate 3d shape of the painting so

00:14:56,880 --> 00:15:00,560
that i can get it into position when i'm

00:14:59,040 --> 00:15:03,519
ready

00:15:00,560 --> 00:15:05,680
and then the next necessary piece for me

00:15:03,519 --> 00:15:09,120
to capture a grid of photos

00:15:05,680 --> 00:15:13,279
is that i have to have a good knowledge

00:15:09,120 --> 00:15:16,880
of the location of

00:15:13,279 --> 00:15:19,440
the depth of field of the camera um so

00:15:16,880 --> 00:15:21,600
to to be more precise um here's a

00:15:19,440 --> 00:15:24,720
schematic of the camera and

00:15:21,600 --> 00:15:26,320
um with an approximately to scale

00:15:24,720 --> 00:15:27,600
representation of how far away it will

00:15:26,320 --> 00:15:29,360
be from the painting

00:15:27,600 --> 00:15:31,519
so when you set the camera on a given

00:15:29,360 --> 00:15:33,600
aperture and you've

00:15:31,519 --> 00:15:35,759
focused it so that you will be capturing

00:15:33,600 --> 00:15:39,120
five micron resolution photos

00:15:35,759 --> 00:15:42,480
then um what you find is

00:15:39,120 --> 00:15:44,079
uh that if you were to

00:15:42,480 --> 00:15:45,839
try to capture a photo of an object

00:15:44,079 --> 00:15:48,079
that's too close it would be

00:15:45,839 --> 00:15:49,600
not sharp if it's too far away it would

00:15:48,079 --> 00:15:50,720
be not sharp

00:15:49,600 --> 00:15:52,720
and then there's a little sort of

00:15:50,720 --> 00:15:55,519
goldilocks zone here

00:15:52,720 --> 00:15:55,920
called the depth of field within which

00:15:55,519 --> 00:15:58,240
the

00:15:55,920 --> 00:15:59,120
image would be acceptably sharp so

00:15:58,240 --> 00:16:01,120
there's one

00:15:59,120 --> 00:16:03,040
specific distance that would at which it

00:16:01,120 --> 00:16:06,079
is the sharpest it can possibly be

00:16:03,040 --> 00:16:07,839
now the problem is that i don't know

00:16:06,079 --> 00:16:09,440
where this is relative to the camera so

00:16:07,839 --> 00:16:12,959
i have to measure it

00:16:09,440 --> 00:16:14,639
and for that i've developed a technique

00:16:12,959 --> 00:16:18,160
that uses

00:16:14,639 --> 00:16:21,279
a procedure called shape from focus

00:16:18,160 --> 00:16:24,560
so the idea is that i'm going to capture

00:16:21,279 --> 00:16:26,160
a depth stack i'm i'm going to take a

00:16:24,560 --> 00:16:27,920
series of photos where the camera is

00:16:26,160 --> 00:16:30,959
closer and closer and closer

00:16:27,920 --> 00:16:32,959
to the painting without adjusting the

00:16:30,959 --> 00:16:35,680
focus of the camera at all

00:16:32,959 --> 00:16:36,560
and when i do this when the camera is

00:16:35,680 --> 00:16:40,079
too far away

00:16:36,560 --> 00:16:41,839
from this flat calibration plate

00:16:40,079 --> 00:16:43,839
uh which is designed to be very

00:16:41,839 --> 00:16:46,880
reflective and very very flat

00:16:43,839 --> 00:16:48,480
then the whole image will be blurry

00:16:46,880 --> 00:16:50,079
and then as i'm moving the camera

00:16:48,480 --> 00:16:50,639
forward and forward and forward taking a

00:16:50,079 --> 00:16:52,720
photo

00:16:50,639 --> 00:16:54,000
at every step there will come a moment

00:16:52,720 --> 00:16:56,800
when

00:16:54,000 --> 00:16:57,519
part of the plate is sharp as you see

00:16:56,800 --> 00:17:00,959
here

00:16:57,519 --> 00:17:01,279
the depth of field and the calibration

00:17:00,959 --> 00:17:03,440
plate

00:17:01,279 --> 00:17:04,640
intersect and so here the bottom of the

00:17:03,440 --> 00:17:05,919
photo would be sharp

00:17:04,640 --> 00:17:08,240
and then if i move the camera still

00:17:05,919 --> 00:17:10,000
closer and this cartoon

00:17:08,240 --> 00:17:11,919
the middle of the photo is sharp and

00:17:10,000 --> 00:17:13,439
then later

00:17:11,919 --> 00:17:14,959
even closer the top of the photo is

00:17:13,439 --> 00:17:17,839
sharp

00:17:14,959 --> 00:17:19,360
so if i would um monitor the overall

00:17:17,839 --> 00:17:20,959
sharpness

00:17:19,360 --> 00:17:22,959
um then it would look something like

00:17:20,959 --> 00:17:24,240
this so how do you monitor the sharpness

00:17:22,959 --> 00:17:28,319
of the painting um

00:17:24,240 --> 00:17:31,360
here again i have a pie torch a stack of

00:17:28,319 --> 00:17:34,480
convolution filters with non-linearities

00:17:31,360 --> 00:17:38,720
that has been trained to estimate the

00:17:34,480 --> 00:17:41,120
local sharpness so this outputs a number

00:17:38,720 --> 00:17:42,880
for each patch that i put in so i cut up

00:17:41,120 --> 00:17:46,240
the image into patches

00:17:42,880 --> 00:17:48,400
and for each patch i see that it's

00:17:46,240 --> 00:17:49,760
blurry and then it will come into focus

00:17:48,400 --> 00:17:50,480
as i'm moving the camera closer and

00:17:49,760 --> 00:17:53,840
closer

00:17:50,480 --> 00:17:56,640
and then it becomes blurry again

00:17:53,840 --> 00:17:57,039
and so if i map that across the entirety

00:17:56,640 --> 00:18:00,559
of

00:17:57,039 --> 00:18:03,280
the of the image

00:18:00,559 --> 00:18:05,520
um of the of the field of view across in

00:18:03,280 --> 00:18:08,080
this case 41 different photos

00:18:05,520 --> 00:18:10,160
then i have a sense of the distance at

00:18:08,080 --> 00:18:13,440
which this would be the most sharp

00:18:10,160 --> 00:18:17,039
and ultimately then this

00:18:13,440 --> 00:18:19,039
can be fit um using scikit-learn

00:18:17,039 --> 00:18:20,960
with a with a plane with robust

00:18:19,039 --> 00:18:23,440
estimation for example bransac

00:18:20,960 --> 00:18:25,280
estimation um to tell me exactly the

00:18:23,440 --> 00:18:28,480
region of space that is

00:18:25,280 --> 00:18:32,640
in focus and

00:18:28,480 --> 00:18:35,200
and then i can position the camera

00:18:32,640 --> 00:18:36,400
as shown in this in this middle image

00:18:35,200 --> 00:18:39,039
here

00:18:36,400 --> 00:18:39,760
so that the entirety of this calibration

00:18:39,039 --> 00:18:42,320
plate is

00:18:39,760 --> 00:18:43,600
sharp so i have just the right angles

00:18:42,320 --> 00:18:45,039
and just the right distance so the

00:18:43,600 --> 00:18:48,000
entire plate is sharp

00:18:45,039 --> 00:18:50,320
and at that moment i can use this laser

00:18:48,000 --> 00:18:52,960
rangefinder to capture those 15

00:18:50,320 --> 00:18:55,360
distances across the field of view

00:18:52,960 --> 00:18:56,000
that would look something like this so

00:18:55,360 --> 00:18:59,360
the calibration

00:18:56,000 --> 00:19:00,960
plate is white and then i can set that

00:18:59,360 --> 00:19:04,799
as the datum

00:19:00,960 --> 00:19:08,000
meaning the geometry that exists

00:19:04,799 --> 00:19:10,160
here where the plate is perfectly sharp

00:19:08,000 --> 00:19:12,240
is the same relative geometry that i

00:19:10,160 --> 00:19:15,360
would like to establish in the painting

00:19:12,240 --> 00:19:17,440
for when i park the

00:19:15,360 --> 00:19:19,600
camera in front of the painting i don't

00:19:17,440 --> 00:19:21,360
have the time to take 41 photos at every

00:19:19,600 --> 00:19:25,320
location because i have

00:19:21,360 --> 00:19:28,559
97 rows 87 columns which is 8

00:19:25,320 --> 00:19:30,240
439 100 megapixel photos that i have to

00:19:28,559 --> 00:19:33,520
capture

00:19:30,240 --> 00:19:35,760
so what i do is i learn how to become

00:19:33,520 --> 00:19:37,280
sharp in front of the calibration plate

00:19:35,760 --> 00:19:39,200
and then i will use these distance

00:19:37,280 --> 00:19:41,679
sensors to re-establish that same

00:19:39,200 --> 00:19:43,039
geometry at every location

00:19:41,679 --> 00:19:45,039
of the painting so i'm always at the

00:19:43,039 --> 00:19:47,440
right distance and i'm always at the

00:19:45,039 --> 00:19:48,480
at the right angle so that the whole

00:19:47,440 --> 00:19:50,559
field of view

00:19:48,480 --> 00:19:53,840
will have the painting inside this depth

00:19:50,559 --> 00:19:53,840
of field

00:19:54,559 --> 00:20:00,240
and then the last step

00:19:57,600 --> 00:20:02,240
to make this whole thing work is that i

00:20:00,240 --> 00:20:04,159
have to be able to

00:20:02,240 --> 00:20:06,159
capture the colors that exist in the

00:20:04,159 --> 00:20:11,280
painting very very accurately

00:20:06,159 --> 00:20:14,480
so i have light sources that may

00:20:11,280 --> 00:20:16,400
give an unwanted tint to the painting

00:20:14,480 --> 00:20:18,159
and of course the camera sensor although

00:20:16,400 --> 00:20:19,840
it's really excellent it may also have

00:20:18,159 --> 00:20:21,280
quirks about how sensitive it is in

00:20:19,840 --> 00:20:22,960
different wavelength bands

00:20:21,280 --> 00:20:25,520
so this means i have to do color

00:20:22,960 --> 00:20:27,120
calibration python to the rescue again

00:20:25,520 --> 00:20:30,400
here there's a really excellent library

00:20:27,120 --> 00:20:33,120
called the color science library

00:20:30,400 --> 00:20:35,360
and i'm also using a non-python piece of

00:20:33,120 --> 00:20:36,559
software but it is open source a color

00:20:35,360 --> 00:20:39,679
management system called

00:20:36,559 --> 00:20:42,080
argyle so to do

00:20:39,679 --> 00:20:42,720
um color calibration what we do is we

00:20:42,080 --> 00:20:44,880
get a

00:20:42,720 --> 00:20:46,400
color card like you see here being very

00:20:44,880 --> 00:20:48,720
meticulously cleaned

00:20:46,400 --> 00:20:50,080
by leila savage one of our conservation

00:20:48,720 --> 00:20:53,440
scientists

00:20:50,080 --> 00:20:55,200
then we mount this next to the painting

00:20:53,440 --> 00:20:57,360
in the same geometry as the painting and

00:20:55,200 --> 00:21:00,720
we use our imaging system to capture

00:20:57,360 --> 00:21:03,120
this very accurately now before this

00:21:00,720 --> 00:21:05,520
we've also done spectrophotometry so we

00:21:03,120 --> 00:21:08,960
know the exact reflectance curve

00:21:05,520 --> 00:21:10,080
of each of these patches so we then have

00:21:08,960 --> 00:21:12,960
a ground truth

00:21:10,080 --> 00:21:15,039
the true color of each patch and we have

00:21:12,960 --> 00:21:16,480
the color of the patch as it occurred in

00:21:15,039 --> 00:21:19,360
our photo so

00:21:16,480 --> 00:21:20,000
what we need is to find out how to adapt

00:21:19,360 --> 00:21:23,360
a photo

00:21:20,000 --> 00:21:26,880
as captured so that the colors

00:21:23,360 --> 00:21:28,720
are what they should be so here's a

00:21:26,880 --> 00:21:33,039
visualization of that

00:21:28,720 --> 00:21:36,240
using seaborne in which i'm seeing

00:21:33,039 --> 00:21:39,039
the ground truth and the as captured

00:21:36,240 --> 00:21:41,440
color for each of those 140 patches so

00:21:39,039 --> 00:21:45,039
you can see as captured it's not great

00:21:41,440 --> 00:21:45,760
then we use argyle to build a color

00:21:45,039 --> 00:21:48,159
profile

00:21:45,760 --> 00:21:48,799
and this color profile will take as

00:21:48,159 --> 00:21:52,240
captured

00:21:48,799 --> 00:21:52,880
colors and bring them into alignment

00:21:52,240 --> 00:21:56,080
with

00:21:52,880 --> 00:21:58,080
the proper colors and then i can use the

00:21:56,080 --> 00:22:01,039
color science library to calculate

00:21:58,080 --> 00:22:03,039
the so-called delta e value that's the

00:22:01,039 --> 00:22:04,000
human perceptual color difference

00:22:03,039 --> 00:22:07,120
between

00:22:04,000 --> 00:22:08,159
what was captured and what i should have

00:22:07,120 --> 00:22:10,240
had

00:22:08,159 --> 00:22:12,880
and these are numbers here where a value

00:22:10,240 --> 00:22:15,840
of less than about 2.3 is not

00:22:12,880 --> 00:22:16,880
detectable by humans so again i'm using

00:22:15,840 --> 00:22:20,320
seaborn here

00:22:16,880 --> 00:22:22,080
and then i can visualize the

00:22:20,320 --> 00:22:23,760
corrected colors to see how well they're

00:22:22,080 --> 00:22:26,559
matching the true colors and so

00:22:23,760 --> 00:22:28,400
ultimately we also make a lot of

00:22:26,559 --> 00:22:30,320
statistical plots that show us things

00:22:28,400 --> 00:22:31,440
like here's a rug plot showing the delta

00:22:30,320 --> 00:22:33,440
e values

00:22:31,440 --> 00:22:35,440
and affirming that all of the colors are

00:22:33,440 --> 00:22:38,559
less than

00:22:35,440 --> 00:22:40,320
a 2.3 of delta e to be

00:22:38,559 --> 00:22:42,559
archival quality you have to have the

00:22:40,320 --> 00:22:44,640
maximum delta e less than 10

00:22:42,559 --> 00:22:46,080
and the average delta e less than four

00:22:44,640 --> 00:22:48,799
so you can see we're doing a really good

00:22:46,080 --> 00:22:51,919
job with color management

00:22:48,799 --> 00:22:54,799
so these pieces

00:22:51,919 --> 00:22:55,919
allow me to drive all over the entire

00:22:54,799 --> 00:22:59,200
painting

00:22:55,919 --> 00:23:01,919
capturing 97 rows and 87 columns

00:22:59,200 --> 00:23:03,280
of of the painting each photo is 100

00:23:01,919 --> 00:23:06,640
megapixels

00:23:03,280 --> 00:23:08,960
600 megabytes on disk and it

00:23:06,640 --> 00:23:10,240
the entire process of moving into place

00:23:08,960 --> 00:23:12,559
um sensing the distance

00:23:10,240 --> 00:23:13,440
adjusting taking a photo checking its

00:23:12,559 --> 00:23:16,480
quality

00:23:13,440 --> 00:23:18,640
um takes about 23 seconds

00:23:16,480 --> 00:23:21,919
so a little bit less than three photos

00:23:18,640 --> 00:23:24,000
per minute and i have 8439 photos so now

00:23:21,919 --> 00:23:26,799
let's suppose i've collected

00:23:24,000 --> 00:23:28,640
that entire grid well now i need to

00:23:26,799 --> 00:23:31,600
somehow stitch them together

00:23:28,640 --> 00:23:32,720
but i need a ground truth for the shape

00:23:31,600 --> 00:23:35,600
of the painting

00:23:32,720 --> 00:23:36,320
so these high resolution image images

00:23:35,600 --> 00:23:38,000
have to be

00:23:36,320 --> 00:23:39,919
hanging on something where i know what

00:23:38,000 --> 00:23:43,520
the true shape of the painting is

00:23:39,919 --> 00:23:47,039
so to do that again python is a big help

00:23:43,520 --> 00:23:49,279
here you can see that before it was

00:23:47,039 --> 00:23:51,200
installed in the glass house

00:23:49,279 --> 00:23:53,520
we secured the painting very securely

00:23:51,200 --> 00:23:57,679
and photographed it from

00:23:53,520 --> 00:24:01,440
across across a long hall the the main

00:23:57,679 --> 00:24:03,919
aisle of the gallery of honor and

00:24:01,440 --> 00:24:06,400
before we did that we put a series of

00:24:03,919 --> 00:24:07,919
four fiducial marks

00:24:06,400 --> 00:24:10,159
on the corners so you can kind of see

00:24:07,919 --> 00:24:12,240
these little white dots here but

00:24:10,159 --> 00:24:13,919
zooming in so this is basically just a

00:24:12,240 --> 00:24:15,440
sticker that's placed outside

00:24:13,919 --> 00:24:18,000
anything that was that was painted by

00:24:15,440 --> 00:24:20,240
rembrandt which has a little target

00:24:18,000 --> 00:24:21,440
on it so that we can identify it in the

00:24:20,240 --> 00:24:24,480
image and also

00:24:21,440 --> 00:24:26,559
pins so what we can do then is

00:24:24,480 --> 00:24:29,679
on the photo we can measure the exact

00:24:26,559 --> 00:24:29,679
coordinates of each of those

00:24:30,559 --> 00:24:36,159
each of those four fiducial marks um

00:24:33,679 --> 00:24:38,480
and then we physically measure the

00:24:36,159 --> 00:24:39,919
pairwise distance between each of the

00:24:38,480 --> 00:24:43,039
pairs of corners

00:24:39,919 --> 00:24:44,320
so all four sides and then two diagonals

00:24:43,039 --> 00:24:46,320
basically

00:24:44,320 --> 00:24:49,679
so then this is an optimization problem

00:24:46,320 --> 00:24:52,320
so scipy.optimize.least sq

00:24:49,679 --> 00:24:53,600
allows me to solve for the xy

00:24:52,320 --> 00:24:57,120
coordinates

00:24:53,600 --> 00:24:59,520
in a millimeters true millimeters space

00:24:57,120 --> 00:25:00,400
of the 2d coordinates of each of these

00:24:59,520 --> 00:25:02,000
corners

00:25:00,400 --> 00:25:03,760
which i can then use to compute a

00:25:02,000 --> 00:25:06,400
homography to take the

00:25:03,760 --> 00:25:08,240
overall captured image and to bring it

00:25:06,400 --> 00:25:10,559
so that the fiducial marks

00:25:08,240 --> 00:25:12,240
appear perfectly at the true coordinates

00:25:10,559 --> 00:25:13,279
this then means i have a so-called

00:25:12,240 --> 00:25:17,120
rectified

00:25:13,279 --> 00:25:19,120
image that i can use to hang the higher

00:25:17,120 --> 00:25:21,520
resolution images on

00:25:19,120 --> 00:25:22,720
now an intermediate step that i'm not

00:25:21,520 --> 00:25:24,400
talking about today

00:25:22,720 --> 00:25:26,080
because it's very similar to the five

00:25:24,400 --> 00:25:28,960
micron step

00:25:26,080 --> 00:25:30,960
is that i also collected 20 micron

00:25:28,960 --> 00:25:36,240
resolution photos so this was

00:25:30,960 --> 00:25:37,760
um 528 photos instead of 8439 photos

00:25:36,240 --> 00:25:39,679
um and this was an image that was

00:25:37,760 --> 00:25:42,240
released um

00:25:39,679 --> 00:25:43,120
to the public about one year ago today

00:25:42,240 --> 00:25:45,360
so

00:25:43,120 --> 00:25:46,320
the procedure is we're going to take the

00:25:45,360 --> 00:25:48,320
20 micron

00:25:46,320 --> 00:25:49,840
images and hang them on the overall

00:25:48,320 --> 00:25:51,520
photo and then we're going to take the 5

00:25:49,840 --> 00:25:54,240
micron images and hang them on the 20

00:25:51,520 --> 00:25:54,240
micron photo

00:25:55,200 --> 00:26:02,000
um okay so now let's look at how

00:25:58,400 --> 00:26:05,120
an image is placed on a ground truth

00:26:02,000 --> 00:26:06,799
lower resolution photo python again is a

00:26:05,120 --> 00:26:09,760
major component of this

00:26:06,799 --> 00:26:10,880
and this is based largely on neural

00:26:09,760 --> 00:26:13,360
networks

00:26:10,880 --> 00:26:14,240
so here's the painting again let's focus

00:26:13,360 --> 00:26:17,279
on

00:26:14,240 --> 00:26:20,480
the face of the main figure

00:26:17,279 --> 00:26:23,200
bangkok so here it is

00:26:20,480 --> 00:26:25,600
and if you would now look at this in

00:26:23,200 --> 00:26:28,000
terms of the individual captures

00:26:25,600 --> 00:26:32,159
here you can see one capture you can see

00:26:28,000 --> 00:26:34,000
that we have a small amount of overlap

00:26:32,159 --> 00:26:35,360
anymore would have made the number of

00:26:34,000 --> 00:26:38,320
photos go way up

00:26:35,360 --> 00:26:40,480
and now the question is how can i place

00:26:38,320 --> 00:26:43,039
this one photo which is

00:26:40,480 --> 00:26:44,880
five and a half centimeters wide onto

00:26:43,039 --> 00:26:48,159
the lower resolution image where i

00:26:44,880 --> 00:26:49,679
really have confidence of the shape

00:26:48,159 --> 00:26:51,279
so here you can see the problem

00:26:49,679 --> 00:26:53,360
illustrated with the green channel

00:26:51,279 --> 00:26:54,640
i have the low resolution anchor image

00:26:53,360 --> 00:26:57,520
which is fixed

00:26:54,640 --> 00:26:59,039
and a high resolution capture and i want

00:26:57,520 --> 00:27:01,039
to know how can i

00:26:59,039 --> 00:27:02,480
position and deform the high resolution

00:27:01,039 --> 00:27:05,279
image so that it's

00:27:02,480 --> 00:27:08,080
perfectly with sub sub pixel accuracy

00:27:05,279 --> 00:27:11,760
positioned on the low resolution anchor

00:27:08,080 --> 00:27:15,520
so um the first step

00:27:11,760 --> 00:27:19,520
is to have a pair of

00:27:15,520 --> 00:27:19,520
neural networks that

00:27:19,600 --> 00:27:24,080
are each operating on an individual tile

00:27:22,559 --> 00:27:26,480
so here you can see a

00:27:24,080 --> 00:27:28,080
tile from the lower resolution image and

00:27:26,480 --> 00:27:29,760
a tile from the high resolution image

00:27:28,080 --> 00:27:33,200
below it

00:27:29,760 --> 00:27:35,039
one neural network by which i mean a

00:27:33,200 --> 00:27:35,520
convolutional neural network that has a

00:27:35,039 --> 00:27:37,440
stack

00:27:35,520 --> 00:27:38,960
of convolution filters with

00:27:37,440 --> 00:27:40,960
non-linearities

00:27:38,960 --> 00:27:42,640
though the convolution filters are

00:27:40,960 --> 00:27:44,320
constrained so that they always have

00:27:42,640 --> 00:27:45,679
rotational symmetry because when i

00:27:44,320 --> 00:27:47,039
filter this image

00:27:45,679 --> 00:27:48,880
i want to make sure that there's no

00:27:47,039 --> 00:27:51,840
lateral or vertical shifts

00:27:48,880 --> 00:27:53,679
so i learn a convolution filter to apply

00:27:51,840 --> 00:27:56,720
to the low resolution tile

00:27:53,679 --> 00:27:58,559
and i learn a second nonlinear

00:27:56,720 --> 00:28:00,000
convolution stack to apply to the high

00:27:58,559 --> 00:28:03,360
resolution tile

00:28:00,000 --> 00:28:05,039
where the objective here um in terms of

00:28:03,360 --> 00:28:08,000
the training of the neural network

00:28:05,039 --> 00:28:10,559
is to find a pair of filters such that

00:28:08,000 --> 00:28:12,240
when i do a normalized cross correlation

00:28:10,559 --> 00:28:13,360
between these filtered images and by

00:28:12,240 --> 00:28:15,200
that i mean

00:28:13,360 --> 00:28:16,720
you slide them past each other and see

00:28:15,200 --> 00:28:18,799
how well they match

00:28:16,720 --> 00:28:20,159
that there should be one and only one

00:28:18,799 --> 00:28:22,799
offset at which

00:28:20,159 --> 00:28:24,880
the correlation is really excellent and

00:28:22,799 --> 00:28:26,480
at all other offsets the correlation is

00:28:24,880 --> 00:28:27,919
nearly zero

00:28:26,480 --> 00:28:30,000
um and then the last thing to mention

00:28:27,919 --> 00:28:32,799
about this is um

00:28:30,000 --> 00:28:33,600
that cross correlation gives me what you

00:28:32,799 --> 00:28:36,880
might call an

00:28:33,600 --> 00:28:38,480
a bayesian worldview a likelihood for

00:28:36,880 --> 00:28:40,080
where i will find it and then i also

00:28:38,480 --> 00:28:41,679
have a prior belief about

00:28:40,080 --> 00:28:44,240
the relative offset between the two

00:28:41,679 --> 00:28:45,600
tiles so i can multiply the two using

00:28:44,240 --> 00:28:48,559
bayes law and that will give me a

00:28:45,600 --> 00:28:50,960
posterior estimate that tells me exactly

00:28:48,559 --> 00:28:52,720
how i should shift the the high

00:28:50,960 --> 00:28:54,480
resolution tile to make it match the

00:28:52,720 --> 00:28:58,559
lower resolution tile

00:28:54,480 --> 00:29:00,559
so i cut up the two images

00:28:58,559 --> 00:29:03,279
where they overlap into a lot of little

00:29:00,559 --> 00:29:04,640
tiles in this case they're 2048 pixels

00:29:03,279 --> 00:29:06,880
on a side

00:29:04,640 --> 00:29:08,640
and i performed this calculation to find

00:29:06,880 --> 00:29:09,520
the relative movement between the two

00:29:08,640 --> 00:29:11,919
tiles

00:29:09,520 --> 00:29:13,279
across the entire image and now i have

00:29:11,919 --> 00:29:15,840
an estimation problem

00:29:13,279 --> 00:29:17,039
to figure out what overall movement

00:29:15,840 --> 00:29:19,039
would i have to apply

00:29:17,039 --> 00:29:20,640
what geometric transformation would i

00:29:19,039 --> 00:29:23,360
have to apply to the high resolution

00:29:20,640 --> 00:29:26,640
image to make it

00:29:23,360 --> 00:29:28,480
align with the low resolution image so

00:29:26,640 --> 00:29:29,919
i'm basically going to estimate this

00:29:28,480 --> 00:29:31,760
with a homography

00:29:29,919 --> 00:29:33,600
the homography is all the kinds of

00:29:31,760 --> 00:29:35,200
transformations that you might encounter

00:29:33,600 --> 00:29:38,000
upon moving the camera

00:29:35,200 --> 00:29:39,120
so scales rotations changes in aspect

00:29:38,000 --> 00:29:42,159
ratio

00:29:39,120 --> 00:29:44,080
and perspective effects that will

00:29:42,159 --> 00:29:45,520
shrink part of the image while enlarging

00:29:44,080 --> 00:29:47,760
the other part

00:29:45,520 --> 00:29:48,960
so i'm using opencv which has a very

00:29:47,760 --> 00:29:50,960
nice

00:29:48,960 --> 00:29:52,159
homography estimator very fast and

00:29:50,960 --> 00:29:53,760
efficient

00:29:52,159 --> 00:29:55,279
and then after i apply that homography

00:29:53,760 --> 00:29:58,320
that will give me something

00:29:55,279 --> 00:30:02,080
um like this so still

00:29:58,320 --> 00:30:04,960
close but but not perfect um so

00:30:02,080 --> 00:30:06,480
then um the next step so this is close

00:30:04,960 --> 00:30:08,480
but not good enough because i want to be

00:30:06,480 --> 00:30:11,600
subpixel accurate

00:30:08,480 --> 00:30:14,159
so then i switch to lower

00:30:11,600 --> 00:30:16,159
resolution tiles or smaller tiles train

00:30:14,159 --> 00:30:18,000
another set of

00:30:16,159 --> 00:30:19,279
neural networks to learn how to filter

00:30:18,000 --> 00:30:22,000
these appropriately

00:30:19,279 --> 00:30:22,480
again so that i have one offset that's

00:30:22,000 --> 00:30:26,320
very

00:30:22,480 --> 00:30:26,320
likely and others that are not so

00:30:26,640 --> 00:30:32,320
not not viewed to be likely and then

00:30:29,840 --> 00:30:34,399
that allows me to make an overall

00:30:32,320 --> 00:30:36,960
deformation field so here you see the

00:30:34,399 --> 00:30:38,480
x offset and the y offset so now in

00:30:36,960 --> 00:30:41,279
principle i know

00:30:38,480 --> 00:30:42,640
exactly what homography i need to apply

00:30:41,279 --> 00:30:45,679
to the image to make it

00:30:42,640 --> 00:30:46,559
match i also know what kind of rubber

00:30:45,679 --> 00:30:49,279
deformation

00:30:46,559 --> 00:30:50,080
after the homography is applied to to

00:30:49,279 --> 00:30:53,279
make them

00:30:50,080 --> 00:30:55,600
match with sub pixel accuracy um

00:30:53,279 --> 00:30:56,399
so then i have to actually implement

00:30:55,600 --> 00:30:58,799
that i know

00:30:56,399 --> 00:30:59,440
how to move the images but i don't know

00:30:58,799 --> 00:31:02,399
how

00:30:59,440 --> 00:31:04,960
uh i haven't done it yet basically um so

00:31:02,399 --> 00:31:06,480
to do this i use um a relatively new

00:31:04,960 --> 00:31:10,640
library from google called

00:31:06,480 --> 00:31:12,240
jaxx um mostly because i wanted to teach

00:31:10,640 --> 00:31:13,519
myself how to do this

00:31:12,240 --> 00:31:15,279
and then after doing some

00:31:13,519 --> 00:31:16,320
experimentation with it i found that i

00:31:15,279 --> 00:31:18,799
could make a

00:31:16,320 --> 00:31:21,120
much faster interpolation algorithm with

00:31:18,799 --> 00:31:24,799
jacks than with any of the other

00:31:21,120 --> 00:31:26,559
gpu libraries that i tried i find jacks

00:31:24,799 --> 00:31:28,799
to be a real delight to use because

00:31:26,559 --> 00:31:30,080
it has an api that's almost identical to

00:31:28,799 --> 00:31:33,440
numpy

00:31:30,080 --> 00:31:35,360
so i can i can do things like um jax

00:31:33,440 --> 00:31:37,279
numpy dot where instead of numpy dot

00:31:35,360 --> 00:31:39,679
where or jax numpy.abs

00:31:37,279 --> 00:31:41,760
instead of numpy.abs um and then there's

00:31:39,679 --> 00:31:43,919
another cool feature and that is that

00:31:41,760 --> 00:31:45,519
um if you write your interpolation

00:31:43,919 --> 00:31:46,960
kernel as you see here

00:31:45,519 --> 00:31:48,880
so that it only takes a single

00:31:46,960 --> 00:31:50,000
coordinate which i want to interpolate

00:31:48,880 --> 00:31:52,640
into my uh

00:31:50,000 --> 00:31:53,679
my as captured image then you can make

00:31:52,640 --> 00:31:56,640
this automatically

00:31:53,679 --> 00:31:59,120
map across um spatial dimensions or

00:31:56,640 --> 00:32:02,480
batch dimensions with this

00:31:59,120 --> 00:32:04,320
v-map decorator and then finally i can

00:32:02,480 --> 00:32:05,279
do just-in-time compilation with this

00:32:04,320 --> 00:32:07,440
which will

00:32:05,279 --> 00:32:10,480
basically optimize the whole thing

00:32:07,440 --> 00:32:12,559
distill it down into an xla

00:32:10,480 --> 00:32:13,840
kernel so that it runs super fast on the

00:32:12,559 --> 00:32:15,440
gpu

00:32:13,840 --> 00:32:18,000
just to give you a sense of the timings

00:32:15,440 --> 00:32:20,559
here i can take an image

00:32:18,000 --> 00:32:21,679
a 100 megapixel image and a hundred

00:32:20,559 --> 00:32:23,360
million

00:32:21,679 --> 00:32:24,799
approximately 100 million different

00:32:23,360 --> 00:32:27,840
locations at which i want to do an

00:32:24,799 --> 00:32:29,279
interpolation and then do this um high

00:32:27,840 --> 00:32:32,640
order interpolation which

00:32:29,279 --> 00:32:33,600
requires looking at seven uh by seven

00:32:32,640 --> 00:32:35,440
different points

00:32:33,600 --> 00:32:38,799
for each interpolated point the whole

00:32:35,440 --> 00:32:42,480
interpolation happens in 45 milliseconds

00:32:38,799 --> 00:32:43,919
so uh okay so now i've um warped each of

00:32:42,480 --> 00:32:48,240
the images and i know

00:32:43,919 --> 00:32:51,120
where they belong um but now i need to

00:32:48,240 --> 00:32:52,640
um and and i've uh i've so i've got them

00:32:51,120 --> 00:32:54,480
so they're all warped they have exactly

00:32:52,640 --> 00:32:55,679
the right orientations and scales and so

00:32:54,480 --> 00:32:57,120
on but now i need to fuse them all

00:32:55,679 --> 00:32:59,919
together

00:32:57,120 --> 00:33:01,600
so again python figures quite heavily in

00:32:59,919 --> 00:33:03,840
this

00:33:01,600 --> 00:33:05,840
in particular shapely is a really

00:33:03,840 --> 00:33:07,279
excellent library for doing computations

00:33:05,840 --> 00:33:09,919
with geometries

00:33:07,279 --> 00:33:12,000
finding areas and perimeters and

00:33:09,919 --> 00:33:14,159
intersection shapes and cutting a shape

00:33:12,000 --> 00:33:17,679
with another shape and so on

00:33:14,159 --> 00:33:19,360
network x is a wonderful library that

00:33:17,679 --> 00:33:21,600
implements

00:33:19,360 --> 00:33:22,720
very efficiently lots of graph theory

00:33:21,600 --> 00:33:26,399
algorithms

00:33:22,720 --> 00:33:26,960
pi amg is the algebraic multi-grid

00:33:26,399 --> 00:33:28,559
solver

00:33:26,960 --> 00:33:30,799
for solving partial differential

00:33:28,559 --> 00:33:33,440
equations and other large linear systems

00:33:30,799 --> 00:33:35,200
and pi vips is really the meat of this

00:33:33,440 --> 00:33:38,640
that's a python interface to the

00:33:35,200 --> 00:33:41,840
really excellent vips library

00:33:38,640 --> 00:33:42,480
that enables one to do computations with

00:33:41,840 --> 00:33:45,519
images

00:33:42,480 --> 00:33:47,840
that are far larger than memory so i'm

00:33:45,519 --> 00:33:50,720
using pandas behind the scenes for

00:33:47,840 --> 00:33:52,000
managing the identities of the images

00:33:50,720 --> 00:33:55,760
and their positions and so

00:33:52,000 --> 00:33:57,679
on then i can use shapely

00:33:55,760 --> 00:33:59,679
to see exactly what the overlap

00:33:57,679 --> 00:34:01,600
structure is of the images after they've

00:33:59,679 --> 00:34:05,519
been deformed into place

00:34:01,600 --> 00:34:08,240
and this then lets me

00:34:05,519 --> 00:34:09,119
calculate which pixels are under

00:34:08,240 --> 00:34:11,280
contention

00:34:09,119 --> 00:34:12,800
by which i mean for some pixels i only

00:34:11,280 --> 00:34:14,320
have one photo that tells me about

00:34:12,800 --> 00:34:16,320
what's happening there but for other

00:34:14,320 --> 00:34:18,960
pixels i have for example here

00:34:16,320 --> 00:34:20,560
i have two opinions about what color

00:34:18,960 --> 00:34:21,280
that pixel should be and for others i

00:34:20,560 --> 00:34:22,800
have three

00:34:21,280 --> 00:34:25,440
opinions and for others i have four

00:34:22,800 --> 00:34:26,560
opinions so what i need to do is find a

00:34:25,440 --> 00:34:30,320
way to

00:34:26,560 --> 00:34:30,800
to settle the dispute when more than one

00:34:30,320 --> 00:34:32,240
image

00:34:30,800 --> 00:34:33,839
claims that it knows what's happening at

00:34:32,240 --> 00:34:36,879
a pixel and

00:34:33,839 --> 00:34:40,320
in fact to blend smoothly across them

00:34:36,879 --> 00:34:41,760
um so i can use um shapely to give me

00:34:40,320 --> 00:34:44,560
the overlap structure

00:34:41,760 --> 00:34:45,040
i can then ask network x for a so-called

00:34:44,560 --> 00:34:48,240
graph

00:34:45,040 --> 00:34:50,879
coloring to divide this into

00:34:48,240 --> 00:34:51,679
the minimum number of sets that are not

00:34:50,879 --> 00:34:53,839
self

00:34:51,679 --> 00:34:54,720
overlapping so you can see it's done

00:34:53,839 --> 00:34:57,200
that here with

00:34:54,720 --> 00:34:57,760
uh into four different sets so i have

00:34:57,200 --> 00:35:00,000
the

00:34:57,760 --> 00:35:01,359
the red team the green team the blue

00:35:00,000 --> 00:35:04,480
team and the yellow team

00:35:01,359 --> 00:35:06,160
if you want that looks like this for the

00:35:04,480 --> 00:35:09,599
little

00:35:06,160 --> 00:35:11,920
10 by 13 image sample that i showed you

00:35:09,599 --> 00:35:13,440
so none of these images overlap with

00:35:11,920 --> 00:35:15,200
each other none of these overlap with

00:35:13,440 --> 00:35:19,839
each other and so on

00:35:15,200 --> 00:35:19,839
and now i need to fuse these together so

00:35:20,240 --> 00:35:23,599
the fact is that when you're doing

00:35:22,320 --> 00:35:25,680
photography

00:35:23,599 --> 00:35:27,839
the pixels near the edge of the image

00:35:25,680 --> 00:35:28,800
are typically less accurate they may

00:35:27,839 --> 00:35:31,119
suffer from

00:35:28,800 --> 00:35:32,480
vignetting or lens distortion even

00:35:31,119 --> 00:35:33,200
though we have very good control over

00:35:32,480 --> 00:35:37,040
those

00:35:33,200 --> 00:35:39,200
still if you have an opinion from one

00:35:37,040 --> 00:35:40,800
image which is at the edge of the image

00:35:39,200 --> 00:35:42,320
and you have an opinion from another

00:35:40,800 --> 00:35:43,280
image which is at the center of the

00:35:42,320 --> 00:35:45,280
second image

00:35:43,280 --> 00:35:47,040
you should prefer the pixels that's at

00:35:45,280 --> 00:35:49,599
the center

00:35:47,040 --> 00:35:51,599
a pixel on the corner is typically very

00:35:49,599 --> 00:35:52,800
low quality so you should sort of double

00:35:51,599 --> 00:35:54,720
penalize that

00:35:52,800 --> 00:35:56,240
so the way i solve that problem is for

00:35:54,720 --> 00:35:59,440
each image

00:35:56,240 --> 00:36:00,560
i solve a poisson equation with a

00:35:59,440 --> 00:36:02,960
constant source term

00:36:00,560 --> 00:36:05,040
so in other words all of these regions

00:36:02,960 --> 00:36:07,599
are assumed to have a constant

00:36:05,040 --> 00:36:10,240
laplacian a constant second derivative a

00:36:07,599 --> 00:36:12,720
constant curvature

00:36:10,240 --> 00:36:15,119
so this is a visualization made after a

00:36:12,720 --> 00:36:17,760
solution with pi mg that shows me

00:36:15,119 --> 00:36:20,160
how uh how heavily i weight each pixel

00:36:17,760 --> 00:36:22,400
what is the value of each pixel

00:36:20,160 --> 00:36:24,720
um here is that when you fuse them all

00:36:22,400 --> 00:36:26,560
together among the four different groups

00:36:24,720 --> 00:36:27,839
and then ultimately i'm going to blend

00:36:26,560 --> 00:36:30,640
them together

00:36:27,839 --> 00:36:32,160
um using something called scale space

00:36:30,640 --> 00:36:36,079
theory

00:36:32,160 --> 00:36:37,200
or a burton adelson gaussian laplacian

00:36:36,079 --> 00:36:42,240
pyramid

00:36:37,200 --> 00:36:42,240
and so the basic idea there is that

00:36:42,400 --> 00:36:46,000
there are large scale trends between the

00:36:44,160 --> 00:36:49,599
images such as the

00:36:46,000 --> 00:36:53,359
overall brightness trends that

00:36:49,599 --> 00:36:54,960
i would like to smooth out across all

00:36:53,359 --> 00:36:56,560
the images because i don't want that

00:36:54,960 --> 00:36:57,280
each photo is slightly brighter on the

00:36:56,560 --> 00:36:59,280
right side

00:36:57,280 --> 00:37:00,720
but the small scale details i want to

00:36:59,280 --> 00:37:01,760
blend together over a very small

00:37:00,720 --> 00:37:05,200
distance

00:37:01,760 --> 00:37:07,040
so scale space theory makes me allows me

00:37:05,200 --> 00:37:08,000
to do this decomposition by doing a

00:37:07,040 --> 00:37:10,480
series of gaussian

00:37:08,000 --> 00:37:12,240
blurs and then the differences of those

00:37:10,480 --> 00:37:14,160
successive gaussian blurs are telling me

00:37:12,240 --> 00:37:17,680
what i took away

00:37:14,160 --> 00:37:19,680
so ultimately i can then use pi vips to

00:37:17,680 --> 00:37:21,680
do this series of gaussian blurs and

00:37:19,680 --> 00:37:22,960
then to blend the images together

00:37:21,680 --> 00:37:24,640
to blend the small scale features

00:37:22,960 --> 00:37:26,240
together over a small length scale and a

00:37:24,640 --> 00:37:29,520
large scale features over a long

00:37:26,240 --> 00:37:32,960
length scale and and then

00:37:29,520 --> 00:37:34,960
that allows me to assemble the image

00:37:32,960 --> 00:37:36,240
in different chunks these were made

00:37:34,960 --> 00:37:38,079
according to the

00:37:36,240 --> 00:37:39,599
desires of our researchers which parts

00:37:38,079 --> 00:37:41,920
they wanted to look at

00:37:39,599 --> 00:37:43,680
and then ultimately these can all be

00:37:41,920 --> 00:37:47,119
output into one

00:37:43,680 --> 00:37:50,320
enormous image so with that

00:37:47,119 --> 00:37:55,359
i will show you the final result

00:37:50,320 --> 00:37:56,839
this now is an image which is 925 000

00:37:55,359 --> 00:38:00,839
pixels wide

00:37:56,839 --> 00:38:04,960
775 000 pixels tall

00:38:00,839 --> 00:38:05,920
717 gigapixels with a resolution of 5

00:38:04,960 --> 00:38:09,440
microns

00:38:05,920 --> 00:38:11,359
so that is to give you a sense of what 5

00:38:09,440 --> 00:38:13,599
microns means

00:38:11,359 --> 00:38:14,560
the diameter of a human red blood cell

00:38:13,599 --> 00:38:17,520
is 8

00:38:14,560 --> 00:38:18,000
micrometers so one of my pixels is

00:38:17,520 --> 00:38:21,040
smaller

00:38:18,000 --> 00:38:22,000
than a human red blood cell so you can

00:38:21,040 --> 00:38:24,480
see now

00:38:22,000 --> 00:38:26,400
that i can zoom in and in and in and

00:38:24,480 --> 00:38:28,000
it's a virtual microscope into the

00:38:26,400 --> 00:38:32,079
entire painting

00:38:28,000 --> 00:38:34,960
i can see every single detail

00:38:32,079 --> 00:38:36,720
of every retouching so these are repairs

00:38:34,960 --> 00:38:40,640
that have been made to the painting

00:38:36,720 --> 00:38:43,040
i can see the beautiful paint work

00:38:40,640 --> 00:38:45,040
for example if i zoom in here i can see

00:38:43,040 --> 00:38:47,119
how rembrandt had multiple different wet

00:38:45,040 --> 00:38:48,640
paints on his palette that he used to

00:38:47,119 --> 00:38:52,560
paint this

00:38:48,640 --> 00:38:56,320
if i look nearby i can see that there is

00:38:52,560 --> 00:38:59,520
a pigment particle this is a small

00:38:56,320 --> 00:39:00,000
particle which is ground up glass really

00:38:59,520 --> 00:39:03,040
every

00:39:00,000 --> 00:39:05,359
little detail of every brushstroke and

00:39:03,040 --> 00:39:07,280
the detailed condition of the painting

00:39:05,359 --> 00:39:10,000
um so it's often said that this is

00:39:07,280 --> 00:39:11,760
rembrandt um a rembrandt self-portrait

00:39:10,000 --> 00:39:14,320
that he painted himself in that may just

00:39:11,760 --> 00:39:16,079
be an urban legend

00:39:14,320 --> 00:39:18,560
but i like to think that it's rembrandt

00:39:16,079 --> 00:39:21,119
and if i go all the way in i can see

00:39:18,560 --> 00:39:21,920
the exact details of the brush strokes

00:39:21,119 --> 00:39:25,760
that were used

00:39:21,920 --> 00:39:27,839
to make this look like a glossy eyeball

00:39:25,760 --> 00:39:28,880
and so on so we haven't released this

00:39:27,839 --> 00:39:32,640
image yet

00:39:28,880 --> 00:39:36,000
but we will be doing so soon um

00:39:32,640 --> 00:39:39,040
so yeah when i was growing up um

00:39:36,000 --> 00:39:40,880
i was obsessed with this toy called

00:39:39,040 --> 00:39:42,079
light bright and it let you take these

00:39:40,880 --> 00:39:44,560
little transparent

00:39:42,079 --> 00:39:45,119
translucent colored pegs and push them

00:39:44,560 --> 00:39:47,440
through some

00:39:45,119 --> 00:39:48,720
black paper to let some light shine

00:39:47,440 --> 00:39:50,720
through

00:39:48,720 --> 00:39:52,160
i used to i used to play with this for

00:39:50,720 --> 00:39:55,200
hours on end that had

00:39:52,160 --> 00:39:58,720
214 pieces

00:39:55,200 --> 00:40:01,200
ultimately um in 1979

00:39:58,720 --> 00:40:02,400
um i made my mother who is a software

00:40:01,200 --> 00:40:04,400
engineer

00:40:02,400 --> 00:40:05,440
teach me how to how to do programming

00:40:04,400 --> 00:40:07,520
and so

00:40:05,440 --> 00:40:09,760
um i've been obsessed with computer

00:40:07,520 --> 00:40:11,440
graphics and computer vision ever since

00:40:09,760 --> 00:40:14,400
so in the spirit of how it started and

00:40:11,440 --> 00:40:15,680
how it's going started with 214 pixels

00:40:14,400 --> 00:40:20,880
and it ended

00:40:15,680 --> 00:40:24,000
with 717 billion pixels

00:40:20,880 --> 00:40:27,200
so with that um i

00:40:24,000 --> 00:40:31,680
would like to thank you very much for

00:40:27,200 --> 00:40:33,920
um being such a wonderful and welcome

00:40:31,680 --> 00:40:35,520
welcoming open source community i

00:40:33,920 --> 00:40:36,079
couldn't have done any of the stage of

00:40:35,520 --> 00:40:39,359
this

00:40:36,079 --> 00:40:42,480
stages of this process without python

00:40:39,359 --> 00:40:44,880
and the incredible environment

00:40:42,480 --> 00:40:46,160
the ecosystem of open source libraries

00:40:44,880 --> 00:40:49,599
to do imaging

00:40:46,160 --> 00:40:52,480
and color computations and data science

00:40:49,599 --> 00:40:54,880
and visualization and so on um

00:40:52,480 --> 00:40:56,319
so again i would like to thank the

00:40:54,880 --> 00:40:59,280
python community

00:40:56,319 --> 00:41:00,960
thank the organizers for inviting me to

00:40:59,280 --> 00:41:04,160
give this talk

00:41:00,960 --> 00:41:07,839
and i would like to thank you for

00:41:04,160 --> 00:41:07,839

YouTube URL: https://www.youtube.com/watch?v=z_hm5oX7ZlE


