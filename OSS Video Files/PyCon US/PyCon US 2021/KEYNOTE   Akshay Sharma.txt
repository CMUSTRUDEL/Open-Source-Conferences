Title: KEYNOTE   Akshay Sharma
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Akshay Sharma is executive vice president of artificial intelligence (AI) at Sharecare, the digital health company that helps people manage all their health in one place. Sharma joined Sharecare in 2021 as part of its acquisition of doc.ai, the Silicon Valley-based company that accelerated digital transformation in healthcare.

Sharma is an entrepreneur, founder, and experienced leader in engineering technology with a focus on healthcare. Since he initially joined doc.ai, he has been integral in the creation of core mobile app offerings including Passport, a privacy-based health-at-work solution; Serenity, a mental health digital offering; NetRunner, an edge computing and inference AI app; and Genewall, a genome app that deals with bioinformatics on the edge. He is passionate about developing and applying poly-omics data combinations to healthcare and life sciences as well as developing AI to assist in medical data understanding. He has built products, technology, and teams focused on edge computing (mobile/MCUs/sensors) and privacy (computations/inferences/learning), and holds several patents in this space.

With doc.ai, Sharma previously held various leadership positions including chief technology officer (CTO), and vice president of engineering, a role in which he developed several key technologies that power mobile-based privacy products in healthcare. Prior to joining the company, he also founded and co-founded several businesses including Swast, a startup focused on doctor-efficiency within the Indian healthcare ecosystem, and PixelSimple, a company engineering nextgen media streaming systems in the U.S. and Bangalore.

In addition to his role at Sharecare, Sharma serves as CTO of TEDxSanFrancisco and also is involved in initiatives to decentralize clinical trials. Sharma holds bachelorâ€™s degrees in engineering and engineering in information science from Visvesvaraya Technological University.
Captions: 
	00:00:04,240 --> 00:00:06,720
hi

00:00:04,880 --> 00:00:08,160
i hope you're having a wonderful time at

00:00:06,720 --> 00:00:09,840
pycon this year

00:00:08,160 --> 00:00:11,440
in this talk i want to walk you through

00:00:09,840 --> 00:00:14,160
some aspects of healthcare

00:00:11,440 --> 00:00:16,400
and why python is becoming the de facto

00:00:14,160 --> 00:00:18,640
language of choice in health care

00:00:16,400 --> 00:00:21,279
my name is akshay sharma and i'm the evp

00:00:18,640 --> 00:00:23,519
of ai at sharecare

00:00:21,279 --> 00:00:25,439
sharecare is a digital health platform

00:00:23,519 --> 00:00:27,519
that helps people manage

00:00:25,439 --> 00:00:28,800
all their health data in one place

00:00:27,519 --> 00:00:31,760
sharecare was founded

00:00:28,800 --> 00:00:33,360
by the same team that created webmd and

00:00:31,760 --> 00:00:34,320
we have rich history working in

00:00:33,360 --> 00:00:36,399
healthcare

00:00:34,320 --> 00:00:38,960
i came to sharecare by the way of an

00:00:36,399 --> 00:00:41,520
acquisition of a startup called doc ai

00:00:38,960 --> 00:00:43,120
where we developed products and models

00:00:41,520 --> 00:00:44,399
to understand and improve an

00:00:43,120 --> 00:00:46,879
individual's health

00:00:44,399 --> 00:00:48,399
we built several ai based technologies

00:00:46,879 --> 00:00:51,039
that allow us to capture

00:00:48,399 --> 00:00:52,559
health signals from users usually use

00:00:51,039 --> 00:00:55,360
using their phone

00:00:52,559 --> 00:00:57,199
for example we could extract medication

00:00:55,360 --> 00:00:59,440
information from pill bottles

00:00:57,199 --> 00:01:01,680
or extracting phenotypic information

00:00:59,440 --> 00:01:04,720
from video and audio signals

00:01:01,680 --> 00:01:06,640
and then we use this rich real world

00:01:04,720 --> 00:01:09,040
data to train and deploy

00:01:06,640 --> 00:01:10,640
machine learning models in clinically

00:01:09,040 --> 00:01:12,720
irb approved studies

00:01:10,640 --> 00:01:13,680
or even for simple engagement with the

00:01:12,720 --> 00:01:16,320
user

00:01:13,680 --> 00:01:18,479
we extensively used python for all of

00:01:16,320 --> 00:01:20,560
this

00:01:18,479 --> 00:01:21,600
so to talk about the future of

00:01:20,560 --> 00:01:23,680
healthcare

00:01:21,600 --> 00:01:25,200
we need to first understand the

00:01:23,680 --> 00:01:28,479
computational history

00:01:25,200 --> 00:01:31,439
of healthcare for a long time

00:01:28,479 --> 00:01:32,240
healthcare has been using a processing

00:01:31,439 --> 00:01:34,799
first thinking

00:01:32,240 --> 00:01:35,680
or processing first paradigm and what i

00:01:34,799 --> 00:01:38,479
mean by that

00:01:35,680 --> 00:01:39,520
is it has mostly been about dealing with

00:01:38,479 --> 00:01:42,640
data formats

00:01:39,520 --> 00:01:46,000
and data exchange and very little about

00:01:42,640 --> 00:01:48,240
data understanding so you will

00:01:46,000 --> 00:01:50,399
often run into having to deal with

00:01:48,240 --> 00:01:51,200
exchanging data from one system to

00:01:50,399 --> 00:01:53,119
another

00:01:51,200 --> 00:01:55,600
and you're mostly dealing with flat

00:01:53,119 --> 00:01:58,320
files or files with certain delimited

00:01:55,600 --> 00:01:59,119
delimiter separated formats a lot of

00:01:58,320 --> 00:02:01,600
investment

00:01:59,119 --> 00:02:02,159
in the industry has been made to just

00:02:01,600 --> 00:02:04,159
parse

00:02:02,159 --> 00:02:05,280
and format this data and put it in a

00:02:04,159 --> 00:02:08,160
database

00:02:05,280 --> 00:02:08,879
so this is an example of an hl7 data

00:02:08,160 --> 00:02:12,000
feed

00:02:08,879 --> 00:02:14,160
of a person's medical record and

00:02:12,000 --> 00:02:15,599
there are enough parsers to like

00:02:14,160 --> 00:02:18,720
understand this data

00:02:15,599 --> 00:02:22,000
and extract information out of that but

00:02:18,720 --> 00:02:24,480
mostly it's always been about data

00:02:22,000 --> 00:02:26,800
processing and very little about data

00:02:24,480 --> 00:02:29,040
understanding

00:02:26,800 --> 00:02:31,040
attempts have been made to standardize

00:02:29,040 --> 00:02:33,440
and facilitate data exchange

00:02:31,040 --> 00:02:34,080
an example of that is continuity of care

00:02:33,440 --> 00:02:37,360
document

00:02:34,080 --> 00:02:38,160
ccd it has emerged as a standard with

00:02:37,360 --> 00:02:41,200
xml

00:02:38,160 --> 00:02:43,440
for formatted data unfortunately

00:02:41,200 --> 00:02:46,400
not all electronic medical record

00:02:43,440 --> 00:02:48,800
systems follow the same standards of ccd

00:02:46,400 --> 00:02:51,120
so you have the information lost to some

00:02:48,800 --> 00:02:53,200
extent when you transform from one ccd

00:02:51,120 --> 00:02:56,160
to another ccd format

00:02:53,200 --> 00:02:56,720
and there's also like parts in the ccd

00:02:56,160 --> 00:02:59,040
that

00:02:56,720 --> 00:02:59,840
don't let you extract full information

00:02:59,040 --> 00:03:03,280
because it's

00:02:59,840 --> 00:03:05,200
completely text based at some point

00:03:03,280 --> 00:03:07,200
if you're lucky some systems will

00:03:05,200 --> 00:03:08,000
support the emerging standard called

00:03:07,200 --> 00:03:11,200
fire

00:03:08,000 --> 00:03:14,400
uh which has both a json and an xml view

00:03:11,200 --> 00:03:16,560
and uh fire allows you to represent

00:03:14,400 --> 00:03:18,000
medical entities in a very structured

00:03:16,560 --> 00:03:20,400
way where you can actually

00:03:18,000 --> 00:03:21,120
now write program and apis around this

00:03:20,400 --> 00:03:24,080
data to

00:03:21,120 --> 00:03:24,640
uh not just process it but also probably

00:03:24,080 --> 00:03:27,760
like

00:03:24,640 --> 00:03:29,599
make use of it um programming uh

00:03:27,760 --> 00:03:32,400
and processing such format is a lot

00:03:29,599 --> 00:03:35,599
easier and you have very little

00:03:32,400 --> 00:03:37,519
information drop out if possible

00:03:35,599 --> 00:03:38,959
in order to support such processing

00:03:37,519 --> 00:03:40,319
based needs

00:03:38,959 --> 00:03:42,000
some of the popular programming

00:03:40,319 --> 00:03:43,519
languages that have been used and some

00:03:42,000 --> 00:03:47,360
of them are still popular

00:03:43,519 --> 00:03:49,760
are cobal mums and java surprisingly

00:03:47,360 --> 00:03:50,400
enterprises use a lot of mainframe and

00:03:49,760 --> 00:03:52,799
cobalt

00:03:50,400 --> 00:03:54,239
is one of the top programming language

00:03:52,799 --> 00:03:57,280
that that's used in

00:03:54,239 --> 00:03:59,120
uh on top of mainframe so we are still

00:03:57,280 --> 00:04:02,400
in the world where we have to deal with

00:03:59,120 --> 00:04:04,239
some of the legacy systems in healthcare

00:04:02,400 --> 00:04:06,239
and historically the systems that have

00:04:04,239 --> 00:04:07,680
been built to deal with the processing

00:04:06,239 --> 00:04:09,760
nature of the data

00:04:07,680 --> 00:04:10,959
have been pretty complex and don't

00:04:09,760 --> 00:04:13,519
really support

00:04:10,959 --> 00:04:14,080
data understanding or building knowledge

00:04:13,519 --> 00:04:16,160
but just

00:04:14,080 --> 00:04:18,000
mostly about taking data from one place

00:04:16,160 --> 00:04:18,639
to another and so you'll often see

00:04:18,000 --> 00:04:21,680
things like

00:04:18,639 --> 00:04:24,639
enterprise service bus or legacy systems

00:04:21,680 --> 00:04:27,280
or j2e systems trying to access and

00:04:24,639 --> 00:04:29,680
process such data

00:04:27,280 --> 00:04:30,720
so let's ask what is changing in

00:04:29,680 --> 00:04:32,720
healthcare

00:04:30,720 --> 00:04:35,120
um we are seeing that healthcare is

00:04:32,720 --> 00:04:37,600
moving from a processing first paradigm

00:04:35,120 --> 00:04:39,759
to a compute first paradigm

00:04:37,600 --> 00:04:40,960
and this is because what constitutes

00:04:39,759 --> 00:04:43,360
health data

00:04:40,960 --> 00:04:44,479
has expanded and they are no longer in a

00:04:43,360 --> 00:04:47,680
central location

00:04:44,479 --> 00:04:48,080
as files or formatted files you need to

00:04:47,680 --> 00:04:50,479
go

00:04:48,080 --> 00:04:53,440
to where the data is generated and that

00:04:50,479 --> 00:04:55,840
could be on the edges

00:04:53,440 --> 00:04:56,800
to explain this a little better here is

00:04:55,840 --> 00:04:58,560
a point of view

00:04:56,800 --> 00:05:00,400
on what we are seeing from the

00:04:58,560 --> 00:05:02,560
computational trends

00:05:00,400 --> 00:05:04,240
in the beginning everyone built their

00:05:02,560 --> 00:05:07,280
own data centers

00:05:04,240 --> 00:05:10,720
and used bare metal to process data

00:05:07,280 --> 00:05:13,440
then in 2006 amazon changed the game by

00:05:10,720 --> 00:05:14,720
making cloud accessible for any company

00:05:13,440 --> 00:05:16,800
to be able to spin up

00:05:14,720 --> 00:05:18,160
a compute cluster and run your own

00:05:16,800 --> 00:05:21,360
processing nodes

00:05:18,160 --> 00:05:22,400
um we saw the dawn of monolithic apps on

00:05:21,360 --> 00:05:26,560
the cloud

00:05:22,400 --> 00:05:28,960
by 2016 um with docker and kubernetes

00:05:26,560 --> 00:05:30,000
already in place we saw the trend of

00:05:28,960 --> 00:05:32,240
moving to much

00:05:30,000 --> 00:05:33,440
more of a microservice architecture

00:05:32,240 --> 00:05:36,160
where you had the

00:05:33,440 --> 00:05:38,320
smaller computes but still have to

00:05:36,160 --> 00:05:40,800
access like larger data payloads

00:05:38,320 --> 00:05:42,240
but what we are seeing now is the world

00:05:40,800 --> 00:05:45,520
of edge native

00:05:42,240 --> 00:05:47,360
where we are seeing phones sensors mcus

00:05:45,520 --> 00:05:48,720
and neural network hardwares are

00:05:47,360 --> 00:05:50,000
generating and

00:05:48,720 --> 00:05:52,639
collecting interesting types of

00:05:50,000 --> 00:05:54,479
information usually on the periphery

00:05:52,639 --> 00:05:56,240
some of which will not even centralize

00:05:54,479 --> 00:05:56,880
the data but they just do predictions on

00:05:56,240 --> 00:05:58,800
the edge

00:05:56,880 --> 00:06:01,120
we are truly entering an era of

00:05:58,800 --> 00:06:04,160
distributed computing

00:06:01,120 --> 00:06:04,880
so the future on one hand looks like the

00:06:04,160 --> 00:06:07,680
hardware

00:06:04,880 --> 00:06:09,199
is scaling horizontally by having new

00:06:07,680 --> 00:06:11,199
products on the edge

00:06:09,199 --> 00:06:12,720
and so is the software if you look at it

00:06:11,199 --> 00:06:15,280
so historically we've

00:06:12,720 --> 00:06:17,120
seen monolithic apps become micro

00:06:15,280 --> 00:06:18,160
services with more distributed computing

00:06:17,120 --> 00:06:20,560
needs

00:06:18,160 --> 00:06:21,199
and then we saw the emergence of

00:06:20,560 --> 00:06:22,880
something like

00:06:21,199 --> 00:06:25,520
machine learning and deep learning which

00:06:22,880 --> 00:06:27,120
allows you to like build the algorithms

00:06:25,520 --> 00:06:29,440
out of the data so you feed a lot of

00:06:27,120 --> 00:06:31,360
data and outcomes basically your model

00:06:29,440 --> 00:06:33,039
model has some rules that it has

00:06:31,360 --> 00:06:34,720
automatically learned so you're not

00:06:33,039 --> 00:06:36,880
programming the rules

00:06:34,720 --> 00:06:38,080
but i think what the future looks like

00:06:36,880 --> 00:06:40,000
because the hardware

00:06:38,080 --> 00:06:41,440
is going more distributed the data gets

00:06:40,000 --> 00:06:43,280
generated on the edge

00:06:41,440 --> 00:06:45,360
we are seeing that these models have to

00:06:43,280 --> 00:06:47,440
be compressed and actually sent

00:06:45,360 --> 00:06:49,039
down to where the compute happens which

00:06:47,440 --> 00:06:51,120
is usually on the edge now

00:06:49,039 --> 00:06:52,400
and for that we are looking at like

00:06:51,120 --> 00:06:54,240
supporting

00:06:52,400 --> 00:06:56,240
tiny models that can do inference on the

00:06:54,240 --> 00:06:58,240
edges but also learn on the edge so the

00:06:56,240 --> 00:06:59,199
federated learning world has already

00:06:58,240 --> 00:07:01,680
emerged

00:06:59,199 --> 00:07:03,759
so that's an important paradigm shift

00:07:01,680 --> 00:07:04,639
and healthcare we are absolutely seeing

00:07:03,759 --> 00:07:08,319
these trends

00:07:04,639 --> 00:07:09,759
emerge right now so

00:07:08,319 --> 00:07:11,840
that's on the hardware and the software

00:07:09,759 --> 00:07:14,160
front but healthcare itself

00:07:11,840 --> 00:07:15,599
is actually moving to a very distributed

00:07:14,160 --> 00:07:18,000
model

00:07:15,599 --> 00:07:18,800
so pandemic has accelerated how care is

00:07:18,000 --> 00:07:20,800
being provided

00:07:18,800 --> 00:07:22,880
so telehealth is now like the first

00:07:20,800 --> 00:07:25,280
point of contacting care

00:07:22,880 --> 00:07:26,319
clinical trials are going virtual and

00:07:25,280 --> 00:07:28,160
remote

00:07:26,319 --> 00:07:30,319
where users can be anywhere in the

00:07:28,160 --> 00:07:31,919
country we are removing

00:07:30,319 --> 00:07:34,240
geographic constraints for example

00:07:31,919 --> 00:07:37,759
that's a very important aspect of where

00:07:34,240 --> 00:07:39,360
the care can be provided gig economy has

00:07:37,759 --> 00:07:41,199
is allowing healthcare workers to

00:07:39,360 --> 00:07:43,360
provide care at home so

00:07:41,199 --> 00:07:44,960
overall we are seeing the similar trends

00:07:43,360 --> 00:07:47,520
in healthcare where

00:07:44,960 --> 00:07:50,080
now there's more data being generated on

00:07:47,520 --> 00:07:50,080
the edge

00:07:50,240 --> 00:07:54,080
and again what we mean what this means

00:07:53,039 --> 00:07:56,400
is that

00:07:54,080 --> 00:07:58,080
everyone with a smartphone is actually

00:07:56,400 --> 00:08:00,000
generating certain health signals

00:07:58,080 --> 00:08:02,080
that can be used for understanding the

00:08:00,000 --> 00:08:03,919
health and also what we know

00:08:02,080 --> 00:08:06,960
is that medical information doubles

00:08:03,919 --> 00:08:06,960
every 73 days

00:08:07,199 --> 00:08:11,599
and what that means is that the only

00:08:10,800 --> 00:08:14,639
node

00:08:11,599 --> 00:08:16,479
that has up-to-date information and

00:08:14,639 --> 00:08:19,280
knows the most about you

00:08:16,479 --> 00:08:21,039
is actually you and usually that means

00:08:19,280 --> 00:08:25,120
that it's somewhere on

00:08:21,039 --> 00:08:28,160
you which typically is your smartphone

00:08:25,120 --> 00:08:30,879
so taking a step back what we need

00:08:28,160 --> 00:08:32,719
is a programming ecosystem that

00:08:30,879 --> 00:08:34,640
facilitates computing

00:08:32,719 --> 00:08:36,560
anywhere given that the data can be

00:08:34,640 --> 00:08:38,560
anywhere

00:08:36,560 --> 00:08:40,000
and python is a perfect fit for this

00:08:38,560 --> 00:08:41,760
because python can be run

00:08:40,000 --> 00:08:43,360
on the back end it can be used for

00:08:41,760 --> 00:08:46,240
building web applications

00:08:43,360 --> 00:08:47,120
uh we can also build and train models

00:08:46,240 --> 00:08:48,959
which are

00:08:47,120 --> 00:08:50,320
really valuable in the in the care in

00:08:48,959 --> 00:08:52,560
the case of healthcare

00:08:50,320 --> 00:08:54,399
and if needed we can also convert them

00:08:52,560 --> 00:08:55,200
and run on embedded systems using

00:08:54,399 --> 00:08:59,279
different

00:08:55,200 --> 00:09:02,000
variants of python for example c python

00:08:59,279 --> 00:09:04,000
so let's ask the question how is python

00:09:02,000 --> 00:09:06,720
changing the computational

00:09:04,000 --> 00:09:09,040
nature of healthcare and the short

00:09:06,720 --> 00:09:11,120
answer here is that by facilitating

00:09:09,040 --> 00:09:12,800
deep collaboration across all

00:09:11,120 --> 00:09:13,839
stakeholders that are involved in

00:09:12,800 --> 00:09:17,920
typically providing

00:09:13,839 --> 00:09:19,760
care and research so

00:09:17,920 --> 00:09:22,480
traditional healthcare systems are very

00:09:19,760 --> 00:09:24,959
siloed the data creation systems

00:09:22,480 --> 00:09:27,040
and teams are isolated from data

00:09:24,959 --> 00:09:28,959
transformation systems and teams

00:09:27,040 --> 00:09:30,720
so you go from one environment to

00:09:28,959 --> 00:09:33,279
another environment and at times

00:09:30,720 --> 00:09:34,080
jump system boundaries where knowledge

00:09:33,279 --> 00:09:37,279
in the data

00:09:34,080 --> 00:09:37,680
gets translated into outside the scope

00:09:37,279 --> 00:09:39,360
of

00:09:37,680 --> 00:09:41,680
programming environment so it goes into

00:09:39,360 --> 00:09:42,800
word or excel and these are lossy

00:09:41,680 --> 00:09:44,880
transformations

00:09:42,800 --> 00:09:46,160
because it's usually one way and there

00:09:44,880 --> 00:09:48,399
is absolutely no

00:09:46,160 --> 00:09:50,080
feedback loops back to how to improve

00:09:48,399 --> 00:09:51,519
your algorithm or the code

00:09:50,080 --> 00:09:54,640
and the stakeholders are not

00:09:51,519 --> 00:09:56,959
collaborating in in a single system view

00:09:54,640 --> 00:09:57,839
so finally when you want to derive

00:09:56,959 --> 00:10:01,200
insights

00:09:57,839 --> 00:10:02,160
from the data you are crossing several

00:10:01,200 --> 00:10:04,480
systems

00:10:02,160 --> 00:10:06,480
and you've lost certain information it's

00:10:04,480 --> 00:10:09,920
almost like playing a telephone game

00:10:06,480 --> 00:10:11,760
of medical knowledge so

00:10:09,920 --> 00:10:13,440
here is how we are seeing the modern

00:10:11,760 --> 00:10:16,480
healthcare staff work

00:10:13,440 --> 00:10:19,279
uh by bringing all the stakeholders onto

00:10:16,480 --> 00:10:21,120
a single data-driven point of view

00:10:19,279 --> 00:10:22,720
without having to jump the different

00:10:21,120 --> 00:10:24,560
system boundaries

00:10:22,720 --> 00:10:25,920
uh you could still be doing the physical

00:10:24,560 --> 00:10:28,800
system boundary jumps but

00:10:25,920 --> 00:10:30,480
logically it's single system view we

00:10:28,800 --> 00:10:32,959
call this data fluency

00:10:30,480 --> 00:10:35,279
which allows all the key stakeholders to

00:10:32,959 --> 00:10:37,839
discover the value on top of the data

00:10:35,279 --> 00:10:38,560
or insights on top of the data in real

00:10:37,839 --> 00:10:41,680
time

00:10:38,560 --> 00:10:44,720
and in an agile iterative way and

00:10:41,680 --> 00:10:46,480
all of this is powered by a notebook

00:10:44,720 --> 00:10:48,720
behind the scenes

00:10:46,480 --> 00:10:50,640
so to walk you through what that looks

00:10:48,720 --> 00:10:52,800
like it starts by building

00:10:50,640 --> 00:10:53,839
uh one or more notebooks on top of the

00:10:52,800 --> 00:10:55,680
data

00:10:53,839 --> 00:10:58,000
this could be our data engineer or data

00:10:55,680 --> 00:11:00,160
scientist for building transformations

00:10:58,000 --> 00:11:01,120
and then obviously building models on

00:11:00,160 --> 00:11:04,160
top of that

00:11:01,120 --> 00:11:05,760
or it could also be simple analytics the

00:11:04,160 --> 00:11:08,880
knowledge on the data

00:11:05,760 --> 00:11:09,279
is contained in one single place which

00:11:08,880 --> 00:11:13,200
is

00:11:09,279 --> 00:11:14,640
a python notebook now the finance or the

00:11:13,200 --> 00:11:17,360
operations person

00:11:14,640 --> 00:11:18,240
is looking at the same insight on top of

00:11:17,360 --> 00:11:20,640
the data

00:11:18,240 --> 00:11:22,560
that is derived out of that notebook and

00:11:20,640 --> 00:11:24,480
it's generated on top of the same

00:11:22,560 --> 00:11:28,399
algorithms that were self-contained

00:11:24,480 --> 00:11:31,920
and more importantly they can actually

00:11:28,399 --> 00:11:32,320
give feedback based on using tools like

00:11:31,920 --> 00:11:33,920
this

00:11:32,320 --> 00:11:36,000
which have been generated on top of the

00:11:33,920 --> 00:11:38,000
data so what they see

00:11:36,000 --> 00:11:39,120
and what they want to change is actually

00:11:38,000 --> 00:11:41,200
an active feedback

00:11:39,120 --> 00:11:43,120
back to the data scientist where you can

00:11:41,200 --> 00:11:44,720
go back and update your notebook and you

00:11:43,120 --> 00:11:46,720
can have a new version

00:11:44,720 --> 00:11:47,839
almost instantaneously you do not have

00:11:46,720 --> 00:11:49,760
to wait for

00:11:47,839 --> 00:11:51,600
knowledge getting translated to a word

00:11:49,760 --> 00:11:52,880
file and then some product manager

00:11:51,600 --> 00:11:55,040
having to come back and

00:11:52,880 --> 00:11:56,240
rewrite that aspect an engineer is going

00:11:55,040 --> 00:11:59,519
to have to build this

00:11:56,240 --> 00:12:02,480
that cycle has actually rapidly reduced

00:11:59,519 --> 00:12:03,279
by having a notebook-based viewpoint now

00:12:02,480 --> 00:12:05,920
what about

00:12:03,279 --> 00:12:07,839
clinicians and care management teams

00:12:05,920 --> 00:12:10,240
it's actually indeed the same system

00:12:07,839 --> 00:12:10,880
the views that are generated here are

00:12:10,240 --> 00:12:14,000
also

00:12:10,880 --> 00:12:17,120
off from the same jupiter notebook by

00:12:14,000 --> 00:12:18,959
slapping in a ui app there are no new

00:12:17,120 --> 00:12:19,839
systems or teams building insights and

00:12:18,959 --> 00:12:21,200
silos

00:12:19,839 --> 00:12:23,200
again the feedback from the care

00:12:21,200 --> 00:12:24,399
management team is immediately

00:12:23,200 --> 00:12:26,160
incorporate back

00:12:24,399 --> 00:12:27,600
incorporated back into the notebook

00:12:26,160 --> 00:12:29,839
workflow

00:12:27,600 --> 00:12:30,639
so in the end what you have is multiple

00:12:29,839 --> 00:12:32,959
views

00:12:30,639 --> 00:12:34,320
to discover insights on the data from

00:12:32,959 --> 00:12:36,880
different stakeholders

00:12:34,320 --> 00:12:39,040
but everything is powered by the same

00:12:36,880 --> 00:12:40,000
set of notebooks and that continues to

00:12:39,040 --> 00:12:44,800
learn and improve

00:12:40,000 --> 00:12:46,959
over time so in order to support

00:12:44,800 --> 00:12:49,760
such workflow data fluency workflow

00:12:46,959 --> 00:12:52,560
systems you almost need to invest

00:12:49,760 --> 00:12:53,920
in certain advanced ai and data fluency

00:12:52,560 --> 00:12:56,079
workflow systems

00:12:53,920 --> 00:12:57,920
the system is designed for agility of

00:12:56,079 --> 00:12:58,639
computing on data and extracting

00:12:57,920 --> 00:13:01,279
knowledge

00:12:58,639 --> 00:13:02,079
uh thankfully a lot of cloud native

00:13:01,279 --> 00:13:04,959
architectures

00:13:02,079 --> 00:13:05,600
allow us to do this a lot of this can

00:13:04,959 --> 00:13:08,079
actually be

00:13:05,600 --> 00:13:09,360
automatically extended to like computing

00:13:08,079 --> 00:13:12,079
on the edge and i'll show you some

00:13:09,360 --> 00:13:12,079
examples of that

00:13:12,800 --> 00:13:16,160
end of the day send me a notebook is now

00:13:15,760 --> 00:13:19,440
the

00:13:16,160 --> 00:13:21,760
new currency of medical knowledge

00:13:19,440 --> 00:13:22,720
let me phrase that again send me a

00:13:21,760 --> 00:13:25,279
notebook

00:13:22,720 --> 00:13:26,639
is the new currency of medical knowledge

00:13:25,279 --> 00:13:29,200
what we are seeing

00:13:26,639 --> 00:13:30,079
is that everything starts with the

00:13:29,200 --> 00:13:32,800
notebook

00:13:30,079 --> 00:13:34,639
you write a program you write a model

00:13:32,800 --> 00:13:36,079
you express the model in different views

00:13:34,639 --> 00:13:39,040
for different stakeholders

00:13:36,079 --> 00:13:39,839
you actively learn from their usage and

00:13:39,040 --> 00:13:41,920
feedback

00:13:39,839 --> 00:13:43,040
and continue to improve the algorithm

00:13:41,920 --> 00:13:44,480
all in one place

00:13:43,040 --> 00:13:46,560
and that's the most important thing that

00:13:44,480 --> 00:13:49,839
we are actually seeing healthcare

00:13:46,560 --> 00:13:52,160
in healthcare right now so i talked

00:13:49,839 --> 00:13:54,480
about how healthcare has moved from

00:13:52,160 --> 00:13:55,760
a processing first paradigm to a compute

00:13:54,480 --> 00:13:57,519
first paradigm

00:13:55,760 --> 00:13:58,800
but where we're actually seeing the

00:13:57,519 --> 00:14:01,519
future go

00:13:58,800 --> 00:14:02,160
is deriving intelligence on top of the

00:14:01,519 --> 00:14:04,720
data

00:14:02,160 --> 00:14:05,440
and keeping the data private a lot of

00:14:04,720 --> 00:14:07,920
this happens

00:14:05,440 --> 00:14:09,199
with users on their phone and using

00:14:07,920 --> 00:14:12,560
their personal devices

00:14:09,199 --> 00:14:14,399
or other sensors at home so privacy is

00:14:12,560 --> 00:14:16,480
actually a first-class citizen

00:14:14,399 --> 00:14:18,639
in the context of healthcare in my

00:14:16,480 --> 00:14:20,160
opinion

00:14:18,639 --> 00:14:21,839
so we are entering an age of

00:14:20,160 --> 00:14:25,360
understanding health using

00:14:21,839 --> 00:14:27,440
dry novel digital biomarkers that can be

00:14:25,360 --> 00:14:29,839
captured and extracted using

00:14:27,440 --> 00:14:31,199
user in the loop so historically

00:14:29,839 --> 00:14:33,120
healthcare has looked at a lot of

00:14:31,199 --> 00:14:36,079
sequencing data what we call as

00:14:33,120 --> 00:14:37,680
wet biomarkers uh molecular sequencing

00:14:36,079 --> 00:14:40,240
or dna sequencing data

00:14:37,680 --> 00:14:41,440
but i think the new world is also

00:14:40,240 --> 00:14:43,040
augmenting that

00:14:41,440 --> 00:14:44,639
by bringing in interesting

00:14:43,040 --> 00:14:47,360
characteristics from

00:14:44,639 --> 00:14:48,880
data that can be collected from images

00:14:47,360 --> 00:14:50,560
audio and video data feed

00:14:48,880 --> 00:14:52,320
so we can extract certain information

00:14:50,560 --> 00:14:54,880
there and see the correlation

00:14:52,320 --> 00:14:55,440
of that to certain conditions we can do

00:14:54,880 --> 00:14:58,560
this

00:14:55,440 --> 00:14:59,360
on edge devices like smartphone for

00:14:58,560 --> 00:15:01,839
example

00:14:59,360 --> 00:15:03,040
and we can predict using ai models on

00:15:01,839 --> 00:15:05,680
the device

00:15:03,040 --> 00:15:07,440
and if privacy is of importance we can

00:15:05,680 --> 00:15:09,120
leave the data on the device and

00:15:07,440 --> 00:15:10,560
improve the model on the edge using

00:15:09,120 --> 00:15:13,360
federated learning

00:15:10,560 --> 00:15:14,639
we will see many sensors at home and at

00:15:13,360 --> 00:15:15,839
hospitals in the future

00:15:14,639 --> 00:15:17,760
that is collecting interesting

00:15:15,839 --> 00:15:19,360
information like this

00:15:17,760 --> 00:15:21,120
then finally of course we can use

00:15:19,360 --> 00:15:22,079
machine learning if you can centralize

00:15:21,120 --> 00:15:24,240
the data

00:15:22,079 --> 00:15:25,920
to actually build certain models and

00:15:24,240 --> 00:15:28,880
then compress them back and send it down

00:15:25,920 --> 00:15:31,519
to these edge devices

00:15:28,880 --> 00:15:32,560
so i'll give you a few examples of what

00:15:31,519 --> 00:15:37,040
this looks like

00:15:32,560 --> 00:15:39,440
so male psoriasis is an example of where

00:15:37,040 --> 00:15:40,320
basically you have certain discoloration

00:15:39,440 --> 00:15:42,399
of

00:15:40,320 --> 00:15:44,079
your fingernail and it's the growth of

00:15:42,399 --> 00:15:46,399
psoriasis the condition

00:15:44,079 --> 00:15:48,639
so it can be measured looking at a

00:15:46,399 --> 00:15:51,680
fingernail by separating it into

00:15:48,639 --> 00:15:54,560
four quadrants and seeing if there is

00:15:51,680 --> 00:15:56,160
evidence of psoriasis changes on each of

00:15:54,560 --> 00:15:58,079
the four patterns

00:15:56,160 --> 00:15:59,519
then you add up all of the four

00:15:58,079 --> 00:16:01,199
quadrants across

00:15:59,519 --> 00:16:03,360
four of your fingernails so you exclude

00:16:01,199 --> 00:16:05,600
the thumb and then you come up with a

00:16:03,360 --> 00:16:08,079
score from 0 to 16

00:16:05,600 --> 00:16:09,360
and based on the score you know whether

00:16:08,079 --> 00:16:11,680
the person actually has

00:16:09,360 --> 00:16:13,199
psoriasis that continues to grow and

00:16:11,680 --> 00:16:13,759
this is an old-school way of doing

00:16:13,199 --> 00:16:16,639
things

00:16:13,759 --> 00:16:18,000
it is simple it was standardized and it

00:16:16,639 --> 00:16:18,720
actually is pretty effective and it's

00:16:18,000 --> 00:16:20,880
being used

00:16:18,720 --> 00:16:23,440
the only caveat is that this requires

00:16:20,880 --> 00:16:26,639
you to go to the hospital with a doctor

00:16:23,440 --> 00:16:30,160
every time you need the diagnosis done

00:16:26,639 --> 00:16:32,639
so not imagine can we ask the question

00:16:30,160 --> 00:16:33,920
is this automatable so what if we can

00:16:32,639 --> 00:16:36,160
take a picture

00:16:33,920 --> 00:16:37,680
of the four fingernails and use a

00:16:36,160 --> 00:16:40,560
computer vision model

00:16:37,680 --> 00:16:41,759
to see the discoloration and compute on

00:16:40,560 --> 00:16:44,160
the smartphone

00:16:41,759 --> 00:16:45,040
the navsea sport the the four quadrants

00:16:44,160 --> 00:16:48,160
course

00:16:45,040 --> 00:16:52,000
and uh do what a clinical site

00:16:48,160 --> 00:16:55,839
person would do now we all know

00:16:52,000 --> 00:16:58,079
that there is already enough ai

00:16:55,839 --> 00:16:59,920
power or sort of you know technologies

00:16:58,079 --> 00:17:01,040
that allow us to like look at certain

00:16:59,920 --> 00:17:03,680
vision images

00:17:01,040 --> 00:17:04,480
and be able to understand what's in the

00:17:03,680 --> 00:17:06,880
image

00:17:04,480 --> 00:17:08,400
uh this is this will probably be a cnn

00:17:06,880 --> 00:17:09,439
convolutional neural net that we can

00:17:08,400 --> 00:17:11,919
implement

00:17:09,439 --> 00:17:12,640
but the question to ask is can we do

00:17:11,919 --> 00:17:14,799
better

00:17:12,640 --> 00:17:15,679
than those four quadrant way of looking

00:17:14,799 --> 00:17:18,319
at things

00:17:15,679 --> 00:17:19,360
what if we could look at and observe

00:17:18,319 --> 00:17:22,559
growth

00:17:19,360 --> 00:17:24,640
much in much smaller regions

00:17:22,559 --> 00:17:26,000
within the nails and actually understand

00:17:24,640 --> 00:17:28,079
the growth of psoriasis

00:17:26,000 --> 00:17:29,039
and come up with a better score if need

00:17:28,079 --> 00:17:31,679
be so

00:17:29,039 --> 00:17:32,480
our ability to improve on the clinical

00:17:31,679 --> 00:17:35,039
grade of the

00:17:32,480 --> 00:17:36,080
old school methods the manual way of

00:17:35,039 --> 00:17:38,400
data collection

00:17:36,080 --> 00:17:40,320
the simplistic and subjective ways of

00:17:38,400 --> 00:17:42,400
measuring the nazi scoring

00:17:40,320 --> 00:17:43,679
into something that is much more

00:17:42,400 --> 00:17:46,960
objective

00:17:43,679 --> 00:17:49,039
automated nuanced high fidelity

00:17:46,960 --> 00:17:50,960
and that can be done at home without

00:17:49,039 --> 00:17:53,520
requiring continuous

00:17:50,960 --> 00:17:55,679
sort of site visits is what is exciting

00:17:53,520 --> 00:17:58,160
about this and this is a great example

00:17:55,679 --> 00:18:00,559
of one such digital biomarker that can

00:17:58,160 --> 00:18:02,640
be done on the phone

00:18:00,559 --> 00:18:03,760
let me give you another example and this

00:18:02,640 --> 00:18:06,799
is in fact something

00:18:03,760 --> 00:18:09,120
we have launched as a study and by study

00:18:06,799 --> 00:18:11,600
it's a clinical trial

00:18:09,120 --> 00:18:12,400
myasthenia gravis is a neuromuscular

00:18:11,600 --> 00:18:15,360
disease

00:18:12,400 --> 00:18:15,919
that is really relatively rare uh and

00:18:15,360 --> 00:18:19,200
it's so

00:18:15,919 --> 00:18:23,200
hard to find and recruit people but on

00:18:19,200 --> 00:18:25,280
the phone it is much easier to recruit

00:18:23,200 --> 00:18:27,120
because you can open it up to anyone in

00:18:25,280 --> 00:18:28,960
the country to participate

00:18:27,120 --> 00:18:30,720
so even though there are fewer people

00:18:28,960 --> 00:18:32,880
with the condition but the fact that i

00:18:30,720 --> 00:18:35,280
can look at a wide geography

00:18:32,880 --> 00:18:36,320
and let the convenience of a smartphone

00:18:35,280 --> 00:18:40,480
at home

00:18:36,320 --> 00:18:42,320
let them participate makes it a lot more

00:18:40,480 --> 00:18:43,679
tenable for people wanting to like you

00:18:42,320 --> 00:18:46,000
know participate here

00:18:43,679 --> 00:18:46,799
so in this study what we are looking for

00:18:46,000 --> 00:18:50,240
are subtle

00:18:46,799 --> 00:18:50,640
changes uh in the voice and also things

00:18:50,240 --> 00:18:53,919
like

00:18:50,640 --> 00:18:55,200
eyeline eyelid drooping uh as markers

00:18:53,919 --> 00:18:57,360
for the condition

00:18:55,200 --> 00:18:58,400
the hypothesis of the study is that a

00:18:57,360 --> 00:18:59,919
selfie video

00:18:58,400 --> 00:19:01,520
that captures both facial

00:18:59,919 --> 00:19:04,720
characteristics and

00:19:01,520 --> 00:19:05,679
auditory or voice characteristics can

00:19:04,720 --> 00:19:08,160
lead to a much

00:19:05,679 --> 00:19:09,679
better score of clinical progression or

00:19:08,160 --> 00:19:12,559
regression of the disease

00:19:09,679 --> 00:19:13,760
allowing people to record selfie uh in a

00:19:12,559 --> 00:19:16,000
daily on a

00:19:13,760 --> 00:19:16,880
daily or a weekly basis with some

00:19:16,000 --> 00:19:20,080
frequency with

00:19:16,880 --> 00:19:21,520
some common phrases utterances whenever

00:19:20,080 --> 00:19:23,840
they would like to do it at their

00:19:21,520 --> 00:19:25,120
convenience would allow us to actually

00:19:23,840 --> 00:19:26,640
bring that data

00:19:25,120 --> 00:19:28,720
with the right options and the right

00:19:26,640 --> 00:19:29,760
privacy constraints and understand

00:19:28,720 --> 00:19:32,080
what's going to happen

00:19:29,760 --> 00:19:33,760
inside the data to the condition and

00:19:32,080 --> 00:19:35,520
this is actually transforming research

00:19:33,760 --> 00:19:38,400
in my opinion

00:19:35,520 --> 00:19:39,440
so here's what we have observed right so

00:19:38,400 --> 00:19:41,919
the plots

00:19:39,440 --> 00:19:42,720
show on the left basically show that

00:19:41,919 --> 00:19:45,919
there is

00:19:42,720 --> 00:19:48,960
the eye height to width ratio for

00:19:45,919 --> 00:19:49,840
three participants so on the all the way

00:19:48,960 --> 00:19:52,000
on the left

00:19:49,840 --> 00:19:53,200
are control groups of patients who do

00:19:52,000 --> 00:19:56,400
not have

00:19:53,200 --> 00:19:58,480
myasthenia gravis and on the right uh

00:19:56,400 --> 00:20:00,080
images of the eyes you will see that

00:19:58,480 --> 00:20:02,159
they are

00:20:00,080 --> 00:20:04,000
they are thoughts from the basically

00:20:02,159 --> 00:20:05,919
their images of people with

00:20:04,000 --> 00:20:08,000
my senior graphics of course these are

00:20:05,919 --> 00:20:10,960
uh not the real images these are

00:20:08,000 --> 00:20:11,919
uh public images that being used uh not

00:20:10,960 --> 00:20:13,840
from the study

00:20:11,919 --> 00:20:15,840
but what you're seeing there is that we

00:20:13,840 --> 00:20:18,720
are able to look at the ratio

00:20:15,840 --> 00:20:20,080
and train a model of the height to width

00:20:18,720 --> 00:20:22,159
of the eyelid

00:20:20,080 --> 00:20:24,320
and compare that both on the left and

00:20:22,159 --> 00:20:27,360
the right and then see if there are lags

00:20:24,320 --> 00:20:28,000
so that allows us to understand if a

00:20:27,360 --> 00:20:31,280
person with

00:20:28,000 --> 00:20:33,280
mg actually indeed has certain of those

00:20:31,280 --> 00:20:35,360
symptom characteristics and now we can

00:20:33,280 --> 00:20:38,880
ask the question can it be predictable

00:20:35,360 --> 00:20:40,080
based on certain other phenotypic and

00:20:38,880 --> 00:20:42,159
other types of data that we are

00:20:40,080 --> 00:20:45,200
collecting as part of the study

00:20:42,159 --> 00:20:46,880
similarly we are evaluating if there are

00:20:45,200 --> 00:20:49,600
speech pattern differences

00:20:46,880 --> 00:20:52,000
for certain keywords that you know these

00:20:49,600 --> 00:20:54,960
patients actually utter on a daily basis

00:20:52,000 --> 00:20:56,480
and we are trying to evaluate to see if

00:20:54,960 --> 00:20:59,039
there are ways to

00:20:56,480 --> 00:21:00,799
predict somebody with certain peach

00:20:59,039 --> 00:21:01,520
speech occurrences could have patterns

00:21:00,799 --> 00:21:04,480
for png

00:21:01,520 --> 00:21:07,360
and we are indeed seeing some some um

00:21:04,480 --> 00:21:10,400
sort of you know correlations there

00:21:07,360 --> 00:21:12,720
so but where is python here so

00:21:10,400 --> 00:21:13,440
as expected the study models are

00:21:12,720 --> 00:21:15,120
actually written

00:21:13,440 --> 00:21:16,640
in python and in this case using

00:21:15,120 --> 00:21:18,799
tensorflow uh

00:21:16,640 --> 00:21:19,919
which is a python uh library that we use

00:21:18,799 --> 00:21:23,360
extensively at uh

00:21:19,919 --> 00:21:25,120
checkhead it is a custom neural network

00:21:23,360 --> 00:21:27,520
based on top of convolutional neural

00:21:25,120 --> 00:21:28,799
network to understand from the temporal

00:21:27,520 --> 00:21:30,880
data of the video feed

00:21:28,799 --> 00:21:31,919
so this is the the model that we are

00:21:30,880 --> 00:21:34,720
actually using for

00:21:31,919 --> 00:21:36,960
understanding the the droopiness of the

00:21:34,720 --> 00:21:39,200
eyelid

00:21:36,960 --> 00:21:40,320
similarly the audio model is also a

00:21:39,200 --> 00:21:42,640
python

00:21:40,320 --> 00:21:44,320
based model based on tensorflow that

00:21:42,640 --> 00:21:46,960
analyzes the speech

00:21:44,320 --> 00:21:48,799
uh using a convolutional neural net 2d

00:21:46,960 --> 00:21:50,720
convolutional neural net

00:21:48,799 --> 00:21:52,559
on top of a spectrogram so we take the

00:21:50,720 --> 00:21:53,280
audio data and convert that to a vision

00:21:52,559 --> 00:21:55,520
problem

00:21:53,280 --> 00:21:57,440
which is pretty standard in how cnns are

00:21:55,520 --> 00:21:59,919
being used for understanding speech

00:21:57,440 --> 00:22:01,440
and of course there's a certain custom

00:21:59,919 --> 00:22:03,760
ffts that allow us to

00:22:01,440 --> 00:22:04,880
extract certain information what i want

00:22:03,760 --> 00:22:06,720
to caution here though

00:22:04,880 --> 00:22:09,360
is that these models require

00:22:06,720 --> 00:22:12,080
extraordinary clinical rigor

00:22:09,360 --> 00:22:12,880
this is this is a study and it's irv

00:22:12,080 --> 00:22:15,280
approved

00:22:12,880 --> 00:22:16,159
and is run and validated by clinicians

00:22:15,280 --> 00:22:17,679
on staff

00:22:16,159 --> 00:22:19,760
so one thing to be aware of in

00:22:17,679 --> 00:22:22,400
healthcare is you just cannot

00:22:19,760 --> 00:22:24,640
build a model and launch it you need to

00:22:22,400 --> 00:22:26,480
go through the rigor of clinical testing

00:22:24,640 --> 00:22:28,559
and bringing in the safety including the

00:22:26,480 --> 00:22:30,880
patient safety and data safety

00:22:28,559 --> 00:22:34,240
in order to you know bring it to market

00:22:30,880 --> 00:22:35,919
so that's what we are working on

00:22:34,240 --> 00:22:38,000
and the final example i want to point

00:22:35,919 --> 00:22:39,120
you to is towards what the community has

00:22:38,000 --> 00:22:41,840
felt

00:22:39,120 --> 00:22:42,640
when the pandemic started many asked the

00:22:41,840 --> 00:22:45,679
question

00:22:42,640 --> 00:22:48,960
could we detect cough sounds using

00:22:45,679 --> 00:22:51,760
phones or devices you can build such a

00:22:48,960 --> 00:22:54,159
model using python and tensorflow lite

00:22:51,760 --> 00:22:56,400
with the micro speech model so if you

00:22:54,159 --> 00:22:59,919
google you'll see enough examples of

00:22:56,400 --> 00:23:03,200
people having built covet based

00:22:59,919 --> 00:23:05,039
or even just cough detection models

00:23:03,200 --> 00:23:07,120
the intuition of how to go about doing

00:23:05,039 --> 00:23:09,679
this is what i'll explain here

00:23:07,120 --> 00:23:11,360
and it's by doing transfer learning on

00:23:09,679 --> 00:23:13,200
the micro speech model

00:23:11,360 --> 00:23:15,679
and training it to learn cough sounds so

00:23:13,200 --> 00:23:18,640
micro speech model is a model that's

00:23:15,679 --> 00:23:19,919
uh open source and available uh on

00:23:18,640 --> 00:23:23,440
google collab

00:23:19,919 --> 00:23:23,840
and it basically has learned yes no

00:23:23,440 --> 00:23:26,480
words

00:23:23,840 --> 00:23:27,840
you can always extend it to train on

00:23:26,480 --> 00:23:29,919
other types of words

00:23:27,840 --> 00:23:31,840
including a cough word so you would

00:23:29,919 --> 00:23:34,159
collect enough cough samples

00:23:31,840 --> 00:23:34,960
and label them as cough the hardest part

00:23:34,159 --> 00:23:37,520
in health care

00:23:34,960 --> 00:23:38,559
is how do you go about collecting the

00:23:37,520 --> 00:23:40,240
critical data

00:23:38,559 --> 00:23:41,760
to make this happen and that's where you

00:23:40,240 --> 00:23:44,960
need to invest quite a bit

00:23:41,760 --> 00:23:46,960
in app experiences and trust and safety

00:23:44,960 --> 00:23:48,400
with the users to let them participate

00:23:46,960 --> 00:23:50,400
in this

00:23:48,400 --> 00:23:52,640
then once you have enough labeled cough

00:23:50,400 --> 00:23:53,760
data you retrain the model with newly

00:23:52,640 --> 00:23:56,559
added labels

00:23:53,760 --> 00:23:58,400
and added uh add the cost data on top of

00:23:56,559 --> 00:24:00,240
that to like train the model

00:23:58,400 --> 00:24:01,520
then of course you need to validate the

00:24:00,240 --> 00:24:03,840
accuracy of the model

00:24:01,520 --> 00:24:05,600
um and maybe you have certain additional

00:24:03,840 --> 00:24:08,080
data sets that you use for

00:24:05,600 --> 00:24:09,679
testing and validation now what you have

00:24:08,080 --> 00:24:10,640
out of that notebook all of this can be

00:24:09,679 --> 00:24:13,440
done on the google

00:24:10,640 --> 00:24:14,720
notebook that's been linked on the slide

00:24:13,440 --> 00:24:17,279
you can actually

00:24:14,720 --> 00:24:18,880
generate a tf lite model which is

00:24:17,279 --> 00:24:21,039
tensorflow lite model

00:24:18,880 --> 00:24:22,559
and the lite model actually lets you run

00:24:21,039 --> 00:24:24,400
it on the phone so we have built some

00:24:22,559 --> 00:24:25,919
tools like tensorio that lets you take

00:24:24,400 --> 00:24:26,559
that model and run it on the phone if

00:24:25,919 --> 00:24:29,279
you need

00:24:26,559 --> 00:24:31,360
but where it can go even further is

00:24:29,279 --> 00:24:33,679
taking the tensorflow lite model

00:24:31,360 --> 00:24:34,480
and basically compiling that to a

00:24:33,679 --> 00:24:37,039
tensorflow

00:24:34,480 --> 00:24:38,640
micro model again that the notebook lets

00:24:37,039 --> 00:24:39,360
you what walks you through how to do

00:24:38,640 --> 00:24:41,760
that

00:24:39,360 --> 00:24:42,880
uh and actually deploying that on

00:24:41,760 --> 00:24:44,720
embedded devices

00:24:42,880 --> 00:24:47,120
uh so devices that's beyond the phone

00:24:44,720 --> 00:24:49,039
where tensorflow lite doesn't run

00:24:47,120 --> 00:24:51,360
and there are additional links out there

00:24:49,039 --> 00:24:53,760
that you can actually

00:24:51,360 --> 00:24:54,480
look look at how it's been done the

00:24:53,760 --> 00:24:57,760
question that

00:24:54,480 --> 00:25:00,480
always comes up though is how can we

00:24:57,760 --> 00:25:02,400
improve a model like this but by keeping

00:25:00,480 --> 00:25:03,840
the data private you don't want the

00:25:02,400 --> 00:25:06,960
user's data

00:25:03,840 --> 00:25:09,039
to be centralized on a server other than

00:25:06,960 --> 00:25:11,440
the fact that you built certain baseline

00:25:09,039 --> 00:25:13,360
model data for building the model right

00:25:11,440 --> 00:25:15,440
so

00:25:13,360 --> 00:25:17,200
so there are some tools at our disposal

00:25:15,440 --> 00:25:19,360
to take privacy seriously

00:25:17,200 --> 00:25:20,240
so federated learning differential

00:25:19,360 --> 00:25:22,559
privacy

00:25:20,240 --> 00:25:23,360
and encryption techniques like including

00:25:22,559 --> 00:25:25,440
homomorphic

00:25:23,360 --> 00:25:28,080
encryption and secure multi-party

00:25:25,440 --> 00:25:28,559
compute are some tools in our back to do

00:25:28,080 --> 00:25:30,799
that

00:25:28,559 --> 00:25:32,080
now not all of them run on the phone

00:25:30,799 --> 00:25:34,480
given the complexity

00:25:32,080 --> 00:25:35,360
of the things but what i can tell you is

00:25:34,480 --> 00:25:37,760
that with

00:25:35,360 --> 00:25:38,720
federated learning we've already tracked

00:25:37,760 --> 00:25:40,880
that problem

00:25:38,720 --> 00:25:41,760
where you can run this on both ios and

00:25:40,880 --> 00:25:43,679
android phones

00:25:41,760 --> 00:25:44,880
so you can keep the data on the edge

00:25:43,679 --> 00:25:46,640
usually the phone

00:25:44,880 --> 00:25:49,120
and just improve the prediction scores

00:25:46,640 --> 00:25:50,080
on the model by aggregating the tensors

00:25:49,120 --> 00:25:52,480
or the gradients

00:25:50,080 --> 00:25:54,480
on the cloud so you have a model running

00:25:52,480 --> 00:25:56,559
on multiple users phones

00:25:54,480 --> 00:25:58,559
they are doing predictions but then you

00:25:56,559 --> 00:25:59,840
could train the model individually on

00:25:58,559 --> 00:26:01,919
these phones

00:25:59,840 --> 00:26:03,760
to new data that the user is collecting

00:26:01,919 --> 00:26:06,799
that's just on the phone

00:26:03,760 --> 00:26:09,120
and as you do that you can then

00:26:06,799 --> 00:26:10,480
then have the model send the gradients

00:26:09,120 --> 00:26:13,120
or basically the trained

00:26:10,480 --> 00:26:14,559
new weights back to a server and you

00:26:13,120 --> 00:26:15,360
aggregate those weights in different

00:26:14,559 --> 00:26:18,080
ways

00:26:15,360 --> 00:26:18,640
and you have a new model empirically the

00:26:18,080 --> 00:26:20,320
question is

00:26:18,640 --> 00:26:22,159
is this model better and what we are

00:26:20,320 --> 00:26:25,039
seeing in in real world is some of these

00:26:22,159 --> 00:26:27,039
models are actually improving

00:26:25,039 --> 00:26:29,120
so we have built tools that allow you to

00:26:27,039 --> 00:26:29,840
do this all of this again is icon based

00:26:29,120 --> 00:26:31,760
so

00:26:29,840 --> 00:26:34,159
you can look at our github repository

00:26:31,760 --> 00:26:36,720
but i'll also call your attention to

00:26:34,159 --> 00:26:37,679
the fantastic team at open mind that

00:26:36,720 --> 00:26:40,240
that is building

00:26:37,679 --> 00:26:42,159
like a lot of these tools uh using

00:26:40,240 --> 00:26:43,760
python so they have done some phenomenal

00:26:42,159 --> 00:26:46,000
work that i'll encourage everyone to

00:26:43,760 --> 00:26:48,720
look at

00:26:46,000 --> 00:26:50,799
so the final key takeaway here is that

00:26:48,720 --> 00:26:53,200
the world of healthcare has moved from a

00:26:50,799 --> 00:26:54,400
processing first thinking to a compute

00:26:53,200 --> 00:26:57,520
first thinking

00:26:54,400 --> 00:26:59,039
compute can be anywhere cloud phones or

00:26:57,520 --> 00:27:01,840
tiny sensors

00:26:59,039 --> 00:27:02,080
compute allows you to build gradients

00:27:01,840 --> 00:27:04,640
and

00:27:02,080 --> 00:27:06,640
intelligence on top of the data and

00:27:04,640 --> 00:27:08,159
privacy will be a first-class citizen

00:27:06,640 --> 00:27:10,720
when it comes to healthcare

00:27:08,159 --> 00:27:11,679
and the good news is that every one of

00:27:10,720 --> 00:27:13,360
you here

00:27:11,679 --> 00:27:15,840
with your python knowledge and

00:27:13,360 --> 00:27:16,559
background can actually participate in

00:27:15,840 --> 00:27:20,640
healthcare

00:27:16,559 --> 00:27:22,480
it all starts by building notebooks

00:27:20,640 --> 00:27:24,320
so i want to end this presentation by

00:27:22,480 --> 00:27:27,440
motivating you to think about some

00:27:24,320 --> 00:27:27,840
hard but tractable problems um a lot of

00:27:27,440 --> 00:27:30,320
which

00:27:27,840 --> 00:27:31,840
healthcare has to offer where your

00:27:30,320 --> 00:27:32,720
python skills will actually make a

00:27:31,840 --> 00:27:34,799
difference

00:27:32,720 --> 00:27:37,360
so privacy on the edge and biometric

00:27:34,799 --> 00:27:37,760
markers on health are just two examples

00:27:37,360 --> 00:27:40,240
of

00:27:37,760 --> 00:27:42,000
what i'm involved in but there are other

00:27:40,240 --> 00:27:45,200
interesting ideas such as

00:27:42,000 --> 00:27:47,279
nlp for text understanding on

00:27:45,200 --> 00:27:49,360
doctor's notes or nurses notes for

00:27:47,279 --> 00:27:52,080
example or understanding like

00:27:49,360 --> 00:27:52,399
you know call center data for people who

00:27:52,080 --> 00:27:54,559
are

00:27:52,399 --> 00:27:55,440
actually talking about their healthcare

00:27:54,559 --> 00:27:57,600
concerns

00:27:55,440 --> 00:27:59,760
and may and of course we are seeing like

00:27:57,600 --> 00:28:00,720
uh workspace systems now going into

00:27:59,760 --> 00:28:04,159
provider

00:28:00,720 --> 00:28:06,720
uh places where they're using uh voice

00:28:04,159 --> 00:28:07,840
to like translate notes into structured

00:28:06,720 --> 00:28:10,320
data so there are

00:28:07,840 --> 00:28:12,159
interesting problems and a lot of that

00:28:10,320 --> 00:28:13,919
can actually start with just a python

00:28:12,159 --> 00:28:17,200
notebook and i would love to like see

00:28:13,919 --> 00:28:18,960
many of you like take on such challenges

00:28:17,200 --> 00:28:21,120
thank you so much if you're interested

00:28:18,960 --> 00:28:22,799
with what we do please follow us

00:28:21,120 --> 00:28:24,320
and i hope all of you are having a

00:28:22,799 --> 00:28:27,600
fantastic time at pycon

00:28:24,320 --> 00:28:27,600

YouTube URL: https://www.youtube.com/watch?v=Jmly1Jfbhak


