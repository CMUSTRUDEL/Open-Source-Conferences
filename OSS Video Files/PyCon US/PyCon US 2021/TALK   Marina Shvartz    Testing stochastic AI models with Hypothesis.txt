Title: TALK   Marina Shvartz    Testing stochastic AI models with Hypothesis
Publication date: 2021-05-29
Playlist: PyCon US 2021
Description: 
	Over the years, testing has become one of the main focus areas in development teams, a good feature is a well tested one. In the field of AI this is many times a real struggle. Since eventually most advanced AI models are stochastic - we canâ€™t manually define all their possible edge cases. This led us to use the hypothesis library which does a lot of that for you, while you can focus on defining the properties and specifications of your system.

In this talk, I will cover shortly the theory of property-based testing and then jump into use cases and examples to demonstrate how we used the hypothesis library to generate random examples of plausible edge cases of our AI model.
Captions: 
	00:00:04,170 --> 00:00:11,869
[Music]

00:00:15,200 --> 00:00:18,080
hi

00:00:16,160 --> 00:00:21,840
my name is marina and this is testing

00:00:18,080 --> 00:00:24,000
stochastic ai models with hypotheses

00:00:21,840 --> 00:00:27,279
before i dive into the talk i want to

00:00:24,000 --> 00:00:29,519
share with you a bit about myself

00:00:27,279 --> 00:00:30,880
i live in tel aviv and i've been working

00:00:29,519 --> 00:00:33,680
as a software engineer

00:00:30,880 --> 00:00:34,320
for about 10 years working on different

00:00:33,680 --> 00:00:36,640
domains

00:00:34,320 --> 00:00:38,000
both in front and back end engineering

00:00:36,640 --> 00:00:41,200
and team leading

00:00:38,000 --> 00:00:43,200
enjoying the different challenges

00:00:41,200 --> 00:00:46,800
in the last two years i've been working

00:00:43,200 --> 00:00:48,800
as senior ai software architect at adoc

00:00:46,800 --> 00:00:50,559
in just a few words about adoc as it

00:00:48,800 --> 00:00:52,320
will be relevant at the last section of

00:00:50,559 --> 00:00:54,640
the talk

00:00:52,320 --> 00:00:57,440
in adoc we develop advanced healthcare

00:00:54,640 --> 00:01:00,160
ai based decision support software

00:00:57,440 --> 00:01:00,960
we analyze medical imaging to provide

00:01:00,160 --> 00:01:03,120
solutions

00:01:00,960 --> 00:01:04,799
for flagging acute abnormalities across

00:01:03,120 --> 00:01:06,960
the body

00:01:04,799 --> 00:01:09,119
our aim is to help radiologists

00:01:06,960 --> 00:01:12,240
prioritize life threatening in cases

00:01:09,119 --> 00:01:14,560
and expedite patient care

00:01:12,240 --> 00:01:16,640
clearly the correctness of such programs

00:01:14,560 --> 00:01:18,400
is that is just as important as using

00:01:16,640 --> 00:01:20,640
the best algorithm to find those

00:01:18,400 --> 00:01:22,880
abnormalities

00:01:20,640 --> 00:01:24,320
as a leading company in that field

00:01:22,880 --> 00:01:25,119
working working with hundreds of

00:01:24,320 --> 00:01:27,920
hospitals

00:01:25,119 --> 00:01:30,479
scalability reliability and robustness

00:01:27,920 --> 00:01:33,119
is a real challenge

00:01:30,479 --> 00:01:35,920
my job together with my team is to build

00:01:33,119 --> 00:01:38,720
systems in these standards

00:01:35,920 --> 00:01:40,720
in general over the years testing has

00:01:38,720 --> 00:01:42,560
become one of the main focus areas in

00:01:40,720 --> 00:01:45,759
development teams

00:01:42,560 --> 00:01:48,079
a good feature is a well tested one

00:01:45,759 --> 00:01:50,079
in the field of ai this is many times a

00:01:48,079 --> 00:01:52,240
real struggle

00:01:50,079 --> 00:01:53,680
since eventually most advanced ai models

00:01:52,240 --> 00:01:55,920
are stochastic

00:01:53,680 --> 00:01:58,320
we can't manually define other possible

00:01:55,920 --> 00:01:59,840
edge cases

00:01:58,320 --> 00:02:03,840
this led us to explore the best

00:01:59,840 --> 00:02:03,840
solutions for testing our software

00:02:04,640 --> 00:02:08,560
this talk consists of three parts the

00:02:06,960 --> 00:02:10,239
first part will be mainly theory of

00:02:08,560 --> 00:02:13,120
testing mythologies in the search for

00:02:10,239 --> 00:02:14,959
the best approach for machine learning

00:02:13,120 --> 00:02:18,080
the second part i will overview the

00:02:14,959 --> 00:02:20,080
hypothesis library with code examples

00:02:18,080 --> 00:02:21,840
and in the last section i will try to

00:02:20,080 --> 00:02:24,080
give you a taste on how we use the

00:02:21,840 --> 00:02:27,280
theory together the hypothesis library

00:02:24,080 --> 00:02:27,280
to test our software

00:02:27,599 --> 00:02:32,800
so how do we write tests the most common

00:02:30,959 --> 00:02:33,760
and used approach is example-based

00:02:32,800 --> 00:02:35,519
testing

00:02:33,760 --> 00:02:36,959
or normal testing as we like to call

00:02:35,519 --> 00:02:39,599
them

00:02:36,959 --> 00:02:42,160
in example-based testing each test sets

00:02:39,599 --> 00:02:43,599
up a single concrete input example

00:02:42,160 --> 00:02:47,760
and it starts to check whether the

00:02:43,599 --> 00:02:49,680
produced output matches our expectations

00:02:47,760 --> 00:02:51,840
we usually aim to think really hard to

00:02:49,680 --> 00:02:52,800
find those edge cases that will fail the

00:02:51,840 --> 00:02:55,040
program

00:02:52,800 --> 00:02:56,480
and once we do we fix the problem

00:02:55,040 --> 00:02:58,959
accordingly and continue

00:02:56,480 --> 00:03:01,920
adding more examples until we feel that

00:02:58,959 --> 00:03:04,000
we covered all of the edge cases

00:03:01,920 --> 00:03:05,680
i must share that i always felt i'm

00:03:04,000 --> 00:03:07,599
really bad at that task

00:03:05,680 --> 00:03:09,519
and it was really time consuming and

00:03:07,599 --> 00:03:13,120
never feeling satisfied that i really

00:03:09,519 --> 00:03:14,800
covered all of the edge cases

00:03:13,120 --> 00:03:17,120
and let's do an example to show you what

00:03:14,800 --> 00:03:19,120
i mean

00:03:17,120 --> 00:03:21,120
we have a function called merge start

00:03:19,120 --> 00:03:24,720
that merges two lists into a single

00:03:21,120 --> 00:03:27,280
sorted list without duplicate elements

00:03:24,720 --> 00:03:28,720
here i wrote multiple unit tests trying

00:03:27,280 --> 00:03:31,200
to cover edge cases

00:03:28,720 --> 00:03:32,080
like empty lists list with duplicate

00:03:31,200 --> 00:03:35,040
values

00:03:32,080 --> 00:03:35,840
list with multiple values it's quite

00:03:35,040 --> 00:03:38,080
easy to say

00:03:35,840 --> 00:03:41,280
there are still many missing edge cases

00:03:38,080 --> 00:03:43,120
like negative numbers or maybe nulls

00:03:41,280 --> 00:03:45,920
and that takes me to the problem of that

00:03:43,120 --> 00:03:46,959
approach and it is that it requires us

00:03:45,920 --> 00:03:50,000
the developers

00:03:46,959 --> 00:03:52,959
to come up with specific examples

00:03:50,000 --> 00:03:54,319
we may choose to write 5 or 10 or 100

00:03:52,959 --> 00:03:56,720
test cases

00:03:54,319 --> 00:03:58,159
but still remain unsure whether whether

00:03:56,720 --> 00:04:01,360
we have safely covered

00:03:58,159 --> 00:04:02,480
most of the edge cases it's really time

00:04:01,360 --> 00:04:04,080
consuming

00:04:02,480 --> 00:04:07,200
and sometimes thinking about the edge

00:04:04,080 --> 00:04:09,439
cases is really hard

00:04:07,200 --> 00:04:11,280
in addition in many cases the

00:04:09,439 --> 00:04:13,680
requirements of the functions

00:04:11,280 --> 00:04:14,480
are not defined properly know how they

00:04:13,680 --> 00:04:17,359
should behave

00:04:14,480 --> 00:04:18,639
for different input types or unexpected

00:04:17,359 --> 00:04:20,799
inputs

00:04:18,639 --> 00:04:22,320
and the interpretation of the developer

00:04:20,799 --> 00:04:24,960
writing the tests

00:04:22,320 --> 00:04:26,720
might be ambiguous or unclear causing

00:04:24,960 --> 00:04:30,479
missing edge cases

00:04:26,720 --> 00:04:33,360
and ending up with non-robust tests

00:04:30,479 --> 00:04:35,199
at last we use these concrete examples

00:04:33,360 --> 00:04:38,639
to suggest a general claim

00:04:35,199 --> 00:04:40,080
about the system behavior to be more

00:04:38,639 --> 00:04:42,720
specific to the merger

00:04:40,080 --> 00:04:44,400
function there are multiple unclear

00:04:42,720 --> 00:04:45,759
questions to the definition of the

00:04:44,400 --> 00:04:48,800
function

00:04:45,759 --> 00:04:51,520
for example what data types in list

00:04:48,800 --> 00:04:54,160
should the function support are mixed

00:04:51,520 --> 00:04:56,080
data types a valid input

00:04:54,160 --> 00:04:59,040
how the function should behave if it

00:04:56,080 --> 00:05:01,120
gets nulled as input

00:04:59,040 --> 00:05:03,520
all these unclear questions may lead to

00:05:01,120 --> 00:05:06,160
missing test cases

00:05:03,520 --> 00:05:08,400
now think about ai software where the

00:05:06,160 --> 00:05:10,720
input of the functions in most cases

00:05:08,400 --> 00:05:12,400
are stochastic and really not easily

00:05:10,720 --> 00:05:14,160
predictable

00:05:12,400 --> 00:05:15,440
how would you think about edge cases

00:05:14,160 --> 00:05:17,199
there

00:05:15,440 --> 00:05:19,759
let me tell you that it's probably

00:05:17,199 --> 00:05:19,759
impossible

00:05:20,000 --> 00:05:24,400
that leads me to the next testing

00:05:21,440 --> 00:05:27,919
methodology the property-based testing

00:05:24,400 --> 00:05:30,240
where many of these issues are mitigated

00:05:27,919 --> 00:05:31,440
the property-based tester has to think

00:05:30,240 --> 00:05:34,320
very carefully

00:05:31,440 --> 00:05:36,080
about the specification starting from

00:05:34,320 --> 00:05:37,919
defining the possible inputs

00:05:36,080 --> 00:05:39,759
and really analyzing the properties of

00:05:37,919 --> 00:05:43,680
the behavior of the function

00:05:39,759 --> 00:05:45,759
and at last also the expected output

00:05:43,680 --> 00:05:47,360
the idea is that once you have a clear

00:05:45,759 --> 00:05:48,720
definition of them

00:05:47,360 --> 00:05:51,039
you can let the computer do the

00:05:48,720 --> 00:05:52,000
exhausting work of generating the random

00:05:51,039 --> 00:05:55,759
examples

00:05:52,000 --> 00:05:57,680
and tests that the properties are met

00:05:55,759 --> 00:06:01,440
to formulate this idea when designing

00:05:57,680 --> 00:06:04,319
tests the approach should be as follows

00:06:01,440 --> 00:06:06,720
under what preconditions and constraints

00:06:04,319 --> 00:06:10,479
should the functionality under test

00:06:06,720 --> 00:06:13,360
lead to particular post conditions

00:06:10,479 --> 00:06:15,360
which are the results of a computation

00:06:13,360 --> 00:06:16,000
and which environments in variance

00:06:15,360 --> 00:06:19,120
should never

00:06:16,000 --> 00:06:21,120
be violated the combination of

00:06:19,120 --> 00:06:21,759
preconditions and qualities are expected

00:06:21,120 --> 00:06:25,280
to be met

00:06:21,759 --> 00:06:26,720
is called a property i guess you

00:06:25,280 --> 00:06:28,800
probably wonder how you come up with

00:06:26,720 --> 00:06:30,720
these properties

00:06:28,800 --> 00:06:32,240
so there is actually quite a small but

00:06:30,720 --> 00:06:34,080
well well-known collection of

00:06:32,240 --> 00:06:35,759
property-based testing patterns

00:06:34,080 --> 00:06:37,280
that you can use to start rolling with

00:06:35,759 --> 00:06:40,639
the idea

00:06:37,280 --> 00:06:40,639
let's look at some examples

00:06:40,880 --> 00:06:44,080
one property usually referred as

00:06:42,479 --> 00:06:45,759
commentativity

00:06:44,080 --> 00:06:47,360
and this is when a change of order in

00:06:45,759 --> 00:06:50,560
applying the function

00:06:47,360 --> 00:06:51,759
should not change the final result here

00:06:50,560 --> 00:06:54,000
for example

00:06:51,759 --> 00:06:57,599
if we change the order in addition then

00:06:54,000 --> 00:06:57,599
the result value wouldn't change

00:06:57,840 --> 00:07:02,880
another property usually referred to as

00:06:59,840 --> 00:07:04,319
invariant function and this is when some

00:07:02,880 --> 00:07:07,440
properties of the code

00:07:04,319 --> 00:07:10,639
do not change after applying the logic

00:07:07,440 --> 00:07:12,000
for example sorting should not change

00:07:10,639 --> 00:07:16,000
the size of the collection

00:07:12,000 --> 00:07:17,599
nor the elements inside

00:07:16,000 --> 00:07:20,240
the last example of properties that i'm

00:07:17,599 --> 00:07:21,840
going to show is the test oracle

00:07:20,240 --> 00:07:23,520
and this is when an alternative

00:07:21,840 --> 00:07:24,400
implementation of the function under

00:07:23,520 --> 00:07:27,520
test

00:07:24,400 --> 00:07:29,120
should return the same output

00:07:27,520 --> 00:07:32,319
this is achieved by writing maybe

00:07:29,120 --> 00:07:34,000
simpler implementation but less optimal

00:07:32,319 --> 00:07:35,680
or it's very useful when testing

00:07:34,000 --> 00:07:37,759
refactor code and you compare the

00:07:35,680 --> 00:07:40,080
prerefactor code with the post refactor

00:07:37,759 --> 00:07:41,919
code

00:07:40,080 --> 00:07:43,840
as mentioned there are multiple more

00:07:41,919 --> 00:07:45,120
common properties that can be explored

00:07:43,840 --> 00:07:46,800
afterwards

00:07:45,120 --> 00:07:48,160
but really having these definitions of

00:07:46,800 --> 00:07:50,319
properties in mind

00:07:48,160 --> 00:07:53,520
really helps identifying the properties

00:07:50,319 --> 00:07:53,520
of the functioning test

00:07:55,120 --> 00:07:58,160
to summarize the purpose of

00:07:56,319 --> 00:07:59,599
property-based testing it is to

00:07:58,160 --> 00:08:01,680
generalize the scenarios

00:07:59,599 --> 00:08:03,599
by focusing on which features of the

00:08:01,680 --> 00:08:06,960
scenario are essential

00:08:03,599 --> 00:08:09,440
and which are allowed to vary the idea

00:08:06,960 --> 00:08:11,520
is that once you define the properties

00:08:09,440 --> 00:08:12,560
one property-based test runs hundreds of

00:08:11,520 --> 00:08:14,879
times with different

00:08:12,560 --> 00:08:16,560
inputs while it really tries to get the

00:08:14,879 --> 00:08:18,080
test to fail

00:08:16,560 --> 00:08:20,479
and it does it by passing all the

00:08:18,080 --> 00:08:24,080
passable possible edge cases

00:08:20,479 --> 00:08:26,160
empty lists negative numbers nulls

00:08:24,080 --> 00:08:28,720
basically it automates the most

00:08:26,160 --> 00:08:32,560
time-consuming part of writing tests

00:08:28,720 --> 00:08:32,560
coming up with specific examples

00:08:32,640 --> 00:08:36,399
to make it more clear let's go back to

00:08:34,479 --> 00:08:37,919
the merge sort function

00:08:36,399 --> 00:08:39,519
and actually all the three properties

00:08:37,919 --> 00:08:41,039
that i showed are relevant for this

00:08:39,519 --> 00:08:43,519
function

00:08:41,039 --> 00:08:45,279
the commutativity because i can call the

00:08:43,519 --> 00:08:46,080
function with a different order of the

00:08:45,279 --> 00:08:48,880
input

00:08:46,080 --> 00:08:50,480
and the result will stay the same the

00:08:48,880 --> 00:08:52,399
function is environment in the sense

00:08:50,480 --> 00:08:54,720
that the elements in the result are all

00:08:52,399 --> 00:08:57,279
included in the input

00:08:54,720 --> 00:08:59,120
and at last we can probably think of a

00:08:57,279 --> 00:09:00,959
different implementation

00:08:59,120 --> 00:09:03,360
for example using a different sorting

00:09:00,959 --> 00:09:05,760
function

00:09:03,360 --> 00:09:08,320
the conclusion you should take from this

00:09:05,760 --> 00:09:10,959
is that we don't always need to know

00:09:08,320 --> 00:09:12,560
the correct answer or even be able to

00:09:10,959 --> 00:09:14,240
check that the answer is correct in

00:09:12,560 --> 00:09:16,080
order to find bugs

00:09:14,240 --> 00:09:19,040
because we can check that the properties

00:09:16,080 --> 00:09:19,040
are met instead

00:09:19,600 --> 00:09:23,760
to complete our research for finding the

00:09:21,600 --> 00:09:26,240
best methodologies to writing tests

00:09:23,760 --> 00:09:28,320
for stochastic ai software we found

00:09:26,240 --> 00:09:28,800
there is an additional method that helps

00:09:28,320 --> 00:09:30,720
adding

00:09:28,800 --> 00:09:31,839
more coverage and completeness of the

00:09:30,720 --> 00:09:35,920
test

00:09:31,839 --> 00:09:38,000
and this is the metamorphic testing

00:09:35,920 --> 00:09:40,720
metamorphic testing was first introduced

00:09:38,000 --> 00:09:43,040
in 1998 in a technical report

00:09:40,720 --> 00:09:45,200
suggesting a new approach for generating

00:09:43,040 --> 00:09:46,880
test cases

00:09:45,200 --> 00:09:49,680
more than 10 years later it was

00:09:46,880 --> 00:09:51,760
published in a paper as a valid approach

00:09:49,680 --> 00:09:55,600
to test machine learning algorithms and

00:09:51,760 --> 00:09:57,680
became quite common in recent years

00:09:55,600 --> 00:09:59,760
this approach addresses the test oracle

00:09:57,680 --> 00:10:01,200
problem and test generation problem in

00:09:59,760 --> 00:10:03,760
machine learning

00:10:01,200 --> 00:10:05,519
where in most cases there is no test or

00:10:03,760 --> 00:10:09,360
code that can be used to verify the

00:10:05,519 --> 00:10:10,880
correctness of the computed outputs

00:10:09,360 --> 00:10:13,360
if you think about the definition of

00:10:10,880 --> 00:10:15,040
property-based testing which i just told

00:10:13,360 --> 00:10:18,079
you should realize that it's actually a

00:10:15,040 --> 00:10:20,800
property-based testing technique it

00:10:18,079 --> 00:10:22,560
gives additional testing possibilities

00:10:20,800 --> 00:10:25,279
in cases where we don't know how to

00:10:22,560 --> 00:10:27,040
relate inputs to outputs

00:10:25,279 --> 00:10:28,959
and rather than focusing on each

00:10:27,040 --> 00:10:30,320
individual combination of input and

00:10:28,959 --> 00:10:32,240
output

00:10:30,320 --> 00:10:34,880
metamorphic testing looks at multiple

00:10:32,240 --> 00:10:37,360
executions of the program

00:10:34,880 --> 00:10:38,640
the idea here is to define metamorphic

00:10:37,360 --> 00:10:40,880
relations

00:10:38,640 --> 00:10:43,040
which allow us to predict the output

00:10:40,880 --> 00:10:45,519
after applying a certain transformation

00:10:43,040 --> 00:10:47,760
on the input

00:10:45,519 --> 00:10:49,920
the corresponding test would be if the

00:10:47,760 --> 00:10:52,720
software reacts to changes in the input

00:10:49,920 --> 00:10:54,000
as it is expected

00:10:52,720 --> 00:10:56,079
and the most important thing about

00:10:54,000 --> 00:11:00,320
metamorphic testing is understanding

00:10:56,079 --> 00:11:02,640
what are metamorphic relations

00:11:00,320 --> 00:11:04,320
so we basically run the program multiple

00:11:02,640 --> 00:11:06,560
executions

00:11:04,320 --> 00:11:08,480
once on a certain input and the second

00:11:06,560 --> 00:11:10,160
time on some transformation of that

00:11:08,480 --> 00:11:12,880
input

00:11:10,160 --> 00:11:14,560
we then use metamorphic relations to

00:11:12,880 --> 00:11:19,040
test if the two outputs

00:11:14,560 --> 00:11:20,560
meet the defined relation for example

00:11:19,040 --> 00:11:22,079
if you have a search engine for

00:11:20,560 --> 00:11:24,000
restaurants and you search for

00:11:22,079 --> 00:11:26,160
restaurants near you

00:11:24,000 --> 00:11:27,760
you probably can't predict the answer

00:11:26,160 --> 00:11:28,480
but you if you add it to the search

00:11:27,760 --> 00:11:30,959
keywords

00:11:28,480 --> 00:11:32,640
asian and you search for asian

00:11:30,959 --> 00:11:34,240
restaurant near you

00:11:32,640 --> 00:11:38,079
you can predict that the answer will be

00:11:34,240 --> 00:11:38,079
a subset of the previous answer

00:11:38,480 --> 00:11:43,040
to give you some examples of metamorphic

00:11:40,880 --> 00:11:45,680
relations for machine learning

00:11:43,040 --> 00:11:47,360
i use the suggested relations formulated

00:11:45,680 --> 00:11:50,639
in the paper by shared l for

00:11:47,360 --> 00:11:52,560
classification algorithms

00:11:50,639 --> 00:11:55,200
so for example what you can use is

00:11:52,560 --> 00:11:58,160
permutation of class labels

00:11:55,200 --> 00:12:01,519
addition of classes by relabeling sample

00:11:58,160 --> 00:12:03,519
and removal of classes

00:12:01,519 --> 00:12:05,440
in follow-up papers it has been

00:12:03,519 --> 00:12:07,440
validated that this approach

00:12:05,440 --> 00:12:08,639
is really effective in machine learning

00:12:07,440 --> 00:12:10,560
algorithms

00:12:08,639 --> 00:12:13,040
and it improves the quality of the

00:12:10,560 --> 00:12:13,040
software

00:12:13,360 --> 00:12:16,959
that summarize the theory i wanted to

00:12:15,120 --> 00:12:18,959
present and what i think you should know

00:12:16,959 --> 00:12:20,160
in order to start writing tests for your

00:12:18,959 --> 00:12:23,440
iii

00:12:20,160 --> 00:12:24,639
for your ai software that takes me to

00:12:23,440 --> 00:12:26,959
the second part of your

00:12:24,639 --> 00:12:28,959
of my talk overview of the hypothesis

00:12:26,959 --> 00:12:30,720
library

00:12:28,959 --> 00:12:32,800
before i dive into explaining the

00:12:30,720 --> 00:12:34,959
library i wanted to also mention

00:12:32,800 --> 00:12:36,560
that property-based testing was actually

00:12:34,959 --> 00:12:38,480
first introduced by quick check

00:12:36,560 --> 00:12:39,839
framework in haskell

00:12:38,480 --> 00:12:41,760
and there are multiple libraries

00:12:39,839 --> 00:12:43,760
designed to help writing property-based

00:12:41,760 --> 00:12:46,240
tests

00:12:43,760 --> 00:12:46,880
hypothesis is such a library and

00:12:46,240 --> 00:12:49,760
actually

00:12:46,880 --> 00:12:50,399
the leading library for python you

00:12:49,760 --> 00:12:52,079
should know

00:12:50,399 --> 00:12:53,760
that all the core developers are

00:12:52,079 --> 00:12:56,560
volunteers

00:12:53,760 --> 00:12:57,440
they really have great documentation and

00:12:56,560 --> 00:12:59,360
for what

00:12:57,440 --> 00:13:01,040
from what i have seen they are really

00:12:59,360 --> 00:13:06,800
attentive to questions

00:13:01,040 --> 00:13:08,959
and requests they get so the key object

00:13:06,800 --> 00:13:11,600
of hypotheses is a strategy

00:13:08,959 --> 00:13:14,399
which is basically the api to describe

00:13:11,600 --> 00:13:16,079
the recipe for generating data

00:13:14,399 --> 00:13:17,839
it's really easy to start playing with

00:13:16,079 --> 00:13:19,279
some simple code testing

00:13:17,839 --> 00:13:21,600
and there are many easy to use

00:13:19,279 --> 00:13:24,000
predefined strategies

00:13:21,600 --> 00:13:25,360
the library was written really in a way

00:13:24,000 --> 00:13:27,920
to do a lot of magic

00:13:25,360 --> 00:13:29,519
and it really focuses on finding edge

00:13:27,920 --> 00:13:31,360
cases

00:13:29,519 --> 00:13:33,760
it's also quite easy to write custom

00:13:31,360 --> 00:13:37,440
strategies

00:13:33,760 --> 00:13:40,000
so let's dive into some basics

00:13:37,440 --> 00:13:41,680
the main entry point of hypothesis is

00:13:40,000 --> 00:13:43,839
the given decorator

00:13:41,680 --> 00:13:44,959
which takes a function that accepts some

00:13:43,839 --> 00:13:47,040
arguments

00:13:44,959 --> 00:13:49,680
and turns it into a normal test function

00:13:47,040 --> 00:13:52,240
with the generated examples

00:13:49,680 --> 00:13:53,760
and in this in this example i used the

00:13:52,240 --> 00:13:55,839
integer strategy

00:13:53,760 --> 00:13:58,800
where just as it sounds generates random

00:13:55,839 --> 00:13:58,800
integer numbers

00:14:00,079 --> 00:14:04,480
another basic strategy is the floats

00:14:02,399 --> 00:14:06,880
where it generates float numbers

00:14:04,480 --> 00:14:08,480
and you can see that in this example the

00:14:06,880 --> 00:14:11,839
test will actually fail

00:14:08,480 --> 00:14:13,519
as votes also generate non-values

00:14:11,839 --> 00:14:15,839
and that won't meet the commutative

00:14:13,519 --> 00:14:17,839
property

00:14:15,839 --> 00:14:20,000
we will fix the recipe for generating

00:14:17,839 --> 00:14:22,320
data to exclude none

00:14:20,000 --> 00:14:23,199
and i actually also include excluded

00:14:22,320 --> 00:14:26,240
infinity

00:14:23,199 --> 00:14:30,800
which will also fail the test and in

00:14:26,240 --> 00:14:32,480
this way the test will actually succeed

00:14:30,800 --> 00:14:34,160
there are many more basic strategies

00:14:32,480 --> 00:14:36,959
like booleans text

00:14:34,160 --> 00:14:37,199
dictionaries and all of them you can see

00:14:36,959 --> 00:14:39,360
in

00:14:37,199 --> 00:14:40,800
their specification and how to use them

00:14:39,360 --> 00:14:42,560
in documentation

00:14:40,800 --> 00:14:44,959
and they actually keep extending more

00:14:42,560 --> 00:14:47,519
strategies all the time

00:14:44,959 --> 00:14:48,639
if you need any strategy search within

00:14:47,519 --> 00:14:52,000
the documentation

00:14:48,639 --> 00:14:53,760
as it will probably be there in addition

00:14:52,000 --> 00:14:55,760
there are quite extensive options for

00:14:53,760 --> 00:14:59,199
numpy and pandas strategies if it's

00:14:55,760 --> 00:15:01,279
relevant for you

00:14:59,199 --> 00:15:04,000
another strategy that i personally find

00:15:01,279 --> 00:15:06,639
quite useful is the share strategy

00:15:04,000 --> 00:15:07,839
that allows two or more shared instances

00:15:06,639 --> 00:15:10,560
with the same key

00:15:07,839 --> 00:15:13,760
to share the same value so basically it

00:15:10,560 --> 00:15:16,000
will draw a single value of burn

00:15:13,760 --> 00:15:18,000
so in this example i use the shared

00:15:16,000 --> 00:15:21,279
strategy to make sure that the shape of

00:15:18,000 --> 00:15:21,279
both arrays are the same

00:15:22,480 --> 00:15:26,560
last in the course strategies i would

00:15:24,399 --> 00:15:28,959
like to show you the data strategy

00:15:26,560 --> 00:15:30,160
that isn't really a normal strategy but

00:15:28,959 --> 00:15:32,639
instead gives you an

00:15:30,160 --> 00:15:33,920
object which which can be used to draw

00:15:32,639 --> 00:15:37,120
data interactively from

00:15:33,920 --> 00:15:37,920
other strategies i found it useful when

00:15:37,120 --> 00:15:40,240
there were some

00:15:37,920 --> 00:15:42,160
where there was strategy that i wanted

00:15:40,240 --> 00:15:44,079
them to be dependent on the drawn value

00:15:42,160 --> 00:15:46,000
of one another

00:15:44,079 --> 00:15:48,000
so in the previous example i could have

00:15:46,000 --> 00:15:51,920
just defined a shape strategy and used

00:15:48,000 --> 00:15:51,920
data to draw the arrays with that shape

00:15:52,880 --> 00:15:56,399
to complete the example from the

00:15:54,240 --> 00:15:58,000
beginning of the talk if

00:15:56,399 --> 00:15:59,839
if i were to test the commutativity

00:15:58,000 --> 00:16:02,480
property of the function

00:15:59,839 --> 00:16:04,639
then this is how i would have done it

00:16:02,480 --> 00:16:07,120
generate two random lists with either

00:16:04,639 --> 00:16:10,639
integers or floats and a start to test

00:16:07,120 --> 00:16:10,639
the commutativity property

00:16:12,160 --> 00:16:18,240
hypothesis also provides apis to define

00:16:15,199 --> 00:16:21,440
your own customized strategy

00:16:18,240 --> 00:16:24,000
let's look at some more examples

00:16:21,440 --> 00:16:26,480
the build strategy generating values by

00:16:24,000 --> 00:16:29,040
drawing from args and kw args

00:16:26,480 --> 00:16:30,720
and passing them to the callable the

00:16:29,040 --> 00:16:34,480
callable can be either a class

00:16:30,720 --> 00:16:34,800
or a function here for example we have

00:16:34,480 --> 00:16:37,680
the

00:16:34,800 --> 00:16:38,399
rectangle class and the rectangle list

00:16:37,680 --> 00:16:40,480
strategy

00:16:38,399 --> 00:16:42,320
will generate a list that will draw

00:16:40,480 --> 00:16:44,880
integers for width and height

00:16:42,320 --> 00:16:46,639
argument and a tuple for the center

00:16:44,880 --> 00:16:48,000
argument

00:16:46,639 --> 00:16:51,279
then it will pass them to the

00:16:48,000 --> 00:16:53,279
constructor of rectangle

00:16:51,279 --> 00:16:55,440
you should note that if you are using

00:16:53,279 --> 00:16:57,839
outer library or other class

00:16:55,440 --> 00:17:01,199
then you can just use from type strategy

00:16:57,839 --> 00:17:01,199
to draw class examples

00:17:02,000 --> 00:17:06,319
there are also transforming data

00:17:03,600 --> 00:17:10,000
functions like filtering and mapping

00:17:06,319 --> 00:17:12,559
that allows additional customizations

00:17:10,000 --> 00:17:13,439
you should note though if your filter is

00:17:12,559 --> 00:17:16,240
too strict

00:17:13,439 --> 00:17:17,360
it may filter all exams all examples or

00:17:16,240 --> 00:17:19,199
many of them

00:17:17,360 --> 00:17:21,439
but don't worry hypothesis will tell you

00:17:19,199 --> 00:17:21,439
that

00:17:22,079 --> 00:17:26,000
when you use this kind of libraries that

00:17:23,919 --> 00:17:26,559
include both randomness and custom

00:17:26,000 --> 00:17:29,360
building

00:17:26,559 --> 00:17:33,840
you want to have debugging tools and

00:17:29,360 --> 00:17:33,840
hypothesis has that

00:17:34,320 --> 00:17:38,640
one example is the example function that

00:17:37,039 --> 00:17:41,280
helps to get the feeling about the

00:17:38,640 --> 00:17:42,960
possible values of the strategy

00:17:41,280 --> 00:17:46,320
when you build your own strategy to see

00:17:42,960 --> 00:17:48,720
if it returns what you expected

00:17:46,320 --> 00:17:50,240
there is also the node function that is

00:17:48,720 --> 00:17:52,720
really like a print function

00:17:50,240 --> 00:17:55,039
that only prints when when the test is

00:17:52,720 --> 00:17:56,960
failing

00:17:55,039 --> 00:17:59,120
there are hypothesis defaults that can

00:17:56,960 --> 00:18:00,799
be overridden by using the settings

00:17:59,120 --> 00:18:03,600
decorator

00:18:00,799 --> 00:18:05,760
one of them is a database which is

00:18:03,600 --> 00:18:08,080
stored locally in the file system

00:18:05,760 --> 00:18:10,559
and stores information of bugs when they

00:18:08,080 --> 00:18:12,640
are found during testing

00:18:10,559 --> 00:18:15,039
this information is later used to

00:18:12,640 --> 00:18:17,280
reproduce the bugs

00:18:15,039 --> 00:18:18,960
at last you can run pi test with

00:18:17,280 --> 00:18:21,840
hypothesis show statistics

00:18:18,960 --> 00:18:23,760
to get statistics of the run including

00:18:21,840 --> 00:18:26,720
how many examples generated

00:18:23,760 --> 00:18:29,840
how many were filtered how many failed

00:18:26,720 --> 00:18:29,840
and some more

00:18:30,240 --> 00:18:34,320
the random nature of hypothesis may

00:18:32,400 --> 00:18:36,720
cause a test run successfully

00:18:34,320 --> 00:18:38,960
a couple of times before finding a

00:18:36,720 --> 00:18:41,200
failing example

00:18:38,960 --> 00:18:42,480
once that happens it will keep failing

00:18:41,200 --> 00:18:44,320
with the same example

00:18:42,480 --> 00:18:45,919
as it will first retrieve the example

00:18:44,320 --> 00:18:49,200
from the previous filler

00:18:45,919 --> 00:18:51,280
stored in the database i just mentioned

00:18:49,200 --> 00:18:52,720
this makes hypothesis repeatable random

00:18:51,280 --> 00:18:55,840
testing

00:18:52,720 --> 00:18:57,760
and then will never go away by chance a

00:18:55,840 --> 00:18:58,960
test will only start passing if the

00:18:57,760 --> 00:19:02,720
examples that previously

00:18:58,960 --> 00:19:02,720
failed no longer fails

00:19:03,120 --> 00:19:06,640
hypothesis tries to produce human

00:19:05,520 --> 00:19:09,200
readable examples

00:19:06,640 --> 00:19:11,280
when it finds a failure it takes a

00:19:09,200 --> 00:19:13,200
complex example that failed and turns it

00:19:11,280 --> 00:19:16,160
into a simpler one

00:19:13,200 --> 00:19:17,919
for example if a very long string fails

00:19:16,160 --> 00:19:19,520
then it will try to find the shortest

00:19:17,919 --> 00:19:22,640
failing string

00:19:19,520 --> 00:19:24,240
and this process is called shrinking

00:19:22,640 --> 00:19:26,080
each strategy is the final order in

00:19:24,240 --> 00:19:28,080
which it shrinks and you can find in the

00:19:26,080 --> 00:19:30,480
commentation the definition for every

00:19:28,080 --> 00:19:32,160
strategy

00:19:30,480 --> 00:19:34,240
you won't usually need to care about

00:19:32,160 --> 00:19:35,120
this much but it can be worth being

00:19:34,240 --> 00:19:36,880
aware of it

00:19:35,120 --> 00:19:40,240
as it can affect what the best way to

00:19:36,880 --> 00:19:40,240
write your own strategies

00:19:41,919 --> 00:19:45,280
so we have the theory and we have the

00:19:44,320 --> 00:19:48,480
tools

00:19:45,280 --> 00:19:50,240
we are ready to hear about real examples

00:19:48,480 --> 00:19:52,160
and that leads me to the last section of

00:19:50,240 --> 00:19:53,039
the talk in which i would like to

00:19:52,160 --> 00:19:55,600
introduce you

00:19:53,039 --> 00:19:58,080
to some use cases where we found the

00:19:55,600 --> 00:20:00,720
benefit to use property-based testing

00:19:58,080 --> 00:20:03,600
together with metamorphic testing using

00:20:00,720 --> 00:20:04,960
the hypothesis library

00:20:03,600 --> 00:20:06,880
in order for you to be able to

00:20:04,960 --> 00:20:09,520
understand

00:20:06,880 --> 00:20:12,799
my examples let me give you some more

00:20:09,520 --> 00:20:15,520
context to what our programs do

00:20:12,799 --> 00:20:16,640
so as i mentioned in the introduction we

00:20:15,520 --> 00:20:20,320
analyze imaging

00:20:16,640 --> 00:20:22,640
to find abnormalities and to do that

00:20:20,320 --> 00:20:24,400
we try machine learning models that

00:20:22,640 --> 00:20:25,360
segment the imaging we receive from

00:20:24,400 --> 00:20:29,120
hospitals

00:20:25,360 --> 00:20:31,200
to identify those abnormalities

00:20:29,120 --> 00:20:33,520
there are many stages to the complete

00:20:31,200 --> 00:20:36,480
process and i chose one of them to base

00:20:33,520 --> 00:20:38,559
my examples for this talk

00:20:36,480 --> 00:20:41,200
and that is the extracting features from

00:20:38,559 --> 00:20:43,520
the segmentation

00:20:41,200 --> 00:20:45,120
in this stage we calculate the features

00:20:43,520 --> 00:20:46,799
for each connected component in the

00:20:45,120 --> 00:20:49,679
segmentation

00:20:46,799 --> 00:20:52,000
and those features help us to decide

00:20:49,679 --> 00:20:56,960
whether the relevant segmentation

00:20:52,000 --> 00:20:56,960
is indeed true or is it false positive

00:20:58,159 --> 00:21:02,480
obviously i cannot disclose too much of

00:21:00,159 --> 00:21:04,960
our code but there are some simplified

00:21:02,480 --> 00:21:09,520
examples to describe the basic concept

00:21:04,960 --> 00:21:09,520
which i hope will just be as insightful

00:21:11,039 --> 00:21:15,280
so very similar to the example i gave in

00:21:13,280 --> 00:21:17,679
metamorphic relations

00:21:15,280 --> 00:21:19,360
if i have a predicted segmentation with

00:21:17,679 --> 00:21:21,600
connected components

00:21:19,360 --> 00:21:22,720
the extracted features will be included

00:21:21,600 --> 00:21:24,799
in the list

00:21:22,720 --> 00:21:27,679
if i add additional connected components

00:21:24,799 --> 00:21:30,159
to the segmentation

00:21:27,679 --> 00:21:32,240
so here i use the composite decorator

00:21:30,159 --> 00:21:33,919
that also helps generate custom

00:21:32,240 --> 00:21:36,320
strategies

00:21:33,919 --> 00:21:37,919
it does it by calling the draw function

00:21:36,320 --> 00:21:39,919
to draw random examples

00:21:37,919 --> 00:21:43,520
and use those values to perform the

00:21:39,919 --> 00:21:46,000
requirement required modifications

00:21:43,520 --> 00:21:47,760
in order to draw random predictions i

00:21:46,000 --> 00:21:49,919
use the array strategy

00:21:47,760 --> 00:21:52,000
that will draw volume values in random

00:21:49,919 --> 00:21:54,559
shapes

00:21:52,000 --> 00:21:56,159
then it will call my custom strategy

00:21:54,559 --> 00:21:58,320
that will add additional random

00:21:56,159 --> 00:22:00,320
connected components on top of the given

00:21:58,320 --> 00:22:01,919
prediction

00:22:00,320 --> 00:22:03,520
to make sure it doesn't override the

00:22:01,919 --> 00:22:05,679
existing components

00:22:03,520 --> 00:22:08,880
i used binary dilation to keep them

00:22:05,679 --> 00:22:11,360
environment for the change

00:22:08,880 --> 00:22:12,240
finally i asserted that my returned

00:22:11,360 --> 00:22:14,400
features list

00:22:12,240 --> 00:22:17,919
from the first prediction is indeed

00:22:14,400 --> 00:22:17,919
included in the second one

00:22:19,440 --> 00:22:24,080
my second example is a result of really

00:22:21,760 --> 00:22:26,559
analyzing the processes we do

00:22:24,080 --> 00:22:28,240
and what i found is that many of the

00:22:26,559 --> 00:22:29,679
features like the diameter of the

00:22:28,240 --> 00:22:32,480
connected component

00:22:29,679 --> 00:22:34,559
or number of pixels are invariant to

00:22:32,480 --> 00:22:37,919
their exact location

00:22:34,559 --> 00:22:39,840
and for those features that do change uh

00:22:37,919 --> 00:22:42,480
if i move the connected component i can

00:22:39,840 --> 00:22:45,360
move the relevant functions

00:22:42,480 --> 00:22:45,840
so in this example i wanted to show you

00:22:45,360 --> 00:22:47,520
uh

00:22:45,840 --> 00:22:49,919
i wanted to show an additional way to

00:22:47,520 --> 00:22:51,919
generate random connected components

00:22:49,919 --> 00:22:54,159
and this is by using random shapes

00:22:51,919 --> 00:22:55,760
function inside image

00:22:54,159 --> 00:22:58,480
so i'm again generating random

00:22:55,760 --> 00:22:59,520
predictions and assert that the features

00:22:58,480 --> 00:23:02,559
don't change

00:22:59,520 --> 00:23:03,520
if i rotate the prediction and as

00:23:02,559 --> 00:23:05,919
mentioned

00:23:03,520 --> 00:23:07,520
to make the test really succeed i have

00:23:05,919 --> 00:23:09,760
to mock the functions

00:23:07,520 --> 00:23:13,440
that do vary when the exact location of

00:23:09,760 --> 00:23:13,440
the connected components changes

00:23:15,360 --> 00:23:20,159
and that takes me to the end of my talk

00:23:18,080 --> 00:23:21,840
to summarize my research to find

00:23:20,159 --> 00:23:23,919
to find the best testing methods to

00:23:21,840 --> 00:23:25,679
stochastic ai models

00:23:23,919 --> 00:23:27,440
i came to the conclusion that there

00:23:25,679 --> 00:23:29,360
isn't one best way

00:23:27,440 --> 00:23:31,120
and really the best method is to have a

00:23:29,360 --> 00:23:32,960
combination of multiple methods

00:23:31,120 --> 00:23:35,840
depending on the specific process and

00:23:32,960 --> 00:23:37,679
function we're testing

00:23:35,840 --> 00:23:39,039
hope i did convince you that properly

00:23:37,679 --> 00:23:41,279
based testing

00:23:39,039 --> 00:23:42,480
will give great addition to testing

00:23:41,279 --> 00:23:44,159
coverage

00:23:42,480 --> 00:23:46,480
and that i gave you all the information

00:23:44,159 --> 00:23:50,320
and tools needed to start working

00:23:46,480 --> 00:23:53,840
on your own testing functions

00:23:50,320 --> 00:23:53,840
thank you for listening to my talk

00:24:02,840 --> 00:24:05,840
goodbye

00:25:02,559 --> 00:25:04,640

YouTube URL: https://www.youtube.com/watch?v=uVjgkqEpgkE


