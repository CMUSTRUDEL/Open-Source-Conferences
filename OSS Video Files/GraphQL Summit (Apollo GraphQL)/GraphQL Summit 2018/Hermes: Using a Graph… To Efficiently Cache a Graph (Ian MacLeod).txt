Title: Hermes: Using a Graph… To Efficiently Cache a Graph (Ian MacLeod)
Publication date: 2018-12-05
Playlist: GraphQL Summit 2018
Description: 
	Talk from GraphQL Summit 2018 - Nov 7, 2018

Ian MacLeod, Principal Developer at Convoy

Ian has worked as a software engineer on a variety of different projects—from the initial build of Kindle for iOS (Amazon), to various bits of the Polymer project (Google), to complicated logistics software (Convoy). All the while, he has stayed active in open source, leaving a trail of contributions in multiple communities (Ruby, iOS, JS, TS, GraphQL)

▸ 
We take you on the journey that lead to the creation of Hermes—a GraphQL cache (for Apollo) that does things a little differently. You'll hear about some of the challenges implementing a fast GraphQL cache, and all the crazy things it has to deal with under the hood.
Captions: 
	00:00:02,770 --> 00:00:06,970
all right everyone I'm here to talk

00:00:05,500 --> 00:00:09,790
about caching I know it's everybody's

00:00:06,970 --> 00:00:11,139
favorite subject but who am I so my name

00:00:09,790 --> 00:00:12,969
is a MacLeod I work at a company called

00:00:11,139 --> 00:00:16,930
convoy where we do trucking related

00:00:12,969 --> 00:00:18,430
things with graph QL so one of the

00:00:16,930 --> 00:00:20,290
interesting things we're doing is we

00:00:18,430 --> 00:00:21,670
have a react native app that we provide

00:00:20,290 --> 00:00:23,080
to our truckers so we're marketplace

00:00:21,670 --> 00:00:23,950
that connects people want to ship things

00:00:23,080 --> 00:00:26,740
with the truckers that actually make

00:00:23,950 --> 00:00:29,140
those things move but we're in this

00:00:26,740 --> 00:00:31,300
unique space where we have to support a

00:00:29,140 --> 00:00:32,649
wide variety of phones we don't want to

00:00:31,300 --> 00:00:33,520
interrupt the work that our truckers do

00:00:32,649 --> 00:00:35,290
and they're usually using their personal

00:00:33,520 --> 00:00:38,829
phones and often you're dealing with

00:00:35,290 --> 00:00:40,930
really old Android phones really old

00:00:38,829 --> 00:00:44,020
versions of OS as things get flow things

00:00:40,930 --> 00:00:46,060
have no memory it's tough another

00:00:44,020 --> 00:00:47,559
challenge we deal with too is our

00:00:46,060 --> 00:00:49,690
truckers go to remote locations they may

00:00:47,559 --> 00:00:51,040
not have cell connectivity or app has to

00:00:49,690 --> 00:00:51,550
work offline they have to be able to do

00:00:51,040 --> 00:00:53,680
their jobs

00:00:51,550 --> 00:00:55,329
so we're concentrating things offline

00:00:53,680 --> 00:00:57,130
we're also executing because of that

00:00:55,329 --> 00:00:59,500
we're kicking off very large queries to

00:00:57,130 --> 00:01:00,700
cache all that data and and to make it

00:00:59,500 --> 00:01:03,160
available while they are offline and

00:01:00,700 --> 00:01:05,049
then to compound that our application

00:01:03,160 --> 00:01:06,700
isn't an application where drivers are

00:01:05,049 --> 00:01:08,139
constant interacting with it large leaf

00:01:06,700 --> 00:01:10,060
stuff is happening in the background so

00:01:08,139 --> 00:01:11,590
we're tracking their location triggering

00:01:10,060 --> 00:01:14,289
different events based on that and so as

00:01:11,590 --> 00:01:16,119
a result the app has a good number of

00:01:14,289 --> 00:01:17,590
query observers active in and given time

00:01:16,119 --> 00:01:19,209
so anytime the cache changes they all

00:01:17,590 --> 00:01:22,409
get updated and various things happen

00:01:19,209 --> 00:01:25,719
and behind the scenes so because of this

00:01:22,409 --> 00:01:27,969
about a year and a half ago we were

00:01:25,719 --> 00:01:29,619
seeing extremely poor performance so

00:01:27,969 --> 00:01:33,039
boot times could take anywhere from 10

00:01:29,619 --> 00:01:35,109
seconds to minutes our app would freeze

00:01:33,039 --> 00:01:36,249
all the time you would tap on something

00:01:35,109 --> 00:01:37,840
you would freeze for a couple seconds

00:01:36,249 --> 00:01:39,099
you would do nothing and would freeze

00:01:37,840 --> 00:01:41,649
for a couple seconds we were really good

00:01:39,099 --> 00:01:43,749
at freezing and and this is this is

00:01:41,649 --> 00:01:47,259
happening even on the fastest phones at

00:01:43,749 --> 00:01:49,749
the time so something was quickly off so

00:01:47,259 --> 00:01:53,499
we dug into it and a lot of it turned

00:01:49,749 --> 00:01:55,029
out to be time spent reading from the

00:01:53,499 --> 00:01:58,179
Apollo cache so we started to profile

00:01:55,029 --> 00:01:59,560
this we were observing that like we were

00:01:58,179 --> 00:02:01,209
a query would come back from the server

00:01:59,560 --> 00:02:02,739
it would write it to the cache -

00:02:01,209 --> 00:02:05,529
Apollo's internal representation of that

00:02:02,739 --> 00:02:07,869
query and then we would see the vast

00:02:05,529 --> 00:02:09,250
majority of times best spent past that

00:02:07,869 --> 00:02:10,930
just reading the data out of the cache

00:02:09,250 --> 00:02:13,350
to provide it to our components or to

00:02:10,930 --> 00:02:15,440
provide it to our background services so

00:02:13,350 --> 00:02:16,910
digging into that all

00:02:15,440 --> 00:02:18,380
so garbage collector was firing all the

00:02:16,910 --> 00:02:20,780
time so there's a bunch of objects being

00:02:18,380 --> 00:02:22,880
created and that's weird and then also

00:02:20,780 --> 00:02:24,950
because we had all these query observers

00:02:22,880 --> 00:02:28,100
we noticed that they were they they were

00:02:24,950 --> 00:02:30,500
a large source this so what exactly was

00:02:28,100 --> 00:02:31,910
going on there and it turned out really

00:02:30,500 --> 00:02:34,280
to be somewhat structural with the cake

00:02:31,910 --> 00:02:35,150
the way the cache works and let me get

00:02:34,280 --> 00:02:37,160
into that so this is what we're gonna

00:02:35,150 --> 00:02:39,770
get into some some guts of how Apollo

00:02:37,160 --> 00:02:42,440
works hopefully in a lightning talk so

00:02:39,770 --> 00:02:44,480
first how does it Paulo represent state

00:02:42,440 --> 00:02:47,510
this is Apollo's default and memory

00:02:44,480 --> 00:02:50,480
implementation the way it does the way

00:02:47,510 --> 00:02:52,430
represents state is with a lookup table

00:02:50,480 --> 00:02:54,020
so on the left here you've got a

00:02:52,430 --> 00:02:55,250
graphical query response coming back

00:02:54,020 --> 00:02:56,960
from the server and on the right is

00:02:55,250 --> 00:02:59,210
roughly how Apollo represents it

00:02:56,960 --> 00:03:00,800
internally so it takes all the nodes in

00:02:59,210 --> 00:03:03,080
your graph it flattens them out has a

00:03:00,800 --> 00:03:04,520
lookup table by ID and then anywhere a

00:03:03,080 --> 00:03:06,830
node reference is another node it

00:03:04,520 --> 00:03:08,330
injects a little pointer an ID string

00:03:06,830 --> 00:03:11,060
and some other some other metadata so

00:03:08,330 --> 00:03:12,950
that it knows to walk that now the

00:03:11,060 --> 00:03:14,930
problem is anytime you read data from

00:03:12,950 --> 00:03:16,670
the cache anytime you ask for data from

00:03:14,930 --> 00:03:18,500
Apollo it has to go and reverse reverse

00:03:16,670 --> 00:03:20,780
that operation so it's taking this flat

00:03:18,500 --> 00:03:22,070
structure building an entire object

00:03:20,780 --> 00:03:23,989
hierarchy out of it and then giving it

00:03:22,070 --> 00:03:26,060
to you so you can render it in react or

00:03:23,989 --> 00:03:28,010
wherever you can use it and it turns out

00:03:26,060 --> 00:03:29,480
that's slow it also ends up generating a

00:03:28,010 --> 00:03:31,489
ton of objects hence all the garbage

00:03:29,480 --> 00:03:33,140
collection we were saying it's also

00:03:31,489 --> 00:03:36,560
worth noting to relay operates in a very

00:03:33,140 --> 00:03:37,780
similar way so we're in a bit of a

00:03:36,560 --> 00:03:39,890
pickle what are we gonna do about this

00:03:37,780 --> 00:03:41,510
one option was to simplify our

00:03:39,890 --> 00:03:44,269
application honestly that was a

00:03:41,510 --> 00:03:46,519
non-starter we would have to remove so

00:03:44,269 --> 00:03:47,660
much of our queries and and behavior

00:03:46,519 --> 00:03:50,300
that it just wouldn't work for our

00:03:47,660 --> 00:03:51,890
drivers another option was relay and

00:03:50,300 --> 00:03:53,209
actually relay turns out to still be

00:03:51,890 --> 00:03:56,989
quite a bit faster for a lot of these

00:03:53,209 --> 00:03:58,610
things but even then it doesn't scale

00:03:56,989 --> 00:04:00,260
with large number of query observers so

00:03:58,610 --> 00:04:02,060
if you look at the the 5 Observer and

00:04:00,260 --> 00:04:04,670
the 25 observer benchmarks you'll notice

00:04:02,060 --> 00:04:05,660
that relay has linear growth in

00:04:04,670 --> 00:04:08,600
performance with that and it just

00:04:05,660 --> 00:04:10,430
doesn't really work for us so we did

00:04:08,600 --> 00:04:13,750
what all developers do and we make them

00:04:10,430 --> 00:04:18,260
sorry and we make the problem worse

00:04:13,750 --> 00:04:20,780
so in luckily around this time apollo

00:04:18,260 --> 00:04:22,520
was actually ippolit to is happening and

00:04:20,780 --> 00:04:24,650
they were just in the FET in the process

00:04:22,520 --> 00:04:26,160
of factoring out their cache API so we

00:04:24,650 --> 00:04:27,870
were able to jump right on that and

00:04:26,160 --> 00:04:28,980
implement an alternative cash without

00:04:27,870 --> 00:04:32,700
implementing a whole new graphical

00:04:28,980 --> 00:04:34,290
client so we call that cash Hermes it's

00:04:32,700 --> 00:04:35,640
an unfortunate code name that stuck

00:04:34,290 --> 00:04:39,750
around and now it's stick now it's great

00:04:35,640 --> 00:04:42,150
so if we stuck with it and largely the

00:04:39,750 --> 00:04:44,040
goal of Hermes is to minimize the amount

00:04:42,150 --> 00:04:46,440
of time done when reading because that's

00:04:44,040 --> 00:04:48,450
really what was hurting us and the way

00:04:46,440 --> 00:04:51,780
it does that is by representing state as

00:04:48,450 --> 00:04:53,760
closely as possible to how your data

00:04:51,780 --> 00:04:55,350
comes back from the cache so the way we

00:04:53,760 --> 00:04:58,470
do that actually is by representing this

00:04:55,350 --> 00:04:59,640
cache as a graph internally so it's hard

00:04:58,470 --> 00:05:01,980
at harder visualize here but if you look

00:04:59,640 --> 00:05:04,740
in this the same query as before rather

00:05:01,980 --> 00:05:06,960
than having string references that point

00:05:04,740 --> 00:05:08,850
to the nodes IDs Hermes points directly

00:05:06,960 --> 00:05:09,930
to the underlying objects so you got the

00:05:08,850 --> 00:05:11,160
post they point to the two posts

00:05:09,930 --> 00:05:13,650
reference by one and two

00:05:11,160 --> 00:05:15,030
within each post there's authors of all

00:05:13,650 --> 00:05:17,100
of those point to the exact same

00:05:15,030 --> 00:05:20,490
JavaScript object that's pointed to by

00:05:17,100 --> 00:05:21,480
the the key a and there's there's a

00:05:20,490 --> 00:05:22,620
bunch of bookkeeping that happens under

00:05:21,480 --> 00:05:24,380
the covers here to make this happen

00:05:22,620 --> 00:05:26,520
Hermes does immutable updates to this

00:05:24,380 --> 00:05:28,110
it's certainly pretty complicated the

00:05:26,520 --> 00:05:29,520
way it does it but as an added benefit

00:05:28,110 --> 00:05:31,650
effectively you can ask for data from

00:05:29,520 --> 00:05:34,860
the cache and it can give you that

00:05:31,650 --> 00:05:36,660
result in constant time straight up so

00:05:34,860 --> 00:05:39,930
this really helps speed up reads at the

00:05:36,660 --> 00:05:42,360
cost of slowing down writes a bit and as

00:05:39,930 --> 00:05:45,480
a result the numbers get pretty great

00:05:42,360 --> 00:05:46,860
with Hermes so this caveat here that

00:05:45,480 --> 00:05:49,440
this is this is a query that we use

00:05:46,860 --> 00:05:52,290
internally that is tuned largely for

00:05:49,440 --> 00:05:53,940
Hermes in regular operation it's a

00:05:52,290 --> 00:05:58,560
little bit slower but still much faster

00:05:53,940 --> 00:05:59,640
than the alternatives yeah so I want to

00:05:58,560 --> 00:06:00,780
get in a little bit more into like what

00:05:59,640 --> 00:06:02,070
the trade-offs to Hermes are making are

00:06:00,780 --> 00:06:04,320
what you need to be aware of if you end

00:06:02,070 --> 00:06:06,510
up wanting to use it so here's more

00:06:04,320 --> 00:06:09,570
examples so one of the challenges with

00:06:06,510 --> 00:06:11,370
Hermes is it can actually over return

00:06:09,570 --> 00:06:12,900
data so in this case we're asking for a

00:06:11,370 --> 00:06:14,820
blog post and some fictional blog

00:06:12,900 --> 00:06:16,950
service you're asking for a bunch of

00:06:14,820 --> 00:06:20,070
bunch of details about it so far so

00:06:16,950 --> 00:06:21,780
forth straightforward I hope now let's

00:06:20,070 --> 00:06:24,750
say you want to query out a list of

00:06:21,780 --> 00:06:26,430
posts and in that list of posts is the

00:06:24,750 --> 00:06:28,770
same post you referenced before by that

00:06:26,430 --> 00:06:30,030
other query sometime in the past because

00:06:28,770 --> 00:06:31,710
Hermes is pointing to the same

00:06:30,030 --> 00:06:33,990
underlying object you're gonna get those

00:06:31,710 --> 00:06:34,890
extra fields because the cache had

00:06:33,990 --> 00:06:36,139
already had them and there's only one

00:06:34,890 --> 00:06:39,259
copy of that entity

00:06:36,139 --> 00:06:40,460
in the in the graph so it can be a

00:06:39,259 --> 00:06:41,930
challenge if you're not careful you

00:06:40,460 --> 00:06:43,729
might reference these fields when you

00:06:41,930 --> 00:06:46,219
shouldn't be but things like type

00:06:43,729 --> 00:06:47,870
checking and Apollo's type script

00:06:46,219 --> 00:06:49,099
generation from this help you avoid

00:06:47,870 --> 00:06:51,110
these sorts of problems and it's also

00:06:49,099 --> 00:06:52,610
worth noting we've been using Hermes in

00:06:51,110 --> 00:06:53,840
production for I guess about a year now

00:06:52,610 --> 00:06:58,009
and we haven't encountered any problems

00:06:53,840 --> 00:07:00,680
from this another thing so Hermes is not

00:06:58,009 --> 00:07:02,029
a silver bullet for performance one of

00:07:00,680 --> 00:07:04,009
their places where it has some

00:07:02,029 --> 00:07:05,479
challenges is with parameterize fields

00:07:04,009 --> 00:07:07,159
so this is gonna be even an even more

00:07:05,479 --> 00:07:11,360
complicated example bear with me

00:07:07,159 --> 00:07:13,189
so let's say you want to query the

00:07:11,360 --> 00:07:15,439
current users active blog posts you

00:07:13,189 --> 00:07:16,639
write a query like this and internally

00:07:15,439 --> 00:07:19,250
Hermes is gonna represent it something

00:07:16,639 --> 00:07:20,750
like this so it's saying okay I know the

00:07:19,250 --> 00:07:22,759
viewers idea name those are static

00:07:20,750 --> 00:07:24,680
static fields they're gonna exist with

00:07:22,759 --> 00:07:28,129
that entity but then we have this post

00:07:24,680 --> 00:07:29,960
status active field that that parameter

00:07:28,129 --> 00:07:32,330
may change the Hermes can't safely just

00:07:29,960 --> 00:07:34,400
toss that as a status field on the on

00:07:32,330 --> 00:07:35,930
the the the viewer it has to store it

00:07:34,400 --> 00:07:37,789
separately so that when you ask for it

00:07:35,930 --> 00:07:40,069
where we ask for a different field it

00:07:37,789 --> 00:07:41,120
doesn't return that as well so this is

00:07:40,069 --> 00:07:42,259
roughly what it's looking like let me

00:07:41,120 --> 00:07:43,159
give another example hopefully to drive

00:07:42,259 --> 00:07:45,620
that point home and make it a bit

00:07:43,159 --> 00:07:47,719
clearer so now let's say you asked for

00:07:45,620 --> 00:07:50,089
the users draft posts so we changed that

00:07:47,719 --> 00:07:51,409
field there when that query kicks off

00:07:50,089 --> 00:07:53,750
hermes is gonna store that as a

00:07:51,409 --> 00:07:56,330
different node in its internal graph and

00:07:53,750 --> 00:07:58,490
then when you read it which is the far

00:07:56,330 --> 00:08:01,460
right it will overlay those values on

00:07:58,490 --> 00:08:02,449
top of your result so it's it's worth

00:08:01,460 --> 00:08:04,189
noting like like this

00:08:02,449 --> 00:08:05,719
this means that Hermes isn't returning

00:08:04,189 --> 00:08:07,639
this data in constant time but it still

00:08:05,719 --> 00:08:09,680
reminds the amount of work it has to do

00:08:07,639 --> 00:08:11,779
for queries like this it's also worth

00:08:09,680 --> 00:08:14,089
noting that we have some tactics that

00:08:11,779 --> 00:08:15,229
help you mitigate this cost but I'm not

00:08:14,089 --> 00:08:17,689
gonna go into it we have a directive you

00:08:15,229 --> 00:08:18,740
can slap on here that that tells her

00:08:17,689 --> 00:08:20,990
means that this thing is never gonna

00:08:18,740 --> 00:08:23,389
change just trust me

00:08:20,990 --> 00:08:26,180
yeah and then another really neat

00:08:23,389 --> 00:08:27,740
benefit of all the bookkeeping that

00:08:26,180 --> 00:08:30,259
hermes is doing is that we can implement

00:08:27,740 --> 00:08:31,669
garbage collection pretty much out of

00:08:30,259 --> 00:08:33,199
the box so Henri's does automatic

00:08:31,669 --> 00:08:34,969
reference counting garbage collection

00:08:33,199 --> 00:08:38,959
it's basic but it's good enough in most

00:08:34,969 --> 00:08:41,089
cases so anytime you run a query and

00:08:38,959 --> 00:08:42,680
then you run a different query and some

00:08:41,089 --> 00:08:44,269
of the data has been orphaned Hermes

00:08:42,680 --> 00:08:45,500
will just get rid of it for you and it

00:08:44,269 --> 00:08:46,640
turns out this is really important for a

00:08:45,500 --> 00:08:48,330
mobile application where you don't have

00:08:46,640 --> 00:08:50,890
much memory available

00:08:48,330 --> 00:08:52,210
it note it's worth noting too that it's

00:08:50,890 --> 00:08:54,400
not smart enough to deal with cyclic

00:08:52,210 --> 00:08:55,780
sub-graphs and other other cases like

00:08:54,400 --> 00:08:58,420
that so it's not perfect but it's good

00:08:55,780 --> 00:08:59,920
enough and that's it

00:08:58,420 --> 00:09:01,090
so if you wanna check out her means you

00:08:59,920 --> 00:09:02,140
can you can take a look at it here also

00:09:01,090 --> 00:09:03,700
if you wanna play with the benchmarks

00:09:02,140 --> 00:09:05,590
take a look at the at them there and

00:09:03,700 --> 00:09:09,160
feel free to contact me

00:09:05,590 --> 00:09:12,640
Twitter github or email thanks

00:09:09,160 --> 00:09:12,640

YouTube URL: https://www.youtube.com/watch?v=Q4yltOuXufE


