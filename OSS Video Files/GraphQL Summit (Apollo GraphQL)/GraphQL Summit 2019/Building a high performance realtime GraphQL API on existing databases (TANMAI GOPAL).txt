Title: Building a high performance realtime GraphQL API on existing databases (TANMAI GOPAL)
Publication date: 2019-11-03
Playlist: GraphQL Summit 2019
Description: 
	I will talk about the approach we took at Hasura of building a GraphQL API that can leverage an existing database. You'll learn how we built a declarative authorization system and addressed the n+1 database query problem. With GraphQL making it easy to “join data” across tables, I’ll also talk about how we can “join data” between a table and an existing API.

Resources:
Learn more about Summit - https://summit.graphql.com/
Captions: 
	00:00:01,179 --> 00:00:08,540
hey folks hi folks I'm glad to be here

00:00:05,990 --> 00:00:10,309
I'm gonna talk a little bit about how

00:00:08,540 --> 00:00:13,070
we've done some interesting performance

00:00:10,309 --> 00:00:15,380
stuff with graph QL talking to a

00:00:13,070 --> 00:00:16,760
database I'm going to touch a little bit

00:00:15,380 --> 00:00:18,800
upon the real time aspect but maybe not

00:00:16,760 --> 00:00:20,329
too much because I have a we have a blog

00:00:18,800 --> 00:00:24,890
post up on it that is definitely worth

00:00:20,329 --> 00:00:27,439
read special thanks to Ryan for laying a

00:00:24,890 --> 00:00:30,079
really good foundation with ASDs

00:00:27,439 --> 00:00:32,480
and with generally optimizing graph QL

00:00:30,079 --> 00:00:34,190
for talking to databases in the previous

00:00:32,480 --> 00:00:36,770
talk and so that's going to be super

00:00:34,190 --> 00:00:38,840
helpful as you can have right deep my

00:00:36,770 --> 00:00:39,950
main intent in this talk is to kind of

00:00:38,840 --> 00:00:41,720
take you over stuff that we've done

00:00:39,950 --> 00:00:45,890
which should hopefully be interesting to

00:00:41,720 --> 00:00:47,750
you and um and it's probably going to be

00:00:45,890 --> 00:00:49,520
a very interesting it's hopefully going

00:00:47,750 --> 00:00:51,560
to be a very interesting idea that

00:00:49,520 --> 00:00:52,820
should inspire some interesting

00:00:51,560 --> 00:00:55,910
techniques in the stuff that you folks

00:00:52,820 --> 00:00:58,730
doing so to start off just to quickly

00:00:55,910 --> 00:00:59,960
tell you kind of what Hazara is so that

00:00:58,730 --> 00:01:02,600
you have the right context to this talk

00:00:59,960 --> 00:01:04,640
husband as a graph QL engine it's not

00:01:02,600 --> 00:01:07,069
library it's graph QL server the trans

00:01:04,640 --> 00:01:10,219
you pointed to your databases or those

00:01:07,069 --> 00:01:13,280
services and it generates a graph QL API

00:01:10,219 --> 00:01:15,530
with databases we generate with Postgres

00:01:13,280 --> 00:01:18,679
and the Postgres family of databases we

00:01:15,530 --> 00:01:20,359
generate an almost real-time API almost

00:01:18,679 --> 00:01:21,829
instantly so if you have thousands and

00:01:20,359 --> 00:01:24,439
thousands of tables or views and

00:01:21,829 --> 00:01:25,700
functions you'll get a graph QL API in

00:01:24,439 --> 00:01:27,829
pretty much 30 seconds and then you can

00:01:25,700 --> 00:01:30,679
start customizing and tweaking it to

00:01:27,829 --> 00:01:32,539
what suits your graph QL API it runs as

00:01:30,679 --> 00:01:34,249
a docker container in your own

00:01:32,539 --> 00:01:36,439
infrastructure and it is 100% open

00:01:34,249 --> 00:01:40,969
source like most open source projects it

00:01:36,439 --> 00:01:45,189
is also apache licensed and with that I

00:01:40,969 --> 00:01:47,719
shall dive in to why house row as fast

00:01:45,189 --> 00:01:49,069
husana is one of the things that our

00:01:47,719 --> 00:01:50,840
users say after using a service that

00:01:49,069 --> 00:01:54,799
house Row is actually super super super

00:01:50,840 --> 00:01:56,149
fast and and and I mean that's a great

00:01:54,799 --> 00:01:59,389
thing to say fast so there are different

00:01:56,149 --> 00:02:00,799
dimensions to it it there is a latency

00:01:59,389 --> 00:02:02,749
dimension but there is also a throughput

00:02:00,799 --> 00:02:05,149
dimension and Hofstra is able to process

00:02:02,749 --> 00:02:09,320
a staggeringly large number of queries

00:02:05,149 --> 00:02:11,870
per second continuously in with a really

00:02:09,320 --> 00:02:13,310
tiny footprint we support a massive

00:02:11,870 --> 00:02:14,930
amount of concurrency this is especially

00:02:13,310 --> 00:02:17,719
useful for real-time use cases with

00:02:14,930 --> 00:02:20,390
graphical subscriptions we recently did

00:02:17,719 --> 00:02:21,890
a benchmark where we took 1 million kind

00:02:20,390 --> 00:02:24,680
of events happening on the back end

00:02:21,890 --> 00:02:26,810
being sent to 1 million unique graph QL

00:02:24,680 --> 00:02:30,709
clients and all of that kind of just

00:02:26,810 --> 00:02:32,329
chugging along fine and Astra also works

00:02:30,709 --> 00:02:34,430
fairly well the techniques that we use

00:02:32,329 --> 00:02:38,120
work fairly well with really large

00:02:34,430 --> 00:02:40,969
queries or large results as well right

00:02:38,120 --> 00:02:43,819
and so the performance is typically

00:02:40,969 --> 00:02:45,980
within 1% of what Postgres itself would

00:02:43,819 --> 00:02:47,780
give you raw assuming that you do not

00:02:45,980 --> 00:02:49,220
have a web tier in the middle but with

00:02:47,780 --> 00:02:51,669
us as the web tier in the middle you'd

00:02:49,220 --> 00:02:54,260
still get about that kind of performance

00:02:51,669 --> 00:02:58,280
I'm gonna talk a little bit about how we

00:02:54,260 --> 00:03:01,359
kind of get these get these kinds of

00:02:58,280 --> 00:03:04,310
results so let's take a simple query

00:03:01,359 --> 00:03:09,620
what you see in front of you is a schema

00:03:04,310 --> 00:03:12,919
for simple let me bring that up here all

00:03:09,620 --> 00:03:15,200
right cool so I'm gonna be talking about

00:03:12,919 --> 00:03:16,879
this schema this is a simple music

00:03:15,200 --> 00:03:19,040
database there's artists there's albums

00:03:16,879 --> 00:03:21,409
and there's tracks every atom has an

00:03:19,040 --> 00:03:24,949
artist every album has a bunch of tracks

00:03:21,409 --> 00:03:27,019
right fairly simple just a simple

00:03:24,949 --> 00:03:30,169
example to kind of ground our discussion

00:03:27,019 --> 00:03:31,459
so with this if we make a fairly simple

00:03:30,169 --> 00:03:32,930
query and we say hey you know what we

00:03:31,459 --> 00:03:36,109
want to build kind of like a artists

00:03:32,930 --> 00:03:38,840
page and show artists with all of their

00:03:36,109 --> 00:03:40,310
albums and every album has a bunch of

00:03:38,840 --> 00:03:42,260
tracks so we want to show the tracks for

00:03:40,310 --> 00:03:45,739
those for those albums as well mmm

00:03:42,260 --> 00:03:47,540
if if you implement this and let's say

00:03:45,739 --> 00:03:49,900
have a resolver for artists for albums

00:03:47,540 --> 00:03:53,209
and for tracks you would have one hit

00:03:49,900 --> 00:03:54,620
which will go to the database and fetch

00:03:53,209 --> 00:03:56,930
all of the artists so let's say of n

00:03:54,620 --> 00:03:58,549
artists if you have n artists then

00:03:56,930 --> 00:04:00,620
you'll hit the next resolver for albums

00:03:58,549 --> 00:04:04,699
which will then go and hit the database

00:04:00,620 --> 00:04:07,849
n times to fetch albums for each of the

00:04:04,699 --> 00:04:10,699
artists right that you have now albums

00:04:07,849 --> 00:04:12,799
have tracks so you're fetching tracks

00:04:10,699 --> 00:04:16,549
for each of the albums but there are n

00:04:12,799 --> 00:04:19,070
albums and then albums for n artists so

00:04:16,549 --> 00:04:20,859
you're making n square hits and you can

00:04:19,070 --> 00:04:23,650
and you can quickly see how this becomes

00:04:20,859 --> 00:04:24,880
kind of a nightmare as

00:04:23,650 --> 00:04:27,580
this kind of goes on right so this is

00:04:24,880 --> 00:04:28,960
obviously not a great thing look mmm so

00:04:27,580 --> 00:04:30,580
a data loaded style approach which was

00:04:28,960 --> 00:04:32,910
an approach optimized for different

00:04:30,580 --> 00:04:35,620
kinds of data sources that you can query

00:04:32,910 --> 00:04:37,750
kind of gives you this ability to say or

00:04:35,620 --> 00:04:39,360
if you if you if you adopt that kind of

00:04:37,750 --> 00:04:41,620
an approach and you use that to build

00:04:39,360 --> 00:04:43,210
and you use that to kind of set up your

00:04:41,620 --> 00:04:45,490
resolvers in that case what you try to

00:04:43,210 --> 00:04:47,530
do is hit the database once for fetching

00:04:45,490 --> 00:04:49,419
all the artists and then next you hit

00:04:47,530 --> 00:04:52,270
the database once for fetching all of

00:04:49,419 --> 00:04:54,340
the album's but instead of fetching it

00:04:52,270 --> 00:04:56,530
per artist you'll try to make a query to

00:04:54,340 --> 00:04:58,389
the database to say hey I would rather

00:04:56,530 --> 00:05:01,389
fetch all of the album's for all the

00:04:58,389 --> 00:05:02,949
artists if the album artist is in that

00:05:01,389 --> 00:05:05,080
set that you have already previously

00:05:02,949 --> 00:05:06,190
fetched from the database right the

00:05:05,080 --> 00:05:08,919
problem is that you kind of need to

00:05:06,190 --> 00:05:10,780
fetch artists before you resolve albums

00:05:08,919 --> 00:05:13,479
and then you do the same thing for

00:05:10,780 --> 00:05:14,889
tracks right so you were making you kind

00:05:13,479 --> 00:05:16,389
of reduce that to three hits so the

00:05:14,889 --> 00:05:19,300
number of unique knowns that you have

00:05:16,389 --> 00:05:20,530
it's proportional to that the number of

00:05:19,300 --> 00:05:23,260
database hits that you'll do will be

00:05:20,530 --> 00:05:26,080
proportional to that but considering

00:05:23,260 --> 00:05:28,360
that large chunks of our query are

00:05:26,080 --> 00:05:29,919
hitting the same data source we can do

00:05:28,360 --> 00:05:31,389
much better because databases were

00:05:29,919 --> 00:05:32,770
actually supposed to do this and they've

00:05:31,389 --> 00:05:36,099
been doing this for a really really

00:05:32,770 --> 00:05:39,340
really long time this is pretty much

00:05:36,099 --> 00:05:41,830
their kind of job description but if you

00:05:39,340 --> 00:05:43,750
if you think about it as compiling

00:05:41,830 --> 00:05:47,800
something or accurate more accurately

00:05:43,750 --> 00:05:51,370
transpiling something and nothing of it

00:05:47,800 --> 00:05:54,669
as resolving unique entities what we

00:05:51,370 --> 00:05:56,080
really want to do is in if you were a

00:05:54,669 --> 00:05:57,699
database person what you would have done

00:05:56,080 --> 00:06:00,729
is just make a single database query

00:05:57,699 --> 00:06:02,830
that would have fetched all of these in

00:06:00,729 --> 00:06:04,030
a single shot right you don't need to

00:06:02,830 --> 00:06:05,440
make you don't need to make multiple

00:06:04,030 --> 00:06:07,539
queries you don't need to make queries

00:06:05,440 --> 00:06:09,190
from previous things take those results

00:06:07,539 --> 00:06:10,690
put it into the next query you don't

00:06:09,190 --> 00:06:12,280
need to do any of that you can actually

00:06:10,690 --> 00:06:14,380
just make one single query to do it

00:06:12,280 --> 00:06:16,150
hypothetically this is possible it's

00:06:14,380 --> 00:06:17,770
just that the weird architecture that we

00:06:16,150 --> 00:06:19,930
usually have for resolving our graph QL

00:06:17,770 --> 00:06:23,979
things doesn't allow for this doesn't

00:06:19,930 --> 00:06:25,900
make it easy so instead what we can do

00:06:23,979 --> 00:06:28,150
is take the entire graph QL ast and

00:06:25,900 --> 00:06:31,150
think about transforming that into a

00:06:28,150 --> 00:06:32,530
sequel ast and thankfully I don't have

00:06:31,150 --> 00:06:35,680
to talk about what an ast is thank you

00:06:32,530 --> 00:06:37,510
Ron but the idea here is to take a

00:06:35,680 --> 00:06:39,010
graphical query parse it

00:06:37,510 --> 00:06:41,440
get an internal representation for that

00:06:39,010 --> 00:06:44,320
graph to LA St and then convert that to

00:06:41,440 --> 00:06:45,430
a sequel ast you'd have to kind of do a

00:06:44,320 --> 00:06:46,900
little bit of the mappings because your

00:06:45,430 --> 00:06:48,400
data models and your API models might

00:06:46,900 --> 00:06:49,990
not map one to once maybe you do a

00:06:48,400 --> 00:06:51,850
little bit of transformation to get the

00:06:49,990 --> 00:06:53,770
right sequel ast you render it out to

00:06:51,850 --> 00:06:56,350
the right sequel and then you make that

00:06:53,770 --> 00:06:58,090
single query right and that's kind of

00:06:56,350 --> 00:07:00,280
the approach that's kind of the approach

00:06:58,090 --> 00:07:02,260
that internally has or takes to

00:07:00,280 --> 00:07:03,670
processing a graph QL query that hits

00:07:02,260 --> 00:07:05,020
the database right the graph query might

00:07:03,670 --> 00:07:06,850
hit several things but the part that

00:07:05,020 --> 00:07:08,470
hits the database can be extracted out

00:07:06,850 --> 00:07:12,280
and this is the part of the pipeline

00:07:08,470 --> 00:07:14,350
that runs for doing that so this is I'm

00:07:12,280 --> 00:07:16,330
gonna show you a demo but this is also

00:07:14,350 --> 00:07:19,210
this is also super interesting because

00:07:16,330 --> 00:07:21,790
it makes it really easy to analyze and

00:07:19,210 --> 00:07:23,620
optimize on graph QL queries and also

00:07:21,790 --> 00:07:25,360
makes it easy to expose the power of the

00:07:23,620 --> 00:07:27,280
underlying data system that you're using

00:07:25,360 --> 00:07:31,090
right so just to give you a quick

00:07:27,280 --> 00:07:35,710
example I have that music database

00:07:31,090 --> 00:07:39,460
loaded up in this graphical here and now

00:07:35,710 --> 00:07:41,620
when I run this query I am actually

00:07:39,460 --> 00:07:44,590
fetching thousands and thousands of

00:07:41,620 --> 00:07:47,110
entities right I'm just gonna scroll

00:07:44,590 --> 00:07:49,450
down to show you the amount of stuff I'm

00:07:47,110 --> 00:07:52,270
fetching I can keep doing this I'm not

00:07:49,450 --> 00:07:53,710
gonna move this I'm on and and you can

00:07:52,270 --> 00:07:55,630
kind of fetch this data in a single shot

00:07:53,710 --> 00:07:57,190
and it's super super fast right the only

00:07:55,630 --> 00:07:59,860
latency that you're noticing is actually

00:07:57,190 --> 00:08:01,930
Network latency right it's not and this

00:07:59,860 --> 00:08:03,280
is this is a app that's making a query

00:08:01,930 --> 00:08:05,110
and that's fetching thousands of

00:08:03,280 --> 00:08:07,570
elements joining across thousands of

00:08:05,110 --> 00:08:11,100
things joining across three things but

00:08:07,570 --> 00:08:14,680
thousands of rows what I can also do is

00:08:11,100 --> 00:08:16,480
look at the underlying sequel right and

00:08:14,680 --> 00:08:18,370
this is nice and I'll talk a little more

00:08:16,480 --> 00:08:20,260
about some peculiarities with the sequel

00:08:18,370 --> 00:08:22,360
that we generated here but the really

00:08:20,260 --> 00:08:25,060
cool thing is that you get a plan and

00:08:22,360 --> 00:08:26,440
this plan kind of tells you what the

00:08:25,060 --> 00:08:28,150
database thinks about this query and

00:08:26,440 --> 00:08:30,730
what the underlying cost of this query

00:08:28,150 --> 00:08:32,200
actually for the database is which is

00:08:30,730 --> 00:08:33,700
really cool because one of the big

00:08:32,200 --> 00:08:35,440
problems that people talk about is how

00:08:33,700 --> 00:08:37,479
do I analyze the cost of a graphical

00:08:35,440 --> 00:08:40,210
query right and so now for a whole

00:08:37,479 --> 00:08:41,500
subset of your graph care query or for

00:08:40,210 --> 00:08:43,030
the entire draft Cal query if the whole

00:08:41,500 --> 00:08:45,580
thing is going to a database you can

00:08:43,030 --> 00:08:47,200
actually look at the cost of a graph QL

00:08:45,580 --> 00:08:49,120
query right and you can use this to do

00:08:47,200 --> 00:08:50,430
cost planning you can use this to do

00:08:49,120 --> 00:08:52,980
brahimi cost planning you can use

00:08:50,430 --> 00:08:56,399
to even do static and say static limits

00:08:52,980 --> 00:08:58,350
for costs this is also nice because now

00:08:56,399 --> 00:09:01,440
you can kind of toggle between sometimes

00:08:58,350 --> 00:09:03,360
a deep query is actually okay but a

00:09:01,440 --> 00:09:05,010
query that is just large is bad because

00:09:03,360 --> 00:09:06,540
you're scanning too many rows and this

00:09:05,010 --> 00:09:08,010
way you can kind of have a more

00:09:06,540 --> 00:09:10,890
realistic cost attached to your query

00:09:08,010 --> 00:09:13,800
which is not just based on say depth or

00:09:10,890 --> 00:09:14,850
not just based on the number of nodes

00:09:13,800 --> 00:09:16,890
that you're fetching or the number of

00:09:14,850 --> 00:09:18,839
results that you're fetching right this

00:09:16,890 --> 00:09:20,700
also makes it really easy for your for

00:09:18,839 --> 00:09:22,709
you to say hey my graph your query is

00:09:20,700 --> 00:09:24,480
slow database percent speed it up and

00:09:22,709 --> 00:09:26,250
the database person says what query are

00:09:24,480 --> 00:09:27,930
you making and so you kind of run this

00:09:26,250 --> 00:09:29,910
and then you show the underlying sequel

00:09:27,930 --> 00:09:31,350
query and this underlying sequel query

00:09:29,910 --> 00:09:33,089
has a plan and so the database the

00:09:31,350 --> 00:09:35,520
person goes and says who there's a

00:09:33,089 --> 00:09:36,870
there's a sequential scan here sequence

00:09:35,520 --> 00:09:38,279
scans are bad we should change this to

00:09:36,870 --> 00:09:40,260
an index scan maybe let's add an index

00:09:38,279 --> 00:09:42,209
here right so this is something that

00:09:40,260 --> 00:09:45,209
there is a whole ecosystem of people of

00:09:42,209 --> 00:09:47,459
tools that can kind of take this input

00:09:45,209 --> 00:09:49,290
and figure and help you figure out how

00:09:47,459 --> 00:09:50,760
to how to optimize things right so you

00:09:49,290 --> 00:09:52,740
don't have to do that and you can kind

00:09:50,760 --> 00:09:56,820
of plug into that ecosystem on quite

00:09:52,740 --> 00:09:58,970
easily mmm all right um the other thing

00:09:56,820 --> 00:10:01,020
that you can do is kind of expose the

00:09:58,970 --> 00:10:02,459
power that you have of an underlying

00:10:01,020 --> 00:10:06,420
system so what I'm gonna do for example

00:10:02,459 --> 00:10:07,800
is say I'm gonna query for let me use

00:10:06,420 --> 00:10:11,790
this build or here so let's say I'm

00:10:07,800 --> 00:10:13,260
going to query for albums or let's

00:10:11,790 --> 00:10:15,810
actually wait let's change that let's

00:10:13,260 --> 00:10:19,020
change it - I'm gonna query for artists

00:10:15,810 --> 00:10:22,140
and artists have ID and name so that's

00:10:19,020 --> 00:10:24,600
great and so I'm seeing a bunch of

00:10:22,140 --> 00:10:26,640
artists come in and that's fine um what

00:10:24,600 --> 00:10:29,220
I can do is also query for artists and

00:10:26,640 --> 00:10:32,940
their albums so I'm going to add albums

00:10:29,220 --> 00:10:34,980
and titles right now one of the examples

00:10:32,940 --> 00:10:36,959
that I talked about was if I have these

00:10:34,980 --> 00:10:38,880
if you're kind of making this kind of

00:10:36,959 --> 00:10:40,589
query right and what you want to do is

00:10:38,880 --> 00:10:42,510
you want to limit the number of albums

00:10:40,589 --> 00:10:44,279
that you're fetching for a particular

00:10:42,510 --> 00:10:47,910
artist right which is that you want to

00:10:44,279 --> 00:10:50,339
do limit one sometimes passing these

00:10:47,910 --> 00:10:52,560
arguments down across resolvers is a

00:10:50,339 --> 00:10:53,520
little bit painful and depending on the

00:10:52,560 --> 00:10:54,750
right framework and depending on the

00:10:53,520 --> 00:10:57,120
framework that you have or depending on

00:10:54,750 --> 00:10:58,410
the or the graphical framework that

00:10:57,120 --> 00:11:00,600
you're using you might have different

00:10:58,410 --> 00:11:03,060
ways of doing this in a compiled kind of

00:11:00,600 --> 00:11:04,110
approach this is actually very easy to

00:11:03,060 --> 00:11:06,630
implement because

00:11:04,110 --> 00:11:08,550
again most data system systems that kind

00:11:06,630 --> 00:11:10,830
of spit out data already have ways to

00:11:08,550 --> 00:11:12,540
help you understand already have the

00:11:10,830 --> 00:11:14,640
right ways of helping you parse where

00:11:12,540 --> 00:11:16,410
this kind of limit statement goes inside

00:11:14,640 --> 00:11:17,100
the generated query so if I look at the

00:11:16,410 --> 00:11:19,950
underlined query

00:11:17,100 --> 00:11:22,410
I'll see that the limit is kind of

00:11:19,950 --> 00:11:24,150
applied at the right place is applied to

00:11:22,410 --> 00:11:26,640
the right place automatically right and

00:11:24,150 --> 00:11:28,350
this happens within within what secret

00:11:26,640 --> 00:11:31,620
calls the lateral join what you can also

00:11:28,350 --> 00:11:34,770
do is use this to start filtering you

00:11:31,620 --> 00:11:37,470
can filter parent objects by children

00:11:34,770 --> 00:11:41,100
object properties right so what I can do

00:11:37,470 --> 00:11:45,990
is I can say where the album has a name

00:11:41,100 --> 00:11:53,580
that you know maybe starts with let's

00:11:45,990 --> 00:11:55,980
say a right oops so I am so you can kind

00:11:53,580 --> 00:11:59,310
of freely traversed up and down sorry

00:11:55,980 --> 00:12:01,710
this is title right so I can kind of run

00:11:59,310 --> 00:12:03,990
queries that traverse up and down the

00:12:01,710 --> 00:12:07,350
entire graph QL query right I can have

00:12:03,990 --> 00:12:09,120
arguments that reference any part of the

00:12:07,350 --> 00:12:11,070
query and all of that just works fairly

00:12:09,120 --> 00:12:12,300
easily because again you're not really

00:12:11,070 --> 00:12:13,920
worried about thinking of these as

00:12:12,300 --> 00:12:16,200
independent functions that are running

00:12:13,920 --> 00:12:18,270
that need to kind of share information

00:12:16,200 --> 00:12:20,160
across them but you just have access to

00:12:18,270 --> 00:12:21,900
the entire ast and you're thinking of

00:12:20,160 --> 00:12:23,880
the entire ast transforming from one

00:12:21,900 --> 00:12:26,880
step to another um which makes a lot of

00:12:23,880 --> 00:12:28,080
stuff like this very easy to do all

00:12:26,880 --> 00:12:30,570
right okay cool

00:12:28,080 --> 00:12:33,720
so next what I'm going to talk about is

00:12:30,570 --> 00:12:35,460
why this is a really stupid idea this is

00:12:33,720 --> 00:12:37,470
a stupid idea because what we just did

00:12:35,460 --> 00:12:39,150
was give all of our data away for free

00:12:37,470 --> 00:12:40,320
without charging people for money right

00:12:39,150 --> 00:12:42,360
the least you can do when you give

00:12:40,320 --> 00:12:44,460
people's data away is charge the money

00:12:42,360 --> 00:12:48,000
and be like Camrys analytic or do stuff

00:12:44,460 --> 00:12:49,500
with it but but and so authorization is

00:12:48,000 --> 00:12:51,150
important right and so how do we think

00:12:49,500 --> 00:12:52,560
about authorization because let's say

00:12:51,150 --> 00:12:54,810
for example we want to make sure that

00:12:52,560 --> 00:12:56,460
end users have access to the data that

00:12:54,810 --> 00:12:57,540
they own through whatever constraints

00:12:56,460 --> 00:12:58,680
that are there right you don't want to

00:12:57,540 --> 00:13:00,630
go and make a query for something and

00:12:58,680 --> 00:13:02,490
then just get everything and so for

00:13:00,630 --> 00:13:04,350
example two kinds of access control

00:13:02,490 --> 00:13:06,690
rules that you typically run into would

00:13:04,350 --> 00:13:09,510
be is this particular entity accessible

00:13:06,690 --> 00:13:11,760
right so if I'm accessing an album if I

00:13:09,510 --> 00:13:13,380
am the artist who kind of created this

00:13:11,760 --> 00:13:14,880
music album then I should have access to

00:13:13,380 --> 00:13:17,640
it maybe it's whatever music app or

00:13:14,880 --> 00:13:18,780
something or or maybe you want

00:13:17,640 --> 00:13:21,210
have access control that says that

00:13:18,780 --> 00:13:22,680
certain fields are accessible or certain

00:13:21,210 --> 00:13:25,500
fields are not accessible depending on

00:13:22,680 --> 00:13:28,860
some property of the end-user that is

00:13:25,500 --> 00:13:31,020
using the API right the way we think

00:13:28,860 --> 00:13:32,730
about this is and I'm gonna take this

00:13:31,020 --> 00:13:35,580
specific example let's say you have

00:13:32,730 --> 00:13:37,650
albums albums have ID and album has a

00:13:35,580 --> 00:13:40,170
title and what you want to do is you

00:13:37,650 --> 00:13:42,930
want to only in that query return those

00:13:40,170 --> 00:13:45,180
albums where the artist ID is me right

00:13:42,930 --> 00:13:50,760
is me as in and the current user using

00:13:45,180 --> 00:13:51,690
the application right so if you the way

00:13:50,760 --> 00:13:53,010
you would have traditionally done this

00:13:51,690 --> 00:13:54,240
is fairly straightforward what you would

00:13:53,010 --> 00:13:56,880
have done is you would have had an

00:13:54,240 --> 00:13:59,310
authorization token or a JWT or a token

00:13:56,880 --> 00:14:00,660
or something and then through that

00:13:59,310 --> 00:14:02,370
session token you would have resolved

00:14:00,660 --> 00:14:03,480
the session token into some property of

00:14:02,370 --> 00:14:05,250
the user so you would have taken that

00:14:03,480 --> 00:14:07,350
token then I look up in your session

00:14:05,250 --> 00:14:10,020
database or opened up the JWT and you

00:14:07,350 --> 00:14:12,570
would have said hmmm this token belongs

00:14:10,020 --> 00:14:14,520
to user ID 1 this token belongs to a

00:14:12,570 --> 00:14:16,110
user that has roles a B and C or

00:14:14,520 --> 00:14:18,180
whatever attributes you might have right

00:14:16,110 --> 00:14:19,950
a belongs to a particular organization

00:14:18,180 --> 00:14:22,410
or a tenant or whatever some some

00:14:19,950 --> 00:14:22,860
property of this user and then what you

00:14:22,410 --> 00:14:24,990
would do

00:14:22,860 --> 00:14:25,980
inside the resolved order and say the

00:14:24,990 --> 00:14:28,260
code that you're writing that is

00:14:25,980 --> 00:14:30,360
actually making the data fetch is you

00:14:28,260 --> 00:14:31,860
would apply those permission policies or

00:14:30,360 --> 00:14:33,900
you would that depend on the current

00:14:31,860 --> 00:14:37,080
user right so what you would have done

00:14:33,900 --> 00:14:39,390
underneath it is if if this was let's

00:14:37,080 --> 00:14:41,460
say just a REST API call or a graph QL

00:14:39,390 --> 00:14:43,380
call and you just had this graphical API

00:14:41,460 --> 00:14:44,910
and nothing else you could have made a

00:14:43,380 --> 00:14:47,910
select call to the database and said

00:14:44,910 --> 00:14:50,430
where the album artist ID is equal to

00:14:47,910 --> 00:14:52,440
the session dot user ID right that's

00:14:50,430 --> 00:14:54,390
kind of that's kind of the extra step

00:14:52,440 --> 00:14:56,130
that you add when you make the database

00:14:54,390 --> 00:14:58,350
query hopefully you're doing this and

00:14:56,130 --> 00:15:00,420
you are not actually doing select star

00:14:58,350 --> 00:15:02,190
from albums and then you get this

00:15:00,420 --> 00:15:03,660
monstrous million row result and then

00:15:02,190 --> 00:15:05,520
you filter that million row result in

00:15:03,660 --> 00:15:09,030
your application that is a terrible

00:15:05,520 --> 00:15:10,380
thing to do I mean I I say this because

00:15:09,030 --> 00:15:12,360
I haven't done so I've done this before

00:15:10,380 --> 00:15:15,390
I had this thing where I built a Django

00:15:12,360 --> 00:15:17,370
up my first Django up and then I had an

00:15:15,390 --> 00:15:19,380
ORM and then the ORM I would make like

00:15:17,370 --> 00:15:21,150
multiple queries and it would work fine

00:15:19,380 --> 00:15:22,830
in development and then the first day it

00:15:21,150 --> 00:15:24,720
went to not even production but like had

00:15:22,830 --> 00:15:26,640
five users the whole thing just blew up

00:15:24,720 --> 00:15:27,960
because of stuff like this right because

00:15:26,640 --> 00:15:28,440
like yeah sure I'll filter it in the

00:15:27,960 --> 00:15:30,899
application

00:15:28,440 --> 00:15:33,029
computers are fast they're not if you

00:15:30,899 --> 00:15:36,149
have like gigabytes of data that you

00:15:33,029 --> 00:15:37,470
need to filter through and so and so

00:15:36,149 --> 00:15:39,180
hopefully we find if make this equal

00:15:37,470 --> 00:15:41,100
query and then you kind of fetch that

00:15:39,180 --> 00:15:43,620
right precise slice of data or the right

00:15:41,100 --> 00:15:46,560
entities from the underlying data system

00:15:43,620 --> 00:15:49,560
to and then and then kind of serve that

00:15:46,560 --> 00:15:52,250
out as a graphical response and and so

00:15:49,560 --> 00:15:54,569
the way the way have solar does it is we

00:15:52,250 --> 00:15:55,920
have permission policies that you can

00:15:54,569 --> 00:15:58,560
attach to types in your graph to your

00:15:55,920 --> 00:16:01,730
schema and what this permission policy

00:15:58,560 --> 00:16:03,959
can do is extract session information

00:16:01,730 --> 00:16:06,839
from your session token' like for

00:16:03,959 --> 00:16:09,899
example user IDs and roles org ID tenant

00:16:06,839 --> 00:16:11,850
ID current temperature current location

00:16:09,899 --> 00:16:13,649
mobile whatever whatever session

00:16:11,850 --> 00:16:15,990
variables you have you can extract that

00:16:13,649 --> 00:16:18,269
and then use that to build a permission

00:16:15,990 --> 00:16:21,480
policy that will automatically be merged

00:16:18,269 --> 00:16:22,980
into the sequel that is generated right

00:16:21,480 --> 00:16:25,889
or into the where clause that is

00:16:22,980 --> 00:16:28,379
generated so what this entire process

00:16:25,889 --> 00:16:30,689
now looks like is you have a graph QL SD

00:16:28,379 --> 00:16:32,490
and then you have this internal tree

00:16:30,689 --> 00:16:34,139
that we're going to manipulate which we

00:16:32,490 --> 00:16:35,430
call the internal SD and then we

00:16:34,139 --> 00:16:36,899
manipulate that by adding certain

00:16:35,430 --> 00:16:38,279
permission rules to it depending on the

00:16:36,899 --> 00:16:40,230
right types that you're looking at or

00:16:38,279 --> 00:16:41,699
the end user session property so this

00:16:40,230 --> 00:16:43,860
becomes an internal AST now with

00:16:41,699 --> 00:16:45,389
permissions in it and then you finally

00:16:43,860 --> 00:16:47,220
render that out into the finest equal

00:16:45,389 --> 00:16:49,620
ast and that just runs is one query

00:16:47,220 --> 00:16:50,639
right so just to give you an example of

00:16:49,620 --> 00:16:54,480
what this looks like

00:16:50,639 --> 00:16:58,610
before my screen goes black is for

00:16:54,480 --> 00:17:01,410
example what I can do here is go to

00:16:58,610 --> 00:17:03,180
write hopefully you can see this I'm

00:17:01,410 --> 00:17:05,100
going to go to Adams and I'm going to

00:17:03,180 --> 00:17:06,209
apply a permission I'm going to delete

00:17:05,100 --> 00:17:09,809
this permission first

00:17:06,209 --> 00:17:11,760
so that you can start from scratch all

00:17:09,809 --> 00:17:12,990
right and so what I'm doing is I'm

00:17:11,760 --> 00:17:14,520
applying a permission here okay I'm

00:17:12,990 --> 00:17:16,079
using a UI for doing this but all of

00:17:14,520 --> 00:17:17,760
this is just configuration this is just

00:17:16,079 --> 00:17:19,350
yeah Mel or JSON or whatever that you

00:17:17,760 --> 00:17:21,179
can use right so what I'm going to do is

00:17:19,350 --> 00:17:23,640
I'm going to specify a check for albums

00:17:21,179 --> 00:17:26,159
and say that an album is accessible only

00:17:23,640 --> 00:17:27,240
if the or actually let me do this for

00:17:26,159 --> 00:17:29,220
tracks so that is a little more

00:17:27,240 --> 00:17:33,780
interesting so let's say I can only

00:17:29,220 --> 00:17:37,530
access the music track if the track that

00:17:33,780 --> 00:17:39,210
the album belongs to that artist is me

00:17:37,530 --> 00:17:41,340
right so we're kind of traversing the

00:17:39,210 --> 00:17:42,179
data graph a little bit so what I can do

00:17:41,340 --> 00:17:46,080
here is

00:17:42,179 --> 00:17:47,460
a hey if the album of this track because

00:17:46,080 --> 00:17:48,779
the track is related to an album some

00:17:47,460 --> 00:17:51,119
traversing a graph to your relationship

00:17:48,779 --> 00:17:54,779
so I'm saying if the art if the track

00:17:51,119 --> 00:17:56,220
dot album dot artist ID is equal to some

00:17:54,779 --> 00:17:57,899
property of the session and we call that

00:17:56,220 --> 00:18:00,509
a user ID it could be artist ID to be

00:17:57,899 --> 00:18:03,149
whatever if that is if that condition

00:18:00,509 --> 00:18:05,279
matches then let's allow access to this

00:18:03,149 --> 00:18:07,590
particular entity right and so then what

00:18:05,279 --> 00:18:13,049
you can do is when we kind of run this

00:18:07,590 --> 00:18:15,480
here and we say oops let me just put

00:18:13,049 --> 00:18:18,509
that in here so if I toggle in the right

00:18:15,480 --> 00:18:22,259
role and I put in the right user ID and

00:18:18,509 --> 00:18:25,549
I say user ID one and now I run this

00:18:22,259 --> 00:18:28,919
query so let's say query tracks and ID

00:18:25,549 --> 00:18:30,600
and name I'm only going to get tracks

00:18:28,919 --> 00:18:32,220
that kind of I'm not gonna get a whole

00:18:30,600 --> 00:18:35,190
set of tracks I just get track that

00:18:32,220 --> 00:18:36,690
belongs to my current artist ID right as

00:18:35,190 --> 00:18:37,970
opposed to this when I remove this and

00:18:36,690 --> 00:18:40,889
I'm running the whole thing as an admin

00:18:37,970 --> 00:18:43,110
I'm gonna get a literally large number

00:18:40,889 --> 00:18:45,450
of tracks which is the entire database

00:18:43,110 --> 00:18:47,519
right which you should not be doing and

00:18:45,450 --> 00:18:48,840
so this this scheme kind of become

00:18:47,519 --> 00:18:51,090
really complicated I just gave you a

00:18:48,840 --> 00:18:53,899
simple example but this the system can

00:18:51,090 --> 00:18:55,919
be as complex as you need it to be the

00:18:53,899 --> 00:18:59,909
permission rule system or the permission

00:18:55,919 --> 00:19:02,119
policy system is per type so per graphic

00:18:59,909 --> 00:19:04,980
you type that you have you can attach

00:19:02,119 --> 00:19:06,539
filtering conditions that are based

00:19:04,980 --> 00:19:08,399
either on some property of the data

00:19:06,539 --> 00:19:09,990
graph or based on some property of the

00:19:08,399 --> 00:19:11,369
session that you have conditions can

00:19:09,990 --> 00:19:13,169
traverse relationships like I just

00:19:11,369 --> 00:19:14,639
showed you with tracks track itself

00:19:13,169 --> 00:19:15,990
doesn't have a property that tells you

00:19:14,639 --> 00:19:17,639
whether you own it or not like track

00:19:15,990 --> 00:19:19,440
doesn't have an artist ID but the track

00:19:17,639 --> 00:19:22,139
has an album which has an artist ID like

00:19:19,440 --> 00:19:25,139
a Google Doc or a github repo that has

00:19:22,139 --> 00:19:27,119
an org that you're a member of and then

00:19:25,139 --> 00:19:29,100
you can namespace these policies you

00:19:27,119 --> 00:19:30,269
know with what we call roles but they

00:19:29,100 --> 00:19:32,309
really just namespaces for these

00:19:30,269 --> 00:19:34,139
policies and you can apply a multiple of

00:19:32,309 --> 00:19:36,809
them at the same time right and this

00:19:34,139 --> 00:19:38,580
kind of creates a graph and then apply

00:19:36,809 --> 00:19:41,460
is a security layer over that graph and

00:19:38,580 --> 00:19:44,129
now you kind of get a graph QL the graph

00:19:41,460 --> 00:19:45,659
QL API that you can securely use um we

00:19:44,129 --> 00:19:47,429
can also do stuff like you also do stuff

00:19:45,659 --> 00:19:49,080
like schema visibility so you can decide

00:19:47,429 --> 00:19:50,460
that hey these are certain fields that

00:19:49,080 --> 00:19:52,470
we want to make accessible from this

00:19:50,460 --> 00:19:53,580
final data graph or not or you want to

00:19:52,470 --> 00:19:55,440
make everything accessible and that's

00:19:53,580 --> 00:19:57,600
also fine to cool things

00:19:55,440 --> 00:20:00,120
coming up or that will allow conditions

00:19:57,600 --> 00:20:02,280
on a part of the data graph or a part of

00:20:00,120 --> 00:20:04,350
the property that comes in from outside

00:20:02,280 --> 00:20:05,730
the same upstream system so you can

00:20:04,350 --> 00:20:07,740
actually use the property of a data

00:20:05,730 --> 00:20:09,990
graph from a different service and apply

00:20:07,740 --> 00:20:11,580
that on to a different service and we'd

00:20:09,990 --> 00:20:12,680
also have memorize and cache functions

00:20:11,580 --> 00:20:15,960
so that you can actually run pretty much

00:20:12,680 --> 00:20:18,870
arbitrary logic for determining access

00:20:15,960 --> 00:20:20,250
to a particular resource as well but all

00:20:18,870 --> 00:20:22,110
of this is kind of possible because of

00:20:20,250 --> 00:20:24,060
that kind of compiler style architecture

00:20:22,110 --> 00:20:25,590
where we can mix in and plug in the

00:20:24,060 --> 00:20:27,690
right things at the right stage of

00:20:25,590 --> 00:20:33,990
processing the graphical query out all

00:20:27,690 --> 00:20:35,970
right how many of you use Postgres okay

00:20:33,990 --> 00:20:38,400
cool so some extra details on Postgres

00:20:35,970 --> 00:20:43,170
and what we do with Postgres that makes

00:20:38,400 --> 00:20:44,730
it really really nice so Posterous is

00:20:43,170 --> 00:20:45,780
something occupied statements how many

00:20:44,730 --> 00:20:47,970
of you know about prepared statements in

00:20:45,780 --> 00:20:49,320
Postgres all right cool that's that's

00:20:47,970 --> 00:20:51,480
good everybody should know about

00:20:49,320 --> 00:20:52,800
prepared statements in Postgres prepared

00:20:51,480 --> 00:20:54,510
statements in Postgres are very similar

00:20:52,800 --> 00:20:57,450
to graph QL queries and graphical query

00:20:54,510 --> 00:20:58,950
variables right what Postgres does has a

00:20:57,450 --> 00:21:01,200
database is pretty much the same thing

00:20:58,950 --> 00:21:03,360
that we do with graph QL right I mean

00:21:01,200 --> 00:21:05,310
not we as a throw but all of us when we

00:21:03,360 --> 00:21:07,290
build a graphical server post-class is a

00:21:05,310 --> 00:21:09,270
sequel server it's a sequel server that

00:21:07,290 --> 00:21:11,010
actually makes file system calls to

00:21:09,270 --> 00:21:12,390
fetch data from the disk just like what

00:21:11,010 --> 00:21:14,370
we are doing which is when we are taking

00:21:12,390 --> 00:21:17,940
with graphical query and fetching data

00:21:14,370 --> 00:21:19,350
from from a network or an API so a lot

00:21:17,940 --> 00:21:21,840
of stuff that you see in Postgres and

00:21:19,350 --> 00:21:24,690
sequel is stuff that you see in graph QL

00:21:21,840 --> 00:21:26,730
as well and that is not a coincidence so

00:21:24,690 --> 00:21:29,070
for example when Postgres plus is a

00:21:26,730 --> 00:21:31,050
sequel query post-class takes the sequel

00:21:29,070 --> 00:21:33,620
query it plans optimized and executes it

00:21:31,050 --> 00:21:36,000
but when you running large queries

00:21:33,620 --> 00:21:37,620
creating the graph QL creating the

00:21:36,000 --> 00:21:39,120
sequel query plan is actually expensive

00:21:37,620 --> 00:21:41,460
which is why when we often talk about

00:21:39,120 --> 00:21:43,620
graph QL query caching we talked a lot

00:21:41,460 --> 00:21:44,970
about query plan caching also because

00:21:43,620 --> 00:21:47,040
sometimes computing the graphical query

00:21:44,970 --> 00:21:49,380
plan is also a little bit expensive that

00:21:47,040 --> 00:21:51,180
you can avoid if you prepare the graph

00:21:49,380 --> 00:21:53,250
cure query plan you can do the same

00:21:51,180 --> 00:21:54,180
thing with sequel which is that you can

00:21:53,250 --> 00:21:56,430
take sequel

00:21:54,180 --> 00:21:57,930
large sequel statements and now you know

00:21:56,430 --> 00:21:59,400
that the graph cure query is going to

00:21:57,930 --> 00:22:00,960
always equal statements so you don't

00:21:59,400 --> 00:22:02,610
need to compile the whole thing again

00:22:00,960 --> 00:22:04,050
right you know that only certain

00:22:02,610 --> 00:22:05,730
variables are going to change for

00:22:04,050 --> 00:22:07,200
example the current user ID which is

00:22:05,730 --> 00:22:09,330
what you're using to filter the current

00:22:07,200 --> 00:22:11,039
user and so what you can do

00:22:09,330 --> 00:22:13,440
is you can take a graph query with graph

00:22:11,039 --> 00:22:14,779
QL query variables now and internally

00:22:13,440 --> 00:22:18,239
convert that to a prepared statement

00:22:14,779 --> 00:22:19,860
with sequel query variables and so what

00:22:18,239 --> 00:22:21,239
you're really doing when you're making a

00:22:19,860 --> 00:22:23,850
graph QL query is you're

00:22:21,239 --> 00:22:25,499
short-circuiting to a sequel query plan

00:22:23,850 --> 00:22:27,809
and you're just injecting the right

00:22:25,499 --> 00:22:29,489
variables or the right IDs from the

00:22:27,809 --> 00:22:31,940
session information into the right

00:22:29,489 --> 00:22:33,539
variables inside the database this

00:22:31,940 --> 00:22:35,999
actually results in a tremendous

00:22:33,539 --> 00:22:37,679
speed-up is not a surprise because when

00:22:35,999 --> 00:22:38,700
you enable graphical query plan caching

00:22:37,679 --> 00:22:40,320
you see a tremendous speed up in your

00:22:38,700 --> 00:22:43,109
graph your servers so you really ought

00:22:40,320 --> 00:22:45,570
to have seen this with databases - the

00:22:43,109 --> 00:22:47,639
really awesome thing about Postgres

00:22:45,570 --> 00:22:49,859
right which unfortunately other

00:22:47,639 --> 00:22:51,720
databases don't have propriety databases

00:22:49,859 --> 00:22:53,460
do have it but other open source

00:22:51,720 --> 00:22:55,019
relational databases don't I think it

00:22:53,460 --> 00:22:57,359
just landed in my sequel is something

00:22:55,019 --> 00:23:01,850
about JSON aggregations and this is a

00:22:57,359 --> 00:23:03,720
super easy trick to speed up even

00:23:01,850 --> 00:23:05,999
hopefully even speed up whatever you're

00:23:03,720 --> 00:23:08,639
doing with graph QL or even with rest in

00:23:05,999 --> 00:23:11,039
a typical in a typical web API back-end

00:23:08,639 --> 00:23:13,440
what you do is when you get a query your

00:23:11,039 --> 00:23:14,850
web back-end weather graph QL or not

00:23:13,440 --> 00:23:16,379
makes a sequel query to the database

00:23:14,850 --> 00:23:18,960
when you make a sequel query to the

00:23:16,379 --> 00:23:23,009
database what you end up doing is you

00:23:18,960 --> 00:23:25,080
say hey run this query for me and then

00:23:23,009 --> 00:23:26,759
you get your response back in the sequel

00:23:25,080 --> 00:23:28,289
protocol alright not secret protocol but

00:23:26,759 --> 00:23:30,600
in some protocol that you're using with

00:23:28,289 --> 00:23:32,629
the ORM then you take that data you load

00:23:30,600 --> 00:23:35,009
that up into memory and then you

00:23:32,629 --> 00:23:36,899
transform it and then you see realize

00:23:35,009 --> 00:23:39,119
that back to JSON and then you send that

00:23:36,899 --> 00:23:40,739
JSON to the client right so if you look

00:23:39,119 --> 00:23:43,739
at if you look at what you're doing

00:23:40,739 --> 00:23:45,119
you're doing an order n traversal on the

00:23:43,739 --> 00:23:47,940
response that you're getting you're

00:23:45,119 --> 00:23:49,590
reading every single byte once from the

00:23:47,940 --> 00:23:51,179
database you're loading it into memory

00:23:49,590 --> 00:23:53,759
and then you're reading every single

00:23:51,179 --> 00:23:55,559
byte again to convert that to JSON and

00:23:53,759 --> 00:23:55,980
then send that back down to the to the

00:23:55,559 --> 00:23:58,889
client

00:23:55,980 --> 00:24:01,139
right when we create the sequel query we

00:23:58,889 --> 00:24:03,330
don't do that what we do is we push the

00:24:01,139 --> 00:24:06,600
JSON aggregation down into the database

00:24:03,330 --> 00:24:08,399
itself so we tell Postgres don't give us

00:24:06,600 --> 00:24:11,549
the result in your weird flat table

00:24:08,399 --> 00:24:13,230
right if you would have noticed that the

00:24:11,549 --> 00:24:14,700
Select statement that I showed you would

00:24:13,230 --> 00:24:16,830
have actually resulted in a flat table

00:24:14,700 --> 00:24:17,820
you would have gotten like one row that

00:24:16,830 --> 00:24:19,409
has all the properties you would have

00:24:17,820 --> 00:24:21,359
not gotten this beautiful nest in JSON

00:24:19,409 --> 00:24:23,020
and so what we do is we tell Postgres

00:24:21,359 --> 00:24:25,210
hey instead of giving

00:24:23,020 --> 00:24:27,400
results back to us in this flat rope why

00:24:25,210 --> 00:24:28,810
don't you just give us a result back in

00:24:27,400 --> 00:24:30,130
the right shape of the JSON that you

00:24:28,810 --> 00:24:32,170
have which is something all JSON

00:24:30,130 --> 00:24:33,580
aggregations and that way we can stream

00:24:32,170 --> 00:24:36,220
the results straight back from the

00:24:33,580 --> 00:24:38,260
database all the way to the client this

00:24:36,220 --> 00:24:40,570
is us in practice in about a 5 to 8 X

00:24:38,260 --> 00:24:42,010
speed-up over what you're doing whether

00:24:40,570 --> 00:24:43,540
you're building rest or graph QL or

00:24:42,010 --> 00:24:44,890
whatever if you have JSON aggregations

00:24:43,540 --> 00:24:47,800
and you know how to use them you should

00:24:44,890 --> 00:24:49,660
probably use them it's amazing cool all

00:24:47,800 --> 00:24:51,340
right now this system that I talked

00:24:49,660 --> 00:24:52,690
about seems like it's fairly coupled to

00:24:51,340 --> 00:24:53,950
what we're doing with the database but

00:24:52,690 --> 00:24:56,350
actually that's not the case

00:24:53,950 --> 00:24:59,080
the underlying architecture of the way

00:24:56,350 --> 00:25:02,020
herself works is that there is a system

00:24:59,080 --> 00:25:03,370
which actually Maps types into the graph

00:25:02,020 --> 00:25:05,380
cure schema to create the graph cure

00:25:03,370 --> 00:25:07,150
schema types this source of types could

00:25:05,380 --> 00:25:09,030
be Postgres catalog but it would

00:25:07,150 --> 00:25:12,790
actually be anything at all for example

00:25:09,030 --> 00:25:15,160
I can use a system like one graph that

00:25:12,790 --> 00:25:17,590
has exposes a graph QL API on top of

00:25:15,160 --> 00:25:19,450
stripe and then I can kind of import

00:25:17,590 --> 00:25:21,220
types from that graph QL server and I

00:25:19,450 --> 00:25:23,530
can then import types from this database

00:25:21,220 --> 00:25:25,180
and then I can configure relationships

00:25:23,530 --> 00:25:26,890
and authorization policies across this

00:25:25,180 --> 00:25:28,120
new graph that I get and then I can

00:25:26,890 --> 00:25:30,970
actually query the whole thing in one

00:25:28,120 --> 00:25:33,190
shot so here's a quick example of what

00:25:30,970 --> 00:25:36,580
that might look like hmm I have a

00:25:33,190 --> 00:25:38,560
customer table in my database which has

00:25:36,580 --> 00:25:43,540
ID name email and something at a stripe

00:25:38,560 --> 00:25:44,650
ID I have a stripe API which is coming

00:25:43,540 --> 00:25:47,290
through me through one graph with a

00:25:44,650 --> 00:25:49,090
secure token that you can take and use

00:25:47,290 --> 00:25:50,170
but it only has dummy cards so nothing

00:25:49,090 --> 00:25:51,760
will happen but this is the back end

00:25:50,170 --> 00:25:53,200
right in the front end is never seeing

00:25:51,760 --> 00:25:55,390
this token so this is my kind of like

00:25:53,200 --> 00:25:57,940
stripe access token or one graph access

00:25:55,390 --> 00:25:59,680
token and so then what I do is I

00:25:57,940 --> 00:26:02,650
configure so now these types are kind of

00:25:59,680 --> 00:26:03,370
loaded into our server and then what you

00:26:02,650 --> 00:26:05,080
can do is you can configure

00:26:03,370 --> 00:26:07,480
relationships across them so you say hey

00:26:05,080 --> 00:26:10,870
you know what this customer it has a

00:26:07,480 --> 00:26:13,570
relationship to stripe and so then I

00:26:10,870 --> 00:26:15,580
configure a relationship to stripe and I

00:26:13,570 --> 00:26:17,530
say that the stripe customer ID kind of

00:26:15,580 --> 00:26:19,900
keys into the stripe graph something

00:26:17,530 --> 00:26:21,370
something ID right and then as soon as I

00:26:19,900 --> 00:26:23,290
kind of put that together what I can do

00:26:21,370 --> 00:26:26,890
is start making graph QL queries that

00:26:23,290 --> 00:26:30,730
say hey I want to fetch ID name an email

00:26:26,890 --> 00:26:32,620
from kind of our database and then save

00:26:30,730 --> 00:26:35,920
guards from stripe so you're kind of

00:26:32,620 --> 00:26:37,930
looking at that data in one shot

00:26:35,920 --> 00:26:41,410
this is secure so a front-end

00:26:37,930 --> 00:26:42,940
application or an API call for like if

00:26:41,410 --> 00:26:45,670
I'm making the API call I won't be able

00:26:42,940 --> 00:26:47,890
to fetch your saved cards and and this

00:26:45,670 --> 00:26:50,920
works this idea can be extended even

00:26:47,890 --> 00:26:53,620
further kind of takes me into what the

00:26:50,920 --> 00:26:55,000
underlying architecture here is um

00:26:53,620 --> 00:26:56,650
this is kind of actually what it looks

00:26:55,000 --> 00:26:57,820
like let's query parsing there's the

00:26:56,650 --> 00:27:00,250
authorization engine and then there's a

00:26:57,820 --> 00:27:02,680
planner and the planner understands how

00:27:00,250 --> 00:27:04,780
to convert that graph to your query into

00:27:02,680 --> 00:27:05,890
the sequel query but what this and we

00:27:04,780 --> 00:27:07,480
call this the data wrapper

00:27:05,890 --> 00:27:10,330
this is actually an interface that

00:27:07,480 --> 00:27:11,710
anybody can program against right and so

00:27:10,330 --> 00:27:13,510
we'll be now putting in work to make

00:27:11,710 --> 00:27:14,980
sure that that open that that interface

00:27:13,510 --> 00:27:16,870
which is already open source is actually

00:27:14,980 --> 00:27:18,820
going to be easier to program against

00:27:16,870 --> 00:27:21,820
and what you can do is you can start

00:27:18,820 --> 00:27:23,290
adding systems that talk to anything so

00:27:21,820 --> 00:27:24,940
what we have today is we have sequel and

00:27:23,290 --> 00:27:27,130
we have graph QL but it can actually

00:27:24,940 --> 00:27:28,900
speak to any kind of upstream system as

00:27:27,130 --> 00:27:31,110
long as that upstream system has some

00:27:28,900 --> 00:27:33,250
way of giving you type information right

00:27:31,110 --> 00:27:34,210
and I had a little Jeff here which

00:27:33,250 --> 00:27:35,830
should have loaded up at the right time

00:27:34,210 --> 00:27:37,000
but anyway it can load it can talk to

00:27:35,830 --> 00:27:39,250
anything right so for example you can

00:27:37,000 --> 00:27:41,530
talk to Athena sequel and let you query

00:27:39,250 --> 00:27:43,660
s3 which is what we heard from a user

00:27:41,530 --> 00:27:45,490
yesterday or you can use that to query

00:27:43,660 --> 00:27:47,590
 with mongoose queries right all

00:27:45,490 --> 00:27:49,840
you need to do is implement the back end

00:27:47,590 --> 00:27:51,580
of this compiler to target whatever

00:27:49,840 --> 00:27:54,160
query language or whatever query system

00:27:51,580 --> 00:27:54,910
you want a very loose and completely

00:27:54,160 --> 00:27:58,570
inaccurate

00:27:54,910 --> 00:28:00,820
analogy is let's say you have GCC and UC

00:27:58,570 --> 00:28:04,540
sees a C compiler and you're using GCC

00:28:00,820 --> 00:28:06,340
to compile C into x86 what and if you

00:28:04,540 --> 00:28:08,170
have a new CPU that you build which is

00:28:06,340 --> 00:28:09,790
let's say arm the arm instruction set

00:28:08,170 --> 00:28:11,290
you're not going to rewrite all of GCC

00:28:09,790 --> 00:28:13,840
right you're going to rewrite that last

00:28:11,290 --> 00:28:15,850
portion that gives you a specific

00:28:13,840 --> 00:28:19,960
instruction set this is a very

00:28:15,850 --> 00:28:20,590
inaccurate loose analogy but it's cool

00:28:19,960 --> 00:28:23,110
all right

00:28:20,590 --> 00:28:25,600
so that's that's that's all that I'm

00:28:23,110 --> 00:28:27,160
gonna be talking about for now I'm also

00:28:25,600 --> 00:28:29,650
hosting a topic table I'm talking about

00:28:27,160 --> 00:28:31,870
graph QL and micro services over lunch

00:28:29,650 --> 00:28:34,750
and so that starts in a bit please do

00:28:31,870 --> 00:28:36,490
pop by and I'd like to make a quick

00:28:34,750 --> 00:28:37,920
announcement about something cool that

00:28:36,490 --> 00:28:41,080
we've been working on which is

00:28:37,920 --> 00:28:42,700
event-driven graph cure mutations please

00:28:41,080 --> 00:28:45,100
do stop by our booth to check this out

00:28:42,700 --> 00:28:48,430
it is extremely exciting it is basically

00:28:45,100 --> 00:28:49,630
the redux CQRS or flux like pattern but

00:28:48,430 --> 00:28:51,970
bringing that to graph cure

00:28:49,630 --> 00:28:53,890
mutations and event-driven stuff that

00:28:51,970 --> 00:28:56,140
people on the backend are quite excited

00:28:53,890 --> 00:28:58,870
about so please do hit me up and let's

00:28:56,140 --> 00:29:01,750
chat about that that's me on Twitter

00:28:58,870 --> 00:29:04,840
we're also hiring so please do check out

00:29:01,750 --> 00:29:05,920
our jobs please feel free to accost me

00:29:04,840 --> 00:29:08,080
with any questions that you may have

00:29:05,920 --> 00:29:12,449
after this thank you

00:29:08,080 --> 00:29:12,449

YouTube URL: https://www.youtube.com/watch?v=HOKMJkBYaqQ


