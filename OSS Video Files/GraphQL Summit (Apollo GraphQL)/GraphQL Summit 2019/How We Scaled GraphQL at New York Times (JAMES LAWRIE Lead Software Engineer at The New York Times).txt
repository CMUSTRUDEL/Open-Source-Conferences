Title: How We Scaled GraphQL at New York Times (JAMES LAWRIE Lead Software Engineer at The New York Times)
Publication date: 2019-11-02
Playlist: GraphQL Summit 2019
Description: 
	The New York Times delivered around 5 billion GraphQL responses last month, but only a fraction of those were executed by the GraphQL server application. This session will go into detail about how we use Fastly to deliver all this data and the challenges that we faced along the way.
Captions: 
	00:00:00,939 --> 00:00:08,620
hello so yes I'm James Laurie I'm see

00:00:09,940 --> 00:00:14,959
yes

00:00:11,240 --> 00:00:17,600
lead engineer at new york times working

00:00:14,959 --> 00:00:23,840
with graph QL and i'm gonna tell you

00:00:17,600 --> 00:00:29,390
about how we scaled a graph QL so first

00:00:23,840 --> 00:00:32,360
a little background on what graph QL

00:00:29,390 --> 00:00:34,430
looks like at the New York Times then

00:00:32,360 --> 00:00:37,430
I'll go over some basics of caching

00:00:34,430 --> 00:00:39,350
graph QL with the CDN if you were here

00:00:37,430 --> 00:00:40,850
for the previous talk there's a little

00:00:39,350 --> 00:00:43,070
overlap but I think this is sort of a

00:00:40,850 --> 00:00:46,340
practical application of what marc-andre

00:00:43,070 --> 00:00:48,380
was talking about then I'll be talking

00:00:46,340 --> 00:00:53,050
about how additional request metadata

00:00:48,380 --> 00:00:55,219
can affect how you cache graph QL

00:00:53,050 --> 00:00:58,190
finally I'll talk about how we ensure

00:00:55,219 --> 00:01:00,260
that our caches are the freshest

00:00:58,190 --> 00:01:02,780
possible which is very important for a

00:01:00,260 --> 00:01:06,409
news organization and how we do active

00:01:02,780 --> 00:01:08,299
cache invalidation so we've been running

00:01:06,409 --> 00:01:13,759
oh sorry finally we'll be doing some

00:01:08,299 --> 00:01:17,659
takeaways at the end so we've been

00:01:13,759 --> 00:01:20,060
running graph QL in production since q1

00:01:17,659 --> 00:01:22,909
of 2017

00:01:20,060 --> 00:01:25,789
so about two and a half years it started

00:01:22,909 --> 00:01:29,179
as an experiment implementing a new

00:01:25,789 --> 00:01:30,579
homepage if you're starting with graph

00:01:29,179 --> 00:01:33,409
QL now I wouldn't necessarily recommend

00:01:30,579 --> 00:01:35,899
choosing your most your largest

00:01:33,409 --> 00:01:40,399
application but that has how we did it

00:01:35,899 --> 00:01:43,340
and it worked out okay for us so this is

00:01:40,399 --> 00:01:45,439
what a graph QL server instance looks

00:01:43,340 --> 00:01:47,479
like at the New York Times on the

00:01:45,439 --> 00:01:49,969
left-hand side you'll see our HTTP

00:01:47,479 --> 00:01:52,789
interface is written using Sinatra and

00:01:49,969 --> 00:01:55,609
finagle which is open source projects

00:01:52,789 --> 00:01:58,520
from Twitter then we have a layer of

00:01:55,609 --> 00:02:01,310
caching of the queries using caffeine

00:01:58,520 --> 00:02:04,249
and Redis in the middle of all that we

00:02:01,310 --> 00:02:07,249
have sangria which is our Scala based

00:02:04,249 --> 00:02:09,470
execution engine and then the other side

00:02:07,249 --> 00:02:11,900
we also have more caching or caching

00:02:09,470 --> 00:02:13,940
each individual response we're getting

00:02:11,900 --> 00:02:14,600
from our data sources and then finally

00:02:13,940 --> 00:02:17,180
we use for

00:02:14,600 --> 00:02:21,200
to manage all the networking with our

00:02:17,180 --> 00:02:23,810
upstream data sources and this is how

00:02:21,200 --> 00:02:25,430
graph QL fits into our front facing

00:02:23,810 --> 00:02:28,130
architectures so again starting on the

00:02:25,430 --> 00:02:31,280
Left we have our major clients our web

00:02:28,130 --> 00:02:33,850
browser our Android app and our iOS app

00:02:31,280 --> 00:02:37,310
two of which are using Apollo client

00:02:33,850 --> 00:02:40,130
then we had this layer fastly so fastly

00:02:37,310 --> 00:02:42,860
as a CDN and it's what allows us to

00:02:40,130 --> 00:02:48,110
deliver a fast reliable cash pool

00:02:42,860 --> 00:02:49,790
content to all our users and we have a

00:02:48,110 --> 00:02:52,700
config for our website and we have a

00:02:49,790 --> 00:02:54,590
config for graph QL and then we have the

00:02:52,700 --> 00:02:57,050
web application which is serving the

00:02:54,590 --> 00:02:59,240
HTML pages connecting to the graph QL

00:02:57,050 --> 00:03:00,740
server behind the scenes and then

00:02:59,240 --> 00:03:06,650
finally on the right we have all our

00:03:00,740 --> 00:03:11,180
data sources so some basic stats about

00:03:06,650 --> 00:03:14,930
our graph QL stack we did about 40

00:03:11,180 --> 00:03:18,440
billion queries last month that was 10

00:03:14,930 --> 00:03:23,300
terabytes of data received and why don't

00:03:18,440 --> 00:03:26,150
have petabytes of data served our we

00:03:23,300 --> 00:03:28,000
have about 70 different clients that are

00:03:26,150 --> 00:03:31,970
using graph QL the New York Times and

00:03:28,000 --> 00:03:37,790
our schema is about 500 types and 3,000

00:03:31,970 --> 00:03:39,530
fields so again if you were at the

00:03:37,790 --> 00:03:42,290
previous talk some of this may be a

00:03:39,530 --> 00:03:45,320
little duplicitous but I'm gonna go over

00:03:42,290 --> 00:03:48,890
sort of the basics of HTTP caching and

00:03:45,320 --> 00:03:51,080
how it relates to a CDN so the graph QL

00:03:48,890 --> 00:03:53,360
spec itself does not define the

00:03:51,080 --> 00:03:57,410
transport of course most people are

00:03:53,360 --> 00:03:59,750
using HTTP with JSON but doesn't

00:03:57,410 --> 00:04:03,620
actually say how you should do it and

00:03:59,750 --> 00:04:06,230
you may not know but you can actually do

00:04:03,620 --> 00:04:08,210
get requests with graph QL commonly

00:04:06,230 --> 00:04:10,400
using posts but gets are also possible

00:04:08,210 --> 00:04:12,470
so this is what a get request might look

00:04:10,400 --> 00:04:14,060
like of course you have your single

00:04:12,470 --> 00:04:16,100
endpoint for every graphic you'll

00:04:14,060 --> 00:04:17,780
request you have your query and your

00:04:16,100 --> 00:04:18,890
query string you might have variables in

00:04:17,780 --> 00:04:23,600
your query string and you might have

00:04:18,890 --> 00:04:28,849
operation names but you quickly run into

00:04:23,600 --> 00:04:30,969
limits using gets so first

00:04:28,849 --> 00:04:32,930
the order of your query string can

00:04:30,969 --> 00:04:36,770
fragment your cache can affect your

00:04:32,930 --> 00:04:38,870
cache ability the order of the variables

00:04:36,770 --> 00:04:40,819
within your query can also affect the

00:04:38,870 --> 00:04:43,909
cache ability if you're not careful

00:04:40,819 --> 00:04:46,639
about keeping order and then finally

00:04:43,909 --> 00:04:49,250
most systems are going to have a limit

00:04:46,639 --> 00:04:52,039
on the size of the the get request that

00:04:49,250 --> 00:04:53,930
first line in the HTTP request in our

00:04:52,039 --> 00:04:57,469
case we're using fastly and they have an

00:04:53,930 --> 00:05:01,759
8k limit which was not what not doable

00:04:57,469 --> 00:05:03,860
for us so if we look at the post the

00:05:01,759 --> 00:05:05,930
post is very similar everything is just

00:05:03,860 --> 00:05:08,569
in the body instead you still have your

00:05:05,930 --> 00:05:11,270
single endpoint and most CMS are not

00:05:08,569 --> 00:05:15,110
going to work well with post but in our

00:05:11,270 --> 00:05:18,080
case fastly does allow you to create a

00:05:15,110 --> 00:05:21,189
cache key based on the post body so that

00:05:18,080 --> 00:05:24,650
is the approach that that we took more

00:05:21,189 --> 00:05:27,080
limitations we ran into again you have

00:05:24,650 --> 00:05:29,840
to be concerned about the order of the

00:05:27,080 --> 00:05:33,289
query in your body and make sure that

00:05:29,840 --> 00:05:35,000
you have normalized queries we also ran

00:05:33,289 --> 00:05:38,690
into a problem in production where the

00:05:35,000 --> 00:05:40,520
post body couldn't be gzipped if fastly

00:05:38,690 --> 00:05:42,169
receives it gzipped request body is just

00:05:40,520 --> 00:05:44,389
gonna ignore it

00:05:42,169 --> 00:05:50,539
which led to some fun production

00:05:44,389 --> 00:05:52,550
problems that day ok so how can we solve

00:05:50,539 --> 00:05:55,550
the problem where in our case we have

00:05:52,550 --> 00:05:57,560
queries which are exceeding the 8k limit

00:05:55,550 --> 00:06:00,860
both forget and posts we have queries

00:05:57,560 --> 00:06:03,469
that are up to 40k in size so along

00:06:00,860 --> 00:06:05,719
comes a protocol that Apollo developed

00:06:03,469 --> 00:06:09,199
called automatic automatic persistent

00:06:05,719 --> 00:06:11,509
queries which basically means that the

00:06:09,199 --> 00:06:14,270
client is instead of sending the entire

00:06:11,509 --> 00:06:17,539
query gonna create a hash of that and

00:06:14,270 --> 00:06:20,810
send that hash instead so that's what

00:06:17,539 --> 00:06:22,610
it's gonna look like and it's gonna be a

00:06:20,810 --> 00:06:24,979
very small post body it can still be a

00:06:22,610 --> 00:06:27,469
post it could still be a get you could

00:06:24,979 --> 00:06:29,629
put the hash right in your query string

00:06:27,469 --> 00:06:36,500
if you wanted but we're continued use

00:06:29,629 --> 00:06:39,529
posts so the basic protocol I won't go

00:06:36,500 --> 00:06:41,149
into in detail there's a great blog post

00:06:39,529 --> 00:06:41,860
on the Apollo site you can read about

00:06:41,149 --> 00:06:44,350
this

00:06:41,860 --> 00:06:47,050
again the client creates a hash of the

00:06:44,350 --> 00:06:49,030
query sends that hash to the server if

00:06:47,050 --> 00:06:51,220
the server doesn't understand this hash

00:06:49,030 --> 00:06:53,110
it's gonna send back a specific response

00:06:51,220 --> 00:06:55,300
which is going to trigger the client to

00:06:53,110 --> 00:06:57,580
send the entire query on the next

00:06:55,300 --> 00:07:00,220
request and then the server will go

00:06:57,580 --> 00:07:08,590
ahead and cache that query with that

00:07:00,220 --> 00:07:12,820
hash and go about its processing so what

00:07:08,590 --> 00:07:15,460
I mean by additional request metadata so

00:07:12,820 --> 00:07:17,980
in our case we have certain pieces of

00:07:15,460 --> 00:07:20,710
information that are not sent along with

00:07:17,980 --> 00:07:22,990
the graph QL query they're coming from

00:07:20,710 --> 00:07:25,330
request headers let's say or cookies or

00:07:22,990 --> 00:07:27,850
whatnot and this is when you're talking

00:07:25,330 --> 00:07:30,550
about personalization and geo-targeting

00:07:27,850 --> 00:07:33,010
this information is sort of available in

00:07:30,550 --> 00:07:34,630
metadata but it's not part of the cache

00:07:33,010 --> 00:07:39,570
keys we were just talking about in terms

00:07:34,630 --> 00:07:42,610
of the post body or the get request line

00:07:39,570 --> 00:07:45,690
so where do we use geo-targeting at the

00:07:42,610 --> 00:07:49,270
times well this is this is the home page

00:07:45,690 --> 00:07:53,190
you can see on the bottom there on the

00:07:49,270 --> 00:07:53,190
bottom right left sorry

00:07:56,810 --> 00:08:07,639
oh no sorry yes all right

00:08:05,510 --> 00:08:13,370
the bottom left we have what we call our

00:08:07,639 --> 00:08:15,230
daily briefing it's aware that you're in

00:08:13,370 --> 00:08:17,150
the morning wherever you are and see

00:08:15,230 --> 00:08:20,090
your New York so and it's in the morning

00:08:17,150 --> 00:08:22,850
so you get your your daily briefing and

00:08:20,090 --> 00:08:24,889
then right next to that knows you're in

00:08:22,850 --> 00:08:30,470
New York so you get your your New York

00:08:24,889 --> 00:08:32,870
today briefing so how how can we add

00:08:30,470 --> 00:08:36,800
geo-targeting to our caching strategy

00:08:32,870 --> 00:08:38,210
here we could just for every single

00:08:36,800 --> 00:08:42,770
graph you all request add that

00:08:38,210 --> 00:08:44,870
geo-targeting metadata maybe if all your

00:08:42,770 --> 00:08:47,150
your whole schema requires do you

00:08:44,870 --> 00:08:50,000
targeting that might make sense but it

00:08:47,150 --> 00:08:51,800
does fragment your cache and for us only

00:08:50,000 --> 00:08:55,510
a few things in the in the schema

00:08:51,800 --> 00:08:55,510
require that geo-targeting information

00:08:56,560 --> 00:09:05,210
so instead we use a strategy using a

00:09:01,730 --> 00:09:09,080
very header so there are limits also

00:09:05,210 --> 00:09:12,620
with very headers you can only have 200

00:09:09,080 --> 00:09:15,320
variations and fastly based on based on

00:09:12,620 --> 00:09:17,450
that single header in our case for the

00:09:15,320 --> 00:09:20,720
geo-targeting things are very targeted

00:09:17,450 --> 00:09:23,390
we have a pretty small set of things of

00:09:20,720 --> 00:09:30,350
queue locations that we target so this

00:09:23,390 --> 00:09:32,930
isn't currently a problem for us so how

00:09:30,350 --> 00:09:38,060
does how does the vary header work with

00:09:32,930 --> 00:09:42,580
fastly so this is a diagram of what

00:09:38,060 --> 00:09:47,990
happens on the initial request response

00:09:42,580 --> 00:09:49,700
let's see here this is a picture of when

00:09:47,990 --> 00:09:52,970
fastly is receiving your initial

00:09:49,700 --> 00:09:55,910
response from graph QL it gets the cache

00:09:52,970 --> 00:09:58,430
object it checks to see if that response

00:09:55,910 --> 00:10:01,520
has a very header if it does it

00:09:58,430 --> 00:10:03,950
calculate something called a very key so

00:10:01,520 --> 00:10:06,260
let's say in this case the very header

00:10:03,950 --> 00:10:08,750
says geo-target that's the name of our

00:10:06,260 --> 00:10:10,190
request header and on the request side

00:10:08,750 --> 00:10:14,810
that geo-target

00:10:10,190 --> 00:10:16,850
was CA California in this case so fast

00:10:14,810 --> 00:10:21,860
Lee's gonna go ahead and store that

00:10:16,850 --> 00:10:24,339
value California as it's very P now when

00:10:21,860 --> 00:10:27,829
another request comes in with the same

00:10:24,339 --> 00:10:30,079
query the same post body it's gonna look

00:10:27,829 --> 00:10:32,509
up all the objects that match that in

00:10:30,079 --> 00:10:34,459
its cache and it's gonna check to see if

00:10:32,509 --> 00:10:38,649
any of those things have a very key on

00:10:34,459 --> 00:10:42,889
it in this case it does but our request

00:10:38,649 --> 00:10:44,540
has a geo target of New York and fastly

00:10:42,889 --> 00:10:46,310
sees that okay I don't have a very key

00:10:44,540 --> 00:10:48,769
for New York yet in cash so it's gonna

00:10:46,310 --> 00:10:55,250
go ahead and send that request on to the

00:10:48,769 --> 00:10:58,750
origin so how do we know which graph QL

00:10:55,250 --> 00:11:01,370
queries to send very headers back with

00:10:58,750 --> 00:11:04,310
and in our case we decide we would use

00:11:01,370 --> 00:11:07,339
schema directives so schema directives

00:11:04,310 --> 00:11:11,420
are in this way a way of sort of

00:11:07,339 --> 00:11:13,850
annotating what fields in your schema

00:11:11,420 --> 00:11:15,649
might follow a certain behavior this is

00:11:13,850 --> 00:11:17,180
sort of a simplified version of what the

00:11:15,649 --> 00:11:19,069
New York Times schema might look like so

00:11:17,180 --> 00:11:21,350
you've got a home page field and that

00:11:19,069 --> 00:11:23,860
home page type you've got a list of

00:11:21,350 --> 00:11:25,930
briefings and we have argued to

00:11:23,860 --> 00:11:28,339
geo-targeting schema directive

00:11:25,930 --> 00:11:32,480
indicating that briefings need that

00:11:28,339 --> 00:11:34,790
geo-targeting information so this is

00:11:32,480 --> 00:11:39,259
what the process on the server looks

00:11:34,790 --> 00:11:41,449
like so first we received the HTTP

00:11:39,259 --> 00:11:42,980
request from our client we go ahead and

00:11:41,449 --> 00:11:46,339
parse that into a more domain-specific

00:11:42,980 --> 00:11:48,350
graph QL request and then we analyze the

00:11:46,339 --> 00:11:50,779
query looking for all these kinds of

00:11:48,350 --> 00:11:52,540
directives that might change the

00:11:50,779 --> 00:11:55,309
behavior of how we set the response back

00:11:52,540 --> 00:11:56,870
so in this case we see they're requiring

00:11:55,309 --> 00:12:01,370
for the field briefings it has a

00:11:56,870 --> 00:12:04,309
geo-targeting schema directive and we

00:12:01,370 --> 00:12:06,410
add we keep those geo-targeting

00:12:04,309 --> 00:12:09,829
variables in our request as we're

00:12:06,410 --> 00:12:11,300
executing the graph QL query and then

00:12:09,829 --> 00:12:13,430
finally on the way out when everything

00:12:11,300 --> 00:12:15,829
has been calculated we add that very

00:12:13,430 --> 00:12:20,540
response header saying okay we want to

00:12:15,829 --> 00:12:22,130
vary on these geo-targeting headers all

00:12:20,540 --> 00:12:23,870
right so we do the same thing for

00:12:22,130 --> 00:12:27,080
personalization

00:12:23,870 --> 00:12:28,760
in terms of annotating our schema for

00:12:27,080 --> 00:12:30,050
personalization the use case is a little

00:12:28,760 --> 00:12:32,300
bit different we certainly have a lot

00:12:30,050 --> 00:12:37,190
more than 200 users so we can't really

00:12:32,300 --> 00:12:40,250
use very to store a user specific object

00:12:37,190 --> 00:12:42,320
in fastly cache but we still use the

00:12:40,250 --> 00:12:44,750
annotations to let the execution no

00:12:42,320 --> 00:12:47,690
engine know that the user cookie needs

00:12:44,750 --> 00:12:51,740
to be passed along through when we're

00:12:47,690 --> 00:12:54,590
executing our graph QL query so this is

00:12:51,740 --> 00:12:57,380
the same process we saw before the only

00:12:54,590 --> 00:12:59,750
difference is the bottom there instead

00:12:57,380 --> 00:13:02,720
of sending a very header back we set a

00:12:59,750 --> 00:13:06,490
cache control no store so that fastly

00:13:02,720 --> 00:13:06,490
does not store personalized information

00:13:07,420 --> 00:13:13,400
so I'm gonna go through another use case

00:13:11,030 --> 00:13:15,590
here where we we're using the

00:13:13,400 --> 00:13:19,240
personalised schema directive in sort of

00:13:15,590 --> 00:13:22,580
a different way so breaking news alerts

00:13:19,240 --> 00:13:25,430
when the news breaks the New York Times

00:13:22,580 --> 00:13:28,190
sends out notifications all your devices

00:13:25,430 --> 00:13:30,260
and the devices thing hey that's a great

00:13:28,190 --> 00:13:34,310
time to go fetch all the New York Times

00:13:30,260 --> 00:13:38,030
content so what we see is a really huge

00:13:34,310 --> 00:13:40,340
spike in traffic at those times those

00:13:38,030 --> 00:13:42,620
little notches represent a minute so we

00:13:40,340 --> 00:13:47,090
get about a spike of 40 times traffic

00:13:42,620 --> 00:13:48,470
within a minute and then it's gone so

00:13:47,090 --> 00:13:53,630
how do we deal with that

00:13:48,470 --> 00:13:55,580
so in some cases all those devices are

00:13:53,630 --> 00:13:57,440
asking for personalized information and

00:13:55,580 --> 00:13:59,570
when I say personalized in this case

00:13:57,440 --> 00:14:02,240
it's not like specific user information

00:13:59,570 --> 00:14:06,530
it's more like recommendations for that

00:14:02,240 --> 00:14:09,200
user so what we do is in addition to our

00:14:06,530 --> 00:14:12,080
personalized schema directive we add a

00:14:09,200 --> 00:14:17,450
rate limiting filter into our execution

00:14:12,080 --> 00:14:20,600
chain so we first check to see if that

00:14:17,450 --> 00:14:22,760
query has a personalized field then next

00:14:20,600 --> 00:14:26,060
we check and see if our rate limiter

00:14:22,760 --> 00:14:28,160
threshold has been reached and if it has

00:14:26,060 --> 00:14:30,230
we go ahead and remove the

00:14:28,160 --> 00:14:34,700
personalization context from the request

00:14:30,230 --> 00:14:37,580
before we execute our graph QL query and

00:14:34,700 --> 00:14:40,430
what that does is it basically all

00:14:37,580 --> 00:14:42,290
the rest of that machinery kicks in it

00:14:40,430 --> 00:14:45,620
sees that there's no personalized

00:14:42,290 --> 00:14:48,710
information in the request and it sends

00:14:45,620 --> 00:14:52,400
back a normal cache control header with

00:14:48,710 --> 00:14:55,070
a max age of say 30 back to fastly and

00:14:52,400 --> 00:14:58,040
so fastly will cache sort of a

00:14:55,070 --> 00:15:00,170
depersonalized version of that query for

00:14:58,040 --> 00:15:04,450
a short amount of time so that we can

00:15:00,170 --> 00:15:04,450
weather those breaking news alert storms

00:15:04,840 --> 00:15:13,550
all right so as I said for the news

00:15:11,330 --> 00:15:17,930
business having the freshest content in

00:15:13,550 --> 00:15:20,270
your caches is very important so we have

00:15:17,930 --> 00:15:22,900
devised a system of active cache

00:15:20,270 --> 00:15:26,360
invalidation so a little background here

00:15:22,900 --> 00:15:26,990
as we were adopting graph QL at the New

00:15:26,360 --> 00:15:29,540
York Times

00:15:26,990 --> 00:15:32,570
we are also completely rewriting our

00:15:29,540 --> 00:15:35,180
publishing stack and now a completely

00:15:32,570 --> 00:15:37,670
advent driven system and we take

00:15:35,180 --> 00:15:40,960
advantage of those events to actively

00:15:37,670 --> 00:15:43,700
invalidate caches throughout the stack

00:15:40,960 --> 00:15:45,800
so there's a great blog post on the New

00:15:43,700 --> 00:15:48,640
York Times open blog about how we're

00:15:45,800 --> 00:15:51,560
using Kafka I'm just gonna very briefly

00:15:48,640 --> 00:15:55,490
show a diagram of what it kind of looks

00:15:51,560 --> 00:15:57,650
like so we have Kafka for our main

00:15:55,490 --> 00:16:01,180
entity topic that publishers are

00:15:57,650 --> 00:16:03,530
publishing - we have consumers that are

00:16:01,180 --> 00:16:05,900
getting all those published events and

00:16:03,530 --> 00:16:08,480
creating materialized views as they need

00:16:05,900 --> 00:16:09,980
and for graph QL purposes the most

00:16:08,480 --> 00:16:11,900
important materialized view is this

00:16:09,980 --> 00:16:15,170
asset datastore which is basically the

00:16:11,900 --> 00:16:18,380
latest version of each asset that's been

00:16:15,170 --> 00:16:21,380
published so what happens after the

00:16:18,380 --> 00:16:23,840
asset data store stores the latest

00:16:21,380 --> 00:16:26,540
version of a published asset it's gonna

00:16:23,840 --> 00:16:29,840
drop a message on another Kafka top yet

00:16:26,540 --> 00:16:32,930
Kafka topic just indicating that ok I've

00:16:29,840 --> 00:16:35,240
processed this publish entity and then

00:16:32,930 --> 00:16:37,820
we have graph QL and we have the web

00:16:35,240 --> 00:16:40,130
application listening for messages on

00:16:37,820 --> 00:16:43,100
this topic and so when graph QL sees

00:16:40,130 --> 00:16:45,020
that a certain SV has been published or

00:16:43,100 --> 00:16:48,660
republished it's gonna go ahead and

00:16:45,020 --> 00:16:50,699
clear its local caches based on the ID

00:16:48,660 --> 00:16:53,190
that published entity and it's gonna

00:16:50,699 --> 00:16:54,930
send a message to the fastly config for

00:16:53,190 --> 00:16:58,529
graph QL saying okay I want to purge

00:16:54,930 --> 00:17:01,079
this entity also then it drops a message

00:16:58,529 --> 00:17:03,500
on that same process topic the web

00:17:01,079 --> 00:17:07,470
application picks it up and it in turn

00:17:03,500 --> 00:17:11,520
invalidates its caches for that entity

00:17:07,470 --> 00:17:14,069
and fastly so this is a little bit

00:17:11,520 --> 00:17:15,929
tricky because if you can think about a

00:17:14,069 --> 00:17:17,760
graphical response is gonna be the

00:17:15,929 --> 00:17:19,679
combination of many many published

00:17:17,760 --> 00:17:22,860
entities so we have our top level

00:17:19,679 --> 00:17:24,870
article but maybe it wasn't the article

00:17:22,860 --> 00:17:26,610
that changed maybe it was the author's

00:17:24,870 --> 00:17:28,760
information that changed maybe it was

00:17:26,610 --> 00:17:31,200
the caption on the image that changed

00:17:28,760 --> 00:17:34,049
maybe it was the video that changed our

00:17:31,200 --> 00:17:36,480
keywords all these things on the screen

00:17:34,049 --> 00:17:39,900
here are all normalized published

00:17:36,480 --> 00:17:42,510
entities in the system likewise on the

00:17:39,900 --> 00:17:44,429
home page we have a home page a

00:17:42,510 --> 00:17:47,309
top-level object that actually doesn't

00:17:44,429 --> 00:17:50,039
change that often but the top stories

00:17:47,309 --> 00:17:51,960
the briefings list all the various lists

00:17:50,039 --> 00:17:54,210
which are going to contain various

00:17:51,960 --> 00:17:56,159
articles and images how do we make sure

00:17:54,210 --> 00:17:57,809
that we can clear the home page when you

00:17:56,159 --> 00:18:01,980
know the third image on the fourth

00:17:57,809 --> 00:18:03,470
article has been changed so we would do

00:18:01,980 --> 00:18:07,679
is we take advantage of something that

00:18:03,470 --> 00:18:10,159
fascicles surrogate keys I've seen other

00:18:07,679 --> 00:18:13,669
CD ends refer to them as cache tags

00:18:10,159 --> 00:18:16,320
basically they're an alternate way of

00:18:13,669 --> 00:18:18,120
purging items out of cache you don't

00:18:16,320 --> 00:18:20,640
have to cat purge items out of cache

00:18:18,120 --> 00:18:25,230
just based on the sort of primary cache

00:18:20,640 --> 00:18:27,169
key that you're using so we wrote some

00:18:25,230 --> 00:18:30,900
middleware in our graph QL stack to

00:18:27,169 --> 00:18:34,530
basically observe the resolution process

00:18:30,900 --> 00:18:36,570
of the graph QL query and store the IDS

00:18:34,530 --> 00:18:38,010
of every single entity that is

00:18:36,570 --> 00:18:41,580
encountered during that resolution

00:18:38,010 --> 00:18:45,419
process and then we drop it in the

00:18:41,580 --> 00:18:48,659
extensions of our graph QL response and

00:18:45,419 --> 00:18:51,000
then as we're creating our HTTP response

00:18:48,659 --> 00:18:54,900
we transform that into a response header

00:18:51,000 --> 00:18:57,450
we send that to fastly fastly stores all

00:18:54,900 --> 00:19:00,030
those tags all those surrogate keys

00:18:57,450 --> 00:19:02,090
along with the cache object we also pass

00:19:00,030 --> 00:19:05,090
it along to the web application which

00:19:02,090 --> 00:19:07,789
or may not at its own surrogate keys and

00:19:05,090 --> 00:19:09,740
all those surrogate keys also get stored

00:19:07,789 --> 00:19:19,669
alongside their cash objects and they're

00:19:09,740 --> 00:19:22,429
fast they can fit again so go back to a

00:19:19,669 --> 00:19:24,830
part of that slide before again we have

00:19:22,429 --> 00:19:27,190
that processed topic which is messages

00:19:24,830 --> 00:19:31,340
as certain parts of the system are

00:19:27,190 --> 00:19:33,740
processing these messages so as graph QL

00:19:31,340 --> 00:19:36,140
sees that certain article has been

00:19:33,740 --> 00:19:38,330
updated it's going to purge its caches

00:19:36,140 --> 00:19:41,419
based on that articles ID and it's gonna

00:19:38,330 --> 00:19:53,510
send a purge message to fastly for that

00:19:41,419 --> 00:19:59,120
ID all right takeaways so caching with

00:19:53,510 --> 00:20:02,140
the CDN is possible gotta know the

00:19:59,120 --> 00:20:05,720
limitations and plan for them be very

00:20:02,140 --> 00:20:09,260
it's good to be very knowledgeable about

00:20:05,720 --> 00:20:11,270
the CDN that you're using if at all

00:20:09,260 --> 00:20:13,640
possible have your clients use get

00:20:11,270 --> 00:20:19,360
requests most CD ends are going to work

00:20:13,640 --> 00:20:21,860
with get requests out of the box if if

00:20:19,360 --> 00:20:25,429
queries exceed the limitations of your

00:20:21,860 --> 00:20:31,429
CDN start using caching persisted

00:20:25,429 --> 00:20:33,470
queries avoid implicit request metadata

00:20:31,429 --> 00:20:35,419
if you can it's much simpler if

00:20:33,470 --> 00:20:37,880
everything you need to calculate your

00:20:35,419 --> 00:20:41,450
graph QL response is in your query

00:20:37,880 --> 00:20:46,630
itself but if you can't avoid that then

00:20:41,450 --> 00:20:46,630
use very headers if at all possible and

00:20:46,809 --> 00:20:53,020
I'm James Laurie and thank you for

00:20:49,730 --> 00:20:53,020
coming and this is Pauly

00:20:53,220 --> 00:20:55,280

YouTube URL: https://www.youtube.com/watch?v=gpd6JtnWs2E


