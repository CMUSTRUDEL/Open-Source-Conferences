Title: Best Practices for GraphQL Clients
Publication date: 2016-11-02
Playlist: GraphQL Summit 2016
Description: 
	Joe Savona, Relay / Facebook Open Source

At Facebook, our GraphQL clients have evolved alongside the language for over 4 years. In this talk we'll explore the lessons we've learned in that time building GraphQL clients for Android, iOS, and JavaScript. We'll look at some missteps and some successes, and share the set of best practices that have emerged across these platforms.

Join our GraphQL SF meetup to hear more about GraphQL best practices and implementations: 
http://meetup.com/GraphQL-SF/

Resources:
Learn more about Summit - https://summit.graphql.com/
Explore the GraphQL FAQs - https://www.apollographql.com/docs/resources/faq
Checkout the Apollo docs - https://www.apollographql.com/docs/
Learn GraphQL using Apollo's Tutorials: https://odyssey.apollographql.com/
Captions: 
	00:00:03,940 --> 00:00:10,240
okay so yeah my name is Joseph Mona I'm

00:00:07,270 --> 00:00:12,040
a software engineer at Facebook and I

00:00:10,240 --> 00:00:14,349
work on the relay team as well as

00:00:12,040 --> 00:00:16,480
working with the graphical teams for

00:00:14,349 --> 00:00:18,880
building our native clients on iOS and

00:00:16,480 --> 00:00:20,740
Android so I've been at Facebook for

00:00:18,880 --> 00:00:22,390
about two years so I haven't been there

00:00:20,740 --> 00:00:24,369
for the entire history of our graphical

00:00:22,390 --> 00:00:26,109
client development so I'm kind of

00:00:24,369 --> 00:00:28,660
preparing for this talk I have looked at

00:00:26,109 --> 00:00:29,769
what we've done in relay as well as

00:00:28,660 --> 00:00:31,900
talking to a whole bunch of you around

00:00:29,769 --> 00:00:33,130
the company and kind of getting more

00:00:31,900 --> 00:00:34,870
information about the early years of

00:00:33,130 --> 00:00:37,329
graph QL just to make sure that I'm kind

00:00:34,870 --> 00:00:39,340
of covering all the bases so at Facebook

00:00:37,329 --> 00:00:41,530
we've developed do we've been developing

00:00:39,340 --> 00:00:43,660
graphical clients for over four years

00:00:41,530 --> 00:00:47,079
and in this talk I want to share about

00:00:43,660 --> 00:00:49,000
the lessons that we've learned so let's

00:00:47,079 --> 00:00:50,559
start by answering the question what is

00:00:49,000 --> 00:00:52,030
a graphical client I mean we think we

00:00:50,559 --> 00:00:53,620
all have an intuitive sense of what it

00:00:52,030 --> 00:00:55,030
should do but let's kind of just nail

00:00:53,620 --> 00:00:57,969
down the specifics of that a little bit

00:00:55,030 --> 00:01:01,809
so a graphical client allows developers

00:00:57,969 --> 00:01:03,910
to specify queries obvious it sends

00:01:01,809 --> 00:01:05,440
those creations that server it parses

00:01:03,910 --> 00:01:08,050
the responses that it gets back and

00:01:05,440 --> 00:01:10,330
makes the parsed objects available to

00:01:08,050 --> 00:01:12,310
user interface code so far so good

00:01:10,330 --> 00:01:14,230
it should also cache data that's already

00:01:12,310 --> 00:01:17,230
been fetched to avoid reaching data

00:01:14,230 --> 00:01:18,910
unnecessarily and also by caching data

00:01:17,230 --> 00:01:20,970
we can allow the application to work

00:01:18,910 --> 00:01:23,940
offline

00:01:20,970 --> 00:01:26,560
it keeps the data and the UI consistent

00:01:23,940 --> 00:01:28,600
so if we've fetched a particular piece

00:01:26,560 --> 00:01:29,800
of information once and that appears in

00:01:28,600 --> 00:01:31,720
multiple places in the in the user

00:01:29,800 --> 00:01:33,580
interface or in a cache and that data

00:01:31,720 --> 00:01:35,230
changes that change should be reflected

00:01:33,580 --> 00:01:39,190
everywhere that piece of information is

00:01:35,230 --> 00:01:40,900
displayed or cached a graphical client

00:01:39,190 --> 00:01:43,180
should make it easy to fetch paginating

00:01:40,900 --> 00:01:45,880
lists so things like the Facebook

00:01:43,180 --> 00:01:48,160
newsfeed or a search results list where

00:01:45,880 --> 00:01:49,990
there may be many more items that we can

00:01:48,160 --> 00:01:52,120
download at one time then maybe

00:01:49,990 --> 00:01:53,380
algorithmic ranking things like that and

00:01:52,120 --> 00:01:56,080
we want to go to fetch lists

00:01:53,380 --> 00:01:58,420
incrementally and finally a client

00:01:56,080 --> 00:02:00,010
should allow us to execute mutations and

00:01:58,420 --> 00:02:01,870
the mutation system should integrate

00:02:00,010 --> 00:02:04,120
with our caching and consistency

00:02:01,870 --> 00:02:06,940
approaches and also with pagination as

00:02:04,120 --> 00:02:08,530
well so let's start at the beginning and

00:02:06,940 --> 00:02:12,370
kind of add things incrementally as we

00:02:08,530 --> 00:02:14,020
go so before we can even send a query to

00:02:12,370 --> 00:02:15,790
the server we have to actually define

00:02:14,020 --> 00:02:17,120
the queries somewhere and we've seen

00:02:15,790 --> 00:02:19,040
some approaches in earlier

00:02:17,120 --> 00:02:22,430
thoughts of using code to generate

00:02:19,040 --> 00:02:23,659
queries and things like that so the

00:02:22,430 --> 00:02:25,069
pattern that we've converged on at

00:02:23,659 --> 00:02:28,700
Facebook is what we refer to as

00:02:25,069 --> 00:02:32,180
colocation again we've heard a lot about

00:02:28,700 --> 00:02:34,640
user interface components today and at

00:02:32,180 --> 00:02:38,269
Facebook where obviously we've developed

00:02:34,640 --> 00:02:41,120
a react component kit for iOS and of

00:02:38,269 --> 00:02:42,950
course we can use Android views as well

00:02:41,120 --> 00:02:45,650
and so the pattern that we've developed

00:02:42,950 --> 00:02:47,720
is to either have a dot graph QL file

00:02:45,650 --> 00:02:50,420
that contains fragments for each

00:02:47,720 --> 00:02:53,599
component or have the graphical embedded

00:02:50,420 --> 00:02:55,670
in the code in in our JavaScript client

00:02:53,599 --> 00:02:58,459
relay we actually embed the graphical

00:02:55,670 --> 00:03:01,040
directly into the JavaScript in other

00:02:58,459 --> 00:03:03,170
case this makes it a lot easier to keep

00:03:01,040 --> 00:03:04,609
the data fetching and the UI in sync

00:03:03,170 --> 00:03:06,260
again we've kind of repeating something

00:03:04,609 --> 00:03:08,269
super today so I'll move to this a bit

00:03:06,260 --> 00:03:11,000
quickly but in general this allows us to

00:03:08,269 --> 00:03:12,349
if we added property to the to the you

00:03:11,000 --> 00:03:14,569
know to the user interface if we're

00:03:12,349 --> 00:03:16,489
adding some a name that wasn't there

00:03:14,569 --> 00:03:19,519
before we can then jump directly to the

00:03:16,489 --> 00:03:21,889
fragment add that field and keep the did

00:03:19,519 --> 00:03:23,720
the component in sync with its data

00:03:21,889 --> 00:03:25,400
dependencies this also means that if we

00:03:23,720 --> 00:03:27,410
reuse a component somewhere else in our

00:03:25,400 --> 00:03:28,549
application we can simply reference its

00:03:27,410 --> 00:03:31,599
fragments and make sure that we've

00:03:28,549 --> 00:03:34,760
fetched all the data to render that UI

00:03:31,599 --> 00:03:36,470
by by kind of avoiding it's really

00:03:34,760 --> 00:03:39,349
tempting I think to create shared

00:03:36,470 --> 00:03:40,879
fragments and have you know some user

00:03:39,349 --> 00:03:42,739
fragment that we reference in multiple

00:03:40,879 --> 00:03:44,540
places in our application but this means

00:03:42,739 --> 00:03:46,699
that if we now have we might have some

00:03:44,540 --> 00:03:48,319
field that exists in that fragment and

00:03:46,699 --> 00:03:50,299
now we have to go and check everywhere

00:03:48,319 --> 00:03:52,099
that uses the data from that fragment to

00:03:50,299 --> 00:03:54,829
see can we actually delete this field

00:03:52,099 --> 00:03:56,959
and so by having shared fragments it

00:03:54,829 --> 00:03:58,730
kind of defeats some of the benefits of

00:03:56,959 --> 00:04:01,400
graph QL and that we don't get this

00:03:58,730 --> 00:04:03,019
clear association of fetching exactly

00:04:01,400 --> 00:04:07,370
what we need for a particular piece of

00:04:03,019 --> 00:04:09,410
code okay so colocation has worked out

00:04:07,370 --> 00:04:11,599
pretty well for us now let's look at a

00:04:09,410 --> 00:04:14,989
query construction again we saw some

00:04:11,599 --> 00:04:17,090
photos to this before so I kind of first

00:04:14,989 --> 00:04:19,190
pass at doing query construction would

00:04:17,090 --> 00:04:22,430
be something like what we do in the

00:04:19,190 --> 00:04:24,740
current version of relay the user app so

00:04:22,430 --> 00:04:27,409
this is at Build time we can look at the

00:04:24,740 --> 00:04:28,920
query string on the Left again a quote

00:04:27,409 --> 00:04:31,440
quote marks around it so

00:04:28,920 --> 00:04:34,410
besides that that this is you know text

00:04:31,440 --> 00:04:36,120
string and at build time we can just you

00:04:34,410 --> 00:04:38,430
know do it an initial parse of this and

00:04:36,120 --> 00:04:42,450
create a simple AST representing the

00:04:38,430 --> 00:04:45,960
query this is how relay works today

00:04:42,450 --> 00:04:47,430
so this works and the object

00:04:45,960 --> 00:04:49,200
representation allows us to describe

00:04:47,430 --> 00:04:51,360
things like when we have a fragment

00:04:49,200 --> 00:04:53,700
reference so the dot dot dot photo we

00:04:51,360 --> 00:04:56,190
can generate code it's actually look up

00:04:53,700 --> 00:04:58,310
the the actual code representation of

00:04:56,190 --> 00:05:01,680
the photo fragment at runtime

00:04:58,310 --> 00:05:04,800
now at runtime we can take this aste

00:05:01,680 --> 00:05:07,200
execute it embed fragments into their

00:05:04,800 --> 00:05:09,180
parents get one giant representation of

00:05:07,200 --> 00:05:10,980
the entire query print that to a string

00:05:09,180 --> 00:05:12,240
and send it to the server this is very

00:05:10,980 --> 00:05:16,170
much like the approach that we saw

00:05:12,240 --> 00:05:18,780
earlier from Shopify so this this works

00:05:16,170 --> 00:05:21,840
fairly well for smaller applications but

00:05:18,780 --> 00:05:23,430
it doesn't necessarily scale so as our

00:05:21,840 --> 00:05:25,530
product cruising in complexity we're

00:05:23,430 --> 00:05:27,270
gonna add more UI components we're gonna

00:05:25,530 --> 00:05:28,890
add more different types so we might

00:05:27,270 --> 00:05:31,170
have had a story type before

00:05:28,890 --> 00:05:33,150
but now we might have a photo story in a

00:05:31,170 --> 00:05:35,430
video story and all these different

00:05:33,150 --> 00:05:37,170
variations and so our queries grow

00:05:35,430 --> 00:05:38,700
larger even as the amount of data we're

00:05:37,170 --> 00:05:40,410
fetching doesn't actually grow as much

00:05:38,700 --> 00:05:43,650
because we have unions and more

00:05:40,410 --> 00:05:45,150
complexity and more fragments so over

00:05:43,650 --> 00:05:49,440
time as you can see from this completely

00:05:45,150 --> 00:05:51,930
non-scientific graph our query size will

00:05:49,440 --> 00:05:54,000
drill right and the thing is this isn't

00:05:51,930 --> 00:05:56,220
just the the query size the crew size

00:05:54,000 --> 00:05:58,470
actually has an impact it means that

00:05:56,220 --> 00:05:59,790
we're spending more time at runtime just

00:05:58,470 --> 00:06:01,860
simply generating the query

00:05:59,790 --> 00:06:04,110
representation and converting that back

00:06:01,860 --> 00:06:06,870
into a string it means we're taking time

00:06:04,110 --> 00:06:09,210
every single user of our application has

00:06:06,870 --> 00:06:11,520
to wait while their phone is doing all

00:06:09,210 --> 00:06:13,050
that work uploading all those bytes to

00:06:11,520 --> 00:06:14,310
the server and every time they run the

00:06:13,050 --> 00:06:16,470
app they're uploading the same bytes

00:06:14,310 --> 00:06:18,450
over and over again and that means that

00:06:16,470 --> 00:06:19,710
the time spent uploading is time that

00:06:18,450 --> 00:06:21,150
they're not spent downloading the

00:06:19,710 --> 00:06:24,780
response and actually using the

00:06:21,150 --> 00:06:28,110
application so again this isn't

00:06:24,780 --> 00:06:29,940
necessarily a problem at first but it

00:06:28,110 --> 00:06:33,210
can become a problem or it may become a

00:06:29,940 --> 00:06:34,350
problem over time so one option that you

00:06:33,210 --> 00:06:35,940
might think of it's okay well it's just

00:06:34,350 --> 00:06:39,120
like stop doing colocation we'll write

00:06:35,940 --> 00:06:40,380
super minimal optimized queries you

00:06:39,120 --> 00:06:42,060
don't have to do that there's an

00:06:40,380 --> 00:06:42,500
alternative that that was kind of

00:06:42,060 --> 00:06:44,270
thorough

00:06:42,500 --> 00:06:45,650
the benefits of colocation and the

00:06:44,270 --> 00:06:48,790
alternative that we've arrived at is

00:06:45,650 --> 00:06:52,220
what we refer to as persistent queries

00:06:48,790 --> 00:06:54,200
so going back to that build time step we

00:06:52,220 --> 00:06:55,730
already have a clear structure for where

00:06:54,200 --> 00:06:56,930
to define the queries so we know that we

00:06:55,730 --> 00:06:58,760
know where to look for them

00:06:56,930 --> 00:07:01,850
they're in either doc graph QL files or

00:06:58,760 --> 00:07:03,500
they're embedded inside the code and we

00:07:01,850 --> 00:07:07,100
already have a mechanism to actually

00:07:03,500 --> 00:07:09,590
convert from these disparate co-located

00:07:07,100 --> 00:07:12,140
fragments back into an ast and convert

00:07:09,590 --> 00:07:13,610
that ast into query text we're doing it

00:07:12,140 --> 00:07:16,280
we're all doing that exact work at

00:07:13,610 --> 00:07:18,710
runtime the key insight is that we don't

00:07:16,280 --> 00:07:20,419
actually have anything happening at run

00:07:18,710 --> 00:07:22,220
time that write work that is we're not

00:07:20,419 --> 00:07:24,020
dependent upon a runtime to know what

00:07:22,220 --> 00:07:25,880
the query is these are all just static

00:07:24,020 --> 00:07:27,530
pieces of text around our code base that

00:07:25,880 --> 00:07:30,050
are just getting put together at runtime

00:07:27,530 --> 00:07:33,400
and sent to the server so we can do that

00:07:30,050 --> 00:07:35,660
query you know aggregation at build time

00:07:33,400 --> 00:07:37,610
so we can take all those different

00:07:35,660 --> 00:07:39,700
fragments put them together send it as

00:07:37,610 --> 00:07:42,710
text to the server again in a build step

00:07:39,700 --> 00:07:45,140
get save it in database assign it an ID

00:07:42,710 --> 00:07:47,690
we can hash the query text to get the

00:07:45,140 --> 00:07:49,160
consistent ID and return that back to

00:07:47,690 --> 00:07:52,220
the build step and then save that ID

00:07:49,160 --> 00:07:54,500
somewhere now at runtime instead of

00:07:52,220 --> 00:07:56,540
sending up this massive query string we

00:07:54,500 --> 00:07:58,970
can send the ID and just the variables

00:07:56,540 --> 00:08:00,440
so if for example you wanted you wanted

00:07:58,970 --> 00:08:02,479
to have some conditional logic in

00:08:00,440 --> 00:08:04,190
certain cases we'll fetch this fragment

00:08:02,479 --> 00:08:05,930
other cases we'll fetch this fragment we

00:08:04,190 --> 00:08:07,729
can just use an add include or at skip

00:08:05,930 --> 00:08:09,350
directive and then send a variable at

00:08:07,729 --> 00:08:11,770
runtime to choose which one we fetch and

00:08:09,350 --> 00:08:14,240
then we can get the data back and render

00:08:11,770 --> 00:08:16,370
so now returning to our completely on

00:08:14,240 --> 00:08:19,310
completely unscientific graph we can see

00:08:16,370 --> 00:08:21,050
that persistent IDs don't get larger

00:08:19,310 --> 00:08:22,130
over time the main thing that changes

00:08:21,050 --> 00:08:24,229
here is we're probably gonna add a bunch

00:08:22,130 --> 00:08:26,510
of variables and so the amount of data

00:08:24,229 --> 00:08:30,950
we send in the variables will grow but

00:08:26,510 --> 00:08:33,200
this is you know much more scalable so

00:08:30,950 --> 00:08:35,900
persistent ID persist queries have been

00:08:33,200 --> 00:08:38,060
working well for us effectively all the

00:08:35,900 --> 00:08:40,159
queries in our iOS and Android clients

00:08:38,060 --> 00:08:42,710
are persistent IDs I say effectively

00:08:40,159 --> 00:08:44,540
because I admit the possibility that

00:08:42,710 --> 00:08:48,589
there's a text being sent somewhere but

00:08:44,540 --> 00:08:50,600
I think in practice yeah yeah so and

00:08:48,589 --> 00:08:51,740
then in relay we are not using the

00:08:50,600 --> 00:08:53,540
current version really doesn't support

00:08:51,740 --> 00:08:54,380
for social queries we're moving to

00:08:53,540 --> 00:08:58,370
to supporting them in our JavaScript

00:08:54,380 --> 00:09:00,800
client as well some ideas if you use

00:08:58,370 --> 00:09:02,120
persist queries are it can be helpful

00:09:00,800 --> 00:09:04,339
when you're debugging a production issue

00:09:02,120 --> 00:09:05,959
you probably have the idea that the

00:09:04,339 --> 00:09:07,579
client sent and so it's helpful to have

00:09:05,959 --> 00:09:09,350
a quick you know an internal tool you

00:09:07,579 --> 00:09:12,290
can go to to quickly get the query text

00:09:09,350 --> 00:09:14,060
for that for that ID you may also want

00:09:12,290 --> 00:09:17,420
to be able to have put in the query name

00:09:14,060 --> 00:09:19,130
and get back the like the list of ID's

00:09:17,420 --> 00:09:21,440
that have been assigned to that query so

00:09:19,130 --> 00:09:24,740
I can see all the versions of the feed

00:09:21,440 --> 00:09:26,120
query over time and and you know what is

00:09:24,740 --> 00:09:28,850
the current ID that's about to be sent

00:09:26,120 --> 00:09:30,529
to master another thing you might want

00:09:28,850 --> 00:09:32,420
to do is to actually log the performance

00:09:30,529 --> 00:09:34,310
so we saw some some tools that allow you

00:09:32,420 --> 00:09:37,279
to log performance characteristics and

00:09:34,310 --> 00:09:39,319
berquist persistent IDs kind of give you

00:09:37,279 --> 00:09:41,449
a really great way to do that so you can

00:09:39,319 --> 00:09:43,160
for example see okay let's look at the

00:09:41,449 --> 00:09:46,009
performance of the feed query over time

00:09:43,160 --> 00:09:48,800
and see how each different ID worked and

00:09:46,009 --> 00:09:51,459
you know how how many times that ID is

00:09:48,800 --> 00:09:54,529
actually being used by clients

00:09:51,459 --> 00:09:56,509
so so far persistent queries give us a

00:09:54,529 --> 00:09:57,769
way to efficiently fetch act like a new

00:09:56,509 --> 00:10:01,339
query that we haven't already fetched

00:09:57,769 --> 00:10:02,540
the for but as we can all imagine you

00:10:01,339 --> 00:10:05,089
know it's we're probably gonna want

00:10:02,540 --> 00:10:07,819
caching all right we open a screen on

00:10:05,089 --> 00:10:09,620
our app we leave that screen a few

00:10:07,819 --> 00:10:11,149
minutes later we come back and now we're

00:10:09,620 --> 00:10:13,130
stuck downloading all the data for that

00:10:11,149 --> 00:10:14,510
screen all over again ideally this would

00:10:13,130 --> 00:10:18,589
be fast and we can use the data that we

00:10:14,510 --> 00:10:20,480
fetched a few minutes before so again

00:10:18,589 --> 00:10:22,250
this is a really specific use case and

00:10:20,480 --> 00:10:24,079
this kind of goes to our development

00:10:22,250 --> 00:10:27,170
philosophy for our clients wishes to try

00:10:24,079 --> 00:10:28,730
to solve specific use cases first see

00:10:27,170 --> 00:10:30,709
whether that works and if it if it does

00:10:28,730 --> 00:10:33,440
great and if there if it has problems

00:10:30,709 --> 00:10:35,899
then we solved the next problem so just

00:10:33,440 --> 00:10:38,060
like with rest or other other HTTP

00:10:35,899 --> 00:10:42,230
protocols we can just simply add a

00:10:38,060 --> 00:10:44,089
response cache so here we'd have a kind

00:10:42,230 --> 00:10:46,699
of read write through cache where we

00:10:44,089 --> 00:10:49,459
have a mapping of IDs and variables we

00:10:46,699 --> 00:10:51,500
kind of do a stable hashing of those to

00:10:49,459 --> 00:10:53,029
get it to get a key if that key is

00:10:51,500 --> 00:10:54,709
already in the cache we just use the

00:10:53,029 --> 00:10:56,300
cached value otherwise we'd go to the

00:10:54,709 --> 00:10:59,079
server get the data throw it into the

00:10:56,300 --> 00:11:02,029
cache and then return it to the UI and

00:10:59,079 --> 00:11:03,709
standard things apply here like TTL so

00:11:02,029 --> 00:11:06,300
some form of expirations that we don't

00:11:03,709 --> 00:11:08,309
keep cached it around for too long

00:11:06,300 --> 00:11:10,290
and all the things about you know LRU or

00:11:08,309 --> 00:11:11,850
other eviction algorithms I'm putting a

00:11:10,290 --> 00:11:13,470
maximum calf size one other thing you

00:11:11,850 --> 00:11:15,779
can do is to persist this cache to disk

00:11:13,470 --> 00:11:18,449
so that if the user is offline they can

00:11:15,779 --> 00:11:20,850
still actually maybe get the the initial

00:11:18,449 --> 00:11:23,699
screens worth of data for example that's

00:11:20,850 --> 00:11:29,220
kind of a easy and cheap way to get some

00:11:23,699 --> 00:11:31,110
amount of offline functionality ok so a

00:11:29,220 --> 00:11:34,110
response cache is really useful but it

00:11:31,110 --> 00:11:35,879
introduces its own problems how do we

00:11:34,110 --> 00:11:38,309
ensure that data is actually correct and

00:11:35,879 --> 00:11:41,999
how do we ensure that it's consistent so

00:11:38,309 --> 00:11:44,040
let's look at an example imagine that we

00:11:41,999 --> 00:11:46,139
fetch a query that includes the message

00:11:44,040 --> 00:11:47,850
counts so if you're familiar with flux

00:11:46,139 --> 00:11:50,999
at all you'll recognize this at this

00:11:47,850 --> 00:11:52,649
example but so we fetched the message

00:11:50,999 --> 00:11:54,869
count that's at a certain point and we

00:11:52,649 --> 00:11:57,420
cache the value for this and the message

00:11:54,869 --> 00:11:59,670
count is 1 at some other point a bit

00:11:57,420 --> 00:12:01,139
later on we fetch a different view that

00:11:59,670 --> 00:12:03,089
also happens to include the message

00:12:01,139 --> 00:12:05,519
count and at that point the message

00:12:03,089 --> 00:12:07,769
count is 2 if we cache these results

00:12:05,519 --> 00:12:10,649
independently the two caches will now

00:12:07,769 --> 00:12:12,149
have different values and if we were to

00:12:10,649 --> 00:12:13,799
go back to that previous screen would

00:12:12,149 --> 00:12:15,119
use the cash value and we'd see message

00:12:13,799 --> 00:12:16,259
count 1 and 1 a part of our app in

00:12:15,119 --> 00:12:18,059
message count 2 in a different part of

00:12:16,259 --> 00:12:21,480
our app and that's not necessarily great

00:12:18,059 --> 00:12:23,850
user experience so without some form of

00:12:21,480 --> 00:12:25,740
coordination these two cached values

00:12:23,850 --> 00:12:28,559
will get out of sync so we'll need

00:12:25,740 --> 00:12:31,529
something to coordinate these there's a

00:12:28,559 --> 00:12:33,420
lot of different approaches to this kind

00:12:31,529 --> 00:12:35,279
of more decentralized approaches more

00:12:33,420 --> 00:12:37,589
normalized the approach that we're kind

00:12:35,279 --> 00:12:40,649
of converging on in our clients in

00:12:37,589 --> 00:12:42,600
particular on our in in relay and in our

00:12:40,649 --> 00:12:44,790
iOS client today and kind of moving

00:12:42,600 --> 00:12:48,059
towards I think on Android is to have a

00:12:44,790 --> 00:12:50,429
more normalized store and that sits kind

00:12:48,059 --> 00:12:53,610
of at the center of the the client side

00:12:50,429 --> 00:12:56,519
of cache so every query that we fetch is

00:12:53,610 --> 00:12:58,889
published into the store and then views

00:12:56,519 --> 00:13:01,220
subscribe to a query and pull the latest

00:12:58,889 --> 00:13:04,110
values of that query from at the store

00:13:01,220 --> 00:13:06,959
so in the case we saw before when we

00:13:04,110 --> 00:13:09,629
fetched a subsequent query not only do

00:13:06,959 --> 00:13:11,549
we see the new count on the on the new

00:13:09,629 --> 00:13:14,339
view but because that data is published

00:13:11,549 --> 00:13:16,049
to the store which then gets notified

00:13:14,339 --> 00:13:17,970
and published to the previous screen

00:13:16,049 --> 00:13:19,400
they now both get the correct message

00:13:17,970 --> 00:13:22,170
count

00:13:19,400 --> 00:13:24,720
the format of the store is basically a

00:13:22,170 --> 00:13:27,960
flat mapping of wreck of kind of

00:13:24,720 --> 00:13:30,390
identifier to records where a record is

00:13:27,960 --> 00:13:35,310
not like another mapping of basically

00:13:30,390 --> 00:13:36,600
field name to value and so here for

00:13:35,310 --> 00:13:39,290
example you can see that we've got a

00:13:36,600 --> 00:13:41,600
store that has two entries in it my ID

00:13:39,290 --> 00:13:44,460
where where the identifiers might be

00:13:41,600 --> 00:13:46,140
might the type user and then my actual

00:13:44,460 --> 00:13:48,090
ID kind of put together as the

00:13:46,140 --> 00:13:50,700
identifier we have a record we have the

00:13:48,090 --> 00:13:52,830
type name we have the ID field the name

00:13:50,700 --> 00:13:53,490
and then you can see that for city where

00:13:52,830 --> 00:13:55,620
I live

00:13:53,490 --> 00:13:57,990
we're not actually encoding the record

00:13:55,620 --> 00:14:00,690
directly we have a reference to this

00:13:57,990 --> 00:14:02,460
object by its ID and so this allows us

00:14:00,690 --> 00:14:04,740
to have a flat mapping where we can take

00:14:02,460 --> 00:14:06,750
in a new set we can give basically take

00:14:04,740 --> 00:14:08,580
a new store in an old store kind of walk

00:14:06,750 --> 00:14:12,990
them together and just merge the two

00:14:08,580 --> 00:14:14,760
together so the main kind of operations

00:14:12,990 --> 00:14:16,140
we have on the store are published where

00:14:14,760 --> 00:14:18,060
we have some new data we've gotten from

00:14:16,140 --> 00:14:20,010
the server and we have existing data in

00:14:18,060 --> 00:14:22,170
the store we take those two mappings

00:14:20,010 --> 00:14:24,090
walk them together add any new entries

00:14:22,170 --> 00:14:26,040
from the server and just simply add them

00:14:24,090 --> 00:14:29,190
if that if the record exists in both we

00:14:26,040 --> 00:14:31,470
merge them together and as we do that we

00:14:29,190 --> 00:14:34,950
can record which IDs have actually

00:14:31,470 --> 00:14:37,860
changed the other side of this is to

00:14:34,950 --> 00:14:39,780
subscribe and notify so a UI subscribes

00:14:37,860 --> 00:14:43,200
to the store with a query or a fragment

00:14:39,780 --> 00:14:45,360
and when new data is published we look

00:14:43,200 --> 00:14:47,520
at the the results what changed in the

00:14:45,360 --> 00:14:50,250
store so we know that you know ID a was

00:14:47,520 --> 00:14:52,380
added ID'd the changed ID c-- got

00:14:50,250 --> 00:14:53,940
deleted and we can look at the

00:14:52,380 --> 00:14:56,310
subscriptions and see which

00:14:53,940 --> 00:14:58,040
subscriptions would be affected and any

00:14:56,310 --> 00:15:00,420
subscriptions that are we basically

00:14:58,040 --> 00:15:03,150
execute the query and give the new

00:15:00,420 --> 00:15:05,220
results to the back to the UI so

00:15:03,150 --> 00:15:06,650
components only kind of get the the

00:15:05,220 --> 00:15:09,870
actual records that they need and we

00:15:06,650 --> 00:15:11,700
avoid unnecessarily updating UI so if

00:15:09,870 --> 00:15:13,589
your query isn't affected by a result

00:15:11,700 --> 00:15:15,230
you don't get notified and you don't

00:15:13,589 --> 00:15:17,940
have to re-render

00:15:15,230 --> 00:15:20,250
so there are some come trade-offs to

00:15:17,940 --> 00:15:21,630
consider here it's not it isn't quite as

00:15:20,250 --> 00:15:25,740
simple as just making everything

00:15:21,630 --> 00:15:29,190
automatically consistent for example we

00:15:25,740 --> 00:15:30,650
may want to delay the publish so imagine

00:15:29,190 --> 00:15:32,930
that we're on a mobile phone and we

00:15:30,650 --> 00:15:35,690
navigate to a new screen the user can

00:15:32,930 --> 00:15:37,460
only see the new screen anyway so we

00:15:35,690 --> 00:15:39,830
might want to prioritize rendering that

00:15:37,460 --> 00:15:41,360
new screen before we update any other

00:15:39,830 --> 00:15:44,420
screens that the user can't see right

00:15:41,360 --> 00:15:45,310
now and so we're kind of having some

00:15:44,420 --> 00:15:47,510
amount of eventual consistency

00:15:45,310 --> 00:15:49,820
technically speaking that screen is is

00:15:47,510 --> 00:15:54,260
not consistent but we can't see it so

00:15:49,820 --> 00:15:55,550
that's okay also there may be certain

00:15:54,260 --> 00:15:59,270
specific cases where we actually want to

00:15:55,550 --> 00:16:00,320
want to opt out of consistency it could

00:15:59,270 --> 00:16:01,520
just happen on your product where you

00:16:00,320 --> 00:16:03,530
have a specific case like maybe the

00:16:01,520 --> 00:16:05,720
users interacted with some data already

00:16:03,530 --> 00:16:06,860
and if I've already modified one field I

00:16:05,720 --> 00:16:09,380
don't want the other fields in the form

00:16:06,860 --> 00:16:11,180
to change you know certain edge cases

00:16:09,380 --> 00:16:12,530
like this can happen and we can usually

00:16:11,180 --> 00:16:14,300
handle them by just letting the store

00:16:12,530 --> 00:16:16,250
notify you and there's ignore the new

00:16:14,300 --> 00:16:18,290
notifications that you get but it's

00:16:16,250 --> 00:16:20,690
something to consider and then finally

00:16:18,290 --> 00:16:22,520
lists are fetched incrementally we

00:16:20,690 --> 00:16:24,500
fetched a first part of a list a second

00:16:22,520 --> 00:16:26,390
part of a list and it isn't quite as

00:16:24,500 --> 00:16:29,510
straightforward to automatically make

00:16:26,390 --> 00:16:31,400
lists consistent and moreover if I have

00:16:29,510 --> 00:16:33,440
if I scroll down in the comments for a

00:16:31,400 --> 00:16:35,540
story over here in the UI I probably

00:16:33,440 --> 00:16:36,980
don't expect comments and some other

00:16:35,540 --> 00:16:39,620
part of the screen to suddenly start

00:16:36,980 --> 00:16:41,510
scrolling as well we kind of expect

00:16:39,620 --> 00:16:42,800
users kind of expect that like different

00:16:41,510 --> 00:16:45,680
instances of a list are actually

00:16:42,800 --> 00:16:48,380
distinct so we'll need a solution for

00:16:45,680 --> 00:16:50,090
that as well so let's briefly briefly

00:16:48,380 --> 00:16:52,310
returned to caching so far we looked at

00:16:50,090 --> 00:16:53,810
just simply going to a screen throwing

00:16:52,310 --> 00:16:55,670
away and coming back to that exact same

00:16:53,810 --> 00:16:59,030
screen but there's another case that can

00:16:55,670 --> 00:17:02,210
be pretty common so it's and that's

00:16:59,030 --> 00:17:04,100
where we have a list view and then we

00:17:02,210 --> 00:17:05,960
click into a detail view this is very

00:17:04,100 --> 00:17:08,480
very common in applications so for

00:17:05,960 --> 00:17:09,770
example the looking at my newsfeed and

00:17:08,480 --> 00:17:11,780
if I we have a lot of information

00:17:09,770 --> 00:17:13,130
already on the client about this story

00:17:11,780 --> 00:17:16,010
that I wrote about the graph Gil summit

00:17:13,130 --> 00:17:17,810
if I click on it I'm gonna see a loading

00:17:16,010 --> 00:17:21,680
spinner because our response cache

00:17:17,810 --> 00:17:23,150
doesn't have this query yet so but

00:17:21,680 --> 00:17:25,010
ideally we've actually have a lot of

00:17:23,150 --> 00:17:26,630
data on the client we could use that to

00:17:25,010 --> 00:17:28,730
more efficiently or you know to more

00:17:26,630 --> 00:17:30,590
quickly render this screen and maybe

00:17:28,730 --> 00:17:34,490
fetch there's any missing information in

00:17:30,590 --> 00:17:36,860
a separate query now we looked at the

00:17:34,490 --> 00:17:38,570
notify operation where when new

00:17:36,860 --> 00:17:41,000
information is published we kind of tell

00:17:38,570 --> 00:17:43,890
the view what's changed well implicit in

00:17:41,000 --> 00:17:46,200
that is the ability to execute a query

00:17:43,890 --> 00:17:48,120
against the cash so you know we're used

00:17:46,200 --> 00:17:50,100
to basically having queries be executed

00:17:48,120 --> 00:17:51,600
in the server where the resolve function

00:17:50,100 --> 00:17:53,730
is calling you know your product

00:17:51,600 --> 00:17:55,380
specific business logic but in this case

00:17:53,730 --> 00:17:58,230
the resolve function is the very very

00:17:55,380 --> 00:18:00,780
simple lookup in this you know in this

00:17:58,230 --> 00:18:02,790
normalized map and as we go down fields

00:18:00,780 --> 00:18:05,220
we just continue to look up references

00:18:02,790 --> 00:18:08,700
so for the case that we just saw of

00:18:05,220 --> 00:18:10,290
navigating to a details page we can look

00:18:08,700 --> 00:18:12,810
in the store and just traverse the query

00:18:10,290 --> 00:18:14,610
do we have everything for this story so

00:18:12,810 --> 00:18:16,320
we'll start at the top of the query look

00:18:14,610 --> 00:18:18,330
in the cache and load the story ID okay

00:18:16,320 --> 00:18:20,010
it's there great now we can traverse

00:18:18,330 --> 00:18:22,320
down to the author of the comments and

00:18:20,010 --> 00:18:24,300
continue down executing the query to see

00:18:22,320 --> 00:18:26,010
if we have all the records if every

00:18:24,300 --> 00:18:27,420
single record is in the cache great we

00:18:26,010 --> 00:18:29,520
have all the data we need we can render

00:18:27,420 --> 00:18:32,630
if we're missing anything okay now I'll

00:18:29,520 --> 00:18:32,630
have to go to the server and fetch that

00:18:33,470 --> 00:18:39,120
so I mentioned that it's a bit there are

00:18:37,380 --> 00:18:42,840
some kind of tricky aspects to data

00:18:39,120 --> 00:18:44,070
consistency and pagination so there's a

00:18:42,840 --> 00:18:45,810
few things to kind of keep in mind with

00:18:44,070 --> 00:18:48,720
pagination and some use cases that we

00:18:45,810 --> 00:18:52,680
have to to work around first is that

00:18:48,720 --> 00:18:54,630
lists often need to have they've special

00:18:52,680 --> 00:18:56,820
relationships so for example my friends

00:18:54,630 --> 00:18:58,740
list it's not just a list of users but

00:18:56,820 --> 00:19:01,080
it's a list of information about when I

00:18:58,740 --> 00:19:01,920
became friends with people how I became

00:19:01,080 --> 00:19:03,570
friends with them those are all

00:19:01,920 --> 00:19:06,270
properties of the relationship and not

00:19:03,570 --> 00:19:07,740
either person so we don't we want our

00:19:06,270 --> 00:19:11,010
schema to kind of give some space for

00:19:07,740 --> 00:19:12,510
that lists are often ranked by an

00:19:11,010 --> 00:19:15,540
algorithm so for example the Facebook

00:19:12,510 --> 00:19:18,600
newsfeed there item ordering may change

00:19:15,540 --> 00:19:20,910
over time so for example a story that

00:19:18,600 --> 00:19:22,350
you know is relevant to me today might

00:19:20,910 --> 00:19:24,630
not be as relevant tomorrow and

00:19:22,350 --> 00:19:27,720
shouldn't appear as high in the list so

00:19:24,630 --> 00:19:29,730
if I reach the same exact arguments even

00:19:27,720 --> 00:19:31,410
like you know a very short time span I

00:19:29,730 --> 00:19:33,690
might see different different results

00:19:31,410 --> 00:19:35,100
and finally there's almost there's with

00:19:33,690 --> 00:19:37,020
these types of Lists there's too many

00:19:35,100 --> 00:19:38,310
items to download in full to the client

00:19:37,020 --> 00:19:40,170
and so we typically fetch them in

00:19:38,310 --> 00:19:42,780
increments so effectively there's this

00:19:40,170 --> 00:19:44,730
virtual list that exists you know in an

00:19:42,780 --> 00:19:46,770
abstract on the server and that we're

00:19:44,730 --> 00:19:49,200
going to construct a client-side

00:19:46,770 --> 00:19:50,520
representation of but it's not clear

00:19:49,200 --> 00:19:51,720
that we'll ever have exactly a

00:19:50,520 --> 00:19:53,670
one-to-one mapping between this

00:19:51,720 --> 00:19:55,680
theoretical virtual representation on

00:19:53,670 --> 00:19:58,000
the server and what we see in the client

00:19:55,680 --> 00:20:01,330
we're always constant

00:19:58,000 --> 00:20:04,270
the view on the on the client so it's

00:20:01,330 --> 00:20:06,610
like an example of that the way that we

00:20:04,270 --> 00:20:09,220
can represent paginate a list sore will

00:20:06,610 --> 00:20:10,419
be in relay we refer to them or really

00:20:09,220 --> 00:20:13,540
in general at Facebook we refer to them

00:20:10,419 --> 00:20:15,700
as Connexions is as a segment and the

00:20:13,540 --> 00:20:18,309
segment says kind of where it starts in

00:20:15,700 --> 00:20:20,200
this virtual list how many items are

00:20:18,309 --> 00:20:22,120
there and whether there's more going in

00:20:20,200 --> 00:20:24,010
that direction or not and then of course

00:20:22,120 --> 00:20:27,790
the edges themselves so we might have

00:20:24,010 --> 00:20:29,530
gotten items a B and C now let's say we

00:20:27,790 --> 00:20:31,660
want to we scroll down and we want to

00:20:29,530 --> 00:20:32,980
load more items so we'll go to the

00:20:31,660 --> 00:20:36,250
server and say okay well we want more

00:20:32,980 --> 00:20:37,480
things after C and again I'm using after

00:20:36,250 --> 00:20:39,640
here because these are the kind of

00:20:37,480 --> 00:20:41,590
arguments that we that are in the real a

00:20:39,640 --> 00:20:43,860
connection spec and that these are the

00:20:41,590 --> 00:20:46,240
the kind of arguments that we use in

00:20:43,860 --> 00:20:47,470
Facebook for connections you can imagine

00:20:46,240 --> 00:20:51,790
doing a very similar thing with like

00:20:47,470 --> 00:20:54,220
with a limit or offset style approach so

00:20:51,790 --> 00:20:55,450
okay we got our new segment so after C

00:20:54,220 --> 00:20:56,440
we've got more three more times and

00:20:55,450 --> 00:20:58,380
there's there's still more things in

00:20:56,440 --> 00:21:00,190
this direction so we have this

00:20:58,380 --> 00:21:01,600
theoretical week at we have these two

00:21:00,190 --> 00:21:03,549
segments that we've to join the other in

00:21:01,600 --> 00:21:06,640
some way and so the question is how and

00:21:03,549 --> 00:21:08,380
you'll notice that I've thrown in C

00:21:06,640 --> 00:21:10,690
twice in this list because items can get

00:21:08,380 --> 00:21:12,610
Arirang tanned now maybe C is a bit less

00:21:10,690 --> 00:21:14,080
important and moves down in the list in

00:21:12,610 --> 00:21:16,000
between the time that we fetched the

00:21:14,080 --> 00:21:17,740
first set in the second set so this is a

00:21:16,000 --> 00:21:20,020
product specific decision what do we do

00:21:17,740 --> 00:21:21,790
here do we put do we keep do we have

00:21:20,020 --> 00:21:23,590
like C twice do we leave it where it was

00:21:21,790 --> 00:21:24,970
do we move it down this is kind of up to

00:21:23,590 --> 00:21:26,650
you if you've already rendered that

00:21:24,970 --> 00:21:28,570
first of you you might not want to have

00:21:26,650 --> 00:21:29,470
an item in your list suddenly jump to

00:21:28,570 --> 00:21:31,270
somewhere else these are might be

00:21:29,470 --> 00:21:32,410
looking at it right now and since is

00:21:31,270 --> 00:21:34,809
something that you have to decide at the

00:21:32,410 --> 00:21:36,850
product level how to handle there's kind

00:21:34,809 --> 00:21:38,679
of two main things you could do here is

00:21:36,850 --> 00:21:40,960
leave C where it was originally and just

00:21:38,679 --> 00:21:44,440
ignore the second one or move it to the

00:21:40,960 --> 00:21:46,120
to the to the new position and so the

00:21:44,440 --> 00:21:48,970
way that we represent this typically is

00:21:46,120 --> 00:21:51,190
as kind of three levels there's the this

00:21:48,970 --> 00:21:53,830
segment which is a chunk like this that

00:21:51,190 --> 00:21:56,290
has where it where it is what edges are

00:21:53,830 --> 00:21:57,669
there and then there's a sequence of

00:21:56,290 --> 00:21:58,720
segments because we might fetch the

00:21:57,669 --> 00:22:01,419
beginning of a list on the end of the

00:21:58,720 --> 00:22:03,340
list and then finally we have the

00:22:01,419 --> 00:22:05,200
connection controller and this is a

00:22:03,340 --> 00:22:08,200
controller in the typical MVC controller

00:22:05,200 --> 00:22:09,090
sense of the word in that it coordinates

00:22:08,200 --> 00:22:10,950
between

00:22:09,090 --> 00:22:13,080
the fetching data from the server

00:22:10,950 --> 00:22:15,270
putting that data into the store and

00:22:13,080 --> 00:22:17,640
allowing the UI to efficiently render

00:22:15,270 --> 00:22:21,630
the the view that we've conceded that

00:22:17,640 --> 00:22:23,780
we've synthesized on the client the

00:22:21,630 --> 00:22:26,340
controller can do a lot of that work for

00:22:23,780 --> 00:22:29,010
automatically by having a consistent you

00:22:26,340 --> 00:22:31,230
know connection scheme with a consistent

00:22:29,010 --> 00:22:33,450
naming pattern for fields and their

00:22:31,230 --> 00:22:35,100
arguments but we'll also probably want

00:22:33,450 --> 00:22:36,480
some amount of product specific logic

00:22:35,100 --> 00:22:39,990
like we talked about for how we actually

00:22:36,480 --> 00:22:41,190
merge new edges you know do we leave

00:22:39,990 --> 00:22:45,450
them where they are do we reorder them

00:22:41,190 --> 00:22:47,070
etc and for things like react or

00:22:45,450 --> 00:22:50,490
component kit where we have a more

00:22:47,070 --> 00:22:52,710
declarative declarative UI architecture

00:22:50,490 --> 00:22:56,220
we can create a connection component

00:22:52,710 --> 00:22:57,630
that takes a configuration and a simple

00:22:56,220 --> 00:22:59,220
function to basically render each edge

00:22:57,630 --> 00:23:01,230
right it's gonna render a list for us

00:22:59,220 --> 00:23:02,880
and it's just gonna render call this

00:23:01,230 --> 00:23:04,620
render edge function once for every item

00:23:02,880 --> 00:23:06,150
when we fetched a new item from the

00:23:04,620 --> 00:23:07,590
server it will call this function

00:23:06,150 --> 00:23:09,780
appropriately to actually render those

00:23:07,590 --> 00:23:11,490
into the UI and the configuration can

00:23:09,780 --> 00:23:12,930
specify things like what is the query

00:23:11,490 --> 00:23:15,080
that we use to actually get more items

00:23:12,930 --> 00:23:18,000
what are the default variables to use

00:23:15,080 --> 00:23:23,760
the logic for emerging new items into

00:23:18,000 --> 00:23:25,710
the list things like that okay so this

00:23:23,760 --> 00:23:29,280
brings us to the last like major segment

00:23:25,710 --> 00:23:30,630
which is mutations and mutations can be

00:23:29,280 --> 00:23:32,580
really simple it might be as simple as

00:23:30,630 --> 00:23:34,680
toggling a boolean but they can also be

00:23:32,580 --> 00:23:35,970
very complex where we're adding a new

00:23:34,680 --> 00:23:37,770
item that should appear at the top of

00:23:35,970 --> 00:23:39,420
one list but in some other part of the

00:23:37,770 --> 00:23:40,680
UI that list is sorted at a different

00:23:39,420 --> 00:23:42,480
order and so the list should the item

00:23:40,680 --> 00:23:46,680
should appear somewhere else we have to

00:23:42,480 --> 00:23:47,760
integrate with caching etc so then our

00:23:46,680 --> 00:23:49,920
thing with mutations that can be

00:23:47,760 --> 00:23:52,260
complicated is that a given change in

00:23:49,920 --> 00:23:54,210
the input can affect a very large number

00:23:52,260 --> 00:23:57,090
of items in the output for example if

00:23:54,210 --> 00:23:59,520
you and I become friends well you should

00:23:57,090 --> 00:24:01,560
show up in my friends list mine

00:23:59,520 --> 00:24:04,020
theoretically my newsfeed should maybe

00:24:01,560 --> 00:24:05,670
now shows stories that come from you

00:24:04,020 --> 00:24:06,840
because that's you're now held your

00:24:05,670 --> 00:24:10,100
stories are now eligible to show up

00:24:06,840 --> 00:24:12,240
there similarly if I block a user then I

00:24:10,100 --> 00:24:13,410
can't see you anything you know things

00:24:12,240 --> 00:24:15,660
like I have cached about you might

00:24:13,410 --> 00:24:16,980
disappear etc so there can be there can

00:24:15,660 --> 00:24:20,040
be certain types of changes that really

00:24:16,980 --> 00:24:22,840
invalidate a wide part of the graph

00:24:20,040 --> 00:24:25,000
also there's complex business logic and

00:24:22,840 --> 00:24:27,340
privacy that make it sometimes hard to

00:24:25,000 --> 00:24:31,410
even figure out on a client how to even

00:24:27,340 --> 00:24:34,000
emulate the results of a mutation and so

00:24:31,410 --> 00:24:38,140
it's definitely definitely pretty

00:24:34,000 --> 00:24:40,450
complex so in general just mutations are

00:24:38,140 --> 00:24:43,120
yeah a bit a bit harder to work with

00:24:40,450 --> 00:24:46,360
than queries because there's because of

00:24:43,120 --> 00:24:48,640
these all these effects but kind of even

00:24:46,360 --> 00:24:51,040
more simply there's the question of when

00:24:48,640 --> 00:24:52,870
I do mutation what do i refetch how much

00:24:51,040 --> 00:24:54,760
do i refetch I might have I might have

00:24:52,870 --> 00:24:56,500
fetched a large amount of the graph but

00:24:54,760 --> 00:24:58,450
I have to make a choice do I eagerly

00:24:56,500 --> 00:25:01,270
fetch everything that could change so

00:24:58,450 --> 00:25:02,770
here's an example of a mutate a like a

00:25:01,270 --> 00:25:05,190
story like mutation where I'm just

00:25:02,770 --> 00:25:08,230
clicking the like button on Facebook

00:25:05,190 --> 00:25:10,030
what do we read that well there's a

00:25:08,230 --> 00:25:12,280
bunch of things that have the word like

00:25:10,030 --> 00:25:13,630
in them that are fields and that are you

00:25:12,280 --> 00:25:15,850
know obvious candidates for things that

00:25:13,630 --> 00:25:18,520
I might want to research like does do I

00:25:15,850 --> 00:25:19,900
like this story yeah I'm I'm toggling

00:25:18,520 --> 00:25:21,790
the buttons that's probably a good one

00:25:19,900 --> 00:25:23,830
to fetch the light count how many times

00:25:21,790 --> 00:25:26,170
has been liked like sentence kind of

00:25:23,830 --> 00:25:27,820
summarizing who's like this the list of

00:25:26,170 --> 00:25:30,070
people who've liked it and the name for

00:25:27,820 --> 00:25:32,650
each one and possibly many many more

00:25:30,070 --> 00:25:35,530
things but the question is do I actually

00:25:32,650 --> 00:25:38,890
need all of those fields well maybe my

00:25:35,530 --> 00:25:40,330
UI is only showing a blue button and it

00:25:38,890 --> 00:25:41,830
just needs to know whether I've liked

00:25:40,330 --> 00:25:43,960
the story or not and all I really need

00:25:41,830 --> 00:25:45,790
is does viewer like maybe I'm showing a

00:25:43,960 --> 00:25:46,870
UI where I'm showing the names of people

00:25:45,790 --> 00:25:50,020
who've liked it and so I'm gonna need

00:25:46,870 --> 00:25:52,630
I'm gonna need likers name maybe I have

00:25:50,020 --> 00:25:54,760
a really complicated UI that shows many

00:25:52,630 --> 00:25:56,080
many more details about each person

00:25:54,760 --> 00:25:58,000
who's liked it and I need a way more

00:25:56,080 --> 00:25:59,440
information than just the name so this

00:25:58,000 --> 00:26:02,260
really depends upon what you're actually

00:25:59,440 --> 00:26:06,220
rendering so there's kind of two main

00:26:02,260 --> 00:26:07,750
options one is to reuse a fragment that

00:26:06,220 --> 00:26:09,280
you've defined you probably have some

00:26:07,750 --> 00:26:10,450
fragment a couple fragments in your app

00:26:09,280 --> 00:26:12,160
that are like high-level story

00:26:10,450 --> 00:26:13,330
components and the simple thing to do

00:26:12,160 --> 00:26:14,770
would just be to reference those

00:26:13,330 --> 00:26:16,660
fragments and fetch everything you need

00:26:14,770 --> 00:26:19,360
about a story again just to make sure

00:26:16,660 --> 00:26:20,710
it's pretty simple it makes it gets to

00:26:19,360 --> 00:26:22,690
you kind of in sync there's no way

00:26:20,710 --> 00:26:24,520
you'll be or it's much harder to have a

00:26:22,690 --> 00:26:26,080
piece of information be out of sync on

00:26:24,520 --> 00:26:28,810
the other hand there's a lot of things

00:26:26,080 --> 00:26:30,220
about a story that don't change and then

00:26:28,810 --> 00:26:31,929
we're gonna end up retouching them

00:26:30,220 --> 00:26:33,820
some more optimized approach would be

00:26:31,929 --> 00:26:35,799
just explicitly list out for example

00:26:33,820 --> 00:26:38,169
just the one field that we know we care

00:26:35,799 --> 00:26:39,730
about and that could change and so you

00:26:38,169 --> 00:26:42,399
can kind of find a balance between these

00:26:39,730 --> 00:26:43,509
two approaches of on the one hand erring

00:26:42,399 --> 00:26:45,159
on the side of keeping everything

00:26:43,509 --> 00:26:47,289
consistent with some over fetching or

00:26:45,159 --> 00:26:52,450
attempting to be more efficient and

00:26:47,289 --> 00:26:55,240
minimal now this is the only slide that

00:26:52,450 --> 00:26:58,049
is in inverted colors because this is an

00:26:55,240 --> 00:27:00,610
example of something that we tried and

00:26:58,049 --> 00:27:03,399
we've learned doesn't necessarily work

00:27:00,610 --> 00:27:05,289
as well so we just saw how it's kind of

00:27:03,399 --> 00:27:08,379
hard to figure out like exactly what we

00:27:05,289 --> 00:27:10,029
should reach in our mutation and we had

00:27:08,379 --> 00:27:12,940
this thought a couple years ago which

00:27:10,029 --> 00:27:14,620
was what if we could infer the mutation

00:27:12,940 --> 00:27:16,899
given the things we've already fetched

00:27:14,620 --> 00:27:18,490
to make that one more concrete let's

00:27:16,899 --> 00:27:20,590
look at an example so this is how

00:27:18,490 --> 00:27:23,200
mutations work in relay which is where

00:27:20,590 --> 00:27:25,149
we tried this experiment on the Left we

00:27:23,200 --> 00:27:28,870
have the fragments that we fetched about

00:27:25,149 --> 00:27:30,309
a story and so in reality our

00:27:28,870 --> 00:27:31,899
application is actually fetched does

00:27:30,309 --> 00:27:33,850
viewer like and the names of people

00:27:31,899 --> 00:27:35,860
who've liked it on the right we have

00:27:33,850 --> 00:27:38,350
what we call and rely the fat query

00:27:35,860 --> 00:27:40,509
these are all the fields that could

00:27:38,350 --> 00:27:42,789
theoretically change about a story given

00:27:40,509 --> 00:27:44,679
this mutation so it does really like

00:27:42,789 --> 00:27:46,659
like count like Santa's Lakers and I put

00:27:44,679 --> 00:27:48,039
question marks under likers because we

00:27:46,659 --> 00:27:50,259
just know that the likers could change

00:27:48,039 --> 00:27:51,850
but we don't know what properties of

00:27:50,259 --> 00:27:56,649
them the UI would actually care about

00:27:51,850 --> 00:27:58,360
and so we we try this experiment with

00:27:56,649 --> 00:28:00,009
relay where we basically said okay well

00:27:58,360 --> 00:28:02,070
we'll take these will record these

00:28:00,009 --> 00:28:04,240
queries over time as the application is

00:28:02,070 --> 00:28:07,000
fetching information from the server and

00:28:04,240 --> 00:28:08,919
we'll let the user to define this

00:28:07,000 --> 00:28:11,590
mutation so this is user defined and

00:28:08,919 --> 00:28:13,269
this is stored by the system and we'll

00:28:11,590 --> 00:28:14,769
base new intersect them and so we'll see

00:28:13,269 --> 00:28:16,899
that does view like appears in both okay

00:28:14,769 --> 00:28:18,309
we'll keep that we see that light count

00:28:16,899 --> 00:28:20,350
and like sentence aren't used by the

00:28:18,309 --> 00:28:22,330
application so we'll skip those queries

00:28:20,350 --> 00:28:23,860
and finally we'll see that the

00:28:22,330 --> 00:28:25,450
application is cleared just like ours

00:28:23,860 --> 00:28:28,539
name and so will populate the likers

00:28:25,450 --> 00:28:29,740
field with just the name so this seems

00:28:28,539 --> 00:28:31,539
really convenient because I can just

00:28:29,740 --> 00:28:32,860
define everything that could change then

00:28:31,539 --> 00:28:35,399
the application is going to construct

00:28:32,860 --> 00:28:37,929
the super minimal efficient query for me

00:28:35,399 --> 00:28:38,980
but we've been here before right we

00:28:37,929 --> 00:28:42,640
talked about this at the beginning of

00:28:38,980 --> 00:28:44,500
the talk query construction time goes up

00:28:42,640 --> 00:28:46,360
system complexity we have to keep all

00:28:44,500 --> 00:28:50,050
these track crews around that that takes

00:28:46,360 --> 00:28:52,600
memory that takes logic on in our actual

00:28:50,050 --> 00:28:54,910
client and of course it means we can't

00:28:52,600 --> 00:28:59,130
do persistent queries or we're stuck in

00:28:54,910 --> 00:29:01,540
sending query text up to the server so

00:28:59,130 --> 00:29:03,370
what we kind of found is that this

00:29:01,540 --> 00:29:04,870
really seemed like a developer

00:29:03,370 --> 00:29:06,190
experience win because it lets the

00:29:04,870 --> 00:29:07,960
product developer not have to think so

00:29:06,190 --> 00:29:10,060
much about the exact right fields to

00:29:07,960 --> 00:29:13,630
fetch but it is it isn't really super

00:29:10,060 --> 00:29:16,000
predictable we we look at our fat query

00:29:13,630 --> 00:29:17,290
and we say oh these things will probably

00:29:16,000 --> 00:29:19,390
be fetched but you don't actually know

00:29:17,290 --> 00:29:22,570
at any given point in time which fields

00:29:19,390 --> 00:29:23,950
are being fetched it introduces client

00:29:22,570 --> 00:29:25,930
complexity it just makes the client have

00:29:23,950 --> 00:29:27,130
to do a lot more things it has to do

00:29:25,930 --> 00:29:29,080
query construction at runtime

00:29:27,130 --> 00:29:31,420
that's the keep track of those queries

00:29:29,080 --> 00:29:34,450
that's to construct the valid mutation

00:29:31,420 --> 00:29:36,490
queries which can be you'd think would

00:29:34,450 --> 00:29:37,810
be easy but it's actually not as there's

00:29:36,490 --> 00:29:39,640
kind of some edge cases that we've run

00:29:37,810 --> 00:29:41,920
into it means we're doing the runtime

00:29:39,640 --> 00:29:43,360
query construction and it means we can't

00:29:41,920 --> 00:29:48,730
do persist queries which we've found to

00:29:43,360 --> 00:29:50,950
be helpful in in reducing query time so

00:29:48,730 --> 00:29:52,030
this is an example of something that we

00:29:50,950 --> 00:29:54,520
really thought would be a great

00:29:52,030 --> 00:29:56,290
experience developer experience when you

00:29:54,520 --> 00:29:59,320
know it's really seemed intuitively like

00:29:56,290 --> 00:30:00,940
a good direction and in retrospect like

00:29:59,320 --> 00:30:02,800
this is something that didn't work out

00:30:00,940 --> 00:30:04,870
as well as we as well as we'd hoped and

00:30:02,800 --> 00:30:06,700
for for really going forward we're

00:30:04,870 --> 00:30:08,470
moving back toward static mutations

00:30:06,700 --> 00:30:10,030
which is what we're doing on our and

00:30:08,470 --> 00:30:13,330
have been doing on our iOS and Android

00:30:10,030 --> 00:30:15,460
clients the entire time so I bring this

00:30:13,330 --> 00:30:17,950
up as kind of an example of how some I

00:30:15,460 --> 00:30:20,590
think intuition with graph QL clients

00:30:17,950 --> 00:30:22,480
can be tricky we have it's very easy to

00:30:20,590 --> 00:30:25,000
imagine like a theoretical ideal of how

00:30:22,480 --> 00:30:26,770
a client should work but in practice

00:30:25,000 --> 00:30:28,870
it's much it's bets better to kind of

00:30:26,770 --> 00:30:31,180
focus on a specific use case and just

00:30:28,870 --> 00:30:32,830
solve that one at a time and I think

00:30:31,180 --> 00:30:35,110
that this is one of those ones we're a

00:30:32,830 --> 00:30:36,790
bit optimistic about oh we can make this

00:30:35,110 --> 00:30:38,290
thing great and we have like a really

00:30:36,790 --> 00:30:42,130
good idea it just didn't quite work out

00:30:38,290 --> 00:30:43,990
as well as we hoped so to recap we've

00:30:42,130 --> 00:30:46,050
tried a lot of ideas over four years

00:30:43,990 --> 00:30:47,980
with graphical clients some worked

00:30:46,050 --> 00:30:48,940
pursue queries is a great example of

00:30:47,980 --> 00:30:52,599
something that is just

00:30:48,940 --> 00:30:54,309
been fairly successful and we've also

00:30:52,599 --> 00:30:55,389
had a few you know missteps things that

00:30:54,309 --> 00:30:59,440
didn't work out as well as we hoped by

00:30:55,389 --> 00:31:01,929
track queries so the takeaway for for me

00:30:59,440 --> 00:31:03,580
and I think I hope for all of you is

00:31:01,929 --> 00:31:05,859
that it's really helpful just to focus

00:31:03,580 --> 00:31:08,379
on specific use cases like with caching

00:31:05,859 --> 00:31:10,029
if we don't have too much data

00:31:08,379 --> 00:31:11,859
consistency concerns we can start with a

00:31:10,029 --> 00:31:13,989
simple request response cache solve this

00:31:11,859 --> 00:31:17,080
specific instance and then move on to

00:31:13,989 --> 00:31:18,700
solve other things as we see them and

00:31:17,080 --> 00:31:21,190
then our takeaway is to exploit the

00:31:18,700 --> 00:31:22,989
declarative nature of graph QL so we

00:31:21,190 --> 00:31:25,989
don't graphical allows us to express the

00:31:22,989 --> 00:31:28,029
data dependencies in a static text form

00:31:25,989 --> 00:31:30,279
and that we don't need anything at

00:31:28,029 --> 00:31:31,989
runtime to generate a query and so we

00:31:30,279 --> 00:31:33,879
can actually use that to you know

00:31:31,989 --> 00:31:35,979
persist queries at Build time to

00:31:33,879 --> 00:31:37,509
generate optimized artifacts based on

00:31:35,979 --> 00:31:40,090
that query we can even generate

00:31:37,509 --> 00:31:43,809
optimized parsers to handle a response

00:31:40,090 --> 00:31:46,570
for example so what's next

00:31:43,809 --> 00:31:48,340
I think that's natural that graph kilis

00:31:46,570 --> 00:31:50,409
is still kind of earth has only been

00:31:48,340 --> 00:31:52,119
open sourced for about a year we're all

00:31:50,409 --> 00:31:53,440
kind of still in like the exploratory

00:31:52,119 --> 00:31:55,720
phase I'm like what is the right way to

00:31:53,440 --> 00:31:57,460
build a client and I think a lot of the

00:31:55,720 --> 00:31:58,840
community still trying out ideas and so

00:31:57,460 --> 00:32:00,279
I hope that this talk has helped kind of

00:31:58,840 --> 00:32:01,690
establish a vocabulary for some of the

00:32:00,279 --> 00:32:04,119
core components within a graphical

00:32:01,690 --> 00:32:05,799
client system and that from you know

00:32:04,119 --> 00:32:07,029
allows us to kind of have have more

00:32:05,799 --> 00:32:09,789
effective discussions as we talk about

00:32:07,029 --> 00:32:11,979
you know common common components and

00:32:09,789 --> 00:32:13,809
also gives us kind of a collective base

00:32:11,979 --> 00:32:16,659
for where to start exploring with new

00:32:13,809 --> 00:32:18,369
things if you have questions you know

00:32:16,659 --> 00:32:22,590
check out graph go about org which I got

00:32:18,369 --> 00:32:22,590

YouTube URL: https://www.youtube.com/watch?v=1Fg_QtzI7SU


