Title: Automatic Installation of Linux on Z with Tessia - Alexander Efremkin, IBM
Publication date: 2020-09-05
Playlist: Open Mainframe Summit 2020 - Virtual
Description: 
	Automatic Installation of Linux on Z with Tessia - Alexander Efremkin, IBM

Whether you are new to IBM Z or have some experience already, installing Linux on Z manually had never been an easy task. Tessia - an IBM open-source project - automates Linux on Z installation, provides datacenter resource management capabilities for better control over project resources and unifies LPAR, z/VM and KVM operations. Alexander Efremkin, Tessia architect, demonstrates how to get a friendlier experience of Linux on Z installation and customization with Tessia.
Captions: 
	00:00:00,160 --> 00:00:03,919
good morning good evening good afternoon

00:00:02,240 --> 00:00:06,319
my name is alexander here franken

00:00:03,919 --> 00:00:08,400
i'm a software engineer at ibm in linux

00:00:06,319 --> 00:00:10,320
on the workload enablement team

00:00:08,400 --> 00:00:11,679
and today i would like to present you

00:00:10,320 --> 00:00:14,480
our open source solution

00:00:11,679 --> 00:00:16,160
tesser when you want to run some

00:00:14,480 --> 00:00:17,840
workload on the linux on z

00:00:16,160 --> 00:00:20,080
installation of a distribution is

00:00:17,840 --> 00:00:22,160
something you cannot really avoid

00:00:20,080 --> 00:00:23,359
and tessia which stands for task

00:00:22,160 --> 00:00:25,599
execution supporter

00:00:23,359 --> 00:00:27,840
and system installation assistant is

00:00:25,599 --> 00:00:30,640
about making that happen a lot

00:00:27,840 --> 00:00:33,440
it automates installation related tasks

00:00:30,640 --> 00:00:35,600
helps managing the resources involved

00:00:33,440 --> 00:00:37,280
and provides a command line interface

00:00:35,600 --> 00:00:40,960
and a rest api

00:00:37,280 --> 00:00:42,399
to be used in other tool chains however

00:00:40,960 --> 00:00:44,239
we need to make certain provisions for

00:00:42,399 --> 00:00:45,280
an installation whether it is manual or

00:00:44,239 --> 00:00:47,039
automatic

00:00:45,280 --> 00:00:48,719
and there are three key components and

00:00:47,039 --> 00:00:51,280
those are not specific to z they can be

00:00:48,719 --> 00:00:53,600
found at the cloud providers as well

00:00:51,280 --> 00:00:54,960
and non-volatile storage that is where

00:00:53,600 --> 00:00:57,840
the files are placed

00:00:54,960 --> 00:00:59,600
a network to communicate with the system

00:00:57,840 --> 00:01:01,280
and the virtualization method and its

00:00:59,600 --> 00:01:02,879
respective choice of a controlling

00:01:01,280 --> 00:01:05,920
entity also known as the

00:01:02,879 --> 00:01:07,680
hypervisor this is supports

00:01:05,920 --> 00:01:10,240
installation on does the ants gather

00:01:07,680 --> 00:01:12,400
disks network is usually provided via

00:01:10,240 --> 00:01:14,640
also cards but hyper sockets and rocky

00:01:12,400 --> 00:01:15,680
cards are also supported

00:01:14,640 --> 00:01:18,000
there is also more than one

00:01:15,680 --> 00:01:18,560
virtualization method a linux on z can

00:01:18,000 --> 00:01:21,040
be run

00:01:18,560 --> 00:01:22,080
on lpar which is as close to bare metal

00:01:21,040 --> 00:01:24,880
as it can get

00:01:22,080 --> 00:01:26,640
it can be run in a zvm guest or it can

00:01:24,880 --> 00:01:29,920
be run in a kvm guest

00:01:26,640 --> 00:01:32,079
this i suppose all of these too with ray

00:01:29,920 --> 00:01:33,600
exceptions as zbox is used by many

00:01:32,079 --> 00:01:36,320
people and there are

00:01:33,600 --> 00:01:38,479
hundreds of guests in just one box and

00:01:36,320 --> 00:01:39,280
thousands of volumes in just one storage

00:01:38,479 --> 00:01:41,119
server

00:01:39,280 --> 00:01:45,280
so it would be best if we put all

00:01:41,119 --> 00:01:49,439
available resources into a database

00:01:45,280 --> 00:01:52,320
say we have a zbox called here cpc1

00:01:49,439 --> 00:01:54,479
it will be the hypervisor for all

00:01:52,320 --> 00:01:56,159
helpers that we define further

00:01:54,479 --> 00:01:58,799
on the right you can see command line

00:01:56,159 --> 00:02:01,439
that can be used to define one

00:01:58,799 --> 00:02:02,479
an lpar can be added to run linux as it

00:02:01,439 --> 00:02:06,240
is

00:02:02,479 --> 00:02:09,840
or it can run a zvm instance

00:02:06,240 --> 00:02:12,480
or it can be used as a kvm host

00:02:09,840 --> 00:02:14,160
we add further zvm guests by setting

00:02:12,480 --> 00:02:16,959
their type to zvm

00:02:14,160 --> 00:02:18,239
and specifying a respective hypervisor

00:02:16,959 --> 00:02:22,080
and likewise for

00:02:18,239 --> 00:02:23,840
kvm for storage we define a storage

00:02:22,080 --> 00:02:25,440
server and all the volumes are then

00:02:23,840 --> 00:02:29,040
added within it

00:02:25,440 --> 00:02:31,040
a volume type can be a dusty on hpaf rs

00:02:29,040 --> 00:02:32,879
they have the simplest configuration but

00:02:31,040 --> 00:02:34,560
they are this specific

00:02:32,879 --> 00:02:36,319
scarcity volumes on the other hand

00:02:34,560 --> 00:02:38,720
should contain path information

00:02:36,319 --> 00:02:41,680
such as the fcp device in use and

00:02:38,720 --> 00:02:44,720
worldwide port number

00:02:41,680 --> 00:02:46,560
network is separated into zones which

00:02:44,720 --> 00:02:49,440
are collection of subnets

00:02:46,560 --> 00:02:50,160
subnets have standard ip configuration

00:02:49,440 --> 00:02:54,080
and contain

00:02:50,160 --> 00:02:56,000
ip addresses to be used by systems

00:02:54,080 --> 00:02:59,040
finally we get everything together and

00:02:56,000 --> 00:03:01,599
combine resources into a system profile

00:02:59,040 --> 00:03:04,239
a system profile defines how much memory

00:03:01,599 --> 00:03:06,640
and cpus do we want to use

00:03:04,239 --> 00:03:09,680
what are the network interfaces and

00:03:06,640 --> 00:03:11,440
device specific configuration for them

00:03:09,680 --> 00:03:12,800
what are their volumes that we want to

00:03:11,440 --> 00:03:15,440
use

00:03:12,800 --> 00:03:16,239
and one question that is often asked by

00:03:15,440 --> 00:03:18,000
installers

00:03:16,239 --> 00:03:19,920
is their partitioning scheme on the

00:03:18,000 --> 00:03:21,360
volumes

00:03:19,920 --> 00:03:23,599
we are almost done configuring the

00:03:21,360 --> 00:03:25,840
hardware part but we have not decided on

00:03:23,599 --> 00:03:28,319
what are we going to install yet

00:03:25,840 --> 00:03:29,840
officially supported for z are red hat

00:03:28,319 --> 00:03:32,400
enterprise linux

00:03:29,840 --> 00:03:34,080
suse linux enterprise server and ubuntu

00:03:32,400 --> 00:03:35,519
distributions

00:03:34,080 --> 00:03:37,920
tessier comes with installation

00:03:35,519 --> 00:03:42,159
templates for all of these starting from

00:03:37,920 --> 00:03:44,159
rail 7 slash 12 and ubuntu 16.

00:03:42,159 --> 00:03:45,440
it is of course possible to add your own

00:03:44,159 --> 00:03:48,720
installation template

00:03:45,440 --> 00:03:51,519
and so tasty also comes with a fedora

00:03:48,720 --> 00:03:53,360
however tessa doesn't provide any

00:03:51,519 --> 00:03:55,680
repositories

00:03:53,360 --> 00:03:57,760
the best way to obtain a repository

00:03:55,680 --> 00:03:59,519
would be to get an iso image from a

00:03:57,760 --> 00:04:01,920
distribution partner

00:03:59,519 --> 00:04:04,000
mount the ic image to a directory and

00:04:01,920 --> 00:04:04,640
then publish this directory via a web

00:04:04,000 --> 00:04:07,920
server

00:04:04,640 --> 00:04:11,040
so that tessia could access it then

00:04:07,920 --> 00:04:14,720
only one command is needed to register

00:04:11,040 --> 00:04:16,320
the repository within tc and now

00:04:14,720 --> 00:04:17,919
we can proceed with the automatic

00:04:16,320 --> 00:04:20,479
installations

00:04:17,919 --> 00:04:21,680
dc performs automated installations

00:04:20,479 --> 00:04:25,120
using a feature

00:04:21,680 --> 00:04:27,520
of nearly all distros the unattended

00:04:25,120 --> 00:04:28,479
installation using data from the

00:04:27,520 --> 00:04:31,360
database

00:04:28,479 --> 00:04:32,800
tester generates a configuration file

00:04:31,360 --> 00:04:36,160
whether it is a kickstart

00:04:32,800 --> 00:04:39,040
or an outlier or a precede and

00:04:36,160 --> 00:04:40,479
then boots the installer with it the

00:04:39,040 --> 00:04:43,280
biggest difference

00:04:40,479 --> 00:04:43,840
apart from different distros having

00:04:43,280 --> 00:04:46,080
different

00:04:43,840 --> 00:04:49,520
configuration files comes from the

00:04:46,080 --> 00:04:49,520
choice of the hypervisor

00:04:50,080 --> 00:04:57,199
for example on cvm tessier connects

00:04:53,280 --> 00:04:58,240
with the cvm hypervisor using the 3270

00:04:57,199 --> 00:05:01,360
protocol

00:04:58,240 --> 00:05:04,160
it then checks the attached hardware

00:05:01,360 --> 00:05:05,039
and downloads kernel and initial ram

00:05:04,160 --> 00:05:07,520
disk

00:05:05,039 --> 00:05:08,240
from the repository to the guest's

00:05:07,520 --> 00:05:10,320
reader

00:05:08,240 --> 00:05:12,400
it also supplies an installer kernel

00:05:10,320 --> 00:05:15,360
command line

00:05:12,400 --> 00:05:16,560
the guest is then ibled from the reader

00:05:15,360 --> 00:05:19,360
and installation is

00:05:16,560 --> 00:05:21,440
started in the automatic mode to which

00:05:19,360 --> 00:05:25,440
then tessa provides the configuration

00:05:21,440 --> 00:05:27,520
file from the template

00:05:25,440 --> 00:05:28,639
on an lpar situation is somewhat

00:05:27,520 --> 00:05:32,080
different

00:05:28,639 --> 00:05:35,199
desi communicates with hmc using hmc

00:05:32,080 --> 00:05:38,160
api and then it

00:05:35,199 --> 00:05:38,960
updates the activation profile of the

00:05:38,160 --> 00:05:41,520
system

00:05:38,960 --> 00:05:43,600
to specify the correct amount of cpus

00:05:41,520 --> 00:05:45,759
and memory

00:05:43,600 --> 00:05:46,880
now normally when you install linux on

00:05:45,759 --> 00:05:49,680
an lpar

00:05:46,880 --> 00:05:50,639
you boot it from a removable media such

00:05:49,680 --> 00:05:53,919
as a dvd

00:05:50,639 --> 00:05:56,639
or a remote ftp server however such

00:05:53,919 --> 00:06:00,080
facilities for tc are unavailable

00:05:56,639 --> 00:06:03,440
and to work around that desi provides

00:06:00,080 --> 00:06:07,440
a pre-installer image so the helper

00:06:03,440 --> 00:06:10,000
is loaded from the preinstaller image

00:06:07,440 --> 00:06:12,400
which then downloads the kernel and the

00:06:10,000 --> 00:06:15,120
initial ram disk from the repository

00:06:12,400 --> 00:06:17,280
and provides a corresponding kernel

00:06:15,120 --> 00:06:19,919
command line

00:06:17,280 --> 00:06:21,680
then the preinstaller image k execs into

00:06:19,919 --> 00:06:24,720
the installation kernel

00:06:21,680 --> 00:06:27,600
and then it starts in the automatic mode

00:06:24,720 --> 00:06:28,800
and downloads the rest of the

00:06:27,600 --> 00:06:32,160
configuration from

00:06:28,800 --> 00:06:35,360
dessert server for

00:06:32,160 --> 00:06:38,880
kvm they say connects to the kvm host

00:06:35,360 --> 00:06:39,840
directly using ssh and creates virtual

00:06:38,880 --> 00:06:43,280
definitions

00:06:39,840 --> 00:06:46,240
for the guest guest is starting

00:06:43,280 --> 00:06:47,280
using the remote kernel and initial

00:06:46,240 --> 00:06:50,000
route disk as

00:06:47,280 --> 00:06:50,960
specified in the repository the

00:06:50,000 --> 00:06:53,520
installation is

00:06:50,960 --> 00:06:54,479
started automatically and the rest of

00:06:53,520 --> 00:06:57,840
the configuration

00:06:54,479 --> 00:06:59,520
is retrieved from tessier server once an

00:06:57,840 --> 00:07:02,240
installation process is started

00:06:59,520 --> 00:07:04,160
it takes a few minutes to complete

00:07:02,240 --> 00:07:05,919
tessier stores the output log

00:07:04,160 --> 00:07:09,360
and displays all the steps it takes to

00:07:05,919 --> 00:07:09,360
get the system up and running

00:07:09,520 --> 00:07:13,360
automated installations could be

00:07:11,039 --> 00:07:16,240
difficult to perform on scale without

00:07:13,360 --> 00:07:17,039
air resource management system also you

00:07:16,240 --> 00:07:19,039
wouldn't want to

00:07:17,039 --> 00:07:20,960
reinstall a live production system or

00:07:19,039 --> 00:07:22,960
erase a valuable disk

00:07:20,960 --> 00:07:24,319
so tyson implements a resource access

00:07:22,960 --> 00:07:26,160
control

00:07:24,319 --> 00:07:27,520
all resources can be assigned to

00:07:26,160 --> 00:07:30,560
different projects

00:07:27,520 --> 00:07:33,520
and distributed between the users

00:07:30,560 --> 00:07:34,319
the users may have a varying degree of

00:07:33,520 --> 00:07:36,560
access

00:07:34,319 --> 00:07:38,400
to the resources depending on their role

00:07:36,560 --> 00:07:41,520
in the projects

00:07:38,400 --> 00:07:43,680
this helps keep track of who owns what

00:07:41,520 --> 00:07:45,199
and doubles as a safeguard against

00:07:43,680 --> 00:07:47,599
mistakes

00:07:45,199 --> 00:07:49,199
on top of that tessel lets you run an

00:07:47,599 --> 00:07:50,080
ansible playbook on the installed

00:07:49,199 --> 00:07:52,000
systems

00:07:50,080 --> 00:07:55,360
here is a simplified example of a

00:07:52,000 --> 00:07:57,680
parameter file that lets you do that

00:07:55,360 --> 00:07:59,280
ansible playbook may be located in a git

00:07:57,680 --> 00:08:01,840
repository

00:07:59,280 --> 00:08:03,759
and the entry point and systems and

00:08:01,840 --> 00:08:05,919
their groups are all listed in the

00:08:03,759 --> 00:08:08,800
parameter file

00:08:05,919 --> 00:08:09,680
once the playbook has started tesse

00:08:08,800 --> 00:08:11,919
records it

00:08:09,680 --> 00:08:12,960
as a job and it is executed in the

00:08:11,919 --> 00:08:16,000
background

00:08:12,960 --> 00:08:19,039
all the output is stored just like as

00:08:16,000 --> 00:08:20,960
in alt installation

00:08:19,039 --> 00:08:23,120
but there is more than that tessie

00:08:20,960 --> 00:08:25,360
exposes a rest api interface

00:08:23,120 --> 00:08:26,720
which opens up limitless possibilities

00:08:25,360 --> 00:08:28,800
to consume it

00:08:26,720 --> 00:08:30,800
so provisioning linux on this system in

00:08:28,800 --> 00:08:34,159
a desired configuration

00:08:30,800 --> 00:08:36,880
could be a part of an existing pipeline

00:08:34,159 --> 00:08:37,760
and by the way command line interface

00:08:36,880 --> 00:08:40,640
also uses

00:08:37,760 --> 00:08:43,120
rest api so it can be installed

00:08:40,640 --> 00:08:44,480
separately to connect to a remote test

00:08:43,120 --> 00:08:46,640
server

00:08:44,480 --> 00:08:48,959
and of course nothing is stopping from

00:08:46,640 --> 00:08:51,200
plugging in a web interface

00:08:48,959 --> 00:08:52,640
which could make self-provisioning of a

00:08:51,200 --> 00:08:55,440
linux on the system

00:08:52,640 --> 00:08:56,880
as simple as that desi is being used

00:08:55,440 --> 00:08:58,560
internally in multiple

00:08:56,880 --> 00:09:00,240
projects and several continuous

00:08:58,560 --> 00:09:02,000
integration systems

00:09:00,240 --> 00:09:03,920
i invite you to take a look at the sa

00:09:02,000 --> 00:09:05,440
documentation and try it out for

00:09:03,920 --> 00:09:07,680
yourself

00:09:05,440 --> 00:09:08,959
feel free to open up pull requests or

00:09:07,680 --> 00:09:11,519
issues in git

00:09:08,959 --> 00:09:14,399
and reach out to us directly if you have

00:09:11,519 --> 00:09:16,320
any questions or suggestions

00:09:14,399 --> 00:09:20,080
thank you very much for your attention

00:09:16,320 --> 00:09:20,080

YouTube URL: https://www.youtube.com/watch?v=_4Uk0pXAhig


