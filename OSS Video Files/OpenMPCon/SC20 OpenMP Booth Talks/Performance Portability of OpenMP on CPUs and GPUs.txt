Title: Performance Portability of OpenMP on CPUs and GPUs
Publication date: 2020-10-12
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Tom Deakin of University of Bristol, is part of the OpenMP Booth Talk series created for Supercomputing 2020. A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20/
Captions: 
	00:00:01,199 --> 00:00:04,080
hi i'm tom deakin from the university of

00:00:03,280 --> 00:00:07,440
bristol

00:00:04,080 --> 00:00:08,559
and in this openmp booth talk at sc20

00:00:07,440 --> 00:00:10,880
i'm going to be talking a little bit

00:00:08,559 --> 00:00:14,400
about performance portability of openmp

00:00:10,880 --> 00:00:15,599
on cpus and gpus

00:00:14,400 --> 00:00:17,840
now the reason why performance

00:00:15,599 --> 00:00:20,240
portability is really important

00:00:17,840 --> 00:00:21,840
is if we look at the upcoming exascale

00:00:20,240 --> 00:00:22,560
systems that are going to be installed

00:00:21,840 --> 00:00:24,720
over the next

00:00:22,560 --> 00:00:25,760
18 months two years something in that

00:00:24,720 --> 00:00:28,000
time frame

00:00:25,760 --> 00:00:29,519
we can see this huge diversity in

00:00:28,000 --> 00:00:31,359
processor technology

00:00:29,519 --> 00:00:33,520
we have cpus from different vendors and

00:00:31,359 --> 00:00:36,320
gpus from different vendors

00:00:33,520 --> 00:00:37,680
just looking at uh in the united states

00:00:36,320 --> 00:00:40,559
we have these machines

00:00:37,680 --> 00:00:43,440
per mata frontier aurora and el capitan

00:00:40,559 --> 00:00:46,239
all in the pre-exoscale and exascale

00:00:43,440 --> 00:00:47,520
sizes these are going to have cpus from

00:00:46,239 --> 00:00:51,280
intel and amd

00:00:47,520 --> 00:00:54,000
and gpus from nvidia amd and intel

00:00:51,280 --> 00:00:56,000
in japan the fugaku supercomputer is

00:00:54,000 --> 00:00:58,399
based on these arm-based processes from

00:00:56,000 --> 00:01:01,039
fujitsu the a64fx

00:00:58,399 --> 00:01:03,359
so in order for us as as code developers

00:01:01,039 --> 00:01:04,799
we'd like to write our parallel programs

00:01:03,359 --> 00:01:06,960
they're going to be targeting these

00:01:04,799 --> 00:01:09,520
these primarily heterogeneous nodes

00:01:06,960 --> 00:01:10,880
that contain diverse processor

00:01:09,520 --> 00:01:11,600
technologies from lots of different

00:01:10,880 --> 00:01:13,439
vendors

00:01:11,600 --> 00:01:15,360
and ideally we'd only like to write our

00:01:13,439 --> 00:01:17,200
code perhaps once

00:01:15,360 --> 00:01:19,119
and be able to port it between the

00:01:17,200 --> 00:01:20,960
different architectures

00:01:19,119 --> 00:01:22,720
and achieve similar levels of high

00:01:20,960 --> 00:01:26,000
performance efficiency between those

00:01:22,720 --> 00:01:28,000
different platforms

00:01:26,000 --> 00:01:29,040
openmp is this shared memory parallel

00:01:28,000 --> 00:01:31,280
programming model

00:01:29,040 --> 00:01:33,280
and that allows you to add these

00:01:31,280 --> 00:01:34,079
compiler directives to your programs

00:01:33,280 --> 00:01:37,200
that are written in c

00:01:34,079 --> 00:01:37,439
c plus and fortran it's an open standard

00:01:37,200 --> 00:01:39,520
it's

00:01:37,439 --> 00:01:40,640
it's contributed to by lots of different

00:01:39,520 --> 00:01:43,119
vendors

00:01:40,640 --> 00:01:44,720
and they all help define what openmp the

00:01:43,119 --> 00:01:46,479
standard is

00:01:44,720 --> 00:01:48,479
so it has these compiler directives that

00:01:46,479 --> 00:01:51,520
you annotate your code with

00:01:48,479 --> 00:01:53,040
and these help the compiler do some

00:01:51,520 --> 00:01:55,439
parallelism for you

00:01:53,040 --> 00:01:56,880
and tell it exactly how you'd like to

00:01:55,439 --> 00:01:58,640
parallelize your code

00:01:56,880 --> 00:02:01,040
there are also api calls that you can

00:01:58,640 --> 00:02:03,040
use to to query the runtime and things

00:02:01,040 --> 00:02:05,840
and but primarily you program with

00:02:03,040 --> 00:02:08,720
openmp using the compiler directives

00:02:05,840 --> 00:02:09,119
now openmp has been around since 1997

00:02:08,720 --> 00:02:11,760
and

00:02:09,119 --> 00:02:14,160
in 2013 they introduced the target model

00:02:11,760 --> 00:02:17,040
in openmp 4.0

00:02:14,160 --> 00:02:19,840
this allowed openmp to start targeting

00:02:17,040 --> 00:02:21,040
accelerated devices such as gpus

00:02:19,840 --> 00:02:22,480
now there are really important

00:02:21,040 --> 00:02:23,200
refinements they released two years

00:02:22,480 --> 00:02:25,680
later in

00:02:23,200 --> 00:02:27,280
openmp 4.5 and that's the version of

00:02:25,680 --> 00:02:29,200
openmp that i'm going to primarily be

00:02:27,280 --> 00:02:30,959
talking about today

00:02:29,200 --> 00:02:33,280
this has those mechanisms for both

00:02:30,959 --> 00:02:35,440
parallelism and data movement between

00:02:33,280 --> 00:02:36,239
the host and the target device this this

00:02:35,440 --> 00:02:38,640
gpu

00:02:36,239 --> 00:02:40,720
so there's apis and compiler directives

00:02:38,640 --> 00:02:42,560
in there that enable you to define

00:02:40,720 --> 00:02:45,200
how the parallelism should be mapped to

00:02:42,560 --> 00:02:47,040
the to the architectures that are that

00:02:45,200 --> 00:02:49,519
are accelerated such as gpu

00:02:47,040 --> 00:02:50,640
and also how to move data from the host

00:02:49,519 --> 00:02:52,879
onto the device

00:02:50,640 --> 00:02:55,440
to be executed with and then back onto

00:02:52,879 --> 00:02:57,599
the host at the end

00:02:55,440 --> 00:02:58,879
the data transfer is a combination of

00:02:57,599 --> 00:03:00,879
these implicit rules

00:02:58,879 --> 00:03:02,879
where stuff automatically moves between

00:03:00,879 --> 00:03:04,800
the hosts and the device and back again

00:03:02,879 --> 00:03:06,800
and also explicit controls that you have

00:03:04,800 --> 00:03:10,239
put in using these directives written by

00:03:06,800 --> 00:03:12,159
us as programmers what this means is

00:03:10,239 --> 00:03:13,760
that openmp is one of the options that

00:03:12,159 --> 00:03:16,720
we have for for programming

00:03:13,760 --> 00:03:19,680
both cpus and gpus using open standards

00:03:16,720 --> 00:03:21,120
in a performance portable way

00:03:19,680 --> 00:03:23,280
and to show that i'm going to use this

00:03:21,120 --> 00:03:25,360
code that i wrote called babelstream

00:03:23,280 --> 00:03:27,040
babelstream is a benchmark based on

00:03:25,360 --> 00:03:28,799
mccalpin stream which i'm sure many of

00:03:27,040 --> 00:03:30,799
you are familiar with

00:03:28,799 --> 00:03:32,080
so mccalpin stream is this benchmark

00:03:30,799 --> 00:03:34,560
that is used to

00:03:32,080 --> 00:03:35,519
measure achievable main memory bandwidth

00:03:34,560 --> 00:03:38,000
you have

00:03:35,519 --> 00:03:38,799
three large arrays that will be resident

00:03:38,000 --> 00:03:40,799
in dram

00:03:38,799 --> 00:03:43,519
in main memory if it's on a gpu that

00:03:40,799 --> 00:03:46,239
would be high bandwidth memory typically

00:03:43,519 --> 00:03:48,080
and it runs these simple vector kernels

00:03:46,239 --> 00:03:49,840
in order to

00:03:48,080 --> 00:03:51,840
measure the the memory bandwidth the

00:03:49,840 --> 00:03:53,680
sustained memory bandwidth that you can

00:03:51,840 --> 00:03:55,680
you can achieve on that particular

00:03:53,680 --> 00:03:58,000
processor

00:03:55,680 --> 00:03:59,680
now babel stream differentiates itself

00:03:58,000 --> 00:04:00,080
from mccalpin stream in some very

00:03:59,680 --> 00:04:02,720
important

00:04:00,080 --> 00:04:03,439
ways firstly we allocate a raise on the

00:04:02,720 --> 00:04:06,080
heap

00:04:03,439 --> 00:04:08,400
and secondly and it's related we we only

00:04:06,080 --> 00:04:10,720
know the problem size at runtime

00:04:08,400 --> 00:04:12,640
now mccalpin stream takes the problem

00:04:10,720 --> 00:04:14,319
size at compile time this means the

00:04:12,640 --> 00:04:16,160
arrays allocated on the stack and

00:04:14,319 --> 00:04:16,959
compilers are able to optimize the

00:04:16,160 --> 00:04:20,000
placement

00:04:16,959 --> 00:04:21,919
and alignment of memory um to

00:04:20,000 --> 00:04:23,680
to maybe best exploit some tricks that

00:04:21,919 --> 00:04:26,000
it knows inside the compiler

00:04:23,680 --> 00:04:28,000
now many hpc codes do not have that

00:04:26,000 --> 00:04:30,800
luxury we have to read the problem size

00:04:28,000 --> 00:04:33,199
in at run time and we also means we have

00:04:30,800 --> 00:04:34,800
to allocate our large arrays on the heap

00:04:33,199 --> 00:04:36,479
this means that if we write a standard

00:04:34,800 --> 00:04:38,160
code we wouldn't expect to achieve

00:04:36,479 --> 00:04:39,120
anywhere near the bandwidth of a stream

00:04:38,160 --> 00:04:40,479
benchmark

00:04:39,120 --> 00:04:42,479
and and these are some of the reasons

00:04:40,479 --> 00:04:44,160
why so for babel stream

00:04:42,479 --> 00:04:47,680
we we make sure we write it in a way

00:04:44,160 --> 00:04:49,280
that is following best practices for hpc

00:04:47,680 --> 00:04:51,120
we allocate our rays on the heap and we

00:04:49,280 --> 00:04:52,800
read the problem size in a runtime

00:04:51,120 --> 00:04:55,199
but we still are able to achieve close

00:04:52,800 --> 00:04:57,280
to the peak performance and by

00:04:55,199 --> 00:04:59,120
you know by ensuring that the compiler

00:04:57,280 --> 00:05:01,759
knows as much information as it can

00:04:59,120 --> 00:05:03,520
and compilers are improving all the time

00:05:01,759 --> 00:05:04,479
importantly and this is where openmp

00:05:03,520 --> 00:05:06,160
fits in

00:05:04,479 --> 00:05:08,160
we we support a wide range of

00:05:06,160 --> 00:05:09,039
programming models so that we can run on

00:05:08,160 --> 00:05:10,880
lots of different

00:05:09,039 --> 00:05:12,560
cpus and gpus from lots of different

00:05:10,880 --> 00:05:13,919
vendors so most of the parallel

00:05:12,560 --> 00:05:15,039
programming models that you'll have come

00:05:13,919 --> 00:05:19,039
across there is a

00:05:15,039 --> 00:05:19,039
version of babel stream that exists

00:05:19,360 --> 00:05:22,720
so there's there's five kernels that we

00:05:21,039 --> 00:05:24,960
include in this benchmark and the one

00:05:22,720 --> 00:05:28,000
i'm going to focus on today is triad

00:05:24,960 --> 00:05:29,600
this takes two vectors it scales one of

00:05:28,000 --> 00:05:30,160
them by a scalar value adds them

00:05:29,600 --> 00:05:33,520
together

00:05:30,160 --> 00:05:35,440
and stores them in a third array

00:05:33,520 --> 00:05:37,199
now the code for that on a cpu using

00:05:35,440 --> 00:05:39,360
openmp might look like this

00:05:37,199 --> 00:05:41,600
this is written in c we have a simple

00:05:39,360 --> 00:05:43,280
for loop and our three arrays that we

00:05:41,600 --> 00:05:45,520
have allocated elsewhere

00:05:43,280 --> 00:05:47,440
we can see we just have our simple loop

00:05:45,520 --> 00:05:49,919
and we annotate the for loop with

00:05:47,440 --> 00:05:52,560
a compiler directive this pragma omp

00:05:49,919 --> 00:05:55,280
parallel 4. this tells

00:05:52,560 --> 00:05:56,800
openmp to to launch a number of threads

00:05:55,280 --> 00:06:00,000
according to the parallel

00:05:56,800 --> 00:06:01,919
directive and then the four part of that

00:06:00,000 --> 00:06:03,680
of that directive says to work share the

00:06:01,919 --> 00:06:05,039
iterations of that loop between all

00:06:03,680 --> 00:06:06,639
those different threads

00:06:05,039 --> 00:06:08,720
so this is now a parallel loop that will

00:06:06,639 --> 00:06:10,720
run on a cpu

00:06:08,720 --> 00:06:12,639
now to make this run on a gpu we have to

00:06:10,720 --> 00:06:14,319
add a little bit more

00:06:12,639 --> 00:06:15,840
so i'm going to focus first of all on

00:06:14,319 --> 00:06:18,240
the parallelism which is

00:06:15,840 --> 00:06:20,319
extending that parallel 4 directive that

00:06:18,240 --> 00:06:22,319
we saw above our loop

00:06:20,319 --> 00:06:24,160
now you can see it reads pragma omp

00:06:22,319 --> 00:06:24,800
target teams distribute parallel for

00:06:24,160 --> 00:06:26,319
cindy

00:06:24,800 --> 00:06:28,720
that's a bit of a mouthful but really

00:06:26,319 --> 00:06:30,479
it's just saying use all the the levels

00:06:28,720 --> 00:06:31,840
of the parallel hierarchy available

00:06:30,479 --> 00:06:33,600
within openmp

00:06:31,840 --> 00:06:35,120
and parallelize the loop across all of

00:06:33,600 --> 00:06:36,800
that hierarchy

00:06:35,120 --> 00:06:38,479
so you're just telling the compiler

00:06:36,800 --> 00:06:40,720
parallelize everything over here

00:06:38,479 --> 00:06:42,880
and the degree to which it assigns bits

00:06:40,720 --> 00:06:44,400
of the iteration space to that hierarchy

00:06:42,880 --> 00:06:45,440
is still very much implementation

00:06:44,400 --> 00:06:46,800
defined

00:06:45,440 --> 00:06:48,160
but we are just saying here is a

00:06:46,800 --> 00:06:50,960
complete parallel loop go and

00:06:48,160 --> 00:06:52,400
parallelize it

00:06:50,960 --> 00:06:54,560
before and after the loop we have to

00:06:52,400 --> 00:06:54,880
ensure our data moves from the host to

00:06:54,560 --> 00:06:56,319
the

00:06:54,880 --> 00:06:58,319
to the device in order to run our

00:06:56,319 --> 00:07:00,639
parallel kernel and then from the

00:06:58,319 --> 00:07:02,960
device back to the host at the end and

00:07:00,639 --> 00:07:03,680
this is what these enter data and exit

00:07:02,960 --> 00:07:06,800
data

00:07:03,680 --> 00:07:07,360
directives are doing all we're doing is

00:07:06,800 --> 00:07:09,440
is

00:07:07,360 --> 00:07:10,479
adding these map clauses which say the

00:07:09,440 --> 00:07:12,960
direction

00:07:10,479 --> 00:07:15,039
of the the data transfer so it's from

00:07:12,960 --> 00:07:16,479
the host perspective so we're mapping to

00:07:15,039 --> 00:07:19,199
the device at the start

00:07:16,479 --> 00:07:20,880
and mapping from the device at the end

00:07:19,199 --> 00:07:21,919
and then we map our three arrays that's

00:07:20,880 --> 00:07:24,800
all start at

00:07:21,919 --> 00:07:25,919
zero and are of length array size so we

00:07:24,800 --> 00:07:28,560
have these simple

00:07:25,919 --> 00:07:30,240
ways of moving memory on demand from the

00:07:28,560 --> 00:07:33,440
device to the host and back

00:07:30,240 --> 00:07:34,880
again this sets up the device data

00:07:33,440 --> 00:07:37,120
environment as it's known so that when

00:07:34,880 --> 00:07:39,039
the target kernel itself is launched

00:07:37,120 --> 00:07:42,319
that data is already there and resident

00:07:39,039 --> 00:07:42,319
on the device and ready to go

00:07:42,479 --> 00:07:46,319
so what does this mean for performance

00:07:44,800 --> 00:07:46,960
well in this heat map on the right hand

00:07:46,319 --> 00:07:48,479
side

00:07:46,960 --> 00:07:50,560
i'm showing the percentage of the

00:07:48,479 --> 00:07:52,319
theoretical peak memory bandwidth

00:07:50,560 --> 00:07:53,759
so babel stream will output the memory

00:07:52,319 --> 00:07:55,280
bandwidth that was attained the

00:07:53,759 --> 00:07:57,520
sustained memory bandwidth

00:07:55,280 --> 00:07:58,720
and we can look on a tech sheet from the

00:07:57,520 --> 00:08:00,319
vendor and see what

00:07:58,720 --> 00:08:02,000
is the peak bandwidth that particular

00:08:00,319 --> 00:08:04,160
processor might provide

00:08:02,000 --> 00:08:05,840
and we compute the percentage and plot

00:08:04,160 --> 00:08:08,080
that in this figure

00:08:05,840 --> 00:08:09,599
so on the y-axis on the left-hand side i

00:08:08,080 --> 00:08:11,599
have lots of different processors

00:08:09,599 --> 00:08:15,280
there's 15 different processors here

00:08:11,599 --> 00:08:17,199
ranging from cpus from intel amd ibm

00:08:15,280 --> 00:08:18,720
and arm based processors such as the

00:08:17,199 --> 00:08:20,919
thunder x2 from marvel

00:08:18,720 --> 00:08:22,720
the amazon graviton 2 and the fujitsu

00:08:20,919 --> 00:08:26,080
a64fx

00:08:22,720 --> 00:08:29,680
i have three hpc gpus from nvidia

00:08:26,080 --> 00:08:31,440
the p100 v100 and ampere the a100 along

00:08:29,680 --> 00:08:34,560
with a consumer device that the

00:08:31,440 --> 00:08:37,599
turing gpu from amd i have the

00:08:34,560 --> 00:08:38,640
radeon 7 and the mi 50 both gpus from

00:08:37,599 --> 00:08:41,360
from amd

00:08:38,640 --> 00:08:42,080
and finally an intel gpu the iris pro

00:08:41,360 --> 00:08:44,959
gen 9

00:08:42,080 --> 00:08:45,839
this is often found in um you know

00:08:44,959 --> 00:08:48,000
desktop

00:08:45,839 --> 00:08:50,480
integrated graphic processors and we're

00:08:48,000 --> 00:08:51,360
using here as an example of an intel gpu

00:08:50,480 --> 00:08:53,120
given that

00:08:51,360 --> 00:08:54,880
one of the large machines in the united

00:08:53,120 --> 00:08:58,080
states the aurora process

00:08:54,880 --> 00:09:01,600
supercomputer is due to have intel gpus

00:08:58,080 --> 00:09:03,839
at exoscale so this gives us a wide

00:09:01,600 --> 00:09:05,760
range of cpus and gpus from all the

00:09:03,839 --> 00:09:08,320
vendors and captures many of the latest

00:09:05,760 --> 00:09:09,839
processes that they've released now on

00:09:08,320 --> 00:09:11,200
the x-axis here i have

00:09:09,839 --> 00:09:13,440
six different programming models

00:09:11,200 --> 00:09:14,720
including openmp on the bottom left

00:09:13,440 --> 00:09:16,720
there

00:09:14,720 --> 00:09:17,920
so reading up the column for openmp we

00:09:16,720 --> 00:09:20,720
can see the

00:09:17,920 --> 00:09:22,000
percentage of achievable memory

00:09:20,720 --> 00:09:24,399
bandwidth that we're measuring with

00:09:22,000 --> 00:09:26,399
openmp across our different processors

00:09:24,399 --> 00:09:27,200
now openmp we have complete coverage and

00:09:26,399 --> 00:09:28,720
of all the

00:09:27,200 --> 00:09:30,399
program modes that we have it's the only

00:09:28,720 --> 00:09:31,600
model that that at this time we were

00:09:30,399 --> 00:09:34,080
able to

00:09:31,600 --> 00:09:35,760
generate results on all of our processes

00:09:34,080 --> 00:09:36,000
things like sickle and cocoa are also

00:09:35,760 --> 00:09:39,040
doing

00:09:36,000 --> 00:09:39,760
very well and we see near complete

00:09:39,040 --> 00:09:42,080
coverage

00:09:39,760 --> 00:09:43,920
only one device is is not able to to

00:09:42,080 --> 00:09:45,440
measure

00:09:43,920 --> 00:09:47,040
what's important though is we see that

00:09:45,440 --> 00:09:49,040
for openmp we get very

00:09:47,040 --> 00:09:50,880
consistent results for this triad kernel

00:09:49,040 --> 00:09:53,120
across all our processes

00:09:50,880 --> 00:09:53,920
all the results are you know around 70

00:09:53,120 --> 00:09:55,519
plus percent

00:09:53,920 --> 00:09:57,360
in the main there's a few results that

00:09:55,519 --> 00:09:59,519
are slightly lower and and we attribute

00:09:57,360 --> 00:10:00,480
those primarily to immaturities in the

00:09:59,519 --> 00:10:02,320
compiler

00:10:00,480 --> 00:10:04,240
for example the lower result on the

00:10:02,320 --> 00:10:07,839
radeon 7 is using the gcc

00:10:04,240 --> 00:10:11,200
compiler which is very immature with its

00:10:07,839 --> 00:10:12,240
openmp back-end targeting amd gpus

00:10:11,200 --> 00:10:14,240
compared to some of the other

00:10:12,240 --> 00:10:16,480
implementations of openmp targeting

00:10:14,240 --> 00:10:18,800
other architectures

00:10:16,480 --> 00:10:19,839
we expect to see the compilers improve

00:10:18,800 --> 00:10:22,959
over time

00:10:19,839 --> 00:10:23,760
especially as many of these compilers

00:10:22,959 --> 00:10:25,920
will be used at

00:10:23,760 --> 00:10:27,839
exascale using openmp to target these

00:10:25,920 --> 00:10:29,200
different processes

00:10:27,839 --> 00:10:31,440
if you'd like to learn a little bit more

00:10:29,200 --> 00:10:33,040
about this and also some of the some

00:10:31,440 --> 00:10:34,480
extra results from the dock kernel as

00:10:33,040 --> 00:10:36,880
well as other benchmarks

00:10:34,480 --> 00:10:37,680
then at the supercomputer conference

00:10:36,880 --> 00:10:40,959
this year

00:10:37,680 --> 00:10:42,079
at the p3 hpc workshop we're going to be

00:10:40,959 --> 00:10:43,600
presenting a paper called

00:10:42,079 --> 00:10:45,519
tracking performance portability on the

00:10:43,600 --> 00:10:46,880
yellow brick road to exascale so if

00:10:45,519 --> 00:10:47,839
you're interested in finding out some

00:10:46,880 --> 00:10:49,920
more about this then

00:10:47,839 --> 00:10:51,279
then do join up for the the workshop

00:10:49,920 --> 00:10:54,800
track and

00:10:51,279 --> 00:10:54,800
um you can hear my talk there

00:10:56,320 --> 00:11:00,079
so openmp5 was released a few years ago

00:10:59,360 --> 00:11:02,959
now

00:11:00,079 --> 00:11:03,279
and um there are some key features there

00:11:02,959 --> 00:11:06,800
that

00:11:03,279 --> 00:11:09,920
help with writing openmp codes targeting

00:11:06,800 --> 00:11:11,680
gpus firstly the the experience of

00:11:09,920 --> 00:11:13,279
writing reductions is now much more

00:11:11,680 --> 00:11:16,959
improved it's much more

00:11:13,279 --> 00:11:20,000
seamless back in openmp 4.5 you had to

00:11:16,959 --> 00:11:23,200
add a map clause on

00:11:20,000 --> 00:11:25,600
on the parallel target kernel that

00:11:23,200 --> 00:11:26,880
moves the reduction result from the

00:11:25,600 --> 00:11:28,800
device back to the host

00:11:26,880 --> 00:11:30,560
at the end of the execution if you

00:11:28,800 --> 00:11:31,920
missed this map clause the reduction

00:11:30,560 --> 00:11:33,360
would happen but you wouldn't get the

00:11:31,920 --> 00:11:36,720
result back on the host

00:11:33,360 --> 00:11:38,160
which is not really what you'd like

00:11:36,720 --> 00:11:39,680
so you'd add the map clause to the

00:11:38,160 --> 00:11:42,079
reduction clause and this means you

00:11:39,680 --> 00:11:43,920
would get the result but in openmp5

00:11:42,079 --> 00:11:46,079
there's no need to add the map clause

00:11:43,920 --> 00:11:46,959
anymore it's now automatically mapped

00:11:46,079 --> 00:11:48,640
back to the host

00:11:46,959 --> 00:11:50,959
so you just have to write the reduction

00:11:48,640 --> 00:11:52,000
clause in openmp5 which is which is a

00:11:50,959 --> 00:11:55,120
much

00:11:52,000 --> 00:11:58,240
much more streamlined experience

00:11:55,120 --> 00:11:59,680
openmp5 also introduces meta directives

00:11:58,240 --> 00:12:01,519
these are these are a kind of

00:11:59,680 --> 00:12:04,639
preprocessor step um

00:12:01,519 --> 00:12:05,600
that that enables you to select which

00:12:04,639 --> 00:12:07,519
batches of

00:12:05,600 --> 00:12:09,680
openmp compiler directives you might

00:12:07,519 --> 00:12:11,519
like to apply to different loops

00:12:09,680 --> 00:12:13,360
so this is a standard way you can you

00:12:11,519 --> 00:12:16,320
can if def different fragments between

00:12:13,360 --> 00:12:16,320
the different kernels

00:12:16,560 --> 00:12:23,519
now most of the compiler directives that

00:12:20,959 --> 00:12:24,240
are used to target the gpu will

00:12:23,519 --> 00:12:26,399
automatically

00:12:24,240 --> 00:12:28,079
run on the host if they can't run or if

00:12:26,399 --> 00:12:30,079
you co if you're not compiling with the

00:12:28,079 --> 00:12:32,560
compiler that will automatically

00:12:30,079 --> 00:12:33,920
support a particular device this means

00:12:32,560 --> 00:12:35,440
that you can just write the target

00:12:33,920 --> 00:12:37,200
directives and they will automatically

00:12:35,440 --> 00:12:39,760
fall back onto the host

00:12:37,200 --> 00:12:40,880
now while this is very useful in in that

00:12:39,760 --> 00:12:43,519
you may only have to

00:12:40,880 --> 00:12:45,600
write one set of directives if you want

00:12:43,519 --> 00:12:47,120
to ensure that that particular kernel is

00:12:45,600 --> 00:12:49,200
offloaded to a gpu

00:12:47,120 --> 00:12:50,240
then openmp5 introduces this new

00:12:49,200 --> 00:12:52,639
environment variable

00:12:50,240 --> 00:12:54,399
rmp target offload you can set this to

00:12:52,639 --> 00:12:58,399
mandatory and this means that the

00:12:54,399 --> 00:13:00,240
the runtime the the kernels the target

00:12:58,399 --> 00:13:01,760
regions must be offloaded to the target

00:13:00,240 --> 00:13:04,079
device they're not allowed to fall back

00:13:01,760 --> 00:13:07,040
to the host

00:13:04,079 --> 00:13:08,560
now some some open question that we um

00:13:07,040 --> 00:13:09,760
that we're thinking about a lot in in

00:13:08,560 --> 00:13:11,839
bristol at the moment

00:13:09,760 --> 00:13:13,680
is in openmp you still have these two

00:13:11,839 --> 00:13:15,120
memory spaces you have the memory space

00:13:13,680 --> 00:13:17,040
of the host and the memory space of the

00:13:15,120 --> 00:13:19,760
device and you have to map

00:13:17,040 --> 00:13:21,440
from between the host and the device now

00:13:19,760 --> 00:13:23,519
whilst unified shared memory means that

00:13:21,440 --> 00:13:24,000
you maybe don't have to map anything at

00:13:23,519 --> 00:13:26,000
all

00:13:24,000 --> 00:13:28,399
and just rely on the support from the

00:13:26,000 --> 00:13:30,480
hardware it's an open question to see

00:13:28,399 --> 00:13:32,320
what in openmp can help with this too

00:13:30,480 --> 00:13:34,320
whether it's through allocators or

00:13:32,320 --> 00:13:35,519
or instead of the map clause so this is

00:13:34,320 --> 00:13:38,160
something that we're thinking about a

00:13:35,519 --> 00:13:38,160
lot at the moment

00:13:38,240 --> 00:13:42,000
so to wrap up it is possible to write

00:13:40,000 --> 00:13:44,079
performance portable code in openmp

00:13:42,000 --> 00:13:45,120
and the results i showed and the link to

00:13:44,079 --> 00:13:48,480
the presentation

00:13:45,120 --> 00:13:49,839
is also you know expands on that further

00:13:48,480 --> 00:13:51,680
the other thing over the last year we've

00:13:49,839 --> 00:13:53,600
seen is really improved support in

00:13:51,680 --> 00:13:54,079
compilers and this is improving all the

00:13:53,600 --> 00:13:55,680
time

00:13:54,079 --> 00:13:57,199
and there's a particular concerted

00:13:55,680 --> 00:13:59,760
effort over the last year

00:13:57,199 --> 00:14:01,519
so we can now see good performance in

00:13:59,760 --> 00:14:04,720
the llvm compiler

00:14:01,519 --> 00:14:06,959
which is targeting nvidia gpus

00:14:04,720 --> 00:14:09,120
as part of one api intel have released a

00:14:06,959 --> 00:14:10,079
new c plus plus compiler and c compiler

00:14:09,120 --> 00:14:11,600
fortran compiler

00:14:10,079 --> 00:14:13,360
and with this they have an openmp

00:14:11,600 --> 00:14:16,079
runtime that will now support

00:14:13,360 --> 00:14:17,680
openmp gpus this is how we obtained our

00:14:16,079 --> 00:14:19,120
results so we have an intel compiler

00:14:17,680 --> 00:14:22,560
that will support an intel

00:14:19,120 --> 00:14:25,360
gpu with using openmp

00:14:22,560 --> 00:14:26,160
in gcc we can also see openmp support

00:14:25,360 --> 00:14:29,040
for

00:14:26,160 --> 00:14:30,000
targeting amd gpus and nvidia gpus as

00:14:29,040 --> 00:14:32,399
well

00:14:30,000 --> 00:14:34,160
and amd's aomp compiler is improving all

00:14:32,399 --> 00:14:37,360
the time based on llvm

00:14:34,160 --> 00:14:40,560
which is adding in support for amd gpus

00:14:37,360 --> 00:14:42,160
the xl compiler from ibm was an initial

00:14:40,560 --> 00:14:43,519
catalyst for some of the support in the

00:14:42,160 --> 00:14:46,880
llvm compiler

00:14:43,519 --> 00:14:48,720
used to target nvidia gpus and with the

00:14:46,880 --> 00:14:52,160
cce compiler that's been around for a

00:14:48,720 --> 00:14:54,079
long time supporting nvidia gpus as well

00:14:52,160 --> 00:14:55,199
do check out the openmp website which

00:14:54,079 --> 00:14:57,680
shows a

00:14:55,199 --> 00:15:00,639
an updated list of all the compilers

00:14:57,680 --> 00:15:02,480
that support openmp and exactly

00:15:00,639 --> 00:15:04,800
what what support is available within

00:15:02,480 --> 00:15:06,560
those compilers

00:15:04,800 --> 00:15:07,920
so that's pretty much it for me if you'd

00:15:06,560 --> 00:15:09,519
like to learn a little bit more about me

00:15:07,920 --> 00:15:11,360
and the university of bristol do have a

00:15:09,519 --> 00:15:12,399
look at my website shown on the bottom

00:15:11,360 --> 00:15:15,760
left

00:15:12,399 --> 00:15:17,120
you can also you know start start

00:15:15,760 --> 00:15:19,680
listening to some of the extra things

00:15:17,120 --> 00:15:21,199
i've i've i've presented about

00:15:19,680 --> 00:15:23,040
different programming models including

00:15:21,199 --> 00:15:24,000
openmp and sickle at those first two

00:15:23,040 --> 00:15:25,279
links there

00:15:24,000 --> 00:15:27,120
and if you're interested in learning

00:15:25,279 --> 00:15:28,079
openmp for the first time do check out

00:15:27,120 --> 00:15:30,320
my tutorial

00:15:28,079 --> 00:15:32,160
same's a computational scientist writing

00:15:30,320 --> 00:15:34,639
fortran so how can you write

00:15:32,160 --> 00:15:36,959
openmp code for fortran and it pretty

00:15:34,639 --> 00:15:38,639
much covers most of the openmp 4.5

00:15:36,959 --> 00:15:41,120
specification

00:15:38,639 --> 00:15:43,440
if you're interested in learning um how

00:15:41,120 --> 00:15:45,120
to target gpus with openmp

00:15:43,440 --> 00:15:46,959
then with my colleague simon mcintosh

00:15:45,120 --> 00:15:48,720
smith we're going to be presenting our

00:15:46,959 --> 00:15:51,199
tutorial at supercomputing

00:15:48,720 --> 00:15:54,320
this year and and the information is on

00:15:51,199 --> 00:15:56,800
the bottom of that slide

00:15:54,320 --> 00:15:58,320
for any more information about openmp do

00:15:56,800 --> 00:16:04,160
have a look at their website

00:15:58,320 --> 00:16:04,160

YouTube URL: https://www.youtube.com/watch?v=75Ej7VPFdow


