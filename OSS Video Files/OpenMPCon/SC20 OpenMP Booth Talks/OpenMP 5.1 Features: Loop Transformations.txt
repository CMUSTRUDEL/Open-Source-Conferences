Title: OpenMP 5.1 Features: Loop Transformations
Publication date: 2020-11-12
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Michael Kruse from the OpenMP language committee, is part of the OpenMP Booth Talk series created for Supercomputing 2020. A PDF of this presentation as well as more videos from this series can be downloaded at link.openmp.org/sc20
Captions: 
	00:00:04,000 --> 00:00:06,000
hi

00:00:04,400 --> 00:00:07,440
welcome to this talk about loop

00:00:06,000 --> 00:00:11,040
transformation new in

00:00:07,440 --> 00:00:12,559
openmp 5.1 i'm merkel cruzer working at

00:00:11,040 --> 00:00:14,719
argon national laboratory on the

00:00:12,559 --> 00:00:17,520
exascape computing project

00:00:14,719 --> 00:00:18,000
let's first go into a preface about uh

00:00:17,520 --> 00:00:19,920
comp

00:00:18,000 --> 00:00:21,920
the history of compiler optimization

00:00:19,920 --> 00:00:24,240
hills one of the first

00:00:21,920 --> 00:00:25,279
uh compiling hints introduced was

00:00:24,240 --> 00:00:27,680
probably

00:00:25,279 --> 00:00:28,320
by the quake compiler in forms of the iv

00:00:27,680 --> 00:00:31,119
depth

00:00:28,320 --> 00:00:32,800
which tells the the compiler that is

00:00:31,119 --> 00:00:35,760
that the loop is allowed to

00:00:32,800 --> 00:00:37,840
be vectorized this was copied by

00:00:35,760 --> 00:00:38,399
multiple other compiler vendors such as

00:00:37,840 --> 00:00:41,840
hdi

00:00:38,399 --> 00:00:44,640
ap intel which also copied the primary

00:00:41,840 --> 00:00:45,840
iv depth and allowed it in their own

00:00:44,640 --> 00:00:49,120
compilers

00:00:45,840 --> 00:00:52,960
recently in open in gcc 9

00:00:49,120 --> 00:00:56,719
it was also introduced in in gcc

00:00:52,960 --> 00:00:59,680
but gcc insists on a

00:00:56,719 --> 00:01:02,000
namespace prefix gcc since technically

00:00:59,680 --> 00:01:04,239
this is not part of the any standardized

00:01:02,000 --> 00:01:06,640
language but a compiler extension

00:01:04,239 --> 00:01:08,000
the cray compiler later introduced this

00:01:06,640 --> 00:01:11,840
as well in form of the

00:01:08,000 --> 00:01:13,760
ci namespace but it's just optional

00:01:11,840 --> 00:01:14,880
other compilers such as microsoft

00:01:13,760 --> 00:01:18,159
compiler and clang

00:01:14,880 --> 00:01:20,400
also provides similar

00:01:18,159 --> 00:01:22,560
programs but have completely different

00:01:20,400 --> 00:01:25,600
syntax here

00:01:22,560 --> 00:01:27,600
in addition to that one some compilers

00:01:25,600 --> 00:01:29,520
support multiple of the spread much to

00:01:27,600 --> 00:01:33,200
the slightly different semantics

00:01:29,520 --> 00:01:35,920
such as the intel pragma vector and

00:01:33,200 --> 00:01:36,799
zmd which not only tell the compiler

00:01:35,920 --> 00:01:38,720
that

00:01:36,799 --> 00:01:40,000
vectorization is allowed but also

00:01:38,720 --> 00:01:44,079
instruct the compiler

00:01:40,000 --> 00:01:47,119
to actually do the uh the vectorization

00:01:44,079 --> 00:01:48,640
um the problem with this in addition to

00:01:47,119 --> 00:01:51,119
the

00:01:48,640 --> 00:01:52,840
lots different syntaxes is that the

00:01:51,119 --> 00:01:56,240
semantics of what these

00:01:52,840 --> 00:01:58,320
um problems actually are doing

00:01:56,240 --> 00:01:59,520
can be quite different so for instance

00:01:58,320 --> 00:02:03,040
create original iv

00:01:59,520 --> 00:02:07,600
depth implementation is implementation

00:02:03,040 --> 00:02:11,120
dependent it basically says that

00:02:07,600 --> 00:02:14,480
the the compiler will detect some

00:02:11,120 --> 00:02:17,200
some dependencies such as reductions

00:02:14,480 --> 00:02:19,680
but is allowed to ignore all the other

00:02:17,200 --> 00:02:22,160
dependencies this means it's sensitive

00:02:19,680 --> 00:02:24,080
to the power analysis power of the

00:02:22,160 --> 00:02:26,000
compiler so if the

00:02:24,080 --> 00:02:27,599
compiler like newer version other

00:02:26,000 --> 00:02:29,920
versions older versions

00:02:27,599 --> 00:02:32,000
does not detect a reduction it will just

00:02:29,920 --> 00:02:35,040
miscompile the

00:02:32,000 --> 00:02:38,319
the application this basically ended

00:02:35,040 --> 00:02:42,160
with openmp 5.0 where openmp

00:02:38,319 --> 00:02:44,160
added a standardized vectorization

00:02:42,160 --> 00:02:46,000
directive in the form of a partner on

00:02:44,160 --> 00:02:49,120
psmd which is standardized

00:02:46,000 --> 00:02:51,280
same semantics same

00:02:49,120 --> 00:02:53,519
same syntax same semantics across all

00:02:51,280 --> 00:02:56,879
the compilers which claim to support

00:02:53,519 --> 00:03:00,959
openmp 5.0 this is usb

00:02:56,879 --> 00:03:03,920
by itself so even if your application

00:03:00,959 --> 00:03:05,040
uses some other means of parallelization

00:03:03,920 --> 00:03:08,560
for instance using the c

00:03:05,040 --> 00:03:10,560
plus plus uh library for threads

00:03:08,560 --> 00:03:12,000
uh some projects still use the program

00:03:10,560 --> 00:03:15,440
psimd directive to

00:03:12,000 --> 00:03:17,599
tell the compiler hey vectorizes loop

00:03:15,440 --> 00:03:18,400
since this is not part of any host

00:03:17,599 --> 00:03:20,879
language

00:03:18,400 --> 00:03:22,319
as a con consequence clang even supports

00:03:20,879 --> 00:03:26,080
a zimbi only mode

00:03:22,319 --> 00:03:29,519
so if you pass the f openmp-7d

00:03:26,080 --> 00:03:31,519
option to clang it will accept the orp

00:03:29,519 --> 00:03:33,440
zmd directive

00:03:31,519 --> 00:03:34,640
but ignore all the other directives and

00:03:33,440 --> 00:03:36,959
does not require

00:03:34,640 --> 00:03:38,159
to link against the openmp standard

00:03:36,959 --> 00:03:40,640
library

00:03:38,159 --> 00:03:42,319
so this is a reality to fix this why

00:03:40,640 --> 00:03:45,760
don't we also do this

00:03:42,319 --> 00:03:48,319
for other directory for instance a lot

00:03:45,760 --> 00:03:51,040
of compilers as shown here

00:03:48,319 --> 00:03:51,599
allow already an annual partner but

00:03:51,040 --> 00:03:55,120
again

00:03:51,599 --> 00:03:56,560
the syntax between all these compilers

00:03:55,120 --> 00:03:58,480
are quite different

00:03:56,560 --> 00:04:00,959
although the semantic difference is not

00:03:58,480 --> 00:04:04,239
a problem because unrolling usually

00:04:00,959 --> 00:04:07,280
should not affect the program semantics

00:04:04,239 --> 00:04:08,959
so with openmp 5.1 we also introduced an

00:04:07,280 --> 00:04:10,959
under a world directive against

00:04:08,959 --> 00:04:12,239
standardized same semantics across

00:04:10,959 --> 00:04:15,200
compilers

00:04:12,239 --> 00:04:16,400
which are going to implement openmp 5.1

00:04:15,200 --> 00:04:19,199
the effect of this

00:04:16,400 --> 00:04:21,680
um depends on the mode so for instance

00:04:19,199 --> 00:04:24,160
we have a clause a full clause if you

00:04:21,680 --> 00:04:27,199
specify problem rp unreal full

00:04:24,160 --> 00:04:28,000
then we do some full unrolling uh in

00:04:27,199 --> 00:04:31,840
that case

00:04:28,000 --> 00:04:34,720
the the number of iterations

00:04:31,840 --> 00:04:36,400
are must be known to the compiler and it

00:04:34,720 --> 00:04:38,880
will just remove the compiler and

00:04:36,400 --> 00:04:41,520
replace it with the instances

00:04:38,880 --> 00:04:42,320
with the instances of the the loop

00:04:41,520 --> 00:04:44,479
bodies

00:04:42,320 --> 00:04:45,759
the other mode is the partial unrolling

00:04:44,479 --> 00:04:49,120
mode in that case

00:04:45,759 --> 00:04:52,800
here we specify an unroll factor of

00:04:49,120 --> 00:04:55,360
4 which copies the loop body code

00:04:52,800 --> 00:04:58,080
the factor number of times and the

00:04:55,360 --> 00:05:00,800
result is something that resembles the

00:04:58,080 --> 00:05:02,000
the basically the code on the left we

00:05:00,800 --> 00:05:06,800
also

00:05:02,000 --> 00:05:10,080
uh is the compiler is also allowed to

00:05:06,800 --> 00:05:11,600
to generate different kinds of codes not

00:05:10,080 --> 00:05:14,000
exactly the same one

00:05:11,600 --> 00:05:16,720
for instance the common optimization or

00:05:14,000 --> 00:05:19,520
how unwarring is usually implemented

00:05:16,720 --> 00:05:20,560
is not checking the action condition and

00:05:19,520 --> 00:05:22,560
every time

00:05:20,560 --> 00:05:23,759
but if you know we are doing four

00:05:22,560 --> 00:05:26,400
iterations we

00:05:23,759 --> 00:05:28,560
just execute this four iterations and do

00:05:26,400 --> 00:05:32,240
a reminder loop at the end of

00:05:28,560 --> 00:05:34,080
of the chord so we have to less overhead

00:05:32,240 --> 00:05:37,360
to check the exit conditions of

00:05:34,080 --> 00:05:41,120
of the loop which can result in some

00:05:37,360 --> 00:05:43,840
performance speed up let's allow to

00:05:41,120 --> 00:05:45,199
to omit the android factor in that case

00:05:43,840 --> 00:05:48,639
the compiler is

00:05:45,199 --> 00:05:50,560
can choose the unroll factor by itself

00:05:48,639 --> 00:05:53,039
usually compilers today already

00:05:50,560 --> 00:05:56,080
implement some unrolling heuristic

00:05:53,039 --> 00:05:57,919
even without a specific grammar and use

00:05:56,080 --> 00:06:00,720
the cost model to determine

00:05:57,919 --> 00:06:02,720
what the unrolling factor is used to be

00:06:00,720 --> 00:06:05,440
and using this syntax

00:06:02,720 --> 00:06:06,880
we tell the compiler hey just use your

00:06:05,440 --> 00:06:10,000
ongoing heuristic

00:06:06,880 --> 00:06:11,759
cost model we already have so we have

00:06:10,000 --> 00:06:15,520
not a specified

00:06:11,759 --> 00:06:16,000
animal factor here we can also emit the

00:06:15,520 --> 00:06:19,039
full

00:06:16,000 --> 00:06:20,960
or partial clause completely

00:06:19,039 --> 00:06:20,960
in not

00:06:32,960 --> 00:06:36,240
basically available whenever you have a

00:06:35,680 --> 00:06:40,400
loop

00:06:36,240 --> 00:06:43,120
with a small constant uh trip count

00:06:40,400 --> 00:06:44,240
uh which removes the loop so have no

00:06:43,120 --> 00:06:46,479
loop overhead and all

00:06:44,240 --> 00:06:48,400
at all and the compiler can do something

00:06:46,479 --> 00:06:50,479
like instruction reordering

00:06:48,400 --> 00:06:53,520
to find a better order between

00:06:50,479 --> 00:06:55,199
instructions from iterations from

00:06:53,520 --> 00:06:57,840
from loop body code from different

00:06:55,199 --> 00:07:01,759
iterations this is basically only

00:06:57,840 --> 00:07:02,160
not beneficial if the l1 instruction

00:07:01,759 --> 00:07:05,039
cache

00:07:02,160 --> 00:07:06,880
is a net but anyway if the compiler

00:07:05,039 --> 00:07:10,720
detects that this there is

00:07:06,880 --> 00:07:13,840
a loop chip count it will unroll it

00:07:10,720 --> 00:07:14,240
probably anyway so keep that in mind it

00:07:13,840 --> 00:07:16,800
might

00:07:14,240 --> 00:07:18,960
be just doing it by themselves but

00:07:16,800 --> 00:07:22,000
having this unroll full

00:07:18,960 --> 00:07:23,680
directives 4 basically forces a compiler

00:07:22,000 --> 00:07:25,599
to do that

00:07:23,680 --> 00:07:26,800
the other case is partial unrolling

00:07:25,599 --> 00:07:30,000
which on

00:07:26,800 --> 00:07:32,479
modern x86 processors probably not

00:07:30,000 --> 00:07:34,639
as useful since these have a very good

00:07:32,479 --> 00:07:37,520
branch predictor and allow

00:07:34,639 --> 00:07:39,120
instruction reordering from different uh

00:07:37,520 --> 00:07:40,560
from its

00:07:39,120 --> 00:07:43,280
instructions from different loop

00:07:40,560 --> 00:07:46,479
iterations so at least i could not

00:07:43,280 --> 00:07:48,479
repeat use something where where the

00:07:46,479 --> 00:07:50,520
unwarranted would actually beneficial

00:07:48,479 --> 00:07:51,759
but for accelerators with less

00:07:50,520 --> 00:07:55,520
sufficient

00:07:51,759 --> 00:07:58,240
um pipelines this might be

00:07:55,520 --> 00:08:00,240
still beneficial for instance from this

00:07:58,240 --> 00:08:00,960
paper called optimal loop unrolling for

00:08:00,240 --> 00:08:04,560
gpu

00:08:00,960 --> 00:08:06,639
gp gpu program they experimented with

00:08:04,560 --> 00:08:09,440
different unroll factors for matrix

00:08:06,639 --> 00:08:12,560
matrix multiplication compiled for

00:08:09,440 --> 00:08:15,680
an nvidia processor and as you can see

00:08:12,560 --> 00:08:19,199
unrolling can give you a speed off

00:08:15,680 --> 00:08:20,560
of a factor of almost two with different

00:08:19,199 --> 00:08:22,800
angle factors

00:08:20,560 --> 00:08:24,160
giving different speed ups where the

00:08:22,800 --> 00:08:27,039
optimum here

00:08:24,160 --> 00:08:28,080
the minimum was an annual factor by 2 of

00:08:27,039 --> 00:08:31,120
the middle

00:08:28,080 --> 00:08:32,959
loop and 16 in the inner loop so you

00:08:31,120 --> 00:08:34,959
it's not required that you only unroll

00:08:32,959 --> 00:08:36,000
the innermost loop but you can basically

00:08:34,959 --> 00:08:39,120
unroll

00:08:36,000 --> 00:08:41,519
all of loops in there so we

00:08:39,120 --> 00:08:44,320
covered unwarning but we also introduced

00:08:41,519 --> 00:08:47,120
uh another directive a tile directive

00:08:44,320 --> 00:08:49,360
the tile directive as hasa now is not

00:08:47,120 --> 00:08:52,880
around in general purpose compiler

00:08:49,360 --> 00:08:54,959
just the open acc model supports a tile

00:08:52,880 --> 00:08:58,480
clause which can be added to

00:08:54,959 --> 00:09:01,519
to the loop directive to tell the

00:08:58,480 --> 00:09:04,640
the compiler to tile this loop

00:09:01,519 --> 00:09:08,000
we added the tile directive as

00:09:04,640 --> 00:09:12,399
a directive to openmp 5.1

00:09:08,000 --> 00:09:15,680
and the tile sizes are specified by

00:09:12,399 --> 00:09:19,680
by a sizes clause this example the psi

00:09:15,680 --> 00:09:23,279
tile size are tiles of sizes 8x8

00:09:19,680 --> 00:09:27,279
which results in code which resembles

00:09:23,279 --> 00:09:29,839
something like on the on the right side

00:09:27,279 --> 00:09:32,160
so with tiling you always get twice as

00:09:29,839 --> 00:09:36,080
many loops as you

00:09:32,160 --> 00:09:38,480
had before as an illustration here

00:09:36,080 --> 00:09:40,880
have something like this we have this

00:09:38,480 --> 00:09:43,920
case a smaller loop with

00:09:40,880 --> 00:09:48,000
16 iterations so 4x4

00:09:43,920 --> 00:09:48,880
tiled into 2x2 tiles and the output code

00:09:48,000 --> 00:09:52,800
you see on the

00:09:48,880 --> 00:09:55,680
bottom right is again for loops

00:09:52,800 --> 00:09:57,120
with the inner two inner loops iterating

00:09:55,680 --> 00:10:01,040
over

00:09:57,120 --> 00:10:03,760
iterations of our tiles so two by two

00:10:01,040 --> 00:10:06,079
sizes and the outer loops it's a over

00:10:03,760 --> 00:10:09,680
over the tiles

00:10:06,079 --> 00:10:12,720
themselves here in this case the

00:10:09,680 --> 00:10:15,600
number of iterations in each dimension

00:10:12,720 --> 00:10:17,360
is a multiple of the time sizes but what

00:10:15,600 --> 00:10:20,959
if this is not the case

00:10:17,360 --> 00:10:21,920
uh in this case we stay with two by two

00:10:20,959 --> 00:10:24,800
tiles

00:10:21,920 --> 00:10:25,440
but we have an iteration space of five

00:10:24,800 --> 00:10:28,800
that uh

00:10:25,440 --> 00:10:30,240
five times five times five so in this

00:10:28,800 --> 00:10:33,440
case we cannot

00:10:30,240 --> 00:10:36,480
uh fill every tile there but

00:10:33,440 --> 00:10:39,519
instead we have some partial tiles where

00:10:36,480 --> 00:10:40,480
some tiles do not iterate four

00:10:39,519 --> 00:10:44,720
iterations

00:10:40,480 --> 00:10:47,360
but just two or maybe even one

00:10:44,720 --> 00:10:49,040
the compiler we made the choice through

00:10:47,360 --> 00:10:51,680
in the specification that we allow the

00:10:49,040 --> 00:10:52,240
compiler to optimize this code in in a

00:10:51,680 --> 00:10:55,680
way

00:10:52,240 --> 00:10:56,959
and that it can separate the code for

00:10:55,680 --> 00:11:00,160
the complete tiles

00:10:56,959 --> 00:11:00,720
from the partial tiles so the main codes

00:11:00,160 --> 00:11:04,320
for

00:11:00,720 --> 00:11:07,440
the the hot code uh can stay

00:11:04,320 --> 00:11:08,160
out can be more optimal in that they are

00:11:07,440 --> 00:11:11,120
not

00:11:08,160 --> 00:11:13,040
really irregularities and that you need

00:11:11,120 --> 00:11:15,360
to check whether it's a complete tile or

00:11:13,040 --> 00:11:18,959
whether an iteration will execute

00:11:15,360 --> 00:11:21,760
at all but it's uh it's a regular tile

00:11:18,959 --> 00:11:23,040
of always two by two which is beneficial

00:11:21,760 --> 00:11:25,120
for instance for

00:11:23,040 --> 00:11:26,399
for vectorization if you already know

00:11:25,120 --> 00:11:29,200
that the tile size

00:11:26,399 --> 00:11:30,079
is multiple of the vector with for

00:11:29,200 --> 00:11:34,320
instance

00:11:30,079 --> 00:11:37,680
and it can execute the the remainder the

00:11:34,320 --> 00:11:38,720
in an epilogue after the loop which if

00:11:37,680 --> 00:11:41,440
you organize the

00:11:38,720 --> 00:11:42,399
code correctly may not execute anything

00:11:41,440 --> 00:11:45,760
at all

00:11:42,399 --> 00:11:49,200
because uh you may choose to

00:11:45,760 --> 00:11:51,200
have your data sizes be a multiple of

00:11:49,200 --> 00:11:53,600
your tile sizes in that case

00:11:51,200 --> 00:11:54,240
uh the special codes is nothing to

00:11:53,600 --> 00:11:56,720
execute

00:11:54,240 --> 00:11:57,760
and you do not clutter your hot code

00:11:56,720 --> 00:12:01,200
that you want to run

00:11:57,760 --> 00:12:04,800
fast using conditions for

00:12:01,200 --> 00:12:08,240
just in case so what it does it help i

00:12:04,800 --> 00:12:10,880
i have here an example of a stencil

00:12:08,240 --> 00:12:11,839
for the heat equation in three

00:12:10,880 --> 00:12:14,240
dimensions

00:12:11,839 --> 00:12:14,880
on my computer that i'm currently using

00:12:14,240 --> 00:12:18,160
to

00:12:14,880 --> 00:12:21,040
uh for presenting this executes in

00:12:18,160 --> 00:12:24,320
about 20 seconds and if i do add some

00:12:21,040 --> 00:12:25,120
tile sizes i can get it down to 14.2

00:12:24,320 --> 00:12:28,240
seconds

00:12:25,120 --> 00:12:29,000
the style sizes in this case is 16 by 1

00:12:28,240 --> 00:12:32,639
by

00:12:29,000 --> 00:12:36,079
1024 this is just something that i tried

00:12:32,639 --> 00:12:38,160
out a bit what's the fastest one

00:12:36,079 --> 00:12:40,240
you they're probably tile styles that

00:12:38,160 --> 00:12:41,360
make it even faster than that one but

00:12:40,240 --> 00:12:43,519
this was always

00:12:41,360 --> 00:12:44,480
just a low-hanging food experiment with

00:12:43,519 --> 00:12:47,040
tile sizes

00:12:44,480 --> 00:12:48,000
and get the code which executes the

00:12:47,040 --> 00:12:50,399
fastest

00:12:48,000 --> 00:12:53,200
so in general what is useful for as i

00:12:50,399 --> 00:12:55,120
shown before it's good for if you

00:12:53,200 --> 00:12:56,639
uh want to localize your working set

00:12:55,120 --> 00:13:00,000
optimize for a

00:12:56,639 --> 00:13:02,880
specific cache for kernels that

00:13:00,000 --> 00:13:05,040
reuse some data multiple times which is

00:13:02,880 --> 00:13:06,000
always the case for stencils so stem

00:13:05,040 --> 00:13:08,959
cells almost

00:13:06,000 --> 00:13:10,639
always benefit from tiling but also

00:13:08,959 --> 00:13:13,519
linear algebra code

00:13:10,639 --> 00:13:15,440
in in plus libraries such as the matrix

00:13:13,519 --> 00:13:17,200
matrix multiplications

00:13:15,440 --> 00:13:19,279
other uses are if you want to split your

00:13:17,200 --> 00:13:22,240
workloads into chunks such they have

00:13:19,279 --> 00:13:23,920
a consequence parallelism or you want to

00:13:22,240 --> 00:13:25,120
use it as a preparation for other

00:13:23,920 --> 00:13:27,680
transformations

00:13:25,120 --> 00:13:29,760
such as even in the specification we

00:13:27,680 --> 00:13:33,120
defined

00:13:29,760 --> 00:13:35,839
partial unrolling by first titling

00:13:33,120 --> 00:13:37,440
by the unworld factor and then full

00:13:35,839 --> 00:13:40,160
unrolling of the

00:13:37,440 --> 00:13:40,639
inner loop so this is an also an example

00:13:40,160 --> 00:13:43,440
for

00:13:40,639 --> 00:13:45,040
a transformation composition of

00:13:43,440 --> 00:13:48,320
transformations

00:13:45,040 --> 00:13:50,720
so we can

00:13:48,320 --> 00:13:52,240
can combine tiling with other loop

00:13:50,720 --> 00:13:54,880
associated construct

00:13:52,240 --> 00:13:55,360
such as the task loop you can generally

00:13:54,880 --> 00:13:57,600
say

00:13:55,360 --> 00:13:58,560
a loop associated construct always

00:13:57,600 --> 00:14:01,199
applies

00:13:58,560 --> 00:14:02,000
to what's on the next line so this task

00:14:01,199 --> 00:14:04,959
loop directive

00:14:02,000 --> 00:14:06,800
applies on the loop that's follows there

00:14:04,959 --> 00:14:09,360
the simple for loop in

00:14:06,800 --> 00:14:11,519
in case of c set plus plus but we can

00:14:09,360 --> 00:14:13,199
also apply it on the output of the loop

00:14:11,519 --> 00:14:13,519
transformation construct in this case

00:14:13,199 --> 00:14:16,560
the

00:14:13,519 --> 00:14:19,199
the tile this has the same effect as if

00:14:16,560 --> 00:14:23,199
you would replace the tile

00:14:19,199 --> 00:14:23,839
first by uh by what tile would replace a

00:14:23,199 --> 00:14:26,639
loop

00:14:23,839 --> 00:14:28,720
and then apply the task loop on the

00:14:26,639 --> 00:14:31,519
outermost loop like in this case

00:14:28,720 --> 00:14:32,079
effectively this has the same effect as

00:14:31,519 --> 00:14:34,320
a grain

00:14:32,079 --> 00:14:35,839
science clause that task loop already

00:14:34,320 --> 00:14:39,839
has

00:14:35,839 --> 00:14:42,560
but we allow this construction allows it

00:14:39,839 --> 00:14:45,839
to apply in a much more orthogonal way

00:14:42,560 --> 00:14:46,800
uh with every loop construct even for

00:14:45,839 --> 00:14:50,160
the

00:14:46,800 --> 00:14:52,240
loop literal loop construct

00:14:50,160 --> 00:14:54,000
even if those do not have specific

00:14:52,240 --> 00:14:56,839
clauses to support this

00:14:54,000 --> 00:14:58,079
kind of coursing we can even apply a

00:14:56,839 --> 00:15:00,480
tile

00:14:58,079 --> 00:15:01,360
tiling on itself so we have multi-level

00:15:00,480 --> 00:15:04,160
tiling

00:15:01,360 --> 00:15:06,320
uh one tiling that optimizes for

00:15:04,160 --> 00:15:06,880
instance for their accessing the l1

00:15:06,320 --> 00:15:09,519
cache

00:15:06,880 --> 00:15:10,240
and another tile length that optimizes

00:15:09,519 --> 00:15:13,680
for

00:15:10,240 --> 00:15:16,720
let's say the l2 or l3 cache

00:15:13,680 --> 00:15:18,480
and you'll see writing the tiling tile

00:15:16,720 --> 00:15:21,519
crop mask can

00:15:18,480 --> 00:15:22,240
be a lot more compact and easier to

00:15:21,519 --> 00:15:25,760
maintain

00:15:22,240 --> 00:15:28,079
than writing out the tiling manually

00:15:25,760 --> 00:15:30,240
we can also do this composition within

00:15:28,079 --> 00:15:34,320
the unwanting we first do the

00:15:30,240 --> 00:15:37,440
unrolling of a loop and apply

00:15:34,320 --> 00:15:40,800
a work sharing loop onto it afterwards

00:15:37,440 --> 00:15:43,839
so we get basically a chunk size but

00:15:40,800 --> 00:15:47,360
each chunk is guaranteed to be on to be

00:15:43,839 --> 00:15:49,199
unrolled using the unwell construct

00:15:47,360 --> 00:15:51,680
the unwork construct actually only

00:15:49,199 --> 00:15:53,199
allows a follow-up transformation if you

00:15:51,680 --> 00:15:55,519
use partial unrolling

00:15:53,199 --> 00:15:57,360
if it's possible that we do full

00:15:55,519 --> 00:15:59,279
unrolling

00:15:57,360 --> 00:16:00,639
there's no guarantee that there actually

00:15:59,279 --> 00:16:03,160
actually is a loop and we do not

00:16:00,639 --> 00:16:05,839
synthetically allow a follow-up

00:16:03,160 --> 00:16:08,880
transmission onto it

00:16:05,839 --> 00:16:11,440
so this is very nice example on how to

00:16:08,880 --> 00:16:12,560
split the optimization of some code from

00:16:11,440 --> 00:16:15,199
its semantics

00:16:12,560 --> 00:16:16,720
we define what the code is doing using

00:16:15,199 --> 00:16:19,839
the host language

00:16:16,720 --> 00:16:22,720
in case of openmp it's fortran c or

00:16:19,839 --> 00:16:24,160
c plus plus but we do the optimizations

00:16:22,720 --> 00:16:27,519
in open a

00:16:24,160 --> 00:16:29,519
open ap by adding traumas for instance

00:16:27,519 --> 00:16:31,759
we can also

00:16:29,519 --> 00:16:34,320
choose different optimization depending

00:16:31,759 --> 00:16:36,560
on what

00:16:34,320 --> 00:16:37,440
platform we are optimizing for so in

00:16:36,560 --> 00:16:40,800
this one we

00:16:37,440 --> 00:16:44,079
can configure a software to either use

00:16:40,800 --> 00:16:46,079
target offloading or use the p-fit-based

00:16:44,079 --> 00:16:50,240
implementation

00:16:46,079 --> 00:16:52,720
for cpus and choose different pro

00:16:50,240 --> 00:16:55,519
masks using the preprocessor depending

00:16:52,720 --> 00:16:59,519
whether enable or floating is enabled

00:16:55,519 --> 00:17:01,279
is set or not with openmp 5.1 we also

00:16:59,519 --> 00:17:02,160
have the possibility to use a meta

00:17:01,279 --> 00:17:05,280
directive

00:17:02,160 --> 00:17:07,520
which has inbuilt selectors for uh

00:17:05,280 --> 00:17:09,600
kinds of platforms one instance whether

00:17:07,520 --> 00:17:12,319
that's a gpu a cpu

00:17:09,600 --> 00:17:14,880
what's the vendor and what's the model

00:17:12,319 --> 00:17:17,520
of your accelerator to use different

00:17:14,880 --> 00:17:19,439
for instance tile sizes on this case

00:17:17,520 --> 00:17:22,559
choose between either use uh

00:17:19,439 --> 00:17:26,240
tiling if you are compiling for a cpu or

00:17:22,559 --> 00:17:28,720
use unrolling when compiling for a gpu

00:17:26,240 --> 00:17:29,440
i think let's go to a conclusion so

00:17:28,720 --> 00:17:32,559
what's

00:17:29,440 --> 00:17:35,200
the the intended audience of this is not

00:17:32,559 --> 00:17:37,600
the most general programmer but uh

00:17:35,200 --> 00:17:40,480
specialists which know the hardware

00:17:37,600 --> 00:17:41,520
who have an idea what's what the code

00:17:40,480 --> 00:17:44,720
should look like

00:17:41,520 --> 00:17:46,400
that most uh that executes the fastest

00:17:44,720 --> 00:17:49,520
on a specific

00:17:46,400 --> 00:17:51,760
hardware in in this case the the

00:17:49,520 --> 00:17:52,960
workflow that i would imagine it would

00:17:51,760 --> 00:17:55,679
have is that first

00:17:52,960 --> 00:17:56,160
the the application is written maybe by

00:17:55,679 --> 00:17:59,200
a

00:17:56,160 --> 00:18:01,600
domain specialist but once it's done

00:17:59,200 --> 00:18:03,039
it's being profiled so using different

00:18:01,600 --> 00:18:06,640
tools tracers

00:18:03,039 --> 00:18:08,799
that may also give you an idea what the

00:18:06,640 --> 00:18:11,200
bottleneck is where the most time

00:18:08,799 --> 00:18:12,320
uh spent whether your application is

00:18:11,200 --> 00:18:14,559
bandwidth bound

00:18:12,320 --> 00:18:15,760
or whether it's compute bound and then

00:18:14,559 --> 00:18:18,720
you can apply

00:18:15,760 --> 00:18:19,120
uh start applying low hanging foods like

00:18:18,720 --> 00:18:22,160
the

00:18:19,120 --> 00:18:24,559
easiest thing if you have uh

00:18:22,160 --> 00:18:26,160
for a kernel a window optimized library

00:18:24,559 --> 00:18:29,360
then use that one

00:18:26,160 --> 00:18:32,400
or add some clauses like a schedules

00:18:29,360 --> 00:18:33,919
clause and specify a chunk size which is

00:18:32,400 --> 00:18:36,960
best for the hardware

00:18:33,919 --> 00:18:37,679
or what is now possible with openmp 5.1

00:18:36,960 --> 00:18:40,080
as well

00:18:37,679 --> 00:18:41,760
to use the unroll and tile directives to

00:18:40,080 --> 00:18:42,640
get some more optimization of your

00:18:41,760 --> 00:18:44,480
hardware

00:18:42,640 --> 00:18:46,160
this is not as a repliment as a

00:18:44,480 --> 00:18:47,120
replacement for really extreme

00:18:46,160 --> 00:18:50,080
optimization

00:18:47,120 --> 00:18:50,400
if you want to go down to the middle and

00:18:50,080 --> 00:18:53,120
uh

00:18:50,400 --> 00:18:54,160
right assembly code for the hardware you

00:18:53,120 --> 00:18:57,520
if you want to go

00:18:54,160 --> 00:19:00,640
that way that's a lot of work to do

00:18:57,520 --> 00:19:01,679
but this doesn't say the new openmp

00:19:00,640 --> 00:19:03,679
construct don't

00:19:01,679 --> 00:19:05,120
save you from that one but it lowers

00:19:03,679 --> 00:19:07,360
this food from

00:19:05,120 --> 00:19:08,799
just applying unrolling and tiling

00:19:07,360 --> 00:19:10,640
experimenting with it

00:19:08,799 --> 00:19:12,480
try out different and roll factors and

00:19:10,640 --> 00:19:15,360
so on to get the best

00:19:12,480 --> 00:19:16,559
performance out of your coach using

00:19:15,360 --> 00:19:19,679
relatively

00:19:16,559 --> 00:19:22,960
uh easy easy means

00:19:19,679 --> 00:19:26,080
so the this is not supported by any

00:19:22,960 --> 00:19:28,160
compiler for now i'm working myself from

00:19:26,080 --> 00:19:29,919
an implementation in playing which is

00:19:28,160 --> 00:19:31,760
under review at the moment

00:19:29,919 --> 00:19:33,840
and since a lot of other compilers are

00:19:31,760 --> 00:19:36,840
based on clang i hope this is going to

00:19:33,840 --> 00:19:40,000
be inherited by other compilers as well

00:19:36,840 --> 00:19:41,039
um after the tiling directive i'll also

00:19:40,000 --> 00:19:44,480
implement the

00:19:41,039 --> 00:19:46,000
the unroll directive um but for openmp

00:19:44,480 --> 00:19:49,120
6.0 i

00:19:46,000 --> 00:19:50,480
imagine we have could have much more for

00:19:49,120 --> 00:19:52,799
instance most

00:19:50,480 --> 00:19:54,240
obviously more transformations such as

00:19:52,799 --> 00:19:57,120
loop interchange

00:19:54,240 --> 00:19:59,840
loop vision and fusion or even something

00:19:57,120 --> 00:20:02,080
as sophisticated as space filling curves

00:19:59,840 --> 00:20:03,679
we may have auxiliary transformations

00:20:02,080 --> 00:20:06,880
which are not useful

00:20:03,679 --> 00:20:09,280
by itself but may enable applying other

00:20:06,880 --> 00:20:12,720
transformations such as in

00:20:09,280 --> 00:20:14,799
interchange that is only really defined

00:20:12,720 --> 00:20:16,640
on perfectly nested loops

00:20:14,799 --> 00:20:18,080
we can add more clauses for instance

00:20:16,640 --> 00:20:20,799
clauses that control

00:20:18,080 --> 00:20:22,000
how the remainder loops for the tile and

00:20:20,799 --> 00:20:24,960
unroll directive

00:20:22,000 --> 00:20:25,679
are generated and what i think is a

00:20:24,960 --> 00:20:28,720
large

00:20:25,679 --> 00:20:32,000
thing which is very important to

00:20:28,720 --> 00:20:36,000
make it useful is to apply

00:20:32,000 --> 00:20:38,240
follow-up follow-up transformations

00:20:36,000 --> 00:20:39,840
not only on the outermost loop but also

00:20:38,240 --> 00:20:41,679
inner generator loops

00:20:39,840 --> 00:20:43,360
i have an example here on the top right

00:20:41,679 --> 00:20:46,480
where you apply tiling

00:20:43,360 --> 00:20:47,360
and after tying you apply a virtual loop

00:20:46,480 --> 00:20:49,840
on the

00:20:47,360 --> 00:20:51,520
autumn outer generator loop but

00:20:49,840 --> 00:20:54,799
vectorization on the

00:20:51,520 --> 00:20:56,240
inner generator loop finally we can also

00:20:54,799 --> 00:20:58,159
add something like semantics and

00:20:56,240 --> 00:21:01,360
optimization hints that just

00:20:58,159 --> 00:21:03,440
do not that are not

00:21:01,360 --> 00:21:05,280
prescriptive in telling that compiler

00:21:03,440 --> 00:21:07,520
what to do but give it

00:21:05,280 --> 00:21:09,440
hints about the semantics of a program

00:21:07,520 --> 00:21:11,520
for instance like the iv depth

00:21:09,440 --> 00:21:13,919
whether code is vectorizable whether

00:21:11,520 --> 00:21:16,559
that's parallelizable or interchangeable

00:21:13,919 --> 00:21:17,679
or just instead of assumption just

00:21:16,559 --> 00:21:19,679
expectations

00:21:17,679 --> 00:21:20,960
that the compiler can use to optimize

00:21:19,679 --> 00:21:23,200
the performance

00:21:20,960 --> 00:21:24,240
for for instance it's not necessary to

00:21:23,200 --> 00:21:27,440
optimize

00:21:24,240 --> 00:21:30,320
invest time to optimize code code

00:21:27,440 --> 00:21:32,159
thank you very much this is my talk i

00:21:30,320 --> 00:21:33,919
hope you find the loop transformations

00:21:32,159 --> 00:21:36,320
as useful as i do

00:21:33,919 --> 00:21:37,760
and we may see each other around at this

00:21:36,320 --> 00:21:42,200
year's virtual

00:21:37,760 --> 00:21:45,200
super computing conference have a good

00:21:42,200 --> 00:21:45,200

YouTube URL: https://www.youtube.com/watch?v=OIXMgkexpeA


