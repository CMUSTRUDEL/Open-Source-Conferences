Title: BOLT: A Lightweight and Highly Interoperable OpenMP Runtime
Publication date: 2020-09-15
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Shintaro Iwasaki of Argonne National Laboratory, is part of the OpenMP Booth Talk series created for Supercomputing 2020. 

A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20
Captions: 
	00:00:02,410 --> 00:00:06,560
[Music]

00:00:04,560 --> 00:00:07,680
uh thank you for joining this sc20

00:00:06,560 --> 00:00:10,080
opentv booth talk

00:00:07,680 --> 00:00:12,000
hello i'm shintaro rasaki a post doctor

00:00:10,080 --> 00:00:13,599
appointee belonging to our programming

00:00:12,000 --> 00:00:16,560
model and runtime system team

00:00:13,599 --> 00:00:17,359
led by pavan balaji at national lab this

00:00:16,560 --> 00:00:19,920
talk is about

00:00:17,359 --> 00:00:21,039
both a low overhead and highly

00:00:19,920 --> 00:00:22,880
interferable op

00:00:21,039 --> 00:00:24,320
runtime system that uses lightweight

00:00:22,880 --> 00:00:26,000
threads first

00:00:24,320 --> 00:00:27,920
i'd like to talk about the background of

00:00:26,000 --> 00:00:29,679
this old quantum system

00:00:27,920 --> 00:00:31,439
as you know it's hard to exploit

00:00:29,679 --> 00:00:33,840
terrorism in both hardware

00:00:31,439 --> 00:00:35,760
and software in terms of however there

00:00:33,840 --> 00:00:37,040
are many parallel compute resources at

00:00:35,760 --> 00:00:39,760
different levels from

00:00:37,040 --> 00:00:41,200
7d instructions to multiple cpus and

00:00:39,760 --> 00:00:42,960
multiple compute nodes

00:00:41,200 --> 00:00:44,879
maybe with some accelerators such as

00:00:42,960 --> 00:00:47,200
gpus and fpgas

00:00:44,879 --> 00:00:48,079
their powerism is increasing sim delay

00:00:47,200 --> 00:00:50,320
is increasing

00:00:48,079 --> 00:00:52,239
powerism in gpu is increasing and number

00:00:50,320 --> 00:00:53,280
of cores is also increasing at showing

00:00:52,239 --> 00:00:55,760
figure

00:00:53,280 --> 00:00:57,680
in terms of software parallel algorithms

00:00:55,760 --> 00:00:59,440
in applications can be complex

00:00:57,680 --> 00:01:01,760
we often combine multiple power

00:00:59,440 --> 00:01:02,559
components sometimes within a program

00:01:01,760 --> 00:01:04,559
but sometimes

00:01:02,559 --> 00:01:07,280
outside the program by linking a program

00:01:04,559 --> 00:01:09,680
to our external power libraries

00:01:07,280 --> 00:01:11,680
in a single application we use multiple

00:01:09,680 --> 00:01:14,880
runtime systems such as open b

00:01:11,680 --> 00:01:16,640
mpi and many other frameworks so

00:01:14,880 --> 00:01:18,240
let's take a look at this situation from

00:01:16,640 --> 00:01:20,560
the openmp perspective

00:01:18,240 --> 00:01:22,720
the current openp implementation often

00:01:20,560 --> 00:01:24,240
poorly utilizes hardware and software

00:01:22,720 --> 00:01:26,880
algorithms in openp

00:01:24,240 --> 00:01:29,280
plus x and we are focusing on the

00:01:26,880 --> 00:01:32,400
multi-spreading and multitasking part

00:01:29,280 --> 00:01:35,280
one example is of np plus open d

00:01:32,400 --> 00:01:37,520
we know that on p plus on p power four

00:01:35,280 --> 00:01:38,000
or on the parallel region performs very

00:01:37,520 --> 00:01:40,880
well

00:01:38,000 --> 00:01:42,479
if it appears alone however if on b

00:01:40,880 --> 00:01:45,680
pirate 4 is nasty

00:01:42,479 --> 00:01:47,759
basically it goes to either two outcomes

00:01:45,680 --> 00:01:49,200
one is creating friends again and

00:01:47,759 --> 00:01:50,479
lowering the performance by over

00:01:49,200 --> 00:01:52,159
subscription

00:01:50,479 --> 00:01:55,200
and the other is disabling the next

00:01:52,159 --> 00:01:56,880
parism and suffers from lack of powers

00:01:55,200 --> 00:01:58,880
since the contemporary software

00:01:56,880 --> 00:02:01,200
components are highly stacked

00:01:58,880 --> 00:02:02,799
they can be inevitably nested so this

00:02:01,200 --> 00:02:05,680
open b plus open b

00:02:02,799 --> 00:02:07,040
is getting more problematic on

00:02:05,680 --> 00:02:11,200
supercomputers

00:02:07,040 --> 00:02:14,720
many people combine open p plus mpi

00:02:11,200 --> 00:02:16,959
however npi thread is not very scalable

00:02:14,720 --> 00:02:18,560
because of internal load contention in

00:02:16,959 --> 00:02:20,959
mpi random system

00:02:18,560 --> 00:02:22,480
also there is mpi plus on p task

00:02:20,959 --> 00:02:25,440
interval ability issue

00:02:22,480 --> 00:02:26,160
npi is not aware of the semantics of onp

00:02:25,440 --> 00:02:28,319
tasks

00:02:26,160 --> 00:02:30,879
so it can easily cause or deadlock if

00:02:28,319 --> 00:02:34,080
omp tasks try to synchronize something

00:02:30,879 --> 00:02:36,080
via mpi operations there should be other

00:02:34,080 --> 00:02:37,280
interpretabilities as well but let's

00:02:36,080 --> 00:02:39,599
focus on those two

00:02:37,280 --> 00:02:40,560
since they commonly appear in hpc

00:02:39,599 --> 00:02:42,720
workloads

00:02:40,560 --> 00:02:44,239
to address this interval ability issue

00:02:42,720 --> 00:02:46,720
above nv plus x

00:02:44,239 --> 00:02:48,879
we are developing a new on p runtime

00:02:46,720 --> 00:02:50,959
systems called both

00:02:48,879 --> 00:02:52,400
bolt is an openmp runtime over

00:02:50,959 --> 00:02:55,200
lightweight trading library

00:02:52,400 --> 00:02:57,120
ergobots vault is part of the ecp salt

00:02:55,200 --> 00:02:59,360
project which is a project that enhances

00:02:57,120 --> 00:03:01,280
the openmp performance and functionality

00:02:59,360 --> 00:03:03,519
for the exoskill machines

00:03:01,280 --> 00:03:04,640
the ecp saw project consists of several

00:03:03,519 --> 00:03:06,400
open v components

00:03:04,640 --> 00:03:08,560
including scheduling compiler

00:03:06,400 --> 00:03:12,000
optimizations testing and training

00:03:08,560 --> 00:03:14,000
and in ecp sol bolt is being developed

00:03:12,000 --> 00:03:15,280
to address challenges from the runtime

00:03:14,000 --> 00:03:17,840
perspective

00:03:15,280 --> 00:03:19,599
in terms of implementation bolt is based

00:03:17,840 --> 00:03:21,840
on llvm open p10

00:03:19,599 --> 00:03:23,360
and most functionality is the same as

00:03:21,840 --> 00:03:26,000
llvm of mp10

00:03:23,360 --> 00:03:28,640
for example both supports most of their

00:03:26,000 --> 00:03:30,799
openp 4.5 and 5.0 features

00:03:28,640 --> 00:03:31,680
including passing target offloading and

00:03:30,799 --> 00:03:33,680
so on

00:03:31,680 --> 00:03:36,640
however the open piece right mapping is

00:03:33,680 --> 00:03:37,200
different most commonly used open random

00:03:36,640 --> 00:03:40,720
systems

00:03:37,200 --> 00:03:41,519
including lvm openp gcc and like intel

00:03:40,720 --> 00:03:43,599
openp

00:03:41,519 --> 00:03:45,519
use p threads for all these threads

00:03:43,599 --> 00:03:46,799
while both maps point these threads to

00:03:45,519 --> 00:03:48,080
user level threads

00:03:46,799 --> 00:03:49,920
since aren't able to use the level

00:03:48,080 --> 00:03:52,239
threads or schedule by piece red

00:03:49,920 --> 00:03:53,840
we can create many usable threads while

00:03:52,239 --> 00:03:55,599
keeping the number of piece threads

00:03:53,840 --> 00:03:57,680
we can also finally control the

00:03:55,599 --> 00:04:00,000
scattering of all these threads and pass

00:03:57,680 --> 00:04:02,080
via the algobot scheduling layer so it

00:04:00,000 --> 00:04:04,239
can efficiently manage spread resources

00:04:02,080 --> 00:04:06,319
across multiple software layers

00:04:04,239 --> 00:04:08,159
actually it can be more than 10 times

00:04:06,319 --> 00:04:10,080
efficient than existing on b random

00:04:08,159 --> 00:04:11,920
systems in terms of what is rating and

00:04:10,080 --> 00:04:14,480
successfully improves the performance

00:04:11,920 --> 00:04:16,320
of necessary applications let me talk

00:04:14,480 --> 00:04:17,840
about the design of both but first i'd

00:04:16,320 --> 00:04:19,600
like to briefly talk about the

00:04:17,840 --> 00:04:23,759
underlying threading framework

00:04:19,600 --> 00:04:25,919
autobots okay so autobots is a highly

00:04:23,759 --> 00:04:28,560
optimized lightweight trading library

00:04:25,919 --> 00:04:30,560
the library is mainly developed by argon

00:04:28,560 --> 00:04:32,479
but there are many collaborators and now

00:04:30,560 --> 00:04:34,160
part of the ecb salt project

00:04:32,479 --> 00:04:36,320
to reduce the overheads of threading

00:04:34,160 --> 00:04:37,199
operations instead of traditional os

00:04:36,320 --> 00:04:39,199
level threads

00:04:37,199 --> 00:04:41,759
argobots uses a user-level spreading

00:04:39,199 --> 00:04:45,440
technique for spreading implementation

00:04:41,759 --> 00:04:48,160
i'd like to show why argobox is fast

00:04:45,440 --> 00:04:49,199
the argument thread is 300 times faster

00:04:48,160 --> 00:04:52,080
than big threats

00:04:49,199 --> 00:04:54,080
the top figures roughly describe why

00:04:52,080 --> 00:04:55,120
argobod is faster than typical piecework

00:04:54,080 --> 00:04:57,040
implementation

00:04:55,120 --> 00:04:58,720
the left figure shows the case of voice

00:04:57,040 --> 00:05:00,560
levels rights or p threats

00:04:58,720 --> 00:05:02,880
trading operations of oil level threats

00:05:00,560 --> 00:05:03,919
are heavy weight because if we want to

00:05:02,880 --> 00:05:06,400
create and join p

00:05:03,919 --> 00:05:08,240
threat or perform a reoperation or

00:05:06,400 --> 00:05:10,720
context switch to another b spread

00:05:08,240 --> 00:05:13,280
the os level threads always access this

00:05:10,720 --> 00:05:14,800
kernel involving heavy system goals

00:05:13,280 --> 00:05:17,039
however in the right figure in their

00:05:14,800 --> 00:05:18,639
arcbot's case all the operations

00:05:17,039 --> 00:05:20,639
including threat forecan join

00:05:18,639 --> 00:05:22,560
synchronization and context switching

00:05:20,639 --> 00:05:24,320
are done in user space

00:05:22,560 --> 00:05:26,320
since everything is managed in user

00:05:24,320 --> 00:05:26,880
space we call these threads user level

00:05:26,320 --> 00:05:30,240
threads

00:05:26,880 --> 00:05:32,639
or ults they are ultimately running on p

00:05:30,240 --> 00:05:34,479
threads but no system call is required

00:05:32,639 --> 00:05:36,639
to perform threading operations

00:05:34,479 --> 00:05:37,680
these ults can be fully controlled in

00:05:36,639 --> 00:05:40,720
the user space

00:05:37,680 --> 00:05:42,080
so we can freely schedule ults efficient

00:05:40,720 --> 00:05:45,840
scheduling can further

00:05:42,080 --> 00:05:47,280
optimize the problem our quality itself

00:05:45,840 --> 00:05:49,280
is a big project and several

00:05:47,280 --> 00:05:51,199
applications and runtime systems use

00:05:49,280 --> 00:05:53,840
argobots as a low-level and highly

00:05:51,199 --> 00:05:56,080
customizable lightweight implementation

00:05:53,840 --> 00:05:57,199
i'll briefly explain two prominent

00:05:56,080 --> 00:06:00,960
projects that use

00:05:57,199 --> 00:06:02,720
argobots okay intel deos is an intel's

00:06:00,960 --> 00:06:05,520
production object storage for

00:06:02,720 --> 00:06:08,000
intel optane and other storage devices

00:06:05,520 --> 00:06:10,240
this will be a primary i o back-end

00:06:08,000 --> 00:06:12,000
of the aurora super computer it should

00:06:10,240 --> 00:06:12,960
be the first exascale supercomputer in

00:06:12,000 --> 00:06:16,080
the u.s

00:06:12,960 --> 00:06:18,319
argobox is used in the intel deo server

00:06:16,080 --> 00:06:20,639
onenio request is translated into a

00:06:18,319 --> 00:06:23,120
lightweight user level thread of argobox

00:06:20,639 --> 00:06:23,840
and the custom scheduler schedule is

00:06:23,120 --> 00:06:26,240
schedules

00:06:23,840 --> 00:06:27,919
io requests based on their types of

00:06:26,240 --> 00:06:30,479
requests

00:06:27,919 --> 00:06:31,680
the other example is margo marlo is a

00:06:30,479 --> 00:06:33,600
wrapper for the lightweight

00:06:31,680 --> 00:06:35,600
communication library called markley

00:06:33,600 --> 00:06:38,080
developed in the multi project and

00:06:35,600 --> 00:06:39,919
marvel uses archebots to handle rpc

00:06:38,080 --> 00:06:40,800
requests which are associated with user

00:06:39,919 --> 00:06:43,360
level threads

00:06:40,800 --> 00:06:45,600
and execution strings schedule them

00:06:43,360 --> 00:06:46,639
these two projects are using robots to

00:06:45,600 --> 00:06:48,560
efficiently manage

00:06:46,639 --> 00:06:49,919
i o and communication requests by

00:06:48,560 --> 00:06:52,160
associating them with

00:06:49,919 --> 00:06:54,160
lightweight usable threads and schedule

00:06:52,160 --> 00:06:55,440
them by user defined scheduler based on

00:06:54,160 --> 00:06:58,240
request priority

00:06:55,440 --> 00:06:59,680
and completion status of request then

00:06:58,240 --> 00:07:02,560
let's go back to the op mp

00:06:59,680 --> 00:07:04,160
and bolt bolt is one of the projects

00:07:02,560 --> 00:07:05,120
that successfully use lightweight

00:07:04,160 --> 00:07:07,680
arcbots

00:07:05,120 --> 00:07:08,479
it's an lvm openmp library based on

00:07:07,680 --> 00:07:10,479
argobot's

00:07:08,479 --> 00:07:12,400
lightweight threads by mapping points

00:07:10,479 --> 00:07:13,599
threads and tasks to arc about the user

00:07:12,400 --> 00:07:16,800
level threads

00:07:13,599 --> 00:07:19,280
how that will work this is very simple

00:07:16,800 --> 00:07:21,120
so open b part four actually this power

00:07:19,280 --> 00:07:24,080
part creates all these threads

00:07:21,120 --> 00:07:25,440
and major on the runtimes uh creates p

00:07:24,080 --> 00:07:27,680
threads for onp threads

00:07:25,440 --> 00:07:28,880
but both creates user level threads for

00:07:27,680 --> 00:07:33,199
open piece threads

00:07:28,880 --> 00:07:34,960
like this so after that it's very easy

00:07:33,199 --> 00:07:37,280
so they are scheduled by random rock

00:07:34,960 --> 00:07:39,520
ceiling so this is a very basic

00:07:37,280 --> 00:07:41,440
implementation of bolt and let me

00:07:39,520 --> 00:07:42,319
explain how to use this lightweight user

00:07:41,440 --> 00:07:46,080
level spreads for

00:07:42,319 --> 00:07:46,400
open b plus open p okay i'd like to talk

00:07:46,080 --> 00:07:48,720
about

00:07:46,400 --> 00:07:50,080
open p plus open b or specifically

00:07:48,720 --> 00:07:54,560
message power regions

00:07:50,080 --> 00:07:58,080
this work got the best power at pac 19.

00:07:54,560 --> 00:08:00,560
now um many hpc applications

00:07:58,080 --> 00:08:01,520
runtimes and libraries are paralyzed by

00:08:00,560 --> 00:08:03,440
openp

00:08:01,520 --> 00:08:04,960
this situation unintentionally

00:08:03,440 --> 00:08:07,599
introduces nasi paris

00:08:04,960 --> 00:08:08,319
existing in multiple software stacks for

00:08:07,599 --> 00:08:10,400
example

00:08:08,319 --> 00:08:12,160
users of np power cold calls and

00:08:10,400 --> 00:08:13,599
external function which happens to be

00:08:12,160 --> 00:08:15,919
parallelized by open b

00:08:13,599 --> 00:08:18,080
again if on these threads are created

00:08:15,919 --> 00:08:19,120
every time the control encounters on the

00:08:18,080 --> 00:08:21,039
power 4

00:08:19,120 --> 00:08:22,319
the growth of on this red count becomes

00:08:21,039 --> 00:08:24,400
exponential

00:08:22,319 --> 00:08:27,199
let's assume a processor that has four

00:08:24,400 --> 00:08:29,199
chords for example the omb power 4

00:08:27,199 --> 00:08:30,639
creates a parallel region which spawns

00:08:29,199 --> 00:08:33,360
threads usually

00:08:30,639 --> 00:08:35,120
as many as number of cores so four

00:08:33,360 --> 00:08:36,719
threads are created in this case

00:08:35,120 --> 00:08:38,959
then each thread called the degen

00:08:36,719 --> 00:08:40,159
function then which internally creates

00:08:38,959 --> 00:08:41,839
parallel regions again

00:08:40,159 --> 00:08:43,599
then the total number of threads becomes

00:08:41,839 --> 00:08:46,880
16. if there's one

00:08:43,599 --> 00:08:49,040
power level in d jam then it can be 64.

00:08:46,880 --> 00:08:50,880
sorry so because the major open random

00:08:49,040 --> 00:08:52,480
system snap on this residual chrono

00:08:50,880 --> 00:08:54,000
threads it causes severe over

00:08:52,480 --> 00:08:57,680
subscription of threads and

00:08:54,000 --> 00:08:59,760
significantly lower the performance so

00:08:57,680 --> 00:09:01,680
and enabling this power regions can

00:08:59,760 --> 00:09:03,839
exponentially create threads and

00:09:01,680 --> 00:09:06,640
significantly degrees performance

00:09:03,839 --> 00:09:06,959
the widely adapted option is disabling

00:09:06,640 --> 00:09:09,120
this

00:09:06,959 --> 00:09:10,320
partisan and this is a default behavior

00:09:09,120 --> 00:09:12,399
of openp

00:09:10,320 --> 00:09:13,519
however such a loss of parallelism can

00:09:12,399 --> 00:09:15,680
hurt the performance

00:09:13,519 --> 00:09:17,600
a typical case is strong scaling on

00:09:15,680 --> 00:09:19,680
massively parallel machines

00:09:17,600 --> 00:09:21,839
as the number of cores for processor and

00:09:19,680 --> 00:09:23,120
also the number of account is increasing

00:09:21,839 --> 00:09:25,279
the outer parallelism

00:09:23,120 --> 00:09:27,120
might not be enough to assign work to

00:09:25,279 --> 00:09:28,720
all the cores

00:09:27,120 --> 00:09:30,240
the problem of nested power region has

00:09:28,720 --> 00:09:32,560
been known for 25 years

00:09:30,240 --> 00:09:33,760
so right now openmp disables nested

00:09:32,560 --> 00:09:36,240
paralysis by default

00:09:33,760 --> 00:09:37,040
and many programs after we are aware

00:09:36,240 --> 00:09:39,839
unaware

00:09:37,040 --> 00:09:41,600
of potential loss of partisan beyond

00:09:39,839 --> 00:09:43,200
this we can find two directions to

00:09:41,600 --> 00:09:45,760
alleviate this issue

00:09:43,200 --> 00:09:47,519
first the open pe specification has

00:09:45,760 --> 00:09:49,680
several workarounds for this issue

00:09:47,519 --> 00:09:51,360
and major objective runtime systems

00:09:49,680 --> 00:09:53,760
implement these workarounds

00:09:51,360 --> 00:09:55,519
however this is only very effective if

00:09:53,760 --> 00:09:56,880
users know and control

00:09:55,519 --> 00:09:58,640
all the parallelism in the program

00:09:56,880 --> 00:09:59,360
including all the software stacks in

00:09:58,640 --> 00:10:01,360
other words

00:09:59,360 --> 00:10:02,480
these workarounds are not very effective

00:10:01,360 --> 00:10:04,000
if users don't

00:10:02,480 --> 00:10:05,680
understand all the powerism in the

00:10:04,000 --> 00:10:07,680
program second

00:10:05,680 --> 00:10:10,079
one is using a lightweight use of

00:10:07,680 --> 00:10:12,560
threads instead of os level threads

00:10:10,079 --> 00:10:14,560
there have been several proposals but

00:10:12,560 --> 00:10:16,480
they don't perform very well if parallel

00:10:14,560 --> 00:10:18,800
regions are not nested or i can say

00:10:16,480 --> 00:10:20,640
if parallelism is flat which is also a

00:10:18,800 --> 00:10:23,600
very common use case of both

00:10:20,640 --> 00:10:24,880
of open b to make the matter worse they

00:10:23,600 --> 00:10:26,399
are often slower than the

00:10:24,880 --> 00:10:29,760
state-of-the-art intel

00:10:26,399 --> 00:10:30,240
or lvm of mp random systems and as a

00:10:29,760 --> 00:10:32,800
result

00:10:30,240 --> 00:10:34,880
people just disable nested powers but as

00:10:32,800 --> 00:10:36,560
the software stacks get complicated and

00:10:34,880 --> 00:10:37,279
multi-threading becomes more and more

00:10:36,560 --> 00:10:39,440
important

00:10:37,279 --> 00:10:41,519
the solution to utilize nasa fire

00:10:39,440 --> 00:10:44,399
regions is highly demanding

00:10:41,519 --> 00:10:45,200
the root cause of this problem is using

00:10:44,399 --> 00:10:46,959
heavyweight

00:10:45,200 --> 00:10:49,279
always level threads as opposing these

00:10:46,959 --> 00:10:50,800
threads so it is natural to use

00:10:49,279 --> 00:10:51,360
lightweight and use valuable stress for

00:10:50,800 --> 00:10:54,560
this pro

00:10:51,360 --> 00:10:56,560
this purpose uh that's what bolt does

00:10:54,560 --> 00:10:57,680
actually replacing piece red part in

00:10:56,560 --> 00:11:00,399
alvin open b

00:10:57,680 --> 00:11:01,040
by our box is a piece of cake in bold

00:11:00,399 --> 00:11:03,200
baseline

00:11:01,040 --> 00:11:05,600
we create p threads as many as in above

00:11:03,200 --> 00:11:07,360
cores each piece threads run a scheduler

00:11:05,600 --> 00:11:09,519
and the scheduler schedule usable

00:11:07,360 --> 00:11:11,200
threads associated with open piece frets

00:11:09,519 --> 00:11:12,800
and compare with the piece right version

00:11:11,200 --> 00:11:14,480
even if we create all these threads more

00:11:12,800 --> 00:11:15,920
than number of course the total number

00:11:14,480 --> 00:11:19,519
of keys for these the same

00:11:15,920 --> 00:11:21,680
so there's least over subscription cost

00:11:19,519 --> 00:11:24,720
many people saw the same thing and many

00:11:21,680 --> 00:11:27,440
papers propose ult based rpp systems

00:11:24,720 --> 00:11:28,000
they have many specifications or

00:11:27,440 --> 00:11:30,079
engineering

00:11:28,000 --> 00:11:32,000
rated issues but the fundamental issue

00:11:30,079 --> 00:11:34,560
of this proposal is that

00:11:32,000 --> 00:11:36,000
they are basically slow so let's see the

00:11:34,560 --> 00:11:38,800
performance

00:11:36,000 --> 00:11:41,120
and we run this nested power micro

00:11:38,800 --> 00:11:43,519
benchmark on 56 core skyline

00:11:41,120 --> 00:11:45,040
the threat count of outer power vision

00:11:43,519 --> 00:11:47,440
is changed while the threat count of the

00:11:45,040 --> 00:11:49,440
inappropriation is fixed to 28

00:11:47,440 --> 00:11:50,560
faster is better so lower is better a

00:11:49,440 --> 00:11:52,720
figure shows that

00:11:50,560 --> 00:11:54,800
this bold baseline is much faster than

00:11:52,720 --> 00:11:57,600
dcc openp

00:11:54,800 --> 00:11:59,600
however compared with existing ult based

00:11:57,600 --> 00:12:01,040
systems the performance of the bulk

00:11:59,600 --> 00:12:04,320
baseline is not good

00:12:01,040 --> 00:12:07,680
but the most surprising thing is that um

00:12:04,320 --> 00:12:08,320
ult based op systems including both

00:12:07,680 --> 00:12:11,680
baseline

00:12:08,320 --> 00:12:14,959
are slower than well-tuned intel

00:12:11,680 --> 00:12:15,440
lvm open piece for example compared with

00:12:14,959 --> 00:12:18,480
intel

00:12:15,440 --> 00:12:20,880
album of mps old baseline is about seven

00:12:18,480 --> 00:12:22,639
times slower even in the case of

00:12:20,880 --> 00:12:25,600
necessary regions

00:12:22,639 --> 00:12:27,600
we investigated these reasons and

00:12:25,600 --> 00:12:29,279
applied a few optimizations to this both

00:12:27,600 --> 00:12:31,200
baseline

00:12:29,279 --> 00:12:33,600
the first one is resource management

00:12:31,200 --> 00:12:36,720
optimizations in lvm open b

00:12:33,600 --> 00:12:38,160
we first found that in lvm open b most

00:12:36,720 --> 00:12:40,000
reading of resources like thread

00:12:38,160 --> 00:12:41,519
descriptors and team descriptors are

00:12:40,000 --> 00:12:43,360
protected by a single lock

00:12:41,519 --> 00:12:44,720
creating a large critical section on the

00:12:43,360 --> 00:12:46,959
function path

00:12:44,720 --> 00:12:48,880
so we first divide the large critical

00:12:46,959 --> 00:12:51,360
section by creating more fingering locks

00:12:48,880 --> 00:12:53,839
per object to alleviate this contention

00:12:51,360 --> 00:12:55,440
furthermore in open b a parallel region

00:12:53,839 --> 00:12:57,279
is always created as a team

00:12:55,440 --> 00:12:59,120
not multiple independent threats so

00:12:57,279 --> 00:13:01,760
using this team concept can

00:12:59,120 --> 00:13:03,600
improve resource management so this team

00:13:01,760 --> 00:13:04,240
concept makes the team level data

00:13:03,600 --> 00:13:06,880
caching

00:13:04,240 --> 00:13:10,160
possible it allows the runtime to

00:13:06,880 --> 00:13:11,760
efficiently access and reuse team data

00:13:10,160 --> 00:13:13,279
in addition we optimize the spread

00:13:11,760 --> 00:13:15,200
creation method previously

00:13:13,279 --> 00:13:17,360
the master thread accessed channel

00:13:15,200 --> 00:13:19,200
threads sequentially on focus join

00:13:17,360 --> 00:13:20,800
but divide and conquer spread creation

00:13:19,200 --> 00:13:21,680
is more efficient in terms of critical

00:13:20,800 --> 00:13:23,760
path

00:13:21,680 --> 00:13:25,519
we know that overall these overheads are

00:13:23,760 --> 00:13:26,000
negligible when using oil syllable

00:13:25,519 --> 00:13:28,079
threats

00:13:26,000 --> 00:13:29,839
but with lightweight ultis the

00:13:28,079 --> 00:13:31,200
bottlenecks become visible and

00:13:29,839 --> 00:13:33,120
significant

00:13:31,200 --> 00:13:35,120
let's see the performance so this is a

00:13:33,120 --> 00:13:36,079
necessity region benchmark that has no

00:13:35,120 --> 00:13:37,600
computation

00:13:36,079 --> 00:13:39,839
efficient resource management

00:13:37,600 --> 00:13:41,360
significantly reduces overheads when

00:13:39,839 --> 00:13:43,279
many ults are created

00:13:41,360 --> 00:13:44,800
while binary's recreation shortens the

00:13:43,279 --> 00:13:46,880
critical path when the parallelism is

00:13:44,800 --> 00:13:48,959
limited

00:13:46,880 --> 00:13:50,160
the previous optimizations are to fix

00:13:48,959 --> 00:13:52,720
kind of artifacts

00:13:50,160 --> 00:13:53,600
of llvm openmp runtime but some of the

00:13:52,720 --> 00:13:55,920
optimizations

00:13:53,600 --> 00:13:57,199
need extensions to the current openmp

00:13:55,920 --> 00:13:59,680
specification

00:13:57,199 --> 00:14:00,880
for example let's focus on affinity

00:13:59,680 --> 00:14:03,199
which is always normally

00:14:00,880 --> 00:14:05,040
uld based implementation so in general

00:14:03,199 --> 00:14:07,279
thread affinity or thread binding is

00:14:05,040 --> 00:14:09,440
useful to improve the locality

00:14:07,279 --> 00:14:10,320
i'd like to explain how open dfinity

00:14:09,440 --> 00:14:12,720
works

00:14:10,320 --> 00:14:15,040
okay so in open b place is a virtual

00:14:12,720 --> 00:14:16,480
location unit and can be specified by an

00:14:15,040 --> 00:14:19,199
environmental variable

00:14:16,480 --> 00:14:19,760
if we specify one p places equal zero

00:14:19,199 --> 00:14:22,639
one

00:14:19,760 --> 00:14:24,079
two three four five and six seven on an

00:14:22,639 --> 00:14:26,480
eight four cpu

00:14:24,079 --> 00:14:27,120
then core zero one becomes a place zero

00:14:26,480 --> 00:14:29,440
two three

00:14:27,120 --> 00:14:31,519
place one four five place two and the

00:14:29,440 --> 00:14:33,839
last two chords become play 3.

00:14:31,519 --> 00:14:34,560
when plot band is specified the thread

00:14:33,839 --> 00:14:36,560
is bound to

00:14:34,560 --> 00:14:37,920
one of the plays according to the

00:14:36,560 --> 00:14:40,079
procline pattern

00:14:37,920 --> 00:14:41,279
with p threads this affinity is set by

00:14:40,079 --> 00:14:44,959
applying

00:14:41,279 --> 00:14:45,519
cpu mask however ults do not have a cpu

00:14:44,959 --> 00:14:47,600
mask

00:14:45,519 --> 00:14:49,199
so bold baseline lacks a notion of

00:14:47,600 --> 00:14:51,440
affinity which is still standard

00:14:49,199 --> 00:14:53,279
compliant since the specification sets

00:14:51,440 --> 00:14:54,720
the effect of affinity setting is

00:14:53,279 --> 00:14:57,519
implementation defined

00:14:54,720 --> 00:14:58,000
but you know this affinity setting is

00:14:57,519 --> 00:15:00,320
very

00:14:58,000 --> 00:15:02,800
useful for both not only to improve

00:15:00,320 --> 00:15:03,760
locality but also to avoid conventions

00:15:02,800 --> 00:15:07,040
of threat pools

00:15:03,760 --> 00:15:08,880
by deterministic scheduling thanks to

00:15:07,040 --> 00:15:09,680
customizable scheduler provided by

00:15:08,880 --> 00:15:11,440
argobots

00:15:09,680 --> 00:15:12,720
all can implement place pools to

00:15:11,440 --> 00:15:15,760
implement affinity

00:15:12,720 --> 00:15:17,680
the idea is very simple so each

00:15:15,760 --> 00:15:19,600
scheduler becomes a virtual core

00:15:17,680 --> 00:15:20,720
or virtual processor and bound to a

00:15:19,600 --> 00:15:22,720
certain chord

00:15:20,720 --> 00:15:24,880
then place pools corresponding to the

00:15:22,720 --> 00:15:27,440
places are created like this

00:15:24,880 --> 00:15:29,839
omb threads running on top of uots can

00:15:27,440 --> 00:15:30,880
be bound to a scheduler associated with

00:15:29,839 --> 00:15:33,120
a certain place

00:15:30,880 --> 00:15:35,120
this is accessed this access control is

00:15:33,120 --> 00:15:38,160
done by a place pool

00:15:35,120 --> 00:15:40,160
so the ult in the right in the playable

00:15:38,160 --> 00:15:41,120
can be taken only by the associated

00:15:40,160 --> 00:15:44,959
schedulers

00:15:41,120 --> 00:15:46,560
for example this scheduler can take ults

00:15:44,959 --> 00:15:48,800
from this place pool since this

00:15:46,560 --> 00:15:51,519
scheduler belongs to this place

00:15:48,800 --> 00:15:52,639
however it cannot take uots from another

00:15:51,519 --> 00:15:54,800
placebook

00:15:52,639 --> 00:15:57,440
and if the pool is here the scheduler

00:15:54,800 --> 00:15:59,759
can pop a ult from such a pool

00:15:57,440 --> 00:16:00,560
it works well and it is standard

00:15:59,759 --> 00:16:02,720
compliant

00:16:00,560 --> 00:16:03,839
but from the performance perspective it

00:16:02,720 --> 00:16:06,839
is not optimal

00:16:03,839 --> 00:16:08,079
because of mp affinity is too

00:16:06,839 --> 00:16:11,440
deterministic

00:16:08,079 --> 00:16:14,639
in openmp once affinity is set in the

00:16:11,440 --> 00:16:16,800
current parallel regions the child power

00:16:14,639 --> 00:16:19,279
region also needs to follow the current

00:16:16,800 --> 00:16:21,279
place partitioning like this so it is

00:16:19,279 --> 00:16:23,199
too restrictive to utilize random work

00:16:21,279 --> 00:16:24,720
stealing for example in bulk it will be

00:16:23,199 --> 00:16:27,199
like this

00:16:24,720 --> 00:16:29,120
so threads in the inaudible star

00:16:27,199 --> 00:16:30,079
revision cannot be scheduled by other

00:16:29,120 --> 00:16:33,199
schedulers

00:16:30,079 --> 00:16:33,920
out of the associated place so one

00:16:33,199 --> 00:16:36,480
promising

00:16:33,920 --> 00:16:37,199
approach is deterministic works work

00:16:36,480 --> 00:16:39,519
sharing

00:16:37,199 --> 00:16:41,920
at the aftermath bar region to speed up

00:16:39,519 --> 00:16:43,519
work distribution but using random steel

00:16:41,920 --> 00:16:45,759
in their innermost star revision for

00:16:43,519 --> 00:16:47,120
load balancing accordingly

00:16:45,759 --> 00:16:50,000
it is not possible in the current

00:16:47,120 --> 00:16:52,399
pthread oriented open b specification

00:16:50,000 --> 00:16:53,519
so we suggest a new keyword unset to

00:16:52,399 --> 00:16:55,600
unset the affinity

00:16:53,519 --> 00:16:56,639
so that the child's rights can be freed

00:16:55,600 --> 00:16:59,120
from this current

00:16:56,639 --> 00:17:01,440
affinity setting like this and run by

00:16:59,120 --> 00:17:05,520
any schedulers

00:17:01,440 --> 00:17:08,160
let's see the performance so in a figure

00:17:05,520 --> 00:17:10,240
bind denotes omp rockbind if you only

00:17:08,160 --> 00:17:12,000
set spread the performance is sometimes

00:17:10,240 --> 00:17:13,760
faster but not better if the total

00:17:12,000 --> 00:17:14,319
number of threads becomes larger because

00:17:13,760 --> 00:17:16,720
of

00:17:14,319 --> 00:17:18,959
rigid authentic behavior if you set

00:17:16,720 --> 00:17:21,039
unset to inner power revision it can

00:17:18,959 --> 00:17:23,120
overall achieve better performance

00:17:21,039 --> 00:17:25,120
after these optimizations both can fully

00:17:23,120 --> 00:17:27,039
utilize lightweight threads

00:17:25,120 --> 00:17:28,960
let's see the performance so we have two

00:17:27,039 --> 00:17:30,080
micro benchmarks both of them require

00:17:28,960 --> 00:17:31,760
these are regions

00:17:30,080 --> 00:17:33,120
the left case has balance but

00:17:31,760 --> 00:17:35,919
insufficient partisan

00:17:33,120 --> 00:17:37,679
the inner loop only has 28 iterations

00:17:35,919 --> 00:17:38,799
and the outer loop only has any

00:17:37,679 --> 00:17:41,360
iterations where n

00:17:38,799 --> 00:17:41,919
is small while their server has 56

00:17:41,360 --> 00:17:44,160
scores

00:17:41,919 --> 00:17:45,679
so to use all fours we need to paralyze

00:17:44,160 --> 00:17:48,000
both power releases

00:17:45,679 --> 00:17:50,160
this is the performance so now as

00:17:48,000 --> 00:17:51,600
expected bolt always performs best since

00:17:50,160 --> 00:17:52,559
now we can fully use the light light

00:17:51,600 --> 00:17:54,720
with ults

00:17:52,559 --> 00:17:56,480
up to four times faster than intel and

00:17:54,720 --> 00:17:58,000
llm of mp

00:17:56,480 --> 00:17:59,520
the right one shows another type of

00:17:58,000 --> 00:18:02,559
micro benchmarks and result

00:17:59,520 --> 00:18:04,400
and but we don't have time to uh

00:18:02,559 --> 00:18:05,919
describe it so i'll make it so basically

00:18:04,400 --> 00:18:08,960
both performs better than

00:18:05,919 --> 00:18:10,320
other runtime systems another

00:18:08,960 --> 00:18:12,400
interesting point is that

00:18:10,320 --> 00:18:14,240
with lightweight threads both performs

00:18:12,400 --> 00:18:16,799
as good as opening the task

00:18:14,240 --> 00:18:17,440
in major opening runtime systems so it

00:18:16,799 --> 00:18:19,360
is very

00:18:17,440 --> 00:18:21,760
interesting because for example there is

00:18:19,360 --> 00:18:22,720
no npi interpretability issue if people

00:18:21,760 --> 00:18:25,440
use live weight or

00:18:22,720 --> 00:18:27,280
threads instead of tasks so tasks and

00:18:25,440 --> 00:18:29,600
threads are different in openp

00:18:27,280 --> 00:18:31,120
in terms of its functionality but i can

00:18:29,600 --> 00:18:33,120
say that their performance

00:18:31,120 --> 00:18:36,240
can be almost the same in terms of

00:18:33,120 --> 00:18:38,720
probabilization overheads

00:18:36,240 --> 00:18:40,960
okay then let's evaluate bolt a typical

00:18:38,720 --> 00:18:43,679
use case of bolt is to exploit an

00:18:40,960 --> 00:18:44,320
intentionally nested power region so

00:18:43,679 --> 00:18:46,559
open b

00:18:44,320 --> 00:18:47,600
is used everywhere and everyone loves

00:18:46,559 --> 00:18:49,360
power four

00:18:47,600 --> 00:18:51,679
and only parallel regions are easily

00:18:49,360 --> 00:18:53,200
nested in such a situation

00:18:51,679 --> 00:18:55,760
okay so because of the time limitation

00:18:53,200 --> 00:18:56,160
i'll skip careful and q box should be

00:18:55,760 --> 00:18:58,720
more

00:18:56,160 --> 00:18:58,720
insightful

00:19:00,720 --> 00:19:05,120
all right so qbox is the first principle

00:19:03,120 --> 00:19:07,200
molecular dynamics goal

00:19:05,120 --> 00:19:08,799
so we've only focused on this fft

00:19:07,200 --> 00:19:11,120
computation part

00:19:08,799 --> 00:19:12,400
on a single node so extract is kernel

00:19:11,120 --> 00:19:15,280
and a variety

00:19:12,400 --> 00:19:17,039
in this case the user code has a power 4

00:19:15,280 --> 00:19:20,240
that runs on fft kernel

00:19:17,039 --> 00:19:21,919
and that fft kernel is also paralyzed by

00:19:20,240 --> 00:19:23,919
fftw3

00:19:21,919 --> 00:19:26,320
okay our input is that of the gold

00:19:23,919 --> 00:19:29,200
benchmark we run this micro benchmark

00:19:26,320 --> 00:19:31,440
on not micro benchmark benchmark on 64

00:19:29,200 --> 00:19:33,120
core until next landing

00:19:31,440 --> 00:19:35,120
we fix the number of threads of the

00:19:33,120 --> 00:19:37,919
outer power region 64

00:19:35,120 --> 00:19:39,520
because this knl has 64 chords but

00:19:37,919 --> 00:19:40,640
change the number of threads of the

00:19:39,520 --> 00:19:42,080
inner parallel regions

00:19:40,640 --> 00:19:45,280
in this case we change the number of

00:19:42,080 --> 00:19:46,000
fftw threads the result shows that if we

00:19:45,280 --> 00:19:48,720
only use

00:19:46,000 --> 00:19:49,440
one thread per in-number region bolt

00:19:48,720 --> 00:19:52,480
performs

00:19:49,440 --> 00:19:54,240
as good as intel mps but if we

00:19:52,480 --> 00:19:56,160
increase the number of inner threads

00:19:54,240 --> 00:19:56,640
both can exploit nested parallelism and

00:19:56,160 --> 00:19:58,480
achieve

00:19:56,640 --> 00:20:00,400
better performance performance

00:19:58,480 --> 00:20:02,559
improvement of bolt is up to four times

00:20:00,400 --> 00:20:05,120
compared with the flat tire reconversion

00:20:02,559 --> 00:20:06,559
and into openmp fails to exploit

00:20:05,120 --> 00:20:09,039
necessarily

00:20:06,559 --> 00:20:10,320
in this case nested power region is more

00:20:09,039 --> 00:20:12,799
beneficial when we use

00:20:10,320 --> 00:20:14,480
fewer atoms and more npi processes which

00:20:12,799 --> 00:20:17,360
limit the other powers

00:20:14,480 --> 00:20:19,600
we can say that fewer atoms and more mpi

00:20:17,360 --> 00:20:20,799
processes is a common case of strong

00:20:19,600 --> 00:20:22,640
scaling

00:20:20,799 --> 00:20:24,080
so sometimes the perform and programs

00:20:22,640 --> 00:20:26,320
have limited other problems

00:20:24,080 --> 00:20:28,559
and require more inner progress

00:20:26,320 --> 00:20:30,960
necessary regions and volt would be very

00:20:28,559 --> 00:20:34,480
helpful in such a case

00:20:30,960 --> 00:20:35,919
okay so both plus mpi so robots has its

00:20:34,480 --> 00:20:39,039
own software ecosystem

00:20:35,919 --> 00:20:40,799
and as a cargobots-based op-amp library

00:20:39,039 --> 00:20:41,760
both can enjoy the interpretability and

00:20:40,799 --> 00:20:43,200
the transparent

00:20:41,760 --> 00:20:45,200
thread resource management via the

00:20:43,200 --> 00:20:47,440
outbox layer here let's see the

00:20:45,200 --> 00:20:49,120
inter-availability of mpi plus both

00:20:47,440 --> 00:20:50,640
so mpi plus bolt can improve the

00:20:49,120 --> 00:20:52,080
performance vrd lightweight threading

00:20:50,640 --> 00:20:53,440
library layer

00:20:52,080 --> 00:20:55,440
remember that lightweight archival

00:20:53,440 --> 00:20:57,360
threads are scheduled on p threads and

00:20:55,440 --> 00:20:58,880
this user level scheduling is very

00:20:57,360 --> 00:21:01,760
lightweight and flexible

00:20:58,880 --> 00:21:03,919
for example our robots aware mpi assume

00:21:01,760 --> 00:21:04,400
arguable sliver lightweight thread other

00:21:03,919 --> 00:21:08,000
thread

00:21:04,400 --> 00:21:10,159
in terms of npr plus x or mp3 multiple

00:21:08,000 --> 00:21:12,000
this lightweight and flexible scheduling

00:21:10,159 --> 00:21:13,520
can achieve better communication and

00:21:12,000 --> 00:21:15,280
competition overlap

00:21:13,520 --> 00:21:18,000
lightweight spreads are also good at

00:21:15,280 --> 00:21:20,400
latency hiding or blocking operations

00:21:18,000 --> 00:21:22,320
so now the inter-availability layer is

00:21:20,400 --> 00:21:25,919
implemented in several mpi

00:21:22,320 --> 00:21:27,679
runtime systems mpich plus both argobots

00:21:25,919 --> 00:21:29,600
is quite stable and functional this

00:21:27,679 --> 00:21:32,640
feature has been already merged into

00:21:29,600 --> 00:21:35,679
mpich main branch one year ago

00:21:32,640 --> 00:21:37,600
and we also test mpi ch plus arc above

00:21:35,679 --> 00:21:39,360
every week to make sure it works well

00:21:37,600 --> 00:21:41,039
the right figure shows a point viewpoint

00:21:39,360 --> 00:21:42,799
latency on the hustle core

00:21:41,039 --> 00:21:44,880
we increase the number of receiver

00:21:42,799 --> 00:21:45,360
threads per piece red while we keep

00:21:44,880 --> 00:21:48,480
using

00:21:45,360 --> 00:21:49,280
older 36 cores by creating 36 piece

00:21:48,480 --> 00:21:51,760
frames

00:21:49,280 --> 00:21:52,720
in this sense we keep using 36 p spreads

00:21:51,760 --> 00:21:54,159
but they run

00:21:52,720 --> 00:21:56,159
a different number of lightweight

00:21:54,159 --> 00:21:58,320
threads and more threads can reduce the

00:21:56,159 --> 00:21:59,919
latency when the message size is small

00:21:58,320 --> 00:22:01,440
because it can achieve better latency

00:21:59,919 --> 00:22:02,480
hiding thanks to lightweight context

00:22:01,440 --> 00:22:04,159
switching

00:22:02,480 --> 00:22:06,799
of course some computers are using

00:22:04,159 --> 00:22:08,880
another npm implementation openmpi

00:22:06,799 --> 00:22:10,799
and we are collaborating with sandeer

00:22:08,880 --> 00:22:11,600
and los alamos researchers including

00:22:10,799 --> 00:22:14,400
steven

00:22:11,600 --> 00:22:16,240
john howard noah to improve the interval

00:22:14,400 --> 00:22:17,760
ability within op api

00:22:16,240 --> 00:22:20,960
the initial support has been already

00:22:17,760 --> 00:22:23,679
marked into the main branch

00:22:20,960 --> 00:22:24,400
all right conclusions so from the user

00:22:23,679 --> 00:22:26,159
perspective

00:22:24,400 --> 00:22:27,679
the most important thing should be how

00:22:26,159 --> 00:22:29,600
easy to use both

00:22:27,679 --> 00:22:31,760
and i can say it's easy so bolt has

00:22:29,600 --> 00:22:33,039
higher avi compatibility with not only

00:22:31,760 --> 00:22:35,600
lfm of mp

00:22:33,039 --> 00:22:36,880
but also intel c c plus plus compilers

00:22:35,600 --> 00:22:38,960
and dc open b

00:22:36,880 --> 00:22:42,080
so if you compile your program with

00:22:38,960 --> 00:22:44,000
clown intel compilers or gcc then you

00:22:42,080 --> 00:22:45,440
can just change the library by using for

00:22:44,000 --> 00:22:46,960
example ld reload

00:22:45,440 --> 00:22:49,039
in other words you don't need to change

00:22:46,960 --> 00:22:50,799
any binary including your calls other

00:22:49,039 --> 00:22:52,799
dependent software packages and your

00:22:50,799 --> 00:22:54,640
closed source math libraries

00:22:52,799 --> 00:22:56,080
okay how about the coverage of mpv

00:22:54,640 --> 00:22:58,159
features so

00:22:56,080 --> 00:22:59,679
except for a few features both supports

00:22:58,159 --> 00:23:02,159
the latest openp standards

00:22:59,679 --> 00:23:02,880
so it supports for example um power

00:23:02,159 --> 00:23:05,840
regions

00:23:02,880 --> 00:23:06,799
tasking target offloading and synthetics

00:23:05,840 --> 00:23:09,120
for example

00:23:06,799 --> 00:23:09,840
unfortunately volt is being actively

00:23:09,120 --> 00:23:11,760
developed

00:23:09,840 --> 00:23:13,200
and not all the features are supported

00:23:11,760 --> 00:23:16,559
for example ompt

00:23:13,200 --> 00:23:19,280
and ompd are currently disabled

00:23:16,559 --> 00:23:21,520
how to try okay i recommend the stack

00:23:19,280 --> 00:23:25,679
version so stack install forks

00:23:21,520 --> 00:23:28,960
spark spec install bolt works very well

00:23:25,679 --> 00:23:30,960
it's very easy how to use it again no

00:23:28,960 --> 00:23:33,360
need to recompile your programs

00:23:30,960 --> 00:23:34,960
just change the runtime library so we

00:23:33,360 --> 00:23:37,520
basically recommend you to use

00:23:34,960 --> 00:23:39,360
ld library path for example if you

00:23:37,520 --> 00:23:39,840
install the program in the direct label

00:23:39,360 --> 00:23:41,919
install

00:23:39,840 --> 00:23:42,880
and you can just set both install leave

00:23:41,919 --> 00:23:45,360
and run the program

00:23:42,880 --> 00:23:46,080
that's it please use ldd to check if

00:23:45,360 --> 00:23:48,159
bolt is

00:23:46,080 --> 00:23:50,320
and properly loaded and this trick

00:23:48,159 --> 00:23:52,400
sometimes doesn't work with gcd open b

00:23:50,320 --> 00:23:53,919
then if it is a case please use ld

00:23:52,400 --> 00:23:55,919
pre-rolled

00:23:53,919 --> 00:23:57,760
all right fourth family of both so bold

00:23:55,919 --> 00:23:58,799
is a lightweight only random systems

00:23:57,760 --> 00:24:02,000
based on ltm

00:23:58,799 --> 00:24:03,520
and p in norbold library open b threads

00:24:02,000 --> 00:24:04,159
and tasks are mapped to lightweight

00:24:03,520 --> 00:24:06,240
threads so

00:24:04,159 --> 00:24:08,480
open b parallel regions and passing are

00:24:06,240 --> 00:24:08,799
optimized this lightweight reading layer

00:24:08,480 --> 00:24:11,360
can

00:24:08,799 --> 00:24:13,200
improve the interoperability of open p

00:24:11,360 --> 00:24:14,000
specifically i'd like to emphasize these

00:24:13,200 --> 00:24:15,840
two points

00:24:14,000 --> 00:24:17,440
first open these threads are very

00:24:15,840 --> 00:24:18,559
lightweight compared with the original

00:24:17,440 --> 00:24:20,960
lvm open p

00:24:18,559 --> 00:24:21,919
so bolt can efficiently handle necessary

00:24:20,960 --> 00:24:24,080
regions

00:24:21,919 --> 00:24:25,120
also bolt has higher interoperability

00:24:24,080 --> 00:24:27,279
with mpi

00:24:25,120 --> 00:24:28,559
some of these ready to mpi programs

00:24:27,279 --> 00:24:31,120
might perform better than

00:24:28,559 --> 00:24:32,960
uh might perform better with both and

00:24:31,120 --> 00:24:35,600
importantly the other features are the

00:24:32,960 --> 00:24:37,840
same as the original lvm openp

00:24:35,600 --> 00:24:39,279
so for example gpu of loading features

00:24:37,840 --> 00:24:42,400
and performance are the same

00:24:39,279 --> 00:24:44,640
as the lvm openp so for now

00:24:42,400 --> 00:24:47,039
both can mainly improve the performance

00:24:44,640 --> 00:24:48,799
on cpu side now if you want to know more

00:24:47,039 --> 00:24:51,039
please find the information at the

00:24:48,799 --> 00:24:51,919
official website or please google both

00:24:51,039 --> 00:24:54,080
plus openp

00:24:51,919 --> 00:24:56,080
or please feel free to ask any question

00:24:54,080 --> 00:24:58,880
via emails

00:24:56,080 --> 00:25:01,440
here's the bulk rated publication and

00:24:58,880 --> 00:25:02,880
it's part of the ecp store project

00:25:01,440 --> 00:25:05,360
thank you very much thank you for your

00:25:02,880 --> 00:25:07,520
listening i'd like to remind you that

00:25:05,360 --> 00:25:09,919
all the recorded materials and slides

00:25:07,520 --> 00:25:11,840
are available on the openld website so

00:25:09,919 --> 00:25:13,600
please visit this fantastic official

00:25:11,840 --> 00:25:14,080
openly website and find the latest

00:25:13,600 --> 00:25:22,880
information

00:25:14,080 --> 00:25:22,880

YouTube URL: https://www.youtube.com/watch?v=FfRB88DuGQI


