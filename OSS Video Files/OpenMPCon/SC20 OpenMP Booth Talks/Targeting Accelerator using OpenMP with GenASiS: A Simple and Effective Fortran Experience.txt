Title: Targeting Accelerator using OpenMP with GenASiS: A Simple and Effective Fortran Experience
Publication date: 2020-11-04
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, from Reuben Budiardja, Oak Ridge National Laboratory, is part of the OpenMP Booth Talk series created for Supercomputing 2020. A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20/
Captions: 
	00:00:08,559 --> 00:00:11,120
in this presentation i'm going to talk

00:00:10,719 --> 00:00:14,240
about

00:00:11,120 --> 00:00:15,759
our experience in using openmp which has

00:00:14,240 --> 00:00:18,560
proven to be a simple

00:00:15,759 --> 00:00:21,359
and effective way to target accelerator

00:00:18,560 --> 00:00:23,439
with our code genesis

00:00:21,359 --> 00:00:26,160
a few words about the application we are

00:00:23,439 --> 00:00:28,400
developing genesis stands for general

00:00:26,160 --> 00:00:30,800
astrophysics simulation system

00:00:28,400 --> 00:00:31,599
which is an application we designed for

00:00:30,800 --> 00:00:33,920
parallel

00:00:31,599 --> 00:00:35,200
large scale simulation it has been

00:00:33,920 --> 00:00:38,879
demonstrated to

00:00:35,200 --> 00:00:41,520
weak scale up to 100 000 mpi processes

00:00:38,879 --> 00:00:43,200
genesis is written entirely in modern

00:00:41,520 --> 00:00:46,320
fortran using feature

00:00:43,200 --> 00:00:48,800
from fortran 2003 and 2008

00:00:46,320 --> 00:00:49,520
to achieve modular object oriented

00:00:48,800 --> 00:00:52,800
design

00:00:49,520 --> 00:00:53,360
and extensible physics solver genesis

00:00:52,800 --> 00:00:56,399
has

00:00:53,360 --> 00:00:59,520
multiphysic solver for hydrodynamics and

00:00:56,399 --> 00:01:02,399
explicit second order time integration

00:00:59,520 --> 00:01:03,359
it has solved for self gravity and with

00:01:02,399 --> 00:01:06,000
polytropic

00:01:03,359 --> 00:01:07,840
and nuclear equation of state and it's

00:01:06,000 --> 00:01:10,159
currently being developed to

00:01:07,840 --> 00:01:12,159
solve the neutrino transport problem

00:01:10,159 --> 00:01:15,280
using gray and spectral

00:01:12,159 --> 00:01:15,920
radiation transport formulation prior to

00:01:15,280 --> 00:01:18,799
this work

00:01:15,920 --> 00:01:19,600
genesis is mainly a cpu-only code with

00:01:18,799 --> 00:01:23,280
openmp

00:01:19,600 --> 00:01:25,920
for multithreading on cpu

00:01:23,280 --> 00:01:27,360
we have used genesis to study the role

00:01:25,920 --> 00:01:29,600
of fluid instabilities

00:01:27,360 --> 00:01:30,400
in supernova dynamics including

00:01:29,600 --> 00:01:32,400
convection

00:01:30,400 --> 00:01:34,240
and what is known as the standing

00:01:32,400 --> 00:01:37,360
accretion shock instability

00:01:34,240 --> 00:01:40,079
or sassy in previous simulation we

00:01:37,360 --> 00:01:42,720
discovered that these instabilities

00:01:40,079 --> 00:01:44,960
can exponentially amplify the magnetic

00:01:42,720 --> 00:01:47,119
fields of the progenitor star

00:01:44,960 --> 00:01:49,040
which speaks to the origin of neutron

00:01:47,119 --> 00:01:51,759
star magnetic fields

00:01:49,040 --> 00:01:53,600
on the right here is a visualization of

00:01:51,759 --> 00:01:56,079
magnetic field during core collapse

00:01:53,600 --> 00:01:58,479
supernova event

00:01:56,079 --> 00:01:59,920
recently we refactor genesis to three

00:01:58,479 --> 00:02:02,719
major subdivision

00:01:59,920 --> 00:02:03,200
basics mathematics and physics this

00:02:02,719 --> 00:02:05,840
allows

00:02:03,200 --> 00:02:08,239
us for a more systematic unit testing

00:02:05,840 --> 00:02:10,720
and building proxy application

00:02:08,239 --> 00:02:12,080
to explore the kind of porting work we

00:02:10,720 --> 00:02:14,080
described here

00:02:12,080 --> 00:02:16,480
i will note that by the end of our

00:02:14,080 --> 00:02:18,000
initial exploration we have applied the

00:02:16,480 --> 00:02:20,640
technique we developed

00:02:18,000 --> 00:02:23,680
in using openmp to the whole code that

00:02:20,640 --> 00:02:26,080
is to all the subdivision of genesis

00:02:23,680 --> 00:02:27,440
in the rest of this talk i will describe

00:02:26,080 --> 00:02:31,280
how we use openmp

00:02:27,440 --> 00:02:31,280
offload to target accelerator

00:02:31,760 --> 00:02:36,879
initially we consider the path available

00:02:34,480 --> 00:02:39,200
to us in targeting accelerators

00:02:36,879 --> 00:02:41,040
one option is to use the native

00:02:39,200 --> 00:02:44,239
accelerator programming model

00:02:41,040 --> 00:02:46,160
such as cuda or hip for nvidia and amd

00:02:44,239 --> 00:02:48,959
gpu respectively

00:02:46,160 --> 00:02:51,440
this however will require us to rewrite

00:02:48,959 --> 00:02:53,840
all of our computational kernels

00:02:51,440 --> 00:02:54,560
and also the laws of fortran semantics

00:02:53,840 --> 00:02:56,959
including

00:02:54,560 --> 00:02:58,480
multi-dimensional array and pointer

00:02:56,959 --> 00:03:00,959
array mapping

00:02:58,480 --> 00:03:03,040
and it will also require a way to

00:03:00,959 --> 00:03:04,080
interface with the rest of the fortran

00:03:03,040 --> 00:03:06,319
code

00:03:04,080 --> 00:03:08,239
another option is to use extension to

00:03:06,319 --> 00:03:09,200
the base language in this case cuda

00:03:08,239 --> 00:03:11,360
fortran

00:03:09,200 --> 00:03:14,080
but then again this is a non-standard

00:03:11,360 --> 00:03:17,599
extension to fortran only supported by

00:03:14,080 --> 00:03:19,680
few compilers none uniformly and another

00:03:17,599 --> 00:03:21,360
issue here is that we cannot easily fall

00:03:19,680 --> 00:03:25,120
back to standard fortran

00:03:21,360 --> 00:03:26,560
for push-only code we decided instead to

00:03:25,120 --> 00:03:29,280
use openmp

00:03:26,560 --> 00:03:31,519
because it is a standard test directive

00:03:29,280 --> 00:03:34,640
and in which we can also retain

00:03:31,519 --> 00:03:36,000
all of our fortran semantics and support

00:03:34,640 --> 00:03:38,239
for openmb with

00:03:36,000 --> 00:03:40,640
offload feature is increasingly

00:03:38,239 --> 00:03:43,200
available by more compiler

00:03:40,640 --> 00:03:45,200
and we have an existing implementation

00:03:43,200 --> 00:03:47,040
on our current docket platform that is

00:03:45,200 --> 00:03:48,879
the summit supercomputer

00:03:47,040 --> 00:03:51,519
at the oakridge leadership computing

00:03:48,879 --> 00:03:51,519
facility

00:03:52,560 --> 00:03:56,879
in genesis the heart of our data storage

00:03:55,360 --> 00:04:00,239
facility is a class

00:03:56,879 --> 00:04:03,120
called storage form this class has

00:04:00,239 --> 00:04:05,519
data and metadata as its member and the

00:04:03,120 --> 00:04:06,560
metadata can include units variable

00:04:05,519 --> 00:04:09,840
names for

00:04:06,560 --> 00:04:12,799
io and visualization and we use this

00:04:09,840 --> 00:04:14,000
class to group together a set of related

00:04:12,799 --> 00:04:17,040
physical variable for

00:04:14,000 --> 00:04:17,919
example the fluid variable utilizing

00:04:17,040 --> 00:04:21,040
this class in

00:04:17,919 --> 00:04:21,519
all of our solver render our software to

00:04:21,040 --> 00:04:24,560
be more

00:04:21,519 --> 00:04:25,360
generic and simplified code for io cause

00:04:24,560 --> 00:04:27,840
exchange

00:04:25,360 --> 00:04:30,479
prolongation and restriction in am rms

00:04:27,840 --> 00:04:30,479
and so on

00:04:32,400 --> 00:04:37,280
here's an example of how we initialize a

00:04:35,520 --> 00:04:40,880
storage form object

00:04:37,280 --> 00:04:43,040
in this case a rank 2 array is allocated

00:04:40,880 --> 00:04:44,160
on the host with six rows and six

00:04:43,040 --> 00:04:46,720
columns

00:04:44,160 --> 00:04:47,440
the rows usually correspond to the cells

00:04:46,720 --> 00:04:49,199
on the mass

00:04:47,440 --> 00:04:51,280
and the column corresponds to the

00:04:49,199 --> 00:04:54,800
variable such as pressure

00:04:51,280 --> 00:04:57,680
density energy and so on

00:04:54,800 --> 00:04:58,800
when we call allocate device method a

00:04:57,680 --> 00:05:01,280
mirror allocation

00:04:58,800 --> 00:05:03,440
is created on the gpu memory with the

00:05:01,280 --> 00:05:06,479
corresponding total size

00:05:03,440 --> 00:05:07,759
however host to device association is

00:05:06,479 --> 00:05:10,479
made per variable

00:05:07,759 --> 00:05:12,560
that is column wise in this case there

00:05:10,479 --> 00:05:14,240
is a separate association for each

00:05:12,560 --> 00:05:17,440
variable such as density

00:05:14,240 --> 00:05:18,240
pressure and energy the bits of code on

00:05:17,440 --> 00:05:21,280
this slide

00:05:18,240 --> 00:05:24,080
illustrate how this association are made

00:05:21,280 --> 00:05:25,759
the association is done this way so that

00:05:24,080 --> 00:05:28,240
our software can address

00:05:25,759 --> 00:05:30,560
its variable independently by

00:05:28,240 --> 00:05:33,600
establishing these associations

00:05:30,560 --> 00:05:35,520
we tell the openmp runtime the data

00:05:33,600 --> 00:05:38,320
location on gpu

00:05:35,520 --> 00:05:38,720
for each of the variable on the host

00:05:38,320 --> 00:05:41,120
this

00:05:38,720 --> 00:05:41,840
avoids implicit allocation and data

00:05:41,120 --> 00:05:46,400
movement

00:05:41,840 --> 00:05:46,400
on the kernel of load region of our code

00:05:47,840 --> 00:05:54,960
for some of its api calls openmp

00:05:50,960 --> 00:05:57,440
4.5 only profiled c interfaces

00:05:54,960 --> 00:05:59,680
so we have created fortran wrappers to

00:05:57,440 --> 00:06:02,479
this course to use in genesys

00:05:59,680 --> 00:06:04,080
the subroutine allocate device is our

00:06:02,479 --> 00:06:06,960
fortran wrapper to

00:06:04,080 --> 00:06:08,240
omb target alloc which allocate memory

00:06:06,960 --> 00:06:10,639
on the gpu

00:06:08,240 --> 00:06:11,840
the subroutine associate host is a

00:06:10,639 --> 00:06:14,800
wrapper to

00:06:11,840 --> 00:06:15,520
omp target associate pointer which

00:06:14,800 --> 00:06:18,560
causes

00:06:15,520 --> 00:06:21,440
reference to a fortran array value

00:06:18,560 --> 00:06:22,319
appearing in appropriate omp openmb

00:06:21,440 --> 00:06:25,120
directive

00:06:22,319 --> 00:06:26,800
to be interpreted as referring to the

00:06:25,120 --> 00:06:30,319
device memory

00:06:26,800 --> 00:06:31,440
update device and update host our data

00:06:30,319 --> 00:06:34,319
transfer wrappers

00:06:31,440 --> 00:06:35,120
to openmp target mem copy this

00:06:34,319 --> 00:06:38,240
subroutine

00:06:35,120 --> 00:06:39,120
let's let us have affirmative control of

00:06:38,240 --> 00:06:41,520
data movement

00:06:39,120 --> 00:06:44,080
with persistent memory allocation on the

00:06:41,520 --> 00:06:44,080
device

00:06:45,360 --> 00:06:50,400
on this slide we give an example of how

00:06:48,160 --> 00:06:52,639
we put all of this together to offload a

00:06:50,400 --> 00:06:55,840
computational kernel

00:06:52,639 --> 00:06:56,479
on the right box here the storage from

00:06:55,840 --> 00:06:58,639
object

00:06:56,479 --> 00:07:00,800
f is initialized with the number of

00:06:58,639 --> 00:07:03,199
cells and number of variables

00:07:00,800 --> 00:07:03,840
and then the method allocate device is

00:07:03,199 --> 00:07:06,639
called

00:07:03,840 --> 00:07:08,160
to mirror the allocation on the gpu

00:07:06,639 --> 00:07:11,599
memory

00:07:08,160 --> 00:07:12,639
and then we call update device to update

00:07:11,599 --> 00:07:16,560
the

00:07:12,639 --> 00:07:20,400
gpu memory with data from the host

00:07:16,560 --> 00:07:22,800
and then we call the kernel add kernel

00:07:20,400 --> 00:07:24,639
with the appropriate variable as the

00:07:22,800 --> 00:07:27,360
argument to the kernel

00:07:24,639 --> 00:07:29,440
note here that during allocate device we

00:07:27,360 --> 00:07:32,000
not only make the allocation on the

00:07:29,440 --> 00:07:34,479
device but also then the

00:07:32,000 --> 00:07:36,240
perforable association as we have seen

00:07:34,479 --> 00:07:39,360
on previous slide

00:07:36,240 --> 00:07:42,880
inside the supporting add kernel

00:07:39,360 --> 00:07:44,879
we add the directive omp target themes

00:07:42,880 --> 00:07:47,919
distribute parallel do

00:07:44,879 --> 00:07:48,400
to offload this computational kernel on

00:07:47,919 --> 00:07:51,840
the

00:07:48,400 --> 00:07:53,680
gpu note that in this offload there is

00:07:51,840 --> 00:07:56,720
no implicit data transfer

00:07:53,680 --> 00:07:57,360
and there is no explicit map clauses

00:07:56,720 --> 00:07:59,919
because

00:07:57,360 --> 00:08:00,800
all the association and allocation is

00:07:59,919 --> 00:08:03,280
already done

00:08:00,800 --> 00:08:04,560
previously prior to the call to this

00:08:03,280 --> 00:08:06,720
kernel

00:08:04,560 --> 00:08:08,560
so again it is important to note that

00:08:06,720 --> 00:08:12,160
the only changes to

00:08:08,560 --> 00:08:14,720
the kernel code is just this directive

00:08:12,160 --> 00:08:17,440
to offload this computational kernel to

00:08:14,720 --> 00:08:17,440
the gpu

00:08:17,520 --> 00:08:22,000
it is also worth noting the degree of

00:08:20,000 --> 00:08:24,560
flexibility available

00:08:22,000 --> 00:08:26,479
in the mapping of host program variables

00:08:24,560 --> 00:08:27,520
to device memory in computational

00:08:26,479 --> 00:08:30,560
kernels

00:08:27,520 --> 00:08:32,560
in our fluid dynamics problem as is of

00:08:30,560 --> 00:08:35,279
course the case in general

00:08:32,560 --> 00:08:36,640
some of our kernels require knowledge of

00:08:35,279 --> 00:08:39,760
special relationship

00:08:36,640 --> 00:08:41,120
in the data in particular such as the

00:08:39,760 --> 00:08:44,399
nearest neighbor

00:08:41,120 --> 00:08:44,959
or wider stencils in this example

00:08:44,399 --> 00:08:47,360
problem

00:08:44,959 --> 00:08:48,000
a single level rectangular mass is

00:08:47,360 --> 00:08:50,160
employed

00:08:48,000 --> 00:08:53,120
so that the stencil relationship can be

00:08:50,160 --> 00:08:56,720
represented through appropriate indexing

00:08:53,120 --> 00:08:58,399
of rank 3 array embodying discretized 3d

00:08:56,720 --> 00:09:01,760
position space

00:08:58,399 --> 00:09:03,040
such 3d arrays can be obtained from a

00:09:01,760 --> 00:09:05,120
column of data

00:09:03,040 --> 00:09:06,800
with the fortran pointer remapping

00:09:05,120 --> 00:09:10,240
facility

00:09:06,800 --> 00:09:11,680
for example here two local rank three

00:09:10,240 --> 00:09:14,880
pointer variable v

00:09:11,680 --> 00:09:16,720
and dv are three dimensional views of

00:09:14,880 --> 00:09:20,240
particular variable

00:09:16,720 --> 00:09:22,320
inside the storage from object f and df

00:09:20,240 --> 00:09:25,040
the bonds of these pointers are

00:09:22,320 --> 00:09:28,320
appropriately modified to account

00:09:25,040 --> 00:09:31,120
for the good cells on the subdomain

00:09:28,320 --> 00:09:32,399
we then use this pointers variables v

00:09:31,120 --> 00:09:34,720
and dv

00:09:32,399 --> 00:09:36,640
as arguments to the subroutine compute

00:09:34,720 --> 00:09:39,760
differences x

00:09:36,640 --> 00:09:42,680
now because the storage from objects

00:09:39,760 --> 00:09:44,880
f and df have already been allocated and

00:09:42,680 --> 00:09:46,959
associated with the correct memory

00:09:44,880 --> 00:09:49,920
location on the gpu

00:09:46,959 --> 00:09:51,200
any reference to v and dv in an

00:09:49,920 --> 00:09:53,519
offloaded kernel

00:09:51,200 --> 00:09:56,880
would translate correctly to the memory

00:09:53,519 --> 00:09:59,600
location on the gpu

00:09:56,880 --> 00:10:02,000
on this slide we see the content of the

00:09:59,600 --> 00:10:05,440
subroutine compute differences x

00:10:02,000 --> 00:10:07,680
called with v and dv as its arguments

00:10:05,440 --> 00:10:09,200
again i wanted to note that the only

00:10:07,680 --> 00:10:12,240
change to this subroutine

00:10:09,200 --> 00:10:13,200
to offload it to gpu is the addition of

00:10:12,240 --> 00:10:15,839
the openmp

00:10:13,200 --> 00:10:18,160
target directive also note here that

00:10:15,839 --> 00:10:21,120
there is no explicit mapping clause

00:10:18,160 --> 00:10:22,160
needed nor there is any implicit data

00:10:21,120 --> 00:10:25,279
movement during

00:10:22,160 --> 00:10:27,360
the kernel execution

00:10:25,279 --> 00:10:29,279
as a concrete working example of

00:10:27,360 --> 00:10:32,320
targeting gpu with openmp

00:10:29,279 --> 00:10:35,279
directives we use our implementation

00:10:32,320 --> 00:10:36,320
to solve a riemann problem using genesis

00:10:35,279 --> 00:10:38,800
basics

00:10:36,320 --> 00:10:39,440
riemann problem is an extension of the

00:10:38,800 --> 00:10:42,399
classic

00:10:39,440 --> 00:10:45,040
1d sort shock tube a standard

00:10:42,399 --> 00:10:48,880
computational fluid dynamic test problem

00:10:45,040 --> 00:10:49,920
to 2d and 3d figure 1 shows the initial

00:10:48,880 --> 00:10:53,760
and final state

00:10:49,920 --> 00:10:56,800
of 1d and 3d versions of riemann problem

00:10:53,760 --> 00:11:00,320
after being evolved to time t

00:10:56,800 --> 00:11:02,880
equals 0.25 on the initial state

00:11:00,320 --> 00:11:03,920
a high density high pressure gas is

00:11:02,880 --> 00:11:06,959
separated by

00:11:03,920 --> 00:11:10,320
membrane which is removed at time t

00:11:06,959 --> 00:11:11,040
equals zero as guest evolve shock wave

00:11:10,320 --> 00:11:14,320
forms

00:11:11,040 --> 00:11:15,200
moving to the right in this plot blue is

00:11:14,320 --> 00:11:17,440
the velocity

00:11:15,200 --> 00:11:18,240
and red is the pressure and green is the

00:11:17,440 --> 00:11:22,560
density

00:11:18,240 --> 00:11:24,800
of the fluid this is a pseudo chord

00:11:22,560 --> 00:11:26,560
outlining fluid dynamic evolution in

00:11:24,800 --> 00:11:28,959
genesis basics

00:11:26,560 --> 00:11:31,760
its iteration of this main loop is a

00:11:28,959 --> 00:11:34,320
second order rangakata consisting two

00:11:31,760 --> 00:11:37,360
substep which consists of several

00:11:34,320 --> 00:11:40,079
computational kernel

00:11:37,360 --> 00:11:41,760
this is the version of pseudocode for

00:11:40,079 --> 00:11:44,480
the gpu

00:11:41,760 --> 00:11:46,720
except for the addition of the transfer

00:11:44,480 --> 00:11:49,600
operation the algorithm is

00:11:46,720 --> 00:11:51,040
the same almost line by line the the

00:11:49,600 --> 00:11:53,519
only difference here is that

00:11:51,040 --> 00:11:55,040
every computational kernel now is done

00:11:53,519 --> 00:11:57,600
on the device

00:11:55,040 --> 00:11:58,480
so then the task here is to offload this

00:11:57,600 --> 00:12:01,600
kernel

00:11:58,480 --> 00:12:04,399
to run on gpu with openmb which we have

00:12:01,600 --> 00:12:04,399
shown before

00:12:05,200 --> 00:12:10,720
we perform our work on summit at the

00:12:07,760 --> 00:12:13,440
oakridge leadership computing facility

00:12:10,720 --> 00:12:14,800
here we have a diagram of summit compute

00:12:13,440 --> 00:12:17,839
node which consists of

00:12:14,800 --> 00:12:22,079
two ibm power nine processor and six

00:12:17,839 --> 00:12:22,959
nvidia folder gpu its power nine has 21

00:12:22,079 --> 00:12:25,839
physical core

00:12:22,959 --> 00:12:28,320
available for running users application

00:12:25,839 --> 00:12:32,000
3 gpus and the power 9 sockets

00:12:28,320 --> 00:12:34,720
are interconnected with env 2 power 9

00:12:32,000 --> 00:12:37,680
sockets are connected by an xbox

00:12:34,720 --> 00:12:40,000
from this diagram we can see the affair

00:12:37,680 --> 00:12:42,720
performance comparison between

00:12:40,000 --> 00:12:44,800
the cpu and gpu version can be obtained

00:12:42,720 --> 00:12:46,000
using what we call the proportional

00:12:44,800 --> 00:12:48,240
resource tests

00:12:46,000 --> 00:12:50,079
where we compare the performance of

00:12:48,240 --> 00:12:53,360
seven physical cpu cores

00:12:50,079 --> 00:12:57,360
using multithreading openmp versus one

00:12:53,360 --> 00:12:57,360
gpu with openmb offload

00:12:57,920 --> 00:13:02,639
on this slide we showed the weak scaling

00:13:00,480 --> 00:13:06,000
of the 3d riemann problem

00:13:02,639 --> 00:13:09,200
with 256 cube cells assigned

00:13:06,000 --> 00:13:11,279
for its mpi process the wall time here

00:13:09,200 --> 00:13:13,920
includes data transfer between host

00:13:11,279 --> 00:13:15,120
memory and gpu memory in the gpu version

00:13:13,920 --> 00:13:18,079
of the code

00:13:15,120 --> 00:13:19,200
near ideal speed up with multi threading

00:13:18,079 --> 00:13:22,959
is achieved

00:13:19,200 --> 00:13:25,200
we have run this test up to 8000 gpus on

00:13:22,959 --> 00:13:28,480
summit and obtains between

00:13:25,200 --> 00:13:31,440
12 to 15 x speed up on the gpu compared

00:13:28,480 --> 00:13:34,000
to the 7 cpu threads

00:13:31,440 --> 00:13:35,279
here we plot the timing for each canal

00:13:34,000 --> 00:13:38,160
in the riemann problem

00:13:35,279 --> 00:13:39,199
application in genesis basics since this

00:13:38,160 --> 00:13:42,399
is a timing plot

00:13:39,199 --> 00:13:42,959
lower value is better on this plot we

00:13:42,399 --> 00:13:44,959
compare

00:13:42,959 --> 00:13:46,959
its kernel performance using either

00:13:44,959 --> 00:13:50,720
openmp threading with 7

00:13:46,959 --> 00:13:53,680
cpu core or openmp offload to gpu

00:13:50,720 --> 00:13:54,480
the first bar with light blue color in

00:13:53,680 --> 00:13:57,680
every kernel

00:13:54,480 --> 00:14:00,880
is 4 open mb cpu threads with the ibm

00:13:57,680 --> 00:14:03,279
xl compiler on summit the red bar

00:14:00,880 --> 00:14:05,279
represents the same open mb trading with

00:14:03,279 --> 00:14:08,320
gcc compiler

00:14:05,279 --> 00:14:09,279
note that the plots for cpu tradings are

00:14:08,320 --> 00:14:12,720
scaled down

00:14:09,279 --> 00:14:14,880
by 25x in order to fit this plot

00:14:12,720 --> 00:14:16,399
so the actual timings for the kernels

00:14:14,880 --> 00:14:19,760
using cpu threadings

00:14:16,399 --> 00:14:22,639
are 25x higher the yellow bar

00:14:19,760 --> 00:14:24,880
represents openmp offload to the gpu

00:14:22,639 --> 00:14:27,680
with the xl compiler

00:14:24,880 --> 00:14:30,160
for performance comparison we have also

00:14:27,680 --> 00:14:32,240
ported some kernels to gude

00:14:30,160 --> 00:14:33,519
their timings are represented by the

00:14:32,240 --> 00:14:36,160
green bars

00:14:33,519 --> 00:14:36,800
since we have not ported all kernels to

00:14:36,160 --> 00:14:39,440
gouda

00:14:36,800 --> 00:14:41,680
some kernels do not have the timings for

00:14:39,440 --> 00:14:44,720
the cuda version yet

00:14:41,680 --> 00:14:47,360
there are two things to note here first

00:14:44,720 --> 00:14:49,040
we can see that we achieve significant

00:14:47,360 --> 00:14:51,680
and meaningful speed up

00:14:49,040 --> 00:14:52,639
for every kernel when openmb offload is

00:14:51,680 --> 00:14:54,880
used

00:14:52,639 --> 00:14:56,399
secondly for the kernel that we have

00:14:54,880 --> 00:14:59,519
ported in cuda

00:14:56,399 --> 00:15:00,160
we see performance parity between openmp

00:14:59,519 --> 00:15:02,959
offload

00:15:00,160 --> 00:15:05,360
and gooda for almost all corners this is

00:15:02,959 --> 00:15:06,560
somewhat in contrary to the previously

00:15:05,360 --> 00:15:08,880
hold common wisdom

00:15:06,560 --> 00:15:10,160
that says that gujarakana will be the

00:15:08,880 --> 00:15:12,720
best performing

00:15:10,160 --> 00:15:14,880
for at least in this case we see that

00:15:12,720 --> 00:15:16,880
openmp offload can get as good

00:15:14,880 --> 00:15:19,519
performance with good implementation

00:15:16,880 --> 00:15:21,839
from the compiler

00:15:19,519 --> 00:15:22,800
on this slide we plot the speed up of

00:15:21,839 --> 00:15:26,480
each kernel

00:15:22,800 --> 00:15:30,160
from 7 cpu cores using openmp threading

00:15:26,480 --> 00:15:30,720
to gpu using openmp offload as we can

00:15:30,160 --> 00:15:32,880
see

00:15:30,720 --> 00:15:35,680
on this plot it cannot achieve

00:15:32,880 --> 00:15:38,720
significant speedups

00:15:35,680 --> 00:15:39,199
in conclusion from our experience we can

00:15:38,720 --> 00:15:41,759
say

00:15:39,199 --> 00:15:44,320
that openmp provides a simple yet

00:15:41,759 --> 00:15:46,240
effective path to port fortran code to

00:15:44,320 --> 00:15:48,320
run on accelerators

00:15:46,240 --> 00:15:50,480
it is also encouraging that more

00:15:48,320 --> 00:15:53,279
compilers are including support for the

00:15:50,480 --> 00:15:55,680
latest openmp standard with offload

00:15:53,279 --> 00:15:57,360
we also note that we were able to

00:15:55,680 --> 00:16:00,000
achieve performance parity

00:15:57,360 --> 00:16:00,560
for most of our kernels between openmp

00:16:00,000 --> 00:16:02,560
offload

00:16:00,560 --> 00:16:05,040
and vendor-specific accelerator

00:16:02,560 --> 00:16:07,519
programming models such as cuda

00:16:05,040 --> 00:16:09,440
however there are several benefits for

00:16:07,519 --> 00:16:11,279
us in using openmb

00:16:09,440 --> 00:16:12,480
in that it is more portable and

00:16:11,279 --> 00:16:14,079
standardized approach

00:16:12,480 --> 00:16:16,639
and can be more natural to the

00:16:14,079 --> 00:16:17,839
applications since we do not need to

00:16:16,639 --> 00:16:20,880
rewrite kernels

00:16:17,839 --> 00:16:22,079
we can also continue to exploit features

00:16:20,880 --> 00:16:24,480
of the base language

00:16:22,079 --> 00:16:27,199
fortran in our case this however

00:16:24,480 --> 00:16:30,399
requires that compilers do a good job in

00:16:27,199 --> 00:16:32,880
optimizing their openmp implementation

00:16:30,399 --> 00:16:35,120
as a closing note our code and paper

00:16:32,880 --> 00:16:36,639
describing this work in more details are

00:16:35,120 --> 00:16:38,880
publicly available

00:16:36,639 --> 00:16:40,240
in our github repository and as a

00:16:38,880 --> 00:16:51,839
preprint publication

00:16:40,240 --> 00:16:51,839
in archive thank you for your attention

00:16:53,680 --> 00:16:55,759

YouTube URL: https://www.youtube.com/watch?v=VhHIN_z-nLA


