Title: Portable extreme-scale turbulence simulations using OpenMP
Publication date: 2020-11-02
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation is from P. K. Yeung and Kiran Ravikumar of the Georgia Institute of Technology, with partial support from Oak Ridge National Laboratory. It is part of the OpenMP Booth Talk series created for Supercomputing 2020. A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20/
Captions: 
	00:00:06,240 --> 00:00:08,320
my name is

00:00:06,960 --> 00:00:10,480
pk young i'm from the school of

00:00:08,320 --> 00:00:12,320
aerospace engineering at georgia tech

00:00:10,480 --> 00:00:14,240
and together with michael over cool

00:00:12,320 --> 00:00:16,720
mafikuma first i'd like to thank the

00:00:14,240 --> 00:00:18,800
organizers for putting us on the program

00:00:16,720 --> 00:00:20,160
and also with national network and

00:00:18,800 --> 00:00:21,920
enabling our work

00:00:20,160 --> 00:00:23,680
in our science specialty which is about

00:00:21,920 --> 00:00:25,760
study of turbulent fluid flows

00:00:23,680 --> 00:00:27,439
and we would like to see perform the

00:00:25,760 --> 00:00:30,080
largest simulations possible that we

00:00:27,439 --> 00:00:31,920
call extreme scale stimulation

00:00:30,080 --> 00:00:33,520
and of course we see the potential

00:00:31,920 --> 00:00:36,640
openmp um

00:00:33,520 --> 00:00:38,800
but before we get to openmp uh let me

00:00:36,640 --> 00:00:40,160
spend the first a few minutes about

00:00:38,800 --> 00:00:41,840
turbulence

00:00:40,160 --> 00:00:43,520
a little bit about what this subject is

00:00:41,840 --> 00:00:46,480
uh why do we need a stream scale

00:00:43,520 --> 00:00:46,960
and then we'll talk about the computing

00:00:46,480 --> 00:00:49,200
uh

00:00:46,960 --> 00:00:50,320
turbulence is something that we usually

00:00:49,200 --> 00:00:52,559
find in our lives in

00:00:50,320 --> 00:00:53,440
many kinds of kinds of applications i

00:00:52,559 --> 00:00:55,199
would say

00:00:53,440 --> 00:00:57,280
that this is very complex which is very

00:00:55,199 --> 00:00:59,520
common and also very important

00:00:57,280 --> 00:01:01,600
some of the examples are the graphics on

00:00:59,520 --> 00:01:04,320
on the screen now on the left hand side

00:01:01,600 --> 00:01:05,760
is the concentration field um

00:01:04,320 --> 00:01:06,720
essentially color coded from the

00:01:05,760 --> 00:01:08,560
surroundings

00:01:06,720 --> 00:01:10,159
in the middle we have a we have a wind

00:01:08,560 --> 00:01:11,520
farm and

00:01:10,159 --> 00:01:13,040
on the right hand side we have an

00:01:11,520 --> 00:01:13,439
airplane you will look very carefully

00:01:13,040 --> 00:01:16,640
the

00:01:13,439 --> 00:01:20,240
uh ticking off and the motion atmosphere

00:01:16,640 --> 00:01:23,439
of course um here um it's actually

00:01:20,240 --> 00:01:25,280
made artificially colorful um

00:01:23,439 --> 00:01:27,040
but the complexity is that in all of

00:01:25,280 --> 00:01:28,640
these applications uh

00:01:27,040 --> 00:01:31,520
they are disorderly fluctuations the

00:01:28,640 --> 00:01:33,439
flow has a random or stochastic nature

00:01:31,520 --> 00:01:34,560
despite being governed by deterministic

00:01:33,439 --> 00:01:36,720
physical laws

00:01:34,560 --> 00:01:37,680
so even one of the uh well-known nobel

00:01:36,720 --> 00:01:40,320
prize winners have

00:01:37,680 --> 00:01:41,759
considered turbulence to be important

00:01:40,320 --> 00:01:43,600
and if you want to have engineering

00:01:41,759 --> 00:01:44,960
devices that are more efficient you

00:01:43,600 --> 00:01:47,360
would like to predict uh

00:01:44,960 --> 00:01:48,240
extreme weather more reliably we want to

00:01:47,360 --> 00:01:50,079
understand

00:01:48,240 --> 00:01:52,159
airborne transmission of disease agents

00:01:50,079 --> 00:01:53,759
and so forth we need to model turbulent

00:01:52,159 --> 00:01:56,240
and before we need more to

00:01:53,759 --> 00:01:57,840
develop before we can do the modeling we

00:01:56,240 --> 00:01:59,360
need we need to have some fundamental

00:01:57,840 --> 00:02:01,200
understanding

00:01:59,360 --> 00:02:02,479
which is what the simulations are about

00:02:01,200 --> 00:02:05,119
um

00:02:02,479 --> 00:02:06,000
we are trying to solve for instantaneous

00:02:05,119 --> 00:02:07,840
um

00:02:06,000 --> 00:02:09,440
velocity and the pressure fluctuations

00:02:07,840 --> 00:02:10,720
are subject to the

00:02:09,440 --> 00:02:12,800
loss of conservation of mass and

00:02:10,720 --> 00:02:13,360
momentum these are exact equations

00:02:12,800 --> 00:02:15,520
coordinate

00:02:13,360 --> 00:02:17,520
source equation in the simplest case

00:02:15,520 --> 00:02:21,360
that we have on the left hand side

00:02:17,520 --> 00:02:22,879
essentially essentially acceleration of

00:02:21,360 --> 00:02:24,480
a fluid element

00:02:22,879 --> 00:02:26,480
on a piece of the foot and on the right

00:02:24,480 --> 00:02:29,360
hand side the forces acting on it

00:02:26,480 --> 00:02:29,680
at a minimum pressure and and which

00:02:29,360 --> 00:02:31,519
could

00:02:29,680 --> 00:02:33,040
viscosity representing the fictional

00:02:31,519 --> 00:02:34,560
forces and sometimes you have some other

00:02:33,040 --> 00:02:36,640
forces as well

00:02:34,560 --> 00:02:38,800
we need a leadership class computational

00:02:36,640 --> 00:02:40,959
resources because of a number of reasons

00:02:38,800 --> 00:02:42,080
uh the flow is unsteady uh

00:02:40,959 --> 00:02:44,640
three-dimensional

00:02:42,080 --> 00:02:45,200
so conflict non-linear a wide range of

00:02:44,640 --> 00:02:47,440
scales

00:02:45,200 --> 00:02:49,680
all of these uh requirements uh that

00:02:47,440 --> 00:02:51,120
would add up very very quickly

00:02:49,680 --> 00:02:53,280
in terms of the number of bridge points

00:02:51,120 --> 00:02:54,080
that we have that we must have the

00:02:53,280 --> 00:02:56,400
domain size

00:02:54,080 --> 00:02:58,239
degree spacing and in this direction for

00:02:56,400 --> 00:03:00,959
high values number four turbulent flows

00:02:58,239 --> 00:03:02,400
must be very large and if we want to

00:03:00,959 --> 00:03:04,800
resolve this small scale button

00:03:02,400 --> 00:03:06,159
near one one two one uh more time skip

00:03:04,800 --> 00:03:08,800
more time steps

00:03:06,159 --> 00:03:10,159
and all of these are together overall

00:03:08,800 --> 00:03:12,640
operation time uh

00:03:10,159 --> 00:03:14,720
increases at least very often more than

00:03:12,640 --> 00:03:16,480
uh n to the fourth

00:03:14,720 --> 00:03:18,000
so that you should say that every

00:03:16,480 --> 00:03:20,879
halfway of the great spacing brings

00:03:18,000 --> 00:03:22,000
us a 16 times more computational power

00:03:20,879 --> 00:03:23,840
as a result is

00:03:22,000 --> 00:03:25,120
unavoidable that we uh we are running

00:03:23,840 --> 00:03:27,599
took a

00:03:25,120 --> 00:03:30,840
large note counts uh long one times and

00:03:27,599 --> 00:03:33,440
huge stuff that we have to deal with

00:03:30,840 --> 00:03:36,080
afterwards uh we need to have a high

00:03:33,440 --> 00:03:38,239
equity uh we uh but we can make some

00:03:36,080 --> 00:03:40,959
compromises in terms of the geometry

00:03:38,239 --> 00:03:42,400
so we use the uh a method uh a numerical

00:03:40,959 --> 00:03:44,959
method that's well known uh

00:03:42,400 --> 00:03:46,799
essentially fourier decomposition uh

00:03:44,959 --> 00:03:48,319
where you have uh here what we said the

00:03:46,799 --> 00:03:49,120
full vehicle efficiency are complex

00:03:48,319 --> 00:03:50,720
values

00:03:49,120 --> 00:03:52,400
and if we take the fourier transform of

00:03:50,720 --> 00:03:54,799
the entire matrix of the equation

00:03:52,400 --> 00:03:56,319
um we can we are going to resolve in the

00:03:54,799 --> 00:03:57,040
following form and when on the right

00:03:56,319 --> 00:03:59,120
hand side

00:03:57,040 --> 00:04:01,519
the main issue is how do we deal with

00:03:59,120 --> 00:04:02,560
the fourier transform of a non-linear

00:04:01,519 --> 00:04:04,799
term

00:04:02,560 --> 00:04:05,840
and an economical way of doing so is

00:04:04,799 --> 00:04:08,879
just to

00:04:05,840 --> 00:04:10,879
multiply perform the modification

00:04:08,879 --> 00:04:12,239
in what we call physical space and then

00:04:10,879 --> 00:04:15,599
take the transform

00:04:12,239 --> 00:04:17,840
and that gives us n naught n of uh uh

00:04:15,599 --> 00:04:19,440
in terms of cos for each line of data

00:04:17,840 --> 00:04:20,959
for three dimensional fourier transform

00:04:19,440 --> 00:04:21,840
we're going to have n square of these

00:04:20,959 --> 00:04:25,600
lines so that

00:04:21,840 --> 00:04:28,160
would have n cube a log base two

00:04:25,600 --> 00:04:30,240
of n however we have a new problem uh

00:04:28,160 --> 00:04:33,680
because of the parallel computing um

00:04:30,240 --> 00:04:35,520
we need to rearrange data um between uh

00:04:33,680 --> 00:04:37,919
uh between different uh parallel

00:04:35,520 --> 00:04:40,320
processors so we have to do a transport

00:04:37,919 --> 00:04:41,759
by using mpi all to all and we'll be

00:04:40,320 --> 00:04:42,560
talking about the major conversation in

00:04:41,759 --> 00:04:44,240
a few minutes

00:04:42,560 --> 00:04:45,680
but john murray got just all too old

00:04:44,240 --> 00:04:47,840
that is taking the most time

00:04:45,680 --> 00:04:49,759
so one may ask the question also uh

00:04:47,840 --> 00:04:52,000
think about gpus they are known for

00:04:49,759 --> 00:04:52,880
fast computation but our problem is in

00:04:52,000 --> 00:04:54,080
uh

00:04:52,880 --> 00:04:56,560
my greatest probabilities in

00:04:54,080 --> 00:04:58,800
communication how is it going to help us

00:04:56,560 --> 00:05:00,720
of course we have some some good answers

00:04:58,800 --> 00:05:03,039
for that

00:05:00,720 --> 00:05:04,240
um so what are we going to do here from

00:05:03,039 --> 00:05:05,840
a science point of view you're going to

00:05:04,240 --> 00:05:08,880
want a very large simulation

00:05:05,840 --> 00:05:09,680
uh we are primarily interested in uh

00:05:08,880 --> 00:05:11,840
first uh

00:05:09,680 --> 00:05:13,199
how large the functionality can be uh

00:05:11,840 --> 00:05:15,120
what are the how extreme can the

00:05:13,199 --> 00:05:16,560
fluctuation be are they likely and so

00:05:15,120 --> 00:05:19,360
forth

00:05:16,560 --> 00:05:21,280
um so we have the uh we are interested

00:05:19,360 --> 00:05:21,919
in the quantity called the dissipation

00:05:21,280 --> 00:05:24,320
rate

00:05:21,919 --> 00:05:25,680
which is a twice of of viscosity on the

00:05:24,320 --> 00:05:27,759
quadratic measure and what's called a

00:05:25,680 --> 00:05:29,360
local strain rate on a fluid element

00:05:27,759 --> 00:05:31,280
you may even have a piece of the food

00:05:29,360 --> 00:05:32,639
initially in some regular shape uh this

00:05:31,280 --> 00:05:33,680
is going to get deformed by the

00:05:32,639 --> 00:05:36,320
turbulence

00:05:33,680 --> 00:05:38,639
by the fluctuation in classical theories

00:05:36,320 --> 00:05:42,160
uh the fluctuation of epsilon

00:05:38,639 --> 00:05:44,960
were completely ignored so they say um

00:05:42,160 --> 00:05:45,600
that's a that's an important uh issue in

00:05:44,960 --> 00:05:48,080
um

00:05:45,600 --> 00:05:49,680
how do we make the immediacy correction

00:05:48,080 --> 00:05:52,400
so and this is going to be a function of

00:05:49,680 --> 00:05:54,080
scale size so we take epsilon and then

00:05:52,400 --> 00:05:56,400
we will take a local average

00:05:54,080 --> 00:05:57,360
around a three-dimensional domain a

00:05:56,400 --> 00:05:59,840
subdomain

00:05:57,360 --> 00:06:01,520
a linear size out take it to some power

00:05:59,840 --> 00:06:02,960
p and see how it depends on the on the

00:06:01,520 --> 00:06:06,880
scale size out

00:06:02,960 --> 00:06:06,880
and so then these c

00:06:08,960 --> 00:06:12,160
the calculation can be very expensive

00:06:10,639 --> 00:06:14,000
because we need both high bandwidth

00:06:12,160 --> 00:06:16,960
numbers and high resolution

00:06:14,000 --> 00:06:19,600
uh fortunately uh uh on summit using

00:06:16,960 --> 00:06:22,400
cooler fortune we have been able to

00:06:19,600 --> 00:06:23,039
produce a reasonably a code that

00:06:22,400 --> 00:06:26,240
performs

00:06:23,039 --> 00:06:26,800
quite well uh 18k resolution that's

00:06:26,240 --> 00:06:30,080
what

00:06:26,800 --> 00:06:31,120
what what might mean by about six

00:06:30,080 --> 00:06:32,479
trillion

00:06:31,120 --> 00:06:34,479
i'm not going to show all the results

00:06:32,479 --> 00:06:37,840
but i'd like to show one uh research

00:06:34,479 --> 00:06:39,600
appearing in the paper that is

00:06:37,840 --> 00:06:41,919
part of a universal collection of papers

00:06:39,600 --> 00:06:45,440
should be published

00:06:41,919 --> 00:06:48,160
sometimes sometime this month um

00:06:45,440 --> 00:06:48,639
so here we have several graphs here and

00:06:48,160 --> 00:06:50,960
and

00:06:48,639 --> 00:06:52,240
on the right hand side now there are two

00:06:50,960 --> 00:06:54,639
columns basically

00:06:52,240 --> 00:06:57,039
uh uh the column on the left of it of

00:06:54,639 --> 00:06:59,039
this is about the dissipation rate

00:06:57,039 --> 00:07:01,039
and the and then on the right hand side

00:06:59,039 --> 00:07:02,400
is so-called entropy and so we have to

00:07:01,039 --> 00:07:04,720
do with the

00:07:02,400 --> 00:07:07,199
change in angular orientation of uh

00:07:04,720 --> 00:07:08,720
essentially of a local fluid element

00:07:07,199 --> 00:07:10,800
this is a simple circular flow that we

00:07:08,720 --> 00:07:12,639
have for force isotropic turbulence

00:07:10,800 --> 00:07:14,400
and the taylor and the reynolds number

00:07:12,639 --> 00:07:16,880
here guys important measure

00:07:14,400 --> 00:07:17,759
about the about the size about the range

00:07:16,880 --> 00:07:19,520
of scale

00:07:17,759 --> 00:07:21,199
increases by a factor of somewhere

00:07:19,520 --> 00:07:22,639
between three and four

00:07:21,199 --> 00:07:25,599
and and the number of quick points are

00:07:22,639 --> 00:07:28,639
going from a 3k to 18k cube

00:07:25,599 --> 00:07:31,440
now we can see that um

00:07:28,639 --> 00:07:34,400
not say at the top rogue we have the

00:07:31,440 --> 00:07:36,720
second moment and then we take uh if

00:07:34,400 --> 00:07:38,160
we get the so-called logarithmic local

00:07:36,720 --> 00:07:41,199
slopes to try to

00:07:38,160 --> 00:07:43,599
uh deduce the exponent if they say well

00:07:41,199 --> 00:07:44,639
well form a scaling region then we have

00:07:43,599 --> 00:07:46,240
a patrol where

00:07:44,639 --> 00:07:48,800
where the curves in the middle would

00:07:46,240 --> 00:07:51,599
would be will be flat here

00:07:48,800 --> 00:07:52,879
uh somewhere here and uh that works

00:07:51,599 --> 00:07:55,039
quite well for both

00:07:52,879 --> 00:07:57,120
both variables that we're considering

00:07:55,039 --> 00:07:57,520
not as well as fourth order because uh

00:07:57,120 --> 00:07:59,039
um

00:07:57,520 --> 00:08:01,520
this is a more challenging problem in

00:07:59,039 --> 00:08:03,280
terms of in terms of of the sampling

00:08:01,520 --> 00:08:04,879
but what we have found is that you know

00:08:03,280 --> 00:08:05,360
you know you know in the middle of the

00:08:04,879 --> 00:08:07,520
you know

00:08:05,360 --> 00:08:09,199
so-called universal range of scale uh

00:08:07,520 --> 00:08:11,919
these two variables have behaved

00:08:09,199 --> 00:08:14,080
very similarly uh that we could not be

00:08:11,919 --> 00:08:14,400
sure of uh you might you don't have data

00:08:14,080 --> 00:08:17,759
as

00:08:14,400 --> 00:08:18,240
a sufficiently high renault number and

00:08:17,759 --> 00:08:23,120
then

00:08:18,240 --> 00:08:27,440
um so so to go uh to

00:08:23,120 --> 00:08:30,639
to continue we are going to

00:08:27,440 --> 00:08:33,200
try to run the larger simulation uh um

00:08:30,639 --> 00:08:34,640
uh add additional capability to the code

00:08:33,200 --> 00:08:35,200
and of course we need a extra scale

00:08:34,640 --> 00:08:36,800
computer

00:08:35,200 --> 00:08:39,440
and a reliable way of using those

00:08:36,800 --> 00:08:43,200
computers so i will turn it over to

00:08:39,440 --> 00:08:44,800
cohen in the second half of this talk we

00:08:43,200 --> 00:08:45,600
would like to focus on the computing

00:08:44,800 --> 00:08:47,040
aspects of

00:08:45,600 --> 00:08:48,880
performing pseudospectral direct

00:08:47,040 --> 00:08:52,320
numerical simulations

00:08:48,880 --> 00:08:54,560
of turbulence and how we can use gpus on

00:08:52,320 --> 00:08:56,880
the newer heterogeneous architectures

00:08:54,560 --> 00:08:57,760
to get the best performance and also we

00:08:56,880 --> 00:09:00,080
will look at how

00:08:57,760 --> 00:09:02,000
openmp and some of the 5.2 features can

00:09:00,080 --> 00:09:03,040
be used to improve the portability of

00:09:02,000 --> 00:09:06,240
our code

00:09:03,040 --> 00:09:07,600
to future architectures so first we

00:09:06,240 --> 00:09:10,160
start off by looking at

00:09:07,600 --> 00:09:11,600
how to perform 3d fourier transforms uh

00:09:10,160 --> 00:09:14,000
using the gpus

00:09:11,600 --> 00:09:15,680
we consider a simple cubic domain with n

00:09:14,000 --> 00:09:17,680
grid points in each direction as shown

00:09:15,680 --> 00:09:19,839
here on the right

00:09:17,680 --> 00:09:22,320
we use a 1d domain decomposition or a

00:09:19,839 --> 00:09:24,720
slabs domain decomposition where

00:09:22,320 --> 00:09:26,080
the cubic domain is divided in one

00:09:24,720 --> 00:09:28,800
particular direction

00:09:26,080 --> 00:09:30,240
in the z direction here among the p mpi

00:09:28,800 --> 00:09:32,240
parallel processes

00:09:30,240 --> 00:09:33,760
and such that each parallel process

00:09:32,240 --> 00:09:35,920
holds one slab of data

00:09:33,760 --> 00:09:36,880
where a slab of data has multiple planes

00:09:35,920 --> 00:09:39,519
of data

00:09:36,880 --> 00:09:40,399
in it in order to perform these 3d

00:09:39,519 --> 00:09:43,279
transforms we

00:09:40,399 --> 00:09:44,640
copy and enter slab from the gpu cpu

00:09:43,279 --> 00:09:47,760
which is the host to the gpu

00:09:44,640 --> 00:09:49,279
the device and perform fourier

00:09:47,760 --> 00:09:51,760
transforms on it

00:09:49,279 --> 00:09:53,440
one direction at a time so in this case

00:09:51,760 --> 00:09:54,880
we can transform the data in the x and

00:09:53,440 --> 00:09:56,640
the y directions directly as they are

00:09:54,880 --> 00:09:58,000
both local to each mpi process on the

00:09:56,640 --> 00:10:00,000
gpu

00:09:58,000 --> 00:10:01,920
in order to transform data in the z

00:10:00,000 --> 00:10:04,000
direction we would first have to

00:10:01,920 --> 00:10:06,480
transpose the data that is con convert

00:10:04,000 --> 00:10:08,560
the xy slabs to xz slabs

00:10:06,480 --> 00:10:09,760
and then perform the transforms in the z

00:10:08,560 --> 00:10:12,000
direction

00:10:09,760 --> 00:10:13,680
uh once these transforms in the z

00:10:12,000 --> 00:10:15,600
direction are performed then we can

00:10:13,680 --> 00:10:17,600
finally copy the data out from the gpu

00:10:15,600 --> 00:10:19,120
to the cpu in this approach

00:10:17,600 --> 00:10:20,720
the operations are performed in a

00:10:19,120 --> 00:10:22,800
synchronous manner and

00:10:20,720 --> 00:10:24,399
also it requires the entire slab of data

00:10:22,800 --> 00:10:26,079
to be available on the gpu which

00:10:24,399 --> 00:10:28,959
limits the problem size that can be

00:10:26,079 --> 00:10:30,720
reached uh so to avoid these

00:10:28,959 --> 00:10:32,000
limitations to basically overcome these

00:10:30,720 --> 00:10:33,680
limitations we

00:10:32,000 --> 00:10:35,600
have come up with a new batched

00:10:33,680 --> 00:10:37,760
asynchronous algorithm that we

00:10:35,600 --> 00:10:39,920
presented at the supercomputing 2019

00:10:37,760 --> 00:10:42,240
conference last year

00:10:39,920 --> 00:10:43,600
in that approach uh what we do is we

00:10:42,240 --> 00:10:45,279
divide the slab

00:10:43,600 --> 00:10:47,519
that each process holds into multiple

00:10:45,279 --> 00:10:50,480
pencils and process each pencil

00:10:47,519 --> 00:10:51,760
separately on the gpu where different

00:10:50,480 --> 00:10:52,720
operations can be taking place on

00:10:51,760 --> 00:10:54,640
different pencils

00:10:52,720 --> 00:10:56,480
and they can also be overlapping each

00:10:54,640 --> 00:10:59,680
other

00:10:56,480 --> 00:11:00,079
for example in this example shown here

00:10:59,680 --> 00:11:02,480
on the

00:11:00,079 --> 00:11:03,920
on the right uh when we are computing on

00:11:02,480 --> 00:11:05,839
the blue pencil

00:11:03,920 --> 00:11:07,440
we could be performing a device to host

00:11:05,839 --> 00:11:09,200
transfer on the green pencil

00:11:07,440 --> 00:11:10,480
and also a hosted device transfer on the

00:11:09,200 --> 00:11:12,640
red pencil

00:11:10,480 --> 00:11:14,000
while also doing an all to wall

00:11:12,640 --> 00:11:17,600
transpose on the

00:11:14,000 --> 00:11:19,279
brown pencil which is achieved by using

00:11:17,600 --> 00:11:22,320
a non-blocking all tool call

00:11:19,279 --> 00:11:24,399
so this way we can overlap the mpi data

00:11:22,320 --> 00:11:27,680
transfers and the computations

00:11:24,399 --> 00:11:29,440
on the gpus this code was implemented

00:11:27,680 --> 00:11:31,440
in the coda fortran and presented at the

00:11:29,440 --> 00:11:34,720
2019 supercomputing conference

00:11:31,440 --> 00:11:38,079
so in this work what we will look at is

00:11:34,720 --> 00:11:41,360
porting this code to use openmp

00:11:38,079 --> 00:11:44,560
and some of the fight auto features

00:11:41,360 --> 00:11:46,480
so the first aspect of this work is uh

00:11:44,560 --> 00:11:48,079
comes up from the need that we need to

00:11:46,480 --> 00:11:50,240
have a

00:11:48,079 --> 00:11:52,079
entire line of data available on the gpu

00:11:50,240 --> 00:11:54,480
to perform fourier transforms

00:11:52,079 --> 00:11:57,440
uh so that requires uh being able to do

00:11:54,480 --> 00:11:59,360
non-contiguous maps and strided copies

00:11:57,440 --> 00:12:00,959
so in the example shown here we have a

00:11:59,360 --> 00:12:03,600
2d plane of data

00:12:00,959 --> 00:12:04,560
and since we are working on a fortran

00:12:03,600 --> 00:12:07,040
code the

00:12:04,560 --> 00:12:09,920
memory is contiguous in the column wise

00:12:07,040 --> 00:12:11,440
direction that is along the x direction

00:12:09,920 --> 00:12:13,279
and say for example if you want to

00:12:11,440 --> 00:12:16,240
transform in the y direction

00:12:13,279 --> 00:12:16,639
we would need data a line line of data

00:12:16,240 --> 00:12:19,279
in the y

00:12:16,639 --> 00:12:20,880
direction so that would mean performing

00:12:19,279 --> 00:12:22,880
strided copies

00:12:20,880 --> 00:12:25,120
some elements in x then striding over

00:12:22,880 --> 00:12:26,639
some memory and then copying

00:12:25,120 --> 00:12:28,639
some more elements in x and striding

00:12:26,639 --> 00:12:29,440
some uh some more in memory and so on

00:12:28,639 --> 00:12:31,120
until

00:12:29,440 --> 00:12:32,959
uh all the elements in the y direction

00:12:31,120 --> 00:12:35,040
have been copied

00:12:32,959 --> 00:12:37,120
uh so this can be done in coda fortran

00:12:35,040 --> 00:12:39,839
easily by using a codem copy 2d

00:12:37,120 --> 00:12:41,360
call where we can specify unique

00:12:39,839 --> 00:12:44,240
destination and source buffers

00:12:41,360 --> 00:12:46,320
and also it enables us to specify the

00:12:44,240 --> 00:12:49,200
stride and the number of elements in the

00:12:46,320 --> 00:12:50,720
in the column direction to copy but how

00:12:49,200 --> 00:12:53,519
do we do this with openmp

00:12:50,720 --> 00:12:54,560
so a simple way would be to basically

00:12:53,519 --> 00:12:57,680
map the necessary

00:12:54,560 --> 00:13:00,320
uh elements using the map clause

00:12:57,680 --> 00:13:01,920
as shown here but such sectional maps

00:13:00,320 --> 00:13:04,800
are not find auto compliant

00:13:01,920 --> 00:13:06,720
so a workaround is to copy the necessary

00:13:04,800 --> 00:13:07,680
elements onto a smaller buffer on the

00:13:06,720 --> 00:13:09,360
host itself

00:13:07,680 --> 00:13:11,680
and then map that smaller buffer to the

00:13:09,360 --> 00:13:12,079
device but this requires some additional

00:13:11,680 --> 00:13:13,760
work

00:13:12,079 --> 00:13:15,279
on the host which incurs some

00:13:13,760 --> 00:13:18,959
performance penalties that

00:13:15,279 --> 00:13:22,320
we would be better off without having

00:13:18,959 --> 00:13:23,839
so what what we decide to use is a

00:13:22,320 --> 00:13:25,680
device memory routine that is part of

00:13:23,839 --> 00:13:28,320
the 4.5 and above standard

00:13:25,680 --> 00:13:30,000
in openmp called the openmp target mam

00:13:28,320 --> 00:13:31,839
copy rectangular

00:13:30,000 --> 00:13:33,680
which basically allows us to copy a

00:13:31,839 --> 00:13:35,920
rectangular sub volume from a larger

00:13:33,680 --> 00:13:38,079
multi-dimensional array

00:13:35,920 --> 00:13:39,839
but this according to the openmp

00:13:38,079 --> 00:13:40,720
standard is only callable from c and c

00:13:39,839 --> 00:13:43,040
plus plus

00:13:40,720 --> 00:13:45,600
so we make use of a c4 run interface to

00:13:43,040 --> 00:13:47,519
be a callable from fortran

00:13:45,600 --> 00:13:49,199
and we also need to take care of the

00:13:47,519 --> 00:13:50,079
fact that c and photon have different

00:13:49,199 --> 00:13:52,399
ordering

00:13:50,079 --> 00:13:54,560
where c is row ordering and fortran is

00:13:52,399 --> 00:13:56,000
called a major ordering

00:13:54,560 --> 00:13:57,680
so that also needs to be taken into

00:13:56,000 --> 00:13:58,240
account when we pass in some parameters

00:13:57,680 --> 00:14:01,360
to the

00:13:58,240 --> 00:14:03,040
target mem copy rectangular call so here

00:14:01,360 --> 00:14:05,920
on the right we're showing a simple

00:14:03,040 --> 00:14:07,519
uh example of how to copy the data from

00:14:05,920 --> 00:14:09,360
a larger source buffer to a smaller

00:14:07,519 --> 00:14:11,279
destination buffer here we are copying

00:14:09,360 --> 00:14:14,800
uh particularly the third pencil from

00:14:11,279 --> 00:14:17,120
the source buffer to the device buffer

00:14:14,800 --> 00:14:18,639
so the parameters for the target mam

00:14:17,120 --> 00:14:20,000
copy rectangular call are

00:14:18,639 --> 00:14:21,680
uh the destination and the source

00:14:20,000 --> 00:14:23,120
buffers similar to how the coda fortran

00:14:21,680 --> 00:14:26,160
codemen copy 2d call

00:14:23,120 --> 00:14:26,720
had the element size which is the number

00:14:26,160 --> 00:14:28,000
of

00:14:26,720 --> 00:14:30,160
what is the precision of the data that

00:14:28,000 --> 00:14:32,240
we're copying number of dimensions which

00:14:30,160 --> 00:14:34,560
is two dimensional in this case

00:14:32,240 --> 00:14:36,000
uh the volume of data being copied so in

00:14:34,560 --> 00:14:38,399
this case we are copying the

00:14:36,000 --> 00:14:39,760
number all the elements in the row wise

00:14:38,399 --> 00:14:40,959
direction and some elements in the

00:14:39,760 --> 00:14:43,120
column wise direction

00:14:40,959 --> 00:14:45,040
so that would be the volume the offset

00:14:43,120 --> 00:14:46,959
would be the number of parameters

00:14:45,040 --> 00:14:48,720
or the number of elements uh from the

00:14:46,959 --> 00:14:51,120
beginning of each dimension

00:14:48,720 --> 00:14:52,240
so for for this example since we're

00:14:51,120 --> 00:14:55,279
copying the third pencil

00:14:52,240 --> 00:14:56,880
the offset for the in the column wise

00:14:55,279 --> 00:14:58,639
direction would be two pencil widths

00:14:56,880 --> 00:15:00,240
that is two and xp

00:14:58,639 --> 00:15:01,600
and then in the row wise direction since

00:15:00,240 --> 00:15:02,480
we're copying from the very first

00:15:01,600 --> 00:15:05,600
element in the row

00:15:02,480 --> 00:15:05,600
the offset is zero

00:15:05,760 --> 00:15:08,880
and then the last set of parameters is

00:15:07,199 --> 00:15:09,519
the dimensions which is basically the

00:15:08,880 --> 00:15:10,800
num

00:15:09,519 --> 00:15:12,639
number of elements in the row and the

00:15:10,800 --> 00:15:15,040
column wise direction for the source

00:15:12,639 --> 00:15:17,519
buffer and the destination buffer

00:15:15,040 --> 00:15:18,720
so this mem copy rectangular call is

00:15:17,519 --> 00:15:21,839
blocking in nature

00:15:18,720 --> 00:15:21,839
so in order to perform

00:15:22,959 --> 00:15:27,680
we use the 5.1 feature which is the mem

00:15:25,279 --> 00:15:29,440
copy rectangular async

00:15:27,680 --> 00:15:32,079
that enables the use of dependency

00:15:29,440 --> 00:15:34,000
objects which helps

00:15:32,079 --> 00:15:35,759
synchronizing with other tasks and makes

00:15:34,000 --> 00:15:37,440
it more easier to work with other

00:15:35,759 --> 00:15:38,959
openmp tasks that are computing or

00:15:37,440 --> 00:15:42,720
transferring data to the device

00:15:38,959 --> 00:15:43,040
and back so the next aspect of this work

00:15:42,720 --> 00:15:44,720
is

00:15:43,040 --> 00:15:46,160
uh looking at the interoperability

00:15:44,720 --> 00:15:47,199
between openmp and non-blocking

00:15:46,160 --> 00:15:49,759
libraries

00:15:47,199 --> 00:15:51,759
uh this is because we perform a lot of

00:15:49,759 --> 00:15:52,839
code fft or rock fft library calls in

00:15:51,759 --> 00:15:55,759
the code

00:15:52,839 --> 00:15:57,040
and uh in in this example we are showing

00:15:55,759 --> 00:15:58,959
how to perform

00:15:57,040 --> 00:16:00,560
a simple forward and inverse transform

00:15:58,959 --> 00:16:03,759
on a set of data

00:16:00,560 --> 00:16:04,480
uh using a non-blocking stream so here

00:16:03,759 --> 00:16:06,720
we have a

00:16:04,480 --> 00:16:07,680
task within which these fft calls are

00:16:06,720 --> 00:16:11,040
being

00:16:07,680 --> 00:16:13,920
called and we call this task as task a

00:16:11,040 --> 00:16:15,519
and then after that we have on this

00:16:13,920 --> 00:16:16,160
transform data we're performing some

00:16:15,519 --> 00:16:18,240
computation

00:16:16,160 --> 00:16:21,519
basically scaling the data by the number

00:16:18,240 --> 00:16:21,519
of elements in the x direction

00:16:21,600 --> 00:16:24,880
the dependencies are set up such that

00:16:24,079 --> 00:16:27,360
task

00:16:24,880 --> 00:16:28,320
b will only execute after task a and or

00:16:27,360 --> 00:16:30,800
that is what is

00:16:28,320 --> 00:16:31,600
the intention of setting it up this way

00:16:30,800 --> 00:16:33,360
uh but

00:16:31,600 --> 00:16:34,720
in practice what we what we have seen is

00:16:33,360 --> 00:16:36,240
that the task is

00:16:34,720 --> 00:16:38,079
considered to be complete as soon as

00:16:36,240 --> 00:16:40,560
these ffts are launched

00:16:38,079 --> 00:16:41,120
by the the thread that is working on the

00:16:40,560 --> 00:16:43,279
stars

00:16:41,120 --> 00:16:45,199
to the device it does not wait for these

00:16:43,279 --> 00:16:46,639
ffts to actually start executing on the

00:16:45,199 --> 00:16:48,959
device

00:16:46,639 --> 00:16:49,839
so what that leads to is the task b is

00:16:48,959 --> 00:16:51,759
actually

00:16:49,839 --> 00:16:53,440
started before these ffts actually

00:16:51,759 --> 00:16:55,680
complete running on the device and that

00:16:53,440 --> 00:16:58,079
gives us some incorrect results

00:16:55,680 --> 00:16:59,920
so we can see from this simple example

00:16:58,079 --> 00:17:00,800
that task dependency is not sufficient

00:16:59,920 --> 00:17:03,839
to enforce the

00:17:00,800 --> 00:17:06,000
required synchronization

00:17:03,839 --> 00:17:07,199
we make use of a 5.0 feature called the

00:17:06,000 --> 00:17:10,400
detach clause

00:17:07,199 --> 00:17:14,000
by adding this detach clause to the task

00:17:10,400 --> 00:17:16,640
construct and then the ffts are launched

00:17:14,000 --> 00:17:18,720
similar to what we've seen before but we

00:17:16,640 --> 00:17:20,559
also add a hipstream add callback

00:17:18,720 --> 00:17:22,079
function here which introduces a

00:17:20,559 --> 00:17:23,439
callback into the stream in which the

00:17:22,079 --> 00:17:25,199
ffts are executing

00:17:23,439 --> 00:17:26,959
so now what happens is that once these

00:17:25,199 --> 00:17:29,440
ffts are launched

00:17:26,959 --> 00:17:31,120
uh the task is still uh considered

00:17:29,440 --> 00:17:31,919
incomplete because this event is still

00:17:31,120 --> 00:17:34,480
not

00:17:31,919 --> 00:17:36,000
fulfilled so once these fft is actually

00:17:34,480 --> 00:17:37,760
finished running on the gpu

00:17:36,000 --> 00:17:39,600
the callback function that we have

00:17:37,760 --> 00:17:41,760
introduced into the stream is now

00:17:39,600 --> 00:17:43,679
executed and this callback function

00:17:41,760 --> 00:17:46,320
basically fulfills the event

00:17:43,679 --> 00:17:47,360
that frees this dependency on this task

00:17:46,320 --> 00:17:49,760
that

00:17:47,360 --> 00:17:51,600
later ensures that the task b which had

00:17:49,760 --> 00:17:52,240
a dependency on the task a can now

00:17:51,600 --> 00:17:54,240
execute

00:17:52,240 --> 00:17:55,679
so this way we are ensuring that task b

00:17:54,240 --> 00:17:57,840
does not begin before

00:17:55,679 --> 00:17:58,799
the fft is actually finished running on

00:17:57,840 --> 00:18:01,440
the gpu

00:17:58,799 --> 00:18:02,160
meanwhile we could have some operations

00:18:01,440 --> 00:18:04,559
on

00:18:02,160 --> 00:18:05,679
in task c occurring at the same time

00:18:04,559 --> 00:18:08,720
when these ffts are

00:18:05,679 --> 00:18:08,720
executing on the device

00:18:08,960 --> 00:18:12,799
so this way we can use the detach clause

00:18:10,720 --> 00:18:13,679
to enforce the required synchronization

00:18:12,799 --> 00:18:17,360
and also

00:18:13,679 --> 00:18:20,400
introduce some asynchronism in the code

00:18:17,360 --> 00:18:21,760
next we look at uh porting the batched

00:18:20,400 --> 00:18:23,440
asynchronous quota for trend code to

00:18:21,760 --> 00:18:24,799
openmp on the left we have the coda

00:18:23,440 --> 00:18:25,840
fortran code and on the right we have

00:18:24,799 --> 00:18:27,200
the openmp code

00:18:25,840 --> 00:18:28,640
so to quickly walk through the coda

00:18:27,200 --> 00:18:30,160
fortran code the operations that we're

00:18:28,640 --> 00:18:34,080
doing here is a

00:18:30,160 --> 00:18:37,919
2d copy from the host to the device ffts

00:18:34,080 --> 00:18:40,080
on a buffer and then the 2d copies again

00:18:37,919 --> 00:18:42,480
back from the device to the host

00:18:40,080 --> 00:18:43,360
while also packing the data making it

00:18:42,480 --> 00:18:46,640
ready for the

00:18:43,360 --> 00:18:49,039
all tool transpose that's coming next uh

00:18:46,640 --> 00:18:51,120
the necessary synchronization is set up

00:18:49,039 --> 00:18:52,720
by using coda stream weight event and

00:18:51,120 --> 00:18:55,600
koda event record

00:18:52,720 --> 00:18:56,480
uh such that say for example if we are

00:18:55,600 --> 00:18:59,679
performing the

00:18:56,480 --> 00:19:03,039
ffts on the current buffer curr uh

00:18:59,679 --> 00:19:05,760
that is only started as soon as the

00:19:03,039 --> 00:19:07,679
device host to device copy has been

00:19:05,760 --> 00:19:09,679
finished on that buffer

00:19:07,679 --> 00:19:11,360
uh setup by this coda stream weight

00:19:09,679 --> 00:19:13,679
event and similarly

00:19:11,360 --> 00:19:15,760
uh the device to host copy is not

00:19:13,679 --> 00:19:17,600
started before the

00:19:15,760 --> 00:19:19,200
the computations on that previous buffer

00:19:17,600 --> 00:19:20,720
is actually finished

00:19:19,200 --> 00:19:22,080
to ensure that the we are not copying

00:19:20,720 --> 00:19:23,280
the data out before it is ready to be

00:19:22,080 --> 00:19:26,160
copied out

00:19:23,280 --> 00:19:28,000
so this is done similarly in the openmp

00:19:26,160 --> 00:19:31,360
by using the task depends

00:19:28,000 --> 00:19:34,000
with an in and out of dependencies

00:19:31,360 --> 00:19:36,240
and the kodam copy 2ds are replaced by

00:19:34,000 --> 00:19:38,080
the omp target mem copy rectangular

00:19:36,240 --> 00:19:40,240
so this way we we can port the coda

00:19:38,080 --> 00:19:42,400
fortran code to openmp and also want to

00:19:40,240 --> 00:19:44,480
point out here that the detach clause is

00:19:42,400 --> 00:19:47,440
attached to the task that the ffts are

00:19:44,480 --> 00:19:47,440
being executed from

00:19:48,400 --> 00:19:52,240
so to briefly present the performance

00:19:50,240 --> 00:19:53,200
numbers from a non-batch synchronous

00:19:52,240 --> 00:19:55,280
code

00:19:53,200 --> 00:19:56,720
we have collected these numbers on

00:19:55,280 --> 00:19:59,360
summit

00:19:56,720 --> 00:20:00,799
using up to thousand nodes and with one

00:19:59,360 --> 00:20:02,559
task per gpu

00:20:00,799 --> 00:20:04,159
the timing provided here in the table is

00:20:02,559 --> 00:20:05,520
for three pairs of forward and inverse

00:20:04,159 --> 00:20:07,919
transforms

00:20:05,520 --> 00:20:09,600
we have data for uh cpu version which

00:20:07,919 --> 00:20:11,200
used all the codes per node

00:20:09,600 --> 00:20:12,720
the code of fortran version and the

00:20:11,200 --> 00:20:14,400
openmp version

00:20:12,720 --> 00:20:16,320
uh we can see that the koda fortran and

00:20:14,400 --> 00:20:18,480
the openmp version are both performing

00:20:16,320 --> 00:20:21,600
similarly with respect to each other

00:20:18,480 --> 00:20:23,039
and are showing a speed up of around 2.6

00:20:21,600 --> 00:20:25,039
compared to the cpu version which used

00:20:23,039 --> 00:20:28,080
all the cores on the node

00:20:25,039 --> 00:20:29,520
for the problem size of 12k cube uh we

00:20:28,080 --> 00:20:29,919
would also like to point out here that

00:20:29,520 --> 00:20:31,440
the

00:20:29,919 --> 00:20:33,360
turbulent simulation code which is a

00:20:31,440 --> 00:20:35,280
full-fledged dns simulation code which

00:20:33,360 --> 00:20:39,039
used coda fortran

00:20:35,280 --> 00:20:40,799
can run on uh 3000 nodes of summit

00:20:39,039 --> 00:20:42,480
and can simulate a problem size of

00:20:40,799 --> 00:20:43,600
around 18 000 grid points in each

00:20:42,480 --> 00:20:46,000
direction

00:20:43,600 --> 00:20:47,280
and this has been shown speed up of

00:20:46,000 --> 00:20:48,960
around 3x

00:20:47,280 --> 00:20:50,480
compared to the cpu code which used all

00:20:48,960 --> 00:20:52,640
the nodes per node all the cores per

00:20:50,480 --> 00:20:55,520
node sorry

00:20:52,640 --> 00:20:57,120
we don't yet have performance numbers

00:20:55,520 --> 00:20:58,320
for the batched asynchronous code using

00:20:57,120 --> 00:21:00,799
openmp

00:20:58,320 --> 00:21:02,240
this is because one of the reasons is

00:21:00,799 --> 00:21:03,520
that the openmp target mam copy

00:21:02,240 --> 00:21:04,799
rectangular call which is used to

00:21:03,520 --> 00:21:07,520
perform the strided copies

00:21:04,799 --> 00:21:09,039
are uh slow and uh compared to the coda

00:21:07,520 --> 00:21:10,320
memcop 2d call and we are currently

00:21:09,039 --> 00:21:12,559
working on trying to

00:21:10,320 --> 00:21:13,360
get better performance out of it and

00:21:12,559 --> 00:21:15,120
also the

00:21:13,360 --> 00:21:16,640
asynchronous features that are enabled

00:21:15,120 --> 00:21:18,240
using the detach clause are still under

00:21:16,640 --> 00:21:20,960
development as it's a

00:21:18,240 --> 00:21:22,240
relatively new feature uh from openmp

00:21:20,960 --> 00:21:24,320
5.2

00:21:22,240 --> 00:21:26,240
uh in regards to portability we'd like

00:21:24,320 --> 00:21:28,000
to point out here that the openmp code

00:21:26,240 --> 00:21:30,000
actually can compile and run on both

00:21:28,000 --> 00:21:32,480
nvidia and amd gpus which makes it

00:21:30,000 --> 00:21:34,720
a portable code to our various

00:21:32,480 --> 00:21:37,520
architectures

00:21:34,720 --> 00:21:37,840
to conclude uh i'd like to say that the

00:21:37,520 --> 00:21:40,720
uh

00:21:37,840 --> 00:21:42,320
using this high uh batch asynchronous

00:21:40,720 --> 00:21:44,240
algorithm we have been able to perform

00:21:42,320 --> 00:21:45,919
direct numerical simulations at record

00:21:44,240 --> 00:21:48,240
setting resolutions

00:21:45,919 --> 00:21:49,280
that has enabled us to get high fidelity

00:21:48,240 --> 00:21:51,600
results on

00:21:49,280 --> 00:21:53,760
intermittency and extreme events at very

00:21:51,600 --> 00:21:55,520
high reynolds numbers

00:21:53,760 --> 00:21:57,120
we are also able to perform some

00:21:55,520 --> 00:21:58,000
computationally demanding studies of

00:21:57,120 --> 00:22:00,480
turbulent mixing

00:21:58,000 --> 00:22:02,400
and particle dispersion using this batch

00:22:00,480 --> 00:22:04,799
asynchronous code

00:22:02,400 --> 00:22:05,440
we are currently working on developing a

00:22:04,799 --> 00:22:08,400
3d

00:22:05,440 --> 00:22:09,679
fft kernel using the openmp 5.0 features

00:22:08,400 --> 00:22:11,919
to target gpus

00:22:09,679 --> 00:22:13,360
at present we have a batch synchronous

00:22:11,919 --> 00:22:14,960
version which enables running large

00:22:13,360 --> 00:22:17,440
problem sizes

00:22:14,960 --> 00:22:19,120
but we are working on porting the

00:22:17,440 --> 00:22:19,440
batched asynchronous code of photon code

00:22:19,120 --> 00:22:22,000
to

00:22:19,440 --> 00:22:22,559
openmp in the future we'd like to

00:22:22,000 --> 00:22:24,960
continue

00:22:22,559 --> 00:22:27,120
this boarding effort and to be able to

00:22:24,960 --> 00:22:27,840
run very large problem sizes even beyond

00:22:27,120 --> 00:22:29,600
the current

00:22:27,840 --> 00:22:31,440
uh largest resolution that is possible

00:22:29,600 --> 00:22:32,720
on summit which is 18 000 bit points in

00:22:31,440 --> 00:22:34,480
each direction

00:22:32,720 --> 00:22:36,480
for that we need the batch asynchronous

00:22:34,480 --> 00:22:37,960
algorithm uh to get the optimal

00:22:36,480 --> 00:22:41,440
performance which uses

00:22:37,960 --> 00:22:43,440
openmp5.detach feature and also be

00:22:41,440 --> 00:22:44,960
able to perform fast strided copies

00:22:43,440 --> 00:22:47,760
between the

00:22:44,960 --> 00:22:49,440
device and the host uh using the target

00:22:47,760 --> 00:22:50,559
mam copy rectangular call and the

00:22:49,440 --> 00:22:54,080
asynchronous

00:22:50,559 --> 00:22:56,480
call of target man copy rectangular

00:22:54,080 --> 00:23:00,159
with that i would like to conclude our

00:22:56,480 --> 00:23:00,159
presentation and thank everyone for your

00:23:00,760 --> 00:23:03,760

YouTube URL: https://www.youtube.com/watch?v=qHIRjGFBhsY


