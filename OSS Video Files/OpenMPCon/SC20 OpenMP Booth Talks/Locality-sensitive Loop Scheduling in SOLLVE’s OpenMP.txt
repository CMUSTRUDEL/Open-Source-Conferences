Title: Locality-sensitive Loop Scheduling in SOLLVEâ€™s OpenMP
Publication date: 2020-11-06
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Vivek Kale of Brookhaven National Laboratory, is part of the OpenMP Booth Talk series created for Supercomputing 2020. A PDF of this presentation as well as more videos from this series can be downloaded at link.openmp.org/sc20
Captions: 
	00:00:00,960 --> 00:00:06,720
hi i'm vivek

00:00:03,760 --> 00:00:08,639
and i'm from brookhaven national lab i'm

00:00:06,720 --> 00:00:09,120
going to talk with you today about my

00:00:08,639 --> 00:00:12,960
work

00:00:09,120 --> 00:00:16,480
on locality sensitive loop scheduling

00:00:12,960 --> 00:00:20,320
within the solve project for

00:00:16,480 --> 00:00:20,320
in the hexascale computing project

00:00:20,800 --> 00:00:24,880
so before i go into detail on the

00:00:23,199 --> 00:00:27,519
scheduling strategies

00:00:24,880 --> 00:00:30,240
let me start with providing you an

00:00:27,519 --> 00:00:33,120
overview of the us department of energy

00:00:30,240 --> 00:00:36,160
solve project that this work is part of

00:00:33,120 --> 00:00:39,520
solve is a project that

00:00:36,160 --> 00:00:42,640
is used is used to develop openmp for

00:00:39,520 --> 00:00:46,079
doe exoscale supercomputers

00:00:42,640 --> 00:00:49,280
it is a fork of the llvm project

00:00:46,079 --> 00:00:52,000
it's part of a a large

00:00:49,280 --> 00:00:53,760
effort in the exoskeletal computing

00:00:52,000 --> 00:00:56,320
project project

00:00:53,760 --> 00:00:57,039
led by doe and one can link solves

00:00:56,320 --> 00:00:58,960
openmp

00:00:57,039 --> 00:01:00,800
implementation to their application

00:00:58,960 --> 00:01:03,199
program by downloading

00:01:00,800 --> 00:01:04,720
the openmp implementation at the github

00:01:03,199 --> 00:01:07,680
link shown

00:01:04,720 --> 00:01:09,360
it can also be used as a package on ecp

00:01:07,680 --> 00:01:12,560
systems like summit

00:01:09,360 --> 00:01:15,200
by downloading it via spec

00:01:12,560 --> 00:01:16,000
the solve areas of impact are shown in

00:01:15,200 --> 00:01:19,280
this 3d

00:01:16,000 --> 00:01:22,560
cube figure below and

00:01:19,280 --> 00:01:25,600
the as noted the salt

00:01:22,560 --> 00:01:28,240
project aims at an openmp

00:01:25,600 --> 00:01:30,960
that provides for portability

00:01:28,240 --> 00:01:33,040
productivity and performance on ecp

00:01:30,960 --> 00:01:36,159
system

00:01:33,040 --> 00:01:41,840
moreover one can see the key components

00:01:36,159 --> 00:01:45,360
of as slivers in this solve slab

00:01:41,840 --> 00:01:48,560
this the solve slab particularly has

00:01:45,360 --> 00:01:51,920
a component of runtimes and compilers

00:01:48,560 --> 00:01:54,320
and our our runtime syst

00:01:51,920 --> 00:01:55,520
and our work that we are going to talk

00:01:54,320 --> 00:01:59,040
about today

00:01:55,520 --> 00:02:02,719
is in the main runtime

00:01:59,040 --> 00:02:06,399
systems liver as well as partly

00:02:02,719 --> 00:02:08,959
the compiler slower now with that

00:02:06,399 --> 00:02:10,239
the diagram on the right shows the solve

00:02:08,959 --> 00:02:12,720
software ecosystem

00:02:10,239 --> 00:02:13,520
and one can most clearly see its

00:02:12,720 --> 00:02:16,720
engagement

00:02:13,520 --> 00:02:20,560
with applications vendors and the openmp

00:02:16,720 --> 00:02:23,440
arb again our strategies are focused

00:02:20,560 --> 00:02:24,160
in the compiler layer and the run time

00:02:23,440 --> 00:02:27,120
layer

00:02:24,160 --> 00:02:27,920
let's motivate our strategies here is a

00:02:27,120 --> 00:02:29,920
curricular

00:02:27,920 --> 00:02:31,360
outline of a typical scientific

00:02:29,920 --> 00:02:33,200
application program run on a

00:02:31,360 --> 00:02:36,959
supercomputer

00:02:33,200 --> 00:02:39,680
the typical structure is compute

00:02:36,959 --> 00:02:40,959
synchronize and communicate through this

00:02:39,680 --> 00:02:44,480
mpi collect call

00:02:40,959 --> 00:02:47,920
collective operation now each time

00:02:44,480 --> 00:02:50,640
step here in this outer while loop

00:02:47,920 --> 00:02:52,640
contains the opera operations of loosely

00:02:50,640 --> 00:02:54,239
synchronous communication

00:02:52,640 --> 00:02:55,680
through the through these eyesight i

00:02:54,239 --> 00:02:59,040
receive weight loss

00:02:55,680 --> 00:03:02,000
and then a computation and then

00:02:59,040 --> 00:03:03,280
a collective operation through this

00:03:02,000 --> 00:03:07,120
collective up

00:03:03,280 --> 00:03:10,800
um and now

00:03:07,120 --> 00:03:13,360
what you what you might see is often

00:03:10,800 --> 00:03:16,159
there's many interleavings of these

00:03:13,360 --> 00:03:19,519
types of compute communicate

00:03:16,159 --> 00:03:22,159
patterns in a real application so

00:03:19,519 --> 00:03:24,879
to be to ensure that the application's

00:03:22,159 --> 00:03:28,080
performance is good at scale

00:03:24,879 --> 00:03:29,360
there's this red inner loop that needs

00:03:28,080 --> 00:03:32,480
to be

00:03:29,360 --> 00:03:35,760
particularly well performing

00:03:32,480 --> 00:03:40,000
and in particular it needs to

00:03:35,760 --> 00:03:43,120
be distributed well across the

00:03:40,000 --> 00:03:46,319
processing elements of a node and that

00:03:43,120 --> 00:03:48,400
involves loop parallelization

00:03:46,319 --> 00:03:50,000
that loop parallelization should

00:03:48,400 --> 00:03:53,040
distribute the

00:03:50,000 --> 00:03:56,879
prop work across processing

00:03:53,040 --> 00:04:00,640
elements in a near uniform

00:03:56,879 --> 00:04:04,080
way and it should do so as efficiently

00:04:00,640 --> 00:04:06,959
as possible so how do we do near

00:04:04,080 --> 00:04:08,000
perfect work distribution within a node

00:04:06,959 --> 00:04:11,200
well

00:04:08,000 --> 00:04:14,480
we need to focus here on this

00:04:11,200 --> 00:04:17,919
computation region and that

00:04:14,480 --> 00:04:21,680
that we've do we focus we

00:04:17,919 --> 00:04:24,000
focus in on by paralyzing using openmp

00:04:21,680 --> 00:04:24,960
particularly the openmp parallel work

00:04:24,000 --> 00:04:28,960
sharing clause

00:04:24,960 --> 00:04:32,560
the key thing here is that we use

00:04:28,960 --> 00:04:35,919
the schedule clause on the work sharing

00:04:32,560 --> 00:04:39,479
directive and that allows

00:04:35,919 --> 00:04:42,560
us for varying how we

00:04:39,479 --> 00:04:46,320
redistribute the work across course

00:04:42,560 --> 00:04:49,199
or cross-processing elements with this

00:04:46,320 --> 00:04:53,040
we show our key approach of low overhead

00:04:49,199 --> 00:04:55,600
or locality sensitive loop scheduling

00:04:53,040 --> 00:04:56,560
the goal underlying the low overhead

00:04:55,600 --> 00:04:59,040
scheduling

00:04:56,560 --> 00:05:00,240
strategies is to efficiently mitigate

00:04:59,040 --> 00:05:03,759
within no

00:05:00,240 --> 00:05:06,560
no load imbalance of a parallel loop

00:05:03,759 --> 00:05:07,840
and the core idea behind our lower head

00:05:06,560 --> 00:05:11,280
scheduling strategies

00:05:07,840 --> 00:05:14,880
is shown in the bottom diagram here

00:05:11,280 --> 00:05:16,880
and that is that each thread

00:05:14,880 --> 00:05:18,800
first executes some number of

00:05:16,880 --> 00:05:22,639
pre-assigned loop iterations

00:05:18,800 --> 00:05:25,759
statically shown in blue and then

00:05:22,639 --> 00:05:28,479
immediately continues to retrieve

00:05:25,759 --> 00:05:28,880
the remaining looper iterations from a

00:05:28,479 --> 00:05:32,639
work

00:05:28,880 --> 00:05:34,720
queue shared among threads and

00:05:32,639 --> 00:05:36,560
these iterations are dynamically

00:05:34,720 --> 00:05:38,880
executed loop iteration

00:05:36,560 --> 00:05:40,479
so we refer to the total number of

00:05:38,880 --> 00:05:43,440
pre-assigned

00:05:40,479 --> 00:05:44,240
loop iterations across all threads or

00:05:43,440 --> 00:05:46,639
cores

00:05:44,240 --> 00:05:47,919
or processing elements as the static

00:05:46,639 --> 00:05:50,840
fraction

00:05:47,919 --> 00:05:52,639
we refer to the remaining number of

00:05:50,840 --> 00:05:55,759
dynamically allocated loop

00:05:52,639 --> 00:05:58,400
iterations as a dynamic fraction and

00:05:55,759 --> 00:06:00,479
whatever method is used to determine the

00:05:58,400 --> 00:06:03,680
dynamic fraction for a particular

00:06:00,479 --> 00:06:04,560
open mp loop having this ability to tune

00:06:03,680 --> 00:06:07,759
the dynamic

00:06:04,560 --> 00:06:08,720
fraction enables these locality

00:06:07,759 --> 00:06:11,919
sensitive

00:06:08,720 --> 00:06:14,240
and low overhead scheduling strategies

00:06:11,919 --> 00:06:15,680
that maximize load balance and minimize

00:06:14,240 --> 00:06:19,039
data movement

00:06:15,680 --> 00:06:22,080
we applied this strategy to a dense

00:06:19,039 --> 00:06:25,440
linear algebra code

00:06:22,080 --> 00:06:28,880
and it is currently part of the

00:06:25,440 --> 00:06:32,400
ecp application library

00:06:28,880 --> 00:06:35,680
called slate what we've done is split

00:06:32,400 --> 00:06:39,120
the the communication a

00:06:35,680 --> 00:06:42,720
lu factorization into two parts

00:06:39,120 --> 00:06:45,759
a static the part and dynamic part and

00:06:42,720 --> 00:06:49,120
then student the dynamic fraction

00:06:45,759 --> 00:06:50,720
and by doing so we get a significant

00:06:49,120 --> 00:06:54,400
performance improvement

00:06:50,720 --> 00:06:56,080
over intel mkl we see that through the

00:06:54,400 --> 00:06:59,759
results on the

00:06:56,080 --> 00:07:00,560
left we also see on the results on the

00:06:59,759 --> 00:07:03,680
right

00:07:00,560 --> 00:07:07,039
that is the histogram diagram that

00:07:03,680 --> 00:07:10,560
the performance variability

00:07:07,039 --> 00:07:13,919
of using a mixed static dynamic

00:07:10,560 --> 00:07:17,440
scheduling scheme is a reduced

00:07:13,919 --> 00:07:21,280
and that improves the scaling when you

00:07:17,440 --> 00:07:24,479
run this code at on millions of nodes

00:07:21,280 --> 00:07:27,599
about paper was published on this work

00:07:24,479 --> 00:07:31,199
um with my collaborator simply stun fact

00:07:27,599 --> 00:07:34,400
and i've shown the citation here

00:07:31,199 --> 00:07:38,160
with that we also we have

00:07:34,400 --> 00:07:41,199
used a similar approach

00:07:38,160 --> 00:07:44,319
for improving

00:07:41,199 --> 00:07:45,039
locality through the use of affinity

00:07:44,319 --> 00:07:48,160
hints

00:07:45,039 --> 00:07:51,840
and in particular we use affinity

00:07:48,160 --> 00:07:55,039
hints for a task loop

00:07:51,840 --> 00:07:56,160
uh for loop so it's similar to the

00:07:55,039 --> 00:07:59,440
regular

00:07:56,160 --> 00:08:02,080
regular parallel for with schedule um

00:07:59,440 --> 00:08:04,000
except it just breaks down the loop into

00:08:02,080 --> 00:08:07,280
tasks rather than chunks

00:08:04,000 --> 00:08:12,080
and we experimented with this uh

00:08:07,280 --> 00:08:13,680
idea on two application codes

00:08:12,080 --> 00:08:15,440
one is a generalized matrix

00:08:13,680 --> 00:08:19,120
multiplication code

00:08:15,440 --> 00:08:22,240
and the other one is the a jacobi 2d

00:08:19,120 --> 00:08:25,440
what we experimented with are our

00:08:22,240 --> 00:08:27,360
different metrics so we show the plot on

00:08:25,440 --> 00:08:28,879
the the two left plots show extra

00:08:27,360 --> 00:08:32,240
execution time

00:08:28,879 --> 00:08:35,279
and the and the cash traffic volume

00:08:32,240 --> 00:08:38,399
and what this shows is that for

00:08:35,279 --> 00:08:41,200
just the jacobi 2d the

00:08:38,399 --> 00:08:41,760
jacobi 2d has a significant sensitivity

00:08:41,200 --> 00:08:43,919
to

00:08:41,760 --> 00:08:44,800
using the locality aware scheduling

00:08:43,919 --> 00:08:47,200
scheme

00:08:44,800 --> 00:08:48,560
and it has a more significant

00:08:47,200 --> 00:08:52,000
sensitivity

00:08:48,560 --> 00:08:55,600
when we have larger problem size

00:08:52,000 --> 00:08:58,880
the interesting thing though to really

00:08:55,600 --> 00:09:01,680
see uh is actually the lower

00:08:58,880 --> 00:09:02,160
energy consumption shown on the right we

00:09:01,680 --> 00:09:04,240
showed

00:09:02,160 --> 00:09:06,839
we see that both the matrix

00:09:04,240 --> 00:09:09,760
multiplication and the jacobi 2d

00:09:06,839 --> 00:09:13,440
are uh actually uh have

00:09:09,760 --> 00:09:16,959
less consume less energy on the

00:09:13,440 --> 00:09:20,000
processor than their uh

00:09:16,959 --> 00:09:20,560
their baseline counterparts now with

00:09:20,000 --> 00:09:23,600
that

00:09:20,560 --> 00:09:27,040
we shift our attention to

00:09:23,600 --> 00:09:30,320
uh to the heater agenity

00:09:27,040 --> 00:09:33,279
of the nodes on supercomputers

00:09:30,320 --> 00:09:34,560
particularly exascale supercomputers on

00:09:33,279 --> 00:09:37,600
that does

00:09:34,560 --> 00:09:41,120
provide systems like summit have

00:09:37,600 --> 00:09:44,160
about six gp have six gpus

00:09:41,120 --> 00:09:47,440
per node and most about 90

00:09:44,160 --> 00:09:51,519
of their computational power on the node

00:09:47,440 --> 00:09:54,560
is from the gpus so it's important to

00:09:51,519 --> 00:09:57,600
make use of these gpus efficiently

00:09:54,560 --> 00:10:00,959
so we have worked on approach

00:09:57,600 --> 00:10:03,920
uh recently in which we use openmp

00:10:00,959 --> 00:10:04,560
to paralyze across the gpus and then to

00:10:03,920 --> 00:10:06,880
use

00:10:04,560 --> 00:10:07,600
and then on top of that use dynamic load

00:10:06,880 --> 00:10:10,240
balancing

00:10:07,600 --> 00:10:11,040
across those gpus through openmp's

00:10:10,240 --> 00:10:14,880
constructs

00:10:11,040 --> 00:10:18,720
task loops and target we found

00:10:14,880 --> 00:10:21,680
actually a very timely application

00:10:18,720 --> 00:10:22,320
on the last several months particularly

00:10:21,680 --> 00:10:25,279
a

00:10:22,320 --> 00:10:25,760
molecular docking code called autodoc

00:10:25,279 --> 00:10:28,959
which

00:10:25,760 --> 00:10:32,000
is currently being used

00:10:28,959 --> 00:10:35,200
on summit to identify

00:10:32,000 --> 00:10:39,040
potential vaccines for the

00:10:35,200 --> 00:10:41,920
disease covet 19. so for this

00:10:39,040 --> 00:10:42,880
we created a mini app to study our

00:10:41,920 --> 00:10:46,399
approach

00:10:42,880 --> 00:10:50,000
for openmp based on

00:10:46,399 --> 00:10:55,360
for the first single gpus

00:10:50,000 --> 00:10:58,880
and then we extended that to a multi-gpu

00:10:55,360 --> 00:11:02,320
multi-gpu mini app what we see

00:10:58,880 --> 00:11:05,920
on this slide on the right here

00:11:02,320 --> 00:11:09,200
is a plot on the lower

00:11:05,920 --> 00:11:09,440
but the x-axis is the computation number

00:11:09,200 --> 00:11:12,000
or

00:11:09,440 --> 00:11:14,200
target region number and the y-axis is

00:11:12,000 --> 00:11:15,680
the time and we see this large number of

00:11:14,200 --> 00:11:18,880
fluctuations

00:11:15,680 --> 00:11:20,560
in the computation number and that

00:11:18,880 --> 00:11:22,480
is indicative of the large load

00:11:20,560 --> 00:11:25,839
imbalances uh

00:11:22,480 --> 00:11:30,000
with of each task

00:11:25,839 --> 00:11:32,240
so here the the real question

00:11:30,000 --> 00:11:35,120
is how do we mitigate the load

00:11:32,240 --> 00:11:38,160
imbalances across the gpus

00:11:35,120 --> 00:11:39,040
we talked about across cores before but

00:11:38,160 --> 00:11:43,040
now

00:11:39,040 --> 00:11:44,560
we need to treat a mult multiple gpus as

00:11:43,040 --> 00:11:47,600
a multi-gpu

00:11:44,560 --> 00:11:50,959
just like in multi-core um we start

00:11:47,600 --> 00:11:54,000
with an openmp offload code using

00:11:50,959 --> 00:11:57,440
one gpu and then we extend the code

00:11:54,000 --> 00:12:00,800
for multiple gpus by having

00:11:57,440 --> 00:12:03,920
a task loop surround the

00:12:00,800 --> 00:12:06,639
key computational loop and the test loop

00:12:03,920 --> 00:12:08,399
bucket the task loop tasks are shown in

00:12:06,639 --> 00:12:11,839
those red buckets

00:12:08,399 --> 00:12:14,480
from that we have a task queue

00:12:11,839 --> 00:12:17,519
distributing the ta those tasks

00:12:14,480 --> 00:12:20,320
to each of the cpus from that

00:12:17,519 --> 00:12:21,279
we have a share the cpus have a shared

00:12:20,320 --> 00:12:24,560
queue

00:12:21,279 --> 00:12:25,920
that represents the which gpus are ready

00:12:24,560 --> 00:12:29,600
to execute

00:12:25,920 --> 00:12:32,800
one of their own tasks and then

00:12:29,600 --> 00:12:33,760
when a cpu finds a gpu in that shared

00:12:32,800 --> 00:12:36,800
ready queue

00:12:33,760 --> 00:12:40,399
it acquires that gpu then

00:12:36,800 --> 00:12:43,440
launches the next task and has us holds

00:12:40,399 --> 00:12:46,800
a signal or flag until

00:12:43,440 --> 00:12:49,839
the gpu has finished computing

00:12:46,800 --> 00:12:53,680
that particular task so this

00:12:49,839 --> 00:12:57,120
is the method that we use

00:12:53,680 --> 00:13:00,240
for assigning these several

00:12:57,120 --> 00:13:03,680
tasks to the multiple gpus

00:13:00,240 --> 00:13:06,720
on a node so question is

00:13:03,680 --> 00:13:10,160
now how do we how does the application

00:13:06,720 --> 00:13:12,720
programmer actually use our strategy

00:13:10,160 --> 00:13:13,279
well this is what the the code on the

00:13:12,720 --> 00:13:16,320
left

00:13:13,279 --> 00:13:17,600
shows what they do to modify their

00:13:16,320 --> 00:13:20,720
application code

00:13:17,600 --> 00:13:23,920
and we and i'll just

00:13:20,720 --> 00:13:26,399
show point out that the task loop here

00:13:23,920 --> 00:13:28,480
is that tesla showed and it's paralyzing

00:13:26,399 --> 00:13:32,399
these tasks across cpus

00:13:28,480 --> 00:13:35,920
then each cpu is identifying hey

00:13:32,399 --> 00:13:38,000
which gpu do i run on and so there's a

00:13:35,920 --> 00:13:41,120
function for that and that function

00:13:38,000 --> 00:13:44,160
calls this larger scheduler function

00:13:41,120 --> 00:13:47,760
and that that function contains the

00:13:44,160 --> 00:13:50,800
shared array of gpus that are ready

00:13:47,760 --> 00:13:53,920
it's this occupancies array uh then

00:13:50,800 --> 00:13:57,040
once it's found the gpu to run on

00:13:53,920 --> 00:13:58,000
it's goes it sets the success flag and

00:13:57,040 --> 00:14:01,279
then

00:13:58,000 --> 00:14:04,720
immediately goes to run

00:14:01,279 --> 00:14:05,600
off or offload the computation that's

00:14:04,720 --> 00:14:09,279
given

00:14:05,600 --> 00:14:13,040
to device dev and it does so

00:14:09,279 --> 00:14:15,680
by using the device clause in openmp

00:14:13,040 --> 00:14:16,240
then this is the core computational work

00:14:15,680 --> 00:14:19,279
of

00:14:16,240 --> 00:14:20,560
of the application then once this

00:14:19,279 --> 00:14:23,839
computation is done

00:14:20,560 --> 00:14:26,959
that thread the the cpu

00:14:23,839 --> 00:14:30,240
which is managing a thread will set

00:14:26,959 --> 00:14:33,440
the occupancy's array back down to um

00:14:30,240 --> 00:14:36,959
for that particular device back down

00:14:33,440 --> 00:14:37,600
and that signals that hey this gpu is

00:14:36,959 --> 00:14:41,279
ready

00:14:37,600 --> 00:14:44,399
with with that strategy we

00:14:41,279 --> 00:14:47,760
show this mini app experimented

00:14:44,399 --> 00:14:50,880
with different scheduling strategies and

00:14:47,760 --> 00:14:54,399
we also compare it with the an

00:14:50,880 --> 00:14:57,279
mpi version that is where we use mpi

00:14:54,399 --> 00:14:58,959
to paralyze across gpus and then within

00:14:57,279 --> 00:15:02,240
that we have openmp

00:14:58,959 --> 00:15:03,199
to um to do the offloading this is that

00:15:02,240 --> 00:15:06,399
is actually

00:15:03,199 --> 00:15:10,000
a one can say conventional way

00:15:06,399 --> 00:15:12,720
that one paralyzes across gpus today

00:15:10,000 --> 00:15:15,279
but the drawback of doing that of course

00:15:12,720 --> 00:15:17,199
is that you can't make use of openmp's

00:15:15,279 --> 00:15:18,639
load balance or load balancing

00:15:17,199 --> 00:15:21,120
capabilities

00:15:18,639 --> 00:15:22,720
so we what we are doing here in this

00:15:21,120 --> 00:15:25,440
experiment our key question is

00:15:22,720 --> 00:15:26,320
can we do better than this conventional

00:15:25,440 --> 00:15:29,920
way of

00:15:26,320 --> 00:15:32,240
paralyzing across these multiple gpus

00:15:29,920 --> 00:15:34,079
we can see in the blue ignore the no

00:15:32,240 --> 00:15:37,279
patch this blue

00:15:34,079 --> 00:15:39,759
here is showing the mpi version the

00:15:37,279 --> 00:15:41,839
execution time then this ignore again

00:15:39,759 --> 00:15:45,040
the stack no patch the static

00:15:41,839 --> 00:15:48,639
here is just the static mapping

00:15:45,040 --> 00:15:52,079
of these target regions to gpus

00:15:48,639 --> 00:15:54,959
and then we have a randomized selection

00:15:52,079 --> 00:15:56,399
of gpu so so randomize each thread

00:15:54,959 --> 00:15:58,959
choose a random

00:15:56,399 --> 00:16:00,880
gpu to run on and then this is just each

00:15:58,959 --> 00:16:04,639
thread chooses the next gpu

00:16:00,880 --> 00:16:06,880
in a cycle um actually and

00:16:04,639 --> 00:16:08,079
both of these already show performance

00:16:06,880 --> 00:16:11,199
improvement over

00:16:08,079 --> 00:16:14,240
the baseline of our original static

00:16:11,199 --> 00:16:17,040
and a very slight improvement over mpi

00:16:14,240 --> 00:16:19,680
but uh we want to point out that the

00:16:17,040 --> 00:16:21,920
real improvement comes from when you use

00:16:19,680 --> 00:16:23,440
dynamic scheduling which we just

00:16:21,920 --> 00:16:24,240
highlighted and outlined in a previous

00:16:23,440 --> 00:16:28,480
slide

00:16:24,240 --> 00:16:31,199
and that provides about just about a 2x

00:16:28,480 --> 00:16:31,680
improved performance improvement we also

00:16:31,199 --> 00:16:34,880
had an

00:16:31,680 --> 00:16:37,920
optimization over dynamic uh where we

00:16:34,880 --> 00:16:41,920
where we have these multiple queues um

00:16:37,920 --> 00:16:44,800
so we reduce data movement across gpus

00:16:41,920 --> 00:16:45,519
that actually improved our performance

00:16:44,800 --> 00:16:49,120
even more

00:16:45,519 --> 00:16:51,759
and so it improves just over 2x

00:16:49,120 --> 00:16:52,240
over mpin and actually a little more

00:16:51,759 --> 00:16:55,360
over

00:16:52,240 --> 00:16:56,720
the static version i'll just note that

00:16:55,360 --> 00:16:59,600
the no patch here

00:16:56,720 --> 00:16:59,920
just means that we had actually started

00:16:59,600 --> 00:17:03,519
with

00:16:59,920 --> 00:17:06,160
uh we used the latest llvm fork

00:17:03,519 --> 00:17:07,199
for all of our experiments the slightest

00:17:06,160 --> 00:17:10,880
solved fork

00:17:07,199 --> 00:17:14,240
of of llvm for all experiments

00:17:10,880 --> 00:17:18,000
and so we are actually using

00:17:14,240 --> 00:17:20,079
the most optimized version two of uh

00:17:18,000 --> 00:17:21,199
solve with all the compiler

00:17:20,079 --> 00:17:23,839
optimizations

00:17:21,199 --> 00:17:26,079
um for and that those are helping for

00:17:23,839 --> 00:17:29,280
the target offload region but the real

00:17:26,079 --> 00:17:32,080
improvements are seen uh in this um

00:17:29,280 --> 00:17:32,559
through dynamic scheduling with that i

00:17:32,080 --> 00:17:36,559
will

00:17:32,559 --> 00:17:39,840
go to some proposals of extensions

00:17:36,559 --> 00:17:42,000
of openmp based on these these

00:17:39,840 --> 00:17:44,799
low overhead scheduling strategies and

00:17:42,000 --> 00:17:47,840
tasks to gpu scheduling strategies

00:17:44,799 --> 00:17:50,559
we have here and a glimpse of how to use

00:17:47,840 --> 00:17:52,080
how a proposed user-defined scheduling

00:17:50,559 --> 00:17:54,400
strategy will work

00:17:52,080 --> 00:17:55,200
through an a particular interface in

00:17:54,400 --> 00:17:58,400
which

00:17:55,200 --> 00:18:01,760
a user declares a new schedule here we

00:17:58,400 --> 00:18:04,799
call it my the users called it myden

00:18:01,760 --> 00:18:07,440
and then they associate three functions

00:18:04,799 --> 00:18:08,640
a on basically an on-cue function a

00:18:07,440 --> 00:18:11,760
dequeueing function

00:18:08,640 --> 00:18:13,760
and a end function a fini function

00:18:11,760 --> 00:18:15,679
the on cueing is really the start

00:18:13,760 --> 00:18:19,039
function the dq is

00:18:15,679 --> 00:18:22,160
next and the the end is fini

00:18:19,039 --> 00:18:24,960
and um then they pro they use that

00:18:22,160 --> 00:18:26,400
declaration of the schedule as the hook

00:18:24,960 --> 00:18:29,600
or the identifier

00:18:26,400 --> 00:18:30,799
in the parallel four schedule shown in

00:18:29,600 --> 00:18:33,520
the blue here

00:18:30,799 --> 00:18:35,280
then with that they can also have their

00:18:33,520 --> 00:18:37,120
own parameters so now you don't just

00:18:35,280 --> 00:18:40,720
need a chunk size you can actually

00:18:37,120 --> 00:18:42,400
have is your own set of a user can

00:18:40,720 --> 00:18:44,080
you know make up make their own

00:18:42,400 --> 00:18:45,200
parameters to the loop scheduling

00:18:44,080 --> 00:18:47,200
strategy

00:18:45,200 --> 00:18:49,440
this is important particularly for our

00:18:47,200 --> 00:18:51,440
tunable loop scheduling strategies where

00:18:49,440 --> 00:18:53,039
we have the static fraction

00:18:51,440 --> 00:18:55,520
and so now you can actually put the

00:18:53,039 --> 00:18:57,840
static fraction as one of these

00:18:55,520 --> 00:18:58,960
parameters you can also put other

00:18:57,840 --> 00:19:02,480
parameters

00:18:58,960 --> 00:19:05,600
like perhaps a distribution of

00:19:02,480 --> 00:19:08,160
of task sizes in a task queue um

00:19:05,600 --> 00:19:10,160
distribution and those could be

00:19:08,160 --> 00:19:13,360
exponentially just decreasing

00:19:10,160 --> 00:19:16,400
or um or decreasing

00:19:13,360 --> 00:19:17,600
through another rate the key thing is

00:19:16,400 --> 00:19:20,320
that the compiler

00:19:17,600 --> 00:19:22,240
instead of calling the calling into the

00:19:20,320 --> 00:19:24,000
runtime library for loop scheduling it's

00:19:22,240 --> 00:19:27,120
going to invoke these functions

00:19:24,000 --> 00:19:28,559
that the user defines within source code

00:19:27,120 --> 00:19:30,400
and typically this is going to be a

00:19:28,559 --> 00:19:32,640
power user

00:19:30,400 --> 00:19:33,600
that helps the full application

00:19:32,640 --> 00:19:36,559
programmer to

00:19:33,600 --> 00:19:37,280
define a real loop schedule work was

00:19:36,559 --> 00:19:40,720
published

00:19:37,280 --> 00:19:44,160
in 2019 and i want

00:19:40,720 --> 00:19:47,440
on an interface for this this shows

00:19:44,160 --> 00:19:49,679
just a little more uh detail on how

00:19:47,440 --> 00:19:50,480
the user defines schedules mechanics

00:19:49,679 --> 00:19:53,919
would work

00:19:50,480 --> 00:19:57,039
you can see here on the on the right the

00:19:53,919 --> 00:19:59,440
the application code on the left it just

00:19:57,039 --> 00:20:02,240
goes into a little more detail on

00:19:59,440 --> 00:20:02,960
what those functions those onq and dq

00:20:02,240 --> 00:20:07,039
functions

00:20:02,960 --> 00:20:09,679
and fini functions look like i add them

00:20:07,039 --> 00:20:10,240
just to mention here but you can look

00:20:09,679 --> 00:20:13,360
online

00:20:10,240 --> 00:20:16,400
for more details the second proposal

00:20:13,360 --> 00:20:17,600
uh is more for the task to gpu

00:20:16,400 --> 00:20:20,000
scheduling

00:20:17,600 --> 00:20:23,280
strategies and that actually builds on

00:20:20,000 --> 00:20:25,200
the user-defined scheduling strategies

00:20:23,280 --> 00:20:26,559
and it's important to note that the

00:20:25,200 --> 00:20:30,320
user-defined scheduling

00:20:26,559 --> 00:20:33,679
schedules can work on uh

00:20:30,320 --> 00:20:36,320
are mainly for the cpus but the the

00:20:33,679 --> 00:20:37,200
proposal itself is not restricted to

00:20:36,320 --> 00:20:39,760
cpus

00:20:37,200 --> 00:20:40,880
um the you can have user defined

00:20:39,760 --> 00:20:43,760
schedules

00:20:40,880 --> 00:20:45,039
that work across gpus so the the point

00:20:43,760 --> 00:20:48,240
that i want to make is

00:20:45,039 --> 00:20:51,039
that here in in our target scheduler

00:20:48,240 --> 00:20:52,720
proposal which allows for tasking across

00:20:51,039 --> 00:20:56,400
multiple gpu so it's

00:20:52,720 --> 00:20:58,400
cpu task to gpu scheduling strategies we

00:20:56,400 --> 00:21:01,280
have a new construct

00:20:58,400 --> 00:21:02,159
called target scheduler and that

00:21:01,280 --> 00:21:05,440
dictates

00:21:02,159 --> 00:21:06,240
the inner task loop uh or the parallel

00:21:05,440 --> 00:21:09,360
four

00:21:06,240 --> 00:21:12,960
to distribute work from

00:21:09,360 --> 00:21:15,919
cpus to gpus in the manner that i

00:21:12,960 --> 00:21:17,039
described previously in that sketch and

00:21:15,919 --> 00:21:19,919
it will do so

00:21:17,039 --> 00:21:20,640
so that the load balancing is improved

00:21:19,919 --> 00:21:23,039
as

00:21:20,640 --> 00:21:25,280
best as possible and i'll just note one

00:21:23,039 --> 00:21:28,640
other thing is the affinity clause

00:21:25,280 --> 00:21:30,080
here this is we've also proposed this

00:21:28,640 --> 00:21:32,480
this actually works

00:21:30,080 --> 00:21:33,520
is relevant for the there's also

00:21:32,480 --> 00:21:36,240
actually already as

00:21:33,520 --> 00:21:37,840
infinity clause for cpus in the standard

00:21:36,240 --> 00:21:39,280
in openmp 5.0 but

00:21:37,840 --> 00:21:41,120
for our proposal we're actually

00:21:39,280 --> 00:21:43,919
proposing this for for the

00:21:41,120 --> 00:21:45,360
tested gpu scheduling so this is also

00:21:43,919 --> 00:21:47,919
part of our proposal

00:21:45,360 --> 00:21:49,039
and affinity clause is a hint to improve

00:21:47,919 --> 00:21:52,240
data locality

00:21:49,039 --> 00:21:54,480
and so we've already shown that

00:21:52,240 --> 00:21:56,000
data locality is important um

00:21:54,480 --> 00:21:59,120
particularly uh this

00:21:56,000 --> 00:22:01,280
this affinity is important for results

00:21:59,120 --> 00:22:03,440
with the jacobi code and the matrix

00:22:01,280 --> 00:22:05,039
multiplication code not just for

00:22:03,440 --> 00:22:06,559
performance but also for energy

00:22:05,039 --> 00:22:09,679
efficiency

00:22:06,559 --> 00:22:10,320
now i would like to just briefly talk

00:22:09,679 --> 00:22:13,520
about

00:22:10,320 --> 00:22:17,120
making with our code for

00:22:13,520 --> 00:22:19,360
uh user-defined schedules um and

00:22:17,120 --> 00:22:20,559
locality sensitive scheduling more

00:22:19,360 --> 00:22:23,760
performance portable

00:22:20,559 --> 00:22:24,159
so this is the original code here and we

00:22:23,760 --> 00:22:26,960
see

00:22:24,159 --> 00:22:28,000
that this uses explicitly lightweight

00:22:26,960 --> 00:22:30,080
scheduling

00:22:28,000 --> 00:22:31,360
but then what we have is a raj

00:22:30,080 --> 00:22:34,880
implementation

00:22:31,360 --> 00:22:37,760
with uh the policy uh with um

00:22:34,880 --> 00:22:38,400
openmpl lightweight scheduling and that

00:22:37,760 --> 00:22:41,360
improves

00:22:38,400 --> 00:22:42,720
um that that gives a more performance

00:22:41,360 --> 00:22:46,080
portable version here

00:22:42,720 --> 00:22:49,520
through this raja and mining and then

00:22:46,080 --> 00:22:51,840
we have all the basically the

00:22:49,520 --> 00:22:52,960
this is the key application code that

00:22:51,840 --> 00:22:54,960
that the

00:22:52,960 --> 00:22:56,240
scientific application programmer would

00:22:54,960 --> 00:22:59,360
use so this this

00:22:56,240 --> 00:23:01,840
is the thought what happens now is that

00:22:59,360 --> 00:23:03,440
the lightweight scheduling is calling

00:23:01,840 --> 00:23:07,200
this code and now we have

00:23:03,440 --> 00:23:10,000
a um more easy to use locality sensitive

00:23:07,200 --> 00:23:12,159
scheduling strategies and it improves

00:23:10,000 --> 00:23:16,080
portability

00:23:12,159 --> 00:23:19,039
so with that we also investigate now

00:23:16,080 --> 00:23:20,320
they there that there's load imbalances

00:23:19,039 --> 00:23:23,760
both across

00:23:20,320 --> 00:23:26,720
and within node and so we use llnl's

00:23:23,760 --> 00:23:27,679
lassen to identify some of these loaded

00:23:26,720 --> 00:23:31,039
balances

00:23:27,679 --> 00:23:34,080
and this is uh courtesy work with um

00:23:31,039 --> 00:23:35,440
hershey thermionon and other members at

00:23:34,080 --> 00:23:38,480
lawrence livermore lab

00:23:35,440 --> 00:23:42,080
cooling brian mccandles um the key thing

00:23:38,480 --> 00:23:43,120
is that the diagrams here show red hot

00:23:42,080 --> 00:23:46,320
spots

00:23:43,120 --> 00:23:47,520
showing a cross node both across node

00:23:46,320 --> 00:23:51,760
and within node

00:23:47,520 --> 00:23:54,960
load imbalance on the top and bottom

00:23:51,760 --> 00:23:58,080
and so to identify

00:23:54,960 --> 00:23:59,039
the given this identification of the

00:23:58,080 --> 00:24:02,880
problem

00:23:59,039 --> 00:24:06,320
we actually just looked at

00:24:02,880 --> 00:24:09,120
a uh solution to

00:24:06,320 --> 00:24:10,559
uh have this synergistic load balancing

00:24:09,120 --> 00:24:13,520
and loop scheduling so

00:24:10,559 --> 00:24:15,919
that involved using an existing runtime

00:24:13,520 --> 00:24:19,120
system that had a lot

00:24:15,919 --> 00:24:22,080
across node load balancing and um

00:24:19,120 --> 00:24:23,360
augmenting it with with within node loop

00:24:22,080 --> 00:24:26,400
scheduling and then

00:24:23,360 --> 00:24:26,960
synergizing it together so that we would

00:24:26,400 --> 00:24:29,840
have

00:24:26,960 --> 00:24:31,279
this mechanism that improved performance

00:24:29,840 --> 00:24:34,240
both through

00:24:31,279 --> 00:24:35,120
through these two complementary

00:24:34,240 --> 00:24:39,039
techniques

00:24:35,120 --> 00:24:42,240
and this resulted in a paper in a poster

00:24:39,039 --> 00:24:45,360
which was uh nsc 17 which is

00:24:42,240 --> 00:24:47,120
also nominated for a best poster so then

00:24:45,360 --> 00:24:49,039
here's some related work

00:24:47,120 --> 00:24:50,640
uh we'll go into details these are

00:24:49,039 --> 00:24:53,520
important uh pieces

00:24:50,640 --> 00:24:54,000
that go in that just that uh that have

00:24:53,520 --> 00:24:57,360
done

00:24:54,000 --> 00:24:58,240
similar work on um uh low overhead loop

00:24:57,360 --> 00:25:01,039
scheduling though

00:24:58,240 --> 00:25:02,400
so none of them have done the tunable

00:25:01,039 --> 00:25:05,440
aspects

00:25:02,400 --> 00:25:06,640
and done gone into any detail on

00:25:05,440 --> 00:25:11,279
locality

00:25:06,640 --> 00:25:14,320
with that we have some future directions

00:25:11,279 --> 00:25:17,520
and those are to namely

00:25:14,320 --> 00:25:19,039
continuing to create examples of

00:25:17,520 --> 00:25:22,159
user-defined schedules

00:25:19,039 --> 00:25:25,039
using the using an existing

00:25:22,159 --> 00:25:26,640
user-defined schedule lvm openmp

00:25:25,039 --> 00:25:29,120
reference implementation

00:25:26,640 --> 00:25:32,159
and then we're also going to work on

00:25:29,120 --> 00:25:35,840
task improvements for tasking your gpus

00:25:32,159 --> 00:25:36,480
and that will include the tasks to gpu

00:25:35,840 --> 00:25:39,440
scheduling

00:25:36,480 --> 00:25:40,400
strategies and locality strategies as

00:25:39,440 --> 00:25:43,360
outlined

00:25:40,400 --> 00:25:44,320
earlier through our prototype we're

00:25:43,360 --> 00:25:46,960
going to be

00:25:44,320 --> 00:25:50,640
in the next year for the um in

00:25:46,960 --> 00:25:54,960
preparation for the openmp 6.0 release

00:25:50,640 --> 00:25:57,760
which is in fy 2023. we will be

00:25:54,960 --> 00:25:58,799
looking at the task to multi gpu

00:25:57,760 --> 00:26:01,120
scheduling and

00:25:58,799 --> 00:26:02,640
um user defined schedule proposals and

00:26:01,120 --> 00:26:05,120
trying to push those in

00:26:02,640 --> 00:26:07,400
and they'll and also a long term goal is

00:26:05,120 --> 00:26:11,360
to upstream our

00:26:07,400 --> 00:26:14,400
implementations uh into llbm

00:26:11,360 --> 00:26:17,360
and then uh finally

00:26:14,400 --> 00:26:18,880
we're going to uh work to develop

00:26:17,360 --> 00:26:21,200
libraries

00:26:18,880 --> 00:26:23,360
that that are beneficial for

00:26:21,200 --> 00:26:26,480
heterogeneous nodes

00:26:23,360 --> 00:26:27,120
particularly libraries that interact

00:26:26,480 --> 00:26:29,440
with

00:26:27,120 --> 00:26:30,320
the cross node load balancing libraries

00:26:29,440 --> 00:26:33,440
um

00:26:30,320 --> 00:26:36,840
like um like charm plus plus and

00:26:33,440 --> 00:26:38,320
also uh they also performance

00:26:36,840 --> 00:26:42,320
portability

00:26:38,320 --> 00:26:45,840
libraries in particular raja as well as

00:26:42,320 --> 00:26:49,039
another one that's cocos thank you for

00:26:45,840 --> 00:26:50,080
listening to my talk and i will invite

00:26:49,039 --> 00:26:53,440
you to

00:26:50,080 --> 00:26:55,120
see more of our work on the openmp

00:26:53,440 --> 00:27:04,080
website

00:26:55,120 --> 00:27:04,080

YouTube URL: https://www.youtube.com/watch?v=y9TwFPSpT6I


