Title: OpenMP offload optimization guide: beyond kernels -Lessons learned in QMCPACK
Publication date: 2020-10-19
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Ye Luo of Argonne National Laboratory, is part of the OpenMP Booth Talk series created for Supercomputing 2020. 

A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20/
Captions: 
	00:00:03,439 --> 00:00:08,160
hello everyone

00:00:04,480 --> 00:00:10,719
i'm ilo from argonne national lab

00:00:08,160 --> 00:00:12,480
i'm an existing computational scientist

00:00:10,719 --> 00:00:15,360
today i'm going to talk about

00:00:12,480 --> 00:00:17,039
openp offload optimization guide

00:00:15,360 --> 00:00:19,840
typically beyond kernels

00:00:17,039 --> 00:00:20,800
and all the lessons i learned from fmc

00:00:19,840 --> 00:00:23,920
pack

00:00:20,800 --> 00:00:25,840
so i'd like first to thank the ecp the

00:00:23,920 --> 00:00:29,199
exa scale computing project

00:00:25,840 --> 00:00:32,000
supported by the department of energy

00:00:29,199 --> 00:00:33,760
and also would like to thank the qmc

00:00:32,000 --> 00:00:36,719
pack ecp developer team

00:00:33,760 --> 00:00:37,840
and also the ecb solve team which helped

00:00:36,719 --> 00:00:41,520
me with open

00:00:37,840 --> 00:00:43,440
a lot with openmp so just one slide

00:00:41,520 --> 00:00:46,079
about qmc pack

00:00:43,440 --> 00:00:48,320
it is a modern high performance open

00:00:46,079 --> 00:00:49,120
source quantum on the color simulation

00:00:48,320 --> 00:00:51,520
code

00:00:49,120 --> 00:00:53,680
it is written in c plus plus and

00:00:51,520 --> 00:00:56,640
designed with

00:00:53,680 --> 00:00:59,199
object or ended programming and meta

00:00:56,640 --> 00:01:02,320
programming with templates to achieve

00:00:59,199 --> 00:01:03,680
the best flexibility and computational

00:01:02,320 --> 00:01:06,640
efficiency

00:01:03,680 --> 00:01:08,960
it is a real world application all the

00:01:06,640 --> 00:01:12,000
examples in this presentation

00:01:08,960 --> 00:01:14,080
reflects the openmp use patterns inside

00:01:12,000 --> 00:01:15,520
qmz pack although they are stripped

00:01:14,080 --> 00:01:18,960
stripped down

00:01:15,520 --> 00:01:21,320
to to help you understanding this talk

00:01:18,960 --> 00:01:22,640
last thing is most of the openmp

00:01:21,320 --> 00:01:26,320
explorations

00:01:22,640 --> 00:01:28,799
prototyping we do it via a qmc pack

00:01:26,320 --> 00:01:28,799
mania

00:01:29,759 --> 00:01:34,240
so let's start talking how to do the

00:01:31,680 --> 00:01:36,320
optimization

00:01:34,240 --> 00:01:37,600
there's a story that you've heard many

00:01:36,320 --> 00:01:41,680
times about

00:01:37,600 --> 00:01:44,799
voting experience of for gpus

00:01:41,680 --> 00:01:46,960
that is you did a lot of work and

00:01:44,799 --> 00:01:48,079
then you realize why i'm spending more

00:01:46,960 --> 00:01:50,399
time

00:01:48,079 --> 00:01:52,880
actually so the measure the host time

00:01:50,399 --> 00:01:55,439
and overall workload time goes up in the

00:01:52,880 --> 00:01:58,719
application but when you throw in

00:01:55,439 --> 00:01:59,680
a profiler and it indicates the gpu

00:01:58,719 --> 00:02:02,560
activity is

00:01:59,680 --> 00:02:03,759
low this is because there are the

00:02:02,560 --> 00:02:08,560
characteristic

00:02:03,759 --> 00:02:11,680
accelerators have many limitations and

00:02:08,560 --> 00:02:13,920
programmers need to be care about and

00:02:11,680 --> 00:02:16,280
when you program using openmp

00:02:13,920 --> 00:02:17,920
you're also limit but limited by those

00:02:16,280 --> 00:02:20,800
characteristics

00:02:17,920 --> 00:02:22,400
so when protein with openp the overall

00:02:20,800 --> 00:02:25,440
goal is to minimize

00:02:22,400 --> 00:02:27,440
open mp overhead on top of those vendor

00:02:25,440 --> 00:02:30,640
programming models

00:02:27,440 --> 00:02:33,920
so it has many aspects

00:02:30,640 --> 00:02:36,560
one is inside the kernel so openmp

00:02:33,920 --> 00:02:38,480
offload regions are transformed

00:02:36,560 --> 00:02:40,160
the source code are transformed into

00:02:38,480 --> 00:02:42,800
kernels and

00:02:40,160 --> 00:02:44,400
queued and executed by the device that

00:02:42,800 --> 00:02:48,840
needs to be efficient

00:02:44,400 --> 00:02:52,160
second is beyond kernels that openmp

00:02:48,840 --> 00:02:55,280
abstraction adds additional overhead

00:02:52,160 --> 00:02:57,200
so we would like to minimize that so for

00:02:55,280 --> 00:03:00,239
application developer we have to

00:02:57,200 --> 00:03:03,519
product best practices in user code

00:03:00,239 --> 00:03:05,120
to help that and also we need a good

00:03:03,519 --> 00:03:07,840
open b compiler or

00:03:05,120 --> 00:03:09,840
runtime implementation to fully achieve

00:03:07,840 --> 00:03:13,280
minimized overhead

00:03:09,840 --> 00:03:15,440
so this talk will focus on all the

00:03:13,280 --> 00:03:17,760
optimizations we can do beyond the

00:03:15,440 --> 00:03:20,959
kernel

00:03:17,760 --> 00:03:22,800
so before talking about uh other

00:03:20,959 --> 00:03:25,840
optimization i'd like to mention

00:03:22,800 --> 00:03:28,319
that we have applications that

00:03:25,840 --> 00:03:28,959
need to be careful about the programming

00:03:28,319 --> 00:03:33,760
styles

00:03:28,959 --> 00:03:37,200
as well so it enables portability

00:03:33,760 --> 00:03:40,000
in in many cases

00:03:37,200 --> 00:03:40,720
so openp offers two programming styles

00:03:40,000 --> 00:03:42,959
actually

00:03:40,720 --> 00:03:44,560
although that mostly openp is a

00:03:42,959 --> 00:03:48,159
directive based

00:03:44,560 --> 00:03:50,959
programming model it also have apis

00:03:48,159 --> 00:03:52,000
when you use a directive like the left

00:03:50,959 --> 00:03:56,319
hand

00:03:52,000 --> 00:03:58,799
left hand side example you have a target

00:03:56,319 --> 00:04:00,000
region and this is tag enter data

00:03:58,799 --> 00:04:03,519
basically to allocate

00:04:00,000 --> 00:04:04,879
the data on the device and on the right

00:04:03,519 --> 00:04:07,360
side is the api

00:04:04,879 --> 00:04:10,000
you directly invoke the target analog

00:04:07,360 --> 00:04:13,760
host functions to applocate the device

00:04:10,000 --> 00:04:15,920
of memory so on the left side it has

00:04:13,760 --> 00:04:18,239
the advantages and there's no side

00:04:15,920 --> 00:04:21,759
effect when you turn off your flag

00:04:18,239 --> 00:04:23,280
in the compiler uh open the offload flag

00:04:21,759 --> 00:04:26,320
in a compiler

00:04:23,280 --> 00:04:28,479
and even if you turn it on you can fall

00:04:26,320 --> 00:04:32,080
back to host for debugging

00:04:28,479 --> 00:04:34,320
by just leveraging the open p5o

00:04:32,080 --> 00:04:37,120
feature it's an environmental variable

00:04:34,320 --> 00:04:40,160
to disable the offload

00:04:37,120 --> 00:04:43,520
it has disadvantage advantage that it

00:04:40,160 --> 00:04:46,400
is less verbose for people

00:04:43,520 --> 00:04:47,199
when you're programming openmp api the

00:04:46,400 --> 00:04:49,759
pros

00:04:47,199 --> 00:04:50,639
are you have the full explicit device

00:04:49,759 --> 00:04:53,520
control

00:04:50,639 --> 00:04:55,919
the disadvantages it's an api so you

00:04:53,520 --> 00:04:58,560
have to protect it with

00:04:55,919 --> 00:05:00,000
pragmas to avoid compiling them when

00:04:58,560 --> 00:05:02,880
open is off

00:05:00,000 --> 00:05:04,880
and also it is hard to design a fullback

00:05:02,880 --> 00:05:07,120
logic to run things on the host

00:05:04,880 --> 00:05:08,000
so qmc pack developers find the left

00:05:07,120 --> 00:05:13,360
side ones

00:05:08,000 --> 00:05:17,280
more preferable in real applications

00:05:13,360 --> 00:05:20,080
so let's start the optimization guide

00:05:17,280 --> 00:05:22,400
so first is the simple example on the

00:05:20,080 --> 00:05:24,800
right side the first one is a single

00:05:22,400 --> 00:05:27,280
target map and array

00:05:24,800 --> 00:05:28,479
a target region with a map of array you

00:05:27,280 --> 00:05:32,400
do some operations

00:05:28,479 --> 00:05:35,039
on on the accelerator this is a test

00:05:32,400 --> 00:05:37,520
testbook example but it's not good for

00:05:35,039 --> 00:05:41,039
performance in real applications

00:05:37,520 --> 00:05:43,039
because when the map happens before and

00:05:41,039 --> 00:05:46,240
after the

00:05:43,039 --> 00:05:49,280
offload region execution that

00:05:46,240 --> 00:05:52,400
the runtime needs to allocate memory

00:05:49,280 --> 00:05:55,520
of array on the device

00:05:52,400 --> 00:05:57,199
and delay allocate after these

00:05:55,520 --> 00:05:59,680
operations are orders of

00:05:57,199 --> 00:06:00,479
magnitude still slower than that you

00:05:59,680 --> 00:06:03,440
typically do

00:06:00,479 --> 00:06:04,560
with malloc and free on the host so how

00:06:03,440 --> 00:06:07,840
to

00:06:04,560 --> 00:06:10,960
avoid those costs it is open p4

00:06:07,840 --> 00:06:11,360
uh openmp offers the target enter data

00:06:10,960 --> 00:06:14,800
and

00:06:11,360 --> 00:06:18,000
exit data directives to help you

00:06:14,800 --> 00:06:20,960
you can pre-arrange the allocation

00:06:18,000 --> 00:06:21,919
that at performance non-critical area of

00:06:20,960 --> 00:06:25,120
your code

00:06:21,919 --> 00:06:28,400
head of your actually outflow region

00:06:25,120 --> 00:06:31,440
so this helps uh bypass

00:06:28,400 --> 00:06:33,919
those um

00:06:31,440 --> 00:06:36,000
allocation deallocation cost at every

00:06:33,919 --> 00:06:38,000
call to the offload region

00:06:36,000 --> 00:06:40,080
and in certain run times those

00:06:38,000 --> 00:06:42,880
operations may also blocking

00:06:40,080 --> 00:06:44,560
asynchronous execution of other offload

00:06:42,880 --> 00:06:45,520
regions so it's critical to your

00:06:44,560 --> 00:06:47,680
performance

00:06:45,520 --> 00:06:48,720
so pre-arrange your allocation di

00:06:47,680 --> 00:06:51,520
location

00:06:48,720 --> 00:06:53,919
outside your frequently called target

00:06:51,520 --> 00:06:53,919
region

00:06:54,080 --> 00:07:00,639
so qmc packs is c plus codes so we can

00:06:57,280 --> 00:07:03,520
even do better with abstractions in c

00:07:00,639 --> 00:07:04,800
plus that is to create an openmp

00:07:03,520 --> 00:07:08,080
allocator

00:07:04,800 --> 00:07:11,560
wrapping all the target enter data

00:07:08,080 --> 00:07:12,880
exit the data inside this customized

00:07:11,560 --> 00:07:15,520
allocator

00:07:12,880 --> 00:07:16,000
this allocator can be used with uh

00:07:15,520 --> 00:07:19,039
regular

00:07:16,000 --> 00:07:21,520
container classes std vectors through

00:07:19,039 --> 00:07:21,520
vectoring

00:07:21,599 --> 00:07:27,360
in in c plus plus code so in the design

00:07:24,720 --> 00:07:30,639
of our openp allocator we actually

00:07:27,360 --> 00:07:33,840
template on top of a host allocator

00:07:30,639 --> 00:07:36,319
it can be a regular student allocator or

00:07:33,840 --> 00:07:37,199
it can be a customized one which you can

00:07:36,319 --> 00:07:39,759
handle

00:07:37,199 --> 00:07:40,639
like memory alignment and or you can

00:07:39,759 --> 00:07:43,680
even add

00:07:40,639 --> 00:07:46,000
registration dx registration of

00:07:43,680 --> 00:07:48,639
this home toast memory into the

00:07:46,000 --> 00:07:51,120
accelerator memory membrane

00:07:48,639 --> 00:07:51,840
accelerator memory space for make

00:07:51,120 --> 00:07:54,800
maximum

00:07:51,840 --> 00:07:55,360
transfer performance so if you pro you

00:07:54,800 --> 00:07:57,360
know

00:07:55,360 --> 00:07:59,039
you're programming buddha you have to

00:07:57,360 --> 00:08:02,160
it's you can add the

00:07:59,039 --> 00:08:05,680
kuda host register the register inside

00:08:02,160 --> 00:08:07,520
your host allocator

00:08:05,680 --> 00:08:09,360
once you get those right there's

00:08:07,520 --> 00:08:12,639
additional

00:08:09,360 --> 00:08:16,160
tips for get best performance

00:08:12,639 --> 00:08:19,520
so the simple case on the right shows

00:08:16,160 --> 00:08:22,639
a very common

00:08:19,520 --> 00:08:24,840
use cases in an offload region you need

00:08:22,639 --> 00:08:28,639
to access some scalars like

00:08:24,840 --> 00:08:32,159
abc you might thought oh i need those

00:08:28,639 --> 00:08:35,919
ones on the accelerator i must map them

00:08:32,159 --> 00:08:38,800
but most compilers implement those

00:08:35,919 --> 00:08:39,519
explicit mapping as allocating memory

00:08:38,800 --> 00:08:42,719
for it

00:08:39,519 --> 00:08:44,240
and and and transfer the data and

00:08:42,719 --> 00:08:46,399
de-allocate

00:08:44,240 --> 00:08:48,480
so i've mentioned allocating the

00:08:46,399 --> 00:08:50,640
allocation is very expensive

00:08:48,480 --> 00:08:54,000
now you are transferring like four bytes

00:08:50,640 --> 00:08:57,440
of eight bytes data it's also extremely

00:08:54,000 --> 00:09:00,000
inefficient so how to avoid that

00:08:57,440 --> 00:09:02,560
basically you don't need to add this map

00:09:00,000 --> 00:09:07,279
because since openmp 4.5

00:09:02,560 --> 00:09:10,320
scalars are first private by default

00:09:07,279 --> 00:09:13,600
so compilers can pack them as

00:09:10,320 --> 00:09:16,560
kernel arguments and there's no

00:09:13,600 --> 00:09:18,160
allocation and explicit transfer

00:09:16,560 --> 00:09:21,360
involved

00:09:18,160 --> 00:09:25,440
so leverage that feature to maximize

00:09:21,360 --> 00:09:25,440
your open view of load performance

00:09:25,519 --> 00:09:30,240
so apart from what we can do in the

00:09:27,279 --> 00:09:34,480
source code the runtime also helps

00:09:30,240 --> 00:09:37,240
a good implementation of openp offload

00:09:34,480 --> 00:09:38,720
runtime can leverage the

00:09:37,240 --> 00:09:42,720
vendor-supported

00:09:38,720 --> 00:09:45,920
concurrency computing models for example

00:09:42,720 --> 00:09:48,959
for nvidia gpus

00:09:45,920 --> 00:09:51,120
cuda has this streams cuda streams for

00:09:48,959 --> 00:09:54,560
asynchronous computing

00:09:51,120 --> 00:09:55,600
and a good implement openmp offloading

00:09:54,560 --> 00:09:58,959
implementation like

00:09:55,600 --> 00:10:02,800
ibm xl and lvm clan can take

00:09:58,959 --> 00:10:06,000
advantage of that on the right is the

00:10:02,800 --> 00:10:09,279
offload region target offload region

00:10:06,000 --> 00:10:11,600
that already used with the

00:10:09,279 --> 00:10:14,000
optimization guide i mentioned earlier

00:10:11,600 --> 00:10:17,360
now this single type region will trigger

00:10:14,000 --> 00:10:20,480
three operations when offloading

00:10:17,360 --> 00:10:22,000
first it will transfer the data of array

00:10:20,480 --> 00:10:24,640
to the device

00:10:22,000 --> 00:10:25,600
and then it will execute the kernel and

00:10:24,640 --> 00:10:28,880
then return

00:10:25,600 --> 00:10:32,399
by transfer back the array

00:10:28,880 --> 00:10:32,959
from device to host all these operations

00:10:32,399 --> 00:10:36,800
can be

00:10:32,959 --> 00:10:39,120
incurred uh in into a cuda stream

00:10:36,800 --> 00:10:41,920
and that and put a single

00:10:39,120 --> 00:10:46,560
synchronization in the end

00:10:41,920 --> 00:10:51,200
so this as additional efficiency

00:10:46,560 --> 00:10:53,120
to our uh to the to the offload

00:10:51,200 --> 00:10:55,200
so cuda stream is one of the features

00:10:53,120 --> 00:10:58,480
how the vendor supports similar features

00:10:55,200 --> 00:11:02,079
let's see how it behaves in the uh

00:10:58,480 --> 00:11:05,360
in real time in real in applications

00:11:02,079 --> 00:11:09,040
so here is a nd prof output

00:11:05,360 --> 00:11:12,480
mbvp uh visualization of the mv

00:11:09,040 --> 00:11:16,560
prof result of mini qmc

00:11:12,480 --> 00:11:19,600
to within this single open floral region

00:11:16,560 --> 00:11:23,519
that open p runtime transform

00:11:19,600 --> 00:11:26,640
uh the the transformed offload region

00:11:23,519 --> 00:11:27,279
into a bunch of calls to the cuda driver

00:11:26,640 --> 00:11:31,040
api

00:11:27,279 --> 00:11:34,000
and and also launching the kernel

00:11:31,040 --> 00:11:35,760
so you'll notice that i point out all

00:11:34,000 --> 00:11:39,120
those driver api calls

00:11:35,760 --> 00:11:41,839
that asynchronously which are the

00:11:39,120 --> 00:11:43,040
host to device and kernel launch and

00:11:41,839 --> 00:11:45,120
device host

00:11:43,040 --> 00:11:47,680
they all spend very little amount of

00:11:45,120 --> 00:11:50,639
time so the asynchronous behavior all

00:11:47,680 --> 00:11:52,639
engaged and the end at the end because

00:11:50,639 --> 00:11:54,560
my offload region is still blocking

00:11:52,639 --> 00:11:56,079
that you see this cuda stream

00:11:54,560 --> 00:12:00,079
synchronized at the very

00:11:56,079 --> 00:12:02,320
end so there's no it's basically

00:12:00,079 --> 00:12:04,560
close to what i would like to program

00:12:02,320 --> 00:12:07,839
directly in cuda so openp

00:12:04,560 --> 00:12:09,120
offloads overhead is really really

00:12:07,839 --> 00:12:11,920
minimized

00:12:09,120 --> 00:12:13,120
in this example i would like to

00:12:11,920 --> 00:12:16,079
emphasize

00:12:13,120 --> 00:12:17,920
the kernel is not a huge kernel you you

00:12:16,079 --> 00:12:20,480
might think that i use that

00:12:17,920 --> 00:12:22,320
huge kernel to amortize the launching

00:12:20,480 --> 00:12:23,519
overhead actually not this kernel is

00:12:22,320 --> 00:12:28,160
only

00:12:23,519 --> 00:12:28,160
about 20 microseconds

00:12:29,680 --> 00:12:35,279
so now since we have a single open

00:12:32,880 --> 00:12:37,120
offload region very efficiently

00:12:35,279 --> 00:12:39,920
implemented

00:12:37,120 --> 00:12:42,079
now we can leverage concurrent execution

00:12:39,920 --> 00:12:43,040
and data transfer which is a technique

00:12:42,079 --> 00:12:47,120
we've mentioned

00:12:43,040 --> 00:12:51,360
very frequently in asynchronous offloads

00:12:47,120 --> 00:12:55,440
partly due to the hardware limitation

00:12:51,360 --> 00:12:59,279
that the runtime offers

00:12:55,440 --> 00:13:03,120
a way to reduce those uh overheads from

00:12:59,279 --> 00:13:07,040
the hardware restrictions so openp

00:13:03,120 --> 00:13:10,240
runtimes like ibm xl and llvm

00:13:07,040 --> 00:13:12,560
can send the open

00:13:10,240 --> 00:13:13,600
offload region into independent cloud

00:13:12,560 --> 00:13:16,320
streams

00:13:13,600 --> 00:13:17,760
by by this feature we are able to

00:13:16,320 --> 00:13:20,720
execute

00:13:17,760 --> 00:13:21,440
target regions concurrently just by

00:13:20,720 --> 00:13:24,639
wrapping them

00:13:21,440 --> 00:13:27,519
up with open pr float open p

00:13:24,639 --> 00:13:29,680
thread so on the right side is an

00:13:27,519 --> 00:13:32,240
example you see that

00:13:29,680 --> 00:13:33,360
i have a outer loop which is distributed

00:13:32,240 --> 00:13:36,639
over threads

00:13:33,360 --> 00:13:40,399
and within each thread there are

00:13:36,639 --> 00:13:43,600
offload computations this is uh

00:13:40,399 --> 00:13:49,839
reflects the open uh reflects

00:13:43,600 --> 00:13:49,839
the real world impact use cases

00:13:53,360 --> 00:13:59,600
so by doing this actually operation

00:13:56,880 --> 00:14:01,600
from different threads the offload from

00:13:59,600 --> 00:14:03,839
different threads can overlap with each

00:14:01,600 --> 00:14:08,639
other i'd like to show you with

00:14:03,839 --> 00:14:11,839
the profiler result so the concurrent

00:14:08,639 --> 00:14:15,920
concurrent execution is reflected by mvp

00:14:11,839 --> 00:14:18,079
in this communicum c execution as well

00:14:15,920 --> 00:14:21,600
you will see that i'm i'm running with

00:14:18,079 --> 00:14:24,320
eight open p-hole threads and that's why

00:14:21,600 --> 00:14:26,000
in the profiler outputs you see eight

00:14:24,320 --> 00:14:29,040
cuda streams

00:14:26,000 --> 00:14:32,720
and you notice that uh this

00:14:29,040 --> 00:14:35,920
the stream 18 and stream 21

00:14:32,720 --> 00:14:39,120
are overlapping the data transfer

00:14:35,920 --> 00:14:42,880
and computation so the first red box

00:14:39,120 --> 00:14:45,519
and on the right right the red box

00:14:42,880 --> 00:14:47,040
you you'll even see that concurrent

00:14:45,519 --> 00:14:49,760
kernel execution

00:14:47,040 --> 00:14:52,000
and also overlap with data transfer as

00:14:49,760 --> 00:14:54,560
well because my kernels are very small

00:14:52,000 --> 00:14:55,360
when the gpu realized there are

00:14:54,560 --> 00:14:57,360
additional

00:14:55,360 --> 00:14:58,880
available resources for other kernels to

00:14:57,360 --> 00:15:02,959
execute it will

00:14:58,880 --> 00:15:02,959
try to run more things

00:15:07,199 --> 00:15:13,600
okay so

00:15:10,839 --> 00:15:16,880
this in addition to that

00:15:13,600 --> 00:15:17,440
um there are more concurrency we can

00:15:16,880 --> 00:15:20,880
leverage

00:15:17,440 --> 00:15:23,040
from open mp that is

00:15:20,880 --> 00:15:23,920
using the testing to tasking to

00:15:23,040 --> 00:15:26,079
coordinate

00:15:23,920 --> 00:15:28,399
coordinate the host and offload

00:15:26,079 --> 00:15:31,680
computation

00:15:28,399 --> 00:15:34,480
so when we put in a real very big

00:15:31,680 --> 00:15:37,120
application that we need to enable a

00:15:34,480 --> 00:15:39,680
feature complete

00:15:37,120 --> 00:15:40,959
experience for users that's why we

00:15:39,680 --> 00:15:43,199
cannot put

00:15:40,959 --> 00:15:44,639
it's impossible to put every feature to

00:15:43,199 --> 00:15:48,079
an accelerator

00:15:44,639 --> 00:15:50,399
in a very short period of time so

00:15:48,079 --> 00:15:51,920
for this reason we would like to have

00:15:50,399 --> 00:15:54,720
partial of the application

00:15:51,920 --> 00:15:55,440
running on the accelerator and partial

00:15:54,720 --> 00:15:58,000
of them

00:15:55,440 --> 00:15:58,480
still running on the host we know that

00:15:58,000 --> 00:16:01,360
when

00:15:58,480 --> 00:16:03,600
the accelerator is executing things this

00:16:01,360 --> 00:16:07,839
the whole side is typically

00:16:03,600 --> 00:16:11,920
idle so we would like to add some work

00:16:07,839 --> 00:16:15,120
the cpu side tasks to those

00:16:11,920 --> 00:16:19,199
idle cpu resources so

00:16:15,120 --> 00:16:22,399
openmp tasking offers such opportunities

00:16:19,199 --> 00:16:24,800
so start from the previous example now

00:16:22,399 --> 00:16:26,959
within a thread we not only just do a

00:16:24,800 --> 00:16:30,079
target region offload

00:16:26,959 --> 00:16:30,800
uh we also have other cpu tasks because

00:16:30,079 --> 00:16:33,680
the target

00:16:30,800 --> 00:16:36,480
can be transformed as a asynchronous tax

00:16:33,680 --> 00:16:37,199
with this normal weight clause we can

00:16:36,480 --> 00:16:40,880
have

00:16:37,199 --> 00:16:46,000
the task target task and regular cpu tax

00:16:40,880 --> 00:16:46,000
all coordinated with the tasking runtime

00:16:46,560 --> 00:16:51,680
so in my example i have a target no wait

00:16:49,040 --> 00:16:54,720
first launching a gpu task

00:16:51,680 --> 00:16:55,120
then it returns immediately so with the

00:16:54,720 --> 00:16:58,160
sec

00:16:55,120 --> 00:17:01,040
then on the second is the cpu task

00:16:58,160 --> 00:17:03,279
the host task one it's completely

00:17:01,040 --> 00:17:06,799
independent and unblocking

00:17:03,279 --> 00:17:08,640
and this then followed by a cpu task

00:17:06,799 --> 00:17:12,640
which depends on the result

00:17:08,640 --> 00:17:15,039
coming out of the target task

00:17:12,640 --> 00:17:16,000
and at the very end i just put a omp

00:17:15,039 --> 00:17:20,880
task weight

00:17:16,000 --> 00:17:20,880
to sync to synchronize all the tasks

00:17:21,600 --> 00:17:27,199
uh just to be careful this perform this

00:17:24,640 --> 00:17:28,720
way of doing tasks happily depends the

00:17:27,199 --> 00:17:33,840
performance heavily depends on the

00:17:28,720 --> 00:17:33,840
compiler runtime implementations

00:17:34,000 --> 00:17:42,240
so there are very nice features

00:17:37,760 --> 00:17:47,280
as open p5o and beyond gets

00:17:42,240 --> 00:17:50,480
published and implemented in compilers

00:17:47,280 --> 00:17:52,080
uh i i list some of them beneficial

00:17:50,480 --> 00:17:53,600
in the view of cumulative pack

00:17:52,080 --> 00:17:56,240
developers

00:17:53,600 --> 00:17:58,160
for example the meta directives and

00:17:56,240 --> 00:18:00,799
declare varying functions

00:17:58,160 --> 00:18:02,559
those really help better organize the

00:18:00,799 --> 00:18:05,440
source code and reduce

00:18:02,559 --> 00:18:08,320
duplications in the application

00:18:05,440 --> 00:18:10,960
duplications

00:18:08,320 --> 00:18:12,480
when reporting gpu code for multiple

00:18:10,960 --> 00:18:14,679
vendors

00:18:12,480 --> 00:18:16,480
sometimes we kind of really avoid

00:18:14,679 --> 00:18:19,600
specialization

00:18:16,480 --> 00:18:21,039
so with these like variant functions we

00:18:19,600 --> 00:18:25,280
can restrict

00:18:21,039 --> 00:18:28,400
the specialization to very

00:18:25,280 --> 00:18:32,400
limited areas and keep the

00:18:28,400 --> 00:18:35,039
upper level code clean

00:18:32,400 --> 00:18:35,520
there are also the detached the detached

00:18:35,039 --> 00:18:38,000
task

00:18:35,520 --> 00:18:40,640
feature which helps the composability

00:18:38,000 --> 00:18:40,640
with other

00:18:41,120 --> 00:18:48,400
asynchronous runtime systems

00:18:44,640 --> 00:18:51,440
and also the interrupt object

00:18:48,400 --> 00:18:51,919
feature helps exposing the vendor native

00:18:51,440 --> 00:18:55,039
queues

00:18:51,919 --> 00:18:58,160
streams to allow

00:18:55,039 --> 00:19:02,080
programmers to directly

00:18:58,160 --> 00:19:04,799
interact with the vendor native runtimes

00:19:02,080 --> 00:19:05,520
for close to metal experience in

00:19:04,799 --> 00:19:09,039
programming

00:19:05,520 --> 00:19:12,160
openmp and also there are

00:19:09,039 --> 00:19:13,440
ompg and ompd support for profiling and

00:19:12,160 --> 00:19:17,919
debugging tools

00:19:13,440 --> 00:19:20,720
which is very nice for developers

00:19:17,919 --> 00:19:22,840
so here is the example of detached task

00:19:20,720 --> 00:19:26,320
we would like to write

00:19:22,840 --> 00:19:30,000
uh this example

00:19:26,320 --> 00:19:33,840
is to asynchronously coco blast without

00:19:30,000 --> 00:19:37,120
explicitly waiting the cuda runtime

00:19:33,840 --> 00:19:40,400
so on the right side is this

00:19:37,120 --> 00:19:43,440
sketch code so if the first detach task

00:19:40,400 --> 00:19:46,000
will launch a blast call and also

00:19:43,440 --> 00:19:47,760
enqueue a callback function to the cuda

00:19:46,000 --> 00:19:51,760
runtime to signal back

00:19:47,760 --> 00:19:54,160
openp runtime a tasking runtime when the

00:19:51,760 --> 00:19:56,240
task of the the asynchronous publish

00:19:54,160 --> 00:19:58,720
code is completed

00:19:56,240 --> 00:19:59,760
it follows by attacking no weight at a

00:19:58,720 --> 00:20:04,400
target task

00:19:59,760 --> 00:20:07,280
with programming openp to execute

00:20:04,400 --> 00:20:07,600
following the first one and then there's

00:20:07,280 --> 00:20:11,360
a

00:20:07,600 --> 00:20:14,320
second uh detached task calling a third

00:20:11,360 --> 00:20:15,039
calling another full blast call and in

00:20:14,320 --> 00:20:17,360
the end

00:20:15,039 --> 00:20:19,120
we use operators to synchronize all of

00:20:17,360 --> 00:20:22,000
them so

00:20:19,120 --> 00:20:24,240
there's no directly called to the uh

00:20:22,000 --> 00:20:27,120
stream way to put a device weight to

00:20:24,240 --> 00:20:30,159
to wait for the cooldown time to return

00:20:27,120 --> 00:20:34,799
so this maximize the efficiency

00:20:30,159 --> 00:20:37,679
and compose who the vendor libraries

00:20:34,799 --> 00:20:38,080
cost and openmp asynchronous computation

00:20:37,679 --> 00:20:41,280
so

00:20:38,080 --> 00:20:42,400
detach tax is a list feature for 5.0

00:20:41,280 --> 00:20:45,919
specs

00:20:42,400 --> 00:20:46,840
but uh we are still waiting for actual

00:20:45,919 --> 00:20:49,840
compiler

00:20:46,840 --> 00:20:49,840
implementation

00:20:51,280 --> 00:20:58,480
beyond that there are

00:20:55,200 --> 00:21:01,360
many features actually needed for open

00:20:58,480 --> 00:21:02,400
applications leveraging openp target

00:21:01,360 --> 00:21:04,480
offloads

00:21:02,400 --> 00:21:06,240
which are not part of the open piece

00:21:04,480 --> 00:21:08,799
back but highly desire

00:21:06,240 --> 00:21:09,280
needed or highly required feature to be

00:21:08,799 --> 00:21:12,480
used

00:21:09,280 --> 00:21:13,600
useful or to to make openp useful for

00:21:12,480 --> 00:21:16,000
applications

00:21:13,600 --> 00:21:16,799
so qmc pack developers struggled with

00:21:16,000 --> 00:21:20,559
those features

00:21:16,799 --> 00:21:23,679
in 2019 and luckily there are a lot of

00:21:20,559 --> 00:21:25,760
improvement in 2020 so i'd

00:21:23,679 --> 00:21:27,679
like to mention them so this is the

00:21:25,760 --> 00:21:30,480
tracking table of

00:21:27,679 --> 00:21:32,640
mini c for different vendor

00:21:30,480 --> 00:21:34,400
compilers and different

00:21:32,640 --> 00:21:36,240
vendor and open source compilers and

00:21:34,400 --> 00:21:39,120
different devices including

00:21:36,240 --> 00:21:40,400
immediate amd and intel so those

00:21:39,120 --> 00:21:44,559
features are

00:21:40,400 --> 00:21:48,159
like math functions based math functions

00:21:44,559 --> 00:21:51,760
cannot be avoided by most applications

00:21:48,159 --> 00:21:56,080
in simulation world second is complex

00:21:51,760 --> 00:21:56,080
math arithmetics uh

00:21:56,240 --> 00:21:59,520
quite a wide range of applications

00:21:58,240 --> 00:22:02,720
actually need that feature

00:21:59,520 --> 00:22:04,960
inside the open the offload region some

00:22:02,720 --> 00:22:06,000
applications also need declared targeted

00:22:04,960 --> 00:22:09,280
static data

00:22:06,000 --> 00:22:13,520
to have some global pre-arranged

00:22:09,280 --> 00:22:16,559
data set and the

00:22:13,520 --> 00:22:19,679
in addition the linking of static

00:22:16,559 --> 00:22:22,159
archives with device code inside is a

00:22:19,679 --> 00:22:26,400
needed feature

00:22:22,159 --> 00:22:27,039
for writing big applications then the

00:22:26,400 --> 00:22:30,559
last one

00:22:27,039 --> 00:22:33,440
is the multi-stream concurrent execution

00:22:30,559 --> 00:22:33,840
like uh kimchi pack is sensitive to that

00:22:33,440 --> 00:22:36,799
so

00:22:33,840 --> 00:22:39,520
that's also needed so luckily the

00:22:36,799 --> 00:22:41,919
situation is a lot better today and i'm

00:22:39,520 --> 00:22:42,559
i hope they are improving over time as

00:22:41,919 --> 00:22:45,200
well

00:22:42,559 --> 00:22:48,159
so it's very promising to use opening

00:22:45,200 --> 00:22:50,960
real applications

00:22:48,159 --> 00:22:50,960
so summary

00:22:52,000 --> 00:22:56,720
so this talk we cover the topics topics

00:22:54,960 --> 00:22:58,480
application developer needs to pay

00:22:56,720 --> 00:23:02,159
attention to

00:22:58,480 --> 00:23:04,720
uh to app to performance

00:23:02,159 --> 00:23:06,480
that is beyond the kernels mostly in the

00:23:04,720 --> 00:23:10,480
runtime and how to minimize

00:23:06,480 --> 00:23:13,520
all sorts of overhead due to openp

00:23:10,480 --> 00:23:17,039
and we've i have shown that

00:23:13,520 --> 00:23:20,559
this overhead can be simply minimized

00:23:17,039 --> 00:23:21,840
and close to nothing compared to the

00:23:20,559 --> 00:23:24,880
actual

00:23:21,840 --> 00:23:25,919
native runtime cost and there are many

00:23:24,880 --> 00:23:29,919
simple patterns

00:23:25,919 --> 00:23:32,240
can be adopted by openb offload

00:23:29,919 --> 00:23:36,880
feature users to have significant

00:23:32,240 --> 00:23:40,000
performance gains

00:23:36,880 --> 00:23:42,640
so in addition that task level

00:23:40,000 --> 00:23:45,840
parallelism can be considered

00:23:42,640 --> 00:23:49,360
so they become essential for accelerator

00:23:45,840 --> 00:23:52,640
asynchronous computation

00:23:49,360 --> 00:23:55,840
last in the in 2020

00:23:52,640 --> 00:23:57,840
the compiler and openmp runtimes in the

00:23:55,840 --> 00:23:58,960
implementation are significantly

00:23:57,840 --> 00:24:02,080
improving

00:23:58,960 --> 00:24:12,640
and allows production use of

00:24:02,080 --> 00:24:12,640

YouTube URL: https://www.youtube.com/watch?v=iPGMYVViQzM


