Title: LB4OMP: A Load Balancing Portfolio for OpenMP
Publication date: 2020-09-24
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation, delivered by Jonas Henrique Müller Korndörfer of University of Basel, is part of the OpenMP Booth Talk series created for Supercomputing 2020. 

 A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20
Captions: 
	00:00:00,830 --> 00:00:04,080
[Music]

00:00:03,600 --> 00:00:07,600
okay

00:00:04,080 --> 00:00:10,080
so hello everyone uh i'm uh jonas

00:00:07,600 --> 00:00:12,880
mueller condorfor i'm a phd student

00:00:10,080 --> 00:00:14,400
at the university of basel and i'm here

00:00:12,880 --> 00:00:17,279
today to present to you

00:00:14,400 --> 00:00:20,160
lb for omp a load balancing portfolio

00:00:17,279 --> 00:00:20,160
for openmp

00:00:20,320 --> 00:00:25,599
so here on the right

00:00:23,439 --> 00:00:26,560
you can see and the collaborators of

00:00:25,599 --> 00:00:29,599
this work

00:00:26,560 --> 00:00:33,040
we are all part of the hpc group of

00:00:29,599 --> 00:00:33,360
the university of basel so here you can

00:00:33,040 --> 00:00:36,160
see

00:00:33,360 --> 00:00:38,000
professor freyna chobro which is the

00:00:36,160 --> 00:00:40,079
head of the group

00:00:38,000 --> 00:00:41,120
ali mohammed which is a post-doc

00:00:40,079 --> 00:00:44,399
researcher

00:00:41,120 --> 00:00:45,840
ali lemi and myself which are phd

00:00:44,399 --> 00:00:48,879
students

00:00:45,840 --> 00:00:51,520
so this work is partially funded by

00:00:48,879 --> 00:00:52,559
the swiss national science foundation or

00:00:51,520 --> 00:00:55,440
snf

00:00:52,559 --> 00:00:56,320
and by the project mls much level

00:00:55,440 --> 00:00:58,000
scheduling in

00:00:56,320 --> 00:00:59,760
large scale high high-performance

00:00:58,000 --> 00:01:03,359
computers

00:00:59,760 --> 00:01:05,360
so as a brief outline i will start by

00:01:03,359 --> 00:01:07,280
explaining our motivation behind this

00:01:05,360 --> 00:01:09,920
work and a little bit of the history

00:01:07,280 --> 00:01:13,760
from where it comes

00:01:09,920 --> 00:01:16,159
i will then talk about lb 4mp itself

00:01:13,760 --> 00:01:17,840
which scheduling techniques it contains

00:01:16,159 --> 00:01:22,400
and which performance measurement

00:01:17,840 --> 00:01:24,080
features it contains and how to use it

00:01:22,400 --> 00:01:26,080
then finally we will look at some

00:01:24,080 --> 00:01:29,600
performance evaluation we conducted

00:01:26,080 --> 00:01:33,040
and some messages

00:01:29,600 --> 00:01:36,640
so it is well known that

00:01:33,040 --> 00:01:40,159
loading balance is a really hard

00:01:36,640 --> 00:01:41,040
performance problem in most codes but

00:01:40,159 --> 00:01:44,000
here we are

00:01:41,040 --> 00:01:45,439
specifically targeting openmp code so

00:01:44,000 --> 00:01:48,880
loading balance can

00:01:45,439 --> 00:01:52,320
lower the performance which in its turn

00:01:48,880 --> 00:01:55,439
waste resources and energy and increase

00:01:52,320 --> 00:01:58,399
weight waiting times in job queues

00:01:55,439 --> 00:01:59,520
so if we take our current situation

00:01:58,399 --> 00:02:02,880
actually

00:01:59,520 --> 00:02:04,320
with the kovid 19 and all the research

00:02:02,880 --> 00:02:07,680
behind it

00:02:04,320 --> 00:02:10,000
so many hpc centers are extremely

00:02:07,680 --> 00:02:12,080
overloaded with a lot of researchers

00:02:10,000 --> 00:02:15,280
trying to execute their simulators to

00:02:12,080 --> 00:02:18,000
find the vaccine to find medicine

00:02:15,280 --> 00:02:19,360
and it is unacceptable if the

00:02:18,000 --> 00:02:22,879
applications are not

00:02:19,360 --> 00:02:25,920
achieving its highest performance

00:02:22,879 --> 00:02:27,520
so that's one side of the coin the other

00:02:25,920 --> 00:02:31,519
kind is

00:02:27,520 --> 00:02:34,720
that there are there are a lot of

00:02:31,519 --> 00:02:37,200
load balancing algorithms

00:02:34,720 --> 00:02:38,160
proposed in the literature that have

00:02:37,200 --> 00:02:41,760
never in

00:02:38,160 --> 00:02:45,360
the light of the day and

00:02:41,760 --> 00:02:48,400
that causes two main problems one

00:02:45,360 --> 00:02:48,959
these algorithms may outperform the the

00:02:48,400 --> 00:02:52,560
current

00:02:48,959 --> 00:02:54,640
standard ones and the design

00:02:52,560 --> 00:02:56,160
and the lack of these algorithms all to

00:02:54,640 --> 00:03:00,879
me in the research called

00:02:56,160 --> 00:03:00,879
novel load balancing algorithms

00:03:02,080 --> 00:03:07,599
that that happens because uh without the

00:03:05,120 --> 00:03:09,599
implementation of

00:03:07,599 --> 00:03:10,720
what is already proposed in the

00:03:09,599 --> 00:03:14,159
literature uh

00:03:10,720 --> 00:03:17,360
it's not possible to provide a fair

00:03:14,159 --> 00:03:19,680
comparison with the novel approach

00:03:17,360 --> 00:03:20,800
so with that in mind we decided to

00:03:19,680 --> 00:03:24,480
approach some

00:03:20,800 --> 00:03:27,040
openmp runtime libraries

00:03:24,480 --> 00:03:28,319
uh and and implement some scheduling

00:03:27,040 --> 00:03:30,959
techniques to it

00:03:28,319 --> 00:03:31,360
so our first effort was published that i

00:03:30,959 --> 00:03:34,640
want

00:03:31,360 --> 00:03:38,080
2018 where we approached the

00:03:34,640 --> 00:03:39,360
guinea plampy runtime library and we

00:03:38,080 --> 00:03:42,480
implemented some

00:03:39,360 --> 00:03:44,239
dynamic and non-adaptive self-scheduling

00:03:42,480 --> 00:03:47,120
techniques to it

00:03:44,239 --> 00:03:49,760
so the results look very promising but

00:03:47,120 --> 00:03:53,120
we noticed that

00:03:49,760 --> 00:03:54,000
it would be probably more interesting to

00:03:53,120 --> 00:03:57,200
approach

00:03:54,000 --> 00:04:00,239
other openmp runtime live libraries

00:03:57,200 --> 00:04:05,360
such as the llvm1

00:04:00,239 --> 00:04:07,599
which is broadly used in

00:04:05,360 --> 00:04:10,080
in the industry and it's also more

00:04:07,599 --> 00:04:14,799
compatible among the different compilers

00:04:10,080 --> 00:04:17,600
such as intel clang and gcc itself so

00:04:14,799 --> 00:04:18,239
this was then our second effort to

00:04:17,600 --> 00:04:20,320
introduce

00:04:18,239 --> 00:04:21,359
some scheduling techniques to openmp

00:04:20,320 --> 00:04:26,479
runtime libraries

00:04:21,359 --> 00:04:26,479
and this time it was published at iispd

00:04:26,840 --> 00:04:31,520
00:04:28,320 --> 00:04:33,360
and it also produced very interesting

00:04:31,520 --> 00:04:37,120
result

00:04:33,360 --> 00:04:38,320
but we noted that although we had

00:04:37,120 --> 00:04:40,960
already implemented

00:04:38,320 --> 00:04:42,479
some of the literature we felt that we

00:04:40,960 --> 00:04:45,600
had to go forward

00:04:42,479 --> 00:04:48,800
with a let's say a stronger approach to

00:04:45,600 --> 00:04:54,000
implement at least most of

00:04:48,800 --> 00:04:57,360
what is out there so then comes

00:04:54,000 --> 00:05:00,000
a very very early version of lb4mp

00:04:57,360 --> 00:05:03,840
itself which was presented in a poster

00:05:00,000 --> 00:05:07,759
at supercomputing in 2019

00:05:03,840 --> 00:05:10,880
and then we come up to today

00:05:07,759 --> 00:05:13,840
and we kindly we kindly like to see

00:05:10,880 --> 00:05:15,039
lb for omp as a swiss army knife for

00:05:13,840 --> 00:05:18,320
load balancing in

00:05:15,039 --> 00:05:21,520
openmp so

00:05:18,320 --> 00:05:25,600
in a nutshell as the title says lb4mp is

00:05:21,520 --> 00:05:29,199
a load balancing portfolio for openmp

00:05:25,600 --> 00:05:32,639
its main goal is to bridge the

00:05:29,199 --> 00:05:33,840
huge gap between what is proposed in the

00:05:32,639 --> 00:05:36,560
literature and

00:05:33,840 --> 00:05:38,479
what is actually existing and

00:05:36,560 --> 00:05:41,759
implemented in practice

00:05:38,479 --> 00:05:46,400
for multi-threaded applications

00:05:41,759 --> 00:05:48,000
uh it is also based on the llvm openmp

00:05:46,400 --> 00:05:50,960
runtime library

00:05:48,000 --> 00:05:52,639
and it contains in total nine dynamic

00:05:50,960 --> 00:05:53,360
and non-adaptive self-scheduling

00:05:52,639 --> 00:05:56,000
techniques

00:05:53,360 --> 00:05:57,840
and eight dynamic and adaptive

00:05:56,000 --> 00:05:59,440
self-scheduling techniques

00:05:57,840 --> 00:06:04,160
and some performance measurement

00:05:59,440 --> 00:06:07,680
features so lb4mp is available on github

00:06:04,160 --> 00:06:09,199
and you can check this out

00:06:07,680 --> 00:06:13,360
so which actually techniques are

00:06:09,199 --> 00:06:16,240
available at lb for mp

00:06:13,360 --> 00:06:18,400
so first it of course also contains a

00:06:16,240 --> 00:06:22,560
static as the openmp

00:06:18,400 --> 00:06:23,199
standard one and then it contains the

00:06:22,560 --> 00:06:25,440
nine

00:06:23,199 --> 00:06:26,400
dynamic and non-adaptive self-scheduling

00:06:25,440 --> 00:06:30,639
techniques which

00:06:26,400 --> 00:06:31,759
are ss or dynamic one gss or guided

00:06:30,639 --> 00:06:34,400
which are both

00:06:31,759 --> 00:06:35,039
from the openmp standard then it

00:06:34,400 --> 00:06:39,360
contains

00:06:35,039 --> 00:06:42,639
fsc which stands for fixed sized chunk

00:06:39,360 --> 00:06:43,440
it contains tss or trapezoid self

00:06:42,639 --> 00:06:46,319
scheduling

00:06:43,440 --> 00:06:48,000
which is highlighted in gray because it

00:06:46,319 --> 00:06:51,520
was already implemented in the

00:06:48,000 --> 00:06:55,199
llvm runtime library

00:06:51,520 --> 00:06:57,840
and then it contains fact or factoring

00:06:55,199 --> 00:07:00,080
it contained mfaq which is a modified

00:06:57,840 --> 00:07:03,120
implementation of vectoring that

00:07:00,080 --> 00:07:05,919
just tries to reduce the amount of

00:07:03,120 --> 00:07:06,800
synchronization between the threads and

00:07:05,919 --> 00:07:09,599
it contains

00:07:06,800 --> 00:07:10,639
effect two with a practical variant of

00:07:09,599 --> 00:07:14,560
factoring

00:07:10,639 --> 00:07:18,479
it contains tap or tapering

00:07:14,560 --> 00:07:21,039
and it contains wf2 which is a practical

00:07:18,479 --> 00:07:22,880
variant of weighted factoring

00:07:21,039 --> 00:07:25,120
so at this point you may have noticed

00:07:22,880 --> 00:07:27,120
that some arrows

00:07:25,120 --> 00:07:28,160
indicate that some of these techniques

00:07:27,120 --> 00:07:31,680
require

00:07:28,160 --> 00:07:33,199
profiling information so we will talk a

00:07:31,680 --> 00:07:36,000
little bit more about this

00:07:33,199 --> 00:07:37,520
later but lb for mp also provides a

00:07:36,000 --> 00:07:41,840
solution for

00:07:37,520 --> 00:07:44,879
such profiling information

00:07:41,840 --> 00:07:48,560
then finally we have the dynamic and

00:07:44,879 --> 00:07:51,759
adaptive self scheduling technique

00:07:48,560 --> 00:07:55,120
bode awf

00:07:51,759 --> 00:07:57,680
a w f b c d and e

00:07:55,120 --> 00:07:59,840
and which stands for adaptive weighted

00:07:57,680 --> 00:08:02,879
factoring and its variants

00:07:59,840 --> 00:08:04,960
then af which is adaptive factoring and

00:08:02,879 --> 00:08:07,199
maf which is also a modified

00:08:04,960 --> 00:08:10,319
implementation of

00:08:07,199 --> 00:08:12,240
adaptive factoring so it's important to

00:08:10,319 --> 00:08:15,840
highlight here that

00:08:12,240 --> 00:08:17,840
that's the first time that the adaptive

00:08:15,840 --> 00:08:19,120
the data dynamic and adaptive self

00:08:17,840 --> 00:08:23,360
scheduling techniques are

00:08:19,120 --> 00:08:27,680
seeing the light of the day in openmp

00:08:23,360 --> 00:08:31,360
and they are very important because

00:08:27,680 --> 00:08:35,039
they are much more malleable

00:08:31,360 --> 00:08:37,599
since they are able to record

00:08:35,039 --> 00:08:38,560
profiling information and performance

00:08:37,599 --> 00:08:41,440
information

00:08:38,560 --> 00:08:42,800
about the loop during the execution of

00:08:41,440 --> 00:08:46,160
the application

00:08:42,800 --> 00:08:49,440
so they can adapt for system

00:08:46,160 --> 00:08:50,880
variations uh system heterogeneity or

00:08:49,440 --> 00:08:53,440
even

00:08:50,880 --> 00:08:54,160
for load imbalancing coming from the

00:08:53,440 --> 00:08:58,399
application

00:08:54,160 --> 00:09:01,440
itself so

00:08:58,399 --> 00:09:03,040
lb4 mp also provides some performance

00:09:01,440 --> 00:09:04,959
measurement features

00:09:03,040 --> 00:09:06,160
it can capture the execution time of

00:09:04,959 --> 00:09:09,360
each thread

00:09:06,160 --> 00:09:12,080
this is how the output look like so it

00:09:09,360 --> 00:09:14,480
prints the looper currency which is

00:09:12,080 --> 00:09:18,640
basically just a counter

00:09:14,480 --> 00:09:21,200
that registers how many times uh

00:09:18,640 --> 00:09:23,120
a loop was executed during a whole

00:09:21,200 --> 00:09:26,640
execution of an application

00:09:23,120 --> 00:09:28,800
so this is mostly important for

00:09:26,640 --> 00:09:30,399
time stepping applications which can

00:09:28,800 --> 00:09:34,240
execute the same loop

00:09:30,399 --> 00:09:37,360
many times then it brings the location

00:09:34,240 --> 00:09:41,600
which indicates

00:09:37,360 --> 00:09:41,600
where in the search code

00:09:41,760 --> 00:09:48,959
the respective loop can be found

00:09:46,000 --> 00:09:49,680
it prints the number of iterations the

00:09:48,959 --> 00:09:54,160
thread

00:09:49,680 --> 00:09:56,959
id and the execution time of this thread

00:09:54,160 --> 00:09:58,080
so it all also allows the user to

00:09:56,959 --> 00:10:02,480
capture

00:09:58,080 --> 00:10:04,560
the parallel the the parallel execution

00:10:02,480 --> 00:10:08,560
time of the loop

00:10:04,560 --> 00:10:10,480
so this is how the output part is look

00:10:08,560 --> 00:10:12,160
like it also prints the location the

00:10:10,480 --> 00:10:14,000
number of iterations of the loop

00:10:12,160 --> 00:10:15,680
and the parallel execution time of the

00:10:14,000 --> 00:10:19,360
loop

00:10:15,680 --> 00:10:22,640
then we also developed a

00:10:19,360 --> 00:10:26,399
functionality that records every

00:10:22,640 --> 00:10:30,000
scheduling round that was performed

00:10:26,399 --> 00:10:32,480
during the execution of a loop

00:10:30,000 --> 00:10:33,279
so this functionality also prints the

00:10:32,480 --> 00:10:36,000
location

00:10:33,279 --> 00:10:36,640
of the loop and it prints the lower

00:10:36,000 --> 00:10:40,399
bound and

00:10:36,640 --> 00:10:41,440
upper bound so these are basically the

00:10:40,399 --> 00:10:45,120
location

00:10:41,440 --> 00:10:48,160
in the main queue of iteration where

00:10:45,120 --> 00:10:50,000
the chunk of iterations start so the

00:10:48,160 --> 00:10:50,800
lower bound and where the chunk of

00:10:50,000 --> 00:10:53,839
iterations

00:10:50,800 --> 00:10:56,959
ends as the upper bound

00:10:53,839 --> 00:11:00,000
then it prints the chunk size or

00:10:56,959 --> 00:11:02,480
the amount of iterations and the thread

00:11:00,000 --> 00:11:04,480
id that you receive or that received

00:11:02,480 --> 00:11:06,079
these amount of iterations

00:11:04,480 --> 00:11:07,920
it's very important to highlight here

00:11:06,079 --> 00:11:11,200
that this functionality

00:11:07,920 --> 00:11:14,640
can produce very very large files

00:11:11,200 --> 00:11:16,000
since it is it registers information for

00:11:14,640 --> 00:11:18,480
every scheduling round

00:11:16,000 --> 00:11:19,519
so depending on the number of iterations

00:11:18,480 --> 00:11:22,560
that they look

00:11:19,519 --> 00:11:24,560
has and the number of scheduling rounds

00:11:22,560 --> 00:11:28,320
that are performed the number of threads

00:11:24,560 --> 00:11:31,760
so it scales pretty quickly so

00:11:28,320 --> 00:11:34,880
lastly we provide a profiling function

00:11:31,760 --> 00:11:38,160
which is exactly

00:11:34,880 --> 00:11:39,600
which produces a an output that is in

00:11:38,160 --> 00:11:42,399
the exact format

00:11:39,600 --> 00:11:44,399
expected by the scheduling technique

00:11:42,399 --> 00:11:47,360
that require profiling

00:11:44,399 --> 00:11:48,240
so this profiling information basically

00:11:47,360 --> 00:11:51,279
contains

00:11:48,240 --> 00:11:54,000
again the location of the loop the

00:11:51,279 --> 00:11:55,440
mean iteration execution time and the

00:11:54,000 --> 00:11:59,440
standard deviation

00:11:55,440 --> 00:12:01,680
of the execution time of the iterations

00:11:59,440 --> 00:12:01,680
so

00:12:04,800 --> 00:12:12,959
we have some basic steps so

00:12:08,639 --> 00:12:14,000
the most uh basic usage let's say like

00:12:12,959 --> 00:12:18,480
this

00:12:14,000 --> 00:12:22,240
but one has to check if the target loops

00:12:18,480 --> 00:12:25,440
in the application already contain

00:12:22,240 --> 00:12:26,320
the schedule run time close if that's

00:12:25,440 --> 00:12:28,639
the case

00:12:26,320 --> 00:12:29,839
then the application does not have to be

00:12:28,639 --> 00:12:31,040
recompiled and

00:12:29,839 --> 00:12:33,040
nothing has to be done in the

00:12:31,040 --> 00:12:35,920
application however if

00:12:33,040 --> 00:12:36,880
it does not contain the schedule runtime

00:12:35,920 --> 00:12:39,760
clause

00:12:36,880 --> 00:12:40,240
then you have to add it to the openmp

00:12:39,760 --> 00:12:43,279
loop

00:12:40,240 --> 00:12:46,399
and recompile the application

00:12:43,279 --> 00:12:49,839
so next you have to add

00:12:46,399 --> 00:12:51,680
the path to the compiled lb for omp to

00:12:49,839 --> 00:12:54,240
the linker environment variable

00:12:51,680 --> 00:12:54,880
so for instance in linux you can add it

00:12:54,240 --> 00:12:58,079
to

00:12:54,880 --> 00:13:01,120
ld library path

00:12:58,079 --> 00:13:03,440
and lastly you just have to select the

00:13:01,120 --> 00:13:04,160
scheduling technique that you want to

00:13:03,440 --> 00:13:07,519
use

00:13:04,160 --> 00:13:10,399
using the standard or omp schedule

00:13:07,519 --> 00:13:11,440
environment variable and passing the

00:13:10,399 --> 00:13:14,880
technique

00:13:11,440 --> 00:13:17,760
and the chunk size however

00:13:14,880 --> 00:13:19,360
for at least the current version of lb

00:13:17,760 --> 00:13:22,720
for omp

00:13:19,360 --> 00:13:24,320
you still have to provide another

00:13:22,720 --> 00:13:28,720
environment variable which is called

00:13:24,320 --> 00:13:31,760
knp cpu speed and it expects

00:13:28,720 --> 00:13:33,120
the clock frequency of the processors in

00:13:31,760 --> 00:13:36,480
megahertz

00:13:33,120 --> 00:13:37,279
so this can be taken from using the

00:13:36,480 --> 00:13:41,360
command

00:13:37,279 --> 00:13:44,800
cat proc cpu info and that is

00:13:41,360 --> 00:13:49,040
mainly used for some low overhand

00:13:44,800 --> 00:13:52,399
timer functions inside lb forum

00:13:49,040 --> 00:13:54,560
okay so to use the performance

00:13:52,399 --> 00:13:55,600
measurement features available in lb for

00:13:54,560 --> 00:13:58,639
mp

00:13:55,600 --> 00:14:01,519
so to capture the threads execution time

00:13:58,639 --> 00:14:02,880
and the execution time of the loop one

00:14:01,519 --> 00:14:05,519
just have to set

00:14:02,880 --> 00:14:05,920
the environment variable called the knp

00:14:05,519 --> 00:14:09,839
time

00:14:05,920 --> 00:14:13,680
loops and define a path and the file

00:14:09,839 --> 00:14:13,680
where this information will be stored

00:14:13,920 --> 00:14:18,720
to register all the scheduling rounds

00:14:16,800 --> 00:14:20,480
and the chunk sizes calculated for each

00:14:18,720 --> 00:14:22,959
trade one has to

00:14:20,480 --> 00:14:24,399
export to the environment variable kmp

00:14:22,959 --> 00:14:27,440
print chunks

00:14:24,399 --> 00:14:28,000
and also again the environment variable

00:14:27,440 --> 00:14:31,760
knp

00:14:28,000 --> 00:14:35,279
time loops defining the file where

00:14:31,760 --> 00:14:38,959
this information will be stored lastly

00:14:35,279 --> 00:14:42,800
the profiling features

00:14:38,959 --> 00:14:46,079
work slightly different so

00:14:42,800 --> 00:14:46,560
to generate the profile information you

00:14:46,079 --> 00:14:51,199
have

00:14:46,560 --> 00:14:54,240
to define omp schedule as profiling

00:14:51,199 --> 00:14:55,519
and you have to define another

00:14:54,240 --> 00:14:58,720
environment variable called

00:14:55,519 --> 00:15:01,600
knp profile data and define where you

00:14:58,720 --> 00:15:04,720
want to store this profile data

00:15:01,600 --> 00:15:05,279
later on the the same key np profile

00:15:04,720 --> 00:15:07,920
data

00:15:05,279 --> 00:15:10,160
environment variable can be used

00:15:07,920 --> 00:15:12,399
together with the techniques that

00:15:10,160 --> 00:15:15,920
require profiling information

00:15:12,399 --> 00:15:19,440
to indicate where they should read

00:15:15,920 --> 00:15:19,440
this profiling information

00:15:20,399 --> 00:15:26,800
so now our

00:15:23,519 --> 00:15:29,519
performance evaluation so we

00:15:26,800 --> 00:15:29,920
approached an application called thinx

00:15:29,519 --> 00:15:33,360
it's

00:15:29,920 --> 00:15:37,279
a smooth particle hydrodynamics

00:15:33,360 --> 00:15:40,399
code it performs some astrophysics

00:15:37,279 --> 00:15:44,240
simulations such as the collision of

00:15:40,399 --> 00:15:47,199
stars or stars collapsing

00:15:44,240 --> 00:15:49,519
and it was executed the five times for

00:15:47,199 --> 00:15:52,880
each configuration

00:15:49,519 --> 00:15:56,399
so it has two main loops

00:15:52,880 --> 00:15:59,920
each of them has 1 million iterations

00:15:56,399 --> 00:16:03,199
and they are executed 20 times

00:15:59,920 --> 00:16:06,720
for each whole execution of sphinx

00:16:03,199 --> 00:16:10,079
which means that sphinx is a

00:16:06,720 --> 00:16:11,759
time stepping application so these two

00:16:10,079 --> 00:16:14,880
loops are called l0

00:16:11,759 --> 00:16:18,160
which just perform fine neighbors

00:16:14,880 --> 00:16:21,519
operation and

00:16:18,160 --> 00:16:22,000
l1 which performs a gravity calculation

00:16:21,519 --> 00:16:24,959
which

00:16:22,000 --> 00:16:25,759
basically calculates the gravity that

00:16:24,959 --> 00:16:28,399
the particle

00:16:25,759 --> 00:16:30,959
how the particles interact regarding the

00:16:28,399 --> 00:16:30,959
gravity

00:16:31,199 --> 00:16:36,800
so we executed

00:16:34,320 --> 00:16:39,519
experiments on queen node pipe so type

00:16:36,800 --> 00:16:40,560
way a is a intel broadwell configured in

00:16:39,519 --> 00:16:44,000
two sockets

00:16:40,560 --> 00:16:47,040
10 cars each type b

00:16:44,000 --> 00:16:51,199
is a knl node which

00:16:47,040 --> 00:16:54,639
is configured in one socket and 64 cores

00:16:51,199 --> 00:16:57,920
and the type three which is a

00:16:54,639 --> 00:17:00,560
intel xeon we've

00:16:57,920 --> 00:17:02,639
configured in one socket and it has 12

00:17:00,560 --> 00:17:06,000
cores

00:17:02,639 --> 00:17:06,559
so the metrics that we evaluate and that

00:17:06,000 --> 00:17:09,280
we

00:17:06,559 --> 00:17:10,880
collect are the parallel execution time

00:17:09,280 --> 00:17:14,079
in the application

00:17:10,880 --> 00:17:15,199
and the par the exclusive parallel

00:17:14,079 --> 00:17:18,799
execution time of

00:17:15,199 --> 00:17:22,240
each loop so by doing so

00:17:18,799 --> 00:17:25,039
we can also derive a third

00:17:22,240 --> 00:17:26,640
metric so since we have the execution

00:17:25,039 --> 00:17:30,080
time of each loop

00:17:26,640 --> 00:17:33,360
we can detect which scheduling technique

00:17:30,080 --> 00:17:36,080
provided the highest performance for

00:17:33,360 --> 00:17:37,840
a given loop and with that we can

00:17:36,080 --> 00:17:41,280
compose a combination

00:17:37,840 --> 00:17:44,840
with of the highest performing

00:17:41,280 --> 00:17:48,880
scheduling techniques that we call

00:17:44,840 --> 00:17:51,679
best and then we can show

00:17:48,880 --> 00:17:53,600
how much performance is lost if you

00:17:51,679 --> 00:17:55,600
execute

00:17:53,600 --> 00:17:56,960
all the loops of an application or in

00:17:55,600 --> 00:17:59,600
this case both

00:17:56,960 --> 00:18:02,640
loop of the application with a single

00:17:59,600 --> 00:18:02,640
scheduling technique

00:18:03,039 --> 00:18:08,320
and yes extinct and lb 4mp were compiled

00:18:07,120 --> 00:18:12,400
with intel

00:18:08,320 --> 00:18:15,919
uh compiler and the threads were

00:18:12,400 --> 00:18:16,480
configured with omp places course and

00:18:15,919 --> 00:18:19,840
omp

00:18:16,480 --> 00:18:19,840
proc buying close

00:18:25,360 --> 00:18:32,960
so here you see the

00:18:29,200 --> 00:18:36,160
uh first plot for the node type a

00:18:32,960 --> 00:18:38,000
so in the x axis you can see

00:18:36,160 --> 00:18:40,000
the loop scheduling techniques and in

00:18:38,000 --> 00:18:41,679
the y-axis you see the parallel

00:18:40,000 --> 00:18:43,760
execution time

00:18:41,679 --> 00:18:45,039
so if you look at the bars you will

00:18:43,760 --> 00:18:48,720
notice that they

00:18:45,039 --> 00:18:52,320
show different colors so in light green

00:18:48,720 --> 00:18:55,440
uh it has shown the performance of

00:18:52,320 --> 00:18:57,760
a loop l0 in dark green

00:18:55,440 --> 00:19:00,320
the performance of loop l1 which is

00:18:57,760 --> 00:19:03,440
actually the most timing consuming loop

00:19:00,320 --> 00:19:06,960
of the application and in

00:19:03,440 --> 00:19:10,320
dark blue it's a t-o-l

00:19:06,960 --> 00:19:11,039
which extends for time outside of the

00:19:10,320 --> 00:19:13,679
loops

00:19:11,039 --> 00:19:16,240
which is basically the remaining time of

00:19:13,679 --> 00:19:19,360
the application which is not affected by

00:19:16,240 --> 00:19:21,679
the scheduling techniques in this case

00:19:19,360 --> 00:19:22,960
so the background of the plots also

00:19:21,679 --> 00:19:25,039
highlight

00:19:22,960 --> 00:19:28,160
let's say from where the techniques come

00:19:25,039 --> 00:19:30,320
from so with a white background you see

00:19:28,160 --> 00:19:31,440
the openmp standard scheduling

00:19:30,320 --> 00:19:33,679
techniques

00:19:31,440 --> 00:19:35,600
in gray you see trapezoid self

00:19:33,679 --> 00:19:38,880
scheduling or tss

00:19:35,600 --> 00:19:41,360
which was already implemented on llvm

00:19:38,880 --> 00:19:43,440
and with a green background you see the

00:19:41,360 --> 00:19:46,960
techniques that were implemented in

00:19:43,440 --> 00:19:50,840
lb for omp so and

00:19:46,960 --> 00:19:53,440
with the light pink

00:19:50,840 --> 00:19:57,120
uh or light red

00:19:53,440 --> 00:20:01,120
background you see the best combination

00:19:57,120 --> 00:20:03,280
of technique so at this point you may

00:20:01,120 --> 00:20:06,400
have noticed that

00:20:03,280 --> 00:20:09,600
also some bars are highlighted

00:20:06,400 --> 00:20:13,919
with a red rectangle so

00:20:09,600 --> 00:20:17,039
these red rectangles are identifying

00:20:13,919 --> 00:20:19,120
which is the best scheduling technique

00:20:17,039 --> 00:20:22,400
for a given loop

00:20:19,120 --> 00:20:23,600
and again the best column on the right

00:20:22,400 --> 00:20:27,200
of the plot

00:20:23,600 --> 00:20:29,679
shows the achievable performance

00:20:27,200 --> 00:20:31,120
by combining all the best scheduling

00:20:29,679 --> 00:20:34,960
technique

00:20:31,120 --> 00:20:37,280
in this specific case fsc

00:20:34,960 --> 00:20:38,960
achieved the highest performance in both

00:20:37,280 --> 00:20:41,760
loops so that's why

00:20:38,960 --> 00:20:42,559
you can see that both loops are

00:20:41,760 --> 00:20:46,159
highlighted

00:20:42,559 --> 00:20:49,280
on the fsc column

00:20:46,159 --> 00:20:51,520
so it's also important to highlight that

00:20:49,280 --> 00:20:53,039
the percentages that you see on the

00:20:51,520 --> 00:20:56,640
plots

00:20:53,039 --> 00:20:59,840
uh they represent

00:20:56,640 --> 00:21:02,880
how much performance is lost

00:20:59,840 --> 00:21:07,280
by executing both loop

00:21:02,880 --> 00:21:09,919
with the indicated scheduling technique

00:21:07,280 --> 00:21:12,080
so this means that lower values are

00:21:09,919 --> 00:21:14,960
better

00:21:12,080 --> 00:21:16,000
so here we can make some other

00:21:14,960 --> 00:21:19,840
observations we

00:21:16,000 --> 00:21:23,880
can see that best

00:21:19,840 --> 00:21:26,480
which in this case is fsc achieve up to

00:21:23,880 --> 00:21:28,320
13.32 percent uh

00:21:26,480 --> 00:21:29,520
higher performance than the best

00:21:28,320 --> 00:21:33,360
standard

00:21:29,520 --> 00:21:36,080
which in this case is gss and

00:21:33,360 --> 00:21:38,480
actually we can also derive the

00:21:36,080 --> 00:21:41,919
difference between

00:21:38,480 --> 00:21:45,679
maf for instance versus guided so

00:21:41,919 --> 00:21:49,679
maf was just presented

00:21:45,679 --> 00:21:53,440
just 3.73 percent lower performance

00:21:49,679 --> 00:21:56,880
compared to the best combination

00:21:53,440 --> 00:22:00,480
however in comparison to gss

00:21:56,880 --> 00:22:04,159
it outperforms gss by 9.

00:22:00,480 --> 00:22:04,159
almost 9.6 percent

00:22:04,960 --> 00:22:08,720
so in the next plot

00:22:10,159 --> 00:22:13,360
it the plot itself is in the same

00:22:12,400 --> 00:22:16,640
configuration

00:22:13,360 --> 00:22:19,840
in as the previous however here

00:22:16,640 --> 00:22:23,120
we can already see uh that

00:22:19,840 --> 00:22:25,679
the combination of that the best

00:22:23,120 --> 00:22:26,000
combination is not a single technique so

00:22:25,679 --> 00:22:29,360
here

00:22:26,000 --> 00:22:32,799
it is a combination of fsc and

00:22:29,360 --> 00:22:34,960
maf and

00:22:32,799 --> 00:22:36,320
we can also it's important to highlight

00:22:34,960 --> 00:22:39,280
that

00:22:36,320 --> 00:22:40,320
so for the first time fsc was able to

00:22:39,280 --> 00:22:43,360
achieve the

00:22:40,320 --> 00:22:46,400
highest performance overall alone

00:22:43,360 --> 00:22:48,480
although in another system

00:22:46,400 --> 00:22:50,480
it could not maintain the same

00:22:48,480 --> 00:22:54,080
performance exactly because

00:22:50,480 --> 00:22:57,679
it's not an adaptive technique so it may

00:22:54,080 --> 00:23:00,880
suffer with a different system

00:22:57,679 --> 00:23:03,120
and so on so in this case fsc was

00:23:00,880 --> 00:23:05,520
actually 23 percent

00:23:03,120 --> 00:23:06,880
presented between 23 percent lower

00:23:05,520 --> 00:23:10,080
performance than

00:23:06,880 --> 00:23:12,159
the best combination it's also important

00:23:10,080 --> 00:23:13,360
to highlight that although the system

00:23:12,159 --> 00:23:16,640
changed

00:23:13,360 --> 00:23:18,400
uh the adaptive techniques remained with

00:23:16,640 --> 00:23:19,760
presenting consistent and high

00:23:18,400 --> 00:23:23,520
performance

00:23:19,760 --> 00:23:27,320
and they can even spew out perform

00:23:23,520 --> 00:23:31,200
gss by not much like

00:23:27,320 --> 00:23:38,159
0.75 but still

00:23:31,200 --> 00:23:41,600
so finally uh this is the third system

00:23:38,159 --> 00:23:42,960
so the plot is again with the same

00:23:41,600 --> 00:23:46,880
configuration

00:23:42,960 --> 00:23:46,880
so it is uh

00:23:47,520 --> 00:23:51,200
yeah it it the configuration of the plot

00:23:50,000 --> 00:23:54,159
doesn't change

00:23:51,200 --> 00:23:54,960
and here we can see that uh once again

00:23:54,159 --> 00:23:57,279
uh

00:23:54,960 --> 00:23:59,039
the best is a combination of different

00:23:57,279 --> 00:24:03,120
scheduling techniques

00:23:59,039 --> 00:24:06,720
and it's actually again fsc and maf

00:24:03,120 --> 00:24:10,080
however at this time fsc is

00:24:06,720 --> 00:24:11,360
practically the best alone it was

00:24:10,080 --> 00:24:15,120
outperformed by

00:24:11,360 --> 00:24:19,440
only 0.01 percent so that is

00:24:15,120 --> 00:24:22,240
that is it's it's low

00:24:19,440 --> 00:24:23,440
uh on the other hand we can see that

00:24:22,240 --> 00:24:27,400
again

00:24:23,440 --> 00:24:30,559
uh best outperformed gss by

00:24:27,400 --> 00:24:35,760
12.89 and that

00:24:30,559 --> 00:24:40,559
maf alone outperformed the gss again by

00:24:35,760 --> 00:24:45,120
10.6 percent so we were able to achieve

00:24:40,559 --> 00:24:49,679
around 10 percent higher performance in

00:24:45,120 --> 00:24:52,799
two systems with adaptive techniques

00:24:49,679 --> 00:24:54,960
and the last comment is that the

00:24:52,799 --> 00:24:59,200
adaptive techniques were able to

00:24:54,960 --> 00:25:03,520
maintain high performance across

00:24:59,200 --> 00:25:03,520
the three different platforms

00:25:03,679 --> 00:25:10,720
so some final considerations

00:25:07,279 --> 00:25:14,320
so lb for omp

00:25:10,720 --> 00:25:17,919
indeed bridge the gap between

00:25:14,320 --> 00:25:22,080
what is proposed in the literature and

00:25:17,919 --> 00:25:25,520
what is what is now finally implemented

00:25:22,080 --> 00:25:28,799
in the in the pr in practice

00:25:25,520 --> 00:25:30,480
for multi-threaded applications so lb

00:25:28,799 --> 00:25:33,919
for mp contains

00:25:30,480 --> 00:25:37,120
14 additional to the openmp standard

00:25:33,919 --> 00:25:37,440
scheduling techniques including dynamic

00:25:37,120 --> 00:25:41,279
and

00:25:37,440 --> 00:25:44,400
adaptive edging techniques

00:25:41,279 --> 00:25:47,840
uh we did this also as

00:25:44,400 --> 00:25:50,640
a first very much necessary step for

00:25:47,840 --> 00:25:51,760
a auto tuning approach for load

00:25:50,640 --> 00:25:55,039
balancing

00:25:51,760 --> 00:25:58,640
in openmp since without

00:25:55,039 --> 00:26:00,240
all the portfolio as i commented in the

00:25:58,640 --> 00:26:03,360
beginning

00:26:00,240 --> 00:26:04,080
the development of novo load balancing

00:26:03,360 --> 00:26:07,600
approach

00:26:04,080 --> 00:26:07,600
is very hard

00:26:08,840 --> 00:26:14,400
so another

00:26:11,120 --> 00:26:16,400
important message is that

00:26:14,400 --> 00:26:19,679
it's very clear that the different

00:26:16,400 --> 00:26:23,279
looped in an application have different

00:26:19,679 --> 00:26:26,880
load balancing need so

00:26:23,279 --> 00:26:27,679
we identified that the best achievable

00:26:26,880 --> 00:26:30,720
performance

00:26:27,679 --> 00:26:31,600
is a combination of different scheduling

00:26:30,720 --> 00:26:34,240
techniques

00:26:31,600 --> 00:26:35,679
very frequently including the ones that

00:26:34,240 --> 00:26:38,880
were implemented now

00:26:35,679 --> 00:26:42,559
in lb for mp however

00:26:38,880 --> 00:26:45,679
it is also a little bit impractical

00:26:42,559 --> 00:26:48,400
to try to achieve or to find

00:26:45,679 --> 00:26:48,799
this best combination since you have to

00:26:48,400 --> 00:26:52,640
make

00:26:48,799 --> 00:26:56,960
really huge exp experiments

00:26:52,640 --> 00:27:00,159
and it takes too much time so too costly

00:26:56,960 --> 00:27:02,159
however the dynamic and adaptive

00:27:00,159 --> 00:27:03,120
self-scheduling techniques were able to

00:27:02,159 --> 00:27:05,919
achieve

00:27:03,120 --> 00:27:06,960
very promising every group and they are

00:27:05,919 --> 00:27:10,080
a very promising

00:27:06,960 --> 00:27:11,360
alternative to what you a performance

00:27:10,080 --> 00:27:14,880
close to the best

00:27:11,360 --> 00:27:17,840
combination so

00:27:14,880 --> 00:27:19,919
what's coming next so we plan to patch

00:27:17,840 --> 00:27:20,880
and upstream the scheduling technique

00:27:19,919 --> 00:27:26,960
implemented

00:27:20,880 --> 00:27:30,080
to uh in la in lb for mp to the llvm

00:27:26,960 --> 00:27:34,000
openmp runtime library and we are

00:27:30,080 --> 00:27:37,200
already working on an automated approach

00:27:34,000 --> 00:27:39,360
to achieve the performance of

00:27:37,200 --> 00:27:41,120
the best combination of scheduling

00:27:39,360 --> 00:27:44,480
techniques

00:27:41,120 --> 00:27:48,720
and last but not least visit

00:27:44,480 --> 00:27:51,600
us on github if you work with uh

00:27:48,720 --> 00:27:52,320
applications with large openmp loop you

00:27:51,600 --> 00:27:54,480
may

00:27:52,320 --> 00:27:57,039
achieve a higher performance than you

00:27:54,480 --> 00:27:59,840
have now

00:27:57,039 --> 00:28:01,520
and don't forget to check the

00:27:59,840 --> 00:28:06,399
presentation that will be

00:28:01,520 --> 00:28:13,279
at openmp.org and

00:28:06,399 --> 00:28:13,279

YouTube URL: https://www.youtube.com/watch?v=jET6EYJSBU4


