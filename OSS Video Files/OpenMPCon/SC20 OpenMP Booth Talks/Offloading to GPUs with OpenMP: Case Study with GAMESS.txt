Title: Offloading to GPUs with OpenMP: Case Study with GAMESS
Publication date: 2020-10-31
Playlist: SC20 OpenMP Booth Talks
Description: 
	This presentation is by Colleen Bertoni and JaeHyuk Kwack of Argonne National Laboratory, as well as Buu Pham of Iowa State University. It is part of the OpenMP Booth Talk series created for Supercomputing 2020. 

A PDF of this presentation as well as more videos from this series can be downloaded at https://www.openmp.org/events/openmp-sc20/
Captions: 
	00:00:02,879 --> 00:00:05,040
hi

00:00:03,199 --> 00:00:06,560
my name is colin bertoni i'm an

00:00:05,040 --> 00:00:07,839
assistant computational scientist at

00:00:06,560 --> 00:00:09,120
argonne national lab

00:00:07,839 --> 00:00:11,120
i'm going to talk about some work i've

00:00:09,120 --> 00:00:12,080
done in collaboration with j-hook kwok

00:00:11,120 --> 00:00:14,080
and bufam

00:00:12,080 --> 00:00:16,160
the work involved porting from cpu to

00:00:14,080 --> 00:00:17,199
gpu two methods in the quantum chemistry

00:00:16,160 --> 00:00:18,480
package games

00:00:17,199 --> 00:00:21,199
and how we did it with the features

00:00:18,480 --> 00:00:23,119
available in openmp offloading

00:00:21,199 --> 00:00:25,119
so first just a couple of

00:00:23,119 --> 00:00:26,400
acknowledgements uh thanks to the people

00:00:25,119 --> 00:00:28,640
on the games team who do

00:00:26,400 --> 00:00:30,160
a lot of the the work as well colleagues

00:00:28,640 --> 00:00:32,399
at argonne national lab

00:00:30,160 --> 00:00:34,239
and also acknowledgments for funding and

00:00:32,399 --> 00:00:35,600
computer time used to the exoskeletal

00:00:34,239 --> 00:00:40,800
computing project

00:00:35,600 --> 00:00:42,320
and use of the jlsc system at argonne

00:00:40,800 --> 00:00:44,079
so first i wanted to give a quick high

00:00:42,320 --> 00:00:45,360
level introduction to games so you can

00:00:44,079 --> 00:00:46,320
understand what type of code we're

00:00:45,360 --> 00:00:47,840
dealing with

00:00:46,320 --> 00:00:49,039
it's a general purpose electronic

00:00:47,840 --> 00:00:50,079
structure code which has a lot of

00:00:49,039 --> 00:00:51,680
functionality

00:00:50,079 --> 00:00:54,160
it's large about a million lines of

00:00:51,680 --> 00:00:55,680
fortran it's been basically around since

00:00:54,160 --> 00:00:57,760
the 1980s as well

00:00:55,680 --> 00:00:59,840
it has some optional c people's plus

00:00:57,760 --> 00:01:01,199
libraries with gpu accelerated code with

00:00:59,840 --> 00:01:03,600
cuda

00:01:01,199 --> 00:01:04,400
one of the exascale computing projects

00:01:03,600 --> 00:01:07,439
main focuses

00:01:04,400 --> 00:01:09,360
is to use a method in games fmori mp2

00:01:07,439 --> 00:01:11,439
to run accurate simulations of catalysis

00:01:09,360 --> 00:01:12,159
reactions inside of giant molecules like

00:01:11,439 --> 00:01:14,479
this

00:01:12,159 --> 00:01:15,680
mesoporous silicon nanoparticle shown on

00:01:14,479 --> 00:01:17,600
the right

00:01:15,680 --> 00:01:20,159
the this current code is paralyzed with

00:01:17,600 --> 00:01:21,759
mpi plus openmp cpu threading

00:01:20,159 --> 00:01:23,600
and although this can scale well on

00:01:21,759 --> 00:01:26,080
current cpu only systems

00:01:23,600 --> 00:01:27,280
on upcoming systems the majority of the

00:01:26,080 --> 00:01:29,680
computing power

00:01:27,280 --> 00:01:31,200
will be on gpus so the node level

00:01:29,680 --> 00:01:32,799
parallel programming model for several

00:01:31,200 --> 00:01:35,439
of the methods

00:01:32,799 --> 00:01:36,159
in games has been updated to use openmp

00:01:35,439 --> 00:01:37,759
to offload

00:01:36,159 --> 00:01:39,520
computationally expensive regions to

00:01:37,759 --> 00:01:41,759
gpus as well

00:01:39,520 --> 00:01:43,119
and that's our focus here we'll focus on

00:01:41,759 --> 00:01:45,600
the gpu porting of

00:01:43,119 --> 00:01:47,520
the hardref method and second order

00:01:45,600 --> 00:01:48,159
molar plastic perturbation theory with

00:01:47,520 --> 00:01:51,840
the

00:01:48,159 --> 00:01:55,759
ri approximation or ramp2 method

00:01:51,840 --> 00:01:57,360
using openmp

00:01:55,759 --> 00:02:00,000
so a little more detail about the

00:01:57,360 --> 00:02:00,960
methods first in terms of computational

00:02:00,000 --> 00:02:03,040
motifs

00:02:00,960 --> 00:02:05,200
the hartree fock method solves a set of

00:02:03,040 --> 00:02:07,119
nonlinear eigenvalue equations

00:02:05,200 --> 00:02:08,319
iteratively for the energy of a

00:02:07,119 --> 00:02:10,080
molecular system

00:02:08,319 --> 00:02:12,480
so there are two main bottlenecks in

00:02:10,080 --> 00:02:13,440
this method the computation of a huge

00:02:12,480 --> 00:02:15,360
number

00:02:13,440 --> 00:02:17,440
uh basically n to the fourth where n is

00:02:15,360 --> 00:02:19,760
a measure of molecular system size

00:02:17,440 --> 00:02:20,720
of four index two electron repulsion

00:02:19,760 --> 00:02:23,440
integrals

00:02:20,720 --> 00:02:24,239
and also um forming the foc matrix by

00:02:23,440 --> 00:02:27,120
contracting

00:02:24,239 --> 00:02:29,280
that n to the fourth uh integral tensor

00:02:27,120 --> 00:02:31,360
with a density matrix

00:02:29,280 --> 00:02:33,360
um the hartifact method is a fundamental

00:02:31,360 --> 00:02:35,120
method which is a starting point for

00:02:33,360 --> 00:02:36,800
many higher accuracy methods such as

00:02:35,120 --> 00:02:39,120
perturbative corrections

00:02:36,800 --> 00:02:41,440
and the next method the rmb2 method is

00:02:39,120 --> 00:02:43,680
one such correction

00:02:41,440 --> 00:02:45,599
so by using the ri approximation the

00:02:43,680 --> 00:02:46,800
integral evaluation is simplified from

00:02:45,599 --> 00:02:48,640
four index to

00:02:46,800 --> 00:02:50,800
three index two electron repulsion

00:02:48,640 --> 00:02:52,640
integrals and this allows the use of

00:02:50,800 --> 00:02:53,920
efficient matrix multiplication

00:02:52,640 --> 00:02:55,120
operations

00:02:53,920 --> 00:02:58,640
um so these are the methods we're

00:02:55,120 --> 00:03:00,800
targeting to offload to gpus

00:02:58,640 --> 00:03:02,800
just quickly before going on i wanted to

00:03:00,800 --> 00:03:05,440
talk about some gpu background to help

00:03:02,800 --> 00:03:07,200
motivate some of the porting we did

00:03:05,440 --> 00:03:09,519
so we're going to use a specific example

00:03:07,200 --> 00:03:11,760
of a v100 here

00:03:09,519 --> 00:03:13,360
so in the hardware gpus are sort of

00:03:11,760 --> 00:03:14,239
organized hierarchically where you can

00:03:13,360 --> 00:03:16,319
think about

00:03:14,239 --> 00:03:18,080
different levels of the hierarchy as

00:03:16,319 --> 00:03:19,360
defined by the resources that are shared

00:03:18,080 --> 00:03:21,040
at that level

00:03:19,360 --> 00:03:23,360
so in v100s there are streaming

00:03:21,040 --> 00:03:27,040
multiprocessors or sms so the

00:03:23,360 --> 00:03:27,840
boxes inside of the gpu and cuda cores

00:03:27,040 --> 00:03:30,480
inside each

00:03:27,840 --> 00:03:30,879
sm um and this is important to note

00:03:30,480 --> 00:03:32,720
because

00:03:30,879 --> 00:03:34,879
inside each sm there's sort of a base

00:03:32,720 --> 00:03:37,599
unit of execution called the warp

00:03:34,879 --> 00:03:38,480
which is basically 32 threads executing

00:03:37,599 --> 00:03:41,680
in lockstep

00:03:38,480 --> 00:03:43,360
on the cuda cores in an sm and so if the

00:03:41,680 --> 00:03:44,959
code is written in such a way that all

00:03:43,360 --> 00:03:45,840
the threads and the warp do different

00:03:44,959 --> 00:03:48,000
things

00:03:45,840 --> 00:03:49,599
then there's thread divergence and less

00:03:48,000 --> 00:03:51,760
efficient use of the hardware

00:03:49,599 --> 00:03:54,080
so two examples and we ran into these

00:03:51,760 --> 00:03:55,040
two examples let's i'll talk about that

00:03:54,080 --> 00:03:58,400
in a bit

00:03:55,040 --> 00:04:00,319
our load imbalance and branching

00:03:58,400 --> 00:04:02,319
so first for load and balance between

00:04:00,319 --> 00:04:03,599
threads you have an instance where one

00:04:02,319 --> 00:04:05,280
thread takes longer

00:04:03,599 --> 00:04:06,959
and the other is to compute something

00:04:05,280 --> 00:04:08,879
sort of shown here

00:04:06,959 --> 00:04:10,879
and branching which results in the warp

00:04:08,879 --> 00:04:12,319
having to execute the same code once for

00:04:10,879 --> 00:04:14,159
every branch

00:04:12,319 --> 00:04:16,639
with part of the warp mask out so sort

00:04:14,159 --> 00:04:19,600
of shown here with the

00:04:16,639 --> 00:04:20,880
having to execute this twice so that was

00:04:19,600 --> 00:04:23,360
just a tiny bit of

00:04:20,880 --> 00:04:24,479
um background about reporting to gpus to

00:04:23,360 --> 00:04:26,400
keep in mind

00:04:24,479 --> 00:04:29,840
for what i talked about later importing

00:04:26,400 --> 00:04:29,840
the code with games

00:04:30,000 --> 00:04:34,080
so starting with the hard default code

00:04:32,479 --> 00:04:35,759
here we focused on

00:04:34,080 --> 00:04:37,360
the four index two electron integral

00:04:35,759 --> 00:04:39,120
evaluations since these are a main

00:04:37,360 --> 00:04:41,120
combinational bottleneck

00:04:39,120 --> 00:04:43,520
the code was paralyzed previously with

00:04:41,120 --> 00:04:44,960
npi plus openmp cpu threading

00:04:43,520 --> 00:04:46,479
but it contained multiple levels of

00:04:44,960 --> 00:04:47,919
conditional statements and the work

00:04:46,479 --> 00:04:50,160
assigned to each thread was not

00:04:47,919 --> 00:04:54,800
equivalent leading to a load imbalance

00:04:50,160 --> 00:04:57,120
so the pseudo code on the left

00:04:54,800 --> 00:04:58,720
sort of shows what the original code was

00:04:57,120 --> 00:05:00,560
like

00:04:58,720 --> 00:05:03,120
basically the cpu threads were paralyzed

00:05:00,560 --> 00:05:04,560
over this large end of the fourth loop

00:05:03,120 --> 00:05:07,280
but each thread will call different

00:05:04,560 --> 00:05:09,280
methods so method one or method two

00:05:07,280 --> 00:05:10,639
and then even inside of each method

00:05:09,280 --> 00:05:13,280
there was even more branching to

00:05:10,639 --> 00:05:15,680
different interval type

00:05:13,280 --> 00:05:16,479
so this wasn't great for a gpu which

00:05:15,680 --> 00:05:18,240
wants

00:05:16,479 --> 00:05:20,800
all the threads in a warp to be doing

00:05:18,240 --> 00:05:22,160
the same thing

00:05:20,800 --> 00:05:24,000
so basically our strategy was to

00:05:22,160 --> 00:05:25,840
substantially reorganize the control

00:05:24,000 --> 00:05:27,520
flow an order in which integrals are

00:05:25,840 --> 00:05:28,720
computed by sorting the integrals ahead

00:05:27,520 --> 00:05:30,560
of time

00:05:28,720 --> 00:05:32,400
and removing separate conditionals into

00:05:30,560 --> 00:05:34,960
separate code blocks sort of as shown

00:05:32,400 --> 00:05:36,560
on the right so in general again this

00:05:34,960 --> 00:05:39,120
strategy is a generic

00:05:36,560 --> 00:05:40,320
transformation for cpu to gpu code and

00:05:39,120 --> 00:05:42,479
we just

00:05:40,320 --> 00:05:44,720
used openmp as our tool of choice to

00:05:42,479 --> 00:05:46,800
express these optimizations

00:05:44,720 --> 00:05:48,160
um and just noting even the code to

00:05:46,800 --> 00:05:50,080
actually do the work to compute the

00:05:48,160 --> 00:05:52,479
integrals like routines and

00:05:50,080 --> 00:05:54,240
wanted in to remain the same as before

00:05:52,479 --> 00:05:55,120
just the order and the flow in which we

00:05:54,240 --> 00:05:58,160
computed them

00:05:55,120 --> 00:05:59,280
was adjusted um and then after we've

00:05:58,160 --> 00:06:01,759
organized to sort

00:05:59,280 --> 00:06:03,600
the integrals and call methods

00:06:01,759 --> 00:06:05,520
separately

00:06:03,600 --> 00:06:07,440
the openmp directives to offload edit or

00:06:05,520 --> 00:06:07,840
minimal so i've shown the figure to the

00:06:07,440 --> 00:06:11,120
right

00:06:07,840 --> 00:06:13,680
we only used a few pragma here

00:06:11,120 --> 00:06:14,800
the target team distribute parallel do

00:06:13,680 --> 00:06:17,440
as well as

00:06:14,800 --> 00:06:18,880
adding declare target to note that we

00:06:17,440 --> 00:06:20,880
are calling routines from inside of

00:06:18,880 --> 00:06:22,800
target regions

00:06:20,880 --> 00:06:24,000
so this code is implemented for one type

00:06:22,800 --> 00:06:25,600
of integrals

00:06:24,000 --> 00:06:27,120
in a development branch of games and

00:06:25,600 --> 00:06:28,880
it's not yet available in the most

00:06:27,120 --> 00:06:30,479
public release of games but it will be

00:06:28,880 --> 00:06:32,000
soon

00:06:30,479 --> 00:06:33,520
um so just let's look at some results

00:06:32,000 --> 00:06:34,800
after we did all the hard work to do

00:06:33,520 --> 00:06:36,800
that

00:06:34,800 --> 00:06:39,039
um here we compare the wall clock time

00:06:36,800 --> 00:06:40,400
for running the original cpu code with

00:06:39,039 --> 00:06:44,080
42 threads

00:06:40,400 --> 00:06:44,560
on a power 9 cpu to the modified version

00:06:44,080 --> 00:06:47,520
with

00:06:44,560 --> 00:06:49,680
42 threads for a cpu portion and one v

00:06:47,520 --> 00:06:52,880
100 for the gpu

00:06:49,680 --> 00:06:54,400
portion um we're running with a variety

00:06:52,880 --> 00:06:56,080
of input sizes where which

00:06:54,400 --> 00:06:58,479
this is what this n denotes how many

00:06:56,080 --> 00:07:00,000
water molecules from 4 to 64.

00:06:58,479 --> 00:07:03,039
and basically what we ended up seeing

00:07:00,000 --> 00:07:06,240
was we can get up to about a 9x speed up

00:07:03,039 --> 00:07:10,560
for our largest input size uh

00:07:06,240 --> 00:07:13,759
speed up of the gpu time over cpu

00:07:10,560 --> 00:07:16,880
so this is a really nice end result

00:07:13,759 --> 00:07:18,560
but to get there we had some challenges

00:07:16,880 --> 00:07:20,160
in particular even though we had the

00:07:18,560 --> 00:07:21,199
needed functionality in the openmp

00:07:20,160 --> 00:07:23,039
specification

00:07:21,199 --> 00:07:24,400
we ran into unexpected performance

00:07:23,039 --> 00:07:26,639
issues for example

00:07:24,400 --> 00:07:28,639
in our initial gpu port we offloaded a

00:07:26,639 --> 00:07:30,240
region inside of a host function

00:07:28,639 --> 00:07:33,120
just called at each iteration of a

00:07:30,240 --> 00:07:35,759
solver so the offloaded

00:07:33,120 --> 00:07:37,599
uh region also mapped private variables

00:07:35,759 --> 00:07:39,440
to the gpu and it contained a function

00:07:37,599 --> 00:07:40,800
call which was marked as declare target

00:07:39,440 --> 00:07:43,120
to allow it to be called from an

00:07:40,800 --> 00:07:44,000
offloaded region when we tested our

00:07:43,120 --> 00:07:46,720
initial port of this

00:07:44,000 --> 00:07:47,280
using the ibm portion openmp compiler on

00:07:46,720 --> 00:07:48,560
summit

00:07:47,280 --> 00:07:50,319
we ended up seeing that the time per

00:07:48,560 --> 00:07:50,639
iteration of the solver increased which

00:07:50,319 --> 00:07:52,160
is

00:07:50,639 --> 00:07:53,680
basically unacceptable for an iterative

00:07:52,160 --> 00:07:55,280
solver because you're doing it

00:07:53,680 --> 00:07:57,360
multiple times and so it just took

00:07:55,280 --> 00:07:58,639
longer and longer

00:07:57,360 --> 00:08:00,479
so to work around this we ended up

00:07:58,639 --> 00:08:01,759
manually aligning the routines called in

00:08:00,479 --> 00:08:03,440
the offloaded region

00:08:01,759 --> 00:08:04,879
which resulted in the time for call

00:08:03,440 --> 00:08:06,720
remaining constant

00:08:04,879 --> 00:08:08,160
which is expected and like essential for

00:08:06,720 --> 00:08:11,759
code with an iterative

00:08:08,160 --> 00:08:14,000
solver so despite the declare target

00:08:11,759 --> 00:08:15,919
being part of the openmp specification

00:08:14,000 --> 00:08:16,960
the ibm compiler's performance in our

00:08:15,919 --> 00:08:18,879
case

00:08:16,960 --> 00:08:20,560
or a specific instance resulted in us

00:08:18,879 --> 00:08:21,840
not being able to use the functionality

00:08:20,560 --> 00:08:23,759
effectively

00:08:21,840 --> 00:08:25,120
this issue has been reported to ibm and

00:08:23,759 --> 00:08:26,879
it actually doesn't occur with other

00:08:25,120 --> 00:08:27,360
fortune compilers we've tried like the

00:08:26,879 --> 00:08:30,400
craig

00:08:27,360 --> 00:08:30,400
fortran compiler

00:08:31,360 --> 00:08:36,560
so moving on to the ramp2 code

00:08:34,640 --> 00:08:38,000
here we focus on the computation of the

00:08:36,560 --> 00:08:39,680
perturbative correction

00:08:38,000 --> 00:08:41,279
since this is the main bottleneck of the

00:08:39,680 --> 00:08:43,839
ram p2 method

00:08:41,279 --> 00:08:44,720
um just for some reference if you want

00:08:43,839 --> 00:08:46,160
to look at the math

00:08:44,720 --> 00:08:47,920
the energy term and the right

00:08:46,160 --> 00:08:48,240
approximation are shown on on the right

00:08:47,920 --> 00:08:52,080
just

00:08:48,240 --> 00:08:54,880
for reference so we can look at

00:08:52,080 --> 00:08:56,080
the original cpu code so the figure on

00:08:54,880 --> 00:08:59,279
the left

00:08:56,080 --> 00:09:01,120
is the original cpu algorithm basically

00:08:59,279 --> 00:09:03,360
we call dgem and then do some

00:09:01,120 --> 00:09:05,200
computation with the results of dgem

00:09:03,360 --> 00:09:07,040
and even on the cpu the majority of the

00:09:05,200 --> 00:09:09,360
time the spent and calls to this

00:09:07,040 --> 00:09:11,200
matrix multiply routine or gem which is

00:09:09,360 --> 00:09:14,720
a standard in

00:09:11,200 --> 00:09:14,720
math libraries like loss

00:09:15,600 --> 00:09:19,440
so to port to gpu's strategy was to

00:09:17,600 --> 00:09:22,080
merge sections of array so that

00:09:19,440 --> 00:09:23,519
the inputs to the gem call are larger

00:09:22,080 --> 00:09:25,600
this is important because it results in

00:09:23,519 --> 00:09:26,160
a higher arithmetic intensity per degen

00:09:25,600 --> 00:09:28,080
call

00:09:26,160 --> 00:09:31,040
and less overhead from kernel launches

00:09:28,080 --> 00:09:33,360
from the cpu to the gpu

00:09:31,040 --> 00:09:34,160
um this results in the code shown on the

00:09:33,360 --> 00:09:36,560
right

00:09:34,160 --> 00:09:38,399
um the changes to sort of note first are

00:09:36,560 --> 00:09:40,080
in red

00:09:38,399 --> 00:09:41,920
we basically increase the dimensions of

00:09:40,080 --> 00:09:44,560
the array sent

00:09:41,920 --> 00:09:45,760
to djem as you can see there's no longer

00:09:44,560 --> 00:09:49,279
this outer loop of

00:09:45,760 --> 00:09:51,680
iact over the call to ram b2

00:09:49,279 --> 00:09:53,120
eij that's sort of been partially

00:09:51,680 --> 00:09:57,600
incorporated into the call

00:09:53,120 --> 00:10:00,000
to djim so as with the hartford code

00:09:57,600 --> 00:10:03,120
this is sort of a generic cpu to gpu

00:10:00,000 --> 00:10:05,040
code optimization trying to put have

00:10:03,120 --> 00:10:07,200
less kernel launches and more work to

00:10:05,040 --> 00:10:09,360
math library calls

00:10:07,200 --> 00:10:11,920
and we just used openmp to express the

00:10:09,360 --> 00:10:13,920
data transfer and the parallelism

00:10:11,920 --> 00:10:15,680
this code was implemented and evaluated

00:10:13,920 --> 00:10:18,399
in a mini app and in a development

00:10:15,680 --> 00:10:21,440
branch of games

00:10:18,399 --> 00:10:23,440
so let's look at some of the results

00:10:21,440 --> 00:10:25,360
which were evaluated in the mini app on

00:10:23,440 --> 00:10:27,760
summit so these are

00:10:25,360 --> 00:10:28,640
from this paper here you can grab it and

00:10:27,760 --> 00:10:31,760
look at it

00:10:28,640 --> 00:10:35,279
later so basically for large enough

00:10:31,760 --> 00:10:37,920
inputs the openmp gpu version running on

00:10:35,279 --> 00:10:39,839
a single v100 gpu was six to seven x

00:10:37,920 --> 00:10:43,120
faster than the cpu version

00:10:39,839 --> 00:10:44,640
running on two ibm power nine cpus and i

00:10:43,120 --> 00:10:46,480
just picked one out that's highlighted

00:10:44,640 --> 00:10:48,720
in green that's what's um this is

00:10:46,480 --> 00:10:50,480
trying to show um the important thing is

00:10:48,720 --> 00:10:52,160
that this is near the best expected

00:10:50,480 --> 00:10:52,720
speed up for floating point dominated

00:10:52,160 --> 00:10:54,480
code

00:10:52,720 --> 00:10:56,720
since the theoretical ratio of the peak

00:10:54,480 --> 00:11:00,480
floating point performance of one v 100

00:10:56,720 --> 00:11:02,959
to 2 power 9 cpus is approximately 7x

00:11:00,480 --> 00:11:04,720
so thus the the gpu openmp version shows

00:11:02,959 --> 00:11:06,480
a speed up over the cpu version which is

00:11:04,720 --> 00:11:07,600
near the theoretical floating point

00:11:06,480 --> 00:11:10,399
performance ratio

00:11:07,600 --> 00:11:12,240
which is really nice um i mean the one

00:11:10,399 --> 00:11:13,600
thing to note from this table

00:11:12,240 --> 00:11:15,760
that you can see is that we tried

00:11:13,600 --> 00:11:17,440
multiple different gpu math libraries

00:11:15,760 --> 00:11:19,600
kubellas kubos xte

00:11:17,440 --> 00:11:21,519
nd boss which have different ways of

00:11:19,600 --> 00:11:23,279
invoking them from openmp so we ended up

00:11:21,519 --> 00:11:23,680
with a bunch of if-defs in our code to

00:11:23,279 --> 00:11:27,040
call

00:11:23,680 --> 00:11:27,040
different math libraries

00:11:27,120 --> 00:11:30,720
additionally i want to note that this

00:11:28,720 --> 00:11:31,760
code can take advantage of multiple gpus

00:11:30,720 --> 00:11:34,720
on summit

00:11:31,760 --> 00:11:36,880
by using mpi to offload the gem calls to

00:11:34,720 --> 00:11:39,040
the six gpus on the summit node

00:11:36,880 --> 00:11:41,120
and given a large enough input again the

00:11:39,040 --> 00:11:43,120
code achieved in approximately 40x speed

00:11:41,120 --> 00:11:44,399
up compared to two power nine cpus which

00:11:43,120 --> 00:11:46,160
is again near the

00:11:44,399 --> 00:11:48,240
theoretical ratio of peak floating point

00:11:46,160 --> 00:11:51,360
performance of six v100s

00:11:48,240 --> 00:11:54,399
to two power nine cpus

00:11:51,360 --> 00:11:55,839
okay so the biggest issue we faced here

00:11:54,399 --> 00:11:57,519
was the multiple different ways of

00:11:55,839 --> 00:11:58,320
calling the vendor math libraries for

00:11:57,519 --> 00:11:59,920
openmp

00:11:58,320 --> 00:12:02,720
so even within the same vendor so

00:11:59,920 --> 00:12:04,399
kubellas coopers xt or mvbloss

00:12:02,720 --> 00:12:06,160
and also having that code it's not

00:12:04,399 --> 00:12:07,440
necessarily portable to different vendor

00:12:06,160 --> 00:12:09,040
gpus

00:12:07,440 --> 00:12:11,600
since it's using an nvidia way of

00:12:09,040 --> 00:12:13,519
calling the standard loss function

00:12:11,600 --> 00:12:17,200
but this this did work really well and

00:12:13,519 --> 00:12:17,200
we achieved nice speedups

00:12:17,519 --> 00:12:22,000
so finally i wanted to just summarize

00:12:19,200 --> 00:12:23,519
some some takeaways

00:12:22,000 --> 00:12:24,720
so the first thing is like basically

00:12:23,519 --> 00:12:25,839
even though this is a talk about how we

00:12:24,720 --> 00:12:27,200
use openmp

00:12:25,839 --> 00:12:28,959
we really just started thinking about

00:12:27,200 --> 00:12:30,880
how to take our cpu code and get it to

00:12:28,959 --> 00:12:32,880
paralyze and run well as gpus

00:12:30,880 --> 00:12:35,440
and then we just chose openmp and use it

00:12:32,880 --> 00:12:38,160
as our tool of choice to do this

00:12:35,440 --> 00:12:40,320
and after we did that um most of this

00:12:38,160 --> 00:12:43,839
work was done with the ob the openmp

00:12:40,320 --> 00:12:44,959
offload compiler on summit so ibm 16.1

00:12:43,839 --> 00:12:46,639
compiler

00:12:44,959 --> 00:12:48,399
we did have a few issues like the

00:12:46,639 --> 00:12:49,279
unexpected poor performance for some

00:12:48,399 --> 00:12:51,120
features

00:12:49,279 --> 00:12:53,440
and figuring out how we wanted to handle

00:12:51,120 --> 00:12:55,360
interactions with vendor math libraries

00:12:53,440 --> 00:12:56,959
but overall we achieved we were able to

00:12:55,360 --> 00:12:59,200
achieve really nice speed ups

00:12:56,959 --> 00:13:00,880
running on the cpu over the gpu and we

00:12:59,200 --> 00:13:04,079
had the needed functionality and

00:13:00,880 --> 00:13:07,279
and the openmp to port our code

00:13:04,079 --> 00:13:10,399
and the plug um for we're

00:13:07,279 --> 00:13:12,320
excited about in openmp 5.1 uh we expect

00:13:10,399 --> 00:13:14,480
and hope that the dispatch construct

00:13:12,320 --> 00:13:16,959
and openmp51 will help with this issue

00:13:14,480 --> 00:13:20,160
of vendor library so we hope

00:13:16,959 --> 00:13:21,120
um it will help make vendor library

00:13:20,160 --> 00:13:23,200
calls that interact

00:13:21,120 --> 00:13:25,360
right with openmp to be more portable

00:13:23,200 --> 00:13:27,839
across vendors

00:13:25,360 --> 00:13:32,880
and that's all i have thank you very

00:13:27,839 --> 00:13:32,880

YouTube URL: https://www.youtube.com/watch?v=9_nYCIjgrn4


