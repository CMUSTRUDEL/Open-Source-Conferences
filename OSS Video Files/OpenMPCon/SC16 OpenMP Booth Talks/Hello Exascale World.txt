Title: Hello Exascale World
Publication date: 2016-11-21
Playlist: SC16 OpenMP Booth Talks
Description: 
	Greg Rodgers, AMD
Captions: 
	00:00:00,000 --> 00:00:06,930
okay here we go okay so this talk is

00:00:03,030 --> 00:00:11,460
about hello exascale world with mpi and

00:00:06,930 --> 00:00:15,830
OpenMP and if let me just ask if you

00:00:11,460 --> 00:00:18,570
guys used MPI before right so you know

00:00:15,830 --> 00:00:20,400
that you have a rank and you have a

00:00:18,570 --> 00:00:22,080
number of ranks and this is the amount

00:00:20,400 --> 00:00:24,480
of parallelism that you have an MPI if

00:00:22,080 --> 00:00:27,660
you're just doing MPI and this has been

00:00:24,480 --> 00:00:30,179
around since the 1990s all right and so

00:00:27,660 --> 00:00:32,610
all of a sudden we had many more cores

00:00:30,179 --> 00:00:35,520
and so we ended up we added openmp to

00:00:32,610 --> 00:00:38,969
that so the you ended up having a number

00:00:35,520 --> 00:00:41,010
of threads in OpenMP times the number of

00:00:38,969 --> 00:00:43,200
ranks and so that became the amount of

00:00:41,010 --> 00:00:45,090
parallelism you had and if you remember

00:00:43,200 --> 00:00:46,350
when for mpi just came out you guys are

00:00:45,090 --> 00:00:48,870
very young so you probably don't

00:00:46,350 --> 00:00:50,789
remember all right a lot of people said

00:00:48,870 --> 00:00:52,590
it was too complex to use right they

00:00:50,789 --> 00:00:54,719
required the domain programmer to

00:00:52,590 --> 00:00:57,660
decompose his application across the

00:00:54,719 --> 00:01:00,930
ranks and then when openmp came along

00:00:57,660 --> 00:01:02,250
same thing too hard to use okay but to

00:01:00,930 --> 00:01:04,589
really effectively do the parallelism

00:01:02,250 --> 00:01:06,689
the domain programmer had it decomposes

00:01:04,589 --> 00:01:09,360
applications based on the number of

00:01:06,689 --> 00:01:12,780
ranks and the number of and the number

00:01:09,360 --> 00:01:17,610
of threads ok so let's just talk about

00:01:12,780 --> 00:01:20,610
that so at exascale though there is

00:01:17,610 --> 00:01:23,850
going to be many more levels of

00:01:20,610 --> 00:01:26,159
parallelism right and if to get the 10

00:01:23,850 --> 00:01:28,049
billion ways parallelism you're going to

00:01:26,159 --> 00:01:30,119
need about a hundred thousand nodes and

00:01:28,049 --> 00:01:32,670
on each node you're going to have about

00:01:30,119 --> 00:01:33,720
a hundred thousand way parallelism so

00:01:32,670 --> 00:01:35,700
how are you going to get that are you

00:01:33,720 --> 00:01:37,079
can have a hundred thousand threads not

00:01:35,700 --> 00:01:39,000
likely and you're going to be you're

00:01:37,079 --> 00:01:41,700
going to use some of the new principles

00:01:39,000 --> 00:01:45,180
in some of the new capabilities of

00:01:41,700 --> 00:01:47,040
OpenMP so you have it's very also very

00:01:45,180 --> 00:01:49,770
possible that the organization of your

00:01:47,040 --> 00:01:51,509
ranks in a cluster will be organized by

00:01:49,770 --> 00:01:53,549
a super node okay so you have a number

00:01:51,509 --> 00:01:55,770
of groups and you'll have a number of

00:01:53,549 --> 00:01:57,479
nodes within that group so so there's

00:01:55,770 --> 00:02:00,689
already one level of hierarchy that

00:01:57,479 --> 00:02:02,670
you'll have with it with mpi then within

00:02:00,689 --> 00:02:05,189
the node itself you'll have a number of

00:02:02,670 --> 00:02:07,369
devices okay that's actually also new

00:02:05,189 --> 00:02:10,200
and openmp where you actually have

00:02:07,369 --> 00:02:12,030
mobility to support multiple offloading

00:02:10,200 --> 00:02:13,560
devices well let's take a look inside a

00:02:12,030 --> 00:02:15,900
device okay a device

00:02:13,560 --> 00:02:18,330
could be a Numa node or a GPU there are

00:02:15,900 --> 00:02:20,220
multiple cores or inside there or

00:02:18,330 --> 00:02:23,790
multiple streaming processors if it's a

00:02:20,220 --> 00:02:26,370
GPU so you have this other thing here

00:02:23,790 --> 00:02:27,840
the the in the in the case of openmp we

00:02:26,370 --> 00:02:29,640
look at that it's kind of like a team

00:02:27,840 --> 00:02:31,500
it's not a direct correlation but you

00:02:29,640 --> 00:02:33,269
have a team in OpenMP that would

00:02:31,500 --> 00:02:35,370
correlate to the to the streaming

00:02:33,269 --> 00:02:38,099
processor and of course the actual core

00:02:35,370 --> 00:02:40,440
itself gets you the total amount of

00:02:38,099 --> 00:02:42,480
parallelism so a texas gal this is sort

00:02:40,440 --> 00:02:45,330
of an order of you know one this is one

00:02:42,480 --> 00:02:47,250
potential way to get there it's and

00:02:45,330 --> 00:02:48,510
these are only orders of magnitude all

00:02:47,250 --> 00:02:50,580
right I'm not saying this is the way

00:02:48,510 --> 00:02:55,140
that anybody is going to build one but

00:02:50,580 --> 00:02:56,640
to get to 10 billion ways you need of 10

00:02:55,140 --> 00:02:58,440
to the 5th cluster nodes that's about a

00:02:56,640 --> 00:03:00,599
hundred thousand nodes and you need 10

00:02:58,440 --> 00:03:03,239
to the 5th IPC instructions per cycle on

00:03:00,599 --> 00:03:05,700
the note to get you to 10 billion ways

00:03:03,239 --> 00:03:07,620
right so the question is what's that

00:03:05,700 --> 00:03:12,269
going to look like from the programmers

00:03:07,620 --> 00:03:16,019
view in in OpenMP so the terminology

00:03:12,269 --> 00:03:18,239
here is going to be in terms of MPI MPI

00:03:16,019 --> 00:03:21,209
support the new MPI three supports these

00:03:18,239 --> 00:03:22,799
new club communication I forget what

00:03:21,209 --> 00:03:25,980
they're called communication primitives

00:03:22,799 --> 00:03:28,140
and we can put things into groups and we

00:03:25,980 --> 00:03:30,299
can actually have communication among

00:03:28,140 --> 00:03:32,340
just the ranks with inner groups right

00:03:30,299 --> 00:03:34,140
so you have groups you got ranks and you

00:03:32,340 --> 00:03:38,220
got devices you'll have teams and

00:03:34,140 --> 00:03:40,260
threads right so let's just back up a

00:03:38,220 --> 00:03:43,230
bit and talk about what we mentioned

00:03:40,260 --> 00:03:45,780
earlier sort of look at the generations

00:03:43,230 --> 00:03:48,389
of decomposition as we went through time

00:03:45,780 --> 00:03:49,920
so initially we had we're getting cut

00:03:48,389 --> 00:03:51,989
off at the end there sorry but can you

00:03:49,920 --> 00:03:56,040
see that MPI at the end all right all

00:03:51,989 --> 00:03:59,340
right so you know this was a 19 vintage

00:03:56,040 --> 00:04:01,620
1980 1990s and I'm not sure whether with

00:03:59,340 --> 00:04:05,790
scale that was at and we had em p i+

00:04:01,620 --> 00:04:08,010
open and P so we had X of number ranks

00:04:05,790 --> 00:04:11,840
you know we rank X of number of ranks we

00:04:08,010 --> 00:04:14,130
got thread y of number of cpu cores

00:04:11,840 --> 00:04:16,519
usually at today i think with petascale

00:04:14,130 --> 00:04:19,019
systems right where you actually now add

00:04:16,519 --> 00:04:22,440
OpenMP acceleration on a lot of

00:04:19,019 --> 00:04:24,720
petascale type machines and so here we

00:04:22,440 --> 00:04:26,400
introduced this notion of a thread

00:04:24,720 --> 00:04:29,710
within a team size now

00:04:26,400 --> 00:04:32,170
openmp is using the word thread within a

00:04:29,710 --> 00:04:35,320
team all right unfortunately it's

00:04:32,170 --> 00:04:37,570
confusing with the thread on a cpu as

00:04:35,320 --> 00:04:39,670
well in fact when you go into a no MP

00:04:37,570 --> 00:04:41,170
target the number of threads actually

00:04:39,670 --> 00:04:43,270
changes to the number of threads on the

00:04:41,170 --> 00:04:45,640
GPU and so it's a different number if

00:04:43,270 --> 00:04:47,740
you did oh NP get number thread so in a

00:04:45,640 --> 00:04:50,650
way I don't like to reuse the term so I

00:04:47,740 --> 00:04:52,840
want to use the term in my program item

00:04:50,650 --> 00:04:55,060
instead of thread so that I'm really

00:04:52,840 --> 00:04:59,200
clear so when I say an item i really

00:04:55,060 --> 00:05:02,980
mean a GPU thread okay so that's why I

00:04:59,200 --> 00:05:04,420
put the word item there now the next

00:05:02,980 --> 00:05:05,680
thing I sort of the pre exit scale

00:05:04,420 --> 00:05:09,220
systems that we're getting into today

00:05:05,680 --> 00:05:12,580
like the Oakridge supercomputer you're

00:05:09,220 --> 00:05:14,050
seeing many more devices on the node you

00:05:12,580 --> 00:05:17,170
don't just see one accelerator you'll

00:05:14,050 --> 00:05:19,270
see half a dozen devices so in this case

00:05:17,170 --> 00:05:22,150
it's that the device d of the number of

00:05:19,270 --> 00:05:25,150
devices so this is sort of four levels

00:05:22,150 --> 00:05:27,520
of parallelism and I really believe that

00:05:25,150 --> 00:05:28,930
as we get towards real exascale we're

00:05:27,520 --> 00:05:31,300
going to get to the fifth level which is

00:05:28,930 --> 00:05:34,270
the grouping of the nodes okay because

00:05:31,300 --> 00:05:35,740
the hierarchy across the across the

00:05:34,270 --> 00:05:38,080
cluster is really going to be important

00:05:35,740 --> 00:05:41,530
for and and it kind of comes back to

00:05:38,080 --> 00:05:44,200
saying you know the people look at this

00:05:41,530 --> 00:05:45,820
and say wow just like they did in the

00:05:44,200 --> 00:05:48,490
past no one's ever going to do this

00:05:45,820 --> 00:05:49,900
right it's too complex well whether it's

00:05:48,490 --> 00:05:51,370
done implicitly by some other

00:05:49,900 --> 00:05:54,310
programming model or it's done

00:05:51,370 --> 00:05:57,340
explicitly by the programmer using

00:05:54,310 --> 00:05:58,540
OpenMP and MPI you're going to have to

00:05:57,340 --> 00:06:00,450
deal with that level of parallelism

00:05:58,540 --> 00:06:03,880
going on so it's better if you

00:06:00,450 --> 00:06:05,740
understand this and think up from today

00:06:03,880 --> 00:06:08,050
because the current OpenMP today

00:06:05,740 --> 00:06:10,570
supports this capability and the MPI we

00:06:08,050 --> 00:06:12,330
can do this today we can start thinking

00:06:10,570 --> 00:06:14,590
about how we would make our codes

00:06:12,330 --> 00:06:17,800
parallelizable and we can even run this

00:06:14,590 --> 00:06:19,360
program on today's system so okay here's

00:06:17,800 --> 00:06:21,520
an eye chart right so this is the source

00:06:19,360 --> 00:06:24,610
code for this hello world that I have

00:06:21,520 --> 00:06:27,460
it's not all of it it's just the the top

00:06:24,610 --> 00:06:28,750
load the top-level file because there's

00:06:27,460 --> 00:06:32,220
an initialization here what r

00:06:28,750 --> 00:06:35,590
initializes MPI it initializes OpenMP

00:06:32,220 --> 00:06:37,840
and even initialize I actually believe a

00:06:35,590 --> 00:06:40,040
texas scale we're all going to need a

00:06:37,840 --> 00:06:41,450
power api we're going to need to

00:06:40,040 --> 00:06:45,920
query power and I'll show you in a

00:06:41,450 --> 00:06:47,450
minute why we believe the that you'll be

00:06:45,920 --> 00:06:50,420
running under a power budget and how the

00:06:47,450 --> 00:06:51,800
hell that will work right so anyhow I'm

00:06:50,420 --> 00:06:54,350
just want to calculate the total amount

00:06:51,800 --> 00:06:55,610
of parallelism here it's the the total

00:06:54,350 --> 00:06:57,650
number of groups times the number of

00:06:55,610 --> 00:06:59,300
group ranks and this stuff the

00:06:57,650 --> 00:07:01,250
initializer figured all this stuff out

00:06:59,300 --> 00:07:03,590
we put a nice little data structure

00:07:01,250 --> 00:07:04,910
called htw that has all these values and

00:07:03,590 --> 00:07:06,950
it even has the values for what the

00:07:04,910 --> 00:07:10,730
current rank is and the current group

00:07:06,950 --> 00:07:12,710
number is so then we get to the point

00:07:10,730 --> 00:07:15,440
now this is a code that's running on a

00:07:12,710 --> 00:07:16,790
node right that's as an MPI environment

00:07:15,440 --> 00:07:18,980
and we note we already know what I rank

00:07:16,790 --> 00:07:21,760
is an order group number is now we're

00:07:18,980 --> 00:07:24,260
going to for each device so we have a

00:07:21,760 --> 00:07:26,660
for each of the devices that we have we

00:07:24,260 --> 00:07:30,020
actually have to note the device number

00:07:26,660 --> 00:07:32,060
using the device clause in openmp so

00:07:30,020 --> 00:07:33,380
that's what this is and then you don't

00:07:32,060 --> 00:07:35,210
really need these these are kind of

00:07:33,380 --> 00:07:37,340
defaulted but I put them here to note

00:07:35,210 --> 00:07:40,580
that you can control the number of teams

00:07:37,340 --> 00:07:42,470
that a code region could use and the

00:07:40,580 --> 00:07:43,910
number and the thread limit which is the

00:07:42,470 --> 00:07:45,320
number of threads that it would use but

00:07:43,910 --> 00:07:47,000
really these are going to be something

00:07:45,320 --> 00:07:49,250
like the wavefront size on the

00:07:47,000 --> 00:07:52,190
particular GPU and if it was an AMD it

00:07:49,250 --> 00:07:55,190
would be 64 if it was nvidia would be 32

00:07:52,190 --> 00:07:57,260
right and then we will distribute that

00:07:55,190 --> 00:07:59,180
says this next for loop is going to be

00:07:57,260 --> 00:08:01,100
distributed across the different teams

00:07:59,180 --> 00:08:03,140
all right so for this is saying for all

00:08:01,100 --> 00:08:07,370
teams in the device and we say for all

00:08:03,140 --> 00:08:09,560
threads in a team and then that's what

00:08:07,370 --> 00:08:11,390
this last four loop is and then I don't

00:08:09,560 --> 00:08:15,050
want to have 10 billion lines of output

00:08:11,390 --> 00:08:17,030
okay so so I have an if statement that

00:08:15,050 --> 00:08:18,350
says of course if you're this if your

00:08:17,030 --> 00:08:19,700
rank is this number and your device

00:08:18,350 --> 00:08:22,940
numbers this and your team numbers this

00:08:19,700 --> 00:08:25,040
is 0 and your item 0 then then then do

00:08:22,940 --> 00:08:26,480
the print hello world right because the

00:08:25,040 --> 00:08:30,590
hello world that exascale is going to

00:08:26,480 --> 00:08:31,850
say i am group x of how many and and so

00:08:30,590 --> 00:08:34,310
this is what it would look like all

00:08:31,850 --> 00:08:35,390
right and oh so then almost will

00:08:34,310 --> 00:08:38,810
calculating the amount of parallelism

00:08:35,390 --> 00:08:41,479
you have on the cpu side of stuff then

00:08:38,810 --> 00:08:43,400
after you've printed we do a power query

00:08:41,479 --> 00:08:46,640
because we'd like to know how much power

00:08:43,400 --> 00:08:49,280
hello world took right and then we we

00:08:46,640 --> 00:08:50,550
stop in the stop is going to print a

00:08:49,280 --> 00:08:54,480
little epilogue

00:08:50,550 --> 00:08:57,180
a that I've created so here's the whole

00:08:54,480 --> 00:08:59,970
world xq world so it's you'll see group

00:08:57,180 --> 00:09:03,240
0 of 144 groups no I don't have a

00:08:59,970 --> 00:09:04,950
machine like this okay so i can run it

00:09:03,240 --> 00:09:06,690
wouldn't be that interesting if i ran my

00:09:04,950 --> 00:09:08,910
hello world on the clusters that i have

00:09:06,690 --> 00:09:11,899
access to so i actually have the ability

00:09:08,910 --> 00:09:14,880
to put a plug to plug in to this code a

00:09:11,899 --> 00:09:17,370
simulation of a system so that we can

00:09:14,880 --> 00:09:18,930
get we can show what an X scale system

00:09:17,370 --> 00:09:20,760
would look like right so this isn't I

00:09:18,930 --> 00:09:22,529
did not run on machine this large this

00:09:20,760 --> 00:09:24,390
is running simulated values but this

00:09:22,529 --> 00:09:26,339
code will run on existing machines are

00:09:24,390 --> 00:09:28,740
an existing subset of a machine and you

00:09:26,339 --> 00:09:29,970
can get this kind of this kind of this

00:09:28,740 --> 00:09:31,800
kind of list so you get a hello world

00:09:29,970 --> 00:09:33,660
and it's kind of nice because what do

00:09:31,800 --> 00:09:35,430
you do when you get a hello world now

00:09:33,660 --> 00:09:37,170
you have the initialization of all the

00:09:35,430 --> 00:09:40,110
subsystems you can just worry about your

00:09:37,170 --> 00:09:42,660
code so steal this template if you want

00:09:40,110 --> 00:09:46,800
just take this and say that's that's how

00:09:42,660 --> 00:09:49,050
it would work so at so so take a look

00:09:46,800 --> 00:09:52,800
here this this total amount of CPU

00:09:49,050 --> 00:09:56,149
parallelism is about 7 million okay 159

00:09:52,800 --> 00:10:01,050
million in the amount of GPU parallelism

00:09:56,149 --> 00:10:02,399
and anybody can I I was hoping that

00:10:01,050 --> 00:10:05,279
enough people here to guess what machine

00:10:02,399 --> 00:10:08,070
this is it's a simulation of a machine

00:10:05,279 --> 00:10:10,980
let not yet active hurry as we go

00:10:08,070 --> 00:10:13,440
through it you might but guess which one

00:10:10,980 --> 00:10:15,959
it is so so that was the user printed

00:10:13,440 --> 00:10:17,880
hello world right now I've also

00:10:15,959 --> 00:10:20,399
developed an epilogue so that at the end

00:10:17,880 --> 00:10:22,740
of a run we just print out all this more

00:10:20,399 --> 00:10:25,920
interesting information the number of no

00:10:22,740 --> 00:10:28,200
groups there was 144 the nodes per group

00:10:25,920 --> 00:10:30,690
of 24 so the total number of compute

00:10:28,200 --> 00:10:33,709
nodes in this cluster was 3,000 for 256

00:10:30,690 --> 00:10:37,199
on the number of ranks per group is 24

00:10:33,709 --> 00:10:39,750
we did this is actually unconventional

00:10:37,199 --> 00:10:41,730
we in a lot of people today would say

00:10:39,750 --> 00:10:42,870
with some of the smaller nodes 1-ranked

00:10:41,730 --> 00:10:45,480
per node is enough but what we're seeing

00:10:42,870 --> 00:10:47,370
with GPU codes that we would really want

00:10:45,480 --> 00:10:48,930
to have multiple ranks per node but just

00:10:47,370 --> 00:10:51,570
for demonstration purposes we did one

00:10:48,930 --> 00:10:53,010
rank / note right and then the total

00:10:51,570 --> 00:10:55,680
number of MPI ranks equals the number of

00:10:53,010 --> 00:10:57,570
nodes and then the next level 3 of it is

00:10:55,680 --> 00:10:59,520
the number devices / rank or six a

00:10:57,570 --> 00:11:02,720
little bit more of a hint if anybody can

00:10:59,520 --> 00:11:06,439
guess which node which machine this is

00:11:02,720 --> 00:11:07,879
alright but visit it is a specific

00:11:06,439 --> 00:11:10,550
machine that has not been brought up yet

00:11:07,879 --> 00:11:15,079
so then the teat number of teams per

00:11:10,550 --> 00:11:18,430
device 60 and the team size at 32 so it

00:11:15,079 --> 00:11:24,980
might tell you something as well right

00:11:18,430 --> 00:11:27,410
it probably is a not an AMD of GPU all

00:11:24,980 --> 00:11:29,870
right and the IPC per thread so this is

00:11:27,410 --> 00:11:32,240
the vector vectorization capability per

00:11:29,870 --> 00:11:34,160
thread how much how many instructions

00:11:32,240 --> 00:11:36,139
per cycle that you could run and that

00:11:34,160 --> 00:11:42,920
the product of these levels times the

00:11:36,139 --> 00:11:43,910
IPC is 159 million all right and you can

00:11:42,920 --> 00:11:48,319
take this sort of the into power

00:11:43,910 --> 00:11:50,120
statistics as well so everyone sort of

00:11:48,319 --> 00:11:51,860
assumes that the the frequency of the

00:11:50,120 --> 00:11:53,569
machines is always running at peak and

00:11:51,860 --> 00:11:54,949
that's actually a good assumption for

00:11:53,569 --> 00:11:58,250
something as simple as hello world but

00:11:54,949 --> 00:12:00,680
there's no computation so in in in hello

00:11:58,250 --> 00:12:02,810
world you can see that the the max

00:12:00,680 --> 00:12:04,279
frequency was one gigahertz and the

00:12:02,810 --> 00:12:06,470
average frequency was when gigahertz oh

00:12:04,279 --> 00:12:08,269
nothing every was throttled right it ran

00:12:06,470 --> 00:12:12,050
at the peak frequency nothing was

00:12:08,269 --> 00:12:14,569
throttled and so but there was also no

00:12:12,050 --> 00:12:16,370
computation going on so the sampled

00:12:14,569 --> 00:12:18,860
power said we're at two megawatts of

00:12:16,370 --> 00:12:21,500
power but the power budget on the

00:12:18,860 --> 00:12:23,209
machine this is a constant was six

00:12:21,500 --> 00:12:25,550
megawatts all right as a matter of fact

00:12:23,209 --> 00:12:27,709
it's actually higher than six megawatts

00:12:25,550 --> 00:12:30,019
because that's the power budget just for

00:12:27,709 --> 00:12:32,180
the computational part right but for the

00:12:30,019 --> 00:12:34,370
whole machine itself you're in the whole

00:12:32,180 --> 00:12:36,529
machine it's got a budget of about 10

00:12:34,370 --> 00:12:39,920
megawatts it's another hint as to what

00:12:36,529 --> 00:12:41,689
machine this is okay so so you so you

00:12:39,920 --> 00:12:44,660
have 10 megawatts and we're only use

00:12:41,689 --> 00:12:46,040
thirty percent of this of this of the

00:12:44,660 --> 00:12:48,079
total budget this is to is thirty

00:12:46,040 --> 00:12:51,350
percent of the six so there wasn't a lot

00:12:48,079 --> 00:12:52,970
of computation as a result we were as

00:12:51,350 --> 00:12:54,620
you get closer to one hundred percent of

00:12:52,970 --> 00:12:55,879
your budget the system is going to

00:12:54,620 --> 00:12:58,399
actually throttle back and lower the

00:12:55,879 --> 00:13:00,170
frequency to stay within the so that you

00:12:58,399 --> 00:13:02,029
stay within the budget power for your

00:13:00,170 --> 00:13:07,490
entire system right so there's software

00:13:02,029 --> 00:13:09,579
control frequency and and so you can see

00:13:07,490 --> 00:13:13,209
there's the device cluster petaflop says

00:13:09,579 --> 00:13:14,810
159 the total cluster with the CPU is

00:13:13,209 --> 00:13:19,310
180 at

00:13:14,810 --> 00:13:22,430
alright so this numbers here of the

00:13:19,310 --> 00:13:26,210
total the peak frequency for the machine

00:13:22,430 --> 00:13:29,029
is going to lower when we start

00:13:26,210 --> 00:13:32,990
throttling back to meet the state within

00:13:29,029 --> 00:13:35,150
the power constraint right so what dumb

00:13:32,990 --> 00:13:36,860
what's kind of interesting is to look at

00:13:35,150 --> 00:13:40,940
the memory statistics here and in the

00:13:36,860 --> 00:13:43,490
ratio of the the computer memory

00:13:40,940 --> 00:13:46,400
bandwidth to compute so if you start

00:13:43,490 --> 00:13:48,110
lowering the compute capacity you get

00:13:46,400 --> 00:13:50,060
that ratio which is really important

00:13:48,110 --> 00:13:52,820
starts to get improve a lot better you

00:13:50,060 --> 00:13:55,279
really have much better ratio of memory

00:13:52,820 --> 00:13:57,670
bandwidth to computing all right how am

00:13:55,279 --> 00:13:57,670
i doing for time

00:14:01,250 --> 00:14:08,990
okay okay all right so so this is a

00:14:07,370 --> 00:14:12,620
simulated from a hypothetical cluster

00:14:08,990 --> 00:14:14,210
and so the next shows us some some

00:14:12,620 --> 00:14:16,430
memory statistics this bite the flop

00:14:14,210 --> 00:14:17,870
ratio here so if there was a situation

00:14:16,430 --> 00:14:20,360
where your code was doing real

00:14:17,870 --> 00:14:22,910
computation and it caused some kind of

00:14:20,360 --> 00:14:24,980
throttling right if your code was memory

00:14:22,910 --> 00:14:26,870
bound it really wouldn't perform worse

00:14:24,980 --> 00:14:28,280
because it got lower frequency because

00:14:26,870 --> 00:14:29,840
it was waiting for waiting for memory

00:14:28,280 --> 00:14:32,570
and this ratio would but actually

00:14:29,840 --> 00:14:34,880
improve when the average frequency went

00:14:32,570 --> 00:14:36,950
down right because the total ratio the

00:14:34,880 --> 00:14:39,260
total rate computation would go down so

00:14:36,950 --> 00:14:41,570
that's kind of interesting aspect when

00:14:39,260 --> 00:14:43,940
we start dealing with power budgets at

00:14:41,570 --> 00:14:46,610
exascale and I that's one of the reasons

00:14:43,940 --> 00:14:49,580
why I believe in addition to MPI in

00:14:46,610 --> 00:14:52,130
addition to openmp we need an API to

00:14:49,580 --> 00:14:54,920
capture the power numbers okay so that

00:14:52,130 --> 00:14:58,010
we can show people that that application

00:14:54,920 --> 00:15:02,240
users become aware that some potential

00:14:58,010 --> 00:15:04,130
throttling occurred to to manage power

00:15:02,240 --> 00:15:06,680
right because I believe that that will

00:15:04,130 --> 00:15:09,020
be the case these are very large

00:15:06,680 --> 00:15:12,680
machines with a with a fairly

00:15:09,020 --> 00:15:13,910
significant power budget okay so any

00:15:12,680 --> 00:15:16,400
questions about that so I'm just going

00:15:13,910 --> 00:15:18,320
to show this chart again because it kind

00:15:16,400 --> 00:15:20,870
of does a summary this is the last chart

00:15:18,320 --> 00:15:23,540
just to say that this is sort of a

00:15:20,870 --> 00:15:25,790
hardware view of an exascale system or

00:15:23,540 --> 00:15:28,070
actually honestly any system if you just

00:15:25,790 --> 00:15:30,980
took away the orders of magnitude of any

00:15:28,070 --> 00:15:33,080
system today and today with OpenMP we

00:15:30,980 --> 00:15:35,780
have this capability to break things

00:15:33,080 --> 00:15:37,460
into multiple devices multiple teams and

00:15:35,780 --> 00:15:40,520
of course the number of threads which

00:15:37,460 --> 00:15:43,240
has always been there okay so with that

00:15:40,520 --> 00:15:43,240
is there any questions

00:15:48,329 --> 00:15:55,199
the question was how does OMP distribute

00:15:51,449 --> 00:16:00,449
things across the note C do I have the

00:15:55,199 --> 00:16:05,220
me bring the code back here yeah all

00:16:00,449 --> 00:16:08,399
right so that the when a target region

00:16:05,220 --> 00:16:11,449
starts for GPU are for anything when a

00:16:08,399 --> 00:16:14,249
target region starts your the the

00:16:11,449 --> 00:16:17,579
environment assigns it a number of teams

00:16:14,249 --> 00:16:19,170
and a number of threads if it doesn't

00:16:17,579 --> 00:16:21,420
have the concept of multiple streaming

00:16:19,170 --> 00:16:22,589
processors the the environment the

00:16:21,420 --> 00:16:23,790
default environment will set at the

00:16:22,589 --> 00:16:26,970
number of teams to one but there will

00:16:23,790 --> 00:16:30,899
always be multiple teams okay so in the

00:16:26,970 --> 00:16:32,879
case of a AMD or an NVIDIA GPU there are

00:16:30,899 --> 00:16:35,699
multiple streaming processors so we

00:16:32,879 --> 00:16:38,069
often set the number of teams to be

00:16:35,699 --> 00:16:39,959
number of streaming processors and the

00:16:38,069 --> 00:16:42,149
number of threads per team to be the

00:16:39,959 --> 00:16:44,129
wavefront size so that we can have some

00:16:42,149 --> 00:16:46,019
any synchronicity right that's not

00:16:44,129 --> 00:16:47,459
always the pot it's not always what you

00:16:46,019 --> 00:16:49,679
do you can control the number of teams

00:16:47,459 --> 00:16:51,029
to come down to one team and then have

00:16:49,679 --> 00:16:53,669
many threads in that team you can

00:16:51,029 --> 00:16:57,059
actually say if you've got a particular

00:16:53,669 --> 00:16:59,399
GPU that has 4000 cores you can say you

00:16:57,059 --> 00:17:00,980
know four thousand threads on one team

00:16:59,399 --> 00:17:05,579
and there's some advantages to

00:17:00,980 --> 00:17:07,019
synchronization of items why if you've

00:17:05,579 --> 00:17:08,399
made it bigger there's also some

00:17:07,019 --> 00:17:11,279
advantage to having me and it will

00:17:08,399 --> 00:17:12,959
synchronize with between multiple teams

00:17:11,279 --> 00:17:15,480
so the number of teams kind of

00:17:12,959 --> 00:17:17,539
correlates to the number of groups of

00:17:15,480 --> 00:17:19,620
threads of things that you would want so

00:17:17,539 --> 00:17:21,449
from a hardware perspective that's

00:17:19,620 --> 00:17:22,889
what's going on of a software

00:17:21,449 --> 00:17:25,860
perspective you actually can control

00:17:22,889 --> 00:17:27,839
this fairly easily and it's usually you

00:17:25,860 --> 00:17:31,019
don't control energy you can but you

00:17:27,839 --> 00:17:33,210
don't reduce the number of threads but

00:17:31,019 --> 00:17:36,330
but who knows I think a lot of this this

00:17:33,210 --> 00:17:39,090
this this use model of acceleration with

00:17:36,330 --> 00:17:41,580
GPUs is relatively new but but there is

00:17:39,090 --> 00:17:43,350
this you know you do have the decomposed

00:17:41,580 --> 00:17:47,190
by teams and you know when we looked at

00:17:43,350 --> 00:17:50,250
this numbers of items here right if you

00:17:47,190 --> 00:17:51,960
don't have a group today right I would

00:17:50,250 --> 00:17:54,090
recommend doing this today and setting

00:17:51,960 --> 00:17:56,639
the number of groups to one right just

00:17:54,090 --> 00:17:58,080
one one group in a flat rank space and

00:17:56,639 --> 00:17:59,429
the same is true with devices set the

00:17:58,080 --> 00:18:01,710
number advice if you only have one

00:17:59,429 --> 00:18:01,950
device but you still have the capability

00:18:01,710 --> 00:18:04,230
to

00:18:01,950 --> 00:18:06,139
a to react to changes in the numbers

00:18:04,230 --> 00:18:08,519
advices and you can think about

00:18:06,139 --> 00:18:10,289
decomposing your codes along those

00:18:08,519 --> 00:18:11,880
boundaries because that's what's going

00:18:10,289 --> 00:18:13,710
to change you will have more devices in

00:18:11,880 --> 00:18:15,750
the future those devices are going to be

00:18:13,710 --> 00:18:18,870
bigger devices with more streaming

00:18:15,750 --> 00:18:22,470
processors I don't think that the number

00:18:18,870 --> 00:18:25,139
of threads per team will change very

00:18:22,470 --> 00:18:28,320
much all right interestingly enough

00:18:25,139 --> 00:18:31,590
it'll be 64 or 32 or 16 those sort of

00:18:28,320 --> 00:18:32,970
things so but yeah this is just trying

00:18:31,590 --> 00:18:36,299
to give you some thought into how to

00:18:32,970 --> 00:18:37,529
decompose your problems to meet the kind

00:18:36,299 --> 00:18:40,309
of parallelism that we're going to see

00:18:37,529 --> 00:18:47,909
and today's OpenMP so will support this

00:18:40,309 --> 00:18:50,159
so any other questions I'm just again

00:18:47,909 --> 00:18:51,779
this is it's not a secret it's only my

00:18:50,159 --> 00:18:53,850
guest from the true literature that I've

00:18:51,779 --> 00:18:56,279
read as what the the summit

00:18:53,850 --> 00:18:59,490
supercomputer at oakridge is an IBM

00:18:56,279 --> 00:19:03,929
machine with six and video cards on it

00:18:59,490 --> 00:19:05,760
and I was told some of the the power you

00:19:03,929 --> 00:19:07,500
saw some of the in the press how much

00:19:05,760 --> 00:19:09,029
power it was going to take so some of

00:19:07,500 --> 00:19:10,409
the other things were contrived in terms

00:19:09,029 --> 00:19:13,350
of how much of power it would have taken

00:19:10,409 --> 00:19:15,059
at idle right that was a guess all right

00:19:13,350 --> 00:19:17,039
so just to make it look good in the

00:19:15,059 --> 00:19:24,480
simulator I put those numbers but it was

00:19:17,039 --> 00:19:25,769
really uh Aurora yeah yeah yeah it's

00:19:24,480 --> 00:19:27,779
kind of fun with the simulated that I

00:19:25,769 --> 00:19:30,090
have it it doesn't really simulate a run

00:19:27,779 --> 00:19:32,519
it just allows me to demonstrate it and

00:19:30,090 --> 00:19:34,200
print out numbers I print out an

00:19:32,519 --> 00:19:38,309
epilogue so give me some comparison of

00:19:34,200 --> 00:19:39,510
some guesses of things so it's it's a

00:19:38,309 --> 00:19:40,740
lot of fun and it is fairly simple

00:19:39,510 --> 00:19:44,809
program there's not a lot of code here

00:19:40,740 --> 00:19:44,809
right so

00:19:51,970 --> 00:19:54,970
because

00:19:56,520 --> 00:20:02,230
yeah that's a good question and and you

00:19:59,590 --> 00:20:06,610
do need a some sort of power API to

00:20:02,230 --> 00:20:08,020
collect that and power api's are you

00:20:06,610 --> 00:20:11,320
know are always saying you know getting

00:20:08,020 --> 00:20:12,940
total energy is power over time so it

00:20:11,320 --> 00:20:15,070
takes enough sampling to kind of get the

00:20:12,940 --> 00:20:18,309
power right I only took one sample there

00:20:15,070 --> 00:20:21,240
of the power at that time but you know a

00:20:18,309 --> 00:20:24,580
lot of people talk about an energy

00:20:21,240 --> 00:20:26,140
minimization right and most people say

00:20:24,580 --> 00:20:28,000
oh the answer is just race to finish

00:20:26,140 --> 00:20:29,620
right go as fast as you can to get the

00:20:28,000 --> 00:20:31,900
finish and you'll save the total amount

00:20:29,620 --> 00:20:34,809
of energy but you know what the energy

00:20:31,900 --> 00:20:36,880
isn't what's budgeted the instantaneous

00:20:34,809 --> 00:20:39,429
power into the machine when you hit

00:20:36,880 --> 00:20:41,590
threshold that's what's budgeted are you

00:20:39,429 --> 00:20:43,059
so you have to kind of stay within a

00:20:41,590 --> 00:20:45,880
certain range and if you get close to it

00:20:43,059 --> 00:20:47,679
the hardware for multiple reasons

00:20:45,880 --> 00:20:50,290
including thermal conditions hardware

00:20:47,679 --> 00:20:52,090
will throttle itself back and change

00:20:50,290 --> 00:20:53,650
frequency and you see this today on your

00:20:52,090 --> 00:20:55,540
laptop with GPUs and stuff in your

00:20:53,650 --> 00:20:57,640
laptop you just look keep looking at the

00:20:55,540 --> 00:21:00,010
proxy PU info you'll see changes in

00:20:57,640 --> 00:21:01,600
frequency that happens to to deal with

00:21:00,010 --> 00:21:03,700
thermal effects all right this is

00:21:01,600 --> 00:21:06,010
frustrating I think for a lot of hpc

00:21:03,700 --> 00:21:08,770
programmers if they start seeing their

00:21:06,010 --> 00:21:10,990
the frequencies change on them what we

00:21:08,770 --> 00:21:12,520
are going to be in an era where the you

00:21:10,990 --> 00:21:14,320
know there were over provisioned with a

00:21:12,520 --> 00:21:16,809
computation for amount of power the

00:21:14,320 --> 00:21:19,390
power budget that's that's available for

00:21:16,809 --> 00:21:20,860
that machine so I think it is going to

00:21:19,390 --> 00:21:23,410
be an important part of the overall

00:21:20,860 --> 00:21:26,530
environment so that we understand that

00:21:23,410 --> 00:21:28,120
when you see that average frequency drop

00:21:26,530 --> 00:21:29,679
below the peak frequency that means

00:21:28,120 --> 00:21:32,530
there's been some amount of throttling

00:21:29,679 --> 00:21:36,549
that's happened and that's a I think

00:21:32,530 --> 00:21:41,740
that's really good to know right and at

00:21:36,549 --> 00:21:45,090
that any other questions well thank you

00:21:41,740 --> 00:21:45,090

YouTube URL: https://www.youtube.com/watch?v=kjCYIHuy9nk


