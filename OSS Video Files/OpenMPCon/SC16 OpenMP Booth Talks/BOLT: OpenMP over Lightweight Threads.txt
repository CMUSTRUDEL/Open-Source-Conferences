Title: BOLT: OpenMP over Lightweight Threads
Publication date: 2016-11-20
Playlist: SC16 OpenMP Booth Talks
Description: 
	Sangmin Seo, Argonne National Laboratory
SC16 OpenMP Booth Talk
Captions: 
	00:00:00,890 --> 00:00:06,779
so yeah let's get started thanks for

00:00:04,140 --> 00:00:09,300
coming to the first busto ghetto plain

00:00:06,779 --> 00:00:11,300
Peebles and I my name is Zhang myself

00:00:09,300 --> 00:00:13,559
from Argonne National Lab and I'm an

00:00:11,300 --> 00:00:17,520
assistant assistant camera scientist

00:00:13,559 --> 00:00:20,880
there and today today I'm going to

00:00:17,520 --> 00:00:21,720
introduce our open MP read the new open

00:00:20,880 --> 00:00:25,560
F in fermentation

00:00:21,720 --> 00:00:27,240
that's called bolt and that's actually

00:00:25,560 --> 00:00:33,480
the open MP implementation over

00:00:27,240 --> 00:00:35,940
lightweight thread so so I think people

00:00:33,480 --> 00:00:38,010
here most of people here are familiar

00:00:35,940 --> 00:00:41,340
with open appear but let me briefly

00:00:38,010 --> 00:00:44,010
introduce our MP well so please open up

00:00:41,340 --> 00:00:45,960
is a directive based prime mother and so

00:00:44,010 --> 00:00:48,750
we have sequential code like this and

00:00:45,960 --> 00:00:51,000
you just augment that code with some

00:00:48,750 --> 00:00:53,399
programmers then the compiler like this

00:00:51,000 --> 00:00:55,860
a compiler will automatically translate

00:00:53,399 --> 00:00:58,410
this code to help code and then the your

00:00:55,860 --> 00:01:00,570
code will be learned in parer on a

00:00:58,410 --> 00:01:03,390
multi-core machine or many co machines

00:01:00,570 --> 00:01:07,040
that's the idea and we have many many

00:01:03,390 --> 00:01:09,990
implementations currently GCC or

00:01:07,040 --> 00:01:15,150
interpretation or LLVM claim and so on

00:01:09,990 --> 00:01:17,670
so but recently we realized that these

00:01:15,150 --> 00:01:21,180
implementations are not a very good at

00:01:17,670 --> 00:01:25,040
supporting nested parallelism or nested

00:01:21,180 --> 00:01:30,990
paradox so we tackled this issue and by

00:01:25,040 --> 00:01:33,630
testing some by testing this simpler

00:01:30,990 --> 00:01:35,880
code so we have this pole and this is

00:01:33,630 --> 00:01:39,119
this is outer loop and then this is

00:01:35,880 --> 00:01:41,280
paralyzed by opening tip powerful and so

00:01:39,119 --> 00:01:44,329
the so we expect this code will be

00:01:41,280 --> 00:01:48,210
learning terror on much cone machine but

00:01:44,329 --> 00:01:51,329
but by accident we this code is using

00:01:48,210 --> 00:01:53,939
library and that's called deep compute

00:01:51,329 --> 00:01:56,520
API and so we this code is calling this

00:01:53,939 --> 00:01:59,909
function but inside this function again

00:01:56,520 --> 00:02:04,020
it has loop and it's using op Lampe to

00:01:59,909 --> 00:02:06,299
paralyze its own code so so at the end

00:02:04,020 --> 00:02:09,030
we have this probe and inside this fall

00:02:06,299 --> 00:02:12,270
and then whose are paralyzed by openmp

00:02:09,030 --> 00:02:13,360
so when we looked at this curve pattern

00:02:12,270 --> 00:02:16,959
and

00:02:13,360 --> 00:02:19,090
and then we tested this code with people

00:02:16,959 --> 00:02:21,750
and open implementations here basically

00:02:19,090 --> 00:02:24,940
GCC and Intel compiler and then

00:02:21,750 --> 00:02:27,430
basically we tested the extreme case so

00:02:24,940 --> 00:02:31,299
we've used all cores for the outer loop

00:02:27,430 --> 00:02:34,480
and then we used on other more thread

00:02:31,299 --> 00:02:44,910
for the inner of in the law that's using

00:02:34,480 --> 00:02:48,879
the inside function okay sorry so

00:02:44,910 --> 00:02:52,569
measure the performance of the previous

00:02:48,879 --> 00:02:54,670
code are using book many thread actually

00:02:52,569 --> 00:02:57,579
we use the 36 thread for the outer loop

00:02:54,670 --> 00:02:59,829
and that's basically using entire course

00:02:57,579 --> 00:03:02,470
on the machine because that we use the

00:02:59,829 --> 00:03:04,150
36 core machine and then we increase the

00:03:02,470 --> 00:03:06,549
number of sled for the inner loop and

00:03:04,150 --> 00:03:09,220
then we notice that the performance was

00:03:06,549 --> 00:03:11,409
dropping significantly this is this log

00:03:09,220 --> 00:03:13,480
scale the y-axis low scale and then

00:03:11,409 --> 00:03:17,799
performance was dropping very

00:03:13,480 --> 00:03:22,419
significantly even when we have 36 by 36

00:03:17,799 --> 00:03:24,489
thread and the the even the base line

00:03:22,419 --> 00:03:27,099
case was less than point to point to

00:03:24,489 --> 00:03:29,709
point a point zero one second but it

00:03:27,099 --> 00:03:33,549
turned out at the end we almost two

00:03:29,709 --> 00:03:36,280
seconds and then it also tested inter

00:03:33,549 --> 00:03:39,250
compiler and even inter compiler was

00:03:36,280 --> 00:03:41,829
performing better because basically the

00:03:39,250 --> 00:03:44,349
difference is how they managed read

00:03:41,829 --> 00:03:46,419
internally so for GCC they don't manage

00:03:44,349 --> 00:03:48,129
thread pool they just create thread and

00:03:46,419 --> 00:03:50,769
they just destroy thread and then read

00:03:48,129 --> 00:03:52,599
again and again on the other hand Intel

00:03:50,769 --> 00:03:54,879
compiler is doing much better

00:03:52,599 --> 00:03:57,069
they they they do manage a thread pool

00:03:54,879 --> 00:03:59,049
and then they do use thread as long as

00:03:57,069 --> 00:04:03,280
they have some idle thread so that's why

00:03:59,049 --> 00:04:06,730
they performed better than GCC and in

00:04:03,280 --> 00:04:09,910
terms of absolute performance but again

00:04:06,730 --> 00:04:12,099
but we tested we manually translate this

00:04:09,910 --> 00:04:14,530
code to the another threading layer

00:04:12,099 --> 00:04:18,220
because we notice that this performance

00:04:14,530 --> 00:04:20,500
came from the fact they are using piece

00:04:18,220 --> 00:04:23,219
read internally so we use the difference

00:04:20,500 --> 00:04:25,570
reading model to test whether the how

00:04:23,219 --> 00:04:26,740
how the threading model affects the

00:04:25,570 --> 00:04:30,460
performance especially

00:04:26,740 --> 00:04:32,319
this case and so we use a static model

00:04:30,460 --> 00:04:34,509
for our robot that's basically user

00:04:32,319 --> 00:04:36,550
arrested at the library and we used use

00:04:34,509 --> 00:04:38,949
the rebus read instead of OS letters

00:04:36,550 --> 00:04:41,199
rather like this red so we manually

00:04:38,949 --> 00:04:44,139
translate the previous code to our goais

00:04:41,199 --> 00:04:46,810
code and then we measured performance

00:04:44,139 --> 00:04:49,300
again and the performance we improved

00:04:46,810 --> 00:04:51,400
this simple benchmark the performance of

00:04:49,300 --> 00:04:54,130
this simple benchmark a lot and

00:04:51,400 --> 00:04:56,919
basically the law is better and these

00:04:54,130 --> 00:05:01,120
two the performance that we obtained

00:04:56,919 --> 00:05:04,539
from using Argos so basically my talk is

00:05:01,120 --> 00:05:09,090
about how we implement the open P /

00:05:04,539 --> 00:05:11,349
light race thread or our bubbles okay so

00:05:09,090 --> 00:05:15,759
the bolts yeah I'm going through this

00:05:11,349 --> 00:05:17,979
first here support is we aim at a very

00:05:15,759 --> 00:05:20,080
lightning fast opening implementation

00:05:17,979 --> 00:05:21,880
here and even though it's not really

00:05:20,080 --> 00:05:24,699
lightning fast here not now but it's

00:05:21,880 --> 00:05:27,159
quite fast and so port basically the

00:05:24,699 --> 00:05:28,979
name boards is recursive acronym that

00:05:27,159 --> 00:05:32,740
stands for a protist

00:05:28,979 --> 00:05:36,180
open pure light race red so we have o e

00:05:32,740 --> 00:05:39,340
o LT but we still haven't found the

00:05:36,180 --> 00:05:42,310
proper world for V so if you one of you

00:05:39,340 --> 00:05:44,759
have an idea about V then please come to

00:05:42,310 --> 00:05:49,780
me and or email me then I'll yeah I

00:05:44,759 --> 00:05:51,969
really appreciate our destination so we

00:05:49,780 --> 00:05:54,610
have enough website and we just release

00:05:51,969 --> 00:05:57,159
the app free V version of our software

00:05:54,610 --> 00:06:02,430
so you guys can go and download and test

00:05:57,159 --> 00:06:06,190
it so we our objective were Bert has

00:06:02,430 --> 00:06:08,740
absolutely zero reactive so the first

00:06:06,190 --> 00:06:11,800
one we wanna improve the nest massive

00:06:08,740 --> 00:06:13,270
parallelism will bolt and we also want

00:06:11,800 --> 00:06:15,789
to enhance the fine-grained task

00:06:13,270 --> 00:06:17,500
parallelism and lastly we want to

00:06:15,789 --> 00:06:20,440
improve the interoperability between

00:06:17,500 --> 00:06:23,620
OpenMP and MPI or other internal

00:06:20,440 --> 00:06:25,719
spreading modern so in this talk so far

00:06:23,620 --> 00:06:28,659
we have focused on the first objective

00:06:25,719 --> 00:06:31,300
and but we since this is Lisa's project

00:06:28,659 --> 00:06:33,400
and we are continuously working on this

00:06:31,300 --> 00:06:37,750
project and so we were will address

00:06:33,400 --> 00:06:41,740
other three issues later

00:06:37,750 --> 00:06:44,380
so basic approach of Ortiz we used

00:06:41,740 --> 00:06:47,430
existing compilers especially Intel

00:06:44,380 --> 00:06:50,980
compiler or clang compiler to generate

00:06:47,430 --> 00:06:53,380
the runtime code were the intermediate

00:06:50,980 --> 00:06:56,020
code from the openmp source code so we

00:06:53,380 --> 00:06:58,510
use the existing compiler but whether

00:06:56,020 --> 00:07:01,630
using the existing longtime code we

00:06:58,510 --> 00:07:05,010
developed our own runtime using the same

00:07:01,630 --> 00:07:08,920
runtime api so that means we kept the

00:07:05,010 --> 00:07:11,890
ABI impediment comparability so you guys

00:07:08,920 --> 00:07:14,460
can use compiler as there but you just

00:07:11,890 --> 00:07:18,190
replaced a long time with bolt up from

00:07:14,460 --> 00:07:21,310
Intel on time or other this is the run

00:07:18,190 --> 00:07:26,290
time then the you code can just learn

00:07:21,310 --> 00:07:29,350
bit bolt best our approach so so to do

00:07:26,290 --> 00:07:32,410
that we our implementation is based on

00:07:29,350 --> 00:07:34,870
Intel prime P runtime api and we

00:07:32,410 --> 00:07:37,120
basically modify the the open source

00:07:34,870 --> 00:07:39,430
version of inter long time and that's

00:07:37,120 --> 00:07:43,240
what help we have been we have done so

00:07:39,430 --> 00:07:45,730
far and that we plan to also develop or

00:07:43,240 --> 00:07:49,419
modify the inter compiler to better

00:07:45,730 --> 00:07:52,630
support open and peer long time that is

00:07:49,419 --> 00:07:54,400
our board so let me briefly introduce

00:07:52,630 --> 00:07:58,290
our go bus because we are using our

00:07:54,400 --> 00:08:01,780
gopis internally and the our buboes is a

00:07:58,290 --> 00:08:05,470
lightweight low-level spreading and

00:08:01,780 --> 00:08:07,840
testing library and the philosophy of

00:08:05,470 --> 00:08:08,950
our go bus basically usually best read

00:08:07,840 --> 00:08:11,680
is not a new concept

00:08:08,950 --> 00:08:14,890
they have been existing for very long

00:08:11,680 --> 00:08:18,400
time but we realized that they what they

00:08:14,890 --> 00:08:21,340
have been that what they have tried is

00:08:18,400 --> 00:08:23,830
they want to replace kiss read beat you

00:08:21,340 --> 00:08:28,390
diverse trade or any others ready modern

00:08:23,830 --> 00:08:30,250
but that's not we realized s Naruto rush

00:08:28,390 --> 00:08:31,900
because the user rest rather than peace

00:08:30,250 --> 00:08:36,969
read they have different characteristics

00:08:31,900 --> 00:08:40,050
and they have different usages so so our

00:08:36,969 --> 00:08:42,640
goal was okay let's try to provide

00:08:40,050 --> 00:08:46,170
threading mechanisms that's basically

00:08:42,640 --> 00:08:49,279
learning on user space rather than

00:08:46,170 --> 00:08:52,879
rather than try to be place

00:08:49,279 --> 00:08:55,639
he's read so the plus of a barber board

00:08:52,879 --> 00:08:57,230
is okay we provide mechanisms but not

00:08:55,639 --> 00:08:59,990
policies even though we have some

00:08:57,230 --> 00:09:03,649
policies we don't impose what we don't

00:08:59,990 --> 00:09:08,870
encourage the user to use our policies

00:09:03,649 --> 00:09:11,839
so whether we recommend the user to

00:09:08,870 --> 00:09:14,060
develop their policy on top of our

00:09:11,839 --> 00:09:16,519
Goebbels mechanisms so we explicitly

00:09:14,060 --> 00:09:20,600
separate mechanisms and policies and

00:09:16,519 --> 00:09:22,819
then we support and support efficient

00:09:20,600 --> 00:09:26,750
mechanisms in our Goebbels

00:09:22,819 --> 00:09:28,879
so this is a simple diagram of our

00:09:26,750 --> 00:09:38,420
robust execution model so we have many

00:09:28,879 --> 00:09:41,779
course but course and we have many

00:09:38,420 --> 00:09:46,129
course it's better we have many cores of

00:09:41,779 --> 00:09:49,069
via pores and that we expose way thread

00:09:46,129 --> 00:09:53,750
like a piece read as extreme streams so

00:09:49,069 --> 00:09:55,639
we explicitly expose the first level of

00:09:53,750 --> 00:09:58,550
parallelism to the user so user can

00:09:55,639 --> 00:10:01,910
control these extremes and on top of

00:09:58,550 --> 00:10:04,100
this extreme stream we also provide user

00:10:01,910 --> 00:10:06,589
best read and task list that can be

00:10:04,100 --> 00:10:09,199
mapped to these extreme streams so you

00:10:06,589 --> 00:10:12,050
can explicitly manage this mapping or

00:10:09,199 --> 00:10:16,819
scheduling on table oh oh or us thread

00:10:12,050 --> 00:10:21,559
and and everything is explicit so that's

00:10:16,819 --> 00:10:23,540
the difference and the with this

00:10:21,559 --> 00:10:26,059
flexible mechanism you guys can

00:10:23,540 --> 00:10:28,339
implement open P or samples plus any

00:10:26,059 --> 00:10:30,259
other high-level forum mutters on top of

00:10:28,339 --> 00:10:34,399
this Argos because we Pro we don't

00:10:30,259 --> 00:10:38,559
impose any specific policy to the high

00:10:34,399 --> 00:10:41,000
level runtime that's our approach so we

00:10:38,559 --> 00:10:44,180
use this our Goebbels to implement

00:10:41,000 --> 00:10:45,949
boards so we have this kind of layer so

00:10:44,180 --> 00:10:49,160
we have machine and we have our gobos

00:10:45,949 --> 00:10:49,910
layer and then on top of this we have

00:10:49,160 --> 00:10:52,339
open a player

00:10:49,910 --> 00:10:56,540
so basically open app is read and open

00:10:52,339 --> 00:10:58,220
tasks are translated to open our Bibles

00:10:56,540 --> 00:11:01,069
user base read our task list

00:10:58,220 --> 00:11:02,780
what a difference was the difference

00:11:01,069 --> 00:11:04,820
between ult and task Lee

00:11:02,780 --> 00:11:07,190
at the realty is more or less the

00:11:04,820 --> 00:11:08,990
conventional state they have is they

00:11:07,190 --> 00:11:10,610
have their own stack and they can

00:11:08,990 --> 00:11:14,630
suspend and they can resume their

00:11:10,610 --> 00:11:16,430
execution and so and the other end the

00:11:14,630 --> 00:11:18,950
test lead doesn't have the stack and

00:11:16,430 --> 00:11:21,500
doesn't cannot expect cannot suspend

00:11:18,950 --> 00:11:24,290
this infusion so that police is just

00:11:21,500 --> 00:11:27,140
atomic rock unit so once you start this

00:11:24,290 --> 00:11:29,720
test clip you have to come to the end so

00:11:27,140 --> 00:11:30,470
that's the difference so in terms of

00:11:29,720 --> 00:11:34,850
overhead

00:11:30,470 --> 00:11:37,400
so since test fluid has limitation so it

00:11:34,850 --> 00:11:40,700
has less overhead annuity so our

00:11:37,400 --> 00:11:44,180
approach is if it's possible we use test

00:11:40,700 --> 00:11:45,770
clip but if it's not possible well you

00:11:44,180 --> 00:11:48,320
don't have any idea about the code and

00:11:45,770 --> 00:11:50,650
we use tactics that's our approach and

00:11:48,320 --> 00:11:54,590
everything is mapped it to again

00:11:50,650 --> 00:11:57,680
external streams and these are and so

00:11:54,590 --> 00:11:59,810
with this kind of mapping we we can

00:11:57,680 --> 00:12:04,040
handle nested parallelism or nested

00:11:59,810 --> 00:12:06,920
loops just just creating ults one task

00:12:04,040 --> 00:12:08,690
list and we map them to issue streams

00:12:06,920 --> 00:12:11,690
and then the scheduler internal

00:12:08,690 --> 00:12:13,750
scheduler that we developed manages the

00:12:11,690 --> 00:12:19,130
scheduling or mapping between this layer

00:12:13,750 --> 00:12:21,320
that's out so let let me show the simple

00:12:19,130 --> 00:12:23,839
translation example so we have this

00:12:21,320 --> 00:12:27,950
portable game and we have Horeb and away

00:12:23,839 --> 00:12:30,200
this is ParaPro loop and a compiler

00:12:27,950 --> 00:12:32,420
inter compiler work clang compiler just

00:12:30,200 --> 00:12:36,290
generate this kind of long time for and

00:12:32,420 --> 00:12:39,680
this is a book or so from the open type

00:12:36,290 --> 00:12:42,860
error and we kept this long time API as

00:12:39,680 --> 00:12:45,170
it is and then we develop we implemented

00:12:42,860 --> 00:12:47,450
a long time functions using our bubbles

00:12:45,170 --> 00:12:50,390
internally so we create each turn

00:12:47,450 --> 00:12:52,040
streams if needed and if they outlets

00:12:50,390 --> 00:12:54,830
they already we just we don't create

00:12:52,040 --> 00:12:58,580
more extreme streams and we create ult

00:12:54,830 --> 00:13:03,080
and task lead when it see these peril

00:12:58,580 --> 00:13:06,320
reasons or tasks and then we excuse this

00:13:03,080 --> 00:13:08,150
view at ease and later we join or all

00:13:06,320 --> 00:13:12,380
Bach units your T's work has to be at

00:13:08,150 --> 00:13:14,210
the end there's a simple idea and again

00:13:12,380 --> 00:13:17,390
this is just

00:13:14,210 --> 00:13:18,890
an illustration of our approach we have

00:13:17,390 --> 00:13:20,450
exchanged streams and we create work

00:13:18,890 --> 00:13:22,760
units and at the end we have some

00:13:20,450 --> 00:13:38,959
synchronization like a barrier at the

00:13:22,760 --> 00:13:43,130
end so I'll go bill is a user level

00:13:38,959 --> 00:13:45,140
library that we we have developed ok the

00:13:43,130 --> 00:13:47,240
question was where's the Argo bus and

00:13:45,140 --> 00:13:50,589
basically algobit is not a part of

00:13:47,240 --> 00:13:53,180
compiler but it's a part of library that

00:13:50,589 --> 00:13:56,750
we implemented actually that's not

00:13:53,180 --> 00:13:59,060
exactly part of our library that I'm

00:13:56,750 --> 00:14:02,750
introducing here but there's a separate

00:13:59,060 --> 00:14:05,060
library but it's also learning in a user

00:14:02,750 --> 00:14:08,120
user space and it's a user level library

00:14:05,060 --> 00:14:10,100
and we integrate desolating library the

00:14:08,120 --> 00:14:12,380
alga basis reading library and we

00:14:10,100 --> 00:14:20,600
integrate their library to our opened

00:14:12,380 --> 00:14:23,540
implementation so again our

00:14:20,600 --> 00:14:27,860
implementation is based on interests

00:14:23,540 --> 00:14:29,990
open-source runtime library and a test

00:14:27,860 --> 00:14:33,860
is there and we kept again the original

00:14:29,990 --> 00:14:38,209
runtime api as release so we the

00:14:33,860 --> 00:14:40,250
baseball what we did was we we developed

00:14:38,209 --> 00:14:43,640
we designed and implemented threading

00:14:40,250 --> 00:14:46,339
layer using our gobos originally the

00:14:43,640 --> 00:14:48,650
runtime is was using his read-only piece

00:14:46,339 --> 00:14:51,950
read or other artists read from

00:14:48,650 --> 00:14:54,680
supported from Wes like Windows res and

00:14:51,950 --> 00:14:56,750
so on but we added this another layer

00:14:54,680 --> 00:15:00,860
and then we also changed some internals

00:14:56,750 --> 00:15:05,300
to support and to support OpenMP more

00:15:00,860 --> 00:15:08,450
efficiently with our gobos okay then

00:15:05,300 --> 00:15:12,290
okay that's the just based approach and

00:15:08,450 --> 00:15:15,500
how can I use port there are two simple

00:15:12,290 --> 00:15:17,600
ways the first one we support currently

00:15:15,500 --> 00:15:19,790
Clank compiler internet open antic

00:15:17,600 --> 00:15:22,370
compiler and GCC because this is the

00:15:19,790 --> 00:15:24,200
same for the Intel on time because Intel

00:15:22,370 --> 00:15:26,360
entitlements provide already provides

00:15:24,200 --> 00:15:26,810
the ABI comparability realities to

00:15:26,360 --> 00:15:32,540
become

00:15:26,810 --> 00:15:37,030
and and we kept that ABI as it is so we

00:15:32,540 --> 00:15:39,890
can use these Civic compilers and then

00:15:37,030 --> 00:15:42,530
so that means you just use it in

00:15:39,890 --> 00:15:45,110
compilers but when you the code you

00:15:42,530 --> 00:15:45,830
use port instead of int a long time or

00:15:45,110 --> 00:15:48,290
welcome

00:15:45,830 --> 00:15:51,230
then the you could get automatically use

00:15:48,290 --> 00:15:53,120
board and the board can use our boss his

00:15:51,230 --> 00:15:56,290
read but we recommend are used to use

00:15:53,120 --> 00:16:00,500
our Goebbels of course and that's it or

00:15:56,290 --> 00:16:01,940
the simple way is to use LD preload you

00:16:00,500 --> 00:16:04,700
don't need to decompile then your port

00:16:01,940 --> 00:16:07,340
so you have your compiled version of

00:16:04,700 --> 00:16:11,930
your code and when you execute your code

00:16:07,340 --> 00:16:15,950
you will set LD preload to the the ports

00:16:11,930 --> 00:16:18,530
runtime then the the runtime will be

00:16:15,950 --> 00:16:20,890
automatically used in your code there

00:16:18,530 --> 00:16:20,890
are two ways

00:16:24,980 --> 00:16:31,590
this question the question is question

00:16:28,200 --> 00:16:35,340
is whether the board to support Tesla

00:16:31,590 --> 00:16:36,720
loops were the task dependencies the the

00:16:35,340 --> 00:16:38,520
answer is currently we support task

00:16:36,720 --> 00:16:40,950
dependencies but not has to look yet

00:16:38,520 --> 00:16:44,100
because because the original long time

00:16:40,950 --> 00:16:46,020
has some limitations yet and they

00:16:44,100 --> 00:16:48,090
haven't implemented everything like the

00:16:46,020 --> 00:16:50,280
implement has to look but the compilers

00:16:48,090 --> 00:16:53,130
not all compilers support to test loop

00:16:50,280 --> 00:16:55,290
yet yeah so that's why currently we

00:16:53,130 --> 00:17:01,080
don't officially support first loop what

00:16:55,290 --> 00:17:03,420
we support has two dependencies and this

00:17:01,080 --> 00:17:06,780
is this is some validation wizard that

00:17:03,420 --> 00:17:10,380
we use the open us open area validations

00:17:06,780 --> 00:17:12,540
to it - yeah to test whether the board

00:17:10,380 --> 00:17:15,210
is correctly implemented and

00:17:12,540 --> 00:17:17,430
interestingly even though we use the

00:17:15,210 --> 00:17:21,440
same compiler here hooked up Intel

00:17:17,430 --> 00:17:24,840
compiler that we got less failure than

00:17:21,440 --> 00:17:27,320
Intel compiler the reason yeah I can

00:17:24,840 --> 00:17:32,040
tell you later in person but I

00:17:27,320 --> 00:17:34,590
explicitly described there and this is

00:17:32,040 --> 00:17:36,780
again the the micro benchmark with port

00:17:34,590 --> 00:17:40,530
not this is a manual translation and

00:17:36,780 --> 00:17:42,540
used to result from volt because we when

00:17:40,530 --> 00:17:46,530
we implement open up a long time we have

00:17:42,540 --> 00:17:48,840
to add some more layers and there's that

00:17:46,530 --> 00:17:53,090
causes some overhead but we try to

00:17:48,840 --> 00:17:56,040
minimize this overhead continuously but

00:17:53,090 --> 00:17:59,280
we have compared to the manual

00:17:56,040 --> 00:18:02,760
translation of OpenMP and if we have

00:17:59,280 --> 00:18:05,160
overhead and this one is using unity and

00:18:02,760 --> 00:18:07,830
this one is using ult as well as Tesla

00:18:05,160 --> 00:18:09,390
and okay the test code has left over and

00:18:07,830 --> 00:18:12,150
we got a better performance here and

00:18:09,390 --> 00:18:14,480
these two are from GCC and again inter

00:18:12,150 --> 00:18:14,480
capella

00:18:21,730 --> 00:18:28,070
so this Isis pers Argos means we

00:18:25,070 --> 00:18:30,140
manually translate the micro benchmark

00:18:28,070 --> 00:18:32,570
that I showed before to the Argo bus

00:18:30,140 --> 00:18:42,260
code yeah I don't go through portal

00:18:32,570 --> 00:18:45,580
right right yeah yes yeah the vault is

00:18:42,260 --> 00:18:45,580
open I appear on time yeah

00:18:46,610 --> 00:18:51,370
and the next yeah

00:18:54,670 --> 00:18:59,530
now this is deep this is a gnar weeks

00:18:57,220 --> 00:19:00,820
calluses from scaling but we

00:18:59,530 --> 00:19:06,370
oversubscribed the machine

00:19:00,820 --> 00:19:08,650
this is netted loop and a number of

00:19:06,370 --> 00:19:12,220
cents will email about outer loop was

00:19:08,650 --> 00:19:15,400
using 36 thread and this is 36 36 for

00:19:12,220 --> 00:19:28,270
machine yeah this is so just to see pure

00:19:15,400 --> 00:19:30,460
overhead of nested loops so okay the

00:19:28,270 --> 00:19:33,610
question is whether we whether our

00:19:30,460 --> 00:19:37,540
implementation provides some benefit for

00:19:33,610 --> 00:19:40,300
the non-nasa to cases so but my answer

00:19:37,540 --> 00:19:43,720
is we don't expect much of benefit

00:19:40,300 --> 00:19:45,730
because with a single nested loops most

00:19:43,720 --> 00:19:48,850
calls that compute bound and in that

00:19:45,730 --> 00:19:51,580
case the creation and destruction of

00:19:48,850 --> 00:19:54,580
thread the the overhead of those two

00:19:51,580 --> 00:19:56,800
operations is very bitter and even you

00:19:54,580 --> 00:19:58,540
cannot see the difference even though

00:19:56,800 --> 00:20:01,090
there is a difference that the magnitude

00:19:58,540 --> 00:20:14,110
of the difference is less than

00:20:01,090 --> 00:20:16,920
millisecond microsecond unit right yeah

00:20:14,110 --> 00:20:16,920
but

00:20:20,630 --> 00:20:28,210
yep

00:20:22,910 --> 00:20:28,210
schedule a scheduler is basically trying

00:20:28,700 --> 00:20:36,550
- whatever can meet parameters to play

00:20:34,130 --> 00:20:36,550
with

00:20:39,360 --> 00:20:43,750
okay

00:20:40,510 --> 00:20:46,030
the question our question is how how we

00:20:43,750 --> 00:20:50,380
could to better performance and the way

00:20:46,030 --> 00:20:52,000
was the main reason of getting better

00:20:50,380 --> 00:20:55,750
performance compared to the existing

00:20:52,000 --> 00:20:58,810
with the main factor was because we used

00:20:55,750 --> 00:21:02,290
the light wasted library instead of his

00:20:58,810 --> 00:21:05,740
read so the cost of managing shred is

00:21:02,290 --> 00:21:08,920
much much lesser then using this read

00:21:05,740 --> 00:21:11,680
that's the main factor and and in this

00:21:08,920 --> 00:21:13,450
case the the critical factor is we

00:21:11,680 --> 00:21:15,520
didn't actually oversubscribed the

00:21:13,450 --> 00:21:18,040
machine because if we created this

00:21:15,520 --> 00:21:20,640
amount of visceral and basically you are

00:21:18,040 --> 00:21:24,970
oversubscribed the machine using many

00:21:20,640 --> 00:21:27,940
israel but with with volt or with our

00:21:24,970 --> 00:21:31,300
gobos we actually didn't over subscribe

00:21:27,940 --> 00:21:33,820
the machine it with us reversal but we

00:21:31,300 --> 00:21:35,800
create more usual restaurant so that's

00:21:33,820 --> 00:21:38,680
not the deal over subscription even

00:21:35,800 --> 00:21:41,260
though we create most read and they only

00:21:38,680 --> 00:21:44,710
some amount and some number of thread

00:21:41,260 --> 00:21:47,260
are learning in terror and some of them

00:21:44,710 --> 00:21:50,490
are not learning in terror that's the

00:21:47,260 --> 00:21:50,490
difference yeah

00:21:58,500 --> 00:22:07,409
what kind of give me extra performance

00:22:16,200 --> 00:22:19,660
yeah

00:22:17,440 --> 00:22:22,480
there's a good case it's how I mean the

00:22:19,660 --> 00:22:24,930
question is how we can tweak the long

00:22:22,480 --> 00:22:27,430
time to get a better performance but

00:22:24,930 --> 00:22:29,440
currently we have provided since we

00:22:27,430 --> 00:22:31,480
haven't touched the compiler past the

00:22:29,440 --> 00:22:34,720
compiler cannot do some magic for you

00:22:31,480 --> 00:22:36,940
but we provide some interface additional

00:22:34,720 --> 00:22:39,640
interface to the user the user can tweak

00:22:36,940 --> 00:22:42,610
something that's the difference between

00:22:39,640 --> 00:22:45,370
these two so basically without you just

00:22:42,610 --> 00:22:47,800
hint or without compiler support the

00:22:45,370 --> 00:22:51,310
long time cannot know whether this code

00:22:47,800 --> 00:22:53,800
is safe to long read test click wait

00:22:51,310 --> 00:22:56,170
yeah so because the test pool has

00:22:53,800 --> 00:22:58,540
limitations but if the user can provide

00:22:56,170 --> 00:23:01,570
that kind of hint what compiler can give

00:22:58,540 --> 00:23:04,180
has that information to the long time

00:23:01,570 --> 00:23:12,250
the long time can exploit task list and

00:23:04,180 --> 00:23:15,160
then you can improve this perform the

00:23:12,250 --> 00:23:18,880
basically empty loop the operation is

00:23:15,160 --> 00:23:22,500
empty the loop body was empty and so

00:23:18,880 --> 00:23:22,500
this is just to see pure word

00:23:24,780 --> 00:23:33,090
yeah yeah here comes this question

00:23:30,140 --> 00:23:34,950
that's a good one yeah so we have two

00:23:33,090 --> 00:23:38,360
applications to this year and the first

00:23:34,950 --> 00:23:42,180
one is the femm test multiple method and

00:23:38,360 --> 00:23:45,060
actually this code the original

00:23:42,180 --> 00:23:48,690
implementation is using Intel MKL to

00:23:45,060 --> 00:23:50,700
upload the DCM function so but the Intel

00:23:48,690 --> 00:23:52,590
MKL has sequential version and OpenMP

00:23:50,700 --> 00:23:57,060
version and and so on TV version is own

00:23:52,590 --> 00:23:59,010
but by default the MK a disables the

00:23:57,060 --> 00:24:01,320
open MP support if the function is

00:23:59,010 --> 00:24:03,170
called inside or frontier because they

00:24:01,320 --> 00:24:05,190
don't want to handle the previous

00:24:03,170 --> 00:24:09,990
oversubscribed occasion occasion and so

00:24:05,190 --> 00:24:13,700
on so what we did was okay when we have

00:24:09,990 --> 00:24:17,120
more cores and when they have I mean

00:24:13,700 --> 00:24:19,740
additional hardware resources and we

00:24:17,120 --> 00:24:23,190
enabled this kind of nested parallelism

00:24:19,740 --> 00:24:26,130
then how we want to see whether we can

00:24:23,190 --> 00:24:29,310
achieve better performance or any speed

00:24:26,130 --> 00:24:31,560
so that's what we did here and is that

00:24:29,310 --> 00:24:33,090
the the number of self for the

00:24:31,560 --> 00:24:36,930
application this is for the outer loop

00:24:33,090 --> 00:24:39,180
and to nine actually we had we had 36

00:24:36,930 --> 00:24:41,340
course in this machine but we just

00:24:39,180 --> 00:24:45,990
wanted to test whether we could achieve

00:24:41,340 --> 00:24:49,580
the speedo and and then put the MK air

00:24:45,990 --> 00:24:51,450
and we increase the number of thread

00:24:49,580 --> 00:24:54,600
from 1 to 8

00:24:51,450 --> 00:24:57,690
and after this the first three cases and

00:24:54,600 --> 00:24:59,400
not over subscribing the machine they so

00:24:57,690 --> 00:25:03,690
the number of total numbers read here is

00:24:59,400 --> 00:25:05,580
36 and this is at 918 and he here 72

00:25:03,690 --> 00:25:08,460
this is that the last cases the or

00:25:05,580 --> 00:25:10,680
substitution case so well for the these

00:25:08,460 --> 00:25:13,770
three cases even though we are not over

00:25:10,680 --> 00:25:17,790
self triangle machine and the Intel on

00:25:13,770 --> 00:25:20,640
time didn't even improve the performance

00:25:17,790 --> 00:25:25,080
much so only only we could see a little

00:25:20,640 --> 00:25:27,210
improvement here but we speculate that

00:25:25,080 --> 00:25:30,840
the reason was probably the management

00:25:27,210 --> 00:25:33,300
of thread or they they they haven't

00:25:30,840 --> 00:25:34,850
optimized the nested loop cases that's

00:25:33,300 --> 00:25:37,990
our suspect

00:25:34,850 --> 00:25:39,820
speculation and but we're bored

00:25:37,990 --> 00:25:42,820
because we are using now we are using

00:25:39,820 --> 00:25:44,470
workers for the interim care actually we

00:25:42,820 --> 00:25:47,440
could achieve speed-up we could reduce

00:25:44,470 --> 00:25:48,399
the external time here and and even with

00:25:47,440 --> 00:25:51,100
oversubscription

00:25:48,399 --> 00:25:52,840
we didn't lose performance much so we

00:25:51,100 --> 00:25:55,360
can almost kept up same performance as

00:25:52,840 --> 00:25:58,120
the previous cases yeah that's the

00:25:55,360 --> 00:26:01,419
leaders case and so that's a while while

00:25:58,120 --> 00:26:03,990
I mean what do you expect for the future

00:26:01,419 --> 00:26:06,460
applications because we are getting

00:26:03,990 --> 00:26:09,789
zippers that have more and more cores

00:26:06,460 --> 00:26:11,799
and if we have a limited parallelism or

00:26:09,789 --> 00:26:14,950
the outer loop were in the application

00:26:11,799 --> 00:26:17,740
level then we are try to exploit more

00:26:14,950 --> 00:26:21,760
parallelism in turn for the internal

00:26:17,740 --> 00:26:23,760
library or inner loops yeah and this is

00:26:21,760 --> 00:26:28,059
another application and this is a

00:26:23,760 --> 00:26:30,909
climate modeling mini app and again we

00:26:28,059 --> 00:26:33,370
tested actually this application is

00:26:30,909 --> 00:26:35,049
already implementing nested loops yeah I

00:26:33,370 --> 00:26:39,909
will put the put for the future user

00:26:35,049 --> 00:26:41,260
cases and again we see you could achieve

00:26:39,909 --> 00:26:47,070
better performance much better

00:26:41,260 --> 00:26:47,070
performance than Intel runtime okay okay

00:26:50,550 --> 00:26:53,550
right

00:26:59,820 --> 00:27:06,410
the question is uh the intel on time is

00:27:03,630 --> 00:27:09,179
supporting open a bt interface but

00:27:06,410 --> 00:27:12,240
whether portal is also supporting that

00:27:09,179 --> 00:27:15,179
currently no yeah because currently we

00:27:12,240 --> 00:27:17,309
are not so we are not supporting of BT's

00:27:15,179 --> 00:27:20,730
interface because there are many issues

00:27:17,309 --> 00:27:23,190
for example we need to handle different

00:27:20,730 --> 00:27:26,190
threading libraries and so on so that is

00:27:23,190 --> 00:27:28,650
done is more much work and so we will

00:27:26,190 --> 00:27:31,440
try to support but they retakes didn't

00:27:28,650 --> 00:27:33,600
need some time okay the last slide our

00:27:31,440 --> 00:27:35,820
summarize the vault is our new

00:27:33,600 --> 00:27:39,289
implementation over libraries thread the

00:27:35,820 --> 00:27:42,360
basically we used our bubbles here and

00:27:39,289 --> 00:27:46,470
we release the pre-release version and

00:27:42,360 --> 00:27:50,100
you guys go there and download and test

00:27:46,470 --> 00:27:52,350
it and this is 14 the the port is main

00:27:50,100 --> 00:27:54,320
mainly managed by Argonne National Lab

00:27:52,350 --> 00:27:57,200
and there we have some collaborators and

00:27:54,320 --> 00:28:01,650
our global was developed by these guys

00:27:57,200 --> 00:28:05,659
including me and there are some pointers

00:28:01,650 --> 00:28:08,429
to the link and we acknowledge our

00:28:05,659 --> 00:28:09,840
funding agencies and actually I'm

00:28:08,429 --> 00:28:13,289
working in the program models and

00:28:09,840 --> 00:28:16,200
runtime system work at the Argonne the

00:28:13,289 --> 00:28:18,179
led by dr. Lodge and I we have step any

00:28:16,200 --> 00:28:21,510
misstep members and students and so on

00:28:18,179 --> 00:28:25,250
and go in there and check some people

00:28:21,510 --> 00:28:25,250

YouTube URL: https://www.youtube.com/watch?v=95gYxOiTcrc


