Title: OpenMP in Embedded Systems
Publication date: 2016-11-21
Playlist: SC16 OpenMP Booth Talks
Description: 
	Sunita Chandrasekaran, University of Delaware
Booth talk at SC16, November 2016 Salt Lake City
Captions: 
	00:00:00,000 --> 00:00:05,009
okay so hello everybody I am Sunita

00:00:03,510 --> 00:00:09,780
chandrasekharan from University of

00:00:05,009 --> 00:00:11,969
Delaware this is a work that was done

00:00:09,780 --> 00:00:14,639
with that that's been done over the past

00:00:11,969 --> 00:00:16,560
several years with folks suits students

00:00:14,639 --> 00:00:19,410
in dr. Chapman when we better at

00:00:16,560 --> 00:00:20,910
University of Houston as an HP ctools

00:00:19,410 --> 00:00:22,380
group so if you look at the

00:00:20,910 --> 00:00:24,930
acknowledgement list it's a laundry list

00:00:22,380 --> 00:00:28,230
we have a pink zone was right here he's

00:00:24,930 --> 00:00:31,949
with AMD a PA he graduated from UHS a

00:00:28,230 --> 00:00:33,570
PhD student so then sue angers a master

00:00:31,949 --> 00:00:36,059
was a master's student at you of h and

00:00:33,570 --> 00:00:38,129
he's at microsoft shang wang graduated

00:00:36,059 --> 00:00:40,290
with a PhD he's at microsoft dr. Chapman

00:00:38,129 --> 00:00:42,180
off to Stony Brook Tobias and markers

00:00:40,290 --> 00:00:43,800
are from Siemens Germany so it's a

00:00:42,180 --> 00:00:46,100
collaborative work as you can see and

00:00:43,800 --> 00:00:50,100
there's also work came out of

00:00:46,100 --> 00:00:54,600
semiconductor research corporation okay

00:00:50,100 --> 00:00:56,870
so let's dive into the so this is

00:00:54,600 --> 00:01:00,210
something that is concentrating on

00:00:56,870 --> 00:01:02,850
embedded systems like how do we need you

00:01:00,210 --> 00:01:04,290
know why do we need need high level

00:01:02,850 --> 00:01:05,970
abstraction or programming models for

00:01:04,290 --> 00:01:07,830
embedded systems so that's the key

00:01:05,970 --> 00:01:09,810
message here so just start with

00:01:07,830 --> 00:01:12,540
heterogeneous embedded systems we have

00:01:09,810 --> 00:01:15,060
you know many number of devices it just

00:01:12,540 --> 00:01:17,610
is not just CPU plus GPU we have you

00:01:15,060 --> 00:01:21,330
know arm plus DSP we have cpu plus FPGA

00:01:17,610 --> 00:01:23,460
the recent Intel alterra machine then we

00:01:21,330 --> 00:01:26,040
have arm plus GPU which is the Nvidia's

00:01:23,460 --> 00:01:27,600
Tegra x1 so all these counters

00:01:26,040 --> 00:01:29,310
heterogeneous systems right so we are

00:01:27,600 --> 00:01:30,900
not just looking at homogeneous systems

00:01:29,310 --> 00:01:33,150
you're looking at head with hydrogen

00:01:30,900 --> 00:01:35,520
assistance systems of different types so

00:01:33,150 --> 00:01:38,280
that's a qualcomm snapdragon this is a

00:01:35,520 --> 00:01:40,380
Tegra tx1 with forearm course in a GPU

00:01:38,280 --> 00:01:42,659
core systems are getting complicated

00:01:40,380 --> 00:01:46,500
right we are having devices of different

00:01:42,659 --> 00:01:48,479
types so programming multi-core embedded

00:01:46,500 --> 00:01:50,670
systems has been a challenge it's

00:01:48,479 --> 00:01:53,670
continuing to be a challenge although

00:01:50,670 --> 00:01:55,560
you know many many intricate details are

00:01:53,670 --> 00:01:57,240
being ironed out the different

00:01:55,560 --> 00:01:59,969
programming models are kind of existing

00:01:57,240 --> 00:02:02,070
but on a on a broad level heterogeneous

00:01:59,969 --> 00:02:03,869
systems are presenting complexity both

00:02:02,070 --> 00:02:07,500
of the silicon and at the system level

00:02:03,869 --> 00:02:09,629
and standards and toolchain are more and

00:02:07,500 --> 00:02:11,489
more proprietary as we have different

00:02:09,629 --> 00:02:12,860
vendors having their own proprietary

00:02:11,489 --> 00:02:14,840
tool chains

00:02:12,860 --> 00:02:16,610
or we have really low level you know

00:02:14,840 --> 00:02:17,900
intrinsic kind of languages which you

00:02:16,610 --> 00:02:20,450
know which is again a problem with

00:02:17,900 --> 00:02:21,830
embedded programming an amulet system we

00:02:20,450 --> 00:02:23,750
have portability scalability issues

00:02:21,830 --> 00:02:25,640
portability meaning you really cannot

00:02:23,750 --> 00:02:28,550
use a tool chain which is proprietary

00:02:25,640 --> 00:02:30,110
proprietary on one target platform on

00:02:28,550 --> 00:02:31,940
another target platform even belonging

00:02:30,110 --> 00:02:34,460
to the same family so that's also a

00:02:31,940 --> 00:02:36,440
challenge scalability issues sometimes a

00:02:34,460 --> 00:02:38,330
you know a small code is to core

00:02:36,440 --> 00:02:40,130
platform and there is another co another

00:02:38,330 --> 00:02:41,900
platform with four cores eight core so

00:02:40,130 --> 00:02:44,420
on so forth so there are scalability

00:02:41,900 --> 00:02:46,130
issues as well and TTM is a very

00:02:44,420 --> 00:02:50,090
important concept in embedded system

00:02:46,130 --> 00:02:51,680
where with not real abstractions or not

00:02:50,090 --> 00:02:53,480
a neat programming model the time to

00:02:51,680 --> 00:02:56,150
market is becoming more and it's not

00:02:53,480 --> 00:02:57,950
becoming large which is not good because

00:02:56,150 --> 00:03:00,260
you know you know what they like iphone

00:02:57,950 --> 00:03:01,700
and you know Samsung on Android phones

00:03:00,260 --> 00:03:03,530
and stuff you wanted to go to the market

00:03:01,700 --> 00:03:05,180
really quickly for which you need the

00:03:03,530 --> 00:03:07,700
software tool chain to be efficient you

00:03:05,180 --> 00:03:10,130
cannot afford to have a longer time to

00:03:07,700 --> 00:03:11,780
you know create that tool chain so

00:03:10,130 --> 00:03:15,380
bottom line is we need industry

00:03:11,780 --> 00:03:17,060
standards we need standards that can you

00:03:15,380 --> 00:03:18,350
know like OpenMP which is industry

00:03:17,060 --> 00:03:20,299
standard put together by different

00:03:18,350 --> 00:03:22,700
different academia universities vendors

00:03:20,299 --> 00:03:24,260
right so we need something like that to

00:03:22,700 --> 00:03:25,790
offer portable scalable software

00:03:24,260 --> 00:03:27,739
solutions so that we're able to target

00:03:25,790 --> 00:03:30,890
more than just one platform it should be

00:03:27,739 --> 00:03:33,830
the key idea for a given programming

00:03:30,890 --> 00:03:35,780
model so looking at the current set of

00:03:33,830 --> 00:03:38,030
you know state-of-the-art solutions for

00:03:35,780 --> 00:03:41,239
heterogeneous embedded systems we are

00:03:38,030 --> 00:03:44,600
seeing different types of solutions and

00:03:41,239 --> 00:03:45,830
among those opencl has been pretty neat

00:03:44,600 --> 00:03:47,989
in terms of being able to target

00:03:45,830 --> 00:03:49,430
different platforms so there they have a

00:03:47,989 --> 00:03:51,680
variety of target platforms that they

00:03:49,430 --> 00:03:53,090
can really target most of the other

00:03:51,680 --> 00:03:55,370
state-of-the-art solutions are either

00:03:53,090 --> 00:03:57,620
too high level are sized too heavy

00:03:55,370 --> 00:03:59,330
weight for emitted platforms where you

00:03:57,620 --> 00:04:01,010
have really scarce resources we are not

00:03:59,330 --> 00:04:02,989
talking about thousand course you're not

00:04:01,010 --> 00:04:04,580
talking about you know a titan

00:04:02,989 --> 00:04:07,100
supercomputer where you have n number of

00:04:04,580 --> 00:04:09,890
GPUs we are talking about four cores one

00:04:07,100 --> 00:04:13,070
GPU eight cores for DSP so it's

00:04:09,890 --> 00:04:14,780
relatively a small platform and most of

00:04:13,070 --> 00:04:16,489
the models rami models seem to require

00:04:14,780 --> 00:04:19,040
support from operating system and

00:04:16,489 --> 00:04:20,900
compilers but some devices do not have

00:04:19,040 --> 00:04:23,320
an OS they are bare metal for example

00:04:20,900 --> 00:04:25,740
one of the research that we did was a

00:04:23,320 --> 00:04:27,569
specialized accelerator from frisk

00:04:25,740 --> 00:04:30,270
which was a bare metal at specialized

00:04:27,569 --> 00:04:31,949
accelerator platforms it had no OS so

00:04:30,270 --> 00:04:33,949
you have to dive into the hardware

00:04:31,949 --> 00:04:35,970
directly from the software stack and

00:04:33,949 --> 00:04:39,630
some of the solutions seem to be very

00:04:35,970 --> 00:04:41,789
restricted to environment of you know or

00:04:39,630 --> 00:04:43,349
course of single type homogenous

00:04:41,789 --> 00:04:44,669
environments it's becoming difficult to

00:04:43,349 --> 00:04:46,830
improve the model to support

00:04:44,669 --> 00:04:48,330
heterogeneous systems so what do we

00:04:46,830 --> 00:04:51,150
really need right that's a fundamental

00:04:48,330 --> 00:04:53,099
question so something that is not too

00:04:51,150 --> 00:04:56,520
low level that requires steep learning

00:04:53,099 --> 00:04:57,900
curve so the idea of you know porting

00:04:56,520 --> 00:04:59,669
becomes a challenge you are spending

00:04:57,900 --> 00:05:01,199
more time in learning the tool chain and

00:04:59,669 --> 00:05:03,780
not spending enough time on the

00:05:01,199 --> 00:05:05,520
algorithm itself so we want something

00:05:03,780 --> 00:05:07,259
that is lightweight because like I said

00:05:05,520 --> 00:05:09,330
embedded systems have really scarce

00:05:07,259 --> 00:05:11,009
resources something that can target

00:05:09,330 --> 00:05:12,840
heterogeneous emitted platforms and I

00:05:11,009 --> 00:05:15,270
insisted that this when I say

00:05:12,840 --> 00:05:16,289
heterogeneous it's beyond CPU GPU you

00:05:15,270 --> 00:05:19,680
have to think about a variety of

00:05:16,289 --> 00:05:21,780
platforms and TTM time to market is key

00:05:19,680 --> 00:05:23,880
when you come to embedded devices and

00:05:21,780 --> 00:05:26,310
last but not the least like I said we

00:05:23,880 --> 00:05:29,150
need industry standards so for this

00:05:26,310 --> 00:05:31,229
particular work we have used OpenMP and

00:05:29,150 --> 00:05:33,419
another industry standard called

00:05:31,229 --> 00:05:37,409
multi-core association and shot MCA

00:05:33,419 --> 00:05:39,900
api's which is primarily designed for

00:05:37,409 --> 00:05:42,120
embedded systems it's low it's it's low

00:05:39,900 --> 00:05:44,340
level it's lightweight but it's catering

00:05:42,120 --> 00:05:47,729
to scarce resources which is the main

00:05:44,340 --> 00:05:49,560
thing in embedded emitted systems so we

00:05:47,729 --> 00:05:52,259
are going to look into both models and

00:05:49,560 --> 00:05:54,180
we are going to look into what could we

00:05:52,259 --> 00:05:56,520
do with both industry standards and

00:05:54,180 --> 00:05:58,860
create a software stack and be able to

00:05:56,520 --> 00:06:00,870
target the list of you know emitter

00:05:58,860 --> 00:06:04,169
platforms we have so that's the work

00:06:00,870 --> 00:06:05,940
that we were doing so that might look

00:06:04,169 --> 00:06:08,099
quite familiar which is a simple

00:06:05,940 --> 00:06:10,710
implementation of you know how an open

00:06:08,099 --> 00:06:12,659
MP co really looks like you take a take

00:06:10,710 --> 00:06:14,400
a piece of code you insert directives

00:06:12,659 --> 00:06:16,830
into the code depending on the kind of

00:06:14,400 --> 00:06:19,110
directives you want your you want your

00:06:16,830 --> 00:06:20,819
in your code and then you have the

00:06:19,110 --> 00:06:22,949
compiler transformation and you have a

00:06:20,819 --> 00:06:26,310
like a runtime library which is key to

00:06:22,949 --> 00:06:29,099
schedule load across a different course

00:06:26,310 --> 00:06:31,050
different platform architectures and you

00:06:29,099 --> 00:06:32,580
have synchronization going on when you

00:06:31,050 --> 00:06:34,949
have different threats you know trying

00:06:32,580 --> 00:06:36,389
to throw us on different cores and you

00:06:34,949 --> 00:06:37,430
want the threads to come together draw a

00:06:36,389 --> 00:06:39,950
synchronization point

00:06:37,430 --> 00:06:41,780
and this was important to note because

00:06:39,950 --> 00:06:43,760
each compiler has customized runtime

00:06:41,780 --> 00:06:45,680
support quality of the runtime system as

00:06:43,760 --> 00:06:49,040
major impact on performance every one

00:06:45,680 --> 00:06:52,040
time has its own every tool has its own

00:06:49,040 --> 00:06:53,900
runtime support so that's a small gist

00:06:52,040 --> 00:06:55,580
of what really happens under the hood

00:06:53,900 --> 00:06:58,000
with a given and OpenMP implementation

00:06:55,580 --> 00:07:01,190
so this is a slide that I borrowed from

00:06:58,000 --> 00:07:03,710
the openmp ARB of course which is it

00:07:01,190 --> 00:07:05,780
shows kind of history of openmp the key

00:07:03,710 --> 00:07:07,670
thing to look here is started off in

00:07:05,780 --> 00:07:12,050
1997 and look at where we are right

00:07:07,670 --> 00:07:14,150
we're in 2016 and we are the question

00:07:12,050 --> 00:07:15,470
mark here would say is Phi dot 0 which

00:07:14,150 --> 00:07:18,620
was what we mentioned in the openmp

00:07:15,470 --> 00:07:20,240
above yesterday release in 2018 but it

00:07:18,620 --> 00:07:22,520
started off as a small group and then it

00:07:20,240 --> 00:07:24,170
expanded and we are in multiple members

00:07:22,520 --> 00:07:28,550
right now different vendors different

00:07:24,170 --> 00:07:30,500
universities 30 right 99 third year so

00:07:28,550 --> 00:07:32,540
it's grown and it's still growing which

00:07:30,500 --> 00:07:34,160
is fascinating and the amount of

00:07:32,540 --> 00:07:35,840
features that has been added to the

00:07:34,160 --> 00:07:37,580
standard it's it's continuously

00:07:35,840 --> 00:07:40,310
improving depending on the application

00:07:37,580 --> 00:07:41,780
needs depending on user needs and you

00:07:40,310 --> 00:07:44,360
know how things are really implemented

00:07:41,780 --> 00:07:48,170
under under the hood so that's a chart

00:07:44,360 --> 00:07:50,750
with the different graphics in there so

00:07:48,170 --> 00:07:53,480
that's a short gist of what open MPs now

00:07:50,750 --> 00:07:55,310
moving on to multi-core Association this

00:07:53,480 --> 00:07:58,220
is also an industry standard which is

00:07:55,310 --> 00:07:59,750
reduced to reduce complexity involved in

00:07:58,220 --> 00:08:02,330
writing software for multi-core chips

00:07:59,750 --> 00:08:04,760
that's the key idea of this ultimate

00:08:02,330 --> 00:08:07,340
Association industry standard they have

00:08:04,760 --> 00:08:09,430
a set of AP ice for communication for

00:08:07,340 --> 00:08:12,380
resource management for task management

00:08:09,430 --> 00:08:14,330
so communication API is communicating

00:08:12,380 --> 00:08:15,980
between cores of different types

00:08:14,330 --> 00:08:18,050
belonging to this you know a single

00:08:15,980 --> 00:08:19,640
board for example you have cpus cpus and

00:08:18,050 --> 00:08:22,250
you are trying to communicate between

00:08:19,640 --> 00:08:24,670
those two different types of devices and

00:08:22,250 --> 00:08:27,410
you have resources you are trying to

00:08:24,670 --> 00:08:29,540
target say I have four ports so I need

00:08:27,410 --> 00:08:31,010
to choose say three three three cores to

00:08:29,540 --> 00:08:32,690
keep it busy there is another for that

00:08:31,010 --> 00:08:35,030
is idle so you're trying to really pin

00:08:32,690 --> 00:08:37,130
down the resources you want to use this

00:08:35,030 --> 00:08:39,080
work focuses on task management whose

00:08:37,130 --> 00:08:41,540
philosophy is very similar to OpenMP

00:08:39,080 --> 00:08:43,220
tasks it's just that it it's meant to be

00:08:41,540 --> 00:08:45,830
lighter weight catering to embedded

00:08:43,220 --> 00:08:47,810
systems you have you have the literal

00:08:45,830 --> 00:08:48,950
concepts of you know what you really do

00:08:47,810 --> 00:08:50,300
with task so do you compose an

00:08:48,950 --> 00:08:50,750
application a bunch of tasks you

00:08:50,300 --> 00:08:52,790
schedule

00:08:50,750 --> 00:08:56,600
them you prioritize them and all sorts

00:08:52,790 --> 00:08:59,120
of things so that's a graphic of the

00:08:56,600 --> 00:09:01,490
task management API which is what this

00:08:59,120 --> 00:09:03,290
work particularly focuses there's a

00:09:01,490 --> 00:09:04,610
standardized API for task parallel

00:09:03,290 --> 00:09:07,190
programming on a wide range of hardware

00:09:04,610 --> 00:09:09,170
architectures and developed and driven

00:09:07,190 --> 00:09:12,590
by several you know marketing market

00:09:09,170 --> 00:09:14,840
leading companies so emre p kappa shim

00:09:12,590 --> 00:09:17,720
and open it open am p so these are the

00:09:14,840 --> 00:09:19,910
this is a dist of the members that have

00:09:17,720 --> 00:09:21,530
contributed to m tapi but MCA itself has

00:09:19,910 --> 00:09:24,080
you know many other members if you go

00:09:21,530 --> 00:09:26,750
take a look at the site so we have tasks

00:09:24,080 --> 00:09:29,570
and we have a queue API concept and this

00:09:26,750 --> 00:09:32,270
picture denotes that it's an API created

00:09:29,570 --> 00:09:34,460
to manage systems that can have shared

00:09:32,270 --> 00:09:36,140
memory distributed memory and different

00:09:34,460 --> 00:09:38,720
instruction set architectures even bare

00:09:36,140 --> 00:09:40,550
metal so and so forth so what did we

00:09:38,720 --> 00:09:42,590
want to do right we learned MCA

00:09:40,550 --> 00:09:44,150
multi-core Association api's we learned

00:09:42,590 --> 00:09:46,400
openmp we don't both of them are

00:09:44,150 --> 00:09:48,560
industry standards and we have this cool

00:09:46,400 --> 00:09:49,670
powerful embedded system bored sitting

00:09:48,560 --> 00:09:52,340
in front of us so how do you really

00:09:49,670 --> 00:09:54,200
program that so the idea we came up with

00:09:52,340 --> 00:09:57,410
is hey so why don't we create a

00:09:54,200 --> 00:09:59,690
translation between OpenMP and MCA in

00:09:57,410 --> 00:10:01,880
the sense that we use MCA as the layer

00:09:59,690 --> 00:10:04,339
just above the hardware and we abstract

00:10:01,880 --> 00:10:05,750
it even further above with the openmp

00:10:04,339 --> 00:10:08,089
layer on top because we want to keep

00:10:05,750 --> 00:10:10,880
programming simple so the idea here was

00:10:08,089 --> 00:10:14,030
start off with a parallel do you have an

00:10:10,880 --> 00:10:15,650
application you program in OpenMP the

00:10:14,030 --> 00:10:17,360
programmer doesn't need to worry about

00:10:15,650 --> 00:10:19,400
the gory details of what's happening

00:10:17,360 --> 00:10:22,339
under the hood while the translation

00:10:19,400 --> 00:10:24,740
takes care of which is the openmp to MCA

00:10:22,339 --> 00:10:26,750
translation and then you have you know

00:10:24,740 --> 00:10:28,400
an architecture with different types of

00:10:26,750 --> 00:10:30,320
oasis for example we have not really

00:10:28,400 --> 00:10:32,270
tested on each of this but the

00:10:30,320 --> 00:10:34,040
fundamental concept of this API is to be

00:10:32,270 --> 00:10:35,030
able to target operating system

00:10:34,040 --> 00:10:37,610
different operating systems and even

00:10:35,030 --> 00:10:39,080
bare metal and then finally you have the

00:10:37,610 --> 00:10:43,130
heterogeneous system as your underlying

00:10:39,080 --> 00:10:46,490
hardware so that was the principle of

00:10:43,130 --> 00:10:49,100
this entire work so what did we do so

00:10:46,490 --> 00:10:52,280
this is a concept of em copy where the

00:10:49,100 --> 00:10:54,260
idea is lie just like we have OpenMP

00:10:52,280 --> 00:10:56,390
tasks in EM papeete is considered to be

00:10:54,260 --> 00:10:58,450
jobs this is different vocabulary here

00:10:56,390 --> 00:11:01,880
but conceptually it is the similar and

00:10:58,450 --> 00:11:02,899
these jobs every job can be associated

00:11:01,880 --> 00:11:04,939
with different actions

00:11:02,899 --> 00:11:06,860
now you can have an action that is going

00:11:04,939 --> 00:11:08,660
into CPU you can have another action

00:11:06,860 --> 00:11:13,430
that goes to GPU you have another action

00:11:08,660 --> 00:11:16,730
that goes to DSP so that's the essence

00:11:13,430 --> 00:11:18,589
of you know task management API and you

00:11:16,730 --> 00:11:20,119
have different applications and stuff so

00:11:18,589 --> 00:11:22,759
this particular picture is borrowed from

00:11:20,119 --> 00:11:25,009
Siemens with whom we worked with on this

00:11:22,759 --> 00:11:28,339
project siemens germany where they have

00:11:25,009 --> 00:11:31,579
plugins for cuda and opencl where the

00:11:28,339 --> 00:11:34,249
idea is to trance be able to use opencl

00:11:31,579 --> 00:11:36,290
from em poppy be able to use cuda plugin

00:11:34,249 --> 00:11:37,550
from mt p so you have an open MP and

00:11:36,290 --> 00:11:39,769
it's actually there's different

00:11:37,550 --> 00:11:41,660
translation layers under until you reach

00:11:39,769 --> 00:11:44,899
the hardware and there is of course

00:11:41,660 --> 00:11:46,550
scheduling going on so the MTP

00:11:44,899 --> 00:11:49,009
implementations right now we have to

00:11:46,550 --> 00:11:51,129
open source implementations one is the

00:11:49,009 --> 00:11:53,480
EMB square which is quite popular

00:11:51,129 --> 00:11:55,179
embedded multi-core building blocks a MV

00:11:53,480 --> 00:11:58,220
square open source library this is

00:11:55,179 --> 00:11:59,779
maintained by mca Marcus Levi who's been

00:11:58,220 --> 00:12:00,920
maintaining it and Siemens contributes

00:11:59,779 --> 00:12:03,559
and I think all other companies

00:12:00,920 --> 00:12:05,679
contribute to it and another

00:12:03,559 --> 00:12:08,059
implementation that we came up with is

00:12:05,679 --> 00:12:09,259
most of the work is done in adversity of

00:12:08,059 --> 00:12:11,499
Houston that I move to University of

00:12:09,259 --> 00:12:13,819
Delaware so we had both logos up there

00:12:11,499 --> 00:12:15,889
so it's basically to open source

00:12:13,819 --> 00:12:18,379
implementations that has github links

00:12:15,889 --> 00:12:20,629
here this paper is also published at the

00:12:18,379 --> 00:12:22,490
rome workshop co-located with europe are

00:12:20,629 --> 00:12:24,949
so there's a proceedings if you want to

00:12:22,490 --> 00:12:28,370
go look at the work more in detail and

00:12:24,949 --> 00:12:30,019
look at the github as well so we had our

00:12:28,370 --> 00:12:31,550
implementation and then you are constant

00:12:30,019 --> 00:12:33,319
dialogue with siemens germany we were

00:12:31,550 --> 00:12:35,420
like ok so you have your open source we

00:12:33,319 --> 00:12:37,490
have our open source let's talk so it is

00:12:35,420 --> 00:12:39,499
not a competitive effort but it's more

00:12:37,490 --> 00:12:41,120
like you have your you know

00:12:39,499 --> 00:12:42,860
implementations targeting platforms I

00:12:41,120 --> 00:12:44,899
have wine limitations targeting my set

00:12:42,860 --> 00:12:46,519
of platforms so let's find the common

00:12:44,899 --> 00:12:48,709
ground and let's build this open source

00:12:46,519 --> 00:12:50,089
architecture and you know because what

00:12:48,709 --> 00:12:52,129
our focus is targeting embittered

00:12:50,089 --> 00:12:54,079
platforms so that was the idea behind

00:12:52,129 --> 00:12:56,120
working with Siemens and they were

00:12:54,079 --> 00:12:57,949
awesome collaborators so we wrote a

00:12:56,120 --> 00:13:00,829
paper together and put the work on

00:12:57,949 --> 00:13:02,990
github and everything so that's a

00:13:00,829 --> 00:13:05,299
picture on scheduling this is an artwork

00:13:02,990 --> 00:13:08,179
that cement state which we have borrowed

00:13:05,299 --> 00:13:10,100
for this talk where you have those the

00:13:08,179 --> 00:13:11,990
literal scheduling mechanism for example

00:13:10,100 --> 00:13:14,569
you have those different jobs or tasks

00:13:11,990 --> 00:13:15,500
in MTP language and you have a scheduler

00:13:14,569 --> 00:13:18,110
that is going to

00:13:15,500 --> 00:13:19,970
define where the jobs will be scheduled

00:13:18,110 --> 00:13:22,790
is a TPU friendly so GPU friendly it is

00:13:19,970 --> 00:13:25,220
a DSP friendly and then you also have

00:13:22,790 --> 00:13:28,250
qap ice which is going to use which is

00:13:25,220 --> 00:13:29,900
going to do the usual job of what a cute

00:13:28,250 --> 00:13:32,720
early does right yes you allocate all

00:13:29,900 --> 00:13:34,580
you Q all the tasks and there's a local

00:13:32,720 --> 00:13:36,560
q there is a global Q and all those kind

00:13:34,580 --> 00:13:41,120
of concepts there is a work ceiling

00:13:36,560 --> 00:13:43,070
scheduling concept so the reason for the

00:13:41,120 --> 00:13:44,630
main reason for this kind of scheduling

00:13:43,070 --> 00:13:46,100
mechanism is you have a variety of

00:13:44,630 --> 00:13:48,380
platforms you're talking about type

00:13:46,100 --> 00:13:50,630
course of different types so how do you

00:13:48,380 --> 00:13:54,530
really channel your work to these course

00:13:50,630 --> 00:13:56,710
of different types and the uhm tap the

00:13:54,530 --> 00:13:58,850
implementation went to step forward

00:13:56,710 --> 00:14:01,430
trying to give us kind of cover the

00:13:58,850 --> 00:14:04,160
scheduling implementation for internal

00:14:01,430 --> 00:14:06,290
communication as well siemens did an

00:14:04,160 --> 00:14:08,900
intra node communication so they had

00:14:06,290 --> 00:14:10,370
they were doing very good in the intro

00:14:08,900 --> 00:14:12,260
node we had that extra stretch doing an

00:14:10,370 --> 00:14:14,030
intern or its tasks so that was another

00:14:12,260 --> 00:14:16,790
reason why the both the work came

00:14:14,030 --> 00:14:19,310
together pretty well so this is a list

00:14:16,790 --> 00:14:21,020
of it's not entirely a list though yeah

00:14:19,310 --> 00:14:23,060
kind of it is so obviously the first

00:14:21,020 --> 00:14:25,790
target platform of x86 but beyond that

00:14:23,060 --> 00:14:29,960
we targeted an Nvidia Jetson th TK 1

00:14:25,790 --> 00:14:32,750
which is got the quad core arm processor

00:14:29,960 --> 00:14:37,010
and a Kepler GPU with all its Jetson

00:14:32,750 --> 00:14:38,420
toolkit and the pulmonary work for limbo

00:14:37,010 --> 00:14:41,570
is done on a Power Architecture from

00:14:38,420 --> 00:14:42,890
freescale and the pattern matching

00:14:41,570 --> 00:14:45,080
engine which is a specialized

00:14:42,890 --> 00:14:48,020
architecture of the power board is the

00:14:45,080 --> 00:14:50,810
one that was bare metal no OS so that's

00:14:48,020 --> 00:14:52,640
where we tried to you know evaluate our

00:14:50,810 --> 00:14:55,339
work on a system that had no operating

00:14:52,640 --> 00:14:57,440
system and as list of benchmark ports

00:14:55,339 --> 00:14:59,900
that we used where the conventional ones

00:14:57,440 --> 00:15:03,230
r edenia bots is Barcelona's tasking

00:14:59,900 --> 00:15:05,710
test quotes and then so that's seeming

00:15:03,230 --> 00:15:08,540
them tap use their implementation and

00:15:05,710 --> 00:15:10,100
yeah we also looked into the GCC OpenMP

00:15:08,540 --> 00:15:13,760
for evaluation purposes so that's our

00:15:10,100 --> 00:15:17,510
test engine the first set of results

00:15:13,760 --> 00:15:19,880
showcase the evaluation of both the

00:15:17,510 --> 00:15:23,150
implementations both uhm copy as well as

00:15:19,880 --> 00:15:26,180
siemens and happy so we ported this on

00:15:23,150 --> 00:15:28,420
only armed force arm plus GPU cores on

00:15:26,180 --> 00:15:30,279
the Tegrity k1 platform

00:15:28,420 --> 00:15:32,110
only GPU course so you are trying to

00:15:30,279 --> 00:15:34,750
find out okay where does it do the best

00:15:32,110 --> 00:15:36,339
you know under what circumstances so all

00:15:34,750 --> 00:15:38,620
those little circles that you see for

00:15:36,339 --> 00:15:40,810
example the blue ones on both the graphs

00:15:38,620 --> 00:15:43,300
we have the left one as you wait right

00:15:40,810 --> 00:15:46,589
one is siemens your x-axis is execution

00:15:43,300 --> 00:15:49,930
time the y-axis is sizes of mattresses

00:15:46,589 --> 00:15:53,050
the blue mark shows the MTP arm is

00:15:49,930 --> 00:15:55,300
faster than the MTP GPU because of the

00:15:53,050 --> 00:15:58,510
overhead due to copying of data which is

00:15:55,300 --> 00:16:01,360
kind of a known issue when you're really

00:15:58,510 --> 00:16:04,889
moving data around the red block is the

00:16:01,360 --> 00:16:07,060
MTP GPU which means empathy action is

00:16:04,889 --> 00:16:09,699
oriented towards channel towards a GPU

00:16:07,060 --> 00:16:12,010
course which is faster than the MTP and

00:16:09,699 --> 00:16:16,240
arm GPU for large matrices because of

00:16:12,010 --> 00:16:19,420
load and balances then MTP arm GPU we

00:16:16,240 --> 00:16:22,240
optimized the MTP arm GPU implementation

00:16:19,420 --> 00:16:23,800
and that was fast because of managing

00:16:22,240 --> 00:16:25,149
asynchronous transfers and you know

00:16:23,800 --> 00:16:26,529
variable block sizes so there was a lot

00:16:25,149 --> 00:16:29,589
of playing around with different block

00:16:26,529 --> 00:16:31,810
sizes which gave us the the the green

00:16:29,589 --> 00:16:33,699
result over here which was oh this was

00:16:31,810 --> 00:16:35,529
actually cement optimization we did not

00:16:33,699 --> 00:16:37,300
uhm type II did not do this but Siemens

00:16:35,529 --> 00:16:41,019
did some more optimizations say God they

00:16:37,300 --> 00:16:43,019
have some more in their chart so this

00:16:41,019 --> 00:16:46,209
was evaluating the MTP implementations

00:16:43,019 --> 00:16:47,860
the next step or I wouldn't call it next

00:16:46,209 --> 00:16:49,959
step these both the steps for a kind of

00:16:47,860 --> 00:16:52,449
interleaving each other so the

00:16:49,959 --> 00:16:55,570
translation right openmp RTL to em happy

00:16:52,449 --> 00:16:57,790
so we have fused open you H for this and

00:16:55,570 --> 00:17:00,130
open you H intermediate representations

00:16:57,790 --> 00:17:02,019
have five levels from high-level very

00:17:00,130 --> 00:17:03,519
high level to low level so we looked

00:17:02,019 --> 00:17:05,260
into the different transformation layers

00:17:03,519 --> 00:17:07,660
on some intermediate representations to

00:17:05,260 --> 00:17:09,850
see which one can give us really good

00:17:07,660 --> 00:17:12,640
information to bank on in order to

00:17:09,850 --> 00:17:16,089
translate om poppy function calls and in

00:17:12,640 --> 00:17:18,699
turn to them tapi jobs and actions such

00:17:16,089 --> 00:17:21,790
that we relieve the scheduling part to

00:17:18,699 --> 00:17:24,400
the MTP layer runtime function because

00:17:21,790 --> 00:17:26,049
the MTP is meant to schedule better

00:17:24,400 --> 00:17:28,059
schedule on the embedded systems because

00:17:26,049 --> 00:17:29,980
it is lightweight so the translation is

00:17:28,059 --> 00:17:31,720
basically the abstraction is the key and

00:17:29,980 --> 00:17:35,049
then the impiety takes over and does the

00:17:31,720 --> 00:17:37,270
scheduling so that's a small graphic

00:17:35,049 --> 00:17:42,220
there so if you look at the numbers here

00:17:37,270 --> 00:17:46,270
this is a chart that shows OpenMP GCC

00:17:42,220 --> 00:17:48,250
versus openmp MTP the beauty was even

00:17:46,270 --> 00:17:50,650
after doing the translation we did not

00:17:48,250 --> 00:17:52,210
infer overhead there was little to

00:17:50,650 --> 00:17:54,610
negligible overhead and we tested that

00:17:52,210 --> 00:17:57,070
using ipcc's benchmarks and several

00:17:54,610 --> 00:18:00,549
other micro benchmarks so if you look at

00:17:57,070 --> 00:18:04,750
the numbers here the purple one is the

00:18:00,549 --> 00:18:06,340
GCC and the openmp MTP RTL is there were

00:18:04,750 --> 00:18:08,020
smaller boxes with number of threat

00:18:06,340 --> 00:18:11,320
scaling up an execution time on your

00:18:08,020 --> 00:18:14,830
y-axis there are some numbers that this

00:18:11,320 --> 00:18:17,230
is really looking good so we we did dive

00:18:14,830 --> 00:18:19,330
deeper into it but we haven't really put

00:18:17,230 --> 00:18:22,419
our fingers on it as to why exactly this

00:18:19,330 --> 00:18:25,570
is doing better I believe it is because

00:18:22,419 --> 00:18:28,510
the MTP scheduling is catered to those

00:18:25,570 --> 00:18:29,740
scarce resources and that could be one

00:18:28,510 --> 00:18:32,740
of the reasons why it is doing even

00:18:29,740 --> 00:18:35,500
better than the GCC openmp but bottom

00:18:32,740 --> 00:18:38,320
line is the opening p.m. type II

00:18:35,500 --> 00:18:41,020
translation is not doing worse than an

00:18:38,320 --> 00:18:42,820
open source GCC implementation it did

00:18:41,020 --> 00:18:46,179
not incur overhead and we're able to

00:18:42,820 --> 00:18:47,710
target more than just x86 64 so with

00:18:46,179 --> 00:18:49,750
this layer we are able to target

00:18:47,710 --> 00:18:51,159
multiple platforms and the amount of

00:18:49,750 --> 00:18:52,929
time taken is also reduced because

00:18:51,159 --> 00:18:54,370
nobody is programming in MCA they're

00:18:52,929 --> 00:18:58,570
still programming in OpenMP which is a

00:18:54,370 --> 00:19:01,419
higher level so the takeaway and summary

00:18:58,570 --> 00:19:03,669
of this work was obviously industry

00:19:01,419 --> 00:19:06,760
standards are the way to go I guess we

00:19:03,669 --> 00:19:10,000
all would agree to that and openmp MCA

00:19:06,760 --> 00:19:11,770
did not incur a little to no overhead

00:19:10,000 --> 00:19:14,100
and we targeted Power Architecture

00:19:11,770 --> 00:19:17,429
specialized architects accelerators

00:19:14,100 --> 00:19:19,570
Tegra platform which is arm plus GPU and

00:19:17,429 --> 00:19:20,919
less learning curve because you need to

00:19:19,570 --> 00:19:23,380
learn OpenMP you don't need to learn em

00:19:20,919 --> 00:19:25,390
happy there and the ability to maintain

00:19:23,380 --> 00:19:27,669
single code base is another key thing

00:19:25,390 --> 00:19:29,440
because if you are writing an open MP

00:19:27,669 --> 00:19:32,890
but you are targeting varying platforms

00:19:29,440 --> 00:19:35,740
so things get a lot lot more easier so

00:19:32,890 --> 00:19:37,860
yes that's what I have so I'll take

00:19:35,740 --> 00:19:37,860
questions

00:19:39,419 --> 00:19:45,419
it's part of his PhD works believe you

00:19:42,520 --> 00:19:45,419
won't have questions

00:19:52,490 --> 00:19:59,690
the testing was the evaluation platform

00:19:56,080 --> 00:20:01,460
yes sure so the question is what kind of

00:19:59,690 --> 00:20:04,370
evaluation platform we use to test his

00:20:01,460 --> 00:20:06,920
work we started off with x86 to test our

00:20:04,370 --> 00:20:09,470
basic implementation into a core simple

00:20:06,920 --> 00:20:13,429
CPU cores the next platform was

00:20:09,470 --> 00:20:15,140
freescale power architecture for course

00:20:13,429 --> 00:20:17,059
eight course I don't remember I think

00:20:15,140 --> 00:20:19,400
eight cores eight core Power

00:20:17,059 --> 00:20:21,410
Architecture from freescale and that had

00:20:19,400 --> 00:20:23,390
this pattern matching engine p.m. e

00:20:21,410 --> 00:20:26,270
specialized accelerator which was bare

00:20:23,390 --> 00:20:28,280
metal no OS that work actually took her

00:20:26,270 --> 00:20:29,870
six to eight months because we had to

00:20:28,280 --> 00:20:31,250
figure out what it takes to lower the

00:20:29,870 --> 00:20:34,580
toolchain to something that doesn't have

00:20:31,250 --> 00:20:37,309
an OS and it's an engineering work so if

00:20:34,580 --> 00:20:39,410
we publish it's not nothing novel their

00:20:37,309 --> 00:20:40,700
rights a bit out of curiosity we just

00:20:39,410 --> 00:20:42,770
went ahead with the work and we did we

00:20:40,700 --> 00:20:45,910
did get something running and the last

00:20:42,770 --> 00:20:49,520
one was the Tegrity k 1 which is the

00:20:45,910 --> 00:20:52,730
nvidia's tegra TK one platform which is

00:20:49,520 --> 00:20:55,400
armed and a kepler GPU this was before

00:20:52,730 --> 00:20:59,050
tx1 was released which was last november

00:20:55,400 --> 00:20:59,050
so we haven't tested on tx1

00:21:05,470 --> 00:21:12,630
II yeah yeah we haven't tested on a DSP

00:21:13,679 --> 00:21:26,710
yes yes yes freescale Power Architecture

00:21:18,789 --> 00:21:31,360
and the specialized accelerator yes well

00:21:26,710 --> 00:21:32,980
the arm + GPU course of different types

00:21:31,360 --> 00:21:36,970
i'm defining them as h wishing his

00:21:32,980 --> 00:21:38,860
platform so the pme was the specialized

00:21:36,970 --> 00:21:40,360
isolated with power course so that

00:21:38,860 --> 00:21:42,539
becomes a kind of a heterogeneous

00:21:40,360 --> 00:21:46,179
platform in a course of different types

00:21:42,539 --> 00:21:49,299
this was fun this almost like four and a

00:21:46,179 --> 00:21:52,600
half five years work with one masters

00:21:49,299 --> 00:21:54,190
and two PhDs out of this work so it's a

00:21:52,600 --> 00:22:04,140
twist taxing but i think we had some

00:21:54,190 --> 00:22:09,549
results to show at the end of it yes yes

00:22:04,140 --> 00:22:12,000
yeah be yeah i mean the question is real

00:22:09,549 --> 00:22:15,000
world application why do you really use

00:22:12,000 --> 00:22:15,000
yes

00:22:26,060 --> 00:22:31,940
right I think the deep learning and the

00:22:29,900 --> 00:22:34,040
tasking that this could be if i convert

00:22:31,940 --> 00:22:36,110
this into something like an auto tuning

00:22:34,040 --> 00:22:37,850
if you like like deep learning is a

00:22:36,110 --> 00:22:40,430
learning method right and then if I use

00:22:37,850 --> 00:22:47,300
that and come up with a nice model and

00:22:40,430 --> 00:22:48,980
use to them happy yes so it's learning

00:22:47,300 --> 00:22:51,590
and then there is real implementation so

00:22:48,980 --> 00:22:53,630
learning is offline and the M tapis work

00:22:51,590 --> 00:23:00,820
is online so I'm not wasting time

00:22:53,630 --> 00:23:00,820
processing data yes yes yes

00:23:02,350 --> 00:23:10,160
bioinformatics J so right right plus I

00:23:08,480 --> 00:23:11,720
think its security concerned that you go

00:23:10,160 --> 00:23:14,270
to hospitals they would like to have

00:23:11,720 --> 00:23:17,510
something which is in house so we are

00:23:14,270 --> 00:23:19,430
using a tx1 on deep learning deep

00:23:17,510 --> 00:23:22,010
learning tx1 board on an image

00:23:19,430 --> 00:23:24,380
construction but not genetics per se but

00:23:22,010 --> 00:23:26,450
the idea is exactly security purposes

00:23:24,380 --> 00:23:28,520
keep everything in house don't move it

00:23:26,450 --> 00:23:30,500
away from the building which I guess the

00:23:28,520 --> 00:23:32,690
hospitals will really appreciate so

00:23:30,500 --> 00:23:35,770
there are various applications that you

00:23:32,690 --> 00:23:35,770

YouTube URL: https://www.youtube.com/watch?v=-hjSEAN_5H0


