Title: Writing Performance-portable OpenMP 4.5
Publication date: 2016-11-21
Playlist: SC16 OpenMP Booth Talks
Description: 
	Matt Martineau, University of Bristol
Captions: 
	00:00:00,000 --> 00:00:04,799
okay so I'm going to be talking to you

00:00:02,280 --> 00:00:08,760
about our experiences trying to write

00:00:04,799 --> 00:00:14,309
performance portable OpenMP 4.5 and for

00:00:08,760 --> 00:00:15,929
as well yeah okay so one of the first

00:00:14,309 --> 00:00:19,410
things that you will come across is that

00:00:15,929 --> 00:00:23,070
there are varying levels of parallelism

00:00:19,410 --> 00:00:26,250
exposed by OpenMP and they don't

00:00:23,070 --> 00:00:27,720
necessarily immediately map directly to

00:00:26,250 --> 00:00:29,039
the architecture in any particular way

00:00:27,720 --> 00:00:40,290
there's a few different ways that you

00:00:29,039 --> 00:00:44,420
can actually achieve this oh shut it

00:00:40,290 --> 00:00:50,100
like here is that good sad better yeah

00:00:44,420 --> 00:00:51,750
lift it up high okay I better that's

00:00:50,100 --> 00:00:56,460
that yeah you can hear me now okay I

00:00:51,750 --> 00:00:58,699
hope yes so yeah great and so the way

00:00:56,460 --> 00:01:00,870
that the way that the actual

00:00:58,699 --> 00:01:03,120
specification maps onto the architecture

00:01:00,870 --> 00:01:05,900
there's a there's a few opportunities

00:01:03,120 --> 00:01:08,130
for people to actually interpret the

00:01:05,900 --> 00:01:09,180
parallelism in a different way and

00:01:08,130 --> 00:01:12,150
that's one of the things that we first

00:01:09,180 --> 00:01:16,520
recognized okay so you'll see here the

00:01:12,150 --> 00:01:19,170
concept of a open mp4 target can have

00:01:16,520 --> 00:01:21,540
teams it can have threads and it can

00:01:19,170 --> 00:01:23,520
also have simdi lanes so this is kind of

00:01:21,540 --> 00:01:26,130
this is three immediate levels of

00:01:23,520 --> 00:01:28,189
parallelism and if you actually look at

00:01:26,130 --> 00:01:32,670
the way that you might naturally program

00:01:28,189 --> 00:01:36,450
CUDA so an NVIDIA GPU or you might

00:01:32,670 --> 00:01:38,610
target a cpu with simdi then actually

00:01:36,450 --> 00:01:41,250
you you really only consider when you're

00:01:38,610 --> 00:01:43,530
programming two levels of parallel is

00:01:41,250 --> 00:01:45,990
aminos so this kind of opens up an

00:01:43,530 --> 00:01:48,420
opportunity for person divergence

00:01:45,990 --> 00:01:49,950
between the implementation so I'm going

00:01:48,420 --> 00:01:53,700
to sort of deep dive into how all the

00:01:49,950 --> 00:01:55,829
compilers have looked at this or how all

00:01:53,700 --> 00:01:57,930
the compilers have looked at this

00:01:55,829 --> 00:02:01,710
problem and they've interpreted it

00:01:57,930 --> 00:02:04,020
differently so an Intel was obviously

00:02:01,710 --> 00:02:07,020
the first to actually provide an

00:02:04,020 --> 00:02:11,129
implementation they supported open mp4

00:02:07,020 --> 00:02:13,800
for their Intel Xeon Phi and actually

00:02:11,129 --> 00:02:17,460
their their initial interpretation was

00:02:13,800 --> 00:02:20,220
it didn't fully take on with the with

00:02:17,460 --> 00:02:23,520
the specification you you really had the

00:02:20,220 --> 00:02:27,750
opportunity to ignore the majority of

00:02:23,520 --> 00:02:31,380
the new features that were added the new

00:02:27,750 --> 00:02:33,990
device construct so in particular teams

00:02:31,380 --> 00:02:37,650
in distribute weren't actually necessary

00:02:33,990 --> 00:02:41,190
okay so implicitly teams are generated

00:02:37,650 --> 00:02:43,710
but you could actually just add a pragma

00:02:41,190 --> 00:02:45,510
OMP target outside a parallel for and

00:02:43,710 --> 00:02:48,630
you would end up with a code that would

00:02:45,510 --> 00:02:51,330
run and perform well and you could use

00:02:48,630 --> 00:02:53,640
simdi to vectorize as well and and this

00:02:51,330 --> 00:02:57,230
the problem with this is that this

00:02:53,640 --> 00:03:01,620
didn't really immediately buy into the

00:02:57,230 --> 00:03:03,360
concept of prescribing the parallel ISM

00:03:01,620 --> 00:03:06,180
in the way that you would for other

00:03:03,360 --> 00:03:09,030
devices so we reach a stage now where

00:03:06,180 --> 00:03:11,790
people probably have written codes that

00:03:09,030 --> 00:03:13,620
maybe have used this approach and it

00:03:11,790 --> 00:03:16,020
kind of leads to some difficulties as

00:03:13,620 --> 00:03:20,550
you start moving to those compilers that

00:03:16,020 --> 00:03:24,780
do require you to have that so the clang

00:03:20,550 --> 00:03:26,820
compiler is an opening openmp 4.5

00:03:24,780 --> 00:03:28,380
feature complete implementation Oh

00:03:26,820 --> 00:03:32,040
pretty much reach complete

00:03:28,380 --> 00:03:34,800
implementation and it has been partly

00:03:32,040 --> 00:03:36,540
developed by Intel partly developed by

00:03:34,800 --> 00:03:40,620
IBM it's been a number of people that

00:03:36,540 --> 00:03:42,180
have fed into the you can see here there

00:03:40,620 --> 00:03:44,610
is there is an old version of it in this

00:03:42,180 --> 00:03:45,780
we sort of do touch on that because it's

00:03:44,610 --> 00:03:48,360
interesting from the point of view of

00:03:45,780 --> 00:03:50,459
interpretation but if you want to see

00:03:48,360 --> 00:03:55,739
the new version that's available at the

00:03:50,459 --> 00:03:57,870
URL there so the way that the clang

00:03:55,739 --> 00:04:00,120
compiler this is that this is the alpha

00:03:57,870 --> 00:04:02,370
version the way that the clang alpha

00:04:00,120 --> 00:04:05,700
version actually would allow you to

00:04:02,370 --> 00:04:09,320
paralyze was by using for instance

00:04:05,700 --> 00:04:12,330
pragma OMP targeting us distribute to

00:04:09,320 --> 00:04:15,360
generate your team's distribute the

00:04:12,330 --> 00:04:18,600
outer loop to all of the master threats

00:04:15,360 --> 00:04:20,519
and you could then actually deter work

00:04:18,600 --> 00:04:23,220
share that inner loop with a parallel

00:04:20,519 --> 00:04:24,750
for the thing was you had to include

00:04:23,220 --> 00:04:26,910
schedule static one when you were

00:04:24,750 --> 00:04:29,550
targeting a GPU

00:04:26,910 --> 00:04:31,770
the thing with this is that it helps

00:04:29,550 --> 00:04:33,630
with coalescence right but it is

00:04:31,770 --> 00:04:34,920
slightly problematic because that's not

00:04:33,630 --> 00:04:39,960
necessarily what you'd want to do if you

00:04:34,920 --> 00:04:44,840
were targeting a CPU so yeah the teams

00:04:39,960 --> 00:04:49,020
were creating a you know an individual

00:04:44,840 --> 00:04:52,530
team / SMX in this case so they actually

00:04:49,020 --> 00:04:54,600
bound it to only be you know 14 teams in

00:04:52,530 --> 00:04:56,790
the case of a k20 X for instance the

00:04:54,600 --> 00:05:00,240
distribute then just chunked and

00:04:56,790 --> 00:05:02,670
distributed the parallel for work shares

00:05:00,240 --> 00:05:06,240
and the show schedule static one was for

00:05:02,670 --> 00:05:09,630
coalescence this is one interpretation

00:05:06,240 --> 00:05:10,830
of it I'll show you that this is good

00:05:09,630 --> 00:05:12,150
because you can see already that this is

00:05:10,830 --> 00:05:16,650
different from how you might have

00:05:12,150 --> 00:05:19,770
programmed for the Intel compiler okay

00:05:16,650 --> 00:05:21,990
so a slight thing that has happened is

00:05:19,770 --> 00:05:23,550
that inversion four-point-oh from

00:05:21,990 --> 00:05:26,250
version four point oh to version 4.5

00:05:23,550 --> 00:05:29,480
there's been a change actually to do

00:05:26,250 --> 00:05:33,630
with the default sharing it's moved from

00:05:29,480 --> 00:05:35,460
implicitly mapping to from to implicitly

00:05:33,630 --> 00:05:37,200
mapping first private and the

00:05:35,460 --> 00:05:39,330
implication of this is that actually a

00:05:37,200 --> 00:05:42,060
lot lots of the time it was possible

00:05:39,330 --> 00:05:44,460
previously to perform a small

00:05:42,060 --> 00:05:45,960
optimization and say okay well I'm going

00:05:44,460 --> 00:05:48,570
to map to because you didn't necessarily

00:05:45,960 --> 00:05:50,520
want scalar variables to come back from

00:05:48,570 --> 00:05:52,200
the device it's very unlikely that

00:05:50,520 --> 00:05:54,150
you'll be changing them so you save a

00:05:52,200 --> 00:05:56,100
memory copy but one of the implications

00:05:54,150 --> 00:05:58,470
of this is that actually going from from

00:05:56,100 --> 00:06:00,780
codes are written in four point oh 24.5

00:05:58,470 --> 00:06:03,060
this inhibits the ability for it to

00:06:00,780 --> 00:06:06,480
optimize it and pass it through as for

00:06:03,060 --> 00:06:08,610
instance to an nvidia GPU as a kernel

00:06:06,480 --> 00:06:10,919
argument which is a much faster lower

00:06:08,610 --> 00:06:12,510
cost option so it's just one of the kind

00:06:10,919 --> 00:06:15,000
of gotchas that we experience while we

00:06:12,510 --> 00:06:16,740
were trying to write the clothes the

00:06:15,000 --> 00:06:20,700
next one I'm going to talk about is cry

00:06:16,740 --> 00:06:23,940
so their implementation was pretty early

00:06:20,700 --> 00:06:26,880
in terms of targeting the GPU and they

00:06:23,940 --> 00:06:29,430
have their they're not 4.5 feature

00:06:26,880 --> 00:06:31,520
complete but they have a pretty much

00:06:29,430 --> 00:06:35,620
full support for the device constructs

00:06:31,520 --> 00:06:39,160
as of about six months ago

00:06:35,620 --> 00:06:42,310
their implementation is we've shown to

00:06:39,160 --> 00:06:44,560
be quite quite performant they achieve

00:06:42,310 --> 00:06:47,199
generally on our applications within say

00:06:44,560 --> 00:06:48,550
twenty percent performance penalty in a

00:06:47,199 --> 00:06:51,699
lot of cases which which was very

00:06:48,550 --> 00:06:52,930
impressive to us so they they took a

00:06:51,699 --> 00:06:58,870
slightly different approach and that

00:06:52,930 --> 00:07:00,370
approach is instead of actually they the

00:06:58,870 --> 00:07:01,870
number of teams and threads that they

00:07:00,370 --> 00:07:05,500
pick by default is obviously different

00:07:01,870 --> 00:07:06,910
so it's 128 teams of 128 threads they

00:07:05,500 --> 00:07:08,430
distribute in the same way because

00:07:06,910 --> 00:07:11,080
there's really only one way to do that

00:07:08,430 --> 00:07:14,500
but the next thing that they do is

00:07:11,080 --> 00:07:17,440
instead of actually a parallel for being

00:07:14,500 --> 00:07:22,000
the signal that you need to work share a

00:07:17,440 --> 00:07:25,270
particular loop they consider the block

00:07:22,000 --> 00:07:27,820
the block to be an wide vector unit and

00:07:25,270 --> 00:07:30,280
the threads almost the CUDA threads

00:07:27,820 --> 00:07:32,440
almost map onto individual simdi lanes

00:07:30,280 --> 00:07:34,150
okay so what you can do is you can say

00:07:32,440 --> 00:07:36,520
okay I don't actually need the parallel

00:07:34,150 --> 00:07:38,050
for I can just have a sim d or in many

00:07:36,520 --> 00:07:41,320
cases you don't even need to put the sim

00:07:38,050 --> 00:07:43,660
d because it will auto vectorize so this

00:07:41,320 --> 00:07:46,570
is another divergence from the way that

00:07:43,660 --> 00:07:52,330
you were programming so we've seen a few

00:07:46,570 --> 00:07:54,990
different cases already right so the GCC

00:07:52,330 --> 00:07:58,090
compiler is the most recent one to add

00:07:54,990 --> 00:08:00,070
and they have had support for some time

00:07:58,090 --> 00:08:02,470
for the for the front end to the passing

00:08:00,070 --> 00:08:04,630
but actually in terms of the back end

00:08:02,470 --> 00:08:08,080
implementation that that's more recent

00:08:04,630 --> 00:08:12,760
we looked at targeting an AMD apu

00:08:08,080 --> 00:08:14,830
because it's HSA supporting device they

00:08:12,760 --> 00:08:17,590
have at the moment only got support for

00:08:14,830 --> 00:08:19,870
the full combined contract OMP targeting

00:08:17,590 --> 00:08:23,199
distribute parallel for but they do not

00:08:19,870 --> 00:08:25,780
support any clauses so you can't change

00:08:23,199 --> 00:08:28,150
the schedule you can't change the number

00:08:25,780 --> 00:08:30,520
of teams you can't use cindy for

00:08:28,150 --> 00:08:32,770
instance when you're targeting an AMD

00:08:30,520 --> 00:08:35,200
device so this hadn't quite heavy weight

00:08:32,770 --> 00:08:37,060
limitations I wouldn't necessarily

00:08:35,200 --> 00:08:38,620
consider that a major problem from a

00:08:37,060 --> 00:08:39,909
portability point of view from them at

00:08:38,620 --> 00:08:41,709
the moment because obviously it's very

00:08:39,909 --> 00:08:43,630
early and that will be extended and

00:08:41,709 --> 00:08:47,320
changed but it's interesting

00:08:43,630 --> 00:08:49,270
nevertheless so yeah that really what we

00:08:47,320 --> 00:08:52,900
came to was that there are many diff

00:08:49,270 --> 00:08:54,880
ways to write the same thing and and

00:08:52,900 --> 00:08:56,560
actually we can all think of lots of

00:08:54,880 --> 00:08:58,060
fringe cases lots of edge cases and

00:08:56,560 --> 00:08:59,350
there will be in the majority of

00:08:58,060 --> 00:09:01,810
applications is going to be one or two

00:08:59,350 --> 00:09:03,700
kernels that need to be parallelized in

00:09:01,810 --> 00:09:05,860
a particular way but you can see here

00:09:03,700 --> 00:09:08,320
even just your standard Sayer like a

00:09:05,860 --> 00:09:09,520
vector at right this this is this is

00:09:08,320 --> 00:09:11,620
four different ways that you could

00:09:09,520 --> 00:09:13,000
approach that and all we want really the

00:09:11,620 --> 00:09:16,870
purpose of what we wanted to say today

00:09:13,000 --> 00:09:18,580
was please don't do that okay we think

00:09:16,870 --> 00:09:21,430
that all all programmers who are

00:09:18,580 --> 00:09:23,860
approaching applications with OpenMP 4.5

00:09:21,430 --> 00:09:25,900
should be considering the impact of what

00:09:23,860 --> 00:09:28,920
directives they choose very carefully so

00:09:25,900 --> 00:09:32,770
is to improve portability going forwards

00:09:28,920 --> 00:09:34,660
so the suggestion that we have is that

00:09:32,770 --> 00:09:37,060
you really want to use the most

00:09:34,660 --> 00:09:38,950
expressive statement that you can and

00:09:37,060 --> 00:09:43,510
prescribe all of the parallel ISM that

00:09:38,950 --> 00:09:44,890
exists within your loop so in openmp 4.5

00:09:43,510 --> 00:09:47,260
the way that you do that is to use the

00:09:44,890 --> 00:09:48,730
full combined construct as soon as you

00:09:47,260 --> 00:09:51,420
start separating these on two different

00:09:48,730 --> 00:09:54,010
lines compilers will struggle to

00:09:51,420 --> 00:09:55,390
optimize it because they have to

00:09:54,010 --> 00:09:57,610
consider that there might be serial

00:09:55,390 --> 00:10:00,040
regions they have to they have to do all

00:09:57,610 --> 00:10:02,560
sorts of different things to handle edge

00:10:00,040 --> 00:10:04,600
cases but if you put if you put a this

00:10:02,560 --> 00:10:07,000
directive this combined directive in

00:10:04,600 --> 00:10:09,190
this way you are saying my loop is

00:10:07,000 --> 00:10:11,410
completely independent you can go ahead

00:10:09,190 --> 00:10:13,120
and paralyze it in an efficient way and

00:10:11,410 --> 00:10:14,940
and pretty much all of the

00:10:13,120 --> 00:10:20,860
implementations do do that at the moment

00:10:14,940 --> 00:10:22,570
the simdi statement is it it's it's a

00:10:20,860 --> 00:10:24,700
good it's a good one to put on there

00:10:22,570 --> 00:10:26,710
because lots of them require it but you

00:10:24,700 --> 00:10:29,380
might at this moment in time experience

00:10:26,710 --> 00:10:32,260
cases where some of them struggle if you

00:10:29,380 --> 00:10:33,340
do at the 70 directive so it's bit take

00:10:32,260 --> 00:10:34,450
it with a pinch of salt you have to

00:10:33,340 --> 00:10:40,000
maybe have to consider that one

00:10:34,450 --> 00:10:41,800
independent yes so the taking this a

00:10:40,000 --> 00:10:44,260
little bit further the really the crux

00:10:41,800 --> 00:10:46,930
of this is if you don't have to put

00:10:44,260 --> 00:10:49,600
distribute parallel for any of that do

00:10:46,930 --> 00:10:51,940
put it anyway it you know because if you

00:10:49,600 --> 00:10:54,010
don't it might not run on other

00:10:51,940 --> 00:10:59,900
compilers or it might run but it might

00:10:54,010 --> 00:11:05,420
perform very very poorly so yeah

00:10:59,900 --> 00:11:07,070
sorry ok so the yeah we do have some

00:11:05,420 --> 00:11:09,950
difficulty with the collapse and

00:11:07,070 --> 00:11:11,270
schedule clauses at the moment it the

00:11:09,950 --> 00:11:13,520
schedule Clause has kind of become a

00:11:11,270 --> 00:11:15,230
null point we don't expect it to come

00:11:13,520 --> 00:11:16,700
about again with the Alpha clang

00:11:15,230 --> 00:11:19,580
compiler you had to add it for

00:11:16,700 --> 00:11:21,710
performance but you know as I said for

00:11:19,580 --> 00:11:24,050
going to a cpu that probably isn't the

00:11:21,710 --> 00:11:25,790
thing you want to do so in general we

00:11:24,050 --> 00:11:27,320
want the compilers to pick the sensible

00:11:25,790 --> 00:11:29,450
default for their architecture that

00:11:27,320 --> 00:11:30,970
they're targeting right so we expect

00:11:29,450 --> 00:11:32,810
that to be something that happens anyway

00:11:30,970 --> 00:11:35,330
collapsing you kind of have to be

00:11:32,810 --> 00:11:37,070
careful because you might damage

00:11:35,330 --> 00:11:38,810
performance on certain architectures but

00:11:37,070 --> 00:11:40,940
that is something that you probably you

00:11:38,810 --> 00:11:41,870
probably can tune or you know there's

00:11:40,940 --> 00:11:45,950
lots of different ways you can approach

00:11:41,870 --> 00:11:47,540
that and lastly the number of teams and

00:11:45,950 --> 00:11:50,360
threads is of really it's a really

00:11:47,540 --> 00:11:52,370
challenging thing it's very important to

00:11:50,360 --> 00:11:54,680
performance there's not necessarily

00:11:52,370 --> 00:11:56,960
general rules it's hard to come up with

00:11:54,680 --> 00:11:58,310
really good defaults we've pushed for

00:11:56,960 --> 00:12:01,700
good defaults in the compilers that

00:11:58,310 --> 00:12:03,800
where we could we expect that it's

00:12:01,700 --> 00:12:05,900
something that you would prefer not to

00:12:03,800 --> 00:12:08,630
change if you can from a performance

00:12:05,900 --> 00:12:11,930
portability perspective because if you

00:12:08,630 --> 00:12:14,150
if you shrink or expand the number of

00:12:11,930 --> 00:12:15,620
teams and threads you have it can lead

00:12:14,150 --> 00:12:18,140
to performance degradation elsewhere

00:12:15,620 --> 00:12:19,730
yeah but it's difficult to give a hard

00:12:18,140 --> 00:12:22,130
and fast rule on that would be we would

00:12:19,730 --> 00:12:24,770
prefer to you know rely on the compiler

00:12:22,130 --> 00:12:25,910
in most cases and finally I'm just going

00:12:24,770 --> 00:12:28,100
to show you some numbers just because

00:12:25,910 --> 00:12:30,890
it's it's kind of interesting to see you

00:12:28,100 --> 00:12:33,940
know the output of what we actually did

00:12:30,890 --> 00:12:35,960
so we tried running on multiple devices

00:12:33,940 --> 00:12:38,030
some of the directives have to be

00:12:35,960 --> 00:12:40,460
slightly different because obviously if

00:12:38,030 --> 00:12:41,570
you looked at GCC it's not necessarily

00:12:40,460 --> 00:12:43,730
immediately compatible with the

00:12:41,570 --> 00:12:45,260
directives of the others but for the

00:12:43,730 --> 00:12:47,360
most part we try to keep them as close

00:12:45,260 --> 00:12:49,310
to each other as possible and and the

00:12:47,360 --> 00:12:51,170
performance in general is is pretty good

00:12:49,310 --> 00:12:53,420
you know across the board we find that

00:12:51,170 --> 00:12:55,760
with we find certainly with Cray that

00:12:53,420 --> 00:12:57,710
we've been able to achieve very good

00:12:55,760 --> 00:13:00,140
performance and more recently with some

00:12:57,710 --> 00:13:01,610
optimizations in klang the the newest

00:13:00,140 --> 00:13:04,760
version of clankers is performing very

00:13:01,610 --> 00:13:06,200
well as well yeah and so yeah that

00:13:04,760 --> 00:13:10,850
that's

00:13:06,200 --> 00:13:15,490
to me thank you yeah great can we take

00:13:10,850 --> 00:13:15,490
questions yeah great okay oh yeah please

00:13:18,889 --> 00:13:24,470
or the notes online yes this that's it

00:13:22,699 --> 00:13:25,939
that's a very good question I can make I

00:13:24,470 --> 00:13:34,339
can certainly make this available yeah

00:13:25,939 --> 00:13:37,149
yeah definitely yeah right right exit

00:13:34,339 --> 00:13:37,149
yeah

00:13:42,080 --> 00:13:48,880
absolutely yeah

00:13:46,040 --> 00:13:48,880

YouTube URL: https://www.youtube.com/watch?v=vIpv9HQEa84


