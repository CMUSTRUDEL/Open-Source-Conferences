Title: Driving OpenMP Development Through Application Needs - Oscar Hernandez 0 SC19
Publication date: 2019-11-25
Playlist: SC19 OpenMP Booth Talks
Description: 
	20 November 2019 SC19 Denver
Captions: 
	00:00:00,650 --> 00:00:09,830
right so let me know when to start okay

00:00:07,189 --> 00:00:12,389
so today we're gonna be talking about

00:00:09,830 --> 00:00:14,670
how we do how are we going to be using

00:00:12,389 --> 00:00:17,250
opening peel different applications and

00:00:14,670 --> 00:00:19,350
what they are they needs and how we're

00:00:17,250 --> 00:00:22,199
basically addressing them in the context

00:00:19,350 --> 00:00:24,210
of the ECP exascale computing project

00:00:22,199 --> 00:00:28,230
and how we are working with the rest of

00:00:24,210 --> 00:00:32,130
the community on doing this we like

00:00:28,230 --> 00:00:35,250
OpenMP because it's a de facto standard

00:00:32,130 --> 00:00:36,780
for many vendors that will allow us to

00:00:35,250 --> 00:00:39,600
run applications on multiple

00:00:36,780 --> 00:00:43,350
architectures we're interested in

00:00:39,600 --> 00:00:46,739
working in a standard that could run on

00:00:43,350 --> 00:00:48,750
multiple leadership class computing

00:00:46,739 --> 00:00:52,559
systems especially in the Department of

00:00:48,750 --> 00:00:54,629
Energy and as a bigger standard is a

00:00:52,559 --> 00:00:57,420
great a place to work but also has

00:00:54,629 --> 00:01:00,480
challenges and we need to understand

00:00:57,420 --> 00:01:02,579
what's the best approach to express the

00:01:00,480 --> 00:01:08,340
needs of applications in the context of

00:01:02,579 --> 00:01:12,090
this to the community as we as we see in

00:01:08,340 --> 00:01:15,990
the top 500 list for the last several

00:01:12,090 --> 00:01:18,270
years we have seen that architectures

00:01:15,990 --> 00:01:22,770
are significantly changing in the

00:01:18,270 --> 00:01:26,369
context of the top 500 list where we

00:01:22,770 --> 00:01:29,430
initially had applications that were

00:01:26,369 --> 00:01:32,939
targeting in the early 2000s multi-core

00:01:29,430 --> 00:01:34,369
architectures but as we evolved lately

00:01:32,939 --> 00:01:37,020
we're moving more in the context of

00:01:34,369 --> 00:01:39,810
accelerate or type of architectures and

00:01:37,020 --> 00:01:41,460
as we see that evolution we also see

00:01:39,810 --> 00:01:46,170
that opening peak has been cannot

00:01:41,460 --> 00:01:49,890
respond to that sort of trend and in the

00:01:46,170 --> 00:01:53,280
leg test a a the likely result is like

00:01:49,890 --> 00:01:57,240
we have a support for for example for a

00:01:53,280 --> 00:02:01,049
stellarators and and initially in the

00:01:57,240 --> 00:02:03,149
earliest a in the mid-2000s we have

00:02:01,049 --> 00:02:05,549
support for tasking and then before that

00:02:03,149 --> 00:02:06,990
we support for a multi-course so this

00:02:05,549 --> 00:02:07,990
has been project that has evolved over

00:02:06,990 --> 00:02:10,270
time and it's

00:02:07,990 --> 00:02:13,480
in response of the type of architectures

00:02:10,270 --> 00:02:16,420
that we are supporting for us in

00:02:13,480 --> 00:02:18,569
Oakridge we definitely like the idea of

00:02:16,420 --> 00:02:21,130
having a portable programming model and

00:02:18,569 --> 00:02:24,519
we see that trend also in the now

00:02:21,130 --> 00:02:26,769
architectures in 2008 we have Jaguar who

00:02:24,519 --> 00:02:29,050
was a multi-core architecture and then

00:02:26,769 --> 00:02:32,380
in 2012 we have a computer called tight

00:02:29,050 --> 00:02:34,209
and that was based on GPUs and there was

00:02:32,380 --> 00:02:36,069
a big gap for us and we wanted a

00:02:34,209 --> 00:02:39,370
directive based programming model to

00:02:36,069 --> 00:02:43,420
target that machine and then in 2017 we

00:02:39,370 --> 00:02:45,549
had a summit and 2021 now we have

00:02:43,420 --> 00:02:49,750
frontier and all of these architectures

00:02:45,549 --> 00:02:51,610
are based on accelerators or GPUs and we

00:02:49,750 --> 00:02:55,480
want to use a open impede to program

00:02:51,610 --> 00:02:57,459
them one of the challenges is that a lot

00:02:55,480 --> 00:03:00,250
of the codes running in these machines

00:02:57,459 --> 00:03:04,090
come from different programming models

00:03:00,250 --> 00:03:07,840
and the traditional ones are Fortran and

00:03:04,090 --> 00:03:10,750
C++ and we see that for example the

00:03:07,840 --> 00:03:12,250
ratio between the what is basically

00:03:10,750 --> 00:03:14,250
generating the cycles of these machines

00:03:12,250 --> 00:03:17,620
come from these two like based languages

00:03:14,250 --> 00:03:19,209
and we need to make sure that these

00:03:17,620 --> 00:03:22,510
languages can also run in the

00:03:19,209 --> 00:03:25,420
accelerator or in the in the Indian or

00:03:22,510 --> 00:03:27,130
also and also in the multi-course so

00:03:25,420 --> 00:03:30,820
when we look at the directive based

00:03:27,130 --> 00:03:32,470
programming approaches we see that we

00:03:30,820 --> 00:03:35,980
need to have something that we can

00:03:32,470 --> 00:03:39,850
easily do that especially in the context

00:03:35,980 --> 00:03:42,640
of a Fortran when we look at how we are

00:03:39,850 --> 00:03:45,549
today we are seeing that the directives

00:03:42,640 --> 00:03:49,299
is being used especially in the in the

00:03:45,549 --> 00:03:51,880
past but we are basically in the growth

00:03:49,299 --> 00:03:54,430
is in the context of a directive that

00:03:51,880 --> 00:03:57,400
has target a specific GPUs but we see

00:03:54,430 --> 00:04:00,130
also a growth in OpenMP and as we are

00:03:57,400 --> 00:04:02,139
making those directors more available we

00:04:00,130 --> 00:04:04,239
are also be generating more cycles in

00:04:02,139 --> 00:04:05,500
the machine that basically are doing

00:04:04,239 --> 00:04:06,940
that and going in the direction of a

00:04:05,500 --> 00:04:12,700
standard that controls on multiple

00:04:06,940 --> 00:04:14,530
architectures so why do we need openmp

00:04:12,700 --> 00:04:17,199
basically the applications needed

00:04:14,530 --> 00:04:18,940
because they need a programming layer to

00:04:17,199 --> 00:04:20,620
program the different types of memories

00:04:18,940 --> 00:04:24,639
available

00:04:20,620 --> 00:04:27,030
in the in the note they neither in an

00:04:24,639 --> 00:04:31,080
API for dealing with data movement a

00:04:27,030 --> 00:04:33,100
data abstractions a basically a

00:04:31,080 --> 00:04:34,690
directives that will help to work a

00:04:33,100 --> 00:04:37,240
straight the work between the host and

00:04:34,690 --> 00:04:39,340
the devices and how they can coordinate

00:04:37,240 --> 00:04:41,740
and distributive work between them and

00:04:39,340 --> 00:04:44,650
we have several applications that have

00:04:41,740 --> 00:04:48,100
been looking at using

00:04:44,650 --> 00:04:51,850
openmp offload and for accelerators and

00:04:48,100 --> 00:04:54,669
there are Asturias m/l QCD candle Kim C

00:04:51,850 --> 00:04:58,510
pack and NW chem and all of them have

00:04:54,669 --> 00:05:00,700
expressed their need to use a openmp but

00:04:58,510 --> 00:05:04,140
on the other hand we have a lot of codes

00:05:00,700 --> 00:05:07,900
that are written in other languages a

00:05:04,140 --> 00:05:10,330
especially Coco's and Rajah which

00:05:07,900 --> 00:05:12,220
basically use openmp can use open up in

00:05:10,330 --> 00:05:14,919
a different way

00:05:12,220 --> 00:05:17,050
currently open MP can be used directly

00:05:14,919 --> 00:05:19,090
in the applications like a Fortran code

00:05:17,050 --> 00:05:21,580
that you basically insert those

00:05:19,090 --> 00:05:23,410
directives there or it can be using

00:05:21,580 --> 00:05:26,590
directly through a framework like cocoa

00:05:23,410 --> 00:05:28,750
sir Raja and that will generate help us

00:05:26,590 --> 00:05:31,720
to generate openmp and and run it on

00:05:28,750 --> 00:05:33,460
multiple architectures on the other hand

00:05:31,720 --> 00:05:37,300
we have other standards that also

00:05:33,460 --> 00:05:41,470
emerging especially in the context of

00:05:37,300 --> 00:05:44,320
AMD we see hip and in the context of

00:05:41,470 --> 00:05:46,330
nvidia we see CUDA and ensuring that

00:05:44,320 --> 00:05:48,510
operative interoperates with these

00:05:46,330 --> 00:05:53,310
languages is also very important and

00:05:48,510 --> 00:05:56,500
also we see the evolution of using M

00:05:53,310 --> 00:05:58,720
also as other directives so we have to

00:05:56,500 --> 00:06:00,610
to figure out how we can do this in a

00:05:58,720 --> 00:06:03,630
way that is inter interoperable and the

00:06:00,610 --> 00:06:07,930
users can use it in their applications

00:06:03,630 --> 00:06:12,280
so one of the additions in OpenMP 4.0

00:06:07,930 --> 00:06:16,030
and now in 4.5 and 5.0 is the use of the

00:06:12,280 --> 00:06:19,120
target directive where it allows us to

00:06:16,030 --> 00:06:22,780
offload computations to the devices to

00:06:19,120 --> 00:06:25,030
the GPUs and that has helped a lot in

00:06:22,780 --> 00:06:28,270
the adoption on where how we gonna be

00:06:25,030 --> 00:06:31,630
running OpenMP in these new systems we

00:06:28,270 --> 00:06:34,060
basically the idea is that we see the

00:06:31,630 --> 00:06:35,800
accelerator as a new openmp empire

00:06:34,060 --> 00:06:39,100
where we can offload data and

00:06:35,800 --> 00:06:42,310
computations and then we can later on

00:06:39,100 --> 00:06:44,410
synchronize with the with the host or

00:06:42,310 --> 00:06:46,900
the initial device that is spawning

00:06:44,410 --> 00:06:48,490
those computations and then we can use

00:06:46,900 --> 00:06:52,360
directives to coordinate the data

00:06:48,490 --> 00:06:54,030
movement and the and the work on the

00:06:52,360 --> 00:06:57,610
other hand we can also use a

00:06:54,030 --> 00:07:00,790
openmp new features that we have to

00:06:57,610 --> 00:07:03,070
program a multiple devices and also

00:07:00,790 --> 00:07:05,830
coordinate the work with the hosts by

00:07:03,070 --> 00:07:08,590
using a some other capabilities we have

00:07:05,830 --> 00:07:11,140
in opening P with tasking where we can

00:07:08,590 --> 00:07:13,420
and dependences where we can basically

00:07:11,140 --> 00:07:16,330
offload computations to the host and

00:07:13,420 --> 00:07:19,840
also to multiple devices through the use

00:07:16,330 --> 00:07:21,340
of dependences and that also helps us to

00:07:19,840 --> 00:07:23,800
work it straight some of the work that

00:07:21,340 --> 00:07:26,050
we need to torque is really a you know

00:07:23,800 --> 00:07:30,040
tame in what the applications are using

00:07:26,050 --> 00:07:32,220
on multiple devices so how those

00:07:30,040 --> 00:07:34,630
applications are used in opening P

00:07:32,220 --> 00:07:38,680
currently opening P is being used in

00:07:34,630 --> 00:07:40,450
several key applications and some of the

00:07:38,680 --> 00:07:42,430
areas that they are trying to look is a

00:07:40,450 --> 00:07:44,530
performance portable programming model

00:07:42,430 --> 00:07:46,660
so for example we have one application

00:07:44,530 --> 00:07:48,670
called QC pack that is trying to run on

00:07:46,660 --> 00:07:51,820
multi-core but also in different

00:07:48,670 --> 00:07:53,200
accelerators and using opening piece

00:07:51,820 --> 00:07:54,970
allowing them to do that

00:07:53,200 --> 00:07:58,210
but one of the challenges that they see

00:07:54,970 --> 00:08:00,400
is is there is not only the portability

00:07:58,210 --> 00:08:03,190
of the programming model but also how we

00:08:00,400 --> 00:08:06,370
can make it performance portable and it

00:08:03,190 --> 00:08:08,050
looks to me that right now the one of

00:08:06,370 --> 00:08:11,530
the challenges that we see for example

00:08:08,050 --> 00:08:13,720
in opening P 44.5 is that we do we

00:08:11,530 --> 00:08:15,760
really want to write code that's good

00:08:13,720 --> 00:08:17,350
for the multi-core or we want to write

00:08:15,760 --> 00:08:19,810
code that is good for the accelerator

00:08:17,350 --> 00:08:22,570
and how do we handle those two things

00:08:19,810 --> 00:08:25,210
and ideally we want to have like one ID

00:08:22,570 --> 00:08:28,090
one source code that can be targeted

00:08:25,210 --> 00:08:30,490
both architectures and what we see is

00:08:28,090 --> 00:08:35,110
that depending on the problem sizes that

00:08:30,490 --> 00:08:36,730
we are trying to run for example the for

00:08:35,110 --> 00:08:41,140
example the CPU version is doing a good

00:08:36,730 --> 00:08:42,730
job but but if the program size is

00:08:41,140 --> 00:08:44,890
bigger enough for example the

00:08:42,730 --> 00:08:47,040
accelerator version is better so

00:08:44,890 --> 00:08:49,709
basically the portability aspect is

00:08:47,040 --> 00:08:51,810
not only it also dependent on the

00:08:49,709 --> 00:08:53,940
problem size that we're running and also

00:08:51,810 --> 00:08:57,480
the target architectures that were using

00:08:53,940 --> 00:09:00,000
so so the other need that we have is

00:08:57,480 --> 00:09:01,620
that these ELISA applications are using

00:09:00,000 --> 00:09:05,160
libraries and they need to interoperate

00:09:01,620 --> 00:09:07,620
with openmp and we also need to have a

00:09:05,160 --> 00:09:11,399
very efficient implementations of the

00:09:07,620 --> 00:09:13,050
offload directives especially because

00:09:11,399 --> 00:09:15,089
many of these codes have very small

00:09:13,050 --> 00:09:17,430
kernels and we need to make sure that

00:09:15,089 --> 00:09:20,370
they are offloading computations to the

00:09:17,430 --> 00:09:23,069
devices quickly with little overheads

00:09:20,370 --> 00:09:25,079
and that's an important aspect that we

00:09:23,069 --> 00:09:29,300
need to to target in the context of

00:09:25,079 --> 00:09:33,120
openmp and then other challenges are

00:09:29,300 --> 00:09:35,639
issues with the tasking basically how

00:09:33,120 --> 00:09:37,199
can we do advanced asking to coordinate

00:09:35,639 --> 00:09:40,050
the work between the multi-course and a

00:09:37,199 --> 00:09:42,120
and the seller raters the other

00:09:40,050 --> 00:09:45,720
challenges that we see in openmp is that

00:09:42,120 --> 00:09:47,880
I know plication is that a lot of these

00:09:45,720 --> 00:09:51,779
applications are using a very complex

00:09:47,880 --> 00:09:53,550
data structures a they need to move back

00:09:51,779 --> 00:09:55,920
and forth from the hospital device and

00:09:53,550 --> 00:09:58,199
we need to make sure that they're those

00:09:55,920 --> 00:09:59,990
are those transfers are efficient and we

00:09:58,199 --> 00:10:03,569
can express it in the language and

00:09:59,990 --> 00:10:06,389
currently a for example we have we have

00:10:03,569 --> 00:10:08,220
analyzed applications look about how

00:10:06,389 --> 00:10:11,010
many of them are have complex data

00:10:08,220 --> 00:10:14,459
structures or a structure to have

00:10:11,010 --> 00:10:17,190
pointers or to other memories and how we

00:10:14,459 --> 00:10:20,100
am handle those back and forth between

00:10:17,190 --> 00:10:26,069
the host and the device and we call that

00:10:20,100 --> 00:10:27,810
the deep copy problem and in OpenMP 5.0

00:10:26,069 --> 00:10:30,689
we came up with a solution to do that

00:10:27,810 --> 00:10:33,540
using custom mappers where we basically

00:10:30,689 --> 00:10:36,420
can move a complete data structure like

00:10:33,540 --> 00:10:39,209
a vector of a vector from the host to

00:10:36,420 --> 00:10:42,060
the device and we can basically specify

00:10:39,209 --> 00:10:43,680
it in a way that OpenMP can understand

00:10:42,060 --> 00:10:46,380
that and take care of the data movement

00:10:43,680 --> 00:10:48,870
so this is one of the challenges that we

00:10:46,380 --> 00:10:50,670
see in when we're moving cause to

00:10:48,870 --> 00:10:52,949
oscillators is basically how can we

00:10:50,670 --> 00:10:56,220
solve the issue of data mapping between

00:10:52,949 --> 00:10:58,800
the host and devices and this is one

00:10:56,220 --> 00:11:02,010
things that we are addressing in the

00:10:58,800 --> 00:11:03,649
context of OpenMP 5.0 the other

00:11:02,010 --> 00:11:06,779
challenge is that we have this

00:11:03,649 --> 00:11:09,420
application that you know that is

00:11:06,779 --> 00:11:11,639
writing calls for the CPU but also may

00:11:09,420 --> 00:11:15,360
have a different strategy for the for

00:11:11,639 --> 00:11:17,370
the GPU and by finding different levels

00:11:15,360 --> 00:11:21,000
of parallelism in the code or performing

00:11:17,370 --> 00:11:23,309
some code transformations and and if we

00:11:21,000 --> 00:11:26,069
want to create one single code one

00:11:23,309 --> 00:11:28,610
single version of the code or or a or

00:11:26,069 --> 00:11:31,139
find a way to make this more manageable

00:11:28,610 --> 00:11:33,689
we definitely need to explore new type

00:11:31,139 --> 00:11:36,209
of directives and in openmp 5.0 we have

00:11:33,689 --> 00:11:38,459
the Metall directive that will help us

00:11:36,209 --> 00:11:40,829
to to change some of the behaviors of

00:11:38,459 --> 00:11:42,980
the of the directives and depending on

00:11:40,829 --> 00:11:46,260
the context that the cordera be invoked

00:11:42,980 --> 00:11:49,649
we also have a capability of creating

00:11:46,260 --> 00:11:51,930
variants and loop transformation

00:11:49,649 --> 00:11:53,939
capabilities also being discussed in the

00:11:51,930 --> 00:11:56,839
standard so that will help to facilitate

00:11:53,939 --> 00:12:00,000
manage the versions of the calls between

00:11:56,839 --> 00:12:04,160
multiple devices including the CPU and

00:12:00,000 --> 00:12:07,860
the GPU tuning is very important

00:12:04,160 --> 00:12:10,889
especially in the context of schedules

00:12:07,860 --> 00:12:13,649
and loop transformations for the GPU

00:12:10,889 --> 00:12:15,779
kernels or for the target carols and we

00:12:13,649 --> 00:12:17,790
need to have better mechanisms to find

00:12:15,779 --> 00:12:20,790
ways to tune the codes so that we can

00:12:17,790 --> 00:12:26,370
get the best performance possible in

00:12:20,790 --> 00:12:28,769
OpenMP 5.0 we have new capabilities that

00:12:26,370 --> 00:12:31,290
is gonna start addressing this a1 are a

00:12:28,769 --> 00:12:33,649
based on the loop directive that we had

00:12:31,290 --> 00:12:36,029
in OpenMP 5.0 where we leave the

00:12:33,649 --> 00:12:37,680
capability that to the compiler to pick

00:12:36,029 --> 00:12:40,709
was the best implementations and

00:12:37,680 --> 00:12:44,339
schedules that we need to do that and we

00:12:40,709 --> 00:12:46,980
also have the capability of adding these

00:12:44,339 --> 00:12:48,899
custom mappers a that I just talked

00:12:46,980 --> 00:12:52,610
about before and also the support for

00:12:48,899 --> 00:12:55,050
shared memory for unified memory which

00:12:52,610 --> 00:12:58,680
reduce the burden of moving costs from

00:12:55,050 --> 00:13:01,829
the from the horse to the device so this

00:12:58,680 --> 00:13:03,179
is just an example of how these

00:13:01,829 --> 00:13:05,390
directives are going to look like in the

00:13:03,179 --> 00:13:07,920
coast where we can just Express

00:13:05,390 --> 00:13:10,880
basically we initially we could have

00:13:07,920 --> 00:13:12,170
three versions of song of a

00:13:10,880 --> 00:13:14,750
of parallelism that we want to express

00:13:12,170 --> 00:13:17,900
in the loop but with the directive we

00:13:14,750 --> 00:13:19,340
will have one where we basically let the

00:13:17,900 --> 00:13:22,370
compiler pick was the best

00:13:19,340 --> 00:13:25,250
implementation and that will allow them

00:13:22,370 --> 00:13:29,390
code to be more portable and it could

00:13:25,250 --> 00:13:32,150
generate a parallelism inside a GPU or a

00:13:29,390 --> 00:13:35,150
device or it could be something eco

00:13:32,150 --> 00:13:37,700
generated task or a every Cindy type of

00:13:35,150 --> 00:13:40,880
parallelism and that's how important a

00:13:37,700 --> 00:13:44,510
future that we want to support but to do

00:13:40,880 --> 00:13:47,420
this we need very important a analysis

00:13:44,510 --> 00:13:49,960
the compiler needs to do basically these

00:13:47,420 --> 00:13:52,340
to focus on cost models where basically

00:13:49,960 --> 00:13:55,010
the compiler needs to decide was the

00:13:52,340 --> 00:13:57,290
best a strategy to do that and what type

00:13:55,010 --> 00:13:59,690
of paralyzation it needs to do to

00:13:57,290 --> 00:14:01,310
generate the efficient code and that

00:13:59,690 --> 00:14:04,820
will definitely help in the context of

00:14:01,310 --> 00:14:07,670
performance portability so one of the

00:14:04,820 --> 00:14:09,380
the what the community is really asking

00:14:07,670 --> 00:14:10,700
in the context of directives for

00:14:09,380 --> 00:14:12,260
operating piece that we have quality

00:14:10,700 --> 00:14:14,380
implementations and full standard

00:14:12,260 --> 00:14:16,520
supports across architectures

00:14:14,380 --> 00:14:20,180
interoperability with other programming

00:14:16,520 --> 00:14:22,190
models tasking is important in OpenMP to

00:14:20,180 --> 00:14:24,710
orchestrate the work across accelerators

00:14:22,190 --> 00:14:27,710
and in the network and the multi-course

00:14:24,710 --> 00:14:29,600
a consistent implementations across

00:14:27,710 --> 00:14:31,490
compilers and architectures so that all

00:14:29,600 --> 00:14:34,190
the compilers are basically translating

00:14:31,490 --> 00:14:37,280
open imp in a similar way so that we

00:14:34,190 --> 00:14:39,680
reduce the cost of a performance

00:14:37,280 --> 00:14:43,940
portability across compilers make sure

00:14:39,680 --> 00:14:46,850
that that works and then look at ec a is

00:14:43,940 --> 00:14:49,670
the transition between a openmp and open

00:14:46,850 --> 00:14:52,760
ACC in the context of having these

00:14:49,670 --> 00:14:54,670
models available in in compilers will

00:14:52,760 --> 00:14:57,620
help to move from one to the other so

00:14:54,670 --> 00:15:00,020
that the user can decide what's the best

00:14:57,620 --> 00:15:02,120
approach that they need for to support

00:15:00,020 --> 00:15:05,000
what they need in their code with idea

00:15:02,120 --> 00:15:06,080
that without opening P having more

00:15:05,000 --> 00:15:08,270
implementations in different

00:15:06,080 --> 00:15:11,750
architectures will be helpful for them

00:15:08,270 --> 00:15:14,330
in the long term so and then M the idea

00:15:11,750 --> 00:15:16,370
of having common runtime api swell when

00:15:14,330 --> 00:15:18,980
different compilers can link together

00:15:16,370 --> 00:15:20,610
and being able to interpret it's also

00:15:18,980 --> 00:15:23,160
very important and expressed

00:15:20,610 --> 00:15:25,110
by that and this is a list of things

00:15:23,160 --> 00:15:27,839
that we have identified right

00:15:25,110 --> 00:15:30,569
applications or and the use cases that

00:15:27,839 --> 00:15:33,569
they have that they need in the context

00:15:30,569 --> 00:15:36,480
of OpenMP and we kind of like working on

00:15:33,569 --> 00:15:38,730
in with the standard communities so that

00:15:36,480 --> 00:15:41,759
we address them and many of them are

00:15:38,730 --> 00:15:44,639
being addressed already by OpenMP 5.0

00:15:41,759 --> 00:15:46,850
and we are just kind of like making sure

00:15:44,639 --> 00:15:49,019
that they are implemented and a

00:15:46,850 --> 00:15:52,649
correctly in the compilers and become

00:15:49,019 --> 00:15:55,529
more available and and these are some of

00:15:52,649 --> 00:15:59,759
the things that we definitely need to

00:15:55,529 --> 00:16:01,470
continue working and supporting and and

00:15:59,759 --> 00:16:03,779
one of the things that we definitely a

00:16:01,470 --> 00:16:08,459
we still need to look forward in the

00:16:03,779 --> 00:16:11,610
future is how do we support better a the

00:16:08,459 --> 00:16:14,540
parallelism in the device especially a

00:16:11,610 --> 00:16:17,670
how do how much the we expose for the

00:16:14,540 --> 00:16:20,129
for the streams in the GPUs because

00:16:17,670 --> 00:16:21,749
currently that's a an area that is a

00:16:20,129 --> 00:16:24,509
little bit a challenging for the

00:16:21,749 --> 00:16:26,989
applications and and these are some of

00:16:24,509 --> 00:16:29,639
the lists that we wanted to do that and

00:16:26,989 --> 00:16:31,350
we're basically working with a many

00:16:29,639 --> 00:16:34,230
applications where we're basically

00:16:31,350 --> 00:16:36,299
paralyzing with openmp offload to make

00:16:34,230 --> 00:16:38,429
sure that you know we can test those

00:16:36,299 --> 00:16:40,980
ideas and make provide examples to the

00:16:38,429 --> 00:16:44,549
community so they can see how it's being

00:16:40,980 --> 00:16:47,730
used and and currently the majority of

00:16:44,549 --> 00:16:50,129
the calls have openmp in their CPUs but

00:16:47,730 --> 00:16:52,019
very few are starting to adopt it in the

00:16:50,129 --> 00:16:54,660
in the GPUs but that's kind of like

00:16:52,019 --> 00:16:57,839
growing and this is a suite that is

00:16:54,660 --> 00:17:00,419
available by the ECP project has types

00:16:57,839 --> 00:17:02,790
of implementations and in addition to

00:17:00,419 --> 00:17:06,589
that we're also working with the spec

00:17:02,790 --> 00:17:11,010
community to create benchmarks that have

00:17:06,589 --> 00:17:14,209
OpenMP of load in those benchmarks as an

00:17:11,010 --> 00:17:17,069
example of how that this can be used and

00:17:14,209 --> 00:17:20,549
and as an example of best practices for

00:17:17,069 --> 00:17:22,760
applications and with that I finished my

00:17:20,549 --> 00:17:22,760
talk

00:17:22,930 --> 00:17:26,490

YouTube URL: https://www.youtube.com/watch?v=MJ3UAo_kLDI


