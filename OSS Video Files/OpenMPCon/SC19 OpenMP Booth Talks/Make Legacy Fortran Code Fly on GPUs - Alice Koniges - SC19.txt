Title: Make Legacy Fortran Code Fly on GPUs - Alice Koniges - SC19
Publication date: 2019-11-25
Playlist: SC19 OpenMP Booth Talks
Description: 
	SC19 November 19, 2019 Denver
Slides: https://www.openmp.org/wp-content/uploads/SC19-Koniges-Legacy-Fortran.pdf
Captions: 
	00:00:00,030 --> 00:00:06,210
all right so I'm gonna tell you about

00:00:02,070 --> 00:00:10,139
how to get your CPU code flying as we

00:00:06,210 --> 00:00:17,130
say in the hypersonics world on to the

00:00:10,139 --> 00:00:20,699
GPUs so this was a team effort we went

00:00:17,130 --> 00:00:22,140
to one of the hackathon sponsored by by

00:00:20,699 --> 00:00:24,090
the Department of Energy and they were

00:00:22,140 --> 00:00:27,660
kind enough to let up a broad group of

00:00:24,090 --> 00:00:31,170
people go it and this is the team so we

00:00:27,660 --> 00:00:34,760
had about six core team members as well

00:00:31,170 --> 00:00:38,640
as two assigned mentors from from the

00:00:34,760 --> 00:00:43,200
hackathon providers which was mostly

00:00:38,640 --> 00:00:48,120
from nurse two experts from Nvidia as

00:00:43,200 --> 00:00:52,320
well so here's a picture and I really

00:00:48,120 --> 00:00:54,239
want to make clear that the hackathon

00:00:52,320 --> 00:00:57,059
experience for this was really good

00:00:54,239 --> 00:00:58,920
nobody ever at the table really sat

00:00:57,059 --> 00:01:01,320
empty not doing anything at any time

00:00:58,920 --> 00:01:03,090
there was always depending on levels

00:01:01,320 --> 00:01:05,549
there was always something for somebody

00:01:03,090 --> 00:01:08,490
to do and having an entire room of

00:01:05,549 --> 00:01:11,310
people to go to and sitting there for

00:01:08,490 --> 00:01:13,140
five days straight really made a huge

00:01:11,310 --> 00:01:15,600
difference in our ability to port this

00:01:13,140 --> 00:01:19,439
code really really really quickly in my

00:01:15,600 --> 00:01:21,479
opinion so what we did in the hackathon

00:01:19,439 --> 00:01:24,450
we studied the multi zone now Sparrow

00:01:21,479 --> 00:01:25,890
benchmark SPM Z and one of the

00:01:24,450 --> 00:01:28,560
interesting things about it was this

00:01:25,890 --> 00:01:31,110
code is an old code written in Fortran

00:01:28,560 --> 00:01:32,729
you know 20-some odd years old maybe a

00:01:31,110 --> 00:01:35,280
little bit of the openmp has been

00:01:32,729 --> 00:01:41,729
optimized over the years but this is an

00:01:35,280 --> 00:01:44,040
old code and that's quite representative

00:01:41,729 --> 00:01:47,280
of a lot of what we call legacy

00:01:44,040 --> 00:01:50,310
applications these days it's a it's a

00:01:47,280 --> 00:01:52,320
mini app but it is really used in CFD

00:01:50,310 --> 00:01:54,960
applications at least the same parallel

00:01:52,320 --> 00:01:57,509
patterns the same kinds of data transfer

00:01:54,960 --> 00:02:02,880
a lot of the operations are used in

00:01:57,509 --> 00:02:04,469
current modern CFD codes the first thing

00:02:02,880 --> 00:02:06,570
we did the very first day was we

00:02:04,469 --> 00:02:09,060
modified the OpenMP directives and

00:02:06,570 --> 00:02:11,069
before five o'clock on the first day we

00:02:09,060 --> 00:02:12,970
had our cpu only code running on the

00:02:11,069 --> 00:02:14,500
GPUs and

00:02:12,970 --> 00:02:16,990
a Fortran code so that was pretty

00:02:14,500 --> 00:02:20,290
incredible to me just to have that done

00:02:16,990 --> 00:02:22,090
in less than eight hours and so then the

00:02:20,290 --> 00:02:24,550
majority the performance from the

00:02:22,090 --> 00:02:26,560
remaining four days was spent on

00:02:24,550 --> 00:02:27,970
optimization we worked on two different

00:02:26,560 --> 00:02:30,850
systems I'll show you the system

00:02:27,970 --> 00:02:33,040
architecture there was an IBM system at

00:02:30,850 --> 00:02:35,500
Oakridge and a crisis team at Berkeley

00:02:33,040 --> 00:02:39,790
and both of those systems are powered

00:02:35,500 --> 00:02:41,800
with a V 100 accelerators we discovered

00:02:39,790 --> 00:02:44,140
a lot of differences in the Cray and IBM

00:02:41,800 --> 00:02:49,240
compilers a lot of that is because the

00:02:44,140 --> 00:02:52,270
standard is maturing and in general we

00:02:49,240 --> 00:02:55,750
found out the performance of the GPUs

00:02:52,270 --> 00:02:58,330
relative the CPUs was a factor in how

00:02:55,750 --> 00:03:01,270
the arrays were were managed but but

00:02:58,330 --> 00:03:03,700
I'll go into that in some more detail so

00:03:01,270 --> 00:03:06,820
here just to show you this is the

00:03:03,700 --> 00:03:08,950
original code we started with higher is

00:03:06,820 --> 00:03:11,260
better these are different class sizes

00:03:08,950 --> 00:03:13,000
so Class D is the biggest benchmark we

00:03:11,260 --> 00:03:16,930
worked with which is even a small

00:03:13,000 --> 00:03:21,100
benchmark compared to real CFD codes but

00:03:16,930 --> 00:03:27,250
you can see the speed-up that we got on

00:03:21,100 --> 00:03:29,680
the GPU version after optimization yeah

00:03:27,250 --> 00:03:31,959
and we really haven't fully optimized

00:03:29,680 --> 00:03:36,760
the code yet this was you know basically

00:03:31,959 --> 00:03:41,470
five days plus a little extra effort so

00:03:36,760 --> 00:03:43,690
the benchmark description here like I

00:03:41,470 --> 00:03:46,750
said it's SPM Z which is a scaler

00:03:43,690 --> 00:03:48,970
penta-diagonal solver and it's the multi

00:03:46,750 --> 00:03:52,780
zone solver which means it also includes

00:03:48,970 --> 00:03:55,480
MPI so it has MPI for Interzone

00:03:52,780 --> 00:03:57,250
communication and the shared multi-core

00:03:55,480 --> 00:04:00,610
parallelization that's what we started

00:03:57,250 --> 00:04:02,650
with it's an open source code which made

00:04:00,610 --> 00:04:08,170
it very easy for us to work on this

00:04:02,650 --> 00:04:10,090
source without any restrictions and like

00:04:08,170 --> 00:04:12,310
I said the parallel patterns in this

00:04:10,090 --> 00:04:16,359
code are representative a lot of modern

00:04:12,310 --> 00:04:20,590
CFD codes because it was a Fortran

00:04:16,359 --> 00:04:22,599
source that's important because one of

00:04:20,590 --> 00:04:24,700
the reasons for doing this was to kind

00:04:22,599 --> 00:04:28,470
of pressure the vendors and the compiler

00:04:24,700 --> 00:04:28,470
writers to continue to support for

00:04:30,110 --> 00:04:35,870
so what did we do basically the openmp

00:04:33,860 --> 00:04:39,010
directives were changed to having

00:04:35,870 --> 00:04:42,530
offload extensions for the accelerator

00:04:39,010 --> 00:04:44,480
Colonels within the zones we did some

00:04:42,530 --> 00:04:47,360
reorganization of the data structure

00:04:44,480 --> 00:04:49,160
I'll show you that we had to do some

00:04:47,360 --> 00:04:52,580
things for the memory management on the

00:04:49,160 --> 00:04:57,650
on the GPU because transfer of data the

00:04:52,580 --> 00:05:00,230
GPU is always hard and we exploited

00:04:57,650 --> 00:05:02,300
various kinds of zone level parallelism

00:05:00,230 --> 00:05:07,490
and even introduce some asynchrony in

00:05:02,300 --> 00:05:09,770
the way the GPU was programmed here's

00:05:07,490 --> 00:05:12,470
the system at Oakridge it's basically a

00:05:09,770 --> 00:05:16,340
mini version of Summit that's more of an

00:05:12,470 --> 00:05:20,750
open training system and so it had six

00:05:16,340 --> 00:05:25,360
NVIDIA GPUs and IBM power nine and that

00:05:20,750 --> 00:05:28,940
was on the node and it has the NV link

00:05:25,360 --> 00:05:31,430
capable of fast data transfers and

00:05:28,940 --> 00:05:33,290
anyway here's the architecture diagram

00:05:31,430 --> 00:05:35,410
for Oak Ridge and we use two different

00:05:33,290 --> 00:05:38,570
systems it was a little hard to do both

00:05:35,410 --> 00:05:43,100
systems but we we got it to work on both

00:05:38,570 --> 00:05:46,640
systems we also use the new Cray system

00:05:43,100 --> 00:05:51,229
at Berkeley Lab here again there's eight

00:05:46,640 --> 00:05:55,690
Nvidia v100 GPS and it's got skylake

00:05:51,229 --> 00:05:55,690
cpus so those were our two test systems

00:05:57,310 --> 00:06:03,650
SPF deep like I've said already supports

00:06:01,490 --> 00:06:05,960
on the host it supports distributed

00:06:03,650 --> 00:06:09,320
shared memory parallel ism with an open

00:06:05,960 --> 00:06:12,260
MP so that's what we started with

00:06:09,320 --> 00:06:14,210
it has 3,000 or so lines of code in the

00:06:12,260 --> 00:06:16,040
total code and as the previous speaker

00:06:14,210 --> 00:06:19,270
mentioned of course you know you want to

00:06:16,040 --> 00:06:22,390
you want to see where where most of the

00:06:19,270 --> 00:06:25,520
most of the time is spent in order to

00:06:22,390 --> 00:06:28,850
concentrate your time there and so we

00:06:25,520 --> 00:06:31,010
concentrated our time on the core CFD

00:06:28,850 --> 00:06:34,130
solver of the mini app with it which has

00:06:31,010 --> 00:06:36,320
1,800 lines of code and the small size

00:06:34,130 --> 00:06:38,510
of that the 1,800 lines allowed us to

00:06:36,320 --> 00:06:40,490
move fairly quickly like I said we got

00:06:38,510 --> 00:06:44,150
the basic offload to the Jeep

00:06:40,490 --> 00:06:47,690
you're done in eight hours this is just

00:06:44,150 --> 00:06:52,940
talks about the ADI scheme and what's

00:06:47,690 --> 00:06:54,830
important for the CFD applications so to

00:06:52,940 --> 00:06:56,300
talk a little bit about the hackathon

00:06:54,830 --> 00:06:58,069
experience because I really think that

00:06:56,300 --> 00:07:01,340
was critical having all the people in

00:06:58,069 --> 00:07:03,050
one place there were eleven teams at

00:07:01,340 --> 00:07:07,220
this hackathon you saw a picture of our

00:07:03,050 --> 00:07:09,650
team and it really allowed daily sharing

00:07:07,220 --> 00:07:12,050
of lessons learned we'd be talking about

00:07:09,650 --> 00:07:13,819
what we did that day any problems we had

00:07:12,050 --> 00:07:15,830
and the ability to share that and really

00:07:13,819 --> 00:07:18,860
have the experts in the room I think was

00:07:15,830 --> 00:07:21,650
critical each team that we had a private

00:07:18,860 --> 00:07:23,569
slack channel so we communicated among

00:07:21,650 --> 00:07:24,139
ourselves in the private slack Channel

00:07:23,569 --> 00:07:26,300
we made

00:07:24,139 --> 00:07:28,780
besides being at a table together but we

00:07:26,300 --> 00:07:31,639
also shared all our information that way

00:07:28,780 --> 00:07:34,280
we used getting big bucket to document

00:07:31,639 --> 00:07:36,949
the code changes and a groove Google

00:07:34,280 --> 00:07:38,630
Drive to share PDFs PowerPoint and so on

00:07:36,949 --> 00:07:41,930
so everybody could contribute to the

00:07:38,630 --> 00:07:44,419
slides like I said each time team was

00:07:41,930 --> 00:07:46,969
assigned one or two mentors and there

00:07:44,419 --> 00:07:49,340
were those came directly from the

00:07:46,969 --> 00:07:51,680
hackathon and there were also vendor

00:07:49,340 --> 00:07:53,000
supporters there and we had to Nvidia

00:07:51,680 --> 00:07:57,500
people who were really helpful

00:07:53,000 --> 00:08:01,039
especially with the tools so for the

00:07:57,500 --> 00:08:06,259
background on OpenMP as you know open

00:08:01,039 --> 00:08:08,900
mp5 dotto was released at SC 18 that

00:08:06,259 --> 00:08:11,840
it's not the first time you could do

00:08:08,900 --> 00:08:14,900
accelerator accelerator has been

00:08:11,840 --> 00:08:19,009
supported there since openmp 4.0 but

00:08:14,900 --> 00:08:21,770
it's really become better in 4.5 and now

00:08:19,009 --> 00:08:24,500
most compilers are supporting 4.5 and

00:08:21,770 --> 00:08:30,400
many hope to be up to the full 5.0

00:08:24,500 --> 00:08:33,469
standard bye-bye you know very soon

00:08:30,400 --> 00:08:35,599
Fortran OpenMP is a little behind some

00:08:33,469 --> 00:08:37,789
of the others but it's already starting

00:08:35,599 --> 00:08:43,760
to be supported by such compilers as G

00:08:37,789 --> 00:08:45,470
Fortran Cray Fortran and IBM XLF just

00:08:43,760 --> 00:08:48,140
want to comment that the nice thing

00:08:45,470 --> 00:08:50,570
about open MP and open MP on your GPUs

00:08:48,140 --> 00:08:53,630
is it does provide a really good vendor

00:08:50,570 --> 00:08:54,529
neutral approach to the standard and the

00:08:53,630 --> 00:08:57,529
standard itself

00:08:54,529 --> 00:08:59,839
is written by a consortium of HPC

00:08:57,529 --> 00:09:05,360
compiler writers application people and

00:08:59,839 --> 00:09:08,749
so on so here's the execution flow of

00:09:05,360 --> 00:09:11,600
the SP MZ benchmark it's nice because it

00:09:08,749 --> 00:09:13,519
itself checks itself which is nice so we

00:09:11,600 --> 00:09:16,300
were able to see all along the way that

00:09:13,519 --> 00:09:18,589
we weren't introducing any any code that

00:09:16,300 --> 00:09:21,350
introduced mistakes or anything like

00:09:18,589 --> 00:09:25,040
that and we focused on open MP in the

00:09:21,350 --> 00:09:26,990
solver actually the the bitbucket tools

00:09:25,040 --> 00:09:29,480
were kind of nice too because when we

00:09:26,990 --> 00:09:30,319
committed our changes along the way to

00:09:29,480 --> 00:09:32,660
bitbucket

00:09:30,319 --> 00:09:34,399
we were able to look at things like this

00:09:32,660 --> 00:09:38,540
and see what was changed for instance

00:09:34,399 --> 00:09:40,339
here you can see that the new version is

00:09:38,540 --> 00:09:42,829
in the green the old version is in the

00:09:40,339 --> 00:09:45,379
in the pink here's a parallel -

00:09:42,829 --> 00:09:48,620
associated with thee with the CPU

00:09:45,379 --> 00:09:52,160
version and the target teams version

00:09:48,620 --> 00:09:55,309
associated with the GPU version we did

00:09:52,160 --> 00:09:56,990
some array modifications and so on and

00:09:55,309 --> 00:09:58,610
like I said it was really nice that in

00:09:56,990 --> 00:10:00,230
the bit bucket you were able to just see

00:09:58,610 --> 00:10:05,360
what was going on with the code along

00:10:00,230 --> 00:10:08,930
the way here - is is the fully optimized

00:10:05,360 --> 00:10:15,529
code and the CPU code for the GPU and

00:10:08,930 --> 00:10:17,779
the CPU and V prof and other tools were

00:10:15,529 --> 00:10:23,300
really critical we used env prof on both

00:10:17,779 --> 00:10:27,439
the Cray version and on the and on the

00:10:23,300 --> 00:10:29,750
and on the IBM version we had the way

00:10:27,439 --> 00:10:33,829
that GPU programming is implemented

00:10:29,750 --> 00:10:35,209
it'sit's not always I won't say

00:10:33,829 --> 00:10:37,069
standardized but if you follow the

00:10:35,209 --> 00:10:39,410
standard you still have some flexibility

00:10:37,069 --> 00:10:42,050
and so it wasn't exactly the same

00:10:39,410 --> 00:10:44,750
implementation on both compilers and we

00:10:42,050 --> 00:10:46,879
noticed that and sometimes some of the

00:10:44,750 --> 00:10:49,339
compilers were more advanced for

00:10:46,879 --> 00:10:51,170
instance the IBM compiler was doing some

00:10:49,339 --> 00:10:53,269
additional copies that the Krait

00:10:51,170 --> 00:10:56,600
compiler got rid of and that really

00:10:53,269 --> 00:10:58,550
helped us to when we figured that out we

00:10:56,600 --> 00:11:00,230
were able to change from a private

00:10:58,550 --> 00:11:02,660
variable to a shared variable and fix

00:11:00,230 --> 00:11:04,220
that but along the way as compilers get

00:11:02,660 --> 00:11:06,110
smarter they'll probably fix some of

00:11:04,220 --> 00:11:07,570
these things but right now with new

00:11:06,110 --> 00:11:10,390
compilers it's

00:11:07,570 --> 00:11:13,380
it's still working so here's here's our

00:11:10,390 --> 00:11:15,790
code these are all the revisions a

00:11:13,380 --> 00:11:17,710
majority of these up to about here we're

00:11:15,790 --> 00:11:20,110
all done at the hackathon so all within

00:11:17,710 --> 00:11:22,890
five days you can see the different

00:11:20,110 --> 00:11:28,540
codes and here what we see is

00:11:22,890 --> 00:11:30,550
performance on the IBM system and this

00:11:28,540 --> 00:11:32,620
is the CPU version so as we're making

00:11:30,550 --> 00:11:35,020
the changes you can also see what's

00:11:32,620 --> 00:11:37,480
happening to the CPU version ideally it

00:11:35,020 --> 00:11:39,940
would stay totally flat sometimes you'll

00:11:37,480 --> 00:11:41,980
see a CPU version gets faster in this

00:11:39,940 --> 00:11:45,820
case it got a tiny bit slower but it was

00:11:41,980 --> 00:11:48,610
sort of negligible and here's all the

00:11:45,820 --> 00:11:52,450
different changes we have at each at

00:11:48,610 --> 00:11:55,120
each at each level of implementation at

00:11:52,450 --> 00:11:57,610
each commit that we times so we're

00:11:55,120 --> 00:11:59,410
writing a paper on this and so it it'll

00:11:57,610 --> 00:12:04,210
be clearly documented as well as making

00:11:59,410 --> 00:12:07,240
the source code available we also

00:12:04,210 --> 00:12:08,860
studied the MPI performance across nodes

00:12:07,240 --> 00:12:11,710
we wanted to make sure that nothing was

00:12:08,860 --> 00:12:14,110
happening to that and on the IBM machine

00:12:11,710 --> 00:12:16,360
we were able to use MPI look across

00:12:14,110 --> 00:12:18,460
nodes and we're pretty close to ideal

00:12:16,360 --> 00:12:20,110
speed-up so we really didn't lose

00:12:18,460 --> 00:12:22,570
anything in any of the transformations

00:12:20,110 --> 00:12:25,750
that we did at the time that we did the

00:12:22,570 --> 00:12:27,730
hackathon we weren't able to use MPI

00:12:25,750 --> 00:12:29,470
across all the nodes yet in the Kreg

00:12:27,730 --> 00:12:33,550
compiler because it wasn't supporting

00:12:29,470 --> 00:12:35,140
that yet it will in a few months but we

00:12:33,550 --> 00:12:37,720
think the scalability on there will be

00:12:35,140 --> 00:12:39,850
about the same as well so some of the

00:12:37,720 --> 00:12:41,620
optimizations that we included we

00:12:39,850 --> 00:12:44,440
changed the memory layout of the the

00:12:41,620 --> 00:12:48,370
arrays we added some extra array

00:12:44,440 --> 00:12:50,680
dimensions to get thread private data on

00:12:48,370 --> 00:12:53,650
the GPU to eliminate some of the data

00:12:50,680 --> 00:12:57,700
transfers we reordered some of the loop

00:12:53,650 --> 00:12:59,440
dimensions we did loop collapsing and a

00:12:57,700 --> 00:13:03,130
few other things that at the end we even

00:12:59,440 --> 00:13:05,890
included some asynchronous execution to

00:13:03,130 --> 00:13:10,960
allow kernels on the GPU to operate

00:13:05,890 --> 00:13:14,080
asynchronously future directions and GPU

00:13:10,960 --> 00:13:17,190
optimization it's really important these

00:13:14,080 --> 00:13:20,590
days that people have parallel code

00:13:17,190 --> 00:13:23,290
compilers will soon hopefully up

00:13:20,590 --> 00:13:25,660
except what is sometimes called the big

00:13:23,290 --> 00:13:27,910
ugly directive but anyway pragma OpenMP

00:13:25,660 --> 00:13:30,930
target to Street teams distribute

00:13:27,910 --> 00:13:33,460
parallel for and that'll just work on

00:13:30,930 --> 00:13:35,530
without so you won't even need to if

00:13:33,460 --> 00:13:40,930
that's your code and have different GPU

00:13:35,530 --> 00:13:42,670
and CPU versions for the GPU the next

00:13:40,930 --> 00:13:45,220
thing we really need to do is focusing

00:13:42,670 --> 00:13:47,560
on optimizing for the GPU and one thing

00:13:45,220 --> 00:13:49,630
I'll mention is that optimizing for the

00:13:47,560 --> 00:13:52,030
GPU when you're doing an open MP code is

00:13:49,630 --> 00:13:55,080
no different than optimizing for the GPU

00:13:52,030 --> 00:14:00,180
with CUDA or optimizing with any other

00:13:55,080 --> 00:14:03,610
GPU language you have to think think

00:14:00,180 --> 00:14:07,420
think about what the what the GPU

00:14:03,610 --> 00:14:10,600
actually does and one of the things that

00:14:07,420 --> 00:14:14,710
that the GPU does is it it really works

00:14:10,600 --> 00:14:18,820
in a warp without with various branches

00:14:14,710 --> 00:14:22,810
and in a in a fashion such that these

00:14:18,820 --> 00:14:24,820
branches once once it has to be SPMD so

00:14:22,810 --> 00:14:27,610
you can't overlap these branches and

00:14:24,820 --> 00:14:30,310
that's a function of the GPU it's not

00:14:27,610 --> 00:14:33,640
function of openmp and so we have to

00:14:30,310 --> 00:14:35,470
deal with that and make sure that the

00:14:33,640 --> 00:14:40,900
work that we have in the branches is

00:14:35,470 --> 00:14:45,220
able to operate asynchronously so this

00:14:40,900 --> 00:14:47,650
is just a slide more on how to how to

00:14:45,220 --> 00:14:49,420
deal with branching compilers are very

00:14:47,650 --> 00:14:51,970
modern as well as some of the

00:14:49,420 --> 00:14:54,100
architectures on CPUs they do branch

00:14:51,970 --> 00:14:55,750
speculation so they decide are you going

00:14:54,100 --> 00:15:00,460
to go this way in a branch and so on

00:14:55,750 --> 00:15:02,110
GPUs are not able to do that and and so

00:15:00,460 --> 00:15:04,510
you have to account for that when you're

00:15:02,110 --> 00:15:06,730
optimizing on the GPU and again this is

00:15:04,510 --> 00:15:10,990
the same kind of optimization you do for

00:15:06,730 --> 00:15:18,550
for any any language that's designed for

00:15:10,990 --> 00:15:20,440
a GPU you might want to hide latency you

00:15:18,550 --> 00:15:24,760
might want to have different work items

00:15:20,440 --> 00:15:27,160
and so on and you never want to add all

00:15:24,760 --> 00:15:30,480
create divergent branches when you're

00:15:27,160 --> 00:15:33,570
optimizing for the GPU

00:15:30,480 --> 00:15:36,300
another thing is arrays and we did

00:15:33,570 --> 00:15:39,509
change our race to be more friendly in

00:15:36,300 --> 00:15:41,820
the data layout to the GPUs there are

00:15:39,509 --> 00:15:44,310
certain data layouts that work better

00:15:41,820 --> 00:15:46,860
for the GPU certain that work better for

00:15:44,310 --> 00:15:48,720
the CPU and that's something that unless

00:15:46,860 --> 00:15:50,519
you get a really really smart compiler

00:15:48,720 --> 00:15:52,290
someday that's always going to be a

00:15:50,519 --> 00:15:55,889
portability problem with going between

00:15:52,290 --> 00:15:58,259
CPUs and GPUs and it what it may end up

00:15:55,889 --> 00:16:00,240
meaning is that you'll have to have

00:15:58,259 --> 00:16:02,279
certain code that works on the CPU and

00:16:00,240 --> 00:16:06,110
certain code that works on this GPU

00:16:02,279 --> 00:16:06,110
depending on how the memory is laid out

00:16:06,709 --> 00:16:11,519
coalescence that's an old technique from

00:16:09,839 --> 00:16:13,860
back in the vector days where you

00:16:11,519 --> 00:16:16,920
coalesce the work and that still works

00:16:13,860 --> 00:16:22,230
very well on the on the GPUs and we use

00:16:16,920 --> 00:16:24,120
that as well in our optimization so

00:16:22,230 --> 00:16:27,569
basically these are the code

00:16:24,120 --> 00:16:30,899
transformations that we were able to do

00:16:27,569 --> 00:16:35,009
at the during the five days of the

00:16:30,899 --> 00:16:38,240
hackathon we implemented all these and

00:16:35,009 --> 00:16:43,769
then we did a few more optimizations

00:16:38,240 --> 00:16:46,230
beyond that so I'll just go over the

00:16:43,769 --> 00:16:48,269
hackathon here again is our is our

00:16:46,230 --> 00:16:52,019
result so this is the original code

00:16:48,269 --> 00:16:55,889
using 42 MPA ranks on the power 9 which

00:16:52,019 --> 00:16:59,190
was the fastest CPU version that you and

00:16:55,889 --> 00:17:01,470
this the CPU version of the Ness payroll

00:16:59,190 --> 00:17:02,610
benchmark is fairly well optimized so we

00:17:01,470 --> 00:17:06,120
don't feel like we're working with an

00:17:02,610 --> 00:17:11,730
optimized OpenMP implementation and

00:17:06,120 --> 00:17:14,040
here's our performance improvement like

00:17:11,730 --> 00:17:17,370
I showed in that one plot there was

00:17:14,040 --> 00:17:19,049
slight CPU performance degradation with

00:17:17,370 --> 00:17:22,740
the changing of the arrays but it was

00:17:19,049 --> 00:17:25,620
pretty minimal in order to make the code

00:17:22,740 --> 00:17:28,049
more relevant to modern applications

00:17:25,620 --> 00:17:30,480
we're looking into some creating some

00:17:28,049 --> 00:17:32,460
benchmarks that have array sizes that

00:17:30,480 --> 00:17:34,559
match kind of the larger computer since

00:17:32,460 --> 00:17:36,150
the original mash fitness peril

00:17:34,559 --> 00:17:38,760
benchmarks are more than 10 years old

00:17:36,150 --> 00:17:42,830
now they don't quite take care of all

00:17:38,760 --> 00:17:45,030
the memory ability of new processors

00:17:42,830 --> 00:17:46,860
OpenMP 5 we'll get some more

00:17:45,030 --> 00:17:47,520
optimization in it and we're hoping that

00:17:46,860 --> 00:17:49,950
these are

00:17:47,520 --> 00:17:51,750
over to Fortran as well and the

00:17:49,950 --> 00:17:54,060
hackathon experience we thought was

00:17:51,750 --> 00:17:56,940
really great like I said you know you

00:17:54,060 --> 00:17:59,610
might you might say oh I could refactor

00:17:56,940 --> 00:18:04,770
a code in and you know six months or a

00:17:59,610 --> 00:18:06,950
year well this was five days so that's

00:18:04,770 --> 00:18:06,950

YouTube URL: https://www.youtube.com/watch?v=6I6H5Gb8wEE


