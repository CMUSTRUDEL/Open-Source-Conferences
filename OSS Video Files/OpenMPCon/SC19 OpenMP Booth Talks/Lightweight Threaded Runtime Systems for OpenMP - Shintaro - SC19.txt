Title: Lightweight Threaded Runtime Systems for OpenMP - Shintaro - SC19
Publication date: 2019-11-25
Playlist: SC19 OpenMP Booth Talks
Description: 
	20 November 2019 - SC19 - Denver
Slides: https://www.openmp.org/wp-content/uploads/SC19-Iwasaki-Threads.pdf
Captions: 
	00:00:00,000 --> 00:00:06,799
hello hello okay yeah then let's start

00:00:07,069 --> 00:00:12,000
hello I'm Shintaro sake a pre-doctoral

00:00:10,110 --> 00:00:13,440
appointee at Argonne National Lab and

00:00:12,000 --> 00:00:16,680
answering a PhD student at the

00:00:13,440 --> 00:00:18,779
University oh I'm belong to the province

00:00:16,680 --> 00:00:21,869
group developing para random systems

00:00:18,779 --> 00:00:24,660
such as mph or Shu MPI and our BOTS

00:00:21,869 --> 00:00:26,789
today's my talk is about our lightweight

00:00:24,660 --> 00:00:30,840
shredding model and I was ready to write

00:00:26,789 --> 00:00:32,780
weight and ours readied a library for op

00:00:30,840 --> 00:00:35,730
MP cold bold

00:00:32,780 --> 00:00:37,800
alright I'm lying on my toque so bold is

00:00:35,730 --> 00:00:41,820
a lightweight OPP random system based on

00:00:37,800 --> 00:00:43,290
LLVM op MP in our bold library open

00:00:41,820 --> 00:00:45,539
piece read and pasture map to

00:00:43,290 --> 00:00:48,420
lightweight shreds provided by alga BOTS

00:00:45,539 --> 00:00:51,120
with mould on P threads are and tasking

00:00:48,420 --> 00:00:52,920
are optimized specifically from the user

00:00:51,120 --> 00:00:55,469
perspective these two points are

00:00:52,920 --> 00:00:57,149
imported first on P threads are very

00:00:55,469 --> 00:01:00,180
lightweight compared with the original

00:00:57,149 --> 00:01:02,820
LVM of MP so both can efficiently handle

00:01:00,180 --> 00:01:05,430
nested power reasons also both has

00:01:02,820 --> 00:01:08,580
higher availability with MPI in terms of

00:01:05,430 --> 00:01:10,860
MPI plus or PB OpenMP tasks but we don't

00:01:08,580 --> 00:01:13,080
talk about it in this talk so this work

00:01:10,860 --> 00:01:15,030
is highly regarded by the HPC community

00:01:13,080 --> 00:01:18,060
so this worked on the best paper our

00:01:15,030 --> 00:01:19,680
data at 19 and anyway so today's

00:01:18,060 --> 00:01:21,979
presentation is about the efficient

00:01:19,680 --> 00:01:24,990
execution of nested para regions

00:01:21,979 --> 00:01:26,729
introduction and as you know

00:01:24,990 --> 00:01:30,030
multi-threading becomes more and more

00:01:26,729 --> 00:01:32,520
popular an HPC Field Op MP is the most

00:01:30,030 --> 00:01:34,740
popular multi threading model as on

00:01:32,520 --> 00:01:37,500
people become as popular but not only

00:01:34,740 --> 00:01:40,320
the user programs but also runtimes and

00:01:37,500 --> 00:01:42,299
libraries are paralyzed by open P as a

00:01:40,320 --> 00:01:44,700
result the current situation and

00:01:42,299 --> 00:01:47,189
intentionally introduces NASA tourism

00:01:44,700 --> 00:01:50,220
existing in multiple software stats for

00:01:47,189 --> 00:01:52,259
example users open people calls call an

00:01:50,220 --> 00:01:55,020
external function which happens to be

00:01:52,259 --> 00:01:58,049
paralysed by open people region again if

00:01:55,020 --> 00:01:59,820
on this rather closed every tiny and if

00:01:58,049 --> 00:02:02,280
open P threads are created every time

00:01:59,820 --> 00:02:04,700
the control encounters para region the

00:02:02,280 --> 00:02:07,350
growth of an open piece read count is

00:02:04,700 --> 00:02:09,410
exponential for example the only power

00:02:07,350 --> 00:02:12,290
for crates of our region

00:02:09,410 --> 00:02:15,860
spawns rest then usually has many us

00:02:12,290 --> 00:02:17,930
number ports so then it's it's read

00:02:15,860 --> 00:02:19,730
calls a deacon function which internally

00:02:17,930 --> 00:02:21,770
creates a per region for example the

00:02:19,730 --> 00:02:25,580
total number of sweat code becomes in

00:02:21,770 --> 00:02:28,910
this case sixteen and if there's one

00:02:25,580 --> 00:02:31,160
more region inside be gem in the can be

00:02:28,910 --> 00:02:33,590
sixty four more so because the major on

00:02:31,160 --> 00:02:35,780
peer anthem system map on P threads to

00:02:33,590 --> 00:02:38,000
Colonels res or peas Reds it causes a

00:02:35,780 --> 00:02:40,100
severe oversubscription of sweat and

00:02:38,000 --> 00:02:42,710
significantly degrades performance

00:02:40,100 --> 00:02:44,810
okay so enabling nested power region

00:02:42,710 --> 00:02:46,940
some exponentially creates Reds and

00:02:44,810 --> 00:02:49,190
significantly degrade performance the

00:02:46,940 --> 00:02:51,500
widely adapted option is disabling

00:02:49,190 --> 00:02:53,300
nested power returns this is the default

00:02:51,500 --> 00:02:55,610
behavior of open peer anthem system

00:02:53,300 --> 00:02:57,890
however such a loss of power isn't can

00:02:55,610 --> 00:02:59,810
hurt the performance a typical case is

00:02:57,890 --> 00:03:02,690
strong scaling on massively power

00:02:59,810 --> 00:03:04,760
machines so as the number of cores per

00:03:02,690 --> 00:03:07,190
processor is increasing and also the now

00:03:04,760 --> 00:03:09,530
note note count is increasing without

00:03:07,190 --> 00:03:11,630
the power isn't my to be enough to

00:03:09,530 --> 00:03:14,870
assign might not be enough to assign

00:03:11,630 --> 00:03:17,000
work to all the cores the problem of

00:03:14,870 --> 00:03:20,480
nested parallelism has been known for 25

00:03:17,000 --> 00:03:22,640
years so right now all P disables nested

00:03:20,480 --> 00:03:25,010
tourism by default II recently and this

00:03:22,640 --> 00:03:26,930
is DC the default OB setting and many

00:03:25,010 --> 00:03:29,750
programs are actually unaware of

00:03:26,930 --> 00:03:32,090
potential loss of terrorism beyond this

00:03:29,750 --> 00:03:34,340
we can follow we can find two directions

00:03:32,090 --> 00:03:36,680
to alleviate this issue the first one is

00:03:34,340 --> 00:03:38,810
about own business I mean open P has

00:03:36,680 --> 00:03:41,690
severe workarounds to solve this message

00:03:38,810 --> 00:03:44,360
power region issue and but however this

00:03:41,690 --> 00:03:47,240
workaround is only very effective if

00:03:44,360 --> 00:03:48,740
users know and control the old terrorism

00:03:47,240 --> 00:03:51,350
in the program including all the

00:03:48,740 --> 00:03:53,390
software stacks in other words this idea

00:03:51,350 --> 00:03:54,980
is not very effective if you don't know

00:03:53,390 --> 00:03:56,959
the inner power ism

00:03:54,980 --> 00:03:58,580
the second one is about using

00:03:56,959 --> 00:04:00,110
lightweight sweats instead of always

00:03:58,580 --> 00:04:02,150
level threads for open be friends

00:04:00,110 --> 00:04:04,280
they're having severe proposal but they

00:04:02,150 --> 00:04:07,700
do not perform very well if my regions

00:04:04,280 --> 00:04:09,650
are not nested any flat and to make the

00:04:07,700 --> 00:04:11,990
matter worse they are often like slower

00:04:09,650 --> 00:04:14,390
than the state-of-the-art Intel or LVM

00:04:11,990 --> 00:04:16,190
OB random systems as a result the rule

00:04:14,390 --> 00:04:18,109
of thumb is disabling nested parallelism

00:04:16,190 --> 00:04:18,900
but as the software starts get more and

00:04:18,109 --> 00:04:20,519
more complex

00:04:18,900 --> 00:04:22,560
and now this reading becomes more and

00:04:20,519 --> 00:04:24,440
more important and the solution to

00:04:22,560 --> 00:04:29,160
utilize mrs. perrault regions is

00:04:24,440 --> 00:04:30,960
demanding all right so we can call these

00:04:29,160 --> 00:04:33,330
mrs. perrault region issue by adopting

00:04:30,960 --> 00:04:35,729
the second option which the Airways deep

00:04:33,330 --> 00:04:38,039
performance analysis and optimizations

00:04:35,729 --> 00:04:39,630
our bowl on time system is a light way

00:04:38,039 --> 00:04:42,090
to use a levels reading library based on

00:04:39,630 --> 00:04:45,750
LLVM op MP which performs best for boss

00:04:42,090 --> 00:04:47,610
flat and nested parens okay we first

00:04:45,750 --> 00:04:50,100
likely talk about the background of this

00:04:47,610 --> 00:04:52,229
work as I explained the nested burro

00:04:50,100 --> 00:04:54,630
region have been considered as a problem

00:04:52,229 --> 00:04:57,000
for decays so since the major of peer

00:04:54,630 --> 00:05:00,060
anthem systems main point is rents to

00:04:57,000 --> 00:05:02,160
kernel threads the open p standard tries

00:05:00,060 --> 00:05:04,020
to solve this problem in severe ways and

00:05:02,160 --> 00:05:05,130
essentially OS level threads are a

00:05:04,020 --> 00:05:06,870
little bit heavy weight so the

00:05:05,130 --> 00:05:09,349
workarounds are trying to reduce the

00:05:06,870 --> 00:05:11,610
number of threads in total for example

00:05:09,349 --> 00:05:13,470
disabling nesset for our region which is

00:05:11,610 --> 00:05:15,979
the default of in behavior till recently

00:05:13,470 --> 00:05:18,389
and all the nests it is lost and

00:05:15,979 --> 00:05:20,039
manufacturing setting for example by

00:05:18,389 --> 00:05:23,039
setting the number of threads per level

00:05:20,039 --> 00:05:25,080
or per region is another option if users

00:05:23,039 --> 00:05:27,180
know everything it might be the best

00:05:25,080 --> 00:05:29,520
solution but it is very hard and may be

00:05:27,180 --> 00:05:31,320
impractical to tweet paracin at each

00:05:29,520 --> 00:05:33,599
level based on the available number of

00:05:31,320 --> 00:05:35,159
cores or input or some kind of like a

00:05:33,599 --> 00:05:37,830
software configuration and like you

00:05:35,159 --> 00:05:39,750
every other stuff and the third option

00:05:37,830 --> 00:05:41,789
is limiting the total number of threads

00:05:39,750 --> 00:05:44,570
in system which can adversely serialize

00:05:41,789 --> 00:05:46,830
in operations there is another option

00:05:44,570 --> 00:05:48,840
dynamic or automatic the number of

00:05:46,830 --> 00:05:51,090
threads is automatically adjusted by the

00:05:48,840 --> 00:05:53,639
system runtime system but such an

00:05:51,090 --> 00:05:55,530
adaptive and dynamic technique sounds

00:05:53,639 --> 00:05:57,180
promising but it won't work in practice

00:05:55,530 --> 00:05:58,650
because and finally the runtime system

00:05:57,180 --> 00:06:01,860
doesn't know the power resuming the

00:05:58,650 --> 00:06:03,240
program then using only tasks instead of

00:06:01,860 --> 00:06:05,340
on these words can reduce the

00:06:03,240 --> 00:06:07,349
oversubscription cost paths can be

00:06:05,340 --> 00:06:09,510
implemented as a function pointer and

00:06:07,349 --> 00:06:11,699
it's argument ultimately so it's very

00:06:09,510 --> 00:06:14,430
lightweight but unfortunately most

00:06:11,699 --> 00:06:16,560
programs still use on B threads I'm the

00:06:14,430 --> 00:06:19,800
only firm forward and more importantly

00:06:16,560 --> 00:06:22,320
tasks lot some features that res have

00:06:19,800 --> 00:06:24,930
for example tasks do not support past

00:06:22,320 --> 00:06:26,490
tasks dependency and we test paths in

00:06:24,930 --> 00:06:30,779
synchronization for example tasks per

00:06:26,490 --> 00:06:32,159
year or TOS or affinity so these

00:06:30,779 --> 00:06:33,479
workarounds do not

00:06:32,159 --> 00:06:35,699
or to say fundamentally solve the

00:06:33,479 --> 00:06:37,559
problem openness eternal region the root

00:06:35,699 --> 00:06:39,929
cause of this problem is using

00:06:37,559 --> 00:06:42,149
heavyweight OS level threads as own P

00:06:39,929 --> 00:06:44,969
threads then how about using lightweight

00:06:42,149 --> 00:06:46,679
threads for on these threads like way to

00:06:44,969 --> 00:06:48,419
use all levels rents yield these are

00:06:46,679 --> 00:06:50,399
often used to abstract lightweight

00:06:48,419 --> 00:06:52,349
flying rain power units please look at

00:06:50,399 --> 00:06:55,319
the left ear and always levels rates are

00:06:52,349 --> 00:06:57,659
fundamentally heavy because this red

00:06:55,319 --> 00:06:59,459
kernel helps a interaction is very

00:06:57,659 --> 00:07:01,349
costly and any context switching

00:06:59,459 --> 00:07:04,589
involved these heavy kernel operations

00:07:01,349 --> 00:07:07,589
guilties are implemented in user space

00:07:04,589 --> 00:07:10,409
so they run on the traditional piece

00:07:07,589 --> 00:07:12,779
read in figure um in their general like

00:07:10,409 --> 00:07:14,729
not say all level threads like this but

00:07:12,779 --> 00:07:16,979
the oil tees do not use all scheduler

00:07:14,729 --> 00:07:18,599
for context switching so every context

00:07:16,979 --> 00:07:20,909
switching is implemented in user space

00:07:18,599 --> 00:07:23,099
and this can significantly reduce

00:07:20,909 --> 00:07:24,509
reading overheads for example buying

00:07:23,099 --> 00:07:27,179
implementation and user levels writes

00:07:24,509 --> 00:07:29,490
our Gobots has 350 times smaller folk

00:07:27,179 --> 00:07:31,769
enjoying overheads than data P threads

00:07:29,490 --> 00:07:34,169
then using user level threads can

00:07:31,769 --> 00:07:37,529
significantly improve overhead improve

00:07:34,169 --> 00:07:39,449
performance so we used LLVM open p one

00:07:37,529 --> 00:07:41,339
of the most efficient open p runtime

00:07:39,449 --> 00:07:42,839
system in the world and for usable

00:07:41,339 --> 00:07:44,639
threads we user state-of-the-art user

00:07:42,839 --> 00:07:46,800
levels rating library our BOTS and

00:07:44,639 --> 00:07:47,399
replace the piece rate layer with our

00:07:46,800 --> 00:07:49,949
BOTS

00:07:47,399 --> 00:07:52,050
this replacement is not technically hard

00:07:49,949 --> 00:07:54,479
so it's very easy just replace it then

00:07:52,050 --> 00:07:57,599
let us call this ult version both

00:07:54,479 --> 00:07:59,759
baseline so in both baseline we create

00:07:57,599 --> 00:08:02,399
these rates as many as number cores and

00:07:59,759 --> 00:08:04,949
then each piece red runs scheduler and

00:08:02,399 --> 00:08:07,139
the schedules schedule user level stress

00:08:04,949 --> 00:08:09,719
associated with open P threads compared

00:08:07,139 --> 00:08:11,159
with the piece rate version even if we

00:08:09,719 --> 00:08:12,869
create on this raised more than number

00:08:11,159 --> 00:08:14,999
of cores the total number of these

00:08:12,869 --> 00:08:17,789
threads or always levels red is the same

00:08:14,999 --> 00:08:20,999
so there's lease over subscription cost

00:08:17,789 --> 00:08:23,339
and does it perform very well so we run

00:08:20,999 --> 00:08:26,249
this nested power micro benchmark on 56

00:08:23,339 --> 00:08:27,779
 on skylake this read count of out

00:08:26,249 --> 00:08:29,490
of our region is changed while the

00:08:27,779 --> 00:08:32,969
thread count of the inner power region

00:08:29,490 --> 00:08:34,740
is fixed to 28 and then this is faster

00:08:32,969 --> 00:08:36,990
is better so lower is better the figure

00:08:34,740 --> 00:08:39,689
shows that the both baseline so without

00:08:36,990 --> 00:08:42,870
any optimization is much faster than new

00:08:39,689 --> 00:08:45,480
or big GCC open P however compared with

00:08:42,870 --> 00:08:49,079
existing ult based runtime systems like

00:08:45,480 --> 00:08:51,630
MPC Oh MPI and open be compatible open

00:08:49,079 --> 00:08:53,699
PSS the performance of the both baseline

00:08:51,630 --> 00:08:56,639
is not very good surprisingly

00:08:53,699 --> 00:08:58,980
compared with well tuned Intel and LVM

00:08:56,639 --> 00:09:01,199
open piece both baselines about seven

00:08:58,980 --> 00:09:04,110
times slower even in the case of nested

00:09:01,199 --> 00:09:06,059
Parisiennes okay then why so in this

00:09:04,110 --> 00:09:09,540
section I'd like to explain like I mean

00:09:06,059 --> 00:09:12,839
why and if we will come up with some

00:09:09,540 --> 00:09:14,399
optimization for this just replacing on

00:09:12,839 --> 00:09:16,230
piece right with you syllabus which does

00:09:14,399 --> 00:09:18,809
not perform very well so to truly

00:09:16,230 --> 00:09:20,250
exploit flat and nested power regions we

00:09:18,809 --> 00:09:22,769
found the necessity of severe

00:09:20,250 --> 00:09:24,660
optimizations first sloping scalability

00:09:22,769 --> 00:09:26,670
bottlenecks in L of a mop NP

00:09:24,660 --> 00:09:28,800
second Yoda friendly affinity

00:09:26,670 --> 00:09:30,990
implementations and third efficient

00:09:28,800 --> 00:09:33,569
thread coordination algorithm so look at

00:09:30,990 --> 00:09:35,550
the evaluation chart so finally

00:09:33,569 --> 00:09:37,199
performance will look like this so using

00:09:35,550 --> 00:09:38,730
the same user levels reading library but

00:09:37,199 --> 00:09:39,300
more than ten times faster than the

00:09:38,730 --> 00:09:41,490
baseline

00:09:39,300 --> 00:09:43,470
okay so I'll explain optimizations one

00:09:41,490 --> 00:09:45,779
by one the first one is resource

00:09:43,470 --> 00:09:48,870
management optimizations we found from

00:09:45,779 --> 00:09:51,029
we first found that in LLVM open p most

00:09:48,870 --> 00:09:52,980
reading library most reading resources

00:09:51,029 --> 00:09:54,870
like thread descriptors and team

00:09:52,980 --> 00:09:57,389
descriptors are protected by a single

00:09:54,870 --> 00:09:59,910
log and creating a large critical

00:09:57,389 --> 00:10:01,589
section on default join pass so we first

00:09:59,910 --> 00:10:03,600
divide their Lord critical sections by

00:10:01,589 --> 00:10:05,360
creating more fine grained logs /

00:10:03,600 --> 00:10:08,009
objects to alleviate the contention

00:10:05,360 --> 00:10:10,380
furthermore in urban people region is

00:10:08,009 --> 00:10:12,660
always created as a team not multiple

00:10:10,380 --> 00:10:15,449
independents read spawn and like a mean

00:10:12,660 --> 00:10:18,420
join so using the team concept and

00:10:15,449 --> 00:10:20,100
improve resource management so the team

00:10:18,420 --> 00:10:21,600
concept makes the team level data

00:10:20,100 --> 00:10:23,819
caching possible and which can

00:10:21,600 --> 00:10:26,910
efficiently access reuse team data this

00:10:23,819 --> 00:10:28,829
is called top team in LLVM of MV so in

00:10:26,910 --> 00:10:31,199
addition we optimize the thread creation

00:10:28,829 --> 00:10:33,149
method previously the Masters read

00:10:31,199 --> 00:10:35,250
access the Charles read sequentially on

00:10:33,149 --> 00:10:36,449
for conjoining pass but the divide and

00:10:35,250 --> 00:10:38,699
conquer spread creation is more

00:10:36,449 --> 00:10:40,620
efficient in terms of critical path we

00:10:38,699 --> 00:10:42,929
know that over all the overheads are

00:10:40,620 --> 00:10:44,790
negligible when you use all levels Reds

00:10:42,929 --> 00:10:47,399
but with lightweight sweats the

00:10:44,790 --> 00:10:49,019
bottlenecks become very significant all

00:10:47,399 --> 00:10:51,449
right so let's see the performance this

00:10:49,019 --> 00:10:53,230
is a nice apparel pinch bergna stood for

00:10:51,449 --> 00:10:55,570
a region benchmark

00:10:53,230 --> 00:10:57,610
all computation so efficient resource

00:10:55,570 --> 00:10:59,830
management significantly reduces

00:10:57,610 --> 00:11:02,140
overheads when many oldies are created

00:10:59,830 --> 00:11:03,850
while binaries read creations shorten

00:11:02,140 --> 00:11:05,980
the critical path when the power is amis

00:11:03,850 --> 00:11:08,350
very limited ok the previous

00:11:05,980 --> 00:11:10,450
optimizations are to fix the kind of

00:11:08,350 --> 00:11:13,060
artifacts of elven open by random

00:11:10,450 --> 00:11:14,920
systems in this section we will focus on

00:11:13,060 --> 00:11:17,140
affinity which is often ignored with

00:11:14,920 --> 00:11:19,150
user levels read based implementation in

00:11:17,140 --> 00:11:21,610
general strength affinity or straight

00:11:19,150 --> 00:11:24,190
binding is useful to improve locality

00:11:21,610 --> 00:11:27,820
I'd like to explain how open PR Finity

00:11:24,190 --> 00:11:29,590
works in OP NP place is a virtual

00:11:27,820 --> 00:11:31,960
location Union and can be specified

00:11:29,590 --> 00:11:34,150
we're an environmental variable so if we

00:11:31,960 --> 00:11:39,730
specify for example won't be places 0 1

00:11:34,150 --> 00:11:42,970
2 3 4 5 6 7 on an 8 core CPU core 0 1

00:11:39,730 --> 00:11:45,310
becomes a place 0 4 2 3 becomes place 1

00:11:42,970 --> 00:11:47,110
4 5 place and place to land the last two

00:11:45,310 --> 00:11:50,080
cores become place for a place free and

00:11:47,110 --> 00:11:52,300
when proc bang is specified the thread

00:11:50,080 --> 00:11:54,760
is bound to one of the round the place

00:11:52,300 --> 00:11:57,280
according to the proc bind pattern so

00:11:54,760 --> 00:12:00,610
with peas race disability is set by CPU

00:11:57,280 --> 00:12:01,090
mask however yo T's do not have a CPU

00:12:00,610 --> 00:12:03,610
mask

00:12:01,090 --> 00:12:05,230
so both baseline lacks the notion of

00:12:03,610 --> 00:12:09,040
affinity which is still standard

00:12:05,230 --> 00:12:10,780
compliant because the effect of affinity

00:12:09,040 --> 00:12:13,390
is actually implementation defined in

00:12:10,780 --> 00:12:15,610
the specification but this affinity

00:12:13,390 --> 00:12:18,310
setting given by users should be used

00:12:15,610 --> 00:12:20,560
over both not only to improve locality

00:12:18,310 --> 00:12:23,530
but also to avoid contentions of sweat

00:12:20,560 --> 00:12:25,240
used by deterministic scheduling okay

00:12:23,530 --> 00:12:27,190
thanks to customizable scheduler

00:12:25,240 --> 00:12:29,620
provided by our BOTS both can implement

00:12:27,190 --> 00:12:30,850
play skills to implement affinity the

00:12:29,620 --> 00:12:33,370
idea is very simple

00:12:30,850 --> 00:12:35,650
so each schedulers become a virtual core

00:12:33,370 --> 00:12:37,930
or virtual processor and bound to

00:12:35,650 --> 00:12:41,020
certain core and then place queues

00:12:37,930 --> 00:12:43,600
corresponding to the places are created

00:12:41,020 --> 00:12:45,460
like this so open peas rails running on

00:12:43,600 --> 00:12:48,220
top of user level threads can be bound

00:12:45,460 --> 00:12:50,800
to a scheduler associated with a certain

00:12:48,220 --> 00:12:53,050
place so this access control is done by

00:12:50,800 --> 00:12:54,910
a place kill the ult in the place Q can

00:12:53,050 --> 00:12:58,960
be taken only by the associative

00:12:54,910 --> 00:13:00,910
schedulers for example for example all

00:12:58,960 --> 00:13:02,380
right so the schedulers and this

00:13:00,910 --> 00:13:04,420
scheduler can take these user level

00:13:02,380 --> 00:13:06,310
threads because it's in this place

00:13:04,420 --> 00:13:08,350
however

00:13:06,310 --> 00:13:11,020
you cannot take this use all of us rest

00:13:08,350 --> 00:13:14,320
because this place I mean this core is

00:13:11,020 --> 00:13:16,600
this core is not belonging to this place

00:13:14,320 --> 00:13:19,420
cube this is okay because this is a

00:13:16,600 --> 00:13:21,340
shared cube so it's like this and

00:13:19,420 --> 00:13:23,589
actually it works well and it's standard

00:13:21,340 --> 00:13:25,420
compliant but from the performance

00:13:23,589 --> 00:13:26,950
perspective it is not very optimal

00:13:25,420 --> 00:13:28,540
because op-amp is setting up in the

00:13:26,950 --> 00:13:31,870
affinity setting is a little bit too

00:13:28,540 --> 00:13:33,790
deterministic in op-amp II once the

00:13:31,870 --> 00:13:36,550
affinity is set in the parent per region

00:13:33,790 --> 00:13:38,440
the child per region also needs to

00:13:36,550 --> 00:13:41,110
follow the parent place partitioning

00:13:38,440 --> 00:13:43,330
like this so it is too restrictive to

00:13:41,110 --> 00:13:45,490
utilize random are dealing in in a polar

00:13:43,330 --> 00:13:48,510
region so for example in bold it will be

00:13:45,490 --> 00:13:51,130
like this so threads in the innermost

00:13:48,510 --> 00:13:53,170
region cannot be scheduled by other

00:13:51,130 --> 00:13:55,810
schedulers out of The Associated place

00:13:53,170 --> 00:13:57,400
it's like this it is forbidden

00:13:55,810 --> 00:13:59,190
so one promising approach is

00:13:57,400 --> 00:14:01,750
deterministic work-stealing

00:13:59,190 --> 00:14:03,730
deterministic work-sharing at the

00:14:01,750 --> 00:14:06,700
outermost peril region to speed up the

00:14:03,730 --> 00:14:08,950
work distribution but using random walk

00:14:06,700 --> 00:14:11,080
sealing in the innermost bioregion for

00:14:08,950 --> 00:14:11,950
load balancing and unfortunately it is

00:14:11,080 --> 00:14:14,260
not possible in the current

00:14:11,950 --> 00:14:16,900
specification so we suggest a new

00:14:14,260 --> 00:14:18,640
keyword and said to answer the affinity

00:14:16,900 --> 00:14:20,470
so that the channels red can be freed

00:14:18,640 --> 00:14:24,670
from the current affinity setting and

00:14:20,470 --> 00:14:26,950
run by any schedulers like this okay so

00:14:24,670 --> 00:14:29,080
let's see the performance now if you

00:14:26,950 --> 00:14:31,330
only said spread the performance is

00:14:29,080 --> 00:14:33,820
sometimes faster but not always better

00:14:31,330 --> 00:14:35,860
if the total number of spread is a large

00:14:33,820 --> 00:14:38,200
because of the rigid affinity settings

00:14:35,860 --> 00:14:39,820
if you said and said to the inner power

00:14:38,200 --> 00:14:41,890
region it can overall achieve better

00:14:39,820 --> 00:14:43,960
performance okay so in the following

00:14:41,890 --> 00:14:47,260
information we will set the affinity for

00:14:43,960 --> 00:14:50,500
bolt all right right now if nested both

00:14:47,260 --> 00:14:52,510
is most efficient however how about flat

00:14:50,500 --> 00:14:54,370
or single level power isn't the

00:14:52,510 --> 00:14:56,530
performance of bolt should be the same

00:14:54,370 --> 00:14:58,209
as others because I mean basically

00:14:56,530 --> 00:14:59,290
there's no point to use likely to use

00:14:58,209 --> 00:15:01,390
all of us right if there is no

00:14:59,290 --> 00:15:04,060
oversubscription so performance should

00:15:01,390 --> 00:15:07,420
be the same however in reality it is not

00:15:04,060 --> 00:15:09,070
the case so both is as slow as GCC or MP

00:15:07,420 --> 00:15:10,900
if we enable a special thread

00:15:09,070 --> 00:15:13,360
coordination optimization for piece rate

00:15:10,900 --> 00:15:16,030
based systems this optimization is

00:15:13,360 --> 00:15:16,780
specifically for flat pair ISM I said

00:15:16,030 --> 00:15:19,000
active

00:15:16,780 --> 00:15:21,100
to OMB weight policy for peace web-based

00:15:19,000 --> 00:15:23,650
systems and it can efficiently handle

00:15:21,100 --> 00:15:26,590
flat peril regions so what is a weight

00:15:23,650 --> 00:15:29,530
policy optimizations or NP white policy

00:15:26,590 --> 00:15:31,690
is a hint to indicate how threads wait

00:15:29,530 --> 00:15:34,030
for something so let me explain any

00:15:31,690 --> 00:15:36,850
policy optimizations so please look at

00:15:34,030 --> 00:15:39,190
the example code after this power region

00:15:36,850 --> 00:15:41,620
so we can say that threads are waiting

00:15:39,190 --> 00:15:44,020
for next Perot region in the next

00:15:41,620 --> 00:15:47,560
iteration if exists and if the weight

00:15:44,020 --> 00:15:49,890
policy is active it's red busy wait for

00:15:47,560 --> 00:15:52,060
the next era region after finishing the

00:15:49,890 --> 00:15:54,250
previous power region for a certain

00:15:52,060 --> 00:15:56,290
frame a certain period and in this case

00:15:54,250 --> 00:15:59,470
in this case the synchronization becomes

00:15:56,290 --> 00:16:00,940
a simple busy wait flag checking so like

00:15:59,470 --> 00:16:03,970
I mean who say these are actually

00:16:00,940 --> 00:16:05,980
checking only a flag so no actual folk

00:16:03,970 --> 00:16:07,930
enjoin is needed so of course such a

00:16:05,980 --> 00:16:10,600
busy wait is catastrophic if there is

00:16:07,930 --> 00:16:12,460
over subscription but I mean and also

00:16:10,600 --> 00:16:14,830
user needs to be responsible for it but

00:16:12,460 --> 00:16:17,490
if there's no other subscription yes

00:16:14,830 --> 00:16:20,440
this is more efficient on the other hand

00:16:17,490 --> 00:16:22,420
guilty based systems always revise on

00:16:20,440 --> 00:16:24,490
user level context switching and we use

00:16:22,420 --> 00:16:27,100
all the before can join so after joining

00:16:24,490 --> 00:16:29,710
yield is go back to schedulers and try

00:16:27,100 --> 00:16:32,530
to find the next available work the

00:16:29,710 --> 00:16:34,450
masters read will prepare new threads

00:16:32,530 --> 00:16:36,490
and they're picked up by schedulers like

00:16:34,450 --> 00:16:39,370
this and then switch this read and run

00:16:36,490 --> 00:16:41,140
computation so this user level

00:16:39,370 --> 00:16:43,510
scheduling is faster than all levels

00:16:41,140 --> 00:16:45,720
read suspend Emily June like Annie

00:16:43,510 --> 00:16:48,850
suspend and resume is very fast but

00:16:45,720 --> 00:16:52,110
still compared with flag checking and

00:16:48,850 --> 00:16:54,730
this user level operation is slower

00:16:52,110 --> 00:16:56,920
alright to solve this issue let's

00:16:54,730 --> 00:16:59,560
implement active policy involved so like

00:16:56,920 --> 00:17:01,600
LLVM open P if active it said it will

00:16:59,560 --> 00:17:04,209
TBC waits for the next power region for

00:17:01,600 --> 00:17:05,920
awhile if not active it said I mean the

00:17:04,209 --> 00:17:08,439
passage case the original function

00:17:05,920 --> 00:17:08,980
algorithm is used here is the

00:17:08,439 --> 00:17:12,939
performance

00:17:08,980 --> 00:17:15,250
so if nested passive is recommended to

00:17:12,939 --> 00:17:17,470
avoid wasting cpu resources by busy

00:17:15,250 --> 00:17:21,970
waiting and bodies of course faster than

00:17:17,470 --> 00:17:23,560
GCC i CCN clamp but if not nested active

00:17:21,970 --> 00:17:25,390
is recommended so then bolt is as fast

00:17:23,560 --> 00:17:27,130
as i CCN clamp it's a little bit faster

00:17:25,390 --> 00:17:29,870
but like i mean it is not obvious like

00:17:27,130 --> 00:17:32,330
in this life so I know that the previous

00:17:29,870 --> 00:17:35,510
system did not implement active policies

00:17:32,330 --> 00:17:37,670
because usually systems only focus on

00:17:35,510 --> 00:17:38,930
massive power isn't extremely speaking

00:17:37,670 --> 00:17:40,820
they don't Larry looking to the

00:17:38,930 --> 00:17:42,680
performance of flat prism so their

00:17:40,820 --> 00:17:44,330
performance of what per region is lower

00:17:42,680 --> 00:17:46,760
than those of widely used piece rate

00:17:44,330 --> 00:17:48,950
based open B systems okay that's good so

00:17:46,760 --> 00:17:51,020
in any case both performs best however

00:17:48,950 --> 00:17:53,300
as we have seen the sweat coordination

00:17:51,020 --> 00:17:56,270
optimizations significantly affects the

00:17:53,300 --> 00:17:59,180
power region overheads so if flat active

00:17:56,270 --> 00:18:00,530
is better while if nested passive is

00:17:59,180 --> 00:18:03,020
better so is there any way to

00:18:00,530 --> 00:18:05,870
transparently support both flat and

00:18:03,020 --> 00:18:09,410
nested tourism okay we first explained

00:18:05,870 --> 00:18:12,050
idea so if both active and passive cases

00:18:09,410 --> 00:18:14,450
after the completion of on be threads

00:18:12,050 --> 00:18:17,420
channels rats will enter the busy wait

00:18:14,450 --> 00:18:19,490
loop in the case of active if busy waits

00:18:17,420 --> 00:18:22,250
for a flag to restart the power region

00:18:19,490 --> 00:18:23,660
quickly in the case of passive it is in

00:18:22,250 --> 00:18:25,760
the busy loop do you find the next

00:18:23,660 --> 00:18:29,420
available work so if so can we combine

00:18:25,760 --> 00:18:31,550
these two busy loops yes so to support

00:18:29,420 --> 00:18:33,620
both flat and nested parallelism we

00:18:31,550 --> 00:18:35,540
devised a new algorithm called hybrid

00:18:33,620 --> 00:18:37,970
which performs flat check and kill check

00:18:35,540 --> 00:18:40,429
alternately in a busy loop so in the

00:18:37,970 --> 00:18:42,200
case of flat pair ISM compare is active

00:18:40,429 --> 00:18:43,850
it adds cube checking overheads but

00:18:42,200 --> 00:18:45,020
still it can research the parallel

00:18:43,850 --> 00:18:47,030
region without going back to the

00:18:45,020 --> 00:18:48,950
scheduler in the case of nested perot

00:18:47,030 --> 00:18:50,960
regions compared with passive it has a

00:18:48,950 --> 00:18:53,480
flag check overheads but still it can

00:18:50,960 --> 00:18:55,820
quickly find like ready thread in the in

00:18:53,480 --> 00:18:58,250
their tasks use ok so let's see the

00:18:55,820 --> 00:19:00,080
performance here's the performance so

00:18:58,250 --> 00:19:02,870
hybrid shows almost the same performance

00:19:00,080 --> 00:19:05,270
as flat active and nested passive it

00:19:02,870 --> 00:19:07,730
means that we don't need to choose wait

00:19:05,270 --> 00:19:09,920
policy always set hybrid and performs

00:19:07,730 --> 00:19:11,510
best it's also improved performance if

00:19:09,920 --> 00:19:13,880
nested so the left

00:19:11,510 --> 00:19:15,860
tourism is limited so left-side tourism

00:19:13,880 --> 00:19:17,630
is very limited so it is not flat but

00:19:15,860 --> 00:19:20,030
tourism is not very abundant

00:19:17,630 --> 00:19:21,410
then hybrid policy can efficiently

00:19:20,030 --> 00:19:23,330
handle such a case and shows better

00:19:21,410 --> 00:19:24,080
performance and than the passive

00:19:23,330 --> 00:19:26,390
implementation

00:19:24,080 --> 00:19:29,150
alright so summary of the talk both

00:19:26,390 --> 00:19:30,890
design and implementation so just

00:19:29,150 --> 00:19:32,780
replacing this rating layer is not

00:19:30,890 --> 00:19:34,520
enough and we apply three kinds of

00:19:32,780 --> 00:19:36,200
optimizations to fully exploit it

00:19:34,520 --> 00:19:40,050
nested parallelism without hurting the

00:19:36,200 --> 00:19:44,010
flapper ISM then it reduces overhead by

00:19:40,050 --> 00:19:45,960
ten times okay a variation and we have

00:19:44,010 --> 00:19:48,690
two micro benchmarks both of them

00:19:45,960 --> 00:19:51,210
require nested paracin the left case is

00:19:48,690 --> 00:19:52,740
balanced but insufficient paracin the

00:19:51,210 --> 00:19:54,720
inner loop only has twenty eight

00:19:52,740 --> 00:19:56,460
iterations and the other loop only has

00:19:54,720 --> 00:19:59,250
any iterations where and it's very small

00:19:56,460 --> 00:20:01,860
while the server has 56 cores to use

00:19:59,250 --> 00:20:04,410
also to use all course we need to

00:20:01,860 --> 00:20:06,510
paralyze both inner and like outer

00:20:04,410 --> 00:20:08,940
barrel regions and then here's the

00:20:06,510 --> 00:20:11,010
performance now as expected both always

00:20:08,940 --> 00:20:12,540
perform best since now we can fully

00:20:11,010 --> 00:20:14,400
utilize lightweight use over threads

00:20:12,540 --> 00:20:17,700
they're up to four times faster than

00:20:14,400 --> 00:20:19,440
Intel and LM of MP the right micro

00:20:17,700 --> 00:20:22,020
benchmark had sufficient loop iterations

00:20:19,440 --> 00:20:24,960
so in this case both iteration counter

00:20:22,020 --> 00:20:27,330
56 but the computation sizes and balance

00:20:24,960 --> 00:20:28,560
to efficiently paralyze it all the power

00:20:27,330 --> 00:20:30,930
isn't must be exploited

00:20:28,560 --> 00:20:33,810
we changed Alfred we change the degree

00:20:30,930 --> 00:20:35,430
of imbalances it shows here so with

00:20:33,810 --> 00:20:37,650
larger Alfred the computation becomes

00:20:35,430 --> 00:20:39,570
more unbalanced and then this is the

00:20:37,650 --> 00:20:42,060
performance so both always performs best

00:20:39,570 --> 00:20:44,520
up to two times faster than Intel and LM

00:20:42,060 --> 00:20:46,140
open P and I also implemented test

00:20:44,520 --> 00:20:49,170
version of this micro benchmark and

00:20:46,140 --> 00:20:51,720
compared with those of GCC and ICC and

00:20:49,170 --> 00:20:54,810
clam so actually the own pace red or

00:20:51,720 --> 00:20:56,670
bold is as fast as task loop so stress

00:20:54,810 --> 00:20:58,490
and tasks have different bottle and

00:20:56,670 --> 00:21:01,680
different functionalities in open P

00:20:58,490 --> 00:21:05,070
specifications for example thread

00:21:01,680 --> 00:21:07,710
support TLS transfer synchronization

00:21:05,070 --> 00:21:08,880
like Maria an affinity well as support

00:21:07,710 --> 00:21:10,830
dependency-based

00:21:08,880 --> 00:21:13,380
synchronization the result indicates

00:21:10,830 --> 00:21:14,940
that in terms of implementation at least

00:21:13,380 --> 00:21:16,740
there is no significant overhead

00:21:14,940 --> 00:21:18,510
difference between on this red and on P

00:21:16,740 --> 00:21:19,950
task and this argument should be more

00:21:18,510 --> 00:21:24,750
like any only difference of

00:21:19,950 --> 00:21:26,550
functionalities okay so let's see ok

00:21:24,750 --> 00:21:29,280
very time all right all right then like

00:21:26,550 --> 00:21:35,640
me we've used can mmm actually both

00:21:29,280 --> 00:21:36,570
shows best performance alright so we can

00:21:35,640 --> 00:21:39,660
go to the summary

00:21:36,570 --> 00:21:41,340
okay so basically yes Adobe para regions

00:21:39,660 --> 00:21:43,110
are commonly seen in the contemporary

00:21:41,340 --> 00:21:46,080
software stacks and the bolt can solve

00:21:43,110 --> 00:21:48,840
the problem I like that a little bit

00:21:46,080 --> 00:21:51,050
talk about the OM bolt as a software so

00:21:48,840 --> 00:21:53,360
this is developed as Earth and

00:21:51,050 --> 00:21:56,710
inaudible ecosystem so if you use bold

00:21:53,360 --> 00:21:59,900
and if you use heard about that where

00:21:56,710 --> 00:22:01,760
random systems you can have like better

00:21:59,900 --> 00:22:04,340
interoperability in terms of correctness

00:22:01,760 --> 00:22:06,140
and performance also both is part of the

00:22:04,340 --> 00:22:08,030
ECP projects so I guess there are many

00:22:06,140 --> 00:22:10,280
talks about ECP soul projects about

00:22:08,030 --> 00:22:12,260
verification and I can mean verification

00:22:10,280 --> 00:22:14,690
Suites also compiler efforts

00:22:12,260 --> 00:22:16,280
loop scheduling but both is working on

00:22:14,690 --> 00:22:18,380
runtime system so we are trying to

00:22:16,280 --> 00:22:21,530
improve the runtime part

00:22:18,380 --> 00:22:24,890
all right so you can download it from

00:22:21,530 --> 00:22:27,350
here or here you can use as well alright

00:22:24,890 --> 00:22:29,150
and you can only change the library path

00:22:27,350 --> 00:22:32,870
and then you can use it so you don't

00:22:29,150 --> 00:22:35,330
need to compile your program again ok so

00:22:32,870 --> 00:22:38,090
you can just change LD pre-rolled or LD

00:22:35,330 --> 00:22:40,580
library path then it works ok features

00:22:38,090 --> 00:22:42,350
and it supports most or P port point

00:22:40,580 --> 00:22:45,290
five features there are few exceptions

00:22:42,350 --> 00:22:47,750
on PT and on PD and past considerations

00:22:45,290 --> 00:22:50,990
but basically I'll say both supports

00:22:47,750 --> 00:22:53,270
most of the healthy significant features

00:22:50,990 --> 00:22:55,570
in open p alright that's it thank you

00:22:53,270 --> 00:22:55,570

YouTube URL: https://www.youtube.com/watch?v=BW-Ss9WSgR8


