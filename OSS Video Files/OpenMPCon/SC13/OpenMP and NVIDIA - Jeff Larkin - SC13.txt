Title: OpenMP and NVIDIA - Jeff Larkin - SC13
Publication date: 2014-12-13
Playlist: SC13
Description: 
	Jeff Larkin (NVIDIA) at Supercomputing 2013, November 2013 in Denver CO.
Captions: 
	00:00:10,890 --> 00:00:15,480
okay so thank you for coming my name is

00:00:13,950 --> 00:00:17,760
Jeff lark and I work in the developer

00:00:15,480 --> 00:00:19,740
technologies groupid nvidia which means

00:00:17,760 --> 00:00:21,930
my job is to make sure that developers

00:00:19,740 --> 00:00:24,300
get the best experience they can on our

00:00:21,930 --> 00:00:26,100
devices so I worked specifically with

00:00:24,300 --> 00:00:28,169
developers at oak ridge national lab but

00:00:26,100 --> 00:00:30,210
but a variety of developers try to get

00:00:28,169 --> 00:00:32,700
the most best performance out of the

00:00:30,210 --> 00:00:37,350
GPUs in addition to that this year I

00:00:32,700 --> 00:00:40,530
became the representative to to open mp4

00:00:37,350 --> 00:00:42,720
for NVIDIA and so I wanted to talk about

00:00:40,530 --> 00:00:47,969
what what is our position where what are

00:00:42,720 --> 00:00:50,969
we doing with with openmp so as you know

00:00:47,969 --> 00:00:52,260
openmp of course is the standard when he

00:00:50,969 --> 00:00:54,210
want to do parallel programming with

00:00:52,260 --> 00:00:57,480
with directives it's been around now for

00:00:54,210 --> 00:00:59,370
more than 15 years and especially as

00:00:57,480 --> 00:01:02,879
multi-core CPUs have become commonplace

00:00:59,370 --> 00:01:04,800
it's become really important skill for

00:01:02,879 --> 00:01:06,720
people to have to know openmp and to use

00:01:04,800 --> 00:01:09,920
open impeded to have parallelism within

00:01:06,720 --> 00:01:11,720
their codes in 2010 there was a lot of

00:01:09,920 --> 00:01:14,310
discussions going around about how

00:01:11,720 --> 00:01:17,280
openmp can be used with accelerators

00:01:14,310 --> 00:01:19,380
GPUs with things like Xeon Phi with da

00:01:17,280 --> 00:01:22,170
speeds any of these things that are not

00:01:19,380 --> 00:01:24,810
CPUs but an additive technology these

00:01:22,170 --> 00:01:26,100
additive accelerators and we wanted to

00:01:24,810 --> 00:01:27,740
be part of that discussion because we

00:01:26,100 --> 00:01:30,210
have a lot of experience obviously with

00:01:27,740 --> 00:01:32,759
parallel processors and attached

00:01:30,210 --> 00:01:36,509
parallel processors so we joined in 2010

00:01:32,759 --> 00:01:39,119
and then give me twenty eleven and then

00:01:36,509 --> 00:01:41,430
in 2012 our first real contribution that

00:01:39,119 --> 00:01:43,409
we made to OpenMP was around the team's

00:01:41,430 --> 00:01:46,829
construct which eventually became part

00:01:43,409 --> 00:01:48,060
of open mp4 and I might mention a little

00:01:46,829 --> 00:01:52,740
more about that a moment so I'll kind of

00:01:48,060 --> 00:01:55,619
glaze over that right now in 20 2013

00:01:52,740 --> 00:01:57,210
earlier this year of course open mp4 was

00:01:55,619 --> 00:02:00,360
released and that includes support for

00:01:57,210 --> 00:02:02,969
accelerators and GPU like devices and so

00:02:00,360 --> 00:02:06,869
as you can see all of this moves very

00:02:02,969 --> 00:02:08,490
very rapidly from discussions in 2010 to

00:02:06,869 --> 00:02:11,790
be just the beginnings of how can we

00:02:08,490 --> 00:02:13,710
support accelerators 22 this year having

00:02:11,790 --> 00:02:14,880
full official support for accelerators

00:02:13,710 --> 00:02:18,120
in the standard so it's moving very

00:02:14,880 --> 00:02:20,849
rapidly but the question that I keep

00:02:18,120 --> 00:02:23,099
getting asked is why on earth is nvidia

00:02:20,849 --> 00:02:24,630
care about OpenMP because we don't make

00:02:23,099 --> 00:02:26,910
shared memory parallel process

00:02:24,630 --> 00:02:28,680
and that's what people think openmp is

00:02:26,910 --> 00:02:31,080
about because traditionally that's what

00:02:28,680 --> 00:02:33,840
it has been designed for but with open

00:02:31,080 --> 00:02:36,630
mp4 of course the target directive in

00:02:33,840 --> 00:02:39,600
particular OpenMP has really broadened

00:02:36,630 --> 00:02:41,750
its scope to more devices and different

00:02:39,600 --> 00:02:44,010
types of parallelism simdi parallelism

00:02:41,750 --> 00:02:45,870
the team's style parallelism now

00:02:44,010 --> 00:02:47,970
discussed in a moment and really

00:02:45,870 --> 00:02:50,670
broadened its scope and so it's really

00:02:47,970 --> 00:02:53,370
important to to Nvidia that we are a

00:02:50,670 --> 00:02:55,650
part of this from our point of view when

00:02:53,370 --> 00:02:57,990
a developer decides they want to write

00:02:55,650 --> 00:02:59,730
their code for an NVIDIA GPU they have a

00:02:57,990 --> 00:03:03,090
choice to make of how they write that

00:02:59,730 --> 00:03:05,130
code the simplest for many users if you

00:03:03,090 --> 00:03:09,480
happen to use math libraries common math

00:03:05,130 --> 00:03:11,970
legs like the blahs and la pack sparse

00:03:09,480 --> 00:03:14,660
linear algebra dense linear algebra you

00:03:11,970 --> 00:03:18,270
can take libraries and pretty much

00:03:14,660 --> 00:03:20,250
little-to-no generate a change to your

00:03:18,270 --> 00:03:22,980
code you can drop in this library and

00:03:20,250 --> 00:03:25,500
get acceleration that's a very small

00:03:22,980 --> 00:03:27,720
class of users that can take advantage

00:03:25,500 --> 00:03:28,980
of that on the far obstinate are the

00:03:27,720 --> 00:03:31,950
parallel programming language that's

00:03:28,980 --> 00:03:33,810
that's CUDA that's opencl where you can

00:03:31,950 --> 00:03:35,820
take your code are you have the maximum

00:03:33,810 --> 00:03:38,130
flexibility of how to take the

00:03:35,820 --> 00:03:39,540
parallelism in the code map it down to

00:03:38,130 --> 00:03:41,850
the hardware but of course you have to

00:03:39,540 --> 00:03:43,470
make substantial changes to your code in

00:03:41,850 --> 00:03:47,280
order to take advantage of that and for

00:03:43,470 --> 00:03:48,840
many users that's not desirable we

00:03:47,280 --> 00:03:50,310
believe compiler directives like open em

00:03:48,840 --> 00:03:53,250
Peaks sit well in the middle they're

00:03:50,310 --> 00:03:55,380
very close to drop in acceleration that

00:03:53,250 --> 00:03:58,200
you don't make major changes to your

00:03:55,380 --> 00:03:59,940
code if very additive to your to your

00:03:58,200 --> 00:04:01,500
code but at the same time you can get a

00:03:59,940 --> 00:04:03,480
lot of the same flexibility that you can

00:04:01,500 --> 00:04:04,950
get out of a parallel programming

00:04:03,480 --> 00:04:07,350
language to really get good performance

00:04:04,950 --> 00:04:11,580
and really Express the parallelism down

00:04:07,350 --> 00:04:13,770
to the hardware level so for us having

00:04:11,580 --> 00:04:15,870
compiler directives having OpenMP is a

00:04:13,770 --> 00:04:19,650
way for us to reach a much broader set

00:04:15,870 --> 00:04:22,170
of developers back in the early parts of

00:04:19,650 --> 00:04:23,550
GPU computing these were grad students

00:04:22,170 --> 00:04:25,290
who were just really wanted to do

00:04:23,550 --> 00:04:27,210
whatever it took to eke out that last

00:04:25,290 --> 00:04:29,070
bit of performance out of the GPUs that

00:04:27,210 --> 00:04:31,530
laid the groundwork for parallel

00:04:29,070 --> 00:04:33,330
languages such as CUDA and that of

00:04:31,530 --> 00:04:37,050
course it's supposed more developers to

00:04:33,330 --> 00:04:38,310
parallel programming two GPUs but in

00:04:37,050 --> 00:04:40,260
order for us to continue to

00:04:38,310 --> 00:04:42,360
reach more and more developers we have

00:04:40,260 --> 00:04:43,380
to make gp's more accessible and open MP

00:04:42,360 --> 00:04:48,870
is one of the approaches that we're

00:04:43,380 --> 00:04:51,080
taking to do that so I want to kind of

00:04:48,870 --> 00:04:53,700
expose you to one part of the

00:04:51,080 --> 00:04:55,590
four-point-oh specification that I think

00:04:53,700 --> 00:04:58,290
is kind of doesn't get enough attention

00:04:55,590 --> 00:05:00,930
and it's one that that really does open

00:04:58,290 --> 00:05:04,080
up a new level of parallelism to open MP

00:05:00,930 --> 00:05:06,690
and makes programming open mp4 GPU as

00:05:04,080 --> 00:05:09,840
possible and that's the team's construct

00:05:06,690 --> 00:05:11,490
so here I have just a traditional

00:05:09,840 --> 00:05:15,210
parallel for loop that everyone is seen

00:05:11,490 --> 00:05:17,640
before what this says is i take this

00:05:15,210 --> 00:05:20,610
loop and i want to run the iterations

00:05:17,640 --> 00:05:22,920
this loop in parallel across a team of

00:05:20,610 --> 00:05:25,230
threads so if you imagine i'm setting

00:05:22,920 --> 00:05:27,450
OMP num threads to eight I now have a

00:05:25,230 --> 00:05:28,590
team of eight threads and the iterations

00:05:27,450 --> 00:05:31,890
this loop are going to be distributed

00:05:28,590 --> 00:05:33,600
across those threads now in open mp4 if

00:05:31,890 --> 00:05:36,510
I want to take that and move that to a

00:05:33,600 --> 00:05:39,090
device like a GPU like a DSP like a xeon

00:05:36,510 --> 00:05:41,280
phi i can add now add the target

00:05:39,090 --> 00:05:44,760
directive what the target directive does

00:05:41,280 --> 00:05:47,160
it says look at what data I need wrap

00:05:44,760 --> 00:05:50,220
all that data up offload it to an

00:05:47,160 --> 00:05:53,130
accelerator device move my entire

00:05:50,220 --> 00:05:56,340
execution to that device and then run in

00:05:53,130 --> 00:05:58,260
parallel across a team of threads and so

00:05:56,340 --> 00:06:01,640
that's the first step towards getting an

00:05:58,260 --> 00:06:05,910
accelerator working with your code now

00:06:01,640 --> 00:06:07,650
for certain types of devices that's a

00:06:05,910 --> 00:06:10,320
great solution that that's far enough

00:06:07,650 --> 00:06:12,990
but for a massively parallel processor

00:06:10,320 --> 00:06:15,600
like a GPU it's really beneficial expose

00:06:12,990 --> 00:06:17,460
one more level of parallelism and in

00:06:15,600 --> 00:06:20,280
OpenMP the way you do that is with the

00:06:17,460 --> 00:06:22,140
team's construct and so here for

00:06:20,280 --> 00:06:24,510
convenience i'm actually using that the

00:06:22,140 --> 00:06:26,370
combined constructs in my code so

00:06:24,510 --> 00:06:29,700
similar to a parallel for here i'm using

00:06:26,370 --> 00:06:32,040
a target teams directive and so what

00:06:29,700 --> 00:06:34,620
this does is it creates what we call a

00:06:32,040 --> 00:06:37,800
league of teams so we're before we had

00:06:34,620 --> 00:06:40,200
one team of threads working on this loop

00:06:37,800 --> 00:06:42,480
now we can have many teams we can have

00:06:40,200 --> 00:06:44,430
what we can still have one or on a

00:06:42,480 --> 00:06:46,860
device that is massively parallel and

00:06:44,430 --> 00:06:50,250
really wants a lot of teams we can have

00:06:46,860 --> 00:06:51,960
many more and so we create a league of

00:06:50,250 --> 00:06:54,360
team and then i'm using the distributive

00:06:51,960 --> 00:06:56,340
directive to take that same loop and I

00:06:54,360 --> 00:06:58,650
say first what I want you to break it up

00:06:56,340 --> 00:07:01,380
and distribute it across my team so each

00:06:58,650 --> 00:07:04,110
team has some sections some chunk of

00:07:01,380 --> 00:07:07,289
that loop and then within each team run

00:07:04,110 --> 00:07:10,169
it in parallel so here's here's what it

00:07:07,289 --> 00:07:12,060
looks like so here is my first example I

00:07:10,169 --> 00:07:15,150
showed you why I have an open MP target

00:07:12,060 --> 00:07:16,740
and an open MP parallel for and so the

00:07:15,150 --> 00:07:19,139
execution and the data is going to get

00:07:16,740 --> 00:07:22,530
offloaded to the device and run in

00:07:19,139 --> 00:07:25,880
parallel on say as i said before 816

00:07:22,530 --> 00:07:29,070
some some fixed number of threads now

00:07:25,880 --> 00:07:30,870
for some devices this is perfect but for

00:07:29,070 --> 00:07:33,660
a GPU which is a massively parallel

00:07:30,870 --> 00:07:35,160
device we really want to expose the

00:07:33,660 --> 00:07:38,490
parallelism a little bit differently and

00:07:35,160 --> 00:07:40,919
so in this example what I've done is by

00:07:38,490 --> 00:07:42,900
introducing the team's you see here I'm

00:07:40,919 --> 00:07:45,659
doing the same computation I'm starting

00:07:42,900 --> 00:07:49,289
at zero I'm ending a 10-1 but now I've

00:07:45,659 --> 00:07:51,630
broken it up in a way that can run well

00:07:49,289 --> 00:07:53,190
on a massively parallel device each of

00:07:51,630 --> 00:07:55,650
these teams is independent of each other

00:07:53,190 --> 00:07:57,539
I'm getting all of the same work the

00:07:55,650 --> 00:08:01,740
work done and what I really like about

00:07:57,539 --> 00:08:04,199
these combined constructs is if I'm

00:08:01,740 --> 00:08:06,930
running on a device that really just

00:08:04,199 --> 00:08:10,169
needs one team well the team's contract

00:08:06,930 --> 00:08:12,509
says will run on one team if you're

00:08:10,169 --> 00:08:14,820
running on device that wants many teams

00:08:12,509 --> 00:08:17,520
once that level parallelism you can use

00:08:14,820 --> 00:08:18,990
the same code to achieve that so on the

00:08:17,520 --> 00:08:21,780
surface I don't even need to think about

00:08:18,990 --> 00:08:23,310
how to break up my code I'd have to

00:08:21,780 --> 00:08:25,229
think about the complexities of what if

00:08:23,310 --> 00:08:27,210
my block size doesn't divide into my

00:08:25,229 --> 00:08:29,630
loop nest I don't listen I have to think

00:08:27,210 --> 00:08:29,630
about that

00:08:31,669 --> 00:08:38,079
so

00:08:34,599 --> 00:08:39,490
so what the question is what am i

00:08:38,079 --> 00:08:41,199
showing with these question marks and

00:08:39,490 --> 00:08:43,419
what I'm showing is I'm starting if

00:08:41,199 --> 00:08:45,759
still starting at zero I'm ending it in

00:08:43,419 --> 00:08:48,370
but I allow the compiler in the run time

00:08:45,759 --> 00:08:50,199
to decide how to divide this up now I

00:08:48,370 --> 00:08:52,750
have the control that I could go back

00:08:50,199 --> 00:08:56,350
and define that explicitly but I also

00:08:52,750 --> 00:08:58,149
can can allow the compiler to generate

00:08:56,350 --> 00:09:00,220
these intermediate values for me so

00:08:58,149 --> 00:09:02,800
whereas i could have explicitly broken

00:09:00,220 --> 00:09:05,560
this up and say go from I equals 0 to

00:09:02,800 --> 00:09:07,630
block size minus 1 i equals block size

00:09:05,560 --> 00:09:09,880
22 block size minus 1 and break it up

00:09:07,630 --> 00:09:11,350
explicitly here all of that is already

00:09:09,880 --> 00:09:14,019
done by the compiler so I don't have to

00:09:11,350 --> 00:09:16,509
think about that complexity and what's

00:09:14,019 --> 00:09:19,240
really nice is again on a device where

00:09:16,509 --> 00:09:22,410
one team is sufficient will run one team

00:09:19,240 --> 00:09:25,000
from 0 to n minus 1 on a device where

00:09:22,410 --> 00:09:26,800
where I want this level of parallelism I

00:09:25,000 --> 00:09:29,529
can allow the compiler and the run time

00:09:26,800 --> 00:09:30,880
to do the more complex part here now I

00:09:29,529 --> 00:09:32,589
could have done explicitly in an

00:09:30,880 --> 00:09:35,380
examples document there is an example

00:09:32,589 --> 00:09:38,880
that shows it explicitly blocked up here

00:09:35,380 --> 00:09:38,880
I'm allowing the compiler to do it

00:09:40,550 --> 00:09:46,490
so this is just the first step in the

00:09:43,910 --> 00:09:49,910
evolution of openmp Michael longest dust

00:09:46,490 --> 00:09:53,029
if the openmp buff two days ago that

00:09:49,910 --> 00:09:56,690
openmp is taking on a new mission

00:09:53,029 --> 00:09:58,190
statement to really drive for new levels

00:09:56,690 --> 00:10:00,890
of peril as new ways to exposure

00:09:58,190 --> 00:10:02,660
parallelism new types of devices from

00:10:00,890 --> 00:10:04,850
not what we've traditionally thought up

00:10:02,660 --> 00:10:07,370
for OpenMP and we continue to want to be

00:10:04,850 --> 00:10:11,089
part of that discussion hardware

00:10:07,370 --> 00:10:12,380
parallelism is not going away any number

00:10:11,089 --> 00:10:14,360
of talks here you could have attended

00:10:12,380 --> 00:10:16,670
this week that demonstrate the fact that

00:10:14,360 --> 00:10:18,649
more and more parallelism is becoming

00:10:16,670 --> 00:10:20,779
the norm from the top of the top 500

00:10:18,649 --> 00:10:23,660
list of your laptops and programmers

00:10:20,779 --> 00:10:26,120
absolutely want a simple portable way to

00:10:23,660 --> 00:10:29,450
program for that and openmp is a great

00:10:26,120 --> 00:10:31,670
solution for that we're really working

00:10:29,450 --> 00:10:34,160
for you for not Oh is one step in the

00:10:31,670 --> 00:10:36,529
process we're working on for dot one

00:10:34,160 --> 00:10:39,980
we're working a 50 and going forward how

00:10:36,529 --> 00:10:42,170
we can extend openmp to a broader range

00:10:39,980 --> 00:10:44,360
of devices and from my perspective I

00:10:42,170 --> 00:10:47,000
believe three things I want to call out

00:10:44,360 --> 00:10:49,480
is my goals is improved interoperability

00:10:47,000 --> 00:10:52,100
with other programming models so that

00:10:49,480 --> 00:10:54,890
can mean anything from cuda and opencl

00:10:52,100 --> 00:10:57,529
to P threads to any other programming

00:10:54,890 --> 00:11:00,620
model make OpenMP interoperate with

00:10:57,529 --> 00:11:03,170
other options improved portability I

00:11:00,620 --> 00:11:05,120
want to make sure that that you as a

00:11:03,170 --> 00:11:07,370
programmer when you write your code it

00:11:05,120 --> 00:11:09,020
will run well and perform well on a wide

00:11:07,370 --> 00:11:10,459
variety devices without having to make

00:11:09,020 --> 00:11:13,790
significant changes that's one of the

00:11:10,459 --> 00:11:15,890
real drawers of OpenMP is the fact that

00:11:13,790 --> 00:11:17,540
you can have a single code base and

00:11:15,890 --> 00:11:19,760
lastly I really feel like there's room

00:11:17,540 --> 00:11:21,230
for a better express ability different

00:11:19,760 --> 00:11:23,329
types of parallelism higher-level

00:11:21,230 --> 00:11:26,959
constructs and so I'd like to really see

00:11:23,329 --> 00:11:28,790
open and P tackle that as well so that's

00:11:26,959 --> 00:11:31,990
all I had to say are there any more

00:11:28,790 --> 00:11:31,990
questions yes

00:11:36,120 --> 00:11:42,160
so the question is what is the status of

00:11:39,430 --> 00:11:43,660
compilers for open mp4 dotto target and

00:11:42,160 --> 00:11:47,890
nvidia and currently there are no

00:11:43,660 --> 00:11:49,780
compilers yet I there I can't speak to a

00:11:47,890 --> 00:11:52,240
road map there they're absolutely will

00:11:49,780 --> 00:11:54,790
be some I can't really speak to a road

00:11:52,240 --> 00:11:58,210
map right now but you know we're aware

00:11:54,790 --> 00:11:59,320
of the fact that right now you don't

00:11:58,210 --> 00:12:01,300
have an option but we're definitely

00:11:59,320 --> 00:12:06,880
driving to try to give you that option

00:12:01,300 --> 00:12:09,040
as fast as possible so yes portable one

00:12:06,880 --> 00:12:12,280
is easy to use thinking if you already

00:12:09,040 --> 00:12:14,950
have a deep understanding you dented

00:12:12,280 --> 00:12:16,670
target that tells not students

00:12:14,950 --> 00:12:20,300
warlock bye

00:12:16,670 --> 00:12:21,400
just and distributed raised as well we

00:12:20,300 --> 00:12:28,300
are super loop

00:12:21,400 --> 00:12:30,760
you have to get up so the question is

00:12:28,300 --> 00:12:33,010
around that in my example you needed the

00:12:30,760 --> 00:12:35,980
teams and the distribute to get the best

00:12:33,010 --> 00:12:38,110
for so on a GPU today to get the best

00:12:35,980 --> 00:12:39,850
performance you would would need the

00:12:38,110 --> 00:12:41,770
teams and distribute just because of the

00:12:39,850 --> 00:12:43,150
way our architecture is designed it

00:12:41,770 --> 00:12:46,690
expects that additional level

00:12:43,150 --> 00:12:48,580
parallelism now we're aware that that

00:12:46,690 --> 00:12:52,210
that is a different model than what

00:12:48,580 --> 00:12:54,550
people have traditionally done and we do

00:12:52,210 --> 00:12:56,890
have a internal roadmap I can't discuss

00:12:54,550 --> 00:12:58,390
it of how to make that simpler in the

00:12:56,890 --> 00:13:00,490
future so that you may not necessarily

00:12:58,390 --> 00:13:02,260
need to do that that extra step I

00:13:00,490 --> 00:13:04,870
believe that what I've shown here is

00:13:02,260 --> 00:13:06,640
portable in that it will run well on a

00:13:04,870 --> 00:13:08,770
variety devices by recognized it is

00:13:06,640 --> 00:13:11,280
different than a traditional loops that

00:13:08,770 --> 00:13:11,280
are already there

00:13:13,529 --> 00:13:20,430
yes exactly and so the question is you

00:13:16,709 --> 00:13:22,170
know how do we get there right now you

00:13:20,430 --> 00:13:24,480
will have to make that sort of change if

00:13:22,170 --> 00:13:26,370
you want to run on GPU type device and

00:13:24,480 --> 00:13:31,699
then you know it's up to us to make that

00:13:26,370 --> 00:13:31,699
even easier for you in the future so yes

00:13:32,899 --> 00:13:40,050
so the question is around OpenMP and

00:13:35,610 --> 00:13:41,579
open ACC right now open ACC has has a

00:13:40,050 --> 00:13:44,160
market advantage and that they've came

00:13:41,579 --> 00:13:45,389
out sooner than the nopin mp4 target so

00:13:44,160 --> 00:13:48,749
they've solved some of the problems that

00:13:45,389 --> 00:13:50,879
we haven't been resolved yet almost

00:13:48,749 --> 00:13:53,009
every member of the openmp accelerate

00:13:50,879 --> 00:13:54,930
working group is also on open ACC so

00:13:53,009 --> 00:13:56,490
we're really in close discussions to

00:13:54,930 --> 00:13:58,499
make sure that we can converge on a

00:13:56,490 --> 00:14:01,879
common set of features open ACC is

00:13:58,499 --> 00:14:05,600
laser-focused it's we are working on

00:14:01,879 --> 00:14:08,100
accelerators solely OpenMP takes a more

00:14:05,600 --> 00:14:09,660
conservative approach and is really

00:14:08,100 --> 00:14:12,029
going to learn from that experience so

00:14:09,660 --> 00:14:14,819
that we can drive and arrive at a common

00:14:12,029 --> 00:14:17,370
set of features so today there are

00:14:14,819 --> 00:14:19,019
differences open ACC has some advantages

00:14:17,370 --> 00:14:20,879
in some places open MP has advantages

00:14:19,019 --> 00:14:23,250
and other in the future the two

00:14:20,879 --> 00:14:25,879
standards are driving towards a common

00:14:23,250 --> 00:14:25,879
feature set

00:14:27,209 --> 00:14:31,199
that's hard to say I mean in the near

00:14:29,519 --> 00:14:32,490
term they're absolutely there's going to

00:14:31,199 --> 00:14:34,769
be two standards for the near term in

00:14:32,490 --> 00:14:37,589
the future what I what I see is that we

00:14:34,769 --> 00:14:41,249
will converge upon common features

00:14:37,589 --> 00:14:42,540
common capabilities and then it will be

00:14:41,249 --> 00:14:46,069
up to the market to decide whether we

00:14:42,540 --> 00:14:46,069
want to sin taxes or one syntax

00:14:48,490 --> 00:14:51,720

YouTube URL: https://www.youtube.com/watch?v=p6hQETqT8Qc


