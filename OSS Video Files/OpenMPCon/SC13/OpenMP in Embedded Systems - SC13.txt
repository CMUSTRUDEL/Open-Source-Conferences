Title: OpenMP in Embedded Systems - SC13
Publication date: 2014-12-13
Playlist: SC13
Description: 
	Sunita Chandrasekaran (U. of Houston) and Sven Brehmer (PolyCore Software)
Presentation at the OpenMP ARB booth at Supercomputing 2013 Denver - November 19, 2013. 
http://openmp.org/
Captions: 
	00:00:00,000 --> 00:00:03,360
so what we have done is we are in

00:00:01,949 --> 00:00:05,730
collaboration with Freescale

00:00:03,360 --> 00:00:07,319
Semiconductor as well as this consortium

00:00:05,730 --> 00:00:10,050
called semiconductor research

00:00:07,319 --> 00:00:12,450
corporation where different

00:00:10,050 --> 00:00:15,780
semiconductor company is actually worked

00:00:12,450 --> 00:00:17,460
in in sync with the SRC guys and we

00:00:15,780 --> 00:00:18,570
submit proposals to SRC and it gets

00:00:17,460 --> 00:00:22,199
funded by one of the semiconductor

00:00:18,570 --> 00:00:23,939
companies so that's the history so what

00:00:22,199 --> 00:00:26,789
we have done is we have created a

00:00:23,939 --> 00:00:30,359
portable runtime library based off of

00:00:26,789 --> 00:00:34,469
openmp for multi-core embedded systems

00:00:30,359 --> 00:00:36,930
the target platform could be anything

00:00:34,469 --> 00:00:40,079
like three scales or texas instruments

00:00:36,930 --> 00:00:42,270
or Qualcomm's so the idea is to keep the

00:00:40,079 --> 00:00:45,870
target platform as versatile as possible

00:00:42,270 --> 00:00:47,760
as flexible as possible so saying is the

00:00:45,870 --> 00:00:49,710
student who has been working on is a PhD

00:00:47,760 --> 00:00:54,510
student and I'm from dr. Chapman's group

00:00:49,710 --> 00:00:57,120
a University of Houston so that was just

00:00:54,510 --> 00:00:59,250
a slight credit to draw attention where

00:00:57,120 --> 00:01:02,969
we're all embedded system is used to be

00:00:59,250 --> 00:01:05,339
at home or be it Oh didn't see that was

00:01:02,969 --> 00:01:06,780
going away so oh yeah so different

00:01:05,339 --> 00:01:09,689
places where embedded system is very

00:01:06,780 --> 00:01:13,080
prominent and that makes it even more

00:01:09,689 --> 00:01:16,320
necessary to have a strong programming

00:01:13,080 --> 00:01:18,720
paradigm to program such devices that's

00:01:16,320 --> 00:01:22,710
the Texas Instruments embedded system

00:01:18,720 --> 00:01:24,990
board which is an 84 EVM platform and

00:01:22,710 --> 00:01:28,590
it's got the cup it's got your a levels

00:01:24,990 --> 00:01:32,250
of cache as well as you can see this one

00:01:28,590 --> 00:01:33,930
is there free scales p48 ii ii ii find

00:01:32,250 --> 00:01:36,720
at MC power core processor which we have

00:01:33,930 --> 00:01:39,350
used in our work we have used to core as

00:01:36,720 --> 00:01:41,970
well as an eight-core our processor for

00:01:39,350 --> 00:01:44,640
evaluating our work it's a target

00:01:41,970 --> 00:01:47,820
platform so if you notice this board has

00:01:44,640 --> 00:01:50,880
a little a unique accelerator where this

00:01:47,820 --> 00:01:53,070
pattern matching engine and sec it's not

00:01:50,880 --> 00:01:55,439
one of those usual accelerator that you

00:01:53,070 --> 00:01:58,770
would see so these are very specific and

00:01:55,439 --> 00:02:00,719
tuned to run specific image processing

00:01:58,770 --> 00:02:03,180
applications or security based

00:02:00,719 --> 00:02:04,979
applications only so that made our work

00:02:03,180 --> 00:02:07,229
a little bit narrowed down so what I

00:02:04,979 --> 00:02:11,099
would like to do is a power processor +

00:02:07,229 --> 00:02:13,910
DSP kind of a target platform so because

00:02:11,099 --> 00:02:16,680
these are very unique and they are very

00:02:13,910 --> 00:02:18,780
specifies accelerators programming them

00:02:16,680 --> 00:02:22,380
was even more complicated they use

00:02:18,780 --> 00:02:25,710
something called DPA a data path I

00:02:22,380 --> 00:02:27,090
forget the what 81 is architecture and I

00:02:25,710 --> 00:02:29,700
don't I forget what is the other a

00:02:27,090 --> 00:02:32,760
standing for Saudi PA are super low

00:02:29,700 --> 00:02:35,040
level ap is and you really need to know

00:02:32,760 --> 00:02:36,690
the architecture of p.m. en su see if

00:02:35,040 --> 00:02:39,450
you want to program these accelerators

00:02:36,690 --> 00:02:41,370
so the idea was do we really want to go

00:02:39,450 --> 00:02:43,860
down that lower level and know the

00:02:41,370 --> 00:02:46,860
entire background off the low-level API

00:02:43,860 --> 00:02:48,840
is as well as the target platform to

00:02:46,860 --> 00:02:51,540
program these or can we abstract all

00:02:48,840 --> 00:02:55,890
these details and go a higher level

00:02:51,540 --> 00:02:58,020
higher okay so since you guys have a

00:02:55,890 --> 00:02:59,880
background of why we require better

00:02:58,020 --> 00:03:01,890
programming pattern programming model I

00:02:59,880 --> 00:03:04,410
wouldn't go deeper into it so the idea

00:03:01,890 --> 00:03:06,570
here is you need to paralyze existing

00:03:04,410 --> 00:03:08,160
codes you need an incremental migration

00:03:06,570 --> 00:03:11,130
path that's where openmp comes into

00:03:08,160 --> 00:03:12,900
pictures and so you keep you do these

00:03:11,130 --> 00:03:14,070
iterations you keep plugging in certain

00:03:12,900 --> 00:03:16,320
partners and you want to improve your

00:03:14,070 --> 00:03:18,110
hold better and so you go further make

00:03:16,320 --> 00:03:20,550
your code even more paralyzed herbal and

00:03:18,110 --> 00:03:23,400
you want to exploit multiple levels of

00:03:20,550 --> 00:03:25,350
parallelism again not all programming

00:03:23,400 --> 00:03:28,320
models are equal and is perfect and

00:03:25,350 --> 00:03:30,360
that's where we need to use the industry

00:03:28,320 --> 00:03:32,580
standards that's where you want to be

00:03:30,360 --> 00:03:35,190
able to create one level of programming

00:03:32,580 --> 00:03:36,780
or with one unified layer layer and be

00:03:35,190 --> 00:03:39,269
able to target more than one kind of

00:03:36,780 --> 00:03:42,450
platform underneath so in order to do

00:03:39,269 --> 00:03:44,519
that we chose OpenMP as our starting

00:03:42,450 --> 00:03:47,220
point again I guess you have good

00:03:44,519 --> 00:03:50,160
background you know what is OpenMP so

00:03:47,220 --> 00:03:53,700
I'm decide to have the slide to say from

00:03:50,160 --> 00:03:56,970
its inception until today how far OpenMP

00:03:53,700 --> 00:03:58,410
has gone and the i highlighted this word

00:03:56,970 --> 00:04:01,830
portable because that's the key word

00:03:58,410 --> 00:04:03,000
when you use this openmp model for an

00:04:01,830 --> 00:04:05,610
embedded platform and that's the sole

00:04:03,000 --> 00:04:07,950
purpose why we are using openmp portable

00:04:05,610 --> 00:04:09,720
so we want to keep the code as portable

00:04:07,950 --> 00:04:14,580
as possible to target more than one type

00:04:09,720 --> 00:04:16,709
of platform okay so we have OpenMP works

00:04:14,580 --> 00:04:18,299
for shared memory processors we have 40

00:04:16,709 --> 00:04:21,180
where you have the 0 capillary for

00:04:18,299 --> 00:04:22,440
accelerators etc so what we're trying to

00:04:21,180 --> 00:04:24,570
make I establish a connection between

00:04:22,440 --> 00:04:27,350
what OpenMP can do and what amulet

00:04:24,570 --> 00:04:30,230
system requires so for example

00:04:27,350 --> 00:04:32,630
we want to be able to concentrate on the

00:04:30,230 --> 00:04:34,910
algorithm itself and not on the lower

00:04:32,630 --> 00:04:36,500
level details of concurrency which is an

00:04:34,910 --> 00:04:39,230
important factor for embedded system

00:04:36,500 --> 00:04:41,840
itself and openmp runtime relies on

00:04:39,230 --> 00:04:44,240
lower level components such as threading

00:04:41,840 --> 00:04:46,250
OS and etc but sometimes you have you

00:04:44,240 --> 00:04:49,460
don't have a I don't have an OS at all

00:04:46,250 --> 00:04:52,370
so how would you actually use OpenMP on

00:04:49,460 --> 00:04:53,750
a you know bare metal platform and every

00:04:52,370 --> 00:04:56,000
system typically lacks some of these

00:04:53,750 --> 00:04:58,220
features so what you do then thus OpenMP

00:04:56,000 --> 00:05:01,100
have that vocabulary if not how you can

00:04:58,220 --> 00:05:03,410
facilitate and use openmp on embedded

00:05:01,100 --> 00:05:05,510
platforms yeah so okay vocabulary for

00:05:03,410 --> 00:05:08,810
heterogeneity required for the embedded

00:05:05,510 --> 00:05:11,420
space so what we do is that's the

00:05:08,810 --> 00:05:13,850
programming stack that we that we have

00:05:11,420 --> 00:05:16,580
worked on we start with openmp

00:05:13,850 --> 00:05:20,090
applications and we create a translation

00:05:16,580 --> 00:05:22,340
of openmp to these AP ice now this is

00:05:20,090 --> 00:05:24,380
when spin is going to give you a much

00:05:22,340 --> 00:05:27,110
more elaborate idea of what these api's

00:05:24,380 --> 00:05:29,330
are and are your stands for resource

00:05:27,110 --> 00:05:31,910
management c is communication and th

00:05:29,330 --> 00:05:34,400
task management these are these belong

00:05:31,910 --> 00:05:36,410
to MCA api's which is again an industry

00:05:34,400 --> 00:05:37,970
standard put together by lot of

00:05:36,410 --> 00:05:40,070
semiconductor industries for the sole

00:05:37,970 --> 00:05:43,010
purpose of creating an industry standard

00:05:40,070 --> 00:05:44,630
for embedded systems so what we do is we

00:05:43,010 --> 00:05:47,960
translate we did a translation between

00:05:44,630 --> 00:05:51,020
OpenMP and this and we bypass all sorts

00:05:47,960 --> 00:05:53,630
of lower-level details of the platform

00:05:51,020 --> 00:05:56,210
embedded platform and we use our

00:05:53,630 --> 00:05:58,250
translation layer directly on the device

00:05:56,210 --> 00:06:00,160
itself so that's where the

00:05:58,250 --> 00:06:03,470
implementation itself comes into picture

00:06:00,160 --> 00:06:05,750
so translate open mp4 MP socks and we

00:06:03,470 --> 00:06:08,420
use the MCA api's as a target for our

00:06:05,750 --> 00:06:10,460
translation and we wanted to keep it

00:06:08,420 --> 00:06:12,350
light weight which is the key idea when

00:06:10,460 --> 00:06:14,960
you want to target an emirate platform

00:06:12,350 --> 00:06:17,420
suppose non-cash coherent systems is

00:06:14,960 --> 00:06:20,030
again a vital point and performance what

00:06:17,420 --> 00:06:22,030
we do is we compare our performance to a

00:06:20,030 --> 00:06:24,320
proprietary vendor specific

00:06:22,030 --> 00:06:27,140
implementation to evaluate where we

00:06:24,320 --> 00:06:30,560
stand and in what we have done and the

00:06:27,140 --> 00:06:33,440
results were pretty cool that's the

00:06:30,560 --> 00:06:35,810
compilation process so what we do is we

00:06:33,440 --> 00:06:38,410
I'm from University of Houston and we

00:06:35,810 --> 00:06:40,720
have an open source compiler that we use

00:06:38,410 --> 00:06:43,630
open you H which is the front end

00:06:40,720 --> 00:06:45,820
source to source compiler and it is

00:06:43,630 --> 00:06:47,770
downloadable from our website and there

00:06:45,820 --> 00:06:49,390
is an email id if you want to ask if you

00:06:47,770 --> 00:06:55,690
have any issues with downloading and

00:06:49,390 --> 00:06:58,870
using it contact us it's it's a GCC

00:06:55,690 --> 00:07:00,430
based it's Rita branch from open 64

00:06:58,870 --> 00:07:04,420
itself University of Delaware that's

00:07:00,430 --> 00:07:06,790
where it started so what we do is we

00:07:04,420 --> 00:07:09,280
create multi-threaded applications out

00:07:06,790 --> 00:07:11,140
of a C chord like we take a C chord put

00:07:09,280 --> 00:07:13,600
in openmp pragmas create a parallelized

00:07:11,140 --> 00:07:16,620
version and that's our starting point so

00:07:13,600 --> 00:07:19,930
that's a PP dot C like a test code and

00:07:16,620 --> 00:07:21,760
we translate that into the intermediate

00:07:19,930 --> 00:07:24,400
representation of open you H which we

00:07:21,760 --> 00:07:26,440
call the W to CW is that the open E

00:07:24,400 --> 00:07:28,750
which has five different levels of

00:07:26,440 --> 00:07:31,930
intermediate representation the idea of

00:07:28,750 --> 00:07:34,810
using open UAH was we wanted NIR which

00:07:31,930 --> 00:07:37,360
is closer to the source code than to the

00:07:34,810 --> 00:07:40,090
lower level code itself so that's where

00:07:37,360 --> 00:07:43,480
the translation becomes tad easier so we

00:07:40,090 --> 00:07:46,630
use the intermediate representation of

00:07:43,480 --> 00:07:49,600
open you H and as for back in that was

00:07:46,630 --> 00:07:52,210
the PowerPC GCC provided to us by the

00:07:49,600 --> 00:07:53,770
freescale semiconductor guys we use that

00:07:52,210 --> 00:07:55,870
to generate the object file libraries

00:07:53,770 --> 00:07:58,450
and here is where we create a link

00:07:55,870 --> 00:08:02,169
between the openmp runtime as well as

00:07:58,450 --> 00:08:04,900
the MCA build libraries and the target

00:08:02,169 --> 00:08:07,450
platform we started with the dual pol

00:08:04,900 --> 00:08:09,640
power processor the IBM power processors

00:08:07,450 --> 00:08:12,040
basically the board is belonging to

00:08:09,640 --> 00:08:14,740
freescale semiconductor and we've moved

00:08:12,040 --> 00:08:16,780
on gradually to an 8 quart which showed

00:08:14,740 --> 00:08:19,720
that the implementation that we created

00:08:16,780 --> 00:08:25,240
didn't quite break or it was scalable to

00:08:19,720 --> 00:08:27,669
more than to force so that's a case

00:08:25,240 --> 00:08:32,080
study that we used so we have considered

00:08:27,669 --> 00:08:33,940
applications from my bench which is an

00:08:32,080 --> 00:08:35,950
embedded benchmark suite and we have

00:08:33,940 --> 00:08:38,400
also used applications from Rodinia

00:08:35,950 --> 00:08:41,890
which is a heterogeneous benchmark suite

00:08:38,400 --> 00:08:45,280
mostly meant for CPU GPUs but we used

00:08:41,890 --> 00:08:46,980
some smaller boat surface but was usable

00:08:45,280 --> 00:08:49,529
for embedded systems

00:08:46,980 --> 00:08:51,420
so Dykstra is from my bench embedded

00:08:49,529 --> 00:08:54,839
benchmark suite that's the openmp

00:08:51,420 --> 00:08:58,610
version for you and that's the

00:08:54,839 --> 00:09:01,079
translation of openmp using open you H

00:08:58,610 --> 00:09:03,600
compiler so intermediate representation

00:09:01,079 --> 00:09:06,870
we call that intermediate representation

00:09:03,600 --> 00:09:10,410
as w to see w signing for world to see

00:09:06,870 --> 00:09:12,839
and saudi this is comprehensible and if

00:09:10,410 --> 00:09:14,579
you go levels down few more levels down

00:09:12,839 --> 00:09:16,290
in the compiler it becomes beyond

00:09:14,579 --> 00:09:18,209
comprehension at least to me i'm not a

00:09:16,290 --> 00:09:21,690
compiler person for over a hunt I'm

00:09:18,209 --> 00:09:24,240
person so that's a translation of the

00:09:21,690 --> 00:09:26,959
openmp itself you can see openmp for the

00:09:24,240 --> 00:09:32,550
number of threats and w 2 c's again your

00:09:26,959 --> 00:09:34,320
intermediate representation and so what

00:09:32,550 --> 00:09:36,480
we do is we try to insert the MCA

00:09:34,320 --> 00:09:39,630
routines like you can see them i'm cappy

00:09:36,480 --> 00:09:41,639
initialize so m kappa api's have these

00:09:39,630 --> 00:09:44,610
node attributes like you can define the

00:09:41,639 --> 00:09:46,860
domain the node and the topology of your

00:09:44,610 --> 00:09:50,550
architecture itself so the number of

00:09:46,860 --> 00:09:52,920
CPUs which we want and yet right so all

00:09:50,550 --> 00:09:55,410
these are again how we try to include

00:09:52,920 --> 00:09:56,880
MCA routines within the week all those

00:09:55,410 --> 00:10:01,920
em co routines within the translation

00:09:56,880 --> 00:10:03,690
itself that is the definition of the OMP

00:10:01,920 --> 00:10:06,240
see fork and again you see that we have

00:10:03,690 --> 00:10:08,880
there is a status and that is all these

00:10:06,240 --> 00:10:10,889
node IDs we have used we have solely

00:10:08,880 --> 00:10:14,160
have this implementation is based off of

00:10:10,889 --> 00:10:16,050
the resource management API of MCA like

00:10:14,160 --> 00:10:18,389
I said there is communication as well as

00:10:16,050 --> 00:10:20,870
task management and that's ongoing work

00:10:18,389 --> 00:10:20,870
so far

00:10:21,800 --> 00:10:27,380
then in the makefile just to give you an

00:10:24,860 --> 00:10:29,510
idea the process that we take to a

00:10:27,380 --> 00:10:31,730
compiler code so you see that we have

00:10:29,510 --> 00:10:35,110
linked the MCA build libraries to our

00:10:31,730 --> 00:10:38,570
open the openmp the library itself and

00:10:35,110 --> 00:10:41,990
so Dykstra and yeah so those are the

00:10:38,570 --> 00:10:45,019
steps for compilation itself and the

00:10:41,990 --> 00:10:47,120
result wise we named our portable run

00:10:45,019 --> 00:10:49,640
time that we created a slip eom p where

00:10:47,120 --> 00:10:51,350
e stands for embedded and this is just

00:10:49,640 --> 00:10:54,529
to keep it in parallel with lip balm

00:10:51,350 --> 00:10:56,510
pune-based open mpi runtime library with

00:10:54,529 --> 00:10:59,540
one thread you initialize the Emirati

00:10:56,510 --> 00:11:01,370
etc that's your execution time and then

00:10:59,540 --> 00:11:03,769
you use the eight cores I'm talking

00:11:01,370 --> 00:11:05,540
about one core and eight course of the

00:11:03,769 --> 00:11:07,970
freescale eight core power processor

00:11:05,540 --> 00:11:12,950
board and we see that the time obviously

00:11:07,970 --> 00:11:17,110
reduced from ten to one so that does

00:11:12,950 --> 00:11:19,660
this slide actually shows how the

00:11:17,110 --> 00:11:22,339
evaluation of our Libby onp on several

00:11:19,660 --> 00:11:25,130
application the takeaway from here is

00:11:22,339 --> 00:11:26,899
that's the proprietary customized

00:11:25,130 --> 00:11:29,089
runtime library from freescale and this

00:11:26,899 --> 00:11:31,279
is ours so you see that there is no

00:11:29,089 --> 00:11:33,770
overhead that we incur incurred due to

00:11:31,279 --> 00:11:35,390
the translation process and execution

00:11:33,770 --> 00:11:37,190
time wise we didn't do better than lip

00:11:35,390 --> 00:11:39,500
calm but we didn't do worse either which

00:11:37,190 --> 00:11:41,270
was good news to us because we could go

00:11:39,500 --> 00:11:43,820
back and change the algorithms and make

00:11:41,270 --> 00:11:47,360
the execution time go lower than the lib

00:11:43,820 --> 00:11:49,940
comp itself and we used EPCC micro

00:11:47,360 --> 00:11:52,220
benchmark suite to measure the overhead

00:11:49,940 --> 00:11:54,350
of our implementation and that's where

00:11:52,220 --> 00:11:56,180
we figured that oh my dear al Walden

00:11:54,350 --> 00:11:57,980
wasn't doing as well as Buddhist it was

00:11:56,180 --> 00:11:59,540
supposed to do so change the barrier

00:11:57,980 --> 00:12:01,310
will call them and we used a tournament

00:11:59,540 --> 00:12:05,029
perioral called them which did better in

00:12:01,310 --> 00:12:07,160
terms of overhead measurement so yeah so

00:12:05,029 --> 00:12:08,540
here you don't use the proprietary

00:12:07,160 --> 00:12:10,220
runtime library you don't do the

00:12:08,540 --> 00:12:12,350
low-level coding you don't need to know

00:12:10,220 --> 00:12:14,660
the underlying architecture you simply

00:12:12,350 --> 00:12:16,520
take an open MP application use the

00:12:14,660 --> 00:12:18,440
portable one time which is libby OMP

00:12:16,520 --> 00:12:20,899
target your freescale and you still

00:12:18,440 --> 00:12:24,110
almost get similar performance to what

00:12:20,899 --> 00:12:26,480
the harder way of implementation you

00:12:24,110 --> 00:12:28,100
could have taken so that forever what we

00:12:26,480 --> 00:12:31,220
are trying to say here is you achieve

00:12:28,100 --> 00:12:33,680
portability to a good extent scalability

00:12:31,220 --> 00:12:35,279
to what extent this is this is from two

00:12:33,680 --> 00:12:37,769
ports 28 course so we

00:12:35,279 --> 00:12:39,350
not lose any anything when we

00:12:37,769 --> 00:12:42,629
transference and when we did a

00:12:39,350 --> 00:12:44,339
transition from a to co2 eight core what

00:12:42,629 --> 00:12:46,560
we would want to do is try and target

00:12:44,339 --> 00:12:49,319
heterogeneous platforms like imagine you

00:12:46,560 --> 00:12:50,910
have a nucleus OS and an android OS so

00:12:49,319 --> 00:12:52,829
again that is a vocabulary for

00:12:50,910 --> 00:12:55,079
heterogeneity how would you create this

00:12:52,829 --> 00:12:57,209
unified layer to target more than one

00:12:55,079 --> 00:12:59,550
type of voice leave alone devices from

00:12:57,209 --> 00:13:01,379
different families and if you take texas

00:12:59,550 --> 00:13:04,769
instruments for example they have this

00:13:01,379 --> 00:13:06,839
48 cold ESP so we'll leave a oh NP how

00:13:04,769 --> 00:13:09,540
much effort do we need to put to libby

00:13:06,839 --> 00:13:12,059
OMP to target a platform that is not

00:13:09,540 --> 00:13:14,550
free scale so that we are in the in the

00:13:12,059 --> 00:13:17,129
face of finding all this out you're the

00:13:14,550 --> 00:13:19,949
the report that we targeted was the p40

00:13:17,129 --> 00:13:21,509
80 where i showed you those pme and sec

00:13:19,949 --> 00:13:24,899
accelerators which were quite tricky to

00:13:21,509 --> 00:13:28,019
use so and and that didn't really help

00:13:24,899 --> 00:13:29,910
me generalize the one time itself so i

00:13:28,019 --> 00:13:32,180
would want to move towards a power

00:13:29,910 --> 00:13:34,319
processor plus DSP which is

00:13:32,180 --> 00:13:38,449
traditionally the kind of Quantico that

00:13:34,319 --> 00:13:41,449
you would use yeah that's what I got

00:13:38,449 --> 00:13:41,449
that's

00:13:44,180 --> 00:13:48,779
so in the previous slide so this one I'm

00:13:46,950 --> 00:13:51,060
showing the scaling and not the actual

00:13:48,779 --> 00:13:55,110
execution time you're right the idea

00:13:51,060 --> 00:13:56,730
here was to the the concentration here

00:13:55,110 --> 00:13:58,500
was to show the scalability I see what

00:13:56,730 --> 00:14:01,650
you're saying the slide doesn't show

00:13:58,500 --> 00:14:03,839
quite that we started from a too cold

00:14:01,650 --> 00:14:06,060
like I said and I wanted to show that

00:14:03,839 --> 00:14:08,279
well we didn't have to do anything much

00:14:06,060 --> 00:14:11,250
to our libo EMP to use an eight-core

00:14:08,279 --> 00:14:13,800
platform so that was a focus and later

00:14:11,250 --> 00:14:15,120
if you want to use like 16 or 48 pours

00:14:13,800 --> 00:14:18,350
we want to make sure that the

00:14:15,120 --> 00:14:18,350
scalability is still maintained

00:14:21,279 --> 00:14:26,189
it was actually wall clock performance

00:14:28,170 --> 00:14:35,350
so the actual performance time of the

00:14:32,220 --> 00:14:38,079
wall clock time of Libby OMP compared to

00:14:35,350 --> 00:14:40,410
that of lip balm so it was almost the

00:14:38,079 --> 00:14:43,559
same except for certain applications

00:14:40,410 --> 00:14:45,819
okay i will take a step back so the

00:14:43,559 --> 00:14:49,059
implementations that used parallel and

00:14:45,819 --> 00:14:51,459
barrier we did better than lip comp by a

00:14:49,059 --> 00:14:53,889
fraction not too much that is because

00:14:51,459 --> 00:14:56,110
the EPCC micro benchmark suite helped us

00:14:53,889 --> 00:14:57,999
improve the algorithms we found there

00:14:56,110 --> 00:14:59,620
were a lot of overhead so we changed the

00:14:57,999 --> 00:15:01,959
viral Waltham's and the tournament it

00:14:59,620 --> 00:15:03,670
better but stuff like single and

00:15:01,959 --> 00:15:06,670
critical we have not done better

00:15:03,670 --> 00:15:08,290
compared to lib calm again we the

00:15:06,670 --> 00:15:11,589
execution time is not bad at all in

00:15:08,290 --> 00:15:13,029
comparison it's a fraction bit Pat so I

00:15:11,589 --> 00:15:15,790
would need to see what happens to my

00:15:13,029 --> 00:15:17,740
time when I use a different board or use

00:15:15,790 --> 00:15:19,769
small course that's why the challenges I

00:15:17,740 --> 00:15:19,769
think

00:15:23,100 --> 00:15:31,290
any more questions the Flyers that you

00:15:26,250 --> 00:15:33,570
have has the M cappy and the M cappy API

00:15:31,290 --> 00:15:36,180
is its reference cards we do not have

00:15:33,570 --> 00:15:38,430
one for MRAP e per se but if you go to

00:15:36,180 --> 00:15:41,340
the mca website multi-pole association

00:15:38,430 --> 00:15:43,260
website you can download the prototype

00:15:41,340 --> 00:15:46,980
implementation of ember RPM Cappy as

00:15:43,260 --> 00:15:49,910
well as the 100 100 150 pages of API

00:15:46,980 --> 00:15:49,910
description itself

00:15:52,630 --> 00:16:00,100
so I will hand it over to spend who is

00:15:56,590 --> 00:16:04,720
from Pollock or suspend and Marcus who

00:16:00,100 --> 00:16:06,820
is the founder of MCA we interact quite

00:16:04,720 --> 00:16:09,250
often to try and figure out the gaps in

00:16:06,820 --> 00:16:11,790
MCA and how to fix the implementation as

00:16:09,250 --> 00:16:28,150
such so that's where we got in touch

00:16:11,790 --> 00:16:31,590
I'll give you this first yeah is that

00:16:28,150 --> 00:16:31,590
good okay

00:16:34,520 --> 00:16:40,810
alright so I'm going to talk about the

00:16:36,980 --> 00:16:43,370
multi-core association which is what

00:16:40,810 --> 00:16:45,410
university of houston this is basing

00:16:43,370 --> 00:16:48,040
their OMP implementation for embedded

00:16:45,410 --> 00:16:48,040
systems on

00:16:49,769 --> 00:16:57,329
so and so i'm i'm a board member and

00:16:55,290 --> 00:17:00,300
co-founder of the multi-core association

00:16:57,329 --> 00:17:01,860
and i'm also the chairman of the mcat be

00:17:00,300 --> 00:17:04,459
working group which is one of the

00:17:01,860 --> 00:17:04,459
working groups

00:17:05,730 --> 00:17:15,030
and embedded systems they span the

00:17:11,520 --> 00:17:18,270
industry from like we have here high

00:17:15,030 --> 00:17:20,790
performance computing to very very small

00:17:18,270 --> 00:17:26,010
devices that go at very very low speed

00:17:20,790 --> 00:17:28,799
and and they almost always come in

00:17:26,010 --> 00:17:31,590
different shapes and forms so my cell

00:17:28,799 --> 00:17:35,570
phone may not be the same as yours so

00:17:31,590 --> 00:17:40,320
and they may have different processors

00:17:35,570 --> 00:17:43,320
accelerators so there is a large variety

00:17:40,320 --> 00:17:48,600
and they increasingly contains

00:17:43,320 --> 00:17:50,429
multi-core and if you look at the space

00:17:48,600 --> 00:17:53,490
that's closest to high performance

00:17:50,429 --> 00:17:56,010
computing so telecom datacom they have

00:17:53,490 --> 00:17:58,380
been around for a long time but we see

00:17:56,010 --> 00:18:02,820
them in the smallest devices that where

00:17:58,380 --> 00:18:04,470
you actually wouldn't think of it and so

00:18:02,820 --> 00:18:10,080
they're they're creeping in everywhere

00:18:04,470 --> 00:18:13,590
and obviously performance is a key

00:18:10,080 --> 00:18:19,140
driver for multi-core another one is

00:18:13,590 --> 00:18:22,860
power consumption so you know in in I

00:18:19,140 --> 00:18:25,230
think this one has for a quad core but a

00:18:22,860 --> 00:18:26,940
friend of mine has told me who is in

00:18:25,230 --> 00:18:31,049
this business is that they're rarely all

00:18:26,940 --> 00:18:33,360
for going at the same time so you you

00:18:31,049 --> 00:18:36,150
know you you clock them down or you shut

00:18:33,360 --> 00:18:38,100
them down so they're there by use you

00:18:36,150 --> 00:18:40,080
save but you have the performance when

00:18:38,100 --> 00:18:43,470
you need it then actually you can get

00:18:40,080 --> 00:18:45,990
the same performance at a lower rate

00:18:43,470 --> 00:18:49,950
than you can with a single processor so

00:18:45,990 --> 00:18:53,720
this trend is just going more and more

00:18:49,950 --> 00:18:57,750
and more and obviously also

00:18:53,720 --> 00:19:00,870
consolidation where you you have a rack

00:18:57,750 --> 00:19:03,240
today and you want to bring it to a

00:19:00,870 --> 00:19:06,929
board and maybe to a chip I mean if you

00:19:03,240 --> 00:19:10,320
look at HP is their advertising the moon

00:19:06,929 --> 00:19:14,220
shot which is a where they took lots of

00:19:10,320 --> 00:19:15,720
blades and put them in one and and we're

00:19:14,220 --> 00:19:18,780
also seeing where

00:19:15,720 --> 00:19:22,590
where you you have two or three products

00:19:18,780 --> 00:19:25,140
that you want to now make one product so

00:19:22,590 --> 00:19:28,770
it could be an analysis product that you

00:19:25,140 --> 00:19:31,110
can couple with a control product and so

00:19:28,770 --> 00:19:33,120
it may be multiple processors that could

00:19:31,110 --> 00:19:35,419
be on different chips or in the same

00:19:33,120 --> 00:19:35,419
term

00:19:36,720 --> 00:19:44,840
and so that creates a lot of challenges

00:19:40,650 --> 00:19:48,690
so we have an enormous array of

00:19:44,840 --> 00:19:52,230
possibility so Sunita talked about some

00:19:48,690 --> 00:19:55,530
of the ti chips and the freescale chips

00:19:52,230 --> 00:19:58,950
and they come with a processor some of

00:19:55,530 --> 00:20:02,549
them have CPUs and DSPs and they have a

00:19:58,950 --> 00:20:04,340
plethora of accelerators and they may

00:20:02,549 --> 00:20:07,169
have different memory architectures

00:20:04,340 --> 00:20:09,990
Sunita talked about cache coherency or

00:20:07,169 --> 00:20:13,669
not and you have local memories and

00:20:09,990 --> 00:20:17,789
shared memories and so forth so the the

00:20:13,669 --> 00:20:20,220
number of variants is enormous and and

00:20:17,789 --> 00:20:23,820
you have to look at how do you program

00:20:20,220 --> 00:20:26,850
these devices with multiple cores

00:20:23,820 --> 00:20:30,659
different types of course different

00:20:26,850 --> 00:20:32,549
operating systems and so forth and and

00:20:30,659 --> 00:20:35,490
you absolutely need the portability and

00:20:32,549 --> 00:20:38,370
the scalability so you have to your same

00:20:35,490 --> 00:20:44,510
application needs to be able to run on

00:20:38,370 --> 00:20:47,490
two cores or 16 cores or if you have a

00:20:44,510 --> 00:20:50,820
RISC processors and you have DSPs you

00:20:47,490 --> 00:20:53,789
can't afford to rebuild your application

00:20:50,820 --> 00:20:56,100
every time because they I mean most

00:20:53,789 --> 00:20:59,190
applications today they just get bigger

00:20:56,100 --> 00:21:01,669
and bigger and bigger and so you have to

00:20:59,190 --> 00:21:03,659
find ways to portably move your

00:21:01,669 --> 00:21:07,039
applications to a variety of platforms

00:21:03,659 --> 00:21:10,830
and obviously concurrency and

00:21:07,039 --> 00:21:12,929
parallelization comes in in different

00:21:10,830 --> 00:21:16,409
shapes and in different granularities so

00:21:12,929 --> 00:21:18,510
you may have paralyzed the race or you

00:21:16,409 --> 00:21:21,270
may have much much bigger chunks where

00:21:18,510 --> 00:21:25,159
you have tasks that are parallel and

00:21:21,270 --> 00:21:27,270
they interoperate and obviously we have

00:21:25,159 --> 00:21:30,289
virtualization in some systems and

00:21:27,270 --> 00:21:30,289
others we don't

00:21:30,770 --> 00:21:35,750
so the multi-core association was

00:21:32,960 --> 00:21:38,330
established in 2005 and the purpose of

00:21:35,750 --> 00:21:40,760
the organization was to enable the

00:21:38,330 --> 00:21:44,660
multi-core ecosystem to make it easier

00:21:40,760 --> 00:21:47,270
to use multi core and we have membership

00:21:44,660 --> 00:21:51,800
across the globe and it's a committee

00:21:47,270 --> 00:21:55,430
based standards development so you know

00:21:51,800 --> 00:21:57,470
we have working groups and anyone in the

00:21:55,430 --> 00:22:01,670
organization or anyone in the

00:21:57,470 --> 00:22:07,120
organization can join a working group

00:22:01,670 --> 00:22:10,490
and so it's all volunteer work and we

00:22:07,120 --> 00:22:13,040
focus in in several different areas we

00:22:10,490 --> 00:22:15,740
started here with the foundation ap is

00:22:13,040 --> 00:22:19,340
so and those were the ones that Sunita

00:22:15,740 --> 00:22:22,130
talked about so it's a communications

00:22:19,340 --> 00:22:26,480
API it's a resource management API and

00:22:22,130 --> 00:22:31,550
it's a task management API and and and

00:22:26,480 --> 00:22:36,400
then we have provided a programming

00:22:31,550 --> 00:22:43,010
guide and yo we have I don't know what

00:22:36,400 --> 00:22:46,390
okay and we also have more recently we

00:22:43,010 --> 00:22:50,210
have started a group that works on

00:22:46,390 --> 00:22:55,550
software hardware interfaces so for

00:22:50,210 --> 00:22:57,890
example you when you have a processor

00:22:55,550 --> 00:23:00,740
that has a you know ten hardware

00:22:57,890 --> 00:23:02,780
accelerators so you know I have a

00:23:00,740 --> 00:23:05,450
different app that it you know I run a

00:23:02,780 --> 00:23:07,310
business and we are in the in the multi

00:23:05,450 --> 00:23:10,040
core software business and we support

00:23:07,310 --> 00:23:12,320
different devices so you know our

00:23:10,040 --> 00:23:13,640
engineers they may have to read manuals

00:23:12,320 --> 00:23:16,340
that are hundreds and sometimes

00:23:13,640 --> 00:23:19,730
thousands of pages to figure out how a

00:23:16,340 --> 00:23:23,210
device works so what we're doing here is

00:23:19,730 --> 00:23:26,780
we're working on a way to describe the

00:23:23,210 --> 00:23:29,590
devices in a standardized way so the

00:23:26,780 --> 00:23:33,500
vendor can describe their device and and

00:23:29,590 --> 00:23:35,630
my company's tools or anyone's tools can

00:23:33,500 --> 00:23:38,840
read this information in a standardized

00:23:35,630 --> 00:23:41,780
format so instead of reading yourselves

00:23:38,840 --> 00:23:44,630
in pages we may only have to read ten so

00:23:41,780 --> 00:23:47,870
we can more quickly support those device

00:23:44,630 --> 00:23:49,760
and we're doing some other things too

00:23:47,870 --> 00:23:53,630
but I'm going to go somewhat quickly

00:23:49,760 --> 00:23:59,810
here and so again the target domain is

00:23:53,630 --> 00:24:02,480
embedded multiprocessing and and we

00:23:59,810 --> 00:24:04,640
refer to is that tire closely

00:24:02,480 --> 00:24:07,520
distributed computing so we were not

00:24:04,640 --> 00:24:09,710
really in the in the widely distributed

00:24:07,520 --> 00:24:11,870
where you have tons of computers in

00:24:09,710 --> 00:24:15,770
different locations it's really more on

00:24:11,870 --> 00:24:19,910
a chip or on a few tips on a board or in

00:24:15,770 --> 00:24:24,470
Iraq maybe the assumption is that it's a

00:24:19,910 --> 00:24:27,680
system that is there all the time you

00:24:24,470 --> 00:24:29,630
may have power down and power up but the

00:24:27,680 --> 00:24:35,930
same number of course or there every

00:24:29,630 --> 00:24:38,570
time and its targets homogeneous or

00:24:35,930 --> 00:24:40,750
heterogeneous systems and do you have a

00:24:38,570 --> 00:24:40,750
question

00:24:43,320 --> 00:24:51,730
not yet but it's going to reckon so yes

00:24:48,129 --> 00:24:53,470
so the question is in SD and more

00:24:51,730 --> 00:24:56,350
embedded systems in this houses of

00:24:53,470 --> 00:24:58,450
course and i would say today not really

00:24:56,350 --> 00:25:01,989
even though i have i have actually seen

00:24:58,450 --> 00:25:04,989
one the military system that had over a

00:25:01,989 --> 00:25:09,039
thousand years piece so but i think

00:25:04,989 --> 00:25:13,570
that's that's not so common but we we're

00:25:09,039 --> 00:25:18,789
on a system today that has 64 course

00:25:13,570 --> 00:25:20,470
it's an embedded system and but we the

00:25:18,789 --> 00:25:23,950
whole idea when you create a standard

00:25:20,470 --> 00:25:26,109
you can't you can't focus on only what's

00:25:23,950 --> 00:25:29,289
there today you have to think we'll this

00:25:26,109 --> 00:25:32,940
standard work your intent in 10 years

00:25:29,289 --> 00:25:36,609
and also very importantly to allow

00:25:32,940 --> 00:25:38,769
implementations that are possible in

00:25:36,609 --> 00:25:42,279
resource-constrained systems and where

00:25:38,769 --> 00:25:45,100
you have real time requirements so you

00:25:42,279 --> 00:25:48,009
know we are in some some big systems and

00:25:45,100 --> 00:25:50,230
we're actually now looking at going into

00:25:48,009 --> 00:25:55,799
a system where we may have a few

00:25:50,230 --> 00:25:55,799
kilobytes to work with that's it and

00:25:57,440 --> 00:26:03,259
and also it's very important to prove

00:26:00,679 --> 00:26:06,529
that the standard provides an

00:26:03,259 --> 00:26:09,110
incremental migration path so it's very

00:26:06,529 --> 00:26:11,149
rare that you start application

00:26:09,110 --> 00:26:14,059
development from scratch today you

00:26:11,149 --> 00:26:16,220
typically take whatever you had in the

00:26:14,059 --> 00:26:18,139
previous generation and you transform it

00:26:16,220 --> 00:26:21,679
to the curve through the next generation

00:26:18,139 --> 00:26:27,649
and again different granularity of

00:26:21,679 --> 00:26:30,289
parallel ism so we currently have these

00:26:27,649 --> 00:26:35,360
working groups or the communications API

00:26:30,289 --> 00:26:38,389
and we have produced version 1 and today

00:26:35,360 --> 00:26:40,610
version to subversion two came out two

00:26:38,389 --> 00:26:42,769
and a half years ago programming

00:26:40,610 --> 00:26:45,730
practices we have the first guide and

00:26:42,769 --> 00:26:50,929
we're now looking at version 2 there and

00:26:45,730 --> 00:26:53,779
resource management in the version 1 and

00:26:50,929 --> 00:26:56,659
then we have software hardware interface

00:26:53,779 --> 00:26:58,730
this is a recently started group task

00:26:56,659 --> 00:27:01,870
management has also produced its first

00:26:58,730 --> 00:27:03,919
standard and these three the

00:27:01,870 --> 00:27:06,289
communications resource and task

00:27:03,919 --> 00:27:10,610
management api's those are the ones that

00:27:06,289 --> 00:27:12,320
Sunita efforts are focused on and we

00:27:10,610 --> 00:27:15,639
also have a tools infrastructure and

00:27:12,320 --> 00:27:15,639
virtualization group

00:27:15,870 --> 00:27:24,059
and so the primary beneficiaries of

00:27:20,820 --> 00:27:27,090
standards or at least these standards

00:27:24,059 --> 00:27:30,720
are the the users of the systems or the

00:27:27,090 --> 00:27:37,559
or I should say the the OEMs and the

00:27:30,720 --> 00:27:40,110
ODMs that produces devices they can more

00:27:37,559 --> 00:27:45,960
easily get to market they will have

00:27:40,110 --> 00:27:48,450
better availability of tools and a more

00:27:45,960 --> 00:27:51,179
portable application so the the whole

00:27:48,450 --> 00:27:53,610
idea is to make it easier and and

00:27:51,179 --> 00:27:58,950
provide for a better ecosystem in the

00:27:53,610 --> 00:28:00,990
multi-core and then of course the other

00:27:58,950 --> 00:28:04,950
side of the coin is that the vendors

00:28:00,990 --> 00:28:08,670
providing tools like my company does or

00:28:04,950 --> 00:28:11,220
the chips or operating systems or

00:28:08,670 --> 00:28:13,340
middleware they can take advantage of

00:28:11,220 --> 00:28:19,950
these standards as well because you have

00:28:13,340 --> 00:28:22,230
you have fewer variants to support if

00:28:19,950 --> 00:28:24,150
you have a standard so if it's

00:28:22,230 --> 00:28:27,660
completely free form it's a lot of work

00:28:24,150 --> 00:28:31,170
if you have some interfaces that are in

00:28:27,660 --> 00:28:34,760
between your value at and what's

00:28:31,170 --> 00:28:34,760
underneath it makes it easier

00:28:37,330 --> 00:28:44,440
so a little bit more about mkp so m-cap

00:28:40,940 --> 00:28:47,480
is the multi core communications api and

00:28:44,440 --> 00:28:49,700
you know i can't think of any system

00:28:47,480 --> 00:28:52,159
today that has more than one course

00:28:49,700 --> 00:28:55,519
where you don't need to communicate you

00:28:52,159 --> 00:28:58,999
can do it in different ways but it yeah

00:28:55,519 --> 00:29:02,480
you can't live without it and mkp has

00:28:58,999 --> 00:29:04,879
three types of communication it has

00:29:02,480 --> 00:29:07,279
connectionless messages connected

00:29:04,879 --> 00:29:12,200
package channels and connected scalar

00:29:07,279 --> 00:29:14,240
channels and messages and packets are

00:29:12,200 --> 00:29:19,789
buffered communication scalars are

00:29:14,240 --> 00:29:25,820
obviously scalars and and we also

00:29:19,789 --> 00:29:28,730
provide some functions to create and

00:29:25,820 --> 00:29:33,799
manage nodes and endpoints youth for the

00:29:28,730 --> 00:29:37,190
communication and and the message

00:29:33,799 --> 00:29:40,369
passing has inherent synchronization and

00:29:37,190 --> 00:29:44,299
which works as well in SMP environments

00:29:40,369 --> 00:29:46,519
and a MP environment so and obviously it

00:29:44,299 --> 00:29:48,230
can be compiled by any standard C

00:29:46,519 --> 00:29:49,970
compiler so the whole idea is that you

00:29:48,230 --> 00:29:54,369
can use this in pretty much any

00:29:49,970 --> 00:29:59,480
environment that you can think of it can

00:29:54,369 --> 00:30:05,960
so you can go out and find out what else

00:29:59,480 --> 00:30:09,100
is available in the topology so a an AM

00:30:05,960 --> 00:30:13,549
capita pology contains that it has

00:30:09,100 --> 00:30:16,700
domains nodes and endpoints that's what

00:30:13,549 --> 00:30:20,570
that's an endpoint is defined by domain

00:30:16,700 --> 00:30:23,119
node and endpoint or a report so what

00:30:20,570 --> 00:30:25,340
you can so it's a it's a statically

00:30:23,119 --> 00:30:27,919
defined topology at least within a

00:30:25,340 --> 00:30:31,009
domain so you can go out and discover

00:30:27,919 --> 00:30:32,929
what other nodes are available and end

00:30:31,009 --> 00:30:34,820
points in the topology and then you

00:30:32,929 --> 00:30:37,580
could do a secondary discovery to find

00:30:34,820 --> 00:30:39,879
out what are the capabilities of those

00:30:37,580 --> 00:30:39,879
nails

00:30:43,260 --> 00:30:46,010
ok

00:30:46,520 --> 00:30:53,410
an MRAP e that's for us again resource

00:30:50,630 --> 00:30:57,740
management so it provides basic

00:30:53,410 --> 00:30:59,840
synchronization locking and the ability

00:30:57,740 --> 00:31:02,210
to manipulate shared memory and remote

00:30:59,840 --> 00:31:06,580
memory and remote memory meaning

00:31:02,210 --> 00:31:06,580
something that you don't it's not a

00:31:06,880 --> 00:31:13,040
memory that everybody can see it could

00:31:10,100 --> 00:31:18,460
be for example through another device

00:31:13,040 --> 00:31:22,070
and it also has some functionality for

00:31:18,460 --> 00:31:24,590
accessing metadata in system so if you

00:31:22,070 --> 00:31:30,520
have performance counters and so forth

00:31:24,590 --> 00:31:30,520
they can be queried through the API and

00:31:31,270 --> 00:31:39,560
the task management API obviously has to

00:31:35,390 --> 00:31:43,790
do with tasks and how you can execute

00:31:39,560 --> 00:31:45,890
tasks on different cores and they don't

00:31:43,790 --> 00:31:47,530
have to be of the same kind they can be

00:31:45,890 --> 00:31:50,900
different kind and they can be

00:31:47,530 --> 00:31:55,550
accessible locally or or through some

00:31:50,900 --> 00:31:58,640
some other device and it's very similar

00:31:55,550 --> 00:32:00,740
to any task that you think of in a in

00:31:58,640 --> 00:32:03,320
the general sense you pass but in this

00:32:00,740 --> 00:32:05,570
case you pass parameters to wherever the

00:32:03,320 --> 00:32:07,870
task is located and it will return a

00:32:05,570 --> 00:32:07,870
result

00:32:10,320 --> 00:32:17,860
and then the shin that's the software

00:32:15,460 --> 00:32:24,070
hardware interface and again that's a

00:32:17,860 --> 00:32:26,320
way to describe hardware by a hardware

00:32:24,070 --> 00:32:29,590
vendor that can be used by a tools

00:32:26,320 --> 00:32:31,900
vendor to more quickly add support for

00:32:29,590 --> 00:32:35,530
whatever their hardware is so you can

00:32:31,900 --> 00:32:38,380
use it for manipulating the device or

00:32:35,530 --> 00:32:40,750
you can use it to get performance data

00:32:38,380 --> 00:32:46,110
out of the device for example memory

00:32:40,750 --> 00:32:48,940
bandwidth or something like that and and

00:32:46,110 --> 00:32:53,530
if this is a more recent recently

00:32:48,940 --> 00:33:02,110
started effort and here's just an

00:32:53,530 --> 00:33:05,560
example of the tools being used for EM

00:33:02,110 --> 00:33:10,050
copy in this case so the the tools

00:33:05,560 --> 00:33:14,560
basically provides an abstraction

00:33:10,050 --> 00:33:18,880
automation and portability so by using

00:33:14,560 --> 00:33:22,390
tools to help you to implement the MK

00:33:18,880 --> 00:33:27,280
p.m. tapia nem'ro p you can completely

00:33:22,390 --> 00:33:29,860
focus on your application code and not

00:33:27,280 --> 00:33:32,410
worry about how this applies to the

00:33:29,860 --> 00:33:37,240
underlying platform so in this case we

00:33:32,410 --> 00:33:40,870
have a tool that that you can use for EM

00:33:37,240 --> 00:33:42,850
kappa coding so the tool will ask you

00:33:40,870 --> 00:33:45,610
what do you want to do and in this case

00:33:42,850 --> 00:33:48,220
I think it's an M Cappy message and you

00:33:45,610 --> 00:33:50,140
can select we have blocking and non

00:33:48,220 --> 00:33:52,840
blocking and then it will give them then

00:33:50,140 --> 00:33:54,880
you say I want to communicate to this

00:33:52,840 --> 00:33:57,340
node and then it will give you all the

00:33:54,880 --> 00:34:00,520
available ports on that node and then

00:33:57,340 --> 00:34:04,260
when you say okay then it will will

00:34:00,520 --> 00:34:04,260
actually insert the code for you

00:34:07,890 --> 00:34:16,200
and again as I said this is a worldwide

00:34:12,630 --> 00:34:18,810
organization and we have the

00:34:16,200 --> 00:34:21,240
organization started mostly with what i

00:34:18,810 --> 00:34:23,310
would call providers and now we have

00:34:21,240 --> 00:34:26,040
both providers and consumers and

00:34:23,310 --> 00:34:33,480
obviously the consumers are the the main

00:34:26,040 --> 00:34:36,290
beneficiaries of this that's it so any

00:34:33,480 --> 00:34:36,290
questions

00:34:40,279 --> 00:34:51,810
yes yes

00:34:48,580 --> 00:34:51,810
if you have sort of

00:34:53,919 --> 00:34:57,720
ask side effects to

00:35:01,039 --> 00:35:06,890
so can you repeat the question he's

00:35:04,559 --> 00:35:20,760
recording so the question was when we

00:35:06,890 --> 00:35:24,539
use and tasks yes okay okay so the

00:35:20,760 --> 00:35:29,210
question was if we can send tasks across

00:35:24,539 --> 00:35:31,200
a network that has side effects and I

00:35:29,210 --> 00:35:32,760
unfortunately i would have to get back

00:35:31,200 --> 00:35:36,329
to you on that because i don't know the

00:35:32,760 --> 00:35:39,180
answer so I'm I'm i have been involved

00:35:36,329 --> 00:35:42,319
in in two of the other groups but not

00:35:39,180 --> 00:35:42,319

YouTube URL: https://www.youtube.com/watch?v=apUdYRemcYs


