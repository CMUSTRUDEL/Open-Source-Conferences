Title: OpenMP 4.0 - A Short Stroll -- SC14
Publication date: 2014-12-13
Playlist: SC14 OpenMP BOF and Booth Talks
Description: 
	Christian Terboven (RWTH Aachen University) and Michael Klemm (Intel), presented at Supercomputing 14, November 2014.
Captions: 
	00:00:00,000 --> 00:00:06,060
okay so welcome folks are we prepared

00:00:03,480 --> 00:00:08,760
and hopefully entertaining talk which is

00:00:06,060 --> 00:00:11,730
not quite the complete overview about oh

00:00:08,760 --> 00:00:14,160
sorry openmp for that we promised it's

00:00:11,730 --> 00:00:19,170
more like a short stroll along the road

00:00:14,160 --> 00:00:21,960
of old man p 4 so who are we so right

00:00:19,170 --> 00:00:25,609
beside to me there's Michael Michael

00:00:21,960 --> 00:00:28,500
works for a big pay a big company and

00:00:25,609 --> 00:00:30,260
whenever we go into this talk to a level

00:00:28,500 --> 00:00:32,550
where we need technical information

00:00:30,260 --> 00:00:35,270
meaning there are things broken in the

00:00:32,550 --> 00:00:37,530
system or unexplainable behavior

00:00:35,270 --> 00:00:39,989
processors are just stupid or whatever

00:00:37,530 --> 00:00:44,730
Michael is going to be blamed but he has

00:00:39,989 --> 00:00:47,160
also to explain what's going on okay and

00:00:44,730 --> 00:00:50,640
here on the left that's Christian turbo

00:00:47,160 --> 00:00:53,010
from Christian is the informed user who

00:00:50,640 --> 00:00:55,680
finds bugging our products are who has

00:00:53,010 --> 00:00:59,850
performance problems and who I'm always

00:00:55,680 --> 00:01:03,600
helping out are just in case right I do

00:00:59,850 --> 00:01:05,430
have problems thank you I do have a

00:01:03,600 --> 00:01:08,159
problem here all right yeah that's the

00:01:05,430 --> 00:01:10,229
Intel legal disclaimer we will just keep

00:01:08,159 --> 00:01:15,960
it in the screen for about 10 seconds

00:01:10,229 --> 00:01:18,390
for after the fact reading so let's

00:01:15,960 --> 00:01:21,509
start with a short motivation for our

00:01:18,390 --> 00:01:24,479
talk so there are some common or not so

00:01:21,509 --> 00:01:27,810
uncommon not so common miss beliefs one

00:01:24,479 --> 00:01:30,119
is open p does not scale so we think

00:01:27,810 --> 00:01:31,799
that's not correct because first OpenMP

00:01:30,119 --> 00:01:34,829
is a standard so how can you stand out

00:01:31,799 --> 00:01:37,380
scale or not scale so maybe things are

00:01:34,829 --> 00:01:39,540
not programmed or expressed in the right

00:01:37,380 --> 00:01:41,820
way so our aim of this talk is to give

00:01:39,540 --> 00:01:43,740
you some idea about standard problems

00:01:41,820 --> 00:01:46,710
and how to avoid them or how to get

00:01:43,740 --> 00:01:48,689
better performance another one is open p

00:01:46,710 --> 00:01:50,220
is only about four loops and simple work

00:01:48,689 --> 00:01:53,310
sharing we will show you that this is

00:01:50,220 --> 00:01:55,740
not true and we hope to convince you

00:01:53,310 --> 00:01:59,340
that at least sometimes OpenMP code even

00:01:55,740 --> 00:02:01,439
for c++ can be elegant shared memory

00:01:59,340 --> 00:02:03,299
paralyzation will not only be about

00:02:01,439 --> 00:02:06,329
course we have to take a look at some of

00:02:03,299 --> 00:02:08,099
these details that the machines exhibit

00:02:06,329 --> 00:02:10,649
and some of these properties that can

00:02:08,099 --> 00:02:12,450
spoil performance and finally we will

00:02:10,649 --> 00:02:13,800
have a glimpse into the future because

00:02:12,450 --> 00:02:16,740
you don't have to stick with

00:02:13,800 --> 00:02:19,140
khuda opencl or other bad ugly

00:02:16,740 --> 00:02:21,510
programming models in order to program

00:02:19,140 --> 00:02:23,880
for accelerators OpenMP as a solution

00:02:21,510 --> 00:02:25,560
for that and this is our agenda we will

00:02:23,880 --> 00:02:28,230
talk about the structure of data in

00:02:25,560 --> 00:02:30,120
memory how to exploit that an open p to

00:02:28,230 --> 00:02:32,040
get the right performance we will talk

00:02:30,120 --> 00:02:34,470
about vectorization meaning similar

00:02:32,040 --> 00:02:37,430
parallelism tasking as one of the

00:02:34,470 --> 00:02:39,840
features I do my best okay to be louder

00:02:37,430 --> 00:02:42,210
tasking on one one of the features

00:02:39,840 --> 00:02:44,880
introduced was open mp3 and we think

00:02:42,210 --> 00:02:47,190
it's it didn't get the attention it

00:02:44,880 --> 00:02:49,400
should have I already deserves so far

00:02:47,190 --> 00:02:53,640
and finally the accelerator stuff and

00:02:49,400 --> 00:02:55,440
let's just jump into the content here so

00:02:53,640 --> 00:02:58,710
that's a code where people think it's

00:02:55,440 --> 00:03:01,680
not really elegant and I agree so what

00:02:58,710 --> 00:03:05,160
did code does is it adds a bounding box

00:03:01,680 --> 00:03:06,600
of a 2d point cloud that looks that that

00:03:05,160 --> 00:03:08,790
sounds interesting in the end it

00:03:06,600 --> 00:03:11,280
computes a minimum and maximum that is

00:03:08,790 --> 00:03:13,800
what this code is doing so we have for

00:03:11,280 --> 00:03:16,230
c++ data structure here which is named

00:03:13,800 --> 00:03:18,530
point 2d and it comes with all the

00:03:16,230 --> 00:03:21,030
functionalities that you expect so i can

00:03:18,530 --> 00:03:22,980
compute i can create a variable named

00:03:21,030 --> 00:03:25,290
lower bound and one upper bound and

00:03:22,980 --> 00:03:27,870
initialize the first with a maximum and

00:03:25,290 --> 00:03:30,150
the second one was a minimum and then i

00:03:27,870 --> 00:03:32,700
have a for loop which applies the c++

00:03:30,150 --> 00:03:34,500
iterator that goes through all the point

00:03:32,700 --> 00:03:37,920
loops or all the points in the cloud

00:03:34,500 --> 00:03:40,260
sorry and i will compare my current

00:03:37,920 --> 00:03:43,890
minimum and maximum with all the points

00:03:40,260 --> 00:03:45,739
in the cloud to find a way or to

00:03:43,890 --> 00:03:47,970
basically find the minimum and maximum

00:03:45,739 --> 00:03:50,370
so people believe this cannot be

00:03:47,970 --> 00:03:52,440
parallelized with OpenMP but before we

00:03:50,370 --> 00:03:54,510
take a look how to do that i will

00:03:52,440 --> 00:03:57,239
promise you this code actually is pretty

00:03:54,510 --> 00:03:58,920
slow and michael has to explain what is

00:03:57,239 --> 00:04:01,680
slow and then we take a look back and

00:03:58,920 --> 00:04:03,300
how to improve this yes I guess I'm

00:04:01,680 --> 00:04:06,239
going to be blamed for that the code is

00:04:03,300 --> 00:04:07,920
slow but it's actually the hardware so

00:04:06,239 --> 00:04:10,080
there's a memory hierarchy between the

00:04:07,920 --> 00:04:13,230
processor and the main memory so their

00:04:10,080 --> 00:04:16,410
registers caches and then obviously the

00:04:13,230 --> 00:04:18,180
capacity main memory so whenever your

00:04:16,410 --> 00:04:20,459
excess teta you should keep it as close

00:04:18,180 --> 00:04:22,169
as possible to the registers if it

00:04:20,459 --> 00:04:24,630
doesn't work out in the registers keep

00:04:22,169 --> 00:04:26,280
it close to the cash and only you know

00:04:24,630 --> 00:04:27,240
if the cash doesn't doesn't work out for

00:04:26,280 --> 00:04:28,650
you then

00:04:27,240 --> 00:04:31,289
you need the data from from main memory

00:04:28,650 --> 00:04:33,270
and this is what what happening in the

00:04:31,289 --> 00:04:36,389
in this particular code because it's

00:04:33,270 --> 00:04:38,910
streaming data from the main memory into

00:04:36,389 --> 00:04:42,120
the cache into the CPU and that takes

00:04:38,910 --> 00:04:44,430
time okay and then there is something

00:04:42,120 --> 00:04:47,490
called fault sharing so that that's

00:04:44,430 --> 00:04:49,860
happening any time course start to share

00:04:47,490 --> 00:04:52,770
the same cache line they are working on

00:04:49,860 --> 00:04:54,569
this joint data but in fact the data is

00:04:52,770 --> 00:04:56,729
located in the same cache line so what's

00:04:54,569 --> 00:04:59,789
happening that the cache line is

00:04:56,729 --> 00:05:01,650
traveling back and forth throughout the

00:04:59,789 --> 00:05:05,009
system and this is also causing

00:05:01,650 --> 00:05:06,690
overheads so it's not actually you know

00:05:05,009 --> 00:05:08,610
the hardware is to blame but actually

00:05:06,690 --> 00:05:11,310
the code is doing something wrong here

00:05:08,610 --> 00:05:14,819
right oh that's actually a nice

00:05:11,310 --> 00:05:16,710
animation here okay yeah so let me

00:05:14,819 --> 00:05:18,659
summarize it if you would build machines

00:05:16,710 --> 00:05:19,949
as nice as this animation we wouldn't

00:05:18,659 --> 00:05:24,060
have the trouble that we are having

00:05:19,949 --> 00:05:26,789
right okay so up mp3 actually introduced

00:05:24,060 --> 00:05:29,610
the support for iterator loops in openmp

00:05:26,789 --> 00:05:32,810
ah well in Oakland people work sharing

00:05:29,610 --> 00:05:36,750
so the openmp do construct for fortran

00:05:32,810 --> 00:05:39,330
in owen photon terminology the four in C

00:05:36,750 --> 00:05:41,969
and C++ do support does support iterator

00:05:39,330 --> 00:05:44,090
loops if the actual iterator is a random

00:05:41,969 --> 00:05:47,820
access iterator so that means there's a

00:05:44,090 --> 00:05:50,550
constant time in order to jump from one

00:05:47,820 --> 00:05:52,680
element in the iteration space to

00:05:50,550 --> 00:05:55,889
another one so you can just write pragma

00:05:52,680 --> 00:05:58,680
on p4 in case of this code snippet here

00:05:55,889 --> 00:06:00,960
and the problem that Michael just

00:05:58,680 --> 00:06:02,759
explains is for sharing is causing the

00:06:00,960 --> 00:06:06,690
bad performance in many codes where

00:06:02,759 --> 00:06:09,060
people try to support the reduction of

00:06:06,690 --> 00:06:11,460
this minimum maximum by creating an

00:06:09,060 --> 00:06:15,509
array of the corresponding types and

00:06:11,460 --> 00:06:17,880
then persuade computing a minimum at the

00:06:15,509 --> 00:06:20,069
max moment finding the global one in the

00:06:17,880 --> 00:06:21,810
second step so if you create an array of

00:06:20,069 --> 00:06:24,150
the dimension of the number of threads

00:06:21,810 --> 00:06:26,849
exactly what Michael Joseph's explained

00:06:24,150 --> 00:06:30,060
to us will happen multiple variables

00:06:26,849 --> 00:06:32,840
will end up on the same cache line so an

00:06:30,060 --> 00:06:35,009
open before we are not only we are not

00:06:32,840 --> 00:06:37,110
eliminating for sharing but we bring

00:06:35,009 --> 00:06:39,419
user-defined reductions and here the

00:06:37,110 --> 00:06:40,909
compiler plus a runtime can do the right

00:06:39,419 --> 00:06:43,680
thing and give you the best performance

00:06:40,909 --> 00:06:47,039
so what we do here is we declare a

00:06:43,680 --> 00:06:49,650
reduction of a type of a name min p on

00:06:47,039 --> 00:06:51,630
the variable point to d and the

00:06:49,650 --> 00:06:54,210
reduction is a binary operation so we

00:06:51,630 --> 00:06:55,800
get an input so here's an input and we

00:06:54,210 --> 00:06:58,080
get an output and the industry

00:06:55,800 --> 00:07:00,389
collaboration we define how the

00:06:58,080 --> 00:07:02,789
operation a reduction operation for two

00:07:00,389 --> 00:07:05,520
elements of the point to the available

00:07:02,789 --> 00:07:07,860
for the name of the reduction min p will

00:07:05,520 --> 00:07:10,349
be performed and we will do the same for

00:07:07,860 --> 00:07:13,110
max p and then we can write pokémon

00:07:10,349 --> 00:07:15,479
people for reduction min p to compute

00:07:13,110 --> 00:07:18,180
the lower bound in max p to compute the

00:07:15,479 --> 00:07:21,060
upper bound and that all it takes to

00:07:18,180 --> 00:07:24,599
paralyze is random access iterator C++

00:07:21,060 --> 00:07:26,339
loop and to get the right performance

00:07:24,599 --> 00:07:30,240
and avoid so for sharing has just

00:07:26,339 --> 00:07:32,639
explained so what do we have to consider

00:07:30,240 --> 00:07:34,949
when this point load is really really

00:07:32,639 --> 00:07:38,580
big talking about many many hundreds of

00:07:34,949 --> 00:07:41,070
megabytes okay so there's more hardware

00:07:38,580 --> 00:07:43,589
story to tell so in fact every system

00:07:41,070 --> 00:07:46,110
today is a numerous system so that means

00:07:43,589 --> 00:07:47,820
we have multiple memories that have

00:07:46,110 --> 00:07:50,159
different Layton sees different

00:07:47,820 --> 00:07:52,529
bandwidth characteristics compared to

00:07:50,159 --> 00:07:54,149
the other parts of the system so what's

00:07:52,529 --> 00:07:56,699
happening and this is what the operating

00:07:54,149 --> 00:08:01,620
system actually does once you do analog

00:07:56,699 --> 00:08:03,960
or Malik ation of data the data is not

00:08:01,620 --> 00:08:06,449
actually allocated it's just that the

00:08:03,960 --> 00:08:08,009
operating system guarantees you that

00:08:06,449 --> 00:08:09,960
there is a valid pointer behind this

00:08:08,009 --> 00:08:13,320
data and once you start touching the

00:08:09,960 --> 00:08:15,300
data and I think this is now here the

00:08:13,320 --> 00:08:16,979
data pages so the physical pages are

00:08:15,300 --> 00:08:19,110
actually allocated somewhere in the

00:08:16,979 --> 00:08:22,289
system this is called first touch policy

00:08:19,110 --> 00:08:24,990
and what's happening is the core that

00:08:22,289 --> 00:08:28,199
actually allocates the different the

00:08:24,990 --> 00:08:30,149
page the first time decides on where

00:08:28,199 --> 00:08:32,520
that page is put and its load lose you

00:08:30,149 --> 00:08:34,560
usually the local memory that is

00:08:32,520 --> 00:08:37,740
attached to this particular package or

00:08:34,560 --> 00:08:40,620
socket as we call it so the trick here

00:08:37,740 --> 00:08:43,169
is to actually power eyes data

00:08:40,620 --> 00:08:45,120
initialization you do the murloc and

00:08:43,169 --> 00:08:48,270
when you touch the data the first time

00:08:45,120 --> 00:08:51,030
you paralyze the initialization the same

00:08:48,270 --> 00:08:52,260
way as you would do the date with the

00:08:51,030 --> 00:08:54,300
computation

00:08:52,260 --> 00:08:56,130
making sure that the data stays close to

00:08:54,300 --> 00:09:01,170
the threats that are actually working on

00:08:56,130 --> 00:09:03,330
it um so the effect you can see here on

00:09:01,170 --> 00:09:04,980
that slide so what's happening is

00:09:03,330 --> 00:09:07,140
depending on whether do the right

00:09:04,980 --> 00:09:08,730
initialization or a wrong in

00:09:07,140 --> 00:09:11,130
initialization you get different

00:09:08,730 --> 00:09:13,680
performance and it can be up to a factor

00:09:11,130 --> 00:09:16,770
of two maybe three that is going wrong

00:09:13,680 --> 00:09:18,590
here okay let's see if Christian my

00:09:16,770 --> 00:09:21,930
informed user actually learned something

00:09:18,590 --> 00:09:23,400
yeah I'm working in this thing called

00:09:21,930 --> 00:09:25,800
the open p language community and

00:09:23,400 --> 00:09:28,080
together as a big group we designed the

00:09:25,800 --> 00:09:29,820
feature to explore it exactly that so

00:09:28,080 --> 00:09:33,090
opening before brought the thing that we

00:09:29,820 --> 00:09:34,920
call thread affinity support so we do

00:09:33,090 --> 00:09:37,290
not want you to worry about how to bind

00:09:34,920 --> 00:09:39,600
threads to specific course what we want

00:09:37,290 --> 00:09:41,910
you to do is to about is to reason about

00:09:39,600 --> 00:09:44,040
how to distribute threads in the system

00:09:41,910 --> 00:09:47,430
so we have a level of abstraction which

00:09:44,040 --> 00:09:49,020
we call OMP places only places defines a

00:09:47,430 --> 00:09:51,450
granularity at which you want to perform

00:09:49,020 --> 00:09:53,220
the binding and you can put several

00:09:51,450 --> 00:09:54,960
different abstract names you can say

00:09:53,220 --> 00:09:57,060
sockets and you distribute the threads

00:09:54,960 --> 00:09:59,040
between sockets course then you're

00:09:57,060 --> 00:10:01,380
talking about physical cores or threads

00:09:59,040 --> 00:10:03,210
then you're talking Hardware threads or

00:10:01,380 --> 00:10:05,790
hyper threads as your company is calling

00:10:03,210 --> 00:10:08,430
it so what I'm saying here is I want to

00:10:05,790 --> 00:10:10,680
understand fine how my sweats are

00:10:08,430 --> 00:10:12,510
distributed among the physical course in

00:10:10,680 --> 00:10:14,700
the system I don't care about hyper

00:10:12,510 --> 00:10:16,860
sweat so that means the threads the open

00:10:14,700 --> 00:10:19,260
p sweats can roam between the hyper

00:10:16,860 --> 00:10:21,540
threads in the physical core and what i

00:10:19,260 --> 00:10:23,520
am adding here now is the prop bind

00:10:21,540 --> 00:10:25,650
clause so propyne in previous version

00:10:23,520 --> 00:10:29,040
versions of open p was either one or

00:10:25,650 --> 00:10:31,740
zero meaning on or off now we have

00:10:29,040 --> 00:10:33,630
different thread affinity policies so

00:10:31,740 --> 00:10:35,490
spread means I want to spread the

00:10:33,630 --> 00:10:38,160
threads of our as far apart as possible

00:10:35,490 --> 00:10:39,690
on my machine to give me the best memory

00:10:38,160 --> 00:10:41,340
bandwidth that means in the case of two

00:10:39,690 --> 00:10:43,950
threads a run on two different sockets

00:10:41,340 --> 00:10:46,350
and we also have closed and master which

00:10:43,950 --> 00:10:49,410
will put strings closer together but not

00:10:46,350 --> 00:10:54,300
uh not the right solution for our

00:10:49,410 --> 00:10:56,550
problem so that's nice but I mean I'm

00:10:54,300 --> 00:10:59,160
user I always want more performance and

00:10:56,550 --> 00:11:01,530
I'm at this conference here in int'l is

00:10:59,160 --> 00:11:03,930
talking about our they're adding to the

00:11:01,530 --> 00:11:05,400
project processors can you explain you

00:11:03,930 --> 00:11:08,010
what's going on here

00:11:05,400 --> 00:11:09,390
sure you get more performance and the

00:11:08,010 --> 00:11:12,840
way to get more performance is

00:11:09,390 --> 00:11:15,090
vectorization so that's where we execute

00:11:12,840 --> 00:11:18,060
the same instructions on the same on the

00:11:15,090 --> 00:11:20,040
same data basically so this is what's

00:11:18,060 --> 00:11:22,890
happening over the past years so we've

00:11:20,040 --> 00:11:25,080
been doubling the performance of the

00:11:22,890 --> 00:11:27,870
system by doubling the vector with over

00:11:25,080 --> 00:11:30,930
the past couple of years are getting to

00:11:27,870 --> 00:11:33,090
the extreme of 512 bits per vector on

00:11:30,930 --> 00:11:34,110
the C and Phi coprocessor now that's

00:11:33,090 --> 00:11:37,380
something that you need to actually

00:11:34,110 --> 00:11:39,600
program for and most compilers did

00:11:37,380 --> 00:11:41,460
something like this so you had your for

00:11:39,600 --> 00:11:43,110
loop you have to convince the compiler

00:11:41,460 --> 00:11:45,450
to actually do something about

00:11:43,110 --> 00:11:48,360
vectorization to own your data

00:11:45,450 --> 00:11:51,330
dependencies to start vectorization and

00:11:48,360 --> 00:11:53,070
then also paralyzed now the problem is

00:11:51,330 --> 00:11:55,110
with that you need to trust your

00:11:53,070 --> 00:11:58,260
favorite compiler to actually do the

00:11:55,110 --> 00:12:00,900
right thing and this is not happening in

00:11:58,260 --> 00:12:02,370
reality for some cases so the way to

00:12:00,900 --> 00:12:03,900
solve that is either change the

00:12:02,370 --> 00:12:06,660
programming model use something like

00:12:03,900 --> 00:12:09,300
Airy notations in C go for the compiler

00:12:06,660 --> 00:12:12,180
pragmas or even go low level using

00:12:09,300 --> 00:12:14,130
something like intrinsics to actually

00:12:12,180 --> 00:12:17,220
tell the Machine more specifically what

00:12:14,130 --> 00:12:22,230
to do on a very low level does that help

00:12:17,220 --> 00:12:24,600
you so low level intrinsic these sound

00:12:22,230 --> 00:12:27,330
really a key but it's very seldom are

00:12:24,600 --> 00:12:29,130
very rare that they can tell good about

00:12:27,330 --> 00:12:31,290
good things about the work that you do

00:12:29,130 --> 00:12:34,260
but here there's one thing because this

00:12:31,290 --> 00:12:36,780
nice guy contributed to the concert i'm

00:12:34,260 --> 00:12:40,170
going to explain here so what we have an

00:12:36,780 --> 00:12:41,940
opening p is explicit simdi support so

00:12:40,170 --> 00:12:44,910
you can put it on for loops and you can

00:12:41,940 --> 00:12:47,690
also combine it with a parallel and the

00:12:44,910 --> 00:12:50,940
work share and construct pokémon p simdi

00:12:47,690 --> 00:12:53,150
for the loop instructs the compiler to

00:12:50,940 --> 00:12:57,020
sim dice simply is how do you call it

00:12:53,150 --> 00:13:00,210
vector eyes vector I thank you the

00:12:57,020 --> 00:13:02,100
following loop and you can combine it

00:13:00,210 --> 00:13:04,950
with for example the way down reduction

00:13:02,100 --> 00:13:07,140
operation OpenMP you can instruct it and

00:13:04,950 --> 00:13:09,510
how many iterations are independent so

00:13:07,140 --> 00:13:13,230
that the compiler can explicit as much

00:13:09,510 --> 00:13:15,420
Cindy parallelism essays in the code and

00:13:13,230 --> 00:13:18,660
he actually you can express a linear

00:13:15,420 --> 00:13:21,420
dependency of the variable pointer

00:13:18,660 --> 00:13:22,949
with respect to the loop index and there

00:13:21,420 --> 00:13:25,199
are some more clauses we don't want to

00:13:22,949 --> 00:13:27,720
go into detail here but the good news is

00:13:25,199 --> 00:13:31,410
that open p4 brought to standardized

00:13:27,720 --> 00:13:32,759
feature the sim deconstruct to support

00:13:31,410 --> 00:13:34,440
vectorization for almost all

00:13:32,759 --> 00:13:36,839
architectures at least the ones that we

00:13:34,440 --> 00:13:38,579
are aware of and it will be available in

00:13:36,839 --> 00:13:40,860
all the different compilers supporting

00:13:38,579 --> 00:13:43,170
OpenMP so that's the first time that we

00:13:40,860 --> 00:13:48,889
have an industry standard to express a

00:13:43,170 --> 00:13:51,389
vector level parallelism yeah so this is

00:13:48,889 --> 00:13:53,939
this was really nice but we had really

00:13:51,389 --> 00:13:56,279
simple quotes so can we do more

00:13:53,939 --> 00:14:00,360
interesting things with openmp like the

00:13:56,279 --> 00:14:02,850
real hard problems like Sudoku and we

00:14:00,360 --> 00:14:04,709
will show yeah we can do that so instead

00:14:02,850 --> 00:14:06,750
Oakley has a problem that the individual

00:14:04,709 --> 00:14:09,240
loops are really short so int in the

00:14:06,750 --> 00:14:13,019
beginning I said element p is more than

00:14:09,240 --> 00:14:15,810
just work sharing of simple loops so in

00:14:13,019 --> 00:14:18,689
Sudoku this is a very simple algorithm

00:14:15,810 --> 00:14:21,149
it gives me all the valid results of

00:14:18,689 --> 00:14:23,189
this Sudoku I first find an empty field

00:14:21,149 --> 00:14:25,920
like this one I insert the number checks

00:14:23,189 --> 00:14:27,839
the Sudoku if the numbers invalid I can

00:14:25,920 --> 00:14:30,750
try the next ones and here I do have

00:14:27,839 --> 00:14:33,509
some parallelism so when trying all the

00:14:30,750 --> 00:14:37,410
next numbers I can spawn what we call

00:14:33,509 --> 00:14:39,240
tasks in order to go parallel here and

00:14:37,410 --> 00:14:42,300
if it's valid we continue to the next

00:14:39,240 --> 00:14:44,550
field so at the end of this we can we

00:14:42,300 --> 00:14:46,110
have all valid solutions there's no big

00:14:44,550 --> 00:14:49,199
loops the longest one is probably this

00:14:46,110 --> 00:14:51,449
one from 1 to 16 in the case of Sudoku

00:14:49,199 --> 00:14:53,639
and I can't parallel Isis for this thing

00:14:51,449 --> 00:14:56,550
like the intro mic which is way too many

00:14:53,639 --> 00:14:59,339
threads to deal with for short loops ok

00:14:56,550 --> 00:15:01,920
and this is a task construct that we

00:14:59,339 --> 00:15:03,990
have an old man p task basically are all

00:15:01,920 --> 00:15:06,480
independent from the other task which is

00:15:03,990 --> 00:15:09,000
a slightly confusing recursive

00:15:06,480 --> 00:15:11,399
definition but if you think about it for

00:15:09,000 --> 00:15:13,350
a bit longer it might be intuitive so

00:15:11,399 --> 00:15:15,959
all the towns are small independent

00:15:13,350 --> 00:15:18,089
pieces of code and data and the open p

00:15:15,959 --> 00:15:20,490
runtime is responsible to scheduling

00:15:18,089 --> 00:15:22,740
them to the threads with open Pete

00:15:20,490 --> 00:15:24,360
asking you express a parallel ISM in the

00:15:22,740 --> 00:15:26,579
code and you have to trust your runtime

00:15:24,360 --> 00:15:29,069
to do a good job of scheduling these

00:15:26,579 --> 00:15:31,730
tasks on to the threads that's

00:15:29,069 --> 00:15:33,829
particularly useful for records

00:15:31,730 --> 00:15:35,660
calls because tasks can be nested so a

00:15:33,829 --> 00:15:38,269
task and create more tasks which can

00:15:35,660 --> 00:15:40,639
create more tasks and so on and you have

00:15:38,269 --> 00:15:42,800
the data scoping closes on these tasks

00:15:40,639 --> 00:15:44,290
as you know them on the task construct

00:15:42,800 --> 00:15:46,959
as you know them from the other

00:15:44,290 --> 00:15:51,110
constructs I don't go into syntax

00:15:46,959 --> 00:15:53,060
details here just two more things at the

00:15:51,110 --> 00:15:55,250
task wait we will see it on the next

00:15:53,060 --> 00:15:58,130
slide we can wait for all the tasks that

00:15:55,250 --> 00:16:00,199
we created so only the direct child has

00:15:58,130 --> 00:16:02,329
not all the other ones and at the

00:16:00,199 --> 00:16:04,010
barrier after the barrier when all

00:16:02,329 --> 00:16:07,459
threads have reached them all the tasks

00:16:04,010 --> 00:16:10,100
are guaranteed to complete so this is

00:16:07,459 --> 00:16:12,500
how task parallel Sudoku looks like an

00:16:10,100 --> 00:16:14,240
old man p first we need to parallel

00:16:12,500 --> 00:16:15,980
region which is a construct that gives

00:16:14,240 --> 00:16:17,990
us a team of threats we always need a

00:16:15,980 --> 00:16:19,880
team of threads and augment p to do

00:16:17,990 --> 00:16:22,370
something in parallel but as my

00:16:19,880 --> 00:16:24,589
algorithm here starts was a single field

00:16:22,370 --> 00:16:28,040
and starts to insert the first number

00:16:24,589 --> 00:16:29,810
many one we have a partner on p single

00:16:28,040 --> 00:16:32,269
here so that means the parallel region

00:16:29,810 --> 00:16:33,920
will be created but only one thread will

00:16:32,269 --> 00:16:36,230
start with the first step in my

00:16:33,920 --> 00:16:38,449
algorithms or the others will jump

00:16:36,230 --> 00:16:40,639
around and they are ready to take a look

00:16:38,449 --> 00:16:42,529
at some imaginary works you so that

00:16:40,639 --> 00:16:44,959
means as soon as tasks have been created

00:16:42,529 --> 00:16:46,639
these Reds waiting at the end of the

00:16:44,959 --> 00:16:48,709
single or maybe even at the end of the

00:16:46,639 --> 00:16:51,350
parallel region can pick up the task and

00:16:48,709 --> 00:16:52,819
bring them to execution I said already

00:16:51,350 --> 00:16:55,760
on the previous slide that these

00:16:52,819 --> 00:16:58,670
checking of the validation of the other

00:16:55,760 --> 00:17:00,949
numbers are the individual tasks meaning

00:16:58,670 --> 00:17:02,300
the individual work packages so this is

00:17:00,949 --> 00:17:06,130
where we use the pragma beat us

00:17:02,300 --> 00:17:08,660
construct which will create a set of

00:17:06,130 --> 00:17:11,750
pseudocode board copies and so on and

00:17:08,660 --> 00:17:15,890
finally we have to wait are at the end

00:17:11,750 --> 00:17:19,280
to print out all the reorder solutions

00:17:15,890 --> 00:17:21,799
that we have found wherever code I'll

00:17:19,280 --> 00:17:23,630
view here as well so that's partner will

00:17:21,799 --> 00:17:25,939
be parallel program piecing let's just

00:17:23,630 --> 00:17:28,540
explained soft parallel is a function

00:17:25,939 --> 00:17:31,790
call that will go through all the

00:17:28,540 --> 00:17:33,710
possible sudoku fields try all the

00:17:31,790 --> 00:17:36,559
possible solutions this is what I just

00:17:33,710 --> 00:17:38,720
explained and here you actually see the

00:17:36,559 --> 00:17:41,210
task construct so the su doku is

00:17:38,720 --> 00:17:42,980
actually a pointer I kept to the pointer

00:17:41,210 --> 00:17:45,260
with the first private construct use a

00:17:42,980 --> 00:17:47,750
C++ copy constructor to get the news

00:17:45,260 --> 00:17:49,250
Doku bored and at the end I wait that's

00:17:47,750 --> 00:17:51,410
all it takes if you're interested in the

00:17:49,250 --> 00:17:54,500
code let send me an email and i will

00:17:51,410 --> 00:17:56,060
answer with the code it's open source so

00:17:54,500 --> 00:17:58,040
we think tasking is really powerful

00:17:56,060 --> 00:18:01,100
constructing OpenMP if you have any

00:17:58,040 --> 00:18:02,750
irregular problems give it a try if you

00:18:01,100 --> 00:18:05,600
have a for loop stick most of work

00:18:02,750 --> 00:18:08,300
sharing constructs but I still sometimes

00:18:05,600 --> 00:18:10,700
not necessarily for Zeus Doku sometimes

00:18:08,300 --> 00:18:13,040
I need more power do you have something

00:18:10,700 --> 00:18:15,980
for me alright I have something for you

00:18:13,040 --> 00:18:17,810
buddy ah so there are accelerators and

00:18:15,980 --> 00:18:20,540
coprocessors so basically we are now

00:18:17,810 --> 00:18:22,340
entering the world of hybrid programming

00:18:20,540 --> 00:18:24,260
and I think you brought us it as a

00:18:22,340 --> 00:18:28,520
single simple exciting sound about it

00:18:24,260 --> 00:18:30,440
right there it is so this particular

00:18:28,520 --> 00:18:32,810
code is really really compute intensive

00:18:30,440 --> 00:18:35,480
and really important it's a median

00:18:32,810 --> 00:18:37,880
filter so what I want to do is I want to

00:18:35,480 --> 00:18:39,710
get the medium color value of all the

00:18:37,880 --> 00:18:41,420
neighboring pixels here so that's very

00:18:39,710 --> 00:18:43,820
good for photoshopping you out of the

00:18:41,420 --> 00:18:46,550
photos that I have with me together here

00:18:43,820 --> 00:18:48,410
so what I do is I get the color of all

00:18:46,550 --> 00:18:51,230
the neighboring pixels saw them as

00:18:48,410 --> 00:18:52,880
scanning and choose a new color value to

00:18:51,230 --> 00:18:55,550
be actually the medium here and then I

00:18:52,880 --> 00:18:57,890
do the update simple algorithm if you

00:18:55,550 --> 00:18:59,630
implement that you can make use of crack

00:18:57,890 --> 00:19:02,390
my own p parallel for the win on work

00:18:59,630 --> 00:19:05,270
sharing construct and here does a bite

00:19:02,390 --> 00:19:07,450
on exalting and it's a long code it's a

00:19:05,270 --> 00:19:11,120
long loop and it takes long on your

00:19:07,450 --> 00:19:14,390
sometimes slow processors know what are

00:19:11,120 --> 00:19:16,820
you kidding me no ok so let's get this

00:19:14,390 --> 00:19:18,800
thing done on the C and Phi coprocessor

00:19:16,820 --> 00:19:21,950
so here's the model that we are looking

00:19:18,800 --> 00:19:23,990
at so open in p4 introduces support for

00:19:21,950 --> 00:19:25,790
accelerators and coprocessors not

00:19:23,990 --> 00:19:28,010
necessarily restrict it to c and phi

00:19:25,790 --> 00:19:30,050
coprocessors but of course intel

00:19:28,010 --> 00:19:32,000
supports them fully here the device

00:19:30,050 --> 00:19:34,760
model basically is we have one single

00:19:32,000 --> 00:19:37,520
host that executes everything in terms

00:19:34,760 --> 00:19:40,010
of no regular multi-threading what we

00:19:37,520 --> 00:19:41,930
executed a on on see on processors for

00:19:40,010 --> 00:19:44,570
example and then there are multiple

00:19:41,930 --> 00:19:47,090
accelerators or coprocessors and the

00:19:44,570 --> 00:19:48,800
only restriction here is you can have as

00:19:47,090 --> 00:19:51,020
many of them as you want but they need

00:19:48,800 --> 00:19:53,510
to be of the same kind so only c and phi

00:19:51,020 --> 00:19:55,879
coprocessors or other types of

00:19:53,510 --> 00:19:58,219
accelerates of the same architecture

00:19:55,879 --> 00:20:00,169
okay and this is how you actually get

00:19:58,219 --> 00:20:02,269
this piece of code over to the

00:20:00,169 --> 00:20:04,789
coprocessor now there's a new construct

00:20:02,269 --> 00:20:07,849
introduced with open mp4 it's called

00:20:04,789 --> 00:20:10,219
OpenMP target ah what it does it tells

00:20:07,849 --> 00:20:12,979
the runtime system and the compiler to

00:20:10,219 --> 00:20:16,219
send an instruction stream over from the

00:20:12,979 --> 00:20:18,469
hose to a target device execute it there

00:20:16,219 --> 00:20:21,169
and then bring control back to the host

00:20:18,469 --> 00:20:22,969
right that's the easy part now every

00:20:21,169 --> 00:20:24,709
code needs to work on data except for

00:20:22,969 --> 00:20:27,139
hello world and this still works on data

00:20:24,709 --> 00:20:29,869
namely hello world so we have a map

00:20:27,139 --> 00:20:31,699
cloth where you can tell how to map the

00:20:29,869 --> 00:20:34,519
data from the host into the target

00:20:31,699 --> 00:20:37,369
device there are directions in out and

00:20:34,519 --> 00:20:40,699
in out in transfer from the host of the

00:20:37,369 --> 00:20:43,129
device out transfer back and in out you

00:20:40,699 --> 00:20:45,319
know do their natural thing transferred

00:20:43,129 --> 00:20:47,869
over compute and transfer it back okay

00:20:45,319 --> 00:20:49,729
ah we stick that in front of the parole

00:20:47,869 --> 00:20:52,249
for and that's basically all you need to

00:20:49,729 --> 00:20:54,769
do for now the compiler will take care

00:20:52,249 --> 00:20:57,589
of compiling that particular code for

00:20:54,769 --> 00:20:59,749
the coprocessor send it over spawn the

00:20:57,589 --> 00:21:02,989
parallelism on the coprocessor execute

00:20:59,749 --> 00:21:05,659
in parallel and then return back there

00:21:02,989 --> 00:21:07,609
is also a way to avoid unnecessary

00:21:05,659 --> 00:21:09,739
copies so usually in typical

00:21:07,609 --> 00:21:11,539
applications what you do is you transfer

00:21:09,739 --> 00:21:13,699
back and forth between the host and the

00:21:11,539 --> 00:21:16,969
coprocessor because it's not so shiny

00:21:13,699 --> 00:21:18,259
and bright in terms of code that you can

00:21:16,969 --> 00:21:19,999
execute everything on the coprocessor

00:21:18,259 --> 00:21:22,519
there are still parts that you want to

00:21:19,999 --> 00:21:25,459
keep on the hose so what you do is you

00:21:22,519 --> 00:21:29,089
can then allocate a target region a

00:21:25,459 --> 00:21:31,549
target data region which are enables you

00:21:29,089 --> 00:21:33,969
to buffer data on the coprocessor you

00:21:31,549 --> 00:21:36,649
can offload into the target region on

00:21:33,969 --> 00:21:38,690
the data is already there you can

00:21:36,649 --> 00:21:41,059
compute their you return to the host you

00:21:38,690 --> 00:21:43,729
can update things or you can transfer

00:21:41,059 --> 00:21:46,129
data back and forth as on a needed basis

00:21:43,729 --> 00:21:49,190
and you can selectively keep data over

00:21:46,129 --> 00:21:51,409
on the coprocessor let me show you how

00:21:49,190 --> 00:21:54,739
that works in terms of a more visual

00:21:51,409 --> 00:21:58,399
experience so we have a pointer yes

00:21:54,739 --> 00:22:00,709
isn't that visual that's racial stop

00:21:58,399 --> 00:22:04,759
interrupting me buddy um so we have the

00:22:00,709 --> 00:22:08,629
host which has some data pointer we map

00:22:04,759 --> 00:22:09,200
it to the coprocessor that having said

00:22:08,629 --> 00:22:12,380
that it's

00:22:09,200 --> 00:22:14,299
over we execute some code on the

00:22:12,380 --> 00:22:16,700
coprocessor and then when we're done

00:22:14,299 --> 00:22:20,059
with transferring data back from from

00:22:16,700 --> 00:22:22,340
the target device back to the host okay

00:22:20,059 --> 00:22:24,860
and then we're done okay that was our

00:22:22,340 --> 00:22:28,250
short tour through open mp4 there is

00:22:24,860 --> 00:22:31,399
more so we skipped a lot of in details

00:22:28,250 --> 00:22:34,279
on on the syntax on there are reference

00:22:31,399 --> 00:22:36,289
cards that you can find a ww OpenMP org

00:22:34,279 --> 00:22:39,200
please feel free to download them

00:22:36,289 --> 00:22:41,269
therefore C C++ and Fortran and of

00:22:39,200 --> 00:22:43,700
course new thing at supercomputing we

00:22:41,269 --> 00:22:46,010
have tr3 out so that's the first step

00:22:43,700 --> 00:22:47,960
towards opening p41 which will add great

00:22:46,010 --> 00:22:55,130
new features and that's about it thank

00:22:47,960 --> 00:22:57,610
you thank you later other questions

00:22:55,130 --> 00:22:57,610
right

00:22:58,720 --> 00:23:08,159

YouTube URL: https://www.youtube.com/watch?v=sYJbtjIbY8E


